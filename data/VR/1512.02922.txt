MetaSpace II: Object and full-body tracking for interaction
and navigation in social VR

Misha Sra
MIT Media Lab
Cambridge, MA, 02142 USA

Chris Schmandt
MIT Media Lab
Cambridge, MA, 02142 USA

5
1
0
2
c
e
D
9

]

C
H
.
s
c
[

1
v
2
2
9
2
0
.
2
1
5
1
:
v
i
X
r
a

Figure 1. MetaSpace II is a social Virtual Reality system where multiple users can not only a) see and hear but also walk around in space, and b) grasp
and manipulate objects. When users touch or pick up an object in the virtual world, they simultaneously also touch or pick up it’s counterpart in the
physical world. (c) In MetaSpace users interact with each other both in the real, and d) the virtual world.

ABSTRACT
MetaSpace II (MS2) is a social Virtual Reality (VR) system
where multiple users can not only see and hear but also in-
teract with each other, grasp and manipulate objects, walk
around in space, and get tactile feedback. MS2 allows walk-
ing in physical space by tracking each user’s skeleton in real-
time and allows users to feel by employing passive haptics
i.e., when users touch or manipulate an object in the virtual
world, they simultaneously also touch or manipulate a corre-
sponding object in the physical world. To enable these ele-
ments in VR, MS2 creates a correspondence in spatial layout
and object placement by building the virtual world on top of
a 3D scan of the real world. Through the association between
the real and virtual world, users are able to walk freely while
wearing a head-mounted device, avoid obstacles like walls
and furniture, and interact with people and objects. Most cur-
rent virtual reality (VR) environments are designed for a sin-
gle user experience where interactions with virtual objects are
mediated by hand-held input devices or hand gestures. Addi-
tionally, users are only shown a representation of their hands
in VR ﬂoating in front of the camera as seen from a ﬁrst per-
son perspective. We believe, representing each user as a full-
body avatar that is controlled by natural movements of the
person in the real world (see Figure 1d), can greatly enhance
believability and a user’s sense immersion in VR.

Author Keywords
Social Virtual Reality; Full-body tracking; Passive Haptics;
Locomotion; Object tracking; Head-mounted Display.
ACM Classiﬁcation Keywords
H.5.1. Information Interfaces and Presentation (e.g. HCI):
Multimedia Information Systems: Artiﬁcial, augmented, and
virtual realities

INTRODUCTION
First person experiences of the real world represent a standard
to which all mediated experiences are compared, either mind-
fully or otherwise [42]. Bodily immersion in VR is rooted in
the way the body is able to redirect a perception of itself as
an object into virtual space (e.g. proprioception) and through

this mirror image, the familiar body is also made the embod-
ied subject during interaction [38]. Presence in VR is based
on the perception of input through visual, auditory, and kines-
thetic senses. For enhancing presence, we need to incorporate
the participant as a part of VR such that, head movements
result in motion parallax, locomotion results in translation
in space, proprioceptive cues are incorporated, vestibular re-
sponses are stimulated, feedback is provided through multiple
sensory channels, and the user has agency.

Ever since its conception in the 1960’s, head-mounted vir-
tual reality systems have been primarily concerned with the
user’s visual senses [47] followed by spatial audio [1]. Since
two major problems faced by virtual environment designers
have been lack of haptic feedback and constraints imposed
on physical movement [28], we attempt to address them in
our prototype system. Additionally, despite new HMD char-
acteristics like low latency head tracking, wide ﬁeld of view
and custom input/output devices, current VR experiences lack
proprioceptive cues related to locomotion which could limit
a user’s sense of presence in the virtual environment. In most
existing VR systems, a seated user is presented with two con-
tradictory motion cues; the visual stimulus signaling move-
ment to the brain, while the vestibular system indicating a
lack of movement creating a disconnect that can be a bit jar-
ring. These conﬂicting inputs can also produce motion sick-
ness and postural instability [26] resulting in, possibly believ-
able, but sub-optimal immersive experiences. For exploring
VR environments, walking in place is also not considered suf-
ﬁcient as it lacks the proprioceptive cues of actual walking
[54]. We propose a social virtual reality system that incorpo-
rates body awareness, physical presence, spatial understand-
ing, sense of orientation, object manipulation, communica-
tion, and multi-sensory feedback to overcome some of the
limitations of existing systems. Along with other researchers,
we also believe including these features can greatly enhance
the user’s sense of presence and immersion in VR [5].

In this paper we present an immersive and interactive social
VR system with possible applications in domains such as en-

 
 
 
 
 
 
tertainment, therapy, travel, real estate, education, social in-
teraction and professional assistance. We use low-cost Kinect
(RGB-D) devices to track multiple users and objects (posi-
tions and orientations) in real-time and use head-mounted dis-
plays (Oculus Rift DK2) for tracking head rotations and pro-
viding visual output. The main research contributions of our
work are the following:

– We build our virtual environment by 3D scanning the phys-
ical environment. This 1:1 mapping allows users to walk
freely in space without fear of colliding with walls or fur-
niture. Their focus stays on the virtual world and maintains
their sense of believability and immersion. This can lead to
a smoother and uninterrupted experience which can in turn
bring forth an enhanced sense of enjoyment.

– The scanned physical environment is textured to make it
visually different from reality. This helps create a sense
of being somewhere else and allows us to use the same
physical space to create many different virtual experiences.

– Given that one of the key features of a VR system is in-
teraction, gaining an understanding of the speciﬁc ways in
which they facilitate this interaction is important. We use
full-body avatars that are directly controlled by the move-
ments of the user in the real world for enhancing engage-
ment in our social VR system.

– We use skeletal tracking that allows Kinect to recognize
people and follow their actions. This, in turn, enables users
to walk naturally in the real world, providing all proprio-
ceptive cues related to locomotion in the virtual world, and
thus positively impacts the user’s sense of presence in VR.
Additionally it also provides a more intuitive and body-
centric way of interacting with objects in VR.

– We use the Kinect to track objects in the real world that
have virtual counterparts. This provides users haptic feed-
back which has been shown to both enhance immersion
and also make virtual tasks easier to accomplish in VR.

– As it is a multi-person system, users can not only see each
other, but also observe the consequences of each other’s
actions reﬂected through the actions of their avatars and
their interactions with virtual objects. For instance, if a
user grabs a virtual cube (that has a cardboard box physical
counterpart) and puts it on top of another virtual cube, each
user will perceive the transformation (i.e., orientation and
position changes) of the cubes from their avatar’s point of
view.

– Our system is entirely based on low cost hardware and

freely available software platforms and tools.

A major challenge for building a social VR world is perfor-
mance in real-time.
In our system, most of the computa-
tional load is concentrated in real-time object and user track-
ing followed by network transmission of the data and ren-
dering of the visual output for each user. It was, therefore,
crucial to make the system computationally tractable for ob-
ject and user tracking, fast rendering, and data transportation.
We achieved this through various strategies like optimizing

the computer vision pipeline for object tracking on the server,
using a connectionless transmission model for data commu-
nication, caching, and optimizing model geometry, lighting,
textures and shaders at the client end.

RELATED WORK
The visual medium evolved from early cave art to the realis-
tic paintings of the classical era to photographs and videos.
Artists have always been inspired by the real world and view-
ers have traditionally experienced a work of art from the
artist’s point of view. In 1968, Sutherland introduced the no-
tion of the viewer being able to choose their point of view,
an idea that forms the basis of VR as we know it today [46].
Virtualized Reality [24] was an realization of the user choos-
ing a viewing angle while looking at a digital scene through a
stereo-viewing system and it was implemented by using tech-
niques from computer vision and computer graphics. While
VR environments are typically constructed using CAD mod-
els, Virtualized Reality starts with the real world and virtual-
izes it. Another related concept is Substitutional Reality that
involves a modiﬁcation of normal perceived reality.
In an
experiment users viewed a live video feed interspersed with
previously recorded video through an HMD and were unable
to discriminate between the two [48]. As perception, pres-
ence, and immersion are all intertwined in VR, our goal is to
create experiences that are perceptually believable, impacting
the user’s sense of presence through an immersive, though
not necessarily realistic, system. In the remainder of this sec-
tion, we describe other research themes that are incorporated
in our prototype design.

Passive Haptics
The majority of research on interaction techniques for VR
has focused on gesture and voice input as prime ways of in-
teracting in VR. These interfaces build on users’ pre-existing
knowledge of the everyday life and experiences [23].
Ide-
ally, we would be able to interact with VR just like we in-
teract with the real world.
In addition to input techniques,
researchers have also pursued a few different approaches for
providing haptic feedback e.g. motion platforms [43], ex-
oskeletons [12], and vibrotactile gloves [44]. While these ap-
proaches have been successful at giving users the experience
of walking, providing some limited haptic feedback, they are
not well suited for recreating the experience of grasping ob-
jects or touching a wall [7]. Opportunistic Controls are a class
of user interaction techniques for augmented reality (AR) that
leverage physical object affordances to provide passive hap-
tics to ease gesture input, simplify gesture recognition, and
provide tangible feedback to the user [14]. In AR, the user
can see the real world objects which they are using to con-
trol or interact with digital elements. However, that is not the
case in VR and we think this brings forth new opportunities
for transforming mundane physical objects into magical vir-
tual items for creating immersive VR experiences.

An early example of the use of a real object associated to a
similar but not identical virtual object was in a desktop VR
environment, where a doll head model was used to control a
brain visualization [15]. Passive haptics have been shown to
both enhance immersion in VR and also make virtual tasks

easier to accomplish by providing haptic feedback. For ex-
ample, Hoffman found adding representations of real objects,
that can be touched, to immersive virtual environments en-
hanced the feeling of presence in those environments [16].
Lindeman found that physical constraints provided by a real
object could signiﬁcantly improve performance in an immer-
sive virtual manipulation task [30]. For example, the presence
of a real tablet and a pen enabled users to easily enter virtual
handwritten commands and annotations [36].

In another study, participants had to place a book on a chair at
the opposite end of a room by walking across a ledge over a
deep pit. The researchers added a real wooden plank for users
to walk on, that corresponded with a virtual ledge, and com-
pared the differences in response when the ledge was only vir-
tual. Since participants could feel a height difference between
the wooden plank and the ﬂoor below, it enhanced the illusion
of standing on the edge of a pit. Results showed signiﬁcant
difference in behavioral presence, heart rate, and skin con-
ductivity changes in the wooden plank group of users [17].
Passive haptics have also been used in VR therapy. In one
study, a physical spider toy replica was present in the virtual
environment so that when the patient’s virtual hand reached
to touch the virtual spider, she could feel the furry texture of
the toy spider [6].

Body, Environment and Social Awareness
Body awareness refers to the familiarity and understanding
that people have of their own bodies, for example, awareness
of the relative position their body parts (proprioception) and
a sense of their range of motion [23]. The brain integrates
information from proprioception and from the vestibular sys-
tem into its overall sense of body position, movement, and
acceleration [52]. Based on these innate abilities, research
in VR interfaces has explored a variety of input techniques
though much of VR interaction is dominated by hand-based
input systems. Our prototype goes beyond hand-based input
by allowing users to experience and interact in VR using their
full body, similar to how they experience the real world. By
representing users’ bodies in the virtual world, our system al-
lows users to perform tasks relative to the body (egocentric),
enhancing their sense of presence.

In the real world, people have a physical presence in their
spatial environment. We are surrounded by objects and land-
scape that facilitate our sense of orientation and spatial under-
standing [23]. For example, the horizon gives us a sense of
directional information, occlusions give us relative distance
cues while atmospheric color, fog, lighting, and shadow pro-
vide depth cues [3]. People also develop skills to manipu-
late objects in their environment, such as picking up, posi-
tioning, altering, and arranging objects [23]. Our VR system
uses these skills along with people’s innate sense of motion
and proprioception for interacting naturally with the virtual
world, grasping and manipulating objects in both the real and
virtual environments.

As humans, we are generally aware of the presence of oth-
ers and since childhood are taught skills for social interac-
tion. These include verbal and non-verbal communication,
the ability to exchange physical objects, and the ability to

work with others to collaborate on a task [23]. Our system
uses social awareness and skills by representing users’s pres-
ence as full-body avatars and by making the avatars’ actions
visible. Additionally the environment allows for multiple co-
located users to interact with each other both in the real and
virtual worlds and collaborate on tasks.

Perception, Immersion, and the VR Experience
Husserl says experience is characterized by intentionality and
Heidegger calls our most basic experiential relation to the
world is a practical one. According to Merleau-Ponty, ex-
perience is essentially corporeal. They all agree that a human
is not a “passive” observer responding to stimuli, but an ac-
tive and creative force shaping the stimuli of their experience
[27]. Gibson adds that reality of experience is grounded in
action and the construct of affordance presents a new funda-
mental basis for reality in the relationship between actor and
environment [38]. These ideas lead to the notion of the body
being the medium for the perception of space [31] and in a
perfect VR experience, we would not be able to tell the dif-
ference between reality and the virtual world. Immersion is
“the dynamic interplay between visual, acoustic, and tactile
feedback and the actions of looking around and manipulating
objects” [11]. The degree of immersion is an objective prop-
erty of the system and therefore measurable and quantiﬁable
[39] while presence is the user’s response to an immersive
environment. In our VR system, the avatar mediates the rela-
tionship between the user and the system. It is an extension
and even a relocation of the user’s body into virtual space, a
vehicle through which the user is given embodied agency and
presence in the virtual environment [38].

Navigation in VR
Two primary approaches to the problem of mapping small
physical spaces to large virtual spaces for navigation and ma-
neuvering have been explored in research:

1. Locomotion: walking in place (natural [50], powered shoes
or other devices [20, 21, 19]), walking in space (natural
[51], mechanical setups [12], redirection techniques [54,
41]), gestural walking [34]

2. Abstractions or metaphors [12]: miniaturized worlds, ﬂy-
ing, driving, bicycling, teleporting, and virtual arm grow-
ing [35].

We build upon the idea of natural locomotion in space al-
lowing users to explore the virtual world by walking in the
physical world. Not only do we track the user’s position, we
also track their full body as represented by 25 joints. This al-
lows us to transfer the user’s bodily motions to the onscreen
avatar where a step taken forward in the real world is visible
as an animation of a step taken forward by their avatar in the
virtual world. A common limitation of a room-scale walk-
ing system is the difference in size of the physical and virtual
worlds. A few mechanisms to overcome that difference have
been explored. For example, a system that can imperceptibly
rotate the virtual scene about the user [28, 37], and a system
that can apply redirection to steer users away from physical
boundaries as they explore virtual environments [45]. We are
currently working on our own solution to allow exploration

Figure 2. Depiction of input and output pathways in our VR system.
Users get visual feedback from the virtual world, audio feedback from
both the real and virtual world, haptic feedback from the real world.
Users use movement and gestures as input techniques and use voice to
communicate.

of large virtual spaces while walking in cluttered physical en-
vironments though that is not the focus of this paper.

RGB-D Sensing
Though RGB-D sensing devices have been custom-built for
years, it is the computer gaming and home entertainment ap-
plications that have made them available for research out-
side specialized computer vision groups. The quality of the
depth sensing, given the low-cost and real-time nature of de-
vices like the Kinect, is compelling, and has made the sensor
popular with researchers and enthusiasts alike. Available 3D
reconstruction systems like KinectFusion [22] enable a user
holding and moving a standard Kinect camera to rapidly cre-
ate detailed 3D reconstructions of an indoor scene. Recon-
structing geometry using active sensors [29], passive cameras
[13, 32], online images [9], or from unordered 3D points [25]
are well-studied areas of research in computer graphics and
vision. There is also extensive literature within the AR and
robotics community on Simultaneous Localization and Map-
ping (SLAM), aimed at tracking a user or robot while creating
a map of the surrounding physical environment [49]. Given
this broad topic, and our need for building a VR environment
that maps 1:1 to the physical environment, we used an exist-
ing reconstruction algorithm for 3D scanning.

METASPACE II SYSTEM DESIGN
We created three different VR scenes experienced using two
different space and device conﬁgurations (see Figure 9). The
ﬁrst two involved scanning and mapping two different hall-
ways to create scenes that matched each of those two phys-
ical spaces (see Figure 3a and 3b).
In the third scene, we
mapped an open lab space to a ﬂoating island virtual world
(see Figure 3c and 3d). In the hallway/bridge scenarios, users
were tasked with walking on the bridge toward each other
and meeting midway for a high ﬁve. In the ﬂoating island
scenario, users were tasked with grabbing a virtual cube and

Figure 3. We created three different VR scenes. a) A hallway repre-
sented as a virtual bridge with planters and pillars shown as barrels and
a stone sculpture (see Figure 4). b) A different hallway also represented
as a bridge with the drop off corresponding to a physical wall and the
crates corresponding to storage cabinets in the real world. The view is
seen from user B’s perspective with user A waving at them. User A is
shown in Figure 5 walking towards user B. c) Open lab space with two
users in the VR scene shown in Figure 3d. Inset shows the view through
the left users’ HMD with the user on the right waving to them. d) Float-
ing island scene that users in Figure 3d are immersed in.

placing it on top of another virtual cube in order to unlock the
portal by manipulating their physical box counterparts (see
Figure 3d).

For each scene, we did a 3D scan of the real world space
and used the resulting 3D model as a base on which to build
the virtual world. Scanning gave us the correct relative sizes
and positions of objects which is necessary to allow users to
walk without colliding with walls or furniture as well as for
interacting with physical objects. We could have achieved
the same goal by measuring the physical space, furniture, and
objects and manually creating a VR world in 3D modeling
software. However, we envisioned an end to end system that
comprised a virtual world creation mechanic with an output
VR scene that users could experience, which made us choose
the former method of building our VR world.

In the VR world, users are represented with full-body avatars
and are tracked in real time such that they can walk around in
the real world for exploring the virtual world. Users can in-
teract with each other as well as with tracked physical objects
and all their interactions are mirrored in the virtual world.
Flow of data between two people using our system is de-
picted in Figure 2. We use a pair of Oculus Rift DK2 head-
mounted displays (HMDs) to provide each user with an ego-
centric view of the virtual world, and Kinect (RGB-D) de-
vices to capture user movements and to track the positions
and orientations of objects. We chose the Kinect camera as
it is widely accessible and low cost compared to virtual real-
ity lab setups that usually have expensive OptiTrack or Vicon
tracking systems.

Pipeline
A depth sensing device (Google Tango tablet) was used to
scan the hallways and lab space (see Figure 4) using an ex-
isting 3D scanning application from the Google Play store.
In our lab space, one wall is entirely made of glass. Because
sensing glass with IR does not work, we manually added a
wooden fence to our scene at the location of the real world

Figure 4. The entire process from scanning to exploring the VR world.
a) 3D scanning of the real world space using the Google Tango tablet. b)
A mesh is generated from the scanned point cloud data. c) The gener-
ated mesh is used as a base for creating the VR scene in Unity3D where
virtual objects (barrels, sculpture, bridge) corresponds to the real world
(planters, pillar, hallway). d) A user about to start walking down the
hallway/bridge while wearing an Oculus Rift DK2.

glass wall. The initial scan gave us a mesh with a very large
number of vertices and triangles and some holes. We im-
ported the mesh into MeshLab to reduce triangle count and
output a watertight mesh. The output from MeshLab was im-
ported into Blender, where we prepared the models for im-
port into Unity3D for building the VR scenes. Unity3D is
a cross platform game engine with support for networking,
cloud-based rendering, texture mapping and lighting to name
a few.

In skeletal
We use Kinect devices to track user skeletons.
tracking, a human body is represented by a number of joints
(25 for Kinect V2) that represent body parts such as head,
neck, shoulders, and arms. Each joint is represented by its 3D
coordinates and mapped to the joints in a rigged 3D model.
Rigging is the process of creating bones and joints for the
avatars and allows the animation system to pose and manip-
ulate them as needed. Joint data from the Kinect is used to
update the movements of the rigged models in Unity3D after
mapping joing data for each skeleton to bone data for each
avatar. This is important as we want to animate the charac-
ter’s movements using the real world movements of the users.
We use RGB data from the Kinects and the OpenCV library
to track position and orientation of two boxes. Each physical
box is represented as a virtual cube in our scene and appears at
at corresponding location in VR to it’s physical location. The
positions and orientations of the virtual cubes match those of
the physical boxes.

Hardware and Software
Our server is implemented using C++ and OpenCV. Our
clients are implemented using Unity and the Oculus Rift plu-
gin. The server is an Intel Core i5-4200 2.3GHz Windows
laptop with 4GB RAM. One client is an Intel Core i7-3520
2.9GHz Windows laptop with 8GB RAM. The second client
is a dual boot Intel Core i5-4258U 2.6GHz OSX laptop with
16GB RAM.

KEY COMPONENTS
Our goal is to leverage the physicality of the real environ-
ment and use our ﬁrst person experiences with the real world

Figure 5. User A walking down the hallway (real) and bridge (virtual)
towards User B (see Figure 3b). Both users’ skeletons are tracked by
the Kinect and their physical movements are mapped to their avatar’s
movements.

for enhancing the sense of presence in virtual ﬁrst person
experiences. We had about 40 users try out our system in
April 2015 and another 50-60 try a different version in Oc-
tober/November 2015. Users walked, waved, high ﬁve’d,
grabbed a box and played with it, and stacked boxes. We re-
ceived positive feedback about the full-body avatar and the
object interaction, as it appeared users were most excited
about those elements. Though these were not formal user
studies, we are very excited about conducting one to under-
stand how a user’s sense of presence in VR is affected by the
elements included in our system.

Body Tracking
In order for each user to have agency in the VR environment,
we track them in real-time using Kinect devices and it’s skele-
ton tracking system. Each joint is represented by its 3D coor-
dinates and mapped to the joints of a rigged 3D model. Thus
each user dynamically controls an avatar through their body
movements. This allows users to walk around in physical
space, for example, a hallway, while walking on a bridge in
virtual space (see Figures 3 and 5).

Real-walking in virtual environments is more natural and pro-
duces a higher sense of presence than other locomotion tech-
niques [50, 40]. However, this is typically restricted in size
to the area of tracked space. Reorientation techniques have
traditionally been utilized to enable free exploration of in-
ﬁnitely large VR spaces without the use of joysticks, walking-
in-place techniques, omni-directional treadmills or bicylces
[4, 8, 18]. Though our current system provides a room-scale
VR experience, we are working on the next version where we
remove that limitation by employing an inside-out position
tracking system paired with motion capture systems for full
body tracking to allow us to expand our room-scale VR to an
entire building ﬂoor and beyond.

Object Tracking
Our approach for interacting with physical objects builds on
the concept of passive haptics [16], i.e. receiving feedback
from touching a physical object that is registered to a vir-
tual object. The physical properties of the tangible interface
can be used to suggest ways in which the attached virtual ob-
jects may be used, in essence adding affordances to virtual
items. Object tracking is an component in a wide range of

foam as they would completely alter the user’s expectations
of how it would feel and create a mismatch with what it ac-
tually feels like when they pick up the real box/virtual cube
(see Figure 6). Additionally, there needs to be an approximate
size match or the user may not be able to pick up the real box
without some trial and error, due to proprioception. Objects
that present similar affordances [10] in the parts most likely
to be interacted with, are the best candidates for virtualizing
and using as passive haptics.

Feedback
The Reality-Virtuality continuum [53] deﬁnes a range of pos-
sible “realities” affecting our senses in different ways. At one
end is the Real Environment and at the other end is the Virtual
Environment, encompassing all possible variations and com-
positions of real and virtual objects. In between is the notion
of Mixed Reality (MR), encompassing Augmented Reality
(AR) and Augmented Virtuality [33]. These types of systems
are usually characterized by adding or subtracting elements
of either side of the continuum to the user experience. Our
system spans the continuum as the user interacts with real
world objects and walks in the real world while interacting
with a fully virtual environment. To address the difﬁculty in
rendering convincing physical and haptic experiences in VR,
we explore the concept of passive haptics.

VR systems and devices are now able to create visually and
aurally convincing experiences. Current technology however,
is still not able to stimulate our other senses with the same res-
olution. In particular, even though some technologies provide
the experience of tactile feedback (e.g., haptic devices), force
feedback (e.g. exoskeletons) and locomotion (e.g., omni-
directional treadmills), these devices are not able to mimic
how we experience the real world. For example, haptic de-
vices predominantly use vibrotactile feedback [12] which is
just one type of tactile feedback that we may encounter in
the real world. Touch also conveys information such as con-
tact surface geometry, roughness, slippage, and temperature.
Through passive haptics, our system is able to convey realistic
haptic feedback to the user (see Figure 6). Additionally, we
receive force feedback [2] in the real world which provides
information on object surface compliance, weight, and iner-
tia. While treadmills allow users to navigate endlessly large
spaces, they are unable to render physical obstacles or other
terrain properties, such as ridges and uneven elevations.

Networking
For realtime performance, we adopt a client-server, dis-
tributed architecture to address computational complexity in-
volved in object tracking. For a synchronous multiuser vir-
tual experience, it is essential that all persons receive the
same exact system state at all times. In Figure 7a, we show
our client-server architecture that is made of four main en-
tities: cloud server providing middleware and service (Pho-
ton Server), laptops running local clients (Unity3D), Kinect
RGB-D tracking devices, and HMDs (Oculus Rift DK2s).
The Kinect devices track user movement and that data is sent
to each client through the cloud server such that each client
is synced and displays the same system state to each user. In
Figure 7b, we show a second client-server architecture that is

Figure 6. A user grabs a box in the real world by reaching out and
picking up the cube in the VR world. Their task is to stack the two
boxes/cubes to unlock the portal.

applications in computer vision, such as trafﬁc monitoring,
video indexing, and self driving vehicles. Given the initial-
ized state (e.g., position and size) of our boxes in an RGB
frame from the Kinect, the goal of tracking was to estimate
the states of the boxes in the subsequent frames and apply-
ing the learned transformations to their virtual counterparts
in our VR scene. Although object tracking has been studied
for several decades, and much progress has been made [55], it
remains a very challenging problem, especially for real-time
application scenarios. We chose a marker based instead of a
natural feature-based 3D object tracking system due to speed
of implementation and higher frame rates as determined em-
pirically for our scenario. Numerous factors affect the perfor-
mance of a tracking algorithm, such as illumination variation,
occlusion, as well as background clutter, and there exists no
single tracking approach that can successfully handle all sce-
narios.

Texturing
Previous research has focused on virtual objects being mod-
eled on the physical proxy [16]. However, our goal is not
the reconstruction of a realistic virtual replica of the physi-
cal space or objects. We want to use the physical space as
a template for placement of walls and furniture in the virtual
world such that a correspondence between the two spaces ex-
ists and are users are able to walk freely in the real world
while wearing HMDs. We do want to change the visual ap-
pearance of virtual objects by texturing the 3D models differ-
ently from their real world textures. For example, the walls
in our scene corresponding to the physical lab space look like
wooden fences (see Figure 3d). They can be made to look
like stone walls of a cave or those of a space station. Simi-
larly, furniture can change appearance and chairs and tables
can look like they belong to a Victorian living room.

We aim to match the virtual representation closely with the
physical characteristics of the actual objects in order to main-
tain sensory coherence. For example, an empty cardboard
box feels light, non-slippery, a little rough, and has a distinct
sound when touched or moved, and so on. Our virtual cube
therefore needs to match some of these tactile characteristics
to maintain coherence in sensory feedback. The texture of
the virtual box should be such that it appears light enough
to be picked up and the material on it is not metal or soft

Figure 7. A user checks out their hands and body in the VR scene before
walking up to interact with the box/cube.

Figure 8. The instant boxes/cubes are stacked, the spiral vortex blocking
the portal disappears and a T-Rex rushes out towards the adventurers.

made of three main entities: user and object tracking server
(C++), laptops running local clients (Unity3D), and HMDs
(Oculus Rift DK2s). Data is read in from the Kinect and
through the cloud server sent to all clients. Joint orientations
are calculated from the joint position data received from the
server and applied to the avatars. The virtual cubes receive
the tracked object pose which transforms their position and
orientation on the client.

USER EXPERIENCE
This section is about the user experience in the VR scene
that corresponds to the physical setup shown in Figure 9b.
The two adventurers try to make sense of their surroundings:
their airship, seemingly grounded beyond the shimmering air.
Where are they? The moon rises massively in the sky as if
only a short distance away. Floating above a wooden fence to
the left, its fullness is obscured by clouds so close they could
reach out and touch them. They realize, no, their airship is
not grounded, they have been stranded on some ﬂoating is-
land. Floating over what, they cannot tell. Beyond the edges
of this land mass they would fall into an inﬁnity of space. A
swirling energy ﬁeld appears before them. The one point of
access to something outside of this island is their ship. They
determine the key to their escape lies within this portal. But
how can they open this portal to allow them their freedom.
They notice some relationship with portal and the two pulsat-
ing cubes resting on pedestals before them. Somehow they
need to interact with these cubes to be able to gain access to
what secrets or escape the portal can give. Will it allow them
to board their ship or will it take them somewhere entirely
new. And so their adventure begins. Little do they know that
opening the portal will let loose a T-Rex (see Figure 8).

Experiencing MS2 consists of three parts. First is receiving
visual input through the Oculus Rift Dk2 head mounted dis-

Figure 9. MetaSpace system conﬁgurations. Each user wears an Ocu-
lus Rift DK2 HMD and can see a ﬁrst person view of the virtual world
and see other user’s who are also present in the same virtual world with
them.

play devices, one for each person. Second is being able to
fully control a virtual avatar through bodily movements in
the real world, and third is interacting with objects in the VR
scene and receiving haptic feedback. We asked both users
to start by looking around the VR scene and looking at their
hands, feet, and body (see Figure 7) followed by acknowl-
edging each others presence in the virtual world. Each user
was then asked to walk up to the virtual cube on the pillar
a few feet in front of them, to pick it up and put it on top
of the other cube. Communication between the users and
the virtual world happens through physical movement tracked
using the Kinect depth sensor. Interaction with objects hap-
pens through physically grasping and moving them as they
are tracking using the Kinect RGB sensor. These mecha-
nisms help create a natural immersive multiuser experience
where interacting with virtual objects is done through inter-
acting with their physical counterparts.

Digital games often present players with tasks where they
need to use objects such as swords, guns, lanterns etc. These
objects are unlikely to be found in users’ living rooms but
that does not need to be a limitation impacting the types of
experiences that can be designed in our VR system. In sev-
eral instances, we believe the role for passive haptics would
almost be like that of toys used in children’s imaginative play
where a ﬂashlight becomes a light saber or a pencil turns into
a magic wand. Because of this, we think an exact match be-
tween real and virtual counterparts would not be required to
successfully provide haptic feedback. Similarly shaped and
sized objects easily found around the house could fulﬁll the
role successfully and provide the necessary feedback to cre-
ate a believable VR experience. Every element in the VR
scene does not need a physical counterpart either. The di-

nosaur does not have any physical counterpart, but it’s fantas-
tical elements mixed with real objects and environments that
are part of the lure of VR, of the escape into new worlds.

CONCLUSION
We have presented the design and realization of a social and
immersive virtual reality system and described the process of
how we created our virtual worlds. By employing RGB-D
devices, head-mounted displays, and a client-server architec-
ture, multiple users simultaneously inhabit the virtual world
and interact with each other as well as with the real world
through VR. We discussed the implementation details and
optimization strategies of each component and their role in
creating an overall VR experience. Our system runs in real-
time and is indeed a proof of concept for practical applica-
tions based on social VR systems with haptic feedback and
full-body tracking. Since our system has already been tried
by over a 100 people, our next plan is to conduct a formal
evaluation of users’ experiences for a variety of tasks involv-
ing interaction with people and with objects.

REFERENCES
1. Begault, D. R., et al. 3-D sound for virtual reality and

multimedia, vol. 955. Citeseer, 1994.

2. Bouzit, M., Burdea, G., Popescu, G., and Boian, R. The
rutgers master ii-new design force-feedback glove.
Mechatronics, IEEE/ASME Transactions on 7, 2 (2002),
256–263.

3. Bowman, D. A., Kruijff, E., LaViola Jr, J. J., and

Poupyrev, I. 3D user interfaces: theory and practice.
Addison-Wesley, 2004.

4. Brooks Jr, F. P. Walkthrough?a dynamic graphics system
for simulating virtual buildings. In In Proc. Workshop on
Interactive 3D graphics, ACM (1987), 9–21.

5. Brooks Jr, F. P. What’s real about virtual reality?

Computer Graphics and Applications, IEEE 19, 6
(1999), 16–27.

6. Carlin, A. S., Hoffman, H. G., and Weghorst, S. Virtual
reality and tactile augmentation in the treatment of
spider phobia: a case report. Behaviour research and
therapy 35, 2 (1997), 153–158.

7. Cheng, L.-P., Roumen, T., Rantzsch, H., K¨ohler, S.,
Schmidt, P., Kovacs, R., Jasper, J., Kemper, J., and
Baudisch, P. Turkdeck: Physical virtual reality based on
people. In In Proc. ACM UIST, ACM (2015), 417–426.

8. Darken, R. P., Cockayne, W. R., and Carmein, D. The
omni-directional treadmill: a locomotion device for
virtual worlds. In In Proc. ACM UIST, ACM (1997),
213–221.

9. Frahm, J.-M., Fite-Georgel, P., Gallup, D., Johnson, T.,
Raguram, R., Wu, C., Jen, Y.-H., Dunn, E., Clipp, B.,
Lazebnik, S., et al. Building rome on a cloudless day. In
Computer Vision–ECCV 2010. Springer, 2010, 368–381.

10. Gibson, J. J. The theory of affordances. Hilldale, USA

(1977).

11. Gibson, J. J. The Ecological Approach to Visual

Perception: Classic Edition. Psychology Press, 2014.

12. Hale, K. S., and Stanney, K. M. Handbook of virtual

environments: Design, implementation, and
applications. CRC Press, 2014.

13. Hartley, R., and Zisserman, A. Multiple view geometry
in computer vision. Cambridge university press, 2003.

14. Henderson, S. J., and Feiner, S. Opportunistic controls:

leveraging natural affordances as tangible user interfaces
for augmented reality. In In Proc. Virtual reality
software and technology, ACM (2008), 211–218.

15. Hinckley, K., Pausch, R., Goble, J. C., and Kassell, N. F.
Passive real-world interface props for neurosurgical
visualization. In In Proc. SIGCHI conference on Human
factors in computing systems, ACM (1994), 452–458.

16. Hoffman, H. G. Physically touching virtual objects
using tactile augmentation enhances the realism of
virtual environments. In In Proc. Virtual Reality Annual
International Symposium, IEEE, IEEE (1998), 59–63.

17. Insko, B. E. Passive haptics signiﬁcantly enhances

virtual environments. PhD thesis, University of North
Carolina at Chapel Hill, 2001.

18. Iwata, H. Walking about virtual environments on an
inﬁnite ﬂoor. In In Proc. Virtual Reality, IEEE, IEEE
(1999), 286–293.

19. Iwata, H., Yano, H., Fukushima, H., and Noma, H.

Circulaﬂoor [locomotion interface]. Computer Graphics
and Applications, IEEE 25, 1 (2005), 64–67.

20. Iwata, H., Yano, H., and Tomioka, H. Powered shoes. In
ACM SIGGRAPH 2006 Emerging technologies, ACM
(2006), 28.

21. Iwata, H., Yano, H., and Tomiyoshi, M. String walker. In
ACM SIGGRAPH 2007 Emerging Technologies, ACM
(2007), 20.

22. Izadi, S., Kim, D., Hilliges, O., Molyneaux, D.,

Newcombe, R., Kohli, P., Shotton, J., Hodges, S.,
Freeman, D., Davison, A., et al. Kinectfusion: real-time
3d reconstruction and interaction using a moving depth
camera. In In Proc. ACM UIST, ACM (2011), 559–568.

23. Jacob, R. J., Girouard, A., Hirshﬁeld, L. M., Horn,
M. S., Shaer, O., Solovey, E. T., and Zigelbaum, J.
Reality-based interaction: a framework for post-wimp
interfaces. In In Proc. SIGCHI conference on Human
factors in computing systems, ACM (2008), 201–210.

24. Kanade, T., Narayanan, P., and Rander, P. W. Virtualized
reality: Concepts and early results. In Representation of
Visual Scenes, 1995.(In Conjuction with ICCV’95),
Proceedings IEEE Workshop on, IEEE (1995), 69–76.

25. Kazhdan, M., Bolitho, M., and Hoppe, H. Poisson
surface reconstruction. In In Proc. Eurographics
symposium on Geometry processing, vol. 7 (2006).

26. Kennedy, R. S., and Stanney, K. M. Postural instability
induced by virtual reality exposure: Development of a
certiﬁcation protocol. International Journal of
Human-Computer Interaction 8, 1 (1996), 25–47.

40. Slater, M., Usoh, M., and Steed, A. Taking steps: the

inﬂuence of a walking technique on presence in virtual
reality. ACM Transactions on Computer-Human
Interaction (TOCHI) 2, 3 (1995), 201–219.

27. Klevjer, R. Enter the avatar: The phenomenology of
prosthetic telepresence in computer games. In The
Philosophy of Computer Games. Springer, 2012, 17–38.

28. Kohli, L., Burns, E., Miller, D., and Fuchs, H.

Combining passive haptics with redirected walking. In
In Proc. international conference on Augmented
tele-existence, ACM (2005), 253–254.

29. Levoy, M., Pulli, K., Curless, B., Rusinkiewicz, S.,

Koller, D., Pereira, L., Ginzton, M., Anderson, S., Davis,
J., Ginsberg, J., et al. The digital michelangelo project:
3d scanning of large statues. In In Proc. Computer
graphics and interactive techniques, ACM
Press/Addison-Wesley Publishing Co. (2000), 131–144.

30. Lindeman, R. W., Sibert, J. L., and Hahn, J. K. Towards
usable vr: an empirical study of user interfaces for
immersive virtual environments. In In Proc. SIGCHI
conference on Human Factors in Computing Systems,
ACM (1999), 64–71.

31. Merleau-Ponty, M., and Smith, C. Phenomenology of
perception. Motilal Banarsidass Publishe, 1996.

32. Merrell, P., Akbarzadeh, A., Wang, L., Mordohai, P.,
Frahm, J.-M., Yang, R., Nist´er, D., and Pollefeys, M.
Real-time visibility-based fusion of depth maps. In
Computer Vision, 2007. ICCV 2007. IEEE 11th
International Conference on, IEEE (2007), 1–8.

33. Milgram, P., and Kishino, F. A taxonomy of mixed
reality visual displays. IEICE Transactions on
Information and Systems 77, 12 (1994), 1321–1329.

34. Nilsson, N. C., Seraﬁn, S., and Nordahl, R. The

perceived naturalness of virtual locomotion methods
devoid of explicit leg movements. In In Proc. Motion on
Games, ACM (2013), 155–164.

35. Poupyrev, I., Billinghurst, M., Weghorst, S., and

Ichikawa, T. The go-go interaction technique: non-linear
mapping for direct manipulation in vr. In In Proc. ACM
UIST, ACM (1996), 79–80.

36. Poupyrev, I., Tomokazu, N., and Weghorst, S. Virtual
notepad: handwriting in immersive vr. In In Proc.
Virtual Reality Annual International Symposium, IEEE,
IEEE (1998), 126–132.

37. Razzaque, S., Kohn, Z., and Whitton, M. C. Redirected
walking. In In Proc. EUROGRAPHICS, vol. 9, Citeseer
(2001), 105–106.

38. Sageng, J. R., Fossheim, H. J., and Larsen, T. M. The
Philosophy of Computer Games, vol. 7. Springer
Science & Business Media, 2012.

39. Slater, M. Measuring presence: A response to the

witmer and singer presence questionnaire. Presence:
Teleoperators and Virtual Environments 8, 5 (1999),
560–565.

41. Steinicke, F., Bruder, G., Kohli, L., Jerald, J., and
Hinrichs, K. Taxonomy and implementation of
redirection techniques for ubiquitous passive haptic
feedback. In Cyberworlds, 2008 International
Conference on, IEEE (2008), 217–223.

42. Steuer, J., Biocca, F., and Levy, M. R. Deﬁning virtual

reality: Dimensions determining telepresence. Comm. in
the age of virtual reality (1995), 33–56.

43. Stewart, D. A platform with six degrees of freedom. In

Proc. Institution of mechanical engineers 180, 1 (1965),
371–386.

44. Sturman, D. J., and Zeltzer, D. A survey of glove-based
input. Computer Graphics and Applications, IEEE 14, 1
(1994), 30–39.

45. Suma, E. A., Azmandian, M., Grechkin, T., Phan, T.,
and Bolas, M. Making small spaces feel large: inﬁnite
walking in virtual reality. In ACM SIGGRAPH 2015
Emerging Technologies, ACM (2015), 16.

46. Sutherland, I. E. The ultimate display. Multimedia:

From Wagner to virtual reality (1965).

47. Sutherland, I. E. A head-mounted three dimensional

display. In In Proc. Fall joint computer conference, part
I, ACM (1968), 757–764.

48. Suzuki, K., Wakisaka, S., and Fujii, N. Substitutional
reality system: a novel experimental platform for
experiencing alternative reality. Scientiﬁc reports 2
(2012).

49. Thrun, S., et al. Robotic mapping: A survey. Exploring
artiﬁcial intelligence in the new millennium (2002),
1–35.

50. Usoh, M., Arthur, K., Whitton, M. C., Bastos, R., Steed,

A., Slater, M., and Brooks Jr, F. P. Walking¿
walking-in-place¿ ﬂying, in virtual environments. In In
Proc. Computer graphics and interactive techniques,
ACM Press/Addison-Wesley Publishing Co. (1999),
359–364.

51. Welch, G., Bishop, G., Vicci, L., Brumback, S., Keller,

K., et al. The hiball tracker: High-performance
wide-area tracking for virtual and augmented
environments. In In Proc. Virtual reality software and
technology, ACM (1999), 1–ff.

52. Wikipedia. Proprioception, 2015. [Online; accessed

27-Nov-2015].

53. Wikipedia. Reality-virtuality continuum, 2015. [Online;

accessed 08-Dec-2015].

54. Williams, B., Narasimham, G., Rump, B., McNamara,
T. P., Carr, T. H., Rieser, J., and Bodenheimer, B.
Exploring large virtual environments with an hmd when

physical space is limited. In In Proc. of the 4th symp on
Applied perception in graphics and visualization, ACM
(2007), 41–48.

55. Yilmaz, A., Javed, O., and Shah, M. Object tracking: A
survey. Acm computing surveys (CSUR) 38, 4 (2006),
13.

