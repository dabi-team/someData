StyleMesh: Style Transfer for Indoor 3D Scene Reconstructions

Lukas H¨ollein1

Justin Johnson2 Matthias Nießner1

1Technical University of Munich

2University of Michigan

Figure 1. We perform style transfer on reconstructed 3D meshes by synthesizing stylized textures. We compute style transfer losses on
views of the scene and backpropagate gradients to the texture. Depth and surface normal data from the mesh enable 3D-aware stylization,
preventing artifacts that arise from standard 2D losses. Our stylized meshes can be rendered using the traditional graphics pipeline.

Abstract

We apply style transfer on mesh reconstructions of in-
door scenes. This enables VR applications like experienc-
ing 3D environments painted in the style of a favorite artist.
Style transfer typically operates on 2D images, making styl-
ization of a mesh challenging. When optimized over a vari-
ety of poses, stylization patterns become stretched out and
inconsistent in size. On the other hand, model-based 3D
style transfer methods exist that allow stylization from a
sparse set of images, but they require a network at infer-
ence time. To this end, we optimize an explicit texture for
the reconstructed mesh of a scene and stylize it jointly from
all available input images. Our depth- and angle-aware op-
timization leverages surface normal and depth data of the
underlying mesh to create a uniform and consistent styliza-
tion for the whole scene. Our experiments show that our
method creates sharp and detailed results for the complete
scene without view-dependent artifacts. Through extensive
ablation studies, we show that the proposed 3D awareness
enables style transfer to be applied to the 3D domain of a
mesh. Our method 1 can be used to render a stylized mesh
in real-time with traditional rendering pipelines.

1. Introduction

Creating 3D content from RGB-D scans is a popular
topic in computer vision [1, 13, 30, 44, 45]. We tackle a

1https://lukashoel.github.io/stylemesh/

novel use case in this area: stylization of a reconstructed
mesh with an explicit RGB texture. Neural Style Trans-
fer (NST) shows great results for stylization of images or
videos, but stylization of 3D content like meshes has been
underexplored. We synthesize a texture for the mesh which
is a combination of observed RGB colors and a painting’s
artistic style. After stylization, one could explore the space
in VR and see it painted in the style of Van Gogh.

Our use case is similar to prior texture mapping meth-
ods [2,17,27,28,30,54,58] which construct a texture from a
set of posed RGB images, but we produce a stylized texture
rather than directly matching input images. This is difficult
since style transfer losses are typically defined on 2D im-
age features [21], so NST does not immediately generalize
to 3D meshes. Recently, style transfer has been combined
with novel view synthesis to stylize arbitrary scenes with a
neural renderer from a sparse set of input images [8,26,36].
These model-based methods require a forward pass during
inference and cannot directly be applied to meshes. Kato et
al. [33] and Mordvintsev et al. [43] use differentiable ren-
dering to bridge the gap between image style transfer and
texture mapping: backpropagating image losses to a texture
representation enables consistent mesh stylization.

However, applying these methods to room-scale geom-
etry is challenging as the resulting stylization patterns are
noisy and can contain view-dependent stretch and size arti-
facts. For example, optimizing a surface from a small graz-
ing angle creates patterns in the image plane for that pose.
Viewing the same surface from an orthogonal angle then

1

shows stretched-out patterns due to the perspective distor-
tion. Similarly, seeing an object from close and far-away
viewpoints mixes small and large patterns on the same sur-
face. Perceiving the depth thus becomes harder, due to in-
consistent stylization sizes. These issues arise because 2D
style transfer losses do not incorporate 3D data like surface
normals and depth. Instead, textures are separately stylized
in each pose’s image plane.

To this end, we formulate an energy minimization prob-
lem over the texture that combines texture mapping with
style transfer (similar to [43]) and minimizes style transfer
losses for each pose in a 3D-aware manner that avoids view-
dependent artifacts. First, we utilize depth to render im-
age patches at increasingly larger screen-space resolutions.
By splitting the style loss calculation over these patches,
we create larger stylization patterns in the foreground than
the background. As a result, patterns have the same size
in world-space and are optimized in a view-independent
way. Second, we use the angle between the surface normal
and view direction to determine the degree of stylization
for each pixel. By calculating Gram matrices from differ-
ent style image resolutions (similar to [40]) areas seen from
small grazing angles are stylized with coarse details, which
are later refined if they are seen from better angles. Third,
we avoid discretization artifacts by scaling gradients with
per-pixel angle and depth weights during backpropagation.
Compared to state-of-the-art 3D style transfer methods,
our experiments show an improvement in terms of 3D con-
sistent stylization both qualitatively and quantitatively. Ad-
ditionally, our explicit texture representation allows for di-
rect usage with traditional rendering pipelines.

To summarize, our contributions are:

• Style transfer for room-scale indoor scene meshes with
a new texture optimization, which results in 3D consis-
tent textures and mitigates view-dependent artifacts.

• A depth-aware optimization at different screen-space
resolutions, that creates equally-sized stylization pat-
terns in the world-space of the mesh.

• An angle-aware optimization at different stylization
details, that creates unstretched stylization patterns in
the world-space of the mesh.

inaccuracies in pose, geometry, color and distortions to find
the best texture for the scene. In contrast, we aim to create
a texture that is also styled to a specific image and avoid
view-dependent stylization artifacts by introducing depth-
and angle-awareness into the optimization.

Image Style Transfer. NST, first introduced in Gatys et
al. [21], can be optimization-based [9, 21, 22] or model-
based [16, 29, 31, 53]. It is inherently defined in the image
domain by matching CNN features either globally or in a
local, patch-based manner [21, 32, 35, 37, 42]. Thus, it can-
not directly utilize 3D data like depth or surface normals of
a mesh. This can lead to view-dependent stylization arti-
facts when optimizing a texture through multiple poses. We
induce 3D-awareness into optimization-based NST by split-
ting the loss calculation across different image segments.

Video Style Transfer. Video style transfer (VST) methods
consistently stylize RGB video frames with a given style.
These methods are optimization-based [47, 48] or model-
based [6, 7, 19, 20, 23, 55, 56] and employ temporal consis-
tency or optical flow constraints. Other methods combine
features in a temporally consistent way, without using opti-
cal flow or depth constraints directly [16,38]. VST methods
can be combined with texture mapping to achieve consistent
stylization of indoor scenes. However, since VST optimiza-
tions are unaware of the underlying 3D structures, the re-
sulting textures are often blurry or low-detail.

3D Style Transfer. Lifting style transfer into 3D has been
explored for texturing individual objects [33, 43, 57] or
faces [24]. However, they focus on isolated objects (not
room-scale scenes) and do not utilize 3D data. In contrast,
our method stylizes complete indoor scenes in a 3D-aware
way. Another line of work applies exemplar-based NST to
3D models [25, 50], guiding the stylization process explic-
itly from (hand-crafted) examples. In contrast, we follow
original NST by stylizing 3D scene models from artistic
paintings and camera images. Cao et al. [4] stylize indoor
scenes using a point cloud that cannot be directly used to
texture a mesh. Other methods combine novel view synthe-
sis and NST for consistent stylization from only a few input
images [8, 26, 36]. In contrast, we do not require a network
during inference to produce stylization results; our results
can be rendered by a standard graphics pipeline.

2. Related Work

3. Method

Our approach is a NST method operating on the texture
parametrization of a mesh. It is related to recent work on
style transfer for videos and 3D objects, as well as texture
generation from RGB-D images.
Texture Mapping. Many methods texture a reconstructed
mesh from multiple RGB images, i.e., they map a texture
onto the geometry that combines the color information of all
images [2,17,27,28,30,54,58]. These methods must handle

Our goal is to stylize the mesh of an indoor scene: we
want to create a texture that is a mixture of original RGB
colors and a style image. To avoid view-dependent arti-
facts, we formulate a depth- and angle-aware optimization
problem over all images. We require a set of N images
{Ik}N
k=1 captured at different poses. We also need a mesh
reconstruction of the scene for which we create a texture
parametrization, i.e., we need a uv coordinate per vertex.

2

Figure 2. We optimize a texture for the mesh reconstruction of a scene using multiple RGB images and a style image. We sample the
texture with uv maps at different resolutions, yielding a render pyramid for each pose. Using depth, we divide the screen space into parts,
each corresponding to a different pyramid level. Each is encoded separately and compared with its RGB image part in the content loss.
Using normals, the style loss is further split into fine and coarse branches. We discard features seen from small grazing angles for fine
stylization. Finally, we scale the resulting image gradients with continuous angle and depth weights before backpropagating to the texture.

For each pose, we sample the texture with the correspond-
ing uv map at multiple resolutions, yielding a render pyra-
mid. Depending on the depth of each pixel, we split the im-
age into multiple render parts, each belonging to one pyra-
mid resolution. Each part is used in content and style losses,
where we only stylize pixels with fine details, that are seen
from good angles. Finally, we smooth the per-pixel gradi-
ents before backpropagating to the texture. The complete
method is visualized in Fig. 2.

3.1. Texture Optimization

We optimize a stylized RGB texture T ∗ from all RGB
images {Ik}N
k=1 and a separate style image Is. Similar
to [43], we formulate a minimization problem with content
and style losses Lc, Ls and add a regularization term Lr:

T ∗ = arg min

T

i=0

N
(cid:88)

(λcLc(Ii, ˆPi)+λsLs(Is, ˆPi)+λrLr(T ))

(1)
where ˆPi is the render pyramid for the current pose, sam-
pled from the texture with the corresponding uv maps and
λc, λs, λr are loss weights. The sampling operation is iden-
tical to traditional graphics and differentiable, i.e., we bi-
linearly interpolate each pixel from four neighboring tex-
els. Similar to Thies et al. [52], we define our texture us-
ing a Laplacian Pyramid to regularize the texels in each
layer with Lr. This helps avoid magnification and minifi-
cation artifacts and reduces visible noise in the texture. For
each pose, we optimize the subset of observed texels. Thus,
we require a pose set covering most of the scene to opti-
mize the texture completely. In contrast, stylizing in texture
space directly [57] is problematic for a room-scale texture
parametrization, which may contain many seams.

3.2. Depth Level Render Parts

Style transfer operates on the CNN features of an im-
age [21]. This leads to a limited sense of depth when op-

3

timizing over multiple poses. Stylization patterns can ap-
pear equally large in the foreground and background, e.g.,
when parts of a surface are seen far-away and close-up
(see Fig. 3). Observing the same surface from multiple
poses thus mixes small and large patterns next to each other.
As a result, renderings using the optimized texture do not
convincingly capture depth. Liu et al. [39] make style trans-
fer depth-aware in the image plane with a depth-loss net-
work. In contrast, we incorporate depth-awareness by opti-
mizing at multiple screen-space resolutions. Larger patterns
appear in the foreground than the background of an image,
ultimately leading to equally large style in world space.

We make use of the relation that area in screen-space is
inversely proportional to depth, i.e., when depth increases
by a factor of p, a given projected area decreases by p2. On
the other hand, style transfer is agnostic to the image resolu-
tion that it is applied on, i.e., when resolution increases by
p2, stylization patterns appear proportionally smaller (be-
cause the receptive field becomes smaller relative to the res-
olution) [22]. We combine both relations to optimize styl-
ization patterns having the same size in world space: when
depth increases by p, we increase image resolution by p2.

We apply the relation to divide the image into parts, sam-
pled from the render pyramid at increasingly larger resolu-
tions. The content and style losses are then calculated inde-
pendently for each part. To discretize into parts, we define a
minimum depth value θd, making the relation absolute. We
calculate the optimal image height per-pixel as

Rxy = θmin ·

dxy
θd

(2)

where dxy is the depth at pixel (x, y) and θmin is the min-
imum resolution. We express resolution Rxy as height in
pixels and scale the width accordingly. We then map Rxy
to the nearest neighbor in the render pyramid, yielding its
index as depth level per-pixel. Finally, we apply a 3×3 ero-
sion kernel to smooth the depth level map over all pixels.

We calculate the normalized weighting factor ˆwl as

wl =

vl
tl

and

ˆwl =

wl
(cid:80)θl
i wi

(4)

where vl is the visible and tl the total number of pixels in
depth level l. Similarly, the content loss is defined as
θl(cid:88)

Lc(I, ˆP ) =

ˆwl · ||F l − ˆF l||2
F

(5)

l

where F l are the features of the content image I, split in a
similar way. For brevity, we omit different VGG layers and
image indices from the notation. As proposed in Gatys et
al. [21], we use the layers relu {1-5} 1 for the style loss and
relu 4 2 for the content loss. We calculate the losses inde-
pendently for every VGG layer and sum them accordingly.

3.5. Per-Pixel Gradient Scaling

Depth levels (Sec. 3.2) and angle filtering (Sec. 3.3) im-
pose hard thresholds on the image of each pose. To avoid
discretization artifacts at decision boundaries, we scale the
per-pixel gradients before backpropagating them to the tex-
ture. First, we calculate a weighting factor wa
xy = cos(αxy)
from the normal-to-view angle αxy. This controls the influ-
ence of a pose on each pixel by preferring orthogonal over
small grazing viewing angles. Scaling features similar to
Gatys et al. [22] instead results in oversaturation artifacts.

Second, we adapt the idea of trilinear Mipmap interpo-
lation [18]. Each pixel contributes to the render parts of its
nearest two pyramid layers, resulting in two per-pixel gra-
dients. We calculate the distance to the nearest layer as

wd

xy = |

Rxy − L1
xy
xy − L2
L1
xy

|

(6)

xy, L2

where Rxy is the optimal resolution for pixel (x, y) and
L1
xy are the resolutions of the nearest and second near-
est pyramid layers. Finally, we linearly interpolate between
the per-pixel gradients as

∂L
∂Ixy

=

∂L1
∂Ixy

· wd

xy · wa

xy +

∂L2
∂Ixy

· (1 − wd

xy) · wa

xy (7)

where L1 is the loss term for the nearest pyramid layer of
pixel (x, y) and L2 for the second nearest, respectively.

3.6. Data Preprocessing

We use the ScanNet [11] and Matterport3D [5] datasets,
which provide RGB-D images and reconstructed meshes
(we use per-region meshes for Matterport3D [5]). We use
the RGB images for optimization, but filter them with a
Laplacian kernel to remove blurry images. We reduce each
mesh’s complexity by merging vertices until ≤ 500K faces
remain. Then, we generate a texture parametrization with
Blender’s smart uv project [10] with an angle limit of 70◦.
We precompute the uv maps for each estimated pose.

(a) Projected Geometry

(b) Geometry in World-Space

Figure 3. Stylizing mesh faces in 2D (screen-space) is dependent
on angle and size of the projected geometry. Optimizing the faces
of a wall from a small grazing angle leads to stretched-out styliza-
tion patterns in world-space (yellow). Regions of similar size can
receive more or less patterns, depending on their projected size
(green and red). We denote stylization patterns as circles that are
optimized from screen-space into texture space.

3.3. Angle Filter

Style transfer in screen-space can create stretched-out
stylization patterns (see Fig. 3). Patterns might look cir-
cular from one view, but are stretched-out ellipses in world
space (e.g., when optimizing from a small grazing angle).
To prevent this, we combine coarse and fine style losses and
optimize fine details only for areas seen from good angles.
Similar to previous work [22,40], we utilize the fact that the
receptive field of high-resolution images is still small [41].
As a result, stylization patterns appear coarser and less de-
tailed, when optimized from a larger style image. We find
that coarse patterns are less prone to stretch artifacts.

For each pixel, we calculate its normal-to-view angle
αxy = ∡(⃗nxy, ⃗v) where ⃗nxy is the interpolated surface nor-
mal at pixel (x, y) and ⃗v is the viewing direction. Only the
pixels where αxy ≤ θa are used for the style loss with a
low-resolution style image that produces fine stylizations.
We always use all pixels and a high-resolution style image
to optimize coarse stylization patterns. This creates a com-
bination of coarse and fine patterns without stretch artifacts.

3.4. Multi-Resolution Part-based Losses

Multiple content and style losses combine depth levels
(Sec. 3.2) and angle filtering (Sec. 3.3) to optimize the tex-
ture without view-dependent artifacts. We encode the ren-
der pyramid ˆP with a pretrained VGG network [49] into
the feature pyramid ˆF . Using the depth level map, we only
keep corresponding features in each layer of ˆF . We com-
pute a coarse Gram matrix ˆGc and an angle-filtered fine one
ˆGf from the features in every layer. Similarly, Gc and Gf
correspond to the high- and low-resolution style images. We
define the style loss as

Ls(Is, ˆP ) =

θl(cid:88)

l

ˆwl · (||Gc − ˆGl

c||2

F + ||Gf − ˆGl

f ||2

F ) (3)

which sums over all depth levels independently (part-based)
and combines coarse and fine stylization (multi-resolution).

4

4. Results

Implementation Details. We optimize textures at a reso-
lution of 4096×4096 as a Laplacian Pyramid [52] with 4
layers and regularization strength λr=5000. We use λc=70
and λs=0.0001 for content and style loss weights. We op-
timize for 7 epochs and repeat each frame 10 times. We set
θmin=32 and use θl=4 render pyramid layers at heights of
{256, 432, 608, 784} pixels. We set θa=30◦, θd=0.25 me-
ters for ScanNet [11] and θa=40◦, θd=0.2 meters for Mat-
terport3D [5]. We incrementally halve the original style im-
age resolution until either the width or height reaches a size
of 256 pixels. We use the resulting image for the styliza-
tion of fine details and a two steps larger image for coarse
details. We use Adam [34] with batch size 1 and initial
learning rate 1 which decays multiplicatively by 0.1 every
3 epochs. We tried L-BFGS [59] which gave similar re-
sults. After optimization, we export the Laplacian Pyramid
to a single texture image and use a standard rasterizer with
Mipmaps and shading [18] for rendering.

Evaluation Metrics. We conduct a user study to show the
advantages of depth- and angle-awareness (Fig. 10). Addi-
tionally, we quantify them by stylizing with a “circle” im-
age (Fig. 8). We calculate the correlation between circle
size and depth in screen-space (Corr. 2D) and world-space
(Corr. 3D), as well as circle stretch as the ratio of horizontal
and vertical radius in world-space (Tab. 2). For quantify-
ing 3D consistency, we calculate the L1 distance between
source and reprojected target frames (Tab. 1). Please refer
to the supplemental material for more details about metrics.

4.1. Style Transfer on Scenes

Our method competes with 3D style transfer methods
that stylize a scene through an explicit or implicit repre-
sentation. Specifically, we compare our method with DIP
of Mordvintsev et al. [43] and NMR of Kato et al. [33]: like
us they also optimize a texture, but they do not utilize an-
gle or depth data. Additionally, we compare with LSNV
of Huang et al. [26], which uses a neural renderer to styl-
ize point clouds. We show results on the Matterport3D [5]
dataset in Fig. 5 and on the ScanNet [11] dataset in Fig. 6.
A visualization of textured meshes is given in Fig. 4. Please
see the supplemental material for more examples.

Our results show that we are able to stylize scenes with-
out view-dependent size or stretch artifacts. In contrast to
the other methods, our approach creates sharp and detailed
effects for the complete scene. Optimizing the complete
texture is especially difficult for DIP [43] and NMR [33],
which both contain noisy texels. LSNV [26] stylizes com-
plete images, but their results are less detailed. To quanti-
tatively evaluate our method and the related approaches, we
compute the mean L1 distance between source frame and a
reprojected target frame. The results are listed in Tab. 1.

RGB Mesh

NMR [33]

DIP [43]

Ours

Figure 4. Top-down view on stylized meshes in comparison to
previous work.

Method
LSNV [26]
NMR [33]
DIP [43]
Ours

Short-Range ↓ Long-Range ↓

4.873
1.565
1.396
1.225

7.207
2.165
1.723
1.566

Table 1. L1 distance between source frame and a reprojected target
frame. We report mean values for short-range (2nd next frame) and
long-range (20th next frame) in 10 different ScanNet [11] scenes.

4.2. Ablation Studies

Qualitative Comparison. Our method uses per-pixel an-
gle and depth as input to optimize the texture in a 3D-
aware manner. This helps avoid view-dependent stretch and
size artifacts being optimized into the texture from differ-
ent poses. We compare only using angle input (no render
pyramid) and not using angle/depth (only 2D texture opti-
mization with Laplacian Pyramid representation). In Fig. 7
we can see that using angle makes it easier to distinguish
between surfaces like the wall and sofa in row 2. Adding
depth creates smaller and detailed patterns in the back-
ground (e.g., the strokes in the background of row 1). Please
see the supplemental material for more examples.

We optimize all ablation modes such that stylization pat-
terns are equally strong, i.e., style should be similar for a
fair comparison. A too low degree of stylization would re-
duce view-dependent artifacts because original RGB colors
get more dominant. Similarly, a too high degree discards
content features too much, which increases artifacts.

Quantitative Comparison. We measure the effects of
angle- and depth-awareness as follows. We stylize a scene
with a “circle” image using only the style loss (see Fig. 8).
We then detect ellipses in the resulting images and measure
their horizontal and vertical axis lengths. Naturally, NST
creates ellipses of different shapes, but their overall distri-
bution reveals the degree of 3D awareness for the complete
scene. Inverse correlation between per-pixel depth and el-
lipse size in screen-space (Corr. 2D) indicates that stylized
features are smaller in the background. A weak correlation

5

RGB and Style

LSNV [26]

NMR [33]

DIP [43]

Ours

Figure 5. Comparison of stylization results for our method and related work on the Matterport3D [5] dataset. We texture the mesh with
each method (point cloud for Huang et al. [26] respectively) and render a single pose that is also captured in the RGB images.

RGB and Style

LSNV [26]

NMR [33]

DIP [43]

Ours

Figure 6. Comparison of stylization results for our method and related work on the ScanNet [11] dataset. We texture the mesh with each
method (point cloud for Huang et al. [26] respectively) and render a single pose that is also captured in the RGB images.

in world-space (Corr. 3D) indicates that absolute size is
independent of the observed poses. Both metrics together
classify the depth-awareness. View-dependent stretch is
larger if ellipse’s horizontal and vertical axes are of dif-
ferent lengths. The stylization is angle-aware if the stretch
is reduced. We do not measure coarse and fine stylization
this way, because the “circle” image contains too few high-
resolution features. Please see the supplemental material
for more details about metric computation. As can be seen
in Tab. 2, using angle and depth improves our method.

Depth Scaling. A key piece of our method is the render
pyramid of different image resolutions. By tuning the value
of θd, we change the threshold of when to sample from the

6

Method
Only 2D
Angle
Angle/Depth

Corr. 2D ↑ Corr. 3D ↓

0.172
0.126
0.538

0.126
0.110
0.125

Stretch ↓
3.512
3.396
3.391

Table 2. Quantitative results of stylizing a scene with a “circle” im-
age. With depth-awareness (Angle/Depth), depth and size strongly
correlate in screen-space (2D), but not in world-space (3D). That
is, the stylized circle size is smaller in the background of an image,
but uniformly distributed in 3D. Adding angle-awareness reduces
the circle stretch (Angle). In contrast, circles show more view-
dependent artifacts without depth/angle (Only 2D).

(a) RGB and Style

(b) Only 2D

(c) With Angle

(d) With Angle and Depth

Figure 7. Qualitative ablation study of our method. We compare ours (d) against only using angle (c) and not using angle and depth (b).
Using angle better distinguishes surfaces and using depth creates smaller/detailed stylization in the background.

User Study. We conduct a user study on the effectiveness of
our proposed depth- and angle-awareness. Users compared
our method against each baseline separately by preferring
one of two images. They judged in which image styliza-
tion patterns (a) have less visible stretch and (b) are smaller
in the background.
In total, 20 users each answered 70
questions, comparing against NMR [33], DIP [43] and ours
without angle- and depth-awareness (Only 2D). As can be
seen in Fig. 10, our method is preferred in both categories.

4.3. Comparison to Video Style Transfer

As an alternative way to optimizing a stylized texture,
one could combine video style transfer (VST) methods and
RGB texture mapping to produce a stylized scene in two
steps (see Fig. 11). We can obtain an RGB texture from all
images of the scene and render arbitrary trajectories, that we
stylize with a VST method (Tex→VST). However, we never
obtain a stylized texture this way and thus need the VST
method during inference for each novel pose. Stylization
details are also much lower, due to missing details in the
RGB texture and reconstructed geometry. By optimizing
directly from camera images, we obtain sharper details.

Alternatively, we can stylize a trajectory of camera im-

RGB

(a) Only 2D

(b) Ours

Style

(c) Only 2D

(d) Ours

Figure 8. Stylizing with a “circle” image quantifies 3D-awareness.
Circles become ellipsoidal on the couch without angle-awareness
(a); using angle, they are less distorted (b). Using depth, circles
are smaller in the back (d); they are equally large without (c).

next higher resolution. This increases (higher θd) or de-
creases (lower θd) the absolute stylization size, while still
retaining relative change in size (see Fig. 9). This allows to
fine-tune the complete scene until a desired look is obtained.

7

θd = 0.05

θd = 0.1

θd = 0.15

θd = 0.2

Figure 9. Variation of the minimum depth θd from 0.05 to 0.2
meters.
Increasing its value leads to overall larger stylizations,
while still retaining relative size differences within the scene. This
can be used to fine-tune the stylization to a satisfying look.

(a) Stretch

(b) Size

Figure 10. We conduct a user study and ask subjects to select
the results where stylization patterns (a) have less visible stretch
and (b) are smaller in the background. The numbers indicate the
percentage of preference for our method.

ages with a VST method and optimize an RGB texture from
these images (VST→Tex). However, we might only have
access to a sparse set of images in some scenarios. Due
to inconsistencies between stylized frames (e.g., caused by
illumination changes), the optimized texture is blurrier, as
well. Our method is 3D-consistent by combining stylization
and texture optimization over all available images directly.

4.4. Runtime Comparison

We propose an optimization-based NST method, that
converges in roughly 3 hours on a single RTX 3090 GPU.
After optimization, we can use the texture in traditional
graphics pipelines and achieve real-time rendering, simi-
lar to [33, 43]. In contrast, model-based NST [26, 33, 36]
might take days to train and needs a forward-pass at infer-
ence. However, these methods can generalize across scenes,
whereas we need to optimize a separate texture per-scene.

8

RGB

(a) VST→Tex

(b) Tex→VST

Ours

Figure 11. Video style transfer (VST) can be combined with RGB
texture mapping to stylize an indoor scene. Either, we optimize a
texture from the output of VST (a), or we render trajectories from
an RGB texture and then apply VST (b). The first row uses VST
of Wang et al. [55] and the second Deng et al. [16], respectively.
In comparison, we produce sharper details and less noise.

4.5. Limitations

By design, our method is a per-scene/per-style NST al-
gorithm, i.e., we optimize each explicit texture image sepa-
rately. Recent work in implicit texture representations [46]
could enable training generative models for our task. We
do not disentangle lighting and albedo, i.e., view-dependent
effects in camera images can be visible in the stylized
texture. One could leverage neural rendering techniques
to train a relightable stylization model [51].
Incomplete
mesh reconstructions lead to holes in rendered poses, which
can be reduced by employing mesh completion techniques
first [12, 14, 15]. Similarly, an insufficient number of poses
may lead to unobserved surfaces during optimization, i.e.,
we do not hallucinate texture. Inpainting techniques [51]
could be utilized to complete those texels.

5. Conclusion

We have shown a method to stylize the mesh of room-
scale indoor scene reconstructions. We lift style transfer
to the 3D domain by optimizing a texture only through 2D
images. Our method makes use of depth and surface nor-
mals of the mesh to achieve uniform world space stylization
without view-dependent artifacts. For that, we split the loss
calculation into image parts and stylize coarse and fine de-
tails separately. The explicit texture representation allows
for real-time rendering of the scene after optimization.

Acknowledgements

This project is funded by a TUM-IAS Rudolf M¨oßbauer
Fellowship, the ERC Starting Grant Scan2CAD (804724),
and the German Research Foundation (DFG) Grant Making
Machine Learning on Static and Dynamic 3D Data Practi-
cal. We also thank Angela Dai for the video voice-over.

References

[1] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis
Savva, Angel X Chang, and Matthias Nießner. Scan2cad:
Learning cad model alignment in rgb-d scans. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 2614–2623, 2019. 1

[2] Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi.
Patch-based optimization for image-based texture mapping.
ACM Trans. Graph., 36(4):106–1, 2017. 1, 2

[3] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of

Software Tools, 2000. 12

[4] Xu Cao, Weimin Wang, Katashi Nagao, and Ryosuke Naka-
mura.
Psnet: A style transfer network for point cloud
In Proceedings of the
stylization on geometry and color.
IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 3337–3345, 2020. 2

[5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej
Halber, Matthias Niessner, Manolis Savva, Shuran Song,
Andy Zeng, and Yinda Zhang. Matterport3d: Learning
arXiv preprint
from rgb-d data in indoor environments.
arXiv:1709.06158, 2017. 4, 5, 6

[6] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang
Hua. Coherent online video style transfer. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 1105–1114, 2017. 2

[7] Xinghao Chen, Yiman Zhang, Yunhe Wang, Han Shu, Chun-
jing Xu, and Chang Xu. Optical flow distillation: Towards
efficient and stable video style transfer. In European Con-
ference on Computer Vision, pages 614–630. Springer, 2020.
2

[8] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei
sheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via im-
plicit representation and hypernetwork, 2021. 1, 2

[9] Tai-Yin Chiu and Danna Gurari.

Iterative feature transfor-
mation for fast and versatile universal style transfer.
In
European Conference on Computer Vision, pages 169–184.
Springer, 2020. 2

[10] Blender Online Community. Blender - a 3D modelling and
rendering package. Blender Foundation, Stichting Blender
Foundation, Amsterdam, 2018. 4

[11] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5828–5839, 2017. 4, 5, 6,
11, 13, 15

[12] Angela Dai, Christian Diller, and Matthias Nießner. Sg-nn:
Sparse generative neural networks for self-supervised scene
completion of rgb-d scans. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 849–858, 2020. 8

[13] Angela Dai, Matthias Nießner, Michael Zoll¨ofer, Shahram
Izadi, and Christian Theobalt. Bundlefusion: Real-time
globally consistent 3d reconstruction using on-the-fly surface
re-integration. ACM Transactions on Graphics 2017 (TOG),
2017. 1

[14] Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed,
J¨urgen Sturm, and Matthias Nießner. Scancomplete: Large-
scale scene completion and semantic segmentation for 3d
scans. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4578–4587, 2018. 8

[15] Angela Dai, Yawar Siddiqui, Justus Thies, Julien Valentin,
and Matthias Nießner. Spsg: Self-supervised photomet-
In Proceedings of
ric scene generation from rgb-d scans.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 1747–1756, 2021. 8

[16] Yingying Deng, Fan Tang, Weiming Dong, Haibin Huang,
Chongyang Ma, and Changsheng Xu. Arbitrary video
style transfer via multi-channel correlation. arXiv preprint
arXiv:2009.08003, 2020. 2, 8

[17] Arnaud Dessein, William AP Smith, Richard C Wilson, and
Edwin R Hancock. Seamless texture stitching on a 3d mesh
by poisson blending in patches. In 2014 IEEE International
Conference on Image Processing (ICIP), pages 2031–2035.
IEEE, 2014. 1, 2

[18] James D Foley, Foley Dan Van, Andries Van Dam, Steven K
Feiner, John F Hughes, and J Hughes. Computer graph-
ics: principles and practice, volume 12110. Addison-Wesley
Professional, 1996. 4, 5

[19] Chang Gao, Derun Gu, Fangjun Zhang, and Yizhou Yu.
Reconet: Real-time coherent video style transfer network.
In Asian Conference on Computer Vision, pages 637–653.
Springer, 2018. 2

[20] Wei Gao, Yijun Li, Yihang Yin, and Ming-Hsuan Yang. Fast
video multi-style transfer. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision,
pages 3222–3230, 2020. 2

[21] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
Image style transfer using convolutional neural networks. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016. 1, 2, 3, 4
[22] Leon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron
Hertzmann, and Eli Shechtman. Controlling perceptual fac-
tors in neural style transfer. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3985–3993, 2017. 2, 3, 4

[23] Agrim Gupta, Justin Johnson, Alexandre Alahi, and Li Fei-
Fei. Characterizing and improving stability in neural style
transfer. In Proceedings of the IEEE International Confer-
ence on Computer Vision, 2017. 2

[24] Fangzhou Han, Shuquan Ye, Mingming He, Menglei Chai,
and Jing Liao. Exemplar-based 3d portrait stylization. arXiv
preprint arXiv:2104.14559, 2021. 2

[25] Filip Hauptfleisch, Ondˇrej Texler, Aneta Texler, Jaroslav
Kˇriv´anek, and Daniel S´ykora.
StyleProp: Real-time
example-based stylization of 3d models. Computer Graphics
Forum, 39(7):575–586, 2020. 2

[26] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh
Singh, and Ming-Hsuan Yang. Learning to stylize novel
views. arXiv preprint arXiv:2105.13509, 2021. 1, 2, 5, 6,
8, 15

[27] Jingwei Huang, Angela Dai, Leonidas J Guibas, and
Matthias Nießner. 3dlite: towards commodity 3d scanning

9

for content creation. ACM Trans. Graph., 36(6):203–1, 2017.
1, 2

[28] Jingwei Huang, Justus Thies, Angela Dai, Abhijit Kundu,
Chiyu Jiang, Leonidas J Guibas, Matthias Nießner, Thomas
Funkhouser, et al. Adversarial texture optimization from
In Proceedings of the IEEE/CVF Conference
rgb-d scans.
on Computer Vision and Pattern Recognition, pages 1559–
1568, 2020. 1, 2

[29] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 1501–1510, 2017. 2

[30] Shahram Izadi, David Kim, Otmar Hilliges, David
Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie
Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
et al. Kinectfusion: real-time 3d reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology, pages 559–568, 2011. 1, 2

[31] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution.
In
European conference on computer vision, pages 694–711.
Springer, 2016. 2

[32] Nikolai Kalischek, Jan D Wegner, and Konrad Schindler.
In the light of feature distributions: moment matching for
neural style transfer. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
9382–9391, 2021. 2

[33] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu-
ral 3d mesh renderer. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 3907–
3916, 2018. 1, 2, 5, 6, 7, 8, 11, 14, 15

[34] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 5

[35] Nicholas Kolkin,

Jason

Salavon,

and Gregory
Shakhnarovich. Style transfer by relaxed optimal transport
In Proceedings of the IEEE/CVF Con-
and self-similarity.
ference on Computer Vision and Pattern Recognition, pages
10051–10060, 2019. 2

[36] Georgios Kopanas, Julien Philip, Thomas Leimk¨uhler, and
George Drettakis. Point-based neural rendering with per-
In Computer Graphics Forum, vol-
view optimization.
ume 40, 2021. 1, 2, 8

[37] Chuan Li and Michael Wand. Combining markov random
fields and convolutional neural networks for image synthesis.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 2479–2486, 2016. 2

[38] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang.
Learning linear transformations for fast image and video
style transfer. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 3809–
3817, 2019. 2

[39] Xiao-Chang Liu, Ming-Ming Cheng, Yu-Kun Lai, and
Paul L Rosin. Depth-aware neural style transfer. In Proceed-
ings of the Symposium on Non-Photorealistic Animation and
Rendering, pages 1–10, 2017. 3

10

[40] Xiao-Chang Liu, Xuan-Yi Li, Ming-Ming Cheng, and
arXiv preprint
Geometric style transfer.

Peter Hall.
arXiv:2007.05471, 2020. 2, 4

[41] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel.
Understanding the effective receptive field in deep convolu-
In Proceedings of the 30th Inter-
tional neural networks.
national Conference on Neural Information Processing Sys-
tems, pages 4905–4913, 2016. 4

[42] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The
contextual loss for image transformation with non-aligned
data. In Proceedings of the European Conference on Com-
puter Vision (ECCV), pages 768–783, 2018. 2

[43] Alexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert,
and Chris Olah. Differentiable image parameterizations.
Distill, 3(7):e12, 2018. 1, 2, 3, 5, 6, 7, 8, 11, 14, 15

[44] Richard A Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J Davison, Pushmeet
Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon.
Kinectfusion: Real-time dense surface mapping and track-
ing. In 2011 10th IEEE international symposium on mixed
and augmented reality, pages 127–136. IEEE, 2011. 1
[45] Matthias Nießner, Michael Zollh¨ofer, Shahram Izadi, and
Marc Stamminger. Real-time 3d reconstruction at scale us-
ing voxel hashing. ACM Transactions on Graphics (ToG),
32(6):1–11, 2013. 1

[46] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo
Strauss, and Andreas Geiger. Texture fields: Learning tex-
ture representations in function space. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 4531–4540, 2019. 8

[47] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Artistic style transfer for videos. In German conference on
pattern recognition, pages 26–36. Springer, 2016. 2

[48] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Artistic style transfer for videos and spherical images. Inter-
national Journal of Computer Vision, 126(11):1199–1219,
2018. 2

[49] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 4

[50] Daniel S´ykora, Ondˇrej Jamriˇska, Ondˇrej Texler, Jakub Fiˇser,
Michal Luk´aˇc, Jingwan Lu, and Eli Shechtman. StyleBlit:
Fast example-based stylization with local guidance. Com-
puter Graphics Forum, 38(2):83–91, 2019. 2

[51] Ayush Tewari, O Fried, J Thies, V Sitzmann, S Lombardi, Z
Xu, T Simon, M Nießner, E Tretschk, L Liu, et al. Advances
In ACM SIGGRAPH 2021 Courses,
in neural rendering.
pages 1–320. 2021. 8

[52] Justus Thies, Michael Zollh¨ofer, and Matthias Nießner. De-
ferred neural rendering: Image synthesis using neural tex-
tures. ACM Transactions on Graphics (TOG), 38(4):1–12,
2019. 3, 5

[53] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Vic-
tor S Lempitsky. Texture networks: Feed-forward synthesis
of textures and stylized images. In ICML, 2016. 2

[54] Michael Waechter, Nils Moehrle, and Michael Goesele. Let
there be color! large-scale texturing of 3d reconstructions.

In European conference on computer vision, pages 836–850.
Springer, 2014. 1, 2

[55] Wenjing Wang, Shuai Yang, Jizheng Xu, and Jiaying Liu.
Consistent video style transfer via relaxation and regular-
ization. IEEE Transactions on Image Processing, 29:9125–
9139, 2020. 2, 8

[56] Xide Xia, Tianfan Xue, Wei-sheng Lai, Zheng Sun, Abby
Chang, Brian Kulis, and Jiawen Chen. Real-time localized
In Proceedings of the
photorealistic video style transfer.
IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 1089–1098, 2021. 2

[57] Kangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, and
Sanja Fidler. 3dstylenet: Creating 3d shapes with geometric
and texture style variations. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 12456–
12465, 2021. 2, 3

[58] Qian-Yi Zhou and Vladlen Koltun. Color map optimization
for 3d reconstruction with consumer depth cameras. ACM
Transactions on Graphics (TOG), 33(4):1–10, 2014. 1, 2
[59] Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge No-
cedal. Algorithm 778: L-bfgs-b: Fortran subroutines for
large-scale bound-constrained optimization. ACM Trans-
actions on mathematical software (TOMS), 23(4):550–560,
1997. 5

A. Supplemental

A.1. Reprojection Error

We calculate reprojection error as the L1 distance be-
tween source frame and reprojected target frame. This al-
lows to quantify the 3D consistency. While a texture rep-
resentation is 3D consistent by design, unoptimized tex-
els might still create visible noise artifacts when rendering
a trajectory. We calculate reprojection error on the Scan-
Net [11] dataset by using their captured camera trajectories.
For each source frame, we select a target frame that is (a)
two frames after the source frame (short-range consistency)
or (b) 20 frames after the source frame (long-range consis-
tency). Using the estimated poses and camera intrinsics, we
warp the pixels of the target frame to the source view. We
calculate L1 distance in the normalized image range [0, 1]
between reprojected target frame and source frame for all
pixels that are visible in both views. Please see Fig. 12 for
a visualization of the procedure.

Because the estimated poses are not perfectly accurate,
the reprojection error may never sink below a certain thresh-
old that captures this inaccuracy. Still, it allows to quantify
the 3D consistency by measuring the additional inconsisten-
cies caused by unoptimized textures, i.e., the error is higher
for unoptimized textures that are less consistent.

A.2. User Study Setup

We conduct a user study on the effectiveness of our pro-
posed depth- and angle-awareness. Users compared our
method against each baseline separately by preferring one

Source Frame Target Frame Reprojected

Residual

Figure 12. Sample images used for calculating the reprojection
error. We select a source frame and a target frame and warp the
target pixels from the target view to the source view. Reprojection
error is calculated as the L1 distance of all pixels between source
frame and reprojected target frame, that are visible in both views.

of two images. They judged in which image stylization pat-
terns (a) have less visible stretch and (b) are smaller in the
background. In total, 20 users each answered 70 questions,
comparing against NMR [33], DIP [43] and ours without
angle- and depth-awareness (Only 2D). We show two sam-
ple questions, one for each type, in Fig. 13. As can be seen,
users have the possibility to decide for one of two images
or to answer that none of the two is better/worse. The or-
der of questions and of the “A” and “B” images is random
and different for each user. Users have the possibility to
zoom-in on the images for better judgement. Additionally,
we add rectangles on image regions that might be especially
interesting for evaluation of the questions. Note that users
still had to consider the whole image in their answer; the
rectangles merely act as additional input.

A.3. Variation of Depth Levels

The number of depth levels θl controls the depth varia-
tion that can be achieved within rendered poses of a scene.
Setting θl=4 is sufficient for our datasets, as larger scene
extent is rarely captured by many pixels. We could precom-
pute uv maps at larger resolutions to enable depth scaling at
even larger depth values. Small scenes may not require the
last layers, in which case they are simply not utilized dur-
ing optimization. Adding more layers in-between maps less
pixels to one layer, which can yield insufficient Gram ma-
trices and is computationally more expensive. Decreasing
θl reduces size variation at different depths (see Fig. 14).

A.4. Circle-Stretch and -Size Metric

We describe in more detail the metrics and principles
used in the main paper to quantify the effects of our depth
and angle awareness. In order to measure the effects, we
stylize a scene with a hand-crafted “circle” image (see main
paper) and only use the (multi-resolution, part-based) style
loss. After optimization, the red circles are stylized all over
the scene and are well-suited to describe the two drawbacks
of missing angle- and depth-awareness. For example, cir-
cles become ellipsoidal if a small grazing angle is used
for stylization and circles change their radius inconsistently
without depth awareness. We can now measure the degree

11

(a) Size Sample

(b) Stretch Sample

Figure 13. Sample images used for the user study. Users judged in which image stylization patterns (a) are smaller in the background and
(b) have less visible stretch.

imum deviation from a convex hull, as measured by “con-
vexityDefects” [3]. We now fit ellipses to the remaining
contours with “fitEllipse”. We extract the pixel-radius as

(a) θl=4

(b) θl=2

Figure 14. Variation of the number of depth levels θl. Decreasing
θl reduces size variation at different depths.

sp = max(

hp
vp

,

vp
hp

)

for every fitted ellipse and calculate its pixel-stretch as

rp =

hp + vp
2

(8)

(9)

we alleviate these issues by measuring the size and stretch
of the circles/ellipses. Naturally, NST creates ellipses of
different shapes, but their overall distribution reveals the de-
gree of 3D awareness for the complete scene.

where hp is the pixel-length of the horizontal ellipse radius
and vp the vertical, respectively. We remove the remain-
ing wrongfully detected ellipses with rp < 10, rp > 1000
and sp > 10 to get a result like in Fig. 15. We use these
ellipse characteristics to calculate metrics for depth- and
angle-awareness.

A.4.1 Segmentation of Ellipses

A.4.2 Calculation of Depth Metrics

First, we automatically segment red ellipses from each im-
age of the stylized scene (see Fig. 15). We first apply an
HSV filter and only keep pixels in the ranges 0.6 ≤ S, V ≤
1.0, 0.0 ≤ H ≤ 0.08 and 0.88 ≤ H ≤ 1.0. Then we
turn the filtered image into a binary mask by thresholding
colors above 0.15 intensity and denoise it with OpenCV’s
“fastNLMeansDenoising” function [3]. Afterwards, we use
OpenCV’s contour detection to get an edge map. We filter
out all contours with maxd > 2, where maxd is the max-

We calculate the correlation between per-pixel depth dxy
and ellipse radius rp to quantify the effect of our depth-
awareness in the 2D image plane (Corr. 2D). For each
detected ellipse we use the depth value of the pixel cor-
responding to the ellipse center. A high negative correla-
tion (e.g., −0.5) signals, that ellipse size decreases with
increasing depth, whereas a low correlation (e.g., −0.05)
signals, that ellipse size is independent of changes in depth.
A method that is able to stylize a scene depth-aware would

12

create ellipses with smaller size in the background and thus
have a high negative correlation in the 2D image plane.

To quantify the correlation in 3D, we backproject hp and
vp to world-space using the estimated pose and camera in-
trinsics and calculate the world-space radius as

rw =

hw + vw
2

(10)

where hw and vw are the backprojected axis lengths. We
then calculate the correlation between rw and the per-pixel
depth dxy (Corr. 3D). A high negative correlation (e.g.,
−0.5) signals,
that ellipse size in world-space still de-
creases with increasing depth, whereas a low correlation
(e.g., −0.05) signals, that ellipse size in world-space is in-
dependent of changes in depth. A method that is able to
stylize a scene depth-aware would create ellipses with uni-
formly distributed size in world-space (because the ellipse
size should only change when rendering a scene from dif-
ferent poses, due to perspective projection).

Note that the stylized ellipses naturally vary in their
sizes (e.g., ellipses can be smaller and larger independent
of depth). Therefore, the correlations will be precise up to
a certain threshold. However, the distribution of all seg-
mented ellipses across the whole scene still allows to quan-
tify the depth-awareness.

A.4.3 Calculation of Angle Metric

We backproject the pixel-stretch sp back to world-space as

sw = max(

hw
vw

,

vw
hw

)

(11)

. Then we calculate the arithmetic mean over all sw values
for all detected ellipses. A higher mean value means that
overall we have more stretch, whereas a lower value signals
a more uniform stylization result. A method that is able to
stylize a scene angle-aware would create ellipses with small
stretch.

A.5. Additional Qualitative Results

We show additional qualitative results for our method.
Additional comparisons on the ScanNet [11] dataset can

be found in Fig. 16 and Fig. 17.

Additional comparisons for our ablation study can be

found in Fig. 18.

A.6. Style Image Assets

Throughout the main paper and the supplemental mate-
rial, we use style images created by artists. In Fig. 19 we
list all images and give credit to their respective creators.

13

Input

Filtered Red

Denoised Binary Mask

Contour Detection

Detected Ellipses

Ellipses with Axis Points

Figure 15. Our pipeline for segmenting ellipses out of a stylized image for quantification of the depth and stretch effects. We filter the
image to only contain red colors (Filtered Red) and then transform that to a denoised binary image to remove remaining pixels (Denoised
Binary Mask). Afterwards we use contour detection to convert the binary mask into edge maps and finally fit ellipses to all applicable
contours (Detected Ellipses). Lastly, we find the horizontal and vertical axis of the ellipses as the distance from the center to corresponding
points on the edge of each ellipse.

RGB Mesh

NMR [33]

DIP [43]

Ours

Figure 16. Top-down view on stylized meshes in comparison to previous work.

14

RGB and Style

LSNV [26]

NMR [33]

DIP [43]

Ours

Figure 17. Comparison of stylization results for our method and related work on the ScanNet [11] dataset. We texture the mesh with each
method (point cloud for Huang et al. [26] respectively) and render a single pose that is also captured in the RGB images.

15

(a) RGB and Style

(b) Only 2D

(c) With Angle

(d) With Angle and Depth

Figure 18. Qualitative ablation study of our method. We compare ours (d) against only using angle (c) and not using angle and depth (b).
Using angle better distinguishes surfaces and using depth creates smaller/detailed stylization in the background.

16

June Tree,
Natasha Wescoat

The Starry Night,
Vincent van Gogh, 1889

Femme au chapeau,
Henri Matisse, 1905

Dinamismo di un’ automobile,
Luigi Russolo, 1913

The Muse,
Pablo Picasso, 1935

Il cavaliere rosso,
Carlo Carra, 1913

The Viaduct,
Henri Edmond Cross

Kanagawa oki nami ura,
Katsushika Hokusai, 1830-1832

Skrik,
Edvard Munch, 1893

Mosaic in Opus tessellatum

Mosaic (unknown),
WikiArt.org

Self-Portrait,
Pablo Picasso, 1907

Feathers Leaves and Petals,
Kathryn Corlett

Small Magellanic Cloud,
NASA, ESA and A. Nota

Edgar Poe, Charles Baudelaire,
Um Orangotango e o Corvo,
Julio Pomar, 1985

Lapin et casserole rouge,
Bernard Buffet, 1948

L’homme `a la tulipe,
Jean Metzinger, 1906

The Shipwreck of the Minotaur,
J.M.W. Turner, 1805

Sketch 2 for composition VII,
Wassily Kandinsky, 1913

Figure 19. List of all artistic paintings used throughout the main paper and supplemental material. We list the name of the painting and its
author if known.

17

