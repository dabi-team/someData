Population-based Respiratory 4D Motion Atlas Construction
and its Application for VR Simulations of Liver Punctures

Andre Mastmeyer, Matthias Wilms, and Heinz Handels

Institute of Medical Informatics, University of Lübeck, Lübeck, Germany

ABSTRACT

Virtual reality (VR) training simulators of liver needle insertion in the hepatic area of breathing virtual patients
often need 4D image data acquisitions as a prerequisite. Here, ﬁrst a population-based breathing virtual patient
4D atlas is built and second the requirement of a dose-relevant or expensive acquisition of a 4D CT or MRI data
set for a new patient can be mitigated by warping the mean atlas motion. The breakthrough contribution of
this work is the construction and reuse of population-based, learned 4D motion models.

Keywords: Virtual Reality, Liver Puncture Training, 4D Motion Models, Application of 4D Motion Models

1. PURPOSE

In recent works, for the risk-free training and planning of surgical needle interventions with visuo-haptic virtual
reality simulators the inclusion of breathing motion models1 is a core component. The simulation of tissue and
needle deformation for puncture interventions and the modelling of virtual patient bodies2–8 has been an active
research branch for years9–14 and breathing motion can be rendered in a recent 4D visuo-haptic simulator (Fig.
1) with GPU support using 4D direct volume rendering.15–20

In our setup,17 the visuo-haptic simulation of the patient’s breathing motions is a new key feature.10, 12, 19 In
the abdominal liver region, respiratory movements dominate the visuo-haptic experience of the physician with
possible breathing displacements of up to 5 cm.21 Conceptually, with a 4D breathing model, a new 3D CT data
set could be animated. The model can be patient-speciﬁc22 or an averaged population-based model, which is the
main topic of this work. Such a model then can be warped by non-linear registration to the 3D CT data set of
a new patient.22

Surrogate free mean motion models were proposed by our group23 for the lungs. The motivation for such
models is to supply a more general model by averaging out patient speciﬁc breathing patterns. Thus, transferring
such models instead of a patient-speciﬁc model to a new static 3D patient can be favorable.

Typically for VR training purposes, the acquisition of new 4D CT or MR data is impossible for ethics and

cost reasons. Thus the transfer of existing motion models is appealing.

Here, we present a ﬁrst study of an eﬀective and eﬃcient building process of a mean 4D breathing atlas
parametrized by a surrogate signal and apply it to 3D patient data for rendering it in a 4D VR environment.17

2.1 Study Data

2. MATERIALS AND METHODS

The 4D mean model is built from Npats4D = 5 4D-CT data sets (Pat. 1-5, Fig. 2a-c) of the thorax and upper
abdomen. Corresponding Nphases = 14 respiratory phase images (up to Nvox =512x512x462, 1 mm3) with low
dimensional spirometry parameter signals vp(t) were used (Fig. 2d). The new patient is represented only by a
static 3D CT data set in a frame of reference (e.g. reference phase from Pat. 3). Selected reference phases jref
correspond to phase images in a corresponding inhalation state (new 3D patients hold breath, Fig. 2a-c).

Further author information: (Send correspondence to Andre Mastmeyer; E-mail: mastmeyer@imi.uni-luebeck.de).

This work is supported by the German Research Foundation: DFG HA 2355 / 11-2.

7
1
0
2
c
e
D
2
2

]

V
C
.
s
c
[

2
v
3
9
8
1
0
.
2
1
7
1
:
v
i
X
r
a

To appear in SPIE Medical Imaging: Image Processing 2018
p. 1

 
 
 
 
 
 
(a)

(b)

Figure 1: VR simulator setup: (a) Semi-transparent rendering mirror with successful needle insertion indicated
by green background, workbench in which a stereo-monitor is mounted above the mirror, haptic steering device
below the mirror with device handle held by the needle steering user. (b) Graphical user interface with breathing
virtual patient and assisting viewports, e.g. ultrasound simulation in the bottom left viewport.

2.2 Intra-patient Motion Modeling in a Frame of Reference
Breathing motion modeling relies on Npats4D 4D CT data sets with Nphases phases, indexed by j ∈ {1..Nphases}
and p ∈ {1..Npats4D }. Spirometry signals vp(t) [ml] serve as surrogate signal components to represent the patient’s
breathing state: zp(t) = (vp(t), v(cid:48)p(t)). The derivative v(cid:48)p(t) allows to model respiratory hysteresis (inhalation
vs. exhalation). Signal and motion dependence are assumed (locally) linear. Nphases −1 intra-patient inter-phase
image registrations to a selected patient speciﬁc reference phases jref are conducted ﬁrst:

ϕp

j→jref

= argmin

ϕ

(cid:16)

Dintra

(cid:104)

I p
jref

, I p

(cid:105)
j ◦ ϕ

+ α · Rintra(ϕ)

(cid:17)

,

(1)

j ∈ {1, .., Nphases}, j (cid:54)= jref
p ∈ {1, ..., Npats4D }.

Using the distance metric Dintra and regularizer Rintra, this step yields motion ﬁelds for each 4D patient data set.
These can be used to calculate vector ﬁeld coeﬃcients ˆA = (ap
3) for the application of a linear multivariate
regression model.

2, ap

1, ap

Let us introduce a bijective serialization operator ser(.) that produces a long column vector in a standardized
way. The serialization of ˆA is A ∈ R3·Nvox×3. For one patient’s reference space, the serialized vector ﬁelds as
regressand V = ser(ϕ1→jref , ..., ϕNphases→jref ) ∈ R3·Nvox×Nphases are approximated by linear regression subject
to an expanded serialized surrogate signal with z1(t) = (vp(t), v(cid:48)p(t), 1) ∈ R3, i.e. Z = ser(z1(t)) ∈ R3×Nphases
as regressor and discrete time points t = j ∈ 1..Nphases:24

ˆA = ser−1

(cid:18)

argmin
A

(cid:19)

||V − A · Z||2
F

where F denotes the Froebenius norm.25

We yield a motion estimate using the solution ˆA for any point in time:

2(x) · v(cid:48)p(t) + ap
Now, we can simulate each 4D patient’s breathing state over time t:

1(x) · vp(t) + ap

ˆϕp(x, t) = ap

3(x), x ∈ Ωp.

(2)

(3)

jref
Our latest VR training simulator can render this compact, personalized representation of a breathing virtual
patient I p(t) in realtime by eﬃcient raycasting using a bent rays approach.17

I p(t) = I p

◦ ˆϕp(x, t)

(4)

To appear in SPIE Medical Imaging: Image Processing 2018
p. 2

2.3 Population-based Breathing Motion Models

The next step is the averaging of the personalized breathing models to yield a mean 4D motion model.

For a common frame of reference image phase, a reference patient pref

is selected from the 4D patient

population. This patient is targeted by inter-patient registrations in the reference phase jref .

Nonlinear registrations ϕ(x) : Ωp → Ωpref , minimizing the distance metric Dinter, warp the patient’s image

data to the selected reference patient pref :

ϕp→pref
jref

= argmin

ϕ

(cid:0)Dinter

(cid:2)I pref
jref

, I p

jref

◦ ϕ(cid:3) + β · Rinter(ϕ)(cid:1) , p (cid:54)= pref ,

(5)

using a distance measure Dinter, regularizer Rinter, and β a regularizer weight.

Next by ϕp→pref

, we warp the intra-patient-inter-phase deformations ϕp

j of the 4D patients to the common

reference frame jref of pref :

jref

j,p = ϕp→pref
ϕpref

jref

◦ ϕp

j ◦ (cid:0)ϕp→pref

jref

(cid:1)−1

.

(6)

With all motion ﬁelds in the same common frame of reference, averaging yields the prerequisite for the mean
motion model:

ˆϕavg
j

(x) =

1
Npats4D

Npats4D(cid:88)

p=1

ϕpref
j,p

(7)

Averaging is also done for the surrogate signals: vavg(t) = 1/Npats4D
motion behavior can now be estimated eﬃciently similar to one of the 4D patients (see section 2.2):

vp(t). Finally, the mean patient’s

(cid:80)Npats4D
p=1

ˆϕavg(x, t) = aavg

1

(x) · vavg(t) + aavg

2

(x) · v(cid:48)avg(t) + aavg

3

(x), x ∈ Ωpref .

(8)

Image data for the artiﬁcial mean patient I avg
jref
as the ﬁelds.

underwent an analogous transformation and averaging pipeline

2.4 Application of Mean Motion Models

Three applications are at hand now at this juncture:

1. An individual patient from the training population can be animated by the mean motion model and be
presented to the user for training purposes (Fig. 3a-d). This is alternative to using the patient-individual
motion model17, 22 providing a less individual or situational breathing pattern. In radiation therapy, this
could make sense in follow-up fractional treatment sessions.

2. The population based patient intensity and motion atlas can be used to present an interactively manipulable

breathing virtual training atlas to the apprentice user (Fig. 3e-h).

3. The most relevant application of the 4D mean patient breathing model is warping it to new unseen 3D

static patient data and render it in our current 4D VR simulator (Fig. 3i-l).

Details of the inter-patient model transfer can be found in.22 To this aim, we restate Eq. 5:

ϕavg→new

jref

= argmin

ϕ

(cid:0)Dinter

(cid:2)I new
jref

, I avg
jref

◦ ϕ(cid:3) + β · Rinter(ϕ)(cid:1) ,

and Eq. 6 accordingly to yield the new patient’s motion behaviour:

ˆϕnew(x, t) then can analogously be built as in Eqs. 3 and 8.

j = ϕavg→new
ϕnew
jref

◦ ϕavg
j

◦ (cid:0)ϕavg→new
jref

(cid:1)−1

.

(9)

(10)

To appear in SPIE Medical Imaging: Image Processing 2018
p. 3

(a)

(b)

(c)

(d)

Figure 2: Field of view diﬀerences of the patient image data: (a) Pat. 1 (lilac), Pat. 2 (wine red). (b) Pat. 3
(yellow), Pat. 4 (turquois). (c) Pat. 5 (purple), Pat. 1 (lilac). (d) Example spirometry signal from Pat. 3.

The respiratory motion applied to the new 3D patient image data can now be calculated eﬃciently (see
and the

section 2.2). Thus based only on a relatively low dose 3D-CT data acquisition in reference phase I new
jref
transferred mean motion model, the breathing movements can be plausibly approximated:

ˆϕnew(x, t) = aavg

1

(x) · v(t) + aavg

2

(x) · v(cid:48)(t) + aavg

3

(x), x ∈ Ωnew.

and animated:

I new(t) = I new
jref

◦ ˆϕnew(x, t).

(11)

(12)

Optionally, simulated surrogate signals v(t) with stochastic irregularity26 in the range of the observed breath-
ing states can be used for the 4D animation of the mean atlas and new 3D CT data by such a mean motion
model. In the data sets used here, we found relative breathing volumes of ca. 0 - 1200 ml.

2.5 Study Set-up
Volume image data was resampled to a size of 2563 voxels to ﬁt into moderate graphics cards with 2 GB
GPU-RAM (Nvidia GTX 680) also comprising the three vector ﬁeld coeﬃcients a1..a3.17 From our 4D patient
population with ﬁelds of view shown in Fig. 2, for one experiment a representative 4D-CT data set pref in
maximum inhalation phase jref of the thorax and upper abdomen serves as common reference frame and is left
out from the motion model building process. The new patient for application option three is represented only
by a static 3D CT data set. The reference patient with the image data I pref
and the new 3D patient image data
jref
I new
are assumed to be in the same breathing state. For each of the Npats4D 4D patients and according to Eq. 1,
jref
we ﬁrst perform the intra-patient inter-phase registrations to the chosen reference phase jref of the currently
considered 4D image data. In Eq. 1, we use the distance measure Dintra = DN SSD (normalized sum of voxel-wise
squared diﬀerences ) and Rintra = RSM P (sliding motion’-preserving) to cope with discontinuities in the pleural
cavity.27 In Eqs. 5 and 9, Dinter = DSSD (sum of squared diﬀerences) and Rinter = RDN L (diﬀusive non-
linear regularization). Regularization weights are set as α = 0.1 or β = 1.0, as interphase registration requires
lesser regularization inﬂuence (same anatomy morphology). DICE coeﬃcients of warped expert segmentation
masks (liver, left/right lungs) from a leave-one-out crossvalidation are given to classify the quality of the diﬃcult
inter-patient registration. We present sample images for the three use cases, four time instants and online movie
footage for reference patient 3 and the spirometry signal from Fig. 2d in application 2 (mean intensity and
motion atlas).

3. RESULTS AND CONCLUSION

In the puncture-relevant liver region, the patients’ breathing states are simulated plausibly for an individual
patient (Fig. 3a-d), a 4D intensity and motion atlas (Fig. 3e-h)∗ and an animated static new 3D patient (Figs.

∗Demo movie of 4D intensity and motion atlas: https://goo.gl/Qog138

To appear in SPIE Medical Imaging: Image Processing 2018
p. 4

3691215182124273033363942454851545760Time [s]02004006008001000Volume [ml](a) 0 ml

(b) 349 ml

(c) 659 ml

(d) 1097 ml

(e) 0 ml

(f) 349 ml

(g) 659 ml

(h) 1097 ml

(i) 0 ml

(j) 349 ml

(k) 659 ml

(l) 1097 ml

Figure 3: 3D renderings of four relative breathing volume [ml] states with coronal clipping of the (a-d) animated
invididual patient (use case 1), (e-h) synthetic 4D intensity and motion atlas (use case 2) and (i-l) animated 3D
patient by the 4D motion atlas and a respiratory signal from the new 3D patient (use case 3). Cavities and skin
surfaces are rendered in beige.

To appear in SPIE Medical Imaging: Image Processing 2018
p. 5

(a) Liver

(b) Right lung
Figure 4: Inter-patient registration DICE results from a leave-one-out study for expert segmentation masks:
Inter-patient (vs. intra-patient) registration still poses problems as can be seen by sporadic outliers.

(c) Left lung

3i-l). Cross-validation DICE values of atlas registrations of liver and lung masks to a left-out static patient
yield overall median values of 0.87 (liver), 0.93 (right lung) and 0.93 (left lung). Sporadic bad inter-patient
registration outliers are possible in the current process as can be seen in Fig. 4a-c by the dots. Here by the
averaged motion, they cause artifacts at bony structures of the hip∗. Their inﬂuence to the process should be
mitigated by a selection strategy in future work. Statistical 4D breathing motion models for the lungs have been
introduced in.23 Here, we use a population of 4D patient image data sets and learn a 4D intensity and motion
atlas parametrized by a surrogate spirometry signal. The surrogate signal for the calculation of a mean motion
model in this work was simply found by averaging. Among other obvious applications, such motion model can
be warped to new 3D patient data. The animated atlas or the 3D patient can be used in a 4D VR training
system with promising outcomes.17 Despite the eﬃcient regression calculus, future work will cover the direct
joint transfer of the motion model coeﬃcients.

REFERENCES

1. J. McClelland, D. Hawkes, T. Schaeﬀter, and A. King, “Respiratory motion models: A review,” Medical

Image Analysis 17(1), pp. 19 – 42, 2013.

2. A. Mastmeyer, G. Pernelle, R. Ma, L. Barber, and T. Kapur, “Accurate model-based segmentation of
gynecologic brachytherapy catheter collections in mri-images,” Medical image analysis 42, pp. 173–188,
2017.

3. A. Mastmeyer, D. Fortmeier, and H. Handels, “Random forest classiﬁcation of large volume structures for
visuo-haptic rendering in CT images,” in Proc. SPIE Medical Imaging: Image Processing, 9784, pp. 97842H–
1–8, International Society for Optics and Photonics, 2016.

4. M. Meike, D. Fortmeier, A. Mastmeyer, and H. Handels, “Real-time resampling of medical images based
on deformed tetrahedral structures for needle insertion vr-simulation,” in German Conference on Medical
Image Processing - BVM 2015, pp. 443–448, Springer, 2015.

5. A. Mastmeyer, D. Fortmeier, and H. Handels, “Eﬃcient patient modeling for visuo-haptic VR simulation

using a generic patient atlas,” Comput Methods Programs Biomed 132, pp. 161–175, 2016.

6. A. Mastmeyer, D. Fortmeier, E. Maghsoudi, M. Simon, and H. Handels, “Patch-based label fusion using local
conﬁdence-measures and weak segmentations,” Proc. SPIE Medical Imaging: Image Processing , pp. 86691N–
1–11, 2013.

7. A. Mastmeyer, D. Fortmeier, and H. Handels, “Anisotropic diﬀusion for direct haptic volume rendering in
lumbar puncture simulation,” German Conference on Medical Image Processing - BVM 2012 , pp. 286–291,
Springer, 2012.

8. K. Engelke, V. Bousson, L. Duchemin, C. Fuchs, D. Mitton, A. Mastmeyer, J. Adams, W. Kalender,
W. Skalli, and J. Laredo, “Eﬀect-the european femur fracture study using ﬁnite element analysis and 3d
computed tomography.,” in Journal of Bone and Mineral Research, 21, p. S86, 2006.

To appear in SPIE Medical Imaging: Image Processing 2018
p. 6

Pat. 1Pat. 2Pat. 3Pat. 4Pat. 500.10.20.30.40.50.60.70.80.91Pat. 1Pat. 2Pat. 3Pat. 4Pat. 500.10.20.30.40.50.60.70.80.91Pat. 1Pat. 2Pat. 3Pat. 4Pat. 500.10.20.30.40.50.60.70.80.919. D. Fortmeier, A. Mastmeyer, J. Schröder, and H. Handels, “A virtual reality system for PTCD simulation
using direct visuo-haptic rendering of partially segmented image data,” IEEE J Biomed Health Inform 20(1),
pp. 355–366, 2016.

10. A. Mastmeyer, T. Hecht, D. Fortmeier, and H. Handels, “Ray-casting based evaluation framework for haptic
force-feedback during percutaneous transhepatic catheter drainage punctures,” Int J Comput Assist Radiol
Surg 9, pp. 421–431, 2014.

11. A. Mastmeyer, T. Hecht, D. Fortmeier, and H. Handels, “Ray-casting-based evaluation framework for needle
insertion force feedback algorithms.,” in German Conference on Medical Image Processing (Bildverarbeitung
für die Medizin), pp. 3–8, 2013.

12. D. Fortmeier, A. Mastmeyer, and H. Handels, “Image-based palpation simulation with soft tissue defor-
mations using chainmail on the GPU,” German Conference on Medical Image Processing - BVM 2013 ,
pp. 140–145, 2013.

13. D. Fortmeier, A. Mastmeyer, and H. Handels, “An image-based multiproxy palpation algorithm for patient-

speciﬁc VR-simulation,” Medicine Meets Virtual Reality 21, MMVR 2014 , pp. 107–113, 2014.

14. D. Fortmeier, A. Mastmeyer, and H. Handels, “Image-based soft tissue deformation algorithms for real-time

simulation of liver puncture,” Current Medical Imaging Reviews 9(2), pp. 154–165, 2013.

15. A. Mastmeyer, D. Fortmeier, and H. Handels, “Evaluation of direct haptic 4d volume rendering of partially

segmented data for liver puncture simulation,” Scientiﬁc Reports 7(1), p. 671, 2017.

16. A. Mastmeyer, M. Wilms, D. Fortmeier, J. Schröder, and H. Handels, “Real-time ultrasound simulation for
training of US-guided needle insertion in breathing virtual patients,” in Medicine Meets Virtual Reality 22,
MMVR 2016, Studies in Health Technology and Informatics 220, pp. 219–226, IOS Press, 2016.

17. D. Fortmeier, M. Wilms, A. Mastmeyer, and H. Handels, “Direct visuo-haptic 4D volume rendering using

respiratory motion models,” IEEE Trans Haptics 8(4), pp. 371–383, 2015.

18. D. Fortmeier, A. Mastmeyer, and H. Handels, “Optimized image-based soft tissue deformation algorithms
for visualization of haptic needle insertion,” in Medicine Meets Virtual Reality 20 - NextMed, MMVR 2013,
San Diego, California, USA, February 20-23, 2013, pp. 136–140, 2013.

19. D. Fortmeier, A. Mastmeyer, and H. Handels, “Gpu-based visualization of deformable volumetric soft-
tissue for real-time simulation of haptic needle insertion,” German Conference on Medical Image Processing
(Bildverarbeitung für die Medizin) , pp. 117–122, 2012.

20. A. Mastmeyer, D. Fortmeier, and H. Handels, “Direct haptic volume rendering in lumbar puncture simu-
lation,” in Medicine Meets Virtual Reality 19, MMVR 2012, Studies in Health Technology and Informatics
173, pp. 280–286, IOS Press, 2012.

21. Y. Seppenwoolde, H. Shirato, K. Kitamura, S. Shimizu, M. van Herk, J. V. Lebesque, and K. Miyasaka,
“Precise and real-time measurement of 3D tumor motion in lung due to breathing and heartbeat, measured
during radiotherapy,” Int J Radiation Oncololgy, Biology, Physics 53, pp. 822–834, Jul 2002.

22. A. Mastmeyer, M. Wilms, and H. Handels, “Interpatient respiratory motion model transfer for virtual reality
simulations of liver punctures,” Journal of World Society of Computer Graphics - J WSCG 25(1), pp. 1–10,
2017.

23. J. Ehrhardt, R. Werner, A. Schmidt-Richberg, and H. Handels, “Statistical modeling of 4D respiratory lung
motion using diﬀeomorphic image registration.,” IEEE Transactions on Medical Imaging 30, pp. 251–265,
Sept. 2011.

24. M. Wilms, R. Werner, T. Yamamoto, H. Handels, and J. Ehrhardt, “Subpopulation-based correspondence
modelling for improved respiratory motion estimation in the presence of inter-fraction motion variations,”
Physics in Medicine and Biology , 2017.

25. T. Hastie, R. Tibshirani, and J. Friedman, The elements of statistical learning 2nd edition, New York:

Springer, 2009.

26. M. Wilms, J. Ehrhardt, R. Werner, M. Marx, and H. Handels, “Statistical analysis of surrogate signals to
incorporate respiratory motion variability into radiotherapy treatment planning,” in SPIE Medical Imaging
2014, Image-Guided Procedures, Robotic Interventions, and Modeling, Z. Yaniv and D. Holmes III, eds.,
9036, p. 90360J, (San Diego, USA), 2014.

27. A. Schmidt-Richberg, R. Werner, H. Handels, and J. Ehrhardt, “Estimation of slipping organ motion by

registration with direction-dependent regularization,” Medical Image Analysis 16(1), pp. 150 – 159, 2012.

To appear in SPIE Medical Imaging: Image Processing 2018
p. 7

