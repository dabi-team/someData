To appear in IEEE Transactions on Visualization and Computer Graphics

Animals in Virtual Environments

Hemal Naik, Renaud Bastien, Nassir Navab and Iain D Couzin

0
2
0
2

n
a
J

2

]

R
G
.
s
c
[

2
v
3
6
7
2
1
.
2
1
9
1
:
v
i
X
r
a

Fig. 1. Animals in Virtual Environments: (left) Tethered fruit ﬂy (credit : Simon Gingins), (center) Locust on a spherical treadmill
(credit : Centre for the Advanced Study of Collective Behaviour, Konstanz), (right) Praying mantis with glasses [62] (credit : Newcastle
University, UK)

Abstract— The core idea in an XR (VR/MR/AR) application is to digitally stimulate one or more sensory systems (e.g. visual,
auditory, olfactory) of the human user in an interactive way to achieve an immersive experience. Since the early 2000s biologists have
been using Virtual Environments (VE) to investigate the mechanisms of behavior in non-human animals including insects, ﬁsh, and
mammals. VEs have become reliable tools for studying vision, cognition, and sensory-motor control in animals. In turn, the knowledge
gained from studying such behaviors can be harnessed by researchers designing biologically inspired robots, smart sensors, and
multi-agent artiﬁcial intelligence. VE for animals is becoming a widely used application of XR technology but such applications have not
previously been reported in the technical literature related to XR. Biologists and computer scientists can beneﬁt greatly from deepening
interdisciplinary research in this emerging ﬁeld and together we can develop new methods for conducting fundamental research in
behavioral sciences and engineering. To support our argument we present this review which provides an overview of animal behavior
experiments conducted in virtual environments.

Index Terms—animal behavior, VR for animals, mechanism of behavior, interactive experiments, closed-loop

1 INTRODUCTION

A wide range of scientiﬁc disciplines use animals as a primary subjects
of study e.g. medicine, neurobiology, physiology. Ethology, the ﬁeld of
animal behavior, is largely concerned with understanding why animals
do what they do, and how. Animals exhibit behavioral strategies that
have evolved to enhance its survival in the natural environment (land,
air or underwater). Each animal’s behavioral interactions with its
own environment, and other organisms, reveals important information
about ecology and evolution. Humans have studied animal behavior
for hundreds of years including during domestication. In 1963, Niko
Tinbergen suggested the ﬁrst framework for studying behavior in form
of four fundamental questions [81]; What is the survival value of the
behavior? How does the behavior develop during the lifetime of the
animal? How did the behavior evolve across generations? And how
does it work (mechanism)? His objective was to propose a framework
that deﬁnes the scope of the scientiﬁc study of behavior. It is widely
accepted among behavior researchers that complete understanding of
behavior can be obtained from following Tinbergen’s framework [5,81].
Different aspects of animal behavior have been studied over the
last 60 years. In neuroscience, neural activity of behaving animals
is recorded to ﬁnd the link between sensory-motor mechanisms and
neural processing [72]. The genetic basis of behavior can be studied
by observing behavior in genetically manipulated animals (such as
mutants). In medicine, small vertebrates (ﬁsh or mice) are preferred
because they exhibit some fundamental behavioral traits that are con-

• Hemal Naik, Renaud Bastien and Iain Couzin are with Max Planck Institute

of Animal Behavior, Konstanz and Centre for the Advanced Study of
Collective Behaviour, University of Konstanz and Department of Biology,
University of Konstanz, E-mail: hnaik,rbastien,icouzin@ab.mpg.de.
• Hemal Naik and Nassir Navab are with Technische Universit¨at M¨unchen

E-mail: nassir.navab@tum.de.

sistent with other vertebrates, including humans. Their behavior can be
closely monitored during experimental drug trials to study the progres-
sion of the disease and to measure the resulting effect on the animal’s
behavior [22]. Revealing the behavioral strategies of the animals are
useful for solving problems in the ﬁelds of engineering and technol-
ogy. For example, behavior of animals has been studied for various
applications in robotics [2, 43, 77]. Biologists and engineers have ben-
eﬁted greatly by working together on novel interdisciplinary projects
where robots are used to investigate the principles of decision making
in animals [47, 50, 84](details in Sec 6).

Animal behavior is studied using various experimental methods. Be-
havior is studied in both indoor (lab, cage) and outdoor (wild, open
area) environments depending on the research questions. Outdoor
environments are more suitable for the observation of realistic behav-
ioral patterns. However, experiments in natural environments can be
time-consuming and expensive. Outdoor experiments are also prone to
unplanned disturbances from external factors which may alter the behav-
ior during the experiment e.g. weather conditions, human disturbances.
Indoor environments provide more control over the experimental con-
ditions and minimize the inﬂuence of external factors. It is thus easier
to develop standardized and repeatable methods for such behavioral
experiments. Indoor environments are suitable for carrying out detailed
studies, but the range of behaviors displayed in such environments
may be limited. Many wild animals do not exhibit natural behaviors in
indoor environments. Often some species, termed as model species, are
studied in more detail than others because they are selected based on
their ability to perform natural behaviors in indoor environments e.g.
zebra ﬁsh, fruit ﬂy etc.

Artiﬁcial sensory stimuli are often used in experiments to invoke
behavioral responses from animals. In natural conditions, animals con-
stantly receive sensory stimulus (visual, auditory, haptic etc.) from
their environment and must react to it appropriately. Stimuli are often

1

 
 
 
 
 
 
designed artiﬁcially to mimic natural conditions. For example, tem-
perature and light manipulation is sufﬁcient to artiﬁcially simulate day
and night cycle for insects and birds. Niko Tinbergen used cardboard
models of adult gulls to invoke begging behavior in gull chicks [82].
Artiﬁcial stimulation is a powerful technique for achieving repeatable
behavioral observations [12]. The experimenter can plan the timing
of stimulus delivery and change properties of the stimulus between
different trials to observe changes in behavior. Such experiments pro-
vide a deeper understanding of the decision-making of animals. After
Tinbergen’s initial ﬁndings, more advanced techniques were developed
to stimulate sensory systems of different animals for behavioral ex-
periments. Technological innovations such as cameras, speakers and
projectors have made a major impact in behavioral studies. They are
used in novel ways to manipulate the information received by the animal
about its surroundings environment (e.g. audio or visual stimulation).
In the late 90s, researchers studying human behavior, psychology,
and perception started exploring the Virtual Environments (VE) as
a tool for manipulating the human perception of reality by artiﬁcial
stimulation of human sensing [76]. The concept of CAVE VR [16] was
introduced with the idea of creating an immersive experience for the
viewer by means of visual stimulation. The viewer enters a room where
head position is tracked and the stimuli are rendered on the walls from
the perspective of the viewer. Two dimensional ﬁgures can appear as
three dimensional objects when presented from a speciﬁc perspective,
an illusion often exploited by graphic artists e.g. M.C.Escher [29].
The CAVE VR was able to create and maintain the illusion in real-
time. Around the same time biologists had shown that the method of
displaying virtual stimuli on a screen was useful for studying behavior
but it was limited due to lack of interactivity. Biologists started adopting
the CAVE VR design and introduced the concept of interactive virtual
environments for animals, almost two decades ago [36, 69]. Their
goal was to design a novel experimental approach where the animal
behaves as if freely navigating in its natural environment. Since then
many techniques have been developed for studying behavior of freely
moving animals (ﬁsh, mammals and insects) in the virtual environment
e.g. FreemoVR [73], FlyVR [72]. Animal VR systems can thus be
considered as a cleverly modiﬁed version of human VR systems.

The technology used for designing VE for animals is similar to that
used for designing XR applications for human users. However, the
sensory perception of animals is different to that of humans which
means that they may sense the environment in a different manner e.g.
UV vision in birds, ultrasonic hearing in bats. The methods developed
for stimulating humans may therefore only be suitable for some ani-
mals. Virtual environments for animals are limited by our ability to
produce the sensory stimuli that matches the animal’s sensory input.
The research in this ﬁeld has shown that it is possible to circumvent
some limitations by exploring new technological solutions i.e. real-time
tracking, realistic graphic rendering. This requires stronger research
collaborations between biologists and the technology developers from
the XR community. The goal of this paper is to introduce the XR
community to the research done in the ﬁeld of animal behavior using ar-
tiﬁcial visual stimuli, especially in virtual environments. In this review
paper, we trace the journey of stimulus-based behavior experiments
from simple non-interactive models to fully interactive VR systems.

2 REVIEW METHOD AND STRUCTURE

The scientiﬁc literature for this review is collected from different re-
search domains associated with the study of animal behavior e.g. ethol-
ogy, neurology, psychology. We learned that virtual stimuli have been
used extensively in behavioral experiments; therefore, we followed a
top-down approach to collect the relevant material. We started from
review papers that focused on virtual environments and virtual stimuli
for animals [8, 24, 27, 52, 72, 80, 86]. Most of this literature is informa-
tive and extensive but it is prepared for readers with backgrounds in
biology and behavioral experiments. Existing review papers discuss
the experiments with speciﬁc focus on an application (e.g. neurobi-
ology [8, 27]) or a type of animal (e.g. rodents [80]). The scope of
such a review is restricted by the topic and the methods used for other
applications or species are not reported. Our review provides a more

general overview of the methods used to create virtual environments
for animals. We also studied papers with a strong emphasis on the limi-
tations of using virtual stimuli with animals [13, 72, 80] and collected
literature on non-interactive methods for artiﬁcial stimulation. These
methods are commonly used for studying behavior and their success is
the strongest argument in favor of developing virtual environments for
studying behavior. Overall our review is designed to serve as a guide
for readers of the computer science community (especially XR) who
wish to later explore more detailed literature in the ﬁeld of behavior
studies. This survey will mainly cover experiments using visual stimuli
since these represent the largest and most diverse body of work to date
(summary of papers in Table 1 and Table 2). VEs for other sensory
modalities (e.g. olfactory, audio) are relatively few and are mentioned
for the sake of completeness.

Structure: Our ﬁrst focus will be on experiments explaining a
non-interactive (open-loop) approach. We start with detailing different
categories of artiﬁcial visual stimuli in the same order as they were intro-
duced in the ﬁeld of behavioral studies (Sec 3). In this way we are able
to begin with older experiments which introduced the method of visual
stimulation and point out gradual rise in usage of technology in behav-
ioral experiments. Then we switch our focus to experiments which
use an interactive (closed-loop) approach to display different types
of artiﬁcial visual stimuli, mainly virtual environments. We present
these experiments in three categories which are based on the novelty
of their approach, namely mechanical, hybrid and digital (Sec. 4). The
advantages and disadvantages of each approach are mentioned with
suitable examples. Following this we discuss the limitations faced by
biologists while designing virtual environments for animals (Sec 5).
The technological shortcomings are pointed out to attract the atten-
tion of the readers from the computer science community who could
contribute important future developments. Presently behavioral experi-
ments with VEs have reached a stage where support from technology
experts is required to design novel techniques for studying complex
behavior patterns. To support this claim we not only present the current
state of the art, but also propose some futuristic ideas for behavioral
experiments which can be realized by using techniques from computer
vision, XR community (Sec.6).

3 ARTIFICIAL STIMULI AND OPEN-LOOP EXPERIMENTS

In this section we aim to introduce the readers with a brief history of
stimulus design and open-loop experiments. This section is impor-
tant for readers from non-biological backgrounds to understand how
biologists came to use virtual environments for studying behavior of
animals. First we explain fundamental ideas behind using artiﬁcial
sensory stimuli and then relate these ideas to the framework of studying
behavior [81]. We cover the four different categories of visual stim-
uli that are commonly used for behavioral experiments: static stimuli,
abstract stimuli, video stimuli and virtual stimuli. Each category repre-
sents one or more types of visual stimuli which share some common
properties in terms of design and/or the method used to present them.
We describe the intuition behind designing each type and with suitable
examples we show how these stimulus were implemented. In the end
the knowledge gained from these experiments is summarised.

In these studies, researchers wanted to design an efﬁcient method
to provoke meaningful behavioral response for scientiﬁc analysis. Ev-
ery novel idea of visual stimulation was ﬁrst tested with an open-loop
(non-interactive) approach. In such an approach the stimulus does not
change or react to the animal but the method is sufﬁcient to check the
feasibility of using a stimuli and to learn the right technique for display-
ing it. Experiments with the open-loop approach are still being used
in behavioral studies and they have provided much of the fundamen-
tal understanding required to build the modern closed-loop behavior
experiments.

3.1 Static stimuli

Static stimuli were some of the ﬁrst employed in behavioral experi-
ments. Here animals are presented with a static object such as a model
or an image, usually of an animal (ﬁgure 2A). We refer to this type

2

To appear in IEEE Transactions on Visualization and Computer Graphics

Table 1. Overview of artiﬁcial visual stimuli used in open loop experiments

Type

Static

Abstract

Video

Virtual

Stimulus

Model, Image, Color
Filter, Paint
conspeciﬁcs

Patterns with points,
lines, circles

Animal-Behavior

Key Attributes

Birds - Feeding [82], Vigilance [30], Mate choice [6], social
hierarchy [68]. Fish - Mate choice [56]

Conﬁgurable properties: shape, size etc.,
reusable method, Non-interactive

Fruit Fly - Perception and Navigation [69], Movement of eye [49],
Motion Control [32], Trajectory correction [79]. Locust - Motion
Parallax [70], Insect Locomotion [77]. Moth - Navigation [36].
Mice - Optomotor Response [1]

Setup can be mechanical design with pattern
cylinder or with screens, popular for studying
vision induced motion e.g. OMR,
OKR [1, 48, 49]

Video recording
displayed through
screen or projector

Lizard - Courtship [44], Communication [64] Jumping spider -
Recognition [14], Birds - Alarm calling [30], Fish - Laterality and
Cooperation [7]. Review - [19, 60].

Computer generated
content through
projector or screen

Fish - Mate preference [4, 46], Predator Response [34, 42],
Communication [39]. Review - [11, 86]

Stimulus can be edited and customized,
reusable setup for multiple behavior
experiments, can display abstract stimuli,
non-interactive.
Stimulus programmable, reusable setup,
semi-interactive or rule based interaction, can
display abstract stimulus.

as visual stimuli [45, 47, 50, 84], but to maintain focus on virtual visual
stimuli we do not cover them in this paper.

Static stimuli have proved to be reliable and repeatable means for
conducting behavioral experiments. The main advantage is that the
same stimulus could be used for different animals and its visual proper-
ties could be modiﬁed between trials. Moreover, the timing of stimulus
delivery and frequency of displaying the stimulus could also be con-
trolled [19]. A major limitation to this approach is that there is no
scope for feedback between the stimuli and the organisms. Conse-
quently a problem that arises is that individuals can habituate to, and
stop responding to, stimuli over time [19].

3.2 Abstract stimuli

The primary intuition for the design of abstract stimuli was to design a
stimulus that is minimalistic yet sufﬁcient to drive behavioral decision-
making process in animals (such as movement). These are the most
widely used stimuli for behavior experiments, especially for studying
mechanisms related to visually-induced locomotion. For example, a
common abstract stimulus consists of simple patterns designed from
primitive geometric shapes such as points and lines (see ﬁgure 2B) e.g.
stripes or circles. The idea is to measure the movement of the animal
in response to the patterns displayed to it. The mapping between the
features observed by the animal and the animal’s movements reveal
the underlying process of behavioral decision making in the context
of such stimuli. This experimental concept is designed to investigate
fundamental questions regarding the behavioral and neural basis of
visually-induced locomotion. Small invertebrates, such as fruit ﬂies
or honey bees, and relatively simple vertebrates such as ﬁsh, are the
preferred model species as they possess less complex nervous systems
and relatively fewer behavior patterns.

One example of visually-induced locomotion behavior is the op-
tomotor response (OMR), which is the property of moving the body
and/or head in concert with the features in the environment for image
stabilization [48, 49]. Similarly, the property of moving the eyes in
concert moving features is called optokinetic response (OKR). One of
the ﬁrst experimental setups for studying OMR and OKR was a me-
chanically controlled stimulus delivery system. It consists of a stage for
placing the animal and a cylindrical drum surrounding the stage. The
cylinder is rotated along its axis using a motor and its inner walls are
painted with abstract patterns (stimulus) cf. ﬁgure 2B. The movements
of the animals are restricted to a small area and sometimes tethering
is used to keep the animals ﬁxed in one spot which simpliﬁed mea-
surement of head or eye movements. The movement is recorded using
video cameras or using a simple array of photodiodes [36] or torque
motor [33]. It is shown that the animals typically display a tendency to
move in the direction of the rotation. The width of the striped pattern,
the rotation speed of the cylinder, and the direction of the rotation are
also inﬂuential in decision making [1].

Another example of visually induced motion is avoidance of or

Fig. 2. Common type of visual stimuli used in open-loop behavior ex-
periments. A. Static stimuli: An experimental setup designed to study
preference of the ﬁsh in the test tank when given a choice between model
ﬁsh and real ﬁsh, B. Abstract stimuli: Common type of patterns used
to study vision induced motion and pictorial depiction of mechanically
controlled Pattern Cylinder (PC) with striped patterns, C. Video stimuli:
Experimental setup for preference study with video playback experiment,
similar to ﬁgure A., the screen is used to display videos ﬁsh as shown
in example image, D. Virtual stimuli: Example of an image displayed as
virtual stimulus. It consists of some ﬁsh around a coral, all of which are
separately designed graphical models (left) combined with an underwa-
ter background. This stimulus is displayed using a screen similar to the
setup shown in ﬁgure C.

as static because the properties of stimuli do not alter or change dur-
ing the experiment. It was hypothesized that animals may perceive a
static model as a real animal and react to it. It was shown that in some
cases such stimuli were sufﬁcient to invoke a response such as fear or
attraction. Tinbergen and Perdeck [82] used a model of a bird to invoke
begging behavior in chicks of gulls. The chicks responded naturally to
the models i.e. as if begging for food from a parent. Following such
studies, other researchers tested artiﬁcial objects extensively using dif-
ferent variants, which differed both in terms of visual properties of the
model (e.g. different colors, shapes), and timing of stimulus delivery
(e.g. time of the day or frequency) [19]. Evans and Marler [30] studied
alarm behavior in chickens using a model of a predator. They created a
setup where a model of a predatory bird would move above the cage on
a rope. This simulated a typical behavior of a predatory bird gliding
in the sky looking for food. It was observed that chickens made alarm
calls when the model was moving over the cage. Images, photographs
and slides were also used as static stimuli [19]. Other examples of
static stimuli are environmental modiﬁcations e.g. light ﬁlters, which
allow or reject of speciﬁc wavelengths [6, 56], and visual modiﬁcation
e.g. painting conspeciﬁcs [68]. Robotic animals are increasingly used

3

B.D. A.Looming Stimulus Optomotor Stimulus Video StimulusTest TankHoldingTankHolding TankModel FishReal FishReal FishHolding TankScreenVirtual StimulusC. ﬂight response to, a rapidly-expanding (or looming) shape on the retina,
typically a black expanding circle with a white background as shown
in ﬁgure 2B. In early designs, looming stimuli were simulated by
mechanically moving a dark circular cardboard cutout closer to the
animal. Electronically controlled display methods (e.g. LED grid,
LCD) have replaced the mechanical methods for displaying the stimu-
lus and computer vision techniques are now deployed for movement
measurements [69, 74]. It is also possible to conduct abstract stimuli-
based experiments in a fully automated closed-loop manner [31, 32, 72]
(covered later in virtual stimuli).

Experiments with abstract stimuli are relatively easy to design and
their results tend to be reproducible and the method has opened doors
for reverse-engineering the process of visually induced motion. Ab-
stract stimuli have been successfully used for more than 70 years for
research studies by biologists and engineers alike. Some examples
include studying the vision properties such as depth perception [69],
motion parallax [70], movement of the eye [49], physiology of the
eye [9], locomotion mechanisms such as ﬂight behavior [77] and mo-
tion control [32], and behavioral patterns like navigation [36, 69], and
trajectory correction [79]. The experiments have been conducted ex-
haustively with small insects (e.g. fruit ﬂies, bees) and vertebrates (e.g.
zebraﬁsh, mice). Mechanical designs have suffered from mechanical
limitations and the ability to measure movement. They also require
manual intervention for changing patterns or other parameters, which
not suitable for conducting high-throughput experiments.

3.3 Video stimuli

This category includes experiments in which recorded video footage is
used as visual stimuli. These experiments are known as video playback
experiments in the behavior literature. For experiments investigating
social behavior video cameras are typically used to record the activity
of an animal and this is then used as a stimulus during the experiment
[19, 60]. The focal animal is usually placed in an enclosure or an arena
and is shown a video sequence through a screen or projector placed
at a reasonable distance (ﬁgure 2 C). The responses of this animal is
recorded using a video camera, which are later used to map behavioral
decisions of the animals to the visuals presented in the stimuli. The
stimuli may involve another animal of the same species (conspeciﬁc)
or a different species (heterospeciﬁc) behaving in a certain way. It was
hypothesized that animals do not comprehend the concept of video
screens and thus will react in a natural and instinctive manner to the
presented stimuli. The intuition behind this method is to simulate the
natural environment in the photo realistic way [19].

Jenssen [44] designed a mate-choice experiment with female lizards
using video playback method. He displayed video sequences of male
lizards performing courtship display and reported that female lizards
did react appropriately to such stimuli. The video playback technique is
commonly used method for studying a wide range of behavior responses
such as: aggression, attraction, fear etc, in species including arachnids
[14], birds [30], reptiles [64], and ﬁshes [7]. It is a reliable technique
for quantitative analysis of behavior. The movements are measured in
2D or 3D space using multiple cameras [66]. Playback methods have
provided many insights into the questions related to the survival value
of a speciﬁc behavior.

Video stimuli have some clear advantages because a customized
sequence of behaviors can be shown. The same setup can be used to
display wide range of behaviors; for example the setup in ﬁgure 2C
can be used for studying either mate preference or aggression. Camera
and display technologies required for the experiment are typically com-
mercially available which makes this method accessible to researchers.
Various software tools are designed to quantify behavioral response
from the video sequences [21]. Detailed behavioral studies are possible
because the entire stimulus sequence is known and it could be mapped
to the response of the animal in an ofﬂine manner.

Video playbacks methods also have many disadvantages. They often
assume that the animal is reacting to the stimulus and perceive the
stimulus as being real. This assumption may not always hold. Stimuli
are customized yet they are mostly pre-programmed sequences [19].
The animal in the videos do not interact fully with the real animal

which may lead to habituation or unnatural reaction. Other limitations
are introduced with the use of display and video technology. The
sampling rate of the camera used to create the stimulus must match
the physiological visual properties of the animal otherwise the motion
in the video may appear blurry, discolored or distorted to the animal.
Technical speciﬁcation of the display screen or projector must also be
considered to avoid similar problems e.g. resolution of the display,
refresh rate of screens, etc. There are some common limitations shared
by all screen-based methods of visual stimulation which are covered in
detail later in section 5. Video playback methods are limited to those
behaviors which are possible to record. Further details on feasibility of
using video stimuli can be found in the review paper of D’eath [19]. In
summary, video playback methods made a strong case in favor of using
technology provided the researcher considers carefully its limitations
and makes reasonable assumptions.

3.4 Virtual stimuli

Virtual stimuli are the most advanced category of artiﬁcial visual stim-
uli. The setup is more or less similar to video playback methods but the
content of the stimulus is created virtually i.e. using computer graph-
ics and animation technology. The initial motivation for using virtual
methods was to increase immersion and remove limitations of earlier
methods. In a computer-generated stimuli, the user can conﬁgure ﬁne
details, which is not typically possible with raw video stimuli (see
ﬁgure 2D). All components of the stimuli are programmable and can
be modiﬁed independently i.e. the shape, size, color, background, and
movements of the animals can be individually changed for each experi-
ment. Graphic design and rendering are performed using techniques
developed by the video game and animation industry.

Virtual stimuli are considered a major improvement over other stim-
ulus types. The stimuli are faster to design and modify which means
multiple experiments can be conducted with different variants. Video
playback and abstract stimuli-based experiments were also performed
with virtual stimuli for cross-validation [86]. Virtual stimuli were pro-
posed as an alternative to other screen-based methods and this method
has been widely adopted for studying different behavior patterns (ﬁsh)
e.g. mate preference [4, 46], predator response [34], and visual com-
munication [39]. New methods have been employed to create realistic
animals and scenes. Ioannou et al. [42] projected small dots onto a
surface of a ﬁsh tank to simulate the movement of very small prey.
The ﬁshes attacked the projections as if they were real prey, which
helped the authors to understand the hunting strategy of the ﬁsh, as
well as to conduct artiﬁcial evolution of the prey. anyFish [83] and
FishSim [58] are software packages to simulate 2D projection of a
3D animated ﬁsh. Joysticks are employed to deﬁne the motion of the
virtual animal in semi-interactive manner, or to introduce perturbations
in the stimuli [52, 58]. Abstract stimuli-based experiments beneﬁted
signiﬁcantly from digitization. Software packages have been designed
to automate the workﬂow i.e. stimulus delivery, behavior measurement
(locomotion) and data analysis. Fry et al. [32] designed fully automated
setup for open-loop experiments with abstract patterns. They used an
optical tracking method for computing 3D trajectory of a freely ﬂying
fruit ﬂy in real-time. Most importantly, the combination of virtual stim-
uli and real-time tracking methods provides an opportunity to design
closed-loop experiments. We cover closed-loop methods with virtual
stimuli separately in the next section.

Virtual stimuli-based methods have many advantages when com-
pared with previously described categories. The modular approach of
software is an advantage as it allows different modules related to dis-
play, rendering or measurement (tracking) to be changed as and when
the new versions are developed. The software is easier to distribute
and share with other scientists in the community. The stimulus itself
can be programmatically controlled which was not the case with any
other method. However, display technologies are usually made for the
human visual system and may not be sufﬁcient to reproduce a realistic
view of the animals i.e.
they may lack sufﬁcient spatial resolution,
spectral resolution. Technological problems are inherent in virtual
stimuli-based methods and they are discussed in detail with limitations
of the closed-loop experiments. The open-loop experiments with virtual

4

To appear in IEEE Transactions on Visualization and Computer Graphics

stimuli are restrictive as they lack interaction. For that reason repeated
trials with the same animals were not advised as they got used to the
stimuli [42]. Butkowski et al. [11] and Woo and Rieucau [86] reviewed
use of animation for open-loop behavior experiments.

4 CLOSED LOOP EXPERIMENTS AND VIRTUAL ENVIRON-

MENTS

In this section, we focus on behavioral experiments using artiﬁcial
visual stimuli in a closed-loop. When navigating in a three-dimensional
environment, the features visible to the eye change appropriately with
perspective and movement of the viewer. The fundamental idea of
a closed-loop experiment is to constantly update the visual stimuli
according to the movement of the animal. This design has two ma-
jor components, tracking and stimulus delivery.
It is necessary to
synchronize these two components in real-time for a realistic appear-
ance. Real-time tracking and perspective correction for a freely moving
animal is a difﬁcult problem. Over the past two decades, different
techniques have been developed to circumvent this problem mostly by
restricting the movement of the animal. Behavioral experiments with
virtual stimuli are often referred to as Virtual Environments (VE) or
Virtual Reality (VR) interchangeably in the behavior literature. For
the sake of clarity, we will use the term VE generally for closed-loop
experiments with virtual stimuli. We reserve the term VR (as a subset
of VE) speciﬁcally for closed-loop experiments where the position of
the animal is tracked or the animal is maintained stationary to render
the stimuli in a perspective correct manner. Based on the design of the
experimental setup we have divided closed-loop experiments into three
categories: mechanical design, hybrid design and digital design.

The idea of VE for animals is largely inspired from the CAVE
VR [16] setup designed for humans. The setups designed for animals
are similar and have remained so from last two decades. An enclo-
sure is designed and the animal is placed on the stage or a platform.
The platform is surrounded by a screen often toroidal or cylindrical,
preferably matching the ﬁeld of view of the animal, for displaying the
stimulus e.g. ﬁgure 3A [80]. VEs are used for numerous behavioral
experiments but the experiments mentioned in this section are selected
based on the novelty of the approach. The focus is to highlight critical
improvements in behavior experiments using virtual environments. We
show that many of these improvements are largely dependent on the
methods developed for computer vision and XR applications.

4.1 Mechanical design: Restricted animal in ﬁxed (non-

virtual) environment

One of the earliest designs of a closed-loop experiment was a mechani-
cally designed ﬂight simulator for insects. The setup is similar to the
rotating pattern cylinder design explained earlier in section 3.2. In
the ﬂight simulator the rotation of the pattern cylinder is coupled to
the motion of the insect via a torque motor. This way the motion of
the insect triggers the rotation of the cylinder in its visual ﬁeld, which
emulates a real-life ﬂight conditions. Dill et al. [23] used a this setup
to study visual pattern recognition in fruit ﬂies and showed that ﬂies
could remember patterns based on how they appear on the retina from
a speciﬁc perspective. The ﬂy was tethered and its head rotation was
immobilized to restrict the movement of head independent of thorax.
The turning response was recorded by measuring the movement of
thorax. Often head ﬁxation is used to force the insect to turn its body
instead of the head. Tethering is used to control the sensory experience
and it simpliﬁes the tracking problem by restricting the movement of
the animal and allows experimental recordings to be made during the
experiment.

A treadmill with a styrofoam ball is another variation of a mechani-
cally designed closed-loop experiment. In this case, a tethered animal
is placed on the ball and walking motion of the animal rotates the
ball which in turn rotates the pattern cylinder. The rotation of ball is
converted to electronic signals which serve as input to servo motor
for rotating the cylinder. The ball is painted with a pattern of infrared
reﬂective dots. An infrared LED is placed near the ball and its reﬂection
is picked up by a photodiode which further decomposed the rotation
and translation using sequence detector logic. The mapping between

Fig. 3. Examples of different type of VR systems (credit : cf. Thurley
and Ayaz [80]). Figure A-F show different techniques used for ﬁxation of
animals, recording of movement and display of stimulus. Details covered
in text.

the animal’s movements and the visual pattern is stored in the computer
for further analysis. B¨ulthhoff [10] used this setup to study the genetic
link between vision and motion perception in fruit ﬂies. He used genet-
ically modiﬁed ﬂies (mutants) and wild fruit ﬂies in the ﬂight simulator
to perform a navigation task. He showed that mutants showed defect
in visual orientation and therefore concluded that optomotor response
may be encoded in genetic experession of the animal.

Mechanically designed closed-loop experiments were mainly used to
study visually induced motion with abstract stimuli. These experiments
did demonstrate that animals show a preference for some patterns and
actively move towards their preferred pattern. However, the patterns
remained unchanged or ﬁxed during the experiments which was consid-
ered a major limitation of this approach. This limitation is alleviated in
the modern closed-loop designs which use projectors instead of pattern
cylinders. The concepts of tethering and treadmills in the modern VE
setups are adopted from the ﬂight simulator experiments.

4.2 Hybrid design: Restricted animals in VE

Closed-loop experiments with the hybrid design were motivated by the
success of virtual stimuli in open-loop experiments. In hybrid designs,
virtual stimuli are displayed using screens or projectors (instead of
rotating pattern cylinder), and treadmills and/or tethering techniques
are used to restrict the animals movements and to simplify the problem
of tracking (cf. ﬁgure 3). In the late 90s, the behavior researchers
started the development of closed-loop experiments with virtual stimuli.
It was easier to conﬁgure virtual stimuli to show desired patterns and
the appropriate display technologies started becoming commercially
available at the time. Treadmills based techniques were readily avail-
able and useful for precise perspective correction while rendering the
stimuli on the screen. These experiments are often referred to as the
ﬁrst experiments which put animals in Virtual Environments. VE al-
lowed researchers to try different types of stimuli which extended the
scope of research to other behaviors in the three-dimensional world i.e.
navigation, foraging, etc.

Schuster et al. [69] designed one of the ﬁrst experiments with
fruit ﬂies in VE. The ﬂy was placed on a stage surrounded by a
360°panoramic LED screen which displayed the stimulus pattern. The
2D movement of a walking ﬂy was measured in the X-Y plane using a
simple computer vision technique of blob detection. The ﬂy was teth-
ered and its wings were clipped to restrict its movement to a plane. The

5

TreadmillCameraServo-motorServoballTreadmillFree rotationTreadmillTreadmillBEProjectorCamerasCAVECDFProjectorTreadmillProjectionscreenAirPCRewardunitMirrorsMovementsensorA270º or 360ºAbove viewType

Design

Feedback method

Animal-Behavior

Key Attributes

Table 2. Overview of artiﬁcial visual stimuli used in closed loop experiments

Mechanical

Arena, Pattern
cylinder

Torque meter,
Treadmill

Fruit Fly - Pattern recognition [23], Motion
perception [10]

Hybrid

Arena, LED
Screen, Projection

Digital

Arena, Projection

Optical Sensor,
Photodiode,
Optical Tracking,
Treadmill

Optical Tracking,
Active treadmill

Fruit ﬂy - Depth Perception [69], Moth -
Neurophysiology [36], Rodent - Navigation [53], Neural
activities [25, 26, 28, 38], Review - Neuroscience [8, 27],
Primate Cognition [24], VR for animals [72], Rodent
- [80]

Motion based rotation of pattern
cylinder, features not conﬁgurable.

Animals are restricted or tethered,
fully interactive, built with open
source software frameworks, setup
conﬁgurable for multiple species.

Fruit ﬂy - Real-time 3D tracking [32], vision induced
motion [72], Flight pattern [73] Spider -
Navigation [64, 72], Ant - Foraging [18], Fish - Social
behavior [73], Rodent- [20, 73], Review -
Neuroscience [8, 27], VR for animals [72], Rodent - [80]

Free moving animals, real time
perspective correction, underwater
projection, arbitrary surfaced arena,
support for multiple species,
conﬁgurable software.

authors claimed that they were able to study depth perception in fruit
ﬂies with the system which was not possible in previously designed
open-loop methods.

Another notable approach is from Gray et al. [36], where they de-
signed a VE to measure neurophysiological activity in moths while
foraging. Moths navigate in a complex 3D environment to ﬁnd the
source of odor and the authors simulated similar conditions in VE by
designing a multisensory stimulation (visual, olfactory and mechanosen-
sory) mechanism. A wind tunnel was placed in front of the moth for
olfactory and mechanosensory stimulus. A 3D scene of a textured
ﬂoor and vertical pillars was generated using computer game engine
(Descent III). The moth was tethered and multichannel neural recording
was obtained by probing the ventral nerve cord of the moth. It was
assumed that ﬂight is at a constant altitude (ﬁxed Z) and navigation
was allowed for in the simulated horizontal plane. The movement of
the moth’s abdomen was measured using optical sensors; an Infrared
(IR) light source and photodiode array. It was shown that the moth
navigated in the virtual space by turning towards the odor emanating
from the wind tunnel. The authors demonstrated effectiveness of virtual
ﬂight simulator by showing that the turning sequence in VE was con-
sistent with ﬁndings of optomotor response observed with freely ﬂying
moths. Generally, ﬁxation of the animal is considered a limitation of
this approach. The researchers studying the neural link between stimuli
and behavior preferred ﬁxation of animals to be able to measure the
neural activity in a reliable manner [72, 80].

Numerous variations of treadmill-based designs have been used to
study the movement of rodents in virtual environments (cf. ﬁgure
3). Each technique has imposed different degrees of constraint on the
movement of the animal e.g. body ﬁxation, head ﬁxation, etc. H¨olscher
et al. [40] designed the ﬁrst VE setup for rodents, similar to ﬁgure
3A. The movement of the treadmill was restricted to the horizontal
axis, the body of the animal was ﬁxed but the head position was not.
Rotation of the treadmill was computed using optical sensors, similar to
tracking the ball in the computer mouse. Visual stimuli were projected
using a DMD projector on a screen, via two reﬂective mirrors to cover
wide ﬁeld of view (360° azimuth and -20°to +60° elevation). The
VE contained cylinders hanging downwards from the ceiling and no
features on the ﬂoor. This design was meant to avoid giving any tactile
feedback that the rat may expect from visual cues. OpenGLPerformer
was used to generate graphics with support of NVIDIA graphic card
for real-time rendering. The stimulus was presented at a ﬁxed distance
and stereoscopic depth cues were not considered. They showed that
rodents could be trained to navigate in a virtual 3D environment using
2D stimuli, which had been shown in primates and humans. The rats
were trained for a real maze navigation task to compare their ability
to learn in the real world and in VE. It was shown that they learned
to operate the treadmill to navigate ”closer” to the objective in VE to
earn a food reward. They got better with the number of attempts and
consistently minimized the distance to, and thus the time taken to reach,
the reward. This method was a signiﬁcant improvement to classical lab
experiments where actual mazes must be constructed in order to study
navigation. Restriction of movement and lack of other stimuli such as

vestibular, tactile or olfactory cues was considered a major limitation of
this approach. However, the ability of rats to learn and adapt to a new
environment while suppressing lack of information from other sensory
inputs, was nevertheless considered positively for the use of virtual
environments. Head ﬁxation and body-ﬁxation techniques (cf. ﬁgure
3B,C) were used for head stabilization during measurement of neural
activity. The techniques included recording membrane potentials [38],
two photon microscopy [26], two photon calcium imaging [25], patch-
clamp recording [28].

In most cases, VE designs are modiﬁed to match the visual properties
or motion properties of the animals. Modiﬁcations are necessary to
answer species-speciﬁc questions and improve the realistic appeal of
the stimulus. Free motion treadmills (cf. ﬁgure 3D), for example, were
designed to introduce vestibular information about rotation, which was
missing in the earlier designs [53]. Takalo et al. [75] used a large
ﬁeld of view and increased the temporal resolution to render stimuli
for fast-moving American cockroaches (tethered). Dahman et al. [18]
used hollowed styrofoam design for accurate registration walking speed
of desert ants (tethered). They showed that ants changed their pace
signiﬁcantly between different approach and search phases, they slowed
down signiﬁcantly while approaching nest position. Stowers et al.
[72] designed visual stimuli with conﬁgurable chromatic properties to
increase the naturalistic appearance of the scene for jumping spiders.
The authors claim that such systems are well suited to study visual
features important for decision-making behavior such as target selection
or predator avoidance.

4.3 Digital design: Free moving animals in VR
In this subsection, we cover experiments with true Virtual Reality
designs which allow free movement of the animal. This means that
the stimulus is rendered in such a way that it creates an illusion of a
three-dimensional space from the animal’s perspective, even though
the projections are on a 2D surface at a ﬁxed distance. This com-
pensation of view is known as perspective correction in human XR
literature. Perspective correction is achieved using sophisticated com-
puter vision techniques of employing real-time tracking (with multiple
cameras) of the animal’s head in 3D. The tracking data is provided
to the rendering engine with a minimum delay to provide real-time
projection, correctly rendered from the perspective of the animal as it
moves through the virtual space. The stimulus can be displayed on ﬂat
or arbitrary shaped surfaces (see ﬁgure 4) using multiple projectors or
screens operating at high framerate. The existing systems use advanced
concepts from computer vision and XR research such as multi camera-
projector synchronization and calibration, real-time 3D tracking and
rendering [32, 72].

One method to design VR systems with freely moving animals is
to use an active treadmill. Active treadmills are used to compensate
motion of the animal and keep them in stationary position (cf. ﬁgure
3E,4). The animal is tracked in 3D using a video camera and counter-
motion of a treadmill is triggered through a servo motor to keep the
animal in the same physical location. This type of design facilitates
behavior studies without creating very large arenas e.g. navigation

6

To appear in IEEE Transactions on Visualization and Computer Graphics

VR is a powerful tool for investigating mechanism of behavior. The
scope of virtual stimulation-based methods go beyond visual stimula-
tion. For exmaple, Sofroniew et al. [71] studied navigation behavior
in rodents using tactile stimulation [71], and Cushman et al. [17] used
directional speakers for acoustic stimulation. Designing multi-sensory
VR system is crucial for studying naturalistic behavior patterns, how-
ever our ability to design such methods are limited. This paper will
only focus on the limitations of VR systems with virtual stimuli.

5 LIMITATIONS OF VIRTUAL ENVIRONMENTS

In this section, we will discuss the limitations faced by researchers
while designing virtual environments for studying animal behavior.
Virtual environments are designed to create believable experiences
for animals by artiﬁcial stimulation of their sensory apparatus. The
main challenge is to create realistic simulations which change contin-
uously based on the behavioral response of animals. Currently, this
is done primarily by displaying visual stimuli to the animal by using
screens or projectors, and tracking their response by using a camera
and treadmills. The stimulus can also be controlled externally to intro-
duce perturbations. This approach is limited to some animals because
existing technology is not capable of solving tracking and simulation
related challenges generally, for all species. Most of these limitations
can be attributed to the physiological properties of the animals. We
examine the limitations of existing tracking and stimulus delivery meth-
ods and link them to the physiological properties of the animals. Our
discussions are inspired from other reviews which focus speciﬁcally
on limitations of using screen/display based artiﬁcial stimuli i.e. video
playback [19], animation [86] and VE [13, 72].

5.1 Limitations of stimulus design and delivery

Animal vision has evolved for enhancing survival, therefore different
animals have different visual properties such as color vision, the ﬁeld
of view, etc. The stimulus employed must therefore be compatible with
the requirements of each animal’s visual system. The stimulus design
and delivery approach are also crucial for the success of behavioral
experiments. In these respects, all commercially available technology
has limitations. In the following text, we outline some important visual
properties and relevant technological considerations.

Multispectral vision is the ability to visualize different spectra of
light. It is also known as spectral resolution in the literature. Humans
and some primates are trichromatic, which means that a combination of
three colors (Red, Blue, and Green) is sufﬁcient to cover the entire color
spectrum seen by humans. In the case of animals, some are dichromatic
(most mammals) or tetrachromatic (birds, reptiles), and some animals
see completely different hues e.g. UV, UV with red, UV with green [19].
Invertebrates commonly use polarized light for navigation and therefore
show a preference towards it. Failure to reproduce such properties may
affect the experiment.

Technological Considerations: LCDs and projectors designed for
human vision are useful for some animals with trichromatic vision and
dichromatic vision. In some cases, lighting conditions are changed [72]
or color ﬁlters are used to match the colors on the display with the color
perception of animals [61](see ﬁgure 1). Creating realistic colors for
animals with multispectral vision (e.g. UV) is difﬁcult and should be
considered when designing experiments. Invertebrates have attractions
towards some wavelengths of light, which should be considered in
order to avoid unintended disruption to behavior.

Flicker fusion threshold is the threshold beyond which a ﬂicker-
ing pattern appears continuous to the observer. Visual information from
the environment is integrated a certain time before it is experienced.
This integration time is varies in different species. Fast-moving animals
typically have a higher ﬂicker fusion threshold. A movie displayed at
a frequency of 25 Hz is sufﬁcient for humans to perceive continuous
motion but for bees, the same movie would appear ﬂickering. The
illusion of motion may be broken by the slow or glitchy movements of
the stimuli.

Technical Considerations: Flicker fusion is important property when
selecting the display and lighting for illumination of experimental are-

(clock-wise) Example of stimuli

Fig. 4.
from the ﬁshVR system
[73] rendered from perspective of ﬁsh (left) and human (right), Mou-
seVR system with free moving rat on circular platform [73] (credit:
https://strawlab.org/freemovr), FlyVR setup with tethered ﬂy (credit: Si-
mon Gingins), top view of VR arena made for terrestrial insects (credit :
Centre for the Advanced Study of Collective Behaviour, Konstanz )

behavior in jumping spider [65] [72] or foraging behavior in desert
ants [18]. Treadmill-based solutions are not suitable for all animals and
therefore the development of novel 3D tracking methods was crucial for
the development of VR solutions for animals. Fry et al. [31, 32] created
TrackFly framework to conduct high throughput closed-loop experi-
ments with unrestrained ﬂying fruit ﬂies. They tracked free moving
ﬂies at 50 Hz using a multi-camera setup. Building upon this tracking
approach Stowers et al. [72] built a FlyVR system. Markerless tracking
was done with infrared ﬁlters to facilitate fast image processing and
block the visible light from stimulus screens. Stowers el at. [72] showed
that it was possible to combine real-time 3D tracking and stimulus de-
livery to induce ﬂight movements in the desired 3D trajectory. They
introduced the concept of a modular and reconﬁgurable framework
designed for animal VR systems. The FlyVR framework supported the
conﬁguration of multiple camera-projector systems along with accurate
geometric and photometric calibration for arbitrary surfaces. This is an
advantage over previous methods as the same framework can be used
with different conﬁgurations (tethered, free-ﬂying and treadmill) for
different animals. They used open-source frameworks such as ROS and
OpenSceneGraph, which are well known in the robotics and graphics
community. Additionally, they also showed a new approach where
multiple ﬂies could be tracked while the stimulus was delivered by
focusing on one of the ﬂies.

Stowers et al. [73] further extended the free moving VR systems
to rodents (MouseVR) and ﬁsh (FishVR) with FreemoVR platform.
This platform can display a wide range of stimuli, naturalistic and
abstract, for experiments on multiple species in different conﬁgurations.
The FishVR system is the ﬁrst underwater VR application, where
visual stimuli are projected on a ﬁsh bowl from below, and infrared
3D tracking is used to for perspective correction (see ﬁgure 4). The
study conﬁrmed that ﬁsh responded to the virtual stimulus as if they
were real. They avoid virtual obstacles placed in the ﬁsh tank by
swimming around it as though it was present. In addition, when a
virtual conspeciﬁc (same-species ﬁsh) was introduced, they swam with
them as though in the real world. VR for freely behaving animals offers
new avenues for research in the ﬁeld of social and collective behavior.
The MouseVR setup is designed to allow mice to move freely on a
raised circular platform (see ﬁgure 4) where stimuli were displayed on
the ﬂoor, a similar setup as used by Del Grosso et al. [20]. Experiments
with checkered patterns show that freely moving mice estimate height
using motion parallax, a ﬁnding that was not possible to test in earlier
mention treadmill based systems [73]. Experiments with freely ﬂying
ﬂies in FlyCave indicated that ﬂight control of ﬂies is fundamentally
altered by tethering, even without head ﬁxation [73]. The authors
used this study to stress the importance of developing new methods for
free-moving animals.

7

nas. The light source may appear to ﬂicker if animal’s ﬂicker fusion
threshold is higher than operational frequency of the light source. Po-
tential biological affects of artiﬁcial light ﬂicker are discussed in detail
by Inger et al. [41]. The same is applicable for operation frequency
of the display or projector, and framerate of the rendered simulation.
Most existing methods use displays at 120 Hz and employed GPUs for
fast rendering. It was shown that the ﬂicker fusion threshold of some
animals can be lowered by manipulating size of stimulus, luminance
of display, brightness of surrounding and region of retina involved in
image formation [19].

Visual acuity

of an animal is it’s ability to resolve spatial detail. It
can also be deﬁned as the spatial resolution of the eyes. It is measured
in degrees; some animals have very high acuity (e.g. eagles, falcons)
and some have very low acuity (e.g. ﬁsh or ants) [19]. The displayed
stimuli may appear pixelated or unwanted holes can be seen in images
when the acuity of animal is not appreciated.

Technical consideration: Visual acuity is considered when selecting
the display screen or projector and the distance at which to present the
stimuli. Animals with very high visual acuity may see pixelated images
on screens made for humans. Similarly, distance from the screen
is essential because the effect is greater at closer distances. If this
requirement is unmet, the illusion of continuous color might be broken,
which may be an important consideration for the experiment [19].

Field of view (FOV)

refers to the area/volume observed by the
eyes at any given moment. It is usually measured in degrees and can
vary widely between different animals. FOV depends on the position
of the eyes and construction of the eye. Animals with front-facing eyes
and overlapping vision (e.g. primates, cats) have considerably smaller
FOV than animals with eyes on the sides of the head (e.g. birds or
insects). FOV may change slightly for animals that can rotate their eyes
in the socket. It is also notable that most animals do not have sharp
vision in all parts of the FOV i.e. high resolution at fovea and less at
the periphery.

Technical consideration: FOV is considered while deciding the dis-
play area and shape of the screen for the stimulus. Most of the time
curved or cylindrical display screens are selected for small insects and
rodents [80]. Projectors are preferred over LCD screens because curved
LCDs are difﬁcult and expensive to produce. A larger FOV is ctypi-
cally overed by using multiple projectors, which adds the additional
complexity of synchronization and calibration of projectors.

Depth Perception Most animals have some mechanism to per-
ceive depth in the environment. Different animals use different cues
such as stereopsis, motion parallax or focusing, overlap, shadow, verti-
cal distance to the horizon, retina to image size ratio, perspective and
texture [19]. Biologically stereo vision is not always necessary for all
species, many species use non-stereoscopic depth cues because they
have limited overlap between ﬁeld of view. Failure to accommodate
some depth cues may reveal the 2D nature of the stimulus [15, 86].

Technical consideration: Depth cues are considered while designing
appearance of virtual stimuli and the experiment. Until now, most arti-
ﬁcial stimuli based methods display stimulus on ﬂat or curved surfaces.
It is likely that animals can perceive ﬂat or curved screen if the stimulus
is rendered without correct perspective correction, which may affect
their behavior. Recent VR methods use 3D head tracking or body track-
ing to maintain perspective of the animal but do not offer stereoscopic
depth cues. Researchers must be careful while designing experiments
which require the animal to may be use depth cues. Stowers et al. [73]
suggest that tracking eye movements is important for introducing depth
cues. Recently, Nityananda et al. [62] glued color ﬁlters to study stereo-
scopic depth perception in insects (see ﬁgure 1). They show that it is
possible to use such modiﬁcation when the research question is chosen
appropriately.

restrictions. Adding markers on animals may affect their natural be-
havior, but recent advances in computer vision have shown promising
results for markerless tracking [67]. Existing tracking limitations often
stem from physiological properties which are explained below.

Locomotion properties Animals possess diverse abilities to move
in their environment, such as ﬂying, swimming or jumping. Often the
speed of locomotion may vary and some movements (e.g. jumping)
have to be restricted in order to keep the animal in a desired space.
Movement of the animal in the arena must be measured for accurate
depiction of stimuli in the virtual environment and for rendering per-
spective correct stimulus. Accurate movement tracking is necessary
for mapping movement decisions of the animal to the visual features
rendered in the virtual world. Mismatch between this mapping can
potentially invalidate behavioral ﬁndings.

Technical Consideration: Locomotion properties of the animal inﬂu-
ence the selection of the tracking approach. Ideally, the animal should
be freely moving but restriction may be necessary depending on the
need of experiment (e.g. neurophysiology). Tethering or treadmill
based approaches (ﬁgure 4) may be selected if feedback from other
sensory systems can be compromised or disregarded for the purpose of
the study. In both cases, optical tracking is used for tracking movement
of the animal. In the case of treadmills, the motion of ball is measured
to infer the movement of the animal. The cameras selected for sampling
the motion of the ball must operate at higher frame rates than the rate
of rotation of the styrofoam ball. Additionally, the rotation mechanism
of the ball must sensitive towards variations in the movement of the
animal. For example, the ball must accelerate and decelerate in sync
with the animals motion otherwise it may create an unwanted pertur-
bation for the animal [18]. Similar considerations must be made when
selecting cameras for tracking motion of the animal in tethered cases.
For example, Stowers et al. [73] used sampling frequency of 100 Hz to
compute motion of the fruit ﬂies.

Computer vision algorithms are used to track motion of freely mov-
ing animals. The performance of such algorithms is dependent on the
visibility of the animals in the images. Fast-moving animals can appear
blurry as a result of improper camera selection. Camera properties
such as e.g. frame rate, resolution, opening angle, rolling/global shut-
ter, must be considered to capture the movements of the animal. 3D
tracking requires multiple cameras which adds technical complexity
regarding calibration and synchronization of cameras. Active treadmills
are designed to restrict animals to one particular spot to reduce track-
ing complexity. Often lighting conﬁgurations are selected to enable
real-time tracking. For example, IR is preferred because it is easier to
add much more light to the scene without disturbing behavior of the
animal [72].

Appearance of many animals differs in terms shape and structure
which presents novel challenges for purely image-based tracking of
animals. Some animals do not have any conspicuous features, and
some have repetitive patterns which makes different parts of the body
appear confusingly similar. Such confusions lead to ﬂuctuations in the
tracking results, and consequently in the presented stimuli.

Technical considerations: The appearance of the animal is an impor-
tant consideration for the selection of tracking software, camera and
light conditions. The software must process the image in real-time and
detect the animal. Paint or reﬂective markers can be used to add fea-
tures to have seamless tracking results. Many recent improvements in
marker-less tracking methods can allow real-time tracking of seeming
featureless objects. Light conditions are often changed to increase the
detection rate of markers or animals, while high-resolution cameras
are useful for capturing ﬁne details of the animals. However, high-
resolution images require a longer time for processing and storage, and
therefore the selection of camera often involves a tradeoff.

5.3 Latency

5.2 Limitations of tracking

Tracking movements and the perspective of the animal is essential
for designing a VR experiment. Tracking freely-moving animals is
challenging and most of the experiments still require tethering or other

Latency of a closed loop system is the overall delay between movement
of the animal and the change of stimulus on the display. Multiple
computational steps are involved between these two events such as
image processing, data storage, graphics rendering, etc. Each of these

8

To appear in IEEE Transactions on Visualization and Computer Graphics

steps introduces a time delay in the system. Overall latency of the
system is caused by both software and hardware components. The
latency must be very low to allow for an interactive experiment in
It is difﬁcult to provide economical solutions for many
real-time.
hardware-related problems e.g., fast computation, higher bandwidth
data transmission, and responsive displays. Because of these challenges,
the achievable overall latency of the system must be considered while
selecting the animal and the behavior to study.

5.4 Lack of technical expertise

VEs for animals are designed by biologists using technology developed
by engineers. Modern VEs are designed using software and algorithmic
methods from computer vision and XR communities such as tracking,
graphical rendering, because their work available through open-source
distribution. These methods are complex and certain expertise are
required to tweak these methods to be able to use them with animals.
Biologists are forced to develop engineering and programming skills
to develop new concepts for behavioral experiments in virtual environ-
ments. Only few biologists have successfully bridged the gap between
these ﬁelds to create customized tracking algorithms and modular soft-
ware frameworks for performing experiments with animals in VE. We
consider that lack of technical expertise, from CV & XR community,
is one of the biggest limitations for the development of VR for animal
behavior experiments.

6 DISCUSSION OF FUTURE DIRECTIONS

The aim of this paper is to promote interdisciplinary research between
engineers, computer scientists, and biologists. In this spirit the focus of
our discussion will be on the future of interactive behavior experiments.
We will report the current research trends in robotics and computer
vision. Based on that we will suggest two novel applications for be-
havior studies, which may be realized with support from experts in
the XR community. Our intention is to provide a starting point that
may encourage further discussion on this topic in the community (Ref.
supplementary document for additional discussions).

Existing VEs are not yet suitable for studying all types of behaviors.
The animal’s sensory feedback is generally restricted to prefer one sen-
sory input using methods previously described such as wing clipping,
body ﬁxation. This is not ideal as lack sensory information affects
the animal’s decision making process in some cases [73]. Moreover,
animals with multi-modal sensory systems combine information from
multiple senses e.g. sound, vision or smell. It is a major challenge to
design multi-sensory VEs for freely moving animals. Biologists have
worked meticulously to gain knowledge about the behavior and sensory
systems of some animal species. Based on this information they have
built interactive VEs for some animals e.g. insects, ﬁsh, and rodents.
There is a strong need for the development of new solutions which will
improve the sensory feedback mechanisms, add multi-sensory feed-
back and extend the application of VR to new species. We argue that
some of these problems can be alleviated in the future by starting new
collaborations between experts in computer science and biology.

6.1 Growing support from robotics and computer vision

Animal behavior studies are moving towards an increasingly interdisci-
plinary approach. The behaviors and mechanisms studied in the animal
VR are gaining attention in the ﬁeld of robotics. Vision induced locomo-
tion and navigation studies in small insects are appealing for designers
of nature-inspired robots [3, 47, 87] and self-navigating drones [43].
Studies focusing on the understanding of sensory mechanisms of small
animals are gaining interest in the ﬁeld of smart sensor design. In 2018,
DARPA launched a robotics challenge to design small, lightweight
and power-efﬁcient micro-robots for use in disaster relief scenarios of
the future. The recent developments in free moving VR systems have
opened doors for conducting new types of studies in collective behavior
and social behavior, with developers of self-organizing robots already
using the theories developed in collective behavior studies [78, 85].
The robotics community is actively involved in development of new
methods for studying behavior using interactive robots [45, 47, 50, 84].

Real-time methods for tracking of the eye positions and head ori-
entations of animals is missing in existing VEs. This improvement in
tracking is important to extend the application of VEs to other animals.
In case of small animals the head position is inferred from the animals
position and orientation, however, it is difﬁcult to do the same animals
with articulated bodies. Recent publications in computer vision litera-
ture show that the community is taking interest in challenging problems
involving animal tracking. Many researchers have proposed easy to use
methods for keypoint based posture computation in animals [35, 54]
(Fig. S2,S3 in supplementary). Moreover, extracting 3D posture of
animals from images and videos is an emerging topic in the computer
vision community [37, 55, 63, 88]. Posture based video analysis and
activity recognition are currently being investigated using deep learning
techniques. High-resolution temporal information with postures may
allow the study of complex behavior patterns e.g. courtship display or
aggression display.

6.2 Introducing new concepts of XR

Spatial Augmented Reality (SAR)

applications are not fully ex-
plored in animal behavior experiments. Projectors are readily used in
behavioral experiments but often their use is limited to displaying stim-
uli in open-loop e.g. predator-pray interaction [42]. Ioannou et al. [42]
used open-loop approach due to the lack of methods to track ﬁsh in
real-time. Now, it is possible to perform similar experiments in closed
loop with the help of real-time tracking methods. One possible appli-
cation is the use of dynamic projection mapping with robotic animals.
It was shown that social behavior can be studied using robots instead
of real animals, and animals can and do interact with robots [47, 50].
Ladgraf et al. [50] tested robotic models with different appearances
and claimed that appearance was crucial for the acceptance of a robotic
agent by real ﬁsh to study the social behavior of ﬁsh. We argue that
dynamic projection mapping techniques [57, 59] can be deployed to
alter features of robotic stimuli. The projector can project different
patterns on a robotic agent while maintaining the perspective of the real
animal using real-time 3D tracking. Experiments with SAR could open
new possibilities such as training animals for navigation or memory
experiments using virtual agents projected on a wall or a robot. Re-
searchers at CASCB 1 are currently building a large scale arena for
conducting interactive experiments with one or more animals using
real-time 3D tracking and projection technology.

Collective behavior studies related to the decision making of
a group and the effect of the individual decision on the group may
be studied using VR. It is shown that real animals do interact with
virtual conspeciﬁcs in the VR e.g. ﬁsh [73]. Multiple VR systems
can be plugged together to create conditions for social behavior in a
virtual manner i.e. collaborative VR space for animals [51]. In such
a virtual social scenario, each animal may interact with a group of
virtual conspeciﬁcs which are projections of real animals from other
VR systems. The visual information available to each individual can be
controlled in such environments and manipulated based on the needs of
the experiment. If the animals start to swarm in virtual environments
the principals governing their decisions can be studied in much detail.

7 CONCLUSION

Applications of virtual environments have previously been discussed
from the perspective of a human user. However, there is substantial
work showing that some animals can, and do, respond to virtual stimuli.
In this review, we discussed the use of the virtual environments for
studying animal behavior. We show that investing in the development of
such concepts is beneﬁcial for research in various disciplines throughout
biology and engineering. A lot of progress has been made in the
past two decades, but support in terms of technology development is
required to extend the use of this technology in biology. We hope that
this review sparks interest in animal oriented applications of VE among
the technology developers in the XR community. Animal behavior
experiments involving XR systems and robotics have the potential to
become an independent ﬁeld of interdisciplinary research.

1Centre for the Advanced Study of Collective Behavior, Konstanz

9

ACKNOWLEDGMENTS

The authors wish to thank John Stowers, Yuji Oyamada, and Bianca
Schell for reviewing the manuscript. Authors who provided their im-
ages. This work is support by funding from the DFG Centre of Excel-
lence 2117 Centre for the Advanced Study of Collective Behaviour”
(ID: 422037984).

REFERENCES

[1] J. Abdeljalil, M. Hamid, O. Abdel-mouttalib, R. Stphane, R. Raymond,
A. Johan, S. Jos, C. Pierre, and P. Serge. The optomotor response: A robust
ﬁrst-line visual screening method for mice. Vision Research, 45(11):1439–
1446, May 2005. doi: 10.1016/j.visres.2004.12.015

[2] R. C. Arkin, R. C. Arkin, et al. Behavior-based robotics. MIT press, 1998.
[3] S. Balasubramanian, Y. M. Chukewad, J. M. James, G. L. Barrows, and
S. B. Fuller. An Insect-Sized Robot That Uses a Custom-Built Onboard
Camera and a Neural Network to Classify and Respond to Visual Input.
In 2018 7th IEEE International Conference on Biomedical Robotics and
Biomechatronics (Biorob), pp. 1297–1302. IEEE, Enschede, Aug. 2018.
doi: 10.1109/BIOROB.2018.8488007

[4] S. Baldauf, H. Kullmann, T. ThNken, S. Winter, and T. Bakker. Computer
animation as a tool to study preferences in the cichlid Pelvicachromis
taeniatus. Journal of Fish Biology, 75(3):738–746, Aug. 2009. doi: 10.
1111/j.1095-8649.2009.02347.x

[5] P. Bateson and K. N. Laland. Tinbergen’s four questions: an appreciation
and an update. Trends in Ecology & Evolution, 28(12):712–718, Dec.
2013. doi: 10.1016/j.tree.2013.09.013

[6] A. T. D. Bennett, I. C. Cuthill, J. C. Partridge, and E. J. Maier. Ultraviolet
vision and mate choice in zebra ﬁnches. Nature, 380(6573):433–435, Apr.
1996. doi: 10.1038/380433a0

[7] A. BISAZZA, A. DE SANTI, and G. VALLORTIGARA. Laterality and
cooperation: mosquitoﬁsh move closer to a predator when the companion
is on their left side. Animal Behaviour, 57(5):1145–1149, 1999.

[8] C. J. Bohil, B. Alicea, and F. A. Biocca. Virtual reality in neuroscience
research and therapy. Nature Reviews Neuroscience, 12(12):752–762, Dec.
2011. doi: 10.1038/nrn3122

[9] A. Borst. Drosophila’s View on Insect Vision. Current Biology, 19(1):R36–

R47, Jan. 2009. doi: 10.1016/j.cub.2008.11.001

[10] H. B¨ulthoff. Drosophila mutants disturbed in visual orientation. Biological

Cybernetics, 45(1):63–70, 1982. doi: 10.1007/BF00387215

[11] T. Butkowski, W. Yan, A. M. Gray, R. Cui, M. N. Verzijden, and G. G.
Rosenthal. Automated Interactive Video Playback for Studies of Animal
Communication. Journal of Visualized Experiments, (48), Feb. 2011. doi:
10.3791/2374

[12] L. CARMICHAEL. The study of instinct. n. tinbergen. new york: Oxford
univ. press, 1951. Science, 115(2990):438–439, 1952. doi: 10.1126/
science.115.2990.438-a

[13] L. Chouinard-Thuly, S. Gierszewski, G. G. Rosenthal, S. M. Reader,
G. Rieucau, K. L. Woo, R. Gerlai, C. Tedore, S. J. Ingley, J. R. Stowers,
J. G. Frommen, F. L. Dolins, and K. Witte. Technical and conceptual
considerations for using animated stimuli in studies of animal behavior.
Current Zoology, 63(1):5–19, Feb. 2017. doi: 10.1093/cz/zow104
[14] D. L. Clark and G. W. Uetz. Video image recognition by the jump-
ing spider, Maevia inclemens (Araneae: Salticidae). Animal Behaviour,
40(5):884–890, 1990.

[15] T. S. Collett. Vision: simple stereopsis. Current Biology, 6(11):1392–1395,

1996.

[16] C. Cruz-Neira, D. J. Sandin, T. A. DeFanti, R. V. Kenyon, and J. C.
Hart. The CAVE: audio visual experience automatic virtual environment.
Communications of the ACM, 35(6):64–72, June 1992. doi: 10.1145/
129888.129892

[17] J. D. Cushman, D. B. Aharoni, B. Willers, P. Ravassard, A. Kees, C. Vuong,
B. Popeney, K. Arisaka, and M. R. Mehta. Multisensory Control of
Multimodal Behavior: Do the Legs Know What the Tongue Is Doing?
PLoS ONE, 8(11):e80465, Nov. 2013. doi: 10.1371/journal.pone.0080465
[18] H. Dahmen, V. L. Wahl, S. E. Pfeffer, H. A. Mallot, and M. Wittlinger.
Naturalistic path integration of Cataglyphis desert ants on an air-cushioned
lightweight spherical treadmill. The Journal of Experimental Biology,
220(4):634–644, Feb. 2017. doi: 10.1242/jeb.148213

[19] R. B. D’EATH. Can video images imitate real stimuli in animal behaviour

experiments? Biological Reviews, 73(3):267–292, 1998.

[20] N. A. Del Grosso, J. J. Graboski, W. Chen, E. B. Hernndez, and A. Sirota.
Virtual Reality system for freely-moving rodents. bioRxiv, p. 161232,
2017.

[21] A. I. Dell, J. A. Bender, K. Branson, I. D. Couzin, G. G. de Polavieja,
L. P. Noldus, A. Prez-Escudero, P. Perona, A. D. Straw, M. Wikelski, and
U. Brose. Automated image-based tracking and its application in ecology.
Trends in Ecology & Evolution, 29(7):417–428, July 2014. doi: 10.1016/j.
tree.2014.05.004

[22] T. Denayer, T. St¨ohr, and M. Van Roy. Animal models in translational
medicine: Validation and prediction. New Horizons in Translational
Medicine, 2(1):5–11, 2014.

[23] M. Dill, R. Wolf, and M. Heisenberg. Visual pattern recognition in
Drosophila involves retinotopic matching. Nature, 365(6448):751–753,
Oct. 1993. doi: 10.1038/365751a0

[24] F. L. Dolins, K. Schweller, and S. Milne. Technology advancing the study
of animal cognition: using virtual reality to present virtually simulated
environments to investigate nonhuman primate spatial cognition. Current
Zoology, 63(1):97–108, Feb. 2017. doi: 10.1093/cz/zow121

[25] D. A. Dombeck, C. D. Harvey, L. Tian, L. L. Looger, and D. W. Tank.
Functional imaging of hippocampal place cells at cellular resolution during
virtual navigation. Nature Neuroscience, 13(11):1433–1440, Nov. 2010.
doi: 10.1038/nn.2648

[26] D. A. Dombeck, A. N. Khabbaz, F. Collman, T. L. Adelman, and D. W.
Tank. Imaging Large-Scale Neural Activity with Cellular Resolution in
Awake, Mobile Mice. Neuron, 56(1):43–57, Oct. 2007. doi: 10.1016/j.
neuron.2007.08.003

[27] D. A. Dombeck and M. B. Reiser. Real neuroscience in virtual worlds.
Current Opinion in Neurobiology, 22(1):3–10, Feb. 2012. doi: 10.1016/j.
conb.2011.10.015

[28] C. Domnisoru, A. A. Kinkhabwala, and D. W. Tank. Membrane potential
dynamics of grid cells. Nature, 495(7440):199–204, Mar. 2013. doi: 10.
1038/nature11973

[29] M. C. Escher and J. W. Vermeulen. Escher on escher exploring the inﬁnite.

1989.

[30] C. S. Evans and P. Marler. On the use of video images as social stimuli in
birds: audience effects on alarm calling. Animal Behaviour, 41(1):17–26,
1991.

[31] S. N. Fry, M. Bichsel, P. Mller, and D. Robert. Tracking of ﬂying insects
using pan-tilt cameras. Journal of Neuroscience Methods, 101(1):59–67,
2000.

[32] S. N. Fry, N. Rohrseitz, A. D. Straw, and M. H. Dickinson. TrackFly:
Virtual reality for a behavioral system analysis in free-ﬂying fruit ﬂies.
Journal of Neuroscience Methods, 171(1):110–117, June 2008. doi: 10.
1016/j.jneumeth.2008.02.016

[33] G. Geiger. Optomotor responses of the ﬂy Musca domestica to transient
stimuli of edges and stripes. Kybernetik, 16(1):37–43, 1974. doi: 10.
1007/BF00270293

[34] R. Gerlai, Y. Fernandes, and T. Pereira. Zebraﬁsh (Danio rerio) responds
to the animated image of a predator: Towards the development of an
automated aversive task. Behavioural Brain Research, 201(2):318–324,
Aug. 2009. doi: 10.1016/j.bbr.2009.03.003

[35] J. M. Graving, D. Chae, H. Naik, L. Li, B. Koger, B. R. Costelloe, and
I. D. Couzin. Deepposekit: a software toolkit for fast and robust pose
estimation using deep learning. eLife, 8:e47994, 2019.

[36] J. R. Gray, V. Pawlowski, and M. A. Willis. A method for recording
behavior and multineuronal CNS activity from tethered insects ﬂying in
virtual space. Journal of Neuroscience Methods, 120(2):211–223, Oct.
2002. doi: 10.1016/S0165-0270(02)00223-6

[37] S. G¨unel, H. Rhodin, D. Morales, J. Campagnolo, P. Ramdya, and P. Fua.
Deepﬂy3d, a deep learning-based approach for 3d limb and appendage
tracking in tethered, adult Drosophila. eLife, 8:e48571, oct 2019. doi: 10.
7554/eLife.48571

[38] C. D. Harvey, F. Collman, D. A. Dombeck, and D. W. Tank. Intracellular
dynamics of hippocampal place cells during virtual navigation. Nature,
461(7266):941–946, Oct. 2009. doi: 10.1038/nature08499

[39] S. Hess, E. Oberhummer, R. Burlaud, A. Fernandez, S. Fischer, J. From-
men, and B. Taborsky. Animated images as a tool to study visual com-
munication: a case study in a cooperatively breeding cichlid. Behaviour,
151(12-13):1921–1942, Oct. 2014. doi: 10.1163/1568539X-00003223

[40] C. Holscher. Rats are able to navigate in virtual environments. Journal
of Experimental Biology, 208(3):561–569, Feb. 2005. doi: 10.1242/jeb.
01371

[41] R. Inger, J. Bennie, T. W. Davies, and K. J. Gaston. Potential biological

10

To appear in IEEE Transactions on Visualization and Computer Graphics

and ecological effects of ﬂickering artiﬁcial light. PloS one, 9(5):e98631,
2014.

[42] C. C. Ioannou, V. Guttal, and I. D. Couzin. Predatory ﬁsh select for
coordinated collective motion in virtual prey. Science, 337(6099):1212–
1215, 2012. doi: 10.1126/science.1218919

[43] N. T. Jafferis, E. F. Helbling, M. Karpelson, and R. J. Wood. Untethered
ﬂight of an insect-sized ﬂapping-wing microscale aerial vehicle. Nature,
570(7762):491–495, June 2019. doi: 10.1038/s41586-019-1322-0
[44] T. A. Jenssen. Female response to ﬁlmed displays of Anolis nebulosus

(Sauria, Iguanidae). Animal Behaviour, 18:640–647, 1970.

[45] B. A. Klein, J. Stein, and R. C. Taylor. Robots in the service of animal
behavior. Communicative & Integrative Biology, 5(5):466–472, Sept.
2012. doi: 10.4161/cib.21304

[46] R. Knzler and T. C. Bakker. Female preferences for single and com-
bined traits in computer animated stickleback males. Behavioral Ecology,
12(6):681–685, 2001.

[47] J. Krause, A. F. Winﬁeld, and J.-L. Deneubourg. Interactive robots in
experimental biology. Trends in Ecology & Evolution, 26(7):369–375,
July 2011. doi: 10.1016/j.tree.2011.03.015

[48] F. Kretschmer, M. Tariq, W. Chatila, B. Wu, and T. C. Badea. Comparison
of optomotor and optokinetic reﬂexes in mice. Journal of Neurophysiology,
118(1):300–316, July 2017. doi: 10.1152/jn.00055.2017

[49] M. Land. Eye movements in man and other animals. Vision Research,

162:1–7, Sept. 2019. doi: 10.1016/j.visres.2019.06.004

[50] T. Landgraf, D. Bierbach, H. Nguyen, N. Muggelberg, P. Romanczuk,
and J. Krause. RoboFish: increased acceptance of interactive robotic
ﬁsh with realistic eyes and natural motion patterns by live Trinidadian
guppies. Bioinspiration & Biomimetics, 11(1):015001, Jan. 2016. doi: 10.
1088/1748-3190/11/1/015001

Canonical 3d pose networks for non-rigid structure from motion.
In
Proceedings of the IEEE International Conference on Computer Vision,
2019.

[64] T. J. Ord, R. A. Peters, C. S. Evans, and A. J. Taylor. Digital video playback
and visual communication in lizards. Animal Behaviour, 63(5):879–890,
May 2002. doi: 10.1006/anbe.2001.1983

[65] T. Peckmezian and P. W. Taylor. A virtual reality paradigm for the study of
visually mediated behaviour and cognition in spiders. Animal Behaviour,
107:87–95, Sept. 2015. doi: 10.1016/j.anbehav.2015.06.018

[66] T. J. Pitcher and J. E. T. Lawrence. A simple stereo television system
with application to the measurement of three-dimensional coordinates of
ﬁsh in schools. Behavior Research Methods, Instruments, & Computers,
16(6):495–501, 1984.

[67] A. Prez-Escudero, J. Vicente-Page, R. C. Hinz, S. Arganda, and G. G.
de Polavieja.
idTracker: tracking individuals in a group by automatic
identiﬁcation of unmarked animals. Nature Methods, 11(7):743–748, July
2014. doi: 10.1038/nmeth.2994

[68] S. Rohwer. Dyed birds achieve higher social status than controls in harris’
sparrows. Animal Behaviour, 33(4):1325 – 1331, 1985. doi: 10.1016/
S0003-3472(85)80193-7

[69] S. Schuster, R. Strauss, and K. G. Gtz. Virtual-reality techniques resolve
the visual cues used by fruit ﬂies to evaluate object distances. Current
Biology, 12(18):1591–1594, 2002.

[70] E. Sobel. The locust’s use of motion parallax to measure distance. Jour-
nal of Comparative Physiology A, 167(5), Nov. 1990. doi: 10.1007/
BF00192653

[71] N. J. Sofroniew, Y. A. Vlasov, S. A. Hires, J. Freeman, and K. Svoboda.
Neural coding in barrel cortex during whisker-guided locomotion. Elife, 4,
2015.

[51] J. Larsch and H. Baier. Biological Motion as an Innate Perceptual Mecha-
nism Driving Social Afﬁliation. Current Biology, 28(22):3523–3532.e4,
Nov. 2018. doi: 10.1016/j.cub.2018.09.014

[72] J. R. Stowers, A. Fuhrmann, M. Hofbauer, M. Streinzer, A. Schmid, M. H.
Dickinson, and A. D. Straw. Reverse engineering animal vision with
virtual reality and genetics. Computer, 47(7):38–45, 2014.

[52] K. A. Leighty and D. M. Fragaszy. Primates in cyberspace: using interac-
tive computer tasks to study perception and action in nonhuman animals.
Animal Cognition, 6(3):137–139, Sept. 2003. doi: 10.1007/s10071-003
-0177-8

[53] M. S. Madhav, R. P. Jayakumar, F. Savelli, H. T. Blair, N. J. Cowan, and
J. J. Knierim. Place cells in virtual reality dome reveal interaction between
conﬂicting self-motion and landmark cues. In Society for Neuroscience.
Chicago, IL, USA, Oct. 2015.

[54] A. Mathis, P. Mamidanna, K. M. Cury, T. Abe, V. N. Murthy, M. W. Mathis,
and M. Bethge. DeepLabCut: markerless pose estimation of user-deﬁned
body parts with deep learning. Nature Neuroscience, 21(9):1281–1289,
Sept. 2018. doi: 10.1038/s41593-018-0209-y

[73] J. R. Stowers, M. Hofbauer, R. Bastien, J. Griessner, P. Higgins, S. Fa-
rooqui, R. M. Fischer, K. Nowikovsky, W. Haubensak, I. D. Couzin,
K. Tessmar-Raible, and A. D. Straw. Virtual reality for freely moving ani-
mals. Nature Methods, 14(10):995–1002, Aug. 2017. doi: 10.1038/nmeth.
4399

[74] R. Strauss, S. Schuster, and K. G. Gtz. Processing of artiﬁcial visual
feedback in the walking fruit ﬂy Drosophila melanogaster. Journal of
Experimental Biology, 200(9):1281–1296, 1997.

[75] J. Takalo, A. Piironen, A. Honkanen, M. Lempe, M. Aikio, T. Tuukkanen,
and M. Vhsyrinki. A fast and ﬂexible panoramic virtual reality system
for behavioural and electrophysiological experiments. Scientiﬁc Reports,
2(1), Dec. 2012. doi: 10.1038/srep00324

[55] M. W. Mathis and A. Mathis. Deep learning tools for the measurement
of animal behavior in neuroscience. arXiv:1909.13868 [cs, q-bio], Oct.
2019. arXiv: 1909.13868.

[76] M. J. Tarr and W. H. Warren. Virtual reality in behavioral neuroscience
and beyond. Nature Neuroscience, 5(S11):1089–1092, Nov. 2002. doi: 10.
1038/nn948

[56] M. Milinski and T. C. M. Bakker. Female sticklebacks use male coloration
in mate choice and hence avoid parasitized males. Nature, 344(6264):330–
333, Mar. 1990. doi: 10.1038/344330a0

[57] L. Miyashita, T. Yamazaki, K. Uehara, Y. Watanabe, and M. Ishikawa.
Portable lumipen: Dynamic sar in your hand. In 2018 IEEE International
Conference on Multimedia and Expo (ICME), pp. 1–6, July 2018. doi: 10.
1109/ICME.2018.8486514

[58] K. Mller, I. Smielik, J.-M. Htwohl, S. Gierszewski, K. Witte, and K.-D.
Kuhnert. The virtual lover: variable and easily guided 3d ﬁsh animations
as an innovative tool in mate-choice experiments with sailﬁn mollies-I.
Design and implementation. Current Zoology, 63(1):55–64, Feb. 2017.
doi: 10.1093/cz/zow106

[59] G. Narita, Y. Watanabe, and M. Ishikawa. Dynamic projection mapping
onto deforming non-rigid surface using deformable dot cluster marker.
IEEE Transactions on Visualization and Computer Graphics, 23(3):1235–
1248, March 2017. doi: 10.1109/TVCG.2016.2592910

[60] X. J. Nelson and N. Fijn. The use of visual media as a tool for investigating
animal behaviour. Animal Behaviour, 85(3):525–536, Mar. 2013. doi: 10.
1016/j.anbehav.2012.12.009

[61] V. Nityananda, G. Tarawneh, S. Henriksen, D. Umeton, A. Simmons, and
J. C. Read. A Novel Form of Stereo Vision in the Praying Mantis. Current
Biology, Feb. 2018. doi: 10.1016/j.cub.2018.01.012

[62] V. Nityananda, G. Tarawneh, R. Rosner, J. Nicolas, S. Crichton, and
J. Read. Insect stereopsis demonstrated using a 3d insect cinema. Scientiﬁc
Reports, 6(1), May 2016. doi: 10.1038/srep18718

[77] G. K. Taylor, M. Bacic, R. J. Bomphrey, A. C. Carruthers, J. Gillies,
S. M. Walker, and A. L. R. Thomas. New experimental approaches to
the biology of ﬂight control systems. Journal of Experimental Biology,
211(2):258–266, Jan. 2008. doi: 10.1242/jeb.012625

[78] A. Tero, S. Takagi, T. Saigusa, K. Ito, D. P. Bebber, M. D. Fricker, K. Yu-
miki, R. Kobayashi, and T. Nakagaki. Rules for Biologically Inspired
Adaptive Network Design. Science, 327(5964):439–442, Jan. 2010. doi:
10.1126/science.1177894

[79] J. C. Theobald, D. L. Ringach, and M. A. Frye. Dynamics of optomo-
tor responses in Drosophila to perturbations in optic ﬂow. Journal of
Experimental Biology, 213(8):1366–1375, Apr. 2010. doi: 10.1242/jeb.
037945

[80] K. Thurley and A. Ayaz. Virtual reality systems for rodents. Current

Zoology, 63(1):109–119, Feb. 2017. doi: 10.1093/cz/zow070

[81] N. Tinbergen. On Aims and Methods of Ethology. Zeitschrift fuer Tierpsy-
chologie, p. 28, 1963. doi: 10.1111/j.1439-0310.1963.tb01161.x
[82] N. Tinbergen and A. C. Perdeck. On the Stimulus Situation Releasing
the Begging Response in the Newly Hatched Herring Gull Chick (Larus
Argentatus Argentatus Pont.). Behaviour, 3(1):1–39, 1950.

[83] T. Veen, S. J. Ingley, R. Cui, J. Simpson, M. R. Asl, J. Zhang, T. Butkowski,
W. Li, C. Hash, and J. B. Johnson. anyFish: an open-source software
to generate animated ﬁsh models for behavioural studies. Evolutionary
Ecology Research, 15(3):361–375, 2013.

[84] B. Webb. What does robotics offer animal behaviour? Animal Behaviour,

60(5):545–558, Nov. 2000. doi: 10.1006/anbe.2000.1514

[63] D. Novotny, N. Ravi, B. Graham, N. Neverova, and A. Vedaldi. C3dpo:

[85] J. Werfel, K. Petersen, and R. Nagpal. Designing Collective Behavior in a

11

Termite-Inspired Robot Construction Team. Science, 343(6172):754–758,
Feb. 2014. doi: 10.1126/science.1245842

[86] K. L. Woo and G. Rieucau. From dummies to animations: a review of
computer-animated stimuli used in animal behavior studies. Behavioral
Ecology and Sociobiology, 65(9):1671–1685, Sept. 2011. doi: 10.1007/
s00265-011-1226-y

[87] Y. Zou, W. Zhang, and Z. Zhang. Liftoff of an electromagnetically driven
IEEE Transactions on Robotics,

insect-inspired ﬂapping-wing robot.
32(5):1285–1289, Oct 2016. doi: 10.1109/TRO.2016.2593449

[88] S. Zufﬁ, A. Kanazawa, and M. J. Black. Lions and Tigers and Bears: Cap-
turing Non-rigid, 3d, Articulated Shape from Images. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 3955–3963.
IEEE, Salt Lake City, UT, June 2018. doi: 10.1109/CVPR.2018.00416

12

