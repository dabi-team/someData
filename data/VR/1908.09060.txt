9
1
0
2

g
u
A
4
2

]

V
C
.
s
c
[

1
v
0
6
0
9
0
.
8
0
9
1
:
v
i
X
r
a

EyeNet: A Multi-Task Network for Off-Axis Eye Gaze Estimation and
User Understanding

Zhengyang Wu Srivignesh Rajendran Tarrence van As

Joelle Zimmermann

Vijay Badrinarayanan Andrew Rabinovich
Magic Leap, Inc.
{zwu,srajendran,tvanas,jzimmermann,vbadrinarayanan,arabinovich}@magicleap.com

Abstract

Eye gaze

estimation and simultaneous

semantic
understanding of a user through eye images is a crucial
component in Virtual and Mixed Reality; enabling energy
efﬁcient rendering, multi-focal displays and effective
In head-mounted VR/MR
interaction with 3D content.
devices the eyes are imaged off-axis to avoid blocking the
this view-point makes drawing eye related
user’s gaze,
inferences very challenging.
In this work, we present
EyeNet, the ﬁrst single deep neural network which solves
multiple heterogeneous tasks related to eye gaze estimation
and semantic user understanding for an off-axis camera
setting. The tasks include eye segmentation, blink detection,
emotive expression classiﬁcation, IR LED glints detection,
To train EyeNet
pupil and cornea center estimation.
end-to-end we employ both hand labelled supervision
and model based supervision. We benchmark all tasks
on MagicEyes, a large and new dataset of 587 subjects
with varying morphology, gender, skin-color, make-up and
imaging conditions.

1. Introduction

Eye gaze estimation and simultaneous understanding of
the user, through eye images, is a critical component for
current and future generations of head-mounted devices
(HMDs) for virtual and mixed reality.
It enables energy
and bandwidth efﬁcient rendering of content (foveated
rendering [10]), drives multi-focal displays for more
realistic rendering of content (minimizing accommodation
vergence conﬂict [24]), and provides an effective and non-
obtrusive method for understanding user’s expressions.

The typical setup in head-mounted Virtual and Mixed
Reality devices for eye gaze estimation is illustrated in
Figure 1. A set of four IR LEDs are placed in and around
the display and their reﬂections (glints) are detected using
the IR-sensitive eye camera, one for each eye. These

1

Figure 1: Off-axis IR camera setting in head-mounted
VR/MR devices. Also shown are some sample eye images
captured in this setting.

glints are used to estimate important geometric quantities
in the eye which are not directly observable from the eye
camera images. A set of example images from such a
setup is also shown in Figure 1. The four main semantic
classes, Face/Background, Sclera, Iris and Pupil, along with
the glints are clearly visible. As can be seen from these
examples, there can be a large angle between the user’s
gaze and the camera axis. This makes eye gaze estimation
challenging due to the increased eccentricity of pupils,
partial occlusions caused by the eyelids and eyelashes [31],
as well as glint distractions caused due to environment
illumination.

A standard geometric model of the human eye is shown
in Figure 2. The eye ball sphere encompasses the inner
corneal sphere and within the corneal sphere lies the pupil

Off-axiscameraGaze vector 
 
 
 
 
 
optimization results in accurate gaze estimation. The shared
learnt representation across several tasks enables robust
estimation of all the desired quantities for gaze estimation
while also amortizing the computational load.

Semantic user understanding from eye images involve
tasks such as accurate blink detection to enable multi-
focal displays
to switch from one focal depth to
another to reduce accomodation-vergence mismatch [24],
emotive expression classiﬁcation for ﬁne-grained avatar
animation for telepresence and to enable more nuanced
communication in mixed reality. Semantic segmentation of
the eyes by itself is useful to animate the eyes and eyebrows
of avatars.

it

is

In this work, we demonstrate that

indeed
possible to share computation across both appearance based
tasks (segmentation, glint, blink and emotive expression
detection) and 2D geometry computation (cornea, pupil)
while delivering accurate semantics and robust gaze
estimates as compared to stand alone geometric gaze
estimation methods. This work is also contemporaneous
with other attempts in computer vision to learn multi-
tasking networks for scene understanding [37].

To summarize, our contributions in this paper are as

follows:

1. EyeNet: The ﬁrst multi-task deep neural network
trained to jointly estimate multiple quantities
relating to eye gaze estimation and semantic user
understanding from off-axis eye images.

2. A computationally efﬁcient and robust estimation
approach arising due to shared feature representation
across both appearance based and geometric tasks
which is useful for resource constrained applications.
3. Benchmarking the performance of EyeNet on a large
and diverse dataset including variations in gender, race
and physiology.

4. The ﬁrst eye tracking dataset with all intermediate
ground truth for a diverse demographic of gender, race
and physiology.

2. Related Work

The problem of eye gaze or point of regard (PoR)
estimation [11] is most commonly studied in the context of
two application scenarios. The ﬁrst is for monitoring user
attention or saliency for content that is projected in modern
electronic devices such as laptops or phones [21, 20, 17].
These devices are held at a ﬁxed (often known) distance
from the user, and are able to image the whole face with
good resolution.
The task then is narrowed down to
estimating which part of the screen (x-y coordinates) the
gaze is directed towards. Recent work has shown successful
application of CNNs for this task [21, 20]. [21] in particular
proposes a CNN trained on a large dataset of 2.5M frames

Figure 2: An illustration of the standard geometric model
of the physiology of human eye.

opening. The optical axis of the eye connects the cornea
center and the pupil (opening) center. The visual axis or
gaze vector, for all practical purposes, is taken to be the
line joining the cornea center and the fovea at the back of
the eye. The angle (κ) between the gaze vector and optical
axis is assumed to be constant for each user. Estimating the
optical axis (pupil and cornea center) is the key problem
underlying gaze tracking. Per subject calibration can be
performed to transform the optical axis to the gaze vector.

axis)

(optical

Accurate gaze

estimation involves
appearance based computation (detection of 2D attributes
such as pupil and glints on the image) followed by
geometry based computation of cornea center, pupil center,
and gaze vector in 3D. Figure 3 illustrates the different
stages for both classical (ubiquitous) and learned (EyeNet)
eye tracking pipelines.

The classical – geometry-based – eye trackers rely on
hand engineered computer vision techniques to detect white
blobs for localizing glints [11] and boundaries for pupil and
iris [31] with many manually tuned thresholds. Following
the appearance based estimation, in the geometric stage,
the process of going from 2D image features to 3D gaze
involves an iterative optimization module which detects
glints and estimates the corneal center alternately. Given
two or more LED locations and their corresponding glint
pairs,
the iterative optimization will estimate the 3D
cornea center. Finally, to compute the optical axis, the
detected 2D pupil center is back projected to 3D given
the estimated 3D cornea center and assumptions about the
eye geometry. Here we refer the reader to [11] for the
complete derivations of the eye geometry based cornea,
pupil estimation methods.

EyeNet as compared to the classical pipeline is a multi-
tasking deep neural network. We show that by training such
a network appropriately it is possible to entirely eliminate
the need to hand design heuristic methods to detect
image features. Furthermore, EyeNet can simultaneously
estimate robust
initial values for glint positions and
cornea center, after which a standard gradient descent

2

optic axis of the eyevisual axispupil centerlensfovearetinaCenter ofcorneal curvaturecollected from ∼1500 subjects, and demonstrates a tracking
error of 1 − 2cm. In general, CNNs are being successfully
employed for saliency detection in images [22, 5, 18, 17].
The common theme behind all these approaches is that the
estimate is a 2D position or saliency map, and all of these
rely on appearance-only eye gaze estimation.

A second and different line of approach to eye gaze
estimation is to infer geometric quantities of the eye such
as pupil and cornea centers, and use them to estimate
the optical/visual axis (gaze) in 3D [11]. These methods
typically rely on appearance based features (pupil and iris
boundaries, glints, etc) to estimate geometric quantities,
based on assumptions from a geometric eye model [33].
The overall aim of these eye tracking systems is to estimate
the 3D gaze vector and the vergence/ﬁxation/point of regard
in 3D without recourse to a screen. Another important
difference with respect to the previous class of methods
is that such systems [3] do not image the entire face, and
instead have separate cameras to image each eye (similar to
the setup in Figure 1). Having individual low resolution eye
cameras that are off the visual axis makes the problem much
more challenging (increased eccentricity of the pupil/iris
boundaries, occlusions due to eye-lids and eye-lashes). For
such approaches with an off-axis camera setup data driven
end-to-end solutions seem to suffer from low precision [30].
Multiple approaches have been proposed to overcome
the challenges posed by the off-axis eye tracking setup.
Recent analysis-by-synthesis approach [34] uses large-scale
procedurally rendered synthetic data for nearest neighbor
based gaze estimation. While the results look encouraging,
the precision is not ideal. Shape based methods [32, 13]
use pupil and iris boundary shape to compute gaze. The use
of traditional computer vision techniques to estimate region
boundaries makes them fragile, with failures frequently
occurring under lighting variations and low image quality.
The Corneal reﬂection methods use external light sources
(LED) to create a pattern of glints on each eye [40, 11, 33,
36, 15]. The location of the LEDs and their corresponding
glint reﬂections in the eye are used to estimate the position
of cornea in 3D. This along with pupil position estimation
provides the optical axis of the camera. These methods tend
to have the highest precision in gaze estimation but rely on
traditional image processing to estimate pupil position and
glint locations, which compromises their robustness.

A recent work aims to train deep neural networks for
low latency gaze estimation using largely synthetically
generated and rendered eye images [19]. The results show
the promise of using synthetic data at large scale to perform
end to end learning when close attention is paid to eye
modeling and rendering images according to real camera
characteristics. EyeNet itself could beneﬁt from training
on such synthetic data when available. However, from
the point of view of gaze estimation, their experiments

were restricted to on-axis images with real data collected
in controlled settings which reduce noise in the ground
truth. In addition, their best reported results use per subject
training and testing on very few subjects. In practice though
the data collected in the wild for any user is usually noisy.
Unlike the data displayed on a screen at a ﬁxed distance, as
done in their experiments, real calibration targets are virtual
objects in complex environments. When users are prompted
to focus on them, invariably there exist errors due to loss
of concentration and distraction, and hence the quality of
the ground truth gaze is noisy. Overall, in comparison, our
experiments were conducted on a much larger scale and in
realistic settings representative of those found in the wild.
Even when noisy ground truth is used for training, our
results are competitive with state of the art trackers while
simultaneously providing a variety of useful inferences.

In this work, we aim to exploit the robustness of the
appearance based approaches and the precision of the
geometric methods (corneal reﬂection) to contribute to the
ﬁnal gaze estimation. To this end, our proposed EyeNet is
trained using supervision from human labelers to estimate
boundaries (segmentation), temporal events (blinks), key
points (pupil, glint detection). 3D cornea position training
uses the geometric eye model based supervision. The use of
a single network with a shared feature representation across
these correlated tasks (for a particular gaze, boundaries,
glints, pupil position, cornea position are all related)
provides implicit regularization leading to robustness.

3. Classical Eye Gaze Estimation

The algorithmic pipeline used in the classical gaze
estimation pipeline can be seen in Figure 3.
In our
implementation of this pipeline the ﬁrst step is to detect the
pupil, iris and glint boundaries in the input eye images. To
obtain the pupil and iris boundaries we use a trained deep
eye parts segmentation network (see Section 4.2.1) which
provides robust and accurate pixel-wise segmentation. A
pupil ellipse is ﬁt to the pupil segment boundary to derive
the 2D pupil center. The iris boundary is used to restrict
search for bright blobs corresponding to LED glints. Binary
maps are created by adaptive thresholding using the image
histogram. Then basic image processing steps are used [1]
to obtain bright blobs. Note that although there are only
4 LEDs on the HMD used in our experiments the number
of detected blobs can be greater or lesser than 4. An
ellipse is then ﬁt to each blob to obtain their centers.
the 3D position of the cornea center
In this pipeline,
and detection of the glints (location and association to
the correct LED i.e labelling) given the blob centers is
done simultaneously using iterative optimization based on
standard back-projection losses (see Section 4.3 for the
details). The estimate of the 3D cornea center is then used to
lift the 2D pupil center to 3D, once again by back projecting

3

Figure 3: Eye Tracking Pipeline. The top part illustrates general stages of eye tracking pipelines. Middle part shows the stages
of the ubiquitous classical baseline. Blue boxes indicate the classical counterparts that are replaced by EyeNet. Bottom part
shows a simpliﬁed (for illustration purpose) version of our network architecture, where green boxes indicate weight sharing.
Red boxes are the outputs of EyeNet with a single feature encoder network and six task decoders. Grey boxes corresponding
to 3D geometry and gaze mapping in both pipelines share the same geometric model of the eye but differ in ways they utilize
the model to estimate the desired quantities. (This ﬁgure is best viewed in color).

the 2D pupil center estimate to intersect with the cornea ball
of ﬁxed radius. The ﬁnal step is to use 3D cornea and pupil
estimates (i.e the optical axis) and ﬁt a standard polynomial
mapper learned from calibration frames to map the optical
axis to the gaze direction [33]. Note that in this pipeline
there are several hand engineered heuristic modules to
detect glints, pupil center, cornea center and reject gaze
estimation etc. These heuristics need to be updated by
In contrast, the trained
experts when new data arrives.
EyeNet model can robustly predict the required geometric
quantities in a feed forward manner using convolutions.
Furthermore, the model weights can be updated as new data
arrives by ﬁnetuning. This simpliﬁes both implementation
and code maintenance.

4. Eye Net

Gaze estimation has been shown to be a challenging
learning task for deep networks. Recent end-to-end learning
approaches [21, 30] have achieved relatively low gaze

tracking precision. While recent work [19] has sought to
overcome this with careful synthetic eye renderings it has
mainly done so for an on-axis camera setting and relatively
noise free real data. The low precision for end-to-end
approaches in real world settings can be attributed to the
fact that direct gaze regression completely ignores the eye
geometry which is a valuable prior. Inspired by the recent
work in SLAM and scene understanding research where
integrating geometry has proved to help deep nets get better
at depth estimation [8] and relative pose prediction [12,
39], we propose a multi-task deep network that estimates
intermediate quantities that geometrically relate to gaze
estimation. Our approach has several practical advantages:

1. Modularity:

the estimates from EyeNet work well
in conjunction with the classical eye gaze estimation
pipeline (see Figure 3). We demonstrate that EyeNet
predictions can be plugged in to the classical pipeline
and vice-versa towards the goal of analyzing gaze
estimation performance.

4

Blob LocalizationBlob LabelingFeature ExtractionGlint EstimationIterativeOptimizationGeometryOptimization3D Cornea2D Geometry3D GeometryPupil/Iris Boundray Detection Optical AxisVisual AxisMappingUser-specificCalibration3D Pupil2D Pupil CenterEllipse FittingClassicalEyeNet2D CorneaGlint Detection2D Pupil CenterSegmentationBlink DetectionBinary PredictionCxCy3D CorneaOptical AxisVisual AxisDeepGazeMapper3D PupilFeature Extraction and 2D Geometry3D GeometryMappingEye Expression Classification4 Class Prediction2. Flexibility: Intermediate results from our multi-stage
eye tracking model can drive other vital applications
in VR/MR. 3D Cornea center estimates can be used
as a proxy for eyeball center of rotation for rendering
in waveguide displays; eye segmentation is useful for
driving the eyes in avatar rigs for telepresence, emotive
expression classiﬁcation is important for enabling
realistic interactions with avatars.

3. Labeling: Ease of ground truthing: collecting large
quantities of gaze ground truth for a large number of
subjects can be noisy and strenuous. Here we decouple
the training of our intermediate predictions (pupil
and cornea estimation) from the ﬁnal 3D gaze vector
estimation. The network used for gaze estimation is
therefore small and hence we reduce the need for large
quantities of ground truth gaze. This idea has also been
successful for other visual tasks [35]).

4. Interpretability: Errors in predictions from end-to-
end deep networks can be hard to interpret. EyeNets
estimates at every stage of the eye gaze estimation
pipeline helps correlate the effects of each estimate on
the ﬁnal gaze prediction.

Figure 3 shows our overall network structure.
It
consists of a feature encoding base network and six task
branches for eye parts semantic segmentation, pupil center
estimation and glint localization, pupil and glints presence
classiﬁcation, 2D cornea estimation, blink detection and
emotive expression classiﬁcation. We describe each
component in detail below.

4.1. Feature Encoding Layers

The feature encoding layers serve as the backbone of the
multi-task EyeNet model, and they are shared across every
task. We employ ResNet50 [14], the standard state-of-art
image feature extraction network and a feature pyramid
(FPN) [27] to capture information from different scales.
The input gray-scale image size is 160 × 120 and we use
features from the topmost layer of the encoder (size 20 ×
15×256) as input to the task branches. The full architectural
details of EyeNet are provided in the Appendix.

4.2. Supervised appearance based task branches

There are three major appearance based tasks in
In the following, we
our multi-task learning model.
discuss their formal deﬁnition, sub-network structure and
associated training losses.

Background, Sclera, Iris and Pupil. For this task, we
take the last layer feature map from the encoder network
(see Section 4.1) and up-sample it using deconvolutional
layers to the same resolution as the input image, similar
to [4, 29]. The resulting four channel output is converted
to class probabilities using a softmax layer for each pixel
independently. The loss is then a conventional cross-
entropy loss between the predicted probability distribution
and the one-hot labels obtained from manually annotated
ground truth. Formally, we are minimizing the following
loss for a pixel x, y with ground truth class c and predicted
probability pk(x, y) for the kth class;

L(x, y) = −

4
(cid:88)

k=1

Ix,y[k == c] log pk(x, y),

(1)

where Ix,y[.] is the indicator function. The overall loss
is simply the sum of the losses over all pixels in the
image. The segmentation task serves as a bootstrap phase
for training the feature encoder layers as it captures rich
semantic information of the eye image. By itself, eye parts
segmentation can help the initial phase of any classical
pipeline in terms of localizing the search for glints (using
iris boundary) and to estimate pupil center (using pupil
boundary). It can also be useful for rendering eyes of digital
avatars.

4.2.2 Pupil and Glint Localization

The pupil and glint localization branch gives us the pixel
location of the four glints and pupil center, i.e a total of
ﬁve keypoints. The network decoder layers for these two
tasks are similar to the eye parts segmentation branch and
predict a set of ﬁve dense maps at the output corresponding
to the ﬁve keypoints. Each dense map is normalized to
sum to unity across all the pixels. A cross-entropy loss
is then calculated across all the pixels of each map during
training. Once trained, the location of the center of the pupil
or a particular glint is the pixel corresponding to maximum
probability at the output. Speciﬁcally, we are minimizing
the following loss for every keypoint (four glints and one
pupil center):

L(keypoint) = −

(cid:88)

x,y

I[x, y] log px,y,

(2)

4.2.1 Eye Parts Segmentation

Eye parts segmentation is deﬁned as the task of assigning
every pixel in the input image a class label from one of

where I[.] is an indicator function that is zero everywhere
location, px,y is
except for the ground truth keypoint
the predicted probability of the keypoint location and the
summation is over all the pixels in the image.

5

4.2.3 Pupil and Glint Presence Classiﬁcation

In realistic settings, often glints and/or the pupil center
can be occluded by the closing of eyelids, nuisance
reﬂections can appear as glints and for some gaze angles
glints may not appear on the reﬂective corneal surface.
Therefore it is important to learn to classify robustly the
presence or absence of glints and the pupil center. These
predictions effectively gate whether a glint should be used
for cornea center estimation and similarly for 3D pupil
center estimation.

We take the topmost layer feature map from the EyeNet
encoder, use one convolution layer to reduce the number of
feature channels, reshape it to a one dimensional array and
add one trainable fully-connected layer of size 1500 × 10
to produce a 5×2 sized output. Each pair represents the
presence, absence probability for one of the four glints
and/or the pupil center. We use a binary cross-entropy loss
to learn from human labeled ground truth.

4.2.4 Blink detection

Detecting blinks is an appearance based task which is useful
to drive multi-focal displays and/or digital avatars. Blinks
are usually captured across a sequence of images so it is
necessary to use temporal information to distinguish blinks
from events such as saccades (rapid sideways movement
of the eyes). In general, it is hard to accurately locate the
blink event in which the eyes are fully closed, particularly
at the standard 30fps frame rate. In other cases, it may be
important to detect the onset of blinks to reduce latency
between detection and application. We, therefore, use a
simple deﬁnition of blinks as the state of the eye when the
upper eyelid covers over 50% of the entire pupil region.
This we found to be useful as a working deﬁnition for non-
expert human labelers. Given the aforementioned deﬁnition
of blinks, we ﬁnd that features from EyeNet trained for tasks
such as eye segmentation transfer well to the blink detection
task.

layer

We use features from the topmost

(shared
representation) of the pre-trained feature encoding network
to train the blink detection branch. As shown in Figure 4,
the features from three continuous time steps at−2, at−1, at
are fed to a simple 3 layer fully connected network that
classiﬁes the current frame (at time t) as being a blink or an
open eye. We experimented with longer temporal window
lengths but this gave diminishing returns in prediction
accuracy. RNNs and LSTMs have similar train and test
performance, therefore we chose to settle with this simple
architecture given the lower compute requirements.

Figure 4: Blink detection at frame t. Features from the
EyeNet encoder for three consecutive frames at−2, at−1, at
are the inputs. Blue boxes indicate fully connected layers
seperated by ReLU, and (cid:76) indicates concatenation.

4.2.5 Facial Expression Classiﬁcation

The facial expression classiﬁcation task involves classifying
the user’s emotive expressions from the input eye images.
The task is particularly challenging because we only
have the user’s eye regions as input rather than the
entire face, as used in most emotive facial expressions
classiﬁcation benchmarks [26]. To our knowledge, only
two previous studies have identiﬁed facial expressions
in these studies a view of the
from eyes, however,
eyebrows was available [25] [16]. Eyebrows are considered
important for emotive expression. The present problem
is more challenging, as we aim to estimate expressions
without a view of the eyebrows. Here, we consider the
following individual emotive facial expressions: happiness,
anger, disgust, fear, and surprise. We grouped these
into 3 discrete states: positive dimension (happiness),
discrimination dimension (anger and disgust), sensitivity
dimension (fear and surprise), and a neutral dimension.
This classiﬁcation was motivated by previous work on how
humans read complex emotional and mental states from
the eye region [25]. Due to the fact that it is difﬁcult
for even human viewers to detect the subtle differences
between individual emotive expressions solely from the eye
regions [25], we only report classiﬁcation results on the
3 dimensions. Like the other task branches, we ﬁxed the
main encoder trunk of EyeNet and only trained the facial
expressions task branch, consisting of several FC layers, for
expression classiﬁcation. Note that we train this task branch
for each subject to produce a personalized model. We
found that training a general model for a large population
of subjects results in poor accuracy.

4.3. Model Based Learning for Cornea Center

Estimation

The center of the cornea is a geometric quantity in 3D
which cannot be observed in a 2D image of an eye. Hence,

6

at-2at-1at3 x R64yt+R64unlike pupil (center of pupil ellipse) or glint labeling, it is
not possible to directly hand label the projected location of
the 3D cornea center on the image. We therefore propose
a two-step method to train a cornea 2D center prediction
branch for EyeNet. First we use well known geometric
constraints and all necessary known/estimated quantities
(LED, glints) to generate cornea 2D supervision. Then
we train the 2D cornea branch using this model based
supervision obtained for each frame.

Predicting the cornea using the learned network has
two main beneﬁts over using geometric constraints during
evaluation. First it’s more robust because deep networks
have a tendency to average out noise during training
and standard out-of-network optimization can occasionally
yield no convergence. Second it only incurs a small and
constant time feed forward compute since the cornea task
branch consists of only a few fully connected layers.

Figure 5: An illustration of the geometric setup used
to derive the model based supervision for cornea center
training. The corneal sphere along with the camera image
plane are shown. According to the law of light reﬂection,
the incident ray L1G1, the reﬂected ray G1O and the normal
n1 at the point of incidence/reﬂection are co-planar. The
same holds for the other LED. By this constraint, the cornea
lies on the ray OC intersecting the two planes.

Figure 5 shows the geometric setup used to derive the
model based supervision for training the cornea branch.
The corneal sphere, two sample LED locations L1, L2 in
3D, the camera center O and the image plane are shown.
Their incident rays L1G1 and L2G2 and the corresponding
reﬂected rays G1O, G2O can be seen along with the
normals n1, n2 at the points of incidence/reﬂection. Given
the cornea is a reﬂective surface it follows from the laws

7

of light reﬂection an incident ray from a point source, e.g.
L1G1, reﬂected ray G1O and the normal n1 at the point
of incidence/reﬂection on the sphere G1 are all co-planar.
Equivalently the corneal center C, LED L1, the projection
of the incident point G1 onto the image plane denoted g1
all lie on the same plane. This holds for other LED point
sources too. More details can also be found in [33].

In the presence of multiple LED’s it is easy to see from
Figure 5 that the cornea center lies on the intersecting line
OC. Line OC lies on all of the planes formed by each
pair of LED and corresponding glint normal. This forms a
linear system whose solution is the cornea ray. To construct
this linear system, we use the cross product of estimated
glint normal OG and LED vector OL which provides the
corresponding plane equations and then use SVD to solve
this linear system. The solution of the linear system is a 3D
ray on which the 3D cornea center lies. We intersect this
cornea ray with image plane to get the cornea 2D point on
an eye image. With this supervision we apply a smooth L1
loss [9] to train the cornea branch of EyeNet.

Given the physiology of an individual’s eyes is unique,
we ﬁnd it necessary to train the cornea branch to each
subject (personalization) in order to produce the most
accurate results, sharing the same idea with many recent
eye tracking works [38, 28]. More details are provided in
Section 5.4.1. We now show how to convert the cornea ray
prediction from EyeNet to 3D coordinates.

4.3.1 Cornea Optimization

One of the key elements in the classical pipeline is
performing cornea position optimization (both 2D and 3D
positions) along with glint detection for each frame starting
from a set of blob hypotheses. To do so it uses the geometric
model described in Section 4.3 and a corresponding back
projection loss described later in Section 4.3.2.
This
optimization is performed for each frame independently
during test time and directly leads to more accuracy in the
ﬁnal gaze estimate.

Inspired by this online optimization step, we introduce a
variant of EyeNet termed EyeNet-Opt that does a per-frame
optimization of the glint and cornea 2D position during
test time. The key differences are; 1. unlike the classical
optimization which is used to label the glints we rely on the
glint labels predicted by EyeNet as our experiments show
they are quite robust, 2. we only use simple gradient descent
optimization on a loss derived from the eye geometric
model in Figure 5.

From Figure 5 we note that the line joining the projection
of an LED onto the image plane li and its corresponding
glint location gi on the image plane intersects the cornea
ray OC due to the co-planarity constraints discussed in
Section 4.3. A pair or more of such lines will intersect at

a discretized search (out-of-network optimization). The
details are described below.

We begin by instantiating a sphere of known radius ‘r’
around a hypothesized cornea center in 3D C3D (see Eqn 3)
by using the inferred 2D cornea center as well as an initial
guess for its ‘z’ dimension:

(cid:107)x − C3D(cid:107)2

2 = r2.

(3)

the cornea center projection (cornea 2D) on the 2D image
plane. The image plane view of this model based argument
is shown in Figure 6. We use this constraint to jointly
optimize for the cornea 2D and glint positions on the image.
The loss we employ is minimizing the distance from
cornea 2D to all li, gi lines as shown in Figure 6. Here
we jointly optimize the cornea 2D location and the glint
locations (excluding glint labels) to reach the lowest overall
average distance from cornea 2D to all valid li, gi lines. In
practice, we perform 100 steps of gradient descent on each
test frame. Our observations are that the initial values of the
cornea 2D and glints from EyeNet are robust and lie close
to the ﬁnal value. This allows for conﬁdent optimization.

We note in passing here that we also attempted to employ
this projection loss to directly train the cornea 2D branch
assuming glint detections from EyeNet and/or hand labelled
glints. This method unfortunately did not yield satisfactory
results.

Figure 6: Projections of the LED, cornea 3D location onto
the image plane shown alongwith the glint positions. The
cornea 2D position should ideally lie on the intersection
of all four LED-glint lines. We optimize both cornea 2D
and glints positions to minimize the average distance from
cornea 2D position to all the glint-led lines.

4.3.2 Lifting 2D Cornea Center to 3D

Figure 7 depicts the fact that the 3D cornea center lies on
a ray passing through the 2D cornea center and the camera
center. Therefore, the search for the 3D cornea center is
constrained to a single dimension on this cornea ray. At
each hypothesized cornea 3D location along this ray, we
can compute the ray passing through a 2d glint e.g. g1
and its intersection with the corneal sphere (of assumed
radius r). This ray can be reﬂected about the normal at the
point of intersection G1.
If the hypothesized corneal 3D
location is correct this reﬂected ray should pass very close
to the known LED location L1. This idea is converted to
a loss function over multiple LEDs and minimized using

Figure 7: Estimating the cornea 3D position by a line
search along the ray connecting the 2D cornea position
and the camera center. The optimal position of the 3D
cornea, according to the spherical model, is the one which
minimizes the distance of the reﬂected glint rays to the
known LED positions. Here the corneal radius is assumed
to be 8mm.

As we see from the geometry in Figure 7, a ray from the
origin that passes through one of the glints should intersect
the cornea sphere if it is located at some nominal camera to
eye distance. The direction of such a line is given by the
glint unit vector (ˆg) in camera centered coordinates. Its line
equation is given by,

y = ˆgt,

(4)

where ’t’ is a scalar.
If the estimate of the 3D cornea
position is correct, this ray can be reﬂected about a normal

8

g1g2g4g3led1led2led3led4Cornead1d2rC3DCameraCenterImage Planed1L1d2L2Z dimC2Dg2g1at the surface of the cornea which would give us a ray along
which the corresponding LED should lie [6]. The point
of intersection G3D between the glint line and the cornea
sphere is given by,

G3D = ˆgt∗,

(5)

where t∗ is obtained by solving for ’t’ in Eqn 3 and 4:

t∗ = ˆg · C3D +−

(cid:112)(ˆg · C3D)2 − (C3D · C3D − r2).

(6)

Note that t∗ must be positive. At the point of intersection,
we estimate a normal and a reﬂected ray as given by the
following equations:

and

ˆn =

C3D − G3D
(cid:107)C3D − G3D(cid:107)

,

ˆr = 2 ∗ (ˆn · ˆg)ˆn − ˆg.

(7)

(8)

Measuring the distance between the actual LED positions
and their respective reﬂected rays from cornea sphere gives
us a loss that we minimize to ﬁnd the optimal ’z’ for the
cornea 3D:

L(C3D) =

1
2

4
(cid:88)

i=1

d2
Li

(C3D),

(9)

where dLi is the distance between the reﬂected ray ˆr and
the respective LED L3D:

dL(C3D) =

(cid:13)
(cid:13)
(cid:13)(G3D − L3D) − (cid:2)(G3D − L3D) · ˆr(cid:3)ˆr
(cid:13)
(cid:13)
(cid:13)2
(10)

.

Note that in the above equation we have dropped the
LED/Glint index for brevity. Also, in practice all the four
glints may not be detected due to occlusion or presence
of distractors. Therefore the sum in Eqn 9 is only over
valid glints.
It is important to note that the glint unit
vectors would not intersect the cornea sphere if the cornea
Z position is too close to or too far from the camera. This
implies the loss in Eqn 9 is not differentiable outside of
a small range of camera to eye distances. This makes
it difﬁcult to train EyeNet to directly minimize the loss.
Therefore, we use a straightforward discretized search
along the 1D ray to locate the optimum 3D cornea position.
In our setup, as show in Figure 1 the ’z’ distance between
the camera and the cornea sphere is usually bounded
between 1 and 5 centimeters. We compute the loss
described in Eqn 9 for every ten thousandth of a centimeter
between the upper and lower bounds and pick the ’z’ value
which minimizes the loss.

4.4. Gaze Estimation

Following the illustration in Figure 2, given the estimate
of the pupil center on the image plane and the cornea in
3D, we can connect the camera center and pupil 2D to form
the pupil normal, where the pupil center lies. We extend
the pupil normal to intersect with the corneal ball to get
an approximate pupil center in 3D. By connecting pupil 3D
and cornea center 3D, we get an estimate of the optical axis.
We learn to estimate the gaze direction or equivalently
the visual axis by training a gaze mapping network termed
as the DeepGazeMapper. This deep network takes as
input the optical axis direction and outputs the visual axis
direction. Speciﬁcally, the visual axis is deﬁned as the
line connecting cornea 3D center to the calibrated visual
target location. In practice, we transform the optical axis
and visual axis to the device coordinate system and unit
normalize them before training. When the cornea 3D is also
transformed to this coordinate system the gaze vector can be
drawn through the cornea center and displayed.

In existing methods, a mapping function in the form of
a hand prescribed second order polynomial function is ﬁt
to map pupil position to the 3d target position, effectively
producing the visual axis [11]. It is also possible to use a
second order polynomial function to map the optical axis
to the visual axis as is done in our implementation of the
classical pipeline. This function is learnt for each subject
using the calibration frames. We have shown here that this
function can be replaced with a simple fully connected deep
network (5 layers and 30K params) also trained on only the
calibration frames, described in experiments section.

5. Experiments

5.1. Dataset

The device setup for data collection with an off-axis
camera is shown in Figure 1. We collected an extensive
in-house dataset of 587 subjects across different genders,
demographic groups with different physiology such as skin
and eye colors. Table 1 shows the distribution of subjects
across different ethnicity and eye colors. The distribution
of subjects is nearly even between both genders.

Brown Hazel Blue Green Other
2%
63%

16% 11%

8%

Caucasian African descent Hispanic Other
4%

12%

59%

25%

Table 1: Eye color and ethnicity distribution of subjects.

In the data collection phase, each subject faces a virtual
3x3 grid of points at six distinct depths of 0.33m, 0.5m,
1m, 1.5m, 2m, 3m. On a given cue, the subject is asked to
focus their gaze on one of these 54 3D points as shown in

9

Figure 8: EyeNet training pipeline. Blue boxes indicate steps where all tasks are jointly trained. Green boxes indicate steps
where only the speciﬁc task branch is ﬁne-tuned and the main encoder is frozen. Red boxes means quantities generated by
the network.

Figure 10. This provides the ground truth gaze vectors for
each frame in the dataset. However, we note that there is
diminishing returns in annotating segmentation, glints and
pupil centers for every frame at 30 or 60 Hz recordings. We
uniformly sample 200 left or right eye image frames in each
subjects recordings to manually annotate segmentation,
glint presence or absence, glint 2D and pupil 2D positions.
Overall, we have 87,000 annotated images with all types
of ground truth in our dataset which is used to train and
validate our EyeNets results. We split this dataset into
334 training subjects and 93 validation subjects to train the
network. We use a test set of 160 subjects with gaze target
ground truth (980K images) to demonstrate the network’s
ability to generalize across different subjects.

5.2. Labeling

We presented annotators with 2D eye images and
polygon marking, ellipse ﬁtting tools. Each of the eye parts,
pupil, iris, sclera and background are segmented by ﬁnely
marking the vertices of their corresponding polygon. The
pupil center is then derived by ﬁtting an ellipse to the pupil
polygon vertices and noting its center. To annotate the glint
locations and labels (association with the corresponding
LED) in a particular frame, human labelers use past and
future frames to mark the glint blobs and also note the
presence or absence of a glint. Each glint location is then
obtained by ﬁtting an ellipse to the marked glint blobs.
The glint labels are assigned by observing their relative
positions.

For blink detection, we collected realistic eye motion and
blink data from 65 subjects. The annotators were presented
with a temporal sequence of eye images and asked to
annotate individual eye frames. The images were labeled
as blink if more than 50% of the pupil is covered. Since it’s
tricky to accurately annotate 50% pupil covered cases, the
labelers were allowed to label each frame as either blink,
open-eye, or unsure. The ”unsure” labels were ignored for
training and for computing accuracy metrics. A total of 40
subjects were used for training and 25 subjects for test.

For the facial expression classiﬁcation, we collected data
from 15 subjects. We trained subjects to perform facial
action units and labelled these data using the Facial Action
Coding system, a widely recognized method for coding
individual facial muscle movements [7]. These facial action
units can then be mapped onto emotive expressions.

5.3. Calibration

In the calibration phase for each user we use 9 targets (on
the 0.5m plane) out of the 54 to personalize (ﬁne tuning) the
2D cornea branch of EyeNet and the gaze mapping net. The
remaining 45 targets are used in the evaluation phase.

5.4. Training and Testing Procedures

The complete training takes several steps because our
framework receives ground truth from different sources as
well as a model based supervision which requires estimates
from the trained network itself.
Figure 8 shows the
overview of the whole training process. The ﬁrst step is
to train the ResNet [14] encoder-decoder network with eye
segmentation labels because it provides the richest semantic
information and is the most complicated supervised task
to train accurately. Secondly, we use human labeled glint,
pupil 2D center data 1 and eye segmentation data together
to jointly train all of these three supervised tasks. We ﬁnd
that initializing with weights trained from eye segmentation
results in much more stable training than from random
initialization. After this step, we generate glint predictions
for all frames in MagicEyes and use these along with known
LED locations to generate cornea 2D GT for training the
cornea branch as discussed in Section 4.3. We emphasize
here that the cornea branch is trained with data from the
whole training set population.
It is further personalized
(ﬁne-tuned) at the per subject calibration phase.

We use the predicted cornea (personalized) and pupil 3D
centers from the calibration frames to deduce the optical
axis. Using the gaze targets ground truth of calibration

1Obtained by ﬁtting an ellipse to the human labelled pupil boundary.

10

Train Eye SegmentationTrain All Supervised TaskManual GT(Sub-sampled)Deep GlintsTrain PersonalizeCorneaGenerateDeep PupilEstimateCornea GTOptical AxisModel Based Supervision(All Frames)Train DeepGazeMapperGazeframes, we train the gaze mapping net to transform the
optical axis to the visual axis. During test time, we obtain
the predicted cornea and pupil 2D centers from EyeNet.
These quantities we lift to 3D (see Section 4.3.2) and get
the optical axis, which is then fed into the gaze mapping net
to infer the predicted gaze direction.

The blink and facial expression classiﬁcation tasks are
trained on top of intermediate features of the main feature
encoding branch. Blink detection is a temporal
task,
we pass three consecutive eye images and extract their
intermediate features. With a set of pre-computed features
the blink detection branch is trained separately, while the
main feature encoding branch of eyeNet remains frozen.
For facial
Similar procedure is followed at test time.
expression classiﬁcation, we freeze the main feature trunk
and only train the expression classiﬁcation head using
expression data. The expression predictions are produced
along with all other tasks during testing time.

5.4.1 Personalizing Cornea 2D Prediction

Estimating the position of the cornea is a subtle regression
task that requires high precision for estimating the 2D
location of the cornea (and subsequently its 3D position)
so that the visual axis can be estimated accurately. It is a
fact that the physiology (e.g radius) of the cornea differs
among subjects and hence EyeNet’s predictions need to be
personalized to each subject for best performance.

To personalize the cornea 2D branch for each test
subject, we generate predicted glints for all calibration
frames of the test subject using the trained EyeNet model.
As described in Section 5.3 we only use a part of the
calibration frames of each test subject to ﬁne-tune the
cornea 2D branch using the model based supervision (see
Section 4.3). The remaining non-overlapping set of frames
from other calibration targets are used for benchmarking the
performance of each personalized EyeNet model. Note also
that predictions from this personalized cornea 2D branch
provide the initial values for cornea 2D optimization for
each frame during test time (see Section 4.3.1).

5.5. Metrics and Results

We measure the performance of predictions from each
branch on the personalized EyeNet models. These help
understand the errors contributed towards the goal of
predicting the users gaze in 3D. All the quantitative results
are reported by averaging ﬁve runs of training and testing.

5.5.1 Eye Segmentation

Eye segmentation accuracy is reported using the classic
confusion matrix metrics shown below with an averaged
trace of 97.29%. The qualitative results are shown in
Figure 9.

GT\Pred
Pupil
Iris
Sclera
BG

Pupil
96.25
0.04
0.00
0.01

Iris
3.75
99.03
3.27
0.72

Sclera
0.00
0.93
96.71
2.09

BG
0.00
0.00
0.02
97.18

Table 2: Eye Segmentation confusion matrix in percentage
values. The accuracy for all four classes are over 97.29%.
EyeNet has very few mis-classiﬁcation of the pixels to their
rightful semantic class.

The eye segmentation result is very accurate in terms
of both quantitative and qualitative evaluations. This is
important since the segmentation boundaries are used to
generate precise pupil 2D center location training data,
particularly for the partially occluded pupil cases, by
carefully tuned ellipse ﬁtting procedures. The segmentation
predictions are also used by the classical geometric pipeline
which we use as a baseline in this work for gaze estimation
comparisons.

5.5.2 Pupil and Glint Detection

The pupil accuracy is deﬁned as the Euclidean distance
between the human labeled (or algorithmically estimated)
ground truth pixel 2D center location and the predicted pixel
location from EyeNet. For the glints, the order (label) of the
glint matters signiﬁcantly in the cornea estimation phase
(glints need to be paired with their corresponding LEDs).
We therefore deﬁne the metric as the labeled Euclidean
error (LEE):

LEE =

4
(cid:88)

(cid:107)Gi − gi(cid:107)2,

i=1

(11)

where the index of the predicted glints gi is required to be
given by the network to match the ground truth glints Gi. If
the network or classical method labels the glints incorrectly,
it would be penalized based on this metric.

The qualitative result of pupil and glint is shown in
Figure 9. The estimated pupil center is indicated by a red
dot, whereas the four glints in order are shown in blue,
yellow, pink and green clockwise for left eye (counter-
clockwise for right eye).

Table 3 shows the quantitative result compared to the
output of the classical pipeline in pixel errors. When
the images are from ideal settings, EyeNet and classical
predictions are all precise with close-to zero errors.
However, when the images have severe reﬂections or the
users gaze is away from the central targets (see Figure 10),
EyeNet is able to ﬁrst detect the presence or absence of the
glints very accurately, and provide robust labeling of the
glints, whereas the classical approach suffers from inferior

11

Figure 9: Qualitative Results for EyeNet (Best viewed in color). First row shows sample input eye images. Second row shows
the predictions from EyeNet. Third row shows the ground truth obtained from human labelers. Col A,B,C show EyeNet’s
high quality predictions in frontal facing images. Col D,E demonstrates robustness of the predictions to partial occlusions
and small sized pupils. Col F demonstrates the robustness to intensity changes for EyeNet. Col G,H indicate the difﬁculty
faced by human labelers to pinpoint glint locations under signiﬁcant reﬂections. However, with sufﬁcient training EyeNet is
capable of ignoring nuisance reﬂections. Col I shows that erroneously labelled pupil center training data can affect EyeNet’s
predictions.

absence indication and mis-labeling of the glints, resulting
in much higher error under our labeled Euclidean error
metric.

Classical
Localization
in pixels
0.64
1.21
1.08
0.84
0.78
0.86

EyeNet
Localization
in pixels
0.46
0.47
0.39
0.23
0.37
0.38

Classical
Presence/
Absence
92.81%
90.16%
90.84%
92.14%
91.56%
91.72%

EyeNet
Presence/
Absence
99.61%
96.94%
96.32%
96.85%
96.34%
98.06%

Pupil
Glint 1
Glint 2
Glint 3
Glint 4
Avg

Table 3: Pupil and glints localization and presence-or-
absence (PoA) classiﬁcation. The ﬁrst two columns are
localization LEE of EyeNet and classical predictions. The
ﬁnal two columns show the PoA classiﬁcation accuracy.
EyeNet provides much more precise PoA classiﬁcation and
this in turn produces more accurate and robust localization.

5.5.3 Cornea Center Estimation

It is hard to obtain ground truth for the cornea center.
Therefore we compare the Euclidean distance of the EyeNet
predicted cornea center to the cornea center obtained by the
classical pipeline. On an average, the direct EyeNet cornea
predictions are 1.77mm away from the classical pipeline
predictions with a standard deviation of 2.97mm. This
distance is reduced post cornea optimization to 0.99mm
with a standard deviation of 2.10mm, a clear improvement

Figure 10: Visualization of gaze targets on each of the
6 planes used in our experiments. The Z value is the
distance from the user. The second plane targets are used
as calibration targets. As expected the mean angular error
increases as depth increases.

that also boosts the ﬁnal gaze accuracy shown in Table 4.
The high standard deviation is due to a few high error
outliers, which are averaged out by the deep gaze mapping

12

ABCDEFGHIPupil CenterGlint 1Glint 2Glint 3Glint 4Model
1
2
3
4
5

Description
Classical
Classical-DeepGazeMapper
EyeNet-DeepGazeMapper
EyeNet-Opt-DeepGazeMapper
EyeNet Glints & Pupil -SVD Cornea-DeepGazeMapper

Mean AE Std AE Q1 AE Q2 AE Q3 AE
290.27
264.16
327.98
258.30
269.05

52.85
65.39
147.27
102.27
92.56

101.34
131.50
231.96
167.89
156.51

204.98
183.43
238.85
186.41
193.51

240.17
153.11
114.88
105.81
131.53

Table 4: Gaze Angular Error (AE) for EyeNet, Classical and mix-and-match models in arcmin units. Note 60 arcmin is 1
degree. Q1, Q2, Q3 correspond to the 25th percentile, median and 75th percentile error respectively. The overall mean and
standard deviation of the error is also listed. Here Opt corresponds to cornea 2D optimization using gradient descent with
initial values estimated from EyeNet. ClsMapper indicates use of the standard polynomial mapping for gaze estimation. SVD
Cornea computes the 3D cornea position given pre-computed glint locations and labels.

Deep Mean Classical Mean Deep Std Classical Std

Top Left
Top Middle
Top Right
Center left
Center Middle
Center Right
Bottom Left
Bottom Middle
Bottom Right

194.18
169.64
184.95
195.15
183.57
193.35
205.55
179.15
181.35

261.05
148.28
162.57
298.44
147.15
161.74
300.94
154.19
166.15

105.63
103.53
109.16
105.78
106.18
108.62
105.56
100.55
103.47

304.42
143.25
154.25
331.17
143.11
151.77
323.00
146.39
161.19

Table 5: Gaze error for each of the 9 target direction aggregated over all the target planes. It is clear that EyeNet predictions
are have smaller and consistent standard deviation across the ﬁeld of view. This is due to the robustness of EyeNet’s estimates
and the use of the DeepGazeMapper.

estimate may not be a good approximation to the true center
of rotation of the eyeball from which the gaze is referenced.
Indeed, getting closer to the classical estimate improves
the ﬁnal gaze estimate but more work needs to be done to
estimate a ’ﬁxed point’ for gaze computation which is stable
over the desired ﬁeld of view of the gaze driven application.

5.5.4 Gaze Estimation

The overall gaze estimation metric is deﬁned as the angular
error between the true gaze vector and the estimated gaze
vector in arcmin units. Figure 10 shows the targets used in
our experiments as coming from various depths. Note that
there are only 18 unique directions although there are 54
targets. This setup was chosen to study the gaze errors in the
same direction over increasing depths. Table 4 compares the
performance of EyeNet to the ubiquitous classical baseline
and other mix-and-match variants.

From the results in Table 4 we can see that the Classical
pipeline (model 1) has the lowest 25th percentile and
median error. However, both the overall mean and standard
deviation (std) is high. The use of the DeepGazeMapper
signiﬁcantly lowers the error variance, including that for
the classical model (model 2), with only a small impact

Figure 11: Error histogram for gaze estimation. EyeNet
results are more centered near average accuracy without
many outliners. Classical approach has more low error
frames but is also more spread resulting in potential jitters.

network during the gaze estimation phase.

It is important to note here that the classical cornea center

13

0200400600800100012001400Angular Error in arcmin0k50k100k150k200k250k300k350k400kFrame CountEyeNetClassicalto accuracy. This shows the DeepGazeMapper by itself
can provide signiﬁcant improvements to stability over the
standard polynomial mapping. In most applications which
use eye tracking in HMDs it is generally preferrable to avoid
jittery tracking while compromising a little on the accuracy.

EyeNet (model 3) estimates have low std but overall
higher error than classical. This implies that joint end-to-
end learning for estimates such as the cornea 2D, glints,
can be used for applications which do not
pupil etc.
have severe accuracy demands. This result however is
still a signiﬁcant
improvement over recent attempts to
directly regress gaze by end-to-end learning [30]. When
the accuracy demands are higher, it is prudent to use these
estimates as starting values for simple gradient descent
optimization for the 3D cornea position (see Section 4.3.1).
As seen from the results for model 4 this improves
the accuracy signiﬁcantly while also delivering improved
robustness.

If the main goal is robust and accurate gaze estimation
in a narrow ﬁeld of view (typically corresponding to data
which make up the Q1, Q2 percentiles) then using EyeNet
pupil and glint estimates to derive the cornea position out-
of-network using SVD lowers the gaze error as in model 5.
However the demands on eye tracking will be over a larger
ﬁeld of view corresponding to newer displays on HMDs.

In Table 5 we report the angular error per direction
(averaged over all depths) for models 1 and 4. It is clear
that model 4 standard deviation is signiﬁcantly better and
similar in all directions. This can primarily be attributed to
robust glint, cornea 2D estimates along with the use of the
DeepGazeMapper.

Figure 10 shows the gaze error for model 4 over
It is clear that the error increases
the different planes.
with depth but there is approximately 20 arcmin increases
between the targets at 0.5m and 3m. This once again
indicates the robustness of EyeNet.

The histogram of gaze angular errors for both model 1
and 4 are displayed in Figure 11. Here we see the EyeNet
error distribution has a shorter tail than the classical pipeline
distribution. The classical pipeline has better accuracy,
primarily due to iterative optimization but poorer robustness
as indicated by the very long tail in the histogram.

Based on all these experiments and result analysis, we
ﬁnd that accurately estimating cornea position from eye
images remains the most sensitive and challenging part in
the whole pipeline. During the training runs we ﬁnd that
outliers in the training data have a signiﬁcant impact on
cornea 2D estimation for example. With more carefully
designed gaze data collection protocols we believe its
possible to further reduce the error in EyeNet estimates
towards the goal of gaze estimation.

GT/Pred
Neutral
Happy
Discrimination
Sensitivity

Neutral Happy Discri.
94.92
2.46
0.71
93.74
0.17
5.85
92.64
0.52
6.57
0.30
0.23
9.78

Sensitivity
1.90
0.25
0.28
89.69

Table 6: Emotive expression classiﬁcation confusion matrix
in percentage. The mis-classiﬁcation to neutral class is
mainly due to data and label noise.

5.5.5 Blink Detection

The blink detection branch takes in EyeNet encoder features
for 3 consecutive frames which help to noticeably reduce
false positive blink predictions that occur when using a
single eye image. Saccades and other partially closed
eye events are better distinguished in our temporal blink
the
detection network. On a test set of 25 subjects,
blink detection branch of EyeNet has a false positive rate
of 1.24% and a false negative rate of 4.01%, indicating
the success of sharing features across multiple tasks.
In
comparison [23] reports a false positive rate of 8.3% and
a false negative rate of 16.7% using the publicly available
Pupil capture [2] software.

5.5.6 Facial Expression Classiﬁcation

We show accurate emotive facial expression classiﬁcation
into 3 discrete dimensions
(Happy, Discrimination,
Sensitivity), and Neutral. We report the classiﬁcation in a
standard confusion matrix below and an overall accuracy of
92.75%.

The expression classiﬁcation is reasonably accurate
given that we are predicting it solely using the eye region.
There is also a certain degree of data noise as it is difﬁcult
for some subjects to produce eye expressions that differ
between classes.

The ability to recognize emotive expressions with the
eyebrows obscured is important because eyebrows are
generally considered critical for emotion expression [25].
The positive dimension captures smiling (happiness), the
discrimination dimension captures the narrow-eyed state of
suspicion (anger and disgust), and the sensitivity dimension
captures the wide-eyed state of awe (fear and surprise).
These categories are consistent with previous classiﬁcations
where eyebrow data was available [25]. In Figure 12, we
showcase sample eye images of positive, discrimination,
and sensitivity dimensions.

6. Computational Efﬁciency

We benchmark our model running time on a Linux
machine with dual Intel E5-2660 CPU and one Nvidia 1080
Ti GPU. We use Pytorch along with CUDA 9 and CuDNN

14

happy

discrimination

sensitivity

Figure 12: Sample eye images of various expression
dimensions.

7 as the software frameworks. The average running time
is 12ms or 83fps for all EyeNet tasks inference calculated
over 1000 iterations. The choice of ResNet50[14] over
ResNet101 or ResNet152 and an input size of 160x120
instead of higher resolution is to be able to deploy the
model to AR/MR hardwares for application in the near
future. Please refer to the appendix for the full network
architecture.

7. Conclusion and Future Work

In this work, we presented EyeNet, the ﬁrst single deep
neural network tackling multiple tasks related to eye gaze
estimation and semantic user understanding. We showed
that it is indeed possible to learn a shared representation to
produce robust estimates for both semantic and geometric
tasks simultaneously. This results in competitive gaze
accuracy for off-axis gaze estimation with a much improved
robustness. Thus, it is not necessary to hand engineer
modules for geometric eye gaze estimation or blink,
and facial expression detection. The muti-task learning
approach utilizes the inductive bias among the relevant tasks
to improve performance. Als, the ensuing advantage is the
simplicity and maintanability of the solution. Finally, an
additional important beneﬁt is that with more training data,
EyeNet estimates can be improved further without much
expert supervision.

Recent works attempting to perform end-to-end learning
for gaze estimation have resulted in robust but inaccurate
estimates.
In contrast, we chose to employ domain
knowledge in the form of eye geometry for deriving model
based supervision. This helps achieve a much higher
accuracy in gaze estimation through high quality cornea
center prediction. While EyeNet is the ﬁrst attempt to
unify the solution to several problems related to eye gaze
estimation and semantics, there are still some limitations
such as the need to do out-of-network optimization for
achieving higher accuracy. Our next steps are to learn this
optimization itself along with the DeepGazeMapper. This
presents an opportunity to jointly learn to estimate a stable
center of rotation of the eye ball along with the gaze for
each frame. Other important considerations for us in the
near future is reducing noise during data capture, exploiting

synthetic data, incorporating more personalized anatomical
and temporal constraints during training.

Acknowledgements

Special thanks to our colleagues Dan Farmer, Brad
Stuart, Sergey Prokushkin and Jean-Yves Bouguet for
providing us with their valuable insights on classical
geometry based eye gaze estimation.
Thanks also to
Alex Kim for his help in rendering ﬁgures explaining eye
geometry.

References

[1] Opencv

simpleblobdetector.

https://docs.

opencv.org/3.4/d0/d7a/classcv_1_
1SimpleBlobDetector.html.

[2] Pupil capture is an open source eye tracking tool from pupil-

labs: https://github.com/pupil-labs/pupil.

[3] Tobii is the world leader in eye tracking, Apr 2015.
[4] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
IEEE Transactions on Pattern Analysis &
segmentation.
Machine Intelligence, (12):2481–2495, 2017.

[5] A. Borji and L. Itti. Cat2000: A large scale ﬁxation dataset
for boosting saliency research. CVPR 2015 Workshop on
”Future of Datasets”, 2015.

[6] D. Eberly. Computing a point of reﬂection on a sphere, Feb

2008.

[7] P. Eckman and W. Friesen. Facial action coding system: A
IEEE

technique for the measurement of facial movement.
10.1109/WACV.2019.00178, 2019.

[8] R. Garg, V. K. BG, G. Carneiro, and I. Reid. Unsupervised
cnn for single view depth estimation: Geometry to the
rescue. In ECCV, pages 740–756. Springer, 2016.

[9] R. Girshick.

Fast r-cnn.

In Proceedings of the IEEE
International Conference on Computer Vision, pages 1440–
1448, 2015.

[10] B. Guenter, M. Finch, S. Drucker, D. Tan, and J. Snyder.
ACM Transactions on Graphics

Foveated 3d graphics.
(TOG), 31(6):164, 2012.

[11] E. D. Guestrin and M. Eizenman. General

theory of
remote gaze estimation using the pupil center and corneal
reﬂections. IEEE Transactions on Biomedical Engineering,
53(6):1124–1133, 2006.

[12] A. Handa, M. Bloesch, V. P˘atr˘aucean,

J. McCormac, and A. Davison.
library for geometric computer vision.
67–82. Springer, 2016.

S. Stent,
gvnn: Neural network
In ECCV, pages

[13] D. W. Hansen and A. E. Pece. Eye tracking in the wild.
Computer Vision and Image Understanding, 98(1):155–181,
2005.

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016.
[15] C. Hennessey, B. Noureddin, and P. Lawrence. A single
camera eye-gaze tracking system with free head motion. In
Proceedings of the 2006 symposium on Eye tracking research
& applications, pages 87–94. ACM, 2006.

15

[16] S. Hickson, N. Dufour, A. Sud, V. Kwatra, and I. Essa.
Eyemotion: Classifying facial expressions in vr using eye-
tracking cameras. IEEE 10.1109/WACV.2019.00178, 2019.

[17] Y. Huang, M. Cai, Z. Li, and Y. Sato. Predicting gaze
in egocentric video by learning task-dependent attention
transition. arXiv preprint arXiv:1803.09125, 2018.

[18] T. Judd, F. Durand, and A. Torralba. A benchmark of
computational models of saliency to predict human ﬁxations.
In MIT Technical Report, 2012.

[19] J. Kim, M. Stengel, A. Majercik, S. De Mello, D. Dunn,
S. Laine, M. McGuire, and D. Luebke. Nvgaze: An
anatomically-informed dataset for low-latency, near-eye
gaze estimation. In Proceedings of the 2019 CHI Conference
on Human Factors in Computing Systems, page 550. ACM,
2019.

[20] M. Kim, O. Wang, and N. Ng. Convolutional neural network

architectures for gaze estimation on mobile devices.

[21] K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan,
S. Bhandarkar, W. Matusik, and A. Torralba. Eye tracking
for everyone. In CVPR, pages 2176–2184, 2016.

[22] S. S. Kruthiventi, K. Ayush, and R. V. Babu. Deepﬁx: A
fully convolutional neural network for predicting human eye
ﬁxations. arXiv preprint arXiv:1510.02927, 2015.

[23] E. Langbehn, F. Steinicke, M. Lappe, G. F. Welch,
and G. Bruder.
In the blink of an eye - leveraging
blink-induced suppression for imperceptible position and
orientation redirection in virtual reality. 2018.

[24] D. Lanman. Reactive displays: Unlocking next-generation

vr/ar visuals with eye tracking, June 2018.

[25] D. H. Lee and A. K. Anderson. Reading what the mind
Psychological Science

thinks from how the eye sees.
https://doi.org/10.1177/0956797616687364, 2017.

[33] A. Villanueva, J. J. Cerrolaza, and R. Cabeza. Geometry
issues of gaze estimation. In Advances in Human Computer
Interaction. InTech, 2008.
[34] E. Wood and A. Bulling.

Eyetab: Model-based
gaze estimation on unmodiﬁed tablet computers.
In
Proceedings of the Symposium on Eye Tracking Research
and Applications, pages 207–210. ACM, 2014.

[35] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum,
A. Torralba, and W. T. Freeman. Single image 3d interpreter
network. In ECCV, pages 365–382. Springer, 2016.

[36] D. H. Yoo and M. J. Chung. A novel non-intrusive eye
gaze estimation using cross-ratio under large head motion.
Computer Vision and Image Understanding, 98(1):25–51,
2005.

[37] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik,
and S. Savarese. Taskonomy: Disentangling task transfer
the IEEE Conference on
learning.
Computer Vision and Pattern Recognition, pages 3712–
3722, 2018.

In Proceedings of

[38] X. Zhang, M. X. Huang, Y. Sugano, and A. Bulling. Training
person-speciﬁc gaze estimators from user interactions with
the 2018 CHI
multiple devices.
Conference on Human Factors in Computing Systems, page
624. ACM, 2018.

In Proceedings of

[39] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe.
Unsupervised learning of depth and ego-motion from video.
In CVPR, volume 2, page 7, 2017.

[40] Z. Zhu, Q. Ji, et al. Novel eye gaze tracking techniques under
natural head movement. IEEE Transactions on Biomedical
Engineering BME, 54(12):2246, 2007.

A. Appendix

[26] S. Li and W. Deng. Deep facial expression recognition: A

A.1. Encoder Architecture

survey. arXiv preprint arXiv:1804.08348, 2018.

[27] T.-Y. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan,
and S. J. Belongie. Feature pyramid networks for object
detection.

[28] E. Lind´en, J. Sj¨ostrand, and A. Proutiere. Appearance-based
3d gaze estimation with personal calibration. arXiv preprint
arXiv:1807.00664, 2018.
[29] O. Ronneberger, P. Fischer,

U-net:
Convolutional networks for biomedical image segmentation.
In International Conference on Medical Image Computing
and Computer-assisted Intervention,
pages 234–241.
Springer, 2015.

and T. Brox.

[30] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training.

[31] L. ´Swirski, A. Bulling, and N. Dodgson. Robust real-time
pupil tracking in highly off-axis images. In Proceedings of
the Symposium on Eye Tracking Research and Applications,
pages 173–176. ACM, 2012.

[32] Y. Tong, J. Chen, and Q. Ji. A uniﬁed probabilistic
framework for spontaneous facial action modeling and
understanding. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(2):258–273, 2010.

16

We emplot a ResNet50 with a feature pyramid network
(FPN) as the encoder architecture of EyeNet [14],[27]. This
part can be replaced with newer architectures for potential
improvements. We select the pyramid layer with stride of 8
pixels with respect to the input image. Thus, the topmost
layer encoder features have a size of 20x15x256. This
shared feature is input to the task branches.

Note also that the original image capture size in our
dataset is 640x480. However, we downsample it to 160x120
for computational efﬁciency as the input to the encoder.

A.2. Decoder Architectures

We tabulate each task decoder (branch) in the following.
All the components are standard. Note that the Cornea 2D
center estimation and facial expression classiﬁcation share
the same structure but they are trained separatedly as two
branches. Also see Section 4.2.4 for the blink task branch
architecture.

Parent Layer
Encoding Layer
DeConv1

Layer Name
DeConv1
ResidualBlock1
ResidualBlock2 ResidualBlock1
ResidualBlock2
DeConv2
Conv1
BN1
Conv2
Conv3

DeConv2
Conv1
BN1+ReLU
Conv2
Conv3
DeConv3

Layer Params
14x4x128(stride 2)
128
128
4x4x64(stride 2)
3x3x32
NA
3x3x16
3x3x8
4x4x4(stride 2)

Input Size Output Size
40x30x128
20x15x256
40x30x128
40x30x128
40x30x128
40x30x128
80x60x64
40x30x128
80x60x32
80x60x64
80x60x32
80x60x32
80x60x16
80x60x32
80x60x8
80x60x16
160x120x4
80x60x8

Table 7: The architecture of the eye parts segmentation branch of EyeNet. See Section 4.2.1 for more details of this task.

Layer Name
Conv1 Loc
BN Loc +ReLU
Conv2 Loc
DeConv Loc

Parent Layer Layer Params

DeConv2
Conv1 Loc
BN Loc
Conv2 Loc

3x3x32
NA
3x3x20
4x4x5

Input Size Output Size
80x60x64
80x60x32
80x60x32
80x60x20

80x60x32
80x60x32
80x60x20
160x120x5

Table 8: The architecture of the pupil and glint detection branch of EyeNet. See Section 4.2.2 for more details of this task.

Layer Name Parent Layer Layer Params
Encoding
Conv1 Cls
Conv1 Cls
Flatten Cls
Flatten Cls
FC Cls

3x3x5
NA
1500x10

Input Size Output Size
20x15x256
20x15x5
1500

20x15x5
1500
10

Table 9: The architecture of the pupil and glint presence/absence classiﬁcation branch of EyeNet. See Section 4.2.3 for more
details of this task.

Layer Name
Conv1 Cor
BN1 Cor+ReLU
Conv2 Cor
BN2 Cor+ReLU
Flatten
FC1 Cor
FC2 Cor
FC Out Cor
FC Out Expre

Parent Layer Layer Params

Encoding
Conv1 Cor
BN1 Cor
Conv2 Cor
BN2 Cor
Flatten
FC1 Cor
FC2 Cor
FC2 Cor

3x3x64
NA
3x3x16
NA
NA
4800x64
64x16
16x2
16x4

Input Size Output Size
20x15x256
20x15x64
20x15x64
20x15x16
20x15x16
4800
64
16
16

20x15x64
20x15x64
20x15x16
20x15x16
4800
64
16
2
4

Table 10: Cornea 2D center estimation and the facial expression classiﬁcation share this architecture but are trained separately
as two branches. See Section 4.3 for more details of this task.

17

