Draft version September 9, 2021
Typeset using LATEX modern style in AASTeX62

Sifting Through the Static: Moving Object Detection in Diﬀerence Images
Hayden Smotherman,1 Andrew J. Connolly,1 J. Bryce Kalmbach,1
Stephen K. N. Portillo,1 Dino Bektesevic,1 Siegfried Eggl,1, 2
Mario Juric,1 Joachim Moeyens,1 and Peter J. Whidden1

1Department of Astronomy, University of Washington, Seattle, WA, 98195, USA
2Department of Aerospace Engineering, University of Illinois, Urbana, IL, 61801, USA

ABSTRACT

Trans-Neptunian Objects (TNOs) provide a window into the history of the Solar
System, but they can be challenging to observe due to their distance from the Sun
and relatively low brightness. Here we report the detection of 75 moving objects
that we could not link to any other known objects, the faintest of which has a VR
magnitude of 25.02 ± 0.93 using the KBMOD platform. We recover an additional 24
sources with previously-known orbits. We place constraints on the barycentric dis-
tance, inclination, and longitude of ascending node of these objects. The unidentiﬁed
objects have a median barycentric distance of 41.28 au, placing them in the outer
Solar System. The observed inclination and magnitude distribution of all detected
objects is consistent with previously published KBO distributions. We describe ex-
tensions to KBMOD, including a robust percentile-based lightcurve ﬁlter, an in-line
graphics processing unit (GPU) ﬁlter, new coadded stamp generation, and a convo-
lutional neural network (CNN) stamp ﬁlter, which allow KBMOD to take advantage
of diﬀerence images. These enchancements mark a signiﬁcant improvement in the
readiness of KBMOD for deployment on future big data surveys such as LSST.

1
2
0
2

p
e
S
7

]
P
E
.
h
p
-
o
r
t
s
a
[

1
v
6
9
2
3
0
.
9
0
1
2
:
v
i
X
r
a

Corresponding author: Hayden Smotherman
smotherh@uw.edu

 
 
 
 
 
 
2

Smotherman et al.

1. INTRODUCTION

Small bodies are the ﬁnal frontier in the study of ﬂux-limited populations in the So-
lar System. While these objects are primarily very small and often very distant, they
are nevertheless critical to our understanding of the formation of the Solar System.
For example, Trans-Neptunian Objects (TNOs) contain dynamically-unperturbed
relics from the formation of the Solar System (Luu & Jewitt 2002). They provide a
window into the early history of the Solar System and enable tests of planetary forma-
tion and migration hypotheses. The Nice model (Tsiganis et al. 2005) suggests that
all the giant planets formed well-interior to 20 au and migrated outwards due to inter-
actions with planetesimals. Better knowledge of the dynamical populations of TNOs
would enable tests of additional and alternative hypotheses regarding the dynami-
cal history of the Solar System, such as the smooth migration of Neptune (Hahn &
Malhotra 2005; Nesvorn´y 2015; Morbidelli & Nesvorn´y 2020), a stellar ﬂyby (Kenyon
& Bromley 2004), or rogue planetary embryos (Gladman & Chan 2006). Improving
our understanding of TNO size and orbital distributions, especially at the low-mass
end where they are more poorly constrained, will be critical for our understanding of
these and other hypotheses.

Well beyond the edge of the Classical Kuiper Belt lies the Oort Cloud. Most fa-
mously, the Inner Oort Cloud includes the object Sedna, which is thought to be a
member of a larger population of sednoids (Brown et al. 2004). Sedna is an Inner
Oort Cloud object with a perihelion of 76 ± 4 au, but a semimajor axis of 480 ± 40
au. Therefore, it spends well over 90% of the time on its orbit beyond the detec-
tion limit of the survey that discovered it. According to the Minor Planet Center
(MPC) database for TNOs, centaurs, and scattered disk objects (SDO), Sedna has
the second largest perihelion (after 2012 VP113) of any detected Solar System object.
It represents one of the only currently-observable links to the Oort Cloud, a region
that contains a wealth of information about the history of the Solar System. If we
could increase the number of known sednoids and other Oort Cloud objects, they
would provide observational constraints on the formation environment of the Sun
(Brasser et al. 2006) and the Sun’s dynamic history in the Milky Way after leaving
its formation environment (Kaib et al. 2011).

New and upcoming approaches to survey astronomy provide exciting opportunities
for the study of these populations. For example, the upcoming Legacy Survey of
Space and Time (LSST; lsst.org; Ivezi´c et al. 2019) expects to survey over 18,000
square degrees of the sky 825 times over a period of 10 years, generating about 20
TB of data every 24 hours.

LSST plans to detect Solar System objects from individual images, with a single-
visit limiting magnitude in the r band of 24.7, and link these detections to measure
orbits. Current projections (LSST Science Collaboration et al. 2009) show that LSST
is expected to detect about 40,000 TNOs, which is by itself a large increase over
the currently-known 4077 Centaurs, KBOs, and SDOs (MPC). However, if we could

Sifting Through the Static

3

√

coadd the images to increase the signal-to-noise ratio (SNR) of the Solar System
detections, then we could recover signiﬁcantly more objects. Following the formula
∆m = (5/2) log
N , coadding just three months of LSST data would increase the
limiting magnitude in the r band from 24.7 to 26.1. This increase in depth means
LSST would detect ∼ 8.0 times more TNOs compared to a single image, assuming the
single power-law r band KBO distribution of Fraser et al. (2008). Instead of 40,000
TNOs, we could detect ∼ 320, 000 TNOs. If we could coadd a year of LSST data,
this increases to over 520,000 new TNOs detected (given our simpliﬁed assumptions).
None of this requires any more data than LSST will already acquire.

Coadding moving objects poses unique challenges compared to coadding stars. Be-
cause stars move very slowly compared to most survey cadences, coaddition of a stack
of aligned images usually increases the limiting magnitude for stars compared to sin-
gle images. Solar System objects, however, generally move at on-sky velocities of
> 1(cid:48)(cid:48) hr−1, due to both the proper motion of the objects and the reﬂex motion caused
by the Earth’s orbit. This means that traditional image coaddition typically does not
increase the number of detectable moving objects. Known moving objects may be
tracked and aligned along their orbits to improve the quality of the detection, but to
use image coaddition to detect new objects with unknown orbits, another approach
is required.

The Kernel-Based Moving Object Detection (KBMOD; Whidden et al. 2019) algo-
rithm takes a time series of images of the same RA and Dec, uses a “track before
detect” (TBD) approach to account for the potential motion of objects on an im-
age, and then coadds the shifted images (increasing the SNR of objects with the
candidate trajectory). To sample all possible orbital parameters requires searching
billions of candidate trajectories even within the footprint of a single charge-coupled
device (CCD). Consequently, current implementations of TBD have generally been
restricted to narrow-ﬁeld surveys (Bernstein et al. 2004). KBMOD addresses this by
using GPU-accelerated computing to search over a wide range of trajectories for a
stack of CCDs in of order 10 minutes.

In this paper, we present a number of algorithmic improvements to KBMOD that
allow us to search for moving objects in diﬀerence images. We use the Dark Energy
Camera (DECam) NEO Data Survey to validate our improvements. This is a larger
survey with a longer and more irregular cadence than KBMOD has been applied to
in the past. Successfully running on diﬀerence images and a more complicated survey
shows that KBMOD is beginning to be applicable at the scale needed for upcoming
big data surveys like LSST. In Section 2, we discuss the DECam NEO Data Survey
and the processing we applied to it using the LSST Software Stack. In Section 3,
we discuss the KBMOD algorithm and present recent improvements. In Section 4,
we discuss the results from our analysis, including the detection of unidentiﬁed outer
Solar System objects. We discuss current limitations and future improvements in
Section 5.

4

Smotherman et al.

2. DATA

2.1. The DECam NEO Survey Data

The DECam NEO Data Survey covered an area on the sky of greater than 2000
square degrees. The ∼6.7 TB data set from the DECam NEO Data Survey (PI Lori
Allen) uses the Dark Energy Camera on the 4m Blanco telescope at the Cerro Tololo
Inter-American Observatory (CTIO) (Flaugher et al. 2015). The DECam NEO Data
Survey consists of 32 nights of data.
In the ﬁrst 10-night observing run in 2014,
Trilling et al. (2017) found 235 unique NEOs.

Each individual image taken by DECam is a composite of 62 2K x 4K science CCDs,
with a ﬁll factor of 0.8 (Herner et al. 2020). Each CCD image covers an area of ∼0.04
square degrees with a pixel scale of 0.27 arcseconds. This results in a total ﬁeld
of view for DECam of about 3 square degrees. The CCDs are 250 µm thick fully
depleted devices, with a peak quantum eﬃciency above 85% at ∼ 6500˚A(Flaugher
et al. 2015). Gaps between CCDs are between 153 pixels (columns) and 201 pixels
(rows). Observations for this data set were taken in the VR ﬁlter, a broad optical
ﬁlter extending from 500 to 760 nm.

We separate this data set into 782 pointing groups based on RA and Dec. CCD 01
and 61 had no data in our images, leading to a set of 60 CCDs per pointing group.
We deﬁne a pointing group as a set of DECam exposures within 25(cid:48)(cid:48) of a common
RA and Dec and deﬁne a pointing as an individual DECam exposure (i.e. a set
of 60 CCDs) in a pointing group. Most pointing groups contain between 5 and 25
pointings. Pointing groups characteristically have 5 pointings per night, with all data
taken over nearly-consecutive nights. The intra-night pointings are taken about ﬁve
minutes apart for a total intra-night timespan of approximately 25 minutes.

24 pointing groups had a high stellar number density, with more than 10000 sources
detected in a CCD. When astrometrically calibrating these images (see Section 2.2),
these pointing groups exceeded the memory limits of the available computational re-
sources and were therefore excluded. The current limitations regarding the processing
of dense ﬁelds with LSST Science Pipelines are described in Sullivan & Bellm (2021).
Detectability of moving objects with KBMOD, however, is driven strongly by the
quality of the diﬀerence images.

372 pointing groups contained data from at least four unique survey nights. Because
of the short intra-night image cadence, which can cause slow-moving objects to exhibit
minimal motion within a night, we only search over pointing groups with at least four
unique survey nights. This ensures that any given KBMOD trajectory will search a
suﬃciently-large number of unique on-sky positions, thereby reducing the probability
of linking of static objects.

In order to comply with computational limitations, we selected 43 pointing groups
from the set of 372 pointing groups, focusing our research on higher-quality data.
These 43 pointing groups have a total eﬀective search area of approximately 132
square degrees. We refer to these 43 pointing groups as the “search sample”. This

Sifting Through the Static

5

down select from 372 pointing groups was as follows. 40 pointing groups existed
where all pointings in the pointing group had a maximum seeing full width at half
maximum (FWHM) of 1.25(cid:48)(cid:48). These 40 pointing groups make up the bulk of the
search sample. There were an additional 12 pointing groups that had over 20 total
pointings, but with only 20 pointings with seeing < 1.25(cid:48)(cid:48). These pointing groups
returned a greater number of erroneous candidate trajectories that required by-eye
rejection. This is possibly due to the inclusion of poor-seeing images in the image
diﬀerencing template (see Section 2.2). Due to computational limitations, we elected
to run KBMOD on only 3 of these pointing groups, focusing our GPU resources on
the 40 pointing groups where all 20 pointings had the required seeing limits. These
3 pointing groups make up the remainder of the search sample.

2.2. Processing the DECam Data

The raw DECam images were processed by the DECam Community Pipeline
(Valdes et al. 2014) resulting in a set of InstCal PROCTYPE images, as deﬁned
in the NOAO Data Handbook (NOAO 2015). These images are bias and linearity
corrected, ﬂat-ﬁelded, and sky-subtracted by the community pipeline. Data quality
masks and inverse variance arrays were provided. We downloaded the compressed
InstCal data from the NOAO Data Archive between July and November of 2017.

Prior to running the KBMOD pipeline, we ﬁrst astrometrically calibrate the images
in all 782 pointing groups. This was undertaken using the LSST Science Pipelines
Software (Juri´c et al. 2017). Sources were detected in the individual images. Sources
with a SNR >= 40 were matched to the data from the GAIA Data Release 1 (DR1).
The median astrometric scatter for the sources used to ﬁt the CCD world coordinate
systems (WCS) was 25 mas; 373 CCDs had an astrometric scatter worse than 100
mas. The median number of sources detected per CCD was 3575.

As a followup to Whidden et al. (2019), we use image diﬀerencing to remove non-
variable and non-moving sources within an image (as opposed to just masking the
sources). We used the LSST Stack to diﬀerence the images in the pointing groups
using a method based on Alard & Lupton (1998). For the DECam NEO Survey data,
we diﬀerence each pointing against a coadded template. Given the short intra-night
time separation between images of a given pointing group, objects moving slower
than of order 1” hr−1 will not move a full psf width over a single night. We therefore
separate a pointing group into two approximately equal groups such that each image
in the ﬁrst group will be separated in time from each image in the second group by at
least twelve hours. A coadded template was independently generated from each group
and used to diﬀerence the opposite group. Because our minimum search velocity is
≥ 92 pixels per day (≥ 24” per day), this guarantees that objects of interest will
be much greater than one PSF away from where they were in the coadded template.
This means that pointings in the middle of a pointing group—with respect to time —

6

Smotherman et al.

Figure 1. Single pointing (pointing group 011, CCD 29, visit 303605) before (left) and
after (right) image diﬀerencing. Similar to DS9, we applied an arcsinh ﬁlter to the pixels
in this example in order to better show objects in each image.

will have the shortest image diﬀerencing baseline, and will therefore set a theoretical
limit on the slowest-moving objects we can detect.

In order to diﬀerence the science images against the coadded template (Alard &
Lupton 1998; Zackay & Ofek 2017a,b; Zackay et al. 2016), we need to ﬁnd a con-
volution kernel K such that for a science image I(x) and a coadded template Φ(x),
I(x) = K ⊗ Φ(x). Following the approach of Alard & Lupton (1998), we separated
the template into local spatial cells of 128x128 pixels. We detected sources in both
images, and grouped them into the spatial cells. Stamps of these sources were created
with sizes between 21x21 pixels and 35x35 pixels, depending on the FWHM of the
source. Stamps in each cell were used to ﬁnd the local spatially-invariant convolution
kernel solutions of each stamp. The local convolutional kernel was modelled as a
set of Gaussian functions multiplied with a polynomial. The coeﬃcients of the kernel
were then found by solving a least-squares problem. One source (and thus one stamp)

050010001500200005001000150020002500300035004000Undifferenced Image050010001500200005001000150020002500300035004000Warped Difference ImagePointing Group 023 | CCD 35 | Visit 303665Sifting Through the Static

7

Figure 2. Number of sources per CCD image for each visit in 10 pointing groups (pointing
group 091 to 100). The median number of sources per science image (orange) is 3396 per
CCD image. The median number of sources per diﬀerence image (blue) is 180 per CCD
image. Diﬀerencing the science images therefore reduces the number of static sources in the
image by a factor of about 18.

was selected for each grid cell based on the clipped mean of all the kernel solutions
in the cell. This gave the local convolution kernel for that cell. Chebyshev polyno-
mials of the ﬁrst kind were ﬁt to the local kernel coeﬃcients in order to determine a
model for spatially-variant global convolution kernel coeﬃcients. This global kernel
was then used to match the PSF of the coadded template to that of the science image.
We matched the template to the science image, rather than the other way around,
because the template has less noise and the convolution correlates noise. The two
images were then subtracted. Finally, a decorrelation algorithm was run to remove
the correlation in the noise of the diﬀerence image. After diﬀerencing the image,
we warp all images in a pointing group to the sky plane of the ﬁrst pointing in the
pointing group. This ensures that a pixel in one pointing will correspond to the same
RA and Dec as that of the same pixel in another pointing.

As an example, Figure 1 shows pointing group 023, CCD 35, visit 303665 before and
after image diﬀerencing and warping. The ﬁnal image size for the KBMOD image
is set by the intersection of the image and the template that is subtracted. Slight
misalignments of the pointings in a pointing group may reduce the ﬁnal image sizes.
All pointing groups were, however, aligned to within 50 arcsec in RA and Dec, with
all but 28 pointing groups aligned to better than 25 arcsec in both RA and Dec.
Therefore the reduction in image area was minimal.

01000200030004000500060007000Number of sources0100020003000400050006000Number of CCD imagesDifference ImagingScience Imaging8

Smotherman et al.

The asteroid search was run for each aligned stack of DECam CCDs independently;
we did not search trajectories across CCD boundaries. The eﬀective area on which
we are able to search for moving objects is, therefore, about 0.04 square degrees. In
other words, a necessary requirement for the detection of a moving object with the
KBMOD algorithm is that the object stays within the ﬁeld of view of an individual
CCD for at least two pointings. In practice, we require that an object stay in the
ﬁeld for at least 3 nights (typically 15 pointings). This means that an object must
move slower than about 15” hr−1 to be detected by KBMOD.

3. TECHNIQUES

i Ψi and Φcoadd = (cid:80)

KBMOD generates images of likelihood (Ψi) and variance (Φi) from a series of
CCD images as described in Whidden et al. (2019). Assuming a Gaussian likelihood
function, a stack of Ψi and Φi images can then be shifted along a potential asteroid
trajectory and summed in order to get the coadded likelihood of a detection (Ψcoadd =
(cid:80)
i Φi). See Ofek & Zackay (2018) for the optimal approach for
source detection with Poisson noise. We deﬁne a SNR ν for a detection such that
Φcoadd. In this ν image, generated for each given angle and velocity
νcoadd = Ψcoadd/
vector, any points above some threshold m can be considered to be m-sigma detections
of a moving source. For a single trajectory, we can deﬁne the summed likelihood as
(cid:80) LH = νtrajectory
. The interested reader is directed to Whidden et al. (2019) for
more detail.

coadd

√

The large number ((cid:29) 109) of potential asteroid trajectories means that these Ψi
and Φi images must be searched many times over. For this reason, KBMOD uses
massively-parallel GPU computing for the core computations. The current software
allows a user to search over 1010 potential moving object trajectories in a stack of 10-15
4K x 4K images in under a minute using a consumer-grade GPU (e.g., Nvidia 1080
Ti) (Whidden et al. 2019). Our pointer-arithmetic approach means that we never
actually shift and stack images. Rather, we merely sum the previously-calculated
likelihoods, utilizing thousands of concurrent GPU threads to keep the computation
feasible on consumer-grade hardware.

The DECam NEO data set presents unique ﬁltering challenges compared to Whid-
den et al. (2019) due to the increased number of potentially-valid trajectories, the
short intra-night cadence, and image diﬀerencing artifacts. In Whidden et al. (2019),
detected sources appearing in the same position in 2 or more images, pixels with
counts above 120 counts, and other mask ﬂags set by the DECam community pipeline
or the LSST software stack were all masked. In the current data set, we use diﬀerence
imaging to subtract static sources. This enables us to decrease the masked area of the
image, only masking sources ﬂagged as detected if they appear in 10 or more images.
However, despite reducing the number of detected individual sources on the image by
a factor of about 18 (see Figure 2), leaving most of the image unmasked, coupled with
diﬀerence imaging artifacts, increases the number of trajectories with (cid:80) LH > 10 by

Sifting Through the Static

9

a factor of 10. This problem is worsened by the intranight cadence. The average
time between images within a single night is about 5 minutes. This means that for
a characteristic trajectory with a velocity of 100 pixels per day, objects will move
by less than 1 pixel between images. Conversely, this also means that if a static
source appears along the potential trajectory, ﬂux from this object will most likely
be present in at least ﬁve trajectory data points, introducing repeated outliers into
the trajectory.

3.1. σG Filtering
In order to deal with the increased number of high likelihood trajectories (i.e. 107
with (cid:80) LH > 10), we developed faster, more-eﬀective ﬁltering. First, we altered
how the GPU and C++ code handed oﬀ data to the Python-based ﬁltering, leading
to a speed increase of up to 300%. Second, we replaced the Kalman ﬁlter used in
Whidden et al. (2019) with a more statistically-robust quantile-based ﬁltering method.
We describe this new ﬁltering method below.

With a traditional quantile-based ﬁlter, the ﬁlter rejects data points that are greater
than nσ from the central value of the distribution, where σ is a measure of the spread
of the distribution.
In the case of a Gaussian distribution, σ might be estimated
by computing the standard deviation of the data and the central value estimated by
computing the mean of the data. If we take n = 1, then this simple ﬁlter would reject
any data points that are greater than 1σ from the mean.

In the presence of signiﬁcant outliers, the mean and standard deviation become
biased estimators for the central value and the spread of the underlying Gaussian dis-
tribution. Following the approach of Ivezi´c et al. (2014), we adopt a robust estimator
for the central value and the true standard deviation of a Gaussian distribution with
outliers. Consider the cumulative distribution function (CDF) of a Gaussian distri-
bution

f (x) =

(cid:20)

1
2

1 + erf

(cid:19)(cid:21)

(cid:18) x − µ
√
2
σG

(1)

where µ is the mean, σG is the standard deviation of the Gaussian, and erf is the
error function. The inverse, then, is given by

√

x = µ + σG

2 erf −1 [2f (x) − 1]

(2)

By sampling the Gaussian distribution at two quantiles f (xi) and f (xj), we can
estimate σG. To do this, we take the diﬀerence of the inverted CDF

√

xj − xi = σG

2 (cid:0)erf −1 [2f (xj) − 1] − erf −1 [2f (xi) − 1](cid:1)

=⇒ σG =

1
erf −1 [2f (xj) − 1] − erf −1 [2f (xi) − 1]

(xj − xi)

=⇒ σG = C [xj − xi]

(3)

(4)

(5)

10

Smotherman et al.

Figure 3. Number of candidate trajectories at various stages of processing for variable
numbers of results per pixel. The solid line shows the number of candidate trajectories with
(cid:80) LH > 10 returned from the GPU for subsequent ﬁltering. The dashed and dotted lines
show the number of candidate trajectories passing CPU σG ﬁltering (dashed) and central
moment stamp ﬁltering and clustering (dotted). GPU ﬁltering decreases the total number of
candidate trajectories with (cid:80) LH > 10, but increases the number of candidate trajectories
that pass subsequent lightcurve ﬁltering and stamp ﬁltering and clustering. Because GPU
memory constraints limit the number of candidate trajectories per starting pixel that can be
saved for subsequent analysis, using a GPU ﬁlter means that the results that are passed out
of the GPU are more likely to be potentially-valid. These results are then processed with
the CNN ﬁlter and subject to human review. These data come from repeated reprocessings
of pointing group 023, CCD 35.

Here, C is a coeﬃcient dependent only on the choice of quantiles. xj and xi are
estimated by selecting values from the lightcurve. The choice of upper and lower
quantiles is user-determinable. Here, we estimate σG using data from the 25th to
75th percentiles, for a coeﬃcient of C25,75 ≈ 0.7413. Then, we can estimate x25 and
x75 from the data by selecting the 25th and 75th percentile values respectively from
the data. We can then estimate the standard deviation of the underlying Gaussian
distribution with σ ≈ 0.7413 (x75 − x25). Given a robust estimator of the spread of
the distribution (i.e. σG), we apply a ﬁlter that rejects any points that are not within
±nσG (e.g. 2σG) of the median of the data.

12345678Max number of candidate trajectories per pixel102103104105106107Number of candidate trajectoriesNo GPU Filter: Trajectories with LH>10No GPU Filter: After LC FilterNo GPU Filter: After Stamp Filtering and ClusteringGPU Filter: Trajectories with LH>10GPU Filter: After LC FilterGPU Filter: After Stamp Filtering and ClusteringSifting Through the Static

11

Figure 4. Number of results per CCD image stack from pointing group 190 requiring
by-eye conﬁrmation or rejection (hereafter “candidate trajectories”) for likelihood limits of
10 (dashed line) and 15 (solid line), with (orange) and without (blue) RESNET 50 CNN
ﬁltering. These results are from pointing group 190, one of the search sample pointing
groups. Here, the CNN was set to ﬁlter out any candidate trajectories with a probability
of true that was less than 75%. When using a LH limit of 15 and the CNN, the number
of candidate trajectories per CCD was reduced to eleven or less, an acceptable number of
trajectories for a human to review.

We apply this method to the likelihood and/or ﬂux values of each trajectory. We
then recompute (cid:80) LH for the trajectory values that pass the ﬁlter and reject the
trajectory if the recomputed likelihood ((cid:80) LH (cid:48)) is less than 10.
In practice, this
ﬁltering method successfully rejects of order 106 erroneous candidate trajectories in
approximately 60s using 30 central processing unit (CPU) cores.

3.2. In-line GPU Filtering

Applying a variant of the σG ﬁlter in the GPU while the search is running, instead of
in post-processing, increases the number of potentially-valid trajectories returned to
the CPU by KBMOD. In Whidden et al. (2019), KBMOD passed the four trajectories

10203040CCD Number100101102103Number of Candidates per CCDFilter EffectivenessFilter TypeLH>10 | No CNNLH>10 | CNNLH>15 | No CNNLH>15 | CNN12

Smotherman et al.

Figure 5. Sample output for object 2015 GQ56 (pointing group 300, CCD 30) when
using trajectory estimates from the JPL Horizons service. The ﬁrst row shows the coadded
stamp (left) and the ﬂux lightcurve (right). Orange points in the ﬂux lightcurve are points
that pass σG lightcurve ﬁltering. The remaining rows show the postage stamps for 2015
GQ56 in each individual image. The coadded stamp was generated by taking the median
value at each pixel; this eﬀectively removes image diﬀerencing artifacts. This trajectory
was generated using orbital values from JPL Horizons. These ﬁgures were generated for all
known KBOs in search sample in order to determine the unﬁltered (cid:80) LH, as well as for
debugging purposes. Each stamp shows the estimated SNR ν of that stamp.

per pixel with the highest (cid:80) LH from the GPU to the CPU. Other trajectories with
the same starting pixel were discarded. Because KBMOD searches of order 1012
trajectories for a 2K x 4K image, it is computationally infeasible to keep the results

Coadd | SNR=16.3770SNR=1.64SNR=0.39SNR=3.61SNR=1.61SNR=0.18SNR=4.26SNR=7.07SNR=5.18SNR=4.98SNR=5.88SNR=2.42SNR=3.49SNR=4.69SNR=4.21SNR=4.47SNR=3.10SNR=3.10SNR=5.09SNR=4.04SNR=5.061234567891011121314151617181920Visit number0100200300400Object fluxFiltered SNR = 17.2540(2015 GQ56)V mag=23.93, Velocity=[-81.01,-238.79]=252.17 px/day,Starting Pixel=[1288.33,2874.14]Sifting Through the Static

13

of all evaluated trajectories in GPU RAM. The disadvantage of this approach is
that lower-likelihood trajectories may get removed from the search even if they are
valid trajectories of true objects. With reduced masking, there are many erroneous
candidate trajectories with high likelihood. This means that removing the masks may
have increased the probability of discarding valid trajectories.

In-line GPU ﬁltering solves this problem by applying the ﬁltering method to com-
pute (cid:80) LH (cid:48) before the trajectory is passed back to the CPU. This in-line GPU ﬁlter
means that if a trajectory has a high (cid:80) LH only due to an outlier in the data,
that trajectory is unlikely to supplant another valid trajectory when GPU results are
passed back to the CPU. We also increased the number of returned results per pixel
from four to eight. This means that we were able to process about four times as many
results per pixel compared to Whidden et al. (2019). The in-line GPU ﬁltering uses
a single GPU and is about 10% faster than comparable CPU ﬁltering using 30 CPU
cores. Figure 3 demonstrates how the in-line GPU ﬁlter returns more potentially-valid
trajectories for a given number of trajectories per pixel.

3.3. Median Stamp Coadd Generation

As shown in Figure 1, saturated cores and small image misalignments leave a num-
ber of artifacts in the diﬀerence image that also have to be accounted for in the
ﬁltering process. As in Whidden et al. (2019), we computed the central moments of
postage stamps for candidate trajectories. Stamps were rejected if they did not have
central moments that were consistent with a Gaussian. In this data, we required that
the x, y, xy, xx, and yy moments be strictly less than 0.5, 0.5, 1.5, 36.5, and 36.5
respectively. These values were chosen empirically based on the central moments of
known KBOs. We generated coadded stamps by computing the median pixel value
for each pixel along the trajectory. This mitigates the eﬀect of image diﬀerencing
artifacts, improving the performance of the central moment ﬁlter.

3.4. CNN Filtering

To further reduce the number of false positives, we ﬁlter using a convolutional
neural network (CNN). We built a Residual Network with 50 layers (ResNet501) (He
et al. 2016). Residual networks are a type of CNN that add “shortcut connections”
into the network architecture, which help to train deeper networks. Training a CNN
requires a large amount of representative data. In this case, we needed a large (>
104) labeled set of 21x21 stamps containing approximately equal numbers of false
positives and true positives. To generate false positives, we ran an untargeted search
(with similar grid spacing as described in 4.1) with a coadded likelihood limit of
(cid:80) LH > 10 along trajectories unlikely to correspond to real objects (approximately
90◦ from the direction of the ecliptic). We ran a total of 53 searches with data from
34 unique pointing groups. These pointing groups were not constrained to the search

1

https://github.com/priya-dwivedi/Deep-Learning/blob/master/resnet keras/Residual

Networks yourself.ipynb

14

Smotherman et al.

sample. These searches yielded 113,549 21x21 false positive postage stamps. Because
KBOs are relatively rare, we could not use real recovered objects to generate the
thousands of true positives needed to train the CNN. To circumvent this limitation,
we generated 44,950 simulated true positives. To make these stamps, we retrieved 25
21x21 postage stamps from a CCD along a semi-random trajectory. Next, we drew
a random brightness from an exponential distribution (with dimmer objects being
the most likely). Using this brightness, we added a Gaussian to each background
stamp with a random standard deviation (1 − 2.1 pixels), a random central oﬀset
(< 2 pixels), and a random linear oﬀset (< 2 pixels over the image time baseline). To
train the CNN, we cut the false positive stamps and simulated true stamps down to
40,000 randomly-selected coadded stamps each. We used 70% of the data for training,
20% for validation, and the remaining 10% for testing. After 20 epochs, the training
set accuracy was about 99%, while the validation set accuracy was about 96%. After
training, the test set accuracy was also about 96%.

This CNN returns a predicted probability that a coadded postage stamp contains
a simulated object. Because the stamps of simulated objects diﬀer from the stamps
of real objects, this probability is not a perfect representation of the likelihood that
a coadded stamp contains a real object. However, it creates a user-programmable
threshold that can be used to reduce false positives enough that the remaining candi-

Figure 6. Recovered known objects as a function of reported magnitude. We ran an
untargeted KBMOD search on all CCDs in the search sample that had known KBOs on
them. Figure 7 shows the recovery statistics for the recovered objects. 18 of the recovered
objects were below the approximate upper-limit single-image 10σ limiting magnitude.

20.521.021.522.022.523.023.524.0V0246810121416Number of ObjectsKnown Object Recovery0.846 CompletenessFaintest single-image 10 depthAll ObjectsRecovered ObjectsSifting Through the Static

15

Figure 7. Statistics for the known objects that were recovered with a untargeted KBMOD
search on CCDs with known objects in the search sample. For object recovery, we discarded
any results that had a starting position more than 5 pixels (approximately 1.35”, or one
PSF FWHM) from the predicted location or had a velocity diﬀerence of more than 5 pixels
per day (approximately 0.056” hr−1). The velocity cutoﬀ was chosen based on the recovery
distribution. As shown in bottom left and bottom right respectively, the median diﬀerence
between predicted and recovered position and speed was signiﬁcantly lower than these cutoﬀ
values. The upper left plot shows each trajectory’s initial predicted and recovered position
on the CCD image for each object. The upper right plot shows each trajectory’s predicted
and recovered x and y velocity on the CCD image for each object.

date trajectories can be analyzed by-eye. We reject any stamps with a CNN proba-
bility of true less than 75%. As shown in Figure 4, when reviewing only objects with
a (cid:80) LH > 15 and using this CNN ﬁlter, there are generally fewer than 10 candidate
trajectories per CCD that require human by-eye conﬁrmation or rejection.

4. RESULTS

4.1. Search, Detection, and Recovery

We ran an untargeted KBMOD search on each stack of CCDs in the search sam-
ple for a total of 2580 searches. Similar to Whidden et al. (2019), an untargeted
search looks for linear trajectories with velocities between 92 and 550 pixels per day
(1.04” hr−1 to 6.19” hr−1) with angles of ±π/10 from the ecliptic angle. Compared
to Whidden et al. (2019), we doubled the resolution of the grid spacing from 256
velocity steps and 128 angle steps to 512 velocity steps and 256 angle steps. This

100200300400Initial X Position (arcsec)4006008001000Initial Y Position (arcsec)Recovered PositionPredicted Position2.01.81.61.41.21.00.8X Velocity (arcsec/hr)4.54.03.53.02.5Y Velocity (arcsec/hr)Recovered VelocityPredicted Velocity0.20.40.60.81.0Position Residual (arcsec)02468Number of ObjectsMedian Residual0.0020.0040.0060.008Speed Residual (arcsec/hr)02468Number of ObjectsMedian ResidualRecovered Results vs Predicted Results16

Smotherman et al.

Figure 8. Best-ﬁt barycentric distance r0, inclination i, and longitude of ascending node Ω
(dots) with respective standard deviations (lines) of the detected known objects (left) and
unidentiﬁed objects (right) using the method of Bernstein & Khushalani (2000). r0, i and Ω
were also ﬁt with Find Orb. When the value from Find Orb is inconsistent with Bernstein
& Khushalani (2000) within 1σ, we show the best-ﬁt value from Bernstein & Khushalani
(2000) with a square instead of a dot. For the known objects, the JPL Horizons value of
the corresponding parameter is overplotted with an x marker. The short time baseline of
the observations allows us only to constrain initial barycentric distance, inclination, and
longitude of ascending node. The medians of the absolute value of the residuals between
the best-ﬁt values and the JPL Horizons values are 0.36 au, 0.32 degrees, and 0.92 degrees
for r0, i, and Ω respectively. As reported by JPL Horizons, the median values of the known
objects for r0 and i are (cid:101)r0 = 41.55 au and (cid:101)i = 5.46◦ respectively. The median values of the
unidentiﬁed objects for the best-ﬁt r0 and i are (cid:101)r0 = 41.28 au and (cid:101)i = 7.67◦.

ensured that trajectories would end up separated by no more than about two PSF
FWHM from neighboring trajectories.

0510152025303540455055Barycentric Distance (au)Known Objects020406025303540455055Barycentric Distance (au)Unidentified Objects051015200255075100125150175i (degrees)02040600255075100125150175i (degrees)05101520Object Number050100150200250300350 (degrees)0204060Object Number050100150200250300350 (degrees)Sifting Through the Static

17

In order to test the eﬃciency of these new ﬁltering methods, we generated a list of
known objects in the search sample. We used Skybot (Berthier et al. 2006) and JPL
Horizons (Giorgini 2015) to ﬁnd all KBOs that were present in the search sample,
with the additional requirement that they be present in the ﬁrst image of the pointing
group. We generated 21x21 pixel postage stamps of the object in each image in which
it is present. We developed a variant of KBMOD that computes the likelihoods
along a single trajectory then runs the aforementioned quantile-based ﬁltering, and
computed the central moments of the postage stamps. Figure 5 shows these results
for pointing group 300, CCD 30, object 2015 GQ56. We removed KBOs with an
unﬁltered (cid:80) LH < 15. This left us with a “recovery sample” of 26 KBOs.

In the untargeted search of the search sample, we recovered 22 out of 26 (or 84.6%)
of the known objects in the recovery sample after all ﬁltering was applied (see Figure
6). The CNN probability threshold was kept at 75%. Recovery statistics for these
objects are shown in Figure 7. For object recovery, we discarded any trajectories that
had a starting position more than 5 pixels (approximately 1.35”, or one PSF FWHM)
from the predicted location or had a velocity diﬀerence from the known velocity of
more than 5 pixels per day (approximately 0.056” hr−1). The median position and
speed residuals were 0.427” and 0.0036” hr−1 respectively, signiﬁcantly below the
chosen cutoﬀ values. This velocity error corresponds to approximately a 1.27 pixel
position error over four days. Using the NOAO DECam Exposure Time Calculator
(ETC), we estimate the single-image 10σ depth to be at most 22.75V. Because the
pointing groups contain data from diﬀerent nights, we computed this limit assuming
a new Moon. It is therefore an upper limit. 18 of the recovered objects were fainter
than the upper-limit single-image 10σ depth. This conﬁrms that KBMOD is able
to use diﬀerence images to ﬁnd moving KBOs that are too dim to detect in a single
image at the 10σ level, extending the result of Whidden et al. (2019) to diﬀerence
images.

We investigated each of the missed known objects individually. 2013 GY136 (point-
ing group 204, CCD 57) failed to process due to a CCD that failed image diﬀerencing.
This reduced the total number of images in CCD 57 to fewer than 20, and CCD 57
was therefore not reprocessed. 2013 GZ137 (pointing group 202, CCD 52) failed CNN
ﬁltering with a threshold of 75%, but passes with a threshold of 50%. 2015 GY55
(pointing group 306, CCD 26) starts within 4 pixels of the chip edge, causing this
trajectory not to be searched by KBMOD. 2013 GH137 (pointing group 192, CCD
41) has two fully-masked stamps, and two more with partial masking, which may
have caused it to be ﬁltered out.

In addition to the detected 22 known objects in the recovery sample, we detected
2 additional known KBOs. These KBOs had an unﬁltered (cid:80) LH < 15 along the
JPL Horizons trajectories, and were therefore not included in the recovery sample.
The best KBMOD trajectories for these objects had a ﬁltered (cid:80) LH (cid:48) > 15. We then
linked these objects back with known KBOs.

18

Smotherman et al.

Figure 9. One-sided Kuiper variant of the Kolmogorov-Smirnov (K-S) test comparing
our recovered inclinations with the inclination distribution predicted by Brown (2001). We
reject the null hypothesis that our inclinations came from the distribution of Brown (2001)
with only 76.6% conﬁdence (less than 1σ). We therefore consider our observed inclinations
to be consistent with the distribution predicted by Brown (2001).

4.2. Orbit Fitting and Analysis

We detected 75 moving objects that we were unable to link to existing objects.
Trajectories with (cid:80) LH > 15 that passed all ﬁltering were accepted or rejected with
a by-eye examination of the individual stamps, the coadded stamp, and the ﬂux
lightcurve.

As shown in Figure 8, we used the method described in Bernstein & Khushalani
(2000) to ﬁt barycentric distance r0, inclination i, and longitude of ascending node
Ω of both the recovered known objects and the unidentiﬁed objects. For the known
objects, we compared the orbital parameters ﬁt to the KBMOD trajectory with their

020406080Object Number0.00.20.40.60.81.0Sorted Pn valuesBrown Inclination Distribution KS TestExpected Uniform DistributionObserved DistributionSifting Through the Static

19

respective parameters as reported by JPL Horizons. The medians of the absolute
value of the residuals between the best-ﬁt values and the JPL Horizons values are
0.36 au, 0.32 degrees, and 0.92 degrees for r0, i, and Ω respectively. The median
values for r0 and i of the known objects reported by JPL Horizons are (cid:101)r0 = 41.55
au and (cid:101)i = 5.46◦ respectively. The median values of the unidentiﬁed objects for the
best-ﬁt r0 and i are (cid:101)r0 = 41.28 au and (cid:101)i = 7.67◦. The three parameters (shown in
Figure 8) that are well-ﬁt with our data constrain the plane of the orbit and the
initial distance of the object from the Solar System barycenter. Individual values are
shown in Table A.

In addition to the method of Bernstein & Khushalani (2000), we used Find Orb2 to
ﬁt r0, i, and Ω. This allowed us to compare the best-ﬁt values between the two orbit
ﬁtting codes. When best-ﬁt values from Find Orb were not within 1σ of the best-ﬁt
value from the Bernstein & Khushalani (2000) code, we show the value as a square
in Figure 8. We discarded values with inconsistent inclinations from the remainder
of the orbit analysis.

There were a few noteworthy limitations to our dataset and apparent outliers in
our best-ﬁt values. Because of the relatively short time baseline of about four days,
we were unable to place any meaningful constraints on the other Keplerian elements
individually. For three unidentiﬁed objects (unidentiﬁed object numbers 58, 69, and
74), the orbit ﬁtting code did not return uncertainties. We therefore consider them
inconsistent between Bernstein & Khushalani (2000) and Find Orb. Unidentiﬁed
object numbers 4, 6, and 8 have a best-ﬁt inclination of iﬁt > 90◦. Similarly, known
object number 20 (2000 EE173) has a best-ﬁt inclination of iﬁt = 173.36◦ ± 0.54◦,
but a JPL Horizons inclination of iHorizons = 5.95◦. However, these 4 objects are all
marked as inconsistent between Find Orb and Bernstein & Khushalani (2000). As
such, their best-ﬁt values are removed from further orbital analysis.

To evaluate the consistency of the properties of our detected asteroids with pub-
lished distributions, we apply the analysis of Whidden et al. (2019) to the detected
objects with consistent inclinations reported in this paper. We compared our observed
inclination distribution with that of Brown (2001) by using a one-sided Kuiper vari-
ant of the Kolmogorov-Smirnov (K-S) test. We use a test statistic of D
N where N
is the number of objects, and D is given by Equation 30 in Whidden et al. (2019).

√

D = max (Pj − j/N )

(6)

Pj is the probability for a given inclination distribution that an object j has an
inclination equal to or below the actual inclination ij. Some TNO sub-populations
have non-uniform inclination distributions around the ecliptic. This is an unmodeled
systematic in our test statistic. We compute Pj using Monte Carlo methods. We
take 105 inclinations from the Brown (2001) distribution, place them randomly along

2 https://github.com/Bill-Gray/ﬁnd orb

20

Smotherman et al.

Figure 10. Inclination distributions of our detected objects (orange) and the distribution
predicted based on Brown (2001) (blue), after accounting for the search sample ecliptic
latitudes. The Brown (2001) is weighted to the number of objects recovered by KBMOD.
Uncertainties in the orange histogram are calculated as 1σ Poisson intervals of

N .

√

√

circular orbits and take all objects within ±0.5◦ of the ecliptic latitude βj of discovery.
These values allow us to ﬁnd Pj by calculating the probability that an object with
a given βj has an inclination at or below ij. We run 1000 Monte Carlo simulations,
using the mean D
N as our test statistic. See Section 4.2.1 of Whidden et al. (2019)
and Section 3 of Brown (2001) for more detail.

√

Our mean value for D

N was 1.40. As shown in Figure 9, we reject the null hy-
pothesis that our observed inclinations come from the distribution of Brown (2001)
with only 76.6% conﬁdence, which is less than the 1σ conﬁdence level of 84.1%
(D
N = 1.47). This is to say that we cannot conﬁdently reject the null hypoth-
esis. We can therefore say that our observed inclinations are consistent with Brown
(2001).

√

We repeated the further comparison of Whidden et al. (2019), using an approximate
survey simulation to identify the distribution of objects with a given inclination that
we would expect to ﬁnd given the central RA and Dec of our search sample. We
modeled the DECam ﬁeld of view as a circle with a diameter of 2.2◦. We used the
inclinations and orbits from the Monte Carlo simulations used to generate Figure
9 and recorded the objects visible within the simulated camera footprint. We then
normalized this simulated object distribution to the number of detected objects in the
search sample. Figure 10 shows the simulated distribution (blue) and the observed

010203040Inclination010203040Number of ObjectsObservedPredicted Brown (2001)Sifting Through the Static

21

distribution (orange). The χ2 value between the simulated and expected distributions
was 8.58, corresponding to a p-value of 0.48. We therefore again say that our observed
inclinations are consistent with Brown (2001).

Figure 11. Best-ﬁt VR magnitudes for the previously-known objects (left) and unidentiﬁed
objects (right).

4.3. Magnitude Estimation and Analysis

Figure 11 shows our estimates of the VR magnitude of the known and unidentiﬁed
objects detected with KBMOD. To ﬁt the VR magnitudes, we generated 25x25 pixel
postage stamps in the undiﬀerenced science images following the KBMOD linear
trajectory. In each stamp, we ﬁt for the location of the object by maximizing the
value of the ﬂux minus the stamp background. The ﬂux was calculated by summing
the counts within a circular top-hat psf with a radius of twice the FWHM of the
stamp. The local stamp background was estimated from the region outside of this
psf. The magnitude zero point was obtained from the InstCal images. We then took
the median magnitude value from each set of 15 to 20 magnitude estimates.

As we did in Whidden et al. (2019), we compared our joint magnitude distribution
with the apparent magnitude luminosity function presented in Fraser et al. (2008),
adjusting for ecliptic latitude by using the inclination distribution of Brown (2001).
We use the < V R − R > KBO color reported by Fraser et al. (2008). We note,
however, that the DECam VR ﬁlter of our observations diﬀers somewhat from the
Mosaic2 VR ﬁlter used in Fraser et al. (2008). They have similar central wavelengths,
but diﬀerent ﬁlter response curves. Individual magnitudes are shown in Table A. The
magnitude uncertainties listed in Table A are reported as σG uncertainties estimated
from each set of magnitude estimates.

21222324VR Magnitude024681012NumberKnown Objects22.523.023.524.024.525.0VR Magnitude0.02.55.07.510.012.515.017.520.0NumberUnidentified Objects22

Smotherman et al.

Figure 12. VR magnitude distribution (orange) of recovered objects (known and unidenti-
ﬁed), along with the number of objects predicted by Fraser et al. (2008) assuming a circular
camera footprint of 3 square degrees with no ﬁll factor (blue) and a ﬁll factor of 0.55 (green).
Uncertainties in the orange histogram are calculated as 1σ Poisson intervals of

N .

√

We approximate the camera footprint as a 3 square degree circle.

In practice,
our trajectories do not cover the entire camera footprint. Each individual KBMOD
search only uses data a single CCD, requiring 60 individual searches to cover a full
camera footprint. We further require that each candidate trajectory have at least 15
observations, corresponding to a time baseline of about 3 days. Depending on the
search velocity and angle, this means that any objects that start near a CCD edge will
not be searched, as the trajectory will go oﬀ the CCD edge before the trajectory has
the requisite 15 observations. We deﬁne an eﬀective search ﬁll factor as the fraction
of the CCD that is actually searched with KBMOD. For our search parameters, the
search ﬁll factor varies from about 0.5 to about 0.9. Assuming a typical KBO speed
and angle of 275 pixels per day with an in-image angle of 4.4 radians gives a typical
search ﬁll factor of around 0.7. Multiplying this by the camera active-pixel ﬁll factor
of about 0.8 gives a typical net ﬁll factor of approximately 0.55.

Figure 12 shows a histogram of our observed VR magnitudes along with the number
expected from Fraser et al. (2008) assuming a ﬁll factor of 1.0 and 0.55. Our joint
magnitude distribution is largely inconsistent with Fraser et al. (2008) assuming a

2021222324VRmag101100101102Number of objects in VRmag binExpected from Fraser et al. (2008) (Fill Factor=0.55)Expected from Fraser et al. (2008) (Fill Factor=1.0)RecoveredSifting Through the Static

23

ﬁll factor of 1.0, but is consistent to within uncertainties up to about V R = 23.25
assuming a ﬁll factor of 0.55.

5. DISCUSSION

The improvements already presented in this paper helped enable KBMOD to detect
22 out of 26 known objects in the recovery sample. The trajectories of these known
objects were recovered with a median error in starting position of less than two
pixels. Furthermore, KBMOD was able to detect 75 objects that we were unable
to link with any previously-known objects. Although the time baseline of the data
was short, we were able to ﬁt the barycentric distance, inclination, and longitude
of ascending node of both the known objects and the unidentiﬁed objects. The
inclination distribution of the recovered objects is consistent with the distribution
from Brown (2001). The number of objects detected as a function of magnitude is
consistent with the distribution from Fraser et al. (2008) assuming a net ﬁll factor of
0.55.

Whidden et al. (2019) validated KBMOD on the High Cadence Transient Survey
(HiTS) (F¨orster et al. 2016). This work validates algorithmic improvements to KB-
MOD ﬁltering with a survey that has a time baseline of up to four nights, compared
to the three nights used in Whidden et al. (2019). Furthermore, this work validates
KBMOD as applied to images that have been diﬀerenced with a coadded template.
In so doing, we demonstrated that KBMOD can recover KBOs in diﬀerence images
from a survey with a longer time baseline and an irregular cadence. However, this
required more robust ﬁltering methods. By adding GPU ﬁltering, we have increased
the eﬀective number of potentially-valid candidate trajectories that can be passed
out of the GPU for further ﬁltering and analysis. With the σG-based ﬁltering, we
have also implemented more robust lightcurve ﬁltering that improves ﬁltering with
an irregular image cadence. The CNN ResNet50 stamp ﬁlter shows great promise for
future stamp ﬁltering methods.

Next-generation astronomy surveys will soon be current-generation. This imminent
wealth of data will require new computational tools in order to access its full potential.
KBMOD has the potential to increase the number of TNOs detected with LSST from
∼ 40, 000 to ∼ 320, 000 as well as investigate the faint and mysterious class of objects
at the very edge of our Solar System. In terms of probing the sednoids, with three
months of coadded data we could detect a Sedna-like object at opposition at over 290
au, as opposed to ∼ 210 au for a single image. With a year of coadded data, 290
au increases to 310 au. If we could coadd the entire LSST survey, 310 au increases
to over 415 au. Note that objects on elliptical orbits spend much more of their time
If Sedna, which was detected near its perihelion around 90
further from the Sun.
au (Brown et al. 2004), is representative of a larger population of sedoids, then most
of these objects should be closer to apocenter than pericenter. Therefore, a linear
increase in detection distance should yield a super-linear increase in the number of

24

Smotherman et al.

detected objects on a similar orbit. With this coaddition approach, it might even be
possible to detect inner Oort Cloud objects with perihelion near 400 au, and aphelion
well beyond.

Further work is needed before KBMOD will be able to run on LSST. We do not
currently address the “look-elsewhere” eﬀect in our search algorithm (e.g. Vitells &
Gross 2011). However, our false positives are already dominated by image artifacts
and real sources. Even after ﬁltering, trajectories require human by-eye conﬁrma-
tion or rejection. Because of this requirement of human review, we consider this
an acceptable limitation. Future work will further investigate necessary algorithmic
improvements to enable machine-only object conﬁrmation, including addressing the
“look-elsewhere” eﬀect.

Enabling KBMOD to search across multiple CCDs will increase the eﬀective ﬁll
factor, enabling greater completeness and longer time baselines. CCD chip gaps and
camera edges will always keep the ﬁll factor below 1.0 (relative to a circular footprint).
However, with a CCD chip gap between 153 (columns) and 201 (rows) pixels, a KBO
would move past the chip gap and onto the next CCD in about one night, assuming
a typical KBO velocity of 275 pixels per day.

Improving image astrometry and image diﬀerencing is likely to reduce the number
of image diﬀerencing artifacts, thereby reducing the number of candidate trajectories
requiring by-eye detection. The non-uniformity of the image time baseline in this sur-
vey means that artifacts appeared in approximately the same location in up to ﬁve
images. This posed a unique challenge to ﬁltering out artifacts from candidate tra-
jectories. Because of these factors, and because we ultimately validate each detected
object by-eye, we save a full eﬃciency analysis of KBMOD for a future survey.

Given the relatively low inclination (median value of (cid:101)i = 7.67◦) and barycentric
distances between 30 au and 50 (median value of (cid:101)r0 = 41.28 au), we ﬁnd it likely
that the majority of the unidentiﬁed objects presented in Figure 8 are Kuiper belt
objects. However, because the short arcs prevent us from placing accurate constraints
on semi-major axis and eccentricity, we are unable to conﬁrm this prediction with the
current data. Future follow-up or precovery attempts for these objects may be able to
extend the observational arcs enough to accurately constrain them to the Kuiper belt,
and perhaps place them within a Kuiper belt subpopulation (e.g. the cold classical
Kuiper belt).

The authors acknowledge support

from NASA awards NNG16PJ23C and
80NSSC21K1528, and NSF awards AST-1715122, AST-1409547, and OAC-1739419.
This work used the Extreme Science and Engineering Discovery Environment
(XSEDE; Towns et al. 2014), which is supported by National Science Founda-
tion grant number ACI-1548562. This work used the XSEDE Bridges GPU and
Bridges-2 GPU-AI at the Pittsburgh Supercomputing Center through allocation TG-
AST200009. The authors acknowledge support from the DIRAC Institute in the
Department of Astronomy at the University of Washington. The DIRAC Institute is

Sifting Through the Static

25

supported through generous gifts from the Charles and Lisa Simonyi Fund for Arts
and Sciences, and the Washington Research Foundation.

Software: KBMOD (Whidden et al. 2019), LSST Science Pipelines (Juri´c et al. 2017),
astropy(AstropyCollaborationetal.2013),scikit-image(vanderWaltetal.2014),numpy
(Oliphant 2006), CUDA (Nickolls et al. 2008), scikit-learn (Pedregosa et al. 2012), pandas
(McKinney 2010), matplotlib (Hunter 2007), tensorﬂow (Abadi et al. 2016)

Abadi, M., Barham, P., Chen, J., et al.

Fraser, W. C., Kavelaars, J., Holman, M.,

REFERENCES

2016, in Proceedings of the 12th
USENIX Conference on Operating
Systems Design and Implementation,
OSDI’16 (USA: USENIX Association),
265–283

Alard, C., & Lupton, R. H. 1998, The
Astrophysical Journal, 503, 325.
http://stacks.iop.org/0004-637X/503/
i=1/a=325

Astropy Collaboration, Robitaille, T. P.,
Tollerud, E. J., et al. 2013, A&A, 558,
A33

Bernstein, G., & Khushalani, B. 2000,
The Astronomical journal, 120, 3323
Bernstein, G. M., Trilling, D. E., Allen,
R. L., et al. 2004, The Astronomical
Journal, 128, 1364

Berthier, J., Vachier, F., Thuillot, W.,

et al. 2006, in Astronomical Society of
the Paciﬁc Conference Series, Vol. 351,
Astronomical Data Analysis Software
and Systems XV, ed. C. Gabriel,
C. Arviset, D. Ponz, & S. Enrique, 367

Brasser, R., Duncan, M., & Levison, H.

2006, Icarus, 184, 59 .
http://www.sciencedirect.com/science/
article/pii/S0019103506001230
Brown, M. E. 2001, AJ, 121, 2804
Brown, M. E., Trujillo, C., & Rabinowitz,
D. 2004, The Astrophysical Journal,
617, 645.
https://doi.org/10.1086%2F422095

Flaugher, B., Diehl, H. T., Honscheid, K.,
et al. 2015, The Astronomical Journal,
150, 150. http://stacks.iop.org/
1538-3881/150/i=5/a=150

F¨orster, F., Maureira, J. C., San Mart´ın,

J., et al. 2016, ApJ, 832, 155

et al. 2008, Icarus, 195, 827 .
http://www.sciencedirect.com/science/
article/pii/S0019103508000705
Giorgini, J. D. 2015, IAU General

Assembly, 22, 2256293

Gladman, B., & Chan, C. 2006, ApJL,

643, L135

Hahn, J. M., & Malhotra, R. 2005, AJ,

130, 2392

He, K., Zhang, X., Ren, S., & Sun, J.
2016, in 2016 IEEE Conference on
Computer Vision and Pattern
Recognition (CVPR), 770–778

Herner, K., Annis, J., Brout, D., et al.

2020, Astronomy and Computing, 33,
100425.
https://www.sciencedirect.com/
science/article/pii/S2213133720300792
Hunter, J. D. 2007, Computing In Science

& Engineering, 9, 90

Ivezi´c, ˇZ., Connolly, A., Vanderplas, J., &
Gray, A. 2014, Statistics, Data Mining
and Machine Learning in Astronomy
(Princeton University Press)

Ivezi´c, ˇZ., Kahn, S. M., Tyson, J. A.,

et al. 2019, The Astrophysical Journal,
873, 111. https:
//doi.org/10.3847/1538-4357/ab042c
Juri´c, M., Kantor, J., Lim, K. T., et al.
2017, in Astronomical Society of the
Paciﬁc Conference Series, Vol. 512,
Astronomical Data Analysis Software
and Systems XXV, ed. N. P. F. Lorente,
K. Shortridge, & R. Wayth, 279
Kaib, N. A., Roˇskar, R., & Quinn, T.

2011, Icarus, 215, 491 .
http://www.sciencedirect.com/science/
article/pii/S0019103511003101

26

Smotherman et al.

Kenyon, S. J., & Bromley, B. C. 2004,

Towns, J., Cockerill, T., Dahan, M., et al.

Nature, 432, 598

LSST Science Collaboration, Abell, P. A.,
Allison, J., et al. 2009, LSST Science
Book, Version 2.0, LSST Corporation,
arXiv:0912.0201

Luu, J. X., & Jewitt, D. C. 2002, Annual
Review of Astronomy and Astrophysics,
40, 63. https://doi.org/10.1146/
annurev.astro.40.060401.093818

McKinney, W. 2010, in Proceedings of the
9th Python in Science Conference, ed.
S. van der Walt & J. Millman, 51 – 56

Morbidelli, A., & Nesvorn´y, D. 2020, in

The Trans-Neptunian Solar System, ed.
D. Prialnik, M. A. Barucci, & L. A.
Young (Elsevier), 25–59. https:
//www.sciencedirect.com/science/
article/pii/B9780128164907000023
Nesvorn´y, D. 2015, The Astronomical

journal, 150, 68

Nickolls, J., Buck, I., Garland, M., &

Skadron, K. 2008, Queue, 6, 40. http://
doi.acm.org/10.1145/1365490.1365500

NOAO. 2015, NOAO Data Handbook

(National Optical Astronomy
Observatory).
http://ast.noao.edu/sites/default/ﬁles/
NOAO DHB v2.2.pdf

Ofek, E. O., & Zackay, B. 2018, AJ, 155,

169

Oliphant, T. E. 2006, A guide to NumPy

(USA: Trelgol Publishing)

Pedregosa, F., Varoquaux, G., Gramfort,

A., et al. 2012, ArXiv e-prints,
arXiv:1201.0490

Sullivan, I., & Bellm, E. 2021,

DMTN-171: Fall 2020 status of crowded
ﬁeld processing with the LSST Alert
Production Pipelines, LSST Data
Management Technical Note
DMTN-171, LSST Data Management,
doi:10.5281/zenodo.5172677. https:
//doi.org/10.5281/zenodo.5172677

2014, Computing in Science &
Engineering, 16, 62

Trilling, D. E., Valdes, F., Allen, L., et al.
2017, The Astronomical Journal, 154,
170. http://stacks.iop.org/1538-3881/
154/i=4/a=170

Tsiganis, K., Gomes, R., Morbidelli, A., &
Levison, H. F. 2005, Nature, 435, 459,
copyright - Copyright Macmillan
Journals Ltd. May 26, 2005; Document
feature - references; graphs; Last
updated - 2017-10-31; CODEN -
NATUAS.
https://search.proquest.com/docview/
204581724?accountid=14784

Valdes, F., Gruendl, R., & DES Project.
2014, in Astronomical Society of the
Paciﬁc Conference Series, Vol. 485,
Astronomical Data Analysis Software
and Systems XXIII, ed. N. Manset &
P. Forshay, 379

van der Walt, S., Sch¨onberger, J. L.,

Nunez-Iglesias, J., et al. 2014, PeerJ, 2,
e453.
http://dx.doi.org/10.7717/peerj.453

Vitells, O., & Gross, E. 2011,

Astroparticle Physics, 35, 230
Whidden, P. J., Kalmbach, J. B.,

Connolly, A. J., et al. 2019, The
Astronomical Journal, 157, 119.
https://doi.org/10.3847%
2F1538-3881%2Faafd2d

Zackay, B., & Ofek, E. O. 2017a, ApJ,

836, 188

—. 2017b, ApJ, 836, 187
Zackay, B., Ofek, E. O., & Gal-Yam, A.

2016, ApJ, 830, 27

Sifting Through the Static

27

APPENDIX

A. TABLE OF DETECTED OBJECT PARAMETERS

Identiﬁer
(pg, ccd)

VR Mag

Barycentric
Distance
(au)

i (degrees)

Ω (degrees)

Linked to
Known Object

(190,20)
(190,23)
(191,27)
(191,47)
(192,06)
(192,36)
(192,42)
(193,05)
(193,18)
(193,21)
(193,23)
(193,50)
(301,46)
(302,06)
(195,47)
(202,48)
(203,09)
(203,11)
(203,43)
(205,18)
(284,29)
(285,22)
(296,28)
(300,30)
(017,46)
(018,20)
(018,52)
(191,19)
(191,50)
(192,05)
(192,08)
(192,08)
(192,54)
(193,07)
(193,10)
(193,14)
(193,14)
(193,26)
(193,32)

24.46±0.22 38.38±2.42
24.13±0.82 42.02±2.43
24.27±0.23 39.76±2.41
24.18±0.40 31.87±2.31
24.51±0.27 54.00±2.60
24.48±0.40 41.97±2.46
24.37±0.29 43.62±2.47
24.61±0.29 39.75±2.41
24.44±0.37 42.30±2.51
24.48±0.36 46.19±2.49
23.72±0.19 43.08±2.46
24.31±0.29 32.36±2.51
23.76±0.57 44.95±2.61
22.83±0.29 38.44±3.69
22.97±0.15 32.48±2.45
23.26±0.17 40.88±2.58
24.18±0.68 44.53±2.48
23.92±0.20 48.25±2.90
24.52±0.47 44.81±2.53
24.04±0.59 41.57±2.47
22.06±0.46 34.99±2.54
20.75±0.07 41.60±3.94
22.69±0.16 41.22±2.71
23.70±0.98 46.90±2.94
23.99±0.33 35.59±2.45
23.93±1.26 38.64±2.45
24.78±0.80 46.17±2.57
24.99±0.58 42.91±2.44
24.14±0.36 30.67±1.17
24.41±0.24 41.54±2.43
24.20±0.23 30.66±1.17
24.74±0.24 40.49±2.42
22.24±0.04 32.95±1.16
24.42±0.33 41.65±2.44
24.65±0.52 42.19±2.44
23.96±0.41 42.47±2.45
25.02±0.93 41.86±2.43
23.97±0.14 42.55±2.44
24.48±0.64 36.70±2.37

True
48.73±4.99
8.58±2.99
84.35±10.33 True
3.10±0.50
True
36.21±0.22
3.60±1.33
31.89±1.58
True
4.34±1.69
206.20±4.28 True
2.52±0.92
True
35.82±0.84
5.16±1.89
True
24.31±5.10
2.14±0.78
True
90.86±9.91
2.75±0.35
206.48±4.19 True
11.27±4.05
73.62±9.82
True
2.90±0.67
173.42±11.60 True
2.82±0.59
True
42.20±1.84
17.95±7.29
2.02±0.74
True
8.43±5.74
30.55±17.70 199.08±2.98 True
True
9.90±3.56
14.35±8.85
47.86±3.56
16.22±5.92
True
166.51±12.63 True
2.08±0.36
215.16±1.73 True
24.39±9.49
True
45.78±2.30
8.68±3.05
3.22±0.04
122.65±4.57 True
173.36±0.54 136.65±10.42 True
349.32±17.52 True
14.00±6.38
252.08±13.39 True
6.75±1.43
True
24.73±0.84
17.79±7.56
False
285.01±7.11
5.31±0.24
False
316.13±5.57
4.92±0.10
False
243.09±8.42
12.08±3.39
False
1.58±0.62
39.44±1.65
False
156.06±4.63 216.93±0.01
False
58.69±10.24
1.07±0.49
False
155.99±4.65 216.93±0.01
False
0.80±0.37
42.48±2.53
False
166.54±2.55 41.03±0.58
False
91.13±9.69
3.00±0.38
False
89.64±10.07
2.70±0.37
False
62.14±7.95
4.01±1.17
False
107.81±9.23
1.83±0.10
False
63.12±8.68
3.06±0.93
False
50.56±5.10
4.86±1.69
Continued on next page

28

Smotherman et al.

Identiﬁer
(pg, ccd)

VR Mag

Barycentric
Distance
(au)

i (degrees)

Ω (degrees)

Linked to
Known Object

(193,40)
(301,30)
(301,40)
(305,14)
(305,28)
(305,60)
(306,47)
(306,48)
(306,49)
(307,05)
(310,29)
(310,29)
(310,36)
(311,46)
(313,60)
(314,13)
(316,45)
(316,55)
(194,18)
(194,21)
(194,27)
(195,20)
(195,60)
(196,30)
(197,19)
(197,34)
(197,36)
(197,58)
(202,05)
(202,20)
(202,27)
(202,36)
(202,40)
(202,42)
(203,12)
(203,43)
(204,21)
(204,21)
(204,27)
(204,41)
(205,22)
(205,23)
(205,49)

24.62±0.31 42.11±2.44
22.65±1.01 41.40±2.73
23.69±0.40 41.28±2.65
23.27±0.34 39.36±2.42
23.40±0.43 41.71±2.44
23.23±0.46 33.71±2.69
23.42±0.28 42.58±2.51
22.90±0.54 38.66±2.59
22.93±0.69 38.26±2.46
23.36±0.69 32.65±2.60
22.95±0.18 46.00±2.53
23.01±0.55 47.06±2.54
23.93±0.68 39.59±3.05
22.38±0.19 41.20±2.90
22.85±0.24 34.45±2.43
23.21±0.26 34.06±2.38
23.17±0.51 45.23±2.49
23.06±0.22 42.89±2.46
24.61±0.68 43.06±2.45
24.42±0.26 43.55±2.46
24.52±...
34.99±2.46
24.14±0.21 48.32±2.78
24.84±0.70 34.81±2.41
24.61±0.57 35.51±2.58
24.46±0.49 43.06±2.46
22.84±0.23 38.20±2.45
24.75±0.73 41.91±2.45
24.13±0.60 35.47±2.71
23.88±0.13 41.77±2.45
23.60±0.40 46.02±2.53
23.44±0.28 39.07±2.44
23.71±0.29 38.38±3.22
23.43±0.24 34.43±3.42
24.27±0.60 42.86±4.42
24.18±0.56 45.41±2.56
24.57±1.29 41.76±2.45
23.95±1.04 46.44±2.54
24.04±0.45 42.46±2.48
24.00±0.42 47.18±2.59
24.07±0.48 44.49±2.50
24.37±0.45 38.39±2.49
24.14±0.33 41.25±2.53
24.26±0.40 43.63±2.49

66.85±9.23
False
2.22±0.62
False
203.61±0.50
16.18±6.70
238.25±15.08 False
0.63±0.24
False
36.55±3.74
7.04±2.54
False
129.48±8.93
1.24±0.04
False
24.33±10.40 206.56±0.24
False
63.85±10.83
4.02±0.99
False
37.81±4.62
13.04±5.12
False
130.35±5.44
2.75±0.07
False
135.87±7.47
5.20±0.24
False
105.05±5.51
4.40±0.11
False
125.85±3.95
4.18±0.03
False
28.70±12.84 202.30±3.92
25.12±10.68 23.80±0.88
False
248.71±11.50 False
5.22±1.24
False
213.63±2.65
4.65±1.81
False
14.69±4.92
6.53±2.19
19.17±3.99
6.75±2.36
False
351.73±13.15 False
1.51±0.34
False
5.44±9.74
2.94±0.83
False
222.73±2.60
14.26±5.49
False
225.49±3.55
21.31±7.74
False
311.05±4.15
4.34±0.03
25.81±5.82
14.99±6.01
False
341.70±11.82 False
2.46±0.34
229.64±4.24
10.77±3.82
False
359.82±11.28 False
3.26±0.80
False
32.45±2.94
23.38±9.82
False
121.93±4.22
3.54±0.03
False
58.42±6.64
8.61±2.62
False
8.62±2.75
57.97±6.73
False
34.18±15.74 42.95±2.59
False
35.99±18.64 41.40±2.35
False
46.16±26.51 41.31±2.46
209.74±3.52
10.71±3.84
False
130.90±14.38 False
1.04±0.01
False
211.76±3.18
5.71±2.04
False
62.47±7.19
2.51±0.78
216.79±1.34
10.79±3.91
False
177.43±21.56 False
0.61±0.24
False
198.45±7.99
7.67±2.68
False
205.70±5.58
11.44±4.13
False
139.21±8.76
2.33±0.06
Continued on next page

Sifting Through the Static

29

Identiﬁer
(pg, ccd)

(284,13)
(284,29)
(284,42)
(284,52)
(284,59)
(288,15)
(288,29)
(288,48)
(289,48)
(289,48)
(290,23)
(291,08)
(291,27)
(296,43)
(297,46)
(298,22)
(298,26)

Linked to
Known Object

VR Mag

i (degrees)

Ω (degrees)

Barycentric
Distance
(au)
10.41±...
False
31.17±...
23.45±0.18 36.40±...
340.77±22.78 False
9.23±4.19
23.59±0.68 37.56±4.53
238.99±18.72 False
10.56±4.43
22.60±0.18 41.54±3.52
340.23±16.58 False
10.45±3.41
23.68±0.27 41.11±3.67
254.19±32.60 False
8.47±3.74
23.25±0.62 36.70±4.97
False
12.57±4.05
17.81±9.31
23.89±0.48 45.70±3.47
False
30.20±20.59 204.55±3.45
23.41±0.28 44.98±4.41
19.55±13.47 208.92±6.35
False
23.54±0.27 40.10±3.98
21.02±27.89 213.72±18.67 False
23.51±0.41 38.02±6.91
233.24±15.66 False
8.96±3.85
23.82±0.51 42.45±3.30
False
26.35±12.60 211.83±4.95
23.84±0.37 47.45±3.38
False
40.90±...
23.25±0.26 34.80±...
23.85±0.61 33.10±10.91 29.45±64.35 206.97±13.80 False
327.10±10.71 False
23.60±0.31 38.91±2.78
6.00±0.71
False
7.33±0.07
23.33±0.27 33.72±4.03
291.61±6.21
False
36.73±20.72 216.21±7.06
23.95±0.41 45.59±4.09
22.38±0.59 33.54±...
False
39.53±...
Table 1. Best-ﬁt parameters estimated from the DECam
NEO Survey data for the objects detected with KBMOD.
Objects are identiﬁed based on their detected pointing group
(pg) and CCD. Orbital values and uncertainties are found
using the method of Bernstein & Khushalani (2000). VR
magnitudes are found as described in 4.3. VR magnitude
uncertainties are σG uncertainties, estimated based on the
individual stamp VR magnitude estimates. Parameters for
which no uncertainty was returned are indicated with an el-
lipsis in the uncertainty value.

17.32±...

18.00±...

