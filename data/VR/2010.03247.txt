Exploration of Hands-free Text Entry Techniques For Virtual Reality

Xueshi Lu
Xi’an Jiaotong-Liverpool University

Wenge Xu
Xi’an Jiaotong-Liverpool University

Difeng Yu
The University of Melbourne
Yuzheng Chen
Xi’an Jiaotong-Liverpool University
Khalad Hasan
University of British Columbia - Okanagan

Hai-Ning Liang*
Xi’an Jiaotong-Liverpool University

Xiang Li
Xi’an Jiaotong-Liverpool University

0
2
0
2

t
c
O
7

]

C
H
.
s
c
[

1
v
7
4
2
3
0
.
0
1
0
2
:
v
i
X
r
a

ABSTRACT
Text entry is a common activity in virtual reality (VR) systems.
There is a limited number of available hands-free techniques, which
allow users to carry out text entry when users’ hands are busy such
as holding items or hand-based devices are not available. The most
used hands-free text entry technique is DwellType, where a user
selects a letter by dwelling over it for a speciﬁc period. However, its
performance is limited due to the ﬁxed dwell time for each character
selection. In this paper, we explore two other hands-free text entry
mechanisms in VR: BlinkType and NeckType, which leverage users’
eye blinks and neck’s forward and backward movements to select
letters. With a user study, we compare the performance of the two
techniques with DwellType. Results show that users can achieve an
average text entry rate of 13.47, 11.18 and 11.65 words per minute
with BlinkType, NeckType, and DwellType, respectively. Users’
subjective feedback shows BlinkType as the preferred technique for
text entry in VR.

Keywords: Virtual reality; Text Entry; Dwelling; Eye Blinking;
NeckType; Head-Mounted Display.

Index Terms:
Human-centered computing—Human com-
puter interaction (HCI)—Interaction paradigms—Virtual reality;
Human-centered computing—Human computer interaction (HCI)—
Interaction techniques—Text input

1 INTRODUCTION
Text entry is essential for all types of interactive systems, includ-
ing Virtual Reality (VR). Users, for example, often need to input
passwords to login to a system or send text messages to chat with
other users in a virtual social environment. Typically, text entry in
VR is commonly performed through hand-held controllers [32, 40].
However, there are situations where users’ hands are occupied and
cannot use their hands to hold a controller to perform text entry [18];
alternatively, oftentimes a controller or other hand-based devices are
not readily available. For example, a doctor who is using a headset
for surgery training might not be able to use a controller to perform
text entry as his/her hands are occupied with holding surgical tools.
In addition, users who have hand/arm disabilities are likely not able
to use their hands to hold a controller [36]. In these scenarios, an
efﬁcient and usable hands-free text entry technique would be the
most convenient solution.

A limited number of hands-free text entry techniques have been
proposed for VR. One of the most commonly used hands-free tech-
nique in VR is the dwell-based technique, where users enter text by
controlling a virtual pointer with head motions and keep the pointer
hovering on a target letter for a certain period of time to trigger
a selection [35, 39, 41]. However, the dwell-based technique has

*Corresponding author: haining.liang@xjtlu.edu.cn

Figure 1: An illustration of the text entry with three hands-free tech-
niques in VR. (a) A user wearing a head-mounted device can use (b)
DwellType where a selection is done with by dwelling over a key; (c)
BlinkType that where eye blinking is used to trigger a selection; and
(d) NeckType, where neck movement is used, invoke a key. (e) After a
successful selection, the target key becomes larger and highlighted.

certain limitations such as: (1) a long dwell time may decrease per-
formance but a short dwell time can cause false-positive selections
and errors [12]; (2) a pre-set dwell time always “pushed” users to
select a target key and quickly move to the next one, a process that
can be stressful [16]; and (3) keeping the pointer static for a while
to avoid selecting unintentional keys could further lead to eye and
neck fatigue [29]. In addition to dwelling, speech-based techniques
have been proposed as viable solutions to allow hands-free text en-
try [25]. However, its performance suffers in noisy environments and
could trigger privacy-related concerns [37]. Gaze-based text entry
is commonly used in other settings, such as a common display (i.e.,
EyeK [29]). However, eye-tracking systems are sensitive and not
very reliable to enable gaze to be used for text entry that is efﬁcient
and have low error rates [23]. In addition, gaze-based techniques
often require rapid eye movements coupled with a period for the eye
to be ﬁxed and looking at one place (i.e., similar to a dwell time).
Both activities can cause visual fatigue due to the repetitive nature
of text entry [4].

In this paper, we present two alternative hands-free text entry
techniques for VR systems: BlinkType and NeckType (see Figure
1). BlinkType is an eye blink-based technique that allows users to
select letters in VR by using their eye blinks (Figure 1c). Neck-
Type allows users to use their neck’s forward and backward motion
as the selection mechanism (Figure 1d). Prior work showed that
eye blinking can also be used for hands-free text entry, where the
blinking is used for letter selection [37]. However, eye blinking was
evaluated on different platforms other than VR and for individuals
with severe motor impairments. In addition, research has shown that
it is still unclear how well-received eye blinking, as a mechanism
for selection conﬁrmation, is by users [20, 37].

We conducted a user study with 36 participants to evaluate the
performance of these two techniques and compared it against Dwell-
Type. Our results showed that BlinkType, NeckType, and DwellType

 
 
 
 
 
 
could achieve an average text entry rate of 13.47, 11.18 and 11.65
words per minute (wpm). Results also revealed that BlinkType is
the most preferred technique for hands-free text entry in VR.

The main contribution of the paper is a comparative evaluation
of three hands-free text entry techniques based on text entry perfor-
mance, user experience, and stimulator sickness.

2 RELATED WORK

While several approaches have been proposed for entering text in
VR systems (see recent reviews in [22, 25, 28]), few of them have
explored and compared hands-free text entry techniques in depth.
In this section, we review previous research related to text entry in
VR and non-VR systems: hands-free typing and dwell-free typing.
Moreover, we give a description of eye blink and its properties.

2.1 Hands-free Typing

Speech-based is one feasible method to achieve hands-free text entry.
Pick et al. [25] devised SWIFTER, a speech-based multimodal text
entry system, which could lead to a text entry rate of 23.6 wpm in
average. Furthermore, Ruan et al. [28] evaluated the state-of-art
speech recognition using Baidu’s Deep Speech 2 and compared it
to mobile touch-based keyboards for both English and Mandarin
Chinese. Their results revealed that speech is nearly three times
faster than keyboard. However, it also led to more errors. There are
two disadvantages of speech techniques: (1) they need a relatively
quiet environment; and (2) they can lead to privacy problems. These
issues prevent speech text entry to be used in many public spaces
like libraries, restaurants, shops, malls, etc.

Gaze typing, as suggested by studies in non-VR settings [22],
could be another possibility to allow hands-free text entry in VR.
The most common way of gaze typing is using a dwell time, which
requires a user to hover the cursor on the target key for a prede-
termined duration to trigger selection. The dwell time is usually
set as a constant value between 400ms-1000ms and leads to a text
entry speed from 5-10 wpm [7, 8, 26]. To enable fast gaze typing,
Majaranta et al. [22] allowed the users to personalize the dwell time.
Personalization is often beneﬁcial since users might have their pre-
ferred dwell time, especially during the different learning stages and
environment settings. Rajanna et al. explored dwell typing while sit-
ting and biking. They observed that sitting+dwell and biking+dwell
achieved 9.36 and 8.07 wpm, respectively [27]. In this research,
we evaluated and compared gaze- and head-based methods in VR
settings.

2.2 Dwell-free Typing

Apart from dwell-based techniques, researchers have proposed dwell-
free approaches. A certain number of dwell-free text entry tech-
niques rely on tracking users’ eyes. Kristensson et al. [16] explored
the potential speed of dwell-free eye-typing. Their results show that
users could reach a theoretical average input rate of 46 wpm after 40
minutes—that is, this approach could be twice as fast as gaze typing
techniques.

Gaze-typing techniques using gesture-based approaches have
also been studied [37]. EyeWriter [34], a technique based on the
gestural unistroke alphabet from EdgeWrite, is a gesture-based eye
typing technique. Eye-S [26] is another type of gesture-based typing
method that requires users to draw letters on 9 speciﬁc hotspots
and can lead to a mean rate of 6.8 wpm. EyeSwipe [17] allows
users gaze-type the ﬁrst and last characters of the words by reverse
crossing and glancing through the vicinity of the middle characters,
then displaying the candidate words. Users could reach 11.7 wpm
after 30 minutes of practice using this system.

Besides gesture-based techniques, selection-based typing tech-
niques are also widely applied to dwell-free typing techniques.
EyeK [29] allows users to type by moving the cursor inside-outside-
inside the speciﬁc area. In Filteryedping [24], the user looks at

characters in the same order of the required word, then the system
automatically ﬁlters out unwanted words while and ranks the can-
didate words based on the length and frequency. This technique
is claimed to reach a mean text entry rate of 15.95 wpm. Blink is
a viable way to control input as well. BlinkWrite2 [1] is an eye
blink text entry system, where participants could reach an average
text entry speed of 5.3 wpm. BlinkWrite2 presents the potential of
blink typing. However, the technique has not been tested with VR
HMDs and its performance is low compared to other techniques.
Our technique, BlinkType, is based on blinking and allows users
a fast input rate (at 13.47 wpm by participants using it for 45 min-
utes). As such, it is faster than other blink based techniques like
BlinkWrite2 (5.3 wpm) and very competitive with other types of
hands-free techniques.

2.3 Eye Blinks and Properties

Blinking is a bodily function of rapid, semiautomatic closing of
the eyelids. In general, eye blinks can be classiﬁed as spontaneous
blinks and reﬂex blinks [2]. Spontaneous blinking occurs without
external stimuli and internal effort, and a person typically blinks
10-20 times per minute every 4-19 seconds [5]. Reﬂex blinks are
activated by an external object touching to the cornea, bright light
shining in front of the eye, or rapidly approaching objects. The
duration of a single blink depends on the situations and could range
from 100-400ms. Dennison et al. [9] stated that blinking frequency
is higher while wearing an HMD compared to a normal display
monitor. This evidence is supported by a study from Kim et al. [14]
who suggested that the increased blinking frequency during VE
immersion is due to side effects as well as visual fatigue and motion
sickness. Following previous research on the use of eye blink outside
of VR to support text entry, we have also explored it in pilot studies
and found it promising. As such, we extended this research and
developed a technique that accurately captures eye blinks and uses
them for letter selection. As reported later, our technique leads to
fast rate and is well-accepted by users.

3 THE THREE EVALUATED HANDS-FREE TEXT ENTRY

TECHNIQUES

We designed and implemented three hands-free text entry mecha-
nisms, NeckType, DwellType and BlinkType, for text entry tasks
in VR HMDs. We were interested in exploring whether letter selec-
tions with NeckType and BlinkType offered any advantages over
DwellType. We used head rotation to control a cursor for all three
techniques, while the same QWERTY virtual keyboard was used
for all three techniques (see Figure 1). The keyboard was 370×156
pixels, which took about 14% of the entire screen and was placed
at the center of users’ view, both horizontally and vertically. It was
within the 60 degree ﬁeld-of-view in the HMD. We used the algo-
rithm described in Goodman et al.’s Language modeling [6] for soft
keyboards to predict key sequences. All three techniques used the
same language model which combined the keypress model to predict
the most probable words, especially in cases where the pointer hit
the boundary of the keys. Additionally, we highlighted selected
letters by enlarging them by 1.2 times and changing the background
color as shown in Figure 1e.

We also explored eye movement-related selection approaches
such as gaze up or down. However, we excluded them due to
some limitations: (1) eye movements could lead to involuntary
typing while moving their gaze to browse letters; and (2) gazing
down or up neededs to be conﬁned within the target key area as
otherwise it could produce selection errors. Thus, we excluded any
eye movement-based selection techniques in our design.

3.1 DwellType

DwellType was a dwell-based text entry technique and allowed text
selection by gazing and keeping the cursor on a target key for a

period of time. Our DwellType technique was consistent with the
other previous implementations of dwell techniques [39]. To enter a
character, users ﬁrst needed to move the cursor to the target key and
kept it within the key area for 400ms. We used a circular progress
bar to indicate dwell-time (Figure 1b), and a light beep sound to
help users know whether the character was selected. We set the
dwell time period based on prior research [39], which has been
also common among dwell-based techniques. If users wanted to
enter the same character twice, the time needed is 800ms. Once the
cursor moved to a new key, the dwell time would restart counting
from zero. Users must move out from the keys to avoid typing
the same letter continually. It is worth nothing that an adjustable
dwell time could in theory possibly lead to higher text entry speed.
However, dwell typing with adjustable dwell time has not been fully
evaluated in VR, and as such, it is unknown how it will perform
in virtual environments. The dwell time used was based on recent
research [39] where 400ms was found to be ‘appropriate, under
which users easily committed unintentional selections’.

3.2 NeckType

NeckType leverages the forward (z-axis) acceleration movements,
which can be accurately captured by the built-in IMU sensor of
current HMDs, as the letter selection mechanism. NeckType is an
enhanced version of DepthText [19, 42] and employed the Dynamic
time warping (DTW) [3] algorithm for input recognition. To perform
a selection with NeckType, users ﬁrst moved the pointer on a target
key with the head rotation and conﬁrmed the character by moving
the head forward for about 1cm, which the human neck can do with
ease [19].

We utilized DTW to recognize the z-axis input signals (see Figure
2). Although acceleration curves in the x-axis and y- axis did not
show regular variation patterns, we recorded them simultaneously
as well. To be able to use DTW, we collected neck movements that
are comfortable and easy to do via a pilot study with 6 participants
to produce the standard curve of DTW [38]. We conducted another
pilot study collecting 5 users’ neck movements to determine the
optimal thresholds for the system to detect a selection without any
false positives. We found that the threshold in the z-axis was at most
0.78 while the thresholds in the x-axis and y-axis were at least 0.4
and 0.65 for the system to be able to recognize the selection accu-
rately and effectively and, at the same time, to avoid unintentional
selections while moving the cursor using head motions.

3.3 BlinkType

Eye blinks detection was achieved with Tobii eye trackers from
the HTC VIVE Pro Eye. We also conducted a pilot study with 6
participants to explore suitable eye blink techniques (i.e., blinking
with the left eye, blinking with the right eye, and blinking with both
eyes) in a set of target selection tasks. Participants were required
to select a total of 288 targets in both cardinal and intercardinal
directions where results showed that the target selection accuracy
was higher with both eyes blinking (∼100%) than left eye blinking
(79.5%) and right eye blinking (69.4%). Many participants often
accidentally close both eyes when they were supposed to perform
tasks with single eye, leading to lower accuracy for the single eye
conditions.

Consequently, blinking with both eyes was chosen for our Blink-
Type technique due to its nearly perfect accuracy. Moreover, to fur-
ther reduce the possibility of false-positive selections, participatns
were allowed to gaze at any space outside of the virtual keyboard
to rest their eyes temporarily. To be consistent with the DwellType,
400ms was used as the minimum gap time between double-blink
entries and so users could type double characters by closing their
eyes for 800ms.

As started earlier, we used head rotations for controlling the
cursor with all these three hands-free text entry techniques. There

Figure 2: (left) Corresponding acceleration variation in the z-axis of
a single head forward-backward motion. Such motion produces two
sine-like waves (the ﬁrst segment of the curve stands for moving
forward and the second one for moving backward), which is not found
in other axes. (right) A user is entering text using NeckType by moving
the head forward in the VR system.

are other potential techniques such as gaze-based pointing that could
be included in the study to control the cursor. However, we excluded
gaze for pointing because in a pilot study we observed low accuracy
with the gaze-based pointing compared to head rotations, especially
when gaze pointing was used in conjunction with blinking and neck
movements. In the end, we only used head rotation for moving the
cursor over the letters.

4 USER STUDY
4.1 Participants and Apparatus
Thirty-six (36) participants who had no issue with neck and body
movements and had normal or corrected-to-normal vision were
recruited from a local university. A between-subjects design was
used to evaluate the three hands-free techniques. There were 9 males
and 3 females for NeckType and BlinkType with an average age of
19.7 and 19.8, respectively. For DwellType, there were 10 males
and 2 females with an average age of 19.6. All participants were
familiar with the QWERTY keyboard. None of them took part in the
pilot studies that we conducted to ﬁnd the suitable parameters for
NeckType and BlinkType. Although the participants were not native
English speakers, all of them were familiar with the English alphabet
because English is the language of instruction at their university. The
experiment was conducted on an HTC VIVE Pro Eye which had a
110 degree FOV. We used the controller only for users to proceed
from one trial to another; otherwise, all three techniques were hands-
free. The experimental application was developed with Unity3D.

4.2 Experiment Design and Procedure
We used a between-subjects design to avoid carry-over effects be-
cause the effect of motion sickness could accumulate overtime for
VR systems. Participants’ demographic information was recorded
with a questionnaire. At the beginning of the experiment, each par-
ticipant was briefed with the details of text entry techniques and the
VR HMD they would use. Participants then proceeded to transcribe
phrases from the corpus in the practice stage (∼10 minutes) to get
familiar with the text entry technique and VR environment. After,
they were asked to transcribe 48 phrases, which were divided into
6 sessions evenly. All phrases were randomly generated from the
MacKenzie phrase set [21]. They were also instructed to enter the
text as quickly and accurately as possible. Between sessions, we
asked the participants to rest until they felt comfortable and ready to

Figure 3: (a) The mean text entry speed of three hands-free input
techniques through 6 sessions (‘S’ represents Session). Error bars
represent ±1 standard error. (b) Mean total error rate (TER) and (c)
not corrected error rate of three techniques through 6 sessions (‘S’
stands for Session).

Figure 4: User Experience Questionnaire data regarding to its sub-
scales: Attractiveness (A), Perspicuity (P), Efﬁciency (E), Dependabil-
ity (D), Stimulation (S), and Novelty (N).

move to the next session. After the experiment, participants were
asked to ﬁll out the Simulator Sickness Questionnaire (SSQ) [13]
and the User Experience Questionnaire (UEQ) [10]. We used SSQ
and UEQ as they are commonly used to gather feedback from users
in VR research [15, 30, 33]. At the end, they were asked to share
their experience and feedback about the text entry technique. The
experiment lasted for about 50 minutes.

4.3 Results
To analyse text entry performance, we employed a two-way mixed
ANOVA with Session (from session one to six) as the within-subjects
variable and Technique (NeckType, DwellType and BlinkType) as
the between-subjects variable. For the analysis of user experience
feedback, we applied a one-way ANOVA with Technique as the
between-subjects variable. Furthermore, Bonferroni correction was
used for pairwise comparisons and Greenhouse-Geisser adjustment
was used in case of violations of the sphericity assumption.

Text entry speed was measured in WPM, with a word deﬁned as
ﬁve consecutive letters, including spaces. The error rate is calculated
based on standard typing metrics [31], where the total error rate
(TER) = not corrected error rate (NCER) + corrected error rate
(CER). We report error rates based on TER and NCER.

4.3.1 Text Entry Speed

The two-way mixed ANOVA tests yielded a signiﬁcant effect of Ses-
sion (F5,165 = 29.530, p < .001) and Technique (F2,33 = 3.716, p <
.05) on text entry speed. No signiﬁcant interaction effect was found
on Session × Technique (F10,165 = 1.217, p > .05). Post-hoc pair-
wise comparisons indicated signiﬁcant differences between sessions
1-2, 1-3, 1-4, 1-5, 1- 6, 2-4, 2-5, 2-6, 3-6, 4-6 (all p < .05). In
addition, post-hoc pairwise comparison found that BlinkType (M =
12.18, SD = 1.17) was signiﬁcantly (p < .05) faster than NeckType
(M = 9.43, SD = 0.41). Figure 3a shows the mean typing rate of
the three techniques. BlinkType was faster, with an average speed
of 13.47 wpm in the last session. NeckType and DwellType had
similar typing rates (11.18 and 11.65 wpm, respectively). NeckType
performed the lowest typing speed of 7.27 wpm in the ﬁrst session
but was improved to 11.18 wpm in the last session, with an increase
of 53.78%. Among the participants, the highest text entry speed
appeared in session 4 of BlinkType (at the rate of 18.92 wpm).

4.3.2 Total and Not Correct Error Rate

The two-way mixed ANOVA tests revealed signiﬁcant effects of
Session on TER (F5,165 = 3.960, p < .01) and NCER (F5,165 =
1.252, p < .05). We did not ﬁnd any signiﬁcant effect of Technique
on TER (F2,33 = 0.181, p > .05) and NCER (F2,33 = 0.446, p > .05).
Figure 3b, 3c represents mean TER and NCER over 6 sessions of the
three text entry techniques. With NeckType, TER dropped from the

Figure 5: Simulator Sickness ratings in terms of Oculomotor (O),
Nausea (N), Disorientation (D), and Total Severity (TS). Error bars
represents ±1 standard error.

ﬁrst session of 14.38% to the sixth session of 9.04%, with a decrease
of 59.14%. The mean TER in the last session with BlinkType and
DwellType reached 10.44% and 9.64% respectively. The NCER
showed irregular variation according to the data and reached at
1.01%, 0.90%, and 0.61%, respectively, with NeckType, BlinkType
and DwellType.

4.3.3 User Experience Questionnaire (UEQ)

The one-way ANOVA indicated that Technique had a signiﬁcant
(F2,33 = 4.395, p < .05) inﬂuence on Perspicuity. Post-hoc pairwise
comparisons showed that the Perspicuity rating for BlinkType (M =
2.00, SD = 0.27) was signiﬁcantly (p < .05) higher than NeckType
(M = 0.81, SD = 0.17). The analysis also revealed that Technique had
a signiﬁcant (F2,27 = 3.883, p < .05) effect on Dependability. Post-
hoc pairwise comparisons showed that the Dependability rating for
BlinkType (M = 1.90, SD = 0.33) was signiﬁcantly (p < .05) higher
than NeckType (M = 0.88, SD = 0.13). No other signiﬁcant effects
were found. We performed a correlation analysis of Perspicuity
and Dependability scores and wpm to understand why there were
higher scores in performance. We found that Perspicuity (R = 0.354,
p < .05) and Dependability (R = 0.436, p < .05) have statistically
signiﬁcant correlation on wpm. This implies that higher scores on
Perspicuity and Dependability were associated with higher typing
speed. Figure 4 depicts each UEQ subscale ratings for the three
techniques.

4.3.4 Simulator Sickness Questionnaire (SSQ)

Figure 5 shows the SSQ ratings for Oculomotor (O), Nausea (N),
Disorientation (D), and Total Severity (TS). We could not observe
any signiﬁcant effect of Technique on N (F2,35 = 0.187, p > .05),

O (F2,35 = 1.134, p > .05), D (F2,35 = 1.072, p > .05), TS (F2,35 =
0.978, p > .05).

5 DISCUSSION AND FUTURE WORK

Our results show that BlinkType is a viable solution for hands-free
text entry in VR. Users experience of BlinkType was rated Good to
Excellent based on the UEQ data where DwellType and NeckType
were only rated Below Average to Above Average (Figure 4). We
believe that BlinkType is faster as it is not constrained by any dwell
time to select or does not require neck movement in a forward-
backward direction, which sometimes is a little cumbersome to
perform. In addition, in short-term studies like this, it is often for
participants’ text entry rate not to have peaked. Our results show
this pattern also, where as can be seen in Figure 3a our participants’
rate has not peaked and may not likely to converge after six sessions.
Additionally, the trend shows a gap between BlinkText and the other
two, which could likely remain in place even if performance goes
up further. Future work based on a longitudinal study will be useful
to determine how long is required for users to reach peak entry
rates and whether the gap between BlinkText and other hands-free
techniques similar to DwellType and NeckType will remain in place.
Though BlinkType is a promising solution for hands-free text
entry, it has some limitations. For instance, BlinkType requires an
eye tracker, which may not be available in all current VR HMDs.
However, eye-tracking technology is becoming cheaper (e.g., IR
LED can be a cheaper alternative to detect blinks on HMDs) and
many VR manufacturers are now integrating eye-tracking solutions
with their VR/AR HMDs—examples include HTC VIVE Pro Eye,
Pico Neo 2 Eye, FOVE, HoloLens 2, Magic Leap 1, and LooxidVR.
As such, eye tracking will likely be a standard feature of these device,
and make BlinkType a very feasible technique.

For all techniques, the average rate shows an increasing trend
even in Session 6. This shows that after around 50 mins the fatigue
factor was not signiﬁcant. As stated earlier, our study did not look
at long-term usage (days or weeks) of hands-free techniques on
typing speed, which could be a future topic to explore. Related to
this, we suspect that high values of oculomotor could have resulted
from somewhat prolong use of HMDs and constantly searching for
target keys. Being immersed in a virtual environment and doing gaze
motions could cause oculomotor fatigue to some extent, especially
for some users not experienced with HMDs; for example, Iskander
et al. [11] indicate that motion is one cause of visual fatigue. In
future, we would like to investigate fatigue issue in a long-term
study. Furthermore, we chose 400ms of dwell time based on a prior
work [22] and was used by other researchers [39]. Exploring the
effect of different dwell time (i.e., higher or lower) can be another
possible avenue of the further work. In addition, data relevant to
InterKeystroke Interval (IKI) analysis could have been recorded in
future studies to provide further insights into the speciﬁc factors that
contribute to performance of hands-free techniques in VR.

DwellType is currently a popular choice for hands-free text entry
with VR HMDs. However, our participants expressed that they are
often pushed to keep moving the cursor over the keyboard to go
to the next character, which can be stressful and tiring. A similar
concern has also been reported in [29]. If users are not skilled at
typing and are not familiar with the English alphabet, they might stop
in a speciﬁc place for a while to avoid selecting unintended keys,
which could further lead to fatigue and lower performance [12].
On the other hand, NeckType is an inexpensive hands-free text
entry technique as it does not require any additional hardware for
implementation. Additionally, it has the same text entry performance,
UEQ, and SSQ as DwellType. Like BlinkType, with the increasing
trend of speed (53.78% improvement of text entry speed) as shown
in Figure 3a, we expect participants could achieve a faster speed in
NeckType than DwellType if further practice sessions are provided.
Nevertheless, one shortcoming of NeckType is that users seem to

need more training to be proﬁcient (e.g., 10 minutes).

Based on our study results, we can extrapolate the following three

recommendations for hands-free text entry in HMD VR:

R.1 BlinkType is the preferred text entry technique in VR if eye-

tracker is available.

R.2 DwellType and NeckType can be used for typing if eye-tracker

is not available.

R.3 Designers should consider using both eyes’ blinking for trig-
gering a selection as single-eye blink techniques lead to poor
performance due to low accuracy.

6 CONCLUSION

In this paper, we report our exploration and evaluation of hands-free
text entry techniques for virtual reality (VR) head-mounted displays
(HMD). We compared three hands-free text entry techniques: Blink-
Type, NeckType and DwellType. Results for a user study with 36
participants showed that BlinkType offered the best performance,
with participants being able to reach a mean typing speed of 13.47
wpm. With NeckType and DwellType, participants were able to
type at the rate of 11.18 WPM and 11.65 WPM, respectively. Based
on our study results, we suggest the use of BlinkType for hands-
free text entry in VR, if eye tracking is available. Additionally, we
found NeckType represents viable alternative approaches to dwell-
based techniques that can offer users more control over the pace of
selecting the characters and still lead to relatively fast entry.

ACKNOWLEDGMENTS

We want to thank our participants for their time and the reviewers for
their comments and suggestions that helped improve our paper. This
research was supported in part by AI University Research Centre
(AIURC) at Xi’an Jiaotong-Liverpool University (XJTLU), XJTLU
Key Program Special Fund (KSF-A-03 and KSF-02), and XJTLU
Research Development Fund.

REFERENCES

[1] B. Ashtiani and I. S. MacKenzie. Blinkwrite2: An improved text entry
method using eye blinks. In Proceedings of the 2010 Symposium on
Eye-Tracking Research & Applications, ETRA ’10, p. 339–345. ACM,
2010.

[2] L. F. Bacher and W. P. Smotherman. Spontaneous eye blinking in
human infants: A review. In Proceedings of the Developmental Psy-
chobiology: The Journal of the International Society for Developmental
Psychobiology, vol. 44, pp. 95–102. Wiley Online Library, 2004.
[3] D. J. Berndt and J. Clifford. Using dynamic time warping to ﬁnd pat-
terns in time series. In Proceedings of the 3rd International Conference
on Knowledge Discovery and Data Mining, AAAIWS’94, p. 359–370.
AAAI Press, 1994.

[4] T. J. Dube and A. S. Arif. Text entry in virtual reality: A comprehensive
review of the literature. In Proceedings of the International Conference
on Human-Computer Interaction, pp. 419–437. Springer, 2019.
[5] T. J. Dube and A. S. Arif. Text entry in virtual reality: A compre-
hensive review of the literature. In M. Kurosu, ed., Human-Computer
Interaction. Recognition and Interaction Technologies, pp. 419–437.
Springer International Publishing, Cham, 2019.

[6] J. Goodman, G. Venolia, K. Steury, and C. Parker. Language modeling
for soft keyboards. In Proceedings of the 7th International Conference
on Intelligent User Interfaces, IUI ’02, p. 194–195. ACM, 2002.
[7] J. Grubert, L. Witzani, E. Ofek, M. Pahud, M. Kranz, and P. O. Kris-
tensson. Text entry in immersive head-mounted display-based virtual
reality using standard keyboards. In Proceedings of the 2018 IEEE Con-
ference on Virtual Reality and 3D User Interfaces (VR), pp. 159–166.
IEEE, 2018.

[8] J. Gugenheimer, E. Stemasov, H. Sareen, and E. Rukzio. Facedis-
play: Enabling multi-user interaction for mobile virtual reality. In

Proceedings of the 2017 CHI Conference Extended Abstracts on Hu-
man Factors in Computing Systems, CHI EA ’17, pp. 369–372. ACM,
2017.

[9] D. Gurung, K. Gokul, and P. Adhikary. Mathematical model of thermal
effects of blinking in human eye. In Proceedings of the International
Journal of Biomathematics, vol. 9, p. 1650006. World Scientiﬁc, 2016.
[10] A. Hinderks, M. Schrepp, F. J. D. Mayo, M. J. E. Cuaresma, and
J. Thomaschewski. Ueq kpi value range based on the ueq benchmark.
2018.

[11] J. Iskander, M. Hossny, and S. Nahavandi. A review on ocular biome-
chanic models for assessing visual fatigue in virtual reality. In Proceed-
ings of the Ieee Access, vol. 6, pp. 19345–19361. IEEE, 2018.
[12] R. J. Jacob. The use of eye movements in human-computer interaction
techniques: what you look at is what you get. In Proceedings of the
ACM Transactions on Information Systems (TOIS), vol. 9, pp. 152–169.
ACM, 1991.

[13] R. S. Kennedy, N. E. Lane, K. S. Berbaum, and M. G. Lilienthal.
Simulator sickness questionnaire: An enhanced method for quantifying
simulator sickness. In Proceedings of the The International Journal of
Aviation Psychology, vol. 3, pp. 203–220. Taylor & Francis, 1993.
[14] D. J. Kim, D. I. Kim, S. H. Kim, K. Y. Lee, J. H. Heo, and S. W.
Han. Rescue localized intra-arterial thrombolysis for hyperacute mca
ischemic stroke patients after early non-responsive intravenous tissue
plasminogen activator therapy. In Proceedings of the Neuroradiology,
vol. 47, pp. 616–621. Springer, 2005.

[15] H. K. Kim, J. Park, Y. Choi, and M. Choe. Virtual reality sickness
questionnaire (vrsq): Motion sickness measurement index in a virtual
reality environment. In Proceedings of the Applied ergonomics, vol. 69,
pp. 66–73. Elsevier, 2018.

[16] P. O. Kristensson and K. Vertanen. The potential of dwell-free eye-
typing for fast assistive gaze communication. In Proceedings of the
Symposium on Eye Tracking Research and Applications, ETRA ’12, p.
241–244. ACM, 2012.

[17] A. Kurauchi, W. Feng, A. Joshi, C. Morimoto, and M. Betke. Eyeswipe:
Dwell-free text entry using gaze paths. In Proceedings of the 2016
CHI Conference on Human Factors in Computing Systems, CHI ’16, p.
1952–1956. ACM, 2016.

[18] H.-N. Liang, F. Lu, Y. Shi, V. Nanjappan, and K. Papangelis. Evaluating
the effects of collaboration and competition in navigation tasks and
spatial knowledge acquisition within virtual reality environments. In
Proceedings of the Future Generation Computer Systems, vol. 95, pp.
855–866. Elsevier, 2019.

[19] X. Lu, D. Yu, H.-N. Liang, X. Feng, and W. Xu. Depthtext: Leveraging
head movements towards the depth dimension for hands-free text entry
in mobile virtual reality systems. In Proceedings of the 2019 IEEE
Conference on Virtual Reality and 3D User Interfaces (VR), pp. 1060–
1061. IEEE, 2019.

[20] I. S. MacKenzie and B. Ashtiani. Blinkwrite: efﬁcient text entry using
eye blinks. In Proceedings of the Universal Access in the Information
Society, vol. 10, pp. 69–80. Springer, 2011.

[21] I. S. MacKenzie and R. W. Soukoreff. Phrase sets for evaluating text
entry techniques. In Proceedings of the CHI’03 Extended Abstracts on
Human Factors in Computing Systems, pp. 754–755, 2003.

[22] P. Majaranta, U.-K. Ahola, and O. ˇSpakov. Fast gaze typing with an
adjustable dwell time. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, CHI ’09, p. 357–360. ACM,
2009.

[23] P. Majaranta and K.-J. R¨aih¨a. Twenty years of eye typing: systems and
design issues. In Proceedings of the 2002 Symposium on Eye Tracking
Research & Applications, ETRA ’02, p. 15–22. ACM, 2002.

[24] D. Pedrosa, M. D. G. Pimentel, A. Wright, and K. N. Truong. Filteryed-
ping: Design challenges and user performance of dwell-free eye typing.
In Proceedings of the ACM Transactions on Accessible Computing
(TACCESS), vol. 6, pp. 1–37. ACM, 2015.

[25] S. Pick, A. S. Puika, and T. W. Kuhlen. Swifter: Design and eval-
uation of a speech-based text input metaphor for immersive virtual
environments. In Proceedings of the 2016 IEEE Symposium on 3D
User Interfaces (3DUI), pp. 109–112. IEEE, 2016.

[26] M. Porta and M. Turina. Eye-s: A full-screen input modality for pure
eye-based communication. In Proceedings of the 2008 Symposium on
Eye Tracking Research & Applications, ETRA ’08, p. 27–34. ACM,
2008.

[27] V. Rajanna and J. P. Hansen. Gaze typing in virtual reality: impact
of keyboard design, selection method, and motion. In Proceedings of
the 2018 ACM Symposium on Eye Tracking Research & Applications,
ETRA ’18. ACM, 2018.

[28] S. Ruan, J. O. Wobbrock, K. Liou, A. Ng, and J. A. Landay. Comparing
speech and keyboard text entry for short messages in two languages on
touchscreen phones. In Proceedings of the ACM on Interactive, Mobile,
Wearable and Ubiquitous Technologies, vol. 1, pp. 1–23. ACM, 2018.
[29] S. Sarcar, P. Panwar, and T. Chakraborty. Eyek: An efﬁcient dwell-free
eye gaze-based text entry system. In Proceedings of the 11th Asia
Paciﬁc Conference on Computer Human Interaction, APCHI ’13, p.
215–220. ACM, 2013.

[30] A. Somrak, I. Humar, M. S. Hossain, M. F. Alhamid, M. A. Hossain,
and J. Guna. Estimating vr sickness and user experience using different
hmd technologies: An evaluation study. In Proceedings of the Future
Generation Computer Systems, vol. 94, pp. 302–316. Elsevier, 2019.

[31] R. W. Soukoreff and I. S. MacKenzie. Metrics for text entry research:
an evaluation of msd and kspc, and a new uniﬁed error metric. In Pro-
ceedings of the SIGCHI Conference on Human Factors in Computing
Systems, CHI ’03, p. 113–120. ACM, 2003.

[32] M. Speicher, A. M. Feit, P. Ziegler, and A. Kr¨uger. Selection-based text
entry in virtual reality. In Proceedings of the 2018 CHI Conference on
Human Factors in Computing Systems, CHI ’18, p. 1–13. ACM, 2018.
[33] C.-H. Wang, Y.-C. Chiang, and M.-J. Wang. Evaluation of an aug-
mented reality embedded on-line shopping system. In Proceedings of
the Manufacturing, vol. 3, pp. 5624–5630. Elsevier, 2015.

[34] J. O. Wobbrock, J. Rubinstein, M. Sawyer, and A. T. Duchowski. Not
typing but writing: Eye-based text entry using letter-like gestures. In
Proceedings of the Conference on Communications by Gaze Interaction
(COGAIN), pp. 61–64, 2007.

[35] W. Xu, H.-N. Liang, A. He, and Z. Wang. Pointing and selection
methods for text entry in augmented reality head mounted displays. In
Proceedings of the 2019 IEEE International Symposium on Mixed and
Augmented Reality (ISMAR), pp. 279–288. IEEE, 2019.

[36] W. Xu, H.-N. Liang, Y. Zhao, D. Yu, and D. Monteiro. Dmove: Direc-
tional motion-based interaction for augmented reality head-mounted
In Proceedings of the 2019 CHI Conference on Human
displays.
Factors in Computing Systems, CHI ’19, pp. 1–14. ACM, 2019.
[37] W. Xu, H.-N. Liang, Y. Zhao, T. Zhang, D. Yu, and D. Monteiro. Ring-
text: Dwell-free and hands-free text entry for mobile head-mounted
displays using head motions. In Proceedings of the IEEE Transac-
tions on Visualization and Computer Graphics, vol. 25, pp. 1991–2001.
IEEE, 2019.

[38] Y. Yan, C. Yu, X. Yi, and Y. Shi. Headgesture: hands-free input
approach leveraging head movements for hmd devices. In Proceed-
ings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
Technologies, vol. 2, pp. 1–23. ACM, 2018.

[39] C. Yu, Y. Gu, Z. Yang, X. Yi, H. Luo, and Y. Shi. Tap, dwell or gesture?
exploring head-based text entry techniques for hmds. In Proceedings
of the 2017 CHI Conference on Human Factors in Computing Systems,
CHI ’17, pp. 4479–4488. ACM, 2017.

[40] D. Yu, K. Fan, H. Zhang, D. Monteiro, W. Xu, and H.-N. Liang. Pizza-
text: text entry for virtual reality systems using dual thumbsticks. In
Proceedings of the IEEE Transactions on Visualization and Computer
Graphics, vol. 24, pp. 2927–2935. IEEE, 2018.

[41] D. Yu, H.-N. Liang, X. Lu, K. Fan, and B. Ens. Modeling endpoint
distribution of pointing selection tasks in virtual reality environments.
In Proceedings of the ACM Transactions on Graphics (TOG), vol. 38,
pp. 1–13. ACM, 2019.

[42] D. Yu, H.-N. Liang, X. Lu, T. Zhang, and W. Xu. Depthmove: Leverag-
ing head motions in the depth dimension to interact with virtual reality
head-worn displays. In Proceedings of the 2019 IEEE International
Symposium on Mixed and Augmented Reality (ISMAR), pp. 103–114.
IEEE, 2019.

