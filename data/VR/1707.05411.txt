1                                                                                                                                                                                 

Hybrid PS-V Technique: A Novel Sensor Fusion 
Approach for Fast Mobile Eye-Tracking with 
Sensor-Shift Aware Correction 

Ioannis Rigas, Hayes Raffle, and Oleg V. Komogortsev 

ï€  

Abstractâ€”This  paper  introduces  and  evaluates  a  hybrid 
technique  that  fuses  efficiently  the  eye-tracking  principles  of 
photosensor oculography (PSOG) and video oculography (VOG). 
The main concept of this novel approach is to use a few fast and 
power-economic  photosensors  as  the  core  mechanism  for 
performing  high  speed  eye-tracking,  whereas  in  parallel,  use  a 
video  sensor  operating  at  low  sampling-rate  (snapshot  mode)  to 
perform  dead-reckoning 
sensor 
error 
movements occur. In order to evaluate the proposed method, we 
simulate the functional components of the technique and present 
our  results 
involving  various 
combinations  of  horizontal  and  vertical  eye  and  sensor 
movements.  Our  evaluation  shows  that  the  developed  technique 
can be used to provide robustness to sensor shifts that otherwise 
could induce error larger than 5Â°. Our analysis suggests that the 
technique  can  potentially  enable  high  speed  eye-tracking  at  low 
power  profiles,  making  it  suitable  to  be  used  in  emerging  head-
mounted devices, e.g. AR/VR headsets.  

correction  when 

in  experimental 

scenarios 

Index Termsâ€” hybrid eye-tracking, photosensor oculography, 

sensor fusion, sensor shift correction, video oculography 

I.  INTRODUCTION 

E

substantially 

ye-tracking  is  expected  to  become  an  essential  tool  for 
seamless  human-computer  interaction  (HCI)  in  modern 
head-mounted  devices.  For  example,  in  the  case  of  AR/VR 
headsets,  eye-tracking  can 
the 
immersion  and  the  overall  user  experience  by  enabling 
applications  like  foveated  rendering  [1],  saccade-contingent 
screen updating [2], touchless interaction [3], and assist on the 
prevention of eye fatigue [4] and cybersickness [5]. In order to 
meet the demands of the growing mobile AR/VR ecosystems, 
two  very  important  requirements  for  eye-tracking  systems 
aiming to enable such applications are high tracking speed and 
relatively low power consumption. 

improve 

I.  Rigas  and  O.V.  Komogortsev  are  with  Texas  State  University, 
Department of Computer Science, 601 University Dr, San Marcos, TX 78666 
USA, (e-mails: rigas.ioann@gmail.com; ok@txstate.edu). 

H. Raffle  is with Google, 1600 Amphitheater Drive, Mountain View, CA 

94043, USA, (e-mail: hraffle@google.com). 

Most  current  eye-tracking  systems  are  based  on  the 
principle  of  video  oculography  (VOG).  In  a  typical  VOG 
implementation  [6],  the  eye  is  illuminated  by  one  (or  more) 
infrared  LED(s),  and  consecutive  images  of  the  eye  are 
captured and processed to extract important features, e.g. pupil 
center  and  corneal  reflection.  The  differences  in  position  of 
these  features  can  be  used  to  estimate  eye  movement  with 
relative  robustness  to  small  sensor  movements.  The  systems 
based  on  VOG  can  provide  high  accuracy  during  gaze 
estimation  but  they  have  certain  limitations  when  high  speed 
eye-tracking  is  needed  combined  to  low  power  consumption. 
These  limitations  arise  from  the  need  to  capture  and  process 
multiple  images,  a  procedure  with  considerable  burden  for 
computational  resources.  For  binocular  eye-tracking  these 
demands and the overall cost become further inflated. 

A  number  of  alternative  eye-tracking  methods  have  been 
explored  in  the  past,  with  the  most  prominent  being:  a)  the 
magnetic  scleral  coil  method  [7],  b)  electrooculography 
(EOG)  [8],  and  c)  photosensor  oculography  (PSOG)  [9]. 
Among  them,  PSOG  appears  to  fulfill  many  of  the  eye-
tracking  needs  posed  by  modern  headsets.  The  principle  of 
PSOG  is  based  on  the  direct  measurement  of  the  amount  of 
reflected  light  from  the  eye  using  simple  pairs  of  photo-
sensitive  sensors.  A  major  advantage  of  PSOG  when 
compared to VOG is the minimal computational burden (just a 
few computations  to combine sensor  outputs) that can  enable 
eye-tracking  with  high  sampling  rate  and 
low  power 
consumption.  Also,  the  PSOG  does  not  need  any  attachment 
on the eye or skin making it  less obtrusive than the  magnetic 
scleral  coil  method  and  EOG.  Despite 
these  obvious 
advantages,  PSOG  has  also  its  Achillesâ€™  heel:  it  is  very 
sensitive  to  sensor  shifts.  Most  headsets  use  head-straps  to 
limit  excessive  mobility,  however,  small  sensor  movements 
can  still  occur  due  to  facial  expressions  or  body  movements 
(e.g. during jumping, walking). Such sensor shifts can result in 
considerable  degradation  of  accuracy  for  the  traditional 
implementations of PSOG. 

In this work, we propose a new approach for addressing the 
limitations  of  traditional  photosensor  oculography  and  video 
oculography  systems  by  selectively  combining  the  best 
characteristics from both worlds. The key contributions of this 
work are: 

  
 
 
 
 
 
 
 
 
 
 
 
1) We  introduce  the  hybrid  PS-V  technique,  a  new  eye-
tracking concept based on the fusion of photosensor and video 
the 
oculography  principles.  We  present 
technique and simulate its functional components. 

the  details  of 

2) We  perform  an  evaluation  of  the  technique  using  the 
developed simulated framework. We feed the models with real 
eye  movements  and  explore  the  baseline  potential  of  the 
technique and the achieved robustness to sensor shifts. 

II.  BACKGROUND 

technique, 

such  as:  photoelectric 

Eye-tracking  techniques  based  on  the  direct  measurement  of 
the  amount  of  reflected  light  from  the  eye  have  been 
investigated  since  the  early  1950â€™s  [10].  Most  of  these 
techniques use invisible infrared light and rely on the existing 
differences in the reflectance properties of different regions of 
the  eye  (sclera,  iris,  and  pupil).  When  the  eye  moves,  the 
transitions between these regions can be tracked using simple 
pairs of photosensors positioned in close proximity to the eye. 
The  term  photosensor  oculography  (PSOG)  can  be  used  to 
collectively  refer  to  the  techniques  based  on  this  principle  of 
operation  but  other  alternative  terms  have  been  also  used  in 
the  past, 
infrared 
oculography, and limbus reflection method. Most of the PSOG 
techniques  are  based  on  the  differential  operation  principle, 
i.e. they calculate relative differences between sensor pairs. In 
order  to  avoid  ambient  light  interferences,  systems  based  on 
PSOG can use modulated (chopped) light, as proposed in [11]. 
techniques  allow  measurement  of  eye-ball 
The  PSOG 
rotations  with  very  good  precision  (a  few  arc  min),  and 
additionally, the fast switching times of the sensors (usually at 
the  order  of  ns)  and  the  minimal  computational  complexity 
can  enable  tracking  with  high  speed.  However,  in  order  to 
provide  acceptable  eye-tracking  accuracy,  a  system  based  on 
PSOG needs to be firmly affixed to the head because even the 
slightest  sensor  shifts  can  induce  large  errors  during  gaze 
estimation.  For  example,  sensor  movements  larger  than  0.5 
mm  can  result  in  accuracy  error  larger  than  1Â°.  A  compact 
overview  of  the  characteristics  of  various  PSOG  systems  can 
be found in [9]. It should be noticed that even though several 
PSOG variations have been developed in order to advance the 
characteristics  of  this  technology  in  terms  of  linearity, 
crosstalk,  sensor  placement,  and  tracking  range  [12-15],  the 
lack  of 
the 
widespread adoption of PSOG. On the other hand, the breadth 
of technical advancement in recent years has focused on VOG 
to 
techniques  where  algorithms  have  been  developed 
accommodate  for  such  sensor  shifts,  giving  the  technology 
robustness in real-world conditions. 

robustness  during  sensor  shifts  hindered 

III.  THE HYBRID PS-V TECHNIQUE 

A.  General Overview 
The main goal of the hybrid PS-V technique is to address the 
sensor-shift related issues of PSOG while keeping the inherent 
advantages of this technology, such as the high speed and low 
power  consumption.  The  developed  approach  to  achieve  this 
goal is based on the combination of information coming from 
two subsystems, a subsystem based on PSOG and a subsystem 
based  VOG.  The  PSOG  subsystem  is  used  to  track  eye 

2 

rotations at high sampling rate (1000 Hz or more). The VOG 
subsystem  is  used  as  the  means  for  estimating  sensor  shifts 
and thus it can operate  at  a much  lower  sampling  rate  (e.g. 5 
Hz  or  less).  A  basic  assumption  of  the  technique  is  the 
existence  of  a  rigid  connection  between  the  PSOG  and  VOG 
subsystems,  so  that  the  sensor  movement  estimated  by  the 
VOG subsystem can be used to rectify the movement-induced 
artifacts appearing  in PSOG  subsystem.  The  rigid connection 
requirement  is  by-design  fulfilled  when  the  two  subsystems 
are  embedded  in  a  headset  setup.  In  Fig.  1  we  present  a 
summarizing  overview  of  the  functional  components  of  the 
hybrid PS-V technique. 

Fig. 1. Overview of the proposed hybrid PS-V technique.  

The  description  and  evaluation  of 

the  hybrid  PS-V 
technique  is  performed  using  a  semi-simulated  framework. 
We  use  model-driven  simulation  to  represent  the  functional 
components  of the  technique,  and  then, during  the evaluation 
phase we use signals from real eye movements as input to the 
simulated models. While the use of real system components is 
essential  for  validating  the  final  robustness  of  the  technique, 
the  initial  evaluation  of  the  proposed  novel  scheme  via 
simulation  can  greatly  facilitate  the  exploration  of  some  very 
crucial  aspects,  such  as:  a)  during  development,  it  allows  for 
the in-depth investigation and modeling of PSOG subsystemâ€™s 
behavior  in  the  case  of  sensor  movements,  and  b)  during 
evaluation,  it  provides  better  control  of  the  performed  sensor 
movements  (exact  magnitudes  and  directions)  and  thus 
enables  the  assessment  of  sensor-shift  robustness  against  a 
well-defined ground truth. 

B.  Generation of 3-D Rendered Synthetic Eye Images 
The  first  step  in  the  developed  simulation  framework  is  to 
generate  synthetic  eye  images  using  the  simulation  software 
introduced by Swirski and Dodgson in [16]. This software was 
built  using  Python  and  the  3-D  graphics  software  Blender 
[17],  and  it  includes  realistic  models  of  the  human  eye 
(supporting movements for eye, eyelid and pupil), the camera 
module, and the light sources. The 3-D scene elements can be 
algorithmically  positioned  and  rotated  (3-DoF)  in  order  to 
simulate different eye-tracking  scenarios. In  our case,  we use 
this software to generate the frames that are then used during 
the  simulation  of  PSOG  and  VOG  subsystems.  To  generate 
the frames used during PSOG subsystemâ€™s simulation we use 
two point-light sources (simulating the IR emitters) positioned 

   
 
  
 
 
 
 
 
 
 
3                                                                                                                                                                                

Â±1.4  cm  horizontally,  1  cm  under  and  3  cm  away  from  the 
pupil center (all distances are measured with respect to the eye 
in  neutral  position  and  refer  to  the  left  eye).  The  camera 
module  is  centered  (horizontally  and  vertically)  and  5  cm 
away from the pupil center with the  field of view set to  fully 
cover  the  eye  area  (in  our  case  45Â°).  To  generate  the  frames 
used  during  VOG  subsystemâ€™s  simulation  we  use  exactly  the 
same  positioning  for  the  point-light  sources  to  simulate  the 
simultaneous  operation  of  the  two  subsystems  (PSOG  and 
VOG). This time, though, the camera  module is placed 1  cm 
under  and  5  cm  away  from  the  pupil  center.  In  all  cases,  the 
resolution of the rendered frames is set to be 240 x 320 pixels. 
To  rotate  the  simulated  3-D  eye  model,  we  either  send  pre-
defined values of specific eye positions (during calibration) or 
send  values  recorded  from  real  eye  movements  (during 
evaluation) using a high-grade eye-tracker as ground truth. To 
simulate sensor movements, we use pre-defined (ground truth) 
values to translate the camera module and the lights together, 
thus conforming with the rigid connection assumption. 

C.  PSOG Subsystem 
An  infrared  detector  can  be  modeled  as  a  controlled  current 
source connected in parallel to an exponential diode [18]. The 
currents of the controlled source ğ¼(cid:3043) and the exponential diode 
ğ¼(cid:3031) can be modeled using Eq. 1-2: 
ğ¼(cid:3043) = ğ‘…(cid:3090) âˆ™ ğ‘ƒ   

(1) 

ğ¼(cid:3031) = ğ¼(cid:3046) âˆ™ (cid:4678)ğ‘’

(cid:3292)âˆ™(cid:3271)(cid:3250)
(cid:3286)(cid:3251)âˆ™(cid:3269) âˆ’ 1(cid:4679)   

(2) 

detection  quadrants.  The  diagram  of  the  developed  PSOG 
design and the respective detection areas are shown in Fig. 3. 
the  horizontal/vertical  components  of  eye 
To  calculate 
(cid:3009),(cid:3023) (cid:3439)  with  the  developed  design,  we  need  to 
movement  (cid:3435)ğ¼(cid:3017)(cid:3020)(cid:3016)(cid:3008)
perform the low-complexity operations described in Eq. 4-5: 

(cid:3009)
ğ¼(cid:3017)(cid:3020)(cid:3016)(cid:3008)
(cid:3023)
ğ¼(cid:3017)(cid:3020)(cid:3016)(cid:3008)

= (cid:3435)ğ¼(cid:3017)(cid:3005)(cid:3117) + ğ¼(cid:3017)(cid:3005)(cid:3120)(cid:3439) âˆ’ (cid:3435)ğ¼(cid:3017)(cid:3005)(cid:3118) + ğ¼(cid:3017)(cid:3005)(cid:3119)(cid:3439)   
= (cid:3435)ğ¼(cid:3017)(cid:3005)(cid:3117) + ğ¼(cid:3017)(cid:3005)(cid:3118)(cid:3439) âˆ’ (cid:3435)ğ¼(cid:3017)(cid:3005)(cid:3119) + ğ¼(cid:3017)(cid:3005)(cid:3120)(cid:3439)   

(4) 
(5) 

The developed PSOG design can be relatively flexible during 
a  practical  implementation  since  it  can  be  alternatively 
realized using four narrow half-angle emitters paired with four 
photodiodes  of  wider  half-angle.  Also,  to  avoid  occlusion  of 
visibility  the  sensors  can  be  placed  more  distant  and  slightly 
angled to point at the eye target areas. 

Fig.  2.  Graphical  presentation  of  the  steps  for  the  simulation  of  a  single 
photodiodeâ€™s output. 

where ğ‘…(cid:3090) is the responsivity at wavelength Î», ğ‘ƒ is the incident 
light  power,  ğ¼(cid:3046)  is  the  reverse  saturation  current,  ğ‘  is  the 
electron  charge,  ğ‘‰(cid:3002)  is  the  applied  bias  voltage,  ğ‘˜(cid:3003) = 1.38 âˆ™
10(cid:2879)(cid:2870) ğ½ ğ¾â„   is  the  Boltzmann  constant,  and  T  is  the  absolute 
temperature.  Furthermore,  when  operating  in  photovoltaic 
mode the photodiode is zero-biased (V = 0), and since ğ¼(cid:3031) â†’ 0 
the  output  of  the  sensor  is  analogous  to  the  incident  light 
power ğ‘ƒ (ğ‘…(cid:3090) can be considered constant for given conditions). 
In  order  to  simulate  the  incident  light  power  on  the  sensor 
after  the  light  is  reflected  by  the  eye,  we  use  a  Gaussian 
modulated  window  binning  operation  applied  on  the  pixel 
intensity  values  of  selected  areas  of  the  3-D  rendered  eye 
frames. The window ğ‘Š(cid:3036),(cid:3037) is selected to be 13Â° x 13Â° and it is 
multiplied  with  a  Gaussian  kernel  ğº(cid:3036),(cid:3037)  of  Ïƒ  =  Â¼  the  window 
side.  This  operation  results  in  the  simulation  of  the  receptive 
area of a photodiode with a half reception angle of about Â±8Â°. 
The  output  of  this  photodiode  is  calculated  by  averaging  the 
Gaussian-modulated  pixel  intensity  values  within  the  defined 
window, as shown in Eq. 3: 
âˆ‘ (cid:3008)(cid:3284),(cid:3285)âˆ™(cid:3024)(cid:3284),(cid:3285)
(cid:3036)âˆ™(cid:3037)

, ğ‘–, ğ‘— = ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ ğ‘ğ‘œğ‘œğ‘Ÿğ‘‘.  

ğ¼(cid:3017)(cid:3005) =

(3) 

(cid:3284),(cid:3285)

In  Fig.  2,  we  show  a  graphical  presentation  of  the  steps 
followed  during  the  simulation  of  a  single  photodiodeâ€™s 
output, using an example frame from our experiments. 

A  PSOG  setup  usually  contains  two  or  more  photodiodes 
positioned to capture the light reflected from different regions 
of the eye. Combining clues from previously proposed PSOG 
design  principles  [9,  12]  we  develop  a  setup  based  on  two 
wide-angle emitters (simulated by the point-light sources) and 
four  photodiodes  (each  simulated  as  described  previously) 
positioned  to  split  the  eye  region  into  four  semi-overlapping 

Fig. 3. The used PSOG design and the respective simulated detection areas. 

D.  VOG Subsystem 
The  simulation  of  the  output  from  the  video  sensor  is  more 
straightforward  since  the  employed  3-D  rendering  software 
already provides a fully  functional model of a camera sensor. 
Hence,  the  rendered  frames  from  the  VOG  setup  simulation 
can  be  used  directly  to  represent  the  output  of  the  video 
sensor. During our experiments the output of the video sensor 
is  sampled  at  low  rate  (5  Hz)  to  accurately  represent  the 
required  specifications  for  camera  operation.  The  VOG 
subsystem  further  processes  the  frames  in  order  to  extract 
features and estimate the sensor movement that will be used to 
perform the correction of PSOG subsystemâ€™s output. 

the  VOG  subsystem 

The  methodology  used  for 

the  estimation  of  sensor 
movements  via 
the 
quantification  the  differences  in  relative  movements  of  the 
pupil  center  (attached  to  the  eyeball)  and  the  corneal 
reflection,  when  eye  and  sensor  movements  occur.  The  basic 
principles  of  this  methodology  were  investigated  in  [19]  for 
the  task  of  sensor  movement  compensation  when  performing 
eye-tracking using a pure VOG system. In our case, the VOG 

is  based  on 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
is  used  only  as 

subsystem 
the  mechanism  for  sensor 
movement  estimation,  and  not  for  performing  the  full  eye-
tracking  queue.  For  this  reason,  we  focus  only  on  the  part 
related  to  the  calculation  of  the  camera  (sensor)  movement 
vector. Let us assume that we have available at each time the 
tracked  positions  of  the  pupil  center  ğ‘ƒğ¶(cid:3047)(cid:3045)  and  corneal 
linear 
reflection  ğ¶ğ‘…(cid:3047)(cid:3045).  If  we  accept  an  approximately 
relationship  for  the  relative  movements  of  pupil  center  and 
corneal  reflection  when  eye  and  sensor  movements  occur, 
then, we can use the formulas of Eq. 6-7 to separate the part of 
the  apparent  pupil  movement  that  is  generated  exclusively 
(cid:3009),(cid:3023)).  For  horizontal  and  vertical 
from  sensor  movements  (ğ‘ƒğ¶(cid:3046)
sensor movements we can use the parameters of the simulated 
VOG  setup  to  convert  this  movement  from  pixel-space  to 
(cid:3009),(cid:3023) . 
millimeters of estimated sensor movement ğ‘¥(cid:3023)(cid:3016)(cid:3008)

(6) 

ğ‘ƒğ¶(cid:3046)

(cid:3009),(cid:3023) =

(cid:3004)(cid:3019)(cid:3295)(cid:3293)

(cid:3257),(cid:3271)

(cid:3257),(cid:3271)âˆ™(cid:3008)(cid:3280)
(cid:3257),(cid:3271)

(cid:3257),(cid:3271)(cid:2879)(cid:3017) (cid:3295)(cid:3293)
(cid:3257),(cid:3271)(cid:2879)(cid:3008)(cid:3280)
(cid:3008)(cid:3294)
(cid:3257),(cid:3271)
(cid:3257),(cid:3271), ğº(cid:3046)

(7) 

(cid:3009),(cid:3023) =

(cid:3057)(cid:3004) (cid:3294)
(cid:3057)(cid:3017)(cid:3004)(cid:3294)

(cid:3009),(cid:3023) =
ğº(cid:3032)
(cid:3009),(cid:3023) and ğº(cid:3046)

(cid:3257),(cid:3271)
(cid:3057)(cid:3004)(cid:3019)(cid:3280)
(cid:3257),(cid:3271)  
(cid:3057)(cid:3017)(cid:3004)(cid:3280)
(cid:3009),(cid:3023) represent the average eye and sensor 
The terms ğº(cid:3032)
movement  gains,  i.e.  the  fraction  of  the  respective  corneal 
reflection movement per unit of pupil center movement when 
each  type  of  movement  (eye  or  sensor)  is  performed 
(cid:3009),(cid:3023) we simulated 
separately. To find the values of ğº(cid:3032)
eye  and  sensor  movements  and  calculated  the  average  values 
of 0.38/0.39 and 0.83/0.81 respectively. 

(cid:3009),(cid:3023) and ğº(cid:3046)

As we mentioned, to use Eq. 6-7 we need to have available 
at  each  time  the  tracked  positions  of  the  pupil  center  and 
corneal  reflection.  To  track  these  positions  we  fed  the  3-D 
rendered  eye  frames  to  the  open-source  tracking  software 
Haytham  [20].  The  software  allows  to  parameterize  the 
thresholds  for  detecting  pupil  center  and  the  closest  (to  it) 
corneal  reflection.  In  our  case  (two  light  sources),  this 
interchangeable  detection  of  the  closest  corneal  reflection 
makes possible the coverage of larger range of movements. It 
should  be  emphasized  that  the  ambiguity  regarding  which 
corneal reflection is captured every time does not affect sensor 
movement  estimation  since  Eq.  6-7  make  use  of  relative 
differences  expressed  with  reference  to  the  primary  eye 
position. 

in  relation 

to  sensor  movement.  After 

E.  Calibration Subsystem 
The  calibration  subsystem  provides  the  composite  mapping 
function  for  performing  the  tasks  of:  a)  eye  movement 
calibration,  i.e.  the  transformation  of  the  PSOG  subsystemâ€™s 
output from raw units to degrees of visual angle, and b) sensor 
movement  calibration,  i.e.  the  transformation  of  calibration 
parameters 
the 
calibration  parameters  are  computed,  they  can  be  stored  so 
that  the  correction  subsystem  can  use  them  routinely  to 
combine  the  outputs  from  PSOG  and  VOG  subsystems,  and 
generate  the  corrected  output  signal.  To  develop  a  powerful 
model  for  the  calibration  function,  we  first  investigate  the 
behavior  of  the  raw  output  of  the  PSOG  subsystem.  The 
developed simulation framework greatly facilitates such a task 
because  it  allows  to  perform  a  controlled  dense  scan  of  eye 
and  sensor  positions  and  observe  the  general  behavior  of  the 
output.  In  our  case,  the  scan  covers  eye  positions  in  range 
Â±10Â°  (horizontal/vertical)  with  step  of  0.5Â°,  and  sensor 

4 

positions in range Â±2 mm (horizontal/vertical) with step of 0.5 
mm. In Fig. 4,  we show the respective clusters of curves that 
represent  the  general  behavior  of  PSOG  subsystemâ€™s  output 
when  sensor  movements  occur.  We  can  observe  that  for  the 
used  PSOG  design  the  sensor  movements  mainly  affect  the 
capturing  of  eye  movements  of  the  same  direction.  For 
example,  horizontal  sensor  movements  induce  a  significant 
translation  of  the  horizontal  output  curves,  whereas  the 
vertical output curves are only slightly affected. An analogous 
effect  can  be  observed  for  the  case  of  vertical  sensor 
movements. Also, we can see that the linearity is very good at 
the primary sensor position (0 mm) but gradually deteriorates 
as we move the sensor. 

Fig.  4.  Behavior  of  PSOG  subsystemâ€™s  output  for  different  combinations  of 
horizontal and vertical eye/sensor movements.  

Based  on 

the  observed  behavior  we  developed 

the 
calibration model of quadratic mapping functions described in 
(cid:3009),(cid:3023) is used to provide the 
Eq. 8-11. The calibration function ğ‘“(cid:3004)
(cid:3009),(cid:3023) and the raw 
mapping between pre-defined  eye positions  ğ‘¥(cid:3032)
(cid:3009),(cid:3023) ).  The 
eye-tracking  output  from  the  PSOG  subsystem  (ğ¼(cid:3017)(cid:3020)(cid:3016)(cid:3008)
mapping is done via the top-level parameters ğ‘(cid:3009),(cid:3023), ğ‘(cid:3009),(cid:3023), ğ‘(cid:3009),(cid:3023), 
which  in  turn  are  mapped  at  a  lower-level  as  functions  of 
sensor  position.  The  training  of  calibration  function  is 
performed using data captured at a number of predefined eye 
(cid:3009),(cid:3023)  (minimum  3/direction)  and  sensor  positions 
positions  ğ‘¥(cid:3032)
(cid:3009),(cid:3023)  (minimum  3/direction).  Data  fitting  is  done  using  Least 
ğ‘¥(cid:3046)
Squares (LS) regression. 
(cid:3009),(cid:3023)(cid:3435)ğ‘¥(cid:3032)
ğ‘“(cid:3004)
ğ‘(cid:3009),(cid:3023)(cid:3435)ğ‘¥(cid:3046)
ğ‘(cid:3009),(cid:3023)(cid:3435)ğ‘¥(cid:3046)
ğ‘(cid:3009),(cid:3023)(cid:3435)ğ‘¥(cid:3046)
ğ‘(cid:3009),(cid:3023)(cid:3435)ğ‘¥(cid:3046)

(cid:3009),(cid:3023), ğ‘¥(cid:3046)
(cid:3009),(cid:3023)(cid:3439)   
(cid:3009),(cid:3023)(cid:3439) = ğ‘(cid:2869)
(cid:3009),(cid:3023)(cid:3439) = ğ‘(cid:2869)
(cid:3009),(cid:3023)(cid:3439) = ğ‘(cid:2869)

(11) 
In  Fig.  5,  we  show  the  fitted  top-level  (Eq.  8)  calibration 
curves when using eye positions -10Â°, 0Â° and +10Â° and sensor 
positions  -2,  0,  +2  mm.  The  calibration  curves  are  shown 
superimposed on the simulated eye-tracking curves previously 
shown  in  Fig.  4  (we  focus  on  the  challenging  cases  of 
eye/sensor  movements  of  same  direction).  It  can  be  verified 
that 
three-point  fitted  curves  can  model  relatively 
accurately the eye-tracking behavior of the PSOG subsystem. 

(cid:3009),(cid:3023) + ğ‘(cid:2871)
(cid:3009),(cid:3023) + ğ‘(cid:2871)
(cid:3009),(cid:3023) + ğ‘(cid:2871)

(cid:3009),(cid:3023) âˆ™ ğ‘¥(cid:3046)
(cid:3009),(cid:3023) âˆ™ ğ‘¥(cid:3046)
(cid:3009),(cid:3023) âˆ™ ğ‘¥(cid:3046)

(cid:3009),(cid:3023) âˆ™ ğ‘¥(cid:3046)
(cid:3009),(cid:3023) âˆ™ ğ‘¥(cid:3046)
(cid:3009),(cid:3023) âˆ™ ğ‘¥(cid:3046)

+ ğ‘(cid:2870)
+ ğ‘(cid:2870)
+ ğ‘(cid:2870)

(cid:3009),(cid:3023) (cid:3439) = ğ‘(cid:3009),(cid:3023)(cid:3435)ğ‘¥(cid:3046)

(cid:3009),(cid:3023) 
(cid:3009),(cid:3023)  
(cid:3009),(cid:3023)   

(cid:3009),(cid:3023) +
(8) 

+ ğ‘(cid:3009),(cid:3023)(cid:3435)ğ‘¥(cid:3046)

(cid:3009),(cid:3023)(cid:3439) âˆ™ ğ‘¥(cid:3032)

(cid:3009),(cid:3023)(cid:3439) âˆ™ ğ‘¥(cid:3032)

(10) 

(cid:3009),(cid:3023) (cid:2870)

(cid:3009),(cid:3023)(cid:2870)

(cid:3009),(cid:3023) (cid:2870)

the 

(cid:3009),(cid:3023)(cid:2870)

(9) 

   
 
  
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5                                                                                                                                                                                

To provide a more detailed view of the composite calibration 
procedure,  in  Fig.  6  we  present  the  respective  low-level  (Eq. 
9-11) fitted curves that model parameters  ğ‘(cid:3009),(cid:3023), ğ‘(cid:3009),(cid:3023), ğ‘(cid:3009),(cid:3023) as a 
function  of  sensor  position.  The  overall  behavior  of  the 
calibration  parameters  can  further  justify  the  universal  use  of 
quadratic functions in the developed formulation. 

Fig. 5. Curves fitted via the calibration function ğ‘“(cid:3004)
sensor movements of same direction.  

(cid:3009),(cid:3023) for the case of eye and 

is  performed  by  applying 

data stream and the low sampling rate VOG data stream. This 
is done by applying zero-order hold filtering on the incoming 
VOG samples, as presented in Eq. 13 (ğ‘“(cid:3010)(cid:3019)(cid:3016)(cid:3008) = 1000 Hz, ğ‘“(cid:3023)(cid:3016)(cid:3008) 
= 5 Hz). After these initial pre-processing steps the correction 
subsystem is ready to use the sensor movement information in 
order  to  select  the  calibration  parameters  and  perform  the 
fusion of data streams. The calculation of the final sensor-shift 
corrected  signal 
inverse 
calibration  function  to  combine  the  pre-processed  samples 
from PSOG and VOG subsystems, as shown in Eq. 14. 
(cid:3009),(cid:3023) (cid:4593)
(cid:3009),(cid:3023) (ğ‘– âˆ’ ğ‘—)
(cid:3041)(cid:2879)(cid:2869)
, ğ‘– = 0, 1, 2, â€¦ , ğ‘(cid:3017)(cid:3020)(cid:3016)(cid:3008)  
ğ¼(cid:3017)(cid:3020)(cid:3016)(cid:3008)
ğ¼(cid:3017)(cid:3020)(cid:3016)(cid:3008)
(cid:3037)(cid:2880)(cid:2868)
(cid:3041)
(cid:3009),(cid:3023) (cid:4593)
(cid:3009),(cid:3023) (ğ‘˜), ğœ = 0, 1, â€¦ ,
(ğ‘˜ + ğœ) = ğ‘¥(cid:3023)(cid:3016)
ğ‘¥(cid:3023)(cid:3016)(cid:3008)
(13) 
0, 1, 2, â€¦ , ğ‘(cid:3023)(cid:3016)(cid:3008) 
(cid:3009),(cid:3023) = ğ‘“(cid:3004)
(14) 
ğ‘¦(cid:3017)(cid:3020)(cid:3023)
The selection of the correct root (from the two) of the inverse 
quadratic function can be performed by considering the exact 
domain and range of the original function. 

(cid:3009),(cid:3023) (cid:4593)
(cid:2879)(cid:2869) (cid:3009),(cid:3023) (cid:4672)ğ¼(cid:3017)(cid:3020)(cid:3016)(cid:3008)

(cid:3009),(cid:3023) (cid:4593)
, ğ‘¥(cid:3023)(cid:3016)(cid:3008)

(cid:3033)(cid:3265)(cid:3268)(cid:3264)(cid:3256)
(cid:3033)(cid:3271)(cid:3264)(cid:3256)

âˆ’ 1, ğ‘˜ =

(ğ‘–) =

(12) 

the 

âˆ™ âˆ‘

(cid:4673) 

(cid:2869)

IV.  EVALUATION RESULTS 

towards 

A.  Experiments  
1)  Sign conventions for eye and sensor movements 
Throughout the paper we use the following sign conventions: 
horizontal  eye  movements  are  positive  when  the  (left)  eye 
moves  towards  the  nasal  area,  and  negative  when  it  moves 
away from the nasal area. Vertical eye movements are positive 
when the eye moves downwards, and negative when it moves 
upwards. Horizontal sensor movements are positive  when the 
sensor moves away from the nasal area, and negative when the 
sensor  moves 
the  nasal  area.  Vertical  sensor 
movements are positive when the sensor moves upwards, and 
negative when it moves downwards. 
2)  Eye-tracking scenarios 
The experiments for the evaluation of the proposed technique 
are  performed  using  two  eye-tracking  scenarios.  In  both 
scenarios,  real  eye-tracking  signals  are  used  as  input  to  the 
simulation framework. The signals were captured from human 
subjects  using  an  EyeLink  1000  eye-tracker  [21]  (vendor 
reported  accuracy  0.5Â°)  at  a  sampling  rate  of  1000  Hz 
(monocular  setup,  left  eye  was  captured).  Subjects  were 
positioned  at  a  distance  of  550  mm  from  a  computer  screen 
(size  297  x  484  mm,  resolution  1050  x  1680  pixels)  where 
visual  stimuli  were  presented.  The  subjectsâ€™  heads  were 
restrained using a head-bar with a forehead. 

The stimulus  of the first  eye-tracking  scenario (HV)  was a 
â€˜jumpingâ€™  point  (horizontal/vertical  â€˜jumpsâ€™)  changing  its 
position  every  1  second  (total  duration  36  seconds).  The 
amplitude  of  the  â€˜jumpsâ€™  increased  from  Â±2.5Â°  to  Â±10Â°,  with 
step of 2.5Â°. This stimulus-type induces horizontal and vertical 
saccades of respective amplitudes as the eye moves from one 
point  of  fixation  to  another,  and  allows  for  the  controlled 
investigation  of  the  eye-tracking  behavior  using  various 
eye/sensor  movement  combinations,  e.g.  horizontal  sensor 
movements when vertical eye movements occur etc. In Fig. 7 
(top),  we  present  the  exact  ground  truth  signals  for  the  first 
eye-tracking scenario (HV). The second eye-tracking scenario 
(TX)  used  a  text  stimulus,  in  specific,  a  few  lines  from  the 
poem of Lewis Carroll â€œThe Hunting of the Snarkâ€. We kept a 

Fig.  6.  Curves  fitted  for  parameters  ğ‘(cid:3009),(cid:3023), ğ‘(cid:3009),(cid:3023), ğ‘(cid:3009),(cid:3023)  for  the  case  of  eye  and 
sensor movements of same direction. 

is 

F.  Correction Subsystem 
to  process, 
the  correction  subsystem 
The  role  of 
synchronize,  and  combine  the  data  streams  coming  from  the 
PSOG and VOG subsystems in order to generate the corrected 
output  signal.  The  first  step  performed  by  the  correction 
subsystem  is  to  apply  any  necessary  filtering  operations  to 
mitigate  noise  in  the  raw  sample  streams.  Given  the  usually 
low levels of noise of single photodiodes and the fact that we 
employ  a  low  sampling  rate  for  the  VOG  subsystem,  the 
existing  noise  can  be  smoothed  by  using  low  complexity 
filters that can run in real-time. In our current experiments we 
use a simple moving average filter of three points (Eq. 12, n = 
3)  but  another  attractive  option  for  real-time  operation  is  the 
Kalman  filter.  Also,  the  correction  subsystem  needs  to 
perform  the  synchronization  of  the  high  sampling  rate  PSOG 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
part of the signal corresponding to 10 seconds of reading. The 
text stimulus allows to explore the eye-tracking behavior in a 
less  constrained  scenario  with  combined  horizontal  and 
vertical eye  movements. In  Fig. 8 (top), we present  the  exact 
ground truth signals for the second eye-tracking scenario. 
3)  Sensor movements 
For  both  scenarios  (HV,  TX)  we  perform  simulated  sensor 
movements by changing the position of the camera module of 
our  setup.  Each  simulated  sensor  movement  lasts  for  2.5 
seconds  for  the  TX  scenario  and  4  seconds  for  the  HV 
scenario. The magnitudes of the simulated sensor movements 
cover  a  range  of  Â±1.75  mm  (horizontal/vertical)  with  step  of 
0.5  mm.  The  sensor  movements  were  performed  in  different 
parts of the signal, resulting thus in a variety of experimental 
combinations of eye and sensor movements. To ensure that we 
cover  the  most  extreme  cases  for  the  used  ranges  of  eye  and 
sensor  movement  we  explicitly  performed  the  largest  sensor 
movements (Â±1.75 mm) at parts of the signal where the largest 
eye movements occurred (Â±10Â°). 

B.  Baseline Performance 
In this section, we examine the baseline characteristics of the 
hybrid  PS-V  technique  when  assuming  no  sensor  movement 
(the  correction  mechanism  is  inactive).  In  Fig.  7-8  (HV-TX 
scenarios)  we  present  the  eye-tracking  ground  truth  signals 
(top),  the  output  signals  from  the  simulated  hybrid  PS-V 
technique  (middle),  and  the  absolute  approximation  error 
between  these  signals  (bottom).  For  the  HV  scenario  we  can 
see that the approximation error remains at levels under 1Â° for 
most  of  the  tested  eye  movement  amplitudes.  The  observed 
fluctuations can be attributed to the exact â€˜goodness-of-fitâ€™ of 
the calibration function at different eye positions. We can also 
observe the interferences (crosstalk) in horizontal and vertical 
â€˜saccadesâ€™  of 
channels,  manifested  as  apparent  small 
increasing  amplitude  in  the  output  of  one  direction  (e.g. 
horizontal)  when  eye  movement  activity  occurs  on  the 
opposite direction (e.g. vertical). The crosstalk in the  vertical 
output  when  horizontal  eye  movements  occur  appears  to  be 
relatively  larger.  For  the  TX  scenario  the  horizontal  and 
vertical eye movements follow a more complex pattern, and as 
a  result,  the  approximation  error  is  dispersed  differently  than 
in  the  previous  case.  Once  again,  the  error  remains  at 
relatively low levels of less than 1Â°. Since in TX scenario the 
eye  movements  can  be  performed  simultaneously  in  both 
directions,  the  observation  of  crosstalk  in  this  case  cannot 
provide reliable information. 

In  order  to  further  quantify  accuracy  and  crosstalk  we  use 
the  measures  presented  in  Eq.  15-18.  To  use  these  formulas, 
first, we need to manually identify the parts of the signals that 
correspond  to  fixations,  and  then,  to  select  the  samples  from 
the  interior  of  these  parts  to avoid  outliers  during  transitions. 
The accuracy for every single fixation is calculated using Eq. 
15-16.  For  the  HV  scenario,  NH  denotes  the  number  of 
fixations during the execution of horizontal saccades (seconds 
1-17)  and  NV  denotes  the  number  of  fixations  during  the 
execution  of  vertical  saccades  (seconds  18-34).  For  the  TX 
scenario, where combined movements are performed, it stands 
NH  =  NV  (seconds  1-10).  In  all  cases  M  is  the  number  of 
samples  within  the  fixation  under  consideration  (M  changes 
from  fixation  to  fixation).  The  output  samples  for  the  hybrid 

6 

(cid:3037)

(cid:3037)

  and  the  ground  truth 

(cid:3009),(cid:3023)
PS-V  technique  are  denoted  as  ğ‘¦(cid:3017)(cid:3020)(cid:3023)
(cid:3009),(cid:3023)
. For the calculation of crosstalk 
samples are denoted as ğ‘¦(cid:3008)(cid:3047)
(cid:3009)(cid:3023)  denoting  the  crosstalk  in 
we  use  Eq.  17-18,  with  ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ (cid:3036)
horizontal  channel  when  vertical  eye  movements  occur,  and 
(cid:3023)(cid:3009)  denoting  the  crosstalk  in  vertical  channel  when 
ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ (cid:3036)
horizontal eye movements occur. As we mentioned before, the 
calculation  of  crosstalk  for  the  TX  scenario  does  not  provide 
reliable information, and for this reason, crosstalk is quantified 
only  for  the  HV  scenario.  In  Table  1,  we  present  the 
summarizing  results  for  accuracy  and  crosstalk  showing  the 
calculated mean values (M) and standard deviations (SD) over 
all  respective  fixations  in  each  case.  We  can  observe  slightly 
different  behavior  of  horizontal  and  vertical  accuracy  in  HV 
and TX scenarios, which can be partially attributed to the fact 
that  in  TX  scenario  the  maximum  absolute  horizontal  eye 
movement range is relatively smaller than in HV scenario. 
(cid:3009)
(cid:3037) âˆ’ ğ‘¦(cid:3008)(cid:3047)
(cid:3023)
(cid:3037) âˆ’ ğ‘¦(cid:3008)(cid:3047)
(cid:2879)âˆ‘

(cid:3037)(cid:4698)
ğ‘€â„ , ğ‘– = 1, â€¦ , ğ‘(cid:3023) 
(cid:3037)(cid:4698)
(cid:3257)
(cid:3052)(cid:3256)(cid:3295)

ğ‘€â„ , ğ‘– = 1, â€¦ , ğ‘(cid:3009)   

(cid:3009) = âˆ‘

(cid:3023) = âˆ‘

ğ‘ğ‘ğ‘(cid:3036)

ğ‘ğ‘ğ‘(cid:3036)

(15) 

(16) 

(cid:3014)
(cid:3037)(cid:2880)(cid:2869)

(cid:3014)
(cid:3037)(cid:2880)(cid:2869)

(cid:3262)
(cid:3285)(cid:3128)(cid:3117)

(cid:3285)

(cid:4708) , ğ‘– = 1, â€¦ , ğ‘(cid:3023) 

(17) 

(cid:3009)
(cid:4698)ğ‘¦(cid:3017)(cid:3020)(cid:3023)
(cid:3023)
(cid:4698)ğ‘¦(cid:3017)(cid:3020)(cid:3023)
(cid:3262)
(cid:3257)
âˆ‘
(cid:3052)(cid:3265)(cid:3268)(cid:3271)
(cid:3285)(cid:3128)(cid:3117)
(cid:3262)
âˆ‘
(cid:3285)(cid:3128)(cid:3117)
(cid:3271)
(cid:3052)(cid:3265)(cid:3268)(cid:3271)
(cid:3262)
âˆ‘
(cid:3285)(cid:3128)(cid:3117)

(cid:3262)
(cid:3285)(cid:3128)(cid:3117)

(cid:3285)

(cid:3285)

(cid:3271)
(cid:3052)(cid:3256)(cid:3295)
(cid:3285)
(cid:3262)
(cid:2879)âˆ‘
(cid:3285)(cid:3128)(cid:3117)

(cid:3257)
(cid:3052)(cid:3256)(cid:3295)

(cid:3285)

(cid:3271)
(cid:3052)(cid:3256)(cid:3295)

(cid:3285)

ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ (cid:3036)

(cid:3009)(cid:3023) = (cid:4708)

ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ (cid:3036)

âˆ‘
(cid:3023)(cid:3009) = (cid:4708)

(cid:4708) , ğ‘– = 1, â€¦ , ğ‘(cid:3009)   

(18) 

TABLE I 
Baseline accuracy and crosstalk for the hybrid PS-V technique 

Accuracy (Â°) 

Crosstalk (%) 

Scen. 

H 

V 

M (SD) 

M (SD) 

Scen. 

H-V 

V-H 

M (SD) 

M (SD) 

HV 

0.38 (0.11) 

0.30 (0.12) 

TX 

0.28 (0.15) 

0.39 (0.17) 

HV 

4.7 (1.9) 

5.8 (5.3) 

Fig.  7.  Eye-tracking  scenario  HV:  ground  truth  signal  (top),  hybrid  PS-V 
output (middle), approximation error (bottom). 

   
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
7                                                                                                                                                                                

involved 

We can qualitatively observe that due to the novel scheme that 
uses  the  estimated  sensor  movement  to  correct  the  eye-
tracking  signal,  the  hybrid  PS-V  technique  can  lead  to 
substantial decrease  of  the sensor shift-induced deformations. 
In  next  sections  we  present  the  quantitative  results  of  the 
performance of the hybrid PS-V technique when sensor shifts 
occur. 
1)  Sensor movement estimation error 
Our  evaluation  experiments 
the  execution  of 
simulated  sensor  movements  in  range  Â±1.75  mm  at  different 
parts  of  the  signals  from  HV  and  TX  scenarios.  Before  we 
examine  the  final  eye-tracking  accuracy  it  would  be  valuable 
to inspect the error of the sensor movement estimation process 
itself.  In  Fig.  10,  we  present  diagrams  that  show  the  error  in 
the estimated sensor movement compared to the ground truth. 
The  diagrams  correspond  to  the  aggregated  results  from  HV 
and  TX  scenarios.  The  points  represent  the  calculated  mean 
values  over  all  samples  that  correspond  to  each  performed 
sensor  movement,  and  the  error-bars  show  the  respective 
standard  deviations.  The  theoretically  perfect  estimation  is 
denoted by a dashed line. We can observe that  the deviations 
of the estimated points from the line of perfect estimation are 
in most cases within the range of Â±0.2Â°. Also, we can see that 
the  vertical  movement  estimation  seems  to  be  slightly  less 
accurate  at  the  extremes  of  the  tested  range.  This  can  be 
attributed  to  the  wider  capturing  angle  that  can  disrupt 
capturing  of  pupil  center  and  corneal  reflection(s)  at  these 
positions. 

Fig. 10. Sensor movement estimation error (mean, standard deviation). Perfect 
estimation denoted with dashed line.  

2)  Evaluation of eye-tracking accuracy correction 
In  order  to  evaluate  the  afforded  eye-tracking  accuracy 
correction when sensor movements occur  we  will employ the 
formulas of Eq. 15-16. In this case, though, the used fixations 
are  the  ones  from  the  signal  parts  where  the  corresponding 
sensor  movements  occurred.  In  Fig.  11,  we  present  the  eye-
tracking  accuracy  diagrams  comparing  traditional  PSOG  and 
the  hybrid  PS-V  technique,  for  the  HV  scenario.  We  can 
clearly observe the large intolerance of the PSOG technology 
to sensor movements, especially when these movements occur 
at the same direction with the eye movements (Fig. 11 top-left, 
bottom-right).  For  sensor  movements of Â±1  mm the  accuracy 
error  can  go  at  levels  of  over  5Â°.  On  the  other  hand,  for  the 
hybrid  PS-V  technique  we  can  observe  only  a  small  increase 
of the accuracy error when increasing the magnitude of sensor 
movements,  with  the  error  generally  kept  at  levels  under  1Â°. 
For  the  case  of  different  directions  of  sensor  and  eye 
movements  the  used  PSOG  design  appears  to  be  also 
relatively  robust,  however, even  in this case the  hybrid PS-V 
technique  is  more  consistent  for  the  whole  range  of  sensor 

Fig.  8.  Eye-tracking  scenario  TX:  ground  truth  signal  (top),  hybrid  PS-V 
output (middle), approximation error (bottom). 

longer  perform  with  accuracy 

C.  Performance for Sensor Movements 
In  the  event  of  sensor  movements,  the  traditional  PSOG 
techniques  cannot  cope  with  the  translation  of  the  captured 
areas  and  can  no 
the 
transformation  from  raw  output  units  to  degrees  of  visual 
angle. This can cause shift-induced deformations on the output 
signal. In Fig. 9 (top) we show the ground-truth signal for the 
HV  scenario  (during  horizontal  saccades)  and  in  Fig.  9 
(bottom-left) the resulting output when using traditional PSOG 
(without  correction),  for  the  case  of  an  example  sequence  of 
sensor  movements.  In  Fig.  9  (bottom-right)  we  show  the 
resulting output when using the hybrid PS-V technique. 

Fig.  9.  Sensor-shift  induced  artifacts:  the  ground  truth  signal  (top),  the 
deformed output signal from traditional PSOG (bottom-left), and the corrected 
output signal from the hybrid PS-V technique (bottom-right).  

 
 
 
 
movements.  An  interesting  observation  is  that  in  both  cases 
the  eye-tracking  accuracy  diagrams  are  not  totally  symmetric 
for  positive  and  negative  sensor  movements.  This  should  be 
attributed  partially  to  the  asymmetries  of  the  eye,  e.g.  shape, 
upper  and  lower  eyelids,  and  partially  to  the  exact  used 
experimental combinations for the relative magnitudes of eye 
and sensor movements. It should be emphasized that the exact 
same  experimental  combinations  were  used  both 
for 
traditional PSOG and for the  hybrid PS-V technique  in order 
to ensure the clear investigation of the achieved improvements 
in  accuracy  when  using  the  developed  technique.  In  Fig.  12, 
we  present  the  corresponding  diagrams  for  the  TX  scenario. 
The  overall  trends  are  similar  as  for  the  HV  scenario, 
however,  the  horizontal  eye-tracking  accuracy  error  for 
traditional PSOG seems to rise even more steeply, whereas for 
the  hybrid  PS-V  technique  the  respective  error  appears  to  be 
slightly  larger  and  more  variable  than  previously,  possibly 
affected by the combined execution of eye movements at both 
directions. 

8 

3)  Demonstration of correction in a practical application 
To  demonstrate  the  practical  importance  of  the  afforded 
correction  we  present  an  example  of  the  effects  from  using 
traditional  PSOG  and  the  hybrid  PS-V  technique  in  an 
simplified  application  of  foveated  rendering  [1].  In  this 
example,  we  utilize  data  from  the  TX  eye-tracking  scenario 
and we demonstrate the effects from sensor movement on the 
selection  of  the  â€˜foveated  rendering  areaâ€™,  i.e.  the  region 
around the userâ€™s point of gaze that will be rendered with high 
quality  (in  this  case  higher  resolution).  In  Fig.  13  on  the  left 
column we present the ground truth eye-tracking signal (top), 
the  traditional  PSOG  output  (middle),  and  the  hybrid  PS-V 
technique output (bottom), for the case of a vertical downward 
sensor movement of 1.25 mm occurring between seconds 5-7 
during  the  TX  scenario  (only  the  vertical  component  of 
movement is shown). On the right column of Fig. 13 we show 
for a specific point in time  (within the duration the of  sensor 
shift) the respective gaze points and the areas that are rendered 
with  high-resolution  on  the  original  text  stimulus.  The  gaze 
points  are  back-projected  on  the  stimulus  space  using  the 
signals  and  the  exact  parameters  of  our  experimental  setup. 
The  rendered  areas  are  modeled  as  circles  with  center  the 
respective  gaze  point  each  time  and  radius  of  5Â°,  chosen  to 
resemble  the  size  of  the  central  fovea  of  the  eye  [1].  To 
facilitate  inspection,  the  gaze  point  from  the  ground  truth 
signal (where the user really looked at) is shown with a cross 
and the gaze points estimated  with  either of the  two  methods 
are shown with small circles. 

Fig.  11.  Eye-tracking  scenario  HV:  accuracy  vs.  sensor  movement  for 
traditional PSOG and the hybrid PS-V technique.  

Fig.  12.  Eye-tracking  scenario  TX:  accuracy  vs.  sensor  movement  for 
traditional PSOG and the hybrid PS-V technique.  

Fig.  13.  Demonstration  of  eye-tracking  signals and  corresponding  effects  on 
the high-resolution rendering area for a vertical sensor movement of +1.25 m.  

As  we  can  observe,  when  using  traditional  PSOG  without 
correction the high-resolution rendering area is far away from 
the really attended area, thus generating an unacceptable effect 
for the  user.  On  the other  hand,  when  using  the  hybrid PS-V 

   
 
  
 
 
 
 
 
 
 
 
 
 
9                                                                                                                                                                                

technique  we  can  see  that  the  achieved  correction,  which 
brings  accuracy  error  at  levels  lower  than  1Â°,  results  on  the 
high-resolution rendering of an area that encloses the attended 
location at a sufficient degree. Practically, this means that the 
high-resolution rendering area can be expanded with minimal 
computational cost in order to make the error less perceptible 
to  the  user.  It  should  be  noticed  that  the  brief  spikes  that 
appear  in  the  corrected  signal  are  an  expected  effect  of  the 
non-zero  duration  of  the  sensor  shift  transition  phases  in 
combination  to  the  low  sampling  rate  of  the  VOG  correction 
mechanism (see discussion about delay in next Section). 

V.  DISCUSSION 

A.  General Analysis of the Hybrid PS-V Technique 
The  main  goal  of  our  current  experiments  was  to  assess  the 
degree  at  which  the  developed  hybrid  PS-V  technique  is 
capable  to  address  the  inherent  flaws  of  traditional  PSOG 
technology  when  sensor  movements  occur.  Our  results  show 
that  the  afforded  improvements  in  eye-tracking  accuracy  are 
considerable, and more importantly, the accuracy error due to 
sensor movements is kept at levels of 1Â° or lower,  which can 
be  acceptable  for  various  HCI  applications.  The  investigated 
performance  covers  a  window  of  eye  movements  in  range  of 
Â±10Â°  and  sensor  movements  in  range  of  Â±2mm.  As  it  can  be 
observed in Fig. 4, though, for sensor movements smaller than 
1 mm the output from the PSOG subsystem remains relatively 
linear  even  for  a  larger  range  of  eye  movements  (e.g.  up  to 
Â±15Â°).  This  means  that  depending  of  the  requirements  of  the 
application  under  consideration,  the  technique  allows  the 
possibility to give more  weight on the coverage of larger eye 
movement  range  at  the  expense  of  covered  sensor  movement 
range  and  vice  versa.  The  developed  simulation  framework 
to  experiment  with  different 
gave  us 
parameters  for  the  used  PSOG  design,  such  as  the  size, 
positioning,  and  overlap  of  the  simulated  sensor  reception 
areas. Our experimentation showed that the variation of these 
parameters  can  affect  conversely  the  properties  of  linearity 
and  crosstalk  of  the  system.  The  final  specifications  for  the 
used  PSOG  design  were  selected  with  the  aim  to  provide  a 
good  trade-off  between  these  characteristics  for  the  target 
ranges of eye movements and sensor movements. It should be 
noticed  that  although  further  algorithmic  optimizations  might 
be possible, it is expected that additional hardware (e.g. more 
IR sensors) will be required for the simultaneous coverage of 
much larger range of eye and sensor movements. Since from a 
point  and  after  the  sensors  move  to  areas  where  no  useful 
information  can  be  captured,  the  problem  is  no  longer  of 
algorithmic/processing nature but of lack of information. 

the  opportunity 

Regarding  the  sensor  movement  estimation  via  the  VOG 
subsystem,  the  investigation  of  Fig.  10  reveals  the  expected 
levels  of  error  when  detecting  the  pupil  center  and  corneal 
reflection under various conditions. The observed inaccuracies 
expose  some  inherent  limitations  of  the  pupil  center-corneal 
reflection  technique,  since  the  apparent  movements  of  these 
features can be affected by factors such as the distance and the 
viewing  angle  of  the  capturing  sensor.  Although  such 
inaccuracies are overshadowed by the levels of afforded error 
correction for larger sensor  movements,  for the case of small 
or  no  sensor  movements  the  resulting  artifacts  can  become 

more  prominent.  For  example,  as  we  can  see  in  Fig.  13, 
although  the  correction  for  the  part  of  the  signal  when  the 
sensor shift occurs is remarkable, the rest of the signal appears 
to have small deformations when compared to the ground truth 
and the uncorrected signals (similar small deformations can be 
seen  in  Fig.  9).  A  possible  method  to  mitigate  such  small 
deformations  during  a  practical  implementation  is  to  use  a 
hard (or adaptive) threshold in order to activate/deactivate the 
correction  mechanism  when  the  estimated  levels  of  sensor 
movement are above/below the selected threshold. 

the  overall  correction  performance.  To 

An  interesting  prospect  for  the  current  technique  is  the 
possibility to perform the calibration for sensor movements in 
an  automatic  (and  thus  more  user-friendly)  manner.  This 
option can be performed by using the VOG subsystem during 
the  calibration  process  as  a  feedback  mechanism  to  indicate 
sensor position. Practically, this means that during calibration 
the  user  will  not  be  needed  to  accurately  place  the  sensor  at 
pre-defined  points  but  he/she  can  move  the  sensor  arbitrarily 
(but within the limits of the desired range) and allow the VOG 
subsystem  to  feed  the  ğ‘¥(cid:3046)  values  needed  from  Eq.  9-11.  It 
should be emphasized that for such an operation the reliability 
of the movement estimation algorithm is of utmost importance 
because  any  errors  at  this  early  stage  will  be  propagated  and 
affect 
the 
automatic  calibration  method  for  the  current  setup  we 
performed  additional  experiments,  and  in  Fig.  14  we  present 
the  resulting  curves  (dot-lines)  when  performing  calibration 
via  feedback  from 
these 
experiments  we  moved  the  sensor  at  positions  -2,  0,  and  +2 
mm  and  then  instead  of  using  directly  these  values  during 
calibration  we  used  the  values  estimated  by  the  VOG 
subsystem. The fitted curves are superimposed on the original 
curves  presented  in  Fig.  5  (where  the  exact  sensor  positions 
were  used).  As  we  can  observe,  the  fitted  curves  for  the 
horizontal  sensor  movements  are  very  close  to  the  original 
calibration  curves  whereas  for  the  vertical  sensor  movements 
there  is  more  noticeable  deviation  at  the  extremes.  This 
behavior  reflects  the  slightly  less  accurate  sensor  movement 
estimation  for  the  vertical  sensor  movements,  previously 
observed in Fig. 10. 

the  VOG  subsystem.  During 

test 

Fig.  14.  Demonstration  of  fitted  curves  for  the  automatic  sensor  movement 
calibration process with feedback from the VOG subsystem. 

Another  important  aspect  when  considering  the  practical 
application  of  the  hybrid  PS-V  technique  regards  the  delay 
needed to detect sensor movements. This delay combined with 
the non-zero duration of the sensor shift transition phases can 
result in brief artifacts (spikes) in the signal, as shown in Fig. 
13.  Given  that  the  sensor  movement  estimation  is  performed 
by  the  VOG  subsystem,  it  is  the  low  sampling  rate  of  this 
subsystem  that  mainly  sets  the  bounds  for  the  maximum 

 
 
expected  delay.  As  a  result,  for  the  current  setup  with  the 
VOG  subsystem  running  at  5  Hz  this  maximum  delay  is 
expected  to  be  about  200  ms.  This  delay  can  be  acceptable 
when considering the expected frequencies-of-occurrence and 
durations  of  events  that  usually  induce  sensor  shifts  in  head-
mounted devices, like facial expressions and body movements. 
It  should  be  clarified  that  this  delay  is  related  only  to  the 
periods when sensor movements occur since for the rest period 
of  normal  operation  the  system  tracks  with  the  fast  rate 
warranted  by  the  low  sampling  interval  (1000  Hz,  sampling 
interval:  1  ms)  and  the  low  computational  complexity  of  the 
PSOG subsystem. 

B.  Practical Considerations for Power Consumption, 
Computational Complexity and Cost 
The hybrid PS-V technique can potentially provide significant 
gains  when  compared  to  systems  based  on  VOG  in  terms  of 
power  consumption  when  fast  eye-tracking  is  required. 
Commercial  high-speed  eye-tracking  systems  based  on  VOG 
(e.g. EyeLink 1000 [21]) can consume power of several Watts, 
and their demands can be fulfilled only via tethered operation. 
At  a  research  level,  there  have  been  some  recent  efforts 
focusing to push the limits of VOG power consumption under 
100  mW  [22,  23],  however,  such  optimizations  can  impose 
certain limitations on operational accuracy and sampling rate. 

During high-speed operation of a VOG system there are two 
main  sources  that  can  inflate  power  consumption  rather 
sharply: pixel acquisition and increased computational burden 
of image processing. The hybrid PS-V technique performs the 
high-speed  acquisition  part  with  the  PSOG  subsystem  that  is 
based on simple IR sensors. This gives the ability to operate at 
high  sampling  rates  while  keeping  the  required  increase  in 
power  at  minimum  levels.  Typical  IR  sensors  can  have  total 
power dissipation of 100-200 mW, however, due to their  fast 
switching times (order of tenths of ns) duty-cycle control can 
be applied and combined with voltage optimizations can result 
in power consumption at the order of tenths or hundredths of 
Î¼W [23]. Considering now that the PSOG subsystem uses just 
a few IR sensors (in contrast to thousands of pixels) the power 
consumption for the  high-speed (e.g. 1000 Hz) acquisition  of 
samples from the PSOG subsystem can be expected to be less 
than  1  mW.  The  hybrid  PS-V  technique  uses  also  a  VOG 
subsystem  but  this  subsystem  operates  constantly  at  low 
sampling rate since it is used only for correction when sensor 
movements occur. This allows to keep the power requirements 
for  the  VOG  subsystem  at  a  minimum  level.  The  second 
source of power efficiency of the hybrid PS-V technique is the 
low  computational  complexity.  The  current  PSOG  design 
requires  only  four  additions  and  two  subtractions  for  the 
combination  of  the  sensor  single-valued  outputs.  Just  a  few 
more simple operations are needed for applying the calibration 
mapping  function  (calibration  parameters  are  pre-calculated) 
and  the  running  average  filters.  Hence,  the  total  number  of 
operations will be just a tiny fraction of the operations needed 
by a pure VOG system operating at a high rate (thousands or 
even millions operations per frame).  As previously, the VOG 
subsystem  of  the  hybrid  PS-V  technique  produces  a  steady 
overhead  irrespectively  of  any  increase  of  the  eye-tracking 
rate that is governed by the low-complexity PSOG subsystem. 
Based  on  the  discussed  considerations  the  total  power 

10 

consumption  (acquisition  and  processing)  of  a  system  based 
on  the  proposed  hybrid  PS-V  technique  is  expected  to  be 
under 15 mW while operating at high sampling rates of 1000 
Hz or more. 

Another important advantage of the hybrid PS-V technique 
from  a  systemâ€™s  perspective  is  the  ability  to  keep  the  overall 
cost  at  very  low  levels  when  compared  to  a  pure  VOG 
implementation running at 1000 Hz. Once again, the reason is 
that  in  the  hybrid  PS-V  technique  the  demanding  high-speed 
eye-tracking  part  is  achieved  via  the  PSOG  subsystem.  The 
cost of the IR photo-sensors and a typical video camera can be 
kept  at  the  order  of  tenths  of  dollars,  whereas  on  the  other 
hand the cost of a  video camera operating at 1000 Hz can be 
hundreds  of  dollars.  The  large  difference  in  the  required 
budget  becomes  even  more  prominent  when  considering 
binocular  eye-tracking,  which  is  expected  to  be  the  norm  for 
emerging interaction devices like the AR/VR headsets. 

also, 

rotational 

C.  Current Limitations and Future Extensions   
The current evaluation of the proposed hybrid PS-V technique 
was  done  within  the  scope  of  certain  limitations.  The 
calibration function is trained with eye and sensor movements 
performed separately at the horizontal and vertical directions. 
The  proposed  technique  can  be  further  strengthened  by 
exploring  more  generalized  calibration  functions  suitable  to 
cope with scenarios involving (large) oblique eye movements, 
and 
sensor  movements. 
and  depth 
Furthermore,  the  currently  used  sensor  movement  estimation 
algorithm  assumes  that  at  least  one  corneal  reflection  can  be 
traced  on  the  eye  image.  However,  such  an  assumption  can 
pose  certain  limitations  for  the  positioning  of  sensors.  The 
investigation of alternative  mechanisms  for sensor  movement 
estimation  based  on  other  characteristics  (e.g.  pupil-ellipse 
shape) can provide more flexibility when larger eye movement 
ranges  and/or  wider  sensor  positioning  angles  are  required. 
Even though we described the favorable characteristics of the 
hybrid  PS-V  technique  in  terms  of  power  consumption,  the 
limits  can  be  pushed  even  further  with  the  development  of  a 
low-complexity mechanism that can detect reliably the onsets 
and  offsets  of  sensor  movements.  This  mechanism  would 
allow  operating  the  VOG  subsystem  at  an  asynchronous 
â€˜detection-triggeredâ€™  rate,  and  also,  it  would  assist  on  the 
mitigation  of  spike-artifacts  appearing  during  the  transition 
periods.  Last,  the  hardware  implementation  of  the  technique 
on  a  head-mounted  device  (e.g.  an  AR/VR  enabled  headset) 
can allow for the exploration of possible design improvements 
and  the  detailed  examination  of  real-eye  artifacts  that  might 
not be covered sufficiently by the current simulation. 

VI.  CONCLUSION 

In this paper we described the hybrid PS-V technique, a novel 
approach  that  combines  the  principles  of  photosensor  and 
video  oculography  in  order  to  tackle  the  accuracy  issues  of 
traditional photosensor oculography when sensor shifts occur. 
Our  investigation  was  based  on  the  use  of  a  semi-simulated 
framework  making  possible  to  explore  the  behavior  of 
different components of the technique in a controlled manner, 
and  leading  to  the  formulation  of  a  composite  calibration 
model that can be used to effectively combine the information 

   
 
  
 
 
 
 
 
 
11                                                                                                                                                                                

[16] 

[17] 
[18] 

[19] 

[20] 

[21] 

[22] 

[23] 

Computing, vol. 26, no. 1, pp. 20-26, 1988. 
L.  Swirski  and  N.  Dodgson,  "Rendering  synthetic  ground  truth 
images for eye tracker evaluation," presented at the Symposium on 
Eye Tracking Research & Applications (ETRA), 2014.  
(2017). Blender. Available: http://www.blender.org/ 
G.  Massobrio  and  P. Antognetti,  Semiconductor  Device  Modeling 
with SPICE, 2nd Edition. McGraw-Hill, 1993. 
S.  M.  Kolakowski  and  J.  B.  Pelz,  "Compensating  for  eye  tracker 
camera movement," presented at the Symposium on Eye Tracking 
Research & Applications (ETRA), 2006.  
(2017). Haytham Gaze-Tracker. Available: 
https://sourceforge.net/projects/haytham/ 
(2017). SR-Research EyeLink 1000. Available: http://www.sr-
research.com/EL_1000.html 
A.  Mayberry,  P.  Hu,  B.  Marlin,  C.  Salthouse,  and  D.  Ganesan, 
"iShadow:  design  of  a  wearable,  real-time  mobile  gaze  tracker," 
presented  at  the 12th Annual  International  Conference  on Mobile 
Systems, Applications, and Services, 2014.  
A.  Mayberry  et  al.,  "CIDER:  Enabling  Robustness-Power 
Tradeoffs  on  a  Computational  Eyeglass,"  presented  at  the  21st 
Annual  International  Conference  on  Mobile  Computing  and 
Networking (MobiCom '15), 2015. 

coming  from  the  PSOG  and  VOG  subsystems.  The  results 
from  our  evaluation  experiments  demonstrate  the  large 
accuracy  improvements  that  can  be  achieved  for  sensor 
movements  in  range  of  Â±2  mm.  The  achieved  levels  of 
correction  combined  to  the  favorable  characteristics  of  the 
photosensor  oculography  subsystem,  reveal  the  promising 
prospects for using the hybrid PS-V technique to enable high-
speed,  low-power  eye-tracking  in  modern  head-mounted 
devices. 

REFERENCES 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

in  virtual 

"A  discussion  of  cybersickness 

B. Guenter, M. Finch, S. Drucker, D. Tan, and J. Snyder, "Foveated 
3D graphics," ACM Trans. Graph., vol. 31, no. 6, pp. 1-10, 2012. 
J.  Triesch,  B.  T.  Sullivan,  M.  M.  Hayhoe,  and  D.  H.  Ballard, 
"Saccade  contingent  updating  in  virtual  reality,"  presented  at  the 
Symposium  on  Eye  Tracking  Research  &  Applications  (ETRA), 
2002.  
F.  Argelaguet  and  C.  Andujar,  "A  survey  of  3D  object  selection 
techniques for virtual environments," Computers & Graphics, vol. 
37, no. 3, pp. 121-136, 2013. 
D.  M.  Hoffman,  A.  R.  Girshick,  K.  Akeley,  and  M.  S.  Banks, 
"Vergence-accommodation conflicts hinder visual performance and 
cause visual fatigue," J Vis., vol. 8, no. 3, pp.:33.1-30 2008. 
J.  LaViola, 
environments," ACM SIGCHI Bull., vol. 32, pp. 47-56, 2000. 
E.  D.  Guestrin and  E. Eizenman, "General theory  of  remote  gaze 
estimation  using  the  pupil  center  and  corneal  reflections,"  IEEE 
Transactions on Biomedical Engineering, vol. 53, no. 6, pp. 1124-
1133, 2006. 
D. A. Robinson, "A Method of Measuring Eye Movement Using a 
Scleral  Search  Coil  in  a  Magnetic  Field,"  IEEE  Transactions  on 
Bio-medical Electronics, vol. 10, no. 4, pp. 137-145, 1963. 
O.  H.  Mowrer,  T.  C.  Ruch,  and  N.  E.  Miller,  "The  corneo-retinal 
potential  difference  as  the  basis  of  the  galvanometric  method  of 
recording  eye  movements,"  American  Journal  of  Physiology,  vol. 
114, no. 2, p. 423, 1935. 
J.  E.  Russo,  "The  limbus  reflection  method  for  measuring  eye 
position,"  Behavior  Research  Methods  &  Instrumentation,  vol.  7, 
no. 2, pp. 205-208, 1975. 
N.  Torok,  V.  J.  Guillemin,  and  J.  M.  Barnothy,  "Photoelectric 
nystagmography,"  Ann  Otol  Rhinol  Laryngol.,  vol.  60,  no.  4,  pp. 
917-926, 1951. 
L.  L.  J.  Wheeless,  R.  M.  Boynton,  and  G.  H.  Cohen,  "Eye-
movement  responses  to  step  and  pulse-step  stimuli,"  J  Opt  Soc 
Am., vol. 56, no. 7, pp. 956-960, 1966. 
A. T. Bahill and L. Stark, "Neurological control of horizontal and 
vertical  components  of  oblique  saccadic  eye  movements," 
Mathematical Biosciences, vol. 27, no. 3, pp. 287-298, 1975. 
R.  Jones,  "Two  dimensional  eye  movement  recording  using  a 
photo-electric matrix method," Vision Research, vol. 13, no. 2, pp. 
425-431, 1973. 
C.  R.  Brown  and  P.  H.  Mowforth,  "An  improved  photoelectric 
system  for  two-dimensional  eye  movement  recording,"  Behavior 
Research Methods & Instrumentation, vol. 12, no. 6, pp. 596-600, 
1980. 
J.  P.  H.  Reulen  et  al.,  "Precise  recording  of  eye  movement:  the 
IRIS  technique  Part  1,"  Medical  and  Biological  Engineering  and 

 
