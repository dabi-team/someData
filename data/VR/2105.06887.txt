A Frequency Domain Constraint for Synthetic and Real 
X-ray Image Super Resolution 

Qing Ma1, Jae Chul Koh2 and WonSook Lee1 

1 University of Ottawa, Ottawa, Canada  
1 Korea University Anam Hospital, Seoul, Korea  
{qma088,wslee}@uottawa.ca 

Abstract. Synthetic X-ray images are simulated X-ray images projected from CT 
data. High-quality synthetic X-ray images can facilitate various applications such 
as surgical image guidance systems and VR training simulations. However, it is 
difficult to produce high-quality arbitrary view synthetic X-ray images in real-
time due to different CT slice thickness, high computational cost, and the com-
plexity of algorithms. Our goal is to generate high-resolution synthetic X-ray im-
ages in real-time by upsampling low-resolution images with deep learning-based 
super-resolution methods. Reference-based Super Resolution (RefSR) has been 
well studied in recent years and has shown higher performance than traditional 
Single Image Super-Resolution (SISR). It can produce fine details by utilizing 
the reference image but still inevitably generates some artifacts and noise. In this 
paper, we introduce frequency domain loss as a constraint to further improve the 
quality of the RefSR results with fine details and without obvious artifacts. To 
the best of our knowledge, this is the first paper utilizing the frequency domain 
for the loss functions in the field of super-resolution. We achieved good results 
in evaluating our method on both synthetic and real X-ray image datasets.  

Keywords: Synthetic X-ray, Super Resolution, Frequency Domain, Digital Re-
constructed Radiographs  

1 

Introduction 

Efforts have been made on converting CT volume or slice images into synthetic X-ray 
images, also known as Digital Reconstructed Radiographs (DRRs), with fast rendering 
algorithms such as ray tracing or ray casting methods [1–4]. In recent years, deep learn-
ing aided algorithms can generate highly realistic X-ray images but require vast training 
data and high computation resources  [5, 6]. In general, there are three challenges to 
high-quality synthetic X-ray image generation. Firstly, it demands high computation 
resources to produce high-quality synthetic X-ray images, preventing the use in clinics 
or hospitals. Secondly, the algorithms for generating high-quality synthetic X-ray im-
ages are often time-consuming or complex. Finally, the thickness of the scan is not thin 
enough due to high dose radiation of CT scanning. The gaps between slices yield lower 
resolution  in  certain  views.  A  basic  requirement  for  generating  fine  synthetic  X-ray 
images is that the resolution is approximately equal in all views [7]. Thus, it is necessary 

2 

to up-sample the lower resolution views, either with interpolation or more sophisticated 
methods such as semantic interpolation [8] or super-resolution (SR) methods.  

Various application scenarios have been explored with synthetic X-ray images. For 
example, synthetic X-ray images are used in virtual reality (VR) training simulation for 
training physicians on fluoroscopy-guided intervention procedures [9, 10]. It intends to 
replace real-life training sessions which are costly and exposed to radiation. High-qual-
ity DDRs can also be used to train deep learning models for diagnostic tasks [5]. It’s 
also been found that using synthetic X-ray images can reduce up to half the amount of 
fluoroscopic images taken during real fluoroscopy-guided intervention procedures [11]. 
The performance of the above applications could all benefit from higher-quality syn-
thetic X-ray image data sources.   

Deep learning-based SR methods have been well explored for natural images. There 
are two main categories which are Single Image Super-Resolution (SISR) and Refer-
ence-based super-resolution (RefSR). SISR aims to reconstruct a high-resolution (HR) 
image from a single low-resolution (LR) image [12]. RefSR works on learning finer 
texture  details  from  a  given  reference  HR  image  [13].  Convolution  neural  networks 
(CNN) and Generative adversarial network (GAN) methods have been widely used in 
SISR.  CNN  models  can  reach  high  performance  on  evaluation  metrics  but  produce 
overly smooth images with coarse details. On the other hand, GAN models generate 
appealing images with fine details but with more artifacts. This issue is especially cru-
cial  in  medical  imaging  as  image  details  have  an  impact  on  diagnosis  and  decision-
making during operations. RefSR can learn fine texture details from the reference im-
age but still generate some artifacts and noise. Fourier transform is rarely utilized in the 
field of super-resolution. We propose to use a frequency domain loss as a constraint to 
mitigate this problem. 

We used a matrix-based projection algorithm and custom-built lookup tables created 
with tissue radiographic opacity parameters to generate fine LR synthetic X-ray images 
from CT slices in arbitrary views. And then, we use a RefSR method combined with 
our frequency domain loss to generate HR synthetic X-ray images with few artifacts 
and  noise.  We  achieved  good  results  on  both  synthetic  and  real  X-ray  image  super-
resolution with our proposed TTSR-FD.  

2 

Related Work 

Deep  learning  was  first  used  in  SISR  by  Dong  et  al.  [14].  They  proposed  SRCNN 
achieved superior results compared to previous conventional SR methods. Lim et al. 
[12] proposed EDSR introduced Residual Block into SISR that further boosts the model 
performance. Zhang et al. [15] added the attention mechanism to improve the network 
performance. These methods used CNN yield a strong PSNR performance. However, 
the results do not have a good visual quality for human perceptions. GAN gradually 
become successful in the field known for good visual quality. Ledig et al. [16] proposed 
SRGAN first adopted GAN and showed appealing image quality. Wang et al. [17] in-
troduced Residual-in-Residual Dense Block and relativistic GAN further improved the 
perceptual quality of the results.  

3 

RefSR takes advantage of learning more accurate texture details from the HR refer-
ence image. The reference image could be selected from adjacent frames in a video, 
images from different viewpoints, etc. [18]. It can achieve visually appealing results 
without generating many artifacts and noise compared to GAN-based SISR. Zheng et 
al. [19] proposed CrossNet that adopted a flow-based cross-scale warping to transfer 
features. SRNTT [18] adopted patch matching with VGG extracted features and can 
use arbitrary images as references. Yang et al. [13] proposed TTSR applied a trans-
former  network  with  attention  models  that  outperformed  traditional  SISR  methods. 
Nevertheless, these models still generate perceivable artifacts and noise in the resulting 
image.  

Fourier transform is a powerful tool in the field of signal processing, and it has also 
shown  great  potential  in  deep  learning-related  research.  Souza  and  Frayne  [20]  pro-
posed  a  hybrid  framework  for  magnetic  resonance  (MR)  image  reconstruction  that 
learns  in  both  frequency  and  spatial  domain.  Li  et  al.  [21]  proposed  the  first  neural 
network SR method by solely learning in the frequency domain. It shows advantages 
on the speed of the model but with an imperceptible loss on the quality of results.  Xu 
et al. [22] proposed a novel learning-based frequency channel selection method that 
achieved superior results on multiple tasks. In this work, we introduce to compute a 
loss  function  in  frequency  domain  as  a  constraint  for  the  neural  network,  instead  of 
transferring the network itself into frequency domain.  

Fig. 1. Overview of our TTSR-FD method. We add the frequency domain loss on the TTSR [13] 
model.  The  four  input  images  are  LR,  up-sampled  LR,  reference  images  (Ref)  and  down/up-
sampled Ref Image. These rescaling operations will help to improve the texture transfer accuracy 
[13]. SR image is the model output. HR image is the ground truth image. 

3  Methods 

We aim to reduce the artifacts generated by the network while not losing fine details. 
To achieve this, we made a thoughtful analysis in the frequency domain for SR methods 
and introduce frequency domain loss with TTSR [13] as illustrated in Fig. 1. We first 

 
4 

explore the frequency domain pattern for images of different quality. Then, we intro-
duce our loss function computed in the frequency domain.  

3.1 

Frequency domain analysis 

We adopt Fourier transform to reveal frequency patterns that are not visible in the spa-
tial domain. Different image samples are shown in both spatial and frequency domain 
in  Fig.  2.  While  the bicubic  interpolated  image  shows the  worst  results,  we observe 
high-frequency details are also not fully learned by any of the SR models. We show our 
TTSR-FD result as a comparison of the improvement. 

Fig. 2. Images in the both spatial and frequency domain from left to right are ground truth, LR 
input, Bicubic upsampling, and RCAN, ESRGAN, TTSR, TTSR-FD SR methods results.  

In general, we can see whiter regions in the center indicating images contain more 
low-frequency content. The input image loses lots of high-frequency details compare 
to the GT. The bicubic upsampling is not able to produce any high-frequency details. 
Limited high-frequency details are generated by RCAN. ESRGAN can learn some good 
high-frequency details and shows sharp details in the spatial domain. But we can see 
abnormal patterns in frequency domain such as a rectangle shape contour in the center. 
This indicates it generates artifacts to satisfy human perceptual. TTSR can generate fine 
high-frequency details but also fails to match some patterns. Our TTSR-FD got the best 
similarity in frequency domain compared to GT. We can find out that the frequency 
domain contains lots of information that is not shown in the spatial domain.  

3.2 

Frequency Domain Loss 

We make a hypothesis based on the observation from the previous section that a loss 
function computed in the frequency domain can increase the resulting image quality of 
RefSR. We aim to use this loss to set up a constraint during training that can force the 
model to learn more from the data and generate fewer artifacts and noise. We choose 
to build it with a pixel-wise loss function to ensure the network learns from the fre-
quency domain patterns. We utilize  1 loss which is proven effective for SR methods, 
also more robust and easier to converge compared to  2 loss. Our network architecture 
is  shown  in  Fig.  1.  In  short,  we  compute  the  loss  by  transferring  model  output  and 

 
5 

ground truth images of each iteration into frequency domain, calculate their  1 loss and 
feed back to the network. We applied this loss to our baseline model TTSR [13], which 
has three loss functions. The overall loss of our TTSR-FD model is:  

ℒ        =     ℒ    +    ℒ   +     ℒ    +     ℒ    

(1) 

where ℒ    is  the  reconstruction  loss  with  1 loss. ℒ    is  the  adversarial  loss  using 
WGAN-GP [23]. ℒ    is the perceptual loss that include a normal perceptual loss and 
a texture wise loss [13]. We added a frequency domain loss ℒ   to improve the network 
performance.   is  the  weight  coefficients  for  the  loss  functions  that  are  optimized 
through vast expiriments. In each iteration, a batch of SR and HR images are transferred 
into the frequency domain and then calculated their  1 loss.  

    =      (   ),         =      (   ) 

ℒ   =

 

   

‖    −     ‖  

(2) 

(3) 

where     and     are high resolution and predicted SR result images.     and      are 
the corresponding frequency domain images. C, H, W are the channel, height and width 
of the HR image.        represent the real-to-complex discrete Fourier transform func-
tion. We adopt real-to-complex discrete Fourier transform from Pytorch to improve the 
computation efficiency. We use the built-in FFT function from PyTorch without the 
need to transfer tensors into other data types. There is no significant increase in time 
complexity when using our frequency domain loss during training. 

4 

Experiments 

4.1  Dataset 

We create two reference-based SR datasets to evaluate our method, a synthetic head X-
ray images dataset and a real chest X-ray images dataset. Both datasets are constructed 
following similar approaches described in [18] while making some adjustments con-
sidering the characteristic of synthetic and real X-ray images. 

The synthetic X-ray images dataset is created from head CT data provided by Korea 
University Anam Hospital. It contains seven different head CT series from five patients 
with Sagittal (SAG) and Coronal (COR) views. The number of slices in these series 
ranges from 48 to 145 and each slice resolution is 512x512. We used different linear 
interpolation variables to fill the gaps between slices for each series. Our projection 
method is developed in Python with Numba. In the training set, we use a sampling step 
of 24º for both x and y-axis to generate 225 (15x15) input images for each CT series. 
The corresponding reference image is generated with a random projection angle range 
from -45º to 45º. The training images are then cropped into five patches of 160x160 for 
each input image and reference image. The LR images are generated by downsampling 
the input images. We build a small and large training set SynXray_S and SynXray_L. 
Each of them consists of 410 and 1350 image pairs. We choose to leave at least one 
patient out during training to make sure our test data is not seen during training. The 

 
 
 
6 

test set images are generated with a sampling step of 20º. There are 30 image groups in 
the test set from all five patients.  

We also create a real chest X-ray images dataset (ChestXrayRef) from ChestX-ray8 
[24] dataset for RefSR. We construct the dataset using a similar method in [18]. How-
ever,  the  ChestX-ray8  dataset  only  has  X-ray  images  in  a  single  frontal  view.  This 
makes it impossible to construct the dataset like synthetic X-ray images where the ref-
erence image is from a different viewpoint. We choose to randomly pick another chest 
X-ray image from the dataset as the reference image. This will certainly weaken the 
ability of RefSR method to reconstruct high-resolution textures, but it is also proven 
that the reference image doesn’t have to be related to the input image [18]. All images 
are randomly picked and resized to 512 x 512 resolution. The training data has 1298 
image pairs. The testing set has 23 image groups. 

4.2  Training Details 

We used a similar training parameter setting as in TTSR [13]. We use Pytorch 1.81 to 
implement  our  network.  We  trained  100  epochs  and  pick  the  highest  performance 
model for all datasets. We applied the same parameter setting in all our training. We 
use a batch size of 4. The learning rate is 1e-4 and Adam optimizer with    = 0.9,    =
0.999 and   = 1  − 8.  The  weight  coefficient  of the  frequency domain  loss  is  1e-2. 
The  weight  coefficient  for ℒ    , ℒ    , ℒ     are  1,  1e-3  and  1e-2,  respectively.  We 
trained our network for a scaling factor of x4. We use bicubic kernel for rescaling the 
input and reference images. We trained our model with a Tesla P100 GPU. The training 
time for large and small synthetic datasets is around 10 and 4 hours respectively. The 
training time for the real X-ray dataset is around 18 hours. 

4.3  Results 

We first compare our method on synthetic X-ray images dataset with state-of-the-art 
methods, such as RCAN [15], ESRGAN[17] and TTSR[13]. We make some modifica-
tions to our baseline model TTSR for training grayscale images, where we duplicate 
the grayscale channel into three channels corresponds to RGB channels for natural im-
ages then feed to the neural network. We retrain RCAN and ESRGAN models on the 
paired SISR dataset version of our dataset for comparison purposes as well. We evalu-
ate our method on PSNR and SSIM as shown in Table 1. TTSR-FD achieves superior 
performance on both metrics. We also observe that TTSR-FD is strong on learning from 
a small dataset which indicates it can learn more from the data. This can be beneficial 
for real-life applications since medical data are hard to obtain.  

We show the visual comparison for our method trained with SynXray_S on the test 
set  in  Fig.  3,  consider  that  the  visual  difference  between  large  and  small  datasets  is 
minimal for our method. RCAN shows the second-highest accuracy in the quantitative 
measurement but we observe the image is overly smoothed and has limited high-fre-
quency details. We can observe that the noise from the red circle area and black dots 

7 

artifacts from the red rectangle area in TTSR have been removed in TTSR-FD. Our 
method can generate fine details without generating much noise and artifacts. 

Table 1. PSNR/SSIM comparison among different SR methods on SynXray_S and SynXray_L 
datasets. The best results are in bold. 

Method 
RCAN 
ESRGAN 
TTSR 
TTSR-FD (ours) 

SynXray_S 
38.597/0.9482 
34.483/0.9023 
37.953/0.9393 
39.009/0.9521 

SynXray_L 
38.880/0.9496 
35.270/0.9140 
38.115/0.9431 
39.261/ 0.9514 

Fig. 3. Visual comparison for our method on the testing set. Example input and Ref images are 
generated from CT series in sagittal (1, 2) and coronal (3) view. TTSR-FD is ours. 

We further test our model on Real chest X-ray images. The two models have similar 
quantitative results as shown in Table 2. However, we find out that the artifacts gener-
ated in TTSR-FD are significantly less than in TTSR results as shown in Fig. 4. This 
validates the effectiveness and generalizability of our proposed method. 

Table 2. PSNR/SSIM comparison among TTSR and TTSR-FD on ChestXrayRef dataset 

Method 
TTSR 
TTSR-FD 

PSNR 
36.767 
36.766 

SSIM 
0.9457 
0.9470 

 
8 

4.4  Ablation study 

In this section, we further verify the effectiveness of our proposed method. We set up 
TTSR-Rec101 that has the same ratio of weight coefficients distribution as TTSR-FD 
where we only increase the weight coefficients of reconstruction loss ℒ    from 1 to 
1.01, compared to the weight coefficients of TTSR-FD for ℒ    and ℒ   are 1 and 0.01. 
The weight coefficients of other loss functions are the same for both models. Our results 
are shown in Table 3. The proposed frequency domain loss significantly improved the 
model performance. We show the resulting images in frequency domain in Fig. 5. We 
can see that without frequency domain loss, increasing the weight  coefficient of the 
reconstruction loss could not improve the similarity of high-frequency patterns in fre-
quency domain. We observe that the vertical line patterns are not learned by TTSR-
rec101 or TTSR as shown in red arrows. 

Fig. 4. Real Chest X-ray visual comparison between TTSR and TTSR-FD 

Fig. 5. Ablation study on frequency domain loss. Vertical line patterns indicated in red arrows 
in the ground truth image are only learned by TTSR-FD. 

Table 3. Ablation study for frequency domain loss with SynXray_S dataset 

Method 
TTSR 
TTSR-Rec101 
TTSR-FD 

PSNR 
37.953 
38.245 
39.009 

SSIM 
0.9393 
0.9405 
0.9521 

 
 
9 

5 

Conclusion 

We proposed a texture transformer super-resolution with frequency domain loss as a 
constraint for synthetic and real X-ray image super-resolution. We demonstrate that an 
additional loss function computed in the frequency domain can improve the image qual-
ity for synthetic and real X-ray images super-resolution. Our work enables the possi-
bility of generating high-quality synthetic X-ray images in real-time for image guiding 
systems and VR simulations. Even though the experiments were done on X-ray images, 
the proposed method is applicable for other medical images. In future works, we would 
like to explore more possible loss variants and other application scenarios such as de-
noising with frequency domain loss.  

References 

1. 

2. 

Siddon,  R.L.:  Fast  calculation  of  the  exact  radiological  path  for  a  three-dimensional  CT 
array. Medical physics. 12, 252–255 (1985). 
Jacobs, F., Sundermann, E., De Sutter, B., Christiaens, M., Lemahieu, I.: A fast algorithm 
to calculate the exact radiological path through a pixel or voxel space. Journal of computing 
and information technology. 6, 89–94 (1998). 

3.  Russakoff, D.B., Rohlfing, T., Mori, K., Rueckert, D., Ho, A., Adler, J.R., Maurer, C.R.: 
Fast generation of digitally reconstructed radiographs using attenuation fields with applica-
tion to 2D-3D image registration. IEEE transactions on medical imaging. 24, 1441–1454 
(2005). 

4.  Vidal, F.P., Garnier, M., Freud, N., Létang, J.-M., John, N.W.: Simulation of X-ray Atten-

uation on the GPU. In: TPCG. pp. 25–32 (2009). 

5.  Unberath, M., Zaech, J.-N., Lee, S.C., Bier, B., Fotouhi, J., Armand, M., Navab, N.: Deep-
drr–a  catalyst  for  machine  learning  in  fluoroscopy-guided  procedures.  In:  International 
Conference  on  Medical  Image  Computing  and  Computer-Assisted  Intervention.  pp.  98–
106. Springer (2018). 

6.  Dhont, J., Verellen, D., Mollaert, I., Vanreusel, V., Vandemeulebroucke, J.: RealDRR–Ren-
dering of realistic digitally reconstructed radiographs using locally trained image-to-image 
translation. Radiotherapy and Oncology. 153, 213–219 (2020). 

7.  Lin, E., Alessio, A.: What are the basic concepts of temporal, contrast, and spatial resolution 
(2009). 
J  Cardiovasc  Comput  Tomogr. 

cardiac  CT? 

403–408 

3, 

in 
https://doi.org/10.1016/j.jcct.2009.07.003. 

8.  Li, J., Koh, J.C., Lee, W.-S.: Hrinet: Alternative Supervision Network For High-Resolution 
Ct  Image  Interpolation.  In:  2020  IEEE  International  Conference  on  Image  Processing 
(ICIP). pp. 1916–1920 (2020). https://doi.org/10.1109/ICIP40778.2020.9191060. 

9.  Allen, D.R.: Simulation Approaches to X-ray C-Arm-based Interventions. 85. 
10.  Korzeniowski, P., White, R.J., Bello, F.: VCSim3: a VR simulator for cardiovascular inter-

ventions. Int J CARS. 13, 135–149 (2018). https://doi.org/10.1007/s11548-017-1679-1. 
11.  Touchette, M., Newell, R., Anglin, C., Guy, P., Lefaivre, K., Amlani, M., Hodgson, A.: The 
effect  of  artificial  X-rays  on  C-arm  positioning  performance  in  a  simulated  orthopaedic 

10 

surgical setting. International Journal of Computer Assisted Radiology and Surgery. 1–12 
(2020). 

12.  Lim, B., Son, S., Kim, H., Nah, S., Mu Lee, K.: Enhanced deep residual networks for single 
image super-resolution. In: Proceedings  of  the IEEE conference on computer vision and 
pattern recognition workshops. pp. 136–144 (2017). 

13.  Yang, F., Yang, H., Fu, J., Lu, H., Guo, B.: Learning Texture Transformer  Network for 

Image Super-Resolution. arXiv:2006.04139 [cs]. (2020). 

14.  Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convolutional 
networks.  IEEE  transactions  on  pattern  analysis  and  machine  intelligence.  38,  295–307 
(2015). 

15.  Zhang, Y., Li, K., Li, K., Wang, L., Zhong, B., Fu, Y.: Image super-resolution using very 
deep residual channel attention networks. In: Proceedings of the European conference on 
computer vision (ECCV). pp. 286–301 (2018). 

16.  Ledig,  C.,  Theis,  L.,  Huszár,  F.,  Caballero,  J.,  Cunningham,  A.,  Acosta,  A.,  Aitken,  A., 
Tejani, A., Totz, J., Wang, Z.: Photo-realistic single image super-resolution using a gener-
ative adversarial network. In: Proceedings of the IEEE conference on computer vision and 
pattern recognition. pp. 4681–4690 (2017). 

17.  Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., Qiao, Y., Change Loy, C.: Esrgan: 
Enhanced super-resolution generative adversarial networks. In: Proceedings of the Euro-
pean Conference on Computer Vision (ECCV) Workshops. pp. 0–0 (2018). 

18.  Zhang, Z., Wang, Z., Lin, Z., Qi, H.: Image super-resolution by neural texture transfer. In: 
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 
pp. 7982–7991 (2019). 

19.  Zheng, H., Ji, M., Wang, H., Liu, Y., Fang, L.: Crossnet: An end-to-end reference-based 
super resolution network using cross-scale warping. In: Proceedings of the European con-
ference on computer vision (ECCV). pp. 88–104 (2018). 

20.  Souza,  R.,  Frayne,  R.:  A  Hybrid  Frequency-Domain/Image-Domain  Deep  Network  for 
Magnetic  Resonance  Image  Reconstruction.  In:  2019  32nd  SIBGRAPI  Conference  on 
Graphics, 
(2019). 
https://doi.org/10.1109/SIBGRAPI.2019.00042. 

(SIBGRAPI). 

257–264 

Patterns 

Images 

and 

pp. 

21.  Li, J., You, S., Robles-Kelly, A.: A frequency domain neural network for fast image super-
resolution. In: 2018 International Joint Conference on Neural Networks (IJCNN). pp. 1–8. 
IEEE (2018). 

22.  Xu, K., Qin, M., Sun, F., Wang, Y., Chen, Y.-K., Ren, F.: Learning in the frequency domain. 
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 
pp. 1740–1749 (2020). 

23.  Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.: Improved training of 

wasserstein gans. arXiv preprint arXiv:1704.00028. (2017). 

24.  Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: ChestX-Ray8: Hospital-
Scale  Chest  X-Ray  Database  and  Benchmarks  on  Weakly-Supervised  Classification  and 
Localization of Common Thorax Diseases. In: 2017 IEEE Conference on Computer Vision 
and 
(2017). 
(CVPR). 
https://doi.org/10.1109/CVPR.2017.369. 

Recognition 

3462–3471 

Pattern 

pp. 

 
