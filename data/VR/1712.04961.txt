7
1
0
2
c
e
D
3
1

]

V
C
.
s
c
[

1
v
1
6
9
4
0
.
2
1
7
1
:
v
i
X
r
a

Real-time Egocentric Gesture Recognition on
Mobile Head Mounted Displays

Rohit Pandey∗ Marie White∗

Pavel Pidlypenskyi

Xue Wang

Christine Kaeser-Chen

Google Inc.
{rohitpandey,mariewhite,podlipensky
xuew,christinech}@google.com

Abstract

Mobile virtual reality (VR) head mounted displays (HMD) have become popu-
lar among consumers in recent years. In this work, we demonstrate real-time
egocentric hand gesture detection and localization on mobile HMDs. Our main
contributions are: 1) A novel mixed-reality data collection tool to automatic an-
notate bounding boxes and gesture labels; 2) The largest-to-date egocentric hand
gesture and bounding box dataset with more than 400,000 annotated frames; 3) A
neural network that runs real time on modern mobile CPUs, and achieves higher
than 76% precision on gesture recognition across 8 classes.

1

Introduction

Mobile virtual reality (VR) head mounted displays (HMD), such as Daydream and GearVR, have
made VR more accessible. Making users believe that they can interact with the virtual environment is
critical to immersion. Since people interact with the real environment mostly with hands, we study
how to bring hand presence to VR. In this work, we focus on hand gesture detection and localization.
Our goal is to reliably recognize and localize hand gestures in real-time on mobile HMD systems.

There are two main challenges to this problem:

1. There is limited dataset available on egocentric hand gestures and bounding boxes.
2. It is challenging for high-capacity machine learning models to run at interaction framerate

on mobile devices

To the ﬁrst challenge, we propose to utilize mobile mixed reality headset as a tool to collect data and
automatically label bounding box. With this method, we collected a large dataset of 33 people in
30 different scenes, with a total of 406,581 annotated frames. To download the dataset, please visit
https://sites.google.com/view/hmd-gesture-dataset.

To the second challenge, we trained a neural network based on the TensorFlow Object Detection API
[1]. The network uses MobileNet [4] as the feature extractor and SSD head [6] to generate multibox
predictions. When running on mobile, one forward pass of our model takes 31.85 milliseconds on
one core mobile CPU, achieving real-time performance.

2 Background

There has been extensive research on hand gesture recognition system. [10], [7], [11] provide
execellent surveys on vision-based gesture recognition systems.

∗Equal Contribution

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

 
 
 
 
 
 
Thumbs_Press

Thumbs_Up

Thumbs_Down

Peace

# of Frames
(% of total)

113206
(27.8%)

120716
(29.7%)

55844
(13.7%)

116815
(18.7%)

Table 1: Sample images and gesture distribution in our dataset

(a)

(b)

(c)

(d)

Figure 1: Image statistics of the dataset. (a) Distribution of bounding boxes; (b) Histogram of bounding box
sizes; (c) Histogram of pixel intensity in all images; (d) Histogram of pixel intensity inside all bounding boxes;

Convolutional neural networks (CNN) such as in [8] and recurrent neural networks (RNN) such as in
[3][2] have further pushed the boundary on gesture recognition results. Unfortunately none of them
has shown real-time performance on mobile devices.

Building large and diverse datasets for hand gestures remains challenging. Existing gestures datasets
such as those from [9] have less than 10,000 annotated frames. The closest datasets to our work are
EgoFingers [5] where 93,729 frames are labeled, and EgoGesture [2] where 3 million frames have
gesture labels but no bounding boxes. Compare to these datasets, our dataset contains 406,581 frames
with both gesture labels and bounding boxes.

3 Dataset

3.1 Label As You Go

In order to scale data collection, we utilize mobile mixed reality headsets to collect automatically
labeled data. Images in our dataset are labeled by the subjects as the images are recorded, instead of
by annotators after the fact.

We used a Daydream View, a Google Pixel XL smartphone, and a monochrome USB camera that is
connected to the phone and faces the world. In the headset, users can see digital video passthrough of
the outward facing camera, and hence see the real world while in VR.

On top of video passthrough, we overlay a bounding box target on each frame in camera image space.
For each gesture class, subjects are instructed to pose the requested gesture, and ﬁt their hands tightly
into the rendered bounding box target. With the mixed reality setting, this task is simply done through
natural hand-eye coordination.

We vary the location of the bounding box to increase coverage of the dataset. To further reduce time,
we animate the bounding box target in a pre-deﬁned zigzagging trajectory which sweeps across the
whole frame. As the trajectory is predictable and easy to remember, the subjects are able to follow
the box, even as it moves to a new location. Each subject participating in data collection was asked to
pose 4 gestures on each hand: Thumbs_Press, Thumbs_Up, Thumbs_Down, and Peace. For
each gesture, we instrument 3 sequences of trajectories. The bounding box size stays the same for a
single sequence, but varies from sequence to sequence. We provide a clicker to the subjects to signal
the start of each data collection sequence.

2

3.2 Dataset Details

With the label as you go approach, we built a dataset of 406,581 frames of egocentric hand gesture
data. The full dataset creation process took only two days. Each frame is labeled with gesture class
and the bounding box of hands. Our dataset contains data from 33 subjects and 4 gesture classes on
each hand. Since the mixed reality setup is mobile, we are able to collect data in different locations.
As a result, we have 30 scenes under varying lighting conditions in the dataset. Table 1 has full
breakdown of the dataset, and Figure 1 has image statistics of our dataset.

4 Mobile Object Detection Model for Gesture Recognition

4.1 MobileNet SSD Architecture

We trained a gesture recognition CNN based on TensorFlow Object Detection API [1]. It is composed
of two parts conceptually: a MobileNet [4] feature extractor to produce feature maps, and a SSD [6]
multibox detector to predict bounding box location and gesture labels (Figure 2). For each anchor
box in the SSD head, the model predicts 4 offset values of the bouding box, and 9 class labels (4
distint gestures multiplexed with either left or right hand, and one None class). We use a cross
entropy loss for classiﬁcation, and smooth L1 loss as in [12] for bounding box localization loss. We
add the two losses together as the ﬁnal loss function.

Figure 2: Illustration of the MobileNet SSD model.

After model inference, we pick the bounding box proposal with the highest conﬁdence in label
prediction, as we only expect one label per image. SSD model natively supports multi-class and
multi-instance prediction too.

4.2 Experiments and Results

Our models are trained with TensorFlow. Our training set contains 342,227 frames, and evaluation set
contains 64,354 frames. The same person only appears in one split. For training, we use a batch size
of 32. Image sizes are 320×240, and of one single color channel. We add random data augmentation
to the training dataset, including brightness and contrast perturbation, and random crop and padding.

On our evaluation dataset, we test the top conﬁdence bounding box prediction against the groundtruth
label. Detailed results of the model performance can be found in Table 2.

4.3 Mobile Inference

The trained TensorFlow models can be exported to run on mobile devices. We benchmarked them on
SnapDragon 821 chipset, which is common among Android devices since 2016. All results in Table
2 reﬂect model inference time on one Big CPU core on device.

In our application, we chose the MobileNet-0.25 model. Our inference framerate is at 30 frames
per second on device. Accounting for pre- and post-processing steps, the whole gesture detection
pipeline can run at 27 fps sustainably.

3

Model

Depth multiplier

Precision

Inference latency
(ms)

Total latency
(ms)

MobileNetSSD-25%
MobileNetSSD-50%
MobileNetSSD-100% 1

0.25
0.5

76.15%
77.43%
80.94%

31.8504
77.4913
265.2109

36.1658
81.6922
269.4694

Table 2: Results of model performance. Total latency includes inference, pre- and post-processing.

5 Conclusion

In this work, we present a mobile egocentric gesture recognition pipeline. We built a mobile mixed-
reality data capture tool, with which we can automatically annotate gestures and bounding box
locations. We created the largest-to-date egocentric gesture and bounding boxes dataset. We trained a
neural network based on the TensorFlow Object Detection API [1], and achieved 76.41% precision
and real-time performance on mobile devices.

As future work, our label as you go approach can be adapted to other data collection tasks, such as
keypoint and segmentation mask annotation. It can also be deployed on smartphones, where users
could be asked to move their phone such that the object in the viewﬁnder ﬁts into the rendered target.

References
[1] TensorFlow Object Detection API. https://github.com/tensorflow/models/

tree/master/research/object_detection, 2017. Accessed: 2017-10-30.

[2] C. Cao, Y. Zhang, Y. Wu, H. Lu, and J. Cheng. Egocentric Gesture Recognition Using Recurrent
3D Convolutional Neural Networks With Spatiotemporal Transformer Modules. In The IEEE
International Conference on Computer Vision (ICCV), Oct 2017.

[3] R. Cui, H. Liu, and C. Zhang. Recurrent Convolutional Neural Networks for Continuous Sign
Language Recognition by Staged Optimization. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.

[4] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and
H. Adam. MobileNets: Efﬁcient Convolutional Neural Networks for Mobile Vision Applications.
CoRR, 2017.

[5] Y. Huang, X. Liu, L. Jin, and X. Zhang. DeepFinger: A Cascade Convolutional Neuron Network
Approach to Finger Key Point Detection in Egocentric Vision with Mobile Camera. In 2015
IEEE International Conference on Systems, Man, and Cybernetics, Oct 2015.

[6] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg. SSD: Single

Shot MultiBox Detector. CoRR, 2015.

[7] S. Mitra and T. Acharya. Gesture Recognition: A Survey. IEEE Transactions on Systems, Man,

and Cybernetics, Part C (Applications and Reviews), 37(3):311–324, 2007.

[8] P. Molchanov, S. Gupta, K. Kim, and J. Kautz. Hand Gesture Recognition With 3D Convolu-
tional Neural Networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) Workshops, June 2015.

[9] E. Ohn-Bar and M. M. Trivedi. Hand Gesture Recognition in Real Time for Automotive
Interfaces: A Multimodal Vision-Based Approach and Evaluations. IEEE Transactions on
Intelligent Transportation Systems, 15(6):2368–2377, 2014.

[10] V. I. Pavlovic, R. Sharma, and T. S. Huang. Visual Interpretation of Hand Gestures for Human-
Computer Interaction: A Review. IEEE Trans. Pattern Anal. Mach. Intell., 19(7):677–695,
1997.

[11] S. S. Rautaray and A. Agrawal. Vision Based Hand Gesture Recognition for Human Computer

Interaction: A Survey. Artif. Intell. Rev., 43(1):1–54, 2015.

[12] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN: Towards Real-Time Object Detection

with Region Proposal Networks. CoRR, 2015.

4

