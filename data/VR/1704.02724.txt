CanvoX: High-resolution VR Painting in Large
Volumetric Canvas

Yeojin Kim∗
Ewha Womans University

Byungmoon Kim†
Adobe Research
Young J. Kim§
Ewha Womans University

Jiyang Kim‡
Ewha Womans University

7
1
0
2

r
p
A
0
1

]

R
G
.
s
c
[

1
v
4
2
7
2
0
.
4
0
7
1
:
v
i
X
r
a

Figure 1: Virtual reality painting in very large canvas (40 km3) and with high details (down to 0.3 mm3).
Background is a large volumetric structure painted by a very large brush (about one kilometers), while
foreground objects are painted with a very ﬁne (a few millimeters) brush. To help audiences not to lost in
the painting, and to allow artist to focus, foreground area, called rooms can be deﬁned, shown as yellow
boxes in the far right image. Audiences starts from a room, navigate (translate and scale) in the canvas,
and switch to another room any time.

Abstract

With virtual reality, digital painting on 2D canvases is now being extended to 3D spaces. Tilt Brush

and Oculus Quill are widely accepted among artists as tools that pave the way to a new form of art

- 3D emmersive painting. Current 3D painting systems are only a start, emitting textured triangular

geometries.

In this paper, we advance this new art of 3D painting to 3D volumetric painting that

enables an artist to draw a huge scene with full control of spatial color ﬁelds. Inspired by the fact that

2D paintings often use vast space to paint background and small but detailed space for foreground, we

claim that supporting a large canvas in varying detail is essential for 3D painting. In order to help artists

focus and audiences to navigate the large canvas space, we provide small artist-deﬁned areas, called

∗e-mail:yeojinkim@ewhain.net
†e-mail:bmkim@adobe.com
‡e-mail:soarmin11@ewhain.net
§e-mail:kimy@ewha.ac.kr

 
 
 
 
 
 
rooms, that serve as beacons for artist-suggested scales, spaces, locations for intended appreciation

view of the painting. Artists and audiences can easily transport themselves between different rooms.

Technically, our canvas is represented as an array of deep octrees of depth 24 or higher, built on CPU

for volume painting and on GPU for volume rendering using accurate ray casting. In CPU side, we

design an efﬁcient iterative algorithm to reﬁne or coarsen octree, as a result of volumetric painting

strokes, at highly interactive rates, and update the corresponding GPU textures. Then we use GPU-

based ray casting algorithms to render the volumetric painting result. We explore precision issues

stemming from ray-casting the octree of high depth, and provide a new analysis and veriﬁcation. From

our experimental results as well as the positive feedback from the participating artists, we strongly

believe that our new 3D volume painting system can open up a new possibility for VR-driven digital

art medium to professional artists as well as to novice users.

1 Introduction

Since prehistoric days human has been painting on 2D canvas. The oldest painting can be traced back

to around 40,000 years ago, made by a homo sapience in El Castillo, and around the same time, a Ne-

anderthal’s rock engraving in Gorham’s cave was made. Throughout human history, pioneering artists

have been continuously searching for new art forms, in particular, new forms of paintings. Certain forms

gained popularity in time - an example would be Van Gogh style paintings. With virtual reality (VR), digi-

tal painting on 2D surfaces is being extended into 3D volumes. VR-based 3D painting applications such as

Tilt Brush and Quill have recently surged and are now widely accepted by artists to be positioned as a new

art form. 3D painting is distinguished from real world sculpting as there are no limits posed by real world

material or gravitational constraints, while VR systems provide realistic or non-realistic emmersive view

of the new art form. We believe these new VR-based 3D paintings will thrive, similarly to 2D paintings

started from the prehistoric cave-wall era.

However, current 3D painting is in an early stage, and has much room for improvement. State-of-the-art

systems such as Tilt brush and Quill output only mesh geometry as a result of artist’s stroking, and relies

on a rasterization pipeline to render the geometry. Such system is cannot easily depict thick volumetric

shapes and tends towards being a collection of thin ribbon likes structures. Color mixing of multiple

brush-strokes is hard to implement and not supported by current systems; e.g. Fig. 3. Since each stroke

needs to add mesh-geometry, the number of applicable strokes is bounded by the number of strokes a GPU

can render. When an artist paints over and over to correct or mix colors, there will be no color mixing

effect and this only increases the stroke count which steadily degrades system performance. Therefore a

common painting task of repetitive stroking is not yet available in existing 3D painting systems. Finally,

semi-transparent strokes are hard to render as a large number of long and highly curved strokes may

complicate the depth-ordering.

Main Results In this paper, we explore a new route for VR painting in 3D space, volumetric painting.

In contrast to volume-less, open-surface strokes, used by the state-of-the-art 3D painting systems, our vol-

umetric strokes applied to a 3D grid is amenable to depicting both solid and non-solid shapes. Moreover,

artists can repeatedly apply strokes to the same area until satisﬁed with the mixed color result and its de-

tail; without having appreciable degradation in performance. Transparency is also naturally handled. In a

3D volumetric canvas, unlike 2D planar canvas, distant objects in the background can be painted relatively

large in size compared to foreground objects - for example distant mountains. To support large canvases,

we use an array of deep octrees of high depth (e.g. level 24 or higher). We then deﬁne rooms to allow

detailed paintings for foreground objects. The remainder of the scene (outside the deﬁned rooms) serves

as background. We can limit details that are not observable from any room. In this way, we can maintain

very large canvas, e.g. a virtual canvas of up to 40 Km3 painting space with very ﬁne details of 0.3mm3

painting of tiny voxels inside a room (see Fig. 1). Artists can also deﬁne multiple rooms anywhere inside

the 3D canvas and add more details when observable from the rooms. In summary, our main contributions

to VR painting are:

• Volumetric painting using large canvas with high detail,

• Large canvas painting with varying resolution based on distance or visibility from rooms,

• Rooms for multiple room-scale appreciations/experience.

To realize this idea on commodity computing and VR platforms, we propose a new painting system capable

of incremental updates to the underlying GPU-based octree data structures along with a novel stereo

volume rendering techniques using accurate ray casting.

In detail, our painting system addresses the

following technical challenges:

• A hybrid octree data structure that utilizes both CPU and GPU architectures and maintains efﬁcient

synchronization between them,

• Incremental budget-limited octree reﬁnement at highly-interactive rates for painting and rendering,

• Precision analysis using cell-local coordinates to allow accurate ray casting for deep octree evalua-

tion.

The rest of this paper is organized as follows. Followed by brief literature survey in section 2, we discuss

our observations on 3D painting in comparison to 2D painting and 3D modeling in section 3. In section

4, we describe our adaptive grid representation and explain our painting modeling and VR interfaces in

section 5. In section 6, we provide an accurate ray-casting algorithm for rendering adaptive grids and

discuss experimental results in section 7. We conclude our paper and discuss future work in section 8.

2 Related Work

We brieﬂy survey the prior works relevant to our new 3D painting system including GPU-based octree

representations and ray-casting on them.

2.1 3D Painting System

The concept of 3D painting system was introduced for more than two decades ago. In early 90s, [Hanrahan

and Haeberli 1990] showed that artists can paint directly on 3D models in real-time, but their work is in-

capable of representing transparency and ﬁne detailed painting. From [Daily and Kiss 1995], 3D painting

systems started storing data in a 2D texture map to support painting details. However, 2D parametriza-

tion leads to artifacts such as distortion or seams. To avoid these problems, [DeBry et al. 2002; Benson

and Davis 2002] suggested an octree representation based on 2D texture map which contains only the

surface of 3D models. With the increasing performance of graphics hardware, it is possible to accelerate

these algorithms by porting them from the CPU to GPU [Lefebvre et al. 2005] or to implement octree-like

data structures entirely on GPU [Lefohn et al. 2006]. More recently, [Schmid et al. 2011] proposed im-

plicit canvas that uses parametric strokes to be projected onto the underlying 3D models. Note that all of

these 3D painting systems focus only on painting the 2D surfaces of models, not extended to volumetric

painting, mainly due to high memory consumption.

With the advent of modern VR systems, it become very natural to extend 2D painting to 3D space; begin-

ning with [Keefe et al. 2001]’s work, VR painting systems such as Tilt Brush1 and Quill2 lead a new trend

for 3D painting and modeling. All these systems render quad strips as a result of painting strokes, thus

have a difﬁculty in representing volumetric objects. Furthermore, voxel-based modeling or painting such

as Oculus Medium3 or High Fidelity4 were introduced. However, due to the space and time complexity of

uniform grid, they support only limited canvas spaces and also limited detail.

2.2 GPU-based Octree

The idea of a GPU-based octree representation starts from [Lefebvre et al. 2005] with pointer pools, as-

suming that the underlying model is static. This work supports top-down traversal on an octree. Other

researchers [Gobbetti et al. 2008; Crassin et al. 2009; Laine and Karras 2010; Museth 2013] extended

the octree application to massive volumetric data sets.

[Gobbetti et al. 2008; Crassin et al. 2009] man-

ages the octree both on CPU and GPU asynchronously in a view-dependent manner. Glift [Lefohn et al.

2006] explored an adaptive resolution tile packed to seamlessly cover a domain entirely on the GPU with

constant-time access.

2.3 Ray Casting on Adaptive Grids

Existing ray-casting algorithms for adaptive grids often utilize acceleration techniques, such as empty

space skipping [Kruger and Westermann 2003], by using multiple rendering passes. In order to reduce

neighbor ﬁnding cost, octree neighbor linking were proposed [Samet 1989; MacDonald and Booth 1990].

1http://www.tiltbrush.com
2http://storystudio.oculus.com
3http://www.oculus.com/medium/
4http://highﬁdelity.io

Also the ROPE algorithm [Havran et al. 1998] was developed to accelerate k-d tree neighbor ﬁnding.

These precomputed neighbors have been continuously used.

[Gobbetti et al. 2008] suggests stackless

ray-casting after updating all six neighbors from the CPU to the GPU in view-dependent manner. [Lefohn

et al. 2006; Crassin et al. 2009] used similarly precomputed neighbors to accelerate neighbor ﬁnding

for samples stored at octree corners. In this paper, we make a small step forward by using only three

neighbors, computed in the GPUs from the primal octree represented by only two indices: parent and the

ﬁrst child.

To our best knowledge, octrees as deep as 24 were not used for ray casting. The deepest trees we found

was 40963 used in [Museth 2013], which is equivalent to the octree depth of 12 (our canvas is equivalent

to (4 × 224)3), and the ray angle drift error has not been identiﬁed as a challenge. We propose a solution

and thoroughly analyze the numerical precision associated with ray drift during ray traversal using a very

deep tree.

3 Observations on 3D Painting

3.1 Transitioning From 2D To 3D Painting

The space representable by 2D paintings is not limited - it is vast while detailed. Imagine an artist wanting

to paint a bacteria sitting at the tip of a needle with the Orion constellation in the background. In a similar

vein, 3D painting should not have spatial or scale limits. However, unlikely in 2D paintings that typically

have details only for near objects with a single view/appreciation point and no 3D-depth, in 3D painting,

highly detailed objects can exist at multiple places in the canvas by placing the audience appreciation

spots anywhere in 3D. Even though artists cannot paint details everywhere, they should be able to paint

details in a few places. In fact, navigating the canvas while zooming in-out to a large space is already

supported by state-of-the-art systems such as Tilt Brush and Quill. Artists are already exploring high

powered zooming options demonstrated by the recent painting worlds-in-worlds5, although audience may

not be able to locate the details as no hint is provided for a point of interest and scale.

5http://youtu.be/EzsG1uqfDTQ

3.2 Comparing 3D Painting and Modeling/Rendering

Painting large canvases in high detail appears to be one of the aspects in 3D paintings different from 3D

modeling, which typically focuses on modeling an object at a given scale precisely, rather than zooming in

or out by a large amount. In conventional 3D graphics pipelines, a scene is constructed by a team of artists

modeling objects, assigning textures and materials, placing lights, and eventually compositing them; in

3D painting, an individual artist alone needs to paint the entire scene. An artist should be able to do so in

an unconstrained manner, i.e., artists should have a direct and intuitive control of coloring at any location.

If they want to express their artistic inspiration to depict a speciﬁc way of lighting, they should be able to

paint lighting effects completely the way they want. Thus, automatic lighting or shading is not a must -

although it will be useful.

Interestingly, in VR-based paintings, artists enjoy expressiveness, owing to the large virtual space; e.g. the

artists tend to make large, initial strokes. They often prefer VR hand controllers to haptic styluses with

higher precision but with much smaller conﬁguration space. Preference of expressiveness to precision is

perhaps another aspect that differentiates 3D painting from 3D modeling.

4 Adaptive and Large Volumetric Grid

It appears to be rather trivial for the surfaced-based 3D painting systems such as Tilt Brush to support large

and highly zoomable canvases. However, this requirement would be a major challenge in volume painting;

existing uniform voxel editing engines, e.g. such as Minecraft6 cannot meet this requirement. To allow

large and detailed canvases, we use an array of deep octrees. Each array element is an octree root that

can be reﬁned 24 times or higher. While the root array helps reducing tree depth and enables immediate

parallelization, all roots that overlap with a brush stroke must be visited. Therefore, we resort to a relatively

coarse, 43 array of octree roots, each of which can be reﬁned to a maximum depth; effectively, this is equal

to an octree of depth 26, hence a deep octree. Using this octree array, we obtain canvas in 40Km3 w.r.t.

the size of a room-scale VR environment with maximum details of 0.3mm3 voxels.

6http://minecraft.net

4.1 Hybrid Representation

We use both CPU and GPU to represent deep octrees for the following reasons. As CPU alone is not

efﬁcient enough to render our large volumetric canvas in stereo for both eyes, each at resolution 1680 ×

1512 and 90 FPS, using the GPU becomes an obvious choice. CPU is primarily used for maintaining and

adjusting the octree, something that is non-trivial to implement on the GPU, especially using OpenGL. The

octrees on the CPU are synchronized with those in GPU in a deferred fashion to maintain high interactive

rates. In addition we found painting strokes on the CPU more ﬂexible, since we have the ability to perform

blend modes not supported natively by GPU hardware.

4.1.1 CPU Memory Layout

Since we are authoring volumetric ﬁelds deﬁned everywhere and also for the sake of simplicity, when a

cell is reﬁned, we always create eight children. Similarly to [Gobbetti et al. 2008], we do not use pointers,

but instead use the index I that uniquely identiﬁes each cell. Our octree is made up of multiple linear

memory pools indexed by I. Painting properties such as color, density, and temporary variables are stored

as separate ﬁeld pools. Field pools can be even added dynamically if needed. We group properties that are

likely to be accessed together and then store them in a single memory pool. For example, we have a pool

that stores the color as well as alpha of a painted voxel packed per each cell. To allow dynamic reﬁnement

and coarsening, we implement linked-list based memory management. The size of an allocation unit is

ﬁxed as we allocate or free eight cells. When a unit is freed, we return to the beginning of the pool so that

the pool is populated from the beginning. The pool begins with a uniform root array that cannot be freed.

We have another separate ﬂag pool for cell depth and other bit ﬁeld ﬂags.

Tree Graph Gp and Neighbor Graph G3

Our tree is deﬁned only by parents and children pools that store the indices of parents and the ﬁrst children.

Note that remaining seven children’s indices are consecutively numbered and hence need not be stored.

Let this primal tree graph be Gp. Since we do not use a dual tree [Lefohn et al. 2006; Laine and Karras

2010], Gp is represented only by two 32-bit indices: parent and the ﬁrst child, at each cell.

Note that ray casting using only Gp may have poor performance as the depth of tree increases; e.g. if the

maximum depth is 24, the distance in Gp to a neighbor can be as long as 48. Therefore, an immediate

neighbor topology such as neighbor linking for octrees [Samet 1989; MacDonald and Booth 1990], or

ropes for KD trees [Havran et al. 1998], is useful to accelerate ray traversal. ROPE connects cells sharing

a face, resulting in the blue connections from cells C and D in Fig. 2. D has ﬁve neighbors. In 2-to-1

balanced octrees [Kim et al. 2015], the number of neighbors can vary from 6 to 24 since one or four

neighbors exist per each of the six cell faces. In our work, we reduce the number of neighbors down to

three.

First, consider the same depth cell (e.g. D1 in Fig. 2) or smaller-depth neighboring cell (C0). Since

each face has only one such neighbor, every cell has six neighbors. We store only three of these six

neighbors. Since eight children have consecutive indices, three of such neighbors that share the same

parent (e.g. C1, C2 of C and D0, D2 of D) and can be obtained immediately from its associated cell

index (e.g., C1 = C + 1). The other three neighbors may have different parents (e.g. C0, C3, D1, D3),

and computing such neighbors requires tree traversals, and hence they are precomputed. We refer to

this neighbor structures as the 3-neighbors topology, generated from the graph denoted by G3 that is a

collection of the three neighbors for each cell.

Using the union of G3 and Gp, we can quickly discover all the six to 24 neighbors, since in Gp ∪ G3, all the

two cells sharing a face have distance 2 in Gp, or 1 or 2 in G3. Therefore, ﬁnding neighbors has a low cost.

For example, the leaves of D1 that contact with D can be found easily as C(D1) and C(D1)+2, where C(·)

denotes the ﬁrst child. In case a ray proceeds to D1, we can quickly identify C(D1) or C(D1) + 2 using the

ray parameters. Since G3 is represented by three indices, we store G3 as a separate neighbor pool.

4.1.2 GPU Memory Layout

Mapping the octree pools in CPU to GPU textures is straightforward. Since GPU textures has a resolution-

limit in each dimension, we cannot use a 1D texture. We must use 2D or 3D textures and map a linear

index I to two or three indices. Since modern GPUs support up to 16 thousand texels per dimension,

2D textures will support up to 256M cells. We pack Gp (parent, child) and depth into a texture, G3 (3-

Figure 2: Neighbors of C (left), and D (right) in quadtree. In this ﬁgure, C, C1, C2 share the same parent,

and hence computing C1 and C2 from C is easy. On the other hand, primal tree distances between C and

C0, C3, and between D and D1, D3 can be very large, and hence we precompute them, and store them in

a separate texture.

neighbors) into another texture, and have RGBA color stored in the other texture. Note that we do not use

G3 on the CPU. G3 texture is rendered from the Gp texture upon updates.

5 Volume Painting and Interfaces

5.1 Color Mix, Pick-Up, and Brush Stamps

In digital painting, pigment deposition from brush and mixing with canvas color itself is a separate topic

and is beyond the scope of this paper. Fortunately, color mixing developed in 2D digital painting appli-

cations is directly applicable to 3D painting. In this paper, we only implement a very simple additive

blending mode. Given brush color (Br, Bg, Bb, Bα) and canvas color (Cr, Cg, Cb, Cα), the output color

will be (Br, Bg, Bb)m + (1 − m) ∗ (Cr, Cg, Cb)), m = bα/(bα + Cα), and the output opacity is bα + cα.

Note that there will be many other alternatives such as the popular per-stroke maximum-opacity based

model [Adobe Systems 2016], physically-based pigment mixing using Kubelka-Munk mode [Baxter et al.

2004], RYB mixing [Chen et al. 2015], or advanced RGB-space color mixing [Lu et al. 2014].

Similarly, 2D brush stamping methods (another orthogonal topic) can be directly applied to 3D painting.

We currently support multiple stamp shapes: sphere, cylinder, box, cone, and procedural Perlin noise. For

CC0C3C1C2D1D0D3D2Dspherical stamping, we support sweeping, resulting in tapered capsules. We place these tapered capsules

to connect the two consecutive samples of a stroke path.

Another common 2D painting practice is color pick-up. We demonstrate this with a simple pick-up im-

plementation. At each sampling point, a brush can pick up color from canvas and blend it with the current

brush color. Similarly to most 2D painting system [Adobe Systems 2016], when a stroke is complete, we

restore the brush color to the original brush color. As show in Fig. 3 (a), color pick-up automatically gen-

erates spatially-varying colors, and this is a popular way that artists generate color variation for painting.

In contrast, surface-based painting systems like Tilt Brush does not have color mixing between strokes as

shown in Fig. 3 (b).

(a) Our system

(b) Tilt Brush

Figure 3: Comparison in terms of color-mix of red and yellow brush strokes.

5.2 Adaptive Brush Size

In 2D painting, artists often want to apply brush-strokes fast and react to immediate feedback. This is

simply continued in 3D painting. For 2D painting, modern CPUs can handle reasonably large brush sizes

of up to a few hundred pixels at interactive rates, but for a larger brush, interactivity starts to diminish.

This could be quite restrictive as well as frustrating to artists; when an artist paints on a large 2D canvas,

the artist has to suffer the delay in a larger brush or should use a smaller brush to ﬁll in large area. In

3D painting, this problem manifests itself at much smaller brush size since a 3D brush can paint a larger

number of voxels than the number of pixels in 2D painting. A solution is to use adaptive grid, where we

can reﬁne the grid only up to a resolution sufﬁcient to represent the brush detail; for a smaller brush, we

reﬁne down to deep, but for a larger brush, we stop at a resolution that is roughly 10 voxels corresponding

to the brush radius. When a brush is applied to an already reﬁned region, this can still be slow. In this case,

we coarsen the grid at the center of the brush where color should be merged into uniform brush color. Not

to destroy exiting details, we coarsen rather conservatively than aggressively. This way, although painting

is not always immediately responsive, but is always interactive and tolerable, even when artist paint a very

large region in one stroke. The sky in Fig. 1 was painted with a very large brush (about a kilometer) whose

effective radius in ﬁnest resolution is millions of voxels.

5.3 Deferred Reﬁning and Coarsening in Octree

Although octree can effectively reject cells that do not intersect with brush stamps, painting a deep octree

can be still expensive, when brush stroke is applied near highly-reﬁned regions. We ease this problem

by developing a multi-step strategy. We ﬁrst paint on an existing tree without tree adjustment and update

it on GPU. Most time, this strategy provides immediate visual feedback to user. The next step is a tree-

adjustment stage. We mark cells that should be reﬁned or coarsened and perform one -level reﬁnement

or coarsening per each frame. To update GPU textures, we split a texture into multiple sub-blocks, mark

blocks dirty when CPU modiﬁes them, and then update only the dirty blocks.

5.4 Painting Interface

Our volume painting system supports conventional painting tools for artists - such as color mixing, eraser,

recoloring and color-blending. As a choice for painting user interfaces, we use off-the-shelf VR controllers

such as HTC Vive controllers. When the controller is triggered, we record the location of controller and

sub-sample the location at lower frequency (about 5Hz) to paint.

Right-handed horizontal touch-swipe changes the painting transparency and the brush radius. The brush

radius is scaled by the zoom level. So, to have a larger brush, the user needs to just zoom out. Trigger

pressure also affects the brush radius. As shown in Fig. 4, we implemented a novel 3D color picking

interface that allows user to select brush-color using a left-handed controller while drawing. Note that

existing color pickers often require a user to use both of their hands, and do not allow changing color

Figure 4: One-handed color picker interface. Using only one hand, a user picks yellowish color (left),

rotates the cube (middle), and changes color while applying strokes at the same time (right).

during stroking. Our color picker displays a RGB-color cube, and the user can move the controller inside

the cube to change the brush color. The user can also rotate or translate the cube. For navigating canvas,

painters use both hand controllers in a way that they grab some points in the canvas and move them around.

6 Rendering Large Volumetric Canvas

Another challenge lurks in rendering 3D volumetric paintings. One viable solution is to extract voxel faces

and render them through raster graphics pipeline using, for instance, OpenGL similarly to Minecraft. How-

ever, as the number of grids in non-uniform size increases, the extracted vertex positions, particularly far

away from the origin, may not be accurate due to numerical error. More signiﬁcantly, geometric extraction

requires a substantial amount of computational time, as the number of voxels grows. Consequently, we

explore an alternative approach: ray casting the volumetric data in octree.

6.1 Accurate Ray Starting Point

To edit ﬁne detail, artists should be able to zoom in to observe cells with highest depth (e.g. 24). However,

the size of these tree cells can be even smaller than the single-precision ﬂoating point granularity except

near the origin. Naively using ﬂoating point for eye position in canvas coordinate will make head positions

snapped to nearby ﬂoating point values, and more signiﬁcantly, the eye distance will be erratic. This leads

to extreme discomfort. Therefore, we carefully maintain canvas-to-VR, VR-to-HMD, and HMD-to-eye

coordinate transformations so that the eye position precision is not lost.

Figure 5: 2D illustration of ray casting on adaptive grids. A ray is ﬁred from the right eye of the user at

pright-eye along d, and the ﬁrst cell-entry and exit points p0, p(cid:48)0 are calculated for the ray. This process

continues for other cells until the ray is terminated.

We propose to compute ray starting point in a cell local coordinate frame, and keep the positioning error

of the starting point sufﬁciently small because of the following reasons. Our cell-local coordinate system

has origin at the cell center and has size one. The coordinates inside the cell has the range of [-0.5, 0.5].

Consequently, the ﬁnest resolution inside a cell is 0.5 × 2−23 in single-precision ﬂoating point regardless

of the size of the cell. If a ray starts from a leaf cell of depth 24, its resolution inside the leaf is extremely

high. In contrast, if a 10km-sized root is not reﬁned hence is a leaf, then the closest point to the boundary

of the cell is 0.3mm away, merely equal to the width of a cell with depth 24. This appears to be coarse.

However, users will not need to zoom in deep inside such a large cell. Even if a user zooms in, nearby

cells will have depths at most one, not providing any detail that can be observed. Moreover, when a user

starts painting, the cell will be reﬁned, and ray starting point will be represented in a coordinate frame of

the reﬁned cell.

6.2 Accurate Ray Traversal

As illustrated in Fig. 5, given a ray and its direction d, and a cell-entry point pi of the ray into the ith

cell, we compute the cell-traversal distance ti and the cell-exit point pi(cid:48) as well as a neighboring cell N

containing pi(cid:48). Since our volumetric canvas covers a large space and cell sizes vary by a large magnitude,

the use of global coordinate system to calculate pi(cid:48) and t can be highly inaccurate. In contrast, cell-local

pright-eyeαℎp1p′1pleft-eyepeye-centerpip′icoordinate system can produce accurate results regardless of zoom level.

Thus, we represent the cell-entry point pi in terms of the cell-local coordinate system, where the origin

is located at the cell center and the cell size is normalized to one. We also ﬁnd the face that contains the

cell-exit point p(cid:48)i with respect to the cell coordinate system. Using the intersecting face and the neighbor

texture described in section 4.1.1, we choose the neighbor cell, the i + 1th cell, to visit, and update pi(cid:48) to

pi+1(cid:48). This process is repeated until the ray terminates after accumulating full opacity or exits the canvas.

6.3 Analysis

As illustrated in Fig. 5, in the ith cell, if the ray hits the top surface, the ray traversal-length ti is computed

as ti = (0.5 − py)/dy, where py is the y coordinate of pi, and dy is the y component of d. The error in

ti will be proportional to ti. Let the machine epsilon (cid:15) = 2−23 for a single ﬂoating point, and f (x) be a

ﬂoating point representation of x. Then f (x + y) = (x + y)(1 + (cid:15)+), with (cid:15)+ ≤ (cid:15). Similarly, the error in

ti is computed as

f (ti) = f

(cid:18) f (0.5 − py)
dy

(cid:19)

=

(0.5 − py)(1 + (cid:15)1)
dy

(1 + (cid:15)2)

= ti(1 + (cid:15)1 + (cid:15)1 + (cid:15)1(cid:15)2) = ti(1 + 2(cid:15)t),

(cid:15)1, (cid:15)2 ≤ (cid:15).

(1)

Note that ignoring (cid:15)1(cid:15)2, we have (cid:15)t ≤ (cid:15). We then compute f (pi(cid:48)) = f (pi + f (ti)) = f (pi + t(1 + 2(cid:15)t)) =

(pi + ti(1 + 2(cid:15)t))(1 + (cid:15)3)) = (pi + ti)(1 + 3(cid:15)p), for some (cid:15)p ≤ (cid:15), ignoring (cid:15)3(cid:15)t. Next, we transform the

coordinates of f (pi(cid:48)) to the neighboring i + 1th cell where the ray continues. This point is computed as

pi+1 = f (f (f (pi(cid:48))s) + c) = ((pi + ti)s + c)(1 + 5(cid:15)i) for some (cid:15)i ≤ (cid:15), where scale s and shift c depends on

the depth and location. This way, f (pi+1) = ˜pi+1(1 + 5(cid:15)), where ˜pi+1 is the exact value computed from

pi. Thus, the numerical error added during the traversal point is proportional to the coordinate values, the

number of ﬂoating point operations 5, and (cid:15).

Since we are using cell coordinate system, each coordinate of pi is in [-0.5,0.5]. Therefore, the error is

always bounded by 2.5(cid:15). In global coordinate system, the error is bounded by 2.5(cid:15)wi, where wi is the

size of the ith cell along the ray. Let e0 be the error in eye location, i.e., the error introduced to compute

pright-eye in cell-local coordinate frame. Starting from this initial error e0, traversing n cells results in

total error e0 + (cid:80)n
(cid:80)n

i=0 2.5(cid:15)wi = e0 + 2.5(cid:15) (cid:80)n

i=0 wi ≤ e0 + 7.5(cid:15)L, where L is the ray length. Note that

i=0 wi ≤ 3L. Thus, by using the cell coordinate system for ray/voxel traversal, we have shown that the

error is proportional to L. Moreover, the error bound in angle sin−1(

2(e0/L + 7.5(cid:15))) does not increase

√

as a function of L, and consequently the ray does not deviate from pixel center by more than a small ﬁxed

angle, regardless of the length of the ray L.

(a) Test Scene

(b) Errors in Ray Angle

(c) # Of Crossed Cells

(d) Ray Length VS Angle Error

(e) Crossed Cells VS Angle Error

Figure 6: Ray traversal using local coordinate system. (a) Test VR painting, (b) Error angle between the

ﬁnal ray location and the initial ray direction, magniﬁed by 109 for display in gray. (c) Number of cells

crossed in gray. (d) and (e) show that the ray-angle error does not grow as the ray traverses. In (d), the

initial error e0/L is dominant when L is small. In (e), the more cells the ray crosses, we see the more

stochastic decay. The red line in (d) is the worst case error bound.

To verify, we performed an experiment, as demonstrated in Fig. 6, and show that the screen-space ray-

deviation from pixel center, formulated as ray-angle error, does not accumulate during ray traversal. In

(c) Test Scene(d) Errors in Ray Angle(e) The Number of Cells Ray Traversed(c) Test Scene(d) Errors in Ray Angle(e) The Number of Cells Ray Traversed(c) Test Scene(d) Errors in Ray Angle(e) The Number of Cells Ray TraversedError in Ray Angle(degrees)Ray Length (meters)Error in Ray Angle(degrees)Number of Cells Crossedfact, the error is indeed very small, and is relatively larger in nearby pixels (the maximum ray-angle error

is 7.5 × 10−9 degrees) due to the initial position error (the position error is 2.5 × 10−7 in L2 norm) and

then slightly decays as L increases. This experimental result implies that we can perform ray-casting even

on mobile GPUs with only half-precision ﬂoats ((cid:15) = 1/1024) using fragment shaders.

We compare the results using world coordinate and local coordinate in Fig. 7.

(a) Canvas Coordinate

(b) Cell Coordinate

Figure 7: When canvas (world) coordinates were used, a ray drifts while traversing cells. This results in

a large angle error (a). We show that this problem is completely solved by using cell-local coordinates

(b).

7 Results and Discussions

We have implemented our prototype VR painting system, CanvoX, that utilizes deep octrees to represent

large volumetric canvas and accurate ray-casting algorithms for volume rendering. Our hardware setup

includes Nvidia GTX 980Ti GPU and Intel Core i7-4790 CPU with 16GB RAM for rendering computation

and HTC Vive for interfacing immersive and personal VR environments.

Volume Painting Results

We invited digital-painting artists to produce ﬁve pieces of volumetric paintings. ”Floating Island and

B-612” in Fig. 1 represents a good example as extension of 2D painting to 3D. The artist painted the scene

with two distinct appreciation rooms with large and colorful sky background. Inside the room, objects

such as little duck, ﬂying birds and apples also show the beneﬁts of our system. By adding rough shades

using recolor mode, the artist drew a fairy-tale island which can be appreciated from any view point.

In a similar way, ”Snow Mountain”(Fig. 10) also shows that an artist can easily draw the mountain in

background and paint detailed objects such as penguins in the room. ”Flying dragons”(Fig. 9) used more

rough and colorful shades on dragon’s body-surface but still maintained ﬁne details on eyes, teeth and

horns. This result show that our system allows artists to paint a large-scale 3D scene to their full control.

Moreover, they can set multiple interesting appreciation rooms at, for instance, back on each dragon, top

of the mountain, or inside clouds. The recoloring and color-mix tools were used for painting naturally-

blended sky. In ”Island”(Fig. 1) and ”Imaginary World”(Fig. 11), artist explores much larger space to

paint outside the room.

Interesting remarks from artists were that they have to change the way they used to perceive painting and

that they need to step away from the familiarity of perspectives on 2D canvas. The second remark is very

interesting as it appears to be result of being able to paint in very large 3D canvas. We believe future

explorations with artist will reveal how we can paint perspective in 3D canvas. For example, recoloring

far away mountains from foreground room would be an interesting way of painting to explore.

8 Conclusions

In this paper, we have proposed a new type of 3D art, volumetric VR painting, that allows artists to express

artistic details on a large volumetric canvas in VR environment. To represent the large volumetric canvas,

we adopt a CPU/GPU hybrid representation of deep octrees with a wide range of varying resolution,

and also exploit the hybrid representation to share the painting workloads so that the system can run at

interactive rates. Moreover, we address the problem of ray-casting precision for deep octrees and analyze

it. Our 3D volume painting system is just the beginning of a new art form, and thus we have a few

issues/limitations that we would like to address in future. First of all, our system still consumes memory

signiﬁcantly due to the sheer size of volumetric data. We plan to port our system to mobile VR platforms

such as Google daydream or GearVR using our accurate ray casting techniques. We will also need to

improve our painting interfaces to be more versatile, competitive to existing 2D digital painting systems.

Finally, we plan to release our volume painting system to the public to initiate and lead a new community

for volume painting.

References

ADOBE SYSTEMS, 2016. Adobe photoshop user guide.

BAXTER, W. V., WENDT, J., AND LIN, M. C. 2004. IMPaSTo: A realistic, interactive model for paint. In

Proceedings of the International Symposium on Non-Photorealistic Animation and Rendering, NPAR,

ACM, New York, NY, S. N. Spencer, Ed., 45–56.

BENSON, D., AND DAVIS, J. 2002. Octree textures. ACM Trans. Graph. 21, 3 (July), 785–790.

CHEN, Z., KIM, B., ITO, D., AND WANG, H. 2015. Wetbrush: Gpu-based 3d painting simulation at the

bristle level. ACM Trans. Graph. 34, 6 (Oct.), 200:1–200:11.

CRASSIN, C., NEYRET, F., LEFEBVRE, S., AND EISEMANN, E. 2009. Gigavoxels: Ray-guided stream-

ing for efﬁcient and detailed voxel rendering. In Proceedings of the 2009 Symposium on Interactive 3D

Graphics and Games, ACM, New York, NY, USA, I3D ’09, 15–22.

DAILY, J., AND KISS, K. 1995. 3d painting: Paradigms for painting in a new dimension. In Conference

Companion on Human Factors in Computing Systems, ACM, New York, NY, USA, CHI ’95, 296–297.

DEBRY, D. G., GIBBS, J., PETTY, D. D., AND ROBINS, N. 2002. Painting and rendering textures on

unparameterized models. ACM Trans. Graph. 21, 3 (July), 763–768.

GOBBETTI, E., MARTON, F., AND GUITI ´AN, J. A. I. 2008. A single-pass gpu ray casting framework

for interactive out-of-core rendering of massive volumetric datasets. The Visual Computer 24, 7-9,

797–806.

HANRAHAN, P., AND HAEBERLI, P. 1990. Direct wysiwyg painting and texturing on 3d shapes. ACM

SIGGRAPH computer graphics 24, 4, 215–223.

HAVRAN, V., BITTNER, J., AND Z ´ARA, J. 1998. Ray tracing with rope trees. In 14th Spring Conference

on Computer Graphics, 130–140.

KEEFE, D. F., FELIZ, D. A., MOSCOVICH, T., LAIDLAW, D. H., AND LAVIOLA, JR., J. J. 2001.

Cavepainting: A fully immersive 3d artistic medium and interactive experience. In Proceedings of the

2001 Symposium on Interactive 3D Graphics, ACM, New York, NY, USA, I3D ’01, 85–93.

KIM, B., TSIOTRAS, P., HONG, J., AND SONG, O. 2015.

Interpolation and parallel adjustment of

center-sampled trees with new balancing constraints. The Visual Computer 31, 10, 1351–1363.

KRUGER, J., AND WESTERMANN, R. 2003. Acceleration techniques for gpu-based volume rendering.

In Proceedings of the 14th IEEE Visualization 2003 (VIS’03), IEEE Computer Society, 38.

LAINE, S., AND KARRAS, T. 2010. Efﬁcient sparse voxel octrees. In Proceedings of the 2010 ACM

SIGGRAPH Symposium on Interactive 3D Graphics and Games, ACM, New York, NY, USA, I3D ’10,

55–63.

LEFEBVRE, S., HORNUS, S., AND NEYRET, F. 2005. Octree textures on the gpu. In GPU Gems 2,

M. Pharr, Ed. Addison-Wesley, 595–613.

LEFOHN, A. E., SENGUPTA, S., KNISS, J., STRZODKA, R., AND OWENS, J. D. 2006. Glift: Generic,

efﬁcient, random-access gpu data structures. ACM Transactions on Graphics (TOG) 25, 1, 60–99.

LU, J., DIVERDI, S., CHEN, W., BARNES, C., AND FINKELSTEIN, A. 2014. RealPigment: Paint

compositing by example. NPAR 2014, Proceedings of the 12th International Symposium on Non-

photorealistic Animation and Rendering (June).

MACDONALD, J. D., AND BOOTH, K. S. 1990. Heuristics for ray tracing using space subdivision. The

Visual Computer 6, 3, 153–166.

MUSETH, K. 2013. Vdb: High-resolution sparse volumes with dynamic topology. ACM Transactions on

Graphics (TOG) 32, 3, 27.

SAMET, H. 1989. Implementing ray tracing with octrees and neighbor ﬁnding. Computers & Graphics

13, 4, 445–460.

SCHMID, J., SENN, M. S., GROSS, M., AND SUMNER, R. W. 2011. Overcoat: An implicit canvas for 3d

painting. In ACM SIGGRAPH 2011 Papers, ACM, New York, NY, USA, SIGGRAPH ’11, 28:1–28:10.

Figure 8: A typical painting procedure. (a) painting ground, (b) painting foreground object, (c) painting

background, (d) painting continues inside the background.

Figure 9: ”Flying dragons” in different view points (top), from a single 3D painting (bottom)

Figure 10: ”Snow Mountain” in different view points.

Figure 11: ”Imaginary Island” in different view points.

