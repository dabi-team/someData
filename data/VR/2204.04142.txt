A NOVEL INTRINSIC IMAGE DECOMPOSITION METHOD TO RECOVER ALBEDO 
FOR AERIAL IMAGES IN PHOTOGRAMMETRY PROCESSING 

Shuang Song 1,2, Rongjun Qin 1,2,3,4,*

 1 Geospatial Data Analytics Laboratory, The Ohio State University, Columbus, USA  
2Department of Civil, Environmental and Geodetic Engineering, The Ohio State University, Columbus, USA   
3 Department of Electrical and Computer Engineering, The Ohio State University, Columbus, USA  
4 Translational Data Analytics Institute, The Ohio State University, Columbus, USA  
Email: <song.1634, qin.324>@osu.edu 

Commission II, WG II/1 

KEY WORDS: Photogrammetry, Aerial Imagery, Albedo, Illumination, Intrinsic Image Decomposition, Shadow Matting 

ABSTRACT: 

Recovering surface albedos from photogrammetric images for realistic rendering and synthetic environments can greatly facilitate its 
downstream  applications  in  VR/AR/MR  and  digital  twins.  The  textured  3D  models  from  standard  photogrammetric  pipelines  are 
suboptimal to these applications because these textures are directly derived from images, which intrinsically embedded the spatially 
and  temporally  variant  environmental  lighting  information,  such  as  the  sun  illumination,  direction,  causing  different  looks  of  the 
surface, making such models less realistic when used in 3D rendering under synthetic lightings. On the other hand, since albedo images 
are  less  variable  by  environmental  lighting,  it  can,  in  turn,  benefit  basic  photogrammetric  processing.  In  this  paper,  we  attack  the 
problem of albedo recovery for aerial images for the photogrammetric process and  demonstrate the benefit  of albedo  recovery for 
photogrammetry data processing through enhanced feature matching and dense matching. To this end, we proposed an image formation 
model with respect to outdoor aerial imagery under natural illumination conditions; we then, derived the inverse model to estimate the 
albedo by utilizing the typical photogrammetric products as an initial approximation of the geometry. The estimated albedo images are 
tested in intrinsic image decomposition, relighting, feature matching, and dense matching/point cloud generation results. Both synthetic 
and  real-world  experiments  have  demonstrated  that  our  method  outperforms  existing  methods  and  can  enhance  photogrammetric 
processing. 

1.  INTRODUCTION 

1.1  Introduction 

Today,  aerial  photogrammetric  3D  reconstruction  from  images 
has  been  developed  into  mature  toolchains  such  as  for 
lightweight  unmanned  aerial  vehicle  systems  (UAVs)  and 
commercial / open-sourced data processing software, which can 
generate  high  resolution,  high  accuracy,  and  photorealistic 
textured  models  for  various  applications  (Alidoost  and  Arefi, 
2017).  However,  the  demands  on  data  quality  for  novel 
applications  are  growing  even  faster,  e.g.,  virtual  reality, 
augmented reality, mixed reality, and the metaverse. It was noted 
that  the  quality  of  photogrammetric  geometry  has  been 
recognized, but the textures do not meet expectations (Innmann 
et al., 2020; Lachambre, 2018). The reason is that classic aerial 
photogrammetry tends to supply texture materials directly from 
the  images  as  â€œalbedosâ€  as  required  by  graphics  rendering 
pipelines, which, however, is drastically different: for example, 
an  albedo  texture  material  is  free  of  shadows  so  the  graphics 
rendering pipeline can cast shadows in a simulated environment, 
while  the  image  textures  may  already  possess  unwanted  cast 
shadows by the actual environmental lighting. On the other hand, 
we are also curious if albedo images can be recovered, and how 
it  may  enhance  or  impact  the  photogrammetric  processing. 
Therefore, in this paper, we aim to study the problem of albedo 
recovery and its application to photogrammetric data processing. 

The concept of deriving albedo from images was widely used in 
the  remote  sensing  community  with  applications  to  satellite 
imagery  (Haest  et  al.,  2009;  Honkavaara  et  al.,  2013,  2012; 
Oâ€™Hara  and  Barnes,  2012).  Various  products  (e.g.,  MODIS 
BRDF/Albedo  products)  were  produced  and  used 
in 
environmental  and  remote  sensing  studies  (Pisek  et  al.,  2016, 
2015;  Z.  Wang  et  al.,  2018).  Nonetheless,  those  models  only 
assume  a  2D/2.5D  world  and  are  not  suitable  for  close-range 
photogrammetry. 

Intrinsic  image  decomposition,  which  originated  in  compute 
vision,  studies the problem of recovering albedo and illumination 
from a single image, it has been investigated since the Retinex 
theory (Land and McCann, 1971) was proposed in 1971. Most 
methods assume a slow varying shading which can be modeled 
with  smoothing  PDE  (partial  differential  equations).  The  latest 
works  involve  deep  learning  models  to  learn  albedo  recovery 
from  the  huge  volume  of  training  data  (Li  and  Snavely,  2018; 
Sheng et al., 2020; Yu and Smith, 2021, 2019). However, those 
methods often operate in black-box mode and lack generalization 
and the ability to comprehend complex physical lighting models 
in outdoor scenarios (Sheng et al., 2020).  

The cast shadow is a common effect in aerial imagery under  a 
clear sky condition which yields a strong discontinuity in shading. 
The goal of shadow detection and removal is to localize masks of 
cast  shadow  in  an  image  and  compensate  pixels  to  produce 
shadow-free images (Mostafa, 2017; Sasi and Govindan, 2015). 
Nonetheless, only the high contrast cast shadows are of concern 

* Corresponding author. 2036 Neil Avenue, Columbus, Ohio, USA. qin.324@osu.edu 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
but the smooth shading is overseen. The deep learning variants 
of those methods trend toward compensating all shading effects, 
but it still unstable and less comprehensive (Qu et al., 2017). 

In this paper, combining principles of intrinsic image and shadow 
detection,  we  propose  a  method  that  utilizes  known  geometry, 
RAW  photos  (digital  negative),  and  their  meta-information 
(capture  date  &  time,  GPS  location)  to  model  the  image 
formation  process,  which  addresses  sharp  and  high-contrast 
shading  of  cast  shadow  in  datasets  with  a  photometric  model 
involving sun and sky irradiance and surface normals (Section 2). 
Then we develop a method to invert the process to estimate the 
albedos  of  the  scene  (Section  3).  The  proposed  method  is 
evaluated w.r.t. various photogrammetric applications, including 
relighting rendering, feature matching, and dense matching. 

1.2  Related Works 

Intrinsic image studies the decomposition of a single image to 
intrinsic layers (Barrow H.G. and Tenenbaum, 1978) including 
but  not  only  diffuse  albedo  (or  reflectance)  and  shading  (or 
illumination).  The  decomposition  is  an  ill-posed  problem  that 
heavily relies on priors to regularize the solution. A few works 
ask users to provide guidance strokes to regularize the problem 
(Bousseau et al., 2009; Shen et al., 2011). Most methods assume 
a sparse or piecewise constant albedo  (Gehler et al., 2011) and 
smooth  monochromatic  illumination.  Automatic  methods  are 
built  on  the  consensus  that  large  derivatives  of  images  are 
attributed  to  the  changes  in  albedo  while  small  derivatives  to 
shading.    Recent  works  are  challenging  the  assumption  by 
detecting  a  shadow  layer  (Sheng  et  al.,  2020)  or  estimating 
complex shading with neural networks (Innamorati et al., 2017; 
Janner et al., 2017; Yu and Smith, 2021, 2019). Physical shading 
is  proposed  to  further  recover  surface  normal  and  lighting 
(Barron  and  Malik,  2015,  2012a,  2012b,  2011),  but  they  are 
limited to convex, non-occluded, single objects. By integrating 
inverse  rendering  techniques,  (Laffont  et  al.,  2013)  works  on 
multi-view  images  of  the  outdoor  scene  by  simulating  light 
transmission  with  the  ray  tracing  engine.  The  method  is 
compatible  with  complex  geometry  but  requires  a  few  manual 
setups  to  collect  environment  irradiance  with  a  light  probe. 
(DuchÃªne,  2015;  DuchÃªne  et  al.,  2015)  utilized  sky  and  sun 
portion  of  ground  collected  images  to  estimate  environment 
irradiance, however, itâ€™s difficult to obtain looking up images for 
aerial imaging. Our method does not rely on direct observation 
of  the  sun  and  sky,  alternatively,  we  estimate  the  essential 
illumination parameters from observed images. 

Shadow  detection  and  removal  studies  the  extraction  and 
removal  of  cast  shadows  from  a  single  image.  Early  works 
including automatic methods (Finlayson et al., 2004) only work 
on images of uncluttered scenes with isolated shadows. Recent 
studies often train neural networks to perform end-to-end shadow 
detection  and  removal  and  have  achieved  great  success  on 
datasets (Cun et al., 2020; Qu et al., 2017; J. Wang et al., 2018), 
but  generalizability  is  of  great  concern  in  practice.  Constraint-
based methods require user-assistant or classifier to indicate the 
shadow  region  (Guo  et  al.,  2013;  Wu  et  al.,  2007). 
Photogrammetry  and  remote  sensing  community  shows  great 
interest in shadow detection removal since for most applications 
(e.g., dense matching, orthorectification, semantic segmentation) 
they are considered a nuisance (Donne and Geiger, 2019; Luo et 
al., 2018; Silva et al., 2018; Wang et al., 2017; Wu and Bauer, 
2013).  Shadow  removal  with  known  DSM  (Digital  Surface 
Model) and sun position was proved to be efficient to supply as 
a constraint (Wang et al., 2017). Pairs of lit and shadow points 
sharing the same albedo have been adopted by works to estimate 

the shadow boundaries (DuchÃªne, 2015; DuchÃªne et al., 2015). 
Similar to the works, pairs of lit and shadow points are heavily 
used  in  this  paper,  not  only  for  shadow  detection  but  also  for 
environmental illumination estimation. 

2.  IMAGE FORMATION MODEL 

The  image  formation  model  describes  the  physical  process  of 
interactions among the light source, surface material, geometry, 
and  the  observed  image,  in  which  our  desired  albedos  are 
involved. 

2.1  Image Model 

Our model follows a few basic assumptions: 
1)  Lambertian  surface  assumption,  which  means  only  diffuse 
albedo is considered; 
2) The intensity of the image is proportional to the irradiance of 
the  corresponding  object,  which  is  implied  in  Equation  1.  The 
RAW  format  (digital  negative)  image  meets  this  requirement, 
which is supported by most cameras and some mobiles. 

ğ‘° (cid:3404) ğ‘¹ âŠ— ğ‘º ,

(cid:4666)1(cid:4667) 

where ğ‘° âˆˆ â„(cid:2871) is the radiance of a surface point observed by the 
camera (i.e., the pixel intensity of the image), ğ‘¹ âˆˆ â„(cid:2871) is albedo 
(or  reflectance)  of  the  object  point, ğ‘º  âˆˆ â„(cid:2871) is  the  shading  (or 
illumination reached to that object point), âŠ— is an element-wise 
product of  two vectors. To be noted, in this paper, vectors and 
matrices are denoted with the bold symbol. 

We  then  decompose  ğ‘º  by  the  outdoor  and  natural  light 
assumption  following  previous  works  (DuchÃªne  et  al.,  2015; 
Laffont  et  al.,  2013),  which  assumes  three  components: 
irradiance from the sun (denoted as ğ‘ºğ’”ğ’–ğ’), sky (denoted as ğ‘ºğ’”ğ’Œğ’š) 
and reflected by other objects (denoted as ğ‘ºğ’Šğ’ğ’…). The Equation 2 
shows the image formation with albedo and shadings. Figure 1 
illustrates the physical process of radiance accumulation. 

ğ‘° (cid:3404) ğ‘¹  âŠ— (cid:3435)ğ‘ºğ’”ğ’–ğ’ (cid:3397) ğ‘ºğ’”ğ’Œğ’š (cid:3397) ğ‘ºğ’Šğ’ğ’…(cid:3439)                                    
(cid:3404) ğ‘¹  âŠ— (cid:3435)ğ‘³ğ’”ğ’–ğ’ğ›¼(cid:3046)(cid:3048)(cid:3041)ğ‘ğ‘œğ‘ (cid:4666)ğœ”(cid:3046)(cid:3048)(cid:3041)(cid:4667) (cid:3397) ğ‘ºğ’”ğ’Œğ’š (cid:3397) ğ‘ºğ’Šğ’ğ’…(cid:3439)  ,

(cid:4666)2(cid:4667) 

where Î±(cid:3046)(cid:3048)(cid:3041) âˆˆ (cid:4670)0,1(cid:4671) is  the  sun  visibility  factor  from  the  point, 
Î±(cid:3046)(cid:3048)(cid:3041) (cid:3404) 1 where  the  point  is  fully  lit  by  the  sun,  or Î±(cid:3046)(cid:3048)(cid:3041) (cid:3404) 0 
when  located  in  the  umbra,  and  other  intermediate  values  if 
located in the penumbra. ğ‘³ğ’”ğ’–ğ’ âˆˆ â„(cid:2871) is the irradiance of the sun, 
Ï‰(cid:3046)(cid:3048)(cid:3041) is the angle between the surface normal ğ’ âˆˆ â„(cid:2871) ( â€–ğ’â€–(cid:2870) (cid:3404)
1 ) and the sun position ğ›‰ğ’”ğ’–ğ’ âˆˆ â„(cid:2871)  ( â€–ğ›‰ğ’”ğ’–ğ’â€–(cid:2870) (cid:3404) 1 ), ğ‘ºğ’”ğ’Œğ’š âˆˆ â„(cid:2871) 
is  the  irradiance  of  the  sky  hemisphere  and ğ‘ºğ’Šğ’ğ’… âˆˆ â„(cid:2871)  is  the 
irradiance of indirect illumination.  

Figure 1.The outdoor illumination model (Yellow arrows are 
sun illumination; blue arrows are sky illumination; red arrows 
are reflected indirect illumination). 

As  we  can  notice  that ğ‘ºğ’”ğ’Œğ’š and ğ‘ºğ’Šğ’ğ’… for each  point  are integral 
terms  that  are  defined  over  a  hemisphere,  which  requires 
predefined/known  environmental  radiance.  Existing  solutions 

 
 
 
 
 
 
 
 
 
 
 
 
 
usually record environmental radiance with a light probe while 
capturing data. (DuchÃªne, 2015; DuchÃªne et al., 2015) avoid light 
probe capturing by reusing sky pixels already captured by images 
in  the  dataset  to  approximate  environmental  radiance.  In  this 
paper, we do not rely on known environmental radiance or direct 
observation from external sources, because our model enables the 
estimation of the essential lighting  parameters from the dataset 
(i.e., aerial images) (Section 3.2). 

2.2  Approximation of Indirect Illumination 

Indirect  illumination  is  an  effect  of  the  reflection  of  diffuse 
surfaces  of  the  scene.  It  is  costly  to  model  and  challenging  to 
integrate  into  inverse  rendering  due  to  its  non-linearity. 
Fortunately, we demonstrate that indirect illumination has minor 
effects  in  the  outdoor  scenario.  Figure  2  presents  two  images 
rendered  without  (left)  and  with  indirect  illumination.  By  both 
visual  comparison  and  evaluating  PSNR  of  two  images,  they 
indicate  that  the  impact  of  ignoring  indirect  illumination  is 
limited.  In  our  model,  considering  the  trade-off  between  costs 
and benefits of indirect illumination, we force ğ‘ºğ’Šğ’ğ’… (cid:3404) ğŸ to make 
our method concise. 

(a) wo/ indirect illumination  (b) w/ indirect illumination 
Figure 2.Evaluate the impact of indirect illumination, the PSNR 
for image (a) with (b) as the reference is 76.75dB. 

2.3  Approximation of Sky Illumination 

We proposed the following sky illumination term to approximate 
the  integral  term  with  the  assumption  that  sky  radiance  is 
uniformly distributed.  

As Figure 3 (b) and (c) show, our model can approximate most 
outdoor  scenarios,  especially,  the  top  and  faÃ§ade  of  buildings. 
The fitting error becomes larger while closing to the building, due 
to  the  ambient  occlusion  effect.  Finally,  our  image  formation 
model is written as Equation 4. In our problem, ğ‘°, ğœ”(cid:3046)(cid:3048)(cid:3041), Ï‰(cid:3053)(cid:3032)(cid:3041)(cid:3036)(cid:3047)(cid:3035) 
are given data, we will estimate the ğ›¼(cid:3046)(cid:3048)(cid:3041), ğ‘³ğ’”ğ’–ğ’, ğ‘³ğ’”ğ’Œğ’š in Section 
3 proposed method. 

ğ‘° (cid:3404) ğ‘¹ âŠ— (cid:4672)ğ‘³ğ’”ğ’–ğ’ğ›¼(cid:3046)(cid:3048)(cid:3041)ğ‘ğ‘œğ‘ (cid:4666)ğœ”(cid:3046)(cid:3048)(cid:3041)(cid:4667) (cid:3397) ğ‘³ğ’”ğ’Œğ’š(cid:4666)0.5 (cid:3397) 0.5 ğ‘ğ‘œğ‘ (cid:4666)Ï‰(cid:3053)(cid:3032)(cid:3041)(cid:3036)(cid:3047)(cid:3035)(cid:4667)(cid:4667)(cid:4673) 

(cid:3404) ğ‘¹ âŠ— (cid:4672)ğ‘³ğ’”ğ’–ğ’ğ›¼(cid:3046)(cid:3048)(cid:3041)(cid:4666)ğ›‰ğ’”ğ’–ğ’ â‹… ğ’(cid:4667) (cid:3397) ğ‘³ğ’”ğ’Œğ’š(cid:4666)0.5 (cid:3397) 0.5 ğ›‰ğ’›ğ’†ğ’ğ’Šğ’•ğ’‰ â‹… ğ’(cid:4667)(cid:4673)    

, (cid:4666)4(cid:4667) 

(cid:3404) ğ‘¹ âŠ— (cid:3435)ğ‘³ğ’”ğ’–ğ’Î±(cid:3046)(cid:3048)(cid:3041)ğ‘˜(cid:3046)(cid:3048)(cid:3041) (cid:3397) ğ‘³ğ’”ğ’Œğ’šğ‘˜(cid:3046)(cid:3038)(cid:3052)(cid:3439)                                                

where ğ‘˜(cid:3046)(cid:3048)(cid:3041) (cid:3404) ğ›‰ğ’”ğ’–ğ’ â‹… ğ’ and ğ‘˜(cid:3046)(cid:3038)(cid:3052) (cid:3404) 0.5 (cid:3397) 0.5 ğ›‰ğ’›ğ’†ğ’ğ’Šğ’•ğ’‰ â‹… ğ’. 

3.  PHYSICAL-BASED INTRINSIC IMAGE 
DECOMPOSITION IN OUTDOOR SCENARIO 

The workflow of our method is shown in Figure 4. The input of 
our method is a linearized RAW image with EXIF information 
(capturing  date and  time,  GPS  location,  etc.).  Firstly,  we  solve 
the  camera  intrinsic  and  extrinsic  parameters  with  a  general 
structure-from-motion (SfM) pipeline and reconstruct dense 3D 
points with a general multi-view stereo (MVS) pipeline from the 
image  collection.  Then,  we  project  geometric  attributes  (i.e., 
ğ›¼(cid:3046)(cid:3048)(cid:3041), ğ’)  back  to  images  and  guide  a  conditional  random  field 
(CRF)  to  propagate  ğ›¼(cid:3046)(cid:3048)(cid:3041)  by  images.  Next,  we  estimate 
parameters of illumination models (ğ‘³ğ’”ğ’–ğ’, ğ‘³ğ’”ğ’Œğ’š) and refine ğ›¼(cid:3046)(cid:3048)(cid:3041) 
simultaneously based on cast shadows. Finally, we assemble the 
total shading ğ‘º and decompose albedo with Equation 1. 

Figure 4.The workflow of our intrinsic image decomposition. 

ğ‘ºğ’”ğ’Œğ’š (cid:3404) ğ‘³ğ’”ğ’Œğ’š(cid:4666)0.5 (cid:3397) 0.5 ğ‘ğ‘œğ‘ (cid:4666)Ï‰(cid:3053)(cid:3032)(cid:3041)(cid:3036)(cid:3047)(cid:3035)(cid:4667)(cid:4667)  ,

(cid:4666)3(cid:4667) 

3.1  Estimate ğœ¶ğ’”ğ’–ğ’ from Geometry and Image 

where ğ‘³ğ’”ğ’Œğ’š âˆˆ â„(cid:2871) is irradiance of the sky hemisphere, Ï‰(cid:3053)(cid:3032)(cid:3041)(cid:3036)(cid:3047)(cid:3035) is 
the  angle  between  the  surface  normal  ğ’ âˆˆ â„(cid:2871)  and  zenith 
direction ğ›‰ğ’›ğ’†ğ’ğ’Šğ’•ğ’‰ âˆˆ â„(cid:2871), in most cases ğ›‰ğ’›ğ’†ğ’ğ’Šğ’•ğ’‰ (cid:3404) (cid:4666)0,0,1(cid:4667)(cid:3021). 

Given  the  geo-location and image  capturing  date  and  time,  the 
local  sun  position ğ›‰ğ’”ğ’–ğ’ can  be  calculated  with  an  astronomical 
algorithm (Meeus, 1991). We use a ray-tracing rendering system 
to initialize ğ›¼(cid:3046)(cid:3048)(cid:3041) by checking the ray visibility toward directional 
light with ğ›‰ğ’”ğ’–ğ’. At meantime, we project the surface normal ğ’ of 
the reconstructed geometry to images. 

(a) Our hemisphere integration approximation with the surface 
normal 

(b) Surface normal ğ’ 
ï¼ˆaï¼‰Sun visibility ğ›¼(cid:3046)(cid:3048)(cid:3041)  
Figure 5.Geometric features projection to images. 

(b) Accurate sky integral 

(c) Approximate sky integral 

Figure 3.  Our sky illumination model. 

To simplify this model, we assume no multi-path reflection in the 
aerial  case,  since  those  effects  are  rather  weak  in  outdoor 
scenarios whose lighting is dominated by the Sun (Section 2.2). 

(a) Projected ğ›¼(cid:3046)(cid:3048)(cid:3041)  

(b) CRF refined ğ›¼(cid:3046)(cid:3048)(cid:3041) 

Figure 6.  Propagate sun visibility with image content. 

 
 
 
 
 
 
 
 
 
      
 
 
 
 
 
 
 
 
    
 
 
 
Figure 5 shows sun visibility and surface normal projected to an 
image plane. Then, we take projected sun visibility as the initial 
result and refine it with Conditional Random Field (CRF) 
(KrÃ¤henbÃ¼hl and Koltun, 2011) guided by the image,  the 
comparison before and after the refinement is illustrated in 
Figure 6. 

and  Figure  8(b)).  To  be  noted  that  for  a  single  image,  the 
estimation  could  be  failed  if  the  shadow  boundary  is  too  few, 
however, the quantity of the ğ‘³ğ’”ğ’–ğ’/ ğ‘³ğ’”ğ’Œğ’š is a constant across the 
entire fixed illumination dataset. Also, the ratio doesnâ€™t effected 
by  exposure  parameter.  By  integrating  more  pairs  across  the 
entire dataset, our method could provide a robust estimation. 

3.2  Estimate ğ‘³ğ’”ğ’–ğ’, ğ‘³ğ’”ğ’Œğ’š with lit-shadow pairs 

Algorithm 1 Criteria-based filters 

In  this  section,  we  introduce  the  main  idea  of  estimating 
illumination  factors  from  observed images  based  on  our  image 
formation  model.  Estimating ğ‘³ğ’”ğ’–ğ’  and ğ‘³ğ’”ğ’Œğ’š  is  ill-posed  as  no 
absolute gauges of the illumination are known aside from pixel 
values.  Alternatively,  we  can  estimate 
the  relative  ratio 
ğ‘³ğ’”ğ’–ğ’/ ğ‘³ğ’”ğ’Œğ’š .  As  we  noticed  the  natural  that  pixels  across  the 
shadow boundary very likely share the same albedo, the changing 
of pixel values reflects the difference in illumination. Explicitly, 
a  fully  lighted  pixel ğ‘°ğ’ğ’Šğ’•  (i.e., ğ›¼(cid:3046)(cid:3048)(cid:3041) (cid:3404) 1)  and  the  paired  pixel 
ğ‘°ğ’”ğ’‰ğ’‚ğ’…ğ’ğ’˜ located in umbra (i.e., ğ›¼(cid:3046)(cid:3048)(cid:3041) (cid:3404) 0) are modeled with: 

(cid:4682)

ğ‘°ğ’ğ’Šğ’•          (cid:3404) ğ‘¹ âŠ— (cid:3435)ğ‘³ğ’”ğ’–ğ’ğ‘˜(cid:3046)(cid:3048)(cid:3041) (cid:3397) ğ‘³ğ’”ğ’Œğ’šğ‘˜(cid:3046)(cid:3038)(cid:3052)(cid:3439)
ğ‘°ğ’”ğ’‰ğ’‚ğ’…ğ’ğ’˜ (cid:3404) ğ‘¹ âŠ— (cid:3435)ğ‘³ğ’”ğ’Œğ’šğ‘˜(cid:3046)(cid:3038)(cid:3052)(cid:3439)                       

  .

(cid:4666)5(cid:4667) 

By  assuming  albedo ğ‘¹,  shading  coefficients ğ‘˜(cid:3046)(cid:3048)(cid:3041), ğ‘˜(cid:3046)(cid:3038)(cid:3052) are  the 
same, we can derive the ratio ğ‘³ğ’”ğ’–ğ’/ ğ‘³ğ’”ğ’Œğ’š as Equation 6. 

1. Remove overexposure and underexposure pixels. 
2. Angle of the two surface normals is less than 5Â°. 
3. Depth difference of the two points is less than a threshold. 
4. ğ‘˜(cid:3046)(cid:3038)(cid:3052)/ğ‘˜(cid:3046)(cid:3048)(cid:3041) âˆˆ (0.1, 10.0) to ensure numerical stability. 

(a) 

(b) 

ğ‘³ğ’”ğ’–ğ’
ğ‘³ğ’”ğ’Œğ’š

(cid:3404)

ğ‘°ğ’ğ’Šğ’• (cid:3398) ğ‘°ğ’”ğ’‰ğ’‚ğ’…ğ’ğ’˜
ğ‘°ğ’”ğ’‰ğ’‚ğ’…ğ’ğ’˜

â‹…

ğ‘˜(cid:3046)(cid:3038)(cid:3052)
ğ‘˜(cid:3046)(cid:3048)(cid:3041)

  .

(cid:4666)6(cid:4667) 

Figure 8. (a) Probability distribution estimated by GMM(n=2). 
(b) Pairs of the major component are inliers (green), and of the 
minor component are outliers (yellow). 

3.3  Optimize ğœ¶ğ’”ğ’–ğ’ with Tikhonov regularization 

At  this  stage,  all  necessary  quantities  of  Equation  4  have  been 
estimated. The albedo component (Figure 9(a)) is resolved with 
the inverse image model ğ‘¹ (cid:3404)  ğ‘° / ğ‘º. Nonetheless, the seamlines 
between  lighted  and  shadowed  regions  are  sharp  because  the 
CRF  framework  made  the  binary  assumption  CRF  (denoted  as 
(cid:2868)
), either shadow or non-shadow. In this section, we further 
Î±(cid:2929)(cid:2931)(cid:2924)
improved it to a continuous representation to match the nature of 
penumbra. Analysis of the profile shown in Figure 9(c) indicates 
the  source  of  spikes  in  albedo  layers.  To  address  the  spiky 
seamline,  we  build  a  1D  optimization  problem  with  Tikhonov 
regularization (Tikhonov, 1963). 

âˆ—
Î±(cid:3046)(cid:3048)(cid:3041)

(cid:4666)ğ‘¡(cid:4667) (cid:3404) ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›

1
2

â€–Î±(cid:3046)(cid:3048)(cid:3041)(cid:4666)ğ‘¡(cid:4667) (cid:3398) Î±(cid:3046)(cid:3048)(cid:3041)

(cid:2868)

(cid:4666)ğ‘¡(cid:4667)â€–(cid:3017)

(cid:2870) (cid:3397)

1
2

with  

(cid:3631)âˆ‡ (cid:3436)

1
ğ‘…(cid:4666)ğ‘¡(cid:4667)

(cid:2870)

, (cid:4666)7(cid:4667) 

(cid:3440)(cid:3631)

1
ğ‘…(cid:4666)ğ‘¡(cid:4667)

(cid:3404)

ğ¿(cid:3046)(cid:3048)(cid:3041)ğ‘˜(cid:3046)(cid:3048)(cid:3041)(cid:4666)ğ‘¡(cid:4667)
ğ¼(cid:4666)ğ‘¡(cid:4667)

Î±(cid:3046)(cid:3048)(cid:3041)(cid:4666)ğ‘¡(cid:4667) (cid:3397)

ğ¿(cid:3046)(cid:3038)(cid:3052)ğ‘˜(cid:3046)(cid:3038)(cid:3052)(cid:4666)ğ‘¡(cid:4667)
ğ¼(cid:4666)ğ‘¡(cid:4667)

  ,

(cid:4666)8(cid:4667)  

where â€– â‹… â€–ğ‘·   (cid:3404) Mahalanobis distance with weight matrix ğ‘· 
           âˆ‡(cid:4666)â‹…(cid:4667)   (cid:3404) Gradient operator 
           ğ‘¡  (cid:3404) distance along profile. 

In Equation 7, ğ‘¡ is the parameter of the 1D signal that represents 
distance along profile (Figure 9(c) and (d)). For the regularization 
term,  we choose to optimize ğŸ/ğ‘¹ since compared with directly 
optimizing regarding ğ‘¹, the ğŸ/ğ‘¹  yields a closed-form solution 
due to its linearity w.r.t. Î±(cid:3046)(cid:3048)(cid:3041)(cid:4666)ğ‘¡(cid:4667), as shown in Equation 8. Weight 
matrix ğ‘· is  a  diagonal  positive  definite  matrix.  We  prefer  to 
adjust  the ğ›¼(cid:3046)(cid:3048)(cid:3041)(cid:4666)ğ‘¡(cid:4667) that  close  to  shadow  boundary  but  keep  the 
two  far  ends  stationery  to  ensure  a  smooth  transition  to  the 
adjusted region. Figure 9(b) and (d) show the estimated albedo 
with optimized soft sun visibility Î±(cid:3046)(cid:3048)(cid:3041)

. 

âˆ—

(a) Shadow boundaries extraction 

(b) Lit-shadow pairs sampling 

(c) Criteria-based filtering. 

(d) Statistical filtering (Green: 
inliers; Yellow: outliers). 

Figure 7.  Find reliable lit-shadow pairs to estimate ğ‘³ğ’”ğ’–ğ’/ğ‘³ğ’”ğ’Œğ’š. 

To generate point pairs for estimation, we propose criteria-based 
filters  (Algorithm  1)  and  a  statistical.  Firstly,  given  the  sun 
visibility  mask  ğ›¼(cid:3046)(cid:3048)(cid:3041) ,  we  extract  the  boundaries  of  shadow 
regions (Figure 7(a)) and samples pairs in the lighted and umbra 
regions  (Figure  7(b)).  Secondly,  since  Equation  6  requires  the 
albedos and surface normals of the pair of points are the same, 
we adapt conditions in Algorithm 1 to eliminate pairs based on 
geometry and appearance properties Figure 7(c). Finally, we fit a 
2-component  Gaussian  Mixture  Model  (GMM)  for  the  ratio 
ğ‘³ğ’”ğ’–ğ’/ ğ‘³ğ’”ğ’Œğ’š computed from remaining  pairs as  shown in Figure 
8(a).  If  the  major  component  is  consists  of  95%  pairs,  and  the 
covariance  is  significantly  smaller  than  the  second  component, 
we accept the mean as the estimation of ğ‘³ğ’”ğ’–ğ’/ ğ‘³ğ’”ğ’Œğ’š (Figure 7(d) 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
âˆ—

Combining Î±(cid:3046)(cid:3048)(cid:3041)
 and ğ‘³ğ’”ğ’–ğ’/ğ‘³ğ’”ğ’Œğ’š, our ultimate solution is shown in 
Figure 10(a) and (b). Figure 10(c) shows the ground truth shading 
image computed from rendered image and diffuse albedo image. 

(a) Albedo with Î±(cid:3046)(cid:3048)(cid:3041)

(cid:2868)

(b) Albedo with Î±(cid:3046)(cid:3048)(cid:3041)

âˆ—

(c) Profiles of (a) 

(d) Profiles of (b) 

et  al.,  2020).  Since  ground  truth  is  absent  in  the  real-world 
dataset,  we  show  the  qualitative  comparison  in  Figure  12. 
Reference methods are both based on deep neural networks and 
pre-trained models are provided. As indicated by the red circles 
in Figure 12, InverseRenderNet v2 failed to recover albedo under 
the shaded region. SMGAN performs better in shadow removal, 
however, only large shadows are removed but missed shadows of 
small  objects.  SMGAN  yields  inconsistent  albedo  in  the  same 
image  as  indicated  by  yellow  boxes.  Furthermore,  we  can 
compare results at image collection level. Due to neural networks 
only  taking  a  single  image  as  input,  the  consistency  between 
images is not guaranteed. In our method, the ratio ğ‘³ğ’”ğ’–ğ’/ ğ‘³ğ’”ğ’Œğ’š is 
a fixed value in an image collection acquired during a short time, 
and  the  ratio  is  irrelevant  to  exposure  settings.  We  estimate 
ğ‘³ğ’”ğ’–ğ’/ ğ‘³ğ’”ğ’Œğ’š with lit-shadow pairs from image collection but not a 
single image, which enables image collection-wide consistency 
and robustness (e.g., no reliable shadow boundary can be found 
from some images). In our results, there are a few flaws can be 
recognized  around  the  edges  of  the  roof,  it  is  because  of  the 
inaccurate  normal  computed  from  the  reconstructed  surface 
model. 

Figure 9. (a) and (c) are the decomposed albedo with binary 
(cid:2868)
Î±(cid:3046)(cid:3048)(cid:3041)
decomposed albedo with refined Î±(cid:3046)(cid:3048)(cid:3041)
seamless transition. 

 from CRF that yields spikes in profile; (b) and (d) are the 
âˆ—

 that yields a smooth 

(a) Our estimated 
albedo image 

(b) Our estimated 
shading image  
Figure 10.  Our albedo and shading estimation. 

(c) Ground truth 
shading image 

4.  EXPERIMENTS 

We collect 350 images in both DNG and JPEG formats with DJI 
Phantom  Pro  4  v2.0  for  evaluation.  Pixels  of  DNG  images  are 
16bit  depth  and  in  linear  color  space.  GPS  metadata  was 
extracted  from  corresponding  JPEG  files.  Then  we  process 
images  with  Agisoft  Metashape  Professional  1.8.0  (Agisoft, 
2022a)  yielding  image  intrinsic  &  extrinsic  parameters,  sparse 
and dense point clouds with normal, as well as a surface model 
with textures as shown in Figure 11. In the following sections, 
we  evaluate  our  method  from  4  perspectives:  intrinsic  image 
decomposition, 
feature  matching,  and  dense 
matching. 

relighting, 

Figure 11.The real-world dataset contains 350 images. 
GSD: 7mm/px, Area: 844 m2, Mission duration: 46min. 

4.1  Albedo Recovering 

We compare our results with those of the state-of-the-art albedo 
decomposition  methods  on  the  real-world  dataset,  including 
InverseRenderNet v2 (Yu and Smith, 2021) and SMGAN (Cun 

(a) 

(b) 

(c) 

(d) 

Figure 12.Qualitative comparison of albedo decomposition. (a) 
Raw image, (b) InverseRenderNet v2, (c) SMGAN, (d) Ours. 

4.2  Relighting 

We evaluate the relighting capability by rendering images from 
novel  views  and  simulated  sky  with  Blender.  We  use  Agisoft 
Metashape Professional to create textures from oriented images. 
The  textured  model  with  original  images  (standard  pipeline)  is 
shown  in  Figure  13(a)  and  (b).  Note  that,  the  left  column  of 
Figure 13 is rendered with uniform lighting and the right column 
is rendered with a different sun position than the collection time. 
Ideally,  under  uniform  lighting,  no  shadow  or  shading  effect 
should be observed. And with a given sun position, the shadow 
azimuth and shading should be coherent with physical law. The 
reference  model  (Figure  13  (c)  and  (d))  is  created  by  Agisoft 
Texture  De-Lighter  (Agisoft,  2022b),  which  is  a  close-sourced 
free software developed by Agisoft LLC. It requires sparse user 
strokes to indicate light and shadow regions on a textured model 
to  drive  the  algorithm.  Our  textures  are  created  by  Agisoft 
Metashape  Professional  but  use  our  albedo  images  instead,  as 
shown in Figure 13 (e) and (f). 

The  original  model  (Figure  13(a))  preserves  all  shadows  and 
illumination changes of Lambertâ€™s cosine law (Pharr et al., 2016). 
De-Lighter is managed to remove the shadow of the building and 
a large portion of shading, but the software assigned the incorrect 
color  to  the  object  indicated  by  the  yellow  boxes.  And  missed 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
shadows also created artifacts in Figure 13(d). Our model shows 
better relighting since there are no artifacts of shadow at all. From 
the  perspective  of  color  fidelity,  our  relighted  model  shows 
visually  better  results  than  the  reference  model.  This  is  further 
validated from a closer view as shown in Figure 14. 

functions  of  OpenCV.  The  results  show  that  with  our  albedo 
images,  the  extracted  and  matched  features  are  much  better 
distributed over the image. 

(a) Metashape Default 

(b) Metashape Default 

(c) De-Lighter 

(d) De-Lighter 

(e) Ours 

(f) Ours 

Figure 13.  Uniform lighting (left column) and sun-sky 
relighting (right column). From top to bottom: (a) and (b) 
standard pipeline; (c) and (d) De-Lighter; (e) and (f) ours. 

Figure 15. Feature matching with epipolar constraint. (Top: 
with original images, bottom: our albedo images) 

We then evaluated the performance on the entire image dataset 
by  using  feature  matching  and  bundle  adjustment  functions  of 
Agisoft Metashape Professional and list metrics of optimization 
in Table 1. It shows that in all metrics, the albedo images incur 
better accuracy. 

Metric 
Matching Inlier Rate 
RMS Reprojection Error 
Max Reprojection Error 

Original 
77.45 % 
1.45 pixels 
125.94 pixels 

Ours 
80.98 % 
1.26 pixels 
81.13 pixels 

Table 1.  Metric of bundle adjustment. 

4.4  Dense Image Matching 

In this experiment, we compared the dense matching quality of 
original images and our images. Figure 16(c) shows point clouds 
of  pairs  with  original  images  Figure  16(a)  and  albedo  images 
Figure 16(b). It is obvious that our albedo image pair yields more 
details in the shadow region than the original. Figure 17 presents 
another experiment in which we manually increase the brightness 
of  original  images,  to  verify  that  the  gain  of  details  is  not 
attributed to brightness changing.  

(a) De-Lighter 

(b) Ours 
Figure 14.Color consistency comparison. 

4.3  Feature Matching 

(a) 

(b) 

the 

following 

two  experiments,  we  discussed 

For 
the 
contributions of our albedo images to a typical photogrammetric 
data processing. Since our albedo images are illumination-free, 
we expect that less illumination variation should result in better 
feature matching with the current pipeline. Figure 15 shows the 
feature  matching  test  with  epipolar  constraint  to  remove  the 
anomaly,  with  the  SIFT  feature  extraction  and  matching 

(c) 
Figure 16. Comparison of dense matching point clouds of 
origin images (a) with our albedo images (b). Left of (c) is 
generated from original images, right is generated from ours. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
doi.org/10.1109/CVPR.2012.6247693. 

Barron, J.T., Malik, J., 2011. High-frequency shape and albedo 
from shading using natural image statistics. Proc. IEEE Comput. 
Soc. 
Recognit. 
doi.org/10.1109/CVPR.2011.5995392. 

Comput. 

Pattern 

Conf. 

Vis. 

Figure 17.  Point clouds of original (left) and manually edited 
images (right). 

Barrow  H.G.,  Tenenbaum,  J.M.,  1978.  Recovering  Intrinsic 
Scene  Characteristics  from  Images,  in:  Computer  Vision 
Systems. 

5.  CONCLUSION 

We  present  a  novel  outdoor  aerial  image  formation  model  and 
derived albedo recovering method based on the proposed model, 
and  demonstrate  with  various  photogrammetry  applications 
including relighting, feature matching, and dense matching. 

Similar  to  most  multi-view  intrinsic  image  decomposition,  our 
approach requires RAW images with linear color space to ensure 
the correctness of radiometric equations. That means our method 
cannot directly apply to existing datasets taken in JPEG format. 
Compared with other multi-view intrinsic image decomposition 
methods,  the  proposed  method  overcomes  the  difficulty  of  the 
lack of direct observation of the environmental radiance, instead, 
our  approach  can  estimate  the  lighting  condition  from  images. 
Thus  the  proposed  method  does  not  require  additional  user 
assistance  or  extra  data  collection,  and  our  method  works  on 
single temporal datasets.  

Regarding albedo recovering, compared with the state-of-the-art 
data-driven  methods,  our  approach  demonstrates  outstanding 
performance  on  correctness  and  multi-view  consistency.  The 
relighting  capacity  also  exceeds  the  commercial  software.  We 
also  investigate  the  possibility  of  improving  geometric  quality 
with our albedo images. It turns out albedo images could refine 
feature matching and dense matching to improve the geometry. 

ACKNOWLEDGEMENT 

This work is supported by the Office of Naval Research (Award 
No. N000141712928). 

REFERENCES 

Agisoft, 2022a. Metashape. Agisoft. 

Agisoft, 2022b. Texture De-Lighter. Agisoft. 

Alidoost,  F.,  Arefi,  H.,  2017.  Comparison  Of  UAS-based 
Photogrammetry  Software  For  3D  Point  Cloud  Generation:  A 
Survey Over A Historical Site. ISPRS Ann. Photogramm. Remote 
Sens. Spat. Inf. Sci., 4W4, 55â€“61. doi.org/10.5194/isprs-annals-
IV-4-W4-55-2017. 

Barron, J.T., Malik, J., 2015. Shape, illumination, and reflectance 
from  shading.  IEEE  Trans.  Pattern  Anal.  Mach.  Intell.,  37(8). 
doi.org/10.1109/TPAMI.2014.2377712. 

Barron, J.T., Malik, J., 2012a. Color constancy, intrinsic images, 
and  shape  estimation.  Lect.  Notes  Comput.  Sci.  (including 
Subser.  Lect.  Notes  Artif.  Intell.  Lect.  Notes  Bioinformatics), 
PART 4. doi.org/10.1007/978-3-642-33765-9_5. 

Barron,  J.T.,  Malik,  J.,  2012b.  Shape,  albedo,  and  illumination 
from a single image of an unknown object. Proc. IEEE Comput. 
Recognit. 
Soc. 

Comput. 

Pattern 

Conf. 

Vis. 

Bousseau,  A.,  Durand,  F.,  Durand,  F.,  Paris,  S.,  2009.  User-
Assisted 
Images.  ACM  Trans.  Graph.,  28(5). 
doi.org/10.1145/1618452.1618476. 

Intrinsic 

Cun, X., Pun, C.M., Shi, C., 2020. Towards ghost-free shadow 
removal  via dual hierarchical aggregation network and shadow 
matting GAN. AAAI 2020 - 34th AAAI Conf. Artif. Intell., 10680â€“
10687. doi.org/10.1609/aaai.v34i07.6695. 

Donne,  S.,  Geiger,  A.,  2019.  Learning  non-volumetric  depth 
fusion using successive reprojections. Proc. IEEE Comput. Soc. 
Conf.  Comput.  Vis.  Pattern  Recognit., 
7626â€“7635. 
doi.org/10.1109/CVPR.2019.00782. 

DuchÃªne,  S.,  2015.  Multi  view  delighting  and  relighting. 
UniversitÃ© Nice Sophia Antipolis. 

DuchÃªne, S., Riant, C., Chaurasia, G., Lopez Moreno, J., Laffont, 
P.Y.,  Popov,  S.,  Bousseau,  A.,  Inria,  G.D.,  2015.  Multiview 
intrinsic  images  of  outdoors  scenes  with  an  application  to 
relighting. ACM Trans. Graph., 34(5). doi.org/10.1145/2756549. 

Finlayson, G.D., Drew, M.S., Lu, C., 2004. Intrinsic images by 
entropy  minimization.  Lect.  Notes  Comput.  Sci.  (including 
Subser.  Lect.  Notes  Artif.  Intell.  Lect.  Notes  Bioinformatics), 
3023, 582â€“595. doi.org/10.1007/978-3-540-24672-5_46. 

Gehler, P.V., Rother, C., Kiefel,  M., Zhang, L., SchÃ¶lkopf,  B., 
2011. Recovering intrinsic images with a global sparsity prior on 
reflectance. Adv. Neural Inf. Process. Syst. 24 25th Annu. Conf. 
Neural Inf. Process. Syst. 2011, NIPS 2011. 

Guo,  R.,  Dai,  Q.,  Hoiem,  D.,  2013.  Paired  regions  for  shadow 
detection and removal. IEEE Trans. Pattern Anal. Mach. Intell., 
35(12), 2956â€“2967. doi.org/10.1109/TPAMI.2012.214. 

Haest, B., Biesemans, J., Horsten, W., Everaerts, J., Van Camp, 
N.,  Van  Valckenborgh,  J.,  2009.  Radiometric  calibration  of 
digital  photogrammetric  camera 
image  data.  Am.  Soc. 
Photogramm. Remote Sens. Annu. Conf. 2009, ASPRS 2009. 

Honkavaara, E., Arbiol, R., Markelin, L., MartÃ­nez, L., Bovet, S., 
Bredif, M., Chandelier, L., Heikkinen, V., Korpela, I., Lelegard, 
L.,  PÃ©rez,  F.,  SchlÃ¤pfer,  D.,  Tokola,  T.,  2012.  The  EuroSDR 
Project  â€œRadiometric  Aspects  Of  Digital  Photogrammetric 
Imagesâ€  â€“  Results  Of  The  Empirical  Phase.  Int.  Arch. 
Photogramm.  Remote  Sens.  Spat.  Inf.  Sci.,  XXXVIII-4/,  123â€“
130. doi.org/10.5194/isprsarchives-xxxviii-4-w19-123-2011. 

Honkavaara,  E.,  Markelin,  L.,  Arbiol,  R.,  MartÃ­nez,  L.,  2013. 
Commission 1 â€œRadiometric aspects of digital photogrammetric 
images.â€ Off. Publ. - EuroSDR, (62). 

Innamorati,  C.,  Ritschel,  T.,  Weyrich,  T.,  Mitra,  N.J.,  2017. 

 
 
 
 
 
 
 
Decomposing  Single  Images  for  Layered  Photo  Retouching. 
Comput. Graph. Forum, 36(4). doi.org/10.1111/cgf.13220. 

Innmann,  M.,  Susmuth,  J.,  Stamminger,  M.,  2020.  BRDF-
reconstruction  in  photogrammetry  studio  setups.  Proc.  -  2020 
IEEE  Winter  Conf.  Appl.  Comput.  Vision,  WACV  2020,  3346â€“
3354. doi.org/10.1109/WACV45572.2020.9093320. 

Janner, M., Wu, J., Kulkarni, T.D., Yildirim, I., Tenenbaum, J.B., 
2017.  Self-supervised  intrinsic  image  decomposition.  Adv. 
Neural Inf. Process. Syst., 5937â€“5947. 

KrÃ¤henbÃ¼hl,  P.,  Koltun,  V.,  2011.  Efficient  inference  in  fully 
connected  crfs  with  gaussian  edge  potentials.  Adv.  Neural  Inf. 
Process. Syst., 24, 109â€“117. 

Lachambre, S., 2018. The Photogrammetry Workflow. Unity, 1. 

Laffont, P.Y., Bousseau, A., Drettakis, G., 2013. Rich intrinsic 
image  decomposition  of  outdoor  scenes  from  multiple  views. 
IEEE 
19(2). 
doi.org/10.1109/TVCG.2012.112. 

Comput. 

Graph., 

Trans. 

Vis. 

Land, E.H., McCann, J.J., 1971. Lightness and retinex theory. J. 
Opt. Soc. Am., 61(1). doi.org/10.1364/JOSA.61.000001. 

Li,  Z.,  Snavely,  N.,  2018.  CGIntrinsics:  Better  Intrinsic  Image 
Decomposition  Through  Physically-Based  Rendering.  Lect. 
Notes  Comput.  Sci.  (including  Subser.  Lect.  Notes  Artif.  Intell. 
Lect.  Notes Bioinformatics).  doi.org/10.1007/978-3-030-01219-
9_23. 

Luo,  S.,  Li,  H.,  Shen,  H.,  2018.  Shadow  removal  based  on 
clustering correction of illumination field for urban aerial remote 
sensing images. Proc. - Int. Conf. Image Process. ICIP, 485â€“489. 
doi.org/10.1109/ICIP.2017.8296328. 

Meeus, J., 1991. Astronomical algorithms. Richmond. 

Mostafa, Y., 2017. A Review on Various Shadow Detection and 
Compensation  Techniques  in  Remote  Sensing  Images.  Can.  J. 
Remote Sens. doi.org/10.1080/07038992.2017.1384310. 

Oâ€™Hara,  R.,  Barnes,  D.,  2012.  A  new  shape  from  shading 
technique  with  application  to  Mars  Express  HRSC  images. 
67(1). 
ISPRS 
doi.org/10.1016/j.isprsjprs.2011.07.004. 

Photogramm. 

Remote 

Sens., 

J. 

Qu,  L.,  Tian,  J.,  He,  S.,  Tang,  Y.,  Lau,  R.W.H.,  2017. 
DeshadowNet:  A  multi-context  embedding  deep  network  for 
shadow removal. Proc. - 30th IEEE Conf. Comput. Vis. Pattern 
Recognition, 
2308â€“2316. 
doi.org/10.1109/CVPR.2017.248. 

CVPR 

2017, 

Sasi, R.K., Govindan, V.K., 2015. Shadow detection and removal 
from real imagesâ€‰: State of art. ACM Int. Conf. Proceeding Ser. 
doi.org/10.1145/2791405.2791450. 

Shen, J., Yang, X., Jia, Y., Li, X., 2011. Intrinsic images using 
optimization.  Proc.  IEEE  Comput.  Soc.  Conf.  Comput.  Vis. 
Pattern Recognit. doi.org/10.1109/CVPR.2011.5995507. 

Sheng, B., Li, P., Jin, Y., Tan, P., Lee, T.Y., 2020. Intrinsic Image 
Decomposition  with  Step  and  Drift  Shading  Separation.  IEEE 
26(2). 
Trans. 
doi.org/10.1109/TVCG.2018.2869326. 

Comput. 

Graph., 

Vis. 

Silva,  G.F.,  Carneiro,  G.B.,  Doth,  R.,  Amaral,  L.A.,  Azevedo, 
D.F.G. d., 2018. Near real-time shadow detection and removal in 
aerial  motion  imagery  application.  ISPRS  J.  Photogramm. 
Remote 
104â€“121. 
doi.org/10.1016/j.isprsjprs.2017.11.005. 

Sens., 

140, 

Tikhonov,  A.N.,  1963.  Regularization  of  incorrectly  posed 
problems. Sov. Math. Dokl., 4. 

Wang, J., Li, X., Yang, J., 2018. Stacked Conditional Generative 
Adversarial  Networks  for  Jointly  Learning  Shadow  Detection 
and Shadow Removal. Proc. IEEE Comput. Soc. Conf. Comput. 
1788â€“1797. 
Pattern 
Vis. 
doi.org/10.1109/CVPR.2018.00192. 

Recognit., 

Wang, Q., Yan, L., Yuan, Q., Ma, Z., 2017. An automatic shadow 
detection method for VHR remote sensing orthoimagery. Remote 
Sens., 9(5). doi.org/10.3390/rs9050469. 

Wang, Z., Schaaf, C.B., Sun, Q., Shuai, Y., RomÃ¡n, M.O., 2018. 
Capturing  rapid  land  surface  dynamics  with  Collection  V006 
MODIS BRDF/NBAR/Albedo (MCD43) products. Remote Sens. 
Environ., 207. doi.org/10.1016/j.rse.2018.02.001. 

Wu,  J.,  Bauer,  M.E.,  2013.  Evaluating  the  effects  of  shadow 
detection 
and 
spectroradiometric  restoration.  Remote  Sens.,  5(9),  4450â€“4469. 
doi.org/10.3390/rs5094450. 

classification 

quickbird 

image 

on 

Pharr,  M.,  Jakob,  W.,  Humphreys,  G.,  2016.  Physically  based 
rendering:  From  theory  to  implementation:  Third  edition, 
Physically  Based  Rendering:  From  Theory  to  Implementation: 
Third Edition. 

Pisek, J., Chen, J.M., Kobayashi, H., Rautiainen, M., Schaepman, 
M.E.,  Karnieli,  A.,  Sprinstin,  M.,  Ryu,  Y.,  Nikopensius,  M., 
Raabe,  K.,  2016.  Retrieval  of  seasonal  dynamics  of  forest 
understory  reflectance  from  semiarid  to  boreal  forests  using 
MODIS  BRDF  data.  J.  Geophys.  Res.  Biogeosciences,  121(3). 
doi.org/10.1002/2016JG003322. 

Pisek,  J.,  Rautiainen,  M.,  Nikopensius,  M.,  Raabe,  K.,  2015. 
Estimation of seasonal dynamics of understory NDVI in northern 
forests  using  MODIS  BRDF  data:  Semi-empirical  versus 
physically-based  approach.  Remote  Sens.  Environ.,  163. 
doi.org/10.1016/j.rse.2015.03.003. 

Wu, T.P., Tang, C.K., Brown, M.S., Shum, H.Y., 2007. Natural 
shadow 
26(2). 
doi.org/10.1145/1243980.1243982. 

matting. 

Graph., 

Trans. 

ACM 

Yu, Y., Smith, W., 2021. Outdoor inverse rendering from a single 
image  using  multiview  self-supervision.  IEEE  Trans.  Pattern 
Anal. Mach. Intell. doi.org/10.1109/TPAMI.2021.3058105. 

Yu, Y., Smith, W.A.P., 2019. Inverserendernet: Learning single 
image  inverse  rendering.  Proc.  IEEE  Comput.  Soc.  Conf. 
Comput. 
Recognit. 
doi.org/10.1109/CVPR.2019.00327. 

Pattern 

Vis. 

 
 
