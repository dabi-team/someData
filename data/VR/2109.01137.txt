The Power of Points for Modeling Humans in Clothing

Qianli Ma1,2

Jinlong Yang1

Siyu Tang2 Michael J. Black1

1Max Planck Institute for Intelligent Systems, T¨ubingen, Germany

2ETH Z¨urich

{qma,jyang,black}@tuebingen.mpg.de, {qianli.ma, siyu.tang}@inf.ethz.ch

Unseen scan

Fitted POP (point cloud)

Animations with pose-dependent clothing deformation

Figure 1: The Power of Points (POP) model for clothed humans. Based on a novel articulated dense point cloud represen-
tation, our cross-outfit model, named POP, produces pose-dependent shapes of clothed humans with coherent global shape
and expressive local garment details. The trained model can be fitted to a single scan of an unseen subject wearing an unseen
outfit, and can animate it with realistic pose-dependent clothing deformations. The results are color-coded with predicted
point normals and rendered with a simple surfel-based renderer.

Abstract

Currently it requires an artist

to create 3D human
avatars with realistic clothing that can move naturally. De-
spite progress on 3D scanning and modeling of human bod-
ies, there is still no technology that can easily turn a static
scan into an animatable avatar. Automating the creation of
such avatars would enable many applications in games, so-
cial networking, animation, and AR/VR to name a few. The
key problem is one of representation. Standard 3D meshes
are widely used in modeling the minimally-clothed body
but do not readily capture the complex topology of cloth-
ing. Recent interest has shifted to implicit surface mod-
els for this task but they are computationally heavy and
lack compatibility with existing 3D tools. What is needed
is a 3D representation that can capture varied topology at
high resolution and that can be learned from data. We ar-
gue that this representation has been with us all along —
the point cloud. Point clouds have properties of both im-
plicit and explicit representations that we exploit to model
3D garment geometry on a human body. We train a neu-
ral network with a novel local clothing geometric feature
to represent the shape of different outfits. The network is
trained from 3D point clouds of many types of clothing,

on many bodies, in many poses, and learns to model pose-
dependent clothing deformations. The geometry feature can
be optimized to fit a previously unseen scan of a person
in clothing, enabling the scan to be reposed realistically.
Our model demonstrates superior quantitative and qualita-
tive results in both multi-outfit modeling and unseen outfit
animation. The code is available for research purposes at
https://qianlim.github.io/POP.

1. Introduction

Animatable clothed human avatars are required in many
applications for 3D content generation. To create avatars
with naturally-deforming clothing, existing solutions either
involve heavy artist work or require 4D scans for train-
ing machine-learning models [65]. These solutions are ex-
pensive and often impractical. Instead, can we turn a sin-
gle static 3D scan — which can be acquired at low cost
today even with hand-held devices — into an animatable
avatar? Currently, no existing technology is able to do this
and produce realistic clothing deformations. Given a static
scan, traditional automatic rigging-and-skinning methods
[3, 20, 38] can be used to animate it, but are unable to
produce pose-dependent clothing deformations. Physics-

1

based simulations can produce realistic deformations, but
require “reverse-engineering” a simulation-ready clothing
mesh from the given scan. This involves expert knowledge
and is not fully automatic.

Taking a data-driven approach, the goal would be to learn
a model that can produce reasonable pose-dependent cloth-
ing deformation across different outfit types and styles and
can generalize to unseen outfits. However, despite the re-
cent progress in modeling clothed human body shape de-
formations [25, 35, 41, 42, 51, 65], most existing models
are outfit-specific and thus cannot generalize to unseen out-
fits. To date, no such cross-garment model exists, due to
several technical challenges.

The first challenge lies in the choice of 3D shape repre-
sentation. To handle outfits of different types and styles at
once, the shape representation must handle changing topol-
ogy, capture high-frequency details, be fast at inference
time, and be easy to render. Classical triangle meshes ex-
cel at rendering efficiency but are fundamentally limited by
their fixed topology. The implicit surface representation
is topologically flexible, but is in general computationally
heavy and lacks compatibility with existing graphics tools.
Because point clouds are an explicit representation, they are
efficient to render, but they can also be viewed as implic-
itly representing a surface. Thus they are flexible in topol-
ogy and, as the resolution of the point cloud increases, they
can capture geometric details. While point clouds are not
commonly applied to representing clothing, they are widely
used to represent rigid objects and many methods exist to
process them efficiently with neural networks [1, 19, 36]. In
this work, we show that the seemingly old-fashioned point
cloud is, in fact, a powerful representation for modeling
clothed humans.

In recent work, SCALE [41] demonstrates that a point
cloud, grouped into local patches, can be exploited to repre-
sent clothed humans with various clothing styles, includ-
ing those with thin structures and open surfaces. How-
the patch-based formulation in SCALE often suf-
ever,
fers from artifacts such as gaps between patches.
In this
work, we propose a new shape representation of dense point
clouds. For simplicity, we avoid using patches, which have
been widely used in recent point cloud shape representa-
tions [4, 17, 18, 24, 41], and show that patches are not nec-
essary. Instead, we introduce smooth local point features on
a 2D manifold that regularize the points and enable arbitrar-
ily dense up-sampling during inference.

Another challenging aspect of cross-outfit modeling con-
cerns how outfits of different types and styles can be
encoded in a single, unified, model.
In most existing
outfit-specific models, the model parameters (typically the
weights of a trained shape decoder network) need to repre-
sent both the intrinsic, pose-independent shape of a clothed
person, and how this shape deforms as a function of the in-

put pose. To factor the problem, we propose to isolate the
intrinsic shape from the shape decoder by explicitly condi-
tioning it with a geometric feature tensor. The geometric
feature tensor is learned in an auto-decoding fashion [50],
with a constraint that a consistent intrinsic shape is shared
across all examples of the same outfit. Consequently, the
shape decoder can focus on modeling the pose-dependent
effects and can leverage common deformation properties
across outfits. At inference time, the geometric feature ten-
sor can be optimized to fit to a scan of a clothed body with
a previously unseen outfit, making it possible for the shape
decoder to predict pose-dependent deformation of it based
on the learned clothing deformation properties.

These ideas lead to POP: our dense point cloud model
that produces pose-dependent clothing geometry across dif-
ferent outfits and demonstrating the Power of Points for
modeling shapes of humans in clothing. POP is evaluated
on both captured and synthetic datasets, showing state-of-
the-art performance on clothing modeling and generaliza-
tion to unseen outfits.

In summary, our contributions are: (1) a novel dense
point cloud shape representation with fine-grained local fea-
tures that produces state-of-the-art detailed clothing shapes
with various clothing styles; (2) a novel geometry feature
tensor that enables cross-garment modeling and general-
ization to unseen outfits; (3) an application of animating
a static scan with reasonable pose-dependent deformations.
The model and code are available for research purposes at
https://qianlim.github.io/POP.

2. Related Work

Shape Representations for Clothed Human Modeling.
Surface meshes are an efficient 3D representation that is
compatible with graphics pipelines, and thus are the dom-
inant choice for modeling clothing and clothed humans.
With meshes, the clothing is represented either by deform-
ing an unclothed body template [6, 8, 42, 47, 71, 78], or
using a separately defined template [25, 26, 35, 51, 66].
While recent work successfully produces detailed geome-
try with graph convolutions [42], multi-layer perceptrons
(MLPs) [8, 51], and UV map convolutions [31, 35], meshes
suffer from two fundamental limitations: the fixed topology
and the requirement for template registration. This restricts
their generalization to outfit styles beyond the pre-defined
templates, and makes it difficult to obtain a common rep-
resentation for various clothing categories. Although re-
cent work proposes adaptable templates [49, 82] that mod-
ify mesh connectivity, the need for registering training data
to the mesh template remains challenging when a complex
garment geometry is involved.

Neural implicit surfaces [12, 44, 50], on the other hand,
do not require any pre-defined template, are flexible with
surface topology, and have recently become a promising

2

choice for reconstructing [7, 27, 28, 56, 63, 64, 79, 80] and
modeling [10, 13, 16, 45, 48, 65] shapes of 3D humans. De-
spite their ability to handle varied clothing topology, it re-
mains an open challenge to realistically represent thin struc-
tures that are often present in daily clothing. Moreover, re-
constructing an explicit surface from the implicit function
costs cubic time with respect to the resolution, which re-
stricts them from many practical applications.

Point clouds are another classic 3D representation that
supports arbitrary topology as well as thin structures. Go-
ing beyond prior work that generates a sparse point set [1,
19, 36], recent approaches [4, 17, 18, 24] use deep learn-
ing to generate structured point clouds with a set of surface
patches. Leveraging the patch regularization, SCALE [41]
proposes an articulated dense point cloud representation
to model clothing deformation. However, the patch-based
point clouds often suffer from overlap [24] or separa-
tion [41] between the patches, which degrades the geomet-
ric fidelity and visual quality. Our model, POP, follows the
spirit of these approaches to generate a dense, structured
point cloud, but, in contrast to prior work, we deprecate the
concept of patches and, instead, decode a dense point cloud
from fine-grained local features. The resulting clothed hu-
man model shows high geometry fidelity, is robust to topol-
ogy variations, and generalizes to various outfit styles.

Modeling Outfit Shape Space. We categorize existing
models for clothing or clothed bodies into three levels of in-
creasing generalization capacity, as summarized in Tab. 1.
Note that non-parametric models for clothed human recon-
struction [63, 64, 80] are out of the scope for this analysis.
Outfit-Specific. Methods from this class need to train
a separate model for every outfit instance [14, 26, 35,
41, 47, 65, 66, 77] or category (e.g. all short-sleeve T-
shirts) [25, 51, 71, 76]. For mesh-based methods in this
category [14, 25, 35, 47, 51, 71, 77], this characteristic
stems from the need to manually define a mesh template:
the fixed mesh topology fundamentally prohibits general-
ization to a different outfit category (e.g. from pants to a
skirt). These methods can, however, deal with size [25, 71]
or style [51, 76] within the template-defined category.
Template-free methods [41, 65] require training a separate
model for each outfit, i.e. the intrinsic, pose-independent
shape information is stored in the model parameters; hence,
the test-time generalization to an unseen outfit is restricted.
Multi-Outfit. Combining multiple pre-defined mesh tem-
plates with a multi-head network or a garment classifier, the
MGN model [6], BCNet [29], and DeepFashion3D [82] can
reconstruct humans in a variety of clothing from images. In
a similar spirit, CAPE [42] uses a pre-labeled one-hot vector
for outfit-type conditioning and can generate new clothing
from four common categories with a single model. While
training a single model for multiple outfits exploits the com-
plementary information among training data, these methods

Table 1: Data-driven models for clothing / clothed humans
classified by garment space generalization.

Outfit-Specific

De Aguiar [14], DRAPE [25], GarNet [26],
DeepWrinkles [35], SCALE [41],
Neophytou [47], TailorNet [51],
SCANimate [65], Santesteban [66],
Sizer [71], Wang [76], Yang [77].

Multi-Outfit

Arbitrary Outfit

MGN [6], BCNet [29], CAPE [42],
Vidaurre [73], DeepFashion3D [82].
SMPLicit [13], Shen [68], POP (Ours).

do not show the ability to handle unseen garments beyond
the pre-defined categories.

Arbitrary Outfit. To overcome the limitations brought
by the fixed topology of meshes, recent work opts for other
representations that can unify different clothing categories
and types. Shen et al. [68] represent garments using 2D
sewing pattern images that are applicable to arbitrary cloth-
ing categories. However, the final 3D garment shape is
represented with a single manifold mesh that is not suffi-
ciently expressive to represent the complexity and variety
of real-world clothing. Using neural implicit surfaces, SM-
PLicit [13] learns a topology-aware generative model for
garments across multiple categories and shows continuous
interpolation between them. However, the clothing geom-
In contrast, our
etry tends to be bulky and lacks details.
POP model faithfully produces geometric details of various
outfits, can generalize to unseen outfits, and demonstrates
state-of-the-art performance on garment space modeling.

3. Method

Our goal is to learn a single, unified, model of high-
fidelity pose-dependent clothing deformation on human
bodies across multiple outfits and subjects. We first in-
troduce an expressive point-based representation that pre-
serves geometric details and flexibly models varied topol-
ogy (Sec. 3.1). Using this, we build a cross-outfit model
enabled by a novel geometric feature tensor (Sec. 3.2).

As illustrated in Fig. 2, given an unclothed body, the
model outputs the 3D clothed body by predicting a displace-
ment field from the body surface based on local pose and ge-
ometric features. The trained model can be fitted to a scan
of a person in previously unseen outfits and this scan can be
animated with pose-dependent deformations (Sec. 3.3).

3.1. Representing Humans with Point Clouds

Point-based representations [23, 41] possess topological
flexibility and fast inference speed, giving them an advan-
tage over meshes or implicit functions for modeling articu-
lated humans. In this work, we formulate a structured point
cloud representation for modeling 3D clothed humans by
learning a mapping from a 2D manifold to a 3D deforma-

3

Figure 2: Overview of POP. Given a posed but unclothed body model (visualized as a blue 2D contour), we record the 3D positions pi
of its surface points on a UV positional map I, and encode this into a pose feature tensor P . A garment geometry feature tensor G is
an optimizable variable that is pixel-aligned with P , and learned per-outfit in an auto-decoder manner. The 2D image-plane coordinate
ui describes the relative location of the points on the body surface manifold. The shape decoder queries these locations and predicts
displacement vectors ri based on the points’ local pose and geometry features.

tion field, in a similar form to AtlasNet [24]:

ri = fw(ui; zi) : R2 × RZ → R3,

(1)

where ri is a displacement vector, fw(·) is a multi-layer per-
ceptron (MLP) with weights w, ui is a 2D parameterization
of a point i that denotes its relative location on the body sur-
face, and zi is the point’s local feature code containing the
shape information.

We deviate from other recent point-based human repre-
sentations in two key ways: 1) We use fine-grained per-
point local features zi as opposed to a single global fea-
ture [23, 24] or per-patch features [41] in prior work. 2) We
predict the clothing deformation field on top of the canoni-
cally posed body, instead of absolute Euclidean point coor-
dinates [23] or local patch deformations [41]. Both design
choices lead to significant improvements in representation
power, as detailed below.
Continuous Local Point Features. Recent work shows the
advantage of using a local latent shape code over a global
code: both for neural implicit surfaces [9, 22, 30, 55] and
point clouds [41]. Decoding shape from local codes signif-
icantly improves geometry quality. In particular for model-
ing humans, SCALE [41] successfully leverages local fea-
tures to represent pose-aware garment shape and demon-
strates a significant qualitative improvement against prior
work that uses a single global feature code [23].

However, the local feature in SCALE is still discretely
distributed on a set of pre-defined basis points ui on the
body manifold. Each feature code is decoded into multiple
points in a neighborhood (a “patch”) in the output space, but
it varies discretely across patch boundaries. This is equiva-
lent to the nearest neighbor (on the body manifold) assign-
ment of the features, Fig. 3(a). This discrete characteristic
limits the quality of the geometry. As shown in Fig. 3 of
[41] and our Sec. 4.1, the patches are typically isolated from
each other, leading to uneven point distributions, hence poor
mesh reconstruction quality.

Figure 3: 2D illustration of the point feature assignment for a
region on the body manifold between the body basis points. Colors
represent features. (a) The nearest neighbor assignment used by
SCALE [41], causes “patchy” predictions. (b) Our bilinear feature
interpolation results in smoothly varying features on the manifold.

To address this problem, we make the local features more
fine-grained in two ways. First, we define a denser set of ba-
sis points ui, together with their local features, on the body
manifold. In practice, this amounts to simply increasing the
resolution of the body UV map (see Sec. 3.2). Second, we
further diffuse the feature over the body surface: for a query
coordinate on the body surface, we compute its feature by
bilinearly interpolating the features from its 4 nearest basis
points, Fig. 3. As a result, the network output can be eval-
uated at arbitrarily high resolution by querying the decoder
fw(·) with any point on the body surface, which we also
denote as ui from now on with a slight abuse of notation.
Local Transformations. The clothing deformation vector
ri in Eq. (1) is predicted on top of the body in the canonical
pose. Thus a large portion of shape variation is explained by
body articulation so that the network can focus on modeling
the residual shape. Note that unlike the mesh vertex offset
representation of clothing [6, 42, 58], our formulation does
not assume a constant topology, and can thus model various
outfit styles such as pants and dresses.

To reconstruct the clothed body in the posed space, we
transform ri according to the transformation matrix Ti at
ui that is given by the fitted body model. The point position
from the clothed body in the posed space is then given by:

xi = Ti · ri + pi,

(2)

4

Clothing onPer-SubjectBody ShapeShape DecoderAuto-DecodeLossesNeutral BodyShapeEncoderPoseKnown Rigid TransformTrainable NetworksOptimizable GeometricFeatures。Body basis pointQuery pointNearest assignmentBilinear interpolation(a)(b)where pi is the 3D Euclidean coordinate of ui on the posed
unclothed body. Note that we branch out the final layers of
fw(·) such that it also predicts the normal n(xi) of each
point, which is transformed with the rotation part of Ti.

Since our local point features are continuous over the
body surface, we also perform barycentric interpolation
to obtain continuously varying Ti, in the same way as
LoopReg [5].

3.2. Cross-garment Modeling with a Single Model

With our structured point cloud representation, we now
build a system that models pose-dependent deformations
of various garments, from different categories, of different
shapes and topology, dressed on different body shapes, us-
ing a single model. This is achieved by introducing a novel
geometric feature tensor. Practically, we decompose the lo-
cal feature zi in Eq. (1) into pose zP
i and garment geometry
zG
i , as illustrated in Fig. 2 and detailed below.
Body Shape Agnostic Pose Encoding. We first condition
our network with learned local body pose features zP
i such
that the output garment deformation is pose-aware. We
adopt the approach based on the UV positional map of the
posed body as used in [41] as it shows better pose gen-
eralization than the traditional pose parameter condition-
ing [42, 51, 77]. As shown in Fig. 2, a UV positional map
I ∈ RH×W ×3 is a 2D parameterization of the body man-
ifold, where each valid pixel corresponds to a point on the
posed body surface. The 2D image-plane coordinate of a
pixel describes its manifold position: ui = (u, v)i. The
pixel value records the point’s location in R3: pi = Iui.
A UNet [62] encodes I into a pose feature tensor P ∈
RH×W ×64, where H, W are the spatial dimensions of the
feature tensor, and each “pixel” from P is a 64-dimensional
i = Pui ∈ R64. Because of the recep-
pose feature code: zP
tive field of the UNet, the learned pose features can contain
global pose information from a larger neighborhood when
necessary.

In most

The UV positional map naturally contains information
about body shape. To generalize to different subjects, we
use posed bodies of a neutral shape for the pose encoding.
Still, the predicted clothing deformations are added to each
subject’s body respectively.
Geometric Feature Tensor.
learning-based,
outfit/subject-specific clothed body models [41, 42, 65, 77],
the clothing shape information is contained in the parame-
ters of the trained shape decoder network, limiting gener-
alization to unseen outfits. What is needed for cross-outfit
modeling is a mechanism that decouples the intrinsic, pose-
independent shape of a clothed person from the decoder,
so that it can focus on modeling how the shape deforms
with the pose. To that end, we propose to explicitly con-
dition the shape decoder with a geometric feature tensor
G ∈ RH×W ×64, Fig. 2.

The geometric feature tensor follows the spirit of being
local and is pixel-aligned with the pose feature tensor. Each
of its “pixels” represents a local shape feature on a body
i = Gui ∈ R64. Unlike the pose features, the ge-
point: zG
ometry features are learned in an auto-decoding [50] fash-
ion; i.e. they are updated during training such that the opti-
mal representation for the garment geometry is discovered
by the network itself. Importantly, we use a consistent G
for each outfit across all of its training examples (in differ-
ent poses). In this way, G is enforced to be a pose-agnostic
canonical representation of each outfit’s geometry.

Our geometry feature tensor plays a similar role as the
pre-defined clothing templates in many mesh-based gar-
ment models [26, 51, 66, 73]. However, by auto-decoding
the geometric features, we get rid of the reliance on man-
ual template definition. More importantly, our decoder net-
work and the neural geometry feature are fully differen-
tiable. This enables generalization to unseen garments at
test time, which is done by optimizing G to fit the target
clothed body scan. See Sec. 3.3 for details.
Shared Garment Shape Decoder. With the introduced lo-
cal pose and geometry features, we can re-write Eq. (1) in a
more concrete form: ri = fw([ui, zP
i ]), where [·, ·, ·]
denotes concatenation. While zG
is optimized for each gar-
i
ment and zP
is acquired from each pose, fw(·) is shared
i
for the entire set of all garments and poses. By training on
many outfits and poses, the decoder learns common prop-
erties of clothing deformation, with which it can animate
scans in unseen outfits at test-time, as described below.

i , zG

3.3. Training and Inference

Loss Functions. We train POP with multi-subject and outfit
data. During training, the parameters of the UNet pose en-
coder, the garment shape decoder, and the geometric feature
tensor G are optimized, by minimizing the loss function:

Ltotal = λdLd + λnLn + λrdLrd + λrgLrg,

(3)

where the λ’s are weights that balance the loss terms, and
the L’s are the following loss terms.

First, the normalized Chamfer Distance Ld is employed
to penalize the average bi-directional point-to-point L2 dis-
tances between the generated point cloud X and a sampled
point set Y from the ground truth surface: Ld = d(x, y) =

1
M

M
(cid:88)

i=1

(cid:13)
(cid:13)xi − yj

min
j

(cid:13)
2
2 +
(cid:13)

1
N

N
(cid:88)

j=1

(cid:13)
(cid:13)xi − yj

min
i

(cid:13)
2
2,
(cid:13)

(4)

where M, N are the number of points from the generated
point cloud and the ground truth surface, respectively.

The normal loss Ln is the averaged L1 discrepancy be-
tween the normal prediction on each generated point and its

5

nearest neighbor from the ground truth point set:

Ln =

1
M

M
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)n(xi) − n(argmin
yj ∈Y

d(xi, yj))

(cid:13)
(cid:13)
(cid:13)1

,

(5)

where n(·) denotes the unit normal of the given point.

An L2 regularizer Lrd discourages the predicted point
displacements from being extremely large. Similarly, the
term Lrg penalizes the L2-norm of the vectorized geometric
feature tensor to regularize the garment shape space:

Lrd =

1
M

M
(cid:88)

i=1

(cid:13)
(cid:13)ri

(cid:13)
2
2,
(cid:13)

Lrg =

1
C

C
(cid:88)

(cid:13)
(cid:13)Gm

(cid:13)
2
2,
(cid:13)

m=1

(6)

where C is the number of garments seen in training.

The detailed model architecture, hyper-parameters and

training procedure are provided in the SupMat.
Inference: Scan Animation. At test-time, POP can gener-
alize to unseen poses of both the previously seen and unseen
outfits. For a seen outfit, we use its geometric feature tensor
optimized from training, and infer the clothing deformation
on unseen poses with a simple forward pass of the network.
To test on a scan ˆY of a human wearing unseen cloth-
ing, we first fix the weights of the UNet pose encoder and
the shape decoder gw(·), and optimize the geometric feature
tensor such that the total loss against the scan is minimized:

ˆG = argmin Ltotal( ˆY).
(7)
Afterwards, the estimated ˆG is fixed, and is then treated as
in the case of a seen garment.

As with other point-based human models, the point cloud
generated by POP can either be meshed using classical tools
such as the Poisson Surface Reconstruction (PSR) [32], or
directly rendered into realistic images using recent point-
based neural rendering techniques [2, 34, 59]. However,
in this work, we do not rely on neural rendering to inpaint
the gaps between the points.
Instead, we show qualita-
tive results using a simple surfel-based renderer to more
directly highlight the geometric properties of the smooth,
high-resolution, human point cloud generated by POP.

4. Experiments

Due to the lack of comparable existing work on POP’s
two key features, namely cross-outfit modeling and single
scan animation, we first evaluate its representation power on
a simpler but related task: outfit-specific shape modeling,
and compare with two state-of-the-art methods (Sec. 4.1).
We then discuss the efficacy of our cross-outfit learning for-
mulation (Sec. 4.2) and demonstrate single scan animation
(Sec. 4.3).
Datasets. We train and evaluate our method and baselines
on both a captured clothed human dataset, CAPE [42], and

6

our new synthetic dataset called ReSynth. From the CAPE
dataset, we use the three subjects (00096, 00215, 03375)
that contain the most abundant outfit variations (14 outfits
in total) to compare the representation capacity of differ-
ent methods. The synthetic ReSynth dataset is created with
a larger variation in outfit shapes, styles, and poses. We
worked with a professional clothing designer to create 3D
outfit designs that faithfully reflect those in a set of commer-
cial 3D clothed human scans (Renderpeople [61]), resulting
in 24 outfits including challenging cases such as skirts and
jackets. We then use physics simulation to drape the cloth-
ing on the 3D bodies from the AGORA dataset [52], which
we animate to generate many poses. Details of the datasets
are provided in the SupMat., and we will release ReSynth
for research purposes.
Baselines. To evaluate the representation power of our
model, we first compare with two recent methods for pose-
dependent human shape modeling (Sec. 4.1): NASA [16]
and SCALE [41]. To evaluate the effectiveness of our cross-
outfit modeling formulation (Sec. 4.2), we compare two
versions of our model: per-outfit trained and a cross-outfit
model trained with data from all outfits, both using the same
architecture. For animating unseen scans (Sec. 4.3), we
qualitatively compare with classical Linear Blend Skinning
using the SMPL [39] body model.
Metrics. We quantitatively evaluate each method using the
Chamfer Distance (in m2, Eq. (4)) and the L1 normal dis-
crepancy (Eq. (5)), computed over the 50K points generated
by our method and SCALE. For the implicit surface base-
line NASA, the points for evaluation are sampled from sur-
face extracted using Marching Cubes [40]. To evaluate the
visual quality of the generated results, we perform a large-
scale user study on the Amazon Mechanical Turk (AMT)
and report the percentage of users that favor the results from
our method over the baseline. Details of the user study are
provided in the SupMat.

4.1. Representation Power

Tab. 2 summarizes the numerical results of reconstruct-
ing pose-dependent garment shape from different methods,
tested with seen outfits on unseen motion sequences. As
the difficulty of shape modeling varies greatly across dif-
ferent outfit types (e.g. how a loose jacket deforms is much
more complex than that of a tight T-shirt), we report three
types of statistics to holistically reflect the performance of
each model: the mean error averaged across all test exam-
ples from all outfits, the median of the per-outfit calculated
mean error (denoted as “Outfit Median” in Tab. 2), and the
per-outfit maximum error (“Outfit Max”).
Comparison with SoTA. The upper section of Tab. 2 shows
a comparison with NASA [16] and SCALE [41]. NASA
represents the body shape with an ensemble of articulated
occupancy functions defined per body part. As it requires

Table 2: Results of pose-dependent deformation prediction on unseen test sequences from the captured CAPE dataset and
our ReSynth data. Best results are in boldface.

CAPE Data

ReSynth Data

Methods

NASA [16]
SCALE [41]
Ours, per-outfit
Ours, multi
1/2 Data
1/4 Data
1/8 Data

Mean

Chamfer-L2 (×10−4m2) ↓
Outfit
Outfit
Median
Max
32.35
1.190
0.971
0.689
0.831
0.607
0.757
0.550
0.765
0.560
0.841
0.586
0.992
0.623

6.087
0.721
0.639
0.592
0.598
0.621
0.662

Mean

Normal diff. (×10−1) ↓
Outfit
Outfit
Median Max
1.497
1.277
1.335
1.170
1.293
1.150
1.256
1.116
1.257
1.127
1.271
1.142
1.310
1.176

1.275
1.168
1.146
1.115
1.122
1.134
1.165

Mean

Chamfer-L2 (×10−4m2) ↓
Outfit
Outfit
Max
Median
–
–
8.451
0.680
7.339
0.651
0.635
7.386
7.414
0.665
7.469
0.674
7.859
0.720

–
1.491
1.356
1.366
1.405
1.406
1.490

Normal diff. (×10−1) ↓
Outfit
Outfit
Median Max

Mean

–
1.041
1.013
1.022
1.032
1.032
1.050

–
1.054
1.006
1.037
1.042
1.043
1.056

–
1.321
1.289
1.280
1.299
1.296
1.326

pre-computing occupancy values for training, it is in gen-
eral not applicable to our synthetic data, which typically
does not contain water-tight meshes. SCALE produces a
point cloud of clothed bodies, where the points are grouped
into local patches. Notably, both methods need to train a
separate model per outfit. Under the same outfit-specific
training setting, POP outperforms both baselines on both
datasets under all metrics. We further conduct an AMT user
study to evaluate the perceptual quality, comparing POP
side-by-side against the strongest baseline, SCALE. On the
CAPE data, 89.8% participants rate POP’s results as hav-
ing “higher visual quality” than SCALE (10.2%); while on
ReSynth, 84.8% users favor POP over SCALE (15.2%).

The differences in perceptual quality can be seen in
Fig. 4. All approaches provide clear pose-dependent effects.
However, NASA suffers from non-smooth transitions be-
tween separately-modeled body parts and is unable to han-
dle thin structures and open surfaces such as the skirt in
ReSynth. SCALE, on the other hand, produces a smooth
global shape with local details, but the isolation between the
patches leads to sub-optimal visual quality. In contrast, the
dense point clouds generated by POP manifest a coherent
overall shape and expressive local details. This illustrates
the advantage of our continuous local point features as op-
posed to the discretely defined patch features.

4.2. Cross-Outfit Modeling

A key feature of POP is the ability to handle mul-
tiple outfits of varied types using a single model, with-
out pre-defining garment templates. As shown in Tab. 2,
our cross-outfit model trained with all subjects and outfits
(“Ours, multi”) reaches overall comparable performance to
the outfit-specific models (“Ours, per-outfit”) on the syn-
thetic ReSynth data, and has significantly lower error on the
CAPE dataset. Still, on both datasets, the cross-outfit model
consistently outperforms the two baselines analyzed above.
The result on the CAPE data reveals the advantage of
the information between different
cross-outfit modeling:
but similar outfit styles can be shared. The CAPE dataset
mostly consists of simple and similar garments such as

NASA [16]

SCALE [41] Meshed

Ours

Meshed

SCALE [41]

Meshed

Ours

Meshed

Figure 4: Comparison with SoTAs on the CAPE (upper 2 rows)
and ReSynth (lower 2 rows) data. The dense point clouds from
POP (cross-outfit model) are clean and preserve local details,
while baseline methods suffer from artifacts. Note the point clouds
are colored according to predicted normal and rendered with sur-
fels [57]. Best viewed zoomed-in on a color screen.

long/short T-shirts and pants, but the motions are performed
differently by different subjects.
training a
cross-outfit model leads to mutual data augmentation in the
pose space among different outfits, hence a better general-
ization to unseen poses as seen in the numerical results. In

Intuitively,

7

contrast, the synthetic data are simulated with a consistent
set of motion sequences for all outfit types with largely var-
ied geometry. As a result, the inter-outfit knowledge sharing
is limited, leading to similar performance between per-outfit
and cross-outfit models.
Robustness Against Limited Data. In the lower section
of Tab. 2, we evaluate the performance of our cross-outfit
model trained with subsets sampled from the full training
set. Even with only 1/4 of data from each outfit, our cross-
outfit model is comparable to the outfit-specific models of
SCALE that are trained with full data.

4.3. Single Scan Animation

Once trained, our cross-outfit POP model can be fit to
a single scan of an unseen subject with an unseen outfit,
and then animated with pose-dependent clothing variation,
as described in Sec. 3.3. Notably, this is a very challeng-
ing task since it requires generalization in both the outfit
shape space and pose space. Figure 5 qualitatively com-
pares our model (trained on CAPE data) and the classical
Linear Blend Skinning (LBS) technique that uses the SMPL
model [39] to animate the given unseen scan with an unseen
motion. Here we use sampled points from the mesh pro-
vided in the CAPE dataset as the target scan. As LBS uses
simple rigid transformations of the body parts only, it can-
In
not produce complex pose-dependent shape variation.
contrast, POP produces reasonable and vivid clothing de-
formation such as the lifted hems. In Fig. 6 we deploy POP
(trained on the ReSynth data) to animate an unseen scan
from ReSynth and one from real-world captures [61], re-
spectively. Note that the latter test is much more challeng-
ing due to the domain gap between our synthetic training
data and the captured test data. POP produces high-quality
clothing geometry on the ReSynth test example and gen-
erates reasonable animation for the challenging real-world
scan, which opens the promising new direction of automatic
3D avatar creation from a “one-shot” observation.

5. Conclusion

We have introduced POP, a dense, structured point cloud
shape representation for articulated humans in clothing. Be-
ing template-free, geometrically expressive, and topologi-
cally flexible, POP models clothed humans in various outfit
styles with a single model, producing high-quality details
and realistic pose-dependent deformation. The learned POP
model can also be used to animate a single human scan from
an unseen subject and clothing. Our evaluations on both
captured data and our synthetic data demonstrate the effi-
cacy of the continuous local features and the advantage of a
cross-outfit model over traditional subject-specific ones.

POP assumes that the minimally closed body under the
clothing is given and the training scans have no noise

Scan

Animations with LBS

Fitted POP

Animations with POP

Figure 5: Comparison of animation with POP and an LBS-based
method. The unseen scan from the CAPE dataset is animated on
unseen motions.

Scan

Fitted POP

Animations with POP

Figure 6: Animation of an unseen example from ReSynth (upper
row) and a real-world capture (lower row) with unseen motions.

or missing data. Dealing with partial, noisy, scans and
combining it with automatic body shape and pose esti-
mation [5, 74] are promising future directions for fully-
automatic scan animation. Our use of the UV map for body
pose conditioning can sometimes lead to “seams” in the
outputs (see SupMat.). Future work should explore more
continuous parameterizations of the human body manifold.
Additionally, here we factor out dependencies of clothing
deformation on body shape for simplicity. Given sufficient
training data, this would be easy to learn by replacing the
neutral shape with the true shape in the UV positional map.

Acknowledgements and Disclosure: We thank Studio Lupas for
helping create the clothing designs, and S. Saito for the help
with data processing during his internship at MPI. Q. Ma is par-
tially funded by DFG (German Research Foundation)-276693517
SFB 1233 and supported by the Max Planck ETH Center for
The conflict of interest of MJB can be
Learning Systems.
found at https://files.is.tue.mpg.de/black/CoI/
ICCV2021.txt.

8

Appendix

A. Implementation Details

A.1. Model Architecture

We use the SMPL [39] (for CAPE data) and SMPL-
X [53] (for ReSynth data) UV maps of 128 × 128 × 3 res-
olution as pose input, where each pixel is encoded into 64
channels by the pose encoder. The pose encoder is a stan-
dard UNet [62] that consists of seven [Conv2d, BatchNorm,
LeakyReLU(0.2)] blocks, followed by seven [ReLU, Con-
vTranspose2d, BatchNorm] blocks. The final layer does not
apply BatchNorm.

The geometric feature tensor has the same resolution as
that of the pose feature tensor, i.e. 128 × 128 × 64. It is
learned in an auto-decoding [50] manner, being treated as
a free variable that is optimized together with the network
weights during training. The geometric feature tensor is fol-
lowed by three learnable convolutional layers, each with a
receptive field of 5, before feeding it to the shape decoder.
We find that these convolutional layers help smooth the fea-
tures spatially, resulting in a lower noise level in the outputs.
The pose and geometric feature tensors are concatenated
along the feature channel. In all experiments, we query the
feature tensor with a 256 × 256 UV map, i.e. the concate-
nated feature tensor is spatially 4× bilinearly upsampled.
The output point cloud has 50K points.

At each query location, the concatenated pose and ge-
ometry feature (64+64-dimensional), together with the 2D
UV coordinate of the query point, are fed into an 8-layer
MLP. The intermediate layers’ dimensions are (256, 256,
256, 386, 256, 256, 256, 3), with a skip connection from
the input to the 4th layer as in DeepSDF [50]. From the
6th layer, the network branches out 2 heads with the same
architecture to predict the displacements and point normals,
respectively. All but the last layer use BatchNorm and a
Softplus non-linearity with β = 20. The predicted normals
are normalized to unit length.

A.2. Training

We train POP with the Adam [33] optimizer with a learn-
ing rate of 3.0×10−4, a batch size of 4, for 400 epochs. The
displacement and normal prediction modules are trained
jointly. As the normal loss relies on the nearest neighbor
ground truth points found by the Chamfer Distance, we
only turn it on when Ld roughly stabilizes from the 250th
epoch. The loss weights are set to λd = 2.0 × 104, λrd =
2.0 × 103, λrg = 1.0, λn = 0.0 at the beginning of the train-
ing, and λn = 0.1 from the 250th epoch.

A.3. Data Processing

We normalize all the data examples by removing the
body translation and global orientation from them. From

each clothed body surface, we sample 40K points to serve
as training ground truth. Note that we do not rely on any
connectivity information in the registered meshes from the
CAPE dataset.

A.4. Baselines

NASA. We re-implement the NASA [16] model in PyTorch
and ensure the performance is on par with that reported in
the original paper. For evaluating NASA results, we first ex-
tract a surface using Marching Cubes [40] and then sample
the same number of points (50K) from it for a fair compar-
ison. The sampling is performed and averaged over three
repetitions.
SCALE. We employ the same training schedule and the
number of patches (798) as in the original SCALE pa-
per [41], using the implementation released by the authors.
We sample 64 points per patch at both training and infer-
ence to achieve the same number of output points as ours
for a fair comparison.
LBS. In the main paper Sec. 4.3, we compare with the Lin-
ear Blend Skinning (LBS) in the single scan animation task.
This is done with the help of the SMPL [39] body model:
we first optimize the SMPL body shape and pose parame-
ters to fit a minimally-clothed body to the given scan, and
then displace the vertices such that the final surface mesh
aligns with the scan. The fitted clothed body model is then
reposed by the target pose parameters.

A.5. User Study

We conduct a large-scale user study on the Amazon Me-
chanical Turk to get a quantitative evaluation of the visual
quality of our model outputs against the point-based method
SCALE [41]. We evaluate over 6,000 unseen test examples
in the CAPE and ReSynth datasets, from different subjects,
performing different poses. For each example, the point
cloud output from POP and SCALE are both rendered with
a surfel-based renderer by Open3D [81] under the same ren-
dering settings (an example of such rendering is the Fig. 1 in
the main paper). We then present both images side-by-side
to the users and ask them to choose the one that they deem a
higher visual quality. The left-right ordering of the images
is randomly shuffled for each example to avoid users’ bias
to a certain side. The users are required to zoom-in the im-
ages before they are able to make the evaluation, and we do
not set a time limit for the viewing. Each image pair is eval-
uated by three users, and the final results in the main paper
is averaged from all the user choices on all examples.

B. Datasets

ReSynth. The 24 outfits designed by the clothing de-
signer include varied types and styles: shirts, T-shirts, pants,
shorts, skirts, long dresses, jackets, to name a few. For

1

signed-rank test). Together with the user study results in
the main paper, this shows a consistent improvement on the
representation power.

In Figs. E.2 and E.3 we show extended qualitative
comparisons with NASA [16] and SCALE [41] from the
pose generalization experiment (Sec. 4.1 in the main pa-
per). Please refer to the supplementary video at https:
//qianlim.github.io/POP for animated results.

D. Run-time Comparison

Here we compare the inference speed of POP with the
implicit surface baseline, NASA [16], and the patch-based
baseline, SCALE [41].

To generate a point cloud with 50K points, POP takes
on average 48.8ms, and SCALE takes 42.4ms. The optional
meshing step using the Poisson Reconstruction [32] takes
1.1s if a mesh is desired. Both explicit representations have
comparable run-time performance. In contrast, NASA re-
quires densely evaluating occupancy values over the space
in order to reconstruct an explicit surface, which takes 12.2s
per example. This shows the speed advantage of the explicit
representations over the implicit ones.

E. Limitations and Failure Cases

As discussed in the final section of the main paper, the
major limitation of our approach lies on the use of the UV
map. Although the UV maps are widely used to reconstruct
and model human faces [21, 43, 70], one can encounter ad-
ditional challenges when applying this technique to human
bodies. On a full body UV map such as that of SMPL, dif-
ferent body parts are represented as separate “islands” on
the UV map, see Fig. 2 in the main paper. Consequently, the
output may suffer from discontinuities at the UV islands’
boundaries. Qualitatively, this may occasionally result in
visible “seams” between certain body parts as shown in
Fig. E.1 (a-b), or an overly sparse distribution of points be-
tween the legs in the case of dresses as shown in Fig. E.1 (c),
leading to sub-optimal performance when training a unified
model for both pants and skirts.

Note, however, that such discontinuities are not always
the case.
Intuitively, as the input UV positional map en-
codes (x, y, z) coordinates on the 3D body, the network can
utilize not only the proximity in the UV space but also that
in the original 3D space. We believe that the problem orig-
inates from the simple 2D convolution in the UNet pose
encoder. A promising solution is to leverage a more con-
tinuous parameterization for the body surface manifold that
is compatible with existing deep learning architectures. We
leave this for future work.

Scans

Designs

Simulations

Figure B.1: Examples from our ReSynth dataset. The cloth-
ing is designed based on real-world scans [61], draped on
the SMPL-X [53] body model, and then simulated using
Deform Dynamics [15].

each outfit, we find the SMPL-X [53] body (provided by
the AGORA [52] dataset) that fits the subject’s body shape
in its corresponding original scan. We then use physics-
based simulation [15] to drape the clothing on the bodies
and animate them with a consistent set of motion sequences
of the subject 00096 from the CAPE dataset. The simula-
tion results are inspected manually to remove problematic
frames, resulting in 984 frames for training and 347 frames
for test for each outfit. Examples from ReSynth are shown
in Fig. B.1. We will release the dataset for research pur-
poses.
CAPE. The CAPE dataset [42] provides registered mesh
pairs of (unclothed body, clothed body) of humans in
clothing performing motion sequences. The three subjects
(00096, 00215, 03375) that we use in the experiments have
in total 14 outfits comprising short/long T-shirts, short/long
pants, a dress shirt, a polo shirt, and a blazer. For each out-
fit, the motion sequences are randomly split into training
(70%) and test (30%) sets.

C. Extended Results and Discussions

Here we provide extended analysis and discussions re-
garding the main paper Tab. 2. The implicit surface base-
line, NASA [16], shows a much higher error than other
methods. We find that it is majorly caused by the occasional
missing body parts in its predictions. This happens more
often for challenging, unseen body poses. The incomplete
predictions thus lead to exceptionally high bi-directional
Chamfer distance on a number of examples, hence a high
average error.

Our approach is based on the SCALE [41] baseline, but
it achieves on average 11.4% (on CAPE data) and 9.1% (on
ReSynth) lower errors than SCALE, with both margins be-
ing statistically significant (p-value≪1e-4 in the Wilcoxon

2

nical practices from each piece of work. It is also interest-
ing to note the connection between our geometric feature
tensor and the neural texture in recent work on neural ren-
dering [60, 69]: both concepts learn a spatial neural rep-
resentation that controls the output, revealing a connection
between modeling the 3D geometry and 2D appearances.

Finally, in concurrent work, MetaAvatar [75] also learns
a multi-subject model of clothed humans, which can gen-
eralize to unseen subjects using only a few depth images.
Unlike our auto-decoding learning of the geometric feature
tensor, MetaAvatar uses meta-learning to learn a prior of
pose-dependent cloth deformation across different subjects
and clothing types that helps generalize to unseen subjects
and clothing types at test-time. We believe both approaches
will inspire future work on cross-garment modeling and au-
tomatic avatar creation.

References

[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
Leonidas Guibas. Learning representations and generative
In Proceedings of Machine
models for 3D point clouds.
Learning and Systems (ICML), pages 40–49. PMLR, 2018.
2, 3

[2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry
Ulyanov, and Victor Lempitsky. Neural point-based graph-
In Proceedings of the European Conference on Com-
ics.
puter Vision (ECCV), pages 696–712, 2020. 6

[3] Ilya Baran and Jovan Popovi´c. Automatic rigging and an-
imation of 3D characters. ACM Transactions on Graphics
(TOG), 26(3):72–es, 2007. 1

[4] Jan Bednaˇr´ık, Shaifali Parashar, Erhan Gundogdu, Mathieu
Salzmann, and Pascal Fua. Shape reconstruction by learning
differentiable surface representations. In Proceedings IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
pages 4715–4724, 2020. 2, 3

[5] Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian
Theobalt, and Gerard Pons-Moll. LoopReg: Self-supervised
learning of implicit surface correspondences, pose and shape
In Advances in Neural
for 3D human mesh registration.
Information Processing Systems (NeurIPS), pages 12909–
12922, 2020. 5, 8

[6] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,
and Gerard Pons-Moll. Multi-Garment Net: Learning to
the
dress 3D people from images.
IEEE/CVF International Conference on Computer Vision
(ICCV), pages 5420–5430, 2019. 2, 3, 4

In Proceedings of

[7] Aljaz Bozic, Pablo Palafox, Michael Zollh¨ofer, Justus Thies,
Angela Dai, and Matthias Nießner. Neural deformation
graphs for globally-consistent non-rigid reconstruction.
In
Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2021. 3

[8] Andrei Burov, Matthias Nießner, and Justus Thies. Dynamic
surface function networks for clothed human bodies.
In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), 2021. 2, 3

[9] Rohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt,
Julian Straub, Steven Lovegrove, and Richard Newcombe.

(a)

(b)

(c)

Figure E.1: Illustrations of our limitations.

F. Further Discussions on Related Work

Here we discuss the relationship of our method to recent
work that uses similar techniques or that aims similar goals.
We represent clothing as a displacement field on the
minimally-clothed body, in the canonical pose space. This
helps factor out the effect of the articulated, rigid trans-
formations that are directly accessible from the underly-
ing posed body. In this way, the network can focus on the
non-rigid, residual clothing deformation. Such technique
is becoming increasingly popular for clothed human shape
models that use meshes [42], point clouds [41], and implicit
functions [10, 37, 48, 65, 72, 75].

Our shape decoder is a coordinate-based multi-layer per-
ceptron (MLP), reminiscent of the recent line of work on
neural implicit surfaces [11, 44, 50] and neural radiance
fields [46]. These methods learn to map a neural feature
at a given query location into a certain quantity, e.g. a
signed distance [50], occupancy [44], color and volume
density [46], or a displacement vector in our case. Our
work differs from others majorly in that the querying coor-
dinates live on a 2-manifold (the body surface)1 instead of
R3. Moreover, our point cloud representation belongs to the
explicit representation category, retaining an advantage in
the inference speed compared to the implicit methods. With
the recent progress in differentiable and efficient surface re-
construction from point clouds [54, 67], it becomes possible
to flexibly convert between point clouds and meshes in var-
ious applications.

Recent work on deformable face modeling [43] and
pose-controlled free-view human synthesis [37] employ
similar network architectures as ours, despite the difference
in the goals, tasks and detailed techniques. While the com-
monality implies the efficacy of the high-level architectural
design, it remains interesting to combine the detailed tech-

1The concurrent work by Burov et al. [8] also maps the points on the
SMPL body surface to a continuous clothing offset field, but the points’
Euclidean coordinates (with positional encoding) are used to query the
shape decoder.

3

Deep local shapes: Learning local sdf priors for detailed 3D
reconstruction. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 608–625, 2020. 4
[10] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,
and Andreas Geiger. SNARF: Differentiable forward skin-
In
ning for animating non-rigid neural implicit shapes.
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), 2021. 3

[11] Zhiqin Chen and Hao Zhang. Learning implicit fields for
generative shape modeling. In Proceedings IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR), pages
5939–5948, 2019. 3

[12] Julian Chibane, Aymen Mir, and Gerard Pons-Moll. Neural
unsigned distance fields for implicit function learning. In Ad-
vances in Neural Information Processing Systems (NeurIPS),
pages 21638–21652, 2020. 2

[13] Enric Corona, Albert Pumarola, Guillem Aleny`a, Ger-
ard Pons-Moll, and Francesc Moreno-Noguer. SMPLicit:
Topology-aware generative model for clothed people.
In
Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), pages 11875–11885, 2021. 3

[14] Edilson De Aguiar, Leonid Sigal, Adrien Treuille, and Jes-
sica K Hodgins. Stable spaces for real-time clothing.
In
ACM Transactions on Graphics (TOG), volume 29, page
106. ACM, 2010. 3
[15] Deform Dynamics.

https://deformdynamics.

com/. 2

[16] Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-
Moll, Geoffrey Hinton, Mohammad Norouzi, and Andrea
Tagliasacchi. Neural articulated shape approximation.
In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 612–628, 2020. 3, 6, 7, 1, 2

[17] Zhantao Deng, Jan Bednaˇr´ık, Mathieu Salzmann, and Pascal
Fua. Better patch stitching for parametric surface reconstruc-
tion. In International Conference on 3D Vision (3DV), pages
593–602, 2020. 2, 3

[18] Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir
Kim, Bryan Russell, and Mathieu Aubry. Learning elemen-
tary structures for 3D shape generation and matching. In Ad-
vances in Neural Information Processing Systems (NeurIPS),
pages 7433–7443, 2019. 2, 3

[19] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set
generation network for 3D object reconstruction from a sin-
gle image. In Proceedings IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), pages 2463–2471, 2017.
2, 3

[20] Andrew Feng, Dan Casas, and Ari Shapiro. Avatar reshap-
ing and automatic rigging using a deformable model. In Pro-
ceedings of the 8th ACM SIGGRAPH Conference on Motion
in Games, pages 57–64, 2015. 1

[21] Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi
Zhou. Joint 3D face reconstruction and dense alignment with
position map regression network. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pages 534–
551, 2018. 2

[22] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,
and Thomas Funkhouser. Local deep implicit functions for
3D shape. In Proceedings IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), pages 4857–4866, 2020. 4

[23] Thibault Groueix, Matthew Fisher, Vladimir G Kim,
Bryan C Russell, and Mathieu Aubry. 3D-CODED: 3D cor-
In Proceedings of the
respondences by deep deformation.
European Conference on Computer Vision (ECCV), pages
230–246, 2018. 3, 4

[24] Thibault Groueix, Matthew Fisher, Vladimir G. Kim,
Bryan C. Russell, and Mathieu Aubry. A papier-mˆach´e ap-
proach to learning 3D surface generation. Proceedings IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
pages 216–224, 2018. 2, 3, 4

[25] Peng Guan, Loretta Reiss, David A Hirshberg, Alexander
Weiss, and Michael J Black. DRAPE: DRessing Any PEr-
son. ACM Transactions on Graphics (TOG), 31(4):35–1,
2012. 2, 3

[26] Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini,
Minh Dang, Mathieu Salzmann, and Pascal Fua. GarNet: A
two-stream network for fast and accurate 3D cloth draping.
In Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), pages 8739–8748, 2019. 2, 3, 5
[27] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and
Tony Tung. ARCH++: Animation-ready clothed human re-
construction revisited. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV), 2021.
3

[28] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and
Tony Tung. ARCH: Animatable reconstruction of clothed
In Proceedings IEEE Conf. on Computer Vision
humans.
and Pattern Recognition (CVPR), pages 3093–3102, 2020. 3
[29] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang
Liu, and Hujun Bao. BCNet: Learning body and cloth shape
from a single image. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 18–35. Springer,
2020. 3

[30] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei
Huang, Matthias Nießner, and Thomas Funkhouser. Local
implicit grid representations for 3D scenes. In Proceedings
IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), pages 6001–6010, 2020. 4

[31] Ning Jin, Yilin Zhu, Zhenglin Geng, and Ronald Fedkiw.
A pixel-based framework for data-driven clothing. In Com-
puter Graphics Forum, volume 39, pages 135–144, 2020. 2
[32] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.
Poisson surface reconstruction. In Proceedings of the fourth
Eurographics Symposium on Geometry Processing, vol-
ume 7, 2006. 6, 2

[33] Diederik P. Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Conference on
Learning Representations (ICLR), 2015. 1

[34] Maria Kolos, Artem Sevastopolsky, and Victor Lempitsky.
TRANSPR: Transparency ray-accumulating neural 3D scene
In International Conference on 3D Vision
point renderer.
(3DV), pages 1167–1175, 2020. 6

[35] Zorah L¨ahner, Daniel Cremers, and Tony Tung. DeepWrin-
In Pro-
kles: Accurate and realistic clothing modeling.
ceedings of the European Conference on Computer Vision
(ECCV), pages 698–715, 2018. 2, 3

[36] Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning
efficient point cloud generation for dense 3D object recon-
struction. In Proceedings of the AAAI Conference on Artifi-

4

cial Intelligence (AAAI), pages 7114–7121, 2018. 2, 3
[37] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu
Sarkar, Jiatao Gu, and Christian Theobalt. Neural Actor:
Neural free-view synthesis of human actors with pose con-
trol. arXiv preprint: 2106.02019, 2021. 3

[38] Lijuan Liu, Youyi Zheng, Di Tang, Yi Yuan, Changjie Fan,
and Kun Zhou. NeuroSkinning: Automatic skin binding
for production characters with deep graph networks. ACM
Transactions on Graphics (TOG), 38(4):1–12, 2019. 1
[39] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. SMPL: A skinned multi-
person linear model. ACM Transactions on Graphics (TOG),
34(6):248, 2015. 6, 8, 1

[40] William E Lorensen and Harvey E Cline. Marching cubes: A
high resolution 3D surface construction algorithm. In ACM
SIGGRAPH Computer Graphics, volume 21, pages 163–
169, 1987. 6, 1

[41] Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, and
Michael J. Black. SCALE: Modeling clothed humans with a
surface codec of articulated local elements. In Proceedings
IEEE/CVF Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), June 2021. 2, 3, 4, 5, 6, 7, 1, 8

[42] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,
Gerard Pons-Moll, Siyu Tang, and Michael J. Black. Learn-
ing to dress 3D people in generative clothing. In Proceed-
ings IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), pages 6468–6477, 2020. 2, 3, 4, 5, 6

[43] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang,
Yuecheng Li, Fernando De la Torre, and Yaser Sheikh. Pixel
codec avatars. In Proceedings IEEE Conf. on Computer Vi-
sion and Pattern Recognition (CVPR), pages 64–73, June
2021. 2, 3

[44] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3D reconstruction in function space. In Proceed-
ings IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), pages 4460–4470, 2019. 2, 3

[45] Marko Mihajlovic, Yan Zhang, Michael J. Black, and Siyu
Tang. LEAP: Learning articulated occupancy of people. In
Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2021. 3

[46] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance fields for view syn-
thesis. In Proceedings of the European Conference on Com-
puter Vision (ECCV), pages 405–421. Springer, 2020. 3
[47] Alexandros Neophytou and Adrian Hilton. A layered model
of human body and garment deformation. In International
Conference on 3D Vision (3DV), pages 171–178, 2014. 2, 3
[48] Pablo Palafox, Aljaz Bozic, Justus Thies, Matthias Nießner,
and Angela Dai. Neural parametric models for 3D de-
formable shapes. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), 2021. 3
[49] Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, and
Kui Jia. Deep mesh reconstruction from single RGB im-
ages via topology modification networks. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV), pages 9963–9972, 2019. 2

[50] Jeong Joon Park, Peter Florence, Julian Straub, Richard

Newcombe, and Steven Lovegrove. DeepSDF: Learning
continuous signed distance functions for shape representa-
In Proceedings IEEE Conf. on Computer Vision and
tion.
Pattern Recognition (CVPR), pages 165–174, 2019. 2, 5, 1,
3

[51] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-
Moll. TailorNet: Predicting clothing in 3D as a function of
human pose, shape and garment style. In Proceedings IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
pages 7363–7373, 2020. 2, 3, 5

[52] Priyanka Patel, Chun-Hao Huang Paul, Joachim Tesch,
David Hoffmann, Shashank Tripathi, and Michael J. Black.
AGORA: Avatars in geography optimized for regression
In Proceedings IEEE Conf. on Computer Vision
analysis.
and Pattern Recognition (CVPR), pages 13468–13478, June
2021. 6, 2

[53] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and
Michael J. Black. Expressive body capture: 3D hands, face,
and body from a single image. In Proceedings IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), pages
10975–10985, 2019. 1, 2

[54] Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer,
Marc Pollefeys, and Andreas Geiger. Shape as points: A
differentiable poisson solver. arXiv preprint: 2106.03452,
2021. 3

[55] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc
Pollefeys, and Andreas Geiger. Convolutional occupancy
In Proceedings of the European Conference on
networks.
Computer Vision (ECCV), pages 523–540, 2020. 4

[56] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,
Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Implicit neural representations with structured latent codes
In Proceed-
for novel view synthesis of dynamic humans.
ings IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), pages 9054–9063, June 2021. 3

[57] Hanspeter Pfister, Matthias Zwicker, Jeroen Van Baar, and
Markus Gross. Surfels: Surface elements as rendering primi-
tives. In Proceedings of the 27th annual conference on Com-
puter graphics and interactive techniques, pages 335–342,
2000. 7

[58] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J
Black. ClothCap: Seamless 4D clothing capture and retar-
geting. ACM Transactions on Graphics (TOG), 36(4):73,
2017. 4

[59] Sergey Prokudin, Michael J. Black, and Javier Romero. SM-
In Win-
PLpix: Neural avatars from 3D human models.
ter Conference on Applications of Computer Vision (WACV),
2021. 6

[60] Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll,
and Christoph Lassner. ANR: Articulated neural rendering
for virtual avatars. In Proceedings IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), pages 3722–3731,
June 2021. 3

[61] Renderpeople, 2020. https://renderpeople.com. 6,

8, 2

[62] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
Net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical Image Com-

5

able clothed human models from few depth images. arXiv
preprint:2106.11944, 2021. 3

[76] Tuanfeng Y Wang, Duygu Ceylan, Jovan Popovi´c, and
Niloy J Mitra. Learning a shared shape space for multimodal
garment design. In ACM Transactions on Graphics (TOG),
pages 203–216, 2018. 3

[77] Jinlong Yang,

Jean-S´ebastien Franco, Franck H´etroy-
Wheeler, and Stefanie Wuhrer. Analyzing clothing layer de-
formation statistics of 3D human motions. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 237–253, 2018. 3, 5

[78] Shan Yang, Zherong Pan, Tanya Amert, Ke Wang, Licheng
Yu, Tamara Berg, and Ming C Lin. Physics-inspired garment
recovery from a single-view image. ACM Transactions on
Graphics (TOG), 37(5):1–14, 2018. 2

[79] Ze Yang, Shenlong Wang, Siva Manivasagam, Zeng Huang,
Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, and Raquel Ur-
tasun. S3: Neural shape, skeleton, and skinning fields for
3D human modeling. In Proceedings IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), pages 13284–
13293, 2021. 3

[80] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.
PaMIR: Parametric model-conditioned implicit representa-
tion for image-based human reconstruction. IEEE Transac-
tions on Pattern Analysis and Machine Intelegence, 2021. 3
[81] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D:
A modern library for 3D data processing. arXiv preprint:
1801.09847, 2018. 1

[82] Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du,
Zhangye Wang, Shuguang Cui, and Xiaoguang Han. Deep
Fashion3D: A dataset and benchmark for 3D garment re-
In Proceedings of the
construction from single images.
European Conference on Computer Vision (ECCV), volume
12346, pages 512–530, 2020. 2, 3

puting and Computer-Assisted Intervention (MICCAI), pages
234–241, 2015. 5, 1

[63] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision (ICCV), pages 2304–2314, 2019.
3

[64] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. PIFuHD: Multi-level pixel-aligned implicit function for
high-resolution 3D human digitization. In Proceedings IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
pages 84–93, 2020. 3

[65] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J.
Black. SCANimate: Weakly supervised learning of skinned
clothed avatar networks. In Proceedings IEEE/CVF Conf. on
Computer Vision and Pattern Recognition (CVPR), June
2021. 1, 2, 3, 5

[66] Igor Santesteban, Miguel A. Otaduy, and Dan Casas.
Learning-Based Animation of Clothing for Virtual Try-On.
Computer Graphics Forum, 38(2):355–366, 2019. 2, 3, 5
[67] Nicholas Sharp and Maks Ovsjanikov. PointTriNet: Learned
triangulation of 3D point sets. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 762–
778, 2020. 3

[68] Yu Shen, Junbang Liang, and Ming C Lin. Gan-based
In Pro-
garment generation using sewing pattern images.
ceedings of the European Conference on Computer Vision
(ECCV), volume 1, page 3, 2020. 3

[69] Justus Thies, Michael Zollh¨ofer, and Matthias Nießner. De-
ferred neural rendering: Image synthesis using neural tex-
tures. ACM Transactions on Graphics (TOG), 38(4):1–12,
2019. 3

[70] Diego Thomas and Rin-Ichiro Taniguchi. Augmented blend-
shapes for real-time simultaneous 3D head modeling and fa-
In Proceedings IEEE Conf. on Com-
cial motion capture.
puter Vision and Pattern Recognition (CVPR), pages 3299–
3308, June 2016. 2

[71] Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Ger-
ard Pons-Moll. SIZER: A dataset and model for parsing 3D
In Pro-
clothing and learning size sensitive 3D clothing.
ceedings of the European Conference on Computer Vision
(ECCV), volume 12348, pages 1–18, 2020. 2, 3

[72] Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, and Gerard
Pons-Moll. Neural-GIF: Neural generalized implicit func-
In Proceedings of
tions for animating people in clothing.
the IEEE/CVF International Conference on Computer Vision
(ICCV), October 2021. 3

[73] Raquel Vidaurre, Igor Santesteban, Elena Garces, and Dan
Casas. Fully convolutional graph neural networks for para-
In Computer Graphics Forum, vol-
metric virtual try-on.
ume 39, pages 145–156. Wiley Online Library, 2020. 3, 5

[74] Shaofei Wang, Andreas Geiger, and Siyu Tang. Locally
aware piecewise transformation fields for 3D human mesh
In Proceedings of the IEEE/CVF Conference
registration.
on Computer Vision and Pattern Recognition (CVPR), pages
7639–7648, June 2021. 8

[75] Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas
Geiger, and Siyu Tang. MetaAvatar: Learning animat-

6

NASA [16]

SCALE [41], point cloud

SCALE [41], meshed

Ours, point cloud

Ours, meshed

Figure E.2: Extended qualitative results from the pose generalization experiment (main paper Sec. 4.1), on the CAPE dataset.
Best viewed zoomed-in on a color screen.

7

SCALE [41], point cloud

SCALE [41], meshed

Ours, point cloud

Ours, meshed

Figure E.3: Extended qualitative results from the pose generalization experiment (main paper Sec. 4.1), on the ReSynth
dataset. Best viewed zoomed-in on a color screen.
8

