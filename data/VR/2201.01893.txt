Flow-Guided Sparse Transformer for Video Deblurring

Jing Lin * 1 Yuanhao Cai * 1 Xiaowan Hu 1 Haoqian Wang† 1 Youliang Yan 2
Xueyi Zou† 2 Henghui Ding 3 Yulun Zhang 3 Radu Timofte 3 Luc Van Gool 3

2
2
0
2

y
a
M
9
2

]

V

I
.
s
s
e
e
[

3
v
3
9
8
1
0
.
1
0
2
2
:
v
i
X
r
a

Abstract
Exploiting similar and sharper scene patches
in spatio-temporal neighborhoods is critical for
video deblurring. However, CNN-based methods
show limitations in capturing long-range depen-
dencies and modeling non-local self-similarity.
In this paper, we propose a novel framework,
Flow-Guided Sparse Transformer (FGST), for
video deblurring. In FGST, we customize a self-
attention module, Flow-Guided Sparse Window-
based Multi-head Self-Attention (FGSW-MSA).
For each query element on the blurry reference
frame, FGSW-MSA enjoys the guidance of the
estimated optical ﬂow to globally sample spa-
tially sparse yet highly related key elements cor-
responding to the same scene patch in neighbor-
ing frames. Besides, we present a Recurrent Em-
bedding (RE) mechanism to transfer information
from past frames and strengthen long-range tem-
poral dependencies. Comprehensive experiments
demonstrate that our proposed FGST outperforms
state-of-the-art (SOTA) methods on both DVD
and GOPRO datasets and yields visually pleas-
ant results in real video deblurring. https://
github.com/linjing7/VR-Baseline

1. Introduction

Video deblurring is a fundamental yet challenging task in
low-level computer vision and graphics communities. It
aims to restore the latent frames from a blurry video se-
quence. Serving as a preprocessing technique, video deblur-
ring has wide applications such as video stabilization (Mat-
sushita et al., 2006), tracking (Jin et al., 2005), autonomous
driving (Yin et al., 2021), etc. Hand-held devices are more
and more popular in capturing videos of dynamic scenes,
where prevalent depth variations, abrupt camera shakes, and
high-speed object movements lead to undesirable blur in

*Equal contribution

1Shenzhen International Graduate
School, Tsinghua University 2Huawei Noah’s Ark Lab 3ETH
Correspondence to: Haoqian Wang <wanghao-
Z¨urich.
qian@tsinghua.edu.cn>, Xueyi Zou <zouxueyi@huawei.com>.

videos. To alleviate the effect of motion blur, researchers
have put a lot of efforts into video deblurring.

Conventional methods are mainly based on hand-crafted
priors and assumptions, which limits the model capacity.
Besides, the assumptions on motion blur and latent frames
usually lead to complex energy functions that are difﬁcult to
solve. Also, the inaccurately estimated motion blur kernel
with hand-crafted priors may easily result in severe artifacts.

In the past decade, video deblurring has witnessed signiﬁ-
cant progresses with the development of deep learning. Con-
volutional neural network (CNN) applies a powerful model
to learn the mapping from blurry videos to sharp videos un-
der the supervision of a large-scale dataset of blurry-sharp
video pairs. CNN-based methods yield impressive perfor-
mance but show limitations in modeling long-range spatial
dependencies and capturing non-local self-similarity.

Recently, the emergence of Transformer provides an alter-
native to alleviate the constraints of CNN-based methods.
Firstly, Transformer excels at modeling long-range spatial
dependencies. The contextual information and spatial cor-
relations are critical to restoring the motion blur. Secondly,
similar and sharper scene patches from neighboring frames
provide crucial cues for video deblurring. Fortunately, the
self-attention module in Transformer is dedicated to calcu-
lating the correlations among pixels and capturing the self-
similarity along the temporal sequence. Thus, Transformer
inherently resonates with the goal of learning similar infor-
mation from spatio-temporal neighborhoods. Nevertheless,
directly using existing Transformers for video deblurring
has two issues. On one hand, when the standard global
Transformer (Dosovitskiy et al., 2021) is utilized, the com-
putational cost is quadratic to the spatio-temporal dimen-
sions. This burden is nontrivial and sometimes unaffordable.
Meanwhile, the global Transformer attends to redundant
key elements, which may easily cause non-convergence is-
sue (Zhu et al., 2020) and over-smoothing results (Li et al.,
2019). On the other hand, when the local window-based
Transformer (Liu et al., 2021) is used, the self-attention is
calculated within position-speciﬁc windows, causing lim-
ited receptive ﬁelds. The model may neglect some key
elements of similar and sharper scene patches in the spatio-
temporal neighborhood when fast motions are present. We

 
 
 
 
 
 
Flow-Guided Sparse Transformer for Video Deblurring

Figure 1. The illustration of our ﬂow-guided self-attention mechanisms. (a) FGS-MSA globally samples spatially sparse yet highly related
key elements of similar and sharper patches in neighboring frames. (b) Instead of sampling a single key element on each neighboring
frame, FGSW-MSA robustly samples all the key elements corresponding to the query elements of the window on the reference frame.

summarize the main reason for the above problems, i.e.,
previous Transformers lack the guidance of motion infor-
mation, when calculating self-attention. We note that the
motion information can be estimated by optical ﬂow.

Exploiting an optical ﬂow estimator to capture motion infor-
mation and align neighboring frames is a common strategy
in video restoration (Makansi et al., 2017; Su et al., 2017;
Xue et al., 2019; Pan et al., 2020). Previous ﬂow-based
methods mainly adopt the pre-warping strategy. Specif-
ically, they employ an optical ﬂow estimator to produce
motion offsets, warp neighboring frames, and align regions
corresponding to the same object but misaligned in neigh-
boring image or feature domains. This scheme suffers from
the following issues: (i) The interpolating operations in the
warping module modify the original image information. As
a result, some critical image priors such as self-similarity
and sharp textures may be sacriﬁced. Undesirable artifacts
may be introduced to the restored video and the deblurring
performance may degrade. (ii) The frame alignment and
subsequent representation aggregation are separated. This
paradigm is inﬂexible and does not make full use of optical
ﬂow. Besides, the deblurring results are easily affected by
the performance of the optical ﬂow estimator. The robust-
ness of this scheme can be further improved.

This work aims to cope with the above problems. We pro-
pose a novel method, Flow-Guided Sparse Transformer
(FGST), for video deblurring. Firstly, we adopt Trans-
former instead of CNN as the deblurring model because
of its advantages of capturing long-range spatial dependen-

cies and non-local self-similarity. Secondly, to alleviate the
limitations of previous Transformers and the pre-warping
strategy, we customize Flow-Guided Sparse Multi-head Self-
Attention (FGS-MSA) as shown in Fig. 1 (a). For each
query element on the reference frame, FGS-MSA guided by
an optical ﬂow estimator globally samples spatially sparse
key elements corresponding to the same scene patch but mis-
aligned in the neighboring frames. These sampled key ele-
ments provide self-similar and highly related image prior in-
formation, which is critical to restoring motion blur. Differ-
ent from original global and local Transformers, our FGST
neither blindly samples redundant key elements nor suffers
from limited receptive ﬁelds. Meanwhile, our alignment
scheme is different from the pre-warping operation mainly
used by previous ﬂow-based methods. Instead of warping
the neighboring frames, our FGST samples key elements in
consecutive frames to calculate the self-attention. Thus, the
original image prior information can be preserved. Thirdly,
we promote FGS-MSA to Flow-Guided Sparse Window-
based Multi-head Self-Attention (FGSW-MSA) as shown in
Fig. 1 (b). The feature maps are split into non-overlapping
Instead of sampling a single key element on
windows.
each neighboring frame for a single query element, FGSW-
MSA samples key elements assigned by the optical ﬂow
corresponding to all the query elements of the window on
the reference frame. Thus, FGSW-MSA is more robust to
accommodate pixel-level ﬂow offset prediction deviations.
Finally, our FGSW-MSA is calculated within a short tem-
poral sequence reducing the computational cost. Hence, the
receptive ﬁeld of FGSW-MSA is spatially global but tempo-

Optical Flow OffsetsEmbeddingEmbeddingOptical Flow EstimatorFGS-MSA+Optical Flow OffsetsEmbeddingEmbeddingFGSW-MSA+Reference FrameNeighboring Frames(a) FGS-MSA(b) FGSW-MSAOptical FlowMapOptical FlowMapReference FrameNeighboring Framesti,jq,tijW,tijF,tijY(){},xyDD(){},xyDDtvoFOptical Flow EstimatoroF-1tvtv1t+vtv-1tvtv1t+vFlow-Guided Sparse Transformer for Video Deblurring

Figure 2. The architecture of FGST. (a) FGST consists of an encoder, a bottleneck, and a decoder. FGST is built up by FGABs. (b) FGAB
is composed of a layer normalization, an FGSW-MSA, and a feed-forward network. (c) RE aggregates the output of the last frame and the
input of the current frame. Some intermediate steps between FGABs are omitted. (d) The components of residual block.

rally local. Motivated by RNN-based methods (Nah et al.,
2019; Zhong et al., 2020), we propose Recurrent Embed-
ding (RE) to transfer information of past frames and capture
long-range temporal dependencies.

Our contributions can be summarized as follows:

• We propose a new method, FGST, for video deblurring.
To the best of our knowledge, it’s the ﬁrst attempt to
explore the potential of Transformer in this task.

• We customize a novel self-attention mechanism, FGS-

MSA, and its improved version, FGSW-MSA.

• We design an embedding scheme, RE, to transfer frame

information and capture temporal dependencies.

• Our FGST outperforms SOTA methods on DVD and
GOPRO datasets by a large margin and yields more
visually pleasing results in real-world video deblurring.

2. Related Work

2.1. Video Deblurring

In recent years, the deblurring research focus is shifting from
single image deblurring (Zoran et al., 2011; Chakrabarti,
2016; Purohit et al., 2020) to the more challenging video
deblurring (Cho et al., 2012; Matsushita et al., 2006). Tradi-
tional methods (Li et al., 2010; Zhang et al., 2013) are based
on hand-crafted image priors and assumptions, which lead
to limited generality and representing capacity. With the
development of deep learning, recent methods are mainly

CNN-based or RNN-based. (Zhang et al., 2018) employ 3D
convolutions to model spatio-temporal relations of frames.
(Hyun Kim et al., 2017) and (Nah et al., 2019) use RNN-
based models to restore the latent frames. However, CNN-
based methods show limitations in capturing long-range
dependencies while RNN-based methods are not sensitive
to patch-level spatial correlation and motion information.

2.2. Vision Transformer

Transformer is ﬁrstly proposed by (Vaswani et al., 2017)
for machine translation. Recently, Transformer has been
introduced to high-level (Dosovitskiy et al., 2021; Liu et al.,
2021; Zhu et al., 2020; Zheng et al., 2021; El-Nouby et al.,
2021; Carion et al., 2020; Li et al., 2021b; Ramachandran
et al., 2019; Wu et al., 2020; Cai et al., 2020) and low-
level vision (Chen et al., 2021; Cai et al., 2021b; Wang
et al., 2021; Cao et al., 2021b; Cai et al., 2021a; Hu et al.,
2021). (Arnab et al., 2021) factorize the spatial and temporal
dimensions of the input video and propose a Transformer
model for video classiﬁcation. (Chen et al., 2021) present
a large model IPT pre-trained on large-scale datasets with
a multi-task learning scheme. (Cao et al., 2021b) propose
VSR-Transformer that uses the self-attention mechanism
for better feature fusion in video super-resolution, but image
features are still extracted from CNN. (Wang et al., 2021)
use Swin Transformer (Liu et al., 2021) blocks to build up a
U-shaped structure for single image restoration. In (Vaswani

Residual BlockFGABPatch MergingPatch MergingFGABFGABResidual BlockFGABPatch ExpandingPatch ExpandingFGAB(a) FGST(b) FGAB++FFNFGSW-MSALNBlurryFramesRestoredFrames…(c) RecurrentEmbeddingEncoderDecoderBottleneckWCWarpingModuleConvLayer(d) ResidualBlockConvConvReLU++IdentityMapping×5×5×1×1×2×1×1V3THW0XTCHW1X222HWTC2X444HWTC'1X222HWTC'2X444HWTC'0XTCHWR3THW'V3THW222HWTCTCHW222HWTC222HWTC422HWTC222HWTCTCHWTCHW2TCHWTCHWConvConcatConvConcatFGABW𝑒𝑡−1𝑙𝑦𝑡−1𝑙−1CFGABW𝑒𝑡𝑙𝑦𝑡𝑙−1FGAB𝑒𝑡+1𝑙𝑦𝑡+1𝑙−1FGABFGABFGAB…𝑘𝑡−1𝑙……𝑞𝑡−1𝑙C𝑘𝑡𝑙𝑞𝑡𝑙C𝑘𝑡+1𝑙𝑞𝑡+1𝑙𝑦𝑡−1𝑙𝑦𝑡𝑙𝑦𝑡+1𝑙Flow-Guided Sparse Transformer for Video Deblurring

et al., 2021; Cao et al., 2021a; Liu et al., 2021), window-
based local self-attention is adopted to replace the global
self-attention module of the standard Transformer. However,
directly using previous global or local Transformers for
video deblurring leads to unaffordable computational cost
or limited receptive ﬁelds.

2.3. Flow-based Video Restoration

Optical ﬂow estimators are widely used in video restoration
tasks (Gast & Roth, 2019; Xue et al., 2019; Gong et al.,
2017; Sun et al., 2015; Makansi et al., 2017; Su et al., 2017;
Pan et al., 2020) to align highly related but mis-aligned
frames. Previous ﬂow-based video deblurring methods (Xue
et al., 2019; Makansi et al., 2017; Su et al., 2017; Pan et al.,
2020; Gast & Roth, 2019) mainly adopt the pre-warping
strategy, which ﬁrstly estimates the optical ﬂow and then
warps the neighboring frames. For example, (Su et al.,
2017) experiments with pre-warping input images based on
classic optical ﬂow methods to register them to the reference
frame. Nonetheless, this ﬂow-based pre-warping scheme
separates the frame alignment and subsequent information
aggregation. The original frame information is sacriﬁced
and the guidance effect of optical ﬂow is not fully explored.

3. Method

3.1. Overall Architecture

Figure 2 (a) shows the architecture of FGST that adopts the
widely used U-shaped structure, consisting of an encoder,
a bottleneck, and a decoder. Figure 2 (b) depicts the basic
unit of FGST, i.e., Flow-Guided Attention Block (FGAB).
The input is a blurry video V ∈ RT ×3×H×W , where T
denotes the sequence length, H and W denote the width
and height of the frame. Firstly, FGST exploits 5 residual
blocks to map V into tokens X0 ∈ RT ×C×H×W , where C
denotes the channel number. The details of residual block
are shown in Fig. 2 (d). Secondly, X0 passes through two
FGABs and patch merging layers to generate hierarchical
features. The patch merging layer is a strided 4×4 convo-
lution that downsamples the feature maps and doubles the
channels. Thus, the tokens of the ith layer in the encoder
are denoted as Xi ∈ RT ×2iC× H
2i . Thirdly, X2 passes
through the bottleneck, which consists of two FGABs.

2i × W

Subsequently, following the spirit of U-Net (Ronneberger
et al., 2015), we customize a symmetrical decoder, which is
composed of two FGABs and patch expanding layers. The
patch expanding layer is a strided 2×2 deconvolution that
upsamples the feature maps. To alleviate the information
loss caused by downsampling, skip connections are used for
feature fusion between the encoder and decoder.

After undergoing the decoder, the feature maps pass through
5 residual blocks to generate a residual frame sequence

R ∈ RT ×3×H×W . Finally, the deblurred video V(cid:48) ∈
RT ×3×H×W can be derived by V(cid:48) = V + R.

3.2. Flow-Guided Attention Block

As analyzed in Sec. 1, the standard global Transformer
brings quadratic computational complexity with respect to
the token number and easily leads to non-convergence issue
and over-smoothing results. The previous window-based
local Transformers suffer from the limited receptive ﬁelds.

To address these problems, we propose to use optical ﬂow as
the guidance to sample key elements from spatio-temporal
neighborhoods when calculating the self-attention. Based
on this motivation, we customize the basic unit, FGAB as
shown in Fig. 2 (b). FGAB consists of a layer normal-
ization (LN), a Flow-Guided Sparse Window-based Multi-
head Self-Attention (FGSW-MSA), a feed-forward network
(FFN), and two identity mappings. The FFN is composed of
5 consecutive residual blocks. In this part, we ﬁrst introduce
Flow-Guided Sparse Multi-head Self-Attention (FGS-MSA)
and then its improved version, FGSW-MSA.

i,j and kt

FGS-MSA. The details of FGS-MSA are shown in Fig. 1
(a). Given the tth input blurry video frame vt ∈ R3×H×W
i,j ∈ RC respectively
as the reference frame, qt
denote the query and key elements at the position (i,j) on vt.
FGS-MSA aims to model long-range spatial dependencies
and capture non-local self-similarity. To this end, FGS-MSA
produces keys from the key elements of similar and sharper
scene patches in the spatio-temporal neighborhood of vt.
The key sampling is directed by the motion information that
is predicted by an optical ﬂow estimator. This set of key
elements is corresponding to qt

i,j and we denote it as

Ωt

i,j = {kf

i+∆xf ,j+∆yf

(cid:12)
(cid:12) |f − t| ≤ r},

(1)

where r represents the temporal radius of the neighboring
frames. (∆xf , ∆yf ) denotes the value at position (i, j) of
the estimated motion offset map, which is predicted from
the reference frame vt to the neighboring frame vf :

(∆xf , ∆yf ) = [Fo(vt, vf ) (i, j)],

(2)

where Fo denotes the mapping function of the optical ﬂow
estimator and [·] refers to the rounding operation. Subse-
quently, FGS-MSA can be formulated as

FGS-MSA(qt

i,j, Ωt

i,j) =

N
(cid:88)

n=1

(cid:88)

Wn

k∈Ωt

i,j

Anqt

i,j k W(cid:48)

n k, (3)

where N is the number of the attention heads. Wn ∈ RC×d
n ∈ Rd×C are learnable parameters, where d = C
and W(cid:48)
N
denotes the representation dimension per head. Anqt
i,j k is
the self-attention of the nth head, which is formulated as

Anqt

(
i,j k = softmax

k∈Ωt

i,j

(qt

i,j)T UT
n Vnk
√
d

),

(4)

Flow-Guided Sparse Transformer for Video Deblurring

where Un and Vn ∈ Rd×C are learnable parameters. Given
an input V ∈ RT ×3×H×W , the computational cost of the
global MSA (Dosovitskiy et al., 2021) and FGS-MSA are

O(global MSA) = 4(T HW )C 2 + 2(T HW )2C,
O(FGS-MSA) = 2(T HW )C(cid:0)2(r + 1)C + 2r + 1(cid:1).

(5)

The standard global MSA leads to quadratic ((T HW )2)
computational complexity while our proposed FGS-MSA
contributes to much cheaper linear computational cost with
respect to the token number (T HW ). Detailed analysis are
provided in the supplementary material (SM).

FGSW-MSA. For each neighboring frame, FGS-MSA only
samples a single key element. When the optical ﬂow estima-
tion is inaccurate, the deblurring performance may be easily
affected. To further improve the robustness and reliability
of our method, we promote FGS-MSA to FGSW-MSA. As
shown in Fig. 1 (b), the feature maps are split into non-
overlapping windows. The spatial size of each window is
M × M . Φt
i,j denotes the set of query elements in the
window centering at position (i, j) of the tth frame:

Figure 3. The pre-warping strategy mainly adopted by previous
video deblurring methods sacriﬁcies the input image information.

two adjacent frames can reach 40 and 38 pixels on GOPRO
and DVD datasets. The input spatial size is 256×256. M is
set to 3. Thus, the receptive ﬁeld of FGSW-MSA can reach
83×83 (83 = 40×2+3) and 79×79 while that of W-MSA
is still 3×3. (ii) Unlike previous ﬂow-based methods that
adopt the pre-warping operation sacriﬁcing the original im-
age information as shown in Fig. 3, our FGST combines
motion cues with self-attention calculation. Thus, the origi-
nal image information can be preserved and the guidance
effect of the optical ﬂow can be further explored. In addi-
tion, our ﬂow-guided scheme enjoys higher ﬂexibility and
robustness because adjacent FGABs sample contents inde-
pendently. Please refer to the SM for detailed discussions.

Φt

i,j = {qt

m,n

(cid:12)
(cid:12) |m − i| ≤ M/2, |n − j| ≤ M/2}.

(6)

3.3. Recurrent Embedding

m,n ∈ Φt
For each qt
i,j, FGSW-MSA samples not only its
corresponding key elements in Ωt
m,n (Eq. (1)) assigned by
the ﬂow offsets but also the key elements corresponding to
other query elements in Φt
i,j. We denote the set of these
key elements as Ψt

i,j, which can be formulated as

Ψt

i,j =

(cid:91)

Ωt
|m−i|≤M/2, |n−j|≤M/2

m,n

.

(7)

Instead of attending to a single key element on each neigh-
boring frame for a single query, FGSW-MSA pays attention
to the key elements from similar and sharper scene patches
corresponding to all query elements in Φt
i,j. The attending
region is enlarged from pixel to window. Thus, FGSW-MSA
is more robust to accommodate pixel-level ﬂow prediction
deviations. FGSW-MSA can be formulated as

Our FGSW-MSA is calculated within a short temporal se-
quence for the computational complexity consideration (ap-
proximately linear to the temporal radius r in Eq. (9) ).
Therefore, the receptive ﬁeld of FGSW-MSA is temporally
local and overlooking the distant frames limits the video
deblurring performance. To further capture more robust
long-range temporal dependencies, we propose Recurrent
Embedding (RE) mechanism. RE is motivated by Recur-
rent Neural Network (RNN). More speciﬁcally, as shown in
Fig. 2 (c), we exploit RE in each Transformer layer to trans-
fer information from past frames and establish long-range
temporal correlations. With RE, the FGAB is calculated
in a recurrent manner for T time steps. yl
t re-
spectively denote the output, RE, query elements, and key
elements of the lth FGAB in the tth time step. We have

t, kl

t, el

t, ql

FGSW-MSA(Φt

i,j, Ψt

i,j) = {FGS-MSA(q, Ψt

i,j)|q ∈ Φt

i,j}.

Given the input V, the computational complexity is

O(FGSW-MSA) = 2(T HW )C(cid:0)C + (2r + 1)(C + M 2)(cid:1). (9)

(8)

The computational cost of FGSW-MSA is linear with re-
spect to the number of tokens (T HW ). Eq. (5) and (9)
reveal the high efﬁciency and resource economy of our
FGST. Please refer to the SM for more detailed analysis.

Discussion. (i) Our FGSW-MSA enjoys much larger re-
ceptive ﬁelds than W-MSA (Liu et al., 2021). Speciﬁcally,
according to Eq. (1), (2), (6), and (7), the receptive ﬁeld of
FGSW-MSA can cover the whole input feature map when
the estimated ﬂow offset is large enough. In practice, the mo-
tion offset predicted by the optical ﬂow estimator between

t = fw(yl
el
(cid:91)
kl

t−1), ql
yl−1
, yl
j
|j−t|≤r

t =

t = fc([el
t = FGAB(ql

t, yl−1
t
t, kl

]),

t),

(10)

where fw(·) represents the spatial warping that align the
feature map at t and t − 1 time step, [·,·] is the concatenating
operation, fc(·) denotes 3×3 convolution to aggregate the
recurrent embedding el
t and the output from last FGAB layer
yl−1
t

t) is formulated in details as

, and yl

t, kl

t = FGAB(ql
ol
t = FGSW-MSA(LN(ql
yl
t = FFN(ol

t) + ol
t,

t), LN(kl

t)) + ql
t,

(11)

where LN denotes the layer normalization and FFN refers to
the Feed Forward Network. Our RE sequentially propagates
the information from the ﬁrst frame to the last frame, thus
capturing reliable long-range temporal dependencies.

EstimatedFlowWarpedFrameOriginalFrameFlow-Guided Sparse Transformer for Video Deblurring

Figure 4. Visual comparisons between FGST and SOTA methods on DVD dataset (Su et al., 2017). Please zoom in for a better view.

Method

PSNR ↑
SSIM ↑

Kim and Lee

FGST
(Kim et al., 2015) (Gong et al., 2017) (Su et al., 2017) (Hyun Kim et al., 2017) (Zhou et al., 2019) (Xiang et al., 2020) (Pan et al., 2020) (Suin et al., 2021) (Li et al., 2021a) (Ours)

Xiang et al.

Gong et al.

Suin et al.

Kim et al.

Su et al.

STFAN

ARVo

TSP

26.94
0.816

28.27
0.846

30.01
0.888

29.95
0.869

31.15
0.905

31.68
0.916

32.13
0.927

32.53
0.947

32.80
0.935

33.36
0.950

Table 1. Video deblurring results compared with other methods on the DVD benchmark (Su et al., 2017). FGST achieves SOTA results.

4. Experiment

4.1. Datasets

DVD. The DVD (Su et al., 2017) dataset consists of 71
videos with 6,708 blurry-sharp image pairs. It is divided
into train/test subsets with 61 videos (5,708 image
pairs) and 10 videos (1,000 image pairs). DVD is captured
with mobile phones and DSLR at a frame rate of 240 fps.

GOPRO. The GOPRO (Nah et al., 2017) benchmark is com-
posed of over 3,300 blurry-sharp image pairs of dynamic
scenes. It is obtained by a high-speed camera. The training
and testing subsets are split in proportional to 2:1.

Real Blurry Videos. To validate the generality of FGST,
we evaluate models on real blurry datasets collected by (Cho
et al., 2012). Because the ground truth (GT) is inaccessible,
we only compare visual results of FGST and others.

4.2. Implementation Details

We implement FGST in PyTorch. We adopt a pre-trained
SPyNet (Ranjan et al., 2017) as the optical ﬂow estimator.
All the modules are trained with the Adam (Kingma & Ba,
2015) optimizer (β1 = 0.9 and β2 = 0.999) for 600 epochs.
The initial learning rate is set to 2×10−4 and 2.5×10−5
respectively for the deblurring model and optical ﬂow esti-
mator. The learning rate is halved every 200 epochs during
the training procedure. Patches at the size of 256×256
cropped from training frames are fed into the models. The
batch size is 8. The temporal radius r of the neighboring
frames is set to 1. The sequence length T is set to 9 in
training and the whole video length in testing. The horizon-
tal and vertical ﬂips are performed for data augmentation.
Peak signal-to-noise ratio (PSNR) and structural similarity
(SSIM) (Wang et al., 2004) are adopted as the evaluation
metrics. The models are trained with 8 V100 GPUs. L1 loss
between the restored and GT videos is used for supervision.

4.3. Quantitative Results

The comparisons between FGST and other SOTA methods
are listed in Tabs. 1, 2, and 3c. As can be observed: (i) Our
FGST outperforms SOTA methods by a large margin on the
two benchmarks. Speciﬁcally, as shown in Tab. 1, our FGST
surpasses the recent best algorithm ARVo (Li et al., 2021a)
by 0.56 dB on DVD. As reported in Tab. 2, our method
exceeds Suin et al. (Suin et al., 2021) and TSP (Pan et al.,
2020) by 0.80 dB and 1.23 dB respectively on GOPRO.
These results demonstrate the effectiveness of our method.
(ii) Tab. 3c exhibits efﬁciency comparisons of different algo-
rithms on GOPRO. The FLOPS is tested at the input size of
1×3×240×240. The running time per frame is tested at the
spatial size of 1,280×720 on the same RTX 2080 GPU. Our
FGST is more cost-effective and achieves a better trade-off
between PSNR, Params, FLOPS, and inference speed. For
instance, when compared to TSP (Pan et al., 2020), FGST
only requires 59.9% (9.70 / 16.19) Params and 36.8% (131.6
/ 357.9) FLOPS while achieving even 1.23 dB improvement
and 2.34× (579.7 / 247.8) speed. This evidence suggests
the promising efﬁciency advantage of our proposed FGST.

4.4. Qualitative Results

We provide visual comparisons on DVD, GOPRO, and real
blurry videos as shown in Figs. 4, 5, and 7. Previous meth-
ods are less favorable to restore abrupt motion blur. They
either yield over-smoothing images sacriﬁcing ﬁne textu-
ral details and structural contents or introduce redundant
blotchy texture and chromatic artifacts when fast motions
exists. In contrast, our FGST excels at modeling long-range
dependencies and exploits motion information to guide the
self-attention module to capture non-local self-similarity
in spatio-temporal neighborhoods. As a result, FGST is
capable of restoring structural contents and textural details
while preserving spatial smoothness of the homogeneous
regions. Supplementary ﬁle provides more visual results.

Blurry22.70/0.677EDVR24.04/0.753Kimetal.25.01/0.751SRN24.93/0.760STFAN27.40/0.845TSP29.41/0.896FGST(Ours)30.54/0.918Ground-TruthPSNR/SSIMFlow-Guided Sparse Transformer for Video Deblurring

Figure 5. Visual comparisons between our FGST and SOTA methods on GOPRO dataset (Nah et al., 2017). Zoom in for a better view.

Method

PSNR ↑
SSIM ↑

Gong et al.

FGST
STFAN
(Gong et al., 2017) (Hyun Kim et al., 2017) (Wang et al., 2019) (Su et al., 2017) (Zhou et al., 2019) (Nah et al., 2019) (Tao et al., 2018) (Pan et al., 2020) (Suin et al., 2021) (Ours)

Suin et al.

Kim et al.

Nah et al.

Tao et al.

Su et al.

EDVR

TSP

26.06
0.863

26.82
0.825

26.83
0.843

27.31
0.826

28.59
0.861

29.97
0.895

30.29
0.901

31.67
0.928

32.10
0.960

32.90
0.961

Table 2. Video deblurring results compared with other methods on the GOPRO dataset (Nah et al., 2017). FGST achieves SOTA results.

key elements, requiring a large amount of computation and
memory resources while leading to ambiguous gradients for
input features (Zhu et al., 2020) and thus non-convergence
problem. Meanwhile, features from global aggregation tend
to over-smooth the predictions of small patterns (Li et al.,
2019). (ii) When using local W-MSA (Liu et al., 2021), the
model gains by only 0.53 dB while adding 3.11M Params
and 64.16G FLOPS. The improvement is limited while the
additional burden is nontrivial. That is because W-MSA
calculates self-attention within position-speciﬁc windows.
The receptive ﬁeld is limited. (iii) Our FGS-MSA exploits
the optical ﬂow as the guidance to sample spatially sparse
keys of similar and sharper regions in the spatio-temporal
neighborhood for each query on the reference frame. Com-
pared to global MSA, the key elements of FGST are less
but highly related to the selected query. Thus, when us-
ing FGS-MSA, the model gains by 1.30 dB while adding
4.54M Params and 81.15G FLOPS. These results show that
FGS-MSA costs cheaper resources but achieves better per-
formance than global MSA. When exploiting FGSW-MSA,
the model yields an improvement of 1.72 dB while adding
4.55M Params and 87.69G FLOPS. This evidence suggests:
(a) FGSW-MSA is more effective than W-MSA in fast mo-
tion blur restoration. (b) FGSW-MSA is more reliable than
FGS-MSA and achieves better deblurring performance.

In addition, we conduct visual analysis on three adjacent
frames by visualizing the last feature map of models with
and without (w/o) FGSW-MSA in Fig. 6. Deeper color indi-
cates larger weights. It can be observed that the model with-
out FGSW-MSA responds weakly to similar regions in the
neighboring frames. In contrast, the model equipped with
FGSW-MSA generates much stronger responses to highly
related but misaligned scene patches. Moreover, FGST pays
more attention to the regions with fast motion blur. These
results demonstrates the effectiveness of FGSW-MSA in
capturing non-local self-similarity in dynamic scenes.

Flow-Guided Deformable Convolution. We compare our
FGSW-MSA with deformable convolution (DeConv) (Wang
et al., 2019) and recent ﬂow-guided deformable convolution

Figure 6. We visualize the last feature maps of the deblurring mod-
els with and without FGSW-MSA. The model using our FGSW-
MSA pays more attention to similar but misaligned scene patches.

4.5. Ablation Study

In this part, we conduct ablation studies on GOPRO dataset.
The baseline model is derived by directly removing all the
proposed RE and FGSW-MSA modules from our FGST.

Break-down Ablation. We ﬁrstly conduct a break-down
ablation to investigate the effect of each component toward
better performance. The results are reported in Tab. 3a. The
baseline model yields 31.18 dB. After applying RE and
FGSW-MSA respectively, the deblurring model achieves
1.16 dB and 1.66 dB improvements. While using both RE
and FGSW-MSA modules, the model gains by 1.72 dB. The
results suggest the effectiveness of RE and FGSW-MSA.

Self-Attention Mechanism. We compare our self-attention
mechanisms with other competitors in Tab. 3b. The baseline
model yields 31.18 dB while costing 5.15M Params and
43.93G FLOPS. (i) When using global MSA (Dosovitskiy
et al., 2021), the feature maps are downsampled into 1
4 size
and the channel is increased by 4 times to avoid out of mem-
ory and information loss. The deblurring model degrades
by 1.98 dB while costing 12.5× Params and 3.2× FLOPS.
This is mainly because global MSA attends to too redundant

Blurry29.68/0.914STFAN31.05/0.927EDVR31.81/0.952Nahetal.33.39/0.954SRN33.82/0.955TSP34.45/0.961FGST(Ours)35.93/0.969Ground-TruthPSNR/SSIMBlurryFramesw/o FGSW-MSA with FGSW-MSA 0.00.20.40.60.81.0𝒗𝒕#𝟏𝒗𝒕𝒗𝒕%𝟏Flow-Guided Sparse Transformer for Video Deblurring

Figure 7. Visual results of FGST and SOTA methods on the real blurry videos of (Cho et al., 2012). Please zoom in for a better view.

Baseline

RE

FGSW-MSA

PSNR ↑

SSIM ↑

Method

Baseline

Global MSA

Local W-MSA

FGS-MSA

FGSW-MSA

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

31.18 (+0.00%)

0.924 (+0.00%)

32.34 (+3.72%)

0.943 (+2.06%)

32.84 (+5.32%)

0.957 (+3.57%)

32.90 (+5.52%)

0.961 (+4.00%)

(cid:88)

(cid:88)

PSNR
SSIM
Params
FLOPS

31.18
0.924
5.15
43.93

29.20
0.880
64.40
138.68

31.71
0.938
8.26
108.09

32.48
0.944
9.69
125.08

32.84
0.957
9.70
125.67

(a) Break-down ablation study toward better performance.

(b) Ablation study of using different self-attention mechanisms.

Method

EDVR

Su et al.

STFAN

TSP

FGST (Ours)

Method

Baseline

+ DeConv

+ FGDeConv

+ FGSW-MSA

PSNR
Params (M)
FLOPS (G)
Time (ms/f)

26.83
23.60
159.2
268.5

27.31
15.30
38.7
133.2

28.59
5.37
35.4
145.9

31.67
16.19
357.9
579.7

32.90
9.70
131.6
247.8

PSNR
SSIM
Params (M)
FLOPS (G)

31.18
0.924
5.15
43.93

32.35
0.941
8.34
108.38

32.59
0.954
9.78
125.96

32.84
0.957
9.70
125.67

(c) Efﬁciency comparisons with SOTA CNN-based methods.

(d) FGSW-MSA v.s. FGDeConv and DeConv on GOPRO dataset.

Method Local W-MSA pre-warping FGSW-MSA

Win Size

1×1

2×2

3×3

4×4

5×5

Method Baseline

FlowNet

SPyNet

PWC-Net

PSNR
SSIM

31.71
0.938

32.54
0.953

32.84
0.957

PSNR
SSIM

32.48
0.944

32.62
0.955

32.90
0.961

32.71
0.955

32.66
0.957

PSNR ↑
SSIM ↑

31.18
0.924

32.85
0.960

32.90
0.961

33.03
0.964

(e) Pre-warping v.s. our FGSW-MSA.

(g) Ablation study of optical ﬂow estimators.
(f) Ablation study of window sizes.
Table 3. Ablation studies. The models are trained and tested on GOPRO. PSNR, SSIM, Params, FLOPS, and inference time are reported.

(FGDeConv) (Chan et al., 2021) in Tab. 3d. Our proposed
FGSW-MSA achieves the most signiﬁcant improvement.
This mainly stems from that FGSW-MSA excels at cap-
turing non-local similarity and long-range dependencies,
which are the limitations of CNN-based methods.

Pre-warping Strategy. We compare our FGSW-MSA with
the pre-warping strategy mainly adopted by previous meth-
ods in Tab. 3e. We start from the baseline model equipped
with W-MSA. It can be observed that using FGSW-MSA
is 0.30 dB and 0.004 in terms of PSNR and SSIM higher
than using pre-warping operation. This performance gap
is mainly because the model using our FGSW-MSA can
learn from non-corrupted representations of input video and
further explore the guidance effect of the optical ﬂow.

Window Size. We change the window size of FGSW-MSA
to study its effect. The results are listed in Tab. 3f. We
start by setting the window size at 1×1 and then gradually
increase it. The performance achieves its maximum when
the window size is 3×3. Thus, the optimal setting is 3×3.

Optical Flow Estimator. We adopt three representative
optical ﬂow estimators (FlowNet (Dosovitskiy et al., 2015),
SPyNet (Ranjan et al., 2017), and PWC-Net (Sun et al.,
2018)) to investigate their effects in Tab. 3g. (i) No matter
what ﬂow estimator is used, FGST reliably outperforms the

baseline model, suggesting the robustness and generality of
our method. (ii) The performance of FGST can be further
improved by using a better ﬂow estimator. To be speciﬁc,
when equipped with PWC-Net, FGST is 0.18 dB and 0.13
dB higher than those using FlowNet and SPyNet. These
results demonstrate that FGST can directly and conveniently
enjoy the beneﬁts of SOTA optical ﬂow estimators.

5. Conclusion

In this paper, we propose a novel Transformer-based method,
FGST, for video deblurring.
In FGST, we customize a
self-attention mechanism, FGS-MSA, and then promote
it to FGSW-MSA. Guided by an optical ﬂow estimator,
FGSW-MSA samples spatially sparse but highly related
key elements corresponding to similar and sharper scene
patches in the spatio-temporal neighborhoods. Besides, we
present an embedding scheme, RE, to transfer information
of past frames and capture long-range temporal dependen-
cies. Comprehensive experiments demonstrate that our
FGST signiﬁcantly surpasses SOTA methods and generates
more visually pleasant results in real video deblurring.

Acknowledgements: This work is partially supported by
the NSFC fund (61831014), the Shenzhen Science and Tech-
nology Project under Grant (CJGJZD20200617102601004,
JSGG20210802153150005).

BlurryEDVRKimetal.Kim&Leeetal.STFANSuetal.TSPFGST(Ours)Flow-Guided Sparse Transformer for Video Deblurring

References

Arnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇci´c, M.,
and Schmid, C. Vivit: A video vision transformer. arXiv
preprint arXiv:2103.15691, 2021.

Cai, Y., Wang, Z., Luo, Z., Yin, B., Du, A., Wang, H., Zhou,
X., Zhou, E., Zhang, X., and Sun, J. Learning delicate
local representations for multi-person pose estimation. In
ECCV, 2020.

Cai, Y., Hu, X., Wang, H., Zhang, Y., Pﬁster, H., and Wei, D.
Learning to generate realistic noisy images via pixel-level
noise-aware adversarial training. In NeurIPS, 2021a.

Cai, Y., Lin, J., Hu, X., Wang, H., Yuan, X., Zhang, Y.,
Timofte, R., and Van Gool, L. Mask-guided spectral-wise
transformer for efﬁcient hyperspectral image reconstruc-
tion. arXiv preprint arXiv:2111.07910, 2021b.

Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian,
Q., and Wang, M. Swin-unet: Unet-like pure trans-
former for medical image segmentation. arXiv preprint
arXiv:2105.05537, 2021a.

Cao, J., Li, Y., Zhang, K., and Van Gool, L. Video super-
resolution transformer. arXiv preprint arXiv:2106.06847,
2021b.

Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
A., and Zagoruyko, S. End-to-end object detection with
transformers. In ECCV, 2020.

Chakrabarti, A. A neural approach to blind motion deblur-

ring. In ECCV, 2016.

Chan, K. C., Zhou, S., Xu, X., and Loy, C. C. Basicvsr++:
Improving video super-resolution with enhanced propa-
gation and alignment. 2021.

Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z.,
Ma, S., Xu, C., Xu, C., and Gao, W. Pre-trained image
processing transformer. In CVPR, 2021.

Cho, S., Wang, J., Lee, S., b, and c. Video deblurring for
hand-held cameras using patch-based synthesis. In ACM
TOG, 2012.

Dosovitskiy, A., Fischer, P., Ilg, E., H¨ausser, P., Hazırbas¸,
C., Golkov, V., v.d. Smagt, P., Cremers, D., and Brox,
T. Flownet: Learning optical ﬂow with convolutional
networks. In ICCV, 2015.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
An image is worth 16x16 words: Transformers for image
recognition at scale. In ICLR, 2021.

El-Nouby, A., Touvron, H., Caron, M., Bojanowski, P.,
Douze, M., Joulin, A., Laptev, I., Neverova, N., Synnaeve,
G., Verbeek, J., et al. Xcit: Cross-covariance image
transformers. arXiv preprint arXiv:2106.09681, 2021.

Gast, J. and Roth, S. Deep video deblurring: The devil is in

the details. In ICCVW, 2019.

Gong, D., Yang, J., Liu, L., Zhang, Y., Reid, I., Shen, C.,
Van Den Hengel, A., and Shi, Q. From motion blur
to motion ﬂow: A deep learning solution for removing
heterogeneous motion blur. In CVPR, 2017.

Hu, X., Ma, R., Liu, Z., Cai, Y., Zhao, X., Zhang, Y., and
Wang, H. Pseudo 3d auto-correlation network for real
image denoising. In CVPR, 2021.

Hyun Kim, T., Mu Lee, K., Scholkopf, B., and Hirsch, M.
Online video deblurring via dynamic temporal blending
network. In ICCV, 2017.

Jin, H., Favaro, P., Cipolla, R., b, and c. Visual tracking in

the presence of motion blur. In CVPR, 2005.

Kim, H., Mu Lee, K., a, and b. Generalized video deblurring

for dynamic scenes. In CVPR, 2015.

Kingma, D. P. and Ba, J. L. Adam: A method for stochastic

optimization. In ICLR, 2015.

Li, D., Xu, C., Zhang, K., Yu, X., Zhong, Y., Ren, W.,
Suominen, H., and Li, H. Arvo: Learning all-range vol-
umetric correspondence for video deblurring. In CVPR,
2021a.

Li, K., Wang, S., Zhang, X., Xu, Y., Xu, W., and Tu, Z. Pose
recognition with cascade transformers. In CVPR, 2021b.

Li, X., Zhang, L., You, A., Yang, M., Yang, K., and Tong,
Y. Global aggregation then local distribution in fully
convolutional networks. In BMVC, 2019.

Li, Y., Kang, S. B., Joshi, N., Seitz, S. M., and Huttenlocher,
D. P. Generating sharp panoramas from motion-blurred
videos. In CVPR, 2010.

Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
S., and Guo, B. Swin transformer: Hierarchical vision
transformer using shifted windows. In ICCV, 2021.

Makansi, O., Ilg, E., Brox, T., b, and c. End-to-end learning
of video super-resolution with motion compensation. In
GCPR, 2017.

Matsushita, Y., Ofek, E., Ge, W., Tang, X., and Shum, H.-
Y. Full-frame video stabilization with motion inpainting.
TPAMI, 2006.

Flow-Guided Sparse Transformer for Video Deblurring

Nah, S., Hyun Kim, T., Mu Lee, K., and b. Deep multi-
scale convolutional neural network for dynamic scene
deblurring. In CVPR, 2017.

Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncell,
E. P. Image quality assessment: from error visibility to
structural similarity. TIP, 2004.

Nah, S., Son, S., Lee, K. M., and b. Recurrent neural
networks with intra-frame iterations for video deblurring.
In CVPR, 2019.

Wang, Z., Cun, X., Bao, J., and Liu, J. Uformer: A general u-
shaped transformer for image restoration. arXiv preprint
2106.03106, 2021.

Pan, J., Bai, H., Tang, J., and b. Cascaded deep video
In CVPR,

deblurring using temporal sharpness prior.
2020.

Purohit, K., Rajagopalan, A., b, and c. Region-adaptive
dense network for efﬁcient motion deblurring. In AAAI,
2020.

Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-
skaya, A., and Shlens, J. Stand-alone self-attention in
vision models. In NeurIPS, 2019.

Ranjan, A., Black, M. J., b, and c. Optical ﬂow estimation

using a spatial pyramid network. In CVPR, 2017.

Ronneberger, O., Fischer, P., Brox, T., a, and b. U-net: Con-
volutional networks for biomedical image segmentation.
In MICCAI, 2015.

Su, S., Delbracio, M., Wang, J., Sapiro, G., Heidrich, W.,
and Wang, O. Deep video deblurring for hand-held cam-
eras. In CVPR, 2017.

Suin, M., Rajagopalan, A. N., b, and c. Gated spatio-
temporal attention-guided video deblurring. In CVPR,
2021.

Sun, D., Yang, X., Liu, M.-Y., and Kautz, J. PWC-Net:
CNNs for optical ﬂow using pyramid, warping, and cost
volume. In CVPR, 2018.

Sun, J., Cao, W., Xu, Z., and Ponce, J. Learning a con-
volutional neural network for non-uniform motion blur
removal. In CVPR, 2015.

Tao, X., Gao, H., Shen, X., Wang, J., and Jia, J. Scale-
recurrent network for deep image deblurring. In CVPR,
2018.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention
is all you need. In NeurIPS, 2017.

Vaswani, A., Ramachandran, P., Srinivas, A., Parmar, N.,
Hechtman, B., and Shlens, J. Scaling local self-attention
for parameter efﬁcient visual backbones. In CVPR, 2021.

Wang, X., Chan, K. C., Yu, K., Dong, C., and Change Loy,
C. Edvr: Video restoration with enhanced deformable
convolutional networks. In CVPRW, 2019.

Wu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Yan, Z.,
Tomizuka, M., Gonzalez, J., Keutzer, K., and Vajda,
P. Visual transformers: Token-based image represen-
tation and processing for computer vision. arXiv preprint
arXiv:2006.03677, 2020.

Xiang, X., Wei, H., Wai, H., and Pan, J. Deep video de-
blurring using sharpness features from exemplars. In TIP,
2020.

Xue, T., Chen, B., Wu, J., Wei, D., and Freeman, W. T.
Video enhancement with task-oriented ﬂow. IJCV, 2019.

Yin, T., Zhou, X., Kr¨ahenb¨uhl, P., b, and c. Center-based 3d

object detection and tracking. In CVPR, 2021.

Zhang, H., Wipf, D., and Zhang, Y. Multi-image blind de-
blurring using a coupled adaptive sparse prior. In CVPR,
2013.

Zhang, K., Luo, W., Zhong, Y., Lin Ma, W. L., and Li, H.
Adversarial spatio-temporal learning for video deblurring.
In TIP, 2018.

Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y.,
Fu, Y., Feng, J., Xiang, T., Torr, P. H., and Zhang, L.
Rethinking semantic segmentation from a sequence-to-
sequence perspective with transformers. In CVPR, 2021.

Zhong, Z., Gao, Y., Zheng, Y., and Zheng, B. Efﬁcient
spatio-temporal recurrent neural network for video de-
blurring. In ECCV, 2020.

Zhou, S., Zhang, J., Pan, J., Xie, H., Zuo, W., and Ren, J.
Spatio-temporal ﬁlter adaptive network for video deblur-
ring. In ICCV, 2019.

Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. De-
formable detr: Deformable transformers for end-to-end
object detection. arXiv preprint arXiv:2010.04159, 2020.

Zoran, D., Weiss, Y., b, and c. From learning models of
natural image patches to whole image restoration.
In
ICCV, 2011.

