1
2
0
2

n
a
J

7
2

]

C
H
.
s
c
[

1
v
8
7
2
1
1
.
1
0
1
2
:
v
i
X
r
a

"Can I Touch This?": Survey of Virtual Reality Interactions via
Haptic Solutions
Revue de Littérature des Interactions en Réalité Virtuelle par le biais de Solutions Haptiques

Elodie Bouzbib
ISIR. Sorbonne Université
ISCD. Sorbonne Université
Paris, France

Gilles Bailly
Sinan Haliyo
ISIR. Sorbonne Université
Paris, France

Pascal Frey
ISCD. Sorbonne Université
Paris, France

ABSTRACT
Haptic feedback has become crucial to enhance the user experi-
ences in Virtual Reality (VR). This justifies the sudden burst of novel
haptic solutions proposed these past years in the HCI community.
This article is a survey of Virtual Reality interactions, relying on
haptic devices. We propose two dimensions to describe and com-
pare the current haptic solutions: their degree of physicality, as
well as their degree of actuation. We depict a compromise between
the user and the designer, highlighting how the range of required
or proposed stimulation in VR is opposed to the haptic interfaces
flexibility and their deployment in real-life use-cases. This paper
(1) outlines the variety of haptic solutions and provides a novel
perspective for analysing their associated interactions, (2) high-
lights the limits of the current evaluation criteria regarding these
interactions, and finally (3) reflects the interaction, operation and
conception potentials of "encountered-type of haptic devices".

CCS CONCEPTS
• Human-centered computing → Virtual reality; Haptic de-
vices; Interaction design theory, concepts and paradigms.

KEYWORDS
haptics, Virtual Reality, human factors, haptic devices

RÉSUMÉ
Le retour haptique est devenu essentiel pour améliorer l’expérience
utilisateur en Réalité Virtuelle (RV). C’est pourquoi nous observons
une explosion du nombre de solutions haptiques proposées ces
dernières années en IHM. Cet article est une revue de littérature
des interactions en RV s’appuyant sur des dispositifs haptiques.
Nous proposons deux dimensions pour décrire et comparer les so-
lutions haptiques : leur degré de physicalité ainsi que leur degré de
robotisation. Nous formulons un compromis utilisateur/concepteur,
reflétant la variété des stimulations requises/proposées en RV, en
opposition à la flexibilité des interfaces et leur déploiement en situa-
tion réelle. Ce travail (1) offre un panorama des solutions haptiques

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IHM ’20’21, April 13–16, 2021, Metz, France
© 2021 Association for Computing Machinery.
ACM ISBN XXX-X-XXXX-XXXX-X/XX/XX. . . $15.00
https://doi.org/10.1145/XXXXXXXXX.XXXXXXXX

en RV ainsi qu’un cadre d’analyse pour étudier les interactions
associées, (2) souligne les limites des critères d’évaluation actuels
pour ce type d’interactions, et finalement (3) reflète les potentiels
interactionnel, opérationnel et conceptuel des interfaces haptiques
"à contacts intermittents".

MOTS-CLÉS
haptique, Réalité Virtuelle, facteurs humains, dispositif haptique

ACM Reference Format:
Elodie Bouzbib, Gilles Bailly, Sinan Haliyo, and Pascal Frey. 2021. "Can I
Touch This?": Survey of Virtual Reality Interactions via Haptic Solutions.
In IHM ’20’21 : 32e conférence Francophone sur l’Interaction Homme-Machine,
April 13–16, 2021, Metz, France. ACM, New York, NY, USA, 16 pages. https:
//doi.org/10.1145/XXXXXXXXX.XXXXXXXX

1 INTRODUCTION
In the last few years, the terms "Virtual Reality" and "Haptics"
have been amongst the most quoted keywords in HCI conferences
such as ACM CHI or ACM UIST. Indeed, Head-Mounted Displays
(HMDs) are now affordable and provide high quality visual and
audio feedback, but augmenting the experience by enhancing VR
through the sense of touch (haptic feedback) has become a main
challenge. A large variety of haptic solutions has currently been
proposed, nonetheless they have highly different scopes, due to the
wide range of haptic features. It is hence difficult to compare their
similarities and differences and have a clear understanding of the
design possibilities.
In this paper, we present a survey of existing haptic interactions
in VR. We use the terms "haptic interactions" to emphasize the
focus on the users actions, and to analyse how the "haptic devices"
influence their behaviours in VR.

We provide a synthesis of existing research on haptic inter-
actions in VR and depict, from the required haptic features
stimulation and interaction opportunities, a design space dis-
cussing and classifying the associated haptic solutions according
to two dimensions: their degree of physicality, i.e. their physical
consistency and level of resemblance as to replicating an object, and
their degree of actuation, i.e. whether they rely on a motor-based
hardware implementation enabling autonomous displacements of
the interface (eg changing its shape or position) (Table 1).
This design space is useful to characterize, classify and compare
haptic interactions and the corresponding haptic solutions. We
also propose two criteria, User experience and Conception costs,
highlighting the implicit trade-offs between the quality of the user

 
 
 
 
 
 
IHM ’20’21, April 13–16, 2021, Metz, France

Bouzbib et al.

experience and the intricacy for the designer to implement these
solutions. Both of the user’s and designer’s perspectives are hence
considered in a novel framework to evaluate haptic interactions.
Finally, we illustrate the utility of our design space by analyzing
and comparing four haptic solutions. This analysis indicates that
(1) the use of real props in a virtual environment benefits the
user experience, but limits the interactions to the existing props
available within the VR arena; (2) the use of robotised interfaces
enables more various interactions; (3) combining them offers the
best user experience/design cost trade-off; (4) current evaluation
methods do not allow a fair representation and comparison of
haptic solutions.

We hence propose guidelines to evaluate haptic interactions
from both the user and designer perspectives. We also outline
how intertwining interfaces can expand haptic opportunities, by
conducting a deeper investigation on Robotic Graphics interfaces
[101] . Indeed, in the quest of the Ultimate Display [147], these
show (a) the largest variety of interactions, (b) the most reliable
interfaces through their automation, and (c) the most natural
interactions as they encounter the users at their positions of
interest without further notice.

2 BACKGROUND
Surveys in Virtual Reality consider the technology itself and its
limits [188, 191], or more specifically its use-case scenarios. VR is
indeed used in industries [20, 195], healthcare [103], or in gaming.
In gaming, the concerns are mainly regarding the evaluation pro-
tocols [102], ie the presence [130] and its related questionnaires
[131, 163]. Surveys for instance compare the results whenever the
questionnaires are asked in VR or in the real world [9, 116]. The
user behaviour in VR is also analysed, through gesture recognition
[123] or system control techniques (eg menus) [25].
The research areas are coincidentally almost similar in haptics.
Indeed, surveys analyse haptics themselves [164], haptic devices
[63, 117, 132, 152] or examine the scenarios which benefit from a
stimulation of the haptic cues. Haptics are used in telemanipulation
[53], for training in the industry [22, 177] or for healthcare purposes
[35, 118], or in gaming [78].

Finally, some surveys have been proposed at the intersection
of VR and haptics and focus either on specific methods (pseudo-
haptic feedback) [96], technology according to stimulated haptic
features (temperature, shape, skin stretch, pressure) [42, 169] or the
motivations and applications of each haptic device category [168].
In contrast our survey outlines the variety of haptic interactions
and technologies in VR and provides a framework to analyse them.

3 SCOPE AND DEFINITIONS
The scope of this article is to analyse how a single user interacts
and is provided with believable haptic feedback in Virtual Reality
[97]. We thus define the terms "virtual reality" and "haptics" and
how they are related.

3.1 Virtual Reality
Virtual reality corresponds to a 3D artificial numeric environment
in which users are immersed in. The environment can be projected

onto a large screen, in a simulation platform for instance, or multiple
ones, such as with CAVE technology (where the image is projected
onto at least 3 distinct walls of a room-scale arena). In this survey,
we consider an artificial reality [172] where users do not perceive
their physical vicinity: the outside world is not noticeable and users
are fully immersed through a head-mounted display (HMD). For
instance, augmented reality (AR), where the physical environment
is augmented with virtual artefacts, is out of our scope.
Through a Head Mounted Display (HMD), Virtual reality creates
immersive experiences for the users. These are only limited by
the designers’ imagination, and are evaluated through presence.
Presence is defined as the "subjective experience of being in one
place, even when one is physically situated in another" [139, 175].
It quantifies the users’ involvement and naturalness of interactions
through control, sensory, distraction and realism factors. This heav-
ily relies on the sensory input and output channels, however, as
VR was mainly integrating audio and visual cues, quantifying the
haptic contribution in an experience remains difficult.

3.2 Haptics: Tactile vs Kinesthetic Perception
Haptics is the general term for the sense of touch. They are a
combination of two cues: tactile and kinesthetic. The tactile cues
are developed through the skin, while the kinesthetic ones come
from proprioception and are through the muscles and the tendons.

3.2.1 Tactile cues: The skin is composed of
four types of
mechanoreceptors [87]. The first ones, "Merkel nerve endings",
transmit mechanical pressure, position and shapes or edges. They
are stimulated whilst reading Braille for instance. The second ones,
"Ruffini corpuscle end-organ", are sensitive to skin stretch and pro-
vide both pressure and slippage information. The third ones are
the "Pacinian corpuscles", which are sensitive to vibration and pres-
sure. The last ones, "Meissner’s corpuscles", are highly sensitive
and provide light touch and vibrations information. It also contains
thermoreceptors, which transmit information about temperature:
the Ruffini endings respond to warmth, while the Krause ones de-
tect cold. Through tactile cues, the human can hence feel shapes or
edges, pressure, vibrations or temperature changes.

3.2.2 Kinesthetic cues: The kinesthetic cues rely on propriocep-
tion, ie the perception and the awareness of our own body parts
positions and movements. Mechanoreceptors into the muscles, the
"spindles", communicate to the nervous system information the
forces muscle generate, as well as their length change [77]. The
primary type of spindle is sensitive to the velocity and acceleration
of a muscle contraction or limb movement, while the second type
provides information about static muscle length or limb positions.
Kinesthetic cues hence allow to feel forces, as well as perceiving
weights or inertia.

3.3 VR & Haptics
Whenever we touch or manipulate an object, the combination of
these two previous cues allows to understand its material, but
also its shape and the constraints it implies to the user. On the
one side, adding physical presence [92] through haptic feedback
in VR enhances the users’ immersion, even at an emotional and
physiological scale: the heart rate of a user can literally increase

"Can I Touch This?"

IHM ’20’21, April 13–16, 2021, Metz, France

with the use of haptics through real objects [73]. Haptics are also
required for interacting with the environment: the user needs to
control the changes in the environment [66] and to be aware of the
modifications he physically has made (eg moving virtual objects,
pushing a button). On the other side, haptics can benefit from VR.
For instance, Lécuyer et al. leverage the users vision and analyse
how it affects their haptic feedback [96]. This approach, "pseudo-
haptic feedback", tricks the users’ perception into feeling virtual
objects’ stiffness, texture, mass. Many more haptic features can be
stimulated, such as temperature, shape, skin stretch, pressure.

4 ANALYZING HAPTIC INTERACTIONS
The main objective of this survey is to provide analytical tools to
evaluate and compare haptic interactions.

4.1 Design space
We propose a two-dimension framework to discuss and classify
haptic solutions in VR (see Table 1).

The first dimension is their degree of physicality, ie how the
haptic perception is tangible/physically consistent/resembling with
the virtual objects. This dimension is drawn as a continuum, from
"no physicality" to "real objects" (see Figure 2). We find that this
continuum can be discretised as a two-category section: whether
they use real objects or not.

The second orthogonal dimension is their degree of actuation,
ie whether haptic solutions rely on a motor-based hardware im-
plementation enabling autonomous displacements (eg enabling to
change its shape, position etc).

4.2 Analysis criteria
We consider two main criteria to analyse haptic interactions in VR.
They cover both the user and designer perspectives.

The User experience is the first criterion and includes
two aspects: interaction opportunities and visuo-haptic consis-
tency/discrepancy. Interaction opportunities represent to which
extent haptic solutions allow users to interact/act (e.g navigate, ex-
plore, manipulate) in a VR scene as opposed as in the real world.
Visuo-haptic consistency/discrepancy refers to the tactile and kines-
thetic perceptual rendering of these interactions. These two sub-
criteria are complementary focusing on both action and perception.
The second criterion is the conception cost, i.e. the challenges
Designers should address when designing haptic interactions. We
distinguish implementation and operation costs. Implementation
costs include several technical aspects related to the acceptability
of a haptic solution such as safety, robustness and ease-of-use [42].
Operation costs include the financial and human costs required to
deploy these technologies.

4.3 Application
We rely on this design space and criteria to highlight and under-
stand the trade-offs between the user’s interactions opportunities
in VR, and the designers’ challenges in conception. This survey
offers a novel perspective for researchers to study haptic interac-
tions in VR. It can be used to compare and analytically evaluate
existing haptic interactions. For a given application, designers can
evaluate the most adapted haptic interaction. For a given technique,

Table 1: We propose two dimensions to classify current tech-
nologies: their degrees of physicality and actuation.

they can evaluate a haptic solution depending on their needs (tasks,
workspace, use-cases etc).
.We first discuss haptic interactions from the User perspective (Sec-
tion 5 - Interaction opportunities, Section 6 - Visuo-Haptic Con-
sistency/Discrepancy). We then adopt the designer perspective in
Section 7. We use our design space on Sections 6 and 7, which
emphasize haptic solutions.

5 INTERACTION OPPORTUNITIES
In the real world, users move freely without constraints, pick
any object of their environment and then interact with their
bare-hands. They also can be interacted with, from the environ-
ment (wind, unexpected obstacles) or from other users, for instance
to catch their attention or to lead them somewhere. A natural envi-
ronment also naturally physically constrains users through their
entire body.

In this section, we discuss the interaction opportunities in VR
and the methods available to provide them. In particular, we discuss
them through four main tasks: navigation, exploration, manipula-
tion and edition.

5.1 Navigation
We qualify a navigation task as the exploration of the environment
through the vision and the ability to navigate through it via the
users displacements. We identify three main techniques to navigate
in VR. The two firsts rely on controllers and push buttons, where
the users do not necessary physically move. The last one is more
natural as it allows the users to walk in the VR arena.

5.1.1 Panning: With grounded desktop haptic solutions, such as
the Virtuose [62], users need to push a button to clutch the devices
and hence move within the environment.

5.1.2 Point & Teleport: With ungrounded solutions, such as con-
trollers, the common technique is teleportation. Users point their
controllers [14] to predetermined teleportation target areas, and
are displaced in position but also in orientation [51] (Figure 1 - 1).

Robotics & ActuationReal ObjectsYesYesNoNoShape Simulation Pseudo-Haptics Visuo-Haptic Illusions Object Primitives Surface Haptic DisplaysPassive Haptics Visuo-Haptic Illusions Pseudo-Haptics Human Actuators Real Props ReassignmentDesktop Haptic Interfaces 2.5D Tabletops Shape-Changing Roboxels Controllers Wearables Electro-Muscle Stimulation Mid-Air Haptics Inﬂatable Room FloorRobotic Shape Displays •Cartesian Robot •Robotic Arm •Swarm Robots •Mobile Platforms •Drones •Merry-Go-Round  PlatformPhysicalityActuationIHM ’20’21, April 13–16, 2021, Metz, France

Bouzbib et al.

Figure 1: Tasks in VR: (1) Navigation through Point & Teleport [51]; (2) Navigation through a building, using redirection
[27]; (3) Exploration with Bare-Hands: A user finds an invisible haptic code [24]; (4) Manipulation: Haptic proxies rearrange
themselves to form a plane the user can manipulate [190]; (5) Edition: the user changes the shape of a haptic 2.5D tabletop
[105]; (6) The user is interacted with by a robotic arm to feel emotions [155].

5.1.3 Real Walking: Real walking in VR, "perambulation", has
shown the best immersion and presence results [142, 162] because
it relies on proprioception and kinesthetic feedback through the
legs and gait awareness. Nonetheless, VR arenas are not infinite and
HMD have a limited tracking space, hence methods need to be devel-
oped for the user to be able to move to any location of interest. One
approach is to mount the previously discussed grounded desktop
haptic solutions over mobile [49, 88, 89, 106, 111, 126] or wearable
[17] interfaces. Users however still have to continuously maintain
the handle in their palm. Other interfaces hence allow for free-
hands Room-Scale VR [24, 170, 181]. For the users to perambulate
in an infinite workspace, the virtual environment can also visually
be warped for the users to unconsciously modify their trajectory or
avoid obstacles [27, 119, 179] (Figure 1 - 2). This infinite redirection
can also be provided from Electro-Muscle Stimulation (EMS) on
the users’ legs [12], with wearable electrodes. The user can also
wear actuated stilts to perceive staircases [129] or a vibrating shoe
to perceive virtual materials [145]. To remain unencumbered from
these wearable techniques, the VR arena can also include robotised
techniques: users can for instance walk on treadmills [50, 166], or
on movable platforms that encounter their feet [74, 75].

5.2 Hand Interactions
In the real world, bare-hands interaction is important to execute
everyday tasks (exploration, manipulation, edition). However, in
VR, users commonly have to hold controllers, wearables or handles,
which create a discrepancy between what the users feel and see
[182]. These exploit the God-object principle [194], as opposed to
bare-hands Real-touch interactions.

5.2.1 God-Object: The controller is considered as a continuity of
the users’ hands, represented by a proxy that does not undergo
physics or rigid collisions, and is attached to a complementary rigid
object with a spring-damper model. This latter hence moves along
with the proxy, but is constrained by the environment. Whenever
it does collide with an object of interest, the users perceive the
previous spring-damper stiffness through kinesthetic feedback.
Users hence interact though a proxy, like a desktop mouse, which
position is not co-located with the users’ vision. Bare-hands in-
teractions are not necessarily needed depending on the use-cases.
For instance, in healthcare and surgery training, users are more

likely to interact with a tool, such as a scalpel or a clamp. Continu-
ously holding the god-object is hence not a constrain, however the
co-location of vision and haptics is recommended [109].

5.2.2 Real Touch: In other scenarios, such as gaming, industry or
tool training [143, 174], using the appropriate tools through props
and real objects is more natural. The users however need to be able
to reach them whenever required. Some interfaces (e.g. Robotic
Graphics; see Section 7.3) are hence developed in these regards, to
encounter the users whenever they feel like interacting.

5.3 Exploration
As opposed to the previous definition of "navigation", based on
vision cues, an "exploration" task consists in the ability to touch the
environment and understand its constraints. Exploring thoroughly
an environment in VR can be done through different haptic features,
and can improve the users depth perception [98] or distances to an
object. The different methods for exploring the environment are
detailed in Section 6.
Whenever a user is exploring the environment, shapes or textures
are felt through his body displacements. He needs to move for his
skin to stretch (through tactile cues) or his muscles to contract
(through kinesthetic cues).

5.3.1 Through Tactile cues: Whenever real props or material
patches are available, users can naturally interact with their finger-
tips to feel different materials [11, 41], textures [19, 93], tempera-
tures [192] or to feel shapes and patterns through their bare-hands
[24, 30] (Figure 1 - 3). When no physicality is available, a stimu-
lation can still be performed. As seen in Surface haptic displays
[18], vibrations between 80 to 400 Hz are felt through the skin,
hence users perceive stickiness, smoothness, pleasure, vibration or
friction, and for instance explore a 3D terrain or volumetric data
[137]. Vibrations can then be combined with auditory and vision
cues to render collisions in VR [23].

5.3.2 Through Kinesthetic cues: Exploring the environment can
also be done through kinesthetic cues: the users can literally be
physically constrained to feel a wall, using electro-muscle stim-
ulation (EMS) for instance [95]. With the god-object principle,
users can also explore the environments’ constraints through force-
feedback. In this configuration, the users’ arms are constrained

MateriableVRoamerZhao FollmerExplorationNavigationManipulationEditionBeing Interacted With213456"Can I Touch This?"

IHM ’20’21, April 13–16, 2021, Metz, France

by haptic desktop interfaces, providing strong enough forces to
simulate a physical collision and discriminate shapes.

5.4 Manipulation
A manipulation task is performed whenever modifying the position
and orientation of an object.

5.4.1 Direct Manipulation: In VR, we distinguish the direct ma-
nipulation [26], "the ability for a user to control objects in a virtual
environment in a direct and natural way, much as objects are manip-
ulated in the real world" from pointing/selecting an object with con-
trollers. A direct manipulation relies on the ability to hold an object
with kinesthetic feedback, feel its weight [67, 95, 124, 134, 186, 187],
shape [48, 85, 146], and constrains from the virtual environment, for
instance when making objects interact with each other [24]. Chang-
ing a virtual object position or orientation can be used as an input
in the virtual environment: in [190] for instance, the user modifies
a light intensity by moving a handle prop in the real environment.
By transposing [94] in VR, an object could even communicate its
dynamic use to the user.

5.4.2 Pseudo-Haptic Manipulation: Leveraging vision over haptics
allows to move an object with different friction, weights or force
perceptions [115, 120, 121, 125]. For instance, visually reducing the
speed of a virtual prop displacement leads to an increase in the users’
forces to move it, modifying their friction/weight perceptions.

5.5 Edition
We qualify an Edition task as a modification of an object property,
other than its orientation or position (for example through its scale
[176] or shape).

5.5.1 Physical Edition: Editing an interface in VR requires it to be
fully equipped with sensors. With wearables for instance, the hand
phalanges positions are known, and can be tightly linked with an
object property [165]. Knowing their own position, modular inter-
faces can be rearranged to provide stretching or bending tasks [46],
or be pushed on with a tool to reduce in size [154].
Shape-changing interfaces have been developed to dynamically
modify material properties [105] (Figure 1 - 5) or augment the inter-
actions in Augmented Reality (AR) [91], however these techniques
only consider HMDs and VR as future work directions.
These interfaces are relevant as 2.5D tabletops are already used
in VR. Physically editing the virtual world through them could be
implemented in a near future, by intertwining these interfaces with
3D modelling techniques [38].

5.5.2 Pseudo-Haptic Edition: The difficulty behind changing a real
object property is to track it in real-time. This is why pseudo-
techniques are relevant: they visually change the object properties
such as their shape [7], compliance [90, 136], or their bending
curvature [68] without physically editing the object.

5.6 Scenario-based Interactions
In the real world, humans are free to interact with any object with-
out further notice. In this regard, common controllers enable inter-
actions with any object through pointing, but they display a high
visuo-haptic discrepancy. In more advanced haptically rendered

Virtual environments, users are often constrained to scenario-based
interactions: only a few interactable objects are available, accord-
ingly with the scenario’s progress.
The greater the virtual:physical haptic consistency, the harder it is
to enhance non-deterministic scenarios, where the user is free to
interact with any object with no regards to the scenario’s progress.
High quality haptic rendering in non-deterministic scenarios can
be achieved through three methods: (a) numerous objects and prim-
itives are available for interactions [69]; (b) the users’ intentions
are to be predicted prior to interaction to make it occur [24, 30];
(c) props modify their own topology to match the users expected
haptic rendering [138].

5.7 Environment-Initiated Interactions
In both real and virtual environments with tangible interfaces, users
usually are the decision makers and get to choose their points of
contact during the next interaction. However, users themselves can
be considered as tangible interfaces: uncontrolled interactions, such
as being touched by a colleague, or feeling a temperature change
in the environment [133, 192], are part of everyday interactions
that can be transposed in Virtual Reality. Replicating a social touch
interaction in VR for instance increases presence [71] or invokes
emotions [155].
This type of interactions are recurrent in sports simulations, where
the user is undergoing forces from his environment and perceiv-
ing impacts (jumping into space [58], shooting a soccer ball [167],
goalkeeping in a soccer game [157], paragliding [180], intercepting
a volleyball [60], flying [29]).
These interactions are involving multiple force types: tension, trac-
tion, reaction, resistance, impact that help enhancing the user ex-
perience in VR [170]. These can be strong enough to even lead the
user through forces [24].

5.8 Whole-Body Involvement
All the previous subsections evoke interactions that mainly in-
volve the hands or the fingers. This paradigm is revoked in [193]:
a user should be able to choose his posture. This is currently only
enabled in room-scale VR applications, where users experience sit-
ting, standing, climbing or crouching [24, 36, 148, 154] and interact
with their whole-body.

6 VISUO-HAPTIC

CONSISTENCY/DISCREPANCY

Visuo-Haptic Consistency is the second aspect of the user experi-
ence. We exploit the dimension degree of physicality of our design
space (Table 1) to discuss the different haptic solutions. In particular,
we distinguish whether these solutions use real objects (exploiting
real objects) or not (simulating objects).

6.1 Simulating Objects
Object properties that need to be simulated are their shape, texture,
temperature, weight.

6.1.1 No Physicality, (Figure 2 - 1). Currently, grounded haptic
devices such as the Virtuose [62] or the PHaNToM [100] simulate
objects through their shapes (Figure 2 - 1). The rendering is only

IHM ’20’21, April 13–16, 2021, Metz, France

Bouzbib et al.

Figure 2: Degree of physicality continuum in VR.(1) Haptic desktop devices enable to explore the environment through a
handle [89] with the god-object principle; (2) A controller [19] or (3) a wearable [45] simulate objects for exploration tasks; (4)
Mid-air technology [117] create vibrations through the user’s hand to simulate an object; (5) Passive proxies are oriented for
the user to feel objects’ primitives with their hands [30]; (6) Objects from the environment are assigned to virtual props with
the same primitives [69]; (7) Real objects or passive props can be manipulated and interacted with each other [24].

done through kinesthetic feedback via a proxy. Conceptually, the
ideal link between the users and this proxy is a massless, infinitely
rigid stick, which would be an equivalent to moving the proxy
directly [63, 127]. These solutions only provide stimulation at the
hand-scale, with no regards to the rest of the body.

Shape Simulation, (Figure 2 - 2-3-4). In the same regard,
6.1.2
gloves or controllers provide some physicality (Figure 2 - 2-3).
Gloves or exoskeletons literally constrain the users hands for simu-
lating shapes [2, 6, 8, 10, 32, 33, 45, 57, 104, 114, 158], or stimulate
other haptic features such as stiffness, friction [165] or slippage
[156]. These can be extended to overall body suits for users to feel
impacts or even temperature changes [3, 37], or even intertwined
with grounded devices to extend their use-cases [141].
Customised controllers are currently designed to be either stimulat-
ing the palm [39, 146, 185] (Figure 3 - 1, 2), or held in the palm while
providing haptic feedback on the fingertips. For instance, [173]
proposes interchangeable haptic wheels with different textures or
shapes, while [19] enables textures and shapes and [90] displays
compliance changes. In these configurations, users hold a single
controller, however bi-manual interactions can be created by com-
bining two controllers. Their link transmits kinesthetic feedback,

Figure 3: Simulating Objects. (1) A controller with an inflat-
able prop in the user’s palm simulates holding a bomb [153].
(2) A pin-based interface shaped as a ball interacts in the user
palm to replicate a hamster [185]. (3) Different primitives
(ball, cube, pyramid) are displayed on a 2.5D tabletop [138].

and constrain their respective positions to each other [144, 171].
Contactless technology has also been developed for simulating
shapes. While studies demonstrated that interacting with bare-
hands increased the user’s cognitive load [52], combining bare-
hands interactions with haptic feedback actually enhances the users
involvement. Since haptic feedback does require contact, "contact-
less" technology defines an interaction where the users are unen-
cumbered, as per Krueger’s postulate [172], and ultrasounds are
sent to their hands, for them to perceive shapes on their skin, with-
out a physical prop contact [117] (Figure 2 - 4).
These unencumbered methods are also achieved through shape-
changing interfaces, for instance with balloons arrays [151] or 2.5D
tabletops (Figure 3 - 3, Figure 1 - 5) [48, 76, 138]. These latter are
constituted from pins, that raise and lower themselves to replicate
different shapes. In the same regard, swarm interfaces rearrange
themselves to display different shapes. These have mainly been
developed in the real world [43, 79, 86, 99, 149, 150] but slowly take
off as VR user interfaces [190] (Figure 1 - 4). Indeed, while these lat-
ter devices are used as desktop interfaces, the swarm robot idea has
extended to the air, with drones for instance [54, 70, 81, 122, 160].
All of these previous interfaces embrace the Roboxel principle
enunciated in Robotic Graphics [101]: "cellular robots that dynami-
cally configure themselves into the desired shape and size".

6.1.3 Object Primitives, (Figure 2 - 5). Finally, a user can interact
with object primitives. These represent the simplest geometries
available: circle, cube, pyramid, cylinder, torus. Simply feeling an
orientation through the fingertips provides the required informa-
tion to understand an object shape, in an exploration task for in-
stance. Panels with diverse orientations can hence be displayed
for a user to explore various objects in a virtual environment [30]
(Figure 2 - 5) or directly encounter the user at their position of
interest [183, 184].
On the opposite, a bare-hands manipulation task requires multiple
primitives to be available at the same time within the hand vicinity.
This is why the exploitation of real objects is necessary.

6.2 Exploiting Real Objects
Passive haptics [73], ie the use of passive props, consist in placing
real objects corresponding to their exact virtual match at their vir-
tual position. Insko demonstrated that passive haptics enhanced

Real ObjectsObject PrimitivesShape SimulationNo Physicality2134567213"Can I Touch This?"

IHM ’20’21, April 13–16, 2021, Metz, France

the virtual environment [73]. Nonetheless, this does suffer from a
main limitation: substituting the physical environment for a virtual
one [135] requires a thorough mapping of objects shapes, sizes,
textures, and requires numerous props [110]. This can be done with
real objects in simulation rooms for instance (e.g plane cockpit, mo-
torcycle), but cheaper methods need to be implemented to facilitate
their use in other fields.

6.2.1 Object Primitives, (Figure 2 - 6). One solution is to extract
the primitives of the objects that are already available in the physi-
cal environment, to map virtual objects of the approximate same
primitive over them [69] (Figure 2 - 6).

6.2.2 Visuo-Proprioceptive Illusions & Pseudo Haptics. The number
of props within the environment can also be reduced, while letting
the users interact at different positions of the physical world. It is
possible to leverage the vision over haptics and modify the users’
proprioception to redirect their trajectory [13, 56, 61, 82–84]. A
user might perceive multiple distinct cubes for instance, while
interacting with a single one. On the same principle, the user hand
displacement can be redirected at an angle, up-/down-scaled [4,
21], or slowed down for friction or weight perception [113, 125].
These techniques also allow for the exploration and manipulation of
various shapes: models can for instance be added to enable complex
virtual shapes to be mapped over real physical objects boundaries
[189]. The user can also be redirected to pinch a multi-primitive
object (cubic, pyramidal and cylindrical) from different locations,
which theoretically widens the variety of available props with a
single one [40]. On the same principle, pseudo-haptics allow to
modify the users’ shape [15, 16] or texture [41] perceptions when
interacting with a physical prop.

6.2.3 Displacing Objects, (Figure 2 - 7). Whenever objects are in-
deed available within the environment, various directions are avail-
able to displace them. This displacement allows for mapping one
physical object over multiple ones, but also to display a multitude of
props. These directions embrace the Robotic Shape Display principle
from Robotic Graphics [101]: "a robot that can reach any location
of a virtual desktop with an end-effector" and matches the user’s
object of interest.
Their usability have been validated through a Wizard-of-Oz im-
plementation, where human operators move real objects or even
people around a Room-scale VR arena to encounter the users [31]
(Figure 4 - 2). The users themselves can also reconfigure and actuate
real props [28].
Robotic Shape Displays, RSDs, are also called encountered-type
of haptic devices, as they literally encounter the users at their
object of interest to provide haptic feedback. They allow to dis-
play real pieces of material [5, 11], physical props to simulate walls
[24, 80, 178], or even display furniture [148] or untethered objects
[24, 64, 65, 72], that can be interacted with each other.

7 CONCEPTION COST
In practice, designers have to trade-off their interaction design
space with implementation and operational costs in the conception
phase. Implementation costs include technical aspects related to
the acceptability of an haptic solution such as safety, robustness
and ease-of-use [42]. For instance, actuated haptic solutions require

a special attention regarding this criterion. Operation costs include
the financial and human cost for using a haptic solution. The fi-
nancial cost is measured through the cost of the haptic device and
additional elements such as motion capture systems to precisely
track the users’ hand or the prior preparation of required props.
Human cost refers to both labour time and number of human oper-
ators required during the user’s interactions. For instance, actuated
haptic solutions generally do not require human operators (low
human cost) but might be mechanically expensive.

In this section, we use our two-dimension design space (Table
1) to discuss haptic solutions according to their conception cost.
As non-actuated solutions globally share the same approaches and
have a low implementation cost, we discuss them together in the
"No Robotics" subsection.

7.1 No Robotics
Regarding implementation costs, all non-actuated haptic solutions
are safe, robust and easy-to-use. We depict here an important design
choice when opting for these solutions: either the designer relies
on graphics solutions, leveraging vision cues over haptic ones,
or needs operators to displace or change the interactable props
(see Table 1).

7.1.1 Passive Props. Passive props [73] only consist in placing real
objects corresponding to their exact virtual match at their virtual
position. They provide a natural way of interacting through the
objects’ natural affordances [107]. They however are limited to the
available objects within the scene as they are not actuated. They
only can be used in a scenario-based experience, where the target
is known in advance. The environment hence requires a prop for
each available virtual object.

7.1.2
Shape Simulation, Pseudo-Haptics, Visuo-Haptic Illusions, Ob-
ject Primitives. For graphics solutions, users are redirected towards
their object of interest [13] using visuo-haptic illusions. However,
physically overlaying a prop or primitive over a virtual object has
a tracking cost, which usually relies on trackers which can be oper-
ationally costly (eg Optitrack [108] or HTC Trackers).
Otherwise, the users intentions have to be predicted for the interac-
tion to occur. The users hands are then redirected to the appropriate
motionless prop, for them to explore their object of interest [30].
Operationally, the cost only relies on the proxy fabrication (Figure
2 - 5). These implementations offer various scenarios in terms of
interaction (even non-deterministic), at an affordable cost.

Surface Haptic Displays. These techniques exclusively allow
7.1.3
for exploration through multiple haptic features such as friction or
textures. They also can integrate a tablet or a smartphone [128], on
which the user can interact at any location.

7.1.4 Human Actuators. This technique consists in using human
operators to displace props in the VR arena. The designers however
come across reliability and speed issues with these operators. Even
though they only are used in scenario-based experiences, delay
mechanisms based on graphics need to be implemented [31] (Figure
4 - 2) to overcome these issues. Conceptually, they broaden the
interaction scope, however this solution is operationally very costly.

IHM ’20’21, April 13–16, 2021, Metz, France

Bouzbib et al.

Figure 4: Degree of Actuation. (1) No actuation is available. The user’s hand is redirected to touch a passive prop that cannot
move [13]. The implementation of this technique relies exclusively on a software development leveraging the vision cues;
(2) Human actuators are used to illustrate the Robotic Graphics [101] principle with a Wizard of Oz technique [31]. They
carry props for the user to feel a real continuous wall; Encountered-type of haptic devices (3-5): (3) A drone encounters the
users’ hand for exploring passive props; (4) A cartesian robot displaces itself autonomously for users to interact with physical
props [24]; (5) A robotic arm with multiple degrees of freedom displaces itself to encounter the users’ hand, and rotates its
shape-approximation device to provide the right material [11].

7.1.5 Real Props Reassignment. Instead of using a tracking system
for passive props, a depth camera for instance allows to reassign
props to different virtual objects of the same primitive [69] (Figure
2 - 6). The objects are hence all available to be interacted with.
This drastically reduces the operational costs as they only rely on
computer vision. This enables non-deterministic scenarios as the
real world is literally substituted for a virtual one [135] and objects
can be reassigned with virtual:physical [65] mappings.

7.2 Robotics & No Real Objects
This section gathers technologies simulating the virtual environ-
ment through actuation: they replicate it to constrain the users.

7.2.1 Desktop Haptic Interfaces. The SPIDAR [127], the Virtuose
[62] and other classic desktop haptic interfaces are already com-
pared in multiple surveys [42, 132, 168] (see Figure 2 - 1). They are
safe as they are controlled by the user and only constrain their arm
movements with kinesthetic feedback and adapt to any available
object from the virtual scene (non-deterministic scenarios). They
show a high perceived stiffness and robustness, but remain really
expensive (>10k$).

Shape-Changing Interfaces, Roboxels, 2.5D Tabletops. These
7.2.2
technologies present a high perceived stiffness and change their
shapes accordingly with the virtual environment [47, 91]. They
hence do not require any operator and allow for non-deterministic
scenarios whenever their displacements are enabled [138] (see Fig-
ure 3 - 3). They are however complex to build and require multiple
motors as they are composed of arrays of numerous pins, which
define their haptic fidelity resolution. Even though they present
high voltages, they remain safe around the users. As they require
bare-hands interactions, they hence show a high ease of use.

7.2.3 Wearables, Controllers, EMS.. These rely on small torques,
which are sufficient to constrain the users body parts. They are safe
and easy to use, but in return are not robust enough to resist to
users’ actions. As they are continuously changing the users’ haptic

perception, they do allow non-deterministic scenarios and change
their rendered stiffness and rigidity as a function of the distance to
a virtual prop [39, 85]. A customised controller usually relies on 3D
printed parts and small servomotors and can be easily replicated
[146] (Figure 2 - 2,3; Figure 3 - 1,2).

7.2.4 Mid-Air Haptics. Providing contactless interactions, mid-air
haptics also provide a high level of safety around the user. They
however do not allow to navigate the VR environment, and hence
cannot consider non-deterministic scenarios. Their robustness is
very low, as they send ultrasounds to the users and do not physically
constrain them [117].

Inflatable Floor. The floor topology can be modified and
7.2.5
inflated to create interactions at the body-scale [154]. The users
cannot inflate them, however they can push some tiles down and
hence, edit them. These are safe, though they do not provide a wide
range of interactions, but offer multiple static body postures.

7.3 Robotics & Real Objects
In this subsection, we detail the different types of Robotic Shapes
Displays - otherwise known as "encountered-type of haptic de-
vices", mentioned in the Table 1. First, these interfaces move to
encounter the users: this feature optimises their ease of use. Second,
as these interfaces move within the user vicinity, safety concerns
are raised in this section, depending on the interfaces robustness.
Encountered-type of haptic devices combine different types of in-
teraction techniques: they can provide the users with passive props,
textures or primitives, and allow navigation, exploration, manip-
ulation tasks. Their mechanical implementations offer a good re-
peatability and reliability.

7.3.1 Cartesian Robot: In [24], CoVR, a physical column mounted
over a Cartesian XY ceiling robot enables interactions at any height
and any position of a room-scale VR arena (see Figure 1 - 2; Fig-
ure 4 - 4). This implementation presents a high perceived stiffness,
and because it carries passive props around the arena, enables a

No actuationWoZ/HumansDroneXY RobotRobotic Arm21345"Can I Touch This?"

IHM ’20’21, April 13–16, 2021, Metz, France

high fidelity haptic rendering. It displays high accuracy and speed,
and presents an algorithm which optimises the column’s displace-
ments as a function of the users intentions. It hence enables non-
deterministic scenarios. Safety measures have been validated in the
field. In practice, the column’s celerity is decreasing around the
user, as it is repulsed by this latter. Its software implementation
ensures a safe environment for the user to perambulate in the arena
without unexpected collision. However, in order to display many
props in different scenarios, an operator is required to create panels
and modify them. The materials however remain cheap, and even
though its structure and motors are more expensive than 3D printed
cases and servomotors, as per customised controllers for instance,
this solution provides a wide range of interactions.

7.3.2 Robotic Arm: A robotic arm provides more degrees of free-
dom than the previous Cartesian robot. This primarily means a
higher cost and a higher safety risk. For instance, H-Wall, using a
Kuka LBR Iiwa robot, presents high motor torques and can hence
increase the safety risks around the users. This implementation
hence does not allow non-deterministic scenarios, and presents
either a wall or a revolving door to the user, with a high robustness.
Implementations with smaller torques, such as [11, 166] are safer
but display a reduced perceived stiffness. The use-cases for all these
interactions are hence drastically different: H-Wall simulates a rigid
wall while VRRobot [166] and Snake Charmer [11] (Figure 4 - 5)
present more interaction opportunities. This latter is also the single
Robotic Shape Display that autonomously changes its end-effector,
without an operator.

7.3.3 Drones, Swarm Robots, Mobile Platforms: With drones, the
interactions are limited to the available props, for instance with a
single wall at a given position [178]. Going from an active mode
(flying) to a passive one (graspable by the user) has a long delay
(10s) [5], which on top of the safety concerns, does not allow non-
deterministic scenarios. [159] however allows the user to change
the drone trajectory to fetch and magnetically recover an object of
interest. Their accuracy and speed are limited [54, 122] compared
to the previous grounded interfaces, and can require dynamic redi-
rection techniques to improve their performances [5]. As they are
ungrounded, they do not have a high robustness nor perceived
stiffness. This is also valid for mobile robots, such as [55, 65], which
only display passive props. To decrease the conception cost, exist-
ing vacuuming robots are used as mobile platforms in [170, 181].
Designers can choose to duplicate them, as swarm robots, to enable
non-deterministic scenarios [148]. These are safe to use around the
users, as their speed and robustness are limited. Instead of swarm
mobile interfaces, a merry-go-round platform can also be designed
to display various props at an equidistant position from the user
[72]. All of the previous interfaces require an operator cost on top
of their mechanical and software ones, to modify the interactable
props available, depending on the use-cases.
On the opposite, [190] proposes autonomous reconfigurable in-
terfaces intertwining both Robotic Shape Displays and Roboxels
[101] principles to get rid of the operator cost (see Figure 1 - 4).
These small robotic volume elements reconfigure themselves into
the users objects of interest. They have a sufficient perceived stiff-
ness to represent objects, but are not robust enough to resist to
body-scaled forces, for instance to simulate a rigid wall.

8 EVALUATION PROTOCOLS
On top of choosing from the different trade-offs between conception
and interaction opportunities, the designer also needs to pick-up
an evaluation protocol. These protocols depend on the VR use-
cases. For instance, the haptic benefits for medical or industrial
assembly training can be evaluated against a real experience condi-
tion [112], with criteria such as completion time, number of errors,
user cognitive load [59]. On the opposite, the haptic benefits for a
gaming experience are more likely to be evaluated through immer-
sion and presence, comparing "with/without haptics" conditions
[31]. Although some papers do compare multiple haptic displays
[44, 161], we point out the lack of referenced evaluation protocols
for evaluating haptic solutions in VR.

8.1 Current Reference Evaluation Methods
The most common evaluation methods in VR are the SUS or WS
presence questionnaires [140, 175]. These questionnaires mainly
focus on graphics rendering and only two Likert-scale questions
actually focus on haptic feedback: "How well could you actively
survey the VE using touch?" and "How well could you manipulate
objects in the VE?". Besides, most of the above technologies are
evaluated against "no haptic feedback", hence the results can seem
biased and most of all, expected. This justifies why some imple-
mentations provide results on single parts of the questionnaire, or
arbitrarily combine their results [34] with new subsections (eg "abil-
ity to examine/act") or tasks specific questions (eg "How realistic
was it to feel different textures?).

8.2 Evaluation Recommendations
Haptics should be more incorporated into the different factors enun-
ciated in [175] ("Control, Sensory, Distraction, Realism"). In this
direction, Kim et al. defined the Haptic Experience model [78], where
they take into account both of the designer and user experiences.
It depicts how Design parameters ("timeliness, intensity, density
and timbre") impact Usability requirements ("utility, causality, con-
sistency, saliency") and target Experiential dimensions ("harmony,
expressivity, autotelics, immersion, realism") on the user’s side.
In the same regards, we propose additional guidelines to evaluate
haptic solutions in VR experiments (see Table 2). We believe that
the different elements of interaction opportunities should be added
to the users control parameters.

In the sensory factors, the number of haptic features available
should be added (eg shape, texture, friction, temperature), in line
with their quality, in terms of "timeliness, intensity, density and tim-
bre". The usability requirements should identify the use-cases
and number of scenarios with the proposed solutions. Hence, a
good evaluation of the interface timeliness and usability should an-
ticipate future deployments and avoid unnecessary developments.

9 EXAMPLES: ENCOUNTERED-TYPE OF

HAPTIC DEVICES

We propose in this section to compare four encountered-type of
haptic devices: Beyond the Force (BTF) drone [5] (Figure 4 - 3),
ShapeShift [138] (Figure 3 - 3), Snake Charmer [11] (Figure 4 - 5),
and CoVR [24] (Figure 4 - 4).

IHM ’20’21, April 13–16, 2021, Metz, France

Bouzbib et al.

Table 2: Comparison & Evaluation of 4 Encountered-type of Haptic Devices, according to the "Evaluation section" parameters.

In terms of interactions and number of props, the drone
is the most limited one. Indeed, because of both safety and
implementation limitations, it only enables free navigation in a
reduced workspace. It also allows exploration (through textures)
and manipulation tasks. However, the manipulation task is at the
moment limited to a single light object as BTF cannot handle large
embedded masses yet. Whenever grabbed, it does not provide a
haptic transparency [63] during the interactions because of its
thrust and inertia. For the users to perform different tasks, an
operator needs to manually change the drone configuration. Its
mechanical implementation does not provide a sufficient speed
for overlaying virtual props in non-deterministic scenarios, but its
accuracy is also unsatisfactory and requires dynamic redirection
techniques for the interactions to occur. It also provides unwanted
noise and wind, which reduces the interaction realism.
ShapeShift [138] is drastically different: it is a 2.5D desktop
interface that displaces itself. Even though a drone is theoretically
available in an infinite workspace, in practice they do share
approximately the same one. As [138] relies on a shape-changing
interface, no operator is required and it shape changes itself to
overlay the users’ virtual objects of interest, in non-deterministic
scenarios. It allows a free navigation at a desktop scale, as well
as bimanual manipulation and exploration. Both of these devices
haptic transparency are limited as they are ungrounded solutions.
We believe that ShapeShift could be updated to allow Edition
tasks, by synchronising the users force actions with the actuated
pins stiffness. In terms of haptic features, it simulates shapes
and stimulates both tactile and kinesthetic cues. As per all 2.5D
tabletops, it can be used in various applications: 3D terrain
exploration, volumetric data etc. Its resolution seems promising as
its studies shows successful object recognition and haptic search.
The same interactions are available at a desktop scale with
Snake Charmer [11], which provides a wide range of props
and stimulation, as each of its end-effector include 6 faces with
various interaction opportunities (textures to explore, buttons
to push, heater and fan to perceive temperature, handle and
lightbulb to grasp and manipulate...). It also can change its shape
approximation device, SAD (ie its end-effector), autonomously,
using magnets. It follows the user hand and orient the expected
interaction face of its SAD prior to the interactions: it hence
enables non-deterministic scenarios. Besides, Snake Charmer has a
promising future regarding its deployment: LobbyBot [1], is already
in the Renault industry research lab, to enable VR haptic feedback
in the automotive industry.
Finally, CoVR [24] enables the largest workspace as well as the
highest range of interactions. The user is free to navigate in a

30 𝑚3 VR arena, and CoVR predicts and physically overlays his
object of interest prior to interaction. These interactions include
tactile exploration, manipulation of untethered objects (full haptic
transparency), body postures. Indeed, CoVR is robust enough to
resist body-scaled users, and shows over a 100N perceived stiffness
and can carry over 80kg of embedded mass. CoVR can also initiate
the interactions with the users, and is strong enough to lead the
users through forces or even to transport them. Moreover, with the
appropriate physical:virtual mapping [65], one physical prop can
overlay multiple virtual ones of the same approximate primitive
without redirection techniques. It however requires an operator to
create, assemble and display panels on its sides.

Room-scale VR becomes more and more relevant, and Snake
Charmer could benefit from being attached to an interface
such as CoVR. Similarly, intertwining CoVR with a robotic arm
autonomously changing its SAD like Snake Charmer or with a
shape-changing interface could reduce its operational costs. This
would display all of the Robotics Graphics concept capabilities.

10 CONCLUSION
We analysed in this paper haptic interactions in VR and their corre-
sponding haptic solutions. We analyzed them from both the user
and designer perspectives by considering interaction opportunities
and visuo-haptic consistency, as well as implementation and op-
eration costs. We proposed a novel framework to classify haptic
displays, through a two-dimension design space: the interfaces’
degree of physicality and degree of actuation.

We then evaluated these latter solutions from an interaction
and conception perspectives. Implementation-wise, we evaluated
the interfaces robustness, their ease of use as well as their safety
considerations. From an operation perspective, we also evaluated
the costs of the proposed solutions.

This survey highlights the variety of props, tasks and haptic
features that a haptic solution can potentially provide in VR. This
survey can be used to analytically evaluate the existing haptic
interactions. It can also help VR designers to choose the desired
haptic interaction and/or haptic solution depending on their needs
(tasks, workspace, use-cases etc).

We believe that combining multiple haptic solutions benefits the
user experience, as it optimises the above criteria. Encountered-type
of haptic interfaces were then highlighted as they already combine
multiple interaction techniques: they displace passive props in po-
tentially large VR arenas and allow for numerous tasks, such as
navigation, exploration, manipulation, and even allow the user to
be interacted with.

2+++++---Prepared prior to useYesYes/---+++Noise & Fear AcceptanceDesktop++++++-++NoNo3D Terrain VolumetricData+++++++++Shape RecognitionDesktop++++++++--++Prepared prior to useYesNoTraining+++++++No Formal Evaluation30+++++++++-++++++Prepared prior to useYesYesArcade Training+++++++++Users Enjoymentm3∞m3Beyond the ForceSnake CharmerShapeShiftCoVRNavigation WorkspaceHaptic FeaturesExplorationManipulationEditionNon-Deterministic ScenariosNumber of PropsDeployment Use-casesRobustnessSafetyWhole-Body InvolvementEase-of-UseEvaluation MethodPassive HapticsOperatorAccuracy Speed"Can I Touch This?"

REFERENCES

[1] [n.d.]. renault. https://www.clarte-lab.fr/component/tags/tag/renault
[2] 2019. CyberGrasp. http://www.cyberglovesystems.com/cybergrasp
[3] 2019. Teslasuit | Full body haptic VR suit for motion capture and training.

https://teslasuit.io/

[4] Parastoo Abtahi and Sean Follmer. 2018. Visuo-Haptic Illusions for Improving
the Perceived Performance of Shape Displays. In Proceedings of the 2018 CHI
Conference on Human Factors in Computing Systems - CHI ’18. ACM Press,
Montreal QC, Canada, 1–13. https://doi.org/10.1145/3173574.3173724

[5] Parastoo Abtahi, Benoit Landry, Jackie (Junrui) Yang, Marco Pavone, Sean
Follmer, and James A. Landay. 2019. Beyond The Force: Using Quadcopters
to Appropriate Objects and the Environment for Haptics in Virtual Reality. In
Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
- CHI ’19. ACM Press, Glasgow, Scotland Uk, 1–13. https://doi.org/10.1145/
3290605.3300589

[6] Merwan Achibet, Adrien Girard, Anthony Talvas, Maud Marchal, and Ana-
tole Lecuyer. 2015. Elastic-Arm: Human-scale passive haptic feedback for
augmenting interaction and perception in virtual environments. In 2015 IEEE
Virtual Reality (VR). IEEE, Arles, Camargue, Provence, France, 63–68. https:
//doi.org/10.1109/VR.2015.7223325

[7] Merwan Achibet, Benoit Le Gouis, Maud Marchal, Pierre-Alexandre Leziart,
Ferran Argelaguet, Adrien Girard, Anatole Lecuyer, and Hiroyuki Kajimoto.
2017. FlexiFingers: Multi-finger interaction in VR combining passive haptics
and pseudo-haptics. In 2017 IEEE Symposium on 3D User Interfaces (3DUI). IEEE,
Los Angeles, CA, USA, 103–106. https://doi.org/10.1109/3DUI.2017.7893325
[8] Merwan Achibet, Maud Marchal, Ferran Argelaguet, and Anatole Lecuyer. 2014.
The Virtual Mitten: A novel interaction paradigm for visuo-haptic manipulation
of objects using grip force. In 2014 IEEE Symposium on 3D User Interfaces (3DUI).
IEEE, MN, USA, 59–66. https://doi.org/10.1109/3DUI.2014.6798843

[9] Dmitry Alexandrovsky, Susanne Putze, Michael Bonfert, Sebastian Höffner,
Pitt Michelmann, Dirk Wenig, Rainer Malaka, and Jan David Smeddinck. 2020.
Examining Design Choices of Questionnaires in VR User Studies. In Proceedings
of the 2020 CHI Conference on Human Factors in Computing Systems. ACM,
Honolulu HI USA, 1–21. https://doi.org/10.1145/3313831.3376260

[10] E. Amirpour, M. Savabi, A. Saboukhi, M. Rahimi Gorii, H. Ghafarirad, R. Fes-
harakifard, and S. Mehdi Rezaei. 2019. Design and Optimization of a Multi-DOF
Hand Exoskeleton for Haptic Applications. In 2019 7th International Confer-
ence on Robotics and Mechatronics (ICRoM). 270–275. https://doi.org/10.1109/
ICRoM48714.2019.9071884 ISSN: 2572-6889.

[11] Bruno Araujo, Ricardo Jota, Varun Perumal, Jia Xian Yao, Karan Singh, and
Daniel Wigdor. 2016. Snake Charmer: Physically Enabling Virtual Objects. In
Proceedings of the TEI ’16: Tenth International Conference on Tangible, Embedded,
and Embodied Interaction - TEI ’16. ACM Press, Eindhoven, Netherlands, 218–226.
https://doi.org/10.1145/2839462.2839484

[12] Jonas Auda, Max Pascher, and Stefan Schneegass. 2019. Around the (Virtual)
World: Infinite Walking in Virtual Reality Using Electrical Muscle Stimulation. In
Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
- CHI ’19. ACM Press, Glasgow, Scotland Uk, 1–8. https://doi.org/10.1145/
3290605.3300661

[13] Mahdi Azmandian, Mark Hancock, Hrvoje Benko, Eyal Ofek, and Andrew D.
Wilson. 2016. Haptic Retargeting: Dynamic Repurposing of Passive Haptics for
Enhanced Virtual Reality Experiences. In Proceedings of the 2016 CHI Conference
on Human Factors in Computing Systems - CHI ’16. ACM Press, Santa Clara,
California, USA, 1968–1979. https://doi.org/10.1145/2858036.2858226

[14] Marc Baloup, Veïs Oudjail, Thomas Pietrzak, and Géry Casiez. 2018. Pointing
techniques for distant targets in virtual reality. In Proceedings of the 30th Con-
ference on l’Interaction Homme-Machine - IHM ’18. ACM Press, Brest, France,
100–107. https://doi.org/10.1145/3286689.3286696

[15] Y. Ban, T. Kajinami, T. Narumi, T. Tanikawa, and M. Hirose. 2012. Modifying an
identified curved surface shape using pseudo-haptic effect. In 2012 IEEE Haptics
Symposium (HAPTICS). 211–216. https://doi.org/10.1109/HAPTIC.2012.6183793
[16] Yuki Ban, Takuji Narumi, Tomohiro Tanikawa, and Michitaka Hirose. 2012.
Modifying an identified position of edged shapes using pseudo-haptic effects. In
Proceedings of the 18th ACM symposium on Virtual reality software and technology
- VRST ’12. ACM Press, Toronto, Ontario, Canada, 93. https://doi.org/10.1145/
2407336.2407353

[17] Gareth Barnaby and Anne Roudaut. 2019. Mantis: A Scalable, Lightweight
and Accessible Architecture to Build Multiform Force Feedback Systems. In
Proceedings of the 32nd Annual ACM Symposium on User Interface Software and
Technology - UIST ’19. ACM Press, New Orleans, LA, USA, 937–948. https:
//doi.org/10.1145/3332165.3347909

[18] Olivier Bau, Ivan Poupyrev, Ali Israr, and Chris Harrison. 2010. TeslaTouch:
electrovibration for touch surfaces. In Proceedings of the 23nd annual ACM
symposium on User interface software and technology - UIST ’10. ACM Press, New
York, New York, USA, 283. https://doi.org/10.1145/1866029.1866074

IHM ’20’21, April 13–16, 2021, Metz, France

[19] Hrvoje Benko, Christian Holz, Mike Sinclair, and Eyal Ofek. 2016. NormalTouch
and TextureTouch: High-fidelity 3D Haptic Shape Rendering on Handheld Vir-
tual Reality Controllers. In Proceedings of the 29th Annual Symposium on User
Interface Software and Technology - UIST ’16. ACM Press, Tokyo, Japan, 717–728.
https://doi.org/10.1145/2984511.2984526

[20] Leif P. Berg and Judy M. Vance. 2017. Industry use of virtual reality in product
design and manufacturing: a survey. Virtual Reality 21, 1 (March 2017), 1–17.
https://doi.org/10.1007/s10055-016-0293-9

[21] Joanna Bergström, Aske Mottelson, and Jarrod Knibbe. 2019. Resized Grasping
in VR: Estimating Thresholds for Object Discrimination. In Proceedings of the
32nd Annual ACM Symposium on User Interface Software and Technology. ACM,
New Orleans LA USA, 1175–1183. https://doi.org/10.1145/3332165.3347939

[22] A. Bloomfield, Yu Deng, J. Wampler, P. Rondot, D. Harth, M. McManus, and N.
Badler. 2003. A taxonomy and comparison of haptic actions for disassembly
tasks. In IEEE Virtual Reality, 2003. Proceedings. IEEE Comput. Soc, Los Angeles,
CA, USA, 225–231. https://doi.org/10.1109/VR.2003.1191143

[23] Mette Boldt, Boxuan Liu, Tram Nguyen, Alina Panova, Ramneek Singh, Alexan-
der Steenbergen, Rainer Malaka, Jan Smeddinck, Michael Bonfert, Inga Lehne,
Melina Cahnbley, Kim Korschinq, Loannis Bikas, Stefan Finke, Martin Hanci,
and Valentin Kraft. 2018. You Shall Not Pass: Non-Intrusive Feedback for Virtual
Walls in VR Environments with Room-Scale Mapping. In 2018 IEEE Confer-
ence on Virtual Reality and 3D User Interfaces (VR). IEEE, Reutlingen, 143–150.
https://doi.org/10.1109/VR.2018.8446177

[24] Elodie Bouzbib, Gilles Bailly, Sinan Haliyo, and Pascal Frey. 2020. CoVR: A
Large-Scale Force-Feedback Robotic Interface for Non-Deterministic Scenarios
in VR. In Proceedings of the 33rd Annual ACM Symposium on User Interface
Software and Technology. ACM, Virtual Event USA, 209–222. https://doi.org/10.
1145/3379337.3415891

[25] D.A. Bowman and C.A. Wingrave. 2001. Design and evaluation of menu systems
for immersive virtual environments. In Proceedings IEEE Virtual Reality 2001.
IEEE Comput. Soc, Yokohama, Japan, 149–156. https://doi.org/10.1109/VR.2001.
913781

[26] Steve Bryson. 2005. Direct Manipulation in Virtual Reality. In Visualization
Handbook. Elsevier, 413–430. https://doi.org/10.1016/B978-012387582-2/50023-
X

[27] Lung-Pan Cheng. 2019. VRoamer: Generating On-The-Fly VR Experiences
While Walking inside Large, Unknown Real-World Building Environments.
(2019), 8.

[28] Lung-Pan Cheng, Li Chang, Sebastian Marwecki, and Patrick Baudisch. 2018.
iTurk: Turning Passive Haptics into Active Haptics by Making Users Reconfigure
Props in Virtual Reality. In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems - CHI ’18. ACM Press, Montreal QC, Canada, 1–10.
https://doi.org/10.1145/3173574.3173663

[29] Lung-Pan Cheng, Patrick Lühne, Pedro Lopes, Christoph Sterz, and Patrick

Baudisch. 2014. Haptic Turk: a Motion Platform Based on People. (2014), 11.

[30] Lung-Pan Cheng, Eyal Ofek, Christian Holz, Hrvoje Benko, and Andrew D.
Wilson. 2017. Sparse Haptic Proxy: Touch Feedback in Virtual Environments
Using a General Passive Prop. In Proceedings of the 2017 CHI Conference on
Human Factors in Computing Systems - CHI ’17. ACM Press, Denver, Colorado,
USA, 3718–3728. https://doi.org/10.1145/3025453.3025753

[31] Lung-Pan Cheng, Thijs Roumen, Hannes Rantzsch, Sven Köhler, Patrick Schmidt,
Robert Kovacs, Johannes Jasper, Jonas Kemper, and Patrick Baudisch. 2015.
TurkDeck: Physical Virtual Reality Based on People. In Proceedings of the 28th
Annual ACM Symposium on User Interface Software & Technology - UIST ’15.
ACM Press, Daegu, Kyungpook, Republic of Korea, 417–426. https://doi.org/10.
1145/2807442.2807463

[32] Inrak Choi, Heather Culbertson, Mark R. Miller, Alex Olwal, and Sean Follmer.
2017. Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping
in Virtual Reality. In Proceedings of the 30th Annual ACM Symposium on User
Interface Software and Technology - UIST ’17. ACM Press, Qu&#233;bec City, QC,
Canada, 119–130. https://doi.org/10.1145/3126594.3126599

[33] Inrak Choi, Elliot W. Hawkes, David L. Christensen, Christopher J. Ploch, and
Sean Follmer. 2016. Wolverine: A wearable haptic interface for grasping in
virtual reality. In 2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS). IEEE, Daejeon, South Korea, 986–993. https://doi.org/10.
1109/IROS.2016.7759169

[34] Inrak Choi, Eyal Ofek, Hrvoje Benko, Mike Sinclair, and Christian Holz. 2018.
CLAW: A Multifunctional Handheld Haptic Controller for Grasping, Touching,
and Triggering in Virtual Reality. In Proceedings of the 2018 CHI Conference
on Human Factors in Computing Systems - CHI ’18. ACM Press, Montreal QC,
Canada, 1–13. https://doi.org/10.1145/3173574.3174228

[35] Timothy R. Coles, Dwight Meglan, and Nigel W. John. 2011. The Role of
Haptics in Medical Training Simulators: A Survey of the State of the Art. IEEE
Transactions on Haptics 4, 1 (Jan. 2011), 51–66. https://doi.org/10.1109/TOH.
2010.19

[36] Fabien Danieau, Julien Fleureau, Philippe Guillotel, Nicolas Mollet, Anatole
Lécuyer, and Marc Christie. 2012. HapSeat: producing motion sensation with
multiple force-feedback devices embedded in a seat. In Proceedings of the 18th

IHM ’20’21, April 13–16, 2021, Metz, France

Bouzbib et al.

ACM symposium on Virtual reality software and technology - VRST ’12. ACM
Press, Toronto, Ontario, Canada, 69. https://doi.org/10.1145/2407336.2407350
[37] Fabien Danieau, Philippe Guillotel, Olivier Dumas, Thomas Lopez, Bertrand
Leroy, and Nicolas Mollet. 2018. HFX studio: haptic editor for full-body im-
mersive experiences. In Proceedings of the 24th ACM Symposium on Virtual
Reality Software and Technology - VRST ’18. ACM Press, Tokyo, Japan, 1–9.
https://doi.org/10.1145/3281505.3281518

[38] Bruno R. De Araújo, Géry Casiez, Joaquim A. Jorge, and Martin Hachet. 2013.
Mockup Builder: 3D modeling on and above the surface. Computers & Graphics
37, 3 (May 2013), 165–178. https://doi.org/10.1016/j.cag.2012.12.005

[39] Xavier de Tinguy, Thomas Howard, Claudio Pacchierotti, Maud Marchal, and
Anatole Lécuyer. 2020. WeATaViX: WEarable Actuated TAngibles for VIrtual
reality eXperiences. (2020), 9.

[40] Xavier de Tinguy, Claudio Pacchierotti, Maud Marchal, and Anatole Lecuyer.
2019. Toward Universal Tangible Objects: Optimizing Haptic Pinching Sensa-
tions in 3D Interaction. In 2019 IEEE Conference on Virtual Reality and 3D User
Interfaces (VR). IEEE, Osaka, Japan, 321–330. https://doi.org/10.1109/VR.2019.
8798205

[41] Donald Degraen, André Zenner, and Antonio Krüger. 2019. Enhancing Texture
Perception in Virtual Reality Using 3D-Printed Hair Structures. In Proceedings of
the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19. ACM
Press, Glasgow, Scotland Uk, 1–12. https://doi.org/10.1145/3290605.3300479

[42] Lionel Dominjon, Jérôme Perret, and Anatole Lécuyer. 2007. Novel devices
and interaction techniques for human-scale haptics. The Visual Computer 23, 4
(March 2007), 257–266. https://doi.org/10.1007/s00371-007-0100-4

[43] Frederick Ducatelle, Gianni A. Di Caro, Carlo Pinciroli, and Luca M. Gambardella.
2011. Self-organized cooperation between robotic swarms. Swarm Intelligence
5, 2 (June 2011), 73–96. https://doi.org/10.1007/s11721-011-0053-0

[44] David Escobar-Castillejos, Julieta Noguez, Luis Neri, Alejandra Magana, and
Bedrich Benes. 2016. A Review of Simulators with Haptic Devices for Medical
Training. Journal of Medical Systems 40, 4 (April 2016), 1–22. https://doi.org/
10.1007/s10916-016-0459-8

[45] Cathy Fang, Yang Zhang, Matthew Dworman, and Chris Harrison. 2020. Wire-
ality: Enabling Complex Tangible Geometries in Virtual Reality with Worn
Multi-String Haptics. (2020), 10.

[46] Martin Feick, Scott Bateman, Anthony Tang, André Miede, and Nicolai Mar-
quardt. 2020. TanGi: Tangible Proxies for Embodied Object Exploration and
Manipulation in Virtual Reality. arXiv:2001.03021 [cs] (Jan. 2020).
http:
//arxiv.org/abs/2001.03021 arXiv: 2001.03021.

[47] Daniel Fitzgerald and Hiroshi Ishii. 2018. Mediate: A Spatial Tangible Interface
for Mixed Reality. In Extended Abstracts of the 2018 CHI Conference on Human
Factors in Computing Systems. ACM, Montreal QC Canada, 1–6. https://doi.
org/10.1145/3170427.3188472

[48] Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, and Hiroshi Ishii.
2013. inFORM: dynamic physical affordances and constraints through shape
and object actuation. In Proceedings of the 26th annual ACM symposium on User
interface software and technology - UIST ’13. ACM Press, St. Andrews, Scotland,
United Kingdom, 417–426. https://doi.org/10.1145/2501988.2502032

[49] A. Formaglio, A. Giannitrapani, M. Franzini, D. Prattichizzo, and F. Barbagli.
2005. Performance of Mobile Haptic Interfaces. In Proceedings of the 44th IEEE
Conference on Decision and Control. 8343–8348. https://doi.org/10.1109/CDC.
2005.1583513

[50] Ilja Frissen, Jennifer L. Campos, Manish Sreenivasa, and Marc O. Ernst. 2013.
Enabling Unconstrained Omnidirectional Walking Through Virtual Environ-
ments: An Overview of the CyberWalk Project. In Human Walking in Virtual
Environments: Perception, Technology, and Applications, Frank Steinicke, Yon
Visell, Jennifer Campos, and Anatole Lécuyer (Eds.). Springer, New York, NY,
113–144. https://doi.org/10.1007/978-1-4419-8432-6_6

[51] Markus Funk, Florian Müller, Marco Fendrich, Megan Shene, Moritz Kolvenbach,
Niclas Dobbertin, Sebastian Günther, and Max Mühlhäuser. 2019. Assessing the
Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual
Reality using Curved Trajectories. In Proceedings of the 2019 CHI Conference on
Human Factors in Computing Systems - CHI ’19. ACM Press, Glasgow, Scotland
Uk, 1–12. https://doi.org/10.1145/3290605.3300377

[52] Thomas Galais, Alexandra Delmas, and Rémy Alonso. 2019. Natural interaction
in virtual reality: impact on the cognitive load. In Proceedings of the 31st Confer-
ence on l’Interaction Homme-Machine Adjunct - IHM ’19. ACM Press, Grenoble,
France, 1–9. https://doi.org/10.1145/3366551.3370342

[53] Péter Galambos. 2012. Vibrotactile Feedback for Haptics and Telemanipulation:
Survey, Concept and Experiment. Acta Polytechnica Hungarica 9, 1 (2012), 25.
[54] Antonio Gomes, Calvin Rubens, Sean Braley, and Roel Vertegaal. 2016. Bit-
Drones: Towards Using 3D Nanocopter Displays as Interactive Self-Levitating
Programmable Matter. In Proceedings of the 2016 CHI Conference on Human
Factors in Computing Systems - CHI ’16. ACM Press, Santa Clara, California,
USA, 770–780. https://doi.org/10.1145/2858036.2858519

[55] Eric J. Gonzalez, Parastoo Abtahi, and Sean Follmer. 2020. REACH+: Ex-
tending the Reachability of Encountered-type Haptics Devices through Dy-
namic Redirection in VR. In Proceedings of the 33rd Annual ACM Symposium

on User Interface Software and Technology. ACM, Virtual Event USA, 236–248.
https://doi.org/10.1145/3379337.3415870

[56] Eric J. Gonzalez and Sean Follmer. 2019. Investigating the Detection of Bimanual
Haptic Retargeting in Virtual Reality. In 25th ACM Symposium on Virtual Reality
Software and Technology on - VRST ’19. ACM Press, Parramatta, NSW, Australia,
1–5. https://doi.org/10.1145/3359996.3364248

[57] Xiaochi Gu, Yifei Zhang, Weize Sun, Yuanzhe Bian, Dao Zhou, and Per Ola Kris-
tensson. 2016. Dexmo: An Inexpensive and Lightweight Mechanical Exoskeleton
for Motion Capture and Force Feedback in VR. In Proceedings of the 2016 CHI
Conference on Human Factors in Computing Systems - CHI ’16. ACM Press, Santa
Clara, California, USA, 1991–1995. https://doi.org/10.1145/2858036.2858487

[58] Jan Gugenheimer, Dennis Wolf, Eythor R. Eiriksson, Pattie Maes, and Enrico
Rukzio. 2016. GyroVR: Simulating Inertia in Virtual Reality using Head Worn
Flywheels. In Proceedings of the 29th Annual Symposium on User Interface Soft-
ware and Technology. ACM, Tokyo Japan, 227–232. https://doi.org/10.1145/
2984511.2984535

[59] T. Gutierrez, J. Rodriguez, Y. Velaz, S. Casado, A. Suescun, and E. J. Sanchez.
IMA-VR: A multimodal virtual training system for skills transfer
19th International Sym-
https:

2010.
in Industrial Maintenance and Assembly tasks.
posium in Robot and Human Interactive Communication (2010).
//www.academia.edu/15623406/IMA_VR_A_multimodal_virtual_training_
system_for_skills_transfer_in_Industrial_Maintenance_and_Assembly_tasks
[60] Sebastian Günther, Dominik Schön, Florian Müller, Max Mühlhäuser, and Martin
Schmitz. 2020. PneumoVolley: Pressure-based Haptic Feedback on the Head
through Pneumatic Actuation. (2020), 10.

[61] Dustin T. Han, Mohamed Suhail, and Eric D. Ragan. 2018. Evaluating Remapped
Physical Reach for Hand Interactions with Passive Haptics in Virtual Reality.
IEEE Transactions on Visualization and Computer Graphics 24, 4 (April 2018),
1467–1476. https://doi.org/10.1109/TVCG.2018.2794659

[62] Haption. 2019. Virtuose™ 6D - HAPTION SA. https://www.haption.com/en/

products-en/virtuose-6d-en.html

[63] Vincent Hayward and Karon Maclean. 2007. Do it yourself haptics: part I. IEEE
Robotics & Automation Magazine 14, 4 (Dec. 2007), 88–104. https://doi.org/10.
1109/M-RA.2007.907921

[64] Zhenyi He, Fengyuan Zhu, Aaron Gaudette, and Ken Perlin. 2017. Robotic
Haptic Proxies for Collaborative Virtual Reality. arXiv:1701.08879 [cs] (Jan.
2017). http://arxiv.org/abs/1701.08879 arXiv: 1701.08879.

[65] Zhenyi He, Fengyuan Zhu, and Ken Perlin. 2017. PhyShare: Sharing Physical
Interaction in Virtual Reality. arXiv:1708.04139 [cs] (Aug. 2017). http://arxiv.
org/abs/1708.04139 arXiv: 1708.04139.

[66] Richard M. Held and Nathaniel I. Durlach. 1992. Telepresence. Presence: Teleop-
erators and Virtual Environments 1, 1 (Jan. 1992), 109–112. https://doi.org/10.
1162/pres.1992.1.1.109

[67] Seongkook Heo, Christina Chung, Geehyuk Lee, and Daniel Wigdor. 2018.
Thor’s Hammer: An Ungrounded Force Feedback Device Utilizing Propeller-
Induced Propulsive Force. In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems - CHI ’18. ACM Press, Montreal QC, Canada, 1–11.
https://doi.org/10.1145/3173574.3174099

[68] Seongkook Heo, Jaeyeon Lee, and Daniel Wigdor. 2019. PseudoBend: Producing
Haptic Illusions of Stretching, Bending, and Twisting Using Grain Vibrations.
In Proceedings of the 32nd Annual ACM Symposium on User Interface Software
and Technology - UIST ’19. ACM Press, New Orleans, LA, USA, 803–813. https:
//doi.org/10.1145/3332165.3347941

[69] Anuruddha Hettiarachchi and Daniel Wigdor. 2016. Annexing Reality: Enabling
Opportunistic Use of Everyday Objects as Tangible Proxies in Augmented
Reality. In Proceedings of the 2016 CHI Conference on Human Factors in Computing
Systems - CHI ’16. ACM Press, Santa Clara, California, USA, 1957–1967. https:
//doi.org/10.1145/2858036.2858134

[70] Matthias Hoppe, Pascal Knierim, Thomas Kosch, Markus Funk, Lauren Futami,
Stefan Schneegass, Niels Henze, Albrecht Schmidt, and Tonja Machulla. 2018.
VRHapticDrones: Providing Haptics in Virtual Reality through Quadcopters.
In Proceedings of the 17th International Conference on Mobile and Ubiquitous
Multimedia - MUM 2018. ACM Press, Cairo, Egypt, 7–18. https://doi.org/10.
1145/3282894.3282898

[71] Matthias Hoppe, Daniel Neumann, Stephan Streuber, Albrecht Schmidt, and
Tonja-Katrin Machulla. 2020. A Human Touch: Social Touch Increases the Perceived
Human-likeness of Agents in Virtual Reality. https://doi.org/10.1145/3313831.
3376719

[72] Hsin-Yu Huang, Chih-Wei Ning, Po-Yao Wang, Jen-Hao Cheng, and Lung-Pan
Cheng. 2020. Haptic-go-round: A Surrounding Platform for Encounter-type
Haptics in Virtual Reality Experiences. In Proceedings of the 2020 CHI Conference
on Human Factors in Computing Systems. ACM, Honolulu HI USA, 1–10. https:
//doi.org/10.1145/3313831.3376476

[73] Brent Edward Insko. 2001. Passive Haptics Significantly Enhances Virtual

Environments. (2001), 111.

[74] Hiroo Iwata. 2005. CirculaFloor. https://ieeexplore.ieee.org/abstract/document/

1381227

"Can I Touch This?"

IHM ’20’21, April 13–16, 2021, Metz, France

[75] Hiroo Iwata. 2013. Locomotion Interfaces. In Human Walking in Virtual Envi-
ronments: Perception, Technology, and Applications, Frank Steinicke, Yon Visell,
Jennifer Campos, and Anatole Lécuyer (Eds.). Springer, New York, NY, 199–219.
https://doi.org/10.1007/978-1-4419-8432-6_9

[76] Hiroo Iwata, Hiroaki Yano, Fumitaka Nakaizumi, and Ryo Kawamura. 2001.
Project FEELEX: adding haptic surface to graphics. In Proceedings of the 28th
annual conference on Computer graphics and interactive techniques - SIGGRAPH
’01. ACM Press, Not Known, 469–476. https://doi.org/10.1145/383259.383314
[77] Lynette Jones. 2000. Kinesthetic Sensing. Human and Machine Haptics (2000).
http://bdml.stanford.edu/twiki/pub/Haptics/PapersInProgress/jones00.pdf
[78] Erin Kim and Oliver Schneider. 2020. Defining Haptic Experience: Foundations
for Understanding, Communicating, and Evaluating HX. In Proceedings of the
2020 CHI Conference on Human Factors in Computing Systems. ACM, Honolulu
HI USA, 1–13. https://doi.org/10.1145/3313831.3376280

[79] Lawrence H. Kim, Daniel S. Drew, Veronika Domova, and Sean Follmer. 2020.
User-defined Swarm Robot Control. In Proceedings of the 2020 CHI Conference
on Human Factors in Computing Systems. ACM, Honolulu HI USA, 1–13. https:
//doi.org/10.1145/3313831.3376814

[80] Yaesol Kim, Hyun Jung Kim, and Young J. Kim. 2018. Encountered-type
haptic display for large VR environment using per-plane reachability maps:
Encountered-type Haptic Display for Large VR Environment. Computer Anima-
tion and Virtual Worlds 29, 3-4 (May 2018), e1814. https://doi.org/10.1002/cav.
1814

[81] Pascal Knierim, Thomas Kosch, Valentin Schwind, Markus Funk, Francisco Kiss,
Stefan Schneegass, and Niels Henze. 2017. Tactile Drones - Providing Immersive
Tactile Feedback in Virtual Reality through Quadcopters. In Proceedings of
the 2017 CHI Conference Extended Abstracts on Human Factors in Computing
Systems - CHI EA ’17. ACM Press, Denver, Colorado, USA, 433–436. https:
//doi.org/10.1145/3027063.3050426

[82] Luv Kohli. 2010. Redirected touching: Warping space to remap passive haptics.
In 2010 IEEE Symposium on 3D User Interfaces (3DUI). IEEE, Waltham, MA, USA,
129–130. https://doi.org/10.1109/3DUI.2010.5444703

[83] L. Kohli, M. C. Whitton, and F. P. Brooks. 2012. Redirected touching: The effect
of warping space on task performance. In 2012 IEEE Symposium on 3D User
Interfaces (3DUI). IEEE, Costa Mesa, CA, 105–112. https://doi.org/10.1109/3DUI.
2012.6184193

[84] Luv Kohli, Mary C. Whitton, and Frederick P. Brooks. 2013. Redirected Touching:
Training and adaptation in warped virtual spaces. In 2013 IEEE Symposium on
3D User Interfaces (3DUI). IEEE, Orlando, FL, 79–86. https://doi.org/10.1109/
3DUI.2013.6550201

[85] Robert Kovacs, Eyal Ofek, Mar Gonzalez Franco, Alexa Fay Siu, Sebastian Mar-
wecki, Christian Holz, and Mike Sinclair. 2020. Haptic PIVOT: On-Demand
Handhelds in VR. In Proceedings of the 33rd Annual ACM Symposium on User
Interface Software and Technology (UIST ’20). Association for Computing Machin-
ery, New York, NY, USA, 1046–1059. https://doi.org/10.1145/3379337.3415854
[86] Mathieu Le Goc, Lawrence H. Kim, Ali Parsaei, Jean-Daniel Fekete, Pierre
Dragicevic, and Sean Follmer. 2016. Zooids: Building Blocks for Swarm User
Interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software
and Technology - UIST ’16. ACM Press, Tokyo, Japan, 97–109. https://doi.org/
10.1145/2984511.2984547

[87] S. J. Lederman and R. L. Klatzky. 2009. Haptic perception: A tutorial. Attention,
Perception & Psychophysics 71, 7 (Oct. 2009), 1439–1459. https://doi.org/10.3758/
APP.71.7.1439

[88] Chaehyun Lee, Min Sik Hong, In Lee, Oh Kyu Choi, Kyung-Lyong Han, Yoo Yeon
Kim, Seungmoon Choi, and Jin S Lee. 2007. Mobile Haptic Interface for Large
Immersive Virtual Environments: PoMHI v0.5. (2007), 2.

[89] In Lee, Inwook Hwang, Kyung-Lyoung Han, Oh Kyu Choi, Seungmoon Choi,
and Jin S. Lee. 2009. System improvements in Mobile Haptic Interface. In World
Haptics 2009 - Third Joint EuroHaptics conference and Symposium on Haptic
Interfaces for Virtual Environment and Teleoperator Systems. IEEE, Salt Lake City,
UT, USA, 109–114. https://doi.org/10.1109/WHC.2009.4810834

[90] Jaeyeon Lee, Mike Sinclair, Mar Gonzalez-Franco, Eyal Ofek, and Christian
Holz. 2019. TORC: A Virtual Reality Controller for In-Hand High-Dexterity
Finger Interaction. In Proceedings of the 2019 CHI Conference on Human Factors
in Computing Systems - CHI ’19. ACM Press, Glasgow, Scotland Uk, 1–13. https:
//doi.org/10.1145/3290605.3300301

[91] Daniel Leithinger, Sean Follmer, Alex Olwal, Samuel Luescher, Akimitsu Hogge,
Jinha Lee, and Hiroshi Ishii. 2013. Sublimate: state-changing virtual and physical
rendering to augment interaction with shape displays. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems - CHI ’13. ACM
Press, Paris, France, 1441. https://doi.org/10.1145/2470654.2466191

[92] Jean-Claude Lepecq, Lionel Bringoux, Jean-Marie Pergandi, Thelma Coyle, and
Daniel Mestre. 2008. Afforded Actions as a Behavioral Assessment of Physical
Presence. (2008), 8.

[93] Jo-Yu Lo, Da-Yuan Huang, Chen-Kuo Sun, Chu-En Hou, and Bing-Yu Chen. 2018.
RollingStone: Using Single Slip Taxel for Enhancing Active Finger Exploration
with a Virtual Reality Controller. In The 31st Annual ACM Symposium on User

Interface Software and Technology - UIST ’18. ACM Press, Berlin, Germany, 839–
851. https://doi.org/10.1145/3242587.3242627

[94] Pedro Lopes, Patrik Jonell, and Patrick Baudisch. 2015. Affordance++: Allowing
Objects to Communicate Dynamic Use. In Proceedings of the 33rd Annual ACM
Conference on Human Factors in Computing Systems - CHI ’15. ACM Press, Seoul,
Republic of Korea, 2515–2524. https://doi.org/10.1145/2702123.2702128
[95] Pedro Lopes, Sijing You, Lung-Pan Cheng, Sebastian Marwecki, and Patrick
Baudisch. 2017. Providing Haptics to Walls & Heavy Objects in Virtual Reality by
Means of Electrical Muscle Stimulation. In Proceedings of the 2017 CHI Conference
on Human Factors in Computing Systems - CHI ’17. ACM Press, Denver, Colorado,
USA, 1471–1482. https://doi.org/10.1145/3025453.3025600

[96] Anatole Lécuyer. 2009. Simulating Haptic Feedback Using Vision: A Survey of
Research and Applications of Pseudo-Haptic Feedback. Presence: Teleoperators
and Virtual Environments 18, 1 (Feb. 2009), 39–53. https://doi.org/10.1162/pres.
18.1.39

[97] N. Magnenat-Thalmann, HyungSeok Kim, A. Egges, and S. Garchery. 2005.
Believability and Interaction in Virtual Worlds. In 11th International Multimedia
Modelling Conference. IEEE, Honolulu, HI, USA, 2–9. https://doi.org/10.1109/
MMMC.2005.24

[98] Lawrence Makin, Gareth Barnaby, and Anne Roudaut. 2019. Tactile and kines-
thetic feedbacks improve distance perception in virtual reality. In Proceedings
of the 31st Conference on l’Interaction Homme-Machine - IHM ’19. ACM Press,
Grenoble, France, 1–9. https://doi.org/10.1145/3366550.3372248

[99] Nicolai Marquardt, Miguel A. Nacenta, James E. Young, Sheelagh Carpendale,
Saul Greenberg, and Ehud Sharlin. 2009. The Haptic Tabletop Puck: tactile
feedback for interactive tabletops. In Proceedings of the ACM International Con-
ference on Interactive Tabletops and Surfaces - ITS ’09. ACM Press, Banff, Alberta,
Canada, 85. https://doi.org/10.1145/1731903.1731922

[100] Thomas H Massie and J K Salisbury. 1994. The PHANTOM Haptic Interface: A

Device for Probing Virtual Objects. (1994), 5.

[101] W. A. McNeely. 1993. Robotic graphics: a new approach to force feedback
for virtual reality. In Proceedings of IEEE Virtual Reality Annual International
Symposium. 336–341. https://doi.org/10.1109/VRAIS.1993.380761

[102] Leonel Merino, Magdalena Schwarzl, Matthias Kraus, Michael Sedlmair, Dieter
Schmalstieg, and Daniel Weiskopf. 2020. Evaluating Mixed and Augmented
Reality: A Systematic Literature Review (2009-2019). arXiv:2010.05988 [cs] (Oct.
2020). http://arxiv.org/abs/2010.05988 arXiv: 2010.05988.

[103] Judi Moline. 1997. Virtual reality for health care: a survey. Technical Report.
[104] Ken Nakagaki, Artem Dementyev, Sean Follmer, Joseph A. Paradiso, and Hiroshi
Ishii. 2016. ChainFORM: A Linear Integrated Modular Hardware System for
Shape Changing Interfaces. In Proceedings of the 29th Annual Symposium on
User Interface Software and Technology - UIST ’16. ACM Press, Tokyo, Japan,
87–96. https://doi.org/10.1145/2984511.2984587

[105] Ken Nakagaki, Luke Vink, Jared Counts, Daniel Windham, Daniel Leithinger,
Sean Follmer, and Hiroshi Ishii. 2016. Materiable: Rendering Dynamic Material
Properties in Response to Direct Physical Touch with Shape Changing Interfaces.
In Proceedings of the 2016 CHI Conference on Human Factors in Computing
Systems - CHI ’16. ACM Press, Santa Clara, California, USA, 2764–2772. https:
//doi.org/10.1145/2858036.2858104

[106] Norbert Nitzsche, Uwe D. Hanebeck, and G. Schmidt. 2003. Design issues of
mobile haptic interfaces. Journal of Robotic Systems 20, 9 (Sept. 2003), 549–556.
https://doi.org/10.1002/rob.10105

[107] Donald A. Norman. 2013. The design of everyday things (revised and expanded

edition ed.). Basic Books, New York, New York.

[108] Optitrack. 2019. Motion Capture Systems. http://optitrack.com/index.html
[109] M. Ortega and S. Coquillart. 2005. Prop-based haptic interaction with co-location
and immersion: an automotive application. In IREE International Worksho on
Haptic Audio Visual Environments and their Applications, 2005. IEEE, Ottawa,
Canada, 23–28. https://doi.org/10.1109/HAVE.2005.1545646

[110] J. Pair, U. Neumann, D. Piepol, and B. Swartout. 2003. FlatWorld: combining Hol-
lywood set-design techniques with VR. IEEE Computer Graphics and Applications
23, 1 (Jan. 2003), 12–15. https://doi.org/10.1109/MCG.2003.1159607

[111] Ryan A. Pavlik, Judy M. Vance, and Greg R. Luecke. 2013. Interacting With
a Large Virtual Environment by Combining a Ground-Based Haptic Device
and a Mobile Robot Base. In Volume 2B: 33rd Computers and Information in
Engineering Conference. ASME, Portland, Oregon, USA, V02BT02A029. https:
//doi.org/10.1115/DETC2013-13441

[112] M Poyade, L Molina-Tanco, A Reyes-Lecuona, A Langley, E Frutos, and S Flores.
2012. Validation of a haptic virtual reality simulation in the context of industrial
maintenance. (2012), 4.

[113] Pragathi Praveena, Daniel Rakita, Bilge Mutlu, and Michael Gleicher. 2020. Sup-
porting Perception of Weight through Motion-induced Sensory Conflicts in
Robot Teleoperation. In Proceedings of the 2020 ACM/IEEE International Confer-
ence on Human-Robot Interaction. ACM, Cambridge United Kingdom, 509–517.
https://doi.org/10.1145/3319502.3374841

[114] William R. Provancher, Mark R. Cutkosky, Katherine J. Kuchenbecker, and
Günter Niemeyer. 2005. Contact Location Display for Haptic Perception of
Curvature and Object Motion. The International Journal of Robotics Research 24,

IHM ’20’21, April 13–16, 2021, Metz, France

Bouzbib et al.

9 (Sept. 2005), 691–702. https://doi.org/10.1177/0278364905057121

[115] Andreas Pusch and Anatole Lécuyer. 2011. Pseudo-haptics: from the theoretical
foundations to practical system design guidelines. In Proceedings of the 13th
international conference on multimodal interfaces - ICMI ’11. ACM Press, Alicante,
Spain, 57. https://doi.org/10.1145/2070481.2070494

[116] Susanne Putze, Dmitry Alexandrovsky, Felix Putze, Sebastian Höffner, Jan David
Smeddinck, and Rainer Malaka. 2020. Breaking The Experience: Effects of
Questionnaires in VR User Studies. In Proceedings of the 2020 CHI Conference on
Human Factors in Computing Systems. ACM, Honolulu HI USA, 1–15. https:
//doi.org/10.1145/3313831.3376144

[117] Ismo Rakkolainen, Euan Freeman, Antti Sand, Roope Raisamo, and Stephen
Brewster. 2020. A Survey of Mid-Air Ultrasound Haptics and Its Applications.
IEEE Transactions on Haptics (2020), 1–1. https://doi.org/10.1109/TOH.2020.
3018754

[118] Karan Rangarajan, Heather Davis, and Philip H. Pucher. 2020. Systematic Review
of Virtual Haptics in Surgical Simulation: A Valid Educational Tool? Journal of
Surgical Education 77, 2 (March 2020), 337–347. https://doi.org/10.1016/j.jsurg.
2019.09.006

[119] Sharif Razzaque, Zachariah Kohn, and Mary C. Whitton. 2001. EUROGRAPHICS
2001 / Jonathan C. Roberts Short Presentation © The Eurographics Association
2001. Redirected Walking.

[120] Michael Rietzler, Florian Geiselhart, Jan Gugenheimer, and Enrico Rukzio. 2018.
Breaking the Tracking: Enabling Weight Perception using Perceivable Tracking
Offsets. In Proceedings of the 2018 CHI Conference on Human Factors in Computing
Systems - CHI ’18. ACM Press, Montreal QC, Canada, 1–12. https://doi.org/10.
1145/3173574.3173702

[121] Michael Rietzler, Gabriel Haas, Thomas Dreja, Florian Geiselhart, and Enrico
Rukzio. 2019. Virtual Muscle Force: Communicating Kinesthetic Forces Through
Pseudo-Haptic Feedback and Muscle Input. In Proceedings of the 32nd Annual
ACM Symposium on User Interface Software and Technology - UIST ’19. ACM
Press, New Orleans, LA, USA, 913–922. https://doi.org/10.1145/3332165.3347871
[122] Calvin Rubens, Sean Braley, Antonio Gomes, Daniel Goc, Xujing Zhang,
Juan Pablo Carrascal, and Roel Vertegaal. 2015. BitDrones: Towards Levitating
Programmable Matter Using Interactive 3D Quadcopter Displays. In Proceedings
of the 28th Annual ACM Symposium on User Interface Software & Technology
- UIST ’15 Adjunct. ACM Press, Daegu, Kyungpook, Republic of Korea, 57–58.
https://doi.org/10.1145/2815585.2817810

[123] K. Martin Sagayam and D. Jude Hemanth. 2017. Hand posture and gesture
recognition techniques for virtual reality applications: a survey. Virtual Reality
21, 2 (June 2017), 91–107. https://doi.org/10.1007/s10055-016-0301-0

[124] Shahabedin Sagheb, Frank Wencheng Liu, Alireza Bahremand, Assegid Kidane,
and Robert LiKamWa. 2019. SWISH: A Shifting-Weight Interface of Simulated
Hydrodynamics for Haptic Perception of Virtual Fluid Vessels. In Proceedings of
the 32nd Annual ACM Symposium on User Interface Software and Technology -
UIST ’19. ACM Press, New Orleans, LA, USA, 751–761. https://doi.org/10.1145/
3332165.3347870

[125] Majed Samad, Elia Gatti, Anne Hermes, Hrvoje Benko, and Cesare Parise. 2019.
Pseudo-Haptic Weight: Changing the Perceived Weight of Virtual Objects By
Manipulating Control-Display Ratio. In Proceedings of the 2019 CHI Conference
on Human Factors in Computing Systems - CHI ’19. ACM Press, Glasgow, Scotland
Uk, 1–13. https://doi.org/10.1145/3290605.3300550

[126] Massimo Satler, Carlo A. Avizzano, and Emanuele Ruffaldi. 2011. Control of a
desktop mobile haptic interface. In 2011 IEEE World Haptics Conference. IEEE,
Istanbul, 415–420. https://doi.org/10.1109/WHC.2011.5945522

[127] M. Sato. 2002. SPIDAR and virtual reality. In Proceedings of the 5th Biannual World
Automation Congress, Vol. 13. 17–23. https://doi.org/10.1109/WAC.2002.1049515
[128] Gian-Luca Savino. 2020. Virtual Smartphone: High Fidelity Interaction with
Proxy Objects in Virtual Reality. arXiv:2010.00942 [cs] (Oct. 2020). http://arxiv.
org/abs/2010.00942 arXiv: 2010.00942.

[129] Dominik Schmidt, Rob Kovacs, Vikram Mehta, Udayan Umapathi, Sven Köhler,
Lung-Pan Cheng, and Patrick Baudisch. 2015. Level-Ups: Motorized Stilts that
Simulate Stair Steps in Virtual Reality. In Proceedings of the 33rd Annual ACM
Conference on Human Factors in Computing Systems - CHI ’15. ACM Press, Seoul,
Republic of Korea, 2157–2160. https://doi.org/10.1145/2702123.2702253
[130] Martijn J. Schuemie, Peter van der Straaten, Merel Krijn, and Charles A.P.G.
van der Mast. 2001. Research on Presence in Virtual Reality: A Survey. Cy-
berPsychology & Behavior 4, 2 (April 2001), 183–201. https://doi.org/10.1089/
109493101300117884

[131] Valentin Schwind, Pascal Knierim, Nico Haas, and Niels Henze. 2019. Using
Presence Questionnaires in Virtual Reality. In Proceedings of the 2019 CHI Con-
ference on Human Factors in Computing Systems - CHI ’19. ACM Press, Glasgow,
Scotland Uk, 1–12. https://doi.org/10.1145/3290605.3300590

[132] Hasti Seifi, Farimah Fazlollahi, Michael Oppermann, John Andrew Sastrillo,
Jessica Ip, Ashutosh Agrawal, Gunhyuk Park, Katherine J. Kuchenbecker, and
Karon E. MacLean. 2019. Haptipedia: Accelerating Haptic Device Discovery
to Support Interaction & Engineering Design. In Proceedings of the 2019 CHI
Conference on Human Factors in Computing Systems - CHI ’19. ACM Press,
Glasgow, Scotland Uk, 1–12. https://doi.org/10.1145/3290605.3300788

[133] Emily Shaw, Tessa Roper, Tommy Nilsson, Glyn Lawson, Sue V. G. Cobb, and
Daniel Miller. 2019. The Heat is On: Exploring User Behaviour in a Multisensory
Virtual Environment for Fire Evacuation. Proceedings of the 2019 CHI Conference
on Human Factors in Computing Systems - CHI ’19 (2019), 1–13. https://doi.org/
10.1145/3290605.3300856 arXiv: 1902.04573.

[134] Jotaro Shigeyama, Takeru Hashimoto, Shigeo Yoshida, Takuji Narumi, Tomohiro
Tanikawa, and Michitaka Hirose. 2019. Transcalibur: A Weight Shifting Virtual
Reality Controller for 2D Shape Rendering based on Computational Perception
Model. In Proceedings of the 2019 CHI Conference on Human Factors in Computing
Systems - CHI ’19. ACM Press, Glasgow, Scotland Uk, 1–11. https://doi.org/10.
1145/3290605.3300241

[135] Adalberto L. Simeone, Eduardo Velloso, and Hans Gellersen. 2015. Substitutional
Reality: Using the Physical Environment to Design Virtual Reality Experiences.
In Proceedings of the 33rd Annual ACM Conference on Human Factors in Com-
puting Systems - CHI ’15. ACM Press, Seoul, Republic of Korea, 3307–3316.
https://doi.org/10.1145/2702123.2702389

[136] Mike Sinclair, Eyal Ofek, Mar Gonzalez-Franco, and Christian Holz. 2019.
CapstanCrunch: A Haptic VR Controller with User-supplied Force Feedback.
In Proceedings of the 32nd Annual ACM Symposium on User Interface Soft-
ware and Technology - UIST ’19. ACM Press, New Orleans, LA, USA, 815–829.
https://doi.org/10.1145/3332165.3347891

[137] Mike Sinclair, Michel Pahud, and Hrvoje Benko. 2014. TouchMover 2.0 - 3D
touchscreen with force feedback and haptic texture. In 2014 IEEE Haptics Sym-
posium (HAPTICS). IEEE, Houston, TX, USA, 1–6. https://doi.org/10.1109/
HAPTICS.2014.6775425

[138] Alexa F. Siu, Eric J. Gonzalez, Shenli Yuan, Jason B. Ginsberg, and Sean Follmer.
2018. shapeShift: 2D Spatial Manipulation and Self-Actuation of Tabletop Shape
Displays for Tangible and Haptic Interaction. In Proceedings of the 2018 CHI
Conference on Human Factors in Computing Systems - CHI ’18. ACM Press,
Montreal QC, Canada, 1–13. https://doi.org/10.1145/3173574.3173865
[139] Mel Slater. 1999. Measuring Presence: A Response to the Witmer and Singer
Presence Questionnaire. Presence: Teleoperators and Virtual Environments 8, 5
(Oct. 1999), 560–565. https://doi.org/10.1162/105474699566477 Publisher: MIT
Press.

[140] Mel Slater, Martin Usoh, and Anthony Steed. 1994. Depth of Presence in Virtual
Environments. Presence: Teleoperators and Virtual Environments 3, 2 (Jan. 1994),
130–144. https://doi.org/10.1162/pres.1994.3.2.130

[141] Anthony Steed, Sebastian Friston, Vijay Pawar, and David Swapp. 2020. Docking
Haptics: Extending the Reach of Haptics by Dynamic Combinations of Grounded
and Worn Devices. arXiv:2002.06093 [cs] (Feb. 2020). http://arxiv.org/abs/2002.
06093 arXiv: 2002.06093.

[142] Frank Steinicke, Visell Yon, Jennifer Campos, and Anatole Lecuyer (Eds.). 2013.
Human walking in virtual environments: perception, technology, and applications.
Springer, New York,NY. OCLC: 856865949.

[143] Patrick L. Strandholt, Oana A. Dogaru, Niels C. Nilsson, Rolf Nordahl, and
Stefania Serafin. 2020. Knock on Wood: Combining Redirected Touching and
Physical Props for Tool-Based Interaction in Virtual Reality. In Proceedings of the
2020 CHI Conference on Human Factors in Computing Systems. ACM, Honolulu
HI USA, 1–13. https://doi.org/10.1145/3313831.3376303

[144] Evan Strasnick, Christian Holz, Eyal Ofek, Mike Sinclair, and Hrvoje Benko.
2018. Haptic Links: Bimanual Haptics for Virtual Reality Using Variable Stiff-
ness Actuation. In Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems - CHI ’18. ACM Press, Montreal QC, Canada, 1–12.
https://doi.org/10.1145/3173574.3174218

[145] Paul Strohmeier, Seref Güngör, Luis Herres, Dennis Gudea, Bruno Fruchard,
and Jürgen Steimle. 2020. bARefoot: Generating Virtual Materials using Motion
Coupled Vibration in Shoes. In Proceedings of the 33rd Annual ACM Symposium
on User Interface Software and Technology. ACM, Virtual Event USA, 579–593.
https://doi.org/10.1145/3379337.3415828

[146] Yuqian Sun, Shigeo Yoshida, Takuji Narumi, and Michitaka Hirose. 2019. PaCaPa:
A Handheld VR Device for Rendering Size, Shape, and Stiffness of Virtual Objects
in Tool-based Interactions. In Proceedings of the 2019 CHI Conference on Human
Factors in Computing Systems - CHI ’19. ACM Press, Glasgow, Scotland Uk, 1–12.
https://doi.org/10.1145/3290605.3300682

[147] Ivan Sutherland. 1965. The Ultimate Display. (1965), 2.
[148] Ryo Suzuki, Hooman Hedayati, Clement Zheng, James Bohn, Daniel Szafir, Ellen
Yi-Luen Do, Mark D Gross, and Daniel Leithinger. 2020. RoomShift: Room-scale
Dynamic Haptics for VR with Furniture-moving Swarm Robots. (2020), 11.

[149] Ryo Suzuki, Junichi Yamaoka, Daniel Leithinger, Tom Yeh, Mark D. Gross, Yoshi-
hiro Kawahara, and Yasuaki Kakehi. 2018. Dynablock: Dynamic 3D Printing
for Instant and Reconstructable Shape Formation. In The 31st Annual ACM
Symposium on User Interface Software and Technology - UIST ’18. ACM Press,
Berlin, Germany, 99–111. https://doi.org/10.1145/3242587.3242659

[150] Ryo Suzuki, Clement Zheng, Yasuaki Kakehi, Tom Yeh, Ellen Yi-Luen Do, Mark D
Gross, and Daniel Leithinger. 2019. ShapeBots: Shape-changing Swarm Robots.
(2019), 13.

[151] N. Takizawa, H. Yano, H. Iwata, Y. Oshiro, and N. Ohkohchi. 2017. Encountered-
Type Haptic Interface for Representation of Shape and Rigidity of 3D Virtual

"Can I Touch This?"

IHM ’20’21, April 13–16, 2021, Metz, France

Objects. IEEE Transactions on Haptics 10, 4 (Oct. 2017), 500–510. https://doi.
org/10.1109/TOH.2017.2740934

[152] Anthony Talvas, Maud Marchal, and Anatole Lecuyer. 2014. A Survey on
IEEE Transactions on Haptics 7, 3 (July 2014),

Bimanual Haptic Interaction.
285–300. https://doi.org/10.1109/TOH.2014.2314456

[153] Shan-Yuan Teng, Tzu-Sheng Kuo, Chi Wang, Chi-huan Chiang, Da-Yuan Huang,
Liwei Chan, and Bing-Yu Chen. 2018. PuPoP: Pop-up Prop on Palm for Virtual
Reality. In The 31st Annual ACM Symposium on User Interface Software and
Technology - UIST ’18. ACM Press, Berlin, Germany, 5–17. https://doi.org/10.
1145/3242587.3242628

[154] Shan-Yuan Teng, Cheng-Lung Lin, Chi-huan Chiang, Tzu-Sheng Kuo, Liwei
Chan, Da-Yuan Huang, and Bing-Yu Chen. 2019. TilePoP: Tile-type Pop-up Prop
for Virtual Reality. (2019), 11.

[155] Marc Teyssier, Gilles Bailly, Catherine Pelachaud, and Eric Lecolinet. 2020.
Conveying Emotions Through Device-Initiated Touch. IEEE Transactions on
Affective Computing (2020), 1–1. https://doi.org/10.1109/TAFFC.2020.3008693
[156] N. G. Tsagarakis, T. Horne, and D. G. Caldwell. 2005. SLIP AESTHEASIS: a
portable 2D slip/skin stretch display for the fingertip. In First Joint Eurohaptics
Conference and Symposium on Haptic Interfaces for Virtual Environment and
Teleoperator Systems. World Haptics Conference. 214–219. https://doi.org/10.
1109/WHC.2005.117

[157] Hsin-Ruey Tsai and Bing-Yu Chen. 2019. ElastImpact: 2.5D Multilevel Instant
Impact Using Elasticity on Head-Mounted Displays. In Proceedings of the 32nd
Annual ACM Symposium on User Interface Software and Technology. ACM, New
Orleans LA USA, 429–437. https://doi.org/10.1145/3332165.3347931

[158] Hsin-Ruey Tsai and Jun Rekimoto. 2018. ElasticVR: Providing Multi-level Active
and Passive Force Feedback in Virtual Reality Using Elasticity. In Extended
Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
- CHI ’18. ACM Press, Montreal QC, Canada, 1–4. https://doi.org/10.1145/
3170427.3186540

[159] Evgeny Tsykunov, Roman Ibrahimov, Derek Vasquez, and Dzmitry Tsetserukou.
2019. SlingDrone: Mixed Reality System for Pointing and Interaction Using a
Single Drone. In 25th ACM Symposium on Virtual Reality Software and Technology
on - VRST ’19. ACM Press, Parramatta, NSW, Australia, 1–5. https://doi.org/10.
1145/3359996.3364271

[160] Evgeny Tsykunov and Dzmitry Tsetserukou. 2019. WiredSwarm: High Res-
olution Haptic Feedback Provided by a Swarm of Drones to the User’s Fin-
gers for VR interaction. In 25th ACM Symposium on Virtual Reality Software
and Technology on - VRST ’19. ACM Press, Parramatta, NSW, Australia, 1–2.
https://doi.org/10.1145/3359996.3364789

[161] Sebastian Ullrich. 2012. Haptic Palpation for Medical Simulation in Virtual
Environments. IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER
GRAPHICS 18, 4 (2012), 9.

[162] Martin Usoh, Kevin Arthur, Mary C. Whitton, Rui Bastos, Anthony Steed, Mel
Slater, and Frederick P. Brooks. 1999. Walking > walking-in-place > flying, in
virtual environments. In Proceedings of the 26th annual conference on Computer
graphics and interactive techniques - SIGGRAPH ’99. ACM Press, Not Known,
359–364. https://doi.org/10.1145/311535.311589

[163] Martin Usoh, Ernest Catena, Sima Arman, and Mel Slater. 2000. Using Presence
Questionnaires in Reality. Presence: Teleoperators and Virtual Environments 9, 5
(Oct. 2000), 497–503. https://doi.org/10.1162/105474600566989

[164] Varalakshmi, Thriveni, Venugopal, and Patnaik. 2012. Haptics: State of the Art
Survey. IJCSI International Journal of Computer Science Issues (2012). https:
//core.ac.uk/download/pdf/25725449.pdf

[165] David Steeven Villa Salazar, Claudio Pacchierotti, Xavier De Tinguy De
La Girouliere, Anderson Maciel, and Maud Marchal. 2020. Altering the Stiff-
ness, Friction, and Shape Perception of Tangible Objects in Virtual Reality
IEEE Transactions on Haptics (2020), 1–1. https:
Using Wearable Haptics.
//doi.org/10.1109/TOH.2020.2967389

[166] Emanuel Vonach, Clemens Gatterer, and Hannes Kaufmann. 2017. VRRobot:
Robot actuated props in an infinite virtual environment. In 2017 IEEE Virtual
Reality (VR). IEEE, Los Angeles, CA, USA, 74–83. https://doi.org/10.1109/VR.
2017.7892233

[167] Chi Wang, Da-Yuan Huang, Shuo-Wen Hsu, Cheng-Lung Lin, Yeu-Luen Chiu,
Chu-En Hou, and Bing-Yu Chen. 2020. Gaiters: Exploring Skin Stretch Feedback
on the Legs for Enhancing Virtual Reality Experiences. (2020), 14.

[168] Dangxiao Wang, Yuan Guo, Zhang Yuru, XY Weiliang, and WWIA Jing. 2020.
Haptic display for virtual reality: progress and challenges | Elsevier Enhanced
Reader. https://doi.org/10.3724/SP.J.2096-5796.2019.0008 ISSN: 2096-5796.

[169] Dangxiao Wang, Kouhei Ohnishi, and Weiliang Xu. 2020. Multimodal Haptic
Display for Virtual Reality: A Survey. IEEE Transactions on Industrial Electronics
67, 1 (Jan. 2020), 610–623. https://doi.org/10.1109/TIE.2019.2920602

[170] Yuntao Wang, Hanchuan Li, Zhengyi Cao, Huiyi Luo, Ke Ou, John Raiti, Chun
Yu, Shwetak Patel, and Yuanchun Shi. 2020. MoveVR: Enabling Multiform Force
Feedback in Virtual Reality using Household Cleaning Robot. (2020), 12.
[171] Tzu-Yun Wei, Hsin-Ruey Tsai, Yu-So Liao, Chieh Tsai, Yi-Shan Chen, Chi Wang,
and Bing-Yu Chen. 2020. ElastiLinks: Force Feedback between VR Controllers
with Dynamic Points of Application of Force. In Proceedings of the 33rd Annual

ACM Symposium on User Interface Software and Technology. ACM, Virtual Event
USA, 1023–1034. https://doi.org/10.1145/3379337.3415836

[172] Alan Wexelblat. 1993.

Virtual reality: applications and explorations.

http://libertar.io/lab/wp-content/uploads/2016/02/Virtual.Reality.-
.Applications.And_.Explorations.pdf/page=164
cial reality 2 An easy entry to Virtual reality Chap 7.

Myron Krueger, Artifi-

[173] Eric Whitmire, Hrvoje Benko, Christian Holz, Eyal Ofek, and Mike Sinclair.
2018. Haptic Revolver: Touch, Shear, Texture, and Shape Rendering on a Recon-
figurable Virtual Reality Controller. In Proceedings of the 2018 CHI Conference
on Human Factors in Computing Systems - CHI ’18. ACM Press, Montreal QC,
Canada, 1–12. https://doi.org/10.1145/3173574.3173660

[174] Frederik Winther, Linoj Ravindran, Kasper Paabol Svendsen, and Tiare Feucht-
ner. 2020. Design and Evaluation of a VR Training Simulation for Pump
Maintenance Based on a Use Case at Grundfos. In 2020 IEEE Conference on
Virtual Reality and 3D User Interfaces (VR). IEEE, Atlanta, GA, USA, 738–746.
https://doi.org/10.1109/VR46266.2020.1580939036664

[175] Bob G. Witmer and Michael J. Singer. 1998. Measuring Presence in Virtual
Environments: A Presence Questionnaire. Presence: Teleoperators and Virtual En-
vironments 7, 3 (June 1998), 225–240. https://doi.org/10.1162/105474698565686
[176] Haijun Xia, Sebastian Herscher, Ken Perlin, and Daniel Wigdor. 2018. Spacetime:
Enabling Fluid Individual and Collaborative Editing in Virtual Reality. In The
31st Annual ACM Symposium on User Interface Software and Technology - UIST ’18.
ACM Press, Berlin, Germany, 853–866. https://doi.org/10.1145/3242587.3242597
[177] Pingjun Xia. 2016. Haptics for Product Design and Manufacturing Simulation.
IEEE Transactions on Haptics 9, 3 (July 2016), 358–375. https://doi.org/10.1109/
TOH.2016.2554551

[178] Kotaro Yamaguchi, Ginga Kato, Yoshihiro Kuroda, Kiyoshi Kiyokawa, and Haruo
Takemura. 2016. A Non-grounded and Encountered-type Haptic Display Using
a Drone. In Proceedings of the 2016 Symposium on Spatial User Interaction - SUI
’16. ACM Press, Tokyo, Japan, 43–46. https://doi.org/10.1145/2983310.2985746
[179] Jackie (Junrui) Yang, Christian Holz, Eyal Ofek, and Andrew D. Wilson. 2019.
DreamWalker: Substituting Real-World Walking Experiences with a Virtual
Reality. In Proceedings of the 32nd Annual ACM Symposium on User Interface
Software and Technology - UIST ’19. ACM Press, New Orleans, LA, USA, 1093–
1107. https://doi.org/10.1145/3332165.3347875

[180] Yuan-Syun Ye, Hsin-Yu Chen, and Liwei Chan. 2019. Pull-Ups: Enhancing
Suspension Activities in Virtual Reality with Body-Scale Kinesthetic Force
Feedback. In Proceedings of the 32nd Annual ACM Symposium on User Interface
Software and Technology - UIST ’19. ACM Press, New Orleans, LA, USA, 791–801.
https://doi.org/10.1145/3332165.3347874

[181] Yan Yixian, Kazuki Takashima, Anthony Tang, Takayuki Tanno, Kazuyuki Fujita,
and Yoshifumi Kitamura. 2020. ZoomWalls: Dynamic Walls that Simulate Haptic
Infrastructure for Room-scale VR World. In Proceedings of the 33rd Annual ACM
Symposium on User Interface Software and Technology (UIST ’20). Association
for Computing Machinery, New York, NY, USA, 223–235. https://doi.org/10.
1145/3379337.3415859

[182] Yasuyoshi Yokokohji, Ralph L. Hollis, and Takeo Kanade. 1999. WYSIWYF Dis-
play: A Visual/Haptic Interface to Virtual Environment. Presence: Teleoperators
and Virtual Environments 8, 4 (Aug. 1999), 412–434. https://doi.org/10.1162/
105474699566314

[183] Y. Yokokohji, J. Kinoshita, and T. Yoshikawa. 2001.

Path planning for
encountered-type haptic devices that render multiple objects in 3D space. In
Proceedings IEEE Virtual Reality 2001. 271–278. https://doi.org/10.1109/VR.2001.
913796

[184] Yasuyoshi Yokokohji, Nobuhiko Muramori, Yuji Sato, and Tsuneo Yoshikawa.
2005. Haptic Display for Multiple Fingertip Contacts Based on the Observation of
Human Grasping Behaviors.

[185] Shigeo Yoshida, Yuqian Sun, and Hideaki Kuzuoka. 2020. PoCoPo: Handheld
Pin-based Shape Display for Haptic Rendering in Virtual Reality. In Proceedings
of the 2020 CHI Conference on Human Factors in Computing Systems. ACM,
Honolulu HI USA, 1–13. https://doi.org/10.1145/3313831.3376358

[186] Andre Zenner and Antonio Kruger. 2017. Shifty: A Weight-Shifting Dynamic
Passive Haptic Proxy to Enhance Object Perception in Virtual Reality. IEEE
Transactions on Visualization and Computer Graphics 23, 4 (April 2017), 1285–
1294. https://doi.org/10.1109/TVCG.2017.2656978

[187] André Zenner and Antonio Krüger. 2019. Drag:on: A Virtual Reality Controller
Providing Haptic Feedback Based on Drag and Weight Shift. In Proceedings of
the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19. ACM
Press, Glasgow, Scotland Uk, 1–12. https://doi.org/10.1145/3290605.3300441

[188] QinPing Zhao. 2009. A survey on virtual reality. Science in China Series F:
Information Sciences 52, 3 (March 2009), 348–400. https://doi.org/10.1007/s11432-
009-0066-0

[189] Yiwei Zhao and Sean Follmer. 2018. A Functional Optimization Based Approach
for Continuous 3D Retargeted Touch of Arbitrary, Complex Boundaries in
Haptic Virtual Reality. In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems - CHI ’18. ACM Press, Montreal QC, Canada, 1–12.
https://doi.org/10.1145/3173574.3174118

IHM ’20’21, April 13–16, 2021, Metz, France

Bouzbib et al.

[190] Yiwei Zhao, Lawrence H. Kim, Ye Wang, Mathieu Le Goc, and Sean Follmer.
2017. Robotic Assembly of Haptic Proxy Objects for TangibleInteraction and
Virtual Reality. In Proceedings of the Interactive Surfaces and Spaces on ZZZ -
ISS ’17. ACM Press, Brighton, United Kingdom, 82–91. https://doi.org/10.1145/
3132272.3134143

[191] Ning-Ning Zhou and Yu-Long Deng. 2009. Virtual reality: A state-of-the-art
survey. International Journal of Automation and Computing 6, 4 (Nov. 2009),
319–325. https://doi.org/10.1007/s11633-009-0319-9

[192] Mounia Ziat, Taylor Rolison, Andrew Shirtz, Daniel Wilbern, and Carrie Anne
Balcer. 2014. Enhancing virtual immersion through tactile feedback. In Proceed-
ings of the adjunct publication of the 27th annual ACM symposium on User inter-
face software and technology - UIST’14 Adjunct. ACM Press, Honolulu, Hawaii,

USA, 65–66. https://doi.org/10.1145/2658779.2659116

[193] Daniel Zielasko and Bernhard E Riecke. 2020. Either Give Me a Reason to Stand

or an Opportunity to Sit in VR. (2020), 3.

[194] C. B. Zilles and J. K. Salisbury. 1995. A constraint-based god-object method for
haptic display. In In International Conference on Intelligent Robots and Systems.
146–151.

[195] Peter Zimmermann. 2008. Virtual Reality Aided Design. A survey of the use
of VR in automotive industry. (Jan. 2008). https://doi.org/10.1007/978-1-4020-
8200-9_13

