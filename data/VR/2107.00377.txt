VREUD - An End-User Development Tool to
Simplify the Creation of Interactive VR Scenes

Enes Yigitbas, Jonas Klauke, Sebastian Gottschalk, Gregor Engels
Paderborn University, Germany
ﬁrstname.lastname@upb.de

1
2
0
2

l
u
J

1

]

C
H
.
s
c
[

1
v
7
7
3
0
0
.
7
0
1
2
:
v
i
X
r
a

Abstract—Recent advances in Virtual Reality (VR) technology
and the increased availability of VR-equipped devices enable
a wide range of consumer-oriented applications. For novice
developers, however, creating interactive scenes for VR appli-
cations is a complex and cumbersome task that requires high
technical knowledge which is often missing. This hinders the
potential of enabling novices to create, modify, and execute their
own interactive VR scenes. Although recent authoring tools for
interactive VR scenes are promising, most of them focus on expert
professionals as the target group and neglect the novices with low
programming knowledge. To lower the entry barrier, we provide
an open-source web-based End-User Development (EUD) tool,
called VREUD, that supports the rapid construction and execu-
tion of interactive VR scenes. Concerning construction, VREUD
enables the speciﬁcation of the VR scene including interactions
and tasks. Furthermore, VREUD supports the execution and
immersive experience of the created interactive VR scenes on
VR head-mounted displays. Based on a user study, we have
analyzed the effectiveness, efﬁciency, and user satisfaction of
VREUD which shows promising results to empower novices in
creating their interactive VR scenes.

Index Terms—Virtual Reality, End-user Development, Author-

ing, Interactive Scene, Development Tool

I. INTRODUCTION

Recent advances in Virtual Reality (VR) technology and the
increased availability of VR-equipped devices slowly shift the
focus away from the entertainment area to new application
areas in training [1], robotics [2], education [3], or health-
care [4]. These wider application areas of VR require, besides
affordable devices, a usable process of authoring to reach the
full potential [5]. Although there is a vast pool of authoring
tools [6], many of them presuppose technological background
in programming or scripting [7]–[9] or knowledge in modeling
languages [10], [11]. This results in a complex and cumber-
some task to develop interactive VR scenes for novices in VR
development, who have typically lower technical knowledge.
One solution to this problem is to apply End-user Devel-
opment (EUD) methods and techniques which allow novices
to create or modify software artifacts [12]. Furthermore,
modeling languages are used in authoring tools to abstract
the complexity of the development, however, the entry barrier
to learning the language could scare away novices. As a
consequence, we believe that the development of an interactive
VR scene should require a minimal entry barrier to reach a
maximum of novices. To reach this goal, we have chosen the
EUD methods component-based and wizard-based develop-
ment. The component-based development allows an efﬁcient

construction by reusing and conﬁguring of provided compo-
nents. Furthermore, we apply wizard-based development to
lead the novice in a step-by-step manner through difﬁcult
sections in the development. These chosen EUD methods
are implemented in an open-source web-based authoring tool
called VREUD. It addresses the challenges in the development
of an interactive VR scene by supporting the novice in the
construction of the scene. Furthermore, it supports the spec-
iﬁcation of interactions which will be performed by the user
of the VR scene, and the deﬁnition of tasks to guide the VR
user through the scene. Since novices usually prefer to learn by
exploring or trial and error mechanism [13], VREUD provides
rapid construction and execution that enables the novices to
prototype the developed interactive VR scene at each step in
the development. Consequently, VREUD enables the novice
to construct and execute the developed web-based interactive
VR scene. VREUD is inspected by a usability evaluation to
analyze its effectiveness, efﬁciency, and user satisfaction. 15
participants with mixed backgrounds in VR development have
been invited to a user study to evaluate if VREUD supports a
simple development of interactive VR scenes with a low entry
barrier. The results of the usability evaluation suggest that
VREUD enables novices to successfully create their interactive
VR scenes and that VREUD is easy to learn and decreases the
entry barrier in the development.

Our contributions are as following: A technique to abstract
the development of interactions in an interactive VR scene by
using a component-based and wizard-based construction, an
open-source web-based authoring tool to develop web-based
interactive VR scenes with a low entry barrier that can be used
or extended by other researchers, and a usability evaluation
inspecting the effectiveness, efﬁciency, and user satisfaction
of the introduced authoring tool as well as a comparison
between the results of users with no and high experience in
VR development.

The rest of the paper is structured as follows. In Section
2, we present and discuss the related work. In Section 3, we
described the conceptual solution of VREUD. In Section 4, we
show the details of the implementation of VREUD. In Section
5, we present the main results of the usability evaluation and
discuss its results. In Section 6, we conclude the paper and
give an outlook for future work.

 
 
 
 
 
 
II. RELATED WORK

Augmented Reality (AR) and Virtual Reality (VR) have
been a topic of intense research in the last decades. In the past
few years, massive advances in affordable consumer hardware
and accessible software frameworks are now bringing AR and
VR to the masses. AR enables the augmentation of real-world
physical objects with virtual elements and has been already
applied for different aspects such as robot programming [14],
product conﬁguration (e.g., [15], [16]), prototyping [17], plan-
ning and measurements [18] or for realizing smart interfaces
(e.g., [19], [20]). In contrast to AR, VR interfaces support the
interaction in an immersive computer generated 3D world and
have been used in different application domains as motivated
already in the introduction. As the creation of interactive VR
scenes is our main focus, in the following, we draw on prior
research into VR Authoring Tools and End-User Development.

A. VR Authoring Tools

VR authoring tools enable the user to develop interactive
VR scenes. They provide the user with the ability to position
virtual objects in a scene and methods to deﬁne the interactions
between these objects. The game engines from Unity [7] and
Unreal [8] are currently the most popular tools to develop
interactive VR scenes [21]. The area of WebVR, which enables
VR on web pages, has produced frameworks to develop in-
teractive VR scenes as web applications, for example, Threejs
[22] and A-Frame [23]. The problem with these introduced
tools is that they require programming languages like C# and
JavaScript that creates an entry barrier for novices. Arising
from this problem, many authoring tools provide an easier
development of scenes to novices [9], [10], [24]–[26]. In
Spoke [24], the user designs social meeting scenes for Hubs
[27]. Hubs is a web-based VR social meeting platform. Spoke
simpliﬁes the development of the scene to a combination of
elements given to the user. These elements can be modiﬁed and
conﬁgured in their appearance. The design of user interactions
is not possible. PolyVR [9] supports the design of interactions
by scripting. The downside of this approach is that the novice
has to learn the modeling language. Sumerian [10] abstracts
interactions with statecharts. This decreases the entry barrier,
but the novices have to be familiar with statecharts. Game
Rules scEnario Platform (GREP) [25] supports novices in the
development of VR games. It decreases the complexity of
the construction by using smaller games that are deﬁned by
a given archetype. These smaller games are conﬁgured and
combined to a resulting more complex game. This enables
the design of user guidance in the developed VR scene
but the user interactions are only indirectly designed by the
chosen archetypes. This shows a lack of an authoring tool
that supports novices in the design of the scene, interactions,
and tasks. Immersive authoring tools enable the novice to
create and modify the interactive VR scene directly inside
the 3D scene instead of the creation and modiﬁcation of a
2D abstraction of the scene in non-immersive authoring tools.
Another advantage is that Head Mounted Displays (HMDs)
provide the novices with natural interactions. VR GREP [28]

simpliﬁes the development of the scene by supplying the
novice with a set of objects to place in it. The novice can
manipulate them directly with natural interactions. However,
in VR GREP, the user is not capable to design interactions or
tasks in an immersive way. This is solved in FlowMatic [11]
by the usage of functional reactive programming [29]. The
approach provides the novice with a way to create interactions
in a visual model by connecting nodes, which are objects
inside the scene, and operations, that transform them. But
similar to the usage of statecharts, this presupposes learning
for novices. The results of [30] show that immersive authoring
beneﬁts the authoring of VR, however, the current natural
interactions in VR lack the accuracy of a desktop solution.
Additionally, a Head Mounted Display (HMD) is required to
construct the scene and long developing sessions are tiring for
the user. As a consequence, a desktop authoring tool holds
still beneﬁts compared to an immersive authoring tool. The
immersive and non-immersive way of authoring VR scenes is
combined in SAVEace [31] to beneﬁt from both approaches. In
summary, we can conclude that existing immersive and non-
immersive VR authoring tools still require a high degree of
programming knowledge which hinders the rapid creation of
interactive VR scenes for novices.

B. End-User Development

The related work of EUD is focused on component-based

development and wizard-based development.

In component-based development, the software is described
by a connection of components. These components have an
embedded functionality and a deﬁned interface to use them.
These interfaces are used to connect them. This abstracts the
development. With the help of visual or textual interfaces,
the connection of components can be further abstracted to
decrease the entry barrier for novices. Another advantage
is the ﬂexible construction of software since components
can be added easily to the component network or exist-
ing components can be switched with other components.
In [32], component-based development is combined with a
domain-speciﬁc language to enable chemistry teachers the
development of virtual chemistry experiments. This resulted
in a fast and easy construction. Furthermore, in [33]–[35],
component-based development has been applied to support
the engineering of adaptive user interfaces. Cicero Designer
[36] enables novices the development of multi-device museum
guides. XRT [26] enables novices the design of cross-reality
interactions by a connection of predeﬁned components. This
shows that component-based development is an often-used
approach to supply novices with methods to develop software
themselves. In wizard-based Development, the user is guided
through the development process with a wizard. This requires
that the performed task can be split into smaller steps. As
a consequence, the user is focused on one step instead of
all steps at the same time. This approach is combined with
other methods in [37] to customize the visualization of data
in smart homes and in [38] to enable novices to develop e-
government services. Altogether we can observe that existing

EUD methods are already used in various application domains
to ease the development of different application types. Inspired
is to provide an end-user
by these approaches, our goal
development tool to simplify the creation of interactive VR
scenes.

III. SOLUTION OVERVIEW

The goal of VREUD is to decrease the complexity of the de-
velopment of interactive VR scenes by supporting novices with
a way to create interactive VR scenes themselves. Component-
based and wizard-based development are chosen to minimize
the learning effort for novices to guarantee a low entry barrier.
Consequently, the interactive VR scene is built completely by
the combination of components, and complex development
processes are split by the usage of step-by-step wizards. In our
solution approach, the development of interactive VR scenes
consists of the following steps: Construction of the Scene,
Construction of Interactions, and Construction of Tasks.

A. Construction of the Scene

The construction of the scene supports novices in the
development of the visual appearance of their interactive VR
scene. The development is abstracted by a combination of
entities to build the VR scene. These entities describe 3D
virtual objects that deﬁne the appearance of the VR scene.
To provide a comprehensive design of VR scenes the entities
have sub-classes to ﬁt different needs of the speciﬁc virtual
object. These types are shown in the class diagram in Fig. 1.
For example, the entities can describe the Geometry primitives
like a box or cylinder, 3D Models which can be added from
external sources, external Media like pictures, videos, and
PDF documents, Interaction Entities like a button, a counter,
or a pressure plate which provide new interactions methods,
Taskbars which show tasks to the VR user and a Navigation
Mesh which limits the VR user in the navigation by deﬁning
the area the VR user can move onto.

Fig. 1. Class diagram of the entities to describe the scene in VREUD

Every entity contains parameters, that deﬁne the represen-
tation and behavior of the virtual object. For example, a 3D
box has representative parameters depth, height, and width to
conﬁgure the shape of the box and color, or texture to set the
appearance. The parameters position and rotation deﬁne the
location of the entity in the scene and behavioral parameters
conﬁgure, for example, the shadow and physics of the entity.
The novices are supported in the construction by visualizing
the interactive VR scene in the development. This ensures
direct feedback of added entities to the scene and modiﬁed
representative parameters of the entities inside the scene.

This enables the novice to spot mistakes immediately. The
modiﬁcation of parameters is either performed in a form-based
representation of all parameters and the associated values, or
by a direct manipulation interface that utilizes the visualization
of the interactive VR scene to change position, rotation, and
scale of the entity directly inside the shown interactive VR
scene.

B. Construction of Interactions

To transform the developed static VR scene into an inter-
active VR scene, interactions have to be added. Interactions
describe modiﬁcations of the entities inside the VR scene.
They enable the VR user to interact with the entities inside
the interactive VR scene. The developed interactions are event-
based. Every interaction describes only an atomic modiﬁcation
of a speciﬁc entity and is triggered by a single event. As
a consequence, the resulting interaction is not powerful, but
interactions can be combined to create more expressive inter-
actions. However, this restriction allows deﬁning interactions
by components that are conﬁgured by a simple selection of
given elements. An interaction is deﬁned by the following ele-
ments: An Event, that describes the trigger of an interaction. If
the event occurs the designed interaction is performed. These
events can be either triggered by the VR user or other entities
in the scene. For example, when the VR user touches the entity
and presses the trigger button on the controller. An Effect, that
deﬁnes the modiﬁcation of an entity in the scene. For example,
an effect can change the color of an entity. Parameters, which
might be needed to perform the effect. For example, the color
blue to change the entity to blue. Entities, that deﬁne the
context of the interaction. Every interaction is deﬁned by two
entities. These entities can be the same entity or two different
entities. The source entity deﬁnes the source of the event and
the target entity sets the target of the effect. Conditions, which
can be added to an interaction. Conditions deﬁne when an
interaction can be performed by a VR user in the scene. All
applied conditions have to be fulﬁlled to perform the effect
of the interaction. For example, the interaction can only be
performed if an entity is placed in a speciﬁc location.

A simple interaction can be performed by the VR user at
every time. As a result, the interaction contains no conditions.
This allows the usage of an interface that selects only a source
entity, an event, a target entity, an effect, and parameters
needed for the effect which is shown on the left side of Fig.
2. Since the event is triggered on the source entity and the
effect is performed on the target entity, the chosen source
entity deﬁnes the set of possible events and the chosen target
entity deﬁnes the set of possible effects. The interface adapts
the chosen event and effect to changes in the selected source
and target entity. This guarantees a correct construction of the
interaction since the novice can not choose an event or effect
which is not possible for the chosen source or target entity.

A complex interaction is an interaction that can not always
be performed by the VR user. Consequently, conditions are
applied to the construction of interactions. To not overload
the interface, a button is added to the interaction interface

EntityModelTextureGeometryMediaTextInteraction EntityNavigation MeshTaskbar10..*LightCameraFig. 2. The interface of the interaction construction. The current conﬁguration
results in an interaction that changes the color of Box1 to blue if the controller
touches Box1 and the trigger is pressed. The complex button opens a wizard
to add conditions. The conﬁgured condition checks if Box1 has not the color
red before performing the interaction.

Fig. 3. The pattern dialog to create interactivity around a video. The user
creates interactions just by the selection of entities. Box1 is conﬁgured as
entity to play the video by pressing on it.

shown on the left side of Fig. 2 to open a wizard that performs
the construction of the complex interaction. The setup of a
condition is shown on the right side of Fig. 2. A condition
is conﬁgured by a condition type and parameters associated
with the chosen type. This enables the usage of a component
to check the speciﬁc condition that is conﬁgured by a simple
selection. For example, the condition type Entity fulﬁlls a
speciﬁc attribute creates a condition that checks if a chosen
attribute of a selected entity fulﬁlls a conﬁgured expression.
As a result, the complex interaction can only be performed by
the VR user if the attribute of the entity fulﬁlls the expression.
The conditions can be combined to deﬁne more complex
conditions. The interface of the condition construction adapts
to already created conditions to prevent conditions that conﬂict
with each other. The previously introduced interfaces produce
interactions for atomic actions that require an understanding
of the events and effects in the scene. To further decrease
the cognitive effort required to perform the construction,
interactivity patterns are supported in VREUD. These patterns
are components in the editor which use a wizard to conﬁgure
the pattern. The components have deﬁned input and output to
ensure the easy extension of new patterns. The patterns can
result in a set of added entities, interactions, tasks, and changed
parameters of existing elements. As a result, these patterns can
perform a set of construction steps in one step. Consequently,
the abstraction level is higher. Fig. 3 shows one step in the
wizard of the construction of the interactivity associated with a
video in the scene. In the pattern, the novice has to only select
entities, which, in this example dialog, start and pause the
video. As a result, the novice has not created both interactions
manually which saves her time in the construction.

C. Construction of Tasks

The construction of tasks supports novices in the develop-
ment of tasks to guide the VR user through their developed
interactive VR scene. Tasks deﬁne actions the VR user has to
perform in the interactive VR scene. The construction of tasks
is inspired by the approach of GREP [25], which resulted in
an easy-to-use and understandable construction of education

games. Similar to GREP, a task is deﬁned by a set of smaller
tasks. These smaller tasks will be called activities. An activity
is conﬁgured by an activity type that deﬁnes the atomic action
the VR user has to perform. This action is tracked by the
activity in the interactive VR scene. This allows encapsulating
the activity in a component that is conﬁgured to the speciﬁc
needs of the novice. Since the VR user has to be aware
of the tasks inside the interactive VR scene, the tasks and
activities have parameters to conﬁgure the names and the
descriptions that are shown inside the interactive VR scene.
The construction uses two dialogs to create the task and the
activities so that the novice only needs to focus on the current
task or activity. The two dialogs are shown in Fig. 4. Since
some activity types are location dependable, an area has to be
deﬁned. These areas are spawned after the task construction
and they are set up similar to the entities in the construction
of the scene. These areas are also used to decrease the issue
to describe the area in the activity description by showing the
transparent area directly in the interactive VR scene to the VR
user. An area is shown in Fig. 5. To not overload the scene,
the area of a solved activity disappears.

Fig. 4. The dialog to create a task and the dialog to create an activity. The
task consists of two activities and the shown activity tracks if the VR user
has entered a speciﬁc location.

Fig. 5. An interactive VR scene developed with VREUD. It is a living room
that shows a presentation (a) to the user. An interaction enables the user to
open the next page by pressing on the remote (b). To complete the task, the
user has to go to the area (c).

IV. IMPLEMENTATION
VREUD is an open-source web-based authoring tool1.
VREUD has a server-side and a client-side. The system
architecture is presented in Fig. 6 and the architecture of the
VREUD Generator, that generates the interactive VR scenes,
is shown in Fig. 7. The server-side uses Express [39] as a
web server. The stored data on the server is divided into
three repositories. Models and Media stores added models
and media by the novices, Interactive VR Scenes saves the
generated interactive VR scenes, and A-Frame Components
stores components that implement the developed interactions
and required behavior of the entities since the generated
interactive VR scene uses A-Frame [23]. A-Frame is a web-
based VR framework. The VREUD client-side utilizes Re-
act [40], a library for building user interfaces, and Threejs
[22], a framework for web-based 3D scenes, to implement
the component-based interface and to show the interactive
VR scene directly in the interface. The component-based
architecture enables easy extensions of new features in the
interface. The interface is presented in Fig. 8. The interactive
VR scene is deﬁned by objects instantiated from a class system
implemented in JavaScript that provides objects for activities,
entities, interactions, and tasks. These objects are stored in the
Current Session. VREUD uses these objects to generate the
interactive VR scene with the help of the VREUD Generator
component. The generated interactive VR scene is translated in
A-Frame. A-Frame builds on Threejs to describe VR scenes.
Consequently, both representations of the interactive VR scene
do not conﬂict with each other, since both use Threejs.

The VREUD Generator produces an interactive VR scene
implemented in A-Frame from a given set of objects stored in
Current Session on the client-side. Virtual objects in A-Frame
are deﬁned by an entity-component-system architecture. In it,
virtual objects are described by entities containing compo-
nents. These components deﬁne the appearance or the behavior
of the entity. To implement the component-based approach of
VREUD, the generator uses predeﬁned components that are

1https://github.com/VREUD/VREUD

Fig. 6. The system architecture of VREUD

conﬁgured by the novices. These components are stored in A-
Frame Components. The architecture of the generator is shown
in Fig. 7. First of all, in the Scene Generator, the entity is
mapped to an A-Frame entity to implement the appearance. If
an external model or media is used, the generator maps the
entity to the stored model or media in Models and Media.
Then Control Components are added if the entity requires
them. For example, a PDF document uses them to control the
shown page. After this step, the scene is completely generated
and interactions are added by the Interaction Generator.
These interactions are implemented by Effect Components
which are associated with the chosen effect in the interaction
construction. The component is conﬁgured and applied to the
source entity since the event has to be listened at this entity.
The optional conditions are also implemented by Checker
Components which are also conﬁgured by the novice. The
Task Generator adds new invisible A-Frame entities to the
scene to implement the tasks and activities. Activity entities
use Tracker Components to check the conﬁgured actions in the
interactive VR scene. If the activity uses an area, a 3D box is
used as the body. The activities are connected to the tasks to
inform them about the completion. Finally, basic user controls
are applied to the scene by the usage of speciﬁc User Control
Components. The resulting interactive VR scene is stored in
Interactive VR Scenes on the server-side to provide access to
the VR user.

Fig. 7. Architectural overview of the VREUD Generator

In summary, VREUD can execute interactive VR scenes. To
utilize this, the construction is designed to enable prototyping
at each step to experience the current interactive VR scene.
This provides direct feedback of the developed interactive VR

Client (VREUD)InterfaceVREUD GeneratorReactThreejsExecution EnvironmentA-FrameCurrent SessionAdd / ModifyShowDataServerExpressModels & MediaInteractive VR ScenesA-Frame ComponentsManageManageDataUploadUpload InteractiveVR SceneModel & MediaComponentsInteractiveVR SceneVREUD GeneratorScene GeneratorInteraction GeneraorTaskGeneratorCurrentSessionEntitiesInteractionsTasksTransform toA-FrameEntitiesAdd ControlComponentsModels & MediaBasicControllsA-Frame ComponentsControlComp.EffectComp.Tracker Comp.User ControlComp.GeneratedInteractiveVR SceneInteractiveVRScenesGenerateInteractionApplyConditionsCheckerComp.Fig. 8. The interface of the VREUD. (a) lists all entities to instantiate in the scene, (b) visualizes the interactive VR scene, (c) shows the direct manipulation
interface of the selected entity, (d) shows the form to modify parameters of the selected entity, (e) lists all entities, interactions, and tasks of the interactive
VR scene, (f) shows the interface to create interactions and (g) executes the developed interactive VR scene.

scene and the novice can explore the construction. Addition-
ally, an interactive tutorial supports the novices in their ﬁrst
construction.

V. EVALUATION

We conducted a usability evaluation to evaluate the ef-
fectiveness, efﬁciency, and user satisfaction of VREUD. To
rate the effectiveness and efﬁciency, the participants had to
solve assignments whose required time and successful com-
pletion are measured. The user satisfaction is measured by
a questionnaire. The evaluation is performed to gain insights
into the required entry barrier of VREUD and to assess the
construction of scenes, interactions, and tasks. To estimate the
usefulness of VREUD for novices, we have invited participants
with mixed knowledge in VR development to compare the
results of VR novices against VR experts. We have decided to
perform the usability evaluation remotely, due to the situation
caused by COVID and the web-based architecture of VREUD
that provides easy remote access.

A. Participants and Procedure

We recruited 15 volunteer participants for the usability
evaluation via e-mail invitation. The reported experience level
of the participants in VR development is divided into 9 novices
(8 no and 1 low experience) and 6 experts (3 medium and
3 high experience). All experts are familiar with other VR
authoring tools.

The usability evaluation was structured in the following
way. At ﬁrst, we sent the participants an archive ﬁle containing
all required data. The archive ﬁle included an introduction to
the evaluation process and VREUD, an assignment list, an
image ﬁle, and a scene as a starting point of the evaluation.
After reading the introduction, each participant had to solve
a list of assignments. The assignments were divided into the
following tasks:

• Navigation in the Scene: To evaluate the navigation
interface of VREUD, each participant had to navigate to
a speciﬁc point, select an entity and delete it.

• Scene Adaptation: To evaluate the direct manipulation
interface of VREUD, each participant had to add an
external image. This image had to be manipulated to
cover a speciﬁc area in the scene by modifying the
position, rotation, and scale of the entity.

• Single Interaction: To evaluate the ﬁrst construction
of a simple interaction, each participant had to add an
interaction to start a video by pressing a button. The video
and button are already contained in the scene. Finally, the
participant had to test the interaction in the VREUD web
editor.

• Combined Interactivity: To further evaluate the inter-
action design, each participant had to set up a pressure
plate, that provides location-based events since it detects
when a VR user enters and leaves the plate. Afterwards,
the participant had to add 5 interactions concerning
the interactivity of a presentation. The ﬁrst interaction
enables to open the next page by pressing on a button, the
second interaction shows the presentation when the VR
user enters the pressure plate, the third interaction resets
the presentation when entering the plate, the fourth inter-
action hides the presentation when leaving the pressure
plate and the last interaction changes the color of a plate
to green when the presentation is completed. Finally, the
developed interactivity had to be tested in the VREUD
web editor by the participant.

• Task Construction: To evaluate the construction of a
task,
the participant had to create a task. The task
contained two activities. The ﬁrst activity is to go on
the stage in the scene. It requires setting up an area.
The second activity is to complete the presentation. If
the participant has given up on the previous assignment,
the participant has to create an activity to look on the
fridge in the scene. Finally, the developed task had to be
tested in the VREUD web editor by the participant.

While performing the assignment, each participant had to
measure the time for completing it. The participant was

allowed to give up on the assignment which was noted
down by them. The participants were not monitored by us,
consequently, they had to save after each assignment their
session. These save ﬁles were used to check if the participant
succeeded in the assignment. The succession rate in the com-
bined interactivity assignment is calculated by the successfully
designed interactions since the assignment is more complex.
After the assignments, the participant had to ﬁll out an online
questionnaire that consisted of a System Usability Scale (SUS)
[41] and statements about the difﬁculty and learnability of
the construction of the scene, interactions, and tasks. The
statements were rated on a 5-point Likert scale (1 strongly
disagree and 5 strongly agree). Finally, the participants had to
sent their measured times, their list of given up assignments,
and their set of saved sessions to us.

B. Results

The results concerning efﬁciency are presented in Fig. 9.
The navigation in the scene was completed in close average
times by both groups (37s experts and 40s novices). The scene
adaptation was successfully performed on average in 115s by
the experts and 206s by the novices. The single interaction
construction was succeeded on average in 175s by the experts
and 286s by the novices. In the combined interaction, the two
groups have a bigger average difference (562s experts and
819s novices). But the median of experts and novices is closer
(553s experts and 666s novices). The task construction was
completed on average in 437s by the experts and 504s by the
novices.

Fig. 10. The effectiveness results (in percent) of the usability evaluation.

presented in Fig. 11. The highlight of the results is the conﬁ-
dence of the participants to design interactions with VREUD
(Q5) which was agreed by all except one novice and one
expert. This also affects the results of the effects of the created
interactions (Q4) that are clear to the participants and the
results of the easy-to-learn statement (Q2) of the interaction
construction. The only not so well-performing statement is
the understandability of the events in the construction of
interactions (Q3) that shows there is room for improvement
in the self-explanatory of the events compared to the results
of the effects and activity types. However, the participants had
still fun in the construction of the interactive VR scene (Q9),
which was agreed upon by all except one novice. The best
results produced the statement if the prototyping supported
the participants in the development of interactive scenes (Q8).

Fig. 9. The efﬁciency results of the usability evaluation.

The results concerning effectiveness are presented in Fig.
10. The navigation in the scene was completed by all partici-
pants. The scene adaptation was successfully performed by all
experts. Two novices manipulated successfully an image in the
scene but they did not add an image. The single interaction
construction was succeeded by all except one novice and
the novices had
one expert. In the combined interaction,
constructed two interactions incorrectly and missed adding
three interactions. The expert missed adding two interactions.
The two participants who failed the previous assignment have
succeeded. The task construction was completed by all, except
for three novices. Two did not set up the area and one put
the area at an incorrect spot. The results of the questionnaire
about the construction of the scene, interactions, and tasks are

Fig. 11. Results of the questionnaire. The results of the negative statements
are inverted

The results of the SUS are presented in Fig. 12. The high-
lights are the results of the statement if the participants require
assistance from a technical person which was disagreed by all
except one novice. Another highlight is that the participants
agreed on the statement that they did not need to learn much
to use VREUD. The statement that VREUD is easy to use was
only disagreed by a novice and an expert which is equal to
the statement if VREUD is too complicated which was only
disagreed by the same participants. The worst results have
produced the statement about the cumbersomeness of VREUD.
This shows that the usability of the editor can be improved to
provide a more intuitive construction. The average SUS score
is 71. This is above the mean score of web interfaces (68.2)
and close to the average score of the adjective good (71.4)
[42].

66,788,988,977,810010093,383,3100100020406080100TaskConstruc�onCombined Interac�vitySingle Interac�onScene Adapta�onNaviga�on in the SceneEﬀec�veness ResultsExpertsNovices#Questionnaire StatementsNovicesExpertsScore P1P2P3P4P5P6P7P8P9P10P11P12P13P14P15Q155445442455555587Q255445545355534587Q355445442324434572Q425455545455555590Q555355555345555592Q655555525355555592Q734553555344554482Q845554555555455595Q945555245544434480The manipulation of entities was hard to learn.The construction of interactions was easy to learn.The events in the interaction design were hard to understand.The effects of interaction on a target were unclear.I felt confident in the creation of my interactions.I understood the activity types in the task constructionThe construction of tasks was too complicated.The prototyping was useful to construct interactive scenes.I had fun in the construction of the interactive scene.Fig. 12. Results of the SUS questionnaire. The results of the negative statements are inverted

C. Discussion and Limitation

Our results suggest that VREUD is easy to use and easy to
learn. This indicates the potential of VREUD to aid novices
in the construction of interactive VR scenes since the results
of efﬁciency and effectiveness from both groups are often
close together. The user satisfaction also suggests that the
component-based and wizard-based development have suc-
cessfully simpliﬁed the construction of interactive VR scenes,
due to the wide acceptance of the participants. The efﬁciency
results of the single and combined construction of interactions
suggest that the novice can design interactions fast after an
initial small learning time. This is shown by the fact that
the average time of the combined interaction design is not
ﬁve times higher than the single construction of interaction,
although the participant had to create ﬁve interactions. The ad-
ditional setup of the pressure plate indicates that the interaction
setup was even faster in the combined interaction construction.
However, the results also present that the participants required
three to ﬁve minutes on average to set up their ﬁrst interac-
tions. Consequently, the logic embedded in the construction
of interactions indicates to require an initial learning time.
Combined with the mixed results of the understandability of
events, it could affect the results of novices with no technical
background. A solution to this issue could be the construction
of interactions with the help of interactivity patterns since the
higher abstraction suggests that it could help novices who
have issues to understand the embedded logic. On the other
hand, compared to the great results of the participants in
the statement that they felt conﬁdent in the construction of
interactions, this suggests being a smaller issue. The results of
the construction of tasks indicate that a wizard combined with
the deﬁnition of a task by a set of smaller activities that are
conﬁgured by an activity type and associated parameters has
successfully lowered the complexity of the development. The
self-explanatory of the activity types was greatly accepted by
the participants. Compared to the logic used in the construction
of interactions, the task construction uses generic activities
which should be also clear to novices without a technical
background, since they are comparable to actions in the real
world. For example, an activity type asks the VR user to put an
object into an area. However, the usability evaluation shows
an issue in the current implementation, since three novices
have forgotten to set up the area after the construction. Apart

from that,
the construction of tasks is still well accepted
among the participants. Finally, the results of the statement
if the participant had fun constructing the interactive VR
scene suggests that VREUD supplies novices with a playful
construction of interactive VR scenes, which could motivate
them to develop their ﬁrst interactive VR scene.

A limitation of the conducted usability evaluation is that
VREUD was not evaluated by novices with no technical
background, since all participants had a background in com-
puter science. The results could not apply to novices with
no technical background. These users could have issues with
the embedded logic in the construction of simple interactions.
However, the interactivity pattern provides a higher abstraction
in the construction and the interactive tutorial supports the
novice in their ﬁrst session. These features combined with
the low entry barrier of VREUD could successfully support
novices with no technical background in the development.

VI. CONCLUSION AND FUTURE WORK

In this paper, we introduced a web-based authoring tool
for web-based interactive VR scenes called VREUD that
lowers the entry barrier for novices to develop their interactive
VR scenes. We achieved this by supporting novices with a
component-based construction of the scene, interactions, and
tasks of the interactive VR scene. We decreased further the
complexity by using wizards to focus the user on smaller
steps in the development. The effectiveness, efﬁciency, and
user satisfaction of VREUD were evaluated by a conducted
usability evaluation. We recruited 15 volunteers with mixed
backgrounds in VR development to compare the results of
novices and experts. Our results show that our tool is easy to
learn and that it succeeds in lowering the entry barrier of the
development of interactive VR scenes.

In future work, VREUD has to be evaluated with a larger
group of heterogeneous end-users. With this regard, it should
be especially investigated if VREUD supports novices with no
technical background in constructing their own interactive VR
scenes. The web-based architecture of VREUD and the gener-
ated interactive VR scenes animate to use them collaboratively.
This arises new challenges in the development, for example, in
the construction of social interactions. VREUD could connect
desktop users and VR users to support a collaborative design
and development of interactive VR scenes.

#SUS StatementsNovicesExpertsP1P2P3P4P5P6P7P8P9P10P11P12P13P14P151I think that I would like to use this system frequently. 444542425232434622I found the system unnecessarily complex. 355553323255445733I thought the system was easy to use. 454454423244345704I think that I would need the support of a technical person to be able to use this system. 554555455555345925I found the various functions in this system were well integrated. 244345524245444686I thought there was too much inconsistency in this system. 554444543554245807I would imagine that most people would learn to use this system very quickly. 544554245442244728I found the system very cumbersome to use. 243422213243444489I felt very confident using the system. 3542524434453457010I needed to learn a lot of things before I could get going with this system. 24545553555512475SUS Score by Participant62,587,577,577,585657047,572,557,582,5755067,587,5Avg. SUS Score: 71SUS Score by QuestionREFERENCES

[1] E. Yigitbas, I. Jovanovikj, J. Scholand, and G. Engels, “VR training
for warehouse management,” in VRST ’20: 26th ACM Symposium
on Virtual Reality Software and Technology, R. J. Teather, C. Joslin,
W. Stuerzlinger, P. Figueroa, Y. Hu, A. U. Batmaz, W. Lee, and F. R.
Ortega, Eds. ACM, 2020, pp. 78:1–78:3.

[2] E. Yigitbas, K. Karakaya, I. Jovanovikj, and G. Engels, “Enhancing
human-in-the-loop adaptive systems through digital
twins and VR
interfaces,” CoRR, vol. abs/2103.10804, 2021. [Online]. Available:
https://arxiv.org/abs/2103.10804

[3] E. Yigitbas, C. B. Tejedor, and G. Engels, “Experiencing and pro-
gramming the ENIAC in VR,” in Mensch und Computer 2020, F. Alt,
S. Schneegass, and E. Hornecker, Eds. ACM, 2020, pp. 505–506.
[4] E. Yigitbas, J. Heind¨orfer, and G. Engels, “A context-aware virtual
reality ﬁrst aid training application,” in Proc. of Mensch und Computer
2019, F. Alt, A. Bulling, and T. D¨oring, Eds. GI / ACM, 2019, pp.
885–888.

[5] R. D¨orner, M. Kallmann, and Y. Huang, Content Creation and Au-
thoring Challenges for Virtual Environments: From User Interfaces to
Autonomous Virtual Characters, 04 2015, pp. 187–212.

[6] M. Nebeling and M. Speicher, “The trouble with augmented real-
ity/virtual reality authoring tools,” in 2018 IEEE International Sympo-
sium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), 2018,
pp. 333–337.

[7] Unity. (2021) Unity real-time development platform 3d, 2d vr & ar

engine. Accessed: 2021-05-07. [Online]. Available: https://unity.com/

[8] Unreal. (2021) The most powerful real-time 3d creation platform. Ac-
cessed: 2021-05-07. [Online]. Available: https://www.unrealengine.com/
[9] V. Haefner, “PolyVR - A Virtual Reality Authoring System,” in EuroVR
2014 - Conference and Exhibition of
the European Association of
Virtual and Augmented Reality, J. Perret, V. Basso, F. Ferrise, K. Helin,
V. Lepetit, J. Ritchie, C. Runde, M. van der Voort, and G. Zachmann,
Eds. The Eurographics Association, 2014.

[10] Amazon. (2021) Amazon sumerian overview. Accessed: 2021-05-07.

[Online]. Available: https://aws.amazon.com/sumerian/

[11] L. Zhang and S. Oney, “Flowmatic: An immersive authoring tool
reality,” in Proceedings
for creating interactive scenes in virtual
of
the 33rd Annual ACM Symposium on User Interface Software
and Technology, ser. UIST ’20. New York, NY, USA: Association
for Computing Machinery, 2020, p. 342–353. [Online]. Available:
https://doi.org/10.1145/3379337.3415824
F.

Patern`o, M. Klann,

Lieberman,

[12] H.

End-User Development: An Emerging Paradigm.
Springer Netherlands,
https://doi.org/10.1007/1-4020-5386-X 1

2006,

1–8.

pp.

and V. Wulf,
Dordrecht:
[Online]. Available:

[13] M. Rettig, “Nobody reads documentation,” Commun. ACM, vol. 34,
no. 7, p. 19–24, Jul. 1991. [Online]. Available: https://doi.org/10.1145/
105783.105788
I.

Jovanovikj, and G. Engels, “Simplifying robot
programming using augmented reality and end-user development,”
CoRR, vol. abs/2106.07944, 2021. [Online]. Available: https://arxiv.org/
abs/2106.07944

[14] E. Yigitbas,

[15] S. Gottschalk, E. Yigitbas, E. Schmidt, and G. Engels, “Model-based
product conﬁguration in augmented reality applications,” in Human-
Centered Software Engineering - 8th IFIP WG 13.2 International
Working Conference, HCSE 2020, Eindhoven, The Netherlands,
November 30 - December 2, 2020, Proceedings, ser. Lecture Notes
in Computer Science, R. Bernhaupt, C. Ardito, and S. Sauer,
Eds., vol. 12481.
Springer, 2020, pp. 84–104. [Online]. Available:
https://doi.org/10.1007/978-3-030-64266-2 5

[16] ——, “Proconar: A tool

support

for model-based AR product
conﬁguration,” in Human-Centered Software Engineering - 8th IFIP
WG 13.2 International Working Conference, HCSE 2020, Eindhoven,
The Netherlands, November 30 - December 2, 2020, Proceedings, ser.
Lecture Notes in Computer Science, R. Bernhaupt, C. Ardito, and
S. Sauer, Eds., vol. 12481.
Springer, 2020, pp. 207–215. [Online].
Available: https://doi.org/10.1007/978-3-030-64266-2 14

[17] I. Jovanovikj, E. Yigitbas, S. Sauer, and G. Engels, “Augmented and
virtual reality object repository for rapid prototyping,” in Human-
Centered Software Engineering - 8th IFIP WG 13.2 International
Working Conference, HCSE 2020, Eindhoven, The Netherlands,
November 30 - December 2, 2020, Proceedings, ser. Lecture Notes
in Computer Science, R. Bernhaupt, C. Ardito, and S. Sauer,

Eds., vol. 12481. Springer, 2020, pp. 216–224. [Online]. Available:
https://doi.org/10.1007/978-3-030-64266-2 15

[18] E. Yigitbas, S. Sauer, and G. Engels, “Using augmented reality for
enhancing planning and measurements in the scaffolding business,”
in EICS ’21: ACM SIGCHI Symposium on Engineering Interactive
Computing Systems, virtual, June 8-11, 2021. ACM, 2021. [Online].
Available: https://doi.org/10.1145/3459926.3464747

[19] S. Krings, E. Yigitbas,

framework
in EICS

I. Jovanovikj, S. Sauer, and G. Engels,
reality
“Development
Symposium on
applications,”
Sophia Antipolis,
Engineering
France,
and
M. Winckler, Eds. ACM, 2020, pp. 9:1–9:6. [Online]. Available:
https://doi.org/10.1145/3393672.3398640

for
’20: ACM SIGCHI

Interactive Computing

June 23-26, 2020,

J. Vanderdonckt,

context-aware

J. Bowen,

augmented

Systems,

[20] E. Yigitbas,

I.

Jovanovikj, S. Sauer, and G. Engels, “On the
development of context-aware augmented reality applications,” in
Beyond Interactions - INTERACT 2019 IFIP TC 13 Workshops, Paphos,
Cyprus, September 2-6, 2019, Revised Selected Papers, ser. Lecture
Notes in Computer Science, J. L. Abdelnour-Nocera, A. Parmaxi,
M. Winckler, F. Loizides, C. Ardito, G. Bhutkar, and P. Dannenmann,
Eds., vol. 11930. Springer, 2019, pp. 107–120. [Online]. Available:
https://doi.org/10.1007/978-3-030-46540-7 11

[21] S. Stephenson. (2019, 8) Tiga survey reveals that unity 3d engine
dominates the uk third party engine market. Accessed: 2021-05-07.
[Online]. Available: https://tiga.org/news/tiga-survey-reveals-that-unity-
3d-engine-dominates-the-uk-third-party-engine-market

[22] R. Cabello. (2021) Three.js. Accessed: 2021-05-07. [Online]. Available:

https://threejs.org/

[23] A-Frame. (2021) A-frame. Accessed: 2021-05-07. [Online]. Available:

https://aframe.io/

[24] Mozilla. (2021) Spoke by mozilla. Accessed: 2021-05-07. [Online].

Available: https://hubs.mozilla.com/spoke

[25] T. Zarraonandia, P. Diaz, and I. Aedo, “Using combinatorial creativity
to support end-user design of digital games,” Multimedia Tools
Appl., vol. 76, no. 6, p. 9073–9098, Mar. 2017. [Online]. Available:
https://doi.org/10.1007/s11042-016-3457-4

[26] A. Bellucci, T. Zarraonandia, P. D´ıaz, and I. Aedo, “End-user
prototyping of cross-reality environments,” in Proceedings of
the
Eleventh International Conference on Tangible, Embedded, and
Embodied Interaction, ser. TEI ’17. New York, NY, USA: Association
for Computing Machinery, 2017, p. 173–182. [Online]. Available:
https://doi.org/10.1145/3024969.3024975

[27] Mozilla. (2021) Hubs - private social vr in your web browser. Accessed:

2021-05-07. [Online]. Available: https://hubs.mozilla.com/#/

[28] T. Zarraonandia, P. D´ıaz, I. Aedo, and A. Montero, “Inmersive end user
development for virtual reality,” in Proceedings of the International
Working Conference on Advanced Visual Interfaces, ser. AVI ’16.
New York, NY, USA: Association for Computing Machinery, 2016, p.
346–347. [Online]. Available: https://doi.org/10.1145/2909132.2926067
[29] C. Elliott and P. Hudak, “Functional reactive animation,” SIGPLAN
Not., vol. 32, no. 8, p. 263–273, Aug. 1997. [Online]. Available:
https://doi.org/10.1145/258949.258973

[30] T. Zarraonandia, P. D´ıaz, A. Montero, and I. Aedo, “Exploring the
beneﬁts of immersive end user development for virtual reality,” in Ubiq-
uitous Computing and Ambient Intelligence, C. R. Garc´ıa, P. Caballero-
Gil, M. Burmester, and A. Quesada-Arencibia, Eds. Cham: Springer
International Publishing, 2016, pp. 450–462.

[31] R. Holm, E. Stauder, R. Wagner, M. Priglinger, and J. Volkert, “A com-
bined immersive and desktop authoring tool for virtual environments,”
in Proceedings IEEE Virtual Reality 2002, 2002, pp. 93–100.

[32] Y. Zhong and C. Liu, “A domain-oriented end-user design environment
for generating interactive 3d virtual chemistry experiments,” Multimedia
Tools Appl., vol. 72, no. 3, p. 2895–2924, Oct. 2014. [Online]. Available:
https://doi.org/10.1007/s11042-013-1554-1

[33] E. Yigitbas, K. Josifovska, I. Jovanovikj, F. Kalinci, A. Anjorin,
and G. Engels, “Component-based development of adaptive user
interfaces,” in Proceedings of
the ACM SIGCHI Symposium on
Engineering Interactive Computing Systems, EICS 2019, Valencia,
Spain, June 18-21, 2019,
J. Vanderdonckt, and
O. Pastor, Eds. ACM, 2019, pp. 13:1–13:7. [Online]. Available:
https://doi.org/10.1145/3319499.3328229

I. Panach,

J.

[34] E. Yigitbas, I. Jovanovikj, K. Biermeier, S. Sauer, and G. Engels,
“Integrated model-driven development of self-adaptive user interfaces,”

Softw. Syst. Model., vol. 19, no. 5, pp. 1057–1081, 2020. [Online].
Available: https://doi.org/10.1007/s10270-020-00777-7

[35] E. Yigitbas, A. Hottung, S. M. Rojas, A. Anjorin, S. Sauer, and
G. Engels, “Context- and data-driven satisfaction analysis of user
interface adaptations based on instant user feedback,” Proc. ACM Hum.
Comput. Interact., vol. 3, no. EICS, pp. 19:1–19:20, 2019. [Online].
Available: https://doi.org/10.1145/3331161

[36] G. Ghiani, F. Patern`o, and L. D. Spano, “Cicero designer: An envi-
ronment for end-user development of multi-device museum guides,” in
End-User Development, V. Pipek, M. B. Rosson, B. de Ruyter, and
V. Wulf, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2009,
pp. 265–274.

[37] N. Castelli, C. Ogonowski, T. Jakobi, M. Stein, G. Stevens, and V. Wulf,
“What happened in my home? an end-user development approach
for smart home data visualization,” in Proceedings of the 2017 CHI
Conference on Human Factors in Computing Systems, ser. CHI ’17.
New York, NY, USA: Association for Computing Machinery, 2017, p.
853–866. [Online]. Available: https://doi.org/10.1145/3025453.3025485
[38] D. Fogli and L. Parasiliti Provenza, “A meta-design approach
to the development of e-government services,” Journal of Visual
Languages & Computing, vol. 23, no. 2, pp. 47–62, 2012,
special
issue dedicated to Prof. Piero Mussio. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1045926X11000747
[Online].

(2021) Express. Accessed: 2021-05-07.

[39] O. Foundation.

Available: https://expressjs.com/

[40] F. O. Source. (2021) React. Accessed: 2021-05-07. [Online]. Available:

https://reactjs.org/

[41] J. Brooke, “Sus: A quick and dirty usability scale,” Usability Eval. Ind.,

vol. 189, 11 1995.

[42] A. Bangor, P. Kortum, and J. Miller, “Determining what individual sus
scores mean: Adding an adjective rating scale,” J. Usability Studies,
vol. 4, no. 3, p. 114–123, May 2009.

