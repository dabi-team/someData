1

No-Reference Quality Assessment for 360-degree
Images by Analysis of Multi-frequency Information
and Local-global Naturalness

Wei Zhou, Student Member, IEEE, Jiahua Xu, Qiuping Jiang, Member, IEEE,
and Zhibo Chen, Senior Member, IEEE

1
2
0
2

b
e
F
2
2

]

V
C
.
s
c
[

1
v
3
9
3
1
1
.
2
0
1
2
:
v
i
X
r
a

(OIs)

images

Abstract—360-degree/omnidirectional

have
achieved remarkable attentions due to the increasing applications
of virtual reality (VR). Compared to conventional 2D images,
OIs can provide more immersive experience to consumers,
beneﬁtting from the higher resolution and plentiful ﬁeld of views
(FoVs). Moreover, observing OIs is usually in the head mounted
display (HMD) without references. Therefore, an efﬁcient blind
quality assessment method, which is speciﬁcally designed for
360-degree images, is urgently desired. In this paper, motivated
by the characteristics of the human visual system (HVS) and the
viewing process of VR visual contents, we propose a novel and
effective no-reference omnidirectional image quality assessment
(NR OIQA) algorithm by Multi-Frequency Information and
Local-Global Naturalness (MFILGN). Speciﬁcally, inspired by
the frequency-dependent property of visual cortex, we ﬁrst
decompose the projected equirectangular projection (ERP) maps
into wavelet subbands by using discrete Haar wavelet transform
(DHWT). Then, the entropy intensities of low-frequency and
high-frequency subbands are exploited to measure the multi-
frequency information of OIs. Besides, except for considering
the global naturalness of ERP maps, owing to the browsed
FoVs, we extract the natural scene statistics (NSS) features
from each viewport image as the measure of local naturalness.
With the proposed multi-frequency information measurement
and local-global naturalness measurement, we utilize support
vector regression (SVR) as the ﬁnal image quality regressor to
train the quality evaluation model from visual quality-related
features to human ratings. To our knowledge, the proposed
model is the ﬁrst no-reference quality assessment method for
360-degreee images that combines multi-frequency information
and image naturalness. Experimental results on two publicly
available OIQA databases demonstrate that our proposed
MFILGN outperforms state-of-the-art full-reference (FR) and
NR approaches.

Index Terms—Omnidirectional

images, no-reference image
quality assessment, multi-frequency information, local-global nat-
uralness, human visual system.

I. INTRODUCTION

I MMERSIVE multimedia technologies, especially the vir-

tual reality (VR), can provide viewers with more realistic

This work was supported in part by NSFC under Grant U1908209,
61632001, 61901236 and the National Key Research and Development
Program of China 2018AAA0101400.

W. Zhou, J. Xu and Z. Chen are with the CAS Key Laboratory of Technol-
ogy in Geo-Spatial Information Processing and Application System, Univer-
sity of Science and Technology of China, Hefei 230027, China (e-mail: wei-
chou@mail.ustc.edu.cn; xujiahua@mail.ustc.edu.cn; chenzhibo@ustc.edu.cn).
Q. Jiang is with the School of Information Science and Engineering, Ningbo
University, Ningbo 315211, China (e-mail: jiangqiuping@nbu.edu.cn).

and interactive user experience [1]. As the most common form
of VR contents, 360-degree/omnidirectional images and videos
◦
record visual information that covers the entire 180 × 360
viewing spherical, thus attract a lot of attentions from both
academy and industry during recent years [2]. With the
commercial head mount display (HMD), users are allowed
to freely view any direction with the speciﬁc content by head
movement, which is different from conventional 2D images
and videos. Moreover, due to the omnidirectional viewing
range, the resolution of 360-degree image/video is usually
ultra-high, e.g. 4K, 8K, or even higher. These would bring
many difﬁculties to the whole 360-degree image/video pro-
cessing chain, such as acquisition, compression, transmission,
reconstruction and display, etc [3]. Additionally, the perceptual
quality of omnidirectional images could be degraded in the
360-degree image/video processing systems. Therefore, the
study on 360-degree/omnidirectional image quality assessment
(OIQA) is more challenging and signiﬁcant
to guide the
development of VR applications.

Recently, there have witnessed ever-increasing interests in
the research ﬁeld of image quality assessment (IQA). Two
types of IQA methods are involved, which consist of subjective
IQA [4]–[8] and objective IQA [9]–[13]. In subjective IQA
tests, subjects are asked to give the human ratings for each
viewed image. After data processing and outlier elimination,
the mean opinion score (MOS) can be obtained by computing
the average quality scores of all subjects for each image,
which can be regarded as the quantitative ground truth of
the perceptual quality [14]. Since human is the ultimate
viewer, subjective IQA is the most reliable quality assess-
ment approach. By conducting such subjective experiments,
several subjective OIQA databases have been established. For
example, a testbed for the subjective measurement of 360-
degree/omnidirectional contents was proposed [15], where 6
reference and 54 distorted omnidirectional images (OIs) were
included. The JPEG compression with various quality parame-
ters and two projection models are considered in this database.
Moreover, in [16], 4 high ﬁdelity uncompressed OIs were
used to generate 100 impaired images with different geometric
projections and three codecs, i.e. JPEG, JPEG2000, and HEVC
intra. Four target bitrates were chosen to compress original
OIs. The compressed VR image quality database (CVIQD)
was built in [17], which includes 5 reference and 165 com-
pressed OIs by JPEG, AVC, and HEVC codecs. Furthermore,
this database was expanded as the CVIQD2018 [18] with more

 
 
 
 
 
 
2

neuron corresponds to speciﬁc spatial and temporal frequency
signals. Therefore,
the input visual signal can be decom-
posed into multiple frequency domains, which is more in
line with the human visual perception. Among various multi-
frequency channel decomposition methods, wavelet decompo-
sition shows the superiority of processing visual signals [24].
Meanwhile, the wavelet decomposition also has been demon-
strated to have good performance in IQA [25], [26]. Moti-
vated by this mechanism, we decompose the projected ERP
maps into wavelet subbands through discrete Haar wavelet
transform (DHWT). Since the decomposed low-frequency and
high-frequency subbands represent luminance information and
textural details, respectively. We then compute the entropy
intensities of low-frequency and high-frequency subbands,
which are used to measure the multi-frequency information
of OIs. Second, due to different viewports during browsing,
we propose the local-global naturalness measurement, where
the natural scene statistics (NSS) features are extracted from
both local viewed FoVs and global ERP maps. To the best of
our knowledge, the naturalness has been proved in a number
of IQA research [27], [28] but it has not been used in OIQA.
Finally, the quality predictions of OIs are obtained by the well-
known support vector regression (SVR). As demonstrated by
extensive experiments, our proposed MFILGN performs better
than state-of-the-art FR and NR algorithms on two publicly
available OIQA databases.

The main contributions of this work are summarized as

follows:

• We propose the ﬁrst blind OIQA algorithm based on the
measurements of multi-frequency information and local-
global naturalness.

• According to the frequency-dependent characteristic of
human visual cortex, we derive the decomposed low
and high frequency subbands and utilize the entropy
intensities of these subband images to reﬂect the multi-
frequency information in omnidirectional images.

• Considering the viewing process of omnidirectional im-
ages, apart from the global naturalness from the projected
ERP maps, we extract the local naturalness features from
various viewports together with global NSS to composite
the local-global naturalness measurement.

The remaining sections of this paper are organized as
follows. In Section II, we present the related works of objective
quality assessment for both traditional 2D images and OIs.
Section III introduces the proposed MFILGN model for NR
OIQA in details. In Section IV, experimental results and
analysis are presented. Section V concludes the whole paper
with possible research directions in the future.

II. RELATED WORKS

Whether for traditional objective IQA or objective OIQA,
when the originally pristine/reference image is available, full-
reference (FR) objective visual quality assessment models are
developed. As for conventional FR IQA, the signal ﬁdelity
metrics such as mean square error (MSE) and peak signal-to-
noise ratio (PSNR), measure the visual quality by computing
the pixel differences between original and distorted images.

Fig. 1: Illustration of the projected ERP map and various
viewports of 360-degree image.

visual contents, leading to 16 pristine and 528 compressed
images. In [19], different resolutions and JPEG compression
were considered, where the database has 12 original and 144
distorted OIs. In addition, an omnidirectional image quality
assessment (OIQA) database [20] was developed, consisting
of 16 reference and 320 distorted images. Apart from JPEG
and JPEG2000 compression artifacts, different levels of white
Gaussian noise and Gaussian blur were taken into account in
this database. Since observers usually exploit the HMD to view
OIs, the absolute category rating with hidden reference (ACR-
HR) methods, which are also referred to as the single-stimulus
(SS) methods, are adopted to build all these subjective quality
assessment databases for OIs.

However, subjective tests are generally time-consuming and
labor-intensive. Thus, objective visual quality assessment al-
gorithms used to automatically measure the perceptual quality
of OIs are required. As shown in Fig. 1, (a) represents the
projected equirectangular projection (ERP) map of a viewed
VR scene. The projected ERP map usually has a higher
resolution, such as 4K or 8K. Hence, it is suitable for multi-
resolution decomposition, which is also an image decompo-
sition in the frequency channels of constant bandwidth on a
logarithmic scale. Moreover, the concept of multi-resolution
can interpret multi-frequency channel decompositions [21]. In
this ﬁgure, except for the projected ERP map, (b) shows 6
viewports from a variety of viewing directions, each of which
is a part of 360-degree image falling into the ﬁeld of view
(FoV) in the HMD. The naturalness characteristics reﬂected by
statistical regularizations from the global ERP map and local
viewports are different. In addition, a no-reference quality
assessment model for 3D/stereoscopic omnidirectional images
has been proposed in [22]. The monocular multi-scale features
are extracted from left and right views separately, while
the binocular perception features are exploited from tensor
decomposition and the absolute difference map as well as
product image of left and right views. The naturalness features
involved in the product image are also validated. Nevertheless,
it is different from 2D omnidirectional images considered in
this paper, where we only have one single view image with
multiple viewports.

Based on these observations, in this paper, we propose a
novel no-reference (NR) OIQA method by Multi-Frequency
Information and Local-Global Naturalness (MFILGN). First,
according to a series of studies in neuroscience on the human
visual system (HVS), the neuronal responses in the visual
cortex are frequency-dependent [23]. In other words, each

(a)(b)3

Fig. 2: The overall framework of our proposed MFILGN. It consists of the multi-frequency information measurement and
local-global naturalness measurement.

Due to the simple calculation and optimization processes,
they are widely used in image processing. Nevertheless, their
performance is relatively unsatisfactory and cannot predict the
human perceived visual quality precisely. Therefore, the char-
acteristics of the HVS are employed to construct perception-
based IQA models. For instance,
the structural similarity
(SSIM) index [29] and its different variants, including the
multiscale SSIM (MS-SSIM) index [30], the feature similarity
(FSIM) index [31], etc.

When coming to practical scenarios, perfect-quality original
images are usually difﬁcult to obtain, and thus no-reference
objective visual quality assessment approaches are urgently
needed. For traditional NR IQA, many methods extract hand-
crafted distortion-discriminative features for predicting the
perceptual image quality, such as the blind/referenceless image
spatial quality evaluator (BRISQUE) [32], the blind multiple
pseudo reference images-based (BMPRI) measure [33], and so
on. In recent decades, due to the powerful feature representa-
tion learning capacity, deep learning has shown unprecedented
success in many image processing and computer vision tasks
[34], [35]. This also presents an opportunity for evaluating
the image visual quality. Typical methods contain the deep
image quality assessment (DeepQA) [36] and the deep bilinear
convolutional neural network (DBCNN) [37] for FR and NR
IQA, respectively. Furthermore, several perception-based pre-
processing strategies have been presented in existing works.
For example, before training CNN models, saliency maps can
be used to assign importance to distorted image patches [38].
The distorted image stream and gradient image stream were
both considered in CNN to predict perceptual image quality
scores [39].

Although there exist

lots of classical objective quality
assessment algorithms for FR and NR IQA, they are designed
for regular ﬂat 2D images and unsuitable for assessing the
perceptual quality of OIs. Yet, limited research works related
to objective OIQA methods have been proposed in the litera-
ture. In general, existing objective OIQA models can be clas-
siﬁed into two categories. The ﬁrst is to extend conventional
FR IQA approaches to FR OIQA [40]–[45]. For example,
several PSNR-based FR OIQA models were presented. Yu
et al. [40] proposed the spherical PSNR (S-PSNR), which
chose speciﬁc points on the spherical surface rather than
the projected panoramic image. Sun et al. [41] developed
the weighted-to-spherically-uniform PSNR (WS-PSNR) by
combining the error map with the weighted map which was
determined by stretched regions. Zakharchenko et al. [42]
presented the craster parabolic projection PSNR (CPP-PSNR),
which computed the PSNR on the craster parabolic projection
domain. Xu et al. [43] put forward the non-content-based
PSNR (NCP-PSNR) and content-based PSNR (CP-PSNR) via
weighting pixels with position information and predicting
viewing direction, respectively. Likewise, the performance of
PSNR-based FR OIQA methods is insufﬁcient because they
do not consider the HVS characteristics. Afterwards, objective
FR OIQA methods based on SSIM have been proposed in
succession. Similar to WS-PSNR, Zhou et al. [44] designed
the weighted-to-spherically-uniform SSIM (WS-SSIM), where
the position weighted map was used to multiply the SSIM. In
order to reduce the inﬂuence of geometric distortions for the
projection, Chen et al. [45] proposed the spherical SSIM (S-
SSIM), which calculated the similarity of each image pixel
on the sphere. The second is the deep learning-based NR IQA

Distorted 360-degree imageDiscrete HaarWavelet TransformLow-frequency SubbandHigh-frequency SubbandEntropy Intensity Multi-Frequency Information MeasurementViewport ExtractionLocal-Global Naturalness MeasurementQuality scoreLocal NaturalnessGlobalNaturalnessSVR……methods [46]–[50]. Considering the spherical representation of
360-degree content, Kim et al. [46] proposed a deep learning
framework for VR image quality assessment (DeepVR-IQA)
based on adversarial learning, in which the quality scores of
sampled patches were predicted and then they were weighted
according to the patch positions on the sphere. Moreover, in
[47], the head movement (HM) and eye movement (EM) were
exploited to weight the quality scores in deep learning models.
But the image patches sampled from the projected plane reveal
non-negligible geometric deformation, which cannot reﬂect
the actual viewing contents. Thus, viewports were utilized to
predict the perceptual quality of OIs. Speciﬁcally, Li et al. [48]
proposed a viewport-based CNN (V-CNN), which predicted
the quality scores of viewports instead of image patches sam-
pled from projected planes. Sun et al. [49] designed a multi-
channel CNN for blind 360-degree image quality assessment
(MC360IQA), including 6 parallel hyper-ResNet34 networks
to process viewport images and an image quality regression
module to aggregate learned features for obtaining the ﬁnal
image quality. Xu et al. [50] proposed the viewport oriented
graph convolution network (VGCN) to tackle the perceptual
quality assessment for OIs, which was guided by different
viewports.

From the above reviewed objective quality assessment mod-
els for OIs, it can be concluded that existing objective OIQA
methods have achieved success to a certain extent. However,
we also notice that
ignore the important multi-
frequency information as well as the statistical regularizations
of OIs from both global projection maps and local viewports.
To ﬁll the blank, in this work, we try to present a no-reference
OIQA method by considering multi-frequency information and
local-global naturalness simultaneously.

they all

III. THE PROPOSED QUALITY ASSESSMENT METHOD

In this section, we will introduce the proposed NR OIQA
method that can blindly predict the perceptual quality of OIs
in technical details. The overall framework of our proposed
MFILGN is shown in Fig. 2, which consists of two sepa-
rate measurements, namely the multi-frequency information
measurement and local-global naturalness measurement. For
the multi-frequency information measurement,
inspired by
the HVS characteristics, the Haar wavelet transform is ﬁrst
applied to decompose the projected ERP maps into multiple
the entropy intensities are calculated for
subbands. Then,
measuring the multi-frequency information. And for the local-
global naturalness measurement, considering the changeable
FoVs during the viewing process, both local naturalness from
different viewports and global naturalness from ERP maps are
extracted. The ﬁnal quality index is obtained by the regression
of these distortion-related features.

A. Image Decomposition

According to the HVS studies, different neurons respond
to different frequencies of visual signals [23]. The input
visual stimulus should be decomposed into various subband
images for the subsequent processing. Moreover, wavelet
transform is one of the multi-frequency channel decomposition

4

Fig. 3: Multi-frequency channel decomposition by DHWT. (a)
An example of distorted 360-degree image; (b) the decom-
posed four subband images of (a).

methods. Here, we choose the wavelet transform to conduct
the distorted image decomposition. Generally, there exist a
number of wavelets, including Haar wavelet, Morse wavelet,
Gabor wavelet, Bump wavelet and so on. In all of these
wavelets, Haar wavelet is symmetric and a special case of
Daubechies wavelet, which shows great success in perceptual
quality assessment [51]–[53]. Therefore, we adopt the DHWT
to decompose the distorted OIs into multi-frequency subbands.
Speciﬁcally, the Haar wavelet can be formulated as:

1
√
2

ψ(t) =

=

1
√
2
+∞
(cid:88)

(φ(t − 1) − φ(t))

(−1)1−uh[1 − u]φ(t − u),

(1)

u=−∞

where the mother wavelet ψ is deﬁned by:

ψ(t) =

1
2

),

1, f or t ∈ [0,



−1, f or t ∈ [


1
2

0, otherwise.

, 1),

(2)

(a)(b)5

Fig. 4: Entropy intensity changes for different distorted 360-images. (a) Low JPEG distortion denoted by JPEG-Level1; (b)
Middle JPEG distortion denoted by JPEG-Level2; (c) High JPEG distortion denoted by JPEG-Level3; (d) The corresponding
entropy intensity changes of low and high frequency subbands for (a), (b) and (c).

Moreover, the father wavelet or scaling function φ and its
corresponding ﬁlter h are computed as:

(cid:40)

φ(t) =

1, f or t ∈ [0, 1),
0, otherwise.

h[u] =

1
√
2






, f or u = 0, 1,

0, otherwise.

(3)

(4)

With the Haar wavelet, we can obtain the Haar wavelet
transform by cross multiplying different shifts and stretches.
Here, let H denote the DHWT matrix. Suppose that a distorted
360-degree image D with resolution I ×J. Then the input 360-
degree image can be decomposed into four I
2 subband
images by the DHWT as follows:

2 × J

HDH T =

(cid:20)DLL DHL
DLH DHH

(cid:21)

,

(5)

where DLL, DHL, DLH , and DHH represent the decomposed
four subbands with low or high frequency in the horizontal or
vertical direction. H T is the transpose matrix of H.

B. Multi-frequency Information

Fig. 3 illustrates the multi-frequency channel decomposition
by DHWT. We show a distorted 360-degree image in (a), and
the four images in (b) are the decomposed results of (a). From
this ﬁgure, we can see that the decomposed subband images
are different from each other, which could reﬂect various
frequency characteristics of the distorted 360-degree image.

Since the entropy intensities could reﬂect average amount
of image information, images with different distortions bring
about various entropy intensities. Especially, after the multi-
frequency channel decompositions of distorted 360-degree
images, the entropy intensities of decomposed low-frequency
and high-frequency subbands differ from each other. We
then compute the entropy intensities of the decomposed four
subband images as:

Es = −

K
(cid:88)

k=0

klog2ps
ps
k,

(6)

where s ∈ {LL, HL, LH, HH} denotes the collection of
frequency decomposition components. ps
k and K indicate the
probability of the pixel equaling to k and the maximum pixel
value in the corresponding decomposed subband image. The
probability can be calculated by:

ps
k =

N s
k
N

,

(7)

where N s
k is the numbers of pixel value equaling to k in the
corresponding decomposed subband image. N represents the
total number of pixels.

After computing the entropy intensities of the decomposed
four subband images, we use their joint component features
to measure multi-frequency information as:

FM F I = [ELL, EHL, ELH , EHH ],

(8)

(a)(b)(c)(d)6

Fig. 5: Effects of the ZCA whitening ﬁlter and MSCN operation on the statistical distribution of distorted ERP map.

where FM F I denotes the multi-frequency information mea-
surement, which reﬂects the discriminative entropy informa-
tion from both low and high frequency subbands.

In Fig. 4, we show three 360-degree images with differ-
ent JPEG distortion levels and their corresponding entropy
intensities of low-frequency and high-frequency subbands. The
image samples are from OIQA database [20]. We can ﬁnd
that the entropy intensities for four frequency subbands change
signiﬁcantly with regard to various distortions. That is, more
JPEG distortions lead to a decrease in entropy intensities
of low and high frequency subbands, which demonstrates
the effectiveness of the proposed multi-frequency information
measurement.

C. Global Naturalness

In addition to the multi-frequency information in the pro-
jected ERP maps, the image naturalness which is reﬂected by
statistical regularizations is crucial to the perceptual quality
of OIs. Although the NSS features have been applied to
traditional multimedia formats, such as 2D [32], 3D [9], etc.
To the best of our knowledge, it has not been used to evaluate
the perceptual quality of VR images yet. Therefore, here we
try to explore the naturalness in OIQA. Intuitively, we can
extract the NSS features from global ERP maps. Given the
distorted 360-degree image D with resolution I × J, to reduce
the spatial redundancy of adjacent image pixels, we ﬁrst adopt
the zero-phase component analysis (ZCA) whitening ﬁlter as
follows:

Dz = Z(D),

(9)

where Z indicates the ZCA whitening ﬁlter. Dz is the distorted
ERP map after ZCA ﬁltering.

Afterwards, the local mean subtracted and contrast normal-
ized (MSCN) coefﬁcients are computed to measure the image
naturalness, which can model the contrast gain masking in
early human visual cortex [54]. For each distorted ERP map
after ZCA ﬁltering, the MSCN coefﬁcients are calculated by:

ˆDz(i, j) =

Dz(i, j) − µ(i, j)
σ(i, j) + C

,

(10)

where ˆDz(i, j) and Dz(i, j) are the distorted ERP map after
local normalization (i.e. the MSCN coefﬁcients) and distorted
ERP map at spatial position (i, j), respectively. µ(i, j) and
σ(i, j) represent the local mean and standard deviation of the
distorted ERP map, which are computed as:

µ(i, j) =

S
(cid:88)

T
(cid:88)

s=−S

t=−T

ws,tDz(i, j),

(11)

σ(i, j) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

S
(cid:88)

T
(cid:88)

s=−S

t=−T

ws,t(Dz(i, j) − µ(i, j))2,

(12)

where w = {ws,t|s = −S, ..., S, t = −T, ...T } stands for the
2D circularly-symmetric Gaussian weighted function.

In Fig. 5, we show the statistical distributions of input
distorted ERP map and the distorted ERP map after ZCA
ﬁltering as well as local normalization process. From ﬁgures
(a-c), we can observe that the ZCA ﬁltering and the MSCN
operation both make the probability distribution of the dis-
torted ERP map more Gaussian-like. Moreover, the statistical
distribution after local normalization is the closest to Gaussian
distribution. Thus, we exploit the MSCN coefﬁcients for the
subsequent feature processing. Additionally, Fig. 6 presents
the statistical distributions of MSCN coefﬁcients for different
distortion types and levels. As can be seen in this ﬁgure, the
probability distributions of the distorted ERP map after local
normalization is inﬂuenced by different distortion types as well
as the distortion levels, which demonstrates that the statistical
distributions of MSCN coefﬁcients are discriminative for the
perceptual quality assessment of 360-degree images.

With the probability distributions of the distorted ERP map
after local normalization, we utilize the zero-mean general-
ized Gaussian distribution (GGD) and asymmetric generalized
Gaussian distribution (AGGD) models to quantify the MSCN
coefﬁcients distribution. The zero-mean AGGD used to ﬁt the
distribution is:

f (x; τ, σ2

l , σ2

r ) =






τ
(υl + υr)Γ( 1
τ )
τ
(υl + υr)Γ( 1
τ )

e−( −x

υl

)τ

, x < 0,

e−( −x

υr

)τ

, x ≥ 0,

(13)

(a)(b)(c)7

Fig. 6: Statistical distributions of MSCN coefﬁcients for different distortion types and levels from OIQA database [20]. (a)
MSCN coefﬁcients vary with various distortion types; (b) MSCN coefﬁcients under four JPEG2000 compression levels.

where

D. Local Naturalness

υl = σl

(cid:115)

Γ( 1
τ )
Γ( 3
τ )

,

υr = σr

(cid:115)

Γ( 1
τ )
Γ( 3
τ )

,

(14)

(15)

and τ denotes the shape parameter which can control the shape
of the statistical distribution. σl and σr are the scales of the
left and right sides for the statistical distribution, respectively.
Γ(·) is the gamma function that is deﬁned as:

+∞
(cid:90)

xa−1e−xdx, a > 0.

Γ(a) =

(16)

0

The best AGGD ﬁtting with parameters (η, τ, σ2
calculated and η is given by:

l , σ2

r ) are then

η = (υl − υr)

Γ( 2
τ )
Γ( 1
τ )

.

(17)

Furthermore, if σl = σr, the AGGD becomes GGD model as
follows:

f (x; τ, σ2) =

τ
2υΓ( 1
τ )

τ
e−( |x|
υ )

,

where

υ = σ

(cid:115)

Γ( 1
τ )
Γ( 3
τ )

.

(18)

(19)

In addition,

the distorted 360-degree images are down-
sampled by a factor of 2. Finally, the two scales, including
the original image scale and a reduced resolution scale, are
exploited to extract the global NSS features FGN SS from the
projected ERP maps.

When observers browse VR images in the HMD, they can
freely change the viewports. Moreover, different FoVs have
various contents and the global 360-degree scenery could
be reconstructed based on what human see from multiple
viewports. Therefore, apart from the global naturalness from
the projected ERP maps, it is also important to explore the
local naturalness according to a variety of FoVs. As illustrated
in Fig. 7 (a), we show an example of global ERP map and local
images from different spatial positions (i.e. the four FoVs).
(b) is the corresponding statistical distributions of MSCN
coefﬁcients. It can be seen that the probability distributions
of global ERP map and four FoVs are different from each
other. Besides, the distributions of FoV-2 and FoV-3 nearing
the equator is close. Thus, we adopt the local NSS from these
FoVs located at various spatial positions as the complementary
features to global NSS from the projected ERP maps.

Specially, suppose that

there exist M FoV images, we
extract the NSS features from them by the same steps as global
naturalness. We then can obtain (F 1
LN SS)
representing all these viewports. Finally, the average of the
NSS features are computed by:

LN SS, ..., F M

LN SS, F 2

FLN SS =

1
M

M
(cid:88)

m=1

F m

LN SS,

(20)

where FLN SS indicates the local NSS features from viewed
FoVs. With the local and global NSS features, we combine
them to constitute the local-global naturalness measurement
as follows:

FLGN = [FLN SS, FGN SS].

(21)

In order to achieve the local-global naturalness measure-
ment, we need to sample M viewports from each distorted
360-degree image. Motivated by that VR images are viewed
on a sphere and polar regions usually stretch resulting in

(a)(b)8

Fig. 7: An example of global and local images from different spatial positions with their corresponding statistical distributions
of MSCN coefﬁcients.

geometric deformation for projected ERP maps. Thus, we
employ the non-uniform sampling strategy [55], [56]. As
shown in Fig. 8, for the equator, we ﬁrst sample M0 viewports
equidistantly. Then, the remaining viewpoints are selected by:

θ =

360◦
M0

,

M1 = (cid:98)M0 cos θ(cid:99) ,

M2 = (cid:98)M0 cos 2θ(cid:99) ,

(cid:22)

Mend =

M0 cos

(cid:23)

,

90◦
θ

(22)

(23)

(24)

(25)

where M1 and M2 are the numbers of viewports sampled on
θ and 2θ degrees north or south latitude, respectively. The
sampling process ends when the maximum latitude reaches
90◦ with Mend sampled viewports. After the computation of
viewports sampling, we can totally obtain M viewports for
each distorted OI as:

M = M0 +

end
(cid:88)

m=1

2 ∗ Mm.

(26)

E. Quality Regression

With the multi-frequency information measurement and
local-global naturalness measurement,
the ultimate quality
score of 360-degree image is obtained by the well-known SVR
[57]. Speciﬁcally, we randomly divide all distorted OIs into a
training set and a testing set, which are denoted by χtrain
and χtest, respectively. By adopting the SVR, our proposed
MFILGN model
is achieved from training the distortion-
related features of OIs in the training set χtrain and their
corresponding MOS values. Given a distorted 360-degree im-
age Dtrain ∈ χtrain and its extracted features [FM F I , FLGN ]
from the multi-frequency information measurement and local-
global naturalness measurement, the MFILGN model is de-
ﬁned by:

M F ILGN = SV R T RAIN ([FM F I , FLGN ], [Q]),

(27)

where Q is the subjective quality rating (i.e. MOS) of the input
360-degree image.

After training the proposed MFILGN model, we verify the
model performance on the testing set χtest. For example, the
predicted quality score of a tested 360-degree image Dtest ∈
χtest that does not appear in the training set is computed as
follows:

q = SV R P REDICT ([ (cid:98)FM F I , (cid:98)FLGN ], M F ILGN ),

(28)

where [ (cid:98)FM F I , (cid:98)FLGN ] represent the extracted features from
the multi-frequency information measurement and local-global
naturalness measurement for the tested 360-degree image.
Finally, the correlation or error between the predicted scores
and the corresponding ground truth MOS values for testing
set is measured as the performance of MFILGN.

IV. EXPERIMENTAL RESULTS AND ANALYSIS

In this section, we ﬁrst introduce the experimental protocol
including OIQA databases and measure criteria: used in our
experiments. Then, we evaluate the proposed MFILGN for
overall performance and performance for individual distortion
type on the OIQA [20] and CVIQD [17], [18] databases. After
that, various weighting methods of viewports as well as differ-
ent parameters containing the adopted viewport numbers and
training percentages are analyzed. Finally, the ablation study
is conducted to prove the effectiveness of each component in
our MFILGN model.

A. Experimental Protocol

1) Databases: Two benchmark OIQA databases are utilized
in the experiments, which consist of the OIQA [20] and
CVIQD [17], [18] databases.

• OIQA comprises 16 pristine images and 320 distorted
OIs degraded by 4 distortion types and 5 distortion levels.
Among the distortion types, two kinds of compression

(a)(b)FoV-1FoV-2FoV-3FoV-4ERP MapGlobal and Local Images9

Fig. 8: Demonstration of sampling viewports when M0 = 8, θ = 45◦ for the sphere and plane, respectively.

TABLE I: OVERALL PERFORMANCE COMPARISONS ON OIQA AND CVIQD DATABASES. THE BEST RESULTS ARE
DENOTED IN BOLD.

Type

FR

NR

Database

Methods

PSNR
S-PSNR [40]
WS-PSNR [41]
CPP-PSNR [42]
SSIM [29]
MS-SSIM [30]
FSIM [31]
DeepQA [36]

BRISQUE [32]
BMPRI [33]
DB-CNN [37]
MC360IQA [49]
VGCN [50]
Proposed MFILGN

OIQA

CVIQD

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

0.5226
0.5399
0.5263
0.5149
0.8588
0.7379
0.8938
0.8973

0.8331
0.6238
0.8653
0.9139
0.9515
0.9614

0.5812
0.5997
0.5819
0.5683
0.8718
0.7710
0.9014
0.9044

0.8424
0.6503
0.8852
0.9267
0.9584
0.9695

1.7005
1.6721
1.6994
1.7193
1.0238
1.3308
0.9047
0.8914

1.1261
1.5874
0.9717
0.7854
0.5967
0.5146

0.6239
0.6449
0.6107
0.6265
0.8842
0.8222
0.9152
0.9292

0.8180
0.7470
0.9308
0.9428
0.9639
0.9670

0.7008
0.7083
0.6729
0.6871
0.9002
0.8521
0.9340
0.9375

0.8376
0.7919
0.9356
0.9429
0.9651
0.9751

9.9599
9.8564
10.3283
10.1448
6.0793
7.3072
4.9864
4.8574

7.6271
8.5258
4.9311
4.6506
3.6573
3.1036

artifacts are involved, namely JPEG and JPEG2000 com-
pression. The remaining distortion types are Gaussian
blur and Gaussian noise. The subjective quality ratings
in the form of MOS are provided in the range [1, 10].
The higher MOS means better perceptual image quality.
• CVIQD contains 528 compressed images derived
from 16 original images. It adopts three popular im-
age/video coding technologies, i.e. JPEG, H.264/AVC,
and H.265/HEVC. Moreover, the MOS values in this
database are normalized and rescaled to the range [0,
100].

2) Measure Criteria: To validate the effectiveness of our
proposed MFILGN and compare with other state-of-the-arts,
three commonly-used measure criteria [58] are employed as
the following descriptions.

• Spearman

rank-order

correlation

coefﬁcient

(SROCC) is computed by:

SROCC = 1 −

2

6

di

N
(cid:80)
i=1
N (N 2 − 1)

,

(29)

where N is the number of image samples. di indicates
the rank difference between the subjective and objective
evaluations for the i − th image.

• Pearson linear correlation coefﬁcient (PLCC) is cal-

culated as:

P LCC =

(cid:115)

N
(cid:80)
i=1

(si − µsi)(oi − µoi)

N
(cid:80)
i=1

(si − µsi) ∗

N
(cid:80)
i=1

(oi − µoi)

,

(30)

10

TABLE II: PERFORMANCE COMPARISONS FOR INDIVIDUAL DISTORTION TYPE ON CVIQD DATABASE. THE BEST
RESULTS ARE DENOTED IN BOLD.

Type

FR

NR

Database

Methods

PSNR
S-PSNR [40]
WS-PSNR [41]
CPP-PSNR [42]
SSIM [29]
MS-SSIM [30]
FSIM [31]
DeepQA [36]

BRISQUE [32]
BMPRI [33]
DB-CNN [37]
MC360IQA [49]
VGCN [50]
Proposed MFILGN

JPEG

AVC

HEVC

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

0.6982
0.7172
0.6848
0.7059
0.9582
0.9047
0.9639
0.9001

0.9031
0.9562
0.9576
0.9693
0.9759
0.9591

0.8682
0.8661
0.8572
0.8585
0.9822
0.9636
0.9839
0.9526

0.9464
0.9874
0.9779
0.9698
0.9894
0.9862

8.0429
8.1008
8.3465
8.3109
3.0468
4.3355
2.8928
4.9290

5.2442
2.5597
3.3862
3.9517
2.3590
2.7904

0.5802
0.6039
0.5521
0.5872
0.9174
0.7650
0.9439
0.9375

0.7714
0.6731
0.9545
0.9569
0.9659
0.9683

0.6141
0.6307
0.5702
0.6137
0.9303
0.7960
0.9534
0.9477

0.7745
0.7161
0.9564
0.9487
0.9719
0.9785

10.552
10.3760
10.9841
10.5615
4.9029
8.0924
4.0327
4.2683

8.4573
9.3318
3.9063
4.2281
3.1490
2.4998

0.5762
0.6150
0.5642
0.5689
0.9452
0.8011
0.9532
0.9288

0.7644
0.6715
0.8693
0.9104
0.9432
0.9485

0.5982
0.6514
0.5884
0.6160
0.9436
0.8072
0.9617
0.9221

0.7548
0.6154
0.8646
0.8976
0.9401
0.9581

9.4697
8.9585
9.5473
9.3009
3.9097
6.9693
3.2385
4.5694

7.7455
9.3071
5.9335
5.2557
4.0257
3.3950

TABLE III: PERFORMANCE RESULTS FOR VARIOUS WEIGHTING STRATEGIES ON OIQA AND CVIQD DATABASES.

Database

Methods

OIQA

CVIQD

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

Average Weighting
Location Weighting
Content Weighting

0.9614
0.9607
0.9598

0.9695
0.9688
0.9681

0.5146
0.5213
0.5252

0.9670
0.9665
0.9667

0.9751
0.9748
0.9749

3.1036
3.1212
3.1073

TABLE IV: PERFORMANCE RESULTS FOR DIFFERENT
VIEWPORT NUMBERS ON OIQA AND CVIQD DATABASES.

Database

OIQA

CVIQD

Number

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

6
20
80

0.9608
0.9614
0.9616

0.9691
0.9695
0.9696

0.5155
0.5146
0.5134

0.9665
0.9670
0.9678

0.9746
0.9751
0.9758

3.1160
3.1036
3.0530

where si and oi denote the i − th subjective and mapped
objective quality values. µsi and µoi represent the corre-
sponding mean values of si and oi, respectively.
• Root mean squared error (RMSE) is deﬁned by:

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

N
(cid:80)
i=1

(si − oi)2

N

RM SE =

.

(31)

In addition, each OIQA database is randomly divided into
80% for training and the remaining 20% for testing. We
perform 1,000 iterations of cross validation on each database.
The median SROCC, PLCC and RMSE performance values
are then taken as the ﬁnal measurement. Before calculating
PLCC and RMSE for different objective quality assessment
approaches, a ﬁve-parameter logistic nonlinear ﬁtting function
is used to map the predicted quality scores into a common
scale as follows:

g(x) = β1(

1
2

−

1
1 + eβ2(x−β3)

) + β4x + β5,

(32)

where {βi|i = 1, 2, ..., 5} are ﬁve parameters to be ﬁtted. x and
g(x) denote the raw objective quality score and the regressed
quality score after the nonlinear mapping process.

In addition, the above-mentioned three measure criteria can
reﬂect different aspects of the performance for various IQA
algorithms. Speciﬁcally, SROCC is generally used to measure
prediction monotonicity, while PLCC and RMSE indicate
prediction accuracy. Note that higher correlation coefﬁcients
and lower error mean better performance.

B. Performance Comparison with Existing Objective Models

In order to demonstrate the effectiveness of our proposed
MFILGN model, we conduct extensive experiments compared
to existing FR and NR objective image quality assessment
algorithms. The FR models include conventional FR IQA as
well as OIQA approaches (i.e. PSNR, SSIM [29], MS-SSIM
[30], FSIM [31], S-PSNR [40], WS-PSNR [41] and CPP-
PSNR [42]) and a deep learning-based FR IQA method (i.e.
DeepQA [36]). Moreover, the NR models consist of traditional
NR IQA approaches (i.e. BRISQUE [32] and BMPRI [33])
and three deep learning-based NR IQA as well as OIQA
methods (i.e. DB-CNN [37], MC360IQA [49] and VGCN
[50]). Among these existing state-of-the-art objective FR
image quality assessment models, the PSNR-related metrics

TABLE V: PERFORMANCE RESULTS FOR DIFFERENT
IMAGE DECOMPOSITION TIMES BY DHWT ON OIQA AND
CVIQD DATABASES.

Database

OIQA

CVIQD

Time

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

1
2
3

0.9614
0.9632
0.9640

0.9695
0.9716
0.9715

0.5146
0.4962
0.4986

0.9670
0.9671
0.9675

0.9751
0.9751
0.9754

3.1036
3.0992
3.0945

including PSNR, S-PSNR, WS-PSNR and CPP-PSNR are the
signal ﬁdelity measurement, which compute the pixel differ-
ences between the reference and distorted images. Considering
the characteristics of the HVS, the SSIM and its variants
(i.e. MS-SSIM and FSIM) extract structural information from
original and distorted images for perceptual image quality
the DeepQA takes the human visual
assessment. Besides,
sensitivity into account in the deep learning framework. For
NR methods, the BRISQUE is based on NSS features in the
spatial domain and designed for conventional 2D IQA, and
the BMPRI generates multiple pseudo reference images and
exploits local binary pattern features for quality estimation.
Additionally, the DB-CNN puts forward the bilinear pooling
for predicting the perceptual image quality in the architecture
of CNN. It is worth noting that the MC360IQA and VGCN
are two deep learning-based methods speciﬁcally designed for
360-degree images. The MC360IQA utilizes 6 parallel sub-
networks for viewport images, while the VGCN builds the
graph convolution network based on different viewports.

Table I shows the overall performance comparisons on
OIQA [20] and CVIQD [17], [18] databases. The best exper-
imental results are highlighted in bold. The compared perfor-
mance values are from [50]. For fair comparison, the perfor-
mance values of traditional image quality assessment models
are tested on the used testing data. Moreover, the learning-
based models are trained on each speciﬁc 360-degree image
quality database (i.e. OIQA database or CVIQD database) with
randomly selected 80% training data, and then tested on the
remaining 20% testing data. We can see that the PSNR-based
metrics are inferior to other objective models considering the
HVS properties. This is a common phenomenon because only
signal errors are involved in the framework of PSNR-related
models, which is far from the human perception. The 2D
IQA models show unsatisfactory performance because they do
not consider the speciﬁc characteristics of 360-degree images,
such as the multiple viewports which are important for quality
perception when browsing VR visual contents. Moreover, the
deep learning-based models have advantages over traditional
objective image quality assessment approaches for both FR
and NR categories, especially the MC360IQA and VGCN
which are two deep learning-based methods speciﬁcally de-
veloped for VR images. In addition, our proposed MFILGN
achieves the best performance among the existing state-of-the-
arts, including the deep learning-based OIQA and NSS-based
IQA algorithms.

11

Fig. 9: The change of PLCC performance for our proposed
MFILGN regarding to different training percentages on OIQA
and CVIQD databases.

C. Performance Validity of Individual Distortion Type

Since a variety of distortion types exist in current OIQA
databases, we validate the performance regarding to each
distortion type. As shown in TABLE II,
the performance
comparisons for individual distortion type are illustrated and
the best results are denoted in bold. We can observe from
this table that our proposed MFILGN outperforms other state-
of-the-art models in terms of AVC and HEVC artifacts. For
JPEG compression distortion, we can see that VGCN delivers
the best performance. One possible explanation may be that
the selected viewports of VGCN adopt graph modeling, which
can better capture block effects caused by JPEG compression.
In addition, even compared to deep learning-based methods
speciﬁcally designed for 360-degree images (i.e. MC360IQA
and VGCN), the proposed MFILGN can still achieve promis-
ing performance in the case of JPEG artifacts.

D. Performance of Various Weighting Methods

In the proposed MFILGN framework, we ﬁrst extract local
NSS features from multiple viewports. After the feature extrac-
tion, we then compute the average of these local NSS features
to obtain the ﬁnal local naturalness representations for each
360-degree image. Besides, except for the average weighting,
there exist some other feature aggregation strategies, such as
the location weighting and content weighting [56]. To be more
speciﬁc, the location weighting strategy considers the statistics
of eye-tracking data and uses the viewing probability to serve
as the location weights for different viewport images. The
content weighting method is based on the spatial information
which reﬂects the spatial details of viewed regions.

We present the performance results for various weighting
strategies on OIQA [20] and CVIQD [17], [18] databases,
as illustrated in TABLE III. We can ﬁnd that our proposed
MFILGN algorithm is insensitive to different weighting meth-
ods, which demonstrates the robustness of the proposed model.

102030405060708090Training percentage (%)0.80.820.840.860.880.90.920.940.960.98PLCC performanceOIQACVIQD12

Fig. 10: Mean performance values and standard error bars for machine learning-based algorithms across 1,000 train-test trials.
(a) Run on OIQA database; (b) Run on CVIQD database.

Therefore, for the sake of simpliﬁcation, we choose the
average weighting in the proposed MFILGN.

E. Effects of Different Parameters

Since viewport extraction is involved in our model, it is
interesting to explore how the performance will be affected
by different viewport numbers to be extracted. We test the
performance results with respect to various viewport numbers
on OIQA [20] and CVIQD [17], [18] databases, as shown
in TABLE IV. Three cases are considered, which include
the number equaling to 6, 20, and 80. As we can observe
from this table, the performance of our proposed MFILGN
gets better as the number of extracted viewports increases.
However, the increasing viewport numbers inevitably could
bring about more computational complexity. Thus, in order
to ﬁnd the balance between performance and computation, 20
viewports for each 360-degree image are utilized in our model,
which is demonstrated in Fig. 8.

Furthermore, we change the DHWT decomposition times to
see if using more image decompositions would lead to better
performance. From the results listed in TABLE V, we can
observe that by adding the number of DHWT layers, i.e. using
more DHWT decomposition times, our model has a small
performance boost. However, the increased layers of image de-
composition could need to extract more features, which results
in more computation time. In order to reduce the computation
complexity, we choose one-time DHWT for measuring multi-
frequency information, which can obtain the tradeoff between
performance values and computation complexity.

In addition, we validate the change of PLCC performance
training
of our proposed MFILGN regarding to different
percentages. As presented in Fig. 9, in general, a large number
of training data will bring about performance increase on
both OIQA [20] and CVIQD [17], [18] databases. In the
proposed method, we choose 80%-20% for the training-testing
split because this is a common practice for perceptual quality

assessment in the literature [9], [27], [28]. Besides, since the
data distributions of OIQA and CVIQD databases are very
different, the correlation performance for these two databases
could be disparate. On one hand, even using 10% training
data,
the MFILGN model can still deliver quite competi-
tive performance results, especially for the CVIQD database,
which further demonstrates the effectiveness of our proposed
MFILGN method. On the other hand, one possible explanation
for the performance differences between the two databases
could be that the OIQA database seems more challenging
compared to the CVIQD database.

F. Statistical Signiﬁcance Analysis

Since the compared BRISQUE [32], BMPRI [33], and our
proposed MFILGN are all based on the machine learning
model called support vector regression, we repeat the process
of database splitting for 1,000 times to compare the mean
and standard deviation of performance values. We show the
performance results in Fig. 10, where the mean and standard
deviations (std) of the SROCC and PLCC values across the
1,000 trials for three algorithms are illustrated. As can be seen
in this ﬁgure, the proposed MFILGN method can achieve the
higher mean value and smaller std compared to the others,
which further suggests that MFILGN performs more precisely
and consistently.

G. Validity of Individual Proposed Quality Measure

We explore the effectiveness of each proposed component
in the MFILGN framework, namely multi-frequency infor-
mation, local naturalness, global naturalness, and local-global
naturalness. The performance values are provided in TABLE
VI. we can see that the local naturalness achieves the best
performance, demonstrating the importance of viewports in
evaluating the perceptual quality of 360-degree images. In
addition, the multi-frequency information measurement can

(a)(b)TABLE VI: ABLATION STUDY ON OIQA AND CVIQD DATABASES. THE BEST RESULTS ARE DENOTED IN BOLD.

13

Database

Methods

OIQA

CVIQD

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

Multi-frequency information
Global naturalness
Local naturalness
Local-global naturalness
Proposed MFILGN

0.7734
0.9260
0.9460
0.9495
0.9614

0.7961
0.9410
0.9549
0.9593
0.9695

1.2666
0.7106
0.6195
0.5915
0.5146

0.7879
0.9520
0.9626
0.9657
0.9670

0.8285
0.9599
0.9723
0.9739
0.9751

7.8328
3.9300
3.2676
3.1753
3.1036

TABLE VII: PERFORMANCE COMPARISONS FOR CROSS DATABASE TEST BY TRAINING ON CVIQD DATABASE AND
TESTING ON OIQA DATABASE. THE BEST RESULTS ARE DENOTED IN BOLD.

Database

Methods

JPEG

JPEG2000

ALL

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

SROCC

PLCC

RMSE

MC360IQA [49]
Proposed MFILGN

0.8412
0.8889

0.8898
0.9027

4.3950
0.9883

0.6221
0.6781

0.6211
0.7107

5.1294
1.5545

0.6981
0.7589

0.7443
0.7885

5.9184
1.3864

be used as a supplement
to local and global naturalness
features for further improving the performance of our proposed
model. Especially for the OIQA database, by adding the multi-
frequency information measurement, the SROCC performance
improves from 0.9495 to 0.9614.

H. Cross Database Test

We validate the generalization capability of our proposed
MFILGN model by cross database test, which is widely used
in veriﬁcation of model generalization ability. Considering that
the CVIQD database has more compression distortion types
than the OIQA database. Except for compression artifacts,
the OIQA database contains Gaussian blur and Gaussian
noise. Followed by [49], we train objective quality assessment
models on CVIQD database and then test JPEG and JP2000
compression for the OIQA database. The comparison results
are shown in Table VII. From this table, we can ﬁnd that our
MFILGN outperforms the state-of-the-art MC360IQA model
[49] for both JPEG and JPEG2000 compression as well as
the overall performance. It is also interesting to observe that
testing on the JPEG compression distortion has a better per-
formance compared to the JPEG2000 compression distortion.
This is mainly because JPEG compression is the only common
distortion t in both the two databases. To sum up, we can
conclude that the proposed MFILGN method can achieve a
good generalization capability.

V. CONCLUSIONS

In this paper, we present

the Multi-Frequency Informa-
tion and Local-Global Naturalness (MFILGN) scheme for
no-reference quality assessment of omnidirectional images.
The proposed MFILGN method is composed of two new
measurements,
including multi-frequency information mea-
surement and local-global naturalness measurement. We de-
sign this model by considering the HVS and the viewing
process of 360-degree images. Speciﬁcally, based on the

frequency-dependent property of visual cortex, we ﬁrst exploit
the multi-frequency channel decomposition to obtain both
low-frequency and high-frequency subbands for 360-degree
images. The entropy intensities of these subbands are then
used to measure the multi-frequency information. Additionally,
according to the viewing process, we adopt both local and
global naturalness features from projected ERP maps and dif-
ferent viewports. The extracted features from our proposed two
measuremenrs are fused together by the regression learning,
which can predict the perceptual quality of omnidirectional
images. We compare our proposed MFILGN with a number
of state-of-the-art image quality assessment approaches on two
publicly available 360-degree image quality databases. The
experimental results demonstrate the superiority of our model.
We plan to develop a parametric model based on the
proposed features and extend our method to omnidirectional
video quality assessment. Furthermore, the optimization of
VR processing systems based on our proposed blind quality
assessment model is also promising in the future work.

REFERENCES

[1] J. Diemer, G. W. Alpers, H. M. Peperkorn, Y. Shiban, and
A. M¨uhlberger, “The impact of perception and presence on emotional
reactions: a review of research in virtual reality,” Frontiers in psychology,
vol. 6, p. 26, 2015.

[2] M. Xu, C. Li, S. Zhang, and P. Le Callet, “State-of-the-art in 360
video/image processing: Perception, assessment and compression,” IEEE
Journal of Selected Topics in Signal Processing, vol. 14, no. 1, pp. 5–26,
2020.

[3] W. Zhou, Z. Chen, and W. Li, “Stereoscopic video quality prediction
based on end-to-end dual stream deep neural networks,” in Paciﬁc Rim
Conference on Multimedia. Springer, 2018, pp. 482–492.

[4] W. Zhou, N. Liao, Z. Chen, and W. Li, “3D-HEVC visual quality as-
sessment: Database and bitstream model,” in 2016 Eighth International
Conference on Quality of Multimedia Experience (QoMEX).
IEEE,
2016, pp. 1–6.

[5] Y. Zhou, W. Zhou, P. An, and Z. Chen, “Visual comfort assessment for
stereoscopic image retargeting,” in 2018 IEEE International Symposium
on Circuits and Systems (ISCAS).

IEEE, 2018, pp. 1–5.

[6] J. Xu, C. Lin, W. Zhou, and Z. Chen, “Subjective quality assessment
of stereoscopic omnidirectional image,” in Paciﬁc Rim Conference on
Multimedia. Springer, 2018, pp. 589–599.

[7] L. Shi, S. Zhao, W. Zhou, and Z. Chen, “Perceptual evaluation of light
ﬁeld image,” in 2018 25th IEEE International Conference on Image
Processing (ICIP).

IEEE, 2018, pp. 41–45.
[8] S. Zhao, J. Xu, Y. Hu, W. Zhou, S. Liu, and Z. Chen, “How do
you perceive differently from an AI-a database for semantic distortion
measurement,” in 2019 IEEE International Symposium on Circuits and
Systems (ISCAS).

IEEE, 2019, pp. 1–5.

[9] Z. Chen, W. Zhou, and W. Li, “Blind stereoscopic video quality assess-
ment: From depth perception to overall experience,” IEEE Transactions
on Image Processing, vol. 27, no. 2, pp. 721–734, 2017.

[10] Q. Jiang, F. Shao, W. Lin, K. Gu, G. Jiang, and H. Sun, “Optimizing mul-
tistage discriminative dictionaries for blind image quality assessment,”
IEEE Transactions on Multimedia, vol. 20, no. 8, pp. 2035–2048, 2017.
[11] Q. Jiang, F. Shao, W. Gao, Z. Chen, G. Jiang, and Y.-S. Ho, “Uniﬁed
no-reference quality assessment of singly and multiply distorted stereo-
scopic images,” IEEE Transactions on Image Processing, vol. 28, no. 4,
pp. 1866–1881, 2018.

[12] J. Xu, W. Zhou, Z. Chen, S. Ling, and P. Le Callet, “Binocular
rivalry oriented predictive auto-encoding network for blind stereoscopic
image quality measurement,” IEEE Transactions on Instrumentation and
Measurement, 2020.

[13] Q. Jiang, W. Zhou, X. Chai, G. Yue, F. Shao, and Z. Chen, “A full-
reference stereoscopic image quality measurement via hierarchical deep
feature degradation fusion,” IEEE Transactions on Instrumentation and
Measurement, 2020.

[14] W. Zhou, Q. Jiang, Y. Wang, Z. Chen, and W. Li, “Blind quality as-
sessment for image superresolution using deep two-stream convolutional
networks,” Information Sciences, 2020.

[15] E. Upenik, M.

ˇReˇr´abek, and T. Ebrahimi, “Testbed for subjective
evaluation of omnidirectional visual content,” in 2016 Picture Coding
Symposium (PCS).

IEEE, 2016, pp. 1–5.

[16] E. Upenik, M. Rerabek, and T. Ebrahimi, “On the performance of
objective metrics for omnidirectional visual content,” in 2017 Ninth In-
ternational Conference on Quality of Multimedia Experience (QoMEX).
IEEE, 2017, pp. 1–6.

[17] W. Sun, K. Gu, G. Zhai, S. Ma, W. Lin, and P. Le Calle, “CVIQD:
Subjective quality evaluation of compressed virtual reality images,”
in 2017 IEEE International Conference on Image Processing (ICIP).
IEEE, 2017, pp. 3450–3454.

[18] W. Sun, K. Gu, S. Ma, W. Zhu, N. Liu, and G. Zhai, “A large-
scale compressed 360-degree spherical image database: From subjec-
tive quality evaluation to objective model comparison,” in 2018 IEEE
20th international workshop on multimedia signal processing (MMSP).
IEEE, 2018, pp. 1–6.

[19] M. Huang, Q. Shen, Z. Ma, A. C. Bovik, P. Gupta, R. Zhou, and X. Cao,
“Modeling the perceptual quality of immersive images rendered on head
mounted displays: Resolution and compression,” IEEE Transactions on
Image Processing, vol. 27, no. 12, pp. 6039–6050, 2018.

[20] H. Duan, G. Zhai, X. Min, Y. Zhu, Y. Fang, and X. Yang, “Perceptual
quality assessment of omnidirectional images,” in 2018 IEEE Interna-
tional Symposium on Circuits and Systems (ISCAS).
IEEE, 2018, pp.
1–5.

[21] S. G. Mallat, “Multifrequency channel decompositions of images and
wavelet models,” IEEE Transactions on Acoustics, Speech, and Signal
Processing, vol. 37, no. 12, pp. 2091–2110, 1989.

[22] Y. Yang, G. Jiang, M. Yu, and Y. Qi, “Latitude and binocular perception
based blind stereoscopic omnidirectional image quality assessment for
VR system,” Signal Processing, vol. 173, p. 107586, 2020.

[23] F. Heitger, L. Rosenthaler, R. Von Der Heydt, E. Peterhans, and
O. K¨ubler, “Simulation of neural contour mechanisms: from simple to
end-stopped cells,” Vision Research, vol. 32, no. 5, pp. 963–981, 1992.
[24] S. G. Mallat, “A theory for multiresolution signal decomposition: the
wavelet representation,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 11, no. 7, pp. 674–693, 1989.

[25] P. V. Vu and D. M. Chandler, “A fast wavelet-based algorithm for global
and local image sharpness estimation,” IEEE Signal Processing Letters,
vol. 19, no. 7, pp. 423–426, 2012.

[26] G. Wang, Z. Wang, K. Gu, L. Li, Z. Xia, and L. Wu, “Blind quality
metric of dibr-synthesized images in the discrete wavelet transform
domain,” IEEE Transactions on Image Processing, vol. 29, pp. 1802–
1814, 2019.

[27] L. Shi, W. Zhou, Z. Chen, and J. Zhang, “No-reference light ﬁeld
image quality assessment based on spatial-angular measurement,” IEEE
Transactions on Circuits and Systems for Video Technology, 2019.
[28] W. Zhou, L. Shi, Z. Chen, and J. Zhang, “Tensor oriented no-reference
light ﬁeld image quality assessment,” IEEE Transactions on Image
Processing, vol. 29, pp. 4070–4084, 2020.

14

[29] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
Transactions on Image Processing, vol. 13, no. 4, pp. 600–612, 2004.
[30] Z. Wang, E. P. Simoncelli, and A. C. Bovik, “Multiscale structural
similarity for image quality assessment,” in The Thrity-Seventh Asilomar
Conference on Signals, Systems & Computers, 2003, vol. 2.
Ieee, 2003,
pp. 1398–1402.

[31] L. Zhang, L. Zhang, X. Mou, and D. Zhang, “FSIM: A feature similarity
index for image quality assessment,” IEEE Transactions on Image
Processing, vol. 20, no. 8, pp. 2378–2386, 2011.

[32] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image
quality assessment in the spatial domain,” IEEE Transactions on Image
Processing, vol. 21, no. 12, pp. 4695–4708, 2012.

[33] X. Min, G. Zhai, K. Gu, Y. Liu, and X. Yang, “Blind image quality esti-
mation via distortion aggravation,” IEEE Transactions on Broadcasting,
vol. 64, no. 2, pp. 508–517, 2018.

[34] X. Jin, Z. Chen, J. Lin, J. Chen, W. Zhou, and C. Shan, “A decomposed
dual-cross generative adversarial network for image rain removal.” in
BMVC, 2018, p. 119.

[35] W. Zhou, Z. Chen, and W. Li, “Dual-stream interactive networks for no-
reference stereoscopic image quality assessment,” IEEE Transactions on
Image Processing, vol. 28, no. 8, pp. 3946–3958, 2019.

[36] J. Kim and S. Lee, “Deep learning of human visual sensitivity in image
quality assessment framework,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2017, pp. 1676–1684.

[37] W. Zhang, K. Ma, J. Yan, D. Deng, and Z. Wang, “Blind image quality
assessment using a deep bilinear convolutional neural network,” IEEE
Transactions on Circuits and Systems for Video Technology, 2018.
[38] S. Jia and Y. Zhang, “Saliency-based deep convolutional neural network
for no-reference image quality assessment,” Multimedia Tools and
Applications, vol. 77, no. 12, pp. 14 859–14 872, 2018.

[39] Q. Yan, D. Gong, and Y. Zhang, “Two-stream convolutional networks
for blind image quality assessment,” IEEE Transactions on Image
Processing, vol. 28, no. 5, pp. 2200–2211, 2018.

[40] M. Yu, H. Lakshman, and B. Girod, “A framework to evaluate omnidi-
rectional video coding schemes,” in 2015 IEEE International Symposium
on Mixed and Augmented Reality.

IEEE, 2015, pp. 31–36.

[41] Y. Sun, A. Lu, and L. Yu, “Weighted-to-spherically-uniform quality
evaluation for omnidirectional video,” IEEE Signal Processing Letters,
vol. 24, no. 9, pp. 1408–1412, 2017.

[42] V. Zakharchenko, K. P. Choi, and J. H. Park, “Quality metric for
spherical panoramic video,” in Optics and Photonics for Information
Processing X, vol. 9970. International Society for Optics and Photonics,
2016, p. 99700C.

[43] M. Xu, C. Li, Z. Chen, Z. Wang, and Z. Guan, “Assessing visual quality
of omnidirectional videos,” IEEE Transactions on Circuits and Systems
for Video Technology, vol. 29, no. 12, pp. 3516–3530, 2018.

[44] Y. Zhou, M. Yu, H. Ma, H. Shao, and G. Jiang, “Weighted-to-spherically-
uniform ssim objective quality evaluation for panoramic video,” in
2018 14th IEEE International Conference on Signal Processing (ICSP).
IEEE, 2018, pp. 54–57.

[45] S. Chen, Y. Zhang, Y. Li, Z. Chen, and Z. Wang, “Spherical structural
similarity index for objective omnidirectional video quality assessment,”
in 2018 IEEE International Conference on Multimedia and Expo
(ICME).

IEEE, 2018, pp. 1–6.

[46] H. G. Kim, H.-T. Lim, and Y. M. Ro, “Deep virtual reality image quality
assessment with human perception guider for omnidirectional image,”
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 30, no. 4, pp. 917–928, 2019.

[47] C. Li, M. Xu, X. Du, and Z. Wang, “Bridge the gap between vqa and
human behavior on omnidirectional video: A large-scale dataset and a
deep learning model,” in Proceedings of the 26th ACM International
Conference on Multimedia, 2018, pp. 932–940.

[48] C. Li, M. Xu, L. Jiang, S. Zhang, and X. Tao, “Viewport proposal CNN
for 360 video quality assessment,” in 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR).
IEEE, 2019, pp.
10 169–10 178.

[49] W. Sun, X. Min, G. Zhai, K. Gu, H. Duan, and S. Ma, “MC360IQA:
A multi-channel CNN for blind 360-degree image quality assessment,”
IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 1,
pp. 64–77, 2019.

[50] J. Xu, W. Zhou, and Z. Chen, “Blind omnidirectional image quality
assessment with viewport oriented graph convolutional networks,” IEEE
Transactions on Circuits and Systems for Video Technology, 2020.
[51] Y.-K. Lai and C.-C. J. Kuo, “A haar wavelet approach to compressed
image quality measurement,” Journal of Visual Communication and
Image Representation, vol. 11, no. 1, pp. 17–40, 2000.

15

[52] R. Reisenhofer, S. Bosse, G. Kutyniok, and T. Wiegand, “A haar wavelet-
based perceptual similarity index for image quality assessment,” Signal
Processing: Image Communication, vol. 61, pp. 33–43, 2018.

[53] W. Zhu, G. Zhai, X. Min, M. Hu, J. Liu, G. Guo, and X. Yang, “Multi-
channel decomposition in tandem with free-energy principle for reduced-
reference image quality assessment,” IEEE Transactions on Multimedia,
vol. 21, no. 9, pp. 2334–2346, 2019.

[54] M. Carandini, D. J. Heeger, and J. A. Movshon, “Linearity and normal-
ization in simple cells of the macaque primary visual cortex,” Journal
of Neuroscience, vol. 17, no. 21, pp. 8621–8644, 1997.

[55] J. Xu, Z. Luo, W. Zhou, W. Zhang, and Z. Chen, “Quality assessment of
stereoscopic 360-degree images from multi-viewports,” in 2019 Picture
Coding Symposium (PCS).

IEEE, 2019, pp. 1–5.

[56] Z. Chen, J. Xu, C. Lin, and W. Zhou, “Stereoscopic omnidirectional
image quality assessment based on predictive coding theory,” IEEE
Journal of Selected Topics in Signal Processing, vol. 14, no. 1, pp.
103–117, 2020.

[57] B. Sch¨olkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett, “New
support vector algorithms,” Neural Computation, vol. 12, no. 5, pp.
1207–1245, 2000.

[58] V. Q. E. Group et al., “Final report from the video quality experts group
on the validation of objective models of video quality assessment, phase
II,” 2003 VQEG, 2003.

