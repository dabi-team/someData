8
1
0
2

r
p
A
0
1

]

C
D
.
s
c
[

3
v
0
1
2
8
0
.
6
0
7
1
:
v
i
X
r
a

IS-ASGD: Accelerating Asynchronous SGD using Importance
Sampling

Fei Wang ∗1, Weichen Li †2, Jason Ye ‡3, and Guihai Chen §1

1Department of Computer Science, Shanghai Jiao Tong University
2Carnegie Mellon University
3Intel Asia Paciﬁc R&D Ltd.

April 11, 2018

Abstract

1 Introduction

Variance reduction (VR) techniques for conver-
gence rate acceleration of stochastic gradient de-
scent (SGD) algorithm have been developed with
great eﬀorts recently. VR’s two variants, stochastic
variance-reduced-gradient (SVRG-SGD) and im-
portance sampling (IS-SGD) have achieved remark-
able progresses. Meanwhile, asynchronous SGD
(ASGD) is becoming more critical due to the ever-
increasing scale of the optimization problems. The
application of VR in ASGD to accelerate its con-
vergence rate has therefore attracted much interest
and SVRG-ASGDs are therefore proposed. How-
ever, we found that SVRG suﬀers dissatisfying per-
formance in accelerating ASGD when the datasets
In such case, SVRG-
are sparse and large-scale.
ASGD’s iterative computation cost is magnitudes
higher than ASGD which makes it very slow. On
the other hand, IS achieves improved convergence
rate with few extra computation cost and is in-
variant to the sparsity of dataset. This advan-
tage makes it very suitable for the acceleration of
ASGD for large-scale sparse datasets. In this pa-
per we propose a novel IS-combined ASGD for ef-
fective convergence rate acceleration, namely, IS-
ASGD. We theoretically prove the superior conver-
gence bound of IS-ASGD. Experimental results also
demonstrate our statements.

∗bomber@sjtu.edu.cn
†weichenli@cmu.edu
‡jason.ye.y@intel.com
§gchen@cs.sjtu.edu.cn

For the empirical risk minimizations (ERM) prob-
lems, stochastic gradient descent (SGD) may be the
most widely adopted solver algorithm. Let w be the
optimizer to be learned, denote

fi(w) = φi(w) + ηr(w),

(1)

1, 2, ..., n

are vector functions that
where φi, i
∈ {
map Rd
R and r(w) is the regularizer and η is
the regularization factor. This paper studies the
following ERM optimization problem:

→

}

min
w∈Rd

F (w) :=

1
n

n

Xi=1

fi(w).

(2)

For SGD algorithm, w is updated as:

wt+1 = wt

−

λ

∇

fit (wt),

(3)

∼

where it
P means i is drawn iteratively with re-
spect to sampling probability distribution P and
λ is the step-size. With the growing concurrency
of hardwares, lock-free asynchronous SGD (ASGD)
algorithms Recht et al. (2011) have been developed
to for speedup. With the improved speed and scal-
ability, ASGDs quickly become indispensable and
are de facto solvers for large-scale sparse optimiza-
tions. With the maturing of ASGDs, many follow-
ing interests naturally shifted to the convergence
acceleration techniques of them.

1

 
 
 
 
 
 
Algorithm 1 Generic SVRG-styled ASGD

1: procedure SVRG-ASGD(T )
2:

Parallel do

for t = 0; t

= T ; t++ do

3:
4:

5:

6:

7:
8:

if sync(t) then
s = wt
µ = 1
n

P
fit (wt)

vt =
∇
wt+1 = wt

−

n
i=0 ∇
− ∇
λvt

fi(s)
fit (s) + µ ⊲ it
P
⊲ wt is the global

∼

model.

9:
10:

End Parallel
return

1.1 Variance Reduction for Conver-
gence Acceleration of ASGD

It is commonly known that the variance of the
stochastic gradient:

and has therefore been studied, and several re-
lated works were proposed. Interestingly, we found
that all these works by far are based on SVRG-
styled ASGD (SVRG-ASGD), e.g., Huo and Huang
(2017); J. Reddi et al. (2015); Liu et al. (2017);
Meng et al. (2017); Zhao and Li (2016) while the
research of IS-styled ASGD is still untouched. One
possible reason is that SVRG-SGD’s iterative con-
vergence rate, i.e., iteration count as the x-axis of
the convergence curve, is much higher than that
of IS-SGD. However, in practical deployments, the
absolute convergence rate, i.e., wall-clock as the x-
axis of the convergence curve, holds the actual sig-
niﬁcance. Unfortunately, previous works validated
SVRG-styled ASGD with small-scale and relative
dense datasets, in which its drawbacks on the abso-
lute convergence rate for large-scale sparse datasets
are not revealed.

E [V(

fit (wt)

∇

F (wt)] ,

− ∇

(4)

1.2 Absolute Convergence Accelera-
tion: Sparsity and Performance

is one of the major reasons that slow down the
convergence rate of SGD. The uses of variance re-
duction (VR) techniques to accelerate the iterative
convergence rate of SGD have therefore attracted
much interest recently. VR improves the iterative
convergence rate by constructing variance-reduced
gradient instead of using the original stochastic gra-
dient directly for model update.

One VR algorithm, stochastic variance-reduced-
gradient (SVRG) Johnson and Zhang (2013) uses
historical true-gradient and model snapshots to
SVRG and its
reduce the gradient variance.
variants, e.g., SAGA Defazio et al. (2014) have
been reported to be successful
in accelerating
the iterative convergence rate of SGD. Mean-
while, along with the intensively-studied SVRG-
styled VR algorithms, another newly proposed VR
technique, namely,
importance sampling (IS) al-
gorithm also achieves decreased stochastic gra-
dient variance and improved convergence bound
for SGD eﬀectively by using non-uniform sam-
pling (of the training samples) schemes as pro-
posed in literatures Csiba and Richtárik (2016);
Needell et al. (2014); T.Strohmer and Vershynin.
(2009); Zhao and Zhang. (2015).

Recently, with the success of VR techniques,
combining ASGD with VR to further improve
its convergence rate shows practical signiﬁcance

After intensive evaluations of the existing SVRG-
ASGD algorithms, we found that its absolute con-
vergence rate is severely limited when dealing with
large-scale sparse datasets, which is, unfortunately,
de facto type of datasets that ASGDs are supposed
to work with. See Algorithm 1 for the generic
scheme of SVRG-ASGD, two bottlenecking issues
decrease its absolute convergence rate drastically.
They are caused by the same reason, i.e., SVRG is
intrinsically dense.

∇

Sparsity for Less Computation As can be
seen in line 7, for each iteration of SVRG-ASGD,
fit (s) and µ are
two additional vector adds, i.e.,
needed.
Intuitively, this increases the computa-
tion cost up to two times. However, the actual
increase of the computation cost can be extremely
large. See Figure 1 for illustration, we should be
noted that in large-scale sparse optimization prob-
lems where ASGD is applied, stochastic gradient
fit (wt) is actually very sparse (as shown in the
∇
three upper rows) and is thus index-compressed,
i.e., only the non-zeros features are stored with
their corresponding indices. The update of wt is
thus proceeded in an index-compressed way and the
add operation is actually executed very few times,
e.g., 10−7 * d, comparing to the dimensionality d.
For sparse datasets with dimensions in tens of mil-

2

6
Figure 1: Data Compression for Performance

fit : Extremely
∇
10−7),
sparse (
∼
compressed for
performance

x2
. . .
xn

x1

x11 x13

x21 x22

x13 x14 . . . x1m
x23 x24 . . . x2m

xn1 xn2 xn3 xn4 . . . xnm

1),
µ: Dense (
non-compressed

∼

µ

µ1

µ2

µ3

µ4 . . . µm

lions (which is not rarely seen in modern optimiza-
tions), index-compression of the sparse gradient is
the most eﬃcient method for ASGDs. However,
for SVRG-ASGD, due to the involvement of the
historical true gradient µ, which is in fact a dense
gradient with size d as shown in the last row in
Figure 1, the index-compressions of µ is meaning-
less. The update of wt has to be proceeded in
the form of raw vector with full length d, which
is typically ﬁve to seven magnitudes larger than
the index-compressed stochastic gradient. Adding
arrays of such large magnitude at each iteration
is completely impractical. In consideration of the
large data sample counts, the training can be ex-
tremely time-consuming even it actually needs less
iterations (SVRG accelerates ASGD in iteration).
In fact, for modern stochastic optimizations where
sparse datasets with extra-high dimensionality are
common, we found that performing SVRG-ASGD
is computationally infeasible and often fails to com-
plete training in a reasonable time due to the dras-
tically increased computation cost, which is caused
by the loss of sparsity.

For large datasets, when the true-gradient is 103
magnitudes higher than
fi, the absolute conver-
∇
gence rate of SVRG-ASGD shows large net de-
crease. Unfortunately,
for the previous SVRG-
ASGD works, the absolute convergence results were
conducted on relative low dimensionality (around
105) datasets; or using larger datasets (107)
102
but the comparison is limited between SVRGs.

∼

We also ﬁnd that the (only) public version of
SVRG-ASGD1 does not follow the proposed algo-
rithm in its corresponding literature J. Reddi et al.
(2015). The public version actually omit the addi-

1https://github.com/CMU-ML-17-102/svrg.git ,

com-

mitted by the author of J. Reddi et al. (2015)

tion of dense gradient µ at each iteration (in line
7) and only do it once at the end of each epoch
by multiplying µ with n. It seems that the inten-
tion of this approximation is to avoid the expensive
dense gradient operation at each iteration. Unfor-
tunately, we found the convergence curve of this
public version far from the literature version.
Sparsity for Less Conﬂicts
It is commonly
known that one fundamental assumption for ASGD
is that the datasets are suﬃciently sparse, oth-
erwise the conﬂict updates for the global model
would certainly raise the risk of non-convergence
or inferior convergence curves. As a consequence,
for SVRG-ASGD, the loss of sparsity due to the
usage of dense true-gradient µ does not only in-
crease the iterative time cost magnitudes higher
but also increases the potentiality of conﬂict up-
dates. Such conﬂicts weaken the beneﬁts of using
variance-reduced gradient, which can be deemed as
another negative eﬀect on the absolute convergence
rate.

Obviously, in oder to achieve the absolute con-
verge rate acceleration of ASGD, a true-gradient-
free VR algorithm has to be designed. Naturally,
importance sampling as an elegant true-gradient-
free VR algorithm comes to our consideration.

1.3 IS-ASGD for Guaranteed Abso-

lute Convergence Acceleration

Clearly, when designing a VR algorithm to achieve
absolute convergence acceleration for ASGD, we
hope it not only remains a minimal increase of it-
erative time cost but also maintains low potential-
ity of conﬂict updates, which seems to be a dif-
ﬁcult task. Fortunately, we notice that IS natu-
it does not rely on the variance-
rally suits in:

3

reduced-gradient vt which makes it free from the
true-gradient µ and thus the above mentioned
performance-bottlenecking problems do not exist.
In fact, IS can be implemented with no extra on-line
computation by generating the sample sequences
beforehand and let the computation threads iter-
ate over the generated sequences, which leaves the
computation kernel the same as ASGD. The cal-
culation of the sampling distribution is typically
fast which can actually be ignored comparing to
the whole training time cost. That is, IS-ASGD
is able to preserve almost the same iterative com-
putation cost and low conﬂict updates with ASGD
while achieving a higher iterative convergence rate.
These are the key advantages for achieving a high
absolute convergence rate acceleration.

As mentioned above,

since SVRG typically
achieves much better performance on iterative con-
vergence rate acceleration, research by far all focus
on the SVRG-ASGD algorithms while the IS-styled
ASGD algorithm is still left unstudied. We consider
this missing ﬁeld worthy to be researched due to its
practical signiﬁcance for high performance large-
scale sparse optimizations and its novelty. Follow-
ing this idea, we analyze and propose the algo-
rithm that uses IS to accelerate the absolute con-
vergence rate of ASGD eﬀectively, i.e., IS-ASGD,
as the novel contribution of this paper.

The rest of this paper is organized as follows.
In section II we analyze the potential problems of
applying IS in ASGD and propose IS-ASGD algo-
rithm with detailed discussion. Section III is dedi-
cated to the theoretical analysis of the convergence
bound improvement of IS for ASGD. The in-depth
evaluations of both iterative and absolute conver-
gence results are provided in Section IV. Finally,
the conclusion of this paper is given in Section V.

2 Importance Sampling
Asynchronous SGD

for

We ﬁrst brieﬂy introduce some key concepts of IS.
Like most previous related literatures, we make
the following necessary assumptions for the conver-
gence analysis of the stochastic optimization prob-
lem studied in this paper.

µ-Convex: fi is strongly convex with param-

•

4

eter µ, that is:

y,

x
h

−

∇

fi(x)

−∇

fi(y)

i ≥

µ

k

x

−

2
2,

y

k

x, y

∀

Rd
(5)

∈

•

Li-Lipschitz: Each fi is continuously diﬀer-
entiable and
fi has Lipschitz constant Li
w.r.t

∇
k · k2, i.e.,

fi(x)

k∇

− ∇

fi(y)

k2 ≤

Li

x

k

y

k2,

−

x, y

∀

∈

Rd
(6)

where

i
∀

∈ {

1, 2, ..., N

.

}

2.1 Importance Sampling

Importance sampling reduces the gradient variance
through a non-uniform sampling procedure instead
of drawing sample uniformly. For conventional
stochastic optimization algorithms, the sampling
probability of i-th sample at t-th iteration, denoted
by pt
i, always equals to 1/N while in an IS scheme,
pt
i is endowed with an importance factor I t
i and
thus the i-th sample is sampled at t-th iteration
with respect to a weighted probability:

i = I t
pt

i /N,

s.t.

pt
i = 1

(7)

N

Xi=1

where N is the number of training samples. With
this non-uniform sampling procedure, to obtain an
unbiased expectation, the update of w is modiﬁed
as:

wt+1 = wt

−

λ
npt
it ∇

fit (wt)

(8)

where it is drawn i.i.d w.r.t the weighted sam-
pling probability distribution P t =
1, 2, ..., N

pt
i}

i
∀

∈

{

.

,

{

}

2.2 Importance Sampling for Vari-

ance Reduction

Recall the optimization problem in Equation 2,
using the analysis result from Zhao and Zhang.
(2015), we have the following lemma:

Lemma 1. Let σ2 = E
2
2 where w⋆ =
k∇
1
µ , with the update scheme
F (w). Set λ
arg min

fi(w⋆)
k

w

≤

deﬁned in Equation 8, the following inequality sat-
isfy:

Algorithm 2 Practical Importance Sampling for
SGD

E[F (wt+1)

F (w⋆)]

1
2λ

E[

≤

w⋆

k

−

wt

µE
k

w⋆

wt

k

−

2
2 +

EV

λt
µ

(cid:16)

−

−

w⋆

2
2 − k
k
(npt

it )−1

where the variance is deﬁned as:

V

it )−1

(npt
(cid:0)

5:
2
6:
F (wt)
2
k
(10)
and the expectation is estimated w.r.t distribution
P t.

fit (wt)
(cid:1)

= E
k

fit (wt)

it)−1

(npt

−∇

∇

∇

1: procedure IS-SGD(T )
2
N
2]
i=1 According to Equa-
Construct P =
2:

pi

{

}

tion 12

Generate Sample Sequence S w.r.t distribu-

tion P .

−

wt+1k
fit (wt)
3:
(cid:17)

∇

(9)

4:

= T ; i++ do

for i = 0; i
it = S[i]
wt+1 = wt

λ
npit ∇

fit (wt)

−

It is easy to verify that in order to minimize the
gradient variance, the optimal sampling probability
pt
i should be set as:

pt
i =

fi(wt)

k2
fj(wt)

k∇
N
j=1 k∇

,

i
∀

1, 2, ..., N

.

P

∈ {

}
k2
(11)
Obviously, such iteratively re-estimation of P t is
completely impractical. The authors propose to
use the supremum of
k2 as an approxi-
mation. Since we have Li-Lipschitz of
fi, by
∇
R for any t, we have
further assuming
wt
k2 = RLi.
k∇
Thus the actual sampling probability of pi is calcu-
lated as:

RLi, i.e., sup

k2 ≤

fi(wt)

fi(wt)

fi(wt)

k ≤

k∇

k∇

k

pi =

Li
N
j=1 Lj

,

i
∀

1, 2, ..., N

∈ {

.

}

(12)

P

With such deﬁnition, P needs no update and is used
throughout the training procedure. The authors
further prove that with Equation 12, IS accelerated
SGD achieves a convergence bound as:

T

2
2

−

w⋆

1
T

Xt=1

w0k

F (w⋆)]

≤ r k

E[F (wt)

n
i=1 Li
n
(13)
while for standard SGD solver that actually sam-
ples xi w.r.t uniform distribution, the convergence
bound is:

(cid:18) P

−
σ

1
T

T

Xt=1

E[F (wt)

−

F (w⋆)]

≤ r k

w⋆

−

2
w0k
2
σn

P

n
i=1(L2
i )

when λ is set as
.
w⋆
(cid:17)
According to Cauchy-Schwarz inequality, we always

w0k

p

−

(cid:16)

σ

k

n

2
2/

√T

(14)

P

n
i=1 Li

n
2
i=1 Li)
n Pn
i=1 L2

have (P
prove convergence bound. Denote

i ≤

1, which implies that IS does im-

ψ =

,

(15)

n
i=1 Li)2
(
n
i=1(L2
P
i )
P

≪

we can conclude that the improvement gets larger
when ψ

n.

1

k

}

w

xi

xi

k∇

k∇

fi(w)

k2 ≤

yiwT xi

For example, for L2-regularized SVM optimiza-
tion problem with squared hinge loss, i.e., fi(w) =
2
2, where xi is the i-th sam-
(
−
⌊
k
ple and yi
is the corresponding label,
fi(w)

⌋+)2 + λ
2 k
1, +1
∈ {−
k2 can be bounded as
2(1 +

k2/√λ)
k

k2 + √λ (16)
The pseudo code of practical IS-SGD algorithm is
shown in Algorithm 2. As can be seen that, the core
procedure of IS is the construction of sampling dis-
tribution P . Once P is constructed, IS-SGD works
as same as SGD except that the training samples
are selected w.r.t to weighted probability distribu-
tion P and the step-size is adjusted with 1/npi.
It is clear that IS-ASGD does not rely on the true
gradient µ, which makes it free from the bottleneck-
ing issues that deteriorate the absolute convergence
1
rate of SVRG-ASGD drastically. This means that
T
IS as an eﬀective VR technique is very suitable for
ASGD solvers with large-scale sparse datasets.

,

(cid:19)

2.3 Importance Imbalance

that

solve
In most ASGD implementations
,
large-scale optimization problems, each training
thread/process runs on its corresponding core/node
and typically works on its local dataset for the sake
of performance and scalability. For IS-ASGD, such
data-segmentation brings in problem since each

1
T

5

6
Figure 2: Importance Balancing for ASGD

2.4 Importance Balancing for IS-

Random Shuﬄed

Balanced

x1
L1 = 1
x1
L1 = 1

x2
L2 = 2
x4
L4 = 4

x3
L3 = 3
x3
L3 = 3

x4
L4 = 4
x2
L2 = 2

node1

node2

Algorithm 3 Importance_Balancing

Calculate L =

1: procedure Importance_Balancing(
D
1, 2, ..., N
2:
}
) w.r.t L

∈ {
{
s =Get_Sorted_Indices(
D

D
for i = 0, idx = 0; i < n/2; i++ do

i
} ∀

Li

3:

)

4:
5:

6:

7:

8:

9:

r[idx + +] =
r[idx + +] =

D
D

if n%2 then

s[i]
s[n

D
D

i]

1

−

−

D
Return

r[idx] =
r, L

D

s[n/2]

D

thread/process (indexed with i) can only calcu-
late the sampling probability distribution Pi based
on its local dataset instead of the whole dataset,
which leads to sub-optimal VR performance of IS
for ASGD. See Figure 2 for illustration, assume we
have two working cores and whole training dataset
as:

=

x1, x2, x3, x4}

{

D

(17)

}

{

1, 2, 3, 4

located on core/node 1
with subset
x1, x2}
D1 =
{
while
located on core/node 2. With-
D2 =
x3, x4}
{
out loss of generality, we further assume their cor-
. For
responding Lipschitz constants as
comparison, in IS-SGD where the only training pro-
cess works on the whole dataset, the probability
distribution of being chosen as the training sample
is P =
while
p1 = 0.1, p2 = 0.2, p3 = 0.3, p4 = 0.4
in IS-ASGD with local-data-training, the sampling
probabilities are P1 =
and
respectively for each
P2 =
core/node. In global-data-training algorithm e.g.,
IS-SGD, p4 is much larger (twice over) than p2
while in IS-ASGD, p4 is even smaller than p2 which
is a heavy distortion from the theoretical optimum.

p3 = 0.43, p4 = 0.57

p1 = 0.33, p2 = 0.67

}

{

{

{

}

}

ASGD

To reduce such imbalance, a rearrangement of the
dataset before dividing/dispatching data segments
to its corresponding core/node should be consid-
ered. See the second row of Figure 2 for illustration,
to achieve a balanced importance segmentation, we
design a simple balancing algorithm as shown in
Algorithm 3. As can be seen that this procedure
generates rearranged dataset indices
r by locating
i] index together sequentially.
1
D
Denote Φa as the importance sum of core/node a:

s[i] and

s[n

−

−

D

D

Na

Φa =

Lai

Xi=1

(18)

where Lai is the Lipschitz constant of the i-th data
sample on core/node a, Na is the number of data
samples on core/node a. According to Equation 12,
we have the sampling probability of i-th data sam-
.
ple on core/node a as pai =
It is easy to prove that by satisfying

1, 2, ..., Na

Lai
Φa ,

i
∀

∈ {

}

Φa = Φb

a, b

∀

∈ {

1, ..., numT

}

(19)

where numT is the number of cores/nodes, then
the importance imbalance is eliminated. We call
this dataset rearrangement procedure as impor-
tance balancing. Obviously Algorithm 3 does not
guarantee to produce an equal-importance dataset
segmentation. However, segmenting dataset into
certain number (e.g. numT ) of equal-importance
subsets is a typical NP-hard problem which can not
be solved easily. We still use this simple head-tail
sequential matching procedure since it is a fast ap-
proximation and generally works well in practice.

Meanwhile, it has to be pointed out that if the
distribution of the Lipschitz constants closes to uni-
form distribution or the dataset is suﬃciently large,
a random shuﬄing would work just ﬁne for IS to
perform VR since the risk of severe importance im-
balance is low. We empirically deﬁne a metric ρ,
which measures the potential of the imbalance to
some extent:

ρ =

N
i=1(Li
N

P

µ)2

,

−

(20)

N
where µ =
i=1 Li/N . A lower ρ indicates lower
potential of severe importance imbalance and vice

P

6

3.1 Perturbed Iterate Analysis

)

D

In perturbed iterate analysis Mania et al. (2017),
the update of wt is modeled as:

)

wt+1 = wt

−

λ

∇

fit (wt + θt)

(21)

Algorithm 4 IS-ASGD Algorithm
1: procedure IS-ASGD(numT , T ,
2:

Calculate ρ
if ρ

ζ then

3:
4:

5:

6:

7:

8:

9:
10:

11:

12:

13:

14:
15:

16:

≤
r, L=Importance_Balancing(
D
else

D

r, L=Random_Shuﬄing(

)

D

D

Parallel do with numT
tid = GetTid()
numT : n∗(tid+1)
r[ n∗tid
tid =
D
D
Ltid = L[
tid]
D
Calculate Ptid w.r.t Ltid
Generate Local Sample Sequence Stid

numT

]

w.r.t Ptid

= T ; i++ do

for i = 0; i
it =
D
wt+1 = wt

tid[Stid[i]]
λ
npit ∇

−

fit (wt)

return

versa. Accordingly, IS-ASGD is designed to per-
form importance balancing in an adaptive manner
depending on ρ. The pseudo code of IS-ASGD is
shown in Algorithm 4, ζ is empirically set as 5−4.

3 Convergence Analysis of IS-

ASGD

Among the many analysis of the convergence bound
of ASGD, Horia et al. model ASGD as SGD
with perturbed inputs i.e., the inconsistent state
of the model is treated as true model with noise
added. Comparing to other convergence analysis,
this scheme is more general, compact and most im-
portantly, makes the analysis of the eﬀect of IS in
ASGD relative simple. We ﬁrst give a brief in-
troduction of the perturbed iterate analysis which
serves as the base of our analysis. For the ease
of analysis, we presume that the dataset is per-
fectly importance-balanced, i.e., Φa = Φb,
∈
, that is, IS achieves its theoretical
0, 1, ..., numT
{
convergence bound proved in previous literatures.

a, b

∀

}

7

where θt is the asynchrony error term caused by
lock-free update at iteration t. Let ˆwt = wt + θt.
We have:
wt+1 −

w⋆

−

λ

k

k

w⋆

∇
w⋆

fit ( ˆwt)
−
2
ˆwt
2λ
2 −
−
h
k
2
ˆwt
2 + 2λ
fit ( ˆwt)
h
k

−

+
fit ( ˆwt)
i
fit ( ˆwt)
i
∇
(22)
Recall the convexity assumption i.e., fi is strongly
convex with parameter µ, we have:

∇
wt,

k∇

−

wt

2
2 =
k
wt
=
k
λ2

2
2
k
w⋆,

ˆwt
h
w⋆

k

−
2
2 ≥

w⋆,
µ
2 k

∇
wt

fit( ˆwt)

w⋆

−

k

ˆwt

ˆwt

µ

k
µ

k

w⋆

wt

−

−

i ≥
2
2 −

ˆwt

µ

k

−

−
2
2. By substituting Equation 23 back to 22, we

Denote by ǫt the relative error of ˆwt, i.e, E
k
w⋆
k
obtain:
ǫt+1 ≤

+2λµ E
k

λµ)ǫt + λ2 E

fit ( ˆwt)
k

k∇

(1

−

2
2

2
2

k

k

2
2
(23)
ˆwt

ˆwt

−
Rt
1
{z

wt

2
2

k

}

|
+ 2λ E
ˆwt
h

wt,

}
|
fit ( ˆwt)
i
∇

Rt
0
{z
−

Rt
2
{z

}

1 and Rt

|
(24)
Among the three labeled terms, notice that Rt
0 is a
common term that exists in both SGD and ASGD
while Rt
2 are additional error terms intro-
duced by the inconsistency of the model. Rt
1 re-
ﬂects the diﬀerence between the true model and
the perturbed (noise added) one, and Rt
2 measures
the projection of such noise on the gradient of each
iteration. Now that the convergence bound can
0, Rt
be obtained once Rt
2 are bounded.
The authors ﬁrst bound E
M , i.e.,
k2 ≤
Rt
2, the concept
of conﬂict graph is introduced as the following.

k∇
M 2. Next, to bound Rt

fit ( ˆwt)
1 and Rt

1 and Rt

0 ≤

j

}

⊆ {

d
Conﬂict graph Denote by ci
j=0 the set
of feature index of data sample xi, i.e., j
ci only
if the j-th feature is provided in xi. In a conﬂict
= j, ver-
graph G =
tices vi and vj are connected with edge eij if and
= ∅. Further deﬁne two factors that
only if ci
reﬂect the extent of conﬂict update:

0, 1, ..., n

eij, vi

, i, j

∈ {

, i

cj

∩

∈

}

}

{

6
6
6
•

•

Delay parameter, τ , i.e., the maximum lag
between when a gradient is computed and
when it is applied to w.
It is assumed that
τ is linearly related to the concurrency.

Conﬂict parameter, ¯∆, which is the aver-
age degree of the conﬂict graph G, obviously,
datasets with higher ¯∆ suﬀers severer extent
of conﬂict updates and vice versa.

These two parameters measure the extent of incon-
sistency from two aspects. τ is set as the proxy of
concurrency of ASGD which can be controlled by
the users while ¯∆ measures the intrinsic potentials
of conﬂict update of dataset which is irrelevant to
the algorithm’s settings. The authors prove that
Rt
and Rt
λ2M 2
1 ≤
2
4λM 2τ ¯∆
bounded as Rt
n . Thus the recursion
2 ≤
can be obtained by plugging Rt
2 back to
Equation 24:

1 is bounded as Rt

2τ + 8τ 2 ¯∆
n

1 and Rt

(cid:16)

(cid:17)

ǫt+1 ≤

(1

−

λµ)ǫt + λ2M 2

|
+ λ2M 2

ξ
{z

8τ

(cid:16)

¯∆
n

}

+ 4λµτ + 16λµτ 2

|

δ
{z

3.2 Bounding IS-ASGD

(25)

¯∆
n

(cid:17)

}

Now the recursion of ǫt is divided into two parts,
i.e., accurate SGD term ξ and noise term δ. With
such scheme of modeling, the diﬃculty of the analy-
sis caused by the inconsistency can be greatly sim-
pliﬁed. We thus have the following lemma that
bounds IS-ASGD.

Lemma 2. For IS-ASGD algorithm that follows
the scheme of Algorithm 4, by satisfying the con-
vexity and continuity conditions in Equation 5
and Equation 6. Denote by σ the residual, i.e.,
E
k2, with a proper stepsize as λ =
ǫµ/(2ǫµ sup L + 2σ2), the iteration steps k which
is suﬃcient to achieve E
ǫ, is deﬁned
w⋆
k
as:

fi(w⋆)

2
2 ≤

k∇

wk

−

k

ˆwt

where ǫ0 := max0≤t≤T E
k
Proof. Using the analysis of Needell et al. (2014),
we know that for ξ, the convergence bound of SGD
is obtained as

w⋆

2
2.

−

k

k = 2 log(ǫ0/ǫ)

sup L
µ

(cid:18)

+

σ2
µ2ǫ (cid:19)

(28)

when λ = ǫµ/(2ǫµ sup L+2σ2) and the convergence
bound of ξ is further reduced from supremum de-
pendence of L to average dependence through the
application of IS, i.e.,

k = 2 log(ǫ0/ǫ)

¯L
µ

(cid:18)

+

¯L
inf L

σ2
µ2ǫ (cid:19)

(29)

With the accurate SGD term ξ bounded as Equa-
tion 29, we are left to bound the noise term δ as
an order-wise constant in order to achieve nearly
linear speedup of IS-SGD.

According to the deﬁnition of pi as shown in
Equation 12, we have (npi)−1 = ¯L
fit
is scaled with (npi)−1 in IS, M is also scaled as
¯L
Ms := (npi)−1M . Since
inf L , Ms is thus
bounded as:

Li . Since

∇

¯L
Li ≤
¯L
inf L

M

Ms

≤

(30)

With this result, from Equation 25, it can be con-
cluded that δ is bounded as an order-wise constant
when the following conditions are satisﬁed:

¯∆
n

τ

= O(1) and τ λµ = O(1)

(31)

Considering that λ is set as ǫµ/(2ǫµ sup L + 2σ2),
Equation 31 is thus satisﬁed by bounding τ as:

O

min

n/ ¯∆,

(cid:16)

n

ǫµ sup L + σ2
ǫµ2

o(cid:17)

(32)

Thus the recursion of IS-ASGD is the same with IS-
SGD plus an additional order-wise constant. We
thus have the convergence bound of IS-ASGD as
shown in Lemma 2.

k = O(1) log(ǫ0/ǫ)

¯L
µ

(cid:18)

+

¯L
inf L

σ2
µ2ǫ (cid:19)

when τ is bounded as

O

min

n/ ¯∆,

(cid:16)

n

ǫµ sup L + σ2
ǫµ2

o(cid:17)

(26)

(27)

Obviously this bound inherits the superiority of
IS-SGD over ASGD, and it shows that IS-ASGD
achieves a nearly linear speedup of IS-SGD which
is similar to the previous result in Fang and Lin
(2017) that shows SVRG-ASGD achieves nearly
linear speedup of SVRG-SGD.

8

In brief, the key to the convergence bound anal-
ysis is the serialization of the asynchrony which
divides the update scheme into two self-bounded
terms, i.e., ξ and δ. Such separation makes the
analysis much simpler, that is, IS decreases the con-
vergence bound of ξ as the same as in SGD while
the two bounded error terms caused by the asyn-
chrony, i.e., Rt
2, increase the convergence bound
up to a constant when certain conditions are met.

1, Rt

4 Experimental Results

In order to make our evaluation representative and
convincing, we conduct the evaluation based on the
following conﬁguration:
Testbed Our testbed is a 2-sockets server with In-
tel XeonE5-2699V4 CPU which has 44 cores in total
(with HyperThreading oﬀ) and 128G main mem-
ory.
Code Base We base our IS-ASGD code on well-
validated open source version of ASGD algorithm2.
We also make our evaluation code of IS-ASGD pub-
licly available3.
Datasets Evaluation datasets are from LibSVM4.
News20 (low dimensionality and relative dense)
was used for the purpose of validation in previ-
ous SVRG-ASGD works. Such small dataset can
not expose the bottlenecking performance problems
as discussed in section 1. Yet we still select this
dataset for comparison. For other three large-scale
datasets, SVRG-ASGD fails to ﬁnish training in a
reasonable time5 and thus we show no comparison
of it.

According to the empirical threshold we set for ρ
in section 2, News20 is importance-balanced while
for other datasets, we use simple random shuﬄing
for IS-ASGD.
Objective Functions We evaluate IS-ASGD
based on the most widely used objective function
in classiﬁcation problems, i.e., L1-regularized cross-
entropy loss. It has been popularly adopted from
simple linear models to neural network based mod-
els.

Concurrency 16, 32 and 44 threads are evalu-
ated.

Algorithms Despite ASGD algorithm which is
our target to accelerate, we also evaluate SGD (as
baseline) and SVRG-ASGD.

SVRG-ASGD We implement SVRG-ASGD by
strictly following the proposed algorithm in
J. Reddi et al. (2015) without the skip-µ approxi-
mation in the available public version since this ap-
proximation deteriorates the convergence rate sig-
niﬁcantly.

Metrics Two metrics: rooted mean squared error
(RMSE, objective value as the error) and error rate
(i.e., misclassiﬁcation) are evaluated and the error
rate is updated once a better result is obtained.

4.1 Iterative Convergence Rate Ac-

celeration

For iterative comparison, our expectation is that for
same epoch counts, IS-ASGD achieves lower RMSE
than ASGD and the error rate of IS-ASGD should
be roughly (not strictly) lower than ASGD since a
lower cost does not meant to be a lower error rate.
Figure 3 shows the iterative convergence results of
all four datasets in their corresponding sub-ﬁgures
respectively.
Comparing to SVRG-ASGD From Figure 3-
a, we see that SVRG-ASGD achieves the best it-
erative convergence rate with large improvement
which comes at the price of magnitudes higher iter-
ative computation cost. Meanwhile, it can also be
noticed that with the increasing of the concurrency,
the improvements of SVRG diminish quickly. This
complies to our previous analysis that SVRG suf-
fers higher potentiality of conﬂict updates due to its
loss of sparsity, which makes it more concurrency-
sensitive.
Comparing to ASGD We notice that the iter-
ative convergence metrics of ASGD are the worst.
It is worse than SGD in datasets that are relative
dense, e.g., News20 and URL, while in datasets that
are suﬃciently sparse, e.g., the KDD datasets, its
convergence rate are close to SGD. It is also clear
that IS-ASGD’s iterative convergence rate is much
better than ASGD in all cases. In fact, IS-ASGD
also achieves better optimum, i.e., a lower ﬁnal er-
ror rate and RMSE as can be seen from the results.

2http://i.stanford.edu/hazy/victor/Hogwild/
3https://github.com/FayW/IS-ASGD.git
4https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
5For KDD datasets, SVRG-ASGD takes about 2 hours

to ﬁnish 1 epoch with 44 threads.

9

0.64

5
.
0
=
λ

6
1
=
τ

E
S
M
R

0.05

0

0.21

5
.
0
=
λ

6
1
=
τ

e
t
a
R
r
o
r
r
E

0.00

0

0.06

5
0
.
0
=
λ

6
1
=
τ

E
S
M
R

0.0

0

0.2
5
0
.
0
=
λ

6
1
=
τ

e
t
a
R
r
o
r
r
E

0.0

SGD
ASGD
IS-ASGD
SVRG-ASGD

Epoch

SGD
ASGD
IS-ASGD
SVRG-ASGD

0.57

5
.
0
=
λ

2
3
=
τ

E
S
M
R

0.06

15

0

0.15

5
.
0
=
λ

2
3
=
τ

e
t
a
R
r
o
r
r
E

SGD
ASGD
IS-ASGD
SVRG-ASGD

Epoch

SGD
ASGD
IS-ASGD
SVRG-ASGD

0.61

5
.
0
=
λ

4
4
=
τ

E
S
M
R

0.05

15

0

0.18

5
.
0
=
λ

4
4
=
τ

e
t
a
R
r
o
r
r
E

SGD
ASGD
IS-ASGD
SVRG-ASGD

Epoch

15

SGD
ASGD
IS-ASGD
SVRG-ASGD

0.00

15

0

0.00

15

0

15

1.16

5
.
0
=
λ

6
1
=
τ

E
S
M
R

0.13

0

0.16

5
.
0
=
λ

6
1
=
τ

e
t
a
R
r
o
r
r
E

0.0

SGD
ASGD
IS-ASGD

Epoch

SGD
ASGD
IS-ASGD

1.12

5
.
0
=
λ

2
3
=
τ

E
S
M
R

0.13

72

0

0.16

5
.
0
=
λ

2
3
=
τ

e
t
a
R
r
o
r
r
E

SGD
ASGD
IS-ASGD

Epoch

SGD
ASGD
IS-ASGD

1.06

5
.
0
=
λ

4
4
=
τ

E
S
M
R

0.14

72

0

0.16

5
.
0
=
λ

4
4
=
τ

e
t
a
R
r
o
r
r
E

SGD
ASGD
IS-ASGD

Epoch

72

SGD
ASGD
IS-ASGD

0.0

72

0.0

70

72

Epoch

Epoch

Epoch

Epoch

Epoch

Epoch

(a) JMLR_News20, λ = 0.5

(b) KDD2010_Alg., λ = 0.5

SGD
ASGD
IS-ASGD

0.05

5
0
.
0
=
λ

2
3
=
τ

E
S
M
R

SGD
ASGD
IS-ASGD

0.07

5
0
.
0
=
λ

4
4
=
τ

E
S
M
R

0.0

0

18

0.0

0

18

Epoch

Epoch

Epoch

SGD
ASGD
IS-ASGD

0.2
5
0
.
0
=
λ

2
3
=
τ

e
t
a
R
r
o
r
r
E

0.0

SGD
ASGD
IS-ASGD

0.2
5
0
.
0
=
λ

4
4
=
τ

e
t
a
R
r
o
r
r
E

0.0

SGD
ASGD
IS-ASGD

SGD
ASGD
IS-ASGD

0.98

5
.
0
=
λ

6
1
=
τ

E
S
M
R

18

0.1

0

0.15

5
.
0
=
λ

6
1
=
τ

e
t
a
R
r
o
r
r
E

0.0

18

16

16

SGD
ASGD
IS-ASGD

Epoch

SGD
ASGD
IS-ASGD

0.99

5
.
0
=
λ

2
3
=
τ

E
S
M
R

0.1

0

72

0.15

5
.
0
=
λ

2
3
=
τ

e
t
a
R
r
o
r
r
E

SGD
ASGD
IS-ASGD

Epoch

SGD
ASGD
IS-ASGD

0.98

5
.
0
=
λ

4
4
=
τ

E
S
M
R

0.1

0

72

0.15

5
.
0
=
λ

4
4
=
τ

e
t
a
R
r
o
r
r
E

SGD
ASGD
IS-ASGD

Epoch

72

SGD
ASGD
IS-ASGD

0.0

72

0.0

72

72

Epoch

Epoch

Epoch

Epoch

Epoch

Epoch

(c) ICML_URL, λ = 0.05

(d) KDD2010_Bri., λ = 0.5

Figure 3: Iterative Convergence Result for SGD, ASGD, SVRG-ASGD and IS-ASGD

They also show diﬀerent concurrency-robustness,
for instance, in Figure 3-c, when τ = 16, ASGD
achieves close convergence rate to SGD with the
increasing of epochs. However its convergence met-
rics deteriorates quickly when τ increases to 32 and
44. Meanwhile, IS-ASGD seems non-eﬀected, it
maintains close convergence results with SGD in
all concurrencies which is a large improvement of
ASGD and shows its concurrency-robustness.

Figure 3-b and d shows similar results that IS-
ASGD achieves signiﬁcant convergence rate ac-
celerations comparing to ASGD and SGD. These
two datasets, i.e., KDD2010_Alg., and Bri. are
sparse and have extremely large dimensionality and
number of samples. They also have lower ψ, as
mentioned in Equation 15, section 2.2, the con-
vergence bound improvement of applying IS in
SGD is negatively correlated to ψ ant thus IS-

ASGD achieves much signiﬁcant convergence im-
provements in these two datasets, which is even
much better than SGD. While in Figure 3-a and
Figure 3-c where the datasets are relatively small
(potentially higher imbalance), dense and have
higher ψ, its convergence bound are close to SGD.

In fact, when conditions discussed in section 3
are satisﬁed, i.e., datasets are suﬃciently sparse,
the iterative convergence rate of IS-ASGD will be
no worse than SGD while the iterative convergence
rate of ASGD will be no better than SGD. When
datasets are even more large-scale and has lower
ψ, the convergence rate improvement of IS-ASGD
increases signiﬁcantly. We can ﬁrmly say that IS-
ASGD accelerates the iterative convergence rate of
ASGD eﬀectively due to its inherited superior con-
vergence bound from IS-SGD. Such improvements
will directly result in absolute convergence rate ac-

10

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 1: Evaluation Datasets

Name Dimension Instances
News20
URL
Algebra
Bridge

1,355,191
3,231,961
20,216,830
29,890,095

19,996
2,396,130
8,407,752
19,264,097

∇fi-Spa.
10−3
10−5
10−7
10−7

ψ
0.972
0.964
0.892
0.877

ρ
5−4
3−4
1−4
2−4

Source
JMLR
ICML
KDD
KDD

celeration of IS-ASGD since its iterative time cost
remains almost the same with ASGD, and most
importantly, it preserves the sparsity.

4.2 Absolute Convergence Rate Ac-

celeration

We present the results of absolute convergence ac-
celeration in two forms as shown in Figure 4 and
Figure 5 respectively. While the iterative conver-
gence results shown above hold a more academic
meaning, the absolute convergence rate is the met-
ric that matters for practical deployments since
people always hope to obtain a trained model with
less time. Figure 4 plots the absolute convergence
curves with the x-axis as wall-clock in seconds. Be
noted that we provide the RMSE comparison be-
tween all algorithms in the ﬁrst column while in the
second and third columns only the RMSE and error
rate comparison between ASGD and IS-ASGD are
shown for a better resolution since their curves are
very short comparing to that of SGD and SVRG-
ASGD.

As can be seen from Figure 4-a, SVRG-ASGD
takes much longer time to achieve the same accu-
racy than other algorithms despite of its superior
iterative convergence rate as shown in Figure 3-a
since its iterative computation cost is several mag-
nitudes higher than others. For the comparison
between ASGD and IS-ASGD, we speciﬁcally plot
the ﬁnal best error rate (referred to as optimum)
of ASGD in red circle while the blue dot corre-
sponds to the same optimum achieved by IS-ASGD.
This comparison directly shows the ﬁnal absolute
speedup results of IS for ASGD. We see that in
Figure 4-d, IS-ASGD achieves a maximum 1.8x ac-
celeration while in other cases its acceleration of the
optimum varies depending the datasets and concur-
rency. In Figure 4-c, it shows that IS-ASGD also
achieves better ﬁnal optimum and higher accelera-
tion of ASGD when concurrency increases.

The results also show that the optimums of error
rate are achieved much earlier than that of RM-
SEs which implies that the acceleration of the early
stage of the convergence is more important since for
later stage the error rate improvements is very lim-
ited. We thus present error-rate/speedup curves for
an in-depth inspection.
In-depth: Slice Inspection Figure 5 is de-
rived from Figure 4 directly, the two 3D-ﬁgures in
each sub-ﬁgure show the speedups of IS-ASGD over
ASGD and SGD in a slicing manner for a deeper in-
spection of the convergence procedure. Its y-axis is
the concurrency and z-axis is the absolute speedup
of reaching the corresponding error-rate (values are
linearly interpolated when needed) on x-axis.

From the speedup curves, we can see that the
speedups are the largest at the early stage and drop
in the middle. We can also conclude that the scales
of the datasets aﬀect the speedup curves in two
aspects, ﬁrst, for large-scale datasets, i.e, in Fig-
ure 5-b and d, the speedups rise at the ﬁnal stage of
the convergence procedures which implies that IS-
ASGD achieves its best acceleration performance in
large-scale datasets when searching for the optimal
models. Second, for large-scale datasets, the aver-
age speedups of IS-ASGD over ASGD seem to be
invariant to the currency as the curves show simi-
lar shape and mean, which indicates concurrency-
robustness.

As can be summarized, the average speedups of
IS-ASGD over ASGD range from 1.26 to 1.97 while
the optimum speedups range from 1.13 to 1.54. For
the raw computational speedup, it can be seen that
the speedups of IS-ASGD over SGD for 16 threads
range from 6.39 to 12.29 and increase to 11.89 to
23.53 when threads count increases to 44 depending
on the size of dataset. In general, small data size
does not achieve a good raw computation speedup.
Taking the sampling time into consideration, the
raw computational speedups of IS-ASGD are typ-
ically 7.7% to 1.1% lower than ASGD which are

11

5
.
0
=
λ

6
1
=
τ

E
S
M
R

0.04

0

0.57

5
.
0
=
λ

2
3
=
τ

E
S
M
R

0.05

0

0.61

5
.
0
=
λ

4
4
=
τ

E
S
M
R

0.04

0

0.06

5
0
.
0
=
λ

6
1
=
τ

E
S
M
R

0.0

0

0.05

5
0
.
0
=
λ

2
3
=
τ

E
S
M
R

0.0

0

0.07

5
0
.
0
=
λ

4
4
=
τ

E
S
M
R

0.0

0

0.67

0.67

0.22

SGD
ASGD
IS-ASGD
SVRG-ASGD

5
.
0
=
λ

6
1
=
τ

E
S
M
R

ASGD
IS-ASGD
SVRG-ASGD

5
.
0
=
λ

6
1
=
τ

e
t
a
R
r
o
r
r
E

ASGD
IS-ASGD
SVRG-ASGD
Optimum of ASGD
IS-ASGD achieve
 same optimum

1.16

5
.
0
=
λ

6
1
=
τ

E
S
M
R

RMSE SGD
RMSE ASGD
RMSE Ramdon IS-ASGD

1.16

5
.
0
=
λ

6
1
=
τ

E
S
M
R

ASGD
IS-ASGD
Optimum of ASGD
IS-ASGD achieve
 same optimum

ASGD
IS-ASGD

0.16

5
.
0
=
λ

6
1
=
τ

e
t
a
R
r
o
r
r
E

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

0.05

96

0

0.0

96

24

0.12

0

0.0

2903

0.0

259

215

0.57

0.15

SGD
ASGD
IS-ASGD
SVRG-ASGD

5
.
0
=
λ

2
3
=
τ

E
S
M
R

ASGD
IS-ASGD
SVRG-ASGD

5
.
0
=
λ

2
3
=
τ

e
t
a
R
r
o
r
r
E

ASGD
IS-ASGD
SVRG-ASGD
Optimum by ASGD
IS-ASGD achieve
 same optimum

1.12

5
.
0
=
λ

2
3
=
τ

E
S
M
R

RMSE SGD
RMSE ASGD
RMSE Ramdon IS-ASGD

1.12

5
.
0
=
λ

2
3
=
τ

E
S
M
R

ASGD
IS-ASGD

0.16

5
.
0
=
λ

2
3
=
τ

e
t
a
R
r
o
r
r
E

ASGD
IS-ASGD
Optimum by ASGD
IS-ASGD achieve
 same optimum

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

0.05

83

0

0.0

63

17

0.12

0

0.0

2903

0.0

159

142

0.61

0.18

SGD
ASGD
IS-ASGD
SVRG-ASGD

5
.
0
=
λ

4
4
=
τ

E
S
M
R

ASGD
IS-ASGD
SVRG-ASGD

5
.
0
=
λ

4
4
=
τ

e
t
a
R
r
o
r
r
E

ASGD
IS-ASGD
SVRG-ASGD
Optimum by ASGD
IS-ASGD achieve
 same optimum

1.06

5
.
0
=
λ

4
4
=
τ

E
S
M
R

RMSE SGD
RMSE ASGD
RMSE Ramdon IS-ASGD

1.06

5
.
0
=
λ

4
4
=
τ

E
S
M
R

ASGD
IS-ASGD

0.16

5
.
0
=
λ

4
4
=
τ

e
t
a
R
r
o
r
r
E

ASGD
IS-ASGD
Optimum by ASGD
IS-ASGD achieve
 same optimum

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

0.05

83

0

0.0

53

14

0.12

0

0.0

2903

0.0

124

110

(a) JMLR_News20, λ = 0.5

(b) KDD2010_Alg., λ = 0.5

SGD
ASGD
IS-ASGD

0.06

5
0
.
0
=
λ

6
1
=
τ

E
S
M
R

0.0

291

ASGD
IS-ASGD

0.19

5
0
.
0
=
λ

6
1
=
τ

e
t
a
R
r
o
r
r
E

0.0

45

ASGD
IS-ASGD
Optimum of ASGD
IS-ASGD achieve
 same optimum

0.98

5
.
0
=
λ

6
1
=
τ

E
S
M
R

RMSE SGD
RMSE ASGD
RMSE Ramdon IS-ASGD

0.98

5
.
0
=
λ

6
1
=
τ

E
S
M
R

0.15

ASGD
IS-ASGD

5
.
0
=
λ

6
1
=
τ

E
S
M
R

ASGD
IS-ASGD
Optimum of ASGD
IS-ASGD achieve
 same optimum

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock (s)

43

0.06

0

0.06

4237

0.0

352

340

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

SGD
ASGD
IS-ASGD

0.05

5
0
.
0
=
λ

2
3
=
τ

E
S
M
R

ASGD
IS-ASGD

0.17

5
0
.
0
=
λ

2
3
=
τ

e
t
a
R
r
o
r
r
E

ASGD
IS-ASGD
Optimum by ASGD
IS-ASGD achieve
 same optimum

0.99

5
.
0
=
λ

2
3
=
τ

E
S
M
R

RMSE SGD
RMSE ASGD
RMSE Ramdon IS-ASGD

0.98

5
.
0
=
λ

2
3
=
τ

E
S
M
R

0.14

ASGD
IS-ASGD

5
.
0
=
λ

2
3
=
τ

E
S
M
R

ASGD
IS-ASGD
Optimum by ASGD
IS-ASGD achieve
 same optimum

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock (s)

0.0

291

0.0

0

29

23

0.06

0

0.06

4237

0.0

224

214

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

SGD
ASGD
IS-ASGD

0.07

5
0
.
0
=
λ

4
4
=
τ

E
S
M
R

ASGD
IS-ASGD

0.22

5
0
.
0
=
λ

4
4
=
τ

e
t
a
R
r
o
r
r
E

ASGD
IS-ASGD
Optimum by ASGD
IS-ASGD achieve
 same optimum

0.98

5
.
0
=
λ

4
4
=
τ

E
S
M
R

RMSE SGD
RMSE ASGD
RMSE Ramdon IS-ASGD

0.98

5
.
0
=
λ

4
4
=
τ

E
S
M
R

0.15

ASGD
IS-ASGD

5
.
0
=
λ

4
4
=
τ

E
S
M
R

ASGD
IS-ASGD
Optimum by ASGD
IS-ASGD achieve
 same optimum

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

Wall-Clock Time (s)

0.0

291

0.0

0

23

20

0.07

0

0.07

4237

0.0

188

181

(c) ICML-URL, λ = 0.05

(d) KDD2010_Bri., λ = 0.5

Figure 4: Absolute Convergence Result for SGD, ASGD, SVRG-ASGD and IS-ASGD

relative small diﬀerences. If we generate the sam-
ple sequence of IS-ASGD for each thread only once
and simply shuﬄe it every epoch, there will be no
computation performance gap between ASGD and
IS-ASGD. In fact, such approximation work well in
practice according to our evaluation.

4.3 Discussion: When Datasets are

Dense

The main reason that causes SVRG performs in-
eﬃciently in large-scale sparse datasets is its re-
liance on the dense gradient µ which is magnitudes
fi. On the
larger than the stochastic gradient

∇

∇

other hand, if the datasets are dense, e.g., when
fi is higher than 10−3 which is
the sparsity of
close to µ, SVRG-ASGD prevails since their itera-
tive computation costs are in same magnitude and
SVRG-ASGD’s iterative convergence rate is much
higher. Additionally, when datasets are small-scale,
the whole training procedure tends to ﬁnish quickly.
For this case, the performance bottleneck is the
overhead of multi-process scheduling, all reduce op-
eration, etc., instead of the computation, and thus
SVRG-ASGD is likely to outperform the other al-
gorithms. Since small datasets are of only aca-
demic meanings, it seems that the proper appli-
cations for SVRG-ASGDs are the scenarios when

12

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
D
G
S
A
f
o
p
u
d
e
e
p
S
e
t
u
o
s
b
A

l

3.0

2.5

2.0

1.5

44

32

T hre a d s

16

0.020

0.015

0.010

Error Rate 0.005

D
G
S
f
o
p
u
d
e
e
p
S
e
t
u
o
s
b
A

l

13

12

11

10

9

8

7

44

32

T hre a d s

16

0.020

0.015

0.010

Error Rate 0.005

2.0

1.8

1.6

1.4

1.2

D
G
S
A
f
o
p
u
d
e
e
p
S
e
t
u
o
s
b
A

l

44

32

T hre a d s

16

D
G
S
f
o
p
u
d
e
e
p
S
e
t
u
o
s
b
A

l

40

35

30

25

20

15

44

32

T hre a d s

0.03

16

0.12

0.09

0.06

Error Rate

0.150

0.125

0.100

0.075

Error Rate

0.050

0.025

0.000

16

32

44

16

32

44

(a) JMLR_News20, λ = 0.5

(b) KDD2010_Algebra, λ = 0.5

D
G
S
A
f
o
p
u
d
e
e
p
S
e
t
u
o
s
b
A

l

3.0

2.5

2.0

1.5

1.0

44

32

T hre a d s

16

0.00

0.15

0.10

Error Rate

0.05

D
G
S
f
o
p
u
d
e
e
p
S
e
t
u
o
s
b
A

l

12

10

8

6

D
G
S
A
f
o
p
u
d
e
e
p
S
e
t
u
o
s
b
A

l

1.8

1.6

1.4

1.2

1.0

D
G
S
f
o
p
u
d
e
e
p
S
e
t
u
o
s
b
A

l

40

35

30

25

20

15

44

32

T hre a d s

0.00

16

0.15

0.10

Error Rate

0.05

0.00

16

44

32

T hre a d s

0.15

0.10

Error Rate

0.05

0.00

16

44

32

T hre a d s

0.15

0.10

Error Rate

0.05

16

32

44

16

32

44

(c) ICML_URL, λ = 0.05

(d) KDD2010_Bridge, λ = 0.5

Figure 5: ErrorRate - Speedup of IS-ASGD

using ASGD for relative dense datasets. However
for most large-scale optimizations, the datasets are
typically sparse with its sparsity signiﬁcantly lower
than 10−3.

5 Conclusion

Techniques for the acceleration of the convergence
rate of asynchronous stochastic optimizations are of
great importance and has long been a hot research
In this paper we located several unidenti-
ﬁeld.
ﬁed bottlenecking issues for current SVRG-based
ASGD acceleration algorithms in large-scale sparse
datasets and propose the novel IS-ASGD algorithm
which avoids the above bottlenecking issues nat-
urally.
Its key advantage lies in the capability
of accelerating the iterative convergence rate of
ASGD with few increasing of the iterative time
cost which in turn results in eﬀective acceleration of
the absolute convergence rate. We use importance-
balancing trick to balance the importance between
asynchronous cores/nodes which helps preserving
the optimal VR performance of IS. Moreover, we
theoretically proved the convergence bound of IS-
ASGD which shows that IS-ASGD speeds up IS-
SGD almost linearly and consequently inherits its
superior convergence bound over ASGD and SGD.
The experimental evaluation results clearly verify
that IS-ASGD achieves 1.13 1.54x absolute con-
vergence rates acceleration for ASGD. Evaluation
codes can be publicly accessed.

References

Dominik Csiba and Peter Richtárik. 2016. Impor-
tance sampling for minibatches. arXiv preprint
arXiv:1602.02283 (2016).

Aaron Defazio, Francis Bach, and Simon Lacoste-
Julien. 2014. SAGA: A Fast Incremental Gradi-
ent Method With Support for Non-Strongly Con-
vex Composite Objectives. In Advances in Neural
Information Processing Systems. 1646–1654.

Cong Fang and Zhouchen Lin. 2017. Parallel
Asynchronous Stochastic Variance Reduction for
Nonconvex Optimization. In Proceedings of the
Thirty-First AAAI Conference on Artiﬁcial In-
telligence. 794–800.

Zhouyuan Huo and Heng Huang. 2017. Asyn-
chronous Mini-Batch Gradient Descent with
Variance Reduction for Non-Convex Optimiza-
tion. In Proceedings of the Thirty-First AAAI
Conference on Artiﬁcial Intelligence, February 4-
9, 2017, San Francisco, California, USA. 2043–
2049.

Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barn-
abas Poczos, and Alex J Smola. 2015. On Vari-
ance Reduction in Stochastic Gradient Descent
and its Asynchronous Variants. In Advances in
Neural Information Processing Systems 28. Cur-
ran Associates, Inc., 2647–2655.

13

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A Lock-Free Approach with Convergence Guar-
antee. In Proceedings of the Thirtieth AAAI Con-
ference on Artiﬁcial Intelligence. 2379–2385.

Rie Johnson and Tong Zhang. 2013. Accelerat-
ing Stochastic Gradient Descent using Predictive
Variance Reduction. In Advances in Neural In-
formation Processing Systems. 315–323.

Yuanyuan Liu, Fanhua Shang, and James Cheng.
2017. Accelerated Variance Reduced Stochastic
ADMM. In Proceedings of the Thirty-First AAAI
Conference on Artiﬁcial Intelligence. 2287–2293.

Horia Mania, Xinghao Pan, Dimitris Papailiopou-
los, Benjamin Recht, Kannan Ramchandran, and
Michael I Jordan. 2017. Perturbed iterate anal-
ysis for asynchronous stochastic optimization.
SIAM Journal on Optimization 27, 4 (2017),
2202–2229.

Qi Meng, Wei Chen, Jingcheng Yu, Taifeng Wang,
Zhiming Ma, and Tie-Yan Liu. 2017. Asyn-
chronous Stochastic Proximal Optimization Al-
gorithms with Variance Reduction. In Proceed-
ings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, February 4-9, 2017, San
Francisco, California, USA. 2329–2335.

Deanna Needell, Rachel Ward, and Nati Srebro.
2014. Stochastic Gradient Descent, Weighted
Sampling, and the Randomized Kaczmarz algo-
rithm. In Advances in Neural Information Pro-
cessing Systems. Curran Associates, Inc., 1017–
1025.

Benjamin Recht, Christopher Re, Stephen Wright,
and Feng Niu. 2011. Hogwild: A Lock-Free Ap-
proach to Parallelizing Stochastic Gradient De-
scent. In Advances in Neural Information Pro-
cessing Systems. 693–701.

T.Strohmer and R. Vershynin. 2009. A random-
ized Kaczmarz algorithm with exponential con-
vergence. In The Journal of Fourier Analysis and
Applications. Vol. 2. 262–278.

Peilin Zhao and Tong Zhang. 2015. Stochastic Op-
timization with Importance Sampling for Reg-
In Proceedings of
ularized Loss Minimization.
the 32nd International Conference on Machine
Learning.

Shen-Yi Zhao and Wu-Jun Li. 2016. Fast Asyn-
chronous Parallel Stochastic Gradient Descent:

14

