2
2
0
2

b
e
F
5
1

]

C
O
.
h
t
a
m

[

1
v
2
6
2
7
0
.
2
0
2
2
:
v
i
X
r
a

Stochastic Gradient Descent-Ascent: Uniﬁed Theory
and New Eﬃcient Methods

Aleksandr Beznosikov1∗ Eduard Gorbunov1∗† Hugo Berard2∗ Nicolas Loizou3

1 Moscow Institute of Physics and Technology, Russian Federation
2 Mila, Universit´e de Montr´eal, Canada
3 Johns Hopkins University, USA

Abstract

Stochastic Gradient Descent-Ascent (SGDA) is one of the most prominent algorithms for
solving min-max optimization and variational inequalities problems (VIP) appearing in var-
ious machine learning tasks. The success of the method led to several advanced extensions
of the classical SGDA, including variants with arbitrary sampling, variance reduction, co-
ordinate randomization, and distributed variants with compression, which were extensively
studied in the literature, especially during the last few years. In this paper, we propose a
uniﬁed convergence analysis that covers a large variety of stochastic gradient descent-ascent
methods, which so far have required diﬀerent intuitions, have diﬀerent applications and
have been developed separately in various communities. A key to our uniﬁed framework is a
parametric assumption on the stochastic estimates. Via our general theoretical framework,
we either recover the sharpest known rates for the known special cases or tighten them.
Moreover, to illustrate the ﬂexibility of our approach we develop several new variants of
SGDA such as a new variance-reduced method (L-SVRGDA), new distributed methods with
compression (QSGDA, DIANA-SGDA, VR-DIANA-SGDA), and a new method with coordinate
randomization (SEGA-SGDA). Although variants of the new methods are known for solving
minimization problems, they were never considered or analyzed for solving min-max prob-
lems and VIPs. We also demonstrate the most important properties of the new methods
through extensive numerical experiments.

1

Introduction

Min-max optimization and, more generally, variational inequality problems (VIPs) appear in a
wide range of research areas including but not limited to statistics [Bach, 2019], online learning
[Cesa-Bianchi and Lugosi, 2006], game theory [Morgenstern and Von Neumann, 1953], and
machine learning [Goodfellow et al., 2014]. Motivated by applications in these areas, in this
paper, we focus on solving the following regularized VIP: Find x∗ ∈ Rd such that

(cid:104)F (x∗), x − x∗(cid:105) + R(x) − R(x∗) ≥ 0 ∀x ∈ Rd,

(1)

where F : Rd → Rd is some operator and R : Rd → R is a regularization term (a proper lower
semicontinuous convex function), which is assumed to have a simple structure. This problem
is quite general and covers a wide range of possible problem formulations. For example, when
operator F (x) is the gradient of a convex function f , then problem (1) is equivalent to the

∗Equal contribution.
†Corresponding author: eduard.gorbunov@phystech.edu.

1

 
 
 
 
 
 
composite minimization problem [Beck, 2017], i.e., minimization of f (x) + R(x). Problem (1)
is also a more abstract formulation of the min-max problem

min
x1∈Q1

max
x2∈Q2

f (x1, x2),

(2)

2 )(cid:62), F (x) = (∇x1f (x1, x2)(cid:62), −∇x2f (x1, x2)(cid:62))(cid:62),
with convex-concave f . In that case, x = (x(cid:62)
and R(x) = δQ1(x1) + δQ2(x2), where δQ(·) is an indicator function of the set Q [Alacaoglu and
Malitsky, 2021]. In addition to formulate the constraints, regularization R allows us to enforce
some properties to the solution x∗, e.g., sparsity [Candes et al., 2008, Beck, 2017].

1 , x(cid:62)

More precisely, we are interested in the situations when operator F is accessible through
the calls of unbiased stochastic oracle. This is natural when F has an expectation form F (x) =
Eξ∼D[Fξ(x)] or a ﬁnite-sum form F (x) = 1
i=1 Fi(x). In the context of machine learning, D
n
corresponds to some unknown distribution on the data, n corresponds to the number of samples,
and Fξ, Fi denote vector ﬁelds corresponding to the samples ξ, and i, respectively [Gidel et al.,
2019, Loizou et al., 2021].

(cid:80)n

One of the most popular methods for solving (1) is Stochastic Gradient Descent-Ascent1
(SGDA) [Dem’yanov and Pevnyi, 1972, Nemirovski et al., 2009]. However, besides its rich his-
tory, SGDA only recently was analyzed without using strong assumptions on the noise [Loizou
et al., 2021] such as uniformly bounded variance. In the last few years, several powerful al-
gorithmic techniques like variance reduction [Palaniappan and Bach, 2016, Yang et al., 2020]
and coordinate-wise randomization [Sadiev et al., 2021], were also combined with SGDA result-
ing in better algorithms. However these methods were analyzed under diﬀerent assumptions,
using diﬀerent analysis approaches, and required diﬀerent intuitions. Moreover, to the best of
our knowledge, fruitful directions such as communication compression for distributed versions
of SGDA or linearly converging variants of coordinate-wise methods for regularized VIPs were
never considered in the literature before.

All of these facts motivate the importance and necessity of a novel general analysis of SGDA
unifying several special cases and providing the ability to design and analyze new SGDA-like
methods ﬁlling existing gaps in the theoretical understanding of the method.

In this work, we develop such uniﬁed analysis.

1.1 Technical Preliminaries

Throughout the paper, we assume that operator F is µ-quasi-strongly monotone and (cid:96)-star-
cocoercive: there exist constants µ ≥ 0 and (cid:96) > 0 such that for all x ∈ Rd

(cid:104)F (x) − F (x∗), x − x∗(cid:105) ≥ µ(cid:107)x − x∗(cid:107)2,
(cid:107)F (x) − F (x∗)(cid:107)2 ≤ (cid:96)(cid:104)F (x) − F (x∗), x − x∗(cid:105),

(3)

(4)

where x∗ = projX ∗(x) := arg miny∈X ∗ (cid:107)y − x(cid:107) is the projection of x on the solution set X ∗ of
(1). If µ = 0, inequality (3) is known as variational stability condition Hsieh et al. [2020], which
is weaker than standard monotonicity: (cid:104)F (x) − F (y), x − y(cid:105) ≥ 0 for all x, y ∈ Rd. It is worth
mentioning that there exist examples of non-monotone operators satisfying (3) with µ > 0
[Loizou et al., 2021]. Condition (4) is a relaxation of standard cocoercivity (cid:107)F (x) − F (y)(cid:107)2 ≤
(cid:96)(cid:104)F (x) − F (y), x − y(cid:105). At this point let us highlight that it is possible for an operator F to
satisfy (4) and not be Lipschitz continuous [Loizou et al., 2021]. This emphasizes the wider

1This name is usually used in the min-max setup. Although we consider a more general problem formulation,

we keep the name SGDA to highlight the connection with min-max problems.

2

applicability of the (cid:96)-star-cocoercivity compared to (cid:96)-cocoercivity. We emphasize that in our
convergence analysis we do not assume (cid:96)-cocoercivity nor L-Lipschitzness of F .

We consider SGDA for solving (1) in its general form:

xk+1 = proxγkR(xk − γkgk),
where gk is an unbiased estimator of F (xk), γk > 0 is a stepsize at iteration k, and proxγR(x) :=
arg miny∈Rd {R(y) + (cid:107)y−x(cid:107)2/2γ} is a proximal operator deﬁned for any γ > 0 and x ∈ Rd. While
gk gives an information about operator F at step k, proximal operator is needed to take into
account regularization term R. We assume that function R is such that proxγR(x) can be easily
computed for all x ∈ Rd. This is a standard assumption satisﬁed for many practically interesting
regularizers [Beck, 2017]. By default we assume that γk ≡ γ > 0 for all k ≥ 0.

(5)

1.2 Our Contributions

Our main contributions are summarized below.

(cid:5) Uniﬁed analysis of SGDA. We propose a general assumption on the stochastic estimates
and the problem (1) (Assumption 2.1) and show that several variants of SGDA (5) satisfy this
assumption. In particular, through our approach we cover SGDA with arbitrary sampling
[Loizou et al., 2021], variance reduction, coordinate randomization, and compressed commu-
nications. Under Assumption 2.1 we derive general convergence results for quasi-strongly
monotone (Theorem 2.2) and monotone problems (Theorem 2.5).

(cid:5) Extensions of known methods and analysis. As a by-product of the generality of our
theoretical framework, we derive new results for the proximal extensions of several known
methods such as proximal SGDA-AS [Loizou et al., 2021] and proximal SGDA with coordinate
randomization [Sadiev et al., 2021]. Moreover, we close some gaps on the convergence of
known methods, e.g., we derive the ﬁrst convergence guarantees in the monotone case for
SGDA-AS [Loizou et al., 2021] and SAGA-SGDA [Palaniappan and Bach, 2016] and we obtain
the ﬁrst result on the convergence of SAGA-SGDA for (averaged star-)cocoercive operators.

(cid:5) Sharp rates for known special cases. For the known methods ﬁtting our framework
our general theorems either recover the best rates known for these methods (SGDA-AS) or
tighten them (SGDA-SAGA, Coordinate SGDA).

(cid:5) New methods. The ﬂexibility of our approach allows us to develop and analyze several
new variants of SGDA. Guided by algorithmic advances for solving minimization problems
we propose a new variance-reduced method (L-SVRGDA), new distributed methods with
compression (QSGDA, DIANA-SGDA, VR-DIANA-SGDA), and a new method with coordinate
randomization (SEGA-SGDA). We show that the proposed new methods ﬁt our theoretical
framework and, using our general theorems, we obtain tight convergence guarantees for
them. Although the analogs of these methods are known for solving minimization problems
[Hofmann et al., 2015, Kovalev et al., 2020, Alistarh et al., 2017, Mishchenko et al., 2019,
Horv´ath et al., 2019, Hanzely et al., 2018], they were never considered for solving min-
max and variational inequality problems. Therefore, by proposing and analyzing these new
methods we close several gaps in the literature on SGDA. For example, VR-DIANA-SGDA is
the ﬁrst SGDA-type linearly converging distributed stochastic method with compression and
SEGA-SGDA is the ﬁrst linearly converging coordinate method for solving regularized VIPs.

(cid:5) Numerical evaluation. In numerical experiments, we illustrate the most important prop-

erties of the new methods. The numerical results corroborate our theoretical ﬁndings.

3

1.3 Closely Related Work

Analysis of SGDA. SGDA is usually analyzed under uniformly bounded variance assumption.
That is, E[(cid:107)gk − F (xk)(cid:107)2 | xk] ≤ σ2 is typically assumed to get convergence guarantees [Ne-
mirovski et al., 2009, Mertikopoulos and Zhou, 2019, Yang et al., 2020]. This assumption rarely
holds, especially for unconstrained VIPs: it is easy to construct an example of (1) with F being
a ﬁnite sum of linear operators such that the variance is unbounded. Lin et al. [2020] provide
a convergence analysis of SGDA under a relative random noise assumption allowing to handle
some special cases not covered by uniformly bounded variance assumption. However, relative
noise is also a quite strong assumption and usually requires a special type of noise appearing in
coordinate methods2 or in the training of overparameterized models [Vaswani et al., 2019]. In
their recent work, Loizou et al. [2021] proposed a new weak condition called expected cocoerciv-
ity. This assumption ﬁts our theoretical framework (see Section 3) and does not imply strong
conditions on the variance of the stochastic estimator but it is stronger than star-cocoercivity
of operator F .

Variance reduction for VIPs. The ﬁrst variance-reduced variants of SGDA (SVRGDA and
SAGA-SGDA – analogs of SVRG [Johnson and Zhang, 2013] and SAGA [Defazio et al., 2014]) for
solving (1) with strongly monotone operator F having a ﬁnite-sum form with Lipschitz sum-
mands were proposed in Palaniappan and Bach [2016]. For two-sided PL min-max problems
without regularization Yang et al. [2020] proposed a variance-reduced version of SGDA with
alternating updates. Since the considered class of problems includes non-strongly-convex-non-
strongly-concave min-max problems, the rates from Yang et al. [2020] are inferior to Palaniappan
and Bach [2016]. There are also several works studying variance-reduced methods based on dif-
ferent methods rather than SGDA. Chavdarova et al. [2019] proposed a combination of SVRG
and Extragradient (EG) [Korpelevich, 1976] called SVRE and analyzed the method for strongly
monotone VIPs without regularization and with cocoercive summands Fi. The cocoercivity
assumption was relaxed to averaged Lipschitzness in Alacaoglu and Malitsky [2021], where the
authors proposed another variance-reduced version of EG (EG-VR) based on Loopless variant
of SVRG [Hofmann et al., 2015, Kovalev et al., 2020]. Loizou et al. [2020] studied stochas-
tic Hamiltonian gradient descent (SHGD), and propose the ﬁrst stochastic variance reduced
Hamiltonian method, named L-SVRHG, for solving stochastic bilinear games and and stochas-
tic games satisfying a “suﬃciently bilinear” condition. Moreover, Loizou et al. [2020] provided
the ﬁrst set of global non-asymptotic last-iterate convergence guarantees for a stochastic game
over a non-compact domain, in the absence of strong monotonicity assumptions.

Communication compression for VIPs. While distributed methods with compression
were extensively studied for solving minimization problems both for unbiased compression op-
erators [Alistarh et al., 2017, Wen et al., 2017, Mishchenko et al., 2019, Horv´ath et al., 2019, Li
et al., 2020, Khaled et al., 2020, Gorbunov et al., 2021b] and biased compression operators [Seide
et al., 2014, Stich et al., 2018, Karimireddy et al., 2019, Beznosikov et al., 2020a, Gorbunov et al.,
2020b, Qian et al., 2021b, Richt´arik et al., 2021], much less is known for min-max problems and
VIPs. To the best of our knowledge, the ﬁrst work on distributed methods with compression
for min-max problems is Yuan et al. [2014], where the authors proposed a distributed version
of Dual Averaging [Nesterov, 2009] with rounding and showed a convergence to the neighbor-
hood of the solution that cannot be reduced via standard tricks like increasing the batchsize
or decreasing the stepsize. More recently, Beznosikov et al. [2021b] proposed new distributed

2For example, see inequality (60) from Appendix H in the case when there is no regularization term, i.e., when

R(x) ≡ 0 and, as a result, F (x∗) = 0 for all x∗ ∈ X ∗.

4

variants of EG with unbiased/biased compression for solving (1) with (strongly) monotone and
Lipschitz operator F . Beznosikov et al. [2021b] obtained the ﬁrst linear convergence guarantees
on distributed VIPs with compressed communication.

2 Uniﬁed Analysis of SGDA

In this section, we describe our theoretical framework.

2.1 Key Assumption

We start by introducing the following parametric assumption, which is a central part of our
approach.

Assumption 2.1. We assume that for all k ≥ 0 the estimator gk from (5) is unbiased:
(cid:2)gk(cid:3) = F (xk), where Ek[·] denotes the expectation w.r.t. the randomness at iteration k.
Ek
Next, we assume that there exist non-negative constants A, B, C, D1, D1 ≥ 0, ρ ∈ (0, 1] and a
sequence of (possibly random) non-negative variables {σk}k≥0 such that for all k ≥ 0

Ek

(cid:107)gk − g∗,k(cid:107)2(cid:105)
(cid:104)
(cid:2)σ2
Ek

k+1

≤ 2A(cid:104)F (xk) − g∗,k, xk − x∗,k(cid:105) + Bσ2
k + D1,
(cid:3) ≤ 2C(cid:104)F (xk) − g∗,k, xk − x∗,k(cid:105) + (1 − ρ)σ2

k + D2,

(6)

(7)

where x∗,k = projX ∗(xk) and g∗,k = F (x∗,k).

While unbiasedness of gk is a standard assumption, inequalitites (6)-(7) are new and require
clariﬁcations. For simplicity, assume that σ2
k ≡ 0, F (x∗) = 0 for all x∗ ∈ X ∗, and focus on
(6). In this case, (6) gives an upper bound for the second moment of the stochastic estimate
gk. For example, such a bound follows from expected cocoercivity assumption [Loizou et al.,
2021], where A denotes some expected/averaged (star-)cocoercivity constant and D1 stands for
the variance at the solution (see also Section 3). When F is not necessary zero on X ∗, the
shift g∗,k helps to take this fact into account. Finally, the sequence {σ2
k}k≥0 is typically needed
to capture the variance reduction process, parameter B is typically some numerical constant,
C is another constant related to (star-)cocoercivity, and D2 is the remaining noise that is not
handled by variance reduction process. As we show in the next sections, inequalitites (6)-(7)
hold for various SGDA-type methods.

We point out that Assumption 2.1 is inspired by similar assumptions appeared in Gorbunov
et al. [2020a, 2021a]. However, the diﬀerence between our assumption and the ones appeared
in these papers is signiﬁcant: Gorbunov et al. [2020a] focuses only on solving minimization
problems and as a result, their assumption includes a much simpler quantity (function sub-
optimality), instead of the (cid:104)F (xk) − g∗,k, xk − x∗,k(cid:105), in the right-hand sides of (6)-(7). The
assumption proposed in Gorbunov et al. [2021a] is closer to ours, but it is designed speciﬁcally
for analyzing Stochastic EG, it does not have {σ2
k}k≥0 sequence, and works only for (1) with
R(x) ≡ 0.

2.2 Quasi-Strongly Monotone Case

Under Assumption 2.1 and quasi-strong monotonicity of F , we derive the following general
result.

5

Theorem 2.2. Let F be µ-quasi-strongly monotone with µ > 0 and let Assumption 2.1 hold.
Assume that 0 < γ ≤ min {1/µ, 1/2(A+CM )} for somea M > B/ρ. Then the iterates of SGDA,
given by (5), satisfy:

E[Vk] ≤

(cid:18)

(cid:26)

1 − min

γµ, ρ −

(cid:27)(cid:19)k

B
M

V0 +

γ2(D1 + M D2)
min {γµ, ρ − B/M}

.

(8)

where the Lyapunov function Vk is deﬁned by Vk = (cid:107)xk − x∗,k(cid:107)2 + M γ2σ2

k for all k ≥ 0.

aWhen B = 0, we suppose M = 0 and B/M := 0 in all following expressions.

The above theorem states that SGDA (5) converges linearly to the neighborhood of the
solution. The size of the neighborhood is proportional to the noises D1 and D2. When D1 =
D2 = 0, i.e., the method is variance reduced, it converges linearly to the exact solution in
expectation. However, in general, to achieve any predeﬁned accuracy, one needs to reduce the
size of the neighborhood somehow. One possible way to that is use a proper stepsize schedule.
We formalize this discussion in the following result.

Corollary 2.3. Let the assumptions of Theorem 2.2 hold. Consider two possible cases.

1. Let D1 = D2 = 0. Then, for any K ≥ 0, M = 2B/ρ, and γ = min {1/µ, 1/2(A+2BC/ρ)}, the

iterates of SGDA, given by (5), satisfy:

E[VK] ≤ V0 exp

(cid:18)

(cid:26)

− min

µ
2(A + 2BC/ρ)

,

ρ
2

(cid:27)

(cid:19)

K

.

2. Let D1 + M D2 > 0. Then, for any K ≥ 0 and M = 2B/ρ one can choose {γk}k≥0 as

follows:

if K ≤

h
µ

,

γk =

and k < k0,

γk =

1
h
1
h

,

,

and k ≥ k0,

γk =

2
µ(κ + k − k0)

,

if K >

if K >

h
µ
h
µ

(9)

where h = max {2(A + 2BC/ρ), 2µ/ρ}, κ = 2h/µ and k0 = (cid:100)K/2(cid:101). For this choice of γk, the
iterates of SGDA, given by (5), satisfy:

E[VK] ≤

32hV0
µ

exp

(cid:16)

−

µ
h

(cid:17)

K

+

36(D1 + 2BD2/ρ)
µ2K

.

2.3 Monotone Case

When µ = 0, we additionally assume that F is monotone. Similar to minimization, in the
case of µ = 0, the squared distance to the solution is not a valid measure of convergence. To
introduce an appropriate convergence measure, we make the following assumption.

Assumption 2.4. There exists a compact convex set C (with the diameter ΩC := maxx,y∈C (cid:107)x−
y(cid:107)) such that X ∗ ⊂ C.

In this settings, we focus on the following quantity called a restricted gap-function [Nesterov,

6

2007] deﬁned for any z ∈ Rd and any C ⊂ Rd satisfying Assumption 2.4:

GapC(z) := max
u∈C

[(cid:104)F (u), z − u(cid:105) + R(z) − R(u)] .

(10)

Assumption 2.4 and function GapC(z) are standard for the convergence analysis of methods
for solving (1) with monotone F [Nesterov, 2007, Alacaoglu and Malitsky, 2021]. Additional
discussion is left to Appendix D.2.

Under these assumptions, Assumption 2.1, and star-cocoercivity of F we derive the following

general result.

Theorem 2.5. Let F be monotone, (cid:96)-star-cocoercive and let Assumptions 2.1, 2.4 hold.
Assume that 0 < γ ≤ 1/2(A+BC/ρ). Then for all K ≥ 0 the iterates of SGDA, given by (5),
satisfy:

(cid:34)

E

GapC

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

3 (cid:2)maxu∈C (cid:107)x0 − u(cid:107)2(cid:3)
2γK

+

8γ(cid:96)2Ω2
C
K

+ (4A + (cid:96) + 8BC/ρ) ·

(cid:107)x0 − x∗,0(cid:107)2
K

+ (4 + (4A + (cid:96) + 8BC/ρ) γ)

γBσ2
0
ρK

+γ(2 + γ (4A + (cid:96) + 8BC/ρ))(D1 + 2BD2/ρ)
+9γ max
x∗∈X ∗

(cid:107)F (x∗)(cid:107)2,

(11)

where GapC() is the restricted gap-function from (10).

The above result establishes O(1/K) rate of convergence to the accuracy proportional to the
stepsize γ multiplied by the noise term D1 + 2BD2/ρ and maxx∗∈X ∗ (cid:107)F (x∗)(cid:107)2. We notice that
if R ≡ 0 in (1), then F (x∗) = 0, meaning that in this case, the last term from (11) equals
zero. Otherwise, even in the deterministic case one needs to use small stepsizes to ensure the
convergence to any predeﬁned accuracy (see Corollary D.4 in Appendix D.2).

3 SGDA with Arbitrary Sampling

We start our consideration of special cases with a standard SGDA (5) with gk = Fξk (xk), ξk ∼ D
under so-called expected cocoercivity assumption from Loizou et al. [2021], which we properly
adjust to the setting of regularized VIPs.

Assumption 3.1 (Expected Cocoercivity). We assume that stochastic operator Fξ(x), ξ ∼ D
is such that for all x ∈ Rd

ED

(cid:2)(cid:107)Fξ(x) − Fξ(x∗)(cid:107)2(cid:3) ≤ (cid:96)D(cid:104)F (x) − F (x∗), x − x∗(cid:105),

(12)

where x∗ = projX ∗(x).

When R(x) ≡ 0, this assumption recovers the original one from Loizou et al. [2021]. We

also emphasize that for operator F Assumption 3.1 implies only star-cocoercivity.

Following Loizou et al. [2021], we mainly focus on ﬁnite-sum case and its stochastic refor-
mulation: we consider a random sampling vector ξ = (ξ1, . . . , ξn)(cid:62) ∈ Rn having a distribution

7

D such that ED[ξi] = 1 for all i ∈ [n]. Using this we can rewrite F (x) = 1
n

(cid:80)n

i=1 Fi(x) as

F (x) =

1
n

n
(cid:88)

i=1

ED[ξiFi(x)] = ED [Fξ(x)] ,

(13)

(cid:80)n

where Fξ(x) = 1
i=1 ξiFi(x). Such a reformulation allows to handle a wide range of samplings:
n
the only assumption on D is ED[ξi] = 1 for all i ∈ [n]. Therefore, this setup is often referred to
as arbitrary sampling [Richt´arik and Tak´ac, 2020, Loizou and Richt´arik, 2020a,b, Gower et al.,
2019, 2021, Hanzely and Richt´arik, 2019, Qian et al., 2019, 2021a]. We elaborate on several
special cases in Appendix E.4.

In this setting, SGDA with Arbitrary Sampling (SGDA-AS)3 ﬁts our framework.

Proposition 3.2. Let Assumption 3.1 hold. Then, SGDA-AS satisﬁes Assumption 2.1 with
A = (cid:96)D, D1 = 2σ2
k ≡ 0, C = 0, ρ = 1,
D2 = 0.

(cid:2)(cid:107)Fξ(x∗) − F (x∗)(cid:107)2(cid:3), B = 0, σ2

∗ := 2 maxx∗∈X ∗ ED

Plugging these parameters to Theorem 2.2 we recover the result4 from Loizou et al. [2021]
when R(x) ≡ 0 and generalize it to the case of R(x) (cid:54)≡ 0 without sacriﬁcing the rate. Applying
Corollary 2.3, we establish the rate of convergence to the exact solution.

Corollary 3.3. Let F be µ-quasi-strongly monotone and Assumption 3.1 hold. Then for all
K > 0 there exists a choice of γ (see (47)) for which the iterates of SGDA-AS, satisfy:

E[(cid:107)xK − x∗,K(cid:107)2] = O

(cid:18) (cid:96)DΩ2
0
µ

(cid:18)

exp

−

(cid:19)

K

+

(cid:19)

,

σ2
∗
µ2K

µ
(cid:96)D

where Ω2

0 = (cid:107)x0 − x∗,0(cid:107)2.

For the diﬀerent stepsize schedule, Loizou et al. [2021] derive the convergence rate O(1/K + 1/K2)
which is inferior to our rate, especially when σ2
∗ is small. In addition, Loizou et al. [2021] consider
explicitly only uniform minibatch sampling without replacement as a special case of arbitrary
sampling. In Appendix E.4, we discuss another prominent sampling strategy called importance
sampling. In Section 6, we provide numerical experiments verifying our theoretical ﬁndings and
showing the beneﬁts of importance sampling over uniform sampling for SGDA.

4 SGDA with Variance Reduction

(cid:80)n

In this section, we focus on variance reduced variants of SGDA for solving ﬁnite-sum prob-
lems F (x) = 1
i=1 Fi(x). We start with the Loopless Stochastic Variance Reduced Gradient
n
Descent-Ascent (L-SVRGDA), which is a generalization of the L-SVRG algorithm proposed in
Hofmann et al. [2015], Kovalev et al. [2020]. L-SVRGDA (see Alg. 2) follows the update rule (5)
with

gk = Fjk (xk) − Fjk (wk) + F (wk),
xk, with prob. p,
wk, with prob. 1 − p,

(cid:40)

wk+1 =

(14)

(15)

3For the pseudo-code of SGDA-AS see Algorithm 1 in Appendix E.
4In the main part of the paper, we focus on µ-quasi strongly monotone case with µ > 0. For simplicity, we
provide here the rates of convergence to the exact solution. Further details, including the rates in monotone case,
are left to the Appendix.

8

where in kth iteration jk is sampled uniformly at random from [n]. Here full operator F is
computed once wk is updated, which happens with probability p. Typically, p is chosen as
p ∼ 1/n ensuring that the expected cost of 1 iteration equals O(1) oracle calls, i.e., computations
of Fi(x) for some i ∈ [n].

We introduce the following assumption about operators Fi.

Assumption 4.1 (Averaged Star-Cocoercivity). We assume that there exists a constant (cid:98)(cid:96) > 0
such that for all x ∈ Rd

1
n

n
(cid:88)

i=1

where x∗ = projX ∗(x).

(cid:107)Fi(x) − Fi(x∗)(cid:107)2 ≤ (cid:98)(cid:96)(cid:104)F (x) − F (x∗), x − x∗(cid:105),

(16)

For example, if Fi is (cid:96)i-cocoercive for i ∈ [n], then (16) holds with (cid:98)(cid:96) ≤ maxi∈[n] (cid:96)i. Next, if
Fi is Li-Lipschitz for all i ∈ [n] and F is µ-quasi strongly monotone, then (16) is satisﬁed for
(cid:98)(cid:96) ∈ [L, L

2
2/µ], where L

(cid:80)n

i=1 L2
i .

= 1
n

Moreover, for the analysis of variance reduced variants of SGDA we also use uniqueness of

the solution.

Assumption 4.2 (Unique Solution). We assume that the solution set X ∗ of problem (1) is
a singleton: X ∗ = {x∗}.

These assumptions are suﬃcient to derive validity of Assumption 2.1 for L-SVRGDA estima-

tor.

Proposition 4.3. Let Assumptions 4.1 and 4.2 hold. Then, L-SVRGDA satisﬁes Assump-
i=1 (cid:107)Fi(wk) − Fi(x∗)(cid:107)2, C = p(cid:98)(cid:96)/2, ρ = p, D1 = D2 = 0.
tion 2.1 with A = (cid:98)(cid:96), B = 2, σ2

(cid:80)n

k = 1
n

Plugging these parameters in our general results on the convergence of SGDA-type algorithms
we derive the convergence results for L-SVRGDA, see Table 1 and Appendix F.1 for the details.
Moreover, in Appendix F.2, we show that SAGA-SGDA [Palaniappan and Bach, 2016] ﬁts our
framework and using our general analysis we tighten the convergence rates for this method.

We compare our convergence guarantees with known results in Table 1. We note that
by neglecting importance sampling scenario, in the worst case, our convergence results match
the best-known results for SGDA-type methods, i.e., ones derived in Palaniappan and Bach
2/µ]. Next, when the diﬀerence between (cid:96) and (cid:98)(cid:96) is not
[2016]. Indeed, this follows from (cid:98)(cid:96) ∈ [L, L
signiﬁcant, our complexity results match the one derived in Chavdarova et al. [2019] for SVRE,
which is EG-type method. Although in general, (cid:96) might be smaller than (cid:98)(cid:96), our analysis does
not require cocoercivity of each Fi and it works for R(x) (cid:54)≡ 0. Finally, Alacaoglu and Malitsky
2/µ2)), but their method is based on EG. Therefore,
[2021] derive a better rate (when n = O(L
our results match the best-known ones in the literature on SGDA-type methods.

5 Distributed SGDA with Compression

In this section, we consider the distributed version of (1), i.e., we assume that F (x) = 1
n
where {Fi}n
i=1 are distributed across n devices connected with parameter-server in a centralized
fashion. Each device i has an access to the computation of the unbiased estimate of Fi at the
given point. Typically, in these settings, the communication is a bottleneck, especially when n

i=1 Fi(x),

(cid:80)n

9

Table 1: Summary of the complexity results for variance reduced methods for solving (1). By complexity
we mean the number of oracle calls required for the method to ﬁnd x such that E[(cid:107)x − x∗(cid:107)2] ≤ ε.
Dependencies on numerical and logarithmic factors are hidden. By default, operator F is assumed
to be µ-strongly monotone and, as the result, the solution is unique. Our results rely on µ-quasi
strong monotonicity of F (3), but we also assume uniqueness of the solution. Methods supporting
R(x) (cid:54)≡ 0 are highlighted with ∗. Our results are highlighted in green. Notation: (cid:96), L = averaged
2
cocoercivity/Lipschitz constants depending on the sampling strategy, e.g., for uniform sampling (cid:96)
=
1
i=1 L2
i=1 Li; (cid:98)(cid:96) = averaged
n
star-cocoercivity constant from Assumption 4.1.

i and for importance sampling (cid:96) = 1
n

i=1 (cid:96)i, L = 1
n

2
i , L

i=1 (cid:96)2

= 1
n

(cid:80)n

(cid:80)n

(cid:80)n

(cid:80)n

Method

SVRE (1)
(1)
EG-VR ∗

SVRGDA ∗
SAGA-SGDA ∗
VR-AGDA

L-SVRGDA ∗
SAGA-SGDA ∗

Citation

[Chavdarova et al., 2019]

[Alacaoglu and Malitsky, 2021]

[Palaniappan and Bach, 2016]

[Palaniappan and Bach, 2016]

[Yang et al., 2020]

This paper

This paper

Assumptions

Fi is (cid:96)i-cocoer.
Fi is Li-Lip.

Fi is Li-Lip.
Fi is Li-Lip.
Fi is Lmax-Lip.(2) min

As. 4.1

As. 4.1

Complexity
n + (cid:96)
µ
√
n L
n +
µ
n + L2
µ2
n + L2
µ2
µ9 , n2/3 L3
n + (cid:98)(cid:96)
µ
n + (cid:98)(cid:96)
µ

max

(cid:110)
n + L9

(cid:111)

max
µ3

(1) The method is based on Extragradient update rule.
(2) Yang et al. [2020] consider saddle point problems satisfying so-called two-sided P(cid:32)L condition,
which is weaker than strong-convexity-strong-concavity of the objective function.

and d are huge. This means that in the naive distributed implementations of SGDA, communi-
cation rounds take much more time than local computations on the clients. Various approaches
are used to circumvent this issue.

One of them is based on the usage of compressed communications. We focus on the unbiased

compression operators.

Deﬁnition 5.1. Operator Q : Rd → Rd (possibly randomized) is called unbiased compres-
sor/quantization if there exists a constant ω ≥ 1 such that for all x ∈ Rd

E[Q(x)] = x, E[(cid:107)Q(x) − x(cid:107)2] ≤ ω(cid:107)x(cid:107)2.

(17)

In this paper, we consider compressed communications in the direction from clients to the
server. The simplest method with compression – QSGDA (Alg. 4) – can be described as SGDA
(5) with gk = 1
i are stochastic estimators satisfying the following assump-
n
tion5.

i ). Here gk

i=1 Q(gk

(cid:80)n

Assumption 5.2 (Bounded variance). All stochastic realizations gk
bounded variance, i.e., for all i ∈ [n] and k ≥ 0 the following holds:

i are unbiased and have

E[gk

i ] = Fi(xk), E[(cid:107)gk

i − Fi(xk)(cid:107)2] ≤ σ2
i .

(18)

Despite its simplicity, QSGDA was never considered in the literature on solving min-max
problems and VIPs. It turns out that under such assumptions QSGDA satisﬁes our Assumption
2.1.

5We use this assumption for illustrating the ﬂexibility of the framework. It is possible to consider Arbitrary

Sampling setup as well.

10

Proposition 5.3. Let F be (cid:96)-star-cocoercive and Assumptions 4.1, 5.2 hold. Then, QSGDA
satisﬁes Assumption 2.1 with A = 3(cid:96)
, C = 0,
ρ = 1, D2 = 0, where σ2 = 1
i=1 σ2
n

k ≡ 0, D1 = 3(1+3ω)σ2+9ωζ2

2 + 9ω(cid:98)(cid:96)
∗ := 1
i , ζ 2

n maxx∗∈X ∗ (cid:80)n

2n , B = 0, σ2

i=1 (cid:107)Fi(x∗)(cid:107)2.

(cid:80)n

n

∗

As for the other special cases, we derive the convergence results for QSGDA using our general
theorems (see Table 2 and Appendix G.1 for the details). The proposed method is simple, but
have a signiﬁcant drawback: even in the deterministic case (σ = 0), QSGDA does not converge
linearly unless ζ 2
∗ = 0. However, when the data on clients is arbitrary heterogeneous the
dissimilarity measure ζ 2

∗ is strictly positive and can be large (even when R(x) ≡ 0).

To resolve this issue, we propose a more advanced scheme based on DIANA update [Mishchenko

et al., 2019, Horv´ath et al., 2019] – DIANA-SGDA (Alg. 5). In a nutshell, DIANA-SGDA is SGDA
(5) with gk deﬁned as follows:

∆k

i = gk

hk+1
i = hk

i + αQ(∆k

i ),

i − hk
i ,
1
n

n
(cid:88)

i=1

gk = hk +

Q(∆k

i ),

hk+1 =

1
n

n
(cid:88)

i=1

hk+1
i = hk + α

1
n

n
(cid:88)

i=1

Q(∆k

i ),

(19)

(20)

(21)

where the ﬁrst two lines correspond to the local computations on the clients and the last two
lines – to the server-side computations. Taking into account the update rule for hk+1, one can
notice that DIANA-SGDA requires workers to send only vectors Q(∆k
i ) to the server at step k,
i.e., the method uses only compressed workers-server communications.

As we show next, DIANA-SGDA ﬁts our framework.

Proposition 5.4. Let Assumptions 4.1, 4.2, 5.2 hold. Suppose that α ≤ 1/(1+ω). Then,
i −Fi(x∗)(cid:107)2
DIANA-SGDA with quantization (17) satisﬁes Assumption 2.1 with σ2
and A = (cid:0) 1
n , D1 = (1+ω)σ2
(cid:80)n
i=1 σ2
i .

i=1 (cid:107)hk
2 , ρ = α, D2 = ασ2, , where σ2 = 1

(cid:98)(cid:96), B = 2ω

, C = α(cid:98)(cid:96)

k = 1
n

2 + ω

(cid:80)n

(cid:1)

n

n

n

DIANA-SGDA can be considered as a variance-reduced method, since it reduces the term
proportional to ωζ 2
∗ that the bound for QSGDA contains (see Table 2 and Appendix G.2 for
the details). As the result, when σ = 0, i.e., workers compute Fi(x) at each step, DIANA-SGDA
enjoys linear convergence to the exact solution.

Next, when local operators Fi have a ﬁnite-sum form Fi(x) = 1
m

(cid:80)m

j=1 Fij(x), one can

combine L-SVRGDA and DIANA-SGDA as follows: consider the scheme from (19)-(21) with

i = Fijk (xk) − Fijk (wk) + F (wk
gk
xk, with prob. p,
wk

i , with prob. 1 − p,

(cid:40)

=

i ),

wk+1
i

(22)

(23)

where jk is sampled uniformly at random from [n]. We call the resulting method VR-DIANA-
SGDA (Alg. 6) and we note that its analog for solving minimization problems (VR-DIANA) was
proposed and analyzed in Horv´ath et al. [2019].

To cast VR-DIANA-SGDA as special case of our general framework, we need to make the

following assumption.

11

(cid:80)n

Table 2: Summary of the complexity results for distributed methods with unbiased compression for
solving distributed (1) with F = 1
i=1 Fi(x). By complexity we mean the number of communication
n
rounds required for the method to ﬁnd x such that E[(cid:107)x − x∗(cid:107)2] ≤ ε. Dependencies on numerical and
logarithmic factors are hidden. Dependencies on numerical and logarithmic factors are hidden. E stands
for the setup, when Fi(x) = Eξi[Fξi(x)]; Σ denotes the case, when Fi(x) = 1
j=1 Fij(x). By default,
m
operator F is assumed to be µ-strongly monotone and, as the result, the solution is unique. Our
results rely on µ-quasi strong monotonicity of F (3), but we also assume uniqueness of the solution.
Methods supporting R(x) (cid:54)≡ 0 are highlighted with ∗. Our results are highlighted in green. Notation:
σ2 = 1
i – averaged upper bound for the variance (see Assumption 5.2 for the deﬁnition of σ2
i );
n
ω = quantization parameter (see Deﬁnition 5.1); ζ 2
[n] Li;
n maxx
∗
(cid:101)(cid:96) = averaged star-cocoercivity constant from Assumption 5.5.

i=1 (cid:107)Fi(x∗)(cid:107)2; Lmax = maxi

X ∗ (cid:80)n

i=1 σ2

= 1

(cid:80)m

(cid:80)n

∗∈

∈

Setup

Method

Citation

Assumptions

E

Σ

QSGDA ∗

DIANA-SGDA ∗

This paper

This paper

As. 4.1, 5.2

As. 4.1, 5.2

MASHA1 ∗(1)

[Beznosikov et al., 2021b]

Fi is Li-Avg. Lip.(2)

VR-DIANA-SGDA ∗

This paper

As. 4.1, 5.5

(cid:96)

µ + ω (cid:98)(cid:96)
ω + (cid:96)

Complexity
nµ + (1+ω)σ2+ωζ2
nµ + (1+ω)σ2
µ + ω (cid:98)(cid:96)
nµ2ε
(m+ω)(1+ ω
n )
Lmax

nµ2ε

(cid:113)

∗

m + ω +

µ

m + ω + (cid:96)

µ + (1+ω)((cid:98)(cid:96)+(cid:101)(cid:96))

nµ
+ (1+ω) max{m,ω}(cid:101)(cid:96)
nmµ

(1) The method is based on Extragradient update rule.
(2) This means that for all x, y ∈ Rd and i ∈ [n] the following inequality holds:
Fij(y)(cid:107)2 ≤ L2

i (cid:107)x − y(cid:107)2.

1
m

(cid:80)m

j=1 (cid:107)Fij(x) −

Assumption 5.5. We assume that there exists a constant (cid:101)(cid:96) > 0 such that for all x ∈ Rd

1
nm

n
(cid:88)

m
(cid:88)

i=1

j=1

where x∗ = projX ∗(x).

(cid:107)Fij(x) − Fij(x∗)(cid:107)2 ≤ (cid:101)(cid:96)(cid:104)F (x) − F (x∗), x − x∗(cid:105),

(24)

Using Assumption 5.5 in combination with previously introduced conditions, we get the

following result.

(cid:111)

Proposition 5.6. Let F be (cid:96)-star-cocoercive and Assumptions 4.1, 4.2, 5.5 hold. Suppose
that α ≤ min
2 +
n + ω((cid:98)(cid:96)+(cid:101)(cid:96))
(cid:101)(cid:96)
i ) − Fij(x∗)(cid:107)2,
C = p(cid:101)l

. Then, VR-DIANA-SGDA satisﬁes Assumption 2.1 with A = (cid:96)

(cid:110) p
1
3 ,
1+ω
, B = 2(ω+1)

i − Fi(x∗)(cid:107)2 + 1
nm

2 + α((cid:101)(cid:96) + (cid:98)(cid:96)), ρ = α, D1 = D2 = 0.

j=1 (cid:107)Fij(wk

i=1 (cid:107)hk

k = 1
n

(cid:80)m

, σ2

(cid:80)n

(cid:80)n

i=1

n

n

Since D1 = D2 = 0, our general results imply linear convergence of VR-DIANA-SGDA when
µ > 0 (see the details in Appendix G.3). That is, VR-DIANA-SGDA is the ﬁrst linearly converging
distributed SGDA-type method with compression. We compare it with MASHA1 [Beznosikov
et al., 2021b] in Table 2. Firstly, let us note that MASHA1 is a method based on EG, and
its convergence guarantees depend on the Lipschitz constants. In addition, we note that the
complexity of MASHA1 could be better than the one of VR-DIANA-SGDA when cocoercivity
constants are large compared to Lipschitz ones. However, our compleixty bound has better
dependency on quantization parameter ω, number of clients n, and the size of the local dataset
m. These parameters can be large meaning that the improvement is noticeable.

12

6 Numerical Experiments

To illustrate our theoretical results, we conduct several numerical experiments on quadratic
games, which are deﬁned through the aﬃne operator:

F (x) =

1
n

n
(cid:88)

i=1

Aix + bi,

(25)

where each matrix Ai ∈ Rd×d is non-symmetric with all eigenvalues having strictly positive real
part. Enforcing all the eigenvalues to have strictly positive real part ensures that the operator
is strongly monotone and cocoercive. We consider two diﬀerent settings: (i) problem without
constraints, and (ii) problem that has (cid:96)1 regularization and constraints forcing the solution to
lie in the (cid:96)∞-ball of radius r. In all experiments, we use a constant step-size for all methods
which was selected manually using a grid-search and picking the best performing step-size for
each methods. For further details about the experiments see Appendix B.

Uniform sampling (US) vs Important sampling (IS). We note that Loizou et al. [2021]
which studies SGDA-AS does not consider IS explicitly. Although we show the theoretical
beneﬁts of IS in comparison to US in Appendix E.4, here we provide a numerical comparison to
illustrate the superiority of IS (on both constrained and unconstrained quadratic games). We
choose the matrices Ai such that (cid:96)max = maxi (cid:96)i (cid:29) ¯(cid:96). In this case, our theory predicts that IS
should perform better than US. We provide the results in Fig. 1. We observe that indeed SGDA
with IS converges faster and to a smaller neighborhood than SGDA with US. This observation
perfectly corroborates our theory.

Figure 1: Uniform sampling (US) vs Importance sampling (IS). Left: Without constraint. Right: With
constraints. As expected by theory IS converges faster and to a smaller neighborhood than US.

Comparison of variance reduced methods.
In this experiment, we test the performance
of our proposed L-SVRGDA (Alg. 2) and compare it to other variance reduced methods on
In particular, we compare it to SVRG [Palaniappan and Bach,
quadratic games, see Fig. 2.
2016], SVRE [Chavdarova et al., 2019], EG-VR [Alacaoglu and Malitsky, 2021] and VR-AGDA
In the constrained setting, we only compare L-SVRGDA to SVRG and
[Yang et al., 2020].
EG-VR, since they are the only methods from this list that handle constrained settings. For
loopless variants we choose p = 1
n and for the non-loopless variants we pick the number of inner-
loop iteration to be n. We observe that all methods converge linearly and that L-SVRGDA is
competitive with the other considered variance-reduced methods, converging slightly faster than
all of them.

13

050100150200250300Numberoforaclescall10−310−210−1100101102103104DistancetooptimalitySGDAwithISSGDA0255075100125150175200Numberoforaclescall10−310−210−1100DistancetooptimalitySGDAwithISSGDAFigure 2: Comparison of variance reduced methods Left: Without constraints. Right: With con-
straints. We observe that L-SVRGDA is very competitive, and outperforms all the other methods.

(cid:80)n

i=1 Fi(x) with each {Fi}n

Distributed setting.
In our last experiment, we consider a distributed version of the quadratic
game, in which we assume that F (x) = 1
i=1 having similar form to
n
(25). The information about operator Fi is stored on node i only. We compare the distributed
methods proposed in the paper: QSGDA, DIANA-SGDA, and VR-DIANA-SGDA. For the quan-
tization we use the RandK sparsiﬁcation [Beznosikov et al., 2020a] with K = 5. We show our
ﬁndings in Fig.3, where the performance is measured both in terms of number of oracle calls
and the number of bits communicated from workers to the server. In Fig.3 we can clearly see
the advantage of using quantization in terms of reducing the communication cost compared to
the baseline SGDA. We also observe that VR-DIANA-SGDA achieves linear convergence to the
solution. However, DIANA-SGDA performs similarly to QSGDA since the noise σ2 is larger than
the dissimilarity constant ζ 2
∗ . To illustrate further the diﬀerence between DIANA-SGDA and
QSGDA, we conduct additional experiments with full-batched methods (σ = 0) in Appendix B.

Figure 3: Comparison of algorithms in distributed setting Left: Number of oracle calls. Right: Number
of bits communicated.

7 Conclusion and Future Work

This paper develops a uniﬁed approach to analyzing and designing a wide class of SGDA-type
methods for regularized VIPs (1). In our work, we focus on analyzing such methods for solving
star-cocoercive quasi-strongly monotone and monotone problems. We believe that our work
could open up many avenues for further development and research. For example, it would be
interesting to extend our uniﬁed approach to problems that are neither quasi-strongly monotone
nor monotone. We conjecture that similar ideas to the ones presented in this work can be used

14

0200040006000800010000Numberoforaclescall10−610−410−2100DistancetooptimalityL-SVRGDASVRGEG-VRVR-AGDASVRE0200040006000800010000Numberoforaclescall10−710−510−310−1DistancetooptimalityL-SVRGDASVRGEG-VR050001000015000200002500030000Numberoforaclescall10−510−410−310−210−1100DistancetooptimalitySGDAQSGDADIANA-SGDAVR-DIANA-SGDA103104105106107108Numberofbitscommunicated10−1010−810−610−410−2100DistancetooptimalitySGDAQSGDADIANA-SGDAVR-DIANA-SGDAfor analyzing methods that do not ﬁt scheme (5), e.g., Stochastic Extragradient and Stochastic
Optimistic Gradient.

References

A. Alacaoglu and Y. Malitsky. Stochastic variance reduction for variational inequality methods. arXiv

preprint arXiv:2102.08352, 2021.

A. Alacaoglu, Y. Malitsky, and V. Cevher. Forward-reﬂected-backward method with variance reduction.

Computational optimization and applications, 80(2):321–346, 2021.

D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. Qsgd: Communication-eﬃcient sgd via
gradient quantization and encoding. Advances in Neural Information Processing Systems, 30:1709–
1720, 2017.

W. Azizian, F. Iutzeler, J. Malick, and P. Mertikopoulos. The last-iterate convergence rate of optimistic
mirror descent in stochastic variational inequalities. In Conference on Learning Theory, pages 326–358.
PMLR, 2021.

F. Bach. The “η-trick” or the eﬀectiveness of reweighted least-squares, 2019.

H. H. Bauschke, P. L. Combettes, et al. Convex analysis and monotone operator theory in Hilbert spaces,

volume 408. Springer, 2011.

A. Beck. First-order methods in optimization. Society for Industrial and Applied Mathematics (SIAM),

2017.

A. Beznosikov, S. Horv´ath, P. Richt´arik, and M. Safaryan. On biased compression for distributed

learning. arXiv preprint arXiv:2002.12410, 2020a.

A. Beznosikov, A. Sadiev, and A. Gasnikov. Gradient-free methods with inexact oracle for convex-
concave stochastic saddle-point problem. In International Conference on Mathematical Optimization
Theory and Operations Research, pages 105–119. Springer, 2020b.

A. Beznosikov, V. Samokhin, and A. Gasnikov. Distributed saddle-point problems: Lower bounds,

optimal algorithms and federated gans. arXiv preprint arXiv:2010.13112, 2020c.

A. Beznosikov, V. Novitskii, and A. Gasnikov. One-point gradient-free methods for smooth and non-
In International Conference on Mathematical Optimization Theory

smooth saddle-point problems.
and Operations Research, pages 144–158. Springer, 2021a.

A. Beznosikov, P. Richt´arik, M. Diskin, M. Ryabinin, and A. Gasnikov. Distributed methods with
compressed communication for solving variational inequalities, with theoretical guarantees. arXiv
preprint arXiv:2110.03313, 2021b.

E. J. Candes, M. B. Wakin, and S. P. Boyd. Enhancing sparsity by reweighted (cid:96)1 minimization. Journal

of Fourier analysis and applications, 14(5):877–905, 2008.

Y. Carmon, Y. Jin, A. Sidford, and K. Tian. Variance reduction for matrix games. Advances in Neural

Information Processing Systems, 32, 2019.

N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university press, 2006.

T. Chavdarova, G. Gidel, F. Fleuret, and S. Lacoste-Julien. Reducing noise in GAN training with
variance reduced extragradient. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
Associates, Inc., 2019.

C. Daskalakis, S. Skoulakis, and M. Zampetakis. The complexity of constrained min-max optimization. In
Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 1466–1478,
2021.

15

D. Davis and W. Yin. A three-operator splitting scheme and its optimization applications. Set-valued

and variational analysis, 25(4):829–858, 2017.

A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support
for non-strongly convex composite objectives. Advances in neural information processing systems, 27,
2014.

V. F. Dem’yanov and A. B. Pevnyi. Numerical methods for ﬁnding saddle points. USSR Computational

Mathematics and Mathematical Physics, 12(5):11–52, 1972.

J. Diakonikolas, C. Daskalakis, and M. Jordan. Eﬃcient methods for structured nonconvex-nonconcave
In International Conference on Artiﬁcial Intelligence and Statistics, pages

min-max optimization.
2746–2754. PMLR, 2021.

G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien. A variational inequality perspective
on generative adversarial networks. In International Conference on Learning Representations (ICLR),
2019.

I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-
gio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q.
Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Asso-
ciates, Inc., 2014.

E. Gorbunov, F. Hanzely, and P. Richtarik. A Uniﬁed Theory of SGD: Variance Reduction, Sampling,
Quantization and Coordinate Descent. In S. Chiappa and R. Calandra, editors, Proceedings of the
Twenty Third International Conference on Artiﬁcial Intelligence and Statistics, volume 108 of Pro-
ceedings of Machine Learning Research, pages 680–690. PMLR, 26–28 Aug 2020a.

E. Gorbunov, D. Kovalev, D. Makarenko, and P. Richtarik. Linearly converging error compensated sgd. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Informa-
tion Processing Systems, volume 33, pages 20889–20900. Curran Associates, Inc., 2020b. URL https:
//proceedings.neurips.cc/paper/2020/file/ef9280fbc5317f17d480e4d4f61b3751-Paper.pdf.

E. Gorbunov, H. Berard, G. Gidel, and N. Loizou. Stochastic extragradient: General analysis and

improved rates. arXiv preprint arXiv:2111.08611, 2021a.

E. Gorbunov, K. P. Burlachenko, Z. Li, and P. Richtarik. MARINA: Faster non-convex distributed
learning with compression. In M. Meila and T. Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages
3788–3798. PMLR, 18–24 Jul 2021b. URL https://proceedings.mlr.press/v139/gorbunov21a.
html.

E. Gorbunov, N. Loizou, and G. Gidel. Extragradient method: O(1/K) last-iterate convergence for
monotone variational inequalities and connections with cocoercivity. arXiv preprint arXiv:2110.04261,
2021c.

R. Gower, O. Sebbouh, and N. Loizou. Sgd for structured nonconvex functions: Learning rates, mini-
batching and interpolation. In International Conference on Artiﬁcial Intelligence and Statistics, pages
1315–1323. PMLR, 2021.

R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richt´arik. SGD: General Analysis
In Proceedings of the 36th International Conference on Machine Learning,

and Improved Rates.
volume 97 of Proceedings of Machine Learning Research, pages 5200–5209, 2019.

Y. Han, G. Xie, and Z. Zhang. Lower complexity bounds of ﬁnite-sum optimization problems: The

results and construction. arXiv preprint arXiv:2103.08280, 2021.

F. Hanzely and P. Richt´arik. Accelerated coordinate descent with arbitrary sampling and best rates
for minibatches. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages
304–312. PMLR, 2019.

16

F. Hanzely, K. Mishchenko, and P. Richt´arik. SEGA: Variance reduction via gradient sketching. Advances

in Neural Information Processing Systems, 31, 2018.

T. Hofmann, A. Lucchi, S. Lacoste-Julien, and B. McWilliams. Variance reduced stochastic gradient

descent with neighbors. Advances in Neural Information Processing Systems, 28, 2015.

S. Horv´ath, D. Kovalev, K. Mishchenko, S. Stich, and P. Richt´arik. Stochastic distributed learning with

gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.

Y.-G. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. On the convergence of single-call stochas-
tic extra-gradient methods. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
Associates, Inc., 2019.

Y.-G. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. Explore aggressively, update conservatively:
Stochastic extragradient methods with variable stepsize scaling. Advances in Neural Information
Processing Systems, 33, 2020.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

Advances in Neural Information Processing Systems, 26, 2013.

A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-prox

algorithm. Stochastic Systems, 1(1):17–58, 2011.

S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi. Error feedback ﬁxes signsgd and other gradient
compression schemes. In International Conference on Machine Learning, pages 3252–3261. PMLR,
2019.

A. Khaled, O. Sebbouh, N. Loizou, R. M. Gower, and P. Richt´arik. Uniﬁed analysis of stochastic gradient
methods for composite convex and smooth optimization. arXiv preprint arXiv:2006.11573, 2020.

G. M. Korpelevich. The extragradient method for ﬁnding saddle points and other problems. Matecon,

12:747–756, 1976.

D. Kovalev, S. Horv´ath, and P. Richt´arik. Don’t jump through hoops and remove those loops: SVRG

and Katyusha are better without the outer loop. In Algorithmic Learning Theory, 2020.

C. J. Li, Y. Yu, N. Loizou, G. Gidel, Y. Ma, N. L. Roux, and M. I. Jordan. On the convergence
of stochastic extragradient for bilinear games with restarted iteration averaging. arXiv preprint
arXiv:2107.00464, 2021.

Z. Li, D. Kovalev, X. Qian, and P. Richtarik. Acceleration for compressed gradient descent in distributed
In International Conference on Machine Learning, pages 5895–5904.

and federated optimization.
PMLR, 2020.

H. Lin, J. Mairal, and Z. Harchaoui. Catalyst acceleration for ﬁrst-order convex optimization:

from

theory to practice. Journal of Machine Learning Research, 18(1):7854–7907, 2018.

T. Lin, Z. Zhou, P. Mertikopoulos, and M. Jordan. Finite-time last-iterate convergence for multi-agent
learning in games. In International Conference on Machine Learning, pages 6161–6171. PMLR, 2020.

S. Liu, S. Lu, X. Chen, Y. Feng, K. Xu, A. Al-Dujaili, M. Hong, and U.-M. O’Reilly. Min-max optimiza-
tion without gradients: Convergence and applications to black-box evasion and poisoning attacks. In
International Conference on Machine Learning, pages 6282–6293. PMLR, 2020.

N. Loizou and P. Richt´arik. Convergence analysis of inexact randomized iterative methods. SIAM

Journal on Scientiﬁc Computing, 42(6):A3979–A4016, 2020a.

N. Loizou and P. Richt´arik. Momentum and stochastic momentum for stochastic gradient, newton,
proximal point and subspace descent methods. Computational Optimization and Applications, 77(3):
653–710, 2020b.

17

N. Loizou, H. Berard, A. Jolicoeur-Martineau, P. Vincent, S. Lacoste-Julien, and I. Mitliagkas. Stochastic
hamiltonian gradient methods for smooth games. In International Conference on Machine Learning,
pages 6370–6381. PMLR, 2020.

N. Loizou, H. Berard, G. Gidel, I. Mitliagkas, and S. Lacoste-Julien. Stochastic gradient descent-ascent
and consensus optimization for smooth games: Convergence analysis under expected co-coercivity.
Advances in Neural Information Processing Systems, 34, 2021.

L. Luo, G. Xie, T. Zhang, and Z. Zhang. Near optimal stochastic algorithms for ﬁnite-sum unbalanced

convex-concave minimax optimization. arXiv preprint arXiv:2106.01761, 2021.

Y. Malitsky and M. K. Tam. A forward-backward splitting method for monotone inclusions without

cocoercivity. SIAM Journal on Optimization, 30(2):1451–1472, 2020.

P. Mertikopoulos and Z. Zhou. Learning in games with continuous action sets and unknown payoﬀ

functions. Mathematical Programming, 173(1):465–507, 2019.

K. Mishchenko, E. Gorbunov, M. Tak´aˇc, and P. Richt´arik. Distributed learning with compressed gradient

diﬀerences. arXiv preprint arXiv:1901.09269, 2019.

K. Mishchenko, D. Kovalev, E. Shulgin, P. Richtarik, and Y. Malitsky. Revisiting stochastic extra-
In S. Chiappa and R. Calandra, editors, Proceedings of the Twenty Third International
gradient.
Conference on Artiﬁcial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning
Research, pages 4573–4582. PMLR, 26–28 Aug 2020.

O. Morgenstern and J. Von Neumann. Theory of games and economic behavior. Princeton university

press, 1953.

A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.

Y. Nesterov. Dual extrapolation and its applications to solving variational inequalities and related

problems. Mathematical Programming, 109(2):319–344, 2007.

Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming, 120

(1):221–259, 2009.

B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems.

In

Advances in Neural Information Processing Systems, pages 1416–1424, 2016.

L. D. Popov. A modiﬁcation of the arrow-hurwicz method for search of saddle points. Mathematical

notes of the Academy of Sciences of the USSR, 28(5):845–848, 1980.

X. Qian, Z. Qu, and P. Richt´arik. Saga with arbitrary sampling. In International Conference on Machine

Learning, pages 5190–5199. PMLR, 2019.

X. Qian, Z. Qu, and P. Richt´arik. L-svrg and l-katyusha with arbitrary sampling. Journal of Machine

Learning Research, 22(112):1–47, 2021a.

X. Qian, P. Richt´arik, and T. Zhang. Error compensated distributed sgd can be accelerated. Advances

in Neural Information Processing Systems, 34, 2021b.

P. Richt´arik and M. Tak´ac. Stochastic reformulations of linear systems: algorithms and convergence

theory. SIAM Journal on Matrix Analysis and Applications, 41(2):487–524, 2020.

P. Richt´arik, I. Sokolov, and I. Fatkhullin. EF21: A new, simpler, theoretically better, and practically

faster error feedback. In Advances in Neural Information Processing Systems, 2021.

A. Sadiev, A. Beznosikov, P. Dvurechensky, and A. Gasnikov. Zeroth-order algorithms for smooth saddle-
point problems. In International Conference on Mathematical Optimization Theory and Operations
Research, pages 71–85. Springer, 2021.

18

F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its application to
data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International
Speech Communication Association, 2014.

C. Song, Z. Zhou, Y. Zhou, Y. Jiang, and Y. Ma. Optimistic dual extrapolation for coherent non-
monotone variational inequalities.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 14303–14314.
Curran Associates, Inc., 2020.

S. U. Stich.

Uniﬁed optimal analysis of the (stochastic) gradient method.

arXiv preprint

arXiv:1907.04232, 2019.

S. U. Stich, J.-B. Cordonnier, and M. Jaggi. Sparsiﬁed sgd with memory. In Proceedings of the 32nd

International Conference on Neural Information Processing Systems, pages 4452–4463, 2018.

V. Tominin, Y. Tominin, E. Borodich, D. Kovalev, A. Gasnikov, and P. Dvurechensky. On accelerated
methods for saddle-point problems with composite structure. arXiv preprint arXiv:2103.09344, 2021.

S. Vaswani, F. Bach, and M. Schmidt. Fast and faster convergence of sgd for over-parameterized models
and an accelerated perceptron. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics, pages 1195–1204. PMLR, 2019.

B. C. V˜u. A splitting algorithm for dual monotone inclusions involving cocoercive operators. Advances

in Computational Mathematics, 38(3):667–681, 2013.

Z. Wang, K. Balasubramanian, S. Ma, and M. Razaviyayn. Zeroth-order algorithms for nonconvex

minimax problems with improved complexities. arXiv preprint arXiv:2001.07819, 2020.

W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad: ternary gradients to reduce
communication in distributed deep learning. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 1508–1518, 2017.

J. Yang, N. Kiyavash, and N. He. Global convergence and variance reduction for a class of nonconvex-
nonconcave minimax problems. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems, volume 33, pages 1153–1165. Curran
Associates, Inc., 2020.

T. Yoon and E. K. Ryu. Accelerated algorithms for smooth convex-concave minimax problems with
In International Conference on Machine Learning, pages

O(1/k2) rate on squared gradient norm.
12098–12109. PMLR, 2021.

D. Yuan, Q. Ma, and Z. Wang. Dual averaging method for solving multi-agent saddle-point problems
with quantized information. Transactions of the Institute of Measurement and Control, 36(1):38–46,
2014.

D. L. Zhu and P. Marcotte. Co-coercivity and its role in the convergence of iterative schemes for solving

variational inequalities. SIAM Journal on Optimization, 6(3):714–726, 1996.

19

Contents

1 Introduction

1.1 Technical Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Our Contributions
1.3 Closely Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Uniﬁed Analysis of SGDA

2.1 Key Assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Quasi-Strongly Monotone Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Monotone Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 SGDA with Arbitrary Sampling

4 SGDA with Variance Reduction

5 Distributed SGDA with Compression

6 Numerical Experiments

7 Conclusion and Future Work

A Further Related Work

1
2
3
4

5
5
5
6

7

8

9

13

14

22

B Missing Details on Numerical Experiments

24
B.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.2 QSGDA vs DIANA-SGDA: Comparison in the Full-Batch Regime . . . . . . . . . . . . . . . 24
B.3 Distributed Quadratic Games with Constraints . . . . . . . . . . . . . . . . . . . . . . . . 25

C Auxiliary Results and Technical Lemmas

26

D Proof of The Main Results

28
D.1 Quasi-Strongly Monotone Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D.2 Monotone Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

E SGDA with Arbitrary Sampling: Missing Proofs and Details

38
E.1 Proof of Proposition 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
E.2 Analysis of SGDA-AS in the Quasi-Strongly Monotone Case . . . . . . . . . . . . . . . . . 38
E.3 Analysis of SGDA-AS in the Monotone Case . . . . . . . . . . . . . . . . . . . . . . . . . . 39
E.4 Missing Details on Arbitrary Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

F SGDA with Variance Reduction: Missing Proofs and Details

42
F.1 L-SVRGDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
F.1.1 Proof of Proposition 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
F.1.2 Analysis of L-SVRGDA in the Quasi-Strongly Monotone Case . . . . . . . . . . . . 43
F.1.3 Analysis of L-SVRGDA in the Monotone Case . . . . . . . . . . . . . . . . . . . . . 43
F.2 SAGA-SGDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
F.2.1 SAGA-SGDA Fits Assumption 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
F.2.2 Analysis of SAGA-SGDA in the Quasi-Strongly Monotone Case . . . . . . . . . . . 45
F.2.3 Analysis of SAGA-SGDA in the Monotone Case . . . . . . . . . . . . . . . . . . . . 46
F.3 Discussion of the Results in the Monotone Case . . . . . . . . . . . . . . . . . . . . . . . . 46

G Distributed SGDA with Compression: Missing Proofs and Details

48
G.1 QSGDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
G.1.1 Proof of Proposition 5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
G.1.2 Analysis of QSGDA in the Quasi-Strongly Monotone Case . . . . . . . . . . . . . . 49
G.1.3 Analysis of QSGDA in the Monotone Case . . . . . . . . . . . . . . . . . . . . . . . 50
G.2 DIANA-SGDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

20

G.2.1 Proof of Proposition 5.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
G.2.2 Analysis of DIANA-SGDA in the Quasi-Strongly Monotone Case . . . . . . . . . . . 51
G.2.3 Analysis of DIANA-SGDA in the Monotone Case
. . . . . . . . . . . . . . . . . . . 52
G.3 VR-DIANA-SGDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
G.3.1 Proof of Proposition 5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
G.3.2 Analysis of VR-DIANA-SGDA in the Quasi-Strongly Monotone Case
. . . . . . . . 57
G.3.3 Analysis of VR-DIANA-SGDA in the Monotone Case . . . . . . . . . . . . . . . . . 58
G.4 Discussion of the Results in the Monotone Case . . . . . . . . . . . . . . . . . . . . . . . . 59

H Coordinate SGDA

60
H.1 CSGDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
H.1.1 CSGDA Fits Assumption 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
H.1.2 Analysis of CSGDA in the Quasi-Strongly Monotone Case . . . . . . . . . . . . . . 60
H.1.3 Analysis of CSGDA in the Monotone Case . . . . . . . . . . . . . . . . . . . . . . . 61
H.2 SEGA-SGDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
H.2.1 SEGA-SGDA Fits Assumption 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
H.2.2 Analysis of SEGA-SGDA in the Quasi-Strongly Monotone Case . . . . . . . . . . . 62
H.2.3 Analysis of SEGA-SGDA in the Monotone Case . . . . . . . . . . . . . . . . . . . . 62
H.3 Comparison with Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

21

A Further Related Work

The references necessary to motivate our work and connect it to the most relevant literature are included
in the appropriate sections of the main body of the paper. Here we present a broader view of the
literature, including some more references to papers of the area that are not directly related with our
work.

Stochastic methods for solving VIPs. Although this paper is devoted to SGDA-type methods,
we brieﬂy mention here the works studying other popular stochastic methods for solving VIPs based
on diﬀerent algorithmic schemes such as Extragradient (EG) method [Korpelevich, 1976] and Optimistic
Gradient (OG) method [Popov, 1980]. The ﬁrst analysis of Stochastic EG for solving (quasi-strongly)
monotone VIPs was proposed in Juditsky et al. [2011] and then was extended and generalized in various
ways [Mishchenko et al., 2020, Hsieh et al., 2020, Beznosikov et al., 2020c, Li et al., 2021, Gorbunov et al.,
2021a]. Stochastic OG was studied in Gidel et al. [2019], Hsieh et al. [2019], Azizian et al. [2021]. In
addition, lightweight second-order methods like stochastic Hamiltonian methods and stochastic consensus
optimization were studied in Loizou et al. [2020], and Loizou et al. [2021], respectively.

Variance reduction for VIPs.
In Section 1.3, we provided the most relevant variance reduced
methods to our setting. Here we expand this discussion a bit more providing more details on some of
the results and some extra references related to acceleration.

We should highlight that the rates from Alacaoglu and Malitsky [2021] match the lower bounds from
Han et al. [2021]. Under additional assumptions similar results were achieved in Carmon et al. [2019]. Ala-
caoglu et al. [2021] developed variance-reduced method (FoRB-VR) based on Forward-Reﬂected-Backward
algorithm [Malitsky and Tam, 2020], but the derived rates are inferior to those from Alacaoglu and Mal-
itsky [2021].

Using Catalyst acceleration framework of Lin et al. [2018], Palaniappan and Bach [2016], Tominin
et al. [2021] achieve (neglecting extra logarithmic factors) similar rates as in Alacaoglu and Malitsky
[2021] and Luo et al. [2021] derive even tighter rates for min-max problems. However, as all Catalyst-
based approaches, these methods require solving an auxiliary problem at each iteration, which reduces
their practical eﬃciency.

On quasi-strong monotonicity and star-cocoercivity.
In this work we focus on quasi-
strongly monotone VI problems, a class of structured non-monotone operators for which we are able
to provide tight convergence guarantees and avoid the standard issues (cycling and divergence of the
methods) appearing in the more general non-monotone regime.

Since in general non-monotone problems, ﬁnding approximate ﬁrst-order locally optimal solutions is
intractable [Daskalakis et al., 2021, Diakonikolas et al., 2021], it is reasonable to consider class of problems
that satisfy special structural assumptions on the objective function for which these intractability barriers
can be bypassed. Examples of problems belong in this category are the ones of our work which satisfy (3)
or, for example, the two-sided PL condition [Yang et al., 2020] or the error-bound condition [Hsieh et al.,
2020]. It is worth highlighting that quasi-strong monotone problems were considered in Mertikopoulos
and Zhou [2019], Song et al. [2020], Loizou et al. [2021], Gorbunov et al. [2021a] as well.

Cocoercivity is a classical assumption in the literature on VIPs [Zhu and Marcotte, 1996] and operator
splittings [Davis and Yin, 2017, V˜u, 2013].
It can be interpreted as an intermediate notion between
monotonicity and strong monotonicity. In general, it is stronger than monotonicity and Lipschitzness
of the operator, e.g., simple bilinear games are non-cocoercive. From Cauchy-Swartz’s inequality, one
can show that a (cid:96)-co-coercive operator is (cid:96)-Lipschitz.
In single-objective minization, one can prove
the converse statement by using convex duality. Thus, a gradient of a function is L–co-coercive if
and only if the function is convex and L-smooth (i.e. L-Lipschitz gradients) [Bauschke et al., 2011].
However, in general, a L-Lipchitz operator is not L–co-coercive. Star-cocoercivity is a new notion
recently introduced in Loizou et al. [2021] and is weaker than classical cocoercivity and can be achieved
via a proper transformation of quasi-monotone Lipschitz operator [Gorbunov et al., 2021c]. Moreover,
any µ-quasi strongly monotone L-Lipschitz operator F is (cid:96)-star-cocoercive with (cid:96) ∈ [L, L2/µ] and there
exist examples of operators that are quasi-strongly monotone and star-cocoercive but neither monotone
nor Lipschitz [Loizou et al., 2021].

22

Coordinate and zeroth-order methods for solving min-max problems and VIPs.
Coordinate methods for solving VIPs are rarely considered in the literature. The most relevant results
are given in the literature on zeroth-order methods for solving min-max problems. Although some of
them can be easily extended to the coordinate versions of methods for solving VIPs, these methods are
usually considered and analyzed for min-max problems. The closest work to our paper is Sadiev et al.
[2021]: they propose and analyze several zeroth-order variants of SGDA and Stochastic EG with two-
point feedback oracle for solving strongly-convex-strongly-concave and convex-concave smooth min-max
problems with bounded domain. Moreover, Sadiev et al. [2021] consider ﬁrmly smooth convex-concave
min-max problems which is an analog of cocoercivity for min-max problems. There are also papers
focusing on diﬀerent problems like non-sonvex-strongly-concave smooth min-max problems [Liu et al.,
2020, Wang et al., 2020], non-smooth strongly-convex-strongly-concave and convex-concave min-max
problems [Beznosikov et al., 2020b] and on diﬀerent methods like ones that use one-point feedback oracle
[Beznosikov et al., 2021a]. These works are less relevant to our paper than Sadiev et al. [2021]. Moreover,
the results derived in these papers are inferior to the ones from Sadiev et al. [2021].

23

B Missing Details on Numerical Experiments

B.1 Setup

We consider the special case of (1) with F and R deﬁned as follows:

F (x) =

1
n

n
(cid:88)

i=1

Fi(x), Fi(x) = Aix + bi,

R(x) = λ(cid:107)x(cid:107)1 + δBr(0)(x) = λ(cid:107)x(cid:107)1 +

(cid:40)

0,
+∞,

if (cid:107)x(cid:107)
if (cid:107)x(cid:107)

∞

∞

≤ r,
> r,

(26)

where each matrix Ai ∈ Rd
bi ∈ Rd, r > 0 is the radius of (cid:96)
Example 6.22 from Beck [2017]) that for the given R(x) prox operator has an explicit formula:

d is non-symmetric with all eigenvalues with strictly positive real part,
-ball, and λ ≥ 0 is regularization parameter. One can show (see

∞

×

proxγR(x) = sign (x) min {max {|x| − γλ, 0} , r} ,

(27)

where sign(·) and | · | are component-wise operators. The considered problem generalizes the following
quadratic game:

min
x1
∞
≤
(cid:107)

r

(cid:107)

max
x2
∞
≤
(cid:107)

r

(cid:107)

1
n

n
(cid:88)

i=1

1
2

x(cid:62)1 A1,ix1 + x(cid:62)1 A2,ix2 −

1
2

x(cid:62)2 A3,ix2 + b(cid:62)1,ix1 − b(cid:62)2,ix2 + λ(cid:107)x1(cid:107)1 − λ(cid:107)x2(cid:107)1

with µiI (cid:52) A1,i (cid:52) LiI and µiI (cid:52) A3,i (cid:52) LiI. Indeed, the above problem is a special case of (1)+(26)
with

x =

(cid:19)

(cid:18)x1
x2

, Ai =

(cid:18) A1,i A2,i
−A2,i A3,i

(cid:19)

,

bi =

(cid:19)

,

(cid:18)b1,i
b2,i

R(x) = λ(cid:107)x1(cid:107)1 + λ(cid:107)x2(cid:107)1 + δBr(0)(x1) + δBr(0)(x2).

In our experiments, to generate the non-symmetric matrices Ai ∈ Rd
×

d deﬁned in (26), we ﬁrst sam-
ple real random matrices Bi where the elements of the matrices are sampled from a normal distribution.
, where the Di are diagonal
We then compute the eigendecomposition of the matrices Bi = QiDiQ−
i
matrices with complex numbers on the diagonal. Next, we construct the matrices Ai = (cid:60)(QiD+
1
)
where (cid:60)(M)i,j = (cid:60)(Mi,j) and D+
is obtained by transforming all the elements of Di to have positive
i
real part. This process ensures that the eigenvalues of Ai all have positive real part, and thus that F (x)
is strongly monotone and cocoercive. The bi ∈ Rd are sampled from a normal distribution with variance
100/d. For all the experiments we choose n = 1000 and d = 100. For the distributed experiments we
simulate m = 10 nodes on a single machine with 2 CPUs. For further details, the code is made available
at: https://github.com/hugobb/sgda.

i Q−
i

1

B.2 QSGDA vs DIANA-SGDA: Comparison in the Full-Batch Regime

To illustrate the diﬀerence between QSGDA and DIANA-SGDA, we conduct an additional experiment
Fig. 4. We consider the full-batch version of QSGDA and DIANA-SGDA. This enables us to separate
the noise coming from the quantization from the noise coming from the stochasticity. We observe that
when using full-batch DIANA-SGDA converges linearly to the solution while QSGDA only converges to a
neighborhood of the solution.

24

Figure 4: QSGDA vs DIANA-SGDA: DIANA-SGDA converges linearly to the solution while QS-
GDA only converges to a neighborhood of the solution.

B.3 Distributed Quadratic Games with Constraints

Similarly to the distributed experiment on quadratic games provided in Section 6, we provide another
distributed experiment on quadratic games but with constraints in Fig. 5. Overall, the methods behavior
is similar to the unconstrained case.

Figure 5: Results on distributed quadratic games with constraints. Letf: Number of oracle
calls. Right: Number of bits communicated between nodes.

25

0200040006000800010000Numberoforaclescall10−710−510−310−1101DistancetooptimalityQSGDADIANA-SGDA02500500075001000012500150001750020000Numberoforaclescall10−410−310−210−1100DistancetooptimalitySGDAQSGDADIANA-SGDAVR-DIANA-SGDA103104105106107108Numberofbitscommunicated10−1010−810−610−410−2100DistancetooptimalitySGDAQSGDADIANA-SGDAVR-DIANA-SGDAC Auxiliary Results and Technical Lemmas

Useful inequalities.
a, b ∈ Rd and α > 0:

In our proofs, we often apply the following inequalities that hold for any

(cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2,

(cid:104)a, b(cid:105) ≤

1
2α

(cid:107)a(cid:107)2 +

α
2

(cid:107)b(cid:107)2.

(28)

(29)

Useful lemmas. The following lemma from Stich [2019] allows us to derive the rates of convergence
to the exact solution.

Lemma C.1 (Simpliﬁed version of Lemma 3 from Stich [2019]). Let the non-negative sequence {rk}k
satisfy the relation

0

≥

rk+1 ≤ (1 − aγk)rk + cγ2
k

for all k ≥ 0, parameters a > 0, c ≥ 0, and any non-negative sequence {γk}k
some h ≥ a, h > 0. Then, for any K ≥ 0 one can choose {γk}k
0 as follows:

≥

0 such that γk ≤ 1/h for

≥

if K ≤

h
a

,

γk =

and k < k0,

γk =

1
h
1
h

,

,

and k ≥ k0,

γk =

2
a(κ + k − k0)

,

if K >

if K >

h
a
h
a

where κ = 2h/a and k0 = (cid:100)K/2(cid:101). For this choice of γk the following inequality holds:

rK ≤

32hr0
a

(cid:18)

exp

−

(cid:19)

aK
2h

+

36c
a2K

.

In the analysis of monotone case, we rely on the classical result from proximal operators the-

ory.

Lemma C.2 (Theorem 6.39 (iii) from Beck [2017]). Let R be a proper lower semicontinuous convex
function and x+ = proxγR(x). Then for all z ∈ Rd the following inequality holds:

(cid:104)x+ − x, z − x+(cid:105) ≥ γ (cid:0)R(x+) − R(z)(cid:1) .

Finally, we rely on the following technical lemma for handling the sums arising in the proofs for the

monotone case.

Lemma C.3. Let K > 0 be a positive integer and η1, η2, . . . , ηK be random vectors such that Ek[ηk] :=
E[ηk | η1, . . . , ηk

1] = 0 for k = 2, . . . , K. Then

−

K
(cid:88)

k=1

E[(cid:107)ηk(cid:107)2].

(30)

2

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

ηk

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

 =

26

Proof. We start with the following derivation:

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

ηk

k=1

2
 = E[(cid:107)ηK(cid:107)2] + 2E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:34)(cid:42)

ηK,

K
1
(cid:88)
−

k=1

(cid:43)(cid:35)

ηk

+ E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
1
(cid:88)
−

k=1

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ηk

= E[(cid:107)ηK(cid:107)2] + 2E

(cid:34)
EK

(cid:34)(cid:42)

= E[(cid:107)ηK(cid:107)2] + 2E

EK[ηK],

(cid:34)(cid:42)

ηK,

(cid:43)(cid:35)(cid:35)

K
1
(cid:88)
−

ηk

+ E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
1
(cid:88)
−

k=1

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

ηk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ηk



k=1

K
1
(cid:88)
−

k=1

(cid:43)(cid:35)

ηk

+ E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
1
(cid:88)
−

k=1

= E[(cid:107)ηK(cid:107)2] + E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
1
(cid:88)
−

k=1

ηk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

 .

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

, . . . , E

(cid:20)(cid:13)
(cid:13)
(cid:13)

(cid:80)2

k=1 ηk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

, we get the result.

Applying similar steps to E

(cid:20)(cid:13)
(cid:13)
(cid:13)

(cid:80)K

1
k=1 ηk
−

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

, E

(cid:20)(cid:13)
(cid:13)
(cid:13)

(cid:80)K

2
k=1 ηk
−

27

D Proof of The Main Results

In this section, we provide complete proofs of our main results.

D.1 Quasi-Strongly Monotone Case

We start with the case when F satisﬁes (3) with µ > 0. For readers convenience, we restate the theorems
below.

Theorem D.1 (Theorem 2.2). Let F be µ-quasi-strongly monotone with µ > 0 and Assumption 2.1
hold. Assume that
(cid:26) 1
µ

1
2(A + CM )

0 < γ ≤ min

(31)

(cid:27)

,

for some M > B/ρ. Then for the Lyapunov function Vk = (cid:107)xk − x∗
have

,k(cid:107)2 + M γ2σ2

k, and for all k ≥ 0 we

E[Vk] ≤

(cid:18)

(cid:26)

1 − min

γµ, ρ −

(cid:27)(cid:19)k

B
M

E[V0] +

γ2(D1 + M D2)
min {γµ, ρ − B/M}

.

(32)

Proof. First of all, we recall a well-known fact about proximal operators: for any solution x∗ of (1) we
have

x∗ = proxγR(x∗ − γF (x∗)).
Using this and non-expansiveness of proximal operator, we derive

(33)

(cid:107)xk+1 − x∗

,k(cid:107)2

,k+1(cid:107)2 ≤ (cid:107)xk+1 − x∗
= (cid:13)
≤ (cid:13)
= (cid:107)xk − x∗

(cid:13)proxγR(xk − γgk) − proxγR(x∗
,k)(cid:13)
2
(cid:13)xk − γgk − x∗
(cid:13)
,k, gk − F (x∗

,k − γF (x∗
,k(cid:107)2 − 2γ (cid:10)xk − x∗

,k − γF (x∗

,k))(cid:13)
2
(cid:13)

,k)(cid:11) + γ2(cid:107)gk − F (x∗

,k)(cid:107)2.

Next, we take an expectation Ek[·] w.r.t. the randomness at iteration k and get

Ek

(cid:2)(cid:107)xk+1 − x∗

,k+1(cid:107)2(cid:3) = (cid:107)xk − x∗
+γ2Ek

,k(cid:107)2 − 2γ (cid:10)xk − x∗
,k, F (xk) − F (x∗
2(cid:105)
(cid:104)(cid:13)
,k)(cid:13)
(cid:13)gk − F (x∗
(cid:13)

,k)(cid:11)

(6)
≤ (cid:107)xk − x∗

,k(cid:107)2 − 2γ (cid:10)xk − x∗, F (xk) − F (x∗

+γ2 (cid:0)2A (cid:10)xk − x∗

,k, F (xk) − F (x∗

,k)(cid:11)
,k)(cid:11) + Bσ2

k + D1

(cid:1) .

Summing up this inequality with (7) multiplied by M γ2, we obtain
,k+1(cid:107)2(cid:3) + M γ2Ek[σ2
≤ (cid:107)xk − x∗

k+1]
,k(cid:107)2 − 2γ (cid:10)xk − x∗

(cid:2)(cid:107)xk+1 − x∗

Ek

+ γ2 (cid:0)2A (cid:10)xk − x∗
+ M γ2 (cid:0)2C (cid:10)xk − x∗

,k, F (xk) − F (x∗
,k)(cid:11) + Bσ2

,k)(cid:11)
k + D1

(cid:1)
,k)(cid:11) + (1 − ρ)σ2

,k, F (xk) − F (x∗

,k, F (xk) − F (x∗
(cid:18)

(cid:19)

(cid:1)

k + D2

= (cid:107)xk − x∗

,k(cid:107)2 + M γ2

B
M
− 2γ (1 − γ(A + CM )) (cid:10)xk − x∗
,k, F (xk) − F (x∗

1 − ρ +

k + γ2(D1 + M D2)
σ2

,k)(cid:11) .

(34)

Since γ ≤
monotonicity of F , we derive

1

2(A+CM ) the factor −2γ (1 − γ(A + CM )) is non-positive. Therefore, applying strong quasi-

Ek

(cid:2)(cid:107)xk+1 − x∗

,k+1(cid:107)2 + M γ2σ2

k+1

(cid:3) ≤ (1 − 2γµ (1 − γ(A + CM ))) (cid:107)xk − x∗

,k(cid:107)2

(cid:18)

+M γ2

1 − ρ +

(cid:19)

B
M

k + γ2(D1 + M D2).
σ2

28

Using γ ≤

1

2(A+CM ) and the deﬁnition Vk = (cid:107)xk − x∗

,k(cid:107)2 + M γ2σ2

Ek [Vk+1] ≤ (1 − γµ) (cid:107)xk − x∗

,k(cid:107)2 + M γ2

(cid:18)

1 − ρ +

k + γ2(D1 + M D2)
σ2

k, we get
(cid:19)

B
M

(cid:18)

(cid:26)

≤

1 − min

γµ, ρ −

(cid:27)(cid:19)

B
M

Vk + γ2(D1 + M D2).

Next, we take the full expectation from the above inequality and establish the following recurrence:

E [Vk+1] ≤

(cid:18)

(cid:26)

1 − min

γµ, ρ −

(cid:27)(cid:19)

B
M

E[Vk] + γ2(D1 + M D2).

(35)

Unrolling the recurrence, we derive

E [Vk] ≤

(cid:18)

(cid:26)

1 − min

γµ, ρ −

(cid:18)

(cid:26)

1 − min

γµ, ρ −

(cid:18)

(cid:26)

1 − min

γµ, ρ −

≤

=

which ﬁnishes the proof.

(cid:27)(cid:19)k

(cid:27)(cid:19)k

(cid:27)(cid:19)k

B
M

B
M

B
M

E[V0] + γ2(D1 + M D2)

k
1
(cid:88)
−

(cid:18)

(cid:26)

1 − min

γµ, ρ −

E[V0] + γ2(D1 + M D2)

t=0

∞(cid:88)

(cid:18)

t=0

(cid:26)

1 − min

γµ, ρ −

(cid:27)(cid:19)t

(cid:27)(cid:19)t

B
M

B
M

E[V0] +

γ2(D1 + M D2)
min {γµ, ρ − B/M}

,

Using this and Lemma C.1, we derive the following result about the convergence to the exact solu-

tion.

Corollary D.2 (Corollary 2.3). Let the assumptions of Theorem 2.2 hold. Consider two possible
cases.

1. Let D1 = D2 = 0. Then, for any K ≥ 0, M = 2B/ρ, and

γ = min

(cid:26) 1
µ

,

1
2(A + 2BC/ρ)

(cid:27)

we have

E[VK] ≤ E[V0] exp

(cid:18)

(cid:26)

− min

µ
2(A + 2BC/ρ)

,

ρ
2

(cid:27)

(cid:19)

K

.

(36)

(37)

2. Let D1 + M D2 > 0. Then, for any K ≥ 0 and M = 2B/ρ one can choose {γk}k

0 as follows:

≥

if K ≤

h
µ

,

γk =

and k < k0,

γk =

1
h
1
h

,

,

and k ≥ k0,

γk =

2
µ(κ + k − k0)

,

if K >

if K >

h
µ
h
µ

(38)

where h = max {2(A + 2BC/ρ), 2µ/ρ}, κ = 2h/µ and k0 = (cid:100)K/2(cid:101). For this choice of γk the following
inequality holds:

E[VK] ≤ 32 max

(cid:26) 2(A + 2BC/ρ)
µ

,

2
ρ

(cid:27)

E[V0] exp

(cid:18)

(cid:26)

− min

µ
2(A + 2BC/ρ)

,

ρ
4

(cid:27)

(cid:19)

K

+

36(D1 + 2BD2/ρ)
µ2K

.

(39)

Proof. The ﬁrst part of the corollary follows from Theorem 2.2 due to

(cid:18)

(cid:26)

1 − min

γµ, ρ −

(cid:27)(cid:19)K

B
M

(cid:16)

=

1 − min

(cid:110)

γµ,

ρ
2

(cid:111)(cid:17)K

≤ exp

(cid:16)

− min

(cid:110)

γµ,

(cid:111)

(cid:17)

.

K

ρ
2

29

Plugging (36) in the above inequality, we derive (37). Next, we consider the case when D1 + M D2 > 0.
First, we notice that (35) holds for non-constant stepsizes γk such that

0 < γk ≤ min

(cid:26) 1
µ

,

1
2(A + CM )

(cid:27)

.

Therefore, for any k ≥ 0 we have

E [Vk+1]

≤

(cid:18)

(cid:26)

1 − min

γkµ, ρ −

(cid:27)(cid:19)

B
M

E[Vk] + γ2

k(D1 + M D2)

M =2B/ρ
=

(1 − min {γkµ, ρ/2}) E[Vk] + γ2

k(D1 + 2BD2/ρ).

Secondly, we assume that for all k ≥ 0

0 < γk ≤ min

(cid:26) ρ
2µ

,

1
2(A + CM )

(cid:27)

.

Applying this to the recurrence for E[Vk], we obtain

E [Vk+1] ≤ (1 − γkµ) E[Vk] + γ2

k(D1 + 2BD2/ρ).

It remains to apply Lemma C.1 with rk = E[Vk], a = µ, c = D1 + 2BD2/ρ, and
h = max {2(A + 2BC/ρ), 2µ/ρ} to the above recurrence.

D.2 Monotone Case

Next, we consider the case when µ = 0. Before deriving the proof, we provide additional discussion of
the setup.

We emphasize that the maximum in (10) is taken over the compact set C containing the solution set
X ∗. Therefore, the quantity Gap
(z) is a valid measure of convergence [Nesterov, 2007]. We point out
that the iterates xk do not have to lie in C. Our analysis works for the problems with unbounded and
bounded domains (see Nesterov [2007], Alacaoglu and Malitsky [2021] for similar setups).

C

Another popular convergence measure for the case when R(x) ≡ 0 in (1) is (cid:107)F (xk)(cid:107)2. Although
the squared norm of the operator is a weaker guarantee, it is easier to compute in practice and better
suited for non-monotone problems [Yoon and Ryu, 2021]. Nevertheless, (cid:107)F (xk)(cid:107)2 is not a valid measure
of convergence for (1) with R(x) (cid:54)≡ 0. Therefore, we focus on Gap
C

(z) in the monotone case.6

Theorem D.3 (Theorem 2.5). Let F be monotone, (cid:96)-star-cocoercive and Assumptions 2.1, 2.4 hold.
Assume that

0 < γ ≤

1
2(A + BC/ρ)

.

(40)

Then for the function Gap

C

(z) from (10) and for all K ≥ 0 we have

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+ (4A + (cid:96) + 8BC/ρ) ·

(cid:107)x0 − x∗

,0(cid:107)2

K

+ (4 + (4A + (cid:96) + 8BC/ρ) γ)

γBσ2
0
ρK

+γ(2 + γ (4A + (cid:96) + 8BC/ρ))(D1 + 2BD2/ρ)
+9γ max
X ∗
x∗

(cid:107)F (x∗)(cid:107)2.

∈

(41)

6When R(x) ≡ 0, our analysis can be modiﬁed to get the guarantees on the squared norm of the operator.

30

Proof. First, we apply the classical result about proximal operators (Lemma C.2) with x+ = xk+1,
x = xk − γgk, and z = u for arbitrary point u ∈ Rd:

(cid:104)xk+1 − xk + γgk, u − xk+1(cid:105) ≥ γ (cid:0)R(xk+1) − R(u)(cid:1) .

Multiplying by the factor of 2 and making small rearrangement, we get

2γ(cid:104)gk, u − xk(cid:105) + 2(cid:104)xk+1 − xk, u − xk(cid:105) + 2(cid:104)xk+1 − xk + γgk, xk − xk+1(cid:105) ≥ 2γ (cid:0)R(xk+1) − R(u)(cid:1)

implying

2γ (cid:0)(cid:104)F (xk), xk − u(cid:105) + R(xk+1) − R(u)(cid:1) ≤ 2(cid:104)xk+1 − xk, u − xk(cid:105) + 2γ(cid:104)F (xk) − gk, xk − u(cid:105)

+2(cid:104)xk+1 − xk, xk − xk+1(cid:105) + 2γ(cid:104)gk, xk − xk+1(cid:105).

Next, we use a squared norm decomposition (cid:107)a + b(cid:107)2 = (cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 + 2(cid:104)a, b(cid:105), and obtain

2γ (cid:0)(cid:104)F (xk), xk − u(cid:105) + R(xk+1) − R(u)(cid:1) ≤ (cid:107)xk+1 − xk(cid:107)2 + (cid:107)xk − u(cid:107)2 − (cid:107)xk+1 − u(cid:107)2

+2γ(cid:104)F (xk) − gk, xk − u(cid:105)
−2(cid:107)xk+1 − xk(cid:107)2 + 2γ(cid:104)gk, xk − xk+1(cid:105).

Then, due to 2(cid:104)a, b(cid:105) ≤ (cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 we have

2γ (cid:0)(cid:104)F (xk), xk − u(cid:105) + R(xk+1) − R(u)(cid:1) ≤ (cid:107)xk+1 − xk(cid:107)2 + (cid:107)xk − u(cid:107)2 − (cid:107)xk+1 − u(cid:107)2

+2γ(cid:104)F (xk) − gk, xk − u(cid:105)
−2(cid:107)xk+1 − xk(cid:107)2 + γ2(cid:107)gk(cid:107)2 + (cid:107)xk − xk+1(cid:107)2

= (cid:107)xk − u(cid:107)2 − (cid:107)xk+1 − u(cid:107)2

+2γ(cid:104)F (xk) − gk, xk − u(cid:105) + γ2(cid:107)gk(cid:107)2.

Monotonicity of F implies (cid:104)F (u), xk − u(cid:105) ≤ (cid:104)F (xk), xk − u(cid:105), allowing us to continue our derivation as
follows:

2γ (cid:0)(cid:104)F (u), xk − u(cid:105) + R(xk+1) − R(u)(cid:1) ≤ (cid:107)xk − u(cid:107)2 − (cid:107)xk+1 − u(cid:107)2

+2γ(cid:104)F (xk) − gk, xk − u(cid:105) + γ2(cid:107)gk(cid:107)2

= (cid:107)xk − u(cid:107)2 − (cid:107)xk+1 − u(cid:107)2

+2γ(cid:104)F (xk) − gk, xk − u(cid:105)
+γ2(cid:107)gk − g∗

,k + g∗

,k(cid:107)2

(28)
≤ (cid:107)xk − u(cid:107)2 − (cid:107)xk+1 − u(cid:107)2

+2γ(cid:104)F (xk) − gk, xk − u(cid:105)
+2γ2(cid:107)gk − g∗

,k(cid:107)2 + 2γ2(cid:107)g∗

,k(cid:107)2.

31

Summing up the above inequality for k = 0, 1, . . . , K − 1, we get

2γ

K
1
(cid:88)
−

k=0

(cid:0)(cid:104)F (u), xk − u(cid:105) + R(xk+1) − R(u)(cid:1) ≤

K
1
(cid:88)
−

k=0

(cid:107)xk − u(cid:107)2 −

K
1
(cid:88)
−

k=0

(cid:107)xk+1 − u(cid:107)2

+2γ2

K
1
(cid:88)
−

k=0

(cid:107)g∗

,k(cid:107)2

K
1
(cid:88)
−

+2γ

(cid:104)F (xk) − gk, xk − u(cid:105)

k=0
K
1
(cid:88)
−

+2γ2

k=0

(cid:107)gk − g∗

,k(cid:107)2

= (cid:107)x0 − u(cid:107)2 − (cid:107)xK − u(cid:107)2 + 2γ2

K
1
(cid:88)
−

k=0

(cid:107)g∗

,k(cid:107)2

K
1
(cid:88)
−

+2γ

(cid:104)F (xk) − gk, xk − u(cid:105)

k=0
K
1
(cid:88)
−

+2γ2

k=0

(cid:107)gk − g∗

,k(cid:107)2.

Next, we divide both sides by 2γK

1
K

K
1
(cid:88)
−

k=0

(cid:0)(cid:104)F (u), xk − u(cid:105) + R(xk+1) − R(u)(cid:1) ≤

(cid:107)x0 − u(cid:107)2 − (cid:107)xK − u(cid:107)2
2γK

+

γ
K

K
1
(cid:88)
−

k=0

(cid:107)g∗

,k(cid:107)2

+

+

1
K

γ
K

K
1
(cid:88)
−
(cid:104)F (xk) − gk, xk − u(cid:105)

k=0
K
1
(cid:88)
−

k=0

(cid:107)gk − g∗

,k(cid:107)2

and, after small rearrangement, we obtain

1
K

K
1
(cid:88)
−

k=0

(cid:0)(cid:104)F (u), xk+1 − u(cid:105) + R(xk+1) − R(u)(cid:1) ≤

(cid:107)x0 − u(cid:107)2 − (cid:107)xK − u(cid:107)2
2γK

+

(cid:104)F (u), xK − x0(cid:105)
K

+

+

+

γ
K

1
K

γ
K

K
1
(cid:88)
−

(cid:107)g∗

,k(cid:107)2

k=0
K
1
(cid:88)
−
(cid:104)F (xk) − gk, xk − u(cid:105)

k=0
K
1
(cid:88)
−

k=0

(cid:107)gk − g∗

,k(cid:107)2.

Applying Jensen’s inequality for convex function R, we get R

(cid:16) 1
K

32

(cid:80)K

k=0 xk+1(cid:17)

−

1

≤ 1
K

(cid:80)K

1

k=0 R(xk+1).

−

Plugging this in the previous inequality, we derive for u∗ being a projection of u on X ∗

(cid:42)

(cid:32)

F (u),

1
K

K
1
(cid:88)
−

k=0

(cid:33)

(cid:43)

(cid:32)

xk+1

− u

+ R

(cid:33)

xk+1

− R(u)

1
K

K
1
(cid:88)
−

k=0

≤

(cid:107)x0 − u(cid:107)2 − (cid:107)xK − u(cid:107)2
2γK

+

(cid:104)F (u), xK − x0(cid:105)
K

+

γ
K

K
1
(cid:88)
−

k=0

(cid:107)g∗

,k(cid:107)2

+

1
K

K
1
(cid:88)
−

k=0

(cid:104)F (xk) − gk, xk − u(cid:105) +

K
1
(cid:88)
−

(cid:107)gk − g∗

,k(cid:107)2

γ
K

(29)
≤

(cid:107)x0 − u(cid:107)2 − (cid:107)xK − u(cid:107)2
2γK

+

k=0
(cid:107)xK − x0(cid:107)2
4γK

+

4γ
K

(cid:107)F (u) − F (u∗) + F (u∗)(cid:107)2

+

γ
K

K
1
(cid:88)
−

k=0

(cid:107)g∗

,k(cid:107)2 +

1
K

K
1
(cid:88)
−
(cid:104)F (xk) − gk, xk − u(cid:105) +

k=0

γ
K

K
1
(cid:88)
−

(cid:107)gk − g∗

,k(cid:107)2

(28)
≤

(cid:107)x0 − u(cid:107)2 − (cid:107)xK − u(cid:107)2
2γK

+

(cid:107)g∗

,k(cid:107)2 + 8γ(cid:107)F (u∗)(cid:107)2 +

(cid:107)gk − g∗

,k(cid:107)2

k=0
8γ
K

(cid:107)x0 − u(cid:107)2 + (cid:107)xK − u(cid:107)2
2γK
K
1
(cid:88)
−
(cid:104)F (xk) − gk, xk − u(cid:105)

+

1
K

k=0

(cid:107)F (u) − F (u∗)(cid:107)2

+

+

γ
K

γ
K

K
1
(cid:88)
−

k=0
K
1
(cid:88)
−

k=0

(4)
≤

(cid:107)x0 − u(cid:107)2
γK
K
1
(cid:88)
−

+

1
K

k=0

+

8γ(cid:96)2(cid:107)u − u∗(cid:107)2
K

+ 9γ max
X ∗
x∗

(cid:107)F (x∗)(cid:107)2

(cid:104)F (xk) − gk, xk − u(cid:105) +

(cid:107)gk − g∗

,k(cid:107)2.

∈
K
1
(cid:88)
−

k=0

γ
K

Next, we take maximum from the both sides in u ∈ C, which gives Gap
C
side by deﬁnition (10), and take the expectation of the result:

(cid:16) 1
K

(cid:80)K

k=1 xk(cid:17)

in the left-hand

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

≤

E (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
γK

8γ(cid:96)2E (cid:2)maxu
∈C
K

+

(cid:107)u − u∗(cid:107)2(cid:3)

+9γ max
X ∗
x∗

(cid:107)F (x∗)(cid:107)2

E

+

1
K
E (cid:2)maxu

∈
(cid:34)

max
u
∈C

∈C
γK
(cid:34)

(cid:35)
K
1
(cid:88)
−
(cid:104)F (xk) − gk, xk − u(cid:105)

+

k=0
(cid:107)x0 − u(cid:107)2(cid:3)

8γ(cid:96)2Ω2
CK

+

+ 9γ max
x∗
X ∗
(cid:35)
K
1
(cid:88)
−
(cid:104)F (xk) − gk, xk − u(cid:105)

∈

E

max
u
∈C

k=0

+

+

1
K

γ
K

γ
K

K
1
(cid:88)
−

k=0

E (cid:2)(cid:107)gk − g∗

,k(cid:107)2(cid:3)

(cid:107)F (x∗)(cid:107)2

K
1
(cid:88)
−

k=0

E (cid:2)(cid:107)gk − g∗

,k(cid:107)2(cid:3) .

(42)

In the last step, we also use that X ∗ ⊂ C and Ω

:= maxx,y

∈C

C

(cid:107)x − y(cid:107) (Assumption 2.4).

It remains to upper bound the terms from the last two lines of (42). We start with the ﬁrst one.

33

(cid:34)K

1
(cid:88)
−
(cid:104)F (xk) − gk, xk(cid:105)

(cid:34)K

1
(cid:88)
−
(cid:104)E[F (xk) − gk | xk], xk(cid:105)

(cid:35)

= 0,

= E

Since

E

E

k=0

(cid:34)K

1
(cid:88)
−

k=0

(cid:35)

(cid:35)

(cid:104)F (xk) − gk, x0(cid:105)

=

we have

(cid:34)

E

1
K

max
u
∈C

K
1
(cid:88)
−
(cid:104)F (xk) − gk, xk − u(cid:105)

(cid:35)

=

k=0

E

1
K

k=0

K
1
(cid:88)
−

k=0

(cid:10)E[F (xk) − gk], x0(cid:11) = 0,

(cid:34)K

1
(cid:88)
−
(cid:104)F (xk) − gk, xk(cid:105)

(cid:35)

k=0
(cid:34)

1
K

E

max
u
∈C

K
1
(cid:88)
−

(cid:35)
(cid:104)F (xk) − gk, −u(cid:105)

k=0

(cid:34)

max
u
∈C

K
1
(cid:88)
−
(cid:104)F (xk) − gk, −u(cid:105)

(cid:35)

k=0

+

E

(cid:34)K

(cid:35)
1
(cid:88)
−
(cid:104)F (xk) − gk, x0(cid:105)

E

k=0
(cid:34)

1
K

E

max
u
∈C

K
1
(cid:88)
−

(cid:35)
(cid:104)F (xk) − gk, −u(cid:105)

k=0

K
1
(cid:88)
−
(F (xk) − gk), x0 − u

(cid:43)(cid:35)

=

=

1
K

1
K

= E

(29)
≤ E

+

(cid:34)

max
u
∈C



max
u
∈C


(cid:42)

1
K

k=0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
K

γK
2






(cid:13)
2
K
1
(cid:13)
(cid:88)
−
(F (xk) − gk)
(cid:13)
(cid:13)
(cid:13)

k=0

+

1
2γK

(cid:107)x0 − u(cid:107)2










=

γ
2K

E



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
K
1
(cid:13)
(cid:88)
−
(F (xk) − gk)
(cid:13)
(cid:13)
(cid:13)

k=0

2

 +

1
2γK

max
u
∈C

(cid:107)x0 − u(cid:107)2.

We notice that E[F (xk) − gk | F (x0) − g0, . . . , F (xk
Lemma C.3 are satisﬁed. Therefore, applying Lemma C.3, we get

1) − gk

−

−

1] = 0 for all k ≥ 1, i.e., conditions of

(cid:34)

E

1
K

max
u
∈C

(cid:35)
K
1
(cid:88)
−
(cid:104)F (xk) − gk, xk − u(cid:105)

≤

k=0

γ
2K

+

K
1
(cid:88)
−

E[(cid:107)F (xk) − gk(cid:107)2]

k=0
1
2γK

max
u
∈C

(cid:107)x0 − u(cid:107)2.

(43)

Combining (42) and (43), we derive

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

3 (cid:2)maxu

≤

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+ 9γ max
X ∗
x∗

∈

(cid:107)F (x∗)(cid:107)2

+

γ
2K
3 (cid:2)maxu

(28)
≤

K
1
(cid:88)
−

E (cid:2)(cid:107)gk − F (xk)(cid:107)2(cid:3) +

γ
K

K
1
(cid:88)
−

k=0

E (cid:2)(cid:107)gk − g∗

,k(cid:107)2(cid:3)

(cid:107)x0 − u(cid:107)2(cid:3)

+

8γ(cid:96)2Ω2
CK

+ 9γ max
X ∗
x∗

(cid:107)F (x∗)(cid:107)2

+

γ
K

E (cid:2)(cid:107)F (xk) − g∗

,k(cid:107)2(cid:3) +

2γ
K

E (cid:2)(cid:107)gk − g∗

,k(cid:107)2(cid:3) .

∈
K
1
(cid:88)
−

k=0

k=0

∈C
2γK
K
1
(cid:88)
−

k=0

34

Using (cid:96)-star-cocoercivity of F together with the ﬁrst part of Assumption 2.1, we continue our derivation
as follows:

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

=

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+ 2γD1 + 9γ max
X ∗

x∗

∈

(cid:107)F (x∗)(cid:107)2

+

γ(4A + (cid:96))
K

3 (cid:2)maxu

k=0
(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

K
1
(cid:88)
−

E (cid:2)(cid:104)F (xk) − g∗

,k, xk − x∗

,k(cid:105)(cid:3) +

2γB
K

K
1
(cid:88)
−

k=0

E (cid:2)σ2

k

(cid:3)

+

8γ(cid:96)2Ω2
CK

+ 2γD1 + 9γ max
X ∗

x∗

(cid:107)F (x∗)(cid:107)2

∈

+

γ(4A + (cid:96))
K

K
1
(cid:88)
−

k=0

E (cid:2)(cid:104)F (xk) − g∗

,k, xk − x∗

,k(cid:105)(cid:3)

+

2γB
K

(cid:18)

1 +

1
ρ

(cid:19) K

1
(cid:88)
−

k=0

E (cid:2)σ2

k

(cid:3) −

2γB
ρK

K
1
(cid:88)
−

k=0

E (cid:2)σ2

k

(cid:3) .

Next, we use the second part of Assumption 2.1 and get

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

≤

≤

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+ 2γD1 + 9γ max
X ∗

x∗

∈

(cid:107)F (x∗)(cid:107)2

+

γ(4A + (cid:96))
K

+

2γB
K

+

2γB
K

(cid:18)

(cid:18)

1 +

1 +

K
1
(cid:88)
−

k=0

E (cid:2)(cid:104)F (xk) − g∗

,k, xk − x∗

,k(cid:105)(cid:3)

1
ρ

1
ρ

(cid:19) K

1
(cid:88)
−

k=1

(cid:19) K

1
(cid:88)
−

k=1

E (cid:2)2C(cid:104)F (xk

1) − g∗

−

,k

1, xk

−

−

1 − x∗

,k

−

1(cid:105)(cid:3)

E (cid:2)(1 − ρ)σ2

k

(cid:3)

1 + D2

−

+

2γB
K
3 (cid:2)maxu

(cid:18)

(cid:19)

σ2
0 −

1 +

1
ρ
(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

K
1
(cid:88)
−

E (cid:2)σ2

k

(cid:3)

2γB
ρK

k=0
8γ(cid:96)2Ω2
CK

+

+

2γB(1 + 1/ρ)
K

σ2
0

+2γ (D1 + B(1 + 1/ρ)D2)

+9γ max
X ∗
x∗

∈
(cid:18)

(cid:107)F (x∗)(cid:107)2 +

γ(4A + (cid:96))
K

K
1
(cid:88)
−

k=0

E (cid:2)(cid:104)F (xk) − g∗

,k, xk − x∗

,k(cid:105)(cid:3)

+

2γB
K

1 +

1
ρ

(cid:19) K

2
(cid:88)
−

k=0

E (cid:2)2C(cid:104)F (xk) − g∗

,k, xk − x∗

,k(cid:105) + (1 − ρ)σ2
k

(cid:3)

−

2γB
ρK
3 (cid:2)maxu

K
1
(cid:88)
−

E (cid:2)σ2

k

(cid:3)

k=0
(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+

2γB(1 + 1/ρ)
K

σ2
0

+2γ (D1 + B(1 + 1/ρ)D2) + 9γ max
X ∗

x∗

(cid:107)F (x∗)(cid:107)2

+ (4A + (cid:96) + 4BC(1 + 1/ρ))

γ
K

E (cid:2)(cid:104)F (xk) − g∗

,k, xk − x∗

,k(cid:105)(cid:3)

∈
K
1
(cid:88)
−

k=0

+

2γB
K

(cid:18)

(1 − ρ)

1 +

1
ρ

(cid:19) K

2
(cid:88)
−

k=0

E (cid:2)σ2

k

(cid:3) −

2γB
ρK

K
1
(cid:88)
−

k=0

E (cid:2)σ2

k

(cid:3) .

35

Since (1 − ρ) (1 + 1/ρ) = −ρ + 1/ρ ≤ 1/ρ, the last row is non-positive and we have

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+

2γB(1 + 1/ρ)
K

σ2
0

+2γ (D1 + B(1 + 1/ρ)D2) + 9γ max
X ∗

x∗

∈

+

γ (4A + (cid:96) + 4BC(1 + 1/ρ))
K

K
1
(cid:88)
−

k=0

(cid:107)F (x∗)(cid:107)2

(44)

E (cid:2)(cid:104)F (xk) − g∗

,k, xk − x∗

,k(cid:105)(cid:3) .

Note that inequality (34) from the proof of Theorem 2.2 is derived using Assumption 2.1 only. With

M = B/ρ it gives

E (cid:2)(cid:107)xk+1 − x∗

,k+1(cid:107)2(cid:3) +

γ2B
ρ

E[σ2

k+1] ≤ E (cid:2)(cid:107)xk − x∗

,k(cid:107)2(cid:3) +

γ2B
ρ

E (cid:2)σ2

k

(cid:3) + γ2(D1 + BD2/ρ)

−2γ (1 − γ(A + BC/ρ)) E (cid:2)(cid:10)xk − x∗

,k, F (xk) − g∗

,k(cid:11)(cid:3) .

Since γ ≤ 1/2(A+BC/ρ) we obtain

γE (cid:2)(cid:10)xk − x∗

,k, F (xk) − g∗

,k(cid:11)(cid:3) ≤ E (cid:2)(cid:107)xk − x∗

,k(cid:107)2(cid:3) +

γ2B
ρ

E (cid:2)σ2

k

(cid:3) − E (cid:2)(cid:107)xk+1 − x∗

,k+1(cid:107)2(cid:3)

−

γ2B
ρ

E[σ2

k+1] + γ2(D1 + BD2/ρ).

Plugging this inequality in (44), we derive
3 (cid:2)maxu

(cid:33)(cid:35)

(cid:32)

(cid:34)

K
(cid:88)

E

Gap
C

1
K

xk

≤

k=1

≤

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+

2γB(1 + 1/ρ)
K

σ2
0

+2γ (D1 + B(1 + 1/ρ)D2) + 9γ max
X ∗

x∗

(cid:107)F (x∗)(cid:107)2

+ (4A + (cid:96) + 4BC(1 + 1/ρ)) ·

− (4A + (cid:96) + 4BC(1 + 1/ρ)) ·

+ (4A + (cid:96) + 4BC(1 + 1/ρ)) ·

∈
K
1
(cid:88)
−

E (cid:2)(cid:107)xk − x∗

,k(cid:107)2(cid:3)

k=0
K
1
(cid:88)
−

E (cid:2)(cid:107)xk+1 − x∗

,k+1(cid:107)2(cid:3)

1
K

1
K

k=0
K
1
(cid:88)
−

k=0

γ2B
ρK

E (cid:2)σ2

k − σ2

k+1

(cid:3)

+γ2 (4A + (cid:96) + 4BC(1 + 1/ρ)) · (D1 + BD2/ρ)

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+ (4A + (cid:96) + 8BC/ρ) ·

(cid:107)x0 − x∗

,0(cid:107)2

K

+ (4 + (4A + (cid:96) + 8BC/ρ) γ)

γBσ2
0
ρK

(cid:18)

+γ

(2 + γ (4A + (cid:96) + 8BC/ρ))(D1 + 2BD2/ρ) + 9 max
X ∗

x∗

∈

(cid:107)F (x∗)(cid:107)2

(cid:19)

,

where in the last inequality we use 1 + 1/ρ ≤ 2/ρ.

Corollary D.4. Let the assumptions of Theorem 2.5 hold. Then, for all K one can choose γ as

(cid:40)

γ = min

1
4A + (cid:96) + 8BC/ρ

,

√

ρ

B

Ω0,
√
C
(cid:98)σ0

,

Ω0,
C
(cid:112)K(D1 + 2BD2/ρ)

,

Ω0,
√
C
K

G

∗

(cid:41)

,

(45)

where Ω0 := (cid:107)x0 − x∗

,0(cid:107)2 and Ω0,
C

, (cid:98)σ0, and G

∗

are some upper bounds for maxu

∈C

(cid:107)x0 − u(cid:107), σ0, and

36

maxx∗

∈

X ∗ (cid:107)F (x∗)(cid:107) respectively. This choice of γ implies E

(cid:104)
Gap

C

(cid:16) 1
K

(cid:80)K

k=1 xk(cid:17)(cid:105)

equals

(cid:32)

O

(A + (cid:96) + BC/ρ)(Ω2
0,
K

C

+ Ω2

0) + (cid:96)Ω2
C

+

√

Ω0,

C (cid:98)σ0
√
ρK

B

Ω0,
C

(

+

(cid:112)D1 + BD2/ρ + G
K

√

)

∗

(cid:33)

.

Proof. First of all, the choice of γ from (45) implies (40) since

1
4A + (cid:96) + 8BC/ρ

≤

1
2 (A + BC/ρ)

.

Using (11), the deﬁnitions of Ω0,
C

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

xk

(cid:33)(cid:35)

≤

, (cid:98)σ0, G
3 (cid:2)maxu

∗

, and γ ≤ 1/(4A+(cid:96)+8BC/ρ), we get

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+ (4A + (cid:96) + 8BC/ρ) ·

(cid:107)x0 − x∗

,0(cid:107)2

K

+ (4 + (4A + (cid:96) + 8BC/ρ) γ)

γBσ2
0
ρK

(cid:18)

+γ

(2 + γ (4A + (cid:96) + 8BC/ρ))(D1 + 2BD2/ρ) + 9 max
X ∗

x∗

(cid:19)

(cid:107)F (x∗)(cid:107)2

≤

3Ω2
0,
C
2γK

+

8γ(cid:96)2Ω2
CK

+

(4A + (cid:96) + 8BC/ρ)Ω2
0
K
γB(cid:98)σ2
ρK

0

+ (4 + (4A + (cid:96) + 8BC/ρ) γ)
+γ (cid:0)(2 + γ (4A + (cid:96) + 8BC/ρ))(D1 + 2BD2/ρ) + 9G2

≤

3Ω2
0,
C
2γK

+

(cid:18)

8γ(cid:96)2Ω2
CK

+

(4A + (cid:96) + 8BC/ρ)Ω2
0
K
(cid:19)

+

5γB(cid:98)σ2
ρK

0

+3γ

D1 +

2BD2
ρ

+ 3G2
∗

.

∈

(cid:1)

∗

Finally, we apply (45):

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

(cid:26)

2 min

3Ω2
0,
4A+(cid:96)+8BC/ρ , Ω0,C√ρ
(cid:98)σ0√B

1

,

C
√

+

1
(cid:96)

·

8(cid:96)2Ω2
CK

(cid:27)

K

, Ω0,C
G∗√K

+

+

(4A + (cid:96) + 8BC/ρ)Ω2
0
K
Ω0,
C
(cid:112)K(D1 + 2BD2/ρ)
(A + (cid:96) + BC/ρ)(Ω2
0,
K

C

(cid:32)

= O

Ω0,C

·

ρ

√

K(D1+2BD2/ρ)
γB(cid:98)σ2
ρK
(cid:19)
2BD2
ρ

Ω0,
√
C
(cid:98)σ0
(cid:18)
D1 +

B

0

+

· 3

+ Ω2

0) + (cid:96)Ω2
C

+

Ω0,

Ω0,
√
C
K

· 9G2
∗

+

∗
√

G
C (cid:98)σ0
√
ρK

B

Ω0,
C

(

+

(cid:112)D1 + BD2/ρ + G
K

√

(cid:33)
.

)

∗

37

E SGDA with Arbitrary Sampling: Missing Proofs and Details

Algorithm 1 SGDA-AS: Stochastic Gradient Descent-Ascent with Arvitrary Sampling
1: Input: starting point x0 ∈ Rd, distribution D, stepsize γ > 0, number of steps K
2: for k = 0 to K − 1 do
3:

Sample ξk ∼ D independently from previous iterations and compute gk = Fξk (xk)
xk+1 = proxγR(xk − γgk)

4:

5:
6: end for

E.1 Proof of Proposition 3.2

Proposition E.1 (Proposition 3.2). Let Assumption 3.1 hold. Then, SGDA satisﬁes Assumption 2.1
with

A = (cid:96)

D

, B = 0,

k ≡ 0, D1 = 2σ2
σ2
∗

:= 2 max
X ∗
x∗
ρ = 1, D2 = 0.

∈

C = 0,

(cid:2)(cid:107)Fξ(x∗) − F (x∗)(cid:107)2(cid:3) ,

E

D

Proof. To prove the result, it is suﬃcient to derive an upper bound for Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3):

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) = E
D
≤ 2E
D
(12)
(cid:104)F (xk) − F (x∗
≤ 2(cid:96)

(cid:2)(cid:107)Fξk (xk) − F (x∗
(cid:2)(cid:107)Fξk (xk) − Fξk (x∗

,k)(cid:107)2(cid:3)

D

,k), xk − x∗

,k(cid:105) + 2σ2
∗

,

,k)(cid:107)2(cid:3) + 2E

D

(cid:2)(cid:107)Fξk (x∗

,k) − F (x∗

,k)(cid:107)2(cid:3)

where σ2
∗
with

:= maxx∗

X ∗ E
D

∈

(cid:2)(cid:107)Fξ(x∗) − F (x∗)(cid:107)2(cid:3). The above inequality implies that Assumption 2.1 holds

A = (cid:96)

D

, B = 0,

k ≡ 0, D1 = 2σ2
σ2
∗

:= 2 max
X ∗
x∗
ρ = 1, D2 = 0.

∈

C = 0,

(cid:2)(cid:107)Fξ(x∗) − F (x∗)(cid:107)2(cid:3) ,

E

D

E.2 Analysis of SGDA-AS in the Quasi-Strongly Monotone Case

Plugging the parameters from the above proposition in Theorem 2.2 and Corollary 2.3 we get the following
results.

Theorem E.2. Let F be µ-quasi strongly monotone, Assumption 3.1 hold, and 0 < γ ≤ 1/2(cid:96)D. Then,
for all k ≥ 0 the iterates produced by SGDA-AS satisfy

E (cid:2)(cid:107)xk − x∗

,k(cid:107)2(cid:3) ≤ (1 − γµ)k(cid:107)x0 − x0,

∗(cid:107)2 +

2γσ2
∗µ

.

(46)

Corollary E.3 (Corollary 3.3). Let the assumptions of Theorem E.2 hold. Then, for any K ≥ 0 one

38

can choose {γk}k

0 as follows:

≥

if K ≤

2(cid:96)
D
µ

,

γk =

and k < k0,

γk =

and k ≥ k0,

γk =

1
2(cid:96)
D
1
2(cid:96)

D

4(cid:96)

D

if K >

if K >

2(cid:96)
D
µ
2(cid:96)
D
µ

,

,

2

+ µ(k − k0)

,

(47)

where k0 = (cid:100)K/2(cid:101). For this choice of γk the following inequality holds for SGDA-AS:

E[(cid:107)xK − x∗

,K(cid:107)2] ≤

64(cid:96)
µ

D

(cid:107)x0 − x∗

,0(cid:107)2 exp

(cid:18)

−

µ
2(cid:96)

D

(cid:19)

K

+

72σ2
∗µ2K

.

E.3 Analysis of SGDA-AS in the Monotone Case

In the monotone case, using Theorem 2.5, we establish the new result for SGDA-AS.

Theorem E.4. Let F be monotone (cid:96)-star-cocoercive and Assumptions 2.1, 2.4, 3.1 hold. Assume that
γ ≤ 1/2(cid:96)D. Then for Gap

(z) from (10) and for all K ≥ 0 the iterates produced by SGDA-AS satisfy

C

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

3 maxu

(cid:107)x0 − u(cid:107)2

∈C
2γK

+

8γ(cid:96)2Ω2
CK

(4(cid:96)

D

+

+ (cid:96)) (cid:107)x0 − x∗
K

,0(cid:107)2

+2γ(2 + γ (4(cid:96)

D

+ (cid:96)))σ2
∗

+ 9γ max
X ∗
x∗

∈

(cid:107)F (x∗)(cid:107)2.

Next, we apply Corollary D.4 and get the following rate of convergence to the exact solution.

Corollary E.5. Let the assumptions of Theorem E.4 hold. Then ∀K > 0 and

γ = min

(cid:26) 1
4(cid:96)

D

+ (cid:96)

,

Ω0,
C√
2Kσ
∗

,

Ω0,
√
C
K

G

∗

(cid:27)

(48)

the iterates produced by SGDA-AS satisfy

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

(cid:33)(cid:35)

xk

= O

(cid:32)

((cid:96)

D

+ (cid:96))(Ω2
0,

+ Ω2

0) + (cid:96)Ω2
C

C
K

Ω0,
C

+

(σ
+ G
√
∗
K

)

∗

(cid:33)

.

As we already mentioned before, the above result is new for SGDA-AS: the only known work on
SGDA-AS [Loizou et al., 2021] focuses on the µ-quasi-strongly monotone case only with µ > 0. Moreover,
neglecting the dependence on problem/noise parameters, the derived convergence rate O (1/K + 1/√K) is
standard for the analysis of stochastic methods for solving monotone VIPs [Juditsky et al., 2011].

E.4 Missing Details on Arbitrary Sampling

In the main part of the paper, we discuss the Arbitrary Sampling paradigm and, in particular, using our
general theoretical framework, we obtain convergence guarantees for SGDA under Expected Cocoercivity
assumption (Assumption 3.1). In this section, we give the particular examples of arbitrary sampling
ﬁtting this setup. In all the examples below, we focus on a special case of stochastic reformulation from
(13) and assume that for all i ∈ [n] operator Fi is ((cid:96)i, X ∗)-cocoercive, i.e., for all i ∈ [n] and x ∈ Rd we
have

(cid:107)Fi(x) − Fi(x∗)(cid:107)2 ≤ (cid:96)i(cid:104)Fi(x) − Fi(x∗), x − x∗(cid:105),

(49)

where x∗ is the projection of x on X ∗. Note that (49) holds whenever Fi are cocoercive.

39

let P {ξ = nei} = 1/n for all
Uniform Sampling. We start with the classical uniform sampling:
i ∈ [n], where ei ∈ Rn is the i-th coordinate vector from the standard basis in Rn. Then, E[ξi] = 1 for
all i ∈ [n] and Assumption 3.1 holds with (cid:96)

= maxi

[n] (cid:96)i:

D

∈

(cid:2)(cid:107)Fξ(x) − Fξ(x∗)(cid:107)2(cid:3) =

E

D

(49)
≤

n
(cid:88)

i=1

(cid:88)

1
n

1
n

(cid:107)Fi(x) − Fi(x∗)(cid:107)2

((cid:96)i(cid:104)Fi(x) − Fi(x∗), x − x∗(cid:105))

∈
In this case, Corollaries E.3 and E.5 imply the following rate for SGDA in µ-quasi strongly monotone
and monotone cases respectively:

≤ max
[n]
i

(cid:96)i(cid:104)F (x) − F (x∗), x − x∗(cid:105)

E[(cid:107)xK − x∗

,K(cid:107)2] ≤

64 maxi
µ

[n] (cid:96)i
∈

(cid:107)x0 − x∗

,0(cid:107)2 exp

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

(cid:32)

(maxi

= O

[n] (cid:96)i + (cid:96))(Ω2
0,
∈

K

+ Ω2

0) + (cid:96)Ω2
C

C

Ω0,
C

+

(cid:18)

−

µ
2 maxi

∈

[n] (cid:96)i

(cid:19)

K

+

72σ2
∗
µ2K

,US

,

(σ

,US + G
√
∗
K

)

∗

(cid:33)

,

where σ2
∗

,US := maxx∗

1
n

X ∗

∈

(cid:80)n

i=1 (cid:107)Fi(x∗) − F (x∗)(cid:107)2.

Importance Sampling. Next, we consider a non-uniform sampling strategy – importance sampling:
let P (cid:8)ξ = ei
i=1 (cid:96)i. Then, E[ξi] = 1 for all i ∈ [n] and
n(cid:96)/(cid:96)i
Assumption 3.1 holds with (cid:96)

(cid:9) = (cid:96)i/n(cid:96) for all i ∈ [n], where (cid:96) = 1

= (cid:96):

(cid:80)n

n

D

(cid:2)(cid:107)Fξ(x) − Fξ(x∗)(cid:107)2(cid:3) =

E

D

=

(49)
≤

(cid:96)i
n(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:96)
(cid:96)i

(Fi(x) − Fi(x∗))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:96)
n(cid:96)i

(cid:107)Fi(x) − Fi(x∗)(cid:107)2

(cid:88)

(cid:104)Fi(x) − Fi(x∗), x − x∗(cid:105)

n
(cid:88)

i=1
n
(cid:88)

i=1
(cid:96)
n

In this case, Corollaries E.3 and E.5 imply the following rate for SGDA in µ-quasi strongly monotone
and monotone cases respectively:

≤ (cid:96)(cid:104)F (x) − F (x∗), x − x∗(cid:105)

E[(cid:107)xK − x∗

,K(cid:107)2] ≤

(cid:107)x0 − x∗

,0(cid:107)2 exp

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

xk

(cid:33)(cid:35)

= O

((cid:96) + (cid:96))(Ω2
0,

+ Ω2
C
K

64(cid:96)
µ
(cid:32)

(cid:19)

+

72σ2
∗
µ2K

,IS

,

(cid:18)

−

K

µ
2(cid:96)
0) + (cid:96)Ω2
C

Ω0,
C

+

(σ

,IS + G
√
∗
K

)

∗

(cid:33)

,

∈

1
n

X ∗

i=1

(cid:80)n

,IS := maxx∗

where σ2
∗

(cid:13)
(cid:96)i
(cid:96)
(cid:13)
[n] (cid:96)i and, in fact,
. We emphasize that (cid:96) ≤ maxi
(cid:13)
(cid:96)i
(cid:96)
(cid:96) might be much smaller than maxi
[n] (cid:96)i. Therefore, compared to SGDA with uniform sampling, SGDA
∈
with importance sampling has better exponentially decaying term in the quasi-strongly monotone case
and converges faster to the neighborhood, if executed with constant stepsize. Moreover, σ2
,US,
∗
X ∗ (cid:107)Fi(x∗)(cid:107) ∼ (cid:96)i. In this case, SGDA with importance sampling has better O(1/K) term
when maxx∗
than SGDA with uniform sampling as well.

Fi(x∗) − F (x∗)

,IS ≤ σ2
∗

(cid:13)
2
(cid:13)
(cid:13)

∈

∈

i=1 ξi, where ξi are i.i.d. samples from
Minibatch Sampling With Replacement. Let ξ = 1
b
some distribution D satisfying (13) and Assumption 3.1. Then, the distribution of ξ satisﬁes (13)
and Assumption 3.1 as well with the same constant (cid:96)
. Therefore, minibatched versions of uniform
sampling and importance sampling ﬁt the framework as well with (cid:96)

and

=

= maxi

D

σ2
∗,US
b

D

[n] (cid:96)i, σ2
∗

∈

(cid:80)b

(cid:96)
D

= (cid:96), σ2
∗

=

σ2
∗,IS
b

.

40

Minibatch Sampling Without Replacement. For given batchsize b ∈ [n] we consider the
(cid:9) =
following sampling strategy: for each subset S ⊆ [n] such that |S| = b we have P (cid:8)ξ = n
b!(n
, i.e., S is chosen uniformly at random from all b-element subsets of [n]. In the special case, when

S ei

(cid:80)

i
∈

b)!

b

R(x) ≡ 0, Loizou et al. [2021] show that this sampling strategy satisﬁes (13) and Assumption 3.1 with

−
n!

=

(cid:96)
D

n(b − 1)
b(n − 1)

(cid:96) +

n − b
b(n − 1)

(cid:96)i,

max
[n]
i
∈

=

σ2
∗

n − b
b(n − 1)

,US.

σ2
∗

(50)

Clearly, both parameters are smaller than corresponding parameters for minibatched version of uniform
sampling with replacement, which indicates the theoretical beneﬁts of sampling without replacement.
Plugging the parameters from (50) in Corollaries E.3 and E.5, we get the rate of convergence for this
sampling strategy. Moreover, in the quasi-strongly monotone case, to guarantee E[(cid:107)xK − x∗
,K(cid:107)2] ≤ ε for
some ε > 0, the method requires

(cid:32)

Kb = O

max

(cid:40)(cid:18)

(cid:32)

(cid:40)

= (cid:101)O

max

(n − b)
n

b

+

(cid:96)
µ
b (cid:0)(cid:96) − 1

(cid:19)

[n] (cid:96)i

maxi
∈
µ
(cid:1) + maxi

n maxi
∈

[n] (cid:96)i
µ

log

(cid:96)
D

(cid:107)x0 − x∗
µε

,0(cid:107)2

,

(n − b)σ2
∗
nµ2ε

,US

(cid:41)(cid:33)

[n] (cid:96)i

,

(n − b)σ2
∗
nµ2ε

,US

(cid:41)(cid:33)

oracle calls,

(51)

∈

where (cid:101)O(·) hides numerical and logarithmic factors. One can notice that the ﬁrst term in the maximum
linearly increases in b (since (cid:96) cannot be smaller than 1
[n] (cid:96)i), while the second term linearly
maxi∈[n] (cid:96)i
decreases in b. The ﬁrst term in the maximum is lower bounded by (n
. Therefore, if
−
µ
n

n maxi
∈

b)

∈

∗,US

[n] (cid:96)i ≥

σ2
maxi
µε , the the ﬁrst term in the maximum is always larger than the second one, meaning
that the optimal batchsize, i.e., the batchsize that minimizes oracle complexity (51) neglecting the
σ2
logarithmic terms, equals b
µε , then there exists a positive value of b such
[n] (cid:96)i <
∗
that the ﬁrst term in the maximum equals the second term. This value equals

= 1. Next, if maxi

∗,US

∈

n (cid:0)σ2
σ2
∗

,US − µε maxi
+ µε (cid:0)n(cid:96) − maxi

∗

(cid:1)
(cid:1) .

[n] (cid:96)i
[n] (cid:96)i

∈

∈

One can easily verify that it is always smaller than n, but it can be non integer and it can be smaller
than 1 as well. Therefore, the optimal batchsize is




1,

max



(cid:26)

1,

=

b
∗

(cid:22) n(σ2

∗,US−
∗+µε(n(cid:96)
σ2

µε maxi∈[n] (cid:96)i)
maxi∈[n] (cid:96)i)

−

if maxi

[n] (cid:96)i ≥

∈

∗,US

σ2
µε ,

(cid:23)(cid:27)

, otherwise.

We notice that Loizou et al. [2021] derive the following formula for the optimal batchsize (ignoring
numerical constants):






1,

(cid:26)

max

1,

=

(cid:101)b
∗

(cid:22) n(σ2

∗,US−
∗+µε(n(cid:96)
σ2

µε(maxi∈[n] (cid:96)i

−
maxi∈[n] (cid:96)i)

(cid:96)))

−

if maxi

∈

[n] (cid:96)i − (cid:96) ≥

∗,US

σ2
µε ,

(cid:23)(cid:27)

, otherwise.

However, in terms of (cid:101)O(·) both formulas give the same complexity result.

41

F SGDA with Variance Reduction: Missing Proofs and Details

In this section, we provide missing proofs and details for Section 4.

F.1 L-SVRGDA

Algorithm 2 L-SVRGDA: Loopless Stochastic Variance Reduced Gradient Descent-Ascent
1: Input: starting point x0 ∈ Rd, probability p ∈ (0, 1], stepsize γ > 0, number of steps K
2: Set w0 = x0 and compute F (w0)
3: for k = 0 to K − 1 do
4:

Draw a fresh sample jk from the uniform distribution on [n] and compute gk = Fjk (xk)−

Fjk (wk) + F (wk)

(cid:40)

5:

wk+1 =

xk, with probability p,
wk, with probability 1 − p,

xk+1 = proxγR(xk − γgk)

6:
7: end for

F.1.1 Proof of Proposition 4.3

Lemma F.1. Let Assumption 4.1 hold. Then for all k ≥ 0 L-SVRGDA satisﬁes

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) ≤ 2(cid:98)(cid:96)(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105) + 2σ2
k,

(52)

where σ2

k := 1
n

(cid:80)n

i=1 (cid:107)Fi(wk) − Fi(x∗

,k)(cid:107)2.

Proof. Since gk = Fjk (xk) − Fjk (wk) + F (wk), we have

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) = Ek
1
n

=

(cid:2)(cid:107)Fjk (xk) − Fjk (wk) + F (wk) − F (x∗
n
(cid:88)

(cid:107)Fi(xk) − Fi(wk) + F (wk) − F (x∗

,k)(cid:107)2(cid:3)

,k)(cid:107)2

≤

≤

2
n

2
n

i=1
n
(cid:88)

(cid:107)Fi(xk) − Fi(x∗

,k)(cid:107)2

i=1

+

2
n

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:107)Fi(wk) − Fi(x∗

,k) − (F (wk) − F (x∗

,k))(cid:107)2

(cid:107)Fi(xk) − Fi(x∗

,k)(cid:107)2 +

2
n

n
(cid:88)

i=1

(cid:107)Fi(wk) − Fi(x∗

,k)(cid:107)2

(16)
≤ 2(cid:98)(cid:96)(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105) + 2σ2
k.

Lemma F.2. Let Assumptions 4.1 and 4.2 hold. Then for all k ≥ 0 L-SVRGDA satisﬁes

Ek

(cid:2)σ2

k+1

(cid:3) ≤ p(cid:98)(cid:96)(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105) + (1 − p)σ2
k,

(53)

where σ2

k := 1
n

(cid:80)n

i=1 (cid:107)Fi(wk) − Fi(x∗

,k)(cid:107)2.

42

Proof. Using the deﬁnitions of σ2

k+1 and wk+1 (see (15)), we derive

Ek

(cid:2)σ2

k+1

(cid:3)

=

As. 4.2=

=

(16)
≤

1
n

1
n

p
n

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

Ek

(cid:2)(cid:107)Fi(wk+1) − Fi(x∗

,k+1)(cid:107)2(cid:3)

Ek

(cid:2)(cid:107)Fi(wk+1) − Fi(x∗

,k)(cid:107)2(cid:3)

(cid:107)Fi(xk) − Fi(x∗

,k)(cid:107)2 +

1 − p
n

n
(cid:88)

i=1

(cid:107)Fi(wk) − Fi(x∗

,k)(cid:107)2

p(cid:98)(cid:96)(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105) + (1 − p)σ2
k.

The above two lemmas imply that Assumption 2.1 is satisﬁed with certain parameters.

Proposition F.3 (Proposition 4.3). Let Assumptions 4.1 and 4.2 hold. Then, L-SVRGDA satisﬁes
Assumption 2.1 with

A = (cid:98)(cid:96), B = 2,

σ2

k =

1
n

n
(cid:88)

i=1

(cid:107)Fi(wk) − Fi(x∗)(cid:107)2, C =

p(cid:98)(cid:96)
,
2

ρ = p, D1 = D2 = 0.

F.1.2 Analysis of L-SVRGDA in the Quasi-Strongly Monotone Case

Plugging the parameters from the above proposition in Theorem 2.2 and Corollary 2.3 with M = 4
get the following results.

p we

Theorem F.4. Let F be µ-quasi strongly monotone, Assumptions 4.1, 4.2 hold, and 0 < γ ≤ 1/6(cid:98)(cid:96).
Then for all k ≥ 0 the iterates produced by L-SVRGDA satisfy

E (cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤ (1 − min {γµ, p/2})k V0,

(54)

where V0 = (cid:107)x0 − x∗(cid:107)2 + 4γ2σ2

0/p.

Corollary F.5. Let the assumptions of Theorem F.4 hold. Then, for p = n, γ = 1/6(cid:98)(cid:96) and any K ≥ 0
we have

E[(cid:107)xk − x∗(cid:107)2] ≤ V0 exp

− min

(cid:18)

(cid:26) µ
,
6(cid:98)(cid:96)

1
2n

(cid:27)

(cid:19)

K

.

F.1.3 Analysis of L-SVRGDA in the Monotone Case

Next, using Theorem 2.5, we establish the convergence of L-SVRGDA in the monotone case.

Theorem F.6. Let F be monotone, (cid:96)-star-cocoercive and Assumptions 2.1, 2.4, 4.1, 4.2 hold. Assume
that γ ≤ 1/6(cid:98)(cid:96). Then for Gap
C

(z) from (10) and for all K ≥ 0 the iterates of L-SVRGDA satisfy

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

3 maxu

≤

(cid:107)x0 − u(cid:107)2

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+

(cid:16)

12(cid:98)(cid:96) + (cid:96)

(cid:17)

(cid:107)x0 − x∗

,0(cid:107)2

K

(cid:16)

+

4 +

(cid:16)

(cid:17)

12(cid:98)(cid:96) + (cid:96)

γ

(cid:17) 2γσ2
0
pK

+ 9γ max
X ∗
x∗

∈

(cid:107)F (x∗)(cid:107)2.

Applying Corollary D.4, we get the rate of convergence to the exact solution.

43

Corollary F.7. Let the assumptions of Theorem F.6 hold and p = 1/n. Then ∀K > 0 one can choose
γ as

(cid:40)

1
12(cid:98)(cid:96) + (cid:96)

,

(cid:112)

1

,

2n(cid:98)(cid:96)(cid:96)

Ω0,
√
C
K

G

∗

(cid:41)

.

(55)

γ = min

This choice of γ implies

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk



= O



((cid:98)(cid:96) + (cid:96))(Ω2
0,

C

+ Ω2

0) +
K

(cid:112)

n(cid:98)(cid:96)(cid:96)Ω2
0,

C

+ (cid:96)Ω2
C

+



 .

G
Ω0,
∗√
C
K

Proof. First of all, (16), (4), and Cauchy-Schwarz inequality imply

σ2
0 =

1
n

n
(cid:88)

i=1

(cid:107)Fi(x0) − Fi(x∗)(cid:107)2

(16)
≤ (cid:98)(cid:96)(cid:104)F (x0) − F (x∗), x0 − x∗(cid:105)
≤ (cid:98)(cid:96)(cid:107)F (x0) − F (x∗)(cid:107) · (cid:107)x0 − x∗(cid:107)
≤ (cid:98)(cid:96)(cid:96)(cid:107)x0 − x∗(cid:107)2 ≤ (cid:98)(cid:96)(cid:96) max
∈C

u

(cid:107)x0 − u(cid:107)2 ≤ (cid:98)(cid:96)(cid:96)Ω2
0,

.

C

Next, applying Corollary D.4 with (cid:98)σ0 :=

(cid:112)

(cid:98)(cid:96)(cid:96)Ω0,
C

, we get the result.

F.2 SAGA-SGDA

In this section, we show that SAGA-SGDA [Palaniappan and Bach, 2016] ﬁts our theoretical framework
and derive new results for this method under averaged star-cocoercivity.

Algorithm 3 SAGA-SGDA [Palaniappan and Bach, 2016]
1: Input: starting point x0 ∈ Rd, stepsize γ > 0, number of steps K
2: Set w0
3: for k = 0 to K − 1 do
4:

i = x0 and compute Fi(w0

i ) for all i ∈ [n]

Draw a fresh sample jk from the uniform distribution on [n] and compute gk = Fjk (xk)−

5:

Fjk (wk

(cid:80)n
jk ) + 1
i=1 Fi(wk
i )
n
Set wk+1
= xk and wk+1
i = wk
jk
xk+1 = proxγR(xk − γgk)

6:
7: end for

i for i (cid:54)= jk

F.2.1

SAGA-SGDA Fits Assumption 2.1

Lemma F.8. Let Assumption 4.1 hold. Then for all k ≥ 0 SAGA-SGDA satisﬁes

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) ≤ 2(cid:98)(cid:96)(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105) + 2σ2
k,

(56)

where σ2

k := 1
n

(cid:80)n

i=1 (cid:107)Fi(wk

i ) − Fi(x∗

,k)(cid:107)2.

Proof. For brevity, we introduce a new notation: Sk = 1
n

44

(cid:80)n

i=1 Fi(wk

i ). Since gk = Fjk (xk) − Fjk (wk

jk ) +

Sk, we have

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) = Ek
1
n

=

(cid:2)(cid:107)Fjk (xk) − Fjk (wk
n
(cid:88)

(cid:107)Fi(xk) − Fi(wk

jk ) + Sk − F (x∗

,k)(cid:107)2(cid:3)

i ) + Sk − F (x∗

,k)(cid:107)2

i=1
n
(cid:88)

(cid:107)Fi(xk) − Fi(x∗

,k)(cid:107)2

≤

≤

2
n

2
n

i=1

+

2
n

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:107)Fi(wk

i ) − Fi(x∗

,k) − (Sk − F (x∗

,k))(cid:107)2

(cid:107)Fi(xk) − Fi(x∗

,k)(cid:107)2 +

2
n

n
(cid:88)

i=1

(cid:107)Fi(wk

i ) − Fi(x∗

,k)(cid:107)2

(16)
≤ 2(cid:98)(cid:96)(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105) + 2σ2
k.

Lemma F.9. Let Assumptions 4.1 and 4.2 hold. Then for all k ≥ 0 SAGA-SGDA satisﬁes

Ek

(cid:2)σ2

k+1

(cid:3) ≤

(cid:98)(cid:96)
n

(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105) + (1 − 1/n)σ2
k,

(57)

where σ2

k := 1
n

(cid:80)n

i=1 (cid:107)Fi(wk

i ) − Fi(x∗

,k)(cid:107)2.

Proof. Using the deﬁnitions of σ2

k+1 and wk+1

i

, we derive

Ek

(cid:2)σ2

k+1

(cid:3)

=

As. 4.2=

=

(16)
≤

n
(cid:88)

i=1
n
(cid:88)

Ek

(cid:2)(cid:107)Fi(wk+1

i

) − Fi(x∗

,k+1)(cid:107)2(cid:3)

Ek

(cid:2)(cid:107)Fi(wk+1

i

) − Fi(x∗

,k)(cid:107)2(cid:3)

1
n

1
n

i=1
n
(cid:88)

i=1

1
n2

(cid:107)Fi(xk) − Fi(x∗

,k)(cid:107)2 +

1 − 1/n
n

n
(cid:88)

i=1

(cid:107)Fi(wk

i ) − Fi(x∗

,k)(cid:107)2

(cid:98)(cid:96)
n

(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105) + (1 − 1/n)σ2
k.

The above two lemmas imply that Assumption 2.1 is satisﬁed with certain parameters.

Proposition F.10. Let Assumptions 4.1 and 4.2 hold. Then, SAGA-SGDA satisﬁes Assumption 2.1
with

A = (cid:98)(cid:96), B = 2,

σ2

k =

1
n

n
(cid:88)

i=1

(cid:107)Fi(wk

i ) − Fi(x∗)(cid:107)2, C =

(cid:98)(cid:96)
2n

,

ρ =

1
n

, D1 = D2 = 0.

F.2.2 Analysis of SAGA-SGDA in the Quasi-Strongly Monotone Case

Applying Theorem 2.2 and Corollary 2.3 with M = 4n, we get the following results.

Theorem F.11. Let F be µ-quasi strongly monotone, Assumptions 4.1, 4.2 hold, and 0 < γ ≤ 1/6(cid:98)(cid:96).

45

Then for all k ≥ 0 the iterates produced by SAGA-SGDA satisfy

E (cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤ (1 − min {γµ, 1/2n})k V0,

(58)

where V0 = (cid:107)x0 − x∗(cid:107)2 + 4nγ2σ2
0.

Corollary F.12. Let the assumptions of Theorem F.11 hold. Then, for γ = 1/6(cid:98)(cid:96) and any K ≥ 0 we
have

E[(cid:107)xK − x∗(cid:107)2] ≤ V0 exp

− min

(cid:18)

(cid:26) µ
,
6(cid:98)(cid:96)

1
2n

(cid:27)

(cid:19)

K

.

F.2.3 Analysis of SAGA-SGDA in the Monotone Case

Next, using Theorem 2.5, we establish the convergence of SAGA-SGDA in the monotone case.

Theorem F.13. Let F be monotone, (cid:96)-star-cocoercive and Assumptions 2.1, 2.4, 4.1, 4.2 hold. Assume
(z) from (10) and for all K ≥ 0 the iterates produced by SAGA-SGDA
that γ ≤ 1/6(cid:98)(cid:96). Then for Gap
C
satisfy

(cid:34)

(cid:32)

E

Gap

C

1
K

K
(cid:88)

k=1

xk

(cid:33)(cid:35)

3 maxu

≤

(cid:107)x0 − u(cid:107)2

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+

(cid:16)

12(cid:98)(cid:96) + (cid:96)

(cid:17)

(cid:107)x0 − x∗

,0(cid:107)2

K

(cid:16)

+

4 +

(cid:16)

(cid:17)

12(cid:98)(cid:96) + (cid:96)

γ

(cid:17) 2γσ2
0
pK

+ 9γ max
X ∗
x∗

∈

(cid:107)F (x∗)(cid:107)2.

Applying Corollary D.4, we get the rate of convergence to the exact solution.

Corollary F.14. Let the assumptions of Theorem F.13 hold. Then ∀K > 0 one can choose γ as

γ = min

(cid:40)

1
12(cid:98)(cid:96) + (cid:96)

,

(cid:112)

1

,

2n(cid:98)(cid:96)(cid:96)

(cid:41)

,

(59)

Ω0,
√
C
K

G

∗

This choice of γ implies

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk



= O



((cid:98)(cid:96) + (cid:96))(Ω2
0,

C

+ Ω2

0) +
K

(cid:112)

n(cid:98)(cid:96)(cid:96)Ω2
0,

C

+ (cid:96)Ω2
C

+



 .

G
Ω0,
∗√
C
K

Proof. Since σ0 for SAGA-SGDA and L-SVRGDA are the same, the proof of this corollary is identical to
the one for Corollary F.7.

F.3 Discussion of the Results in the Monotone Case

Among the papers mentioned in the related work on variance-reduced methods (see Section ??), only
Alacaoglu and Malitsky [2021], Carmon et al. [2019], Alacaoglu et al. [2021], Tominin et al. [2021],
Luo et al. [2021] consider monotone (convex-concave) and Lipschitz (smooth) VIPs (min-max problems)
without assuming strong monotonicity (strong-convexity-strong-concavity) of the problem. In this case,
(cid:17)
Alacaoglu and Malitsky [2021] derive O
convergence rate (neglecting the dependence on the
quantities like Ω2
(cid:107)x0 − u(cid:107)2), which is optimal for the considered setting [Han et al., 2021].
0,
Under additional assumptions a similar rate is derived in Carmon et al. [2019]. Tominin et al. [2021], Luo
et al. [2021] also achieve this rate but using Catalyst. Finally, Alacaoglu et al. [2021] derive O (cid:0)n + nL
(cid:1),
which is worse than the one from Alacaoglu and Malitsky [2021].

n + √nL
K

= maxu

∈C

(cid:16)

K

C

Our results for monotone and star-cocoercive regularized VIPs give O

typically worse than O

(cid:16)

n + √nL
K

(cid:17)

rate from Alacaoglu and Malitsky [2021] due to the relation between

46

(cid:18) √

(cid:19)

n(cid:96)(cid:98)(cid:96)+(cid:98)(cid:96)
K + G∗
√K

rate, which is

cocoercivity constants and Lipschitz constants (even when R(x) ≡ 0, i.e., G
= 0). However, in general,
it is possible that star-cocoercivity holds, while Lipschitzness does not [Loizou et al., 2021]. Moreover,
we emphasize here that Alacaoglu and Malitsky [2021] and other works do not consider SGDA as the
basis for their methods. To the best of our knowledge, our results are the ﬁrst ones for variance-reduced
SGDA-type methods derived in the monotone case without assuming (quasi-)strong monotonicity.

∗

47

G Distributed SGDA with Compression: Missing Proofs and De-

tails

In this section, we provide missing proofs and details for Section 5.

G.1 QSGDA

In this section (and in the one about DIANA-SGDA), we assume that each Fi has an expectation form:
Fi(x) = Eξi

i[Fξi(x)].

∼D

Algorithm 4 QSGDA: Quantized Stochastic Gradient Descent-Ascent
1: Input: starting point x0 ∈ Rd, stepsize γ > 0, number of steps K
2: for k = 0 to K − 1 do
3:

4:

5:

6:

7:

Broadcast xk to all workers
for i = 1, . . . , n in parallel do
i and send Q(gk

Compute gk

end for
gk = 1
n
xk+1 = proxγR

(cid:80)n

i=1 Q(gk
i )

(cid:0)xk − γgk(cid:1)

8:
9: end for

i ) to the server

G.1.1 Proof of Proposition 5.3

Proposition G.1 (Proposition 5.3). Let F be (cid:96)-star-cocoercive and Assumptions 4.1, 5.2 hold. Then,
QSGDA with quantization (17) satisﬁes Assumption 2.1 with

A =

(cid:32)

3(cid:96)
2

+

(cid:33)

9ω(cid:98)(cid:96)
2n

, D1 =

3(1 + 3ω)σ2 + 9ωζ 2
∗
n

,

σ2
k = 0, B = 0,

where σ2 = 1
n

(cid:80)n

i=1 σ2

i and ζ 2
∗

C = 0,

= 1

n maxx

X ∗

∗∈

ρ = 1, D2 = 0,
i=1 (cid:107)Fi(x∗)(cid:107)2(cid:105)

.

(cid:104)(cid:80)n

Proof. Since gk = 1
1
n
are independent for ﬁxed xk, we have

(cid:1), Q (cid:0)gk

Q (cid:0)gk

i

n
(cid:80)
i=1

(cid:1) , . . . , Q (cid:0)gk

n

(cid:1) are independent for ﬁxed gk

1 , . . . , gk

n, and gk

1 , . . . , gk
n

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) = Ek

= Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+3 (cid:13)

Q (cid:0)gk

i

(cid:1) − F (x∗

,k)

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:2)Q (cid:0)gk

i

(cid:1) − gk

i + gk

i − Fi(xk)(cid:3) + F (xk) − F (x∗

1
n

1
n

n
(cid:88)

i=1

n
(cid:88)

i=1

2



(cid:13)
(cid:13)
,k)
(cid:13)
(cid:13)
(cid:13)
2

≤ 3Ek

1
n

n
(cid:88)

i=1

[Q (cid:0)gk

i

(cid:1) − gk
i ]

2
 + 3Ek

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

[gk

(cid:13)
(cid:13)
i − Fi(xk)]
(cid:13)
(cid:13)
(cid:13)



(cid:13)F (xk) − F (x∗

,k)(cid:13)
2
(cid:13)

=

3
n2

n
(cid:88)

i=1

Ek

(cid:104)(cid:13)
(cid:13)Q (cid:0)gk

i

(cid:1) − gk

i

2(cid:105)

(cid:13)
(cid:13)

+

3
n2

n
(cid:88)

i=1

Ek

+3 (cid:13)

(cid:13)F (xk) − F (x∗

,k)(cid:13)
2
(cid:13)

.

48

(cid:104)(cid:13)
(cid:13)gk

i − Fi(xk)(cid:13)
(cid:13)

2(cid:105)

Next, we use Assumption 5.2, σ2 = 1
n

(cid:80)n

i=1 σ2

i , and the deﬁnition of quantization (17) and get

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) ≤

≤

≤

n
(cid:88)

i=1
n
(cid:88)

Ek

(cid:104)(cid:13)
(cid:13)gk
i

2(cid:105)

(cid:13)
(cid:13)

+

3σ2
n

+ 3 (cid:13)

(cid:13)F (xk) − F (x∗

,k)(cid:13)
2
(cid:13)

Ek

(cid:104)(cid:13)
(cid:13)gk

i − Fi(xk) + Fi(xk) − Fi(x∗

,k) + Fi(x∗

2(cid:105)

,k)(cid:13)
(cid:13)

+ 3 (cid:13)

(cid:13)F (xk) − F (x∗

,k)(cid:13)
2
(cid:13)

Ek

(cid:104)(cid:13)
(cid:13)gk

i − Fi(xk)(cid:13)
(cid:13)

2(cid:105)

+

9ω
n2

n
(cid:88)

i=1

(cid:104)(cid:13)
(cid:13)Fi(xk) − Fi(x∗

Ek

2(cid:105)

,k)(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:104)(cid:13)
(cid:13)Fi(x∗

,k)

2(cid:105)

(cid:13)
(cid:13)

Ek

+

3σ2
n

+ 3

(cid:13)
(cid:13)F (xk) − F (x∗

,k)

(cid:13)
2
(cid:13)

(18)
≤

9ω
n2

n
(cid:88)

(cid:104)(cid:13)
(cid:13)Fi(xk) − Fi(x∗

Ek

2(cid:105)

(cid:13)
,k)
(cid:13)

+ 3

(cid:13)
(cid:13)F (xk) − F (x∗

(cid:13)
2
,k)
(cid:13)

n
(cid:88)

i=1

(cid:104)(cid:13)
(cid:13)Fi(x∗

,k)

2(cid:105)

(cid:13)
(cid:13)

Ek

+

3(1 + 3ω)σ2
n

.

3ω
n2

3ω
n2

9ω
n2

+

i=1
3σ2
n
n
(cid:88)

i=1
9ω
n2

+

i=1
9ω
n2

+

Star-cocoercivity of F and Assumption 4.1 give

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) ≤

(cid:18)

3(cid:96) +

(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105)

+

9ω
n2

(cid:104)(cid:13)
(cid:13)Fi(x∗

Ek

2(cid:105)

,k)(cid:13)
(cid:13)

+

3(1 + 3ω)σ2
n

(cid:18)

≤

3(cid:96) +

(cid:104)F (xk) − F (x∗

,k), xk − x∗

,k(cid:105)

(cid:19)
(cid:98)(cid:96)

9ω
n
n
(cid:88)

i=1
9ω
n

(cid:19)
(cid:98)(cid:96)

+

9ω
n2 max

x

X ∗

∗∈

(cid:35)

(cid:107)Fi(x∗)(cid:107)2

+

(cid:34) n
(cid:88)

i=1

3(1 + 3ω)σ2
n

.

G.1.2 Analysis of QSGDA in the Quasi-Strongly Monotone Case

Applying Theorem 2.2 and Corollary 2.3, we get the following results.

Theorem G.2. Let F be µ-quasi strongly monotone, (cid:96)-star-cocoercive, Assumptions 4.1, 5.2 hold,
and

0 < γ ≤

1
3(cid:96) + 9ω(cid:98)(cid:96)
n

.

Then, for all k ≥ 0 the iterates produced by QSGDA satisfy

E (cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤ (1 − γµ)k (cid:107)x0 − x∗(cid:107)2 + γ

3(1 + 3ω)σ2 + 9ωζ 2
∗
nµ

.

Corollary G.3. Let the assumptions of Theorem G.2 hold. Then, for any K ≥ 0 one can choose

49

{γk}k

≥

0 as follows:

if K ≤

(cid:32)

·

3(cid:96) +

(cid:33)

9ω(cid:98)(cid:96)
n

1
µ

(cid:32)

,

γk =

3(cid:96) +

and k < k0,

γk =

3(cid:96) +

(cid:32)

if K >

if K >

(cid:32)

3(cid:96) +

(cid:32)

3(cid:96) +

(cid:33)

9ω(cid:98)(cid:96)
n

(cid:33)

9ω(cid:98)(cid:96)
n

1
µ

1
µ

·

·

and k ≥ k0,

γk =

2
(6(cid:96) + 18ω(cid:98)(cid:96)/n + µ(k − k0))

,

(cid:33)−

9ω(cid:98)(cid:96)
n

(cid:33)−

9ω(cid:98)(cid:96)
n

1

1

,

,

where k0 = (cid:100)K/2(cid:101). For this choice of γk the following inequality holds:

E[(cid:107)xK − x∗

,K(cid:107)2] ≤

32(3(cid:96) + 9ω(cid:98)(cid:96)/n)
µ

(cid:107)x0 − x∗

,0(cid:107)2 exp

(cid:18)

−

(cid:19)

µ
(3(cid:96) + 9ω(cid:98)(cid:96)/n)

K

+

36
µ2K

·

3(1 + 3ω)σ2 + 9ωζ 2
∗
n

.

G.1.3 Analysis of QSGDA in the Monotone Case

Next, using Theorem 2.5, we establish the convergence of QSGDA in the monotone case.

Theorem G.4. Let F be monotone, (cid:96)-star-cocoercive and Assumptions 2.1, 2.4, 4.1, 5.2 hold. Assume

. Then for Gap
C

(z) from (10) and for all K ≥ 0 the iterates produced by QSGDA

(cid:16)

3(cid:96) + 9ω(cid:98)(cid:96)
n

1

(cid:17)−

that γ ≤
satisfy

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

xk

(cid:33)(cid:35)

≤

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

(cid:32)

+

7(cid:96) +

(cid:33)

18ω(cid:98)(cid:96)
n

·

(cid:107)x0 − x∗

,0(cid:107)2

K

(cid:32)

+γ

2 + γ

+9γ max
X ∗
x∗

∈

(cid:33)(cid:33)

(cid:32)

7(cid:96) +

18ω(cid:98)(cid:96)
n
(cid:2)(cid:107)F (x∗)(cid:107)2(cid:3)

·

3(1 + 3ω)σ2 + 9ωζ 2
∗
n

Applying Corollary D.4, we get the rate of convergence to the exact solution.

Corollary G.5. Let the assumptions of Theorem G.4 hold. Then ∀K > 0 one can choose γ as

(cid:40)

γ = min

1
7(cid:96) + 18ω(cid:98)(cid:96)

n

,

(cid:112)

This choice of γ implies

√

n

Ω0,
C

3K(1 + 3ω)σ2 + 9Kωζ 2
∗

(cid:41)

.

,

Ω0,
√
C
K

G

∗

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

(cid:33)(cid:35)

xk

= O

(cid:32) (cid:0)(cid:96) + ω(cid:98)(cid:96)/n(cid:1) (Ω2
0,
C
K

+ Ω2

0) + (cid:96)Ω2
C

Ω0,
C

(σ

+

√

1 + ω + G
√
∗
nK

√

n + ζ
∗

√

(cid:33)

ω)

.

G.2 DIANA-SGDA

G.2.1 Proof of Proposition 5.4

The following result follows from Lemmas 1 and 2 from Horv´ath et al. [2019]. It holds in our settings as
well, since it does not rely on the exact form of Fi(xk).

50

Algorithm 5 DIANA-SGDA: DIANA Stochastic Gradient Descent-Ascent Mishchenko et al.
[2019], Horv´ath et al. [2019]
1: Input: starting points x0, h0

i , stepsizes γ, α > 0, number of

n ∈ Rd, h0 = 1
n

1, . . . , h0

i=1 h0

(cid:80)n

steps K

2: for k = 0 to K − 1 do
3:

Broadcast xk to all workers
for i = 1, . . . , n in parallel do
i and ∆k
i = gk
i ) to the server
i + αQ(∆k
i )

i − hk
i

4:

5:

6:

7:

8:

9:

10:

11:

Compute gk
Send Q(∆k
hk+1
i = hk
end for
gk = hk + 1
n

n
(cid:80)
i=1
xk+1 = proxγR
hk+1 = hk + α 1
n

n
(cid:80)
i=1

(hk

i + Q(∆k

i ))

i ) = 1
Q(∆k
n
(cid:0)xk − γgk(cid:1)
n
(cid:80)
Q(∆k
i=1

i ) = 1
n

n
(cid:80)
i=1

hk
i

12: end for

Lemma G.6 (Lemmas 1 and 2 from Horv´ath et al. [2019]). Let Assumptions 4.2, 5.2 hold. Suppose
that α ≤ 1/(1+ω). Then, for all k ≥ 0 DIANA-SGDA satisﬁes

Ek

(cid:2)gk(cid:3) = F (xk),

Ek

(cid:2)(cid:107)gk − F (x∗)(cid:107)2(cid:3) ≤

(cid:18)

1 +

2ω
n

(cid:19) 1
n

Ek

(cid:2)σ2

k+1

(cid:3) ≤ (1 − α)σ2

k +

i=1
n
α
(cid:88)
n

i=1

n
(cid:88)

(cid:107)Fi(xk) − Fi(x∗)(cid:107)2 +

2ωσ2
k
n

+

(1 + ω)σ2
n

,

(cid:107)Fi(xk) − Fi(x∗)(cid:107)2 + ασ2,

where σ2

k = 1
n

n
(cid:80)
i=1

(cid:107)hk

i − Fi(x∗)(cid:107)2 and σ2 = 1
n

(cid:80)n

i=1 σ2
i .

The lemma above implies that Assumption 2.1 is satisﬁed with certain parameters.

Proposition G.7 (Proposition 5.4). Let Assumptions 4.1, 4.2, 5.2 hold. Suppose that α ≤ 1
Then, DIANA-SGDA with quantization (17) satisﬁes Assumption 2.1 with σ2
and

1+ω .
i − Fi(x∗)(cid:107)2

i=1 (cid:107)hk

k = 1
n

(cid:80)n

A =

(cid:18) 1
2

(cid:19)

ω
n

+

(cid:98)(cid:96), B =

2ω
n

, D1 =

(1 + ω)σ2
n

, C =

α(cid:98)(cid:96)
,
2

ρ = α, D2 = ασ2.

Proof. To get the result, one needs to apply Assumption 4.1 to estimate 1
n
Lemma G.6.

(cid:80)n

i=1 (cid:107)Fi(xk) − Fi(x∗)(cid:107)2 from

G.2.2 Analysis of DIANA-SGDA in the Quasi-Strongly Monotone Case

Applying Theorem 2.2 and Corollary 2.3 with M = 4ω

αn , we get the following results.

Theorem G.8. Let F be µ-quasi strongly monotone, Assumptions 4.1, 4.2, 5.2 hold, α ≤ 1/(1+ω), and

0 < γ ≤

1
(cid:0)1 + 6ω

n

(cid:1)

.
(cid:98)(cid:96)

51

Then, for all k ≥ 0 the iterates produced by DIANA-SGDA satisfy

E (cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤

(cid:16)

1 − min

(cid:111)(cid:17)k

(cid:110)

γµ,

α
2

E[V0] +

γ2σ2(1 + 5ω)
n · min {γµ, α/2}

,

where V0 = (cid:107)x0 − x∗(cid:107)2 + 4ωγ2σ2

0/αn.

Corollary G.9. Let the assumptions of Theorem 5.4 hold. Then, for any K ≥ 0 one can choose
α = 1/(1+ω) and {γk}k

0 as follows:

≥

if K ≤

h
µ

,

γk =

and k < k0,

γk =

1
h
1
h

,

,

and k ≥ k0,

γk =

2
2h + µ(k − k0)

,

if K >

if K >

h
µ
h
µ

where h = max
holds:

(cid:110)(cid:0)1 + 6ω

n

(cid:1)

(cid:98)(cid:96), 2µ(1 + ω)

, k0 = (cid:100)K/2(cid:101). For this choice of γk the following inequality

(cid:111)

E[(cid:107)xK − x∗

,K(cid:107)2] ≤ 32 max

(cid:1)

(cid:40) (cid:0)1 + 6ω
µ

n

(cid:41)

(cid:98)(cid:96)
, 2(1 + ω)

(cid:32)

(cid:40)

V0 exp

− min

µ
(cid:98)(cid:96)(1 + 6ω
n )

,

1
1 + ω

(cid:41)

(cid:33)

K

+

36(1 + 5ω)σ2
µ2nK

.

G.2.3 Analysis of DIANA-SGDA in the Monotone Case

Next, using Theorem 2.5, we establish the convergence of DIANA-SGDA in the monotone case.

Theorem G.10. Let F be monotone, (cid:96)-star-cocoercive and Assumptions 2.1, 2.4, 4.1, 4.2, 5.2 hold.
Assume that

0 < γ ≤

1
(cid:0)1 + 4ω

n

(cid:1)

.
(cid:98)(cid:96)

(z) from (10) and for all K ≥ 0 the iterates produced by DIANA-SGDA satisfy

Then for Gap
C
(cid:32)

(cid:34)

E

Gap
C

1
K

(cid:33)(cid:35)

K
(cid:88)

k=1

xk

≤

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK
(cid:32)

(cid:32)

+

8γ(cid:96)2Ω2
CK
(cid:33)(cid:33)

+

4 + γ

2(cid:98)(cid:96) +

12ω(cid:98)(cid:96)
n

+ (cid:96)

γBσ2
0
ρK

(cid:32)

+

2(cid:98)(cid:96) +

(cid:33)

+ (cid:96)

12ω(cid:98)(cid:96)
n

(cid:107)x0 − x∗

,0(cid:107)2

K

(cid:32)(cid:32)

(cid:32)

+γ

2 + γ

2(cid:98)(cid:96) +

12ω(cid:98)(cid:96)
n

+9γ max
X ∗
x∗

∈

(cid:107)F (x∗)(cid:107)2.

+ (cid:96)

(cid:33)(cid:33) (cid:18) (1 + 5ω)σ2

(cid:19)(cid:33)

n

Applying Corollary D.4, we get the rate of convergence to the exact solution.

Corollary G.11. Let the assumptions of Theorem G.10 hold. Then ∀K > 0 one can choose γ as

γ = min






(cid:32)

(cid:96) + 2(cid:98)(cid:96) +

(cid:33)−

12ω(cid:98)(cid:96)
n

,

Ω0,
C
σ(cid:112)K (1+3ω)/n

,






,

Ω0,
√
C
K

G

∗

1

√

,

αn

(cid:112)

2ω(cid:98)(cid:96)(cid:96)

52

This choice of γ implies that E

(cid:104)
Gap
C

(cid:16) 1
K

(cid:80)K

k=1 xk(cid:17)(cid:105)

equals



O



((cid:96) + (cid:98)(cid:96) + ω(cid:98)(cid:96)/n)(Ω2
0,
K

C

+ Ω2

0) + (cid:96)Ω2
C

+

Ω2
0,
√
C

√

(cid:112)

(cid:98)(cid:96)(cid:96)
αnK

ω

Ω0,
C

(

+

(cid:112)(1+ω)σ2/n + G
K

√

)

∗



 .

Proof. The proof follows from the next upper bound (cid:98)σ2

0 for σ2

0 with initialization h0

i = Fi(x0)

σ2
0 =

1
n

n
(cid:88)

i=1

(cid:107)Fi(x0) − Fi(x∗)(cid:107)2

≤ (cid:98)(cid:96)(cid:104)F (x0) − F (x∗), x0 − x∗(cid:105)
≤ (cid:98)(cid:96)(cid:107)F (x0) − F (x∗)(cid:107) · (cid:107)x0 − x∗(cid:107)
≤ (cid:98)(cid:96)(cid:96)(cid:107)x0 − x∗(cid:107)2 ≤ (cid:98)(cid:96)(cid:96) max
∈C

u

(cid:107)x0 − u(cid:107)2 ≤ (cid:98)(cid:96)(cid:96)Ω2
0,

.

C

Next, applying Corollary D.4 with (cid:98)σ0 :=

(cid:112)

(cid:98)(cid:96)(cid:96)Ω0,
C

, we get the result.

G.3 VR-DIANA-SGDA

In this section, we assume that each Fi has a ﬁnite-sum form: Fi(x) = 1
m

(cid:80)m

j=1 Fij(x).

Algorithm 6 VR-DIANA-SGDA: VR-DIANA Stochastic Gradient Descent-Ascent Horv´ath et al.
[2019]

1: Input: starting points x0, h0

1, . . . , h0

n ∈ Rd, h0 = 1
n

γ, α > 0, number of steps K,

n
(cid:80)
i=1

h0
i , probability p ∈ (0, 1] stepsizes

from the uniform distribution on [m] and compute gk

i =

2: for k = 0 to K − 1 do
3:

Broadcast xk to all workers
for i = 1, . . . , n in parallel do
Draw a fresh sample jk
i

Fijk

i

(xk) − Fijk

wk+1

i =

i

(wk
(cid:40)

i ) + Fi(wk
i )
xk, with probability p,
wk
i − hk
i

i , with probability 1 − p,

i = gk
∆k
Send Q(∆k
hk+1
i = hk
end for
gk = hk + 1
n

n
(cid:80)
i=1
xk+1 = proxγR
hk+1 = hk + α 1
n

i ) to the server
i + αQ(∆k
i )

n
(cid:80)
i=1

(hk

i + Q(∆k

i ))

i ) = 1
Q(∆k
n
(cid:0)xk − γgk(cid:1)
n
(cid:80)
Q(∆k
i=1

i ) = 1
n

n
(cid:80)
i=1

hk
i

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14: end for

G.3.1 Proof of Proposition 5.6

Lemma G.12 (Modiﬁcation of Lemmas 3 and 7 from Horv´ath et al. [2019]). Let F be (cid:96)-star-cocoercive

53

and Assumptions 4.1, 4.2, 5.5 hold. Then for all k ≥ 0 VR-DIANA-SGDA satisﬁes

Ek

(cid:2)gk(cid:3) = F (xk),

Ek

(cid:2)(cid:107)gk − F (x∗)(cid:107)(cid:3) ≤

(cid:32)

(cid:96) +

(cid:33)

2(cid:101)(cid:96)
n

+

2ω((cid:98)(cid:96) + (cid:101)(cid:96))
n

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105) +

2(ω + 1)
n

σ2
k,

where σ2

k = H k

n + Dk

nm with H k =

n
(cid:80)
i=1

(cid:13)
(cid:13)hk

i − Fi(x∗)(cid:13)
2
(cid:13)

and Dk =

n
(cid:80)
i=1

m
(cid:80)
j=1

(cid:13)
(cid:13)Fij(wk

i ) − Fij(x∗)(cid:13)
2
(cid:13)

.

Proof. First of all, we derive unbiasedness:

E (cid:2)gk(cid:3) =

1
n

n
(cid:88)

i=1

E (cid:2)Q(gk

i − hk

i ) + hk
i

(cid:3) =

1
n

n
(cid:88)

i=1

E (cid:2)gk

i − hk

i + hk
i

(cid:3) =

1
n

n
(cid:88)

i=1

Fi(xk) = F (xk).

By deﬁnition of the variance we get

(cid:104)(cid:13)
(cid:13)
(cid:13)gk − F (x∗)
(cid:13)

2(cid:105)

=

E

Q

(cid:13)
(cid:13)E
Q
(cid:124)

(cid:13)
(cid:2)gk(cid:3) − F (x∗)
2
(cid:13)
(cid:125)
(cid:123)(cid:122)
T1

+ EQ
(cid:124)

(cid:104)(cid:13)
(cid:13)gk − EQ

2(cid:105)

(cid:2)gk(cid:3)(cid:13)
(cid:13)

.

(cid:123)(cid:122)
T2

(cid:125)

Next, we derive the upper bounds for terms T1 and T2 separately. For T2 we use unbiasedness of
quantization and independence of workers:

T2 = EQ





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

Q(gk

i − hk

i ) − (gk

(cid:13)
(cid:13)
i − hk
(cid:13)
i )
(cid:13)
(cid:13)

2



=

1
n2

n
(cid:88)

i=1

E

Q

(cid:104)(cid:13)
(cid:13)Q(gk

i − hk

i ) − (gk

i − hk

i )(cid:13)
(cid:13)

2(cid:105) (17)
≤

ω
n2

n
(cid:88)

i=1

(cid:13)
(cid:13)gk

i − hk
i

(cid:13)
2
(cid:13)

.

Taking Ek[·] from the both sides of the above inequality, we derive

Ek [T2] ≤

=

=

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

ω
n2

ω
n2

ω
n2

i=1
n
(cid:18)
(cid:88)

≤

ω
n2

≤

2ω
n2

+

i=1
n
(cid:88)

i=1
2ω
n2

Ek

(cid:104)(cid:13)
(cid:13)gk

i − hk
i

2(cid:105)

(cid:13)
(cid:13)

=

ω
n2

(cid:16)(cid:13)
(cid:13)Fi(xk) − hk
i

(cid:13)
2
(cid:13)

+ Ek

n
(cid:88)

(cid:16)(cid:13)
(cid:13)Ek

(cid:2)gk

i − hk
i

(cid:3)(cid:13)
2
(cid:13)

+ Ek

(cid:104)(cid:13)
(cid:13)gk

i − hk

i − Ek

(cid:2)gk

i − hk
i

2(cid:105)(cid:17)

(cid:3)(cid:13)
(cid:13)

i=1
(cid:104)(cid:13)
(cid:13)gk

i − Fi(xk)(cid:13)
(cid:13)

2(cid:105)(cid:17)

(cid:18)

(cid:13)
(cid:13)Fi(xk) − hk
i

(cid:13)
2
(cid:13)

+ Ek

(cid:20)(cid:13)
(cid:13)
(cid:13)Fijk

i

(xk) − Fijk

i

(wk

i ) − Ek

(cid:104)

Fijk

i

(xk) − Fijk

i

2(cid:21)(cid:19)

(wk
i )

(cid:105)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)Fi(xk) − hk
i

(cid:13)
2
(cid:13)

+ Ek

(cid:20)(cid:13)
(cid:13)
(cid:13)Fijk

i

(xk) − Fijk

i

(wk
i )

2(cid:21)(cid:19)

(cid:13)
(cid:13)
(cid:13)

(cid:16)(cid:13)
(cid:13)hk

(cid:13)
2
i − Fi(x(cid:63))
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)Fi(xk) − Fi(x(cid:63))
(cid:13)

2(cid:17)

n
(cid:88)

(cid:18)

i=1

(cid:20)(cid:13)
(cid:13)
(cid:13)Fijk

i

Ek

(wk

i ) − Fijk

i

(x(cid:63))

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+ Ek

(cid:20)(cid:13)
(cid:13)
(cid:13)Fijk

i

(xk) − Fijk

i

(x(cid:63))

2(cid:21)(cid:19)

(cid:13)
(cid:13)
(cid:13)

.

Since jk
i

is sampled uniformly at random from [m], we have

Ek [T2]

≤

2ω
n2

n
(cid:88)

(cid:16)(cid:13)
(cid:13)hk

(cid:13)
2
i − Fi(x(cid:63))
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)Fi(xk) − Fi(x(cid:63))
(cid:13)

2(cid:17)

i=1
2ω
mn2

+

n
(cid:88)

m
(cid:88)

(cid:16)

Ek

i=1

j=1

(cid:104)(cid:13)
(cid:13)Fij(wk

(cid:13)
i ) − Fij(x(cid:63))
(cid:13)

2(cid:105)

+ Ek

(cid:104)(cid:13)
(cid:13)Fij(xk) − Fij(x(cid:63))

2(cid:105)(cid:17)

(cid:13)
(cid:13)

(16),(24)
≤

2ω
n2

H k +

2ω
mn2

Dk +

2ω((cid:98)(cid:96) + (cid:101)(cid:96))
n

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105).

54

In last line, we also use the deﬁnitions of H k, Dk. For T1 we use deﬁnition of gk:

T1 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

E

Q

(cid:2)Q(gk

i − hk

i ) + hk
i

(cid:13)
2
(cid:13)
(cid:3) − F (x∗)
(cid:13)
(cid:13)
(cid:13)

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

gk
i − F (x∗).

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Next, we estimate Ek[T1] similarly to Ek[T2]:

Ek [T1] = Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

gk
i − F (x∗)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

E (cid:2)gk

i

(cid:13)
2
(cid:13)
(cid:3) − F (x∗)
(cid:13)
(cid:13)
(cid:13)

+ Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

(cid:0)gk

i − E (cid:2)gk

i





(cid:3)(cid:1)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

= (cid:13)

(cid:13)F (xk) − F (x∗)(cid:13)
2
(cid:13)

+

1
n2

n
(cid:88)

i=1

Ek

(cid:104)(cid:13)
(cid:13)gk

i − Fi(xk)(cid:13)
(cid:13)

2(cid:105)

(4)
≤ (cid:96)(cid:104)F (xk) − F (x∗), xk − x∗(cid:105)
(cid:20)(cid:13)
(cid:13)
(cid:13)Fijk

(xk) − Fijk

n
(cid:88)

+

E

i

i

1
n2

i=1

(wk

i ) − Ek

(cid:104)

Fijk

i

(xk) − Fijk

i

(wk
i )

2(cid:21)

(cid:105)(cid:13)
(cid:13)
(cid:13)

≤(cid:96)(cid:104)F (xk) − F (x∗), xk − x∗(cid:105) +

1
n2

n
(cid:88)

i=1

(cid:20)(cid:13)
(cid:13)
(cid:13)Fijk

i

Ek

(xk) − Fijk

i

2(cid:21)

(cid:13)
(wk
(cid:13)
i )
(cid:13)

=(cid:96)(cid:104)F (xk) − F (x∗), xk − x∗(cid:105) +

≤(cid:96)(cid:104)F (xk) − F (x∗), xk − x∗(cid:105) +

1
mn2

2
mn2

n
(cid:88)

m
(cid:88)

i=1
n
(cid:88)

j=1
m
(cid:88)

i=1

j=1

(cid:13)
(cid:13)Fij(xk) − Fij(wk

i )(cid:13)
2
(cid:13)

(cid:16)(cid:13)
(cid:13)Fij(wk

(cid:13)
2
i ) − Fij(x(cid:63))
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)Fij(xk) − Fij(x(cid:63))
(cid:13)

2(cid:17)

(cid:32)

(cid:96) +

(24)
≤

(cid:33)

2(cid:101)(cid:96)
n

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105) +

2
mn2

Dk.

Finally, summing E [T1] and E [T2] we get

E

(cid:104)(cid:13)
(cid:13)gk − F (x∗)(cid:13)
(cid:13)

2(cid:105)

= E [T1 + T2]
(cid:33)

(cid:32)

2(cid:101)(cid:96)
n

≤

(cid:96) +

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105) +

2
mn2

Dk

2ω
n2

+
(cid:32)

≤

(cid:96) +

H k +

2(cid:101)(cid:96)
n

+

Dk +
(cid:33)

2ω
mn2
2ω((cid:98)(cid:96) + (cid:101)(cid:96))
n

2ω((cid:98)(cid:96) + (cid:101)(cid:96))
n

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105)

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105) +

2ω
n2

H k +

2(ω + 1)
mn2

Dk,

which concludes the proof since σ2

k = H k

n + Dk
nm .

Lemma G.13 (Modiﬁcation of Lemmas 5 and 6 from Horv´ath et al. [2019]). Let F be (cid:96)-star-cocoercive
and Assumptions 4.1, 4.2, 5.5 hold. Suppose that α ≤ min
. Then for all k ≥ 0 VR-DIANA-
SGDA satisﬁes

(cid:110) p
3 ;

1
1+ω

(cid:111)

Ek

(cid:2)σ2

k+1

(cid:3) ≤ (1 − α)σ2

k +

(cid:16)

p(cid:101)(cid:96) + 2α((cid:101)(cid:96) + (cid:98)(cid:96))

(cid:17)

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105),

where σ2

k = H k

n + Dk

nm with H k =

n
(cid:80)
i=1

(cid:13)
(cid:13)hk

i − Fi(x∗)(cid:13)
2
(cid:13)

and Dk =

n
(cid:80)
i=1

m
(cid:80)
j=1

(cid:13)
(cid:13)Fij(wk

i ) − Fij(x∗)(cid:13)
2
(cid:13)

.

55

Proof. We start with considering H k+1:

Ek

(cid:2)H k+1(cid:3) = Ek

(cid:13)
(cid:13)hk+1

i − Fi(x(cid:63))(cid:13)
2
(cid:13)

(cid:35)

(cid:34) n
(cid:88)

i=1

=

n
(cid:88)

i=1

(cid:13)
(cid:13)hk

i − Fi(x(cid:63))(cid:13)
2
(cid:13)

+

(cid:104)

Ek

n
(cid:88)

i=1

2(cid:104)αQ(gk

i − hk

i ), hk

i − Fi(x(cid:63))(cid:105) + α2 (cid:13)

(cid:13)Q(gk

i − hk

i )(cid:13)
(cid:13)

2(cid:105)

(17)
≤ H k +

n
(cid:88)

i=1

Ek

Since α ≤ 1/(ω+1), we have

(cid:104)
2α(cid:104)gk

i − hk

i , hk

i − Fi(x(cid:63))(cid:105) + α2(ω + 1) (cid:13)

(cid:13)gk

i − hk
i

2(cid:105)
.

(cid:13)
(cid:13)

Ek

(cid:2)H k+1(cid:3)

≤

=

=

=

≤

=

≤

≤

H k + Ek

H k + Ek

H k + Ek

(cid:34) n
(cid:88)

i=1
(cid:34) n
(cid:88)

i=1
(cid:34) n
(cid:88)

i=1

α(cid:104)gk

i − hk

i , gk

i + hk

i − 2Fi(x(cid:63))(cid:105)

(cid:35)

α(cid:104)gk

i − Fi(x(cid:63)) + Fi(x(cid:63)) − hk

i , gk

i − Fi(x(cid:63)) + hk

(cid:35)
i − Fi(x(cid:63))(cid:105)

(cid:16)(cid:13)
(cid:13)gk

i − Fi(x(cid:63))(cid:13)
2
(cid:13)

α

− (cid:13)

(cid:13)hk

i − Fi(x(cid:63))(cid:13)
(cid:13)

(cid:35)

2(cid:17)

H k(1 − α) + Ek

(cid:34) n
(cid:88)

α

(cid:16)(cid:13)
(cid:13)gk

i − Fi(x(cid:63))(cid:13)
(cid:13)

2(cid:17)

(cid:35)

H k(1 − α) +

H k(1 − α) +

n
(cid:88)

i=1
n
(cid:88)

i=1

i=1
(cid:16)

2αEk

(cid:104)(cid:13)
(cid:13)gk

i − Fi(xk)(cid:13)
(cid:13)

2(cid:105)

+ 2α (cid:13)

(cid:13)Fi(xk) − Fi(x(cid:63))(cid:13)
(cid:13)

2(cid:17)

(cid:20)
2α

Ek

(cid:13)
(cid:13)
(cid:13)Fijk

i

(xk) − Fijk

i

(wk

i ) − Ek

(cid:104)

Fijk

i

(xk) − Fijk

i

(wk
i )

2(cid:21)

(cid:105)(cid:13)
(cid:13)
(cid:13)

+2α

n
(cid:88)

i=1

(cid:13)Fi(xk) − Fi(x(cid:63))(cid:13)
(cid:13)
2
(cid:13)

H k(1 − α) +

n
(cid:88)

(cid:20)

(cid:18)

Ek

2α

(cid:13)
(cid:13)
(cid:13)Fijk

i

(xk) − Fijk

i

(wk
i )

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+ 2α (cid:13)

2(cid:19)
(cid:13)
(cid:13)Fi(xk) − Fi(x(cid:63))
(cid:13)

H k(1 − α) +

i=1
2α
m

n
(cid:88)

m
(cid:88)

i=1

j=1

(cid:16)(cid:13)
(cid:13)Fij(xk) − Fij(x(cid:63))(cid:13)
2
(cid:13)

+ (cid:13)

(cid:13)Fij(wk

i ) − Fij(x(cid:63))(cid:13)
(cid:13)

2(cid:17)

+2α

n
(cid:88)

i=1

(cid:13)Fi(xk) − Fi(x(cid:63))(cid:13)
(cid:13)
2
(cid:13)
2

(16),(24)
≤

H k(1 − α) +

n
(cid:88)

m
(cid:88)

(cid:13)
(cid:13)Fij(wk

(cid:13)
2
ij) − Fij(x(cid:63))
(cid:13)
2

2α
m

j=1
+2αn((cid:101)(cid:96) + (cid:98)(cid:96))(cid:104)F (xk) − F (x∗), xk − x∗(cid:105)

i=1

=

H k(1 − α) +

2α
m

Dk + 2αn((cid:101)(cid:96) + (cid:98)(cid:96))(cid:104)F (xk) − F (x∗), xk − x∗(cid:105).

Next, we consider Dk+1

Ek

(cid:2)Dk+1(cid:3) =

n
(cid:88)

m
(cid:88)

Ek

(cid:104)(cid:13)
(cid:13)Fij(wk+1

i

(cid:13)
) − Fij(x(cid:63))
(cid:13)

2(cid:105)

i=1
n
(cid:88)

j=1
m
(cid:88)

(cid:104)

i=1

j=1

=

(1 − p)

(cid:13)
(cid:13)Fij(wk

(cid:13)
2
ij) − Fij(x(cid:63))
(cid:13)
2

+ p (cid:13)

(cid:13)
2
(cid:13)Fij(xk) − Fij(x(cid:63))
(cid:13)
2

(cid:105)

(24)
≤ Dk (1 − p) + nmp(cid:101)(cid:96)(cid:104)F (xk) − F (x∗), xk − x∗(cid:105).

56

It remains put the upper bounds on Dk+1, H k+1 together and use the deﬁnition of σ2

k+1:

Ek

(cid:2)σ2

k+1

(cid:3) =

Ek

(cid:2)H k+1(cid:3)
n

Ek

+

(cid:2)Dk+1(cid:3)
nm

≤ (1 − α)

H k
n

+ (1 + 2α − p)

Dk
nm

(cid:16)

+

p(cid:101)(cid:96) + 2α((cid:101)(cid:96) + (cid:98)(cid:96))

(cid:17)

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105)

With α ≤ p

3 we get −p ≤ −3α, implying

Ek

(cid:2)σ2

k+1

(cid:3) ≤ (1 − α)

H k
n
= (1 − α)σ2

+ (1 − α)
(cid:16)

(cid:16)

Dk
nm

+

(cid:17)

p(cid:101)(cid:96) + 2α((cid:101)(cid:96) + (cid:98)(cid:96))
(cid:17)

k +

p(cid:101)(cid:96) + 2α((cid:101)(cid:96) + (cid:98)(cid:96))

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105).

(cid:104)F (xk) − F (x∗), xk − x∗(cid:105)

The above two lemmas imply that Assumption 2.1 is satisﬁed with certain parameters.

Proposition G.14 (Proposition 5.6). Let F be (cid:96)-star-cocoercive and Assumptions 4.1, 4.2, 5.5 hold.
Suppose that α ≤ min

. Then, VR-DIANA-SGDA satisﬁes Assumption 2.1 with

(cid:111)

(cid:110) p
3 ;

1
1+ω

(cid:32)

A =

(cid:33)

(cid:96)
2

+

(cid:101)(cid:96)
n

+

ω((cid:98)(cid:96) + (cid:101)(cid:96))
n

, B =

2(ω + 1)
n

,

σ2

k =

1
n

n
(cid:88)

i=1

(cid:13)
(cid:13)hk

i − Fi(x∗)(cid:13)
2
(cid:13)

+

1
nm

C =

(cid:32)

p(cid:101)l
2

(cid:33)

+ α((cid:101)(cid:96) + (cid:98)(cid:96))

,

ρ = α ≤ min

i=1

j=1
(cid:26) p
3

;

n
(cid:88)

m
(cid:88)

(cid:13)
(cid:13)Fij(wk

i ) − Fij(x∗)(cid:13)
2
(cid:13)

,

(cid:27)

1
1 + ω

, D1 = D2 = 0.

G.3.2 Analysis of VR-DIANA-SGDA in the Quasi-Strongly Monotone Case

Applying Theorem 2.2 and Corollary 2.3 with M = 4(ω+1)

nα , we get the following results.

Theorem G.15. Let F be µ-quasi strongly monotone, (cid:96)-star-cocoercive and Assumptions 4.1, 4.2, 5.5
hold. Suppose that α ≤ min

and

(cid:111)

(cid:110) p
3 ;

1
1+ω

(cid:32)

0 < γ ≤

(cid:96) +

10(ω + 1)((cid:98)(cid:96) + (cid:101)(cid:96))
n

+

(cid:33)−

4(ω + 1)p(cid:101)l
αn

1

.

Then for all k ≥ 0 the iterates of VR-DIANA-SGDA satisfy

E (cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤ (1 − min {γµ, 1/αn})k V0,

where V0 = (cid:107)x0 − x∗(cid:107)2 + 4(ω+1)γ2

nα

σ2
0.

Corollary G.16. Let the assumptions of Theorem G.15 hold. Then, for p = 1

m , α = min

(cid:110) 1

3m ,

(cid:111)
,

1
1+ω

(cid:32)

γ =

(cid:96) +

10(ω + 1)((cid:98)(cid:96) + (cid:101)(cid:96))
n

+

4(ω + 1) max{3m, 1 + ω}(cid:101)(cid:96)
nm

(cid:33)−
1

57

and any K ≥ 0 we have

E[(cid:107)xk − x∗(cid:107)2] ≤ V0 exp

− min

(cid:32)

(cid:40)

µ

(cid:96) + 10(ω+1)((cid:98)(cid:96)+(cid:101)(cid:96))

n

+ 4(ω+1) max
{
nm

3m,1+ω

(cid:101)(cid:96)

}

,

1
6m

,

1
2(1 + ω)

(cid:41)

(cid:33)

K

.

G.3.3 Analysis of VR-DIANA-SGDA in the Monotone Case

Next, using Theorem 2.5, we establish the convergence of VR-DIANA-SGDA in the monotone case.

Theorem G.17. Let F be monotone, (cid:96)-star-cocoercive and Assumptions 2.1, 2.4, 4.1, 4.2, 5.5 hold.
Assume that

(cid:32)

6(ω + 1)((cid:98)(cid:96) + (cid:101)(cid:96))
n

+

2(ω + 1)p(cid:101)l
αn

(cid:33)−
1

0 < γ ≤

(cid:96) +

and α = min
VR-DIANA-SGDA satisfy

1
1+ω

(cid:110) p
3 ,

(cid:111)

. Then for Gap
C

(z) from (10) and for all K ≥ 0 the iterates produced by

(cid:34)

E

Gap
C

(cid:33)(cid:35)

(cid:32)

1
K

K
(cid:88)

k=1

xk

≤

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

(cid:32)

+

3(cid:96) +

12(ω + 1)((cid:98)(cid:96) + (cid:101)(cid:96))
n

+

(cid:33)

8(ω + 1)p(cid:101)l
αn

·

(cid:107)x0 − x∗

,0(cid:107)2

K
(cid:33)(cid:33)

8(ω + 1)p(cid:101)l
αn

γBσ2
0
ρK

(cid:32)

(cid:32)

+

4 + γ

3(cid:96) +

12(ω + 1)((cid:98)(cid:96) + (cid:101)(cid:96))
n

+

+9γ max
X ∗
x∗

∈

(cid:107)F (x∗)(cid:107)2.

Applying Corollary D.4, we get the rate of convergence to the exact solution.

Corollary G.18. Let the assumptions of Theorem G.17 hold. Then ∀K > 0 one can choose p = 1
m ,
α = min

and γ as

(cid:110) 1

(cid:111)

3m ,

1
1+ω

(cid:40)

γ = min

3(cid:96) + 12(ω+1)((cid:98)(cid:96)+(cid:101)(cid:96))

n

1

+ 8(ω+1) max
{
mn
√

3m,1+ω

,

(cid:101)(cid:96)

}

(cid:113)

Ω0,
C

n

,

2 max{3m, 1 + ω}(ω + 1)((cid:101)(cid:96) + (cid:98)(cid:96))(cid:96)

(cid:41)
.

Ω0,
√
C
K

G

∗

This choice of α and γ implies

Ω0,
C

(cid:34)

E

Gap
C

(cid:32)

1
K

K
(cid:88)

k=1

(cid:33)(cid:35)

xk

= O

(cid:32) (cid:0)(cid:96) + (ω+1)((cid:98)(cid:96)+(cid:101)(cid:96))/n + (ω+1) max
{
K

m,ω

}

(cid:101)(cid:96)/mn(cid:1) (Ω2
0,

C

+ Ω2

0) + (cid:96)Ω2
C

max{m, ω}(ω + 1)((cid:101)(cid:96) + (cid:98)(cid:96))(cid:96)

√

nK

+

G
Ω0,
∗√
C
K

(cid:33)
.

(cid:113)

Ω2
0,

C

+

58

Proof. The proof follows from the next upper bound (cid:98)σ2

0 for σ2

0 with initialization h0

i = Fi(x0) and wi = x0

σ2
0 =

1
nm

n
(cid:88)

m
(cid:88)

i=1

j=1

(cid:107)Fij(x0) − Fij(x∗)(cid:107)2 +

1
n

n
(cid:88)

i=1

(cid:107)Fi(x0) − Fi(x∗)(cid:107)2

≤ ((cid:101)(cid:96) + (cid:98)(cid:96))(cid:104)F (x0) − F (x∗), x0 − x∗(cid:105)
≤ ((cid:101)(cid:96) + (cid:98)(cid:96))(cid:107)F (x0) − F (x∗)(cid:107) · (cid:107)x0 − x∗(cid:107)
≤ ((cid:101)(cid:96) + (cid:98)(cid:96))(cid:96)(cid:107)x0 − x∗(cid:107)2 ≤ ((cid:101)(cid:96) + (cid:98)(cid:96))(cid:96) max
∈C

u

(cid:107)x0 − u(cid:107)2 ≤ ((cid:101)(cid:96) + (cid:98)(cid:96))(cid:96)Ω2
0,

.

C

Next, applying Corollary D.4 with (cid:98)σ0 :=

(cid:113)

((cid:101)(cid:96) + (cid:98)(cid:96))(cid:96)Ω0,
C

, we get the result.

G.4 Discussion of the Results in the Monotone Case

Beznosikov et al. [2021b] also consider monotone case and derive the following rate for MASHA1 (ne-
(cid:107)x0 − u(cid:107)2):
glecting the dependence on Lipschitz parameters and the quantities like Ω2
0,

= maxu

C

∈C

. In general, due to the term proportional to 1/√K and due to the relation

(m + ω)(1 + ω/n) 1
K

(cid:17)

between (star-)cocoercivity constants and Lipschitz constants our rate

(1+ω)

nK + (1+ω) max

{
mnK

m,ω

}

+

max

m,ω
{
}
√nK

(1+ω)

+ G∗
√K

√

(cid:19)

our rate is worse than the one from Beznosikov

(cid:16)(cid:112)

O

(cid:18)

O

et al. [2021b] (even when R(x) ≡ 0, i.e., G
= 0). However, when the diﬀerence between cocoercivity
and Lipschitz constants is not signiﬁcant, and m, n or ω are suﬃciently large, our result might be better.
Moreover, we emphasize here that Beznosikov et al. [2021b] do not consider SGDA as the basis for their
methods. To the best of our knowledge, our results are the ﬁrst ones for distributed SGDA-type methods
with compression derived in the monotone case without assuming (quasi-)strong monotonicity.

∗

59

H Coordinate SGDA

In this section, we focus on the coordinate versions of SGDA. To denote i-th component of the vector x
we use [x]i. Vectors e1, . . . , ed ∈ Rd form a standard basis in Rd.

H.1 CSGDA

Algorithm 7 CSGDA: Coordinate Stochastic Gradient Descent-Ascent
1: Input: starting point x0 ∈ Rd, stepsize γ > 0, number of steps K
2: for k = 0 to K − 1 do
3:

Sample uniformly at random j ∈ [d]
gk = dej[F (xk)]j
xk+1 = proxγR

(cid:0)xk − γgk(cid:1)

4:

5:
6: end for

H.1.1 CSGDA Fits Assumption 2.1

Proposition H.1. Let F be (cid:96)-star-cocoercive. Then, CSGDA satisﬁes Assumption 2.1 with

A = d(cid:96), D1 = 2d max
X ∗

x

∗∈

(cid:104)

(cid:107)F (x∗)(cid:107)2(cid:105)

,

σ2
k = 0, B = 0, C = 0,

ρ = 1, D2 = 0.

Proof. First of all, for all a ∈ Rd and for random index j uniformly distributed on [d] we have
Ej[(cid:107)ej[a]j(cid:107)2] = 1
d

d (cid:107)a(cid:107)2. Using this and gk = dej[F (xk)]j, we derive

j = 1

(cid:80)d

Ek

(cid:2)(cid:107)gk − F (x∗

(cid:2)(cid:107)dej[F (xk) − F (x∗
(cid:2)(cid:107)dej[F (xk) − F (x∗

,k)]j + dej[F (x∗
,k)]j(cid:107)2(cid:3) + 2Ek

i=1[a]2
,k)(cid:107)2(cid:3) = Ek
≤ 2Ek
= 2d(cid:107)F (xk) − F (x∗
≤ 2d(cid:107)F (xk) − F (x∗
= 2d(cid:107)F (xk) − F (x∗

,k)(cid:107)2(cid:3)
,k)]j − F (x∗

,k)]j − F (x∗
(cid:2)(cid:107)dej[F (x∗
,k)]j − Ek[dej[F (x∗
,k)]j(cid:107)2(cid:3)

,k)(cid:107)2(cid:3)
,k)]j](cid:107)2(cid:3)

,k)(cid:107)2 + 2Ek
,k)(cid:107)2 + 2Ek
,k)(cid:107)2 + 2d(cid:107)F (x∗

(cid:2)(cid:107)dej[F (x∗
(cid:2)(cid:107)dej[F (x∗
,k)(cid:107)2.

Finally, the star-cocoercivity of F implies

Ek

(cid:2)(cid:107)gk − F (x∗

,k)(cid:107)2(cid:3) ≤ 2d(cid:96)(cid:104)F (xk) − F (x∗
≤ 2d(cid:96)(cid:104)F (xk) − F (x∗

,k), xk − x∗(cid:105) + 2d(cid:107)F (x∗

,k), xk − x∗(cid:105) + 2d max
X ∗

x

∗∈

(60)

,k)(cid:107)2
(cid:107)F (x∗)(cid:107)2(cid:105)
(cid:104)

.

H.1.2 Analysis of CSGDA in the Quasi-Strongly Monotone Case

Applying Theorem 2.2 and Corollary 2.3, we get the following results.

Theorem H.2. Let F be µ-quasi strongly monotone and (cid:96)-star-cocoercive, 0 < γ ≤ 1/2d(cid:96). Then for
all k ≥ 0

E (cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤ (1 − γµ)k (cid:107)x0 − x∗

,0(cid:107)2 +

2γd
µ

· max
X ∗
x
∗∈

(cid:104)

(cid:107)F (x∗)(cid:107)2(cid:105)

.

Corollary H.3. Let the assumptions of Theorem H.2 hold. Then, for any K ≥ 0 one can choose

60

{γk}k

≥

0 as follows:

if K ≤

2d(cid:96)
µ

,

γk =

and k < k0,

γk =

1
2d(cid:96)
1
2d(cid:96)

,

,

and k ≥ k0,

γk =

2
µ(4d(cid:96) + µ(k − k0))

,

if K >

if K >

2d(cid:96)
µ
2d(cid:96)
µ

where k0 = (cid:100)K/2(cid:101). For this choice of γk the following inequality holds:

E[VK] ≤

64d(cid:96)
µ

(cid:107)x0 − x∗

,0(cid:107)2 exp

(cid:19)

(cid:18)

−

µK
2d(cid:96)

+

72d
µ2K

· max
X ∗
x
∗∈

(cid:104)

(cid:107)F (x∗)(cid:107)2(cid:105)

.

H.1.3 Analysis of CSGDA in the Monotone Case

Next, using Theorem 2.5, we establish the convergence of CSGDA in the monotone case.

Theorem H.4. Let F be monotone, (cid:96)-star-cocoercive and Assumptions 2.1, 2.4 hold. Assume that
γ ≤ 1/2d(cid:96). Then for Gap

(z) from (10) and for all K ≥ 0 the iterates produced by CSGDA satisfy

C

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

xk

(cid:33)(cid:35)

≤

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+

5d(cid:96)(cid:107)x0 − x∗

,0(cid:107)2

K

+20γd · max
X ∗

x

∗∈

(cid:107)F (x∗)(cid:107)2(cid:105)
(cid:104)

.

Applying Corollary D.4, we get the rate of convergence to the exact solution.

Corollary H.5. Let the assumptions of Theorem H.4 hold. Then ∀K > 0 one can choose γ as
(cid:26) 1
5d(cid:96)

γ = min

Ω0,
√
C
2dK

G

(cid:27)

,

.

∗

This choice of γ implies

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

(cid:33)(cid:35)

(cid:32)

xk

= O

d(cid:96)(Ω2
0,

C

0) + (cid:96)Ω2
C

+ Ω2
K

+

G∗

dΩ0,
C
K

+

G
Ω0,
∗√
C
K

(cid:33)

.

H.2 SEGA-SGDA

In this section, we consider a modiﬁcation of SEGA [Hanzely et al., 2018] – the linearly converging
coordinate method for composite optimization problems working even for non-separable regularizers.

Algorithm 8 SEGA-SGDA: SEGA Stochastic Gradient Descent-Ascent Hanzely et al. [2018]
1: Input: starting point x0 ∈ Rd, stepsize γ > 0, number of steps K
2: Set h0 = 0
3: for k = 0 to K − 1 do
4:

Sample uniformly at random j ∈ [d]
hk+1 = hk + ej([F (xk)]j − hk
j )
gk = dej([F (xk)]j − hk
j ) + hk
xk+1 = proxγR

(cid:0)xk − γgk(cid:1)

5:

6:

7:
8: end for

61

H.2.1

SEGA-SGDA Fits Assumption 2.1

The following result from Hanzely et al. [2018] does not rely on the fact that F (x) is the gradient of
some function. Therefore, it holds in our settings as well.

Lemma H.6 (Lemmas A.3 and A.4 from Hanzely et al. [2018]). Let Assumption 4.2 hold. Then for
all k ≥ 0 SEGA-SGDA satisﬁes

Ek

(cid:2)(cid:107)gk − F (x∗)(cid:107)2(cid:3) ≤ 2d(cid:107)F (xk) − F (x∗)(cid:107)2 + 2dσ2
k,

Ek

(cid:2)σ2

k+1

(cid:3) ≤

(cid:18)

1 −

(cid:19)

1
d

σ2

k +

1
d

(cid:107)F (xk) − F (x∗)(cid:107)2,

where σ2

k = (cid:107)hk − F (x∗)(cid:107)2.

The lemma above implies that Assumption 2.1 is satisﬁed with certain parameters.

Proposition H.7. Let F be (cid:96)-star-cocoercive and Assumption 4.2 holds. Then, SEGA-SGDA satisﬁes
Assumption 2.1 with σ2

k = (cid:107)hk − F (x∗)(cid:107)2 and

A = d(cid:96), B = 2d, D1 = 0, C =

(cid:96)
2d

,

ρ =

1
d

, D2 = 0.

Proof. The result follows from Lemma H.6 and star-cocoercivity of F .

H.2.2 Analysis of SEGA-SGDA in the Quasi-Strongly Monotone Case

Applying Theorem 2.2 and Corollary 2.3 with M = 4d2, we get the following results.

Theorem H.8. Let F be µ-quasi strongly monotone, (cid:96)-star-cocoercive, Assumption 4.2 holds, and
0 < γ ≤ 1

6d(cid:96) . Then, for all k ≥ 0 the iterates produced by SEGA-SGDA satisfy

E (cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤

(cid:18)

(cid:26)

1 − min

γµ,

(cid:27)(cid:19)k

1
2d

· V0,

where V0 = (cid:107)x0 − x∗(cid:107)2 + 4d2γ2σ2
0.

Corollary H.9. Let the assumptions of Theorem H.8 hold. Then, for γ = 1

6d(cid:96) and any K ≥ 0 we have

E[(cid:107)xk − x∗(cid:107)2] ≤ V0 exp

(cid:18)

− min

(cid:26) µ
6d(cid:96)

,

1
2d

(cid:27)

(cid:19)

K

.

H.2.3 Analysis of SEGA-SGDA in the Monotone Case

Next, using Theorem 2.5, we establish the convergence of CSGDA in the monotone case.

Theorem H.10. Let F be monotone, (cid:96)-star-cocoercive and Assumptions 2.1, 2.4, 4.2 hold. Assume
(z) from (10) and for all K ≥ 0 the iterates produced by SEGA-SGDA
that γ ≤ 1/6d(cid:96). Then for Gap
satisfy

C

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

(cid:33)(cid:35)

xk

≤

3 (cid:2)maxu

(cid:107)x0 − u(cid:107)2(cid:3)

∈C
2γK

+

8γ(cid:96)2Ω2
CK

+ 13d(cid:96) ·

(cid:107)x0 − x∗

,0(cid:107)2

K

+ (4 + 13γd(cid:96))

2dγσ2
0
K

+ 9γ · max
X ∗

x∗

∈

(cid:2)(cid:107)F (x∗)(cid:107)2(cid:3) .

Applying Corollary D.4, we get the rate of convergence to the exact solution.

62

Corollary H.11. Let the assumptions of Theorem H.10 hold. Then ∀K > 0 one can choose γ as

γ = min

(cid:26) 1

13d(cid:96)

,

Ω0,
C√
2G∗d

,

This choice of γ implies

(cid:27)

.

Ω0,
√
C
K

G

∗

(cid:34)

(cid:32)

E

Gap
C

1
K

K
(cid:88)

k=1

(cid:33)(cid:35)

(cid:32)

xk

= O

d(cid:96)(Ω2
0,

C

0) + (cid:96)Ω2
C

+ Ω2
K

+

G

dΩ0,
C
K

∗

+

G
Ω0,
∗√
C
K

(cid:33)

.

Proof. The proof follows from the next upper bound (cid:98)σ2

0 for σ2

0 with initialization h0 = 0

0 = (cid:107)h0 − F (x∗)(cid:107)2 = (cid:107)F (x∗)(cid:107)2 ≤ G2
σ2
∗

.

H.3 Comparison with Related Work

The summary of rates in the (quasi-) strongly monotone case is provided in Table 3. First of all, our
results are the ﬁrst convergence for solving regularized VIPs via coordinate methods.
In particular,
SEGA-SGDA is the ﬁrst linearly converging coordinate method for solving regularized VIPs. Next, when
q = 2 in zoVIA from Sadiev et al. [2021], i.e., Euclidean proximal setup is used, our rate for SEGA-SGDA
is better than the one derived for zoVIA in Sadiev et al. [2021] since (cid:96) ≤ L2/µ. Finally, zoscESVIA might
have better rate, but it is based on EG and it uses approximation of each component of operator F at
each iteration, which makes one iteration of the method costly.

In the monotone case, our result and the results from Sadiev et al. [2021] are comparable modulo

the diﬀerence between star-cocoercivity and Lipschitz constants.

Table 3: Summary of the complexity results for zeroth-order methods with two-points feedback oracles
for solving (1). By complexity we mean the number of oracle calls required for the method to ﬁnd x
such that E[(cid:107)x − x∗(cid:107)2] ≤ ε. By default, operator F is assumed to be µ-strongly monotone and, as the
result, the solution is unique. Our results rely on µ-quasi strong monotonicity of F (3). Methods
supporting R(x) (cid:54)≡ 0 are highlighted with ∗. Our results are highlighted in green. Notation: q = the
parameter depending on the proximal setup, q = 2 in Euclidean case and q = +∞ in the (cid:96)1-proximal
setup; G

X ∗ (cid:107)F (x∗)(cid:107), which is zero when R(x) ≡ 0.

= maxx

∗

∗∈

Method

Citation

zoscESVIA (1)

[Sadiev et al., 2021]

Assumptions
F is L-Lip.(2)

zoVIA

[Sadiev et al., 2021]

F is L-Lip.(2)

CSGDA ∗

This paper

SEGA-SGDA ∗

This paper

F is (cid:96)-cocoer.
F is (cid:96)-cocoer.
As. 4.2

(cid:17)

(cid:17)

(cid:101)O
(cid:16)

Complexity
(cid:16)
d L
µ
d2/q L2
µ2
µ + dG2
d (cid:96)
∗
µ2ε
(cid:17)
(cid:16)
d + d (cid:96)
µ

(cid:101)O
(cid:16)

(cid:101)O

(cid:101)O

(cid:17)

(1) The method is based on Extragradient update rule. Moreover, at each step full
operator is approximated.
(2) The problem is deﬁned on a bounded set.

63

