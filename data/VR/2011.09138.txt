Combining Gesture and Voice Control for Mid-Air Manipulation of CAD
Models in VR Environments

Markus Friedrich1, Stefan Langer1and Fabian Frey1
1Institute of Informatics, LMU Munich, Oettingenstraße 67, 80538 Munich, Germany
{markus.friedrich, stefan.langer}@iﬁ.lmu.de, fabian.frey@campus.lmu.de

0
2
0
2

v
o
N
8
1

]

C
H
.
s
c
[

1
v
8
3
1
9
0
.
1
1
0
2
:
v
i
X
r
a

Keywords:

CAD Modeling, Virtual Reality, Speech Recognition, Constructive Solid Geometry

Abstract:

Modeling 3D objects in domains like Computer Aided Design (CAD) is time-consuming and comes with a
steep learning curve needed to master the design process as well as tool complexities. In order to simplify the
modeling process, we designed and implemented a prototypical system that leverages the strengths of Virtual
Reality (VR) hand gesture recognition in combination with the expressiveness of a voice-based interface for
the task of 3D modeling. Furthermore, we use the Constructive Solid Geometry (CSG) tree representation
for 3D models within the VR environment to let the user manipulate objects from the ground up, giving an
intuitive understanding of how the underlying basic shapes connect. The system uses standard mid-air 3D
object manipulation techniques and adds a set of voice commands to help mitigate the deﬁciencies of current
hand gesture recognition techniques. A user study was conducted to evaluate the proposed prototype. The
combination of our hybrid input paradigm shows to be a promising step towards easier to use CAD modeling.

1 INTRODUCTION

Current Computer Aided Design (CAD) modeling
tools use mainly mouse control-based object manipu-
lation techniques in predeﬁned views of the modelled
objects. However, manipulating 3D objects with a 2D
input device (mouse) on a 2D output device (moni-
tor) always necessitates a complex transfer, making it
difﬁcult for beginners to grasp the needed interaction
concepts.
With the advent of affordable Virtual Reality (VR)
devices and robust hand gesture recognition systems,
more possibilities are at the ﬁngertips of interaction
designers and researchers. The goal is to leverage the
potential of these immersive input methodologies to
improve intuitiveness, ﬂattening the learning curve,
and increasing efﬁciency in the 3D modeling task.
However, hand gesture recognition systems face
many challenges in productive environments. Com-
pared to expert devices like 3D mouses, these systems
lack robustness as well as precision. Moreover, con-
stant arm movements can cause fatigue syndroms like
the Gorilla Arm Syndrom (LaValle, 2017), making it
hard to use systems over an extended period of time.
This is where we hypothesize that a hybrid approach,
combining gesture recognition and voice control, is
beneﬁcial. While using hands for intuitive model
part manipulation, voice commands replace complex

to recognize gestures with simple to say and easy to
memorize word commands.
In this work, we propose a novel interaction con-
cept, implemented as a prototype, which combines
hand gesture recognition and voice control for CAD
model manipulation in a VR environment. The pro-
totype uses gesture-based mid-air 3D object manip-
ulation techniques where feasible and combines it
with a set of voice commands to complement the in-
teraction concept. CAD models are represented as
a combination of geometric primitives and Boolean
set-operations (so called Constructive Solid Geome-
try (CSG) trees) enabling transformation operations
that are more intuitive for beginners. The whole sys-
tem is evaluated in a user study, showing its potential.
The paper makes the following contributions:

• A new interaction concept for intuitive, CSG
tree-based, CAD modeling in VR leveraging the
strengths of both, gesture- and voice-based inter-
actions.

• A prototypical implementation of the interaction
concept with off-the-shelf hard- and software.

• A detailed user study proving the advantages of

the proposed approach.

The paper is structured as follows: The problem
solved by the proposed system is detailed in Section
2. Essential terms are explained in Section 3. Re-

 
 
 
 
 
 
lated work is discussed in Section 4 which focuses
on mid-air manipulation techniques. This is followed
by an explanation of the concept (Section 5) which is
evaluated in Section 6. Since the proposed prototype
opens up a multitude of different new implementation
and research directions, the paper concludes with a
summary and a thorough description of possible fu-
ture work (Section 7).

2 PROBLEM DESCRIPTION

The goal is to provide a system with which the
user can manipulate 3D CAD models in an intuitive
way, leveraging gesture and voice recognition tech-
niques in a VR environment. CAD models are rep-
resented as CSG tree structures which combine ge-
ometric primitives (spheres, cylinders, boxes) with
Boolean set-operations (intersection, union, differ-
ence). This representation was chosen since it is
proven to be highly intuitive, supporting our goal of
designing an easy-to-learn modeling system and inter-
action concept. This also implies that the user should
be able to modify both, geometric primitives with ba-
sic transformations (scaling, rotating, translating), as
well as properties of the underlying CSG tree.

3 BACKGROUND

3.1 Construction Trees

Construction or CSG trees (Requicha, 1980) is a rep-
resentation and modeling technique for geometric ob-
jects mostly used in CAD use cases. Complex 3D
models are created by combining primitive geomet-
ric shapes (spheres, cylinders, convex polytopes) with
Boolean operators (union,
intersection, difference,
complement) in a tree-like structure. The inner nodes
describe the Boolean operators while the leave nodes
represent primitives. The CSG tree representation has
two big advantages over other 3D model representa-
tions: Firstly, it is memory-saving, and secondly, it is
intuitive to use.

3.2 Mid-Air Interactions for 3D Object

Manipulation

The manipulation of 3D objects in a virtual scene is
a fundamental interaction in immersive virtual envi-
ronments. Manipulation is the task of changing char-
acteristics of a selected object using spatial transfor-
mations (Bowman and Hodges, 1999). Translation,

rotation, and scaling are referred to as basic manipu-
lation tasks (Bowman et al., 2004). Each task can be
performed in any axis direction. In general, a single
transformation in one speciﬁc axis is deﬁned as a de-
gree of freedom (DOF). Thus, a system that provides
a translation-only interface on all three axes supports
three DOFs whereas a system that offers all three
manipulation tasks on all axes has 9 DOFs (Mendes
et al., 2019).
In so-called mid-air interactions, inputs are passed on
through the user’s body, including posture or hand
gestures (Mendes et al., 2019). This kind of inter-
action provides a particularly immersive and natural
way to grab, move, or rotate objects in VR which
allows for more natural interfaces that can increase
both, usability and user performance (Caputo, 2019).
In this paper, the term mid-air manipulation refers
speciﬁcally to the application of basic transformations
to virtual objects in a 3D environment using hand ges-
tures. Apart from the allowed degrees of freedom, ex-
isting techniques can be classiﬁed by the existence of
a separation between translation, rotation, and scaling
(Mendes et al., 2019) which also affects manipula-
tion precision. Furthermore, two additional interac-
tion categories exist: Bimanual and unimanual. Bi-
manual interfaces imply that users need both hands
to perform the intended manipulation, whereas appli-
cations using unimanual interfaces can be controlled
with a single hand (Mendes et al., 2019).
Helpful guidelines for the design of mid-air object
manipulation interaction concepts can be found in
(Mendes et al., 2019) which were also considered dur-
ing the design of the proposed system.

4 RELATED WORK

This section provides an overview of interaction
concepts for CAD modelling in VR environments. A
detailed survey on this topic is provided in (Mendes
et al., 2019).
Object Manipulation. The so-called Handle Box ap-
proach (Houde, 1992) is essentially a bounding box
around a selected object. The Handle Box consists
of a lifting handle, which moves the object up and
down, as well as four turning handles to rotate the ob-
ject around its central axis. To move the object hori-
zontally, an activation handle is missing and instead,
the user can easily click and slide (see Figure 1 (left)).
In (Conner et al., 1992), so-called Virtual Handles that
allow full 9 DOF control are proposed. The handles
have a small sphere at their ends, which are used to
constrain geometric transformations to a single plane
or axis (Mendes et al., 2019). The user selects the ma-

nipulation mode with the mouse button. During rota-
tion, the initial user gesture is recognized to determine
the rotation axis (see Figure 1 (right)). Both tech-

(a) Handle Box

(b) Virtual Handles

Figure 1: Mouse manipulation techniques.
(Mendes et al., 2019).

Source:

niques are initially designed for classic mouse/screen
interactions but are seamlessly transferable to hand
gesture/VR environments.
Mid-Air Interactions. Caputo et al. state that users
often like mid-air interactions more than other meth-
ods, ﬁnding the accuracy in manipulation sufﬁciently
good for many proposed tasks (Caputo, 2019). First
systems used gloves or other manual input devices
providing the functionality to manipulate or to grab
objects. In (Robinett and Holloway, 1992), a system
for translating, rotating, and scaling objects in VR is
proposed. In (Bowman and Hodges, 1997), the Go-
Go and the Ray-casting technique are proposed which
allow users to grab objects which are farther away
than an arm’s length (see Figure 2). For now, our con-
cept focuses on small objects, leaving this aspect for
future work. The formalization of the object manip-
ulation task in three sub tasks, namely (object) selec-
tion, (object) transformation and (object) release was
proposed in (Bowman and Hodges, 1999). One of the

Figure 2: Go-Go (left) and Ray-casting (right) technique.
Source: (Mendes et al., 2019).

most inﬂuential techniques in this ﬁeld is the Hand-
centered Object Manipulation Extending Ray-casting
(HOMER) method (Bowman and Hodges, 1997). Af-
ter selecting an object with a light ray, the hand moves
to the object. The hand and the object are linked and
the user can manipulate the object with a virtual hand
until the object has been dropped. After that, the con-
nection is removed and the hand returns to its natural
position.
Metaphors For Manipulation.
In the so-called
Spindle system (Mapes and Moshell, 1995), the cen-

ter point of both hands is used to select objects. Trans-
formation is done by moving both hands simultane-
ously. Hand movements around the center point rotate
the object and changing the distance between both
hands scales it.
A bimanual 7 DOF manipulation technique is the
Handlebar introduced by Song et al.
(Song et al.,
2012). It is adopted by multiple real-world applica-
tions. The Handlebar uses the relative position and
movement of both hands to identify translation, ro-
tation, scaling, and to map the transformation into a
virtual object. It was developed for low-cost hardware
with reasonable accuracy without any need for precise
ﬁnger detection. The main strength of this method is
the intuitiveness due to the physical familiarity with
real-world actions like rotating and stretching a bar.
However, holding an arm position may exhaust the
user (Gorilla Arm Syndrome (LaValle, 2017)).
In (Wang et al., 2011), manipulations (in particular
translation and rotation) are separated. Translation is
applied by moving the closed hand and rotation by us-
ing two hands around the three main axes, mimicking
the physical action of rotating a sheet of paper (see
Figure 3).

Figure 3: Manipulation based on the sheet of paper
metaphor. Source: (Wang et al., 2011).

Another bimanual metaphor is the so-called 2HR
technique proposed by Caputo et. al (Caputo and Gia-
chetti, 2015), which assigns interaction manipulation
to both hands. Movements of the dominant hand trig-
ger a translation of objects whereas the non-dominant
hand’s rotation is mapped to the object rotation. How-
ever, overall performance of this technique is poor in
contrast to the Handlebar metaphor (Caputo and Gia-
chetti, 2015).
The most direct unimanual method is the Simple Vir-
tual Hand metaphor (Caputo, 2019) where objects are
selected by grasping them. After the selection, move-
ment and rotation of the user’s hand are mapped di-
rectly to the objects. This method is characterized by
its high intuitiveness. However, accurate, reliable and
robust hand tracking is required. Kim et al. extended
this idea in (Kim and Park, 2014) by placing a sphere
around the selected object and additionally enable the
user to control the reference frame during manipula-

tion which shows great improvements, in particular
for rotation.
In order to demonstrate the advantages of DOF sep-
aration, Mendes et al. applied the Virtual Handles
concept (Conner et al., 1992) to VR environments
(Mendes et al., 2016) as depicted in Figure 4. Since
Virtual Handles allow users to choose a single axis,
transformations in unwanted axes are improbable.
However, transformations in more than one axis take
a bit more time. We use this concept for basic trans-

Figure 4: Virtual Handles in VR. Source: (Mendes et al.,
2016).

formations in our prototype.
In the ﬁeld of mid-air interactions open challenges
still exist. For example, there are no widely ac-
cepted manipulation metaphors (Caputo, 2019) and
most techniques follow a direct mapping strategy with
multiple DOFs controlled at the same time, which
is only suitable for coarse transformations (Mendes
et al., 2019).

5 CONCEPT

In order to simplify the modeling process as much
as possible, we utilized mid-air manipulation tech-
niques without any controllers or gloves - hand track-
ing only. The main feature of our technique is the
use of only the grasping gesture. This simple gesture
allows the usage of low-cost hand-tracking sensors,
more speciﬁcally the Leap Motion Controller sold by
Ultrahaptics. More complex instructions are operated
via voice control. To the best of our knowledge, this is
the ﬁrst approach that combines mid-air manipulation
techniques with voice commands in order to create an

intuitive and fast way of manipulating 3D objects in
VR. The design of the system is presented in the fol-
lowing sections.

5.1 System Overview

The proposed system consists of hardware and soft-
ware components and is detailed in Figure 5. The
hand tracking controller (Leap Motion Controller)
is mounted on the VR headset which is an off-the-
shelf HTC Vive that uses laser-based head tracking
and comes with two 1080x1200 90Hz displays. The
speech audio signal is recorded using a wearable mi-
crophone (Headset). A central computing device
(Workstation) takes the input sensor data, processes
it, applies the interaction logic and renders the next
frame which is then displayed in the VR device’s
screens.
The software architecture consists of several sub-
components. The Gesture Recognition module trans-
lates input signals from the Leap Motion sensor into
precise hand poses and furthermore recognizes hand
gestures by assembling temporal chains of poses. The
Speech Recognition module takes the recorded speech
audio signal and translates it into a textual representa-
tion. The CSG model, which is later used for editing,
is parsed by the CSG Tree Parser based on a JSON-
based input format and transformed into a triangle
mesh by the Triangle Mesh Transformer. This step is
necessary since the used rendering engine (Unity3D)
is restricted to this particular geometry representa-
tion. The main component (VR Engine & Interaction
Logic) reacts on recognized gestures and voice com-
mands, executes corresponding actions (Interaction
Logic), and handles VR scene rendering (Unity3D,
SteamVR). For example, if the user applies a basic
scaling transformation to a model part consisting of
a sphere, the geometric parameters of the sphere (in
this case its radius) are changed based on the hand
pose. Then, the model is re-transformed into the tri-
angle mesh representation, rendered, and the resulting
image is sent to the VR device.

5.2

Interaction Concept

The interaction concept is based on hand gestures
(visible as two virtual gloves that visualize the user’s
hand and ﬁnger poses) and simple voice commands.
It furthermore consists of two tools, the model tool
and the tree tool, as well as the information board
serving as a user guidance system.

Figure 5: Architecture overview of the developed system.

5.2.1 Model Tool

The model tool is used for direct model manipula-
tion and is active right after system startup.
It can
be divided in two different input modes, the selec-
tion mode and the manipulation mode. Modes can
be switched using the voice command ’select’ (to
selection mode) and one of the manipulation com-
mands ’scale’, ’translate’ and ’rotate’ (to manipula-
tion mode). Figure 6 details the interaction concept
of the model tool.
The main reason for separating interactions that way
is the ambiguity of hand gestures. In some cases, the
recognition system was unable to separate between
a model part selection and a translation or rotation
transformation since both use the same grasp gesture.
The grasp gesture was chosen since initial tests which
involved basic recognition tasks for all supported ges-
tures revealed that it is the most robustly recognized
gesture available.
Selection Mode. The initial application state can
be seen in Figure 7 (a): The triangle mesh of the
loaded model is displayed in grey. The user can enter
the selection mode by saying the word ’select’. The
mode change is additionally highlighted through an
on-screen message. Using virtual hands, the user can
enter the volume of the model which highlights the
hovered primitives in green (Figure 7 (b)). This imi-
tates the hovering gesture well known from desktop-
based mouse interfaces. Once highlighted, the user
can append the primitive to the list of selected primi-
tives by using the voice command ’append’. Selected
primitives are rendered in red (Figure 7 (c)). This
way, multiple primitives can be selected. In order to

Figure 6: Interaction concept of the model tool.

remove a primitive from the selection, the user’s hand
must enter the primitives volume and use the voice
command ’remove’. Multiple selected primitives can
be grouped together with the voice command ’group’
which is useful in situations where the selected primi-
tives should behave like a single primitive during ma-
nipulation, e.g., when a rotation is applied. A group
is displayed in blue (see Figure 7 (d)) and can be dis-
solved by saying ’un-group’.
Manipulation Mode. For the manipulation of se-
lected or grouped primitives (in the following: a
model part), three basic transformations are avail-
able: translation, rotation and scaling. The currently
used transformation is selected via a voice command
(’translate’, ’rotate’ and ’scale’). The manipulation
mode is entered automatically after invoking one of
these commands. The selected transformation is high-
lighted through the displayed Virtual Handles (Con-
ner et al., 1992) (three coordinate axes with a small
box at their ends, pointing in x-, y-, and z-direction as
shown in Figure 8).

HTC Vive with mountedLeap Motion ControllerWorkstationHeadsetHead and hand poseImage of rendered sceneAudio signal (speech)Hardware ArchitectureSoftware ArchitectureCommandProcessorSpeech RecognitionMicrosoftWindows SpeechRecognitionGesture RecognitionLeap MotionUnity APIVR Engine & Interaction LogicUnity3DSteamVRInteraction LogicTriangleMeshTransformerCSG TreeParser<<move hand intoprimitive>>Selection ModeManipulation Mode"append""remove"Primitive ishighlightedPrimitive isselectedPrimitive is de-selected"group""un-group"Selectedprimitives aregroupedGroup does notexist anymore"translate""rotate"Start"scale"Virtual handlesare visible<<grab virtual handlesand performtransformations>>"select"The transformation to use can be switched in manip-
ulation mode by invoking the aforementioned voice
commands. If the primitive selection or group should
be changed, a switch to the selection mode is neces-
sary.

5.2.2 Tree Tool

The tree tool displays a representation of the model’s
CSG tree using small spheres as nodes.
It appears
above the user’s left hand when the user holds it up-
wards (see Figure 10 and Figure 11 (b)). Each leaf
node corresponds to a primitive, the inner nodes rep-
resent the Boolean operators. Operation nodes have
textures that depict the operation type (∪, ∩, −). The
user can change the operation type of a node by grab-
bing the corresponding sphere and invoking one of
the following voice commands (’change to union’,
’change to inter’, ’change to sub’). The tree tool
also allows highlighting multiple primitives at once
by grabbing their parent node. Once primitives are
highlighted, their corresponding nodes are displayed
in green as well (see Figure 9).

Figure 9: Highlighting a primitive using the tree tool.

(a) While the hand is
not pointing upwards,
the tree is hidden.

(b) The tree is displayed
when the palm of the
hand points upwards.

Figure 10: Activating the the Tree Tool.

5.2.3

Information Board

The Information Board depicts the current state of
the application, the manipulation task, and all voice
commands including their explanations (see Figure
12). The board is always visible and helps the user
to memorize voice commands and to be aware of the

(a) Initial State.

(b) Highlighted.

(c) Selected.

(d) Grouped.

Figure 7: Illustration of all possible states of a primitive.

Rotation. Per-axis rotations are done by grabbing the
small boxes at the end of each coordinate axis and
performing a wrist rotation. The grab gesture is de-
picted in Figure 11 (a). The rotation is directly ap-
plied to the corresponding model part axis. Alterna-
tively, the sphere displayed at the center of the coor-
dinate system can be grabbed and rotated. The sphere
rotations are directly applied to the model part. This
by-passes the restrictions of per-axis rotations and al-
lows for faster manipulation at the cost of precision.
Translation. Per-axis translations work like per-axis
rotations. However, instead of rotating the wrist, the
grabbed boxes can be moved along the corresponding
axis which results in a direct translation of the model
part. The sphere in the center of the coordinate axis
can be grabbed and moved around without any restric-
tions. Resulting translations are directly applied to the
model part.
Scaling. Per-axis scaling works similar to per-axis
translation. The use of the sphere at the center of the
coordinate system is not supported since it cannot be
combined with a meaningful gesture for scaling.

Figure 8: Virtual Handles are used for all basic transforma-
tions in manipulation mode.

6.2 User Study Setup

Participants could move around freely within a radius
of two square meters. The overall study setting is
shown in Figure 13. As an introduction, the partic-

Figure 13: Participants conducting the user study

ipants were shown a simple CAD object (Object 1 in
Figure 14) together with the information board to fa-
miliarize themselves with the VR environment. Up-
coming questions were answered directly by the study
director.
After the introduction, the participants were asked to
perform selection and modiﬁcation tasks (see Table
2 for the list of tasks) where they were encouraged
to share their thoughts and feelings using a Thinking-
Aloud methodology (Boren and Ramey, 2000). The
tasks were picked such that all possible interactions
with the prototype were covered. After completing
these tasks, two more objects (Objects 2 and 3 in Fig-
ure 14) were shown to the participants. The whole
process was recorded on video for further research
and evaluation purposes.
The participants’ actions, comments and experienced
difﬁculties were logged in order to distill the advan-
tages and disadvantages of the proposed interaction
concept. In addition, problems that came up during
the test could be directly addressed and discussed.

Tasks
Try to select the circle and the cube together.
Deselect the selected primitives.
Select the same primitives using the object tree.
Rotate a primitive by 90 degrees.
Scale a primitive.
Move a cylinder.
Change an operator from union to subtraction.

Table 2: Tasks given the participants during user study.

(a) Grab Virtual Handle.

(b) Open Tree Tool.

Figure 11: Used hand gestures.

current interaction mode.

Figure 12: The Information Board as shown to the user.

6 EVALUATION

A usability study was conducted in order to eval-
uate the proposed interaction concept. Its goal was to
validate whether the concept is easy to understand,
also for novices in the ﬁeld of VR and CAD, and
whether the combination of hand gesture- and voice
control is perceived as a promising idea for intuitive
CAD modeling.

6.1 Participants

Five student volunteers, three females and two males
participated in the study. Two of them have never
used VR headsets before. Four participants have a
background in Computer Science, one participant is a
student of Molecular Life Science. Details about the
participants are depicted in Table 1.

Gender
male
female
female
female
male

Age
30-40
20-30
20-30
20-30
20-30

Background VR CAD
No
Yes
No
No
No

UXD
CS
MLS
CS
CS

Yes
Yes
No
No
No

Table 1: Overview of the participants. Abbreviations:
Background in Computer Science (CS), User Experience
Design (UXD) and Molecular Life Science (MLS). Expe-
rience in Computer Aided Design (CAD) or Virtual Reality
(VR).

(a) Object 1.

(b) Object 2.

(c) Object 3.

Figure 14: Objects uses in the user study.
(Friedrich et al., 2019).

Source:

6.3

Interviews

After ﬁnishing the aforementioned tasks, an interview
was conducted in order to gain further insight into the
participants’ user experience. Each interview started
with general questions about pre-existing knowledge
of VR and CAD software which should help with the
classiﬁcation of the given answers (see Table 3).

Questions
How much experience do you have in virtual
reality applications?
How much experience do you have in CAD/3D
modeling software?
How do you see our control method compared to a
mouse based interface?
What is your opinion on pointer devices?

Table 3: Selected questions for the interview.

Statements were assembled based on the cata-
logue of questions (Bossavit et al., 2014) (see Table
4) which served as an opener for discussion. Inter-
views were digitally recorded and then transcribed.

Statements
I found the technique easy to understand.
I found the technique easy to learn.
I found it easy to select an object.
I found it easy to group multiple objects.
I found translation easy to do.
I found rotation easy to do.
I found scaling easy to do.
I found the tree tool easy to understand.
The tree tool was useful to understand the
structure of the object.
The change of the Boolean operators was easy to
understand.

Table 4: Statements given to initiate discussion.

6.4 Results

In general, the interaction design was well received
by the participants even in this early stage of proto-
type development. All users stated that they under-
stood the basic interaction idea and all features of the
application. Furthermore, they were able to learn the
controls quickly.
However, it was observed that users had to get used
to the separation between selection and manipulation
mode which was not entirely intuitive to them. For in-
stance, some participants tried to highlight primitives

in the edit mode. In this case, additional guidance was
needed. After a short while, however, the user’s un-
derstanding increased noticeably. All users stated that
the colorization of objects is an important indication
to determine the state of the application and which ac-
tions can be executed.
The information board was perceived as being very
helpful but was actively used by only three partici-
pants. Two users struggled with the information board
not always being in the ﬁeld of view which necessi-
tated a turn of the head to see it. Furthermore, only
one user realized that the current state of the applica-
tion is displayed there as well.
Hand gesture-based model manipulation was very
well received by the users. The hover and grab ges-
tures did not need any explanation. Four users had
trouble using their hands in the beginning because
they were not within the range of the hand gesture
sensor.
The voice control was evaluated positively. However,
participants reported that the command recognition
did not always work. The overall success rate was
70.7% (see Table 5). Interestingly, both male partici-
pants have signiﬁcantly higher success rates than the
three female participants which might be due to dif-
ferent pitches of their voices.
In addition, some command words like ’append’ con-
fused the users, because they expected a new primi-
tive to be appended to the object. This indicates that
an intuitive voice command design is essential but, at
the same time, hard to achieve.

User
P1
P2
P3
P4
P5

recognized
60
52
49
55
36
252

not recog.
12
35
23
27
12
109

Success Rate
83.3%
59.8%
68.1%
67.1%
75.0%
∅70.7%

Table 5: Overview of voice control success rates. Success
means, that a command was successfully recognized by the
system.

The manipulation technique including the Virtual
Handles was also evaluated positively. All users high-
lighted the intuitiveness of translation and scaling in
particular. Two participants missed the functionality
of uniform scaling. One participant needed an extra
explanation in order to understand the intention of the
sphere in the center (axis-free object manipulation).
The rotation was not fully transparent to the partici-
pants. All mentioned that they did not immediately
recognize the selected rotation axis and instead of ro-
tating the cubes at the end of the handle tried to move

the complete handle.
The tree tool was evaluated positively. All partici-
pants used it to understand and manipulate the struc-
ture of the object. One participant liked that it was
easy-to-use and always accessible. Two participants
rated it as an intuitive tool without having an in-depth
knowledge of what exactly a CSG tree is. However,
one participant could not directly see the connection
between the spheres in the tree and the corresponding
primitives. Thus, we could observe users hovering
random nodes to see the assigned primitive, trying to
select speciﬁc primitives. One participant noted that
the tree would be better understood if a small minia-
ture of the primitive was depicted instead of a white
sphere. On the contrary, three users reported difﬁcul-
ties in understanding the Boolean set-operations.
The participants showed different preferences for
highlighting primitives. Two users rather used the tree
tool while the others favored the hover gesture-based
approach. The selection as well as the de-selection
was rated positively mostly for being simple to use.
Especially while dealing with more complex objects,
it was perceived more difﬁcult to select the desired
primitive. This was also due to the size of the ele-
ments and due to a smaller distance to the next prim-
itive, which makes hovering more difﬁcult.
It was
also observed that in unfavorable cases, the selection
hand covered the left hand so that the sensor could
no longer detect its palm. As a result, the tree disap-
peared. This made it more difﬁcult for users to select
a sphere in the tree.

6.5 Discussion

This section discusses the results of the evaluation.

6.5.1 Limitations

The accuracy of hand tracking is very important.
Since only very simple gestures were needed to con-
trol the application, the sensor technology showed to
be sufﬁciently robust and precise. However, track-
ing problems occurred repeatedly for no speciﬁc or
obvious reason. This led to problems like unwanted
transformations, especially when the end of a gesture
was not detected correctly. Despite the existence of
program and tracking errors, the study participants
were able to gain comprehensive insights in the ap-
plication’s interaction ﬂow.
Furthermore,
the group of participants was rather
small and none of them had any experience with ex-
isting mid-air interaction techniques. Future versions
of the prototype should be tested with more users,
also taking different target user groups into account

(CAD modelling experts and beginners, ...). In addi-
tion, tasks should be performed using standard CAD
modeling software in order to be able to compare the
approaches properly.

6.5.2 Findings

The reported ﬂat learning curve shows the potential of
the proposed approach. Not being dependent on any
buttons on physical controllers is advantageous. After
just a few explanations and a few minutes of training,
users were able to modify 3D objects, even those par-
ticipants without VR or CAD modelling experience.
The Virtual Handle approach is a suitable and intu-
itive three DOF manipulation tool which can be used
for translation, rotation, and scaling in virtual envi-
ronments. For those actions, our prototype needs to
be improved, since the implementation of the rotation
transformation was not perceived well.
Using additional voice commands for interacting with
the system is a good way to simplify the graphical
user interface. However, voice interfaces must be de-
signed carefully in order to be intuitive while com-
mand recognition must work ﬂawlessly.
The examination and manipulation of objects using
the CSG tree structure works well even for novices.
The tree tool can be understood and used without hav-
ing any knowledge about the theory of CSG trees. Es-
pecially simple objects can be manipulated very intu-
itively. However, the system quickly reaches its limits
if the object consists of many small primitives. Fur-
thermore, some users reported that they had difﬁcul-
ties imagining intersection and subtraction operations
in advance. The advantage of the proposed interaction
concept becomes apparent when using objects with
small corresponding CSG trees. The more complex
the objects become, the more difﬁcult it is to manipu-
late them using the proposed method.

7 CONCLUSION

We described and evaluated an interaction design
for manipulating CAD models, represented as CSG
trees, using hand gestures and voice commands in a
VR environment. A user study with CAD novices
could show that the main concepts come with a ﬂat
learning curve and are intuitive to use. However, there
are some interesting directions for future work:
An important additional function is the ability to ap-
pend/remove primitives to/from the CSG tree using
speciﬁc voice commands - also for determining shape
and position. This would also necessitate a solution
for the large model problem for both, interacting with

the model directly as well as with the CSG tree, which
could be solved by adapting classical zoom & scroll
paradigms to VR environments (complementary to
the Go-Go and Ray-casting techniques described in
Section 4). Furthermore, our manipulation technique
has not yet solved the general precision problem: Ex-
act rotations, for example, are very difﬁcult to realize.
One idea would be to use a ’magnetic’ grid, which
is placed at the most important surface points of the
object. As soon as transformations near these points
are completed, the primitive can snap automatically
to these points. A system based on Natural Language
Processing could analyze and interpret not only sim-
ple commands but complete sentences. Users would
not have to stick to exact voice commands anymore
but would rather describe which action to perform.
The application gives speciﬁc feedback by executing
the action or giving further instructions on how to ex-
ecute it. This could also help solving the aforemen-
tioned precision problem since the user could directly
specify coordinates and rotation angles by saying the
precise numbers.

ACKNOWLEDGEMENTS

We thank [Anonymous] for creating the hand gesture
illustrations in Figure 11.

REFERENCES

Boren, T. and Ramey, J. (2000). Thinking aloud: reconcil-
ing theory and practice. IEEE Transactions on Profes-
sional Communication, 43(3):261–278.

Bossavit, B., Marzo, A., Ardaiz, O., De Cerio, L. D., and
Pina, A. (2014). Design choices and their implications
for 3d mid-air manipulation techniques. Presence:
Teleoperators and Virtual Environments, 23(4):377–
392.

Bowman, D., Kruijff, E., LaViola Jr, J. J., and Poupyrev,
I. P. (2004). 3D User interfaces: theory and practice,
CourseSmart eTextbook. Addison-Wesley.

Bowman, D. A. and Hodges, L. F. (1997). An evaluation of
techniques for grabbing and manipulating remote ob-
jects in immersive virtual environments. In Proceed-
ings of the 1997 symposium on Interactive 3D graph-
ics, pages 35–ff.

Bowman, D. A. and Hodges, L. F. (1999). Formalizing the
design, evaluation, and application of interaction tech-
niques for immersive virtual environments. Journal of
Visual Languages & Computing, 10(1):37–53.
Caputo, F. M. (2019). Gestural interaction in virtual envi-
ronments: User studies and applications. PhD thesis,
University of Verona.

Caputo, F. M. and Giachetti, A. (2015). Evaluation of ba-
sic object manipulation modes for low-cost immersive
In Proceedings of the 11th Biannual
virtual reality.
Conference on Italian SIGCHI Chapter, pages 74–77.
Conner, B. D., Snibbe, S. S., Herndon, K. P., Robbins,
D. C., Zeleznik, R. C., and Van Dam, A. (1992).
In Proceedings of the
Three-dimensional widgets.
1992 symposium on Interactive 3D graphics, pages
183–188.

Friedrich, M., Fayolle, P.-A., Gabor, T., and Linnhoff-
Popien, C. (2019). Optimizing evolutionary csg tree
extraction. In Proceedings of the Genetic and Evolu-
tionary Computation Conference, GECCO ’19, page
1183–1191, New York, NY, USA. Association for
Computing Machinery.

Houde, S. (1992). Iterative design of an interface for easy 3-
d direct manipulation. In Proceedings of the SIGCHI
conference on Human factors in computing systems,
pages 135–142.

Kim, T. and Park, J. (2014). 3d object manipulation using
virtual handles with a grabbing metaphor. IEEE com-
puter graphics and applications, 34(3):30–38.
LaValle, S. (2017). Virtual Reality, volume 1. Cambridge

University Press.

Mapes, D. P. and Moshell, J. M. (1995). A two-handed
interface for object manipulation in virtual environ-
ments. Presence: Teleoperators & Virtual Environ-
ments, 4(4):403–416.

Mendes, D., Caputo, F. M., Giachetti, A., Ferreira, A., and
Jorge, J. (2019). A survey on 3d virtual object manip-
ulation: From the desktop to immersive virtual envi-
ronments. In Computer graphics forum, volume 38,
pages 21–45. Wiley Online Library.

Mendes, D., Relvas, F., Ferreira, A., and Jorge, J. (2016).
The beneﬁts of dof separation in mid-air 3d object
manipulation. In Proceedings of the 22nd ACM Con-
ference on Virtual Reality Software and Technology,
pages 261–268.

Requicha, A. G. (1980). Representations for rigid solids:
Theory, methods, and systems. ACM Comput. Surv.,
12(4):437–464.

Robinett, W. and Holloway, R. (1992).

Implementation
of ﬂying, scaling and grabbing in virtual worlds. In
Proceedings of the 1992 symposium on Interactive 3D
graphics, pages 189–192.

Song, P., Goh, W. B., Hutama, W., Fu, C.-W., and Liu, X.
(2012). A handle bar metaphor for virtual object ma-
nipulation with mid-air interaction. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, pages 1297–1306.

Wang, R., Paris, S., and Popovi´c, J. (2011). 6d hands: mark-
In
erless hand-tracking for computer aided design.
Proceedings of the 24th annual ACM symposium on
User interface software and technology, pages 549–
558.

