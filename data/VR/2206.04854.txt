JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

1

Heterogeneous Face Recognition via Face Synthesis
with Identity-Attribute Disentanglement

Ziming Yang, Jian Liang, Chaoyou Fu, Mandi Luo, Member, IEEE, and Xiao-Yu Zhang, Senior

Member, IEEE

2
2
0
2

n
u
J

0
1

]

V
C
.
s
c
[

1
v
4
5
8
4
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—Heterogeneous Face Recognition (HFR) aims to
match faces across different domains (e.g., visible to near-infrared
images), which has been widely applied in authentication and
forensics scenarios. However, HFR is a challenging problem
because of the large cross-domain discrepancy, limited hetero-
geneous data pairs, and large variation of facial attributes.
To address these challenges, we propose a new HFR method
from the perspective of heterogeneous data augmentation, named
Face Synthesis with Identity-Attribute Disentanglement (FSIAD).
Firstly, the identity-attribute disentanglement (IAD) decouples
face images into identity-related representations and identity-
unrelated representations (called attributes), and then decreases
the correlation between identities and attributes. Secondly, we
devise a face synthesis module (FSM) to generate a large number
of images with stochastic combinations of disentangled identities
and attributes for enriching the attribute diversity of synthetic
images. Both the original
images and the synthetic ones are
utilized to train the HFR network for tackling the challenges and
improving the performance of HFR. Extensive experiments on
ﬁve HFR databases validate that FSIAD obtains superior per-
formance than previous HFR approaches. Particularly, FSIAD
obtains 4.8% improvement over state of the art in terms of
VR@FAR=0.01% on LAMP-HQ, the largest HFR database so
far.

Index Terms—Heterogeneous face recognition, cross-domain,

face augmentation, face disentanglement.

I. INTRODUCTION

I N recent years, face recognition has made signiﬁcant

progress with deep convolution neural networks (CNNs)
and has been widely applied in real-world scenarios like
surveillance and payment [1], [2], [3], [4], [5], [6]. Face

This work was supported in part by the National Natural Science Foundation
of China under Grant U2003111 and Grant 61871378, and in part by
the Beijing Nova Program under Grant Z211100002121108. (Corresponding
author: Xiao-Yu Zhang.)

Ziming Yang and Xiao-Yu Zhang are with the Institute of Information
Engineering, Chinese Academy of Sciences, Beijing 100093, China, and
also with the School of Cyber Security, University of Chinese Academy
of Sciences, Beijing 101408, China (email: yangziming@iie.ac.cn; zhangx-
iaoyu@iie.ac.cn).

Jian Liang, Chaoyou Fu, and Mandi Luo are with the National
for Research on Intelligent
Laboratory of Pattern Recognition, Center
Perception and Computing,
Institute of Automation, Chinese Academy
of Sciences, Beijing 100190, China, and also with the CAS Cen-
for Excellence in Brain Science and Intelligence Technology, Bei-
ter
jing 100190, China, and also with the School of Artiﬁcial
Intelli-
gence, University of Chinese Academy of Sciences, Beijing 101408,
China
luo-
(e-mail:
mandi2019@ia.ac.cn).

chaoyou.fu@nlpr.ia.ac.cn;

liangjian92@gmail.com;

©2022 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

recognition methods always assume that face images are
captured with visible imaging (VIS) devices [7]. However,
this assumption does not hold in many realistic scenarios
where face images are captured by different sensors. For
instance, near-infrared (NIR) sensors are universally adopted
in authentication systems and video surveillance cameras. The
large discrepancy between different domains degrades face
recognition performance. It arises the demand for heteroge-
neous face recognition (HFR) that refers to identifying faces
across different domains, such as NIR-VIS, Sketch-Photo,
and Thermal-VIS. Generally, HFR is confronted with three
major challenges: (i) Large cross-domain discrepancy. The
large discrepancy between faces in different domains enlarges
intra-class distance and thus worsens the performance of HFR
[8]. (ii) Lack of heterogeneous face data. It is time-consuming
and expensive to collect large-scale heterogeneous face im-
ages. The limited number of subjects and small-scale HFR
dataset are prone to result in the over-ﬁtting problem [9]. (iii)
Large variation on facial attributes. Face images have various
facial attributes including pose, complexion, expression, and
illumination, which further increase intra-class distance and
make it difﬁcult for face matching [10].

Over the last decade, HFR has attracted considerable atten-
tion of researchers to developing effective approaches [11],
[12]. Existing HFR methods can be divided into three main
categories: domain-invariant feature based methods, common
subspace learning based methods, and image synthesis based
methods. The domain-invariant feature based methods seek
to extract the discriminative feature of each subject that is
invariant across heterogeneous domains [13], [14], [15]. The
common subspace learning based methods map heterogeneous
face images into a subspace for face recognition [16], [8], [17].
The image synthesis based methods refer to the transformation
of images from one modality into the others to recognize
faces within the same modality [18], [19], [20]. Previous HFR
methods mainly focus on reducing the domain discrepancy
and have achieved promising performance. Nevertheless, they
have neglected the variations of facial attributes in real-world
applications.

To further tackle this issue, it is desirable to utilize image
synthesis based methods to synthesize large numbers of im-
ages with diverse facial attributes. These synthetic images are
used to train HFR models for promoting the generalization of
face recognition [21]. Recently, [22] employs data augmen-
tation to generate images with various face deformations for
alleviating the negative impacts of facial attribute variations
on face recognition. Inspired by [22], we propose a novel

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

2

attr and EV

Fig. 1. The pipeline of FSIAD. FSIAD consists of two main components, Identity-Attribute Disentanglement (IAD) and Face Synthesis Module (FSM). IAD:
Given paired images I = {IN , IV } and X = {XN , XV }, an identity encoder Eid extracts identity representations zid from I. Meanwhile, attribute encoders
EN
attr learn facial attribute representations from I and X. Solid and dotted arrows indicate the processing ﬂows of I and X, respectively. FSM:
A generator G reconstructs and synthesizes images with different combinations of representations. In the training procedure, a discriminator D supervises
G to generate high-ﬁdelity images. The disentanglement loss Ldis makes identities uncorrelated with attributes. The reconstruction loss LRec reduces the
difference between reconstructed images and the corresponding input. The identity preserving loss Lip keeps identities of ˆI consistent with those of I. The
attribute similarity loss Lattr and Lsim minimize the distance between attribute representations and structural features, respectively.

method called Face Synthesis with Identity-Attribute Disen-
tangle (FSIAD) to synthesize abundant heterogeneous face
images with diverse facial attributes. Rather than randomly
sampling from the distributions of heterogeneous data [23],
FSIAD disentangles facial identities and attributes, and then
generates a large number of images with stochastic combi-
nations of disentangled identities and attributes to augment
the raw HFR databases. As depicted in Fig. 1, the scheme
of FSIAD contains two components: Identity-Attribute Disen-
tanglement (IAD) and Face Synthesis Module (FSM). First,
IAD introduces an identity encoder and attribute encoders
the identity-related representations and identity-
to extract
unrelated representations from images, respectively. These two
representations are denoted as facial identities and attributes.
To disentangle attributes and identities, we regularize facial
attributes to be orthogonal to identities. It decreases the cor-
relation between facial attributes and identities and facilitates
attribute representation learning. Second, FSIAD utilizes FSM
to synthesize images with the combinations of disentangled
identities and attributes. Massive combinations of identities
and attributes signiﬁcantly increase the diversity in facial at-
tributes of synthetic images. We propose an identity preserving
constraint to ensure that the identities of synthetic images
are consistent with those of original images. In addition, a
discriminator is applied to promote FSIAD to generate high-

ﬁdelity images by discriminating between synthetic images
and real ones. The large-scale heterogeneous faces synthesized
by FSIAD are used for training HFR models to fundamentally
supply sufﬁcient heterogeneous faces,
learn various facial
attributes and reduce the cross-domain discrepancy.

To sum up, the main contributions of this work are as

follows:

• To improve the performance of HFR, we propose a
Face Synthesis with Identity-Attribute Disentanglement
(FSIAD) framework that naturally tackles three major
challenges of HFR through data augmentation.

• We devise an Identity-Attribute Disentanglement (IAD)
module to decouple facial attributes and identities from
face images. The facial attributes are constrained to be
orthogonal to identities for decreasing their correlation.
• We introduce a Face Synthesis Module (FSM) that gen-
erates large-scale images with the integration of disentan-
gled facial identities and attributes to augment the HFR
databases and enrich the diversity of facial attributes.

II. RELATED WORK

Heterogeneous face recognition methods can be mainly di-
vided into three categories, including domain-invariant feature
learning, common subspace learning, and image synthesis.

𝑬𝒊𝒅Identity𝑰𝑵𝑰𝑽𝔃𝑰𝑵𝔃𝒊𝒅𝑬𝒂𝒕𝒕𝒓𝑽Attribute𝑿𝑵Attribute𝑿𝑽𝔃𝑿𝑵𝑬𝒂𝒕𝒕𝒓𝑵Identity-Attribute Disentanglement(IAD)Face Synthesis Module(FSM)𝔃𝒊𝒅𝔃𝑰𝑵𝔃𝑰𝑽𝔃𝑿𝑽𝔃𝑰𝑽𝔃𝒊𝒅𝔃𝒊𝒅𝔃𝑿𝑵𝔃𝑿𝑽𝔃𝒊𝒅ReconstructionIntegrationReconstructionIntegration෠𝑰𝑵෠𝑰𝑽෡𝑿𝑵෡𝑿𝑽TrainingReal𝓛𝒓𝒆𝒄መ𝐼𝐼𝐼𝐷෠𝑋𝓛𝒂𝒅𝒗መ𝐼𝐸𝑖𝑑𝓛𝒊𝒑𝓏𝑖𝑑෠𝑋𝑁𝐸𝑎𝑡𝑡𝑟𝑁𝓛𝒂𝒕𝒕𝒓𝐸𝑎𝑡𝑡𝑟𝑉෠𝑋𝑉𝓏𝑋𝑉𝓏𝑋𝑁𝓛𝒔𝒊𝒎𝑋෠𝑋𝓛𝒅𝒊𝒔𝓏𝑖𝑑𝓏𝐼𝑉𝓏𝐼𝑁JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

3

In this section, we review previous HFR methods and the
representative generative models for image synthesis.

A. Heterogeneous Face Recognition

1) Domain-invariant Feature Learning Methods: Domain-
invariant feature learning methods aim to extract the identity-
related features that are invariant across spectral domains.
Conventional methods are mainly supported by hand-crafted
features, such as Local Binary Patterns (LBP) [24], Difference-
of-Gaussian (DoG) [25], Histograms of Oriented Gradients
(HoG) [13], and Local Binary Pattern Histogram (LBPH) [26].
Deep learning has achieved great success in feature learn-
ing. Many efforts are devoted to extracting domain-invariant
features with deep learning algorithms. The center loss [27]
and triple loss [28] are utilized to reduce NIR-VIS discrep-
ancy. [9] proposes a Wasserstein CNN (W-CNN) to capture
invariant deep features by minimizing the Wasserstein distance
between NIR and VIS features. Based on [9], [29] designs a
Disentangled Variational Representation (DVR) framework to
disentangle the identity information and within-person vari-
ations from heterogeneous faces. [14] and [30] learn global
relationships between local features of heterogeneous faces to
represent the domain-invariant identity information.

Domain-invariant feature learning methods provide solu-
tions to extract representations that are rarely related to facial
identities. However, they are prone to suffer from the over-
ﬁtting problem due to the small-scale heterogeneous face
datasets [31].

2) Common Subspace Learning Methods: Common sub-
space learning methods project faces of different domains into
a compact latent space, where the distance between faces of
the same subject is short. Many approaches of dimensionality
reduction are used to map heterogeneous features to a low-
dimensional space, including Linear Discriminant Analysis
(LDA) [32], [33], Principal Component Analysis (PCA) [34],
Canonical Correlation Analysis (CCA) [18], [35], and Partial
Least Squares (PLS) [36]. The emergence of deep learning
has signiﬁcantly promoted subspace learning. Invariant Deep
Representation (IDR) [37] and Coupled Deep Learning (CDL)
[38] introduce orthogonal constraint and relevance constraint
to learn a shared feature space, respectively. [17] disentangles
identity-related, modality-related, and residual features from
heterogeneous faces for reducing variations of modality and
irrelevance (e.g.pose and expression). Extended from [17],
[39] further improves disentanglement with an orthogonal
constraint and then learns residual-invariant representations by
aligning high-level features of the non-neutral face and neutral
face.

The common subspace learning methods intuitively project
heterogeneous faces to a shared space through increasing intra-
class compactness and inter-class separability. Unfortunately,
since these methods require pairs of faces in different modali-
ties for training, their performances are limited by the lack of
paired heterogeneous faces.

3) Image Synthesis Methods:

Image synthesis methods
mainly include conditional and unconditional approaches. For
images are transformed from one
conditional approaches,

modality to another modality for matching faces in the same
modality. Face photo-sketch synthesis [40], [41] offers the
ﬁrst insight into face recognition via generation. [42], [43],
[44] perform cross-spectral image reconstruction with coupled
or joint dictionary learning. The advanced deep generative
models [45], [46] have made rapid progress in image synthesis.
[47] and [48] extend CycleGAN [49] to handle heterogeneous
image transformation. Later, [50] improves [48] by decompos-
ing synthesis into texture inpainting and pose correction. Pose-
preserving Cross-spectral Face Hallucination (PCFH) [51] and
Pose Aligned Cross-spectral Hallucination (PACH) [52] align
the poses and expressions of NIR faces to those of VIS faces
for producing paired NIR-VIS faces.

Synthetic images produced from unconditional approaches
are allowed to be inconsistent with original images. [23] and
[31] propose a Dual Variational Generation (DVG) framework
to learn a joint distribution of paired heterogeneous faces and
then generate diverse heterogeneous images from noise. [53]
and [54] employ knowledge distillation to use teacher net-
works pre-trained on abundant VIS images to train generators
with insufﬁcient paired heterogeneous faces. Heterogeneous
Face Interpretable Disentangled Representation (HFIDR) [55]
explicitly interprets semantic information of face representa-
tions to synthesize cross-modality face images. [56] and [57]
synthesize multimodal faces from the predetermined visual
descriptions of facial attributes.

Unlike DVR [29] that only disentangles the identity infor-
mation and within-person variations, we perform disentangle-
ment and image synthesis in a uniﬁed framework. Contrary
to DVG [23] and DVG-Face [31] that generate the entire
faces from noise and introduce external identity information
from large-scale VIS faces to enrich the identity diversity
of generated faces, FSIAD enrich the diversity in facial
attributes of generated faces with the combinations of dis-
entangled representations instead of external information and
simultaneously tackle the challenges of HFR. Compared with
HFIDR [55] that only interprets disentangled representations
of identity and modality, FSIAD can separate representations
of identity, modality, and facial attributes. Different from [56]
and [57] that synthesize images from limited descriptions of
facial attributes, FSIAD automatically extracts facial attributes
without manual descriptions for synthesis. Rather than de-
creasing variations in domains and facial attributes to learn
discriminative features [39], we increase variation in facial
attributes of synthetic faces to improve the generalization of
HFR models to diverse attributes.

In contrast to other categories, the largest advantage of
image synthesis based methods is yielding sufﬁcient heteroge-
neous faces. These methods intuitively solve the lack of paired
HFR data. But there are two challenges of HFR still remain:
large cross-domain discrepancy and large variation of facial
attributes. The performances of these methods are strongly
related to the quality of synthetic images. Meanwhile, face
synthesis is an ill-posed problem that there exist multiple so-
lutions for each input [10]. Our method tackles this problem by
introducing comprehensive constraints to control the identities
and facial attributes of synthetic faces, thereby avoiding the
uncertainty of face synthesis.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

4

B. Face Generation

Powered by deep generative models including Generative
Adversarial Networks (GANs) [45] and Variational Anto-
Encoders (VAEs) [46], many works have achieved outstanding
performance on face generation. MUNIT [58] learns content
features and style features from images for multimodal image
generation with unsupervised learning. Nirkin et al. [59] use
3D Morphable face Models (3DMM) for face segmentation
and manipulation. [60] introduces a FaceShifter algorithm for
face swapping and tackles the occlusion challenge to generate
high-ﬁdelity images. [61], [62], [63] propose StyleGANs to
automatically separate facial styles and stochastic effects, for
improving the controllability of face synthesis. Based on [62],
Nitzan et al. [64] propose a novel face identity disentan-
glement framework to disentangle identity and attribute with
weakly supervised learning. Zhu et al. [65] formulate face
swapping as an optimal transfer problem, and propose an
Appearance Optimal Transfer (AOT) algorithm to synthesize
realistic face images with large appearance discrepancies.
Daniel and Tamar [66] introduce a Soft-IntroVAE to synthesize
high-resolution face images through introspective variational
inference. Zhang et al. [67] design a multi-identity face
reenactment network FReeNet to transfer expressions among
different subjects. InfoSwap [68] utilizes the information bot-
tleneck to extract representations of identity and perceptual
features for subject-agnostic face swapping.

III. METHOD

In this section, we describe the proposed method – FSIAD
in detail. As shown in Fig. 1, FSIAD contains two components:
Identity-Attribute Disentanglement (IAD) and Face Synthesis
Module (FSM). First of all, IAD is designed to disentangle
identity representations and facial attribute representations
from face images. The identity representations are supposed
to be independent of facial attribute representations. Secondly,
FSM aims to produce large-scale heterogeneous faces with
disentangled representations for improving the performance of
HFR.

Our method takes two pairs of heterogeneous face images
I = {IN , IV } and X = {XN , XV } as input data, where N
and V denote two different spectral domains. It is applicable
to NIR-VIS, Thermal-VIS, and Sketch-Photo heterogeneous
faces synthesis. On the one hand, the paired images IN and
IV have the same identity, which is expected to be preserved
during face synthesis. On the other hand, images {XN , XV }
are randomly sampled from HFR datasets and provide external
attribute features for enriching the diversity in attributes of
generated faces.

A. Identity-Attribute Disentanglement (IAD)

The key idea of IAD is to decouple identities and at-
the identity features from
tributes from faces. To extract
face images, we employ a pre-trained face recognition model
LightCNN [69] as the identity encoder Eid. Latent vector
zid = 1
2 (Eid(IN ) + Eid(IV )) contains the information merely
relevant to facial identity, which is a L2-normalized feature
embedding extracted by Eid. In addition, we design facial

attr and EV

attr and EV

attribute encoders EN
attr based on VAEs as illus-
trated in Table I. The Conv1 layer comprises a convolution
layer, an instance normalization layer, and a Leaky ReLU
activation layer. The FC layer denotes a fully connected
layer. The parameters of EN
attr are denoted as
qφN and qφV , respectively. Taking NIR image IN as input
data, attribute encoder EN
and
the standard deviation σN
for approximating the posterior
I
2
I |IN ) = N (zN
distribution qφN (zN
I , σN
). The attribute
I
representation zN
is sampled via the re-parameterization trick,
I
I + σN
I = µN
i.e., zN
I · (cid:15), where (cid:15) denotes the random noise that
(cid:15) ∼ N (0, I). Similarly, zV
is sampled from the posterior dis-
I
I ; µV
tribution qφV (zV
I · (cid:15).

attr computes the mean µN
I

I |IV ) = N (zV

I = µV

I + σV

I ; µN

I , σV
I

): zV

2

THE STRUCTURE OF THE ATTRIBUTE ENCODERS EN

attr AND EV

attr .

TABLE I

Input

Layer Kernel/Stride/Padding Output

Output size

image Conv1
x
Conv1
x
Conv1
x
Conv1
x
Conv1
x
Conv1
x
FC

5 / 1 / 2
3 / 2 / 1
3 / 2 / 1
3 / 2 / 1
3 / 2 / 1
3 / 2 / 1
-

x
x
x
x
x
x
µ, σ

32×128×128
64×64×64
128×32×32
256×16×16
512×8×8
512×4×4
256, 256

The training of IAD is divided into two steps, i.e., feature
disentanglement and distribution learning. Feature disentan-
glement aims to decrease the correlation between identity and
attribute representations. Hence we introduce a disentangle-
ment objective function Ldis between identity representation
zid and attribute representations {zN

Ldis = cos(zid, zN

I }:

I , zV
I ) + cos(zid, zV

I ),

(1)

where cos(·, ·) is the cosine similarity function. The identity
representation zid is orthogonal to attribute representations zN
I
and zV
I when Ldis descents to 0. In other words, the learned
facial attributes become independent to identity.

log pθ(IN ) ≥ E

The second step is designed for attribute distribution learn-
ing. Motivated by VAEs [46] that are optimized through
maximizing the evidence lower bound objective (ELBO), we
adopt its formulation for our method as follows:
I |IN ) log pθ(IN |zN
I |IN )||p(zN
I )),

qφN (zN
DKL(qφN (zN
where the ﬁrst term denotes the reconstruction objective of
IN , and the second term is a Kullback-Leibler divergence
between the approximated posterior distribution qφN (zN
I |IN )
and the prior distribution p(zN
I ). The ELBO of pθ(IV ) can be
formed by simply replacing N to V of Eq. (2). Therefore, we
deﬁne the distribution learning objective Lkl to minimize the
Kullback-Leibler divergence:

I ) −

(2)

Lkl =DKL(qφN (zN
DKL(qφV (zV
where the prior distributions p(zN
obey the multivariate normal distribution N (0, I).

I |IN )||p(zN
I |IV )||p(zV
I ) and p(zV

I ))+
I )),

I ) are assumed to

(3)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

5

In brief, the loss function of IAD is formulated as:

LIAD = λdisLdis + Lkl,

(4)

where λdis is a trade-off parameter.

B. Face Synthesis Module (FSM)

As depicted in the previous section, the disentangled iden-
tities and attributes are obtained from IAD. We design an
FSM to generate images with combinations of disentangled
identities and attributes. Apart from the encoders Eid, EN
attr
and EV
attr shared with IAD, FSM also contains a generator
G and a discriminator D. The structures of G and D are
shown in TABLE II and III, respectively. The TransConv
layer contains a transposed convolution layer, an Adaptive
Instance Normalization (AdaIN) [70] layer, a Leaky ReLU
activation layer, and a residual block. The Tanh denotes the
Tanh activation layer. The RefPad is the reﬂection padding
operation. The Conv2 layer is comprised of a convolution
layer, a batch normalization layer, and a ReLU activation layer.
The Skip Connection layer adds the result τ to the input x.

TABLE II
THE STRUCTURE OF THE GENERATOR G.

Input

zid, zX
x
x
x
x
x
x
x
x

Layer

Kernel/Stride/Padding Output

Output size

FC
TransConv
TransConv
TransConv
TransConv
TransConv
Conv1
Conv1
Tanh

-
4 / 2 / 1
4 / 2 / 1
4 / 2 / 1
4 / 2 / 1
4 / 2 / 1
3 / 1 / 1
3 / 1 / 1
-

x
x
x
x
x
x
x
x
image

8192
256×8×8
128×16×16
64×32×32
32×64×64
32×128×128
32×128×128
3×128×128
3×128×128

TABLE III
THE NETWORK ARCHITECTURE OF THE DISCRIMINATOR D.

(a) The structure of D.

Layer

Kernel/Stride/Padding Output

Output size

RefPad
Conv2
Conv2
Conv2
ResBlock
ResBlock
ResBlock
Sigmoid

-
7 / 1 / 0
3 / 2 / 1
3 / 2 / 1
-
-
-
-

x
x
x
x
x
x
x
y

3×134×134
64×128×128
128×64×64
256×32×32
256×32×32
256×32×32
256×32×32
1

Input

image
x
x
x
x
x
x
x

(b) The structure of ResBlock.

Input

Layer

Kernel/Stride/Padding Output Output size

x
τ
τ
τ
τ

RefPad
Conv2
RefPad
Conv2
Skip Connection

-
3 / 1 / 0
-
3 / 1 / 0
-

τ
τ
τ
τ
x

256×34×34
256×32×32
256×34×34
256×32×32
256×32×32

The training procedure of FSM is simultaneously conducted
through two branches: reconstruction and integration. The ﬁrst
branch aims to reconstruct images with identities and attributes

that are derived from the same subjects, ˆIN = G(zid, zN
ˆIV = G(zid, zV
and attributes of X to synthesize images ˆXN = G(zid, zN
ˆXV = G(zid, zV
tions of reference images XN and XV , respectively.

I ),
I ). The second branch integrates identities of I
X ),
X are attribute representa-

X ), where zN

X and zV

Reconstruction. The reconstructed images { ˆIN , ˆIV } are
supposed to be identical to original images {IN , IV }. We use
the reconstruction loss LRec to reduce the squared Euclidean
distances between original images and reconstructed ones:

Lrec = (cid:107)IN − ˆIN (cid:107)2

2 + (cid:107)IV − ˆIV (cid:107)2
2.

(5)

Integration. In order to preserve identities of synthetic
images, we extract the identity representations from synthetic
images { ˆIN , ˆIV } and { ˆXN , ˆXV }. Then we utilize identity
preserving objective Lip to increase the similarity between
identity representations. Lip is formulates as:

Lip =(cid:107)zid − Eid( ˆIN )(cid:107)2
(cid:107)zid − Eid( ˆXN )(cid:107)2

2 + (cid:107)zid − Eid( ˆIV )(cid:107)2
2+
2 + (cid:107)zid − Eid( ˆXV )(cid:107)2
2.

(6)

In addition, the facial attributes of synthesized images { ˆXN ,
ˆXV } are expected to be similar to the reference images
{XN , XV }. We take both latent representations and image
contents into consideration in terms of similarity. From the
perspective of latent representations, the distances between
attribute representations of {XN , XV } and those of { ˆXN ,
ˆXV } ought to be short. Similar to Lip, we deﬁne the attribute
similarity objective Lattr as:

X (cid:107)2

X (cid:107)2
2,

X − ˆzV

2 + (cid:107)zV

X − ˆzN

Lattr = (cid:107)zN

where ˆzN

X and ˆzV

(7)
X are attribute representations of ˆXN and ˆXV .
Attribute-related contents including pose, complexion, and
illumination, are transferred from input images into the synthe-
sized ones. The similarity of contents can be measured from
two aspects: error metric [71] and structural similarity. Error
metrics are prevailing approaches to evaluate the difference
between images, which are calculated between images pixel
by pixel. Owing to the advantage of color and illumination
consistency, L1-norm loss function is a predominant error
metric to regularize networks for image generation. However,
networks tend to generate a complete copy of the targeted
image when only constrained by error metrics. Besides, error
metrics are not sensitive to the pixel dependencies that contain
critical structural features of objects in the visual scene [72].
It results in the low perceptual quality of synthetic images.
Therefore, we employ the Multi-Scale Structural Similarity
index (MS-SSIM) [72] as structural similarity loss to increase
the structural similarity between input images and synthesized
images. Speciﬁcally, MS-SSIM aims to measure the difference
between structural information of images, which is related
to the attributes of objects in the view and independent to
extrinsic factors such as illumination, contrast, and noise [71].
MS-SSIM is highly adapted for human visual perception and
is generally used for image synthesis. The higher MS-SSIM
means the smaller difference between objects in different
images. We propose a similarity objective Lsim with an

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

6

combination of a L1-norm loss and MS-SSIM1:

Lsim = (1 − α) · (cid:107)X − ˆX(cid:107)1 + α · LM S(X, ˆX),
where LM S(·, ·) = (1 − M S-SSIM (·, ·)), ˆX means the
concatenation of synthesized images, i.e., ˆX = [ ˆXN , ˆXV ].
α is a trade-off hyperparameter for tuning the ratio of LM S to
L1-norm loss, which is set to 0.84 according to the empirical
investigation [73].

(8)

According to Eq. (6-8), we summarize the loss function of

integration LInt as follows:

Lint = Lip + Lattr + Lsim.

(9)

For the sake of further improvement on image synthesis,
generator G is optimized in an adversarial manner. We impose
a discriminator D to differentiate real images from synthetic
images that generated by G. Meanwhile, a generator G seeks
to generate photo-realistic images for confusing D. The adver-
sarial loss Ladv is formulated as:
Ladv =EI∼Pdata(I)[log D(I)] + E ˆX∼PG( ˆX)[log(1 − D( ˆX))].
(10)

Hereby, we deﬁne loss function of FSM as LF SM :

LF SM = Lrec + λintLint + λadvLadv,

(11)

where λint and λadv are trade-off parameters. According to
Eq. (4, 11), the overall loss of FSIAD is summarized as:

Lall = LIAD + LF SM .

(12)

C. Heterogeneous Face Recognition

We utilize FSIAD to synthesize a large number of heteroge-
neous faces for data augmentation. The HFR network F learns
abundant features of facial attributes from synthetic faces
and alleviates the degradation caused by variations in pose,
expression, and other extrinsic factors. Both real images and
augmented images are jointly used to train the HFR network.
Basically, HFR network is trained on a pair of real images
{IN , IV }. We deﬁne HFR objective Lce with the cross-entropy
loss to measure the classiﬁcation error:

On the whole, the loss function for training HFR network is
deﬁned as:

LHF R = Lce + γLin,

(15)

where γ is a trade-off parameter.

For comprehensive elaboration of our work, we introduce

the generic training strategy in algorithm 1.

Algorithm 1 Training strategy of FSIAD.
Input: Source
images

I={IN , IV }. Reference

images
X={XN , XV }. A pre-trained identity encoder Eid.
Attribute encoders EN
attr, EV
attr. A generator G. A
discriminator D. A heterogeneous
face recognition
network F . The number of synthesized images n.

Output: The parameters of EN

attr, EV

attr, G, D and F : φN ,

φV , ΨG, ΨD and Θ.

1: for i = 1 to T do
2:
3:
4:

2 (Eid(IN ) + Eid(IV )).

// IAD component.
zid = 1
I = EV
attr(IN ); zV
I = EN
zN
zN
x = EV
attr(XN ); zV
X = EN
Update φN , φV by Eq. (4).
// FSM component.
Reconstruction: ˆIN = G(zid, zN
Integration: ˆXN = G(zid, zN
Fix φN , φV , ΨG:

Update ΨD by Eq. (10).

5:
6:
7:
8:

9:
10:
11:

attr(IV ).
attr(XV ).

I ); ˆIV = G(zid, zV
X ).

X ); ˆXV = G(zid, zV

I ).

Fix ΨD:

Update φN , φV , ΨG by Eq. (9).

12:
13:
14: end for
15: // HFR network.
16: Initialize Θ by a pre-trained model.
17: for iteration = 1 to n do
18:
19:
20:
21:
22:
23: end for
24: return Θ, φN , φV , ΨG, ΨD.

2 (Eid(IN ) + Eid(IV )).
X =EV
attr(XN ; φN ); zV

Randomly sample I={IN , IV } and X={XN , XV }.
zid = 1
zN
X =EN
˜XN = G(zid, zN
Update Θ by Eq. (15).

attr(XV ; φV ).
X ; ΨG); ˜XV = G(zid, zV

X ; ΨG).

n
(cid:88)

(cid:88)

Lce = −

i

M ∈{N,V }

yi log(softmax(F (I i

M ))),

(13)

IV. EXPERIMENTS

where n is the number of paired real images, yi is the identity
label of i-th paired real images {I i

N , I i

V }.

In addition, the HFR network is supposed to diminish the
intra-class discrepancies and extract discriminative representa-
tions. The augmented images are applied to reduce discrepan-
cies in domains and attributes. Since paired augmented images
{ ˜XN , ˜XV } are generated from the same subject, the identities
of { ˜XN , ˜XV } are expected to be closed to each other [23].
To shorten the intra-class distance, we formulate the intra-class
loss Lin as:

Lin = (cid:107)F ( ˜XN ) − F ( ˜XV )(cid:107)2
2.

(14)

1https://github.com/VainF/pytorch-msssim

In this section, we perform extensive experiments to eval-
uate our proposed FSIAD qualitatively and quantitatively
on ﬁve heterogeneous face recognition databases, including
CASIA NIR-VIS 2.0 [74], BUAA-VisNir [75], Oulu-CASIA
NIR-VIS [76], Tufts Face [77], and LAMP-HQ [10]. To
demonstrate the superior advantage of our proposed FSIAD,
we make comparisons with state-of-the-art methods. Finally,
ablation studies on the effectiveness of different loss functions
are conducted.

A. Databases and Protocols

1) CASIA NIR-VIS 2.0: the most challenging public HFR
database. It contains 725 subjects and each subject has 5∼50
NIR and 1∼22 VIS images with large within-class variations

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

7

including pose, expression, and illumination. We conduct
experiments with 10-fold cross-validation. For each fold, the
training set consists of about 6,100 NIR and 2,500 VIS images
from 360 identities. For evaluation, the testing set is composed
of over 6,000 NIR and 358 VIS images from 358 identities
who are excluded from those in the training set.

2) BUAA-VisNir: a standard HFR database. It contains 150
subjects and each subject has 9 pairs of NIR-VIS images,
including one frontal view, four different other views, and
four expressions. Since the NIR and VIS images are captured
simultaneously with a multi-spectral sensor, paired NIR-VIS
images are identical except for the spectral domain. The
training set consists of 900 images from 50 subjects and the
testing set consists of 1,800 images from the remaining 100
subjects.

3) Oulu-CASIA NIR-VIS: contains 80 subjects. The paired
NIR-VIS images of each subject are captured with six different
expressions (anger, disgust, fear, happiness, sadness, and sur-
prise) and three different illuminations (normal indoor, weak,
and dark). Following the protocol introduced in [78], we select
40 subjects and each subject has 48 paired NIR-VIS images
as the training set and testing set. Both the training set and
the testing set comprise 20 subjects.

4) Tufts Face: a Thermal-VIS HFR database. It contains
1,582 paired Thermal-VIS images of 113 subjects. Each
subject has 14 pairs of Thermal-VIS images with 9 different
yaw angles and 5 different expressions. Since this database
has not designed a protocol for evaluation, we divide it into
a training set with 85 subjects and a testing set with the rest
28 subjects. The training set is composed of 1,190 pairs of
Thermal-VIS images, while the testing set has 28 VIS gallery
images and 275 thermal probe images.

5) LAMP-HQ: the latest and largest HFR database. It is
featured with large-scale, high-resolution, and wide-diversity
(e.g., age, race, and accessories), which contains 56,788 NIR
and 16,828 VIS images from 573 subjects. Each subject has
66 paired NIR-VIS images that are captured with three distinct
expressions and three yaw angles in ﬁve illumination scenes.
The evaluation is conducted by 10-fold experiments. For each
fold, the training dataset consists of approximately 29,000 NIR
and 8,800 VIS images from about 300 subjects. For testing,
the gallery set contains the remaining 273 individuals and each
individual has one VIS image, while the probe set has about
27,000 NIR images from the same individuals.

B. Experimental Settings

Implementation details. We ﬁrstly transform the size of in-
put images to 128 × 128. A face recognition model LightCNN
[69] pre-trained on the MS-Celeb-1M dataset [79] is adopted
as the identity encoder Eid to extract identity features from
face images. The L2 normalized latent vector output from the
penultimate fully connected layer in LightCNN is utilized as
identity feature representation. The attribute encoders EattrN ,
EattrV , generator (or decoder) G, and discriminator D are
built with the architectures that are described in TABLE I, II
and III, respectively. Our proposed network is implemented
with the deep learning framework Pytorch. One Nvidia RTX

GPU is used for acceleration. For optimization on training,
we employ an Adam [80] optimizer that is conﬁgured with
a learning rate of 2e-4, coefﬁcients β1=0.5 and β2=0.99 that
are used to compute the running averages of gradient and its
square. The trade-off parameters λdis, λint, λadv, and γ in
Eq.(4,11,15) are set to 2, 5, 1, and 0.001, respectively.

For fair comparisons, we follow the protocols of [55], [23],
[29], [52], [9] to use LightCNN [69] as the HFR model.
The pre-trained LightCNN model
is imported to initialize
the parameters of the network. In the experiments, we uti-
lize FSIAD to synthesize 100,000 pairs of heterogeneous
faces as augmented data. Then we ﬁne-tune the LightCNN
network with the combination of the original HFR dataset
and augmented data. For optimization of ﬁne-tuning, we
use the Stochastic Gradient Descent algorithm (SGD) with
the following conﬁgurations: momentum factor=0.9,
initial
learning rate=1e-3, and weight decay=1e-4. To tune the hyper-
parameters of our proposed loss functions, we split the training
data of each HFR database into a training set and a validation
set by a ratio of 9:1.

C. Qualitative Analyses

In order to analyze the effectiveness of face augmentation,
we conduct qualitative experiments on the CASIA NIR-VIS
2.0 database [74] to evaluate our proposed FSIAD with
the comparison of state-of-the-art methods MUNIT [58] and
FaceShifter [60]. Both the disentanglement and generation of
face augmentation are evaluated in the qualitative experiments.
We ﬁrstly decouple face images into identity representations
and attribute representations. Then we produce images of
128×128 resolution with stochastic combinations of identities
and attributes. The results generated by the aforementioned
methods are displayed in Fig. 2. Intuitively, although MU-
NIT produces clear results, it fails to disentangle identities
from attributes. The synthetic images of MUNIT are almost
equal to the source images and have low similarity in facial
attributes compared with the reference images. Additionally,
we can observe that FaceShifter can swap identity features
from the subject of the source image to that of the reference
image. However, the results of FaceShifter are mixed with
artifacts and lack the smoothness of facial textures. Although
FaceShifter [60] has gained outstanding performance of face
generation on the large-scale VIS databases, the insufﬁcient
HFR data hinders it from generating high-quality heteroge-
neous faces. In contrast to these methods, FSIAD achieves
better performance in feature disentanglement and generation.
For one thing, the identities of source images are disentangled
from other facial attributes, which are well preserved in the
synthetic images. For another, our method can well blend
the identities with facial attributes and prevent the emergence
of artifacts and rough textures. Moreover, facial attributes
such as expression, pose, and complexion are transferred from
reference images to the generated images.

For further analysis, we visualize the results of reconstruc-
tion and integration to demonstrate the ability of our method.
Fig. 3 illustrates that the reconstructed images are high-quality
and realistic. Besides, we synthesize images with the integra-
tion of identity representations and attribute representations.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

8

Fig. 2. Experimental results on feature disentanglement and generation with
FSIAD and state-of-the-art methods MUNIT and FaceShifter.

As shown in Fig. 4,
the synthesized results maintain the
identities of source images and have similar facial attributes
with reference images. In the meantime, we explore cross-
domain synthesis that source and reference images are derived
from different spectral domains. Fig. 5 reveals that FSIAD is
also applicable to cross-domain synthesis and has outstanding
abilities in feature disentanglement and face generation. Note
that the source images are different from reference images in
both spectral domains and subjects, which raises the difﬁculty
in face synthesis.

Fig. 4. A grid of synthetic faces. The images in the top row and left most
column are source images and reference images, respectively. The rest images
are synthesized by FSIAD with identities of source images and attributes of
reference images.

Fig. 3. An example of face reconstruction on the CASIA NIR-VIS 2.0
database.

D. Quantitative Analyses

Quantitative analyses are carried out to evaluate the perfor-
mance of our proposed method with comparison to the state-
of-the-art methods. In this section, quantitative analyses are
divided into three parts, including quantitative evaluations on
image synthesis, heterogeneous face recognition experiments
and ablation studies.

1) Quantitative evaluations on image synthesis: We con-
duct quantitative evaluations to compare the effectiveness
and efﬁciency of MUNIT [58], FaceShifter [60] and our

Fig. 5. The visual results of cross-domain synthesis. In contrast to Fig. 4,
the source images and reference images are not only derived from different
subjects, but also belong to different spectral domains.

proposed FSIAD. Three metrics are employed for evaluations:
Fr´echet Inception Distance (FID) [81], Structural Similarity
Index (SSIM) [71] and inference speed. FID is a metric to
measure the distance between the distribution of real images
and that of synthetic images. The lower FID reveals that

SourceReferenceMUNITFaceShifterFSIADReconstructionSourceFSIADSourceFSIADSourceReferenceSourceReferenceJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

9

the model achieves better performance to synthesize high-
ﬁdelity images like real images. In the experiments, 10,000
pairs of NIR-VIS images are randomly sampled from the
CASIA NIR-VIS 2.0 dataset. Each pair contains source images
{IN , IV } and reference images {XN , XV }. Then 10,000 pairs
of NIR-VIS synthetic images { ˆXN , ˆXV } are generated by
the aforementioned methods. Furthermore, we utilize a pre-
trained Inception V3 network [82] to map the real images
and the synthetic images into 192-dimensional feature vectors
for computing FID [83]. As illustrated in TABLE IV, our
proposed method achieves the best FID values compared
with MUNIT and FaceShifter. In other words, our method is
preferable to generate realistic and high-quality images for
face augmentation.

TABLE IV
THE FR ´ECHET INCEPTION DISTANCES BETWEEN DISTRIBUTIONS OF REAL
IMAGES ON THE CASIA NIR-VIS 2.0 DATASET AND THE SYNTHETIC
IMAGES GENERATED BY DIFFERENT METHODS.

Method

FID↓

NIR

VIS

MUNIT[58]
FaceShifter[60]
FSIAD

17.4735
11.5227
3.2995

17.5492
9.0366
2.9839

Apart from FID, we quantitatively evaluate the performance
of feature disentanglement. To assess the similarity of facial
attributes between references and synthetic images, we employ
the SSIM to measure similarity in structural
information
between images. Structural
information is deﬁned as the
attributes of objects in the scene, which is highly adapted for
human visual perception [71]. We measure the SSIM score
between references and synthetic images. The higher SSIM
score suggests that facial attributes of synthetic images are
more similar to those of references. As depicted in TABLE
V, FSIAD performs better than state-of-the-art methods and
achieves the best ability in feature disentanglement.

TABLE V
THE QUANTITATIVE RESULTS OF ATTRIBUTE SIMILARITY ON THE CASIA
NIR-VIS 2.0 DATASET. SSIM DENOTES THE STRUCTURAL SIMILARITY
INDEX.

Method

SSIM↑

NIR

VIS

MUNIT[58]
FaceShifter[60]
FSIAD

0.2572
0.2294
0.4139

0.1310
0.1623
0.3617

To further assess the efﬁciency of the aforementioned
methods, we measure the time consumed by synthesizing
12,480 pairs of images with these methods. Inference speed is
measured by the frames per second (FPS) score that equals a
ratio of the number of synthesized images to the time spent on
synthesis. As shown in TABLE VI, the proposed FSIAD is the
most efﬁcient method and particularly outperforms FaceShifter
by 11 times.

TABLE VI
THE INFERENCE SPEEDS OF DIFFERENT METHODS. THE NUMBER OF
SYNTHESIZED IMAGES IS 12,480.

Method

Time(s)↓

Speed(FPS)↑

MUNIT[58]
FaceShifter[60]
FSIAD

65.4009
415.9640
36.7743

190.823
30.002
339.367

2) Heterogeneous face recognition experiments: In order
to validate the effectiveness of FSIAD on heterogeneous face
recognition, we conduct extensive experiments on ﬁve HFR
databases, including CASIA NIR-VIS 2.0 [74], BUAA-VisNir
[75], Oulu-CASIA NIR-VIS [76], Tufts Face [77] and LAMP-
HQ [10]. Our method is compared with the state-of-the-
art methods TRIVET [28], IDR [37], ADFL [48], W-CNN
[9], PACH [52], DVR [29], DVG [23], HFIDR [55], and
OMDRA [39]. Since our method utilizes LightCNN [69] as
HFR model, the LightCNN only trained with original HFR
dataset is selected as a baseline method. Interestingly, we also
train LightCNN [69] with the synthetic face images generated
by MUNIT [58] and FaceShifter [60] to explore the effec-
tiveness of these two methods. We denote LightCNN models
trained with synthetic faces generated by these two methods
as LightCNN+MUNIT and LightCNN+FS, respectively. The
experiments are conducted by following the protocols as
described in Section IV-A. The speciﬁc experimental analyses
are provided as follows.

CASIA NIR-VIS 2.0 database. We conduct 10-fold
cross validation on the CASIA NIR-VIS 2.0 database.
The Rank-1 accuracy, Veriﬁcation Rate(VR)@False Accept
Rate(FAR)=0.1% and VR@FAR=0.01% are employed for
testing. TABLE VII and Fig. 6(a) show that most methods gain
satisfactory performance of HFR and achieve Rank-1 accuracy
over 90%. Although HFIDR(LightCNN-9) [55] has the low-
est Rank-1 accuracy, it can dramatically improve by simply
replacing the backbone from LightCNN-9 to LightCNN-29.
Compared with TRIVET [28], IDR [37], LightCNN [69] and
ADFL [48], advanced methods including W-CNN [9], DVR
[29], HFIDR [55], OMDRA [39], PACH [52], DVG [23]
effectively address the over-ﬁtting problem and obtain better
performance. Our proposed FSIAD outperforms the state-
of-the-art methods and demonstrates its superiority. FSIAD
surpasses PACH by 1.6% of VR@FAR=0.1%, which reveals
unconditional face generation provides sufﬁcient heteroge-
neous faces and fundamentally deals with a key obstacle
of HFR. Furthermore, FSIAD outperforms DVG [23] by
0.4% in terms of VR@FAR=0.01%. It shows that enriching
the diversity in facial attributes is conducive to improving
HFR. We also ﬁnd that synthetic images generated by MU-
NIT [58] and FaceShifter [60] facilitate the performance of
LightCNN and exceed state-of-the-art methods OMDRA [39]
and HFIDR [55]. However, the results of LightCNN+MUNIT
and LightCNN+FS fail
to surpass DVG due to the poor
capabilities of face generation and feature disentanglement.
Moreover, FSIAD has the smallest standard deviation in the
performance of HFR among state-of-the-art methods. Experi-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

10

(a) CASIA-NIR-VIS 2.0

(b) BUAA-VisNir

(c) Oulu-CASIA NIR-VIS

Fig. 6. The ROC curves of different methods on the CASIA-NIR-VIS 2.0, BUAA-VisNir, and Oulu-CASIA NIR-VIS datasets.

mental results substantiate that our method effectively tackles
the challenges of HFR and facilitates the performance of HFR.

TABLE VIII
THE RESULTS OF RECOGNITION ON THE BUAA-VISNIR DATABASE.

TABLE VII
THE 10-FOLD EXPERIMENTAL RESULTS OF RECOGNITION ON THE CASIA
NIR-VIS 2.0 DATABASE.

Method

Rank-1
(%)

VR@FAR
=0.1%(%)

VR@FAR
=0.01%(%)

TRIVET[28]
IDR[37]
ADFL[48]
W-CNN[9]
PACH[52]
DVR[29]
DVG[23]
HFIDR(LightCNN-9)[55]
HFIDR(LightCNN-29)[55]
OMDRA[39]
LightCNN[69]
LightCNN+MUNIT[58]
LightCNN+FS[60]
FSIAD

95.7±0.5
97.3±0.4
98.2±0.3
98.7±0.3
98.9±0.2
99.7±0.1
99.8±0.1
87.5±0.0
98.6±0.0
99.6±0.1
96.7±0.2
99.6±0.0
99.7±0.0
99.9±0.0

91.0±1.3
95.7±0.7
97.2±0.5
98.4±0.4
98.3±0.2
99.6±0.3
99.8±0.1
-
-
99.4±0.2
94.8±0.4
99.5±0.0
99.6±0.0
99.9±0.0

74.5±0.7
-
-
94.3±0.4
-
98.6±0.3
98.8±0.2
-
-
-
88.5±0.2
98.1±0.1
98.1±0.2
99.2±0.1

The

Rank-1

database.

BUAA-VisNir

accuracy,
VR@FAR=1%, and VR@FAR=0.1% are used for testing on
the BUAA-VisNir database. As illustrated in TABLE VIII
and Fig. 6(b), we observe the results of most state-of-the-art
methods including TRIVET [28], IDR [37], ADFL [48],
W-CNN [9], PACH [52], DVR [29], and DVG [23] are
lower than 99% in terms of VR@FAR=1%. The possible
reason is that BUAA-VisNir is a small-scale HFR dataset
limited subjects. These methods
and contains images of
except DVG suffer
from the over-ﬁtting problem and
get unsatisfactory performance. Although DVG generates
sufﬁcient heterogeneous faces as auxiliary training data, it
does not take various facial appearances into consideration
and leads to degradation due to variations in the pose and
expression of
faces on this dataset. LightCNN+MUNIT
[58], LightCNN+FS [60] and the proposed FSIAD enrich
the diversity of facial attributes in generated images and
achieve outstanding performance, whose results are higher
than 99.5% in terms of VR@FAR=1%. OMDRA [39] learns
residual-independent identity representations and obtains the
best results. The results of FSIAD are close to those of
OMDRA. FSIAD reaches 99.7% in terms of VR@FAR=1%,
which is second to OMDRA (99.9%).

Method

TRIVET[28]
IDR[37]
ADFL[48]
W-CNN[9]
PACH[52]
DVR[29]
DVG[23]
OMDRA[39]
LightCNN[69]
LightCNN+MUNIT[58]
LightCNN+FS[60]
FSIAD

Rank-1
(%)

VR@FAR
=1%(%)

VR@FAR
=0.1%(%)

93.9
94.3
95.2
97.4
98.6
99.2
99.3
100.0
96.5
99.4
99.5
99.8

93.0
93.4
95.3
96.0
98.0
98.5
98.5
99.9
95.4
99.5
99.5
99.7

80.9
84.7
88.0
91.9
93.5
96.9
97.3
99.7
86.7
98.7
98.8
99.1

Oulu-CASIA NIR-VIS database. The Rank-1 accuracy,
VR@FAR=1%, and VR@FAR=0.1% are used for testing.
TABLE IX and Fig. 6(c) report that all methods confront
degradation of performance on the Oulu-CASIA NIR-VIS
dataset compared with CASIA NIR-VIS 2.0 and BUA-VisNir
datasets. Since this dataset contains more types of expres-
sions and illuminations but has fewer subjects than those
it makes HFR more difﬁcult.
of the former two datasets,
The results of TRIVET [28], IDR [37], ADFL [48], W-
CNN [9] are lower than 80% in terms of VR@FAR=0.1%.
These methods are weak in generalization for HFR and fail
to solve the difﬁculty of insufﬁcient subjects on this dataset.
Besides, DVR [29], PACH [52], DVG [23] have not considered
feature disentanglement and can not adapt to variations in
facial attributes for better performance. HFIDF [55], OM-
DRA [39], LightCNN+MUNIT [58], LightCNN+FS [60] and
FSIAD achieve prominent results and gain 100% in terms
of Rank-1 accuracy. It is notable that LightCNN+MUNIT
and LightCNN+FS improve LightCNN [69] from 65.1 to
81.0% and 87.6% in terms of VR@FAR=0.1%, respectively.
This signiﬁcant improvement indicates that the diverse facial
attributes of augmented heterogeneous faces exert a positive
inﬂuence on HFR. OMDRA achieves the best performance
among other methods. FSIAD obtains comparable results to
those of OMDRA, where the difference of VR@FAR=0.1%
is 0.2%.

105104103102101100False Acceptance Rate0.860.880.900.920.940.960.981.00Verification RateDVGDVRFSIADIDRLightCNN+FSLightCNN+MUNITTRIVETWCNN105104103102101100False Acceptance Rate0.860.880.900.920.940.960.981.00Verification RateDVGDVRFSIADIDRLightCNN+FSLightCNN+MUNITTRIVETWCNN105104103102101100False Acceptance Rate0.30.40.50.60.70.80.91.0Verification RateDVGDVRFSIADIDRLightCNN+FSLightCNN+MUNITTRIVETWCNNJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

11

TABLE IX
THE RESULTS OF RECOGNITION ON THE OULU-CASIA NIR-VIS
DATABASE.

Method

TRIVET[28]
IDR[37]
ADFL[48]
W-CNN[9]
DVR[29]
PACH[52]
DVG[23]
HFIDF(LightCNN-9)[55]
HFIDF(LightCNN-29)[55]
OMDRA[39]
LightCNN[69]
LightCNN+MUNIT[58]
LightCNN+FS[60]
FSIAD

Rank-1
(%)

VR@FAR
=1%(%)

VR@FAR
=0.1%(%)

92.2
94.3
95.5
98.0
100.0
100.0
100.0
100.0
100.0
100.0
96.7
100.0
100.0
100.0

67.9
73.4
83.0
81.5
97.2
97.9
97.5
-
-
98.5
92.4
97.2
97.0
98.1

33.6
46.2
60.7
54.6
84.9
88.2
90.6
-
-
92.2
65.1
81.0
87.6
92.0

in terms of Rank-1 accuracy and VR@FAR=1%, respectively.
FSIAD has made signiﬁcant progress and outperforms all the
state-of-the-art methods. Especially, FSIAD improves Rank-
1 accuracy and VR@FAR=0.01% from 98.3% to 98.8% and
from 88.2% to 93.0%, respectively. These results prove that
the diversity in facial attributes of training data has a critical
impact on the improvement of HFR.

TABLE XI
THE 1-FOLD EXPERIMENTAL RESULTS ON THE LAMP-HQ DATABASE.
RESULTS OF ADFL[48] AND PACH[52] ARE CITED FROM [10].

Method

Rank-1
(%)

VR@FAR
=1%(%)

VR@FAR
=0.1%(%)

VR@FAR
=0.01%(%)

LightCNN[69]
ADFL[48]
PACH[52]
DVG[23]
FSIAD

96.2
95.8
96.9
98.3
98.8

96.1
91.5
93.9
98.8
99.1

85.3
71.0
78.7
96.0
97.9

69.3
-
-
88.2
93.0

Tufts Face database. We compare our method with
LightCNN and DVG on the Tufts Face database. TABLE X
demonstrates that it is challenging to match faces between
thermal and VIS domains due to the lack of facial textures and
geometries and the large cross-domain discrepancy. Owing to
the small-scale training data, LightCNN [69] gets poor results
on the Tufts Face dataset and reaches 15.3% in terms of
Rank-1 accuracy. Beneﬁted from synthetic faces, DVG [23]
increases the Rank-1 accuracy to 51.6%. It is noted that there
are variations in expressions, angles, and accessories such as
sunglasses. FSIAD further improves the performance of HFR
and reaches 57.1% in terms of Rank-1 accuracy. The results
reveal that augmentation of facial attributes is conducive to
Thermal-VIS face recognition.

TABLE X
THE RESULTS OF RECOGNITION ON THE TUFTS FACE DATABASE.

Method

Rank-1(%) VR@FAR=1%(%)

LightCNN[69]
DVG[23]
FSIAD

15.3
51.6
57.1

6.1
31.6
35.7

LAMP-HQ. We conduct 1-fold and 10-fold experiments
on the LAMP-HQ database to validate the effectiveness of
our method. Four metrics are used for evaluations, includ-
ing Rank-1 accuracy, VR@FAR=1%, VR@FAR=0.1% and
VR@FAR=0.01%. The results of the 1-fold experiment are
reported in TABLE XI and Fig. 7. Conditional synthesis based
methods ADFL [48] and PACH [52] have poor generalization
abilities to deal with various facial attributes and get results
close to those of baseline LightCNN [69]. DVG [23] takes the
advantage of unconditional face generation to synthesize sufﬁ-
cient heterogeneous faces as training data. Unfortunately, since
DVG is not able to generate faces with diverse facial attributes,
large variations in facial attributes hinder the enhancement of
its performance. DVG exceeds LightCNN by 2.1% and 2.7%

Fig. 7. The ROC curves of LightCNN, DVG, the proposed FSIAD and its
ﬁve variants on the LAMP-HQ database.

The results of 10-fold experiments are presented in TABLE
XII and Fig. 8. We observe that conditional synthesis based
methods ADFL [48] and PACH [52] have similar performance
with LightCNN [69]. It reveals the limited paired hetero-
geneous faces hinder the improvement of HFR. Compared
with ADFL and PACH, DVG gains progress and improves
VR@FAR=1% from 95.5% to 99.0%. The results of DVG sug-
gest that increasing heterogeneous faces by unconditional syn-
thesis boosts the performance of HFR. FSIAD exceeds state-
of-the-art methods, whose Rank-1 accuracy, VR@FAR=1%,
VR@FAR=0.1%, and VR@FAR=0.01% are further improved
by 0.4%, 0.2%, 0.9, and 4.0%, respectively. Moreover, Fig.
8 illustrates FSIAD also has stable performance and achieves
the minimal standard deviation of results. Experimental results
indicate it
to enrich the attribute diversity
of synthetic heterogeneous faces to facilitate HFR. FSIAD
produces sufﬁcient heterogeneous faces with abundant facial
attributes, which improve the generalization ability of the

is fundamental

104103102101100False Acceptance Rate0.750.800.850.900.951.00Verification RateDVGFSIADLightCNNw/o advw/o attrw/o disw/o ipw/o simJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

12

HFR network and substantially alleviate the degradation of
performance caused by large variations in attributes.

TABLE XII
THE 10-FOLD EXPERIMENTAL RESULTS ON THE LAMP-HQ DATABASE.
RESULTS OF ADFL[48] AND PACH[52] ARE CITED FROM [10].

Method

Rank-1
(%)

VR@FAR
=1%(%)

VR@FAR
=0.1%(%)

VR@FAR
=0.01%(%)

LightCNN[69]
ADFL[48]
PACH[52]
DVG[23]
FSIAD

95.8±0.1
95.1±0.5
95.4±0.5
98.3±0.1
98.7±0.1

95.5±0.3
92.1±0.9
93.1±0.4
99.0±0.1
99.2±0.1

82.4±2.3
73.3±2.2
75.3±1.7
96.4±0.2
97.3±0.2

62.5±10.4
-
-
88.6±1.5
92.6±1.2

Fig. 8. The box plots of LightCNN, DVG and the proposed FSIAD on the
LAMP-HQ database.

E. Ablation Studies

To investigate the effectiveness of the proposed loss func-
tions in FSIAD, we conduct ablation studies on the LAMP-
HQ database. Since we follow the experimental settings of
[55], [23], [29], [52], [9] to adopt LightCNN [69] as the HFR
network, we initialize its parameters with a model pre-trained
on the MS-Celeb-1M dataset [79] as a baseline (BL). Then
the baseline model is ﬁne-tuned on the LAMP-HQ dataset and
denoted as LightCNN in TABLE XIII. For ablation studies,
we construct ﬁve variants and each variant is trained without
a speciﬁc loss function.

Qualitative and quantitative evaluations are conducted. We
ﬁrstly train variants on the LAMP-HQ dataset and then syn-
thesize faces from disentangled representations of identities
and attributes. Fig. 9 illustrates the synthetic faces generated
by ﬁve variants. Obviously, the faces of FSIAD w/o Lip are
worst and lose most of the information of identity. Without
the identity preserving loss Lip, it is difﬁcult for FSIAD to
synthesize faces whose identities are consistent with those
of source faces. Hence the results of inferior quality indicate
that identity preserving loss plays an elementary role in face
the facial
synthesis. As for facial attributes, we ﬁnd that
attributes are dissimilar with those of reference images when

the attribute loss Lattr is removed. For example, we discover
that complexions of synthetic faces in the ﬁrst and second rows
are different from those of their corresponding reference faces.
Therefore, the attribute loss Lattr can supervise FSIAD to
align facial attributes of synthetic faces to those of references.
However, only relying on Lattr is not enough for attribute
similarity constraint. We observe that the facial textures are
blurry and ragged when FSIAD is trained without the struc-
tural similarity loss Lsim. As we can see in Eq. (8), Lsim
is composed of a L1-norm loss and a multi-scale structural
the L1-norm loss
similarity loss LM S. On the one hand,
contributes to pixel consistency for face synthesis, and makes
global features including color and illumination of synthetic
faces similar to those of reference faces. On the other hand,
the LM S loss further regularizes the local features such as eye,
mouth, texture, and contour to close with those of references.
Consequently, similarities of either attribute representations or
structural information can not be dispensed with. The faces
generated by FSIAD w/o Ladv are prototypes of synthetic
results. But these faces are full of artifacts since the generator
lacks the supervision of adversarial discriminator. It indicates
that Ladv optimizes FSIAD to reﬁne synthetic images and
improve ﬁdelity. Compared with the previous results, FSIAD
w/o Ldis produces images of better quality. However, these
synthetic faces exist blending boundaries and incoherent tex-
tures, especially the ﬁrst and second images. This phenomenon
is caused by the lack of feature disentanglement. Without Ldis,
the attribute representations learned from attribute encoders are
mixed up with the identity features of reference faces. It is hard
for a generator to produce faces with confused representations
and thus result
in fuzzy synthetic faces. Compared with
variants, FSIAD takes the advantage of our proposed loss
functions and successfully produces high-quality faces with
the combinations of identity representations and facial attribute
representations. The synthetic faces of FSIAD have smooth
textures, coherent contours but no noise or artifact. Qualitative
ablation studies reveal that all the proposed loss functions
are essential for FSIAD to produce high-quality faces and
implement heterogeneous face augmentation.

Fig. 9. The qualitative results of ablation studies on the LAMP-HQ database.
The ﬁrst and second columns are source images and reference images. The
images in the rightmost column are synthesized by FSIAD, while the rest
columns are synthesized by the ablation versions without loss functions Lip,
Lattr, Lsim, Ladv, and Ldis, respectively.

Apart from qualitative analyses, we also conduct quantita-
tive ablation studies on the HFR performance of these variants.
As demonstrated in TABLE XIII and Fig. 7, the baseline (BL)

LightCNNDVGFSIAD6065707580859095VR@FAR=0.01%LAMP-HQw/o 𝓛𝒂𝒅𝒗SourceReferencew/o 𝓛𝒊𝒑w/o 𝓛𝒔𝒊𝒎w/o 𝓛𝒅𝒊𝒔FSIADw/o 𝓛𝒂𝒕𝒕𝒓JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

13

of ablation studies performs worst on the LAMP-HQ dataset
because it is only pre-trained on the VIS faces. After BL
is ﬁne-tuned on LAMP-HQ, it achieves better performance
and improves the VR@FAR=1% from 92.5% to 96.1%. To
reveal the effectiveness of the synthetic faces, we construct a
variant FSIAD w/o Lin by setting the γ to 0 in Eq. (15). It is
equivalent to LightCNN, hence its results are identical to those
of LightCNN. Owing to the enrichment of facial attributes, all
variants of FSIAD outperform LightCNN and DVG. However,
FSIAD w/o Lip neglects the identity preservation and causes
inconsistency of identity between source faces and synthetic
faces. Hence, it directly damages face recognition and slightly
improves VR@FAR=0.1% to 96.5%. Due to lacking feature
disentanglement of identity and facial attributes, FSIAD w/o
Ldis has a ﬂaw in synthesis and causes incoherent textures in
the generated faces. It hinders FSIAD from facilitating the
performance of HFR and its results are close to those of
FSIAD w/o Lip. Without Lsim, FSIAD lacks the structural
information of face from the reference image and synthesizes
irregular faces. It results in the unsatisfactory performance of
HFR. Compared with FSIAD w/o Lsim, FSIAD w/o Lattr has
better ability in face synthesis and improves VR@FAR=0.1%
to 97.0%. FSIAD w/o Ladv can basically accomplish the goal
of FSIAD that produces faces from identities of source and
attributes of reference, and so achieves the best VR@FAR=1%
among the aforementioned variants. But its synthetic faces
are of low quality and ﬁlled with artifacts and thus lead
to the medium performance of HFR. FSIAD outperforms
the variants and remarkably improves VR@FAR=0.01% from
90.9% to 93.0%. Only when all the proposed loss functions
are activated can FSIAD exploit its optimal performance to
achieve the best results. In summary, the quantitative and qual-
itative analyses indicate that all the proposed loss functions in
FSIAD are indispensable, and they jointly supervise FSIAD
to produce high-quality heterogeneous faces and facilitate the
performance of HFR.

TABLE XIII
THE QUANTITATIVE RESULTS OF ABLATION STUDIES ON THE LAMP-HQ
DATABASE.

Method

BL
LightCNN[69]
DVG[23]
w/o Lin
w/o Lip
w/o Ldis
w/o Lsim
w/o Lattr
w/o Ladv
FSIAD

Rank-1
(%)

VR@FAR
=1%(%)

VR@FAR
=0.1%(%)

VR@FAR
=0.01%(%)

94.5
96.2
98.3
96.2
98.5
98.5
98.5
98.6
98.6
98.8

92.5
96.1
98.8
96.1
98.8
98.9
98.9
98.9
99.0
99.1

77.3
85.3
96.0
85.3
96.5
96.6
96.6
97.0
96.8
97.9

60.3
69.3
88.2
69.3
89.9
89.6
89.0
90.9
90.7
93.0

V. CONCLUSIONS

In this paper, we propose a novel method Face Synthesis
with Identity-Attribute Disentanglement (FSIAD) to augment
heterogeneous face images for facilitating the performance of

HFR. FSIAD consists of Identity-Attribute Disentanglement
(IAD) and Face Synthesis Module (FSM) components. The
IAD component is designed to decouple faces into repre-
sentations of identities and attributes, where attribute repre-
sentations are regularized to be uncorrelated with identities.
Then FSM synthesizes abundant heterogeneous faces from
the combinations of disentangled representations of identities
and attributes, which augment the insufﬁcient training data
of HFR and increase diversity in facial attributes. We train
the HFR network with both original HFR data and synthetic
heterogeneous faces for improving its performance on face
recognition. Through disentanglement and synthesis, FSIAD
naturally tackles three major challenges of HFR. Extensive
experiments on ﬁve HFR databases reveal that our method
is superior to previous methods and yields state-of-the-art
performance.

ACKNOWLEDGMENT

The authors sincerely thank the associate editor and the

reviewers for their professional comments and suggestions.

REFERENCES

[1] I. Masi, Y. Wu, T. Hassner, and P. Natarajan, “Deep face recognition:
A survey,” in SIBGRAPI Conference on Graphics, Patterns and Images,
2018, pp. 471–478.

[2] Y.-H. Tsai, H.-M. Hsu, C.-A. Hou, and Y.-C. F. Wang, “Person-speciﬁc
domain adaptation with applications to heterogeneous face recognition,”
in IEEE International Conference on Image Processing, 2014, pp. 338–
342.

[3] B. S. Riggan, N. J. Short, M. S. Sarfraz, S. Hu, H. Zhang, V. M. Patel,
S. Rasnayaka, J. Li, T. Sim, S. M. Iranmanesh, and N. M. Nasrabadi,
“Icme grand challenge results on heterogeneous face recognition: Polari-
metric thermal-to-visible matching,” in IEEE International Conference
on Multimedia Expo Workshops, 2018, pp. 1–4.

[4] A. Kantarcı and H. K. Ekenel, “Thermal to visible face recognition
using deep autoencoders,” in International Conference of the Biometrics
Special Interest Group, 2019, pp. 1–5.

[5] J. Huo, Y. Gao, Y. Shi, W. Yang, and H. Yin, “Heterogeneous face
recognition by margin-based cross-modality metric learning,” IEEE
Transactions on Cybernetics, vol. 48, no. 6, pp. 1814–1826, 2018.
[6] C. Peng, N. Wang, J. Li, and X. Gao, “Re-ranking high-dimensional deep
local representation for nir-vis face recognition,” IEEE Transactions on
Image Processing, vol. 28, no. 9, pp. 4553–4565, 2019.

[7] G. Guo and N. Zhang, “A survey on deep learning based face recogni-
tion,” Computer Vision and Image Understanding, vol. 189, p. 102805,
2019.

[8] T. de Freitas Pereira, A. Anjos, and S. Marcel, “Heterogeneous face
recognition using domain speciﬁc units,” IEEE Transactions on Infor-
mation Forensics and Security, vol. 14, no. 7, pp. 1803–1816, 2019.
[9] R. He, X. Wu, Z. Sun, and T. Tan, “Wasserstein cnn: Learning invariant
features for nir-vis face recognition,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 41, no. 7, pp. 1761–1773, 2019.
[10] A. Yu, H. Wu, H. Huang, Z. Lei, and R. He, “Lamp-hq: A large-
scale multi-pose high-quality database and benchmark for nir-vis face
recognition,” International Journal of Computer Vision, pp. 1–17, 2021.
[11] S. Ouyang, T. Hospedales, Y.-Z. Song, X. Li, C. C. Loy, and X. Wang,
“A survey on heterogeneous face recognition: Sketch, infra-red, 3d and
low-resolution,” Image and Vision Computing, vol. 56, pp. 28–48, 2016.
[12] Y. Jin, J. Lu, and Q. Ruan, “Coupled discriminative feature learning
for heterogeneous face recognition,” IEEE Transactions on Information
Forensics and Security, vol. 10, no. 3, pp. 640–652, 2015.

[13] B. Klare, Z. Li, and A. K. Jain, “Matching forensic sketches to mug
shot photos,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 33, no. 3, pp. 639–646, 2011.

[14] M. Cho, T. Kim, I.-J. Kim, K. Lee, and S. Lee, “Relational deep feature
learning for heterogeneous face recognition,” IEEE Transactions on
Information Forensics and Security, vol. 16, pp. 376–388, 2021.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

14

[15] S. Yang, K. Fu, X. Yang, Y. Lin, J. Zhang, and C. Peng, “Learning
domain-invariant discriminative features for heterogeneous face recog-
nition,” IEEE Access, vol. 8, pp. 209 790–209 801, 2020.

[16] M. Kan, S. Shan, H. Zhang, S. Lao, and X. Chen, “Multi-view discrim-
inant analysis,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 1, pp. 188–194, 2016.

[17] W. Hu and H. Hu, “Dual adversarial disentanglement and deep repre-
sentation decorrelation for nir-vis face recognition,” IEEE Transactions
on Information Forensics and Security, vol. 16, pp. 70–85, 2021.
[18] Z. Lei, Q. Bai, R. He, and S. Z. Li, “Face shape recovery from a single
image using cca mapping between tensor spaces,” in IEEE Conference
on Computer Vision and Pattern Recognition, 2008, pp. 1–7.

[19] J. Lezama, Q. Qiu, and G. Sapiro, “Not afraid of the dark: Nir-vis face
recognition via cross-spectral hallucination and low-rank embedding,” in
IEEE Conference on Computer Vision and Pattern Recognition, 2017,
pp. 6628–6637.

[20] H. Zhang, B. S. Riggan, S. Hu, N. J. Short, and V. M. Patel, “Synthesis
of high-quality visible faces from polarimetric thermal faces using
generative adversarial networks,” International Journal of Computer
Vision, vol. 127, no. 6, pp. 845–862, 2019.

[21] A. Kortylewski, A. Schneider, T. Gerig, B. Egger, A. Morel-Forster, and
T. Vetter, “Training deep face recognition systems with synthetic data,”
arXiv preprint arXiv:1802.05891, 2018.

[22] M. Luo, J. Cao, X. Ma, X. Zhang, and R. He, “Fa-gan: Face augmenta-
tion gan for deformation-invariant face recognition,” IEEE Transactions
on Information Forensics and Security, vol. 16, pp. 2341–2355, 2021.
[23] C. Fu, X. Wu, Y. Hu, H. Huang, and R. He, “Dual variational generation
for low shot heterogeneous face recognition,” in Advances in Neural
Information Processing Systems, 2019, pp. 2674–2683.

[24] T. Ahonen, A. Hadid, and M. Pietikainen, “Face description with local
binary patterns: Application to face recognition,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 28, no. 12, pp. 2037–
2041, 2006.

[25] S. Liao, D. Yi, Z. Lei, R. Qin, and S. Z. Li, “Heterogeneous face recog-
nition from local structures of normalized appearance,” in Advances in
Biometrics, 2009, pp. 209–218.

[26] D. Goswami, C. H. Chan, D. Windridge, and J. Kittler, “Evaluation of
face recognition system in heterogeneous environments (visible vs nir),”
in IEEE International Conference on Computer Vision Workshops, 2011,
pp. 2160–2167.

[27] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature learning
approach for deep face recognition,” in Proceedings of the European
Conference on Computer Vision, 2016, pp. 499–515.

[28] X. Liu, L. Song, X. Wu, and T. Tan, “Transferring deep representation
for nir-vis heterogeneous face recognition,” in International Conference
on Biometrics, 2016, pp. 1–8.

[29] X. Wu, H. Huang, V. M. Patel, R. He, and Z. Sun, “Disentangled varia-
tional representation for heterogeneous face recognition,” in Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, no. 01, 2019, pp.
9005–9012.

[30] M. Cho, T.-y. Chung, T. Kim, and S. Lee, “Nir-to-vis face recognition
via embedding relations and coordinates of the pairwise features,” in
International Conference on Biometrics, 2019, pp. 1–8.

[31] C. Fu, X. Wu, Y. Hu, H. Huang, and R. He, “Dvg-face: Dual variational
generation for heterogeneous face recognition,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, pp. 1–1, 2021.

[32] D. Yi, R. Liu, R. Chu, Z. Lei, and S. Z. Li, “Face matching between
near infrared and visible light images,” in International Conference on
Biometrics, 2007, pp. 523–530.

[33] A. Sharma, A. Kumar, H. Daume, and D. W. Jacobs, “Generalized
multiview analysis: A discriminative latent space,” in IEEE Conference
on Computer Vision and Pattern Recognition, 2012, pp. 2160–2167.

[34] D. Yi, Z. Lei, and S. Z. Li, “Shared representation learning for
heterogeneous face recognition,” in IEEE International Conference and
Workshops on Automatic Face and Gesture Recognition, 2015, pp. 1–7.
[35] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanckriet,
R. Levy, and N. Vasconcelos, “A new approach to cross-modal multi-
media retrieval,” in Proceedings of the ACM International Conference
on Multimedia, 2010, p. 251–260.

[36] A. Sharma and D. Jacobs, “Bypassing synthesis: Pls for face recognition
the IEEE
low-resolution and sketch,” in Proceedings of
with pose,
Conference on Computer Vision and Pattern Recognition, 2011, pp. 593–
600.

[37] R. He, X. Wu, Z. Sun, and T. Tan, “Learning invariant deep representa-
tion for nir-vis face recognition,” in Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, 2017, p. 2000–2006.

[38] X. Wu, L. Song, R. He, and T. Tan, “Coupled deep learning for
heterogeneous face recognition,” in Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, no. 1, 2018.

[39] W. Hu and H. Hu, “Orthogonal modality disentanglement and represen-
tation alignment network for nir-vis face recognition,” IEEE Transac-
tions on Circuits and Systems for Video Technology, pp. 1–1, 2021.
[40] X. Tang and X. Wang, “Face sketch synthesis and recognition,” in IEEE
International Conference on Computer Vision, 2003, pp. 687–694.
[41] X. Wang and X. Tang, “Face photo-sketch synthesis and recogni-
tion,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 31, no. 11, pp. 1955–1967, 2009.

[42] S. Wang, L. Zhang, Y. Liang, and Q. Pan, “Semi-coupled dictionary
learning with applications to image super-resolution and photo-sketch
synthesis,” in IEEE Conference on Computer Vision and Pattern Recog-
nition, 2012, pp. 2216–2223.

[43] D.-A. Huang and Y.-C. F. Wang, “Coupled dictionary and feature
space learning with applications to cross-domain image synthesis and
recognition,” in IEEE International Conference on Computer Vision,
2013, pp. 2496–2503.

[44] F. Juefei-Xu, D. K. Pal, and M. Savvides, “Nir-vis heterogeneous face
recognition via cross-spectral joint dictionary learning and reconstruc-
tion,” in IEEE Conference on Computer Vision and Pattern Recognition
Workshops, 2015, pp. 141–150.

[45] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”
Information Processing Systems, 2014, p.
in Advances in Neural
2672–2680.

[46] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv

preprint arXiv:1312.6114, 2013.

[47] H. B. Bae, T. Jeon, Y. Lee, S. Jang, and S. Lee, “Non-visual to visual
translation for cross-domain face recognition,” IEEE Access, vol. 8, pp.
50 452–50 464, 2020.

[48] L. Song, M. Zhang, X. Wu, and R. He, “Adversarial discriminative
heterogeneous face recognition,” in Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, no. 1, 2018, pp. 7355–7363.

[49] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-
image translation using cycle-consistent adversarial networks,” in IEEE
International Conference on Computer Vision, 2017, pp. 2223–2232.

[50] R. He, J. Cao, L. Song, Z. Sun, and T. Tan, “Adversarial cross-spectral
face completion for nir-vis face recognition,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 42, no. 5, pp. 1025–
1037, 2020.

[51] J. Yu, J. Cao, Y. Li, X. Jia, and R. He, “Pose-preserving cross spectral
face hallucination,” in Proceedings of the International Joint Conference
on Artiﬁcial Intelligence, 2019, pp. 1018–1024.

[52] B. Duan, C. Fu, Y. Li, X. Song, and R. He, “Cross-spectral face halluci-
nation via disentangling independent factors,” in IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2020, pp. 7927–7935.

[53] M. Zhu, J. Li, N. Wang, and X. Gao, “Knowledge distillation for face
photo-sketch synthesis,” IEEE Transactions on Neural Networks and
Learning Systems, pp. 1–14, 2020.

[54] W. Hu, W. Yan, and H. Hua, “Dual face alignment learning network for
nir-vis face recognition,” IEEE Transactions on Circuits and Systems for
Video Technology, pp. 1–1, 2021.

[55] D. Liu, X. Gao, C. Peng, N. Wang, and J. Li, “Heterogeneous face
interpretable disentangled representation for joint face recognition and
synthesis,” IEEE Transactions on Neural Networks and Learning Sys-
tems, pp. 1–15, 2021.

[56] X. Di and V. M. Patel, “Multimodal face synthesis from visual at-
tributes,” IEEE Transactions on Biometrics, Behavior, and Identity
Science, vol. 3, no. 3, pp. 427–439, 2021.

[57] ——, “Facial synthesis from visual attributes via sketch using multiscale
generators,” IEEE Transactions on Biometrics, Behavior, and Identity
Science, vol. 2, no. 1, pp. 55–67, 2020.

[58] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz, “Multimodal unsu-
pervised image-to-image translation,” in Proceedings of the European
Conference on Computer Vision, 2018, pp. 172–189.

[59] Y. Nirkin, I. Masi, A. Tran Tuan, T. Hassner, and G. Medioni, “On
face segmentation, face swapping, and face perception,” in IEEE Inter-
national Conference on Automatic Face Gesture Recognition, 2018, pp.
98–105.

[60] L. Li, J. Bao, H. Yang, D. Chen, and F. Wen, “Advancing high ﬁdelity
identity swapping for forgery detection,” in IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2020, pp. 5074–5083.
[61] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for
generative adversarial networks,” in IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2019, pp. 4396–4405.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, JUNE 2021

15

[62] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,
“Analyzing and improving the image quality of stylegan,” in IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020, pp.
8107–8116.

[63] T. Karras et al., “Alias-free generative adversarial networks,” arXiv

preprint arXiv:2106.12423, 2021.

[64] Y. Nitzan, A. Bermano, Y. Li, and D. Cohen-Or, “Face identity disen-
tanglement via latent space mapping,” ACM Transactions on Graphics,
vol. 39, no. 6, pp. 1–14, 2020.

[65] H. Zhu, C. Fu, Q. Wu, W. Wu, C. Qian, and R. He, “Aot: Appearance
optimal transport based identity swapping for forgery detection,” in
Advances in Neural Information Processing Systems, 2020, pp. 21 699–
21 712.

[66] T. Daniel and A. Tamar, “Soft-introvae: Analyzing and improving
the introspective variational autoencoder,” in IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2021, pp. 4391–4400.
[67] J. Zhang, X. Zeng, M. Wang, Y. Pan, L. Liu, Y. Liu, Y. Ding, and C. Fan,
“Freenet: Multi-identity face reenactment,” in IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2020, pp. 5326–5335.
[68] G. Gao, H. Huang, C. Fu, Z. Li, and R. He, “Information bottleneck
disentanglement for identity swapping,” in IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2021, pp. 3404–3413.
[69] X. Wu, R. He, Z. Sun, and T. Tan, “A light cnn for deep face
representation with noisy labels,” IEEE Transactions on Information
Forensics and Security, vol. 13, no. 11, pp. 2884–2896, 2018.

[70] X. Huang and S. Belongie, “Arbitrary style transfer in real-time with
adaptive instance normalization,” in IEEE International Conference on
Computer Vision, 2017, pp. 1501–1510.

[71] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality assess-
ment: From error visibility to structural similarity,” IEEE Transactions
on Image Processing, vol. 13, no. 4, pp. 600–612, 2004.

[72] Z. Wang, E. Simoncelli, and A. Bovik, “Multiscale structural similarity
for image quality assessment,” in The Asilomar Conference on Signals,
Systems Computers, 2003, pp. 1398–1402.

[73] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image
restoration with neural networks,” IEEE Transactions on Computational
Imaging, vol. 3, no. 1, pp. 47–57, 2017.

[74] S. Z. Li, D. Yi, Z. Lei, and S. Liao, “The casia nir-vis 2.0 face database,”
in IEEE Conference on Computer Vision and Pattern Recognition
Workshops, 2013, pp. 348–353.

[75] D. Huang, J. Sun, and Y. Wang, “The buaa-visnir face database instruc-
tions,” School Comput. Sci. Eng., Beihang Univ., Beijing, China, Tech.
Rep. IRIP-TR-12-FR-001, vol. 3, 2012.

[76] J. Chen, D. Yi, J. Yang, G. Zhao, S. Z. Li, and M. Pietikainen, “Learning
mappings for face synthesis from near infrared to visual light images,”
in IEEE Conference on Computer Vision and Pattern Recognition, 2009,
pp. 156–163.

[77] K. Panetta, Q. Wan, S. Agaian, S. Rajeev, S. Kamath, R. Rajendran,
S. P. Rao, A. Kaszowska, H. A. Taylor, A. Samani, and X. Yuan,
“A comprehensive database for benchmarking imaging systems,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 42,
no. 3, pp. 509–520, 2018.

[78] M. Shao and Y. Fu, “Cross-modality feature learning through generic hi-
erarchical hyperlingual-words,” IEEE Transactions on Neural Networks
and Learning Systems, vol. 28, no. 2, pp. 451–463, 2017.

[79] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, “Ms-celeb-1m: A dataset
and benchmark for large-scale face recognition,” in Proceedings of the
European Conference on Computer Vision, 2016, pp. 87–102.

[80] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[81] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
“Gans trained by a two time-scale update rule converge to a local nash
equilibrium,” in Advances in Neural Information Processing Systems,
2017, pp. 6629–6640.

[82] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in IEEE Conference on
Computer Vision and Pattern Recognition, 2016, pp. 2818–2826.
[83] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,
X. Chen, and X. Chen, “Improved techniques for training gans,” in
Advances in Neural Information Processing Systems, 2016, pp. 2234–
2242.

