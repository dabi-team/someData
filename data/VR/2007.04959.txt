Assistive VR Gym: Interactions with Real People
to Improve Virtual Assistive Robots

Zackory Erickson∗, Yijun Gu∗, and Charles C. Kemp

0
2
0
2

l
u
J

2
2

]

O
R
.
s
c
[

2
v
9
5
9
4
0
.
7
0
0
2
:
v
i
X
r
a

Abstract— Versatile robotic caregivers could beneﬁt millions
of people worldwide, including older adults and people with
disabilities. Recent work has explored how robotic caregivers
can learn to interact with people through physics simulations,
yet transferring what has been learned to real robots remains
challenging. Virtual reality (VR) has the potential to help bridge
the gap between simulations and the real world. We present
Assistive VR Gym (AVR Gym), which enables real people to
interact with virtual assistive robots. We also provide evidence
that AVR Gym can help researchers improve the performance
of simulation-trained assistive robots with real people. Prior to
AVR Gym, we trained robot control policies (Original Policies)
solely in simulation for four robotic caregiving tasks (robot-
assisted feeding, drinking, itch scratching, and bed bathing)
with two simulated robots (PR2 from Willow Garage and Jaco
from Kinova). With AVR Gym, we developed Revised Policies
based on insights gained from testing the Original policies with
real people. Through a formal study with eight participants
in AVR Gym, we found that the Original policies performed
poorly, the Revised policies performed signiﬁcantly better, and
that improvements to the biomechanical models used to train
the Revised policies resulted in simulated people that better
match real participants. Notably, participants signiﬁcantly dis-
agreed that the Original policies were successful at assistance,
but signiﬁcantly agreed that the Revised policies were successful
at assistance. Overall, our results suggest that VR can be used to
improve the performance of simulation-trained control policies
with real people without putting people at risk, thereby serving
as a valuable stepping stone to real robotic assistance.

I. INTRODUCTION

Robotic assistance with activities of daily living (ADLs)
could increase the independence of people with disabilities,
improve quality of life, and help address pressing societal
needs, such as aging populations, high healthcare costs, and
shortages of healthcare workers [1], [2].

Physics simulations provide an opportunity for robots
to safely learn how to physically assist people. Yet, the
reality gap between physics simulations and real assistance
poses several challenges. By bringing real people into the
robot’s virtual world, virtual reality (VR) has the potential
to serve as a stepping stone between simulated worlds and
the real world. For assistive robots, the person receiving
assistance is an extremely important and complex part of the
environment. As we show through a formal study, enabling
real people to interact with a virtual robot can quickly reveal
deﬁciencies through both objective and subjective measures
of performance.

Zackory Erickson, Yijun Gu, and Charles C. Kemp are with the Health-

care Robotics Lab, Georgia Institute of Technology, Atlanta, GA., USA.

Zackory Erickson is the corresponding author zackory@gatech.edu.
∗ Equal contribution.

Fig. 1. A participant using virtual reality to interact with a PR2 robot that
has learned how to provide feeding assistance through physics simulation.

To facilitate the use of VR in the development of assistive
robots, we present Assistive VR Gym1 (AVR Gym) an
open source framework that enables real people to interact
with virtual assistive robots within a physics simulation (see
Fig. 1). AVR Gym builds on Assistive Gym, which is an open
source physics simulation framework for assistive robots that
models multiple assistive tasks [3]. AVR Gym enables people
to interact with virtual robots without putting themselves at
risk, which is especially valuable when evaluating controllers
that have been trained in simulation with virtual humans.

As conﬁrmed through a formal study with eight partici-
pants, our use of AVR Gym enabled us to identify signiﬁcant,
unexpected shortcomings in the simulation-trained baseline
control policies that were originally released with Assistive
Gym. Moreover, we were able to use AVR Gym to improve
the control policies by discovering that the simulated humans
did not adequately represent the biomechanics of real people.
Improving the simulated humans used to train control poli-
cies, resulted in Revised control policies that dramatically
outperformed our Original policies.

In this paper, we make the following contributions:
• Present Assistive VR Gym (AVR Gym), an open source
framework that enables real people to interact with
virtual assistive robots.

• Present experimental methods, including objective and
subjective measures, to evaluate virtual assistive robots.
• Provide evidence that VR can be used to improve per-
formance of simulation-trained robot control policies.
• Provide evidence that biomechanical models signiﬁ-
cantly impact policy performance and can be assessed
and improved via VR.

1https://github.com/Healthcare-Robotics/assistive-vr-gym

 
 
 
 
 
 
II. RELATED WORK

III. ASSISTIVE VR GYM

A. Physically Assistive Robotics

A signiﬁcant amount of research has been conducted on
real robotic systems for providing physical assistance to peo-
ple. For example, researchers have demonstrated how table-
mounted robotic arms can provide drinking assistance to peo-
ple using a force sensing smart cup [4] and a brain-machine
interface for shared-autonomy [5]. Similarly, robots have
shown promising results for providing feeding assistance [6],
[7], [8], [9], [10], [11]. Prior research has also investigated
robotic assistance for bed bathing [12], [13], which can be
especially valuable for people who are conﬁned to a bed due
to disabilities or injuries. Robots also present an opportunity
to help individuals in getting dressed with a variety of
garments. Robot-assisted dressing has been demonstrated
on several robotic platforms for assisting both able-bodied
participants and real people with disabilities [14], [15], [16],
[17], [18], [13], [19], [20], [21], [22].

Despite these promising results in physical robotic assis-
tance, it remains challenging to design robotic systems that
can assist with multiple tasks across a wide spectrum of
human shapes, sizes, weights, and disabilities. This is due
in part to difﬁculties and costs associated with evaluating
assistive robotic systems across a large distribution of people.
Physics simulation presents an opportunity for robots to
learn to safely interact with people over many tasks and
environments. For instance, researchers have demonstrated
the use of physics simulation for learning robot controllers
for robot-assisted dressing tasks [23], [24], [25], [26]. Clegg
et al. has also used Assistive Gym to learn control policies
for a real PR2 robot to dress an arm of a humanoid robot
with a hospital gown [27].

B. Virtual Reality

We use virtual reality to evaluate and improve simulation-
trained assistive robots with real people. Studies have pro-
vided evidence that virtual reality can offer people both a
sense of presence and embodiment [28], [29]. Accordingly,
VR provides an opportunity to assess important attributes
including proxemics,
for human-robot
legibility of motion, and embodiment [30], [31].

interaction (HRI),

Virtual reality has been widely used as a tool to improve
robot performance across an assortment of tasks. A common
use for virtual reality is for teleoperation of robots for
dexterous manipulation tasks [32], [33]. This virtual reality
teleoperation has also been used to collect high-quality
robotic manipulation demonstrations [34], [35], [36]. Within
robotic rehabilitation, virtual reality has been used to enhance
rehabilitation training and assess rehabilitation metrics in real
time [37], [38], [39], [40], [41].

Across these works, virtual reality has often been used for
people to take control of robots; to provide demonstrations
and to train robots for performing various tasks. In this paper,
we show that virtual reality can be used to safely evaluate and
improve simulation-trained assistive robot controllers with
real people.

AVR Gym builds on Assistive Gym and is implemented
using the open source PyBullet physics engine [42]. We
connect assistive environments from Assistive Gym into
virtual reality with vrBullet, a virtual reality physics server
that uses the OpenVR API. We use the Oculus Rift S virtual
reality head-mounted display, which uses inside-out tracking,
and two Oculus Touch controllers, each of which provide 3-
DoF position and 3-DoF orientation tracking. Fig. 2 depicts
the virtual reality setup with 3rd and 1st person perspectives
of the virtual environment.

The simulated human model

in virtual reality has 20
controllable joints including two 7-DoF arms, a 3-DoF head,
and a 3-DoF waist. Using the Oculus’ inside-out tracking,
we can estimate the height of each human participant as the
difference from the headset to the ground. Given this height
estimate, we modify the height of the simulated human body
to match that of the real human participant. We then used the
VR headset to align the head and waist joints of a virtual
human model to the person’s pose. At each time step, we
queried the headset for its 3D Cartesian position, χ, and 3D
orientation, θ = (θr, θp, θy). We directly set the roll, pitch,
and yaw joints of the simulated human head to align with
the orientation, θ, of the headset.

To align the waist pose, we computed the appropriate
angles needed such that the simulated human had the same
3D Cartesian head position as the real person. We ﬁrst
observed the ﬁxed 3D position for the center of a person’s
waist, W , given as a ﬁxed offset above the wheelchair
or hospital bed. Let ψ = (ψx, ψy, ψz) = χ − W be
the 3D vector from the center of a person’s waist to the
center of their head. We then computed the roll, pitch, and
yaw (rψ, pψ, yψ) orientations for the waist of the simulated
human according to:

rψ = atan2(ψy, ψz)
pψ = atan2(ψx · cos(rψ), ψz)
yψ = atan2(cos(rψ), sin(rψ) · sin(pψ))

Finally, we observe that people often prefer to use some waist
motion when looking left or right. To account for this, we
distributed some of the measured head yaw orientation to the
waist. Speciﬁcally, we set the yaw orientation of the head to
be 0.7(θy − yψ), and the yaw orientation of the waist joint
to be 0.3(θy − yψ).

As shown in Fig. 2, participants also hold two Oculus
Touch controllers, which we use to control both arms of
the simulated human in virtual reality. At each time step, we
queried each handheld controller for its 3D Cartesian position
and orientation in global space. Using inverse kinematics
with a damped least squares method, we compute the 7-DoF
arm joint angles necessary for the hand of the simulated
person to have the same position and orientation as measured
by the controller. Since we are optimizing for a 7-DoF arm
pose using a 6-DoF controller pose measurement, this inverse
problem is ill-posed, which implies that stable solutions
are not guaranteed. However, we have found this inverse

Fig. 2.
of the simulated human model that the human participant is controlling. (Right) 1st person perspective of what a participant observed in virtual reality.

(Left) Human participant using the virtual reality interface to receive drinking assistance from a simulated robot. (Middle) 3rd person perspective

IV. SIMULATION-TRAINED ROBOT CONTROL POLICIES

Prior to developing AVR Gym, we trained eight robot
control policies solely in simulation for the four caregiving
tasks (robot-assisted feeding, drinking, itch scratching, and
bed bathing) and two collaborative robots (the PR2 from
Willow Garage and the 7-DoF Jaco (Gen2) arm from Ki-
nova). These policies were the baseline policies released with
Assistive Gym and described in the corresponding paper [3].
We trained slightly modiﬁed versions of these eight robot
control policies to run in AVR Gym, which we refer to as
the Original Policies.

A. Original Policy Training

For each assistive task and robot, we follow the same
training procedure presented in [3]. At each time step during
simulation, the robot executes an action and then receives a
reward and an observation based on the state of the world.
Actions for a robot’s 7-DoF arm are represented as changes
in joint angles, ∆P ∈ R7. The PR2 uses only its right or
left arm depending on the assistive task. The observations
for a robot include the 7D joint angles of the robot’s arm,
the 3D position and orientation of the end effector, and the
forces applied at the end effector. The robot’s observation
also includes details of task-relevant human joints, including
3D positions of the shoulder, wrist, and elbow during the bed
bathing and itch scratching assistive tasks, and the position
and orientation of the person’s head during feeding and
drinking assistance. Both the Jaco and PR2 robots use the
same observation and reward functions during training.

Our Original policies were trained using proximal policy
optimization (PPO) [44], which is an actor-critic deep rein-
forcement learning approach that has recently shown success
in assistive robotic contexts [45], [27], [3]. These policies
are modeled using a fully-connected neural network with
two hidden layers of 64 nodes and tanh activations. We train
policies for 50,000 simulation rollouts (trials), where each
rollout consists of 200 consecutive time steps (20 seconds
of simulation time at 10 time steps per second). Prior to
each simulation rollout, we randomly initialize the simulated
person’s arm pose for itch scratching and bed bathing tasks,
and we randomize the head orientation for feeding and
drinking tasks. Once initialized, the human holds a static
pose throughout the entire rollout. Each policy is trained
with default male and female human models, with heights,
body sizes, weights, and joint limits matching published 50th
percentile values [46]. Note that these control policies are
not temporal models and do not consider observations from

Fig. 3. Assistive environments in physics simulation with the Jaco and
PR2 robots. (Top row) Feeding and drinking assistance. (Bottom row) Itch
scratching and bed bathing assistance.

kinematics optimization to work well in practice, with only
minor offsets in the estimated pose of a person’s full arm.

A. Test Environments

AVR Gym builds upon four physics-based assistive envi-
ronments from Assistive Gym. A key aspect of achieving
realism for users was to match the simulated wheelchair and
bed to a real wheelchair and bed. Each of these environments,
shown in Fig. 3, is associated with activities of daily living
(ADLs) [43], including:
• Feeding: A robot holds a spoon and aims to move the
spoon to a person’s mouth. We place small spheres on the
spoon, representing food. The person sits in a wheelchair
during assistance.

• Drinking Water: A robot holds a cup containing small
particles that represent water. The robot aims to move the
cup toward the person’s mouth and tilt the cup to help the
person drink the water. The person sits in a wheelchair
during assistance.

• Itch Scratching: A robot aims to scratch a randomly
generated location along the person’s right arm. The robot
grasps a scratching tool in its left end effector. The person
sits in a wheelchair during assistance.

• Bed Bathing: The robot holds a simulated washcloth tool
while the person lies on a hospital bed in a randomly gen-
erated resting pose. The robot aims to move the washcloth
around the person’s right arm in order to clean the arm.
These four assistive environments in AVR Gym are nearly
identical to the original environments presented in [3], in-
cluding the same male and female human models, realistic
human joint
limit models, robot base pose optimization,
reward functions with human preferences, and task goals.

previous time steps in a rollout. As such, these models are
not affected by the speciﬁc nature of human motion, allowing
us to evaluate these policies with real people without needing
to model realistic human motion in simulation.

B. Revised Policies

During early pilot studies in AVR Gym with lab members,
we observed that the Original policies exhibited unexpected
deﬁciencies and poor performance when providing assistance
to human participants in virtual reality. Most notable were
failures in the itch scratching and bed bathing assistance
tasks, where the robots would fail to move their end effectors
closer to the person’s body. Through iterative investigation
and development with real people in AVR Gym, we de-
veloped the Revised Policies, based on the key discovery
that the biomechanical models of simulated people we used
to train the Original policies signiﬁcantly differed from
the biomechanics of real people. The two leading factors
were variation among human heights and waist bending
movements, wherein the Original policies were trained on
male and female models with ﬁxed heights and were not
trained on any variation among the human waist joints.

Given these ﬁndings, we modiﬁed each simulation en-
the simulated human biomechanics
vironment such that
better match people in VR. Speciﬁcally, we randomized the
simulated human torso height and initial waist orientation
before each training trial began. The person’s torso height,
measured from hipbone to center of the head, was uniformly
sampled from 50 cm to 70 cm for male models and 44 cm to
64 cm for female models. We then randomized the simulated
person’s initial three kinematic waist joints to angles within
the range of (-10◦, 10◦). With these improved biomechanical
simulations, we trained a new set of eight Revised Policies,
one for each robot and assistive task, using the same training
process discussed in Section IV-A.

When evaluated with simulation humans, these Revised
policies achieve similar rewards and task success rates as the
Original policies. However, as conﬁrmed by our formal study
(Section V), the Revised policies overcame the limitations
of the Original policies and performed signiﬁcantly better
across both subjective and objective metrics when evaluated
with real people in VR.

V. FORMAL STUDY

We conducted a formal study with eight participants
to evaluate the performance of the Original policies and
the Revised policies in terms of objective and subjective
measures. In addition, we performed a posthoc analysis of
biomechanical differences between the simulated humans
used to train the Original policies, the simulated humans
used to train the Revised policies, and the real humans who
participated in our study.

A. Experimental Procedure

We conducted an experiment with eight able-bodied hu-
man participants (four females and four males). We obtained
informed consent from all participants and approval from the

Georgia Institute of Technology Institutional Review Board
(IRB). We recruited participants to meet the following inclu-
sion/exclusion criteria: 18 years of age or older; able-bodied;
no cognitive or visual impairments; ﬂuent in spoken and
written English; and not diagnosed with epilepsy. Participant
ages ranged from 19 to 24 years old with torso heights varied
between 0.51 and 0.58 meters.

For each participant, we conducted a total of 16 trials
in VR with four trials for each of the four assistive tasks.
The four trials for each task were organized such that we
evaluated both the Original and Revised policies on both the
PR2 and Jaco robots. We randomized the ordering of assis-
tive tasks, robots, and control policies for each participant
according to a randomized counterbalancing design.

During the study, participants wore the virtual reality head-
set and held both controllers. Participants sat in a wheelchair
for the itch scratching, feeding, and drinking tasks and laid
in a hospital bed for bed bathing assistance. Similar to the
simulation environments used for training control policies,
each trial in virtual reality lasted 20 seconds for a total of
200 time steps. The robot executed an action at each time
step in virtual reality, once every 0.1 seconds. Participants
were instructed that they could move their arms, upper body,
and head to interact with the simulated robots towards the
goal of successfully receiving assistance for each task. Prior
to each assistive task, we gave participants an unscripted
practice trial to familiarize themselves on interacting with the
robot and how to accomplish the task in virtual reality. We
randomly selected either the PR2 or Jaco robot and used the
Revised control policies (see Section IV-B) for each practice
trial. In order to elicit a wide range of human motion and
interactions throughout our study, we instructed participants
to accomplish a task (e.g. drink water from the cup), but
we did not provide instructions on how to interact with the
robot or how to appropriately complete the task. Our study
and experiments can be found in the supplementary video.

B. Objective Measures

We used the reward functions and success percentages
deﬁned in Assistive Gym as objective measures of perfor-
mance [3].
• Feeding: The robot is rewarded for moving the spoon
closer to the person’s mouth and when food enters the
person’s mouth. The robot is penalized for dropping food
or applying large forces to the person. Task success is
deﬁned by the robot feeding at least 75% of all food
particles to the person’s mouth.

• Drinking Water: The robot is rewarded for moving the
cup closer to the person’s mouth, tilting the cup for drink-
ing, and when water particles enter the person’s mouth.
The robot is penalized for spilling water or applying large
forces to the person. Task success is deﬁned by the robot
pouring at least 75% of all water into the person’s mouth.
• Itch Scratching: The robot is rewarded for moving the
scratching tool closer to the itch location and for per-
forming scratching motions around the itch. The robot
is penalized for applying large forces to the person, or

Fig. 4. The PR2 robot fails to provide assistance with bed bathing in virtual reality when using the Original control policy. Blue markers are placed
uniformly around the person’s right arm, which the robot can clean off with the bottom of the wiping tool.

Fig. 5. The Jaco fails to provide itch scratching assistance in virtual reality with the Original control policy. The itch is represented by a blue marker.

more than 10 N of force [25] near the itch. Task success
is deﬁned by the robot performing at least 25 scratching
motions near the itch.
• Bed Bathing: The robot

is rewarded for moving the
washcloth closer to the person’s arm and for using the
bottom of the washcloth to wipe off markers uniformly
distributed (3 cm apart) along the person’s right arm. The
robot is penalized for applying large forces to the person.
Task success is deﬁned by the robot wiping off at least
30% of all markers along a person’s arm)

C. Subjective Measures

In order to assess participants’ perceptions of the con-
trol policies, we used a questionnaire with four statements
pertaining to perceptions of the robot’s performance, safety,
comfort and speed. For each statement, participants were
asked to record how much they agreed with the statement
on an interval scale from 1 (strongly disagree) to 7 (strongly
agree) with 4 being neutral. We based this 7-point scale
questionnaire on past work on robot-assisted feeding [47].

The four statements follow:
• L1: The robot successfully assisted me with the task.
• L2: I felt comfortable with how the robot assisted me

in VR.

• L3: I felt I would be safe if this were a real robot.
• L4: The robot moved with appropriate speed.

A. Performance of the Original Policies

VI. RESULTS

Table I depicts the average reward and task success when
both robots used the Original policies in simulation with
static human models and in AVR Gym with the eight

TABLE I
AVERAGE REWARD AND TASK SUCCESS BETWEEN SIMULATION AND
VIRTUAL REALITY USING THE ORIGINAL CONTROL POLICIES.

Simulation

Virtual Reality

Task

PR2

Jaco

PR2

Jaco

Feeding
Drinking
Scratching
Bathing

103 (77%)
380 (59%)
62 (35%)
85 (13%)

82 (75%)
144 (32%)
218 (48%)
118 (28%)

122 (100%)
502 (75%)
25 (25%)
-87 (0%)

37 (63%)
-92 (0%)
-69 (0%)
-126 (0%)

Avg. Success

46%

46%

50%

16%

human participants. When evaluating the Original policy
for a given robot and assistive task in virtual reality, we
averaged rewards and task success over trials from all eight
participants. For simulation, we averaged rewards and task
success over 100 simulation rollouts with a random initial
human pose for each rollout. Since the simulated human
holds a static pose throughout an entire trial, we take an
average over a larger number of simulation trials to evaluate
performance over multiple human poses. We note that each
task uses a slightly different reward function due to task-
speciﬁc reward elements and hence reward values are not
directly comparable across different tasks.

For the feeding assistance task,

the Original policies
achieved similar average reward and task success with real
people as they achieved in simulation with ﬁxed human
models. However, performance was more varied for the
drinking, itch scratching, and bed bathing tasks. The most
noticeable errors occurred with the itch scratching and bed
bathing tasks, where the Original policies for both the Jaco
and PR2 robots frequently failed to move their end effectors

TABLE II
AVERAGE REWARD AND TASK SUCCESS FOR THE ORIGINAL AND

REVISED CONTROL POLICIES IN VR WITH REAL PEOPLE.

Original Policies

Revised Policies

Task

PR2

Jaco

PR2

Jaco

Feeding
Drinking
Scratching
Bathing

122 (100%)
502 (75%)
25 (25%)
-87 (0%)

37 (63%)
-92 (0%)
-69 (0%)
-126 (0%)

113 (100%)
458 (75%)
36 (45%)
-18 (0%)

36 (45%)
199 (42%)
95 (62%)
-16 (0%)

Avg. Success

50%

16%

55%

37%

closer to a person’s body. These errors can be visually seen in
Fig. 4 where the PR2 actively moved away from the person
during bed bathing assistance and in Fig. 5 where the Jaco
robot actuated itself to behind the wheelchair rather than
scratching an itch on the person’s arm. Differences in results
between the two robots can be partially attributed to differ-
ences in robot kinematics and base positioning. The Jaco is
mounted to a ﬁxed position on the wheelchair, whereas we
optimize the PR2’s base pose near a person according to
joint-limit-weighted kinematic isotropy (JLWKI) and task-
centric manipulability metrics [3], [48].

Except for responses about the robot’s speed (L4), par-
ticipants’ average responses were negative or neutral (L1,
L2, and L3) (see Fig. 8). Most notably, participants tended
to perceive the robots as being unsuccessful at assistance,
as evidenced by responses to L1 being signiﬁcantly below
neutral (4) with p < 0.05 using a Wilcoxon signed-rank test.
Responses did vary by task. When the robots used the
Original policies for itch scratching assistance, participants
slightly disagreed with the statement on successful assistance
(L1), with an average response of 3.3 across all participants.
When using the Original policies for bed bathing assistance,
participants reported an even lower average rating of 1.2,
where almost all participants strongly disagreed that
the
robots provided successful bathing assistance.

B. Performance of the Revised Policies

From the table, we observe that

Table II compares the average reward and task success
between the Original and Revised control policies during our
VR study. For a given task and robot, we average the rewards
and task success rates over trials from all eight participants.
the Revised policies
for both robots outperformed the Original policies when
providing itch scratching and bed bathing assistance. Fig. 6
demonstrates the Jaco robot using a Revised policy to provide
bed bathing assistance to a participant
in virtual reality.
While the Original policies for the Jaco robot failed to
reliably assist human participants with drinking, we observe
that the Revised policies for the Jaco exhibited a greater
level of success. Fig. 7 depicts an image sequence of the
Jaco robot using a Revised control policy to provide drinking
assistance. Based on these success metrics and qualitative
image sequences, we ﬁnd that the Revised polices exhibited
reasonable performance for all of the assistive tasks. How-
ever, we note that these revised controllers are not perfect
and still exhibit some errors. This is especially true for hard

to reach locations, such as scratching an itch underneath
a person’s arm. These control policies also assume that
the person receiving assistance acts collaboratively with the
robot. Additional research is required to explore how robot
controllers learned in simulation can act appropriately when
exposed to uncooperative or adversarial human motion.

For all four statements (L1, L2, L3, and L4), participants’
average responses were positive or neutral (see Fig. 8). Most
notably, participants tended to perceive the robots as being
successful at assistance, as evidenced by responses to L1
being signiﬁcantly above neutral (4) with p < 0.001 using a
Wilcoxon signed-rank test.

Fig. 8 displays a comparison of questionnaire responses
between the Original and Revised policies when responses
are averaged over all participants, tasks, and robots for each
questionnaire statement. On average, participants provide
higher responses for trials in which a robot used a Revised
policy to provide assistance. To determine a statistical dif-
ference between the Original and Revised policies, we apply
a Wilcoxon signed-rank test, a non-parametric test, between
participant responses from trials using the Original control
policies and responses when using the Revised policies. The
computed p-values are depicted in Fig. 8, where we observed
a statistically signiﬁcant difference at the p < .001 level for
the ﬁrst three questions relating to task success, comfort, and
safety. These results indicate that participants perceived a
noticeable improvement in a robot’s performance and safety
when using Revised control policies that were trained based
on biomechanics feedback from virtual reality.

Overall, we found that that our Revised policies signif-
icantly outperformed the Original policies, which indicates
that VR can be used to efﬁciently improve simulation-trained
controllers for physical human-robot interaction.

C. Posthoc Biomechanical Analysis

Our iterative research and development using AVR Gym,
discussed in Section IV-B, resulted in Revised policies
trained with new distributions of human torso heights and
waist orientations. Here, we provide a posthoc statistical
analysis of these biomechanical parameters for our simulated
humans and our real participants in virtual reality.

During our human study, we aligned the human model in
virtual reality with a real participant’s pose, as described in
Section III. This alignment enabled us to record the full state,
height, and pose of a participant at each time step during the
virtual reality trials.

To determine whether a statistical difference exists be-
tween human torso heights, we applied a Wilcoxon signed-
rank test between human torso heights in the simulation envi-
ronments used to train the Original policies and torso heights
of human participants in virtual reality. This test returned a
p-value of p < 0.001, indicating that the original simulation
environments did not properly model the distribution of real
human heights observed during virtual reality.

We then applied the same test to a set of torso heights
from the updated simulation environments used to train the
Revised policies. When compared to torso heights of human

Fig. 6.

Image sequence of a table-mounted Jaco providing bed bathing assistance using the Revised control policy.

Fig. 7.

Image sequence of drinking assistance with the Jaco and the Revised control policy.

simulation environments used to train our Revised policies
better matched the biomechanics of our participants in VR.

VII. CONCLUSION

We presented Assistive VR Gym (AVR Gym), which
enables real people to interact with virtual assistive robots.
We also provided evidence that AVR Gym can help re-
searchers improve the performance of simulation-trained
assistive robots. Overall, our results suggest that VR can be
used to help bridge the gap between physics simulations and
the real world by enabling real people to interact with virtual
robots.

ACKNOWLEDGMENT
We thank Pieter Abbeel, Anca Dragan, and Deepak Pathak for
feedback about this work. This work was supported by NSF award
IIS-1514258 and AWS Cloud Credits for Research. Dr. Kemp owns
equity in and works for Hello Robot, a company commercializing
robotic assistance technologies.

REFERENCES

[1] D. M. Taylor, “Americans with disabilities: 2014,” United States

Census Bureau, 2018.

[2] S. for Enhancing Geriatric Understanding, E. A. Surgical, and A. G. S.
Medical Specialists (SEGUE), “Retooling for an aging america: Build-
ing the healthcare workforce,” Journal of the American Geriatrics
Society, vol. 59, no. 8, pp. 1537–1539, 2011.

[3] Z. Erickson, V. Gangaram, A. Kapusta, C. K. Liu, and C. C.
Kemp, “Assistive gym: A physics simulation framework for assistive
robotics,” IEEE International Conference on Robotics and Automation
(ICRA), 2020.

[4] F. F. Goldau, T. K. Shastha, M. Kyrarini, and A. Gr¨aser, “Autonomous
multi-sensory robotic assistant for a drinking task,” in 2019 IEEE 16th
International Conference on Rehabilitation Robotics (ICORR), 2019.
[5] S. Schr¨oer, I. Killmann, B. Frank, M. V¨olker, L. Fiederer, T. Ball, and
W. Burgard, “An autonomous robotic assistant for drinking,” in 2015
IEEE International Conference on Robotics and Automation (ICRA).
IEEE, 2015, pp. 6482–6487.

Fig. 8.
Comparison of 7-point scale questionnaire responses for both
Original and Revised policies. Responses are averaged over all participants,
tasks, and robots. p-values are computed with a Wilcoxon signed-rank test.

participants, the test returns a p-value of p = 0.29, which
indicates that there is a less signiﬁcant difference between
the distributions of human heights used to train the Revised
control policies and human heights observed in virtual reality.
In order to evaluate differences in waist orientations be-
tween simulation and virtual reality, we consider the 3D
position of a person’s head, averaged over time. Since the
human model in virtual reality is aligned with a real partici-
pant’s pose, we track the 3D head position of a participant by
querying the 3D head position of the virtual human model.
In order to separate biomechanical differences in torso height
from waist orientations, we modiﬁed the original simulation
environments to include random variation among human
torso heights, but no variation among waist orientations.
Using a Wilcoxon signed-rank test, we found a statistically
signiﬁcant difference in head position (averaged over all time
steps for a trial) between data from these new simulation
environments and the head positions of participants in virtual
reality, with a p-value of p < 0.01. However, by performing
this same test between head positions from the revised
simulation environments (Section IV-B) and from VR, we
found a p-value of p = 0.26, indicating that the improved

***p < .001p < .05***********[6] D. Park, Y. K. Kim, Z. M. Erickson, and C. C. Kemp, “Towards
assistive feeding with a general-purpose mobile manipulator,” arXiv
preprint arXiv:1605.07996, 2016.

[7] G. Canal, G. Aleny`a, and C. Torras, “Personalization framework for
adaptive robotic feeding assistance,” in International Conference on
Social Robotics. Springer, 2016, pp. 22–31.

[8] C. J. Perera, T. D. Lalitharatne, and K. Kiguchi, “Eeg-controlled meal
assistance robot with camera-based automatic mouth position tracking
and mouth open detection,” in 2017 IEEE International Conference
on Robotics and Automation (ICRA).

IEEE, 2017, pp. 1760–1765.

[9] D. Park, Y. Hoshi, and C. C. Kemp, “A multimodal anomaly detector
for robot-assisted feeding using an lstm-based variational autoen-
coder,” IEEE Robotics and Automation Letters, 2018.

[10] T. Rhodes and M. Veloso, “Robot-driven trajectory improvement
for feeding tasks,” in 2018 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS).
IEEE, 2018, pp. 2991–2996.
[11] T. Bhattacharjee, E. K. Gordon, R. Scalise, M. E. Cabrera, A. Caspi,
M. Cakmak, and S. S. Srinivasa, “Is more autonomy always better?
exploring preferences of users with mobility impairments in robot-
assisted feeding,” in Proceedings of the 2020 ACM/IEEE International
Conference on Human-Robot Interaction, 2020, pp. 181–190.
[12] C.-H. King, T. L. Chen, A. Jain, and C. C. Kemp, “Towards an assistive
robot that autonomously performs bed baths for patient hygiene,” in
2010 IEEE/RSJ International Conference on Intelligent Robots and
Systems.

IEEE, 2010, pp. 319–324.

[13] Z. Erickson, H. M. Clever, V. Gangaram, G. Turk, C. K. Liu, and
C. C. Kemp, “Multidimensional capacitive sensing for robot-assisted
dressing and bathing,” in 2019 IEEE 16th International Conference
on Rehabilitation Robotics (ICORR).

IEEE, 2019, pp. 224–231.

[14] S. D. Klee, B. Q. Ferreira, R. Silva, J. P. Costeira, F. S. Melo, and
M. Veloso, “Personalized assistance for dressing users,” in Interna-
tional Conference on Social Robotics. Springer, 2015, pp. 359–369.
[15] R. P. Joshi, N. Koganti, and T. Shibata, “Robotic cloth manipulation
for clothing assistance task using dynamic movement primitives,” in
Proceedings of the Advances in Robotics, 2017, pp. 1–6.

[16] G. Canal, E. Pignat, G. Aleny`a, S. Calinon, and C. Torras, “Joining
high-level symbolic planning with low-level motion primitives in adap-
tive hri: application to dressing assistance,” in 2018 IEEE International
Conference on Robotics and Automation (ICRA).

IEEE, 2018.

[17] A. Jevti´c, A. F. Valle, G. Aleny`a, G. Chance, P. Caleb-Solly, S. Do-
gramadzi, and C. Torras, “Personalized robot assistant for support in
dressing,” IEEE transactions on cognitive and developmental systems,
vol. 11, no. 3, pp. 363–374, 2018.

[18] Z. Erickson, M. Collier, A. Kapusta, and C. C. Kemp, “Tracking
human pose during robot-assisted dressing using single-axis capacitive
proximity sensing,” IEEE Robotics and Automation Letters, vol. 3,
no. 3, pp. 2245–2252, 2018.

[19] G. Chance, A. Jevti´c, P. Caleb-Solly, G. Alenya, C. Torras, and
S. Dogramadzi, “elbows outpredictive tracking of partially occluded
pose for robot-assisted dressing,” IEEE Robotics and Automation
Letters, vol. 3, no. 4, pp. 3598–3605, 2018.

[20] E. Pignat and S. Calinon, “Learning adaptive dressing assistance from
human demonstration,” Robotics and Autonomous Systems, 2017.
[21] N. Koganti, T. Tamei, K. Ikeda, and T. Shibata, “Bayesian nonparamet-
ric motor-skill representations for efﬁcient learning of robotic clothing
assistance,” in Workshop on Practical Bayesian Nonparametrics, Neu-
ral Information Processing Systems, 2016, 2016, pp. 1–5.

[22] Y. Gao, H. J. Chang, and Y. Demiris, “Iterative path optimisation for
personalised dressing assistance using vision and force information,”
in 2016 IEEE/RSJ international conference on intelligent robots and
systems (IROS).

IEEE, 2016, pp. 4398–4403.
[23] Z. Erickson, A. Clegg, W. Yu, G. Turk, C. K. Liu, and C. C. Kemp,
“What does the person feel? learning to infer applied forces during
robot-assisted dressing,” in 2017 IEEE International Conference on
Robotics and Automation (ICRA).

IEEE, 2017, pp. 6058–6065.

[24] W. Yu, A. Kapusta, J. Tan, C. C. Kemp, G. Turk, and C. K.
Liu, “Haptic simulation for robot-assisted dressing,” in 2017 IEEE
International Conference on Robotics and Automation (ICRA).
IEEE,
2017, pp. 6044–6051.

[25] Z. Erickson, H. M. Clever, G. Turk, C. K. Liu, and C. C. Kemp, “Deep
haptic model predictive control for robot-assisted dressing,” in 2018
IEEE International Conference on Robotics and Automation (ICRA).
IEEE, 2018, pp. 1–8.

[26] A. Kapusta, Z. Erickson, H. M. Clever, W. Yu, C. K. Liu, G. Turk,

and C. C. Kemp, “Personalized collaborative plans for robot-assisted
dressing via optimization and simulation,” Autonomous Robots, 2019.
[27] A. Clegg, Z. Erickson, P. Grady, G. Turk, C. C. Kemp, and C. K. Liu,
“Learning to collaborate from simulation for robot-assisted dressing,”
IEEE Robotics and Automation Letters, 2020.

[28] R. M. Ba˜nos, C. Botella, M. Alca˜niz, V. Lia˜no, B. Guerrero, and
B. Rey, “Immersion and emotion:
their impact on the sense of
presence,” Cyberpsychology & behavior, vol. 7, no. 6, pp. 734–741,
2004.

[29] K. Kilteni, R. Groten, and M. Slater, “The sense of embodiment
in virtual reality,” Presence: Teleoperators and Virtual Environments,
vol. 21, no. 4, pp. 373–387, 2012.

[30] L. Takayama and C. Pantofaru, “Inﬂuences on proxemic behaviors in
human-robot interaction,” in 2009 IEEE/RSJ International Conference
on Intelligent Robots and Systems.

IEEE, 2009, pp. 5495–5502.

[31] J. Wainer, D. J. Feil-Seifer, D. A. Shell, and M. J. Mataric, “The
role of physical embodiment in human-robot interaction,” in ROMAN
2006-The 15th IEEE International Symposium on Robot and Human
Interactive Communication.

IEEE, 2006, pp. 117–122.

[32] V. Kumar and E. Todorov, “Mujoco haptix: A virtual reality system for
hand manipulation,” in 2015 IEEE-RAS 15th International Conference
on Humanoid Robots (Humanoids).

IEEE, 2015, pp. 657–663.

[33] J. I. Lipton, A. J. Fay, and D. Rus, “Baxter’s homunculus: Virtual
reality spaces for teleoperation in manufacturing,” IEEE Robotics and
Automation Letters, vol. 3, no. 1, pp. 179–186, 2017.

[34] V. Kumar, A. Gupta, E. Todorov, and S. Levine, “Learning dexterous
manipulation policies from experience and imitation,” arXiv preprint
arXiv:1611.05095, 2016.

[35] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg,
and P. Abbeel, “Deep imitation learning for complex manipulation
tasks from virtual reality teleoperation,” in 2018 IEEE International
Conference on Robotics and Automation (ICRA).

IEEE, 2018.

[36] H. Liu, Z. Zhang, X. Xie, Y. Zhu, Y. Liu, Y. Wang, and S.-C. Zhu,
“High-ﬁdelity grasping in virtual reality using a glove-based system,”
in 2019 International Conference on Robotics and Automation (ICRA).
IEEE, 2019, pp. 5180–5186.

[37] A. Frisoli, M. Bergamasco, M. C. Carboncini, and B. Rossi, “Robotic
assisted rehabilitation in virtual reality with the l-exos,” Stud Health
Technol Inform, vol. 145, pp. 40–54, 2009.

[38] K. Br¨utsch, A. Koenig, L. Zimmerli, S. M´erillat-Koeneke, R. Riener,
L. J¨ancke, H. J. van Hedel, and A. Meyer-Heim, “Virtual reality for
enhancement of robot-assisted gait training in children with neurolog-
ical gait disorders,” Journal of rehabilitation medicine, 2011.
[39] B. R. Ballester, L. S. Oliva, A. Duff, and P. Verschure, “Acceler-
ating motor adaptation by virtual reality based modulation of error
memories,” in 2015 IEEE International Conference on Rehabilitation
Robotics (ICORR).
IEEE, 2015, pp. 623–629.

[40] X. Huang, F. Naghdy, G. Naghdy, H. Du, and C. Todd, “The combined
effects of adaptive control and virtual reality on robot-assisted ﬁne
hand motion rehabilitation in chronic stroke patients: A case study,”
Journal of Stroke and Cerebrovascular Diseases, 2018.

[41] F. Bernardoni, ¨O. ¨Ozen, K. Buetler, and L. Marchal-Crespo, “Virtual
reality environments and haptic strategies to enhance implicit learning
and motivation in robot-assisted training,” in 2019 IEEE 16th Interna-
tional Conference on Rehabilitation Robotics (ICORR).
IEEE, 2019.
[42] E. Coumans and Y. Bai, “Pybullet, a python module for physics
simulation for games, robotics and machine learning,” http://pybullet.
org, 2016–2019.

[43] M. P. Lawton and E. M. Brody, “Assessment of older people: self-
maintaining and instrumental activities of daily living,” The gerontol-
ogist, vol. 9, no. 3 Part 1, pp. 179–186, 1969.

[44] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
preprint

optimization

algorithms,”

policy

arXiv

“Proximal
arXiv:1707.06347, 2017.

[45] A. Clegg, “Modeling human and robot behavior during dressing tasks,”
Ph.D. dissertation, The Georgia Institute of Technology, 2019.
[46] A. R. Tilley, The measure of man and woman: human factors in design.

John Wiley & Sons, 2002.

[47] D. Park, Y. Hoshi, H. P. Mahajan, H. K. Kim, Z. Erickson, W. A.
Rogers, and C. C. Kemp, “Active robot-assisted feeding with a general-
purpose mobile manipulator: Design, evaluation, and lessons learned,”
Robotics and Autonomous Systems, vol. 124, p. 103344, 2020.
[48] A. Kapusta and C. C. Kemp, “Task-centric optimization of conﬁgu-
rations for assistive robots,” Autonomous Robots, vol. 43, no. 8, pp.
2033–2054, 2019.

