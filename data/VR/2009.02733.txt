A Convolutional Neural Network-Based Low
Complexity Filter

Chao Liu, Student Member, IEEE Heming Sun, Member, IEEE Jiro Katto, Member, IEEE
Xiaoyang Zeng, Member, IEEE and Yibo Fan

1

0
2
0
2

p
e
S
6

]

V

I
.
s
s
e
e
[

1
v
3
3
7
2
0
.
9
0
0
2
:
v
i
X
r
a

Abstract—Convolutional Neural Network (CNN)-based ﬁlters
have achieved signiﬁcant performance in video artifacts reduc-
tion. However, the high complexity of existing methods makes it
difﬁcult to be applied in real usage. In this paper, a CNN-based
low complexity ﬁlter is proposed. We utilize depth separable
convolution (DSC) merged with the batch normalization (BN) as
the backbone of our proposed CNN-based network. Besides, a
weight initialization method is proposed to enhance the training
performance. To solve the well known over smoothing problem
for the inter frames, a frame-level residual mapping (RM) is
presented. We analyze some of the mainstream methods like
frame-level and block-level based ﬁlters quantitatively and build
our CNN-based ﬁlter with frame-level control to avoid the
extra complexity and artiﬁcial boundaries caused by block-level
control. In addition, a novel module called RM is designed to
restore the distortion from the learned residuals. As a result, we
can effectively improve the generalization ability of the learning-
based ﬁlter and reach an adaptive ﬁltering effect. Moreover, this
module is ﬂexible and can be combined with other learning-
based ﬁlters. The experimental results show that our proposed
method achieves signiﬁcant BD-rate reduction than H.265/HEVC.
It achieves about 1.2% BD-rate reduction and 79.1% decrease in
FLOPs than VR-CNN. Finally, the measurement on H.266/VVC
and ablation studies are also conducted to ensure the effectiveness
of the proposed method.

Index Terms—In-loop ﬁlter, HEVC, convolutional neural net-

work, VTM.

I. INTRODUCTION

T HE performance of video compression has been contin-

uously improved with the development from H.264/AVC
[1], H.265/HEVC [2] to H.266/VVC [3]. These standards
share a similar hybrid video coding framework, which adopts
prediction [4], [5], transformation [6], quantization [7], and

This work was supported in part by the National Natural Science Foundation
of China under Grant 61674041, in part by Alibaba Group through Alibaba
Innovative Research (AIR) Program, in part by the STCSM under Grant
16XD1400300, in part by the pioneering project of academy for engineering
and technology and Fudan-CIOMP joint fund, in part by the National Natural
Science Foundation of China under Grant 61525401, in part by the Program of
Shanghai Academic/Technology Research Leader under Grant 16XD1400300,
in part by the Innovation Program of Shanghai Municipal Education Com-
mission,
in part by JST, PRESTO Grant Number JPMJPR19M5, Japan.
(Corresponding author: Heming Sun and Yibo Fan.)

C. Liu, Y. Fan and X. Zeng are with the State Key Laboratory of
ASIC and System, Fudan University, Shanghai 200433, China (e-mail:
chaoliu18@fudan.edu.cn; fanyibo@fudan.edu.cn; xyzeng@fudan.edu.cn).

H. Sun is with the Waseda Research Institute for Science and Engineer-
ing, Tokyo 169-8555, Japan and JST, PRESTO, 4-1-8 Honcho, Kawaguchi,
Saitama, 332-0012, Japan (e-mail: hemingsun@aoni.waseda.jp).

J. Katto is with Waseda Research Institute for Science and Engineer-
ing, Tokyo 169-8555, Japan and the Graduate School of Fundamental
Science and Engineering, Waseda University, Tokyo 169-8555, Japan (e-
mail:katto@waseda.jp).

context adaptive binary arithmetic coding (CABAC) [8]. Ow-
ing to the modules like quantization and ﬂexible partition,
some unavoidable artifacts are produced and cause degradation
of video quality, such as blocking effect, Gibbs effect, and
ringing. To compensate for those artifacts, many advanced
ﬁltering tools are designed, for instance, de-blocking(DB
[9]), sample adaptive offset(SAO [10]), and adaptive loop
ﬁlter(ALF [11]). These tools reduce the artifacts effectively
with acceptable complexity.

In the past decades, the learning-based methods make great
progress in both low-level and high-level computer vision tasks
[12]–[17], such as object detection [12], [13], semantic image
segmentation [14], [15], and super resolution [16], [17]. By
virtue of the powerful non-linear capability of learning-based
tools, they also have been utilized to replace the existing
modules in video coding and show great potential, for instance,
inter prediction [21], [22], and
intra prediction [18]–[20],
entropy coding [23], [24]. Learning-based models, especially
CNN, have achieved excellent performances for the in-loop
ﬁlter of video coding [25]–[33]. Dai et al. [26], [27] proposed
VR-CNN, which adopts a variable ﬁlter size technique to have
different receptive ﬁelds in one-layers and achieves excellent
performance with relatively low complexity. Zhang et al. [30]
proposed a 13-layer RHCNN for both intra and inter frames.
The relatively deep network has a strong mapping capability to
learn the difference between the original and the reconstructed
inter frames. To further adapt to the image content, Jia et al.
[33] designed a multi-model ﬁltering mechanism and proposed
a content-aware CNN with a discriminative network. This
method uses the discriminative network to select the most
suitable deep learning model for each region.

Most of the learning-based ﬁlters can achieve considerable
BD-rate [34] savings than H.265/HEVC anchor. However,
real-world applications often require lightweight models. High
memory usage and computing resource consumption make
it difﬁcult
to apply complex models to various hardware
platforms. Therefore, designing a light network is essential
to popularize learning-based in-loop ﬁlters. Considering this,
some model compression methods that reduce the model com-
plexity while maintaining performance are needed. In recent
years, some famous methods have been proposed, including
lightweight layers [35], [36], knowledge transfer [37]–[39],
low-bit quantization [40], [41], and network pruning [42], [43].
DSC [35], [36] is one of the famous lightweight layers. It
preserves the essential features of standard convolution while
greatly reducing the complexity by using grouping convolution
[35]. In this paper, we build our learning-based ﬁlter with DSC

 
 
 
 
 
 
2

instead of the standard convolution. And knowledge transfer
is used to help the initialization of the trainable parameters
without increasing the complexity.

Besides the learning-based ﬁlter itself, we also need a
lightweight mechanism for the ﬁltering of inter frames. Some
inter blocks fully inherit the texture from their reference blocks
and have almost no residuals. If the learning-based ﬁlter is
used for each frame, those blocks will be repeatedly ﬁltered
and cause over-smoothing in inter blocks [33], [44]. One
solution to solve this problem is training a speciﬁc ﬁlter for
the coding of intra and inter
inter frames [30]. However,
frame share some of the same modules in H.265/HEVC like
transformation, quantization, and block partitions. This means
the learning-based ﬁlter trained with intra frames can also be
used for inter frames to some extent. Considering this, previous
works [27], [33], [44]–[47] designed a syntax element control
ﬂag to indicate whether an inter CTU uses the learning-based
ﬁlter or not. It chooses a selective ﬁltering strategy for each
CTU. For this strategy, we compare it with frame-level control
in Section IV-A and found the CTU-level control may lead
to artiﬁcial boundaries between the neighboring CTUs. So we
propose to use the frame-level based ﬁlter to avoid unnecessary
artiﬁcial boundaries. In order to improve the performance of
frame-level based ﬁltering, we propose a novel module called
residual mapping (RM) in this paper.

In summary, we propose a novel

light CNN-based in-
loop ﬁlter for both intra and inter frames based on [48],
[49]. Experimental results show this model achieves excellent
performance in terms of both video quality and complexity.
Speciﬁcally, our contributions are as follows.

• A CNN-based lightweight in-loop ﬁlter is designed for
H.265/HEVC. Low-complexity DSC merged with the BN
is used as the backbone of this model. Besides, we use
attention transfer to pre-train it to help the initialization
of parameters.

• For the ﬁltering of inter frames, we analyze and build
our CNN-ﬁlter based on frame-level to avoid the artiﬁ-
cial boundaries caused by CTU-level. Besides, a novel
post-processing module RM is proposed to improve the
generalization ability of the frame-level based model and
enhance the subjective and objective quality.

• We integrate the proposed method into HEVC and VVC
reference software and signiﬁcant performance has been
achieved by our proposed method. Besides, we conduct
some extensive experiments like ablation studies to prove
the effectiveness of our proposed methods.

The following of this paper is organized as follows. In
Section II, we present the related works, including the in-
loop ﬁlter in video coding and the lightweight network design.
including
Section III elaborates on the proposed network,
network structure and its loss function. Section IV focuses on
the proposed RM module and provides an analysis of different
control strategies. Experiment results and ablation studies are
shown in Section V. In Section VI, we conclude this paper
with future work.

II. RELATED WORKS

A. In-loop Filters in Video Coding

1) DB, SAO, and ALF: DB, SAO, and ALF that are adopted
in the latest video coding standard H.266/VVC [3] are aimed
at removing the artifacts in video coding. De-blocking [9] has
been used to reduce the discontinuity at block boundaries since
the publication of coding standard H.263+ [50]. Depend on the
boundary strength and reconstructed average luminance level,
DB chooses different coding parameters to ﬁlter the distorted
boundaries. Meanwhile, by classifying the reconstructed sam-
ples into various categories, SAO [10] gives each category
a different offset to compensate for the error between the
reconstructed and original pixels. Based on the Wiener ﬁlter,
ALF [11] tries different ﬁlter coefﬁcients by minimizing the
square error between the original and reconstructed pixels. The
signal of the ﬁlter coefﬁcient needs to be sent to the decoder
side to ensure the consistency between encoder and decoder.
All these aforementioned ﬁlters can effectively alleviate the
various artifacts in reconstructed images. However, there is
still much room for improvement.

2) Learning-based Filter: Recently, the learning-based ﬁl-
ters have far outperformed the DB, SAO, and ALF in terms
of both objective and subjective quality. Different from SAO
and ALF, they hardly need extra bits but can compensate
for errors adaptively as well. Most of them are based on
CNNs and have achieved great success in this ﬁeld. For the
ﬁltering of intra frames, Park et al. [25] ﬁrst proposed a CNN-
based in-loop ﬁlter IFCNN for video coding. Dai et al. [26]
proposed VR-CNN as post-processing to replace DB and SAO
in HEVC. Based on inception, Liu et al. [28] proposed a CNN-
based ﬁlter with 475,233 trainable parameters. Meanwhile,
Kang et al. [29] proposed a multi-modal/multi-scale neural
network with up to 2,298,160 parameters. Considering the
coding unit (CU) size information, He et al. [31] proposed
a partition-masked CNN with a dozen residual blocks. Sun
et al. [48] proposed a learning-based ﬁlter with ResNet [51]
for the VTM. Liu et al. [49] proposed a lightweight learning-
based ﬁlter based on DSC. Apart from what was mentioned
above, Zhang et al. [44] proposed a residual convolution neural
network with a recursive mechanism.

Different from the training the ﬁlter for intra samples,
training the ﬁlter with inter samples need to consider the
problem of repeated ﬁltering [33], [47]. Jia et al. [33] proposed
a content-aware CNN based in-loop ﬁltering method that
applies multiple CNN models and a discriminative network
in the H.265/HEVC. This discriminative network can be used
to judge the degree of distortion of the current block and select
the most appropriate ﬁlter for it. However, the discriminative
network requires additional complexity and memory usage,
some researchers [27], [45] proposed to use block-level syntax
elements to replace it. This method requires extra bit con-
sumption but gets a more accurate judgment on whether to
use the learning-based ﬁlter. Similarly, some researchers [25],
[52] proposed to use frame-level syntax elements to control
the ﬁltering of inter frames. Besides, complicated models
[30], [45], [53] like spatial-temporal networks are also useful
for solving this problem. Jia et al. [45] proposed spatial-

3

(AT) to get student model performance improved by letting it
mimic the attention maps from a teacher model. Meanwhile,
Huang et al. [39] design a loss function by minimizing
the maximum mean discrepancy (MMD) metric between the
distributions of the teacher and the student model, where
MMD is a distance metric for probability distributions [56].

Fig. 1.
convolution.

The depthwise separable convolution, where ”Conv.” indicates

III. PROPOSED CNN-BASED FILTER

A. Network Structure

temporal residue network (STResNet) with CTU level control
to suppress visual artifacts. RHCNN that is trained for both
intra and inter frames was proposed by Zhang et al. [30].
Filtering in the decoder side [32], [54], [55] can also solve
the problem of repeated enhancement well. For example, DS-
CNN was designed by Yao et al. [32] to achieve quality
enhancement as well. Li et al. [54] adopted a 20-layers deep
CNN to improve the ﬁltering performance. Zhang et al. [55]
proposed a post-processing network for VTM 4.0.1.

In summary, ﬁltering in inter frames is more challenging
than that of intra frames. In most cases, the CNN-based in-loop
ﬁlter with higher complexity can achieve better performance
on intra frames. But for the ﬁltering of inter frames, the ex-
isting methods have their own problems. For example, frame-
level control may lead to an over-smoothing problem, CTU-
level control will cause the additional artiﬁcial boundaries, the
out-loop ﬁlters cannot use the ﬁltered image as a reference,
adding discriminative network and complex model may lead
to over-complexity and impractical. Therefore, we should pay
attention to a more effective method for this task.

B. Lightweight Network Design

1) Depthwise Separable Convolution: As a novel neural
network layer, DSC achieves great success in practical appli-
cations because of its low complexity. It is initially introduced
in [35] and subsequently used in MobileNets [36]. As shown
in Fig. 1, DSC divides the calculation of standard convo-
lution into two parts, depthwise convolution, and pointwise
convolution. Different from standard convolution, depthwise
convolution decompose the calculation of standard convolution
into group convolution to reduce the complexity. Meanwhile,
the pointwise convolution is the same as the standard convolu-
tion with kernel 1 × 1. In other words, depthwise convolution
is used to convolute the separate features whereas pointwise
convolution is utilized to combine them to get the output
feature maps. These two parts together form a complete DSC.
2) Knowledge Distillation and Transfer: Previous studies
[37]–[39] have shown that the ”knowledge” in pre-trained
models can be transferred to another model. Hinton et al.
[37] propose a distillation method that uses a teacher model
to get a ”soft target”, which helps a student model that has
a similar structure perform better in the classiﬁcation task.
Besides softening the target in classiﬁcation tasks, some other
methods [38], [39] use the intermediate representations of the
pre-trained model to transfer the ”knowledge”. For example,
Zagoruyko et al. [38] devise a method called attention transfer

As shown in Fig. 2, we design a network structure that
functions on both the teacher and the proposed model. This
structure is composed of convolution, BN layer, and activation
ReLU [57]. The backbone of this structure is K layers of
DSC with dozens of feature maps F . The input
to this
structure is the HM reconstruction without ﬁltering and the
output is the ﬁltered reconstructed samples. The last part is
a standard convolution with only 1 feature map. And we add
the reconstruction samples to the output inspired by residual
learning [51]. The depthwise and the standard convolution
kernel are both 3 × 3. Every convolution is followed by the
ReLU except for the last one. The reason why choose ReLU
instead of other advanced activation functions is that ReLU
has a lower complexity while a considerable nonlinearity. In
our implementation, the values of K and F are 24 and 64
for the teacher model, 9 and 32 for the proposed model. The
description of the parameters in the proposed model is shown
in Table I.

We use the BN layer in the training phase, this layer could
improve the back-propagation of the gradients. What’s more,
both BN and convolution are linear computations for the ten-
sors in the proposed model. Therefore, the BN can be merged
into the convolution to further reduce the computational during
the inference phase. As shown in (1), depthwise convolution
output χdwConv can be formulated as:

χdwConv = wdwConv ∗ χ

(1)

where ∗ indicates the convolution operation, wdwConv is the
kernel and χ is the depthwise convolution input. Similarly, the
piecewise convolution output χpwConv can be written as:

χpwConv = wpwConv ∗ χdwConv + bpwConv

(2)

is noticeable in (1) that

where wpwConv and bpwConv denote the kernel and bias.
It
the convolution of depthwise
convolution has no bias, this is because the bias bdwConv can
be merged into bpwConv when there is no activation between
depthwise and pointwise convolution. After convolution, the
output of BN can be obtained by (3). (The reason why we
use ∗ operation here is because actually the calculation of
BN is equivalent to that of the depthwise convolution by
simpliﬁcation)

χbn = γ ∗

(cid:18) χpwConv − mean

√

var + (cid:15)

(cid:19)

+ β

Substituting (2) into (3), we obtain (4) as follows:

χbn = (cid:98)wpwConv ∗ χdwConv + (cid:98)bpwConv

(3)

(4)

Depthwise Conv.Pointwise Conv.4

Fig. 2. The architecture of teacher model and the proposed model, where ”Rec.” indicates ”reconstructed pixels”. The top-right and bottom-right are the
teacher model and the proposed student model, respectively. The rectangle on the right implies the knowledge transfer.

TABLE I
DESCRIPTION OF THE PARAMETERS IN THE PROPOSED MODEL

Index

Block1

Block2

Block3

Std Conv.a Sum

Parameters 73 + 2 × 1, 344 3 × 1, 344 3 × 1, 344

289

11,114

a Standard Convolution.

where (cid:98)wpwConv and (cid:98)bpwConv in (4) are:

γ ∗ wpwConv
√
var + (cid:15)

(cid:98)wpwConv =

(cid:98)bpwConv =

γ ∗ (bpwConv − mean)
var + (cid:15)

√

+ β

(5)

(6)

In (5) and (6), γ and β are trainable parameters of BN,
mean and var are non-trainable parameters of BN. Hyper-
parameter (cid:15) represents a positive number that prevents division
zero errors. In the inference phase, we use the (cid:98)wpwConv and
(cid:98)bpwConv to replace the weight wpwConv and bias bpwConv in
depthwise convolution, thus merging the BN into the DSC and
reducing the model complexity.

B. Standard Convolution of the Proposed Structure

In this subsection, the last part of the proposed structure
is detailed. Because the standard convolution uses fewer
calculations than DSC when the number of convolution output
channels is only one. It is worth noting that the last convolution
of the proposed model is standard convolution, which isn’t
consistent with the backbone of the proposed model. The
DSC consists of two steps, including depthwise convolution
and pointwise convolution. The depthwise convolution is the
simpliﬁcation of the standard convolution to reduce the amount
of computation while preserving the ability to convolve the
input feature maps. Meanwhile, the pointwise convolution is
equivalent to the standard convolution with 1 × 1 kernel, it
is utilized to fuse the different depthwise convolution output.
According to their computing methods, the ratio r of the
calculation of the DSC to that of the standard convolution
is calculated as:

r =

KW KH CI W H + CI COW H
KW KH CI COW H

=

1
CO

+

1
KW KH

(7)

+

where W , H is the width and height of the input frame,
respectively. KW , KH is the width and height of the convo-
lution kernel, respectively. CI , CO are the number of feature
maps for the convolution input and output, respectively. In
our proposed model, CO = 1 and KW = 3, KH = 3.
So r = 1
= 10
9 , which is bigger than 1.
CO
This represents DSC consumes more computing sources than
standard convolution. The extra calculation is caused by point-
wise convolution, which is utilized to combine feature maps.
However, the standard convolution also can combine features,
which indicates the extra calculation of pointwise convolution
is meaningless. Therefore, we choose the standard convolution
at the end of the model to avoid meaningless calculations.

1
KW KH

C. Proposed Initialization and Training Scheme

In this subsection, we will introduce the training process
and loss functions of the proposed network. In most cases, a
suitable initialization of parameters can help the model better
converge to the minimum. Inspired by transfer learning, a pre-
trained teacher model is used to guide the initialization of the
parameters in the proposed model. By using such initialization,
we hope the proposed model can obtain the output similar
to that of the teacher model before the real training begins.
The pre-trained model uses the mean square errors (MSE) loss
between the output YT of teacher model and the original pixels
YO.

LT =

1
N

N
(cid:88)

i=1

(cid:107)Y i

T − Y i

O(cid:107)2
2

(8)

After the training of the teacher model, we use the intermediate
outputs of it
to guide the proposed model on parameter
initialization. This process is denoted by the bold lines in Fig.
2. Because the vanishing of gradients may lead to insufﬁcient
training of shallow layers, the teacher model is divided into
differently-sized blocks to produce the intermediate hints. The
metric of the distance between teacher and the proposed
including MMD [39] and
student model
attention loss [38]. The loss function LMM D2(FT , FS) with

tries two forms,

x10x3x8x3x6x3Guide the initialization of parameterTeacher OutputStudent OutputDepthwise SeparableConvolution +(Batch Normalization) +ReluStandard Convolution Element-wise AdditionAttentionLossAttention Attention Input Rec.linear kernel function (k(x, y) = xT y) could be written as
follows:

LMM D2 (FT , FS) = (cid:107)

1
CT

CT(cid:88)

i=1

f i
T
(cid:107)f i
T (cid:107)2

−

1
CS

CS(cid:88)

j=1

f j
S
(cid:107)f j
S(cid:107)2

(cid:107)2
2 (9)

where F represents the attention map, f indicates a single fea-
ture map, C is the number of feature maps, and the subscript
T and S identify the teacher and student model. Meanwhile,
the loss function LAT (FT , FS) of attention transfer (AT) [38]
could be written as follows:

(cid:80)CS

−

(cid:80)CT
(cid:107) (cid:80)CT

j=1 |f j
j=1 |f j

S|p
S|p(cid:107)2

(cid:107) (cid:80)CS

LAT (FT , FS) = (cid:107)

i=1 |f i
i=1 |f i

T |p
T |p(cid:107)2
We set p to 2 in our implementation, because these two
methods are similar except for their normalization methods
when p = 1 [39]. After the initialization, we start the real
training process of using MSE LS in (11) to train the proposed
model, where YS indicates the output of the proposed model.

(cid:107)2
2 (10)

5

(a) Convolution with valid padding

(b) Convolution with same padding

LS =

1
N

N
(cid:88)

i=1

(cid:107)Y i

S − Y i

O(cid:107)2
2

(11)

In summary, the whole process can be divided into the

Fig. 3. The diagrams of convolution with different pad methods.

following steps.

Algorithm 1 The process of building the trained proposed
model.
Input: The dataset pair of HM reconstruction samples X and

TABLE II
COMPLEXITY COMPARISON OF CTU-LEVEL CONTROL BETWEEN VALID
PADDING AND SAME PADDING

original samples YO;

Output: The trained proposed model;

1: Constructing the teacher model T and training it for n1

epochs with MSE LT ;

2: Extracting the attention maps FT from the trained T ;
3: Constructing the student model S with BN and training it
for n2 epochs with LAT (FT , FS) or LMM D2 (FT , FS);

4: Training S with MSE LS for n3 epochs;
5: Calculating the (cid:98)wpwConv and (cid:98)bpwConv for S;
6: Removing the BN from S;
7: Using the (cid:98)wpwConv and (cid:98)bpwConv to replace the weight
wpwConv and bias bpwConv in depthwise convolution of
S;

8: return S;

IV. PROPOSED RESIDUAL MAPPING FOR THE CNN-BASED
FILTERING

A. Analysis of CTU-level and Frame-level Control

From the size of ﬁltered samples, ﬁltering methods can be
divided into CTU-level (block-level) and frame-level. Com-
pared with CTU-level control, there are two main advantages
of frame-level control in CNN-based ﬁlter design, including
the required computational resource and the video quality. In
this subsection, the difference is analyzed from the perspec-
tives of the padding methods and the ﬁlter kernels.

Firstly, to keep the input frames size unchanged, the CNN-
based ﬁlter needs to pad the boundaries of input with some
including
samples. There are usually two padding ways,

Items

RHCNN [30]

Jia et al. [33] VR-CNN [26]

Padding type

Valid

Same Valid Same Valid

Same

16.21

10.89

Flopsa(G)
Maddb(G)
Memoryc(MB)
5.84
MemR+Wd(MB) 193.05 130.36 55.09 39.42 13.88

25.43 18.02

91.06

32.38

60.11

21.76

1.49

0.25

2.02

0.49

2.97

4.04

0.22

0.44

5.02

11.99

a Theoretical amount of ﬂoating point arithmetics.
b Theoretical amount of multiply-adds.
c Memory useage.
d Memory read /write.

valid padding (padded with reconstructed samples) and same
padding (padded with zero samples). In one case, if the CTUs
are padded with reconstructed pixels to maintain the same
accuracy as frame-level ﬁltering, most of the networks need
to pad the input block with plenty of pixels and require
considerable calculation. Fig. 3 intuitively shows the difference
in the amount of calculation between valid and same padding.
The quantitative calculations [58] are illustrated in Table II
(we assume that both of their output sizes of ﬁltered samples
are 64 × 64), it can be found that the valid padding (see
”Valid” columns) of works [26], [30], [33] all have consid-
erable complexity increasing than same padding (see ”Same”
columns). In the other case, if the same padding is selected, it
will cause calculation errors around the boundaries as shown
in Fig. 4. We assume that the size of the block control is h×h,
and the width of the boundary area affected by the pad is a.
The proportion pf c of affected pixels under frame-control is

Input SampleFiltered SampleCon-volutionExtra CalculationCon-volutionInput SampleFiltered SampleZero SampleAffected SampleUnaffected Sample6

(a) CTU-level control

(b) Frame-level control

Fig. 4. The diagrams of convolution with different control methods.

calculated as follows:

pf c = 1 −

(W − 2a)(H − 2a)
W H

=

2a(W + H − 2a)
W H

(12)

Similarly, the proportion pbc of affected pixels under block
control can be approximated as follows. (No incomplete CTU
are considered)

(a) Org. frame

(b) Distortion

(c) Learned residual

Fig. 5. A frame from CLIC dataset [59] is coded with HM-16.16 and QP
37. The original frame, the distortion and the learned residual of this frame
are shown in (a), (b) and (c).

≈

(13)

4a
h

pbc =

4a(h − a)
h2
It can be found from (13) that the area affected by same
padding is approximately proportional
to the perimeter of
the ﬁltered samples. Therefore, the frame-level control with a
higher area-to-perimeter ratio is less affected than block-level
control. According to (12) and (13), it can be obtained that for
the HEVC test sequence, the same-padding of our network
will affect an average of 45% of the pixels under CTU-
level control, whereas that of frame-level control is only 3%.
Therefore, choosing frame-level control lays a solid foundation
for the application of the CNN-based ﬁlter.

Secondly, the frames ﬁltered by frame-level control has the
property of integrity. Frame-level control uses the same kernel
for ﬁltering of the entire frame whereas CTU-level control may
use the different kernels for two consecutive CTUs, which may
lead to some artiﬁcial errors in the boundaries. As shown in
Fig. 4, two consecutive CTUs with different ﬁltering strategies
have some errors along the boundaries because of the different
kernels used in the ﬁltering. Especially for the condition that
one of the CTUs uses the learning-based ﬁlter while the
other one doesn’t. This further demonstrates the advantages
of frame-level control.

In summary, for the design of lightweight CNN-based
ﬁlters, the frame-level control has some advantages over block-
level control. On the one hand, compared with frame-level
control, CTU-level control leads to calculation cost with the
same padding or calculation error with the valid padding.
On the other hand, frame-level control has the property of
integrity and it brings better subjective quality. To reduce the
padding error brought by the multi-layer neural network and
complexity, we built our CNN-based in-loop ﬁlters on a frame-
level control. However, the ability to directly use frame-based
control is weak because it only has two states of using or not
using the ﬁlter, we need some added methods to improve its
performance.

B. Residual Mapping

In this subsection, a novel post-processing module RM
is proposed to improve the performance of the frame-level

Fig. 6.
(“Race-
The comparison of different ﬁltering mechanisms
Horses 416x240”, qp22, LDP conﬁguration). Linear, quadratic, and cubic
represent the mapping function of linear, quadratic, and cubic functions,
respectively. We can ﬁnd in the red box that the performance of using CNN-
ﬁlter directly is not satisfactory, and even leads to a decrease in PSNR.

control based CNN ﬁlter. It can effectively improve the over-
smoothing problem [33], [47] of inter frame. Besides, we
found that it also has a considerable improvement to intra
frames in Section V-D. Most of the trained neural networks are
ﬁtting to a certain training set. Since the distribution of training
data is often very complicated, the training is actually a trade-
off of the data set. For a speciﬁc image, the trained ﬁlter may
be under-ﬁtted or over-ﬁtted. This may cause distortion or blur
for a learning-based ﬁlter. What’s more, if we want to use the
neural network trained with intra samples for the ﬁltering of
inter samples, this phenomenon will be more serious because
of the difference in the distribution of the intra and inter
datasets. With this in mind, we proposed to use a parametric
RM after the learning-based ﬁlter, which is some sort of non-
parametric ﬁlter, to improve its generalization ability. Inspired
by the potential correlation of distortion and learned residual
shown in Fig. 5, we handle this ﬁlter from the perspective that
of restoring distortion from the learning-based ﬁltered residual,
which is equivalent to improving the quality of the distorted
frames. The distortion RO is deﬁned as the difference between
the original samples YO and reconstruction of de-blocking X
:

RO = YO − X

(14)

haFilteredCTUUnfilteredCTUArtificial BoundaryWHFilteredFrame(a) Serial structure

(b) Parallel structure

(c) Proposed structure

Fig. 7. The schemes of the different frameworks with CNN-based ﬁlter.

Similarly, the learned residual RS is deﬁnes as the difference
between the output of learning-based ﬁlter and X.

RS = YS − X

(15)

A function fλ(·) with parameters λ is designed as the para-
metric ﬁlter to map RS to RO. We choose MSE as the metric:

λ = arg min

λ

(fλ(RS) − RO)2

(16)

is convenient

We should use a model with a small amount of parameters
to construct fλ(·), so that
to encode the
it
parameters λ into the bitstream to ensure the consistency of
encoding and decoding. For the expression form of fλ(·), we
have tried linear functions and polynomial functions as shown
in Fig. 6. From the red box on the left, it can be found that
only using the CNN ﬁlter (see red dotted line) may lead to
a decrease in coding performance, this proves that directly
using CNN ﬁlters for inter frames may degrade video quality.
And the performance is improved after adopting RM. It is
noticeable that there is little difference in performance between
different polynomial functions. So we choose simple linear
functions to build RM.

λ = arg min

λ

(λRS − RO)2

(17)

So we add X and the output of RM ˆRS to get the ﬁltered
frame ˆYS. After sending it to SAO, the entire ﬁltering process
is completed.

7

TABLE III
EXPERIMENTAL ENVIRONMENT

Items

Optimizer

Processor

GPU

Speciﬁcation

Adam [60]

Intel Xeon Gold 6134 at 3.20 GHz

NVIDIA GeForce RTX 2080

Operating system CentOS Linux release 7.6.1810

HM version

16.16

DNN framework

Keras 2.2.4 [61] and TensorFlow 1.12.0 [62]

it

the ﬁlters. Conversely,

is easy to produce artiﬁcial

kinds of ﬁltering as shown in Fig. 7. From the perspective
of reconstructed frames, the proposed RM can be interpreted
as a post-processing module that fully utilizes the advantages
of both distorted reconstruction and learned ﬁltered output.
The full use of these two aspects makes RM have excellent
performance. For example, we assume that the reference frame
is a frame ﬁltered by a learning-based ﬁlter, so if the current
frame and the reference frame are almost identical, the current
frame does not need to use all
if
the current frame and the reference frame are completely
imprints because
different,
of the distorted residue, so the ﬁlters should be used in
this case. For a speciﬁc frame, however, it is often difﬁcult
to obtain an accurate judgment about whether to use the
ﬁlters by using its encoded information, such as residuals or
motion vectors. Considering the good generalization ability of
traditional ﬁlters, we keep them working and focused on the
CNN ﬁlter. So we introduce a parametric module RM, which
uses an RDO process to give an appropriate ﬁltering effect of
the CNN ﬁlter. From (16), it can be observed that the ﬁltering
strength varies with the change of the λ. So we can traverse
all of the candidate λ and code the one with the smallest
reconstruction error into bitstreams. We can also derivative
the objective function to obtain the optimal parameters, and
code the quantized parameters in the bitstream. In this case, we
need to consider the inﬂuence of parameter quantization, those
mapping functions that are sensitive to quantization noise, such
as high-order polynomials, should be abandoned. Otherwise,
this may result in larger quantization errors in the decoded
frames.

ˆYS = X + ˆRS = X + λRS

(18)

V. EXPERIMENTAL RESULTS

We quantify the candidate λ with n bits for each component,
where λ = i/(2n − 1), i ∈ 0, 1, ..., 2n − 1. In the imple-
mentation, the number of required bits n is set to 5, so each
frame needs 15 bits for the RM module. And a rate-distortion
optimization (RDO) process is designed to ﬁnd the best λ.
The regular mode of CABAC is used to code λ. RM does not
need speciﬁc models for inter frames or additional classiﬁers
for each CTU. What’s more, it is independent of the proposed
network and can be combined with other learning-based ﬁlters
to alleviate the over-smoothing problem as well.

Different from previous strategy [47] of choosing one
between traditional ﬁltering and learning-based ﬁltering, RM
uses a serial structure and makes full use of both these two

A. Experimental Setting

For the experiment, we mainly focus on objective quality,
subjective quality, complexity, and ablation studies to illustrate
the performance of our model. Nine hundred pictures from
DIV2K [63] are cropped into the resolution of 1024 × 1024,
and then down-sampled to 512 × 512. These two sets of
pictures are spliced into two videos as our training sets.
Only the luminance component is used for training, and the
chrominance components are also tested by using the proposed
model. The patch size in training is 32 × 32 of H.265/HEVC
and 64 × 64 of H.266/VVC, which is consistent with the
largest size of the TU. Considering that the reconstructed
images with different QPs often have different degrees of

DBCNN-filterSAODBSAOCNN-filterDBCNN-filterRMSAO8

distortion and artifacts, the whole QP band is divided into
four parts, below 24, 25 to 29, 30 to 34, and above 35. So
four proprietary models are trained for each QP band. The
parameter initialization method is normal distribution [64]
for both the teacher model and the proposed model. The
training epochs n1 and n3 are both set to 50. We use more
training epochs for the model with higher QP in the special
initialization phase because there are often more artifacts in the
reconstructed images with higher QP. Speciﬁcally, parameters
n2 is set as 10 for the lower QPs and 20 for the higher
QPs. After the training phase, we save the trained model
and call it to infer in HEVC reference software (HM) and
video coding test model (VTM). In the test phase, the ﬁrst
64 frames from HEVC test sequences are used to evaluate
the generalization ability of our model. We test four different
conﬁgurations with default settings, including all-intra (AI),
low-delay-B (LDB), low-delay-P (LDP), and random-access
(RA) for H.265/HEVC anchor. For the H.266/VVC anchor,
we test it with default AI and RA conﬁgurations. Four typical
QPs in common test conditions are tested, including 22, 27,
32, 37. The other important test conditions are shown in Table
III. For a fair comparison with previous works, we use the
coding experimental results from their original papers. The
complexities of the reference papers are tested on our local
server to avoid the inﬂuence of the hardware platforms.

B. Experiment on H.265/HEVC

1) Objective Evaluation: In this subsection, the objective
evaluation is conducted to evaluate the performance of our
proposed model. The experimental results compared with the
HM-16.16 anchor are shown in Table IV. For the luminance
component, the proposed model achieves 6.3%, 4.5%, 5.4%,
and 5.7% BD-rate reduction compared with HEVC baseline
under AI, LDB, LDP, and RA conﬁguration, respectively.
For chrominance components, the proposed model achieves
more BD-rate reduction than the luminance component. It
demonstrates the generalization ability for the proposed model
because we only use the luminance components of intra
samples for training. Furthermore, the comparisons with the
previous works [26], [33] are conducted and the BD-rate
reduction is shown in Table V. It can be seen that our model
achieves more BD-rate reduction for AI conﬁguration.

For the performance evaluation of inter conﬁgurations, we
introduce the comparison of our proposed model with frame-
level control [25], CTU-level control [45], and Jia et al.
[33] as shown in Table VI. To compare fairly, we select
the same padding and use the proposed model to test the
it
different control methods. From the experiment results,
can be seen that our proposed model achieves about 1%
extra BD-rate reduction than both CTU-level and frame-level
control for all inter conﬁgurations. Compared with Jia et al.
[33], our model achieves comparative BD-rate reduction in
inter conﬁgurations. For the chrominance components, our
model achieves about 3% extra BD-rate reduction, it further
demonstrates the generalization ability of our model.

2) Subjective Evaluation: We also conduct the subjective
evaluation as shown in Fig. 8 and Fig. 9. It can be seen

from the experimental results that our model has a great de-
artifacts capability. First, we re-deploy the proposed model
in HM-16.9 for a fair subjective evaluation with Jia et al.
[33]. From Fig. 8, it can be found that the various kinds of
artifacts in (a) are eliminated by the proposed model and the
man’s face looks smoother and plump. At the same time, some
vertical blocky effects are produced by Jia et al. [33], probably
because it uses different ﬁlters for consecutive CTUs while our
proposed model uses the same ﬁlter for the whole images and
have no additional boundaries. Besides, the man’s eyes seem
to be blurred by [33] and lead to the degradation of visual
quality. Second, the subjective evaluation for the inter frames is
conducted in Fig. 9. The default HM and HM with CTU-level
control [27] are used as the anchors. As shown in Fig. 9, the
contouring and blocky artifacts on the number are eliminated
by the proposed model. For CTU-level control [27] based
ﬁltering, the subjective quality of this frame is reduced due
to the artiﬁcial boundaries on the knee, whereas our proposed
model has no boundaries on it and achieves a better visual
quality. To sum up, because our proposed method makes full
use of the frame-level ﬁltering strategy, the proposed method
has signiﬁcantly better visual effects than previous CTU-based
methods.

3) Complexity Analysis: As shown in Table V, we compare
the complexity of Jia et al. [33], VR-CNN [26], and our
proposed model from two aspects, including computational
complexity and storage consumption. Firstly, for the coding
complexity evaluation, we use the following equation to cal-
culate the ∆T :

∆T =

T (cid:48)
T

(19)

where T (cid:48) and T denote the HM coding time with and without
the learning-based ﬁlter, respectively. FLOPs in Table V are
also tested for the frame with a resolution of 720p. Compared
with VR-CNN [26], the FLOPs of our model is reduced by
79.1%. The decoding complexity is reduced by approximately
50% and the encoding complexity is reduced by 4%. The
processing time of the proposed model is almost the same
for both encoder and decoder. The difference in relative time
is caused by that the network inference time accounts for a
small proportion of the encoding complexity but comparative
for the decoding.

In terms of storage consumption, compared with [26], the
is
number of trainable parameters in the proposed model
reduced by 79.6%. It is almost the same with the reduction
of model size because we use the same precision (ﬂoat32)
to save the models. The main reason why our model has
relatively fewer parameters is that the design of the proposed
model focuses more on complexity instead of performance.
For example, we use the DSC as the backbone of the proposed
model, whereas previous works [26], [33] utilize the standard
convolution. Meanwhile, we also use many useful methods
to limit the model size while maintaining the performance,
including BN merge and special initialization of parameters.
What’s more, our proposed model only needs one learning-
based network for both intra and inter frames. So there
is no need for additional models in practical applications.
Compared with previous works that need multiple models or

9

V

-2.7%

-5.0%

-3.4%

-0.3%

-7.2%

-8.9%

-3.3%

-9.7%

-5.1%

-2.0%

RA

U

-3.6%

-4.6%

-5.7%

-0.6%

-9.5%

-4.4%

-3.8%

-8.0%

-4.7%

-1.4%

-9.7% -14.2%

-4.7%

-1.0%

-6.1%

-6.1%

-2.9%

-4.4%

-9.4% -12.2%

-8.4%

-4.1%

-9.7%

-5.1%

-9.8% -10.1%

TABLE IV
BD-RATE REDUCTION OF THE PROPOSED METHOD THAN HM-16.16 ANCHOR

V

Y

V

Y

V

Y

Sequences

ClassA

Trafﬁc

PeopleOnStreet

Kimono

ParkScene

ClassB

Cactus

BasketballDrive

BQTerrace

Y

-7.3%

-6.8%

-4.9%

-5.5%

-5.3%

-4.3%

-3.7%

AI

U

-3.4%

-7.1%

-2.6%

-3.2%

-4.7%

-6.9%

-2.5%

-2.3%

-4.1% -10.1%

-8.9% -11.7%

-4.3%

-4.8%

BasketballDrill

-8.0% -11.7% -14.1%

BQMall

PartyScene

RaceHorses

BasketballPass

BQSquare

BlowingBubbles

-6.0%

-3.7%

-3.9%

-6.5%

-4.2%

-5.3%

-6.3%

-4.8%

-7.2%

-5.7%

-6.9% -12.0%

-7.3% -10.3%

-3.0%

-9.3%

-6.8%

-9.8%

RaceHorses

-7.5% -10.5% -14.6%

-8.9%

-7.0%

-8.7% -10.5%

-5.2%

-5.3%

Vidyo1

Vidyo3

Vidyo4

ClassC

ClassD

ClassE

-7.3% -11.5%

-2.4% -10.6%

LDB

U

-2.4%

-0.6%

-7.5%

-0.3%

-3.7%

-4.2%

-2.1%

-4.9%

-3.2%

-0.1%

-0.9%

-0.9%

-4.7%

-0.7%

-4.4%

-7.8%

-4.6%

-4.7%

-0.2%

-6.6% -11.3%

-3.3%

-1.6%

-5.8%

-4.6%

-2.8%

-2.1%

-5.2% -10.3%

-9.0%

-5.9%

-9.6%

-3.1%

-4.3%

-3.1%

-1.5%

-5.4%

-6.0%

-3.4%

-4.6%

-1.8%

-4.2%

-4.3%

-4.1%

-3.9%

-6.7%

-7.4%

-4.6%

LDP

U

-3.4%

-4.4%

-0.5%

-5.6%

-1.5%

-2.4%

-6.7%

-0.7%

-5.8%

-9.2% -11.7%

-4.5%

-5.6%

-4.5%

-0.4%

-3.9%

-6.2%

-5.6%

-0.4%

-7.9% -12.7%

-4.8%

-1.8%

-5.4%

-5.8%

-2.9%

-1.8%

-7.4% -10.8%

-9.4%

-6.3%

-8.9%

-2.5%

-4.6%

-4.5%

-4.6%

-1.9%

-4.4%

-3.4%

-6.1%

-2.8%

-3.8%

-0.8%

-4.1%

-4.4%

-2.4%

-3.6%

-6.3%

-6.7%

-4.0%

-6.4%

-6.1%

-4.2%

-3.8%

-6.8%

-4.4%

-8.8%

-4.2%

-5.1%

-1.7%

-4.7%

-3.9%

-2.4%

-4.0%

-6.8%

-8.1%

-6.5%

-5.6%

-9.4%

-6.3% -10.1% -10.8%

-3.8% -11.5% -10.9%

-3.9% -12.1% -11.2%

FourPeople

-9.4%

-8.1%

-9.0%

-8.6%

-9.2%

Johnny

-8.3% -12.3% -11.0%

-7.0% -11.4%

KristenAndSara

-8.6% -10.2% -11.1%

-7.7%

-8.3%

-9.4%

-9.1%

-8.6%

-9.0%

-9.7% -10.8%

-7.7%

-9.6% -13.1% -10.7%

-8.3% -10.9%

-8.3% -10.1% -11.2%

-8.2%

-8.9%

-8.1%

-9.7%

-9.6%

Average

-6.3% -7.0% -8.6% -4.5% -5.1% -5.4% -5.4% -6.6% -6.4% -5.7% -6.1% -6.6%

TABLE V
BD-RATE REDUCTION AND COMPLEXITY (GPU) OF THE PROPOSED METHOD COMPARED WITH PREVIOUS WORKS [26], [33] IN AI CONFIGURATION

Sequences

Y

U

V

∆Tenc ∆Tdec

Y

U

V

∆Tenc ∆Tdec

Y

U

V

∆Tenc ∆Tdec

Jia et al. [33]

VR-CNN [26]

Proposed model

ClassA

-4.7% -3.3% -2.6% 108.1% 734.9% -5.5% -4.7% -4.9% 108.3% 561.1% -7.1% -5.4% -5.9% 105.8% 281.0%

ClassB

ClassC

-3.5% -2.8% -3.0% 109.0% 659.8% -3.3% -3.2% -3.7% 110.3% 505.3% -4.8% -4.8% -6.4% 106.2% 265.2%

-3.4% -3.5% -5.0% 113.1% 894.9% -5.0% -5.5% -6.9% 113.0% 685.1% -5.4% -7.5% -9.9% 106.5% 326.3%

ClassD

-3.2% -4.7% -6.0% 128.9% 1406.0% -5.4% -6.4% -8.1% 121.6% 1047.1% -5.9% -7.8% -10.5% 114.4% 548.0%

ClassE

-5.8% -4.1% -5.2% 112.3% 1110.2% -6.5% -5.5% -5.6% 111.1% 836.7% -8.1% -9.2% -9.7% 107.2% 401.1%

Average

-4.1% -3.7% -4.4% 114.3% 961.2% -5.1% -5.1% -5.8% 112.9% 727.0% -6.3% -7.0% -8.6% 108.0% 364.3%

FLOPs

Parameters

Model size

334.84G

362,753

1.38MB

50.39G

54,512

220KB

10.51G

11,114

58KB

TABLE VI
OVERALL BD-RATE COMPARISON OF PREVIOUS METHODS [25], [33], [45] IN LDB, LDP, AND RA CONFIGURATION

Methods

LDB

U

Y

V

Y

LDP

U

V

Y

RA

U

V

Jia et al. [33]

-6.0% -2.9%

-3.5%

-4.7%

-1.0%

-1.2% -6.0% -3.2%

-3.8%

Our network + RM

-4.5% -5.1% -5.4% -5.4% -6.6% -6.4% -5.7% -6.1% -6.6%

Our network + Frame control [25]

Our network + CTU control [45]

-3.7%

-4.1%

-3.3%

-4.4%

-3.2%

-4.9%

-4.4%

-4.6%

-4.6%

-5.8%

-3.9%

-5.9%

-4.6%

-4.5%

-4.8%

-5.1%

-5.1%

-5.8%

10

(a) HM Rec.

(b) Jia et al. [33]

(c) Proposed model

Fig. 8. Visual quality comparison of Jia et al. [33] and the proposed model for AI conﬁguration. The test qp is 37 and this is the 1st frame for FourPeople(Anchor
HM-16.9).

(a) HM Rec.

(b) CTU-level control [27]

(c) Proposed model

Fig. 9. Visual quality comparison of CTU-level control [27] and the proposed model for RA conﬁguration. The test QP is 37 and this is the 16th frame for
RaceHorse(Anchor HM-16.16).

TABLE VII
BD-RATE REDUCTION AND COMPUTATIONAL COMPLEXITY (GPU) OF THE PROPOSED METHOD THAN VTM-6.3 ANCHOR

Sequences

Y

U

ClassA

Trafﬁc

PeopleOnStreet

Kimono

ParkScene

ClassB

Cactus

BasketballDrive

BQTerrace

BasketballDrill

BQMall

PartyScene

RaceHorses

BasketballPass

BQSquare

BlowingBubbles

RaceHorses

Vidyo1

Vidyo3

Vidyo4

FourPeople

Johnny

KristenAndSara

ClassC

ClassD

ClassE

-1.6%

-1.3%

-0.3%

-1.9%

-1.3%

-0.3%

-1.0%

-2.7%

-2.2%

-1.8%

-0.9%

-2.1%

-3.0%

-2.1%

-2.8%

-1.3%

-1.1%

-0.8%

-2.1%

-1.3%

-1.7%

-0.2%

-0.4%

0.1%

0.1%

-0.5%

-0.8%

-0.6%

-3.8%

-0.8%

-1.1%

-1.1%

-1.4%

-0.2%

-1.4%

-2.7%

-0.1%

0.2%

-0.3%

-0.5%

-0.4%

-0.6%

AI

V

-0.4%

-0.3%

-0.3%

-0.1%

-0.8%

-1.0%

-0.6%

-5.5%

-0.7%

-1.5%

-2.3%

-4.7%

-1.0%

-1.0%

-4.6%

-0.3%

-0.2%

-0.2%

-0.5%

-0.6%

-0.7%

∆Tenc

∆Tdec

Y

U

100.7%

234.1%

98.0%

225.4%

104.9%

317.5%

108.3%

232.4%

100.4%

244.5%

103.7%

282.7%

101.6%

228.4%

101.6%

219.2%

100.4%

220.6%

100.5%

198.7%

101.6%

233.5%

99.4%

407.5%

103.0%

319.1%

101.1%

352.0%

99.6%

366.6%

101.5%

340.3%

101.8%

298.8%

106.4%

291.0%

99.2%

99.9%

263.3%

317.9%

101.1%

344.5%

-1.1%

-0.9%

-0.2%

-1.4%

-1.4%

-0.4%

-1.9%

-1.6%

-2.0%

-1.3%

-1.1%

-1.2%

-3.6%

-1.6%

-2.4%

-1.0%

-1.2%

-1.0%

-1.8%

-2.6%

-1.6%

-0.7%

-0.1%

0.0%

0.3%

-1.8%

-0.8%

-1.6%

-3.4%

-1.1%

-1.6%

-1.5%

-2.4%

-1.0%

-2.3%

-3.1%

-0.1%

1.5%

0.3%

-0.8%

-1.2%

-0.4%

RA

V

-0.5%

-0.2%

-0.4%

-0.2%

-1.7%

-0.5%

-1.5%

-2.9%

-0.5%

-1.8%

-2.5%

-1.0%

-1.6%

-2.4%

-6.5%

0.4%

0.9%

-1.4%

-0.7%

-1.1%

-1.4%

∆Tenc

∆Tdec

99.6%

99.7%

357.0%

266.3%

99.8%

319.3%

101.2%

302.1%

103.1%

343.9%

100.5%

321.0%

101.6%

313.0%

102.9%

270.8%

104.6%

285.1%

101.7%

242.7%

99.5%

243.8%

98.0%

406.1%

105.6%

421.0%

101.0%

371.1%

98.6%

312.1%

102.6%

486.6%

102.0%

457.4%

101.3%

425.5%

106.2%

449.3%

100.1%

452.4%

100.2%

428.0%

Average

-1.6% -0.8% -1.3% 101.6% 282.8% -1.5% -1.0% -1.3% 101.4% 355.9%

TABLE VIII
ABLATION STUDY OF RM (AI, VTM-6.3)

Our network

Our network+RM

Y

U

V

Y

U

V

-0.5% -0.1%

0.3%

1.3%

0.4%

0.1%

-1.6% -1.8% -2.8%

-2.6% -2.2% -3.6%

2.6%

1.5%

-0.3%

-0.9%

-1.5%

-1.0%

-1.9%

-2.5%

-1.4%

-0.3%

-0.3%

-1.7%

-1.4%

-0.3%

-0.4%

-0.5%

-2.5%

-2.8%

-0.4%

0.3% -0.7% -1.6% -0.8% -1.3%

Sequences

ClassA

ClassB

ClassC

ClassD

ClassE

Average

TABLE IX
ABLATION STUDY OF PARAMETER INITIALIZATION (AI, VTM-6.3)

Methods

Student

Student + MMD [39]

Student + AT [38]

∆PSNR(dB)

Y

U

V

0.310

0.320

0.329

0.231

0.245

0.256

0.295

0.313

0.328

classiﬁers, our proposed method reduces the required storage
consumption effectively beneﬁt from the RM module.

C. Experiment on H.266/VVC

To further evaluate the performance of our proposed model,
we use the same test condition to test its performance in VTM-
6.3. The only difference is that we use the entire DIV2k
instead of the down-sampled dataset to train the proposed
model. From the experimental results shown in Table VII,
it can be found that our model achieves about 1.6% and
1.5% BD-rate reduction on the luminance component for AI
and RA conﬁgurations. For chrominance components, it also
achieved similar performance on BD-rate reduction. In terms
of complexity, the proposed method introduces a negligible
increase on the encoding side and brings about 3 times
complexity to the decoding side.

D. Ablation Study

1) RM for intra frames: RM can effectively improve the
generalization ability of learning-based ﬁlters. The experi-
ments of RM about the inter frames have been carried out in
Section V-B. Based on VTM here, we further conduct ablation
experiments on intra frames to illustrate the performance of
RM. Its test setting is the same as before. From the experi-
ment shown in Table VIII, we can ﬁnd about 0.8% BD-rate
reduction has been achieved by the RM module. Regarding the
performance of class-B, only using the proposed CNN-ﬁlter
may even have a negative effect and leads to 0.3% BD-rate
increment. But its performance has been well improved after
using RM. For most other classes, the performance has also
been improved more or less after using RM.

2) The initialization of parameters: The 1-st frame of
all HEVC test sequences is tested and the overall PSNR
increments are shown in Table IX, where the student model

11

without transfer learning is indicated as ”Student” row. MMD
and AT in Table IX represent different transfer learning ways
that act on the student model. By comparing the ”Student”
row with the other rows, we can ﬁnd that the PSNR of the
student model is improved by both MMD and AT. What’s
more, the improvements of the chrominance components are
more obvious than that of the luminance component.

VI. CONCLUSION

In this paper, a CNN-based low complexity ﬁlter is proposed
for video coding. The lightweight DSC merged with the batch
normalization is used as the backbone. Based on the transfer
learning, attention transfer is utilized to initialize the param-
eters of the proposed network. By adding a novel parametric
module RM after the CNN ﬁlter, the generality of the CNN
ﬁlter is improved and can also handle the ﬁltering problem of
inter frames. What’s more, RM is independent of the proposed
network and can also combine with other learning-based ﬁlters
to alleviate the over-smoothing problem. The experimental re-
sults show our proposed model achieves excellent performance
in terms of both BD-rate and complexity. For HEVC test
sequences, our proposed model achieves about 1.2% BD-rate
reduction and 79.1% FLOPs than VR-CNN anchor. Compared
with Jia et al. [33], our model achieves comparative BD-
rate reduction with much lower complexity. Finally, we also
conduct the experiments on H.266/VVC and ablation studies
to demonstrate the effectiveness of the model. Our future work
aims at further performance improvement of the learning-
based ﬁlter in video coding.

REFERENCES

[1] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, “Overview
of the h. 264/avc video coding standard,” IEEE Transactions on circuits
and systems for video technology, vol. 13, no. 7, pp. 560–576, 2003.
[2] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, “Overview of
the high efﬁciency video coding (hevc) standard,” IEEE Transactions
on circuits and systems for video technology, vol. 22, no. 12, pp. 1649–
1668, 2012.

[3] H266. https://de.wikipedia.org/wiki/h.266/, 2018. 4.
[4] J. Lainema, F. Bossen, W.-J. Han, J. Min, and K. Ugur, “Intra coding
of the hevc standard,” IEEE Transactions on Circuits and Systems for
Video Technology, vol. 22, no. 12, pp. 1792–1801, 2012.

[5] J.-L. Lin, Y.-W. Chen, Y.-W. Huang, and S.-M. Lei, “Motion vector
coding in the hevc standard,” IEEE Journal of selected topics in Signal
Processing, vol. 7, no. 6, pp. 957–968, 2013.

[6] T. Nguyen, P. Helle, M. Winken, B. Bross, D. Marpe, H. Schwarz, and
T. Wiegand, “Transform coding techniques in hevc,” IEEE Journal of
Selected Topics in Signal Processing, vol. 7, no. 6, pp. 978–989, 2013.
[7] O. Crave, B. Pesquet-Popescu, and C. Guillemot, “Robust video coding
based on multiple description scalar quantization with side information,”
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 20, no. 6, pp. 769–779, 2010.

[8] D. Marpe, H. Schwarz, and T. Wiegand, “Context-based adaptive binary
arithmetic coding in the h. 264/avc video compression standard,” IEEE
Transactions on circuits and systems for video technology, vol. 13, no. 7,
pp. 620–636, 2003.

[9] A. Norkin, G. Bjontegaard, A. Fuldseth, M. Narroschke, M. Ikeda,
K. Andersson, M. Zhou, and G. Van der Auwera, “Hevc deblocking
ﬁlter,” IEEE Transactions on Circuits and Systems for Video Technology,
vol. 22, no. 12, pp. 1746–1754, 2012.

[10] C.-M. Fu, E. Alshina, A. Alshin, Y.-W. Huang, C.-Y. Chen, C.-Y. Tsai,
C.-W. Hsu, S.-M. Lei, J.-H. Park, and W.-J. Han, “Sample adaptive offset
in the hevc standard,” IEEE Transactions on Circuits and Systems for
Video technology, vol. 22, no. 12, pp. 1755–1764, 2012.

12

[11] C.-Y. Tsai, C.-Y. Chen, T. Yamakage, I. S. Chong, Y.-W. Huang, C.-M.
Fu, T. Itoh, T. Watanabe, T. Chujoh, M. Karczewicz et al., “Adaptive
loop ﬁltering for video coding,” IEEE Journal of Selected Topics in
Signal Processing, vol. 7, no. 6, pp. 934–945, 2013.

[12] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet:
Keypoint triplets for object detection,” in Proceedings of the IEEE
International Conference on Computer Vision, 2019, pp. 6569–6578.

[13] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep
learning: A review,” IEEE transactions on neural networks and learning
systems, 2019.

[14] X. Liu, Z. Deng, and Y. Yang, “Recent progress in semantic image
segmentation,” Artiﬁcial Intelligence Review, vol. 52, no. 2, pp. 1089–
1106, 2019.

[15] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and
L. Fei-Fei, “Auto-deeplab: Hierarchical neural architecture search for
semantic image segmentation,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2019, pp. 82–92.
[16] X. Hu, H. Mu, X. Zhang, Z. Wang, T. Tan, and J. Sun, “Meta-sr:
A magniﬁcation-arbitrary network for super-resolution,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2019, pp. 1575–1584.

[17] J. W. Soh, G. Y. Park, J. Jo, and N. I. Cho, “Natural and realistic single
image super-resolution with explicit natural manifold discrimination,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2019, pp. 8122–8131.

[18] J. Li, B. Li, J. Xu, R. Xiong, and W. Gao, “Fully connected network-
based intra prediction for image coding,” IEEE Transactions on Image
Processing, vol. 27, no. 7, pp. 3236–3247, 2018.

[19] Y. Hu, W. Yang, M. Li, and J. Liu, “Progressive spatial recurrent neural
network for intra prediction,” IEEE Transactions on Multimedia, vol. 21,
no. 12, pp. 3024–3037, 2019.

[20] H. Sun, Z. Cheng, M. Takeuchi, and J. Katto, “Enhanced intra prediction
for video coding by using multiple neural networks,” IEEE Transactions
on Multimedia, 2020.

[21] J. Liu, S. Xia, W. Yang, M. Li, and D. Liu, “One-for-all: Grouped
variation network-based fractional interpolation in video coding,” IEEE
Transactions on Image Processing, vol. 28, no. 5, pp. 2140–2151, 2018.
[22] L. Zhao, S. Wang, X. Zhang, S. Wang, S. Ma, and W. Gao, “Enhanced
motion-compensated video coding with deep virtual reference frame
generation,” IEEE Transactions on Image Processing, 2019.

[23] R. Song, D. Liu, H. Li, and F. Wu, “Neural network-based arithmetic
coding of intra prediction modes in hevc,” in 2017 IEEE Visual Com-
munications and Image Processing (VCIP).

IEEE, 2017, pp. 1–4.

[24] C. Ma, D. Liu, X. Peng, Z.-J. Zha, and F. Wu, “Neural network-based
arithmetic coding for inter prediction information in hevc,” in 2019 IEEE
International Symposium on Circuits and Systems (ISCAS). IEEE, 2019,
pp. 1–5.

[25] W. Park and M. Kim, “Cnn-based in-loop ﬁltering for coding efﬁciency
improvement,” in 2016 IEEE 12th Image, Video, and Multidimensional
Signal Processing Workshop (IVMSP), July 2016, pp. 1–5.

[26] Y. Dai, D. Liu, and F. Wu, “A convolutional neural network approach
for post-processing in hevc intra coding,” in International Conference
on Multimedia Modeling. Springer, 2017, pp. 28–39.

[27] Y. Dai, D. Liu, Z.-J. Zha, and F. Wu, “A cnn-based in-loop ﬁlter with
cu classiﬁcation for hevc,” in 2018 IEEE Visual Communications and
Image Processing (VCIP).

IEEE, 2018, pp. 1–4.

[28] C. Liu, H. Sun, J. Chen, Z. Cheng, M. Takeuchi, J. Katto, X. Zeng, and
Y. Fan, “Dual learning-based video coding with inception dense blocks,”
in 2019 Picture Coding Symposium (PCS), Nov 2019, pp. 1–5.
[29] J. Kang, S. Kim, and K. M. Lee, “Multi-modal/multi-scale convolutional
neural network based in-loop ﬁlter design for next generation video
codec,” in 2017 IEEE International Conference on Image Processing
(ICIP), Sep. 2017, pp. 26–30.

[30] Y. Zhang, T. Shen, X. Ji, Y. Zhang, R. Xiong, and Q. Dai, “Residual
highway convolutional neural networks for in-loop ﬁltering in hevc,”
IEEE Transactions on Image Processing, vol. 27, no. 8, pp. 3827–3841,
Aug 2018.

[31] X. He, Q. Hu, X. Zhang, C. Zhang, W. Lin, and X. Han, “Enhancing
hevc compressed videos with a partition-masked convolutional neural
network,” in 2018 25th IEEE International Conference on Image Pro-
cessing (ICIP), Oct 2018, pp. 216–220.

[32] R. Yang, M. Xu, and Z. Wang, “Decoder-side hevc quality enhancement
with scalable convolutional neural network,” in 2017 IEEE International
Conference on Multimedia and Expo (ICME).
IEEE, 2017, pp. 817–
822.

[33] C. Jia, S. Wang, X. Zhang, S. Wang, J. Liu, S. Pu, and S. Ma,
“Content-aware convolutional neural network for in-loop ﬁltering in

high efﬁciency video coding,” IEEE Transactions on Image Processing,
vol. 28, no. 7, pp. 3343–3356, 2019.

[34] G. Bjontegarrd, “Calculation of average psnr differences between rd-

curves,” VCEG-M33, 2001.

[35] L. Sifre and S. Mallat, “Rigid-motion scattering for image classiﬁcation,

2014,” Ph.D. dissertation, Ph. D. thesis, 2014.

[36] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, 2017.

[37] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural

network,” Computer Science, vol. 14, no. 7, pp. 38–39, 2015.

[38] S. Zagoruyko and N. Komodakis, “Paying more attention to attention:
Improving the performance of convolutional neural networks via atten-
tion transfer,” arXiv preprint arXiv:1612.03928, 2016.

[39] Z. Huang and N. Wang, “Like what you like: Knowledge distill via
neuron selectivity transfer,” arXiv preprint arXiv:1707.01219, 2017.
[40] J. Yang, X. Shen, J. Xing, X. Tian, H. Li, B. Deng, J. Huang, and X.-s.
Hua, “Quantization networks,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2019, pp. 7308–7316.

[41] M. Nagel, M. van Baalen, T. Blankevoort, and M. Welling, “Data-free
quantization through weight equalization and bias correction,” arXiv
preprint arXiv:1906.04721, 2019.

[42] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, “Importance
estimation for neural network pruning,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2019, pp.
11 264–11 272.

[43] C. Zhao, B. Ni, J. Zhang, Q. Zhao, W. Zhang, and Q. Tian, “Variational
convolutional neural network pruning,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2019, pp.
2780–2789.

[44] S. Zhang, Z. Fan, N. Ling, and M. Jiang, “Recursive residual convo-
lutional neural network- based in-loop ﬁltering for intra frames,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 30,
no. 7, pp. 1888–1900, 2020.

[45] C. Jia, S. Wang, X. Zhang, S. Wang, and S. Ma, “Spatial-temporal
residue network based in-loop ﬁlter for video coding,” in 2017 IEEE
Visual Communications and Image Processing (VCIP), Dec 2017, pp.
1–4.

[46] J. Yao, X. Song, S. Fang, and L. Wang, “Ahg9: Convolutional neural

network ﬁlter for inter frame,” Apr 2018, jVET-J0043.

[47] D. Ding, L. Kong, G. Chen, Z. Liu, and Y. Fang, “A switchable
deep learning approach for in-loop ﬁltering in video coding,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 30,
no. 7, pp. 1871–1887, 2020.

[48] H. Sun, C. Liu, J. Katto, and Y. Fan, “An image compression framework
with learning-based ﬁlter,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops, 2020, pp. 152–
153.

[49] C. Liu, H. Sun, J. Katto, X. Zeng, and Y. Fan, “A learning-based low
complexity in-loop ﬁlter for video coding,” in 2020 IEEE International
Conference on Multimedia & Expo Workshops (ICMEW).
IEEE, 2020,
pp. 1–6.

[50] G. Cote, B. Erol, M. Gallant, and F. Kossentini, “H. 263+: Video coding
at low bit rates,” IEEE Transactions on circuits and systems for video
technology, vol. 8, no. 7, pp. 849–866, 1998.

[51] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[52] Y. Wang, H. Zhu, Y. Li, Z. Chen, and S. Liu, “Dense residual convolu-
tional neural network based in-loop ﬁlter for hevc,” in 2018 IEEE Visual
Communications and Image Processing (VCIP).
IEEE, 2018, pp. 1–4.
[53] J. W. Soh, J. Park, Y. Kim, B. Ahn, H.-S. Lee, Y.-S. Moon, and N. I.
Cho, “Reduction of video compression artifacts based on deep temporal
networks,” IEEE Access, vol. 6, pp. 63 094–63 106, 2018.

[54] C. Li, L. Song, R. Xie, and W. Zhang, “Cnn based post-processing
to improve hevc,” in 2017 IEEE International Conference on Image
Processing (ICIP).
IEEE, 2017, pp. 4577–4580.

[55] F. Zhang, C. Feng, and D. R. Bull, “Enhancing vvc through cnn-based
post-processing,” in 2020 IEEE International Conference on Multimedia
and Expo (ICME).

IEEE, 2020, pp. 1–6.
[56] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola,
“A kernel two-sample test,” Journal of Machine Learning Research,
vol. 13, no. Mar, pp. 723–773, 2012.

[57] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltz-
mann machines,” in Proceedings of the 27th international conference on
machine learning (ICML-10), 2010, pp. 807–814.

13

[58] Https://github.com/Lyken17/pytorch-OpCounter.
[59] Workshop and Challenge on Learned Image Compression. 2020. [On-

line]. http://www.compression.cc/challenge.

[60] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

Irving, M.

[61] F. Chollet et al., “Keras,” https://github.com/fchollet/keras, 2015.
[62] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,
Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
A. Harp, G.
M. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,
P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng,
“TensorFlow: Large-scale machine learning on heterogeneous systems,”
[Online]. Available:
2015, software available from tensorﬂow.org.
http://tensorﬂow.org/

[63] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single image
super-resolution: Dataset and study,” in The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) Workshops, July 2017.

[64] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation,” in
Proceedings of the IEEE international conference on computer vision,
2015, pp. 1026–1034.

