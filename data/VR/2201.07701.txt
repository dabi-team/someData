Noname manuscript No.
(will be inserted by the editor)

On the impact of VR assessment on the Quality of Experience of Highly
Realistic Digital Humans
A Volumetric Video Case Study

Irene Viola · Shishir Subramanyam · Jie Li · Pablo Cesar

2
2
0
2

n
a
J

9
1

]

M
M

.
s
c
[

1
v
1
0
7
7
0
.
1
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Fuelled by the increase in popularity of virtual and
augmented reality applications, point clouds have emerged as
a popular 3D format for acquisition and rendering of digital
humans, thanks to their versatility and real-time capabilities.
Due to technological constraints and real-time rendering lim-
itations, however, the visual quality of dynamic point cloud
contents is seldom evaluated using virtual and augmented re-
ality devices, instead relying on prerecorded videos displayed
on conventional 2D screens. In this study, we evaluate how
the visual quality of point clouds representing digital humans
is affected by compression distortions. In particular, we com-
pare three different viewing conditions based on the degrees
of freedom that are granted to the viewer: passive viewing
(2DTV), head rotation (3DoF), and rotation and translation
(6DoF), to understand how interacting in the virtual space
affects the perception of quality. We provide both quantita-
tive and qualitative results of our evaluation involving 78
participants, and we make the data publicly available. To the
best of our knowledge, this is the ﬁrst study evaluating the
quality of dynamic point clouds in virtual reality, and com-
paring it to traditional viewing settings. Results highlight the
dependency of visual quality on the content under test, and
limitations in the way current data sets are used to evaluate
compression solutions. Moreover, inﬂuencing factors in qual-
ity evaluation in VR, and shortcomings in how point cloud

This work is funded by the European Commission H2020 program,
under the grant agreement 762111, VRTogether, http://vrtogether.eu/.
On behalf of all authors, the corresponding author states that there is no
conﬂict of interest.

Irene Viola and Jie Li
CWI, Amsterdam, The Netherlands
E-mail: irene@cwi.nl, jie.li@cwi.nl
Shishir Subramanyam and Pablo Cesar
CWI, Amsterdam, and TU Delft, Delft, The Netherlands
E-mail: subraman@cwi.nl, p.s.cesar@cwi.nl

encoding solutions handle visually-lossless compression, are
discussed.

Keywords Virtual Reality · Point Clouds · Quality of
Experience (QoE) · Visual Quality Assessment · Degrees of
Freedom (DoF) · Subjective Quality Evaluation

1 Introduction

Recent technological advances in devices for capturing and
rendering immersive media contents, together with the fast
processing capabilities of commodity hardware, have fos-
tered the development of new applications for Virtual Reality
(VR), Augmented Reality (AR) and Mixed Reality (XR).
Such applications usher a new way to engage and interact
with media contents: whereas in traditional 2D consumption,
users are passive receivers and have limited possibilities of
manipulating the contents they are visualizing, immersive
media allow for more interactivity in deciding which content
should be displayed by each user. Commonly, immersive
media applications can be classiﬁed based on the Degrees
of Freedom (DoF) that are available to the end user to ex-
plore the virtual world: 3DoF refers to the availability of
only head rotation as a tool for interaction, as for example in
omnidirectional imagining, whereas 6DoF refers to the abil-
ity to operate translational movements as well as rotational
movements in the 3D space.

In order to populate immersive VR, AR and XR applica-
tions, volumetric contents are needed. In this context, point
clouds have emerged as a popular format to capture and rep-
resent volumetric reconstructions of real-world objects and
people, due to their simplicity and versatility. Geometrical
representation in point clouds is obtained through a collection
of points with x, y and z coordinates in Euclidean space; in
addition, attributes such as colour may be included included
at each point location. This enables a simple representation

 
 
 
 
 
 
2

Irene Viola et al.

that requires no additional pre-processing, is resilient to noise
introduced during capture, and enforces no restrictions on the
attributes that can be encoded at each point location. However,
one main drawback for the deployment of this type of content
is the large amount of data that is required in order to produce
a photorealistic representation: uncompressed, a single point
cloud frame containing one million points requires roughly
20MB to be transmitted. Compression becomes therefore
essential for efﬁcient storage and feasible transmission over
bandwidth-limited networks. Thus, signiﬁcant research [34]
and industrial [43] effort has been focused on optimizing
encoding and transmission, as demonstrated by the ongoing
standardization endeavors by bodies such as JPEG [14, 44]
and MPEG [27].

Given the signiﬁcant storage and bandwidth requirements
for dense dynamic point clouds, decisions need to be taken
regarding the delivery (type of encoder, bit-rate) to ensure
an acceptable quality of experience, depending on the view-
ing conditions. In a previous paper [45], we analysed the
impact of different viewing conditions in VR environments,
namely, with 3DoF or 6DoF locomotion. With this work, we
aim to extend our previous analysis by including results and
discussions obtained in a baseline viewing condition using
traditional 2D screens (2DTV), which is by large the most
commonly used environment for user studies for point cloud
quality assessment.

In this paper, we report ﬁndings obtained in a user study
involving 78 participants assessing 72 stimuli based on eight
dynamic point clouds sequences depicting humans. Each
point cloud sequence was compressed using two encoding
solutions at 4 bit-rates, and evaluated in three viewing con-
ditions (6DoF, 3DoF and 2DTV). The gathered data include
rating scores, presence questionnaires, simulator sickness
reports, along with average watching time. Contributions of
the paper are three-fold:

1. An extensive evaluation of the quality of highly realistic
digital humans represented as dynamic point clouds in
immersive and traditional TV viewing conditions is pro-
vided. Existing protocols [4, 6, 7, 52, 56] did not consider
the dynamic nature of the point clouds, focused on one
type of data set, did not take into account VR viewing
conditions, and did not compare VR ﬁndings with 2DTV
conditions using dynamic contents;

2. Quantitative subjective results about the perceived quality
of the contents, along with qualitative insights on what
is important for users in interacting with digital humans
in VR, are presented. Such results will help in better
conﬁguring the network conditions for the delivery of
points clouds for real-time transmission, and have impli-
cations over ongoing research and standardisation work
regarding the underlying compression technology;

3. The collected raw data, which is comprised of rating
scores, presence questionnaires, and simulator sickness

reports, is made available to the research community,
along with scripts to faithfully recreate the stimuli under
exam1. This will aid reproducibility, while contributing
to ongoing research in the area.

The remainder of the paper is organised as follows. Sec-
tion 2 summarises the related work in the ﬁeld of point cloud
compression and subjective visual quality assessment. Sec-
tion 3 details the methodology that was followed to conduct
the experiments and analyse the data. In Section 4, we re-
port the quantitative and qualitative results of the subjective
visual quality assessment, along with commenting the ﬁnd-
ings in terms of simulator sickness, presence, and interaction
time. Key factors and issues for visual quality assessment
of dynamic point clouds are discussed in Section 5. Finally,
Section 6 concludes the paper. Additional data regarding the
statistical analysis of the results is offered in Appendix 6.

2 Related work

2.1 Quality assessment for point clouds

There is a growing interest on subjective quality assessment
of point clouds rendered on 2D displays. Zhang et al. [56]
evaluated the quality degradation effect of resolution, shape
and color on static point clouds. The results indicate that reso-
lution is almost linearly correlated with the perceived quality,
and color has less impact than shape on the perceived quality.
Zerman et al. [52] compressed two dynamic human point
clouds using a state-of-the-art algorithm [32], and assessed
the effects of this algorithm and input point counts on the per-
ceived quality. Their results showed that no direct correlation
was found between human viewers’ quality ratings and input
point counts. In a recent study [44], a protocol to conduct sub-
jective quality evaluations and benchmark objective quality
metrics were proposed. The viewers passively assessed the
quality of a set of static point clouds, as animations with pre-
deﬁned movement path. In a comprehensive work by Alexiou
et al. [7], the entire set of emerging point cloud compression
encoders developed in the MPEG committee were evaluated
through a series of subjective quality assessment experiments.
Nine static models, including both humans and objects, were
used in the experiments. The experiments provided insights
regarding the performance of the encoders and the types of
degradation they introduce. Zerman et al. [54] compared
the visual quality of point cloud and mesh contents com-
pressed using state-of-the-art algorithms, concluding that,
while meshes are more suitable for high bitrate streaming,
point cloud compression appears to be more advantageous at
lower bitrates. Perry et al. [37] conducted an experiment in 4
laboratories, comparing the latest standardized compression
solutions on static point cloud contents.

1 https://github.com/cwi-dis/2DTV_VR_QoE

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

3

(a) V-PCC

(b) MPEG Anchor

Fig. 1: Point Cloud Digital Humans compressed using two point cloud codecs, V-PCC (left) and MPEG anchor (right), at the
4 selected bit-rates.

Only a limited number of point cloud quality assessment
studies have been conducted in immersive environments.
Mekuria et al. [34] evaluated the subjective quality of their
codec performance in a realistic 3D tele-immersive system,
in which users were represented as 3D avatars and/or 3D
dynamic point clouds, and could navigate in the virtual space
using mouse cursor in a desktop setting. Several aspects of
quality, such as level of immersiveness, togetherness, realism,
quality of motion, were considered. Alexiou and Ebrahimi [6]
proposed the use of AR to subjectively evaluate the quality of
colorless point cloud geometry. Zerman et al. [53] presented
a behaviour analysis of users interacting with colored volu-
metric media in a AR application. Tran et al. [48] suggested
that, in case of evaluating video quality in an immersive
setup, aspects such as cybersickness and presence should not
be overlooked. Recently, an evaluation of static point cloud
contents was conducted in a VR environment [9].

Our work aims at comparing different viewing paradigms
for dynamic point clouds, namely 6DoF, 3DoF, and 2DTV
conditions. Such a comparison is largely absent in the liter-
ature, as previous work has mainly focused on static point
cloud contents and single viewing conditions.

2.2 Point cloud compression

A single point cloud frame is represented by an unordered
collection of points sampled from the surface of an object. In
a dynamic sequence of point clouds, there are no correspon-
dences of points maintained across frames. Thus, detecting
spatial and temporal redundancies is often difﬁcult, making
point cloud compression challenging. Octrees have been used
extensively as a space partitioning structure to represent point
cloud geometry [26, 33]. They are a 3D extension of the 2D
quadtree used to encode video and images.

Research into point cloud compression can be broadly di-
vided into two categories: model-based and projection-based.
The ﬁrst uses signal processing or deep learning techniques
to compress either the geometrical composition of the point
cloud, or its attributes, such as color. Zhang et al. [55] pro-

posed a method to compress point cloud attributes using a
Graph Fourier Transform. They assume that an octree has
been created and separately coded for geometry prior to cod-
ing attributes. De Queiroz and Chou [40] used a Region
Adaptive Hierarchical Transform (RAHT) to use the colors
of nodes in lower levels of the octree to predict the colors of
nodes in the next level. In [11], authors adopt techniques from
traditional image and video processing, using 3D block pre-
diction in combination with shape-adaptive DCT and graph
transforms.

The second category of point cloud codecs aim at pro-
jecting the point cloud information onto a 2D canvas, sub-
sequently using legacy image and video compression solu-
tions to encode them. Intra Frame coding in octrees can be
achieved by entropy coding the occupancy codes, and then
compress the color attributes by mapping them to a 2D grid
and using legacy JPEG image compression, as shown in [34].
In 2017, MPEG started a standardization activity to deter-
mine a new standard codec for point clouds. They used the
codec created by Mekuria et al. [34] as an anchor to evaluate
proposals. To encode dynamic point cloud sequences, MPEG
has currently standardized one method for dynamic dense
point clouds, namely V-PCC [22], and is in the process of
standardizing one method for dynamically acquired, sparse
point clouds, namely G-PCC [21].

Recently, deep learning solutions for point cloud com-
pressions have been proposed, to encode either geometry or
color information, or a combination thereof. Quach et al. [38]
propose the use of an auto-encoder to efﬁciently compress
geometry information, and they subsequently analyse the
impact of several parameters on the performance [39]. Sim-
ilarly, Guarda et al. [18] propose a Convolutional Neural
Network (CNN) architecture to encode and decode point
cloud contents. They further analyse its performance in [17],
and extend the work in [19] by employing explicit and im-
plicit quantization. A deeper architecture is proposed in [50],
which uses 3D convolutional layers along with variational
auto-encoders to achieve favorable compression efﬁciency.
In [5], Alexiou et al. propose a deep learning architecture to
encode both geometry and color attributes, and analyse the

4

Irene Viola et al.

performance of various parameters on the coding efﬁciency
and visual quality.

A complete survey of point cloud compression solutions
can be found here [10]. In our work, we elected to adopt
the MPEG Anchor that was used to evaluate the Call for
Proposals for the MPEG standardization efforts in point cloud
compression [43], and the MPEG standard for dynamic point
clouds V-PCC [22], as they have both been widely used in
quality evaluation campaigns in the literature.

3 Methodology

3.1 Dataset Preparation

A dataset of dynamic point cloud sequences was used from
the MPEG repository. All sequences were clipped to ﬁve
seconds and sampled at 30 frames per second. This included
point cloud sequences [12] captured using photogrammetry
(Longdress, Loot, Red and black, Soldier, shown in ﬁgure
2) and one sequence of a synthetic character sampled from
an animated mesh (Queen). Four additional point cloud se-
quences; Manfred, Despoina, Sarge (shown in Figure 2) were
added for the evaluation. These sequences were created using
motion-captured animated mesh sequences.

Keyframes were selected at 30 frames per second and
extracted along with the associated mesh materials. Partic-
ular care was put in ensuring the selected sequences have
the characters facing the user and speaking in their general
direction. Then, 1 million points were randomly sampled, in-
dependently per key frame to create a consistent groundtruth
dataset. The points were sampled from the mesh surface with
a probability proportional to the area of the underlying mesh
face. This was done to ensure no direct point correspondences
across point cloud frames, to mimic realistic acquisition and
maintain consistency with the rest of the dataset. The X, Y, Z
coordinates of each point was represented using an unsigned
integer, as is required for the current version of the V-PCC
software. Texture information was encoded as 8-bit RGB.

The contents were compressed using two widely avail-
able codecs: the MPEG V-PCC codec in the Release 7.0 [43]
(C1), as the state-of-the-art solution in point cloud compres-
sion, and the MPEG anchor codec [34] (C2), as a baseline
release with real-time capabilities. Bitrate points were se-
lected based on the provided presets for C1, to ensure fair
use. For the additional point cloud sequences for which no
conﬁguration ﬁle was available, the one provided by MPEG
for the Queen sequence was used for all the contents. We
selected the rate points 1, 3 and 5 from the provided preset
V-PCC conﬁgurations and extended it with an additional ﬁnal
rate point using a texture Quantization Parameter (QP) of 8,
a geometry QP of 12, and an occupancy precision of 2. We
re-label the rate points as R1, R2, R3 and R4, respectively.
All sequences are encoded using the C2AI (Category 2 All

Intra) conﬁg. For the photogrammetry sequences, we use the
predeﬁned dedicated conﬁguration ﬁles for each sequence,
at the same rate points.

C2 is used in an all-intra conﬁguration to match the bit-
rates per sequence and rate point (R1-R4) with a tolerance
of 10%, as deﬁned in the MPEG call for proposals. We use
an octree depth from 7 to 10 for the rate points R1 to R4,
respectively. The highest possible JPEG quantization param-
eter values were then chosen per sequence, while meeting
the target bit rate set using C1.An example of content Loot
encoded with the two compression solutions at the selected
rate points is shown in Figure 1.

3.2 Experiment setup

All point cloud sequences were rendered using the Unity
game engine, by storing all the points of each frame in a
vertex buffer, and then drawing procedural geometry on the
GPU. The point clouds were rendered using a quadrilateral
at each point location with a ﬁxed offset of 0.08 units (this
corresponds to a side length of approximately 2mm) around
each point (placed at the centre) for all the sequences, to
be consistent. In the case of bitrate R1 generated using the
MPEG anchor, we increased the offset value to 0.16 by eye,
as the resulting point clouds were too sparse (shown in Figure
1b). We maintain a ﬁxed frame rate of 30fps throughout the
experiment.

Three viewing conditions were selected for comparison:
6DoF, 3DoF, and 2DTV condition. For the 6DoF and 3DoF
viewing conditions, participants were asked to wear an Ocu-
lus Rift CV1 HMD to view each of the point cloud sequences.
For the 3DoF condition, participants were asked to sit on a
swivel chair placed at a ﬁxed location in the room and nav-
igate using head movements alone, whereas for the 6DoF
condition, participants were allowed to navigate freely within
the room. Each sequence was 5 seconds long, after which the
playback looped around. We set the background of the virtual
room to mid-grey, to avoid distractions. The Oculus Guardian
System was used to display in-application wall and ﬂoor
markers if the participants got too close to the boundary. We
used a workstation with 2 GeForce GTX 1080 Ti in SLI for
the GPU and an Intel Core i9 Skylake-X 2.9GHz CPU. For
the 2DTV condition, the videos were created ofﬂine, using
the same rendering as the other two viewing condition, and
played back to the users using MPV2. A 25” Dell UltraSharp
U2515H QHD (2560x1440 px) monitor was used to display
the videos. The monitor was calibrated using an i1Display
Pro color calibration device according to the following pro-
ﬁle: sRGB Gamut, D65 white point, 120cd/m2 brightness,
and minimum black level of 0.2 cd/m2. The test was per-
formed in a room with controlled lighting and mid-grey

2 https://mpv.io/

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

5

Fig. 2: Sequences used for the test, from left to right: Manfred, Sarge, Despoina, Queen, Longdress, Loot, Red and black,
Soldier

walls, in accordance with ITU-T Recommendation BT.500-
13 [23].The illumination level measured on the screens was
15 lux.

3.3 Subjective methodology

To perform the experiments, the subjective methodology
Absolute Category Rating with Hidden References (ACR-
HR) was selected, according to ITU-T Recommendations
P.910 [24]. Participants were asked to observe the video se-
quences depicting digital humans, and rate the corresponding
visual quality on a scale from 1 to 5 (1-Bad, 2-Poor, 3-Fair,
4-Good, and 5-Excellent).

A series of pilot studies were conducted to determine
the positioning of digital humans in the virtual space and
the length of each sequence, to ensure the sequences were
running smoothly within the limited computer RAM. Due
to the huge size of the test material, it was not possible to
evaluate all 8 point cloud contents in one single session, as
long loading times would have brought fatigue to the partic-
ipants and corrupted the results. Thus, we decided to split
the evaluation into two separate tests: one focused on the
evaluation of contents obtained from random sampling of
meshes (T1: contents Queen, Manfred, Despoina and Sarge),
and one focused on contents acquired through photogram-
metry (T2: contents Long dress, Soldier, Red and black, and
Loot). From each sequence, a subset of frames comprising 5
seconds was selected.

Before the test took place, 3 training sequences depicting
examples of 1-Bad, 5-Excellent and 3-Fair were shown to
the users to help them familiarize with the viewing condition
and test setup, and to guide their rating. Following ITU-T
Recommendation BT.500-13 [23], the training sequences
were created using one additional content not shown during
the test, to prevent biased results. Each content sequence was
encoded using the point cloud compression algorithms under
test.

For each test and viewing condition, 36 stimuli were eval-
uated. For each stimulus, the 5 second sequence was played
at least once in full, and kept in loop until the participants

gave their score. The order of the displayed stimuli was ran-
domized per participant and per viewing condition, and the
same content was never displayed twice in a row to avoid bias.
Moreover, the presentation order of viewing conditions was
randomized between participants, to prevent any confound-
ing effect. Two dummy samples were added at the beginning
of each viewing session to ease participants into the task, and
the corresponding scores were subsequently discarded.

After each VR viewing conditions (6DoF and 3DoF),
participants were requested to ﬁll in the Igroup Presence
Questionnaire (IPQ) [42] on a 1-7 discrete scale (1=fully
disagree to 7=totally agree) and Simulator Sickness Ques-
tionnaire (SSQ) on a 1-4 discrete scale (1=none to 4=se-
vere) [29]. IPQ has three subscales, namely Spatial Presence
(SP), Involvement (INV) and Experienced Realism (REAL),
and one additional general item (G) not belonging to a sub-
scale, which assesses the general ”sense of being there”, and
has high loadings on all three factors, with an especially
strong loading on SP [42]. SSQ was developed to measure
cybersickness in computer simulation and was derived from
a measure of motion sickness [29]. For both T1 and T2, after
the two viewing conditions, participants were interviewed to
1) compare their experiences of assessing quality in 3DoF
and 6DoF, and 2) reﬂect on the factors they considered when
assessing the quality.

For the 6DoF and 3DoF viewing conditions, a total of
27 participants were recruited for T1 (12 males, 15 female,
average age: 22,48 years old), whereas 25 participants were
recruited for T2 (17 males, 8 females, average age: 28,39
years old). The 2DTV viewing condition was conducted sep-
arately, with 26 participants for both T1 and T2 (17 males,
9 females, average age: 34,76 years old). All participants
were screened for color vision and visual acuity, using Isi-
hara and Snellen charts, respectively, according to ITU-T
Recommendations P.910 [24].

3.4 Data analysis

Outlier detection was performed separately for each of the
test datasets T1 and T2, following ITU-T Recommendations

6

Irene Viola et al.

P.913 [25]. The recommended threshold values of r1 = 0.75
and r2 = 0.8 were used. One outlier was found in test dataset
T1, and the corresponding scores were discarded. No outliers
were found in the scores collected for T2. Since the 2DTV
viewing condition was tested with a different subject popula-
tion, outlier detection was conducted separately. No outlier
was detected for this viewing condition.

After outlier detection, the Mean Opinion Score (MOS)
was computed for each stimulus, independently per viewing
condition. The associated 95% Conﬁdence Intervals (CIs)
were obtained assuming a Student’s t-distribution. Addition-
ally, the Differential MOS (DMOS) was obtained by apply-
ing HR removal, following the procedure described in ITU-
T Recommendations P.913 [25]. Non-parametric statistical
analysis was then used to analyze if there are statistical dif-
ferences among variables, using the MATLAB Statistics and
Machine Learning Toolbox, along with the ARTool package
in R [15, 28].

4 Results

4.1 Subjective quality assessment

Figures 3 and 4 shows the results of the subjective quality
assessment of the contents comprising test T1 and test T2, for
the 6DoF, 3DoF and 2DTV viewing conditions. In particular,
the DMOS scores associated with the compressed contents
are shown with solid lines, along with relative CIs, whereas
the dashed lines represent the respective HR scores with the
corresponding conﬁdence intervals.

To assess whether signiﬁcant differences could be found
between the three viewing conditions under test, we ran a non-
parametric Kruskal-Wallis test, separately for test T1 and T2.
The test was chosen as the gathered data was not found to be
normally distributed, according to the Shapiro-Wilk normal-
ity test (T1: W = 0.906, p < .001; T2: W = 0.909, p < .001).
We found a signiﬁcant effect of the viewing condition on the
scores for test T1 (χ 2 = 37.56, p < .001). Post-hoc analy-
sis using Mann-Whitney U test with Bonferroni correction
(α = .05/3) revealed signiﬁcant differences between 6DoF
and 2DTV viewing conditions (p < .001, r = 0.09), and
between 3DoF and 2DTV viewing conditions (p < .001,
r = 0.14), but not between 6DoF and 3DoF (p = 0.101,
r = 0.04). Values seem to indicate an effect of VR view-
ing condition with respect to traditional TV viewing on the
ﬁnal scores; however, the effect sizes suggest that, if the ef-
fect indeed exists, it is small. For test T2, no signiﬁcant effect
was found for viewing condition on the scores (χ 2 = 5.19,
p = 0.075).

It can be observed that codec C1 has generally a more
favorable performance with respect to C2. This is especially
evident for the contents acquired through photogrammetry
(see Fig. 4), for which the gap among the two codecs is

Table 1: Pairwise post-hoc test on the contents for test T1
and T2, using Wilcoxon signed-rank test with Bonferroni
correction.

1
T

2
T

Manfred - Sarge
Manfred - Despoina
Manfred - Queen
Sarge - Despoina
Sarge - Queen
Despoina - Queen

Long dress - Loot
Long dress - Red and black
Long dress - Soldier
Loot - Red and black
Loot - Soldier
Red and black - Soldier

Z

p

0.270
-1.10
-1.59
0.111
5.57 <.001
0.588
-0.54
6.75 <.001
7.04 <.001

-5.19 <.001
0.960
-0.05
-2.73
0.006
4.99 <.001
0.037
2.08
0.009
-2.61

r

0.03
0.04
0.15
0.01
0.18
0.19

0.14
0.00
0.07
0.14
0.06
0.07

more pronounced. Mann-Whitney U test conﬁrmed statisti-
cal signiﬁcance for the two codecs (T1: Z = 6.60, p < .001,
T2: Z = 22.06, p < .001), albeit with notably different ef-
fect sizes between test T1 and T2 (r = 0.13 and r = 0.45,
respectively).

A Kruskal-Wallis test performed on the scores revealed a
signiﬁcant effect of the content on the ﬁnal scores, for both
sets of contents (T1: χ 2 = 64.91, p < .001, T2: χ 2 = 35.23,
p < .001). Table 1 shows the results of the post-hoc test
conducted using Mann-Whitney U test with Bonferroni cor-
rection (α = .05/6). Contents Manfred, Sarge and Despoina
all show statistical signiﬁcance with respect to content Queen
(p < .001, r ≥ 0.15 for all pairs). For contents acquired
through photogrammetry, statistical signiﬁcance was found
between contents Longdress and Loot, and Loot and Red and
black (p < .001, r = 0.14 in both cases), as well as between
contents Long dress and Soldier (p = 0.006, r = 0.07). Re-
sults suggests that contents Long dress and Red and black
appear to be given different scores with respect to contents
Loot and Soldier. However, the effect sizes suggest that the
effect, if existent, is quite small.

We also ran a Kruskal-Wallis test on the scores to assess
whether the selected bit-rates were showing statistical signiﬁ-
cance. Results conﬁrmed that the bit-rates have a signiﬁcant
effect for both tests (T1: χ 2 = 1164.14, p < .001, T2: χ 2 =
1008.42, p < .001). Post-hoc analysis using Mann-Whitney
U test with Bonferroni correction (α = .05/6), shown in Ta-
ble 2 further conﬁrmed that all pairwise comparisons were
statistically signiﬁcant, for both test T1 and T2 (p < .001, r >
0.20 for all pairs).

4.2 Data analysis

4.2.1 T1

In order to further analyze the effect of DoF conditions, con-
tents, codecs and bit-rates, and relative interactions, on the

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

7

(a) Manfred

(b) Sarge

(c) Despoina

(d) Queen

(e) Manfred

(f) Sarge

(g) Despoina

(h) Queen

(i) Manfred

(j) Sarge

(k) Despoina

(l) Queen

Fig. 3: DMOS against achieved bit-rate. HR scores are shown using a dashed plot. Each column represents a content in test
T1, whereas the rows depict results obtained using the viewing conditions 6DoF, 3DoF and 2DTV, respectively.

Table 2: Pairwise post-hoc test on the bitrates for test T1
and T2, using Wilcoxon signed-rank test with Bonferroni
correction.

1
T

2
T

R1 - R2
R1 - R3
R1 - R4
R2 - R3
R2 - R4
R3 - R4

R1 - R2
R1 - R3
R1 - R4
R2 - R3
R2 - R4
R3 - R4

Z

p

-14.30 <.001
-25.54 <.001
-27.36 <.001
-17.03 <.001
-21.05 <.001
<.001
-7.30

-14.61 <.001
-23.84 <.001
-27.39 <.001
-11.20 <.001
-17.95 <.001
<.001
-8.53

r

0.41
0.73
0.78
0.49
0.60
0.21

0.42
0.68
0.79
0.32
0.51
0.24

gathered scores, we ﬁtted a full linear mixed-effects model
on the data, accounting for randomness introduced by the par-
ticipants. Due to the non-normality of our data, the aligned
rank transform was applied prior to the ﬁtting [51]. Since the
transform is designed for a fully randomized test, it is not
suitable for the scores collected during the test, as the HR

addition makes the design matrix rank deﬁcient. However,
the transform can be applied to the differential scores used
to obtain DMOS, as it follows a fully randomized design.
Thus, it was decided to perform the analysis on the differen-
tial scores. Post-hoc contrast tests were performed using the
ART-C tool [15].

For test T1, analysis of deviance on the full mixed-effects
model showed signiﬁcance for main effects Content (F =
53.98, d f = 3, p < .001), Codec (F = 65.19, d f = 1, p <
.001) and bit-rate (F = 595.38, d f = 3, p < .001), but not for
DoF (F = 0.07, d f = 2, p = 0.936). Moreover, signiﬁcant
interaction effects were found for DoF - Content (F = 11.39,
d f = 6, p < .001), Content - Codec (F = 9.97, d f = 3, p <
.001), Content - bit-rate (F = 6.04, d f = 9, p < .001) and
Codec - bit-rate (F = 8.96, d f = 3, p < .001). No interaction
effect beyond the ﬁrst level was found to be signiﬁcant. The
full results of the analysis of deviance can be found in Table 3.

Post-hoc interaction analysis using ART-C revealed sig-
niﬁcant differences at 5% level in 3DoF for content pairs
Manfred - Queen (p < .001), Sarge - Queen (p < .001) and
Despoina - Queen (p < .001); in 6DoF, for content pairs Man-

8

Irene Viola et al.

(a) Long dress

(b) Loot

(c) Red and black

(d) Soldier

(e) Long dress

(f) Loot

(g) Red and black

(h) Soldier

(i) Long dress

(j) Loot

(k) Red and black

(l) Soldier

Fig. 4: DMOS against achieved bit-rate. HR scores are shown using a dashed plot. Each column represents a content in test
T2, whereas the rows depict results obtained using the viewing conditions 6DoF, 3DoF and 2DTV, respectively.

Table 3: Analysis of deviance on the full mixed-effects model,
for test T1.

F

d f

p

DoF
Content
Codec
Bitrate
DoF: Content
DoF: Codec
Content: Codec
DoF: Bitrate
Content: Bitrate
Codec: Bitrate
DoF: Content: Codec
DoF: Content: Bitrate
DoF: Codec: Bitrate
Content: Codec: Bitrate
DoF: Content: Codec: Bitrate

0.07
53.98
65.19
595.38
11.39
0.58
9.97
1.70
6.04
8.96
0.71
0.98
0.73
1.23
0.48

0.936
2
3 < .001
1 < .001
3 < .001
6 < .001
0.660
2
3 < .001
0.116
6
9 < .001
3 < .001
0.643
6
0.480
18
0.627
6
0.273
9
0.969
18

fred - Sarge (p < .001) and Despoina - Queen (p < .001);
in the 2DTV condition, for content pairs Manfred - Sarge
(p = 0.019) Manfred - Queen (p < .001), Sarge - Despoina
(p < .001), Sarge - Queen (p < .001), and Despoina - Queen

(p = 0.024). Additionally, several content pairs exhibited sig-
niﬁcant contrasts when different DoF were employed; for a
full report of the contrasts, we invite readers to consult the
appendix. Most notably, no signiﬁcant effect was found when
the same content was displayed in different DoF mediums;
that is, for every content under exam, the pairs 3DoF - 6DoF,
3DoF - 2DTV, and 6DoF - 2DTV were consistently above
the 5% threshold of signiﬁcance.

Regarding the interaction between factors Content and
Codec, post-hoc analysis using ART-C showed signiﬁcant
differences at 5% level between the two codecs for all con-
tents (p < .001) except Queen, for which the two codecs
were deemed equivalent (p = 0.995). Furthermore, statis-
tically signiﬁcant differences were found, for C1, in con-
tent pairs Manfred - Despoina (p = 0.006), and Despoina
- Queen (p < .001), and for C2, in content pairs Manfred -
Despoina (p = 0.010), Manfred - Queen (p < .001), Sarge -
Queen (p < .001), and Despoina - Queen (p < .001). Several
content pairs were found to have signiﬁcant contrasts when
different codecs were employed; for a complete overview,
we refer readers to the appendix.

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

9

Table 4: Analysis of deviance on the full mixed-effects model,
for test T2.

F

d f

p

DoF
Content
Codec
Bitrate
DoF: Content
DoF: Codec
Content: Codec
DoF: Bitrate
Content: Bitrate
Codec: Bitrate
DoF: Content: Codec
DoF: Content: Bitrate
DoF: Codec: Bitrate
Content: Codec: Bitrate
DoF: Content: Codec: Bitrate

2.59
165.56
1059.81
702.54
1.99
1.08
5.81
0.39
6.30
44.89
0.80
1.10
1.12
2.08
0.69

0.082
2
3 < .001
1 < .001
3 < .001
0.064
6
0.340
2
3 < .001
0.881
6
9 < .001
3 < .001
0.569
6
0.341
18
0.346
6
0.028
9
0.820
18

Signiﬁcant differences at 5% level were also found when
considering post-hoc interactions between factors Content
and Bitrate. In particular, the pair Manfred - Queen was found
to have signiﬁcant differences for rate R2 (p < .001) and R3
(p = 0.018); pair Sarge - Queen for rate R1 (p = 0.033),
R2 (p < .001), and R3 (p = 0.005); pair Despoina - Queen,
for rate R1 (p < .001), R2 (p < .001), and R3 (p < .001).
No signiﬁcant difference was found among contents at rate
R4, indicating that, at the highest quality level, the contents
were rated similarly. As expected, most of the comparisons
between different bitrates, be it with the same or among
different contents, yield statistically signiﬁcant differences;
the complete results are available in the appendix. Finally,
results of the post-hoc analysis of interactions between fac-
tors Codec and Bitrate showed statistically signiﬁcant differ-
ences at 5% level among the codecs for rate R1 (p < .001)
and R3 (p = 0.007), but not for rate R2 (p = 0.052 and R4)
(p = 0.986). In the ﬁrst case, the p-value is quite close to
signiﬁcance, whereas in the second, the test conﬁrms our
previous observations: at the highest quality level, the dif-
ference among codecs seems to be imperceptible. The rest
of the combinations between codecs and bitrates lead to
signiﬁcant differences, with the exception of C1-R3 versus
C2-R4 (p = 0.054), indicating that the null hypothesis can-
not be rejected for C2 at rate R4 when compared to C1 at
rate R3; in other words, to achieve similar ratings to codec
C1 (MPEG V-PCC), codec C2 (the MPEG anchor) requires
higher bandwidth, which is in line with what observed in the
previous section. More exhaustive results can be found in the
appendix.

4.2.2 T2

Results of analysis of deviance on the full mixed-effects
model for test T2 showed signiﬁcance for main effects Con-
tent (F = 165.56, d f = 3, p < .001), Codec (F = 1059.81,

d f = 1, p < .001) and Bitrate (F = 702.54, d f = 3, p < .001)
but not for DoF (F = 2.59, d f = 2, p = 0.0825), similarly
to what was seen for test T1. Two-way interactions were
found signiﬁcant at 5% level between Content and Codec
(F = 5.81, d f = 3, p < .001), Content and Bitrate (F = 6.30,
d f = 9, p < .001), and Codec and Bitrate (F = 44.89, d f = 3,
p < .001), as well as the three-way interaction between Con-
tent, Codec, and Bitrate (F = 2.08, d f = 9, p = 0.028).

Post-hoc interaction analysis using ART-C for the three-
way interaction between factors Content, Codec, and Bitrate
revealed signiﬁcant differences at 5% level between the two
codecs under exam, when ﬁxing content and bitrate level, for
touples involving content Longdress at bitrate R1 (p < .001),
R2 (p < .001), and R3 (p < .001), but not for the highest
bitrate R4 (p = 0.964). Similarly, for content Soldier, touples
had signiﬁcant interactions at rate R1 (p = 0.010), R2 (p <
.001), and R3 (p < .001), but not R4 (p = 0.085). For the
other two contents, all touples at same bitrate were signiﬁcant
(Loot, C1, R1 - Loot, C2, R1: p < .001; Loot, C1, R2 - Loot,
C2, R2: p < .001; Loot, C1, R3 - Loot, C2, R3: p < .001;
Loot, C1, R4 - Loot, C2, R4: p = 0.016; Red and black, C1,
R1 - Red and black, C2, R1: p < .001; Red and black, C1,
R2 - Red and black, C2, R2: p < .001; Red and black, C1, R3
- Red and black, C2, R3: p < .001; Red and black, C1, R4 -
Red and black, C2, R4: p = 0.047). This indicates that, with
the exception of Longdress and Soldier at bitrate R4, for all
rate points and all contents the two codecs were signiﬁcantly
different at 5% level.

Post-hoc interaction also revealed statistical difference
among different contents: considering codec C1 at bitrate
R1, signiﬁcant differences were found in content touples
Longdress - Loot (p < .001), Longdress - Soldier (p < .001),
Loot - Soldier (p = 0.004), and Red and black - Soldier (p <
.001), but not for touples Longdress - Red and black (p =
0.626) and Loot - Red and black (p = 0.091); analogously,
at bitrate R2, differences were found for touples Longdress
- Loot (p = 0.004), Longdress - Soldier (p < .001), Loot -
Soldier (p < .001), and Red and black - Soldier (p < .001),
but not for touples Longdress - Red and black (p = 0.198)
and Loot - Red and black (p = 1). At bitrate R3, signiﬁcant
differences were found only between touples Longdress -
Soldier (p < .001); at bitrate R4, all content touples failed to
reject the null hypothesis. This indicates that differences in
DMOS scores among different contents are more prominent
at lowest bitrates for codec C1, whereas for higher bitrates,
contents received similar scores. When considering codec
C2, at bitrate R1, signiﬁcant differences were observed for
content touples Longdress - Loot (p < .001), Longdress -
Soldier (p < .001), Loot - Red and black (p = 0.047), and
Red and black - Soldier (p = 0.004), but not for touples
Longdress - Red and black (p = 0.999) and Loot - Soldier
(p = 1); similarly, at bitrate R2, differences were deemed
signiﬁcant for content touples Longdress - Loot (p < .001),

10

Irene Viola et al.

Longdress - Soldier (p < .001), Loot - Red and black, (p <
.001), and Red and black - Soldier (p < .001), but not for
Longdress - Red and black (p = 0.994) and Loot - Soldier
(p = 1). This seems to indicate that, a lower bitrates, contents
presenting the same gender were rated similarly. At bitrate
R3, statistical difference could be observed among touples
Longdress - Loot (p < .001), Longdress - Red and black (p =
0.018), Longdress - Soldier (p < .001), Loot - Red and black
(p = 0.009), and Red and black - Soldier (p < .001), but
not for touple Loot - Soldier (p = 1), whereas for bitrate R4,
statistical differences at a signiﬁcant level were only observed
between Longdress - Soldier (p = 0.015). Results indicate
that for the highest bitrate, similar trends can be generally
observed between different contents for both codecs under
consideration, whereas as bitrate decreases, more differences
can be spotted in DMOS trends.

Finally, post-hoc interaction analysis revealed signiﬁcant
differences at 5% level, for content Longdress encoded with
codec C1, between bitrate R1 with respect with all the other
bitrates (R1 - R2: p < .001; R1 - R3: p < .001; R1 - R4:
p < .001); however, no signiﬁcant difference in rating was
found when comparing R2, R3, and R4. Remarkably, dif-
ferent trends can be observed for codec C2, for which only
bitrates R3 - R4 are to be considered statistically equiva-
lent (p = 0.055), whereas all other touples for content Long-
dress present signiﬁcant differences (R1 - R2: p = 0.005;
R1 - R3: p < .001; R1 - R4: p < .001; R2 - R3: p < .001;
R2 - R4: p < .001). For content Loot, statistical differences
are observed, for codec C1, among all bitrates, bar R2 - R3
(p = 0.142) and R3 - R4 (p = 1); the rest falls below the
signiﬁcance level (R1 - R2: p < .001; R1 - R3: p < .001; R1
- R4: p < .001; R2 - R4: p = 0.004). For codec C2, however,
a different trend is observed: bitrates R1 - R2 are the only
touple, for Loot, for which no statistical difference is found
(p = 0.377), whereas all other cases present statistically sig-
niﬁcant differences (p < .001 for all touples). Content Red
and black exhibits similar behaviour as Longdress: for codec
C1, only differences among bitrate R1 with all the other bi-
trates are signiﬁcant (R1 - R2: p < .001; R1 - R3: p < .001;
R1 - R4: p < .001), whereas for codec C2, all bitrate touples
are statistically different at 5% signiﬁcance level (R1 - R2:
p = 0.018; R1 - R3: p < .001; R1 - R4: p < .001; R2 - R3:
p < .001; R2 - R4: p < .001; R3 - R4: p < .001). For con-
tent Soldier, signiﬁcant differences were found, for codec
C1, for all bitrate touples, bar R3 - R4 (p = 0.648; for all
other touples, p < .001); conversely, for codec C2, the only
bitrates who did not exhibit signiﬁcant differences were R1 -
R2 (p = 0.373; for all other touples, p < .001). This seems to
indicate that generally, for codec C1 statistical differences in
score distributions are usually found between lower bitrates,
whereas the highest bitrates have a more uniform behaviour;
however, for codec C2, more differences can be found across
score distribution for all the bitrates, bar certain cases (such

(a) T1

(b) T2

Fig. 5: Average time spent looking at the sequence (in sec-
onds) and relative CIs, against score given to the sequence,
for 3DoF (blue) and 6DoF (red), in test T1 (left) and T2
(right).

as Loot and Soldier) for which the lowest bitrates are deemed
equivalent. Several other combinations of the three factors
under exam were deemed statistically signiﬁcant, more than
what we could cover in this pages: we refer interested readers
to the appendix for a full coverage of the post-hoc interaction
results.

4.3 Additional questionnaires and interaction data

4.3.1 IPQ & SSQ Questionnaires

For T1 and T2, the collected IPQ data under each subscale
are all normally distributed as examined by the Shapiro-Wilk
test (p > 0.05). A paired sample t-test was applied to check
the differences between 3DoF and 6DoF in terms of SP, INV,
REAL and G. For T1, there was a signiﬁcant difference in
SP between 3DoF (M=4.13, SD=0.92) and 6DoF (M=5.04,
SD=0.67), t(26)=-4.44, p < .001, Cohen’s d = 0.52 and also a
signiﬁcant difference in G between 3DoF (M=4.11, SD=1.28)
and 6DoF (M=4.96, SD=1.13), t(26)=-2.60, p < .01, Cohen’s
d = 0.64. For T2, SP was also signiﬁcantly different in 3DoF
(M=4.16, SD=1.17) and 6DoF (M=4.83, SD=1.12), t(24)=-
3.48, p < .01, Cohen’s d = 0.45 and so was G between 3DoF
(M=4.20, SD=1.61) and 6DoF (M=5.08, SD=1.19), t(24)=-
3.56, p < .01, Cohen’s d = 0.71. Other factors showed no
signiﬁcant differences between 3DoF and 6DoF in both T1
and T2.

With respect to SSQ, no signiﬁcant differences (p > 0.05)
were found between 3DoF and 6DoF in terms of cybersick-
ness. We further tested whether there were order effects in
experiencing cybersickness, where half of the participants
started with 6DoF as the ﬁrst condition and 3DoF as the sec-
ond, and the remainder the inverse. No signiﬁcant differences
(p > 0.05) were found for any order effects in experiencing
cybersickness.

12345Given score02468101214Avg time (s)3DoF6DoF12345Given score02468101214Avg time (s)3DoF6DoFOn the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

11

4.3.2 Interaction time

Interaction time was found to be strongly correlated with
MOS values in a study conducted on light ﬁeld image quality
assessment [49], as well as in studies conducted with point
cloud contents in interactive environments [9]. In particular,
it was found that users tended to spend more time interact-
ing with contents at high quality, whereas for low quality
scores, less time was spent looking at the contents. In order
to see whether similar trends could be observed in our data,
we compared the average time spent watching the sequence
in 3DoF and 6DoF, separately for each quality score given
by the participants. Since no interactivity was present in the
2DTV condition, the data was discarded for the analysis. Re-
sults are shown in Fig. 5. A positive trend can be observed
between the given score and the average time spent looking
at the sequence, with the exception of score 5, which for test
T2 shows a negative trend with respect to the time. However,
it should be considered that on average, a small percentage
of scores equal to 5 were given in test T2 (10% of the to-
tal scores), thus, variations may be due to the difference in
sample size. It is also worth noting that, on average, partic-
ipants spent more time looking at the sequences in 6DoF,
with respect to the 3DoF case. Indeed, several participants
pointed out that the lowest scores were the fastest to be given,
whereas for higher quality, it was harder to decide on the
rating.

4.3.3 Interviews

The interviews were only conducted for the VR conditions,
due to time limitations. We asked the same interview ques-
tions for T1 and T2; so, we combined the interview transcripts
of 52 participants (T1=27, T2=25). Participants were labelled
as T1P1-T1P27 or T2P1-T2P25. The categorized answers
are presented as follows:

Factors considered when assessing quality. 57% of the
participants mentioned that they assessed the quality based
on three criteria: 1) overall outline and pattern distortion on
body and on clothes, 2) natural gestures and movements of
the digital humans, and 3) visual artifacts such as blockiness,
blurriness, and extraneous ﬂoating artifacts. As T2P3 com-
mented, “I paid attention to the blurriness of the clothing
patterns, the ﬁngers, and whether the gestures or movements
were smooth. ” 48% of the participants mentioned the quality
assessment criteria are content related, who agreed that it is
easier to spot artifacts for the content with complex patterns
(e.g., Long dress) and dominant colors (e.g., Red and black)
than the content with uniformed colors (e.g., Soldier and
Sarge). T2P5 said, “The two ladies were easier [to assess
the visual quality], because their clothes have strong colors.
The man playing keyboard or something is quite difﬁcult.
His clothes were mainly monotone.” 46% of the participants

considered facial expressions as an unignorable factor for
quality assessment, which they believe is an important cue
for social connectedness. For example, T1P15 mentioned,
“The robotic lady was static and had no [facial] expression
at all. It was difﬁcult to tell the difference [between different
quality levels]. ” Similarly, T2P8 said, “If I could see her
[the lady with red dress] smile and her teeth, I gave the score
of fair to good. ” For the extraneous ﬂoating artifacts (e.g.,
bubbles ﬂickering outside the digital humans), 23% found it
very annoying and lowered the overall quality for the content,
but a few participants (8%) thought these artifacts do not
inﬂuence their quality judgement. T1P4 commented, “The
ﬂickering blocks were annoying and distracting at the begin-
ning, but later I got used to them. It felt like watching an old
movie.”

Difﬁculties in assessment. 42% of the participants pointed
out the difﬁculties in assessing the quality, especially for the
high quality contents, which are not perfect and still have
missing details like blurry faces or wrong ﬁngers. 15% of
the participants speciﬁcally pointed out that it is difﬁcult
to distinguish between quality level 3 to 5. As T2P21 men-
tioned, “It was difﬁcult to give [score] 5, the best ones were
still missing many details like ﬁngers or feet. I was hesitating
all the time between [score] 4 and 5.” 17% of the partici-
pants commented that it gradually became easier in rating
the quality when they adapted to the contents. So, the second
viewing condition was easier for them. For example, T1P12
said, “I noticed that the rating got easier as I got familiar
to the quality levels, and had seen the best and the worst
qualities.”

Comparison between 3DoF and 6DoF. 52% of the par-
ticipants preferred 6DoF, because it allowed them to move
closer to examine the details (e.g., shoes and ﬁngers). They
felt more realistic when walking in the virtual space. How-
ever, they also commented that 3DoF offered a ﬁxed distance
between them and digital humans, enabling a more stable
and focused assessment. For example, T2P13 commented,
“Walking around [6DoF] allowed me to get really close [to
the digital humans] and see more details, like pixel sizes,
shoes, ﬁngers. It felt more realistic. Sitting down [3DoF] has
a distance [between me and the digital humans]. Sometimes,
I found it difﬁcult to assess the quality.” 21% of the partici-
pants preferred relaxation and passiveness in 3DoF, because
they did not ﬁnd much differences between 3DoF and 6DoF
in terms of quality assessment, but they found 3DoF is less
nauseous than 6DoF. As T1P13 mentioned, “I felt more fo-
cused, secured and relaxed when sitting down. I got worried
to be trapped by the cables. I also felt more dizzy when I
walked around [in VR].”

12

5 Discussion

5.1 Testing in VR

Results of our experiment indicate a very small, if not neg-
ligible, effect of viewing condition on the distribution of
the scores. In particular, the viewing condition was deemed
to have a signiﬁcant effect on the distribution of the scores
for the ﬁrst set of contents we tested, revealing signiﬁcant
differences between VR testing and the 2DTV counterpart,
with small effect sizes; for the second set of contents, no
signiﬁcant difference was spotted. The implications of it are
twofold. On one hand, results show that the score distribu-
tions follow similar trends in VR with respect to passive
video consumption, thus conﬁrming the validity of testing
volumetric contents on traditional 2D screens, as it has been
done for the majority of tests performed in the literature.
More speciﬁcally, no signiﬁcant interaction was found, for
both tests, between viewing condition and codec under exam,
indicating that differences among compression solutions are
not affected by the choice of displaying device and interac-
tion paradigm. However, the interaction between viewing
condition and contents was found to be signiﬁcant for test T1:
in particular, results indicate that differences among contents
vary depending on the visualization medium. This might be
due to a variety of factors: the possibility of interacting with
the content, moving closer to inspect details; the presence of
a ﬁxed viewing point, which allows for easier comparison;
the absence of confounding factors such as simulator-induced
sickness or novelty effect. Particular care should be adopted
in choosing contents depending on the type of test that needs
to be carried, making sure that artifacts are visible at the
distance that is selected for passive viewing, for example.

The second implication involves the constituents of qual-
ity of experience. The MOS of visual quality is only one
of the factors that inﬂuences the quality of experience of a
given user; other factors, such as presence and immersion, are
equally important in determining the enjoyment of a given
user using the system. Even though small or no effect of
viewing condition on the MOS distribution was found, re-
sults of the IPQ revealed a strong effect of viewing condition
on spatial presence and general sense of being there. Such
factors should be taken into consideration when designing
new experiments: if visual quality is the main constituent that
needs to be assessed, traditional screens might be substituted
(with caution) for VR assessment; however, if other factors
need to be evaluated, such choice might have a larger impact.

5.2 Datasets

Despite the rich literature in point cloud acquisition and com-
pression, few point cloud datasets are publicly available. This

Irene Viola et al.

is especially true when considering point cloud datasets de-
picting photo-realistic humans. One of the most popular and
widely used full-body dataset, created by 8i Labs [12], con-
sists of only 4 individual contents, whereas the HHI Fraun-
hofer dataset has 1 individual content [13]. In the context
of point cloud compression, such scarcity of available data
may lead to compression solutions being designed, optimized
and tested while considering a considerably narrow range
of input data, thus leading to algorithms that are overﬁtted
to the speciﬁcs of the acquisition method used to obtain the
contents. The consequences of such a scenario are reﬂected
in our results. Whereas for the contents assessed in test T2
a large difference was observed between codec V-PCC and
the MPEG anchor, for the contents in test T1 the gap was
markedly lower, and indeed the signiﬁcance of the effect of
the codec selection had a smaller effect size for test T1 with
respect to test T2, as seen in section 4.1. Test T2 consisted of
contents that had been used in multiple quality assessment ex-
periments [7,8,37,44,47], notably including the performance
evaluation of the upcoming MPEG standard [43]. On the
other hand, test T1 included contents that have not been used
so far in assessment of point cloud compression solutions.
The discrepancies in the results of the subjective quality as-
sessment campaign indicate that performance gains may vary
considerably when new contents are evaluated. A larger body
of contents depicting digital humans, involving several ac-
quisition technologies, is needed in order to properly design,
train and evaluate new compression solutions in a robust way.

5.3 Personal preferences and bias

Subjective evaluation experiments are complicated by many
aspects of human psychology and viewing conditions, such as
participants’ vision ability, translation of quality perception
into ranking scores, adaptations and personal preferences
for contents. Through carefully following the ITU-T Rec-
ommendations P.913 [25], we are able to control some of
the aspects. For example, eliminate the scores given by the
participants with vision problems; train participants to help
them understand the quality levels; randomize the stimuli and
viewing conditions to minimize the order effects. However,
we noticed that personal preferences towards certain con-
tents are difﬁcult to control. Satgunam et al. [41]) found that
their participants were divided into two preference groups:
prefer sharper content versus smoother content. Similarly,
Kortum and Sullivan [30] found that the ”desirability” of
participants had an impact on video quality responses, with a
more desirable video clip being given a higher rating. In our
experiments, content Queen is generally given lower ratings
with respect to the other contents. In the interviews, 40%
of the participants showed their preference towards Soldier,
due to his high-resolution facial features, unitoned clothes

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

13

and natural movements, whereas 27% expressed dislike to-
wards Queen, because of her lifeless look and static gestures.
Body motion has been shown to be important in increasing
the naturalness of virtual characters, especially when involv-
ing complex motions [35]. Moreover, several studies have
highlighted the importance of matching the appearance of
the avatar to the naturalness of the motion, indicating that
“appropriateness” between visual presentation and gesture
plays an important role in realism [16, 31]. Our ﬁndings sug-
gests that realism and naturalness of interaction might have
an impact on the perceived visual quality of the contents as
well. Quality assessment may need to be adjusted based on
content and viewer preferences, and offering training with
different contents, as well as account for factors such as
realism, naturalness of motion, and uncanny valley effect.

5.4 Technological constraints and limitations

The two codecs used in this experiment introduce different
distortions during compression. As the MPEG anchor codec
uses the octree data structure to represent geometry, the num-
ber of points in the decoded cloud varies exponentially based
on the tree depth. Thus, at lower bitrates, the decoded point
clouds are quite sparse, and when the point size is increased
to make them appear watertight, they have a block-y appear-
ance. However, the low delay encoding and decoding of this
codec makes it suitable for real time applications such as
social VR. On the other hand, the V-PCC codec leverages
existing 2D video codecs to compress both geometry and
color, which introduces noise in terms of extraneous objects,
and general geometric artifacts such as misaligned seams
(see Figure 1). However, the approach yields better results
at low bitrates, as demonstrated in our results. The codec is
optimized for human perception of 2D video, which might
lead to undesired results when applied to 3D objects in VR.
The mapping from 3D to 2D is critical to codec performance,
which explains why the encoding phase has high complexity.
Decoding has a lower delay, as it beneﬁts from hardware ac-
celeration of video decoders on GPUs, making this approach
suitable for on-demand streaming.

One of the main shortcoming of both compression solu-
tions lays in their inability to reach visually-lossless quality,
as demonstrated by our results. Achieving a visually pleasant
result is of paramount importance for the market adoption of
the technology; indeed, poor visual quality might lead con-
sumers to tune off from the experience altogether [1]. Visual
perception should be taken into account when designing com-
pression solutions, especially at high bitrates, to ensure that
in absence of strict bandwidth constraints, excellent quality
can be achieved.

5.5 Rendering environment considerations

Previous research on subjective assessment of dynamic point
cloud contents has been primarily conducted in desktop se-
tups [2, 52], whereas VR/AR technologies have been em-
ployed with static content [6, 8], or limited amount of dy-
namic contents [53]. However, placing dynamic contents to
be rendered in a VR/AR environment in real time, in order
for users to interact, adds several technological constraints.
In previous research on subjective assessment of point cloud
content such as the MPEG standardisation activity [2], partic-
ipants were asked to view a video of the point cloud sequence
rendered from a predeﬁned camera path. The same approach
was used by van der Hooft et al. to subjectively and objec-
tively assess the quality of adaptive streaming for point cloud
contents [20]; however, the inﬂuence of camera paths on ob-
jective quality assessment was shown to be signiﬁcant in [46].
In order to allow users to interact with the content in a VR/AR
environment, the dynamic point clouds need to be rendered in
real time. This is, however, extremely resource intensive, and
poses many technical constraints. For our test in particular,
the point clouds needed to be stored as uncompressed ﬁles,
to avoid confounding factors with the compression solutions
under evaluation. The point cloud ﬁles were stored as binary
PLY ﬁles to allow faster read from disk. Yet, real-time pro-
gressive loading of the sequences was not possible, as the
loading operation was interfering with the rendering, thus
leading to drops in the frame rate. Waiting for each sequence
to be loaded, on the other hand, was unattainable, as the read
time from disk would mean long waiting times between one
sequence and another, thus adding to subjects’ fatigue. To
ﬁx the issue, we decided to load all sequences in physical
memory before the test. However, this impacted the amount
of sequences that could be tested in one session, as well as
the length of each. New systems for rendering point clouds
in real time, while respecting the constraints introduced by
quality assessment scenarios, should be developed in order
to foster research in the ﬁeld.

5.6 Protocols for subjective assessment in VR

Choosing the right methodology to follow in order to collect
users’ opinions is a delicate matter, as it can inﬂuence the
statistical power of the collected score, and in some cases lead
to difference in results. Single stimulus methodologies, in
particular, lead to larger CIs with respect to double stimulus
methodologies, and are more subject to be inﬂuenced by
individual content preference [25]. An early study comparing
single and double stimulus methodologies for the evaluation
of colorless point cloud contents indicated that the latter was
more consistent in recognizing the level of impairment, as
relative differences facilitate the rating task [3]. However, the
study pointed out that the single stimulus methodology shows

14

Irene Viola et al.

more discrimination power for compression-like artifacts,
albeit at the cost of wider CIs.

Double stimulus methodologies, while commonly used
in video quality assessment and widely adopted in 2D-based
quality assessment of point cloud contents [7, 43, 44], are
tricky to adopt in VR technology, due to the difﬁculties in
displaying both contents simultaneously in a perceptually sat-
isfying way [36], while ensuring a fair comparison between
the contents under evaluation. When dealing with interac-
tive methodologies, in particular, synchronous display of
any modiﬁcation in viewport is usually enforced, to ensure
that the two contents are always visible at the same condi-
tion [7, 49]. This is clearly challenging to implement in a
6DoF scenario, in which users are free to change their po-
sition in the VR space at any given time. Positioning the
two contents side by side in the same virtual space would
mean that, at any given time, they are seen from two differ-
ent angles; the same problem would arise when temporal
sequencing is employed. A toggle-based method like the one
proposed in [36] is not applicable to moving sequences, as
different frames would be seen between stimuli.

In our study, we saw that content preference had an im-
pact on the ratings, as several contents were deemed of lower
quality, as the scores given to the HR exemplify. Such bias
resulted in a reduced rating range for the contents. Results
of the interviews also pointed out that naturalness of ges-
tures were an important criteria in assessing the visual qual-
ity. Such components would not be normally evaluated in a
double stimulus scenario; however, they are important in un-
derstanding how human perception reacts to digital humans.

6 Conclusion

In this paper, we extend our previous work by comparing the
performance of the point cloud compression standard V-PCC
against an octree-based anchor codec (MPEG anchor). Par-
ticipants were invited to assess the quality of digital humans
represented as dynamic point clouds, in 2DTV screen, as
well as VR with 3DoF and 6DoF conditions. Results indicate
a small effect of viewing condition on the ﬁnal scores for one
of the two sets of contents under test. Moreover, results show
that codec V-PCC has a more favorable performance than
the MPEG anchor, especially at low bit-rates. For the highest
bit-rate, the two codecs are often statistically equivalent. The
contents under test appear to have a signiﬁcant inﬂuence on
how the scores are distributed; thus, new data sets are needed
in order to comprehensively evaluate compression distortions.
Moreover, current encoding solutions, while efﬁcient at low
bitrates, are unable to provide visually lossless results, even
when large volumes of data are available, revealing signiﬁ-
cant shortcomings in point cloud compression. We also point
out that commonly-used double stimulus methodologies for
quality evaluation often reduce the rating task to a difference

recognition, while insights on the quality of the original con-
tents are missed. The raw data is made available at the follow-
ing link: https://github.com/cwi-dis/2DTV_VR_QoE.

References

1. OTT: Beyond

port.
ott-beyond-entertainment/

Entertainment

Re-
Consumer
https://www.conviva.com/research/

Survey

2. Call for proposals for point cloud compression iso/iec jtc1/sc29

wg11 n16732, geneva ch (2017)

3. Alexiou, E., Ebrahimi, T.: On the performance of metrics to predict
quality in point cloud representations. In: Applications of Digi-
tal Image Processing XL, vol. 10396, p. 103961H. International
Society for Optics and Photonics (2017)

4. Alexiou, E., Ebrahimi, T.: Impact of visualisation strategy for sub-
jective quality assessment of point clouds. In: 2018 IEEE Interna-
tional Conference on Multimedia & Expo Workshops (ICMEW),
pp. 1–6. IEEE (2018)

5. Alexiou, E., Tung, K., Ebrahimi, T.: Towards neural network ap-
proaches for point cloud compression. In: Applications of Digital
Image Processing XLIII, vol. 11510, p. 1151008. International
Society for Optics and Photonics (2020)

6. Alexiou, E., Upenik, E., Ebrahimi, T.: Towards subjective qual-
ity assessment of point cloud imaging in augmented reality. In:
2017 IEEE 19th International Workshop on Multimedia Signal
Processing (MMSP), pp. 1–6. IEEE (2017)

7. Alexiou, E., Viola, I., Borges, T.M., Fonseca, T.A., de Queiroz,
R.L., Ebrahimi, T.: A comprehensive study of the rate-distortion
performance in mpeg point cloud compression. APSIPA Transac-
tions on Signal and Information Processing 8, 27 (2019). DOI
10.1017/ATSIP.2019.20. URL http://infoscience.epfl.ch/
record/272124

8. Alexiou, E., Xu, P., Ebrahimi, T.: Towards modelling of visual
saliency in point clouds for immersive applications. In: 26th IEEE
International Conference on Image Processing (ICIP) (2019)
9. Alexiou, E., Yang, N., Ebrahimi, T.: Pointxr: A toolbox for vi-
sualization and subjective evaluation of point clouds in virtual
reality. In: 2020 Twelfth International Conference on Quality of
Multimedia Experience (QoMEX), pp. 1–6. IEEE (2020)

10. Cao, C., Preda, M., Zakharchenko, V., Jang, E.S., Zaharia, T.:
Compression of sparse and dense dynamic point clouds–methods
and standards. Proceedings of the IEEE (2021)

11. Cohen, R.A., Tian, D., Vetro, A.: Point cloud attribute compression
using 3-d intra prediction and shape-adaptive transforms. In: 2016
Data Compression Conference (DCC), pp. 141–150. IEEE (2016)
12. d’Eon, E., Harrison, B., Myers, T., Chou, P.A.: 8i Voxelized
Full Bodies - A Voxelized Point Cloud Dataset, ISO/IEC
JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG) input document
WG11M40059/WG1M74006, Geneva (2017)

13. Ebner, T., Feldmann, I., Schreer, O., Kauff, P., v. Unger, T.: HHI
Point cloud dataset of a boxing trainer, ISO/IEC JTC1/SC29 Joint
WG11/WG1 (MPEG/JPEG) input document MPEG2018/m42921,
Ljubljana (2018)

14. Ebrahimi, T., Foessel, S., Pereira, F., Schelkens, P.: Jpeg pleno:
Toward an efﬁcient representation of visual reality. Ieee Multimedia
23(4), 14–20 (2016)

15. Elkin, L.A., Kay, M., Higgins, J.J., Wobbrock, J.O.: An aligned
rank transform procedure for multifactor contrast tests. arXiv
preprint arXiv:2102.11824 (2021)

16. Ferstl, Y., Thomas, S., Guiard, C., Ennis, C., McDonnell, R.: Hu-
man or robot? investigating voice, appearance and gesture motion
realism of conversational social agents. In: Proceedings of the 21st
ACM International Conference on Intelligent Virtual Agents, pp.
76–83 (2021)

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

15

17. Guarda, A.F., Rodrigues, N.M., Pereira, F.: Deep learning-based
point cloud coding: A behavior and performance study. In: 2019 8th
European Workshop on Visual Information Processing (EUVIP),
pp. 34–39. IEEE (2019)

18. Guarda, A.F., Rodrigues, N.M., Pereira, F.: Point cloud coding:
Adopting a deep learning-based approach. In: 2019 Picture Coding
Symposium (PCS), pp. 1–5. IEEE (2019)

19. Guarda, A.F., Rodrigues, N.M., Pereira, F.: Deep learning-based
point cloud geometry coding: Rd control through implicit and
explicit quantization. In: 2020 IEEE International Conference on
Multimedia & Expo Workshops (ICMEW), pp. 1–6. IEEE (2020)
20. van der Hooft, J., Vega, M.T., Timmerer, C., Begen, A.C., De Turck,
F., Schatz, R.: Objective and subjective qoe evaluation for adaptive
point cloud streaming. In: 2020 twelfth international conference on
quality of multimedia experience (QoMEX), pp. 1–6. IEEE (2020)
21. ISO/IEC DIS 23090-5: Geometry-based point cloud compression.

International Organization for Standardization

22. ISO/IEC DIS 23090-5: Visual volumetric video-based coding
In-
(V3C) and video-based point cloud compression (V-PCC).
ternational Organization for Standardization (2021)

23. ITU-R BT.500-13: Methodology for the subjective assessment of
the quality of television pictures. International Telecommunication
Union (2012)

24. ITU-T P.910: Subjective video quality assessment methods for
multimedia applications. International Telecommunication Union
(2008)

25. ITU-T P.913: Methods for the subjective assessment of video qual-
ity, audio quality and audiovisual quality of Internet video and
distribution quality television in any environment. International
Telecommunication Union (2016)

26. Jackins, C.L., Tanimoto, S.L.: Oct-trees and their use in repre-
senting three-dimensional objects. Computer Graphics and Image
Processing 14(3), 249–270 (1980)

27. Jang, E.S., Preda, M., Mammou, K., Tourapis, A.M., Kim, J.,
Graziosi, D.B., Rhyu, S., Budagavi, M.: Video-based point-cloud-
compression standard in mpeg: from evidence collection to com-
mittee draft [standards in a nutshell].
IEEE Signal Processing
Magazine 36(3), 118–123 (2019)

28. Kay, M., Wobbrock, J.: mjskay/artool: Artool 0.10.6 (2019). DOI
10.5281/zenodo.2556415. URL https://doi.org/10.5281/
zenodo.2556415

29. Kennedy, R.S., Lane, N.E., Berbaum, K.S., Lilienthal, M.G.: Simu-
lator sickness questionnaire: An enhanced method for quantifying
simulator sickness. The international journal of aviation psychol-
ogy 3(3), 203–220 (1993)

30. Kortum, P., Sullivan, M.: The effect of content desirability on
subjective video quality ratings. Human factors 52(1), 105–118
(2010)

31. Kucherenko, T., Jonell, P., Yoon, Y., Wolfert, P., Henter, G.E.: A
large, crowdsourced evaluation of gesture generation systems on
common data: The genea challenge 2020. In: 26th International
Conference on Intelligent User Interfaces, pp. 11–21 (2021)
32. Mammou, K.: PCC test model category 2 v0. ISO/IEC JTC1/SC29/

WG11 N17248 1 (2017)

33. Meagher, D.: Geometric modeling using octree encoding. Com-
puter graphics and image processing 19(2), 129–147 (1982)
34. Mekuria, R., Blom, K., Cesar, P.: Design, implementation, and
evaluation of a point cloud codec for tele-immersive video. IEEE
Transactions on Circuits and Systems for Video Technology 27(4),
828–842 (2017)

35. Neff, M.: Hand gesture synthesis for conversational characters.

Handbook of Human Motion pp. 1–12 (2016)

36. Perrin, A.F., Bist, C., Cozot, R., Ebrahimi, T.: Measuring quality of
omnidirectional high dynamic range content. In: Applications of
Digital Image Processing XL, vol. 10396, p. 1039613. International
Society for Optics and Photonics (2017)

37. Perry, S., Cong, H.P., da Silva Cruz, L.A., Prazeres, J., Pereira,
M., Pinheiro, A., Dumic, E., Alexiou, E., Ebrahimi, T.: Quality
evaluation of static point clouds encoded using mpeg codecs. In:
2020 IEEE International Conference on Image Processing (ICIP),
pp. 3428–3432. IEEE (2020)

38. Quach, M., Valenzise, G., Dufaux, F.: Learning convolutional trans-
forms for lossy point cloud geometry compression. In: 2019 IEEE
International Conference on Image Processing (ICIP), pp. 4320–
4324. IEEE (2019)

39. Quach, M., Valenzise, G., Dufaux, F.: Improved deep point cloud
geometry compression. In: 2020 IEEE 22nd International Work-
shop on Multimedia Signal Processing (MMSP), pp. 1–6. IEEE
(2020)

40. Queiroz, R.D., Chou, P.A.: Compression of 3D Point Clouds Using
a Region-Adaptive Hierarchical Transform. IEEE Transactions on
Image Processing 25 (2016)

41. Satgunam, P.N., Woods, R.L., Bronstad, P.M., Peli, E.: Factors
affecting enhanced video quality preferences. IEEE Transactions
on Image Processing 22(12), 5146–5157 (2013)

42. Schubert, T.W.: The sense of presence in virtual environments: A
three-component scale measuring spatial presence, involvement,
and realness. Zeitschrift f¨ur Medienpsychologie 15(2), 69–71
(2003)

43. Schwarz, S., Preda, M., Baroncini, V., Budagavi, M., Cesar, P.,
Chou, P.A., Cohen, R.A., Krivoku´ca, M., Lasserre, S., Li, Z., et al.:
Emerging MPEG standards for point cloud compression. IEEE
Journal on Emerging and Selected Topics in Circuits and Systems
9(1), 133–148 (2018)

44. da Silva Cruz, L.A., Dumi´c, E., Alexiou, E., Prazeres, J., Duarte,
R., Pereira, M., Pinheiro, A., Ebrahimi, T.: Point cloud quality eval-
uation: Towards a deﬁnition for test conditions. In: 2019 Eleventh
International Conference on Quality of Multimedia Experience
(QoMEX), pp. 1–6. IEEE (2019)

45. Subramanyam, S., Li, J., Viola, I., Cesar, P.: Comparing the quality
of highly realistic digital humans in 3dof and 6dof: A volumetric
video case study. In: 2020 IEEE Conference on Virtual Reality and
3D User Interfaces (VR), pp. 127–136. IEEE (2020)

46. Subramanyam, S., Viola, I., Hanjalic, A., Cesar, P.: User centered
adaptive streaming of dynamic point clouds with low complexity
tiling. In: Proceedings of the 28th ACM International Conference
on Multimedia, pp. 3669–3677 (2020)

47. Torlig, E.M., Alexiou, E., Fonseca, T.A., de Queiroz, R.L.,
Ebrahimi, T.: A novel methodology for quality assessment of vox-
elized point clouds. In: Applications of Digital Image Processing
XLI, vol. 10752, p. 107520I. International Society for Optics and
Photonics (2018)

48. TT Tran, H., Ngoc, N.P., Pham, C.T., Jung, Y.J., Thang, T.C.:
A subjective study on user perception aspects in virtual reality.
Applied Sciences 9(16), 3384 (2019)

49. Viola, I., Ebrahimi, T.: A new framework for interactive quality as-
sessment with application to light ﬁeld coding. In: Applications of
Digital Image Processing XL, vol. 10396, p. 103961F. International
Society for Optics and Photonics (2017)

50. Wang, J., Zhu, H., Liu, H., Ma, Z.: Lossy point cloud geometry
compression via end-to-end learning. IEEE Transactions on Cir-
cuits and Systems for Video Technology (2021)

51. Wobbrock, J.O., Findlater, L., Gergle, D., Higgins, J.J.: The
Aligned Rank Transform for nonparametric factorial analyses using
only ANOVA procedures. In: Proceedings of the SIGCHI confer-
ence on human factors in computing systems, pp. 143–146. ACM
(2011)

52. Zerman, E., Gao, P., Ozcinar, C., Smolic, A.: Subjective and objec-
tive quality assessment for volumetric video compression. In: Fast
track article for IST International Symposium on Electronic Imag-
ing 2019: Image Quality and System Performance XVI proceedings
(2019)

16

Irene Viola et al.

53. Zerman, E., Kulkarni, R., Smolic, A.: User behaviour analysis of
volumetric video in augmented reality. In: 2021 13th International
Conference on Quality of Multimedia Experience (QoMEX), pp.
129–132. IEEE (2021)

54. Zerman, E., Ozcinar, C., Gao, P., Smolic, A.: Textured mesh vs
coloured point cloud: A subjective study for volumetric video
compression. In: 2020 Twelfth International Conference on Quality
of Multimedia Experience (QoMEX), pp. 1–6. IEEE (2020)
55. Zhang, C., Florencio, D., Loop, C.: Point cloud attribute compres-
sion with graph transform. Image Processing (ICIP), 2014 IEEE
International Conference on (2014)

56. Zhang, J., Huang, W., Zhu, X., Hwang, J.N.: A subjective quality
evaluation for 3D point cloud models. In: 2014 International Con-
ference on Audio, Language and Image Processing, pp. 827–831.
IEEE (2014)

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

17

Appendix A

Table 5: Contrast test between factors DoF and Content

contrast
3DoF, Manfred - 3DoF, Sarge
3DoF, Manfred - 3DoF, Despoina
3DoF, Manfred - 3DoF, Queen
3DoF, Manfred - 6DoF, Manfred
3DoF, Manfred - 6DoF, Sarge
3DoF, Manfred - 6DoF, Despoina
3DoF, Manfred - 6DoF, Queen
3DoF, Manfred - 2DTV, Manfred
3DoF, Manfred - 2DTV, Sarge
3DoF, Manfred - 2DTV, Despoina
3DoF, Manfred - 2DTV, Queen
3DoF, Sarge - 3DoF, Despoina
3DoF, Sarge - 3DoF, Queen
3DoF, Sarge - 6DoF, Manfred
3DoF, Sarge - 6DoF, Sarge
3DoF, Sarge - 6DoF, Despoina
3DoF, Sarge - 6DoF, Queen
3DoF, Sarge - 2DTV, Manfred
3DoF, Sarge - 2DTV, Sarge
3DoF, Sarge - 2DTV, Despoina
3DoF, Sarge - 2DTV, Queen
3DoF, Despoina - 3DoF, Queen
3DoF, Despoina - 6DoF, Manfred
3DoF, Despoina - 6DoF, Sarge
3DoF, Despoina - 6DoF, Despoina
3DoF, Despoina - 6DoF, Queen
3DoF, Despoina - 2DTV, Manfred
3DoF, Despoina - 2DTV, Sarge
3DoF, Despoina - 2DTV, Despoina
3DoF, Despoina - 2DTV, Queen
3DoF, Queen - 6DoF, Manfred
3DoF, Queen - 6DoF, Sarge
3DoF, Queen - 6DoF, Despoina
3DoF, Queen - 6DoF, Queen
3DoF, Queen - 2DTV, Manfred
3DoF, Queen - 2DTV, Sarge
3DoF, Queen - 2DTV, Despoina
3DoF, Queen - 2DTV, Queen
6DoF, Manfred - 6DoF, Sarge
6DoF, Manfred - 6DoF, Despoina
6DoF, Manfred - 6DoF, Queen
6DoF, Manfred - 2DTV, Manfred
6DoF, Manfred - 2DTV, Sarge
6DoF, Manfred - 2DTV, Despoina
6DoF, Manfred - 2DTV, Queen
6DoF, Sarge - 6DoF, Despoina
6DoF, Sarge - 6DoF, Queen
6DoF, Sarge - 2DTV, Manfred

estimate
-9.0337
178.6466
-388.5697
-88.2236
-72.3942
279.4808
-188.9303
-30.8302
189.7498
-84.0802
-300.8752
187.6803
-379.5361
-79.1899
-63.3606
288.5144
-179.8966
-21.7965
198.7835
-75.0465
-291.8415
-567.2163
-266.8702
-251.0409
100.8341
-367.5769
-209.4768
11.1032
-262.7268
-479.5218
300.3462
316.1755
668.0505
199.6394
357.7395
578.3195
304.4895
87.6945
15.8293
367.7043
-100.7067
57.3934
277.9734
4.1434
-212.6516
351.8750
-116.5361
41.5640

SE
60.7066
60.7066
60.7066
108.4749
108.4749
108.4749
108.4749
109.5543
109.5543
109.5543
109.5543
60.7066
60.7066
108.4749
108.4749
108.4749
108.4749
109.5543
109.5543
109.5543
109.5543
60.7066
108.4749
108.4749
108.4749
108.4749
109.5543
109.5543
109.5543
109.5543
108.4749
108.4749
108.4749
108.4749
109.5543
109.5543
109.5543
109.5543
60.7066
60.7066
60.7066
109.5543
109.5543
109.5543
109.5543
60.7066
60.7066
109.5543

df
2294.00
2294.00
2294.00
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
2294.00
2294.00
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
2294.00
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
2294.00
2294.00
2294.00
126.03
126.03
126.03
126.03
2294.00
2294.00
126.03

p.value
t.ratio
1.0000
-0.149
0.1268
2.943
-6.401 <.0001
0.9996
-0.813
0.9999
-0.667
0.3044
2.576
0.8452
-1.742
1.0000
-0.281
0.8499
1.732
0.9998
-0.767
0.2166
-2.746
3.092
0.0848
-6.252 <.0001
0.9999
-0.730
1.0000
-0.584
0.2590
2.660
0.8831
-1.658
1.0000
-0.199
0.8071
1.814
0.9999
-0.685
-2.664
0.2568
-9.344 <.0001
0.3747
-2.460
0.4722
-2.314
0.9987
0.930
0.0422
-3.389
0.7495
-1.912
1.0000
0.101
0.4151
-2.398
0.0015
-4.377
0.2064
2.769
2.915
0.1485
6.159 <.0001
0.7924
1.840
3.265
0.0601
5.279 <.0001
0.2018
2.779
0.9997
0.800
0.261
1.0000
6.057 <.0001
0.8865
-1.659
1.0000
0.524
0.3272
2.537
1.0000
0.038
-1.941
0.7312
5.796 <.0001
0.7470
-1.920
1.0000
0.379

18

Irene Viola et al.

Table 5 – Continued from previous page

contrast
6DoF, Sarge - 2DTV, Sarge
6DoF, Sarge - 2DTV, Despoina
6DoF, Sarge - 2DTV, Queen
6DoF, Despoina - 6DoF, Queen
6DoF, Despoina - 2DTV, Manfred
6DoF, Despoina - 2DTV, Sarge
6DoF, Despoina - 2DTV, Despoina
6DoF, Despoina - 2DTV, Queen
6DoF, Queen - 2DTV, Manfred
6DoF, Queen - 2DTV, Sarge
6DoF, Queen - 2DTV, Despoina
6DoF, Queen - 2DTV, Queen
2DTV, Manfred - 2DTV, Sarge
2DTV, Manfred - 2DTV, Despoina
2DTV, Manfred - 2DTV, Queen
2DTV, Sarge - 2DTV, Despoina
2DTV, Sarge - 2DTV, Queen
2DTV, Despoina - 2DTV, Queen
Results are averaged over the levels of: Codecs, Bitrates

estimate
262.1440
-11.6860
-228.4810
-468.4111
-310.3110
-89.7310
-363.5610
-580.3560
158.1001
378.6801
104.8501
-111.9449
220.5800
-53.2500
-270.0450
-273.8300
-490.6250
-216.7950

Degrees-of-freedom method: kenward-roger

SE
109.5543
109.5543
109.5543
60.7066
109.5543
109.5543
109.5543
109.5543
109.5543
109.5543
109.5543
109.5543
61.9088
61.9088
61.9088
61.9088
61.9088
61.9088

df
126.03
126.03
126.03
2294.00
126.03
126.03
126.03
126.03
126.03
126.03
126.03
126.03
2294.00
2294.00
2294.00
2294.00
2294.00
2294.00

p.value
t.ratio
0.4187
2.393
1.0000
-0.107
-2.086
0.6338
-7.716 <.0001
0.1794
-2.832
0.9996
-0.819
-3.319
0.0517
-5.297 <.0001
0.9524
1.443
0.0345
3.457
0.9983
0.957
0.9969
-1.022
0.0193
3.563
0.9994
-0.860
0.0008
-4.362
-4.423
0.0006
-7.925 <.0001
0.0238
-3.502

P value adjustment: tukey method for comparing a family of 12 estimates

Table 6: Contrast test between factors DoF and Content

contrast
Manfred, C1 - Manfred, C2
Manfred, C1 - Sarge, C1
Manfred, C1 - Sarge, C2
Manfred, C1 - Despoina, C1
Manfred, C1 - Despoina, C2
Manfred, C1 - Queen, C1
Manfred, C1 - Queen, C2
Manfred, C2 - Sarge, C1
Manfred, C2 - Sarge, C2
Manfred, C2 - Despoina, C1
Manfred, C2 - Despoina, C2
Manfred, C2 - Queen, C1
Manfred, C2 - Queen, C2
Sarge, C1 - Sarge, C2
Sarge, C1 - Despoina, C1
Sarge, C1 - Despoina, C2
Sarge, C1 - Queen, C1
Sarge, C1 - Queen, C2
Sarge, C2 - Despoina, C1
Sarge, C2 - Despoina, C2
Sarge, C2 - Queen, C1
Sarge, C2 - Queen, C2
Despoina, C1 - Despoina, C2
Despoina, C1 - Queen, C1
Despoina, C1 - Queen, C2
Despoina, C2 - Queen, C1

estimate
259.8001
37.6988
358.0433
181.0136
435.7672
-103.4447
-141.3435
-222.1013
98.2432
-78.7865
175.9671
-363.2449
-401.1436
320.3445
143.3147
398.0683
-141.1436
-179.0423
-177.0297
77.7238
-461.4881
-499.3868
254.7536
-284.4583
-322.3571
-539.2119

SE
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388
49.6388

df
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294

p.value
t.ratio
5.234 <.0001
0.759
0.9950
7.213 <.0001
0.0066
3.647
8.779 <.0001
0.4256
-2.084
0.0842
-2.847
0.0002
-4.474
0.4963
1.979
0.7582
-1.587
3.545
0.0095
-7.318 <.0001
-8.081 <.0001
6.454 <.0001
2.887
0.0756
8.019 <.0001
0.0851
-2.843
0.0076
-3.607
0.0088
-3.566
0.7708
1.566
-9.297 <.0001
-10.060 <.0001
5.132 <.0001
-5.731 <.0001
-6.494 <.0001
-10.863 <.0001

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

19

Table 6 – Continued from previous page

contrast
estimate
Despoina, C2 - Queen, C2
-577.1106
-37.8987
Queen, C1 - Queen, C2
Results are averaged over the levels of: DoF, Bitrates

Degrees-of-freedom method: kenward-roger

SE
49.6388
49.6388

df
2294
2294

t.ratio

p.value
-11.626 <.0001
0.9949
-0.763

P value adjustment: tukey method for comparing a family of 8 estimates

Table 7: Contrast test between factors Content and Bitrate

contrast
Manfred, R1 - Manfred, R2
Manfred, R1 - Manfred, R3
Manfred, R1 - Manfred, R4
Manfred, R1 - Sarge, R1
Manfred, R1 - Sarge, R2
Manfred, R1 - Sarge, R3
Manfred, R1 - Sarge, R4
Manfred, R1 - Despoina, R1
Manfred, R1 - Despoina, R2
Manfred, R1 - Despoina, R3
Manfred, R1 - Despoina, R4
Manfred, R1 - Queen, R1
Manfred, R1 - Queen, R2
Manfred, R1 - Queen, R3
Manfred, R1 - Queen, R4
Manfred, R2 - Manfred, R3
Manfred, R2 - Manfred, R4
Manfred, R2 - Sarge, R1
Manfred, R2 - Sarge, R2
Manfred, R2 - Sarge, R3
Manfred, R2 - Sarge, R4
Manfred, R2 - Despoina, R1
Manfred, R2 - Despoina, R2
Manfred, R2 - Despoina, R3
Manfred, R2 - Despoina, R4
Manfred, R2 - Queen, R1
Manfred, R2 - Queen, R2
Manfred, R2 - Queen, R3
Manfred, R2 - Queen, R4
Manfred, R3 - Manfred, R4
Manfred, R3 - Sarge, R1
Manfred, R3 - Sarge, R2
Manfred, R3 - Sarge, R3
Manfred, R3 - Sarge, R4
Manfred, R3 - Despoina, R1
Manfred, R3 - Despoina, R2
Manfred, R3 - Despoina, R3
Manfred, R3 - Despoina, R4
Manfred, R3 - Queen, R1
Manfred, R3 - Queen, R2
Manfred, R3 - Queen, R3
Manfred, R3 - Queen, R4

estimate
-261.0203
-813.6400
-1080.8721
90.9803
-221.5797
-794.1628
-996.3062
156.6036
-85.3759
-649.9931
-1005.8467
-111.2328
-697.9362
-1025.7954
-1068.4754
-552.6197
-819.8518
352.0005
39.4405
-533.1426
-735.2859
417.6238
175.6444
-388.9728
-744.8264
149.7874
-436.9159
-764.7751
-807.4551
-267.2321
904.6203
592.0603
19.4772
-182.6662
970.2436
728.2641
163.6469
-192.2067
702.4072
115.7038
-212.1554
-254.8354

SE
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360

df
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294

p.value
t.ratio
-4.593
0.0005
-14.316 <.0001
-19.017 <.0001
0.9684
1.601
-3.899
0.0097
-13.973 <.0001
-17.529 <.0001
0.2961
2.755
-1.502
0.9824
-11.436 <.0001
-17.697 <.0001
0.8485
-1.957
-12.280 <.0001
-18.048 <.0001
-18.799 <.0001
-9.723 <.0001
-14.425 <.0001
6.193 <.0001
0.694
1.0000
-9.380 <.0001
-12.937 <.0001
7.348 <.0001
3.090
0.1344
-6.844 <.0001
-13.105 <.0001
2.635
0.3741
-7.687 <.0001
-13.456 <.0001
-14.207 <.0001
-4.702
0.0003
15.916 <.0001
10.417 <.0001
1.0000
0.343
-3.214
0.0958
17.071 <.0001
12.813 <.0001
0.2262
2.879
-3.382
0.0581
12.358 <.0001
0.8050
2.036
0.0179
-3.733
0.0008
-4.484

20

Irene Viola et al.

Table 7 – Continued from previous page

contrast
Manfred, R4 - Sarge, R1
Manfred, R4 - Sarge, R2
Manfred, R4 - Sarge, R3
Manfred, R4 - Sarge, R4
Manfred, R4 - Despoina, R1
Manfred, R4 - Despoina, R2
Manfred, R4 - Despoina, R3
Manfred, R4 - Despoina, R4
Manfred, R4 - Queen, R1
Manfred, R4 - Queen, R2
Manfred, R4 - Queen, R3
Manfred, R4 - Queen, R4
Sarge, R1 - Sarge, R2
Sarge, R1 - Sarge, R3
Sarge, R1 - Sarge, R4
Sarge, R1 - Despoina, R1
Sarge, R1 - Despoina, R2
Sarge, R1 - Despoina, R3
Sarge, R1 - Despoina, R4
Sarge, R1 - Queen, R1
Sarge, R1 - Queen, R2
Sarge, R1 - Queen, R3
Sarge, R1 - Queen, R4
Sarge, R2 - Sarge, R3
Sarge, R2 - Sarge, R4
Sarge, R2 - Despoina, R1
Sarge, R2 - Despoina, R2
Sarge, R2 - Despoina, R3
Sarge, R2 - Despoina, R4
Sarge, R2 - Queen, R1
Sarge, R2 - Queen, R2
Sarge, R2 - Queen, R3
Sarge, R2 - Queen, R4
Sarge, R3 - Sarge, R4
Sarge, R3 - Despoina, R1
Sarge, R3 - Despoina, R2
Sarge, R3 - Despoina, R3
Sarge, R3 - Despoina, R4
Sarge, R3 - Queen, R1
Sarge, R3 - Queen, R2
Sarge, R3 - Queen, R3
Sarge, R3 - Queen, R4
Sarge, R4 - Despoina, R1
Sarge, R4 - Despoina, R2
Sarge, R4 - Despoina, R3
Sarge, R4 - Despoina, R4
Sarge, R4 - Queen, R1
Sarge, R4 - Queen, R2
Sarge, R4 - Queen, R3
Sarge, R4 - Queen, R4
Despoina, R1 - Despoina, R2

estimate
1171.8523
859.2923
286.7092
84.5659
1237.4756
995.4962
430.8790
75.0254
969.6392
382.9359
55.0767
12.3967
-312.5600
-885.1431
-1087.2864
65.6233
-176.3562
-740.9733
-1096.8269
-202.2131
-788.9164
-1116.7756
-1159.4556
-572.5831
-774.7264
378.1833
136.2038
-428.4133
-784.2669
110.3469
-476.3564
-804.2156
-846.8956
-202.1433
950.7664
708.7869
144.1697
-211.6838
682.9300
96.2267
-231.6326
-274.3126
1152.9097
910.9303
346.3131
-9.5405
885.0733
298.3700
-29.4892
-72.1692
-241.9795

SE
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360

df
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294

t.ratio
p.value
20.618 <.0001
15.119 <.0001
0.0001
5.044
1.488
0.9839
21.773 <.0001
17.515 <.0001
7.581 <.0001
1.320
0.9952
17.060 <.0001
6.738 <.0001
0.9999
0.969
0.218
1.0000
-5.499 <.0001
-15.574 <.0001
-19.130 <.0001
0.9989
1.155
-3.103
0.1300
-13.037 <.0001
-19.298 <.0001
-3.558
0.0329
-13.881 <.0001
-19.649 <.0001
-20.400 <.0001
-10.074 <.0001
-13.631 <.0001
6.654 <.0001
2.396
0.5505
-7.538 <.0001
-13.799 <.0001
0.8564
1.941
-8.381 <.0001
-14.150 <.0001
-14.901 <.0001
0.0331
-3.557
16.728 <.0001
12.471 <.0001
0.4447
2.537
0.0185
-3.724
12.016 <.0001
0.9489
1.693
0.0048
-4.075
0.0002
-4.826
20.285 <.0001
16.027 <.0001
6.093 <.0001
1.0000
-0.168
15.572 <.0001
5.250 <.0001
1.0000
-0.519
0.9968
-1.270
0.0023
-4.258

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

21

Table 7 – Continued from previous page

contrast
Despoina, R1 - Despoina, R3
Despoina, R1 - Despoina, R4
Despoina, R1 - Queen, R1
Despoina, R1 - Queen, R2
Despoina, R1 - Queen, R3
Despoina, R1 - Queen, R4
Despoina, R2 - Despoina, R3
Despoina, R2 - Despoina, R4
Despoina, R2 - Queen, R1
Despoina, R2 - Queen, R2
Despoina, R2 - Queen, R3
Despoina, R2 - Queen, R4
Despoina, R3 - Despoina, R4
Despoina, R3 - Queen, R1
Despoina, R3 - Queen, R2
Despoina, R3 - Queen, R3
Despoina, R3 - Queen, R4
Despoina, R4 - Queen, R1
Despoina, R4 - Queen, R2
Despoina, R4 - Queen, R3
Despoina, R4 - Queen, R4
Queen, R1 - Queen, R2
Queen, R1 - Queen, R3
Queen, R1 - Queen, R4
Queen, R2 - Queen, R3
Queen, R2 - Queen, R4
Queen, R3 - Queen, R4
Results are averaged over the levels of: DoF, Codecs

estimate
-806.5967
-1162.4503
-267.8364
-854.5397
-1182.3990
-1225.0790
-564.6172
-920.4708
-25.8569
-612.5603
-940.4195
-983.0995
-355.8536
538.7603
-47.9431
-375.8023
-418.4823
894.6138
307.9105
-19.9487
-62.6287
-586.7033
-914.5626
-957.2426
-327.8592
-370.5392
-42.6800

Degrees-of-freedom method: kenward-roger

SE
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360
56.8360

df
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294

t.ratio

p.value
-14.192 <.0001
-20.453 <.0001
-4.712
0.0003
-15.035 <.0001
-20.804 <.0001
-21.555 <.0001
-9.934 <.0001
-16.195 <.0001
-0.455
1.0000
-10.778 <.0001
-16.546 <.0001
-17.297 <.0001
-6.261 <.0001
9.479 <.0001
-0.844
1.0000
-6.612 <.0001
-7.363 <.0001
15.740 <.0001
5.418 <.0001
1.0000
-0.351
-1.102
0.9994
-10.323 <.0001
-16.091 <.0001
-16.842 <.0001
-5.769 <.0001
-6.519 <.0001
1.0000
-0.751

P value adjustment: tukey method for comparing a family of 16 estimates

Table 8: Contrast test between factors Content, Codec and Bitrate

contrast
Longdress, C1, R1 - Longdress, C1, R2
Longdress, C1, R1 - Longdress, C1, R3
Longdress, C1, R1 - Longdress, C1, R4
Longdress, C1, R1 - Longdress, C2, R1
Longdress, C1, R1 - Longdress, C2, R2
Longdress, C1, R1 - Longdress, C2, R3
Longdress, C1, R1 - Longdress, C2, R4
Longdress, C1, R1 - Loot, C1, R1
Longdress, C1, R1 - Loot, C1, R2
Longdress, C1, R1 - Loot, C1, R3
Longdress, C1, R1 - Loot, C1, R4
Longdress, C1, R1 - Loot, C2, R1
Longdress, C1, R1 - Loot, C2, R2
Longdress, C1, R1 - Loot, C2, R3
Longdress, C1, R1 - Loot, C2, R4
Longdress, C1, R1 - Red and black, C1, R1
Longdress, C1, R1 - Red and black, C1, R2

estimate
-535.4682
-619.6446
-657.2754
636.6415
328.2036
-242.6631
-507.2836
446.7518
-224.4031
-467.3159
-535.5154
1014.3595
799.7723
343.0297
-247.0190
193.1303
-301.1995

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio
p.value
-7.605 <.0001
-8.800 <.0001
-9.335 <.0001
9.042 <.0001
0.0015
4.661
-3.446
0.1438
-7.204 <.0001
6.345 <.0001
-3.187
0.2794
-6.637 <.0001
-7.605 <.0001
14.406 <.0001
11.358 <.0001
0.0005
4.872
0.1204
-3.508
0.6257
2.743
0.0078
-4.278

22

Irene Viola et al.

Table 8 – Continued from previous page

contrast
Longdress, C1, R1 - Red and black, C1, R3
Longdress, C1, R1 - Red and black, C1, R4
Longdress, C1, R1 - Red and black, C2, R1
Longdress, C1, R1 - Red and black, C2, R2
Longdress, C1, R1 - Red and black, C2, R3
Longdress, C1, R1 - Red and black, C2, R4
Longdress, C1, R1 - Soldier, C1, R1
Longdress, C1, R1 - Soldier, C1, R2
Longdress, C1, R1 - Soldier, C1, R3
Longdress, C1, R1 - Soldier, C1, R4
Longdress, C1, R1 - Soldier, C2, R1
Longdress, C1, R1 - Soldier, C2, R2
Longdress, C1, R1 - Soldier, C2, R3
Longdress, C1, R1 - Soldier, C2, R4
Longdress, C1, R2 - Longdress, C1, R3
Longdress, C1, R2 - Longdress, C1, R4
Longdress, C1, R2 - Longdress, C2, R1
Longdress, C1, R2 - Longdress, C2, R2
Longdress, C1, R2 - Longdress, C2, R3
Longdress, C1, R2 - Longdress, C2, R4
Longdress, C1, R2 - Loot, C1, R1
Longdress, C1, R2 - Loot, C1, R2
Longdress, C1, R2 - Loot, C1, R3
Longdress, C1, R2 - Loot, C1, R4
Longdress, C1, R2 - Loot, C2, R1
Longdress, C1, R2 - Loot, C2, R2
Longdress, C1, R2 - Loot, C2, R3
Longdress, C1, R2 - Loot, C2, R4
Longdress, C1, R2 - Red and black, C1, R1
Longdress, C1, R2 - Red and black, C1, R2
Longdress, C1, R2 - Red and black, C1, R3
Longdress, C1, R2 - Red and black, C1, R4
Longdress, C1, R2 - Red and black, C2, R1
Longdress, C1, R2 - Red and black, C2, R2
Longdress, C1, R2 - Red and black, C2, R3
Longdress, C1, R2 - Red and black, C2, R4
Longdress, C1, R2 - Soldier, C1, R1
Longdress, C1, R2 - Soldier, C1, R2
Longdress, C1, R2 - Soldier, C1, R3
Longdress, C1, R2 - Soldier, C1, R4
Longdress, C1, R2 - Soldier, C2, R1
Longdress, C1, R2 - Soldier, C2, R2
Longdress, C1, R2 - Soldier, C2, R3
Longdress, C1, R2 - Soldier, C2, R4
Longdress, C1, R3 - Longdress, C1, R4
Longdress, C1, R3 - Longdress, C2, R1
Longdress, C1, R3 - Longdress, C2, R2
Longdress, C1, R3 - Longdress, C2, R3
Longdress, C1, R3 - Longdress, C2, R4
Longdress, C1, R3 - Loot, C1, R1
Longdress, C1, R3 - Loot, C1, R2

estimate
-537.7385
-565.3785
746.5923
459.9036
43.6846
-297.4928
760.3185
150.9123
-281.4026
-472.6487
1057.4472
842.4862
403.9667
-217.4087
-84.1764
-121.8072
1172.1097
863.6718
292.8051
28.1846
982.2200
311.0651
68.1523
-0.0472
1549.8277
1335.2405
878.4979
288.4492
728.5985
234.2687
-2.2703
-29.9103
1282.0605
995.3718
579.1528
237.9754
1295.7867
686.3805
254.0656
62.8195
1592.9154
1377.9544
939.4349
318.0595
-37.6308
1256.2862
947.8482
376.9815
112.3610
1066.3964
395.2415

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio
p.value
-7.637 <.0001
-8.030 <.0001
10.603 <.0001
6.532 <.0001
0.620
1.0000
0.0097
-4.225
10.798 <.0001
0.9611
2.143
-3.996
0.0235
-6.713 <.0001
15.018 <.0001
11.965 <.0001
5.737 <.0001
0.3475
-3.088
1.0000
-1.195
-1.730
0.9984
16.646 <.0001
12.266 <.0001
0.0126
4.158
0.400
1.0000
13.950 <.0001
0.0043
4.418
1.0000
0.968
-0.001
1.0000
22.011 <.0001
18.963 <.0001
12.476 <.0001
4.097
0.0161
10.348 <.0001
0.1984
3.327
1.0000
-0.032
-0.425
1.0000
18.208 <.0001
14.136 <.0001
8.225 <.0001
3.380
0.1727
18.403 <.0001
9.748 <.0001
0.0890
3.608
0.892
1.0000
22.623 <.0001
19.570 <.0001
13.342 <.0001
0.0028
4.517
-0.534
1.0000
17.842 <.0001
13.461 <.0001
5.354 <.0001
1.596
0.9996
15.145 <.0001
5.613 <.0001

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

23

Table 8 – Continued from previous page

contrast
Longdress, C1, R3 - Loot, C1, R3
Longdress, C1, R3 - Loot, C1, R4
Longdress, C1, R3 - Loot, C2, R1
Longdress, C1, R3 - Loot, C2, R2
Longdress, C1, R3 - Loot, C2, R3
Longdress, C1, R3 - Loot, C2, R4
Longdress, C1, R3 - Red and black, C1, R1
Longdress, C1, R3 - Red and black, C1, R2
Longdress, C1, R3 - Red and black, C1, R3
Longdress, C1, R3 - Red and black, C1, R4
Longdress, C1, R3 - Red and black, C2, R1
Longdress, C1, R3 - Red and black, C2, R2
Longdress, C1, R3 - Red and black, C2, R3
Longdress, C1, R3 - Red and black, C2, R4
Longdress, C1, R3 - Soldier, C1, R1
Longdress, C1, R3 - Soldier, C1, R2
Longdress, C1, R3 - Soldier, C1, R3
Longdress, C1, R3 - Soldier, C1, R4
Longdress, C1, R3 - Soldier, C2, R1
Longdress, C1, R3 - Soldier, C2, R2
Longdress, C1, R3 - Soldier, C2, R3
Longdress, C1, R3 - Soldier, C2, R4
Longdress, C1, R4 - Longdress, C2, R1
Longdress, C1, R4 - Longdress, C2, R2
Longdress, C1, R4 - Longdress, C2, R3
Longdress, C1, R4 - Longdress, C2, R4
Longdress, C1, R4 - Loot, C1, R1
Longdress, C1, R4 - Loot, C1, R2
Longdress, C1, R4 - Loot, C1, R3
Longdress, C1, R4 - Loot, C1, R4
Longdress, C1, R4 - Loot, C2, R1
Longdress, C1, R4 - Loot, C2, R2
Longdress, C1, R4 - Loot, C2, R3
Longdress, C1, R4 - Loot, C2, R4
Longdress, C1, R4 - Red and black, C1, R1
Longdress, C1, R4 - Red and black, C1, R2
Longdress, C1, R4 - Red and black, C1, R3
Longdress, C1, R4 - Red and black, C1, R4
Longdress, C1, R4 - Red and black, C2, R1
Longdress, C1, R4 - Red and black, C2, R2
Longdress, C1, R4 - Red and black, C2, R3
Longdress, C1, R4 - Red and black, C2, R4
Longdress, C1, R4 - Soldier, C1, R1
Longdress, C1, R4 - Soldier, C1, R2
Longdress, C1, R4 - Soldier, C1, R3
Longdress, C1, R4 - Soldier, C1, R4
Longdress, C1, R4 - Soldier, C2, R1
Longdress, C1, R4 - Soldier, C2, R2
Longdress, C1, R4 - Soldier, C2, R3
Longdress, C1, R4 - Soldier, C2, R4
Longdress, C2, R1 - Longdress, C2, R2

estimate
152.3287
84.1292
1634.0041
1419.4169
962.6744
372.6256
812.7749
318.4451
81.9062
54.2662
1366.2369
1079.5482
663.3292
322.1518
1379.9631
770.5569
338.2421
146.9959
1677.0918
1462.1308
1023.6113
402.2359
1293.9169
985.4790
414.6123
149.9918
1104.0272
432.8723
189.9595
121.7600
1671.6349
1457.0477
1000.3051
410.2564
850.4056
356.0759
119.5369
91.8969
1403.8677
1117.1790
700.9600
359.7826
1417.5938
808.1877
375.8728
184.6267
1714.7226
1499.7615
1061.2421
439.8667
-308.4379

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio
p.value
2.163
0.9564
1.0000
1.195
23.206 <.0001
20.159 <.0001
13.672 <.0001
0.0001
5.292
11.543 <.0001
0.0027
4.523
1.163
1.0000
1.0000
0.771
19.403 <.0001
15.332 <.0001
9.421 <.0001
0.0021
4.575
19.598 <.0001
10.944 <.0001
4.804
0.0007
0.9721
2.088
23.818 <.0001
20.765 <.0001
14.537 <.0001
5.713 <.0001
18.376 <.0001
13.996 <.0001
5.888 <.0001
0.9639
2.130
15.679 <.0001
6.148 <.0001
2.698
0.6625
0.9984
1.729
23.741 <.0001
20.693 <.0001
14.206 <.0001
5.826 <.0001
12.078 <.0001
0.0002
5.057
0.9989
1.698
1.0000
1.305
19.938 <.0001
15.866 <.0001
9.955 <.0001
0.0002
5.110
20.133 <.0001
11.478 <.0001
5.338 <.0001
0.7219
2.622
24.353 <.0001
21.300 <.0001
15.072 <.0001
6.247 <.0001
0.0051
-4.380

24

Irene Viola et al.

Table 8 – Continued from previous page

contrast
Longdress, C2, R1 - Longdress, C2, R3
Longdress, C2, R1 - Longdress, C2, R4
Longdress, C2, R1 - Loot, C1, R1
Longdress, C2, R1 - Loot, C1, R2
Longdress, C2, R1 - Loot, C1, R3
Longdress, C2, R1 - Loot, C1, R4
Longdress, C2, R1 - Loot, C2, R1
Longdress, C2, R1 - Loot, C2, R2
Longdress, C2, R1 - Loot, C2, R3
Longdress, C2, R1 - Loot, C2, R4
Longdress, C2, R1 - Red and black, C1, R1
Longdress, C2, R1 - Red and black, C1, R2
Longdress, C2, R1 - Red and black, C1, R3
Longdress, C2, R1 - Red and black, C1, R4
Longdress, C2, R1 - Red and black, C2, R1
Longdress, C2, R1 - Red and black, C2, R2
Longdress, C2, R1 - Red and black, C2, R3
Longdress, C2, R1 - Red and black, C2, R4
Longdress, C2, R1 - Soldier, C1, R1
Longdress, C2, R1 - Soldier, C1, R2
Longdress, C2, R1 - Soldier, C1, R3
Longdress, C2, R1 - Soldier, C1, R4
Longdress, C2, R1 - Soldier, C2, R1
Longdress, C2, R1 - Soldier, C2, R2
Longdress, C2, R1 - Soldier, C2, R3
Longdress, C2, R1 - Soldier, C2, R4
Longdress, C2, R2 - Longdress, C2, R3
Longdress, C2, R2 - Longdress, C2, R4
Longdress, C2, R2 - Loot, C1, R1
Longdress, C2, R2 - Loot, C1, R2
Longdress, C2, R2 - Loot, C1, R3
Longdress, C2, R2 - Loot, C1, R4
Longdress, C2, R2 - Loot, C2, R1
Longdress, C2, R2 - Loot, C2, R2
Longdress, C2, R2 - Loot, C2, R3
Longdress, C2, R2 - Loot, C2, R4
Longdress, C2, R2 - Red and black, C1, R1
Longdress, C2, R2 - Red and black, C1, R2
Longdress, C2, R2 - Red and black, C1, R3
Longdress, C2, R2 - Red and black, C1, R4
Longdress, C2, R2 - Red and black, C2, R1
Longdress, C2, R2 - Red and black, C2, R2
Longdress, C2, R2 - Red and black, C2, R3
Longdress, C2, R2 - Red and black, C2, R4
Longdress, C2, R2 - Soldier, C1, R1
Longdress, C2, R2 - Soldier, C1, R2
Longdress, C2, R2 - Soldier, C1, R3
Longdress, C2, R2 - Soldier, C1, R4
Longdress, C2, R2 - Soldier, C2, R1
Longdress, C2, R2 - Soldier, C2, R2
Longdress, C2, R2 - Soldier, C2, R3

estimate
-879.3046
-1143.9251
-189.8897
-861.0446
-1103.9574
-1172.1569
377.7179
163.1308
-293.6118
-883.6605
-443.5113
-937.8410
-1174.3800
-1202.0200
109.9508
-176.7379
-592.9569
-934.1344
123.6769
-485.7292
-918.0441
-1109.2903
420.8056
205.8446
-232.6749
-854.0503
-570.8667
-835.4872
118.5482
-552.6067
-795.5195
-863.7190
686.1559
471.5687
14.8262
-575.2226
-135.0733
-629.4031
-865.9421
-893.5821
418.3887
131.7000
-284.5190
-625.6964
432.1149
-177.2913
-609.6062
-800.8523
729.2436
514.2826
75.7631

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio

p.value
-12.488 <.0001
-16.246 <.0001
-2.697
0.6633
-12.229 <.0001
-15.678 <.0001
-16.647 <.0001
5.364 <.0001
0.9058
2.317
-4.170
0.0120
-12.550 <.0001
-6.299 <.0001
-13.319 <.0001
-16.679 <.0001
-17.071 <.0001
0.9998
1.562
-2.510
0.8014
-8.421 <.0001
-13.267 <.0001
1.756
0.9979
-6.898 <.0001
-13.038 <.0001
-15.754 <.0001
5.976 <.0001
0.4754
2.923
-3.304
0.2103
-12.129 <.0001
-8.107 <.0001
-11.866 <.0001
1.684
0.9990
-7.848 <.0001
-11.298 <.0001
-12.267 <.0001
9.745 <.0001
6.697 <.0001
0.211
1.0000
-8.169 <.0001
-1.918
0.9915
-8.939 <.0001
-12.298 <.0001
-12.691 <.0001
5.942 <.0001
0.9942
1.870
-4.041
0.0199
-8.886 <.0001
6.137 <.0001
0.7962
-2.518
-8.658 <.0001
-11.374 <.0001
10.357 <.0001
7.304 <.0001
1.0000
1.076

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

25

Table 8 – Continued from previous page

contrast
Longdress, C2, R2 - Soldier, C2, R4
Longdress, C2, R3 - Longdress, C2, R4
Longdress, C2, R3 - Loot, C1, R1
Longdress, C2, R3 - Loot, C1, R2
Longdress, C2, R3 - Loot, C1, R3
Longdress, C2, R3 - Loot, C1, R4
Longdress, C2, R3 - Loot, C2, R1
Longdress, C2, R3 - Loot, C2, R2
Longdress, C2, R3 - Loot, C2, R3
Longdress, C2, R3 - Loot, C2, R4
Longdress, C2, R3 - Red and black, C1, R1
Longdress, C2, R3 - Red and black, C1, R2
Longdress, C2, R3 - Red and black, C1, R3
Longdress, C2, R3 - Red and black, C1, R4
Longdress, C2, R3 - Red and black, C2, R1
Longdress, C2, R3 - Red and black, C2, R2
Longdress, C2, R3 - Red and black, C2, R3
Longdress, C2, R3 - Red and black, C2, R4
Longdress, C2, R3 - Soldier, C1, R1
Longdress, C2, R3 - Soldier, C1, R2
Longdress, C2, R3 - Soldier, C1, R3
Longdress, C2, R3 - Soldier, C1, R4
Longdress, C2, R3 - Soldier, C2, R1
Longdress, C2, R3 - Soldier, C2, R2
Longdress, C2, R3 - Soldier, C2, R3
Longdress, C2, R3 - Soldier, C2, R4
Longdress, C2, R4 - Loot, C1, R1
Longdress, C2, R4 - Loot, C1, R2
Longdress, C2, R4 - Loot, C1, R3
Longdress, C2, R4 - Loot, C1, R4
Longdress, C2, R4 - Loot, C2, R1
Longdress, C2, R4 - Loot, C2, R2
Longdress, C2, R4 - Loot, C2, R3
Longdress, C2, R4 - Loot, C2, R4
Longdress, C2, R4 - Red and black, C1, R1
Longdress, C2, R4 - Red and black, C1, R2
Longdress, C2, R4 - Red and black, C1, R3
Longdress, C2, R4 - Red and black, C1, R4
Longdress, C2, R4 - Red and black, C2, R1
Longdress, C2, R4 - Red and black, C2, R2
Longdress, C2, R4 - Red and black, C2, R3
Longdress, C2, R4 - Red and black, C2, R4
Longdress, C2, R4 - Soldier, C1, R1
Longdress, C2, R4 - Soldier, C1, R2
Longdress, C2, R4 - Soldier, C1, R3
Longdress, C2, R4 - Soldier, C1, R4
Longdress, C2, R4 - Soldier, C2, R1
Longdress, C2, R4 - Soldier, C2, R2
Longdress, C2, R4 - Soldier, C2, R3
Longdress, C2, R4 - Soldier, C2, R4
Loot, C1, R1 - Loot, C1, R2

estimate
-545.6123
-264.6205
689.4149
18.2600
-224.6528
-292.8523
1257.0226
1042.4354
585.6928
-4.3559
435.7933
-58.5364
-295.0754
-322.7154
989.2554
702.5667
286.3477
-54.8297
1002.9815
393.5754
-38.7395
-229.9856
1300.1103
1085.1492
646.6297
25.2544
954.0354
282.8805
39.9677
-28.2318
1521.6431
1307.0559
850.3133
260.2646
700.4138
206.0841
-30.4549
-58.0949
1253.8759
967.1872
550.9682
209.7908
1267.6021
658.1959
225.8810
34.6349
1564.7308
1349.7697
911.2503
289.8749
-671.1549

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio
p.value
-7.749 <.0001
0.0548
-3.758
9.791 <.0001
1.0000
0.259
-3.191
0.2771
0.0126
-4.159
17.852 <.0001
14.805 <.0001
8.318 <.0001
1.0000
-0.062
6.189 <.0001
1.0000
-0.831
-4.191
0.0111
0.0021
-4.583
14.049 <.0001
9.978 <.0001
4.067
0.0180
1.0000
-0.779
14.244 <.0001
5.590 <.0001
-0.550
1.0000
0.2313
-3.266
18.464 <.0001
15.411 <.0001
9.183 <.0001
1.0000
0.359
13.549 <.0001
0.0217
4.017
0.568
1.0000
1.0000
-0.401
21.610 <.0001
18.563 <.0001
12.076 <.0001
0.0673
3.696
9.947 <.0001
0.4726
2.927
1.0000
-0.433
1.0000
-0.825
17.808 <.0001
13.736 <.0001
7.825 <.0001
0.4300
2.979
18.003 <.0001
9.348 <.0001
0.2661
3.208
1.0000
0.492
22.222 <.0001
19.170 <.0001
12.942 <.0001
0.0149
4.117
-9.532 <.0001

26

Irene Viola et al.

Table 8 – Continued from previous page

contrast
Loot, C1, R1 - Loot, C1, R3
Loot, C1, R1 - Loot, C1, R4
Loot, C1, R1 - Loot, C2, R1
Loot, C1, R1 - Loot, C2, R2
Loot, C1, R1 - Loot, C2, R3
Loot, C1, R1 - Loot, C2, R4
Loot, C1, R1 - Red and black, C1, R1
Loot, C1, R1 - Red and black, C1, R2
Loot, C1, R1 - Red and black, C1, R3
Loot, C1, R1 - Red and black, C1, R4
Loot, C1, R1 - Red and black, C2, R1
Loot, C1, R1 - Red and black, C2, R2
Loot, C1, R1 - Red and black, C2, R3
Loot, C1, R1 - Red and black, C2, R4
Loot, C1, R1 - Soldier, C1, R1
Loot, C1, R1 - Soldier, C1, R2
Loot, C1, R1 - Soldier, C1, R3
Loot, C1, R1 - Soldier, C1, R4
Loot, C1, R1 - Soldier, C2, R1
Loot, C1, R1 - Soldier, C2, R2
Loot, C1, R1 - Soldier, C2, R3
Loot, C1, R1 - Soldier, C2, R4
Loot, C1, R2 - Loot, C1, R3
Loot, C1, R2 - Loot, C1, R4
Loot, C1, R2 - Loot, C2, R1
Loot, C1, R2 - Loot, C2, R2
Loot, C1, R2 - Loot, C2, R3
Loot, C1, R2 - Loot, C2, R4
Loot, C1, R2 - Red and black, C1, R1
Loot, C1, R2 - Red and black, C1, R2
Loot, C1, R2 - Red and black, C1, R3
Loot, C1, R2 - Red and black, C1, R4
Loot, C1, R2 - Red and black, C2, R1
Loot, C1, R2 - Red and black, C2, R2
Loot, C1, R2 - Red and black, C2, R3
Loot, C1, R2 - Red and black, C2, R4
Loot, C1, R2 - Soldier, C1, R1
Loot, C1, R2 - Soldier, C1, R2
Loot, C1, R2 - Soldier, C1, R3
Loot, C1, R2 - Soldier, C1, R4
Loot, C1, R2 - Soldier, C2, R1
Loot, C1, R2 - Soldier, C2, R2
Loot, C1, R2 - Soldier, C2, R3
Loot, C1, R2 - Soldier, C2, R4
Loot, C1, R3 - Loot, C1, R4
Loot, C1, R3 - Loot, C2, R1
Loot, C1, R3 - Loot, C2, R2
Loot, C1, R3 - Loot, C2, R3
Loot, C1, R3 - Loot, C2, R4
Loot, C1, R3 - Red and black, C1, R1
Loot, C1, R3 - Red and black, C1, R2

estimate
-914.0677
-982.2672
567.6077
353.0205
-103.7221
-693.7708
-253.6215
-747.9513
-984.4903
-1012.1303
299.8405
13.1518
-403.0672
-744.2446
313.5667
-295.8395
-728.1544
-919.4005
610.6954
395.7344
-42.7851
-664.1605
-242.9128
-311.1123
1238.7626
1024.1754
567.4328
-22.6159
417.5333
-76.7964
-313.3354
-340.9754
970.9954
684.3067
268.0877
-73.0897
984.7215
375.3154
-56.9995
-248.2456
1281.8503
1066.8892
628.3697
6.9944
-68.1995
1481.6754
1267.0882
810.3456
220.2969
660.4462
166.1164

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio

p.value
-12.982 <.0001
-13.950 <.0001
8.061 <.0001
0.0003
5.014
-1.473
0.9999
-9.853 <.0001
-3.602
0.0908
-10.622 <.0001
-13.982 <.0001
-14.374 <.0001
0.0084
4.258
0.187
1.0000
-5.724 <.0001
-10.570 <.0001
0.0037
4.453
-4.202
0.0106
-10.341 <.0001
-13.057 <.0001
8.673 <.0001
5.620 <.0001
-0.608
1.0000
-9.432 <.0001
0.1424
-3.450
-4.418
0.0043
17.593 <.0001
14.545 <.0001
8.059 <.0001
-0.321
1.0000
5.930 <.0001
1.0000
-1.091
0.0037
-4.450
-4.843
0.0006
13.790 <.0001
9.719 <.0001
0.0464
3.807
-1.038
1.0000
13.985 <.0001
0.0001
5.330
1.0000
-0.810
-3.526
0.1144
18.205 <.0001
15.152 <.0001
8.924 <.0001
1.0000
0.099
-0.969
1.0000
21.043 <.0001
17.995 <.0001
11.509 <.0001
3.129
0.3184
9.380 <.0001
0.8868
2.359

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

27

Table 8 – Continued from previous page

contrast
Loot, C1, R3 - Red and black, C1, R3
Loot, C1, R3 - Red and black, C1, R4
Loot, C1, R3 - Red and black, C2, R1
Loot, C1, R3 - Red and black, C2, R2
Loot, C1, R3 - Red and black, C2, R3
Loot, C1, R3 - Red and black, C2, R4
Loot, C1, R3 - Soldier, C1, R1
Loot, C1, R3 - Soldier, C1, R2
Loot, C1, R3 - Soldier, C1, R3
Loot, C1, R3 - Soldier, C1, R4
Loot, C1, R3 - Soldier, C2, R1
Loot, C1, R3 - Soldier, C2, R2
Loot, C1, R3 - Soldier, C2, R3
Loot, C1, R3 - Soldier, C2, R4
Loot, C1, R4 - Loot, C2, R1
Loot, C1, R4 - Loot, C2, R2
Loot, C1, R4 - Loot, C2, R3
Loot, C1, R4 - Loot, C2, R4
Loot, C1, R4 - Red and black, C1, R1
Loot, C1, R4 - Red and black, C1, R2
Loot, C1, R4 - Red and black, C1, R3
Loot, C1, R4 - Red and black, C1, R4
Loot, C1, R4 - Red and black, C2, R1
Loot, C1, R4 - Red and black, C2, R2
Loot, C1, R4 - Red and black, C2, R3
Loot, C1, R4 - Red and black, C2, R4
Loot, C1, R4 - Soldier, C1, R1
Loot, C1, R4 - Soldier, C1, R2
Loot, C1, R4 - Soldier, C1, R3
Loot, C1, R4 - Soldier, C1, R4
Loot, C1, R4 - Soldier, C2, R1
Loot, C1, R4 - Soldier, C2, R2
Loot, C1, R4 - Soldier, C2, R3
Loot, C1, R4 - Soldier, C2, R4
Loot, C2, R1 - Loot, C2, R2
Loot, C2, R1 - Loot, C2, R3
Loot, C2, R1 - Loot, C2, R4
Loot, C2, R1 - Red and black, C1, R1
Loot, C2, R1 - Red and black, C1, R2
Loot, C2, R1 - Red and black, C1, R3
Loot, C2, R1 - Red and black, C1, R4
Loot, C2, R1 - Red and black, C2, R1
Loot, C2, R1 - Red and black, C2, R2
Loot, C2, R1 - Red and black, C2, R3
Loot, C2, R1 - Red and black, C2, R4
Loot, C2, R1 - Soldier, C1, R1
Loot, C2, R1 - Soldier, C1, R2
Loot, C2, R1 - Soldier, C1, R3
Loot, C2, R1 - Soldier, C1, R4
Loot, C2, R1 - Soldier, C2, R1
Loot, C2, R1 - Soldier, C2, R2

estimate
-70.4226
-98.0626
1213.9082
927.2195
511.0005
169.8231
1227.6344
618.2282
185.9133
-5.3328
1524.7631
1309.8021
871.2826
249.9072
1549.8749
1335.2877
878.5451
288.4964
728.6456
234.3159
-2.2231
-29.8631
1282.1077
995.4190
579.2000
238.0226
1295.8338
686.4277
254.1128
62.8667
1592.9626
1378.0015
939.4821
318.1067
-214.5872
-671.3297
-1261.3785
-821.2292
-1315.5590
-1552.0979
-1579.7379
-267.7672
-554.4559
-970.6749
-1311.8523
-254.0410
-863.4472
-1295.7621
-1487.0082
43.0877
-171.8733

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio
p.value
-1.000
1.0000
1.0000
-1.393
17.240 <.0001
13.168 <.0001
7.257 <.0001
0.8601
2.412
17.435 <.0001
8.780 <.0001
2.640
0.7079
1.0000
-0.076
21.655 <.0001
18.602 <.0001
12.374 <.0001
0.1066
3.549
22.011 <.0001
18.964 <.0001
12.477 <.0001
0.0160
4.097
10.348 <.0001
0.1981
3.328
-0.032
1.0000
1.0000
-0.424
18.209 <.0001
14.137 <.0001
8.226 <.0001
0.1724
3.380
18.404 <.0001
9.749 <.0001
3.609
0.0888
1.0000
0.893
22.623 <.0001
19.570 <.0001
13.343 <.0001
0.0028
4.518
-3.048
0.3771
-9.534 <.0001
-17.914 <.0001
-11.663 <.0001
-18.684 <.0001
-22.043 <.0001
-22.436 <.0001
0.0471
-3.803
-7.874 <.0001
-13.786 <.0001
-18.631 <.0001
0.0891
-3.608
-12.263 <.0001
-18.402 <.0001
-21.119 <.0001
1.0000
0.8439

0.612
-2.441

28

Irene Viola et al.

Table 8 – Continued from previous page

contrast
Loot, C2, R1 - Soldier, C2, R3
Loot, C2, R1 - Soldier, C2, R4
Loot, C2, R2 - Loot, C2, R3
Loot, C2, R2 - Loot, C2, R4
Loot, C2, R2 - Red and black, C1, R1
Loot, C2, R2 - Red and black, C1, R2
Loot, C2, R2 - Red and black, C1, R3
Loot, C2, R2 - Red and black, C1, R4
Loot, C2, R2 - Red and black, C2, R1
Loot, C2, R2 - Red and black, C2, R2
Loot, C2, R2 - Red and black, C2, R3
Loot, C2, R2 - Red and black, C2, R4
Loot, C2, R2 - Soldier, C1, R1
Loot, C2, R2 - Soldier, C1, R2
Loot, C2, R2 - Soldier, C1, R3
Loot, C2, R2 - Soldier, C1, R4
Loot, C2, R2 - Soldier, C2, R1
Loot, C2, R2 - Soldier, C2, R2
Loot, C2, R2 - Soldier, C2, R3
Loot, C2, R2 - Soldier, C2, R4
Loot, C2, R3 - Loot, C2, R4
Loot, C2, R3 - Red and black, C1, R1
Loot, C2, R3 - Red and black, C1, R2
Loot, C2, R3 - Red and black, C1, R3
Loot, C2, R3 - Red and black, C1, R4
Loot, C2, R3 - Red and black, C2, R1
Loot, C2, R3 - Red and black, C2, R2
Loot, C2, R3 - Red and black, C2, R3
Loot, C2, R3 - Red and black, C2, R4
Loot, C2, R3 - Soldier, C1, R1
Loot, C2, R3 - Soldier, C1, R2
Loot, C2, R3 - Soldier, C1, R3
Loot, C2, R3 - Soldier, C1, R4
Loot, C2, R3 - Soldier, C2, R1
Loot, C2, R3 - Soldier, C2, R2
Loot, C2, R3 - Soldier, C2, R3
Loot, C2, R3 - Soldier, C2, R4
Loot, C2, R4 - Red and black, C1, R1
Loot, C2, R4 - Red and black, C1, R2
Loot, C2, R4 - Red and black, C1, R3
Loot, C2, R4 - Red and black, C1, R4
Loot, C2, R4 - Red and black, C2, R1
Loot, C2, R4 - Red and black, C2, R2
Loot, C2, R4 - Red and black, C2, R3
Loot, C2, R4 - Red and black, C2, R4
Loot, C2, R4 - Soldier, C1, R1
Loot, C2, R4 - Soldier, C1, R2
Loot, C2, R4 - Soldier, C1, R3
Loot, C2, R4 - Soldier, C1, R4
Loot, C2, R4 - Soldier, C2, R1
Loot, C2, R4 - Soldier, C2, R2

estimate
-610.3928
-1231.7682
-456.7426
-1046.7913
-606.6421
-1100.9718
-1337.5108
-1365.1508
-53.1800
-339.8687
-756.0877
-1097.2651
-39.4538
-648.8600
-1081.1749
-1272.4210
257.6749
42.7138
-395.8056
-1017.1810
-590.0487
-149.8995
-644.2292
-880.7682
-908.4082
403.5626
116.8738
-299.3451
-640.5226
417.2887
-192.1174
-624.4323
-815.6785
714.4174
499.4564
60.9369
-560.4385
440.1492
-54.1805
-290.7195
-318.3595
993.6113
706.9226
290.7036
-50.4738
1007.3374
397.9313
-34.3836
-225.6297
1304.4662
1089.5051

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio
p.value
-8.669 <.0001
-17.494 <.0001
-6.487 <.0001
-14.867 <.0001
-8.616 <.0001
-15.636 <.0001
-18.995 <.0001
-19.388 <.0001
-0.755
1.0000
0.0007
-4.827
-10.738 <.0001
-15.583 <.0001
-0.560
1.0000
-9.215 <.0001
-15.355 <.0001
-18.071 <.0001
3.660
0.0757
1.0000
0.607
-5.621 <.0001
-14.446 <.0001
-8.380 <.0001
0.9642
-2.129
-9.149 <.0001
-12.509 <.0001
-12.901 <.0001
5.731 <.0001
0.9992
1.660
-4.251
0.0087
-9.097 <.0001
5.926 <.0001
-2.728
0.6376
-8.868 <.0001
-11.584 <.0001
10.146 <.0001
7.093 <.0001
0.865
1.0000
-7.959 <.0001
6.251 <.0001
1.0000
-0.769
0.0142
-4.129
-4.521
0.0027
14.111 <.0001
10.040 <.0001
0.0142
4.129
-0.717
1.0000
14.306 <.0001
5.651 <.0001
1.0000
-0.488
-3.204
0.2684
18.526 <.0001
15.473 <.0001

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

29

Table 8 – Continued from previous page

contrast
Loot, C2, R4 - Soldier, C2, R3
Loot, C2, R4 - Soldier, C2, R4
Red and black, C1, R1 - Red and black, C1, R2
Red and black, C1, R1 - Red and black, C1, R3
Red and black, C1, R1 - Red and black, C1, R4
Red and black, C1, R1 - Red and black, C2, R1
Red and black, C1, R1 - Red and black, C2, R2
Red and black, C1, R1 - Red and black, C2, R3
Red and black, C1, R1 - Red and black, C2, R4
Red and black, C1, R1 - Soldier, C1, R1
Red and black, C1, R1 - Soldier, C1, R2
Red and black, C1, R1 - Soldier, C1, R3
Red and black, C1, R1 - Soldier, C1, R4
Red and black, C1, R1 - Soldier, C2, R1
Red and black, C1, R1 - Soldier, C2, R2
Red and black, C1, R1 - Soldier, C2, R3
Red and black, C1, R1 - Soldier, C2, R4
Red and black, C1, R2 - Red and black, C1, R3
Red and black, C1, R2 - Red and black, C1, R4
Red and black, C1, R2 - Red and black, C2, R1
Red and black, C1, R2 - Red and black, C2, R2
Red and black, C1, R2 - Red and black, C2, R3
Red and black, C1, R2 - Red and black, C2, R4
Red and black, C1, R2 - Soldier, C1, R1
Red and black, C1, R2 - Soldier, C1, R2
Red and black, C1, R2 - Soldier, C1, R3
Red and black, C1, R2 - Soldier, C1, R4
Red and black, C1, R2 - Soldier, C2, R1
Red and black, C1, R2 - Soldier, C2, R2
Red and black, C1, R2 - Soldier, C2, R3
Red and black, C1, R2 - Soldier, C2, R4
Red and black, C1, R3 - Red and black, C1, R4
Red and black, C1, R3 - Red and black, C2, R1
Red and black, C1, R3 - Red and black, C2, R2
Red and black, C1, R3 - Red and black, C2, R3
Red and black, C1, R3 - Red and black, C2, R4
Red and black, C1, R3 - Soldier, C1, R1
Red and black, C1, R3 - Soldier, C1, R2
Red and black, C1, R3 - Soldier, C1, R3
Red and black, C1, R3 - Soldier, C1, R4
Red and black, C1, R3 - Soldier, C2, R1
Red and black, C1, R3 - Soldier, C2, R2
Red and black, C1, R3 - Soldier, C2, R3
Red and black, C1, R3 - Soldier, C2, R4
Red and black, C1, R4 - Red and black, C2, R1
Red and black, C1, R4 - Red and black, C2, R2
Red and black, C1, R4 - Red and black, C2, R3
Red and black, C1, R4 - Red and black, C2, R4
Red and black, C1, R4 - Soldier, C1, R1
Red and black, C1, R4 - Soldier, C1, R2
Red and black, C1, R4 - Soldier, C1, R3

estimate
650.9856
29.6103
-494.3297
-730.8687
-758.5087
553.4621
266.7733
-149.4456
-490.6231
567.1882
-42.2179
-474.5328
-665.7790
864.3169
649.3559
210.8364
-410.5390
-236.5390
-264.1790
1047.7918
761.1031
344.8841
3.7067
1061.5179
452.1118
19.7969
-171.4492
1358.6467
1143.6856
705.1662
83.7908
-27.6400
1284.3308
997.6421
581.4231
240.2456
1298.0569
688.6508
256.3359
65.0897
1595.1856
1380.2246
941.7051
320.3297
1311.9708
1025.2821
609.0631
267.8856
1325.6969
716.2908
283.9759

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio
p.value
9.245 <.0001
1.0000
0.421
-7.021 <.0001
-10.380 <.0001
-10.772 <.0001
7.860 <.0001
0.0494
3.789
-2.122
0.9655
-6.968 <.0001
8.055 <.0001
-0.600
1.0000
-6.739 <.0001
-9.455 <.0001
12.275 <.0001
9.222 <.0001
2.994
0.4183
-5.831 <.0001
0.1824
-3.359
-3.752
0.0560
14.881 <.0001
10.809 <.0001
0.0005
4.898
0.053
1.0000
15.076 <.0001
6.421 <.0001
1.0000
0.281
-2.435
0.8474
19.296 <.0001
16.243 <.0001
10.015 <.0001
1.0000
1.190
-0.393
1.0000
18.240 <.0001
14.169 <.0001
8.257 <.0001
3.412
0.1582
18.435 <.0001
9.780 <.0001
0.0804
3.640
0.924
1.0000
22.655 <.0001
19.602 <.0001
13.374 <.0001
4.549
0.0024
18.633 <.0001
14.561 <.0001
8.650 <.0001
3.805
0.0468
18.828 <.0001
10.173 <.0001
0.0205
4.033

30

Irene Viola et al.

Table 8 – Continued from previous page

contrast
Red and black, C1, R4 - Soldier, C1, R4
Red and black, C1, R4 - Soldier, C2, R1
Red and black, C1, R4 - Soldier, C2, R2
Red and black, C1, R4 - Soldier, C2, R3
Red and black, C1, R4 - Soldier, C2, R4
Red and black, C2, R1 - Red and black, C2, R2
Red and black, C2, R1 - Red and black, C2, R3
Red and black, C2, R1 - Red and black, C2, R4
Red and black, C2, R1 - Soldier, C1, R1
Red and black, C2, R1 - Soldier, C1, R2
Red and black, C2, R1 - Soldier, C1, R3
Red and black, C2, R1 - Soldier, C1, R4
Red and black, C2, R1 - Soldier, C2, R1
Red and black, C2, R1 - Soldier, C2, R2
Red and black, C2, R1 - Soldier, C2, R3
Red and black, C2, R1 - Soldier, C2, R4
Red and black, C2, R2 - Red and black, C2, R3
Red and black, C2, R2 - Red and black, C2, R4
Red and black, C2, R2 - Soldier, C1, R1
Red and black, C2, R2 - Soldier, C1, R2
Red and black, C2, R2 - Soldier, C1, R3
Red and black, C2, R2 - Soldier, C1, R4
Red and black, C2, R2 - Soldier, C2, R1
Red and black, C2, R2 - Soldier, C2, R2
Red and black, C2, R2 - Soldier, C2, R3
Red and black, C2, R2 - Soldier, C2, R4
Red and black, C2, R3 - Red and black, C2, R4
Red and black, C2, R3 - Soldier, C1, R1
Red and black, C2, R3 - Soldier, C1, R2
Red and black, C2, R3 - Soldier, C1, R3
Red and black, C2, R3 - Soldier, C1, R4
Red and black, C2, R3 - Soldier, C2, R1
Red and black, C2, R3 - Soldier, C2, R2
Red and black, C2, R3 - Soldier, C2, R3
Red and black, C2, R3 - Soldier, C2, R4
Red and black, C2, R4 - Soldier, C1, R1
Red and black, C2, R4 - Soldier, C1, R2
Red and black, C2, R4 - Soldier, C1, R3
Red and black, C2, R4 - Soldier, C1, R4
Red and black, C2, R4 - Soldier, C2, R1
Red and black, C2, R4 - Soldier, C2, R2
Red and black, C2, R4 - Soldier, C2, R3
Red and black, C2, R4 - Soldier, C2, R4
Soldier, C1, R1 - Soldier, C1, R2
Soldier, C1, R1 - Soldier, C1, R3
Soldier, C1, R1 - Soldier, C1, R4
Soldier, C1, R1 - Soldier, C2, R1
Soldier, C1, R1 - Soldier, C2, R2
Soldier, C1, R1 - Soldier, C2, R3
Soldier, C1, R1 - Soldier, C2, R4
Soldier, C1, R2 - Soldier, C1, R3

estimate
92.7297
1622.8256
1407.8646
969.3451
347.9697
-286.6887
-702.9077
-1044.0851
13.7262
-595.6800
-1027.9949
-1219.2410
310.8549
95.8938
-342.6256
-964.0010
-416.2190
-757.3964
300.4149
-308.9913
-741.3062
-932.5523
597.5436
382.5826
-55.9369
-677.3123
-341.1774
716.6338
107.2277
-325.0872
-516.3333
1013.7626
798.8015
360.2821
-261.0933
1057.8113
448.4051
16.0903
-175.1559
1354.9400
1139.9790
701.4595
80.0841
-609.4062
-1041.7210
-1232.9672
297.1287
82.1677
-356.3518
-977.7272
-432.3149

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

p.value
t.ratio
1.317
1.0000
23.047 <.0001
19.995 <.0001
13.767 <.0001
4.942
0.0004
0.0177
-4.072
-9.983 <.0001
-14.828 <.0001
0.195
1.0000
-8.460 <.0001
-14.600 <.0001
-17.316 <.0001
0.0044
4.415
1.0000
1.362
-4.866
0.0006
-13.691 <.0001
-5.911 <.0001
-10.757 <.0001
0.0081
4.267
-4.388
0.0049
-10.528 <.0001
-13.244 <.0001
8.486 <.0001
5.433 <.0001
-0.794
1.0000
-9.619 <.0001
-4.845
0.0006
10.178 <.0001
1.523
0.9999
0.0018
-4.617
-7.333 <.0001
14.398 <.0001
11.345 <.0001
0.0002
5.117
-3.708
0.0647
15.023 <.0001
6.368 <.0001
1.0000
0.229
-2.488
0.8158
19.243 <.0001
16.190 <.0001
9.962 <.0001
1.137
1.0000
-8.655 <.0001
-14.795 <.0001
-17.511 <.0001
0.0099
4.220
1.0000
1.167
-5.061
0.0002
-13.886 <.0001
-6.140 <.0001

On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans

31

Table 8 – Continued from previous page

contrast
Soldier, C1, R2 - Soldier, C1, R4
Soldier, C1, R2 - Soldier, C2, R1
Soldier, C1, R2 - Soldier, C2, R2
Soldier, C1, R2 - Soldier, C2, R3
Soldier, C1, R2 - Soldier, C2, R4
Soldier, C1, R3 - Soldier, C1, R4
Soldier, C1, R3 - Soldier, C2, R1
Soldier, C1, R3 - Soldier, C2, R2
Soldier, C1, R3 - Soldier, C2, R3
Soldier, C1, R3 - Soldier, C2, R4
Soldier, C1, R4 - Soldier, C2, R1
Soldier, C1, R4 - Soldier, C2, R2
Soldier, C1, R4 - Soldier, C2, R3
Soldier, C1, R4 - Soldier, C2, R4
Soldier, C2, R1 - Soldier, C2, R2
Soldier, C2, R1 - Soldier, C2, R3
Soldier, C2, R1 - Soldier, C2, R4
Soldier, C2, R2 - Soldier, C2, R3
Soldier, C2, R2 - Soldier, C2, R4
Soldier, C2, R3 - Soldier, C2, R4
Results are averaged over the levels of: DoF

Degrees-of-freedom method: kenward-roger

estimate
-623.5610
906.5349
691.5738
253.0544
-368.3210
-191.2462
1338.8497
1123.8887
685.3692
63.9938
1530.0959
1315.1349
876.6154
255.2400
-214.9610
-653.4805
-1274.8559
-438.5195
-1059.8949
-621.3754

SE
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123
70.4123

df
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263
2263

t.ratio
p.value
-8.856 <.0001
12.875 <.0001
9.822 <.0001
0.0931
3.594
-5.231
0.0001
0.6477
-2.716
19.014 <.0001
15.962 <.0001
9.734 <.0001
1.0000
0.909
21.731 <.0001
18.678 <.0001
12.450 <.0001
0.0845
3.625
-3.053
0.3731
-9.281 <.0001
-18.106 <.0001
-6.228 <.0001
-15.053 <.0001
-8.825 <.0001

P value adjustment: tukey method for comparing a family of 32 estimates

Table 9: Contrast test between factors DoF and Content

contrast
C1, R1 - C1, R2
C1, R1 - C1, R3
C1, R1 - C1, R4
C1, R1 - C2, R1
C1, R1 - C2, R2
C1, R1 - C2, R3
C1, R1 - C2, R4
C1, R2 - C1, R3
C1, R2 - C1, R4
C1, R2 - C2, R1
C1, R2 - C2, R2
C1, R2 - C2, R3
C1, R2 - C2, R4
C1, R3 - C1, R4
C1, R3 - C2, R1
C1, R3 - C2, R2
C1, R3 - C2, R3
C1, R3 - C2, R4
C1, R4 - C2, R1
C1, R4 - C2, R2
C1, R4 - C2, R3
C1, R4 - C2, R4

estimate
-263.7783
-803.3192
-962.2388
281.7562
-140.8103
-654.9223
-925.6729
-539.5409
-698.4605
545.5345
122.9681
-391.1440
-661.8946
-158.9196
1085.0754
662.5090
148.3969
-122.3537
1243.9950
821.4286
307.3165
36.5659

SE
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258

df
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294
2294

p.value
t.ratio
-6.477 <.0001
-19.725 <.0001
-23.627 <.0001
6.918 <.0001
-3.458
0.0129
-16.081 <.0001
-22.729 <.0001
-13.248 <.0001
-17.150 <.0001
13.395 <.0001
3.019
0.0521
-9.604 <.0001
-16.252 <.0001
-3.902
0.0025
26.643 <.0001
16.268 <.0001
0.0067
3.644
-3.004
0.0544
30.546 <.0001
20.170 <.0001
7.546 <.0001
0.9863
0.898

32

Irene Viola et al.

Table 9 – Continued from previous page

contrast
C2, R1 - C2, R2
C2, R1 - C2, R3
C2, R1 - C2, R4
C2, R2 - C2, R3
C2, R2 - C2, R4
C2, R3 - C2, R4
Results are averaged over the levels of: DoF, Contents

estimate
-422.5664
-936.6785
-1207.4291
-514.1121
-784.8627
-270.7506

SE
40.7258
40.7258
40.7258
40.7258
40.7258
40.7258

df
2294
2294
2294
2294
2294
2294

t.ratio

p.value
-10.376 <.0001
-23.000 <.0001
-29.648 <.0001
-12.624 <.0001
-19.272 <.0001
-6.648 <.0001

Degrees-of-freedom method: kenward-roger

P value adjustment: tukey method for comparing a family of 8 estimates

