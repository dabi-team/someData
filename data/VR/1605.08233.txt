6
1
0
2

y
a
M
7
2

]

G
L
.
s
c
[

2
v
3
3
2
8
0
.
5
0
6
1
:
v
i
X
r
a

Stochastic Variance Reduced Riemannian Eigensolver

Zhiqiang Xu
Institute for Infocomm Research
A*STAR, Singapore
zhiqiangxu2001@gmail.com

Yiping Ke
Nanyang Technological University
Singapore
ypke@ntu.edu.sg

Abstract

We study the stochastic Riemannian gradient algorithm for matrix eigen-
decomposition. The state-of-the-art stochastic Riemannian algorithm requires the
learning rate to decay to zero and thus suffers from slow convergence and sub-
optimal solutions. In this paper, we address this issue by deploying the variance
reduction (VR) technique of stochastic gradient descent (SGD). The technique
was originally developed to solve convex problems in the Euclidean space. We
generalize it to Riemannian manifolds and realize it to solve the non-convex eigen-
decomposition problem. We are the ﬁrst to propose and analyze the generalization
of SVRG to Riemannian manifolds. Speciﬁcally, we propose the general variance
reduction form, SVRRG, in the framework of the stochastic Riemannian gradient
optimization. It’s then specialized to the problem with eigensolvers and induces
the SVRRG-EIGS algorithm. We provide a novel and elegant theoretical analysis
on this algorithm. The theory shows that a ﬁxed learning rate can be used in the
Riemannian setting with an exponential global convergence rate guaranteed. The
theoretical results make a signiﬁcant improvement over existing studies, with the
effectiveness empirically veriﬁed.

1 Introduction

Matrix eigen-decomposition is among the core and long-standing topics in numerical computing
[29]. It plays fundamental roles in various scientiﬁc and engineering computing problems (such as
numerical computation [9, 22] and structural analysis [25]) as well as machine learning tasks (such
as kernel approximation [6], dimensionality reduction [14] and spectral clustering [20]). Thus far,
there hasn’t been many algorithms proposed for this problem. Pioneering ones include the method
of power iteration [9] and the (block) Lanczos algorithm [21], while randomized SVD [10] and
online learning of eigenvectors [8] are recently proposed. The problem can also be expressed as a
quadratically constrained quadratic program (QCQP), and thus can be approached by various opti-
mization methods, such as trace penalty minimization [27] and Riemannian optimization algorithms
[1, 7, 28]. Most of these algorithms perform the batch learning, i.e., using the entire dataset to
perform the update at each step. This could be well addressed by designing appropriate stochastic
algorithms. However, the state-of-the-art stochastic algorithm DSRG-EIGS [2] requires the learning
rate to repeatedly decay till vanishing in order to guarantee convergence, which results in a slow
convergence of sub-linear rate.

We propose a new stochastic Riemannian algorithm that makes a signiﬁcant breakthrough theoret-
ically. It improves the state-of-the-art sub-linear convergence rate to an exponential convergence
one. The algorithm is inspired by the stochastic variance reduced gradient (SVRG) optimization
[12], which was originally developed to solve convex problems in the Euclidean space. We propose
the general form of variance reduction, called SVRRG, in the framework of the stochastic Rieman-
nian gradient (SRG) optimization [4], such that it is able to enjoy the convergence properties (e.g.,
almost sure local convergence) of the SRG framework. We then get it specialized to the Riemannian
eigensolver (RG-EIGS) problem so that it gives rise to our stochastic variance reduced Riemannian

 
 
 
 
 
 
eigensolver, termed as SVRRG-EIGS. Our theoretical analysis shows that SVRRG-EIGS can use
a constant learning rate, thus eliminating the need of using the decaying learning rate. Moreover, it
not only possesses the global convergence in expectation compared to SRG [4], but also gains an ac-
celerated convergence of exponential rate compared to DSRG-EIGS. To the best of our knowledge,
we are the ﬁrst to propose and analyze the generalization of SVRG to Riemannian manifolds.

The rest of the paper is organized as follows. Section 2 brieﬂy reviews some preliminary knowledge
on matrix eigen-decomposition, stochastic Riemannian gradient optimization and stochastic Rie-
mannian eigensolver. Section 3 presents our stochastic variance reduced Riemannian eigensolver
algorithm, starting from establishing the general form of variance reduction for the stochastic Rie-
mannian gradient optimization. Theoretical analysis is conducted in Section 4, followed by the
empirical study of our algorithm in Section 5. Section 6 discusses related works. Finally, Section 7
concludes the paper.

2 Preliminaries and Notations

2.1 Matrix Eigen-decomposition

The eigen-decomposition of a symmetric1 matrix A
n can be written as A = UΛU⊤, where
U⊤U = UU⊤ = I (identity matrix), and Λ is a diagonal matrix. The j-th column uj of U is called
the eigenvector corresponding to the eigenvalue λj (j-th diagonal element of Λ), i.e., Auj = λj uj.
, uk] and V
Assume that λ1 ≥ · · · ≥
, λk)
and Σ
, λn). In practice, matrix eigen-decomposition only aims at the set of
= diag(λk+1,
top eigenvectors V. From the optimization perspective, this can be formulated as the following
non-convex QCQP problem:

, un], Σ = diag(λ1,

λn, V = [u1,

= [uk+1,

· · ·

· · ·

· · ·

· · ·

∈

⊥

⊥

×

Rn

max

Rn×k:X⊤X=I

X

∈

(1/2)tr(X⊤AX),

(1)

where k
a square matrix. It can be easily veriﬁed that X = V maximizes the trace at (1/2) P

) represents the trace of a square matrix, i.e., the sum of diagonal elements of

n and tr(
·

k
i=1 λi.

≪

2.2 Stochastic Riemannian Gradient Optimizaiton

Given a Riemmanian manifold
Euclidean space that locally linearizes
optimization on

, is a
∈ M
around X [17]. One iterate of the Riemannian gradient
takes the form similar to that of the Euclidean case [1]:

, the tangent space at a point X

, denoted as TX

M

M

M

M

X(t+1) = RX(t) (αt+1ξX(t) ),

(2)

∈

M

TX(t)

at X(t) and represents the search direction at
where ξX(t)
is a tangent vector of
the t-th step, αt+1 > 0 is the learning rate (i.e., step size), and RX(t) (
) represents the retraction
·
at X(t) that maps a tangent vector ξ
. Tangent vectors that serve as
TX(t)
search directions are generally gradient-related. The gradient of a function f (X) on
, denoted
as Gradf (X), depends on the Riemannian metric, which is a family of smoothly varying inner
products on tangent spaces, i.e.,
. The Riemannian
gradient Gradf (X)

is the unique tangent vector that satisﬁes

X, where ξ, η
i

to a point on

for any X

ξ, η
h

∈ M

TX

TX

M

M

M

M

M

∈

∈

∈

M

Gradf (X), ξ
h

X = Df (X)[ξ]
i

(3)

TX

, where Df (X)[ξ] represents the directional derivative of f (X) in the tangent
for any ξ
direction ξ. Setting ξX(t) = Gradf (X(t)) in (2) leads to the Riemannian gradient (RG) ascent
method:

M

∈

X(t+1) = RX(t) (αt+1Gradf (X(t))).

(4)

We can also set ξX(t) = G(yt+1, X(t)) in (2) and induce the stochastic Riemannian gradient (SRG)
ascent method [4]:

X(t+1) = RX(t) (αt+1G(yt+1, X(t))),

(5)

1The given matrix A is assumed to be symmetric throughout the paper, i.e., A⊤ = A.

2

where yt+1 is an observation of the random variable y at the t-th step that follows some distribution
and satisﬁes E[f (y, X)] = f (X), and G(y, X)
is the stochastic Riemannian gradient such
∈
M
that E[G(y, X)] = Gradf (X). According to [4], the SRG method possesses the almost sure (local)
convergence under certain conditions, including Pt αt =
(the latter condition
implies that αt →
→ ∞
2.3 Stochastic Riemannian Eigensolver

and Pt α2

0 as t

t <

TX

∞

∞

).

The constraint set in problem (1) constitutes a Stiefel manifold, St(n, k) =
I
}

, which turns (1) into a Riemannian optimization problem:

X

{

∈

X

max
St(n,k)
∈

f (X),

Rn

×

k : X⊤X =

(6)

where f (X) = 1
the Euclidean space Rn
ξ, η
h
as:

2 tr(X⊤AX). Note that St(n, k) is an embedded Riemannian sub-manifold of
k [1]. With the metric inherited from the embedding space Rn
k, i.e.,
TXSt(n, k)

X = tr(ξ⊤η), and using (3), we can get the Riemannian gradient2 Gradf (X)
i

∈

×

×

The orthogonal projection onto TXSt(n, k) under this metric is given by:

Gradf (X) = (I

XX⊤)AX.

−

for any ζ
[1]

∈

TXRn
×

k

Rn

×

≃

PX(ζ) = (I

XX⊤)ζ + Xskew(X⊤ζ)

TXSt(n, k)

∈

−

(7)

k, where skew(H) = (H

−

H ⊤)/2. In this paper, we use the retraction

RX(ξ) = (X + ξ)(I + ξ⊤ξ)−

1/2

(8)

∈

TXSt(n, k). The deployment of (4) and (5) here will then generate the Riemannian
for any ξ
eigensolver (denoted as RG-EIGS) and the stochastic Riemannian eigensolver (denoted as SRG-
EIGS), respectively. To the best of our knowledge, there is no existing stochastic Riemannian eigen-
solver that uses this retraction. The closest counterpart is the DSRG-EIGS that uses the Cayley
transformation based retraction. However, based on the work of DSRG-EIGS, it can be shown that
SRG-EIGS possesses the same theoretical properties as DSRG-EIGS, e.g., sub-linear convergence
to global solutions.

3 SVRRG-EIGS

In this section, we propose the stochastic variance reduced Riemannian gradient (SVRRG) and
specialize it to the eigensolver problem.

3.1 SVRRG

Recall that the stochastic variance reduced gradient (SVRG) [13] is built on the vanilla stochastic
gradient and achieves variance reduction through constructing control variates [26]. Control variates
are stochastic and zero-mean, serving to augment and correct stochastic gradients towards the true
gradients. Following [13], SVRG is encoded as

gt(ξt, w(t
−

1)) =

ψit (w(t
−

∇

1))

ψit ( ˜w)

(
∇

−

P ( ˜w)),

− ∇

(9)

where ˜w is a version of the estimated w that is kept as a snapshot after every m SGD steps, and

P ( ˜w) = 1

n
n P
i=1 ∇

ψi( ˜w) is the full gradient at ˜w.

∇
Our task here is to develop the Riemannian counterpart SVRRG of SVRG. Denote the SVRRG as
˜G(yt+1, X(t)). A naive adaptation of (9) to a Riemannian manifold

reads

M

˜G(yt+1, X(t)) = G(yt+1, X(t))

(G(yt+1, ˜X)

−

−

Gradf ( ˜X)),

2Due to the symmetry of A, the Riemannian gradients under Euclidean metric and canonical metric are the
same [28]. However, since the orthogonal projector used in the sequel requires the metrics for the embedded
Riemannian sub-manifold and the embedding space to be the same, we choose the Euclidean metric here.

3

∈

∈

−

M

M

T ˜X

TX(t)

and G(yt+1, ˜X), Gradf ( ˜X)
where G(yt+1, X(t))
. However, this adaptation
is not sound theoretically: the stochastic Riemannian gradient G(yt+1, X(t)) and the control variate
G(yt+1, ˜X)
Gradf ( ˜X) reside in two different tangent spaces, and thus making their difference
˜G(yt+1, X(t)) not well-deﬁned. We rectify this problem by the parallel transport [1], which moves
tangent vectors from one point to another (accordingly from one tangent space to another) along
geodesics in parallel. More speciﬁcally, we parallel transport the control variate from ˜X to X(t).
For computational efﬁciency, the ﬁrst-order approximation, called vector transport [1], is used.
Vector transport of a tangent vector from point ˜X to point X(t), denoted as
T
to tangent space TX(t)
from tangent space T ˜X
manifold of a Euclidean space, vector transport can be simply deﬁned as [1]:

X(t) , is a mapping
is an embedded Riemannian sub-

. When

M

M

M

→

˜X

→
) represents the orthogonal projector onto TX(t)

where PX(t) (
·
With the vector transport, we obtain the well-deﬁned SVRRG in TX(t)

M

X(t) (ξ ˜X) = PX(t) (ξ ˜X),

˜X

T

˜G(yt+1, X(t)) = G(yt+1, X(t))

We then arrive at our SVRRG method:

X(t) (G(yt+1, ˜X)

˜X

− T

→

−

:

M
Gradf ( ˜X)).

for the embedding Euclidean space.

X(t+1) = RX(t) (αt+1 ˜G(yt+1, X(t))),

(10)

by setting ξX(t) = ˜G(yt+1, X(t)) in (2). Note that the SVRRG method (10) is naturally subsumed
into the SRG method (5), and thus enjoys all the properties of SRG.

1)

do

1))

, m do

Algorithm 1 SVRRG
Require: Data A, initial ˜X(0), learning rate α, epoch length m
1: for s = 1, 2,
· · ·
Compute Gradf ( ˜X(s
2:
−
3: X(0) = ˜X(s
−
for t = 1, 2,
4:
5:
6:
7:
8:
9:
10:
11:
12: end for

Pick yt from the sample space uniformly at random
1)) and G(yt, ˜X(s
Compute G(yt, X(t
−
−
X(t−1) (G(yt, ˜X(s
1))
Compute
−
T
→
X(t−1) (G(yt, ˜X(s
Compute G(yt, X(t
−
−
→
Compute X(t) = RX(t−1) (α ˜G(yt, X(t
−

end for
˜X(s) = X(m)

1)))

˜X(s−1)

˜X(s−1)

− T

1))

1))

· · ·

−

Gradf ( ˜X(s
−
1))

1)))

Gradf ( ˜X(s
−

1)))

−

3.2 SVRRG-EIGS

With the SVRRG described above, we can now proceed to develop an effective eigensolver by
specializing (6). This new eigensolver is named SVRRG-EIGS. The update can be written as

X(t+1) = (X(t) + αt+1 ˜G(yt+1, X(t)))(I + α2

t+1

˜G⊤(yt+1, X(t)) ˜G(yt+1, X(t)))−

1/2,

(11)

t+1

˜G⊤(yt+1, X(t)) ˜G(yt+1, X(t)))−

which can be decomposed into two substeps: Y(t+1) , X(t) + αt+1 ˜G(yt+1, X(t)) and X(t+1) =
Y(t+1)(I + α2
1/2. Intuitively, the ﬁrst substep moves along the
direction ˜G(yt+1, X(t)) from the current point X(t) to the intermediate point Y(t+1) in the tangent
space TX(t) St(n, k). The second substep then gets the intermediate point Y(t+1) retracted back onto
the Stiefel manifold St(n, k) to reach the next point X(t+1).
Let’s delve into the ﬁrst substep. Except for the vector transport inside ˜G(yt+1, X(t)), it looks
much like an SVRG step since it works in the Euclidean tangent space. Assume that we have
A = 1
, At+1 = A(yt+1), and

L
l=1 A(l), y is a random variable taking values in

1, 2,

, L

L P

{

· · ·

}

4

stochastic gradient takes the form G(yt+1, X) = (I
We can get the control variate as

−

XX⊤)At+1X (i.e., sampling over data A).

G(yt+1, ˜X)

−

Gradf ( ˜X) = (I

˜X ˜X⊤)(At+1 −

−

A) ˜X.

By using the orthogonal projector in (7), the transported control variate can be written as

T
= (I

= (I

˜X

→

−

X(t) (G(yt+1, ˜X)
X(t)X(t)⊤
X(t)X(t)⊤
X(t)skew(X(t)⊤

Gradf ( ˜X))
−
A) ˜X + X(t)skew(X(t)⊤
˜X ˜X⊤)(At+1 −
X(t)X(t)⊤
A) ˜X
(I
−
A) ˜X).
˜X ˜X⊤)(At+1 −

)(I
−
)(At+1 −
(I
−

−

−

˜X ˜X⊤)(At+1 −
(I
−
A) ˜X +
) ˜X ˜X⊤(At+1 −

A) ˜X)

Accordingly, we have the SVRRG expressed as

˜G(yt+1, X(t)) = (I
= (I

−

X(t)X(t)⊤
X(t)X(t)⊤
X(t)X(t)⊤
, Gradf (X(t)) + W(t+1),

)At+1X(t)
)AX(t) + (I
−
) ˜X ˜X⊤(At+1 −

− T

(I

−

−

Gradf ( ˜X))

X(t) (G(yt+1, ˜X)
−
A)(X(t)
)(At+1 −
X(t)skew(X(t)⊤

˜X
→
X(t)X(t)⊤
A) ˜X

(I

−

˜X) +
−
˜X ˜X⊤)(At+1 −

A) ˜X)

−

∈
−

where W(t+1)
TX(t) St(n, k) is a stochastic zero-mean term conditioned on X(t). Note that
˜X) in W(t+1) might be theoretically harsh3, because an eigenspace could have
the factor (X(t)
k orthogonal matrix, and thus it is only
distinct representations which are the same up to a k
expected that X(t) and ˜X have the same column space at convergence. The ideal replacement
would be (col(X(t))
) represents the column space. Numerically it could be
achieved by replacing ˜X with ˜XB(t) where B(t) = Q2Q⊤1 and X(t)⊤ ˜X = Q1ΩQ⊤2 is the SVD of
X(t)⊤ ˜X [24].
The ﬁrst substep can now be rewritten as

col( ˜X)) where col(
·

×

−

SVRRG-EIGS : Y(t+1) = (X(t) + αt+1Gradf (X(t))) + αt+1W(t+1).

As a comparison, we can similarly decompose the update steps (4) and (5) of RG-EIGS and SRG-
EIGS into two substeps and then have:

RG-EIGS : Y(t+1) = X(t) + αt+1Gradf (X(t)),
SRG-EIGS : Y(t+1) = (X(t) + αt+1Gradf (X(t))) + αt+1(I

X(t)X(t)⊤

−

)(At+1 −

A)X(t).

−

X(t)X(t)⊤

)(At+1 −

Compared to that in RG-EIGS, each step in both SRG-EIGS and SVRRG-EIGS amounts to tak-
ing one Riemannian gradient step in the tangent space, adding a stochastic zero-mean term in the
tangent space, and then retracting back to the manifold. However, the stochastic zero-mean term
A)X(t) in SRG-EIGS has a constant variance. Therefore it needs the
(I
learning rate αt to decay to zero to reduce the variance and to ensure the convergence, and conse-
quently compromises on the convergence rate. In contrast, SVRRG-EIGS keeps boosting the vari-
ance reduction of the stochastic zero-mean term W(t+1) during iterations. The variance of W(t+1)
is not constant but dominated by three quantities
and
X(t)⊤
. These quantities repeatedly decay till vanishing in expectation, as X(t) and
k
˜XB(t) are expected to get closer and closer to each other gradually. This induces a decaying vari-
ance without the learning rate involved. Therefore, SVRRG-EIGS is able to use a ﬁxed learning rate
αt = α and achieve a much faster convergence rate.

˜X ˜X⊤)
k

X(t)X(t)⊤

) ˜XB(t)

˜XB(t)

X(t)

(I

(I

−

−

−

k

k

k

k

,

4 Theoretical Analysis

We give the main theoretical results in this section. The proofs are provided in the supplementary
material.

3It works well empirically.

5

×

Rn

n which can be written as A = 1

L
l=1 A(l)
1. The eigen-decomposition of A is as deﬁned in Section 2.1. And the
k2 ≤
λk+1 > 0. Then the top k eigenvectors V can be approximated to arbitrary

Theorem 4.1. Consider a symmetric matrix A
A(l)
such that maxl k
eigen-gap τ = λk −
accuracy ε
∈
epochs of our SVRRG-EIGS algorithm, in the sense that the potential function Θ( ˜X(T )) = k
V⊤ ˜X(T )
k
about initial iterate ˜X(0), ﬁxed learning rate α and epoch length m, are simultaneously satisﬁed:

log(1/ε)
log(2/ϕ) ⌉
−
ϕ, provided that the following conditions

(0, 1) and with any conﬁdence level ϕ

ε with probability at least 1

log2(1/ε)
⌉

) by running T =

1
log2(1/ε)
⌈

2
F ≤

L P

− ⌈

(0,

∈

∈

k

⌈

⌉

˜b0 = k

V⊤ ˜X(0)

− k
3 log(2/ϕ)
c1ατ

m

≥

2
F <

k

1
2

, α

(0, min
{

∈

c0τ,

c1
8c2

,

c3kmα2 + c5kpmα2 log(2/ϕ)

≤

τ ϕ2

1
2 −

),

}

˜b0,

where the constants are positive and deﬁned as

c0 = min
{
1
8

c1 =

2
τ

(

τ

−

,

1
32√3kτ 2

1
c1τ 2 , −
2α(1 + 2α)(1 + 24k2)

c3 = 4(1 + 2α) + 192(k2(1 + 2α) +

(118406 + 144k2) + p(118406 + 144k2)2 + 18τ (1 + 24k2)
24τ (1 + 24k2)

,

}

118400
3

−
7400
9

α),

c2 = 96(k2(1 + 2α) + 823),

),

c4 =

20
5c0τ

−

1

+ c0c3τ,

c5 = √2c4.

A(l)

A(l)

k2 ≤

k2 ≤

V⊤ ˜X(0)

1 in the theorem. In fact,
Note that we have no loss of generality from assuming that maxl k
A(l)
r with r > 1 (which could be estimated by, e.g., Gershgorin circle theorem), we
if maxl k
could replace A(l) with 1
r A(l) to get maxl k
1 and arrive at the same eigen-space. Another
k2 ≤
way of addressing this generality is to adopt the idea of [24], that is, replacing the learning rate α
with rα and the eigen-gap τ with τ /r, with some of the constants in the theorem re-derived. The
condition on the initial iterate, i.e., k
2 , is theoretically non-trivial. However,
empirically this condition can be well satisﬁed by running other stochastic algorithms (e.g., SRG-
EIGS or DSRG-EIGS) or a few steps of deterministic iterative algorithms (e.g., RG-EIGS), because
In our experiments, we use SRG-EIGS for this
they are good at ﬁnding sub-optimal solutions.
purpose, which makes the theorem amount to a convergence analysis at a later stage of the hybrid
algorithm (e.g., starting from t0 > 0 instead of t0 = 0). The convergence rate of our algorithm
can be roughly identiﬁed by the iteration number O(mT ) = O(m
) which establishes an
exponential global convergence rate. Compared to the sub-linear rate O(1/ε) of DSRG-EIGS by [2],
it achieves a signiﬁcant improvement since the complexity of a single iteration in the two algorithms
only differs by constants. In summary, initialized by a low-precision eigensolver, our SVRG-EIGS
algorithm would obtain a high-precision solution in a limited number of epochs (data passes), which
is theoretically guaranteed by Theorem 4.1.

log(1/ε)
log(2/ϕ) ⌉

F < 1
2

− k

k

⌈

We provide an elegant proof of Theorem 4.1 in Appendix, though it is a bit involved. For ease of
exposition and understanding, we decompose this course into three steps in a way similar to [24],
including the analysis on one iteration, one epoch and one run of the algorithm. Among them, the
ﬁrst step (i.e., one iteration analysis) lies at the core of the main proof, where the techniques we use
are dramatically different from those in [3, 24] due to our new context of Rimannian manifolds, or
more precisely, Stiefel manifolds. This inherently different context requires new techniques, which
in turn yield an improved exponential global convergence and accordingly bring more improvements
over the convergence of sub-linear rate [2].

5 Experiments

In this section, we empirically verify the exponential convergence rate of our SVRRG-EIGS algo-
rithm and demonstrate its capability of ﬁnding solutions of high precision when combined with other
algorithms of low precision. Speciﬁcally, we use SRG-EIGS to generate a low-precision solution for
initializing SVRRG-EIGS, and do the comparison with both RG-EIGS and SRG-EIGS. Among var-
ious implementations of RG-EIGS with different choices of metric and retraction in (2), we choose

6

the one with canonical metric and Cayley transformation based retraction [28] since its code is pub-
lically available4. This version of RG-EIGS uses the non-monotone line search with the well-known
Barzilai-Borwein step size, which signiﬁcantly reduces the iteration number, and performs well in
practice. Both RG-EIGS and SRG-EIGS are fed with the same random initial value of X, where
(0, 1) and then all entries as a whole
each entry is sampled from the standard normal distribution
are orthogonalized. SRG-EIGS uses the decaying learning rate αt = η
t where η will be tuned.
We verify the properties of our algorithm on a real symmetric matrix, Schenk5, of 10, 728
10, 728
size, with 85, 000 nonzero entries. We partition A into column blocks with block size equal to 100
so that we can write A = 1
and each A(l) having only one column
block of A and all others zero. We set k = 3. For SVRRG-EIGS, we are able to use a ﬁxed learning
k · k1 represents the matrix 1-norm), similar to that
rate α and adopt the heuristic α =
in [24]. We set ζ = 4.442 and epoch length m = 1
2 L, i.e., each epoch takes 1.5 passes over A
(including one pass for computing the full gradient). Accordingly, the epoch length of SRG-EIGS
is set to m = 3

L
l=1 A(l) with L =

2 L. In addition, we set B(t) = I.

k1√n (

10728
100 ⌉

L P

N

×

A

⌈

k

ζ

X(t))

The performance of different algorithms is evaluated using three quality measures:
X(t)⊤

kF , relative error function E(X) , 1
I

k
malized potential function Θ( ˜X(t))/k = 1
. The ground truths in these measures,
k
including both V and maxX
i=1 λi, are obtained using
Matlab’s EIGS function for benchmarking. For each measure, lower values indicate higher quality.

2 tr(X⊤t AXt) that is set to (1/2) P

V⊤ ˜X(t)
k

2 tr(X(t)⊤ AX(t)))

feasibility

, and nor-

maxX∈St(n,k)

2 tr(X(t)

−
2
F
k

AX(t))

St(n,k)

−

−

∈

k

1

⊤

1

1

Given a solution X(0) of low precision6 at E(X(0))
precision, that is, E(X)
precision requirement is met or the maximum number of epoches (set as 20) is reached.

6, our SVRRG-EIGS targets a double
10−
12. Each algorithm terminates when the

12 or Θ( ˜X(t))/k

≤
10−

10−

≤

≤

10-11

10-12

F

|
|

I

-

X

T

X

|
|

10-13

10-14

10-15

10-16

0

5

Schenk

RG-EIGS
SRG-EIGS, η=7.5e-05
SRG-EIGS, η=1.0e-04
SRG-EIGS, η=1.2e-04
SRG-EIGS, η=1.5e-04
SRG-EIGS, η=1.7e-04
SVRRG-EIGS

)

X
E

(

20

10

15
# data passes
(a) Feasibility

25

30

100

10-2

10-4

10-6

10-8

10-10

10-12

10-14

0

Schenk

5

10

15
# data passes

20

25

30

100

10-2

10-4

k
/
)

(

X
Θ

10-6

10-8

10-10

10-12

10-14

0

Schenk

5

10

15
# data passes

20

25

30

(b) Relative error function

(c) Normalized potential function

Figure 1: Performance on Schenk. Note that the y-axis in each ﬁgure is in log scale.

We report the convergence curves in terms of each measure, on which empirical convergence rates
of the algorithms can be observed. Figure 1 reports the performance of different algorithms. In
terms of feasibility, both SRG-EIGS and SVRRG-EIGS perform well, while RG-EIGS produces
much poorer results. This is because the Cayley transformation based retraction used therein relies
heavily on the Sherman-Morrison-Woodbury formula, which suffers from the numerical instability.
From Figures 1(b) and 1(c), we observe similar convergence trends for each algorithm under the
two different measures. All three algorithms improve their solutions with more iteration. There are
several exceptions in RG-EIGS. This is due to the non-monotone step size used in its implementation.
We also observe that SRG-EIGS presents an exponential convergence rate at an early stage thanks to
a relatively large learning rate. However, it subsequently steps into a long period of sub-exponential
convergence, which leads to small progress towards the optimal solution. In contrast, our SVRRG-
EIGS inherits the initial momentum from SRG-EIGS and keeps the exponential convergence rate
throughout the entire process. This enables it to approach the optimal solution at a fast speed. RG-
EIGS has a different trend. It converges sub-exponentially at the beginning and performs the worst.

4optman.blogs.rice.edu/
5www.cise.ufl.edu/research/sparse/matrices/
6This low precision could be problem dependent.

7

 
 
 
 
Though it converges fast at a later stage, it still needs more passes over data than SVRRG-EIGS in
order to achieve a high precision.

6 Related Work

Existing methods on eigensolvers include the power method [9], the (block) Lanczos algorithms [5],
Randomized SVD [10], Riemannian methods [1, 25], and so on. All these methods performs the
batch learning, while our focus in this paper is on stochastic algorithms. From this perspective, few
existing works include online learning of eigenvectors [8] which aims at the leading eigenvector,
i.e., k = 1, and doubly stochastic Riemannian method (DSRG-EIGS) [2] where the learning rate
has to decay to zero. [8] provides the regret analysis without empirical veriﬁcation for their method,
while DSRG-EIGS belongs to one of implementations of SRG-EIGS in this paper where the double
stochasticity comes from sampling over both data and coordinates of Riemmanian gradients. On the
other hand, since the work of [13], variance reduction (SVRG) has become an appealing technique to
stochastic optimization. There are quite some variants developed from different perspectives, such
as practical SVRG [11], second-order SVRG [15], distributed or asynchronous SVRG [12, 16], and
non-convex SVRG [23, 24]. Our SVRRG belongs to non-convex SVRG, but is addressed from the
Riemannian optimization perspective. The core techniques we use are dramatically different from
existing ones due to our new context.

7 Conclusion

In this paper, we proposed the generalization of SVRG to Riemannian manifolds, and established
the general framework of SVRG in this setting, SVRRG, which requires the key ingredient, vector
transport, to make itself well-deﬁned. It is then deployed to the eigensolver problem and induces
the SVRRG-EIGS algorithm. We analyzed its theoretical properties in detail. As suggested by our
theoretical results, the proposed algorithm is guaranteed to ﬁnd high-precision solutions at an expo-
nential convergence rate. The theoretical implications are veriﬁed on a real dataset. For future work,
we will explore the possibility of addressing the limitations of SVRRG-EIGS, e.g., dependence on
eigen-gap and non-trivial initialization. We may also conduct more empirical investigations on the
performance of SVRRG-EIGS.

APPENDIX: Supplementary Material

A Useful Lemmas

In this section, some deﬁnition, basics, and a group of useful lemmas are provided. All the matrices
are assumed to be real.

A.1 Deﬁnitions and Basics

A.1.1 Matrix facts: symmetry, positive semi-deﬁniteness, trace, norm and orthogonality

k

B

≻

≻

0 (

B⊤

2
F and

(cid:23)
1
0 then B−

2
F = tr(B⊤B) = tr(BB⊤) =

k
) represents the maximum eigenvalue of an n

The matrix B
0) represents that B is symmetric and positive semideﬁnite (deﬁnite), and if
≻
0 as well. The trace of a square matrix, tr(B), is the sum of diagonal entries of B.
B
A useful fact about trace is the circular property, e.g., tr(BCD) = tr(CDB) = tr(DBC) for matri-
ces B, C, D.
k2 = pλmax(B⊤B) = σmax(B)
k
represents the Frobenious-norm and spectral norm (i.e., matrix 2-norm) of matrix B, respectively.
) represents the max-
Here λmax(
·
m matrix. Note that BC and CB have the same set of nonzero
imum singular value of an n
×
Rm
k2. In this doc-
eigenvalues for two matrices B
k2 =
k
ument, we always assume that the eigenvalues of an n
≥
×
ρ(B) , maxi |
k2 for
λ2(B)
B
λi| ≤ k
λi(B)
|
, where ρ(B) is called the spectral radius of a square matrix B. And also
any i
, n
· · ·
}
tr(B) = Pi λi(B), which in turn implies
kF for any matrix C. For any two matrices B
and C that make BC well-deﬁned,
holds for both Frobenious-norm and spectral
BC
k2 holds. Furthermore, the orthogonal invariance also holds for both
norm, and

k
n matrix, σmax(
·

k
n matrix B takes the form λ1(B)

λn(B). Thus λmax(B) = λ1(B) and

×
n. Thus,

≥ · · · ≥
1, 2,
∈ {

k2 ≤ k
C
B
kk

k
k ≤ k

m and C

kF ≤ k

kF k

BC

BT

| ≤

Rn

B

B

B

C

C

C

∈

∈

k

k

k

k

×

×

8

Frobenious-norm and spectral norm, i.e.,
k
Q (i.e., P ⊤P = I and Q⊤Q = I). For X
i.e., [X, X

][X, X

]⊤ = I which implies X ⊤X

P CQ⊤

∈

⊥

⊥

=
k
St(n, k), let X

C

k

k

for column-orthonormal matrices P and
represent its orthogonal complement,

⊥
= 0 and X

⊥

St(n, n

k).

−

⊥ ∈

A.1.2 Martingale

≤

≥

for t

0, meaning that

t. In our context,

Ft for brevity. Let H =

The ﬁltration, deﬁned on a measurable probability space, is an increasing sequence of sub-sigma
algebras
Ft encodes the set of all
{Ft}
the random variables seen thus far (i.e., from 0 to t). In this document, conditioned on X (t) refers to
be a stochastic process and a ﬁltration,
conditioned on
F
respectively, on the same probability space. Then H is called a martingale (super-martingale) with
, and E[Ht+1|Ft] = Ht (E[Ht+1|Ft]
respect to
Ht|
Ht). Given a random variable X
≥
(Markov inequality). Let X0, X1,
· · ·
Xt
1| ≤
and any a > 0, the probability P (Xt −
inequality) [19].

Fs ⊂ Ft for all s
and
=
{
Ft-measurable, E[

|
dt (i.e., bounded difference) where dt is a deterministic function of t. Then for all t

≤
E[X]/a
Xt −
0
≥
(Azuma-Hoeffding

≤
, XT be a martingale or supermartingale such that

∞
0 and a constant a > 0, the probability P (X

if for each t, Ht is

t
s=1 d2
t )
}

a2/(2 P

X0 ≥

Ht}

{Ft}

exp

] <

{−

a)

a)

≥

≤

F

−

|

A.2 Lemmas

The proofs of Lemma A.1-A.5 can be found in [24].

Lemma A.1. For any B, C, D

0, it holds that

(cid:23)
tr(B(C

−

tr(BC)

≥
0 and C

D)) and tr(BC)

tr((B

−

D)C).

≥
tr(B(2I

Lemma A.2. If B
(cid:23)
Lemma A.3. Let B1, B2, Z1, Z2 be k
k square matrix, where B1, B2 are ﬁxed and Z1, Z2 are
stochastic zero-mean. Furthermore, suppose that for some ﬁxed β, γ, δ > 0, it holds with probability
1 that

0, then tr(BC−

C)).

−

≻

×

≥

1)

•

•

max

For all ν

[0, 1], B2 + νZ2 (cid:23)
Z2kF } ≤
γ

∈
Z1kF ,
k
B1 + αZ1k2 ≤

{k

β

δI

• k

Then

E[tr((B1 + Z1)(B2 + Z2)−

β2(1 + γ/δ)
δ2
≥
k matrix with minimal singular value σ and

tr(B1B−

1
2 )

1)]

−

B

.

k

k2 ≤

1. Then

Lemma A.4. Let B be a k

×

k

1

−

2
B⊤B
F
k
2
B
F ≥

k

k

σ2
k

(k

2
F ).

B

k

− k

k matrices C, D with orthonormal columns,

let B⋆ =

Lemma A.5. For any n
×
2
F . Then
arg minB⊤B=I k
B⋆ = Q2Q⊤1 ,

k
DB⋆

DB

−

C

C

2
F ≤ k

C

DB

2
F

and

C

DB⋆

2
F ≤

2(k

C⊤D

2
F ),

k

k

−

−
where C⊤D = Q1ΩQ⊤2 is the SVD of C⊤D.
Lemma A.6. Let Y (t) and X (t+1) be as deﬁned in Section 3.2 of the main paper. Assume
A(l)
k matrix V with orthonormal columns, it
maxl k
holds that

1 and α < 1/5. Then for any n

k2 ≤

− k

−

×

k

k

k

k

V ⊤X (t+1)

2
F − k

k

V ⊤X (t)

(cid:12)
(cid:12)
(cid:12)k

−
˜G⊤(yt+1, X (t)) ˜G(yt+1, X (t)) = Y (t+1)⊤

20kα
5α
1

.

2
F (cid:12)
(cid:12)
(cid:12) ≤

k

Proof. Note that I + α2
t
∈
TX(t) St(n, k) and thus X (t)⊤ ˜G(yt+1, X (t)) = 0 [1]. Based on the proof of Lemma 9 in [24], it

Y (t+1) since ˜G(yt+1, X (t))

9

−

= (I

)At+1X (t)

sufﬁces for us to show that Y (t+1) = X (t) + αN and
main paper, we have
N = ˜G(yt+1, X (t))
X (t)X (t)⊤
X (t)X (t)⊤
(I
−
X (t)⊤
= X (t)
⊥
⊥
A(l)
Since maxl k
k2 ≤
2. Note that
A
k2 ≤
k
k
X (t)⊤

−
˜X ˜X ⊤)(At+1 −
X (t)⊤
X (t)
˜X
−
⊥
At+1k2 ≤
k
X (t)
˜X
⊥ k2 =
k
X (t)
k2 ≤ k

1, we have
X (t)
k2 =
k
At+1X (t)

−
At+1X (t)

A) ˜X
˜X ⊤
⊥

X (t)
⊥

⊥ k2k

−
(At+1 −
1,
A
k2 ≤
k
˜X
k2 =
k
X (t)⊤

⊥ k2k

)(I

k

⊥

⊥

⊥

Similarly,

N

k

k2 ≤

5. In fact, from Section 3.2 of the

X (t)skew(X (t)⊤
A) ˜X

(I

−
X (t)skew(X (t)⊤ ˜X

−

˜X ˜X ⊤)(At+1 −
˜X ⊤
⊥
At+1k2 +

⊥
k2 ≤ k

A) ˜X)
(At+1 −

1 and thus

At+1 −
A
k
⊥k2 = 1. Then we have
1.

X (t)

At+1k2k

k2 ≤

A) ˜X).

X (t)⊤
X (t)
˜X
⊥
X (t)skew(X (t)⊤ ˜X

k

⊥

˜X ⊤
⊥
⊥
˜X ⊤
⊥

(At+1 −
(At+1 −

A) ˜X
A) ˜X)

k2 ≤
k2 ≤

2,

2.

⊥

k

0 and D

C

(cid:23)

≻

(cid:23)

0, then tr(BC−

1)

tr(BD−

1).

≥

Thus,

N

k

k2 ≤

5.

Lemma A.7. If B

Proof.

BC−

1 = B(D
= BD−

(D
−
1/2(I

By Lemma A.1-A.2, we have

C))−

1

D−

1/2(D

−

−

−

C)D−

1/2)−

1D−

1/2.

tr(BC−

1) = tr(BD−
= tr(D−

tr(D−

tr(D−

≥

≥

C)D−

1/2(D
1/2(I
D−
−
1/2(I
1/2BD−
D−
−
1/2(I + D−
1/2BD−
1/2) = tr(BD−
1/2BD−

−
1/2(D
1/2(D
1)

−

−

1/2)−
C)D−

C)D−

1/2)
1)

1D−
1/2)−
1/2))

Lemma A.8 (von Neumann’s trace inequality [18]). For two symmetric n
holds that

n

n matrices B and C, it

×

tr(BC)

≤

X
i=1

λi(B)λi(C).

Lemma A.9. For two symmetric n

×
n

n matrices B and C, it holds that

tr(BC)

≥

max

λn

−

{

X
i=1

i+1(B)λ1(C),

n

X
i=1

λi(B)λn

−

i+1(C)
}

.

Proof. The proof is done by replacing B with
inequality.

−

B or replacing C with

−

C in von Neumann’s trace

B Main Proof

The proof of the theorem is a bit involved. For ease of exposition and understanding, we decompose
this course into three steps in a way similar to [24], including the analysis on one iteration, one
epoch and one run of the algorithm. Among them, the ﬁrst step (i.e., one iteration analysis) lies
at the core of the main proof, where the techniques we use are dramatically different from those
in [3, 24] due to our new context of Rimannian manifolds, more precisely, Stiefel manifolds. This
inherently different context requires new techniques, which yield an improved exponential global
convergence and accordingly bring more improvements over the convergence of sub-linear rate by
[2].

10

B.1 One Iteration Analysis

In the ﬁrst step, we consider a single iteration t of our SVRRG-EIGS algorithm. The goal here is
V ⊤X (t)
2
to establish a stochastic recurrence relation on
F tends to k as t
k
k implies that X (t)
goes to inﬁnity with high probability (w.h.p.). Note that

2
F such that
2
V ⊤X (t)
F

k

V ⊤X (t)
k
t
→∞
−−−−→w.h.p.

k

k
k

converges to the global solution V up to a k
k orthogonal matrix w.h.p., which is exactly one of
our ultimate goals (i.e., convergence to global solutions w.h.p., ﬁxed learning rate and exponential
convergence rate). For brevity, we omit the lengthy superscripts by letting X = X (t), X ′ = X (t+1),
B = B(t), and ˜X = ˜X (s). And assume that maxl k
Lemma B.1. Follow the notations and assumptions made in Lemma B.4. Then it holds that

k2 ≤

A(l)

1.

×

tr(X ⊤V V ⊤X

X ⊤
⊥

⊥

AX)

τ (
k

≥

V ⊤X

2
F − k

k

X ⊤V V ⊤X

2
F )

k

Proof. Based on Section 2.1 of the main paper, the eigen-decomposition of matrix A can be written
as A = V ΣV ⊤ + V

. Then

Σ

V ⊤
⊥
⊥
tr(X ⊤V V ⊤X

⊥

= tr(X ⊤V V ⊤X

X ⊤
⊥
X ⊤
⊥
= tr(V ⊤XX ⊤V V ⊤X

⊥

⊥

AX)

By Lemma A.9, we have

V ΣV ⊤X) + tr(X ⊤V V ⊤X

X ⊤
⊥

V Σ) + tr(V ⊤
⊥

⊥

X ⊤
⊥
XX ⊤V V ⊤X

⊥

V

Σ

⊥
⊥
V
X ⊤
⊥

⊥

V ⊤
⊥
Σ

⊥

X)

).

⊥

tr(V ⊤XX ⊤V V ⊤X

X ⊤
⊥

⊥

V Σ)

tr(V ⊤
⊥

XX ⊤V V ⊤X

X ⊤
⊥

⊥

V

⊥

Σ

)

⊥

≥

≥

k

X
i=1

λi(V ⊤XX ⊤V V ⊤X

X ⊤
⊥

⊥

V )λk

−

i+1(Σ), and

n

k

−
X
i=1

λi(V ⊤
⊥

XX ⊤V V ⊤X

X ⊤
⊥

⊥

V

⊥

)λn

−

i+1(Σ

).

⊥

k

−

Note that both matrices above, i.e., V ⊤XX ⊤V V ⊤X
symmetric and thus Lemma A.9 can be applied. In fact,

⊥

X ⊤
⊥

V and V ⊤
⊥

XX ⊤V V ⊤X

X ⊤
⊥

⊥

V

⊥

are

V ⊤XX ⊤V V ⊤X

X ⊤
⊥

⊥

V = V ⊤XX ⊤V V ⊤(I

XX ⊤)V

= V ⊤XX ⊤V
= V ⊤XX ⊤V

V ⊤XX ⊤V V ⊤XX ⊤V
(V ⊤XX ⊤V )2,

−

−
−

which is symmetric. Furthermore, it is positive seme-deﬁnite, because

and thus

ρ(V ⊤XX ⊤V )

V ⊤XX ⊤V

≤ k

k2 ≤

X ⊤

(
k

V

k2)2 = 1,

k2k

λi(V ⊤XX ⊤V V ⊤X

Likewise, we have

X ⊤
⊥

⊥

V ) = λi(V ⊤XX ⊤V )

λ2
i (V ⊤XX ⊤V )

0.

≥

−

V ⊤
⊥

XX ⊤V V ⊤X

X ⊤
⊥

⊥

V

⊥

⊥
X
which is symmetric but negative semi-deﬁnite, because

−

−
X

=

(I

= V ⊤
⊥
V ⊤
⊥
(V ⊤
⊥

−

=

X ⊤
⊥
(I

X

⊥
X ⊤
⊥
X ⊤
⊥

⊥

V

)V V ⊤X

V

V ⊤
⊥
⊥
(V ⊤
⊥

−

⊥ −

⊥
)X

V

X ⊤
⊥
⊥
X ⊤
⊥
⊥
V
X ⊤
⊥

⊥

V

⊥

X

⊥
)2),

ρ(V ⊤
⊥

X

⊥

X ⊤
⊥

V

⊥

)

X

V ⊤
⊥

⊥

X ⊤
⊥

V
⊥k2 ≤

(
k

X ⊤

⊥ k2k

≤ k

⊥k2)2 = 1,
V

and thus

λi(V ⊤
⊥

XX ⊤V V ⊤X

We now can write

X ⊤
⊥

⊥

V

⊥

) =

(λi(V ⊤
⊥

X

⊥

X ⊤
⊥

−

V

⊥

)

−

λ2
i (V ⊤
⊥

X

X ⊤
⊥

⊥

V

⊥

))

≤

0.

tr(V ⊤XX ⊤V V ⊤X
k

X ⊤
⊥

V Σ) + tr(V ⊤
⊥

⊥

XX ⊤V V ⊤X
n

k

X ⊤
⊥

⊥

V

⊥

Σ

)

⊥

≥

≥

X
i=1

k

X
i=1

λi(V ⊤XX ⊤V V ⊤X

X ⊤
⊥

⊥

V )λk

−

i+1(Σ) +

λi(V ⊤XX ⊤V V ⊤X

X ⊤
⊥

⊥

V )λk(A) +

n

k

−
X
i=1

11

−
X
i=1

λi(V ⊤
⊥

XX ⊤V V ⊤X

X ⊤
⊥

⊥

V

⊥

)λn

−

i+1(Σ

)

⊥

k

−

λi(V ⊤
⊥

XX ⊤V V ⊤X

X ⊤
⊥

⊥

V

⊥

)λk+1(A),

in which, we ﬁnd that

k

X
i=1

λi(V ⊤XX ⊤V V ⊤X

X ⊤
⊥

⊥

V ) =

k

X
i=1

λi(V ⊤XX ⊤V )

k

−

X
i=1

λ2
i (V ⊤XX ⊤V )

= tr(V ⊤XX ⊤V )

=

X ⊤V

k

2
F − k

k

tr((V ⊤XX ⊤V )2)
2
F

V ⊤XX ⊤V

−

k

and similarly

k

X
i=1

Note that

X ⊤
⊥

k

V

⊥k

λi(V ⊤
⊥

XX ⊤V V ⊤X

X ⊤
⊥

⊥

V

⊥

) =

(
k

X ⊤
⊥

−

V

⊥k

2
F − k

V ⊤
⊥

X

X ⊤
⊥

⊥

V

⊥k

2
F ).

2
F = tr(V ⊤
⊥
= tr(I

X

X ⊤
⊥
⊥
V V ⊤

V

) = tr(V

X ⊤
⊥
⊥
⊥
XX ⊤ + V V ⊤XX ⊤)

V ⊤
⊥

X

⊥

) = tr((I

V V ⊤)(I

XX ⊤))

−

−

= n

−

and

−
2k +

−
X ⊤V

k

2
F

k

X

V ⊤
k
⊥
= tr((I

V

X ⊤
⊥k
⊥
⊥
V V ⊤)(I

= tr((I

= tr((I

V V ⊤)(I

V V ⊤)(I

−

−

−

2
F

−

−

−

= tr(I

= tr(I

−

V V ⊤

V V ⊤

−

−

−
2k +

= n

V ⊤XX ⊤V

−
Therefore, we arrive at

k

2
F .

k

XX ⊤)(I

XX ⊤)(I

V V ⊤)(I

V V ⊤

−

−

−

XX ⊤)(I + V V ⊤XX ⊤))

XX ⊤))

−
XX ⊤ + V V ⊤XX ⊤))

XX ⊤ + V V ⊤XX ⊤ + (I

V V ⊤

−

XX ⊤ + V V ⊤XX ⊤)V V ⊤XX ⊤)

−

XX ⊤ + V V ⊤XX ⊤V V ⊤XX ⊤)

tr(V ⊤XX ⊤V V ⊤X

X ⊤
⊥

V Σ) + tr(V ⊤
⊥

⊥

XX ⊤V V ⊤X
k

n

X ⊤
⊥

⊥

V

⊥

Σ

)

⊥

λi(V ⊤XX ⊤V V ⊤X

X ⊤
⊥

⊥

V ) + λk+1

λi(V ⊤
⊥

XX ⊤V V ⊤X

X ⊤
⊥

⊥

V

⊥

)

−
X
i=1

λk

k

X
i=1

≥

= (λk −
= τ (
k

λk+1)(
k
2
F − k

k

V ⊤X

X ⊤V

2
F − k
k
X ⊤V V ⊤X

2
F ).

k

V ⊤XX ⊤V

2
F )

k

Lemma B.2. Let B1 and B2 be deﬁned by (13) and (12), respectively, and follow the notations and
assumptions made in Lemma B.4. Then it holds that

tr(B1B−

1
2 )

≥ k

V ⊤X
k
α2(1 + 2α)(4(k

2
F + 2ατ (
k

V ⊤X

k
V ⊤X

− k

2
F − k
F ) + k2κ2
2
k

X ⊤V V ⊤X
F ).

2
F )

k

−

Proof. Note that

B1 = (X ⊤(I + αAX

= X ⊤V V ⊤X + αX ⊤V V ⊤X

X ⊤
⊥

⊥

)V V ⊤(I + αX

A)X)

X ⊤
(cid:23)
⊥
AX + αX ⊤AX

⊥

0

V V ⊤X +

X ⊤
⊥

⊥

and

α2X ⊤AX

X ⊤
⊥

⊥

V V ⊤X

⊥

X ⊤
⊥
AX

⊥
X ⊤
⊥

B2 = I + α2X ⊤AX

X ⊤
⊥

⊥

AX + α2κ2

F I

0.

≻

Then by Lemma A.2, we get

tr(B1B−

1
2 )

tr(B1(2I
≥
= tr(B1(I

B2))
−
α2X ⊤AX

−

X ⊤
⊥

⊥

AX

−

α2κ2

F I)).

12

Since α

1

4 , ακF ≤

≤

1
4 and

X ⊤AX

X ⊤
⊥

⊥

AX

k2 ≤

(
k

X ⊤

⊥ k2k

A

k2k

k

X

k2)2 =

A
k

2
2 ≤

k

1,

we have

(cid:23)

(cid:23)
And note that α2X ⊤AX

⊥

I

I

I

−

−

−

α2X ⊤AX
⊥
α2
X ⊤AX

k
α2I

−

⊥
α2κ2
F I

AX

X ⊤
⊥
X ⊤
⊥

AX

(1

(cid:23)

α2κ2
−
k2I
1
16 −

F I
α2κ2
1
16

−

−

F I

)I =

7
8

I

0.

≻

X ⊤
⊥

V V ⊤X

X ⊤
⊥

⊥

AX

(cid:23)

0. Then by Lemma A.1, we can arrive at

X ⊤
⊥

V V ⊤X

X ⊤
⊥

⊥

AX)(I

−

α2X ⊤AX

X ⊤
⊥

⊥

AX

−

α2κ2

F I)).

1
2 )

≥

α2X ⊤AX

tr(B1B−
tr((B1 −
To simplify above inequality, deﬁne
C1 , B1 −

⊥

α2X ⊤AX
= X ⊤V V ⊤X + αX ⊤V V ⊤X

X ⊤
⊥

V V ⊤X

⊥

AX

X ⊤
⊥
AX + αX ⊤AX

⊥
X ⊤
⊥

⊥

X ⊤
⊥

⊥

V V ⊤X.

Then

tr(B1B−

1
2 )

tr(C1(I
≥
= tr(C1)

α2X ⊤AX
X ⊤
−
⊥
α2tr(C1X ⊤AX

⊥

AX

α2κ2

−
AX)

X ⊤
⊥

⊥

F I))
α2κ2

−

−

F tr(C1).

we now lower bound each of three items above. On one hand, by Lemma B.1, we get

tr(C1) = tr(X ⊤V V ⊤X) + 2αtr(X ⊤V V ⊤X

2
F − k
On the other hand, by Cauchy-Schwarz inequality, we can obtain

2
F + 2ατ (
k

V ⊤X

V ⊤X

≥ k

k

k

X ⊤
⊥

⊥

X ⊤V V ⊤X

AX)
2
F ).

k

tr(C1) = tr(X ⊤V V ⊤X) + 2αtr(X ⊤V V ⊤X

⊥
X ⊤V V ⊤X

AX)

X ⊤
⊥
X ⊤
⊥
⊥
V V ⊤X

A

X ⊤

kF k
X ⊤

X

⊥

A

⊥ k2k

X

kF k
X ⊤
⊥
X

⊥
k2k

k

kF
A
k2k
2
F

X

kF

≤ k

≤ k

≤ k

≤

X

X ⊤

V V ⊤

X ⊤V V ⊤

kF + 2α
kF k
k
V V ⊤
X
kF + 2α
k2k
k
2
V V ⊤
F + 2α
k2k
k
k
2
F = (1 + 2α)k2.
k2 and
A

kF k
k2k
(1 + 2α)
k
k
Σ
k2 ≤ k

X

X

Σ

k

kF ≤ k
X ⊤V V ⊤X

C1k2k
X ⊤
⊥

X ⊤AX
⊥kF k
k2)
k

X ⊤
⊥
X ⊤AX

AX

⊥

AX
kF
2
F

⊥k

AX

AX)

k
X ⊤
⊥
⊥
X ⊤
⊥kF k
⊥
k2 + 2α
k
2
F
⊥k
X ⊤(V ΣV ⊤ + V

X ⊤AX

tr(C1X ⊤AX
C1X ⊤AX
X ⊤V V ⊤X

≤

≤ k
(
k
(1 + 2α)
k
≤
= (1 + 2α)
k
(1 + 2α)(
k
(1 + 2α)(
k
(1 + 2α)(
k
(1 + 2α)(
k

≤

≤

≤

≤

X ⊤V ΣV ⊤X

⊥

⊥

)X

k
V ⊤X

2
V ⊤
F
⊥k
⊥
X ⊤V
Σ
V ⊤
⊥
⊥kF +
X ⊤V
k
k2)2
A
⊥kF k

Σ
⊥kF +
Σ
k2k
k2k
⊥kF +
X ⊤V
k
⊥kF )2,
X ⊤V
k

X

⊥

⊥

V

k2k
V ⊤X
⊥kF +

X ⊤

A

k2k
V ⊤X

⊥kF )2
Σ
⊥kF k

⊥k2k

V ⊤
⊥ k2k

X

⊥k2)2

For the middle term, noting that

⊥k2 ≤ k

k2, then it can be derived as follows
A

where

V ⊤X

k

⊥k

2
F = tr(X ⊤
⊥

= tr(V V ⊤(I

V V ⊤X

) = tr(V V ⊤X

)

X ⊤
⊥

⊥

⊥
XX ⊤)) = k

= k

−

tr(X ⊤V V ⊤X) = k

−

tr(V V ⊤XX ⊤)
2
F ,

V ⊤X

− k

k

−

13

and similarly

X ⊤V

2
F = k
⊥k
k
tr(C1X ⊤AX

⊥

− k
X ⊤
⊥

k

AX)

V ⊤X

2
F . Thus, we could write

(1 + 2α)(
k

≤
= 4(1 + 2α)(k

V ⊤X

⊥kF +
V ⊤X
k

k
2
F ).

− k

X ⊤V

⊥kF )2

Therefore, we now can arrive at

tr(B1B−

1
2 )

≥

α2tr(C1X ⊤AX
−
2
X ⊤V
F + 2ατ (
k

tr(C1)
V ⊤X
k
α2(1 + 2α)(4(k

k
V ⊤X

≥ k

− k

AX)

X ⊤
⊥
⊥
2
F − k
F ) + k2κ2
2
k

α2κ2
−
V ⊤XX ⊤V
F ).

k

F tr(C1)
2
F )

−

Lemma B.3. Follow the notations made in Lemma B.4, assume A = 1
maxl k

1), and let

1 (thus

A(l)

A

k2 ≤
W = (I

k

k2 ≤
XX ⊤)(At+1 −

−

Xskew(X ⊤(I

A)(X
˜X ˜X ⊤)(At+1 −

−

˜XB) + (I
A) ˜XB)

−

XX ⊤) ˜X ˜X ⊤(At+1 −

A) ˜XB

−

−

recalling from Section 3.2 of the main paper. Then it holds that E[W

X] = 0 and we can take

L
l=1 A(l) with

L P

κ2 = 8

and κ2

F = 96(k

V ⊤X

2
F + k

k

− k

− k

|
V ⊤ ˜X

2
F ).

k

Proof. Note that E[At+1] = A and E[B
E[W

XX ⊤)(E[At+1]

X] = (I

|
A)(X

|

−

Xskew(X ⊤(I

−

−
˜X ˜X ⊤)(E[At+1]

= 0.

−

˜XB) + (I
A) ˜XB)

−

−

X] = B. Then we have

XX ⊤) ˜X ˜X ⊤(E[At+1]

A) ˜XB

−

−

We now upper bound the spectral norm and Frobenius norm of W . First we rewrite it as

W = X

A)(X
˜X ⊤
⊥
Noting that B⊤B = BB⊤ = I, we get

(At+1 −
X ⊤
⊥
Xskew(X ⊤ ˜X

⊥

⊥

−
(At+1 −

˜XB) + X

⊥
A) ˜XB).

X ⊤
⊥

˜X ˜X ⊤(At+1 −

A) ˜XB

−

W

k

⊥

k2 ≤ k
k
(
k
8 , κ2,

(At+1 −
X
X ⊤
A)(X
−
⊥
˜X ⊤
Xskew(X ⊤ ˜X
(At+1 −
⊥
⊥
k2 +
X
k2)(
A
At+1k2 +
k
k

≤

k

˜XB)

⊥

X

k2 +
k
A) ˜XB)
k2
˜XB
k2) + 2(
k

≤

X ⊤
⊥

˜X ˜X ⊤(At+1 −

A) ˜XB

k2 +

At+1k2 +

k2)
A

k

while
2
W
F ≤

k

k

≤

≤

≤

˜XB)

⊥

⊥

⊥

(At+1 −

k
(
k
X

X ⊤
X
(
k
⊥
Xskew(X ⊤ ˜X
X ⊤
X
⊥
X ⊤ ˜X
˜XB
˜XB

⊥
k2k
X
−

(At+1 −
⊥kF k
kF +
k
2
F +
k

A)(X
−
˜X ⊤
(At+1 −
⊥
X
A)
k2k
−
˜X ⊤
(At+1 −
⊥
˜X
kF +
X ⊤
k
⊥
˜X
2
F +
X ⊤
k
⊥

kF +
A) ˜XB)
˜XB
kF +
A) ˜XB
X ⊤ ˜X
X ⊤ ˜X

X
k
kF )2
X
k
k2)2
⊥kF )2
2
F ).
⊥k

k
4(
k
12(
k

X

−

k

k

X ⊤
⊥

˜X ˜X ⊤(At+1 −

A) ˜XB

kF +

⊥k2k

˜X

X ⊤
⊥

˜X ⊤(At+1 −

kF k

A) ˜XB

k2 +

To proceed further, each of three items in above bracket needs to upper bounded. To this end,
2
note that B = arg minD k
F by the deﬁnition of B and Lemma A.5. Then if we let
C⋆ = arg minC k
V C
X
k
˜XB
X
k

2
F , we can get

V C⋆

˜XD

˜XD

−

−

−

k

k

X

−
2
F ≤ k
=
k
(
k
2(
k
4(k

X
k
2
F and D⋆ = arg minD k
˜XD⋆
2
X
F
k
V C⋆ + V C⋆
V C⋆
kF +
k
2
V C⋆
F +
k
k
2
F + k
2
F + k

≤
= 4(k

V ⊤X

X ⊤V

− k

X

X

−

≤

−

≤

−

k

− k

k

−
(Lemma A.5)
˜XD⋆
2
F
k
˜XD⋆
kF )2
˜XD⋆
2
F )
k
V ⊤ ˜X
2
F )
2
(orthogonal invariance)
F ).

−
V C⋆
−
V C⋆

−
C⋆⊤
V ⊤ ˜X

(Lemma A.5)

− k

k

− k

k

14

For other two items, noting that I = V V ⊤ + V

, we have

˜X

X ⊤
⊥

k

k

2
F =

⊥
(V V ⊤ + V
V V ⊤ ˜X
V

⊥

V ⊤
⊥
V ⊤
⊥
kF +
V ⊤ ˜X
V ⊤

) ˜X
2
F
k
X ⊤
V
k
⊥
⊥
k2 +
X ⊤
k
⊥
˜X
k2 +
k2k
˜X
kF )2
V ⊤
⊥
˜X
2
V ⊤
F )
k
k
⊥
2
F + k

k

kF k
kF k
kF +
k
2
V
F +
k
V ⊤X

k

≤

≤

X ⊤
⊥
X ⊤
⊥
X ⊤
⊥
X ⊤
⊥
X ⊤
⊥
X ⊤
⊥

k
(
k
(
k
(
≤
k
= (
k
2(
k
2(k

≤

V

V

≤

− k

V ⊤ ˜X

2
F ),

k

− k

˜X

V ⊤
⊥
V
⊥k2k
⊥ k2k

kF )2
V ⊤
⊥
V
⊥k2k

X ⊤

˜X

kF )2
V ⊤
⊥

˜X

kF )2

and similarly

Therefore, we get

X ⊤ ˜X

2
F ≤

⊥k

2(k

− k

V ⊤X

2
F + k

k

− k

V ⊤ ˜X

2
F ).

k

k

W

k

2
F ≤

k

96(k

− k

V ⊤X

2
F + k

k

− k

V ⊤ ˜X

k

2

F ) , κ2
F .

Lemma B.4. Assume A is an n
and the eigen-gap τ = λk −
A(l)
A
maxl k
k2 ≤
k2 ≤
with
W
k2 ≤
k

κ2 and

1 (thus

k2 ≤

k
W

k

n symmetric matrix with the eigenvalues λ1 ≥
×
λk+1 > 0. And it could be written as A = 1
1). Let W be an n
κF almost surely. Let X

λn
L
l=1 A(l) with
k stochastic zero-mean matrix (i.e., E[W ] = 0)

λ2 ≥ · · · ≥

St(n, k) and deﬁne

L P

×

∈
XX ⊤)A)X + αW, X ′ = Y (Y ⊤Y )−

1/2

∈

4 max

1
for some α
[0,
0,κF }
eigenvalues λ1 ≥
λ2 ≥ · · · ≥
eigenvectors corresponding to eigenvalues λk+1 ≥
V ⊤X
≥ k

2
F + 2ατ (
k

λk and accordingly V

V ⊤X ′

V ⊤X

2
F ]

E[

∈

k

k

{

St(n, k) consisting of A’s k eigenvectors corresponding to
k

k) consisting of A’s n

St(n, n

−

λn, then it holds that

−

⊥ ∈
λk+2 ≥ · · · ≥
2
F − k
k
F ) + k2κ2
2
F )
V ⊤X

X ⊤V V ⊤X

− k

k

−

2
F )
k
200
α2(31 + 10κ2)κ2
F
27

−

k
α2(1 + 2α)(4(k

Y = (I + α(I

−
]. If V

Proof. First, we have

V ⊤X ′

k

k

2
F = tr(X ′⊤V V ⊤X ′)
= tr((Y ⊤Y )−

= tr(Y ⊤V V ⊤Y (Y ⊤Y )−

1/2Y ⊤V V ⊤Y (Y ⊤Y )−
1).

1/2)

Using the deﬁnition of Y and the fact XX ⊤ + X
˜B1 + Z1 (cid:23)

0 where

X ⊤
⊥

⊥

= I, we have the expansion Y ⊤V V ⊤Y =

˜B1 = X ⊤(I + αAX
⊥
Z1 = αX ⊤(I + αAX

X ⊤
⊥
X ⊤
⊥

⊥

)V V ⊤(I + αX

)V V ⊤W + αW ⊤V V ⊤(I + αX

X ⊤
⊥

⊥

A)X + α2W ⊤V V ⊤W
A)X.

≻

X ⊤
⊥

⊥

Similarly, Y ⊤Y can be written as Y ⊤Y = ˜B2 + Z2 with
˜B2 = X ⊤(I + αAX
⊥
= I + α2X ⊤AX
⊥
Z2 = αX ⊤(I + αAX

)(I + αX
X ⊤
⊥
AX + α2W ⊤W
)W + αW ⊤(I + αX

0,

≻

⊥

X ⊤
⊥
X ⊤
⊥
X ⊤
⊥

⊥

A)X + α2W ⊤W

X ⊤
⊥

⊥

A)X.

Then we get

Note that W ⊤W

(cid:22)

V ⊤X ′

k

k

2

F = tr(( ˜B1 + Z1)( ˜B2 + Z2)−

1).

λmax(W ⊤W )I =
˜B2 (cid:22)

k
I + α2X ⊤AX

W

k

2
2I

≤ k

W

2
F I
k
≤
AX + α2κ2

κ2
F I. Thus,
F I , B2

X ⊤
⊥

⊥

15

0,

(12)

In addition, let

B1 , X ⊤(I + αAX

X ⊤
⊥

⊥

)V V ⊤(I + αX

X ⊤
⊥

⊥

A)X.

(13)

and note that α2W ⊤V V ⊤W

(cid:23)

0. Then by Lemma A.7 and A.1, we obtain

k

V ⊤X ′

tr((B1 + α2W ⊤V V ⊤W + Z1)(B2 + Z2)−
tr((B1 + Z1)(B2 + Z2)−
We now would like to apply Lemma7 A.3 for removing Z1 and Z2. Doing so needs to meet the
conditions of Lemma A.3. In fact,

2
F ≥
≥

1)

1)

k

•

•

•

•

Z1 and Z2 are stochastic zero-mean: Z1 and Z2 are linear functions of the stochastic zero-
mean matrix W . Thus they are stochastic zero-mean as well.

B1 and B2 are ﬁxed. This is true since no stochastic quantities are involved.
B2 + νZ2 (cid:23)
X
A
k2 ≤
k
k

[0, 1]. It’s easy to see that B2 (cid:23)
1
4 and α

1
4 , we get

X

1,

∈

0, and meanwhile since

3
8 I for all ν
k2 =
k
Z2k2 ≤
≤

k

⊥k2 = 1, ακF ≤
W ⊤(I + αX
2α
k
2α
k
2α
k

k2(
k
kF (1 + α)

W

W

I

≤

≤
A)X

⊥

X ⊤
⊥
X

k2
X ⊤

k2 + α
⊥ k2k
k
2ακF (1 + α)

⊥k2k

X

k2)
A
k2
k
1
(1 + α)
2

≤
≤
Z2k2. We thus could write Z2 (cid:23) −

≤

Z2k2)I

(cid:23)

3
8 I.

5
8

.

ρ(Z2)I

(cid:23)

Note that Z2 is symmetric and ρ(Z2)
v

Z2k2I. Then B2 + νZ2 (cid:23)

(1

−k
max

{k

Z1kF ,
k
Z1kF ≤
≤

Z2kF } ≤
2α
k
2α
k

k

k

−
5
2 ακF . Note that

k
W ⊤V V ⊤(I + αX

(1

≤ k
Z2k2)I
V

(cid:23)
− k
k2 = 1. Then
X ⊤
A)X
kF
⊥
k2 + α
I
k2(
k
k
5
1
)κF =
2α(1 +
2
4

V ⊤

X

⊥

≤

W ⊤

V

kF k

⊥k2k

k2k
2α(1 + α)κF ≤
Z1kF ≤
. Note that similar to right above, we could have

5
2 ακF .

⊥ k2k

ακF .

k
5(5+2κ2)
16

X ⊤

k2)
A
k

X

k2

Z1k2 ≤

k

5
2 ακ2.

Similarly, we could get

B1 + αZ1k2 ≤
Then

• k

B1k2 ≤ k
(
k
≤

V ⊤(I + αX
k2(
k

X ⊤
⊥
k2 + α
k

V ⊤

I

⊥

A)X

X

k
⊥k2k

2
2

k

Thus,

X ⊤

⊥ k2k

k2)
A
k

X

k2)2

≤

(1 + α)2.

B1 + αZ1k2 ≤ k

k

B1k2 + α
k
1
)2 +
(1 +
4

Z1k2 ≤
5
κ2 =
8

≤

(1 + α)2 +

5
2

ακ2

5(5 + 2κ2)
16

.

Thus, we have δ = 3

8 , β = 5

2 ακF and γ = 5(5+2κ2)

16

. Then by Lemma A.3, we get

E[

k

V ⊤X ′

2
F ]

k

≥

≥

E[tr((B1 + Z1)(B2 + Z2)−

1)]

tr(B1B−

1
2 )

= tr(B1B−

1
2 )

400
9
200
27

−

−

α2κ2

F (1 +

5(5 + 2κ2)
6
α2(31 + 10κ2)κ2
F .

)

7Note that this lemma was mistakenly applied in [24] since B1 and B2 are not ﬁxed. It could be rectiﬁed

by what we do here.

16

Moreover, by Lemma B.2, we arrive at

E[

k

V ⊤X ′

2
F ]

k

≥ k

V ⊤X

2
F + 2ατ (
k

k
α2(1 + 2α)(4(k

X ⊤V V ⊤X

V ⊤X

k
V ⊤X

2
F − k
F ) + k2κ2
2
F )

−

2
F )
k
200
α2(31 + 10κ2)κ2
F .
27

−

− k

k

k

2
F ≥

Lemma B.5. Let A and W be deﬁned as by our SVRRG-EIGS algorithm in Section 3.2 of the main
paper. Assume that A has the eigen-decomposition as deﬁned in Section 2.1 of the main paper, the
λk+1 > 0, and maxl k
eigen-gap τ = λk −
]
k2 ≤
1
2 . Then it holds that
and
k
V ⊤X
−
k
E[k
V ⊤X ′

2
F )(1
where the expectation is taken with respect to the random yt+1 for X ′ = X (t+1) conditioned on
X = X (t), and in addition c1 = 2( 1
(0, c0)
with

1. Further suppose that α = µτ

2µ(1 + 2µτ )(1 + 24k2)

3 µ) > 0 for any µ

c1µτ 2) + c2µ2τ 2(k

1
32√3k

V ⊤ ˜X

V ⊤X

A(l)

2
F ),

8 −

118400

− k

− k

− k

2
F ]

(0,

(k

≤

−

−

∈

∈

k

k

k

c0 = min
{

1
32√3kτ 2
and c2 = 96(k2(1 + 2µτ ) + 823).

1
c1τ 2 , −

,

(118406 + 144k2) + p(118406 + 144k2)2 + 18τ (1 + 24k2)
24τ (1 + 24k2)

> 0,

}

Proof. First by Lemma B.3, W is conditionally stochastic zero-mean. And 4 max
{
4 max
{
plied, and we have

1, κF } ≤
. Thus Lemma B.4 can be ap-

= 32√3k. Then α

1
1,κF }

32√3k ≤

1, √96

4 max

2k

≤

×

}

{

1

E[

k

V ⊤X ′

2
F ]

k

V ⊤X

2
F + 2ατ (
k

V ⊤X

k

≥ k

k
4α2(1 + 2α)(k

V ⊤X

− k

k

2
F − k
2
F )

−

2
F )

X ⊤V V ⊤X

−
α2(k2(1 + 2α) +

k

200
27

(31 + 10κ2))κ2
F .

Let σ be the minimum singular value of V ⊤X. Since
Lemma A.4 we have

k

V ⊤X

k2 ≤ k

V ⊤

k2k

X

k2 = 1, then by

E[

k

V ⊤X ′

2
F ]

k

V ⊤X

2
F + 2ατ

V ⊤X

k

2
F (1

k

−

k

k

≥ k

X ⊤V V ⊤X
2
V ⊤X
F

k

k

2
F

k

)

4α2(1 + 2α)(k

V ⊤X

2
F )

k

−

α2(k2(1 + 2α) +

≥ k

V ⊤X

2
F + 2ατ

k
4α2(1 + 2α)(k

V ⊤X

k

2
F (1

V ⊤X

2
F )

k

− k
−
α2(k2(1 + 2α) +

V ⊤X

2
F )

k

−

− k

− k
δ2
k k

−
200
27

200
27

(31 + 10κ2))κ2
F

(31 + 10κ2))κ2
F ,

and then by Lemma B.3,

E[k

V ⊤X ′

2
F ]

k

− k

k

− k

V ⊤X

2
F −

k

2ατ

≤

σ2
k k

V ⊤X

2
F (1

k

− k

V ⊤X

k

F ) + 4α2(1 + 2α)(k
2

V ⊤X

2
F ) +

k

− k

96α2(k2(1 + 2α) +

200
27

(31 + 80))(k

V ⊤X

2
F + k

k

− k

V ⊤ ˜X

2
F )

k

− k

= (k

V ⊤X

2
F )(1

k

−

(2α

− k

τ σ2
k k

V ⊤X

2
F −

k

4α2(1 + 2α)

−

96α2(k2(1 + 2α) +

200
27

(31 + 80))))

+ 96α2(k2(1 + 2α) +

200
27

(31 + 80))(k

V ⊤ ˜X

2
F )

k

− k

= (k

V ⊤X

2
F )(1

k

−

2α(

− k

τ σ2
k k

V ⊤X

+ 96α2(k2(1 + 2α) +

7400
9

)(k

− k

2
F −
k
V ⊤ ˜X

2
F ).

k

2α(1 + 2α)(1 + 24k2)

118400
3

−

α))

17

V ⊤X

Note that
k
2
F < k
V ⊤X
k
1
2 . Furthermore,
σ

−

k

k2 ≤
1 + 1

k

≥
E[k

V ⊤X ′

2
F ]

k

− k

1

k

2 = k

1 implies the singular values of V ⊤X fall into [0, 1].
1
2
2 , which contradicts the assumption
2 = k
F ≥
−
1
2
k
V ⊤X
F ≥
2 −
2 ≥
1
8
7400
9

k
k
2 . We thus get

2α(1 + 2α)(1 + 24k2)

+ 96α2(k2(1 + 2α) +

2 + k

V ⊤ ˜X

2
F )(1

V ⊤X

V ⊤X

2
F ).

2α(

− k

)(k

− k

(k

−

−

−

≤

k

k

k

τ

−

If σ < 1

2 then
1
2 . Thus,

k

−

118400
3

α))

Since α = µτ , 0 < µ

≤

1

32√3kτ 2 . Then
1
8
= 2µτ 2(

2α(

−
1
8 −

τ

2α(1 + 2α)(1 + 24k2)

−
2µ(1 + 2µτ )(1 + 24k2)

α)

118400
3
118400
3

−

µ),

and

Further let

96α2(k2(1 + 2α) +

7400
9

) = 96µ2τ 2(k2(1 + 2µτ ) +

7400
9

).

c1 = 2(

1
8 −
c2 = 96(k2(1 + 2µτ ) +

2µ(1 + 2µτ )(1 + 24k2)

7400
9

).

118400
3

µ),

−

We then arrive at

E[k

V ⊤X ′

2
F )(1
And solving the equation c1 = 0, ensuring c1µτ 2 < 1, together with the assumption about µ made
in this lemma, yields µ

c1µτ 2) + c2µ2τ 2(k

(0, c0) with

V ⊤X

2
F ).

− k

− k

− k

2
F ]

(k

−

≤

V ⊤ ˜X

k

k

k

∈
1
c1τ 2 , −
which simultaneously satisﬁes c1 > 0, c1µτ 2 < 1, and µ

1
32√3kτ 2

c0 = min
{

,

(118406 + 144k2) + p(118406 + 144k2)2 + 18τ (1 + 24k2)
24τ (1 + 24k2)

> 0,

}

1

32√3kτ 2 .

≤

B.2 One Epoch Analysis

We now solve the stochastic recurrence relation for a single epoch8 of our SVRRG-EIGS algorithm.
2
In this subsection, we still assume that maxl k
F
and ˜b = k
k
− k
1
2 , then
bt
1 ≤

k
F (note that X (0) = ˜X). Then by Lemma B.5 we have that if µ < c0 and

1. Let ˜X = ˜X (s
−

1), bt = k

V ⊤X (t)

V ⊤ ˜X

k2 ≤

A(l)

− k

−

2

X (t
where the expectation is taken with respect to the random yt for X (t).
Lemma B.6. Assume X (0) is ﬁxed and ˜b
2 . Let b0 = ˜b and Et =
Then

c1µτ 2)bt
−

E[bt|

1)]

(1

≤

≤

−

1 + c2µ2τ 2˜b,

{

−

1

bt′

≤

1
2 : t′ = 0, 1, 2,

, t

.

}

· · ·

E[bm|

Em]

≤

((1

−

c1µτ 2)m +

µ)˜b.

c2
c1

Proof. We need to examine the evolution of bt as a function of t, while bt itself is a deterministic
function of X (t) and X (t) = RX (t−1) (α ˜G(yt, X (t
−

1))). Then we have

E[bt|

X (t

−

1), Et] = E[bt|
E[bt|
(1
−

≤

≤

−

X (t

1), Et
−
1), Et
X (t
−
−
c1µτ 2)[bt
1|
−

1, bt ≤
1]
X (t

−

1
2

]

1), Et
−

1] + c2µ2τ 2˜b.

8Note that the proofs of Lemma B.6-B.7 are a bit different from those in [24], without similar errors.

18

Taking expectation over X (t
recursion and noting that ˜b is ﬁxed, we have

−

1) (on behalf of the ﬁltration

1) on both sides, unwinding the

Ft
−

E[bt|

Et]

≤

≤

≤

(1

(1

−

−

c1µτ 2)E[bt
−
c1µτ 2)2E[bt
−

1|

1] + c2µ2τ 2˜b

Et

−

2] + c2µ2τ 2˜b

Et

−

2|

1

(1

X
i=0

−

c1µτ 2)i

≤ · · ·

(1

c1µτ 2)tE[b0|

−

E0] + c2µ2τ 2˜b

1

t
−
X
i=0

(1

c1µτ 2)i

−

= (1

(1

≤

= (1

−

−

−

c1µτ 2)t˜b + c2µ2τ 2˜b

1

t
−
X
i=0

(1

−

c1µτ 2)i

c1µτ 2)t˜b + c2µ2τ 2˜b

c1µτ 2)t˜b + c2µ2τ 2˜b

∞
X
i=0
1

(1

c1µτ 2)i

−

c1µτ 2 = ((1

c1µτ 2)t +

−

µ)˜b.

c2
c1

Setting t = m above completes the proof.

We now need to show that the event Em occurs w.h.p. so that Lemma B.6 makes sense in practice.
Lemma B.7. Assume µ < c0. Then for any ̺

(0, 1) and m, if

∈
˜b + c3kmµ2τ 2 + c5kpmµ2τ 2 log(1/̺)

1
2

,

≤

then it holds that the event Em (i.e., bt ≤
least 1

̺.

−

1
2 for all t = 0, 1, 2,

· · ·

, m) occurs with probability at

, bm as a stochastic process induces a super-martingale with
Proof. The key here is that b0, b1,
respect to the ﬁltration
Ft
{Ft}
for brevity), and thus is amenable to a concentration of measure argument. In fact, according to the
proof of Lemma B.5 and noting that

· · ·
about random draws yt (note that X (t) is used on behalf of

=

F

X ⊤V V ⊤X

kF ≤ k

X ⊤V

k2k

V ⊤X

kF ≤ k

V ⊤X

kF ,

k

we have

E[bt+1|

X (t)]

≤

V ⊤X

2ατ (
k

bt −
4α2(1 + 2α)(k

k

2
F − k
V ⊤X

− k

X ⊤V V ⊤X

2
F ) +
k
F ) + α2(k2(1 + 2α) +
2
7400
9

k

bt + 4α2(1 + 2α)k + 192α2(k2(1 + 2α) +

)k

≤
= bt + c3kµ2τ 2

200
27

(31 + 10κ2))κ2
F

where c3 = 4(1+2µτ )+192(k2(1+2µτ )+ 7400
Note that 0
thus the natural continuation can be applied to arrive at an inﬁnite sequence such that

9 ). Deﬁne Ψt = bt−
· · ·

, m.
, m} is a ﬁnite sequence of random variables and

c3kµ2τ 2t for t = 0, 1, 2,

k, and {Ψt : t = 0, 1, 2,

bt ≤

· · ·

≤

Ψt| ≤

|

bt + c3kµ2τ 2m

≤

k + c3kµ2τ 2m

for any t including t > m. Meanwhile, we have
1)] = E[bt|
E[Ψt|
bt
≤
= bt

X (t

−

−

c3kµ2τ 2t

1)]

X (t

−
−
1 + c3kµ2τ 2

−
c3kµ2τ 2(t

c3kµ2τ 2t
1) = Ψt

−

1.

−

1 −

−

19

Thus,

Ψt}

{

|

Ψt| ≤ |k

is a super-martingale. Furthermore, by Lemma A.6, we have
Ψt+1 −

2
F |
k
20kµτ
1
5c0τ
−
5c0τ + c0c3τ . Now we are able to apply Azuma-Hoeffding inequality and have that

V ⊤X (t+1)
20kµτ
5µτ
1

2
F − k
+ c3kµ2τ 2

+ c3c0kµτ 2 = c4kµτ

+ c3kµ2τ 2

V ⊤X (t)

≤

−

≤

k

where c4 = 20
for any t

1
0 and a > 0

−

≥

P (Ψt −

Ψ0 ≥

a)

exp

{−

exp

{−

≤

≤

a2

t
s=1(c4kµτ )2 }
2 P
a2
2c4mk2µ2τ 2 }

, ̺

= exp

{−

a2
2c4tk2µ2τ 2 }

(0, 1).

where ̺
Solving ̺ = exp
c5kpmµ2τ 2 log(1/̺) with c5 = √2c4. Therefore, we get that Ψt −

a2
2c4mk2µ2τ 2 }

{−

∈

Ψ0 < a , i.e.,

with respect

to a yields a =

˜b + c3kµ2τ 2t + a

bt ≤

≤

˜b + c3kmµ2τ 2 + c5kpmµ2τ 2 log(1/̺)

, m, with probability at least 1
1

≤
̺. Note that the condition ˜b +
−
2 implies that ˜b < 1
2 . Then it’s reduced to c3kmµ2τ 2 +
(0, c0)

˜b, which can be satisﬁed by using a sufﬁciently small µ

≤

1
2

for all t = 0, 1, 2,
c3kmµ2τ 2 + c5kpmµ2τ 2 log(1/̺)
c5kpmµ2τ 2 log(1/̺)
when ̺ is set properly, e.g., ̺ = exp

1
2 −

· · ·

≤

∈

.

1/µ

}

{−

Lemma B.8. Fix conﬁdence parameters ̺, ϑ
and

∈
˜b + c3kmµ2τ 2 + c5kpmµ2τ 2 log(1/̺)

1
2

.

≤

(0, 1

2 ) and assume that µ, m are set such that µ < c0

Then it holds that with probability at least 1

(̺ + ϑ),

−

bm ≤

1
ϑ

c1µτ 2)m +

((1

−

µ)˜b.

c2
c1

Proof. By Markov inequality, we get
E[bm|
ϑ

P (bm ≥
while by Lemma B.6, we have

Em]

)

≤

E[bm|

Em]/

Em]

E[bm|
ϑ

= ϑ,

E[bm|

Em]

≤

((1

−

c1µτ 2)m +

µ)˜b.

c2
c1

Thus,

((1

1
ϑ

c1µτ 2)m +

c2
P (bm ≥
c1
ϑ, bm ≤
That is, with probability at least 1
bining with Lemma B.7, we get that bm ≤
1
−

(̺ + ϑ).

−

−

B.3 One Run Analysis

µ)˜b

|
1
ϑ ((1
1
ϑ ((1

Em)

≤

P (bm ≥
c1µτ 2)m + c2
c1
c1µτ 2)m + c2
c1

−
−

)

ϑ.

Em]

E[bm|
ϑ
µ)˜b conditioned on Em. Com-
µ)˜b with probability at least

≤

We now proceed to the analysis on one complete run of our SVRRG-EIGS algorithm. Again, assume
2
that maxl k
2 . Then by Lemma B.8,
for any ̺, ϑ

F and assume that ˜b0 < 1
such that

V ⊤ ˜X (s)
ϑ2

k
), and m

k2 ≤
(0, 1

A(l)

1. Let ˜bs = k
(0, min
{

2 ), µ

− k
c0, c1
2c2

3 log(1/ϑ)
c1µτ 2

∈

∈
}
˜b0 + c3kmµ2τ 2 + c5kpmµ2τ 2 log(1/̺)

≥

1
2

,

≤

20

we have that

bm ≤

((1

−

c1µτ 2)m +

c2
c1

µ)˜b0

1
ϑ
1
ϑ

−

≤

−

((1

3 log(1/ϑ)

ϑ2)˜b0

c1
2c2

c1µτ 2)

c1µτ 2 +

(̺+ ϑ). Note that 1 + x
x log(1

c2
c1
for any x and hence log(1
with probability at least 1
exp
x
}
{
for any 0 < x < 1 which in turn induces a
a, i.e., (1
x)
x)
≤ −
0 < x < 1 and a > 0. Since 0 < c1µτ 2 < 1 by µ < c0, we can write
c2
bm ≤
c1
Noting that ˜b1 = bm, we get ˜b1 ≤
fashion, since ˜b1 ≤

1
2
˜b0 with probability at least 1

c1
2c2
ϑ˜b0 ≤

ϑ2)˜b0 = ϑ(ϑ +

ϑ2)˜b0 ≤

˜b0 and thus

c1µτ 2 +

c1µτ 2)

(ϑ3 +

≤
−

exp

3 log(1/ϑ)

1
ϑ

1
ϑ

((1

1
2

−

−

≤

−

a
x

x)
x
≤ −
, for any

−
a
}

{−

1
2

.

ϑ˜b0 ≤
)˜b0 ≤
(̺ + ϑ). In a similar

1
˜b1 + c3kmµ2τ 2 + c5kpmµ2τ 2 log(1/̺)
2
≤
ϑ2˜b0 with probability at least
ϑ˜b1 ≤
(̺ + ϑ), conditioned on the ﬁrst epoch. If conditioned on the initial setting, we then have
ϑ2˜b0 with probability at least 1
2(̺ + ϑ) provided that ̺, ϑ
4 ). In this way, we can see

we can apply Lemma B.8 on the second epoch and get ˜b2 ≤
1
−
˜b2 ≤
that repeating above process till the T -th epoch will result in

(0, 1

−

∈

,

k

− k

V ⊤ ˜X (T )

2

F = ˜bT ≤

k

ϑT ˜b0 < ϑT

with probability at least 1
solving ϑT

−
ε for T tells that T =

T (̺ + ϑ), conditioned on the initial setting and ̺, ϑ

2T ). Then
epochs sufﬁce to achieve any aimed accuracy

∈

(0, 1

≤
(0, 1) for k

ε

V ⊤ ˜X (T )
To simplify these expressions, setting ̺ = ϑ = ϕ

2
F ≤

− k

∈

k

ε with probability at least 1

2 leads to

log(1/ε)
log(1/ϑ) ⌉

⌈

log(1/ε)
log(1/ϑ) ⌉

− ⌈

(̺ + ϑ).

log(1/ε)
log(1/ϑ) ⌉

⌈

(̺ + ϑ) =

log(1/ε)
log(2/ϕ) ⌉

⌈

ϕ

log(1/ε)
log(2) ⌉

ϕ =

⌈

≤ ⌈

Accordingly, the initial conditions become ϕ
3 log(2/ϕ)
c1µτ 2

and

(0,

1
log2(1/ε)
⌈

⌉

∈

), µ

∈

With the assumption ˜b0 < 1

˜b0 + c3kmµ2τ 2 + c5kpmµ2τ 2 log(2/ϕ)
≤
2 , we could rewrite the above inequality as

1
2

.

ϕ.

log2(1/ε)
⌉
c0, c1
8c2

(0, min
{

), m

ϕ2

}

≥

c3kmµ2τ 2 + c5kpmµ2τ 2 log(2/ϕ)

≤

˜b0.

1
2 −
(0,

2
F ≤

ε with probability at least 1

Now we can conclude that, for any ε
V ⊤ ˜X (T )
k
of our SVRRG-EIGS algorithm, if the following conditions are satisﬁed:
c1
8c2

log2(1/ε)
⌉

(0, 1) and any ϕ

(0, min
{

˜b0 <

, α

A(l)

max

c0τ,

l k

− ⌈

1
2

1,

∈

∈

∈

k

1
log2(1/ε)
⌈
ϕ by running T =

τ ϕ2

), we have k
log(1/ε)
log(2/ϕ) ⌉

−
epochs

⌉

⌈

k2 ≤
3 log(2/ϕ)
c1ατ

m

≥

,

c3kmα2 + c5kpmα2 log(2/ϕ)

≤

),

}
1
2 −

˜b0,

where the positive constants are

c0 = min
{
1
8

c1 =

2
τ

(

τ

−

,

1
32√3kτ 2

1
c1τ 2 , −
2α(1 + 2α)(1 + 24k2)

c3 = 4(1 + 2α) + 192(k2(1 + 2α) +

(118406 + 144k2) + p(118406 + 144k2)2 + 18τ (1 + 24k2)
24τ (1 + 24k2)

,

}

118400
3

−
7400
9

α),

c2 = 96(k2(1 + 2α) + 823),

+ c0c3τ,

c5 = √2c4.

),

c4 =

20
5c0τ

−

1

21

References

[1] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix man-

ifolds. Princeton University Press, 2008.

[2] A. Anonymous, B. Anonymous, C. Anonymous, and D. Anonymous. Matrix eigendecomposi-
tion via doubly stochastic riemannian optimization. In Proceedings of the 33nd International
Conference on Machine Learning, ICML 2016, New York City, New York, 19-24 June 2016,
2016.

[3] Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremen-
tal pca. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,
Advances in Neural Information Processing Systems 26, pages 3174–3182. Curran Associates,
Inc., 2013.

[4] Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Trans. Automat.

Contr., 58(9):2217–2229, 2013.

[5] Jane K. Cullum and Ralph A. Willoughby. Lanczos Algorithms for Large Symmetric Eigen-
value Computations, Vol. 1. Society for Industrial and Applied Mathematics, Philadelphia, PA,
USA, 2002.

[6] Petros Drineas and Michael W. Mahoney. On the nyström method for approximating a gram
matrix for improved kernel-based learning. J. Mach. Learn. Res., 6:2153–2175, December
2005.

[7] Alan Edelman, Tomás A. Arias, and Steven T. Smith. The geometry of algorithms with orthog-

onality constraints. SIAM J. Matrix Anal. Appl., 20(2):303–353, April 1999.

[8] Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In Proceedings of
the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July
2015, pages 560–568, 2015.

[9] Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.). Johns Hopkins

University Press, Baltimore, MD, USA, 1996.

[10] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217–288,
May 2011.

[11] Reza Harikandeh, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Koneˇcný,
and Scott Sallinen. Stopwasting my gradients: Practical svrg. In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing
Systems 28, pages 2251–2259. Curran Associates, Inc., 2015.

[12] Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex J Smola. On variance
reduction in stochastic gradient descent and its asynchronous variants.
In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information
Processing Systems 28, pages 2647–2655. Curran Associates, Inc., 2015.

[13] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-
berger, editors, Advances in Neural Information Processing Systems 26, pages 315–323. Cur-
ran Associates, Inc., 2013.

[14] I. T. Jolliffe. Principal component analysis. Hardcover, October 2002.

[15] Ritesh Kolte, Murat Erdogdu, and Ayfer Ozgür. Accelerating svrg via second-order informa-

tion. In NIPS Workshop on Optimization for Machine Learning, 2015.

[16] Jason Lee, Tengyu Ma, and Qihang Lin. Distributed stochastic variance reduced gradient

methods. CoRR, abs/1507.07595, 2015.

[17] John M. Lee. Introduction to smooth manifolds. Springer, 2012.

22

[18] A. S. Lewis. Convex analysis on the hermitian matrices. SIAM Journal on Optimization,

6:164–177, 1996.

[19] Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms

and Probabilistic Analysis. Cambridge University Press, New York, NY, USA, 2005.

[20] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an
In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural

algorithm.
Information Processing Systems 14, pages 849–856. MIT Press, 2002.

[21] Beresford N. Parlett. The Symmetric Eigenvalue Problem. Prentice-Hall, Inc., Upper Saddle

River, NJ, USA, 1998.

[22] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical
Recipes 3rd Edition: The Art of Scientiﬁc Computing. Cambridge University Press, New York,
NY, USA, 3 edition, 2007.

[23] Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabás Póczos, and Alexander J. Smola.
Stochastic variance reduction for nonconvex optimization. CoRR, abs/1603.06160, 2016.

[24] Ohad Shamir. Fast stochastic algorithms for svd and pca: Convergence properties and convex-

ity. arXiv preprint arXiv:1507.08788, 2015.

[25] U. Torbjorn Ringertz. Eigenvalues in optimum structural design. Institute for Mathematics

and Its Applications, 92:135, 1997.

[26] Chong Wang, Xi Chen, Alex J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 181–189.
Curran Associates, Inc., 2013.

[27] Zaiwen Wen, Chao Yang, Xin Liu, and Yin Zhang. Trace-penalty minimization for large-scale
eigenspace computation. Technical report, RICE UNIV HOUSTON TX DEPT OF COMPU-
TATIONAL AND APPLIED MATHEMATICS, 2013.

[28] Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints.

Math. Program., 142(1-2):397–434, 2013.

[29] J.H. Wilkinson. The Algebraic Eigenvalue Problem. Monographs on numerical analysis.

Clarendon Press, 1988.

23

