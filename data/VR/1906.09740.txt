Gaze-ContingentOcularParallaxRenderingforVirtualRealityROBERTKONRAD,StanfordUniversityANASTASIOSANGELOPOULOS,StanfordUniversityGORDONWETZSTEIN,StanfordUniversityFixated CenterFixated LeftFig.1.Thecentersofrotationandprojectionoftheeyesarenotthesame.Asaconsequence,smallamountsofparallaxarecreatedintheretinalimageaswefixateondifferentobjectsinthescene.Thenodalpointsoftheeye,representingthecentersofprojection,areshownassmallbluecirclesontheleftalongwitharaydiagramillustratingtheopticalmechanismofocularparallax.Simulatedretinalimagesthatincludethefalloffofacuityintheperipheryofthevisualfieldareshownontheright.Asauserfixatesonthecandleinthecenterofthescene(center,redcircleindicatesfixationpoint),thebottleispartlyoccludedbythecandle.Astheirgazemovestotheleft,ocularparallaxrevealsthebottlebehindthecandleinthecenter(right).Ocularparallaxisagaze-contingenteffectexhibitingthestrongesteffectsizeinneartomidperipheralvision,wherevisualacuityislowerthaninthefovea.Inthispaper,weintroduceocularparallaxrenderingforeye-tracking-enabledvirtualrealitydisplays,andstudythecomplexinterplaybetweenmicroparallax,occlusion,visualacuity,andotherperceptualaspectsofthistechnologyinsimulationandwithaseriesofuserexperiments.Immersivecomputergraphicssystemsstrivetogenerateperceptuallyreal-isticuserexperiences.Current-generationvirtualreality(VR)displaysaresuccessfulinaccuratelyrenderingmanyperceptuallyimportanteffects,in-cludingperspective,disparity,motionparallax,andotherdepthcues.Inthispaperweintroduceocularparallaxrendering,atechnologythataccuratelyrenderssmallamountsofgaze-contingentparallaxcapableofimprovingdepthperceptionandrealisminVR.Ocularparallaxdescribesthesmallamountsofdepth-dependentimageshiftsontheretinathatarecreatedastheeyerotates.Theeffectoccursbecausethecentersofrotationandprojectionoftheeyearenotthesame.Westudytheperceptualimplicationsofocularparallaxrenderingbydesigningandconductingaseriesofuserexperiments.Specifically,weestimateperceptualdetectionanddiscrimina-tionthresholdsforthiseffectanddemonstratethatitisclearlyvisibleinmostVRapplications.Additionally,weshowthatocularparallaxrenderingprovidesaneffectiveordinaldepthcueanditimprovestheimpressionofrealisticdepthinVR.AdditionalKeyWordsandPhrases:computationaldisplays,virtualreality,augmentedreality,eyetracking,gaze-contingentrendering1INTRODUCTIONImmersivecomputergraphicssystems,suchasvirtualreality(VR)displays,aimatsynthesizingaperceptuallyrealisticuserexperience.Toachievethisgoal,severalcomponentsarerequired:interactive,photorealisticrendering;ahigh-resolution,low-persistence,stereo-scopicdisplay;andlow-latencyheadtracking.ModernVRsystemsprovideallofthesecapabilitiesandcreateexperiencesthatsupportAuthors’addresses:RobertKonrad,StanfordUniversity,rkkonrad@stanford.edu;Anas-tasiosAngelopoulos,StanfordUniversity,nikolasa@stanford.edu;GordonWetzstein,StanfordUniversity,gordon.wetzstein@stanford.edu.many,butnotall,ofthemonocularandbinoculardepthcuesofthehumanvisualsystem,includingocclusions,shading,binoculardisparity,andmotionparallax.Thesupportoffocuscues(e.g.accom-modationandretinalblur)hasalsoreceivedattentioninresearchandindustryoverthelastfewyears.Inthispaper,westudyadepthcueofhumanvisionthathasnotbeendiscussedinthecontextofvirtualrealityandthatmayhelpfurtherimprovedepthperceptionandperceptualrealism:ocularparallax.Thecentersofrotationandprojectioninthehumaneyearenotthesame.Therefore,changesingazedirectioncreatesmallamountsofdepth-dependentimageshiftsonourretina—aneffectknownasocularparallax.ThisdepthcuewasfirstdescribedbyBrewster[1845]andithasbeendemonstratedtoproduceparallaxwellwithintherangeofhumanvisualacuity[Bingham1993;Hadanietal.1980;MappandOno1986].Interestingly,speciesasdiverseasthechameleonandthesandlancecriticallyrelyonthisdepthcuetojudgedistance[Land1995;Pettigrewetal.1999].TorenderocularparallaxintoaVR/ARexperience,eyetrackingisrequired.Conveniently,manyemergingwearabledisplaysystemsalreadyhaveeyetrackingintegrated,eithertosupportfoveatedrendering[Guenteretal.2012;Patneyetal.2016],accurateregistra-tionofphysicalanddigitalimagesinAR,orothergaze-contingentdisplaymodes.Witheyetrackingavailable,thereisnoadditionalcomputationalcosttointegrateocularparallax.Theperspectiveoftherenderedimagesimplychangesdependingonthegazedirection.However,themagnitudeofdepth-dependentmotioninducedbyocularparallaxrenderingincreasesintheperipheryofthevisualarXiv:1906.09740v2  [cs.GR]  13 May 20202•Konrad,R.etalfield,wherevisualacuityislowerthaninthefovea.Moreover,theresolutionofferedbycurrent-generationVRdisplaysiswellbelowthevisualacuityofhumanvisionanditisnotclearifthesubtleocularparallaxeffectisperceivableinVRatall.Tofurtherourunderstandingofocularparallaxanditspercep-tualeffectsinVR,wethoroughlyanalyzethetradeoffsbetweenperceivedparallax,visualacuity,anddisparityfornear-eyedisplays.Webuildaprototypegaze-trackedVRdisplay,conductaseriesofuserexperimentsthatquantifyeffectsizesofocularparallaxren-dering,andmeasureitsimpactondepthperceptionandtheuserexperienceingeneral.Wefindthatdetectionthresholdsforocularparallaxrenderingarealmostanorderofmagnitudelowerthanthevisualacuityatthesameextrafoveallocus,whichverifiesthatoursensitivitytosmallamountsofdifferentialmotioniswellbelowtheacuitylimit,especiallyintheperipheryofthevisualfield[MckeeandNakayama1984].Wealsoshowthattherelativeocularparallaxofobjectswithrespecttoabackgroundtargetcanbediscriminatedaccuratelyevenforrelativelysmallobjectdistancesthatfallwellwithinthedepthrangesofmostvirtualenvironments.Furthermore,weshowthatocularparallaxrenderingprovidesaneffectiveordinaldepthcue,helpingusersbetterdistinguishtherelativedepthorder-ingofascene,butthatitdoesnotnecessarilybenefitabsolute,ormetrical,distanceestimatestoobjects.Finally,weshowthatocularparallaxrenderingimprovestheimpressionofrealisticdepthina3Dscene.Fornoadditionalcomputationalcost,ocularparallaxrenderinghasthepotentialtoimprovebothdepthperceptionandperceptualrealismofeye-trackedAR/VRsystems.Specifically,wemakethefollowingcontributions:•Weintroducegaze-contingentocularparallaxrenderingforVRdisplaysystems.•Wedesignandconductuserexperimentstoquantifydetec-tionanddiscriminationthresholdsofocularparallaxrender-ing.•Wedesignandconductuserexperimentstoquantifytheeffectivenessofocularparallaxrenderingasbothanordinalandabsolutedepthcue.•Weconductauserexperimentthatdemonstratesimprovedperceptualrealismusingocularparallaxrendering.2RELATEDWORKDepthCues.Humandepthperceptionreliesonavarietyofcues[HowardandRogers2002;Palmer1999].Manyofthesecuesarepictorialandcanbesynthesizedusingphotorealisticrenderingtech-niques,includingocclusions,perspectiveforeshortening,textureandshadinggradients,aswellasrelativeandfamiliarobjectsize.Unlikeconventional2Ddisplays,head-mounteddisplays(HMDs)usestereoscopicdisplaysandheadtrackingandcanthussupporttwoadditionalvisualdepthcues,disparityandmotionparallax,aswellasoneoculomotorcue,vergence.Emergingnear-eyedisplaysalsosupportvisualfocuscueslikeretinalblurandchromaticaber-rations,whichinturndriveaccommodation,anotheroculomotorcue(seediscussionbelow).Allofthesecuesareimportantforhu-mandepthperceptiontovaryingdegreesdependingonthefixationdistance[CuttingandVishton1995].Studyingvisualcues,suchasdisparity[Didyketal.2011]ormotionparallax[Kellnhoferetal.2016],andtheirimpactoncomputationaldisplayapplicationshasbeenanintegralpartofgraphicsresearch.Inthiswork,weexploreocularparallaxasanothervisualcuethatmayimprovetheuserexperienceinimmersivecomputergraphicsapplications.OcularParallax.Ocularparallaxdescribesthechangeinperspec-tiveastheeyerotates,primarilyduetotheuserfixatingondifferentpartsofthescene.Thisvisualcueiswellknown[Brewster1845]andhasameasurableeffectondepthperception[Bingham1993;KudoandOhnishi1998;Kudoetal.1999;MappandOno1986].Similartoothermonocularvisualcues,suchasretinalblurandchromaticaberration,thechangeoftheretinalimagecausedbyocularparal-laxmaybesmall.Nonetheless,supportingallofthesecueswithanHMDcanimprovevisualcomfort[Hoffmanetal.2008],perceivedrealism,andtheuserexperienceasawhole.KudoandOhnishi[2000]discussgaze-contingentopticaldistor-tionsinhead-mounteddisplays(HMDs)andattributetheminparttoocularparallax.Thiseffectiscommonlyknownas“pupilswim”.However,theydidnotproposeocularparallaxrenderingforHMDsorstudyitsperceptualeffectswithHMDs.Buildingonemerginghead-mounteddisplayswitheye-trackingtechnology,toourknowl-edgewearethefirsttoproposeocularparallaxasagaze-contingentrenderingmodeforVR/ARandevaluateitsperceptualimplicationswithaseriesofuserexperiments.Gaze-contingentandComputationalDisplays.Eyetrackingen-ablesgaze-contingentrenderingtechniquesthatadapteffectslikemagnification,stylization,orgeometriclevel-of-detailtotheuser’sviewpoint[Duchowskietal.2004].Gaze-contingentrenderingisbecominganintegralpartofmodernnear-eyedisplaysystems,en-ablingtechniquessuchasfoveatedrendering[Guenteretal.2012;Patneyetal.2016],andgaze-contingentvarifocal[Dunnetal.2017;Johnsonetal.2016;Konradetal.2015;Liuetal.2008;Padmanabanetal.2017]ormultifocal[Akeleyetal.2004;Mercieretal.2017;Rollandetal.2000]displays.Althoughrenderingaccommodation-dependenteffects,suchaschromaticaberrations[Cholewiaketal.2017]andbluratdepthedges[Marshalletal.1996;Zannolietal.2014],havenotbeendirectlyevaluatedwitheye-trackeddisplays,thesetechniquescouldbeoptimizedbytrackingtheuser’sgazeoraccommodation.Ocularparallaxrenderingiscomplimentarytothesetechniquesandcouldbeintegrated,withoutcomputationaloverhead,intoconventionalHMDswitheyetrackingandoptionallycombinedwithothergaze-contingentrenderingalgorithms.Otherthantheproposedmethod,theonlytechniquesdescribedintheliteraturethatinherentlyprovideocularparallaxcuesarenear-eyemultifocaldisplays[HuandHua2014;Llulletal.2015;Loveetal.2009;Narainetal.2015],lightfielddisplays[HuaandJavidi2014;Huangetal.2015;LanmanandLuebke2013],andholo-graphicdisplays[Maimoneetal.2017;Padmanabanetal.2019].However,theeffectofocularparallaxorlackthereofhasnotbeeninvestigatedinanyoftheaforementionedtechnologies.Infact,oc-ularparallaxinmultifocaldisplaysisoftenundesirablebecauseitrevealsmisalignmentsbetweenthevirtualimageplanes;Mercieretal.[2017]proposedamultifocaldisplaythateffectivelyremovestheocularparallaxcuebyshiftingthedecomposedlayersaccordingtothetrackedpupilposition.Gaze-ContingentOcularParallaxRenderingforVirtualReality•3Fig.2.Illustrationofaschematiceye,includingthefrontandrearnodalpointsN,N′,thecenterofrotationC,andtheanteriorvertexofthecorneaV.Thenodalpointsaretwoparametersofathicklensmodelthatrefractslightraysasdepicted.Theexactlocationsofthesepointsdependontheschematiceyemodelused.3OCULARPARALLAXInthissection,wediscussschematicmodelsofthehumaneyewithaspecificfocusonhowtheymodelthecentersofprojectionandrotation.Moreover,wediscussandanalyzethetrade-offbetweenocularparallaxandimagesharpness,accountingforthedecreaseinperipheralvisualacuityaswellasretinalblurduetoaccommoda-tion.EyeModels.ComplexopticalsystemswithmultiplerefractivesurfacescanbereducedtosixcardinalpointsthatfullydefinetheGaussianimagingandmagnificationpropertiesofthesystem.Thesecardinalpointsincludethefrontandrearfocalpoints,thefrontandrearprinciplepoints,andthefrontandrearnodalpoints,NandN′.Forthepurposeofmodelingocularparallax,weonlyre-quirethefrontnodalpointN,whichisthecenterofprojectionoftheeye,aswellasthecenterofrotationC(seeFigure2).Severalschematiceyemodelshavebeenproposedintheliterature,eachlistingslightlydifferentvaluesforthecardinalpoints[Atchison2017].SomeofthemostpopularmodelsincludetheGullstrandnumber1,theGullstrand-Emsley,andtheEmsleyreducedeyes,whicharemodelsofdecreasingcomplexity.WeoutlinethelocationoftheirrespectivenodalpointsandcentersofrotationinTable1.Thecenterofrotationoftheeyewasmeasuredtobe14.7536mmfromthecornea,onaverage,foremmetropicsubjects[FryandHill1962].Althoughthespecificlocationsofthecardinalpointsareslightlydifferentforeacheyemodel,thedistancebetweencenterofrotationCandcenterofprojectionNis7–8mminallcases.NotethatnodalpointsfortheGullstrandnumber1andGullstrand-Emsleymodelsareaccommodationdependent,i.e.thenodalpointsmoveslightlytowardthecorneawhentheeyeaccommodatestoclosedistances.However,inmostcurrent-generationVR/ARsystems,thefocalplaneofthedisplayisfixed.Forexample,thefocalplanesoftheOculusRiftandMicrosoftHololensareapproximately1.3mand2m1infrontoftheuser,respectively.Sinceuserswillaccommodateatthatfixedfocaldistance[Padmanabanetal.2017],weusetherelaxedsettingofthepopularGullstrand-Emsleyeyemodelforallexperimentsinthispaper,i.e.NC=7.6916mm.1https://docs.microsoft.com/en-us/windows/mixed-reality/comfortTable1.Overviewofparametersofpopularschematiceyemodels:Gull-strandNumber1(Gull.1),Gullstrand-Emsley(Gull.-Ems.),andEmsleyreduced(Ems.).ThedistancesoffrontandrearnodalpointsN,N′arelistedinmmwithrespecttotheanteriorvertexofthecorneaVfortherelaxedandaccommodated(acc.)state.Gull.1Gull.1Gull.-Ems.Gull.-Ems.Ems.RelaxedAcc.RelaxedAcc.VN7.0786.5337.0626.5625.556VN’7.3316.8477.3636.9095.556ParallaxandAcuity.Wecanmodeltheamountofperceivedocularparallaxexpectedinvariousviewingconditions.SimilartotheillustrationinFigure1,wesimulatetwopointsthataredirectlyinfrontoftheeyeatsomerelativedistancetooneanother.Astheeyerotates,theretinalimagesofthesepointswillbeperceivedatanincreasingeccentricity,ordistancefromthefovea,measuredindegreesofvisualangle.Thelargertheeccentricity,thelargertheparallax,orrelativedistance,ofthepointsontheretina.However,thedensityofphotoreceptors,inparticularthecones,decreasesrapidlywithincreasingeccentricity.Thus,whileonewouldexpectalargeramountofparallaxintheperiphery,thethresholdtoperceiveitthereishigherduetothefalloffinvisualacuity.Figure3illustratesthetradeoffbetweenocularparallaxandvisualacuity.Here,theminimumangleofresolution(MAR)representsameasureforvisualacuityandapproximatestheresolutionofhumanvisionforastatictarget.Tomodelfalloffofacuityintheperiph-eralvisualfield,weusethelinearmodelproposedbyGuenteretal.[2012]:ω=me+ω0.Here,ωistheminimumangleofresolution(dashedredlinesinFig.3),eistheeccentricityindegrees,mistheslopemodelingthefalloffofacuity,andω0istheMARatthefovea.Wesetω0=1/60tomodel20/20visionandm=0.022asproposedbyGuenteretal.[2012].AsseeninFigure3,arelativedistanceof3D(dioptersorinversemeters)shouldtheoreticallybedetectablebyahumanobserverforeccentricityanglessmallerorequalto40°.TheleftpartofFigure3alsoshowsasemi-logarithmicplotzoom-ingintothefovealregion.Weseethatthemagnitudeofocularparallaxexpectedinthefoveola,i.e.e<1°,maynotbesufficienttobeperceivable.Yetforeccentricitieslargerthan1°,relativeobjectdistancesof2–3Dmaymakeocularparallaxausefuldepthcue.ThedashedbluelinesinFigure3alsoshowtheresolutionoftheHTCVivePro,oneofthehighest-resolutionVRdisplaysavailabletoday.Thesizeofonepixelofthisdisplayisapproximately4.58ar-cminofvisualangle,whichisabout5×higherthantheMARinthefoveola.TheresolutionofthisdisplaystartstoexceedtheMARatadistanceof2–3°ofvisualangle,implyingthatthistechnologyiswellsuitedtorenderocularparallaxpreciselywhereitisexpectedtobeperceivedbytheuser.However,ocularparallaxisamotioncueandvisualacuityalonemaybeinsufficienttofullydescribeitsperceivedeffects,becausethatmodelisonlyvalidforstaticscenes.Duringeyemovement,theperceiveddepth-dependentmotioncreatedbyparallaxresultsintime-varyingretinalstimuli.Detectionthresholdsfordifferentialvelocitiesofthistypehavebeenshowntobeconsiderablylowerthanthelimitsofvisualacuityforallretinalloci.Forexample,Mckee4•Konrad,R.etal02040608012342340.05000.10.1550EccentricityAinAdegreesEccentricityAinAdegreesParallaxAinAdegrees1D3D5D7D5D7D3DFoveolaFoveaMAR9DMAR9D1D1HTCAViveAProHTCAViveAProFig.3.Theamountofocularparallax,measuredindegreesofvisualangle,increaseswithincreasingeccentricityandrelativedistancebetweenobjects.Relativeobjectdistancesof3diopters(inversemeters)andgreaterareabovetheminimumangleofresolution(MAR,reddashedline)andmaythereforebedetectablebyahumanobserver.However,theamountofparallaxonthefoveolamaybetoosmalltobedetectedforstaticstimuli(left).TheresolutionavailablebymodernVRdisplays,liketheHTCVivePro(dashedbluelines),isslightlylowerthantheMARofhumanvisioninthefoveola,butitexceedstheMARfor2–3°ofvisualangle—preciselywhereweexpecttoseetheeffectsofocularparallax.andNakayama[1984]measuredresolutionthresholdsof2.7arcminand4.8arcminintwoparticipantsat10°eccentricity,butfoundthattheircomparablemotionthresholdswerelessthan1arcmin,indicatingthatthevisualacuity-basedanalysisaboveisanoverlyconservativeestimatefortheconditionsinwhichocularparallaxmaybedetectable.RetinalBlur.Asdiscussedabove,largerrelativedistancesbe-tweenobjectsresultinanincreasingamountofocularparallax.Forvisioninthephysicalworld,suchincreasingdistancesalsoresultinanincreasingamountofdefocusblurbecauseobjectsatdifferentopticaldistancesrequiretheeyetoaccommodateatoneofthem,placingtheotheroutoffocus.However,currentVR/ARdisplaysprovideasubstantiallydifferentviewingconditioninthattheyusuallyprovideonlyasinglefocalplaneatafixedopticaldis-tance.Usersmustaccommodatetothisfixeddistancetoperceiveasharpimage[Padmanabanetal.2017].WithAR/VRdisplays,wehavefullcontroloverhowmuchblurtorenderintothepresentedimagesandmaychoosetoignorethisrenderingaltogetherasitaddssubstantialcomputationalcost.Whiledepth-of-fieldrenderingcanbeusedtoreducevisualdiscomfortinconventionalstereodis-plays[Duchowskietal.2014],ithasnotprovensuccessfulindrivingaccommodationormitigatingthevergence-accommodationcon-flict[Johnsonetal.2016;Konradetal.2015;Maudereretal.2014],oractingasareliabledepthcueforsingleplanedisplays[Marshalletal.1996;MatherandSmith2002;PalmerandBrooks2008;Zan-nolietal.2016].Withoutdepth-of-fieldrendering,ocularparallaxisnotaffectedbydefocusblurinVR/AR,butitisstillaffectedbyperipheralfalloffsinacuityanddifferentialmotionthresholds.Inthefollowingsection,wemodelandrenderocularparallaxwithoutretinalblurasacomputationallyefficientapproximation.4RENDERINGOCULARPARALLAX4.1Gaze-contingentRenderingInthissection,wedescribenecessarymodificationsofthegraphicspipelinetorenderocularparallax.NodalPoints.Ocularparallaxisagaze-contingenteffect,andassuch,eyetrackingisnecessarytorenderitappropriately.Weassumethatabinoculareyetrackerestimatesthe3DfixationpointF,whichistypicallydefinedwithrespecttothemidpointbetweenthetwoeyes(seeFig.4).Thecenterofrotationofeacheyeisoffsetfromthismidpointbyhalftheinterpupillarydistance(ipd).DefiningthenodalpointsofeacheyeNL/Rrelativetotheirrespectivecenterofrotation,theycanbecomputedasFL/R=F±(cid:18)ipd200(cid:19),NL/R=NC|FL/R|FL/R(1)whereFL/Rdefinesthefixationpointrelativetoeacheye’scenterofrotationCandNCisthedistancebetweenthecenterofrotationandthefrontnodalpoint.Thelocationsofthesenodalpointsarethenusedtoupdatetheviewandprojectiontransformsineachrenderedframe.ViewandEyeMatrix.Thestandardgraphicspipelinetrans-formseachvertexvtoviewspacebymultiplyingitwiththemodel(M)andview(V)matrices.Inbinoculardisplays,suchasVRandARsystems,anadditionalper-eyetranslationbyhalftheipdisappliedtocreatecorrectstereoscopiccuesbytransformingverticesintoeyespace.Toaccountforocularparallax,wemodifythetrans-formtoeyespacewithanadditionaltranslationby−NL/R.Thefulltransformationfromeachvertextoeyespaceisthendefinedasv(eye)L/R=EL/R·V·M·v,(2)EL/R=100−N(x)L/R010−N(y)L/R001−N(z)L/R0001100±ipd2010000100001,(3)whereEL/Ristheeyematrix,i.e.thetransformationfromviewspacetoeyespace,andv(eye)L/Rdefineseachvertexineyespace.ProjectionMatrix.Verticesineyespacearetransformedintoclipspaceusingtheprojectionmatrix.Aperspectiveprojectioninstereorenderingisusuallyrepresentedasanasymmetricoff-axisviewfrustumdefinedbyanearandfarclippingplane,znearandzfar,aswellastheleft(l),right(r),top(t)andbottom(b)boundaryvaluesonthenearclippingplane[Shirleyetal.2009].Usingaright-handedcoordinatesystem,thecorrespondingprojectionmatrixhasthegeneralstructureoutlinedinEquation5.Forclarity,weonlyshowtheprojectiontransformoftherighteye,butamatrixofsimilarformisappliedforthelefteye:v(clip)L/R=PL/R·v(eye)L/R,(4)PR=2·znearrR−lR0rR+lRrR−lR002·zneartR−bRtR+bRtR−bR000−(zfar+znear)zfar−znear−2·zfar·znearzfar−znear00−10.(5)Thefrustumboundaryvaluesaredeterminedfromparametersofthephysicalsetup,includingthefieldofviewofthedisplay,thedistanceGaze-ContingentOcularParallaxRenderingforVirtualReality•5View Frustum (Top View)Virtual imageMidpointWith Ocular ParallaxWithout Ocular ParallaxFig.4.IllustrationofparametersusedtocomputethenodalpointsNforeacheye,definedwithrespecttothecenterofrotationCoftherespectiveeye,fromthefixationpointF,whichisestimatedbytheeyetracking.Thepreciselocationsofthesenodalpointsarerequiredforcalculatingtheviewandprojectionmatricesintherenderingpipeline.tothevirtualimage2dandthepositionofthefrontnodalpointsNL/RdefinedwithrespecttothecorrespondingcenterofrotationC.Assumingthatweknowthefieldsofviewα{l,r,t,b}definingtheasymmetricfrustumoftheconventionalstereorenderingmode(seeFig.4),wecancomputetheasymmetricviewfrustumofocularparallaxrenderingas{lR,rR}=znear+N(z)Rd+N(z)R(cid:16)d·tan(cid:16)α{l,r}R(cid:17)+N(x)R(cid:17),(6){tR,bR}=znear+N(z)Rd+N(z)R(cid:16)d·tan(cid:16)α{t,b}R(cid:17)+N(y)R(cid:17).(7)Theprojectionmatrixisupdatedonaper-framebasisusingthetrackednodalpoints.Applyingtheabovemodificationstotheviewandprojectiontransformsrendersperceptuallyaccurateocularpar-allaxusingslightmodificationsofthegraphicspipeline,undertheassumptionthattherearenoopticaldistortions.Currently,weusethemanufacturer-suppliedopticaldistortioncorrectiontoaccountforopticalaberrationsoftheHMDlensesacrossthevisualfield.4.2PerceptualEffectsofOcularParallaxRenderingOcularparallaxrenderingisexpectedtohaveimplicationsonsev-eraldepthcues.Webrieflydiscusstheseheretomotivatetheuserexperimentsthatspecificallyevaluateeachoftheseeffectsinthefollowingsections.MicroParallax.Parallaxdescribesbothmagnitudeanddirec-tionoftheretinalvelocityofobjectsatdifferentdepthswhileeitherthesceneisinmotionortheuser’sheadmoves.Weexpectasimilareffecttooccur,albeitatasignificantlysmallermagnitude,dueto2Wemaketheassumptionthatthevirtualimageisatopticalinfinity,i.e.d=∞,suchthatdistantobjectsdonotmoveduringeyemotionbutobjectsatcloserdistancesshiftrelativetothebackground.changesingazedirectionwhenocularparallaxrenderingisen-abled.Suchchangesingazedirectionrotatethenodalpointaboutthecenterofrotation,inducingtinyamountsofdepth-dependent“microparallax”intotheretinalimage.Therelativemagnitudesofthevelocitiesofobjectsatdifferentdepthscouldprovideanordi-naldepthcue,helpingusersbetterunderstandtherelativedepthorderingofascene.Thishas,forexample,beenshowntobethecaseforheadmotionparallax[Yonasetal.1987],butitisnotclearwhetherocularparallaxisaneffectiveordinaldepthcueaswell.Furthermore,theabsolutemagnitudeofretinalvelocityinducedbyocularparallaxcouldserveasanabsolutedepthcue,butthesmallretinalvelocitymagnitudesmaynotbesufficienttorobustlyestimateabsolutedistance.Whilemicroparallaxhasaneffectonallconducteduserexperiments,wespecificallyquantifyitsperceptualeffectsizeinthediscriminationthresholdexperimentinSection5.2.Gaze-contingentOcclusion.Microparallaxnearocclusionbou-ndariesisparticularlyinterestingbecausethereweobservegaze-contingentocclusion(seeFig.1).Whenobjectsatdifferentdepthsoverlap,theobservedparallaxduetoeyerotationscausestheaccr-etion-deletionofonlythefartherobject’stexture.Whileocclusioncanonlyprovideordinaldepthinformation,itisconsideredoneofthestrongestdepthcuesinstaticenvironments[CuttingandVishton1995].Particularlyrelevanttogaze-contingentocclusionisthefactthattime-varyingaccretion-deletionoftextureduetohead-motion-inducedparallaxhasbeenshowntobeaneffectiveordinaldepthcue[Yonasetal.1987].Yet,itisunknownwhetherthesameistrueforthesmallamountsofaccretion-deletionoftextureobservedwithocularparallaxrendering.Weevaluatetheperceptualeffectsizeofgaze-inducedocclusionbyestimatingthedetectionthresholdsforocularparallaxinSection5.1anditseffectivenessasanordinaldepthcueinSection6.1.DisparityDistortion.Conventionalstereoscopicrenderingtech-niquesassumethatthecentersofprojectionandrotationoftheeyesareequivalent.Weshowthatthisisnotthecase.Therefore,ocularparallaxcould,inprinciple,alsoaffecttherendereddisparityvaluesinstereographicimagepairsandthereforedistortperceiveddepth.WeevaluatethishypothesiswithauserexperimentthatstudiestheeffectofocularparallaxonabsolutedepthperceptioninSection6.2.4.3ImplementationHardware.Weimplementocularparallaxrenderingwithapro-totypevirtualrealitysystem.TheVRsystemisanHTCViveProconnectedtotheopensourcebinocularPupilLabseyetracker.TheHTCViveProhasafieldofviewof110°,arefreshrateof90Hz,anda1440×1600pixelorganiclight-emittingdiodedisplay,resultinginatheoreticalresolutionof4.58arcmin/pixel.TheHTCViveProsupportsbuilt-inipdadjustment.ThePupilLabseyetrackersnapsintothetheHTCViveProandestimatesaglobalfixationpoint(i.e.F)at120Hz,supportingabout1°ofmanufacturer-reportedgazeaccuracyand0.08°ofgazeprecision(seesupplementforadditionalcharacterization).Intheegocentricdepthperceptionstudy,anHTCViveTrackerestimatesthepositionandorientationoftheusers’6•Konrad,R.etalhandat120Hzwithanaccuracyof1.9mmroot-mean-square-error(RMSE)andaprecisionof1.5mmRMSE3.SoftwareandCalibration.Unitywasusedastherenderingen-gineforboththeocularparallaxrenderingandtheuserexperiments.Pixel-preciserenderingandanti-aliasingwereusedforallrenderedstimuli.PupilLabsprovidesaUnitypluginthatinterfaceswiththeirPythonlibrary,providingeye-trackingcalibrationandgazetrack-ing.AllsoftwarerelatedtoacquiringsubjectdatawaswrittenasC#scriptsinUnity.ThedatawerethenanalyzedinPython.5PERCEPTUALTHRESHOLDSFOROCULARPARALLAXINVRTheprimarygoalofthissectionistoestablishdepth-dependentdetectionanddiscriminationthresholdsforocularparallaxinVR.Althoughocularparallaxhasbeenshowntobewellwithintherangeofhumanvisualacuityfornaturalviewingconditions(e.g.,Bingham[1993]),wearenotawareofanyworkthatactuallyverifiedthatthissmalleffectsizeisevenperceivablewiththelimitedresolutionofferedbycurrent-generationVRdisplaysor,ifitis,whatthespe-cificthresholdsare.Thesethresholdsarecrucialforunderstandinginwhichconditionsocularparallaxrenderingisvisibleandhowreliableofadepthcueitmaybe.Toestimatethesethresholds,weperformtwopsychophysicalexperimentsthatarediscussedinthefollowingsectionswiththeapparatusdescribedinSection4.3.5.1DetectionThresholdsforOcularParallaxWiththisexperiment,weaimtoestimateadepth-dependentdetec-tionthresholdatwhichocularparallaxisperceivable.Stimuli.AsseeninFigure5(left),wepresentedtwocircularsurfacesatvaryingdistancestothesubject.Thesurfaceswerescaledtosubtend2°ofvisualangleirrespectiveofdepth.Thefarthersurfacewassolidredandthefrontonewastexturedwithwhitenoise.Withoutocularparallaxenabled,thefrontsurfaceexactlyoccludedthebackone.Anadditionalsmallredandwhitefixationtargetwasthenren-dered,circlingaroundthesceneatadistanceof16°fromthecenter,or15°fromtheedgesofthefrontandbacksurfaces.Thiswasthelargesteccentricitythatkeptthetargetwithintheaberration-freeviewingzoneoftheVivePro.Thisfixationtargetrotatedwithanangularvelocityof90°/s,resultinginaperceivedretinalmotionof24.81°/s,whichfallswithinsmoothpursuitrates[Westheimer1954].Thestartingpositionandrotationdirectionofthefixationtargetwererandomizedpertrial.Conditions.Allstimuliwerepresentedmonocularlytotherighteye.Ineachtrial,theabsolutedistancebetweentheviewerandbacksurfacewasrandomlychosenas1,2,or3D.Therelativedistancebetweenthefrontandbacksurfacewasalsorandomlychosenas0,0.25,0.5,0.75,or1D.Ineachtrial,subjectsviewedthestimuluswithocularparallaxrenderingenabledanddisabled.Thesetwoconditionswerepresentedindifferent2-secondintervalsinrandomorder,separatedbya0.5secondblankframe.3http://doc-ok.org/?p=1478Subjects.Sixadultsparticipated(agerange26–38,1female).Duetothedemandingnatureofourpsychophysicalexperiment,onlyafewsubjectswererecruited,whichiscommonforlow-levelpsychophysics(seee.g.Patneyetal.[2016]).Allsubjectsinthisandallfollowingexperimentshadnormalorcorrectedtonormalvision,nohistoryofvisualdeficiency,andnocolorblindness.Allsubjectsgaveinformedconsent.TheresearchprotocolwasapprovedbytheInstitutionalReviewBoardattheparticipatinguniversity.Procedure.Tostartthesession,eachsubjectperformeda7-pointeye-trackercalibrationthatisprovidedbythemanufacturer.Tominimizeeye-trackererroroverthecourseoftheexperiment,eachtrialbeganwithasingle-pointre-calibrationoftheeyetrackers;subjectswereinstructedtofixateonacrosstargetcenteredonthescreen,andthemeasuredgazedirectionwasusedtocompensateforpossibledrifterror.Subjectsthenviewedthestimulusrenderedwithoneofthetwoocularparallaxrenderingconditionsfor2seconds,thenablankscreenfor0.5seconds,andthenagainthestimuluswiththeotherrenderingconditionfromthefirstintervalforanother2seconds.Eachtrialconstitutedatwo-alternativeforcedchoicetest,andsubjectswereaskedtochoosethetimeintervalwhichexhibitedmorerelativemotionbetweenthetwosurfaceswithakeyboard.Thisconcludedthetrial.Nofeedbackwasprovided.Subjectswereinstructedtofixateonlyonthemovingfixationtarget,andnevertothecentersurfaces.Therewere15distanceconfigurationsand15trialsforeachcon-figurationforatotalof225trialspersubject.Theexperimenttookabout25minutestocomplete,includinginstructionandeyetrackingcalibrationpersubject.Analysis.Foreachofthe15distanceconfigurations,wecom-putedtheproportionofcorrectresponses.UsingBayesianinferencemethods[Schüttetal.2016;WichmannandHill2001a,b],wefitapsychometricfunctiontoeachsubject’sperformanceateachofthethreeabsolutedepthsofthebacksurface.Eachpsychometricfunctiongivesusadetectionthreshold,measuredindioptersofrelativedistancefromtheabsolutedistanceofthebacksurface.Thethresholdsrepresentwherethepsychometricfunctionexceededa75%chanceforacorrectresponse.AnexampleofoneofthesepsychometricfunctionsisshowninFigure5(topright)andallmeasuredpsychometricfunctionsareavailableinthesupplement.Results.Thedetectionthresholds,averagedacrosssubjects,areplottedinFigure5(centerleft).Weseethatthesethresholdsarein-varianttotheabsolutedistanceofthebacksurface.Thisisexpectedbecausetheconditionswereequallyspacedindioptersandocularparallax,likeothertypesofparallaxcues,isperceptuallylinearindioptricspace(seesupplementforananalysis).Theestimateddetectionthresholdisapproximately0.36D.Thisimpliesthat,foraneccentricityof15°andarelativeobjectdistanceaslowas0.36D,ocularparallaxmaybeperceivableinVR.Thisresultissurprisingbecausevisualacuityalone(seeFig.3)predictsdetectionthresholdsthatareanorderofmagnitudehigherthatwhatwemeasured.Yet,ourresultsareconsistentwithdatare-portedforphysicalviewingconditions(i.e.non-VRsettings)[Bing-ham1993]andwithdifferentialvelocitythresholds[MckeeandGaze-ContingentOcularParallaxRenderingforVirtualReality•7Relative Offset in D123Relative Distance From 0D in DDiscrimination Threshold0.810.60.40.20.0123Absolute Distance in DDetection Threshold0.810.60.40.20.0Relative Distance  in D Stimulus15°2°90°/sBackTargetFrontTarget2°Experimental SetupPsychometric Fits0.51.010.750.50.2501.81.350.90.4500.51.0Distance Between Targets in DProportion CorrectDetectionDiscriminationFig.5.DetectionanddiscriminationthresholdsforocularparallaxinVR.TwoexperimentswereconductedtoestimateperceptualthresholdsusinganHTCViveProheadmounteddisplaypresentingstimulioftheformshownontheleft.Aredsurfacesubtending2°ofvisualanglewascompletelyoccludedbyanoisygraysurfaceinfrontofit(left).Therelativedistancesofthesesurfaceswerevariedwithconditionsdescribedinthetext.Detectionthresholds(centerleft)anddiscriminationthresholds(centerright)wereestimatedfromthepsychometricfunctionsfittedtotherecordedsubjectdata(examplesshownright).Nakayama1984].Weexpectthedetectionthresholdstoincreaselinearlywitheccentricityintheextra-fovealregionbecauseboththemagnitudeofmicroparallax(seeFig.3)anddifferentialvelocitythresholdsincreaseroughlylinearlythere[MckeeandNakayama1984].Also,weexpectdetectionthresholdstodecreasewithfastereyemovementsbecausedifferentialvelocitythresholdsinverselyvarywithretinalvelocity[MckeeandNakayama1984].Ourresultsemphasizethatthehumanvisualsystemismuchmoresensitivetosmallamountsofmotion,eveninperipheralvision,thannaïvelyexpected.EventhesmallamountofparallaxinducedbyocularparallaxrenderingmaybevisibleinmanyVRapplications.Animportantquestionthatarisesfromthisinsightiswhetherocularparallaxrenderingcanimprovedepthperceptionortherealismofa3Dscene.WeperformseveralexperimentsinSections6and7thataimatansweringthisquestion.5.2DiscriminationThresholdsforOcularParallaxAdiscriminationthresholdtellsuswhatthesmallestamountofperceivablechangeinocularparallaxis.Forafixedeyeeccentricity,thisthresholddependsonboththeabsolutedistanceofthereferencesurface(i.e.,thebacksurface)andalsotherelativeoffsetfromthatsurface.Conceptually,onewouldhavetoestimatediscriminationthresholdsforeachcombinationofabsoluteandrelativedistance,whichwouldbeanarduoustask.However,duetothefactthatthedetectionthresholdsaredepthindependent,weassumethatthedis-criminationthresholdsarealsoindependentoftheabsolutedistancetothebacksurface.ThisassumptionmakesiteasiertosetupanexperimenttoestimatediscriminationthresholdsforocularparallaxinVR,whichwedidbyperformingasecondexperimentthatusesthesameapparatus,stimuli,andanalysisasthefirstexperimentandaverysimilarprocedure,butwithslightlydifferentconditions.Sixadultsparticipated(agerange26–32,1female).Conditions.Thebacksurfacewasfixedto0Dforallconditions.Insteadofpresentingthestimuluswithocularparallaxrenderingenabledforonlyoneinterval,asdoneinthepreviousexperiment,weenabledocularparallaxrenderingforbothintervals.Therelativeoffsetfromthebacksurfacewasrandomlychosenas1,2,or3Dandassignedasthedepthforoneofthefrontsurfacesshownineachtrial.Theotherappearedatoneofthefollowingdistancesfromthepreviouslychosenrelativeoffset:0,0.45,0.9,1.35,and1.8Dforthe1Dand2Dinitialoffsets;and0,0.7,1.4,2.1,and2.8Dforthe3Dinitialoffset.Again,therewere15distanceconfigurationsand15trialsforeachconfigurationforatotalof225trialspersubject.Results.Weusedasimilaranalysisastheonedescribedintheprevioussubsectiontoestimatethediscriminationthresholds,whichareplottedinFigure5(centerright).Asexpected,thediscrimina-tionthresholdsincreaselinearlywithincreasingamountsofocularparallax,perWeber’slaw,butduetotheproximitytothedetectionthresholdmagnitude,theslopeislessthan1.Alinearfitbetweenthediscriminationthresholdsandrelativeoffsetfrom0Dhasaslopeof0.11andaninterceptof0.38D,furtherverifyingthemeasured0.36Ddetectionthresholdfromthepreviousexperiment.Inconclusion,theestimateddiscriminationthresholdsarewellwithintherangeofthedepthrangesofnaturalscenescommonlyrenderedforimmersivevirtualrealityapplications.Thisresultmoti-vatesfurtherstudiestoinvestigateiforwhenocularparallaxcouldbeusedasareliabledepthcueintheseapplications.Wetakeafirststepatansweringthisquestionbyconductingexperimentsthatstudywhetherocularparallaxhasameasurableeffectondepthperceptioninthefollowingsections.6OCULARPARALLAXANDDEPTHPERCEPTIONMotivatedbythediscussioninSection4.2andthesurprisinglylowdetectionanddiscriminationthresholdsmeasuredintheprevioussection,weproceedtoinvestigatetheimportanceofocularparallaxasadepthcue.Here,wedistinguishbetweenordinaldepthpercep-tion,whichprovidesinformationabouttherelativedepthorderingofascene(i.e.,objectAiscloserthanobjectB),andmetricaldepthcues,whichalsoprovideabsolutedistanceestimates(i.e.,objectAis1mawayandobjectBis2maway).Itseemsintuitivethatgaze-contingentocclusioncanprovideordinaldepthinformationduetothedeletionandaccretionofthefartherobject’stexturenearocclusionboundaries.Moreover,gaze-inducedmicroparallaxalsoprovidesordinalinformationintherelativemagnitudesofretinal8•Konrad,R.etalvelocitiesofobjectsatdifferentdepths.However,theeffectivenessofocularparallaxasanabsolutedepthcueisquestionablebecausedetectingabsolutemotionvelocitiesofobjectsatdifferentdepthsmaybeunreliable,andocclusiononlyprovidesordinaldepthin-formation.WeinvestigatetheeffectofocularparallaxrenderingonbothordinalandabsolutedepthperceptioninthefollowingexperimentswiththeapparatusdescribedinSection4.3.6.1EffectofOcularParallaxonOrdinalDepthEstimationInthissection,westudythebenefitsofocularparallaxrenderingonordinaldepthestimation.Wealsoinvestigatewhetherdepthperceptiongainedfromocularparallaxispurelyavisualprocess,orrequiressomenon-visual,extra-retinalsignallikethemagnitudeordirectionofeyerotation.Theperceptionofdepthfromhead-motionparallax,forexample,hasbeenshowntorelyonanextra-retinalsignalintheformofoptokineticresponseeyemovements[Nawrot2003].Thisexperimentismodeledafterarelatedexperimentinves-tigatingretinalblurasanordinaldepthcue[Zannolietal.2016].Twenty-oneadultsparticipated(agerange22–43,4females),ofwhichtwowereexcludedforfailingtofollowinstructions.Stimuli.Themonocularstimuli,displayedonasinglevirtualimageplane,consistedoftwodifferentlytextured,frontoparallelsurfacesatdifferentdepths(Fig.6,top).Thetextures,thesameasthoseintheaforementioneddepthorderstudy[Zannolietal.2016],hadthesamespace-averageluminanceandcontrastenergy,andexhibitedsimilaramplitudespectra.Therearsurfacewasfixedat0.5Dandthefrontsurfaceappearedateither1.5Dor2.5D.Theborderbetweenthetwosurfaceshadasinusoidalshape.Thesurfaceswerescaledtosubtendthesamevisualangleirrespectiveofdepth.Subjectsviewedthesurfacesthrougha20°circularaperturethatwasunaffectedbyocularparallaxrendering.Procedure.First,allsubjectsperformeda7-pointeye-trackercalibrationprovidedbythemanufacturertostartthesession.Asinthepsychophysicalexperiments,eachindividualtrialbeganwithasingle-pointre-calibrationoftheeyetrackers.Followingthere-calibration,thestimuluswaspresentedmonocularlytotherighteyefor3secondswithoneofthethreerenderingconditions.Subjectswereinstructedtofreelygazeanywherewithintheaperturewherebothsurfaceswerevisible.Thestimuluswasthenreplacedwithapromptaskingwhichsurface,leftorright,appearednearer,andakeyboardresponseconcludedthetrial.Nofeedbackwasprovided.Conditions.Weevaluatedthreedifferentrenderingconditions(conventional,ocularparallax,andreversedocularparallax)andtwonearsurfacedistances(1.5Dand2.5D).Withthemonocularstimulidevoidofperspectiveandaccommodation/retinalblurcues,theonlyexpectedsourcesofdepthinformationaregaze-contingentmicroparallaxandocclusion.Weincludedbothcorrectandreversedocularparallaxrenderingtounderstandwhetherdepthperceptioninthisscenarioispurelyavisualprocess,requiringonlytheretinalimagewithmicroparallax,orwhetheritalsorequiresanextra-retinalsignalintheformofeyerotationdirection.Withreversedocularparallaxrendering,thesignofthevisualsignalisnegated,causingoccludedsurfacetexturestoaccretewhentheywouldnormallybedeleted.Subjectsexpectingthevisualandextra-retinalsignalstoFig.6.Effectofocularparallaxonordinaldepthperception.Subjectsviewedthemonocularstimuliconsistingoftwodistinctlytexturedsurfacessep-aratedby1Dor2D(top)andwereaskedwhichonewascloser.Theproportionsofcorrectresponses,averagedacrosssubjectspercondition,areplottedonthebottom.Subjectsperformedsignificantlybetterwithocularparallaxrenderingenabledcomparedtoconventionalrendering.However,theyalsoperformedslightlybetterthanconventionalrenderingwithre-versedocularparallaxrendering,indicatingthattheextra-retinalsignalofeyerotationmaynotbecrucialfordepthperception.Significanceisindi-catedatthep≤0.05,0.01,and0.001levelswith∗,∗∗,and∗∗∗,respectively.Errorbarsrepresentstandarderror.beconsistentcouldmisinterpretdepthorderings.Reversedocularparallaxwasimplementedbynegatingthexandycomponentsoftheestimatednodalpointsoftheeyes,NL/R.Thenearsurfacedistancescreatea1Dand2Dseparationfromtherearsurface,resultingindifferentmagnitudesofthegaze-contingenteffects.Followingthethresholdexperiments,weexpecttheproportionofcorrectresponsestoincreasewithsurfacesep-aration.Overall,therewere6conditionsandeachconditionwasevaluatedwith15trialsforatotalof90trialspersubject.Results.Theproportionofcorrectresponses,averagedacrosssubjectspercondition,isplottedinFigure6(bottom).Asexpected,subjectsintheconventionalstaticrenderingconditionperformedclosetorandom,correctlyidentifyingthenearersurfacein48.8%and52.6%oftrialsfor1Dand2Dofsurfaceseparation,respec-tively.Enablingocularparallaxrenderingclearlyimprovedordinaldepthjudgmentwithsubjectsperformingat66.7%and75.8%correctidentificationforthe1Dand2Dseparations,respectively.Subjectsinthereversedocularparallaxconditionfellin-between,perform-ingat60.4%and67.4%correctidentificationforthetwoseparationdistances.Weconducteda2×3repeated-measuresANOVAonthepropor-tionofcorrectresponseswithindependentvariablesofrenderingmode(conventional,ocularparallax,reversedocularparallax)andseparationdistance(1Dor2D).Greenhouse-Geissersphericitycorrectionwasapplied.TheANOVAshowsaverysignificanteffectGaze-ContingentOcularParallaxRenderingforVirtualReality•9ofrenderingmode(F(1.7,30.65)=17.98,p<0.0001)aswellasasignificanteffectofdistance(F(1,18)=8.14,p<0.05).TheANOVAdoesnotrevealasignificantinteractionbetweenrenderingmodeanddistance(F(1.78,31.96)=0.42,p=0.64).Post-hoctestswereconductedaspairwiset-testsbetweenrender-ingmodesateachseparationdistance,withBonferronicorrectionappliedtothep-values.Thepost-hoctestsfoundthat,at1Dofseparation,ocularparallaxrenderingshowsasignificantimprove-mentoverconventionalrendering(p<0.001),butnotoverreversedocularparallaxrendering.Reversedocularparallaxrenderingdoesnotshowasignificantimprovementoverconventionalrendering.At2Dofseparation,ocularparallaxrenderingshowsasignificantimprovementoverconventionalrendering(p<0.001)aswellasre-versedocularparallaxrendering(p<0.05).Reversedocularparallaxrenderingalsoshowsasignificantimprovementoverconventionalrendering(p<0.05)atthisseparation.Insummary,thisexperimentdemonstratesthatocularparallaxsignificantlyimprovesordinaldepthperceptionoverconventionalrendering.However,reversedocularparallaxalsoimprovesordinaldepthperceptionoverconventionalrendering,butnotnearlyasmuchaswhenrenderedcorrectly.Thereducedperformanceinthereversedocularparallaxconditioncomparedtothecorrectocularparallaxconditionsuggeststhatextra-retinalsignals,likeeyerota-tion,playanimportantroleintheperceptionofdepth.However,thiseffectseemstobeweakerthanformotionparallax,whereareversalineyemovementcausesareversalinthesignoftheper-ceiveddepth[Nawrot2003].Still,subjectsperformedbetter,insomeconditionssignificantlyso,whenthedirectionsoftheretinalimagemotionandeyerotationwereconsistent.Therefore,enablingcor-rectocularparallaxrenderingcanbenefitordinaldepthestimationwhichcanbeparticularlyusefulforviewing3Dscenesthatoftencontainmanyocclusionboundaries.6.2EffectofOcularParallaxonAbsoluteDepthEstimationNext,westudywhetherocularparallaxrenderingcanbenefitego-centricdistance(distancefromone’sself)estimationinastereoenvironment.Whileunlikely,gaze-inducedmicroparallaxandthereductioninperipheraldisparityerrorscouldaffectabsolutedis-tanceestimation.Intheexperiment,subjectsperformedablindreachingtaskintro-ducedbyNapieralskietal.[2011]toestimatedistancestoobjectsbecauseverbalestimateshavebeenshowntobeinaccurate[Renneretal.2013].Inaphoto-realisticdiningroomscene,subjectsviewedastereoscopicallyrenderedpencilforaminimumof5secondsaf-terwhichthescreenwasblankedandtheyreachedtheirhandtowheretheyhadlastseenthepencil(Figure7,topleft).Eachsubjectperformedthetaskwithocularparallaxenabledanddisabled,andthereachtargetdistancesweresetproportionally—50,58,67,75,82,and90%—toeachsubjects’maximumarmreach.Eachofthe12conditionswereevaluatedwithfiveblindreachingtasksforatotalof60randomly-orderedtrialspersubject.Duringthesession,6-DOFheadposetrackingwasenabled,andthehand’spositionwastrackedviaanHTCVivetrackermountedonanopticalpost(Figure7,topright).Sixteenyoungadultsparticipated(agerange22–32,5females),ofwhichonewasexcludedfornotpassingaFig.7.Effectofocularparallaxonabsolute,egocentricdepthperception.Subjectsviewedatarget(pencil)andthen,withthedisplayturnedoff,wereaskedtoreachforit(topright).Wedidnotfindastatisticallysignificantdifferencebetweenconventionalstereorenderingandstereorenderingwithocularparallaxinthisexperiment(bottomplot).standardRandotstereovisiontest,andtwootherswereexcludedduetotheeyetrackerfailingtotracktheirpupils.Subjectssawlittletonoimprovementintheiregocentricdis-tanceestimateswithocularparallaxrenderingenabled.Figure7showseachsubject’sreach,asapercentageoftheirmaximumarmreach,toapresentedtargetdistancefortheocularparallaxenabledanddisabledconditions.Linearmodelswerefittothetwosetsofreaches;theslopesfortheocularparallaxenabledanddisabledmodeswere1.141and1.128,respectively,whiletheinterceptswere0.167and0.165,respectively.Amultipleregressionanalysisdidnotshowasignificantdifferencebetweenthetwoconditions.Itisthereforeunlikelythatthesystematicunderestimationofdistancestovirtualobjectscomparedtorealones[Renneretal.2013]canbeexplainedbytheomissionofocularparallaxrendering.Formoredetailsregardingthisexperiment,pleaserefertothesupplement.7OCULARPARALLAXANDPERCEPTUALREALISMWenextmeasuredtheeffectofocularparallaxonperceptualrealism.Twenty-oneadultsparticipatedintheexperiment(agerange22–43,4females),ofwhichtwowereexcludedforfailingtofollowinstructions.StimuliandConditions.A3Dscene(Fig.8,left)waspresentedtotherighteyewithconventional,ocularparallax,orreversedocularparallaxrendering.Thescenewascomposedoffourgridplanesextendingintothedistanceandafieldoffrontoparalleltargets.10•Konrad,R.etalFig.8.Evaluatingperceptualrealism.Subjectsvieweda3Dsceneconsistingoftargetsthatarerandomlydistributedindepthbutthatdonotoccludeoneanother(left).Thisstimuluswaspresentedwitheitherconventional,ocularparallax,orreversedocularparallaxrenderingandweaskedsubjectstoindicatedwhichrenderingmodeprovidedastrongerimpressionofrealisticdepth.Resultsofpairwisecomparisonsbetweentheserenderingmodesshowthepercentoftimesthefirstmemberofthepairwaschosenoverthesecond(right).Renderingwithcorrectandreversedocularparallaxconveyedastrongerimpressionofdepthcomparedtoconventionalrendering,butwhencomparedagainstoneanothernodifferencewasobserved.Thisresultindicatesthattherelativemagnitudesofdepth-dependentmotionvelocitiesaremostimportantforperceptualrealismbutnotnecessarilytheirdirection.Significanceisindicatedatthep≤0.05,0.01,and0.001levelswith∗,∗∗,and∗∗∗,respectively.Errorbarsrepresentstandarderror.Thesetargetswererandomlydistributedindepthbetween0.5Dand3.5Dandwerescaledtosubtend2.6°ofvisualangleregardlessofdepth.Thetargets’lateralpositionswererandomlychosensuchthattheydidnotoccludeoneanotherandtheentiretargetfieldsubtended60°.Procedure.First,allsubjectsperformeda7-pointeye-trackercalibrationprovidedbythemanufacturertostartthesession.Asinthepsychophysicalexperiments,eachindividualtrialbeganwithasingle-pointre-calibrationoftheeyetrackers.Then,thetargetpositionsusedforthesubsequentstimuliweregenerated.Thefirststimuluswaspresentedfor3secondswithoneofthethreerenderingconditions.Thetargetsthendisappearedfor1second,leavingonlythegridpatternshownwithconventionalrendering.Thesecondstimuluswasthenpresentedfor3secondsusingoneofthethreerenderingconditions,butnotthesameastheoneusedforthefirststimulus.Afterthesecondstimulus,apromptappearedaskingthesubjectstodetermine,inaforced-choicejudgment,thestimulusthatportrayedastrongerimpressionofrealisticdepth.Nofeedbackwasprovided.Eachtrialthereforeconsistedofthreepairwisecom-parisons:conventionalvs.ocularparallax,conventionalvs.reversedocularparallax,andocularparallaxvs.reversedocularparallax.Results.TheresultsofthepairwisecomparisonsareaveragedacrossusersandtrialsandplottedinFigure8(right).In76.8%oftrials,subjectsreportedastrongersenseofrealisticdepthwhenviewingocularparallaxrenderingoverconventionalrendering.In-terestingly,subjectsalsoreportedmorerealisticdepthforreversedocularparallaxrenderingoverconventionalrenderingin75.1%oftrials.Eachofthesepercentagesissignificantlygreaterthan50%(p<0.001andp<0.001respectively,one-tailedbinomialtest).Usershadmoredifficultydifferentiatingbetweenthetwoocularparallaxmodesandreportedmorerealisticdepthwhenocularpar-allaxwascorrectlyrenderedinonly49.1%oftrials,whichwasnotsignificantlygreaterthan50%.Theseresultssuggestthatgaze-inducedmicroparallax,offeredbyocularparallaxrendering,hasasignificanteffectonperceptualrealismwhencomparedtoconventionalrendering.Moreover,theresultssuggestthattherelativemotionmagnitudesofthedepth-dependentretinalvelocitiesaremoreimportantthantheirdirection.Thisinsightfurtheremphasizesthat,similartotheordinaldepthperceptiontask(Sec.6.1),extra-retinalsignalswerelikelynotno-tablyfactoredintotheperceptionofdepthonthissubjectivetask.UnliketheexperimentinSection6.1,wherebothgaze-contingentocclusionandmicroparallaxwerepresentascues,inthisexperi-ment,microparallaxwastheprimaryindicatorofdepthbecausethetargetsdidnotoverlap.8DISCUSSIONInsummary,ourprimarycontributionistointroduceanewtech-nologyforvirtualandaugmentedreality:ocularparallaxrendering.Thistechniqueisenabledbyeyetrackingsystemswhichcanal-readybefoundinsomeofthelatestheadsets,liketheMicrosoftHololens2,MagicLeapOne,Varjo,Fove,andHTCViveProEye.Ocularparallaxrenderingcouldbejointlyimplementedwithothergaze-contingentrenderingmethods,suchasfoveatedrendering,anditrequiresnoadditionalcomputationalcostcomparedtocon-ventionalstereorendering.Toevaluateocularparallaxrendering,wedesignedandconductedaseriesofuserexperiments.First,wemeasureddetectionthresholdsandshowthattheeffectisperceivablewhentherelativedistancebetweentwopartiallyoccludingobjectsisaslowas0.36diopters.Wealsomeasureddiscriminationthresholdsandconfirmthatthejustnoticeabledifferencebetweentwoobjectsatdifferentrelativedepthsinfrontofabackgroundstimulusisdirectlyproportionaltotheirabsolutedepthfromthebackground.Thesethresholdscon-firmthatocularparallaxisanimportvisualcueinVR.Ourthirdandfourthexperimentsshowthatocularparallaxactsasreliableordinaldepthcuebutthatitmaynotbeareliableabsolutedepthcue.Finally,ourfifthexperimentdemonstratesthatocularparallaxrenderingsignificantlyimprovestheimpressionofrealisticdepthwhenviewinga3Dsceneoverconventionalrendering.OcularParallaxandRetinalBlur.Retinalblurdescribesthedepth-dependentdefocusblurofobjectsontheretina,relativetotheaccommodationdistance.Inphysicalenvironments,retinalblurdiscriminationthresholdsarecomparabletothoseofocularparallax—approximately±0.35D[OgleandSchwartz1959]—andhavebeenshowntoserveasbothareliableordinalandabsolutedepthcue[VishwanathandBlaser2010].WhilerecentresearcheffortsinVR/ARhavestudiedfocuscuessuchasretinalblur,chro-maticaberrations,andaccommodation,nonehavestudiedocularparallax,eventhoughtheireffectsizesareverysimilar.Gaze-contingentretinalblurandocularparallaxrenderingcancertainlybeimplementedsimultaneouslyastheyarecomplemen-tarycues.Whiledepth-of-fieldrenderingblursobjectsatdifferentGaze-ContingentOcularParallaxRenderingforVirtualReality•11depths,wedonotexpectittoalterourmeasurementsorinsightssignificantly.Thisisbecausetheperceptionofmotionisunderstoodtouselowspatialfrequencies[SmithandSnowden1994],sothelossofthehighspatialfrequenciesduetodepth-of-fieldrenderingshouldnotimpairmotion,orocularparallax,perception.Indeed,ourmeasureddetectionthresholdsareconsistentwiththosemeasuredinaphysicalenvironment[Bingham1993],whereretinalblurwasapparent.EyeModel.Throughoutthepaper,weuseaschematiceyethatmakesseveralsimplifyingassumptions.First,weassumethatthefrontnodalpointisindeedthecenterofprojection.Second,weassumethattheuserdoesnotaccommodate.Third,weassumethattheopticalandvisualaxisoftheeyearethesame.Thefirstassump-tionisreasonableandinlinewiththeGullstrand-Emsleyschematiceye.However,itcouldbearguedthatthereisnoexactcenterofprojectionintheeye,thatitvariessignificantlybetweenusers,orthatitisthecenteroftheentrancepupiloftheeye,whichlaterallyshiftswithchangingpupildiameter[AtchisonandMathur2014],insteadofthefrontnodalpoint.Thepreciselocationofthecenterofprojectionisatopicthatdeservesfurtherdiscussionandthatshouldalsobeexperimentallylocated,whichweleaveforfuturework.Thesecondassumptionrequirestheusertoaccommodateatafixed,fardistance.Fornear-eyedisplaysthatsupportaccommodation,vergenceoraccommodationtrackingcouldbeusedtomodeltheaccommodation-dependentnodalpoint.Finally,weassumethatop-ticalandvisualaxisoftheeyearethesame.Usingthisassumption,wedemonstratethatocularparallaxrenderinghasnosignificanteffectondisparitydistortion,thusabsolutedepthperception,inSec-tion6.However,furtherstudiesonthistopicshouldbeconductedusingmoreaccurateeyemodelsthatinclude,forexample,anoffsetbetweenvisualandopticalaxisoftheeye.Limitations.Althoughoursystemusessomeofthehighest-endcomponentsavailable,includingahigh-resolutionwide-field-of-viewVRdisplayanda120Hzeyetracker,thelatencyof20msforgaze-contingentocularparallaxrenderingishigh.Fasterandmoreaccurateeyetrackingwouldcertainlyhelpimprovetheuserexperienceforallgaze-contingentrenderingschemes,includingocularparallax.Applications.Weenvisionocularparallaxrenderingtobeastandardpartofthegraphicspipelineofeye-tracking-enablednear-eyedisplays.Itimprovesperceptualrealism,ordinaldepthpercep-tion,anditmayofferotherperceptualbenefits.Inparticular,opticalsee-throughaugmentedrealitysystemsmaybenefitfromocularpar-allaxrenderingasauserusuallyseesadigitallyrenderedstimulusoverlaidonareferencestimulus(i.e.thephysicalworld);visualcueconsistencybetweenthesestimulimaybeevenmoreimportantthaninVR.Finally,onecouldalsoimaginethatanamplifiedversionofocularparallaxrenderingcouldbeaneffectivegaze-contingentuserinterfacethatallowsuserstotransformobjects,navigatethroughvirtualenvironments,orperformothertasks.Tasksthatrequirehands-freeoperationcouldparticularlybenefitfromthistypeofgaze-contingentinteractionmode.FutureWork.AsthefieldsofviewofemergingVRandARsystemskeepincreasing,understandingperceptualeffectsinpe-ripheralvisionbecomesevermoreimportant.Withthiswork,wethoroughlyevaluatetheperceptualimplicationsofonetechnique,ocularparallaxrendering,whichshowsthestrongesteffectsinnear-midperipheralvision.However,manyothertechnologies,suchasfoveatedrendering,reducingmotionsickness,orothermeanstoimprovedepthperceptionorperceptualrealism,couldalsobenefitfromstudyingtheirperceptualeffectsonperipheralvision.Addition-ally,asmanyoftheseeffectsvaryfrompersontoperson,performingaper-usercalibrationofthedistancebetweenthecentersofrotationandprojectioncouldfurtherincreaseperceptualrealism.Conclusions.Virtualandaugmentedrealitysystemshavefo-cusedonimprovingresolution,fieldofview,deviceformfactor,andothercharacteristics.Withthiswork,wehopetostimulatenewdirectionsforgaze-contingentrenderingandimprovepercep-tualrealismanddepthperceptionwithnext-generationnear-eyedisplays.ACKNOWLEDGMENTSThisprojectwasgenerouslysupportedbyfundingfromtheNationalScienceFoundation(NSF,awardnumbers1553333and1839974),aSloanFellowship,anOkawaResearchGrant,andIntel.REFERENCESKurtAkeley,SimonJ.Watt,AhnaR.Girshick,andMartinS.Banks.2004.AStereoDisplayPrototypewithMultipleFocalDistances.ACMTrans.Graph.(SIGGRAPH)23,3(2004),804–813.DavidA.Atchison.2017.SchematicEyes.InHandbookofVisualOptics,VolumeI-FundamentalsandEyeOptics,PabloArtal(Ed.).CRCPress,Chapter16.DavidAAtchisonandAnkitMathur.2014.Effectsofpupilcentershiftonocularaberrations.Investigativeophthalmology&visualscience55,9(2014),5862–5870.GeoffreyP.Bingham.1993.Opticalflowfromeyemovementwithheadimmobilized:“Ocularocclusion”beyondthenose.VisionResearch33,5(1993),777–789.DavidBrewster.1845.OntheLawofVisiblePositioninSingleandBinocularVision,andontherepresentationofSolidFiguresbytheUnionofdissimilarPlanePicturesontheRetina.Proc.RoyalSocietyofEdinburgh1(1845).StevenA.Cholewiak,GordonD.Love,PratulP.Srinivasan,RenNg,andMartinS.Banks.2017.Chromablur:RenderingChromaticEyeAberrationImprovesAccommodationandRealism.ACMTrans.Graph.(SIGGRAPHAsia)36,6(2017),210:1–210:12.JamesCuttingandPeterVishton.1995.Perceivinglayoutandknowingdistances:Theinteraction,relativepotency,andcontextualuseofdifferentinformationaboutdepth.InPerceptionofSpaceandMotion,WilliamEpsteinandSheenaRogers(Eds.).AcademicPress,Chapter3,69–117.PiotrDidyk,TobiasRitschel,ElmarEisemann,KarolMyszkowski,andHans-PeterSeidel.2011.APerceptualModelforDisparity.ACMTrans.Graph.(SIGGRAPH)30,4(2011),96:1–96:10.AndrewT.Duchowski,NathanCournia,andHunterA.Murphy.2004.Gaze-ContingentDisplays:AReview.Cyberpsychology&behavior7(2004),621–34.AndrewT.Duchowski,DonaldH.House,JordanGestring,RuiI.Wang,KrzysztofKrejtz,IzabelaKrejtz,RadoslawMantiuk,andBartoszBazyluk.2014.ReducingVisualDiscomfortof3DStereoscopicDisplayswithGaze-contingentDepth-of-field.InACMSymposiumonAppliedPerception.39–46.DavidDunn,CaryTippets,KentTorell,PetrKellnhofer,KaanAksit,PiotrDidyk,KarolMyszkowski,DavidLuebke,andHenryFuchs.2017.WideFieldOfViewVarifocalNear-EyeDisplayUsingSee-ThroughDeformableMembraneMirrors.IEEETVCG23,4(2017),1322–1331.GlennA.FryandW.W.Hill.1962.TheCenterofRotationoftheEye.OptometryandVisionScience(1962),581–595.BrianGuenter,MarkFinch,StevenDrucker,DesneyTan,andJohnSnyder.2012.Foveated3DGraphics.ACMTrans.Graph.(SIGGRAPHAsia)31,6(2012),164:1–164:10.ItzhakHadani,GideonIshai,andMosheGur.1980.Visualstabilityandspaceperceptioninmonocularvision:mathematicalmodel.OSAJ.Opt.Soc.Am.70,1(1980),60–65.DavidM.Hoffman,AhnaR.Girshick,KurtAkeley,andMartinS.Banks.2008.Ver-genceâĂŞaccommodationconflictshindervisualperformanceandcausevisual12•Konrad,R.etalfatigue.JournalofVision8,3(2008),33.IanP.HowardandBrianJ.Rogers.2002.SeeinginDepth.OxfordUniversityPress.XindaHuandHongHua.2014.High-resolutionopticalsee-throughmulti-focal-planehead-mounteddisplayusingfreeformoptics.Opt.Express22,11(Jun2014),13896–13903.HongHuaandBahramJavidi.2014.A3Dintegralimagingopticalsee-throughhead-mounteddisplay.OpticsExpress22,11(2014),13484–13491.Fu-ChungHuang,KevinChen,andGordonWetzstein.2015.TheLightFieldStereoscope:ImmersiveComputerGraphicsviaFactoredNear-EyeLightFieldDisplaywithFocusCues.ACMTrans.Graph.(SIGGRAPH)34,4(2015).PaulV.Johnson,JaredAQ.Parnell,JoohwanKim,ChristopherD.Saunter,GordonD.Love,andMartinS.Banks.2016.Dynamiclensandmonovision3Ddisplaystoimproveviewercomfort.OSAOpt.Express24,11(2016),11808–11827.PetrKellnhofer,PiotrDidyk,TobiasRitschel,BelenMasia,KarolMyszkowski,andHans-PeterSeidel.2016.MotionParallaxinStereo3D:ModelandApplications.ACMTrans.Graph.(SIGGRAPH)35,6(2016),176:1–176:12.RobertKonrad,EmilyCooper,andGordonWetzstein.2015.NovelOpticalConfig-urationsforVirtualReality:EvaluatingUserPreferenceandPerformancewithFocus-tunableandMonovisionNear-eyeDisplays.InProc.SIGCHI.HiroakiKudoandNoboruOhnishi.1998.Studyontheocularparallaxasamonoculardepthcueinducedbysmalleyemovementsduringagaze.InProc.IEEEEngineeringinMedicineandBiologySociety,Vol.6.3180–3183.HiroakiKudoandNoboruOhnishi.2000.Effectofthesightlineshiftwhenahead-mounteddisplayisused.InProc.EMBSInternationalConference,Vol.1.548–550.HiroakiKudo,MasayaSaito,TsuyoshiYamamura,andNoboruOhnishi.1999.Mea-surementoftheabilityinmonoculardepthperceptionduringgazingatnearvisualtarget-effectoftheocularparallaxcue.InProc.IEEEInternationalConferenceonSystems,Man,andCybernetics,Vol.2.34–37.MichaelF.Land.1995.Fast-focustelephotoeye.Nature373(1995),658–659.DouglasLanmanandDavidLuebke.2013.Near-eyeLightFieldDisplays.ACMTrans.Graph.(SIGGRAPHAsia)32,6(2013),220:1–220:10.ShengLiu,DewenCheng,andHongHua.2008.AnOpticalSee-throughHeadMountedDisplaywithAddressableFocalPlanes.InProc.ISMAR.33–42.PatrickLlull,NoahBedard,WanminWu,IvanaTošić,KathrinBerkner,andNikhilBalram.2015.Designandoptimizationofanear-eyemultifocaldisplaysystemforaugmentedreality,InImagingandAppliedOptics2015.ImagingandAppliedOptics2015,JTh3A.5.GordonD.Love,DavidM.Hoffman,PhilipJ.W.Hands,JamesGao,AndrewK.Kirby,andMartinS.Banks.2009.High-speedswitchablelensenablesthedevelopmentofavolumetricstereoscopicdisplay.Opt.Express17,18(Aug2009),15716–15725.AndrewMaimone,AndreasGeorgiou,andJoelS.Kollin.2017.HolographicNear-eyeDisplaysforVirtualandAugmentedReality.ACMTrans.Graph.(SIGGRAPH)36,4(2017),85:1–85:16.AlistairP.MappandHiroshiOno.1986.Therhino-opticalphenomenon:Ocularparallaxandthevisiblefieldbeyondthenose.VisionResearch26,7(1986),1163–1165.JonathanA.Marshall,ChristinaA.Burbeck,DanAriely,JannickP.Rolland,andKevinE.Martin.1996.Occlusionedgeblur:acuetorelativevisualdepth.J.Opt.Soc.Am.A13,4(1996),681–688.GeorgeMatherandDavidR.R.Smith.2002.BlurDiscriminationanditsRelationtoBlur-MediatedDepthPerception.Perception31,10(2002),1211–1219.MichaelMauderer,SimoneConte,MiguelA.Nacenta,andDhanrajVishwanath.2014.Depthperceptionwithgaze-contingentdepthoffield.(2014),217–226.SuzanneP.MckeeandKenNakayama.1984.Thedetectionofmotionintheperipheralvisualfield.VisionResearch24,1(1984),25–32.OlivierMercier,YusufuSulai,KevinMackenzie,MarinaZannoli,JamesHillis,DerekNowrouzezahrai,andDouglasLanman.2017.FastGaze-contingentOptimalDe-compositionsforMultifocalDisplays.ACMTrans.Graph.(SIGGRAPHAsia)36,6(2017).PhillipE.Napieralski,BlissM.Altenhoff,JeffreyW.Bertrand,LindsayO.Long,Sabar-ishV.Babu,ChristopherC.Pagano,JustinKern,andTimothyA.Davis.2011.Near-fieldDistancePerceptioninRealandVirtualEnvironmentsUsingBothVerbalandActionResponses.ACMTrans.Appl.Percept.8,3,Article18(Aug.2011),19pages.R.Narain,R.Albert,A.Bulbul,G.J.Ward,M.S.Banks,andJ.F.O’Brien.2015.OptimalPresentationofImagerywithFocusCuesonMulti-PlaneDisplays.ACMTrans.Graph.(SIGGRAPH)34,4(2015).MarkNawrot.2003.Eyemovementsprovidetheextra-retinalsignalrequiredfortheperceptionofdepthfrommotionparallax.Visionresearch43,14(2003),1553–1562.KennethN.OgleandJ.TheodoreSchwartz.1959.Depthoffocusofthehumaneye.JOSA49,3(1959),273–280.NitishPadmanaban,RobertKonrad,TalStramer,EmilyA.Cooper,andGordonWet-zstein.2017.Optimizingvirtualrealityforallusersthroughgaze-contingentandadaptivefocusdisplays.PNAS114,9(2017),2183–2188.NitishPadmanaban,YifanPeng,andGordonWetzstein.2019.HolographicNear-eyeDisplaysBasedonOverlap-addStereograms.ACMTrans.Graph.38,6,Article214(Nov.2019),13pages.https://doi.org/10.1145/3355089.3356517StephenE.Palmer.1999.VisionScience-PhotonstoPhenomenology.MITPress.StephenE.PalmerandJosephL.Brooks.2008.Edge-regiongroupinginfigure-groundorganizationanddepthperception.JournalofExperimentalPsychology:HumanPerceptionandPerformance34,6(2008),1353.AnjulPatney,MarcoSalvi,JoohwanKim,AntonKaplanyan,ChrisWyman,NirBenty,DavidLuebke,andAaronLefohn.2016.TowardsFoveatedRenderingforGaze-trackedVirtualReality.ACMTrans.Graph.(SIGGRAPHAsia)35,6(2016),179:1–179:12.JohnD.Pettigrew,ShaunP.Collin,andMatthiasOtt.1999.Convergenceofspecialisedbehaviour,eyemovementsandvisualopticsinthesandlance(Teleostei)andthechameleon(Reptilia).CurrentBiology9,8(1999),421–424.RebekkaS.Renner,BorisM.Velichkovsky,andJensR.Helmert.2013.ThePerceptionofEgocentricDistancesinVirtualEnvironments-AReview.ACMComput.Surv.46,2(2013),23:1–23:40.JannickP.Rolland,MyronW.Krueger,andAlexeiGoon.2000.Multifocalplaneshead-mounteddisplays.OSAAppl.Opt.39,19(2000),3209–3215.HeikoH.Schütt,StefanHarmeling,JakobH.Macke,andFelixA.Wichmann.2016.PainfreeandaccurateBayesianestimationofpsychometricfunctionsfor(poten-tially)overdisperseddata.VisionResearch122(2016),105–123.PeterShirley,MichaelAshikhmin,andSteveMarschner.2009.FundamentalsofCom-puterGraphics.AKPeters/CRCPress.AndrewT.SmithandRobertJ.Snowden.1994.VisualDetectionofMotion.AcademicPress.https://books.google.com/books?id=p11qAAAAMAAJDhanrajVishwanathandErikBlaser.2010.Retinalblurandtheperceptionofegocentricdistance.JournalofVision10,10(082010),26–26.GeraldWestheimer.1954.Eyemovementresponsestoahorizontallymovingvisualstimulus.A.M.A.ArchivesofOphthalmology52,6(1954),932–941.FelixA.WichmannandN.JeremyHill.2001a.Thepsychometricfunction:I.Fitting,sampling,andgoodnessoffit.Perception&Psychophysics63,8(01Nov2001),1293–1313.FelixA.WichmannandN.JeremyHill.2001b.Thepsychometricfunction:II.Bootstrap-basedconfidenceintervalsandsampling.Perception&Psychophysics63,8(01Nov2001),1314–1329.AlbertYonas,LincolnG.Craton,andWilliamB.Thompson.1987.Relativemotion:Kineticinformationfortheorderofdepthatanedge.Perception&Psychophysics41,1(01Jan1987),53–59.MarinaZannoli,RachelA.Albert,AbdullahBulbul,RahulNarain,JamesF.O’Brien,andMartinS.Banks.2014.Correctblurandaccommodationinformationisareliablecuetodepthordering.JournalofVision14,10(2014),138.MarinaZannoli,GordonD.Love,RahulNarain,andMartinS.Banks.2016.Blurandtheperceptionofdepthatocclusions.JournalofVision16,6(042016),17–17.SupplementaryInformation:Gaze-ContingentOcularParallaxRenderingforVirtualRealityROBERTKONRAD,StanfordUniversityANASTASIOSANGELOPOULOS,StanfordUniversityGORDONWETZSTEIN,StanfordUniversityInthisdocumentweprovideadditionaldiscussionandresultsinsupportoftheprimarytext.1OCULARPARALLAXLINEARITYINDIOPTRICSPACETheamountofparallaxduetoeyerotationsthatisobservednearanocclusionboundaryisafunctionofthemagnitudeoftheeyerotation,thedistancetothefarthersurface,andtheseparationbetweentwosurfaces.Bingham[1993]presentsamodeltakingthesefactorsintoaccountandtheocularparallaxobservedwhenthedistancesareconsideredinmetricunitsisshowninFigure1(left).Clearly,ocularparallaxisnotlinearwithseparationdistanceinmeters.However,theamountofocularparallaxobservedontheretinaislinearwhenthedistancesareconsideredindiopters,orinversemeters(Figure1,right).ThesimulatedocularparallaxresultsinFigure1assumed15°ofeyerotationandadistancebetweenthenodalpointandcenterofrotationof7.69mm.012345Relative distance between surfaces (D)123Back surface distance (D)3025201510503025201510500246810Relative distance between surfaces (m)Simulated Ocular ParallaxOcular Parallax (arcmin)Fig.1.Modelofocularparallaxinmetricanddioptricspaces.Theplotsshowtheestimatedamountofocularparallaxbetweentwoobjectsseparatedindepth.Thedifferentlycoloredlinesindicatetheocularparallaxwhenthebackobjectissetto1,2,or3Dforvaryingdistancestothefrontobject.Theleftplotshowsthattheocularparallaxisnotlinearwithmetricdistance,islinearwithdioptricdistance.Authors’addresses:RobertKonrad,StanfordUniversity,rkkonrad@stanford.edu;AnastasiosAngelopoulos,StanfordUniversity,nikolasa@stanford.edu;GordonWetzstein,StanfordUniversity,gordon.wetzstein@stanford.edu.arXiv:1906.09740v2  [cs.GR]  13 May 20202•RobertKonrad,AnastasiosAngelopoulos,andGordonWetzstein2EYETRACKERCHARACTERIZATIONWecharacterizedthePupilLabseyetrackeraccuracyonasubsetofthesubjectsfromtheexperimentsinthemainpaper(4subjects,ages22–28,1female).Subjectsperformedthemanufacturer-provided7-pointeyetrackercalibrationfollowedbyanadditionalsingle-pointeye-trackerre-calibrationtomimictheprocedureofthemainexperiments.Subjectsthenviewedeachofthefive×-shapedtargetsintheordershowninFigure2(left);eachtargetsubtended1°ofvisualfield.Thetargetswereplacedatthecenteroftheirvisualfieldand15°above,below,totheleft,andtotherightofthecenter.Thesubjectswereinstructedtolooktothecenterofthetargetsandindicatedtheyweredoingsowithakeyboardpresswhichrecorded5secondsofgazedata.Usersrepeatedthisprocedureforeachtarget.Theaccuracycomputationassumedthattheuserlookedatthecenterofthetarget.TheresultsoftheeyetrackeraccuracycharacterizationcanbefoundinFigure2(right).Theerroraveragedacrossallsubjectsandfieldpositionswas1.18°,whichisworsethanthe1.0°errorclaimedbyPupilLabs.Clearly,theeyetrackererrorincreaseswithincreasingeccentricity,butwasneverhigherthan1.6°(forthefieldposition#3).Onesubjectworecontactswhichisknowntodeteriorateeyetrackerperformance.Indeed,theeye-trackeraccuracyforthissubjectwas1.415°whiletheaverageeye-trackeraccuracyforthesubjectswithoutcontactswas1.1°.StimulusEye Tracker Errors at Different Field Positions1234515°1°Target Position12345Gaze Error(°)1.60.80.41.20Avg. Error1.18°Fig.2.Eyetrackercharacterization.Aftercalibratingtheeyetracker,subjectsviewedeachofthe5targetsontheleftandtheirgazedatawasrecorded.Theaverageerrorsacrosssubjectsforeachfieldpositionisshownontheright.Naturally,thelowesterrorwasobservedatthecenterofthevisualfield,buttheerrorneverroseabove1.6°(targetposition#3)atthemeasuredfieldpositions.SupplementaryInformation:Gaze-ContingentOcularParallaxRenderingforVirtualReality•33PSYCHOMETRICFUNCTIONSWepresentthefullsetofpsychometricfunctionsfromourdetectionanddiscriminationthresholdexperimentsinFigure3.ThepsignifitPythonpackage[Schüttetal.2016;WichmannandHill2001a,b],wasusedforfittingpsychometricfunctionstothedatausingBayesianinference.Detection Psychometric FunctionsAbsolute Distance: 2DRelative Offset in DRelative Offset in DProportion CorrectProportion CorrectProportion CorrectProportion CorrectProportion CorrectProportion CorrectProportion CorrectProportion CorrectProportion CorrectProportion CorrectProportion CorrectProportion CorrectRelative Offset in DRelative Offset in DRelative Offset in DRelative Offset in DAbsolute Distance: 3DAbsolute Distance: 1DDistance from 0D: 2DDistance from 0D: 3DDistance From 0D: 1DDiscrimination Psychometric FunctionsFig.3.DetectionandDiscriminationpsychometricfunctions.Theleftcolumnshowsthepsychometricfunctionsfortheocularparallaxdetectionexperiment,whiletherightcolumnpresentsthefunctionsfortheocularparallaxdiscriminationexperiment.Eachrowofeachcolumncorrespondstothepsychometricfunctionsmeasuredforonesubjectfor3differentstimulusconditions.Theverticallinesintersectingthepsychometricfitindicatethethresholdcorrespondingtothe75%correctresponserate,andthehorizontallineabuttingitrepresentingthe95%confidenceinterval.Lowerthresholdscorrespondtosubjectsbeingmoresensitivetotheocularparallaxeffect.4•RobertKonrad,AnastasiosAngelopoulos,andGordonWetzstein4ADDITIONALDETAILSONEGOCENTRICDEPTHSTUDYMultiplereportshaveshownthategocentricdepthperception,orthesubjectiveperceiveddistancetoobjectsaroundone’sself,isshorterinvirtualenvironmentsthaninrealones[Renneretal.2013].Factorslikevergenceandaccommodation[Wattetal.2005],thequalityofgraphics[KnappandLoomis2003],andeventheweightandinertiaoftheHMDitselfhavebeendeemedtocontributetothiseffect.Ocularparallaxrenderingcouldreducetheunderestimationbyminimizingdisparitydistortionsandprovidingadditionaldepthinformationthroughmicroparallaxandgaze-contingentocclusions.Weinvestigateparticipants’egocentricdepthestimationintheiractionorpersonalspace(i.e.withinarm’sreach),wheretheocularparallaxeffectisstrongest.Becauseverbalestimateshavebeenshowntobevariable[Napieralskietal.2011],werelyonablindreachingtaskwhereusersviewatargetandthen,withthedisplayturnedoff,reachtowhereitwaslastseen.Theblindreachingtaskisthestandardwaytoevaluateegocentricdistanceestimationforobjectswithinarm’sreach[Altenhoffetal.2012].Sixteenvolunteersparticipatedinthestudy(agerange22-32,5females),ofwhichonewasexcludedfornotpassingastandardRandotstereovisiontest,andtwootherswereexcludedduetotheeyetrackerfailingtotracktheirpupils.Stimuliandstudysetup.Tominimizeanypotentialeffectsofgraphicsqualityondepthjudgment[KnappandLoomis2003],theviewingenvironmentfortheblindreachingtaskconsistedofaphoto-realisticdiningroomsceneasseeninFigure4.Afloatingpencilservedasthereachingtargetandwassurroundedbyobjectsatdifferentdepthsandafeaturerichbackground,emphasizingtheocularparallaxeffect.TheHTCViveProwithPupilLabseyetrackersservedastheHMD;6-DOFheadposetrackingwasenabled.Thehand’spositionwastrackedviaanHTCVivetrackermountedonanopticalpostthatwasheldupright(Figure4,topright).Tofacilitatenaturalreaching,participantsheldthetrackerintheirdominanthandandanHTCControllerintheirotherhandforinteraction.Conditions.Eachparticipantperformedtheblindreachingtaskwithocularparallaxenabledanddisabled.Tomaintainacommonreachmetricbetweenparticipants,reachtargetdistancesweresetproportionallytoeachparticipants’armreach.Thereachtargetsappearedat50,58,67,75,82,and90%oftheparticipant’smaximumarmreach.Theparticipantscompletedtrialsundereachofthe12conditions5timesandthepresentationorderofthe60trialswasrandomized.Procedure.Eachparticipantperformedanipd,eyetracker,andmaximumarmreachcalibrationprocedureatthebeginningofthesession.Todeterminetheirmaximumarmreach,participantswereinstructedtofullyextendtheirarmdirectlyinfrontoftheirhead.Thedistancebetweentheeyes’midpointandthehanddefinesthemaximumarmreach.Astandardblindreachingtaskwasthenconducted[Altenhoffetal.2012].Participantsfirstobservedtheviewingenvironmenttofamiliarizethemselveswithitssizeandobjectdepths.Theythenbeganthetrials.Eachparticipantperformedtwopracticetrialsfollowedby60recordeddistanceestimateswitheachtrialconsistingofatargetpresentationphaseandablindreachingphase.Inthefinalpresentationphase,thepencilappearedinfrontoftheusersandtheywereinstructedtodetermineitsdepthbylookingatit,butalsotogazetoothersceneobjectstomaximizeeyerotationandtheocularparallaxeffect.Participantswererequiredtoviewtheenvironmentforaminimumof5secondsbeforetheyweregiventheabilitytoturnoffthedisplaywhentheywerereadytoperformtheblindreach.Withthedisplayoff,theywereinstructedtoreachforthepencilwithonlytheirarmwhiletheirtorsoremainedsteady.Theyprogressedontothenexttrialbypullingthecontroller’striggerwhensatisfiedwiththeirreachposition.Allheadandhandposetrackingdata,aswellasthepencil’sposition,wererecordedduringtheblindreachingphase.SupplementaryInformation:Gaze-ContingentOcularParallaxRenderingforVirtualReality•5Analysis.Thereachestimatewascomputedastheaverageofthelast10samplesoftheparticipant’sreachdata.Because6-DOFheadtrackingwasenabled,naturalheadmovementresultedindeviationfromthetheheadtotargetdistancesreportedabove.Toaccountforthis,thedatawasanalyzedalongthehorizontal2Dvectorbetweentheeyes’midpointandthetarget’sposition.Boththetarget’sdistanceandreachdistancewerecomputedalongthisvectorasapercentageofthearm’smaximumreach.Results.Enablingocularparallaxrenderinghadlittletonoeffectontheegocentricdepthperceptionoftheparticipants.Figure4showseachparticipant’sreach,asapercentageoftheirmaximumarmreach,toapresentedtargetdistancefortheocularparallaxenabledanddisabledconditions.Linearmodelswerefittothetwosetsofreaches,findingthattheslopesfortheocularparallaxenabledanddisabledmodeswere1.141and1.128,respectively,whiletheinterceptswere0.167and0.165,respectively.Amultipleregressionanalysisdidnotshowasignificantdifferencebetweenthetwoconditions.Fig.4.Egocentricdepthperception.Thisstudyinvestigateswhethertheadditionaldepthcuesinocularparallaxaidegocentricdepthperception.Theviewingenvironmentincludingthereachtarget,apencil,areshowninthetopleft.Usersviewedthetargetandthen,withthedisplayturnedoff,reachedtoitasseeninthetopright.Participants’reachesintheocularparallaxenabledanddisabledmodesarefoundinthebottomplot.6•RobertKonrad,AnastasiosAngelopoulos,andGordonWetzsteinREFERENCESBlissM.Altenhoff,PhillipE.Napieralski,LindsayO.Long,JeffreyW.Bertrand,ChristopherC.Pagano,SabarishV.Babu,andTimothyA.Davis.2012.EffectsofCalibrationtoVisualandHapticFeedbackonNear-fieldDepthPerceptioninanImmersiveVirtualEnvironment.InProceedingsoftheACMSymposiumonAppliedPerception(SAP’12).ACM,NewYork,NY,USA,71–78.GeoffreyP.Bingham.1993.Opticalflowfromeyemovementwithheadimmobilized:“Ocularocclusion”beyondthenose.VisionResearch33,5(1993),777–789.JoshuaKnappandJackLoomis.2003.VisualPerceptionofEgocentricDistanceinRealandVirtualEnvironments.Vol.11.21–46.PhillipE.Napieralski,BlissM.Altenhoff,JeffreyW.Bertrand,LindsayO.Long,SabarishV.Babu,ChristopherC.Pagano,JustinKern,andTimothyA.Davis.2011.Near-fieldDistancePerceptioninRealandVirtualEnvironmentsUsingBothVerbalandActionResponses.ACMTrans.Appl.Percept.8,3,Article18(Aug.2011),19pages.RebekkaS.Renner,BorisM.Velichkovsky,andJensR.Helmert.2013.ThePerceptionofEgocentricDistancesinVirtualEnvironments-AReview.ACMComput.Surv.46,2(2013),23:1–23:40.S.J.Watt,K.Akeley,M.O.Ernst,andM.S.Banks.2005.Focuscuesaffectperceiveddepth.JournalofVision5,10(2005),834–862.