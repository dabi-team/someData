Variance-Reduced Splitting Schemes for Monotone

1

Stochastic Generalized Equations

Shisheng Cui and Uday V. Shanbhag

1
2
0
2

t
c
O
8
1

]

C
O
.
h
t
a
m

[

4
v
8
4
3
1
1
.
8
0
0
2
:
v
i
X
r
a

Abstract

We consider monotone inclusion problems where the operators may be expectation-valued, a class of problems that subsumes

convex stochastic optimization problems as well as subclasses of stochastic variational inequality and equilibrium problems. A direct

application of splitting schemes is complicated by the need to resolve problems with expectation-valued maps at each step, a concern

that is addressed by using sampling. Accordingly, we propose an avenue for addressing uncertainty in the mapping: Variance-

reduced stochastic modiﬁed forward-backward splitting scheme (vr-SMFBS). In constrained settings, we consider structured

settings when the map can be decomposed into an expectation-valued map A and a maximal monotone map B with a tractable

resolvent. We show that the proposed schemes are equipped with a.s. convergence guarantees, linear (strongly monotone A) and

O(1/k) (monotone A) rates of convergence while achieving optimal oracle complexity bounds. The rate statements in monotone

regimes appear to be amongst the ﬁrst and rely on leveraging the Fitzpatrick gap function for monotone inclusions. Furthermore, the

schemes rely on weaker moment requirements on noise and allow for weakening unbiasedness requirements on oracles in strongly

monotone regimes. Preliminary numerics on a class of two-stage stochastic variational inequality problems reﬂect these ﬁndings

and show that the variance-reduced schemes outperform stochastic approximation schemes and sample-average approximation

approaches. The beneﬁts of attaining deterministic rates of convergence become even more salient when resolvent computation

is expensive.

I. INTRODUCTION

T HE generalized equation (alternately referred to as the inclusion problem) represents a crucial mathematical object in

decision and control theory, representing a set-valued generalization to the more standard root-ﬁnding problem which
requires solving F (x) = 0, where F : Rn → Rn is single-valued. Speciﬁcally, if T is a set-valued map, deﬁned as T : Rn ⇒ Rn,

and T is characterized by a distinct structure in that it can be cast as the sum of two operators A and B, then the generalized

equation (GE) takes the form

0 ∈ T (x) (cid:44) A(x) + B(x).

(GE)

Here A : Rn → Rn is a single-valued map and B : Rn ⇒ Rn is a set-valued map. While such objects have a storied history,

an excellent overview was ﬁrst provided by Robinson [1]. Generalized equations have been extensively examined since the 70s

when Rockafellar [2] developed a proximal point scheme for a generalized equation characterized by monotone operators. In

fact, this scheme subsumes a range of well known schemes such as the augmented Lagrangian method [3], Douglas-Rachford

(To be modiﬁed)This paragraph of the ﬁrst footnote will contain the date on which you submitted your paper for review. It will also contain support

information, including sponsor and ﬁnancial support acknowledgment. For example, “This work was supported in part by the U.S. Department of Commerce

under Grant BS123456.”

Shisheng Cui and Uday V. Shanbhag are with the Department of Industrial and Manufacturing Engineering, The Pennsylvania State University, University

Park, PA 16802 USA (e-mail: suc256@psu.edu; udaybag@psu.edu).

 
 
 
 
 
 
2

splitting [4], amongst others. It can be observed that a large class of optimization and equilibrium problems can be modeled

as (GE), including the necessary conditions of nonlinear programming problems, variational inequality and complementarity

problems, and a broad range of equilibrium problems (cf. [1]). Under suitable requirements on A and B, a range of splitting

methods can be developed and has represented a vibrant area of research over the last two decades [4]–[7].

In this paper, we consider addressing the stochastic counterpart of generalized equations, a class of problems that has seen

recent study via sample-average approximation (SAA) techniques [8]. Formally, the stochastic generalized equation requires
an x ∈ Rn such that

0 ∈ E[T (x, ξ(ω))] (cid:44) E[A(x, ξ(ω))] + B(x),

(SGE)

where the components of the map A are denoted by Ai, i = 1, . . . , n, ξ : Ω → Rd is a random variable, Ai : Rn × Ω ⇒ Rn
is a set-valued map, E[·] denotes the expectation, and the associated probability space is given by (Ω, F, P). In the remainder

of this paper, we refer to A(x, ξ(ω)) by A(x, ω). The expectation of a set-valued map leverages the Aumann integral [9] and
vi(ω) ∈ Ai(x, ξ(ω))(cid:9) . Consequently, the expectation E[A(x, ω)]
is formally deﬁned as E[Ai(x, ξ(ω))] = (cid:8)(cid:82) vi(ω)dP (ω) |
can be deﬁned as a Cartesian product of the sets E[Ai(x, ω)], deﬁned as E[A(x, ω)] (cid:44) (cid:81)n
E[Ai(x, ω)]. We motivate (SGE)
E[g(x, ω)], where
by considering some examples. Consider the stochastic convex optimization problem [10]–[12] given by min
x∈X

i=1

g(•, ω) is a differentiable convex function for every ω and X is a closed and convex set. Such a problem can be equivalently
stated as 0 ∈ T (x) (cid:44) E[G(x, ω)] + NX (x), where G(x, ω) = ∇g(x, ω) and NX (x) denotes the normal cone of X at x.

In fact, the single-valued stochastic variational inequality problems [13]–[15] can be cast as stochastic inclusions as well as
seen by 0 ∈ T (x) (cid:44) E[F (x, ω)] + NX (x), where F (•, ω) is a realization of the mapping. This introduces a pathway for

examining stochastic analogs of trafﬁc equilibrium [16] and Nash equilibrium problems [17] as well as a myriad of other

problems subsumed by variational inequality problems [18]. We describe two problems that have seen recent study which can

also be modeled as SGEs, allowing for developing new computational techniques.

A. Two motivating examples

(a) A subclass of stochastic multi-leader multi-follower games. Consider a class of multi-leader multi-follower games [19]–[22]

with N leaders, denoted by {1, · · · , N} and M followers, given by {1, · · · , M}. In general, this class of games is challenging

to analyze since the player problems are nonconvex and early existence statements have relied on eliminating follower-level

decisions, leading to a noncooperative game with convex nonsmooth player problems. Adopting a similar approach in examining

a stochastic generalization of a quadratic setting examined in [23] with a single follower where M = 1, suppose the follower

problem is

min
yi ≥ li(xi)

1

2 yT

i Qiyi − bi(xi)Tyi,

(Followi(xi))

where Qi is a positive deﬁnite and diagonal matrix, bi(•) and li(•) are afﬁne functions. Suppose the leaders compete in a

Cournot game in which the ith leader solves

min
xi ∈ Xi

ci(xi) − E[p(X, ω)xi] + aiyi(xi),

(Leaderi(x−i))

where ci : Xi → R is a smooth convex function, the inverse-demand function p(•) is deﬁned as p(X) (cid:44) d(ω) − r(ω)X,
d(ω), r(ω) > 0 for every ω ∈ Ω, X (cid:44) (cid:80)N

i=1 xi, yi(xi) denotes a best-response of follower i, and Xi is a closed and convex

3

set in R. Follower i’s best-response yi(x), given leader-level decisions x, can be derived by considering the necessary and

sufﬁcient conditions of optimality:

yi(xi) = max{Q−1

i bi(xi), li(xi)}.

Consequently, we may eliminate the follower-level decision in the leader level problem, leading to a nonsmooth stochastic

Nash equilibrium problem given by the following:

ci(xi) − E[p(X, ω)xi]

min
xi∈Xi

(Leaderi(x−i))

+ aT

i max{Q−1

i bi(xi), li(xi)}.

Under convexity of bi(•) and li(•), and suitable assumptions on Qi and ai, the expression aT

i bi(xi), li(xi)} is a
convex function in xi, a fact that follows from observing that this term is a scaling of the maximum of two convex functions.
Consequently, the necessary and sufﬁcient equilibrium conditions of this game are given by 0 ∈ ∇xici(xi) + E[r(ω)(X +
xi) − d(ω)] + ∂xihi(xi) + NXi (xi) for i = 1, . . . , N where hi(•), deﬁned as hi(xi) (cid:44) aT
i bi(xi), li(xi)}, is a convex
function in xi. Then the necessary and sufﬁcient equilibrium conditions are given by

i max{Q−1

i max{Q−1

0 ∈ T (x) (cid:44) A(x) + B(x),

(SGEmlf )

where A(x) (cid:44) G(x) + R(x), B(x) (cid:44) D(x) + NX (x).

i(xi))N

Here G(x) (cid:44) (c(cid:48)

i=1, R(x) (cid:44) E[r(ω)(X1 + x) − d(ω)1] and D(x) (cid:44) E[∂xihi(xi)]. We observe that G is a monotone
map while D(x) is the Cartesian product of the expectations of subdifferentials of convex functions, implying that D is also
monotone. Furthermore, R is monotone since ∇xR(x) = E[r(ω)(I + 11T)] (cid:23) 0. Since NX is a normal cone of a convex set,

it is also a monotone map, implying that T is monotone.

(b) Model predictive control (MPC) with probabilistic and risk constraints. Model-predictive control (MPC) is a framework for

the control of complex systems [24]. It obviates the challenging derivation/computation of a feedback control law with repeated

resolution of a ﬁnite-horizon constrained optimization problem. Contending with uncertainty has prompted the development

of several approaches: (i) Robust approaches. Robust frameworks for MPC [25]–[30] often require bounded and deterministic

descriptions of uncertainty, a property inherited from robust optimization [31]; (ii) Probabilistic framework. Under a probabilistic

representation of the uncertainty, chance-constrained MPC framework [32]–[37] can be adopted, allowing for shaping the

probability distribution of system states. Such avenues have assumed relevance in settings such as climate control, process

control, power systems operation, and vehicle path planning (cf. [38] for an excellent survey). Suppose the dynamics are

captured by a linear discrete-time system, deﬁned as

xt+1 = A(δ)xt + Bu(δ)ut + Bw(δ)wt,

(1)

where x0 is given, xt ∈ X denotes the state of the system at time k, ut ∈ U ⊆ Rm represents the control input vector at
time t, and wt ∈ Rp is an unmeasurable disturbance signal at time t. In addition, X and U represent the set of states and
controls, respectively while the random matrices A(δ), Bu(δ), Bw(δ) lie in Rn×n, Rn×m, and Rn×p, respectively. We assume
access to the distributions governing δ and wk. Suppose π (cid:44) {π0(·), · · · , πN −1(·)} represents feedback-control policy where
πi : Rn → U ⊆ Rm denotes the state feedback control law for i = 0, 1, · · · , N − 1. We may then formally deﬁne the value
(cid:105)
where Ex0 [•] (cid:44) E[• | x0]. In addition, suppose Xc denotes a
function VN as VN (x, π) (cid:44) Ex0
i=1 J(xi, ui) + JN (xN )
set of undesirable outcomes. The resulting chance-constrained stochastic control problem requires determining the feedback-

(cid:104)(cid:80)N

control law π that minimizes VN (x, π) subject to the prescribed dynamics and probabilistic requirements on the state. This

4

problem is challenging, motivating the construction of a ﬁnite-horizon open-loop counterpart. To this end, we deﬁne xt,T and
ut,T as xt,T (cid:44) {xt, · · · , xt+T −1)} and ut,T (cid:44) {ut, · · · , ut+T −1)}, respectively while the ﬁnite-horizon value function at
(cid:105)
the t-th step looking T periods ahead, denoted by Vt,T (xt,T , ut,T ), is deﬁned as Vt,T (xt,T , ut,T ) (cid:44) Ext
i=t+1 J(xi, ui)
where T (cid:28) N . Given a horizon T , the resulting MPC framework [39] requires minimizing Vt,T (xt,T , ut,T ) subject to the

(cid:104)(cid:80)T

prescribed dynamics and the probabilistic state-constraints, given xt. A formal deﬁnition of the chance-constrained stochastic

control problem (CC-SC) and its ﬁnite-horizon counterpart (CC-MPCT

t ) is provided next.

(CC-SC) max
π,x

VN (x, π)

(CC-MPCT
t )

max
ut,T ,xt,T

Vt,T (xt,T , ut,T )

s.t. Px0

(cid:8)ω : xi ∈ Xc , ∀i ∈ [1; N ](cid:9) ≤ (cid:15)

s.t. Pxt

(cid:8)ω : xi ∈ Xc , ∀i ∈ [k; T ](cid:9) ≤ (cid:15)

.

where xk evolves as per (1)

.

where xt evolves as per (1)

The control decision ut is obtained from resolving (CC-MPCt,T ) and is then applied to the system after which the window is

moved ahead. The resulting problem (CC-MPCt+1,T ) is then resolved when t + T < N (alternately, the horizon T is reduced

appropriately). This formulation is relatively ﬂexbile and and can be used to address diverse types of objectives and constraints.

In general, the problem (CC-MPCT

t ) is challenging, owing to the presence of the chance constraint. The probability function

can be recast as an expectation of an indicator function over a set but this leads to discontinuous integrands. Recently, the

second author has developed avenues where under prescribed assumptions under which the following holds [40].

P [ζ ∈ K | ζ ∈ K(x)] = Eξ [F (x, ξ)] ,

(2)

where K is a set in Rn symmetric about the origin, K(x) is deﬁned as K(x) (cid:44) {ζ : c(x, ζ) ≥ 0}, T ∈ Rd×n, and

c(x, ζ) (cid:44)




1 − |ζ Tx|m,

Setting A



T x − ζ.

Setting B

The integrand F (•, ξ) is deﬁned appropriately in Settings A and B where in each case, it is shown that ξ. In fact, we can then
show that a composition of E[F (•, ξ)] is convex; e.g. in Setting A, 1/E[F (•, ξ)] is convex. For expository ease, we may recast

(CC-MPCT

t ) as the following chance-constrained problem (CCP) and provide its necessary and sufﬁcient optimality conditions

in (SGEccp).

where g(x) (cid:44)

deﬁned as

(CCP) min

x

Eω[h(x, ω)]

s.t. g(x) ≤ 0,

(λ)

(Reg. conds)
≡

(SGEccp) Find z (cid:44) (x, λ) such that

0 ∈ T (z) (cid:44) H(z) + NZ (z),

.

1

Eξ[F (x,ξ)] − 1

(1−(cid:15)) , Z (cid:44) X × R+

m, NZ (z) denotes normal cone of Z at z, H is a monotone set-valued map

H(x, λ) (cid:44) {∇xf (x) + λ∂xg(x)} × {g(x)}.

We close by noting that monotone inclusions with expectation-valued operators are of crucial relevance in decision and

control problems, providing a strong motivation for addressing their tractable resolution.

B. Related work.

We provide a brief review of prior research.

(a) Stochastic operator splitting schemes. The regime where the maps are expectation-valued has seen relatively less study [43].

Stochastic proximal gradient schemes [44]–[48] are an instance of stochastic operator splitting techniques where A is either the

TABLE I

VARIANCE-REDUCED VS STOCHASTIC OPERATOR-SPLITTING

SCHEMES FOR SGES

5

Alg/Prob.

A, B

Biased

[41]

[41]

[42]

LA, MM

LA,
σA , µB
LA, MM

(vr-SMFBS)

σA , MM

(vr-SMFBS)

LA, MM

N

N

N

Y

N

γk ; Nk
SS, NS; 1

SS, NS; 1

SS, NS; 1

γ;(cid:98)ρ−(k+1)(cid:99)

ρ < 1

γ;(cid:98)ka (cid:99)

a > 1

Statements

xk

k→∞
−−−−−→
a.s

x∗ ∈ X ∗

E[(cid:107)xk − x∗ (cid:107)2 ] ≤ O(1/k)

x∗ ∈ X ∗

xk

k→∞
−−−−−→
a.s
k→∞
−−−−−→
a.s

x∗
xk
E[(cid:107)xk − x∗ (cid:107)2] ≤ O(qk )
Sample-complexity: O(1/(cid:15))
x∗ ∈ X ∗

xk
E[G(¯xk )] ≤ O(1/k)
Sample-complexity: O(1/(cid:15)2+δ )

k→∞
−−−−−→
a.s

LA: Lipschitz constant of A, MM: Maximal monotone
σA, µB : strong monotonicity constants of A and B
SS: square-summable, NS: non-summable

gradient or the subdifferential operator. In the context of monotone inclusions, when A is a more general monotone operator,

a.s. convergence of the iterates has been proven in [42] and [41] when A is Lipschitz and expectation-valued while B is

maximal monotone. In fact, in our prior work [49], we prove a.s. convergence and derive an optimal rate in terms of the

gap function in the context of stochastic variational inequality problems with structured operators. Stability analysis [50] and

two-timescale variants [51] have also been examined. A rate statement of O( 1

k ) in terms of mean-squared error has also been
provided when A is additionally strongly monotone in [41]. A more general problem which ﬁnds a zero of the sum of three

maximally monotone operators is proposed in [52]. A comparison of rate statements for stochastic operator-splitting schemes

is provided in Table I from which we note that (vr-SMBFS) is equipped with deterministic (optimal) rate statements, optimal

or near-optimal sample-complexity, a.s. convergence guarantees, and does not require imposing a conditional unbiasedness

requirement on the oracle in the strongly monotone regime. We believe our rate statements are amongst the ﬁrst in maximal

monotone settings (to the best of our knowledge).

(b) Other related schemes. A natural approach for resolving SGEs is via sample-average approximation [53], [54]. It proves

the weak a.s. convergence and establishes the rate of convergence in expectation under strong monotonicity assumption.

C. Gaps and resolution.

(i) Poorer empirical performance when resolvents are costly. Deterministic schemes for strongly monotone and monotone

generalized equations display linear and O(1/k) rate in resolvent operations while stochastic analogs display rates

of O(1/k) and O(1/

k), respectively. This leads to far poorer practical behavior particularly when the resolvent

√

is challenging to compute, e.g., in strongly monotone regimes, the complexity in resolvent operations can increase

from O(log(1/(cid:15))) to O(1/(cid:15)). The proposed scheme (vr-SMBFS) achieve deterministic rates of convergence with either

identical or slightly worse oracle complexities in both monotone and strongly monotone regimes, allowing for run-times

comparable to deterministic counterpart.

(ii) Absence of rate statements for monotone operators. To the best of our knowledge, there appear to be no non-asymptotic

rate statements available in monotone regimes. In (vr-SMBFS), rate statements are now provided.

(iii) Biased oracles. In many settings, conditional unbiasedness of the oracle may be harder to impose and one may need to

impose weaker assumptions. Our proposed scheme allows for possibly biased oracles in some select settings.

(iv) State-dependent bounds on subgradients and second moments. Many subgradient and stochastic approximation schemes
impose bounds of the form E[(cid:107)G(x, ω)(cid:107)2] ≤ M 2 where G(x, ω) ∈ T (x, ω) or E[(cid:107)w(cid:107)2 | x] ≤ ν2 where w = ∇xf (x, ω)−

6

∇xf (x). Both sets of assumptions are often challenging to impose non-compact regimes. Our scheme can accommodate

state-dependent bounds to allow for non-compact domains.

D. Outline and contributions.

We now articulate our contributions. In Section III, we consider the resolution of monotone inclusions in structured regimes

where the map can be expressed as the sum of two maps, facilitating the use of splitting. In this context, when one of the maps

is expectation-valued while the other has a cheap resolvent, we consider a scheme where a sample-average of the expectation-

valued map is utilized in the forward step. When the sample-size is increased at a suitable rate, the sequence of iterates is

shown to converge a.s. to a solution of the constrained stochastic generalized equation in both monotone and strongly monotone

regimes. In addition, the resulting sequence of iterates converges either at a linear rate (strongly monotone) or at a rate of

O(1/k) (maximal monotone), leading to optimal oracle complexities of O(1/(cid:15)) and O(1/(cid:15)2+δ) (δ > 0), respectively. Notably,

the strong monotonicity claim is made without an unbiasedness requirement on the oracle while weaker state-dependent noise

requirements are assumed througout. Rate statements in maximally monotone regimes rely on using the Fitzpatrick gap function

for inclusion problems. We believe that the rate statements in monotone regimes are amongst the ﬁrst. In addition, we provide

some background in Section II while preliminary numerics are presented in Section IV.

E. Comments on variance-reduced schemes.

Before proceeding, we brieﬂy digress regarding the term variance-reduced.

(i) Terminology and applicability. The moniker “variance-reduced” reﬂects the usage of increasing accurate approximations of

the expectation-valued map, as opposed to noisy sampled variants that are used in single sample schemes. The resulting schemes

are often referred to as mini-batch SA schemes and often achieve deterministic rates of convergence. This work draws inspiration

from the early work by Friedlander and Schmidt [55] and Byrd et al. [56] which demonstrated how increasing sample-sizes

can enable achieving deterministic rates of convergence. Similar ﬁndings regarding the nature of sampling rates have been

presented in [57]. This avenue has proven particularly useful in developing accelerated gradient schemes for smooth [47], [58]

and nonsmooth [46] convex/nonconvex stochastic optimization, variance-reduced quasi-Newton schemes [59], [60], amongst

others. Schemes such as SVRG [61] and SAGA [62] also achieve deterministic rates of convergence but are customized for

ﬁnite sum problems unlike mini-batch schemes that can process expectations over general probability spaces. Unlike in mini-

batch schemes where increasing batch-sizes are employed, in schemes such as SVRG, the entire set of samples is periodically

employed for computing a step.

(ii) Weaker assumptions and stronger statements. The proposed variance-reduced framework has several crucial beneﬁts that

can often not be reaped in the single-sample regime. For instance, rate statements are derived in monotone regimes which

have hitherto been unavailable. Second, a.s. convergence guarantees are obtained and in some cases require far weaker moment

assumptions. Finally, since the schemes allow for deterministic rates, this leads to far better practical behavior as the numerics

reveal.

(iii) Sampling requirements. Naturally, variance-reduced schemes can generally be employed only when sampling is relatively

cheap compared to the main computational step (such as computing a projection or a prox.) In terms of overall sample-

complexity, the proposed schemes are near optimal. As k becomes large, one might question how one might contend with

Nk tending to +∞. This issue does not arise since most schemes of this form are meant to provide (cid:15)-approximations. For
instance, if (cid:15) = 10−3, then such a scheme requires approximately O(103) steps (in monotone settings). Since Nk ≈ (cid:98)ka(cid:99) and

7

a > 1, we require approximately (O(103))1+a samples in total. In a setting where multi-core architecture is ubiquitous, such

requirements are not terribly onerous particularly since computational costs have been reduced from O(106) (single-sample)

to O(103). It is worth noting that ﬁnite-sum problems routinely have 109 or more samples and competing schemes such as

SVRG would require taking the full batch-size intermittently, which means they use O(109) samples at least to achieve the

same accuracy as our scheme.

II. BACKGROUND

In this section, we provide some background on splitting schemes, building a foundation for the subsequent sections.

Consider the generalized equation

0 ∈ T (x) (cid:44) A(x) + B(x).

(GE)

If the resolvent of either A or B (or both) is tractable, then splitting schemes assume relevance. Notable instances include

Douglas-Rachford splitting [4], [6], Peaceman-Rachford splitting [5], [6], and Forward-Backward splitting (FBS) [6], [7]. (a)

Douglas-Rachford Splitting [4], [6]. In this scheme, the resolvent of A and B can be separately evaluated to generate a sequence

deﬁned as follows.

xk+ 1

2

:= (I + γA)−1(xk)

xk+1 := (I + γB)−1(2xk+ 1

2

− xk) + xk − xk+ 1

2

.

(DRS)

(b) Peaceman-Rachford Splitting [5], [6]. In contrast, in the Peaceman-Rachford splitting method, the the roles of A and B

are exchanged in each iteration, given by the following.

xk+ 1

2

:= (I + γB)−1(I − γA)(xk),

xk+1 := (I + γA)−1(I − γB)(xk+ 1

2

).

(PRS)

(c) Forward-backward splitting [6], [7]. Moreover, if the resolvent of B is easier to evaluate and A and B are maximal

monotone, the forward-backward splitting method [6], [7] was applied to convex optimization in [63]:

xk+1 := (I + γB)−1(I − γA)(xk).

(FBS)

In [42], a stochastic variant of the FBS method, developed for strongly monotone maps, is equipped with a rate of O(1/k)

while in [41], maximal monotone regimes are examined and a.s. convergence statements are provided. A drawback of (FBS)
is the requirement of either a strong monotonicity assumption on A−1, or that A be Lipschitz continuous on dom(A) = Rn

and T be strongly monotone; this motivated the modiﬁed FBS scheme where convergence was proven when A is monotone

and Lipschitz [64].

xk+ 1

2

:= (I + γB)−1(I − γA)(xk),

xk+1 := xk+ 1

2

− γ(A(xk+ 1

2

) − A(xk)).

(MFBS)

In Section III, we develop a variance-reduced stochastic MFBS scheme where A is Lipschitz and monotone, A(x) (cid:44) E[A(x, ω)],

and B is maximal monotone with a tractable resolvent; we derive linear and sublinear convergence under strongly monotone

and merely monotone A, respectively, achieving deterministic rates of convergence.

III. STOCHASTIC MODIFIED FORWARD-BACKWARD SPLITTING SCHEMES

In this section we analyze stochastic (operator) splitting schemes. In the case where A(x) (cid:44) E[A(x, ω)], A : Rn × Ω → Rn,

and B has a cheap resolvent, we develop a variance-reduced splitting framework. In Section III-A, we provide some background

and outline the assumptions and derive convergence theory for monotone and strongly monotone settings in Section III-B and

8

III-C, respectively.

A. Background and assumptions

Akin to other settings that employ stochastic approximation, we assume the presence of a stochastic ﬁrst-order oracle for

operator A that produces a sample A(x, ω) given a vector x. Such a sample is core to developing a variance-reduced modiﬁed
(cid:80)Nk
to approximate E[A(xk, ω)] at iteration k. Given

j=1 A(xk,ωj,k)

forward-backward splitting (vr-SMFBS) scheme reliant on
an x0 ∈ Rn, we formally deﬁne such a scheme next.

Nk

xk+ 1

2

:= (I + γB)−1(xk − γAk),

xk+1 := xk+ 1

2

− γ(Ak+ 1

2

− Ak),

(vr-SMFBS)

where Ak (cid:44)

(cid:80)Nk

j=1 A(xk,ωj,k)

Nk

, Ak+ 1

2

(cid:44)

the following on operators A and B.

(cid:80)Nk

j=1 A(xk+1/2,ωj,k+1/2)

Nk

are estimators of A(xk) and A(xk+ 1

2

), respectively. We assume

Assumption 1. The operator A is single-valued, monotone and L-Lipschitz on Rn, i.e., ∀x, y ∈ Rn, (cid:107)A(x)−A(y)(cid:107) ≤ L(cid:107)x−y(cid:107)
and (A(x) − A(y))T(x − y) ≥ 0; the operator B is maximal monotone on Rn.

Suppose Fk denotes the history up to iteration k, i.e., F0 = {x0},

(cid:40)

Fk

(cid:44)

x0, {A(x0 , ωj,0 )}

N0
j=1

, {A(x 1
2

, ω

j, 1
2

)}

N0
j=1

, · · · , {A(xk−1 , ωj,k−1)}

Nk−1
j=1

,

{A(x

k− 1
2

, ω

j,k− 1
2

(cid:41)

)}

Nk−1
j=1

, F

k+ 1
2

(cid:44) Fk ∪ {A(xk , ωj,k )}

Nk
j=1

.

Suppose wk (cid:44) A(xk, ωk) − A(xk), ¯wk (cid:44)
)−A(xk+ 1
j=1(A(xk+ 1
2
2

))

(cid:80)Nk

,ωj,k+ 1
2
Nk

(cid:80)Nk

j=1(A(xk,ωj,k)−A(xk))

Nk

, wk+ 1

2

(cid:44) A(xk+ 1

2

, ωk+ 1

2

) − A(xk+ 1

2

) and ¯wk+ 1

2

(cid:44)

, where Nk denotes the batch-size of samples A(x, ωj,k) at iteration k. We impose the following

bias and moment assumptions on ¯wk and ¯wk+1/2. Note that Assumption 2(ii) is weakened in the strongly monotone regime,

allowing for biased oracles.

Assumption 2. At iteration k, the following hold in an a.s. sense: (i) The conditional means E[wk | Fk] and E[wk+ 1
are zero for all k in an a.s. sense; (ii) The conditional second moments are bounded in an a.s. sense as follows, i.e. there
exists ν1, ν2 such that E[(cid:107)wk(cid:107)2 | Fk] ≤ ν2

2 and E[(cid:107)wk+1/2(cid:107)2 | Fk+1/2] ≤ ν2

2 for all k in an a.s.

1 (cid:107)xk+1/2(cid:107)2 + ν2

1 (cid:107)xk(cid:107)2 + ν2

| Fk+ 1

]

2

2

sense.

When the feasible set X is possibly unbounded, the assumption that the conditional second moment wk is uniformly bounded

a.s. is often a stringent requirement. Instead, we impose a state-dependent assumption on wk. We conclude this subsection by

deﬁning a residual function for a generalized equation.

Lemma 1 (Residual function for (GE)). Suppose γ > 0, T = A + B, and

rγ(x) (cid:44) (cid:107)x − (I + γB)−1(x − γA(x))(cid:107).

Then rγ is a residual function for (GE).

Proof. By deﬁnition, rγ(x) = 0 if and only if x = (I + γB)−1(x − γA(x)). This can be interpreted as follows, leading to the
conclusion that x ∈ T −1(0).

9

x = ((I + γB)−1(x − γA(x))

⇔ x − γA(x) ∈ (I + γB)(x) ⇔ 0 ∈ A(x) + B(x).

We conclude this subsection with two lemmas [65] crucial for proving claims of almost sure convergence.

Lemma 2. Let vk, uk, δk, ψk be nonnegative random variables adapted to σ-algebra Fk, and let the following relations hold

almost surely:

E[vk+1 | Fk] ≤ (1 + uk)vk − δk + ψk,

∀k;

∞
(cid:88)

k=0

uk < ∞, and

∞
(cid:88)

k=0

ψk < ∞.

Then a.s., limk→∞ vk = v and (cid:80)∞

k=0 δk < ∞, where v ≥ 0 is a random variable.

Lemma 3. Consider a sequence vk of nonnegative random variables adapted to the σ-algebra Fk and satisfying E[vk+1 | Fk] ≤
(1−ak)vk +bk for k ≥ 0 where ak ∈ [0, 1], bk ≥ 0 for every k ≥ 0 and (cid:80)∞
Then vk → 0 a.s. as k → ∞.

k=0 ak = ∞, (cid:80)∞

k=0 bk < ∞, and limk→∞

= 0.

bk
ak

B. Convergence analysis under merely monotone A

In this subsection, we derive a.s. convergence guarantees and rate statements. First, we prove the a.s. convergence of the

sequence generated by this scheme. We start with a lemma.

Lemma 4. Consider a sequence {xk} generated by (vr-SMFBS). Let Assumptions 1 and 2 hold. Suppose γ ≤ 1
2 ˜L
˜L2 (cid:44) L2 + 4ν2
1
N0

. Then for any k ≥ 0,

and

Proof. From the deﬁnition of xk+ 1

2

and xk+1, we have

1 (cid:107)x∗(cid:107)2
+ 25γ2ν2
Nk

E[(cid:107)xk+1 − x∗(cid:107)2 | Fk] ≤

(cid:16)

(cid:17)

1

1 + 25γ2ν2
r2
γ (xk)
4

+ 17γ2ν2
2Nk

−

Nk

2

.

(cid:107)xk − x∗(cid:107)2

xk+ 1

2

+ γvk+ 1

2

= xk − γ(uk + ¯wk),

xk+1 = xk+ 1

2

− γ(uk+ 1

2

+ ¯wk+ 1

2

− uk − ¯wk).

where uk = A(xk), uk+ 1

2

= A(xk+ 1

2

), vk+ 1

2

∈ B(xk+ 1

2

). From 0 ∈ A(x∗) + B(x∗),

u∗ + v∗ = 0, where u∗ = A(x∗),

v∗ ∈ B(x∗)

We have the following equality:

(cid:107)xk − x∗(cid:107)2 = (cid:107)xk − xk+ 1

2

+ xk+ 1

2

− xk+1 + xk+1 − x∗(cid:107)2

= (cid:107)xk − xk+ 1

2

(cid:107)2 + (cid:107)xk+ 1

2

− xk+1(cid:107)2 + (cid:107)xk+1 − x∗(cid:107)2

+ 2(xk − xk+ 1

2

)T(xk+ 1

2

− x∗)

+ 2(xk+ 1

2

− xk+1)T(xk+1 − x∗)

= (cid:107)xk − xk+ 1

2

(cid:107)2 + (cid:107)xk+ 1

2

− xk+1(cid:107)2 + (cid:107)xk+1 − x∗(cid:107)2

+ 2(xk − xk+ 1

2

)T(xk+ 1

2

− x∗) − 2(cid:107)xk+ 1

2

− xk+1(cid:107)2

+ 2(xk+ 1

2

− xk+1)T(xk+ 1

2

− x∗)

= (cid:107)xk − xk+ 1

2

(cid:107)2 − (cid:107)xk+ 1

2

− xk+1(cid:107)2 + (cid:107)xk+1 − x∗(cid:107)2

+ 2(xk − xk+1)T(xk+ 1

2

− x∗)

= (cid:107)xk − xk+ 1

2

(cid:107)2 − γ2(cid:107)uk+ 1

2

+ ¯wk+ 1

2

− uk − ¯wk(cid:107)2

+ (cid:107)xk+1 − x∗(cid:107)2 + 2γ(uk+ 1

2

+ vk+ 1

2

+ ¯wk+ 1

2

)T(xk+ 1

2

− x∗).

By Lemma 1, rγ is a residual function for (GE), deﬁned as rγ(x) (cid:44) (cid:107)x − ((I + γB)−1(x − γA(x))(cid:107). It follows that

γ(xk) = (cid:107)xk − (I + γB)−1(xk − γA(xk))(cid:107)2
r2

= (cid:107)xk − xk+ 1

2

+ xk+ 1

2

− (I + γB)−1(xk − γA(xk))(cid:107)2

≤ 2(cid:107)xk − xk+ 1

2

(cid:107)2

+ 2(cid:107)(I + γB)−1(xk − γAk) − (I + γB)−1(xk − γA(xk))(cid:107)2

≤ 2(cid:107)xk − xk+ 1

2

(cid:107)2 + 2γ2(cid:107) ¯wk(cid:107)2,

where the last inequality holds because (I + γB)−1 is a non-expansive operator. Consequently, we have that

Following (3), we have

−(cid:107)xk − xk+ 1

2

(cid:107)2 ≤ − 1

2 r2

γ(xk) + γ2(cid:107) ¯wk(cid:107)2.

10

(3)

(4)

(cid:107)xk+1 − x∗(cid:107)2 = (cid:107)xk − x∗(cid:107)2 − (cid:107)xk − xk+ 1

2

(cid:107)2

+ γ2(cid:107)uk+ 1

2

+ ¯wk+ 1

2

− uk − ¯wk(cid:107)2

− 2γ(uk+ 1

2

+ vk+ 1

2

+ ¯wk+ 1

2

)T (xk+ 1

2

− x∗)

= (cid:107)xk − x∗(cid:107)2 − (cid:107)xk − xk+ 1

2

(cid:107)2

+ γ2(cid:107)uk+ 1

2

+ ¯wk+ 1

2

− uk − ¯wk(cid:107)2

− 2γ(uk+ 1

2

+ vk+ 1

2

)T(xk+ 1

2

− x∗) − 2γ ¯wT

k+ 1
2

(xk+ 1

2

− x∗)

≤ (cid:107)xk − x∗(cid:107)2 − (cid:107)xk − xk+ 1

2

(cid:107)2 + 2γ2

k(cid:107)uk+ 1

2

− uk(cid:107)2

+ 2γ2

k(cid:107) ¯wk+ 1

2

− ¯wk(cid:107)2 − 2γ(uk+ 1

2

+ vk+ 1

2

)T(xk+ 1

2

− x∗)

− 2γ ¯wT

k+ 1
2

(xk+ 1

2

− x∗)

≤ (cid:107)xk − x∗(cid:107)2 − (1 − 2γ2L2)(cid:107)xk − xk+ 1

2

(cid:107)2

2

+ vk+ 1

− 2γ (uk+ 1
(cid:124)
+ 2γ2(cid:107) ¯wk+ 1
≤ (cid:107)xk − x∗(cid:107)2 − (cid:0) 1

2

+ 2γ2(cid:107) ¯wk − ¯wk+ 1

2

)T(xk+ 1
2
(cid:123)(cid:122)
≥ 0

2

− x∗)
(cid:125)

2 − 2γ2L2(cid:1) (cid:107)xk − xk+ 1
(x∗ − xk+ 1
(cid:107)2 + 2γ ¯wT

2

(cid:107)2

)

k+ 1
2

2

− ¯wk(cid:107)2 − 2γ ¯wT

k+ 1
2

(xk+ 1

2

− x∗)

(5)

(cid:107)2

− 1

2 (cid:107)xk − xk+ 1
≤ (cid:107)xk − x∗(cid:107)2 − (cid:0) 1

(4)

2

2 − 2γ2L2(cid:1) (cid:107)xk − xk+ 1

(cid:107)2

2

+ 2γ2(cid:107) ¯wk − ¯w

(cid:107)2 + 2γ ¯wT

k+ 1
2

(x∗ − xk+ 1

2

)

k+

1
2

11

4 r2

γ(xk) + 1
− 1
≤ (cid:107)xk − x∗(cid:107)2 − (cid:0) 1

2 γ2(cid:107) ¯wk(cid:107)2

+ 4γ2(cid:107) ¯wk+ 1

2

(cid:107)2 + 2γ ¯wT

2 − 2γ2L2(cid:1) (cid:107)xk − xk+ 1
(x∗ − xk+ 1
) − 1

2

k+ 1
2

2

4 r2

γ(xk).

(cid:107)2 + 9

2 γ2(cid:107) ¯wk(cid:107)2

Taking expectations conditioned on Fk, we obtain the following bound:

E[(cid:107)xk+1 − x∗(cid:107)2 | Fk] ≤ (cid:107)xk − x∗(cid:107)2

− ( 1

2 − 2γ2L2)E[(cid:107)xk − xk+ 1

+ E[E[4γ2(cid:107) ¯wk+ 1

2

(cid:107)2 | Fk+ 1

2

2

(cid:107)2| Fk]
] | Fk] + E (cid:2) 9

− E[E[2γ ¯wT

k+ 1
2

(xk+ 1

2

− x∗) | Fk+ 1

2

] | Fk] − 1

≤ (cid:107)xk − x∗(cid:107)2 − ( 1

4γ2(ν2
1

E[(cid:107)xk+ 1
2
Nk

+

2 − 2γ2L2)E[(cid:107)xk − xk+ 1
1 (cid:107)xk(cid:107)2+ν2
2 γ2(ν2
2 )

(cid:107)2|Fk]+ν2
2 )

9

2

+

Nk

≤ (cid:107)xk − x∗(cid:107)2 − ( 1
E[(cid:107)xk−xk+ 1

4γ2(2ν2
1

2 − 2γ2L2)E[(cid:107)xk − xk+ 1

2

(cid:107)2|Fk]+2ν2
2
Nk

1 (cid:107)xk(cid:107)2+ν2
2 )

(cid:3)

2 γ2(cid:107) ¯wk(cid:107)2 | Fk
4 r2
(cid:107)2| Fk]

γ(xk)

γ(xk)

− 1

4 r2
(cid:107)2| Fk]

+

+

+

≤

9

2 γ2(ν2

1 (cid:107)xk(cid:107)2+ν2
2 )

Nk
≤ (cid:107)xk − x∗(cid:107)2 − ( 1

4 r2

− 1
γ(xk)
2 − 2γ2L2 − 8γ2ν2
1 (cid:107)x∗(cid:107)2)

Nk
1 (cid:107)xk−x∗(cid:107)2+2ν2
+ 17γ2ν2
Nk
2Nk
(cid:17)
(cid:107)xk − x∗(cid:107)2 + 25γ2ν2
1 (cid:107)x∗(cid:107)2
Nk

Nk

2

1

25
2 γ2(2ν2

(cid:16)

1 + 25γ2ν2

1

(cid:107)2| Fk]

)E[(cid:107)xk − xk+ 1
r2
γ (xk)
4

−

2

+ 17γ2ν2
2Nk

2

−

r2
γ (xk)
4

,

where the penultimate inequality follows from noting that 1
˜L2 (cid:44) L2 + 4ν2
1
N0

.

2 − 2γ2L2 − 8γ2ν2

Nk

1

≥ 1

2 − 2γ2(L2 + 4ν2

1
N0

)) ≥ 0, if γ ≤ 1
2 ˜L

and

Theorem 1 (a.s. convergence of (vr-SMFBS)). Consider a sequence {xk} generated by (vr-SMFBS). Let Assumptions 1
and ˜L2 (cid:44) L2 + 4ν2
and 2 hold. Suppose γ ≤ 1
1
2 ˜L
N0
any x0 ∈ Rn, {xk} converges to a solution x∗ ∈ X ∗ (cid:44) T −1(0) in an a.s. sense.

, where {Nk} is a non-decreasing sequence, and (cid:80)∞

< M . Then for

1
Nk

k=0

Proof. We may now apply Lemma 2 which allows us to claim that {(cid:107)xk − x∗(cid:107)} is convergent for any x∗ ∈ X ∗ and
(cid:80)

k rγ(xk)2 < ∞ in an a.s. sense. Therefore, in an a.s. sense, we have

lim
k→∞

r2
γ(xk) = 0.

Since {(cid:107)xk −x∗(cid:107)2} is a convergent sequence in an a.s. sense, {xk} is bounded a.s. and has a convergent subsequence. Consider

any convergent subsequence of {xk} with index set denoted by K and suppose its limit point is denoted by ¯x. We have that

limk∈K rγ(xk) = rγ(¯x) = 0 a.s. since rγ(·) is a continuous function. It follows that ¯x is a solution to 0 ∈ T (x). Consequently,
some convergent subsequence of {xk}k≥0 , denoted by K, satisﬁes limk∈K xk = ¯x ∈ X ∗ a.s.. Since {(cid:107)xk − x∗(cid:107)} is convergent
a.s. for any x∗ ∈ X ∗, it follows that {(cid:107)xk − ¯x(cid:107)2} is convergent a.s. and its unique limit point is zero. Thus every subsequence
of {xk} converge a.s. to ¯x which leads to the claim that the entire sequence of {xk} is convergent to a point ¯x ∈ X ∗.

When the sampling process is computationally expensive (i.e., such as in the queueing systems or PDE, etc.), we prove the

following corollary regarding (vr-SMFBS) with Nk = 1 for every k.

12

Corollary 1 (a.s. convergence under single sample). Consider a sequence {xk} generated by (vr-SMFBS). Let Assumptions 1
and 2 hold. Suppose (cid:80)∞
k < ∞ and Nk = 1 for every k ∈ Z+. In addition, suppose A is co-coercive with
. Then {xk} converges to a solution

k γk = ∞, (cid:80)∞
constant c and strictly monotone. Furthermore, suppose γk ≤ min
x∗ ∈ X ∗ (cid:44) T −1(0) in an a.s. sense.

2(L2+4ν2
1 )

1
4cL2 ,

k γ2

1√

(cid:26)

(cid:27)

To establish the rate under maximal monotonicity, we need introduce a metric for ascertaining progress. In strongly monotone

regimes, the mean-squared error serves as such a metric while the function value represents such a metric in optimization

regimes. In merely monotone variational inequality problems, a special case of monotone inclusion problems, the gap function

has proved useful (cf. [18], [66]). When considering the more general monotone inclusion problem, Borwein and Dutta presented

a gap function [67], inspired by the Fitzpatrick function [68], [69].

Deﬁnition 1 (Gap function). Given a set-valued mapping T : Rn ⇒ Rn, then the gap function G associated with the inclusion

problem 0 ∈ T (x) is deﬁned as

G(x) (cid:44) sup

y∈dom(T )

sup
z∈T (y)

zT(x − y),

∀x ∈ Rn.

The gap function is nonnegative for all x ∈ Rn and is zero if and only if 0 ∈ T (x). To derive the convergence rate under

maximal monotonicity, we require boundedness of the domain of T as formalized by the next assumption.

Assumption 3. The domain of T is bounded, i.e. (cid:107)x(cid:107) ≤ DT ,

∀x ∈ {v ∈ Rn | T (v) (cid:54)= ∅}.

Clearly, from the deﬁnition, a convex gap function can be extended-valued and its domain is contingent on the boundedness

properties of dom T . When dom T is bounded, the gap function is globally deﬁned but when dom T is unbounded, one

resolution is based on the notion of restricted merit functions, ﬁrst introduced in [70]. In this approach, the gap function is

deﬁned on a bounded set which belongs to dom T . In such instances, a local rate of convergence can be obtained.

We begin by establishing an intermediate result.

Lemma 5. Let Assumptions 1 and 2 hold. Suppose {xk} denotes a sequence generated by (vr-SMBFS). Then for all y ∈

dom(T ), z ∈ T (y) and all k ≥ 0,

2γzT(xk+ 1

2

− y) ≤ (cid:107)xk − y(cid:107)2 − (cid:107)xk+1 − y(cid:107)2

− (1 − 2γ2L2)(cid:107)xk − xk+ 1

2

(cid:107)2 + 2γ2(cid:107) ¯wk+ 1

2

− ¯wk(cid:107)2

+ 2γ ¯wT

k+ 1
2

(y − xk+ 1

2

).

Proof. According to (5) and replacing x∗ with y ∈ dom(T ), we have that

2γzT(xk+ 1

2

− y) ≤ (cid:107)xk − y(cid:107)2 − (cid:107)xk+1 − y(cid:107)2

− (1 − 2γ2L2)(cid:107)xk − xk+ 1

2

(cid:107)2 + 2γ2(cid:107) ¯wk+ 1

2

− ¯wk(cid:107)2

− 2γ ¯wT

k+ 1
2

(xk+ 1

2

− y)

≤ (cid:107)xk − y(cid:107)2 − (cid:107)xk+1 − y(cid:107)2 − (1 − 2γ2L2)(cid:107)xk − xk+ 1

2

(cid:107)2

+ 2γ2(cid:107) ¯wk+ 1

2

− ¯wk(cid:107)2 + 2γ ¯wT

k+ 1
2

(y − xk+ 1

2

).

(6)

Invoking Lemma 5, we derive a rate statement for ¯xK, an average of the iterates {xk+1/2} generated by (vr-SMFBS) over

the window constructed from 0 to K − 1:

¯xK (cid:44)

(cid:80)K−1

k=0 xk+ 1
2
K

.

(7)

13

k=0 Nk < M .

Proposition 1 (Rate statement under monotonicity). Consider the (vr-SMFBS) scheme. Suppose x0 ∈ Rn and let {¯xK}
be deﬁned in (7). Let Assumptions 1 – 3 hold. Suppose γ ≤ 1
2 ˜L
(cid:80)∞
(a) For any K ≥ 1, E[G(¯xK)] = O (cid:0) 1
(b) Suppose Nk = (cid:98)ka(cid:99), for a > 1. Then the oracle complexity to compute an ¯xK+1 such that E[G(¯xK+1) ≤ (cid:15) is bounded as
(cid:80)K

, {Nk} is a non-decreasing sequence, and

and ˜L2 (cid:44) L2 + 4ν2
1
N0

(cid:1) .

K

k=0 Nk ≤ O (cid:0) 1

(cid:15)a+1

(cid:1) .

Proof. (a) We ﬁrst deﬁne an auxiliary sequence {uk} such that

where u0 ∈ Rn. We may then express the last term on the right in (6) as follows.

uk+1 := uk − γ ¯wk+ 1

2

,

2γ ¯wT

k+ 1
2

(y − xk+ 1

2

) = 2γ ¯wT

k+ 1
2

(y − uk)

+ 2γ ¯wT

k+ 1
2

(uk − xk+ 1

2

)

= (cid:107)uk − y(cid:107)2 − (cid:107)uk+1 − y(cid:107)2 + γ2(cid:107) ¯wk+ 1

2

(cid:107)2

+ 2γ ¯wT

k+ 1
2

(uk − xk+ 1

2

).

Invoking Lemma 5 and summing over k, we have

K−1
(cid:88)

k=0

2γzT(xk+ 1

2

− y) ≤ (cid:107)x0 − y(cid:107)2 + 2γ2

K−1
(cid:88)

k=0

(cid:107) ¯wk − ¯wk+ 1

2

(cid:107)2

+ 2γ

K−1
(cid:88)

k=0

¯wT

k+ 1
2

(y − xk+ 1

2

).

Dividing (9) by K, we obtain the following.

K−1
(cid:88)

1
K

2γzT(xk+ 1

2

− y) ≤ 1

K (cid:107)x0 − y(cid:107)2+

k=0
2γ2 (cid:80)K−1
k=0 (cid:107) ¯wk− ¯wk+ 1
2

K

(cid:107)2

+

Using (8) in (10) and invoking (7), it follows that

(cid:80)K−1

k=0 2γ ¯wT

k+ 1
2
K

(y−xk+ 1
2

)

.

(8)

(9)

(10)

2K (cid:107)x0 − y(cid:107)2

γzT(¯xK − y) ≤ 1
2γ2 (cid:80)K−1

k=0 (cid:107) ¯wk− ¯wk+ 1
2

2K

(cid:107)2

+

(cid:80)K−1

k=0 2γ ¯wT

(y−xk+ 1
2

)

k+ 1
2
2K
(cid:107)2

≤ 1

2K (cid:107)x0 − y(cid:107)2 +
(cid:107)u0−y(cid:107)2+(cid:80)K−1

2γ2 (cid:80)K−1

k=0 (cid:107) ¯wk− ¯wk+ 1
2

2K
(cid:107)2+2γ ¯wT

k=0 (γ2(cid:107) ¯wk+ 1
2
2K

k+ 1
2

(uk−xk+ 1
2

))

.

+

+

Taking supremum over z ∈ T (y) and y ∈ dom(T ) and leveraging the compactness of dom(T ), we obtain the following

inequality.

γ

sup
y∈dom(T )

sup
z∈T (y)

zT(¯xK − y)

γ2 (cid:80)K−1

k=0 (2(cid:107) ¯wk− ¯wk+ 1
2

2K

+

(cid:107)2+(cid:107) ¯wk+ 1

2

(cid:107)2)

≤ 2D2

T +(cid:107)x0(cid:107)2+(cid:107)u0(cid:107)2

K
k=0 ¯wT

γ (cid:80)K−1

+

(uk−xk+ 1
2

)

.

k+ 1
2
K

By invoking the deﬁnition of G(x) and letting D (cid:44) 2D2

T + (cid:107)x0(cid:107)2 + (cid:107)u0(cid:107)2, we obtain the following relation.
γ2 (cid:80)K−1

(cid:107)2)

k=0 (2(cid:107) ¯wk− ¯wk+ 1
2

(cid:107)2+(cid:107) ¯wk+ 1

2

γG(¯xK) ≤ D2

K +
γ (cid:80)K−1

k=0 ¯wT

+

2K

(uk−xk+ 1
2

)

.

k+ 1
2
K

14

(11)

Before proceeding, we establish bounds for xk and xk+ 1
(cid:107)xk − x∗(cid:107) is bounded. We denote this bound by (cid:107)xk − x∗(cid:107) ≤ D∗. By deﬁnition of xk+ 1

2

. From Proposition 1, we know {xk} converges to x∗ which indicates

, it follow that

2

− x∗(cid:107) = (cid:13)

(cid:107)xk+ 1
−(I + γB)−1(x∗ − γA(x∗))(cid:13)
(cid:13)

2

(cid:13)(I + γB)−1(xk − γA(xk))

≤ (cid:107)(xk − γA(xk)) − (x∗ − γA(x∗))(cid:107)

≤ (1 + γL)(cid:107)xk − x∗(cid:107) ≤ (1 + γL)D∗,

where the ﬁrst inequality follows from that (I + γB)−1 is a non-expansive operator. Taking expectations on both sides of (11),

leads to the following inequality.

γ2 (cid:80)K−1

k=0 2E[(cid:107) ¯wk− ¯wk+ 1

(cid:107)2]+2E[(cid:107) ¯wk+ 1

2

(cid:107)2]

2

2K

E[γG(¯xK)] ≤ D2
γ (cid:80)K−1
E[ ¯wT
k=0

K +

+

(uk−xk+ 1
2

)]

k+ 1
2
K

2D2+γ2 (cid:80)K−1
k=0

≤

1 (4(cid:107)xk(cid:107)2+6(cid:107)xk+ 1
ν2

(cid:107)2)+10ν2
2

2

Nk

2K

ν2
1 ((8+12(1+γL)2)D2
Nk

∗+20(cid:107)x∗(cid:107)2)+10ν2

2

2D2+γ2 (cid:80)K−1
k=0

≤
≤ 2D2+γ2M (ν2

2K

2K

1 ((8+12(1+γL)2)D2

∗+20(cid:107)x∗(cid:107)2)+10ν2
2 )

= (cid:98)C
K ,

(12)

by deﬁning (cid:98)C (cid:44) (2D2 +
γ2M (ν2

1 ((8 + 12(1 + γL)2)D2

∗ + 20(cid:107)x∗(cid:107)2) + 10ν2

2 ))/2. It follows that E[G(¯xK)] ≤ (cid:98)C/(γK) = O(1/K).

(b) For (cid:15) sufﬁciently small and when (cid:101)C is an appropriate constant, the result follows.

K
(cid:88)

k=0

(cid:100)( (cid:98)C/(cid:15))(cid:101)
(cid:88)

(cid:100)( (cid:98)C/(cid:15))(cid:101)
(cid:88)

ka ≤

Nk ≤

Nk ≤

(cid:90) ( (cid:98)C/(cid:15))+1

xadx

k=0
≤ (( (cid:98)C/(cid:15))+1)a+1
a+1

k=0
(cid:16)

≤

(cid:101)C
(cid:15)a+1

k=0

(cid:17)

.

γ(xk)] ≤ (cid:15)} where ﬁniteness of K(cid:15) can be shown a ﬁnite number, allowing for showing that E[r2

Comment. A rate statement for the last iterate can also be derived as well as shown in [71], [72]. Let K(cid:15) (cid:44) inf{k ≥ 1 :
(cid:17)
E[r2
.
Therefore for K ≥ K(cid:15) iterations, we obtain a rate E[r2
unclear regarding the number of steps required to satisfy E[r2

γ(xK)]. However, this avenue produces a local rate since we remain

γ(xK(cid:15) )] ≤ O

(cid:16) 1
K(cid:15)

γ(xK)] ≤ (cid:15).

15

(13)

(14)

C. Convergence analysis under strongly monotone A

In this subsection, we conduct an analysis under a strong monotonicity requirement.

Assumption 4. The mapping A is σ-strongly monotone, i.e., (A(x) − A(y))T(x − y) ≥ σ(cid:107)x − y(cid:107)2,

∀x, y ∈ Rn.

The following lemma is essential to our rate of convergence analysis.

Lemma 6. Let Assumptions 1 and 4 hold. Then the following holds for every k.

(cid:107)xk+1 − x∗(cid:107)2 ≤ (1 − σγ + γ2)(cid:107)xk − x∗(cid:107)2

− (1 − 2γ2(L2 + 1

+ (4γ2 + 2)(cid:107) ¯wk+ 1

2

2 ) − 2σγ)(cid:107)xk − xk+ 1
(cid:107)2 + 4γ2(cid:107) ¯wk(cid:107)2.

2

(cid:107)2

Proof. According to Assumption 4, we have

Using (14) in (5), we deduce

−2γ(uk+ 1

2

+ vk+ 1

2

)T(xk+ 1

2

− x∗) ≤ −2γσ(cid:107)xk+ 1

2

− x∗(cid:107)2

≤ 2γσ(cid:107)xk+ 1

2

− xk(cid:107)2 − γσ(cid:107)xk − x∗(cid:107)2.

(cid:107)xk+1 − x∗(cid:107)2 ≤ (cid:107)xk − x∗(cid:107)2 − (1 − 2γ2L2)(cid:107)xk − xk+ 1

2

(cid:107)2

− 2γσ(cid:107)xk+ 1

2

− x∗(cid:107)2 + 2γ2(cid:107) ¯wk+ 1

2

− ¯wk(cid:107)2

− 2γ ¯wT

k+ 1
2

(xk+ 1

2

− x∗)

≤ (cid:107)xk − x∗(cid:107)2 − (1 − 2γ2L2)(cid:107)xk − xk+ 1

2

(cid:107)2

+ 2γσ(cid:107)xk+ 1

2

− xk(cid:107)2 − γσ(cid:107)xk − x∗(cid:107)2 + 2γ2(cid:107) ¯wk+ 1

2

− ¯wk(cid:107)2

− 2γ ¯wT

k+ 1
2

(xk+ 1

2

− x∗)

≤ (1 − σγ)(cid:107)xk − x∗(cid:107)2 − (1 − 2γ2L2 − 2σγ)(cid:107)xk − xk+ 1

2

+ 2γ2(cid:107) ¯wk+ 1

2

− ¯wk(cid:107)2 − 2γ ¯wT

k+ 1
2

(xk+ 1

2

− x∗)

≤ (1 − σγ)(cid:107)xk − x∗(cid:107)2 − (1 − 2γ2L2 − 2σγ)(cid:107)xk − xk+ 1

2

+ 4γ2(cid:107) ¯wk+ 1

2

(cid:107)2 + 4γ2(cid:107) ¯wk(cid:107)2 − 2γ ¯wT

k+ 1
2

(xk+ 1

2

− x∗)

= (1 − σγ)(cid:107)xk − x∗(cid:107)2 − (1 − 2γ2L2 − 2σγ)(cid:107)xk − xk+ 1

2

+ 4γ2(cid:107) ¯wk+ 1

2

(cid:107)2 + 4γ2(cid:107) ¯wk(cid:107)2 − 2γ ¯wT

k+ 1
2

(xk+ 1

2

− xk)

(cid:107)2

(cid:107)2

(cid:107)2

− 2γ ¯wT

k+ 1
2

(xk − x∗)

≤ (1 − σγ + γ2)(cid:107)xk − x∗(cid:107)2 + (4γ2 + 2)(cid:107) ¯wk+ 1

2

(cid:107)2

− (1 − 2γ2(L2 + 1

2 ) − 2σγ)(cid:107)xk − xk+ 1

2

(cid:107)2 + 4γ2(cid:107) ¯wk(cid:107)2.

Theorem 2 (a.s. convergence without unbiasedness). Let Assumptions 1, 2(ii) and 4 hold. Consider a sequence {xk}
generated by (vr-SMBFS). Suppose N0 ≥ 2(24γ2+8)ν2
(cid:80)
k

4 , 1
2 . Then {xk} converges to x∗ in an a.s. sense.

, {Nk} is a non-decreasing sequence, and

< ∞, ˜L2 = L2 + 1

, γ < min

20σ ,

(cid:110) σ

7
4 ˜L

(cid:111)

σγ

√

1

1
Nk

Proof. By taking conditional expectations on both sides of (13), we obtain the following relation by invoking Assumption 2(ii)
and deﬁning ˜L2 = L2 + 1
2 .

E[(cid:107)xk+1 − x∗(cid:107)2 | Fk] ≤ (1 − σγ + γ2)(cid:107)xk − x∗(cid:107)2

16

− (1 − 2γ2 ˜L2 − 2σγ)(cid:107)xk − xk+ 1
+ (4γ2 + 2)E[(cid:107) ¯wk+ 1

2

2

(cid:107)2

(cid:107)2 | Fk] + 4γ2E[(cid:107) ¯wk(cid:107)2 | Fk]

≤ (1 − σγ + γ2)(cid:107)xk − x∗(cid:107)2

− (1 − 2γ2 ˜L2 − 2σγ)(cid:107)xk − xk+ 1
+ (4γ2 + 2)E[E[(cid:107) ¯wk+ 1

(cid:107)2 | Fk+ 1

2

2

2

≤ (1 − σγ + γ2)(cid:107)xk − x∗(cid:107)2

− (1 − 2γ2 ˜L2 − 2σγ)(cid:107)xk − xk+ 1
+ 4γ2(ν2

(4γ2+2)(ν2

(cid:107)2+ν2
2 )

+

1 (cid:107)xk+ 1
2
Nk

2

1 (cid:107)xk(cid:107)2+ν2
2 )

.

Nk

(cid:107)2 + 4γ2E[(cid:107) ¯wk(cid:107)2 | Fk]

] | Fk]

(cid:107)2

We now derive bounds on the last two terms, leading to the following inequality.

E[(cid:107)xk+1 − x∗(cid:107)2 | Fk] ≤ (1 − σγ + γ2)(cid:107)xk − x∗(cid:107)2

(4γ2+2)(ν2

− (1 − 2γ2 ˜L2 − 2σγ)(cid:107)xk − xk+ 1
+ 4γ2(ν2

1 (cid:107)xk+ 1
2
Nk
≤ (1 − σγ + γ2)(cid:107)xk − x∗(cid:107)2

(cid:107)2+ν2
2 )

+

2

(cid:107)2

Nk

1 (cid:107)xk(cid:107)2+ν2
2 )

(4γ2+2)(2ν2

− (1 − 2γ2 ˜L2 − 2σγ)(cid:107)xk − xk+ 1
1 (cid:107)xk+ 1
2
Nk
≤ (1 − σγ + γ2)(cid:107)xk − x∗(cid:107)2

−xk(cid:107)2)

+

2

(cid:107)2

+ (12γ2+4)ν2

1 (cid:107)xk(cid:107)2+(12γ2+4)ν2
2 )

Nk

− (1 − 2γ2 ˜L2 − 2σγ)(cid:107)xk − xk+ 1

2

(cid:107)2

(8γ2+4)(ν2

+
+ (24γ2+8)ν2

−xk(cid:107)2)

1 (cid:107)xk+ 1
2
Nk
1 ((cid:107)xk−x∗(cid:107)2+(cid:107)x∗(cid:107)2)+(12γ2+4)ν2
2 )

Nk

≤

−

≤

−

(cid:16)

(cid:16)

(cid:16)

1 − σγ + γ2 + (24γ2+8)ν2

1

N0

(cid:17)

(cid:107)xk − x∗(cid:107)2

1 − 2γ2 ˜L2 − 2σγ − (8γ2+4)ν2
(cid:16)

N0
(cid:17)
1 − σγ + γ2 + (24γ2+8)ν2

1

1

N0

(cid:107)xk − x∗(cid:107)2

(cid:17)

(cid:107)xk − xk+ 1

2

(cid:107)2 + δk

(cid:17)

(cid:107)xk − xk+ 1

2

(cid:107)2 + δk

1 − 2γ2 ˜L2 − 2σγ − (24γ2+8)ν2
2 σγ + γ2(cid:1) (cid:107)xk − x∗(cid:107)2
2 σγ

1 − 2γ2 ˜L2 − 5

(cid:107)xk − xk+ 1

≤ (cid:0)1 − 1
(cid:16)

N0

(cid:17)

−

1

2

(cid:107)2 + δk,

(15)

where δk (cid:44) (24γ2+8)ν2

1 (cid:107)x∗(cid:107)2+(12γ2+4)ν2

2

Nk

and the ﬁnal inequality follows from N0 ≥ 2(24γ2+8)ν2
2 − γ)(cid:1) γ< σ
σ> (1 − 1 + γ2) > 0

4< (cid:0)1 − γσ

(cid:1) < 1

σγ

4

1

(cid:0)1 − 1

(cid:0)1 − 1

2 σγ + γ2(cid:1) = (cid:0)1 − γ( σ
2 σγ + γ2(cid:1) γ< 2
1 − 2γ2 ˜L2 − 5

(cid:17) γ< 1

2 σγ

(cid:16)

−

20σ< −

8 − 2γ2 ˜L2(cid:17) γ2< 7
(cid:16) 7

16 ˜L2
< 0.

. We observe that

17

In other words, if γ < min

(cid:110) σ

4 , 1

20σ ,

√

7
4 ˜L

(cid:111)

, (15) can be further bounded by (1 − ak)(cid:107)xk − x∗(cid:107)2 + δk where ak = γσ

4 for all k

and ak, δk satisfy Lemma 3. Consequently, (cid:107)xk − x∗(cid:107)2 → 0 in an a.s. sense as k → ∞.

Next we provide rate and complexity statements involving (vr-SMFBS) under geometrically increasing Nk.

Proposition 2 (Linear convergence). Let Assumptions 1, 2(ii) and 4 hold. Consider a sequence {xk} generated by (vr-
(cid:111)
SMBFS). Suppose N0 ≥ 2(24γ2+8)ν2
2 , (cid:107)x0 − x∗(cid:107) ≤ D0 and Nk (cid:44)
N0(cid:98)ρ−(k+1)(cid:99) for all k > 0. Then the following hold.
(a) Suppose q (cid:44) (1 − σγ
(b) Suppose xK+1 is such that E[(cid:107)xK+1 − x∗(cid:107)2] ≤ (cid:15). Then the oracle complexity is (cid:80)K

4 ) < 1. Then E[(cid:107)xk − x∗(cid:107)2] ≤ ˜D ˜ρk where ˜D > 0 and ˜ρ = max{q, ρ} if q (cid:54)= ρ and ˜ρ ∈ (q, 1) if q = ρ.

< ∞, ˜L2 = L2 + 1

, γ < min

, (cid:80)
k

4 , 1

k=0 Nk ≤ O (cid:0) 1

20σ ,

(cid:110) σ

7
4 ˜L

1
Nk

(cid:1) .

σγ

√

1

(cid:15)

Proof. (a) By taking unconditional expectations on both sides of (15), we obtain

E[(cid:107)xk+1 − x∗(cid:107)2] ≤ qE[(cid:107)xk − x∗(cid:107)2] + D
Nk

,

where D (cid:44) (24γ2 + 8)ν2

1 (cid:107)x∗(cid:107)2 + (12γ2 + 4)ν2

2 and q (cid:44) (1 − σγ
4 ). Recall that Nk can be bounded as seen next.
2 ρ−(k+1)(cid:109)
(cid:108) 1

2 ρ−(k+1).

≥ N0

Nk = N0(cid:98)ρ−(k+1)(cid:99) ≥ N0

(16)

(17)

We now consider three cases.
(i): q < ρ < 1. Using (17) in (16) and deﬁning ¯D (cid:44) 2D
N0

and ˜D (cid:44) D0 + ¯D, we obtain

E[(cid:107)xk+1 − x∗(cid:107)2] ≤ qE[(cid:107)xk − x∗(cid:107)2] + D
Nk

≤ qE[(cid:107)xk − x∗(cid:107)2] + 2D
N0

ρk+1

≤ qk+1(cid:107)x0 − x∗(cid:107) + ¯D

k+1
(cid:88)

j=1

qk+1−jρj

≤ D0qk+1 + ¯Dρk+1

k+1
(cid:88)

ρ )k+1−j ≤ ˜Dρk+1.
( q

j=1

(ii): ρ < q < 1. Akin to (i) and deﬁning ˜D apprioriately, E[(cid:107)xk+1 − x∗(cid:107)2] ≤ ˜Dqk+1.

(iii): ρ = q < 1. If ˜ρ ∈ (q, 1) and (cid:98)D >

1

ln( ˜ρ/q)e , proceeding similarly we obtain

E[(cid:107)xk+1 − x∗(cid:107)2] ≤ qk+1E[(cid:107)x0 − x∗(cid:107)2] + ¯D

k+1
(cid:88)

j=1

qk+1

≤ D0qk+1 + ¯D

k+1
(cid:88)

j=1

qk+1 = D0qk+1 + ¯D(k + 1)qk+1

[73, Lemma 4]
≤

˜D ˜ρk+1, where ˜D (cid:44) (D0 + ¯D · (cid:98)D).

Thus, {xk} converges linearly in an expected-value sense.

(b) Case (i): If q < ρ < 1. From (a), it follows that

E[(cid:107)xK+1 − x∗(cid:107)2] ≤ ˜DρK+1 ≤ (cid:15) =⇒ K ≥ log1/ρ( ˜D/(cid:15)) − 1.

If K = (cid:100)log1/ρ( ˜D/(cid:15))(cid:101) − 1, then (vr-SMFBS) requires (cid:80)K
we have

k=0 Nk evaluations. Since Nk = N0(cid:98)ρ−(k+1)(cid:99) ≤ N0ρ−(k+1), then

18

(cid:100)log1/ρ( ˜D/(cid:15))(cid:101)−1
(cid:88)

N0ρ−(k+1) =

(cid:100)log1/ρ( ˜D/(cid:15))(cid:101)
(cid:88)

N0ρ−t

k=0

t=1

(cid:17)(cid:100)log1/ρ( ˜D/(cid:15))(cid:101)

≤

N0
(cid:17)
ρ2(cid:16) 1
ρ −1

(cid:16) 1
ρ

≤ N0
(cid:16) 1
ρ

(cid:17)
ρ −1

(cid:16) 1
ρ

(cid:17)log1/ρ( ˜D/(cid:15))+1

≤ N0
(1−ρ)

(cid:16) 1
ρ

(cid:17)log1/ρ( ˜D/(cid:15))

≤ N0
(1−ρ)

(cid:17)

(cid:16) ˜D
(cid:15)

.

We omit cases (ii) and (iii) which lead to similar complexities.

Remark. We comment on our ﬁndings next.

(a) Rates and asymptotics. We believe that the ﬁndings ﬁll important gaps in terms of providing rate statements for monotone

inclusions. In particular, the rate statement in monotone regimes relies on utilizing a lesser known gap function while the

variance-reduced schemes achieve deterministic rates of convergence. In addition, the oracle complexities are near-optimal.

(b) Algorithm parameters. Akin to more traditional ﬁrst-order schemes, these schemes rely on utilizing constant steplengths

and leverage problem parameters such as Lipschitz and strong monotonicity constants. We believe that by using diminishing

steplength sequences, we may be able to derive weaker rate statements that do not rely on problem parameters.

(c) Expectation-valued B. We may consider a setting where B is expectation-valued and the resolvent operation is approximated

via stochastic approximation.

IV. NUMERICAL RESULTS

In this section, we apply the proposed schemes on a 2-stage SVI problem described in Section I-A (Example b).

Problem parameters for 2-stage SVI. We generate a set of J i.i.d samples {ξi}J
i=1, where ξi ∼ U [−5, 0]. Suppose hi(ω) = ξi
i + (cid:96)ixi, M ∈ RJ×J is a diagonal matrix with nonnegative elements M =
for i = 1, · · · , J. In addition, ci(xi) = 1
diag(m1, . . . , mJ ) while (cid:96) = [(cid:96)1, . . . , (cid:96)J ]T ∈ RJ where (cid:96)i ∈ U (2, 3). Furthermore, the inverse demand function p is deﬁned

2 mix2

as p(X) = d − rX where d = 1 and r = 1. Thus G(x), as deﬁned in Section 1.1 (b), can be simpliﬁed as G(x) = M x + (cid:96).
In this setting, A(x) = M x + (cid:96) + R(x) + D(cid:15)(x) and B(x) = NX (x), where X (cid:44) R+
D where LB = maxi mi, LR = r(cid:107)I + 11T(cid:107) and L(cid:15)
L = LB + LR + L(cid:15)

J . The Lipschitz of A is given by
(cid:15) . All the schemes are implemented in MATLAB

D = 1

on a PC with 16GB RAM and 6-Core Intel Core i7 processor (2.6GHz).

We describe the three schemes being compared and specify their algorithm parameters. Solution quality is compared by

estimating the residual function res(x) = (cid:107)x − ΠX (x − γA(x))(cid:107).

A. Algorithm speciﬁcations.

(i) (SA): Stochastic approximation scheme. The (SA) scheme utilizes the following update.

xk+1 := ΠX [xk − γkA(xk, ωk)] ,

(SA)

where A(xk) = E[A(xk, ωk)], and γk (cid:44) 1√
k
(ii) (vr-SMFBS): Variance-reduction stochastic modiﬁed forward-backward scheme. We choose a constant γ = 1

4L which
satisﬁes the steplength assumption and we assume Nk = (cid:98)k1.01(cid:99) for merely monotone problems, Nk = (cid:98)1.01k+1(cid:99) for strongly

. x0 is randomly generated in [0, 1]J .

monotone problems.

19

Fig. 1. Trajectories for (SA) and (vr-SMBFS) (up: monotone; down: s-monotone)

B. Performance comparison and insights.

In Fig. 1, we compare both schemes under mere monotonicity and strong monotonicity, respectively and examine sensitivities

to the sample growth rate. Standard SA schemes may struggle when the problem is ill-conditioned and we examine the

performance of the schemes in such regimes and provide the results in for merely monotone and strongly monotone settings

in Tables II and III, respectively.

TABLE II

COMPARISON OF (vr-SMFBS) WITH (SA)

merely monotone, 20000 evaluations

vr-SMFBS

SA

error

time

CI

error

time

CI

1.6e-3

1.9e-3

2.2e-3

5.9e-3

2.6

2.6

2.6

2.6

[1.3e-3,1.8e-3]

5.3e-2

[1.6e-3,2.1e-3]

6.1e-2

[2.0e-3,2.5e-3]

7.6e-2

[5.4e-3,6.2e-3]

9.4e-2

2.7

2.7

2.5

2.6

[5.0e-2,5.7e-2]

[5.8e-2,6.4e-2]

[7.3e-2,7.9e-2]

[9.0e-1,9.7e-1]

L

1e1

1e2

1e3

1e4

Complicated X , merely monotone, 2000 evaluations

1e2

1.9e-3

6.8

[1.6e-3,2.0e-3]

6.0e-2

232

[5.7e-2,6.3e-2]

TABLE III

COMPARISON OF (vr-SMFBS) WITH (SA)

strongly monotone, 20000 evaluations

vr-SMFBS

SA

error

time

CI

error

time

CI

1.5e-5

3.6e-5

5.6e-5

7.4e-5

2.6

2.5

2.5

2.5

[1.2e-5,1.7e-5]

2.9e-2

[3.3e-5,3.9e-5]

4.1e-2

[4.2e-6,4.7e-6]

5.5e-2

[7.1e-5,7.7e-5]

6.0e-2

2.5

2.5

2.4

2.5

[2.7e-2,3.1e-2]

[3.8e-2,4.4e-2]

[5.2e-2,5.7e-2]

[5.7e-2,6.3e-2]

L

1e1

1e2

1e3

1e4

Complicated X , strongly monotone, 2000 evaluations

1e3

5.6e-5

18

[4.2e-6,4.7e-6]

5.5e-2

234

[5.2e-2,5.8e-2]

Key ﬁndings. (vr-SMFBS) trajectories are characterized by signiﬁcantly smaller empirical errors than (SA). There is little

impact on (vr-SMFBS) when varying the sample growth rate. Moreover, (vr-SMFBS) appears to cope better with large Lipchitz

20

constant. Since we utilize the analytical L to set γ, we see smaller steps for large L. To show the efﬁciency of (vr-SMFBS)
with complicated feasible set, we change X (cid:44) {x ∈ R+

i xi ≤ 10} which leads to computationally expensive projection

J | (cid:80)

steps. As seen in the last row of Tables II and III, (vr-SMFBS) takes far less time than (SA).

C. Comparison with SAA schemes

To show the performance of our proposed schemes , we consider the (SAA) scheme used in [74]. Let (ω1i)J

i=1, (ω2i)J

i=1,

. . . , (ωνi)J

i=1 denote independent identically distributed (i.i.d.) samples. Then, with (SAA) we solve the following formulation

of problem:






0 ≤ xi ⊥ c(cid:48)

i(xi) + r · (X + xi) − d + 1
ν

0 ≤ yi(ωli) ⊥ hi(ωli) + λi(ωli) ≥ 0

ν
(cid:88)

l=1

λi(ωli) ≥ 0

0 ≤ λi(ωli) ⊥ xi − yi(ωli) + (cid:15)λi(ωli) ≥ 0, ∀l = 1, . . . , ν






∀i = 1, . . . , J.

This problem is cast as a linear complementarity problem (LCP), allowing for utilizing PATH [75] to compute a solution.

We compare (SAA) with (vr-SMFBS) in Table IV. From the results, we observe that although the empirical errors of both

schemes are similar, the (SAA) scheme takes far longer than (vr-SMFBS) when using a large number of samples. In fact,

(vr-SMFBS) scale well with overall number of evaluations.

COMPARISON OF (SAA) WITH (VR-SMFBS) (U: S-MONOTONE, D: MONOTONE)

TABLE IV

ν

1000

2000

4000

10000

20000

ν

1000

2000

4000

10000

20000

SAA

vr-SMFBS

time/s

res

time/s

res

0.7

3.3

6.2

32.7

117.7

4.5e-4

3.5e-4

1.6e-4

3.7e-5

2.8e-5

0.3

0.5

0.6

1.2

2.5

4.8e-4

2.3e-4

1.0e-4

3.4e-5

1.5e-5

SAA

vr-SMFBS

time/s

res

time/s

res

0.7

3.0

5.8

61.8

115.0

5.6e-2

3.4e-2

2.2e-2

7.8e-3

2.5e-3

0.3

0.5

0.6

1.2

2.6

2.7e-2

2.0e-2

1.2e-2

5.3e-3

1.9e-3

V. CONCLUDING REMARKS

Monotone inclusions represent an important class of problems and their stochastic counterpart subsumes a large class of

stochastic optimization and equilibrium problems. Such objects arise in optimization, game-theoretic, and model-predictive con-

trol problems afﬂicted by uncertainty. We propose a variance-reduced splitting framework for resolving such problems when the

map is structured. Under suitable assumptions on the sample-size, we prove that the scheme displays a.s. convergence guarantees

21

and achieves optimal linear and sublinear rates in strongly monotone and monotone regimes while achieving either optimal or

near-optimal sample-complexities. By incorporating state-dependent bounds on noise and weakening unbiasedness requirements

(in strongly monotone settubfs), we develop techniques that can accommodate far more general settings. Preliminary numerics

on a class of two-stage stochastic variational inequality problems suggest that the scheme outperform stochastic approximation

schemes, as well as sample-average approximation approaches.

REFERENCES

[1] S. M. Robinson, Generalized Equations, pp. 346–367. Berlin, Heidelberg: Springer Berlin Heidelberg, 1983.

[2] R. T. Rockafellar, “Monotone operators and the proximal point algorithm,” SIAM Journal on Control and Optimization, vol. 14, no. 5, pp. 877–898,

1976.

[3] R. Glowinski and P. Le Tallec, Augmented Lagrangian and operator-splitting methods in nonlinear mechanics, vol. 9. SIAM, 1989.

[4] J. Douglas and H. H. Rachford, “On the numerical solution of heat conduction problems in two and three space variables,” Transactions of the American

mathematical Society, vol. 82, no. 2, pp. 421–439, 1956.

[5] D. W. Peaceman and H. H. Rachford, Jr, “The numerical solution of parabolic and elliptic differential equations,” Journal of the Society for industrial

and Applied Mathematics, vol. 3, no. 1, pp. 28–41, 1955.

[6] P.-L. Lions and B. Mercier, “Splitting algorithms for the sum of two nonlinear operators,” SIAM Journal on Numerical Analysis, vol. 16, no. 6,

pp. 964–979, 1979.

[7] G. B. Passty, “Ergodic convergence to a zero of the sum of monotone operators in Hilbert space,” Journal of Mathematical Analysis and Applications,

vol. 72, no. 2, pp. 383–390, 1979.

[8] X. Chen, A. Shapiro, and H. Sun, “Convergence analysis of sample average approximation of two-stage stochastic generalized equations,” SIAM Journal

on Optimization, vol. 29, no. 1, pp. 135–161, 2019.

[9] R. J. Aumann, “Integrals of set-valued functions,” Journal of Mathematical Analysis and Applications, vol. 12, no. 1, pp. 1–12, 1965.

[10] G. B. Dantzig, “Linear programming under uncertainty,” in Stochastic programming, pp. 1–11, Springer, 2010.

[11] J. R. Birge and F. Louveaux, Introduction to stochastic programming. Springer Science & Business Media, 2011.

[12] A. Shapiro, D. Dentcheva, and A. Ruszczy´nski, Lectures on stochastic programming: modeling and theory. SIAM, 2014.

[13] H. Jiang and H. Xu, “Stochastic approximation approaches to the stochastic variational inequality problem,” IEEE Transactions on Automatic Control,

vol. 53, no. 6, pp. 1462–1475, 2008.

[14] A. Juditsky, A. Nemirovski, and C. Tauvel, “Solving variational inequalities with stochastic mirror-prox algorithm,” Stochastic Systems, vol. 1, no. 1,

pp. 17–58, 2011.

[15] U. V. Shanbhag, “Stochastic variational inequality problems: Applications, analysis, and algorithms,” in Theory Driven by Inﬂuential Applications,

pp. 71–107, INFORMS, 2013.

[16] U. Ravat and U. V. Shanbhag, “On the existence of solutions to stochastic quasi-variational inequality and complementarity problems,” Mathematical

Programming, vol. 165, no. 1, pp. 291–330, 2017.

[17] U. Ravat and U. V. Shanbhag, “On the characterization of solution sets of smooth and nonsmooth convex stochastic nash games,” SIAM Journal on

Optimization, vol. 21, no. 3, pp. 1168–1199, 2011.

[18] F. Facchinei and J.-S. Pang, Finite-dimensional variational inequalities and complementarity problems. Springer Science & Business Media, 2007.

[19] H. D. Sherali, “A multiple leader Stackelberg model and analysis,” Operations Research, vol. 32, no. 2, pp. 390–404, 1984.

[20] J.-S. Pang and M. Fukushima, “Quasi-variational inequalities, generalized Nash equilibria, and multi-leader-follower games,” Computational Management

Science, vol. 2, no. 1, pp. 21–56, 2005.

[21] C.-L. Su, “Analysis on the forward market equilibrium model,” Operations Research Letters, vol. 35, no. 1, pp. 74–82, 2007.

[22] A. A. Kulkarni and U. V. Shanbhag, “An existence result for hierarchical Stackelberg v/s Stackelberg games,” IEEE Transactions on Automatic Control,

vol. 60, no. 12, pp. 3379–3384, 2015.

[23] M. Herty, S. Steffensen, and A. Th¨unen, “Solving quadratic multi-leader-follower games by smoothing the follower’s best response,” Optimization

Methods and Software, pp. 1–28, 2020.

[24] J. B. Rawlings, D. Q. Mayne, and M. Diehl, Model predictive control: theory, computation, and design, vol. 2. Nob Hill Publishing Madison, WI, 2017.

[25] D. A. Allan, C. N. Bates, M. J. Risbeck, and J. B. Rawlings, “On the inherent robustness of optimal and suboptimal nonlinear MPC,” Systems Control

Lett., vol. 106, pp. 68–78, 2017.

[26] A. Bemporad and M. Morari, “Robust model predictive control: A survey,” in Robustness in identiﬁcation and control, pp. 207–226, Springer, 1999.

22

[27] F. A. Cuzzola, J. C. Geromel, and M. Morari, “An improved approach for constrained robust model predictive control,” Automatica, vol. 38, no. 7,

pp. 1183–1189, 2002.

[28] S. Hojjatinia, C. M. Lagoa, and F. Dabbene, “Identiﬁcation of switched autoregressive exogenous systems from large noisy datasets,” Internat. J. Robust

Nonlinear Control, vol. 30, no. 15, pp. 5777–5801, 2020.

[29] M. Chamanbaz, F. Dabbene, and C. M. Lagoa, “Probabilistically robust AC optimal power ﬂow,” IEEE Trans. Control Netw. Syst., vol. 6, no. 3,

pp. 1135–1147, 2019.

[30] C. Feng, F. Dabbene, and C. M. Lagoa, “A kinship function approach to robust and probabilistic optimization under polynomial uncertainty,” IEEE

Trans. Automat. Control, vol. 56, no. 7, pp. 1509–1523, 2011.

[31] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski, Robust optimization. Princeton university press, 2009.

[32] P. K. Mishra, S. S. Diwale, C. N. Jones, and D. Chatterjee, “Reference tracking stochastic model predictive control over unreliable channels and bounded

control actions,” Automatica J. IFAC, vol. 127, pp. 109512, 10, 2021.

[33] J. Zhang and T. Ohtsuka, “Stochastic model predictive control using simpliﬁed afﬁne disturbance feedback for chance-constrained systems,” IEEE Control

Syst. Lett., vol. 5, no. 5, pp. 1633–1638, 2021.

[34] Y. Tan, Q. Cao, L. Li, T. Hu, and M. Su, “A chance-constrained stochastic model predictive control problem with disturbance feedback,” J. Ind. Manag.

Optim., vol. 17, no. 1, pp. 67–79, 2021.

[35] L. Hewing and M. N. Zeilinger, “Scenario-based probabilistic reachable sets for recursively feasible stochastic model predictive control,” IEEE Control

Syst. Lett., vol. 4, no. 2, pp. 450–455, 2020.

[36] A. Groß, C. Wittwer, and M. Diehl, “Stochastic model predictive control of photovoltaic battery systems using a probabilistic forecast model,” Eur. J.

Control, vol. 56, pp. 254–264, 2020.

[37] L. Hewing, K. P. Wabersich, and M. N. Zeilinger, “Recursively feasible stochastic model predictive control using indirect feedback,” Automatica J. IFAC,

vol. 119, pp. 109095, 7, 2020.

[38] A. Mesbah, “Stochastic model predictive control: An overview and perspectives for future research,” IEEE Control Systems Magazine, vol. 36, no. 6,

pp. 30–44, 2016.

[39] E. Camacho and C. Alba, Model Predictive Control. Advanced Textbooks in Control and Signal Processing, Springer London, 2013.

[40] I. E. Bardakci, A. Jalilzadeh, C. Lagoa, and U. V. Shanbhag, “Probability maximization via minkowski functionals: Convex representations and tractable

resolution,” arXiv preprint arXiv:1802.09682, 2018.

[41] L. Rosasco, S. Villa, and B. C. V˜u, “Stochastic forward–backward splitting for monotone inclusions,” Journal of Optimization Theory and Applications,

vol. 169, no. 2, pp. 388–406, 2016.

[42] P. L. Combettes and J.-C. Pesquet, “Stochastic approximations and perturbations in forward-backward splitting for monotone operators,” Pure and Applied

Functional Analysis, vol. 1, no. 1, pp. 13–37, 2016.

[43] A. Ruszczy´nski, “Decomposition methods in stochastic programming,” Mathematical Programming, vol. 79, no. 1-3, pp. 333–353, 1997.

[44] M. Schmidt, N. L. Roux, and F. R. Bach, “Convergence rates of inexact proximal-gradient methods for convex optimization,” in Advances in neural

information processing systems, pp. 1458–1466, 2011.

[45] S. Ghadimi, G. Lan, and H. Zhang, “Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization,” Mathematical

Programming, vol. 155, no. 1-2, Ser. A, pp. 267–305, 2016.

[46] A. Jalilzadeh, U. V. Shanbhag, J. H. Blanchet, and P. W. Glynn, “Smoothed variable sample-size accelerated proximal methods for nonsmooth stochastic

convex programs,” arXiv preprint arXiv:1803.00718, 2018.

[47] A. Jofr´e and P. Thompson, “On variance reduction for stochastic smooth convex optimization with multiplicative noise,” Mathematical Programming,

vol. 174, no. 1-2, pp. 253–292, 2019.

[48] L. Rosasco, S. Villa, and B. C. V˜u, “Convergence of stochastic proximal gradient algorithm,” Applied Mathematics & Optimization, pp. 1–27, 2019.

[49] S. Cui and U. V. Shanbhag, “On the analysis of reﬂected gradient and splitting methods for monotone stochastic variational inequality problems,” in

55th IEEE Conference on Decision and Control, CDC 2016, Las Vegas, NV, USA, December 12-14, 2016, pp. 4510–4515, IEEE, 2016.

[50] V. G. Yaji and S. Bhatnagar, “Analysis of stochastic approximation schemes with set-valued maps in the absence of a stability guarantee and their

stabilization,” IEEE Trans. Autom. Control., vol. 65, no. 3, pp. 1100–1115, 2020.

[51] V. G. Yaji and S. Bhatnagar, “Stochastic recursive inclusions in two timescales with nonadditive iterate-dependent markov noise,” Math. Oper. Res.,

vol. 45, no. 4, pp. 1405–1444, 2020.

[52] V. Cevher, B. C. V˜u, and A. Yurtsever, “Stochastic forward Douglas-Rachford splitting method for monotone inclusions,” in Large-Scale and Distributed

Optimization, pp. 149–179, Springer, 2018.

[53] X. Chen, R. J.-B. Wets, and Y. Zhang, “Stochastic variational inequalities: residual minimization smoothing sample average approximations,” SIAM

Journal on Optimization, vol. 22, no. 2, pp. 649–673, 2012.

23

[54] A. Shapiro and H. Xu, “Stochastic mathematical programs with equilibrium constraints, modelling and sample average approximation,” Optimization,

vol. 57, no. 3, pp. 395–418, 2008.

[55] M. P. Friedlander and M. Schmidt, “Hybrid deterministic-stochastic methods for data ﬁtting,” SIAM Journal on Scientiﬁc Computing, vol. 34, no. 3,

pp. A1380–A1405, 2012.

[56] R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu, “Sample size selection in optimization methods for machine learning,” Mathematical Programming,

vol. 134, no. 1, pp. 127–155, 2012.

[57] R. Pasupathy, P. Glynn, S. Ghosh, and F. S. Hashemi, “On sampling rates in simulation-based recursions,” SIAM Journal on Optimization, vol. 28, no. 1,

pp. 45–73, 2018.

[58] S. Ghadimi, G. Lan, and H. Zhang, “Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization,” Mathematical

Programming, vol. 155, no. 1-2, pp. 267–305, 2016.

[59] R. Bollapragada, D. Mudigere, J. Nocedal, H. M. Shi, and P. T. P. Tang, “A progressive batching L-BFGS method for machine learning,” in Proceedings of

the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018 (J. G. Dy and A. Krause,

eds.), vol. 80 of Proceedings of Machine Learning Research, pp. 619–628, PMLR, 2018.

[60] A. Jalilzadeh, A. Nedic, U. V. Shanbhag, and F. Youseﬁan, “A variable sample-size stochastic quasi-newton method for smooth and nonsmooth stochastic

convex optimization,” Mathematics of Operations Research (to appear), https://arxiv.org/abs/1804.05368, 2020.

[61] R. Johnson and T. Zhang, “Accelerating stochastic gradient descent using predictive variance reduction,” in Advances in Neural Information Processing

Systems (C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, eds.), vol. 26, pp. 315–323, Curran Associates, Inc., 2013.

[62] A. Defazio, F. Bach, and S. Lacoste-Julien, “SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives,”

in Advances in Neural Information Processing Systems (Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, eds.), vol. 27,

pp. 1646–1654, Curran Associates, Inc., 2014.

[63] S.-P. Han and G. Lou, “A parallel algorithm for a class of convex programs,” SIAM Journal on Control and Optimization, vol. 26, no. 2, pp. 345–355,

1988.

[64] P. Tseng, “A modiﬁed forward-backward splitting method for maximal monotone mappings,” SIAM Journal on Control and Optimization, vol. 38, no. 2,

pp. 431–446, 2000.

[65] B. T. Polyak, Introduction to optimization. Optimization Software New York, 1987.

[66] T. Larsson and M. Patriksson, “A class of gap functions for variational inequalities,” Mathematical Programming, vol. 64, no. 1, Ser. A, pp. 53–79,

1994.

[67] J. M. Borwein and J. Dutta, “Maximal monotone inclusions and Fitzpatrick functions,” Journal of Optimization Theory and Applications, vol. 171, no. 3,

pp. 757–784, 2016.

[68] J. M. Borwein and J. D. Vanderwerff, Convex functions: constructions, characterizations and counterexamples, vol. 109. Cambridge University Press

Cambridge, 2010.

[69] S. Fitzpatrick, “Representing monotone operators by convex functions,” in Workshop/Miniconference on Functional Analysis and Optimization, pp. 59–65,

Centre for Mathematics and its Applications, Mathematical Sciences Institute, The Australian National University, 1988.

[70] Y. Nesterov, “Dual extrapolation and its applications to solving variational inequalities and related problems,” Mathematical Programming, vol. 109,

no. 2, pp. 319–344, 2007.

[71] A. Iusem, A. Jofr´e, R. I. Oliveira, and P. Thompson, “Extragradient method with variance reduction for stochastic variational inequalities,” SIAM Journal

on Optimization, vol. 27, no. 2, pp. 686–724, 2017.

[72] R. Bot, P. Mertikopoulos, M. Staudigl, and P. Vuong, “Mini-batch forward-backward-forward methods for solving stochastic variational inequalities,”

Stochastic Systems, 2020.

[73] H. Ahmadi, On the analysis of data-driven and distributed algorithms for convex optimization problems. The Pennsylvania State University, 2016.

[74] J. Jiang, Y. Shi, X. Wang, and X. Chen, “Regularized two-stage stochastic variational inequalities for cournot-nash equilibrium under uncertainty,” arXiv

preprint arXiv:1907.07317, 2019.

[75] S. P. Dirkse and M. C. Ferris, “The path solver: a nommonotone stabilization scheme for mixed complementarity problems,” Optimization Methods and

Software, vol. 5, no. 2, pp. 123–156, 1995.

