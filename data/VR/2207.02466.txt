GLENet: Boosting 3D Object Detectors with Generative Label
Uncertainty Estimation

Yifan Zhang ¬∑ Qƒ≥ian Zhang ¬∑ Zhiyu Zhu ¬∑ Junhui Hou ¬∑ Yixuan Yuan

2
2
0
2

l
u
J

0
2

]

V
C
.
s
c
[

2
v
6
6
4
2
0
.
7
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract The inherent ambiguity in ground-truth annota-
tions of 3D bounding boxes caused by occlusions, signal
missing, or manual annotation errors can confuse deep 3D
object detectors during training, thus deteriorating the de-
tection accuracy. However, existing methods overlook such
issues to some extent and treat the labels as deterministic. In
this paper, we formulate the label uncertainty problem as the
diversity of potentially plausible bounding boxes of objects,
then propose GLENet, a generative framework adapted from
conditional variational autoencoders, to model the one-to-
many relationship between a typical 3D object and its po-
tential ground-truth bounding boxes with latent variables.
The label uncertainty generated by GLENet is a plug-and-
play module and can be conveniently integrated into existing
deep 3D detectors to build probabilistic detectors and super-
vise the learning of the localization uncertainty. Besides, we
propose an uncertainty-aware quality estimator architecture
in probabilistic detectors to guide the training of IoU-branch
with predicted localization uncertainty. We incorporate the
proposed methods into various popular base 3D detectors and
demonstrate signiÔ¨Åcant and consistent performance gains on
both KITTI and Waymo benchmark datasets. Especially, the
proposed GLENet-VR outperforms all published LiDAR-
based approaches by a large margin and ranks 1ùë†ùë° among
single-modal methods on the challenging KITTI test set. We

Yifan Zhang, Qƒ≥ian Zhang, Zhiyu Zhu, and Junhui Hou
Department of Computer Science, City University of Hong Kong.
E-mail:
c}@my.cityu.edu.hk; jh.hou@cityu.edu.hk;

{yzhang3362-c,

qƒ≥izhang3-c,

zhiyuzhu2-

Yixuan Yuan
Department of Electrical Engineering, City University of Hong Kong.
E-mail: yxyuan.ee@cityu.edu.hk

This project was partly supported by the Hong Kong Research Grants
Council under Grants 11202320 and 11218121, and partly by the
Natural Science Foundation of China under Grant 61871342.

will make the source code and pre-trained models publicly
available.

Keywords 3D object detection ¬∑ label uncertainty ¬∑
conditional variational autoencoders ¬∑ probabilistic object
detection ¬∑ 3D point cloud

1 Introduction

As one of the most practical application scenarios of com-
puter vision, 3D object detection has been attracting much
academic and industrial attention in the current deep learning
era with the rise of autonomous driving and the emergence
of large-scale annotated datasets (e.g., KITTI (Geiger et al.,
2012), and Waymo (Sun et al., 2020)).

In the current community, despite the proliferation of
various deep learning-based 3D detection pipelines, it is
observed that mainstream 3D object detectors are typically
designed as deterministic models, without considering the
critical issue of the ambiguity of annotated ground-truth
labels. However, diÔ¨Äerent aspects of ambiguity/inaccuracy
inevitably exist in the ground-truth annotations of object-
level bounding boxes, which may signiÔ¨Åcantly inÔ¨Çuence the
overall learning process of such deterministic detectors. For
example, in the data collection phase, raw point clouds can
be highly incomplete due to the intrinsic properties of Li-
DAR sensors as well as uncontrollable environmental occlu-
sion. Moreover, in the data labeling phase, ambiguity nat-
urally occurs when diÔ¨Äerent human annotators subjectively
estimate object shapes and locations from 2D images and
partial 3D points. To facilitate intuitive understandings, we
provide typical examples in Fig. 1, from which we can ob-
serve that an incomplete LiDAR observation can correspond
to multiple potentially plausible labels and objects with sim-

 
 
 
 
 
 
2

Yifan Zhang et al.

Fig. 1: (a) Given an object with an incomplete LiDAR observation, there may exist multiple potentially plausible ground-truth
bounding boxes with varying sizes and shapes. (b) Ambiguity and inaccuracy can be inevitable in the labeling process when
annotations are derived from 2D images and partial points. In the given cases, similar point clouds of the car category with
only the rear part can be annotated with diÔ¨Äerent ground-truth boxes of varying lengths.

Fig. 2: Illustration of two diÔ¨Äerent learning paradigms of
probabilistic object detectors. (a) Methods that adopt proba-
bilistic modeling in the detection head but essentially still ig-
nore the issue of ambiguity in ground-truth bounding boxes.
(b) Methods that explicitly estimate ground-truth bounding
box distributions to be used as more reliable supervision
signals.

ilar LiDAR observation can be annotated with signiÔ¨Åcantly
varying bounding boxes.

Motivated by the afore-mentioned phenomena, there also
exists another family of probabilistic detectors that explicitly
consider the potential inÔ¨Çuence of label ambiguity. Conclu-
sively, these methods can be categorized into two paradigms,
as illustrated in Fig. 2. The Ô¨Årst paradigm of learning frame-
works (He et al., 2019; Meyer et al., 2019; Feng et al., 2018,
2019) tend to output the probabilistic distribution of bound-
ing boxes, instead of directly regressing deÔ¨Ånite box coor-
dinates in a deterministic fashion. For example, under the
pre-assumption of Gaussian distribution, the detection head
accordingly predicts mean and variance of the distribution.
To supervise such probabilistic models, these works simply
treat ground-truth bounding boxes as the Dirac delta distribu-
tion, after which KL divergence is applied between the esti-
mated distributions and ground-truths. Obviously, the major

limitation of these methods lies in that they fail to essentially
address the problem of label ambiguity, since the ground-
truth bounding boxes are still considered as deterministic
with zero uncertainty (i.e., modeled as a Dirac delta func-
tion). To this end, the second paradigm of learning frame-
works attempts to quantify label uncertainty derived from
some simple heuristics (Meyer and Thakurdesai (2020)) or
Bayes (Wang et al. (2020)), such that the detectors can be
supervised under more reliable bounding box distributions.
However, it is not surprising that these approaches still can-
not produce satisfactory label uncertainty estimation results
due to insuÔ¨Écient modeling capacity. In general, this line
of works is still at its initial stage with very limited number
of studies, despite its greater potential in generating higher-
quality label uncertainty estimation in a data-driven manner.

Architecturally, this work follows the second type of de-
sign philosophy, where we particularly customize a powerful
deep learning-based label uncertainty quantiÔ¨Åcation frame-
work to enhance the reliability of the estimated ground-truth
bounding box distributions. Technically, we formulate the la-
bel uncertainty problem as the diversity of potentially plau-
sible bounding boxes and explicitly model the one-to-many
relationship between a typical 3D object and its potentially
plausible ground-truth boxes in a learning-based framework.
Technically, we propose GLENet, a novel deep generative
network adapted from conditional variational auto-encoders
(CVAE), which introduces a latent variable to capture the
distribution over potentially plausible bounding boxes of
point cloud objects. During inference, we sample latent vari-
ables multiple times to generate diverse bounding boxes (see
Fig. 3), the variance of which is taken as label uncertainty
to guide the learning of localization uncertainty estimation

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

3

Fig. 3: Illustration of multiple potentially plausible bounding boxes from GLENet by sampling latext variables multiple times.
The point cloud, annotated ground-truth boxes and predictions of GLENet are colored in black, red and green, respectively.
GLENet produces diverse predictions for objects represented with sparse point clouds and incomplete outlines, and consistent
bounding boxes for objects with high-quality point clouds. The variance of the multiple predictions by GLENet is used to
estimate the uncertainty of the annotated ground-truth bounding boxes.

in the downstream detection task. Besides, based on the ob-
servation that detection results with low localization uncer-
tainty in probabilistic detectors tend to have accurate actual
localization quality (see Section 4.2), we further propose
uncertainty-aware quality estimator (UAQE), which facili-
tates the training of the IoU-branch with the localization
uncertainty estimation.

To demonstrate our eÔ¨Äectiveness and universality, we
integrate GLENet into several popular 3D object detection
frameworks to build powerful probabilistic detectors. Exper-
iments on KITTI (Geiger et al., 2012) and Waymo (Sun et al.,
2020) datasets demonstrate that our method can bring con-
sistent performance gains and achieve the current state-of-
the-art. Particularly, the proposed GLENet-VR surpasses all
published single-modal detection methods by a large margin
and ranks 1ùë†ùë° among all published LiDAR-based approaches
on the highly competitive KITTI 3D detection benchmark on
March 29ùë° ‚Ñé, 20221.

We summarize the main contributions of this paper as

follows.

‚Äì We are the Ô¨Årst to formulate the 3D label uncertainty
problem as the diversity of potentially plausible bounding
boxes of objects. To capture the one-to-many relationship
between a typical 3D object and the potentially plausible
ground-truth bounding boxes, we present a deep gen-
erative model named GLENet. Besides, we introduce a
general and uniÔ¨Åed deep learning-based paradigm, in-
cluding the network structure, loss function, evaluation
metric, etc.

1 www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d

‚Äì Inspired by the strong correlation between the localiza-
tion quality and the predicted uncertainty in probabilistic
detectors, we propose UAQE to facilitate the training of
the IoU-branch.

The remainder of the paper is organized as follows. Sec-
tion 2 reviews existing works on LiDAR-based detectors and
label uncertainty estimation methods. In Section 3, we ex-
plicitly formulate the label uncertainty estimation problem
from the probabilistic distribution perspective, followed by
the technical implementation of GLENet. In Section 4, we
introduce a uniÔ¨Åed way of integrating the label uncertainty
statistics predicted by GLENet into the existing 3D object
detection frameworks to build more powerful probabilistic
detectors, as well as some theoretical analysis. In Section 5,
we conduct the experiments on the KITTI dataset and the
Waymo Open dataset to demonstrate the eÔ¨Äectiveness of our
method in enhancing existing 3D detectors and the ablation
study to analyze the eÔ¨Äect of diÔ¨Äerent components. Finally,
Section 6 concludes this paper.

2 Related Work

2.1 LiDAR-based 3D Object Detection

Existing 3D object detectors can be classiÔ¨Åed into two cat-
egories: single-stage and two-stage. For single-stage detec-
tors, Zhou and Tuzel (2018) proposed to convert raw point
clouds to regular volumetric representations and adopted
voxel-based feature encoding. Yan et al. (2018b) presented
a more eÔ¨Écient sparse convolution. Lang et al. (2019) con-
verted point clouds to sparse fake images using pillars. Shi

4

Yifan Zhang et al.

and Rajkumar (2020a) aggregated point information via a
graph structure. He et al. (2020) introduced point segmenta-
tion and center estimation as auxiliary tasks in the training
phase to enhance model capacity. Zheng et al. (2021a) con-
structed an SSFA module for robust feature extraction and
a multi-task head for conÔ¨Ådence rectiÔ¨Åcation, and proposed
DI-NMS for post-processing. For two-stage detectors, Shi
et al. (2020b) exploited a voxel-based network to learn the
additional spatial relationship between intra-object parts un-
der the supervision of 3D box annotations. Shi et al. (2019)
proposed to directly generate 3D proposals from raw point
clouds in a bottom-up manner, using semantic segmenta-
tion to valid point to regress detection boxes. The follow-
up work (Yang et al., 2019) further proposed PointsPool to
convert sparse proposal features to compact representations
and used spherical anchors to generate accurate proposals.
Shi et al. (2020a) utilized both point-based and voxel-based
methods to fuse multi-scale voxel and point features. Deng
et al. (2021) proposed voxel RoI pooling to extract RoI fea-
tures from coarse voxels.

Compared with 2D object detection, there are more se-
rious boundary ambiguity problems in 3D object detection
due to occlusion and signal miss. Studies, such as SPG (Xu
et al., 2021), try to use point cloud completion methods to
restore full shape of objects and improve the detection per-
formance (Yan et al., 2021; Najibi et al., 2020). However,
it‚Äôs non-trivial to generate complete and precise shapes with
incomplete point clouds only.

2.2 Probabilistic 3D Object Detector

There are two types of uncertainty in deep learning predic-
tions. A type of uncertainty, called aleatoric uncertainty, is
caused by the inherent noise in observational data, which
cannot be eliminated. The other type is called epistemic Un-
certainty or model uncertainty, which is caused by incom-
plete training and can be alleviated with more training data.
Most existing state-of-the-art 2D (Liu et al., 2016; Tan et al.,
2020; Carion et al., 2020) and 3D (Shi et al., 2020b) ob-
ject detectors produce a deterministic box with a conÔ¨Ådence
score for each detection. While the probability score repre-
sents the existence and semantic conÔ¨Ådence, it cannot reÔ¨Çect
the uncertainty about predicted localization well. By con-
trast, probabilistic object detectors (He et al., 2019; Meyer
et al., 2019; Li et al., 2020; Varamesh and Tuytelaars, 2020)
estimate the probabilistic distribution of predicted bound-
ing boxes rather than take them as deterministic results. For
example, He et al. (2019) and Choi et al. (2019) modeled
the predicted boxes as Gaussian distributions, the variance
of which can indicate the localization uncertainty and is
predicted with additional layer in the detection head. It in-
troduces the KL Loss between the predicted Gaussian dis-
tribution and the ground-truth bounding boxes modeled as

a Dirac delta function, so the regression branch is expected
to output a larger variance and get a smaller loss for inac-
curate localization estimation for the cases with ambiguous
boundaries. Unlike common practice to model the box as a
Gaussian distribution, Harakeh et al. (2020) learned the oÔ¨Ä-
diagonal elements of the covariance matrix of a multivariate
Gaussian distribution as uncertainty estimation. Meyer et al.
(2019) proposed a probabilistic 3D object detector model-
ing the distribution of bounding box corners as a Laplacian
distribution.

However, most probabilistic detectors take the ground-
truth bounding box as a deterministic Dirac delta distribution
and ignore the ambiguity in ground truth. Therefore, the lo-
calization variance is actually learned in an unsupervised
manner, which may result in sub-optimal localization pre-
cision and erratic training (see our theoretical analysis in
Section 4.1).

2.3 Label Uncertainty Estimation

Label noise (or uncertainty) is a common problem in real-
world datasets and could seriously aÔ¨Äect the performance
of supervised learning algorithms. As the neural network is
prone to overÔ¨Åt to even complete random noise (Zhang et al.
(2021)), it is important to prevent the network from over-
Ô¨Åtting noisy labels. An obvious solution is to consider the
label of a misclassiÔ¨Åed sample to be uncertain and remove
the samples (Delany et al., 2012). Garcia et al. (2015) used
a soft voting approach to approximate a noise level for each
sample based on the aggregation of the noise degree predic-
tion calculated for a set of binary classiÔ¨Åers. Luengo et al.
(2018) extended this work by correcting the label when most
classiÔ¨Åers predict the same label for noisy samples. ConÔ¨Å-
dent Learning Northcutt et al. (2021) estimated uncertainty
in dataset labels by estimating the joint distribution of noisy
labels and true labels. However, the above studies mainly
focus on the image classiÔ¨Åcation task.

There only exists a limited number of previous works
focusing on quantifying uncertainty statistics of annotated
ground-truth bounding boxes. Meyer and Thakurdesai (2020)
proposed to model label uncertainty by the IoU between
the label bounding box and the corresponding convex hull
of the aggregated LiDAR observations. However, it is non-
learning-based and thus has limited modeling capacity. Be-
sides, it only produces uncertainty of the ground-truth box as
a whole instead of each dimension. Wang et al. (2020) pro-
posed a Bayes method to estimate label noises by quantifying
the matching degree of point clouds for the given boundary
box with the Gaussian Mixture Model. However, its assump-
tion of conditional probabilistic independence between point
clouds is often untenable in practice. DiÔ¨Äerently, we formu-
late label uncertainty as diversity of potentially plausible
bounding boxes. There may be some objects with few points

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

5

that exactly match the learned surface points of correspond-
ing labeled Bbox, so the label is considered by (Wang et al.
(2020)) to be deterministic. But for an object with sparse
point clouds, our GLENet will output diÔ¨Äerent and plausible
Bboxes and further estimate high label uncertainty based on
them, regardless of whether points match the given label. In
general, Wang et al. (2020) used the Bayesian paradigm to
estimate the correctness of the annotated box as the label
uncertainty, while our method formulates it as the diversity
of potentially plausible bounding boxes and predicts it by
GLENet.

2.4 Conditional Variational Auto-Encoder

The variational auto-encoder (VAE) (Kingma and Welling,
2014) has been widely used in image and shape genera-
tion tasks (Yan et al., 2016; Nash and Williams, 2017). It
transforms natural samples into a distribution where latent
variables can be drawn and passed to a decoder network to
generate diverse samples. Sohn et al. (2015) proposed condi-
tional variational auto-encoder (CVAE) extending the VAE
with an extra condition to supervise the generative process.
In the NLP Ô¨Åeld, VAE has been widely applied to many
text generation tasks, such as dialogue response (Zhao et al.,
2017), machine translation (Zhang et al., 2016), story gen-
eration (Wang and Wan, 2019), and poem composing (Li
et al., 2018). VAE and CVAE have also been applied in com-
puter vision tasks, like image generation (Yan et al., 2016),
human pose estimation (Sharma et al., 2019), medical im-
age segmentation (Painchaud et al., 2020), salient object de-
tection (Li et al., 2019; Zhang et al., 2020), and modeling
human motion dynamics (Yan et al., 2018a). Recently, VAE
and CVAE algorithms have also been applied extensively
to applications of 3D point clouds, such as generating grasp
poses (Mousavian et al., 2019) and instance segmentation (Yi
et al., 2019).

Inspired by CVAE for generating diverse reasonable re-
sponses in dialogue systems, we propose GLENet adapted
from CVAE to capture the one-to-many relationship between
objects with incomplete point cloud and the potentially plau-
sible ground-truth bounding boxes. To the best of our knowl-
edge, we are the Ô¨Årst to employ CVAE in 3D object detection
to model label uncertainty.

3 Proposed Label Uncertainty Estimation

As aforementioned, the ambiguity of annotated ground-truth
labels widely exists in 3D object detection scenarios and has
adverse eÔ¨Äects on the deep model learning process, which
is not well addressed or even completely ignored by pre-
vious works. To this end, we propose GLENet, a generic

and uniÔ¨Åed deep learning framework that generates label un-
certainty by modeling the one-to-many relationship between
point cloud objects and potentially plausible bounding box
labels. Then the variance of the multiple outputs of GLENet
for a single object is computed as the label uncertainty, which
is extended as an auxiliary regression objective to enhance
the performance of the downstream 3D object detection task.

3.1 Problem Formulation

Let ùê∂ = {ùëêùëñ }ùëõ
ùëñ=1 be a set of ùëõ observed LiDAR points belong-
ing to an object, where ùëêùëñ ‚àà R3 is a 3D point represented
with spatial coordinates. Let ùëã be the annotated ground-
truth bounding box of ùê∂ parameterized by the center location
(ùëê ùë•, ùëê ùë¶, ùëêùëß), the size ( length ùëô , width ùë§, and height ‚Ñé), and
the orientation ùëü, i.e., ùëã = [ùëê ùë•, ùëê ùë¶, ùëêùëß, ùë§, ùëô, ‚Ñé, ùëü] ‚àà R7.

We formulate the uncertainty of the annotated ground-
truth label of an object as the diversity of potentially plausible
bounding boxes of the object, which could be quantitatively
measured with the variance of the distribution of the po-
tential bounding boxes. First, we model the distribution of
these potential boxes conditioned on point cloud ùê∂, denoted
as ùëù(ùëã |ùê∂). SpeciÔ¨Åcally, based on the Bayes theorem, we in-
troduce an intermediate variable ùëß to write the conditional
distribution as

ùëù(ùëã |ùê∂) =

‚à´

ùëß

ùëù(ùëã |ùëß, ùê∂) ùëù(ùëß|ùê∂)ùëëùëß.

(1)

Then, with ùëù(ùëã |ùëß, ùê∂) and ùëù(ùëß|ùê∂) known, we can adopt a
Monte Carol method to get multiple bounding box predic-
tions by sampling ùëß multiple times and approximate the vari-
ance of ùëù(ùëã |ùê∂) with that of the sampled predictions.

In the following, we will introduce our learning-based
framework named GLENet to realize the estimation process.

3.2 Inference Process of GLENet

Fig. 4 (a) shows the Ô¨Çowchart of GLENet parameterized
by neural parameters ùúÉ, which aims to predict ùëù(ùëß|ùê∂) and
ùëù(ùëã |ùëß, ùê∂). SpeciÔ¨Åcally, under the assumption that the prior
distribution ùëù(ùëß|ùê∂) subjects to a multivariate Gaussian dis-
tribution parameterized by(ùúáùëß, ùúéùëß), denoted as N (ùúáùëß, ùúé2
ùëß ),
we design a prior network, which is composed of Point-
Net (Qi et al., 2017) and additional MLP layers, from the
input point cloud ùê∂ to predict the values of (ùúáùëß, ùúéùëß). Then,
we employ a context encoder to embed the input point cloud
ùê∂ into a high dimensional feature space, leading to the geo-
metric feature representation ùëìùê∂ , which is concatenated with
ùëß sampled from N (ùúáùëß, ùúé2
ùëß ) and fed into a prediction network
composed of MLPs to regress the bounding box distribution
ùëù(ùëã |ùëß, ùê∂), i.e., the localization, dimension and orientation
of the bounding box.

6

Yifan Zhang et al.

Fig. 4: The overall workÔ¨Çow of GLENet. In the training phase, we learn parameters ùúá and ùúé (resp. ùúá(cid:48) and ùúé(cid:48) ) of latent
variable ùëß (resp. ùëß(cid:48)) through the prior network (resp. recognition network), after which a sample of ùëß(cid:48) and the corresponding
geometrical embedding produced by the context encoder are jointly exploited to estimate the bounding box distribution. In
the inference phase, we sample from the distribution of ùëß multiple times to generate diÔ¨Äerent bounding boxes, whose variance
we use as label uncertainty.

As empirically observed in various related domains (Goyal
et al., 2017), it could be diÔ¨Écult to make use of latent vari-
ables when the prediction network can generate a plausi-
ble output only using the suÔ¨Éciently expressive features of
condition ùê∂. Therefore, we utilize a simpliÔ¨Åed PointNet ar-
chitecture as the backbone of the context encoder to avoid
posterior collapse. We refer the readers to Section 5.1.3 for
the implementation details of these modules. In the following
sections, we also use ùëù ùúÉ (ùëß|ùê∂), ùëù ùúÉ (ùëã |ùëß, ùê∂), and ùëù ùúÉ (ùëã |ùê∂) to
denote the predictions of ùëù(ùëß|ùê∂), ùëù(ùëã |ùëß, ùê∂), and ùëù(ùëã |ùê∂) by
GLENet, respectively.

3.3 Training Process of GLENet

3.3.1 Recognition Network

Given ùê∂ and its annotated bounding box ùëã, we assume there
is a true posterior distribution ùëû(ùëß|ùëã, ùê∂). Thus, during train-
ing, we construct a recognition network parameterized by
network parameters ùúô (see Fig. 4 (b)) to learn an auxiliary
posterior distribution ùëû ùúô (ùëß(cid:48)|ùëã, ùê∂) subjecting to a Gaussian
ùëß ), to regularize ùëù ùúÉ (ùëß|ùê∂),
distribution, denoted as N (ùúá(cid:48)
i.e., ùëù ùúÉ (ùëß|ùê∂) should be close to ùëû ùúô (ùëß(cid:48)|ùëã, ùê∂).

ùëß, ùúé(cid:48)2

SpeciÔ¨Åcally, for the recognition network, we adopt the
same learning architecture as the prior network to gener-
ate point cloud embeddings, which are concatenated with
ground-truth bounding box information and fed into the sub-
sequent MLP layers to learn ùëû ùúô (ùëß(cid:48)|ùëã, ùê∂). Moreover, to fa-
cilitate the learning process, we encode the information ùëã
into oÔ¨Äsets relative to predeÔ¨Åned anchors, and then perform

normalization as:

ùëêùëîùë°
ùë•
ùëë ùëé

ùë°ùëêùë• =

, ùë°ùëêùë¶ =
ùë§ùëîùë°
ùë°ùë§ = log
ùë§ùëé
ùë°ùëü = sin(ùëü ùëîùë° ),

ùëêùëîùë°
ùëß
‚Ñéùëé

,

ùëêùëîùë°
ùë¶
ùëë ùëé

, ùë°ùëêùëß =
ùëôùëîùë°
ùëô ùëé

, ùë°ùëô = log

, ùë°‚Ñé = log

‚Ñéùëîùë°
‚Ñéùëé

,

(2)

where (ùë§ùëé, ùëô ùëé, ‚Ñéùëé) is the size of the predeÔ¨Åned anchor located
in the center of the point cloud, and ùëë ùëé = ‚àöÔ∏Å(ùëô ùëé)2 + (ùë§ùëé)2
is the diagonal of the anchor box. We also take cos(ùëü) as
the additional input of the recognition network to handle the
issue of angle periodicity.

3.3.2 Objective Function

Following CAVE, we optimize GLENet via maximizing the
variational lower bound of the conditional log likelihood
ùëù ùúÉ (ùëã |ùê∂):

log ùëù ùúÉ (ùëã |ùê∂) ‚â• ùê∏ùëûùúô (ùëß(cid:48) |ùëã ,ùê∂) [log ùëù ùúÉ (ùëã |ùëß, ùê∂)]‚àí

ùêæ ùêø (ùëû ùúô (ùëß(cid:48)|ùëã, ùê∂)|| ùëù ùúÉ (ùëß|ùê∂)),

(3)

where ùê∏ùëû [ ùëù] returns the expectation of ùëù on the distribution
of ùëû, and ùêæ ùêø (¬∑) denotes KL-divergence.

SpeciÔ¨Åcally, the Ô¨Årst term ùê∏ùëûùúô (ùëß(cid:48) |ùëã ,ùê∂) [log ùëù ùúÉ (ùëã |ùëß, ùê∂)]
enforces the prediction network to be able to restore ground-
truth bounding box from latent variables. Following (Yan
et al. (2018b)) and (Deng et al. (2021)), we explicitly deÔ¨Åne
the bounding box reconstruction loss as

ùêøùëü ùëíùëê = ùêøùëü ùëíùëî

ùëü ùëíùëê + ùúÜùêøùëëùëñùëü
ùëü ùëíùëê,

(4)

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

7

where ùêøùëü ùëíùëî
ùëü ùëíùëê denotes the Huber loss imposed on the prediction
and encoded regression targets as described in Eq. (2), and
ùêøùëëùëñùëü
ùëü ùëíùëê denotes the binary cross-entropy loss used for direction
classiÔ¨Åcation.

The second term ùêæ ùêø (ùëû ùúô (ùëß(cid:48)|ùëã, ùê∂)(cid:107) ùëù ùúÉ (ùëß|ùê∂)) is aimed
at regularizing the distribution of ùëß by minimizing the KL-
divergence between ùëù ùúÉ (ùëß|ùê∂) and ùëû ùúô (ùëß(cid:48)|ùëã, ùê∂). Since ùëù ùúÉ (ùëß|ùê∂)
and ùëû ùúô (ùëß|ùëã, ùê∂) are re-parameterized as N (ùúáùëß, ùúé2
ùëß ) and
N (ùúá(cid:48)
ùëß ) through the prior network and the recognition
network, respectively, we can explicitly deÔ¨Åne the regular-
ization loss as:

ùëß, ùúé(cid:48)2

ùêøùêæ ùêø (ùëû ùúô (ùëß|ùëã, ùê∂)(cid:107) ùëù ùúÉ (ùëß|ùê∂)) = log

ùúé(cid:48)
ùëß
ùúéùëß

+

ùúé2
ùëß
2ùúé(cid:48)2
ùëß

+

ùëß)2

(ùúáùëß ‚àí ùúá(cid:48)
2ùúé(cid:48)2
ùëß

,

Thus, the overall objective function is written as

ùêø = ùêøùëü ùëíùëê + ùõæ ùêøùêæ ùêø,

(5)

(6)

where we empirically set the hyperparameter ùõæ set to 1 in all
experiments.

4 Probabilistic 3D Detectors with Label Uncertainty

To reform a typical detector to be a probabilistic object de-
tector, we can enforce the detection head to estimate a prob-
ability distribution over bounding boxes, denoted as ùëÉŒò (ùë¶),
instead of a deterministic bounding box location:

ùëÉŒò(ùë¶) =

‚àö

ùëí‚àí ( ùë¶‚àí ÀÜùë¶) 2

2 ÀÜùúé2

,

1
2ùúã ÀÜùúé2

(7)

where Œò indicates learnable network weights of a typical
detector, ÀÜùë¶ is the predicted bounding box location, and ÀÜùúé is
the predicted localization variance.

Accordingly, we also assume the ground-truth bound-
ing box as a Gaussian distribution ùëÉùê∑ (ùë¶) with variance ùúé2,
whose value is estimated by GLENet:

ùëÉùê∑ (ùë¶) =

‚àö

( ùë¶‚àíùë¶ùëî )2
2ùúé2

,

ùëí‚àí

1
2ùúãùúé2

(8)

where ùë¶ùëî represents the ground-truth bounding box. There-
fore, we can incorporate the generated label uncertainty in the
KL loss between the distribution of prediction and ground-
truth in the detection head:

ùêøùëü ùëíùëî = ùê∑ ùêæ ùêø (ùëÉùê∑ (ùë¶)||ùëÉŒò(ùë¶))

= log

ÀÜùúé
ùúé

+

ùúé2
2 ÀÜùúé2

+

(ùë¶ùëî ‚àí ÀÜùë¶)2
2 ÀÜùúé2

.

(9)

4.1 More Analysis of KL-Loss

When ignoring label ambiguity and formulating the ground-
truth bounding box as a Dirac delta function, as done in (He
et al. (2019)), the loss in Eq. (9) degenerates into

ùêø ùëùùëü ùëúùëè
ùëü ùëíùëî ‚àù

log( ÀÜùúé2)
2

+

(ùë¶ùëî ‚àí ÀÜùë¶)2
2 ÀÜùúé2

,

(10)

and the partial derivative of Eq. (10) with respect to the
predicted variance ÀÜùúé is:
ùúïùêø ùëùùëü ùëúùëè
ùëü ùëíùëî
ùúï ÀÜùúé

(ùë¶ùëî ‚àí ÀÜùë¶)2
ÀÜùúé3

(11)

1
ÀÜùúé

‚àí

=

.

When minimizing Eq. (10), a potential issue is that with
|ùë¶ùëî ‚àí ÀÜùë¶| ‚Üí 0,

ùúïùêø ùëùùëü ùëúùëè
ùëü ùëíùëî
ùúï ÀÜùúé

‚Üí

,

1
ÀÜùúé

(12)

resulting in that the derivative for ÀÜùúé will explode when ÀÜùúé ‚Üí
0. Based on the property of KL-loss, the prediction is optimal
only when the estimated ÀÜùúé = 0 and the localization error
|ùë¶ùëî ‚àí ÀÜùë¶| = 0. Therefore, the gradient explosion may result in
erratic training and sub-optimal localization precision.

By contrast, after modeling the ground-truth bounding
box as a Gaussian distribution, the partial derivative of Eq. (9)
with respect to prediction is:

=

1
ÀÜùúé

‚àí

ùúé2
ÀÜùúé3

‚àí

(ùë¶ùëî ‚àí ÀÜùë¶)2
ÀÜùúé3

,

ùúïùêøùëü ùëíùëî
ùúï ÀÜùúé
and
ùúïùêøùëü ùëíùëî
ùúï ÀÜùë¶

=

ÀÜùë¶ ‚àí ùë¶ùëî
ÀÜùúé2
As |ùë¶ùëî ‚àí ÀÜùë¶| ‚Üí 0 and ÀÜùúé > 0,

.

ùúïùêø ùëùùëü ùëúùëè
ùëü ùëíùëî
ùúï ÀÜùúé

and
ùúïùêø ùëùùëü ùëúùëè
ùëü ùëíùëî
ùúï ÀÜùë¶

‚Üí

1
ÀÜùúé

(1 ‚àí

ùúé2
ÀÜùúé2

),

‚Üí 0.

(13)

(14)

(15)

(16)

Thus, when the predicted distribution reaches the optimal
solution that is the distribution of ground-truth, i.e., |ùë¶ùëî ‚àí
ÀÜùë¶| ‚Üí 0 and ÀÜùúé ‚Üí ùúé, the derivatives for both ÀÜùë¶ and ÀÜùúé become
zero, which is an ideal property for the loss function and
avoids the aforementioned gradient explosion issue.

Fig. 5 shows the landscape of the KL-divergence loss
function under diÔ¨Äerent label uncertainty ùúé, which are mark-
edly diÔ¨Äerent in shape and property. The ùêø ùëùùëü ùëúùëè
approaches
ùëü ùëíùëî
inÔ¨Ånitesimal and the gradient explodes as |ùë¶ùëî ‚àí ÀÜùë¶| ‚Üí 0
and ÀÜùúé ‚Üí 0. However, when we introduce the estimated
label uncertainty and the predicted distribution is equal to
the ground-truth distribution, the KL Loss has a determined
minimum value of 0.5 and the gradient is smoother.

8

Yifan Zhang et al.

(a) ùêø ùëùùëü ùëúùëè

ùëü ùëíùëî ( ùúé = 0)

(b) ùêøùëü ùëíùëî ( ùúé = 0.2)

(c) ùêøùëü ùëíùëî ( ùúé = 0.5)

Fig. 5: Illustration of the KL-divergence between distributions as a function of localization error |ùë¶ùëî ‚àí ÀÜùë¶| and estimated
localization variance ÀÜùúé given diÔ¨Äerent label uncertainty ùúé. With label uncertainty ùúé estimated by GLENet instead of zero,
the gradient is smoother when the loss converges to the minimum. Besides, the ùêøùëü ùëíùëî is smaller when ùúé is larger, which
prevents the model from overÔ¨Åtting to uncertain annotations.

Fig. 6: (a) Illustration of the relationship between the actual localization precision (i.e., IoU between predicted and ground-
truth bounding box) and the variance predicted by a probabilistic detector. Here, we reduce the dimension of the variance
with PCA to facilitate visualization. (b) Two examples: for the sparse sample, the prediction has high uncertainty and low
localization quality, while for the dense sample, the prediction has high localization quality and low uncertainty estimation.

4.2 Uncertainty-aware Quality Estimator

Most state-of-the-art two-stage 3D object detectors predict
an IoU-related conÔ¨Ådence score indicating the localization
quality, rather than the classiÔ¨Åcation score as the sorting cri-
terion in NMS (non-maximum suppression). As illustrated
in Fig. 6, it can be observed that there is a strong correlation
between the uncertainty and actual localization quality for
each bounding box, which encourages us to use uncertainty
as a criterion for judging the quality of boxes. However, the
estimated uncertainty is 7-dimensional, making it infeasible
to directly replace the IoU conÔ¨Ådence score with the un-
certainty. To this end, we propose uncertainty-aware quality
estimator (UAQE), which introduces the uncertainty infor-
mation to facilitate the training of the IoU-branch and im-
prove the IoU estimation accuracy. SpeciÔ¨Åcally, as shown in
Fig. 7, given the predicted uncertainty as input, we construct

Fig. 7: Illustration of the proposed UAQE module in the de-
tection head using the learned localization variance to assist
the training of localization quality (IoU) estimation branch.

a lightweight sub-module consisting of two fully-connected
(FC) layers followed by the Sigmoid activation to generate
a coeÔ¨Écient. Then we multiply the original output of the
IoU-branch with the coeÔ¨Écient as the Ô¨Ånal estimation.

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

9

Algorithm 1: 3D var voting

Data: ùêµ is an ùëÅ √ó 7 matrix of predicted bounding boxes with
parameter ( ùë•, ùë¶, ùëß, ùë§ , ùëô, ‚Ñé, ùúÉ). ùê∂ is the corresponding
variance. ùëÜ is a set of N corresponding conÔ¨Ådence
values. ùúéùë° is a tunable hyperparameter.

Result: The Ô¨Ånal voting results ùê∑ of selected candidate boxes.

1 ùêµ = {ùëè1, ùëè2, ..., ùëèùëÅ }; and ùê∂ = {ùëê1, ùëê2, ..., ùëêùëÅ };
2 ùëÜ = {ùë†1, ùë†2, ..., ùë†ùëÅ }; and ùêø = {1, 2, ..., ùëÅ };
3 ùê∑ ‚Üê {};
4 ùëñùëúùë¢ùë° ‚Ñéùëü ùëíùë†‚Ñé ‚Üê ùúá;
5 while ùêø ‚â† ‚àÖ do
idx =argmax
6

ùëÜ, ùëè(cid:48) = ùëèùëñùëëùë•;

ùëñ‚ààùêø

ùêø(cid:48) = {ùëñ |ùëñ ‚àà ùêø, ùêº ùëúùëà (ùëèùëñ , ùëè(cid:48)) > ùëñùëúùë¢ùë° ‚Ñéùëü ùëíùë†‚Ñé };
ùëÉ ‚Üê {};
for ùëñ ‚àà ùêø(cid:48) do

ùëùùëñ = ùëí‚àí(1‚àíùêº ùëúùëà (ùëèùëñ ,ùëè) ) 2/ùúéùë° ;
if |ùë° ùëéùëõ(ùëè ùúÉ
ùëù ùúÉ
ùëñ = 0;

ùëñ ‚àí ùëè(cid:48) ùúÉ ) | > 1 then

end
ùëÉ ‚Üê ùëÉ (cid:208) ùëùùëñ;

(cid:205)ùëñ‚ààùêø(cid:48) ùëèùëñ ¬∑ ùëùùëñ /ùëêùëñ
(cid:205)ùëñ‚ààùêø(cid:48) ùëùùëñ /ùëêùëñ

end
ùëèùëö =
ùê∑ ‚Üê ùê∑ (cid:208) ùëèùëö;
ùêø ‚Üê ùêø ‚àí ùêø(cid:48);

, ùëùùëñ ‚àà ùëÉ, ùëèùëñ ‚àà ùêµ, ùëêùëñ ‚àà ùê∂;

7

8

9

10

11

12

13

14

15

16

17

18
19 end

4.3 3D Variance Voting

Considering that in probabilistic object detectors, the learned
localization variance by the KL loss can reÔ¨Çect the uncer-
tainty of the predicted bounding boxes, following (He et al.,
2019), we also propose 3D variance voting to combine neigh-
boring bounding boxes to seek a more precise box represen-
tation. SpeciÔ¨Åcally, at a single iteration in the loop, box ùëè
with maximum score is selected and its new location is cal-
culated according to itself and the neighboring boxes. During
the merging process, the neighboring boxes that are closer
and have a low variance are assigned with higher weights.
Note that neighboring boxes with a large angle diÔ¨Äerence
from ùëè do not participate in the ensembling of angles. We
refer the readers to Algorithm 1 for the details.

series of ablation studies to verify the necessity of diÔ¨Äerent
key components and conÔ¨Ågurations in Section 5.4.

5.1 Experiment Settings

5.1.1 Benchmark Datasets

The KITTI dataset contains 7481 training samples with an-
notations in the camera Ô¨Åeld of vision and 7518 testing sam-
ples. According to the occlusion level, visibility and bound-
ing box size, the samples are further divided into three dif-
Ô¨Åculty levels: simple, moderate and hard. Following com-
mon practice, when performing experiments on the val set,
we further split all training samples into a subset with 3712
samples for training and the rest 3769 samples for validation.
We report the performance on both the val set and online test
leaderboard for comparison. And we use all training data for
the test server submission.

The Waymo Open dataset is a large-scale autonomous driv-
ing dataset with more diverse scenes and object annotations
in full 360‚ó¶, which contains 798 sequences (158361 Li-
DAR frames) for training and 202 sequences (40077 LiDAR
frames) for validation. These frames are further divided into
two diÔ¨Éculty levels: LEVEL1 for boxes with more than Ô¨Åve
points and LEVEL2 for boxes with at least one point. We
report performance on both LEVEL 1 and LEVEL 2 diÔ¨É-
culty objects using the recommended metrics, mean Average
Precision (mAP) and mean Average Precision weighted by
heading accuracy (mAPH).

5.1.2 Evaluation Metric for GLENet

Due to the unavailability of the true distribution of a ground-
truth bounding box, we propose to evaluate GLENet in a
non-reference manner, in which the negative log-likelihood
between the estimated distribution of ground-truth ùëùùê∑ (ùëã |ùê∂)
subjecting to a Gaussian distribution N (ÀÜùë°, ùúé2) and ùëù ùúÉ (ùëã |ùê∂)
is computed:

5 Experiments

ùêø ùëÅ ùêøùêø (ùúÉ) = ‚àí

ùëù ùúÉ (ùëã |ùëê) log ùëùùê∑ (ùëã |ùê∂)dùëã

(17)

To reveal the eÔ¨Äectiveness and universality of our method,
we integrated GLENet into several popular types of 3D ob-
ject detection frameworks to form probabilistic detectors,
which were evaluated on two commonly used benchmark
datasets, i.e., the Waymo Open dataset (WOD) (Sun et al.,
2020) and the KITTI dataset (Geiger et al., 2012). Specif-
ically, we start by introducing speciÔ¨Åc experiment settings
and implementation details in Section 5.1. After that, we
report detection performance of the resulting probabilistic
detectors and make comparisons with previous state-of-the-
art approaches in Sections 5.2 and 5.3. Finally, we conduct a

‚à´

1
ùëÜ

1
ùëÜ

ùëÜ
‚àëÔ∏Å

ùëñ=1
ùëÜ
‚àëÔ∏Å

ùëñ=1

‚âà ‚àí

= ‚àí

log ùëùùê∑ (ùëãùëñ |ùê∂)

ùëò )2

ùëò ‚àí ÀÜùë°ùëñ
(ùë°ùëñ
2ùúéùëò 2

+

log(ùúé2
ùëò )
2

+

log(2ùúã)
2

,

‚àëÔ∏Å

ùëò‚àà{ùëêùë• ,ùëêùë¶ ,
ùëêùëß ,ùë§ ,ùëô,‚Ñé,ùëü }

where ùëÜ denotes the number of inference times, ùëãùëñ is the
result of the ùëñ-th inference, and ÀÜùë°ùëñ
ùëò represent the re-
gression targets and the predicted oÔ¨Äsets, respectively. We
estimate the integral by randomly sampling multiple pre-
diction results via the Monte Carlo method. Generally, the
value of ùêø ùëÅ ùêøùêø is small when GLENet outputs reasonable

ùëò and ùë°ùëñ

10

Yifan Zhang et al.

Fig. 8: Illustration of the occlusion data augmentation. (a) The point cloud of the original object associated with the annotated
ground-truth bounding box. (b) A sampled dense object (red) is placed between the LiDAR sensor and original object (blue).
(c) The projected range image from the point cloud in (b), where the convex hull (the red polygon) of the sampled object
is calculated and further jittered to increase the diversity of occluded samples. Based on the convex hull (the blue polygon)
of the original point cloud, the occluded area can be obtained. The point cloud of the original object corresponding to the
occluded area is removed. (d) Final augmented object with the annotated ground-truth bounding boxes.

bounding boxes, i.e., predicting diverse plausible boxes with
high variance for incomplete point cloud and consistent pre-
cise boxes with low variance for high-quality point cloud,
respectively.

5.1.3 Implementation Details

To prevent data leakage, we kept the dataset division of
GLENet consistent with that of the downstream detectors.
As the initial input of GLENet, the point cloud of each ob-
ject was uniformly pre-processed into 512 points via random
subsampling/upsampling. Then we decentralized the point
cloud by subtracting the coordinates of the center point to
eliminate the local impact of translation.

Architecturally, we realized the prior network and recog-
nition network with an identical PointNet structure consist-
ing of three FC layers of output dimensions (64, 128, 512),
followed by another FC layer to generate an 8-dim latent
variable. To avoid posterior collapse, we particularly chose a
lightweight PointNet structure with channel dimensions (8,
8, 8) in the context encoder. The prediction network concate-
nates the generated latent variable and context features and
feeds them into subsequent FC layers of channels (64, 64)
before predicting oÔ¨Äsets and directions.

5.1.4 Training and Inference Strategies

We adopted Adam (Kingma and Ba, 2015) (ùõΩ1=0.9, ùõΩ2=0.99)
to optimize GLENet, which was trained for totally 400 epochs
on KITTI and 40 epochs on Waymo while maintaining a
batch size of 64 on 2 GPUs. We initialized the learning rate
as 0.003 and updated it with the one cycle policy (Smith,
2017).

In the training process, we applied common data aug-
mentation strategies, including random Ô¨Çipping, scaling, and
rotation, in which the scaling factor and rotation angle were

uniformly drawn from [0.95, 1.05] and [‚àíùúã/4, ùúã/4], respec-
tively. It is important to include multiple plausible ground-
truth boxes in training especially for incomplete point clouds,
so we further propose an occlusion-driven augmentation ap-
proach, as illustrated in Fig. 8, after which a complete point
cloud may look similar to another incomplete point cloud,
while the ground-truth boxes of them are completely dif-
ferent. To overcome posterior collapse, we also adopted KL
annealing (Bowman et al., 2016) to gradually increase the
weight of the KL loss from 0 to 1. We followed k-fold cross-
sampling to divide all training objects into 10 mutually exclu-
sive subsets. To overcome overÔ¨Åtting, each time we trained
GLENet on 9 subsets and then made predictions on the re-
maining subset to generate label uncertainty estimations on
the whole training set. During inference, we sampled the la-
tent variable ùëß from the predicted prior distribution ùëù ùúÉ (ùëß|ùëê)
30 times to form multiple predictions, the variance of which
was used as the label uncertainty.

5.1.5 Base Detectors

We integrated GLENet into three popular deep 3D object
detection frameworks, i.e., SECOND (Yan et al., 2018b),
CIA-SSD (Zheng et al., 2021a), and Voxel R-CNN (Deng
et al., 2021), to construct probabilistic detectors, which are
dubbed as GLENet-S, GLENet-C, and GLENet-VR, respec-
tively. SpeciÔ¨Åcally, we introduced an extra FC layer on the top
of the detection head to estimate standard deviations along
with the box locations. Meanwhile, we applied the proposed
UAQE to GLENet-VR to facilitate the training of the IoU-
branch. Note that we kept all the other network conÔ¨Ågurations
of these base detectors unchanged for fair comparisons.

5.2 Evaluation on the KITTI Dataset

We compared GLENet-VR with state-of-the-art detectors on
the KITTI test set, and Table 1 reports the AP and mAP that

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

11

Table 1: Quantitative comparison with state-of-the-art methods on the KITTI test set for vehicle detection, under the evaluation
metric of 3D Average Precision (AP) of 40 sampling recall points. The best and second best results are highlighted in bold
and underlined, respectively.

Method

MV3D (Chen et al., 2017)
F-PointNet (Qi et al., 2018)
MMF (Liang et al., 2019)
PointPainting (Vora et al., 2020)
CLOCs (Pang et al., 2020)
EPNet (Huang et al., 2020)
3D-CVF (Yoo et al., 2020)
STD (Yang et al., 2019)
Part-A2 (Shi et al., 2020b)
3DSSD (Yang et al., 2020)
SA-SSD (He et al., 2020)
PV-RCNN (Shi et al., 2020a)
PointGNN (Shi and Rajkumar, 2020b)
Voxel-RCNN (Deng et al., 2021)
SE-SSD (Zheng et al., 2021b)
VoTR (Mao et al., 2021b)
Pyramid-PV (Mao et al., 2021a)
CT3D (Sheng et al., 2021)
GLENet-VR (Ours)

Reference

CVPR‚Äô17
CVPR‚Äô18
CVPR‚Äô19
CVPR‚Äô20
IROS‚Äô20
ECCV‚Äô20
ECCV‚Äô20
ICCV‚Äô19
TPAMI‚Äô20
CVPR‚Äô20
CVPR‚Äô20
CVPR‚Äô20
CVPR‚Äô 20
AAAI‚Äô21
CVPR‚Äô21
ICCV‚Äô21
ICCV‚Äô21
ICCV‚Äô21
-

Modality

RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR

3D APùëÖ40

Easy
74.97
82.19
88.40
82.11
88.94
89.81
89.20
87.95
87.81
88.36
88.80
90.25
88.33
90.90
91.49
89.90
88.39
87.83
91.67

Mod.
63.63
69.79
77.43
71.70
80.67
79.28
80.05
79.71
78.49
79.57
79.52
81.43
79.47
81.62
82.54
82.09
82.08
81.77
83.23

Hard
54.00
60.59
70.22
67.08
77.15
74.59
73.11
75.09
73.51
74.55
72.30
76.82
72.29
77.06
77.15
79.14
77.49
77.16
78.43

mAP
64.20
70.86
78.68
73.63
82.25
81.23
80.79
80.92
79.94
80.83
80.21
82.83
80.03
83.19
83.73
83.71
82.65
82.25
84.44

comparable to the exiting two-stage approaches while achiev-
ing relatively lower inference costs. It is worth noting that
our method is compatible with mainstream detectors and can
be expected to achieve better performance when combined
with stronger base detectors.

5.3 Evaluation on the Waymo Open Dataset

Table 3 lists the evaluation results of diÔ¨Äerent approaches on
both LEVEL_1 and LEVEL_2 of the Waymo Open dataset,
where it can be seen that our method contributes 2.44%
and 1.24% improvements in terms of LEVEL_1 mAP for
SECOND and Voxel R-CNN, respectively. Besides, the per-
formance boost brought by our method becomes much more
obvious in the range of 30-50m and 50m-Inf. Intuitively, this
is because distant point cloud objects tend to be sparser and
thus have more serious issues of bounding box ambiguity.
GLENet-VR achieves better performance than the existing
methods with 77.32% mAP and 69.68% mAP for the LEVEL
1 and LEVEL 2 diÔ¨Éculty, respectively.

5.4 Ablation Study

We conducted ablative analyses to verify the eÔ¨Äectiveness
and characteristics of our processing pipeline. In this section,
all the involved model variants are built upon the Voxel R-
CNN baseline and evaluated on the KITTI dataset, under
the evaluation metric of average precision calculated with 40
recall positions.

Fig. 9: PR curves of GLENet-VR on the car class of the
KITTI test set.

averages over the APs of easy, moderate and hard objects.
As of March 29ùë° ‚Ñé, 2022, our GLENet-VR surpasses all pub-
lished single-modal detection methods by a large margin and
ranks 1ùë†ùë° among all published LiDAR-based approaches. Be-
sides, Fig. 9 also provides the detailed Prevision-Recall (PR)
curves of GLENet-VR on KITTI test split.

Table 2 lists the validation results of diÔ¨Äerent detection
frameworks on the KITTI dataset, from which we can ob-
serve that GLENet-S, GLENet-C, and GLENet-VR consis-
tently outperform their corresponding baseline methods, i.e.,
SECOND, CIA-SSD, and Voxel R-CNN, by 4.79%, 4.78%,
and 1.84% in terms of 3D R11 AP on the category of mod-
erate car. Particularly, GLENet-VR achieves 86.36% AP on
the moderate car class, which surpasses all other state-of-the-
art methods. Besides, as a single-stage method, GLENet-C
achieves 84.59% AP for the moderate vehicle class, which is

12

Yifan Zhang et al.

Table 2: Quantitative comparison of diÔ¨Äerent methods on the KITTI validation set for vehicle detection, under the evaluation
metric of 3D Average Precision (AP) calculated with 11 sampling recall positions. The 3D APs under 40 recall sampling recall
points are also reported for the moderate car class. The best and second best results are highlighted in bold and underlined,
respectively.

Methods

Part-ùê¥2 (Shi et al., 2020b)
3DSSD (Yang et al., 2020)
SA-SSD (He et al., 2020)
PV-RCNN (Shi et al., 2020a)
SE-SSD (Zheng et al., 2021b)
VoTR (Mao et al., 2021b)
Pyramid-PV (Mao et al., 2021a)
CT3D (Sheng et al., 2021)
SECOND (Yan et al., 2018b)
GLENet-S (Ours)
CIA-SSD (Zheng et al., 2021a)
GLENet-C (Ours)
Voxel R-CNN (Deng et al., 2021)
GLENet-VR (Ours)

Reference

TPAMI‚Äô20
CVPR‚Äô20
CVPR‚Äô20
CVPR‚Äô20
CVPR‚Äô21
ICCV‚Äô21
ICCV‚Äô21
ICCV‚Äô21
Sensors‚Äô18
-
AAAI‚Äô21
-
AAAI‚Äô21
-

Easy
89.47
89.71
90.15
89.35
90.21
89.04
89.37
89.54
88.61
88.68
90.04
89.82
89.41
89.93

3D APùëÖ11
Moderate
79.47
79.45
79.91
83.69
85.71
84.04
84.38
86.06
78.62
82.95
79.81
84.59
84.52
86.46

Hard
78.54
78.67
78.78
78.70
79.22
78.68
78.84
78.99
77.22
78.19
78.80
78.78
78.93
79.19

Easy
-
-
92.23
92.57
93.19
-
-
92.85
91.16
91.73
93.59
93.20
92.38
93.51

3D APùëÖ40
Moderate
-
-
84.30
84.83
86.12
-
-
85.82
81.99
84.11
84.16
85.16
85.29
86.10

Hard
-
-
81.36
83.31
83.31
-
-
83.46
78.82
81.35
81.20
81.94
82.86
83.60

Table 3: Quantitative comparison of diÔ¨Äerent methods on the Waymo validation set for vehicle detection. ‚òÖ: experiment results
re-produced with the oÔ¨Écial code. The best and second best results are highlighted in bold and underlined, respectively.

Methods

PointPillar (Lang et al., 2019)
MVF (Zhou et al., 2020)
PV-RCNN (Shi et al., 2020a)
VoTr-TSD (Mao et al., 2021b)
Pyramid-PV (Mao et al., 2021a)
CT3D (Sheng et al., 2021)
SECOND‚òÖ (Yan et al., 2018b)
GLENet-S (Ours)
Voxel R-CNN‚òÖ (Deng et al., 2021)
GLENet-VR (Ours)

Overall
56.62
62.93
70.30
74.95
76.30
76.30
69.85
72.29
76.08
77.32

LEVEL_1 3D mAP
mAPH
0-30m 30-50m 50m-inf Overall
81.01
86.30
91.92
92.28
92.67
92.51
90.71
91.02
92.44
92.97

51.75
60.02
69.21
73.36
74.91
75.07
68.93
71.86
74.67
76.28

27.94
36.02
42.17
51.09
54.54
55.36
41.17
45.43
54.69
55.98

-
-
69.69
74.25
75.68
-
69.40
71.85
75.67
76.85

Overall
-
-
65.36
65.91
67.23
69.04
62.76
64.78
68.06
69.68

LEVEL_2 3D mAP
mAPH
0-30m 30-50m 50m-inf Overall

-
-
91.58
-
-
91.76
86.92
87.56
91.56
92.09

-
-
65.13
-
-
68.93
62.57
65.11
69.62
71.21

-
-
36.46
-
-
42.60
35.89
38.60
42.80
44.36

-
-
64.79
65.29
66.68
-
62.30
64.25
67.64
68.97

5.4.1 Comparison with Other Label Uncertainty Estimation

(Wang et al., 2020) signiÔ¨Åcantly in terms of APùêµùê∏ùëâ on both
moderate and hard levels.

We compared with other two ways of label uncertainty esti-
mation: 1) treating the label distribution as the deterministic
Dirac delta distribution with zero uncertainty; 2) estimating
the label uncertainty with simple heuristics, i.e., the num-
ber of points in the ground-truth bounding box or the IoU
between the label bounding box and its convex hull of the
aggregated LiDAR observations (Meyer and Thakurdesai,
2020). As shown in Table 4, our method consistently outper-
forms existing label uncertainty estimation paradigms. Com-
pared with heuristic strategies, our deep generative learning
paradigm can adaptively estimate label uncertainty statistics
in 7 dimensions, instead of the uncertainty of bounding boxes
as a whole, considering the variance in each dimension could
be very diÔ¨Äerent.

Besides, to compare with (Wang et al., 2020), whose code
is not publicly available, we evaluated our method under its
experiment settings and compared results with its reported
performance. As shown in Table 5, our method outperforms

Table 4: Comparison of diÔ¨Äerent label uncertainty estimation
approaches. "Convex hull" refers to the method in (Meyer and
Thakurdesai, 2020).

Methods

Voxel R-CNN
GLENet-VR w/ ùêøùêæ ùêøùê∑ (ùúé2=0)
GLENet-VR w/ ùêøùêæ ùêøùê∑ (points num)
GLENet-VR w/ ùêøùêæ ùêøùê∑ (convex hull)
GLENet-VR w/ ùêøùêæ ùêøùê∑ (Ours)

3D APùëÖ40
Easy Moderate Hard
82.86
85.29
92.38
83.05
85.37
92.48
83.16
85.58
92.46
82.81
85.45
92.33
83.56
86.10
93.49

5.4.2 Key Components of Probabilistic Detectors

We analyzed the contributions of diÔ¨Äerent key components
in our constructed probabilistic detectors and reported results

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

13

Table 5: Comparison of our method with (Wang et al., 2020) on the
KITTI val set.

Method

ùê¥ùëÉùêµùê∏ùëâ for IoU@0.7
Easy Mod. Hard
76.60
86.79 80.75
PIXOR (Yang et al., 2018)
ProbPIXOR + Lùêæ ùêøùê∑ (ùúé = 0)
78.74
88.60 80.44
ProbPIXOR + Lùêæ ùêøùê∑ (Wang et al., 2020) 92.22 82.03
79.16
ProbPIXOR + Lùêæ ùêøùê∑ (Ours)
81.85
91.50 84.23

Table 6: Contribution of each component in our constructed
GLENet-VR pipeline. ‚ÄúLU" denotes the label uncertainty.

KL loss LU var voting UAQE

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

Easy Moderate Hard
82.86
85.29
92.38
82.99
85.25
92.45
83.05
85.37
92.48
83.29
85.76
93.20
83.41
85.91
93.24
83.56
86.10
93.49

in Table 6. According to the second row, we can conclude
that only training with the KL loss brings little performance
gain. Introducing the label uncertainty generated by GLENet
into the KL Loss contributes 0.75%, 0.51%, and 0.3% im-
provements on the APs of easy, moderate, and hard classes,
respectively, which demonstrates its regularization eÔ¨Äect on
KLD-loss (Eq. 9) and its ability to estimate more reliable
uncertainty statistics of bounding box labels. The proposed
UAQE module in the probabilistic detection head boosts the
easy, moderate, and hard APs by 0.25%, 0.19% and 0.15%,
respectively, validating its eÔ¨Äectiveness in estimating the lo-
calization quality.

5.4.3 InÔ¨Çuence of Data Augmentation

To generate similar point cloud shapes with diverse ground-
truth bounding boxes during training of GLENet, we pro-
posed an occlusion data augmentation strategy and gener-
ated more incomplete point clouds while keeping the bound-
ing boxes unchanged (see Fig. 8). As listed in Table 7, it
can be seen that the occlusion data augmentation eÔ¨Äectively
enhances the performance of GLENet and the downstream
detection task. Besides, the eÔ¨Äectiveness of the ùêø ùëÅ ùêøùêø met-
ric is also validated, which is proposed to evaluate GLENet
and select optimal conÔ¨Ågurations to generate reliable label
uncertainty.

5.4.4 Conditional Analysis

To Ô¨Ågure out in what cases our method improves the base de-
tector most, we evaluated GLENet-VR on diÔ¨Äerent occlusion
levels and distance ranges. As shown in Table 8, compared
with the baseline, our method mainly improves on the heavily

Table 7: Ablation study on occlusion augmentation tech-
niques in GLENet, in which we report the ùêø ùëÅ ùêøùêø for evalua-
tion of GLENet and the 3D average precisions of 40 sampling
recall points for evaluation of downstream detectors.

Occlusion
√ó
(cid:88)

ùêøùëÅ ùêøùêø ‚Üì
230.1
91.5

Easy
93.21
93.49

Mod.
85.86
86.10

Hard
83.35
83.56

Table 8: Comparison on diÔ¨Äerent occlusion levels and dis-
tance rangesa, evaluated by the 3D Average Precision (AP)
calculated with 40 sampling recall positions on the KITTI
val set.

Methods

Voxel R-CNN
(Deng et al., 2021)

GLENet-VR
(Ours)

Improvement

Occlusionb

Distance

0
1
2
0-20m
20-40m
40m-Inf

92.35
76.91
54.32
96.42
83.82
38.86

93.51
78.64
56.93
96.69
86.87
39.82

+1.16
+1.73
+2.61
+0.27
+3.05
+0.96

a The results include separate APs for objects belonging to diÔ¨Äerent
occlusion levels and APs for moderate vehicle class in diÔ¨Äerent
distance ranges.

b DeÔ¨Ånition of occlusion levels: levels 0, 1 and 2 correspond to fully
visible samples, partly occluded samples, and samples diÔ¨Écult to
see respectively.

Table 9: Inference time comparison for diÔ¨Äerent baselines on
the KITTI dataset.

Method
SECOND Yan et al. (2018b)
GLENet-S (Ours)
CIA-SSD Zheng et al. (2021a)
GLENet-C (Ours)
Voxel R-CNN Deng et al. (2021)
GLENet-VR (Ours)

FPS (Hz)
23.36
22.80
27.18
28.76
21.08
20.82

occluded and distant samples, which suÔ¨Äer from more seri-
ous boundary ambiguities of ground-truth bounding boxes.

5.4.5 Inference EÔ¨Éciency

We evaluated the inference speed of diÔ¨Äerent baselines with
a batch size of 1 on a desktop with Intel CPU E5-2560
@ 2.10 GHz and NVIDIA GeForce RTX 2080Ti GPU. As
shown in Table 9, our approach doesn‚Äôt signiÔ¨Åcantly increase
the computational overhead. Particularly, GLENet-VR only
takes 0.6 more ms than the base Voxel R-CNN, since the
number of candidates for the input of var voting is relatively
small in two-stage detectors.

14

Yifan Zhang et al.

Fig. 10: Visual comparison of the results by GLENet-VR and Voxel R-CNN on the KITTI dataset. The ground-truth, true
positive and false positive bounding boxes are visualized in red, green and yellow, respectively, on both the point cloud and
image. Best viewed in color.

5.5 Comparison of Visual Results

improvement and produced state-of-the-art performance on
both KITTI and Waymo datasets.

Fig. 10 visualizes the detection results of our GLENet-VR
and the baseline Voxel R-CNN on the KITTI val set, where
it can be seen that our GLENet-VR obtains better detection
results with fewer false-positive bounding boxes and fewer
missed heavily occluded and distant objects than Voxel R-
CNN. We also compared detection results of SECOND and
GLENet-S on the Waymo validation set in Fig. 11, where it
can be seen that compared with SECOND (Yan et al., 2018b),
our GLENet-S has fewer false predictions and achieves more
accurate localization.

6 Conclusion

We presented a general and uniÔ¨Åed deep learning-based
paradigm for modeling 3D object-level label uncertainty.
Technically, we proposed GLENet, adapted from the learn-
ing framework of CVAE, to capture one-to-many relation-
ships between incomplete point cloud objects and poten-
tially plausible bounding boxes. As a plug-and-play compo-
nent, GLENet can generate reliable label uncertainty statis-
tics that can be conveniently integrated into various 3D de-
tection pipelines to build powerful probabilistic detectors.
We veriÔ¨Åed the eÔ¨Äectiveness and universality of our method
by incorporating the proposed GLENet into several exist-
ing deep 3D object detectors, which demonstrated consistent

References

Bowman S, Vilnis L, Vinyals O, Dai A, Jozefowicz R, Bengio
S (2016) Generating sentences from a continuous space.
In: Proceedings of The 20th SIGNLL Conference on Com-
putational Natural Language Learning, pp 10‚Äì21

Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A,
Zagoruyko S (2020) End-to-end object detection with
transformers. In: European conference on computer vi-
sion, pp 213‚Äì229

Chen X, Ma H, Wan J, Li B, Xia T (2017) Multi-view 3d
object detection network for autonomous driving. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 1907‚Äì1915

Choi J, Chun D, Kim H, Lee HJ (2019) Gaussian yolov3:
An accurate and fast object detector using localization
uncertainty for autonomous driving. In: Proceedings of
the IEEE International Conference on Computer Vision,
vol 2019-October, pp 502‚Äì511

Delany SJ, Segata N, Mac Namee B (2012) ProÔ¨Åling in-
stances in noise reduction. Knowledge-Based Systems
31:28‚Äì40

Deng J, Shi S, Li P, Zhou W, Zhang Y, Li H (2021) Voxel
r-cnn: Towards high performance voxel-based 3d object

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

15

(a) SECOND

(b) GLENet-S (Ours)

Fig. 11: Visual comparison of the results by SECOND and GLENet-S on the Waymo val set. The ground-truth, true positive
and false positive bounding boxes are visualized in red, green and yellow, respectively. Best viewed in color and zoom in for
more details. Additional NMS is conducted for better visualization.

detection. In: Proceedings of the AAAI Conference on
ArtiÔ¨Åcial Intelligence, pp 1201‚Äì1209

Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 3354‚Äì3361

Feng D, Rosenbaum L, Dietmayer K (2018) Towards safe au-
tonomous driving: Capture uncertainty in the deep neural
network for lidar 3d vehicle detection. In: IEEE Confer-
ence on Intelligent Transportation Systems, pp 3266‚Äì3273
Feng D, Rosenbaum L, Timm F, Dietmayer K (2019) Lever-
aging heteroscedastic aleatoric uncertainties for robust
real-time lidar 3d object detection. In: IEEE Intelligent
Vehicles Symposium, pp 1280‚Äì1287

Garcia LP, S√°ez JA, Luengo J, Lorena AC, de Carvalho AC,
Herrera F (2015) Using the one-vs-one decomposition to
improve the performance of class noise Ô¨Ålters via an ag-
gregation strategy in multi-class classiÔ¨Åcation problems.
Knowledge-Based Systems 90:153‚Äì164

Geiger A, Lenz P, Urtasun R (2012) Are we ready for au-
tonomous driving? the kitti vision benchmark suite. In:

Goyal A, Sordoni A, C√¥t√© MA, Ke N, Bengio Y (2017) Z-
forcing: Training stochastic recurrent networks. In: Ad-
vances in Neural Information Processing Systems, pp
6714‚Äì6724

Harakeh A, Smart M, Waslander SL (2020) Bayesod: A
bayesian approach for uncertainty estimation in deep
object detectors. In: IEEE International Conference on
Robotics and Automation, IEEE, pp 87‚Äì93

He C, Zeng H, Huang J, Hua XS, Zhang L (2020) Structure
aware single-stage 3d object detection from point cloud.
In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 11870‚Äì11879

He Y, Zhu C, Wang J, Savvides M, Zhang X (2019) Bound-
ing box regression with uncertainty for accurate object de-
tection. In: Proceedings of the IEEE/CVF conference on

16

Yifan Zhang et al.

computer vision and pattern recognition, pp 2888‚Äì2897
Huang T, Liu Z, Chen X, Bai X (2020) Epnet: Enhanc-
ing point features with image semantics for 3d object de-
tection. In: European Conference on Computer Vision,
Springer, pp 35‚Äì52

Kingma D, Ba J (2015) Adam: A method for stochastic opti-
mization. In: International Conference on Learning Rep-
resentations, pp 1‚Äì15

Kingma DP, Welling M (2014) Auto-encoding variational
bayes. In: International Conference on Learning Repre-
sentations, pp 1‚Äì14

Lang A, Vora S, Caesar H, Zhou L, Yang J, Beƒ≥bom O (2019)
Pointpillars: Fast encoders for object detection from point
clouds. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp 12689‚Äì12697
Li B, Sun Z, Guo Y (2019) Supervae: Superpixelwise vari-
ational autoencoder for salient object detection. In: Pro-
ceedings of the AAAI Conference on ArtiÔ¨Åcial Intelli-
gence, vol 33, pp 8569‚Äì8576

Li J, Song Y, Zhang H, Chen D, Shi S, Zhao D, Yan R
(2018) Generating classical chinese poems via conditional
variational autoencoder and adversarial training. In: Pro-
ceedings of the 2018 conference on empirical methods in
natural language processing, pp 3890‚Äì3900

Li X, Wang W, Wu L, Chen S, Hu X, Li J, Tang J, Yang
J (2020) Generalized focal loss: Learning qualiÔ¨Åed and
distributed bounding boxes for dense object detection. In:
Advances in Neural Information Processing Systems, pp
21002‚Äì21012

Liang M, Yang B, Chen Y, Hu R, Urtasun R (2019) Multi-
task multi-sensor fusion for 3d object detection. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 7345‚Äì7353

Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY,
Berg AC (2016) Ssd: Single shot multibox detector. In:
European conference on computer vision, Springer, pp
21‚Äì37

Luengo J, Shim SO, Alshomrani S, Altalhi A, Herrera F
(2018) Cnc-nos: Class noise cleaning by ensemble Ô¨Ålter-
ing and noise scoring. Knowledge-Based Systems 140:27‚Äì
49

Mao J, Niu M, Bai H, Liang X, Xu H, Xu C (2021a) Pyramid
r-cnn: Towards better performance and adaptability for
3d object detection. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp 2723‚Äì
2732

Mao J, Xue Y, Niu M, Bai H, Feng J, Liang X, Xu H, Xu
C (2021b) Voxel transformer for 3d object detection. In:
Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp 3164‚Äì3173

Meyer G, Thakurdesai N (2020) Learning an uncertainty-
aware object detector for autonomous driving. In: IEEE
International Conference on Intelligent Robots and Sys-

tems, pp 10521‚Äì10527

Meyer GP, Laddha A, Kee E, Vallespi-Gonzalez C, Welling-
ton CK (2019) Lasernet: An eÔ¨Écient probabilistic 3d ob-
ject detector for autonomous driving. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 12677‚Äì12686

Mousavian A, Eppner C, Fox D (2019) 6-dof graspnet: Vari-
ational grasp generation for object manipulation. In: Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision, pp 2901‚Äì2910

Najibi M, Lai G, Kundu A, Lu Z, Rathod V, Funkhouser
T, Pantofaru C, Ross D, Davis L, Fathi A (2020) Dops:
Learning to detect 3d objects and predict their 3d shapes.
In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 11910‚Äì11919

Nash C, Williams C (2017) The shape variational autoen-
coder: A deep generative model of part-segmented 3d ob-
jects. Computer Graphics Forum 36(5):1‚Äì12

Northcutt C, Jiang L, Chuang I (2021) ConÔ¨Ådent learning:
Estimating uncertainty in dataset labels. Journal of ArtiÔ¨Å-
cial Intelligence Research 70:1373‚Äì1411

Painchaud N, Skandarani Y, Judge T, Bernard O, Lalande
A, Jodoin PM (2020) Cardiac segmentation with strong
anatomical guarantees. IEEE transactions on medical
imaging 39(11):3703‚Äì3713

Pang S, Morris D, Radha H (2020) Clocs: Camera-lidar ob-
ject candidates fusion for 3d object detection. In: IEEE
International Conference on Intelligent Robots and Sys-
tems, IEEE, pp 10386‚Äì10393

Qi CR, Su H, Mo K, Guibas LJ (2017) Pointnet: Deep learn-
ing on point sets for 3d classiÔ¨Åcation and segmentation.
In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 652‚Äì660

Qi CR, Liu W, Wu C, Su H, Guibas LJ (2018) Frustum
pointnets for 3d object detection from rgb-d data. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 918‚Äì927

Sharma S, Varigonda PT, Bindal P, Sharma A, Jain A (2019)
Monocular 3d human pose estimation by generation and
ordinal ranking. In: Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pp 2325‚Äì2334
Sheng H, Cai S, Liu Y, Deng B, Huang J, Hua XS, Zhao MJ
(2021) Improving 3d object detection with channel-wise
transformer. In: Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pp 2743‚Äì2752
Shi S, Wang X, Li H (2019) Pointrcnn: 3d object proposal
generation and detection from point cloud. In: Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition, pp 770‚Äì779

Shi S, Guo C, Jiang L, Wang Z, Shi J, Wang X, Li H (2020a)
Pv-rcnn: Point-voxel feature set abstraction for 3d object
detection. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp 10529‚Äì10538

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

17

Shi S, Wang Z, Shi J, Wang X, Li H (2020b) From points
to parts: 3d object detection from point cloud with part-
aware and part-aggregation network. IEEE Transactions
on Pattern Analysis and Machine Intelligence 43(8):2647‚Äì
2664

Shi W, Rajkumar R (2020a) Point-gnn: Graph neural network
for 3d object detection in a point cloud. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 1708‚Äì1716

Shi W, Rajkumar R (2020b) Point-gnn: Graph neural network
for 3d object detection in a point cloud. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 1711‚Äì1719

Smith LN (2017) Cyclical learning rates for training neural
networks. In: Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision, IEEE, pp
464‚Äì472

Sohn K, Lee H, Yan X (2015) Learning structured output
representation using deep conditional generative models.
In: Advances in Neural Information Processing Systems,
pp 3483‚Äì3491

Sun P, Kretzschmar H, Dotiwalla X, Chouard A, Patnaik V,
Tsui P, Guo J, Zhou Y, Chai Y, Caine B, Vasudevan V, Han
W, Ngiam J, Zhao H, Timofeev A, Ettinger S, Krivokon M,
Gao A, Joshi A, Zhang Y, Shlens J, Chen Z, Anguelov D
(2020) Scalability in perception for autonomous driving:
Waymo open dataset. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pp 2443‚Äì2451

Tan M, Pang R, Le QV (2020) EÔ¨Écientdet: Scalable and ef-
Ô¨Åcient object detection. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pp 10781‚Äì10790

Varamesh A, Tuytelaars T (2020) Mixture dense regression
for object detection and human pose estimation. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 13086‚Äì13095

Vora S, Lang AH, Helou B, Beƒ≥bom O (2020) Pointpainting:
Sequential fusion for 3d object detection. In: Proceedings
of the IEEE/CVF conference on computer vision and pat-
tern recognition, pp 4604‚Äì4612

Wang T, Wan X (2019) T-cvae: Transformer-based condi-
tioned variational autoencoder for story completion. In:
International Joint Conference on ArtiÔ¨Åcial Intelligence,
pp 5233‚Äì5239

Wang Z, Feng D, Zhou Y, Rosenbaum L, Timm F, Diet-
mayer K, Tomizuka M, Zhan W (2020) Inferring spa-
tial uncertainty in object detection. In: IEEE International
Conference on Intelligent Robots and Systems, IEEE, pp
5792‚Äì5799

Xu Q, Zhou Y, Wang W, Qi CR, Anguelov D (2021) Spg:
Unsupervised domain adaptation for 3d object detection
via semantic point generation. In: Proceedings of the

IEEE/CVF International Conference on Computer Vision,
pp 15446‚Äì15456

Yan X, Yang J, Sohn K, Lee H (2016) Attribute2image:
Conditional image generation from visual attributes. In:
European conference on computer vision, Springer, pp
776‚Äì791

Yan X, Rastogi A, Villegas R, Sunkavalli K, Shechtman
E, Hadap S, Yumer E, Lee H (2018a) Mt-vae: Learning
motion transformations to generate multimodal human dy-
namics. In: European conference on computer vision, pp
265‚Äì281

Yan X, Gao J, Li J, Zhang R, Li Z, Huang R, Cui S (2021)
Sparse single sweep lidar point cloud segmentation via
learning contextual shape priors from scene completion.
In: Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence, vol 35, pp 3101‚Äì3109

Yan Y, Mao Y, Li B (2018b) Second: Sparsely embedded

convolutional detection. Sensors 18(10):3337

Yang B, Luo W, Urtasun R (2018) Pixor: Real-time 3d ob-
ject detection from point clouds. In: Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition, pp 7652‚Äì7660

Yang Z, Sun Y, Liu S, Shen X, Jia J (2019) Std: Sparse-to-
dense 3d object detector for point cloud. In: Proceedings
of the IEEE/CVF International Conference on Computer
Vision, pp 1951‚Äì1960

Yang Z, Sun Y, Liu S, Jia J (2020) 3dssd: Point-based 3d sin-
gle stage object detector. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp
11040‚Äì11048

Yi L, Zhao W, Wang H, Sung M, Guibas L (2019) Gspn: Gen-
erative shape proposal network for 3d instance segmen-
tation in point cloud. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pp 3942‚Äì3951

Yoo JH, Kim Y, Kim J, Choi JW (2020) 3d-cvf: Generating
joint camera and lidar features using cross-view spatial
feature fusion for 3d object detection. In: European Con-
ference on Computer Vision, Springer, pp 720‚Äì736

Zhang B, Xiong D, Su J, Duan H, Zhang M (2016) Varia-
tional neural machine translation. In: Proceedings of the
2016 conference on empirical methods in natural language
processing, Association for Computational Linguistics,
Austin, Texas, pp 521‚Äì530

Zhang C, Bengio S, Hardt M, Recht B, Vinyals O (2021) Un-
derstanding deep learning (still) requires rethinking gen-
eralization. Communications of the ACM 64(3):107‚Äì115
Zhang J, Fan DP, Dai Y, Anwar S, Saleh FS, Zhang T, Barnes
N (2020) Uc-net: Uncertainty inspired rgb-d saliency de-
tection via conditional variational autoencoders. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 8582‚Äì8591

18

Yifan Zhang et al.

Zhao T, Zhao R, Eskenazi M (2017) Learning discourse-
level diversity for neural dialog models using conditional
variational autoencoders. In: ACL 2017 - 55th Annual
Meeting of the Association for Computational Linguistics,
Proceedings of the Conference, pp 654‚Äì664

Zheng W, Tang W, Chen S, Jiang L, Fu CW (2021a) Cia-
ssd: ConÔ¨Ådent iou-aware single-stage object detector from
point cloud. In: Proceedings of the AAAI Conference on
ArtiÔ¨Åcial Intelligence, pp 3555‚Äì3562

Zheng W, Tang W, Jiang L, Fu CW (2021b) Se-ssd: Self-
ensembling single-stage object detector from point cloud.
In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 14494‚Äì14503

Zhou Y, Tuzel O (2018) Voxelnet: End-to-end learning for
point cloud based 3d object detection. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 4490‚Äì4499

Zhou Y, Sun P, Zhang Y, Anguelov D, Gao J, Ouyang T, Guo
J, Ngiam J, Vasudevan V (2020) End-to-end multi-view
fusion for 3d object detection in lidar point clouds. In:
Conference on Robot Learning, PMLR, pp 923‚Äì932

