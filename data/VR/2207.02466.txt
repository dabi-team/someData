GLENet: Boosting 3D Object Detectors with Generative Label
Uncertainty Estimation

Yifan Zhang Â· QÄ³ian Zhang Â· Zhiyu Zhu Â· Junhui Hou Â· Yixuan Yuan

2
2
0
2

l
u
J

0
2

]

V
C
.
s
c
[

2
v
6
6
4
2
0
.
7
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract The inherent ambiguity in ground-truth annota-
tions of 3D bounding boxes caused by occlusions, signal
missing, or manual annotation errors can confuse deep 3D
object detectors during training, thus deteriorating the de-
tection accuracy. However, existing methods overlook such
issues to some extent and treat the labels as deterministic. In
this paper, we formulate the label uncertainty problem as the
diversity of potentially plausible bounding boxes of objects,
then propose GLENet, a generative framework adapted from
conditional variational autoencoders, to model the one-to-
many relationship between a typical 3D object and its po-
tential ground-truth bounding boxes with latent variables.
The label uncertainty generated by GLENet is a plug-and-
play module and can be conveniently integrated into existing
deep 3D detectors to build probabilistic detectors and super-
vise the learning of the localization uncertainty. Besides, we
propose an uncertainty-aware quality estimator architecture
in probabilistic detectors to guide the training of IoU-branch
with predicted localization uncertainty. We incorporate the
proposed methods into various popular base 3D detectors and
demonstrate signiï¬cant and consistent performance gains on
both KITTI and Waymo benchmark datasets. Especially, the
proposed GLENet-VR outperforms all published LiDAR-
based approaches by a large margin and ranks 1ğ‘ ğ‘¡ among
single-modal methods on the challenging KITTI test set. We

Yifan Zhang, QÄ³ian Zhang, Zhiyu Zhu, and Junhui Hou
Department of Computer Science, City University of Hong Kong.
E-mail:
c}@my.cityu.edu.hk; jh.hou@cityu.edu.hk;

{yzhang3362-c,

qÄ³izhang3-c,

zhiyuzhu2-

Yixuan Yuan
Department of Electrical Engineering, City University of Hong Kong.
E-mail: yxyuan.ee@cityu.edu.hk

This project was partly supported by the Hong Kong Research Grants
Council under Grants 11202320 and 11218121, and partly by the
Natural Science Foundation of China under Grant 61871342.

will make the source code and pre-trained models publicly
available.

Keywords 3D object detection Â· label uncertainty Â·
conditional variational autoencoders Â· probabilistic object
detection Â· 3D point cloud

1 Introduction

As one of the most practical application scenarios of com-
puter vision, 3D object detection has been attracting much
academic and industrial attention in the current deep learning
era with the rise of autonomous driving and the emergence
of large-scale annotated datasets (e.g., KITTI (Geiger et al.,
2012), and Waymo (Sun et al., 2020)).

In the current community, despite the proliferation of
various deep learning-based 3D detection pipelines, it is
observed that mainstream 3D object detectors are typically
designed as deterministic models, without considering the
critical issue of the ambiguity of annotated ground-truth
labels. However, diï¬€erent aspects of ambiguity/inaccuracy
inevitably exist in the ground-truth annotations of object-
level bounding boxes, which may signiï¬cantly inï¬‚uence the
overall learning process of such deterministic detectors. For
example, in the data collection phase, raw point clouds can
be highly incomplete due to the intrinsic properties of Li-
DAR sensors as well as uncontrollable environmental occlu-
sion. Moreover, in the data labeling phase, ambiguity nat-
urally occurs when diï¬€erent human annotators subjectively
estimate object shapes and locations from 2D images and
partial 3D points. To facilitate intuitive understandings, we
provide typical examples in Fig. 1, from which we can ob-
serve that an incomplete LiDAR observation can correspond
to multiple potentially plausible labels and objects with sim-

 
 
 
 
 
 
2

Yifan Zhang et al.

Fig. 1: (a) Given an object with an incomplete LiDAR observation, there may exist multiple potentially plausible ground-truth
bounding boxes with varying sizes and shapes. (b) Ambiguity and inaccuracy can be inevitable in the labeling process when
annotations are derived from 2D images and partial points. In the given cases, similar point clouds of the car category with
only the rear part can be annotated with diï¬€erent ground-truth boxes of varying lengths.

Fig. 2: Illustration of two diï¬€erent learning paradigms of
probabilistic object detectors. (a) Methods that adopt proba-
bilistic modeling in the detection head but essentially still ig-
nore the issue of ambiguity in ground-truth bounding boxes.
(b) Methods that explicitly estimate ground-truth bounding
box distributions to be used as more reliable supervision
signals.

ilar LiDAR observation can be annotated with signiï¬cantly
varying bounding boxes.

Motivated by the afore-mentioned phenomena, there also
exists another family of probabilistic detectors that explicitly
consider the potential inï¬‚uence of label ambiguity. Conclu-
sively, these methods can be categorized into two paradigms,
as illustrated in Fig. 2. The ï¬rst paradigm of learning frame-
works (He et al., 2019; Meyer et al., 2019; Feng et al., 2018,
2019) tend to output the probabilistic distribution of bound-
ing boxes, instead of directly regressing deï¬nite box coor-
dinates in a deterministic fashion. For example, under the
pre-assumption of Gaussian distribution, the detection head
accordingly predicts mean and variance of the distribution.
To supervise such probabilistic models, these works simply
treat ground-truth bounding boxes as the Dirac delta distribu-
tion, after which KL divergence is applied between the esti-
mated distributions and ground-truths. Obviously, the major

limitation of these methods lies in that they fail to essentially
address the problem of label ambiguity, since the ground-
truth bounding boxes are still considered as deterministic
with zero uncertainty (i.e., modeled as a Dirac delta func-
tion). To this end, the second paradigm of learning frame-
works attempts to quantify label uncertainty derived from
some simple heuristics (Meyer and Thakurdesai (2020)) or
Bayes (Wang et al. (2020)), such that the detectors can be
supervised under more reliable bounding box distributions.
However, it is not surprising that these approaches still can-
not produce satisfactory label uncertainty estimation results
due to insuï¬ƒcient modeling capacity. In general, this line
of works is still at its initial stage with very limited number
of studies, despite its greater potential in generating higher-
quality label uncertainty estimation in a data-driven manner.

Architecturally, this work follows the second type of de-
sign philosophy, where we particularly customize a powerful
deep learning-based label uncertainty quantiï¬cation frame-
work to enhance the reliability of the estimated ground-truth
bounding box distributions. Technically, we formulate the la-
bel uncertainty problem as the diversity of potentially plau-
sible bounding boxes and explicitly model the one-to-many
relationship between a typical 3D object and its potentially
plausible ground-truth boxes in a learning-based framework.
Technically, we propose GLENet, a novel deep generative
network adapted from conditional variational auto-encoders
(CVAE), which introduces a latent variable to capture the
distribution over potentially plausible bounding boxes of
point cloud objects. During inference, we sample latent vari-
ables multiple times to generate diverse bounding boxes (see
Fig. 3), the variance of which is taken as label uncertainty
to guide the learning of localization uncertainty estimation

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

3

Fig. 3: Illustration of multiple potentially plausible bounding boxes from GLENet by sampling latext variables multiple times.
The point cloud, annotated ground-truth boxes and predictions of GLENet are colored in black, red and green, respectively.
GLENet produces diverse predictions for objects represented with sparse point clouds and incomplete outlines, and consistent
bounding boxes for objects with high-quality point clouds. The variance of the multiple predictions by GLENet is used to
estimate the uncertainty of the annotated ground-truth bounding boxes.

in the downstream detection task. Besides, based on the ob-
servation that detection results with low localization uncer-
tainty in probabilistic detectors tend to have accurate actual
localization quality (see Section 4.2), we further propose
uncertainty-aware quality estimator (UAQE), which facili-
tates the training of the IoU-branch with the localization
uncertainty estimation.

To demonstrate our eï¬€ectiveness and universality, we
integrate GLENet into several popular 3D object detection
frameworks to build powerful probabilistic detectors. Exper-
iments on KITTI (Geiger et al., 2012) and Waymo (Sun et al.,
2020) datasets demonstrate that our method can bring con-
sistent performance gains and achieve the current state-of-
the-art. Particularly, the proposed GLENet-VR surpasses all
published single-modal detection methods by a large margin
and ranks 1ğ‘ ğ‘¡ among all published LiDAR-based approaches
on the highly competitive KITTI 3D detection benchmark on
March 29ğ‘¡ â„, 20221.

We summarize the main contributions of this paper as

follows.

â€“ We are the ï¬rst to formulate the 3D label uncertainty
problem as the diversity of potentially plausible bounding
boxes of objects. To capture the one-to-many relationship
between a typical 3D object and the potentially plausible
ground-truth bounding boxes, we present a deep gen-
erative model named GLENet. Besides, we introduce a
general and uniï¬ed deep learning-based paradigm, in-
cluding the network structure, loss function, evaluation
metric, etc.

1 www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d

â€“ Inspired by the strong correlation between the localiza-
tion quality and the predicted uncertainty in probabilistic
detectors, we propose UAQE to facilitate the training of
the IoU-branch.

The remainder of the paper is organized as follows. Sec-
tion 2 reviews existing works on LiDAR-based detectors and
label uncertainty estimation methods. In Section 3, we ex-
plicitly formulate the label uncertainty estimation problem
from the probabilistic distribution perspective, followed by
the technical implementation of GLENet. In Section 4, we
introduce a uniï¬ed way of integrating the label uncertainty
statistics predicted by GLENet into the existing 3D object
detection frameworks to build more powerful probabilistic
detectors, as well as some theoretical analysis. In Section 5,
we conduct the experiments on the KITTI dataset and the
Waymo Open dataset to demonstrate the eï¬€ectiveness of our
method in enhancing existing 3D detectors and the ablation
study to analyze the eï¬€ect of diï¬€erent components. Finally,
Section 6 concludes this paper.

2 Related Work

2.1 LiDAR-based 3D Object Detection

Existing 3D object detectors can be classiï¬ed into two cat-
egories: single-stage and two-stage. For single-stage detec-
tors, Zhou and Tuzel (2018) proposed to convert raw point
clouds to regular volumetric representations and adopted
voxel-based feature encoding. Yan et al. (2018b) presented
a more eï¬ƒcient sparse convolution. Lang et al. (2019) con-
verted point clouds to sparse fake images using pillars. Shi

4

Yifan Zhang et al.

and Rajkumar (2020a) aggregated point information via a
graph structure. He et al. (2020) introduced point segmenta-
tion and center estimation as auxiliary tasks in the training
phase to enhance model capacity. Zheng et al. (2021a) con-
structed an SSFA module for robust feature extraction and
a multi-task head for conï¬dence rectiï¬cation, and proposed
DI-NMS for post-processing. For two-stage detectors, Shi
et al. (2020b) exploited a voxel-based network to learn the
additional spatial relationship between intra-object parts un-
der the supervision of 3D box annotations. Shi et al. (2019)
proposed to directly generate 3D proposals from raw point
clouds in a bottom-up manner, using semantic segmenta-
tion to valid point to regress detection boxes. The follow-
up work (Yang et al., 2019) further proposed PointsPool to
convert sparse proposal features to compact representations
and used spherical anchors to generate accurate proposals.
Shi et al. (2020a) utilized both point-based and voxel-based
methods to fuse multi-scale voxel and point features. Deng
et al. (2021) proposed voxel RoI pooling to extract RoI fea-
tures from coarse voxels.

Compared with 2D object detection, there are more se-
rious boundary ambiguity problems in 3D object detection
due to occlusion and signal miss. Studies, such as SPG (Xu
et al., 2021), try to use point cloud completion methods to
restore full shape of objects and improve the detection per-
formance (Yan et al., 2021; Najibi et al., 2020). However,
itâ€™s non-trivial to generate complete and precise shapes with
incomplete point clouds only.

2.2 Probabilistic 3D Object Detector

There are two types of uncertainty in deep learning predic-
tions. A type of uncertainty, called aleatoric uncertainty, is
caused by the inherent noise in observational data, which
cannot be eliminated. The other type is called epistemic Un-
certainty or model uncertainty, which is caused by incom-
plete training and can be alleviated with more training data.
Most existing state-of-the-art 2D (Liu et al., 2016; Tan et al.,
2020; Carion et al., 2020) and 3D (Shi et al., 2020b) ob-
ject detectors produce a deterministic box with a conï¬dence
score for each detection. While the probability score repre-
sents the existence and semantic conï¬dence, it cannot reï¬‚ect
the uncertainty about predicted localization well. By con-
trast, probabilistic object detectors (He et al., 2019; Meyer
et al., 2019; Li et al., 2020; Varamesh and Tuytelaars, 2020)
estimate the probabilistic distribution of predicted bound-
ing boxes rather than take them as deterministic results. For
example, He et al. (2019) and Choi et al. (2019) modeled
the predicted boxes as Gaussian distributions, the variance
of which can indicate the localization uncertainty and is
predicted with additional layer in the detection head. It in-
troduces the KL Loss between the predicted Gaussian dis-
tribution and the ground-truth bounding boxes modeled as

a Dirac delta function, so the regression branch is expected
to output a larger variance and get a smaller loss for inac-
curate localization estimation for the cases with ambiguous
boundaries. Unlike common practice to model the box as a
Gaussian distribution, Harakeh et al. (2020) learned the oï¬€-
diagonal elements of the covariance matrix of a multivariate
Gaussian distribution as uncertainty estimation. Meyer et al.
(2019) proposed a probabilistic 3D object detector model-
ing the distribution of bounding box corners as a Laplacian
distribution.

However, most probabilistic detectors take the ground-
truth bounding box as a deterministic Dirac delta distribution
and ignore the ambiguity in ground truth. Therefore, the lo-
calization variance is actually learned in an unsupervised
manner, which may result in sub-optimal localization pre-
cision and erratic training (see our theoretical analysis in
Section 4.1).

2.3 Label Uncertainty Estimation

Label noise (or uncertainty) is a common problem in real-
world datasets and could seriously aï¬€ect the performance
of supervised learning algorithms. As the neural network is
prone to overï¬t to even complete random noise (Zhang et al.
(2021)), it is important to prevent the network from over-
ï¬tting noisy labels. An obvious solution is to consider the
label of a misclassiï¬ed sample to be uncertain and remove
the samples (Delany et al., 2012). Garcia et al. (2015) used
a soft voting approach to approximate a noise level for each
sample based on the aggregation of the noise degree predic-
tion calculated for a set of binary classiï¬ers. Luengo et al.
(2018) extended this work by correcting the label when most
classiï¬ers predict the same label for noisy samples. Conï¬-
dent Learning Northcutt et al. (2021) estimated uncertainty
in dataset labels by estimating the joint distribution of noisy
labels and true labels. However, the above studies mainly
focus on the image classiï¬cation task.

There only exists a limited number of previous works
focusing on quantifying uncertainty statistics of annotated
ground-truth bounding boxes. Meyer and Thakurdesai (2020)
proposed to model label uncertainty by the IoU between
the label bounding box and the corresponding convex hull
of the aggregated LiDAR observations. However, it is non-
learning-based and thus has limited modeling capacity. Be-
sides, it only produces uncertainty of the ground-truth box as
a whole instead of each dimension. Wang et al. (2020) pro-
posed a Bayes method to estimate label noises by quantifying
the matching degree of point clouds for the given boundary
box with the Gaussian Mixture Model. However, its assump-
tion of conditional probabilistic independence between point
clouds is often untenable in practice. Diï¬€erently, we formu-
late label uncertainty as diversity of potentially plausible
bounding boxes. There may be some objects with few points

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

5

that exactly match the learned surface points of correspond-
ing labeled Bbox, so the label is considered by (Wang et al.
(2020)) to be deterministic. But for an object with sparse
point clouds, our GLENet will output diï¬€erent and plausible
Bboxes and further estimate high label uncertainty based on
them, regardless of whether points match the given label. In
general, Wang et al. (2020) used the Bayesian paradigm to
estimate the correctness of the annotated box as the label
uncertainty, while our method formulates it as the diversity
of potentially plausible bounding boxes and predicts it by
GLENet.

2.4 Conditional Variational Auto-Encoder

The variational auto-encoder (VAE) (Kingma and Welling,
2014) has been widely used in image and shape genera-
tion tasks (Yan et al., 2016; Nash and Williams, 2017). It
transforms natural samples into a distribution where latent
variables can be drawn and passed to a decoder network to
generate diverse samples. Sohn et al. (2015) proposed condi-
tional variational auto-encoder (CVAE) extending the VAE
with an extra condition to supervise the generative process.
In the NLP ï¬eld, VAE has been widely applied to many
text generation tasks, such as dialogue response (Zhao et al.,
2017), machine translation (Zhang et al., 2016), story gen-
eration (Wang and Wan, 2019), and poem composing (Li
et al., 2018). VAE and CVAE have also been applied in com-
puter vision tasks, like image generation (Yan et al., 2016),
human pose estimation (Sharma et al., 2019), medical im-
age segmentation (Painchaud et al., 2020), salient object de-
tection (Li et al., 2019; Zhang et al., 2020), and modeling
human motion dynamics (Yan et al., 2018a). Recently, VAE
and CVAE algorithms have also been applied extensively
to applications of 3D point clouds, such as generating grasp
poses (Mousavian et al., 2019) and instance segmentation (Yi
et al., 2019).

Inspired by CVAE for generating diverse reasonable re-
sponses in dialogue systems, we propose GLENet adapted
from CVAE to capture the one-to-many relationship between
objects with incomplete point cloud and the potentially plau-
sible ground-truth bounding boxes. To the best of our knowl-
edge, we are the ï¬rst to employ CVAE in 3D object detection
to model label uncertainty.

3 Proposed Label Uncertainty Estimation

As aforementioned, the ambiguity of annotated ground-truth
labels widely exists in 3D object detection scenarios and has
adverse eï¬€ects on the deep model learning process, which
is not well addressed or even completely ignored by pre-
vious works. To this end, we propose GLENet, a generic

and uniï¬ed deep learning framework that generates label un-
certainty by modeling the one-to-many relationship between
point cloud objects and potentially plausible bounding box
labels. Then the variance of the multiple outputs of GLENet
for a single object is computed as the label uncertainty, which
is extended as an auxiliary regression objective to enhance
the performance of the downstream 3D object detection task.

3.1 Problem Formulation

Let ğ¶ = {ğ‘ğ‘– }ğ‘›
ğ‘–=1 be a set of ğ‘› observed LiDAR points belong-
ing to an object, where ğ‘ğ‘– âˆˆ R3 is a 3D point represented
with spatial coordinates. Let ğ‘‹ be the annotated ground-
truth bounding box of ğ¶ parameterized by the center location
(ğ‘ ğ‘¥, ğ‘ ğ‘¦, ğ‘ğ‘§), the size ( length ğ‘™ , width ğ‘¤, and height â„), and
the orientation ğ‘Ÿ, i.e., ğ‘‹ = [ğ‘ ğ‘¥, ğ‘ ğ‘¦, ğ‘ğ‘§, ğ‘¤, ğ‘™, â„, ğ‘Ÿ] âˆˆ R7.

We formulate the uncertainty of the annotated ground-
truth label of an object as the diversity of potentially plausible
bounding boxes of the object, which could be quantitatively
measured with the variance of the distribution of the po-
tential bounding boxes. First, we model the distribution of
these potential boxes conditioned on point cloud ğ¶, denoted
as ğ‘(ğ‘‹ |ğ¶). Speciï¬cally, based on the Bayes theorem, we in-
troduce an intermediate variable ğ‘§ to write the conditional
distribution as

ğ‘(ğ‘‹ |ğ¶) =

âˆ«

ğ‘§

ğ‘(ğ‘‹ |ğ‘§, ğ¶) ğ‘(ğ‘§|ğ¶)ğ‘‘ğ‘§.

(1)

Then, with ğ‘(ğ‘‹ |ğ‘§, ğ¶) and ğ‘(ğ‘§|ğ¶) known, we can adopt a
Monte Carol method to get multiple bounding box predic-
tions by sampling ğ‘§ multiple times and approximate the vari-
ance of ğ‘(ğ‘‹ |ğ¶) with that of the sampled predictions.

In the following, we will introduce our learning-based
framework named GLENet to realize the estimation process.

3.2 Inference Process of GLENet

Fig. 4 (a) shows the ï¬‚owchart of GLENet parameterized
by neural parameters ğœƒ, which aims to predict ğ‘(ğ‘§|ğ¶) and
ğ‘(ğ‘‹ |ğ‘§, ğ¶). Speciï¬cally, under the assumption that the prior
distribution ğ‘(ğ‘§|ğ¶) subjects to a multivariate Gaussian dis-
tribution parameterized by(ğœ‡ğ‘§, ğœğ‘§), denoted as N (ğœ‡ğ‘§, ğœ2
ğ‘§ ),
we design a prior network, which is composed of Point-
Net (Qi et al., 2017) and additional MLP layers, from the
input point cloud ğ¶ to predict the values of (ğœ‡ğ‘§, ğœğ‘§). Then,
we employ a context encoder to embed the input point cloud
ğ¶ into a high dimensional feature space, leading to the geo-
metric feature representation ğ‘“ğ¶ , which is concatenated with
ğ‘§ sampled from N (ğœ‡ğ‘§, ğœ2
ğ‘§ ) and fed into a prediction network
composed of MLPs to regress the bounding box distribution
ğ‘(ğ‘‹ |ğ‘§, ğ¶), i.e., the localization, dimension and orientation
of the bounding box.

6

Yifan Zhang et al.

Fig. 4: The overall workï¬‚ow of GLENet. In the training phase, we learn parameters ğœ‡ and ğœ (resp. ğœ‡(cid:48) and ğœ(cid:48) ) of latent
variable ğ‘§ (resp. ğ‘§(cid:48)) through the prior network (resp. recognition network), after which a sample of ğ‘§(cid:48) and the corresponding
geometrical embedding produced by the context encoder are jointly exploited to estimate the bounding box distribution. In
the inference phase, we sample from the distribution of ğ‘§ multiple times to generate diï¬€erent bounding boxes, whose variance
we use as label uncertainty.

As empirically observed in various related domains (Goyal
et al., 2017), it could be diï¬ƒcult to make use of latent vari-
ables when the prediction network can generate a plausi-
ble output only using the suï¬ƒciently expressive features of
condition ğ¶. Therefore, we utilize a simpliï¬ed PointNet ar-
chitecture as the backbone of the context encoder to avoid
posterior collapse. We refer the readers to Section 5.1.3 for
the implementation details of these modules. In the following
sections, we also use ğ‘ ğœƒ (ğ‘§|ğ¶), ğ‘ ğœƒ (ğ‘‹ |ğ‘§, ğ¶), and ğ‘ ğœƒ (ğ‘‹ |ğ¶) to
denote the predictions of ğ‘(ğ‘§|ğ¶), ğ‘(ğ‘‹ |ğ‘§, ğ¶), and ğ‘(ğ‘‹ |ğ¶) by
GLENet, respectively.

3.3 Training Process of GLENet

3.3.1 Recognition Network

Given ğ¶ and its annotated bounding box ğ‘‹, we assume there
is a true posterior distribution ğ‘(ğ‘§|ğ‘‹, ğ¶). Thus, during train-
ing, we construct a recognition network parameterized by
network parameters ğœ™ (see Fig. 4 (b)) to learn an auxiliary
posterior distribution ğ‘ ğœ™ (ğ‘§(cid:48)|ğ‘‹, ğ¶) subjecting to a Gaussian
ğ‘§ ), to regularize ğ‘ ğœƒ (ğ‘§|ğ¶),
distribution, denoted as N (ğœ‡(cid:48)
i.e., ğ‘ ğœƒ (ğ‘§|ğ¶) should be close to ğ‘ ğœ™ (ğ‘§(cid:48)|ğ‘‹, ğ¶).

ğ‘§, ğœ(cid:48)2

Speciï¬cally, for the recognition network, we adopt the
same learning architecture as the prior network to gener-
ate point cloud embeddings, which are concatenated with
ground-truth bounding box information and fed into the sub-
sequent MLP layers to learn ğ‘ ğœ™ (ğ‘§(cid:48)|ğ‘‹, ğ¶). Moreover, to fa-
cilitate the learning process, we encode the information ğ‘‹
into oï¬€sets relative to predeï¬ned anchors, and then perform

normalization as:

ğ‘ğ‘”ğ‘¡
ğ‘¥
ğ‘‘ ğ‘

ğ‘¡ğ‘ğ‘¥ =

, ğ‘¡ğ‘ğ‘¦ =
ğ‘¤ğ‘”ğ‘¡
ğ‘¡ğ‘¤ = log
ğ‘¤ğ‘
ğ‘¡ğ‘Ÿ = sin(ğ‘Ÿ ğ‘”ğ‘¡ ),

ğ‘ğ‘”ğ‘¡
ğ‘§
â„ğ‘

,

ğ‘ğ‘”ğ‘¡
ğ‘¦
ğ‘‘ ğ‘

, ğ‘¡ğ‘ğ‘§ =
ğ‘™ğ‘”ğ‘¡
ğ‘™ ğ‘

, ğ‘¡ğ‘™ = log

, ğ‘¡â„ = log

â„ğ‘”ğ‘¡
â„ğ‘

,

(2)

where (ğ‘¤ğ‘, ğ‘™ ğ‘, â„ğ‘) is the size of the predeï¬ned anchor located
in the center of the point cloud, and ğ‘‘ ğ‘ = âˆšï¸(ğ‘™ ğ‘)2 + (ğ‘¤ğ‘)2
is the diagonal of the anchor box. We also take cos(ğ‘Ÿ) as
the additional input of the recognition network to handle the
issue of angle periodicity.

3.3.2 Objective Function

Following CAVE, we optimize GLENet via maximizing the
variational lower bound of the conditional log likelihood
ğ‘ ğœƒ (ğ‘‹ |ğ¶):

log ğ‘ ğœƒ (ğ‘‹ |ğ¶) â‰¥ ğ¸ğ‘ğœ™ (ğ‘§(cid:48) |ğ‘‹ ,ğ¶) [log ğ‘ ğœƒ (ğ‘‹ |ğ‘§, ğ¶)]âˆ’

ğ¾ ğ¿ (ğ‘ ğœ™ (ğ‘§(cid:48)|ğ‘‹, ğ¶)|| ğ‘ ğœƒ (ğ‘§|ğ¶)),

(3)

where ğ¸ğ‘ [ ğ‘] returns the expectation of ğ‘ on the distribution
of ğ‘, and ğ¾ ğ¿ (Â·) denotes KL-divergence.

Speciï¬cally, the ï¬rst term ğ¸ğ‘ğœ™ (ğ‘§(cid:48) |ğ‘‹ ,ğ¶) [log ğ‘ ğœƒ (ğ‘‹ |ğ‘§, ğ¶)]
enforces the prediction network to be able to restore ground-
truth bounding box from latent variables. Following (Yan
et al. (2018b)) and (Deng et al. (2021)), we explicitly deï¬ne
the bounding box reconstruction loss as

ğ¿ğ‘Ÿ ğ‘’ğ‘ = ğ¿ğ‘Ÿ ğ‘’ğ‘”

ğ‘Ÿ ğ‘’ğ‘ + ğœ†ğ¿ğ‘‘ğ‘–ğ‘Ÿ
ğ‘Ÿ ğ‘’ğ‘,

(4)

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

7

where ğ¿ğ‘Ÿ ğ‘’ğ‘”
ğ‘Ÿ ğ‘’ğ‘ denotes the Huber loss imposed on the prediction
and encoded regression targets as described in Eq. (2), and
ğ¿ğ‘‘ğ‘–ğ‘Ÿ
ğ‘Ÿ ğ‘’ğ‘ denotes the binary cross-entropy loss used for direction
classiï¬cation.

The second term ğ¾ ğ¿ (ğ‘ ğœ™ (ğ‘§(cid:48)|ğ‘‹, ğ¶)(cid:107) ğ‘ ğœƒ (ğ‘§|ğ¶)) is aimed
at regularizing the distribution of ğ‘§ by minimizing the KL-
divergence between ğ‘ ğœƒ (ğ‘§|ğ¶) and ğ‘ ğœ™ (ğ‘§(cid:48)|ğ‘‹, ğ¶). Since ğ‘ ğœƒ (ğ‘§|ğ¶)
and ğ‘ ğœ™ (ğ‘§|ğ‘‹, ğ¶) are re-parameterized as N (ğœ‡ğ‘§, ğœ2
ğ‘§ ) and
N (ğœ‡(cid:48)
ğ‘§ ) through the prior network and the recognition
network, respectively, we can explicitly deï¬ne the regular-
ization loss as:

ğ‘§, ğœ(cid:48)2

ğ¿ğ¾ ğ¿ (ğ‘ ğœ™ (ğ‘§|ğ‘‹, ğ¶)(cid:107) ğ‘ ğœƒ (ğ‘§|ğ¶)) = log

ğœ(cid:48)
ğ‘§
ğœğ‘§

+

ğœ2
ğ‘§
2ğœ(cid:48)2
ğ‘§

+

ğ‘§)2

(ğœ‡ğ‘§ âˆ’ ğœ‡(cid:48)
2ğœ(cid:48)2
ğ‘§

,

Thus, the overall objective function is written as

ğ¿ = ğ¿ğ‘Ÿ ğ‘’ğ‘ + ğ›¾ ğ¿ğ¾ ğ¿,

(5)

(6)

where we empirically set the hyperparameter ğ›¾ set to 1 in all
experiments.

4 Probabilistic 3D Detectors with Label Uncertainty

To reform a typical detector to be a probabilistic object de-
tector, we can enforce the detection head to estimate a prob-
ability distribution over bounding boxes, denoted as ğ‘ƒÎ˜ (ğ‘¦),
instead of a deterministic bounding box location:

ğ‘ƒÎ˜(ğ‘¦) =

âˆš

ğ‘’âˆ’ ( ğ‘¦âˆ’ Ë†ğ‘¦) 2

2 Ë†ğœ2

,

1
2ğœ‹ Ë†ğœ2

(7)

where Î˜ indicates learnable network weights of a typical
detector, Ë†ğ‘¦ is the predicted bounding box location, and Ë†ğœ is
the predicted localization variance.

Accordingly, we also assume the ground-truth bound-
ing box as a Gaussian distribution ğ‘ƒğ· (ğ‘¦) with variance ğœ2,
whose value is estimated by GLENet:

ğ‘ƒğ· (ğ‘¦) =

âˆš

( ğ‘¦âˆ’ğ‘¦ğ‘” )2
2ğœ2

,

ğ‘’âˆ’

1
2ğœ‹ğœ2

(8)

where ğ‘¦ğ‘” represents the ground-truth bounding box. There-
fore, we can incorporate the generated label uncertainty in the
KL loss between the distribution of prediction and ground-
truth in the detection head:

ğ¿ğ‘Ÿ ğ‘’ğ‘” = ğ· ğ¾ ğ¿ (ğ‘ƒğ· (ğ‘¦)||ğ‘ƒÎ˜(ğ‘¦))

= log

Ë†ğœ
ğœ

+

ğœ2
2 Ë†ğœ2

+

(ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦)2
2 Ë†ğœ2

.

(9)

4.1 More Analysis of KL-Loss

When ignoring label ambiguity and formulating the ground-
truth bounding box as a Dirac delta function, as done in (He
et al. (2019)), the loss in Eq. (9) degenerates into

ğ¿ ğ‘ğ‘Ÿ ğ‘œğ‘
ğ‘Ÿ ğ‘’ğ‘” âˆ

log( Ë†ğœ2)
2

+

(ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦)2
2 Ë†ğœ2

,

(10)

and the partial derivative of Eq. (10) with respect to the
predicted variance Ë†ğœ is:
ğœ•ğ¿ ğ‘ğ‘Ÿ ğ‘œğ‘
ğ‘Ÿ ğ‘’ğ‘”
ğœ• Ë†ğœ

(ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦)2
Ë†ğœ3

(11)

1
Ë†ğœ

âˆ’

=

.

When minimizing Eq. (10), a potential issue is that with
|ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦| â†’ 0,

ğœ•ğ¿ ğ‘ğ‘Ÿ ğ‘œğ‘
ğ‘Ÿ ğ‘’ğ‘”
ğœ• Ë†ğœ

â†’

,

1
Ë†ğœ

(12)

resulting in that the derivative for Ë†ğœ will explode when Ë†ğœ â†’
0. Based on the property of KL-loss, the prediction is optimal
only when the estimated Ë†ğœ = 0 and the localization error
|ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦| = 0. Therefore, the gradient explosion may result in
erratic training and sub-optimal localization precision.

By contrast, after modeling the ground-truth bounding
box as a Gaussian distribution, the partial derivative of Eq. (9)
with respect to prediction is:

=

1
Ë†ğœ

âˆ’

ğœ2
Ë†ğœ3

âˆ’

(ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦)2
Ë†ğœ3

,

ğœ•ğ¿ğ‘Ÿ ğ‘’ğ‘”
ğœ• Ë†ğœ
and
ğœ•ğ¿ğ‘Ÿ ğ‘’ğ‘”
ğœ• Ë†ğ‘¦

=

Ë†ğ‘¦ âˆ’ ğ‘¦ğ‘”
Ë†ğœ2
As |ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦| â†’ 0 and Ë†ğœ > 0,

.

ğœ•ğ¿ ğ‘ğ‘Ÿ ğ‘œğ‘
ğ‘Ÿ ğ‘’ğ‘”
ğœ• Ë†ğœ

and
ğœ•ğ¿ ğ‘ğ‘Ÿ ğ‘œğ‘
ğ‘Ÿ ğ‘’ğ‘”
ğœ• Ë†ğ‘¦

â†’

1
Ë†ğœ

(1 âˆ’

ğœ2
Ë†ğœ2

),

â†’ 0.

(13)

(14)

(15)

(16)

Thus, when the predicted distribution reaches the optimal
solution that is the distribution of ground-truth, i.e., |ğ‘¦ğ‘” âˆ’
Ë†ğ‘¦| â†’ 0 and Ë†ğœ â†’ ğœ, the derivatives for both Ë†ğ‘¦ and Ë†ğœ become
zero, which is an ideal property for the loss function and
avoids the aforementioned gradient explosion issue.

Fig. 5 shows the landscape of the KL-divergence loss
function under diï¬€erent label uncertainty ğœ, which are mark-
edly diï¬€erent in shape and property. The ğ¿ ğ‘ğ‘Ÿ ğ‘œğ‘
approaches
ğ‘Ÿ ğ‘’ğ‘”
inï¬nitesimal and the gradient explodes as |ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦| â†’ 0
and Ë†ğœ â†’ 0. However, when we introduce the estimated
label uncertainty and the predicted distribution is equal to
the ground-truth distribution, the KL Loss has a determined
minimum value of 0.5 and the gradient is smoother.

8

Yifan Zhang et al.

(a) ğ¿ ğ‘ğ‘Ÿ ğ‘œğ‘

ğ‘Ÿ ğ‘’ğ‘” ( ğœ = 0)

(b) ğ¿ğ‘Ÿ ğ‘’ğ‘” ( ğœ = 0.2)

(c) ğ¿ğ‘Ÿ ğ‘’ğ‘” ( ğœ = 0.5)

Fig. 5: Illustration of the KL-divergence between distributions as a function of localization error |ğ‘¦ğ‘” âˆ’ Ë†ğ‘¦| and estimated
localization variance Ë†ğœ given diï¬€erent label uncertainty ğœ. With label uncertainty ğœ estimated by GLENet instead of zero,
the gradient is smoother when the loss converges to the minimum. Besides, the ğ¿ğ‘Ÿ ğ‘’ğ‘” is smaller when ğœ is larger, which
prevents the model from overï¬tting to uncertain annotations.

Fig. 6: (a) Illustration of the relationship between the actual localization precision (i.e., IoU between predicted and ground-
truth bounding box) and the variance predicted by a probabilistic detector. Here, we reduce the dimension of the variance
with PCA to facilitate visualization. (b) Two examples: for the sparse sample, the prediction has high uncertainty and low
localization quality, while for the dense sample, the prediction has high localization quality and low uncertainty estimation.

4.2 Uncertainty-aware Quality Estimator

Most state-of-the-art two-stage 3D object detectors predict
an IoU-related conï¬dence score indicating the localization
quality, rather than the classiï¬cation score as the sorting cri-
terion in NMS (non-maximum suppression). As illustrated
in Fig. 6, it can be observed that there is a strong correlation
between the uncertainty and actual localization quality for
each bounding box, which encourages us to use uncertainty
as a criterion for judging the quality of boxes. However, the
estimated uncertainty is 7-dimensional, making it infeasible
to directly replace the IoU conï¬dence score with the un-
certainty. To this end, we propose uncertainty-aware quality
estimator (UAQE), which introduces the uncertainty infor-
mation to facilitate the training of the IoU-branch and im-
prove the IoU estimation accuracy. Speciï¬cally, as shown in
Fig. 7, given the predicted uncertainty as input, we construct

Fig. 7: Illustration of the proposed UAQE module in the de-
tection head using the learned localization variance to assist
the training of localization quality (IoU) estimation branch.

a lightweight sub-module consisting of two fully-connected
(FC) layers followed by the Sigmoid activation to generate
a coeï¬ƒcient. Then we multiply the original output of the
IoU-branch with the coeï¬ƒcient as the ï¬nal estimation.

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

9

Algorithm 1: 3D var voting

Data: ğµ is an ğ‘ Ã— 7 matrix of predicted bounding boxes with
parameter ( ğ‘¥, ğ‘¦, ğ‘§, ğ‘¤ , ğ‘™, â„, ğœƒ). ğ¶ is the corresponding
variance. ğ‘† is a set of N corresponding conï¬dence
values. ğœğ‘¡ is a tunable hyperparameter.

Result: The ï¬nal voting results ğ· of selected candidate boxes.

1 ğµ = {ğ‘1, ğ‘2, ..., ğ‘ğ‘ }; and ğ¶ = {ğ‘1, ğ‘2, ..., ğ‘ğ‘ };
2 ğ‘† = {ğ‘ 1, ğ‘ 2, ..., ğ‘ ğ‘ }; and ğ¿ = {1, 2, ..., ğ‘ };
3 ğ· â† {};
4 ğ‘–ğ‘œğ‘¢ğ‘¡ â„ğ‘Ÿ ğ‘’ğ‘ â„ â† ğœ‡;
5 while ğ¿ â‰  âˆ… do
idx =argmax
6

ğ‘†, ğ‘(cid:48) = ğ‘ğ‘–ğ‘‘ğ‘¥;

ğ‘–âˆˆğ¿

ğ¿(cid:48) = {ğ‘– |ğ‘– âˆˆ ğ¿, ğ¼ ğ‘œğ‘ˆ (ğ‘ğ‘– , ğ‘(cid:48)) > ğ‘–ğ‘œğ‘¢ğ‘¡ â„ğ‘Ÿ ğ‘’ğ‘ â„ };
ğ‘ƒ â† {};
for ğ‘– âˆˆ ğ¿(cid:48) do

ğ‘ğ‘– = ğ‘’âˆ’(1âˆ’ğ¼ ğ‘œğ‘ˆ (ğ‘ğ‘– ,ğ‘) ) 2/ğœğ‘¡ ;
if |ğ‘¡ ğ‘ğ‘›(ğ‘ ğœƒ
ğ‘ ğœƒ
ğ‘– = 0;

ğ‘– âˆ’ ğ‘(cid:48) ğœƒ ) | > 1 then

end
ğ‘ƒ â† ğ‘ƒ (cid:208) ğ‘ğ‘–;

(cid:205)ğ‘–âˆˆğ¿(cid:48) ğ‘ğ‘– Â· ğ‘ğ‘– /ğ‘ğ‘–
(cid:205)ğ‘–âˆˆğ¿(cid:48) ğ‘ğ‘– /ğ‘ğ‘–

end
ğ‘ğ‘š =
ğ· â† ğ· (cid:208) ğ‘ğ‘š;
ğ¿ â† ğ¿ âˆ’ ğ¿(cid:48);

, ğ‘ğ‘– âˆˆ ğ‘ƒ, ğ‘ğ‘– âˆˆ ğµ, ğ‘ğ‘– âˆˆ ğ¶;

7

8

9

10

11

12

13

14

15

16

17

18
19 end

4.3 3D Variance Voting

Considering that in probabilistic object detectors, the learned
localization variance by the KL loss can reï¬‚ect the uncer-
tainty of the predicted bounding boxes, following (He et al.,
2019), we also propose 3D variance voting to combine neigh-
boring bounding boxes to seek a more precise box represen-
tation. Speciï¬cally, at a single iteration in the loop, box ğ‘
with maximum score is selected and its new location is cal-
culated according to itself and the neighboring boxes. During
the merging process, the neighboring boxes that are closer
and have a low variance are assigned with higher weights.
Note that neighboring boxes with a large angle diï¬€erence
from ğ‘ do not participate in the ensembling of angles. We
refer the readers to Algorithm 1 for the details.

series of ablation studies to verify the necessity of diï¬€erent
key components and conï¬gurations in Section 5.4.

5.1 Experiment Settings

5.1.1 Benchmark Datasets

The KITTI dataset contains 7481 training samples with an-
notations in the camera ï¬eld of vision and 7518 testing sam-
ples. According to the occlusion level, visibility and bound-
ing box size, the samples are further divided into three dif-
ï¬culty levels: simple, moderate and hard. Following com-
mon practice, when performing experiments on the val set,
we further split all training samples into a subset with 3712
samples for training and the rest 3769 samples for validation.
We report the performance on both the val set and online test
leaderboard for comparison. And we use all training data for
the test server submission.

The Waymo Open dataset is a large-scale autonomous driv-
ing dataset with more diverse scenes and object annotations
in full 360â—¦, which contains 798 sequences (158361 Li-
DAR frames) for training and 202 sequences (40077 LiDAR
frames) for validation. These frames are further divided into
two diï¬ƒculty levels: LEVEL1 for boxes with more than ï¬ve
points and LEVEL2 for boxes with at least one point. We
report performance on both LEVEL 1 and LEVEL 2 diï¬ƒ-
culty objects using the recommended metrics, mean Average
Precision (mAP) and mean Average Precision weighted by
heading accuracy (mAPH).

5.1.2 Evaluation Metric for GLENet

Due to the unavailability of the true distribution of a ground-
truth bounding box, we propose to evaluate GLENet in a
non-reference manner, in which the negative log-likelihood
between the estimated distribution of ground-truth ğ‘ğ· (ğ‘‹ |ğ¶)
subjecting to a Gaussian distribution N (Ë†ğ‘¡, ğœ2) and ğ‘ ğœƒ (ğ‘‹ |ğ¶)
is computed:

5 Experiments

ğ¿ ğ‘ ğ¿ğ¿ (ğœƒ) = âˆ’

ğ‘ ğœƒ (ğ‘‹ |ğ‘) log ğ‘ğ· (ğ‘‹ |ğ¶)dğ‘‹

(17)

To reveal the eï¬€ectiveness and universality of our method,
we integrated GLENet into several popular types of 3D ob-
ject detection frameworks to form probabilistic detectors,
which were evaluated on two commonly used benchmark
datasets, i.e., the Waymo Open dataset (WOD) (Sun et al.,
2020) and the KITTI dataset (Geiger et al., 2012). Specif-
ically, we start by introducing speciï¬c experiment settings
and implementation details in Section 5.1. After that, we
report detection performance of the resulting probabilistic
detectors and make comparisons with previous state-of-the-
art approaches in Sections 5.2 and 5.3. Finally, we conduct a

âˆ«

1
ğ‘†

1
ğ‘†

ğ‘†
âˆ‘ï¸

ğ‘–=1
ğ‘†
âˆ‘ï¸

ğ‘–=1

â‰ˆ âˆ’

= âˆ’

log ğ‘ğ· (ğ‘‹ğ‘– |ğ¶)

ğ‘˜ )2

ğ‘˜ âˆ’ Ë†ğ‘¡ğ‘–
(ğ‘¡ğ‘–
2ğœğ‘˜ 2

+

log(ğœ2
ğ‘˜ )
2

+

log(2ğœ‹)
2

,

âˆ‘ï¸

ğ‘˜âˆˆ{ğ‘ğ‘¥ ,ğ‘ğ‘¦ ,
ğ‘ğ‘§ ,ğ‘¤ ,ğ‘™,â„,ğ‘Ÿ }

where ğ‘† denotes the number of inference times, ğ‘‹ğ‘– is the
result of the ğ‘–-th inference, and Ë†ğ‘¡ğ‘–
ğ‘˜ represent the re-
gression targets and the predicted oï¬€sets, respectively. We
estimate the integral by randomly sampling multiple pre-
diction results via the Monte Carlo method. Generally, the
value of ğ¿ ğ‘ ğ¿ğ¿ is small when GLENet outputs reasonable

ğ‘˜ and ğ‘¡ğ‘–

10

Yifan Zhang et al.

Fig. 8: Illustration of the occlusion data augmentation. (a) The point cloud of the original object associated with the annotated
ground-truth bounding box. (b) A sampled dense object (red) is placed between the LiDAR sensor and original object (blue).
(c) The projected range image from the point cloud in (b), where the convex hull (the red polygon) of the sampled object
is calculated and further jittered to increase the diversity of occluded samples. Based on the convex hull (the blue polygon)
of the original point cloud, the occluded area can be obtained. The point cloud of the original object corresponding to the
occluded area is removed. (d) Final augmented object with the annotated ground-truth bounding boxes.

bounding boxes, i.e., predicting diverse plausible boxes with
high variance for incomplete point cloud and consistent pre-
cise boxes with low variance for high-quality point cloud,
respectively.

5.1.3 Implementation Details

To prevent data leakage, we kept the dataset division of
GLENet consistent with that of the downstream detectors.
As the initial input of GLENet, the point cloud of each ob-
ject was uniformly pre-processed into 512 points via random
subsampling/upsampling. Then we decentralized the point
cloud by subtracting the coordinates of the center point to
eliminate the local impact of translation.

Architecturally, we realized the prior network and recog-
nition network with an identical PointNet structure consist-
ing of three FC layers of output dimensions (64, 128, 512),
followed by another FC layer to generate an 8-dim latent
variable. To avoid posterior collapse, we particularly chose a
lightweight PointNet structure with channel dimensions (8,
8, 8) in the context encoder. The prediction network concate-
nates the generated latent variable and context features and
feeds them into subsequent FC layers of channels (64, 64)
before predicting oï¬€sets and directions.

5.1.4 Training and Inference Strategies

We adopted Adam (Kingma and Ba, 2015) (ğ›½1=0.9, ğ›½2=0.99)
to optimize GLENet, which was trained for totally 400 epochs
on KITTI and 40 epochs on Waymo while maintaining a
batch size of 64 on 2 GPUs. We initialized the learning rate
as 0.003 and updated it with the one cycle policy (Smith,
2017).

In the training process, we applied common data aug-
mentation strategies, including random ï¬‚ipping, scaling, and
rotation, in which the scaling factor and rotation angle were

uniformly drawn from [0.95, 1.05] and [âˆ’ğœ‹/4, ğœ‹/4], respec-
tively. It is important to include multiple plausible ground-
truth boxes in training especially for incomplete point clouds,
so we further propose an occlusion-driven augmentation ap-
proach, as illustrated in Fig. 8, after which a complete point
cloud may look similar to another incomplete point cloud,
while the ground-truth boxes of them are completely dif-
ferent. To overcome posterior collapse, we also adopted KL
annealing (Bowman et al., 2016) to gradually increase the
weight of the KL loss from 0 to 1. We followed k-fold cross-
sampling to divide all training objects into 10 mutually exclu-
sive subsets. To overcome overï¬tting, each time we trained
GLENet on 9 subsets and then made predictions on the re-
maining subset to generate label uncertainty estimations on
the whole training set. During inference, we sampled the la-
tent variable ğ‘§ from the predicted prior distribution ğ‘ ğœƒ (ğ‘§|ğ‘)
30 times to form multiple predictions, the variance of which
was used as the label uncertainty.

5.1.5 Base Detectors

We integrated GLENet into three popular deep 3D object
detection frameworks, i.e., SECOND (Yan et al., 2018b),
CIA-SSD (Zheng et al., 2021a), and Voxel R-CNN (Deng
et al., 2021), to construct probabilistic detectors, which are
dubbed as GLENet-S, GLENet-C, and GLENet-VR, respec-
tively. Speciï¬cally, we introduced an extra FC layer on the top
of the detection head to estimate standard deviations along
with the box locations. Meanwhile, we applied the proposed
UAQE to GLENet-VR to facilitate the training of the IoU-
branch. Note that we kept all the other network conï¬gurations
of these base detectors unchanged for fair comparisons.

5.2 Evaluation on the KITTI Dataset

We compared GLENet-VR with state-of-the-art detectors on
the KITTI test set, and Table 1 reports the AP and mAP that

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

11

Table 1: Quantitative comparison with state-of-the-art methods on the KITTI test set for vehicle detection, under the evaluation
metric of 3D Average Precision (AP) of 40 sampling recall points. The best and second best results are highlighted in bold
and underlined, respectively.

Method

MV3D (Chen et al., 2017)
F-PointNet (Qi et al., 2018)
MMF (Liang et al., 2019)
PointPainting (Vora et al., 2020)
CLOCs (Pang et al., 2020)
EPNet (Huang et al., 2020)
3D-CVF (Yoo et al., 2020)
STD (Yang et al., 2019)
Part-A2 (Shi et al., 2020b)
3DSSD (Yang et al., 2020)
SA-SSD (He et al., 2020)
PV-RCNN (Shi et al., 2020a)
PointGNN (Shi and Rajkumar, 2020b)
Voxel-RCNN (Deng et al., 2021)
SE-SSD (Zheng et al., 2021b)
VoTR (Mao et al., 2021b)
Pyramid-PV (Mao et al., 2021a)
CT3D (Sheng et al., 2021)
GLENet-VR (Ours)

Reference

CVPRâ€™17
CVPRâ€™18
CVPRâ€™19
CVPRâ€™20
IROSâ€™20
ECCVâ€™20
ECCVâ€™20
ICCVâ€™19
TPAMIâ€™20
CVPRâ€™20
CVPRâ€™20
CVPRâ€™20
CVPRâ€™ 20
AAAIâ€™21
CVPRâ€™21
ICCVâ€™21
ICCVâ€™21
ICCVâ€™21
-

Modality

RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
RGB+LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR
LiDAR

3D APğ‘…40

Easy
74.97
82.19
88.40
82.11
88.94
89.81
89.20
87.95
87.81
88.36
88.80
90.25
88.33
90.90
91.49
89.90
88.39
87.83
91.67

Mod.
63.63
69.79
77.43
71.70
80.67
79.28
80.05
79.71
78.49
79.57
79.52
81.43
79.47
81.62
82.54
82.09
82.08
81.77
83.23

Hard
54.00
60.59
70.22
67.08
77.15
74.59
73.11
75.09
73.51
74.55
72.30
76.82
72.29
77.06
77.15
79.14
77.49
77.16
78.43

mAP
64.20
70.86
78.68
73.63
82.25
81.23
80.79
80.92
79.94
80.83
80.21
82.83
80.03
83.19
83.73
83.71
82.65
82.25
84.44

comparable to the exiting two-stage approaches while achiev-
ing relatively lower inference costs. It is worth noting that
our method is compatible with mainstream detectors and can
be expected to achieve better performance when combined
with stronger base detectors.

5.3 Evaluation on the Waymo Open Dataset

Table 3 lists the evaluation results of diï¬€erent approaches on
both LEVEL_1 and LEVEL_2 of the Waymo Open dataset,
where it can be seen that our method contributes 2.44%
and 1.24% improvements in terms of LEVEL_1 mAP for
SECOND and Voxel R-CNN, respectively. Besides, the per-
formance boost brought by our method becomes much more
obvious in the range of 30-50m and 50m-Inf. Intuitively, this
is because distant point cloud objects tend to be sparser and
thus have more serious issues of bounding box ambiguity.
GLENet-VR achieves better performance than the existing
methods with 77.32% mAP and 69.68% mAP for the LEVEL
1 and LEVEL 2 diï¬ƒculty, respectively.

5.4 Ablation Study

We conducted ablative analyses to verify the eï¬€ectiveness
and characteristics of our processing pipeline. In this section,
all the involved model variants are built upon the Voxel R-
CNN baseline and evaluated on the KITTI dataset, under
the evaluation metric of average precision calculated with 40
recall positions.

Fig. 9: PR curves of GLENet-VR on the car class of the
KITTI test set.

averages over the APs of easy, moderate and hard objects.
As of March 29ğ‘¡ â„, 2022, our GLENet-VR surpasses all pub-
lished single-modal detection methods by a large margin and
ranks 1ğ‘ ğ‘¡ among all published LiDAR-based approaches. Be-
sides, Fig. 9 also provides the detailed Prevision-Recall (PR)
curves of GLENet-VR on KITTI test split.

Table 2 lists the validation results of diï¬€erent detection
frameworks on the KITTI dataset, from which we can ob-
serve that GLENet-S, GLENet-C, and GLENet-VR consis-
tently outperform their corresponding baseline methods, i.e.,
SECOND, CIA-SSD, and Voxel R-CNN, by 4.79%, 4.78%,
and 1.84% in terms of 3D R11 AP on the category of mod-
erate car. Particularly, GLENet-VR achieves 86.36% AP on
the moderate car class, which surpasses all other state-of-the-
art methods. Besides, as a single-stage method, GLENet-C
achieves 84.59% AP for the moderate vehicle class, which is

12

Yifan Zhang et al.

Table 2: Quantitative comparison of diï¬€erent methods on the KITTI validation set for vehicle detection, under the evaluation
metric of 3D Average Precision (AP) calculated with 11 sampling recall positions. The 3D APs under 40 recall sampling recall
points are also reported for the moderate car class. The best and second best results are highlighted in bold and underlined,
respectively.

Methods

Part-ğ´2 (Shi et al., 2020b)
3DSSD (Yang et al., 2020)
SA-SSD (He et al., 2020)
PV-RCNN (Shi et al., 2020a)
SE-SSD (Zheng et al., 2021b)
VoTR (Mao et al., 2021b)
Pyramid-PV (Mao et al., 2021a)
CT3D (Sheng et al., 2021)
SECOND (Yan et al., 2018b)
GLENet-S (Ours)
CIA-SSD (Zheng et al., 2021a)
GLENet-C (Ours)
Voxel R-CNN (Deng et al., 2021)
GLENet-VR (Ours)

Reference

TPAMIâ€™20
CVPRâ€™20
CVPRâ€™20
CVPRâ€™20
CVPRâ€™21
ICCVâ€™21
ICCVâ€™21
ICCVâ€™21
Sensorsâ€™18
-
AAAIâ€™21
-
AAAIâ€™21
-

Easy
89.47
89.71
90.15
89.35
90.21
89.04
89.37
89.54
88.61
88.68
90.04
89.82
89.41
89.93

3D APğ‘…11
Moderate
79.47
79.45
79.91
83.69
85.71
84.04
84.38
86.06
78.62
82.95
79.81
84.59
84.52
86.46

Hard
78.54
78.67
78.78
78.70
79.22
78.68
78.84
78.99
77.22
78.19
78.80
78.78
78.93
79.19

Easy
-
-
92.23
92.57
93.19
-
-
92.85
91.16
91.73
93.59
93.20
92.38
93.51

3D APğ‘…40
Moderate
-
-
84.30
84.83
86.12
-
-
85.82
81.99
84.11
84.16
85.16
85.29
86.10

Hard
-
-
81.36
83.31
83.31
-
-
83.46
78.82
81.35
81.20
81.94
82.86
83.60

Table 3: Quantitative comparison of diï¬€erent methods on the Waymo validation set for vehicle detection. â˜…: experiment results
re-produced with the oï¬ƒcial code. The best and second best results are highlighted in bold and underlined, respectively.

Methods

PointPillar (Lang et al., 2019)
MVF (Zhou et al., 2020)
PV-RCNN (Shi et al., 2020a)
VoTr-TSD (Mao et al., 2021b)
Pyramid-PV (Mao et al., 2021a)
CT3D (Sheng et al., 2021)
SECONDâ˜… (Yan et al., 2018b)
GLENet-S (Ours)
Voxel R-CNNâ˜… (Deng et al., 2021)
GLENet-VR (Ours)

Overall
56.62
62.93
70.30
74.95
76.30
76.30
69.85
72.29
76.08
77.32

LEVEL_1 3D mAP
mAPH
0-30m 30-50m 50m-inf Overall
81.01
86.30
91.92
92.28
92.67
92.51
90.71
91.02
92.44
92.97

51.75
60.02
69.21
73.36
74.91
75.07
68.93
71.86
74.67
76.28

27.94
36.02
42.17
51.09
54.54
55.36
41.17
45.43
54.69
55.98

-
-
69.69
74.25
75.68
-
69.40
71.85
75.67
76.85

Overall
-
-
65.36
65.91
67.23
69.04
62.76
64.78
68.06
69.68

LEVEL_2 3D mAP
mAPH
0-30m 30-50m 50m-inf Overall

-
-
91.58
-
-
91.76
86.92
87.56
91.56
92.09

-
-
65.13
-
-
68.93
62.57
65.11
69.62
71.21

-
-
36.46
-
-
42.60
35.89
38.60
42.80
44.36

-
-
64.79
65.29
66.68
-
62.30
64.25
67.64
68.97

5.4.1 Comparison with Other Label Uncertainty Estimation

(Wang et al., 2020) signiï¬cantly in terms of APğµğ¸ğ‘‰ on both
moderate and hard levels.

We compared with other two ways of label uncertainty esti-
mation: 1) treating the label distribution as the deterministic
Dirac delta distribution with zero uncertainty; 2) estimating
the label uncertainty with simple heuristics, i.e., the num-
ber of points in the ground-truth bounding box or the IoU
between the label bounding box and its convex hull of the
aggregated LiDAR observations (Meyer and Thakurdesai,
2020). As shown in Table 4, our method consistently outper-
forms existing label uncertainty estimation paradigms. Com-
pared with heuristic strategies, our deep generative learning
paradigm can adaptively estimate label uncertainty statistics
in 7 dimensions, instead of the uncertainty of bounding boxes
as a whole, considering the variance in each dimension could
be very diï¬€erent.

Besides, to compare with (Wang et al., 2020), whose code
is not publicly available, we evaluated our method under its
experiment settings and compared results with its reported
performance. As shown in Table 5, our method outperforms

Table 4: Comparison of diï¬€erent label uncertainty estimation
approaches. "Convex hull" refers to the method in (Meyer and
Thakurdesai, 2020).

Methods

Voxel R-CNN
GLENet-VR w/ ğ¿ğ¾ ğ¿ğ· (ğœ2=0)
GLENet-VR w/ ğ¿ğ¾ ğ¿ğ· (points num)
GLENet-VR w/ ğ¿ğ¾ ğ¿ğ· (convex hull)
GLENet-VR w/ ğ¿ğ¾ ğ¿ğ· (Ours)

3D APğ‘…40
Easy Moderate Hard
82.86
85.29
92.38
83.05
85.37
92.48
83.16
85.58
92.46
82.81
85.45
92.33
83.56
86.10
93.49

5.4.2 Key Components of Probabilistic Detectors

We analyzed the contributions of diï¬€erent key components
in our constructed probabilistic detectors and reported results

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

13

Table 5: Comparison of our method with (Wang et al., 2020) on the
KITTI val set.

Method

ğ´ğ‘ƒğµğ¸ğ‘‰ for IoU@0.7
Easy Mod. Hard
76.60
86.79 80.75
PIXOR (Yang et al., 2018)
ProbPIXOR + Lğ¾ ğ¿ğ· (ğœ = 0)
78.74
88.60 80.44
ProbPIXOR + Lğ¾ ğ¿ğ· (Wang et al., 2020) 92.22 82.03
79.16
ProbPIXOR + Lğ¾ ğ¿ğ· (Ours)
81.85
91.50 84.23

Table 6: Contribution of each component in our constructed
GLENet-VR pipeline. â€œLU" denotes the label uncertainty.

KL loss LU var voting UAQE

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

Easy Moderate Hard
82.86
85.29
92.38
82.99
85.25
92.45
83.05
85.37
92.48
83.29
85.76
93.20
83.41
85.91
93.24
83.56
86.10
93.49

in Table 6. According to the second row, we can conclude
that only training with the KL loss brings little performance
gain. Introducing the label uncertainty generated by GLENet
into the KL Loss contributes 0.75%, 0.51%, and 0.3% im-
provements on the APs of easy, moderate, and hard classes,
respectively, which demonstrates its regularization eï¬€ect on
KLD-loss (Eq. 9) and its ability to estimate more reliable
uncertainty statistics of bounding box labels. The proposed
UAQE module in the probabilistic detection head boosts the
easy, moderate, and hard APs by 0.25%, 0.19% and 0.15%,
respectively, validating its eï¬€ectiveness in estimating the lo-
calization quality.

5.4.3 Inï¬‚uence of Data Augmentation

To generate similar point cloud shapes with diverse ground-
truth bounding boxes during training of GLENet, we pro-
posed an occlusion data augmentation strategy and gener-
ated more incomplete point clouds while keeping the bound-
ing boxes unchanged (see Fig. 8). As listed in Table 7, it
can be seen that the occlusion data augmentation eï¬€ectively
enhances the performance of GLENet and the downstream
detection task. Besides, the eï¬€ectiveness of the ğ¿ ğ‘ ğ¿ğ¿ met-
ric is also validated, which is proposed to evaluate GLENet
and select optimal conï¬gurations to generate reliable label
uncertainty.

5.4.4 Conditional Analysis

To ï¬gure out in what cases our method improves the base de-
tector most, we evaluated GLENet-VR on diï¬€erent occlusion
levels and distance ranges. As shown in Table 8, compared
with the baseline, our method mainly improves on the heavily

Table 7: Ablation study on occlusion augmentation tech-
niques in GLENet, in which we report the ğ¿ ğ‘ ğ¿ğ¿ for evalua-
tion of GLENet and the 3D average precisions of 40 sampling
recall points for evaluation of downstream detectors.

Occlusion
Ã—
(cid:88)

ğ¿ğ‘ ğ¿ğ¿ â†“
230.1
91.5

Easy
93.21
93.49

Mod.
85.86
86.10

Hard
83.35
83.56

Table 8: Comparison on diï¬€erent occlusion levels and dis-
tance rangesa, evaluated by the 3D Average Precision (AP)
calculated with 40 sampling recall positions on the KITTI
val set.

Methods

Voxel R-CNN
(Deng et al., 2021)

GLENet-VR
(Ours)

Improvement

Occlusionb

Distance

0
1
2
0-20m
20-40m
40m-Inf

92.35
76.91
54.32
96.42
83.82
38.86

93.51
78.64
56.93
96.69
86.87
39.82

+1.16
+1.73
+2.61
+0.27
+3.05
+0.96

a The results include separate APs for objects belonging to diï¬€erent
occlusion levels and APs for moderate vehicle class in diï¬€erent
distance ranges.

b Deï¬nition of occlusion levels: levels 0, 1 and 2 correspond to fully
visible samples, partly occluded samples, and samples diï¬ƒcult to
see respectively.

Table 9: Inference time comparison for diï¬€erent baselines on
the KITTI dataset.

Method
SECOND Yan et al. (2018b)
GLENet-S (Ours)
CIA-SSD Zheng et al. (2021a)
GLENet-C (Ours)
Voxel R-CNN Deng et al. (2021)
GLENet-VR (Ours)

FPS (Hz)
23.36
22.80
27.18
28.76
21.08
20.82

occluded and distant samples, which suï¬€er from more seri-
ous boundary ambiguities of ground-truth bounding boxes.

5.4.5 Inference Eï¬ƒciency

We evaluated the inference speed of diï¬€erent baselines with
a batch size of 1 on a desktop with Intel CPU E5-2560
@ 2.10 GHz and NVIDIA GeForce RTX 2080Ti GPU. As
shown in Table 9, our approach doesnâ€™t signiï¬cantly increase
the computational overhead. Particularly, GLENet-VR only
takes 0.6 more ms than the base Voxel R-CNN, since the
number of candidates for the input of var voting is relatively
small in two-stage detectors.

14

Yifan Zhang et al.

Fig. 10: Visual comparison of the results by GLENet-VR and Voxel R-CNN on the KITTI dataset. The ground-truth, true
positive and false positive bounding boxes are visualized in red, green and yellow, respectively, on both the point cloud and
image. Best viewed in color.

5.5 Comparison of Visual Results

improvement and produced state-of-the-art performance on
both KITTI and Waymo datasets.

Fig. 10 visualizes the detection results of our GLENet-VR
and the baseline Voxel R-CNN on the KITTI val set, where
it can be seen that our GLENet-VR obtains better detection
results with fewer false-positive bounding boxes and fewer
missed heavily occluded and distant objects than Voxel R-
CNN. We also compared detection results of SECOND and
GLENet-S on the Waymo validation set in Fig. 11, where it
can be seen that compared with SECOND (Yan et al., 2018b),
our GLENet-S has fewer false predictions and achieves more
accurate localization.

6 Conclusion

We presented a general and uniï¬ed deep learning-based
paradigm for modeling 3D object-level label uncertainty.
Technically, we proposed GLENet, adapted from the learn-
ing framework of CVAE, to capture one-to-many relation-
ships between incomplete point cloud objects and poten-
tially plausible bounding boxes. As a plug-and-play compo-
nent, GLENet can generate reliable label uncertainty statis-
tics that can be conveniently integrated into various 3D de-
tection pipelines to build powerful probabilistic detectors.
We veriï¬ed the eï¬€ectiveness and universality of our method
by incorporating the proposed GLENet into several exist-
ing deep 3D object detectors, which demonstrated consistent

References

Bowman S, Vilnis L, Vinyals O, Dai A, Jozefowicz R, Bengio
S (2016) Generating sentences from a continuous space.
In: Proceedings of The 20th SIGNLL Conference on Com-
putational Natural Language Learning, pp 10â€“21

Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A,
Zagoruyko S (2020) End-to-end object detection with
transformers. In: European conference on computer vi-
sion, pp 213â€“229

Chen X, Ma H, Wan J, Li B, Xia T (2017) Multi-view 3d
object detection network for autonomous driving. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 1907â€“1915

Choi J, Chun D, Kim H, Lee HJ (2019) Gaussian yolov3:
An accurate and fast object detector using localization
uncertainty for autonomous driving. In: Proceedings of
the IEEE International Conference on Computer Vision,
vol 2019-October, pp 502â€“511

Delany SJ, Segata N, Mac Namee B (2012) Proï¬ling in-
stances in noise reduction. Knowledge-Based Systems
31:28â€“40

Deng J, Shi S, Li P, Zhou W, Zhang Y, Li H (2021) Voxel
r-cnn: Towards high performance voxel-based 3d object

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

15

(a) SECOND

(b) GLENet-S (Ours)

Fig. 11: Visual comparison of the results by SECOND and GLENet-S on the Waymo val set. The ground-truth, true positive
and false positive bounding boxes are visualized in red, green and yellow, respectively. Best viewed in color and zoom in for
more details. Additional NMS is conducted for better visualization.

detection. In: Proceedings of the AAAI Conference on
Artiï¬cial Intelligence, pp 1201â€“1209

Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 3354â€“3361

Feng D, Rosenbaum L, Dietmayer K (2018) Towards safe au-
tonomous driving: Capture uncertainty in the deep neural
network for lidar 3d vehicle detection. In: IEEE Confer-
ence on Intelligent Transportation Systems, pp 3266â€“3273
Feng D, Rosenbaum L, Timm F, Dietmayer K (2019) Lever-
aging heteroscedastic aleatoric uncertainties for robust
real-time lidar 3d object detection. In: IEEE Intelligent
Vehicles Symposium, pp 1280â€“1287

Garcia LP, SÃ¡ez JA, Luengo J, Lorena AC, de Carvalho AC,
Herrera F (2015) Using the one-vs-one decomposition to
improve the performance of class noise ï¬lters via an ag-
gregation strategy in multi-class classiï¬cation problems.
Knowledge-Based Systems 90:153â€“164

Geiger A, Lenz P, Urtasun R (2012) Are we ready for au-
tonomous driving? the kitti vision benchmark suite. In:

Goyal A, Sordoni A, CÃ´tÃ© MA, Ke N, Bengio Y (2017) Z-
forcing: Training stochastic recurrent networks. In: Ad-
vances in Neural Information Processing Systems, pp
6714â€“6724

Harakeh A, Smart M, Waslander SL (2020) Bayesod: A
bayesian approach for uncertainty estimation in deep
object detectors. In: IEEE International Conference on
Robotics and Automation, IEEE, pp 87â€“93

He C, Zeng H, Huang J, Hua XS, Zhang L (2020) Structure
aware single-stage 3d object detection from point cloud.
In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 11870â€“11879

He Y, Zhu C, Wang J, Savvides M, Zhang X (2019) Bound-
ing box regression with uncertainty for accurate object de-
tection. In: Proceedings of the IEEE/CVF conference on

16

Yifan Zhang et al.

computer vision and pattern recognition, pp 2888â€“2897
Huang T, Liu Z, Chen X, Bai X (2020) Epnet: Enhanc-
ing point features with image semantics for 3d object de-
tection. In: European Conference on Computer Vision,
Springer, pp 35â€“52

Kingma D, Ba J (2015) Adam: A method for stochastic opti-
mization. In: International Conference on Learning Rep-
resentations, pp 1â€“15

Kingma DP, Welling M (2014) Auto-encoding variational
bayes. In: International Conference on Learning Repre-
sentations, pp 1â€“14

Lang A, Vora S, Caesar H, Zhou L, Yang J, BeÄ³bom O (2019)
Pointpillars: Fast encoders for object detection from point
clouds. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp 12689â€“12697
Li B, Sun Z, Guo Y (2019) Supervae: Superpixelwise vari-
ational autoencoder for salient object detection. In: Pro-
ceedings of the AAAI Conference on Artiï¬cial Intelli-
gence, vol 33, pp 8569â€“8576

Li J, Song Y, Zhang H, Chen D, Shi S, Zhao D, Yan R
(2018) Generating classical chinese poems via conditional
variational autoencoder and adversarial training. In: Pro-
ceedings of the 2018 conference on empirical methods in
natural language processing, pp 3890â€“3900

Li X, Wang W, Wu L, Chen S, Hu X, Li J, Tang J, Yang
J (2020) Generalized focal loss: Learning qualiï¬ed and
distributed bounding boxes for dense object detection. In:
Advances in Neural Information Processing Systems, pp
21002â€“21012

Liang M, Yang B, Chen Y, Hu R, Urtasun R (2019) Multi-
task multi-sensor fusion for 3d object detection. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 7345â€“7353

Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY,
Berg AC (2016) Ssd: Single shot multibox detector. In:
European conference on computer vision, Springer, pp
21â€“37

Luengo J, Shim SO, Alshomrani S, Altalhi A, Herrera F
(2018) Cnc-nos: Class noise cleaning by ensemble ï¬lter-
ing and noise scoring. Knowledge-Based Systems 140:27â€“
49

Mao J, Niu M, Bai H, Liang X, Xu H, Xu C (2021a) Pyramid
r-cnn: Towards better performance and adaptability for
3d object detection. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp 2723â€“
2732

Mao J, Xue Y, Niu M, Bai H, Feng J, Liang X, Xu H, Xu
C (2021b) Voxel transformer for 3d object detection. In:
Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp 3164â€“3173

Meyer G, Thakurdesai N (2020) Learning an uncertainty-
aware object detector for autonomous driving. In: IEEE
International Conference on Intelligent Robots and Sys-

tems, pp 10521â€“10527

Meyer GP, Laddha A, Kee E, Vallespi-Gonzalez C, Welling-
ton CK (2019) Lasernet: An eï¬ƒcient probabilistic 3d ob-
ject detector for autonomous driving. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 12677â€“12686

Mousavian A, Eppner C, Fox D (2019) 6-dof graspnet: Vari-
ational grasp generation for object manipulation. In: Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision, pp 2901â€“2910

Najibi M, Lai G, Kundu A, Lu Z, Rathod V, Funkhouser
T, Pantofaru C, Ross D, Davis L, Fathi A (2020) Dops:
Learning to detect 3d objects and predict their 3d shapes.
In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 11910â€“11919

Nash C, Williams C (2017) The shape variational autoen-
coder: A deep generative model of part-segmented 3d ob-
jects. Computer Graphics Forum 36(5):1â€“12

Northcutt C, Jiang L, Chuang I (2021) Conï¬dent learning:
Estimating uncertainty in dataset labels. Journal of Artiï¬-
cial Intelligence Research 70:1373â€“1411

Painchaud N, Skandarani Y, Judge T, Bernard O, Lalande
A, Jodoin PM (2020) Cardiac segmentation with strong
anatomical guarantees. IEEE transactions on medical
imaging 39(11):3703â€“3713

Pang S, Morris D, Radha H (2020) Clocs: Camera-lidar ob-
ject candidates fusion for 3d object detection. In: IEEE
International Conference on Intelligent Robots and Sys-
tems, IEEE, pp 10386â€“10393

Qi CR, Su H, Mo K, Guibas LJ (2017) Pointnet: Deep learn-
ing on point sets for 3d classiï¬cation and segmentation.
In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 652â€“660

Qi CR, Liu W, Wu C, Su H, Guibas LJ (2018) Frustum
pointnets for 3d object detection from rgb-d data. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 918â€“927

Sharma S, Varigonda PT, Bindal P, Sharma A, Jain A (2019)
Monocular 3d human pose estimation by generation and
ordinal ranking. In: Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pp 2325â€“2334
Sheng H, Cai S, Liu Y, Deng B, Huang J, Hua XS, Zhao MJ
(2021) Improving 3d object detection with channel-wise
transformer. In: Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pp 2743â€“2752
Shi S, Wang X, Li H (2019) Pointrcnn: 3d object proposal
generation and detection from point cloud. In: Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition, pp 770â€“779

Shi S, Guo C, Jiang L, Wang Z, Shi J, Wang X, Li H (2020a)
Pv-rcnn: Point-voxel feature set abstraction for 3d object
detection. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp 10529â€“10538

GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation

17

Shi S, Wang Z, Shi J, Wang X, Li H (2020b) From points
to parts: 3d object detection from point cloud with part-
aware and part-aggregation network. IEEE Transactions
on Pattern Analysis and Machine Intelligence 43(8):2647â€“
2664

Shi W, Rajkumar R (2020a) Point-gnn: Graph neural network
for 3d object detection in a point cloud. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 1708â€“1716

Shi W, Rajkumar R (2020b) Point-gnn: Graph neural network
for 3d object detection in a point cloud. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 1711â€“1719

Smith LN (2017) Cyclical learning rates for training neural
networks. In: Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision, IEEE, pp
464â€“472

Sohn K, Lee H, Yan X (2015) Learning structured output
representation using deep conditional generative models.
In: Advances in Neural Information Processing Systems,
pp 3483â€“3491

Sun P, Kretzschmar H, Dotiwalla X, Chouard A, Patnaik V,
Tsui P, Guo J, Zhou Y, Chai Y, Caine B, Vasudevan V, Han
W, Ngiam J, Zhao H, Timofeev A, Ettinger S, Krivokon M,
Gao A, Joshi A, Zhang Y, Shlens J, Chen Z, Anguelov D
(2020) Scalability in perception for autonomous driving:
Waymo open dataset. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pp 2443â€“2451

Tan M, Pang R, Le QV (2020) Eï¬ƒcientdet: Scalable and ef-
ï¬cient object detection. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pp 10781â€“10790

Varamesh A, Tuytelaars T (2020) Mixture dense regression
for object detection and human pose estimation. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 13086â€“13095

Vora S, Lang AH, Helou B, BeÄ³bom O (2020) Pointpainting:
Sequential fusion for 3d object detection. In: Proceedings
of the IEEE/CVF conference on computer vision and pat-
tern recognition, pp 4604â€“4612

Wang T, Wan X (2019) T-cvae: Transformer-based condi-
tioned variational autoencoder for story completion. In:
International Joint Conference on Artiï¬cial Intelligence,
pp 5233â€“5239

Wang Z, Feng D, Zhou Y, Rosenbaum L, Timm F, Diet-
mayer K, Tomizuka M, Zhan W (2020) Inferring spa-
tial uncertainty in object detection. In: IEEE International
Conference on Intelligent Robots and Systems, IEEE, pp
5792â€“5799

Xu Q, Zhou Y, Wang W, Qi CR, Anguelov D (2021) Spg:
Unsupervised domain adaptation for 3d object detection
via semantic point generation. In: Proceedings of the

IEEE/CVF International Conference on Computer Vision,
pp 15446â€“15456

Yan X, Yang J, Sohn K, Lee H (2016) Attribute2image:
Conditional image generation from visual attributes. In:
European conference on computer vision, Springer, pp
776â€“791

Yan X, Rastogi A, Villegas R, Sunkavalli K, Shechtman
E, Hadap S, Yumer E, Lee H (2018a) Mt-vae: Learning
motion transformations to generate multimodal human dy-
namics. In: European conference on computer vision, pp
265â€“281

Yan X, Gao J, Li J, Zhang R, Li Z, Huang R, Cui S (2021)
Sparse single sweep lidar point cloud segmentation via
learning contextual shape priors from scene completion.
In: Proceedings of the AAAI Conference on Artiï¬cial
Intelligence, vol 35, pp 3101â€“3109

Yan Y, Mao Y, Li B (2018b) Second: Sparsely embedded

convolutional detection. Sensors 18(10):3337

Yang B, Luo W, Urtasun R (2018) Pixor: Real-time 3d ob-
ject detection from point clouds. In: Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition, pp 7652â€“7660

Yang Z, Sun Y, Liu S, Shen X, Jia J (2019) Std: Sparse-to-
dense 3d object detector for point cloud. In: Proceedings
of the IEEE/CVF International Conference on Computer
Vision, pp 1951â€“1960

Yang Z, Sun Y, Liu S, Jia J (2020) 3dssd: Point-based 3d sin-
gle stage object detector. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp
11040â€“11048

Yi L, Zhao W, Wang H, Sung M, Guibas L (2019) Gspn: Gen-
erative shape proposal network for 3d instance segmen-
tation in point cloud. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pp 3942â€“3951

Yoo JH, Kim Y, Kim J, Choi JW (2020) 3d-cvf: Generating
joint camera and lidar features using cross-view spatial
feature fusion for 3d object detection. In: European Con-
ference on Computer Vision, Springer, pp 720â€“736

Zhang B, Xiong D, Su J, Duan H, Zhang M (2016) Varia-
tional neural machine translation. In: Proceedings of the
2016 conference on empirical methods in natural language
processing, Association for Computational Linguistics,
Austin, Texas, pp 521â€“530

Zhang C, Bengio S, Hardt M, Recht B, Vinyals O (2021) Un-
derstanding deep learning (still) requires rethinking gen-
eralization. Communications of the ACM 64(3):107â€“115
Zhang J, Fan DP, Dai Y, Anwar S, Saleh FS, Zhang T, Barnes
N (2020) Uc-net: Uncertainty inspired rgb-d saliency de-
tection via conditional variational autoencoders. In: Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 8582â€“8591

18

Yifan Zhang et al.

Zhao T, Zhao R, Eskenazi M (2017) Learning discourse-
level diversity for neural dialog models using conditional
variational autoencoders. In: ACL 2017 - 55th Annual
Meeting of the Association for Computational Linguistics,
Proceedings of the Conference, pp 654â€“664

Zheng W, Tang W, Chen S, Jiang L, Fu CW (2021a) Cia-
ssd: Conï¬dent iou-aware single-stage object detector from
point cloud. In: Proceedings of the AAAI Conference on
Artiï¬cial Intelligence, pp 3555â€“3562

Zheng W, Tang W, Jiang L, Fu CW (2021b) Se-ssd: Self-
ensembling single-stage object detector from point cloud.
In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 14494â€“14503

Zhou Y, Tuzel O (2018) Voxelnet: End-to-end learning for
point cloud based 3d object detection. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 4490â€“4499

Zhou Y, Sun P, Zhang Y, Anguelov D, Gao J, Ouyang T, Guo
J, Ngiam J, Vasudevan V (2020) End-to-end multi-view
fusion for 3d object detection in lidar point clouds. In:
Conference on Robot Learning, PMLR, pp 923â€“932

