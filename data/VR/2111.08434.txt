Robust 3D Scene Segmentation through Hierarchical and Learnable Part-Fusion

Anirud Thyagharajan, Benjamin Ummenhofer, Prashant Laddha, Om J Omer, Sreenivas Subramoney
Intel Labs

1
2
0
2

v
o
N
6
1

]

V
C
.
s
c
[

1
v
4
3
4
8
0
.
1
1
1
2
:
v
i
X
r
a

Abstract

3D semantic segmentation is a fundamental building
block for several scene understanding applications such as
autonomous driving, robotics and AR/VR. Several state-of-
the-art semantic segmentation models suffer from the part-
misclassiÔ¨Åcation problem, wherein parts of the same object
are labelled incorrectly. Previous methods have utilized hi-
erarchical, iterative methods to fuse semantic and instance
information, but they lack learnability in context fusion, and
are computationally complex and heuristic driven. This
paper presents Segment-Fusion, a novel attention-based
method for hierarchical fusion of semantic and instance in-
formation to address the part misclassiÔ¨Åcations. The pre-
sented method includes a graph segmentation algorithm for
grouping points into segments that pools point-wise features
into segment-wise features, a learnable attention-based net-
work to fuse these segments based on their semantic and
instance features, and followed by a simple yet effective
connected component labelling algorithm to convert seg-
ment features to instance labels. Segment-Fusion can be
Ô¨Çexibly employed with any network architecture for seman-
tic/instance segmentation. It improves the qualitative and
quantitative performance of several semantic segmentation
backbones by upto 5% when evaluated on the ScanNet and
S3DIS datasets.

1. Introduction

The growing resolution and availability of 3D visual sen-
sors in recent years (e.g. Kinect, RealSense, Xtion) have
enabled high Ô¨Ådelity representation of real world scenes us-
ing 3D data, allowing machines to understand 3D scenes
with higher accuracy. One of the most important tasks in
scene understanding includes 3D semantic segmentation,
which aims to recognize the object class that each point
in the scene belongs to. 3D semantic segmentation is fun-
damental to various applications, including but not limited
to autonomous driving, robotic navigation, localization and
mapping, and scene understanding [11, 27, 30, 37].

Recent advancements in 3D scene understanding have
been greatly inÔ¨Çuenced by deep neural networks achieving

Figure 1. Semantic labels with baseline [3] and our Segment-
Fusion approach: The highlighted regions show a partly misla-
beled object, a common problem for semantic segmentation meth-
ods, and the improved result after applying Segment-Fusion on a
sample point cloud from the ScanNet validation set [5].

state-of-the-art results [3,9,10,15‚Äì17,20,22,33,36] on mul-
tiple benchmarks and wide range of datasets [1, 5]. How-
ever, many of these methods are often plagued by the part-
misclassiÔ¨Åcation problem, where parts of the same object
are labelled incorrectly as shown in Figure 1. This problem
can be better addressed if object instance boundaries can be
correctly estimated, which includes annotating points with
object instance identiÔ¨Åers. This helps to group all points
associated with an object instance and utilize consensus to
rectify semantic mispredictions.

Jointly solving semantic and instance segmentation has
been explored by several previous works [2, 7, 12, 13, 18,
21, 23, 34, 35]. Many of these methods [13, 18, 23, 34, 35]
focus on employing models for instance and semantic seg-
mentation, before fusing pointwise features. However, they
require higher compute resources as they fuse at a Ô¨Åne-
grained level and do not exploit local continuities in scene
structure. Methods such as [2,7,12,21] include hierarchical
pooling of features over a spatial regionaddressing the part
misclassiÔ¨Åcation problem by fusing these regions correctly,
with an assumption that these regions do not stretch over
object boundaries. Such hierarchical methods are more ef-
Ô¨Åcient since they work with regions and not directly with
points.

While such methods are more efÔ¨Åcient, they employ iter-
ative clustering and aggregation techniques that are known
to be heuristic-driven, non-learnable and speciÔ¨Åc to the
backbone models used for extracting semantic and instance

Part misclassificationwith baseline+Segment-Fusion 
 
 
 
 
 
Figure 2. Overview of the Segment-Fusion approach. The Graph Segmentation module (GS) groups points into segments and enables
pooling pointwise semantic and instance features S, I into segment-wise features ¬ØS, ¬ØI. The Segment Fusion Network maps these features
to a common feature space ¬ØF which is then used in the connected component labelling algorithm (CCL) to create the Ô¨Ånal set of segments.

features. They also use complex post-processing [6, 26] to
transform instance features to labels. Thus, there is a need
for hierarchical, learnable methods for fusing such regions
with simple post-processing.

Our prime objective is to improve semantic segmentation
performance of a generic semantic segmentation model by
using a hierarchical fusion of semantic and instance infor-
mation. We use a graph segmentation algorithm to group
points into regions and pool region-wise semantic and in-
stance features but downstream, we propose a learnable
clustering algorithm. It is desirable that the method should
be learnable and agnostic to dataset speciÔ¨Åc heuristics.

‚Ä¢ We propose a graph segmentation algorithm optimized
for semantic segmentation across datasets, which is
used to compose semantic and instance features at a
coarser level (which we term as segments).

‚Ä¢ We propose a learnable attention based network, Seg-
ment Fusion,
to hierarchically fuse these segments
based on the similarity of their semantic and instance
features, thus understanding the appropriate granular-
ity of context (local to global: points to instance to
scene).

‚Ä¢ Our approach offers the advantages of adapting to in-
puts from other datasets and other backbones, as well
as offering the possibility to perform end-to-end train-
ing and maintain efÔ¨Åciency (while working on a hi-
erarchically smaller representation, and not working
on individual points). These proposals help ameliorate
the problem of part-misclassiÔ¨Åcation by improving the
performance (mIoU) of multiple semantic backbones
on datasets like ScanNet V2 [5] and S3DIS [1] upto
5%. We also compare the impact of our proposed
method with the iterative clustering method proposed

by Occuseg [12] and report 1-2% mIoU improvement
in semantic segmentation.

2. Related Work

3D Semantic Segmentation.

Semantic segmenta-
tion techniques can be broadly classiÔ¨Åed into 2D projection
based and 3D methods. Methods that use 3D processing are
known to provide superior accuracy because they can natu-
rally overcome occlusion or scale ambiguity [12]. Several
3D methods have been proposed for semantic segmentation,
which can be further categorized into point-based [24, 25]
and volumetric methods [3, 10, 15‚Äì17, 33]. Methods from
the PointNet family [24, 25] work on unstructured point
clouds directly, employing pointwise networks to extract
features and using ball queries and hierarchical grouping to
encode spatial locality.

On the other hand, volumetric methods voxelize point
clouds into regular grids and process them with regular
structured computations. SparseConvNet [10] proposed
submanifold sparse convolutions to process only the active
voxels in a scene, while MinkowskiNet [3] generalized this
concept with the help of hybrid kernels to express different
types of local structure. Apart from these classes of meth-
ods, hybrid methods that utilize deformable/parameteric
convolutions [29] on points have been proposed, in addi-
tion to employing graph convolutions [14, 32, 38] on points.
All the above models apply voxel/point-wise cross entropy
loss to optimize the model weights. Eventhough they em-
ploy network topologies such as U-Nets that are intended to
comprehend scale and local-to-global structure, they exhibit
part-misclassiÔ¨Åcation issues as observed in Figure 3. There-
fore, the model needs additional information about which
instance of the object each point is a part of.

Joint Semantic-Instance Segmentation. Instance seg-
mentation techniques aim to predict features for every point

Graph SegmentationSegment Fusion NetworkCCLPoints (ùí´), Colors (‚Ñ±), Normals(ùí©) Semantic Features (S)Instance Features (I)Attention-based encoder stacksSegmented Point Cloud(SSF)ùêÖ(cid:3364)ùêí(cid:3364),ùêàÃÖSegment-FusionPool Features(Œ¶)‚ÑêFigure 3. Illustrations of the part misclassiÔ¨Åcation problem while
using SCN [10] (Ô¨Årst row) and MinkowskiNet [3] (second row) as
backbones for predicting the semantic classes of the ScanNet V2
dataset [5].

that indicate the object with which it is associated. Several
methods have been proposed to perform semantic and in-
stance segmentation jointly, since there is shared context
between two similar tasks. Methods such as ASIS [35],
SGPN [34] and JSIS3D [23] exercise models to fuse se-
mantic and instance features pointwise, and subsequently
compute the instance labels through clustering algorithms;
ASIS [35] uses mean-shift clustering, SGPN [34] uses NMS
to Ô¨Ålter proposals and JSIS3D [23] uses a CRF to cluster in-
stance embeddings into labels. However, these methods are
heuristic driven (eg. mean-shift clustering [6] depends upon
the window size) and are computationally complex (since
they work with the pointwise features).

Works such as Occuseg, 3D-MPA and HAIS [2, 7] sim-
plify this complexity by grouping features at a higher ab-
stractive level of surfaces (than points), thus proposing hi-
erarchically fusing these features to form instances. Oc-
cuseg [12] computes instance and semantic features per su-
pervoxel, where supervoxels are composed using a graph
segmentation scheme [8], which are fed to an iterative clus-
tering algorithm to merge supervoxels into instances. 3D-
MPA [7] samples keypoints through a deep voting scheme,
uses a graph convolutional network to reÔ¨Åne the keypoint
features, which are fed to DBSCAN [26] to obtain instance
labels. HAIS [2] proposed a hierarchical method to generate
point sets (based on the semantic features) and employ an
iterative set aggregation algorithm to aggregate these point
sets into instances. SST-Net [21] propose a tree classiÔ¨Åer to
hierarchically build a subtree of superpoint proposals.

While these methods may be effective for certain
datasets, they propose iterative algorithms, which are not
learnable. This makes it difÔ¨Åcult for the algorithm to gen-
eralize for multiple sets of input features and/or datasets.
Eventhough 3D-MPA uses a graph convolutional layer to
reÔ¨Åne the higher-level features, (a) they work with a Ô¨Åxed
number of proposals in a scene (due to the deep voting
scheme) which does not help scale with scenes, and (b) they
eventually use a heuristic driven classiÔ¨Åer (DBSCAN [26])
after the reÔ¨Ånement.

Figure 4. Impact of proposed J-GS method on an example S3DIS
point cloud. As observed in 1(cid:13) [8], graph segmentation purely
based on difference in normals fails to recognise the boundary be-
tween the door and the wall, while augmenting it with color infor-
mation helps in discerning this (J-GS 2(cid:13)).

Graph Segmentation for hierarchical processing. As
discussed above, graph segmentation algorithms [8] have
been used by previous methods to group points into larger
regions (based on similarity of normals) work at the abstrac-
tion level of these regions.
It is observed that represen-
tations in higher-level abstractions such as these segments
offer geometric continuity, particularly since they provide
guarantees that the point normals in a segment vary only
within a bound. Graph segmentation solely based on nor-
mals is effective but in some datasets normals across object
boundaries are not distinctive enough, as seen in Figure 4.
This problem cannot be solved by lowering threshold for
discriminating normals, because it leads to a highly over-
segmented graph and defeats the purpose of hierarchical
processing. Previous work [28] aims to solve this problem
by performing segmentation using dissimilarity in normals
and colors, but does not generalize for other kinds of fea-
tures, or propose methods for selecting the thresholds for
normals/colors.

Ideally, a semantic-instance fusion algorithm should

have the following desirable properties:

‚Ä¢ Hierarchical. To be efÔ¨Åcient, the system should ab-
stract features from the points level to a higher-level
spatial representation (segments: eg. surfaces, super-
voxels) and process at this level.

‚Ä¢ Learnable. The task of fusion of features to spatially
merge segments into instances should be done by a
learnable model to accommodate transferring knowl-
edge to different datasets and different semantic and
instance backbones.

‚Ä¢ Agnostic to semantic and instance backbones. The
fusion method should be compatible with multiple
methods for extracting semantic and instance features.

‚Ä¢ Simple post-processing. As far as possible, the task
of converting instance features to labels should be done
simplistically to avoid imparting any heuristic driven

GT Semantic LabelsPredicted LabelsColorsGT Instance LabelsJoint GS (normals& colors)GS (only normals)12processing (and absorb learning to the learnable com-
ponent).

3. Methods

We present an overview of our method in Figure 2, at-
tempting to incorporate the desirable properties discussed
in Section 2. The proposed method assumes a (i) semantic
model that emits per-point semantic features S and (ii) an
instance model that emits per-point instance features I. Our
method applies equivocally to those cases where these mod-
els could be sharing a common backbone network too. For
instance models like Occuseg [12] that emit multiple types
of embeddings (eg. instance embeddings, centroid and oc-
cupancy estimates), I can be thought of as a concatenation
of such features pointwise. In general, we denote point-wise
features by X and segment-wise features by X.

3.1. Joint Graph Segmentation (J-GS)

We propose a hierarchical strategy, segmenting points
into 3D surfaces (segments or super-voxels) using efÔ¨Åcient
graph segmentation algorithms such as [8, 19, 28]. The in-
put graph is deÔ¨Åned by vertices (points P) joined by edges
(neighbors E), where the goal of the segmentation is to Ô¨Ånd
a mapping J that assigns the same segment ID to a set of
grouped points. The algorithm also records the connections
(E) between the segments, reresented by an adjacency ma-
trix A. As a result of our proposed method, we observe that
we obtain over-segmented classiÔ¨Åcation boundaries which
largely do not violate object boundaries, such that points
belonging to different objects end up in different segments
(Figure 4).

To cover a gamut of datasets, we generalize our graph

segmentation algorithm to work across two options:

‚Ä¢ Point clouds without mesh information. Datasets
like ScanNet [5] have mesh information available,
which allows us to compute per vertex normals N
from the adjacent polygonal faces. However, for
datasets like S3DIS [1] which do not have such
connectivity information, we propose obtaining nor-
mals using plane-Ô¨Åtting techniques, as obtaining mesh
information would incur additional expensive pre-
processing.

‚Ä¢ Indiscriminative Normals. We propose to generalize
this method across datasets over previous work [28] by
(a) allowing to use arbitrary pointwise features F (not
speciÔ¨Åcally colors) in addition to normals, and then
performing a joint segmentation over two spaces (F
and N ), as shown in Algorithm 1. An example using
colors as F for the S3DIS dataset is shown in Figure 4.
(b) we generalize sorting orders (s : {norm, f eats},
choosing to sort edges along similarity in N or F), and

(c) we employ a voting-based methodology (see Algo-
rithm 2) to decide the optimal similarity threshold for
a generic dataset for the semantic segmentation task.

In Algorithm 1, computeEdgeWeights computes
dissimilarity scores along N (cosine product) and F ((cid:96)1/(cid:96)2
norms). updateThresh is similar to previous work [28]
where the threshold slightly increases to accommodate a
growing segment, where it approaches the largest intra-
segment weight.

Algorithm 1 Proposed Graph Segmentation
Inputs: Normals N , Features F , Polygonal edges E, feature and normal

similarity thresholds (fth, nth), sort-Ô¨Çag (s)

(cid:46) initialize threshold vectors

(cid:46) edge joining ith and jth points

Ewn, Ewf ‚Üê computeEdgeWeights(N , F )
E ‚Üê sortEdgesByNormalsOrFeats(s)
thf , thn ‚Üê fth, nth
J ‚Üê initUnionFind(E)
for eij ‚àà E do

1: function SEGMENT GRAPH(N , F , E, nth, fth, s)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
end for
14:
return J
15:
16: end function

end if

newID ‚Üê J .join(i, j)
updateThresh(ewn, ewf , thn, thf , newID)

Si ‚Üê J .Ô¨Ånd(i)
Sj ‚Üê J .Ô¨Ånd(j)
ewn, ewf ‚Üê Ewn[i, j], Ewf [i, j]
if (ewn < thn[i], thn[j]) and (ewf < thf [i], thf [j]) then

(cid:46) edge weights

Algorithm 2 Graph Segmentation based Voting algorithm

Inputs: List of J-GS Params gj = (nth, fth, s), predicted semantic la-

bels S, groundtruth labels G

function J-GSV(GP, S)

init scores
for gj in GP do

Jj ‚Üê SEGMENT GRAPH(gj )
Sj,maj ‚Üê majorityVoting(Jj , S)
scores[j] ‚Üê getIoU(Sj,maj , G)

end for
return argmin(scores)

end function

To select the right variant for given dataset, we en-
able quantitative comparison across the above options by
proposing the J-GSV score (Joint Graph Segmentation-
based Voting) (Algorithm 2). Across point clouds from the
training set, for the j-th variant, we obtain a list of sets of
points Jj, with the i-th set Jj,i containing points pertaining
to the same segment. We compute the semantic predictions
for the j-th variant by pooling the per-point semantic pre-
dictions S for each segment, and broadcasting the majority
label to all points in the segment. The mean Intersection-
over-Union (mIoU) for the modiÔ¨Åed semantic predictions
Sj,maj is the score associated with the j-th variant. For

of losses - (i) Instance loss and (ii) Segment Loss. Thus, the
overall loss function is:

LSF = Linstance + Lsegment

(2)

3.2.1 SF-Instance Loss

We propose attraction and repulsion instance losses at
the segment level. These losses ensure that segments of
the same instance are clustered together (Equation (4)),
whereas the centroids of the instance features are repelled
from each other (Equation (5)). While this appears sim-
ilar to the instance losses employed at the per-point level
[12, 35] in principle, but in our case, the losses are applied
at the segment-level features. Additionally, we parameterise
the thresholds for repulsion and attraction to be ‚àÜD and ‚àÜV
respectively, essentially to unify the thresholds used in the
SF-Segment loss and CCL algorithm.

Linstance = Lattract + Lrepel + Lreg

(3)

Figure 5. Proposed Ô¨Çow for generalizing graph segmentation
across a variety of point cloud datasets.

the dataset, we select the variant with the highest score and
denote by J ‚àó. Consequently, we employ a differentiable
function œÜ, to transform pointwise features to segment-wise
features (X = œÜ(X, J ‚àó)). To simplify, our implementa-
tion of œÜ(.) is the mean function, average-pooling across
pointwise semantic and instance features (S, I) to obtain
segment-wise features (¬ØS, ¬ØI), similar to [12]. For pointwise
features X we get the feature for a segment as

Xi =

1
|Ji|

(cid:88)

j‚ààJi

Xj.

(1)

Lattract =

1
K

K
(cid:88)

i=1

1
Ni

Ni(cid:88)

j=1

H(d( ¬ØFj, ¬µi) ‚àí ‚àÜV )2

(4)

Thus, the proposed joint graph segmentation hierarchi-
cally composes features of a scene at a coarser level (than
at the level of points) to enable efÔ¨Åcient fusion decisions of
these segments.

3.2. Segment-Fusion Network

Having formed segments out of the underlying points,
the next goal is to understand how to form objects out of
these segments. To this effect, we train a network to jointly
associate the instance-level and semantic-level information
to essentially form decisions if a pair of segments belong to
the same object (fusable) or not (separable).

We project the instance and semantic features per seg-
ment {¬ØS, ¬ØI} to a transformed set of features ¬ØF in the
joint semantic-instance space using the Segment-Fusion
network.We compose the Segment-Fusion network using a
stack of attention encoder blocks (taking cues from a Trans-
former [31] block). Attention blocks are known to capture
larger contextual information among different segments by
extracting useful intermediate representations using scaled-
dot-product attention. Additionally, in each block, we pro-
pose multiplying the attention-matrix element-wise with the
adjacency matrix A describing the connections between the
segments. This helps to constrain the interactions only be-
tween the segment pairs that are spatially connected. A
more detailed description of the network architecture is pro-
vided in the supplementary material.

To supervise this learning process, we propose two sets

Lrepel =

1
K(K ‚àí 1)

K
(cid:88)

K
(cid:88)

i=1

j=1;i(cid:54)=j

H(‚àÜD ‚àí d(¬µi, ¬µj))2 (5)

Lreg =

1
K

K
(cid:88)

i=1

||¬µi||1

(6)

where K denotes the number of ground truth instances in
the scene; ||.|| is the (cid:96)1 norm; ¬µi is the average of the seg-
ment features across the segments belonging to the ith in-
stance; d(fi, fj) indicates feature distance using a suitable
norm ((cid:96)1/(cid:96)2), and H(..) is the hinge loss; we set the values
of the thresholds ‚àÜV and ‚àÜD to be 0.10 and 1 respectively.

3.2.2 SF-Segment Loss

The SF-instance losses aid in clustering segment features
appropriately, but relying solely on them requires iterative
post-processing clustering algorithms such as DBSCAN
[26], mean-shift clustering [6] etc. In this work, we propose
a much simpler the clustering algorithm for projecting fea-
tures to labels (Section 3.3), by using penalties on pairwise
distances in the segment feature metric space. To this end,
we propose a loss function that focuses on fusable and sep-
arable edges independently (Equations (7), (8) and (9)).
We reuse the same threshold ‚àÜV here to place constraints
along the edges of the graph.

PointsMeshingNormal Estimation (kNN)kNNneighbours(edges)GS (direct edges,normals)GS (original; internally extracts edges, computes normals)Segment labelsAdjacency Matrix (A)Datasets with meshed faces (e.g.ScanNet)Datasets which do not have meshing info (faces) e.g.S3DISFeatures()()Lsegment = wfuseLfuse + wsepLsep

Lsep =

1
|Esep|

(cid:88)

eij ‚ààEsep

H(‚àÜV ‚àí d( ¬ØFi, ¬ØFj))

(7)

(8)

Lfuse =

1
|Efuse|

(cid:88)

eij ‚ààEfuse

H(d( ¬ØFi, ¬ØFj) ‚àí ‚àÜV )

(9)

where Esep and Efuse denote the set of edges which
should be kept separate and fused respectively, decided us-
ing ground truth instance information.

Since the number of objects in a scene are Ô¨Ånite, the
number of edges between dissimilar objects are much lower
than the edges within the same object. This inÔ¨Çuences the
relative weighing of wfuse and wsep, which we set to be 1
and 0.01 respectively, to counter this imbalance.

3.3. Connected Component Labelling (CCL)

To convert the segment features ¬ØF to fusion decisions,
we employ a connected component labelling algorithm over
the feature space that ¬ØF resides in. Pairwise-distances are
computed d( ¬ØFi, ¬ØFj) ‚àÄ eij ‚àà E and thresholded using ‚àÜV ,
depicted through a piecewise function as in Equation 10.
Thus, each positive entry of Bij indicates a fusing decision
between segments i and j.

(cid:40)

Bij =

1 d( ¬ØFi, ¬ØFj) < ‚àÜV ‚àÄ eij ‚àà E
0

otherwise

(10)

For every pair of indices for which Bij = 1, we use an efÔ¨Å-
cient disjoint-set forest to implement Union-Find to record
the connectivity information [4]. For every connected com-
ponent composed of a set of segments, we compute the
mean semantic probability vector of the component and
identify the most probable semantic label for the entire com-
ponent, thus forming the Ô¨Ånal set of predictions SSF . This
procedure enables us to (i) identify and assign a single label
to an entire object, and (ii) allow the Segment-Fusion block
to override erroneous point-wise semantic predictions for
parts of the object by using consensus information across
larger parts of the object.

4. Evaluation

4.1. Training Setup

We apply and evaluate the proposed Segment-Fusion
method with multiple state-of-the-art backbones of seman-
tic segmentation across ScanNet and S3DIS datasets us-
ing the mean Intersection-over-Union (mIoU) metric. To
assess qualitative improvements with Segmet-Fusion, we
also compare results with and without Segment-Fusion (and
show how it helps in regions with part misclassiÔ¨Åcation).

4.2. ScanNet Dataset

We pick three state-of-the-art semantic segmentation
[10], PointConv [36] and
backbones ‚Äì SparseConvNet
MinkowskiNet
[3]. We augment each of these seman-
tic backbones with an instance backbone trained with the
losses proposed in Occuseg [12]. On the ScanNet val-
idation set, as shown in Table 1, we observe signiÔ¨Åcant
improvements of 4.4%, 5.1% and 2.8% respectively in
mIoU scores. Also, we notice a consistent upswing in IoU
scores across all classes. Qualitative improvements can be
observed in illustrations of speciÔ¨Åc point clouds in Fig-
ure 6. Thus, we observe that the improvements provided by
Segment-Fusion do not depend speciÔ¨Åcally on the choice
of the semantic segmentation backbone used. On the Scan-
Net test set, we applied Segment-Fusion on the Minkowsk-
iNet semantic backbone and obtained the 5th position on
the leaderboard (74.7% mIoU), obtaining a signiÔ¨Åcant im-
provement of 2.3% in mIoU score.

4.3. S3DIS Dataset

To demonstrate the effectiveness of our approach on
datasets that do not have mesh information, we evaluate
Segment-Fusion on the Area 5 set from S3DIS dataset. We
choose three semantic backbone networks: MinkNet18 [3],
KPConv [29] and ASIS [35] to show improvements with
Segment-Fusion. Table 2 presents semantic segmentation
results on the S3DIS Area 5 set on a variety of seman-
tic backbones. We augment each of these semantic back-
bones with instance features from ASIS [35]. The results
show that we obtain 1.5%, 0.6% and 0.8% improvement in
mIoU on using Segment-Fusion over MinkNet18 [3], KP-
Conv [29] and ASIS [35] respectively.

4.4. Comparisons with Occuseg clustering

We also assess Segment-Fusion with heuristic driven
graph clustering approach as employed in OccuSeg [12].
For this we use semantic and instance backbones simi-
lar to OccuSeg, and compare the improvement in seman-
tic segmentation performance obtained through the iterative
clustering algorithm they propose, with that of Segment-
Fusion. Figure 7 shows that Segment-Fusion outperforms
the heuristic driven iterative clustering algorithm across se-
mantic backbones; lending the conclusion that a learnable
algorithm like Segment-Fusion may be more scalable and
generic to apply on generic semantic and instance back-
bones as compared to a hand-crafted algorithm.

4.5. Ablation Studies

4.5.1

Impact of learning beyond J-GS

To better understand the effectiveness of our approach, we
study the individual contributions of the components of our

Table 1. Performance impact (mIoU) of Segment-Fusion on state-of-the-art semantic segmentation backbones on the ScanNet.

Model

SparseConvNet
SparseConvNet + SF

PointConv
PointConv + SF

MinkNet42
MinkNet42 + SF

MinkNet42
MinkNet42 + SF

t
e
S

val
val

val
val

val
val

test
test

l
l
a
w

82.4
85.3

74.1
78.6

84.3
86.7

83.9
85.6

r
o
o
Ô¨Ç

94.8
97.1

94.7
97.3

95.1
97.3

95.4
97.5

t
e
n
i
b
a
c

58.0
61.9

46.9
50.7

63.3
65.9

71.1
72.5

d
e
b

77.2
79.5

70.0
78.0

78.9
80.0

82.4
83.2

r
i
a
h
c

88.5
91.6

83.0
87.6

91.5
93.9

84.5
86.3

a
f
o
s

78.4
82.1

70.0
76.2

87.7
90.0

76.1
77.4

e
l
b
a
t

68.1
72.3

64.9
68.0

74.4
77.0

66
67.4

r
o
o
d

58.3
61.4

32.1
36.0

60.2
63.6

59
63

w
o
d
n
i
w

58.7
61.0

46.7
49.3

65.0
66.4

66.8
67.7

f
l
e
h
s
k
o
o
b

70.1
71.6

68.4
75.1

80.1
82.9

80
81.2

e
r
u
t
c
i
p

29.4
36.1

11.7
13.3

24.9
26.5

19.7
22

r
e
t
n
u
o
c

61.9
71.0

56.6
64.2

65.1
68.9

50.5
54.2

k
s
e
d

56.7
61.7

52.4
59.9

65.8
68.7

59.4
59.3

n
i
a
t
r
u
c

62.3
65.8

58.2
62.9

78.1
80.7

82.1
82.9

r
o
t
a
r
e
g
d
i
r
f
e
r

46.6
47.1

36.6
38.1

55.4
55.7

19.7
22

n
i
a
t
r
u
c
r
e
w
o
h
s

61.5
69.9

46.8
54.1

69.9
72.8

89.1
93.4

t
e
l
i
o
t

91.2
95.5

83.2
89.9

92.2
95.4

88.7
92.6

k
n
i
s

63.7
73.8

58.0
63.6

69.1
76.0

72.8
77.6

b
u
t
h
t
a
b

84.0
92.3

77.3
90.5

86.4
91.7

91.2
97.4

e
r
u
t
i
n
r
u
f
r
e
h
t
o

50.3
54.0

34.6
38.2

61.0
64.5

54.5
56.1

n
a
e
m

67.1
71.5

58.3
63.6

72.4
75.2

72.4
74.7

Figure 6. Qualitative results of Segment-Fusion on some sample point clouds of the ScanNet validation set using the MinkowskiNet-42 [3],
PointConv [36] and SparseConvNet [10] semantic backbones. The highlighted regions indicate mispredictions in the base semantic model
(S). Note how Segment-Fusion builds consensus using the instance features to correct the semantic predictions in these areas (without
degrading the performance in the other regions).

system. Table 3 compares the mIoU of the 3 semantic seg-
mentation backbones employing (i) only graph segmenta-
tion based voting (J-GSV) without any fusion network, (ii)
J-GS along with Segment-Fusion (SF), but trained only with
instance losses (Linstance), and (iii) SF trained with both seg-
ment and instance losses (Lsegment + Linstance).
In J-GSV,
we obtain semantic labels by performing majority voting
within the segment; while in SF, semantic labels are com-
puted as described in Section 3.3. It is noteworthy that em-
ploying only Linstance degrades performance, essentially be-
cause we use a simplistic CCL algorithm to compute labels
from the features; using only instance loss results in over-
fusion of segments, leading to poorer performance. We
observe a signiÔ¨Åcant performance improvement on adding
segment losses to constrain individual edges. Thus, noting
steady improvements in mIoU on account of J-GS and SF,
we conclude that (i) a hierarchical approach consisting of
abstracting pointwise features to segment-wise features and
(ii) learnable methods to fuse these segments are both re-
quired and effective.

Table 3. Impact of individual components of Segment-Fusion on
overall mIoU (ScanNet validation set) (Lins = Linstance, Lseg =
Lsegment)

Model

Base

Base +
J-GSV

Base +
SF(Lins)

Base +
SF(Lins+Lseg )

SparseConvNet
PointConv
MinkNet42

67.1
58.3
72.4

70.4
61.5
74.6

69.7
61.4
73.11

71.5
63.6
75.2

4.5.2 Does J-GS result in over-fusion?

While graph segmentation aids in abstracting the problem
to a higher level spatially, it runs the risk of over-fusing
points belonging to two different classes into one segment.
In this section, we evaluate the impact of such occurrences
on overall performance using the proposed J-GSV score.
For each point cloud, we analyse the base semantic pre-
dictions S with the predictions after J-GSV Smaj(without
Segment-Fusion Network) and count the number of points

GT Semantic LabelsBase Semantic LabelsSF InstancesSF Semantic LabelsBase Instance FeaturesMinkowskiNetPointConvSparseConvNetTable 2. Impact of Segment-Fusion on state-of-the-art semantic segmentation back-
bones on the S3DIS Area 5

r
e
t
t
u
l
c

51.4
52.4

54.3
55.2

41.5
43.2

m
a
e
b

0.0
0.0

0.0
0.0

0.0
0.0

d
r
a
o
b

66.5
69.3

60.9
61.8

52.5
61.6

e
s
a
c
k
o
o
b

67.4
67.2

72.5
73.3

0.0
0.0

g
n
i
l
i
e
c

92.3
93.0

92.3
92.4

89.6
89.7

r
i
a
h
c

87.1
88.4

89.7
89.5

1.4
0.0

n
m
u
l
o
c

34.9
37.6

20.2
20.7

7.8
2.9

r
o
o
d

68.4
67.7

73.2
75.6

17.2
16.9

r
o
o
Ô¨Ç

95.9
96.7

96.8
96.9

96.5
96.6

a
f
o
s

59.8
67.7

70.1
70.0

0.0
0.0

e
l
b
a
t

76.0
76.5

77.9
77.9

1.4
0.0

l
l
a
w

81.0
81.6

80.1
80.7

73.1
74.5

w
o
d
n
i
w

49.2
51.2

52.0
53.4

39.9
43.4

n
a
e
m

63.8
65.3

64.6
65.2

32.4
33.2

Model

MinkNet18
MinkNet18 + SF

KPConv
KPConv + SF

ASIS
ASIS + SF

for which they disagree with each other. We deÔ¨Åne over-
fusion to occur when the J-GSV predictions are incorrect
while the base predictor is correct. In Figure 8 we observe
that J-GS based voting helps in an improvement of atleast
2% in ‚àº 32% of the point clouds in the validation set of the
ScanNet dataset, while being 2% incorrect in ‚àº 1% of the
point clouds. This indicates that J-GSV is consistently ben-
eÔ¨Åcial across point clouds of the dataset and provides higher
improvement than the degradation due to overfusion.

Figure 7. Comparison between the semantic
segmentation performance of Occuseg‚Äôs clus-
tering algorithm and Segment-Fusion‚Äôs learn-
able fusion algorithm.

Figure 8. Top: Fraction of points in a point cloud where J-GSV
is incorrect and the base semantic predictor is correct. Bottom:
Fraction points in a point cloud where J-GSV is correct and the
base semantic predictor is incorrect.

4.5.3 J-GS on S3DIS

We present an ablative study to describe the methodology
of selecting the thresholds for the proposed J-GS method.
Figure 9 illustrates the performance of J-GSV on the train-
ing and validation sets of the S3DIS dataset. As discussed
in Section 3.1, we evaluate the performance of various J-
GSV variants on the training set to select the J-GS parame-
ters and apply them to the validation set. It is observed that
on an average, sorting of edge weights along feature sim-
ilarity performs better than sorting along similarity along
normals. Also, relying solely on normals ends up reduc-
ing performance as compared to the base validation perfor-
mance (without J-GSV). We observe that (s : f eats, fth :
0.5) provides optimal performance for the training set, and
use this to apply J-GS on the validation set and generate
segment-wise data for Segment-Fusion Network. To evalu-
ate how well these parameters generalize from the training

Figure 9. J-GS Voting Score results for the S3DIS validation set
across a variety of feature thresholds. The threshold used in the
normals space nth is 0.01; results along this axes are omitted here
due to lack of sensitivity.

to the validation set, we evaluate the same parameter set
on the validation set as well. Though (s : f eats, fth : 2)
performs better on the validation set, the difference with
fth : 0.5 is low (lower than the gains we see with SF down-
stream, Table 2). On the validation set, the sorting parame-
ter s has a higher sensitivity than the feature-threshold pa-
rameter. The ScanNet dataset has distinct normals, hence
we did not observe much sensitivity with the fth threshold.

5. Limitations and Future Work

While we have demonstrated the effectiveness of our ap-
proach on multiple semantic and instance backbones, the
method still requires supervision. There is also potential in
using deeper and more complex network architectures in the
Segment-Fusion network block to increase improvements.
In the future, we plan to extend this work by using similar
strategies to fuse learnings from problems with shared con-
text, along with focusing on semi/self-supervised learning.

6. Conclusion

We presented Segment-Fusion, a learnable, hierarchical
method to fuse segments, aimed at improving semantic seg-

 6 S D U V H & R Q Y 1 H W 3 R L Q W & R Q Y 0 L Q N 1 H W   6 H P D Q W L F  E D F N E R Q H V    P , R 8  * D L Q      2 F F X V H J  6 H J P H Q W  ) X V L R Q                   3 R L Q W  F O R X G V  L Q  6 F D Q 1 H W  9 D O L G D W L R Q  6 H W      : U R Q J -  * 6 9  : U R Q J  	  % D V H  3 U H G  & R U U H F W -  * 6 9  & R U U H F W  	  % D V H  3 U H G  : U R Q J             ) H D W X U H  7 K U H V K R O G                               P , R 8   Y D O  V H W  % D V H  9 D O  P , R 8                                   P , R 8   W U D L Q  V H W  Y D O   V R U W   Q R U P D O V Y D O   V R U W   I H D W X U H V W U D L Q   V R U W   Q R U P D O V W U D L Q   V R U W   I H D W X U H Vmentation performance of generic base models. We pro-
posed a voting based algorithm to select optimal graph seg-
mentation hyperparameters for varying datasets. Our learn-
able method can be Ô¨Çexibly used in conjunction with ex-
isting semantic/instance models. Finally, we demonstrated
comprehensive evaluations to demonstrate the effectiveness
of our methods on multiple types of semantic backbones.

References

[1] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.
Joint 2d-3d-semantic data for indoor scene understanding.
arXiv preprint arXiv:1702.01105, 2017. 1, 2, 4

[2] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and
Xinggang Wang. Hierarchical aggregation for 3d instance
In Proceedings of the IEEE/CVF Interna-
segmentation.
tional Conference on Computer Vision, pages 15467‚Äì15476,
2021. 1, 3

[3] Christopher Choy, JunYoung Gwak, and Silvio Savarese.
4d spatio-temporal convnets: Minkowski convolutional neu-
ral networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 3075‚Äì
3084, 2019. 1, 2, 3, 6, 7

[4] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest,
and Clifford Stein. Introduction to algorithms. MIT press,
2009. 6

[5] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nie√üner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes.
In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 5828‚Äì5839, 2017. 1, 2, 3, 4
[6] Konstantinos G Derpanis. Mean shift clustering. Lecture

Notes, page 32, 2005. 2, 3, 5

[7] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian
Leibe, and Matthias Nie√üner. 3d-mpa: Multi-proposal ag-
In Pro-
gregation for 3d semantic instance segmentation.
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 9031‚Äì9040, 2020. 1, 3
[8] Pedro F Felzenszwalb and Daniel P Huttenlocher. EfÔ¨Åcient
International journal of

graph-based image segmentation.
computer vision, 59(2):167‚Äì181, 2004. 3, 4

[9] Jingyu Gong, Jiachen Xu, Xin Tan, Haichuan Song, Yanyun
Qu, Yuan Xie, and Lizhuang Ma. Omni-supervised point
cloud segmentation via gradual receptive Ô¨Åeld component
In Proceedings of the IEEE/CVF Conference
reasoning.
on Computer Vision and Pattern Recognition, pages 11673‚Äì
11682, 2021. 1

[10] Benjamin Graham, Martin Engelcke, and Laurens Van
3d semantic segmentation with submani-
Der Maaten.
In Proceedings of the
fold sparse convolutional networks.
IEEE conference on computer vision and pattern recogni-
tion, pages 9224‚Äì9232, 2018. 1, 2, 3, 6, 7

[11] Saurabh Gupta, Pablo Arbel¬¥aez, Ross Girshick, and Jiten-
dra Malik.
Indoor scene understanding with rgb-d im-
ages: Bottom-up segmentation, object detection and seman-
tic segmentation. International Journal of Computer Vision,
112(2):133‚Äì149, 2015. 1

[12] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg:
Occupancy-aware 3d instance segmentation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pages 2940‚Äì2949, 2020. 1, 2, 3, 4, 5, 6

[13] Ji Hou, Angela Dai, and Matthias Nie√üner. 3d-sis: 3d seman-
tic instance segmentation of rgb-d scans. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 4421‚Äì4430, 2019. 1

[14] Hanzhe Hu, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, and
Junjie Yan. Class-wise dynamic graph convolution for se-
mantic segmentation. In Computer Vision‚ÄìECCV 2020: 16th
European Conference, Glasgow, UK, August 23‚Äì28, 2020,
Proceedings, Part XVII 16, pages 1‚Äì17. Springer, 2020. 2

[15] Wenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, and
Tien-Tsin Wong. Bidirectional projection network for cross
In Proceedings of the
dimension scene understanding.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 14373‚Äì14382, 2021. 1, 2

[16] Zeyu Hu, Xuyang Bai, Jiaxiang Shang, Runze Zhang, Jiayu
Dong, Xin Wang, Guangyuan Sun, Hongbo Fu, and Chiew-
Lan Tai. Vmnet: Voxel-mesh network for geodesic-aware
3d semantic segmentation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 15488‚Äì
15498, 2021. 1, 2

[17] Zeyu Hu, Mingmin Zhen, Xuyang Bai, Hongbo Fu, and
Chiew-lan Tai. Jsenet: Joint semantic segmentation and edge
detection network for 3d point clouds. In Computer Vision‚Äì
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23‚Äì28, 2020, Proceedings, Part XX 16, pages 222‚Äì239.
Springer, 2020. 1, 2

[18] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-
Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point group-
In Proceedings of the
ing for 3d instance segmentation.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 4867‚Äì4876, 2020. 1

[19] Klaas Klasing, Dirk Wollherr, and Martin Buss. A clustering
method for efÔ¨Åcient segmentation of 3d laser data. In 2008
IEEE international conference on robotics and automation,
pages 4043‚Äì4048. IEEE, 2008. 4

[20] Abhijit Kundu, Xiaoqi Yin, Alireza Fathi, David Ross, Brian
Brewington, Thomas Funkhouser, and Caroline Pantofaru.
Virtual multi-view fusion for 3d semantic segmentation. In
European Conference on Computer Vision, pages 518‚Äì535.
Springer, 2020. 1

[21] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and
Kui Jia. Instance segmentation in 3d scenes using semantic
superpoint tree networks. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 2783‚Äì
2792, 2021. 1, 3

[22] Alexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe,
and Francis Engelmann. Mix3d: Out-of-context data aug-
mentation for 3d scenes. arXiv preprint arXiv:2110.02210,
2021. 1

[23] Quang-Hieu Pham, Thanh Nguyen, Binh-Son Hua, Gemma
Jsis3d: Joint semantic-instance
Roig, and Sai-Kit Yeung.
segmentation of 3d point clouds with multi-task pointwise
networks and multi-value conditional random Ô¨Åelds. In Pro-

[36] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep
convolutional networks on 3d point clouds. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 9621‚Äì9630, 2019. 1, 6, 7

[37] Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi
Wei, and Qiao Fei. Ds-slam: A semantic visual slam to-
In 2018 IEEE/RSJ Interna-
wards dynamic environments.
tional Conference on Intelligent Robots and Systems (IROS),
pages 1168‚Äì1174. IEEE, 2018. 1

[38] Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang,
Yunhai Tong, and Philip HS Torr. Dual graph convolu-
tional network for semantic segmentation. arXiv preprint
arXiv:1909.06121, 2019. 2

ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 8827‚Äì8836, 2019. 1, 3
[24] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiÔ¨Åcation
In Proceedings of the IEEE conference
and segmentation.
on computer vision and pattern recognition, pages 652‚Äì660,
2017. 2

[25] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-
net++: Deep hierarchical feature learning on point sets in a
metric space. arXiv preprint arXiv:1706.02413, 2017. 2
[26] Erich Schubert, J¬®org Sander, Martin Ester, Hans Peter
Kriegel, and Xiaowei Xu. Dbscan revisited, revisited: why
and how you should (still) use dbscan. ACM Transactions on
Database Systems (TODS), 42(3):1‚Äì21, 2017. 2, 3, 5
[27] Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek,
Senthil Yogamani, Martin Jagersand, and Hong Zhang. A
comparative study of real-time semantic segmentation for
In Proceedings of the IEEE confer-
autonomous driving.
ence on computer vision and pattern recognition workshops,
pages 587‚Äì597, 2018. 1

[28] Johannes Strom, Andrew Richardson, and Edwin Olson.
Graph-based segmentation for colored 3d laser point clouds.
In 2010 IEEE/RSJ international conference on intelligent
robots and systems, pages 2131‚Äì2136. IEEE, 2010. 3, 4
[29] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,
Beatriz Marcotegui, Franc¬∏ois Goulette, and Leonidas J
Guibas. Kpconv: Flexible and deformable convolution for
point clouds. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 6411‚Äì6420, 2019. 2,
6

[30] Abhinav Valada, Johan Vertens, Ankit Dhall, and Wol-
fram Burgard. Adapnet: Adaptive semantic segmentation
In 2017 IEEE Inter-
in adverse environmental conditions.
national Conference on Robotics and Automation (ICRA),
pages 4644‚Äì4651. IEEE, 2017. 1

[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998‚Äì6008, 2017. 5
[32] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and
Jie Shan. Graph attention convolution for point cloud se-
mantic segmentation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
10296‚Äì10305, 2019. 2

[33] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,
and Xin Tong. O-cnn: Octree-based convolutional neu-
ral networks for 3d shape analysis. ACM Transactions On
Graphics (TOG), 36(4):1‚Äì11, 2017. 1, 2

[34] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neu-
mann. Sgpn: Similarity group proposal network for 3d
In Proceedings of the
point cloud instance segmentation.
IEEE conference on computer vision and pattern recogni-
tion, pages 2569‚Äì2578, 2018. 1, 3

[35] Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, and
Jiaya Jia. Associatively segmenting instances and semantics
in point clouds. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 4096‚Äì
4105, 2019. 1, 3, 5, 6

