The Impact of Virtual Reality and Viewpoints
in Body Motion Based Drone Teleoperation

Matteo Macchini, Student Member, IEEE, Manana Lortkipanidze, Fabrizio Schiano, Member, IEEE,
and Dario Floreano, Senior Member, IEEE *

1
2
0
2

n
a
J

0
3

]

O
R
.
s
c
[

1
v
6
2
2
0
0
.
2
0
1
2
:
v
i
X
r
a

Figure 1: Spontaneous body motion acquisition during a robot imitation task using different viewpoints. The acquired data can be
used to deﬁne personalized Body-Machine Interfaces. (top) VR-disabled conditions. (bottom) VR-enabled conditions.

ABSTRACT

1 INTRODUCTION

The operation of telerobotic systems can be a challenging task, re-
quiring intuitive and efﬁcient interfaces to enable inexperienced
users to attain a high level of proﬁciency. Body-Machine Interfaces
(BoMI) represent a promising alternative to standard control devices,
such as joysticks, because they leverage intuitive body motion and
gestures. It has been shown that the use of Virtual Reality (VR) and
ﬁrst-person view perspectives can increase the user’s sense of pres-
ence in avatars. However, it is unclear if these beneﬁcial effects occur
also in the teleoperation of non-anthropomorphic robots that display
motion patterns different from those of humans. Here we describe
experimental results on teleoperation of a non-anthropomorphic
drone showing that VR correlates with a higher sense of spatial
presence, whereas viewpoints moving coherently with the robot are
associated with a higher sense of embodiment. Furthermore, the
experimental results show that spontaneous body motion patterns
are affected by VR and viewpoint conditions in terms of variability,
amplitude, and robot correlates, suggesting that the design of BoMIs
for drone teleoperation must take into account the use of Virtual
Reality and the choice of the viewpoint.

Keywords: Virtual Reality. Presence. Human-Robot Interfaces.
Human Body Motion.

Index Terms: Human-centered computing—Human computer
interaction (HCI)—Interaction paradigms—Virtual reality Human-
centered computing—Human computer interaction (HCI)—HCI
design and evaluation methods—User studies

*The authors are with the Laboratory of Intelligent Systems, ´Ecole Poly-
technique F´ed´erale de Lausanne, CH-1015 Lausanne (EPFL), Switzerland.

Telerobotic systems are needed in many ﬁelds in which human cog-
nition and decision-making capacities are still crucial to accomplish
a mission [14]. Such ﬁelds include but are not limited to navigation
in challenging and unstructured environments, search and rescue
missions, and minimally invasive surgery [5, 12, 16, 25]. To provide
ﬁne control of the telerobotic system, the implementation of an ef-
ﬁcient Human-Robot Interface (HRI) is crucial. Most telerobotic
applications are currently restricted to a small set of experts who
need to undergo long training processes to gain experience and ex-
pertise in the task [7, 9]. With the fast advancements in the ﬁeld of
robotics, new systems require control interfaces that are sufﬁciently
powerful and intuitive also for inexperienced users [26].

Body-Machine Interfaces (BoMIs) are the subdomain of HRIs
that consist of the acquisition and processing of body signals for the
generation of control inputs for the telerobotic system [6]. BoMIs
are showing great potential in improving user’s comfort and perfor-
mance during the operation of mobile robots [24], representing a
more intuitive alternative to standard interfaces. The acquisition of
body motion is particularly suitable to be applied to BoMIs due to
the natural control capabilities that humans exert and train on it by
daily activities. The scientiﬁc literature offers several examples of
motion-based interfaces, proposing a heterogeneous set of robots
to be controlled and a multitude of methods to track the operator’s
motion.

Among other mobile robots, drones are showing disruptive poten-
tial in both industrial and research applications [11, 13, 32]. In the
implementation of motion-based BoMIs, one of the most challenging
aspects is the deﬁnition of a mapping function, which translates the
user’s body motion into robot commands. For non-anthropomorphic
systems, mapping functions can be designed based on the observa-
tion of the user’s spontaneous behavior while they imitate the motion
of the robot performing a set of predeﬁned actions (Fig. 1). This
procedure, known as calibration or imitation phase, has been used in

 
 
 
 
 
 
prior works to identify the spontaneous motion patterns of users for
the control of different robots [22, 24, 28]. Some key metrics used
to discriminate the relevance of body motion features are their mo-
tion amplitude and their correlation with the robot’s motion. Some
studies have been dedicated solely to the identiﬁcation of common
human motion patterns following this paradigm, in the case of both
anthropomorphic and non-anthropomorphic robotic systems [8, 28].
Several interfaces have been proposed for the motion-based con-
trol of drones, based on the use of different body parts such as
hands, torso, or the user’s full body [21, 29, 31]. These works show
that BoMIs can outperform standard interfaces, such as remote
controllers, both in terms of performance and in user’s personal
preference.

Among the metrics used to qualify an HRI, presence is a subjec-
tive measure of the feeling of ”being there” in the virtual or distal
scene [36]. Different examples in the literature support the hypothe-
sis that increasing the sense of presence of an operator can improve
their performance in controlling the robot [20, 37, 40]. The concept
of presence can be split into three dimensions: spatial presence,
self-presence, and social presence [19]. In our study, we focus on
the ﬁrst two as social presence requires the interaction with different
agents, which is not the case in most telerobotic missions. While
spatial presence is relative to the feeling of being surrounded by the
virtual environment, self-presence deﬁnes the shift of the user’s per-
ception of self from their own body into the virtual or distal one [17].
A strong sense of self-presence can improve the operator’s sense of
embodiment and give them the sensation of being the robot, instead
of merely controlling it. The sense of self-presence and the concept
of embodiment are thus closely related. Several factors concur to
enhancing the sense of presence in the user during teleoperation,
including the insulation from the real environment, and the amount
and type of provided multi-modal feedback [30].

The change of viewpoint can strongly alter the perception of a
virtual environment. Some works correlate the ﬁrst-person view
(1V) to a higher sense of embodiment [27, 35]. Studies in the ﬁeld
of video games conclude that 1V is also associated with higher per-
formance in manipulation tasks, and in general when an interaction
with static objects is needed [39]. The same research states that
a third-person view (3V) can increase the user’s sense of spatial
presence in the environment, which translates into a higher capacity
of navigation and perception of the surrounding space. Nonetheless,
this advantage comes at the cost of a lower sense of embodiment.
Gorisse et al. studied the effects of different viewpoints (1V and
3V) on the sense of presence and embodiment during the control
of an anthropomorphic avatar [15]. Through a survey, they con-
cluded that the viewpoint impact on spatial presence is very limited.
However, the ﬁrst-person view positively and signiﬁcantly affects
embodiment.

VR applications are supported by a Head-Mounted Display
(HMD), a device that a user wears on their head, which provides
stereoscopic vision and can track the user’s head and eyes to provide
them with the ability to explore the virtual environment (VE). Prior
literature investigated the effects of viewpoint in VEs as a compar-
ison with standard ﬂat displays, conﬁrming the higher degree of
immersion in 1V [10]. Also, some studies show that large freedom
of motion positively inﬂuences the sense of presence [30, 34].

Despite the extensive work conducted on the link between VR,
viewpoint, and presence for the control of anthropomorphic robots
and virtual avatars, few studies have been conducted in the control
of non-anthropomorphic robots, which are the most common type
of robotic systems. Research shows that humans can identify them-
selves with agents presenting visual aspects and kinematics different
from the human body, provided that they present human-like motion
and move synchronously with the user [2, 3]. The same concept has
been extended to virtual supernumerary robotic arms [38].

(b) the use of VR on the teleoperation of a ﬁxed-wing drone, as an
example of a non-anthropomorphic robot with non-human motion
behaviors. Speciﬁcally, we conducted a set of experiments to assess:

• The effects of viewpoint and VR on the user’s sense of spatial
presence and embodiment, when they perceive the environment
and move as a ﬁxed-wing drone.

• The effects of viewpoint and VR on the user’s spontaneous
body motion when they are asked to mimic the drone’s behav-
ior with their body. We focus our analysis on three dimensions
of body motion: variability, correlation with the robot’s move-
ments, and gesture amplitude.

2 METHODS
2.1 Simulation
The simulation environment used in this work is based on the
Unity3D engine. We used a robot model reproducing the dynamics
of the commercial drone eBee by SenseFly1. The drone’s attitude
is stabilized through a PID controller and the speed is regulated to
a constant value of 8m/s. The simulation displays a sequence of 4
maneuvers performed by the ﬁxed-wing drone: two roll maneuvers
(right, left), and two pitch maneuvers (up, down), after a horizontal
ﬂight section. Each maneuver’s duration was set to T = 8s to give
the participant enough time to recognize the robot’s behavior, for a
total duration of 32s per experiment. We display a red path in front
of the drone to inform the participants of the drone’s future trajectory
(Fig. 2B). We considered 3 different conditions on the viewpoints:
a standard ﬁrst-person view (1V) from the drone’s front camera, a
third-person view from behind the drone (3V), and a third-person
view from a ground observer (GV). While most teleoperation tasks
are carried out in 1V and GV condition, the inclusion of 3V allows
us to decouple the effects of immersive viewpoints (1V vs. 3V, GV)
and the effects of the camera moving together with the robot (1V,
3V vs. GV). The 3V behind the drone follows at a constant distance
and rotates with the drone. The three viewpoints are depicted in Fig.
2B. We performed the experiments with two types of visual displays.
In the VR condition, subjects used an Oculus Rift S HMD. In the
non-VR condition, subjects saw the scene on a computer monitor
positioned at a distance of 1.5m. In total, there are 6 experimental
conditions, hereafter referred to as in Table 1.

Table 1: Experimental conditions

First-person view (1V)
Third-person view (3V)
Ground view (GV)

non-VR
1V-N
3V-N
GV-N

VR
1V-V
3V-V
GV-V

2.2 Apparatus
We run the experimental sessions in a room equipped with an Op-
tiTrack Motion Capture (MoCap) System to track the participants’
body. Body motion tracking was performed through a set of 25
reﬂective markers strapped on a velcro vest worn by each subject.
The subjects’ upper body was modeled as the concatenation of 13
different rigid bodies interconnected by sphere joints, as depicted
in Fig. 2A. We consider only 9 rigid bodies for our study: torso,
shoulders, arms, forearms, and hands. This representation has al-
ready been adopted in relevant previous studies and demonstrated
to be sufﬁciently powerful to derive personalized BoMIs for drone
teleoperation [22, 24]. Moreover, being the representation decou-
pled by construction, it prevents redundancy which might affect

In this paper, we study the effects of (a) different viewpoints and

1https://www.senseﬂy.com

Figure 2: Overview of the experimental protocol. (A) Data acquisition scenario during the imitation task. The participant is free to move
spontaneously to mimic the robot’s behavior during a set of predeﬁned maneuvers. During this phase, their body is tracked by a motion capture
system, and synchronous data are acquired for the drone’s trajectory and the user’s motion. (B) Experimental conditions. A total of 30 participants
took part in the experiment. We assigned each participant to one group, corresponding to one of the viewpoint conditions: ﬁrst-person view (1V),
third-person view (3V), or ground-view (GV). Each subject took part in the imitation task with and without the use of VR.

the subsequent data analysis. The orientation of each rigid body,
expressed as a quaternion, was recorded at a frequency of 100Hz.
During the experiments, we acquired synchronous data from both
the drone simulator and the MoCap. Encoded body pose and drone
actions were streamed through a UDP protocol and concatenated
into a dataset for subsequent analysis.

2.3 Presence
The sense of presence was measured through the post-experimental
questionnaire shown in Table 2. The questionnaire consisted of
seven questions, where each item was given on a semantic scale
from one to ﬁve (one corresponds to not at all, ﬁve corresponds
to completely). The questions were designed to investigate two
dimensions of presence:

• Embodiment, composed of three items representing two dif-
ferent embodiment dimensions: self-location and ownership
of the virtual body [17]. The sense of agency was purpose-
fully neglected since no actual teleoperation happens during
our experiment. Questions are adapted from previous litera-
ture [1, 10, 15].

• Spatial presence, composed of four items, refers to the sense
of environmental location and it is originally adapted from the
MEC-SPQ test [41].

2.4 Participants
30 volunteers participated in our user study. All subjects had no
know prior experience of motion sickness or discomfort using VR
headsets, and a correct or corrected sight. The age of the participants
varied from 20 to 31 years (24.29 ± 2.78) and 83% of participants
were male. Informed consent was obtained from everyone before the
experiment and the study was conducted while adhering to standard
ethical principles.2.

2.5 Procedure
Every subject ﬁlled a questionnaire about personal information be-
fore participating in the experiment. Later, they were asked to sit

2The experiments were approved by the Human Research Ethics Com-

mittee of the ´Ecole Polytechnique F´ed´erale de Lausanne.

Table 2: Presence questionnaire composed of two blocks: Embodi-
ment (E1-E3) and Spatial Presence (SP1-SP4).

ID
E1

E2

E3

SP1

SP2

SP3

SP4

Question
To what extent did you feel that you were located inside
the virtual body?
To what extent did you feel that the virtual body was
your own body?
To what extent did you did you forget your actual body
in favor of the virtual body?
To what extent did you feel that you were actually there
in the virtual environment?
To what extent did you feel that the objects in the virtual
environment surrounded you?
To what extent did it seem to you that your true location
had shifted into the virtual environment?
To what extent did you feel that you were physically
present in the virtual environment?

on a stool in front of the computer and were shown the simulation
through the screen or the VR headset depending on the group to
which they were pseudo-randomly assigned. During this procedure,
they were asked to move spontaneously, as if they were controlling
the drone’s ﬂight. Since the maneuvers were predeﬁned, no control
was exerted on the simulator by the user.

Each participant took place in the calibration phase with two dif-
ferent conditions: with and without VR, and was pseudo-randomly
assigned a viewpoint among the options above (1PV, 3V, 3PV). The
simulation was shown twice per condition to each participant: a ﬁrst
one to get familiar with the scenario, and a second one for the actual
data acquisition. The order of the experiment (with/without VR)
they performed was determined pseudo-randomly to compensate for
possible bias caused by the previous simulation. At the end of the
experiment, each subject ﬁlled the presence questionnaire shown in
Table 2.

Body motionRobot motion1V3VGVNO VRVRx 10ABx 10x 10x 10x 10x 102.6 Motion data preprocessing

As mentioned in 2.2, raw data correspond to a set of signals consist-
ing of time series of orientation information expressed in quaternions
for the body motion, and two additional signals representing the
drone roll and pitch angles during the experiment. The user’s upper
body was modeled as a kinematic chain consisting of 9 rigid bodies,
interconnected through sphere joints (Fig. 2A). We ﬁrst computed
the relative orientation of each rigid body with respect to its parent
limb in the human kinematic chain. For example, the shoulder ro-
tation is expressed as the relative rotation with respect to the torso,
and the arm rotation with respect to the shoulder. Subsequently, the
initial rotation of the body segment was reset to zero to compensate
for its initial bias and the orientations are converted from quaternions
to Euler angles. The order convention is chosen to minimize the risk
of gimbal lock. We ﬁltered the Euler Angles with a moving average
(N = 100) low-pass ﬁlter to mitigate the effects of quantization noise.
Finally, 27 angles (9 limbs x 3 angles) and 2 robot commands time
series (roll and pitch) were used for our analysis.

3 RESULTS

In this section, we report on the experimental results regarding the
presence survey, and the users’ body motion analysis.

3.1 Presence Survey

As a preliminary validation, Cronbach’s alpha coefﬁcient was cal-
culated to measure the reliability of the questionnaire across both
dimensions for all of the experimental conditions [4]. The coefﬁcient
was > 0.73 for all cases, thus indicating acceptable reliability of the
questionnaire. The Shapiro–Wilk test was carried out to check the
normality of the distributions of the answers to the post-experiment
questionnaire [33]. As not all variables followed a normal distribu-
tion, we used a non-parametric test (the Wilcoxon Signed-Ranks)
to compare objective performance data of paired groups, keeping
the VR variable constant (1V vs 3V vs GV) [42]. We used the
Mann–Whitney U test for the independent groups, keeping the view-
point constant (VR vs non-VR) [23].

Table 3 shows the survey results across all conditions and Table 4
the statistical signiﬁcance of our ﬁndings. Each column refers to a
pair of conditions, with pa,b representing the p-value relative to the
t-test applied between condition a and condition b.

VR affects Spatial Presence. In Table 4 (left) we show the survey
results across the VR condition. The p-values are calculated for each
viewpoint varying the VR condition using the Mann–Whitney U
test. From E1-E3 in Table 4 (left), we can observe that no signiﬁcant
difference was present between the VR groups and the non-VR
groups (p1N,1V , p3N,3V , pGN,GV > 0.05). We cannot thus conclude
that the use of VR affects the sense of embodiment. Instead, SP1-
SP4 show signiﬁcant differences for almost all cases for both 1V
and 3V. The ﬁrst-person view group reported higher values of spatial
presence when using VR (SP1V −N = 2.20 ± 0.78, SP1V −V = 3.20 ±
0.81). Also for third-person view the use of VR correlated with a
higher sense of spatial presence (SP3V −N = 2.71 ± 1.13, SP3V −V =
3.82 ± 1.19). This result suggests a higher sense of spatial presence
for 1V-V compared to 1V-N and of 3V-V compared to 3V-N. In
average, the sense of spatial presence increased by 45% for 1V and
40% for 3V when using VR. No signiﬁcant effects were observed
for the GV case. These results are highlighted in light blue in Table
4 (left) and summarized in Fig. 3.

Viewpoint affects Embodiment. In Table 4 (right) we show the
survey results across the viewpoint conditions. The p-values are
calculated between each pair of viewpoints using the Wilcoxon
Signed-Ranks Test. As a ﬁrst observation, we can see that no sig-
niﬁcant effects are observable for any of the questions SP1-SP4,
meaning that we cannot observe a signiﬁcant correlation between
the viewpoint and spatial presence. Secondly, no signiﬁcant effects
are observable in the non-VR case. On the other hand, there is a

Figure 3: Survey results relative to the Spatial Presence dimension
and average responses (Table 4, blue highlight). (∗∗ p < 0.01, ∗ p < 0.05)

Figure 4: Survey results relative to the Embodiment dimension and
average responses (Table 4, yellow highlight). (∗∗ p < 0.01, ∗ p < 0.05)

clear trend in the embodiment perception when changing the view-
point while using VR: both p1V,GV and p3V,GV are signiﬁcant in
E1-E3. GV group reported a lower sense of embodiment in aver-
age: (EGV −V = 1.91 ± 1.00) than 1V group (E1V −V = 3.50 ± 0.96,
p < 0.01) and 3V group (E3V −V = 3.00 ± 1.18, p < 0.01). The
sense of embodiment was higher by 83% for 1V and 57% for 3V
with respect to GV, only when using VR. These results are high-
lighted in light yellow in Table 4 (right) and summarized in Fig.

15SP1*15SP2*****15SP3**15SP4***1V-N1V-V3V-N3V-VGV-NGV-V15SP-All15E1***15E2***15E3***1V-N1V-V3V-N3V-VGV-NGV-V15E-AllTable 3: Mean and standard deviation of the responses to the presence survey in a scale 1 to 5

Condition
ID
E1
E2
E3
SP1
SP2
SP3
SP4

1V-N
1N
3.00 ± 0.94
2.80 ± 0.92
2.60 ± 1.07
2.40 ± 0.84
2.20 ± 0.79
2.20 ± 0.79
2.00 ± 0.82

3V-N
3N
2.92 ± 1.19
2.92 ± 1.04
2.62 ± 1.12
3.00 ± 1.29
2.31 ± 1.03
2.85 ± 1.07
2.69 ± 1.18

GV-N
GN
2.08 ± 1.12
2.38 ± 1.19
2.31 ± 1.18
2.85 ± 1.14
2.31 ± 1.11
2.62 ± 1.12
2.23 ± 1.17

1V-V
1V
3.60 ± 0.84
3.50 ± 1.08
3.40 ± 1.07
3.10 ± 0.99
3.30 ± 0.67
3.30 ± 0.95
3.10 ± 0.74

3V-V
3V
2.73 ± 1.27
2.82 ± 0.87
3.45 ± 1.37
4.00 ± 1.26
3.64 ± 1.12
3.91 ± 1.22
3.73 ± 1.35

GV-V
GV
1.82 ± 1.08
1.82 ± 0.98
2.09 ± 1.04
3.45 ± 1.04
3.27 ± 1.01
3.00 ± 1.00
3.09 ± 1.38

Table 4: Statistical signiﬁcance of the responses to the presence survey. p1N,1V , p3N,3V , pGN,GV refer to the VR condition, and are evaluated with
Mann–Whitney U test. p1N,3N , p1N,GN , p3N,GN , p1V,3V , p1V,GV , p3V,GV refer to the viewpoint condition, for which we used the Wilcoxon Signed-Ranks
test. Signiﬁcant p-values are shown in bold characters. p-values are considered signiﬁcant if p < 0.05. In yellow and blue, the sections relative to
the most relevant results.

VR Effect
p3N,3V
0.382
0.463
0.052
0.028
0.003
0.011
0.028

pGN,GV
0.297
0.106
0.357
0.133
0.026
0.199
0.065

p1N,1V
0.084
0.089
0.052
0.064
0.004
0.010
0.005

E1
E2
E3
SP1
SP2
SP3
SP4

non-VR
p1N,GN
0.028
0.129
0.272
0.151
0.436
0.209
0.397

Viewpoint Effect

p3N,GN
0.043
0.087
0.244
0.374
0.500
0.297
0.159

p1V,3V
0.064
0.085
0.341
0.029
0.128
0.067
0.072

VR
p1V,GV
0.001
0.002
0.008
0.220
0.485
0.315
0.485

p3V,GV
0.049
0.011
0.015
0.092
0.176
0.022
0.133

p1N,3N
0.411
0.447
0.487
0.117
0.383
0.056
0.087

E1
E3
E5
SP1
SP2
SP3
SP4

4.

3.2 Body Motion Analysis
The second part of the data analysis was dedicated to the body
motion data. These data were analyzed to extract relevant analo-
gies and differences between the spontaneous body motion patterns
during the calibration phase. Due to the limited amount of sub-
jects per condition (N=10), we chose non-parametric methods to
assess the signiﬁcance of our results. For motion data, we used the
Kruskal-Wallis test to assess the equality of the medians of differ-
ent groups [18]. We focus on three aspects of motion, which are
relevant for motion-based teleoperation: motion variability, human-
robot motion correlation, and gesture amplitude of different body
segments.

Spontaneous motion displays higher variability in Ground
View. First, we analyzed the intra-group motion variability. We
concatenated motion data for each subject to a single timeseries con-
taining the motion of all body segments (Fig. 5A). To quantify the
motion difference between subjects, the datasets were compressed
to two-dimensional data using Principal Component Analysis (PCA)
decomposition (Fig. 5B). PCA emphasizes the data covariance
across principal axes, revealing a main cluster of motion behav-
iors containing most of the 1V and 3V participants. Oppositely,
GV participant’s motion is scattered further from the cluster center.
Considering the centroid of the cluster formed by 1V and 3V par-
ticipants, the average euclidean distance of the distribution is lower
for 1V (d1V = 0.83 ± 0.60) and 3V (d3V = 1.17 ± 1.16) than for GV
(dGV = 7.00 ± 6.93, p1V,GV , p3V,GV < 0.01).

We then computed the median value of the concatenated features
(Fig. 5B). We computed the average MSE across all subjects in each
group as a measure of intra-group motion variability (Fig. 5C). Our
results conﬁrm the aforementioned observation: participants in GV
condition moved more differently from each other, while groups
1V and 3V show a signiﬁcantly lower motion variability, and thus
a higher agreement with each other. Speciﬁcally, the variability of

group GV-N (variabGV −N = 1.01 ± 0.46) was higher than the vari-
ability of groups 1V-N (variab1V −N = 0.30 ± 0.11, p < 0.01) and
3V-N (variab3V −N = 0.33 ± 0.13, p < 0.01). Similar results were
observed comparing GV-V (variabGV −V = 0.78 ± 0.52) with 1V-
V (variab1V −V = 0.25 ± 0.12, p < 0.01) and 3V-V (variab3V −V =
0.33 ± 0.14, p < 0.01).

Human-robot motion correlation is higher in First-Person
View and Third-Person View. Secondly, we considered the cor-
relation between the user’s movements and the drone’s inputs, as
a measure of similarity of the two datasets (Fig. 6). We mea-
sured the correlation of a body segment with a robot command
through the Pearson’s correlation coefﬁcient on the sample distri-
bution. We deﬁne the correlation of the Euler Angle related to a
body segment with the robot motion as the sum of its absolute cor-
relations with the drone roll and pitch. Additionally, we deﬁne the
correlation of a body segment with the robot motion the maximum
correlation value among all the associated Euler Angles. Finally,
we compute the mean across all body segments to evaluate the cor-
relation score of the whole dataset. We observed that group GV-N
showed a lower correlation (corrGV −N = 0.54 ± 0.15) than 1V-N
(corr1V −N = 0.66±0.07, p = 0.034) and 3V-N (corr3V −N = 0.68±
0.08, p = 0.033). Similarly, group GV-V showed a lower correla-
tion (corrGV −V = 0.52 ± 0.13) than 1V-V (corr1V −V = 0.70 ± 0.10,
p < 0.01) and 3V-V (corr3V −V = 0.65 ± 0.04, p = 0.047).

VR and viewpoint affect motion amplitude in different ways.
Finally, we evaluated the differences in the gesture amplitude for
the various body segments. We deﬁne Amount of Motion (AoM)
of a body segment as the mean value of the norm of its angu-
lar velocity vector. Our data indicate that the AoM signiﬁcantly
varies across conditions (Fig. 7). We observed two different ef-
fects of the viewpoint between VR and non-VR groups. A ﬁrst
observation is that no signiﬁcant VR effects are observable, ex-
cept for group 1V-V. Group 1V-V employed smaller body ges-
tures (AoM1V −V = 0.46 ± 0.15) than group 1V-N (AoM1V −N =
0.68 ± 0.16, p < 0.01). Additionally, Group 1V-V moved signiﬁ-

Figure 5: Observed motion variability during the imitation task. (A) Concatenated motion data timeseries. In black, the median value of the
corresponding angle. (B) Scatter plot of the results of a two-dimensional PCA compression on the whole motion timeseries. Group GV exhibits a
higher intra-group variability, while data belonging to groups 1V and 3V are more densely aggregated. (C) Motion variability metric expressed as
MSE from the median value. Both GV-N and GV-V values are signiﬁcantly higher compared to the different viewpoints. (∗∗ p < 0.01)

Figure 7: Amount of Motion score during the imitation task. 1V-V
condition corresponds to smaller body gestures compared to both 1V-
N and the other VR viewpoints, 3V-V and GV-V. (∗∗ p < 0.01, ∗ p < 0.05)

variables: the use of VR and the viewpoint (ﬁrst-person: 1V; third-
person: 3V; ground: GV), for a total of six conditions. Here, we
discuss our main ﬁndings.

Our experiments produced two main sets of results. First, we
observed that, for a given viewpoint, the use of VR can increase the
sense of spatial presence. VR-enabled experiments increased the
user’s sense of spatial presence for both 1V and 3V viewpoints up
to 45% (Table 3, Table 4, Fig. 3). In comparison, the viewpoint per-
spective does not play such an important role on the sense of spatial
presence. However, VR signiﬁcantly increases spatial presence only
in 1V and 3V perspectives.

Furthermore, our data suggest that the viewpoint affects the user’s
sense of embodiment in VR-enabled experiments. 1V and 3V con-
ditions provided a higher sense of embodiment (respectively, 83%
and 57%) compared with GV (Table 3, Table 4, Fig. 4). . The same
effect was not observed for VR-disabled experimental conditions.
These results are in agreement with prior literature [15], and show
that they hold also for non-anthropomorphic robots with non-human
motion patterns.

In summary, our study shows that the sense of presence in virtual
environments is not exclusively limited to anthropomorphic charac-
ters with human-like motion, but can be elicited during the operation
of other types of robots, such as the ﬁxed-wing drone used here.
VR and viewpoint appear to affect different dimensions of teleop-
eration: while the ﬁrst correlates with the sense of spatial presence,

Figure 6: Correlation of human body motion and drone motion.
(A) Drone roll superimposed to torso roll of different subjects.
(B) Correlation score between human and robot motion. Both GV-N
and GV-V conditions correspond to a lower correlation score. (∗∗ p <
0.01, ∗ p < 0.05)

cantly less than groups 3V-V (AoM3V −V = 0.77 ± 0.26, p < 0.01)
and GV-V (AoMGV −V = 0.85 ± 0.62, p < 0.01), suggesting that
users tend to move their torso more when they see the robot when
controlling it in an immersive perspective. Such an effect is not
observable in VR-disabled experiments.

4 DISCUSSION
In this study, we investigated how the use of VR and the viewpoint
change affect two central aspects of motion-based teleoperation: the
sense of presence and the user’s spontaneous body motion. We con-
sidered a ﬁxed-wing drone as an example of a non-anthropomorphic
robot with non-human motion patterns. We run a user study (N=30)
on an imitation task for motion pattern identiﬁcation for motion-
based telerobotics systems (Fig. 2), considering two experimental

−111V-N−111V-V−11Bodyangles[rad]3V-N−113V-V−11GV-N−11GV-VPC1PC21V3VGVPC1PC2PC1PC2PC1PC21V-N3V-NGV-N1V-V3V-VGV-V123Motionvariability********ACB−3031V-N−3031V-V−303Torsoroll(normalized)3V-N−3033V-V−303GV-N−303GV-V1V-N3V-NGV-N1V-V3V-VGV-V0.20.40.60.81correlation*****TimeTime1V-N3V-NGV-N1V-V3V-VGV-V0.511.52AoM[rad/s]******the second mainly impacts the sense of embodiment. Moreover,
the embodiment dimension seems to be related to a camera motion
coherent with the robot’ motion (1V, 3V) more than by using an
immersive point of view (only 1V). These effects do not apply to all
conditions of VR and viewpoint: no change in the sense of spatial
presence was observed in ground view, and no change in the sense
of embodiment was observed without the use of VR.

Our second result concerns the human motion during a robot
imitation task. We observed that human motion is mostly affected
by the viewpoint perspective: while 1V and 3V groups presented
similar intra-group motion patterns, subjects in group GV displayed
much higher variability (Fig. 5) both in VR-enabled (+223%) and
VR-disabled conditions (+169%). Since very different motion pat-
terns cannot be recognized by a predeﬁned BoMI, this result implies
a higher need for personalization in applications where the teleoper-
ation of the robot must take place with a ground view.

We also found a signiﬁcant effect of the viewpoint on the correla-
tion between the participants’ and the robot’s motion. Speciﬁcally,
condition GV reduces motion correlation up to 26% in VR-enabled
and by 35% in VR-disabled experiments (Fig. 6). This result can
be explained by the higher difﬁculty to understand the robot’s be-
havior in GV, since the drone is further away from the user and the
perspective is not aligned with their view. As human-robot motion
correlation is a desirable feature for the deﬁnition of linear mapping
functions, this result suggests that nonlinear mappings could be
more effective for the deﬁnition of a BoMI for third-person view
teleoperation.

Finally, gesture amplitude was affected in different ways by the
viewpoint depending on the VR condition. In VR-disabled exper-
iments, between-groups motion amplitude did not change signiﬁ-
cantly. Comparing with the VR-enabled condition, we observed that
group 1V-V moved 32% less than group 1V-N (Fig. 7). As it has
been demonstrated that body motion is one of the main contribu-
tors to subjective presence in virtual environments [34], it would
be reasonable to expect that immersive experiences are associated
with body movements with higher amplitude. Our results could
be explained by the nature of our task: while previous literature
focuses on active motion, in our study the subjects were passively
following a trajectory, being able to rotate the camera but not to
change the robot’s motion. Consequently, Group 1V saw only a
camera moving, without the possibility to control its motion and
this may have reduced the user’s involvement in the task, and thus
reduced the amplitude of the body movements. Also, group 1V-V
moved their body less than groups 3V-V and GV-V by 40% and
46%, respectively. This result suggests that having the robot in the
user’s ﬁeld of view encourages users to move more than using an
immersive viewpoint, possibly due to the aforementioned reasons.
In summary, we observed that viewpoint, and particularly the GV
perspective translates in both a higher motion variability between
subjects and in a lower user-robot motion correlation, suggesting
that in this condition the implementation of a motion-based BoMI
for the control of the drone would require both a high level of person-
alization and nonlinear mapping methods. These results correlate
with our ﬁndings regarding the sense of presence, and, particularly,
the dimension of embodiment: according to our ﬁndings, the use of
a GV viewpoint translates in both a higher sense of embodiment (in
VR) and in a set of more various motion patterns among different
participants. Moreover, GV also corresponds to a lower correlation
between human and robot motion. Further, we observed a lower
motion amplitude linked to 1V-V, compared to both 1V-N, 3V-V and
GV-V. This effect might be related to the nature of our task.

Although our tests showed clear effects of VR and viewpoint
over the sense of presence and the user’s spontaneous body motion
in teleoperation, several aspects should be investigated in future
research. First, the study of different robotic systems with different
types of motion patterns would be needed to assess if our results

extend to other platforms. The extension to different and more
complex robots is left for future work. This extension would add
substantial value to our ﬁndings, and assess the transferability of the
method. Second, the study of active teleoperation tasks, in addition
to our passive imitation studies, could help to understand whether
that has an effect on the sense of presence and thus on body motion.
However, this step will require a signiﬁcant advancement in state-
of-art methods for the automatic deﬁnition of motion-based HRIs,
as the implementation of HRIs from arbitrary body motion can be
challenging on such variable motion datasets. Finally, our study took
into account a limited set of motion variables consisting of body joint
angles. Although this choice is backed by relevant literature [24, 29],
the observation of different kinematic variables (e.g., the position of
the center of mass of the body segments) could unveil further results.

5 CONCLUSIONS
In this paper, we show new results explaining the effects of VR and
viewpoint on the user’s sense of presence and their spontaneous
motion in teleoperation tasks. Our ﬁndings provide new insights
on presence for teleoperation in the case of a non-anthropomorphic
robot with non-human motion, such as ﬁxed-wing drones. Addi-
tionally, we show that the users’ motion patterns when mimicking
the robot’s motion are affected by VR and viewpoint conditions.
Our work suggests preferred experimental conditions for the deﬁ-
nition of personalized body-machine interfaces for such machines.
Since personalized interfaces have been demonstrated to be more
effective than generic ones for the control of ﬁxed-wing drones,
the application of these results could facilitate the design and the
implementation of new motion-based telerobotic systems.

6 ACKNOWLEDGEMENTS
This work was partially funded by the European Union’s Horizon
2020 research and innovation programme under grant agreement ID:
871479 AERIAL-CORE, the Swiss National Science Foundation
(SNSF) with grand number 200021-155907, and the National Centre
of Competence in Research (NCCR) Robotics

REFERENCES

[1] F. Argelaguet, L. Hoyet, M. Trico, and A. Lecuyer. The role of interac-
tion in virtual embodiment: Effects of the virtual hand representation.
In 2016 IEEE Virtual Reality (VR), pp. 3–10, 2016. doi: 10.1109/VR.
2016.7504682

[2] L. Aymerich-Franch. Can we identify with a block? identiﬁcation with
non-anthropomorphic avatars in virtual reality games. In Proceedings
of the International Society for Presence Research (ISPR), 2012.
[3] L. Aymerich-Franch, D. Petit, G. Ganesh, and A. Kheddar. Non-human
International
looking robot arms induce illusion of embodiment.
Journal of Social Robotics, 9(4):479–490, 2017. doi: 10.1007/s12369
-017-0397-8

[4] J. M. Bland and G. D. Altman. Statistics notes: Cronbach’s alpha. p.

572. Bmj, 1997.

[5] J. Bodner, H. Wykypiel, G. Wetscher, and T. Schmid. First experiences
with the da vinci™ operating robot in thoracic surgery. European
Journal of Cardio-Thoracic Surgery, 25:844–851, 2004. doi: 10.1016/j
.ejcts.2004.02.001

[6] M. Casadio, R. Ranganathan, and F. A. Mussa-Ivaldi. The body-
machine interface: A new perspective on an old theme. Journal of
Motor Behavior, 44:419–433, 2012. doi: 10.1080/00222895.2012.
700968

[7] J. Casper and R. Murphy. Human-robot interactions during the robot-
assisted urban search and rescue response at the world trade center.
IEEE Transactions on Systems, Man, and Cybernetics, 33:367–385,
2003. doi: 10.1109/TSMCB.2003.811794

[8] J. R. Cauchard, J. L. E, K. Y. Zhai, and J. A. Landay. Drone & me:
an exploration into natural human-drone interaction. In Proceedings
of the 2015 ACM International Joint Conference on Pervasive and
Ubiquitous Computing - UbiComp ’15, pp. 361–365, 2015. doi: 10.
1145/2750858.2805823

Determines Full-Body Ownership. Frontiers in Psychology, 2, 2011.
doi: 10.3389/fpsyg.2011.00035

[28] R. M. Pierce and K. J. Kuchenbecker. A data-driven method for
determining natural human-robot motion mappings in teleoperation. In
2012 4th IEEE RAS & EMBS International Conference on Biomedical
Robotics and Biomechatronics (BioRob), pp. 169–176, 2012. doi: 10.
1109/BioRob.2012.6290927

[29] C. Rognon, S. Mintchev, F. DellAgnola, A. Cherpillod, D. Atienza, and
D. Floreano. FlyJacket: An upper body soft exoskeleton for immersive
drone control. IEEE Robotics and Automation Letters, 3:2362–2369,
2018. doi: 10.1109/LRA.2018.2810955

[30] Sanchez-Vives, Maria and Slater, Mel. From presence to consciousness
through virtual reality. Nature Reviews Neuroscience, 6:332–339, 2005.
doi: 10.1038/nrn1651

[31] A. Sanna, F. Lamberti, G. Paravati, and F. Manuri. A kinect-based
natural interface for quadrotor control. Entertainment Computing,
4:179–186, 2013. doi: 10.1016/j.entcom.2013.01.001

[32] SESAR. European drones outlook study, 2016.
[33] S. S. Shapiro and M. B. Wilk. An analysis of variance test for normality

(complete samples). p. 22, 2020.

[34] M. Slater, J. McCarthy, and F. Maringelli. The inﬂuence of body
movement on subjective presence in virtual environments. Human
Factors, 40:469–477, 1998. doi: 10.1518/001872098779591368
[35] M. Slater, B. Spanlang, M. V. Sanchez-Vives, and O. Blanke. First per-
son experience of body transfer in virtual reality. PLoS ONE, 5:e10564,
2010. doi: 10.1371/journal.pone.0010564

[36] M. Slater, M. Usoh, and A. Steed. Depth of presence in virtual environ-
ments. Presence: Teleoperators and Virtual Environments, 3:130–144,
1994. doi: 10.1162/pres.1994.3.2.130

[37] M. S. Song. Flight of mind: Sensorimotor and multisensory embodi-
ment with aviation robotics, ﬂight simulator, and virtual reality. PHD
Thesis, EPFL, Lausanne, 2020.

[38] R. Takizawa, A. Verhulst, K. Seaborn, M. Fukuoka, A. Hiyama, M. Ki-
tazaki, M. Inami, and M. Sugimoto. Exploring perspective dependency
in a shared body with virtual supernumerary robotic arms. In 2019
IEEE International Conference on Artiﬁcial Intelligence and Virtual
Reality (AIVR), pp. 25–257. IEEE. doi: 10.1109/AIVR46125.2019.
00014

[39] L. N. Taylor. Video games: perspective, point-of-view, and immersion.
Unpublished master’s thesis, University of Florida, Gainesville, FL.,
p. 44.

[40] A. Toet, I. A. Kuling, B. N. Krom, and J. B. F. van Erp. Toward
enhanced teleoperation through embodiment. Frontiers in Robotics
and AI, 7:14, 2020. doi: 10.3389/frobt.2020.00014

[41] P. Vorderer, W. Wirth, F. R. Gouveia, F. Biocca, T. Saari, L. J¨ancke,
S. B¨ocking, H. Schramm, A. Gysbers, T. Hartmann, C. Klimmt,
N. Ravaja, A. Sacau, T. Baumgartner, and P. J¨ancke. MEC spatial
presence questionnaire (MEC-SPQ). 2004.

[42] F. Wilcoxon. Individual comparisons by ranking methods. In Break-
throughs in Statistics, Springer Series in Statistics (Perspectives in
Statistics). 1992.

[9] J. Y. C. Chen, E. C. Haas, and M. J. Barnes. Human performance issues
and user interface design for teleoperated robots. IEEE Transactions
on Systems, Man, and Cybernetics, 37:1231–1245, 2007. doi: 10.
1109/TSMCC.2007.905819

[10] H. G. Debarba, E. Molla, B. Herbelin, and R. Boulic. Characterizing
embodied interaction in ﬁrst and third person perspective viewpoints.
In 2015 IEEE Symposium on 3D User Interfaces (3DUI), pp. 67–72,
2015. doi: 10.1109/3DUI.2015.7131728

[11] V. Delafontaine, F. Schiano, G. Cocco, A. Rusu, and D. Floreano.
Drone-aided localization in lora iot networks. In 2020 IEEE Interna-
tional Conference on Robotics and Automation, pp. 286–292, 2020.
doi: 10.1109/ICRA40945.2020.9196869

[12] M. A. Diftler, J. S. Mehling, M. E. Abdallah, N. A. Radford, L. B.
Bridgwater, A. M. Sanders, R. S. Askew, D. M. Linn, J. D. Yamokoski,
F. A. Permenter, B. K. Hargrave, R. Platt, R. T. Savely, and R. O.
Ambrose. Robonaut 2 - the ﬁrst humanoid robot in space. In 2011
IEEE International Conference on Robotics and Automation, pp. 2178–
2183, 2011. doi: 10.1109/ICRA.2011.5979830

[13] D. Floreano and R. J. Wood. Science, technology and the future of
small autonomous drones. Nature, 521:460–466, 2015. doi: 10.1038/
nature14542

[14] T. Gibo. The shared control committee [society news]. IEEE Systems,
Man, and Cybernetics Magazine, 2:51–55, 2016. doi: 10.1109/MSMC.
2016.2557494

[15] G. Gorisse, O. Christmann, E. A. Amato, and S. Richir. First- and
third-person perspectives in immersive virtual environments: Presence
and performance analysis of embodied users. Frontiers in Robotics
and AI, 4, 2017. doi: 10.3389/frobt.2017.00033

[16] O. Khatib, X. Yeh, G. Brantner, B. Soe, B. Kim, S. Ganguly, H. Stuart,
S. Wang, M. Cutkosky, A. Edsinger, P. Mullins, M. Barham, C. R. Vool-
stra, K. N. Salama, M. L’Hour, and V. Creuze. Ocean one: A robotic
avatar for oceanic discovery. IEEE Robotics Automation Magazine,
23:20–29, 2016. doi: 10.1109/MRA.2016.2613281

[17] K. Kilteni, R. Groten, and M. Slater. The sense of embodiment in
virtual reality. Presence: Teleoperators and Virtual Environments,
21:373–387, 2012. doi: 10.1162/PRES a 00124

[18] W. H. Kruskal and W. A. Wallis. Use of ranks in one-criterion variance
analysis. Journal of the American Statistical Association, 47:583–621,
1952. doi: 10.2307/2280779

[19] K. M. Lee. Presence, explicated. Communication Theory, 14(1):27–50,

2004. doi: 10.1111/j.1468-2885.2004.tb00302.x

[20] R. Ma and D. B. Kaber. Presence, workload and performance effects of
synthetic environment design factors. International Journal of Human-
Computer Studies, 64:541–552, 2006. doi: 10.1016/j.ijhcs.2005.12.
003

[21] M. Macchini, T. Havy, A. Weber, F. Schiano, and D. Floreano. Hand-
worn haptic interface for drone teleoperation. In 2020 IEEE Interna-
tional Conference on Robotics and Automation, 2020. doi: 10.1109/
ICRA40945.2020.9196664

[22] M. Macchini, F. Schiano, and D. Floreano. Personalized telerobotics
by fast machine learning of body-machine interfaces. IEEE Robotics
and Automation Letters, 5:179–186, 2020. doi: 10.1109/LRA.2019.
2950816

[23] H. B. Mann and D. R. Whitney. On a test of whether one of two random
variables is stochastically larger than the other. Annals of Mathematical
Statistics, 18:50–60, 1947. doi: 10.1214/aoms/1177730491

[24] J. Miehlbradt, A. Cherpillod, S. Mintchev, M. Coscia, F. Artoni, D. Flo-
reano, and S. Micera. Data-driven body–machine interface for the
accurate control of drones. Proceedings of the National Academy of
Sciences of the United States of America, 115:7913–7918, 2018. doi:
10.1073/pnas.1718648115

[25] R. R. Murphy, S. Tadokoro, D. Nardi, A. Jacoff, P. Fiorini, H. Choset,
and A. M. Erkmen. Search and rescue robotics. In Springer Handbook
of Robotics, p. 23. 2008.

[26] J. M. Peschel and R. R. Murphy. On the human–machine interaction
of unmanned aerial system mission specialists. IEEE Transactions
on Human-Machine Systems, 43:53–62, 2013. doi: 10.1109/TSMCC.
2012.2220133

[27] V. I. Petkova, M. Khoshnevis, and H. H. Ehrsson. The Perspective
Matters! Multisensory Integration in Ego-Centric Reference Frames

