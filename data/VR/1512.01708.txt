7
1
0
2

r
p
A
7

]

G
L
.
s
c
[

2
v
8
0
7
1
0
.
2
1
5
1
:
v
i
X
r
a

Variance Reduction for Distributed Stochastic Gradient Descent

Soham De1, Gavin Taylor2 and Tom Goldstein1

1 Department of Computer Science, University of Maryland, College Park
2 United States Naval Academy
{sohamde, tomg}@cs.umd.edu, taylor@usna.edu

April 10, 2017

Abstract

Variance reduction (VR) methods boost the performance of stochastic gradient descent (SGD) by
enabling the use of larger, constant stepsizes and preserving linear convergence rates. However, current
variance reduced SGD methods require either high memory usage or an exact gradient computation (using
the entire dataset) at the end of each epoch. This limits the use of VR methods in practical distributed
settings. In this paper, we propose a variance reduction method, called VR-lite, that does not require full
gradient computations or extra storage. We explore distributed synchronous and asynchronous variants
that are scalable and remain stable with low communication frequency. We empirically compare both
the sequential and distributed algorithms to state-of-the-art stochastic optimization methods, and ﬁnd
that our proposed algorithms perform favorably to other stochastic methods.

1 Introduction

Many problems in machine learning and statistics involve minimizations of the form

min
x

f (x), f (x) =

1
n

n
(cid:88)

i=1

fi(x),

(1)

where each fi : Rd → R. Problems of this form include logistic regression, support vector machines, matrix
factorization, and others [13]. Such problems frequently arise when ﬁtting a model to data, where each
fi measures how well a model ﬁts a particular data point. For example, if {ai} is a collection of feature
vectors and {bi} is a collection of model outputs, a simple least-squares regression is obtained by setting
fi(x) = (aT

i x − bi)2.

When large datasets are involved, problems of the form (1) are traditionally solved using stochastic gradient
descent (SGD) updates of the form

xk+1 = xk − ηkgk,

where gk = ∇fik (x) ≈ ∇f (xk) is a “noisy” gradient estimate using a randomly chosen index ik, and ηk is a
stepsize parameter [15]. This crude approximate gradient is inexpensive and highly eﬀective when the error
is large, resulting in fast convergence to low accuracy solutions. Unfortunately, as the iterates approach the
k)) to dampen the eﬀects of noise,
true solution, the gradient descent stepsize must vanish (ηk ≤ O(1/
resulting in extremely slow convergence in the high accuracy regime.

√

Variance reduction (VR) schemes [9, 6, 14, 16, 7, 19, 18, 8] are able to maintain a large constant stepsize and
achieve fast convergence to high accuracy. While the diﬀerence between the stochastic and exact gradient is
potentially large, this diﬀerence is predictable – the gradient errors are highly correlated between diﬀerent
uses of the same function fik .

1

 
 
 
 
 
 
VR methods exploit this property by approximating the gradient error at the most recent use of fik , and
subtracting this error from ∇fik (xk) to obtain a more accurate solution. This results in approximate gradients
of the form

gk =

∇fik (x)
(cid:124) (cid:123)(cid:122) (cid:125)
approximate gradient

− ∇fik (y) + (cid:101)gy
(cid:125)

(cid:123)(cid:122)
error correction term

(cid:124)

,

(2)

where y is an old iterate, and (cid:101)gy is an approximation of the true gradient ∇f (y). The algorithm SVRG
[9], for example, sets y to be an iterate from the algorithm history and sets (cid:101)gy = ∇f (y) to be the exact
gradient at y. The more recent SAGA scheme [6] stores the most recent value of ∇fi (for all i) and
chooses (cid:101)gy to be the average of these values. This error correction term in (2) allows VR methods to use
large (non-vanishing) stepsizes, preserve linear convergence rates (under strong convexity assumptions), and
signiﬁcantly outperform classical SGD.

Current VR methods suﬀer from a few caveats that limit their use in practical settings. First, some methods
(e.g., SAGA) require storing a complete history of n previous iterates, thus increasing memory requirements
(although this requirement can be mitigated for some simple minimization problems [6]). Other methods
(e.g., SVRG) require computation of the exact gradient ∇f over the entire dataset after each epoch. This
cost is often signiﬁcant in real-world scenarios where SGD methods typically converge after relatively few
epochs, and also prevent its use in asynchronous distributed environments.

Another caveat of VR algorithms is that they have not been adequately studied in the distributed setting.
Current methods are impractical in large scale settings as explained later in the paper, and only a couple of
recent papers have tried to address this [4, 21]. For large-scale problems, asynchronous variants of distributed
SGD are still widely used [5, 13, 1, 10, 17, 22, 23, 20], and most of the work has been focused on the parameter
server model of computation [5, 13, 22, 1, 10], where updates are immediately communicated to the central
server. Relatively little work has been done in the truly distributed case where communications costs are
high [23, 20, 11], since infrequent communication in SGD-type methods usually leads to slower convergence
and instability; a problem that could be mitigated using VR.

1.1 Contributions

This work has two main contributions. First, we present a new approach to variance reduction, VR-lite, that
requires neither extra memory to store the algorithm history, nor the computation of exact gradients. For
this reason, VR-lite has lower memory requirements and faster iterations than previous approaches. This is
achieved by storing averages of previous iterates rather than a complete history. We show empirically that
this approach converges much faster than competing methods.

Next, we study the application of VR-lite in a large scale distributed setting. We present synchronous and
asynchronous variants of VR-lite in a fully distributed setting with high communication periods between
nodes. We ﬁnd that our algorithms are scalable and remain stable despite high communication periods.
The proposed distributed algorithms outperform a number of other distributed SGD methods on a variety
of classiﬁcation and regression tasks.

2 Proposed Algorithm: VR-lite

Most VR schemes are divided into epochs, during which a pass is made over the entire dataset. Thus,
n updates take place during the m-th epoch (one update per data record/feature vector). The iterates
generated in the m-th epoch can be written as {xj
j=1.

m}n

At the end of an epoch, the SVRG method sets y = xn
m), the exact
gradient. These values of y and (cid:101)gy are then used to perform corrected gradient updates of the form (2). This
approach avoids the extra memory requirements of “epoch-free algorithms” such as SAGA [6], but requires
an expensive gradient evaluation over the entire dataset on each iteration.

m, and (cid:101)gy = ∇f (y) = 1

j=1 ∇fj(xn

n

(cid:80)n

2

To speed computation, VR-lite accumulates the average gradient vector over an epoch. This average gradient
is then used as (cid:101)gy, thus avoiding costly loops over the entire (large) dataset. These accumulated averages
can be computed cheaply “on the ﬂy” as the algorithm runs without noticeable overhead.

Let πm denote a random permutation of the data indices {1, 2, · · · , n}, with πj
chosen in the j-th step in πm. Then, the update rule for VR-lite is given by

m denoting the data index

xk+1
m+1 = xk

m+1 − η(cid:0)∇fik (xk

m+1) − ∇fik (xm) + gm

(cid:1),

where ik = πk

m+1, and xm and gm denote the average over iterations and gradients, given by

xm =

1
n

n
(cid:88)

j=1

xj
m, and gm =

1
n

n
(cid:88)

j=1

∇fπj

m

(xj

m).

(3)

(4)

Note that for VR-lite, E[∇fik (xm) − gm] (cid:54)= 0, unlike most other variance reduction methods. This makes it
hard to theoretically prove convergence guarantees for VR-lite.

The values of x0 and g0 are initialized using a single epoch of vanilla SGD with no VR correction. This
algorithm ﬁts into the general VR framework (2) with y = xm and (cid:101)gy = gm, does not require any extra
storage apart from a place to store and update these averages, and only requires 2 gradient computations
per epoch. The full method is described in Algorithm 1.

Algorithm 1 VR-lite Algorithm
1: parameters learning rate η
2: initialize x, x, and g using plain SGD for 1 epoch
3: while not converged do
4:
5:
6:

initialize variables to accumulate averages over an epoch: (cid:101)x = (cid:101)g = 0
for k in {1, . . . , n} do

sample ik ∈ {1, . . . , n} without replacement
update x according to (3): x ← x − η(cid:0)∇fik (x) − ∇fik (x) + g(cid:1)
update running averages: (cid:101)x ← (cid:101)x + x, (cid:101)g ← (cid:101)g + ∇fij (x)

end for
set x and g for next epoch: x = (cid:101)x/n, g = (cid:101)g/n

7:
8:
9:
10:
11: end while

3 Distributed Algorithms

We now consider a setting where the data is distributed across p local nodes, and our goal is to minimize the
global objective function. We consider that the p local nodes can only communicate with a central server,
i.e., a centralized setting. Let the set of data indices {1, 2, · · · , n} be decomposed into disjoint subsets {Ψs},
with each Ψs representing the indices of data stored on server s. The objective (1) now has the form

f (x) =

1
n

p
(cid:88)

(cid:88)

s=1

j∈Ψs

fj(x).

(5)

VR-lite is easily distributed in an asynchronous manner since it does not involve calculating the full gradient
of the objective function as in SVRG. Moreover, since the algorithm needs to update the average of the
gradients only at the end of an epoch, communication periods between the central node and the local nodes
can be increased while still ensuring a fast and stable algorithm.

In this section, we propose fast and stable synchronous and asynchronous variants of VR-lite in the fully
distributed setting with high communication latency between the central and local nodes.

3

Algorithm 2 Sync VR-lite Algorithm

for each local node s do

1: parameters learning rate η
2: initialize x, x, g using plain SGD for 1 epoch
3: while not converged do
4:
5:
6:
7:
8:

initialize variables to accumulate averages over an epoch: (cid:101)x = (cid:101)g = 0
for k in {1, . . . , n} do

random ik ∈ Ψs without replacement
update x according to (3): x = x − η(cid:0)∇fik (x) − ∇fik (x) + g(cid:1)
update running averages: (cid:101)x = (cid:101)x + x, (cid:101)g = (cid:101)g + ∇fi(x)

end for
set average variables: xs = (cid:101)x/n, gs = (cid:101)g/n
send xs, xs, gs to central node
receive updated x, x, g from central node

9:
10:
11:
12:
13:
14:

end for
central node:

15:
16:
17:
18: end while

average all xs, xs, gs received from the p local nodes
broadcast averaged x, x, g to each local node s

3.1 Synchronous VR-lite

In a fully distributed setting, the objective is to decrease the frequency of (slow) communication with the
central server so as to not impede (fast) gradient updates on each local node. This is achieved by updating
local copies of x on each remote client, and communicating with the central server periodically to report
the change. Furthermore, the same average gradient term will be shared across all local nodes and used
for variance reduction, ensuring that no local node drifts far away from the global solution even when
communication gaps are large. This approach leads to a synchronous algorithm, Sync VR-lite, detailed in
Algorithm 2.

In Sync VR-lite, each local node communicates with the central server only once in each epoch. Thus, during
one full epoch over the dataset, Sync VR-lite only requires a total of p communications with the central
server. This is much less communication compared to the commonly used “parameter server” model of
computation, where a communication phase is required for each update to a centrally stored iterate, leading
to n communications per epoch.

3.2 Asynchronous VR-lite

Sync VR-lite can be made asynchronous very easily, as demonstrated in Algorithm 3. The key idea for the
asynchronous algorithm, Async VR-lite, is that only the change in x, xs and gs, is sent from local node s to
the central server. This ensures that when updating the centrally stored x, x, and g, the previous contribution
to the average from that local worker is subtracted out before adding in the new value. Thus, the central x
and g always store unbiased estimates of the average of the distributed mean {¯xs} and {¯gs} This is critical
for ensuring that a fast working local node does not bias the solution, making the algorithm more robust to
heterogeneous computing environments where local nodes work at drastically diﬀerent speeds.

4 Experiments

In this section, we present our empirical results. We test our algorithms on a binary classiﬁcation problem
with (cid:96)2-regularized logistic regression and an (cid:96)2-regularized linear regression (ridge regression) problem.

4

Algorithm 3 Async VR-lite Algorithm

for each local node s do

1: parameters learning rate η
2: initialize x, x and g using plain SGD for 1 epoch; α = 1/p, xold = xold = gold = 0
3: while not converged do
4:
5:
6:
7:
8:

initialize variables to accumulate averages over an epoch: (cid:101)x = (cid:101)g = 0
for k in {1, . . . , n} do

random ik ∈ Ψs without replacement
update x according to (3): x = x − η(cid:0)∇fi(x) − ∇fi(x) + g(cid:1)
update running averages: (cid:101)x = (cid:101)x + x, (cid:101)g = (cid:101)g + ∇fi(x)

end for
set average variables: x = (cid:101)x/n, g = (cid:101)g/n
calculate the change in variables: ∆xs = x − xold, ∆xs = x − xold, ∆gs = g − gold
saving the current average variables: xold = x, gold = g
send ∆xs, ∆xs, ∆gs to central node
receive updated x, x, g from central node

9:
10:
11:
12:
13:
14:

end for
central node:

15:
16:
17:
18:
19:
20:
21: end while

receive ∆xs, ∆xs, ∆gs from a local worker s
update central variables: x = x + α∆x, x = x + α∆x, g = g + α∆g
send new x, x, g back to local worker s

Figure 1: Sequential Results. Left to right: Logistic regression on toy dataset; Ridge regression on toy data; Logistic
regression on IJCNN1 dataset; Ridge regression on MILLIONSONG dataset

More formally, we are interested in the following two optimization problems:

logistic regression: min

x

ridge regression: min

x

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

log (cid:0)1 + exp(biaT

i x)(cid:1) + λ(cid:107)x(cid:107)2

(aT

i x − bi)2 + λ(cid:107)x(cid:107)2,

where each ai ∈ Rd is a data sample with label bi ∈ R, and x ∈ Rd. In all our experiments, we set the (cid:96)2
regularization parameter to λ = 10−4.

4.1 Sequential Results

We ﬁrst investigate the performance of VR-lite in the sequential, non-distributed setting. It is well-established
by now that VR methods out-perform regular SGD by a wide margin on convex models. Thus, we compare
our algorithm’s performance with two popular variance reduction methods, SVRG [9] and SAGA [6].

We tested VR-lite on two toy datasets as well as two real world datasets. For binary classiﬁcation, we
generated a random Gaussian data matrix for each class, where the ﬁrst class had zero mean and the

5

Figure 2: Distributed Results. Left to right: Logistic regression on SUSY over 250 local workers; Ridge regression
on MILLIONSONG over 240 local workers; Time required for convergence as number of local workers is increased on
SUSY and MILLIONSONG, respectively.

second class had mean 1 entries. The variances of the distributions were varied such that the dataset was
not perfectly linearly separable. For ridge regression, we generated a random Gaussian matrix A with
observation vector b = Ax + (cid:15), where (cid:15) denotes standard Gaussian noise. We built the toy datasets to have
n = 5000 data samples with d = 20 features, with 2500 points of each class for the binary classiﬁcation case.
We also tested our method on two standard real world datasets: IJCNN1 [12] for binary classiﬁcation and
the MILLIONSONG [3] dataset for least squares regression.

Figure 1 shows results of our experiments. For all algorithms, we pick the constant learning rate which gives
fastest convergence. Since a gradient computation is the most expensive step of VR methods, we compare
the rate of convergence with the number of epochs used. It is important to note that comparing VR-lite to
SAGA may not be a fair comparison since SAGA uses considerably more storage space. Despite this fact,
we ﬁnd that VR-lite converges faster than both SAGA and SVRG in all cases.

4.2 Distributed Results

We now present results of our distributed algorithms. All algorithms were implemented using Python and
MPI, and run on an HPC cluster with 24 cores per node. Our asynchronous implementations are “locked”,
i.e., only a single local node can update the parameters on the central server at a given time. However,
all our implementations can be made faster in a simple extension to a lock-free setting. We compare our
algorithms to the following popular asynchronous stochastic optimization methods:

• Hogwild!: A basic asynchronous variant of SGD [13] using a “parameter server” model.

• Elastic Averaging SGD (EASGD): A recent asynchronous SGD method [20] that has been shown to
accelerate the training of neural networks eﬃciently. We found the basic EASGD algorithm performed
better than the momentum method (M-EASGD), so we present results only on EASGD. As recom-
mended in [20], we set the parameter β = 0.9. The communication period was varied as τ = 1, 4, 16, 64.

• Asynchronous SVRG: This is the ﬁrst work to our knowledge to propose an asynchronous variant
of VR algorithms [14]. The authors show that the asynchronous SVRG algorithm enjoys the same
beneﬁts as its sequential counterpart over standard distributed stochastic optimization methods. As
recommended in both [9] and [14], we set the epoch size to 2n. Note that, after each epoch of size 2n,
a synchronization step is required for this algorithm to calculate the full gradient. Thus, this is not a
fully asynchronous method.

Note that there is no accepted standard stochastic optimization algorithm for the distributed setting, and
so we choose to compare VR-lite with the above recently developed algorithms.

For all algorithms and all experiments, we present results using the constant step size that achieves fastest
convergence. We present results for binary classiﬁcation on the dataset SUSY [2] and for least squares
regression on the dataset MILLIONSONG [3]. MILLIONSONG contains over 500,000 data samples while
SUSY contains 5,000,000 samples.

Figure 2 shows results of our distributed experiments. The ﬁrst two plots compare our algorithms with

6

the other methods on SUSY with 250 local workers and MILLIONSONG with 240 local workers. We plot
the relative norm of the gradient on the y-axis, and wall clock time on the x-axis. For both the binary
classiﬁcation and least squares regression problems, we see that both Sync VR-lite and Async VR-lite
signiﬁcantly outperform all other algorithms. We further notice that the methods are extremely stable
inspite of the high communication latency with the central server.

The last two plots in Figure 2 show the scalability of our algorithms. We plot the wall clock time required for
convergence on the y-axis and increase the number of local workers on the x-axis. We compare convergence
rates of Sync VR-lite and Async VR-lite with 60, 120, 240 and 480 local workers for MILLIONSONG and
with 50, 250, 500, 750 local workers for SUSY. We notice that for MILLIONSONG, there is a large drop in
convergence rate initially with the gains leveling out as we keep increasing the number of local workers. The
proposed algorithms take only about 10 seconds over hundreds of local workers to train the dataset and the
convergence rates of the asynchronous algorithm can potentially be further improved by using a lock-free
implementation. For the dataset SUSY, which is 10 times larger than MILLIONSONG, we see a consistent
gain in convergence rates as we keep increasing the number of local workers, with our algorithms training
the dataset in less than 5 seconds when distributed over 750 local workers.

5 Conclusion

In this paper, we presented VR-lite, a variance reduction SGD algorithm which, unlike other popular variance
reduction schemes, does not require any full gradient computations or extra storage. We presented distributed
variants of this algorithm, Sync VR-lite and Async VR-lite, for a setting with low frequency of communication
between the local nodes and the central server. Sync VR-lite and Async VR-lite are scalable in the distributed
setting, and perform favorably to other parallel and distributed SGD algorithms on standard models typically
encountered in machine learning.

References

[1] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances in Neural Infor-

mation Processing Systems, pages 873–881, 2011.

[2] Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with

deep learning. Nature communications, 5, 2014.

[3] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The million song dataset. In

Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011), 2011.

[4] Soham De and Tom Goldstein. Eﬃcient distributed SGD with variance reduction. In 2016 IEEE International

Conference on Data Mining. IEEE, 2016.

[5] Jeﬀrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker,
Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in Neural Information Processing
Systems, pages 1223–1231, 2012.

[6] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support
In Advances in Neural Information Processing Systems, pages

for non-strongly convex composite objectives.
1646–1654, 2014.

[7] Aaron Defazio, Justin Domke, et al. Finito: A faster, permutable incremental gradient method for big data
problems. In Proceedings of The 31st International Conference on Machine Learning, pages 1125–1133, 2014.

[8] Reza Harikandeh, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Koneˇcn`y, and Scott Sallinen.
Stop wasting my gradients: Practical svrg. In Advances in Neural Information Processing Systems, pages 2242–
2250, 2015.

[9] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Advances in Neural Information Processing Systems, pages 315–323, 2013.

[10] Mu Li, David Andersen, Alex Smola, and Kai Yu. Communication eﬃcient distributed machine learning with

the parameter server. In Advances in Neural Information Processing Systems, pages 19–27, 2014.

7

[11] Aryan Mokhtari and Alejandro Ribeiro. Dsa: Decentralized double stochastic averaging gradient algorithm.

Journal of Machine Learning Research, 17(61):1–35, 2016.

[12] Danil Prokhorov. Ijcnn 2001 neural network competition. Slide presentation in IJCNN, 1, 2001.

[13] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing
stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693–701, 2011.

[14] Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnab´as P´oczos, and Alex J Smola. On variance reduction
In Advances in Neural Information Processing

in stochastic gradient descent and its asynchronous variants.
Systems, pages 2629–2637, 2015.

[15] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics,

pages 400–407, 1951.

[16] Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential con-
vergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems, pages 2663–2671,
2012.

[17] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In Communication, Control,

and Computing (Allerton), 2014 52nd Annual Allerton Conference on, pages 850–857, 2014.

[18] Chong Wang, Xi Chen, Alex J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization.

In Advances in Neural Information Processing Systems, pages 181–189, 2013.

[19] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM

Journal on Optimization, 24(4):2057–2075, 2014.

[20] Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In Advances in

Neural Information Processing Systems, pages 685–693, 2015.

[21] Shen-Yi Zhao, Ru Xiang, Ying-Hao Shi, Peng Gao, and Wu-Jun Li. Scope: Scalable composite optimization for

learning on spark. arXiv preprint arXiv:1602.00133, 2016.

[22] Martin Zinkevich, John Langford, and Alex J Smola. Slow learners are fast. In Advances in Neural Information

Processing Systems, pages 2331–2339, 2009.

[23] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In

Advances in neural information processing systems, pages 2595–2603, 2010.

8

