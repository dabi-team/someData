Attention based Occlusion Removal for Hybrid Telepresence Systems

Surabhi Gupta

Ashwath Shetty

Avinash Sharma

International Institute of Information Technology Hyderabad
{surabhi.gupta, ashwath.shetty}@research.iiit.ac.in, asharma@iiit.ac.in

1
2
0
2
c
e
D
2

]

V
C
.
s
c
[

1
v
8
9
0
1
0
.
2
1
1
2
:
v
i
X
r
a

Fig. 1: Our proposed approach can reconstruct high-quality unoccluded image from a given occluded face image as input.

Abstract— Traditionally, video conferencing is a widely
adopted solution for
telecommunication, but a lack of
immersiveness comes inherently due to the 2D nature of facial
representation. The integration of Virtual Reality (VR) in a
communication/telepresence system through Head Mounted
Displays (HMDs) promises to provide users a much better
immersive experience. However, HMDs cause hindrance by
blocking the facial appearance and expressions of the user. To
overcome these issues, we propose a novel attention-enabled
encoder-decoder architecture for HMD de-occlusion. We also
propose to train our person-speciﬁc model using short videos
(1-2 minutes) of the user, captured in varying appearances, and
demonstrated generalization to unseen poses and appearances
of the user. We report superior qualitative and quantitative
results
also present
state-of-the-art methods. We
applications of this approach to hybrid video teleconferencing
using existing animation and 3D face reconstruction pipelines.

over

I. INTRODUCTION

Globalization has led to an acute need for tele-interactions
for effective communication that has been further boosted
due to the current pandemic situation across the world. Tra-
ditionally, video conferencing is a widely adopted solution
for telecommunication, but a lack of immersiveness comes
inherently due to the 2D nature of facial representation.
The integration of Virtual Reality (VR) in a communica-

tion/telepresence system through Head Mounted Displays
(HMDs) promises to provide users a much better immersive
experience. Nevertheless, HMDs signiﬁcantly occlude the
user’s face, hindering facial appearance capture, including
gaze and expressions. Therefore, HMD removal in images is
vital for enabling an improved user experience.

Traditionally, Analysis-by-Synthesis techniques for HMD
de-occlusion proposed in the literature [16] animate para-
metric face models [1] using features extracted from an
HMD occluded input image. However, such models often
generate overly smooth geometrical details and compromise
the realism of facial appearance. On the other hand, re-
cent facial avatar-based models [11] achieves photorealistic
results for HMD de-occlusion. However, avatar modeling
methods require a large amount of calibrated multi-view
data of a single user in different poses and expressions
for avatar creation (e.g., [11] uses setup consisting of 40
machine vision cameras capable of synchronously capturing
5120×3840 images at 30 frames per second). Thus, such
avatar creation is non-trivial and challenging to scale up for
a large user base. Additionally, it is a one-time process for
each user. Therefore, such parametric model or avatar-based
techniques have a signiﬁcant limitation: they lack the user’s
actual appearance and background during the interaction

 
 
 
 
 
 
(i.e., unable to model the everyday appearance of the user),
thereby hindering the user experience.

HMD de-occlusion problem can also be posed as a face
completion/inpainting problem. Existing face completion
methods in literature attempt to learn a single inpainting
network over a large training population [20], [22], hoping
for good generalization on unseen face images. However,
these methods frequently suffer from issues like loss of
identity and fail to generalize with even minor variations
in head pose, as shown in Figure 2. Another set of methods
in [17], [13] uses a reference image along with the occluded
area to ﬁll a masked region and preserve identity. However,
as shown in [17], their work fails to generalize with non-
frontal head-poses and [13] requires additional information
(depth and mask) and do not generate the entire face (with
hairs, ears, and background), which hinders user experience.

Fig. 2: Failure cases of LaFin [20]

The primary research challenge with the face comple-
tion/inpainting task comes from it’s ill-posed nature as a
signiﬁcant part of the face is occluded by HMD. Learning
a single common face inpainting/de-occlusion network with
the capability to hallucinate diverse expressions in varying
appearances and head poses across a large set of human
faces is difﬁcult to achieve. It is due to the broad space
of facial geometry and appearance as well as the highly
subjective way of articulating expressions/emotions across
individuals [2]. Additionally, in the context of VR teleconfer-
encing applications, the desired solution should be scalable,
requiring minimal efforts and hardware at the user’s end
to capture their everyday appearance. An additional desired
characteristic might be regarding integration ability in a
hybrid VR teleconferencing setup where users with only
video capability should also be able to participate as in
regular video conferencing.

To overcome these challenges, we propose to tackle
this problem in a person-speciﬁc setting where we aim
to train a dedicated model for each user to learn user-
speciﬁc appearance, head-pose, gaze, and facial expression
traits. The input
to our method is a video frame with
HMD occluded face, and our model generates a de-occluded
plausible face by removing the HMD occlusion. To achieve
this, we introduce a novel attention enabled encoder-decoder
architecture and a novel training strategy to train our person-
speciﬁc model using short videos (1-2 minutes) of the user,
capturing varying appearance of the user/background and
more speciﬁcally variety of head poses and facial expressions
without HMD occlusion. As part of our training strategy, we
ﬁrst train the encoder-decoder module (sans attention) on
large face image datasets to learn generic face appearance

features. Subsequently, we ﬁnetune it on unoccluded user
videos. Finally, we train/ﬁnetune our full model (encoder-
decoder with attention) on the same training data with
synthetic HMD masks. It allows our model to learn the
person-speciﬁc facial geometry and expression traits and help
generate occluded areas with varying appearances, poses,
and expressions. Our attention module allows the network to
preserve high-frequency appearance/background details (like
hairs, wall texture, etc.) from the unoccluded part of the
input HMD occluded image while generating the plausible
facial appearance for the occluded part. Our novel mask-
loss function helps the model to emphasize the occluded
region. Figure 1 shows high-quality de-occlusion achieved
by our method. It is important to note that learning a person-
speciﬁc model is not equivalent to overﬁtting on a speciﬁc
user as there is a signiﬁcant change in user appearance, back-
ground, and lighting across sessions. Similar person-speciﬁc
model learning has been successfully explored in this context
[11]) as well as another related context of egocentric
(e.g.,
frontal face recovery [5]. We conduct a thorough empirical
evaluation and report superior qualitative and quantitative
results of our face de-occlusion method w.r.t. SOTA methods.
Additionally, we also demonstrate the broader applicability
of the proposed HMD de-occlusion method with two use
cases. First, it can be used to build hybrid VR systems
by integrating it with [18], [15] kind of video-driven face
animation solutions. As a second use case, we show how
we can integrate our method with a 3D face reconstruction
pipeline to generate 3D face video for VR teleconferencing
system. To summarise, we make the following contributions:

1) We present a deep learning framework for person-
speciﬁc HMD occlusion removal. Our method does
not rely on high-end hardware (e.g., HMD with gaze
tracking) and calibrated data from the user (for avatar
creation).

2) We introduced a novel attention module knitted with
the encoder-decoder architecture that has the ability to
use background and appearance details from the input
image and allows the model to focus on inpainting the
occluded region with plausible details.

3) We collected a small dataset of multiple users in
different appearance/facial expression/head-pose and
lighting variations. We intend to release this for the
academic community.

4) We also present applications of our model where it can
be integrated with neural animation models [18], [15]
to animate an occluded video and can also be used to
recover 3D face from occluded input.

II. RELATED WORK

Our approach is an inpainting method that learns to ﬁll
in user-speciﬁc details. It is related to traditional inpainting
methods and recent approaches that use a reference image to
ﬁll an occluded region faithfully. In the following, we discuss
the most relavent literature in detail.

A. Person Speciﬁc Models

Recent deep learning-powered advances in vision and
graphics have led to the rise of personalized models that
are animated/rendered using deep learning. [11] learns auto-
encoder network to predict view conditioned texture and
mesh geometry from HMD occluded input. [6] trains a
dynamic neural radiance ﬁeld model on a short video (2-
3 min) of the person with different expressions and poses.
Also closely related to our work is [5]. They use a video to
video GAN [7], which takes in egocentric frames of a person
and generates the corresponding frontal view. These methods
show the ability of person-speciﬁc models to capture high-
frequency details. However, they require calibrated multi-
view data from the user, which adds additional hardware
constraint, and animating these models is also expensive. Our
approach requires the user to send a short uncalibrated video
session for training and is considerably lightweight compared
to avatar-based methods.

B. Image Inpainting Methods

Image inpainting describes the task of ﬁlling missing
image regions with realistic content. Recent work [22], [21]
and [20] train a conditional GAN [8] on a face dataset as
a solution to this problem. These methods show impressive
generalization to examples with frontal head pose and arbi-
trary occlusion. However, they are biased to their training
distribution, do not generalize well to even slightly non-
frontal head poses, and suffer from a loss of identity. As
we train our method only on images of the same person
with considerable variations in head poses and expressions,
we can overcome the challenges of identity loss and manage
to generalize to various head poses.

C. HMD Removal

Recent works like [13] and [17] propose an image-based
approach to HMD de-occlusion. They try to solve the identity
problem mentioned above by using a reference image to
guide the inpainting procedure and learn a general model
for the task. However [17] fails to work well with cases of
pose variations between the reference and occluded image
and suffers from loss of identity in these cases. Also, [13]
requires additional depth data and train and evaluate on
synthetic data, which may not work or be available in a real-
world teleconferencing scenario. We train and evaluate our
model on data of real-world conversations and scenarios and
show our model’s ability to generalize to unseen appearances.

III. METHODOLOGY

A. Overview

The primary focus of our work is to learn a personalized
model for face de-occlusion, particularly as an application in
VR teleconferencing, where the face is partially occluded due
to HMD. To tackle this, we formulate the face de-occlusion
problem as an image inpainting task. Given an occluded
face image as an input Xocc, our network aims to hallucinate
the missing region with plausible and perceptually consistent

facial details in order to reconstruct the generated unoccluded
image, Xrec against the ground truth unoccluded image, Xgt .
Inspired by the autoencoder architecture proposed in [3],
we use an novel attention enabled encoder-decoder frame-
work with generative capabilities that learns to reconstruct
high-ﬁdelity unoccluded faces from HMD occluded input
images. Additionally, we also propose a novel mask-based
loss function and a novel training strategy to learn our model.
Figure 3 shows the outline of our proposed architecture.

B. Proposed Architecture

1) Encoder-Decoder Module: Our encoder-decoder mod-
ule comprises a stack of ResNet and inverted ResNet blocks.
Each ResNet block consists of a set of convolutions with
residual connections. For the inverted ResNet block, the ﬁrst
convolution in the ResNet block is replaced by a 4×4 deconv
layer. We also provide the additional generative capability to
the network using an adversarial loss. The encoder learns
a 99-dimensional feature representation of the input image.
This bottleneck representation is subsequently fed to the
decoder network to reconstruct the target image.

2) Attention Module: Inspired from existing literature on
attention-based learning strategies as proposed in [19], [10],
we append our encoder-decoder module with an attention
module. We perform spatial attention by taking the encoder
output from the second layer, Fenc of spatial dimension
64 × 64 and perform a channel-wise concatenation with the
corresponding decoder layer output, Fdec (i.e., with same
spatial dimension) and feed it to our attention module which
subsequently generates attention maps of same dimension
as shown in Figure. 3(A). We then decouple these attention
maps and use them for a weighted fusion of respective
feature maps (i.e., Fenc and Fdec). The fused feature maps
are subsequently fed downstream to convolution layers to
reconstruct the de-occluded face image.

Such attention-based spatial feature map fusion allows our
network to preserve high-frequency appearance/background
details (like hairs, wall texture, etc.) from the visible part of
the input image while generating a plausible facial appear-
ance for the occluded part.

These attention maps can be learned using fully convo-
lutional networks. As shown in Figure. 3(B), the attention
module consists of Conv(4 ∗ m, 3), Conv(4 ∗ m, 3), Conv(8 ∗
m, 3) and Conv(2 ∗ m, 3), where m denotes the base number
of ﬁlters and Conv(m, k) denotes a convolutional layer with
output number of channels m and kernel size k. The ﬁnal
output with 2 ∗ m channels is then split into two, Attnenc and
Attndec, each of m channels and spatial dimension of 64×64.
This acts as a attention mask for the inputs, Frec and Fdec
which is then fused again using a channel-wise summation
according to Equation 1.

Ff used = Fenc ∗ Attnenc + Fdec ∗ Attndec

(1)

3) Loss Function: We employ a combination of four
different loss functions as our training objective. First, in
order to penalize reconstruction errors, we use pixel-based

Fig. 3: Detailed architecture diagram of our proposed network.

L1 loss.

C. Our Training Strategy

Lrec = (cid:107)Xgt − Xrec(cid:107)1

(2)

However, using only the L1 reconstruction loss produces
blurry outputs. To overcome this, we add a discrimina-
tor, D in the architecture to compute the adversarial loss.
loss term forces the encoder-decoder to
This adversarial
reconstruct high-ﬁdelity outputs by sharpening the blurred
images. For D, we adopt the architecture of the DCGAN
discriminator[14].

Ladv = log(D(Xgt )) + log(1 − D(Xrec))

(3)

We also use SSIM based structural similarity loss (as deﬁned
in [3]) that helps to improve the alignment of high-frequency
image elements to stabilize the adversarial training.

Lssim = SSIM(Xrec, Xgt )

(4)

To further improve the quality of reconstruction in the
HMD occluded area of the generated image, we propose
a novel mask-based loss. Here, we use the binary mask
image as an additional supervision to the network along with
input image while training. Minimizing this loss helps the
model to emphasize more on quality of reconstruction in
the masked region. This also helps to mitigate the blinking
artifacts around the eye region for stable reconstructions. We
formulate the mask-based loss function as:

Lmask = (cid:107)Imask (cid:12) Igt − Imask (cid:12) Irec(cid:107)1

(5)

where, Imask refers to single channel binary mask image
where white pixels (1) correspond to occluded region and
black pixels (0) correspond to the remaining unoccluded
region and (cid:12) is element-wise multiplication. Thus, the ﬁnal
training objective loss function can be written as,

Loss = λrec ∗ Lrec + λadv ∗ Ladv + λssim ∗ Lssim + λmask ∗ Lmask
(6)
where, λrec, λadv, λssim and λmask are the corresponding
weight parameters for each loss term.

For the task of learning a person-speciﬁc model, we
adopt a two-step training process, where the ﬁrst step and
second step are trained on unoccluded and occluded face
images of the person, respectively. In the ﬁrst step, we freeze
the attention module and only train the encoder-decoder
to reconstruct the unoccluded images. Here we start with
unsupervised training of only encoder-decoder on publicly
available state-of-the-art face datasets such as VGGFace [4]
and AffectNet [12] to leverage the inherent knowledge about
the face structure. This enables the model to grasp knowledge
about the basic facial structure and features such as eyes,
nose, mouth, etc. This step is common for all users, and later
we perform ﬁnetuning on user’s images with a wide range of
pose, expression, and appearance variations. This step helps
the model to learn the exact geometry of the user’s face.

In the second step, we unfreeze our attention module and
train the entire architecture on occluded images. This can
be considered as a self-supervised learning approach, where
the input is an occluded image, and the target image is its
corresponding unoccluded image. We, therefore, minimize
the loss between the reconstructed face image and the
unoccluded ground truth image. This enables the attention
module to learn to retain the high-frequency details from the
visible part of the occluded input image while performing
a soft fusion with the generated image. Thus, our two-
step training strategy yields superior de-occlusion with no
explicit boundaries between occluded and visible regions of
the reconstructed image.

IV. EXPERIMENTS AND RESULTS

1) Dataset: Our method uses short monocular RGB video
sequences. Hence, we captured various human subjects in
different appearances at a 1280x720 pixels resolution with
30 frames per second frame rate from a mobile phone
camera. The images are cropped and scaled to 256x256.
These sequences have a length of around 2 min, i.e., ap-

Fig. 4: Comparison with baseline and variants of our method.

prox—3600 frames per session. We use mutually exclusive
sets of these video sequences from the same subject
in
different appearances to train and evaluate our user-speciﬁc
model. The subjects were asked to engage in day-to-day
conversation and demonstrate variations in head poses.

To create a synthetic mask that simulates an HMD for
each video frame, we generated a binary mask guided by
the facial landmarks and placed it over the eye region to
occlude the face.

2) Implementation Details: For step 1 of training strategy
(see Section III-C), we train the encoder-decoder architec-
ture on unoccluded images with three loss functions (i.e.,
Equations 2,3,4) in a stage-wise manner with each loss
term is being added in the training objective with every
stage. We chose a batch size of 50 and an input resolution
of 256x256. We train the network for 300, 100, and 300
epochs, respectively, for each loss function’s incremental
addition. For step 2, we similarly train the encoder-decoder
architecture with an additional attention module with mask
loss (Equation 5) on occluded images for 300, 100, and 200
epochs, respectively, for each of the incremental addition
of loss functions. We use the Adam optimizer [9] with a
constant learning rate of 0.00002. We use λrec = 1, λadv =

0.25, λssim = 60 and λmask = 1.

3) Evaluation Protocols: We choose SSIM (Structural
Similarity Index), PSNR (Peak signal-to-noise ratio), and
LPIPS (Learned Perceptual Image Patch Similarity) as our
evaluation metrics for quantitative comparisons. For SSIM
and PSNR, higher the value better the reconstruction quality,
and for LPIPS, lower the value better the perceptual quality.
4) Results: In this section, we demonstrate the superiority
of our framework in terms of qualitative and quantitative
results. For a baseline method, we train the architecture from
[3], with the strategy mentioned in III-C.

Figure 4 shows qualitative results comparing our method
with baseline and variant of our method without mask loss.
Here we use test examples that are completely unseen ap-
pearances (not part of the training set). We can observe here
that the baseline model is unable to capture the background
appearance as it tries to hallucinate it from the training ex-
amples, whereas introducing the attention framework allows
the model to use the high-frequency background information
from the input image. Furthermore, adding the mask loss
introduces more consistency in the hallucination of the
masked region. As can be observed in row 3 of Figure 4,
introducing the mask allows the eyes to be open as it is with

Fig. 5: Qualitative comparison with different loss functions. From left to right: (A) Input, (B) GT, (C) with Lrec, (D) with
Lrec and Ladv, (E) with Lrec, Ladv and Lssim, (F) Ours (with Lrec, Ladv, Lssim and Lmask).

the ground truth face image. Similarly, Table I reports the
quantitative evaluation indicating the beneﬁt of our approach
for unseen appearances. We can observe that our method
achieves superior results in terms of all evaluation metrics.
It is important to note that though our method seems to
perform only marginally better than LaFin [20] in terms of
SSIM and LPIPS measures, yet there is a huge difference in
terms of qualitative results where our method signiﬁcantly
outperforms LaFin [20] in terms of consistency of face
appearance as shown Figure 6. This might be because, unlike
our methods, their method only inpaint the mask region and
hence might obtain a better score due to overlapping visible
regions in input and output images.

TABLE I: Quantitative comparison with other methods on
face reconstruction.

Method
Baseline [3]
LaFin [20]
Ours

SSIM↑
0.706
0.914
0.918

PSNR↑
19.627
23.693
29.025

LPIPS↓
0.176
0.0601
0.042

TABLE II: Ablation Study on different loss functions.

Method
Ours (Lrec)
Ours (Lrec + Ladv)
Ours (Lrec + Ladv + Lssim)
Ours (Lrec + Ladv + Lssim +Lmask)

SSIM↑
0.920
0.822
0.916
0.918

PSNR↑
29.249
27.638
28.973
29.025

LPIPS↓
0.072
0.141
0.045
0.042

image. The addition of Ladv introduces more sharpness to
the image. Finally, Lssim and Lmask make it more consistent
with facial features in the original image. Table II reports
quantitative evaluation indicating incremental importance of
all loss functions in our formulation. It is important to note
that, although the reported SSIM and PSNR value of our
combined loss function seems to be marginally lower than
using only L1 loss, we can clearly observe the disparity
in reconstructed images as shown in Figure 5(F) where
our combined loss yields superior reconstruction, including
sharper facial features as well as consistent plausible recon-
struction (closed eyes) as compared to using only L1 loss in
as shown in Figure 5(C).

6) Qualitative comparison with SOTA Inpainting Meth-
ods: We also compute qualitative evaluation against some
SOTA face inpainting methods. Figure 6 demonstrates re-
construction results using our person-speciﬁc method on an
unseen appearance v/s generalized inpainting from their pre-
trained models. It can be observed that these methods fail
to generalize well on images that are out of their training
distribution and also suffer from severe loss of identity in
frontal head poses itself. Nevertheless, we need to keep this
in mind while comparing that our model is user-speciﬁc and
has been trained on videos of the same user (but in different
appearance), whereas their model is generic. Please refer to
our supplementary video for an extended set of video results.

V. APPLICATION TO HYBRID TELEPRESENCE SYSTEM

5) Ablation Study on Losses: We also perform an ablation
study on the various loss functions used in our method
and demonstrate their performance qualitatively and quantita-
tively. In Figure 5, we show qualitative results on the unseen
appearances after training with different loss functions. We
can observe that training only with Lrec generates a blurry

Recent works on face video animation, such as [15],[18],
demonstrate that by just using sparse landmarks, a face image
can be animated reasonably well from a reference image and
show its application in low data bandwidth environments.

Figure 7 shows how our approach can be easily integrated
with these methods to handle HMD occlusions. We start

Fig. 6: Comparison with state-of-the-art inpainting methods.

Fig. 7: 2D facial animation using FOMM[15].

with our inpainting module that can de-occlude the input
image and reconstructs the missing region. Sparse features
are then generated on top of this using [15] and used to drive
a reference image. Thus, we can generate a consistent 2D
video feed from the input occluded video feed. Additionally,
this animated face can also be used for per-frame 3D face re-
construction tasks [23] and fed to other VR teleconferencing
users wearing a VR headset (as shown in Figure 7). Thus,
we can use our method to integrate users with or without

VR headsets in a single hybrid teleconferencing application.

VI. DISCUSSION

Our proposed approach promises to give superior results,
both qualitatively and quantitatively, as shown earlier. How-
ever, there still exists a scope of improvement by incorporat-
ing eye tracking and gaze information for further reﬁning our
results. As future work, we would like to improve our work

Fig. 8: 3D reconstruction of de-occluded frame using DF2Net[23].

in facial video inpainting by leveraging temporal information
across consecutive frames. Using modern HMD devices, we
can give extra supervision about the eye information to the
network to produces stable reconstruction in the eye region
that is consistent across frames.

VII. CONCLUSIONS
We proposed to learn a personalized model for face de-
occlusion, particularly as an application in VR teleconfer-
encing, where the face is partially occluded due to HMD.
To tackle this, we formulate the face de-occlusion problem
as an image inpainting task. Our proposed attention enabled
encoder-decoder network takes an HMD occluded face as
input and completes missing facial features, particularly the
eye region. The experiments show that our method works rea-
sonably well with the same person wearing different clothes,
facial appearances, poses, and expressions. To justify our
proposed solution, we compared against a variety of related
techniques such as image inpainting and facial animation and
evaluated quantitatively and qualitatively.

REFERENCES

[1] V. Blanz and T. Vetter. A morphable model for the synthesis of 3d
In Proceedings of the 26th Annual Conference on Computer
faces.
Graphics and Interactive Techniques, SIGGRAPH ’99, page 187–194,
USA, 1999. ACM Press/Addison-Wesley Publishing Co.

[2] K. W. Bowyer, K. Chang, and P. Flynn. A survey of approaches and
challenges in 3d and multi-modal 3d+ 2d face recognition. Computer
vision and image understanding, 101(1):1–15, 2006.

[3] B. Browatzki and C. Wallraven. 3fabrec: Fast few-shot face alignment
In Proceedings of the IEEE/CVF Conference on

by reconstruction.
Computer Vision and Pattern Recognition, pages 6110–6120, 2020.

[4] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vggface2:
A dataset for recognising faces across pose and age. In 2018 13th IEEE
international conference on automatic face & gesture recognition (FG
2018), pages 67–74. IEEE, 2018.

[5] M. Elgharib, M. Mendiratta, J. Thies, M. Niessner, H.-P. Seidel,
A. Tewari, V. Golyanik, and C. Theobalt. Egocentric videoconfer-
encing. ACM Transactions on Graphics (TOG), 39(6):1–16, 2020.
[6] G. Gafni, J. Thies, M. Zollh¨ofer, and M. Nießner. Dynamic neural
radiance ﬁelds for monocular 4d facial avatar reconstruction.
In
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 8649–8658, June 2021.

[7] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks,
2014.

Image-to-image
[8] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
In Proceedings
translation with conditional adversarial networks.
of the IEEE conference on computer vision and pattern recognition,
pages 1125–1134, 2017.

[9] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

arXiv preprint arXiv:1412.6980, 2014.

[10] A. Kubade, D. Patel, A. Sharma, and K. Rajan. Afn: Attentional
feedback network based 3d terrain super-resolution. In Proceedings
of the Asian Conference on Computer Vision, 2020.

[11] S. Lombardi, J. Saragih, T. Simon, and Y. Sheikh. Deep appear-
ance models for face rendering. ACM Transactions on Graphics,
37(4):1–13, Aug 2018.

[12] A. Mollahosseini, B. Hasani, and M. H. Mahoor. Affectnet: A database
for facial expression, valence, and arousal computing in the wild. IEEE
Transactions on Affective Computing, 10(1):18–31, 2017.

[13] N. Numan, F. ter Haar, and P. Cesar. Generative rgb-d face completion
In 2021 IEEE Conference on
for head-mounted display removal.
Virtual Reality and 3D User Interfaces Abstracts and Workshops
(VRW), pages 109–116. IEEE, 2021.

[14] A. Radford, L. Metz, and S. Chintala. Unsupervised representation
learning with deep convolutional generative adversarial networks,
2016.

[15] A. Siarohin, S. Lathuili`ere, S. Tulyakov, E. Ricci, and N. Sebe.
First order motion model for image animation. Advances in Neural
Information Processing Systems, 32:7137–7147, 2019.

[16] J. Thies, M. Zoll¨ofer, M. Stamminger, C. Theobalt, and M. Nießner.
FaceVR: Real-Time Facial Reenactment and Eye Gaze Control in
Virtual Reality. arXiv preprint arXiv:1610.03151, 2016.

[17] M. Wang, X. Wen, and S.-M. Hu. Faithful face image completion for
hmd occlusion removal. In 2019 IEEE International Symposium on
Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), pages 251–
256. IEEE, 2019.

[18] T.-C. Wang, A. Mallya, and M.-Y. Liu. One-shot free-view neural
talking-head synthesis for video conferencing. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 10039–10049, 2021.

[19] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption
generation with visual attention. Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua
Bengio arXiv (2015-02-10) https://arxiv. org/abs/1502.03044 v3, 2015.
[20] Y. Yang, X. Guo, J. Ma, L. Ma, and H. Ling. Laﬁn: Generative
landmark guided face inpainting. arXiv preprint arXiv:1911.11394,
2019.

[21] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang. Free-
arXiv preprint

form image inpainting with gated convolution.
arXiv:1806.03589, 2018.

[22] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang. Gen-
erative image inpainting with contextual attention. arXiv preprint
arXiv:1801.07892, 2018.

[23] X. Zeng, X. Peng, and Y. Qiao. Df2net: A dense-ﬁne-ﬁner network
for detailed 3d face reconstruction. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019.

