Fast Light-Weight Near-Field Photometric Stereo

Daniel Lichy1

Soumyadip Sengupta2

David W. Jacobs1

1University of Maryland, College Park

2University of Washington

dlichy@umd.edu, soumya91@cs.washington.edu, djacobs@cs.umd.edu

2
2
0
2

r
a

M
0
3

]

V
C
.
s
c
[

1
v
5
1
5
6
1
.
3
0
2
2
:
v
i
X
r
a

Figure 1. We present a fast light-weight approach for solving near-ﬁeld Photometric Stereo (PS), which is particularly useful for capturing
large objects, e.g. a sofa, in a small conﬁned space. We capture an object with an iPhone camera and a handheld ﬂashlight. Our method is
signiﬁcantly faster during inference and produces more accurate reconstructions than existing methods S20 [31] and L20 [18].

Abstract

We introduce the ﬁrst end-to-end learning-based solu-
tion to near-ﬁeld Photometric Stereo (PS), where the light
sources are close to the object of interest. This setup is es-
pecially useful for reconstructing large immobile objects.
Our method is fast, producing a mesh from 52 512×384
resolution images in about 1 second on a commodity GPU,
thus potentially unlocking several AR/VR applications. Ex-
isting approaches rely on optimization coupled with a far-
ﬁeld PS network operating on pixels or small patches. Us-
ing optimization makes these approaches slow and memory
intensive (requiring 17GB GPU and 27GB of CPU memory)
while using only pixels or patches makes them highly sus-
ceptible to noise and calibration errors. To address these
issues, we develop a recursive multi-resolution scheme to
estimate surface normal and depth maps of the whole im-
age at each step. The predicted depth map at each scale is
then used to estimate ‘per-pixel lighting’ for the next scale.
This design makes our approach almost 45× faster and 2◦
more accurate (11.3◦ vs. 13.3◦ Mean Angular Error) than
the state-of-the-art near-ﬁeld PS reconstruction technique,
which uses iterative optimization.
1. Introduction

In this work, we introduce a fast light-weight Photomet-
ric Stereo (PS) technique for near-ﬁeld illumination. Pho-
tometric Stereo aims to reconstruct object geometry from
a sequence of images captured with a static camera and
varying light sources. Existing near-ﬁeld PS approaches are
slow and extremely memory intensive. Being fast and light-
weight enables users to capture images and process them on
their laptop within a few seconds, allowing multiple retakes

if needed. This light-weight reconstruction technique can
be extremely useful for several AR/VR applications. While
our method is primarily developed for calibrated lighting,
in line with existing far-ﬁeld approaches, we also show how
our method can be extended to uncalibrated real-world cap-
tures by introducing a calibration network.

Near-ﬁeld PS is often preferred over far-ﬁeld or distant
lighting-based PS for both practical and theoretical rea-
sons. It is extremely useful for capturing large objects, e.g.
furniture or humans, especially in a conﬁned space like a
room [3, 16, 25]. This is because far-ﬁeld PS approaches
assume the lighting to be distant, e.g. 10× the object di-
mensions is suggested by [33, 34], causing it to be unsuit-
able for 3D imaging in many indoor spaces. Additionally,
low-intensity LED lights on handheld devices (e.g. ﬂash-
light on a phone) may not be bright enough to illuminate an
object from a large distance [28]. Theoretically, in the case
of uncalibrated lighting, near-ﬁeld PS has no linear ambigu-
ity in contrast to far-ﬁeld PS where there is the well-known
Generalized Bas-Relief ambiguity [4], as shown in [25].

We make our method fast and accurate by forgoing tra-
ditional optimization in favor of a recursive multi-scale al-
gorithm. Our proposed method consists of two recursive
networks one for predicting surface normal and another for
depth maps. At each step of the recursion, we increase the
input image resolution by a factor of 2. We ﬁrst analytically
estimate the relative lighting direction and attenuation fac-
tor for each pixel in the image (termed ‘per-pixel lighting’
for clarity) by upsampling the predicted depth map from
the previous step. We then infer the surface normal for this
scale given the input image, ‘per-pixel lighting,’ and esti-

1

 
 
 
 
 
 
mated normal map from the previous scale. Finally, the
depth map is predicted conditioned on the estimated nor-
mal map and the depth map from the previous scale. The
number of steps for this recursion is dictated by the input
image resolution making the inference extremely fast, re-
quiring only a few forward passes. We also improve infer-
ence speed by using a recursive deep network for estimating
depth map from normals instead of solving normal integra-
tion by e.g. solving the Poisson equation [13, 30], making
it more robust to outliers during training. The recursion al-
lows the use of one network for all scales, thus heavily re-
ducing the memory footprint. This approach is also more
robust to noise and lighting calibration errors than existing
per-pixel based methods [18, 31] as the recursion leads to a
larger receptive ﬁeld for the network.

Our method is built on the shoulders of existing near-
ﬁeld and far-ﬁeld PS techniques by adapting the ideas that
can best improve performance, inference speed, and mem-
ory requirements. Our recursive approach is inspired by
[17], which uses a single network for predicting normal at
each scale conditioned on the image and the estimated nor-
mal from the previous scale. It is non-trivial to adapt the
recursion idea proposed in [17] from far-ﬁeld distant light-
ing to near-ﬁeld because per-pixel lighting directions are not
known a priori. Our ablation study shows that a trivial ex-
tension of [17] to near-ﬁeld PS that does not reﬁne lighting
directions based on depth performs signiﬁcantly worse (by
3.5◦) than our proposed approach. The idea of using depth
map to predict ‘per-pixel lighting’ is inspired by [18,25,31].
However, these approaches operate on pixels or patches us-
ing iterative optimization, causing extensive memory usage,
slow inference speed and making them highly susceptible to
noise and lighting calibration errors.

We ﬁrst evaluate our method quantitatively on the
LUCES dataset [21] with calibrated lighting and show that
our method is 2◦ more accurate in surface normal predic-
tion (11.3◦ vs. 13.3◦ Mean Angular Error) than state-of-the-
art near-ﬁeld PS approach L20 [18], and another prior ap-
proach S20 [31]. In terms of computational efﬁciency, our
method requires 4GB CPU memory and 12GB GPU mem-
ory compared to 27GB CPU and 17GB GPU of L20 [18]
for 1024×786 resolution, while S20 [31] fails to scale up to
this resolution. Our inference speed is 1.3 secs compared
to 59.5 secs of L20 [18] and 2435 secs of S20 [31] for 52
512×384 resolution images; tested on the same hardware.
For many practical applications, such as quickly recon-
structing 3D models at the home, calibrated lighting is im-
practical. In the absence of calibrated lighting, we also in-
troduce an additional lighting calibration network. We ﬁrst
show that on the LUCES dataset with uncalibrated lighting
our method is more robust than existing approaches, pro-
ducing 14.11◦ Mean Angular Error (MAE) vs 18.85◦ of
L20 and 16.03◦ of S20. Finally, we capture a few real-world

objects with near-ﬁeld lighting with a commodity ﬂashlight
and show that our reconstructed mesh is qualitatively more
accurate than existing approaches S20 [31] and L20 [18],
after using the same calibration network, see Fig. 1 and 4.

In summary our contributions are as follows:

• A state-of-the-art, fast, light-weight, near-ﬁeld PS method
with 45× faster inference speed and signiﬁcantly lower
memory requirements than existing methods.

• We build on [17], developed for far-ﬁeld PS, by incor-
porating ‘per-pixel lighting’, adding recursive depth pre-
diction from normal, and allowing the ﬂexibility to use
unstructured lighting.

• We also introduce a calibration network to facilitate un-
calibrated capture in-the-wild with an iPhone camera and
a handheld ﬂashlight.

2. Prior Work

Research on Photometric Stereo (PS), introduced in [33],
can be divided along a number of dimensions: diffuse vs.
specular materials, calibrated vs. uncalibrated lighting, dis-
tant vs. nearby lights. In this work, we focus on near-ﬁeld
PS with both known and unknown lighting conditions.

Far-Field Photometric Stereo. We brieﬂy mention
some recent far-ﬁeld PS works that are particularly relevant
to this work. For a more comprehensive survey see [2, 9].
Our work is inspired by [17] which introduces a recursive
neural net to predict surface normal at each scale given the
input image at that scale and the predicted normal map from
the previous scale. The authors showed that using a re-
cursive architecture signiﬁcantly improves performance by
capturing global context that is often absent in per-pixel
techniques [14] and patch-based techniques [6].

Near-Field Photometric Stereo.
Solutions to near-ﬁeld PS can be roughly divided into

two broad approaches.

The ﬁrst approach relies on a three step iterative reﬁne-
ment [3, 5, 8, 18, 24, 25, 28], starting with an initial shape,
e.g. a plane, until convergence: (1) based on the current
shape calculate the light directions and intensity at each
point; (2) using these light estimates, predict surface nor-
mals; (3) integrate normals to update the shape. Logothetis
et al. [18] uses a per-pixel far-ﬁeld deep neural network in
step (2) while the rest of these methods are purely optimiza-
tion driven. In contrast, we use two deep recursive neural
nets for steps (2) and (3), trained on the whole image for
near-ﬁeld lighting.

Direct optimization approaches rely on inverting the im-
age formation process, often by solving a system of PDEs
[22, 23, 27–29]. For a detailed discussion of these methods
see [29]. In [34] the authors use a local-global mesh defor-
mation scheme to optimize a mesh that reconstructs the im-
ages. Santo et al. [31] also optimizes a reconstruction loss.
However, as part of their forward pass they decompose ob-

2

R
r0, ..., rR−1

I j
i
Ni
Di
Aj

i , Lj

i

pj, dj, µj
U p(I)
ones(r × r)

number of resolutions
sequence of resolutions r0 = 64,
ri+1 = 2ri, rR−1 input image reso-
lution
jth image at resolution ri
normal at resolution ri
depth at resolution ri
per-pixel light attenuation and direc-
tion at resolution ri for image j
light parameters of jth image
upsample I by a factor of 2
r × r array of ones

Table 1. Summary of major notations used throughout the text.

servations into reﬂectance and normal using a far-ﬁeld deep
neural network.

Light Calibration. Research on uncalibrated PS either
separately estimates lighting or alternately solves for light
and shape simultaneously using a variational approach [10].
For the former, the lighting estimation can be physically
performed by inserting additional objects [11, 16] in the
scene or by using a deep network for prediction [6, 7, 15].
While the these methods have been introduced for far-ﬁeld
PS, we propose a calibration network for near-ﬁeld PS.

Normal Integration. Normal integration techniques es-
timate a depth map that is consistent with a normal map. For
a detailed discussion see [30]. Ho et al. [12] uses the sim-
ilarity between normal integration and shape from shading
(SfS) to develop a normal integration technique. Similarly,
we also introduce a deep network for faster and stable nor-
mal integration during training based on SfS.

3. Background

In this section, we describe our image formation model
for near-ﬁeld Photometric Stereo (PS). Given M images
of an object (I 1, ..., I M ) captured under different known
anisotropic point light sources from a ﬁxed viewpoint, we
estimate the surface normal and the depth map. Addition-
ally, we assume the camera has known intrinsic parameters,
and the mean distance to the object is known (WLOG as-
sume mean distance is 1. See Appendix 7.2 for details).
This is the same setup as [18, 31]. In Sec. 4.4, we show
how to remove the restriction on known lights and mean
distance.

Camera Model We use the standard pinhole camera
model centered at the origin in world coordinates and look-
ing down the z-axis. The camera is speciﬁed by a 3×3 in-
trinics matrix K. Any world point X = (x, y, z), projects
onto a pixel (u, v) by the formula:

(u, v, 1)T ∼ K(x, y, z)T .

(1)

Geometry Model We only consider reconstructing the
visible region of an object. Therefore the object is com-
pletely described by a normal and depth map. Concretely,

X(u, v) ∈ R3 describes a point on the object appearing
in pixel (u, v). Then we can deﬁne the depth map by
D(u, v) = X(u, v)3, where the subscript 3 refers to the 3rd
i.e. z component of X(u, v). We can also recover X(u, v)
from the depth map D(u, v) following eqn. 2:

X(u, v) = D(u, v)K −1(u, v, 1)T

(2)
If n(X) is the normal at the point X then the normal
map is deﬁned by N (u, v) = n(X(u, v)). Since X(u, v) is
a parametrization, we can also calculate the normal map as:

.

N =

∂u × ∂X
( ∂X
∂v )
∂u × ∂X
(cid:107)( ∂X
∂v )(cid:107)
Light Model We assume each image I j is illuminated
by an anisotropic point light source. We describe this light
by a position pj ∈ R3, a direction dj ∈ S2, and an angular
attenuation coefﬁcient µj ∈ R. We assume all lights have
unit intensity. If that is not the case, we divide the image by
the intensity of the light sources.

(3)

We can then describe the direction of the light arriving at

a point X on the surface of the object by:

Lj(X) =

(X − pj)
(cid:107)X − pj(cid:107)

,

and the attenuation of the light at the same point by:

Aj(X) =

(Lj · dj)µj
||X − pj||2 .

(4)

(5)

Thus lighting at any pixel (u, v), given the depth map
D(u, v), can be described by a direction term Lj(X(u, v))
and an intensity attenuation term Aj(X(u, v)) (where X is
expressed with depth D by eqn. 2). To keep it concise, we
term these lighting factors, relative direction and attenua-
tion, at each pixel ‘per-pixel lighting’.

Admissible

lights The

conﬁguration of possible
anisotropic point lights is huge, taking 3+2+1 parameters
to describe. To remedy this we restrict ourselves to lights
with positions in a cylinder around the camera and direction
pointing roughly toward the object. We term this region the
‘admissible light region’. It covers positions of lights used
in most existing datasets (e.g. [21, 31]) and the uncalibrated
data we capture. For the exact speciﬁcation of the admissi-
ble light region please see Appendix 7.3.

Reﬂectance Model We model the reﬂectance as a gen-
eral spatially varying BRDF that depends on the lighting
direction ωl, the viewing direction ωv and the position on
the surface X. Denote this as B(ωl, ωv, X).

Rendering Equation Now given the depth map D, nor-
mal N, camera intrinsics K, and light parameters pj,dj,µj,
we can write the rendering equation for the jth image as a
function of (u, v):
I j(u, v) = Aj(X)B(ωv, Lj(X))(N (u, v) · Lj(X)) + η(u, v)
(6)
where η represents indirect lighting effects such as shadows
and inter-reﬂections. Note that ωv = −X/(cid:107)X(cid:107) because the
camera is centered at the origin.

3

4. Our Approach

We aim to predict normal map N and depth map D,
given a set of images I 1, ..., I M . We propose a recursive
solution to this problem. We introduce two recursive net-
works, one for predicting normal GRN (·; θRN ) and another
for predicting depth GRD(·; θRD) given normals. At each
step of the recursion we increase the image resolution by
a factor of two and use these two networks to predict the
depth map and the normal map. For a robust and accurate
normal estimation, we calculate the ‘per-pixel lighting’ (Lj
and Aj) and use it as an input to the normal estimation net-
work, which we ablate in Sec. 5.4.

Lichy et al. [17] introduced a similar recursive normal
estimation network, RecNet, for far-ﬁeld PS. They showed
that the recursive network has a large receptive ﬁeld and
produces high-quality reconstruction by reﬁning the predic-
tions from the previous scale. We also ﬁnd this idea to be
suitable to produce fast and light-weight inference. Thus
we developed our own version of recursively reconstructing
the object for near-ﬁeld PS which we describe in Sec. 4.1.
Network architecture, training data and loss functions are
described in detail in Sec. 4.2.

The key differences between our approach and [17] are:
• We create synthetic data for training that emulates near-

ﬁeld capture with lighting in the admissible region.

• We calculate the per-pixel lighting and use it as an extra
input to the recursive normal estimation network, which
improves performance by 3.5◦, as shown in ablation study
(Sec. 5.4).

• We introduce a recursive normal to depth integration net-
work, which is fast and robust during training. Predicted
depth map is then used for calculating the per-pixel light-
ing in the next scale.

• Unlike RecNet, which requires a ﬁxed sequence of lights,
our method is permutation invariant to lighting order and
can use arbitrary lighting within the admissable region.

4.1. Recursive Reconstruction

We ﬁrst initialize the recursion with input resolution of

r0 = 64 × 64.
• We ﬁrst calculate the ‘per-pixel

lighting’ parameters
0(X) and Aj
Lj
0 by assuming the depth map is a plane at
depth 1 (see, Sec. 3 and Appendix 7.2). This calculation
is done following Algo. 1.

N0 = GIN ({I j

• Then we use an initial normal estimation network
GIN (·; θIN ), which takes the input image and the per-
pixel lighting parameters to predict the normal map N0:
0 , Lj
(7)
• Finally, we introduce another initialization network to
predict a depth from the normals: D0 = GID(N0; θID)
The recursive network progressively increases input im-
age resolution by a factor of 2, until it reaches the input
image resolution. The steps of the recursive network are in

j=1); θIN )

0, Aj

0}M

principle similar to the initialization network, except for the
fact that the normal and depth estimation networks GIN and
GID do not use any recursion and simply predict at low res-
olution in a feed-forward fashion. The steps of the recursion
are explained below:
• For each step i with resolution ri × ri, we ﬁrst calculate
the per-pixel lighting (Algo. 1) using the depth map of
the previous scale Di−1 upsampled by a factor of 2.

• Then normal map Ni is predicted with the recursive nor-
mal prediction network GRN (·; θRN ), given the input im-
ages and per-pixel lighting along with depth map Di−1
and normal map Ni−1 of the previous scale following:
i }M

j=1, Ni−1, Di−1; θRN )

Ni = GRN ({I j

i , Lj

i , Aj

(8)

• Finally we predict the depth map Di from the normal
map Ni using another recursive network GRD(·; θRD),
which is conditioned on the depth map of the previous
scale Di−1: Di = GRD(Ni, Di−1; θRD)
The forward pass of our recursive process is also sum-

marized in Algo. 2.

Algorithm 1 Calculate the per-pixel lighting given depth D.

1: PPLight(K, D, µ, p, d)
2: X[u, v] = D[u, v]K −1(u, v, 1)T
3: L[u, v] = normalize(X[u, v] − p)
4: A[u, v] = (L[u,v]·d)µ
||x[u,v]−p||2
5: return A, L

0, Aj

0, Aj

0 = PPLight(K, ones(r0 × r0), µj, pj, dj)
0)}M

Algorithm 2 Forward pass of our approach: See Tab. 1
deﬁnition of the notation.
1: Lj
2: N0 = GIN ({(I j
0 , Lj
3: D0 = GID(N0; θID)
4: for i = 1 to R-1 do
Lj
i , Aj
i = PPLight(K, U p(Di−1), µj, pj, dj)
5:
6: Ni = GRN ({(I j
i )}M
j=1, Ni−1; θRN )
7: Di = GRD(Ni, Di−1; θRD)
8: end for

j=1; θIN )

i , Lj

i , Aj

4.2. Implementation Details

Network Architectures. Our method consists of four
neural networks, two for initialization and two for recursion
with similar architectures for initialization and recursion.

i and Lj

i and returns a feature F j

The normal estimation networks consist of a shared en-
coder that takes in each image I j
i concatenated with its per-
pixel lighting maps Aj
i with
dimension 128 at 1/4’th of the input resolution. In the re-
cursion step, the normal from the previous step bilinearly
upsampled by a factor of 2 is used as additional input. Then
we perform a max pooling operation over the features F j
i s
from all input images to produce a combined feature, which
is passed to a decoder to produce a normal map.

4

The depth prediction network takes in the normal esti-
mated by the normal prediction network (in the recursive
case the encoder takes in the depth from the previous step
bilinearly upsampled by a factor of 2) and produces a depth
map. It also does some preprocessing to correct for a per-
spective camera. Speciﬁcally, it applies a transformation
(e.g., see [30] or Appendix 7.4.3) so that in the perspective
case normal integration amounts to solving ∇u = (p, q)
where u is the logarithm of depth and p, q are determined
by the normal map and camera intrinsics. Architecturally,
it is an encoder-decoder ResNet architecture similar to [17].
Details can be found in Sec. 4.3 and Appendix 7.5.

Loss Function. We train our network with three loss
functions. We use depth loss Ldepth and normal loss
Lnormal to produce accurate reconstruction. We also use a
loss to ensure the normals derived from the predicted depth
map are consistent with those derived from the ground truth
depth map. This loss is necessary to produce smooth depth
maps. We term this loss Lnf d, nfd is an abbreviation for
‘normal from depth’. The losses are deﬁned as:

Ldepth = (cid:80)R−1
Lnormal = (cid:80)R−1
Lnf d = (cid:80)R−1

i=0 ||Di − ¯Di||1,
i=0 ||Ni − ¯Ni||1,
i=0 ||nf d(Di) − nf d( ¯Di)||1,

(9)

(10)

(11)

where we use a bar above a letter to indicate the Ground
Truth (GT) measurement. nf d is the function that takes
a depth map and produces a normal map. This is imple-
mented using eqn. 2 and 3. In eqn. 3 we approximate the
derivatives with a central ﬁnite difference.

Training Details. Our network is trained completely on
synthetic data. First we generate depth, normal, spatially-
varying albedo, and Cook-Torrance roughness maps using
14 objects from the statue dataset [32] and freely available
albedo maps from [1]. These are rendered at 512×512 reso-
lution. At training time, for each normal, depth, albedo, and
roughness, 10 lights are uniformly randomly sampled from
the admissible region 3. With a 50% probability, we replace
the object’s material with one from the MERL dataset [20].
We then render the 10 images using eqn. 6.

For augmentation, we randomly zero patches and add
random noise to each pixel to simulate the indirect light-
ing term η in eqn. 6. Images are also randomly cropped
to simulate a diverse set of camera intrinsics. We trained
our network end-to-end for 22 epochs using the Adam opti-
mizer with learning rate 0.0001. Training took about 2 days
on 4 Nvidia P6000 GPUs.
4.3. Normal Integration Network

We found that existing normal integration algorithms are
too slow for use during training of a neural network. Addi-
tionally, they fail on our challenging synthetic data due to
large discontinuities. Our solution is to replace a classical
normal integration routine with a network, but this is a non-

trivial task. Solving normal integration requires global in-
formation (details in Appendix 7.2), but convolutional net-
works have limited receptive ﬁelds, and therefore cannot
take global information into account for large enough im-
ages.

RecNet, a recursive architecture introduced in [17], cre-
ates a convolutional network with potentially inﬁnite recep-
tive ﬁeld. We found a straight forward application of Rec-
Net fails for normal integration. We believe this has to do
with the relation between normals and depth. To understand
this, we look at the opposite problem i.e. we want to train
a network to predict normals from depth. To keep things
simple let’s consider the orthographic case in 1D, where es-
timating the normal is the same as estimating the derivative.
We consider an image as discrete samples of a function
on domain [0, 1]. Let 0 = x1, ..., xr = 1 be the sample
points and let h = 1/r be the distance between them, where
r is the image resolution. Let u be the depth and ui =
u(xi). Let u(cid:48) be the derivative of u and [u(cid:48)]i = u(xi). Let
{ui} indicate the sequence of all the elements ui.

h

h

Suppose we train a fully convolutional network to pre-
dict normal {[u(cid:48)]i} from depth {ui} at a resolution r.
It
will learn something similar to a ﬁnite difference and re-
turn { ui+1−ui−1
}. Now if we test the network on an image
{vi} that has a higher resolution say e.g. 2r. Then the net-
work will predict { vi+1−vi−1
}, but this is not the desired
result. The correct result is { vi+1−vi−1
}, this is because
the network does not know the resolution has changed. In
this case, there is a simple solution: predict {ui} from the
resolution independent {[u(cid:48)]i · h} instead of {[u(cid:48)]i}

h/2

This suggests that when we solve the inverse problem
we should try to learn a function G that takes {[u(cid:48)]i · h} and
predicts {ui}: {ui} = G({[u(cid:48)]i · h}). This is impossible
for a fully convolutional network because it requires global
information. However, if we already know a low-resolution
estimate of {ui}, termed {wj}, we then learn a function:
{ui} = G({[u(cid:48)]i · h}, {wj}),

(12)

i.e. we predict depth from normal and a low resolution es-
timate of depth. We argue that this is possible for a fully
convolutional network. By applying eqn. 12 recursively,
we can gradually reconstruct a full resolution depth map.
This is the essential idea of our depth prediction network.
For more on this argument and the depth prediction network
please see Appendix 7.4.
4.4. Lighting calibration

Setting up calibrated lights in-the-wild is very challeng-
ing. Recent works have shown that in the far-ﬁeld case
lighting calibration can be accomplished with a neural net-
work [6, 15]. We are not aware of any learning based ap-
proach to near-ﬁeld lighting calibration.

Since, in the near-ﬁeld case, there is much more freedom
of possible light conﬁgurations, we make some additional

5

simplifying assumptions on the light: (1) the light intensity
is the same in all images. (2) the light is well modeled by
an isotropic point source i.e µ = 0 and d is irrelevant. (3)
The light is within the admissible region. We found that
these assumptions are good enough to estimate light from a
handheld ﬂashlight used for capture in-the-wild.

We use essentially the same architecture as [6] to esti-
mate light positions. This network uses a shared feature
extractor to extract a feature F j from each image I j. It then
creates a context c = maxjF j. Finally, a second network
is applied to feature Fj and the context c to produce a light
position estimate pj for image I j. To deal with a perspec-
tive camera, all input images are cropped or zero padded to
have the same intrinsics.

5. Experimental Evaluation

We evaluate our method quantitatively on the LUCES
dataset [21] in Sec. 5.1, and qualitatively on a dataset we
captured with a handheld ﬂashlight and iPhone in Sec. 5.3.
We mainly compare our results to two state-of-the-art near-
ﬁeld Photometric Stereo (PS) algorithms S20 [31] and L20
[18]. In the case of uncalibrated capture, we use our calibra-
tion network described in Sec. 4.4 for S20 and L20, which
are only developed for calibrated lighting conditions.

5.1. Quantitative Evaluation on LUCES [21]

The LUCES dataset consists of 14 objects, each captured
in HDR under 52 calibrated near-ﬁeld lighting conditions.
We evaluate using the mean angular error (MAE) for normal
and mean depth error (MZE) metrics.

Calibrated. In Tab. 2, we present MAE and MZE ob-
tained by our method and compare it with existing works
as reported in LUCES [21]. The table includes results
from two pure optimization near-ﬁeld methods L17 [19] and
Q17 [29], two hybrid near-ﬁeld methods using deep learn-
ing and optimization S20 [31] and L20 [18], and the far-
ﬁeld deep method I18 [14]. All methods are evaluated at the
2048×1536 resolution, except S20 which was evaluated at
512×384 due to its GPU memory requirements [21].

Tab. 2 shows our method outperforms all existing meth-
ods in terms of MAE, especially the state-of-the-art method
L20 (MAE 13.33◦ vs. 11.32◦). Using our integration net-
work we are the second best in MZE, L20 outperforming us
by 1.26mm. We found that our normal integration network
can develop jumps at discontinuities (see Appendix 7.8 for
details), which increase our MZE. We tried resolving this
issue by integrating our normal map predictions as a post
processing step. We used the optimization approach of [26],
also used by L20 for this step. After this post processing,
our MZE dropped to 2.93mm, and our method becomes the
top performer. We label our method with this post process-
ing step as MZE+int in Tab. 2. It is tempting to try replacing
our normal integration network with a traditional integra-
tion algorithm during training, but we ﬁnd these algorithms

Figure 2. We compare the predicted normal map and an error map
w.r.t. GT of our approach with that of S20 [31] and L20 [18] on
sample objects from the LUCES [21] with calibrated lighting.
struggle with our synthetic training data due to discontinu-
ities in the data. Furthermore, these methods are very slow
for use during network training.

Tab. 2 also reports the MAE achieved by differenti-
ating the ground truth depth map with a ﬁnite difference
(Diff-MAE) and the depth error obtained by integrating the
ground truth normal with [26] (Int-MZE). These errors are
due to the discrete nature of images and discontinuities in
the object, see [21] for detailed discussion.

We show normal prediction results and error maps from
each of these methods in Fig. 2. Depth error visualizations
are included in Appendix 7.9.

Uncalibrated.

In Tab. 3 we compare our method to
L20 and S20 where the ground truth lighting calibration is
replaced with the results of our calibration network. Since
our calibration network only handles the case of equal inten-
sity lights we scale each image by their ground truth inten-
sity. Additionally lights are assumed to be isotropic point

6

Method
L17- [19]

Q18- [29]

S20- [31]

L20- [18]

I18- [14]

Ours

GT

Error
MAE
MZE
MAE
MZE
MAE
MZE
MAE
MZE
MAE
MZE
MAE
MZE
MZE int
Diff-MAE
Int [26]-MZE

Bell
28.25
4.45
25.8
12.03
9.5
1.9
14.74
1.53
23.55
5.93
6.20
2.28
1.71
2.5
0.08

Ball
9.77
0.81
12.12
2.5
25.42
5.5
12.43
0.67
44.29
6.59
8.55
1.83
1.26
2.69
0.22

Buddha Bunny
20.15
7.51
13.73
7.06
12.5
6.02
8.15
2.49
36
6.88
8.63
2.73
2.16
2.93
2.30

11.5
4.67
14.07
9.28
19.17
5.53
10.73
3.27
35.29
10.92
12.69
16.60
3.93
2.69
3.28

Die
11.95
4.58
13.77
5.91
5.23
2.76
6.55
4.44
41.52
7.83
5.16
2.76
2.16
2.49
0.56

Hippo House
29.69
15.42
6.99
3.19
30.63
18.51
8.02
6.8
28.02
23.12
6.15
7.04
7.75
30.03
1.82
9.14
49.05
44.9
8.98
7.59
29.00
8.01
7.39
3.52
7.07
3.44
9.19
3.2
7.43
1.28

Cup
30.76
2.67
37.63
4.83
14.22
1.62
23.35
2.04
35.78
3.17
17.28
2.00
1.82
2.85
0.02

Owl
13.77
3.64
14.74
5.83
13.08
3.75
12.39
3.44
40.27
8.67
12.32
3.08
2.85
4.3
3.51

Jar
10.56
6.56
15.66
16.87
9.27
6.09
8.6
3.86
40.66
15.54
5.32
6.58
3.99
1.79
0.12

Queen
13.05
1.89
13.16
6.92
16.62
3.91
10.96
1.94
32.89
8.08
12.90
3.09
2.90
4.22
3.25

Squirrel Bowl
12.5
15.93
4.37
1.82
11.19
14.06
6.48
2.55
12.44
14.07
5.22
2.81
8.78
15.12
2.80
1.01
28.04
41.09
6.69
5.8
7.07
13.00
3.78
3.94
3.61
1.88
2.27
3.26
0.12
1.12

Tool Average
15.1
3.25
16.12
6.69
17.42
4.68
17.05
5.90
31.71
12.45
12.33
2.48
2.27
2.34
0.13

17.03
4.02
17.94
7.27
15.72
4.5
13.33
3.17
37.5
8.22
11.32
4.43
2.93
3.34
1.67

Table 2. Evaluation on LUCES dataset with calibrated lighting. Mean angular error (MAE in degrees) and mean depth error (MZE in mm).

Method
S20- [31]

L20- [18]

Ours

Error
MAE
MZE
MAE
MZE
MAE
MZE
MZE int

Bell
13.43
2.98
13.97
4.21
7.17
1.80
2.32

Ball
13.68
2.58
15.50
3.21
6.59
1.40
0.76

Buddha Bunny
11.41
21.85
4.91
8.48
14.71
18.92
4.88
10.39
14.50
11.75
3.84
10.27
2.79
5.52

Die
5.86
2.52
16.14
5.58
8.63
2.77
2.41

Hippo House
36.28
11.33
2.85
10.31
32.06
16.20
12.04
4.29
31.00
10.64
10.04
3.59
8.68
3.32

Cup
17.63
1.95
23.80
2.18
18.98
2.64
2.07

Owl
17.67
6.44
17.81
5.59
15.92
4.13
4.77

Jar
11.60
6.18
17.65
10.14
9.14
7.35
4.97

Queen
15.96
4.85
20.79
8.27
18.39
4.59
5.15

Squirrel Bowl
13.92
17.24
4.39
3.18
11.83
21.45
3.32
3.26
10.17
15.97
3.15
3.51
2.60
3.59

Tool Average
16.54
2.38
23.00
6.12
18.61
6.93
6.08

16.03
4.57
18.85
5.96
14.11
4.71
3.93

Table 3. Evaluation on LUCES with uncalibrated lighting. Mean angular error (MAE in degrees) and mean depth error (MZE in mm).

sources i.e. we set µj = 0 in each method. Our method
and L20 were evaluated at 1024×768 resolution. S20 was
evaluated at 512×384 resolution due to GPU memory lim-
itations. Results of S20 were then bilinearly upsampled to
1024×768 for error evaluation.

We again observe that our method is the best in MAE
(14.11◦ vs. 16.03◦ for S20). Using our normal integration
network we are slightly surpassed by S20 in MZE (4.57mm
vs. 4.71mm). However, after using post-processing normal
integration we improve our MZE to 3.93mm. Results and
error maps are shown in Fig. 3.
5.2. Computational Resources

method
S20- [31]
L20- [18]
Ours
L20- [18]
Ours

res.
512
512
512
1024
1024

time(s)
2435.0
59.5
1.3 (2.0)
200.0
4.0 (6.9)

cpu (GB)
5
8
4
27
4

gpu (GB)
20
5
9
17
12

Table 4. Comparison of computational resources. Our method
produces signiﬁcantly faster inference while consuming less CPU
and GPU memory than S20 and L20. The quantities in brackets
for our method indicate post-processing normal integration. S20
cannot operate on 1024 resolution (res) due to memory limitations.
We compare the memory usage and inference speed
of our method to that of S20 and L20 in Tab. 4. All
methods were tested on the same machine with a 24GB
Nvidia P6000 GPU and 128GB of main memory. We com-
pare our method and L20 at two resolutions 512×384 and
1024×768. S20 is only compared at 512×384 due to its
GPU memory requirements.

Inference Speed We calculate inference speed without
the time required for data reading and writing, which can

vary depending on the cluster load. Our method is 45×
faster than the closest competitor L20 at both 512 and 1024
resolution. Adding the normal integration post-processing
step to our method increases the runtime by about 50%, still
leaving our method 30× faster than L20. S20 is over 1000×
slower than our method.

CPU memory The amount of CPU memory used by S20
and our method are essentially ﬁxed for a given number of
input images. In contrast, L20 uses CPU memory approxi-
mately proportional to image resolution.

GPU memory At 512 resolution S20 requires 20GB of
GPU memory to process LUCES, see Tab. 4. This was the
highest resolution we were able to run on a 24GB GPU and
is consistent with that reported in [21]. GPU usage for L20
and our method are more moderate, however there are some
subtleties that must be taken into account to fairly compare
GPU usage. See Appendix 7.6 for details.

with per-pixel lighting estimation
w/o per-pixel lighting estimation

MAE MZE
4.43
11.32
4.89
14.88

Table 5. We show that using per-pixel lighting as input to the recur-
sive normal prediction network improves reconstruction accuracy.
5.3. Qualitative Comparison on Captured Data

We captured a dataset of medium to large size objects us-
ing an iPhone12 mounted on a tripod and a handheld ﬂash-
light. We capture a short video (5-10 second) of each object
while moving the ﬂashlight around the admissible region.
Every ﬁfth frame from the video was then used as input im-
ages. Images were inverse tonemapped by raising them to
the power 2.2. Lighting positions were estimated with our
calibration network.

We compare our method with that of S20 [31] and

7

Figure 3. We compare the predicted normal map and an error map
w.r.t. GT of our approach with that of S20 [31] and L20 [18] on
sample objects from the LUCES [21] with uncalibrated lighting.
L20 [18], presented in Fig. 4 and 1, with more in Appendix
7.9. L20’s performance is strongly affected by noise due
to its per-pixel normal prediction network. S20’s predicted
normal maps show strong checkerboard patterns.
5.4. Ablation Study

i and Lj

Importance of Per-Pixel Lighting We train a variant of
our network that does not require per-pixel lighting estima-
tion as input to the normal estimation network. Concretely,
rather than using the per-pixel lighting Aj
i as input
to the network GIN and GRN we only input the lights’ pa-
rameters pj, dj, µj at each pixel. Note that, these light pa-
rameters are associated with global lighting conditions and
i and Lj
do not reﬂect per-pixel lighting effects unlike Aj
i .
We observe that explicitly estimating and using per-pixel
lighting as input improves the performance from 14.88◦ to
11.32◦ MAE. This also shows that a more direct adapta-
tion of the recursion idea presented by Lichy et al. in [17]
from far-ﬁeld to near-ﬁeld PS, is less effective without us-
ing per-pixel lighting estimation. We discuss this result in

8

Figure 4. Qualitative evaluation on images captured with a hand-
held ﬂashlight and iPhone 12 camera mounted on a tripod. Our
method outperforms L20 and S20.
more depth in Appendix 7.7.

6. Conclusion

In this work, we introduce a fast light-weight solution to
near-ﬁeld Photometric Stereo. Existing approaches rely on
optimization with point or patch based inferences, which
are more susceptible to noise, lack global context, and
are memory intensive and slow. The key innovation of
this work lies in creating a system that can capture global
context to produce accurate predictions while being light-
weight and fast. We adapt [17] from far-ﬁeld PS to near-
ﬁeld by introducing per-pixel lighting estimation, a recur-
sive normal integration network, and extend it to handle
arbitrary lighting. We show in an ablation study that a
straightforward adaption of [17] produces worse results
than our approach. We also introduce a calibration net-
work and show how our approach can be used for capturing
3D geometry of large and midsize real-world objects, like
furniture, backpacks, etc. Our method signiﬁcantly outper-
forms state-of-the-art methods on both the LUCES dataset
and real-world captures while being fast and light-weight.

Acknowledgment This research is supported by the

NSF under grant no. IIS-1910132.

References

[1] 3d textures. https://3dtextures.me/. Accessed:

2020. 5

[2] Jens Ackermann and Michael Goesele. A survey of photo-
metric stereo techniques. Foundations and Trends® in Com-
puter Graphics and Vision, 9(3-4):149–254, 2015. 2

[3] Jahanzeb Ahmad, Jiuai Sun, Lyndon Smith, and Melvyn
Smith. An improved photometric stereo through distance es-
timation and light vector optimization from diffused maxima
region. Pattern Recognition Letters, 50:15–22, 2014. 1, 2
[4] P.N. Belhumeur, D.J. Kriegman, and A.L. Yuille. The bas-
In Proceedings of IEEE Computer Soci-
relief ambiguity.
ety Conference on Computer Vision and Pattern Recognition,
pages 1060–1066, 1997. 1

[5] Alexandre Bony, Benjamin Bringier, and Majdi Khoudeir.
Tridimensional reconstruction by photometric stereo with
near spot light sources. 21st European Signal Processing
Conference (EUSIPCO 2013), pages 1–5, 2013. 2

[6] Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita,
and Kwan-Yee K. Wong. Sdps-net: Self-calibrating deep
photometric stereo networks. In CVPR, 2019. 2, 3, 5, 6, 13
[7] Guanying Chen, Michael Waechter, Boxin Shi, Kwan-Yee K
Wong, and Yasuyuki Matsushita. What is learned in deep
uncalibrated photometric stereo? In European Conference
on Computer Vision, 2020. 3

[8] Toby Collins and Adrien Bartoli. 3d reconstruction in la-
paroscopy with close-range photometric stereo. Medical im-
age computing and computer-assisted intervention : MIC-
CAI ... International Conference on Medical Image Com-
puting and Computer-Assisted Intervention, 15 Pt 2:634–42,
2012. 2

[9] Jean-Denis Durou, Maurizio Falcone, Yvain Qu´eau, and Sil-
via Tozza. Advances in photometric 3d-reconstruction, 2020.
2

[10] Bjoern Haefner, Zhenzhang Ye, Maolin Gao, Tao Wu, Yvain
Qu´eau, and Daniel Cremers. Variational uncalibrated photo-
metric stereo under general lighting. 11 2019. 3

[11] Aaron Hertzmann and Steven M. Seitz. Example-based
photometric stereo: Shape reconstruction with general,
IEEE Trans. Pattern Anal. Mach. Intell.,
varying brdfs.
27(8):1254–1264, aug 2005. 3

[12] Jeffrey Ho, Jongwoo Lim, Ming Hsuan Yang, and David
Kriegman.
Integrating surface normal vectors using fast
marching method. In Computer Vision - ECCV 2006, 9th Eu-
ropean Conference on Computer Vision, Proceedings, Lec-
ture Notes in Computer Science (including subseries Lecture
Notes in Artiﬁcial Intelligence and Lecture Notes in Bioin-
formatics), pages 239–250, 2006. 9th European Conference
on Computer Vision, ECCV 2006 ; Conference date: 07-05-
2006 Through 13-05-2006. 3

[13] Berthold K.P Horn and Michael J Brooks. The variational
approach to shape from shading. Computer Vision, Graphics,
and Image Processing, 33(2):174–208, 1986. 2

[14] Satoshi Ikehata. Cnn-ps: Cnn-based photometric stereo for
In Proceedings of the Euro-
general non-convex surfaces.
pean Conference on Computer Vision (ECCV), September
2018. 2, 6, 7

[15] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Fer-
rari, and Luc Van Gool. Uncalibrated neural inverse render-
ing for photometric stereo of general surfaces. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 3804–3814, June 2021.
3, 5

[16] Jingtang Liao, Bert Buchholz, Jean-Marc Thiery, Pablo
Bauszat, and Elmar Eisemann. Indoor scene reconstruction
using near-light photometric stereo. IEEE Transactions on
Image Processing, 26:1089–1101, 2017. 1, 3

[17] Daniel Lichy, Jiaye Wu, Soumyadip Sengupta, and David W.
Jacobs. Shape and material capture at home. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 6123–6133, June 2021. 2,
4, 5, 8

[18] Fotios Logothetis, Ignas Budvytis, Roberto Mecca, and
Roberto Cipolla. A cnn based approach for the near-ﬁeld
photometric stereo problem. ArXiv, abs/2009.05792, 2020.
1, 2, 3, 6, 7, 8, 13, 14, 15

[19] Fotios Logothetis, Roberto Mecca, and Roberto Cipolla.
Semi-calibrated near ﬁeld photometric stereo. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017. 6, 7

[20] Wojciech Matusik, Hanspeter Pﬁster, Matt Brand, and
Leonard McMillan. A data-driven reﬂectance model. ACM
Transactions on Graphics, 22(3):759–769, July 2003. 5
[21] Roberto Mecca, Fotios Logothetis, Ignas Budvytis, and
Roberto Cipolla. Luces: A dataset for near-ﬁeld point light
source photometric stereo. ArXiv, abs/2104.13135, 2021. 2,
3, 6, 7, 8

[22] Roberto Mecca, Yvain Qu´eau, Fotios Logothetis, and
Roberto Cipolla. A single-lobe photometric stereo approach
for heterogeneous material. SIAM J. Imaging Sci., 9:1858–
1888, 2016. 2

[23] Roberto Mecca, Aaron Wetzler, A. Bruckstein, and R. Kim-
mel. Near ﬁeld photometric stereo with point light sources.
SIAM J. Imaging Sci., 7:2732–2770, 2014. 2

[24] Ying Nie, Zhan Song, Ming Ji, and Lei Zhu. A novel cali-
bration method for the photometric stereo system with non-
isotropic led lamps. 2016 IEEE International Conference on
Real-time Computing and Robotics (RCAR), pages 289–294,
2016. 2

[25] Thoma Papadhimitri and Paolo Favaro. Uncalibrated near-
light photometric stereo. BMVC 2014 - Proceedings of the
British Machine Vision Conference 2014, 01 2014. 1, 2
[26] Yvain Qu´eau and Jean-Denis Durou. Edge-preserving inte-
gration of a normal ﬁeld: Weighted least-squares, tv and l1
approaches. In SSVM, 2015. 6, 7, 14

[27] Yvain Qu´eau, Roberto Mecca, and Jean-Denis Durou. Un-
biased photometric stereo for colored surfaces: A variational
approach. 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 4359–4368, 2016. 2
[28] Yvain Qu´eau, Tao Wu, and Daniel Cremers. Semi-calibrated

near-light photometric stereo. In SSVM, 2017. 1, 2

[29] Yvain Qu´eau, Bastien Durix, Tao Wu, Daniel Cremers, Fran-
cois Lauze, and Jean-Denis Durou. Led-based photometric

9

stereo: Modeling, calibration and numerical solution. Jour-
nal of Mathematical Imaging and Vision, 60, 03 2018. 2, 6,
7

[30] Yvain Qu´eau, Jean-Denis Durou, and Jean-Franc¸ois Aujol.
Journal of Mathematical

Normal integration: A survey.
Imaging and Vision, 60, 05 2018. 2, 3, 5, 12

[31] Hiroaki Santo, Michael Waechter, and Yasuyuki Matsushita.
Deep near-light photometric stereo for spatially varying re-
In European Conference on Computer Vision
ﬂectances.
(ECCV), 2020. 1, 2, 3, 6, 7, 8, 13, 14, 15

[32] Olivia Wiles and Andrew Zisserman. Silnet : Single- and
multi-view reconstruction by learning from silhouettes.
In
British Machine Vision Conference 2017, BMVC 2017, Lon-
don, UK, September 4-7, 2017. BMVA Press, 2017. 5
[33] Robert J. Woodham. Photometric Method For Determining
Surface Orientation From Multiple Images. Optical Engi-
neering, 19(1):139 – 144, 1980. 1, 2

[34] Wuyuan Xie, Chengkai Dai, and Charlie C. L. Wang. Pho-
tometric stereo with near point lighting: A solution by mesh
deformation. 2015 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 4585–4593, 2015.
1, 2

10

7. Appendix

7.1. Overview

This supplement includes additional details that could
not be put into the main paper due to space restrictions. In
Sec. 7.2 we deﬁne a relative coordinate system (where mean
depth is 1) and show how to relate absolute coordinates to
this coordinate system. In Sec. 7.3, we deﬁne the admissi-
ble light region in terms of this relative coordinate system.
In Sec. 7.4, we discuss the normal integration problem and
elaborate more on our depth prediction network. In Sec. 7.5
we give the low-level details of our network architectures.
In Sec. 7.6, we add some additional details regarding GPU
memory usage. In Sec. 7.7, we explain why the network
without per-pixel lighting from our ablation study performs
poorly. In Sec. 7.8, we demonstrate how certain errors can
form in our depth prediction network. Finally, in Sec. 7.9,
we present some additional experimental results.

7.2. Coordinate System and Scale

This section shows how to pick a coordinate system with
a mean depth of one and how this resolves the global scale
ambiguity in the uncalibrated case. Equations from the
main paper Sec. 3 are included below for quick reference.

X(u, v) = D(u, v)K −1(u, v, 1)T

Lj(X) =

Aj(X) =

,

(X − pj)
(cid:107)X − pj(cid:107)
(Lj · dj)µj
||X − pj||2 .

(13)

(14)

(15)

I j(u, v) = Aj(X)B(ωv, Lj(X))(N (u, v) · Lj(X)) + η(u, v)
(16)
Light scale We assume that all images have the same
light intensity. However, we train our model such that the
exact value of this intensity is unimportant, i.e. if we mul-
tiply all the images by a constant factor, we get the same
results.

We get this effect by dividing each image by the mean in-
tensity of the ﬁrst image I 0 in the input set i.e. µintensity =
meanu,vI 0(u, v) then the input to the network is the image
set I j(u, v)/µintensity.

Mean depth We assume mean depth is known. We can
assume the mean depth is one by changing units. In particu-
lar, if µdepth is the mean depth then we can replace D(u, v)
with Dj(u, v)/µdepth and pj with pj/µdepth. From equa-
tions 13, 14, 15, and 16 we see that this just scales the image
intensity by µ2
depth, but, as stated above, our network is in-
variant to the image intensity scale factor.

Uncalibrated Scale Ambiguity There is a global scale
factor ambiguity between the light positions and depth,
which is exactly why we can assume the mean depth is one.

We train our calibration network on data with mean depth
one, so the network predicts lights in this relative coordinate
system. This resolves the scale ambiguity.

7.3. Admissible Light Region

Now that we have deﬁned our relative coordinate system
with mean depth one 7.2, we can deﬁne the admissible light
region in terms of it.

We deﬁne the admissible light region as a cylinder with
its axis along the camera optical axis (a.k.a. z-axis) and ra-
dius 0.75. The extent of the cylinder is from 0.15 behind
the camera plane to 0.15 in front of the camera plane. Fur-
thermore, we specify the admissible light directions as the
directions making an angle of 30◦ or less with the z-axis.

Note that in absolute units, the size of the admissible re-
gion depends on the distance the object is from the camera.
For example, if we are capturing a big object, we would
place the camera farther away, and thus in absolute units,
the admissible region will be larger.

7.4. Normal Integration

In this section, we present the mathematical intuition that
inspired our depth prediction network. We then explain the
details of the network’s application. This section is not par-
ticularly rigorous, but we found that the network it inspired
works well in practice.

7.4.1 High Level Idea

Problem Statement Give functions p and q on some do-
main, we want to ﬁnd a function on the domain satisfying
the PDE

∇U = (p, q)

(17)

This is equivalent estimating depth from a normal map,
see 7.4.3. In general, a solution U may not exist. In which
case, we want to ﬁnd some approximate solution.

Necessity of Global Information We can see that solv-
ing eq. 17 requires global information as follows. Observe
that given any solution to eq. 17 we can obtain another so-
lution by adding a constant to it. Now suppose we broke the
domain of interest into patches and produced a solution for
each patch. Because each patch solution could have a dif-
ferent offset, we would have to look outside the patch to ﬁnd
the proper offset needed to glue the patch solutions together
continuously. Therefore, a standard feed-forward network,
which can only look at patches in very high-resolution im-
ages due to its limited receptive ﬁeld, can not generalize to
high-resolution data.

Proposed Solution Suppose we divide the depth into
patches as before, but we are given the mean depth of each
patch. Then we could solve the equation on each patch and

11

set the patch mean to the given mean, thus producing a so-
lution.

Concretely, consider a rectangular domain. Divide the
region into smaller rectangles call them P1,...,PN . Let µk =
meanx∈Pk U (x). Suppose we knew the function V (x) given
by

V (x) = µk if x ∈ Pk

(18)

i.e. V is constant on the patches.

Now we solve 17 individually on each patch Pk, call the
solution Uk. Furthermore, choose the Uk such that they
have mean value zero. Deﬁne

A-B
BN
Relu(x)

conv kn fm sp

Conv kn fm sp
ConvL kn fm sp
convt kn fm sp

apply layer A then layer B
BatchNorm
Leaky Relu activation with parameter
x if ‘(x)’ is omitted x=0 (i.e. standard
Relu)
convolution layer with kernel of size
n with m ﬁlters and stride p. If stride
is 1 we will omit s1.
conv kn fm sp - BN - Relu
conv kn fm sp - BN - Relu(0.1)
transposed convolution layer with
kernel of size n and m ﬁlters and
stride p.
conv k3 fn - BN - Relu - conv k3 fn
- BN - +input. This deﬁnes the resid-
ual block. +input indicates adding the
input value to the output value.
bicubic upsampling by a factor of 2
hyperbolic tangent activation

Res n

Upsample
tanh

Table 6. Deﬁnition of notations for network architecture

7.4.3 Perspective correction

Suppose an image is taken with focal length f, depth,
D(u, v) and normals, N (u, v). Deﬁne U = ln(D) and
p = −
and q = −
. Then
U ,p, and q satisfy 17. Therefore, given normals we can
make this transformation and solve for U then recover D as
D = exp(U ). For a derivations see [30].

N2
uN1+vN2+f N3

N1
uN1+vN2+f N3

7.4.4 Alternative Fast Normal Integration Methods

We considered FFT and DCT based integration methods
that are fast enough for network training. However, be-
cause these assume a periodic or rectangular domain, they
perform poorly on the datasets we tested, which all have ir-
regular domains. For more discussion on this issue please
see [30] Sec. 3.3 and 3.4.

7.5. Network Architectures

In this section, we deﬁne the low-level architectures of
all the networks used in the paper. In Tab. 6 we deﬁne the
notation used to describe network layers.

7.5.1 Normal Prediction Network

Both the initial, GIN , and recursive, GRN , normal predic-
tion networks have the same architecture. They consist of a
shared feature extractor deﬁned by

• FE = Conv k7 f32 - Res 32 - Res 32 - Conv k3 f64 s2
- Res 64 - Res 64 - Conv k3 f128 s2 - Res 128 -
conv k3 f128

and a normal regressor deﬁned by

12

W (x) = Uk(x) if x ∈ Pk

(19)

Then we can produce a solution to 17 as

U (x) = W (x) + V (x) if x ∈ Pk

(20)

In other words

∇W = ∇(U (x) − V (x)) = (p, q) − ∇V (x)

(21)

can be solved by just looking at the individual patches
Pk. Then U (x) satisfying 17 can be recovered as W (x) +
V (x). Thinking of V as an upsampled version of a low-
resolution approximation to U is the motivation for our
depth prediction network explained next.

7.4.2 Depth Prediction Network

Now we give the details of the depth prediction networks
forward pass. The main paper does not distinguish between
the preprocessing the depth prediction network does and the
convolutional network proper. Here we use GID and GRD
for the networks and preprocessing combined (G∗D to refer
to both) and N ET∗D to refer to the convolution nets proper.
Algo. 3 gives the forward pass for G∗D (in the initial
network, GID, the input depth is just a plane at z = 1).
Where D is the central ﬁnite-difference

D[U ] = (U(m+1)n − U(m−1)n, Um(n+1) − Um(n−1)) (22)

Note that in Algo. 3 line (**) is just the discrete analog
of Eq. 21, where we multiply (p, q) by the step size hi for
the reasons explained in the main paper.

Algorithm 3 Forward pass of depth prediction network

1: G∗D(Ni, Di−1)
2: pi, qi = from Ni using perspective correction
3: Ui−1 = ln(U psample[Di−1])
4: res = N ET∗D(hi(pi, qi) − D[Ui−1]) (**)
5: Ui = Ui−1 + res
6: Di = exp(Ui)
7: return Di

• NR = Res 128 - convt k3 f64 s2 - BN - Relu - Res 64
- Res 64 - convt k3 f32 s2 - BN - Relu - conv k7 f3

The application of the network is given by

Ni = NR(

M
max
j=1

FE(I j

i , U psample(Ni−1)))+U psample(Ni−1)

(23)
Due to the max-pooling the network is invariant to the

image ordering.

7.5.2 Depth Prediction Network

The architectures of the initial depth prediction network,
GID, and the recursive depth prediction network, GRD, are
the same. They are given by

• Conv k7 f32 - Res 32 - Res 32 - Conv k3 f64 s2
- Res 64 - Res 64 - Conv k3 f128 s2 - Res 128 -
Res 128 - Upsample - Conv k3 f64 - Res 64 - Res 64
- Upsample - Conv k3 f32 - conv k7 f1 - tanh

The full application of the normal prediction network is

deﬁned in Sec. 7.4.

7.5.3 Light Prediction Network

The general architecture of the light prediction network is
taken from [6]. Like the normal prediction network, it con-
sists of a feature extractor LFE and a regressor LR deﬁned
by

• LFE = ConvL k3 f64 s2 - ConvL k3 f128 s2 -
ConvL k3 f128 - ConvL k3 f128 s2 - ConvL k3 f128
- ConvL k3 f256 s2 - ConvL k3 f256

• LR = ConvL k3 f256

- ConvL k3 f256 s2

-

ConvL k3 f256 s2 - ConvL k3 f256 s2

It also has three ﬁnal coordinate regressors CR i for each

coordinate x = x1, y = x2, z = x3 deﬁned by:

• CR i = ConvL k1 f64 - convL k1 f1

First the network extracts a feature F j from each image
with the LFE network F j = LFE(I j). It then forms a con-
text c = maxj F j. Then for each feature F j it applies the
LR network to F j concatenated with the context c to form
position features P F j = LR(F j, c). Finally, it applies the
coordinate regessor CR i to each position feature P F j to
the k coordinate of the light
get the light coordinates i.e.
position in image j is:

• pj

k = CR i(P F j)

method
S20- [31]
L20- [18]
Ours
L20- [18]
Ours

res.
512
512
512
1024
1024

time(s)
2435.0
59.5
1.3 (2.0)
200.0
4.0 (6.9)

cpu (GB)
5
8
4
27
4

gpu (GB)
20
5
9
17
12

Table 7. Comparison of computational resources. Our method
produces signiﬁcantly faster inference while consuming less CPU
and GPU memory than S20 and L20. The quantities in brackets
for our method indicate post-processing normal integration. S20
cannot operate on 1024 resolution (res) due to memory limitations.
7.6. GPU Memory Usage Details

This section includes some additional information re-
garding the GPU memory usage of method L20 and our
method.

L20 The memory usage of L20 is dependent on the batch
size (number of pixels) that the network processes at one
time. For our experiments, we used a batch size of 512.
GPU memory usage could potentially be reduced by de-
creasing the batch size.

Ours Despite our method requiring 12GB of GPU mem-
ory to run LUCES at 1024 on our cluster, with a Nvidia
P6000, we were able to run LUCES at 2048 resolution on
a desktop computer with an 8GB Nvidia RTX2080 GPU.
This indicates that our method is even more light-weight
than indicated in table 7, which is also in the main paper.

7.7. Ablation Details

This section explains why the network without per-pixel

lighting performs worse than with per-pixel lighting.

A convolutional neural network essentially applies the
same function to each input patch. Suppose we have two
patches, one on the left side of the image and one on the
In the ablated network
right side, that appear the same.
with only global light positions as input, the patches look
identical, and the network must predict the same normal.
However, the lighting direction and intensity at these two
patches are really different, so the patches need to be inter-
preted differently. The network using per-pixel lighting can
distinguish between these two patches because they have
different per-pixel lighting, and therefore it can produce dif-
ferent accurate results.

7.8. Limitations

Jumps at discontinuities As we mention in the main
text, our depth prediction method can develop jumps at dis-
continuities. We show the most extreme example of this
type of jump in Fig. 5.

7.9. Additional Results

This section we presents some additional results and ﬁg-

ures.

13

Figure 6. Additional results on data captured by us.

Figure 5. Depth prediction network developing jumps at disconti-
nuities. The jump is highlighted in the red box. Traditional normal
integration method [26] solves this issue (bottom row).

8 Image Results We compared S20 [31], L20 [18], and
our method on a subset of 8 randomly selected images from
the LUCES dataset. They are images: 5, 7, 13, 19, 26, 39,
48, 50. We show the results in Tab. 8. S20’s [31] code has
a bug that caused it to fail with NaN values on two objects:
Bell and House. Therefore, we report mean errors for all
objects (average 14) for L20 and our methods, and the av-
erage over the 12 objects that S20 worked on (average 12).
We observe that our method only drops 1.2◦ MAE (from
11.32◦ to 12.44◦) when tested on this subset. Whereas the
other methods drop nearly 3◦ MAE.

Additional Results on Our Data In Fig. 6, we compare
the results of S20 [31], L20 [18], and our method on data
we captured.

Additional Calibrated Results In Fig. 7 we present
some additional normal maps from our tests on the LUCES
dataset in the calibrated case. In Fig. 8 we show the depth
error maps for each method in the calibrated case.

Additional Uncalibrated Results In Fig. 9 we present
some additional normal maps from our tests on the LUCES
dataset in the uncalibrated case. In Fig. 10 we show the
depth error maps for each method in the uncalibrated case.

Figure 7. Additional normal predictions and error maps (in de-
grees) on the LUCES dataset in the calibrated case.

14

Method
S20- [31]

L20- [18]

Ours

Error
MAE
MZE
MAE
MZE
MAE
MZE
MZE int

Bell

20.03
3.42
8.84
2.04
1.68

Ball
18.08
2.91
24.43
6.44
9.64
2.12
1.46

Buddha Bunny
11.69
3.85
11.85
3.20
9.31
3.21
2.22

27.2
6.09
12.67
4.15
13.59
13.27
4.70

Die
10.06
3.31
7.18
1.78
5.99
2.91
2.43

Hippo House
12.58
2.37
14.12
3.22
8.75
3.20
3.13

30.74
8.49
29.43
7.13
6.21

Cup
23.06
3.62
25.63
3.33
21.62
2.85
2.23

Owl
13.36
4.52
15.72
5.73
11.43
3.51
4.01

Jar
14.19
8.83
9.22
4.47
7.13
7.84
4.41

Queen
16.93
3.30
13.12
4.26
13.38
3.06
3.01

Squirrel Bowl
12.46
18.59
3.71
3.13
17.88
15.68
8.08
2.05
8.73
13.10
3.84
3.68
1.97
4.09

Tool Average 14 Average 12
15.32
3.62
19.01
9.64
13.18
3.04
3.07

16.13
4.10
15.54
4.69
11.32
4.38
3.06

16.95
4.87
12.44
4.41
3.19

Table 8. Evaluation on LUCES with only 8 input images per object with calibrated lighting. Mean angular error (MAE in degrees) and
mean depth error (MZE in mm). S20 failed with NaN errors on Bell and House. Average 14 is the average error with all 14 LUCES objects
and Average 12 is the average error of the objects excluding Bell and House.

15

Figure 8. Depth error maps (in mm) on the LUCES dataset in the calibrated case

16

Figure 9. Additional normal predictions and error maps (in de-
grees) on the LUCES dataset in the uncalibrated case.

17

Figure 10. Depth error maps (in mm) on the LUCES dataset in the uncalibrated case

18

