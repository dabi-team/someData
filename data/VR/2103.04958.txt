F-CAD: A Framework to Explore Hardware Accelerators for
Codec Avatar Decoding
Xiaofan Zhang1, Dawei Wang2, Pierce Chuang2, Shugao Ma2, Deming Chen1, Yuecheng Li2
1University of Illinois at Urbana-Champaign, 2Facebook Reality Labs Research
{xiaofan3, dchen}@illinois.edu, {dawei.wang, pichuang, shugao, yuecheng.li}@fb.com

1
2
0
2

r
a

M
8

]

R
A
.
s
c
[

1
v
8
5
9
4
0
.
3
0
1
2
:
v
i
X
r
a

Abstract—Creating virtual avatars with realistic rendering is one of the
most essential and challenging tasks to provide highly immersive virtual
reality (VR) experiences. It requires not only sophisticated deep neural
network (DNN) based codec avatar decoders to ensure high visual quality
and precise motion expression, but also efﬁcient hardware accelerators to
guarantee smooth real-time rendering using lightweight edge devices, like
untethered VR headsets. Existing hardware accelerators, however, fail
to deliver sufﬁcient performance and efﬁciency targeting such decoders
which consist of multi-branch DNNs and require demanding compute and
memory resources. To address these problems, we propose an automation
framework, called F-CAD (Facebook Codec avatar Accelerator Design),
to explore and deliver optimized hardware accelerators for codec avatar
decoding. Novel technologies include 1) a new accelerator architecture
to efﬁciently handle multi-branch DNNs; 2) a multi-branch dynamic
design space to enable ﬁne-grained architecture conﬁgurations; and 3)
an efﬁcient architecture search for picking the optimized hardware
design based on both application-speciﬁc demands and hardware resource
constraints. To the best of our knowledge, F-CAD is the ﬁrst automation
tool that supports the whole design ﬂow of hardware acceleration of
codec avatar decoders, allowing joint optimization on decoder designs
in popular machine learning frameworks and corresponding customized
accelerator design with cycle-accurate evaluation. Results show that the
accelerators generated by F-CAD can deliver up to 122.1 frames per
second (FPS) and 91.6% hardware efﬁciency when running the latest
codec avatar decoder. Compared to the state-of-the-art designs, F-CAD
achieves 4.0× and 2.8× higher throughput, 62.5% and 21.2% higher
efﬁciency than DNNBuilder [1] and HybridDNN [2] by targeting the
same hardware device.

I. INTRODUCTION

Codec avatars are photo-realistic and three-dimensional reproduc-
tions of human appearances and real-time expressions. It is one of the
most impressive breakthroughs that helps people achieve VR telep-
resence with more effective communications with not only speaking
and listening but also facial expressions and body languages [3]–[5].
The whole system is shown in Fig. 1, where all the information (e.g.,
a wry smile and a furrowed brow) of the transmitter (TX) will be
encoded, transmitted, and decoded after reaching the receiver (RX)
to generate the TX’s codec avatar for high-ﬁdelity social presence.
Among them, the decoder is the most complex module, occupying
90% of the calculations required by the entire system. Without
effective optimizations, it can easily become the bottleneck and hinder
the smooth running of VR telepresence.

Following the popularity of VR/AR headsets, social demands have
been increasing and requesting real-time and high-quality codec
avatar decoding. However, deploying codec avatar decoders on VR
headsets presents signiﬁcant challenges. The state-of-the-art decoders
are compute- and memory-intensive. For example, the decoder we
introduce in Sec. II contains more than 13.6 GOP (Giga
will
operations) and 7.2 million parameters, while most of the headsets
can only provide limited computation, memory, and power budgets.
VR applications also ask for higher refresh rate (90 or even 120
FPS) compared to non-VR applications (30 FPS) to prevent motion
sickness and provide real-time response for smooth user interactions.
It requires hardware to deliver high throughput without using large
batch sizes, as the extra delay in collecting batch inputs may fail to
meet the real-time requirement. In addition, emerging decoders start

Fig. 1. Social interactions with codec avatars using head-mount devices
(HMDs) in VR. The TX’s information is captured by the built-in sensor,
compressed into a TX code by the encoder, and passed through the network.
The RX then starts decoding based on the virtual environment and the state
of the RX, and eventually displays TX’s information on the HMD.

adopting complicated multi-branch DNNs with customized neural
network layers to generate different components of the codec avatar,
such as one branch for facial geometry and another for textures, and
these branches may have very different requirements. These unique
challenges make codec avatar decoders difﬁcult to be effectively
handled by existing hardware accelerators. Evaluated by a state-of-
the-art commercial SoC processor (Snapdragon 865 [6]) and two
recently published DNN accelerators from academia (DNNBuilder
[1] and HybridDNN [2]), we have found that all these accelerators
failed to deliver satisfactory performance and efﬁciency required by
this compelling application.

To address these challenges, we propose F-CAD, a new automation
tool for accelerating multi-branch DNNs with complicated layer
dependencies. We focus on codec avatar decoding in this paper
as an important and practical use case of F-CAD to deliver the
optimized hardware accelerators by meeting speciﬁc performance
targets under resource budgets. To summarize, the main contributions
are as follows.

• This is the ﬁrst work that focuses on building electronic design
automation tools and providing rapid hardware accelerator design
and exploration to leverage VR avatar applications for resource-
constrained devices.

• We propose a novel elastic architecture to ﬂexibly support multi-
branch DNNs with complicated layer dependencies and a well-
constructed architecture unit to support up to three-dimensional
parallelism (3D parallelism) for high throughput and efﬁciency.
• We deﬁne a multi-branch dynamic design space to cover all possi-
ble hardware design combinations, which allows F-CAD to obtain
the optimized solution with the best achievable performance.
• We integrate a DSE (design space exploration) engine to leverage
efﬁcient explorations within the predeﬁned space and deliver the
best accelerator by considering various customized constraints,
such as available resources, maximum parallelism, maximum
batch size, different branch priority, etc.

II. BACKGROUND OF CODEC AVATAR

Codec avatar is formulated as a view-dependent Variational Auto-
the

Encoder (VAE) framework [3], [4]. As described in Fig. 2,

 
 
 
 
 
 
TABLE I
NETWORK ARCHITECTURE OF THE TARGETED DECODER

Br.
1
2
3

[Input size]→Network→[Output size]
[4,8,8]→[CAU]×5+C→[3,256,256]
[7,8,8]→ [CAU]×2+C→[3,1k,1k]
[CAU]×5+ C→[2,256,256]

GOP
1.9 (10.5%)
11.3 (62.4%)
4.9 (27.1%)

Parameters
1.1M (12.1%)
6.1M (67.0%)
1.9M (20.9%)

TABLE II
EVALUATIONS BY 865 SOC (@1450MHZ) [6], DNNBUILDER
(@200MHZ) [1] AND HYBRIDDNN (@200MHZ) [2]

865 (8-bit)

DNNBuilder
(8-bit)

HybridDNN
(16-bit)

Scheme
-
1
2
3
1
2 & 3

Utilization
-
DSP: 644, BRAM: 723
DSP: 1044, BRAM: 861
DSP: 1820, BRAM: 1197
DSP: 512, BRAM: 576
DSP: 1024, BRAM: 1120

FPS
35.8
30.5
30.5
30.5
12.1
22.0

Efﬁciency
16.9%
81.6%
50.4%
28.8%
77.5%
70.4%

Fig. 2. The pipeline of running codec avatar: from images captured by the
TX to VR avatar displayed by the RX. The decoder is the most complicated
part with a multi-branch DNN to generate different components of the avatar
(e.g., mesh vertices in Br. 1, the view-speciﬁc texture in Br. 2, etc.).

encoder E takes sensor-captured images X as inputs and generates
an l-dimensional latent (TX) code z:

z ← E(X), z ∈ Rl
(1)
where E is a trained DNN transforming spatial features into a
latent code. This code and the view code (v, indicating the RX’s
view direction) are processed by the decoder D to generate graphic
components of avatars. In Fig 2, D outputs facial geometry and
appearance at the ﬁrst two branches:

M, T ← D(z, v), M ∈ Rn×3, T ∈ Rw×h
(2)
where D is a multi-branch DNN with deconvolution-like structures
for reconstructing TX’s realistic VR appearances. M represents the
facial shape comprising n-vertices, and T is the view-dependent RGB
texture with w×h resolution. In the state-of-the-art designs, decoders
are much more complicated than encoders, which contribute more
than 90% of operations of the whole VAE framework. Therefore,
decoders urgently need high-performance and efﬁcient accelerators.
In this paper, we target a state-of-the-art codec avatar decoder for
facial animation. It is shown in Table I with three branches (Br.) for
generating facial geometry (3D vertices), UV texture (a 2D surface
of a 3D model following U- and V-axis), and warp ﬁeld (specular
effects), of which the second and third branches have a common front
part. The input of Br. 1 is reshaped from a 256-dimensional latent
code while the input of Br. 2 and 3 is the combination of both latent
and view code to have texture appearance conditioned by different
view angles. We adopt C, A, and U to represent the customized
Conv, activation, and up-sampling layer, respectively, while use ×
to indicate the number of repetitions. In total, the decoder contains
13.6 GOP and 7.2 million parameters without repeatedly count-
ing the shared part. Unlike general DNNs, the decoder introduces
complex data dependencies by adopting multi-branch structures and
high-deﬁnition (HD) intermediate results for high-quality VR avatar
textures. Other features come from the customized Conv, where each
output pixel has its dedicated bias (aka untied bias) instead of sharing
one bias across pixels within the same output channel.

III. ACCELERATOR DESIGN CHALLENGES

The unique multi-branch feature and customized layers from
codec avatar decoders cause complicated dataﬂows and high compute
and memory demands during inference, which make existing DNN

Fig. 3. Latency of the last ﬁve Conv layers in Br. 2 executed by DNNBuilder
when targeting FPGAs with increasing resource budgets in Scheme 1 ∼ 3 .

accelerators ineffective. Challenges include the enormous (13.6 GOP)
and unevenly distributed (mainly occurring in Br. 2) computations and
the substantial memory footprints (with the intermediate feature map
size up to 16×1024×1024). This becomes even more challenging for
hardware accelerators with a limited resource but aiming at real-time
response with high throughput performance.

We select three existing accelerators from industry (Snapdragon
865 SoC [6]) and academia (DNNBuilder [1], HybridDNN [2]) to
accelerate codec avatar decoding. For 865 SoC, we run the targeted
decoder shown in Table I. Since DNNBuilder and HybridDNN have
not supported the customized Conv, we create a mimic decoder
by replacing the customized Conv with the conventional one while
keeping the rest of network structure unchanged. The mimic decoder
has a highly similar structure but 3.7% less computations, which
can still provide insights to identify bottlenecks of the existing
accelerator designs. During evaluation, we use two performance
indicators including FPS (to indicate the throughput) and efﬁciency
(the ratio between actual and theoretical peak throughput as Eq. 3 to
evaluate whether an accelerator works efﬁciently). β represents the
number of operations handled by one multiplier in one clock cycle.
For example, β = 2 for 16-bit operands in FPGA where multipliers
are implemented by DSPs.

EF F I =

GOP per Second
β × # of M ultiplier × F REQ

(3)

As shown in Table II, the 865 SoC only delivers 35.8 FPS even
though it integrates an AI engine for DNN workloads. The major
bottleneck is its limited cache size, which causes frequent data trans-
fers and severely restricts performance. So its overall efﬁciency barely
reaches 16.9%. We then target three FPGAs (Xilinx Z7045, ZU17EG,
and ZU9CG corresponding to Scheme 1, 2, and 3) with increasing
resource budgets and let DNNBuilder and HybridDNN generate ac-
celerators with unfolded and folded accelerator structures for running
the mimic decoder, respectively. DNNBuilder achieves slightly lower
throughput (30.5 FPS) and much higher efﬁciency (81.6%) using the

ﬂow will get enough attentions in the Optimization step. For example,
Br. 2 and 3 of the targeted decoder share the same front-end, layers
from this part will be assigned to Br. 2 as it is more critical and
contains higher computation demand. After fusion and reorganization,
F-CAD imports and expands the proposed elastic architecture (Sec.
V) along X and Y dimensions according to the layer and branch
number, respectively. Eventually, a basic accelerator is generated and
will be optimized in Step 3.

In Optimization step, the accelerator design space (Sec. VI-A) is
ﬁrst determined. The layers and branches of the decoder contribute
to the higher dimensional design space so it becomes complex to
search for the optimized design. F-CAD introduces a DSE engine
(Sec. VI-B) to leverage both cross-branch and in-branch optimization.
A stochastic search is applied in the cross-branch optimization
to explore resource distribution schemes across branches; while a
greedy search is adopted for in-branch optimization, ﬁnding the best
accelerator candidate for each branch by considering design spaces
and available resources. After that, accelerator candidates are evalu-
ated against performance, efﬁciency, and customization requirements.
The DSE engine eventually generates the globally optimized design
through an iterative process.

V. ACCELERATOR ARCHITECTURE

A. A layer-based multi-pipeline accelerator paradigm

The design paradigm of our proposed accelerator is presented in
Fig. 5 (a). Inputs of each branch (e.g., three 256-dimensional latent
codes named In a1 ∼ a3 for branch 1) are processed in a pipeline
manner and passed through all pipeline stages belonging to that
branch. For branches with the shared part, corresponding stages are
assigned to one of the branches following the layer reorganization
strategy. For example, Br. 2 and 3 share the ﬁrst two layers, so stage
1 ∼ 2 are assigned to Br. 2 in this case, while the subsequent stages
are separately executed. The results of stage 2 are distributed to two
different branches. We also adopt the ﬁne-grained pipeline design
from [1] to lower the pipeline initial latency.

B. The elastic architecture with 2D expansion capability

To enable the proposed accelerator paradigm, F-CAD introduces
an elastic architecture to ﬂexibly expand the accelerator following
two dimensions. In Fig. 5 (b), this elastic architecture consists of
basic architecture units that are arranged in a two-dimensional plane
reﬂecting the layer reorganization results and each unit is responsible
for one pipeline stage. For example, the expansion following the X-
axis, such as unit instances (1,1), (1,2), and (1,3), means more stages
(three in this case) need to be handled in this branch; while the
expansion along the Y-axis represents more branches are used in the
targeted decoder. In this example, F-CAD generates an accelerator
with three pipelines corresponding to Br. 1 ∼ 3.

Inside the basic architecture unit, there are three types of resources
as: computation (yellow area), on-chip memory (blue and green
areas), and external memory (red area) resources. The input feature
map from the previous layer is passed horizontally from the left and
a fraction of it is kept in the input buffer (InBuf) to provide timely
data supply. Meanwhile, DNN parameters are fetched from external
memory and stored in the weight buffer (WeightBuf) following the
computation order. Each basic architecture unit is highly conﬁgurable
to meet various requirements from different layer stages. It supports
the proposed 3D parallelism, which includes two unrolling factors
along the output and input channels (kernel parallelism factor kpf and
channel parallelism factor cpf ) and the partition factor of the input
feature map (H-partition). After conﬁguration, H-partition number

Fig. 4. The proposed F-CAD design ﬂow with three major steps to deliver
optimized hardware accelerators for codec avatar decoding.

Z7045 FPGA in Scheme 1 compared to the 865 SoC. Its unfolded
structure allows dedicated DNN layer acceleration for higher design
speciﬁcity. Still, it fails to scale using more resources in Scheme 2
and 3 and suffers deteriorating efﬁciency. By analyzing the last ﬁve
Conv layers in Br. 2, there are layers (circled in Fig. 3) that can
not be further accelerated due to the lack of parallelism. Regarding
a Conv-like layer with InCh input and OutCh output channels,
DNNBuilder only provides two-level parallelism with the maximum
parallel factor pf = InCh × OutCh. A layer with limited channel
number (e.g., Conv7 with 16 input and output channels) is likely
to become a computation bottleneck once DNNBuilder reaches the
maximum parallel factor (e.g., 16 × 16 = 256) and stops scaling. For
HybridDNN, we adopt a 16-bit mimic decoder as 8-bit model is not
supported. The scalability is slightly better than DNNBuilder when
handling more abundant resources in Scheme 2. However, its folded
structure and coarse-grained conﬁguration prevent further scaling in
Scheme 3, and it generates an accelerator with the same size as that
from Scheme 2.

IV. F-CAD AUTOMATION DESIGN FLOW

To address these challenges, we propose F-CAD to design and
explore customized hardware accelerators for multi-branch DNNs and
leveraging codec avatar decoding with high throughput and efﬁciency.
As shown in Fig. 4, F-CAD directly connects to popular machine
learning frameworks and takes the developed decoder models as
inputs as well as arbitrary hardware budgets and differentiated branch
priority for more reﬁned customization.

In Analysis step, F-CAD starts analyzing the targeted network by
extracting not only layer-wise information (e.g., layer types, layer
conﬁgurations), but also branch-wise information (e.g., branch num-
ber, number of layers in each branch, and layer dependencies). Then,
the proﬁler begins to calculate the compute and memory demands
of each layer and provides statistics on branch-wise demands to
help mapping the targeted decoder onto our proposed accelerator
architecture. Inputs also contain resource budgets and branch priority
to set up resource boundaries and highlight the importance of different
branches for architecture exploration in Step 3.

In Construction step,

layer fusion is performed to reduce the
layer number, where lightweight layers (e.g., activation layers) are
aggregated to their neighbouring major layers, such as Conv-like
and up-sampling layers, which dominate the computation or memory
consumption. Branches with shared parts are then separated to create
individual dataﬂows and the corresponding layers are reorganized
and assigned to the ﬂow with the highest computation demand. This
strategy helps avoid hardware redundancy as no duplicated hardware
units will be instantiated, and it creates a clear critical ﬂow (with
the most computations) from the shared branches and guarantees this

Fig. 5.
(a) F-CAD generated accelerators follow a layer-based multi-pipeline accelerator paradigm, where each layer after reorganization contributes to a
pipeline stage; (b) The proposed elastic architecture features a ﬂexible expansion capability, which can be extended along X- and Y-axis to instantiate
multiple basic architecture units according to the branch number and the layer number in each branch for parallel processing across branches. Each basic
architecture unit is equipped with external memory, on-chip memory (for weight buffers and input buffers), and computation resources (for PEs). It provides
maximum three-dimensional parallelism (3D parallelism) following input channel, output channel, and the height of the input feature map when handling
compute-intensive DNN layers; (c) Example of data partitioning to enable the proposed 3D parallelism.

TABLE III
THE MULTI-BRANCH DYNAMIC DESIGN SPACE

Br. Hardware conﬁgurable parameters
conf ig1 ← batchsize1, cpf 1
1
conf ig2 ← batchsize2, cpf 2
2
· · · · · ·
· · ·
B conf igB ← batchsizeB, cpf B

1 · · · cpf 1
1 · · · cpf 2

1 · · · cpf B

n , kpf B

1 · · · kpf B
n ,

hB
1 · · · hB
n
Q, BatchSize1 · · · BatchSizeB, P1 · · · PB
Cmax, Mmax, BWmax

Customization
Resource budgets

l , kpf 1
m, kpf 2

1 · · · kpf 1
1 · · · kpf 2

l , h1
m, h2

1 · · · h1
l
1 · · · h2
m

of compute engines are instantiated and each engine contains kpf
process elements (PEs) to handle computations. The proposed basic
architecture unit also allows customized bitwidth of the input features
(DW), the weights (WW), and the external memory bus (MW).

parameters in Table III. Assuming a decoder with B branches, we
pick the ﬁrst two (with l and m layers) and the last branch (with
n layers) as examples. Each of them can be conﬁgured in four
major aspects as the cpf, kpf, and H-partition (h) from the proposed
3D parallelism and batch size (batchsize). Parameters of the same
branch are passed to a conﬁguration ﬁle (conf ig) and all these ﬁles
together describe the overall accelerator conﬁguration. The goal of
F-CAD is to explore the best accelerator conﬁguration in the design
space by considering the customization and resource constraints. The
customization includes the data quantization (Q), the branch-wise
targeted batch size (BatchSize), and the priority (P ) to indicate
different importance of each branch. While the resource budgets
specify three major resources as compute resource Cmax, on-chip
memory Mmax, and external memory access bandwidth BWmax.

C. The basic architecture unit with 3D parallelism

B. Design space exploration

Fig. 5 (c) provides a detailed illustration of the proposed 3D
parallelism. Assuming a Conv-like layer with a 4×6×3 input feature
map (InFM) and two 4×2×2 kernels. We have the maximum input
parallel factor as cpf max = 4 and the maximum output parallel factor
as kpf max = 2, since this layer contains four input channels and
two output channels that can be processed in parallel. In this case,
we conﬁgure both input and output parallel factors as 2 (cpf = kpf
= 2), so each compute engine will instantiate two PEs and each PE
performs two multiply-accumulations (MACs) in parallel. Since the
parallelism from input/output channels may not be sufﬁcient for codec
avatar decoding, we add one additional parallelism by partitioning the
InFM along the height dimension. So, all InFM subsections can be
processed in parallel. The total parallel factors of this example are
cpf × kpf × H-partition = 8 with four PEs instantiated.

VI. MULTI-BRANCH DESIGN SPACE EXPLORATION

A. Multi-branch dynamic design space

Although the highly conﬁgurable and scalable features of the
proposed elastic architecture help address the unique network struc-
tures adopted by codec avatar decoders,
they also introduce a
complicated and high-dimensional design space. The more branches
in the decoder, or more layers in a branch there are, the higher
dimensional design space it becomes. We deﬁne it as a multi-branch
dynamic design space and summarize all conﬁgurable hardware

To effectively search for the best conﬁguration, the proposed DSE
engine adopts a two-step strategy with a cross-branch stochastic
search and an in-branch greedy search. It follows the divide and
conquer idea, to ﬁrst conﬁrm the resource distribution for every
branch and then aim for the best individual branch conﬁguration with
given resources.

1) Cross-branch optimization: In Algorithm 1, the proposed DSE
engine ﬁrst randomly generates P resource distribution schemes
(rd). Each scheme is considered as a candidate, corresponding to
a cross-branch resource distribution regarding compute resource C,
on-chip memory M , and external memory access bandwidth BW .
In each iteration, rd is then passed to Algorithm 2 for a detailed
hardware conﬁguration (Conf ig) following the proposed architec-
ture template. With the Conf ig, we can evaluate the accelerator
performance and calculate the ﬁtness score for every candidate. We
build a function S to provide a weighted score based on the branch
performance P erf ={perf1 · · · perfB} and branch priority factor
as (cid:80)B
j=1 perfj × Pj. We also introduce a penalty term P to control
the branch-wise performance variance as: α × σ2(P erf ). Then, we
calculate the ﬁtness score by subtracting P from S. A candidate with
higher ﬁtness score means it is better than others. We deﬁne rdbest
and rdbest
global to keep track of the local best of each candidate across
all iterations and the global best candidates. rdbest
global can
clarify the optimization directions in each iteration, so that each

and rdbest

i

i

(cid:46) Save the best HW conﬁg.
, rdbest

global, budget)

Fig. 7. F-CAD efﬁciency estimation errors targeting eight benchmark DNNs.

Algorithm 1: Cross-branch optimization algorithm
1 Setup resource budgets: budget ={Cmax, Mmax, BWmax}
2 Setup maximum iteration number: N
3 Import user customization: U = {BatchSize, P riority}, where

BatchSize ={BatchSize1, · · · , BatchSizeB},
P riority ={P1, · · · , PB}

4 Randomly initialize RD0 with P population: {rd0
5 for iter in range(N ) do
6

for rditer

in RDiter do
for brj in {br1, · · · , brB} do

i

1, · · · , rd0
p}

conf igj ← InBranchOptim(rditer

, U )

(cid:46) Algorithm 2

i

end
Conf ig = {conf ig1, · · · , conf igB}
P erf ← Eval(Conf ig)
f itness ← S(P erf , U ) − P(P erf )
rdbest
i
if rdbest

, rdbest

global ← Update(f itness, rdbest

i

Conf igbest

global has changed then
global = Congf ig
← Evolve(rditer

rditer+1

i

, rdbest
i

i

(cid:46) Evaluate performance
(cid:46) Get ﬁtness score
, rdbest
global)

7

8
9

10
11
12

13

14

15

16

end

17
18 end
19 return rdbest

global, Conf igbest

global (cid:46) Output the global optimal design

opk ← GetOP(layerk)
norm paramk ← GetReuse(layerk)

Algorithm 2: In-branch optimization algorithm
1 Input resource distribution: rd = {C, M , BW }
2 Input user customization from U : BatchSize
3 Initialize conf ig for a l-layer branch: {pf1, · · · , pfl}
4 for layerk in {layer1, · · · , layerl } do
5
6
7 end
8 opmin = min(op1, · · · , opl)
9 norm bw = (cid:80)l
10 for k in range(l) do
11
12 end
13 while True do
14
15
16
17
18

cpfk, kpfk, hk ← GetPF(pfk, layerk)
ck, mk, bwk ← Utilization(cpfk, kpfk, hk)

pfk = (cid:100)BW/norm bw × (opk/opmin)(cid:101)

for layerk in {layer1, · · · , layerl } do

end
batchsize =
min(C/ (cid:80)l
if batchsize < BatchSize then

k=1 ck, M/ (cid:80)l

k=1 mk, BW/ (cid:80)l

19

k=1 bwk)

k=1(opk/opmin) × norm paramk × F req

(cid:46) Parallelism targets

{pf1, · · · pfk}/2

else

batchsize = BatchSize break

20
21
22
23
24 end
25 conf ig ← batchsize, {cpf1 · · · cpfk}, {kpf1 · · · kpfk},

end

{h1 · · · hk}

26 return conf ig

(cid:46) Output HW conﬁg.

candidate can be evolved iteratively to approach the local and the
global best positions by a random distance. By performing such a
stochastic search, eventually, Algorithm 1 discovers the global best
design by considering the given constraints.

2) In-branch optimization: rd is passed to Algorithm 2 for the
best in-branch hardware conﬁguration. Since the proposed accelerator
follows an unfolded pipeline architecture, its throughput can be maxi-
mized when all pipeline stages are load-balanced with similar latency.
To achieve this goal, we capture the layer-wise compute demands
(op) and data reuse characteristics (norm param) to obtain the
most optimistic layer-wise parallelism targets (pf ) by exhausting the
allocated bandwidth resources. After that, a greedy search algorithm
is applied to approach hardware conﬁgurations (conf ig) with the
largest level of parallelism under resource constraints. It will converge
once the parallelism fails to grow.

Fig. 6. F-CAD FPS estimation errors targeting eight benchmark DNNs.

3) Performance estimation: We adopt highly accurate analytical
models to provide performance and resource utilization feedback and
help the DSE engine make the most suitable decisions. Since each
branch is individually evaluated, we take a branch with l Conv-like
layers as an example. For layer i, we assume the input feature map
size InChi × Hi × Wi and the kernel size OutChi × InChi ×
Ki × Ki. With hardware conﬁguration Conf ig, the latency Lati
when executing layer i with working frequency f can be determined
as:

Lati =

OutChi × InChi × Hi × Wi × Ki × Ki
cpfi × kpfi × hi × f

The overall throughput (FPS) of this branch is:

F P S =

BatchSize
max(Lat1, Lat2, ..., Latl)

(4)

(5)

Estimations also include the resource utilization {C, M , BW } (by
summing up the resource consumed by all layers) and efﬁciency (by
following Eq. 3). To verify the accuracy of our method, we select
DNN benchmarks including AlexNet, ZFNet, VGG16, and Tiny-
YOLO with 16-bit (benchmarks 1 ∼ 4) and 8-bit (benchmarks 5
∼ 8) quantization schemes and compare their estimated performance
to the real performance after board-level implementation on a Xilinx
KU115 FPGA. As shown in Fig. 6, we normalize the FPS to the real
results in every case to better illustrate the error rate. Real FPS results
are also listed in the green bars. The maximum error is only 2.89%
while the average error is 2.02%. Similarly, we present the efﬁciency
error in Fig. 7 with 3.96% maximum error and 1.91% average error.

VII. EXPERIMENTAL RESULTS

In this section, we target three embedded FPGA platforms (Xilinx
Z7045, ZU17EG, and ZU9CG) to demonstrate F-CAD’s capability
and scalability for accelerating codec avatar decoding. Since the
targeted platforms are FPGAs, we setup resource budgets Cmax and
Mmax as the available DSPs and BRAMs in the targeted FPGA, and
BWmax as the DDR3 memory bandwidth. The clock frequency is
set to 200MHz for all platforms. The targeted decoder is described in
Table I with customized batch size {1, 2, 2} corresponding to Br. 1 ∼
3. Such customization is considered by most VR avatar applications
where Br. 2 and 3 need to render two HD textures with specular
effects seen by both eyes, while the Br. 1 only outputs one facial
geometry that can be shared by both eyes.

Experimental results are listed in Table IV, where F-CAD generates
ﬁve accelerators following the proposed elastic architecture. To

TABLE IV
F-CAD GENERATED ACCELERATORS FOR CODEC AVATAR DECODING

Case 1: Z7045 (8-bit)
Resource budget:
900 DSPs, 1090 BRAMs
Case 2: ZU17EG (8-bit)
Resource budget:
1590 DSPs, 1592 BRAMs
Case 3: ZU17EG (16-bit)
Resource budget:
1590 DSPs, 1592 BRAMs
Case 4: ZU9CG (8-bit)
Resource budget:
2520 DSPs, 1824 BRAMs
Case 5: ZU9CG (16-bit)
Resource budget:
2520 DSPs, 1824 BRAMs

Br.
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3

DSP Usage
199
500
38
351
936
70
351
928
22
351
1808
70
351
1792
70

Total DSPs

737 (81.8%)

1357 (83.5%)

1301 (81.8%)

2229 (88.5%)

2213 (87.8%)

BRAM Usage
221
551
112
280
642
102
382
983
208
280
786
102
382
1183
188

Total BRAMs

884 (81.1%)

1024 (64.3%)

1573 (98.8%)

1168 (64.0%)

1735 (96.1%)

FPS
61.0
30.5
61.0
122.1
61.0
122.1
61.0
30.5
15.3
122.1
122.1
122.1
61.0
61.0
61.0

Efﬁciency (%)
76.6
86.6
84.2
86.8
92.6
91.4
86.8
93.4
72.7
86.8
95.8
91.4
86.8
96.7
91.4

DSE Running Time (s)

101.8

77.3

82.8

56.9

67.6

TABLE V
RESULT COMPARISON TO EXISTING ACCELERATORS (@200MHZ)

DNNBuilder [1] HybridDNN [2]

Precision
DSP
BRAM
FPS
Efﬁciency

8-bit
1820
1197
30.5
28.8%

16-bit
1024
1120
22.0
70.4%

F-CAD (our work)
8-bit
2229
1168
122.1
91.3%

16-bit
2213
1735
61.0
91.6%

evaluate the search speed, we perform 10 independent searches with
N = 20 (meaning the search contains 20 iterations) and P = 200
(meaning 200 resource distribution candidates are initialized) for
each case and all of them converge in minutes using an Intel i7
CPU working at 2.6 GHz. The average iteration for convergence is
9.2 (min: 6.8; max: 13.6). Eventually, F-CAD generates optimized
designs by considering customization and resource constraints. In
particular, the accelerator for case 4 reaches the highest 122.1 FPS,
which fully satisﬁes the VR requirements; while accelerator for case 5
delivers the highest efﬁciency peaking at 96.7%, which can efﬁciently
leverage codec avatar decoding using lightweight HMDs.

We compare F-CAD generated accelerators to existing designs in
Table V by targeting the same ZU9CG FPGA with 2520 DSPs and
1824 BRAMs. We use the same mimic decoder mentioned in Sec. III
for DNNBuilder and HybridDNN, while using the targeted decoder
(a real-life decoder) for F-CAD. The batch size is uniformly set
to one for fair comparison as DNNBuilder and HybridDNN do not
support differentiated batch scheme. The performance and efﬁciency
of DNNBuilder are limited by the insufﬁcient parallelism, so the allo-
cated resources are not fully utilized. On the other hand, HybridDNN
fails to allocate more DSPs and leaves more than half of available
DSPs unallocated. The reason is that the coarse-grained conﬁguration
requires double-sized accelerator instance to continue scaling, but
the BRAM budget is not enough and becomes a bottleneck. In our
design, F-CAD delivers the highest FPS and efﬁciency given the same
resource budgets. Compared to DNNBuilder, we achieve 4.0× higher
throughput and 62.5% higher efﬁciency for running the 8-bit codec
avatar decoder. Compared to HybridDNN, we can deliver 2.8× higher
throughput by allocating only 2.2× more DSPs and 21.2% higher
efﬁciency when running the 16-bit model. F-CAD can also target
ASIC designs with the resource budgets {Cmax, Mmax, BWmax}
associating to three most commonly used resources in ASIC DNN
accelerators: the available MAC units, the on-chip buffer size, and
the external memory bandwidth.

VIII. RELATED WORKS

VR telepresence is a recently developed technology that can re-
produce authentic human presence including real-time expressions in
VR environments to reform remote communication [7]–[9]. As efforts
on improving the telepresence experience, authors in [3] introduce a
DNN-based VAE framework to ﬁrst encode human’s facial geometry
and view-dependent appearances into latent codes and then reproduce
the corresponding avatar. To enable high quality avatars, authors in
[4] adopt a generative adversarial network (GAN) in the decoder to
create synthetic images while the work proposed in [5] focuses on
producing more robust and accurate facial animations. In [10], audio
data is also captured for driving realistic facial expressions. With the
compute- and memory-demanding designs, telepresence with codec
avatar urgently needs hardware acceleration to guarantee authentic
presence with sufﬁcient visual quality when deploying onto hardware-
restricted HMDs. We have seen recently published works begin to
target VR applications from the perspective of hardware and system
design, such as [11], [12]. However, the multi-GPU system proposed
by [11] can not be accommodated by VR headsets, while the VR
video processing methods proposed by [12] have not addressed the
challenges from compute- and memory-intensive DNN workloads
(e.g., our targeted VR avatar decoding). Although DNN hardware
accelerators have been designed for accelerating different workloads
with diverse hardware devices [13]–[19], none of them have dedicated
optimization strategies to address the unique challenges of running
VAE codec avatar models for VR avatar applications and satisfy their
increasing demands.

IX. CONCLUSIONS

In this paper, we presented F-CAD, an automation tool to design
and explore optimized hardware accelerators for VR avatar decoding
with high throughput and efﬁciency. To address the unique challenges
coming from the special DNN structure and demanding performance
requirements, we proposed an expandable elastic architecture to sup-
port multi-branch DNNs and a highly conﬁgurable basic architecture
unit to provide ﬂexible and scalable parallel processing. We then
introduced a multi-branch dynamic design space to describe hardware
conﬁgurations and an efﬁcient DSE engine to explore the opti-
mized accelerator by considering various customized constraints and
available resource budgets. F-CAD delivered the highest throughput
and efﬁciency, peaking at 122.1 FPS and 91.6%. Compared to the
state-of-the-art accelerators, F-CAD achieved 4.0× and 2.8× higher
throughput and 62.5% and 21.2% higher efﬁciency than DNNBuilder
and HybridDNN when targeting the same FPGA.

REFERENCES

[1] Xiaofan Zhang, Junsong Wang, Chao Zhu, et al. DNNBuilder: an au-
tomated tool for building high-performance DNN hardware accelerators
In Proc. of International Conference on Computer-Aided
for FPGAs.
Design (ICCAD), pages 1–8, 2018.

[2] Hanchen Ye, Xiaofan Zhang, Zhize Huang, et al. HybridDNN: A
framework for high-performance hybrid DNN accelerator design and
In Proc. of Design Automation Conference (DAC),
implementation.
pages 1–6, 2020.

[3] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh.
Deep appearance models for face rendering. ACM Transactions on
Graphics (TOG), 37(4):1–13, 2018.

[4] Shih-En Wei, Jason Saragih, Tomas Simon, et al. VR facial animation
via multiview image translation. ACM Transactions on Graphics (TOG),
38(4):1–16, 2019.

[5] Hang Chu, Shugao Ma, Fernando De la Torre, et al. Expressive
telepresence via modular codec avatars. In Prof. of European Conference
on Computer Vision (ECCV), pages 330–345, 2020.

[6] Qualcomm. Snapdragon 865 5G mobile platform. Accessed: 2020-11-2.
[7] Mike Seymour, Chris Evans, and Kim Libreri. Meet mike: epic avatars.

In ACM SIGGRAPH VR Village, pages 1–2. 2017.

[8] Christian Frueh, Avneesh Sud, and Vivek Kwatra. Headset removal for
virtual and mixed reality. In ACM SIGGRAPH, pages 1–2. 2017.
[9] Stephen Lombardi, Tomas Simon, Jason Saragih, et al. Neural volumes:
Learning dynamic renderable volumes from images. ACM Transactions
on Graphics (TOG), 38(4), 2019.

[10] Alexander Richard, Colin Lea, Shugao Ma, et al. Audio-and gaze-
driven facial animation of codec avatars. Proc. of Winter Conference
on Applications of Computer Vision (WACV), pages 41–50, 2020.
[11] Chenhao Xie, Fu Xin, Mingsong Chen, et al. OO-VR: NUMA friendly
object-oriented VR rendering framework for future NUMA-based multi-
In Proc. of International Symposium on Computer
GPU systems.
Architecture (ISCA), pages 53–65, 2019.

[12] Yue Leng, Chi-Chun Chen, Qiuyue Sun, et al. Energy-efﬁcient video
processing for virtual reality. In Proc. of International Symposium on
Computer Architecture (ISCA), pages 91–103, 2019.

[13] Jiantao Qiu, Jie Wang, Song Yao, et al. Going deeper with embedded
FPGA platform for convolutional neural network. In Proc. of Interna-
tional Symposium on Field-Programmable Gate Arrays (FPGA), pages
26–35, 2016.

[14] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. Eyeriss:
An Energy-Efﬁcient Reconﬁgurable Accelerator for Deep Convolu-
tional Neural Networks. IEEE Journal of Solid-State Circuits (JSSC),
52(1):127–138, 2016.

[15] Norman P Jouppi, Cliff Young, Nishant Patil, et al. In-datacenter per-
formance analysis of a tensor processing unit. In Proc. of International
symposium on computer architecture (ISCA), pages 1–12, 2017.
[16] Dustin Franklin. NVIDIA Jetson TX2 delivers twice the intelligence to
the edge. NVIDIA Accelerated Computing— Parallel For all, 2017.
[17] Pengfei Xu, Xiaofan Zhang, Cong Hao, et al. AutoDNNchip: An
automated DNN chip predictor and builder for both FPGAs and ASICs.
In Proc. of International Symposium on Field-Programmable Gate
Arrays (FPGA), pages 40–50, 2020.

[18] Xiaofan Zhang, Hanchen Ye, Junsong Wang, et al. DNNExplorer: a
framework for modeling and exploring a novel paradigm of FPGA-based
DNN accelerator. In Proc. of the International Conference on Computer-
Aided Design (ICCAD), pages 1–9, 2020.

[19] Qin Li, Xiaofan Zhang, Jinjun Xiong, et al. Efﬁcient methods for
IEEE Transactions on

mapping neural machine translator on FPGAs.
Parallel and Distributed Systems, 32(7):1866–1877, 2021.

