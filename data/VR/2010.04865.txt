Learning Acoustic Scattering Fields for Dynamic Interactive Sound
Propagation

Zhenyu Tang*, Hsien-Yu Meng*, and Dinesh Manocha

0
2
0
2
c
e
D
7

]

D
S
.
s
c
[

2
v
5
6
8
4
0
.
0
1
0
2
:
v
i
X
r
a

Figure 1: We show the dynamic scenes with various moving objects that are used to evaluate our hybrid sound propagation
algorithm. We compute the acoustic scattered ﬁelds of each object using a neural network and couple them with interactive ray
tracing to generate diffraction and occlusion effects. Our approach can generate plausible acoustic effects in dynamic scenes in a
few milliseconds and we demonstrate its beneﬁts for sound rendering in virtual environments.

ABSTRACT

We present a novel hybrid sound propagation algorithm for interac-
tive applications. Our approach is designed for dynamic scenes and
uses a neural network-based learned scattered ﬁeld representation
along with ray tracing to generate specular, diffuse, diffraction, and
occlusion effects efﬁciently. We use geometric deep learning to
approximate the acoustic scattering ﬁeld using spherical harmonics.
We use a large 3D dataset for training, and compare its accuracy
with the ground truth generated using an accurate wave-based solver.
The additional overhead of computing the learned scattered ﬁeld at
runtime is small and we demonstrate its interactive performance by
generating plausible sound effects in dynamic scenes with diffrac-
tion and occlusion effects. We demonstrate the perceptual beneﬁts
of our approach based on an audio-visual user study.

1 INTRODUCTION

Interactive sound propagation and rendering are increasingly used
to generate plausible sounds that can improve a user’s sense of pres-
ence and immersion in virtual environments [23]. Recent advances
in geometric and wave-based simulation methods have lead to in-
tegration of these methods into current games and virtual reality
(VR) applications to generate plausible acoustic effects, including
Project Acoustics [2], Oculus Spatializer [3], and Steam Audio [1].
The underlying propagation algorithms are based on using reverber-
ation ﬁlters [54], ray tracing [43, 44], or precomputed wave-based
acoustics [35].

A key challenge in interactive sound rendering is handling dy-
namic scenes that are frequently used in games and VR applications.
Not only can the objects undergo large motion or deformation, but
their topologies may also change. In addition to specular and diffuse
effects, it is also important to simulate complex diffracted scatter-
ing, occlusions, and inter-reﬂections that are perceptible [18, 33, 35].
Prior geometric methods are accurate in terms of simulating high-
frequency effects and can be augmented with approximate edge
diffraction methods that may work well in certain cases [44, 52],
though their behavior can be erratic [39]. On the other hand, wave-
based precomputation methods can accurately simulate these effects,
but are limited to static scenes [35, 36]. Some hybrid methods are
limited to interactive dynamic scenes with well-separated rigid ob-
jects [40]. Our goal is to design similar hybrid methods that can

overcome these restrictions and can generate diffraction and occlu-
sion effects that translate into good perceptual differentiation [39].
Many recent works use machine learning techniques for audio
processing, including recovering acoustic parameters of real-world
scenes from recordings [10, 14, 53]. Furthermore, learning methods
have been used to approximate diffraction scattering and occlusion
effects from rectangular plate objects [33] and frequency-dependent
loudness ﬁelds for 2D convex shapes [11]. These results are promis-
ing and have motivated us to develop good learning based methods
for more general 3D objects.
Main Results: We present a novel approach to approximate the
acoustic scattering ﬁeld of an object in 3D using neural networks
for interactive sound propagation in dynamic scenes. Our approach
makes no assumption about the motion or topology of the objects.
We exploit properties of the acoustic scattering ﬁeld of objects for
lower frequencies and use neural networks to learn this ﬁeld from
geometric representations of the objects. Given an object in 3D,
we use the neural network to estimate the scattered ﬁeld at runtime,
which is used to compute the propagation paths when sound waves
interact with objects in the scene. The radial part of the acoustic
scattering ﬁeld is estimated using geometric ray tracing, along with
specular and diffuse reﬂections. Some of the novel components of
our work include:

• Learning acoustic scattering ﬁelds: We use techniques
based on geometric deep learning to approximate the angular
component of acoustic wave propagation in the wave-ﬁeld.
Our neural network takes the point cloud as the input and
outputs the spherical harmonic coefﬁcients that represent the
acoustic scattering ﬁeld. We compare the accuracy of our
learning method with an exact BEM solver, and the error on
new, unseen objects (as compared to training data). Our empir-
ical results are promising and we observe average normalized
reproduction error [6, 26] of 8.8% in the pressure ﬁelds.

• Interactive wave-geometric sound propagation: We
present a hybrid propagation algorithm that uses a neural
network-based scattering ﬁeld representation along with ray
tracing to efﬁciently generate specular, diffuse, diffraction, and
occlusion effects at interactive rates.

• Plausible sound rendering for dynamic scenes: We present
the ﬁrst interactive approach for plausible sound rendering

*Equal contribution

Our code and data will be released after publication.

1

 
 
 
 
 
 
in dynamic scenes with diffraction modeling and occlusion
effects. As the objects deform or change topology, we com-
pute a new spherical harmonic representation using the neural
network. Compared with prior interactive methods, we can
handle unseen objects at real-time, without using precomputed
transfer functions for each object.

• Perceptual evaluation: We perform a user study to validate
the perceptual beneﬁts of our method. Our propagation al-
gorithm generates more smooth and realistic sound and has
increased perceptual differentiation over prior methods used
for dynamic scenes [40, 42].

We demonstrate the performance in dynamic scenes with multiple
moving objects and changing topologies. The additional runtime
overhead of estimating the scattering ﬁeld from neural networks is
less than 1ms per object on a NVIDIA GeForce RTX 2080 Ti GPU.
The overall running time of sound propagation is governed by the
underlying ray tracing system and takes few milliseconds per frame
on multi-core desktop PC. We also evaluate the accuracy of acoustic
scattering ﬁelds, as shown in Figure 7.

2 RELATED WORK
2.1 Sound Propagation
Wave-based techniques to model sound propagation solve the acous-
tic wave equation directly using numerical solvers such as the
ﬁnite-element method [50], the boundary-element method [57], the
ﬁnite-difference time domain [7], adaptive rectangular decomposi-
tion [34], etc. Their complexity increases linearly with the size of
the environment (surface area or volume) and as a third or fourth
power of frequencies. As a result, they are limited to lower frequen-
cies [28, 37, 58].

Geometric techniques model the acoustic effects based on ray
theory and typically work well for high-frequency sounds to model
specular and diffuse reﬂections [13, 20, 24, 41]. These techniques
can be enhanced to simulate low-frequency diffraction effects. This
includes the accurate time-domain Biot-Tolstoy-Medwin (BTM)
model, which can be expensive and is limited to ofﬂine computa-
tions [45]. For interactive applications, commonly used techniques
are based on the uniform theory of diffraction (UTD), which is a less
accurate frequency-domain model that can generate plausible results
in some cases [44, 48, 52]. Moreover, the complexity of edge-based
diffraction algorithms can increase exponentially with the maximum
diffraction order.

2.2 Interactive Sound Rendering in Dynamic Scenes
At a broad level, techniques for dynamic scenes can be classiﬁed
into reverberation ﬁlters, geometric and wave-based methods, and
hybrid combinations. The simplest and lowest-cost algorithms are
based on artiﬁcial reverberators [54], which simulate the decay
of sound in rooms. These ﬁlters are designed based on different
parameters and are either speciﬁed by an artist or computed using
scene characteristics [51]. They can handle dynamic scenes but
assume that the reverberant sound ﬁeld is diffuse, making them
unable to generate directional reverberation or time-varying effects.
Many interactive techniques based on geometric acoustics and ray
tracing have been proposed for dynamic scenes [42, 48, 55]. They
use spatial data structures along with multiple cores on commodity
processors and caching techniques to achieve higher performance.
Furthermore, hybrid combinations of ray tracing and reverberation
ﬁlters [43] have been proposed for low-power, mobile devices. In
practice, these methods can handle scenes with a large number of
moving objects, along with sources and the listener, but can’t model
diffraction or occlusion effects well.

Many precomputation-based wave acoustics techniques tend to
compute a global representation of the acoustic pressure ﬁeld. They
are limited to static scenes, but can handle real-time movement of

both sources and the listener [29, 37]. These representations are
computed based on uniform or adaptive sampling techniques [8].
Overall, the acoustic wave ﬁeld is a complex high-dimensional
function and many efﬁcient techniques have been designed to encode
this ﬁeld [35, 36] within 100MB and with a small runtime overhead.
A hybrid combination of BEM and ray tracing has been presented
for dynamic scenes with well-separated rigid objects [40]. A recent
Planeverb system [38] is able to perform 2D wave simulation at
interactive rates and calculate perceptual acoustic parameters that
can be used for sound rendering.

2.3 Machine Learning and Acoustic Processing
Machine learning techniques are increasingly used for acoustic
processing applications. These include isolating the source lo-
cations in multipath environments [12] and recovering the room
acoustic parameters corresponding to reverberation time, direct-to-
reverberant ratio, room volume, equalization, etc. from recorded
signals [10, 14, 47, 53]. These parameters are used for speech pro-
cessing or audio rendering in real-world scenes. Neural networks
have also been used to replace the expensive convolution operations
for fast auralization [49], to render the acoustic effects of scattering
from rectangular plate objects for VR applications [33], or to learn
the mapping from convex shapes to the frequency dependent loud-
ness ﬁeld [11]. The last method formulates the scattering function
computation as a high-dimension image-to-image regression and is
mainly limited to convex objects that are isomorphic to spheres.

3 BACKGROUND AND OVERVIEW
3.1 Global and Localized Sound Fields
Sound ﬁelds typically refer to the sound energy/pressure distribution
over a bounded space as generated by one or more sound sources.
The global sound ﬁeld in an acoustic environment depends on each
sound source location, the propagating medium, and any reﬂec-
tions from boundary surfaces and objects. This requires solving
the wave equation in the free-ﬁeld condition and evaluating inter-
boundary interactions of sound energy using a global numeric solver
(details in Appendix A). In this case, the position of all scene ob-
jects/boundaries and sound sources needs to be speciﬁed beforehand,
and any change in these conditions changes the sound ﬁeld. The
exact computation of the global pressure ﬁeld is very expensive and
can takes tens of hours on a cluster [28, 35, 37].

Our goal is to generate plausible sounds in virtual environments
with dynamic objects. Therefore, it is important to model the acous-
tic scattering ﬁeld (ASF) of each object. The ASFs of different
objects are used to represent the localized pressure ﬁeld, which is
needed for diffraction and inter-reﬂection effects [18, 28]. At the
same time, the sound ﬁeld in the free space (e.g., the far-ﬁeld) be-
tween two distant objects is approximated using ray tracing, and we
do not compute that pressure ﬁeld accurately using a wave-solver.
In practice, computing the sound ﬁeld in a localized space for each
object in the scene is much simpler and easier to represent than using
a global solver [28, 40].

3.2 Overview
We present a learning method to approximate the ASFs of static
or dynamic 3D objects of moderate sizes. In terms of correlation
between the object shape and its scattering ﬁeld, the volume of the
scatterer closely relates to its low-order shape characteristics that
can be represented by coarse triangle faces, which dominate the
low-frequency scattering behaviors; while at high frequencies, this
relationship shifts to high-order shape characteristics (i.e., geometri-
cal details). Given the powerfulness of deep learning inference, we
hypothesize the scattering sound distribution can be directly learned
from the scatterer geometry, without solving the complicated wave
equations. The inference speed on a modern GPU far exceeds con-
ventional wave solvers, making deep neural networks suitable for

2

Figure 2: Overview: Our algorithm consists of the training stage and the runtime stage. The training stage uses a large dataset of 3D objects
and their associated acoustic pressure ﬁelds computed using an accurate BEM solver to train the network. The runtime stage uses the trained
neural network to predict the sound pressure ﬁeld from a point cloud approximation of different objects at interactive rates.

interactive sound rendering applications. Therefore, we propose us-
ing appropriate 3D representation of objects to feed a neural network
that can learn its corresponding scattered acoustic pressure ﬁeld. We
build and evaluate our method mainly on low frequency sounds and
leverage state-of-the-art geometric ray-tracing techniques to handle
high frequency sounds.

For each object, we consider a spherical grid of incoming direc-
tions and model the plane-waves from each direction of this grid.
For each plane wave, our goal is to compute the scattered ﬁeld for
the object on an offset surface of the object. Our geometric deep
learning method is used to compute the angular portion of the scat-
tered ﬁeld (Equation 12). If two objects move and are in a touching
conﬁguration, our learning algorithm treats them as a one large ob-
ject and estimates its scattered ﬁeld. Similarly, we can recompute the
scattered ﬁeld for a deforming object. An overview of our approach
is illustrated in Figure 2.

4 LEARNING-BASED SOUND SCATTERING

4.1 Wave Propagation Modeling

Our approach is designed for synthetic scenes and we assume a
geometric representation (e.g., triangle mesh) is given to us. So
the acoustic scattering ﬁeld p(x, ω) around the object can be solved
numerically (derivation in Appendix A and B). In this work, we
propose modeling the angular part of the scattering ﬁeld using our
learning based pressure ﬁeld inference. The radial part is approxi-
mated using geometric sound propagation techniques.

4.1.1 Radial Decoupling

Our goal is to determine the scattering ﬁeld over the exterior space
E using a wave-solver. This ﬁeld needs to be compactly encoded
for efﬁcient training. As shown in Equation (12), acoustic wave
propagation in the free-ﬁeld can be decomposed into radial and
angular components. Furthermore, the radial sound pressure in the
far-ﬁeld follows the inverse-distance law [5]: p ∼ 1/r, as shown in
Figure 3. We utilize this property to extrapolate the full ASF from
one of its far-ﬁeld “snapshots” at a ﬁxed radius, so that the full ASF
does not need to be stored. Following the inverse-distance law, the
sound pressure at any far-ﬁeld location (r, θ , φ ) can be computed as

p(r, θ , φ , ω) =

rre f
r

p(rre f , θ , φ , ω),

(1)

where rre f is the reference distance and only p(rre f , ·, ·, ·) needs to
be computed and stored. For brevity, we omit r in following sections.

4.1.2 Angular Pressure Field Encoding

A spherical ﬁeld consisting of a ﬁxed number of points (e.g., 642
points evenly distributed on a sphere surface) is obtained by generat-
ing an icosphere with 4 subdivisions. Real valued scattered sound

Figure 3: Simulated sound pressure fall-off and inverse-distance
law ﬁtted curves: We calculate the sound pressure around a sound
scatterer in our dataset using the BEM solver as reference. We exam-
ine the sound pressure from 1m to 10m scattered along 5 directions
(0◦, 72◦, 144◦, 216◦, and 288◦). We regard the sound pressure value
at 10m to correspond to far-ﬁeld condition, and inversely ﬁt the
pressure values for distance within 10m according to Equation 1.
We userre f = 5m is used for generating our ASFs, although other
values can be used as well.

pressures are evaluated at these ﬁeld points during wave-based sim-
ulation. Spherical harmonics (SH) can represent a spherical scalar
ﬁeld compactly using a set of SH coefﬁcients; they have been widely
used for 3D sound ﬁeld recording and reproduction [32]. SH func-
tion up to order lmax has M = (lmax + 1)2 coefﬁcients. The angular
pressure at the outgoing direction (θ , φ ) can be evaluated as

p(θ , φ , ω) =

lmax
∑
l=0

+l
∑
m=−l

Y m
l (θ , φ )cm

l (ω),

(2)

where cm
l (ω) are the SH coefﬁcients that encode our angular pres-
sure ﬁelds. Increasing the number of coefﬁcients can lead to more
challenges because the dimension of our learning target is raised.

4.2 Learning Spherical Pressure Fields

We need an appropriate geometric representation for the underlying
objects in the scene so that we can apply geometric deep learning
methods to compute the ASF. It is important that our approach should
be able handle dynamic scenes with moving objects or changing
topology. It can be difﬁcult to handle such scenarios with mesh-
based representations [15, 46, 59]. For example, [15] calculates
intrinsic geodesic distances for convolution operations, which cannot
be applied when one big object breaks into two.

Our approach uses a point cloud representation of the objects in
the scene as an input. And we leverage the PointNet [9] architecture
to regress the spherical harmonics term cm
in Equation 2. PointNet
l
is a highly efﬁcient and effective network architecture that works

3

on raw point cloud input, and can perform various tasks including
3D object classiﬁcation, semantic segmentation and our ASF re-
gression. It also respects the permutation invariance of points. We
slightly modify its output layers to predict the SH vector as shown
in Figure 4.

Figure 4: PointNet regression: Given an input point cloud with
N = 1024 3D points, we feed it to the PointNet architecture [9] until
maxpooling to extract the global feature. Then we use multi-layer
perceptrons (MLPs) of layer size 256, 128, and 16 to map the feature
to a SH vector of length 16 representing the scattering ﬁeld.

5 INTERACTIVE SOUND PROPAGATION WITH WAVE-RAY

COUPLING

In this section, we describe how our learning-based method can be
combined with geometric sound propagation techniques to compute
the impulse responses for given source and listener positions. Then,
we can render them in highly dynamic scenes.

Hybrid Sound Propagation We use a hybrid sound propaga-
tion algorithm that combines wave-based and ray acoustics. Each of
them handles different parts of wave acoustics phenomena, but they
are coupled in terms of incoming and outgoing energies at multiple
localized scattering ﬁelds. Speciﬁcally, our trained neural network
estimates the scattering ﬁeld and is used to compute propagation
paths when sound interacts with obstacles in the scene. On the other
hand, modeling sound propagation in the air along with specular and
diffuse reﬂections at large boundary surfaces (e.g., walls, ﬂoors) is
computed using ray tracing methods [40, 42, 44].

Ray Tracing with Localized Fields Our localized ASFs are
represented using SH coefﬁcients. Given the most general ray tracing
formulation at a scattering surface, the sound intensity Iout of an
outgoing direction (θo, φo) from a scattering surface is given by the
integral of the incoming intensity from all directions:

We use the Monte Carlo integration to numerically evaluate the
outgoing scattered intensity:

Iout (θo, φo, ω) ≈

1
N

N
∑
j=1

Iin(θ j, φ j, ω)p2(θ j, φ j, ω)
Pr(θ j, φ j)

,

(5)

where N is the number of samples and Pr(θ j, φ j) is the probability
of generating a sample for direction (θ j, φ j). A uniform sampling
over the sphere surface gives Pr(θ j, φ j) = 1
4π . As N increases, the
approximation becomes more accurate.

Diffraction Compensation In wave acoustics, the total sound
ﬁeld at a position can be decomposed into the sum of the free-ﬁeld
sound pressure and the scattered sound ﬁeld. Similar to [40], we
only have computed the scattered sound ﬁeld up to now. But when
the listener is obstructed from the sound source, the traditional
ray-tracing algorithm will miss the contribution from the free-ﬁeld,
which will result in a very unnatural phenomenon: the sound would
be greatly attenuated by a single obstacle if we only render the
scattered sound, whereas in a realistic setup, low-frequency sound
should not be attenuated by a small obstacle by much. To address
this issue in a ray-tracing context, we propose to approximate sound
interference with and without an obstacle depending on an extra
visibility check. Speciﬁcally, for a sound source from direction
(θ j, φ j) and the listener at (θo, φo), we calculate the sound at the
listener position based on whether they are blocked by a scatterer
from each other as:

Iout (θo, φo, ω) ≈






N ∑N
1
j=1
N ∑N
1

Iin(θ j,φ j,ω)(1−p2(θ j,φ j,ω))
Pr(θ j,φ j)
Iin(θ j,φ j,ω)p2(θ j,φ j,ω)
Pr(θ j,φ j)

j=1

, if invisible

, if visible

(6)
Note that the visible case remains the same as Equation 5, because
the direct response will be automatically accounted for by the orig-
inal ray-tracing pipeline. Obviously, this implementation is not
physically accurate compared with wave acoustic simulations, since
additional phase information is missing. However, this formulation
will generate more realistic and more smooth sound rendering than
prior work that only considers the scattering ﬁeld, and we verify its
beneﬁts through a perceptual evaluation in § 7.

Iout (θo, φo, ω) =

(cid:90)

S

Iin(θi, φi, ω) f (θi, φi, θo, φo, ω)dS,

(3)

6 IMPLEMENTATION AND RESULTS

where S represents the directions on a spherical surface around the
ray hit point, Iin(θi, φi, ω) is the incoming sound intensity from di-
rection (θi, φi), and f (θi, φi, θo, φo, ω) is the bi-directional scattering
distribution function (BSDF) that is commonly used in visual ren-
dering [30]. Our problem of acoustic wave scattering is different
from visual rendering in two aspects: (1) sound wave scatters around
objects, whereas light mostly transmits to visible directions or prop-
agates through transparent materials; (2) BSDFs are point-based
functions that depend on both incoming and outgoing directions,
whereas our localized scattered ﬁelds are region-based functions.
Therefore, we replace BSDFs in Equation (3) with our localized
scattered ﬁeld p(θ , φ , ω) representation from Equation (2). Our
choice of a spherical offset surface to model the scattered ﬁeld also
enables us to perform integration over the whole spherical surface
in a straightforward manner, since evaluating spherical coordinates
is efﬁcient with SH functions. Although p(θ , φ , ω) encodes only
the outgoing directions and assumes incoming plane waves to −x
direction, one can easily rotate the point cloud to align any incoming
direction to the −x direction and use our network to infer p(θ , φ , ω)
at that direction. We update Equation (3) to

Iout (θo, φo, ω) =

(cid:90)

S

Iin(θi, φi, ω)p2(θi, φi, ω)dS.

(4)

In this section, we describe our implementation details and demon-
strate the performance on many dynamic benchmarks.

6.1 Data Generation

Dataset To generate our learning examples, we choose to use
the ABC Dataset [19]. This dataset is a collection of one million
general Computer-Aided Design (CAD) models and is widely used
for evaluation of geometric deep learning methods and applications.
In particular, this dataset has been used to estimate of differential
quantities (e.g., normals) and sharp features, which makes it attrac-
tive for learning ASFs as well. We sample 100,000 models from
the ABC Dataset and process them by scaling objects such that their
longest dimension is in the range of [1m, 2m]. The choice of such
an object size limit is not ﬁxed and can depend on the speciﬁc prob-
lem domain (e.g., size of objects used in applications like games or
VR). Because the scattered pressure ﬁeld is orientation-dependent,
we augment our models by applying random 3D rotations to the
original dataset to create an equal-sized rotation augmented dataset.
To generate accurate labeled data, we use an accurate BEM wave
solver, placing a plane wave source with unit strength propagating to
the −x direction. The solver outputs the ASF for each object, which
becomes our learning target. The dataset pipeline is also illustrated
in Figure 5.

4

Max Spherical Harmonics Order We experiment with the
number of SH coefﬁcients by projecting our scattered sound pressure
ﬁelds to SH functions with different orders, as shown in Figure 6.
Based on this analysis, we choose to use up to a 3rd order SH
projection, which yields sufﬁciently small ﬁtting errors (relative
error smaller than 2%) with 16 SH coefﬁcients. This sets the output
of our neural network (Section 4.2.3) to be a vector of length 16.

Figure 5: Our dataset generation pipeline for neural network
training: Given a set of CAD models, we apply random rotations
with respect to their center of mass to generate a larger augmented
dataset and use a BEM solver to calculate the ASFs.

Mesh Pre-processing The original meshes from the ABC
Dataset have high levels of details with ﬁne edges of length shorter
than 1cm. Dense point cloud inputs could also be modeled or col-
lected from the real-world scenes with granularity similar to this
dataset. However, a high number of triangle elements in a mesh will
signiﬁcantly increase the simulation time of BEM solvers. For wave-
based solver, our highest simulation frequency is 1000Hz, which
converts to a wavelength of 34cm. Therefore, we use the standard
procedure of mesh simpliﬁcation and mesh clustering algorithm
from the vcglib 1 to ensure that our meshes have a minimum edge
length of 1.7cm, which is 1/20 of our shortest target wavelength.
This is sufﬁcient according to the standard techniques used in BEM
simulators [27]. Most meshes after pre-processing have fewer than
20% number of elements than the original and the BEM simulation
for dataset generation gains over 10× speedup.

BEM Solver We use the FastBEM Acoustics software 2 as our
wave-based solver. Simulations are run on a Windows 10 work-
station that has 32 Intel(R) Xeon(R) Gold 5218 CPUs with multi-
threading. First we use the adaptive cross approximation (ACA)
BEM [21] to compute the ASF since it can achieve near O(N) com-
putational performance for small to medium sized models (e.g.,
element count N ≤ 100, 000). If it fails to converge within some
ﬁxed number of iterations, we use the conventional and accurate
BEM solver. Overall, it takes about 12 days to compute the ASF
up to 1000Hz frequency of about 100,000 objects from the ABC
Dataset. The sound pressure ﬁeld is evaluated at 642 ﬁeld points
that are evenly distributed on the spherical ﬁeld surface. Next, we
use pyshtools 3 software [56] to compute the spherical harmonics
coefﬁcients from the pressure ﬁeld using least squares inversion.

Reference Field Distance Since the inverse-distance law has
increasing error in the near-ﬁeld of objects, we need to ﬁnd a suit-
able distance for computing our reference ﬁeld. We experimentally
simulate the sound pressure fall-off with respect to distance and
observe that sound pressure that is 5m or further away from the scat-
terer closely agrees with this far-ﬁeld approximation (see Figure 3).
Therefore, we choose to calculate the pressure ﬁeld on an offset
surface 5m away from the scatterer’s center using a BEM solver (i.e.,
setting rre f = 5m in Equation 1). Note that this choice of 5m is not
strict or ﬁxed. If higher accuracy along the radial line is desired,
multiple locations (especially in the near ﬁeld) can be sampled dur-
ing the simulation to interpolate the curve at a higher accuracy. The
precomputation time and memory overhead will increase linearly
with respect to the number of sampled distance ﬁelds.

1http://vcg.isti.cnr.it/vcglib/
2https://www.fastbem.com/
3https://shtools.oca.eu/shtools/public/index.html

Figure 6: Spherical harmonics approximation of sound pressure
ﬁelds: We evaluate different orders of SH functions to ﬁt our pres-
sure ﬁelds at 4 frequencies and calculate the relative ﬁtting errors.

6.2 Network Training

Our network model is trained on a GeForce RTX 2080 Ti GPU using
the Tensorﬂow framework [4]. The dataset is split into training set
and test set using the ratio 9 : 1. In the training stage, we use Adam
optimizer to minimize L2 norm loss between predicted spherical
harmonic coefﬁcients and the groundtruth. In practice, the initial
learning rate is set to 1 × 10−3, which decays exponentially at a
rate of 0.9 and clips at 1 × 10−5. The batch size is set to 128 and
typically our network converges after 100 epochs in 8 hours. The
number of our trainable parameters is about 800k.

6.3 Runtime System and Benchmarks

We use the geometric sound propagation and rendering algorithm
described in [44]. Our sound rendering system traces sound rays at
octave frequency bands at 125Hz, 250Hz, 500Hz, 1000Hz, 2000Hz,
4000Hz, and 8000Hz. The direct output from ray tracing for each
frequency band is the energy histogram with respect to propaga-
tion delays. We take square root of these responses to compute
the frequency dependent pressure response envelopes. Broadband
frequency responses are interpolated from our traced frequency
bands, and the inverse Fourier transform is used to re-construct
the broadband impulse response. Our method does not preserve
phase information, so a random phase spectrum is used during the
inverse Fourier transform. In practice, this random spectrum does
not introduce noticeable sound difference [22].

We require that the wall boundaries are explicitly marked in our
scenes. As a result, when a ray hits the wall, only conventional
sound reﬂections occur for all frequencies. During audio-visual
rendering, when a ray hits a scattering object, we ﬁrst extend the
hit point along its ray direction by 0.5m and use it as the scattering
region center. We include all the points within a search radius of
1m from the region center to generate a point cloud approximation
of the scatterer. This point cloud is resampled using furthest point
sampling and fed into our neural networks. Our network predicts
the ASFs for sound frequencies corresponding to 125Hz, 250Hz,
500Hz and 1000Hz. The higher frequencies (i.e., 2000Hz, 4000Hz,
and 8000Hz) are handled by conventional geometric ray-tracing
with specular and diffuse reﬂections and it does not use ASFs. Our
neural network has small prediction overhead of less than 1ms per
view on an NVIDIA GeForce RTX 2080 Ti GPU. The interactive

5

Scene
Floor

Sibenik

Trinity

Havana

Benchmark Description
One static sound scatterer and one static sound source above an inﬁnitely large ﬂoor. The listener
moves horizontally so that the sound source visibility changes periodically. This is the simplest case
where no sound reverberation occurs so as to accentuate the effect of sound diffraction.
Two disjoint moving objects are used as scatterers in a church. The two scatterers revolve around
each other in close proximity such that there are complicated near-ﬁeld interactions of sound waves.
This scene is a reverberant benchmark.
Six objects ﬂy across a large indoor room and dynamically generate new composite scatterers
or decompose into separate scatterers (i.e., changing topologies). As a result, the total number
of separate scattering entities in the scene change and prior methods [40] are not effective. The
occluded regions also change dynamically and create challenging scenarios for sound propagation.
Two rotating walls that are generally larger than scatterers in previous benchmarks in a half-open
space. We use this benchmark to show that our approach can also handle large static objects, in
addition to a large number of dynamic objects. It is an outdoor scene with moderate reverberation.

#Triangle
4065

Frame time
10.65ms

122798

6.87ms

386007

12.95ms

54383

6.78ms

Table 1: Runtime performance on our benchmarks. The computation of ASFs takes ≤ 1ms per view and most frame time is spent in ray tracing.

runtime propagation system is illustrated in Figure 2. Our ray-tracer
performs 200 orders of reﬂections to generate late reverberation.

We evaluate the performance of our hybrid sound propagation and
rendering algorithms several benchmark scenes shown in Figure 1
and Table 6.3. They have with varying levels of dynamism in terms
of moving objects and are demonstrated in our supplemental video.

6.4 Analysis

Accuracy Evaluation Our goal is to approximate the acoustic
scattering ﬁelds of general 3D objects. While there is a prelimi-
nary 2D scattering dataset [11], there are no general or well-known
datasets or benchmarks for evaluating such ASFs or related com-
putations. Therefore, we use 10k objects from our test dataset to
evaluate the performance of our trained network in terms of accu-
racy. Compared with the original ABC Dataset, our test dataset has
been augmented in terms of scale and using different orientations
to evaluate the performance of our learning method. Since the pre-
diction p(θ , φ , ω) ∈ [0, 1] from our network is used as the BSDF
in Equation (3), by ﬁxing ω and varying θ and φ , we visualize the
ﬁeld using latitude-longitude plots in Figure 7. We use the common
normalized reproduction error (NRE) [6, 26] to measure the error
level of our predicted ﬁelds, which is deﬁned as:

E(ω) =

(cid:82) 2π
0

(cid:82) π
0 |ptarget (θ , φ , ω) − ppredict (θ , φ , ω)|2dφ dθ
(cid:82) π
0 |ptarget (θ , φ , ω)|2dφ dθ

(cid:82) 2π
0

.

(7)

We analyze three types of results. 1) Static Objects: Figure
7a shows a subset of CAD objects sampled from our test set,
which is from the same distribution as the training set. The av-
erage NREs over the entire test set are 4.2%, 7.6%, 8.5%, 10% for
125Hz, 250Hz, 500Hz, and 1000Hz respectively, with an overall
NRE of 8.8%. In addition, we show the NRE distribution in Figure 8,
where we see most test errors are contained below the average NRE.
We observe a close visual match in most objects across frequencies.
2) Dynamic Objects: Figure 7b shows an artiﬁcial example where
two disjoint objects moves in proximity. Such scenarios are not cre-
ated for the training set. We show the compraison and NREs at the
lowest and highest frequencies. 3) Deforming objects: Figure 7c
shows an artiﬁcial example where one sphere undergoes deformation
in different parts.

These examples show that our network is able to perform consis-
tently well on a large unseen test set when they are similar to the
CAD models in training. Preliminary results on dynamic objects and
deforming objects indicate that our network has the potential to gen-
eralize to more complicated scenarios that are not explicitly modeled
during training, although we cannot provide the error bound on these
cases. Note that the ASFs are not directly the perceived sound ﬁeld
at speciﬁc listener positions - instead they are intermediate transfer

functions as one part in the sound rendering pipeline. Therefore, we
further demonstrate the perceptual beneﬁts of our predicted ASFs
in Section 7 and show that we can reliably generate plausible sound
rendering under this error level.

Frequency Growth In theory, our learning-based framework
and runtime system can also incorporate wave frequencies beyond
1000Hz. However, two important factors need to be considered
when extending our setup: 1) the wave simulation time increases
with the simulation frequency (e.g., between a square and cubic func-
tion for an accurate BEM solver); and 2) the ASF becomes more
complicated at higher frequencies, which makes it more difﬁcult to
be learned or approximated using the same neural network. The per-
object simulation time in our experiment is 0.87s, 1.10s, 2.04s, 2.80s
for 125Hz, 250Hz, 500Hz, and 1000Hz, respectively. Note that the
simulation time is governed much by the choice of the wave solver,
as well as the relevant parameters/strategies used. We pre-processed
our meshes according to the highest simulation frequency (i.e., the
one with the shortest wavelength) and used that mesh representation
for all frequencies. When a higher frequency needs to be added, the
meshes need to have ﬁner details, meaning more boundary elements
will be involved (e.g., at least four times more elements when the
simulation frequency doubles). A frequency-adaptive mesh simpli-
ﬁcation strategy [25] can be used to reduce the simulation time at
low frequencies. Our network prediction error also grows with the
target frequency, but not at a prohibitive rate. We can reduce this
error by using more training examples and more sophisticated neural
network designs.

7 PERCEPTUAL EVALUATION

We perceptually evaluate our method using audio-visual listening
tests. Our goal is to verify that our method generates plausible sound
renderings and identify conditions it may or may not work well. We
evaluate three sound rendering pipelines in our study: 1) Using pre-
dicted ASFs and our diffraction handling (ours); 2) Using predicted
ASFs and the scattering sound rendering pipeline in diffraction ker-
nels (DK) [40]; and 3) Using geometric sound propagation only
(GSound) [42]. The reason for choosing the two alternatives is that
GSound is the state-of-the-art for interactive sound propagation with-
out diffraction modeling. DK is regarded as state of the art hybrid
algorithm for interactive sound propagation in dynamic scenes with
rigid objects and uses accurate ASFs precomputed using a BEM
solver. Since wave-based methods are limited to static scenes, we
do not include them in our perceptual evaluation.

6

(a) ASF of static objects from the unseen test set.

(b) ASF of dynamically moving objects (lowest and highest frequencies). We recompute the ASF at each time instance using our network.

(c) ASFs of a deforming object (lowest and highest frequencies), computed using our network.

Figure 7: Comparing ASF prediction accuracy in latitude-longitude plots: We highlight the ASFs for different simulation frequencies.
For each image block, the left column shows the mesh rendering of the objects. The Lat-Long plots visualize the ASF used in Equation (3) by
frequency using perceptually uniform colormaps: the top row (Target) is the groundtruth ASF computed using a BEM solver on the original
mesh; the bottom row (Predicted) represents the ASF computed using our neural network based on point-cloud representation. The error
metric NRE from Equation (7) is annotated above predicted ASFs.

Speciﬁcally, we convolved three impulse responses of reverberation
times 0.2s, 0.6s and 1.0s with a 5-second long clean human speech
recording to generate three corresponding reverberant speech. The
commonly used just-noticeable-difference (JND) of reverberation
times is a 5% relative change [17], so in normal conditions we expect
a listener to correctly rank our three audios by their reverberation
levels. Each participant is asked to listen to the three audios with no
time limit, and sort them from the most reverberant/echoy to the least
reverberant/echoy. The initial presentation order of the three audios
is randomized for each participant. Out of the 71 participants who
attempted our test, 51 participants qualiﬁed. After pre-screening,
our participants consist of 35 males and 16 females, with an average
age of 35.9 and a standard deviation of 9.5 years.

7.2 Training

During the training, we provide educational materials about sound
diffraction including texts in non-academic language and a short
YouTube video showing this phenomenon in the real world (where
the sound travels around a pillar while the sound source is invisible).
These materials require about one minute to read and watch but they
are allowed to stay longer as needed.

In addition, our participants become familiar with the video play-

Figure 8: Distribution of test set prediction errors: We also mark
the 50%, 75% and 95% percentiles in the error histogram.

7.1 Participants
We performed our studies using Amazon Mechanical Turk4 (AMT),
a popular online crowdsourcing platform that can help data collec-
tion. We recruited 71 participants on AMT to take our study. To
ensure the quality of our evaluation, we pre-screened our participants
for this study. The pre-screening question is designed to test whether
the participant has the proper listening device (e.g., a headphone)
and is in a comfortable listening environment (i.e., not too noisy),
so that they can tell basic qualitative differences between audios.

4https://www.mturk.com/

7

ing interface and are asked to adjust their audio playing volume to a
comfortable level before the main listening tasks.

7.3 Stimuli and Procedure

We use the four scenes from benchmarks in §6.3 in combination
with the three sound rendering pipelines to populate 12 audio-visual
renderings that we ask our participants to give ratings on, with no
time limit. We present the videos in four pages one after another,
each page containing only three videos from the same scene (e.g.,
Floor (ours), Floor (DK), and Floor (GSound)). The presentation
order of the pages, as well as the order of videos within each page,
have been randomized for each participant. Immediately after each
video, participants are asked to give a sound reality rating and a
sound smoothness rating. Both ratings are given from 0 to 5 stars,
with a half-star granularity (i.e., there are a total 11 discrete levels).
Participants are instructed to “give 5 stars for the most realistic and
most smooth video and 0 star for the least realistic and smooth”.
Although we believe the standard of sound being realistic or smooth
can vary among individuals, we expect that participants will be able
to recognize cases where unnatural abrupt sound changes occur in
response to scene dynamics, and will penalize them in their ratings.

7.4 Results

The average study completion time is 13 minutes. We show the box
plots of user ratings in Figure 9. We are interested in user’s rating
differences under the 3 test conditions (i.e., GSound, DK, and ours)
on a per scene basis. Therefore, we perform within-group statistical
analysis to identify potential signiﬁcant differences. A signiﬁcance
level of 0.05 is adopted for all results in our discussions.

(a) Sound reality ratings by scene.

(b) Sound smoothness ratings by scene.

Figure 9: Perceptual evaluation results: User ratings are visual-
ized as box plots. A higher rating means better quality. Results are
grouped by benchmark scene and each box represents the rating of a
speciﬁc rendering pipeline in that scene.

Sound Reality Ratings First we conduct a non-parametric
Friedman test to the ratings given to the 3 rendering conditions,
and ﬁnd signiﬁcant group differences in Floor (χ 2 = 10.82, p <
0.01) and Havana (χ 2 = 8.27, p = 0.02), but not in Trinity (χ 2 =
0.16, p = 0.92) or Sibenik (χ 2 = 3.70, p = 0.16). Note that Floor
and Havana are basically open space scenes with less reverberation,
whereas Trinity and Sibenik are common indoor environments that

8

have a lot of reverberation. Considering that the sound power of
reverberation is usually more dominant than diffraction, this result
indicates that it is harder to tell the perceptual difference between
these rendering pipelines when there is a strong reverberation. To
identify the source of differences in Floor and Havana scenes, we
perform post-hoc non-parametric Wilcoxon signed-rank tests with
Bonferroni correction [16]. We observe that ours receives higher
ratings than DK and GSound in both Floor (Z = {215.0, 144.0}, p <
0.01) and Havana (Z = {254.0, 186.5}, p < 0.01). However, there
are no signiﬁcant differences between GSound and DK in any scene.

Sound Smoothness Ratings Following the same procedure,
we perform a Friedman test to the smoothness ratings, and dis-
cover that there are signiﬁcant group differences in Floor (χ 2 =
10.29, p < 0.01), Havana (χ 2 = 7.63, p = 0.02), and Sibenik (χ 2 =
12.59, p < 0.01). Post-hoc Wilcoxon tests show consistent results
with reality ratings - we are only able to see a higher smoothness rat-
ing of ours compared with both DK and GSound in Floor (Z =
{203.5, 186.0}, p = 0.01) and Havana (Z = {233.5, 127.5}, p <
0.01). In Sibenik, both ours and DK receive a higher rating than
GSound (Z = {146.5, 171.0}, p = 0.01).

In conclusion, our pipeline receives better perceptual ratings than
the other two methods in moderately reverberant conditions, which
may not hold in highly reverberant scenes. We have increased
perceptual differentiation over the DK method. This is due to our
better computation of the ASF for dynamic objects which DK cannot
handle well and our diffraction handling that aligns better with wave
acoustic observations.

8 CONCLUSIONS, LIMITATIONS, AND FUTURE WORK
We present a new learning-based approach to approximate the acous-
tic scattering ﬁelds of objects for interactive sound propagation. We
exploit properties of the acoustic scattering ﬁeld and use a geometric
learning algorithm based on point-based approximation and the lo-
cal shapes are encoded using implicit surfaces. We use a four-layer
neural network that computes a ﬁeld representation using 3rd order
spherical harmonics. We use a large training dataset of 100,000 ob-
jects, along with random 3D orientations and scaling of each object,
and generate the accurate labeled data with a BEM solver. We evalu-
ate the accuracy of our learning method on a large number of objects
not seen in the training dataset, also undergoing topology changes.
We observe low relative error in our benchmarks. Furthermore, we
combine with a ray-tracing based sound propagation algorithm for
sound rendering in highly dynamic scenes. A perceptual study con-
ﬁrms that our approach generates smooth and realistic sound effects
in dynamic environments with increased perceptual differentiation
over prior interactive methods.

Our approach has several limitations. These include all the chal-
lenges of geometric deep learning in terms of choosing an appropri-
ate training dataset and long training time. Even though we observe
an overall relative error of 8.8% on thousands of new objects, it
is very hard to provide any rigorous guarantees in terms of error
bounds on arbitrary objects. Furthermore, we assume that objects
in the scene are sound-hard and do not take into account various
material properties. Our network has been tested for frequencies
up to 1000Hz, and we may need to design better learning methods
for higher frequencies. The overall accuracy of our hybrid propaga-
tion algorithm lies between a pure geometric method and a global
numeric solver. There is a linear scaling of training time with the
number of frequencies and the number of scattering objects, while
the simulation time could scale as a cubic function of the frequency.
As a result, the precomputation overhead can be high. One mitiga-
tion is to limit the training to the kind of objects that are frequently
used in an interactive application (e.g., a game or VR scenario). This
equals to customized training for a speciﬁc application.

There are many avenues for future work. In addition to over-
coming these limitations, we need to evaluate its performance in

other scenarios and integrate with different applications. It would
be useful to take into account the material properties by consider-
ing them as an additional object characteristic during training. We
would also like to use other techniques from geometric processing
and geometric deep learning to improve the performance of our
approach. Our runtime ray tracing algorithm could use a different
sampling scheme that exploits the properties of ASF. In-person user
study using a VR headset or standardized lab listening tests may
add more insights to how spatial sound perception is affected by
different sound propagation schemes.

REFERENCES

[1] Steam audio.

steam-audio, 2018.

https://valvesoftware.github.io/

[2] Microsoft project acoustics. https://aka.ms/acoustics, 2019.
[3] Oculus spatializer. https://developer.oculus.com/downloads/

package/oculus-spatializer-unity, 2019.

[4] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, et al. Tensorﬂow: A system for large-
scale machine learning. In 12th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 16), pp. 265–283, 2016.
[5] L. L. Beranek and T. Mellow. Acoustics: sound ﬁelds and transducers.

Academic Press, 2012.

[6] T. Betlehem and T. D. Abhayapala. Theory and design of sound ﬁeld
reproduction in reverberant rooms. The Journal of the Acoustical
Society of America, 117(4):2100–2111, 2005.

[7] D. Botteldooren. Finite-difference time-domain simulation of low-
frequency room acoustic problems. The Journal of the Acoustical
Society of America, 98(6):3302–3308, 1995.

[8] C. R. A. Chaitanya, J. M. Snyder, K. Godin, D. Nowrouzezahrai,
and N. Raghuvanshi. Adaptive sampling for sound propagation. IEEE
transactions on visualization and computer graphics, 25(5):1846–1854,
2019.

[9] R. Q. Charles, H. Su, M. Kaichun, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation. 2017
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Jul 2017. doi: 10.1109/cvpr.2017.16

[10] J. Eaton, N. D. Gaubitch, A. H. Moore, P. A. Naylor, J. Eaton, N. D.
Gaubitch, A. H. Moore, P. A. Naylor, N. D. Gaubitch, J. Eaton, et al. Es-
timation of room acoustic parameters: The ace challenge. IEEE/ACM
Transactions on Audio, Speech and Language Processing (TASLP),
24(10):1681–1693, 2016.

[11] Z. Fan, V. Vineet, H. Gamper, and N. Raghuvanshi. Fast acoustic
In ICASSP 2020-
scattering using convolutional neural networks.
2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 171–175. IEEE, 2020.

[12] E. L. Ferguson, S. B. Williams, and C. T. Jin. Sound source localization
in a multipath environment using convolutional neural networks. In
2018 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 2386–2390. IEEE, 2018.

[13] T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and J. West.
A beam tracing approach to acoustic modeling for interactive virtual en-
vironments. In Proceedings of the 25th annual conference on Computer
graphics and interactive techniques, pp. 21–32. ACM, 1998.

[14] A. F. Genovese, H. Gamper, V. Pulkki, N. Raghuvanshi, and I. J.
Tashev. Blind room volume estimation from single-channel noisy
In ICASSP 2019-2019 IEEE International Conference on
speech.
Acoustics, Speech and Signal Processing (ICASSP), pp. 231–235. IEEE,
2019.

[15] R. Hanocka, A. Hertz, N. Fish, R. Giryes, S. Fleishman, and D. Cohen-
Or. Meshcnn: a network with an edge. ACM Transactions on Graphics
(TOG), 38(4):1–12, 2019.

[16] S. Holm. A simple sequentially rejective multiple test procedure.

Scandinavian journal of statistics, pp. 65–70, 1979.

[17] A. ISO. Measurement of room acoustic parameters - part 1. ISO Std,

2009.

[18] D. L. James, J. Barbiˇc, and D. K. Pai. Precomputed acoustic transfer:
output-sensitive, accurate sound generation for geometrically complex

vibration sources. In ACM Transactions on Graphics (TOG), vol. 25,
pp. 987–995. ACM, 2006.

[19] S. Koch, A. Matveev, Z. Jiang, F. Williams, A. Artemov, E. Burnaev,
M. Alexa, D. Zorin, and D. Panozzo. Abc: A big cad model dataset for
geometric deep learning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.

[20] A. Krokstad, S. Strom, and S. Sørsdal. Calculating the acoustical room
response by the use of a ray tracing technique. Journal of Sound and
Vibration, 8(1):118–125, 1968.

[21] S. Kurz, O. Rain, and S. Rjasanow. The adaptive cross-approximation
technique for the 3d boundary-element method. IEEE transactions on
Magnetics, 38(2):421–424, 2002.

[22] K. H. Kuttruff. Auralization of impulse responses modeled on the
basis of ray-tracing results. Journal of the Audio Engineering Society,
41(11):876–880, 1993.

[23] P. Larsson, D. Vastfjall, and M. Kleiner. Better presence and perfor-
mance in virtual environments by improved binaural sound rendering.
In Virtual, Synthetic, and Entertainment Audio conference, Jun 2002.
[24] C. Lauterbach, A. Chandak, and D. Manocha. Interactive sound ren-
dering in complex and dynamic scenes using frustum tracing. IEEE
Transactions on Visualization and Computer Graphics, 13(6):1672–
1679, 2007.

[25] D. Li, Y. Fei, and C. Zheng. Interactive acoustic transfer approximation
for modal sound. ACM Transactions on Graphics (TOG), 35(1):1–16,
2015.

[26] G. N. Lilis, D. Angelosante, and G. B. Giannakis. Sound ﬁeld repro-
duction using the lasso. IEEE Transactions on Audio, Speech, and
Language Processing, 18(8):1902–1912, 2010.

[27] S. Marburg. Six boundary elements per wavelength: Is that enough?

Journal of computational acoustics, 10(01):25–51, 2002.

[28] R. Mehra, N. Raghuvanshi, L. Antani, A. Chandak, S. Curtis, and
D. Manocha. Wave-based sound propagation in large open scenes
using an equivalent source formulation. ACM Transactions on Graphics
(TOG), 32(2):19, 2013.

[29] R. Mehra, A. Rungta, A. Golas, M. Lin, and D. Manocha. Wave:
Interactive wave-based sound propagation for virtual environments.
IEEE transactions on visualization and computer graphics, 21(4):434–
442, 2015.

[30] M. Pharr, W. Jakob, and G. Humphreys. Physically based rendering:

From theory to implementation. Morgan Kaufmann, 2016.

[31] A. D. Pierce and R. T. Beyer. Acoustics: An introduction to its physical

principles and applications. 1989 edition, 1990.

[32] M. A. Poletti. Three-dimensional surround sound systems based
on spherical harmonics. Journal of the Audio Engineering Society,
53(11):1004–1025, 2005.

[33] V. Pulkki and U. P. Svensson. Machine-learning-based estimation and
rendering of scattering in virtual reality. The Journal of the Acoustical
Society of America, 145(4):2664–2676, 2019.

[34] N. Raghuvanshi, R. Narain, and M. C. Lin. Efﬁcient and accurate
sound propagation using adaptive rectangular decomposition. IEEE
Transactions on Visualization and Computer Graphics, 15(5):789–801,
2009.

[35] N. Raghuvanshi and J. Snyder. Parametric wave ﬁeld coding for pre-
computed sound propagation. ACM Transactions on Graphics (TOG),
33(4):38, 2014.

[36] N. Raghuvanshi and J. Snyder. Parametric directional coding for
precomputed sound propagation. ACM Transactions on Graphics
(TOG), 37(4):108, 2018.

[37] N. Raghuvanshi, J. Snyder, R. Mehra, M. Lin, and N. Govindaraju. Pre-
computed wave simulation for real-time sound propagation of dynamic
sources in complex scenes. ACM Trans. Graph., 29(4):68:1–68:11,
July 2010.

[38] M. Rosen, K. W. Godin, and N. Raghuvanshi. Interactive Sound Prop-
agation For Dynamic Scenes Using 2d Wave Simulation. Computer
Graphics Forum, 2020. doi: 10.1111/cgf.14099

[39] A. Rungta, S. Rust, N. Morales, R. Klatzky, M. Lin, and D. Manocha.
Psychoacoustic characterization of propagation effects in virtual envi-
ronments. ACM Transactions on Applied Perception (TAP), 13(4):21,
2016.

[40] A. Rungta, C. Schissler, N. Rewkowski, R. Mehra, and D. Manocha.

9

Diffraction kernels for interactive sound propagation in dynamic envi-
ronments. IEEE Transactions on Visualization and Computer Graphics,
24(4):1613–1622, 2018.

[41] L. Savioja and U. P. Svensson. Overview of geometrical room acoustic
modeling techniques. The Journal of the Acoustical Society of America,
138(2):708–730, 2015.

[42] C. Schissler and D. Manocha. Interactive sound propagation and ren-
dering for large multi-source scenes. ACM Transactions on Graphics
(TOG), 36(1):2, 2017.

[43] C. Schissler and D. Manocha. Interactive sound rendering on mobile
devices using ray-parameterized reverberation ﬁlters. arXiv preprint
arXiv:1803.00430, 2018.

[44] C. Schissler, R. Mehra, and D. Manocha. High-order diffraction and dif-
fuse reﬂections for interactive sound propagation in large environments.
ACM Transactions on Graphics (TOG), 33(4):39, 2014.

[45] U. P. Svensson, R. I. Fred, and J. Vanderkooy. An analytic secondary
source model of edge diffraction impulse responses. The Journal of
the Acoustical Society of America, 106(5):2331–2344, 1999.

[46] Q. Tan, L. Gao, Y.-K. Lai, J. Yang, and S. Xia. Mesh-based autoen-
coders for localized deformation component analysis. In Thirty-Second
AAAI Conference on Artiﬁcial Intelligence, 2018.

[47] Z. Tang, N. J. Bryan, D. Li, T. R. Langlois, and D. Manocha. Scene-
aware audio rendering via deep acoustic analysis. IEEE Transactions
on Visualization and Computer Graphics, 2020.

[48] M. Taylor, A. Chandak, Q. Mo, C. Lauterbach, C. Schissler, and
D. Manocha. Guided multiview ray tracing for fast auralization. IEEE
Transactions on Visualization and Computer Graphics, 18:1797–1810,
2012.

[49] R. A. Tenenbaum, F. O. Taminaro, and V. Melo. Room acoustics mod-
eling using a hybrid method with fast auralization with artiﬁcial neural
network techniques. In Proc. International Congress on Acoustics
(ICA), pp. 6420–6427, 2019.

[50] L. L. Thompson. A review of ﬁnite-element methods for time-
harmonic acoustics. The Journal of the Acoustical Society of America,
119(3):1315–1330, 2006.

[51] N. Tsingos. Precomputing geometry-based reverberation effects for
games. In Audio Engineering Society Conference: 35th International
Conference: Audio for Games. Audio Engineering Society, 2009.
[52] N. Tsingos, T. Funkhouser, A. Ngan, and I. Carlbom. Modeling acous-
tics in virtual environments using the uniform theory of diffraction. In
Proceedings of the 28th annual conference on Computer graphics and
interactive techniques, pp. 545–552. ACM, 2001.

[53] D. Tsokaktsidis, T. Von Wysocki, F. Gauterin, and S. Marburg. Artiﬁ-
cial neural network predicts noise transfer as a function of excitation
and geometry. In Proc. International Congress on Acoustics (ICA), pp.
4392–4396, 2019.

[54] V. Valimaki, J. D. Parker, L. Savioja, J. O. Smith, and J. S. Abel. Fifty
years of artiﬁcial reverberation. IEEE Transactions on Audio, Speech,
and Language Processing, 20(5):1421–1448, 2012.

[55] M. Vorl¨ander. Simulation of the transient and steady-state sound prop-
agation in rooms using a new combined ray-tracing/image-source algo-
rithm. The Journal of the Acoustical Society of America, 86(1):172–178,
1989.

[56] M. A. Wieczorek and M. Meschede. Shtools: Tools for working
with spherical harmonics. Geochemistry, Geophysics, Geosystems,
19(8):2574–2592, 2018.

[57] L. C. Wrobel and A. Kassab. Boundary element method, volume
1: Applications in thermo-ﬂuids and acoustics. Appl. Mech. Rev.,
56(2):B17–B17, 2003.

[58] H. Yeh, R. Mehra, Z. Ren, L. Antani, D. Manocha, and M. Lin. Wave-
ray coupling for interactive sound propagation in large complex scenes.
ACM Transactions on Graphics (TOG), 32(6):165, 2013.

[59] X. Zheng, C. Wen, N. Lei, M. Ma, and X. Gu. Surface registration via
foliation. In The IEEE International Conference on Computer Vision
(ICCV), Oct 2017.

10

C COMPARISON WITH PRIOR HYBRID SCHEMES
Our hybrid pipeline is similar to the diffraction-kernel method
(DK) [40]. We compute the acoustic scattering ﬁelds (ASFs) for
each object in the scene, which tend to capture all the interactions
betweeen the sound waves with the object. These include reﬂections,
diffraction, scattering and interference. In particular, the ASFs map
the incoming sound ﬁeld reaching the object to outgoing, diffracted
ﬁeld that emanates from the object. At runtime, these ASFs are
integrated with ray tracing to compute the specular and diffuse re-
ﬂections at interactive rates.

The DK algorithm essentially precomputes the ASF for each
object using a BEM solver and relies on the symmetry of an object
to save precomputation time. On the other hand, our learning-based
algorithm approximates the ASFs using a neural network at runtime
and performs GPU computations. Our approach is designed to
overcome two main limitations of DK:

1. DK is limited to rigid objects and precomputes the exact acous-
tic scattering ﬁeld (ASF) using an accurate wave-solver like
BEM. However, if the object undergoes any small deforma-
tion at runtime, the symmetry of the object breaks and DK
is no longer effective (full simulation required). Instead, our
learning method can compute a new approximation of the ASF
using our neural network for deforming objects.

2. DK assumes that the known rigid objects are well-separated
at runtime. If two objects are in close proximity or “glue”
together (as seen in Sibenik and Trinity benchmarks in Ta-
ble 1), DK will not work. Instead, our learning method can
approximate the ASF using our neural network.

At the same time, our learning-based algorithm only approximates
the ASF using a neural-network. The accuracy of our learning
method can change depending on the training dataset, the loss func-
tions and hyperparameters used by the network. We have tested the
performance on thousands of unseen objects in the ABC Dataset
and observed 8.8% overall error in the resulting pressure ﬁeld (see
Fig.7). However, it is hard to provide any rigorous guarantees on
the maximum error in the ASF for an arbitrary object. This is a
fundamental limitation of machine learning methods.

In terms of precomputation costs and runtime costs, our learning-

based method is slightly more expensive than DK.
Precomputation Cost: Our precomputation cost is governed by
data generation process, as explained in Section 6.1. We compute
the exact ASF of these objects using BEM solver (which is similar
to computing the diffraction kernel). However, we perform the
additional step of network training, as explained in Section 6.2,
which can take 8 hours for each frequency. After this training step,
we only store the network, and do not need to store the ASFs. On the
other hand, DK may only compute the ASFs of all the rigid objects
that are used in an application, when there is no demand for adding
new objects.
Runtime Cost: Most of the runtime cost is dominated by ray tracing,
which is almost the same for [DK] and our method. While DK uses
precomputed ASFs of rigid objects, we approximate the new ASF of
an object using the network, which takes less than 1ms on NVIDIA
GPU (as explained Section 6.3). This runtime GPU computation is
an ignorable overhead of our learning method, although it poses an
extra hard-ware requirement.

Appendices

A WAVE ACOUSTICS AND THE HELMHOLTZ EQUATION

A scalar acoustic pressure ﬁeld, P(x,t), satisﬁes the homogeneous
wave equation

∇2P −

1
c2

∂ 2P
∂t2 = 0,

(8)

where c is the speed of sound. We can analyze the pressure ﬁeld in
the frequency domain using Fourier transform

p(x, ω) = Ft {P(x,t)} =

(cid:90) ∞

−∞

P(x,t)e− jωt dt.

(9)

At each frequency ω the pressure ﬁeld satisﬁes the homogeneous
Helmholtz wave equation

(∇2 + k2)p(x, ω) = 0,

(10)

where k = ω
operator in terms of spherical coordinates (r, θ , φ ) as

c is the wavenumber. We can expand the Laplacian

(cid:18) ∂ 2
∂ r2 +

2
r

∂
∂ r

+

1
r2 sin θ

∂
∂ θ

(cid:18)

sin θ

(cid:19)

+

∂
∂ θ

1
r2 sin2 θ

∂ 2
∂ φ 2 + k2

(cid:19)

p = 0.

(11)

The general free-ﬁeld solution of (11) can be formulated as

p(x, ω) =

∞
∑
l=0

+l
∑
m=−l

(cid:104)
Almh(1)
l

(kr) + Blmh(2)

l

(cid:105)
(kr)

Y m
l (θ , φ ),

(12)

l

l

and h(2)

are Hankel functions of the ﬁrst and the second

where h(1)
kind, respectively. Alm and Blm are arbitrary constants, Almh(1)
Blmh(2)
l
the spherical harmonics term Y m
of the solution.

(kr) together represents the radial part of the solution and
l (θ , φ ) represents the angular part

(kr)+

l

B ACOUSTIC WAVE SCATTERING

Equation (10) describes the behavior of acoustic waves in free-ﬁeld
conditions. When a propagating acoustic wave generated by a sound
source interacts with an obstacle (the scatterer), a scattered ﬁeld is
generated outside the scatterer. The Helmholtz equation can be used
to describe this scenario:

(∇2 + k2)p(x, ω) = −Q(x, ω), ∀ x ∈ E,

(13)

where E is the space that is exterior to the scatterer and Q(x, ω)
represents the acoustic sources in the frequency domain. Common
types of sound sources include monopole sources, dipole sources,
and plane wave sources. To obtain an exact solution to (13), the
boundary conditions on the scatterer surface S need to be speciﬁed.
In this work, we assume all the scattering objects are sound-hard
(i.e. all energy is scattered, not absorbed) and therefore use the zero
Neumann boundary condition for all S:

∂ p
∂ n(x)

= 0, ∀ x ∈ S,

(14)

where n(x) is the normal vector at x. Alternatively, other conditions
including the sound-soft Dirichlet boundary condition and the mixed
Robin boundary condition [31] can be used to model different acous-
tic scattering problems. When the boundary conditions are fully
deﬁned, the constants in Equation 12 can be uniquely determined.

11

