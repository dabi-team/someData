1
2
0
2

r
a

M
3
1

]

V
C
.
s
c
[

1
v
0
0
7
7
0
.
3
0
1
2
:
v
i
X
r
a

NeuralHumanFVV: Real-Time Neural Volumetric Human Performance
Rendering using RGB Cameras

Xin Suo1 Yuheng Jiang1 Pei Lin 1 Yingliang Zhang2 Kaiwen Guo 3 Minye Wu1 Lan Xu1,4

1ShanghaiTech University

2Degene

3Google

4Shanghai Engineering Research Center of Intelligent Vision and Imaging

Abstract

4D reconstruction and rendering of human activities is
critical for immersive VR/AR experience. Recent advances
still fail to recover ﬁne geometry and texture results with
the level of detail present in the input images from sparse
multi-view RGB cameras. In this paper, we propose Neural-
HumanFVV, a real-time neural human performance capture
and rendering system to generate both high-quality geom-
etry and photo-realistic texture of human activities in ar-
bitrary novel views. We propose a neural geometry gen-
eration scheme with a hierarchical sampling strategy for
real-time implicit geometry inference, as well as a novel
neural blending scheme to generate high resolution (e.g.,
1k) and photo-realistic texture results in the novel views.
Furthermore, we adopt neural normal blending to enhance
geometry details and formulate our neural geometry and
texture rendering into a multi-task learning framework. Ex-
tensive experiments demonstrate the effectiveness of our ap-
proach to achieve high-quality geometry and photo-realistic
free view-point reconstruction for challenging human per-
formances.

1. Introduction

The rise of virtual and augmented reality (VR and AR) to
present information in an immersive way has increased the
demand of the 4D (3D spatial plus 1D time) content gener-
ation. Further reconstructing human activities and provid-
ing photo-realistic rendering from a free viewpoint conve-
niently evolves as a cutting-edge yet bottleneck technique.
Early solutions [28, 29, 58, 11] require pre-scanned tem-
plates or two to four orders of magnitude more time than is
available for daily usages such as immersive tele-presence.
Recently, volumetric approaches have enabled real-time
human performance reconstruction and eliminated the re-
liance of a pre-scanned template model, by leveraging the
RGBD sensors and modern GPUs. The high-end solu-
tions [14, 13, 24, 67] rely on multi-view studio setup to

Figure 1. Our NeuralHumanFVV achieves real-time and photo-
realistic reconstruction results of human performance in novel
views, using only 6 RGB cameras.

achieve high-ﬁdelity reconstruction and rendering in a novel
view but are expensive and difﬁcult to be deployed, while
the low-end approaches [40, 53, 65, 71, 55] adopt the most
handy monocular setup with a temporal fusion pipeline [41]
but suffer from inherent self-occlusion constraint. More-
over, these approaches above rely on depth cameras which
are not as cheap and ubiquitous as color cameras.

The recent learning-based techniques enable robust hu-
man attribute reconstruction [36, 48, 72, 30] using only
RGB input.
In particular, the approaches PIFu [48] and
PIFuHD [49] utilize pixel-aligned implicit function to re-
construct clothed humans with ﬁne geometry details, while
MonoPort [30] further enables real-time inference in a novel
view. However, these methods fail to generate compelling
photo-realistic texture due to the reliance of implicit texture
representation. On the other hand, neural rendering tech-
niques [33, 9, 63, 39, 26, 46] bring huge potential for photo-
realistic novel view synthesis. However, existing solutions
rely on per-scene training or are hard to achieve real-time
performance due to the heavy network and the complicated
3D representation. Moreover, few researchers explore to
combine volumetric geometry modeling and photo-realistic
novel view synthesis of human performance in a data-driven
manner simultaneously, especially under the light-weight
multi-RGB and real-time setting.

In this paper, we attack the above challenges and present

1

 
 
 
 
 
 
NeuralHumanFVV – a real-time human neural volumetric
rendering system using only light-weight and sparse RGB
cameras surrounding the performer. As illustrated in Fig. 1,
our novel approach generates both high-quality geometry
and photo-realistic texture of human activities in arbitrary
novel views, whilst still maintaining real-time computation
and light-weight setup.

Generating such a human free-viewpoint video by com-
bining volumetric geometry modeling and neural texture
synthesis in a data-driven manner is non-trivial. Our key
idea is to encode the local ﬁne-detailed geometry and tex-
ture information of the adjacent input views into the novel
target view, besides utilizing the inherent global informa-
tion from our multi-view setting. To this end, we ﬁrst in-
troduce a neural geometry generation scheme to implicitly
reason about the underlying geometry in a novel view. With
a hierarchical sampling strategy along the camera rays in a
coarse-to-ﬁne manner, we achieve real-time detailed geom-
etry inference. Then, based on the geometry proxy above, a
novel neural blending scheme is proposed to map the input
adjacent images into a photo-realistic texture output in the
target view, through efﬁcient occlusion analysis and blend-
ing weight learning. A boundary-aware upsampling strat-
egy is further adopted to generate high resolution (e.g., 1k)
novel view synthesis result without sacriﬁcing the real-time
performance. Finally, we recover the normal information
in the target view using the same neural blending strategy,
which not only enhances the output ﬁne-grained geome-
try details but also combines our neural geometry genera-
tion and texturing blending into a multi-task learning frame-
work. To summarize, our main contributions include:

• We present a real-time human performance rendering
approach, which is the ﬁrst to reconstruct high quality
geometry and photo-realistic texture results in a novel
view using sparse multiple RGB cameras, achieving
signiﬁcant superiority to existing state-of-the-arts.

• We propose an efﬁcient neural implicit generation
scheme to recover ﬁne geometry details in the novel
view via a hierarchical and coarse-to-ﬁne strategy.

• We propose a novel neural blending scheme to pro-
vide high-resolution and photo-realistic texture result
as well as normal result to further reﬁne the geometry.

2. Related Work

Human Performance Capture. Markerless human perfor-
mance capture [7, 60] technologies have been widely in-
vestigated to generate human free-viewpoint video or ge-
ometry reconstruction. The high-end approaches require
studio-setup with hundreds of cameras and a controlled
imaging environment [54, 32, 23, 11, 24, 16] to produce
high quality surface motion and appearance reconstruction.

Some recent work only relies on the light-weight and single-
view setup [69, 19, 68] and even enables hand-held cap-
ture [62, 44, 64] or drone-based capture [66]. However,
these methods require the pre-scanned template or naked
human model. Only recently, monocular free-form dynamic
reconstruction methods [40, 17, 71, 65, 55] with real-time
performance have been proposed by combining the volu-
metric fusion [12] and the nonrigid tracking [56, 28, 73]
using RGBD camera. However, these monocular methods
still suffer from the inherent self-occlusion constraint and
cannot capture the motions in occluded regions. The light-
weight multi-view solutions [14, 13, 67] serve as a good
compromising settlement between over-demanding hard-
ware setup and high-ﬁdelity reconstruction but still rely on
3 to 8 RGBD streams as input. Comparably, our approach
enables real-time high-quality geometry and photo-realistic
texture reconstruction in novel views only using 6 RGB
cameras surrounding the performer.

Data-Driven Human Modeling. Early human modeling
approaches [50, 15] formulate the discriminative perfor-
mance capture into a regression or classiﬁcation problem
using machine learning techniques. With the advent of
deep neural networks, recent approaches obtain various hu-
man attributes successfully from only RGB input. Some
recent work [8, 36, 25, 18] learns the skeletal pose and
even human shape prior by using human parametric mod-
els [5, 34]. Various approaches [57, 45, 4, 72] propose to
predict human geometry from a single RGB image by uti-
lizing parametric human model as a basic estimation. Sev-
eral work [21, 43, 37, 48, 49] further reveals the effective-
ness of learning the implicit occupancy directly for textured
geometry modeling and even real-time inference [30]. Be-
sides, researchers [6, 27] propose to fetch the garment or
texture information of the human model. However, these
data-driven human modeling methods still fail to recover
ﬁne geometry and texture results simultaneously with the
level of detail present in the RGB inputs. In contrast, we
explore to combine implicit geometry modeling with novel
view synthesis in a data driven manner for real-time, high-
quality and photo-realistic human performance rendering,
achieving signiﬁcant superiority to previous methods.

Neural Rendering. The recent progress of neural render-
ing techniques [59, 9, 63, 26] brings huge potential for
constructing neural scene representations [51, 33, 52, 39]
and photo-realistic novel view blending [38, 20, 61, 46].
For reconstructing neural scenes, various data representa-
tions have been explored, such as point-clouds [3, 63], vox-
els [51, 33] or implicit representations [52, 39, 31]. How-
ever, dedicated per-scene training is required in these meth-
ods when applying the representation to a new scene. Var-
ious methods [20, 46] learn the mapping of features from
source images to novel target views to avoid per-scene train-
ing, while some recent work [61, 70] further models the

2

Figure 2. The pipeline of NeuralHumanFVV. Assuming the video input from six RGB cameras surrounding the performer, our approach
consists of a neural geometry generation stage (Sec. 4.1) and a neural blending stage (Sec. 4.2) to generate live 4D rendering results.

view-dependent effects. However, these methods rely on
heavy networks or complicated 3D proxies which are un-
suitable for real-time applications like immersive telepre-
sense. Chen et al. [9] propose to predict the output tex-
ture using implicit underlying geometry, which enables
continues view generation from monocular image. Re-
searchers [26, 22] also utilize such underlying latent ge-
ometry for novel view synthesis of human performance in
the encoder-decoder manner. However, these approaches
suffer from limited representation ability of a single latent
code for complex human inferior texture output. Besides,
some recent methods [42, 35] combine the neural render-
ing techniques to provide more visually pleasant results
under the traditional RGBD fusion pipeline [14]. Com-
parably, our method is the ﬁrst to embrace neural blend-
ing into the implicit geometry modeling pipeline under the
light-weight multi-RGB and real-time setting, which en-
ables photo-realistic texture and geometry reconstruction in
novel views.

3. Overview

The proposed NeuralHumanFVV marries implicit volu-
metric modeling with neural texture rendering, which gen-
erates high-quality geometry and photo-realistic texture of
human activities in arbitrary novel views in real-time, and
enables various applications like immersive telepresense.
Fig. 2 illustrates the high-level components of our system,
which takes 6 RGB videos surrounding the performer as in-
put and generates high-quality novel-view synthesis results
in challenging scenarios with various poses, clothing types
and topology changes as output.
Neural Geometry Generation. We ﬁrst utilize the inherent
geometry prior from our multi-view setting via the shape-
from-silhouette [10] technique. Then, we adopt the pixel-
aligned implicit function [21, 48, 49] to maintain the com-
plete and continues geometry of the scene. Differently, we
further recover the underlying geometry in novel views with
a multi-stage hierarchical sampling strategy along the cam-
era rays which enables both real-time detailed geometry in-
ference and the following neural blending stage (Sec. 4.1).
Neural Blending. The core of our pipeline is to encode

the local ﬁne-detailed geometry and texture information of
the adjacent input views into the novel target view. A novel
neural blending scheme is proposed to map the input adja-
cent images into a photo-realistic texture output in the tar-
get view, through efﬁcient occlusion analysis and blending
weight learning. A boundary-aware upsampling strategy is
further adopted to generate high resolution (e.g., 1k) novel
view synthesis result without sacriﬁcing the real-time per-
formance. We also recover the normal information in the
target view using the same neural blending strategy, which
not only enhances the output ﬁne-grained geometry details
but also formulates our neural geometry and texture gener-
ation in a multi-task learning framework (Sec. 4.2).

4. NeuralHumanFVV Method

4.1. Neural Geometry Reconstruction

Given the six RGB images input at each frame, we in-
troduce a coarse-to-ﬁne multi-stage neural geometry recon-
struction scheme to generate the inherent detailed human
geometry in novel views in real-time, as illustrated in Fig. 3.
Coarse Geometry Generation. Firstly, we extract the
coarse inherent geometry prior from our multi-view setting.
We apply the Shape-from-Silhouette (SfS) [10] algorithm
on the human masks segmented off-the-shelf video segmen-
tation method to obtain a coarse human shape.
Accelerated Multi-View Implicit Function. We extend
the pixel-aligned implicit function [48, 49] to our multi-
view setting. Such multi-view implicit function (MVIFu)
maintains the complete and continues geometry of the cap-
tured scene, and encodes the human shape priors. Similar to
[48], the implicit function f deﬁnes the occupancy of every
3D point X in the space, which is formulated as:

f (φ(X), z(X)) = s : s ∈ [0.0, 1.0],

φ(X) =

1
n

n
(cid:88)

i

Fi(πi(X)),

(1)

where πi() projects a 3D point into i-th source view; z(X)
is the depth value in the camera coordinate space. The pro-
jected image feature at the pixel coordinate x is formulated

3

TextureGeometryCoarseRefinedMVIFuNeural Geometry ModuleTarget CameraNeural Blending Module෢𝑰𝟏,𝒕𝑵𝟏,𝒕𝑵𝟐,𝒕Blending Map෢𝑰𝟐,𝒕⨂4.2. Neural Blending

t ) and the two input views (Dr

We introduce a neural blending pipeline to encode more
local ﬁne-detailed geometry and texture information of the
adjacent input views than traditional image-based rendering
approaches, so as to produce photo-realistic output in the
target view in a data-driven manner, as illustrated in Fig. 4.
Image Warping and Occlusion Analysis. Most of the tex-
ture information in a target view can be recovered by its
only two adjacent input views in our multi-view setting.
Based on this ﬁnding, we ﬁrst generate the depth maps of
1 and Dr
the target view (Dr
2,
respectively) as described in Sec. 4.1. Then, we use Dr
t to
warp the input image I1 and I2 into the target view, denoted
by I1,t and I2,t. We also warp source view depth maps into
target view and obtain Dr
1,t and Dr
2,t so as to obtain the oc-
clusion map Oi = Dr
t (i = 1, 2), which implies the
occlusion information.
Texture Blending Network(TBN). I1,t and I2,t may be
incorrect due to self-occlusion and inaccurate geometry
proxy. Simply blending them will raise strong artifacts.
Thus, we introduce a blending network ΘT BN , which uti-
lizes the inherent global information from our multi-view
setting, and fuse local ﬁne-detailed geometry and texture
information of the adjacent input views with the pixel-wise
blending map W , which can be formulated as:

i,t − Dr

W = ΘT BN (I1,t, O1, I2,t, O2).

(4)

Boundary-Aware Depth Upsampling. For real-time per-
low resolution
fomance, depth maps are generated at
(256×256). Aiming to photo-realistic rendering, we need to
upsample both the depth map and blending map to 1K reso-
lution. However, na¨ıve upsampling will cause severe zigzag
effect near the boundary due to depth inference ambiguity.
Thus, we propose a boundary-aware scheme to reﬁne the
human boundary area on the depth map. Speciﬁcally, we
use bilinear interpolation to upsample Dr
t . Then a erosion
operation is applied to extract boundary area. Depth values
inside boundary area are recalculated by using the pipeline
as described in Sec. 4.1 and form ˆDr
t at 1K resolution. Then
we warp the original high resolution input images into the
t to obtain ˆIi,t. To this end, our ﬁnal
target view with ˆDr
texture blending result is formulated as:

Figure 3. Illustration of our hierarchical and coarse-to-ﬁne strat-
egy in neural geometry generation. Orange curves are the coarse
geometry surface recovered by SfS; Green curves are the real ge-
ometry. Gray dot line and points are discarded in our hierarchical
sampling algorithm. (a) is the result of coarse reconstruction; (b)
is the result of MVIFu; (c) is the result after reﬁnement.

as Fi(x) = g(Ii(x)), where g denotes a feature extraction
network.

Since extracting the whole human geometry is expen-
sive and unnecessary for real-time immersive application,
the MVIFu in our pipeline only generates geometry explic-
itly in the novel view. Thus, we sample evenly spaced 3D
points from near to far based on the coarse geometry along
each pixel ray with a distance of k in the target view. We
select ﬁrst two adjacent sample points to deﬁne the range
where the depth value of the ray falls. Speciﬁcally, let Xa
and Xb be these two points, and sa, sb are their occupancy,
which satisfy z(Xa) < z(Xb) and sa < 0.5, sb ≥ 0.5.
The predicted depth of this pixel x is given by Dm(x) =
z(Xa)+z(Xb)
. Moreover, point sampling after Xb can be
2
early terminated. We also prune unnecessary sample points
on the background pixel rays outside the coarse geometry
generated by SfS algorithm, which inherently contains the
whole performer so as to enable real-time reconstruction.
Depth Fine-tuning. The geometry Dm obtained through
our accelerated MVIFu is still over smooth because of the
depth averaging. In order to recover the geometry details
(e.g. clothes wrinkles), we introduce a hierarchical sam-
pling strategy. Speciﬁcally, we introduce a depth ﬁne-
tuning network h which takes the feature of the midpoint
between two selected sample points as input, and outputs
the displacement of depth value:

h(φ(

Xa + Xb
2

)) = o : o ∈ [−1.0, 1.0].

(2)

Ir = ˆW · ˆI1,t + (1.0 − ˆW ) · ˆI2,t,

(5)

Here, positions on this segment are mapped from −1.0 to
1.0 linearly, and the reﬁned depth value Dr(x) can be com-
posed with the offset o to encode more geometry details:

Dr(x) = Dm(x) + k ·

o + 1
2

.

(3)

where ˆW is the high resolution blending map upsampled by
bilinear interpolation directly.
Neural Normal Reﬁnement. We apply networks intro-
duced in [49] on the input RGB images to inference its
normal maps. Then, the normal information in the target
view is restored via the same neural blending strategy. The
blended normal map Nt can further enable the geometry

4

𝑫𝒎(𝒙)𝑫𝒄(𝒙)𝑫𝒓(𝒙)𝑿𝒂𝑿𝒃Displacement(a)(b)(c)Target CamerakFigure 4. Illustration of our neural blending scheme, which encodes the local ﬁne-detailed geometry and texture information of the adjacent
input views into the novel target view.

reﬁnement. Speciﬁcally, we introduce a normal reﬁnement
network ΘN RN to infer the displacement of the target depth
map from Nt and ˆDr

t , as illustrated in Fig. 4.

4.3. Data and implementation Details

The key of our NeuralHumanFVV is to train the neural
networks in Sec. 4.1 and Sec.4.2 properly, including the fea-
ture extractor g, continuous implicit function f , depth ﬁne-
tuning network h, as well as our TBN ΘT BN and NRN
ΘN RN . Speciﬁcally, g is a U-Net [47] and outputs a 64
channels feature maps. f represented by MLPs has the same
structure in [48] but the network dimension is further re-
duced for real-time performance, while h has the same net-
work architecture, only with a different activation function
of the last layer replaced by hyperbolic tangent. Besides,
both our TBN ΘT BN and NRN ΘN RN adopt the U-Net
structure.

We utilize 1820 scans from Twindom [1] and augment
the dataset by rigging the 3D model to add more challeng-
ing poses, so as to enhance the generation ability of our
networks. We ﬁx the six input camera views as a rig sur-
rounding the performer, and sample 180 virtual target views
on a sphere. Note that all the 3D models locate on the cen-
tral regions of the sphere and all the cameras face towards
the model. Our training dataset contains the RGB images,
normal maps and depth maps for all the views and models.
For the training of g and h in our MVIFu module, we
only use the six input camera views in our dataset, and fol-
low the training procedure similar to previous work [48]
Then, we train our depth ﬁne-tuning network h using the
corresponding pair-wised data provided by the MVIFu.

For the training of our texture blending network ΘT BN ,
we set out to apply a multi-task learning scheme so as to
enable more robust blending weight learning. The training
objective is to make both the blended texture and normal

map as close as possible to the ground truth, as these two
tasks share the same blending map in our ΘT BN . To this
end, the loss function includes a appearance term and a nor-
mal term with perceptual loss:

Lrgb =

Lnorm =

1
n

1
n

n
(cid:88)

j
n
(cid:88)

j

((cid:107)I j

r − I j

gt(cid:107)2

2) + (cid:107)ϕ(I j

r ) − ϕ(I j

gt)(cid:107)2

2),

((cid:107)N j

t − N j

gt(cid:107)2

2) + (cid:107)ϕ(N j

t ) − ϕ(N j

gt)(cid:107)2

2),

L = λ · Lrgb + (1.0 − λ) · Lnorm,

(6)

where Igt and Ngt are the ground truth RGB images and
normal maps; ϕ(·) denotes the output features of the third-
layer of pretrained VGG-19.

Our normal reﬁnement network (NRN) ΘN RN need to
be adapted to real data, which means we cannot supervise
the network training using the same synthetic dataset. Thus,
we introduce a self-supervise learning scheme where all the
training inputs are collected from the real data generated in
our pipeline. The objective is to minimize the loss function:

L =

1
n

n
(cid:88)

j

(cid:107)∇(

ˆ
t + ΘN RN (N j
Dr,j
t ,

ˆ
t )) − N j
Dr,j

t (cid:107)2
2

(7)

where ∇(·) is the operator which calculates the normal map
from input depth map.

5. Experimental Results

In this section, we evaluate our NeuralHumanFVV
method on a variety of challenging scenarios. We run our
experiments on a PC with 3.7 GHz Intel i7-8700k CPU
32GB RAM, and Nvidia GeForce RTX3090 GPU. With the

5

𝑰𝟏,𝒕𝑶𝟏𝑰𝟐,𝒕𝑶𝟐෢𝑰𝟐,𝒕𝑵𝟏,𝒕෢𝑫𝒕𝑵𝒕𝑾𝑵𝒈𝒕LossLoss𝑰𝒈𝒕Normal Refinement Network 𝚯𝑵𝑹𝑵Texture Blending Network 𝚯𝑻𝑩𝑵𝑰𝒓Refined෢𝑫𝒕෢𝑰𝟏,𝒕𝑵𝟐,𝒕Figure 5. The geometry and texture results of our NeuralHumanFVV on several sequences, including “spiderman”, “ironman”, “undress-
ing”, “ﬂoral dress”, “basketball” and “backpack” from the upper left to lower right.

live stream data from six RGB cameras, our system gen-
erates high-quality geometry and texture results in novel
views at 12 fps to enable various interactive immersive ap-
plications. The whole pipeline costs approximate 80 ms per
frame, where the neural geometry generation takes 64 ms
and 16 ms for the neural blending stage. Fig. 5 demon-
strates several results of our NeuralHumanFVV, which can
generate free-view high quality geometry and texture results
simultaneously. Noted that our approach can handle human
object interaction scenarios with topology changes, such as
playing basketball, carrying bag and removing clothes.

In our real testing data, performers act complex motions
with self-occlusion, and dress rich texture clothes, such as
ﬂoral skirt and plaid shirt. We compare our NeuralHuman-
FVV against the state-of-the-art methods MonoPort [30],
Multi-PIFu [48] and Continuous View Control [9] both in
geometry and texture.

As shown in Fig. 6, our approach achieves signiﬁcantly
better texture results even when the texture is extraordinar-
ily complex like the ﬂoral dress and the geometry we es-
timated ﬁlled with elegant details instead of a sketchy and
bloated object. Even applying per-vertex texture mapping

Method MonoPort Multi-
PIFu

Multi-
PIFu*

CVC

Ours

RGB -
1
RGB -
6

95.1±4.8 67.5±3.8 43.7±1.9 98.1±21.9

27.6 ± 1.6

144.8±6.6 66.4±2.2 56.1±2.1 103.1±27.4 26.1 ± 1.2

Table 1. Quantitative comparison of MonoPort [30], Multi-
PIFu [48], Continuous View Control [9] and NeuralHumanFVV.
Multi-PIFu* denotes per-vertex texture mapping using the geom-
etry from Multi-PIFu as input. RGB 1 and RGB 6 respectively
present the MAE in one view and six views.

in the geometry from Multi-PIFu [48], image blurs occur
much more frequently in contrast to our results.

5.1. Comparison

Then, we make a quantitative comparison on the whole
real testing dataset. We use mean absolute error (MAE)
as the error metric. For overall MAE calculation, we av-
erage all MAEs from all images and frames. Since Mono-
port [30] only takes one image as input, we also evaluate
methods with single camera input (RGB 1), compared with

6

Figure 6. Qualitative comparison. (a) Input images. (b-e) are the geometry and texture results from MonoPort [30], Multi-PIFu [48],
Continuous View Control [9] and ours, respectively. Note that the two texture results in (c) corresponds to the implicit texture and per-
vertex texture, respectively.

using all six cameras (RGB 6). As illustrated in Table. 1,
our approach surpasses other methods in all scenarios with
distinct differences in overall MAE.

We make a comparison on a synthetic dynamic sequence
with 600 frames, and generate 90 different target views
to evaluate the MAE. The result is shown in Fig. 7. Our
method can stay lowest MAE in the entire sequence.

5.2. Ablation Study

Neural Geometry Generation. Here, we evaluate our neu-
ral geometry generation scheme. As shown in Fig. 8 (b), the
results from SfS [10] only provide coarse geometry priors
since only boundary information are utilized. Our scheme
without the normal reﬁnement in Fig. 8 (c) can generate
mid-level geometry details such as the clothing wrinkles but
still suffers from over-smooth results, especially on the face
regions. In contrast, our approach with full pipeline in Fig. 8
(d) enables high-quality geometry detail generation almost
with the level of details present in the input images.
Neural Texture Blending. We further evaluate our neural
texture blending scheme. In Fig. 9, we compare with our
variations with different texturing schemes using the same
geometry proxy. The per-vertex texturing in Fig. 9 (a) suf-
fers from severe block artifacts, while the ofﬂine scheme us-
ing the software AGI [2] in Fig. 9 (b) causes inferior results
in those regions near the stitching seams. And our neural
scheme at low resolution situation in Fig. 9 (c) and the one
without the boundary optimization in Fig. 9 (d) suffer from
over-smooth texture or coarse boundary, respectively.
In
contrast, our full neural texture scheme in Fig. 9 (e) enables
photo-realistic texture reconstruction in novel views.

For quantitative analysis of the individual components of
NeuralHumanFVV, we utilize two different geometries as

Figure 7. Quantitative and qualitative comparison on synthe-
sis sequence against NeuralHumanFVV, Multi-PIFu [48], Mono-
Port [30] and Continuous View Control [9].(a) Color image in
ground truth; (b) NeuralHumanFVV; (c) Multi-PIFu; (d)Multi-
PIFu* (e) MonoPort;
(g)Error
curves. Denote that Multi-PIFu* is the result of per-vertex texture
mapping using the geometry from Multi-PIFu.

(f) Continuous View Control;

bases and two different texture methods to make a compari-
son among these four outputs as shown in as Fig. 10. Fig. 10
(a) is from complete NeuralHumanFVV while Fig. 10 (b)
using the geometry from Multi-PIFu, Fig. 10 (c) using the
same geometry as Fig. 10 (a) but per-vertex texture map-
ping, and Fig. 10 (d) is yielded by Multi-PIFu and per-
vertex texture mapping. Not only our image outcome which
has fewer artifacts but also the per-frame mean error for the
three variation of our approach without model completion
in Fig. 10 (e) shows the advancement of our NeuralHuman-
FVV.
Camera Number. To evaluate the inﬂuence of input views
in our multi-view setting, we compare to the variation of our

7

(a)(e)(c)(b)(f)(d)(g)Figure 8. Evaluation of our neural geometry generation. (a) In-
put images. (b) Geometry from SfS [10]; (c) Geometry without
normal reﬁnement; (d) Geometry from NeuralHumanFVV.

Figure 10. Quantitative evaluation. (a-d) The reconstructed results
of ours, w/o neural geometry, w/o neural texture, w/o both.(e) Nu-
merical error curves.

Figure 9. Qualitative evaluation of our neural texture blending
scheme.
(a) Per-vertex texture mapping; (b) AGI[2]; (c) Neu-
ralHumanFVV at 256 × 256 resolution ; (d) NeuralHumanFVV
without boundary optimization; (e) NeuralHumanFVV.

pipeline using various numbers of input camera views. As
shown in Fig. 11, the reconstruction results without enough
camera views suffer from severe geometry and blending ar-
tifacts and the average error increases signiﬁcantly as the
the camera number decreases. Empirically, the setting with
six cameras serve as a good compromising settlement.

6. Discussion

Limitation. As the ﬁrst trial to enable real-time and photo-
realistic neural human performance capture and rendering
from only sparse RGB inputs, the proposed NeuralHuman-
FVV system still owns some limitations. First, inaccuracy
of segmentation leads to incomplete regions in our ﬁnal syn-
thesized images. Our texturing results depend on the input
image resolutions. Thin structures like ﬁngers are difﬁcult
to reconstruct due to the input with limited resolution. Our
system generates plausible geometry detail from RGB im-

8

Figure 11. Evaluation of the number input camera views. (a) Cu-
mulative distribution function of the mean absolute error. (b) The
reference capture scene. (c, d, e) Our reconstructed texture results
using six, four and two cameras, respectively.

ages but physically inaccurate when the testing images de-
viate much from the training ones.

Conclusion.We have presented a real-time neural perfor-
mance rendering system to generate high-quality geome-
try and photo-realistic textures of human activities in novel
views only using sparse multiple RGB cameras. Our neu-
ral geometry generation beneﬁts inherently from our multi-
view setting and enables efﬁcient and implicit reasoning of
underlying geometry in a novel view. Our neural blend-
ing scheme with occlusion analysis and boundary-aware up-
sampling further enables to recover high resolution (e.g.,
1k) and photo-realistic textures without sacriﬁcing the real-
time performance. Our experimental results demonstrate
the effectiveness of NeuralHumanFVV for high-quality hu-
man performance rendering in challenging scenarios with
various poses, clothing types and topology changes. We
believe that our approach is a critical step to virtually but
realistic teleport human performances, with many potential
applications in VR/AR like gaming, entertainment and im-
mersive telepresense.

(a)(b)(c)(d)(e)References

[1] Twindom dataset. https://https://web.twindom.

com/. 5
[2] Agisoft

photoscan
http://www.agisoft.com/downloads/installer/, 2019.
8

professional.
7,

[3] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry
Ulyanov, and Victor Lempitsky. Neural point-based graph-
ics. arXiv preprint arXiv:1906.08240, 2019. 2

[4] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,
and Marcus Magnor. Tex2shape: Detailed full human body
In The IEEE International
geometry from a single image.
Conference on Computer Vision (ICCV), October 2019. 2
[5] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. Scape: Shape
completion and animation of people. In ACM SIGGRAPH
2005 Papers, SIGGRAPH ’05, page 408–416, New York,
NY, USA, 2005. Association for Computing Machinery. 2
[6] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,
and Gerard Pons-Moll. Multi-garment net: Learning to dress
3d people from images. In IEEE International Conference on
Computer Vision (ICCV). IEEE, oct 2019. 2

[7] Chris Bregler and Jitendra Malik. Tracking people with
twists and exponential maps. In Computer Vision and Pat-
tern Recognition (CVPR), 1998. 2

[8] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In Computer Vision and Pattern Recognition (CVPR),
2017. 2

[9] Xu Chen, Jie Song, and Otmar Hilliges. Monocular neu-
ral image based rendering with continuous view control. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 4089–4099, October 2019.
1, 2, 3, 6, 7

[10] Kong Man Cheung, Simon Baker, and Takeo Kanade. Shape-
from-silhouette of articulated objects and its use for human
In 2003
body kinematics estimation and motion capture.
IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, 2003. Proceedings., volume 1, pages I–
I, 2003. 3, 7, 8

[11] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-
nis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,
and Steve Sullivan. High-quality streamable free-viewpoint
video. ACM Transactions on Graphics (TOG), 34(4):69,
2015. 1, 2

[12] Brian Curless and Marc Levoy. A volumetric method for
In Proceed-
building complex models from range images.
ings of the 23rd Annual Conference on Computer Graph-
ics and Interactive Techniques, SIGGRAPH ’96, pages 303–
312, New York, NY, USA, 1996. ACM. 2

[13] Mingsong Dou, Philip Davidson, Sean Ryan Fanello, Sameh
Khamis, Adarsh Kowdle, Christoph Rhemann, Vladimir
Tankovich, and Shahram Izadi. Motion2fusion: Real-
time volumetric performance capture. ACM Trans. Graph.,
36(6):246:1–246:16, Nov. 2017. 1, 2

colano, Christoph Rhemann, David Kim, Jonathan Taylor,
Pushmeet Kohli, Vladimir Tankovich, and Shahram Izadi.
Fusion4D: Real-time Performance Capture of Challenging
In ACM SIGGRAPH Conference on Computer
Scenes.
Graphics and Interactive Techniques, 2016. 1, 2, 3

[15] Varun Ganapathi, Christian Plagemann, Daphne Koller, and
Sebastian Thrun. Real time motion capture using a single
time-of-ﬂight camera. 2010. 2

[16] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,
Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-
Escolano, Rohit Pandey, Jason Dourgarian, and et al. The re-
lightables: Volumetric performance capture of humans with
realistic relighting. ACM Trans. Graph., 38(6), Nov. 2019. 2
[17] Kaiwen Guo, Feng Xu, Tao Yu, Xiaoyang Liu, Qionghai Dai,
and Yebin Liu. Real-time geometry, albedo and motion re-
construction using a single rgbd camera. ACM Transactions
on Graphics (TOG), 2017. 2

[18] Rıza Alp G¨uler, Natalia Neverova, and Iasonas Kokkinos.
In
Densepose: Dense human pose estimation in the wild.
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018. 2

[19] Marc Habermann, Weipeng Xu, Michael Zollh¨ofer, Gerard
Pons-Moll, and Christian Theobalt. Livecap: Real-time
human performance capture from monocular video. ACM
Transactions on Graphics (TOG), 38(2):14:1–14:17, 2019.
2

[20] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,
George Drettakis, and Gabriel Brostow. Deep blending for
free-viewpoint image-based rendering. ACM Trans. Graph.,
37(6), Dec. 2018. 2

[21] Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao, Jun Xing,
Chloe LeGendre, Linjie Luo, Chongyang Ma, and Hao Li.
Deep volumetric video from very sparse multi-view perfor-
mance capture. In The European Conference on Computer
Vision (ECCV), September 2018. 2, 3

[22] Shi Jin, Ruiynag Liu, Yu Ji, Jinwei Ye, and Jingyi Yu. Learn-
ing to dodge a bullet: Concyclic view morphing via deep
learning. In Vittorio Ferrari, Martial Hebert, Cristian Smin-
chisescu, and Yair Weiss, editors, Computer Vision – ECCV
2018, pages 230–246, Cham, 2018. Springer International
Publishing. 3

[23] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,
Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser
Sheikh. Panoptic Studio: A Massively Multiview System for
Social Motion Capture. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 3334–3342,
2015. 2

[24] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-
ture: A 3d deformation model for tracking faces, hands, and
In The IEEE Conference on Computer Vision and
bodies.
Pattern Recognition (CVPR), June 2018. 1, 2

[25] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In Computer Vision and Pattern Regognition (CVPR),
2018. 2

[14] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip
Davidson, Sean Fanello, Adarsh Kowdle, Sergio Orts Es-

[26] Youngjoong Kwon, Stefano Petrangeli, Dahun Kim, Hao-
liang Wang, Eunbyung Park, Viswanathan Swaminathan,

9

and Henry Fuchs. Rotationally-temporally consistent novel
view synthesis of human performance video.
In Andrea
Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael
Frahm, editors, Computer Vision – ECCV 2020, pages 387–
402, Cham, 2020. Springer International Publishing. 1, 2,
3

[27] Verica Lazova, Eldar Insafutdinov, and Gerard Pons-Moll.
360-degree textures of people in clothing from a single im-
age. In International Conference on 3D Vision (3DV), sep
2019. 2

[28] Hao Li, Bart Adams, Leonidas J Guibas, and Mark Pauly.
Robust single-view geometry and motion reconstruction.
28(5):175, 2009. 1, 2

[29] Hao Li, Linjie Luo, Daniel Vlasic, Pieter Peers, Jovan
Popovi´c, Mark Pauly, and Szymon Rusinkiewicz. Tempo-
rally coherent completion of dynamic shapes. In ACM Trans.
Graph., volume 31, Feb. 2012. 1

[30] Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle
Olszewski, and Hao Li. Monocular real-time volumetric
performance capture.
In Andrea Vedaldi, Horst Bischof,
Thomas Brox, and Jan-Michael Frahm, editors, Computer
Vision – ECCV 2020, pages 49–67, Cham, 2020. Springer
International Publishing. 1, 2, 6, 7

[31] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
Christian Theobalt. Neural sparse voxel ﬁelds. NeurIPS,
2020. 2

[32] Yebin Liu, Juergen Gall, Carsten Stoll, Qionghai Dai, Hans-
Peter Seidel, and Christian Theobalt. Markerless motion
capture of multiple characters using multiview image seg-
mentation. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 35(11):2720–2735, 2013. 2

[33] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel
Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-
umes: Learning dynamic renderable volumes from images.
ACM Trans. Graph., 38(4), July 2019. 1, 2

[34] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. Smpl: A skinned multi-
person linear model. ACM Trans. Graph., 34(6):248:1–
248:16, Oct. 2015. 2

[35] Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel
Pidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh
Khamis, Philip Davidson, Anastasia Tkach, Peter Lincoln,
and et al. Lookingood: Enhancing performance capture with
real-time neural re-rendering. ACM Trans. Graph., 37(6),
Dec. 2018. 3

[36] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,
Helge Rhodin, Mohammad Shaﬁei, Hans-Peter Seidel,
Weipeng Xu, Dan Casas, and Christian Theobalt. Vnect:
Real-time 3d human pose estimation with a single rgb cam-
era. ACM Transactions on Graphics (TOG), 36(4), 2017. 1,
2

[37] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019. 2

Brualla. Neural rerendering in the wild. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019. 2

[39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and
Jan-Michael Frahm, editors, Computer Vision – ECCV 2020,
pages 405–421, Cham, 2020. Springer International Publish-
ing. 1, 2

[40] Richard A. Newcombe, Dieter Fox, and Steven M. Seitz.
DynamicFusion: Reconstruction and Tracking of Non-Rigid
Scenes in Real-Time. June 2015. 1, 2

[41] Richard A. Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J. Davison, Push-
meet Kohli, Jamie Shotton, Steve Hodges, and Andrew
Fitzgibbon. KinectFusion: Real-Time Dense Surface Map-
ping and Tracking. In Proc. of ISMAR, pages 127–136, 2011.
1

[42] Rohit Pandey, Anastasia Tkach, Shuoran Yang, Pavel Pid-
lypenskyi, Jonathan Taylor, Ricardo Martin-Brualla, Andrea
Tagliasacchi, George Papandreou, Philip Davidson, Cem Ke-
skin, Shahram Izadi, and Sean Fanello. Volumetric capture
of humans with a single rgbd camera via semi-parametric
learning. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019. 3

[43] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019. 2
[44] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and
Michael J. Black. Expressive body capture: 3d hands, face,
and body from a single image. In Proceedings IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), pages
10975–10985, June 2019. 2

[45] Albert Pumarola, Jordi Sanchez-Riera, Gary P. T. Choi, Al-
berto Sanfeliu, and Francesc Moreno-Noguer.
3dpeople:
Modeling the geometry of dressed humans. In The IEEE In-
ternational Conference on Computer Vision (ICCV), October
2019. 2

[46] Gernot Riegler and Vladlen Koltun. Free view synthesis.
In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-
Michael Frahm, editors, Computer Vision – ECCV 2020,
Cham, 2020. Springer International Publishing. 1, 2

[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention, pages 234–241.
Springer, 2015. 5

[48] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
In The IEEE International Conference on Computer
tion.
Vision (ICCV), October 2019. 1, 2, 3, 5, 6, 7

[38] Moustafa Meshry, Dan B. Goldman, Sameh Khamis, Hugues
Hoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-

[49] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. Pifuhd: Multi-level pixel-aligned implicit function for

10

In Proceedings of
high-resolution 3d human digitization.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2020. 1, 2, 3, 4

[50] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio,
R. Moore, A. Kipman, and A. Blake. Real-time Human Pose
Recognition in Parts from Single Depth Images. 2011. 2
[51] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias
Niessner, Gordon Wetzstein, and Michael Zollhofer. Deep-
voxels: Learning persistent 3d feature embeddings. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019. 2

[52] Vincent Sitzmann, Michael Zollhoefer, and Gordon Wet-
zstein.
Scene representation networks: Continuous 3d-
structure-aware neural scene representations. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett, editors, Advances in Neural Information Pro-
cessing Systems, volume 32, pages 1121–1132. Curran As-
sociates, Inc., 2019. 2

[53] M. Slavcheva, M. Baust, D. Cremers, and S. Ilic. Killing-
Fusion: Non-rigid 3D Reconstruction without Correspon-
dences. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 1

[54] Carsten Stoll, Nils Hasler, Juergen Gall, Hans-Peter Seidel,
and Christian Theobalt. Fast articulated motion tracking us-
ing a sums of Gaussians body model. In International Con-
ference on Computer Vision (ICCV), 2011. 2

[55] Zhuo Su, Lan Xu, Zerong Zheng, Tao Yu, Yebin Liu, and Lu
Fang. Robustfusion: Human volumetric capture with data-
driven visual cues using a rgbd camera. In Andrea Vedaldi,
Horst Bischof, Thomas Brox, and Jan-Michael Frahm, edi-
tors, Computer Vision – ECCV 2020, pages 246–264, Cham,
2020. Springer International Publishing. 1, 2

[56] Robert W Sumner, Johannes Schmid, and Mark Pauly. Em-
bedded deformation for shape manipulation. ACM Transac-
tions on Graphics (TOG), 26(3):80, 2007. 2

[57] Sicong Tang, Feitong Tan, Kelvin Cheng, Zhaoyang Li, Siyu
Zhu, and Ping Tan. A neural network for detailed human
In The IEEE Inter-
depth estimation from a single image.
national Conference on Computer Vision (ICCV), October
2019. 2

[58] J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon. The vitru-
vian manifold: Inferring dense correspondences for one-shot
human pose estimation. In 2012 IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 103–110, 2012.
1

[59] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann,
Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-
Brualla, Tomas Simon, Jason Saragih, Matthias Nießner,
Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan
Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman,
Dan B. Goldman, and Michael Zollh¨ofer. State of the Art on
Neural Rendering. Computer Graphics Forum, 2020. 2
[60] Christian Theobalt, Edilson de Aguiar, Carsten Stoll, Hans-
Peter Seidel, and Sebastian Thrun. Performance capture
from multi-view video. In Image and Geometry Processing
for 3-D Cinematography, pages 127–149. Springer, 2010. 2
[61] Justus Thies, Michael Zollh¨ofer, Christian Theobalt, Marc
Image-guided neural

Stamminger, and Matthias Nießner.

object rendering. In International Conference on Learning
Representations, 2020. 2

[62] Chenglei Wu, Carsten Stoll, Levi Valgaerts, and Christian
Theobalt. On-set performance capture of multiple actors
with a stereo camera. 32(6), 2013. 2

[63] Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu.
In Proceedings of
Multi-view neural human rendering.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2020. 1, 2

[64] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocu-
lar total capture: Posing face, body, and hands in the wild.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019. 2

[65] L. Xu, W. Cheng, K. Guo, L. Han, Y. Liu, and L. Fang. Fly-
fusion: Realtime dynamic scene reconstruction using a ﬂy-
ing depth camera. IEEE Transactions on Visualization and
Computer Graphics, pages 1–1, 2019. 1, 2

[66] Lan Xu, Yebin Liu, Wei Cheng, Kaiwen Guo, Guyue
Zhou, Qionghai Dai, and Lu Fang. Flycap: Markerless
motion capture using multiple autonomous ﬂying cameras.
IEEE Transactions on Visualization and Computer Graph-
ics, 24(8):2284–2297, Aug 2018. 2

[67] L. Xu, Z. Su, L. Han, T. Yu, Y. Liu, and L. FANG. Unstruc-
turedfusion: Realtime 4d geometry and texture reconstruc-
tion using commercialrgbd cameras. IEEE Transactions on
Pattern Analysis and Machine Intelligence, pages 1–1, 2019.
1, 2

[68] Lan Xu, Weipeng Xu, Vladislav Golyanik, Marc Haber-
mann, Lu Fang, and Christian Theobalt. Eventcap: Monoc-
ular 3d capture of high-speed human motions using an event
In Proceedings of the IEEE/CVF Conference on
camera.
Computer Vision and Pattern Recognition (CVPR), June
2020. 2

[69] Weipeng Xu, Avishek Chatterjee, Michael Zollh¨ofer, Helge
Rhodin, Dushyant Mehta, Hans-Peter Seidel, and Christian
Theobalt. Monoperfcap: Human performance capture from
monocular video. ACM Transactions on Graphics (TOG),
37(2):27:1–27:15, 2018. 2

[70] Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao
Su, and Ravi Ramamoorthi. Deep view synthesis from sparse
photometric images. ACM Trans. Graph., 38(4), July 2019.
2

[71] Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai
Dai, Hao Li, Gerard Pons-Moll, and Yebin Liu. Doublefu-
sion: Real-time capture of human performances with inner
body shapes from a single depth sensor. Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), 2019.
1, 2

[72] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and
Yebin Liu. Deephuman: 3d human reconstruction from a sin-
gle image. In The IEEE International Conference on Com-
puter Vision (ICCV), October 2019. 1, 2

[73] Michael Zollh¨ofer, Matthias Nießner, Shahram Izadi,
Christoph Rehmann, Christopher Zach, Matthew Fisher,
Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian
Theobalt, et al. Real-time Non-rigid Reconstruction using
an RGB-D Camera. ACM Transactions on Graphics (TOG),
33(4):156, 2014. 2

11

