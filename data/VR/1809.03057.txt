Variance Reduction in Monte Carlo Counterfactual Regret Minimization
(VR-MCCFR) for Extensive Form Games using Baselines

Martin Schmid1, Neil Burch1, Marc Lanctot1, Matej Moravcik1, Rudolf Kadlec1, Michael Bowling1,2
DeepMind1
University of Alberta2
{mschmid,burchn,lanctot,moravcik,rudolfkadlec,bowlingm}@google.com

8
1
0
2

p
e
S
9

]
T
G
.
s
c
[

1
v
7
5
0
3
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Learning strategies for imperfect information games from
samples of interaction is a challenging problem. A common
method for this setting, Monte Carlo Counterfactual Regret
Minimization (MCCFR), can have slow long-term conver-
gence rates due to high variance. In this paper, we introduce
a variance reduction technique (VR-MCCFR) that applies to
any sampling variant of MCCFR. Using this technique, per-
iteration estimated values and updates are reformulated as a
function of sampled values and state-action baselines, simi-
lar to their use in policy gradient reinforcement learning. The
new formulation allows estimates to be bootstrapped from
other estimates within the same episode, propagating the ben-
eﬁts of baselines along the sampled trajectory; the estimates
remain unbiased even when bootstrapping from other esti-
mates. Finally, we show that given a perfect baseline, the vari-
ance of the value estimates can be reduced to zero. Experi-
mental evaluation shows that VR-MCCFR brings an order of
magnitude speedup, while the empirical variance decreases
by three orders of magnitude. The decreased variance allows
for the ﬁrst time CFR+ to be used with sampling, increasing
the speedup to two orders of magnitude.

Introduction

Policy gradient algorithms have shown remarkable success
in single-agent reinforcement learning (RL) (Mnih et al.
2016; Schulman et al. 2017). While there has been evi-
dence of empirical success in multiagent problems (Foerster
et al. 2017; Bansal et al. 2018), the assumptions made by
RL methods generally do not hold in multiagent partially-
observable environments. Hence, they are not guaranteed to
ﬁnd an optimal policy, even with tabular representations in
two-player zero-sum (competitive) games (Littman 1994).
As a result, policy iteration algorithms based on computa-
tional game theory and regret minimization have been the
preferred formalism in this setting. Counterfactual regret
minimization (Zinkevich et al. 2008) has been a core compo-
nent of this progress in Poker AI, leading to solving Heads-
Up Limit Texas Hold’em (Bowling et al. 2015) and defeat-
ing professional poker players in No-Limit (Moravˇc´ık et al.
2017; Brown and Sandholm 2017).

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: High-level overview of Variance Reduction MC-
CFR (VR-MCCFR) and related methods. a) CFR traverses
the entire tree on every iteration. b) MCCFR samples tra-
jectories and computes the values only for the sampled ac-
tions, while the off-trajectory actions are treated as zero-
valued. While MCCFR uses importance sampling weight to
ensure the values are unbiased, the sampling introduces high
variance. c) VR-MCCFR follows the same sampling frame-
work as MCCFR, but uses baseline values for both sam-
pled actions (in blue) as well as the off-trajectory actions
(in red). These baselines use control variates and send up
bootstrapped estimates to decrease the per-iteration variance
thus speeding up the convergence.

The two ﬁelds of RL and computational game theory have
largely grown independently. However, there has been re-
cent work that relates approaches within these two com-
munities. Fictitious self-play uses RL to compute approxi-
mate best responses and supervised learning to combine re-
sponses (Heinrich et al. 2015). This idea is extended to a uni-
ﬁed training framework that can produce more general poli-
cies by regularizing over generated response oracles (Lanc-
tot et al. 2017). RL-style regressors were ﬁrst used to com-
press regrets in game theorietic algorithms (Waugh et al.
2015). DeepStack introduced deep neural networks as gen-
eralized value-function approximators for online planning in
imperfect information games (Moravˇc´ık et al. 2017). These
value functions operate on a belief-space over all possible
states consistent with the players’ observations.

This paper similarly unites concepts from both ﬁelds,
proposing an unbiased variance reduction technique for
Monte Carlo counterfactual regret minimization using an
analog of state-action baselines from actor-critic RL meth-
ods. While policy gradient methods typically involve Monte
Carlo estimates, the analog in imperfect information settings
is Monte Carlo Counterfactual Regret Minimization (MC-
CFR) (Lanctot et al. 2009). Policy gradient estimates based

c) VR-MCCFRb) MCCFRa) CFR 
 
 
 
 
 
on a single sample of an episode suffer signiﬁcantly from
variance. A common technique to decrease the variance is
a state or state-action dependent baseline value that is sub-
tracted from the observed return. These methods can dras-
tically improve the convergence speed. However, no such
methods are known for MCCFR.

MCCFR is a sample based algorithm in imperfect infor-
mation settings, which approximates counterfactual regret
minimization (CFR) by estimating regret quantities neces-
sary for updating the policy. While MCCFR can offer faster
short-term convergence than original CFR in large games, it
suffers from high variance which leads to slower long-term
convergence.

CFR+ provides signiﬁcantly faster empirical performance
and made solving Heads-Up Limit Texas Hold’em possi-
ble (Bowling et al. 2015). Unfortunately, CFR+ has so far
did not outperform CFR in Monte Carlo settings (Burch
2017) (also see Figure ( 7) in the appendix for an experi-
ment).

In this work, we reformulate the value estimates using
a control variate and a state-action baseline. The new for-
mulation includes any approximation of the counterfactual
values, which allows for a range of different ways to insert
domain-speciﬁc knowledge (if available) but also to design
values that are learned online.

Our experiments show two orders of magnitude improve-
ment over MCCFR. For the common testbed imperfect in-
formation game – Leduc Poker – VR-MCCFR with a state-
action baseline needs 250 times fewer iterations than MC-
CFR to reach the same solution quality. In contrast to RL al-
gorithms in perfect information settings, where state-action
baselines bring little to no improvement over state base-
lines (Tucker et al. 2018), state-action baselines lead to
signiﬁcant improvement over state baselines in multiagent
partially-observable settings. We suspect this is due to vari-
ance from the environment and different dynamics of the
policies during the computation.

Related Work
There are standard variance reduction techniques for Monte
Carlo sampling methods (Owen 2013) and the use of control
variates in these settings has a long history (Boyle 1977).
Reducing variance is particularly important when estimat-
ing gradients from sample trajectories. Consequentially, the
use of a control variates using baseline has become stan-
dard practice in policy gradient methods (Williams 1992;
Sutton and Barto 2017). In RL, action-dependent baselines
have recently shown promise (Wu et al. 2018; Liu et al.
2018) but the degree to which variance is indeed reduced re-
mains unclear (Tucker et al. 2018). We show that in our set-
ting of MCCFR in imperfect information multiplayer games,
action-dependent baselines necessarily inﬂuence the vari-
ance of the estimates, and we conﬁrm the reduction empir-
ically. This is important because lower-variance estimates
lead to better regret bounds (Gibson et al. 2012).

There have been a few uses of variance reduction tech-
niques in multiplayer games, within Monte Carlo tree search
(MCTS). In MCTS, control variates have used to augment
the reward along a trajectory using a property of the state

before and after a transition (Veness et al. 2011) and to aug-
ment the outcome of a rollout from its length or some pre-
determined quality of the states visited (Pepels et al. 2014).
Our baseline-improved estimates are similar to the ones
used in AIVAT (Burch et al. 2018). AIVAT deﬁnes estimates
of expected values using heuristic values of states as base-
lines in practice. Unlike this work, AIVAT was only used for
evaluation of strategies.

To the best of our knowledge, there has been two ap-
plications of variance reduction in Monte Carlo CFR: by
manipulating the chance node distribution (Lanctot 2013,
Section 7.5) and by sampling (“probing”) more trajectories
for more estimates of the underlying values (Gibson et al.
2012). The variance reduction (and resulting drop in conver-
gence rate) is modest in both cases, whereas we show more
than a two order of magnitude speed-up in convergence us-
ing our method.

Background
We start with the formal background necessary to under-
stand our method. For details, see (Shoham and Leyton-
Brown 2009; Sutton and Barto 2017).

A two

player

extensive-form game

is

tuple

(N , A, H, Z, τ, u, I).

N = {1, 2, c} is a ﬁnite set of players, where c is a special
player called chance. A is a ﬁnite set of actions. Players take
turns choosing actions, which are composed into sequences
called histories; the set of all valid histories is H, and the
set of all terminal histories (games) is Z ⊆ H. We use the
notation h(cid:48) (cid:118) h to mean that h(cid:48) is a preﬁx sequence or equal
to h. Given a nonterminal history h, the player function τ :
H \ Z → N determines who acts at h. The utility function
u : (N \ {c}) × Z → [umin, umax] ⊂ R assigns a payoff to
each player for each terminal history z ∈ Z.

The notion of a state in imperfect information games re-
quires groupings of histories: Ii for some player i ∈ N is
a partition of {h ∈ H | τ (h) = i} into parts I ∈ Ii such
that h, h(cid:48) ∈ I if player i cannot distinguish h from h(cid:48) given
the information known to player i at the two histories. We
call these information sets. For example, in Texas Hold’em
poker, for all I ∈ Ii, the (public) actions are the same for
all h, h(cid:48) ∈ I, and h only differs from h(cid:48) in cards dealt to the
opponents (actions chosen by chance). For convenience, we
refer to I(h) as the information state that contains h.

At any I, there is a subset of legal actions A(I) ⊆ A.
To choose actions, each player i uses a strategy σi : I →
∆(A(I)), where ∆(X) refers to the set of probability dis-
tributions over X. We use the shorthand σ(h, a) to refer to
σ(I(h), a). Given some history h, we deﬁne the reach prob-
ability πσ(h) = Πh(cid:48)a(cid:64)hστ (h(cid:48))(I(h(cid:48)), a) to be the product
of all action probabilities leading up to h. This reach prob-
ability contains all players’ actions, but can be separated
πσ(h) = πσ
−i(h) into player i’s actions’ contribution
and the contribution of the opponents’ of player i (including
chance).

i (h)πσ

Finally, it is often useful to consider the augmented in-
formation sets (Burch et al. 2014). While an information set
I groups histories h that player i = τ (h) cannot distinguish,

an augmented information set groups histories that player i
can not distinguish, including these where τ (h) (cid:54)= i. For a
history h, we denote an augmented information set of player
i as Ii(h). Note that the if τ (h) = i then Ii(h) = I(h) and
I(h) = Iτ (h)(h).

Counterfactual Regret Minimization
Counterfactual Regret (CFR) Minimization is an itera-
tive algorithm that produces a sequence of strategies
σ0, σ1, . . . , σT , whose average strategy ¯σT converges to an
approximate Nash equilibrium as T → ∞ in two-player
zero-sum games (Zinkevich et al. 2008). Speciﬁcally, on it-
eration t, for each I, it computes counterfactual values.
Deﬁne ZI = {(h, z) ∈ H × Z | h ∈ I, h (cid:118) z}, and
uσt
i (h, z) = πσt
(h, z)ui(z). We will also sometimes use the
short form uσ
i (h, z). A counterfactual
value is:

i (h) = (cid:80)

z∈Z,h(cid:118)z uσ

vi(σt, I) =

(cid:88)

−i(h)uσt
πσt

i (h, z).

(1)

(h,z)∈ZI

We also deﬁne an action-dependent counterfactual value,

vi(σ, I, a) =

(cid:88)

(h,z)∈ZI

−i(ha)uσ(ha, z),
πσ

(2)

where ha is the sequence h followed by the action a.
The values are analogous to the difference in Q-values
and V -values in RL, and indeed we have vi(σ, I) =
(cid:80)
a σ(I, a)vi(σ, I, a). CFR then computes a counterfac-
tual regret for not taking a at I:

rt(I, a) = vi(σt, I, a) − vi(σt, I),
(3)
This regret is then accumulated RT (I, a) = (cid:80)T
t=1 rt(I, a),
which is used to update the strategies using regret-
matching (Hart and Mas-Colell 2000):

σT +1(I, a) =

(RT (I, a))+
a∈A(I)(RT (I, a))+ ,

(cid:80)

(4)

where (x)+ = max(x, 0), or to the uniform strategy if
(cid:80)
a(RT (I, a))+ = 0. CFR+ works by thresholding the
quantity at each round (Tammelin et al. 2015): deﬁne
Q0(I, a) = 0 and QT (I, a) = (QT −1 + rT (I, a))+; CFR+
updates the policy by replacing RT by QT in equation 4. In
addition, it always alternates the regret updates of the play-
ers (whereas some variants of CFR update both players), and
the average strategy places more (linearly increasing) weight
on more recent iterations.

If for player i we denote u(σ) = ui(σi, σ−i), and run
CFR for T iterations, then we can deﬁne the overall regret
of the strategies produced as:

RT

i = max

σ(cid:48)
i

T
(cid:88)

t=1

(cid:0)vi(σ(cid:48)

i, σt

−i) − vi(σt)(cid:1) .

CFR ensures that RT
i /T → 0 as T → ∞. When two players
minimize regret, the folk theorem then guarantees a bound
on the distance to a Nash equilibrium as a function of RT
i /T .
To compute vi precisely, each iteration requires travers-
ing over subtrees under each a ∈ A(I) at each I. Next, we
describe variants that allow sampling parts of the trees and
using estimates of these quantities.

Monte Carlo CFR
Monte Carlo CFR (MCCFR) introduces sample estimates of
the counterfactual values, by visiting and updating quantities
over only part of the entire tree. MCCFR is a general fam-
ily of algorithms: each instance deﬁned by a speciﬁc sam-
pling policy. For ease of exposition and to show the sim-
ilarity to RL, we focus on outcome sampling (Lanctot et
al. 2009); however, our baseline-enhanced estimates can be
used in all MCCFR variants. A sampling policy ξ is deﬁned
in the same way as a strategy (a distribution over A(I) for
all I) with a restriction that ξ(h, a) > 0 for all histories and
actions. Given a terminal history sampled with probability
q(z) = πξ(z), a sampled counterfactual value ˜vi(σ, I|z)

= ˜vi(σ, h|z) =

−i(h)uσ
πσ
q(z)

i (h, z)

, for h ∈ I, h (cid:118) z,

(5)

and 0 for histories that were not played, h (cid:54)(cid:118) z. The estimate
is unbiased: Ez∼ξ[˜vi(σ, I|z)] = vi(σ, I), by (Lanctot et al.
2009, Lemma 1). As a result, ˜vi can be used in Equation 3
to accumulate estimated regrets ˜rt(I, a) = ˜vi(σt, I, a) −
˜vi(σt, I) instead. The regret bound requires an additional
minz∈Z q(z) , which is exponential in the length of z and
term
similar observations have been made in RL (Arjona-Medina
et al. 2018). The main problem with the sampling variants is
that they introduce variance that can have a signiﬁcant effect
on long-term convergence (Gibson et al. 2012).

1

(cid:80)n

Control Variates
Suppose one is trying to estimate a statistic of a ran-
dom variable, X, such as its mean, from samples X =
(X1, X2, · · · , Xn). A crude Monte Carlo estimator is de-
ﬁned to be ˆX mc = 1
i=1 Xi. A control variate is a
n
random variable Y with a known mean µY = E[Y ], that
is paired with the original variable, such that samples are
instead of the form (X, Y) (Owen 2013). A new random
variable is then deﬁned, Zi = Xi + c(Yi − µY ). An es-
timator ˆZ cv = 1
i=1 Zi. Since E[Zi] = E[Xi] for any
n
value of c, ˆZ cv can be used in place of ˆX mc. with variance
Var[Zi] = Var[Xi] + c2Var[Yi] + 2cCov[Xi, Yi]. So when
X and Y are positively correlated and c < 0, variance is
reduced when Cov[X, Y ] > c2
2

Var[Y ].

(cid:80)n

Reinforcement Learning Mapping
There are several analogies to make between Monte Carlo
CFR in imperfect information games and reinforcement
learning. Since our technique builds on ideas that have been
widely used in RL, we end the background by providing a
small discussion of the links.

First, dynamics of an imperfect information game are sim-
ilar to a partially-observable episodic MDP without any cy-
cles. Policies and strategies are identically deﬁned, but in
imperfect information games a deterministic optimal (Nash)
strategy may not exist causing most of the RL methods to fail
to converge. The search for a minmax-optimal strategy with
several players is the main reason CFR is used instead of, for
example, value iteration. However, both operate by deﬁning
values of states which are analogous (counterfactual values

versus expected values) since they are both functions of the
strategy/policy; therefore, can be viewed as a kind of policy
iteration which computes the values and from which a pol-
icy is derived. However, the iterates σt are not guaranteed to
converge to the optimal strategy, only the average strategy
¯σt does.

Monte Carlo CFR is an off-policy Monte Carlo analog.
The value estimates are unbiased speciﬁcally because they
are corrected by importance sampling. Most applications of
MCCFR have operated with tabular representations, but this
is mostly due to the differences in objectives. Function ap-
proximation methods have been proposed for CFR (Waugh
et al. 2015) but the variance from pure Monte Carlo methods
may prevent such techniques in MCCFR. The use of base-
lines has been widely successful in policy gradient methods,
so reducing the variance could enable the practical use of
function approximation in MCCFR.

Monte Carlo CFR with Baselines
We now introduce our technique: MCCFR with baselines.
While the baselines are analogous to those from policy gra-
dient methods (using counterfactual values), there are slight
differences in their construction.

Our technique constructs value estimates using control
variates. Note that MCCFR is using sampled estimates of
counterfactual values ˜vi(σ, I) whose expected value is the
counterfactual value vi(σ, I). First, we introduce an esti-
mated counterfactual value ˆvi(σ, I) to be any estimator of
the counterfactual value (not necessarily ˜vi as deﬁned above,
but this is one possibility).

We now deﬁne an action-dependent baseline bi(I, a) that,
as in RL, serves as a basis for the sampled values. The intent
is to deﬁne a baseline function to approximate or be cor-
related with E[ˆvi(σ, I, a)]. We also deﬁne a sampled base-
line ˆbi(I, a) as an estimator such that E[ˆbi(I, a)] = bi(I, a).
From this, we construct a new baseline-enhanced estimate
for the counterfactual values:

(6)

i (σ, I, a) = (cid:98)vi(σ, I, a) − ˆbi(σ, I, a) + bi(σ, I, a)
(cid:98)vb
First, note that ˆbi is a control variate with c = −1. Therefore,
it is important that ˆbi be correlated with ˆvi. The main idea
of our technique is to replace ˜vi(σ, I, a) with ˆvb
i (σ, I, a). A
key property is that by doing so, the expectation remains
unchanged.
Lemma 1. For any i ∈ N − {c}, σi, I ∈ I, a ∈ A(I), if
E[ˆbi(I, a)] = bi(I, a) and E[ˆvi(σ, I, a)] = vi(σ, I, a), then
E[ˆvb

i (σ, I, a)] = vi(σ, I, a).
The proof is in the appendix. As a result, any baseline
whose expectation is known can be used and the baseline-
enhanced estimates are consistent. However, not all base-
lines will decrease variance. For example, if Cov[ˆvi, ˆbi] is
too low, then the Var[ˆbi] term could dominate and actually
increase the variance.

Recursive Bootstrapping
Consider the individual computation (1) for all the informa-
tion sets on the path to a sampled terminal history z. Given

that the counterfactual values up the tree can be computed
from the counterfactual values down the tree, it is natu-
ral to consider propagating the already baseline-enhanced
counterfactual values (6) rather than the original noisy sam-
pled values - thus propagating the beneﬁts up the tree. The
Lemma (2) then shows that by doing so, the updates remain
unbiased. Our experimental section shows that such boot-
strapping a crucial component for the proper performance
of the method.

To properly formalize this bootstrapping computation, we

must ﬁrst recursively deﬁne the expected value:

ˆui(σ, h, a|z) =

(cid:26) ˆui(σ, ha|z)/ξ(h, a)

0

if ha (cid:118) z
otherwise

,

(7)

and

ˆui(σ, h|z) =

(cid:40) ui(h)
(cid:80)
0

a σ(h, a)ˆui(σ, h, a|z)

if h = z
if h (cid:64) z
otherwise

.

(8)

Next, we deﬁne a baseline-enhanced version of the ex-
pected value. Note that the baseline bi(I, a) can be arbitrary,
but we discuss a particular choice and update of the base-
line in the later section. For every action, given a speciﬁc
sampled trajectory z, then ˆub

i (σ, h, a|z) =






bi(Ii(h), a) + ˆub
bi(Ii(h), a)
0

i (σ,ha|z)−bi(Ii(h),a)
ξ(h,a)

if ha (cid:118) z
if h (cid:64) z, ha (cid:54)(cid:118) z
otherwise

(9)

and

ˆub
i (σ, h|z) =






ui(h)
(cid:80)
0

a σ(h, a)ˆub

i (σ, h, a|z)

if h = z
if h (cid:64) z
otherwise

.

(10)

These are the values that are bootstrapped. We estimate
counterfactual values needed for the regret updates using
these values as:

i (σ, I(h), a|z) = ˆvb
ˆvb

i (σ, h, a|z) =

πσ
−i(h)
q(h)

ˆub
i (σ, h, a|z).

(11)

We can now formally state that the bootstrapping keeps

the counterfactual values unbiased:
Lemma 2. Let ˆvb
i be deﬁned as in Equation 11. Then, for
any i ∈ N − {c}, σi, I ∈ I, a ∈ A(I), it holds that
Ez[ˆvb

i (σ, I, a|z)] = vi(σ, I, a).

The proof is in the appendix. Since each estimate builds
on other estimates, the beneﬁt of the reduction in variance
can be propagated up through the tree.

Another key result is that there exists a perfect baseline
that leads to zero-variance estimates at the updated informa-
tion sets.

(a) CFR

(b) MCCFR

(c) VR-MCCFR

Figure 2: Values and updates for the discussed methods: (a) CFR udpates the full tree and thus uses the exact values for all the
actions, (b) MCCFR updates only a single path, and uses the sampled values for the sampled actions and zero values for the
off-trajectory actions, (c) VR-MCCFR also updates only a single path, but uses the bootstrapped baseline-enhanced values for
the sampled actions and baseline-enhanced values for the off-trajectory actions.

Lemma 3. There exists a perfect baseline b∗ and optimal
unbiased estimator ˆv∗
i (σ, h, a) such that under a speciﬁc up-
date scheme: Varh,z∼ξ,h∈I,h(cid:118)z[ˆv∗

i (σ, h, a|z)] = 0.

The proof and description of the update scheme are in
the appendix. We will refer to b∗ as the oracle baseline.
Note that even when using the oracle baseline, the conver-
gence rate of MCCFR is still not identical to CFR because
each iteration applies regret updates to a portion of the tree,
whereas CFR updates the entire tree.

Finally, using unbiased estimates to tabulate regrets
ˆr(I, a) for each I and a leads to a probabilistic regret bound:
Theorem 1. (Gibson et al. 2012, Theorem 2) For some un-
biased estimator of the counterfactual values ˆvi and a bound
on the difference in its value ˆ∆i = |ˆvi(σ, I, a)−ˆvi(σ, I, a(cid:48))|,
with probability 1-p, RT

i
T

(cid:32)

≤

ˆ∆i +

(cid:112)maxt,I,a Var[rt
i(I, a) − ˆrt
√
p

i(I, a)]

(cid:33)

|Ii||Ai|
√
T

.

Choice of Baselines
How does one choose a baseline, given that we want these to
be good estimates of the individual counterfactual values? A
common choice of the baseline in policy gradient algorithms
is the mean value of the state, which is learned online (Mnih
et al. 2016). Inspired by this, we choose a similar quantity:
the average expected value ¯ˆui(Ii, a). That is, in addition to
accumulating regret for each I, average expected values are
also tracked.

While a direct average can be tracked, we found that an
exponentially-decaying average that places heavier weight
on more recent samples to be more effective in practice. On
the kth visit to I at iteration t,
(cid:26) 0

¯ˆuk
i (Ii, a) =

(1 − α)¯ˆuk−1

i (σt, Ii, a)
i
We then deﬁne the baseline bi(Ii, a) = ¯ˆui(Ii, a), and
(cid:26) bi(Ii, a)/ξ(Ii, a)

(Ii, a) + αˆub

ˆbi(Ii, a|z) =

if ha (cid:118) z, h ∈ Ii
otherwise.

0

if k = 0
if k > 0

The baseline can therefore be thought as local to Ii since it
depends only on quantities deﬁned and tracked at Ii. Note
that Ea∼ξ(Ii)[ˆbi(Ii, a|z)] = bi(Ii, a) as required.

Summary of the Full Algorithm
We now summarize the technique developed above. One it-
eration of the algorithm consists of:

1. Repeat the steps below for each i ∈ N − {c}.
2. Sample a trajectory z ∼ ξ.
3. For each history h (cid:118) z in reverse order (longest ﬁrst):

(a) If h is terminal, simply return ui(h)
(b) Obtain current strategy σ(I) from Eq. 4 using cumula-

tive regrets R(I, a) where h ∈ I.

(c) Use the child value ˆub

i (σ, ha) to compute ˆub

i (σ, h) as in

Eq. 9.

i (σ, I).

i (σ, I, a) − ˆvb

(d) If τ (h) = i then for a ∈ A(I), compute ˆvb

i (σ, I, a) =
i (σ, ha) and accumulate regrets R(I, a) ←

π−i(h)
q(h) ˆub
R(I, a) + ˆvb
(e) Update ¯ˆu(σ, Ii, a).
(f) Finally, return ˆub
Note that the original outcome sampling is an instance
of this algorithm. Speciﬁcally, when bi(Ii, a) = 0, then
ˆvb
i (σ, I, a) = ˜vi(σ, I, a). Step by step example of the com-
putation is in the appendix.

i (σ, h).

Experimental Results
We evaluate the performance of our method on Leduc
poker (Southey et al. 2005), a commonly used benchmark
poker game. Players have an unlimited number of chips,
and the deck has six cards, divided into two suits of three
identically-ranked cards. There are two rounds of betting;
after the ﬁrst round a single public card is revealed from the
deck. Each player antes 1 chip to play, receiving one private
card. There are at most two bet or raise actions per round,
with a ﬁxed size of 2 chips in the ﬁrst round, and 4 chips in
the second round.

For the experiments, we use a vectorized form of CFR
that applies regret updates to each information set consistent
with the public information. The ﬁrst vector variants were
introduced in (Johanson et al. 2012), and have been used
in DeepStack and Libratus (Moravˇc´ık et al. 2017; Brown
and Sandholm 2017). See the appendix for more detail on
the implementation. Baseline average values ¯ˆub
i (I, a) used a

decay factor of α = 0.5. We used a uniform sampling in all
our experiments, ξ(I, a) = 1
|A(I)| .
We also consider the best case performance of our algo-
rithm by using the oracle baseline. It uses baseline values of
the true counterfactual values. We also experiment with and
without CFR+, demonstrating that our technique allows the
CFR+ to be for the ﬁrst time efﬁciently used with sampling.

Convergence

We compared MCCFR, MCCFR+, VR-MCCFR, VR-
MCCFR+, and VR-MCCFR+ with the oracle baseline,
see Fig. 3. The variance-reduced VR-MCCFR and VR-
MCCFR+ variants converge signiﬁcantly faster than plain
MCCFR. Moreover, the speedup grows as the baseline im-
proves during the computation. A similar trend is shown by
both VR-MCCFR and VR-MCCFR+, see Fig. 4. MCCFR
needs hundreds of millions of iterations to reach the same
exploitability as VR-MCCFR+ achieves in one million iter-
ations: a 250-times speedup. VR-MCCFR+ with the oracle
baseline signiﬁcantly outperforms VR-MCCFR+ at the start
of the computation, but as time progresses and the learned
baseline improves, the difference shrinks. After one mil-
lion iterations, exploitability of VR-MCCFR+ with a learned
baseline approaches the exploitability of VR-MCCFR+ with
the oracle baseline. This oracle baseline result gives a bound
on the gains we can get by constructing better learned base-
lines.

Observed Variance

To verify that the observed speedup of the technique is due to
variance reduction, we experimentally observed variance of
counterfactual value estimates for MCCFR+ and MCCFR,
see Fig. 5. We did that by sampling 1000 alternative trajecto-
ries for all visited information sets, with each trajectory sam-
pling a different estimate of the counterfactual value. While
the variance of value estimates in the plain algorithm seems
to be more or less constant, the variance of VR-MCCFR and
VR-MCCFR+ value estimates is lower, and continues to de-
crease as more iterations are run. This conﬁrms that the com-
bination of baseline and bootstrapping is reducing variance,
which implies better performance given the connection be-
tween variance and MCCFR’s performance (Theorem 1).

Evaluation of Bootstrapping and Baseline
Dependence on Actions

Recent work that evaluates action-dependent baselines in RL
(Tucker et al. 2018), shows that there is often no real advan-
tage compared to baselines that depend just on the state. It
is also not common to bootstrap the value estimates in RL.
Since VR-MCCFR uses both of these techniques it is nat-
ural to explore the contribution of each idea. We compared
four VR-MCCFR+ variants: with or without bootstrapping
and with baseline that is state or state-action dependant, see
Fig. 6. The conclusion is that the improvement in the perfor-
mance is very small unless we use both bootstrapping and
an action-dependant baseline.

Figure 3: Convergence of exploitability for different MC-
CFR variants on logarithmic scale. VR-MCCFR converges
substantially faster than plain MCCFR. VR-MCCFR+ bring
roughly two orders of magnitude speedup. VR-MCCFR+
with oracle baseline (actual true values are used as base-
lines) is used as a bound for VR-MCCFR’s performace to
show possible room for improvement. When run for 106 it-
erations VR-MCCFR+ approaches performance of the ora-
cle version. The ribbons show 5th and 95th percentile over
100 runs.

Figure 4: Speedup of VR-MCCFR and VR-MCCFR+ com-
pared to plain MCCFR. Y-axis show how many times more
iterations are required by MCCFR to reach the same ex-
ploitability as VR-MCCFR or VR-MCCFR+.

Conclusions
We have presented a new technique for variance reduction
for Monte Carlo counterfactual regret minimization. This
technique has close connections to existing RL methods of

104105106Iterations10-310-210-1100101ExploitabilityLeduc pokerMCCFRMCCFR+VR-MCCFR α=0.5VR-MCCFR+ α=0.5VR-MCCFR+ (oracle baseline)02000004000006000008000001000000Iterations of VR-MCCFR/VR-MCCFR+100101102103Speedup versus MCCFRSpeedup caused by variance reductionVR-MCCFR+ α=0.5VR-MCCFR α=0.5deed reduced, speeding up the convergence by an order of
magnitude. The decreased variance allows for the ﬁrst time
CFR+ to be used with sampling, bringing the speedup to two
orders of magnitude.

References

A.

[Arjona-Medina et al. 2018] Jose
Arjona-Medina,
Michael Gillhofer, Michael Widrich, Thomas Unterthiner,
and Sepp Hochreiter. Rudder: Return decomposition for
delayed rewards. CoRR, abs/1806.07857, 2018.
[Bansal et al. 2018] Trapit Bansal, Jakub Pachocki, Szymon
Sidor, Ilya Sutskever, and Igor Mordatch. Emergent com-
In Proceedings of
plexity via multi-agent competition.
the Sixth International Conference on Learning Represen-
tations, 2018.
[Bowling et al. 2015] Michael Bowling, Neil Burch,
Michael Johanson, and Oskari Tammelin. Heads-up Limit
Science, 347(6218):145–149,
Hold’em Poker is solved.
January 2015.
[Boyle 1977] Phelim P Boyle. Options: A monte carlo ap-
proach. Journal of ﬁnancial economics, 4(3):323–338, 1977.
[Brown and Sandholm 2017] Noam Brown and Tuomas
Sandholm. Superhuman AI for heads-up no-limit poker: Li-
bratus beats top professionals. Science, 360(6385), Decem-
ber 2017.
[Burch et al. 2014] Neil Burch, Michael
Johanson, and
Michael Bowling. Solving imperfect information games us-
In Proceedings of the Twenty-Eighth
ing decomposition.
AAAI Conference on Artiﬁcial Intelligence (AAAI), 2014.
[Burch et al. 2018] Neil Burch, Martin Schmid, Matej
Moravcik, Dustin Morill, and Michael Bowling. Aivat: A
new variance reduction technique for agent evaluation in im-
perfect information games, 2018.
[Burch 2017] Neil Burch. Time and Space: Why Imperfect
Information Games are Hard. PhD thesis, University of Al-
berta, 2017.
[Foerster et al. 2017] Jakob N. Foerster, Richard Y. Chen,
Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and
Igor Mordatch. Learning with opponent-learning aware-
ness. In Proceedings of the International Conference on Au-
tonomous Agents and Multiagent Systems (AAMAS), 2017.
[Gibson et al. 2012] Richard Gibson, Marc Lanctot, Neil
Burch, Duane Szafron, and Michael Bowling. Generalized
sampling and variance in counterfactual regret minimiza-
tion. In Proceedings of the Twenty-Sixth Conference on Ar-
tiﬁcial Intelligence (AAAI-12)., pages 1355–1361, 2012.
[Hart and Mas-Colell 2000] S. Hart and A. Mas-Colell. A
simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127–1150, 2000.
[Heinrich et al. 2015] Johannes Heinrich, Marc Lanctot, and
David Silver. Fictitious self-play in extensive-form games.
In Proceedings of the 32nd International Conference on Ma-
chine Learning (ICML 2015), 2015.
[Johanson et al. 2011] Michael Johanson, Michael Bowling,
Kevin Waugh, and Martin Zinkevich. Accelerating best re-
sponse calculation in large extensive games. In Proceedings

Figure 5: Variance of counterfactual values in VR-MCCFR
and plain MCCFR with both regret matching and regret
matching+. The curves were smoothed by computing mov-
ing average over a sliding window of 100 iterations.

Figure 6: Detailed comparison of different VR-MCCFR
variants on logarithmic scale. The curves for MCCFR, VR-
MCCFR and VR-MCCFR+ are the same as in the previous
plot, the other lines show how the algorithm performs when
using state baselines instead of state-action baselines, and
without bootstrapping. All of these reduced variants perform
better than plain MCCFR, however they are worse than full
VR-MCCFR. This ablation study shows that the combina-
tion of all VR-MCCFR features is important for ﬁnal per-
formance.

state and state-action baselines. In contrast to RL environ-
ments, our experiments in imperfect information games sug-
gest that state-action baselines are superior to state baselines.
Using this technique, we show that empirical variance is in-

102103104105106Iterations10-510-410-310-210-1100101VarianceLeduc pokerMCCFRMCCFR+VR-MCCFR α=0.5VR-MCCFR+ α=0.5104105106Iterations10-310-210-1100101ExploitabilityLeduc pokerMCCFRVR-MCCFR+ avg, w/o boot.VR-MCCFR+ avg, state base.VR-MCCFR+ avg, w/o boot., state base.VR-MCCFR α=0.5VR-MCCFR+ α=0.5Shoham

arXiv preprint

texas hold’em.

imal policy optimization algorithms.
arXiv:1707.06347, 2017.
and
[Shoham and Leyton-Brown 2009] Y.
Multiagent Systems: Algorithmic,
K. Leyton-Brown.
Game-Theoretic, and Logical Foundations. Cambridge
University Press, 2009.
[Southey et al. 2005] Finnegan Southey, Michael H. Bowl-
ing, Bryce Larson, Carmelo Piccione, Neil Burch, Darse
Billings, and D. Chris Rayner. Bayes’ bluff: Opponent mod-
elling in poker. In UAI ’05, Proceedings of the 21st Con-
ference in Uncertainty in Artiﬁcial Intelligence, pages 550–
558, 2005.
[Sutton and Barto 2017] R. Sutton and A. Barto. Reinforce-
ment Learning: An Introduction. MIT Press, 2nd edition,
2017. Draft, in progress.
[Tammelin et al. 2015] Oskari Tammelin, Neil Burch,
Michael Johanson, and Michael Bowling. Solving heads-up
the 24th In-
In Proceedings of
limit
ternational Joint Conference on Artiﬁcial
Intelligence,
2015.
[Tucker et al. 2018] George Tucker, Surya Bhupatiraju,
Shixiang Gu, Richard E Turner, Zoubin Ghahramani, and
Sergey Levine. The mirage of action-dependent baselines in
reinforcement learning. arXiv preprint arXiv:1802.10031,
2018.
[Veness et al. 2011] Joel Veness, Marc Lanctot, and Michael
Bowling. Variance reduction in Monte-Carlo tree search.
In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F. Pereira, and
K.Q. Weinberger, editors, Advances in Neural Information
Processing Systems 24, pages 1836–1844, 2011.
[Waugh et al. 2015] Kevin Waugh, Dustin Morrill, J. An-
drew Bagnell, and Michael Bowling. Solving games with
In Proceedongs of the AAAI
functional regret estimation.
Conference on Artiﬁcial Intelligence, 2015.
[Williams 1992] R.J. Williams. Simple statistical gradient-
following algorithms for connectionist reinforcement learn-
ing. Machine Learning, 8(3):229–256, 1992.
[Wu et al. 2018] Cathy Wu, Aravind Rajeswaran, Yan Duan,
Vikash Kumar, Alexandre M. Bayen, Sham Kakade, Igor
Mordatch, and Pieter Abbeel. Variance reduction for policy
gradient with action-dependent factorized baselines. CoRR,
abs/1803.07246, 2018.
[Zinkevich et al. 2008] M.
Johanson,
M. Bowling, and C. Piccione. Regret minimization in
games with incomplete information. In Advances in Neural
Information Processing Systems 20 (NIPS 2007), 2008.

Zinkevich, M.

of the Twenty-Second International Joint Conference on Ar-
tiﬁcial Intelligence (IJCAI), pages 258–265, 2011.
[Johanson et al. 2012] Michael Johanson, Nolan Bard, Marc
Lanctot, Richard Gibson, and Michael Bowling.
Efﬁ-
cient nash equilibrium approximation through Monte Carlo
In Proceedings of the
counterfactual regret minimization.
Eleventh International Conference on Autonomous Agents
and Multi-Agent Systems (AAMAS), 2012.
[Kuhn poker 2018] Kuhn poker. Kuhn poker — Wikipedia,
the free encyclopedia, 2018. [Online; accessed 28-August-
2018].
[Lanctot et al. 2009] Marc Lanctot, Kevin Waugh, Martin
Zinkevich, and Michael Bowling. Monte Carlo sampling
for regret minimization in extensive games. In Y. Bengio,
D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Cu-
lotta, editors, Advances in Neural Information Processing
Systems 22, pages 1078–1086, 2009.
[Lanctot et al. 2017] Marc Lanctot, Vinicius Zambaldi, Au-
drunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Per-
olat, David Silver, and Thore Graepel. A uniﬁed game-
theoretic approach to multiagent reinforcement learning. In
Advances in Neural Information Processing Systems, 2017.
[Lanctot 2013] Marc Lanctot. Monte Carlo Sampling and
Regret Minimization for Equilibrium Computation and
Decision-Making in Large Extensive Form Games. PhD the-
sis, University of Alberta, University of Alberta, Computing
Science, 116 St. and 85 Ave., Edmonton, Alberta T6G 2R3,
June 2013.
[Littman 1994] Michael L. Littman. Markov games as a
In In
framework for multi-agent reinforcement learning.
Proceedings of the Eleventh International Conference on
Machine Learning, pages 157–163. Morgan Kaufmann,
1994.
[Liu et al. 2018] Hao Liu, Yihao Feng, Yi Mao, Dengyong
Zhou, Jian Peng, and Qiang Liu. Action-dependent control
variates for policy optimization via stein identity. 2018.
[Mnih et al. 2016] Volodymyr Mnih, Adri`a Puigdom`enech
Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asyn-
chronous methods for deep reinforcement learning. In Pro-
ceedings of the 33rd International Conference on Machine
Learning (ICML), pages 1928–1937, 2016.
[Moravˇc´ık et al. 2017] Matej Moravˇc´ık, Martin Schmid,
Neil Burch, Viliam Lis´y, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowl-
ing. Deepstack: Expert-level artiﬁcial intelligence in heads-
up no-limit poker. Science, 358(6362), October 2017.
[Owen 2013] Art B. Owen. Monte Carlo theory, methods
and examples. 2013.
[Pepels et al. 2014] Tom Pepels, Mandy J.W. Tak, Marc
Lanctot, and Mark H.M. Winands. Quality-based rewards
In Proceedings
for Monte-Carlo tree search simulations.
of the 21st European Conference on Artiﬁcial Intelligence
(ECAI), 2014.
[Schulman et al. 2017] John Schulman, Filip Wolski, Pra-
fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox-

Appendices

MCCFR and MCCFR+ comparison
While it is known in that MCCFR+ is outperformed by MC-
CFR (Burch 2017), we are not aware on any explicit com-
parison of these two algorithms in literature. Fig. 7 shows
experimental evaluation of these two techniques on Leduc
poker.

Figure 7: Convergence of MCCFR and MCCFR+ on log-
arithmic scale. For the ﬁrst 106 iterations, MCCFR+ per-
forms similllary to the MCCFR. After approximately 107
iterations, the difference in favor of MCCFR starts to be vis-
ible and the gap in exploitability widens as the number of
iterations grows.

Vector Form of CFR
The ﬁrst appearance of the vector form was presented in (Jo-
hanson et al. 2011). In this paper, the best response compu-
tation, needed to compute exploitability, was sped-up by re-
deﬁning the computation using the notion of a public tree.
At the heart of a public tree is the notion of a public state
which contains a set of information sets whose histories are
consistent with the public information revealed so far (Jo-
hanson et al. 2011, Deﬁnition 2). This allowed the method
to compute quantities for all information sets consistent with
a public state at once (stored in vectors) and operations to
compute them could be vectorized during a traversal of the
public tree. There are also game-speciﬁc optimizations that
could be applied at leaf nodes to asymptotically reduce the
total computation necessary.

A similar construction was used in several sampling vari-
ants introduced in (Johanson et al. 2012). Here, instead of
computing necessary for best response, counterfactual val-
ues were vectorized and stored instead. The paper describes
several ways to sample at various types of chance nodes
(ones which reveal public information, or private informa-
tion to each player), but the concept of a vectorized form of
CFR was general. In fact, a vector form of vanilla CFR is
possible in any game: when traversing down the tree, these
vectors store the probability of reaching each information set
(called a range in (Moravˇc´ık et al. 2017)) and return vectors

of counterfactual values. Both DeepStack and Libratus used
vector forms of CFR and CFR+ in No-Limit poker.

For the MCCFR variants in this paper, the idea is the same
as the previous sample variants. For any sequence of public
actions, we concurrently maintain and update all informa-
tion sets consistent with the sequence of public actions. For
example in Leduc poker, six trajectories per player are main-
tained which all share the same sequence of public actions.

The main difference in our implementation is that base-
lines are kept as vectors at each public state, each repre-
senting a baseline for the information sets corresponding
to the public state. Also, the average values tracked are
counterfactual and normalized by the range. So, for exam-
ple in Leduc, for ﬁve information sets in some public state,
(I1, I2, . . . , I5), quantity tracked by the baseline at this pub-
lic state for action a is:

ˆvb
i (σ, Ik, a)
opp(I opp
k(cid:48) πσ
k(cid:48) )

(cid:80)

,

where πσ
opp is the reach probability of the opponent only
(excluding chance), and I opp refers to the augmented in-
formation set belonging to the opponent at I. Then, when
using the baseline values to compute the modiﬁed coun-
terfactual values, we need to multiply them by the current
(cid:80)
k(cid:48) πσ
k(cid:48) ) to get the baseline values under the current
strategy σ.

opp(I opp

Proofs

Proof of Lemma 1

E[ˆvb(σ, I, a)] = E[ˆvi(σ, I, a)] − E[ˆbi(I, a)] + E[bi(I, a)]

= vi(σ, I, a) − bi(I, a) + bi(I, a)
= vi(σ, I, a).

Proof of Lemma 2

We begin by proving a few supporting lemmas regarding lo-
cal expectations over actions at speciﬁc histories:

Lemma 4. Given some h ∈ H, for any z ∈ Z gen-
erated by sampling ξ
: H (cid:55)→ A and all actions a,
Ez∼ξ[ˆub
z,ha(cid:118)z q(z)ˆub

i (σ, h, a|z)] = (cid:80)

i (σ, ha|z)/ξ(h, a):

Proof. ˆub

i (σ, h, a|z) has three cases, from which we get

1041051061071081091010Iterations10-410-310-210-1100101ExploitabilityConvergence on Leduc pokerMCCFR+MCCFREz∼ξ[ˆub

i (σ, h, a|z)]
(cid:88)

q(z)

=

+

+

=

z,ha(cid:118)z
(cid:18)

bi(Ii(h), a) +

−bi(Ii(h), a) + ˆub
ξ(h, a)

i (σ, ha|z)

(cid:19)

q(z)(bi(Ii(h), a))

(cid:88)

z,h(cid:64)z,ha(cid:54)(cid:118)z
(cid:88)

0

h(cid:54)(cid:118)z

(cid:88)

z,ha(cid:118)z

q(z)ˆub

i (σ, ha|z)/ξ(h, a)

i (σ, h, a|z)]
(cid:88)

=

+ (q(ha) − q(ha)/ξ(h, a))bi(Ii(h), a)
+ q(h)(1 − ξ(h, a))bi(Ii(h), a)

(cid:88)

=

z,ha(cid:118)z

q(z)ˆub

i (σ, ha|z)/ξ(h, a)

Lemma 5. Given some h ∈ H, for any z ∈ Z generated by
sampling ξ : H (cid:55)→ A, the local baseline-enhanced estimate
is an unbiased estimate of expected values for all actions a:

Ez∼ξ[ˆub

i (σ, h, a|z)] = Ez∼ξ[ˆui(σ, h, a|z)].

Proof. We prove this by induction on the maximum dis-
tance from ha to any terminal. The base case is ha ∈ Z.
Ez∼ξ[ˆub

q(z)ˆub

i (σ, ha|z)/ξ(h, a) by Lemma 4

=

z,ha(cid:118)z
(cid:88)

z,ha(cid:118)z

q(z)ˆui(σ, ha|z)/ξ(h, a) by Eq. 10

= Ez∼ξ[ˆui(σ, h, a|z)] by Eq. 7, 8

Now assume for i ≥ 0 that the lemma property holds
for all h(cid:48)a(cid:48) that are at most j ≤ i steps from a terminal.
Consider history ha being i + 1 steps from some terminal,
which implies that ha (cid:54)∈ Z. We have Ez∼ξ[ˆub

i (σ, h, a|z)]

=

=

=

=

q(z)ˆub

i (σ, ha|z)/ξ(h, a) by Lemma 4

q(z)

σ(ha, a(cid:48))ˆub

i (σ, ha, a(cid:48)|z)/ξ(h, a)

(cid:88)

a(cid:48)

(cid:88)

z,ha(cid:118)z
(cid:88)

z,ha(cid:118)z
by Eq. 10

(cid:88)

(cid:88)

q(z)

a(cid:48)
z,ha(cid:118)z
by assumption

σ(ha, a(cid:48))ˆui(σ, ha, a(cid:48)|z)/ξ(h, a)

(cid:88)

z,ha(cid:118)z

q(z)ˆui(σ, ha|z)/ξ(h, a) by Eq. 8

= Ez∼ξ[ˆui(σ, h, a|z)]

by Eq. 7

The lemma property holds for distance i + 1, and so by in-
duction the property holds for all h and a.

Lemma 6. Given some h ∈ H, for any z ∈ Z gen-
erated by sampling ξ
: H (cid:55)→ A and for all ac-
tions a, the local baseline-enhanced estimate is an unbi-
ased estimate of the original sampled counterfactual value:
Ez∼ξ[ˆvb
i (σ, Ii(h), a|z)] = Ez∼ξ[˜vi(σ, Ii(h), a|z)].
Proof. First, Ez∼ξ[ˆvb
i (σ, Ii(h), a|z)]
(cid:20) πσ
−i(h)
q(h)

(cid:21)
ˆub
i (σ, h, a|z)

= Ez∼ξ

by Eq. 11

=

=

πσ
−i(h)
q(h)
πσ
−i(h)
q(h)

Ez∼ξ[ˆub

i (σ, h, a|z)]

Ez∼ξ[ˆui(σ, h, a|z)] by Lemma 5

= Ez∼ξ[˜vi(σ, Ii(h), a|z)] by Eq. 5, 7.

Proof of Lemma 2. The proof now follows directly:
Ez∼ξ[ˆvb

i (σ, I, a|z)]

= Ez∼ξ[˜vi(σ, I, a|z)] by Lemma 6
= vi(σ, I, a) by (Lanctot et al. 2009, Lemma 1).

Proof of Lemma 3
We start by proving that given an oracle baseline,
the
baseline-enhanced expected value is always equal to the true
expected value, and therefore has zero variance.
Lemma 7. Using an oracle baseline deﬁned over histo-
ries, b∗
i (h, a) = uσ
i (ha), then for all z such that h (cid:118) z,
ˆub∗
i (σ, h, a|z) = uσ
i (ha).
Proof. Similar to above, we prove this by induction on the
maximum distance from ha to z. The base case is ha ∈ Z.
By assumption h (cid:118) z so we have ˆub∗
i (h, a) + ˆub∗
i (σ,ha|z)−b∗
b∗
b∗
i (h, a)
by Eq. 9
(cid:26) uσ
i (ha) + uσ
uσ
i (ha)

if ha = z
otherwise

if ha = z
otherwise

i (ha)−uσ
ξ(h,a)

i (σ, h, a|z)

i (h,a)

i (ha)

ξ(h,a)

(cid:40)

=

=

by Eq. 10 and deﬁnition of b∗
i (ha)

= uσ

i (h, a)

Now assume for i ≥ 0 that the lemma property holds
for all h(cid:48)a(cid:48) that are at most j ≤ i steps from a terminal.
Consider history ha being i + 1 steps from some terminal,
which implies ha (cid:54)∈ Z. We have
ˆub∗
i (σ, ha|z) = uσ

i (ha)

(12)

because ˆub∗
=

i (σ, ha|z)
(cid:88)

σ(ha, a(cid:48))ˆub∗

i (σ, ha, a(cid:48)|z) by Eq. 10

a(cid:48)
(cid:88)

=

σ(ha, a(cid:48))uσ

i (haa(cid:48)) by assumption

a(cid:48)
= uσ
i (ha) by deﬁnition of uσ
i

=

=

We now look at ˆub∗
(cid:40)

i (σ, h, a|z)

i (ha) + ˆub∗
uσ
uσ
i (ha)

i (σ,ha|z)−uσ

i (ha)

ξ(h,a)

if ha (cid:64) z
otherwise

i (ha)−uσ
ξ(h,a)

by Eq. 9 and deﬁnition of b∗
(cid:26) uσ
i (ha) + uσ
i (ha)
uσ
i (ha)
by Eq. 12
i (ha)

= uσ

i (h, a)

if ha (cid:64) z
otherwise

The lemma property holds for distance i + 1, and so by in-
duction the property holds for all h and a.

Proof of Lemma 3. Given z such that h (cid:118) z, we have
ˆv∗
i (σ, h, a|z)

=

=

πσ
−i(h)
q(h)
πσ
−i(h)
q(h)

ˆub∗
i (σ, h, a|z) by Eq. 11

uσ
i (ha) by Lemma 7

None of the terms above depend on z, and so we have
Varh,z∼ξ,h∈I,h(cid:118)z[ˆv∗
i (σ, h, a|z)] = 0. Note as well that
−i(h)uσ
πσ
i (ha) corresponds to the terms in the summation
of Equation 2, so abusing notation, we have ˆv∗
i (σ, h, a|z) =
vi(σ, h, a)/q(h): the counterfactual value of taking action a
at h, with an importance sampling weight to correct for the
likelihood of reaching h.

In MCCFR, the optimal baseline b∗ is not known, as it
would require traversing the entire tree, taking away any
advantages of sampling. However, b∗ can be approximated
(learned online), which motivates the choice for tracking its
average value presented in the main part of the paper.

Kuhn Example
In this section, we present a step-by-step example of one
iteration of the algorithm on Kuhn poker (Kuhn poker 2018).
Kuhn poker is a simpliﬁed version of poker with three cards
and is therefore suitable for demonstration purposes. Table 1
show forward pass of VR-MCCFR algorithm, Table 2 shows
backward pass.

h
History

Game tree trajectory

Forward pass
πσ
−1(h)
Reach prob.

q(h)
Sampling prob.

I1 = I1(h)
Infoset for Pl1

I2 = I2(h)
Infoset for Pl2

∅

K

KQ

KQB

KQBC

1

1
3

1
6

1
6

1
24

1

1
3

1
6

1
12

1
24

∅

K

∅

?

K?

?Q

K?B

?QB

K?BC

?QBC

Table 1: Detailed example of updates computed for player 1 in Kuhn poker during forward pass of the algorithm. Backward pass
that uses these values is shown in Table 2. In our representation history h is a concatenation of all public and private actions.
The game tree trajectory column shows the path in the game tree that was sampled. Solid arrows denote sampled actions while
dashed arrows show other available actions, all actions have their probability under current strategy σ next to them. The sampled
history in this case is: chance deals (K)ing to player 1, chance deals (Q)ueen to player 2, player 1 (B)ets, player 2 (C)alls. We
will use shorter notation KQBC to refer to this history. For each history h reach probability πσ
−1(h) shows how likely the
history is reached when player 1 plays in a way to get to this history. The sampling probabilities q(h) are computed following
sampling policy ξ which is uniform in this case, i.e. for each history all available actions have the same probability that they
will be sampled. The last two columns show augmented information sets for each player in each history. For example for player
1 history KQB is represented by information set K?B since he does not know what card was dealt to PLAYER 2. Light gray
background marks cells where the values are well deﬁned however they are not used in our example update for player 1.

Forward passC (J)ack ⅓(K)ing⅓(Q)ueen⅓ C(J)ack½(Q)ueen½1(C)heck⅓b=-1(B)et⅔b=+0.52(F)old¾b=-2(C)all¼b=+1+2 h
History

Def.

Game tree trajectory

ˆu1

b(σ, h, a|z)
Sampled corrected history-action utility
Eq. 9

ˆu1

b(σ, h|z)
Sampled corrected history utility
Eq. 10

ˆv1

b(σ, I1, a|z)
Sampled corrected cf-value
Eq. 11

Backward pass

∅

K

KQ

KQB

KQBC

ˆu1

b(σ, h, B|z) =

ˆu1

b(σ,hB|z)−b(I1,B)
ξ(h,B)

+ b(I1,B)

+ 0.5

= − 3

4 −0.5
1
2
= −2
b(σ,h,C|z) = b(I1,C)
= −1

ˆu1

ˆu1

b(σ,h,C|z) =

ˆu1

b(σ,hC|z)−b(I1,c)
ξ(h,C)

+ b(I1,C)

+ 1

= 2−1
1
2
= 3
b(σ,h,F|z) = b(I1,F )
= −2

ˆu1

(cid:80)
= 1

1(σ, h|z) =

ˆub
a σ(h, a)ˆub
3 ∗ (−1) + 2
= − 5
3

1(σ, h, a|z)
3 ∗ (−2)

ˆvb
1 (σ, I1, B|z) =
πσ
−1(h)
q(h) ˆub
1(σ, h, B|z)
=
∗ (−2)

1
6
1
6
= −2
ˆvb
1 (σ, I1, C|z) =
πσ
−1(h)
q(h) ˆub
1(σ, h, C|z)
∗ (−1)
=

1
6
1
6
= −1

(cid:80)

1(σ, h|z) =

ˆub
a σ(h, a)ˆub
= 3

4 ∗ (−2) + 1
= − 3
4

1(σ, h, a|z)
4 ∗ 3

ˆu1

b(σ,h,|z) = u1(h)
= 2

b(σ, KQBC|KQBC) = +2 since player
Table 2: The backward pass starts by evaluating utility of the terminal history: ˆu1
1 has (K)ing which is better card than opponent’s (Q)ueen. In the next step computation updates values for history KQB.
b(σ, KQB, Call|KQBC) is computed based on current sample and then
Expected baseline corrected history-action value ˆu1
b(σ, KQB|KQBC). When updating values for history KQ
b(σ, KQB, F old|KQBC) to compute ˆu1
used together with ˆu1
b(σ, KQ, Bet|KQBC) for the sampled
baseline corrected sampled counterfactual values are computed based on just updated ˆu1
b(σ, KQ, Check|KQBC) for Check action that was not sampled. Reach probability
Bet action and on a baseline value ˆu1
b(σ, K?, a|KQBC) were
πσ
−1(KQ) and sampling probability q(KQ) that are also needed to compute counterfactual-values ˆv1
already computed in the forward pass. The counterfactual values are then used to compute actions’ regrets (Eq. 3) which is not
shown in the table. Values in cell with light gray background are not used in computation of ˆv1

b(σ, K?, a|KQBC).

Backward passC (J)ack ⅓(K)ing⅓(Q)ueen⅓ C(J)ack½(Q)ueen½1(C)heck⅓b=-1(B)et⅔b=+0.52(F)old¾b=-2(C)all¼b=+1+2 