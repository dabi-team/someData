8
1
0
2

n
u
J

0
2

]
T
I
.
s
c
[

2
v
9
0
2
4
0
.
3
0
7
1
:
v
i
X
r
a

Virtual Reality over Wireless Networks: Quality-of-Service
Model and Learning-Based Resource Management

Mingzhe Chen∗, Walid Saad†, and Changchuan Yin∗
∗Beijing Laboratory of Advanced Information Network, Beijing University of Posts and Telecommunications, Beijing, China 100876,
Emails: chenmingzhe@bupt.edu.cn, ccyin@ieee.org.
†Wireless@VT, Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA, Email: walids@vt.edu.

Abstract—In this paper, the problem of resource management
is studied for a network of wireless virtual reality (VR) users
communicating over small cell networks (SCNs). In order to
capture the VR users’ quality-of-service (QoS) in SCNs, a novel
VR model, based on multi-attribute utility theory, is proposed.
This model jointly accounts for VR metrics such as tracking
accuracy, processing delay, and transmission delay. In this model,
the small base stations (SBSs) act as the VR control centers that
collect the tracking information from VR users over the cellular
uplink. Once this information is collected, the SBSs will then send
the three-dimensional images and accompanying audio to the
VR users over the downlink. Therefore, the resource allocation
problem in VR wireless networks must jointly consider both
the uplink and downlink. This problem is then formulated as a
noncooperative game and a distributed algorithm based on the
machine learning framework of echo state networks (ESNs) is
proposed to ﬁnd the solution of this game. The proposed ESN
algorithm enables the SBSs to predict the VR QoS of each
SBS and is guaranteed to converge to a mixed-strategy Nash
equilibrium. The analytical result shows that each user’s VR
QoS jointly depends on both VR tracking accuracy and wireless
resource allocation. Simulation results show that the proposed
algorithm yields signiﬁcant gains, in terms of VR QoS utility,
that reach up to 22.2% and 37.5%, respectively, compared to Q-
learning and a baseline proportional fair algorithm. The results
also show that the proposed algorithm has a faster convergence
time than Q-learning and can guarantee low delays for VR
services.

I. INTRODUCTION

Virtual reality (VR) services will enable users to experience
and interact with virtual and immersive environments through
a ﬁrst-person view [2]. For instance, individuals can use a VR
device to walk around in a fully immersive world and travel
to any destination, within the conﬁnes of their own home.
Compared to a static high deﬁnition (HD) video, a VR video is
generated based on the users’ movement such as head and eye
movements. Therefore, generating a VR video requires track-
ing information related to the users’ interactions with the VR
environment. In consequence, the tracking accuracy and the
delay of tracking information transmission will signiﬁcantly
affect the creation and transmission of VR videos hence affect-
ing the users’ experience. Such tracking delay does not exist

This work was supported in part by the National Natural Science
Foundation of China under Grant 61671086 and Grant 61629101, the 111
Project under Grant B17007, the Director Funds of Beijing Key Laboratory
of Network System Architecture and Convergence under Grant 2017BKL-
NSAC-ZJ-04, the Beijing Natural Science Foundation under Grant L172032,
the BUPT Excellent Ph.D. Students Foundation, and in part by the U.S.
National Science Foundation under Grant CNS-1460316.

A preliminary version of this work was published in the IEEE GLOBECOM

conference [1].

for regular HD videos and, thus, constitutes a fundamental
difference between VR and HD videos. If VR devices such
as HTC Vive [3] rely on wired connections to a VR control
center, such as a computer, for processing their information,
then the users will be signiﬁcantly restricted in the type of
actions that they can take and the VR applications that they can
experience. To enable truly immersive VR applications, one
must deploy VR systems [4] over wireless cellular networks.
In particular, VR systems can use the wireless connectivity
of small cell networks (SCNs) [4] in which small cell base
stations (SBSs) can act as VR control centers that directly
connect to the VR devices over wireless links, collect the
tracking1 information from the VR devices, and send the
VR images to the VR devices over wireless links. However,
operating VR devices over SCNs faces many challenges in
terms of tracking, low delay (typically less than 20 ms), and
high data rate (typically over 25 Mbps) [4].

A. Related Work

The existing literature has studied a number of problems
related to VR such as in [2], [4]–[13]. The authors in [2] and
[4] provided qualitative surveys that motivate the deployment
of VR over wireless systems, but these works do not provide
any mathematically rigorous modeling. In [5]. the authors
proposed a distortion-aware concurrent multipath data transfer
algorithm to minimize the end-to-end video distortion. The
work in [6] deveploped a modeling-based approach to optimize
the high frame rate video transmission over wireless networks.
However, the works in [5] and [6] only consider the video
content transmission which cannot be directly applied to the
VR content transmission since the VR contents are generated
based on the users’ tracking information. In [7], the authors
proposed a streaming scheme that delivers only the visible
portion of a 360◦ video based on head movement prediction.
Meanwhile, the authors in [8] developed an algorithm for
generating high-quality stereo panoramas. The work in [9] pro-
posed a real-time solution that uses a single commodity RGB-
D camera to track hand manipulation. In [10], a reinforcement
learning algorithm is proposed to guide a user’s movement
within a VR immersive environment. The authors in [11]
proposed an approach based on the three-dimensional (3D)
heat maps to address the delay challenges. The work in [12]
designed several experiments for quantifying the performance

1Here, tracking pertains to the fact that the immersive VR applications must
continuously collect a very accurate localization of each user including the
positions, orientation, and eye movement (i.e., gaze tracking).

 
 
 
 
 
 
of tile-based 360◦ video streaming over a real cellular network.
In [13], the authors performed a WiFi experiment for wireless
VR for a single user within a single room. However, beyond
the survey in [4], which motivated the use of VR over wireless
and the WiFi experiment for a single VR user in [13], the
majority of existing VR works in [2], [7]–[12] focus on VR
systems that are deployed over wired networks and, as such,
they do not capture any challenges of deploying VR over
wireless SCNs. Moreover, most of these existing works [2],
[4], [7]–[13] focus only on improving a single VR quality-
of-service (QoS) metric such as tracking or generation of 3D
images. Indeed, this prior art does not develop any VR-speciﬁc
model that can capture all factors of VR QoS and, hence,
these existing works fall short in addressing the challenges of
optimizing VR QoS for wireless users.

B. Main Contributions

The main contribution of this paper is a novel framework
for enabling wireless cellular networks to integrate VR appli-
cations and services. To our best knowledge, this is the ﬁrst
work that develops a comprehensive framework for analyzing
the performance of VR services over cellular networks. Our
main contribution include:

• We propose a novel VR model based on multi-attribute
utility theory [14], to jointly capture the tracking ac-
curacy, transmission delay, and processing delay thus
effectively quantifying the VR QoS for all wireless users.
In this VR model, the tracking information is transmitted
from the VR users to the SBSs over the cellular uplink
while the VR images are transmitted in the downlink from
the SBSs to their users.

• We analyze resource (resource blocks) allocation jointly
over the uplink and downlink. We formulate the problem
as a noncooperative game in which the players are the
SBSs. Each player seeks to ﬁnd an optimal spectrum
allocation scheme to optimize a utility function that
captures the VR QoS.

• To solve this VR resource management game, we propose
a learning algorithm based on echo state networks (ESNs)
[15] that can predict the VR QoS value resulting from
resource allocation and reach a mixed-strategy Nash equi-
librium (NE). Compared to existing learning algorithms
[16]–[18], the proposed algorithm has lower complexity,
requires less information due to its neural network nature,
and is guaranteed to converge to a mixed-strategy NE.
One unique feature of this algorithm is that, after training,
it can use the stored ESN information to effectively ﬁnd
an optimal converging path to a mixed-strategy NE.
• We perform fundamental analysis on the gains and trade-
offs that stem from changing the number of uplink and
downlink resource blocks for each user. This analytical
result shows that, in order to improve the VR QoS of
each user, we can improve the tracking system or increase
the number of the resource blocks allocated to each user
according to each user’s speciﬁc state.

Sensor

SBS

T(cid:85)acking information 
Transmission(cid:3)VR entertainment
transmission
Fig. 1. A network of immersive VR application.

Uplink subcarrier
Downlink subcarrier

• Simulation results show that the proposed algorithm can
yield, respectively, 22.2% and 37.5% gains in terms of
VR QoS utility compared to Q-learning and proportional
fair algorithm. The results also show that the proposed
algorithm signiﬁcantly improves the convergence time of
up to 24.2% compared to Q-learning.

The rest of this paper is organized as follows. The problem
is formulated in Section II. The resource allocation algorithm
is proposed in Section III. In Section IV, simulation results are
analyzed. Finally, conclusions are drawn in Section V.

II. SYSTEM MODEL AND PROBLEM FORMULATION

Consider the downlink transmission of a cloud-based small
cell network (SCN)2 servicing a set U of V wireless VR
users via a set B of B SBSs [20]. Here, we focus on
entertainment VR application such as watching immersive
videos and playing immersive games [7]. VR allows users
to be immersed in a virtual environment within which they
can experience a 3D and high-resolution 360◦ vision with 3D
surround stereo. In particular, immersive VR will provide a
360◦ panoramic image for each eye of a VR user. Compared
to a conventional 120◦ image, a 360◦ panoramic image enables
a VR user to have a surrounded vision without any dead
spots. However, a 360◦ VR image needs more pixels than a
traditional two-dimensional image, and, hence, VR transmis-
sion will require more stringent data rate [4] and delay (less
than 20 ms) requirements than traditional multimedia services.
Moreover, for an HD video, we only need to consider the
video transmission delay as part of the QoS. In contrast, for a
VR video, we need to jointly consider the video transmission
delay, tracking information transmission delay, and the delay
of generating the VR videos based on the tracking information.
Note that, for VR applications, the transmission delay and
processing delay will directly determine how each VR video
is transmitted and, thus, they are a key determinant of the VR
video quality.

The SBSs adopt an orthogonal frequency division multiple
access (OFDMA) technique and transmit over a set of uplink
resource blocks Su and a set of downlink resource blocks Sd,
as shown in Fig. 1. The uplink resource blocks are used to

2Since next-generation cellular networks will all rely on a small cell
architecture [19], we consider SCNs as the basis for our analysis. However,
the proposed VR model and algorithm can also be used for any other type of
cellular networks.

TABLE I
LIST OF NOTATIONS

Notation
V
B
W in
PB
Aj
aj
aji
W out
Nw
Ki
Di
χ
λ
DP
i
W
µj
A

Description
Number of users
Number of SBSs
Input weight matrix
Transmit power of SBSs
Set of SBS j’s actions
One action of SBS j
Action i of SBS j
Output weight matrix
Number of reservoir units
Tracking accuracy
Total delay of user i
Vector of user’s localization
Learning rate
Processing delay of user i
Reservoir weight matrix
Reservoir state of SBS j
Data size of tracking vector

Notation
Sd, Su
S d, S u
DT
ij
dij
cij
ij, su
sd
ij
dj
¯uj
ˆuj
γK
vj
Ui (Di, Ki)
Ui (Di |Ki )
Vj
|Aj |
a−j
L

Description
Number of downlink and uplink resource blocks
Sets of downlink and uplink resource blocks
Transmission delay between user i and SBS j
Distance between user i and SBS j
Data rate of user i associated with SBS j
Resource allocation vector
Downlink resource allocation of SBS j
Average utility function of SBS j
Utility function of SBS j
Maximal tracking inaccuracy
Uplink resource allocation of SBS j
Total utility function of user i’s VR QoS
Conditional utility function of user i’s VR QoS
Number of users associated with SBS j
Number of SBS j’s actions
Actions of all SBSs other than SBS j
Maximum Data size of each VR image

transmit the data that is collected by the VR sensors placed at
a VR user’s headset or near the VR user while the downlink
resource blocks are used to transmit the image displayed on
each user’s VR device. We deﬁne the coverage of each SBS
as a circular area of radius rB and we assume that each SBS
will only allocate resource blocks to the users located in its
coverage range. Table I provides a summary of the notations
used hereinafter.

A. VR Model

For our VR model, we consider delay and tracking accuracy
as the main VR QoS metrics of interest. Based on the accurate
localization of each user, the SBS can build the immersive and
virtual environment for each user. Among the components of
the VR QoS, a delay metric can be deﬁned to capture two key
VR service requirements: high data rate and low delay. Next,
we will explicitly discuss all the components of the considered
VR QoS metrics.

1) Tracking Model: For any VR QoS metric,

tracking
consists of position tracking and orientation tracking [7].
VR tracking directly affects the construction of the users’
virtual environment since the SBSs need to use each user’s
localization information to construct the virtual environment.
Hereinafter, we use the term “localization information” to
represent the information related to the user’s location and
orientation. The localization data of each user is used as
the primary component of tracking [4]. The tracking vector
of each user i is χi = [χi1, χi2, χi3, χi4, χi5, χi6], where
[χi1, χi2, χi3] represents the position of each VR user while
[χi4, χi5, χi6] represents the orientation of each user. Here,
we note that the position and orientation of each user are
determined by the SBS via the information collected by the
sensors. For each VR user i, the tracking accuracy Ki
can be given by:

su
ij

χi

γ u
i

max
(cid:13)
su
(cid:13)
ij

χi
(cid:0)

su
ij
su
γu
(cid:1)(cid:1)
(cid:0)
ij
i

− χR
i
− χR
(cid:13)
i
(cid:13)

(cid:0)

(cid:1)

,

(1)

(cid:13)
(cid:13)
where χi
is the tracking vector transmitted from
the VR headset to the SBS over wireless links that depends

(cid:1)(cid:1)

(cid:13)
(cid:13)

(cid:0)

(cid:0)

Ki

su
ij

= 1 −

(cid:0)
γ u
i

(cid:1)
su
ij

(cid:0)

(cid:0)

(cid:1)(cid:1)

σ2+ P

PU hk
il

l∈Uk ,l6=i

i

ijSu

on the signal-to-interference-plus-noise (SINR) ratio. su
ij =
su
ij1, . . . , su
represents the vector of uplink resource
blocks that SBS j allocates to user i with su
ijk ∈ {1, 0}. Here,
h
the uplink (downlink) resource blocks are equally divided into
Su (Sd) groups. Hereinafter, the term resource block refers to
one of those groups. su
ijk = 1 indicates that resource block
k is allocated to user i. γu
su
su
ijSu
i
ij1
represents the SINR of SBS j with resource blocks su
ij where
(cid:16)
(cid:17)i
(cid:0)
(cid:1)
is the SINR between user i and

, . . . , γ

PU hk
ij

su
ij

=

γ

γ

h

(cid:1)

(cid:0)

=

su
ijk

(cid:16)

(cid:17)

ij

(cid:0)

(cid:0)

ij.

χi

γu
i

su
ij,τ

ij,τ d−β

SBS j over resource block k. Here, Uk is the set of users that
use uplink resource block k. PU is the transmit power of user
i (assumed to be equal for all users), σ2 is the variance of the
Gaussian noise and hk
ij,τ = gk
is the path loss between
user i and SBS j over resource block k with gk
ij,τ being the
Rayleigh fading parameter, dij the distance between user i
and SBS j, and β the path loss exponent. In χi
,
the resource block vector su
ij determines the uplink SINR of
(cid:1)(cid:1)
the tracking vector’s transmission. The tracking vector will be
subject to bit errors and, hence, it will depend on the SINR
− χR
su
and the resource block vector su
i
ij
represents the wireless tracking inaccuracy. χR
is obtained
i
(cid:13)
(cid:0)
from the users’ force feedback and transmitted via a dedicated
(cid:13)
wireless channel. Force feedback [21] represents the feedback
that the users send to the SBSs whenever they are not satisﬁed
with the displayed VR image. χR
is only transmitted from
i
the users to the SBSs when the users feel uncomfortable
in the environment. Since χR
is not transmitted every time
i
slot, the SBSs can use orthogonal resource blocks over a
i . Thus, χR
dedicated channel to transmit χR
is generally much
i
su
more accurate than χi
. Note that, (1) is based on
ij
normalized root mean square errors [22], which is a popular
measure of the difference between two datasets. From (1), we
can see that, when the SINR increases, the bit error rate will
γ u
su
decrease and, hence,
decreases and the
i
ij
tracking accuracy improves.

− χR
i

γu
i

γu
i

χi

(cid:1)(cid:1)

(cid:1)(cid:1)

(cid:13)
(cid:13)

(cid:0)

(cid:0)

(cid:0)

(cid:13)
(cid:13)

(cid:0)

(cid:0)

(cid:1)(cid:1)

(cid:13)
(cid:13)

2) Delay: Next, we deﬁne the delay component that con-
sists of the transmission delay and processing (and computing)

delay. The transmission delay of each user i will be:

DT
ij

sd
ij , su
ij

=

(cid:0)

(cid:1)

L
sd
ij

cij

+

cij

A
su
ij

,

(2)

where L is the maximum size of VR image that each SBS
needs to transmit to its users, A is the size of the tracking
vector that each user needs to transmit to the associated SBS,

(cid:1)

(cid:0)

(cid:0)

(cid:1)

Sd

k=1
P

σ2+ P

PB hk
il

l∈Bk ,l6=j

and cij

sd
ij

=

sd
ijkBRlog2

1 + γ

sd
ijk

is the down-

(cid:1)
link rate of user i. cij

(cid:0)

(cid:16)
Su

=

(cid:16)
su
ijkBRlog2

(cid:17)(cid:17)

su
ij

1+γ

su
ijk

(cid:0)

k=1
(cid:17)(cid:17)
P
is the uplink rate of user i. sd
is the
ij =
vector of resource blocks that SBS j allocates to user i with
h
ijk ∈ {0, 1}. sd
sd
ijk = 1 indicates that resource block k is
PB hk
ij
allocated to user i. γ
is the signal-

ij1, . . . , sd
sd

ijSd

(cid:16)

(cid:16)

i

(cid:1)

=

sd
ijk

(cid:16)

(cid:17)

to-interference-plus-noise ratio between user i and SBS j over
resource block k with Bk being the set of the SBSs that
use downlink resource block k. BR is the bandwidth of each
resource block, PB is the transmit power of SBS j (assumed
to be equal for all SBSs).

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:1)

. l

χi

χi

γu
i

γu
i

γu
i

γu
i

γ u
i

(cid:1)(cid:1)(cid:1)

to l

su
ij

su
ij

su
ij

su
ij

χR
i

(cid:0)
su
ij

(cid:0)
to l

based on χR

i , they must construct l

In the VR QoS, the processing delay primarily stems from
the tracking accuracy. To properly capture the processing
delay, we deﬁne a vector l
χi
that represents
a VR image of user i constructed by the tracking vector
(cid:0)
(cid:0)
χi
will be typically generated
before the SBSs receive the force feedback tracking vector χR
i ,
(cid:1)(cid:1)(cid:1)
as the VR system will use the historical tracking information
to predict the future tracking vector. When the SBSs receive
χR
χR
i . The simplest
i
way of constructing the VR image l
is to correct it
(cid:1)
χR
from l
. The processing delay will
i
(cid:0)
then represent the time that an SBS must spend to change the
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
χR
VR image from l
χi
. The bits that the
i
su
χi
SBSs use to change the VR image from l
to
ij
(cid:0)
l
can be calculated by using the motion search algorithm
[23]. The motion search algorithm can ﬁnd the different pixels
between l
and, hence, an SBS can
directly replace those different pixels [23]. In our model, the
VR sensors can accurately collect the VR users’ movement
[24]. Moreover, if the SBSs receive accurate tracking data,
they can accurately extract each user’s location and orientation.
Hence, the tracking accuracy depends only on the data error
that is caused by the uplink transmission over the wireless
link. To compute the processing delay, we ﬁrst assume that
the total amount of computational resources available at each
SBS is M which is equally allocated to the associated users.
M represents the number of bits that can be processed by each
SBS which is determined by each SBS’s central processing
unit (CPU). Then, the processing delay can be given by:

and l

χR
i

χR
i

su
ij

(cid:1)(cid:1)(cid:1)

(cid:1)(cid:1)(cid:1)

(cid:1)(cid:1)(cid:1)

(cid:1)
(cid:0)

γ u
i

γu
i

χi

(cid:0)

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

DP
i

Ki

su
ij

υ

l

χi

=

, l

χR
i

,

(3)

(cid:0)

(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
≤ L represents the
where 0 ≤ υ
number of bits that must be changed when SBS j transforms
(cid:1)(cid:1)(cid:1)

(cid:0)
su
ij

(cid:1)(cid:1)
χi

(cid:0)
, l

γu
i

(cid:1)(cid:1)

(cid:0)
l

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

γu
su
i
ij
M /Nj
(cid:0)
χR
i

l

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

χi

χi

γu
i

γu
i

(cid:1)(cid:1)(cid:1)

to l

χR
i

χR
i

γu
i
su
ij
(cid:0)
(cid:0)
χR
i
(cid:1)(cid:1)

γu
l
χi
i
of user
(cid:0)

su
the VR image from l
. v (·) depends
ij
on the image size, the number of bits used to store a pixel,
(cid:0)
and the content of the VR image and is the result of the
motion search algorithm. Here, we adopt the motion search
su
, l
algorithm of [25] to determine υ
.
ij
su
When the SINR χi
i
the
increases,
ij
(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:0)
γu
bit errors in χi
will decrease. In consequence,
i
(cid:1)(cid:1)
su
, l
υ
will decrease and, hence, the
ij
(cid:0)
(cid:0)
γu
processing delay decreases. Here,
i
(cid:0)
(cid:1)(cid:1)(cid:1)
of user i depends on the resource blocks su
ij allocated to
(cid:1)(cid:1)
γ u
user i. When the deviation between l
and
i
l
increases, more data is needed to correct the image.
(cid:0)
≤ L is a constraint that captures
υ
(cid:0)
the maximum number of bits that must be corrected. Nj is
(cid:1)(cid:1)
(cid:0)
the number of the users associated with SBS j and M
is the
Nj
computation resources allocated to any user i’s VR session.
From (3), we can see that the processing delay depends on
the tracking accuracy, the number of the users associated with
SBS j, and the resource blocks allocated to user i. The total
delay of each user i will hence be:

the SINR χi

χR
i
l

(cid:0)
su
ij

χi
(cid:1)

χR
i

su
ij

su
ij

(cid:1)(cid:1)(cid:1)

(cid:1)(cid:1)(cid:1)

γu
i

χi

(cid:1)(cid:1)

, l

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

Di

sd
ij , su
ij

= DP
i

Ki

su
ij

+ DT
i

sd
ij , su
ij

.

(4)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:1)(cid:1)

(cid:0)

(cid:1)

B. Utility Function Model

Next, we use the framework multi-attribute utility theory
[14] to deﬁne a utility function that can effectively capture
VR delay and tracking. Using multi-attribute utility theory,
we construct a total utility function that jointly considers the
delay and tracking of the VR QoS. Conventional techniques
for deﬁning a utility function, such as by directly summing up
delay and tracking are only valid under the assumption that
delay and tracking are independent and that their relationship
the
is linear [26, Chapters 2 and 3]. However, for VR,
relationship between the delay and tracking is not necessarily
linear nor independent. Therefore, we use multi-attribute utility
theory [14] to deﬁne the utility function. The deﬁned utility
function can assign to each delay and tracking components of
the VR QoS a unique utility value without any constraints.

In order to construct the total utility function, we ﬁrst
deﬁne a conditional utility function for the delay component
of the VR QoS [14]. In the total utility function, both
tracking and delay will contribute to the utility value. In
contrast, in the conditional utility function of delay, only delay
contributes to the utility function and the tracking value is
assumed to be given. The method that uses a conditional
utility function to derive the total utility function is anal-
ogous to the approach used in probability theory to derive
a joint probability distribution from conditional probability
distributions [27]. For VR, the total utility function of user i is
su
ij , su
sd
, Ki
given by Ui
. The conditional utility
ij
ij
ij, su
sd
Ki
Di
function of delay, Ui
, represents the
ij
(cid:1)(cid:1)
(cid:0)
(cid:1)
total utility function given a certain value of the tracking
(cid:1) (cid:12)
(cid:0)
accuracy. Based on [14], the conditional utility function of
(cid:12)

su
ij

Di

(cid:1) (cid:1)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

delay for user i can be given by:

Ui

=

Di

su
ij

sd
ij, su
Ki
ij
ij))−Di(sd
Dmax,i(Ki(su
(cid:1) (cid:12)
(cid:0)
(cid:12)
Dmax,i(Ki(su
1,

(cid:0)

(cid:0)



ij ,Ki(su
(cid:1) (cid:1)
ij))−γD

ij))

, Di

sd
ij , Ki
sd
ij , Ki

su
ij
su
ij

≥ γD,

< γD,
(5)
where γD is the maximal tolerable delay for each VR user
(maximum supported by the VR system being used) and
ij, su
sd
Dmax,i
is the maximum delay
ij

su
ij

Di

Di

(cid:1)(cid:1)

(cid:1)(cid:1)



(cid:0)

(cid:0)

(cid:0)

(cid:0)

= max
sd
ij

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:1)

(cid:1)(cid:1)

Di

Ki

(cid:1)(cid:12)
(cid:12)

su
ij

su
ij

su
ij

su
ij

, Ki

(cid:1)(cid:1)
Dmax,i

ij, su
sd
ij

ij , su
sd
ij

that, when delay Di

of VR user i given su
(cid:0)
(cid:0)
(cid:1)
(cid:0)
ij. Here, Ui
= 0
Ki
γD
and Ui
= 1. From (5), we can see that, when
(cid:0)
sd
ij , su
Di
< γD, the conditional utility value will remain
ij
(cid:12)
(cid:0)
(cid:12)
1. This is due to the fact
(cid:1)
reaches the delay requirement γD, the utility value will reach
(cid:1)
its maximum of 1. Since delay and tracking are both domi-
nant components, we can construct the total utility function,
Ui
, that jointly considers the delay
and tracking based on [14]. Here, a dominant component
represents the component that will minimize the total utility
function regardless of the value of other components. For VR
QoS, delay and tracking are both dominant components. For
example, the VR QoS will be minimized when the value of
the delay function is at a minimum regardless of the value of
tracking accuracy. The use of dominant components such as
delay and tracking will simplify the formulation of the total
utility function [14]. Therefore, the total utility function of
tracking and delay will be [14]:

(cid:1)(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

Ui(cid:16)Di(cid:16)sd

ij , su

ij(cid:17), Ki

su
ij
(cid:0)



(a)
=

χi (cid:0)
1− (cid:13)
(cid:13)
max
ij (cid:13)
su
(cid:13)




su
γu
ij
i (cid:0)
γu
χi(cid:0)
i(cid:0)

(cid:1)(cid:1)
su
ij (cid:1)(cid:1)

Ki

ij , su

(cid:1)(cid:17) = Ui(cid:16)Di(cid:16)sd
− χR
Dmax,i

i (cid:13)
(cid:13)
−χR

i (cid:13)

(cid:13)

ij(cid:17)(cid:12)
(cid:12)
su
Ki
ij
(cid:0)
(cid:1)(cid:1)
(cid:0)
Ki
Dmax,i
(cid:0)

su
ij
(cid:0)

su
ij
(cid:0)

Ki
(cid:0)

(cid:1)(cid:17)Ui
su
sd
ij,Ki
−Di
ij
(cid:0)
(cid:0)
su
− γD
ij (cid:1)(cid:1)
(cid:0)

,

(cid:1)(cid:1)

(cid:1)(cid:1)

.

(cid:0)

(cid:0)

(cid:1)(cid:1)

Ki

su
ij

(6)
where Ui
is the utility function of tracking accu-
racy. (a) is obtained by substituting (1) and (5) into (6). From
(6), we can see that, the vector of resource blocks allocated
to user i for data transmission, sd
ij, and the resource blocks
allocated to user i for obtaining the tracking information,
su
ij , jointly determine the value of the total utility function.
Moreover, this total utility function can assign a unique value
to each tracking and delay component of the VR QoS.

C. Problem Formulation

Our goal

is to develop an effective resource allocation
scheme that allocates resource blocks so as to maximize the
users’ VR QoS. This maximization jointly considers the cou-
pled problems of user association, uplink resource allocation,
and downlink resource allocation. This optimization problem
can formalized as follows:

T

max
ij ,su
sd

ij ,Uj

Uit

Di

ij , su
sd
ij

, Ki

su
ij

(7)

t=1
X

i∈Uj
j∈B X
X

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:1)(cid:1)

s. t.

|Uj| 6 Vj,
∀j ∈ B,
sd
ij,k ∈ {0, 1} , ∀i ∈ U, ∀j ∈ B,
su
ij,k ∈ {0, 1} , ∀i ∈ U, ∀j ∈ B,

(7a)
(7b)

(7c)

where Uj is the set of the VR users associated with SBS j, |Uj|
is the number of the VR users associated with SBS j, Vj is the
number of the VR users located in the coverage of SBS j. Uit
is the utility value of user i at time t. (1a) indicates that the
number of VR users associated with SBS j must not exceed
the number of VR users located in the coverage of SBS j. (1b)
and (1c) indicate that each downlink resource block sd
ij,k or
uplink resource block su
ij,k can be allocated to one VR user.
In (7), the VR QoS of each SBS depends not only on its
resource allocation scheme but also on the resource allocation
decisions of other SBSs. Consequently, the use of centralized
optimization for such a complex problem is not possible
as it is largely intractable and yields signiﬁcant overhead.
To overcome this challenge, we formulate a noncooperative
between the SBSs that
game G =
can be implemented in a distributed way. Each player j
has a set Aj =
of |Aj | actions. In this
game, each action of SBS j, aj = (dj, vj) consists of: (i)
downlink resource allocation vector, dj =
.
sd
represents the vector of downlink
ij =
i
resource blocks that SBS j allocates to user i and sijk ∈ {1, 0}

B, {Aj }j∈B , {uj}
h

(cid:2)
sd
ijk = 1. sijk = 1 indicating that channel k is
allocated to user i and sijk = 0 otherwise. Vj is the number of
all users in the coverage area of SBS j, and (ii) uplink resource

aj1, . . . , aj|Aj |

sij1, . . . , sijSd

1j, . . . , sd
sd

i=1
P

with

j∈B

Vj j

(cid:9)

(cid:8)

Vj

i

h

(cid:3)

Vj

allocation vector, vj =

su
ijk = 1.
a = (a1, a2, . . . , aB) ∈ A, represents the action proﬁle of all
players and A =

1j, . . . , su
su

i=1
P

h
j∈B Aj.

with

To maximize the VR QoS of each user, the utility function

Vj j

i

of each SBS j can be given by:

Q

Vj

uj (aj, a−j) =

Ui

Di

ij , su
sd
ij

, Ki

su
ij

,

(8)

(cid:0)
where aj ∈ Aj is an action of SBS j and a−j denotes the
action proﬁle of all SBSs other than SBS j. Let πj,ai =

(cid:1)(cid:1)

(cid:1)

(cid:0)

(cid:0)

i=1
X

T

1

1
T

t=1
P

{aj =aji} = Pr (aj = aji) be the probability that

lim
T →∞
SBS j uses action aji. Hence, πj =
πj,aj1 , . . . , πj,aj|An |
will be a probability distribution for SBS j. We assume that
(cid:3)
the VR transmission is analyzed during a period that consists
of T time slots. Therefore, the average value of the utility
function can be given by:

(cid:2)

¯uj (aj, a−j) = lim
T →∞

1
T

T

t=1
X

uj (aj, a−j)

=

uj (aj, a−j)

a∈A  
X

πk,ak

.

!

(9)

k∈B
Y

To solve the proposed resource allocation game, we use the

concept of a mixed-strategy NE, formally [28]:

Deﬁnition 1. (mixed-strategy Nash Equilibrium): A mixed
1, . . . , π∗
strategy proﬁle π∗ = (π∗
is a mixed-
strategy Nash equilibrium if, ∀j ∈ B and πj, we have:

j , π∗
−j

B) =

π∗

¯uj

π∗

j , π∗
−j

≥ ¯uj

(cid:0)
πj, π∗
−j

(cid:1)

,

(10)

where ¯uj (πj, π−j) =

(cid:0)

πk,ak is the expected

(cid:1)

utility of SBS j selecting the mixed strategy πj.

(cid:1)
a∈A
P

uj (a)
(cid:0)
k∈B
Q

The mixed-strategy NE for the SBSs represents a solution
at which each SBS j can maximize the average VR QoS for
its users, given the actions of its opponents.

III. ECHO STATE NETWORKS FOR SELF-ORGANIZING
RESOURCE ALLOCATION

Next, we introduce a learning algorithm used to solve the
VR game and ﬁnd its NE. Since we consider both uplink and
downlink resource allocation, the number of actions will be
much larger than conventional resource allocation scenarios
typically consider only uplink or downlink resource
that
allocation. Therefore, as the number of actions signiﬁcantly
increases, by using traditional game-theoretic algorithms such
as ﬁctitious play [29], each SBS may not be able to collect
all of the information used to calculate the average utility
function. Moreover, using such conventional game-theoretic
and learning techniques, the SBSs will typically need to re-run
the entire steps of the algorithm to reach a mixed-strategy NE
as the states of the users and network vary. Hence, the delay
during the convergence process of such algorithms may not be
able to satisfy the QoS requirement of a dynamic VR network.
To satisfy the QoS requirement for the VR transmission of
each SBS, we propose a learning algorithm based on the
powerful framework of echo state networks (ESN) [30]. The
proposed ESN-based learning algorithm enables each SBS to
predict the value of VR QoS that results from each action
and, hence, can reach a mixed-strategy NE without having to
traverse all actions. Since the proposed algorithm can store the
past ESN information, then, it can ﬁnd an optimal convergence
path from the initial state to a mixed-strategy NE. Compared
to our work in [31] that is based on two echo state network
schemes, the proposed algorithm relies on only one echo state
network, which reduces complexity. Next, we introduce the
components of an ESN-based learning algorithm and, then,
we introduce its update process.

A. ESN Components

actions that each SBS will perform during the learning process
while harmonizing the tradeoff between exploitation and ex-
ploration. Therefore, the probability with which SBS j chooses
action i will be given by:

πj,aj = 


1 − ε + ε
ε
|Aj|

|Aj|
,

ˆuj (aj) ,

, arg max
aj ∈Aj
otherwise,

(12)


where ˆuτ,j (aj) =

uj (aj, a−j)π−j,a−j is the expected

a−j∈A−j
P

Q

utility of an SBS j with respect to the actions of its opponents,
k6=j,k∈B Ak is the set of actions other than the
A−j =
πk,ak . ˆuτ,j (aj) is
action of SBS j and π−j,a−j =

k∈B,k6=j,
Q

the marginal probability distribution over the action set of SBS
j. ˆuτ,j (aj) is the average utility function over all SBSs other
than SBS j while ¯uj (aj, a−j) is the average utility value over
all SBSs. From (12), we can see that each SBS will assign the
highest probability, 1 − ε + ε
|Aj | , to the action that results in
the maximum utility value, ˆuτ,j. For other actions, the SBS
will assign the probability ε
|Aj | . The value of ε determines the
convergence speed. In this case, as each SBS maximizes the
utility ˆuτ,j, the average utility ¯uj reaches maximum. Note that,
(12) is used to ﬁnd the optimal action for each SBS during
the training stage. After training, the SBSs will directly select
the optimal action that can maximize ¯uj. To capture the gain
that stems from the change of the number of resource blocks
allocated to each user i, we state the following result:

Theorem 1. The gain of user i’s VR QoS due to the change
of the number of resource blocks allocated to user i is:
i) The gain of user i that stems from the change of the number
of uplink resource blocks allocated to user i, ∆U u

i , is:

L

(cij (sd)) − L
cij(sd

ij) 



max
sd
ij

Ki

ij + ∆su
su
ij


DT
ij

sd, su

ij + ∆su
ij


+ DP
i

(cid:0)
ij + ∆su
su
ij

(cid:1)
− γD

∆U u

i =

max
sd
ij

(cid:0)

(cid:0)

(cid:1)(cid:1)

max
sd
ij

−




L
(cij (sd))

−

cij

L
sd
ij

(cid:0)




(cid:1)


DT
ij

max
sd
ij

(cid:0)
Ki
sd, su
(cid:0)
ij

su
ij
+DP
(cid:1)
i

(cid:1)

,

su
ij

−γD

(cid:0)

(cid:0)

(cid:1)(cid:1)

(cid:0)

(cid:1)
(13)

ii) The gain of user i that stems from the change of number
of downlink resource blocks allocated to user i, ∆U d

i , is:

The proposed ESN-based learning algorithm consists of ﬁve
components: a) agents, b) inputs, c) ESN model d), actions,
and e) output, deﬁned as follows.

• Agent: The agents in our ESN are the SBSs in the set B.
• Actions: Each action aj of SBS j jointly considers the

uplink and downlink resource blocks, which is given by:

∆U d

i =

(Dmax,i(su
Ki(su
(Dmax,i(su
Ki(su
(Dmax,i(su

ij)L

Ki(su
ij)−γi,D)cij(sd
ij)Lcij(∆sd
ij)−γi,D)cij(sd
ij)L
ij)−γi,D)

ij)

×

ij)

ij)

cij(sd

ij)

, cij

∆sd
ij

≫ cij

sd
ij

,

, cij

(cid:1)

≪ cij

(cid:0)
∆sd
ij
cij(∆sd
ij)
(cid:1)
(cid:0)
ij)cij(∆sd
+cij(sd

2

(cid:0)
sd
ij

(cid:1)
,

(cid:1)
(cid:0)
, else.
ij)

(14)






aj = (dj, vj) =

1j · · · sd
sd

Vj j, su

1j · · · su

Vj j

T

.

(11)

Proof. See Appendix A.

In order to guarantee that any action always has a non-
zero probability to be chosen, the ε-greedy exploration [32]
is adopted. This mechanism is responsible for selecting the

(cid:3)

(cid:2)

From Theorem 1, we can see that the tracking accuracy, Ki,
and the number of uplink resource blocks allocated to user i,
will directly affect the VR QoS gain of user i. Therefore, in

order to improve the VR QoS of each user, we can either
improve the tracking system or increase the number of the
resource blocks allocated to each user according to each user’s
speciﬁc state. Moreover, the gain due to increasing the number
of downlink resource blocks depends on the values of data
rates cij
. Hence, the proposed learning
algorithm needs to choose the optimal resource allocation
scheme to maximize the VR users’ QoS.

and cij

∆sd
ij

sd
ij

• Input: The ESN input is a vector xτ,j = [x1, · · · , xB]T
where xj represents the index of the probability distribution
that SBS j uses at time τ . xτ,j is then used to estimate the
value of ˆuj that captures the average VR QoS of SBS j.

(cid:0)

(cid:1)

(cid:1)

(cid:0)

• ESN Model: For each SBS j, the ESN model is a learning
architecture that can ﬁnd the relationship between the input
xτ,j and output yτ,j, thus building the function between the
SBS’s probability distribution and the utility value. Mathemat-
ically, the ESN model consists of the output weight matrix
j ∈ R|Aj |×(Nw+B) and the dynamic reservoir containing
W out
j ∈ RNw×B, and the recurrent
the input weight matrix W in

0

0



w11







0
0

with Nw being the

matrix W j = 

0
. . .
0 wNw Nw
number of the dynamic reservoir units. To guarantee that the
ESN algorithm can predict the utility values, Nw must be
larger than the size of the input vector xτ,j [22]. Here, the
dynamic reservoir is used to store historical ESN information
includes input, reservoir state, and output. Note that
that
the historical ESN information can be used to ﬁnd a fast
converging process from the initial state to the mixed-strategy
NE. Here, the number of actions for each SBS determines the
output weight matrix and recurrent matrix of each ESN. Next,
we derive the number of the actions of each SBS j, |Aj|.

Proposition 1. Given the number of the downlink and uplink
resource blocks, Sd and Su, as well as the users located in
the coverage of SBS j, Vj, the number of actions for each
SBS j, |Aj|, is given by:

|Aj| =

(cid:18)

Sd − 1
|N (Vj)| − 1

x
y

where

Vj

n|

(cid:19)

(cid:18)
ni = Sd, ni > 0

(
elements in N (Vj ).

i=1
P

Vj −1

ni
Su −

i−1

,
k=1 ni (cid:19)

P

and N (Vj ) =

n∈N (Vj )

(cid:19) X
= x(x−1)···(x−y+1)

i=1 (cid:18)
Y

y(y−1)···1

with |N (Vj)| being the number of

)

Proof. See Appendix B.

Based on Proposition 1, we can determine the matrix size
for both W out
and W j. From Proposition 1, we can see
j
that, as the number of users increases, the number of actions
increases. Moreover, the increasing number of resource blocks
will also increase the number of actions. From Proposition 1,
we can also see that the number of actions in the uplink is
much larger than the number of actions in the downlink. This

TABLE II
ESN-BASED LEARNING ALGORITHM FOR RESOURCE ALLOCATION

Inputs: Mixed strategy xτ,j
j , W j , W out

Initialize: W in

for each time τ do.

j , and yj = 0.

(a) Estimate the value of the utility function based on (16).
if τ = 1

(b) Set the mixed strategy πτ,j uniformly.

else

(c) Set the mixed strategy πτ,j based on (12).

end if
(d) Broadcast the index of the mixed strategy to other SBSs.
(e) Receive the index of the mixed strategy as input xτ,j.
(f) Perform an action based on πτ,j and calculate the actual

utility value.

(e) Update the dynamic reservoir state based on (15).
(g) Update the output weight matrix based on (17).

end for

is due to the fact that, in the uplink, the interference of each
user changes as the resource blocks allocated to each user
vary. However, in the downlink, the actions will not affect the
interference of each user.

• Output: The output of

the ESN-based learning al-
time t is a vector of utility values yτ,j =
. Here, yτ,ji represents the estimated

gorithm at
yτ,j1, yτ,j2, . . . , yτ,j|Aj |
value of utility ˆuτ,j (aji).
(cid:2)
(cid:3)

B. ESN-Based Learning Algorithm for Resource Allocation

We now present the proposed ESN-based learning algorithm
to ﬁnd a mixed strategy NE. The proposed learning algorithm
can ﬁnd an optimal convergence path from any initial state
to a mixed-strategy NE. In particular, the proposed algorithm
enables each SBS to reach a mixed-strategy NE by traversing
minimum number of strategies after training. In order to ﬁnd
the optimal convergence path, the proposed algorithm must
store the past ESN information that consists of input, reservoir
states, and output. The past ESN information from time 0 up
until time τ is stored by the dynamic reservoir state µτ,j. The
dynamic reservoir state of SBS j at time τ is:

µτ,j = f

W jµτ −1,j + W in

j xτ,j

,

(15)

(cid:1)

(cid:0)
where f (x) = ex−e−x
ex+e−x is the tanh function. From (15), we
can see that the dynamic reservoir state consists of the past
dynamic reservoir states and the mixed strategy at time τ .
Thus, the dynamic reservoir state actually stores the mixed
strategy from time 0 to time τ . Based on the dynamic reservoir
the proposed ESN algorithm will combine with the
state,
output weight matrix to estimate the value of conditional utility
value. The estimation of the utility value can be given by:

yτ,j = W out
τ,j

µτ,j
xτ,j

,

(cid:21)

(cid:20)

(16)

where W out
τ,j is the output weight matrix at time slot τ . To
enable the ESN to use reservoir state µτ,j to predict the utility
value, ˆuτ,ji, due to action aji, we must train the output matrix
W out
j using a linear gradient descent approach, which is:

W out

τ +1,ji = W out

τ,ji +λ

ˆuτ,ji − yτ,ji

xj

τ,j, aji

µT

τ,j, (17)

(cid:16)

(cid:16)

(cid:17)(cid:17)

τ,ji is row i of W out

where W out
τ,j, λ is the learning rate, and
ˆuτ,ji is the actual utility value. Here, yτ,ji is the estimation of
the utility value ˆuτ,ji. Table II summarizes our algorithm.

C. Convergence of the ESN-Based Learning Algorithm

Now, we analyze the convergence of the proposed ESN-

based learning algorithm.

Theorem 2. The proposed ESN-based learning algorithm
converges to the utility value, ˆuj, if any following conditions
is satisﬁed:
i) λ is a constant and

W in
ji represents the row i of W in
j .

where W in
ii) λ satisﬁes the Robbins-Monro conditions [33] (λ (t) >
0,

∞
t=0 λ2 (t) < +∞).

min
ji,xτ,j ,x′

xτ,j − x′

W in
ji

≥ 2,

τ ′,j

(cid:1)

(cid:0)

τ,j

∞
t=0 λ (t) = +∞,
Proof. See Appendix C.
P

P

From Theorem 2, we can see that

the convergence of
the proposed algorithm depends on the values of the input
weight matrix W in
ji and the input xτ,j. These values also
affect the capacity of the ESN’s memory. Here, the memory
of a ESN represents the ability that an ESN can store the
past ESN information. Therefore, the proposed algorithm of
SBS j can converge to the conditional utility function ˆuj
by choosing appropriate W in
ji and xτ,j. Indeed, the proposed
learning algorithm can converge to ˆuj even when W in
ji and
xτ,j are generated randomly. This is due to the fact that the
probability of uτ,j = u′
τ ′,j is particularly small since uτ,j
has a larger number of elements (i.e. more than 500). Here,
we note that, compared to our work in [31] that uses two
echo state networks to guarantee convergence, the proposed
ESN algorithm uses only one echo state network and is still
guaranteed to converge as shown in Theorem 2. Moreover,
this new proof of convergence, we can invoke our result in the
fact that the algorithm will reach a mixed NE follows directly
from [31, Theorem 2] to guarantee that the convergence point
will be a mixed NE, as follows.

Corollary 1. The ESN-based learning algorithm of each SBS
j converges to a mixed-strategy Nash equilibrium, with the
mixed strategy probability π∗

j , ∀j ∈ B.

Based on (12), each SBS will assign the highest proba-
bility to the action that results in the maximum value of
ˆuτ,j(aj). Therefore, when each SBS j reaches
ˆuj,max = max
aj ∈Aj
the optimal mixed strategy π∗
j and the ESN reaches the
maximal utility ˆuj, the maximum value of the average utility
ˆuj,max. Since the mixed-
¯uj can be given by
strategy NE depends on the utility values that are not unique,
then, the resulting mixed-strategy NE is not unique. While
characterizing all possible NEs is challenging analytically, in
our simulations in Section IV, we will analyze the performance
of different NEs.

1 − ε + ε
|Aj |

(cid:17)

(cid:16)

ESN will be zero. During each iteration,
the output and
reservoir of the ESN will be updated based on (15)-(16).
Based on the ESN’s output, each SBS will update its mixed
strategy and broadcast the index of this mixed strategy to
other SBSs. Compared to the size of VR content and tracking
information, the size of the index of the strategy is very small
(it can be represented with less than 16 bits). Therefore, these
interactions between SBSs are independent of the network
size and, hence, they incur no notable overhead. The resulting
mixed-strategy NE depends on the utility value resulting from
each action of each SBS.

The objective of our game is to ﬁnd the equilibrium mixed
strategy for each SBS. Hence, the complexity of the proposed
algorithm depends on the number of mixed strategies. Based
on (12), we can see that the number of mixed strategies is
equal to the number of actions. Since the worst-case for each
SBS is to traverse all actions, the worst-case complexity of
the proposed algorithm is O(|A1| × · · · × |AB|). However, the
worst-case complexity pertains to a rather unlikely scenario
in which all SBSs choose their optimal probability strategies
after traversing all other mixed strategies during each period
τ and, hence, the probability of occurrence of the worst-
|A1|−1
3.
case scenario is
Therefore, the proposed algorithm will converge faster than

1 − ε
|A1|

1 − ε

× · · ·×

|AB |−1

|AB |

(cid:16)
the worst-case with probability 1 −

(cid:17)

(cid:16)
1 − ε
|A1|

(cid:17)
|A1|−1

× · · · ×

|AB |−1

(cid:16)

(cid:17)

(cid:17)

|AB |

1 − ε
. Moreover, as the number of SBSs in-
creases, based on Proposition 1, the number of actions for each
(cid:16)
SBS signiﬁcantly decreases. Thus, the worst-case complexity
of the proposed algorithm will also decrease. In addition, our
ESN-based learning algorithm can use the past information
to predict the value of utility ˆuj and, hence, each SBS can
obtain the VR QoS of each SBS without implementing all of
its possible actions, which will also reduce the complexity of
the proposed algorithm. During the convergence process, the
proposed algorithm needs to harmonize the tradeoff between
exploration and exploitation. Exploration is used to enable
each SBS to explore actions so as to ﬁnd a better solution.
Exploitation refers to the case in which each SBS will use
the current optimal action at this iteration. This tradeoff is
controlled by the ε-greedy exploration speciﬁced in (12). If we
increase the probability of exploration, the proposed algorithm
will use more actions and, hence, the number of iterations
that the proposed algorithm requires to reach a mixed-strategy
NE will decrease. However,
increasing the probability of
exploration may reduce the VR QoS of each user when the
selected action is worse than the current optimal action.

3 Based on (12), for each SBS, the probability that the optimal action is
1 − ε

and hence, the probability that

not selected at each iteration is

(cid:18)

|Aj| (cid:19)

D. Implementation and Complexity

the optimal action is selected at the last iteration is

The proposed algorithm can be implemented in a distributed
the reservoir and output of the

the initial state,

way. At

Therefore,

iteration is

the probability of all SBSs select
1 − ε

× · · · ×

|A1|−1

1 − ε

|A1|

(cid:16)

(cid:17)

(cid:16)

|AB |

1 − ε

|Aj | (cid:19)

(cid:18)

|Aj|−1

.

their optimal action at last

|AB |−1

.

(cid:17)

TABLE III
SYSTEM PARAMETERS

Parameter
F
B
Nw
Nv

Value
1000
4
1000
6

Parameter
PB
Su
σ2
λ

Value
20 dBm
5
-95 dBm
0.03

Parameter
υ
Sd
rB
A

Value
5
5
25 m
100 kB

Fig. 2. Total VR QoS utility of each user vs. the tracking and delay utilities.
Here, total VR QoS utility refers to (6).

IV. SIMULATION RESULTS AND ANALYSIS

For our simulations, we consider an SCN deployed within
a circular area with radius r = 100 m. U = 25 users and
B = 4 SBSs are uniformly distributed in this SCN area.
The HTC Vive VR device is considered in our simulations
and, hence, the number of pixels for a panoramic image is
1920 × 1080 and each pixel is stored in 16 bits [34]. The
ﬂashed rate which represents the update rate of a VR image,
is 60 images per second and the factor of compression is
150 [4]. Since two panoramic images consist of one VR
image (one panoramic image per eye), the rate requirement
for wireless VR transmission will be 25.32 Mbit/s4. The
bandwidth of each resource block is set to 10 × 180 kHz [35].
We use a standard H.264/AVC video codec. The VR video
is encoded at 60 frames per second and the encoding rate of
each VR video is 4 Mbps. The detailed simulation parameters
are listed in Table III. For comparison purposes, we use three
baselines: a) Proportional fair algorithm in [36] that allocates
the resource blocks based on the number of the resource
blocks needed to maximize each user’s utility value, b) Q-
learning algorithm in [37] that has considered the historical
utility value to estimate the future utility value, and c) Optimal
heuristic search algorithm. Note that, in order to compare with
the proportional fair algorithm, all SBSs choose the action
with highest probability among the mixed strategy when the
proposed algorithm reaches a mixed-strategy NE. This is used
for all results in which we compare to proportional fair. Here,
the users’ localization data is measured from actual wired HTC
Vive VR devices and the wireless transmission is simulated, in
order to compute the tracking accuracy. All statistical results
are averaged over a large number of independent runs.

4Here, for each second, each SBS needs to transmit 1920×1080×16×60×
2 = 3, 981, 312, 000 bits = 3796.875 Mbits to each user. Since the factor
of compression is 150, the rate requirement is 3796.875.75/150 = 25.3125
Mbit/s.

Fig. 3. Total VR QoS utility of each user vs. the tracking and delay utilities.
Here, total VR QoS utility refers to (6).

Fig. 2 shows how each user’s VR QoS utility varies as the
tracking and delay utilities change. In Fig. 2, different colors
indicate different total VR QoS utilities. From Fig. 2, we can
see that, when the delay (tracking) utility is 0, the total VR
QoS utility will be 0 regardless of the tracking (delay) value.
Thus, both tracking accuracy and delay will affect the VR QoS.
In Fig. 2, we can also see that only when both tracking and
delay utilities are 1, the total VR QoS utility is maximized.
This is due to the fact that the multi-attribute utility theory
model assigns to each tracking and delay components of the
VR QoS a unique value. Clearly, it is clear that, the proposed
total utility function can effectively capture the VR QoS.

Fig. 3 shows how each user’s total utility changes as
function of the tracking accuracy and the bandwidth of each
downlink resource block group (Fig. 3 uses a similar color
legend as Fig. 2). From Fig. 3, we can see that when
the bandwidth of downlink resource block group (tracking
accuracy) is 0, the total VR QoS utility is 0 regardless of the
tracking accuracy (bandwidth of a downlink resource block
group). This is due to the fact that the VR QoS depends on
both delay and tracking. This corresponds to a scenario in
which SBS j has enough downlink bandwidth to send a VR
image to the user while the tracking information is inaccurate.
In this case, SBS j cannot construct the accurate VR image
due to the inaccuracy of user’s localization and, hence, the
VR QoS of this user will be 0. Fig. 3 also shows that, when
the tracking accuracy is 1 and the bandwidth of a downlink
resource block group is over 4 MHz, the total VR QoS utility
will be maximized. This veriﬁes the result of Theorem 1.

In Fig. 4, we show how the average delay utility for each
user varies as the number of SBSs increases. From Figs. 4(a)
and 4(b), we can see that, as the number of SBSs increases,
the average delay utility for each serviced user increases and
the transmission delay of each user decreases. This is due to
the fact that as the number of SBSs increases, the number of
users located in each SBS’s coverage decreases and, hence,
the average delay utility increases. However, as the number
of SBSs keeps increasing, the average delay utility increases
slowly. This stems from the fact that the interference from the
SBSs to the users increases as the number of SBSs continues
to increase. Fig. 4(b) also shows that the proposed algorithm

r
e
s
u
d
e
c
v
r
e
s

i

h
c
a
e
f
o

y
t
i
l
i
t
u

l

y
a
e
d
e
g
a
r
e
v
A

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

1

ESN-based learning algorithm
Optimal heuristic search algorithm
Q-learning algorithm
Proportional fair algorithm

2

3

4

5

6

Number of SBSs

(a) Average delay utility of each serviced user

)
s
m

(

r
e
s
u

i

d
e
c
v
r
e
s

h
c
a
e

f
o

l

y
a
e
d

e
g
a
r
e
v
A

28

26

24

22

20

18

16

14

12

10

1

ESN-based learning algorithm
Optimal heuristic search algorithm
Q-learning algorithm
Proportional fair algorithm

2

3

4

5

6

Number of SBSs

(b) Average delay of each serviced user

ESN-based learning algorithm
Optimal heuristic search algorithm
Q-learning algorithm
Proportional fair algorithm

32

30

28

26

24

22

20

18

16

14

12

10

)
s
m

(

r
e
s
u

i

d
e
c
v
r
e
s

h
c
a
e
f
o

l

y
a
e
d
e
g
a
r
e
v
A

15

25
Number of users
Fig. 6. Average delay for each serviced user as the number of users varies.

20

35

30

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

F
D
C

ESN-based learning
Optimal heuristic search
Q-learning algorithm
Proportional fair

0
0.05

1   

20  
Delay (ms)
Fig. 7. CDFs of the delay resulting from the different algorithms.

100 

10  

30  

15  

25  

50  

5   

Fig. 4. Delay for each serviced user vs. the number of SBSs.

)
s
m

(

r
e
s
u

i

d
e
c
v
r
e
s

h
c
a
e

f
o

l

y
a
e
d

e
g
a
r
e
v
A

14

12

10

8

6

4

2

0

1

Uplink transmission delay
Processing delay
Downlink transmission delay

2

3

4

5

6

Number of SBSs

Fig. 5. Delay for each serviced user vs. the number of SBSs.

achieves up to 19.6% gain in terms of average delay compared
to the Q-learning algorithm for the case with 6 SBSs. In Fig.
4(b), we can also see that the proposed ESN-based learning
algorithm enables the wireless VR transmission to meet typical
delay requirement of VR applications that consists of both the
transmission delay and processing delay (typically 20 ms [38]).
These gains stem from the fact that the proposed algorithm
uses the past ESN information stored at the ESN model to
ﬁnd a better solution for the proposed game.

Fig. 5 shows how the transmission and processing delays
change as the number of the SBSs varies. From Fig. 5, we
can see that all delay components of each user decrease as
the number of SBSs increases. This is due to the fact that,

as the number of SBSs increases, the users will have more
SBS choices and the distances from the SBSs to the users
decrease, and, hence, the SINR and the potential resources
(resource blocks) allocated to each user will increase.

In Fig. 6, we show how the average delay for each serviced
user changes as the number of VR users varies. From Fig.
6, we can see that, as the number of VR users increases, the
average delay of each VR user increases. This is due to the fact
that, as the number of users increases, the number of resource
blocks that is allocated to each VR user decreases. Fig. 6 also
shows that the deviation between the proposed algorithm and
Q-learning increases as the number of VR users increases.
This is due to the fact that, as the number of users increases,
the number of the users associated with each SBS increases.
In consequence, the number of actions for each SBS increases
and, hence, the SBSs need to record more QoS values resulting
from these actions. Compared to Q-learning that uses a matrix
to record the QoS values, the proposed algorithm uses an
ESN to approximate the function of QoS values and, hence,
the proposed algorithm can record more QoS utility values
compared to Q-learning. Fig. 6 also shows that the proposed
algorithm can yield 29.8% gain of the average delay compared
to proportional fair algorithm. This gain stems from the fact
that the proposed algorithm can ﬁnd the relationship between
resource block allocation strategies and utility values so as to
maximize the utility values.

Fig. 7 shows the cumulative distribution function (CDF) for
the total delay resulting from all of the considered schemes.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
9

8

7

6

5

4

3

s
r
e
s
u

l
l

a
r
o
f

y
t
i
l
i
t
u
S
o
Q
R
V
e
g
a
r
e
v
A

2

1

ESN-based learning algorithm
Q-learning algorithm

2

3

4

5

6

Number of SBSs

(a) Average total VR QoS utility for all users.

s
r
e
s
u

l
l

a

r
o

f

y
t
i
l
i
t
u
S
o
Q
R
V

l

a
t
o
T

12

11

10

9

8

7

6

5

4

3

2

ESN-based learning algorithm
Optimal heuristic search algorithm
Q-learning algorithm
Proportional fair algorithm

1

2

3

4

5

6

Number of SBSs

(b) Total VR QoS utility for all users

Fig. 8. VR QoS for all users vs. the number of SBSs. Here, total VR QoS
utility refers to (8). The proportional fair algorithm and optimal heuristic
search do not use mixed strategies and, hence, they are not shown in Fig.
8(a).

In Fig. 7, we can see that,
the delay of 100% of users
resulting from all of the considered algorithms will be above
0.05 ms. This is due to the fact that, the delay requirement
of each user is higher than 0.05 ms and, hence, when the
user’s delay requirement is satisﬁed, the SBSs will allocate
the resource blocks to other users. Fig. 7 also shows that
the proposed approach improves the CDF of up to 25% and
50% gains at a delay of 20 ms compared to Q-learning and
proportional fair algorithm. These gains stem from the fact that
the proposed algorithm can estimate the VR QoS resulting
from each SBS’s actions accurately and, hence, can ﬁnd a
better solution compared to Q-learning and proportional fair
algorithms. Fig. 7 also shows that there exists a delay variance
for users who achieve the 20 ms delay target. If needed, the
network can reduce this variance by adjusting the size of the
resource blocks.

Fig. 8 shows how the VR QoS for all users changes as the
number of SBSs varies. From Figs. 8(a) and 8(b), we can see
that both total utility values and average total utility values (at
the mixed-strategy NE) of all considered algorithms increase
as the number of SBSs increases. This is due to the fact that,
as the number of SBSs increases, the number of users located
within the coverage of each SBS increases and the distances
from the SBSs to their associated users decrease. Fig. 8(a)

13

12

11

10

9

8

7

6

5

4

s
r
e
s
u

l
l

a

r
o
f

y
t
i
l
i
t
u
S
o
Q
R
V

l

a
t
o
T

3
10

ESN-based learning algorithm
Optimal heuristic search algorithm
Q-learning algorithm
Proportional fair algorithm

15

20

25

30

35

Number of users
Fig. 9. The total VR QoS utility for all users vs. total number of users. Here,
the total VR QoS utility refers to (8).

S
B
S

l
l

a
f
o

y
t
i
l
i
t
u
S
o
Q
R
V

l

a
t
o
T

25

20

15

10

5

0

Best mixed strategy NE
Mean mixed strategy NE
Worst mixed strategy NE
Convergence of of Q-learning

5

10
Number of iterations
Fig. 10. Convergence of the proposed algorithm and Q-learning. Here, total
VR QoS utility refers to (8)

15

20

25

ESN-based learning algorithm
Q-learning algorithm

s
n
o
i
t
a
r
e
t
I

35

30

25

20

15

10

5

0

4

1

2

5

3
Number of SBSs
Fig. 11. The convergence time as a function of the number of SBSs.
shows that the proposed algorithm can yield up to of 15.3%
gain in terms of the average of total VR QoS utility compared
to the Q-learning for the case with 5 SBSs. In Fig. 8(b), we
can also see that the proposed ESN-based learning algorithm
achieves, respectively, up to 17.1% and 36.7% improvements
in terms of the total utility value compared to Q-learning and
proportional fair algorithms for the case with 4 SBSs. These
gains are due to the fact that our ESN algorithm can store
the past ESN information and use it to build the relationship
between the input and output. Hence, the proposed learning
algorithm can predict the output (utility value) and, hence, ﬁnd
a better solution for allocating resources.

In Fig. 9, we show how the total utility value of VR QoS
for all users changes as the total number of users varies. From

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Coverage
SBS
Users

25

20

15

10

5

0

-5

-10

-15

-20

-25

Coverage
SBS
Users

25

20

15

10

5

0

-5

-10

-15

-20

-25

-25

-20

-15

-10

-5

0

5

10

15

20

25

-25

-20

-15

-10

-5

0

5

10

15

20

25

(a) Optimal action of the ESN-based algorithm for the users over downlink and uplink

Coverage
SBS
Users

25

20

15

10

5

0

-5

-10

-15

-20

-25

Coverage
SBS
Users

25

20

15

10

5

0

-5

-10

-15

-20

-25

-25

-20

-15

-10

-5

0

5

10

15

20

25

-25

-20

-15

-10

-5

0

5

10

15

20

25

(b) Optimal action of proportional fair algorithm for the users over downlink and uplink

Fig. 12. Optimal actions resulting from the different algorithms (Each black arrow represents the downlink resource blocks while each color arrow represents
an group of uplink resource blocks).

Fig. 9, we can see that, as the number of users increases, the
total utility values of all considered algorithms increase. This
is due to the fact that, in all algorithms, each SBS has a limited
coverage area and, hence, the number of users located in each
SBS’s coverage increases with the network size. Moreover,
since each SBS has a limited number of resource blocks, the
number of users that can associate with each SBS is also
bounded. In particular, as the number of users located within
the coverage of a given SBS exceeds the maximal number of
users that each SBS can provide service, the SBS will only
service the users that can maximize the total utility value. The
VR QoS of the remaining users will be 0. In this case, the total
utility value will also increase with the number of the users.
From Fig. 9, we can also see that the proposed algorithm
achieves, respectively, up to 22.2% and 37.5% gains in terms
of the total utility value of VR QoS compared to Q-learning
and proportional fair algorithms for the case with 35 users.

Fig. 10 shows the number of iterations needed till con-
vergence for both Q-learning and the proposed ESN-based
learning approach with different mixed-strategy NEs and Q-
learning. In this ﬁgure, we can see that, as time elapses, the
total VR QoS utilities for both the proposed algorithm and Q-
learning increase until convergence to their ﬁnal values. Fig. 10
also shows that the proposed algorithm for best mixed strategy
NE needs 19 iterations to reach convergence while Q-learning
needs 25 iterations to reach convergence. Hence, the proposed
algorithm achieves 24.2% gain in terms of the number of

the iterations needed to reach convergence compared to Q-
learning. This is because the ESN in the proposed algorithm
can store the SBSs’ action strategies and its corresponding
total utility values. Fig. 10 also shows that, different mixed-
strategy NEs will result in different total utility values of
all SBSs. However, the total utility values achieved by these
mixed-strategy NEs are very close to each other. Moreover,
Fig. 10 also shows that the worst mixed-strategy NE resulting
from the proposed algorithm can still achieve 14% gain of the
total utility value compared to Q-learning. Hence, regardless
of the reached NE, the total utility value of the proposed
algorithm will be higher than the utility value achieved by
Q-learning.

In Fig. 11, we show how the convergence time changes
as the number of SBSs varies. In this ﬁgure, we can see
that as the number of the SBSs increases, the convergence
time of both algorithms increases. Indeed, as the number of
SBSs increases, the proposed ESN algorithm will require more
time to accurately calculate the VR QoS utility. From Fig.
11, we can also see that as the number of SBSs increases,
the difference in the convergence time between the proposed
algorithm and Q-learning increases. This stems from the fact
that as the number of SBSs increases, the number of actions for
each SBS decreases and, hence, the number of output weight
matrix used to predict the VR QoS utility for each action
decreases.

Fig. 12 shows the optimal actions resulting from the pro-

posed ESN-based learning algorithm and proportional fair
algorithm. Here, each color arrow represents a unique group
of uplink resource blocks. From Figs. 12(a) and 12(b), we
can see that, for downlink resource allocation, the proportional
fair algorithm allocates most of the downlink resource blocks
to the user located farthest to the SBS while the proposed
learning algorithm allocates only two groups of resource
blocks to the farthest user. This is due to the fact that the
proportional fair algorithm only considers the users’ resource
blocks demands while the proposed learning algorithm con-
siders how to maximize the total utility values of VR QoS
for all associated users. Figs. 12(a) and 12(b) also show
that, both the proposed ESN-based learning algorithm and
proportional fair algorithm allocate three groups of resource
blocks to the farthest user. However, the uplink resource blocks
allocated to each user are different. This is due to the fact
that, in uplink, the proportional fair algorithm only considers
the users’ resource blocks demands while ignoring the uplink
interference pertaining to the allocation of uplink resource
blocks. In this case, the interference of users in uplink will
signiﬁcantly decrease the total VR QoS utility for each user.
Note that, since each SBS allocates their all downlink and
uplink resource blocks to its associated users, the interference
of the users in downlink will not change as the actions vary
while the interference in uplink depends on the actions.

V. CONCLUSION

In this paper, we have developed a novel multi-attribute
utility theory based VR model that can capture the tracking
and delay components of VR QoS. Based on this model,
we have proposed a novel resource allocation framework for
optimizing the VR QoS for all users. We have formulated
the problem as a noncooperative game between the SBSs that
seeks to maximize the average VR QoS utilities for all users.
To solve this problem, we have developed a novel algorithm
based on the machine learning tools of echo state networks.
The proposed algorithm enables each SBS to decide on its
actions autonomously according to the users’ and networks’
states. Moreover, the proposed learning algorithm only needs
to update the mixed strategy during the training process
and, hence, can quickly converge to a mixed-strategy NE.
Simulation results have shown that the proposed VR model
can capture the VR QoS in wireless networks while providing
signiﬁcant performance gains.

A. Proof of Theorem 1

APPENDIX

For i), the gain that stems from increasing the number of

ij , su

i , is:

ij+∆su

Ui(cid:16)Di(cid:16)sd

uplink resource blocks allocated to user i, ∆U u
ij +∆su
su
ij, su
(cid:1)(cid:17)−Ui(cid:16)Di(cid:16)sd
ij (cid:17), Ki
ij
(cid:0)
ij, su
sd
su
ij + ∆su
− Di
Dmax,i
ij
(cid:0)
(cid:0)
(cid:1)
ij + ∆su
su
− γD
Dmax,i
ij(cid:1)
(cid:0)
ij, su
sd
ij
(cid:0)
− γD

ij + ∆su
su
ij
(cid:0)

su
ij
(cid:0)

Dmax,i

− Ki

=Ki

su
− Di
ij
(cid:1)
(cid:0)
su
Dmax,i
ij (cid:1)
(cid:0)

×

×

(cid:1)

(cid:1)

(cid:1)

,

su
ij(cid:17), Ki
ij
(cid:0)
ij + ∆su
ij

(cid:1)(cid:17)

(cid:1)

(18)

where (13) is obtained by substituting (2) and (4) into (18).
Since Ki (x) is determined by the bit errors due to the wireless
transmission, (13) cannot be further simpliﬁed.

For ii), the gain of changing the downlink resource blocks,
i , can be given by:

∆U d

∆U d

i = Ki

su
ij

×

(cid:1)

(cid:0)
su
ij

×

= Ki

Di

(cid:0)
DT
i

=

=

(cid:0)

(cid:1)
Ki
Dmax,i
Ki
Dmax,i

su
ij
su
(cid:0)
ij
su
(cid:0)
ij
su
(cid:1)
ij

(cid:0)

− γD
(cid:1)
L
(cid:1)
−γD

(cid:1)

(cid:0)

ij, su
ij

sd
ij
Dmax,i
(cid:1)

ij + ∆sd
sd
ij, su
sd
− Di
ij
su
− γD
Dmax
(cid:0)
(cid:1)
ij
− DT
ij + ∆sd
sd
(cid:0)
(cid:1)
i
ij
su
− γD
(cid:0)
ij
Lcij
(cid:1)
sd
cij
ij
cij
(cid:1)
2
+cij
(cid:0)

(cid:0)
cij

(cid:0)
sd
ij

sd
ij

cij

×

×

∆sd
ij
ij + ∆sd
sd
(cid:0)
(cid:1)
ij
∆sd
(cid:0)
ij

cij
(cid:1)

(cid:1)

,

Here, when cij

(cid:1)

(cid:0)
∆sd
ij

≫cij

sd
ij

(cid:0)
,

(cid:1)

cij(sd

ij)2

(cid:1)
, and, consequently, ∆U d
i =

(cid:1)

(cid:0)

(cid:0)

ij)

1
cij(sd
Moreover, as cij

∆sd
ij

(cid:0)

(cid:1)
(cid:0)
cij(∆sd
ij)
ij)cij(∆sd
+cij(sd
Ki(su
ij)L

≈

ij)

(Dmax,i(su

ij)−γD)cij(sd

ij)

cij(∆sd
+cij(sd

ij)
ij)cij(∆sd

ij)

.

≈

. This completes

,

≪ cij

sd
ij
cij(sd
ij)Lcij(∆sd

ij)2
Ki(su
ij)
(cid:1)
(cid:0)
ij)−γD)cij(sd
(Dmax,i(su

ij)

ij)
2 . Thus, ∆U d

(cid:0)

(cid:1)
i =

cij(∆sd
cij(sd
ij)
the proof.

(cid:1)
∆sd
ij

.

(cid:1)

B. Proof of Proposition 1

(cid:17)

Sd − 1
|N (Vj )| − 1

To prove Proposition 1, we ﬁrst need to prove that
the number of actions for the users over the downlink is
. Since the SBSs will allocate all downlink
resource blocks to their associated users, the interference from
(cid:16)
each SBS to its associated users is unchanged when the actions
change. For example, the interference when SBS j allocates
resource block 1 to user i is the same as the interference when
SBS j allocates resource block 2 to user i. Therefore, we only
need to consider the number of downlink resource blocks allo-
cated to each user and, consequently, the number of actions for
the users over the downlink is
. Then, we need
to prove that the number of actions for the users over the uplink
(cid:16)

Sd − 1
|N (Vj)| − 1

. For each vector n, SBS j

Vj −1
Qi=1 (cid:18)
has
actions to allocate the resource blocks to the ﬁrst
user. Based on the resource blocks allocated to the ﬁrst user,
SBS j will have
actions to allocate the resource
blocks to the second user. For other associated users, the
number of actions can be derived using a similar method as the
method of the second user. Therefore, the number of actions

n2
Su − n1

i−1
k=1 ni (cid:19)

ni
Su −

Pn∈N (Vj )

n1
Su

P

is

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

for the users over uplink is

i−1
k=1 ni (cid:19)
and, hence, the number of actions for the users over uplink and

Pn∈N (Vj)

P

,

Vj −1
Qi=1 (cid:18)

ni
Su −

downlink is

(cid:18)
This completes the proof.

Sd − 1
|N (Vj )| − 1 (cid:19)

Vj −1
Qi=1 (cid:18)

ni
Su −

i−1
k=1 ni (cid:19)

.

P

Pn∈N (Vj)

C. Proof of Theorem 2

In order to prove this theorem, we ﬁrst need to prove that
the ESN-based learning algorithm converges to a constant
value. Here, we do not know the exact value to which the
proposed algorithm converges. Hence, our purpose is to prove
that the proposed algorithm cannot diverge. Then, we derive
the exact value to which the ESN converges. For i), based on
[33, Theorem 8], the conditions of convergence for an ESN
are: a) The ESN is k-step unambiguous and b) The ESN-based
learning process is k order Markov decision process (MDP).
Here, the deﬁnition of k-step unambiguous can be given by:

Deﬁnition 2. Given an ESN with initial state µ0,j, we
assume that the input sequence x0,j, . . . , xτ,j results in an
internal state µτ,j, and the input sequence x′
results in an internal state µ′
xτ −i,j = x′
unambiguous.

0,j, . . . , x′
τ ′,j
τ ′,j. If µτ,j = µ′
τ ′,j implies that
τ ′−i,j, for all i = 0, . . . , τ , then the ESN is k-step

Here, uτ,j = u′

τ ′,j can be rewritten as:

uτ,j − u′

τ ′,j = W j

τ ′−1,j

+ W in
j

xτ,j − x′

τ ′,j

µτ −1,j − µ′
w11

(cid:0)

= 




− 

wNwNw
W in
(cid:0)
j1

µτ −1,j1 − µ′
(cid:1)
...
(cid:0)
µτ −1,jNw − µ′
xτ,j − x′
...
jNw (xτ,j − x′

(cid:1)
τ ′,j)

τ ′,j

(cid:0)

τ ′−1,j1

(cid:0)

(cid:1)

τ ′−1,jNw

(cid:1)






(cid:1)

,



k

(cid:17)




< 2.

µτ −1,jk − µ′
(cid:16)

W in


where µτ −1,jk is element k of µτ −1,j and µ′
τ ′−1,jk element k
of µ′
τ ′−1,j. Since the tanh function in (15) ranges from -1 to 1,
is 2. As wkk ∈

the maximum value of

τ ′−1,jk
µτ −1,jk − µ′
wkk
τ −1,jk
τ,j) ≥ 2, then µτ,j −µ′
(cid:16)

i.e., µτ −1,j, . . . , µτ −k,j. Moreover,

(−1, 1) , k = 1, . . . , Nw, max
In this case, if W in
jk (xτ,j − x′
(cid:17)
τ ′,j 6= 0.
τ ′,j = 0, then µτ −1,j = µ′
Therefore, if µτ,j − µ′
τ ′−1,j and
xτ,j = x′
τ ′,j. In this case, an ESN is k-step unambiguous
when W in
ji (xτ,j − x′
τ,j) ≥ 2. Since the dynamic reservoir
can only store limited ESN information [30], the dynamic
reservoir state µτ,j only depends on the ﬁnite past reservoir
states,
the number of
reservoir states and actions in the proposed algorithm is ﬁnite.
Therefore, the proposed ESN-based algorithm is a k order
MDP and, hence, condition 2) is satisﬁed. For case 2), if
the learning rate of the proposed algorithm satisﬁes Robbins-
Monro conditions and the proposed algorithm is a k order
MDP, the proposed algorithm will satisfy the conditions in
[31, Theorem 1] and, hence, converges to a region. For both
cases i) and ii), based on [31, Theorem 1], the proposed ESN-
based learning algorithm will converge to the utility value, ˆuj.
This completes the proof.

REFERENCES

[1] M. Chen, W. Saad, and C. Yin, “Resource management for wireless
virtual reality: Machine learning meets multi-attribute utility,” in Proc.
of IEEE Global Communications Conference (GLOBECOM), Singapore,
Dec. 2017.

[2] P. Rosedale,

“Virtual reality: The next disruptor: A new kind of
worldwide communication,” IEEE Consumer Electronics Magazine, vol.
6, no. 1, pp. 48–50, Jan. 2017.

[3] HTC, “HTC Vive,” https://www.vive.com/us/.
[4] E. Ba¸stu˘g, M. Bennis, M. Médard, and M. Debbah, “Towards inter-
connected virtual reality: Opportunities, challenges and enablers,” IEEE
Communications Magazine, vol. 55, no. 6, pp. 110–117, June 2017.
[5] J. Wu, B. Cheng, C. Yuen, Y. Shang, and J. Chen, “Distortion-aware
concurrent multipath transfer for mobile video streaming in heteroge-
neous wireless networks,” IEEE Transactions on Mobile Computing,
vol. 14, no. 4, pp. 688–701, April 2015.

[6] J. Wu, C. Yuen, N. M. Cheung, J. Chen, and C. W. Chen, “Modeling
and optimization of high frame rate video transmission over wireless
networks,” IEEE Transactions on Wireless Communications, vol. 15,
no. 4, pp. 2713–2726, April 2016.

[7] F. Qian, L. Ji, B. Han, and V. Gopalakrishnan, “Optimizing 360 video
delivery over cellular networks,” in Proc. of All Things Cellular (ATC)
Workshop on Operations, Applications and Challenges, New York, USA,
Oct. 2016.

[8] C. Richardt, Y. Pritch, H. Zimmer, and A. Sorkine-Hornung,
“Megastereo: Constructing high-resolution stereo panoramas,” in Proc.
of the IEEE Conference on Computer Vision and Pattern Recognition,
Portland, OR, USA, June 2013.

[9] S. Sridhar, F. Mueller, M. Zollhöfer, D. Casas, A. Oulasvirta, and
C. Theobalt, “Real-time joint tracking of a hand manipulating an object
from RGB-D input,” in Proc. of European Conference on Computer
Vision, Amsterdam, Netherlands, Oct. 2016.

[10] A. Rovira and M. Slater,

to
make people move to a speciﬁc location in immersive virtual reality,”
International Journal of Human-Computer Studies, vol. 98, pp. 89–94,
Feb. 2017.

learning as a tool

“Reinforcement

[11] T. Pfeiffer and C. Memili,

“Model-based real-time visualization of
realistic three-dimensional heat maps for mobile eye tracking and eye
the Ninth Biennial ACM
tracking in virtual reality,”
Symposium on Eye Tracking Research & Applications, Charleston, South
Carolina, Mar. 2016.

in Proc. of

[12] W. C. Lo, C. L. Fan, S. C. Yen, and C. H. Hsu,

“Performance
measurements of 360◦ video streaming to head-mounted displays over
live 4G cellular networks,” in Proc. of Asia-Paciﬁc Network Operations
and Management Symposium (APNOMS), Seoul, South Korea, Sept.
2017.

[13] O. Abari, D. Bharadia, A. Dufﬁeld, and D. Katabi, “Cutting the cord
in virtual reality,” in Proc. of the ACM Workshop on Hot Topics in
Networks, Atlanta, GA, USA, Nov. 2016.

[14] A. E. Abbas, “Constructing multiattribute utility functions for decision
analysis,” INFORMS Tutorials in Operations Research, pp. 62–98, Oct.
2010.

[15] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong,
“Caching in the sky: Proactive deployment of cache-enabled unmanned
aerial vehicles for optimized quality-of-experience,” IEEE Journal on
Selected Areas on Communications (JSAC), vol. 35, no. 5, pp. 1046–
1061, May 2017.

[16] S. B. Chekroun, E. Sabir, A. Kobbane, H. Tembine, E. H. Bouyakhf,
and K. Ibrahimi,
“A distributed open-close access for small-cell
networks: A random matrix game analysis,” in Proc. of International
Wireless Communications and Mobile Computing Conference (IWCMC),
Dubrovnik, Croatia, 2015.

[17] M. Bennis, S. Guruacharya, and D. Niyato,

“Distributed learning
strategies for interference mitigation in femtocell networks,” in Proc. of
IEEE Global Communications Conference (GLOBECOM), Kathmandu,
Nepal, Dec. 2011.

[18] Q. Zhu, W. Saad, Z. Han, H. V. Poor, and T. Ba¸sar, “Eavesdropping
and jamming in next-generation wireless networks: A game-theoretic
approach,” in Proc. of Military Communications Conference, Baltimore,
MD, USA, 2011, pp. 119–124.

[19] J. G. Andrews, S. Buzzi, W. Choi, S. V. Hanly, A. Lozano, A. C. K.
Soong, and J. C. Zhang, “What will 5G be?,” IEEE Journal on Selected
Areas in Communications, vol. 32, no. 6, pp. 1065–1082, June 2014.

[20] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, “Unmanned aerial
vehicle with underlaid device-to-device communications: Performance
and tradeoffs,” IEEE Transactions on Wireless Communications, vol.
15, no. 6, pp. 3949–3963, June 2016.

[21] G. C. Burdea and P. Coiffet, Virtual Reality Technology, John Wiley &

Sons, 2003.

[22] M. Luko˘sevicius, A Practical Guide to Applying Echo State Networks,

Springer Berlin Heidelberg, 2012.

[23] J. F. Yang, S. C. Chang, and C. Y. Chen, “Computation reduction for
motion search in low rate video coders,” IEEE Transactions on Circuits
and Systems for Video Technology, vol. 12, no. 10, pp. 948–951, Oct.
2002.

[24] “HTC Vive install support,” https://support.steampowered.com/kb_article.php?ref=2001-UXCM-4439.
[25] L. Wei, J. Cai, C. H. Foh, and B. He, “QoS-aware resource allocation
for video transcoding in clouds,” IEEE Transactions on Circuits and
Systems for Video Technology, vol. 27, no. 1, pp. 49–61, 2017.

[26] D. C. Montgomery, E. A. Peck, and G. G. Vining, Introduction to Linear

Regression Analysis, John Wiley & Sons, 2015.

[27] A. M Mood, “Introduction to the theory of statistics.,” 1950.
[28] Z. Han, D. Niyato, W. Saad, T. Basar, and A. HjÃ¸rungnes, Game
Theory in Wireless and Communication Networks: Theory, Models, and
Applications, Cambridge University Press, 2012.
[29] G. Bacci, S. Lasaulce, W. Saad, and L. Sanguinetti,

“Game theory
for networks: A tutorial on game-theoretic tools for emerging signal
processing applications,” IEEE Signal Processing Magazine, vol. 33,
no. 1, pp. 94–119, Jan. 2016.

[30] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Machine
learning for wireless networks with artiﬁcial
intelligence: A tutorial
on neural networks,” available online: arxiv.org/abs/1710.02913, Oct.
2017.

[31] M. Chen, W. Saad, and C. Yin, “Echo state networks for self-organizing
resource allocation in LTE-U with uplink-downlink decoupling,” IEEE
Transactions on Wireless Communications, vol. 16, no. 1, January 2017.
[32] R. Sutton and A. Barto, Reinforcement Learning: An Introduction, MIT

Press, 1998.

[33] I. Szita, V. Gyenes, and A. L˝orincz, “Reinforcement learning with echo
state networks,” Lecture Notes in Computer Science, vol. 4131, pp.
830–839, 2006.

[34] Oculus,

“Mobile

VR

media

overview,”

https://developer3.oculus.com/documentation/mobilesdk/latest/concepts/mobile-media-overview/.

[35] M. Bennis, S. M. Perlaza, P. Blasco, Z. Han, and H.V. Poor, “Self-
organization in small cell networks: A reinforcement learning approach,”
IEEE Transactions on Wireless Communications, vol. 12, no. 7, pp.
3202–3212, June 2013.

[36] F. Kelly,

“Charging and rate control for elastic trafﬁc,” European

Transactions on Telecommunications, vol. 8, no. 1, pp. 33–37, 1997.

[37] M. Bennis and D. Niyato, “A Q-learning based approach to interference
in Proc. of IEEE
avoidance in self-organized femtocell networks,”
Global Commun. Conference (GLOBECOM) Workshop on Femtocell
Networks, Miami, FL, USA, Dec. 2010.

[38] M.

Abrash,

almost
https://media.steampowered.com/apps/abrashblog/Abrash%20Dev%20Days%202014.pdf.

certainly

within

be

“What
will

VR

could,

should,
two

and
years,”

