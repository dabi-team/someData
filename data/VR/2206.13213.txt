2
2
0
2

n
u
J

7
2

]

C
H
.
s
c
[

1
v
3
1
2
3
1
.
6
0
2
2
:
v
i
X
r
a

Immersive and Interactive Visualization of 3D
Spatio-Temporal Data using a Space Time
Hypercube Applied to Biological Imaging Data

Gwendal Fouch´e1, Ferran Argelaguet1, Emmanuel Faure2, and Charles
Kervrann3

1 Inria, Univ Rennes, IRISA, CNRS, Rennes, France.
2 LIRMM, Univ Montpellier, CNRS, Montpellier, France.
3 Inria Centre Rennes-Bretagne Atlantique, UMR144 CNRS Institut Curie, PSL
Research University, Sorbonne Universit´es, Paris, France

Abstract. We propose an extension of the well-known Space-Time Cube
(STC) visualization technique in order to visualize time-varying 3D spa-
tial data, taking advantage of the interaction capabilities of Virtual Real-
ity (VR). The analysis of multidimensional time-varying datasets, which
size grows as recording and simulating techniques advance, faces chal-
lenges on the representation and visualization of dense data, as well as
on the study of temporal variations. First, we propose the Space-Time
Hypercube (STH) as an abstraction for 3D temporal data, extended
from the STC concept. Second, through the example of embryo develop-
ment imaging dataset, we detail the construction and visualization of a
STC based on a user-driven projection of the spatial and temporal infor-
mation. This projection yields a 3D STC visualization, which can also
encode additional numerical and categorical data. Additionally, we pro-
pose a set of tools allowing the user to ﬁlter and manipulate the 3D STC
which beneﬁts from the visualization, exploration and interaction possi-
bilities oﬀered by VR. Finally, we evaluated the proposed visualization
method in the context of the visualization of spatio-temporal biological
data. Several biology experts accompanied the application design to pro-
vide insight on how the STC visualization could be used to explore such
data. We report a user study (n=12) using non-expert users performing
a set of exploration and query tasks to evaluate the system.

Keywords: Spatio-Temporal Visualization · Dimension Reduction · Space-
Time Cube · Interaction · Virtual Reality

1

Introduction

Immersive Analytics is an emerging research topic which “investigates how new
interaction and display technologies can be used to support analytical reason-
ing. [...] Immersive Analytics builds on technologies such as large touch surfaces,

 
 
 
 
 
 
2

G. Fouch´e et al.

Fig. 1. From a cross-section on the 3D surface-based temporal data shown on the ﬁrst
ﬁgure, we generate a Space-Time Cube visualization, displayed in the second image,
showing the evolution over time of the spatial data of the cross-section displayed on
the x and y axes. The third picture shows how the visualization can be enriched with
quantitative and qualitative data using diﬀerent color coding. A set of interaction tools
help the user to explore the generated visualization, as seen in the last image.

immersive Virtual and Augmented Reality environments, haptic and audio dis-
plays, and modern fabrication techniques” [11]. Immersive analytics, as reported
by Skarbez et al. [37], leverages large ﬁeld of view displays, stereoscopic render-
ing and rich spatial 3D interactions in order to increase spatial understanding,
decrease information clutter and help with visual analytics tasks. In particular,
immersive 3D environments can help with ﬂexible exploration and interaction
with complex time-varying 3D spatial data. For example, the enhanced depth
perception oﬀered by stereoscopy in Virtual Reality improves the visualization
of details in volumetric data [23], while, having a large workspace allows for the
juxtaposition of multiple coordinated and interactive views [21].

Methods from the ﬁeld of Immersive Analytics could notably be beneﬁcial for
biological imaging, where large time-varying datasets are more and more present,
and have emerging constraints from the growing spatio-temporal resolutions. For
instance, Lattice Light-Sheet Microscopy[12] allows the capture of volumes at a
resolution of a few hundred nanometers at a few seconds interval. This technology
is notably used for recording embryo development[15], creating terabytes of raw
data, in contexts of analysis in embryology, morphodynamics, and intracellular
dynamics. In such context, extracting surface meshes to describe volumes (see
Figure 1, left) is a commonly used solution in order to display and analyze data,
as proposes the web-based application MorphoNet [25]. The output dataset is
composed a group of objects deﬁned by their shapes, that evolve over time.
Moreover, spatio-temporal numerical and categorical data are often available
for these datasets, for example, cell information (e.g. volume, lifespan, name,
genetic information) can be available for each time step. Thus, visualization and
interaction for analyzing this type of time-varying multidimensional datasets
become challenging as well. Widely used software in biology imaging, such as
ImageJ [34] or Imaris [33], only propose very basic non-interactive 3D viewers.

tnt1t0xytimeMinMaxCellVolumeImmersive Space-Time Hypercube Visualization for Biological Imaging

3

The main objective of this paper was to propose visualization methods en-
abling the comparison of 3D spatio-temporal data in a more direct way. For this
purpose, we propose an extension of the Space-Time Cube (STC) visualization
technique [7] to 4D spatio-temporal data, named hereinafter Space-Time Hyper-
cube (STH). Conceptually, the STH considers that the data to visualize lays in
a 4D hypercube with three spatial dimensions and one temporal dimension. To
enable the direct visualization of the 4D hypercube, we extended the classical
operations of the STC to generate varied meaningful 3D visualizations, that are
juxtaposed to obtain a compact overview of spatio-temporal data. As an exam-
ple, we propose a projection operation on the hypercube on datasets of embryo
development 3D temporal imagery. The method is independent of the type of
spatial representation (e.g. mesh, volumetric), and projects the hypercube into
a 3D volume that can be directly visualized. The projection operation relies
on a user-deﬁned cross-section on the spatial dimension. This cross-section is
computed along the temporal dimension and stacked into a 3D volume, hence a
STC, which can be enriched with color mapped data (see Figure. 1).

Due to the complexity of the visualized structures and the need for 3D inter-
action methods to deﬁne the projection operation, we proposed the use of this
method in an immersive context to leverage the beneﬁts of immersive analyt-
ics. We expected that the enhanced depth perception would improve the visual
extraction of meaningful structures of the generated STC, and the use of 3D
user interfaces would allow users to explore naturally the STC from diﬀerent
perspectives, as suggested in several works, in general cases [28] and speciﬁcally
for STCs [6,40,42]. Furthermore, the enlarged interaction space of VR allows the
juxtaposition of the STC and snapshots of the original spatio-temporal data, en-
abling the synchronized exploration and manipulation of the two visualizations.
In order to provide adequate tools for basic analysis of our use case data, we con-
sidered the input from domain experts to improve the design of the application.
Finally, the paper presents a use-case illustrating the usages of the STH to gen-
erate meaningful visualization of spatio-temporal data in embryo developmental
studies, as well as an evaluation of this method with non-expert users.

In summary, the main contributions of the paper are:

– An extension of the well-known Space-Time Cube technique as an abstrac-

tion of 3D temporal data, named Space-Time Hypercube;

– A projection operator of the 4D STH to generate a 3D STC that can be

visualized in an Immersive Analytics application;

– A formal evaluation of the proposed method on embryology use cases.

2 Related Work

Time-varying data are more and more present in scientiﬁc visualization, and
numerous methods have been developed to visualize the time component and
space-time relationships. Such methods can focus on the visualization of point
clouds [26,29], more [32,18,27,20] or less [3,4] large trajectory datasets, simulated

4

G. Fouch´e et al.

ﬂow data [43,35] and surface or volumetric datasets [22]. Immersive visualization
methods are also becoming commonplace [19,17]. However, due to the vast liter-
ature in this domain, we constrain the state of the art to visualization methods
adapted to datasets representing time-varying 3D spatial data with a particular
focus on Space-Time Cube visualizations methods, which is the main scope of
the paper. For a comprehensive review of spatio-temporal visualization methods,
we refer the reader to the following comprehensive works [1,5,9].

2.1 Visualization of Time-varying Volume Data

A number of approaches exist to visualize time-varying data: dynamic methods,
implying animation and interaction, and static visualizations of either the full
data, or extracted lower-dimension data.

Dynamic Visualizations. Animation is an intuitive method to explore
time evolution in 3D temporal datasets. In designs using animation, time in-
creases automatically, showing the evolution of the data through time. Several
works mention the use of animated and interactive design choices for time ex-
ploration [14,2], yet animation can be less adequate for comparison tasks [22]
or the analysis of space-time-value relationships [44]. There is a higher cognitive
load for the user who has to remember the state of the data between diﬀerent
moments, which can limit the observation of details, as the slogan “Eyes Beat
Memory” suggests. Some methods have been proposed to cope with this issue,
such as time-warping in animation [38], yet, a more intuitive solution lies in
static visualizations.

Static Visualizations. Woodring and Shen [44] proposed a solution for
static temporal visualization for time-varying volumetric data, based on set and
numerical operations to ﬁlter the displayed information, reduce occlusion and
focus on points of interest. A color mapping can be applied to overlay chrono-
logical information [44]. With this method, the temporal evolution is visualized
by merging the spatio-temporal data into a volume. Although, there is a com-
promise to make between occlusion and the amount of displayed data, it is less
suited for dense data. Alternatively, in order to reduce the data to display, re-
gions of interest can be deﬁned either manually or automatically. For example,
Lu and Shen [29] used a dissimilarity measure to extract single time steps in the
volumetric temporal data, eventually displaying only the essential time steps on
a color mapped timeline. In the context of a more recent work by Johnson et
al. [21] proposed BentoBox, a Virtual Reality data visualization interface for
simulated volumetric and temporal data, which juxtaposes several instances of a
volumetric dataset under diﬀerent parameters, in an array layout. BentoBox dis-
poses of a range of tools focused on the comparison of datasets and manipulation
of parameters, using animations, color mapping and 3D bimanual interactions.
Finally, a versatile visualization technique which enables the compact visu-
alization and manipulation of time-varying data, is the Space-Time Cube which
is detailed in the following section.

Immersive Space-Time Hypercube Visualization for Biological Imaging

5

2.2 Space-Time Cube Visualization

First introduced by H¨agerstrand [16] as time-space volume and paths in a context
of socio-economic study, the Space-Time Cube (STC ) is a representation using
two axes for data and a third axis for time. It is used for numerous representa-
tions of time-varying data, would it be for geometrical illustration, geographic
data [40,24] or trajectories[42]. Bach et al. [7] summarized, in their review of
temporal visualizations based on the STC, diﬀerent types of operations that can
be applied to visualize data in a STC. Cutting operations allow the extraction
of an image at a particular moment in time, as short exposure photography, or a
particular plane in the data space, a slice of the cube. On contrary, ﬂattening op-
erations collapse the STC along an axis to obtain a 2D representation, when the
time is collapsed, it can be depicted as a long exposure photograph. In addition,
depending on the data and the visualization design, 3D rendering, interpolation,
volume extraction, non-orthogonal or non-planar operations can also be used.
Later work by the same authors [6] generalized the notion of STC visualization,
modelling various visualization methods as associations of operations on a STC.
Among the topics emerging from STC visualizations, two kept our attention.
First, higher dimensions in the data, such as 3D temporal data, can be visual-
ized as a higher dimension Space-Time Hypercube. In the case of spatio-temporal
data, operations of cutting, corresponding to a projection of the 4D data, could
yield a visualizable 3D image. If the idea of a higher dimension STC was evoked
in Bach et al. [7,6] reviews, this lead has not been thoroughly explored. Among
the works that are the closest to it, Woodring et al. [45] proposed a method to
generate and render 3D slices based on an arbitrary hyperplane equation. The
authors gave a few guidelines to interpret the output visualization with par-
ticular value of the hyperplane equation [45]. However, the intuitiveness of the
equation parameterization and concrete interpretation of the visualization have
not been formally evaluated.

The second emerging topic from STC visualizations is interactivity. If Bach
et al. [6] advice against operations of 3D rendering of STCs, it seems that inter-
action and navigation methods, notably with immersive technologies [8] could
help in solving some of the diﬃculties evoked [6], such as occlusion, depth ambi-
guity and perspective distortion. This would help take advantage of the general
overview of the data that this operation can oﬀer. STC visualization can also
take advantage to direct interactions in immersive environments. For instance,
Filho et al. [42] implemented a STC displaying trajectory datasets in immer-
sive environment, with a basic interaction tool set. They report that it helps
in usability and partially addresses the issue of the steep learning curve of the
visualization method.

This literature review highlighted the lack of methods for the visualization
of 3D spatio-temporal data, and especially for surface-mesh based datasets [22].
The use of the STH, although slightly explored in the literature, showed a high
promise in order to produce various meaningful views with reduced dimension-
ality but with the potential drawback of generating complex and cluttered vi-
sualizations. However, the beneﬁts of Virtual Reality systems in terms of depth

6

G. Fouch´e et al.

perception and interaction can overcome such limitations. McIntire et al. [30] re-
port in their review that stereoscopic displays are better than desktop displays
for tasks of identiﬁcation and classiﬁcation of objects in cluttered static environ-
ments, overall complex spatial manipulation and spatial understanding. These
are the type of tasks that can be expected during the exploration of complex
static visualization, such as STCs, by analysts.

3 Space-Time Hypercube Visualization

This section presents our main visualization method, based on the Space-Time
Cube. First, we present a generalization of the STC in 4 dimensions, the STH,
then we detail the design choices for the generation and visualization, as well as
the interaction methods available.

As a support example for this section, we will use a dataset of a time-varying
3D spatial data, a 6-hour-long live recording of an embryo development. In this
work, we use 100 of the 180 time points recorded, from a 65 to 383 cells embryo.
Figure 1 shows some of the time steps that were used. The dataset is composed
of surface meshes, for each cell at each time step. The objects are close together,
resulting in a quite dense view. The tracking of cells in time, notably after
division, are also present, as well as quantitative and qualitative information.
Movements between time points are of a lower order of magnitude than the size
of the cells. The dataset will be described more thoroughly in Section 4.

3.1 Generalization in 4D

In a classic 3D STC, the data visualized is composed of 2 spatial dimensions
as well as a temporal dimension (x, y, t). In contrast, a STH can contain 3 spa-
tial dimensions and a temporal component (x, y, z, t). With this abstraction,
any elementary operations applicable on a STC, notably described by Bach et
al. [7,6], should be extendable and applicable to the STH. Geometry and con-
tent transformations now modify a 4D volume, ﬁlling operations now includes 4D
interpolation, extraction operations can render 4D volumes and ﬂattening oper-
ations can compress 4D data into a 3D volume. Complex operations for a STH
can be designed based on this extended set of operations, in order to provide
meaningful visualizations of the data, but with varied points of view and di-
mensionality, notably including time. Although, several constraints appear with
this additional dimension. First, while a STC can be directly rendered, a STH
cannot be directly rendered as is. To enable a direct visualization, a projection
operation on the STH has to be deﬁned in order to yield a 3D visualization.
Second, the design of a 4D operation can be diﬃcult because of the diﬃculty to
mentally visualize a 4D volume.

We tried to design such operation on our example dataset. Initially, we consid-
ered a ﬂattening operation on the spatial dimension, projecting the 3D snapshots
of the dataset into 2D planes, in order to reduce the dimension of the STH by
one. However, because of the topology of our example dataset, using this method

Immersive Space-Time Hypercube Visualization for Biological Imaging

7

Fig. 2. Flow diagram of the STC generation. In step 1○, the user places the interactive
clipping plane to get the desired cross-section. In step 2○, camera parameters are
automatically set in order to render the cross section at each time point. The image
presents the output of the rendering operation, using the RGB channels to save cell
identiﬁers. Stacking the rendered images yields a 3d volume, as shown in step 3○. Each
voxel contains a cell identiﬁer, and its depth indicates a time point ti.

on such dense data can result in a volume which would be prone to occlusion
issues, because of the increased density due to data aggregation. In contrast, a
volume extraction operator would enable the reduction of the data dimension
without the increased data density. Thus, we opt for a volume extraction op-
erating by deﬁning a hyperplane laying in a 4D space. Nevertheless, two major
issues remain:

– An extraction operation implies a loss of data during the creation of the
visualization. Thus, the user must have control over which data is selected.
– The concept of a 4D hypercube is not easy to comprehend; the extraction
of a volume along a hyperplane is even more diﬃcult. The complexity of the
model will be a constraint during the extraction operation, as well as any
other interactive operation based on the 4D data.

These two constraints will inﬂuence our design choices, especially the gener-

ation process for our 3D STC visualization.

3.2 Extracting a 3D STC from a 4D STH

The proposed projection method relies on the use of a 4D hyperplane, manually
deﬁned by the user, to extract the 3D visualization. However, the deﬁnition
of a hyperplane of this nature is an abstract task which cannot be directly
visualized. A simple example would be to deﬁne a hyperplane perpendicular
to the time axis which would result on the extraction of the spatial data at a
given time point. In order to ease the process and obtain a hyperplane yielding
more informative STCs, we propose a user deﬁned approach based on the cross-
section, a commonly used tool in scientiﬁc visualization to explore spatial 3D
data, extracting a 2D view from 3D data.

Precisely, the user can place an interactive clipping plane, that will act as
a cutting plane, as they would do to get a cut-away view of 3D spatial data.

8

G. Fouch´e et al.

With the same analogy that we used to consider our spatio-temporal data as
a 4D volume, the plane, as a time-varying object in the 3D space, can be con-
sidered as a hyperplane in the 4D space. The operation thus corresponds to a
projection such as (x, y, z, t) → (x(cid:48), y(cid:48), t), with x(cid:48), y(cid:48) the projected coordinates
on the cutting plane. Such projection avoids any spatial distortion that could
impair the interpretation of the output shape. Considering the whole dataset as
a 4D Space-Time Hypercube, it sums up as a space cutting [7] by a hyperplane,
extracting a 3D space-time cube.

The proposed generation method is data agnostic (mesh-based or volumetric)
and its three main steps are illustrated in Figure 2. 1○ The user places a clipping
plane on the model at any time point in order to deﬁne the “hyperplane” which
will determine the projection. 2○ For each time step, a 2D image of the cross-
section of the 3D model is computed. The rendering is achieved by setting an
orthogonal camera perpendicular to the clipping plane and setting the near and
far planes at a distance of (cid:15) = (znear − zf ar)/2 towards the clipping plane. The ﬁeld
of view of the virtual camera is minimized according to the maximum size of the
cross-section over time. The color channel of the rendered image encodes objects
(cells in our examples) identiﬁers, which allows indexing numerical or categor-
ical data during visualization. This requires object information, which implies
potentially complex segmentation processing in the case of biology imaging. If
this type of information is not available, raw position data can be encoded in
the volume either way. 3○ The output images are stacked into a 3D texture in
which the RGB channel is used to keep track of the object identiﬁer. The depth
coordinate for each pixel encodes the time step. The use of object identiﬁers is
considered for convenience when the spatio-temporal data has this information.
In case that this information is not available, other information could be stored,
such as density values from a CT or fMRI volumetric dataset.

3.3 Visualization: 3D rendering and quantitative data

The projection operator generates a 3D texture which can be directly rendered
using Direct Volume Rendering methods [41]. Due to the nature of our datasets,
we consider an opaque rendering avoiding semi-transparency, in order to limit
color distortion when using color to encode information. We encode numerical
or categorical information that has to be rendered in textures that can be pre-
computed. This way, using the object identiﬁer, such information can be mapped
directly on the volumetric visualization, and can also be switched at low cost.
We apply a Blinn-Phong illumination model on the cube to enhance depth cues
and get a better understanding of the shape of the volumetric data. To reduce
the rendering time, normals are pre-computed during the generation of the STC
and stored in an additional 3D texture. Normals are computed using 3D Sobel-
Feldman gradient operators on each direction. The rendered STCs, before and
after color mapping, are shown in the second and third images of Figure 1.

However, the STC could still contain a lot of information and could present a
complex layout. The following section details manipulation techniques proposed
in order to improve the exploration of the STC.

Immersive Space-Time Hypercube Visualization for Biological Imaging

9

Fig. 3. STC (right) and meshed model (left) visualizations in the VR framework. On
the laser selector, a list of available information allowed the user to color map the
remaining lifespan of cells on the visualizations. A cell is selected on the meshed model.
The name of the cell is displayed on the pointer, and feedback appears on the STC,
highlighting the lineage of the cell.

3.4 Interaction

Interaction will be a key concept to compensate for issues implied by direct
rendering, such as occlusion. Bach et al.[7,6] described several generic operations
that could be applied to a STC, and mentioned dynamic uses of those operations.
In addition, by displaying the STC in a VR environment, additional depth cues
will be available, such as binocular cues and motion parallax cues [28], which
will increase the identiﬁcation of the intricate structures in the STC.

In the ﬁrst place, as previously mentioned, the STC is interactively generated,
since the user can choose the base cross-section, and thus the hyperplane, and
start the generation process at run time, doing a space-cutting operation of the
4D data. Taking advantage of immersive environment in terms of interaction,
we designed a number of interactions adapted to our data structure and the
beneﬁts it would imply for data visualization. All the techniques described in
the following sections assume that the user is wearing a head mounted display
(HMD) and holding by the hands two 6DoF hand-held controllers. As such, the
user will be able to explore the STC by just naturally walking in the virtual
environment and use both hands to use the diﬀerent tools. We refer the reader
to the accompanying video which showcases the diﬀerent interaction tools.

Exploration tools The tools described in this section are meant to help the
user explore potentially dense and occluded volume representations.

Light: The user can grab and move a point light source, represented as a
yellow sphere. This helps understand the shape of the volumetric data by adding
depth cues, reveal shape, reliefs and details of the STC.

x, ytime10

G. Fouch´e et al.

Clipping Plane: An interactive clipping plane generate cut-away views of
the STC. The user can directly grab the clipping plane and manipulate it using
remote hand translations and rotations. Some orientations are easier to interpret:

– Time cutting, i.e. putting the plane perpendicularly to the Z-axis, corre-

sponds to going through the original cross-section at given instants.

– Linear space cutting, i.e. putting the plane along the Z-axis, corresponds to
the evolution of a segment of the spatial data over time, giving for instance
information of the evolution of the spatial structure.

– In case of moving objects, slightly oblique cutting can help track the object

over time. This type of cut was used to generate the cut in Figure 4.

Selection operator: The user can point and display summarized information
of a hovered object, such as its identiﬁer. This is a detail-on-demand operator
that can select and display additional information on the object on click.

Switching the color mapped information: When various numerical or categor-
ical information are present in the dataset, the user can scroll through the list of
information displayable (cf Figure 3). As detailed in section 3.3, the underlying
operation corresponds to switching the texture containing the information. This
is thus a light operation allowing interactive use.

Filtering Operations As occlusion remains an issue in dense volumetric data,
reducing the amount of data displayed becomes necessary.

Value Filter: This tool allows the user to ﬁlter the range of continuous data
displayed on the STC by using a slider. Filtering on value can help identify
groups of objects sharing same characteristics, complementing the color mapping
by removing the occluding other objects.

Time Filter: This ﬁltering operation lets the user deﬁne a temporal window
to constraint the data rendered in the STC within two time steps, selected using
a slider. Events ongoing during this speciﬁc temporal window can then be more
easily pinpointed.

Object Filter and Tracking: Focusing on objects, two issues came up. First, we
need to keep track of objects in the temporal dimension. Second, the presence
of numerous objects can occlude the visualization. The user can solve these
issues respectively by highlighting or hiding the objects. It can be done either by
pointing the object using the same tool that gives details on demand, described
in 3.4, or by selecting an object in a list of identiﬁers. The selected object thus
changes state —from normal to highlighted to masked —upon clicking the trigger
of the controller. If the objects follow a tree hierarchy in time, such as the cells
with their children cells in our example dataset, the operation is applied to the
children objects. Figure 3 shows an example of object tracking. It should also
be noted that these options can be used only if object and tracking information
are present in the dataset.

Linking Multiple Visualizations We also took advantage of the large workspace
oﬀered by virtual environments to display the meshed model in addition to the

Immersive Space-Time Hypercube Visualization for Biological Imaging

11

STC. We followed advice from Munzner [31] for the design choices implied by
juxtaposition: multiple views should be coordinated, and even interactively co-
ordinated. Consequently, the operations we described above are applied to the
STH, and feedback of these operations is given through the two visualizations
displayed - the STC and the meshed representation. Links can be set up between
the visualization by applying a shared encoding of the eﬀect of the operations.
Linking the time exploration: In addition to the time ﬁltering sliders evoked
previously, a middle cursor controls the current time displayed on the meshed
model. Feedback of this cursor appears as a transparent plane orthogonal to the
temporal axis of the STC, as shown in Figure 3.

Linking the spatial exploration tools: The same color mapping is applied
to the STC and meshed model, notably after switching the color mapped in-
formation interactively, as shown in Figure 3. The clipping plane used on the
meshed model to generate the STC can go back on click to a default position,
which is the one of the displayed STC’s base cross-section. A marker line on this
cross-section gives context information about the position of the clipping plane
applied to the STC. This line corresponds to the intersection between the STC’s
clipping plane and the temporal feedback plane.

Linking the ﬁlters: Any ﬁltering operation done on the STC is also applied
to the meshed model, and vice versa. The same objects are highlighted, hidden
or ﬁltered out by time or value.

These operations were designed to help users to explore temporal evolution
with the static visualization oﬀered by the STC, and correlate the multidimen-
sional data or pinpoint events by combining the two visualizations and the in-
teractive tools available, as illustrated with the following use case.

3.5 Performance

The VR application runs on Unity 2018.2.21 and is supported by a PC with
Windows 10, an Intel Xeon W-2104 CPU (4 cores, 3.2 Ghz base frequency) and
a RTX 2080 GPU. All interaction methods described previously are designed in
order to maintain a framerate above 45 fps.

In terms of performance and texture resolution, the STC used in the ﬁgures
has a resolution of 256x256x100. The full generation process takes about 5 sec-
onds. This process is GPU-friendly, which enables the generation of the STC
during runtime. Normals are for instance generated and saved instantly using
compute shaders. However, the bottleneck of the current implementation of the
generation algorithm is the CPU - GPU transfer, provoking a framerate drop.

4 Use Case: Application for Morphogenesis

This section presents an application of the STH to the visualization and analysis
of spatio-temporal data from the recordings of embryonic developments of a
tunicate, a marine invertebrate. We present the studied dataset, which has been
already shown in the previous section, and detail a VR application in which

12

G. Fouch´e et al.

the STH has been integrated. Finally, we describe two use cases illustrating the
potential beneﬁts of the STH.

4.1 Presentation of the Dataset

The data set used to illustrate the STH data is a live recording of the embryonic
development of the Phallusia mammillata, a marine invertebrate animal of the
ascidian class.

The embryo was imaged every two minutes for six hours using confocal multi-
view light-sheet microscopy, generating a 4D dataset with isotropic spatial reso-
lution of several terabytes [15]. The data was then segmented using the ASTEC
pipeline (Adaptive Segmentation and Tracking of Embryonic Cells) [15] and
meshed using the VTK library [36] and MeshLab [13], producing a surface-based
spatio-temporal dataset of a few hundred megabytes representing the embryo.
Examples of the surface meshed model of some of the 180 time points recorded
are shown in Figure 1.

In addition, the ASTEC pipeline extracted global lineage trees of the embryo,
i.e. the tracing of cellular genealogy derived from cell divisions and migration.
Other categorical or continuous data, such as the volume of cells or the remain-
ing lifespan - i.e time before the next division - were computed or added by
the community of experts working on the dataset. We also had access to the
ANISEED[10] database, providing us with categorical information about the
expression of various genes in the cells.

4.2 VR Visualization Application

Taking advantage of the large workspace oﬀered by immersive environments, we
developed a VR framework for the user to interact with the meshed model, in
addition to the STC visualization. We based our environment on the framework
MorphoNet [25], an online interactive browser for the exploration of morpholog-
ical data. The development of the application was done using Unity 3D and the
HTC Vive was the main visualization and interaction system.

Our framework displays the dataset described above using meshed model of
the embryo at each time point, as well as a STC based on a cross-section placed
close to the median plan of the embryo, as shown in Figure 5. The color mapped
on the visualizations here corresponds to the volume of the cells.

Regarding the user interface, a virtual desk contains all the available tools
that the user could grab using the trigger button of the HTC Vive controller.
Once a tool is grabbed, it can be used either using the trigger, or by performing
circular motions on the circular touch pad of the HTC Vive controller. This last
interaction allows to control a continuous value (e.g. the time) or to scroll on a
list. The user could grab a tool in each hand.

The diﬀerent designed interactions were a laser selector to select a cell (see
Figure 3), a screen to display information on the selected cell and a remote
control clipping plane, also used to select the plan for the STC generation.

Immersive Space-Time Hypercube Visualization for Biological Imaging

13

In addition, a bi-manual hand manipulation, similar to a remote Handlebar
technique [39], allows applying rotations to the meshed model by holding of
the trigger button on both controllers. This can be enabled at any time, even
while using a tool. The meshed model is virtually attached to an axis deﬁned by
the position of both hands. In order to activate the rotation, users have to press
the trigger of both controllers. The modiﬁcation of the length of the user’s hands
gap modiﬁes the scale of the meshed model. The rotation can also be applied to
the STC, depending on which visualization the users is looking at.

This set of tools, as well as the ones dedicated to the STC, provides the user

with primary controls to explore the dataset.

4.3 Use Cases

To study functional organization and arrangement of the tissue, embryo develop-
mental studies remain essential to access cellular functions inside a self-organized
isolated system.

With a fast development, a few hundreds of cells, ascidian embryo is the
perfect choice to analyze the link between cells or tissue geometries and diﬀeren-
tiation. In the case of Phallusia mammillata, the cell membranes are very trans-
parent, which makes for an easier imaging of the membranes or endosomes of the
cell through light sheet microscopy. Without any apoptosis, i.e. programmed cell
death, or cell migration in early ascidian development, embryo topological com-
plexity can be summarized in the result of unequal divisions of cells, resulting
in two diﬀerent volumes for each daughter cell, and/or asynchronous divisions,
i.e. two daughter cells having diﬀerent cell cycle duration. Each of these events
correlates with cell fate decision of the cells. This way, with the exploration of
cell architecture and adjacency in a dynamical view in embryos, we have access
on a major part of the story and decisions taken by each cell.

We will illustrate in the following examples of use of the STC to observe
events and behaviors with a strong temporal component related to the explo-
ration issues described above. For this purpose, we generated a STC from the
embryo cross-section shown in Figure 5.

Cellular Deformation One of the main steps of the embryonic development
for most animals is gastrulation. It is characterized by morphogenetic movements
that form a cavity in the embryo. These movements set the base of the deter-
mination of the morphology of the future individual. The actual source of these
movements and deformations of the cells is still unsure, and no formal measure
of deformation exists. Visualization of this deformation over time is necessary
for the analysis of the embryonic development.

The STC shows the deformation of the embryo over time, as shown in Figure
4. A cross-section allows the exploration of the main cells involved in the move-
ments and provides a view of the global and local deformation. The user can
then identify the cell to visualize by selecting them on the STC and continue
the exploration on the 3D representation.

14

G. Fouch´e et al.

Fig. 4. The STC at the left presents the cavity formed by gastrulation. The cross-
section at the right shows the deformation of cells involved in this particular morpho-
genetic movement.

Fig. 5. Cross-section and related STC
generated. The color displayed on the
STC corresponds to the volume of cells,
and helps distinguish 4 parts.

Fig. 6. Cross-section of the STC. Two
divisions are indicated. The membrane
of the original cells and their respective
daughters cells are highlighted in red.

Waves of Cell Divisions During the early stages of the embryonic devel-
opment, cells tend to divide quite simultaneously. This phenomenon results in
“waves” of division, during which numerous cells divides within a short time.

By sliding through the time component, one can notice this kind of event on
the surface-based representation through several aspects. First, the characteristic
dynamics of the membranes of a cell can be observed on a large number of cells:
on the surface of the embryo, and inside by using a clipping plane. Second, as
cells divide, they reduce their volume. Thus, the color mapped on the embryo
will change a lot during such event.

Keeping this last property in mind while exploring the STC, color clusters
become distinguishable thanks to the volume information displayed. This obser-
vation can thus be done directly, without having to do a time-sliding operation,
as shown in Figure 5. Up to four color clusters, hence three waves of division,
can be identiﬁed.

Asynchronous Behavior in Cell Divisions Regarding cell determination,
cells with diﬀerent fates will not result in the same number of cells, and embryonic

xytimex, ytimetimeImmersive Space-Time Hypercube Visualization for Biological Imaging

15

development has to be controlled. Thus, at speciﬁc moments, some cells will not
divide at the same time as the other ones and skip a wave of division.

Exploring the surface-based representation in order to ﬁnd cells presenting
this behavior, the user has to slide back and forth in the time window corre-
sponding to the wave of division and observe which cell will not divide.

The STC can give a more global view of this time window, with a part of the
context of the embryo. In Figure 6, membranes of a cell are highlighted in red.
Only one division can be observed for each of these cells during the time window
represented. Up to three were to be expected here, according to the waves of
division identiﬁed before, which conﬁrms an asynchronous behaviour for these
cells. With the feedback of the cell selection tools on the STC, the user can ﬁnd
quickly the related cell on the 3D model of the embryo.

5 User Evaluation

In this section, we present and discuss a user evaluation aiming to assess the
beneﬁts and limitations of the proposed method using the previously deﬁned
use cases. The evaluation was motivated due to the complex and intricate 3D
visualizations that are generated by the proposed projection, which could be
diﬃcult to interpret. Thus, the main goal of the evaluation was to provide a ﬁrst
assessment on whether and how users could take advantage of the generated
STC visualizations when punctual and continuous events have to be located.
Although other use cases could be envisioned, we decided to focus on embryology
use cases, as they were our ﬁrst motivation for proposing this approach for the
projection of the STH. In this context, as biologists mainly explore this datasets
by exploring one time point at a time, as a baseline condition, we considered
the mesh visualization on which users can navigate in time (e.g. using a slider);
hereinafter referred to as the meshed model alone condition. In the meshed model
& STC condition, the mesh data was also juxtaposed to the STC visualization.

5.1 Tasks and Hypotheses

We focused our evaluation on the eﬃciency and accuracy provided by the STC
through tasks of identiﬁcation of cell divisions, waves of divisions and asyn-
chronous behavior, as described in 4.3. Such events and behaviors have temporal
and spatial components, thus can be considered as 3D temporal objects. Using
the same abstraction as described in section 3, they qualify as 4D objects. As
such, we suggest that one of the visualizations will render more information de-
pending on the characteristics of the event. We discern two type of events. First,
punctual events can be identiﬁed by comparing the state between 2 time points.
To detect such events, the user requires more complete spatial contextual infor-
mation over a very local temporal context, i.e. a few time points. For this reason,
we hypothesize that the meshed model visualization will be more helpful is the
detection of punctual events. The second type of events evolves over time, and is
thus mostly characterized by a temporal component extended in numerous time

16

G. Fouch´e et al.

points. As the generated STC provides complete temporal information over a
reduced spatial context, we hypothesize that the use of the STC will be more
relevant to detect such events.

For this study, we considered the complexity of the tasks and the fact that
our participants were not domain experts. Consequently, we chose to focus our
evaluation on the accuracy of the analysis and not on eﬃciency, which could be
more biased, notably by the approach taken by the participant. Based on our
previous rationale, our main hypotheses were:

H1: Punctual events in time will not be pointed more accurately using the STC

than on the meshed model only;

H2: Events or behaviors with a larger temporal component will be identiﬁed

more accurately using the STC than on the meshed model only.

5.2 Apparatus and Participants

The PC conﬁguration is the same as described in 3.5. The virtual environment
was displayed in a HTC Vive Pro HMD, with a resolution of 1440 x 1600 per eye.
Two Vive Pro controllers were available, including 3 buttons and a touchpad.
The HMD and the controllers were tracked in a space of a 2.5m x 2.5m surface.
12 participants, 10 men and 2 women, were recruited from the local labo-
ratory for this experiment, aged from 22 to 32. They all had medium to high
experience in VR environments, but had no expertise on the dataset nor in
embryology.

5.3 Experimental Task

In order to keep the morphogenetic analysis context, we designed tasks of anno-
tation accessible to non-expert users. From the use cases presented in section 4.3,
tasks of pinpointing waves of cell divisions or identifying asynchronous behavior
in cell divisions corresponded to events with large temporal component.

As a task of pinpointing punctual events, we considered cell divisions. They
are common events yet essential for the analysis of the embryonic development.
The temporal resolution of the dataset allows us to consider the divisions as
punctual events. The spatial deformation occasioned by a division can be easily
noticed on the 3D representation of the embryo by sliding through time. An
additional hint would be the abrupt change in the color displayed, which can be
used to pinpoint this event on the STC. As described in 3.4, the hierarchy, corre-
sponding here to the cell lineage, is highlighted, which can also help identifying
a cell division on this visualization.

5.4 Experimental Protocol

At the beginning of the experiment, participants signed an information consent
form and were briefed regarding the equipment used, the data recorded and the
experimental tasks. In order for the participant to understand the context of

Immersive Space-Time Hypercube Visualization for Biological Imaging

17

the tasks, we explained the nature of the dataset, specifying the methods of
recording and the purpose of analyzing such data. Emphasizing on the temporal
component of the data, we then presented the STC visualization, its generation
process and its purpose in terms of temporal visualization. Since the participants
were non-experts, we explained for each task the characteristics of the events to
pinpoint, the scientiﬁc purpose in embryology analysis as well a few methods
to identify those events in each visualization. Finally, we presented the virtual
environment and the available tools to the participant for about 8 minutes,
before having themselves equip the HMD and get familiar with the framework,
for about 8 minutes as well. Participants were then asked to perform the three
exploratory tasks:

– Discern the number of waves of cell divisions, providing an approximate
estimate of the time for each wave of divisions; duration of the task: 2 minutes
– Find cell divisions and give corresponding instant and identiﬁer; duration of

the task: 4 minutes

– Find cells with asynchronous division behavior and give corresponding iden-

tiﬁer; duration of the task: 4 minutes

For the tasks of identifying divisions and asynchronous behaviors, the events
were present abundantly enough not for anyone to ﬁnish the task early. The
experiment was divided in two blocks. Depending on the visualization method
available, the participant performed the tasks ﬁrst with the meshed model alone
and then with meshed model & STC, or vice-versa. The order of the visualization
method was counter-balanced to minimize ordering eﬀects.

The main goal of the experiment is to evaluate how users will take advantage
of the generated STC visualizations. We thus decided to provide an initial STC
visualization. In addition, the participants worked on the same dataset and the
same STC view, generating smaller variability and enabling better comparison
of results between participants. Second, considering the potential learning curve
in STC based visualizations, we assumed that the level of comprehension of the
STH would be very diﬀerent from user to user. Such diﬀerences could induce an
important bias in the experiment, especially since the users are not familiar with
the datasets observed. We thus generated a STC arbitrarily, though following one
objective criterion: the plane would be placed in order to capture the gastrulation
movement described in part 4.3. We expected this to act as a point of reference
to help participants apprehend the data. Nonetheless, we left the opportunity
for the participants to generate another STC from a diﬀerent cross-section, yet
none of them did.

For each condition, we recorded the answers of the participant for each task,
the total time use of each tool, and the total time watching either visualization.
At the end of the experiment, participants had to ﬁll a short questionnaire in
order to gather subjective impressions of the overall app and demographic data
(age, gender, and VR experience).

18

G. Fouch´e et al.

Fig. 7. Box plots of the precision of the participant for each tasks and condition, i.e.
with or without STC. Each plot corresponds to 12 observations.

5.5 Results

Eventually, we get a total of 24 paired observations, characterized by 2 two-level
factors, the order of the observation and the condition - i.e. whether or not
the STC is present. Focusing the study on accuracy, we computed the precision
(true positives on total selected elements) for each task. Repeated measures
two-way ANOVA was used to analyze the results, considering the order as a
between-subjects factor and the technique as a within-subjects factor. When the
normality assumption was not met (Shapiro–Wilk normality test), the Aligned
Rank Transform (ART) was used before performing the ANOVA analysis. The
statistical analysis was done using R language.

Waves of Cell Division Counting Task The two-way ART ANOVA order and
condition vs precision showed a signiﬁcant eﬀect of the condition (F(1,10) =
24.72; p <0.001; η2
p=0.64), and no signiﬁcant order nor interaction eﬀect. Post-
hoc tests showed a signiﬁcantly (p < 0.001) better performance in terms of accu-
racy using meshed model & STC (M =0.94; SD=0.16) rather than the meshed
model alone (M =0.72; SD=0.16). This supports H2.

Asynchronous Behavior Identiﬁcation Task The two-way ANOVA order and
condition vs precision showed a signiﬁcant eﬀect of the condition (F(1,10) =
5.61; p <0.05; η2
p=0.36), and no signiﬁcant order nor interaction eﬀect. Post-hoc
tests showed a signiﬁcantly (p < 0.05) better performance in terms of accuracy
using meshed model & STC (M =0.87; SD=0.14) rather than the meshed model
alone (M =0.64; SD=0.27), supporting H2.

Cell Division Identiﬁcation Task The two-way ANOVA order and condition vs
precision showed no signiﬁcant result on any factor nor interaction, so we can

lllllllll0.000.250.500.751.001 − Waves2 − Asynchronies3 − DivisionsPrecisionConditionMeshaloneMesh&STCImmersive Space-Time Hypercube Visualization for Biological Imaging

19

give no conclusion about H1. The box plot of the precision of the participants
on this task (cf Figure 7) does not show important diﬀerence either.

5.6 Discussion

The results from the waves of divisions and asynchronous behaviors tasks, con-
sidered as objects with large temporal component as explained in 5.3, showed
that the use of the STC provided higher accuracy in those identiﬁcation tasks,
supporting H2. However, these tasks were designed with numerous known tar-
gets but unknown location in space and time. Placing the user in more diverse
situations would help to estimate more thoroughly the added value of the STC
in general exploration tasks. The results from the cell division task gave no
signiﬁcant results, allowing no conclusion on hypothesis H1.

As evoked by our hypotheses, we focused this study on accuracy. For various
reasons, the protocol as it is could not allow a signiﬁcant eﬃciency evaluation.
Participants were given the same advice on how to approach the tasks, described
in 4.3 and 5.3. However, the numerous tools available made it diﬃcult for them
to choose an optimized strategy. Users usually chose a strategy at the begin-
ning of a task and followed it until the end, even if it felt suboptimal. Such
phenomenon creates subgroups. To counter this eﬀect, we would require either
more participants, or ones that are trained with the system. Furthermore, we
focused our study on only one dataset, since it was the most complete one at
our disposal. Considering our task design, we could expect an important order
eﬀect for our eﬃciency measurements, since users would remember what they
did on the same task in the other condition.

Despite all these constraints, we could output a result on the eﬃciency of
the system. On the waves of division task, some users could ﬁnish at least 20
seconds before the 2 minutes time limit. In the meshed model only condition,
only 1 out of 12 ﬁnished the task early (0 on the ﬁrst part, 1 on the second part).
In the meshed model & STC condition, 8 out of 12 ﬁnished the task early (4 on
the ﬁrst part, 4 on the second part). The two-way ANOVA order and condition
vs the boolean value ”ﬁnished the task early” showed a signiﬁcant eﬀect of the
condition (F(1,10) = 14.41; p <0.01; η2
p=0.59), and no signiﬁcant order nor
interaction eﬀect.

6 General Discussion

While the evaluation described in the paper have showed the potential usages
and advantages of the STH for the visualization of 3D spatio-temporal data,
there is yet a number of additional future works in order to cope with its current
limitations. First, the generated STC visualizations are based on a projection
operator which enables the visualization of a subset of the original 4D data. Al-
though this approach generates a compact representation of the spatio-temporal
data, it also results with a loss of spatial information and can be sensible to

20

G. Fouch´e et al.

strong motions within the dataset. Strong motions can be particularly problem-
atic for objects close to the cutting plane that can result in wrong interpretations
(e.g. an object shrinking when it is just moving away from the clipping plane). In
the current implementation of the system, we tried to compensate for this neg-
ative eﬀects by the juxtaposition of the meshed model, which gave additional
contextual information missing in the STC visualizations. Nevertheless, future
works should explore how to adapt the projection operation to extract more re-
liable information. For example, the clipping plane could be adjusted over time
to track an object of interest or take into account the internal motions of the
objects, potentially extending the cutting plane to an arbitrary surface.

A second aspect that should be further studied, is the usage of the STH
for other spatio-temporal datasets. The examples and use cases described in
this paper came from the biology domain, and in particular on the analysis of
morphogenesis. In the explored datasets, the temporal and spatial coherency was
high, i.e. motions between two consecutive data points were relatively low, with
quite a low spatial density, and the diﬀerent structures visualized are well-deﬁned
(i.e. cells). It remains unknown whether the STH approach would be still usable
for spatio-temporal datasets with lower temporal and spatial coherency.In such
situations, the STC visualization could be denser and less continuous, potentially
impacting its comprehension.

Finally, a recurrent comment from the domain experts was the addition of
annotation functionalities to the system. In this respect, in the morphogenesis
context, domain experts considered that the tool could be of interest for data
curation, in particular to validate/complete the object tracking algorithms over
time, notably after cell divisions, or to label cells. Such operations can be very
cumbersome on desktop applications due to the multidimensional nature of the
data. The STC visualizations could help for detecting errors or areas of interests
and use them to directly annotate those events.

7 Conclusion

In this paper, we have proposed a novel spatio-temporal visualization based on
the Space-Time Cube visualization. The proposed visualization, the Space-Time
Hypercube, extends the STC visualization to consider a third spatial dimension
in the data. However, the interaction and manipulation of 4D remains unpracti-
cal. Thus, to enable a direct visualization, we proposed a projection operator on
based on a user-driven cross-section deﬁned in 3D space. The projection of the
hypercube, consequently a STC, contains only a partial spatial information of
the dataset, but creates a view containing temporal information. Numerical and
categorical information could be displayed as well on the visualization. We juxta-
posed and linked the STC and original dataset visualizations in a VR application,
taking advantage of immersive environments beneﬁts in terms of visualization
and interaction in 3D. Various tools for exploration, ﬁltering or tracking objects
apply transformations on both visualization and come to assist in the analysis
of the dataset. Moreover, we illustrated the potential usages of the STC in sev-

Immersive Space-Time Hypercube Visualization for Biological Imaging

21

eral use cases in the context of morphogenesis. The evaluation of the STH in a
context of embryology showed that the STC visualization presents a number of
beneﬁts with respect to traditional visualizations, specially for the detection of
events with a temporal relevance. The STH approach could pave the way to new
types of visualization and interaction methods for 3D spatio-temporal data, and
we believe that such tools will help the adoption of VR technologies for data
visualization.

References

1. Aigner, W., Miksch, S., Schumann, H., Tominski, C.: Visualization of time-oriented

data. Springer Science & Business Media (2011)

2. Akiba, H., Wang, C., Ma, K.: AniViz: A Template-Based Animation Tool for Vol-
ume Visualization. IEEE Computer Graphics and Applications 30(5), 61–71 (2010)
3. Amirkhanov, A., Kosiuk, I., Szmolyan, P., Amirkhanov, A., Mistelbauer, G.,
Gr¨oller, M.E., Raidou, R.G.: Manylands: A journey across 4d phase space of trajec-
tories. In: Computer Graphics Forum. vol. 38, pp. 191–202. Wiley Online Library
(2019)

4. Andrienko, G., Andrienko, N., Fuchs, G., Garcia, J.M.C.: Clustering trajectories
by relevant parts for air traﬃc analysis. IEEE transactions on visualization and
computer graphics 24(1), 34–44 (2017)

5. Andrienko, N., Andrienko, G.: Visual analytics of movement: An overview of meth-

ods, tools and procedures. Information Visualization 12(1), 3–24 (2013)

6. Bach, B., Dragicevic, P., Archambault, D., Hurter, C., Carpendale, S.: A Descrip-
tive Framework for Temporal Data Visualizations Based on Generalized Space-
Time Cubes. Computer Graphics Forum 36(6), 36–61 (2017)

7. Bach, B., Dragicevic, P., Archambault, D., Hurter, C., Carpendale, S.: A Review
of Temporal Data Visualizations Based on Space-Time Cube Operations. Euro-
graphics conference on visualization p. 20 (2014)

8. Bach, B., Sicat, R., Beyer, J., Cordeil, M., Pﬁster, H.: The hologram in my hand:
How eﬀective is interactive exploration of 3d visualizations in immersive tangi-
ble augmented reality? IEEE transactions on visualization and computer graphics
24(1), 457–467 (2017)

9. Bai, Z., Tao, Y., Lin, H.: Time-varying volume visualization: a survey. Journal of

Visualization pp. 1–17 (2020)

10. Brozovic, M., Dantec, C., Dardaillon, J., Dauga, D., Faure, E., Gineste, M., Louis,
A., Naville, M., Nitta, K.R., Piette, J., et al.: Aniseed 2017: extending the in-
tegrated ascidian database to the exploration and evolutionary comparison of
genome-scale datasets. Nucleic acids research 46(D1), D718–D725 (2017)

11. Chandler, T., Cordeil, M., Czauderna, T., Dwyer, T., Glowacki, J., Goncu, C.,
Klapperstueck, M., Klein, K., Marriott, K., Schreiber, F., et al.: Immersive ana-
lytics. In: 2015 Big Data Visual Analytics (BDVA). pp. 1–8. IEEE (2015)

12. Chen, B.C., Legant, W.R., Wang, K., Shao, L., Milkie, D.E., Davidson, M.W.,
Janetopoulos, C., Wu, X.S., Hammer, J.A., Liu, Z., et al.: Lattice light-sheet mi-
croscopy: imaging molecules to embryos at high spatiotemporal resolution. Science
346(6208), 1257998 (2014)

13. Cignoni, P., Callieri, M., Corsini, M., Dellepiane, M., Ganovelli, F., Ranzuglia, G.:
Meshlab: an open-source mesh processing tool. In: Eurographics Italian chapter
conference. vol. 2008, pp. 129–136. Salerno (2008)

22

G. Fouch´e et al.

14. Coﬀey, D., Korsakov, F., Ewert, M., Hagh-Shenas, H., Thorson, L., Ellingson,
A., Nuckley, D., Keefe, D.F.: Visualizing Motion Data in Virtual Reality: Under-
standing the Roles of Animation, Interaction, and Static Presentation. Computer
Graphics Forum 31(3pt3), 1215–1224 (2012)

15. Guignard, L., Fiuza, U.M., Leggio, B., Faure, E., Laussu, J., Hufnagel, L., Ma-
landain, G., Godin, C., Lemaire, P.: Contact-dependent cell communications drive
morphological invariance during ascidian embryogenesis (2017)

16. H¨agerstrand, T.: what about people in regional science? 1970. In: European

Congress of The Regional Science Association Copenhagen. vol. 69 (1970)

17. Homps, F., Beugin, Y., Vuillemot, R.: Revivd: Exploration and ﬁltering of trajec-
tories in an immersive environment using 3d shapes. In: 2020 IEEE Conference on
Virtual Reality and 3D User Interfaces (VR). pp. 729–737. IEEE (2020)

18. Huang, X., Zhao, Y., Ma, C., Yang, J., Ye, X., Zhang, C.: Trajgraph: A graph-
based visual analytics approach to studying urban network centralities using taxi
trajectory data. IEEE transactions on visualization and computer graphics 22(1),
160–169 (2015)

19. Hurter, C., Riche, N.H., Drucker, S.M., Cordeil, M., Alligier, R., Vuillemot, R.:
Fiberclay: Sculpting three dimensional trajectories to reveal structural insights.
IEEE transactions on visualization and computer graphics 25(1), 704–714 (2018)
20. Hurter, C., Tissoires, B., Conversy, S.: Fromdady: Spreading aircraft trajectories
across views to support iterative queries. IEEE transactions on visualization and
computer graphics 15(6), 1017–1024 (2009)

21. Johnson, S., Orban, D., Runesha, H.B., Meng, L., Juhnke, B., Erdman, A., Sam-
sel, F., Keefe, D.F.: Bento Box: An Interactive and Zoomable Small Multiples
Technique for Visualizing 4d Simulation Ensembles in Virtual Reality. Frontiers in
Robotics and AI 6 (2019)

22. Kim, K., Carlis, J.V., Keefe, D.F.: Comparison techniques utilized in spatial 3d
and 4d data visualizations: A survey and future directions. Computers & Graphics
67, 138–147 (2017)

23. Laha, B., Sensharma, K., Schiﬀbauer, J.D., Bowman, D.A.: Eﬀects of immersion on
visual analysis of volume data. IEEE Transactions On Visualization & Computer
Graphics 4, 597–606 (2012)

24. Leduc, T., Tourre, V., Servi`eres, M.: The space-time cube as an eﬀective way of
representing and analysing the streetscape along a pedestrian route in an urban
environment. In: SHS Web of Conferences. vol. 64, p. 03005. EDP Sciences (2019)
25. Leggio, B., Laussu, J., Carlier, A., Godin, C., Lemaire, P., Faure, E.: Morphonet:
an interactive online morphological browser to explore complex multi-scale data.
Nature Communications 10(1), 2812 (2019)

26. Li, Y., Fan, X., Mitra, N.J., Chamovitz, D., Cohen-Or, D., Chen, B.: Analyzing
growing plants from 4d point cloud data. ACM Transactions on Graphics (TOG)
32(6), 1–10 (2013)

27. Liu, D., Weng, D., Li, Y., Bao, J., Zheng, Y., Qu, H., Wu, Y.: Smartadp: Vi-
sual analytics of large-scale taxi trajectories for selecting billboard locations. IEEE
transactions on visualization and computer graphics 23(1), 1–10 (2016)

28. Loomis, J.M., Knapp, J.M., et al.: Visual perception of egocentric distance in real
and virtual environments. Virtual and adaptive environments 11, 21–46 (2003)
29. Lu, A., Shen, H.W.: Interactive storyboard for overall time-varying data visualiza-

tion. In: 2008 IEEE Paciﬁc Visualization Symposium. pp. 143–150. IEEE (2008)

30. McIntire, J.P., Havig, P.R., Geiselman, E.E.: Stereoscopic 3d displays and human

performance: A comprehensive review. Displays 35(1), 18–26 (2014)

Immersive Space-Time Hypercube Visualization for Biological Imaging

23

31. Munzner, T.: Visualization analysis and design. CRC press (2014)
32. Neuroth, T., Sauer, F., Ma, K.L.: An interactive visualization system for large sets
of phase space trajectories. In: Computer Graphics Forum. vol. 38, pp. 297–309.
Wiley Online Library (2019)

33. Oxford Instruments plc.: Imaris Software. https://imaris.oxinst.com/ (1993)
34. Rasband, W.S., et al.: Imagej (1997)
35. Schindler, B., Waser, J., Ribiˇci´c, H., Fuchs, R., Peikert, R.: Multiverse data-ﬂow
control. IEEE transactions on visualization and computer graphics 19(6), 1005–
1019 (2012)

36. Schroeder, W.J., Lorensen, B., Martin, K.: The visualization toolkit: an object-

oriented approach to 3D graphics. Kitware (2004)

37. Skarbez, R., Polys, N.F., Ogle, J.T., North, C., Bowman, D.A.: Immersive analyt-
ics: Theory and research agenda. Frontiers in Robotics and AI 6, 82 (2019)
38. Solteszova, V., Smit, N.N., Stoppel, S., Gr¨uner, R., Bruckner, S.: Memento: Lo-
calized time-warping for spatio-temporal selection. In: Computer Graphics Forum.
vol. 39, pp. 231–243. Wiley Online Library (2020)

39. Song, P., Goh, W.B., Hutama, W., Fu, C.W., Liu, X.: A handle bar metaphor
for virtual object manipulation with mid-air interaction. In: Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems. pp. 1297–1306
(2012)

40. Ssin, S.Y., Walsh, J.A., Smith, R.T., Cunningham, A., Thomas, B.H.: GeoGate:
Correlating Geo-Temporal Datasets Using an Augmented Reality Space-Time
Cube and Tangible Interactions. In: 2019 IEEE Conference on Virtual Reality
and 3D User Interfaces (VR). pp. 210–219 (2019)

41. Stegmaier, S., Strengert, M., Klein, T., Ertl, T.: A simple and ﬂexible volume ren-
dering framework for graphics-hardware-based raycasting. In: Fourth International
Workshop on Volume Graphics, 2005. pp. 187–241. IEEE (2005)

42. Wagner Filho, J.A., Stuerzlinger, W., Nedel, L.: Evaluating an immersive space-
time cube geovisualization for intuitive trajectory data exploration. IEEE Trans-
actions on Visualization and Computer Graphics 26(1), 514–524 (2019)

43. Waser, J., Fuchs, R., Ribicic, H., Schindler, B., Bloschl, G., Groller, E.: World
lines. IEEE transactions on visualization and computer graphics 16(6), 1458–1467
(2010)

44. Woodring, J., Shen, H.: Multi-variate, Time Varying, and Comparative Visual-
ization with Contextual Cues. IEEE Transactions on Visualization and Computer
Graphics 12(5), 909–916 (2006)

45. Woodring, J., Wang, C., Shen, H.W.: High dimensional direct rendering of time-

varying volumetric data. IEEE (2003)

