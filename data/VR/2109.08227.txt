1
2
0
2

p
e
S
6
1

]

V

I
.
s
s
e
e
[

1
v
7
2
2
8
0
.
9
0
1
2
:
v
i
X
r
a

Proceedings of Machine Learning Research 1–16

Stereo Video Reconstruction Without Explicit Depth Maps for
Endoscopic Surgery

Annika Brundyn
Jesse Swanson
Kyunghyun Cho
Center for Data Science, New York University

annika.brundyn@nyu.edu
jesse.swanson@nyu.edu
kyunghyun.cho@nyu.edu

Doug Kondziolka
Eric Oermann
Department of Neurosurgery, Langone Health, New York University

douglas.kondziolka@nyulangone.org
eric.oermann@nyulangone.org

Abstract

Keywords: deep learning, U-Net, endoscopy,
stereo reconstruction

We introduce the task of stereo video reconstruc-
tion or, equivalently, 2D-to-3D video conversion
for minimally invasive surgical video. We de-
sign and implement a series of end-to-end U-
Net–based solutions for this task by varying the
input (single frame vs. multiple consecutive
frames), loss function (MSE, MAE, or percep-
tual losses), and network architecture. We eval-
uate these solutions by surveying ten experts
– surgeons who routinely perform endoscopic
surgery. We run two separate reader studies:
one evaluating individual frames and the other
evaluating fully reconstructed 3D video played
on a VR headset. In the ﬁrst reader study, a
variant of the U-Net that takes as input mul-
tiple consecutive video frames and outputs the
missing view performs best. We draw two con-
clusions from this outcome. First, motion in-
formation coming from multiple past frames is
crucial in recreating stereo vision. Second, the
proposed U-Net variant can indeed exploit such
motion information for solving this task. The re-
sult from the second study further conﬁrms the
eﬀectiveness of the proposed U-Net variant. The
surgeons reported that they could successfully
perceive depth from the reconstructed 3D video
clips. They also expressed a clear preference for
the reconstructed 3D video over the original 2D
video. These two reader studies strongly sup-
port the usefulness of the proposed task of stereo
reconstruction for minimally invasive surgical
video and indicate that deep learning is a promis-
ing approach to this task. Finally, we identify
two automatic metrics, LPIPS and DISTS, that
are strongly correlated with expert judgement
and that could serve as proxies for the latter in
future studies.

1. Introduction

Figure 1: Proposed task of stereo reconstruction to
facilitate depth perception in endoscopic
surgery. Adapted from Correa et al. (2017)
and Mozilla Contributors.

The development and use of endoscopes for min-
imally invasive surgery was one of the major surgi-
cal innovations of the 20th century (Li et al., 2005).
An endoscope is a thin, tubular instrument with an
attached camera. During surgery, an endoscope is
inserted through a small incision or natural oriﬁce
so that a surgeon is able to view the internal area
and operate on it by way of tiny surgical instruments
(see Figure 1). This obviates the need for large inci-
sions, reducing patient healing time and the risk of
post-operative infection. Technological advances in
optics, illumination, and miniaturization over the past
hundred years have expanded the reach of endoscopy,
which has in turn revolutionized surgery (Berci and
Forde, 2000).

A major drawback of early endoscopy was the lack
of depth perception arising from 2D vision. Although

© A. Brundyn, J. Swanson, K. Cho, D. Kondziolka & E. Oermann.

 
 
 
 
 
 
Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

many surgeries can safely be performed using a 2D
endoscope, it is more challenging to operate around
critical and highly complex physiological structures in
this setting. For example, it is diﬃcult for a surgeon
to judge distances between a brain tumor and the sur-
rounding nerves and tissue without depth perception.
Such diﬃculty is naturally resolved with 3D stereo
imaging. Furthermore, 3D stereo endoscopes have a
shallower learning curve and can help novice surgeons
take advantage of minimally invasive techniques (Koc
et al., 2006). While modern 3D stereo endoscopes
exist, they cannot be miniaturized to the same degree
as 2D endoscopes (4mm vs 2.7mm in diameter for
the smallest in each class). They also suﬀer from a
smaller ﬁeld of view (Gompel et al., 2014), rendering
them infeasible for some types of surgeries.

In this paper, we frame the task of automatically
converting 2D endoscopic video to a 3D format as
stereo video reconstruction. 3D content is typically
stored in stereo format: i.e., two perspectives of the
same scene. When viewed together, the disparity
between the two perspectives simulates natural binoc-
ular vision, resulting in a 3D experience (Xie et al.,
2016). In the absence of a 3D stereo endoscope with
two cameras, one can generate an alternate view of
the existing 2D input, which, when combined with
the original view, forms a stereo pair. The result can
be viewed using 3D glasses or a head-mounted VR
display (see Figure 1), eﬀectively recreating 3D depth
perception for the operating surgeon.

We propose a deep neural network (DNN) as a solu-
tion to this task. In our approach, a DNN is trained to
reconstruct an alternative perspective of the current
frame using the information available in past video
frames. This is necessary, as we will demonstrate em-
pirically, since creating an alternative perspective from
a single view is a highly under-constrained problem.
Past frames may contain information about occluded
regions in the current view and may capture more
accurate depth information.

We run a series of experiments with DNNs to de-
termine whether multiple past frames improve 3D
stereo reconstruction and to identify an optimal net-
work architecture and optimal learning parameters.
We conduct an extensive and rigorous evaluation of
these DNN variants through two sets of reader studies
involving experienced surgeons. Our reader studies
conﬁrm that DNNs designed to leverage temporal in-
formation from past frames result in superior stereo
reconstruction when compared to a single-frame ap-
proach. We also test a diverse set of automated met-

rics and correlate them against the outcome of these
reader studies. We identify two perceptual metrics,
DISTS (Ding et al., 2020) and LPIPS (Zhang et al.,
2018), that correlate strongly with expert judgement.
The task and the approach presented in this pa-
per are not restricted to endoscopic video but rather
generally applicable to any video content. We ﬁnd
this particular use case interesting and technically
meaningful because factors such as sharpness, hallu-
cinations, and accurate depth perception are more
salient here than in many other applications. We
anticipate, however, that the ﬁndings from this work
will be applicable and transferable to other similar
use cases in the future.

2. Related Work

Here, we review related prior work in 2D-to-3D recon-
struction, applications in endoscopy, and evaluation
metrics for the task.

2.1. 2D-to-3D Reconstruction

Traditional 2D-to-3D reconstruction methods often
consist of two stages. First, a depth map is con-
structed from the 2D input; then, a depth image-based
rendering (DIBR) algorithm combines the depth map
with the input view to generate the missing view
of the stereo pair (Fehn, 2004). Depth maps can
be constructed using various techniques, among them
manual construction by artists, structure-from-motion
(SfM) approaches, and, more recently, machine learn-
ing algorithms (Li et al., 2006; Han and Kanade, 2002;
Ummenhofer et al., 2016).

Monocular (i.e., 2D) depth estimation is itself a
diﬃcult task. Recent trends in deep learning have
shifted toward training end-to-end diﬀerentiable sys-
tems, allowing explicit depth estimation to be by-
passed. Accordingly, our proposed model only requires
stereo pairs for training and learns to directly predict
the right view from the left view. This is similar
to Deep3D (Xie et al., 2016), a stereo reconstruction
model trained without supervisory ground-truth depth
maps. Deep3D uses a single RGB frame as input, but
the authors suggest incorporating temporal informa-
tion from multiple frames as a promising direction
for future research. In our work, we ﬁnd that using
multiple frames as input to our model achieves supe-
rior quantitative and qualitative performance when
compared to single-frame models.

2

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Our method is built as a fully convolutional en-
coder–decoder architecture, largely adapting the U-
Net architecture (Ronneberger et al., 2015). U-Nets
have been used in depth estimation (Wiles et al., 2020)
and novel-view synthesis (Kim and Kim, 2020). They
have also been applied to the task of semi-supervised
video segmentation (Sibechi et al., 2019).

found to closely correspond with human qualitative
assessments (Ding et al., 2021). In this ﬁrst attempt
at establishing the suitability of automated metrics
in the domain of endoscopic surgery, we correlate
a diverse set of automated metrics against human-
perceived quality assessments obtained via a series of
reader studies with ten experienced surgeons.

2.2. Deep Learning Applications to

Endoscopy

3. Methods

Most of the existing work related to this paper focuses
on depth estimation and topographical reconstruction
in endoscopy (Mahmood and Durr, 2018b,a; Liu et al.,
2018; Turan et al., 2018; Liu et al., 2019). For instance,
Kumar et al. (2014) design a multi-stage procedure –
involving 3D shape reconstruction, registration with a
3D CT model, endoscope position tracking, and depth
map calculation – to allow for stereo image synthesis in
laparoscopic surgery. These stages are often developed
and tuned separately from one another and require
expensive ground-truth annotations. Unlike these
earlier studies, we investigate an end-to-end approach
that requires minimal engineering and annotation.

It is tempting to generalize the success of deep learn-
ing for “natural” video to surgical video. It is, however,
unclear whether this generalization is reasonable, since
there are many features that are speciﬁc to surgical
video that are not present in natural video. These
include soft textures, homogeneous colors, inconsis-
tent sharpness, variable depth of ﬁeld and optical
zoom, inconsistent motion, and obstructions arising
from ﬂuid and smoke (Petscharnig and Schöﬀmann,
2018). Therefore, it is necessary to investigate the
applicability and eﬀectiveness of deep learning–based
end-to-end approaches to 2D-to-3D reconstruction in
surgical video, as we do in this paper.

2.3. Evaluation Metrics

Quantitatively assessing the quality of generated
stereo video an open problem. In our own evalua-
tion, we simplify the problem by assessing the image
quality of the generated frames rather than the full
video, following the convention of previous work (see
Xie et al. (2016)). Various automated metrics have
been proposed to assess the quality of generated im-
ages, including structural similarity index (SSIM),
peak signal-to-noise ratio (PSNR), visual information
ﬁdelity (VIF), and Fréchet inception distance (FID)
(Li et al., 2019; Song et al., 2014). More recently, a
family of metrics called perceptual metrics have been

In this section, we describe the task of generating
stereo video from 2D video and present a neural net-
work–based approach devised for this purpose.

3.1. Task Description

We refer to “stereo vision” as a pair of images (or video
frames), corresponding to the left and right views of
the same scene, and deﬁne the task of 2D-to-3D recon-
struction, or equivalently stereo video reconstruction,
as the problem of generating a missing view (either
left or right) given the other (observed) view. Without
loss of generality, we take the left view as the observed
input and the right view as the missing view to be
reconstructed. In the case of video, we have multiple
past frames of the left view available, while we do not
assume the availability of any right-view frames.

More speciﬁcally, we propose a solution to the
task of online frame-level stereo video reconstruction.
Given a sequence of left-view frames x≤t up to time t,
we predict the corresponding right view yt at time t.
Surgical video can be of arbitrary length, and it
is unlikely for distant past frames to be relevant to
the current scene. We therefore consider only a few
frames K from the immediate past as the input. In
other words, a system solving this task takes the
most recent K frames, including the current frame,
xt−K+1:t ∈ RK×3×H×W , and outputs the right view
yt ∈ R3×H×W , where we assume three-channel RGB
frames of width W and height H.

3.2. Deep Learning for 2D-to-3D Video

Reconstruction

We present a DNN that takes as its input the left
view and directly renders the right view. We train
this neural network using a dataset of 3D endoscopy
procedures, to minimize the reconstruction error be-
tween the generated and the true right view. We do
not require any explicit depth information or intrinsic
camera parameters.

3

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Figure 2: Proposed approach. Models take as input a
variable number of consecutive video frames
from the left view and directly predict the
missing right view for the current (latest)
frame.

3.2.1. A Base Network Architecture: A

Modified U-Net

We modify a fully convolutional U-Net as the base
architecture for the task of stereo view generation
(Ronneberger et al., 2015). Our U-Net implementa-
tion diﬀers from the original architecture in two ways:
1) decoder blocks use padding instead of cropping to
concatenate features from the skip connections, and
2) the decoder uses either strided transpose convo-
lution or bilinear interpolation for upsampling. We
denote this U-Net model as gθ. As in the original
implementation, we do not include any batch normal-
ization layers. The 2D-to-3D reconstruction task is,
then, to predict the corresponding right view given
the left-view frames: i.e., yt = gθ (xt−K+1:t).

3.2.2. Capturing Temporal Information

Unlike conventional applications of the U-Net, such as
semantic segmentation (Ronneberger et al., 2015; Sid-
dique et al., 2020), our modiﬁed U-Net must handle
a temporal series of left-view frames. The temporal
structure contained in the input frames can be ex-
ploited by the U-Net in diﬀerent ways, and we explore
three possible approaches in this paper.

(1) Current Frame Only. The simplest, but least
eﬀective, approach is to discard all the past frames
and keep only the current frame as input (K = 1) to
the network gθ, in order to predict the correspond-
ing stereo frame yt. We use this approach to verify
our hypothesis that temporal information from the
observed view is useful for reconstructing the missing
view, which has not yet been conﬁrmed or refuted in
the context of surgical video.

(2) Stacking Multiple Frames. We present the
temporal ordering of observed frames by stacking

them along the channel axis, similarly to color. In
other words, we reshape the 4D input tensor x≤t ∈
RK×3×H×W into a 3D tensor of size 3K × H × W and
feed this 3D tensor to the U-Net architecture gθ, as if
it were an image with 3 × K color channels.

(3) A Spatio-Temporal U-Net. Like spatial
structures, temporal structures exhibit themselves at
multiple scales. Pixel-level temporal information allow
ﬁne-grained textures to be inferred, while object-level
temporal information can help overcome occlusion in
individual frames. In order to better incorporate the
multi-scale nature of temporal information, we modify
the U-Net architecture by inserting a “temporal mod-
ule” at each scale between the encoder and decoder,
capturing unique aspects of temporal information at
each scale.

More speciﬁcally, each of the K consecutive frames
in the input xt−K+1:t is fed to the encoder separately
and results in K spatial feature maps. This sequence
of feature maps is processed by the temporal module,
before being sent to the corresponding layer at the de-
coder for concatenation. For a graphical illustration of
the method, see can be seen in Figure 3. In this work,
we use a single 3D convolutional layer as the default
temporal module. We compare this module against
two naive approaches: the element-wise average and
element-wise maximum modules.

Figure 3: Spatio-temporal U-Net architecture.

3.3. Loss Functions

The choice of loss function is known to have a large
impact on the quality of generated images. Pixel-wise
loss functions, such as mean squared error (MSE) and
mean absolute error (MAE), correlate poorly with
perceived image quality (Ding et al., 2021). However,
when these losses are computed in perceptually ap-
propriate representation space, they have been found
to correlate better with human perception.

In our experiments, we investigate three loss func-
tions: MSE, MAE, and a perceptual loss function.
We use the sum of the MAEs computed using the
feature maps extracted from the ﬁrst three blocks of

4

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

VGG16 (Simonyan and Zisserman, 2015), pretrained
on the ImageNet dataset (Deng et al., 2009) for the
perceptual loss.

4. Experimental Setup

In this section, we describe the training procedure
for our DNN-based computational models, and the
subjective testing procedure used to collect human
ratings of the generated images and VR videos. We
ensure the reproducibility of our experiments by re-
porting on public datasets, and by publicly releasing
our code.

4.1. Dataset

We use the da Vinci endoscopic dataset from the
Hamlyn Center for Robotic Surgery (Ye et al., 2017).
This dataset consists of rectiﬁed stereo images of size
384 × 192, partitioned into 34,241 train and 7,192
test image sequences from video. Each video was
captured in vivo during a partial nephrectomy pro-
cedure performed using a da Vinci surgical system.
Details regarding the how the speciﬁc train and val-
idation dataset splits were obtained is discussed in
Appendix A.

4.2. Training

To investigate the eﬀect of using multiple frames for
2D-to-3D reconstruction, we try diﬀerent training set-
ups. Speciﬁcally, we vary the number of input frames
between one (current only), ﬁve, and ten consecutive
frames. We also test ﬁve- and six-layer U-Nets, to
understand whether more layers allow better temporal
information to be captured.

All networks are trained with the Adam opti-
mizer (Kingma and Ba, 2017) from scratch, with a
constant learning rate of 0.0001. Each model is trained
on a single NVIDIA V100 GPU for no more than 30
hours. We used a mini-batch size of 16, although we
sometimes use smaller mini-batches to work with the
limited onboard memory (e.g., when we train a net-
work with the 10-frame input). For memory eﬃciency,
we use 16-bit precision wherever possible.

4.3. Constant Shift Baselines

In order to ensure that the proposed U-Net–based
approach performs non-trivial reconstruction, we in-
clude two baselines for comparison. In both cases, we
shift all the pixels in the current left-view frame by a

constant disparity δ. The disparity δ is estimated by
minimizing the loss (MSE, MAE, or perceptual) on
the validation set. These two baselines diﬀer in that
one ﬁlls the newly added pixels with zeros (i.e., the
color black), while the other ﬁlls the missing pixels
with the original values from the input, resulting in a
duplicated strip on one side of the generated frame.

4.4. Automatic Evaluation Metrics

Image-quality assessment is an active area of research
Ding et al. (2021). Widely used evaluation protocols
for video generation often rely on image-similarity-
based metrics Oprea et al. (2020). We select and re-
port ﬁve image-quality metrics – PSNR, SSIM (Wang
et al., 2004), FID (Heusel et al., 2018), LPIPS (Zhang
et al., 2018), and DISTS (Ding et al., 2020) – for
the purpose of understanding these metrics’ correla-
tion with human perception in the context of surgical
endoscopy video.

4.5. Expert Evaluation: Reader Studies

In the ﬁrst
We conduct two main reader studies.
reader study, we focus on evaluating individual predic-
tions from the U-Net variants. Each expert is asked to
choose between two candidate frames, based on their
quality. In the second study, we model a more realistic
scenario in which we ask experts to assess the quality
of stereo-vision video using a VR kit. Due to time
constraints, we use a subset of promising models from
the ﬁrst reader study for 3D-video-quality assessment.

4.5.1. Frame-Level Reader Study

Selected Models. We select eight models for the
frame-level reader study. Our selection is based on a
mix of automatic evaluation metrics and qualitative
feedback from a supervising surgeon. Descriptions of
the eight models, as well as further detail regarding
the selection process, can be found in Appendix D.1.
The only multi-frame models selected were those that
used the spatio-temporal U-Net architecture with a
single 3D convolutional layer as the temporal mod-
ule. Initial evaluation showed that stacking multiple
frames resulted in similar or worse performance com-
pared to using just a single frame. Spatio-temporal
networks with an average or maximum temporal mod-
ule produced visible inconsistencies such as double
vision and were thus not included in the reader study.

5

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Reader Study Design. To acquire human percep-
tual assessment of the generated results at the frame
level, we conduct a reader study with 10 experts. Ex-
perts are surgeons with between 6 and 30 years of
medical experience, and all have experience perform-
ing endoscopic surgery.

We use the two-alternative forced choice (2AFC)
method to compare the eight diﬀerent models. Each
model is compared against every other model three
times, using the same three examples. Additionally,
each model is compared against the target once, re-
sulting in a total of 92 comparisons. In a given trial,
a surgeon is shown two frames generated by two dis-
tinct models (candidate synthetic right-view frames),
as well as the corresponding left view that was used
as the input to the models. The surgeon is then asked
the following question: Given the left view, which of
the generated right views (A or B) is of better quality?
Each image is shown in the native resolution of the
dataset (384 × 192 pixels), with the option to zoom
in on any portion of the presented frame. No time
limit is applied. An example is shown in Figure 9 in
Appendix D.2. We employ the Bradley-Terry model
(Bradley and Terry, 1952) to convert pair-wise compar-
ison results to global rankings and calculate log-worth
scores. Further details can be found in Appendix D.5.

4.5.2. Video-Level Reader Study: VR

The quality of individual frames is an insuﬃcient
proxy for stereo video quality. Successful stereo video
reconstruction requires that the viewer’s perception of
3D depth remain consistent across consecutive frames.
The shift between the two views that form a stereo
pair is subtle. It is diﬃcult to assess from a single left
view whether a given right view would in fact result in
realistic 3D depth perception when viewed in stereo.
Moreover, presented with only a single stereo pair,
it is impossible to judge whether the model achieves
consistent reconstruction across time.

We select ﬁve models from the ﬁrst reader study
and further evaluate them using a VR kit. Experts
are ﬁrst presented with a 25-second 2D clip. They
then watch the same clip in 3D, generated from one
of the selected models. The 3D videos are played
using a Google Cardboard VR headset in conjunction
with an iPhone 12. Experts are asked 1) whether the
3D video provided a better viewing experience than
the 2D video did, and 2) to rate the quality of the
reconstructed 3D video on a scale from 1 (worst) to 5
(best).

5. Results and Discussion

5.1. Frame-Level Reader Study

Multiple Frames and Perceptual Loss. A ma-
jor ﬁnding from the ﬁrst reader study is that the
proposed U-Net variant beneﬁts from the availability
of multiple left-view frames as input. This is clear
from the top-three entries in Table 1, according to
both the log-worth and the win rate. All three models
take ﬁve left-view frames as input. This observation
conﬁrms our earlier hypotheses that 1) multiple frames
from one view facilitate reconstructing the other view,
and 2) the proposed variant of the U-Net is capable
of exploiting such temporal information from multiple
frames.

Another ﬁnding – in line with earlier observations
(Ding et al., 2021) – is the importance of using a
perceptual loss function for training image-generation
models. The top ﬁve models were all trained to min-
imize the perceptual loss and outperformed models
trained with either MSE or MAE in the original pixel
space.

Advanced Perceptual Metrics with Expert
Judgement. The ﬁve ﬁnal columns of Table 1 re-
veal that recently proposed advanced perceptual met-
rics – in particular, DISTS and LPIPS – select the
best models, as deﬁned by expert judgement. Table 1
reports the metric values on our held-out validation
set and we also show that performance is roughly con-
sistent on the oﬃcial da Vinci test set in Appendix B.
Given the high costs associated with obtaining surgeon
feedback, this is a positive ﬁnding. Future research can
rely on DISTS and/or LPIPS for faster and cheaper
iteration in designing approaches to 2D-to-3D surgical
video reconstruction.

Unlike DISTS and LPIPS, MSE-based PSNR did
not show any discriminative capability among these
models, resulting in more or less similar scores for all
the tested models. On the other hand, SSIM, which
has been successful with natural images (not surgical
images), ended up choosing the two worst models,
suggesting major diﬀerences between natural and sur-
gical images. FID preferred models trained with the
perceptual loss, perhaps unsurprisingly because FID
itself is a perceptual loss. However, FID ended up fa-
voring the pixel-shift baselines over any of the learned
models, which signiﬁcantly limits the applicability and
reliability of FID. See Appendix C for more details.
These observations are further conﬁrmed by the
rank correlations among automatic metrics and ex-

6

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Reader Study

Automated Metrics

Rank Model

log-worth win % DISTS ↓ LPIPS ↓ FID ↓ PSNR ↑

SSIM ↑

1
2
3
4
5
6
7
8

5 fr+perceptual+6 layers
5 fr+perceptual
5 fr+perceptual+bilinear
1 fr+perceptual
10 fr+perceptual
1 fr+MSE+6 layers
1 fr+MAE
5 fr+MSE+6 layers

0.00 ± 0.20
-0.91 ± 0.17
-1.85 ± 0.16
-2.04 ± 0.16
-2.41 ± 0.16
-2.55 ± 0.16
-3.74 ± 0.20
-4.42 ± 0.23

100
86
52
52
38
48
19
5

0.110
0.116
0.117
0.119
0.120
0.143
0.140
0.143

0.116
0.119
0.124
0.125
0.131
0.156
0.156
0.159

50.53
48.42
53.68
52.56
50.16
65.78
74.66
66.28

22.88
22.77
23.25
22.63
22.63
23.04
23.14
23.78

0.627
0.616
0.710
0.624
0.620
0.700
0.716
0.722

Table 1: Comparison of model architectures, ranked according to reader study results. We calculate the
percentage of times (out of 24) a given model wins the majority vote and report this in the column
"win %". The top two models as ranked by each metric are indicated in bold.

perts, as shown in Table 2. The rankings of the models
by both DISTS and LPIPS correlate almost perfectly
with the expert ranking. This correlation does not
exist with the other automatic metrics tested.

produced by the only three models that did not use
the perceptual loss.

Expert DISTS LPIPS FID SSIM PSNR

1.0

0.98
1.0

Expert
DISTS
LPIPS
FID
SSIM
PSNR

0.95 0.76 0.64
0.71 0.60
0.98
0.74 0.67
1.0
0.93
1.0
1.0.

0.41
0.39
0.52
0.73
0.88
1.0

Table 2: Spearman’s rank correlation coeﬃcient be-
tween expert and automatic metric rankings.

In order to get a better
Qualitative Inspection.
sense of how models diﬀer, we manually inspect se-
lected output frames. Figure 4 shows a representative
example, displaying cropped tiles from the outputs of
the eight ranked models. These are ordered accord-
ing to their ranks (best to worst). Full frames and
additional examples are found in Appendix D.2.

The models trained using the perceptual loss pro-
duce visibly sharper images. One can easily discern
visual details in the gauze in examples (b–f) that are
not present in examples (g–i), the latter of which were

7

Expert Judgement vs. Non-Expert Judge-
ment. As additional analysis, we conducted another
frame-level reader study with 10 non-expert readers
with no medical expertise. We calculate how often
the majority vote of the experts agrees with the ma-
jority vote of the non-experts over all questions. We
found that, for 80% of the questions, the expert and
non-expert majority votes were in agreement. We at-
tribute this high level of agreement to visual anchors
common for both the expert and non-expert readers,
such as the legibility of text or the sharp edges of
a surgical tool. Disagreements between experts and
non-experts, on the other hand, seem to be caused by
certain features that are visually subtle but that im-
pact surgical outcomes and whose presence surgeons
are therefore trained to search for. For instance, two
surgeons commented on the appearance of the vascu-
lature in the generated images. Although vasculature
is critical in surgical processes, since blood vessels
bleed when cut, non-experts are unlikely to notice it
in a given image.

The average within-group disagreement was slightly
higher among the non-experts (0.26) than among the
experts (0.17), where the within-group disagreement
was calculated as the percentage of members who
disagreed with the majority vote of the group, aver-
aged over all questions. We conjecture that this lower

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

(a)

(b)

(c)

(d )

(e)

(f )

(g)

(h)

(i )

Figure 4: Cropped regions from frames generated by the diﬀerent models based on the same input. (a)

Target right view. (b)-(i ) Model results ordered by ranking, from left to right (best to worst).

within-group disagreement rate among the experts
is also due to their attention to surgically relevant
details that are not taken into account by non-expert
readers.

Based on this comparison between the expert and
non-expert readers, we conclude that one cannot fully
substitute expert judgment with non-expert judge-
ment. Nevertheless, reasonably high agreement indi-
cates that non-expert surveys – which are easier to
come by than expert surveys – can be used to iterate
more eﬃciently in future work.

5.2. VR Video Reader Study

We choose the top three models from Table 1 in addi-
tion to the best MSE model and the best MAE model
for testing in the second reader study, a VR video
survey. We recruit two surgeons for this study. The
full version of Table 3 includes detailed individual
surgeon comments and can be found in Appendix E.
The ﬁrst ﬁnding from this reader study is that
both surgeons strongly favored the 3D reconstructed
surgical video clips over the corresponding 2D clips.
This supports the validity and importance of the
proposed task of 2D-to-3D reconstruction in surgi-
cal video and demonstrates the eﬀectiveness of the
proposed U-Net–based approach to this problem.

Model description

5 fr, perceptual, bilinear

5 fr, perceptual

Avg

4.67

3.83

5 fr, perceptual, 6 layers

3.50

1 fr, MSE, 6 layers

1 fr, MAE

2.83

2.83

StdDev

0.52

0.98

0.55

0.75

0.98

Table 3: Expert evaluation in VR. Scores range from

1 (worst) to 5 (best).

The second ﬁnding is that models trained with the
perceptual loss were strongly preferred over models
trained with MSE or MAE, which conﬁrms our obser-
vation from the ﬁrst reader study. Comments by the
readers suggest that one major cause of this distinc-
tion is the blurriness associated with pixel-wise losses
such as MSE or MAE. In particular, two comments
describing the MSE-trained model mention the lack
of detail in the right periphery.

We observe that the favorite model among the ex-
perts in the second reader study was the third model

8

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

from the ﬁrst, frame-level reader study. The major
diﬀerence between this model and the other two mod-
els, also trained with the perceptual loss, is that it
uses bilinear interpolation for upsampling in the de-
coder rather than transposed convolution. In order
to study the diﬀerence between these two upsampling
implementations, we manually inspect the generated
frames from the models (one with bilinear upsampling
and the other with transposed convolution). Doing so
revealed that transposed convolution tends to amplify
checkerboard artifacts, as shown in Figure 5, which is
in line with earlier observations by Odena et al. (2016).
Although this eﬀect is subtle at the frame level, we
suspect that, when viewed in 3D, the artifacts produce
a more disorientating visual experience.

(a) TC (6)

(b) TC (5)

(c) BI (5)

Figure 5: Visual artifacts associated with upsampling
layers. All models use ﬁve frames, a 3D con-
volutional temporal module, and perceptual
loss. Models diﬀer only in the upsampling
layers. (a) and (b) use transposed convolu-
tion (TC), with six and ﬁve layers respec-
tively, while (c) has ﬁve layers of bilinear
interpolation (BI).

6. Conclusion

Minimally invasive endoscopic surgery is one of the
most important surgical advances of the past half-
century. The use of endoscopy, however, is often
limited by the steep learning curve associated with a
lack of depth perception, and the challenge of deliver-
ing high-quality visualization via increasingly smaller
optical instruments. Enhanced endoscopic imaging
through computer vision has the potential to further
democratize minimally invasive surgery.

In this paper, we proposed the task of converting 2D
endoscopy video to 3D video and investigated the fea-
sibility of deep learning for tackling the lack of depth
perception in surgical endoscopy. More speciﬁcally, we

9

designed modiﬁed U-Nets to generate a novel perspec-
tive (right view) given a series of consecutive left-view
frames and tested their eﬀectiveness by running an
extensive set of reader studies, both frame-level and
video-level, with experienced surgeons.

The reader studies along with thorough analysis re-
vealed a few major ﬁndings. First, surgeons preferred
the generated stereo video over the corresponding 2D
video. This conﬁrms the usefulness of the proposed
task and the eﬀectiveness of the proposed deep learn-
ing–based solution. Second, the models that were
favored by expert surgeons were all trained with a per-
ceptual loss and utilized multiple consecutive frames
of the observed view with a convolutional temporal
module. This ﬁnding 1) veriﬁes our hypothesis that
temporal information, available from multiple past
frames, is critical in reconstructing the missing view
in the stereo pair, and 2) shows that the proposed
variant of the U-Net is able to exploit such information
from input frames. Third, we identiﬁed two percep-
tual loss functions, DISTS and LPIPS, that correlate
strongly with expert judgement. These automatic
metrics will enable rapid iteration to improve algo-
rithms for solving the proposed task without relying
on time-consuming and costly reader studies. Finally,
we demonstrated the overall importance of expert
readers compared to non-expert readers (without any
medical experience) when assessing 2D-to-3D recon-
struction algorithms for surgical video. Overall, these
ﬁndings indicate that the proposed task is feasible and
useful, and that the proposed approach, despite being
the very ﬁrst attempt at this problem, is a promising
one.

Future Directions. The positive ﬁndings from this
paper present us with a number of future directions.
First, running another set of video-based reader stud-
ies with higher-resolution reconstruction and higher-
quality VR gear would further strengthen the useful-
ness of both the proposed task and the proposed ap-
proach. Second, better network architectures as well
as learning hyperparameters should be sought out,
now that our experiments have revealed advanced per-
ceptual loss to be a good proxy for expensive human
evaluation. Third, we expect more advanced tempo-
ral modules, such as a recurrent DNN-based module,
to generate better images by capturing longer-range
dependencies. Finally, this task is not necessarily lim-
ited to endoscopy, and we anticipate that our ﬁndings
will be transferable to similar problems in medicine
and other domains.

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

References

G. Berci and K. A. Forde. History of endoscopy:
What lessons have we learned from the past?
Surgical endoscopy, 14(1):5–15, 01 2000. URL http:
//proxy.library.nyu.edu/login?url=https:
//www.proquest.com/scholarly-journals/
history-endoscopy/docview/219490456/se-2?
accountid=12768. Copyright - Springer-Verlag
2000; Last updated - 2010-08-26.

Ralph Allan Bradley and Milton E Terry. Rank anal-
ysis of incomplete block designs: I. the method of
paired comparisons. Biometrika, 39(3/4):324–345,
1952.

Karin Correa, Andres Vivas, and Jose Sabater-
Navarro. Neurosurgery and brain shift: review
of the state of the art and main contributions of
robotics. TecnoLógicas, 20:125–138, 12 2017. doi:
10.22430/22565337.719.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.

Keyan Ding, Kede Ma, Shiqi Wang, and Eero P.
Image quality assessment: Unifying
Simoncelli.
structure and texture similarity.
IEEE Trans-
actions on Pattern Analysis and Machine Intel-
ISSN 1939-3539. doi:
ligence, page 1–1, 2020.
10.1109/tpami.2020.3045810. URL http://dx.doi.
org/10.1109/TPAMI.2020.3045810.

Keyan Ding, Kede Ma, Shiqi Wang, and Eero P.
Simoncelli. Comparison of full-reference image
quality models for optimization of image process-
ing systems. International Journal of Computer
ISSN 1573-
Vision, 129(4):1258–1281, Jan 2021.
1405. doi: 10.1007/s11263-020-01419-7. URL http:
//dx.doi.org/10.1007/s11263-020-01419-7.

Christoph Fehn.

Depth-image-based rendering
(DIBR), compression, and transmission for a new
approach on 3D-TV.
In Mark T. Bolas, An-
drew J. Woods, John O. Merritt, and Stephen A.
Benton, editors, Stereoscopic Displays and Vir-
tual Reality Systems XI, volume 5291, pages 93
– 104. International Society for Optics and Photon-
ics, SPIE, 2004. doi: 10.1117/12.524762. URL
https://doi.org/10.1117/12.524762.

David Firth. 1. overcoming the reference category
problem in the presentation of statistical models.

Sociological Methodology, 33(1):1–18, 2003. doi:
10.1111/j.0081-1750.2003.t01-1-00125.x.

Jamie Gompel, Mark Tabor, Ahmed Soliman Youssef,
Tsz Lau, Andrew Carlson, Harry Loveren, and
Siviero Agazzi. Field of view comparison be-
tween two-dimensional and three-dimensional en-
doscopy. The Laryngoscope, 124, 02 2014. doi:
10.1002/lary.24222.

Mei Han and Takeo Kanade. A perspective fac-
torization method for euclidean reconstruction
with uncalibrated cameras. The Journal of Vi-
sualization and Computer Animation, 13(4):211–
223, 2002.
https://doi.org/10.1002/vis.
290. URL https://onlinelibrary.wiley.com/
doi/abs/10.1002/vis.290.

doi:

Martin Heusel, Hubert Ramsauer, Thomas Un-
terthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule con-
verge to a local nash equilibrium, 2018.

Juhyeon Kim and Young Min Kim. Novel view syn-
thesis with skip connections. In 2020 IEEE Inter-
national Conference on Image Processing (ICIP),
pages 1616–1620, 2020. doi: 10.1109/ICIP40778.
2020.9191076.

Diederik P. Kingma and Jimmy Ba. Adam: A method

for stochastic optimization, 2017.

Kenan Koc, Ihsan Anik, Dilek Ozdamar, Burak
Cabuk, Gurkan Keskin, and Savas Ceylan. The
learning curve in endoscopic pituitary surgery and
our experience. Neurosurg. Rev., 29(4):298–305;
discussion 305, October 2006.

Atul Kumar, Yen-Yu Wang, Ching-Jen Wu, Kai-Che
Liu, and Hurng-Sheng Wu. Stereoscopic visualiza-
tion of laparoscope image using depth information
from 3d model. Computer Methods and Programs
in Biomedicine, 113(3):862–868, 2014. ISSN 0169-
2607. doi: https://doi.org/10.1016/j.cmpb.2013.
12.013. URL https://www.sciencedirect.com/
science/article/pii/S0169260713004033.

Khan W Li, Clarke Nelson, Ian Suk, and George I
Jallo. Neuroendoscopy: past, present, and future.
Neurosurg. Focus, 19(6):E1, December 2005.

Leida Li, Xi Chen, Yu Zhou, Jinjian Wu, and Guang-
ming Shi. Depth image quality assessment for view
synthesis based on weighted edge similarity. In Pro-
ceedings of the IEEE/CVF Conference on Computer

10

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Vision and Pattern Recognition (CVPR) Workshops,
June 2019.

06.005. URL https://www.sciencedirect.com/
science/article/pii/S1361841518303761.

Ping Li, Dirk Farin, Rene Gunnewiek, and Peter With.
On creating depth maps from monoscopic video us-
ing structure from motion. In Proc. 27th Symposium
on Information Theory in the Benelux, June 8-9,
2006, Noordwijk, The Netherlands, pages 85–91.
Werkgemeenschap voor Informatie- en Communi-
catietheorie (WIC), 2006. ISBN 978-90-71048-22-7.

Xingtong Liu, Ayushi Sinha, Mathias Unberath,
Masaru Ishii, Gregory D. Hager, Russell H. Tay-
lor, and Austin Reiter. Self-supervised learning for
dense depth estimation in monocular endoscopy.
In Danail Stoyanov, Zeike Taylor, Duygu Sarikaya,
Jonathan McLeod, Miguel Angel González Ballester,
Noel C.F. Codella, Anne Martel, Lena Maier-Hein,
Anand Malpani, Marco A. Zenati, Sandrine De Rib-
aupierre, Luo Xiongbiao, Toby Collins, Tobias Re-
ichl, Klaus Drechsler, Marius Erdt, Marius George
Linguraru, Cristina Oyarzun Laura, Raj Shekhar,
Stefan Wesarg, M. Emre Celebi, Kristin Dana,
and Allan Halpern, editors, OR 2.0 Context-Aware
Operating Theaters, Computer Assisted Robotic
Endoscopy, Clinical Image-Based Procedures, and
Skin Image Analysis, pages 128–138, Cham, 2018.
Springer International Publishing. ISBN 978-3-030-
01201-4.

Xingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory D.
Hager, Austin Reiter, Russell H. Taylor, and Math-
ias Unberath. Self-supervised learning for dense
depth estimation in monocular endoscopy. CoRR,
abs/1902.07766, 2019. URL http://arxiv.org/
abs/1902.07766.

Faisal Mahmood and Nicholas J. Durr. Deep learning-
based depth estimation from a synthetic endoscopy
image training set. In Elsa D. Angelini and Ben-
nett A. Landman, editors, Medical Imaging 2018:
Image Processing, volume 10574, pages 521 – 526.
International Society for Optics and Photonics,
SPIE, 2018a.
doi: 10.1117/12.2293785. URL
https://doi.org/10.1117/12.2293785.

Faisal Mahmood and Nicholas J. Durr. Deep
learning and conditional
random ﬁelds-based
depth estimation and topographical reconstruc-
tion from conventional endoscopy. Medical Im-
age Analysis, 48:230–243, 2018b.
ISSN 1361-
8415. doi: https://doi.org/10.1016/j.media.2018.

Web VR Concepts.
Mozilla Contributors.
Accessed Sept. 10,
URL
https://developer.mozilla.org/en-US/docs/
Web/API/WebVR_API/Concepts.

2021 [Online].

Augustus Odena, Vincent Dumoulin, and Chris Olah.
Deconvolution and checkerboard artifacts. Distill,
2016. doi: 10.23915/distill.00003. URL http://
distill.pub/2016/deconv-checkerboard.

Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto
Garcia-Garcia, John Alejandro Castro-Vargas, Ser-
gio Orts-Escolano, Jose Garcia-Rodriguez, and
Antonis Argyros. A review on deep learning
techniques for video prediction.
IEEE Transac-
tions on Pattern Analysis and Machine Intelli-
gence, page 1–1, 2020.
doi:
10.1109/tpami.2020.3045007. URL http://dx.doi.
org/10.1109/TPAMI.2020.3045007.

ISSN 1939-3539.

Stefan Petscharnig and Klaus Schöﬀmann. Learn-
ing laparoscopic video shot classiﬁcation for gy-
necological surgery. Multimedia Tools and Appli-
cations, 77(7):8061–8079, Apr 2018.
ISSN 1573-
7721. doi: 10.1007/s11042-017-4699-5. URL https:
//doi.org/10.1007/s11042-017-4699-5.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks for biomedical image
segmentation, 2015.

Radu Sibechi, Olaf Booij, Nora Baka, and Peter
Bloem. Exploiting temporality for semi-supervised
video segmentation, 2019.

Nahian Siddique, Paheding Sidike, Colin Elkin, and
Vijay Devabhaktuni. U-net and its variants for med-
ical image segmentation: theory and applications,
2020.

Karen Simonyan and Andrew Zisserman. Very deep
convolutional networks for large-scale image recog-
nition, 2015.

Rui Song, Hyunsuk Ko, and C.-C. Jay Kuo. MCL-3D:
a database for stereoscopic image quality assess-
ment using 2d-image-plus-depth source. CoRR,
abs/1405.1403, 2014. URL http://arxiv.org/
abs/1405.1403.

11

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Mehmet Turan, Yusuf Yigit Pilavci, Ipek Ganiyusu-
foglu, Helder Araujo, Ender Konukoglu, and Metin
Sitti. Sparse-then-dense alignment-based 3d map re-
construction method for endoscopic capsule robots.
Machine Vision and Applications, 29(2):345–359,
2018.

Heather L. Turner, Jacob van Etten, David Firth,
and Ioannis Kosmidis. Modelling rankings in
R: The PlackettLuce package. Computational
Statistics, 35:1027–1057, 2020.
10.1007/
s00180-020-00959-3. URL https://doi.org/10.
1007/s00180-020-00959-3.

doi:

Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig,
Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy,
and Thomas Brox. Demon: Depth and motion
network for learning monocular stereo. CoRR,
abs/1612.02401, 2016. URL http://arxiv.org/
abs/1612.02401.

Zhou Wang, Alan Conrad Bovik, Hamid Rahim
Sheikh, Student Member, Eero P. Simoncelli, and
Senior Member. Image quality assessment: From
error visibility to structural similarity. IEEE Trans-
actions on Image Processing, 13:600–612, 2004.

Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and
Justin Johnson. Synsin: End-to-end view synthesis
from a single image, 2020.

Junyuan Xie, Ross Girshick, and Ali Farhadi. Deep3d:
Fully automatic 2d-to-3d video conversion with deep
convolutional neural networks, 2016.

Menglong Ye, Edward Johns, Ankur Handa, Lin
Zhang, Philip Pratt, and Guang-Zhong Yang. Self-
supervised siamese learning on stereo image pairs
for depth estimation in robotic surgery, 2017.

Richard Zhang, Phillip Isola, Alexei A. Efros, Eli
Shechtman, and Oliver Wang. The unreasonable
eﬀectiveness of deep features as a perceptual metric,
2018.

Appendix A. Hamlyn da Vinci

Dataset

The da Vinci dataset from the Hamlyn Center for
Robotic Surgery (Ye et al., 2017) contains rectiﬁed
stereo images of size 384 × 192 pixels partitioned into
34,241 train and 7,192 test image sequences from video.

To generate train and validation data splits, the full
train image sequence is split into sequences of 1000
images. After this, 80% of the splits are randomly
allocated to the train set and the remaining 20% are
allocated to the validation set. To generate samples
from each data split, a sliding window of length K,
where K is the image input sequence length for the
model, is applied to each sequence of 1000 images
in their respective data split. Individual samples are
shuﬄed again before training.

The dataset does not disclose the frame rate of
the original video stream. We found that generating
video from the dataset images at 20 frames per second
yielded realistic results for evaluation.

(a) Left view (input)

(b) Right view (target)

Figure 6: A sample of the left and right views from
the dataset released by the Hamlyn Center
for Robotic Surgery (Ye et al., 2017).

Appendix B. Test Set Performance

We verify that the model performance in terms of
DISTS and LPIPS, the two most successful automatic
metrics, is consistent on the oﬃcial Da Vinci test
dataset.

Appendix C. Baseline Results

Baseline results were generated using the procedure
described in Section 4.3. Although most automatic
metrics ranked baseline models poorly when compared
to U-Net based models, FID failed as a reliable mea-
sure in this study. The FID metric ranked the base-
line models higher than the trained models (Table 5).
Images generated by the baseline models (Figure 7)
contain discontinuities, a duplicated strip of the orig-
inal image or a black strip in place of the missing
pixels, and a constant global disparity. Since the base-
line models are not candidate solutions to the task
of 2D-to-3D reconstruction, we ﬁnd FID using 2048

12

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Rank Model

DISTS LPIPS

1
2
3
4
5
6
7
8

5 fr+perceptual+6 layers 0.129
5 fr+perceptual
0.141
5 fr+perceptual+bilinear 0.147
0.156
1 fr+perceptual
0.142
10 fr+perceptual
0.157
1 fr+MSE+6 layers
0.159
1 fr+MAE
0.152
5 fr+MSE+6 layers

0.148
0.155
0.166
0.180
0.167
0.194
0.200
0.187

Table 4: Perceptual metric performance on oﬃcial da

Vinci test set.

ﬁnal average pooling features to be unsuitable in the
context of our task.

(a) Left view (in-

(b) Right view (tar-

put)

get)

(c) Baseline with
pixel

original
ﬁll

(d ) Baseline with

black ﬁll

Figure 7: Examples of baseline outputs with global

disparity and basic in-painting methods.

supervising surgeon to assess perceptual diﬀerences
and rankings of the generated images as additional
screening for the reader study.

All spatio-temporal models based on maximum or
average pooling temporal modules were eliminate from
the study as the generated images were poor qual-
ity (Figure 8). Stacking multiple frames along the
channel axis did not result in signiﬁcant qualitative or
quantitative improvements and were also ﬁltered from
the study. Thus, all multi-frame models included in
the reader study used a spatio-temporal architecture
with 3D convolutions.

D.2. Image-based reader study format

The image-based reader study was completed remotely
by subjects using a Google Forms survey. The ﬁrst
page summarized the background, goals, and methods
for the project. In addition, we state instructions for
the survey. The second page of the survey requests
the subject’s profession and if they have performed
endoscopic surgery (2D and/or 3D). On the third
page, readers are presented a set of instructions, a set
of questions, and a link to the directory containing
the image comparison questions (Figure 9).

D.3. Qualitative Examples

D.4. Multi-frame models vs. single frame

models

To validate that increased performance for multi-frame
models was not due to increased model complexity
(3× increased model complexity when compared to the
equivalent single frame model), we re-train the best
multi-frame model (as ranked by LPIPS and DISTS
and the reader study) with the repeated frames as
input. This repeated frame model resulted in sig-
niﬁcantly lower LPIPS and DISTS values than the
equivalent model conditioned on past frames.

Appendix D. Image Reader Study

D.5. Bradley Terry ranking model

D.1. Initial model screening for reader study

Eight models (Table 7) were selected for the frame-
level reader study from an initial set of over 40 diﬀerent
conﬁgurations with varying architecture and loss as
presented in Table 6. As part of the eﬀort to identify
automatic metrics for model selection, we selected
the top eight models based on four metrics: LPIPS,
FID, SSIM and PSNR. In addition, we ran multiple
multiple rounds of qualitative assessments with the

Given a set of models to be ranked, the probability
of selecting the output from some model i from a
presented pair of outputs from model i and j is

P (i) =

αi
αi + αj

where αi is the worth of item i. We use the Plack-
ettLuce package (Turner et al., 2020), which reduces
to the Bradley-Terry model given a list of pairwise-
comparisons, to estimate relative rankings of the

13

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Baseline Model

Pixel Shift DISTS LPIPS

FID

SSIM PSNR

Copy pixels, perceptual loss
Copy pixels, MSE loss
Copy pixels, MAE loss
Fill zeros, perceptual loss
Fill zeros, MSE loss
Fill zeros, MAE loss

-31
-37
-37
-384
-32
-33

0.129
0.128
0.128
0.800
0.145
0.146

0.201
0.205
0.205
0.840
0.237
0.237

30.079
32.648
32.648
549.658
35.148
35.655

0.493
0.486
0.486
0.002
0.458
0.455

17.675
17.728
17.728
5.254
16.067
16.043

Table 5: Constant global disparity baseline results optimized on the validation set. Results are reported on

the validation set.

Model Component

Tested Conﬁgurations

Loss function
Temporal structure
Number of input frames
U-Net depth
Upsampling mechanism transposed convolution or bilinear interpolation upsampling
Sigmoid output
Extra skip connection

MSE, MAE, Perceptual
channel-stacking, spatio-temporal, 3D convolutions, max-pooling, avg-pooling
1, 5, or 10
5 or 6 blocks

Yes/No
Yes/No

Table 6: A summary of the U-Net architecture and losses tested in the initial search for candidate models for

the expert reader study.

(a)

(b)

(c)

.

Figure 8: Models using maximum (b) or average pool-
ing (b) temporal modules were found to be
unsuitable for the task due to poor quan-
titative and qualitative performance when
compared to the target output (a)

reader study models. We estimate worth and bounded
quasi-standard errors (Firth, 2003) for each model
and transform them to log-scale. Since we expect all
models compared to have worth greater than 0, we
transform worth to the log scale to provide bounds
on conﬁdence intervals such that then are constrained
to be greater than 0. We choose the target images as
the reference such that the quasi-standard errors for
the other models can be estimated.

Appendix E. VR Video Reader Study

We selected the three top ranked models from Ta-
ble 1 as well as the top MAE and MSE models to
generate stereo video for the VR video reader study.
We selected a diverse set of models to determine if
image-based reader study results match video-based
reader study results.

In the VR video reader study, subjects were given
a Google Cardboard headset with an iPhone 12 Pro

14

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Model description

Loss

1 frame
1 frame
1 frame, 6 layers
5 frames
5 frames, 6 layers
5 frames, 6 layers
5 frames, bilinear upsampling Perceptual
10 frames

Perceptual
MAE
MSE
Perceptual
Perceptual
MSE

MSE

Table 7: The eight U-Net models selected for the
frame-level reader study. Unless speciﬁed
otherwise, each model used ﬁve double con-
volution layers and used transpose convolu-
tion for upsampling. If multiple input frames
were used, a spatio-temporal U-Net architec-
ture was used with 3D convolution temporal
module.

(a)

(b)

(c)

(d )

(e)

(f )

(g)

(h)

Figure 9: Reader study participants were shown a
series of two-alternative forced choice ques-
tions with no time limit and asked to select
the higher quality candidate stereo image
with the question: Given the left eye view,
which of the generated right eye views (A
or B) is better quality?

Figure 10: Generated images from the top eight mod-
els as ranked by the image-based reader
study. Models are ranked from best (a)
to worst (h). Refer to Table 1 for model
descriptions and model performance.

as the video source. Subjects were shown six videos
of three scenes of approximately 20 seconds in length
per model. Diﬀerent scenes were selected based on
variation in depth, movement, and color in each scene.
Subjects were ﬁrst shown the scene in 2D in which

both eyes are shown the exact same video. Subjects
are then shown a generated stereo 3D video of the
same scene seen previously. Subjects are asked three
questions: 1) Would you prefer the ﬁrst video or
second video when doing surgery? 2) Rate the stereo

15

Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery

Model description

Avg

StdDev Comments

5 fr, perceptual, bilinear

4.67

0.52

5 fr, perceptual

3.83

0.98

5 fr, perceptual, 6 layers

3.50

0.55

1 fr, MSE, 6 layers

2.83

0.75

1 fr, MAE

2.83

0.98

“Perfect.”
“Great.”
“Really like this one.”
“Temporal inconsistencies in grasper with shearing
between frames”
“Loves it in 3D. Surprised at how much I like it.”
“Looked good. Couldn’t see text on instrument.”
“Periphery is not as crisp.”
“Right periphery is not very good.”
“Blurrier, little disorienting.”
“Globally less sharpness. Less details. Less pro-
nounced stereoscopic eﬀect”
“Depth perception was not as good as the ﬁrst round.”
“Blurrier, still pretty good.”
“Not as detailed.”

Table 8: We expand Table 3 and include informative comment excerpts from the readers for VR model
evaluation. For all models, both surgeons preferred the generated stereo video compared to the raw
2D alternative.

3D video quality from 1-5 with 5 being best? 3)
Provide general feedback about the quality of the
video based on your experience in surgery.

16

