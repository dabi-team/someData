2
2
0
2

r
p
A
0
1

]

M
M

.
s
c
[

2
v
7
0
1
3
0
.
3
0
2
2
:
v
i
X
r
a

Privacy Leakage in Proactive VR Streaming:

1

Modeling and Tradeoff

Xing Wei, Chenyang Yang, and Chengjian Sun

Abstract

Proactive tile-based virtual reality (VR) video streaming employs the viewpoint of a user to predict

the tiles to be requested, renders and delivers the predicted tiles before playback. Recently, it has

been found that the identity and preference of the user can be inferred from the trace of viewpoint

uploaded for proactive streaming, which indicates that viewpoint leakage incurs privacy leakage. In this

paper, we strive to answer the following questions regarding viewpoint leakage during proactive VR

video streaming. When is the viewpoint leaked? Can privacy-preserving approaches (e.g., federated or

individual training, using predictors with no need for training, or predicting locally) avoid viewpoint

leakage? We ﬁnd that if the prediction error or the quality of experience (QoE) metric is uploaded

for adaptive streaming, the real viewpoint can be inferred even with the privacy-preserving approaches.

Then, we deﬁne viewpoint leakage probability to characterize the accuracy of the inferred viewpoint,

and respectively derive the probability when uploading prediction error and QoE metric. We ﬁnd that

the viewpoint leakage probability can be reduced by sacriﬁcing QoE or increasing resources. Simulation

with the state-of-the-art predictor over a real dataset shows that such a tradeoff does not exist only in

rare cases.

Privacy-preserving VR, viewpoint leakage, proactive VR video streaming

Index Terms

I. INTRODUCTION

As the main type of wireless virtual reality (VR) services, 360◦ video streaming consumes a

large amount of computing and communication resources to avoid playback stalls and black holes

that degrade the quality of experience (QoE) [1], [2]. However, humans can only see a small

April 12, 2022

DRAFT

 
 
 
 
 
 
2

spherical cap of the full panoramic sphere (e.g., 18% of the sphere [3]) at arbitrary time, which is

called ﬁeld of view (FoV). Considering that the center of the FoV (i.e., viewpoint) is predictable,

proactive VR video streaming is proposed to improve the QoE with less resources [4], which

has been investigated intensively. In [5]–[12], communication, computing, or caching resource

allocation was optimized in various scenarios. In [13], [14], viewpoint prediction methods were

proposed and evaluated. In [15]–[20], both viewpoint prediction and resources allocation were

studied. The QoE depends on the performance of prediction and the conﬁgured resources for

communication and computing. To keep the QoE stable during video streaming, the instantaneous

prediction error or the QoE should be uploaded for adaptive streaming [11], [12], [18], [21].

A. Motivation and Major Contributions

Proactive VR video streaming contains two stages: predictor training and online streaming.

Predictor training can be operated at the multi-access edge computing (MEC) server or the head

mounted display (HMD), and is unnecessary for some simple predictors as those evaluated in

[4], [14]. Online streaming contains viewpoint predicting, proactive rendering and delivering.

Most existing works in literature neglect a fact that privacy may be leaked during proactive

VR video streaming, where the request of a VR video and the trace of viewpoint should be

uploaded. Although the request of a VR video can be anonymous to avoid privacy leakage, as

the key information to improve QoE and save resources, the trace of viewpoint is inevitable to

be uploaded. Then, will privacy be leaked from the viewpoint trace?

Recent works reveal the risk. With less than ﬁve minutes’ viewpoint trace, the identity of 95%

users among all the 511 users can be correctly identiﬁed as demonstrated in [22]. Furthermore,

with the trace of viewpoint, the content in the FoV that a user chooses to see is also leaked. This

can be used to infer the intent and preference of the user [23]. Therefore, viewpoint leakage

indeed incurs privacy leakage.

When considering viewpoint leakage in proactive VR video streaming procedure, several

DRAFT

April 12, 2022

3

fundamental questions arise:

• Can existing privacy-preserving approaches, e.g., federated learning in predictor training

stage and local predicting in online streaming stage, avoid viewpoint leakage?

• If the viewpoint leakage is unavoidable, when does the most serious leakage happens, and

when does the most minor leakage happens?

• Which factors affect viewpoint leakage? Is there any relation between viewpoint leakage,

QoE, and conﬁgured resources?

In this paper, we strive to answer these questions. Our main contributions are as follows.

• We ﬁnd that although the viewpoint leakage can be avoided with privacy-preserving ap-

proaches in predictor training, the real viewpoint can be inferred during online streaming.

• To characterize the leakage, we deﬁne viewpoint leakage probability and derive the prob-

ability when uploading the prediction error and the QoE metric, from which we ﬁnd the

conditions that the probability achieves the maximum or minimum.

• We ﬁnd that viewpoint leakage probability depends on the prediction error and conﬁgured

resources. Moreover, there is a privacy-QoE tradeoff and a privacy-resources tradeoff, where

to reduce the leakage probability, either the QoE has to be scariﬁed or more communica-

tion and computing resources should be conﬁgured. Simulations with the state-of-the-art

predictor over a real dataset show that the tradeoffs exist only except rarely happened cases

when uploading the QoE metric.

B. Related Works

As far as the authors known, there are no prior works to investigate viewpoint leakage in

proactive VR video streaming.

Existing works in the ﬁeld of computer science propose to protect real gaze position (another

useful data for viewpoint prediction) by adding noise in spatial domain [23]–[26] or reducing

the samples of uploaded gaze position in temporal domain [23], [27]. However, gaze position

April 12, 2022

DRAFT

4

may not always be useful for viewpoint prediction. In fact, the state-of-the-art accuracy of

viewpoint prediction on real dataset [3] can be achieved only with the trace of viewpoint [14].

Moreover, the real gaze position (and also the real viewpoint) can be stored at the HMD and

hence is unnecessary to protect when the privacy-preserving approaches (say federated learning)

are used. Existing works never consider the fact that the gaze position or viewpoint can still

be inferred with the privacy-preserving approaches. Besides, Existing works consider the gaze

position on two dimensional (2D) plane. Yet the actual gaze position or viewpoint is on the

sphere, the impact of which on the privacy leakage has never been investigated.

There is only one related work in the ﬁeld of wireless communications, which considers

federated training and local predicting for privacy-preserving [19] but does not study if the

viewpoint can really be protected.

C. Outline

The rest of this paper is organized as follows. Section II introduces the system model, the

procedure of proactive VR video streaming, and the deﬁnitions of the prediction error and

QoE metric. Section III analyzes the viewpoint leakage issue and deﬁnes viewpoint leakage

probability. Section IV and V derive and analyze the leakage probability when uploading the

prediction error and the QoE metric, respectively. Trace-driven simulation results are provided

in Section VI. Section VII concludes the paper.

II. SYSTEM MODEL

Consider a proactive VR video streaming system serving K users with a MEC server, which

accesses a VR video library by local caching or high-speed backhaul, thus the delay from the

Internet to the server can be ignored. The server is also equipped with powerful computing units

for training, predicting, and rendering.

Each user wears an HMD, which can measure and upload the trace of viewpoint,1 calculate

1Since tile requests can be transformed into the viewpoints [28], we do not consider the case that uploading the tile requests.

DRAFT

April 12, 2022

5

and upload the value of real-time QoE or prediction error, and pre-buffer segments. The HMD

can also equip with a light-weighted computing unit for training a predictor and predicting the

viewpoint. During the playback, each user can turn around freely to view FoVs.

Each 360◦ video consists of L segments, each segment consists of Nf panoramic video frames

in temporal domain, and each frame consists of M tiles in spatial domain. For ease of analysis,

we assume that the areas of all the tiles within a FoV and a streamed ﬁeld of view (SFoV) in a

panoramic frame can be respectively approximated as the area of the FoV and SFoV [7]. This

can be achieved by sufﬁciently ﬁne grained tiling or adaptive tiling [12].

Fig. 1: Streaming the ﬁrst four segments of a VR video. tb is the start time of the observation

window, te is the start time of playback of the l0th segment, l0 = 3.

A. Proactive VR Video Streaming Procedure

To predict the viewpoints, if a predictor needs to be trained, the proactive streaming procedure

contains two stages: (1) ofﬂine predictor training, and (2) online video streaming. Otherwise,

stage (1) can be omitted (say for the simple predictors used in [4], [14]).

April 12, 2022

DRAFT

2D SFoVs Viewpoint tracePlayback PassiveProactiveInitial delayt1RenderingTransmitting 2343344Predicting and tile mappingcpttcomtpdwsegTTccTpsTbtetobwT6

Predictor training can be operated at the MEC server or the HMD. When training at the server

(i.e., centralized learning), the real viewpoint traces of each user should be uploaded. When

training at each HMD (i.e., federated learning), the real viewpoint traces can be stored locally

and only the model parameters of the predictor are uploaded to the server. When training local

predictors individually at each HMD, both the real viewpoint traces and the model parameters

of predictors can be stored at the HMD.

The procedure in the online streaming stage is shown in Fig. 1. When a user requests a VR

video, the MEC server ﬁrst renders and transmits the initial (l0 −1) segments in a passive manner

[29]. After an initial delay, the ﬁrst segment begins to play at the time instant tb, i.e., the start

time of an observation window. Then, proactive streaming begins. In the sequel, we take the l0th

segment as an example for elaboration.

After the viewpoint trace in the observation window with duration Tobw is collected (i.e., at

the end time of the window), the viewpoint sequence in a prediction window with duration

Tpdw = Tseg can be predicted at the MEC server or a HMD, and the computing and communica-

tion resources can be conﬁgured at the MEC. According to the distance between the predicted

viewpoint and the center of each tile, the MEC server can determine the tiles for the Nf frames

of the segment to be streamed [20], [28]. Then, the server renders these tiles with duration tcpt

to generate the Nf images of SFoVs, which are then transmitted with duration tcom. To avoid

stalling, Nf SFoVs in the l0th segment should be delivered before the playback start time of the

l0th segment, i.e., the time instant te. The duration from tb to te is the online proactive streaming

time for a segment, denoted as Tps. We can observe from the ﬁgure that Tps = (l0 − 1)Tseg,

where in the example l0 = 3 and hence Tps = 2Tseg. The durations for observation, computing,

and transmission should satisfy Tobw + Tcc = Tps, where Tcc (cid:44) tcom + tcpt is the total duration

for communication and computing. A predictor can be more accurate with a smaller value of

Tcc. This is because the viewpoints to be predicted are closer to and hence are more correlated

with the viewpoint sequence in the observation window [20]. Given a predictor and the required

DRAFT

April 12, 2022

viewpoint prediction accuracy, the value of Tcc can be pre-determined [20], [28].

During the playback of each segment, the instantaneous QoE metric or the instantaneous

prediction error can be calculated at the HMD and uploaded to the MEC server for adaptive

streaming, i.e., when the prediction error or the QoE is unacceptable, the server streams more

7

tiles [11], [12], [18], [21].

B. Field of View on Sphere

When watching a panoramic video, the user wearing an HMD is at the center of the unit sphere,

i.e., O, as illustrated in Fig. 2a. The FoV of the HMD can be considered as a spherical cap of the

sphere [3], [14], [28]. For any given HMD, the size of FoV is determined. Denote viewpoint as

Ov. The spherical distance from Ov to the base of the cap is rfov (in radian), and is referred to as

the “cap radius”. The half apex angle of the cone corresponding the cap is α. When measures in

radian, α = rfov

r = rfov. Then, the area of the FoV is Afov = 2π (1 − cos(α)) = 2π (1 − cos(rfov)).

(a) FoV

(b) SFoV

(c) Aol (red region)

Fig. 2: FoV, SFoV, and QoE on the unit sphere, Ov and Op are respectively the real and predicted

viewpoint, rfov and rsv are respectively the radius of FoV and SFoV, e is the prediction error.

C. Computing and Transmission Model

1) Computing Model: For each user, the number of bits that can be rendered per second,

referred to as the computing rate [20], is ccpt,k (cid:44) Fcpt
K·µr

(in bit/s), where Fcpt is the conﬁgured

April 12, 2022

DRAFT

fovrOrvOrsvrpOOeOpOvO8

resource at the server for rendering (in ﬂoating-point operations per second, FLOPs/s), µr is the

required ﬂoating-point operations (FLOPs) for rendering one bit of FoV (in FLOPs/bit) [20].

2) Transmission Model: The base station co-located with the server equipped with Nt antennas

serves K single-antenna users using zero-forcing beamforming. The instantaneous data rate in

tcom for the kth user is ccom,k = B log2

(cid:16)

1 + pkd−β

k |(hk)H wk|2

σ2

(cid:17)

, where B is the bandwidth, hk and

wk are respectively the channel vector and beamforming vector, dk is the distance from the BS

to the user, pk is the transmit power, β is the path-loss exponent, σ2 is the noise power, and (·)H

denotes conjugate transpose. With duration tcom, the number of bits in a segment that can be

protectively transmitted is ccom,ktcom, where ccom,k is the time-average data rate over tcom. Since

the future channels of each user are unknown when the communication resource is allocated at

the end of the observation window, we use ensemble-average rate Eh{ccom,k} to approximate

ccom,k [20], where Eh{·} is the expectation over h.

Since the resources are conﬁgured orthogonally among K users, we consider an arbitrary user

and omit the subscript k in the computing rate and ensemble-average data rate in the sequel.

3) Capability for Streaming: We use the ratio of tiles in a segment that can be rendered and

transmitted with conﬁgured resources (reﬂected implicitly by the data rate and computing rate)

to measure the capability of the system for streaming tiles, which is [20]

®

C = min

Nf M (

Tcc
scom

Eh{ccom} + scpt

ccpt

´

, 1

∈ [0, 1]

)

(1)

where scom = pxw·pxh·b/γc [30] is the number of bits in a tile for transmission, scpt = pxw·pxh·b

is the number of bits in a tile for rendering, pxw and pxh are the pixels in width and height of

a tile, b is the number of bits per pixel relevant to color depth, and γc is the compression ratio

of video.

The number of tiles within SFoV is identical among Nf frames in a segment, hence the ratio

of the SFoV to the panoramic frame is also C, i.e., C = Asv

4π , where Asv denotes the area of the

SFoV, and 4π is the surface area of the unit sphere.

DRAFT

April 12, 2022

9

D. Streamed Field of View on Sphere

Taking the predicted viewpoint Op as the center, SFoV contains all the tiles whose spherical

distances from their centers to Op are no more than a given value, thus we can assume that the

SFoV is also a spherical cap with center Op, as shown in Fig. 2b. Then, the area of the SFoV is

Asv = 2π

(cid:16)

1 − cos (cid:0)rsv

(cid:1)(cid:17)

(2)

where rsv is the cap radius of the SFoV.

By substituting (1) and C = Asv

4π into (2), we obtain the radius of SFoV as

rsv = arccos (1 − 2C) = arccos

1 − 2 min

Nf M (

Ç

®

Tcc
scom

Eh{ccom} + scpt

ccpt

´å

, 1

)

∈ [0, π]

(3)

Remark 1: As shown in (3), rsv respectively increases with Eh{ccom} or ccpt. Hence, rsv can

be used to reﬂect the amount of resources conﬁgured for VR video streaming.

When rsv = π, according to (3) and (2), C = 1 and Asv = 4π, which indicates that the

complete panoramic sphere is streamed. In this case, proactive VR streaming degrades into

passive streaming and prediction is unnecessary [29]. When rsv = 0, C = 0 and Asv = 0, which

indicates that no resources are conﬁgured and the panoramic frame is not streamed at all.

E. Spherical Distance as the Prediction Error

Spherical distance, known as orthodromic distance, has been considered as the most appropri-

ate metric to measure the viewpoint prediction error of FoV on the unit sphere [14]. As shown in

Fig. 2c, the spherical distance from the real viewpoint Ov = (θv, ϕv) to the predicted viewpoint

Op = (θp, ϕp) can be expressed as

e (cid:44) d(Ov, Op) = arccos (cos(ϕv) cos(ϕp) cos(|θv − θp|) + sin(ϕv) sin(ϕp)) ∈ [0, π]

(4)

where d(x, y) denotes the spherical distance between two points x and y, θ and ϕ are respectively

the longitude and latitude of a point on a unit sphere [14], [31]. When e = 0, the predicted and

real viewpoints coincide. When e = π, the prediction error reaches the maximum.

April 12, 2022

DRAFT

10

After the user watches a video frame of a segment, the instantaneous prediction error can be

calculated from (4) at the HMD.

F. Quality of Experience

For proactive VR video streaming, there will be no latency if the SFoVs in a segment can be

delivered before the playback of the segment. However, due to the prediction error or insufﬁcient

resources, black holes may appear [1]. We deﬁne the correctly streamed portion in a FoV as the

QoE metric [20], i.e., the ratio of the overlapped area of FoV and SFoV to the area of FoV, as

shown in Fig. 2c. Denote the overlapped area of FoV and SFoV as Aol ∈ [0, min{Afov, Asv}],

then the QoE metric can be expressed as

QoE (cid:44) Aol
Afov

∈ [0, 100%]

(5)

which is larger when the prediction error is smaller [20]. After the user watches a video frame

of a segment, the value of QoE can be calculated at the HMD.

III. VIEWPOINT LEAKAGE PROBABILITY IN PROACTIVE VR VIDEO STREAMING

In this section, we show that although real viewpoint is unnecessary to be uploaded during

proactive VR video streaming, it can still be inferred from the uploaded viewpoint prediction

and the prediction error or the QoE metric. Then, we deﬁne the viewpoint leakage probability.

A. Viewpoint Leakage in Proactive Streaming Procedure

During the proactive streaming procedure, the viewpoint may be leaked out in the predictor

training or the online streaming stage. According to whether a predictor requires training and

where the training and predicting are respectively executed, we provide seven cases in Table I.

We can ﬁnd that privacy-preserving approaches (i.e., training in federated manner [19], training

individually at each HMD, selecting training-free predictor [4], [14], or predicting locally [19])

can avoid the real viewpoint leakage in the predictor training stage and viewpoint observation of

DRAFT

April 12, 2022

TABLE I: Uploaded data from HMD in proactive VR video streaming procedure

11

No.

Where

Where

to train

to predict

Predictor training

Online streaming

Viewpoint observation Viewpoint prediction Adaptive streaming

1

2

3

4

5

6

7

MEC

MEC

HMD∗

HMD

HMD

MEC

HMD

HMD

Real viewpoint

Real viewpoint

Real viewpoint

Predicted viewpoint

Predicted viewpoint

Prediction error or

HMD

Model parameters

Predicted viewpoint

QoE metric

MEC

Model parameters

Real viewpoint

No need

MEC

No need

HMD

Real viewpoint

Predicted viewpoint

∗ This is training individually at each HMD for local predictors. Other cases that training at the HMD are federated.

online streaming stage. Nevertheless, the predicted viewpoint needs to be uploaded for viewpoint

prediction in the online streaming stage. With the uploaded instantaneous prediction error or QoE

metric, the real viewpoint may still be inferred. For example, when e = 0 or QoE = 100% and

the area of SFoV equals to the area of FoV (i.e., rsv = rfov), one can infer that the predicted

viewpoint is the real viewpoint, because the prediction error or QoE indicates the accuracy of

the predicted viewpoint. Then, a natural question is, with the predicted viewpoint at hand, with

what probability the real viewpoint can be inferred from the prediction error or the QoE?

B. ε-Viewpoint Leakage Probability

We ﬁrst deﬁne the viewpoint-sensitive neighborhood, viewpoint leakage event, and the possible

viewpoint zone. Denote V as the set of all points on a unit sphere.

Deﬁnition 1: ε-viewpoint-sensitive neighborhood N (Ov, ε): A subset of V where the spherical

distance between every point in the subset and the real viewpoint Ov is no larger than ε, i.e.,

N (Ov, ε) (cid:44) {x|d(x, Ov) ≤ ε and x ∈ V}, where ε ∈ [0, rfov].

N (Ov, ε) is a region within FoV that a user is not willing to be leaked. The value of ε

reﬂects the privacy requirement of the user. When ε = 0, the user has no privacy requirement.

When ε = rfov, the whole FoV is required not to be leaked. We refer to ε as “the radius of the

viewpoint-sensitive neighborhood”.

April 12, 2022

DRAFT

12

Deﬁnition 2: Possible viewpoint zone Zv: A subset of V that consists of all possible inferred

viewpoints, given the predicted viewpoint Op and the prediction error or the QoE.

If e = 0 or QoE = 100% and rsv = rfov, the only one inferred viewpoint is the real viewpoint

and Zv = {Ov}. If the prediction error or the QoE cannot provide any useful information, the

possibly inferred viewpoint can be arbitrary one viewpoint on the unit sphere and Zv = V.

Deﬁnition 3: ε-viewpoint leakage event: When inferring the real viewpoint, the inferred view-

point falls in the ε-viewpoint-sensitive neighborhood, i.e., (cid:98)Ov ∈ N (Ov, ε), where (cid:98)Ov is the

inferred viewpoint.

Deﬁnition 4: ε-viewpoint leakage probability: The probability that ε-viewpoint leakage event

happens, which is

¶

(cid:98)Ov ∈ N (Ov, ε)

©

= min

Pr

ß λ [N (Ov, ε)]
λ[Zv]

™

, 1

(6)

where λ[X ] is the Lebesgue measure of X . When X is a set of all viewpoints in a curve or a

surface, λ[X ] is the length of the curve or the area of the surface.

When the prediction error or the QoE can provide sufﬁcient information such that the possible

viewpoint zone is limited in the ε-viewpoint-sensitive neighborhood (i.e., λ[Zv] ≤ λ [N (Ov, ε)]),

the real viewpoint is ε-leaked (i.e., Pr

(cid:98)Ov ∈ N (Ov, ε)

= 1).

¶

©

When the prediction error or the QoE cannot provide any useful information such that the

possible viewpoint zone is the whole sphere, the real viewpoint can be fully protected. In this

case, the measure of the possible viewpoint zone is the surface area of the sphere (i.e., λ[Zv] =

4π), the measure of viewpoint sensitive neighborhood is the area of a spherical cap with center

Ov and radius ε (i.e., λ[N (Ov, ε)] = 2π(1 − cos(ε))), and the viewpoint leakage probability

achieves its global minimum, which is Prmin = 1−cos(ε)

2

.

Remark 2: Pr

(cid:98)Ov ∈ N (Ov, ε)

is a monotonically increasing function of ε.

¶

©

This is because Pr(cid:8)

(cid:98)Ov ∈ N (Ov, ε)(cid:9) is a monotonically increasing function of λ[N (Ov, ε)],

and λ[N (Ov, ε)] monotonically increases with ε. Remark 2 indicates that the viewpoint privacy

will be harder to be protected if the required viewpoint-sensitive neighborhood is larger.

DRAFT

April 12, 2022

13

In the sequel, we derive and analyze the ε-viewpoint leakage probability when uploading

prediction error and QoE metric, respectively.

IV. VIEWPOINT LEAKAGE PROBABILITY WHEN UPLOADING PREDICTION ERROR

In this section, we derive the leakage probability when uploading prediction error. We ﬁnd

a conditional tradeoff between reducing the leakage probability and improving QoE, while

the targets of satisfying the viewpoint privacy requirement and maximizing QoE are always

contradictory.

A. ε-Viewpoint Leakage Probability

When the server obtains the predicted viewpoint Op and prediction error e, one can only infer

that the real viewpoint Ov is on a circle, i.e., the possible viewpoint zone is a circle with center

Oe as shown in Fig. 3a. The ε-viewpoint-sensitive neighborhood becomes an arc of the circle

with center Ov. In the sequel, we ﬁrst derive the measures of the zone Zv and neighborhood

N (Ov, ε), respectively, based on which we obtain the leakage probability.

(a) Possible viewpoint zone and ε-

viewpoint-sensitive neighborhood.

(b) Pre v.s. e and ε

(c) Pre v.s. e, ε = 0.4rfov.

Fig. 3: Viewpoint leakage probability when uploading e. There exist a tradeoff region and a

consistent region between Pre and QoE.

The measure of Zv is the circumference of the circle, i.e., λ[Zv] = 2πre, where re is the

radius of the circle and is a function of e as derived in the following. The straight-line distance

April 12, 2022

DRAFT

1r=Possible viewpoint zone -viewpointsensitiveneighborhood eerOpOaOeOvO00.20.40.60.800.20.40.60.81TradeoffConsistency14

from an arbitrary viewpoint on the circle Oa to the center of the sphere O is the radius of the

sphere r = 1. The angle between the line OpO and the line OaO is θ. When measured in radian,

θ = e. The radius of the circle is re = sin(θ) · r = sin(e). By substituting re = sin(e) into

λ[Zv] = 2πre, the measure of Zv is λ[Zv] = 2π sin(e).

The measure of N (Ov, ε) is the arc length 2ε. Since the length of the neighborhood is no

more than the circumference of the circle, λ[N (Ov, ε)] = min{2ε, 2π sin(e)}.

According to Deﬁnition 4, the ε-viewpoint leakage probability when uploading e is

Pre (cid:44) min

ßmin{2ε, 2π sin(e)}
2π sin(e)

™

, 1

= min

ß

ε
π sin(e)

™

, 1

(7)

which increases as sin(e) decreases. Then, the minimal viewpoint leakage probability is Prmin

e =

ε
π , which is achieved when sin(e) achieves the maximum, i.e., e = 0.5π.

From (7), Pre = 1 when π sin(e) ≤ ε, from which we obtain the range of e as

e ∈ [0, arcsin(

ε
π

)] ∪ [π − arcsin(

ε
π

), π]

(8)

If e falls in this range, ε-viewpoint leakage event happens, the inferred viewpoint can be arbitrary

viewpoint in the possible viewpoint zone. In Fig. 3b, we provide the value of Pre given the

normalized values of e and ε computed from (7).

B. Relations of Viewpoint Leakage with QoE and Prediction Performance

From (7), as the prediction error e increases from arcsin( ε

π ) to π−arcsin( ε

π ), Pre ﬁrst decreases

and then increases, and is mirror symmetric with respect to (w.r.t.) e = 0.5π, as illustrated in

Fig. 3b and 3c.

When e ∈ [arcsin( ε

π ), 0.5π], the viewpoint leakage probability Pre decreases as e increases,

i.e., privacy can be better protected with larger prediction error, which will degrade QoE. Then,

there is a tradeoff between protecting privacy and improving QoE in this region. When e ∈

[0.5π, π − arcsin( ε

π )], Pre decreases as e decreases, i.e., privacy can be better protected with

smaller prediction error, and improving privacy protection is consistent with improving QoE. In

DRAFT

April 12, 2022

15

other words, to reduce the viewpoint leakage probability, the prediction error can neither be too

small nor be too large, depending on the viewpoint privacy requirement of a user.

The following corollary provides the relation of required prediction error with the viewpoint

privacy requirement. The detailed proof is provided in Appendix A of [32].

Corollary 1: Denote the maximal viewpoint leakage probability allowed by a user as Pru

e , the

required range of prediction error for satisfying Pre ≤ Pru

e is



e ∈



e π ) and eu

[0, π],

Pru

e = 1,

[eu

min, eu

max] , Pru

e ∈ [Prmin

e

, 1),

(9)

Infeasible,

Pru

e ∈ [0, Prmin

e

).

where eu

min

(cid:44) arcsin( ε
Pru

max

(cid:44) π − arcsin( ε
Pru

e π ).

The corollary implies that if a user has no privacy viewpoint requirement, i.e., Pru

e = 1, then

the prediction error should be minimized for maximizing the QoE. When Pru

e ∈ [Prmin

e

, 1), there

is a contradiction between the target of satisfying the viewpoint privacy requirement and the

target of maximizing the QoE. For example, in Fig. 3c, when Pru

e = 0.2 and ε = 0.4rfov, to satisfy

the privacy requirement, e ≥ eu

Remark 3: When e ∈ [eu

min, eu

min = 0.19π, while to maximize QoE, e should be minimized.
max], the privacy-QoE tradeoff exists if e ∈ [eu

min, 0.5π], otherwise

protecting privacy is consistent with improving QoE.

V. VIEWPOINT LEAKAGE PROBABILITY WHEN UPLOADING THE QOE METRIC

Although the real viewpoint Ov cannot be inferred directly from the QoE metric in (5), the

prediction error e can be inferred from the QoE. In this section, we ﬁrst derive QoE as a function

of rsv (which reﬂects the amount of conﬁgured resources as stated in Remark 1) and e, from

which the prediction error can be inferred. Then, we derive the ε-viewpoint leakage probability

when uploading the QoE metric.

A. QoE as a Function of Conﬁgured Resources and Prediction Error

If rsv = 0 (i.e., no streaming), then QoE = 0. If rsv = π (i.e., streaming the sphere), then

QoE = 100%. The QoE metric as a function of rsv ∈ (0, π) and e in different cases is provided

April 12, 2022

DRAFT

16

in the following proposition. The detailed proof is provided in Appendix B of [32].

Proposition 1: When rsv ∈ (0, π), the expressions of QoE w.r.t. rsv and e are as follows.

QoE = 100%, if rsv ≥ rfov + e (i.e., FoV ⊂ SFoV).

QoE =

1 − cos(rsv)
1 − cos(rfov)

, if rfov ≥ rsv + e (i.e., SFoV ⊂ FoV).

QoE = 0, if e ≥ rfov + rsv(i.e., FoV ∩ SFoV = ∅).

− cos(rsv) − cos(rfov)
1 − cos(rfov)
Arm
ol (rsv, e)
2π(1 − cos(rfov))

QoE =

QoE =

where

, if rfov + rsv + e ≥ 2π (i.e., SFoV-C⊂FoV).

, if rsv ∈ (cid:2)rrm

sv,min(e), rrm

sv,max(e)(cid:3) (i.e., remaining case),

(10a)

(10b)

(10c)

(10d)

(10e)

ã

Arm

ol (rsv, e) (cid:44)2π − 2π cos(rsv) − 2π cos(rfov) − 2 arccos

Åcos(e) − cos(rsv) cos(rfov)
sin(rsv) sin(rfov)
ã

+ 2 cos(rsv) arccos

+ 2 cos(rfov) arccos

Å− cos(rfov) + cos(e) cos(rsv)
sin(e) sin(rsv)
Å− cos(rsv) + cos(e) cos(rfov)
sin(e) sin(rfov)

ã

,

(11)

sv,min(e) (cid:44) |rfov − e|, and rrm
rrm

sv,max(e) (cid:44) min{rfov + e, 2π − (rfov + e)}.

In the proposition, FoV ⊂ SFoV, SFoV ⊂ FoV, FoV ∩ SFoV=∅, and SFoV-C⊂FoV respectively

denote the cases that FoV being included in SFoV, SFoV being included in FoV, no intersection

between FoV and SFoV, and the complement set of SFoV being included in FoV. These four

cases are illustrated in Fig. (4a)-(4d).

Remark 4: In the four cases that FoV ⊂ SFoV, SFoV ⊂ FoV, FoV ∩ SFoV=∅, or SFoV-C⊂FoV,

QoE does not depend on prediction error e.
Remark 5: In the remaining case, QoE = Arm

ol (rsv,e)

2π(1−cos(rfov)), which is a strictly monotonically

decreasing function of e and increasing function of rsv.

To visualize the impact of conﬁgured resources rsv and prediction error e on QoE as stated

in Remarks 3 and 4, we provide the values of QoE obtained from (10) given all possible values

of rsv and e in Fig. 4e.

DRAFT

April 12, 2022

17

(a) FoV ⊂ SFoV

(b) SFoV ⊂ FoV

(c) FoV ∩ SFoV = ∅

(d) SFoV-C⊂FoV

(e) QoE v.s. rsv and e.

Fig. 4: Four cases when rsv ∈ (0, π), and QoE v.s. rsv and e.

B. Inferring Prediction Error from QoE

Except the QoE metric, the server also knows the predicted viewpoint Op, the radius of the

FoV rfov, and the radius of the SFoV rsv for proactive VR video streaming.

The following proposition provides the prediction error inferred from these information. The

proof is provided in Appendix C of [32].

Proposition 2: When rsv ∈ (0, π), given the value of QoE, the range or value of e is

e ∈ [0, rsv − rfov], if QoE = 100% (i.e., FoV ⊂ SFoV).

e ∈ [0, rfov − rsv], if QoE =

1 − cos(rsv)
1 − cos(rfov)

(i.e., SFoV ⊂ FoV).

April 12, 2022

(12a)

(12b)

DRAFT

OvOesvrfovrpOsvrOfovrpOvOevOpOOefovrsvrsvrOfovrpOvOececOcsvr18

e ∈ [rfov + rsv, π], if QoE = 0 (i.e., FoV ∩ SFoV = ∅).

(12c)

e ∈ [2π − rfov − rsv, π], if QoE =

− cos(rsv) − cos(rfov)
1 − cos(rfov)

(i.e., SFoV-C⊂FoV).

(12d)

e = ebi, otherwise (i.e., remaining case).

(12e)

where ebi is the value of e determined by (10e) with bisection searching.

C. ε-Viewpoint Leakage Probability

Based on Propositions 1 and 2 as well as Remark 5, we can derive the ε-viewpoint leakage

probability when uploading the QoE metric, denoted as Prq. When rsv = 0 or π, neither the

range nor the value of e can be inferred. Then, the leakage probability achieves its minimum as

Prmin

q = Prmin = 1−cos(ε)

2

.

In the following, we derive the probability when rsv ∈ (0, π).

(a) FoV⊂SFoV or SFoV⊂FoV

(b) FoV ∩ SFoV=∅ or SFoV-C⊂FoV

Fig. 5: Possible viewpoint zone when FoV⊂SFoV, SFoV⊂FoV, FoV ∩ SFoV=∅, and SFoV-C⊂FoV

1) FoV⊂SFoV: From (12a), e ∈ [0, rsv − rfov]. As shown in Fig. 5a, given the predicted

viewpoint Op, the possible viewpoint zone Zv is a spherical cap with radius rz = rsv − rfov ≥ 0

and area AZv = 2π(1 − cos(rz)). The ε-viewpoint-sensitive neighborhood is a spherical cap with

DRAFT

April 12, 2022

OOpPossible viewpoint zone -viewpoint sensitiveneighborhood ||svfovrrOOpOvPossible viewpoint zone -viewpointsensitiveneighborhood ||fovsvrr19

center Ov and radius ε, whose area is AN (Ov,ε) = 2π(1 − cos(ε)). Then, according to Deﬁnition

4, the ε-viewpoint leakage probability is

PrV⊂SV
q

(ε, rsv) = min

ß 2π(1 − cos(ε))
2π(1 − cos(rz))

™

, 1

= min

ß

1 − cos(ε)
1 − cos(rsv − rfov)

™

, 1

2) SFoV⊂FoV: From (12b), e ∈ [0, rfov − rsv]. As shown in Fig. 5a, the only difference

from the case where FoV⊂SFoV is that the radius of the possible viewpoint zone becomes

rz = rfov − rsv ≥ 0. Then, the ε-viewpoint leakage probability is

PrSV⊂V
q

(ε, rsv) = min

ß

1 − cos(ε)
1 − cos(rfov − rsv)

™

, 1

3) FoV ∩ SFoV=∅: From (12c), e ∈ [rfov + rsv, π]. As shown in Fig. 5b, Zv is a spherical cap

with radius rz = π − (rsv + rfov) ≥ 0. Then, the ε-viewpoint leakage probability is

PrV∩SV=∅
q

(ε, rsv) = min

ß

1 − cos(ε)
1 − cos(π − (rsv + rfov))

™

, 1

ß

= min

1 − cos(ε)
1 + cos(rsv + rfov)

™

, 1

4) SFoV-C⊂FoV: From (12d), e ∈ [2π −rfov −rsv, π]. As shown in Fig. 5b, the only difference

from the case that FoV ∩ SFoV=∅ is that the radius of the possible viewpoint zone becomes

rz = π − (2π − rfov − rsv) = rfov + rsv − π ≥ 0. The ε-viewpoint leakage probability is

PrSV-C⊂V
q

ß

(ε, rsv) = min

1 − cos(ε)
1 − cos(rfov + rsv − π)

™

, 1

= min

ß

1 − cos(ε)
1 + cos(rsv + rfov)

™

, 1

The viewpoint leakage probability in the four cases can be uniﬁed as

Prq(ε, rsv) = min

ß 1 − cos(ε)
1 − cos(rz)

™

, 1

(13)

where rz = |rsv − rfov| for FoV⊂SFoV and SFoV⊂FoV, and rz = |π − rsv − rfov| for FoV ∩ SFoV=∅

and SFoV-C⊂FoV. With rz, the corresponding range of e can be expressed as follows:

e ≤ rz,

if FoV ⊂ SFoV or SFoV ⊂ FoV

e ≥ π − rz,

if FoV∩SFoV = ∅ or SFoV-C ⊂ FoV

(14)

DRAFT

April 12, 2022

20

5) Remaining case: In this case, since the value of e can be obtained from (10e) with bisection

searching according to Remark 5, Prq(ε, rsv) = Pre. According to (7), the ε-viewpoint leakage

probability is PrRM

q = min

¶ ε

©
π sin(e) , 1
.

Remark 6: For the four cases, it is shown from (13) that the viewpoint leakage probability does

not depend on e. For the remaining case, the leakage probability does not depend on rsv.

The viewpoint leakage probability are illustrated in Fig. 6.

(a) Frontal view.

(b) Dorsal view.

Fig. 6: Viewpoint leakage probability v.s. rsv and e, ε = 0.4rfov.

D. When the ε-Viewpoint Leakage Probability Achieves the Maximum and Minimum?

Since for the cases where rsv = 0 or π, the global minimal value of Prq can be achieved as

Prmin

q = 1−cos(ε)

2

, we consider the cases where rsv ∈ (0, π).

For the four cases, the conditions that achieve maximal and inﬁmum of viewpoint leakage

probabilities can be obtained from (13) and (14), which are listed in Table II. To show the

impact of the conﬁgured resources on the viewpoint leakage probability, we also provide the

monotonicity of Prq w.r.t. rsv.

One condition for Prq achieving one, e ≤ ε or e ≥ π − ε,

indicates that either the predicted

viewpoint or its symmetry point is in the ε-viewpoint sensitive neighborhood. The other condition,

DRAFT

April 12, 2022

TABLE II: Maximal and inﬁmum of Prq in the four cases when rsv ∈ (0, π)

21

Case

FoV⊂SFoV

SFoV⊂FoV

FoV ∩ SFoV=∅

SFoV-C⊂FoV

Prq = 1

Monotonicity of Prq

Prq = Prinf
q

∗

Conditions w.r.t. e and rsv

w.r.t. rsv when Prq < 1

Value of Prinf
q

Condition w.r.t. rsv

(cid:77)

e ≤ ε

|rsv − rfov| ≤ ε

e ≥ π − ε

|π − rsv − rfov| ≤ ε

Increasing

Decreasing

Decreasing

Increasing

1−cos(ε)

1−cos(ε)

1+cos(rfov) > Prmin
1−cos(rfov) > Prmin
1+cos(rfov) > Prmin
1−cos(rfov) > Prmin

1−cos(ε)

1−cos(ε)

π − rsv ≤ δ(cid:63)

rsv ≤ δ

rsv ≤ δ

π − rsv ≤ δ

∗ Prinf
q

is the inﬁmum of Prq.

(cid:77) e can be an arbitrary value.

(cid:63)δ is an arbitrarily small positive number.

rz ≤ ε, where rz = |rsv − rfov| or |π − rsv − rfov|, indicates that the conﬁgured resources make

the difference between the radius of SFoV or SFoV-C and the radius of FoV no larger than ε.

Speciﬁcally, for FoV⊂SFoV and SFoV⊂FoV, e = d(Op, Ov) ≤ ε. This indicates that the predicted

viewpoint lies in the viewpoint sensitive neighborhood. For FoV ∩ SFoV=∅ and SFoV-C⊂FoV,

the arc length d(Ov, (cid:101)Op) = d(Op, (cid:101)Op) − d(Op, Ov) = π − e ≤ ε, where (cid:101)Op is the symmetry

point of Op w.r.t. the center of the unit sphere. This indicates that the symmetry point of Op

is in the viewpoint-sensitive neighborhood. However, low or high prediction error (i.e., e ≤ ε

or e ≥ π − ε) itself does not lead to Prq = 1. Only if the conditions of prediction error and

conﬁgured resources hold simultaneously, Prq = 1.

When |rsv − rfov| > ε or |π − rsv − rfov| > ε, Prq < 1 and the viewpoint leakage probability

decreases as rsv → 0 or rsv → π. The viewpoint leakage probability achieves the global minimum

when rsv = 0 or π, which however is not deﬁned in the four cases. Nevertheless, the inﬁmum

still exists, and the inﬁma of viewpoint leakage probabilities in the four cases are larger than

the global minimum Prmin. This is because the radius of possible viewpoint zone rz is not

continuous when rsv = 0 or π. For example, when FoV⊂SFoV, the limit of rz from the left is

rz = π − rfov. However, when rsv = π that is not the case of FoV⊂SFoV, the possibly

lim
rsv→π−
inferred viewpoint can be arbitrary on the sphere and the radius of possible viewpoint zone

becomes rz = π.

For the remaining case, since e can be determined from the values of QoE and rsv, the

April 12, 2022

DRAFT

22

viewpoint leakage probability achieves the maximum when e ∈ [0, arcsin( ε

π )]∪[π−arcsin( ε

π ), π],

as shown in (8). As analyzed in subsection IV-A, Prmin

e = ε

π , which is achieved when e = 0.5π.

When ε ∈ [0, rfov] and rfov ∈ [0, 0.5π], after some regular derivations, we can obtain that

1−cos(ε)
2

< ε

π . This indicates that the minimal viewpoint leakage probability when uploading the

QoE metric is less than the minimal viewpoint leakage probability when uploading the prediction

error, i.e., Prmin

q < Prmin

e

. This is because the real viewpoint is inferred indirectly from the QoE

metric via prediction error, which agrees with the intuition.

E. Relations of Viewpoint Leakage with QoE and Conﬁgured Resources

For the four cases, when |rsv − rfov| ≤ ε or |π − rsv − rfov| ≤ ε, Prq = 1, otherwise Prq is

a monotonic function of rsv. When Prq decreases as rsv increases, more resources are required

to protect the viewpoint privacy, which also improves the QoE. When Prq decreases as rsv

decreases, less resources should be conﬁgured to protect the viewpoint privacy, which however

sacriﬁces the QoE unless the QoE metric is already zero. That is to say, unless Prq = 1 or

QoE = 0, either a privacy-resources tradeoff or a privacy-QoE tradeoff exists. To visualize the

relation, we provide values of QoE obtained from (10) and Prq obtained from (13) in Fig. 7.

Remark 7: We can observe from Fig. 7 that neither of the tradeoffs exist (i.e., Prq = 1 or

QoE = 0) when (a) rsv ≤ rfov + ε in FoV ⊂ SFoV, (b) rsv ≥ rfov − ε in SFoV ⊂ FoV, (c)

rsv ≤ π − rfov + ε in SFoV-C⊂FoV, (d) in FoV ∩ SFoV=∅. We refer to (a)-(d) as the “exceptional

cases”. The conditions for these cases appearing depend on the values of rsv and e, as shown in

(10).

VI. TRACE-DRIVEN SIMULATION RESULTS

In this section, with a state-of-the-art predictor, we show that the privacy-QoE tradeoff always

exists when uploading the prediction error, and the exceptional cases where neither the privacy-

resources tradeoff nor the privacy-QoE tradeoff exists rarely occur when uploading the QoE

metric.

DRAFT

April 12, 2022

23

(a) FoV⊂SFoV, trade resources for privacy.

(b) SFoV⊂FoV, trade resources and privcay for QoE.

(c) FoV ∩ SFoV=∅, exception because of QoE = 0 .

(d) SFoV-C⊂FoV, trade resources for QoE and privacy.

Fig. 7: ε-viewpoint leakage probability and QoE in the four cases when rsv ∈ (0, π). In Figs. 7a

and 7b, e = 0.1π. In Figs. 7c and 7d, e = 0.9π.

We consider the viewpoint prediction on a real dataset [3], where rfov = 50◦ and 300 traces of

viewpoints from 30 users watching 10 VR videos are used for training and testing predictors.2

The total traces are randomly split into training and test sets with ratio 8:2 [14]. The number of

traces in test set is ntrace = 30 × 10 × 0.2 = 60. For each trace with the playback duration 60

2According to the analysis in [14], the traces of the ﬁrst 20 users in the dataset have mistakes, thus we only use the traces

of the other 30 users. Besides, the conclusion obtained in this section is also true for other datasets.

April 12, 2022

DRAFT

10:20%0.840%0.6PrqandQoE60%0:8:80%rsv0.4100%0:6:"=rfov0.20rfov+ePrV;SVqQoErsv=rfov+"10rfov!e20%0.840%0.6PrqandQoE60%"=rfovrsv0:1:80%0.4100%0.200PrSV;VqQoErsv=rfov!"10rsve!rfov20%"=rfov0.840%0.6PrqandQoE60%0:4:80%0.4100%0:2:0.200PrV\SV=;qQoErsv=:!rfov!"10:20%0.840%0.6PrqandQoE60%"=rfovrsv80%0.40:9:100%0.202:!rfov!ePrSV!C;VqQoErsv=:!rfov+"24

s, the viewpoint is sampled ﬁve times per second [14]. The total proactive streaming time is set

as Tps = 2 s, the playback duration of a segment is Tseg = 1 s [20]. The initial two segments

are streamed in the passive manner. Then, the number of viewpoint samples in each trace to be

predicted is nsp = 60 × 5 − 2 × 5 = 290. The total number of viewpoint samples to be predicted

is Nsp = ntrace × nsp = 17400.

We use the deep-position-only predictor, which achieves the best accuracy for the dataset

according to evaluation in [14]. The predictor employs a sequence-to-sequence long short term

memory (LSTM)-based architecture, which uses the viewpoint positions in an observation win-

dow as input to predict the positions in a prediction window [14]. In the initial setting of the

predictor, the prediction window starts immediately after the observation window. To reserve

time for computing and communication, we tailor the predictor by setting the duration between

the end of the observation window and the beginning of the prediction windows as Tcc = 1 s,

and set the durations of observation and prediction windows as Tobw = 1 s and Tpdw = Tseg = 1

s, respectively. To protect the viewpoint, we consider training and predicting at the HMD,

which is case 4 in Table I. When training the predictor, we consider a classical federated

learning algorithm, FederatedAveraging [33]. The settings of the federated learning are

as follows. In each round, the model parameters of the predictor are updated at the HMDs of

all of the K = 30 users. The number of local epochs for each user is El = 50, the number

of communication rounds is R = 10. The weighting coefﬁcient of the kth user on the model

parameter is ck = nk
Ntrain

, where Ntrain = 300 × 0.8 = 240 is the total number of traces in the

training set, and nk is the number of video traces of the kth user in the training set. Due to

the random division of training and test sets, nk varies from 6 to 10. We refer to the predictor

as tailored federated position-only predictor. Other details and hyper-parameters of the tailored

predictor are the same as the deep-position-only predictor [14]. To gain useful insight, we assume

that all the users have identical privacy requirements (ε, Pru

e ) for all the videos. After making

prediction, the prediction errors for all the viewpoint samples E (cid:44) {e1, ..., eNsp} can be obtained.

DRAFT

April 12, 2022

A. Relations of Viewpoint Privacy with Prediction Performance and QoE When Uploading e

To evaluate the relation between the viewpoint privacy requirement and the prediction perfor-

25

mance, we ﬁrst obtain the subset of E that can satisfy the privacy requirement. According to
(9), the subset is Eu (cid:44) (cid:8)ei|ei ∈ [eu

max] and ei ∈ E(cid:9) given ε ∈ [0, rfov] and Pru

e ∈ [Prmin

min, eu

, 1).

e

Then, we provide the average prediction error over the subset, denoted as eu, in Fig. 8a. We

can observe that as the viewpoint privacy requirement becomes more stringent (reﬂected by

increasing ε or decreasing Pru

e ), the average prediction error always increases. This implies that

the prediction performance of the state-of-the-art predictor should be degraded for a user with

stringent privacy requirement.

Recalling that QoE is degraded with the increase of prediction error, this indicates that the

tradeoff between privacy requirement and QoE improvement always exists.

To explain why the consistency in Fig. 3c does not appear, we provide the ratios that trade-

off and consistency occur in Fig. 8b, deﬁned as γtradeoff (cid:44)
(cid:80)Nsp
i=1

max])

1(ei∈[0.5π,eu
Nsp

according to Remark 3, and 1(·) is the indicator function. We can observe that

(cid:80)Nsp
i=1

min,0.5π])

1(ei∈[eu
Nsp

and γconsist (cid:44)

γtradeoff (cid:29) γconsist. For example, when ε = 0.4rfov and Pru

e = 0.4, γtradeoff = 0.47 and γconsist = 0.04.

This is because a state-of-the-art predictor is employed, and the achieved prediction error is

smaller than 0.5π in high probability.

B. Relations of Viewpoint Privacy with QoE and Conﬁgured Resources When Uploading QoE

For the viewpoint leakage probability when uploading the QoE metric, neither of the tradeoffs

exist in the exceptional cases mentioned in Remark 7, which depend on the prediction error and

conﬁgured resources. To evaluate the relations of viewpoint leakage probability with QoE and

conﬁgured resources, we ﬁrst analyze the relation of the average viewpoint leakage probability

over the prediction errors with the conﬁgured resources, and obtain the average QoE over the

prediction errors (denoted as QoE). Based on which we show that the exceptional cases are

signiﬁcantly reduced.

April 12, 2022

DRAFT

26

(a) Average e over Eu v.s. ε and Pru
e

(b) Ratios of tradeoff and consistency

Fig. 8: Tradeoff and consistency when uploading e, Pre ∈ [Prmin

e

, 1)

Given rsv, the average viewpoint leakage probability over E can be obtained as

Prq = PrV⊂SV
(cid:123)(cid:122)
(cid:124)
V⊂SV
(cid:44)Pr
q

· γV⊂SV
(cid:125)

q

+ PrSV⊂V
q

· γSV⊂V
(cid:125)
(cid:123)(cid:122)
SV⊂V
(cid:44)Pr
q

(cid:124)

+ PrV∩SV=∅
q

· γV∩SV=∅
(cid:125)
(cid:123)(cid:122)
V∩SV=∅
(cid:44)Pr
q

+ PrSV-C⊂V
q

+

· γSV-C⊂V
(cid:125)
(cid:123)(cid:122)
SV-C⊂V
(cid:44)Pr
q

(cid:124)

(cid:124)

Nsp
(cid:88)

i=1

1
Nsp
(cid:124)

PrRM

q (ei) · 1 (cid:0)rsv ∈ (cid:2)rrm

sv,min(ei), rrm

sv,max(ei)(cid:3)(cid:1)

,

(cid:123)(cid:122)
RM
(cid:44)Pr
q

(cid:125)

where γx is the ratio of case x on E. For example, from (10), γV⊂SV = 1
Nsp

(cid:80)Nsp
i=1

1(rsv ≥ rfov +ei).

Except the ratios of the four cases, the ratio of the remaining case can be obtained as γRM =

1
Nsp

(cid:80)Nsp
i=1

1(rsv ∈ (cid:2)rrm

sv,min(ei), rrm

sv,max(ei)(cid:3)).

We provide the value of Prq in Fig. 9a. We can observe that given arbitrary ε, the range of rsv

can be divided in two regions (I1 and I2) where Prq increases with rsv, two regions (D1 and D2)

where Prq decreases with rsv, and a constant region (rsv ∈ C : (rfov − arcsin( ε

π ), rfov + arcsin( ε

π )))

where the conﬁgured resources make the difference between the radius of SFoV and FoV no

larger than arcsin( ε

π ). Since Prq is a monotonic function of rsv unless rsv ∈ C, we can infer that

the exceptional cases only lie in rsv ∈ C. This is veriﬁed in Fig. 9b. Since the relation of Prq

with rsv is similar for other values of ε as shown in Fig. 9a, the exceptional cases happen only

DRAFT

April 12, 2022

00.10.210.20.80.40.30.60.60.40.40.80.20.510000.210.40.20.80.60.40.60.810.60.40.80.21027

when the difference between the radius of SFoV and FoV is no larger than arcsin( ε

π ).

To explain why the exceptional cases are reduced from cases (a)-(d) in Remark 7 to rsv ∈ C,

we provide the values of average viewpoint leakage probabilities in Fig. 9c and the ratio of each

case in Fig. 9d.

V⊂SV
As shown in Fig. 9c, when the exceptional cases (a) and (b) in Remark 7 appear, Pr
q

,

SV⊂V
Pr
q

, and Pr

RM
q monotonically increases, decreases, ﬁrst increases then decreases with rsv,

respectively, although the value of rsv does not affect Prq. From Fig. 9d we can observe that this

is because the monotonicity of the ratios γV⊂SV, γSV⊂V, and γRM versus rsv, respectively. Only

RM
when rsv ∈ C, the increase and decrease of Pr
q

are respectively counteracted by the decrease

SV⊂V
of Pr
q

V⊂SV
and the increase of Pr
q

. That is to say, the exceptional cases (a) and (b) in Remark

7 is shrunk to rsv ∈ C because of the relation of γV⊂SV, γSV⊂V, and γRM with rsv for the given

predictor.

As shown in Fig. 9d, the values of γSV-C⊂V and γV∩SV=∅ are small. This is because the two

cases happens when the prediction error is large according to (14), while most prediction errors

are small with the state-of-the art predictor, as shown in Fig. 8b. That is to say, the exceptional

cases (c) and (d) in Remark 7 rarely occur because prediction errors for most samples are small.

We can ﬁnd from Fig. 9a and 9b that to reduce the average viewpoint leakage probability,

either the average QoE has to be sacriﬁced or more resources are required unless rsv ∈ C.

VII. CONCLUSION

In this paper, we studied viewpoint leakage issue during proactive VR streaming, and found

that existing privacy-preserving approaches cannot avoid the leakage. We deﬁned and derived

the viewpoint leakage probability when prediction error or QoE metric is uploaded for adaptive

streaming. When uploading the error of predicting viewpoint, if the error is no larger than

half of its maximum, there is a tradeoff between protecting viewpoint privacy and improving

QoE, otherwise improving privacy protection is consistent with improving QoE. The targets of

April 12, 2022

DRAFT

28

(a) Prq v.s. rsv and ε, regions of rsv: I1: (0, rfov−ε), D2: (rfov−

(b) Prq and QoE v.s. rsv, I1 and I2: QoE can be improved

ε, rfov − arcsin( ε

π )), C: (rfov − arcsin( ε

π ), rfov + arcsin( ε

π )),

with more resources at the cost of sacriﬁcing privacy, D1 and

I2: (rfov + arcsin( ε

π ), rfov + ε), D1: (rfov + ε, π).

D2: both QoE and privacy protection can be improved with

more resources.

(c) Average viewpoint leakage probabilities v.s. rsv.

(d) Ratio of each case v.s. rsv.

Fig. 9: Average viewpoint leakage probabilities, QoE, and ratio of each cases when uploading

QoE metric, ε = 0.4rfov in Fig. (b), (c), and (d).

maximizing QoE and satisfying viewpoint privacy requirement are always contradictory. The

leakage probability achieves its minimum when the prediction error is the half of its maximum.

When uploading the QoE metric and from which the range of prediction error can be inferred,

either the privacy-QoE or the privacy-resources tradeoff exists unless the leakage probability is

one or the QoE metric is zero. To reduce the leakage probability, either QoE has to be sacriﬁced or

DRAFT

April 12, 2022

010%20%30%40%50%60%70%020%40%60%80%100%010%20%30%40%50%60%70%Average viewpoint leakage probability00.20.40.60.81Ratio29

more resources should be conﬁgured. The leakage probability achieves one when the following

two conditions hold simultaneously. (1) Either the predicted viewpoint or its symmetry point

is in the viewpoint-sensitive neighborhood. (2) The difference between the radius of SFoV or

SFoV-C and the radius of FoV is no larger than the radius of viewpoint-sensitive neighborhood.

The minimal viewpoint leakage probability when uploading the prediction error is larger than

the minimal viewpoint leakage probability when uploading the QoE metric.

Simulation results with the state-of-the-art predictor over a real dataset show that when

uploading the prediction error, the tradeoff between viewpoint privacy and QoE improvement

always exists. To reduce the viewpoint leakage probability, the prediction performance should

be degraded rather than improved. When uploading the QoE metric, the cases where neither the

privacy-resources tradeoff nor the privacy-QoE tradeoff exist rarely occur. To reduce the average

viewpoint leakage probability, either the average QoE has to be sacriﬁced or more resources are

required.

REFERENCES

[1] C.-L. Fan, W.-C. Lo, Y.-T. Pai, and C.-H. Hsu, “A survey on 360◦ video streaming: Acquisition, transmission, and display,”

ACM Comput. Surv., vol. 52, no. 4, Aug. 2019.

[2] C. Chaccour, M. N. Soorki, W. Saad, M. Bennis, and P. Popovski, “Can terahertz provide high-rate reliable low latency

communications for wireless VR?” IEEE Internet Things J., early access, 2022.

[3] W.-C. Lo, C.-L. Fan, J. Lee, C.-Y. Huang, K.-T. Chen, and C.-H. Hsu, “360◦ video viewing dataset in head-mounted

virtual reality,” ACM MMSys, 2017.

[4] F. Qian, L. Ji, B. Han, and V. Gopalakrishnan, “Optimizing 360 video delivery over cellular networks,” ACM SIGCOMM

Workshop, 2015.

[5] L. Teng, G. Zhai, Y. Wu, X. Min, W. Zhang, Z. Ding, and C. Xiao, “QoE driven VR 360◦ video massive MIMO

transmission,” IEEE Trans. Wireless Commun., vol. 21, no. 1, pp. 18–33, 2022.

[6] L. Zhong, X. Chen, C. Xu, Y. Ma, M. Wang, Y. Zhao, and G.-M. Muntean, “A multi-user cost-efﬁcient crowd-assisted VR

content delivery solution in 5G-and-beyond heterogeneous networks,” IEEE Trans. Mobile Comput., early access, 2022.

[7] C. Guo, L. Zhao, Y. Cui, Z. Liu, and D. W. K. Ng, “Power-efﬁcient wireless streaming of multi-quality tiled 360 VR

video in MIMO-OFDMA systems,” IEEE Trans. Wireless Commun., vol. 20, no. 8, pp. 5408–5422, 2021.

[8] F. Hu, Y. Deng, and A. Hamid Aghvami, “Cooperative multigroup broadcast 360◦ video delivery network: A hierarchical

federated deep reinforcement learning approach,” IEEE Trans. Wireless Commun., early access, 2021.

[9] H. Xiao, C. Xu, Z. Feng, R. Ding, S. Yang, L. Zhong, J. Liang, and G.-M. Muntean, “A transcoding-enabled 360◦ VR video
caching and delivery framework for edge-enhanced next-generation wireless networks,” IEEE J. Select. Areas Commun.,
early access, 2022.

[10] Z. Gu, H. Lu, and C. Zou, “Horizontal and vertical collaboration for VR delivery in MEC-enabled small-cell networks,”

IEEE Commun. Lett., vol. 26, no. 3, pp. 627–631, 2022.

[11] S. Li, C. She, Y. Li, and B. Vucetic, “Constrained deep reinforcement learning for low-latency wireless VR video streaming,”

IEEE GLOBECOM, 2021.

April 12, 2022

DRAFT

30

[12] N. Kan, J. Zou, C. Li, W. Dai, and H. Xiong, “Rapt360: Reinforcement learning-based rate adaptation for 360-degree video
streaming with adaptive prediction and tiling,” IEEE Trans. Circuits Syst. Video Technol., vol. 32, no. 3, pp. 1607–1623,
2022.

[13] X. Hou and S. Dey, “Motion prediction and pre-rendering at the edge to enable ultra-low latency mobile 6DoF experiences,”

IEEE Open Journal of the Communications Society, vol. 1, pp. 1674–1690, 2020.

[14] M. F. Romero Rondon, L. Sassatelli, R. Aparicio-Pardo, and F. Precioso, “TRACK: A new method from a re-examination
of deep architectures for head motion prediction in 360-degree videos,” IEEE Trans. Pattern Anal. Machine Intell., early
access, 2021. [Online]. Available: https://gitlab.com/miguelfromeror/head-motion-prediction/tree/master/

[15] X. Liu and Y. Deng, “Learning-based prediction, rendering and association optimization for MEC-enabled wireless virtual

reality (VR) networks,” IEEE Trans. Wireless Commun., vol. 20, no. 10, pp. 6356–6370, 2021.

[16] X. Liu, Y. Deng, C. Han, and M. D. Renzo, “Learning-based prediction, rendering and transmission for interactive virtual

reality in RIS-assisted terahertz networks,” IEEE J. Select. Areas Commun., vol. 40, no. 2, pp. 710–724, 2022.

[17] C. Perfecto, M. S. Elbamby, J. Del Ser, and M. Bennis, “Taming the latency in multi-user VR 360°: A QoE-aware deep

learning-aided multicast framework,” IEEE Trans. Commun., vol. 68, no. 4, pp. 2491–2508, 2020.

[18] G. Xiao, M. Wu, Q. Shi, Z. Zhou, and X. Chen, “DeepVR: Deep reinforcement learning for predictive panoramic video

streaming,” IEEE Trans. Cogn. Commun. Netw., vol. 5, no. 4, pp. 1167–1177, 2019.

[19] R. Zhang, J. Liu, F. Liu, T. Huang, Q. Tang, S. Wang, and F. R. Yu, “Buffer-aware virtual reality video streaming with
personalized and private viewport prediction,” IEEE J. Select. Areas Commun., vol. 40, no. 2, pp. 694–709, 2022.
[20] X. Wei, C. Yang, and S. Han, “Prediction, communication, and computing duration optimization for VR video streaming,”

IEEE Trans. Commun., vol. 69, no. 3, pp. 1947–1959, 2021.

[21] A. Yaqoob, T. Bi, and G.-M. Muntean, “A survey on adaptive 360◦ video streaming: Solutions, challenges and

opportunities,” IEEE Communications Surveys Tutorials, vol. 22, no. 4, pp. 2801–2838, 2020.

[22] M. R. Miller, F. Herrera, H. Jun, J. A. Landay, and J. N. Bailenson, “Personal identiﬁability of user tracking data during

observation of 360-degree VR video,” Scientiﬁc Reports, vol. 10, no. 1, pp. 1–10, 2020.

[23] B. David-John, D. Hosfelt, K. Butler, and E. Jain, “A privacy-preserving approach to streaming eye-tracking data,” IEEE

Trans. Vis. Comput. Graphics, vol. 27, no. 5, pp. 2555–2565, 2021.

[24] J. Li, A. R. Chowdhury, K. Fawaz, and Y. Kim, “Kal(cid:15)ido: Real-time privacy control for eye-tracking systems,” 30th

USENIX Security Symposium, Aug. 2021.

[25] E. Bozkir, A. B. ¨Unal, M. Akg¨un, E. Kasneci, and N. Pfeifer, “Privacy preserving gaze estimation using synthetic images
via a randomized encoding based framework,” ACM Symposium on Eye Tracking Research and Applications, 2020.
[26] A. Liu, L. Xia, A. Duchowski, R. Bailey, K. Holmqvist, and E. Jain, “Differential privacy for eye-tracking data,” ACM

Symposium on Eye Tracking Research and Applications, 2019.

[27] J. Steil, M. Koelle, W. Heuten, S. Boll, and A. Bulling, “Privaceye: Privacy-preserving head-mounted eye tracking using
egocentric scene image and eye movement features,” ACM Symposium on Eye Tracking Research and Applications, 2019.
[28] C. Fan, S. Yen, C. Huang, and C. Hsu, “Optimizing ﬁxation prediction using recurrent neural networks for 360◦ video

streaming in head-mounted virtual reality,” IEEE Trans. Multimedia, vol. 22, no. 3, pp. 744–759, March 2020.

[29] 3GPP, “Extended reality (XR) in 5G,” 2020, 3GPP TR 26.928 version 16.0.0 release 16.
[30]

iLab, “Cloud VR network solution whitepaper,” Huawei Technologies CO., LTD., Tech. Rep., 2018. [Online]. Available:
https://www.huawei.com/minisite/pdf/ilab/cloud vr network solution white paper en.pdf

[31] J. Zou, C. Li, C. Liu, Q. Yang, H. Xiong, and E. Steinbach, “Probabilistic tile visibility-based server-side rate adaptation
for adaptive 360-degree video streaming,” IEEE J. Sel. Topics Signal Process., vol. 14, no. 1, pp. 161–176, 2020.
[32] X. Wei, C. Yang, and C. Sun, “Viewpoint leakage in proactive VR streaming: Modeling and tradeoff,” arXiv:2203.03107.
[33] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efﬁcient learning of deep networks

from decentralized data,” PMLR AISTATS, 2017.

DRAFT

April 12, 2022

