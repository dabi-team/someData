MetroLoc: Metro Vehicle Mapping and Localization 
with LiDAR-Camera-Inertial Integration 

Yusheng Wang, Graduate Student Member, IEEE, Weiwei Song, Yi Zhang, Fei Huang, Zhiyong Tu and Yidong Lou 

Abstract—We  propose  an  accurate  and  robust  multi-modal 
sensor  fusion  framework,  MetroLoc,  towards  one  of  the  most 
extreme  scenarios,  the large-scale  metro  vehicle  localization  and 
mapping. MetroLoc is built atop an IMU-centric state estimator 
that  tightly couples  light detection  and  ranging (LiDAR), visual, 
and inertial information with the convenience of loosely coupled 
methods.  The  proposed  framework 
is  composed  of  three 
submodules: IMU odometry, LiDAR-inertial odometry (LIO), and 
Visual-inertial odometry (VIO). The IMU is treated as the primary 
sensor,  which  achieves  the  observations  from  LIO  and  VIO  to 
constrain  the  accelerometer  and  gyroscope  biases.  Compared  to 
previous  point-only  LIO  methods,  our  approach  leverages  more 
geometry information by introducing both line and plane features 
into  motion  estimation.  The  VIO  also  utilizes  the  environmental 
structure  information  by  employing  both  lines  and  points.  Our 
proposed  method  has  been  extensively  tested  in  the  long-during 
metro  environments  with  a  maintenance  vehicle.  Experimental 
results show the system more accurate and robust than the state-
of-the-art  approaches  with  real-time  performance.  Besides,  we 
develop  a  series  of  Virtual  Reality  (VR)  applications  towards 
efficient,  economical,  and  interactive  rail  vehicle  state  and 
trackside  infrastructure  monitoring,  which  has  already  been 
deployed to an outdoor testing railroad. 

Index  Terms—Metro  vehicle,  sensor  fusion,  mapping  and 

positioning, train localization. 

I. INTRODUCTION 

T

RAIN  positioning  and  railway  monitoring  is  of  critical 
importance  for  railroad  systems  since  either  a  train 
localization  failure  or  a  railroad  clearance  intrusion 
might  lead  to  fatal  accidents.  The  train  positioning  strategy 
nowadays  is dominated by  trackside  infrastructures  like track 
circuits  and  Balise  [1].  The  latter  approach  models  the  train 
positioning as a 1D problem and divides the railway track into 
separate cantons, with a Balise placed at the beginning of each 
canton.  When  a  train  passes  over  one  Balise,  the  Automatic 
Train Control (ATC) system knows that a train is within that 
canton. If another train is detected within the safe limits to this 
canton, the train is stopped automatically by Automatic Train 
Protection  (ATP)  system.  Therefore,  the  train  positioning 
accuracy  is  restricted  by  the  length  of  a  canton.  Besides,  the 
track-side  infrastructure-based  systems  require  substantial 
capital  investment  and  inherent  maintenance  cost,  where 
hundreds of Balises are needed in a single railway line and each 
Balise means thousands of dollars. 

Fig. 1. Visual illustration of the mapping result. The above two 
are  the  ground  truth  generated  by  survey  grade  instruments, 
whereas the bottom two are from the proposed MetroLoc using 
a hybrid solid-state LiDAR, a supervisory camera and an IMU. 

With the rapid development of sensor technologies and the 
worldwide standardization of railroad systems, new odometry 
and localization on-board sensors have found a gap in railway 
applications,  where  they  can  supplement  the  limitations  of 
trackside ones. The precise location of the rail vehicles can be 
obtained  through  global  navigation  satellite  system  (GNSS), 
such as the global positioning system (GPS) and Beidou. There 
are numerous works on precision evaluation of GNSS for rail 
vehicles [2]–[4] and data fusion with IMU, odometer as well as 
track  geometry  map  in  [5]–[7].  These  localization  methods 
merely  acquire  the  vehicle  positioning  data  without  extra 
perceptual  information,  which  can  be  further  utilized  to 
operation environment monitoring and hazard forecasting. For 
instance,  a  detected  crack  may  prevent  a  potential  tunnel 
collapse  and  an  indicated  powerline  failure  may  inhibit  an 
accident in the future. 

With the framework of estimating odometry and mapping the 
surroundings at the same time, simultaneously localization and 
mapping  (SLAM)  is  a  promising  solution  to  this  tricky 
conundrum.  Recent  advances  in  light  detection  and  ranging 
(LiDAR)  hardware  have  cultivated  research  into  LiDAR-
inertial  fusion  [8]–[10].  The  accurate  range  measurement, 
invulnerability  to  illumination  variations  and  long  detection 
range  of  LiDARs  make 
them  suitable  of  navigation, 
localization, and mapping tasks.  And they have been applied to 
supplement the IMU-odometer odometry in [11], [12]. 

* Manuscript submitted Nov 1, 2021.  This work was supported by the Joint 
Foundation  for  Ministry  of  Education  of  China  under  Grant  6141A0211907 
(Corresponding author: Weiwei Song). 

Yusheng Wang, Weiwei Song, Zhiyong Tu and Yidong Lou are with the 
GNSS Research Center, Wuhan University, 129 Luoyu Road, Wuhan 430079, 

China 
yushengwhu@whu.edu.cn; 
2012301650022@whu.edu.cn; ydlou@whu.edu.cn). 

(email: 

sww@whu.edu.cn; 

Yi Zhang and Fei Huang are with the School of Geodesy and Geomatics, 
Wuhan  University,  129  Luoyu  Road,  Wuhan  430079,  China  (email: 
yzhang@sgg.whu.edu.cn; feihuang28@whu.edu.cn).  

 
 
 
*
 
 
 
 
However, in environments with degenerate geometries, such 
as  the  long  metro  tunnels  with  repetitive  structures,  LiDAR-
only  approaches  may  fail  as  frame-to-frame  correspondences 
are  estimated  using  highly  consistent  scans.  Since  the  IMU 
mechanization alone cannot provide reliable pose estimates for 
more  than  a  few  minutes,  the  system  failure  is  often 
nonreversible.  To  cope  with  these  challenging  situations, 
integration  with  extra  sensors  are  required,  and  the  LiDAR-
visual-inertial fusion has already been successfully deployed in 
autonomous cave and mine exploration [13]–[15].  

We are motivated to tackle the metro vehicle localization and 
mapping  problem  by  LiDAR-visual-inertial  fusion.  There  are 
many similarities between underground mine and metro tunnels, 
such as low illumination, textureless and self-repetitive. But the 
metro  vehicle  mapping  and  localization  is  a  much  more 
demanding task over tunnel exploration for three reasons: 

High  speed:  The  unmanned  aerospace  vehicles  (UAV), 
unmanned ground vehicles (UGV) and legged robots used for 
fast deployment all run with low velocities. On the other hand, 
the metro vehicles have a regular speed of 35 km/h to 45 km/h, 
which  raises  the  awareness  of  computation  efficiency  and 
velocity-related optimizations, such as distortion removal. 

Long journey: Many subterrain datasets only have hundreds 
of meters length coverage, whereas, the distance between two 
metro  stations  is  already  more  than  one  kilometer.  The 
accumulated  odometry  errors  will  be  incredibly  exaggerated 
without proper compensation methods. Besides, metro scenes 
are  more  structured  than  the  caves,  with  more  than  85% 
column-like and repetitive tunnels. 

No loops: Although the metro vehicles run in a circle, there 
are no loops for a one-way operated transportation system, thus 
no loop can be detected and optimized at the back-end.  

Addressing  the  problems  mentioned  above,  we  present 
MetroLoc, a specific study on LiDAR-visual-inertial fusion for 
metro vehicle localization and mapping in this paper. The main 
contributions of our work can be summarized as follows: 

  We  propose  a  real-time  and  lightweight  simultaneous 
localization  and  mapping  scheme  for  large-scale  map 
building in perceptually-challenging metro tunnels. 
  We  propose  an  IMU-centric  LiDAR-visual-inertial 
individual 

fusion  pipeline 
measurements with constructed factor graphs. 

integrates 

tightly 

that 

  We  propose  an  efficient  method  to  extract  and  track 
LiDAR  features,  which  significantly  improve  the 
accuracy and robustness in structured areas.  

  Our proposed method is extensively evaluated in metro 
environments, and we also implement the approach into 
applications towards future railroad monitoring system.  

II. RELATED WORK 

Prior works on multi-modal sensor  fusion  has gained great 
popularity using combinations of LiDAR, camera and IMU and 
can be classified into either loosely or tightly coupled methods. 
The  former  scheme  processes 
the  measurements  from 
individual sensors separately, and they are preferred more for 
their  extendibility  and  low  computation  consumption.  In 

contrast,  tightly  coupled  methods  jointly  optimize  sensor 
measurements  to  obtain  odometry  estimation,  with  favorable 
accuracy and robustness. 

A.  Loosely Coupled LiDAR-Visual-Inertial Odometry 

One of the early works of LiDAR-visual-inertial, V-LOAM, 
is  proposed  in  [16],  which  leverages  the  Visual-inertial 
odometry as the motion model for LiDAR scan matching. Since 
this scheme only performs frame-to-frame motion estimation, 
the  global  consistency  is  not  guaranteed.  To  cope  with  this 
problem,  Wang  et  al  propose  a  direct  Visual-LiDAR  fusion 
scheme DV-loam [17]. The coarse states are estimated using a 
two-stage  direct  visual  odometry  module,  and  are  further 
refined by the LiDAR mapping module, finally a Teaser-based 
[18] loop detector is utilized for correcting accumulated drifts. 
The  robustness  of  the  loosely  coupled  system  can  be  further 
increased through incorporating additional constraints, such as 
thermal-inertial  prior  for  smoky  scenarios  [19],  incremental 
odometer  [20]  or  legged  odometry  [21]  for  autonomous 
exploring robots.  

B.  Tightly Coupled LiDAR-Visual-Inertial Odometry  

In  many of the recent works [22]–[27], tight integration of 
multi-modal  sensing  capabilities  are  explored  for  degeneracy 
avoidance  and  robustness  enhancement.  The  filter-based 
approaches employs a Kalman filter for joint state estimation, 
such as the error state Kalman filter (ESKF) utilized in [23] and 
the  multi-state  constraint  Kalman  filter  (MSCKF)  applied  in 
[26].  And  the  authors  of  the  latter  work  refine  a  novel  plane 
feature tracking across multiple LiDAR scans within a sliding-
window, making the pose estimation process more efficient and 
robust. The filter-based methods are usually less extendible to 
other sensors and may be vulnerable to potential sensor failures. 
In  contrast,  the  optimization-based  approaches  have  proved 
advantageous for their expandability, where each sensor input 
can be encapsulated as a factor in the constructed graph. 

Shan  et  al  proposes  LVI-SAM  in  [24],  where  the  LiDAR-
inertial  and  Visual-inertial  subsystems  can  run  jointly  in 
feature-rich scenarios, or independently with detected failures 
in one of them. However, the “point to line” and “point-to-plane” 
based LiDAR odometry factor cost functions are not robust to 
feature-poor  environments,  and  may  generate  meaningless 
result in metro or cave scenarios. To handle these situations, the 
geometry of 3D primitives are employed with the line and plane 
landmarks  extracted  in  [22].  The  robustness  and  accuracy  of 
this  system  has  been  proved  with  two  small-scale  DARPA 
SubT  datasets  (one  167  m  long,  the  other  is  490  m  long).  In 
addition,  the  performance  of  Visual-inertial  subsystem  in 
structured man-made environments can also be improved with 
the detected line features in [28]. 

From  the  discussion  of  the  literature,  we  can  see  that  the 
LiDAR-visual-inertial integrated pose estimation and mapping 
has not been well solved and evaluated in large-scale datasets. 
And this paper aims to achieve real-time, low-drift and robust 
odometry and mapping for large-scale metro environments. To 
the best of the authors’ knowledge, our work is the first SLAM-
based metro vehicle localization and mapping system.  

 
 
 
 
III. SYSTEM OVERVIEW 

IV. METHODOLOGY 

The  overview  of  our  system  is  shown  in  Fig.  2,  which  is 
composed  of  three  subsystems:  IMU  odometry,  LiDAR-inertial 
odometry (LIO), and Visual-inertial odometry (VIO). Our system 
follows  the  idea  of  [25],  where  IMU  is  viewed  as  the  primary 
sensor as long as the bias can be well-constrained by other sensors. 
The constrained IMU odometry provides the prediction to the VIO 
and LIO. The LIO and VIO submodule extracts features from raw 
scans and images, which are used for state estimation. Both the two 
modules leverage the factor graph optimization to refine the poses. 
Finally, the IMU odometry submodule achieves these observations 
to constrain the accelerometer bias and gyroscope.  

We  use  a  factor  graph  to  model  this  maximum  a  posterior 
(MAP) problem, and we adopt five types of factors for graph 
construction as shown in Fig. 3, namely: (1) IMU preintegration 
factors;  (2)  IMU  odometry  factors;  (3)  LIO  factors;  (4)  VIO 
factors; (5) prior factors. 

Fig. 3. Overview of the constructed factor graph. 

A. IMU Odometry Factors 

Fig. 2. Overview of the MetroLoc algorithm. 

The notations used throughout this paper is shown in TABLE I. 
 as  the  transformation  from  world 
 can 

In  addition,  we  define   
frame to the IMU frame. And the k-th vehicle state vector 
be written as: 

(cid:5)
(cid:1)∙(cid:3)(cid:4)

(cid:6)(cid:7)

(cid:4)
(cid:4)
(cid:6)(cid:7) (cid:8) (cid:9)(cid:10)(cid:5)(cid:11)
, (cid:13)(cid:5)(cid:11)
, and 
, 
(cid:22)

where 
 are the position, 
linear  velocity,  and  orientation  vector.  The  last  two  elements 
∈ ℝ
are the IMU gyroscope and accelerometer biases. 

∈ SO(cid:1)3(cid:3)

(cid:4)
(cid:10)(cid:5)(cid:11)

(cid:4)
(cid:13)(cid:5)(cid:11)

∈ ℝ

(cid:22)

, (cid:15)(cid:16), (cid:15)(cid:17)(cid:18)

(cid:1)1(cid:3)

(cid:4)
, (cid:14)(cid:5)(cid:11)
(cid:25)
(cid:14)(cid:23)(cid:24)

TABLE I 
NOTATIONS THROUGHOUT THE PAPER 

Notations 

Explanations 

Coordinates 

(cid:4)

(cid:1)∙(cid:3)
(cid:5)
(cid:1)∙(cid:3)
(cid:29)
(cid:1)∙(cid:3)

(cid:1)∙̂(cid:3)
⊗
(cid:1)∙(cid:3) :"

(cid:6)
(cid:1)∙(cid:3)
#
R, q 
$(cid:1)∙(cid:3)

The coordinate of vector 
The coordinate of vector 
The coordinate of vector 

 in global frame. 
 in IMU frame.  
 in LiDAR frame. 

(cid:1)∙(cid:3)
(cid:1)∙(cid:3)
(cid:1)∙(cid:3)

Expression 

. 

(cid:1)∙(cid:3)

Noisy measurement or estimation of vector 
Multiplication between two quaternions. 
Taking  out  the  last  three  elements  from  a 
quaternion vector 
. 
The full state vector. 
(cid:1)∙(cid:3)
. 
The residual of 
The estimated error of 
(cid:1)∙(cid:3)
Two forms of rotation expression, 
the rotation vector, q represents quaternions. 
% ∈ &’(cid:1)3(cid:3)

 is 

(cid:1)∙(cid:3)

. 

Following [25], we add the accelerometer and gyroscope bias   
 into the estimated state of IMU odometry. The edge 
 and 
 can  be  obtained 
between  two  consecutive  state  nodes 
(cid:15)(cid:16)
from  the  relative  motion  measurement.    And  the  residual  of 

 and 

(cid:15)(cid:17)

(

)

preintegrated  IMU  measurements 

formulated by:  

(cid:5).
*+,(cid:5)-

(cid:5).
/0(cid:5)-

(cid:5).
1,(cid:5)-

2

 can  be 

567

#34

(cid:5).
(cid:8) *$+,(cid:5)-

(cid:5).
$/0(cid:5)-

(cid:5).
$80(cid:5)-

: 

   $(cid:15)(cid:16) $(cid:15)(cid:17)2

K

(cid:4)

(cid:25)

B

@

(cid:8)

∆D

(cid:1)2(cid:3)

N :"

(cid:4)
2 IJ(cid:14)(cid:5)-

(cid:4)
⊗ G(cid:14)(cid:5).

1
2
@ B

(cid:4)
>(cid:10)(cid:5).
(cid:5)-
%(cid:4)

(cid:4)
? (cid:10)(cid:5)-
(cid:4)
G (cid:13)(cid:5).
LM

(cid:4)
?  (cid:13)(cid:5)-
(cid:4)
∆D ?  (cid:13)(cid:5)-

(cid:23)-
∆DE ? F,(cid:23).
(cid:5)-
H  ? /0(cid:5).
LM
(cid:5)-
H ⊗ G1,(cid:5).
H

(cid:23)-
⎡%(cid:25)
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
:
(cid:4)
 is  the  time  sweep  between 
(cid:8) R0,0, TU
B

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
 is the gravity factor in the world frame 
where 
.  Since  the  VIO  and 
and 
IMU  preintegration 
LIO  are  both  used 
 )
measurements,  we  can  obtain
 from  them. 
 and 
(cid:4)
Besides, the bias errors are jointly optimized in the graph. With 
(cid:14)(cid:5)-
 calculated  by  LIO  and  VIO, 
the  IMU  residuals 
W5V
the IMU odometry optimization problem can be defined by: 
#34

 and
to  constrain 

(cid:15)(cid:16).   ?   (cid:15)(cid:16)-
(cid:15)(cid:17).   ?   (cid:15)(cid:17)-

(cid:4)
 (cid:10)(cid:5)-

(cid:4)
 (cid:13)(cid:5)-

 and 

(
, 

#34

∆D

(cid:29)5V

:

:

K

(cid:29)5V

(cid:29)5V

W5V

LM
Z34

@ YJ#34

X (cid:8) YJ#34

#34
K
 is the covariance matrix and 

LM
Z34
 is the marginalized 
where 
prior. Since both the pose estimation of LIO and VIO are not 
globally referenced, we only utilize the relative state estimation 
as local constraints to correct the bias of IMU preintegration. 

@ X[ (cid:1)3(cid:3)

Z34

X\

#34

W5V

 
 
 
 
 
 
 
 
 
   
 
 
  
 
 
 
 
 
 
   
B. LIO Factors 

As illustrated in Fig. 4, we utilize a hybrid solid state LiDAR 
Innovusion  Jaguar  Prime 1  throughout  our  experiments.  The 
 field  of  view  (FoV)  has  a 
point  cloud  within  the 
similar density coverage with that of a 300-line rotating LiDAR. 
The  general  scan  matching  method  is  not  suitable  for  this 
scenario for lack of computation efficiency. Besides, employing 
all the feature points for state estimation is not robust towards 
degeneracy problems. 

65° ‘ 40°

Fig. 5. Visual illustration of the extracted line features, blue 
indicates the rail tracks and red describes other lines. 

   The infinite straight lines can be parameterized by a rotation 
matrix 
between two lines 

.  Let  the  error  operator 

 and  two  scalars 

, 

 be defined as: 
b, c  ∈ ℝ

⊖

%

e3

e4
:

Fig.  4.  Visual  illustration  of  the  scan  pattern  of  Innovusion 
Jaguar Prime. The left inset is the raw point cloud of a single 
frame and the right one indicates the corresponding image. 

We  notice  that  the  metro  tunnel  is  a  highly  structured 
scenario with many line features, for instance, rail tracks, power 
lines,  and  pipelines.  Such  line  features  have  proved  to  be  a 
promising  approach  towards  man  made  structured  scenarios 
[28],  [29].  Since  the  rail  tracks  are  the  most  distinctive  line 
structures in the point cloud, they are first extracted based on 
the geometric patterns.  

We first determine the track bed area using the LiDAR sensor 
mounting  height.  The  track  bed  is  composed  of  rail  sleepers, 
rail tracks, and track-side sensors, where the rail tracks are the 
highest  infrastructures. With  the assumption of  the LiDAR is 
centered between two rail tracks, we can set two candidate areas 
around the left and right rail tracks and search the points above 
a  certain  threshold over  the  track  bed.  Two  straight  lines can 
then be fixed using random sample consensus (RANSAC) [30] 
method. Finally, we exploit the idea of region growing [31] for 
further  refinement.  As  a  prevailing  segmentation  algorithm, 
region growing examines neighboring points of initial seed area 
and decides whether to add the point to the seed region or not. 
We set the initial seed area within the distance of 3 m ahead of 
the LiDAR, and the distance threshold of the search region to 
the fitted line is set to 0.07 m, which is the width of the track 
head. Note that we only extract the current two tracks where the 
metro vehicle is on, and a maximum length of 15 m tracks are 
selected for each frame.  

The    other  line  features  are  extracted  following  the  depth-
continuous edge extraction method proposed in [32]. The point 
cloud  is  divided  into  small  voxels  for  each  input  frame,  and 
planes  are  fitted  repeatedly  in  the  voxels.  Then  the  depth-
continuous edges can be detected at the plane intersections. We 
sample multiple points on each extracted line including the two 
tracks, and retain the  lines that satisfy the infinite straight line 
model [33]. The extracted lines are shown in Fig. 5 thanks to 
the high point cloud density coverage. 

1 https://www.innovusion.com/jaguar-prime 

e3 ⊖ e4 (cid:8) fg

:
ijTJ%3

h

%4K, b3 ? b4, c3 ? c4k (cid:1)4(cid:3)

  Then the residual between the two lines can be formulated by: 

1 0
0 1
0 0

(cid:29)5Vl-→l-

#34

e.
(cid:8) Gne-

e3H ⊖ e4

(cid:1)5(cid:3)

where 

 describes the transformation between the 

e.
ne-

4
(cid:8) (cid:1)%3

4
, (cid:10)3

two  lines.  And  we  use  the  derivatives  of  equation  (5)  in  the 
optimization process with symmetric difference method. 

(cid:3)

Additionally,  a  fitted  plane 
united  normal  direction  vector 
o
such that 
:
point to a plane can be expressed as: 
o (cid:8) Rpq

, rqU

pq

 can  be  parameterized  by  the 
, 
 and  a  distance  scalar 
. And the measurement residual from a 

rq

:

(cid:29)5Vst→sl
#
 sets  of  point-to-plane  correspondences 

(cid:1)6(cid:3)

o3

u.
(cid:8) no-

 points, the plane-to-plane residual can be formulated by:  

Suppose  we  have 
and 

vwq

vw

(cid:29)5Vsl→sl

vs{

vs

(cid:29)5Vst→sl

#

4zM

(cid:1)7(cid:3)

(cid:8) x(y Y Y #34
We use the second-order derivatives of point-to-plane cost for 
3zM
efficient optimization as illustrated in [34]. In addition, we use 
the  IMU  forward  propagation  to  predict  the  location  of  the 
previous  lines  and  planes  in  the  current  scan.  Two  lines  are 
considered  a  match  if  their  directions  and  center  distances 
 and  0.5  m.  Similarly,  two 
divergences  are  lower  than 
planes are regarded as a match when difference between their 
10°
 and 0.25 m. 
normal and the distance scalar are smaller than 
Given equation (2), (5) and (7), the minimization problem of 

5°

LIO can be expressed as follows:  

(cid:29)5Vl-→l-

(cid:29)5Vsl→sl

Y #3
3∈v(cid:129)

min (cid:128)

@ Y #3
3∈v{

567

@ Y #3
3∈v(cid:130)

(cid:135)

(cid:1)8(cid:3)

, 

where 
plane correspondences, and IMU preintegration factors. 
the marginalization factors and 

(cid:29)5V
@#[
 are the number of line-to-line, plane-to-
 is 
 is the predicted pose 

w(cid:134)3(cid:132)(cid:134)
@ #3[(cid:131)(cid:132)(cid:133)(cid:132)[

, and 

vq

v(cid:29)

v5

(cid:29)5V
#[

w(cid:134)3(cid:132)(cid:134)
#3[(cid:131)(cid:132)(cid:133)(cid:132)[

 
 
 
 
 
 
 
 
 
 
 
constraints from IMU odometry. The optimization problem can 
be efficiently solved by Levenberg-Marquardt algorithm [35]. 

post-processing  software.  The  initial  global  position  is  set  by 
Real Time Kinematic (RTK) at the starting point outdoors.  

C. VIO Factors 

Our VIO follows the pipeline of Vins-mono [36] as shown in 
Fig. 3. The point features are detected by [37], tracked by KLT 
sparse optical flow [38], and refined by RANSAC.  For the line 
features, we employ the LSD [39] for line segment extraction 
and  PL-VINS  [40]  for  line  feature  fused  state  estimation  as 
shown in Fig. 6(a). In addition, we register LiDAR frames to 
the  camera  frames,  and  project  the  3D  distortion-free  point 
cloud to the 2D image as illustrated in Fig. 6(b). Then we can 
use the depth information for scale correction and joint graph 
optimization following [28]. And the minimization problem of 
VIO can be expressed as follows:  

W5Vl-→l-

W5Vst→st

Y #3
3∈vl-

min (cid:128)

@ Y #3
3∈vst

567

@ Y #3
3∈v(cid:130)

(cid:135) (cid:1)9(cid:3)

w(cid:134)3(cid:132)(cid:134)
@ #3[(cid:131)(cid:132)(cid:133)(cid:132)[

W5V
 ,  and 

@#[
W5Vst→st

, 

W5Vl-→l-

where 
point  reprojection  error,  and  marginalization  factors. 

are  the  relative  line  and 
 and 
#[
 represent the amount of line and point features tracked by 

W5V

#3

#3

v(cid:138)3

other frames. 
vw(cid:132)

(a)                                             (b) 

Fig. 6. Visual illustration of the extracted line feature in (a) and 
the depth frame in (b), the color is coded by the depth value. 

V. EXPERIMENTS 

A. Hardware Setup and Ground Truth Description 

We select a metro maintenance vehicle as shown in Fig. 7, 
which includes a hybrid solid-state LiDAR Innovusion Jaguar 
Prime, a tactical grade IMU Sensonor stim 300, and a Hikvision 
supervisory  camera.  All  the  three  sensors  are  hardware 
synchronized  with  an  external  atomic  clock.  The  dataset  is 
processed  by  a  high-performance  onboard  computer,  with i9-
10980HK  CPU,  64  GB  RAM.  Besides,  our  algorithms  are 
implemented in C++ and executed in Ubuntu Linux using the 
ROS [41].  Since  the  output of the  Hikvision camera is  in  the 
video format, we first merge the video with the recorded dataset 
using the synchronized timestamp. The nonlinear optimization 
and  factor  graph  optimization  problem  is  solved  using  Ceres 
Solver  and  GTSAM  [42],  respectively.  Our  proposed  system 
can reach real-time performance for all the captured datasets. 

The  odometry  and  map  ground  truth  is  captured  by  two 
survey grade 3D laser scanners, a near navigation grade IMU 
containing  fiber  optic  gyros,  and  processed  by  professional 

Fig. 7. The hardware setup of our system. 

We conduct extensive experiments utilizing the maintenance 
vehicle platform, and two  novel  LiDAR-visual-inertial  fusion 
algorithms  are  selected  for  comparison:  LVI-SAM  [24]  and 
R2LIVE [23]. In addition, LOAM [10], Lio-sam [9], and VINS-
Mono [36] are also selected for ablation study here. The LIO 
and  VIO  part  of  our  system  is  denoted  as  MetroLIO  and 
MetroVIO, respectively. 

B. Experiment-1: Feature-Rich Crossover and Station  

Depicted in Fig. 8(a) and Fig. 8(b), the metro crossover and 
station are feature-rich areas with multiple rail tracks, guardrails, 
pipelines, and platforms. Besides, the visual sequences are not 
degraded because of the head light and station lighting.  

In this experiment, we manually drive the metro maintenance 
vehicle for 420 m with a relative low velocity. TABLE II shows 
the evaluation results comparing to different approaches. And 
we can infer that the short-during scenario seems to be effortless 
for state-of-the-art SLAM algorithms. 

C. Experiment-2: Structured Tunnel 

In this experiment, we aim to evaluate the robustness of our 
algorithm under a 750 m metro tunnel scenario, where the only 
observable  features  are  repetitive  and  structured  man-made 
infrastructures  as  shown  in  Fig.  9(a)  and  Fig.  9(b).  Since  the 
tunnel walls are almost flat and no extra dynamic objects can 
be employed for feature matching, the metro tunnel is one of 
the most challenging scenarios for SLAM.  

The state estimation errors of different methods are listed in 
TABLE II. Due to the highly regulated and constrained motion on 
the rail tracks, the IMU biases cannot be initialized and estimated 
correctly  with  inadequate  axis  excitement.  This  leads  to  serious 
scale drift and result in significant pose estimation errors in VINS-
Mono. Besides, the wrong feature matches from repetitive pattern 
of the surround also exacerbate the deviations.  

The feature-based  LiDAR  SLAM also suffers from  feature 
tracking problems. With only frame-to-frame matching, LOAM 
fails several times in the tunnel, and the LiDAR odometry even 
moves backwards without accurate pose estimation. Since Lio- 
sam relies heavily on the LiDAR odometry to further constrain 
the  pre-integrated  IMU  states,  it  also  generates  unfavorable 
results.  On  the  contrary,  both  the  MetroLIO  and  MetroVIO 
achieves  less  drift  state  estimation  employing  plane-line  and 
point-line feature tracking. 

 
 
 
 
 
 
 
 
TABLE II 
ACCURACY EVALUATION OF EXPERIMENT-1 AND EXPERIMENT-2 

Absolute Trajectory Error (ATE) – Translational [m] / Rotational [

] 

  Sequence 

Experiment-1 
Experiment-2 

LVI-SAM  R2LIVE 

LOAM 

Lio-sam  VINS-Mono  MetroLIO  MetroVIO  MetroLoc 

°

0.49 / 2.56 

0.32 / 1.80 

1.45 / 3.68 

0.53 / 1.75 

4.23 / 6.92 

0.58 / 1.64 

22.79 / 45.79 

8.43 / 9.87 

48.75 / 26.23 

38.92 / 22.00 

80.94 / 55.67 

4.75 / 6.41 

1.74 / 4.98 

7.62 / 8.07 

0.63 / 2.72 

1.92 / 3.58 

(a)                                         (b) 

D. Experiment-3: Long-During Mapping and Odometry 

In this experiment, we aim to show that MetroLoc is accurate 
and robust enough to reconstruct a  long-during map  in  metro 
environments.  The  overall  length  is  around  5.2  km  with  4 
stations along the path. We plot the real-time reconstructed 3D 
maps  as  well  as  the  ground  truth  in  Fig.  10,  where  both  the 
horizontal  headings  and  height  variations  are  visually  well 
matched. Note that there are many outlier points around tunnel 
structures in the reconstructed map, we believe they are mainly 
caused by the LiDAR range measurement noises. In addition, 
we adopt the EVO2 package to compare each trajectory against 
ground  truth  as  shown  in  Fig.  11.  We  can  infer  that  the 
monocular vision-only methods have the worst performance in 
this situation, as explained above. Our MetroLoc outperforms 
the selected methods with the lowest ATE of 5.62 m. 

(c) 

Fig. 8. Visual illustration of the crossover and station. (a) and 
(b)  denotes  the  images  from  a  crossover  and  a  metro  station, 
respectively. (c) is the MetroLoc mapping result aligned with 
RGB color. 

Station4 

Station3 

Station2 

Station1 

Fig. 10. Visual comparison of the mapping result from ground 
truth (above) and MetroLoc (bottom), the color indicates height 
variations. The blue areas are filled by the outlier points. 

THE AVERAGE RUNNING TIME OF SUBMODULES [MS] 

TABLE III 

Data 
Experiment-1 
Experiment-2 

MetroLIO  MetroVIO 

37 
32 

21 
19 

IMUODOM 
0.5 
0.5 

E. Runtime Analysis 

Thanks to the multi-threaded computation, all the subsystems 
can  run  in  parallel.  The  average  time  consumption  of  each 
submodule  is  listed  in  TABLE  III,  illustrating  that  MetroLoc 
can achieve real-time performance for the onboard computer. 

(a)                                                (b) 

(c)                                              

Fig. 9. Visual illustration of the structured tunnel. (a) and (b) 
are images from the supervisory camera. (c) denotes the vertical 
view of the mapping result from MetroLoc. 

2 https://github.com/MichaelGrupp/evo 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(a) LVI-SAM                                 (b) R2LIVE                                   (c) LOAM                                (d) Lio-sam 

(e) Vins-mono                              (f) MetroLIO                                 (g) MetroVIO                            (h) MetroLoc 

Fig. 11. Trajectory evaluations of different methods. 

F. Towards Next-generation Railway Monitoring 

The current railway and metro rail maintenance system is still 
dominated by human force. For the metro system, the workers 
always drive the maintenance vehicle at midnight to manually 
check  the  safety  of  power  line,  pipeline,  as  well  as  trackside 
infrastructures.  We  hereby  propose  a  prototype  design  of  a 
semi-auto monitoring system using the reconstructed map from 
MetroLoc.  The  maps  are  exported  to  a  game  engine  Unity3, 
which  can  be  easily  visualized  through  Virtual  Reality  (VR) 
glasses, for instance, HTC VIVE Pro. Since the 3D points all 
have  pose  information,  the  operator  can  easily  locate  the 
potential  districts.  Besides,  the  abnormal  powerlines  can  be 
indicated  with  integrated  infrared  cameras.  We  also  develop 
various  tools  in  VR,  such  as  direct  range  measuring,  region 
picking, and view sharing. In addition, we can use the AirSim4 
and  the  pose  estimation  from  MetroLoc  to  visualize  the  real-
time train location information as illustrated in Fig. 12. 

Fig. 12. Visual illustration of the train pose monitoring system, 
the color is coded by height variations.  

Fig. 13. Visual illustration of the hectometer post (recognized 
and marked) and station indicator signs (yellow) on the wall. 

VI. CONCLUSION 

In this paper, we propose a robust and versatile simultaneous 
localization  and  mapping  approach  for  metro  vehicles.  Our 
method tightly integrates measurements from LiDAR, camera, 
and IMU with an IMU-centric state estimator. To cope with the 
highly repeated man-made structures, we propose to extract and 
track  geometric  features.  The  proposed  method  has  been 
validated in extreme metro environments. And the results show 
that our framework is accurate and robust towards long-during 
motion  estimation  and  mapping  problems.  We  hope  that  our 
experimental  work  and  extensive  evaluation  could  inspire 
follow-up  works  to  explore  more  structural  or  situational 
awareness  information  in  highly-repetitive  scenarios,  such  as 
tags or signs on the wall as shown in Fig. 13. Besides, we wish 
the  community  pay  more  attention  to  railroad  applications, 
especially  for  facility  and  environment  monitoring.  A  small 
advance  towards  autonomous  system  will  save  tremendous 
amount of manpower for construction and maintenance. 

3 https://unity.com/ 

4 https://microsoft.github.io/AirSim/ 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ACKNOWLEDGMENT 

We  would  like  to  thanks  colleagues  from  CRRC  Zhuzhou 
Locomotive  CO.,  LTD  for  providing  testing  platform  and 
gathering experimental data. 

REFERENCES 

[1] 

J.  Otegui,  A.  Bahillo,  I.  Lopetegi,  and  L.  E.  Díez,  “A  survey  of  train 
positioning solutions,” IEEE Sensors Journal, vol. 17, no. 20, pp. 6788–
6797, 2017. 

[4] 

[3] 

[2]  D.  Lu  and  E.  Schnieder,  “Performance  evaluation  of  GNSS  for  train 
localization,”  IEEE  transactions  on  intelligent  transportation  systems, 
vol. 16, no. 2, pp. 1054–1059, 2014. 
J.  Marais,  J.  Beugin,  and  M.  Berbineau,  “A  survey  of  GNSS-based 
research and developments for the  European railway signaling,” IEEE 
Transactions on Intelligent Transportation Systems, vol. 18, no. 10, pp. 
2602–2618, 2017. 
J.  Otegui,  A.  Bahillo,  I.  Lopetegi,  and  L.  E.  Díez,  “Evaluation  of 
experimental GNSS and 10-DOF MEMS IMU measurements for train 
positioning,” IEEE Transactions on Instrumentation and Measurement, 
vol. 68, no. 1, pp. 269–279, 2018. 

[5]  W. Jiang, S. Chen, B. Cai, J. Wang, W. ShangGuan, and C. Rizos, “A 
multi-sensor positioning method-based train localization system for low 
density line,” IEEE Transactions on Vehicular Technology, vol. 67, no. 
11, pp. 10425–10437, 2018. 
J. Liu, B. Cai, and J. Wang, “Track-constrained GNSS/odometer-based 
train  localization  using  a  particle  filter,”  in  2016  IEEE  Intelligent 
Vehicles Symposium (IV), 2016, pp. 877–882. 

[7]  H.  Winter,  V.  Willert,  and  J.  Adamy,  “Increasing  accuracy  in  train 
localization  exploiting  track-geometry  constraints,”  in  2018  21st 
International Conference on Intelligent Transportation Systems (ITSC), 
2018, pp. 1572–1579. 

[8]  W. Xu, Y. Cai, D. He, J. Lin, and F. Zhang, “FAST-LIO2: Fast Direct 
LiDAR-inertial Odometry,” arXiv preprint arXiv:2107.06829, 2021. 

[9]  T. Shan, B. Englot, D. Meyers, W. Wang, C. Ratti, and D. Rus, “Lio-
sam:  Tightly-coupled  lidar  inertial  odometry  via  smoothing  and 
mapping,”  in  2020  IEEE/RSJ  International  Conference  on  Intelligent 
Robots and Systems (IROS), 2020, pp. 5135–5142. 

[6] 

[10]  J. Zhang and S. Singh, “LOAM: Lidar Odometry and Mapping in Real-

time.,” in Robotics: Science and Systems, 2014, vol. 2, no. 9. 

[11]  T.  Daoust,  F.  Pomerleau,  and  T.  D.  Barfoot,  “Light  at  the  end  of  the 
tunnel:  High-speed 
in  challenging 
underground environments,” in 2016 13th Conference on Computer and 
Robot Vision (CRV), 2016, pp. 93–100. 

localization 

lidar-based 

train 

[12]  W. Jiang et al., “A  Seamless  Train  Positioning System using  a Lidar-
aided  Hybrid  Integration  Methodology,”  IEEE  Transactions  on 
Vehicular Technology, 2021. 

[13]  M. Palieri et al., “Locus: A multi-sensor lidar-centric solution for high-
precision odometry  and 3d mapping  in  real-time,”  IEEE  Robotics  and 
Automation Letters, vol. 6, no. 2, pp. 421–428, 2020. 

[14]  K.  Ebadi  et  al.,  “LAMP:  Large-scale  autonomous  mapping  and 
positioning  for  exploration  of  perceptually-degraded  subterranean 
environments,” in 2020 IEEE International Conference on Robotics and 
Automation (ICRA), 2020, pp. 80–86. 

[15]  W. Tabib, K. Goel, J. Yao, C. Boirum, and N. Michael, “Autonomous 
cave  surveying  with an  aerial  robot,”  IEEE Transactions  on  Robotics, 
2021. 

[16]  J.  Zhang  and  S.  Singh,  “Laser–visual–inertial  odometry  and  mapping 
with high robustness and low drift,” Journal of Field Robotics, vol. 35, 
no. 8, pp. 1242–1264, 2018. 

[17]  W.  Wang,  J.  Liu,  C.  Wang,  B.  Luo,  and  C.  Zhang,  “Dv-loam:  Direct 
visual lidar odometry and mapping,” Remote Sensing, vol. 13, no. 16, p. 
3340, 2021. 

[18]  H. Yang, J. Shi, and L. Carlone, “Teaser: Fast and certifiable point cloud 
registration,” IEEE Transactions on Robotics, vol. 37, no. 2, pp. 314–
333, 2020. 

[19]  S.  Khattak,  H.  Nguyen,  F.  Mascarich,  T.  Dang,  and  K.  Alexis, 
“Complementary  multi–modal  sensor  fusion  for  resilient  robot  pose 
estimation  in  subterranean  environments,”  in  2020  International 
Conference on Unmanned Aircraft Systems (ICUAS), 2020, pp. 1024–
1029. 

[20]  Y. Su, T. Wang, S. Shao, C. Yao, and Z. Wang, “GR-LOAM: LiDAR-
based  sensor  fusion  SLAM  for  ground  robots  on  complex  terrain,” 
Robotics and Autonomous Systems, vol. 140, p. 103759, 2021. 
[21]  M. Camurri, M. Ramezani, S. Nobili, and M. Fallon, “Pronto: A multi-
sensor  state  estimator  for  legged  robots  in  real-world  scenarios,” 
Frontiers in Robotics and AI, vol. 7, p. 68, 2020. 

[22]  D.  Wisth,  M.  Camurri,  S.  Das,  and  M.  Fallon,  “Unified  Multi-Modal 
Landmark  Tracking 
for  Tightly  Coupled  Lidar-Visual-Inertial 
Odometry,”  IEEE  Robotics  and  Automation  Letters,  vol.  6,  no.  2,  pp. 
1004–1011, 2021. 

[23]  J. Lin, C. Zheng, W. Xu, and F. Zhang, “R2LIVE: A Robust, Real-time, 
LiDAR-Inertial-Visual  tightly-coupled  state  Estimator  and  mapping,” 
arXiv preprint arXiv:2102.12400, 2021. 

[24]  T. Shan, B. Englot, C. Ratti, and D. Rus, “LVI-SAM: Tightly-coupled 
Lidar-Visual-Inertial  Odometry  via  Smoothing  and  Mapping,”  arXiv 
preprint arXiv:2104.10831, 2021. 

[25]  S.  Zhao,  H.  Zhang,  P.  Wang,  L.  Nogueira,  and  S.  Scherer,  “Super 
for 

Odometry: 
Challenging Environments,” arXiv preprint arXiv:2104.14938, 2021. 

IMU-centric  LiDAR-Visual-Inertial  Estimator 

[26]  X. Zuo, P. Geneva, W. Lee, Y. Liu, and G. Huang, “Lic-fusion: Lidar-
inertial-camera odometry,” in 2019 IEEE/RSJ International Conference 
on Intelligent Robots and Systems (IROS), 2019, pp. 5848–5854. 
[27]  X.  Zuo  et  al.,  “Lic-fusion  2.0:  Lidar-inertial-camera  odometry  with 
sliding-window plane-feature tracking,” in 2020 IEEE/RSJ International 
Conference on Intelligent Robots and Systems (IROS), 2020, pp. 5112–
5119. 

[28]  S.-S. Huang, Z.-Y. Ma, T.-J. Mu, H. Fu, and S.-M. Hu, “Lidar-monocular 
visual  odometry  using  point  and  line  features,”  in  2020  IEEE 
International Conference on Robotics and Automation (ICRA), 2020, pp. 
1091–1097. 

[29]  D. Zou, Y. Wu, L. Pei, H. Ling, and W. Yu, “StructVIO: visual-inertial 
odometry with structural regularity of man-made environments,” IEEE 
Transactions on Robotics, vol. 35, no. 4, pp. 999–1013, 2019. 

[30]  M. A. Fischler and R. C. Bolles, “Random sample consensus: a paradigm 
for  model  fitting  with  applications  to  image  analysis  and  automated 
cartography,” Communications of the ACM, vol. 24, no. 6, pp. 381–395, 
1981. 

[31]  R. Adams and L. Bischof, “Seeded region growing,” IEEE Transactions 
on pattern analysis and machine intelligence, vol. 16, no. 6, pp. 641–647, 
1994. 

[32]  C.  Yuan,  X.  Liu,  X.  Hong,  and  F.  Zhang,  “Pixel-level  Extrinsic  Self 
Calibration  of  High  Resolution  LiDAR  and  Camera  in  Targetless 
Environments,” arXiv preprint arXiv:2103.01627, 2021. 

[33]  C. J. Taylor and D. J. Kriegman, “Minimization on the Lie group SO (3) 
and related manifolds,” Yale University, vol. 16, no. 155, p. 6, 1994. 
[34]  Z.  Liu  and  F.  Zhang,  “Balm:  Bundle  adjustment  for  lidar  mapping,” 
IEEE  Robotics  and  Automation  Letters,  vol.  6,  no.  2,  pp.  3184–3191, 
2021. 

[35]  J. J.  Moré, “The  Levenberg-Marquardt  algorithm:  implementation and 

theory,” in Numerical analysis, Springer, 1978, pp. 105–116. 

[36]  T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monocular 
visual-inertial state estimator,” IEEE Transactions on Robotics, vol. 34, 
no. 4, pp. 1004–1020, 2018. 

[37]  J. Shi, “Good features to track,” in 1994 Proceedings of IEEE conference 
on computer vision and pattern recognition, 1994, pp. 593–600. 
[38]  B. D. Lucas and T. Kanade, “An iterative image registration technique 

with an application to stereo vision,” 1981. 

[39]  R. G. Von Gioi, J. Jakubowicz, J.-M. Morel, and G. Randall, “LSD: A 
fast  line  segment  detector  with  a  false  detection  control,”  IEEE 
transactions on pattern analysis and machine intelligence, vol. 32, no. 4, 
pp. 722–732, 2008. 

[40]  Q. Fu, J. Wang, H. Yu, I. Ali, F. Guo, and H. Zhang, “PL-VINS: Real-
Time Monocular Visual-Inertial SLAM with Point and Line,” arXiv e-
prints, p. arXiv-2009, 2020. 

[41]  M. Quigley et al., “ROS: an open-source Robot Operating System,” in 
ICRA workshop on open source software, 2009, vol. 3, no. 3.2, p. 5. 
[42]  F.  Dellaert,  “Factor  graphs  and  GTSAM:  A  hands-on  introduction,” 

Georgia Institute of Technology, 2012. 

 
 
 
