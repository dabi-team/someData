1
2
0
2

l
u
J

8

]
I

N
.
s
c
[

2
v
1
0
0
1
0
.
7
0
1
2
:
v
i
X
r
a

1

Feeling of Presence Maximization:

mmWave-Enabled Virtual Reality Meets Deep

Reinforcement Learning

Peng Yang, Member, IEEE, Tony Q. S. Quek, Fellow, IEEE, Jingxuan Chen,

Chaoqun You, Member, IEEE, and Xianbin Cao, Senior Member, IEEE

Abstract

This paper investigates the problem of providing ultra-reliable and energy-efﬁcient virtual reality

(VR) experiences for wireless mobile users. To ensure reliable ultra-high-deﬁnition (UHD) video frame

delivery to mobile users and enhance their immersive visual experiences, a coordinated multipoint

(CoMP) transmission technique and millimeter wave (mmWave) communications are exploited. Owing

to user movement and time-varying wireless channels, the wireless VR experience enhancement problem

is formulated as a sequence-dependent and mixed-integer problem with a goal of maximizing users’

feeling of presence (FoP) in the virtual world, subject to power consumption constraints on access points

(APs) and users’ head-mounted displays (HMDs). The problem, however, is hard to be directly solved

due to the lack of users’ accurate tracking information and the sequence-dependent and mixed-integer

characteristics. To overcome this challenge, we develop a parallel echo state network (ESN) learning

method to predict users’ tracking information by training fresh and historical tracking samples separately

collected by APs. With the learnt results, we propose a deep reinforcement learning (DRL) based

optimization algorithm to solve the formulated problem. In this algorithm, we implement deep neural

networks (DNNs) as a scalable solution to produce integer decision variables and solve a continuous

power control problem to criticize the integer decision variables. Finally, the performance of the proposed

algorithm is compared with various benchmark algorithms, and the impact of different design parameters
is also discussed. Simulation results demonstrate that the proposed algorithm is more 4.14% energy-

efﬁcient than the benchmark algorithms.

P. Yang, T. Q. S. Quek, and C. You are with the Information Systems Technology and Design, Singapore University of

Technology and Design, 487372 Singapore. J. Chen and X. Cao are with the School of Electronic and Information Engineering,

Beihang University, Beijing 100083, China.

 
 
 
 
 
 
2

Index Terms

Virtual reality, coordinated multipoint transmission, feeling of presence, parallel echo state network,

deep reinforcement learning

I. INTRODUCTION

Virtual reality (VR) applications have attracted tremendous interest in various ﬁelds, including

entertainment, education, manufacturing, transportation, healthcare, and many other consumer-

oriented services [1]. These applications exhibit enormous potential in the next generation of

multimedia content envisioned by enterprises and consumers through providing richer and more

engaging, and immersive experiences. According to market research [2], the VR ecosystem is

predicted to be an 80 billion market by 2025, roughly the size of the desktop PC market today.

However, several major challenges need to be overcome such that businesses and consumers

can get fully on board with VR technology [3], one of which is to provide compelling content.

To this aim, the resolution of provided content must be guaranteed. In VR applications, VR

wearers can either view objects up close or across a wide ﬁeld of view (FoV) via head-mounted

or goggle-type displays (HMDs). As a result, very subtle defects such as poorly rendering pixels

at any point on an HMD may be observed by a user up close, which may degrade users’ truly

visual experiences. To create visually realistic images across the HMD, it must have more display

pixels per eye, which indicates that ultra-high-deﬁnition (UHD) video frame transmission must be

enabled for VR applications. However, the transmission of UHD video frames typically requires

4 − 5 times the system bandwidth occupied for delivering a regular high-deﬁnition (HD) video

[4], [5]. Further, to achieve good user visual experiences, the motion-to-photon latency should be

ultra-low (e.g., 10 − 25 ms) [6]–[8]. High motion-to-photon values will send conﬂicting signals

to the Vestibulo-ocular reﬂex (VOR) and then might cause dizziness or motion sickness.

Hence, today’s high-end VR systems such as Oculus Rift [9] and HTC Vive [10] that offer high

quality and accurate positional tracking remain tethered to deliver UHD VR video frames while

satisfying the stringent low-latency requirement. Nevertheless, wired VR display may degrade

users’ seamless visual experiences due to the constraint on the movement of users. Besides, a

tethered VR headset presents a potential tripping hazard for users. Therefore, to provide ultimate

VR experiences, VR systems or at least the headset component should be untethered [6].

Recently, the investigation on wireless VR has attracted numerous attention from both industry

and academe; of particular interest is how to a) develop mobile (wireless and lightweight) HMDs,

3

b) how to enable seamless and immersive VR experiences on mobile HMDs in a bandwidth-

efﬁciency manner, while satisfying ultra-low-latency requirements.

A. Related work

On the aspect of designing lightweight VR HMDs, considering heavy image processing tasks,

which are usually insufﬁcient in the graphics processing unit (GPU) of a local HMD, one might

be persuaded to transfer the image processing from the local HMD to a cloud or network edge

units (e.g., edge servers, base stations, and access points (APs)). For example, the work in [1]

proposed to enable mobile VR with lightweight VR glasses by completing computation-intensive

tasks (such as encoding and rendering) on a cloud/edge server and then delivering video streams

to users. The framework of fog radio access networks, which could signiﬁcantly relieve the

computation burden by taking full advantages of the edge fog computing, was explored in [11]

to facilitate the lightweight HMD design.

In terms of proposing VR solutions with improved bandwidth utilization, current studies can

be classiﬁed into two groups: tiling and video coding [12] As for tiling, some VR solutions

propose to spatially divide VR video frames into small parts called tiles, and only tiles within

users’ FoV are delivered to users [13]–[15]. The FoV of a user is deﬁned as the extent of the

observable environment at any given time. By sending HD tiles in users’ FoV, the bandwidth

utilization is improved. On the aspect of video coding, the VR video is encoded into multiple

versions of different quality levels. Viewers receive appropriate versions based on their viewing

directions [16].

Summarily, to improve bandwidth utilization, the aforementioned works [13]–[16] either trans-

mit relatively narrow user FoV or deliver HD video frames. Nevertheless, wider FoV is signiﬁ-

cantly important for a user to have immersive and presence experiences. Meanwhile, transmitting

UHD video frames can enhance users’ visual experiences. To this aim, advanced wireless

communication techniques (particularly, millimeter wave (mmWave)), which can signiﬁcantly

improve data rates and reduce propagation latency via providing wide bandwidth transmission,

are explored in VR video transmission [4], [17], [18]. For example, the work in [4] utilized

a mmWave-enabled communication architecture to support the panoramic and UHD VR video

transmission. Aiming to improve users’ immersive VR experiences in a wireless multi-user

VR network, a mmWave multicast transmission framework was developed in [17]. Besides, the

mmWave communication for ultra-reliable and low latency wireless VR was investigated in [18].

4

B. Motivation and contributions

Although mmWave techniques can alleviate the current bottleneck for UHD video delivery,

mmWave links are prone to outage as they require line-of-sight (LoS) propagation. Various

physical obstacles in the environment (including users’ bodies) may completely break mmWave

links [19]. As a result, VR requirements for a perceptible image-quality degradation-free uniform

experience cannot be accommodated. However, the mmWave VR-related works in [4], [17], [18]

did not effectively investigate the crucial issue of guaranteeing the transmission reliability of VR

video frames. To signiﬁcantly improve the transmission reliability of VR video frames under

low-latency constraints, the coordinated multipoint (CoMP) transmission technique, which can

improve the reliability via spatial diversity, can be explored [20]. Besides, it is extensively

considered that proactive computing (e.g., image processing or frame rendering) enabled by

adopting machine learning methods is a crucial ability for a wireless VR network to mandate

the stringent low-latency requirement of UHD VR video transmission [1], [19], [21], [22].

Therefore, this paper investigates the issue of maximizing users’ feeling of presence (FoP) in

their virtual world in a mmWave-enabled VR network incorporating CoMP transmission and

machine learning. The main contributions of this paper are summarized as follows:

• Owing to the user movement and the time-varying wireless channel conditions, we formulate

the issue of maximizing users’ FoP in virtual environments as a mixed-integer and sequential

decision problem, subject to power consumption constraints on APs and users’ HMDs. This

problem is difﬁcult to be directly solved by exploring conventional numerical optimization

methods due to the lack of accurate users’ tracking information (including users’ locations

and orientation angles) and mixed-integer and sequence-dependent characteristics.

• As users’ historical tracking information is separately collected by diverse APs, a parallel

echo state network (ESN) learning method is designed to predict users’ tracking information

while accelerating the learning process.

• With the predicted results, we develop a deep reinforcement learning (DRL) based optimiza-

tion algorithm to tackle the mixed-integer and sequential decision problem. Particularly, to

avoid generating infeasible solutions by simultaneously optimizing all variables while allevi-

ating the curse of dimensionality issue, the DRL-based optimization algorithm decomposes

the formulated mixed-integer optimization problem into an integer association optimization

problem and a continuous power control problem. Next, deep neural networks (DNNs) with

5

continuous action output spaces followed by an action quantization scheme are implemented

to solve the integer association problem. Given the association results, the power control

problem is solved to criticize them and optimize the transmit power.

• Finally, the performance of the proposed DRL-based optimization algorithm is compared

with various benchmark algorithms, and the impact of different design parameters is also

discussed. Simulation results demonstrate the effectiveness of the proposed algorithm.

II. SYSTEM MODEL AND PROBLEM FORMULATION

As shown in Fig. 1, we consider a mmWave-enabled VR network incorporating a CoMP

transmission technique. This network includes a centralized unit (CU) connecting to J distributed

units (DUs) via optical ﬁber links, a set J of J access points (APs) connected with the DUs,

and a set of U of N ground mobile users wearing HMDs. To acquire immersive and interactive

experiences, users will report their tracking information to their connected APs via reliable

uplink communication links. Further, with collected users’ tracking information, the CU will

centrally simulate and construct virtual environments and coordinately transmit UHD VR videos

to users via all APs in real time. To accomplish the task of enhancing users’ immersive and

interactive experiences in virtual environments, joint uplink and downlink communications should

be considered. We assume that APs and users can work at both mmWave (exactly, 28 GHz) and

sub-6 GHz frequency bands, where the mmWave frequency band is reserved for downlink UHD

VR video delivery, and the sub-6 GHz frequency band is allocated for uplink users’ tracking

information transmission. This is because an ultra-high data rate can be achieved on the mmWave

frequency band, and sub-6 GHz can support reliable communications. Besides, to theoretically

model the joint uplink and downlink communications, we suppose that the time domain is

discretized into a sequence of time slots in the mmWave-enabled VR network and conduct the

system modelling including uplink and downlink transmission models, FoP model, and power

consumption model.

A. Uplink and downlink transmission models

1) Uplink transmission model: Denote x3D

it = [xit, yit, hi]T as the three dimensional (3D)
Cartesian coordinate of the HMD worn by user i for all i ∈ U at time slot t and hi ∼ N (¯h, σ2
h)
is the user height. [xit, yit]T is the two dimensional (2D) location of user i at time slot t. Denote
j = [xj, yj, Hj]T as the 3D coordinate of the antenna of AP j and Hj is the antenna height.
v3D

6

(cid:39)(cid:56)(cid:3)(cid:20)
(cid:39)(cid:56)(cid:3)(cid:21)
(cid:39)(cid:56)(cid:3)(cid:45)

(cid:1006)(cid:1012)(cid:3)(cid:39)(cid:44)(cid:460)
(cid:94)(cid:437)(cid:271)(cid:882)(cid:1010)(cid:3)(cid:39)(cid:44)(cid:460)

(cid:38)(cid:56)

(cid:60)

(cid:59)

j

(cid:36)(cid:81)(cid:87)(cid:72)(cid:81)(cid:81)(cid:68)(cid:3)
(cid:71)(cid:82)(cid:90)(cid:81)(cid:87)(cid:76)(cid:79)(cid:87)(cid:3)
(cid:537)j

(cid:36)(cid:81)(cid:87)(cid:72)(cid:81)(cid:81)(cid:68)(cid:3)
(cid:75)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)Hj

(cid:43)(cid:68)(cid:79)(cid:73)(cid:16)(cid:83)(cid:82)(cid:90)(cid:72)(cid:85)(cid:3)
(cid:69)(cid:68)(cid:81)(cid:71)(cid:90)(cid:76)(cid:71)(cid:87)(cid:75)(cid:3)(cid:1483)  

(cid:56)(cid:86)(cid:72)(cid:85)(cid:3)(cid:87)(cid:76)(cid:79)(cid:87)(cid:3)(cid:68)(cid:81)(cid:74)(cid:79)(cid:72)(cid:3)
(cid:287)BjCjDit

(cid:56)(cid:86)(cid:72)(cid:85)(cid:3)
(cid:75)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)hi

it

j

Fig. 1. A mmWave-enabled VR network incorporating

Fig. 2. Sectored antenna model of an AP.

CoMP transmission.

Owing to the reliability requirement, users’ data information (e.g., users’ tracking information

and proﬁles) is required to be successfully decoded by corresponding APs. We express the

condition that an AP can successfully decode the received user data packets as follows

SNRul

ijt =

ijtpitcijˆhijt
aul
N0W ul/N

≥ θth, ∀i, j, t,

(1)

where aul

be successfully decoded by AP j at time slot t. The data packets can be decoded if aul

ijt ∈ {0, 1} is an association variable indicating whether user i’s uplink data packets can
ijt = 1;
ijt = 0. pit is the uplink transmit power of user i’s HMD, cij is the Rayleigh
j ) is the uplink path-loss from user i to AP j with α being the
fading exponent, dijt(·) denotes the Euclidean distance between user i and AP j, N0 denotes the

otherwise, aul
channel gain, ˆhijt = d−α

ijt (x3D

it , v3D

single-side noise spectral density, W ul represents the uplink bandwidth. θth is the target signal-

to-noise ratio (SNR) experienced at AP j for successfully decoding data packets from user i.

Besides, considering the reliability requirement of uplink transmission and the stringent power

constraint on HMDs, frequency division multiplexing (FDM) technique is adopted in this paper.

The adoption of FDM technique can avoid the decoding failure resulting from uplink signal

interferences and signiﬁcantly reduce power consumption without compensating the signal-to-

interference-plus-noise ratio (SINR) loss caused by uplink interferences.

Additionally, we assume that each user i can connect to at most one AP j via the uplink

channel at each time slot t, i.e.,

ijt ≤ 1, ∀i. This is reasonable because it is unnecessary
for each AP to decode all users’ data successfully at each time slot t. A user merely connects

j∈J aul

P

to an AP (e.g., the nearest AP if possible) will greatly reduce power consumption. Meanwhile,

considering the stringent low-latency requirements of VR applications and the time consumption

7

of processing (e.g., decoding and checking) received user data packets, we assume that an AP
can serve up to ˜M users during a time slot, i.e.,

ijt ≤ ˜M , ∀j.

i∈U aul

2) Downlink transmission model: In the downlink transmission conﬁguration, antenna arrays

P

are deployed to perform directional beamforming. For analysis facilitation, a sectored antenna

model [23], which consists of four components, i.e., the half-power beamwidth φ, the antenna

downtilt angle θj ∀j, the antenna gain of the mainlobe G, and the antenna gain of the sidelobe

g, shown in Fig. 2, is exploited to approximate actual array beam patterns. The antenna gain of

the transmission link from AP j to user i is

fijt =

G ∠BjCjDit ≤ φ
2 ,




g

otherwise,

∀i, j, t,

(2)

where ∠BjCjDit represents user i’s tilt angle towards AP j, the location of the point ‘Bj’ can
j = [xj, yj]T and θj, the point ‘Dit’ represent user
be determined by AP j’s 2D coordinate v2D



i’s position, the point ‘Cj’ denotes the position of AP j’s antenna.

For any AP j, the 2D coordinate x2D

bj = [xbj, ybj]T of point ‘Bj’ can be given by

xbj = dj(xo − xj)/rj + xj, ∀j,

ybj = dj(yo − yj)/rj + yj, ∀j,

(3)

(4)

where dj = Hj/ tan(θj), rj = ||xo − v2D

j

||2, and xo = [xo, yo]T is 2D coordinate of the center

point of the considered communication area.

Then, user i’s tilt angle towards AP j can be written as

−−−→
CjBj ·

−−−→
CjDit

∠BjCjDit = arccos

−−−→
CjBj = (xbj −xj, ybj −yj, −Hj) and

kCjBjk2kCjDitk2 !
−−−→
CjDit = (xit −xj, yit −yj, hi −Hj).

, ∀i, j, t,

(5)

where direction vectors

A mmWave link may be blocked if a user turns around; this is because the user wears an

HMD in front of his/her forehead. Denote ϑ as the maximum angle within which an AP can

experience LoS transmission towards its downlink associated users. For user i at time slot t, an

indicator variable bijt introduced to indicate the blockage effect of user i’s body is given by

1 ∠( ~Ajit, ~xit) > ϑ,

0

otherwise,

∀i, j, t,

bijt =






(6)

 
8

where ∠( ~Ajit, ~xit) represents the orientation angle of user i at time slot t, which can be deter-
mined by locations of both user i and AP j,1 ~xit = (xit − xit−1, yit − yit−1) is a direction vector.
When t = 1, the direction vector ~xi1 = (xi1, yi1). ~Ajit = (xj − xit, yj − yit) is a direction vector
between the AP j and user i.

Given ~Ajit and ~xit, we can calculate the orientation angle of user i that is also the angle

between ~Ajit and ~xit by

∠( ~Ajit, ~xit) = arccos

~Ajit · ~xit
|| ~Ajit||2||~xit||2 !

, ∀i, j, t.

(7)

The channel gain coefﬁcient hijkt of an LoS link and a non line-of-sight (NLoS) link between

the k-th antenna element of AP j and user i at time slot t can take the form [23]

10log10(hijkthH

ijkt) =

10ηLoSlog10(dijt(x3D

it , v3D

j ))+20log10

10log10fijt + µLoS
k
it , v3D

10ηNLoSlog10(dijt(x3D

,

(cid:0)

j ))+20log10

10log10fijt + µNLoS

k

,

4πfc
c

+

(cid:1)
4πfc
c

+

(cid:0)

(cid:1)

bijt = 0

bijt = 1

∀i, j, k, t,

(8)






where fc (in Hz) is the carrier frequency, c (in m/s) the light speed, ηLoS (in dB) and ηNLoS (in
dB) the path-loss exponents of LoS and NLoS links, respectively, µLoS ∼ CN (0, σ2
and µNLoS ∼ CN (0, σ2

LoS) (in dB)

NLoS) (in dB).

For any user i, to satisfy its immersive experience requirement, its downlink achievable data
it ) from cooperative APs should be no less than a data rate threshold γth, i.e.,

rate (denoted by rdl

it ≥ γth, ∀i, t.
rdl

(9)

Deﬁne adl

requirement can be satisﬁed at time slot t. adl

it ∈ {0, 1} as an association variable indicating whether the user i’s data rate
it = 1 indicates that its data rate requirement
it = 0. Then, for any user i at time slot t, according to Shannon

can be satisﬁed; otherwise, adl

capacity formula and the principle of CoMP transmission, we can calculate rdl

it by

it = W dllog2
rdl

j∈J hH
N0W dl + I dl
P
it
where hijt = [hij1t, . . . , hijKt]T ∈ CK is a channel gain coefﬁcient vector with K denoting the
number of antenna elements, gijt ∈ CK is the transmit beamformer pointed at user i from AP

, ∀i, t,

adl
it |

(10)

1 +

!

ijtgijt|2

1In this paper, we consider the case of determining users’ orientation angles via the locations of both APs and users. Certainly,

our proposed learning method is also applicable to scenarios where users’ orientation angles need to be predicted.

 
 
9

j, W dl represents the downlink system bandwidth. Owing to the directional propagation, for

user i, not all users will be its interfering users. It is regarded that users whose distances from

user i are small than Dth will be user i’s interfering users, where Dth is determined by antenna

conﬁguration of APs (e.g., antenna height and downtilt angle). Denote the set of interfering users

of user i at time slot t by Mit, then, we have I dl

it =

m∈Mit adl
mt|

j∈J hH

mjtgmjt|2.

B. Feeling of presence model

P

P

In VR applications, FoP represents an event that does not drag users back from engaging and

immersive ﬁctitious environments [24]. For wireless VR, the degrading FoP can be caused by

the collection of inaccurate users’ tracking information via APs and the reception of low-quality

VR video frames. Therefore, we consider the uplink user tracking information transmission and

downlink VR video delivery when modelling the FoP experienced by users. Mathematically,

over a period of time slots, we model the FoP experienced by users as the following

¯B(T ) =

T

1
T
t=1
(cid:0)
(cid:0)
(cid:1)
aul
ijt with aul
t = [aul
11t, . . . , aul

Bul
t

aul
t

X

+ Bdl
t

adl
t

,

(cid:1)(cid:1)
(cid:0)
N Jt]T, Bdl
ijt, . . . , aul
t

where Bul
t

with adl

t = [adl
(cid:0)

aul
t

= 1
N
i∈U
j∈J
it , . . . , adl
1t, . . . , adl
P
P

(cid:1)

N t]T.

(11)

adl
it

i∈U
P

adl
t

= 1
N

(cid:0)

(cid:1)

C. Power consumption model

HMDs are generally battery-driven and constrained by the maximum instantaneous power. For

any user i’s HMD, deﬁne ptot

it as its instantaneous power consumption including the transmit
power and circuit power consumption (e.g., power consumption of mixers, frequency synthesiz-

ers, and digital-to-analog converters) at time slot t, we then have

ptot
it ≤ ˜pi, ∀i, t,

(12)

where ptot

it = pit + pc

i denotes the HMD’s circuit power consumption during a time slot, and
˜pi is a constant. Without loss of generality, we assume that all users’ HMDs are homogenous.

i , pc

The instantaneous power consumption of each AP is also constrained. As CoMP transmission

technique is explored, for any AP j, we can model its instantaneous power consumption at time

slot t as the following

i∈U

it gH
adl

ijtgijt + Ec

j ≤ ˜Ej, ∀j, t,

(13)

where Ec

j is a constant representing the circuit power consumption, ˜Ej is the maximum instan-

X

taneous power of AP j.

10

D. Objective function and problem formulation

To guarantee immersive and interactive VR experiences of users over a period of time slots,

uplink user data packets should be successfully decoded, and downlink data rate requirements of

users should be satisﬁed at each time slot; that is, users’ FoP should be maximized. According

to (1) and (11), one might believe that increasing the transmit power of users’ HMDs would

be an appropriate way of enhancing users’ FoP. However, as users’ HMDs are usually powered

by batteries, they are encouraged to work in an energy-efﬁcient mode to prolong their working

duration. Further, reducing HMDs’ power consumption indicates less heat generation, which can

enhance users’ VR experiences. Therefore, our goal is to maximize users’ FoP while minimizing

the power consumption of HMDs over a period of time slots. Combining with the above analysis,

we can formulate the problem of enhancing users’ immersive experiences as below

maximize
t ,adl

t ,pt,gijt}

{aul

lim inf
T →∞

1
T

T

t=1
X

s.t.

aul
ijt ≤ 1, ∀i, t

j∈J

X

i∈U

ijt ≤ ˜M, ∀j, t
aul

X
aul
ijt ∈ {0, 1}, ∀i, j, t

adl
it ∈ {0, 1}, ∀i, t

0 ≤ pit ≤ ˜pi − pc

i , ∀i, t

(1), (9), (13),

where pt = [p1t, p2t, . . . , pN t]T.

Bul
t

aul
t

+ Bdl
t

adl
t

−

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:1)(cid:1)

1
T

T

t=1
X

j∈J
i∈U X
X

ijtptot
aul

it /˜pi

(14a)

(14b)

(14c)

(14d)

(14e)

(14f)

(14g)

However, the solution to (14) is highly challenging due to the unknown users’ tracking

information at each time slot. Given users’ tracking information, the solution to (14) is still

NP-hard or even non-detectable. It can be conﬁrmed that (14) is a mixed-integer non-linear

programming (MINLP) problem as it simultaneously contains zero-one variables, continuous

variables, and non-linear constraints. Further, we can know that (9) and (13) are non-convex

with respect to (w.r.t) adl

it and gijt, ∀i, j, by evaluating the Hessian matrix. To tackle the tricky
problem, we develop a novel solution framework as depicted in Fig. 3. In this framework, we

ﬁrst propose to predict users’ tracking information using a machine learning method. With the

predicted results, we then develop a DRL-based optimization algorithm to solve the MINLP

problem. The procedure of solving (14) is elaborated in the following sections.

11

Input 

ESN model 

Ou
Output 

Master VM 

Aggregation: 

(cid:3045)
(cid:2179)(cid:2919)(cid:2924)

(cid:2179)(cid:3047)
(cid:2179)(cid:3047)

(cid:3045)
(cid:2179)(cid:2928)
Global 
model 
update 

Compute 

,
,

(cid:2179)(cid:2869)
(cid:3047)
(cid:3047)(cid:2179)(cid:2869) (cid:3047)(cid:2179)(cid:2869) (cid:3047)
(cid:2179)(cid:2869)

Slave VM 1 

Local dataset 

;

(cid:3045)
(cid:2179)(cid:2869)

(cid:2919)(cid:2924)

;

(cid:3045)
(cid:2179)(cid:2869)

(cid:2928)

Local model update 

Compute 

,
,

(cid:3047)(cid:2179)(cid:3011)(cid:2179)(cid:2179) (cid:3047)
(cid:2179)(cid:3011)(cid:2179)(cid:2179)
(cid:2179)(cid:3011)
(cid:3047)

Slave VM J 

Local dataset 

;
(cid:3045)
(cid:2179)(cid:3037)

(cid:2919)(cid:2924)

;
(cid:3045)
(cid:2179)(cid:3011)

(cid:2928)

Local model update 

VR scenario 

(cid:20)(cid:3)

Network information (tracking, association & transmit power) 

Fresh data 

Historical data 

(cid:21)(cid:3)

Learning 
Users’ location prediction 

ESN 

(cid:23)(cid:3)

Resource 
allocation 

DRL-based optimization algorithm 

(cid:22)(cid:3)

State 

Policy 

Actor 

Critic 
Optimization 

Reward 
Environment 

Action 

Fig. 3. Working diagram of a framework of solving (14).

Fig. 4. Architecture of the parallel ESN learning method.

III. USERS’ LOCATION PREDICTION

As analyzed above, the efﬁcient user-AP association and transmit power of both HMDs and

APs are conﬁgured on the basis of the accurate perception of users’ tracking information. If the

association and transmit power are identiﬁed without knowledge of users’ tracking information,

users may have degrading VR experiences, and the working duration of users’ HMDs may be

dramatically shortened. Meanwhile, owing to the stringent low latency requirement, the user-AP

association and transmit power should be proactively determined to enhance users’ immersive and

interactive VR experiences. Hence, APs must collect fresh and historical tracking information for

users’ tracking information prediction in future time slots. With predicted tracking information,

the user-AP association and transmit power can be conﬁgured in advance. Certainly, from (7),

we observe that users’ orientation angles can be obtained by their and APs’ locations; thus,

we only predict users’ locations in this section. Machine learning is convinced as a promising

proposal to predict users’ locations. In machine learning methods, the accuracy and completeness

of sample collection are crucial for accurate model training. However, the user-AP association

may vary with user movement, which indicates that location information of each user may scatter

in multiple APs, and each AP may only collect partial location information of its associated users

after a period of time. To tackle this issue, we develop a parallel machine learning method, which

exploits J slave virtual machines (VMs) created in the CU to train learning models for each user,

as shown in Fig. 4. Besides, for each AP, it will feed its locally collected location information

to a slave VM for training. In this way, the prediction process can also be accelerated. With the

predicted results, the CU can then proactively allocate system resources by solving (14).

 
 
 
 
 
 
 
 
 
12

A. Echo state network

In this section, the principle of echo state network (ESN) is exploited to train users’ location

prediction model as the ESN method can efﬁciently analyze the correlation of users’ location

information and quickly converge to obtain users’ predicted locations [25]. It is noteworthy

that there are some differences between the traditional ESN method and the developed parallel

ESN learning method. The traditional ESN method is a centralized learning method with the

requirement of the aggregation of all users’ locations scattered in all APs, which is not required

for the parallel ESN learning method. What’s more, the traditional ESN method can only be

used to conduct data prediction in a time slot while the parallel ESN learning method can

predict users’ locations in M > 1 time slots. An ESN is a recurrent neural network that can be

partitioned into three components: input, ESN model, and output, as shown in Fig. 4. For any
user i ∈ U, the Ni-dimensional input vector xit ∈ RNi is fed to an Nr-dimensional reservoir
whose internal state si(t−1) ∈ RNr is updated according to the state equation

sit = tanh

W r

inxit + W r

r si(t−1)

,

(15)

where W r

in ∈ RNr×Ni and W r
element locating in the interval (0, 1).

(cid:1)
r ∈ RNr×Nr are randomly generated matrices with each matrix

(cid:0)

The evaluated output of the ESN at time slot t is given by

ˆyi(t+1) = W o

inxit + W o

r sit,

(16)

where W o

in ∈ RNo×Ni, W o

r ∈ RNo×Nr are trained based on collected training data samples.

To train the ESN model, suppose we are provided with a sequence of Q desired input-outputs
pairs {(xi1, yi1), . . . , (xiQ, yiQ)} of user i, where yit ∈ RNo is the target location of user i at
time slot t. Deﬁne the hidden matrix Xit as

Xit =



xi1

si1

· · ·

xiQ
siQ 


.

(17)

The optimal output weight matrix is then achieved by solving the following regularized least-



square problem

W ⋆

t =

arg min
Wt∈R(Ni+Nr )×No

1
Q

l

X T

it Wt

+ ξr(Wt)

(18)

where Wt = [W o
function l(X T

inW o
it Wt) = 1

matrix Yit = [yT

i1; . . . ; yT

(cid:0)

(cid:1)
r ]T, ξ ∈ R+ is a positive scalar known as regularization factor, the loss
2||X T
F , and the target location
iQ] ∈ RQ×No.

F , the regulator r(Wt) = ||Wt||2

it Wt − Yit||2

13

B. Parallel ESN learning method for users’ location prediction

Based on the principle of the ESN method, we next elaborate on the procedure of the parallel

ESN learning method for users’ location prediction. To facilitate the analysis, we make the

following assumptions on the regulator and the loss function.

Assumption 1. The function r : Rm×n → R is ζ-strongly convex, i.e., ∀i ∈ {1, 2, . . . , n}, ∀X,
and ∆X ∈ Rm×n, we have [26]

r(X + ∆X) ≥ r(X) + ∇r(X) ⊙ ∆X + ζ||∆X||2

F /2,

(19)

where ∇r(·) denotes the gradient of r(·).

Assumption 2. The function l : R → R are 1

µ -smooth, i.e., ∀i ∈ {1, 2, . . . , n}, ∀x, and ∆x ∈ R,

we have

l(x + ∆x) ≤ l(x) + ∇l(x)∆x + (∆x)2/2µ,

(20)

where ∇l(·) represents the gradient of l(·).

According to Fenchel-Rockafeller duality, we can formulate the local dual optimization prob-

lem of (18) in the following way.

Lemma 1. For a set of J slave VMs and a typical user i, the dual problem of (18) can be

written as follows

where

maximize
A∈RQ×No (

−ξr⋆

1
ξQ

(cid:18)

ATX T

−

(cid:19)

1
Q

Q

No

m=1
X

n=1
X

l⋆(−amn)

)

r⋆(C) =

1
4

No

n=1

n CC Tzn,
zT

X
l⋆(−amn) = −amnymn + a2

mn/2,

(21)

(22)

(23)

A ∈ RQ×No is a Lagrangian multiplier matrix, zn ∈ RNo is a column vector with the n-th

element being one and all other elements being zero, X is a lightened notation of Xit =

xi(t−1)

xi(t−Q)

· · ·

si(t−1)


si(t−Q)
location of the m-th row and the n-th column.






, and ymn is an element of matrix Y = [yT

it; . . . ; yT

i(t−Q+1)] at the

Proof. Please refer to Appendix A.

Denote the objective function of (21) as D(A), and deﬁne V (A) := 1

ξQ(XA)T ∈ RNo×(Ni+Nr),

we can then rewrite D(A) as

14

D(A) = −ξr⋆(V (A)) −

J

j=1

Rj(A[j]),

(24)

No

l⋆(−amn), A[j] = ˆZjA, and ˆZj ∈ RQ×Q is a square matrix with

X

where Rj(A[j]) = 1
Q

m∈Qj
P

n=1
P

J × J blocks. In ˆZj, the block in the j-th row and j-th column is a qj × qj identity matrix with

qj being the cardinality of a set Qj and all other blocks are zero matrices, Qj is an index set

including the indices of Q data samples fed to slave VM j.

Then, for a given matrix At, varying its value by ∆At will change (24) as below

D(At + ∆At) = −ξr⋆(V (At + ∆At)) −

J

j=1 Rj(At

[j] + ∆At

[j]),

(25)

where ∆At

[j] = ˆZj∆At.

P

Note that the second term of the right-hand side (RHS) of (25) includes the local changes of

each VM j, while the ﬁrst term involves the global variations.

As r(·) is ζ-strongly convex, r⋆(·) is then 1

ζ -smooth [26]. Thus, we can calculate the upper

bound of r⋆(V (At + ∆At)) as follows

No

r⋆(V (At + ∆At)) ≤ r⋆ (V (At)) + 1
ξQ

n ∇r⋆(V (At))X∆Atzn + κ
zT

= r⋆ (V (At)) + 1
ξQ

J

No

n=1
P
n ∇r⋆(V (At))X[j]∆At
zT

[j]zn + κ

2(ξQ)2

No

kX∆Atznk2

J

No

2(ξQ)2

n=1
P
X[j]∆At

2

,

[j]zn

j=1
P

n=1
P
where X[j] = X ˆZj, κ > 1
ζ is a data dependent constant measuring the difﬁculty of the partition
to the whole samples.

n=1
P

j=1
P

(26)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

By substituting (26) into (25), we obtain

D(At + ∆At) ≥ −ξr⋆ (V (At)) − 1
Q

n ∇r⋆(V (At))X[j]∆At
zT

[j]zn

J

No

− κ
2ξQ2

J

No

j=1
P

n=1
P

n=1
P
[j]zn

j=1
P
X[j]∆At
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

j=1
P

J

2

−

Rj(At

[j] + ∆At

[j]).

(27)

From (27), we observe that the problem of maximizing D(At + ∆At) can be decomposed

into J subproblems, and J slave VMs can then be exploited to optimize these subproblems

separately. If slave VM j can optimize ∆At using its collected data samples by maximizing the

RHS of (27), the resultant improvements can be aggregated to drive D(At) toward the optimum.

The detailed procedure is described below.

As shown in Fig. 4, during any communication round t, a master VM produces V (At) using

updates received at the last round and shares it with all slave VMs. The task at any slave VM

j is to obtain ∆At

[j] by maximizing the following problem

15

∆At⋆

[j] = arg max
[j]∈RQ×No
∆At

= arg max

−Rj

∆Dj

∆At

[j]; V (At), At
[j]

(cid:16)
[j] + ∆At
[j]

At

(cid:17)
J r⋆(V (At)) − 1

Q

− ξ

∆At

[j]∈RQ×No
No

κ
2ξQ2

n

(cid:16)
X[j]∆At
[j]zn
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

n=1
P

(cid:17)

2

.

(cid:27)

n ∇r⋆(V (At))X[j]∆At
zT

[j]zn−

No

n=1
P

(28)

Calculate the derivative of ∆Dj

∆At

[j]; V (At), At
[j]

over ∆At

[j], and force the derivative

result to be zero, we have

(cid:16)

(cid:17)

∆At⋆

[j] =

where Y[j] = ˆZjY .

ξQX T

[j]X[j]

ˆZj + κ
(cid:16)

−1

(cid:17)

(cid:16)

Y[j] − At

[j] − 1

2X T

[j]V T(At)

,

(cid:17)

(29)

Next, slave VM j, ∀j, sends ∆V t

ξQ(X[j]∆At⋆
updates the global model as V (At + ∆At) = V (At) +

[j])T to the master VM. The master VM
[j]. Finally, alteratively update
j=1 on the global and local sides, respectively. It is expected that the solution
P

V (At) and {∆At⋆

j=1 ∆V t

[j] = 1

[j]}J

J

to the dual problem can be enhanced at every step and will converge after several iterations.

At time slot t, based on the above derivation, the parallel ESN learning method for predicting

locations of user i, ∀i, in M time slots can be summarized in Algorithm 1.

IV. DRL-BASED OPTIMIZATION ALGORITHM

Given the predicted locations of all users, it is still challenging to solve the original problem

owing to its non-linear and mixed-integer characteristics. Alternative optimization is extensively

considered as an effective scheme of solving MINLP problems. Unfortunately, the popular

alternative optimization scheme cannot be adopted in this paper. This is because the alternative

optimization scheme is of often high computational complexity, and the original problem is

also a sequential decision problem requiring an MINLP problem to be solved at each time slot.

Remarkably, calling an optimization scheme with a high computational complexity at each time

slot is unacceptable for latency-sensitive VR applications.

Reinforcement learning methods can be explored to solve sequential decision problems. For

example, the works in [27], [28] proposed reinforcement learning methods to solve sequential

decision problems with a discrete decision space and a continuous decision space, respectively.

Algorithm 1 Parallel ESN learning for user location prediction

16

1: Initialization: Data samples of all slave VMs. For any slave VM j, it randomly initiates a
[j])T from all slave VMs,
[j])T, and then share the model V (A0) with all slave

[j] ∈ RQ×No. The master VM collects 1

generates V (A0) =

starting point A0

ξQ(X[j]A0

ξQ(X[j]A0

J
j=1

1

VMs. Let κ = J/ζ.

P
2: for r = 0 : ¯rmax − 1 do

3:

4:

5:

6:

7:

for each slave VM j ∈ {1, 2, . . . , J} in parallel do

Calculate ∆Ar⋆

[j] using (29), update and store the local Lagrangian multiplier

Ar+1

[j] = Ar

[j] + ∆Ar⋆

[j]/(r + 1).

Compute the following local model and send it to the master VM

end for

∆V r

[j] =

X[j]∆Ar⋆
[j]

T

/ξQ.

(cid:0)

(cid:1)

Given local models, the master VM updates the global model as

V (Ar+1) = V (Ar) +

J

j=1

∆V r
[j],

and then share the updated global model V (Ar+1) with all slave VMs.

X

(30)

(31)

(32)

8: end for

9: Let W T = ∇r⋆(V (Ar)) and predict user i’s location ˆyit by (16). Meanwhile, by iteratively

assigning xi(t+1) = ˆyit, each user i’s locations in M time slots can be obtained.

10: Output: The predicted locations of user i, ˆYit = [ ˆyT

i(t+1); . . . ; ˆyT

i(t+M )], ∀i.

However, how to solve sequential decision problems simultaneously involving discrete and

continuous decision variables (e.g., the problem (14)) is a signiﬁcant and understudied problem.

In this paper, we propose a deep reinforcement learning (DRL)-based optimization algorithm

to solve (14). Speciﬁcally, we design a DNN joint with an action quantization scheme to

produce a set of association actions of high diversity. Given the association actions, a continuous

optimization problem is solved to criticize them and optimize the continuous variables. The

detailed procedure is presented in the following subsections.

A. Vertical decomposition

17

Deﬁne a vector git = [gi1t; . . . ; gijt; . . . ; giJt] ∈ CJK and a vector hit = [fi1thi1t; . . . ; fijthijt;
it. As tr(AB) =
tr(BA) for matrices A and B of compatible dimensions, the signal power received by user

. . . ; fiJthiJt] ∈ CJK, ∀i, t. Let matrix Git = gitgT

it and matrix Hit = hithT

itgitgT

tr(hithT

j∈J fithT

i ∈ U can be expressed as |

itgit|2 =
itgit) =
it) = tr(HitGit). Likewise, by introducing a square matrix Zj ∈ RJK×JK with
ijtgijt = tr(ZjGit).
Besides, each block in Zj is a K × K matrix. In Zj, the block in the j-th row and j-th column

J × J blocks, the transmit power for serving users can be written as gT

itgit = tr(gT

ithithT

itgit

hT

P

(cid:0)

(cid:1)

itgijt|2 = |hT

ThT

is a K ×K identity matrix, and all other blocks are zero matrices. Then, by applying Git = gitgT
it

⇔ Git (cid:23) 0 and rank(Git) ≤ 1, we can convert (14) to the following problem
T

ijtptot
aul

it /˜pi

(33a)

maximize
t ,adl

t ,pt,Git}

{aul

¯B(T ) −

1
T

s.t.

log2

1 +

N0W dl +

mttr(HmtGmt) !

t=1
j∈J
i∈U
X
X
X
adl
it tr(HitGit)
m∈Mit adl

it tr(ZjGit) + ˜Ej ≤ Ej, ∀j, t
P
adl

i∈U

X
Git (cid:23) 0, ∀i, t

rank(Git) ≤ 1, ∀i, t

(1), (14b) − (14f).

≥ γth/W dl, ∀i, t

(33b)

(33c)

(33d)

(33e)

(33f)

Like (14), (33) is difﬁcult to be directly solved; thus, we ﬁrst vertically decompose it into the

following two subproblems.

• Uplink optimization subproblem: The uplink optimization subproblem is formulated as

maximize
t ,pt}

{aul

1
T

T

t=1  
X

(cid:1)
(1), (14b) − (14d), (14f).

(cid:0)

s.t.

Bul
t

aul
t

−

ijtptot
aul

it /˜pi

!

j∈J
i∈U X
X

(34a)

(34b)

• Downlink optimization subproblem: The downlink optimization subproblem can be formu-

lated as follows

maximize
t ,Git}

{adl

T

Bdl
t

adl
t

1
T

t=1
X
(14e), (33b) − (33e).

(cid:0)

s.t.

(cid:1)

(35a)

(35b)

Next, we propose to solve the two subproblems separately by exploring DRL approaches.

 
18

,
(cid:2931)(cid:2922)
(cid:3047)
(cid:1870)(cid:3049)
Decision 
selection 

,

Output 
decisions at 
time slot t 

(cid:52)(cid:88)(cid:68)(cid:81)(cid:87)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)

Map 

:  
(cid:3)

(cid:54)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)

arg max

(cid:2879)(cid:2869)

(cid:1858)(cid:4634)
(cid:1372) (cid:1337)

(cid:3015)

(cid:1337)

;
(cid:2931)(cid:2922)
(cid:2869)
(cid:2183)(cid:3549)(cid:3047)
(cid:39)(cid:76)(cid:89)(cid:72)(cid:85)(cid:86)(cid:76)(cid:87)(cid:92)(cid:3)

Map 

:  

(cid:3)

(cid:1858)(cid:4634)
(cid:1372) (cid:1337)

(cid:3015)

(cid:3015)(cid:3011)

(cid:1337)

(cid:2931)(cid:2922)
(cid:2183)(cid:3365)(cid:3047)

Continuous 
actions 

;
(cid:2931)(cid:2922)
(cid:3023)(cid:3561)
(cid:2183)(cid:3549)(cid:3047)

(cid:3015)(cid:3011)

;
(cid:2931)(cid:2922)
(cid:2869)
(cid:2183)(cid:3557)(cid:3047)

;
(cid:2931)(cid:2922)
(cid:3023)(cid:3561)
(cid:2183)(cid:3557)(cid:3047)

(cid:3049)

Compute 

 by 
,
solving 
(cid:2931)(cid:2922)
(cid:1870)(cid:3049)
(cid:3047)
(33) 

(cid:2931)(cid:2922)
(cid:2183)(cid:3047)
(cid:53)(cid:72)(cid:83)(cid:79)(cid:92)(cid:3)(cid:80)(cid:72)(cid:80)(cid:82)(cid:85)(cid:92)(cid:3)

(cid:2198)(cid:3047)

(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)

(cid:53)(cid:68)(cid:81)(cid:71)(cid:82)(cid:80)(cid:3)
(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)

(cid:39)(cid:49)(cid:49)(cid:3)

(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:69)(cid:68)(cid:87)(cid:70)(cid:75)(cid:3)

Policy update 

Action and 
reward 

(cid:42)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:72)(cid:3)
(cid:86)(cid:87)(cid:68)(cid:87)(cid:72)(cid:3)(cid:86)(cid:83)(cid:68)(cid:70)(cid:72)(cid:3)

Channels 

(cid:1860)(cid:3047)

Fig. 5. A DRL approach of association and transmit power optimization.

B. Solution to the uplink optimization subproblem

(34) is conﬁrmed to be a mixed-integer and sequence-dependent optimization subproblem.

Fig. 5 shows a DRL approach of solving (34). In this ﬁgure, a DNN is trained to produce

continuous actions. The continuous actions are then quantized into a group of association (or

discrete) actions. Given the association actions, we solve an optimization problem to select an

association action maximizing the reward. Next, we describe the designing process of solving

(34) using a DRL-based uplink optimization method in detail.

1) Action, state, and reward design: First, we elaborate on the design of the state space,

action space, and reward function of the DRL-based method. The HMDs’ transmit power and

the varying channel gains caused by users’ movement and/or time-varying wireless channel en-

vironments have a signiﬁcant impact on whether uplink transmission signals can be successfully

decoded by APs. In addition, each AP has a limited ability to decode uplink transmission signals

simultaneously. Therefore, we design the state space, action space, and reward function of the

DRL-based method as the following.
t = [mt; ˆhul

• state space sul

t : sul

t ; pt] is a column vector, where mjt ∈ mt ∈ RJ , ∀j, denotes
the number of users successfully access to AP j at time slot t. Besides, the state space
involves the path-loss from user i to AP j, ˆhijt ∈ ˆhul
t ∈ RN J , ∀i, j, t, and the transmit
power of user i’s HMD at time slot t, pit ∈ pt ∈ RN , ∀i, t.

 
  
 
 
 
 
 
 
• action space aul

1Jt, . . . , aul
the DRL-based method is to deliver users’ data information to associated APs.

N Jt]T ∈ RN J with aul

11t, . . . , aul

t = [aul

t : aul

ijt ∈ {0, 1}. The action of

19

• reward rul

t : given aul

t , the reward rul
t

is the objective function value of the following power

control subproblem.

rul
t = maximize

pt

s.t.

(1), (14f).

Bul

t (aul

t ) −

ijtptot
aul

it /˜pi

i∈U

j∈J

X

X

(36a)

(36b)

2) Training process of the DNN: For the DNN module ¯aul

t = µ(sul

t |θµ

t ) shown in Fig. 5,

where ¯aul

t = [¯aul

1t; . . . ; ¯aul

N t] and θµ

t represents network parameters, we explore a two-layer fully-

connected feedforward neural network with network parameters being initialized by a Xavier
initialization scheme. There are N µ

2 neurons in the 1st and 2nd hidden layers of the
constructed DNN, respectively. Here, we adopt the ReLU function as the activation function in

1 and N µ

these hidden layers. For the output layer, a sigmoid activation function is leveraged such that

relaxed association variables satisfy ¯aul

ijt ∈ (0, 1). In the action-exploration phase, the exploration
noise ǫNf is added to the output layer of the DNN, where ǫ ∈ (0, 1) decays over time and

Nf ∼ N (0, σ2).

To train the DNN effectively, the experience replay technique is exploited. This is because

there are two special characteristics in the process of enhancing users’ ﬁctitious experiences: 1)

the collected input state values sul
t

incrementally arrive as users move to new positions, instead

of all made available at the beginning of the training; 2) APs consecutively collect state values

indicating that the collected state values may be closely correlated. The DNN may oscillate

or diverge without breaking the correlation among the input state values. Speciﬁcally, at each

training epoch t, a new training sample (sul

t , sul
t+1) is added to the replay memory. When the
memory is ﬁlled, the newly generated sample replaces the oldest one. We randomly choose a

t , aul

minibatch of training samples {(sul
set of training epoch indices. The network parameters θµ

τ +1)|τ ∈ Tt} from the replay memory, where Tt is a
t are trained using the ADAM method

τ , aul

τ , sul

[29] to reduce the averaged cross-entropy loss

L(θµ

t ) = − 1
|Tt|

τ ∈Tt((aul

τ )T log ¯aul

τ + (1 − aul

τ )T log(1 − ¯aul

τ )).

(37)

As evaluated in the simulation, we can train the DNN every Tti epochs after collecting a

P

sufﬁcient number of new data samples.

20

3) Action quantization and selection method: In the previous subsection, we design a contin-

uous policy function and generate a continuous action space. However, a discrete action space

is required in this paper. To this aim, the generated continuous action should be quantized, as

shown in Fig. 5. A quantized action will directly determine the feasibility of the optimization

subproblem and then the convergence performance of the DRL-based optimization method. To

improve the convergence performance, we should increase the diversity of the quantized action

set, which including all quantized actions. Speciﬁcally, we quantize the continuous action ¯aul
t
to obtain ˜V ∈ {1, 2, . . . , 2N } groups of association actions and denote by ¯aul
t;v the v-th group
of actions. Given ¯aul
it;v, (36) is reduced to a linear programming problem, and we can derive its

closed-form solution as below

ijtθthN0W ul
aul
N fiˆhijt

,

ijtθthN0W ul
aul
N fiˆhijt

j
P
otherwise.

≤ ˜pi − pc
i ,

pit = 


j
P
0,

(38)

Besides, a great ˜V will result in higher diversity in the quantized action set but a higher



computational complexity, and vice versa. To balance the performance and complexity, we set
˜V = N and propose a lightweight action quantization and selection method. The detailed steps

of quantizing and selecting association actions are given in Algorithm 2.

Summarily, the proposed DRL-based uplink optimization method can be presented in Algo-

rithm 3.

C. Solution to the downlink optimization subproblem

Like (34), (35) is also a mixed-integer and sequence-dependent optimization problem. There-

fore, the procedure of solving (35) is similar to that of solving (34), and we do not present the

detailed steps of the DRL-based downlink optimization method in this subsection for brevity.

However, there are differences in some aspects, for example, the design of action and state space

and the reward function. For the DRL-based downlink optimization method, we design its action

space, state space, and the reward function as the following.

• state space sdl

t : sdl

t = [ot; ht; I dl

t ; gt] is a column vector, where ojt ∈ ot ∈ RJ indicates
the number of users to which AP j transmits VR video frames, hijkt ∈ ht ∈ CN JK, Iimt ∈
t denotes whether user m is the interfering user of user i, and gijkt ∈ gt ∈ CN JK.
RN ×N ∈ I dl
N t]T with adl
it ∈ {0, 1}. The action of the DRL-

it , . . . , adl

1t, . . . , adl

t = [adl

t : adl

• action space adl

based method at time slot t is to transmit VR video frames to corresponding users.

Algorithm 2 Action quantization and selection

1: Input: The output action of the uplink DNN, ¯aul
t .

2: Arrange ¯aul

t as a matrix of size N × J and generate a vector ˆaul

t =

max[¯aul

i1t, . . . , ¯aul

iJt], ∀i

21

.

(cid:9)
3: Generate the reference action vector ¯bt = [¯b1t, . . . , ¯bvt, . . . , ¯b ˜V t]T by sorting the absolute

(cid:8)

value of all elements of ˆaul
t

in ascending order.

4: For any user i, generate the 1st group of association actions by

ˆaul
it;1 =




1, ˆaul

it > 0.5,

0, ˆaul

it ≤ 0.5.

5: For any user i, generate the remaining ˜V − 1 groups of association actions by



ˆaul
it;v =




1, ˆaul

0, ˆaul

it > ¯b(v−1)t, v = 2, . . . , ˜V ,
it ≤ ¯b(v−1)t, v = 2, . . . , ˜V .

6: For each group of association actions v ∈ {1, 2, . . . , ˜V }, user i, and AP j, set



˜aul
ijt;v =

where, j⋆ = arg max

[¯aul

i1t, . . . , ¯aul

iJt].

j

1, ˆaul

it;v = 1, j = j⋆,
otherwise.

0,






(39)

(40)

(41)

7: For each group of association actions v ∈ {1, 2, . . . , ˜V }, given the vector ˜aul

t;v =

[˜aul

i1t;v, . . . , ˜aul

iJt;v]T
8: Select the association action aul

i , ∀i, solve (36) to obtain rul
vt.
ijt;v} rul
vt.
t = arg max{˜aul

9: Output: The association action aul
t .

• reward rdl

t : given adl

t , the reward rdl
t

is the objective function value of the following power

control subproblem.

rdl
t = maximize

Git

Bdl
t

adl
t

s.t.

(cid:0)
(33b) − (33e).

(cid:1)

(42a)

(42b)

To solve (42), Algorithm 2 can be adopted to obtain the downlink association action adl
t .
t , it is still hard to solve (42) as (42) is a non-convex programming problem
with the existence of the non-convex low-rank constraint (33e). To handle the non-convexity, a

However, given adl

Algorithm 3 DRL-based uplink optimization

1: Initialize: The maximum number of episodes Nepi, the maximum number of epochs per

22

episode Nepo, initial exploration decaying rate ǫ, DNN µ(sul
θµ
t , initial reward rul

0 = 1, and users’ randomly initialized transmit power.

t |θµ

t ) with network parameters

2: Initialize: Replay memory with capacity C, minibatch size |Tt|, and DNN training interval

Tti.

3: for each episode in {1, . . . , Nepi} do

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

Calculate the state space according to locations of APs and users and users’ randomly

initialized transmit power.
for each epoch ¯t = 1, . . . , Nepo do
Select a relaxed action vector ¯aul

¯t = µ(sul

¯t |θµ

¯t ) + ǫNf , where ǫ decays over time.

Call Algorithm 2 to choose the association action aul
¯t .

if aul
¯t

results in the violation of constraints in (34) then

Cancel the action and update the reward by rul

¯t = rul

¯t − ̟|rul

¯t−1|.

else

Execute the action and observe the subsequent state sul

¯t+1.

end if

Store the transition (sul
If ¯t ≥ |Tt|, sample a random minibatch of |Tt| transitions (sul

¯t+1) in the memory.

¯t , aul

¯t , sul

m, aul

m, sul

m+1) from the

memory.
If ¯t mod Tti == 0, update the network parameters θµ
L(θµ

¯t ) using the ADAM method.

¯t by minimizing the loss function

16:

end for

17: end for

semideﬁnite relaxation (SDR) scheme is exploited. The idea of the SDR scheme is to directly

drop out the non-convex low-rank constraint. After dropping the constraint (33e), it can conﬁrm

that (42) becomes a standard convex semideﬁnite programming (SDP) problem. This is because

(33b) are (33c) are linear constraints w.r.t Git and (42a) is a constant objective function. We can

then explore some optimization tools such as MOSEK to solve the standard convex SDP problem

effectively. However, owing to the relaxation, power matrices {Git} obtained by mitigating (42)

without low-rank constraints will not satisfy the low-rank constraint in general. This is due to the

fact that the (convex) feasible set of the relaxed (42) is a superset of the (non-convex) feasible

set of (42). The following lemma reveals the tightness of exploring the SDR scheme.

23

Lemma 2. For any user i at time slot t, denote by G⋆

SDR for Git in (42) is tight, that is, rank(G⋆

it the solution to (42). If Mit = ∅, then the
it) ≤ 1.

it) ≤ 1; otherwise, we can not claim rank(G⋆

Proof. The Karush-Kuhn-Tucker (KKT) conditions can be explored to prove the tightness of

resorting to the SDR scheme. Nevertheless, we omit the detailed proof for brevity as a similar

proof can be found in Appendix of the work [30].

With the conclusion in Lemma 2, we can recover beamformers from the obtained power
it) ≤ 1, ∀i, then execute eigenvalue decomposition on G⋆

matrices. If rank(G⋆

component is the optimal beamformer g⋆

tion/scale scheme [31] should be performed on G⋆

it, and the principal
it; otherwise, some manipulations such as a randomiza-
it to impose the low-rank constraint.

Note that (42) should be solved for ˜V times at each time slot. To speed up the computation,

they can be optimized in parallel. Moreover, it is tolerable to complete the computation within

the interval [t, t + M] as users’ locations in M time slots are obtained.

Finally, we can summarize the DRL-based optimization algorithm of mitigating the problem

of enhancing users’ VR experiences in Algorithm 4.

V. SIMULATION AND PERFORMANCE EVALUATION

A. Comparison algorithms and parameter setting

To verify the effectiveness of the proposed algorithm, we compare it with three benchmark

algorithms: 1) k-nearest neighbors (KNN) based action quantization algorithm: The unique

difference between the KNN-based algorithm and the proposed algorithm lies in the scheme of

quantizing uplink and downlink action spaces. For the KNN-based algorithm, it adopts the KNN

method [32] to quantize both uplink and downlink action spaces; 2) DROO algorithm: Different

from the proposed algorithm, DROO leverages the order-preserving quantization method [32] to

quantize both uplink and downlink action spaces; 3) Heuristic algorithm: The heuristic algorithm

leverages the greedy admission algorithm in [33] to determine aul

t at each time slot t.
Besides, the user consuming less power in this algorithm will establish the connection with an

t and adl

AP(s) on priority.

Algorithm 4 DRL-based optimization algorithm

1: Initialization: Run initialization steps of Algorithms 1, 2, and 3, and initialize the ESN

training interval Tpr.

24

2: Call Algorithm 3 to pre-train the uplink DNN µ(sul

t |θµ

t ). Likewise, pre-train the downlink

DNN µ(sdl

t |θQ
t ).

3: Run steps 2-8 of Algorithm 1 to pre-train ESN models.

4: for each time slot t = 1, 2, . . . , T do

5:

6:

7:

8:

9:

Run step 9 of Algorithm 1 to obtain predicted location ˆyi(t+M ) of each user i.

Run steps 6-12 of Algorithm 3 to obtain uplink association action aul

pt+M . Likewise, optimize the downlink association action adl

t+M and transmit power
t+M and transmit beamformer

gi(t+M ) for each user i.

if t mod Tpr == 0 then

Steps 2-8 of Algorithm 1.

end if

10: end for

To test the practicality of the developed parallel ESN learning method, realistic user movement

datasets are generated via Google Map. Particularly, for a user, we randomly select its starting

position and ending position on the campus of Singapore University of Technology and Design

(SUTD). Given two endpoints, we use Google Map to generate the user’s 2D trajectory. Next,

we linearly zoom all N users’ trajectories into the communication area of size 0.5 × 0.5 km2.

Additionally, the parameters related to APs and downlink transmission channels are listed

as follows: the number of APs J = 3, the number of antenna elements K = 2, the antenna

LoS = 5.3, σ2

gain G = 5 dB, g = 1 dB, φ = π/3, ϑ = π/2, W dl = 800 MHz, γth = 1 Gb/s, ηLoS = 2.0,
NLoS = 5.27, Dth = 50 m, xo = yo = 250 m, θj = π/3, ˜Ej = 40 dBm,
ηNLoS = 2.4, σ2
Ec
j = 30 dBm, Hj = 5.5 m, ∀j [19]. User and uplink transmission channel-related parameters are
shown as below: uplink system bandwidth W ul = 200 MHz, θth = 200, ¯h = 1.8 m, σ2
m, α = 5, cij = 0.3, pc

i = 23 dBm, ˜pi = 27 dBm, ∀i, j.

h = 0.05

Set other learning-correlated parameters as below: ζ = 1, ξ = 0.25, ¯rmax = 1000, the sample

number Q = 6, the number of future time slots M = 8, Ni = 2, ∀i, No = 2, Nr = 300, and

Tpr = 5. For both uplink DNN and downlink DNN, the ﬁrst hidden layer has 120 neurons, and

the second hidden layer has 80 neurons. The replay memory capacity C = 1e+6, Nepi = 10,

25

500

400

300

200

100

0
100

user's predicted trajectory
user's true trajectory

NRMSE = 0.0277

200

300
x-coordinate (m)

400

500

0.03

0.025

E
S
M
R
N

0.02

0.015

0.01

0.005

2

4

6

8

10

12

14

16

)

m

(

i

t

e
a
n
d
r
o
o
c
-
y

(a) A user’s true and predicted trajectories

(b) NRMSE of predicted trajectories of N users

Fig. 6. Prediction accuracy of the parallel ESN learning method.

Nepo = 1000, ̟ = 10, σ2 = 0.36, ǫ = 0.99. More system parameters are listed as follows:

carrier frequency fc = 28 GHz, light of speed c = 3.0e+8 m/s, noise power spectral density

N0 = −167 dBm/Hz, and T = 5000 time slots.

B. Performance evaluation

To comprehensively understand the accuracy and the availability of the developed learning

and optimization methods, we illustrate their performance results. In this simulation, we ﬁrst let

the AP number J = 3 and the mobile user number N = 16.

To validate the accuracy of the parallel ESN learning method on predicting mobile users’

locations, we plot the actual trajectory of a randomly selected mobile user and its correspondingly

predicted trajectory in Fig. 6(a). In Fig. 6(b), the accuracy, which is measured by the normalized

root mean-squared error (NRMSE) [25], of predicted trajectories of 16 mobile users is plotted.

From Fig. 6, we can observe that: i) when the orientation angles of users will not change

fast, the learning method can exactly predict users’ locations. When users change their moving

directions quickly, the method loses their true trajectories. However, the method will re-capture

users’ tracks after training ESN models based on newly collected users’ location samples; ii) the

obtained NRMSE of the predicted trajectories of all mobile users will not be greater than 0.03.

Therefore, we may conclude that the developed parallel ESN learning method can be utilized to

predict mobile users’ locations.

Next, to evaluate the performance of the proposed DRL-based optimization algorithm com-

prehensively, we illustrate the impact of some DRL-related crucial parameters such as minibatch

size, training interval, and learning rate on the convergence performance of the proposed al-

 
6

5

4

3

2

1

0

Minibatch size = 64
Minibatch size = 128
Minibatch size = 256
Minibatch size = 512
Minibatch size = 1024
Minibatch size = 2048

6

4

2

0

0

Minibatch size = 64

200

400

5

4

3

2

1

0

Minibatch size = 512

0

200

400

0

100

200

300

400

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0

26

Minibatch size = 64
Minibatch size = 128
Minibatch size = 256
Minibatch size = 512
Minibatch size = 1024
Minibatch size = 2048

2000

4000

6000

8000

10000

(a) Uplink training loss vs. minibatch size

(b) Uplink reward vs. minibatch size

1

0.8

0.6

0.4

0.2

0

0

Minibatch size = 64
Minibatch size = 128
Minibatch size = 256
Minibatch size = 512
Minibatch size = 1024
Minibatch size = 2048

100

200

300

400

1

0.9

0.8

0.7

0.6

0.5

0

Minibatch size = 64
Minibatch size = 128
Minibatch size = 256
Minibatch size = 512
Minibatch size = 1024
Minibatch size = 2048

2000

4000

6000

8000

10000

(c) Downlink training loss vs. minibatch size

(d) Downlink reward vs. minibatch size

Fig. 7. The impact of minibatch size |Tt| on the convergence performance of the proposed algorithm.

gorithm. DNN training loss and moving average reward, which is the average of the achieved

rewards over the last 50 epochs, are leveraged as the evaluation indicators.

Fig. 7 plots the tendency of the DNN training loss and the achieved moving average reward

of the proposed algorithm under diverse minibatch sizes. This ﬁgure illustrates that: i) a great

minibatch size value will cause the DNN to converge slowly or even not. As shown in Fig. 7(a),
L(θµ

465) = 0.1023 when we let |Tt| = 64. The
result in Fig. 7(b) shows that DNN does not converge after 10000 epochs when |Tt| = 2048.

465) = 0.1885 when we set |Tt| = 512. Yet, L(θµ

This is because a great |Tt| indicates overtraining, resulting in the local minima and degraded

convergence performance. Further, a large minibatch size value consumes more training time at

each training epoch. Therefore, we set the training minibatch size |Tt| = 64 in the simulation;

ii) when |Tt| = 64, rul

¯t gradually increase and stabilize at around 0.7141 and 0.9375,
respectively. The ﬂuctuation is mainly caused by the random sampling of training data and user

¯t and rdl

movement.

Fig. 8 illustrates the tendency of obtained uplink and downlink DNN training losses and

moving average rewards under diverse training interval values. From this ﬁgure, we can observe

that a small training interval value indicates faster convergence speed. For example, if we set
¯t converges to 0.7156 when epoch ¯t > 439. If we

the training interval Tti = 5, the obtained rul

6

5

4

3

2

1

0

Training interval = 5
Training interval = 10
Training interval = 20
Training interval = 50
Training interval = 100

0

500

1000

1500

27

0.75

0.7

0.65

0.6

0.55

0.5

0

Training interval = 5
Training interval = 10
Training interval = 20
Training interval = 50
Training interval = 100

2000

4000

6000

8000

10000

(a) Uplink training loss vs. training interval

(b) Uplink reward vs. training interval

1

0.8

0.6

0.4

0.2

0

0

Training interval = 5
Training interval = 10
Training interval = 20
Training interval = 50
Training interval = 100

500

1000

1500

1

0.9

0.8

0.7

0.6

0.5

0

Training interval = 5
Training interval = 10
Training interval = 20
Training interval = 50
Training interval = 100

2000

4000

6000

8000

10000

(c) Downlink training loss vs. training interval

(d) Downlink reward vs. training interval

Fig. 8. The impact of DNN training interval Tti on the convergence performance of the proposed algorithm.

let the training interval Tti = 100, rul

¯t converges to 0.7149 when epoch ¯t > 4975, as shown in
Fig. 8(b). However, it is unnecessary to train and update the DNN frequently, which will bring

more frequent policy updates, if the DNN can converge. Thus, to achieve the trade-off between

the convergence speed and the policy update speed, we set Tti = 20 in the simulation.

Fig. 9 depicts the tendency of achieved DNN training loss and moving average reward of the

proposed algorithm under different learning rate conﬁgurations. From this ﬁgure, we have the

following observations: i) for the uplink DNN, when given a small learning rate value, it may

converge to the local optimum or even not; ii) for the downlink DNN, both a small and a great

learning rate value will degrade convergence performance. Therefore, when training the uplink

DNN, we set the learning rate lul

For instance, rul
decreases to zero with an increasing epoch ¯t. We set the learning rate ldl
the downlink DNN. Given this parameter setting, the obtained L(θQ

r = 0.1, which can lead to good convergence performance.
¯t gradually
r = 0.01 when training
¯t ) is smaller than 0.2 after

¯t converges to 0.7141 when epoch ¯t ≥ 1300 and the variance of rul

training for 200 epochs.

At last, we verify the superiority of the proposed algorithm by comparing it with other compar-

ison algorithms. Particularly, we plot the achieved objective function values of all comparison

algorithms under varying number of mobile users N ∈ {8, 12, 16, 20} in Fig. 10. Before the

6

5

4

3

2

1

0

Learning rate = 0.1
Learning rate = 0.01
Learning rate = 0.001
Learning rate = 0.0001
Learning rate = 0.00001

0

100

200

300

400

28

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0

Learning rate = 0.1
Learning rate = 0.01
Learning rate = 0.001
Learning rate = 0.0001
Learning rate = 0.00001

2000

4000

6000

8000

10000

(a) Uplink training loss vs. learning rate

(b) Uplink reward vs. learning rate

6

4

2

0

-2

0

Learning rate = 0.1
Learning rate = 0.01
Learning rate = 0.001
Learning rate = 0.0001
Learning rate = 0.00001

100

200

300

400

1

0.9

0.8

0.7

0.6

0.5

0.4

0

Learning rate = 0.1
Learning rate = 0.01
Learning rate = 0.001
Learning rate = 0.0001
Learning rate = 0.00001

2000

4000

6000

8000

10000

(c) Downlink training loss vs. learning rate

(d) Downlink reward vs. learning rate

Fig. 9. The impact of learning rates lul

r and ldl

r on the convergence performance of the proposed algorithm.

1.8

1.6

1.4

1.2

1

0.8

e
u
a
v

l

n
o
i
t
c
n
u
f
e
v
i
t
c
e
b
O

j

0.6

8

+4.14%

Proposed
DROO
KNN-based
Heuristic

10

12

14

16

18

20

Fig. 10. Comparison of obtained objective function values of all comparison algorithms.

evaluation, the proposed algorithm and the other two action quantization algorithms have been

trained with 10000 independent wireless channel realizations, and their downlink and uplink

action quantization policies have converged. This is reasonable because we are more interested

in the long-term operation performance for ﬁeld deployment. Besides, we let the service ability
of an AP ˜M vary with N with the (N, ˜M ) pair being (8, 3), (12, 5), (16, 6), and (20, 7).

We have the following observations from this ﬁgure: i) the proposed algorithm achieves the

greatest objective function value. For the DROO algorithm, it gains a smaller objective function

 
 
29

value than the proposed algorithm; for example, the achieved objective function value of DROO

is 4.14% less than that of the proposed algorithm. For the KNN-based algorithm, it obtains the

smallest objective function value because it offers the smallest diversity in the produced uplink

and downlink association action set; ii) except for heuristic algorithm, the achieved objective

function values of the other comparison algorithms decrease with the number of users owing

to the increasing total power consumption. For the heuristic algorithm, its obtained objective

function value increases with N mainly because more users can successfully access to APs.

VI. CONCLUSION

This paper investigated the problem of enhancing VR visual experiences for mobile users

and formulated the problem as a sequence-dependent problem aiming at maximizing users’

feeling of presence in VR environments while minimizing the total power consumption of

users’ HMDs. This problem was conﬁrmed to be a mixed-integer and non-convex optimization

problem, the solution of which also needed accurate users’ tracking information. To solve this

problem effectively, we developed a parallel ESN learning method to predict users’ tracking

information, with which a DRL-based optimization algorithm was proposed. Speciﬁcally, this

algorithm ﬁrst decomposed the formulated problem into an association subproblem and a power

control subproblem. Then, a DNN joint with an action quantization scheme was implemented

as a scalable solution that learnt association variables from experience. Next, the power control

subproblem with an SDR scheme being explored to tackle its non-convexity was leveraged

to criticize the association variables. Finally, simulation results were provided to verify the

accuracy of the learning method and showed that the proposed algorithm could improve the

energy efﬁciency by at least 4.14% compared with various benchmark algorithms.

A. Proof of Lemma 1

APPENDIX

For any user i ∈ U, suppose we are provided with a sequence of Q desired input-output

pairs {(xi(t−Q), yi(t−Q+1)), . . . , (xi(t−1), yit)}. With the input-output pairs, generate the hidden

matrix Xit =

xi(t−1)

si(t−1)



· · ·

xi(t−Q)

si(t−Q)



and the corresponding target location matrix Yit =

[yT

it; . . . ; yT

i(t−Q+1)] at time slot t. We next introduce an auxiliary matrix U = X TW ∈ RQ×No,





wherein we lighten the notation Xit for X. According to the Lagrange dual decomposition

method, we can rewrite (18) as follows

1
Q minimize
W ,U

l(X TW ) + ξQr(W ) + A ⊙ U − A ⊙ X TW

30

No
(cid:8)
[Azn]TX TW zn + ξQr(W )

−

+

(cid:9)

(cid:27)

−

(43)

(cid:27)

= 1

Q inf
W

(cid:26)

= −ξ sup

n=1
P
1
Q inf
U

(cid:26)
No

1
ξQ

J

W (cid:26)
1
Q

No

l(U ) +

[Azn]TU zn

n=1
P

sup
umn
J

[Azn]TX TW zn − r(W )

n=1
P

No

j=1
m∈Qj
P
P
ξQATX T

1

n=1
P
− 1
Q

(cid:27)

{−amnumn − l(umn)}

No

l⋆(−amn)

(cid:17)

j=1
P

m∈Qj
P

n=1
P

= −ξr⋆

(cid:16)
∆= D(A)

where zn ∈ RNo is a column vector with the n-th element being one and all other elements

being zero, Qj is an index set including the indices of Q data samples fed to slave VM j.

Let ¯r(C) = 1
ξQ
¯r(C). Then, calculate the derivative of ¯r(C) w.r.t W ,

n CW zn − r(W ), where C = 1
zT

ξQATX T, and denote W ⋆ as the optimal

solution to sup
W

No

n=1
P

where Cn = C Tzn.

d¯r(C)
dW

=

No

n=1
X

CnzT

n − 2W

(44)

As W ∈ R(Ni+Nr)×No, the necessary and sufﬁcient condition for obtaining W ⋆ is to enforce

d¯r(C)
dW ⋆ = 0. Then, we have

W ⋆ =

1
2

By substituting (45) into r⋆(C), we can obtain (22).

n=1
X

No

CnzT
n

(45)

Similarly, denote u⋆

solution to l⋆(−amn). As U ∈ RQ×No, the necessary and sufﬁcient condition for u⋆
execute dl⋆(−amn)

mn for any m ∈ {1, 2, . . . , Q} and n ∈ {1, 2, . . . , No} as the optimal
mn is to
mn into l⋆(−amn), we can obtain

mn + ymn = 0. By substituting u⋆

= −amn − u⋆

du⋆

mn

(23). This completes the proof.

REFERENCES

[1] X. Hou, S. Dey, J. Zhang, and M. Budagavi, “Predictive adaptive streaming to enable mobile 360-degree and VR

experiences,” IEEE Trans. Multimedia, vol. 23, pp. 716–731, 2021.

31

[2] H. Bellini, “The real deal with virtual and augmented reality,” https://www.goldmansachs.com/insights/pages/virtual-and-

augmented-reality.html, Feb. 2016.

[3] C. Wiltz, “5 major challenges for VR to overcome,” https://www.designnews.com/electronics-test/5-major-challenges-vr-

overcome, Apr, 2017.

[4] Y. Liu, J. Liu, A. Argyriou, and S. Ci, “MEC-assisted panoramic VR video streaming over millimeter wave mobile

networks,” IEEE Trans. Multimedia, vol. 21, no. 5, pp. 1302–1316, 2019.

[5] J. Dai, Z. Zhang, S. Mao, and D. Liu, “A view synthesis-based 360◦ VR caching system over MEC-enabled C-RAN,”

IEEE Trans. Circuits Syst. Video Technol., vol. 30, no. 10, pp. 3843–3855, 2020.

[6] Z. Lai, Y. C. Hu, Y. Cui, L. Sun, N. Dai, and H. Lee, “Furion: Engineering high-quality immersive virtual reality on

today’s mobile devices,” IEEE Trans. Mob. Comput., vol. 19, no. 7, pp. 1586–1602, 2020.

[7] X. Hou, Y. Lu, and S. Dey, “Wireless VR/AR with edge/cloud computing,” in ICCCN.

IEEE, 2017, pp. 1–8.

[8] Qualcomm,

“Whitepaper:

Making

immersive

virtual

reality

possible

in

mobile,”

https://www.qualcomm.com/media/documents/ﬁles/whitepaper-making-immersive-virtual-reality-possible-in-mobile.pdf,

Mar. 2016.

[9] Oculus, “Mobile VR media overview. Accessed: Sep. 2018,” https://www.oculus.com/, 2018.

[10] HTC, “HTC Vive. Accessed: Sep. 2018,” https://www.vive.com/us/, 2018.

[11] T. Dang and M. Peng, “Joint radio communication, caching, and computing design for mobile virtual reality delivery in

fog radio access networks,” IEEE J. Sel. Areas Commun., vol. 37, no. 7, pp. 1594–1607, 2019.

[12] X. Liu, Q. Xiao, V. Gopalakrishnan, B. Han, F. Qian, and M. Varvello, “360◦ innovations for panoramic video streaming,”

in HotNets. ACM, 2017, pp. 50–56.

[13] R. Ju, J. He, F. Sun, J. Li, F. Li, J. Zhu, and L. Han, “Ultra wide view based panoramic VR streaming,” in VR/AR

NetworkSIGCOMM. ACM, 2017, pp. 19–23.

[14] S. Mangiante, G. Klas, A. Navon, G. Zhuang, R. Ju, and M. D. Silva, “VR is on the edge: How to deliver 360◦ videos

in mobile networks,” in VR/AR NetworkSIGCOMM. ACM, 2017, pp. 30–35.

[15] V. R. Gaddam, M. Riegler, R. Eg, C. Griwodz, and P. Halvorsen, “Tiling in interactive panoramic video: Approaches and

evaluation,” IEEE Trans. Multimedia, vol. 18, no. 9, pp. 1819–1831, 2016.

[16] X. Corbillon, G. Simon, A. Devlic, and J. Chakareski, “Viewport-adaptive navigable 360-degree video delivery,” in ICC.

IEEE, 2017, pp. 1–7.

[17] C. Perfecto, M. S. ElBamby, J. D. Ser, and M. Bennis, “Taming the latency in multi-user VR 360◦: A QoE-aware deep

learning-aided multicast framework,” IEEE Trans. Commun., vol. 68, no. 4, pp. 2491–2508, 2020.

[18] M. S. ElBamby, C. Perfecto, M. Bennis, and K. Doppler, “Edge computing meets millimeter-wave enabled VR: paving

the way to cutting the cord,” in WCNC.

IEEE, 2018, pp. 1–6.

[19] M. Chen, O. Semiari, W. Saad, X. Liu, and C. Yin, “Federated echo state learning for minimizing breaks in presence in

wireless virtual reality networks,” IEEE Trans. Wirel. Commun., vol. 19, no. 1, pp. 177–191, 2020.

[20] P. Yang, X. Xi, Y. Fu, T. Q. S. Quek, X. Cao, and D. O. Wu, “Multicast eMBB and bursty URLLC service multiplexing

in a CoMP-enabled RAN,” IEEE Trans. Wirel. Commun., vol. 20, no. 5, pp. 3061–3077, 2021.

[21] Q. Cheng, H. Shan, W. Zhuang, L. Yu, Z. Zhang, and T. Q. S. Quek, “Design and analysis of MEC-and proactive caching-

based 360◦ mobile VR video streaming,” IEEE Trans. Multimedia, 2021, in press. DOI: 10.1109/TMM.2021.3067205.

[22] Y. Sun, Z. Chen, M. Tao, and H. Liu, “Communications, caching, and computing for mobile virtual reality: Modeling and

tradeoff,” IEEE Trans. Commun., vol. 67, no. 11, pp. 7573–7586, 2019.

[23] O. Semiari, W. Saad, M. Bennis, and Z. Dawy, “Inter-operator resource management for millimeter wave multi-hop backhaul

networks,” IEEE Trans. Wirel. Commun., vol. 16, no. 8, pp. 5258–5272, 2017.

32

[24] S. Bouchard, J. St-Jacques, G. Robillard, and P. Renaud, “Anxiety increases the feeling of presence in virtual reality,”

Presence Teleoperators Virtual Environ., vol. 17, no. 4, pp. 376–391, 2008.

[25] S. Scardapane, D. Wang, and M. Panella, “A decentralized training algorithm for echo state networks in distributed big

data applications,” Neural Networks, vol. 78, pp. 65–74, 2016.

[26] H. H. Yang, Z. Liu, T. Q. S. Quek, and H. V. Poor, “Scheduling policies for federated learning in wireless networks,”

IEEE Trans. Commun., vol. 68, no. 1, pp. 317–333, 2020.

[27] M. Bennis, S. M. Perlaza, P. Blasco, Z. Han, and H. V. Poor, “Self-organization in small cell networks: A reinforcement

learning approach,” IEEE Trans. Wirel. Commun., vol. 12, no. 7, pp. 3202–3212, 2013.

[28] P. Yang, X. Cao, X. Xi, W. Du, Z. Xiao, and D. O. Wu, “Three-dimensional continuous movement control of drone cells

for energy-efﬁcient communication coverage,” IEEE Trans. Veh. Technol., vol. 68, no. 7, pp. 6535–6546, 2019.

[29] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in ICLR (Poster), 2015.

[30] P. Yang, X. Xi, T. Q. S. Quek, J. Chen, X. Cao, and D. Wu, “How should I orchestrate resources of my slices for bursty

URLLC service provision?” IEEE Trans. Commun., vol. 69, no. 2, pp. 1134–1146, 2020.

[31] Z. Luo, W. Ma, A. M. C. So, Y. Ye, and S. Zhang, “Semideﬁnite relaxation of quadratic optimization problems,” IEEE

Signal Processing Magazine, vol. 27, no. 3, pp. 20–34, 2010.

[32] L. Huang, S. Bi, and Y. A. Zhang, “Deep reinforcement learning for online computation ofﬂoading in wireless powered

mobile-edge computing networks,” IEEE Trans. Mob. Comput., vol. 19, no. 11, pp. 2581–2593, 2020.

[33] J. Tang, B. Shim, and T. Q. S. Quek, “Service multiplexing and revenue maximization in sliced C-RAN incorporated with

URLLC and multicast eMBB,” IEEE J. Sel. Areas Commun., vol. 37, no. 4, pp. 881–895, 2019.

