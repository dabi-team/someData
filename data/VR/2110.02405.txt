Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction

JUSTIN WILSON, University of North Carolina at Chapel Hill, USA
NICHOLAS REWKOWSKI, University of Maryland at College Park, USA
MING C. LIN, University of Maryland at College Park, USA
HENRY FUCHS, University of North Carolina at Chapel Hill, USA

1
2
0
2

t
c
O
5

]

V
C
.
s
c
[

1
v
5
0
4
2
0
.
0
1
1
2
:
v
i
X
r
a

Fig. 1. Left: ground truth image. Before (Middle) and after (Right) audio-augmented rendering of an indoor scene with an open and closed reflective surfaces.
The reconstruction is enhanced by EchoCNN inferences of surface detection, depth estimation, and material classification based on audio-visual reflecting
sound and image inputs. Green arrows highlight areas enhanced by our method. Glass surfaces added for closed windows and open areas remain unfilled.

Reflective and textureless surfaces such as windows, mirrors, and walls
can be a challenge for object and scene reconstruction. These surfaces are
often poorly reconstructed and filled with depth discontinuities and holes,
making it difficult to cohesively reconstruct scenes that contain these planar
discontinuities. We propose â€Echoreconstructionâ€, an audio-visual method
that uses the reflections of sound to aid in geometry and audio reconstruc-
tion for virtual conferencing, teleimmersion, and other AR/VR experience.
The mobile phone prototype emits pulsed audio, while recording video
for RGB-based 3D reconstruction and audio-visual classification. Reflected
sound and images from the video are input into our audio (EchoCNN-A)
and audio-visual (EchoCNN-AV) convolutional neural networks for surface
and sound source detection, depth estimation, and material classification.
The inferences from these classifications enhance scene 3D reconstructions
containing open spaces and reflective surfaces by depth filtering, inpainting,
and placement of unmixed sound sources in the scene. Our prototype, VR
demo, and experimental results from real-world and virtual scenes with chal-
lenging surfaces and sound indicate high success rates on classification of

Authorsâ€™ addresses: Justin Wilson, University of North Carolina at Chapel Hill, Chapel
Hill, NC, 27599, USA, wilson@cs.unc.edu; Nicholas Rewkowski, University of Maryland
at College Park, College Park, MD, 20740, USA, nick1@umd.edu; Ming C. Lin, University
of Maryland at College Park, College Park, MD, 20740, USA, lin@cs.umd.edu; Henry
Fuchs, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA,
fuchs@cs.unc.edu.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Â© 2021 Association for Computing Machinery.
0730-0301/2021/10-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

material, depth estimation, and closed/open surfaces, leading to considerable
visual and audio improvement in 3D scenes (see Figure 1).

CCS Concepts: â€¢ Computing methodologies â†’ Virtual reality.

Additional Key Words and Phrases: 3D scanning, textureless, audio-visual, re-
flective surfaces, windows, echolocation, echoreconstruction, sound sources

ACM Reference Format:
Justin Wilson, Nicholas Rewkowski, Ming C. Lin, and Henry Fuchs. 2021.
Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction . ACM
Trans. Graph. 1, 1 (October 2021), 12 pages. https://doi.org/10.1145/nnnnnnn.
nnnnnnn

INTRODUCTION

1
Scenes containing open and reflective surfaces, such as windows and
mirrors, can enhance AR/VR immersion in terms of both graphics
and sound; for example, a window open in spring compared to closed
in winter. However, they also present a unique set of challenges. First,
they are difficult to detect and reconstruct due to their transparency
and high reflectivity. Distinguishing between glass (e.g. window)
and an opening in the space is an important part of the audio-visual
experience for AR/VR engagement. Also, illumination, background
objects, and min/max depth ranges can be confounding factors.

Reconstruction of scenes for teleimmersion have led to advances
in detection [Lea et al. 2016], segmentation [Arnab et al. 2015;
Golodetz* et al. 2015], and semantic understanding [Song et al. 2017]
and are used to generate large-scale, labeled datasets of object [Wu
et al. 2015] and scene [Dai et al. 2017a] geometric models to further
aid training and sensing in a 3D environment. Advances have also
been made to account for challenging surfaces [Chabra et al. 2019;
Sinha et al. 2012; Whelan et al. 2018]. Yet, scenes containing open

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

 
 
 
 
 
 
2

â€¢ Wilson, J. et al

and reflective surfaces, such as windows and mirrors, remain an
open research area. Our work augments existing visual methods
by adding audio context of surface detection, depth, and material
estimation for recreating a virtual environment from a real one.

Previous work has used sound to better understand objects in
scenes. For instance, impact sounds from interacting with objects in
a scene to perform segmentation [Arnab et al. 2015] and to emulate
the sensory interactions of human information processing [Zhang
et al. 2017a]. Audio has also been used to compute material [Ren et al.
2013], object [Zhang et al. 2017a], scene [Schissler et al. 2018], and
acoustical [Tang et al. 2020] properties. Moreover, using both audio
and visual sensory inputs has proven more effective; for example,
multi-modal learning for object classification [Sterling et al. 2018;
Wilson et al. 2019] and object tracking [Wilson and Lin 2020].

Fusing multiple modalities, such as vision and sound, provide a
wider range of possibilities than either single modality alone. In this
work, we show that augmenting vision-based techniques with audio,
referred to as â€œEchoCNN,â€ can detect open or reflective surfaces,
its depth, and material, thereby enhancing 3D object and scene
reconstruction for AR/VR systems. We highlight key results below:

â€¢ EchoReconstruction, a staged audio-visual 3D reconstruction
pipeline that uses mobile devices to enhance scene geom-
etry containing windows, mirrors, and open surfaces with
depth filtering and inpainting based on EchoCNN inferences
(section 3);

â€¢ EchoCNN, a fused audio-visual CNN architecture for classify-
ing open/closed surfaces, their depth, and material or sound
source placement (section 4);

â€¢ Automated data collection process and audio-visual ground
truth data for real and synthetic scenes containing windows
and mirrors (section 5).

Using EchoReconstruction, we have been able to achieve consis-
tently higher accuracy (up to 100%) in classification of open/closed
surfaces, depth estimation, and materials in both real-world scenes
and controlled experiments, resulting in considerably improved 3D
scene reconstruction with glass doors, windows and mirrors.

2 RELATED WORK
Previous research in 3D reconstruction, audio-based classifications,
and echolocation are discussed in this section in addition to existing
techniques for reconstructing open and reflective surfaces.

3D reconstruction

2.1
Object and scene reconstruction methods generate 3D scans us-
ing RGB and RGB-D data. For example, Structure from Motion
(SFM) [Westoby et al. 2012], Multi-View Stereo (MVS) [Seitz et al.
2006], and Shape from Shading [Zhang et al. 1999] are all techniques
to scan a scene and its objects. Static [Golodetz* et al. 2015; New-
combe et al. 2011] and dynamic [Dai et al. 2017b; Newcombe et al.
2015] scenes can also be scanned in real-time using commodity
sensors such as the Microsoft Kinect and GPU hardware. 3D scene
reconstructions have also been performed with sound based on time
of flight sensing [Crocco et al. 2016]. Not only has this previous
research generated large amounts of 3D scene [Silberman et al. 2012;
Song et al. 2017] and object [Lai et al. 2011; Singh et al. 2014; Wu

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

Fig. 2. Top row: closed window in winter. Middle row: opened in spring. Bot-
tom row: controlled experiment virtual scene. Column 1: mobile echorecon-
struction prototype in real world and virtual scenes with a video of emitted
pulsed audio and images from the scene. Column 2: initial RGB based 3D
reconstruction using state-of-the-art visual methods (live [Tanskanen et al.
2013] or photogrammetric [Metashape 2020]). Column 3: our audio-visual
EchoCNN convolutional neural network classifies open or closed surface,
depth, and material for inpainting to resolve planar discontinuities caused
by reflective surfaces, such as windows and mirrors. Column 4: seman-
tic rendering of the window given material estimation and point of view.
Green arrows highlight areas enhanced by our method, such as detecting
closed/open parts of a window and filling a reflective mirror.

et al. 2015] data, they also benefit from these datasets by using
them for training vision-based neural networks for classification,
segmentation, and other downstream tasks. Depth estimation algo-
rithms [Alhashim and Wonka 2018; Chabra et al. 2019; Eigen and
Fergus 2014] also create 3D reconstructions by fusing depth maps
using ICP and volumetric fusion [Izadi et al. 2011].

2.1.1 Glass and mirror reconstruction. Reflective surfaces produce
identifiable audio and visual artifacts that can be used to help their
detection. For example, researchers have developed algorithms to
detect reflections in images taken through glass using correlations
of 8-by-8 pixel blocks [Shih et al. 2015], image gradients [Kopf et al.
2013], two layer renderings [Sinha et al. 2012], polarization imaging
reflectometry [Riviere et al. 2017], and diffraction effects [Toisoul
and Ghosh 2017]. Adding hardware, [Sutherland 1968] used ultra-
sonic sensor logic to track continuous wave ultrasound and [Zhang
et al. 2017b] to detect obstacles such as glass and mirrors by using
frequencies outside of the human audible range. More recently, re-
flective surfaces have been detected by utilizing a mirrored variation
of an AprilTag [Olson 2011; Wang and Olson 2016]. [Whelan et al.
2018] use the reflective surface to their advantage by recognizing the
AprilTag attached to their Kinect scanning device when it appears
in the scene. Depth jumps and incomplete reconstructions have also
been used [Lysenkov et al. 2012]. However, vision based approaches

Example 3D Reconstruction Methods

Type
Active (RGB-D) KinectFusion, DynamicFusion, BundleFusion
Passive (RGB)

Methods

Stereo
Lidar
Ultrasonic
Time of flight

SLAM, SFM, [Tanskanen et al. 2013]
ScanNet, [Whelan et al. 2018]
MVS, StereoDRNet
[Kada and Mckinley 2009]
[Zhang et al. 2017b]
[Crocco et al. 2016]

Table 1. 3D reconstruction methods by type such as passive (RGB), active
(RGB-D), or other sensor (e.g. ultrasound, lidar, etc.); single or multiple
views; and static or dynamic scenes.

require the right illumination, non-blurred imagery, and limited
clutter behind the surface that may limit the reflection. We show
that sound creates a distinct audio signal, providing reconstruction
methods complementary data about the presence of windows and
mirrors without additional sensors.

2.2 Acoustic imaging and audio-based classifiers
We begin with an introduction into sound propagation, room acous-
tics, and audio-visual classifiers.

Acoustics: various models have been developed to simulate sound
propagation in a 3D environment, such as wave-based [Mehra
et al. 2015], ray tracing based [Rungta et al. 2016], sound source
clustering [Tsingos et al. 2004], multipole equivalent source meth-
ods [James et al. 2006], and a single point multipole expansion
method [Zheng and James 2011], representing outgoing pressure
fields. [Godoy et al. 2018] uses acoustics and a smartphone for an
app to detect car location and distance from walking pedestrians us-
ing temporal dynamics. [Bianco et al. 2019] further discusses theory
and applications of machine learning in acoustics. Computational
imaging approaches have also used acoustics for non-line-of-sight
imaging [Lindell et al. 2019], 3D room geometry reconstruction
from audio-visual sensors [Kim et al. 2017], and acoustic imaging
on a mobile device [Mao et al. 2018]. To reconstruct windows and
mirrors, our work uses room acoustics given the surface materials
of the room [Schissler et al. 2018] and distance from sound source.
However, prior work and downstream processes often require a
watertight reconstruction which can be difficult to generate in the
presence of glass. Our approach addresses these issues using an
integrated audio-visual CNN that can detect discontinuity, depth,
and materials.

Audio-based classification and reconstruction: using princi-
ples from sound synthesis, propagation, and room acoustics, audio
classifiers have been developed for environmental sound [Gem-
meke et al. 2017; Piczak 2015; Salamon et al. 2014], material [Arnab
et al. 2015], and object shape [Zhang et al. 2017a] classification.
For audio-based reconstruction, Bat-G net uses ultrasonic echoes
to train an auditory encoder and 3D decoder for 3D image recon-
struction [Hwang et al. 2019]. Audio input can take the form of raw
audio, spectral shape descriptors [Cowling and Sitte 2003; Michael
et al. 2005; Smith III 2020], or frequency spectral coefficients that
we also adopt. In our method, we use reflecting sound to perform
surface detection, depth estimation, and material classification.

Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction

â€¢

3

Audio-visual learning: similar to its applications in natural
language processing (NLP) and visual questing & answering sys-
tems [Hannan et al. 2020; Kim et al. 2020, 2016], multi-modal learn-
ing using both audio-visual sensory inputs has also been used
for classification tasks [Sterling et al. 2018; Wilson et al. 2019],
audio-visual zooming [Nair et al. 2019], and sound source separa-
tion [Ephrat et al. 2018; Lee and Seung 2000] which have also isolated
waves for specific generation tasks. Although similar in spirit, our
audio-visual method, â€œEchoreconstruction,â€ differs from the existing
methods by learning absorption and reflectance properties to detect
a reflective surface, its depth, and material.

3 TECHNICAL APPROACH
In this work, we adopt â€œecholocationâ€ as an analog for our echore-
construction method. According to [Egan 1988], echo is defined as
distinct reflections of the original sound with a sufficient sound level
to be clearly heard above the general reverberation. Although per-
ceptible echo is abated because of precedence (known as the Haas
effect) [Long 2014], returning sound waves are received after reflect-
ing off of a solid surface. We use these distinct, reflecting sounds to
design a staged approach of audio and audio-visual convolutional
neural networks. EchoCNN-A and EchoCNN-AV can be used to
estimate depth based on reverberation times (Figure 9), recognize
material based on frequency and amplitude, and handle both static
and dynamic scenes with moving objects based on Doppler shift.
All of which enhance scene and object reconstruction by detect-
ing planar discontinuities from open or closed surfaces and then
estimating depth and material.

3.1 Echolocation
Echolocation is the use of reflected sound to locate and identify
objects, particularly used by animals like dolphins and bats. Accord-
ing to [Szabo 2014], bats emit ultrasound pulses, ranging between
20-150 kHz, to catch an insect prey with a resolution of 2-15 mm.
This involves signal processing such as:

(1) Doppler shift (the relative speed of the target),

Î”ğ‘“ = ğ‘“ğ· âˆ’ ğ‘“0 = ğ‘“0

ğ‘ğ‘ 
ğ‘0

ğ‘ğ‘œğ‘  (ğœƒ )

(1)

(2) time delay (distance to the target), and
(3) frequency and amplitude in relation to distance (target object

size and type recognition);

where the Doppler shift (or effect) is the perceived change in fre-
quency (Doppler frequency ğ‘“ğ· minus transmitted frequency ğ‘“0) as
a sound source with velocity ğ‘ğ‘  moves toward or away from the
listener/observer with velocity ğ‘ğ‘œ and angle ğœƒ .

3.2 Staged classification and reconstruction pipeline
As depicted in Figure 3, we take a staged approach to enhance scene
and object reconstruction using audio-visual data. Our echorecon-
struction prototype consists of two smartphones - one recording
(top) and one emitting/reconstructing (bottom). Each audio emis-
sion is 100 ms of sound followed by 900 ms of silence to allow for
the receiving microphone to capture reflections and reverberations
(subsection 3.3). After the 3D scan is complete, an .obj file containing
geometry and texture information is generated. 1 second frames are

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

4

â€¢ Wilson, J. et al

Fig. 3. Staged approach to enhance 3D virtual scene and object reconstruction using audio-visual data. Our echoreconstruction prototype consists of two
smartphones - one recording (top) and one emitting/reconstructing (bottom). As the bottom smartphone moves to reconstruct the scene and emits 100 ms
pulsed audio , the top smartphone is used to record video of the direct and reflecting sound. The receiving audio is split into 1.0 second intervals to allow for
reverberation. These audio intervals are converted into mel-scaled spectrograms and passed through a multimodal echoreconstruction convolutional neural
network (we refer to as EchoCNN) comprised of 2D convolutional, max pooling, fully connected, and softmax layers. EchoCNN classifications inform depth
filtering and hole filling steps to resolve planar discontinuities in scans caused by reflective surfaces, such as windows and mirrors. Binary classification is
used to predict if a window is open or closed. Multi-class classification is used for depth and material estimation.

noise (white, pink, and brownian). All of which can be generated as
either pulsed (PW) or continuous waves (CW). PW is preferred for
theoretical and empirical reasons. First, the transmission frequency
ğ‘“0 may experience considerable downshift as a result of absorption
and diffraction effects [Szabo 2014]. Therefore, using pulsed waves
independent for each emission is theoretically better than continu-
ous waves compared to ğ‘“0. Furthermore, section 6 shows superior
PW results over CW for the given classification tasks.

Pure tones were generated with default 0.8 out of 1 amplitudes
using the Audacity computer program and center frequencies of
63 Hz, 125 Hz, 250 Hz, 500 Hz, 1 kHz, 2 kHz, 4 kHz, 8 kHz, and
16 kHz. Human voice ranges from about 63 Hz to 1 kHz [Long
2014] (125 Hz to 8 kHz [Egan 1988]) and an untrained whistler
between 500 Hz to 5 kHz [Nilsson et al. 2008]. Chirps were linearly
interpolated from 440 Hz to 1320 Hz in 100 ms. A hand clap is an
impulsive sound that yields a flat spectrum [Long 2014]. All sound
sources were recorded and played back with max volume (Figure 4).
While recorded sounds were used for consistency, we plan to add live
audio for augmentation and future ease of use during reconstruction.
Please see our supplementary materials for spectrograms across all
sound sources.

Audio input: audio was generated in pulsed waves (PW). One
smartphone to emit the sound while performing a RGB-based re-
construction and the second smartphone to capture video. As future
work, a single mobile device or Microsoft Kinect paired with audio
chirps could be used for audio-visual capture and reconstruction
instead of two separate devices. Each pulsed wave emitted into
the scene was a total of 1 second consisting of an 100 ms impulse

Fig. 4. Mel-scaled spectrograms of recorded impulses of different sound
sources used. From left to right: narrow to disperse spectra. Not shown
are other pure tone frequencies, chirp, pink noise, and brownian noise.
Horizontal axis is time and vertical axis is frequency.

extracted from the recorded video to generate audio and visual input
into the EchoCNN neural networks (section 4). These networks are
independently trained to detect whether a surface is open or closed,
estimate depth to the surface from the sound source, and classify
the material of the surface. Using mobile accelerometer data and a
multi-scale neural network, such as [Eigen and Fergus 2014], but
using audio as a coarse global output refined using finer-scale visual
data to augment depth estimation will be explored as future work.

3.3 Sound source
A smartphone emits recordings of human experimenter voice, whis-
tle, hand clap, pure tones (ranging from 63 Hz to 16 kHz), chirps, and

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

Table 2. According to the Sabine Formula (Equation 3.3), reverberation time
can be calculated as room volume V divided by total room absorption a. For
an indoor sound source in a reverberant field, a is the total room absorption
at a given frequency (sabins), S is the surface area (ft2), and ğ›¼ is the sound
absorption coefficient at a given frequency (decimal percent). At 250 Hz, the
total room absorption a for our real-world bathroom scene is 69.23 sabins.

Total room absorption a using ğ‘ = (cid:205) ğ‘†ğ›¼ at 250 Hz
a (sabins)
S
Real bathroom scene
43.20
432 x
Painted walls
1.75
175 x
Tile floor
15.00
60 x
Glass
0.78
39 x
Ceramic
8.50
34 x
Mirror
69.23 sabins

ğ›¼
0.10 =
0.01 =
0.25 =
0.02 =
0.25 =
Total a =

followed by silence. 1 second audio frames is based on the Sabine
Formula of reverberation time for a compact room of like dimensions
calculated as:

ğ‘‡ = 0.05

ğ‘‰
ğ‘

= 0.05

ğ‘‰
(cid:205) ğ‘†ğ›¼

= (0.05

sec
ft

)

1, 296 ft3
69.23 ft2

= 0.94 sec

(2)

where ğ‘‡ is the reverberation time (time required for sound to decay
60 dB after source has stopped), ğ‘‰ is room volume (ft3), and ğ‘ is the
total room absorption at a given frequency (e.g. 250 Hz). For the
bathroom scene, ğ‘‰ = 9 ft âˆ— 16 ft âˆ— 9 ft = 1, 296 ft3 and ğ‘ = 69.23 ft2,
which is the sum of sound absorption from the materials in Table 2.
Visual input: images were captured from the same smartphone
video as the audio recordings. Each corresponding image was cropped
and grayscaled for illumination invariance and data augmentation.
Image dimensions were 64 by 25 pixels. Visual data served as inputs
for visual only and audio-visual model variation EchoCNN-AV.

Initial 3D Reconstruction

3.4
We evaluated the following smartphone based reconstruction ap-
plications to obtain an initial 3D geometry for which our method
would enhance. The Astrivis application, based on [Tanskanen et al.
2013], generates better live 3D geometries for closed object rather
than scene reconstructions since it limits feature points per scan.
On the other hand, Agisoft Metashape produces scene reconstruc-
tions offline from smartphone video. Enabling the softwareâ€™s depth
point and guided camera matching features further improved recon-
structed geometries.

4 MODEL ARCHITECTURE
To augment visually based approaches, we use a multimodal CNN
with mel-scaled spectrogram and image inputs. First, we perform
surface detection to determine if a space with depth jumps and
holes is in error or in fact open (i.e. open/closed classification). In
the event of error, we estimate distance from recorder to surface
using audio-visual data for depth filtering and inpainting. Finally,
we determine the material. All of these classifications are performed
using our audio and audio-visual convolutional neural networks,
referred to as EchoCNN-A and EchoCNN-AV (Fig. 3).

Audio sub-network: our frame-based EchoCNN-A consists of
a single convolutional layer followed by two dense layers with

Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction

â€¢

5

Fig. 5. Sample visualizations of the filters for the two convolutional layers
in the audio-based EchoCNN-A neural network. The model learns filters for
octave bands, frequencies, reflections, reverberations, and damping.

feature normalization. Sampled at ğ¹ğ‘  = 44.1 kHz to cover the full
audible range, audio frames are 1 second mel-scaled spectrograms
with STFT coefficients ğœ’ (Equation 4). Each audio example is clas-
sified independently and 1 second intervals to reflect an estimated
reverberation time based on a compact room size (Equation 3.3).
With a 2048 sample Hann window (N), 25% overlap, and hop length
(ğ» = 2048/4), this results in a frequency dimension of 21.5 Hz
(Equation 4) and temporal dimension of 12 ms (Equation 4) or 12%
of each 100 ms pulsed audio. Each spectrogram is individually nor-
malized and downsampled to a size of 62 frequency bins by 25 time
bins.

We define the frequency spectral coefficients [Mller 2015] as:

ğœ’ (ğ‘š, ğ‘˜) =

ğ‘ âˆ’1
âˆ‘ï¸

ğ‘›=0

ğ‘¥ (ğ‘› + ğ‘šğ» )ğ‘¤ (ğ‘›)ğ‘’ğ‘¥ğ‘ (âˆ’2ğœ‹ğ‘–ğ‘˜ğ‘›/ğ‘ )

(3)

for ğ‘šğ‘¡â„ time frame and ğ‘˜ğ‘¡â„ Fourier coefficient with real-valued
DT signal ğ‘¥ : Z â†’ R, sampled window function ğ‘¤ (ğ‘›) for n âˆˆ [0 :
ğ‘ âˆ’ 1] â†’ R of length ğ‘ âˆˆ N, and hop size ğ» âˆˆ N [Mller 2015].
R denotes continuous time and Z denotes discrete time. Equal to
| ğœ’ (ğ‘š, ğ‘˜)|2, spectrograms have been demonstrated to perform well
as inputs into convolutional neural networks (CNNs) [Huzaifah
2017]. Their horizontal axis is time and vertical axis is frequency.

ğ¹ğ‘ğ‘œğ‘’ ğ‘“ (ğ‘˜) =

ğ‘‡ğ‘ğ‘œğ‘’ ğ‘“ (ğ‘š) =

ğ‘š (cid:164)ğ»
ğ¹ğ‘ 

ğ‘˜ (cid:164)ğ¹ğ‘ 
ğ‘

= ğ‘˜ 44100
2048
= ğ‘š 2048 âˆ— 0.25
44100

= ğ‘˜ âˆ— 21.5 Hz

(4)

= ğ‘š âˆ— 0.012 seconds

(5)

A hop length of ğ» = ğ‘ /2 achieves a reasonable temporal res-
olution and data volume of generated spectral coefficients [Mller
2015]. Temporal resolution is important in order to detect when a
reflecting sound reaches the receiver. Therefore, we decided to use a
shorter window length ğ‘ = 2048 instead of ğ‘ = 4096 for instance.
This resulted in a shorter hop length and accepting the trade-off of
a higher temporal dimension for increased data volume.

Visual sub-network: while audio information is generally use-
ful for all three classifications tasks (Table 3) visual information
is particularly useful to aid material classification. We use Ima-
geNet [Krizhevsky et al. 2012] as a visual-based baseline to compare
to our audio and audio-visual methods. It also serves as an input into
our audio-visual merge layer. Future work will explore whether or

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

6

â€¢ Wilson, J. et al

not another image classification method is better suited as a baseline
and to fuse with audio.

Merge layer: we evaluated concatenation and multi-modal fac-
torized bilinear (MFB) pooling [Yu et al. 2017] to fuse audio and
visual fully connected layers. Concatenation of the two vectors
serves as a straightforward baseline. MFB allows for additional
learning in the form of a weighted projection matrix factorized into
two low-rank matrices.

ğ‘§ğ‘– = ğ‘¥ğ‘‡ğ‘Šğ‘–ğ‘¦ = ğ‘¥ğ‘‡ ğ‘ˆğ‘–ğ‘‰ ğ‘‡

(6)
where k is the factor or latent dimensionality with index i of the
factorized matrices, â—¦ is the Hadmard product or element-wise mul-
tiplication, and 1 âˆˆ Rğ‘˜ is an all-one vector.

ğ‘– ğ‘¥ â—¦ ğ‘‰ ğ‘‡

ğ‘– ğ‘¦ = 1

ğ‘‡ (ğ‘ˆ ğ‘‡

ğ‘– ğ‘¦)

4.1 Loss Function
For open/closed predictions, categorical cross entropy loss (Equa-
tion 4.1) is used instead of binary if estimating the extent of the
surface opening (e.g. all the way open, halfway, or closed). A regres-
sion model is not used for depth estimation because ground truth
data is collected in discrete 0.5 m or 1 ft increments within the free
field for better noise reduction [Egan 1988]. The Softmax function
is used for output activations.

âˆ’

ğ‘€
âˆ‘ï¸

ğ‘=1

ğ‘¦ğ‘œ,ğ‘ log(ğ‘ğ‘œ,ğ‘ )

(7)

where M is number of classes, y indicator for correct classification,
and p for predicted probability that observation (o) is of class (c).
4.2 Depth filtering and planar inpainting
The outputs of our EchoCNN inform enhancements for 3D recon-
struction (algorithm 1). If depth jumps in the reconstruction are first
classified as an open surface, then no change is required other than
filtering loose geometry and small components. Otherwise, there
is a planar discontinuity (e.g. window or mirror) that needs to be
filled. With depth estimated by EchoCNN, we filter the initial 3D
mesh to within a threshold of that depth. This gives us the plane
size needed to fill. Finally, EchoCNN classifies its surface material.

Implementation details

4.3
We implemented all EchoCNN and baseline models with Tensor-
flow [Abadi et al. 2015] and Keras [Chollet et al. 2015]. Training was
performed using a TITAN X GPU running on Ubuntu 16.04.5 LTS.
We used categorical cross entropy loss with Stochastic Gradient
Descent optimized by ADAM [Kingma and Ba 2014]. Using a batch
size of 32, remaining hyperparameters were tuned manually based
on a separate validation set. We make our real-world and synthetic
datasets available to aid future research in this area.

5 DATASETS AND APPLICATIONS
Our audio-based EchoCNN-A and audio-visual EchoCNN-AV con-
volutional neural networks are trained across nine octave bands
with center frequencies 63 Hz, 125 Hz, 250 Hz, 500 Hz, 1 kHz, 2 kHz,
4 kHz, 8 kHz, and 16 kHz. Training is done using these pulsed pure
tone impulses along with experimenter hand clap. The hold out test
data is comprised of sound sources excluded from training - white

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

ALGORITHM 1: Echoreconstruction via EchoCNN inference

Input: EchoCNN classifications (open/closed, depth, and material)

and initial 3D reconstruction (.obj and.jpg).
Output: Enhanced 3D echoreconstruction (updated .objâ€™).
Variables: .obj = initial 3D reconstruction, .objâ€™ = enhanced 3D
echoreconstruction, ğ‘– = ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘– ğ‘“ ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘– based on EchoCNN
inputs, ğ‘— = ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘›ğ‘¡ğ‘–ğ‘›ğ‘¢ğ‘–ğ‘¡ ğ‘¦ ğ‘— , ğ‘˜ = ğ‘£ğ‘’ğ‘Ÿğ‘¡ğ‘’ğ‘¥ğ‘˜ in .objâ€™

.objâ€™ = .obj
for each ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘– ğ‘“ ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘– of audiovisual input do

if ((ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘– ğ‘“ ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘– [ğ‘œğ‘ğ‘’ğ‘›/ğ‘ğ‘™ğ‘œğ‘ ğ‘’ğ‘‘ ] == ğ‘ğ‘™ğ‘œğ‘ ğ‘’ğ‘‘) and
(ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘– ğ‘“ ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘– [ğ‘šğ‘ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘ğ‘™ ] in (ğ‘”ğ‘™ğ‘ğ‘ ğ‘ ,ğ‘šğ‘–ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ )) and
(overlap(ğ‘–ğ‘šğ‘ğ‘”ğ‘’ğ‘– ,previous ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘›ğ‘¡ğ‘–ğ‘›ğ‘¢ğ‘–ğ‘¡ ğ‘¦ ğ‘— âˆ’1) < ğœ–)) then

if ğ‘ ğ‘–ğ‘šğ‘ğ‘™ğ‘– ğ‘“ ğ‘¦ğºğ‘’ğ‘œğ‘šğ‘’ğ‘¡ğ‘Ÿ ğ‘¦ then

ğ‘›ğ‘’ğ‘¤ğ¹ğ‘ğ‘ğ‘’ğ‘– = convex hull of ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘›ğ‘¡ğ‘–ğ‘›ğ‘¢ğ‘–ğ‘¡ ğ‘¦ ğ‘—
depth(ğ‘›ğ‘’ğ‘¤ğ¹ğ‘ğ‘ğ‘’ ğ‘— ) = ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘– ğ‘“ ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘– [ğ‘‘ğ‘’ğ‘ğ‘¡â„]

else

for each ğ‘£ğ‘’ğ‘Ÿğ‘¡ğ‘’ğ‘¥ğ‘˜ in convex hull of planar
ğ‘‘ğ‘–ğ‘ ğ‘ğ‘œğ‘›ğ‘¡ğ‘–ğ‘›ğ‘¢ğ‘–ğ‘¡ ğ‘¦ ğ‘— in .objâ€™ do

depth(ğ‘£ğ‘’ğ‘Ÿğ‘¡ğ‘’ğ‘¥ğ‘˜ ) = ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘– ğ‘“ ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘– [ğ‘‘ğ‘’ğ‘ğ‘¡â„]

end

end

end

end

Assumptions: Scan of initial 3D reconstruction (.obj) with partials
of object perimeter. Discontinuity detection using color and
vertices on planes based on input. Future work, define a mapping
from classifications to 3D geometry using RGB-D data, tracking, or
Iterative Closest Point (ICP) [Izadi et al. 2011].
Description: compare discontinuities in reconstructed geometry
with EchoCNN inferences. If EchoCNN classifies a hole as closed,
select planar vertices surrounding gap at EchoCNN estimated
depth, create a new mesh based on its convex hull, and assign
material based on EchoCNN estimated material.

noise, experimenter whistle, and voice. The test set contains sound
sources not in the training set to evaluate generalization.

5.1 Real and synthetic datasets
Real: training data is comprised of 1 second pulsed spectrograms
(Figure 7) from recorded pure tones, experimenter hand claps, brow-
nian noise, and pink noise (N=857). Training and test examples were
collected via video recordings and labeled for material, open/closed,
and in 1 ft depth increments based on the distance from the surface.
Nine octaves of pure tones, hand claps, and white noise cover a
disperse range of frequencies and were used to train our models.

The hold out test dataset consists of 1-second pulsed spectrograms
from recorded experimenter voice, whistle, chirp, and white noise
(N=431). Voice and whistle recordings were chosen for the hold
out test set to ease future transition to live and hands-free emitted
sounds during reconstruction. Hold out test data is excluded from
training and only evaluated during testing. While the same hold out
sets were used for visual and audio-visual evaluation, unheard is
not the same as unseen. Unheard audio can have the same visual
appearance between training and test. Other new training and test
datasets for visual and audio-visual methods will be future work.

Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction

â€¢

7

Fig. 6. Listener at different distances from sound source (from 0.5 to 3 m) in a virtual environment (left: bathroom, middle: kitchen, right: bedroom) used to
generate synthetic audio-visual data. This dataset is comprised of multiple 12-second video clips in front of reflective surfaces at increments from 0.5 m to 3 m
for 15 different sound sources. Absorption and transmission coefficients were set on materials (e.g. mirror, thick glass, ordinary glass) inside and outside of
rooms in the virtual scenes. In addition to open/closed, depth, and material, we make synthetic, unmixed reflection separation data (direct, early, or late)
available for future research.

Accuracy of Reflecting Sounds used for Open/Closed, Depth Estimation, and Material Classification in Real World Scenes

Method
kNN [Cover and Hart 1967]
Linear SVM [Bottou 2010]
SoundNet5 [Aytar et al. 2016]
SoundNet8 [Aytar et al. 2016]
EchoCNN-A (Ours)
ImageNet [Krizhevsky et al. 2012]
Acoustic Classification [Schissler et al. 2018]
EchoCNN-AV Cat (Ours)
EchoCNN-AV MFB (Ours)

Input
A
A
A
A
A
V
AV
AV
AV

Open/Closed

Depth Estimation

Shower Window Overall
21.3%
56.5%
100%
37.6%
91.7%
61.5%
39.7%
46.6%
45.2%
42.5%
46.6%
50.7%
71.8%
100%
71.2%
45.2%
96.1%
78.1%
N/A
N/A
N/A
89.5%
100%
100%
84.9%
100%
100%

1 ft
2 ft
3 ft
25%
21%
16%
41%
32%
38%
8%
71%
40%
33%
0%
92%
54%
86%
76%
0%
52%
83%
N/A
N/A N/A
73%
95% 100%
54% 100% 100%

Sound Material
Overall Glass Mirror
52.4%
47.5%
44.0%
57.1%
46.0%
51.9%
1.6%
98.4%
71.0%
16.4%
44.4%
85.7%
62.3% 92.0%
77.4%
60.7%
100%
80.6%
â€”â€”â€”- 48% * â€”â€”â€”-
100%
60.7%

100%
80.6%

100%
100%

Table 3. Multiple models (ours is EchoCNN) and baselines were evaluated for audio and audio-visual based scene reconstruction analysis. Overall, 71.2% of
held out reflecting sounds and 100% of audio-visual frames were correctly classified as an open or closed interior surface (i.e. glass shower door). Open/closed
classification is even higher for external facing windows due to outside noise. According to [Long 2014], 10 dB of exterior to interior noise reduction can be
attributed to closed compared to open windows. Example shower and window views in Figure 10. 71.8% of 1-second audio frames were correctly classified as
1 ft, 2 ft, or 3 ft away from surface based on audio alone; 89.5% when concatenating with its corresponding image. Finally, 77.4% and 100% of audio and
audio-visual inputs correctly labeled the surface material. * According to [Schissler et al. 2018], 48% of the triangles in the scene are correctly classified, where
its classification is more granular and covers more material classes.

Accuracy of Reflecting Sounds used for Classification in Controlled Experiment (Gl = Glass)
.

Depth Est. (+/- 0.5 m)

Open/Closed

Method
kNN [Cover and Hart 1967]
Linear SVM [Bottou 2010]
SoundNet5 [Aytar et al. 2016]
SoundNet8 [Aytar et al. 2016]
EchoCNN-A (Ours)
ImageNet [Krizhevsky et al. 2012]
Acoustic Classification [Schissler et al. 2018]
EchoCNN-AV Cat (Ours)
EchoCNN-AV MFB (Ours)

Input Thick Gl Thin Gl Thick Gl

A
A
A
A
A
V
AV
AV
AV

53.8%
54.7%
60.0%
60.0%
61.1%
95.8%
N/A
98.9%
100%

64.1%
63.2%
40.1%
42.6%
65.1%
80.8%
N/A
100%
100%

11.5%
11.5%
18.8%
25.0%
44.4%
83.3%
N/A
99.4%
100%

Thin Gl Material Est
21.4%
20.5%
19.1%
19.1%
44.6%
66.7%
N/A
92.2%
99.0%

66.5%
61.1%
67.4%
34.0%
68.1%
87.5%
â€“ 48% * â€“
76.6%
100%

Table 4. Multiple models (ours is EchoCNN) and baselines were evaluated for audio and audio-visual based scene reconstruction analysis. * According
to [Schissler et al. 2018], 48% of the triangles in the scene are correctly classified, where its classification is more granular and covers more material classes.
Compared to other existing methods, ours is able to correctly classify open/closed surfaces and depth estimation at nearly 100%, while achieving much higher
accuracy in material estimation as well.

Synthetic: The automated synthetic data collection was per-
formed in Unreal Engine 4.25 where SteamAudio employs a ray-
based geometric sound propagation approach, with support for
dynamic geometry using the Intel Embree CPU-based ray-tracer.
We describe similar prior work [Schissler and Manocha 2011] here
for more details on this approach. Given scene materials (e.g. carpet,

glass, painted, tile, etc.), a sound source (e.g. voice), environmental
geometry, and listener position, we generate impulse responses for
a given scene of varying sizes. From each listener, specular and
diffuse rays are randomly generated and traced into the scene. The
energy-time curve for simulated impulse response ğ‘†ğ‘“ (ğ‘¡) is the sum

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

8

â€¢ Wilson, J. et al

Fig. 7. Spectrograms from a recorded hand clap in front of an interior glass
shower door and exterior glass window. For the interior door, reflected
sounds experience intensified damping as we go from opened (left) to closed
(middle) and then from 3 feet to 1 foot depth (right). Damping increases with
fewer late reverberations and intensity increases with more early reflections.
For the exterior window, closing it decreases outside noise up to a distance.

Fig. 8. EchoCNN-A (Left) Confusion matrix to classify open/closed for an
interior glass shower door. Open predictions (86%) were more accurate than
closed (56%). (Right) Confusion matrix to classify depth from same interior
glass door. Notice that our EchoCNN is learning to differentiate distance
based on reflecting sounds from pulsed ambient waves of a smartphone.

of these rays:

ğ‘†ğ‘“ (ğ‘¡) =

âˆ‘ï¸ ğ›¿ (ğ‘¡ âˆ’ ğ‘¡ ğ‘— )ğ¼ ğ‘—,ğ‘“

(8)

where ğ¼ ğ‘—,ğ‘“ is the sound intensity for path j and frequency band f, ğ‘¡ ğ‘—
is the propagation delay time for path j, and ğ›¿ (ğ‘¡ âˆ’ ğ‘¡ ğ‘— ) is the Dirac
delta function or impulse function. As these sound rays collide in the
scene, their paths change based on absorption and scattering coeffi-
cients of the colliding objects. Common acoustic material properties
can be referenced in [Egan 1988]. We assume a sound absorption
coefficient, ğ›¼ = 1.0 for open windows.

Along with sound intensity ğ‘†ğ‘“ (ğ‘¡), a weight matrix ğ‘Šğ‘“ is com-
puted corresponding to materials within the scene. Each entry ğ‘¤ ğ‘“ ,ğ‘š
is the average number of reflections from material m for all paths
that arrived at the listener. It is defined as:

ğ‘¤ ğ‘“ ,ğ‘š =

(cid:205) ğ¼ ğ‘—,ğ‘“ ğ‘‘ ğ‘—,ğ‘š
(cid:205) ğ¼ ğ‘—,ğ‘“

(9)

where ğ‘‘ ğ‘—,ğ‘š is the number of times rays on path j collide with material
m, weighted according to the sound intensity ğ¼ ğ‘—,ğ‘“ of the path j. To
mirror our real-world data, sound source directivity was disabled.
Future work is needed to compare ambient and directed sound
sources. This data may also be used for material sound separation.

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

Fig. 9. From left to right: audio input (i.e. mel-scaled spectrogram) which
would produce the highest activation for a given depth class from 1 ft, 2 ft,
and 3 ft away from an object. Longer reverberation times tend to occur at
lower frequencies (3 ft) than at high frequencies (1 and 2 ft) due to typical
high frequency damping and absorption.

Given a 720p 30fps video walkthrough of the virtual environment
(VE) with the camera moving along a keyframed spline, we recon-
struct the virtual scene by extracting the individual frames of the
video and using Agisoft Metashape (v1.7)â€™s reconstruction pipeline
to solve for each imageâ€™s camera transform. Metashape, previously
known as PhotoScan, is considered state-of-the-art in commercial
photogrammetry software. The general process [Linder 2009] is:
create a sparse point cloud containing only keypoints and solve
for the transforms of cameras that can see the keypoints, create a
dense cloud by matching more features between keypoint-seeing
cameras and the rest of them, project the dense cloud depth data to
each camera to build per-camera depth maps, use the depth maps
to build a mesh, and create a texture map by projecting the image
frames that best see each polygon onto the mesh [Catmull 1974].

We disable motion blur of the camera in order to have more usable
frames, but real camera data generally requires blurry frames to be
removed to avoid noisy reconstructions, especially at low framerates.
For accurate visual feedback of the specular surfaces, we also enable
UE4â€™s DirectX12 ray-tracing for reflective and translucent surfaces.
We used a PC with the following specs for reconstruction: GTX
1080 GPU, i9-9900k CPU, 64gb RAM, Windows 10 x64. It takes
approximately 2 hours to process a 720p image sequence of 2000
frames from start to finish with this setup. We use three sections of
the "HQ Residential House" environment on the Unreal Marketplace
for synthetic data. The kitchen and bathroom sequences result in
about 2,000 images when they are extracted from the video at a step
size of 3, and the master bedroom has about 4,000.

5.2 Applications in VR Systems
When using a head mounted display (HMD) users are alerted when
approaching the boundaries in physical space. However, if room
setup does not accurately reflect these boundaries or changes occur
after setup, a user risks walking into unseen real-world objects such
as glass and walls. Using our method, transmitted sound from the
HMD could be used to locate physical objects and appropriately
notify the user as an added safety measure. Audio directly from the
real-world environment could also be used for depth estimation. The

Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction

â€¢

9

Fig. 10. We evaluated our method using a controlled experiment of virtual scenes (row 1: bedroom, row 2: kitchen, and row 3: bathroom) and applied to real
world scenes (row 4: physical bedroom). Column 1: input video recording reflecting sounds emitted from the camera as the scene is scanned. Column 2: an
initial reconstruction using state-of-the-art commercial MetaShape application. We tested glass, mirror, and other objects and surfaces within each scene at
different depths, materials, and open/closed. Column 3: audio-augmented echoreconstruction after depth inpainting and semantic rendering is applied during
post-processing. Failure case: while virtual bedroom window remained open (success, green), TV was not enhanced (failure, red) and in supplemental.

sounds unmixed and placed in the virtual environment, reconstruct-
ing both the scene geometry and sound sources (Figure 11). Finally,
seasonal variations in the 3D sound and visual reconstruction of a
window open in the spring and closed in the winter also enhance
the AR/VR experience. See Figure 2 and supplementary demo video.

ImageNet, a visual only baseline, is higher at 78.1% than audio-
only EchoCNN-A for open/closed classification. This is partly due
to the fact that the hold out set was to test audio generalization
(i.e. unheard sound sources). But unheard sound sources does not
guarantee unseen visual data. Images similar to those found in
training are present in test. A hold out set based on image (e.g.
different depths) should be evaluated as future work.

6 EXPERIMENTS AND RESULTS
Overall, 71.2% of hold out reflecting sounds and 100% of audio-visual
frames were correctly classified as an open or closed boundary in
the home (Table 3). 71.8% of 1 second audio frames were correctly
classified as 1 ft, 2 ft, or 3 ft away from the surface based on audio
alone; 89.5% when concatenating with its corresponding image.
Finally, 77.4% of audio and 100% of audio-visual inputs correctly
labeled the surface material.

6.1 Experimental setup
Listener (top smartphone, e.g. Galaxy Note 4) and sound source
(bottom smartphone, e.g. iPhone 6) are separated vertically by 7 cm.
Pulsed sounds are emitted 3 feet, 2 feet, and 1 feet away from the
reconstructing surface. Three feet was selected to remain in the free
field. Beyond that, there will be less noise reduction due to reflecting

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

10

â€¢ Wilson, J. et al

cases of the shower from initial RGB-based reconstructions using
either [Tanskanen et al. 2013] or [Metashape 2020]. We leave calcu-
lating the background depth as future work. We compare our 3D
reconstructions to depth estimates based on related work.

6.4 Results by source frequency and object size
We evaluate a range of source frequencies to account for different
sound wave behavior based on the size of the reconstructing objects.
For example, if an object is much smaller than the wavelength, the
sound flows around it rather than scattering [Long 2014]:

ğœ† =

ğ‘
ğ‘“

(10)

where ğœ† is wavelength (ft) of sound in air at a specific frequency, ğ‘“
is frequency (1 Hz), and ğ‘ is speed of sound in air (ft/s). Dynamically
setting source frequency based on object size would be future work.

7 CONCLUSION AND FUTURE WORK
To the best of our knowledge, our work introduces the first audio
and audio-visual techniques for enhancing scene reconstructions
that contain windows and mirrors. Our smartphone prototype and
staged EchoReconstruction pipeline emits and receives pulsed audio
from a variety of sound sources for surface detection, depth esti-
mation, and material classification. These classifications enhance
scene and object 3D reconstruction by resolving planar disconti-
nuities caused by open spaces and reflective surfaces using depth
filtering and planar filling. Our system performs well compared
to baseline methods given experiment results for real-world and
virtual scenes containing windows, mirrors, and open surfaces. We
intend to publicly release our real and synthetic audio-visual ground
truth data in addition to reflection separation data (direct, early, or
late reverberations) for future research.

This work offer many exciting possibilities in teleimmersion,
teleconferencing, and many other AR/VR applications, where the
improved quality and accessibility of scanning a room with mobile
phones can significantly enhance the presence of users. This work
can be integrated into VR headsets with cameras and microphones,
enabling a remote walk-through of a space like museums, architec-
tures, future homes, or a cultural/heritage site, for example. The
scanning process could give feedback in real time so the user wear-
ing the HMD could see the quality of the reconstruction and move
to areas with issues, like mirrors and open windows.
Future Work: To further extend this research, performing audio
emission, reception, and 3D reconstruction simultaneously in real
time instead of having a staged approach is an alternative to explore.
This approach could enable mapping classifications to 3D geome-
try more densely than fusing RGB-D, tracking, or Iterative Closest
Point (ICP) [Izadi et al. 2011]. An integrated approach, such as a
multi-scale neural network [Eigen and Fergus 2014] using audio
for coarse and visual for finer predictions, may not only be more
efficient but also more effective by using audio feedback as part of
the reconstruction code. Another possible avenue of exploration
is to investigate the impact of live audio for training and/or test-
ing our neural network variations. With a defined set of output
classes for EchoCNN, alternative baselines such as Non-Negative
Matrix Factorization (NMF), source separation techniques, and the
pYIN algorithm [Mauch and Dixon 2014] to extract the fundamental

Fig. 11. EchoCNN may also be used to reconstruct the audio of a virtual
scene from a video of a physical room. Instead of depth estimation, our
method can be trained to approximate sound source position, which is
especially useful for objects that are outside of the camera field of view.
Ground truth (green dots) and estimated (red dots) sound source placements
are shown (top). Seen and heard sound source (TV) from the video capture
is placed more accurately than unseen but heard sound sources (cradle and
laptop). Please see our supplementary video for a VR demo.

sounds in the reverberant field [Egan 1988]. Within a few feet of
the reconstructing surface also create finer detail reconstructions.
We labeled our data based on scene, sound source, and surface
properties - type of surface, material, and depth from sound source.
The training set included pulsed sounds of pure tone frequencies, a
single hand clap, brownian noise, and pink noise. The hold out test
set consisted of voice, whistle, chirp, and white noise. For rooms with
different sound-absorbing treatments, our real-world recordings
include a bedroom (e.g. carpet and painted) and bathroom (e.g. tiled).

6.2 Activation Maximization
The objective of activation maximization is to generate an input that
maximizes layer activations for a given class. This provides insights
into the types of patterns the neural network is learning. Figure 9
shows the different inputs that would maximize EchoCNN activa-
tions for depth estimation. Notice lower frequencies tend to occur at
3 ft (longer reverberation times) than at 1 and 2 ft (high frequencies)
due to typical high frequency damping and absorption.

6.3 Analysis
Using audio, we noticed noise reduction between winter and spring
due to more foliage on the trees. We also observed flutter echoes,
which can be heard as a "rattle" or "clicking" from a hand clap and
have been simulated in spatial audio [Halmrast 2019]. They became
more pronounced the closer to the wall surface in the bathroom
scene. Background UV textures are placed at a fixed 1 ft (0.3 m)
behind estimated surface depth. Audio unable to augment failure

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

frequency ğ‘“0, i.e. the frequency of the lowest partial of the sound,
are suggested as future directions. Finally, our current implemen-
tation holds out voice and whistle data, which is different from
the audio used during training. However, unheard sounds does not
equate to unseen images. Therefore, some insights can be possibly
gained by experimenting with a different training dataset for testing
audio-only, visual-only, and audio-visual methods.

REFERENCES
MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig
Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing
Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dande-
lion ManÃ©, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernanda ViÃ©gas, Oriol Vinyals, Pete Warden, Martin
Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-
Scale Machine Learning on Heterogeneous Systems. https://www.tensorflow.org/
Software available from tensorflow.org.

Ibraheem Alhashim and Peter Wonka. 2018. High Quality Monocular Depth Estimation
via Transfer Learning. arXiv e-prints abs/1812.11941, Article arXiv:1812.11941 (2018).
arXiv:1812.11941 https://arxiv.org/abs/1812.11941

Anurag Arnab, Michael Sapienza, Stuart Golodetz, Julien Valentin, Ondrej Miksik,
Shahram Izadi, and Philip Torr. 2015. Joint Object-Material Category Segmentation
from Audio-Visual Cues. In Proceedings of the British Machine Vision Conference
(BMVC). https://doi.org/10.5244/C.29.40

Yusuf Aytar, Carl Vondrick, and Antonio Torralba. 2016. Soundnet: Learning sound
representations from unlabeled video. In Advances in Neural Information Processing
Systems.

Michael Bianco, Peter Gerstoft, James Traer, Emma Ozanich, Marie Roch, Sharon
Gannot, and Charles Deledalle. 2019. Machine learning in acoustics: Theory and
applications. The Journal of the Acoustical Society of America 146 (11 2019), 3590â€“3628.
https://doi.org/10.1121/1.5133944

LÃ©on Bottou. 2010. Large-Scale Machine Learning with Stochastic Gradient Descent,
In Proceedings of the 19th International Conference on Computational Statistics
(COMPSTATâ€™2010), Yves Lechevallier and Gilbert Saporta (Eds.). Proc. of COMPSTAT,
177â€“187. https://doi.org/10.1007/978-3-7908-2604-3_16

Edwin Catmull. 1974. A subdivision algorithm for computer display of curved surfaces.
Technical Report. UTAH UNIV SALT LAKE CITY SCHOOL OF COMPUTING.
Rohan Chabra, Julian Straub, Chris Sweeney, Richard A. Newcombe, and Henry Fuchs.
2019. StereoDRNet: Dilated Residual Stereo Net. CoRR abs/1904.02251 (2019).
arXiv:1904.02251 http://arxiv.org/abs/1904.02251

FranÃ§ois Chollet et al. 2015. Keras. https://keras.io.
T. Cover and P. Hart. 1967. Nearest neighbor pattern classification. IEEE Transactions
on Information Theory 13 (01 1967), 21â€“27. https://doi.org/10.1109/TIT.1967.1053964
Michael Cowling and Renate Sitte. 2003. Comparison of techniques for environmental
sound recognition. Pattern Recognition Letters 24, 15 (2003), 2895 â€“ 2907. https:
//doi.org/10.1016/S0167-8655(03)00147-8

Marco Crocco, Andrea Trucco, and Alessio Del Bue. 2016. Uncalibrated 3D Room
Reconstruction from Sound. CoRR abs/1606.06258 (2016). arXiv:1606.06258 http:
//arxiv.org/abs/1606.06258

Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. 2017a. ScanNet: Richly-annotated 3D Reconstructions of Indoor
Scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE.

Angela Dai, Matthias NieÃŸner, Michael ZollhÃ¶fer, Shahram Izadi, and Christian Theobalt.
2017b. BundleFusion: real-time globally consistent 3D reconstruction using on-the-
fly surface re-integration. ACM Transactions on Graphics 36 (07 2017), 1. https:
//doi.org/10.1145/3072959.3126814

M. D. Egan. 1988. Architectural Acoustics. McGraw-Hill Custom Publishing.
David Eigen and Rob Fergus. 2014. Predicting Depth, Surface Normals and Semantic
Labels with a Common Multi-Scale Convolutional Architecture. CoRR abs/1411.4734
(2014). arXiv:1411.4734 http://arxiv.org/abs/1411.4734

Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim,
William T. Freeman, and Michael Rubinstein. 2018. Looking to Listen at the Cocktail
Party: A Speaker-Independent Audio-Visual Model for Speech Separation. CoRR
abs/1804.03619 (2018). arXiv:1804.03619 http://arxiv.org/abs/1804.03619

Jort Gemmeke, Daniel Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Moore,
Manoj Plakal, and Marvin Ritter. 2017. Audio Set: An ontology and human-labeled
dataset for audio events. In Proc. IEEE ICASSP 2017. New Orleans, LA, 776â€“780.
https://doi.org/10.1109/ICASSP.2017.7952261

Daniel Godoy, Bashima Islam, Stephen Xia, Md Tamzeed Islam, Rishikanth Chan-
drasekaran, Yen-Chun Chen, Shahriar Nirjon, Peter Kinget, and Xiaofan Jiang. 2018.
PAWS: A Wearable Acoustic System for Pedestrian Safety. 2018 IEEE/ACM Third

Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction

â€¢

11

International Conference on Internet-of-Things Design and Implementation (IoTDI),
237â€“248. https://doi.org/10.1109/IoTDI.2018.00031

Stuart Golodetz*, Michael Sapienza*, Julien P C Valentin, Vibhav Vineet, Ming-Ming
Cheng, Anurag Arnab, Victor A Prisacariu, Olaf KÃ¤hler, Carl Yuheng Ren, David W
Murray, Shahram Izadi, and Philip H S Torr. 2015. SemanticPaint: A Framework for
the Interactive Segmentation of 3D Scenes. Technical Report TVG-2015-1. Department
of Engineering Science, University of Oxford. Released as arXiv e-print 1510.03727.
Tor Halmrast. 2019. A very simple way to simulate the timbre of flutter echoes in

spatial audio.

Darryl Hannan, Akshay Jain, and Mohit Bansal. 2020. ManyModalQA: Modality Dis-

ambiguation and QA over Diverse Inputs. arXiv:2001.08034 [cs.CL]

Muhammad Huzaifah. 2017. Comparison of Time-Frequency Representations for
Environmental Sound Classification using Convolutional Neural Networks. CoRR
abs/1706.07156 (2017). arXiv:1706.07156 http://arxiv.org/abs/1706.07156

G. Hwang, Seohyeon Kim, and Hyeon-Min Bae. 2019. Bat-G net: Bat-inspired High-

Resolution 3D Image Reconstruction using Ultrasonic Echoes. In NeurIPS.

Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe,
Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
and Andrew Fitzgibbon. 2011. KinectFusion: Real-Time 3D Reconstruction and
Interaction Using a Moving Depth Camera. In Proceedings of the 24th Annual ACM
Symposium on User Interface Software and Technology (Santa Barbara, California,
USA) (UIST â€™11). Association for Computing Machinery, New York, NY, USA, 559â€“568.
https://doi.org/10.1145/2047196.2047270

Doug L. James, Jernej BarbiÄ, and Dinesh K. Pai. 2006. Precomputed Acoustic Transfer:
Output-sensitive, Accurate Sound Generation for Geometrically Complex Vibration
Sources. In ACM SIGGRAPH 2006 Papers (Boston, Massachusetts) (SIGGRAPH â€™06).
ACM, New York, NY, USA, 987â€“995. https://doi.org/10.1145/1179352.1141983
Martin Kada and Laurence Mckinley. 2009. 3D building reconstruction from LiDAR
based on a cell decomposition approach. International Archives of the Photogramme-
try, Remote Sensing and Spatial Information Sciences 38 (09 2009).

Hansung Kim, Luca Remaggi, Philip J. B. Jackson, Filippo Maria Fazi, and Adrian
Hilton. 2017. 3D Room Geometry Reconstruction Using Audio-Visual Sensors. 2017
International Conference on 3D Vision (3DV), 621â€“629. https://doi.org/10.1109/3DV.
2017.00076

Hyounghun Kim, Hao Tan, and Mohit Bansal. 2020. Modality-Balanced Models for

Visual Dialogue. arXiv:2001.06354 [cs.CL]

Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo
Ha, and Byoung-Tak Zhang. 2016. Multimodal Residual Learning for Visual QA.
arXiv:1606.01455 [cs.CV]

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization.
http://arxiv.org/abs/1412.6980 cite arxiv:1412.6980Comment: Published as a confer-
ence paper at the 3rd International Conference for Learning Representations, San
Diego, 2015.

Johannes Kopf, Fabian Langguth, Daniel Scharstein, Richard Szeliski, and Michael
Goesele. 2013. Image-Based Rendering in the Gradient Domain. ACM Transactions on
Graphics (TOG) 32 (11 2013), 199:1â€“199:9. https://doi.org/10.1145/2508363.2508369
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Classification
with Deep Convolutional Neural Networks.
In Advances in Neural Information
Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.).
Curran Associates, Inc., 1097â€“1105. http://papers.nips.cc/paper/4824-imagenet-
classification-with-deep-convolutional-neural-networks.pdf

Kevin Lai, Liefeng Bo, Xiaofeng Ren, and Dieter Fox. 2011. A Large-Scale Hierarchical
Multi-View RGB-D Object Dataset. Proceedings - IEEE International Conference on
Robotics and Automation, 1817â€“1824. https://doi.org/10.1109/ICRA.2011.5980382
Colin Lea, Michael D. Flynn, RenÃ© Vidal, Austin Reiter, and Gregory D. Hager. 2016.
Temporal Convolutional Networks for Action Segmentation and Detection. CoRR
abs/1611.05267 (2016). arXiv:1611.05267 http://arxiv.org/abs/1611.05267

Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for Non-negative Matrix
Factorization. In Proceedings of the 13th International Conference on Neural Informa-
tion Processing Systems (Denver, CO) (NIPSâ€™00). MIT Press, Cambridge, MA, USA,
535â€“541. http://dl.acm.org/citation.cfm?id=3008751.3008829

David B. Lindell, Gordon Wetzstein, and Vladlen Koltun. 2019. Acoustic Non-Line-Of-
Sight Imaging. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 6773â€“6782. https://doi.org/10.1109/CVPR.2019.00694
Wilfried Linder. 2009. Digital photogrammetry. Vol. 1. Springer.
Marshall Long. 2014. Architectural Acoustics (2nd. ed.). Academic Press.
Ilya Lysenkov, Victor Eruhimov, and Gary R. Bradski. 2012. Recognition and Pose
Estimation of Rigid Transparent Objects with a Kinect Sensor. In Robotics: Science
and Systems.

Wenguang Mao, Mei Wang, and Lili Qiu. 2018. AIM: Acoustic Imaging on a Mobile. In
Proceedings of the 16th Annual International Conference on Mobile Systems, Applica-
tions, and Services (Munich, Germany) (MobiSys â€™18). Association for Computing
Machinery, New York, NY, USA, 468â€“481. https://doi.org/10.1145/3210240.3210325
Matthias Mauch and Simon Dixon. 2014. PYIN: A fundamental frequency estimator
using probabilistic threshold distributions. 2014 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 659â€“663. https://doi.org/10.1109/

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

Auston Sterling, Justin Wilson, Sam Lowe, and Ming C. Lin. 2018. ISNN: Impact Sound
Neural Network for Audio-Visual Object Classification. In Computer Vision â€“ ECCV
2018, Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.).
Springer International Publishing, Cham, 578â€“595.

Ivan E. Sutherland. 1968. A head-mounted three dimensional display. In AFIPS â€™68 (Fall,

part I).

Thomas L. Szabo. 2014. Chapter 12 - Nonlinear Acoustics and Imaging. In Diagnostic
Ultrasound Imaging: Inside Out (Second Edition) (second edition ed.), Thomas L.
Szabo (Ed.). Academic Press, Boston, 501 â€“ 563. https://doi.org/10.1016/B978-0-12-
396487-8.00012-4

Zhenyu Tang, Nicholas J. Bryan, Dingzeyu Li, Timothy R. Langlois, and Dinesh
Manocha. 2020. Scene-Aware Audio Rendering via Deep Acoustic Analysis. IEEE
Transactions on Visualization and Computer Graphics 26, 5 (2020), 1991â€“2001.
Petri Tanskanen, Kalin Kolev, Lorenz Meier, Federico Camposeco, Olivier Saurer, and
Marc Pollefeys. 2013. Live Metric 3D Reconstruction on Mobile Phones. Proceedings
of the IEEE International Conference on Computer Vision, 65â€“72. https://doi.org/10.
1109/ICCV.2013.15

Antoine Toisoul and Abhijeet Ghosh. 2017. Practical acquisition and rendering of
diffraction effects in surface reflectance. ACM Transactions on Graphics 36 (07 2017),
1. https://doi.org/10.1145/3072959.3126805

Nicolas Tsingos, Emmanuel Gallo, and George Drettakis. 2004. Perceptual Audio
Rendering of Complex Virtual Environments. ACM Trans. Graph. 23, 3 (Aug. 2004),
249â€“258. https://doi.org/10.1145/1015706.1015710

John Wang and Edwin Olson. 2016. AprilTag 2: Efficient and robust fiducial detection.
2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
4193â€“4198. https://doi.org/10.1109/IROS.2016.7759617

M.J. Westoby, J. Brasington, N.F. Glasser, M.J. Hambrey, and J.M. Reynolds. 2012.
â€˜Structure-from-Motionâ€™ photogrammetry: A low-cost, effective tool for geoscience
applications. Geomorphology 179 (2012), 300 â€“ 314.
https://doi.org/10.1016/j.
geomorph.2012.08.021

Thomas Whelan, Michael Goesele, Steven J. Lovegrove, Julian Straub, Simon Green,
Richard Szeliski, Steven Butterfield, Shobhit Verma, and Richard Newcombe. 2018.
Reconstructing Scenes with Mirror and Glass Surfaces. ACM Trans. Graph. 37, 4,
Article 102 (July 2018), 11 pages. https://doi.org/10.1145/3197517.3201319

Justin Wilson and Ming C. Lin. 2020. AVOT: Audio-Visual Object Tracking of Multiple

Objects for Robotics. In ICRA 2020.

Justin Wilson, Auston Sterling, and Ming Lin. 2019. Analyzing Liquid Pouring Sequences
via Audio-Visual Neural Networks. 2019 IEEE/RSJ International Conference on In-
telligent Robots and Systems (IROS), 7702â€“7709. https://doi.org/10.1109/IROS40897.
2019.8968118

Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and
Jianxiong Xiao. 2015. 3D ShapeNets: A deep representation for volumetric shapes.
Proceedings of 28th IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 1912â€“1920. https://doi.org/10.1109/CVPR.2015.7298801

Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. 2017. Multi-modal Factorized Bi-
linear Pooling with Co-Attention Learning for Visual Question Answering. CoRR
abs/1708.01471 (2017). arXiv:1708.01471 http://arxiv.org/abs/1708.01471

Ruo Zhang, Ping-Sing Tsai, J. E. Cryer, and M. Shah. 1999. Shape-from-shading: a
survey. IEEE Transactions on Pattern Analysis and Machine Intelligence 21, 8 (Aug
1999), 690â€“706. https://doi.org/10.1109/34.784284

Yu Zhang, Mao Ye, Dinesh Manocha, and Ruigang Yang. 2017b. 3D Reconstruction in
the Presence of Glass and Mirrors by Acoustic and Visual Fusion. IEEE Transactions
on Pattern Analysis and Machine Intelligence PP (07 2017), 1â€“1. https://doi.org/10.
1109/TPAMI.2017.2723883

Zhoutong Zhang, Jiajun Wu, Qiujia Li, Zhengjia Huang, James Traer, Josh H. McDer-
mott, Joshua B. Tenenbaum, and William T. Freeman. 2017a. Generative Modeling
of Audible Shapes for Object Perception. 2017 IEEE International Conference on
Computer Vision (ICCV) (2017), 1260â€“1269.

Changxi Zheng and Doug L. James. 2011. Toward High-quality Modal Contact Sound.
ACM Trans. Graph. 30, 4, Article 38 (July 2011), 12 pages. https://doi.org/10.1145/
2010324.1964933

12

â€¢ Wilson, J. et al

ICASSP.2014.6853678

Ravish Mehra, Atul Rungta, Abhinav Golas, Ming Lin, and Dinesh Manocha. 2015.
WAVE: Interactive Wave-based Sound Propagation for Virtual Environments. Vi-
sualization and Computer Graphics, IEEE Transactions on 21 (04 2015), 434â€“442.
https://doi.org/10.1109/TVCG.2015.2391858

Metashape. 2020. AgiSoft Metashape Standard. https://www.agisoft.com/downloads/

installer/

BÃ¼chler Michael, Allegro Silvia, Stefan Launer, and Norbert Dillier. 2005. Sound Classi-
fication in Hearing Aids Inspired by Auditory Scene Analysis. EURASIP Journal on
Advances in Signal Processing 18 (01 2005). https://doi.org/10.1155/ASP.2005.2991
Meinard Mller. 2015. Fundamentals of Music Processing: Audio, Analysis, Algorithms,

Applications (1st ed.). Springer Publishing Company, Incorporated.

Arun Asokan Nair, Austin Reiter, Changxi Zheng, and Shree Nayar. 2019. Audiovisual
Zooming: What You See Is What You Hear. In Proceedings of the 27th ACM Interna-
tional Conference on Multimedia (Nice, France) (MM â€™19). ACM, New York, NY, USA,
1107â€“1118. https://doi.org/10.1145/3343031.3351010

Richard Newcombe, Andrew Davison, Shahram Izadi, Pushmeet Kohli, Otmar Hilliges,
Jamie Shotton, Steve Hodges, David Kim, and Andrew Fitzgibbon. 2011. KinectFu-
sion: Real-time dense surface mapping and tracking. 2011 10th IEEE International
Symposium on Mixed and Augmented Reality, 127â€“136. https://doi.org/10.1109/
ISMAR.2011.6162880

Richard Newcombe, Dieter Fox, and Steven Seitz. 2015. DynamicFusion: Reconstruction
and tracking of non-rigid scenes in real-time. 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 343â€“352. https://doi.org/10.1109/CVPR.2015.
7298631

Mikael Nilsson, J.s Bartunek, JÃ¶rgen Nordberg, and I. Claesson. 2008. Human Whistle
Detection and Frequency Estimation. Image and Signal Processing, Congress on 5 (05
2008), 737â€“741. https://doi.org/10.1109/CISP.2008.415

Edwin Olson. 2011. AprilTag: A robust and flexible visual fiducial system. Proceedings
- IEEE International Conference on Robotics and Automation, 3400 â€“ 3407. https:
//doi.org/10.1109/ICRA.2011.5979561

Karol J. Piczak. 2015. ESC: Dataset for Environmental Sound Classification. In Proceed-
ings of the 23rd ACM International Conference on Multimedia (Brisbane, Australia)
(MM â€™15). ACM, New York, NY, USA, 1015â€“1018. https://doi.org/10.1145/2733373.
2806390

Zhimin Ren, Hengchin Yeh, and Ming C. Lin. 2013. Example-guided Physically Based
Modal Sound Synthesis. ACM Trans. Graph. 32, 1, Article 1 (Feb. 2013), 16 pages.
https://doi.org/10.1145/2421636.2421637

JÃ©rÃ©my Riviere, Ilya Reshetouski, Luka Filipi, and A. Ghosh. 2017. Polarization imaging
reflectometry in the wild. ACM Transactions on Graphics (TOG) 36 (2017), 1 â€“ 14.
Atul Rungta, Carl Schissler, Ravish Mehra, Chris Malloy, Ming Lin, and Dinesh Manocha.
2016. SynCoPation: Interactive Synthesis-Coupled Sound Propagation.
IEEE
Transactions on Visualization and Computer Graphics 22, 4 (April 2016), 1346â€“1355.
https://doi.org/10.1109/TVCG.2016.2518421

Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. 2014. A Dataset and Tax-
onomy for Urban Sound Research. In Proceedings of the 22Nd ACM International
Conference on Multimedia (Orlando, Florida, USA) (MM â€™14). ACM, New York, NY,
USA, 1041â€“1044. https://doi.org/10.1145/2647868.2655045

Carl Schissler, Christian Loftin, and Dinesh Manocha. 2018. Acoustic Classification and
Optimization for Multi-Modal Rendering of Real-World Scenes. IEEE Transactions
on Visualization and Computer Graphics 24 (2018), 1246â€“1259.

Carl Schissler and Dinesh Manocha. 2011. GSound: Interactive Sound Propagation for

Games. Proceedings of the AES International Conference (02 2011).

Steven Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. 2006.
A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms.
IEEE Trans Image Process 1, 519â€“528. https://doi.org/10.1109/CVPR.2006.19

YiChang Shih, Dilip Krishnan, Fredo Durand, and William Freeman. 2015. Reflection
removal using ghosting cues. 2015 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 3193â€“3201. https://doi.org/10.1109/CVPR.2015.7298939

Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. 2012.

Indoor
Segmentation and Support Inference from RGBD Images. In ECCV. 746â€“760. https:
//doi.org/10.1007/978-3-642-33715-4_54

Arjun Singh, James Sha, Karthik Narayan, Tudor Achim, and Pieter Abbeel. 2014.
BigBIRD: A large-scale 3D database of object instances. 2014 IEEE International
Conference on Robotics and Automation (ICRA), 509â€“516. https://doi.org/10.1109/
ICRA.2014.6906903

Sudipta Sinha, Johannes Kopf, Michael Goesele, Daniel Scharstein, and Richard Szeliski.
2012. Image-Based Rendering for Scenes with Reflections. ACM Transactions on
Graphics - TOG 31 (07 2012), 1â€“10. https://doi.org/10.1145/2185520.2185596

Julius Orion Smith III. 2020. Physical Audio Signal Processing. https://ccrma.stanford.

edu/~jos/pasp/.

Shuran Song, Fisher Yu, Andy Zeng, Angel Chang, Manolis Savva, and Thomas
Funkhouser. 2017. Semantic Scene Completion from a Single Depth Image. 2017
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 190â€“198.
https://doi.org/10.1109/CVPR.2017.28

ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: October 2021.

