1
2
0
2

r
a

M
8
1

]

V
C
.
s
c
[

1
v
1
5
4
0
1
.
3
0
1
2
:
v
i
X
r
a

Neural Networks for Semantic Gaze Analysis in XR Settings∗

LENA STUBBEMANN †, Department of Quality and Process Management, University of Kassel, Germany
‡†, Knowledge & Data Engineering Group, University of Kassel, Germany
DOMINIK DÜRRSCHNABEL
ROBERT REFFLINGHAUS, Department of Quality and Process Management, University of Kassel, Germany

Fig. 1. Experimental design research (right) is one example among many, where augmented and virtual reality technologies are used
in combination with eye-tracking. This work addresses how semantic gaze analysis in real-world and virtual-reality settings can be
done using convolutional neural networks (left). ©Lena Stubbemann

Virtual-reality (VR) and augmented-reality (AR) technology is increasingly combined with eye-tracking. This combination broadens

both fields and opens up new areas of application, in which visual perception and related cognitive processes can be studied in

interactive but still well controlled settings. However, performing a semantic gaze analysis of eye-tracking data from interactive

three-dimensional scenes is a resource-intense task, which so far has been an obstacle to economic use. In this paper we present a

novel approach which minimizes time and information necessary to annotate volumes of interest (VOIs) by using techniques from

object recognition. To do so, we train convolutional neural networks (CNNs) on synthetic data sets derived from virtual models using

image augmentation techniques. We evaluate our method in real and virtual environments, showing that the method can compete with

state-of-the-art approaches, while not relying on additional markers or preexisting databases but instead offering cross-platform use.

CCS Concepts: • Human-centered computing → Virtual reality; • Computing methodologies → Object recognition; Super-

vised learning by classification; Image processing; Perception.

∗©Stubbemann, L. Dürrschnabel, D., Refflinghaus R. 2021. This is the author’s version of the work. It is posted here for your personal use. Not for
redistribution. The definitive version was published in ETRA ’21 Full Papers, May 25–27, 2021, https://doi.org/10.1145/3448017.3457380
†Both authors contributed equally to the paper
‡Also with, Interdisciplinary Research Center for Information System Design, University of Kassel.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on
servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
Manuscript submitted to ACM

1

 
 
 
 
 
 
ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Stubbemann, Dürrschnabel, and Refflinghaus

Additional Key Words and Phrases: Volumes of Interest (VOI), Semantic Gaze Analysis, Synthetic Training Data, Neural Network, User

Centered Dynamic Recordings

ACM Reference Format:

Lena Stubbemann , Dominik Dürrschnabel

, and Robert Refflinghaus. 2021. Neural Networks for Semantic Gaze Analysis in XR

Settings. In 2021 Symposium on Eye Tracking Research and Applications (ETRA ’21 Full Papers), May 25–27, 2021, Virtual Event, Germany.

ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3448017.3457380

1 INTRODUCTION

Increasing attempts are being made to investigate human perception systematically and as objectively as possible using

biometrics such as eye tracking [Borgianni and Maccioni 2020]. This development goes along with the availability of

continuously improved eye-trackers and the increasing use of eye-tracking with XR environments and applications.

Thereby, visual perception and related cognitive processes can be studied in interactive but still well controlled settings.

However, for many use cases visual patterns and oculometric parameters alone are insufficient to access relevant

information about users. Rather, it must additionally be determined what subjects are looking at. The process to identify

which objects or features subjects place their visual and cognitive attention upon is referred to as semantic gaze analysis.

In a video frame the two-dimensional depiction of an object is usually refered to as region of interest. Thus, a volume

of interest is commonly defined by the intersection of three-dimensional extrusions of all regions of interest in different

video frames in [Duchowski 1997]. In this work we therefore refer to volumes of interest (VOIs) as the three-dimensional

object that emerges from this intersection, which we derive directly from the three-dimensional bodies of the studied

object.

What makes the annotation challenging is that interactive three-dimensional scenes result in user-specific gaze

videos with constantly changing perspectives on the target object. VOIs can thereby move, vanish, reappear, and change

shape, size or illumination, aggravating the semantic gaze analysis [Kurzhals et al. 2017]. As soon as participants freely

interact with dynamic three-dimensional stimuli (i.e. prototypes) in virtual or real-world settings, manual annotations

are thus still considered a standard procedure [Holmqvist 2011]. However, besides being time-intensive, this method

is also prone to evaluator effects [Kurzhals et al. 2017]. Even though some approaches try to tackle those problems,

available methods still require large resource and time investments (cf. Section 2). When it comes to resources, especially

data science approaches for semantic gaze analysis suffer from a lack of suiting annotated training data sets and thus to

often fail to be widely applied. This is particularly true when VOIs need to be annotated on a feature instead of an

object level, e.g. in product design, medical or usability tasks. Hence, the problem of inefficient annotation is regarded

as one of the biggest challenges for the economic use of eye tracking in interactive three-dimensional scenes and is

currently rendering many studies unfeasible [Pfeiffer and Renner 2014].

In this work we present a method which aims to reduce the necessary resources involved in the process of annotating

VOIs in interactive three-dimensional scenes. While other approaches either require additional motion tracking

systems [Paletta et al. 2013; Pfeiffer et al. 2016], preexisting data sets [Sattar et al. 2017; Steil et al. 2018] or manually

annotated data [Kurzhals et al. 2015; Toyama et al. 2012] to address the annotation problem, all we need is a computer

aided design (CAD) model or another virtual representation of the scene. This makes our approach particularly attractive

for all disciplines where eye-tracking studies are conducted either on virtual objects, in virtual environments or on

real objects with digital twins, which is the case for most use cases in virtual-reality (VR) and augmented-reality (AR)

settings. Our method thereby allows easy switching between a real pilot product and the virtual prototype, enabling,

beyond many, application in use cases such as experimental design research, product usability testing as well as

2

Neural Networks for Semantic Gaze Analysis in XR Settings

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

shopping or marketing tasks. To do so, we treat the VOI detection task as an image classification problem and thereby

classify the individual fixation-relevant frames of the gaze replay videos using CNNs (cf. Figure fig:cnn). In doing so, we

simplify the three-dimensional task to a two-dimensional one. Even though the neural networks is only trained with

two-dimensional images, by training it with depictions of the same VOI from all perspectives, it can also recognize

different perspectives on the same three-dimensional body. To solve the problem of insufficiently available, scene

specific, and annotated databases, we make use of the virtual model (cf. Figure 2). Furthermore, we use a generative

adversarial network (GAN) as an image augmentation technique to adapt the training data to real environmental factors,

and thus overcome the need for challenging photorealistic simulations. In this paper we will give a proof of concept of

our method, by carrying out semantic gaze analyses for eye-tracking-supported design reviews on a real coffee machine

and its virtual prototype. Among the data science based approaches, this is to our knowledge the first one that allows

VOI annotation not only on an object level but also on a product feature level, while not relying on pre-annotated

training data. The main contributions of our paper are:

Contribution 1. We show how arbitrary new training data sets for the annotation of VOIs can be built up almost

fully automatically doing image augmentations with Cycle-GAN and thereby represent the experimental world (virtual

and even real environments) sufficiently accurate.

Contribution 2. We present a machine learning approach to annotate VOIs at a feature level and only on the basis of

synthetically generated training data, which reaches state-of-the-art accuracy, while allowing cross-platform use.

2 RELATED WORK

2.1 Manual and Bounding Shape Approaches

The naive way is to annotate data on the basis of scene videos using human annotators. According to Holmqvist

[2011] this can take up to 15 times the duration of the scene video. Manual annotations of areas of interest (AOIs) and

VOIs are therefore often supported by bounding shape approaches. Most analysis software of eye-tracking hardware

vendors provide such functions, to define the delimitations of the AOIs [Simko and Vrba 2019]. These bounding shapes

are manually drawn, which is time consuming for dynamic stimuli, where they have to be defined frame by frame.

To reduce the manual bounding box definition efforts, Kurzhals et al. [2015] propose a method, which only requires

to manually draw key bounding boxes in sparse video frames between key positions. The bounding boxes of the

remaining frames are interpolated linearly. The detection of the AOIs is thereby supported by a cluster visualization.

Other approaches equip bounding shape approaches with feature tracking methods [Bertolino 2012, 2014]. However,

those approaches are just applicable for synchronized data within the same coordinate system, such as achieved from

video based experiments with remote eye tracking. When it comes to mobile eye-tracking data in 3D scenes those

approaches reach their limits. This is because either the bounding boxes have to be defined individually for each video,

or the fixation positions have to be manually transferred to annotated reference pictures.

2.2 Tracker and Model-Driven Approaches

The basis of tracker and model-driven approaches are makers or motion-tracking systems, i.e. additional hardware.

A comparatively old, but widely applied approach is the use of physical markers, such as infrared based markers.

Those are attached to plane surfaces within the experimental environment for automated mapping. For an overview

see Köhler et al. [2010]. Especially when it comes to AR applications, fiducial markers such as ARToolKit, ARTag,

AprilTag or ArUco are commonly used, as the help to integrate synthetic content into the real world view at exact

3

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Stubbemann, Dürrschnabel, and Refflinghaus

positions. This technique can also be applied to transform AOI visualizations from camera space to world space as

shown in Duchowski et al. [2020]. However, those approaches reach their limits, when it comes to interactive use

cases involving 3D objects with non-planar and undercut surfaces. Here, many markers would be necessary to ensure

their visibility from every possible viewing angles, while representing a disturbing factor as they expand the visually

perceptible content (i.e. in product design evaluations tasks). When it comes to interactive three-dimensional scenes,

geometry-based approaches [Paletta et al. 2013; Pfeiffer and Renner 2014; Pfeiffer et al. 2016] define the state-of-the-art.

They rely on annotated geometrical models of the stimulus content. The gaze data are then used to calculate the

intersection of the VOIs in the proxy model and the ray of gaze. To do so, the geometrical representation must be

aligned with the real world, which is called registration. For this an isomorphic coordinate system has to be generated

using markers or external tracking technologies.

2.3 Computer Vision and Data Science Approaches

In recent years methods from the fields of computer vision and data science became available. Using those, it is

increasingly possible to identify the visual content looked upon. Thereby, images of the scene camera represent the basis

of the semantic gaze analysis. Automated methods are usually based on images of the scene video. Some approaches

use feature extraction algorithms, such as Scale-Invariant Feature Transform (SIFT) or Speeded-Up Robust Features

(SURF), along with a classifier [Brône et al. 2011; Simko and Vrba 2019; Toyama et al. 2012]. Others rely on CNNs [Sattar

et al. 2017; Steil et al. 2018]. For a detailed overview on neural networks in object recognition refer to Zhao et al. [2019].

However, a big unsolved problem of these approaches is that all of them require a huge database of scene specific

images from various perspectives. A manual setup of such a database, is very time consuming. Yet, existing databases,

such as [Deng et al. 2009; Everingham et al. 2010; Lin et al. 2014] limit the granularity of classification to delimited

objects (e.g. car, street, tree, dog) but do not offer to distinguish between their components. As a solution Brône et al.

[2011] proposed a “training-by-looking-at”-step to be done prior to the experiments. Semi-automated approaches, such

as [Kurzhals et al. 2017; Pontillo et al. 2010; Schöning et al. 2016], on the other hand, do not require any training data.

Here, the classification is carried out immediately on the experimental data. However, the classification proposed by the

algorithm must then be presented to a human annotator, who interactively corrects proposed labels for active learning.

3 OWN APPROACH

Our approach tackles the annotation problem described in Section 1 using recent object recognition algorithms. An

overview of the proposed method is given in this section, as well as in the Figure 2 and 3. Methodological details

are than given in Sections 3.1 to 3.3. An application of the proposed method along with use case specific details on

experiments and network trainings are presented in Section 4. The system is composed of the following main steps:

(1) Use a CAD model to prepare training data for the Cyle-GAN (Figure 2).

(2) Use the Cycle-GAN to create a synthetic data set with real-world-alike appearance (Figure 2).

(3) Use the synthetic data set to train a convolutional neural network (CNN) (Figure 3).

(4) Take the experimental data and predict the VOIs with the help of the trained CNN model. (Figure 3).

The essential resource for using object recognition algorithms is a suitable database. To provide an annotated database

on a feature level, we make use of a CAD model or virtual prototype, in which we geometrically define the VOIs. In a

simulation a fixation marker is moved over all visible surfaces of the VOIs (cf. Section 3.2.1). Henceforth, the frames

exported from the simulation represent the annotated training database for the classification task (cf. Figure 2, left).

4

Neural Networks for Semantic Gaze Analysis in XR Settings

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Fig. 2. Creation of Synthetic Training Data using Cycle-GAN. In preparation of the network training simulation data and artificial
fixation data are created (phase I). Those are then used as source and target domain to train Cycle-GAN models of the following
architecture (phase II): (A) simulation image (source domain), (B) image A translated to target domain, (C) image B re-translated to
source domain (cyclic A), (D) artificial fixation data image (target domain), (E) image D translated to source domain, (F) image E
re-translated to target-domain (cyclic E). The transformation utilized for the channel-shift in our approach is the one from (A) to (B).
It is used to create a synthetic training database for later classification network training.

At the same time, we are capable to generate an approximately complete population of all possible perspectives,

viewing angles and experimental situations, while getting rid of time-consuming manual effort to create suitable

databases. However, the creation of photorealistic data is a challenging task [Mayer et al. 2018]. It is especially not

obvious which aspects of the real world are relevant and must be modeled. To overcome this obstacle, while avoiding

intense simulation efforts, we propose an image augmentation technique called Cycle-GAN to achieve photo-realistic

optics in an highly automated process (cf. Section 3.2.2). Thereby, we perform a channel shift to transform the simulation

data into images that appear to look like the pictures of the desired target domain (cf. Figure 2, right). On those synthetic

data, we train a classifier (cf. Section 3.3). The resulting CNN model is finally used to predict the fixated features during

real experiments and to annotate the VOIs (cf. Figure 3).

3.1 Preparing Data for Classification

For our approach we rely on two different data sets. The neural networks are trained on synthetic data described in the

next section. In this section we show how the experimental data, which will later be classified upon, are preprocessed.

3.1.1 Experimental Data. To prepare our experimental data for the image classification task, we rely on egocentric

videos, which are split into frames. Thus, our approach requires scene videos, which is available for all eye trackers

that include a field-of-view camera. These scene videos are usually enriched with gaze events (fixations and saccades).

The resulting visualization is called gaze replay [Holmqvist 2011]. In contrast to scene videos prepared for manual

annotations, for our approach the fixation markers should only be displayed over the actual fixation duration and

without connection paths (i.e. saccade representation). It is thus guaranteed, that only one fixation marker is contained

in each frame. However, a data export with gaze coordinates mapped to the coordinate system of the scene video

and gaze event classification can be used alternatively to the gaze replay. Subsequently, the gaze replays are divided

into individual frames. Thereby, only frames that fall within the time span of a fixation have to be considered for the

further annotation process. To perform this task, we rely on a data export, from which the times of the gaze events, or

5

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Stubbemann, Dürrschnabel, and Refflinghaus

respectively at minimum the start and end time of the fixations, can be derived. As a result, the videos recorded in the

experiments have now been decomposed into classifiable images (cf. Figure 3). We refer to these as experimental data.

3.1.2 Computational Feasibility. As the data is derived from gaze videos, we face high resolution images. Since the

image size grows quadratically, it is neither temporally nor economically or ecologically reasonable to work on full size

images. Thus, the frame size has to be vastly reduced. For this, images can be resized or cropped to relevant details.

While resizing provides additional scene information, it entails the obstacle that the relevant area, which is tagged by

the fixation marker is represented with less pixels. Thus, by resizing relevant information about the fixation marker

and its highlighting area are lost. Cropping on the other hand might ensures that information encoded in the pixels

of the relevant area remain unchanged. Additionally, in the case of cropping to relevant image sections, additional

preprocessing is provided as the fixation marker is positioned in the center of the image and bloating information

about background features are eliminated. In consideration of our task (cf. 4) and in accordance with a pretest, we

decided to crop to an image section in the vicinity of the fixation, as this section represents the currently watched

region [Holmqvist 2011]. The resolution is thereby determined by the eye tracker’s scene camera. One should always

opt for the best resolution here, as this helps to determine the watched VOI, when fixations are close to VOI borders. As

the cropping strategy is commonly used in research [Kurzhals et al. 2017; Steil et al. 2018], we adopt the pre-existing

term of thumbnails. To decide on the thumbnail size, it has to be ensured that relevant features around the viewed

region are covered. For this, the average viewing distance has to be taken into account, i.e. the further subject and object

are distanced, the more features are depicted in the recorded images and thus it can be cropped to smaller thumbnails.

We thus choose the smallest possible thumbnails, such that in the majority of cases at least one neighboring VOIs is
visible. Modern eye-tracking systems provide 𝑥- and 𝑦-coordinates of the fixation marker, i.e. the exact position of the
foveal fixation. Those can be used for cropping the images to thumbnails, such that the fixation marker is in the center

of the image.

3.2 Generation of Synthetic Training Data

In general, CNN-based classification of data is more promising, the more representative the training data set is for the

experimental data. Thus, the next sections deal with the creation of data sets to train the classifier upon. A challenge,

that our approach faces, is that we use a purely synthetic and automatically created data set to train the model, while

later classifying real experimental data. Thus, the synthetic features, which the fixation marker is placed upon in the

training stage, should represent the real-world features as closely as possible. This is achieved through a combination

of simulation (cf. Section 3.2.1) and image augmentation (cf. Section 3.2.2)

3.2.1

Simulated Data. In order to train our classification network exclusively on synthetic data, we rely on a virtual

model, which resembles the presented stimulus reasonably. In our approach a CAD model, enriched with textures, as

well as light sources and shadow simulations is built. The VOIs are defined in the virtual model using simple cubic

or cylindrical shapes suiting the volumes. Next, a marker is created, which resembles the fixation marker of the gaze

replays. This marker is then systematically moved across the surfaces of each VOI (cf. Figure 2). This is done by defining

a moving path for the marker and exporting the resulting simulation. As the VOIs resemble volumetric bodies, which

can be seen from different perspectives, usually several surfaces of the VOI have to be taken into account, certainly

those, to which the view is not constructively obstructed. If the experimental scene is designed to be interactive, surfaces

that can become visible through interactions must be taken into account, too. Additionally, the same process has to be

6

Neural Networks for Semantic Gaze Analysis in XR Settings

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

done with non-VOI surfaces such as model surroundings. Those images will later present the default class if no VOI is

looked at. The resulting simulation is exported as a set of frames to build the synthetic database.

As we are dealing with interactive three-dimensional stimuli and freely moving participants, VOIs can be seen from

various perspectives. Thus, a sufficiently large number of different viewing angles has to be created. To accommodate

different viewing directions, the virtual space camera is placed in variable positions relative to the vertical axis of the

model. Which positions have to be taken into account is strongly dependent on the movement space available to the

subjects during the trials (cf. Section 4.1.2), i.e. if subjects move around an object, it is beneficial to use uniform angular

distances. Moreover, tilting of the head can be included by rotating the simulated camera. The distance of the camera

should thereby be chosen equal to the average distance of the participant to the target objects and features recorded in

the experimental data. Furthermore, the heights of VOIs with exceptionally high attention-attracting capacity should

be generated. This viewing direction is considered additionally, knowing that people approach important features in

order to get a more direct view. For simulation detail on our use case refer to Section 4.2.1. In addition, the virtual

space camera is to be placed at mutable eye heights to take different body sizes into account. This is done with respect

to the standardized human measurements defined in DIN 33402-2 [Deutsches Institut für Normung 2005], by taking

the eye level heights of the 50-percentile and the 95-percentile for men (which correspondents closely with the 95

percentile for women) as well as the 50-percentile for women (which correspondents closely with the 5 percentile

for men) in the age span from 18-65 years into account for the simulation. Thus, a decent representation of viewing

angles from the experimental data is achieved. However, the reader is advised to check the degree of compliance of

resulting viewing angles with those of the experimental data to be analyzed. It must be considered, that the given

recommendations always depend on the individual characteristics, such as the heights of the participants, and may not

consider particularities of any given experimental setup. As this process can be conducted primarily automated the

manual effort is reduced to the initial definition of VOI and possible camera positions. In the following we refer to these

generated images as simulation data.

3.2.2

Image Augmentation. The goal of our approach is to be able to carry out VOI classifications without investing

considerable effort in the generation of a manually annotated, experiment-specific database. Therefore, we decided

to generate our training database using a simulation of the experimental scene. As noted in [Mayer et al. 2018], it is

difficult and additionally enormously time-consuming to produce realistic simulations. In accordance therewith, instead

of investing a lot of time in simulations, the virtual model is only enriched with simple elements such as textures,

lighting and shadows as described in Section 3.2.1. Subsequently, a domain transfer is carried out using generative

adversarial networks (GAN) in order to adapt the simulation images to the appearance of the experimental data . This

has the advantage to work fully automated and thus reduces the simulation effort as it does not have to depict every

aspect of reality. For this, our approach uses a method called Cycle-GAN from Zhu et al. [2017]. This method can be

used to transfer images between two different domains, in the absence of paired examples. In our case Cycle-GAN

translates an image from the domain of the simulation data (source domain), to the domain of the experimental data

(target domain). The data basis thus achieved is referred to as synthetic data (cf. Figure 2).

3.3 Image Classification

With the synthetic database at hand, we can now address the annotation problem described in Section 1. Most object

detection approaches broadly consist of an object localization combined with an image classification, which allocate

pixels to instances by means of adjacent pixels that share textures, colors, or intensities. Both tasks are particularly

7

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Stubbemann, Dürrschnabel, and Refflinghaus

Fig. 3. Visualization of the classification model training (phase III) and the flow of data up to the semantic gaze analysis (phase IV).

difficult on a feature level, as the boundaries of the individual features are often not distinguishable by hard contrasting

edges. However, we can make use of the advantage, that the eye tracking data already provide us with the exact

coordinates of the fixation relative to the gaze replay and hence we know exactly, which area of the image has to be

considered for the classification. The recently popular but computationally more expensive methods of semantic or

instance segmentation can therefore be dispensed with, as the localization of each VOI in the image is of marginal

importance while increasing computational costs. By also requiring additional simulation effort, those methods become

both, uneconomically and time consuming. We thus tackle the problem as an object recognition task.

The image classification is done using the neural network architecture ResNet50v2 from He et al. [2016]. We decided

to choose this architecture based on empirical experience and as it is a state-of-the-art approach in image classification,

as current papers such as Touvron et al. [2019] show. The general network architecture is modified such that the input

layer corresponds to the size of the thumbnails. Additionally, the final layer is replaced by a dense layer consisting of as

many neurons as the number of VOIs to be classified. This layer is equipped with a softmax activation function. The

training data is generated as described in Section 3.2.1. Emerging from the simulated viewing points and differently

sized VOIs, an over-representation of prominent VOIs in the synthetic data set is expected, resulting in unbalanced sets

of training data. To compensate for this, the smaller classes should be oversampled in the training stage. To enrich the

variety of the training data set, image augmentations are applied, such as rotations, translations, and color shifts.

4 EVALUATION

To evaluate the approach presented above, we check upon our main contribution goals (cf. Section 1), whether

C1. an almost fully automatically created synthetic database using Cycle-GAN can represent the experimental world

(virtual and even real environments) sufficiently accurate.

C2. a machine learning approach to annotate VOIs at a feature level and only on the basis of synthetic training data

reaches results that can compete with state-of-the-art approaches both in real and virtual application cases.

To do so, our approach has been applied and evaluated based on an experimental setup in the field of real and virtual

prototype testing. Thereby, the application of eye tracking is aimed to track the focus of interest of a person reviewing

8

Neural Networks for Semantic Gaze Analysis in XR Settings

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

a product. For more information on the original experiment of the study, refer to Blackert et al. [2019]. In this work, we

will only discuss the experimental information relevant for the application of our approach.

4.1 Experimental Setup and Baseline

The proposed semantic gaze analysis is applied to two data sets, both derived from the user study described in the

following subsections. Thereby, a fully automated coffee machine serves as the experimental object. While the first data

set contains eye-tracking data acquired from inspecting the physical coffee machine (real-world setting), the second

data set is derived from gaze data on a stereoscopic three-dimensional virtual prototype (virtual-reality setting). The

VOI definition is carried out manually and in accordance with the geometries of the product features defined in the

CAD model (cf. Figure 4). We perform the analyses for both settings, to show the range of application of the proposed

semantic gaze analysis, ranging from experiments in virtual environments over arbitrary XR setups to laboratory

real world experiments. It further enables us to estimate how much the annotation result is influenced by the degree

of similarity between the virtual model and the stimulus under experimental conditions. As we consider the virtual

prototype, which is derived directly from the CAD model, to be much more similar to the simulation data, it is expected

to show a better overall performance.

4.1.1 Conditions / Baseline. We investigate the performance of our proposed semantic gaze analysis approach by

comparing our algorithm to EyeSee3D, an approach proposed in [Pfeiffer and Renner 2014; Pfeiffer et al. 2016]. Similar

to our approach, it requires a three-dimensional geometric model. We thereby compare approaches both offering a

solution for the same field of applications. To receive ground truth results for a fair method comparison, we rely on the

results of manual annotation. With the ground truth being set, we calculate the weighted precision and recall as well as

the weighted F1-Score for each method. The accuracy corresponds to the recall in the weighted case.

4.1.2 User Study Design. A total of 24 participants (6 female and 18 male) between the age of 23 and 62 (M = 30.65,

SD = 9.13) took part in the experiments. First, participants are introduced to the experiments and asked for voluntary

participation and anonymized data use approval. Afterwards, a 3-point calibration of the eye-tracking system is

performed with each participant. Next, all subjects are asked to interact with the product in both the virtual and the real

settings, however in counterbalanced order. The experiments are divided into two phases. In the first phase, perception

is studied by asking the subjects to freely explore the object for 60 seconds. This includes free movements around

the machine, whereby the average distance to the object is set at around one meter for both settings. However, the
semicircle, in which one is allowed to move around the machine, is limited to ±45◦. This prevents distortion effects
in the virtual representation otherwise resulting from attempts to look behind the machine. In the second phase of

the experiments the subjects are asked about their perceptual impressions. Thereby, their attention is led to certain

product features as they have to solve tasks such as brewing coffee. Due to the free movements of the test persons

around the front and the sides of the product, we deal with subject individual scan paths and scenes. This combination

is particularly challenging to analyze.

4.1.3 Apparatus. We use Unity3D to prepare the virtual prototype on the basis of a CAD model and present it stereo-
scopically three-dimensional on a 100-inch Powerwall using two projectors with a resolution of 1920 × 1200 pixels each.
In parallel, we record the subjects eye movements with mobile eye-tracking glasses of Senso Motoric Instruments (SMI)
at a 60Hz binocular sampling rate. The scene cameras video resolution is set to 1280 × 960 pixels, the corresponding
frame rate is 24fps. To ensure a stereoscopic three-dimensional vision in the virtual setup, we equip the glasses with

9

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Stubbemann, Dürrschnabel, and Refflinghaus

Fig. 4. VOIs as defined in the case study.

Fig. 5. An obstacles for the
semantic gaze analyses are
unambiguous VOIs.

polarizing filters. Additionally, the glasses are equipped with SMIs 3D-6D head and eye-tracking system including the
optical outside-in motion tracking system OptiTrack Primex13W and the associated motion capture software OptiTrack
Motive 1.10.2. This allows the recording of the users’ head position and orientation within a defined space. To access

motion capturing and eye-tracking data we apply the SMI iView VRPN Server included in SMIs iView ETG 3D-6D

SDK 2.7 and SMI iView ETG Software 2.7. Furthermore, we use BeGaze 3.7 to classify relevant gaze events (saccades,

fixations, and blinks) and to generate video-based gaze replays.

4.1.4 Evaluation of the Tracking. When it comes to eye-tracking data quality, there are mainly two critical aspects to

judge upon. The first one is accuracy, which describes the average difference between the real stimulus position and the

measured gaze position. The second one is precision, which gives an estimation about the ability of the eye tracker to

reliably reproduce the same gaze point measurement. To give an estimation about those measures we rely on a button

on the coffee machine that participants have to revisit several times throughout the experiment. Thus the button serves
as gaze target area (cf. yellow marking in Figure 4). With a diameter of 10mm, this area corresponds to a 0.6◦ viewing
angle, which complies approximately with the accuracy specification of the tracking given by the manufacturer. In order
to only consider reliable tracking data, a 1.5◦ target area is defined around that button, in which all forced revisiting
fixations have to occur. This condition is met by 42 of the 48 data sets. For the evaluation of our approach, we will thus

only continue to consider these data sets. The inaccuracies of the remaining data sets could be attributed to a slight

slippage of the glasses, e.g. by touching them during the experiments, and can be compensated by an offset correction.

This would allow a subsequent annotation with our proposed method, but not for the live annotation using EyeSee3D.

The data exclusion is therefore retained to ensure fair comparability of the methods. In respect thereof, we also check

the motion tracking coverage rate, as the geometric-based EyeSee3D approach is strongly dependent on stable motion

tracking. For that, we examine, if poses are identified during each time span in which a fixation is detected. This is the

case for 99.65% of all fixations, which the authors perceive as sufficiently high.

4.2 Data Preparation and Network Training

Building upon the data basis discussed in the last section, we now prepare our training data for the classification task

and train the neural networks using these data.

4.2.1 Generation of Simulation Data. As described in Section 3.2.1 our simulation data are produced in reference to

the standardized human measurements. In our use case we have the display as VOI with exceptional high attention-

attracting capacity. Therefore, we further include the display height as well as 30 cm above and below in order to mimic

10

Neural Networks for Semantic Gaze Analysis in XR Settings

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

stooping for a better view on the machine. The generated images are in a resolution of 1280 × 960, as this matches the
resolution of the recorded SMI scene videos.

4.2.2 Details of the Network Trainings. When it comes to network training, the first step is cropping the images to
thumbnails. As we deal with a viewing distance of around one meter, we decided to go with a thumbnail size of 224 × 224
pixel. This guarantees that the shapes of the different VOIs as well as their surroundings are still recognizable. The data
size is thus reduced to 4.08% of the original image size, yielding in computational feasibility, as discussed in Section 3.1.2.
Subsequently, the two Cycle-GAN networks are trained as described in Section 3.2.2. Both Cycle-GANs use 1000 images

from the simulation data as source domain. The first network additionally uses 1000 images recorded in the virtual-

reality experiment as target domain, cropped to thumbnails. The second Cycle-GAN instead uses 1000 thumbnails

gathered in the real-world setting. Refer to the image augmentation section (right) of Figure 2 for a demonstration of

such a domain transfer where an image in the source domain is depicted as A and the target domain as B. We use a
preprovided implementation1 for the Cycle-GAN. Except changing the numbers of epochs to 50, we do not branch
from the standard parameters as described in the original paper [Zhu et al. 2017].

Using the so trained models, we can now generate our training data for the image classification. All images are in

thumbnail form. We rely on a total of 100,000 simulated training images. Those are augmented using both trained

Cycle-GAN models. Thus, we create 100,000 additional images in each of the two domains (real world and virtual

reality). A data set of 300,000 images (simulated data, real-world data and virtual-reality data) is consequently generated

for the upcoming training of the classifiers, compare to Figure 3. Each image is thus contained three times in the training

set, however differing in its domains. By doing so, we achieve a higher generalization of the classifying neural network.

The classification is carried out as described in Section 3.3. As training data for the network we use the data discussed

in the last paragraph. We train the ResNet50v2 network exactly as described in the original paper [He et al. 2016]. As
we use thumbnails of size 224 × 224 we do not have to modify the input layer. The output layer is replaced by a layer
consisting of as many neurons, as VOIs are to be classified. In our cases we deal with 12 classes; the 10 VOI classes

depicted in Figure 4 as well as two default classes (“coffee machine but no VOI” and “no coffee machine”). We train the

network using the Adam optimizer and a learning rate of 0.001 over 20 epochs with the sparse categorical crossentropy

as loss. Furthermore, we use popular vote to annotate the fixations as, due to sampling rates and fixation duration,

fixations consist of scanning points (EyeSee3D) or frames (CNNs).

4.2.3 Computational Costs. All our computations are carried out using a GPU with an Nvidia GeForce RTX 2060

SUPER chip and 8 GB GDDR6-RAM. The training of the Cycle-GAN networks takes around 7 hours each while training

the classification ResNet50v2 takes a total of 9 hours.

4.3 Results and Discussion

As the experiment is conducted on two different domains, i.e. in a real-world setting and a virtual-reality setting, our

evaluation is divided into two parts, compare to Table 1. As we deal with an unbalanced multi class classification

problem, we use the weighted average precision and recall (which is equivalent to the accuracy) as well as the weighted

F1-score. Thereby, the over-representation of the large classes in the test data set is compensated, i.e. only predicting

the largest class is not rewarded. This is a common technique for unbalanced multi class problems.

1https://github.com/LynnHo/CycleGAN-Tensorflow-2

11

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Stubbemann, Dürrschnabel, and Refflinghaus

Table 1. Comparison of our approach (4) to two models (1, 2) showing the performance share of the image augmentation using
Cycle-GAN and the synthetic data set, as well as a comparison to our baseline EyeSee3D (3).

Precision

Real world
Recall/
Accuracy

F1-Score

Precision

F1-Score

Virtual reality
Recall/
Accuracy

1) ResNet50v2 (human annotations)
2) ResNet50v2 (synthetic)

3) EyeSee3D
4) ResNet50v2 (synthetic + Cycle-GAN)

0.33
0.32

0.60
0.59

0.34
0.19

0.58
0.59

0.33
0.14

0.56
0.58

0.38
0.41

0.66
0.63

0.37
0.32

0.59
0.61

0.35
0.31

0.56
0.61

4.3.1 Results of the Semantic Gaze Analysis. Generally, it can be said that both approaches, our neural network

framework and the geometrical EyeSee3D approach, yield comparable results. In both settings our approach reaches

better results in recall/accuracy then EyeSee3D, while EyeSee3D yields slightly better results for the precision. Still, our

approach achieves a higher F1-score, which is used in order to balance recall and precision. Overall, the CNN-approach

performs slightly better in virtual reality then in the real world. This is the expected behavior as visual aspects of the

virtual reality are closer represented by the simulation data, while in the real-world scenario environmental influences

such as reflections complicate the task. The authors however expect both scenarios to increase the performance, if the

image augmentation using Cycle-GAN can be further improved. We draw this conclusion as we saw geometrical and

visual modifications performed by the Cycle-GANs concerning relevant features. However, as results in Section 4.3.2

demonstrate, Cycle-GAN already vastly compensates the reality deficit, which synthetic data generally suffer from.

4.3.2 Reasoning on the Synthetic Data Approach. To evaluate the performance share of the image augmentations, we

train a ResNet50v2 architecture on the unaugmented simulation data set exclusively, i.e. on the underlying 100,000

simulation images. The results are listed in Table 1. As expected, the classification performance of the so trained neural

network is significantly weaker than the performance of the network trained on the synthetic data with the Cycle-GAN

image shift. The unaugmented data set does not seem to represent the experimental data sufficiently, especially in the

real world. Contrarily, the proposed image augmentation using Cycle-GAN seems to be able to compensate these reality

deficits to a sufficient degree. We come to this conclusion, as the network trained on the augmented data outperforms

the instance without the augmented data by almost doubling its accuracy in the virtual-reality setting while improving

the results in the real world by an even larger degree.

An alternative way to overcome the reality deficit would be to generate a scene specific data basis from scratch

using hand-annotated images. As discussed before though, generating broad and high quality databases for training

CNNs manually is a time intense and economically highly inefficient task. To demonstrate this, we do a comparison

to the human annotator method, which is still the most-used way to generate training databases for neural network

approaches. Thus, we generate a database of 30,000 manually annotated images from scene videos. This process takes

around 25 hours if the annotation rate is as fast as 20 images per minute. Using the same hyper parameters as in our

ResNet50v2 classification, we train a second instance of this neural network architecture on these manually annotated

training data. Even though the generation of the training data takes around 10 hours longer than in our approach and

requires a drastically higher human effort, the so trained model only achieves an accuracy of 34% in the real world

and an accuracy of 37% in the virtual reality, compare to Table 1. It can thus be seen, that our network trained on the

synthetic data achieves a much higher accuracy at lower time input, which also makes it economically more viable.

12

Neural Networks for Semantic Gaze Analysis in XR Settings

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Therefore our empirical experiment seems to justify the assumption, that it is not efficient to manually annotate a data

basis from scratch. While it is very time consuming, the database is not broad and precise enough to train a complex

neural network such as ResNet50v2. Moreover, the network is likely to overfit on small data sets. Ideally, one would

repeat the same experiment with a manually annotated dataset of the same size as the computer generated one. In this

case, the authors expect the neural network trained on the manual annotations to outperform our approach. Because of

the high manual effort for conducting user experiments and annotating data this is however hardly feasible.

4.3.3 Discussion. Nevertheless, the results of our approach, might initially look weaker than expected from a practi-

tioners’ point of view. However, when judging the results certain aspects have to be taken into consideration. First of

all, we rely on manually annotated data as a ground truth. As human annotators are prone to biased annotations, a

perfect annotator agreement can never be hoped for. Experiments on the annotation agreement such as in [Pfeiffer et al.

2016] give reason to expect an agreement of around 80% between two human annotators as an expectable optimum. To

better understand this problem, we advise the reader to look at Figure 5. There the fixation marker is ambiguously

located between four different VOIs and default classes some of which are adjacent and others which are simultaneously

hidden due to depth effects. Thus, if the ground truth is annotated by a human, as in our case, one should never expect

to achieve an optimum of 100%. Furthermore, both approaches deal with a problem which is caused by the SMI BeGaze

fixation detection algorithm, used during data aggregation. This phenomenon is called “running fixation” and is also

described in [Pfeiffer et al. 2016]. Thereby, the gaze cursor is not stable on a single VOI during the whole fixation

duration but moves over different VOIs. Overall, it can thus be said that we can not act on the assumption of a perfect

ground truth. Even a perfect method might therefore not be able to achieve an accuracy far beyond 80%.

However, this fact alone might not explain the results. As a closer look at the metrics for each VOI class reveals, our

approach still suffers from some challenges. While some VOIs such as the display are classified to high satisfaction,

others such as the powder lid, show a weaker stability in classification. For the display this might be traced back to

high contrast edges between it and its surrounding housing. The power lid however does not have such contrasting

edges and is thus harder to distinguish from its surrounding. The classification of such non-standalone objects is

however not only a problem of our approach, but a general problem of classification using neural networks, as those

are known to work better on detached objects then on a detailed feature level. In general, the wrong assignments did

not follow from random predictions but from predicting the default classes. Moreover, it is surprising to see, is that

even though in virtual reality and real world the powder lid is not detected well, in the real world it is still more often

assigned to one of the default classes, while in virtual reality it is often mistaken for the bean lid. This indicates, that the

classification is further complicated as some shapes are only differing slightly, such as coffee powder lid and bean lid.

While this is expectable in product design as usually uniform design languages are followed, in other VR applications

this challenge might not accrue. Furthermore, VOIs might be covered behind other parts and only be partially visible

while consisting of transparent or reflecting materials. Our approach tackles this by using the simulation together with

the image augmentation technique. However, Cycle-GAN can also impair the quality of the images. An example for this

phenomena can be seen in the right example of Figure 6. The marker is on the hot water outlet that is clearly visible in

the simulated image on the left. However, in the images after the domain transfer, i.e., the middle and the right image,

the hot water outlet is barely visible because of artifacts generated by the Cycle-GAN. Still, the marker is needed for the

neural network to determine the position of gaze. A possible approach to solve this is to omit positioning the marker
directly on the image and provide the network with additional input variables for the 𝑥- and 𝑦-coordinate. However
this might not necessarily result in a better classification because of the large number of input variables for the image.

13

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Stubbemann, Dürrschnabel, and Refflinghaus

Fig. 6. Two additional examples demonstrating the domain transfer. The left image depicts the simulation while the middle and the
right image depict the domain transfer into the real-world and the VR-environment respectively.

Results nevertheless show that our work can already compete with a state-of-the-art approach in two different domains.

Furthermore the comparison reveals that, due to the different operating principals of the systems, they also struggle

in different ways. While EyeSee3D mistakes VOIs with other VOIs close by, our approach primarily mistakes VOIs

with similar appearances. Still, we are convinced that the scores of the approach have potential for being improved.

Additionally, carefully combining both approaches might further boost the performance.

Overall, the approach has high potential to be applied in agile product development and experimental prototype

testing, as those fields typically rely on virtual product models. For example it enables all of early prototype testing to

be done in virtual-reality settings while allowing switching between a real pilot product and the virtual prototype in

later stages. Thereby, the tracking devices can even be interchanged in the course of the experiments as our approach

only relies on gaze replays which are provided by all major eye-tracking vendors. This makes it possible to switch

between devices such as mobile eye tracking used in combination with Powerwalls, CAVEs or real prototypes and HMD

integrated eye tracking. Furthermore, the described fields also deal with incremental changes in products that might

have to be evaluated in iterative steps. Caused by the nature of iterative virtual product development which deals with

minor modifications of product designs between subsequent iterations, the training of the neural networks does not

have to be repeated from scratch for each new iteration. Instead the existing models from previous iterations can be

fine-tuned for the new product design changes. Some layers of the neural networks might be frozen in this context,

which resembles a recent technique known as transfer learning. Such methods help to lower the computational costs

that come with the training of such deep and complex neural networks. It can also be reported that complex, extensively

trained, neural networks may not always deliver the best results. This is especially true for Cycle-GAN, where we

achieve better results using 50 epochs instead of 200. The authors suspect that this is caused by overfitting effects that

occur from homogeneous training data. In the case of using complex architectures such as ResNet50v2 or Cycle-GAN,

we advise the reader to train the models on few epochs first to evaluate whether a training on more epochs is sensible.

4.3.4

Limitations. The proposed method can easily be applied to pre-modeled environments. However, in use cases, in

which a digital twin would have to be built first (e.g. field experiments), the method is limited. In these cases the efforts

involved in building up the digital representation might not justify leaving the previously known paths to annotate

VOIs (Section 2). Another limitation is, that while the study gave a proof of concept for two different domains, it is

not yet a full demonstration of the generalizability of the approach. The study should thus ideally be repeated with

additional use cases beyond the coffee machine. However, this requires that a new user study has to be conducted

and a database with annotated ground truth labeling has to be collected, which is a task with high manual effort. The

limitation likewise applies to the achieved precision and recall, which are not impressively high yet. Therefore, the

proposed method might not be reliable enough yet to be used in daily research projects without double-ckecking the

results. Finally, like all approaches with rigid classes, our approach shares the problem of classifying ambiguous cases,

14

Neural Networks for Semantic Gaze Analysis in XR Settings

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

i.e., cases where the marker is in-between classes. This problem can not be fully eliminated, because the annotated

ground truth struggles from the same problem.

5 CONCLUSION AND FUTURE WORK

In this work we propose a method for semantic gaze analysis using machine learning, while eliminating the resource-

intense process of human annotations. To do so, we rely on a reasonable big and scene specific data basis, which

we generate using a basic simulation together with an image augmentation technique called Cycle-GAN. Thus, we

exclusively rely on synthetic data. In doing so, our method especially suits applications for which a virtual model is

already accessible. This enables the approach to be used in all virtual and augmented reality use cases, as well as in

experiments with real products for which a virtual representation exists. We demonstrate the feasibility of our method

by applying it to annotate data derived from an experimental design study. Those are conducted in two domains, one

being a real-world and the other being a virtual-reality setup. Thereby, we show that our approach can not only be

broadly applied but can also already compete against state-of-the-art methods in semantic gaze analysis, while still

having potential for future improvements.

A particular strength of our approach is, that in comparison to other methods for semantic gaze analysis, neither

markers nor motion tracking systems are required. This minimizes the risk of errors through latencies and registration

failures. Against manual or semi-automated annotation approaches, our method furthermore generates reproducible

results while it does not contain a personal bias and is thus not prone to evaluator effects. Our approach is particularly

suited for fields of applications that can benefit from shifting between virtual, augmented and real-world settings. For

example in agile product development the built models, remain flexible to include iterated product developments and

design changes from the first CAD model through virtual prototypes up to the real product. Meanwhile, the same

methodical evaluation can be used across platforms, i.e. if the data is captured using different tracking and XR systems

such as Powerwall VRs combined with mobile eye tracking or HMDs with integrated eye tracking. Another advantage

is that the annotation can be done in real time or after the experiments with human test subjects was already conducted.

It is even possible to define or change the volumes of interest after the user study. Nevertheless, our work is to be

seen as a proof of concept. We expect potential future work to further increase the accuracy of predictions. Chances

for improving our approach beyond many are advanced image classification methods or further improving the image

augmentations techniques. All in all, this paper should be seen as a contribution in solving the problem of semantic

gaze analysis in three-dimensional interactive scenes. It furthermore enables cross-platform use of eye tracking in

virtual and mixed reality, a property that competing approaches lack.

ACKNOWLEDGMENTS

Thanks to Maximilian Stubbemann for fruitful discussions, motivating words and comments on the manuscript. We

also thank our former colleague Christian Esser for the permission to use the jointly collected experimental data.

REFERENCES

Pascal Bertolino. 2012. Sensarea: An authoring tool to create accurate clickable videos. In International Workshop on Content-Based Multimedia Indexing,

CBMI. IEEE, 1–4.

Pascal Bertolino. 2014. Sensarea, a general public video editing application. In International Conference on Image Processing, ICIP. IEEE, 3429–3431.
Lena Blackert, Christian Esser, and Robert Refflinghaus. 2019. Development of an experimental design for QFD-guided requirement validations of virtual

prototypes. In International Symposium on QFD, ISQFD. QFD Institute & International Council for QFD, 52–69.

15

ETRA ’21 Full Papers, May 25–27, 2021, Virtual Event, Germany

Stubbemann, Dürrschnabel, and Refflinghaus

Yuri Borgianni and Lorenzo Maccioni. 2020. Review of the use of neurophysiological and biometric measures in experimental design research. Artificial

Intelligence for Engineering Design, Analysis and Manufacturing 34, 2 (2020), 248–285.

Geert Brône, Bert Oben, and Toon Goedemé. 2011. Towards a more effective method for analyzing mobile eye-tracking data: integrating gaze data with

object recognition algorithms. In International workshop on pervasive eye tracking & mobile eye-based interaction, PETMEI. ACM, 53–56.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. ImageNet: A large-scale hierarchical image database. In Computer Society

Conference on Computer Vision and Pattern Recognition, CVPR. IEEE, 248–255.

Deutsches Institut für Normung. 2005. DIN 33402-2:2005-12: Ergonomics - Human body dimensions - Part 2: Values.
Andrew Duchowski, Vsevolod Peysakhovich, and Krzysztof Krejtz. 2020. Using Pose Estimation to Map Gaze to Detected Fiducial Markers. In Knowledge-

Based and Intelligent Information & Engineering Systems, KES (Procedia Computer Science, Vol. 176). Elsevier, 3771–3779.

Andrew Ted Duchowski. 1997. Gaze-Contingent Visual Communication. Ph.D. Dissertation. Texas A&M University.
Mark Everingham, Luc Van Gool, Christopher Williams, John Winn, and Andrew Zisserman. 2010. The Pascal Visual Object Classes (VOC) Challenge.

International Journal of Computer Vision 88, 2 (2010), 303–338.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity Mappings in Deep Residual Networks. In European Conference on Computer

Vision, ECCV (Lecture Notes in Computer Science, Vol. 9908). Springer, 630–645.

Kenneth Holmqvist. 2011. Eye tracking: a comprehensive guide to methods and measures. Oxford University Press.
Johannes Köhler, Alain Pagani, and Didier Stricker. 2010. Detection and Identification Techniques for Markers Used in Computer Vision. In Visualization
of Large and Unstructured Data Sets - Applications in Geospatial Planning, Modeling and Engineering, VLUDS (OASICS, Vol. 19). Schloss Dagstuhl -
Leibniz-Zentrum fuer Informatik, 36–44.

Kuno Kurzhals, Michael Burch, Thies Pfeiffer, and Daniel Weiskopf. 2015. Eye Tracking in Computer-Based Visualization. Computing in Science &

Engineering 17, 5 (2015), 64–71.

Kuno Kurzhals, Marcel Hlawatsch, Christof Seeger, and Daniel Weiskopf. 2017. Visual Analytics for Mobile Eye Tracking. IEEE Transactions on Visualization

and Computer Graphics 23, 1 (2017), 301–310.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft COCO:
Common Objects in Context. In European Conference on Computer Vision, ECCV (Lecture Notes in Computer Science, Vol. 8693). Springer, 740–755.
Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. 2018. What Makes Good Synthetic

Training Data for Learning Disparity and Optical Flow Estimation? International Journal of Computer Vision 126, 9 (2018), 942–960.

Lucas Paletta, Katrin Santner, Gerald Fritz, Heinz Mayer, and Johann Schrammel. 2013. 3D attention: measurement of visual saliency using eye tracking

glasses. In Conference on Human Factors in Computing Systems, CHI. ACM, 199–204.

Thies Pfeiffer and Patrick Renner. 2014. EyeSee3D: a low-cost approach for analyzing mobile 3D eye tracking data using computer vision and augmented

reality technology. In Eye Tracking Research and Applications, ETRA. ACM, 195–202.

Thies Pfeiffer, Patrick Renner, and Nadine Pfeiffer-Leßmann. 2016. EyeSee3D 2.0: model-based real-time analysis of mobile eye-tracking in static and

dynamic three-dimensional scenes. In Eye Tracking Research and Applications, ETRA. ACM, 189–196.

Daniel Pontillo, Thomas Kinsman, and Jeff Pelz. 2010. SemantiCode: using content similarity and database-driven matching to code wearable eyetracker

gaze data. In Eye Tracking Research and Applications, ETRA. ACM, 267–270.

Hosnieh Sattar, Andreas Bulling, and Mario Fritz. 2017. Predicting the Category and Attributes of Visual Search Targets Using Deep Gaze Pooling. In

International Conference on Computer Vision Workshops, ICCV. IEEE, 2740–2748.

Julius Schöning, Patrick Faion, and Gunther Heidemann. 2016. Pixel-wise Ground Truth Annotation in Videos - An Semi-automatic Approach for
Pixel-wise and Semantic Object Annotation. In International Conference on Pattern Recognition Applications and Methods, ICPRAM. SciTePress, 690–697.
Jakub Simko and Jakub Vrba. 2019. Screen recording segmentation to scenes for eye-tracking analysis. Multimedia Tools and Applications 78, 2 (2019),

2401–2425.

Julian Steil, Michael Xuelin Huang, and Andreas Bulling. 2018. Fixation detection for head-mounted eye tracking based on visual similarity of gaze

targets. In Eye Tracking Research and Applications, ETRA. ACM, 23:1–23:9.

Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. 2019. Fixing the train-test resolution discrepancy. In Neural Information Processing

Systems, NeurIPS. Curran Associates, Inc., 8250–8260.

Takumi Toyama, Thomas Kieninger, Faisal Shafait, and Andreas Dengel. 2012. Gaze guided object recognition using a head-mounted eye tracker. In Eye

Tracking Research and Applications, ETRA. ACM, 91–98.

Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. 2019. Object Detection With Deep Learning: A Review. IEEE Transactions on Neural

Networks and Learning Systems 30, 11 (2019), 3212–3232.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. 2017. Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. In

International Conference on Computer Vision, ICCV. IEEE, 2242–2251.

16

