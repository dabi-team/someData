1
2
0
2

y
a
M
7

]

C
H
.
s
c
[

2
v
6
7
5
9
0
.
1
0
1
2
:
v
i
X
r
a

Digital Transformations of Classrooms in Virtual Reality

Hong Gao‚àó
Efe Bozkir‚àó
hong.gao@informatik.uni-
tuebingen.de
efe.bozkir@uni-tuebingen.de
Human-Computer Interaction
University of T√ºbingen
T√ºbingen, Germany

Lisa Hasenbein
Hector Research Institute of
Education Sciences and Psychology
University of T√ºbingen
T√ºbingen, Germany
lisa.hasenbein@uni-tuebingen.de

Jens-Uwe Hahn
Hochschule der Medien Stuttgart
Stuttgart, Germany
hahn@hdm-stuttgart.de

Richard G√∂llner
Hector Research Institute of
Education Sciences and Psychology
University of T√ºbingen
T√ºbingen, Germany
richard.goellner@uni-tuebingen.de

Enkelejda Kasneci
Human-Computer Interaction
University of T√ºbingen
T√ºbingen, Germany
enkelejda.kasneci@uni-tuebingen.de

Figure 1: Immersive virtual reality classroom.

ABSTRACT
With rapid developments in consumer-level head-mounted displays
and computer graphics, immersive VR has the potential to take on-
line and remote learning closer to real-world settings. However,
the effects of such digital transformations on learners, particularly
for VR, have not been evaluated in depth. This work investigates
the interaction-related effects of sitting positions of learners, vi-
sualization styles of peer-learners and teachers, and hand-raising
behaviors of virtual peer-learners on learners in an immersive VR
classroom, using eye tracking data. Our results indicate that learners
sitting in the back of the virtual classroom may have difficulties ex-
tracting information. Additionally, we find indications that learners
engage with lectures more efficiently if virtual avatars are visualized

‚àóBoth authors contributed equally to this research.

with realistic styles. Lastly, we find different eye movement behav-
iors towards different performance levels of virtual peer-learners,
which should be investigated further. Our findings present an im-
portant baseline for design decisions for VR classrooms.

CCS CONCEPTS
‚Ä¢ Human-centered computing ‚Üí Empirical studies in HCI;
Virtual reality; ‚Ä¢ Computing methodologies ‚Üí Perception; Sim-
ulation environments.

KEYWORDS
immersive virtual reality, eye tracking, education, perception, avatars

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan
¬© 2021 Association for Computing Machinery.
This is the author‚Äôs version of the work. It is posted here for your personal use. Not
for redistribution. The definitive Version of Record was published in CHI Conference
on Human Factors in Computing Systems (CHI ‚Äô21), May 8‚Äì13, 2021, Yokohama, Japan,
https://doi.org/10.1145/3411764.3445596.

ACM Reference Format:
Hong Gao, Efe Bozkir, Lisa Hasenbein, Jens-Uwe Hahn, Richard G√∂llner,
and Enkelejda Kasneci. 2021. Digital Transformations of Classrooms in
Virtual Reality. In CHI Conference on Human Factors in Computing Systems
(CHI ‚Äô21), May 8‚Äì13, 2021, Yokohama, Japan. ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/3411764.3445596

 
 
 
 
 
 
CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

Gao and Bozkir, et al.

1 INTRODUCTION
Recently, many universities and schools have switched to online
teaching due to the COVID-19 pandemic. Online and remote learn-
ing may become more prevalent in the near future. However, one
of the disadvantages of teaching and learning in such ways com-
pared to conventional classroom-based settings is the limited social
interaction with teachers and peer-learners. As this may demoti-
vate learners in the long term, better social engagement providing
solutions such as immersive virtual reality (IVR) can be used for
teaching and learning. Next-generation VR platforms such as En-
gage1 or Mozilla Hubs2 may offer better social engagement for
learners in the virtual environments; however, the effects of such
environments on learners have to be better investigated. In addi-
tion to the opportunity to provide more efficient social engagement
configurations, VR also enables building and evaluating situations
that are difficult to set up in real life (e.g., due to the privacy-related
concerns or current availability).

While VR technology has a long history in the education do-
main [22, 48], the current availability of consumer-grade head-
mounted displays (HMDs) allows for the creation of immersive
experiences at a reasonable cost, making it possible to employ im-
mersive personalized VR experiences in classrooms in the near fu-
ture [13]. However, the digital transformations of classrooms reflect
an important and critical step when developing VR environments
for learning purposes and require further research. A unique oppor-
tunity to understand the gaze-based behavior, and consequently,
attention distribution of learners in such VR settings is provided
through the analysis of the eye movement of learners [49]. Since
some of the high-end HMDs already consist of integrated eye track-
ers, it does not require extensive effort to extract eye movement
patterns during simulations in VR. A thorough analysis of the eye
movements allows to infer information on the users going beyond
the gaze position, for example stress [24], cognitive load [12], visual
attention [11], evaluation and diagnosis of diseases [46], future gaze
locations [27], or training evaluation [35]. In the virtual classroom,
this rich source of information could even be combined with the
virtual teachers‚Äô attention, similar to real-world classrooms [21, 59],
to design more responsive and engaging learning environments.

In this study, we design an immersive VR classroom that is similar
to a real classroom, enabling students to perceive an immersive
virtual classroom experience. We focus on exploring the impact
of the digital transformation from the classroom to immersive VR
on learners by analyzing their eye movements. For this purpose,
three design factors are studied, including sitting positions of the
participating students, different visualization styles of the virtual
peer-learners and teachers, and different performance levels of
virtual peer-learners with different hand-raising behaviors. Figure 1
shows the overall design of the virtual classroom. Consequently,
our main contributions are as follows.

‚Ä¢ We design an immersive VR classroom and conduct a user
study to enable students to virtually perceive ‚Äúinteractive‚Äù
learning.

1https://engagevr.io/
2https://hubs.mozilla.com/

‚Ä¢ We analyze the effect of different sitting positions on learn-
ers, including sitting in the front and back. We find signifi-
cantly different effects in fixation and saccade durations, and
saccade amplitudes in relation to the sitting position.

‚Ä¢ We evaluate the effect of different visualization styles of
virtual avatars on learners including cartoon and realistic
styles and find significantly different effects in fixation and
saccade durations, and pupil diameters.

‚Ä¢ We assess the effect of different performance levels of virtual
peer-learners on learners by evaluating various hand-raising
percentages, and find significant effects particularly in pupil
diameters and number of eye fixations.

2 RELATED WORK
As head-mounted displays (HMDs) and related hardware become
more accessible and affordable, VR technology may become an
important factor in the educational domain, particularly given its
provided immersion and potential for teaching [18, 62]. Various
recent works on VR and education indicate that VR may offer sig-
nificant advantages for learning and teaching. For instance, based
on the post-session knowledge tests, both augmented and virtual
reality (AR/VR) are found to promote intrinsic benefits such as
increasing learners‚Äô immersion and engagement when used for
learning structural anatomy [42]. In [2], the impact of VR systems
on student achievements in engineering colleges was investigated
by evaluating the results of post-quizzes and the results show that
VR conditions present significant advantages when compared to
no-VR conditions since students improve their performance, which
indicates that VR can successfully support teaching engineering
classes. Additionally, VR was also evaluated to help teachers develop
specific skills that can be helpful in their teaching processes [33]. In
addition to teaching and learning processes, another aspect under
evaluation concerns the types of virtual environment configurations
that are used not only for learning, but also for exploring immer-
sion, motivation, and interaction. To this end, different types of VR
setups have been studied. [13] introduced an immersive VR tool to
support teaching and studying art history, which indicates, when
used for high-school students, an increased motivation towards art
history. [50] explored the possibility of using low-cost VR setups
to improve daily classroom teaching by using a smartphone-based
VR system. According to the evaluations using pre- and post tests,
the proposed VR setup helps students perform better compared
to traditional teaching using whiteboard and slides. Furthermore,
HMD-based VR environment was studied in an elementary class-
room for teachers to guide their students in exploring learning
elements in immersive virtual field trips [14]. It has been concluded
that students‚Äô motivation was enhanced after the virtual field trips.
Overall, such works imply that while increasing motivation and
engagement, different types of VR environments provide plenty of
benefits and can be used to assist learning and teaching processes
by providing users with immersive experiences.

One disadvantage of such VR and online learning tools is that
learners‚Äô motivation and performance may be affected by lack of
social interaction [26], peer accompaniment [39], or immersion [45].
Furthermore, realism in immersive environments can have vari-
ous implications [23], related to both learning and interaction. To

Digital Transformations of Classrooms in Virtual Reality

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

address these issues, several works have focused on how to pro-
vide more realistic and immersive environments. For example, [56]
discusses the design of the VR environments for classrooms by repli-
cating real learning conditions and enhancing learning through
real-time interaction between learners and instructors. Further-
more, [36] constructed virtual classmates by synthesizing previous
learners‚Äô time-anchored comments and indicates that when stu-
dents are accompanied by a small number of virtual peer-learners
built with prior learners‚Äô comments, their learning outcomes are
improved. In addition to virtual peer-learners, the presence of vir-
tual instructors may also have an impact on learning in VR. [58]
investigated this and reports that learners engaged more with the
environment and progressed further with the interaction prompts
when a virtual instructor was provided. These works and findings
indicate that the styles and types of virtual agents in the virtual
environments may have several effects on students‚Äô attention and
perception during immersion and should be taken into account.
The evaluation of real-time visual attention towards similar con-
figurations, which could be carried out using sensors such as eye
trackers, may not only help to understand learning processes but
also provide empirical insights about interactions during virtual
classes for digital transformations of classrooms in VR.

From immersion and interaction point of view, video telecon-
ferencing systems share similar goals with the VR classrooms as
such systems enable people to experience highly immersive and
interactive environments [31] and have been studied in the VR
context as well. For example, [32] proposed a video teleconference
experience using a VR headset and found that the sense of immer-
sion and feeling of presence of a remote person increases with VR.
Furthermore, different mixed reality (MR)-based 3D collaborative
mediums were studied in terms of teleconference backgrounds and
user visualization styles [29]. The real background scene and real-
istically constructed avatars promote a higher sense of co-presence.
Low-cost setups were investigated also for real-time VR telecon-
ferencing [34], as it was done for VR learning environments and
it is found that it is possible to improve image quality using head-
sets in these setups. The possibility of having low-cost setups may
become an important factor in the future when accessibility and
extensive usage of everyday VR environments for learning [2] and
interaction [56] are considered.

In general, while the visualization styles and rendering are con-
sidered to affect learners‚Äô perception and attention, in virtual learn-
ing environments particularly in IVR classrooms, other design fac-
tors are also important for attention-related tasks. For instance, [7]
has studied the effect of being closer to the teacher, being in the
teacher‚Äôs field of view (FOV), and the availability of virtual co-
learners in virtual classrooms. In particular, the authors found that
students learn more if they are closer to the teacher and by being
in the center of the teacher‚Äôs FOV. In addition, when no co-learners
or co-learners who have positive attitudes towards the lecture (e.g.,
looking at the teacher or taking notes) are available, students learn
more information about the lecture instead of the virtual room.
Gazing time was approximated according to the time students kept
the virtual teacher in their FOVs; however, real-time gaze informa-
tion was missing during the experiments. Exact gazing patterns
and different eye movement events during learning are particularly
needed for understanding moment-to-moment visual behaviors

of students. In another work, [9] studied the effect of the sitting
position on attention-deficit/hyperactivity disorder (ADHD) expe-
riencing students in such classrooms and found indications that
front-seated students are affected positively by this configuration
in terms of learning. However, similar to [7], the authors did not
have gaze information available but identified that the evaluation
of eye movements may provide additional insights during learning,
particularly in terms of real-time visual interaction, when learning
and cognitive processes are taken into consideration. In addition,
eye movements are also considered as choice of measurements
to study visual perception during learning [25, 28]. [17] and [55]
have studied attention measures and social interaction in similar
setups using continuous performance tests and head movements,
respectively. The latter work has used head movements as a proxy
for visual attention and found that head movements shift between
target and interaction partner. This finding partly supports the find-
ing of [58] that the learners‚Äô engagement increases when a virtual
instructor is presented. However, both works lack eye movement
measurements. As also reported by [55], eye movements should
be examined along with head movements to understand attention
and interaction more in-depth, since eyes can move differently. In
addition, [44] studied the relationship between performance, sense
of presence, and cybersickness, whereas [38] examined attention,
more particularly ADHD with continuous performance task in a
virtual classroom. However, both works are more in the clinical
domain, which are relatively different from an everyday classroom
setup. [51] provides a general overview more from clinical perspec-
tive. Lastly, although has not been studied extensively in VR yet,
peer-learners‚Äô engagement expressed by hand-raising behavior [10]
may also affect the attention and visual behaviors of learners in the
VR classrooms, which could be further studied.

In summary, while showing that VR could be a useful technology
to support education, the aforementioned works primarily focused
on the importance of used mediums and configurations, visualiza-
tion styles, participant locations for visual attention, engagement,
motivation, and learning of participants in VR classrooms. Yet, real-
time and moment-to-moment interactions with the environment
and visual behaviors of students in an everyday VR classroom setup
were not studied in depth. Although obtaining such information
in real-time is challenging, analyzing eye-gaze and eye movement
features can provide valuable understanding into visual attention
and interaction in a non-intrusive way, especially for designing
such classroom configurations. For instance, long fixations can be
related to the increased amount of cognitive process [30], whereas
long saccadic behaviors are related to inefficient search behav-
ior [19]. Furthermore, pupillometry is highly related to cognitive
workload [3, 4]. Such information is also argued for consideration
in IVR environments [5, 6]. In fact, when designing immersive VR
environments for digital transformations of classrooms in virtual
worlds, such features can be key to understand visual attention,
cognitive processes, and visual interactions towards different class-
room manipulations, which may also affect learning and teaching
processes. To address this research gap, we study three config-
urations in an everyday VR classroom setup including different
visualization styles of virtual avatars, sitting positions of partici-
pants, and hand-raising based performance levels of peer-learners
by using eye movement features.

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

Gao and Bozkir, et al.

(a) Back sitting participant experiencing the VR classroom.

(b) Front sitting participant experiencing the VR classroom.

(c) Cartoon-styled avatars.

(d) Realistic-styled avatars.

Figure 2: Views from the immersive virtual reality classroom.

3 METHODOLOGY
The main purpose of our study is to investigate the effects of digital
transformations of the classrooms to VR settings on learners. There-
fore, we designed a user-study to study these effects. In this section,
we discuss the participant information, apparatus, experimental
design, experiment procedure, measurements, data pre-processing
steps, and our hypotheses. Our study and data collection were ap-
proved by the institutional ethics committee at the University of
T√ºbingen (date of approval: 25/11/2019, file number: A2.5.4-106_aa)
as well as the regional council responsible for educational affairs at
the district of T√ºbingen.

workflow with 55 similar aged (ùëÄ = 11.35, ùëÜùê∑ = 0.52) sixth-grade
students (20 female, 35 male).

3.2 Apparatus
In our experiments we employed HTC Vive Pro Eye devices with a
refresh rate of 90 Hz and a field of view of 110‚ó¶. The VR environment
was designed and rendered using the Unreal Game Engine3 v4.23.1.
The screen resolution for each eye was set to 1440 √ó 1600. To collect
eye movement data, we used the integrated Tobii eye tracker with
a 120 Hz sampling rate and a default calibration with 0.5‚ó¶ ‚àí 1.1‚ó¶
accuracy.

3.1 Participants
Participants were recruited from local academic track schools via
e-mails and invitation letters. After obtaining written informed
consent from both students and their parents or legal guardians,
all students who indicated interest were admitted to the study.
381 volunteer sixth-grade students (179 female, 202 male), whose
ages range from 10 to 13 (ùëÄ = 11.51, ùëÜùê∑ = 0.56), were recruited
to participate in the experiment. Due to hardware problems or
incorrect calibration, data from 32 participants were removed. In
addition, data from 61 participants were also removed due to eye
tracker related issues including low eye tracking ratio (lower than
90%). Therefore, data from 288 participants (137 female, 151 male),
whose ages range from 10 to 13 (ùëÄ = 11.47, ùëÜùê∑ = 0.51), were used
for evaluations. We had 16 different conditions in the experiment
and the average number of participants for each condition was 18
(ùëÜùê∑ = 5.3). In addition to the actual study and data collection, we
successfully piloted both our technical setup and the experimental

3.3 Experimental Design
The virtual classroom designed in our study has 4 rows and 2
columns of desks along with chairs, as well as other objects which
typically exist in the conventional classrooms such as a board and
display. In total, there are 24 virtual peer-learners sitting on the
chairs. A virtual teacher standing in front of the classroom teaches
a ‚âà 15-minute virtual lecture to the students about computational
thinking [60]. During the lecture, the virtual teacher walks around
the podium. The virtual peer-learners and participants sit on the
chairs throughout the lecture. The lecture has four phases including
(a) topic introduction (‚âà 3 minutes), (b) knowledge input (‚âà 4.5
minutes), (c) exercises (‚âà 5.5 minutes), and (d) summary (‚âà 1.5
minutes). There are distracting behaviors from virtual peer-learners
(e.g., raising hands, turning around) in the first, second, and third
phases of the lecture.

3https://www.unrealengine.com/

Digital Transformations of Classrooms in Virtual Reality

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

Table 1: Head and eye movement event identification thresholds.

Event

Stationary HMD
Fixation
Saccade

Conditions for velocity (ùë£)
ùë£‚Ñéùëíùëéùëë < 7‚ó¶/ùë†
ùë£‚Ñéùëíùëéùëë < 7‚ó¶/ùë† and ùë£ùëîùëéùëßùëí < 30‚ó¶/ùë†
ùë£ùëîùëéùëßùëí > 60‚ó¶/ùë†

Conditions for duration (Œî)

-
100ùëöùë† < Œîùëì ùëñùë•ùëéùë°ùëñùëúùëõ < 500ùëöùë†
30ùëöùë† < Œîùë†ùëéùëêùëêùëéùëëùëí < 80ùëöùë†

In the beginning of the first phase, the teacher enters the class-
room, stays in the classroom for a while, and then leaves for ‚âà 20
seconds, giving participants the opportunity to look around and ad-
just to the virtual environment. The topic of the lecture is displayed
on the board as ‚ÄúUnderstanding how computers think‚Äù. During the
first phase, the teacher asks five simple questions to interact with
the students. Some of the peer-learners raise their hands and answer
the questions. In the second phase, the teacher explains two terms
to the students, namely, the terms ‚Äúloop‚Äù and ‚Äúsequence‚Äù. These
terms are also shown on the display. Then, the teacher asks four
questions about each term and the peer-learners raise their hands
to answer the questions. In the third phase, the teacher gives the
students two exercises to evaluate whether or not they understand
the terms correctly. For each exercise, the students have some time
to think. Then, the teacher provides the answers for each exercise,
and the peer-learners vote for the correct answer by raising their
hands. In the last phase, the teacher stands in the middle of the
classroom to summarize the lecture. No questions are asked in this
phase; therefore, none of the peer-learners raise their hands.

Our study is in between-subjects design. The participants are
located either in the front or back region of the virtual classroom.
The participants that sit in the front of the virtual classroom have
one row in front of them, whereas the participants that sit in the
back have three rows in front of them. The visualization styles
of the avatars have two levels as well, in particular cartoon and
realistic. Lastly, the hand-raising percentages, which are intended to
show the performance levels of the virtual peer-learners, have four
different levels, including 20%, 35%, 65%, and 80%. Combining all,
we have a 2√ó2√ó4 factorial design that forms 16 different conditions
in total. Participants‚Äô views from back and front sitting positions,
cartoon- and realistic-styled avatars are depicted in Figures 2 (a),
(b), (c), and (d), respectively.

3.4 Procedure
Each experimental session took ‚âà 45 minutes including preparation
time. We conducted the experiments in groups of ten participants
by assigning each participant randomly to one of the sixteen con-
ditions. Before the data assessment took place at the participating
schools, students were informed that they could drop out of the
study at any time without consequences. After a brief introduc-
tion to the experiment and the data collection process, participants
had the opportunity to acclimate with the hardware and the VR
environment.

The experiment started with the eye tracker calibration. After
calibration success, the experimenters pressed the ‚ÄúEnter‚Äù button
to start the actual experiment and data collection process, wherein
participants experienced the immersive virtual environment and
the lecture. The experiments were supposed to be carried out in one

session without breaks, mimicking thus a real classroom teaching
session, lasting about 15 minutes. At the end of the experiment, the
VR application displayed a message telling the participants to take
off their HMDs. Lastly, participants filled out questionnaires about
their experienced presence and perceived realism.

3.5 Measurements
For this work, our main focus was eye-gaze, head-pose, and pupil
related activities of the participants as these are considered to be
rich information sources, especially in VR. Fixations are the periods
during which eyes are stationary within the head while fixated on
an area of interest. Saccades, on the other hand, are the high-speed
ballistic eye movements that shift eye-gaze from one fixation to
another.

Using fixations, saccades, and pupil diameters, plenty of eye
movement features are extracted. In this study, we extracted the
number of fixations, fixation durations, saccade durations, saccade
amplitudes, and normalized pupil diameters to analyze different
conditions of the experiment. In the eye tracking literature, longer
fixation durations correspond to engaging more with the object
or increased cognitive process [30]. Fixation durations are mainly
related to cognition and attention; however, it is argued that they
are affected by the procedures that lead to learning and it is reported
that fixation durations can be used to understand learning processes
as well [43]. For instance, [15] has studied fixation patterns during
learning in simulation- and microcomputer-based laboratory and
found that simulation group had longer fixation duration, which
means more attention and deeper cognitive processing. In addition
to the fixations, longer saccade durations correspond to less efficient
scanning or searching [19], whereas longer saccade amplitudes
mean that attention is drawn from a distance [20]. Furthermore,
a larger pupil diameter is related to higher cognitive load [8]. In
addition, while being task dependent, [16] has indicated that pupil
diameter measurements in high task load correlate with individual‚Äôs
performance. However, as pupil diameter values are also affected
by the illumination, a controlled environment is needed to assess
it. In our VR setup, the illumination is controlled across different
conditions. Besides, a general overview of considering eye tracking
as a tool to enhance learning with graphics is provided in [41].

Additionally, the self-reported presence and realism were as-
sessed by questionnaires. The items in the questionnaires were
based on the conceptualizations of [54] and [37] which were devel-
oped particularly to assess students‚Äô perception of the VR classroom
situation. The experienced presence and perceived realism were
assessed via using a 4-point Likert scales ranging from 1 (‚Äúdo not
agree at all‚Äù) to 4 (‚Äúcompletely agree‚Äù) with nine (e.g., ‚ÄúI felt like
I was sitting in the virtual classroom.‚Äù or ‚ÄúI felt like the teacher
in the virtual classroom really addressed me.‚Äù) and six items (e.g.,

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

Gao and Bozkir, et al.

(a) Mean fixation durations.

(b) Mean saccade durations.

(c) Mean saccade amplitudes.

Figure 3: Results for different sitting positions. Significant differences are highlighted with * and *** for ùëù < .05 and ùëù < .001,
respectively.

‚ÄúWhat I experienced in the virtual classroom, could also happen in a
real classroom.‚Äù or ‚ÄúThe students in the virtual classroom behaved
similarly to real classmates.‚Äù), respectively.

3.6 Data Pre-processing
As the raw eye tracking data collected from the VR device does not
include fixations, saccades or similar eye movements, we first pre-
processed the data to identify these events. Detecting different eye
movements in the VR setup is a challenging task and different from
the traditional eye tracking experiments that include equipment
such as chin-rests, as participants have opportunity to move their
heads freely in VR. In the eye tracking literature, Velocity-Threshold
Identification (I-VT) method is used to classify fixations based on
velocities [52]. In the VR context, [1] applied a similar method to
detect eye movement events. We opted for a similar approach.

Before applying the I-VT, we first applied linear interpolation
for the missing gaze vectors. After the interpolation, we identified
the fixations when the HMD was stationary. However, the iden-
tification of saccades was not restricted by the HMD movement.
The used velocity and duration thresholds for the HMD movement
states, fixations, and saccades are depicted in Table 1, where the
velocities and durations are given as ùë£ and Œî, respectively. Unlike
the fixations and saccades, the pupil diameter values are reported
by the eye tracker. As raw pupil diameter values are affected by
blinks and noisy sensor readings, we smoothed and normalized the
pupil diameter readings using Savitzky-Golay filter [53] and divisive
baseline correction using a baseline duration of ‚âà 1 seconds [40],
respectively.

3.7 Hypotheses
We developed three hypotheses, each corresponds to one design
factor.

information about the lecture. However, as they have a nar-
rower field of view, particularly towards the frontal part of
the classroom, they need to shift their attention more than
the participants sitting in the back.

‚Ä¢ Hypothesis-2 (H2): We hypothesize that different visualiza-
tion styles of virtual avatars affect student visual behaviors
differently. More particularly, as students are familiar with
realistic styles in the conventional classrooms, we claim that
compared to cartoon-styled visualization condition, they at-
tend the scene shorter during fixations in the realistic-styled
visualization setting as cartoon-styled avatars are more at-
tractive to the students. Therefore, students engage with
the environment more in the cartoon-styled visualization
condition than in the realistic-styled condition.

‚Ä¢ Hypothesis-3 (H3): We hypothesize that different hand-
raising percentages of virtual peer-learners can distinctively
affect the behaviors of participants. Specifically, we antici-
pate that when relatively higher percentages of hand-raising
levels are provided, such as 65% or 80%, the participant‚Äôs
cognitive load will be higher due to the fact that many of
the peer-learners attend the lecture with a high focus. Simi-
larly, participants have more fixations in the classroom in
the higher hand-raising percentage conditions as a higher
number of hand-raising percentage creates an opportunity
for various attention and distraction points.

4 RESULTS
As we have three factors that form 16 different conditions, we
applied 3-way full-factorial analysis of variance (ANOVA) by setting
the level of significance to ùõº = 0.05 with Tukey-Kramer post-hoc
test. For the non-parametric factorial analysis, we used the Aligned
Rank Transform (ART) [61] before applying ANOVA procedures.

‚Ä¢ Hypothesis-1 (H1): We hypothesize that the different sit-
ting positions of the participants yield different effects on
the eye movements. As the participants that sit in the front
are closer to the board, displays, and the teacher, we assume
that they can attend the virtual lecture more efficiently than
participants in the back and have less difficulty extracting

4.1 Analysis on Different Sitting Positions
Different sitting positions have an impact on the mean fixation and
saccade durations, and mean saccade amplitudes. The mean fixation
durations of the front and back sitting participants are illustrated in
Figure 3 (a). The participants that sit in the back have significantly
longer mean fixation durations (ùëÄ = 222.6ùëöùë†, ùëÜùê∑ = 14.57ùëöùë†) than

Digital Transformations of Classrooms in Virtual Reality

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

(a) Mean fixation durations.

(b) Mean saccade durations.

(c) Pupil diameters.

Figure 4: Results for different avatar visualization styles. Significant differences are highlighted with * for ùëù < .05.

the participants that sit in the front (ùëÄ = 218.75ùëöùë†, ùëÜùê∑ = 13.11ùëöùë†),
with ùêπ (1, 272) = 6.7, ùëù = .01.

1112.92, ùëÜùê∑ = 245.07) than in the 80% hand-raising condition (ùëÄ =
995.49, ùëÜùê∑ = 211.98), with ùêπ (3, 272) = 3.01, ùëù = .028.

Both saccade durations and amplitudes are influenced by the sit-
ting positions and are depicted in Figures 3 (b) and (c), respectively.
The results reveal significantly longer saccade durations in the front
condition (ùëÄ = 50.23ùëöùë†, ùëÜùê∑ = 1.7ùëöùë†) than in the back condition
(ùëÄ = 47.9ùëöùë†, ùëÜùê∑ = 2.62ùëöùë†), with ùêπ (1, 272) = 73.76, ùëù < .001. Simi-
larly, the mean saccade amplitude is significantly larger in the front
condition (ùëÄ = 10.93‚ó¶, ùëÜùê∑ = 1.54‚ó¶) than in the back condition
(ùëÄ = 10.05‚ó¶, ùëÜùê∑ = 1.38‚ó¶), with ùêπ (1, 272) = 22.6, ùëù < .001.

4.2 Analysis on Different Avatar Styles
Different avatar visualization styles affect the mean fixation and
saccade durations, and pupil diameters. The results are depicted in
Figures 4 (a), (b), and (c), respectively. The mean fixation durations
are significantly longer in the cartoon-styled avatar condition (ùëÄ =
222.88ùëöùë†, ùëÜùê∑ = 14.06ùëöùë†) than in the realistic-styled avatar condi-
tion (ùëÄ = 218.6ùëöùë†, ùëÜùê∑ = 13.76ùëöùë†), with ùêπ (1, 272) = 5.27, ùëù = .022.
By contrast, the mean saccade durations are significantly shorter
in the cartoon-styled avatar condition (ùëÄ = 48.58ùëöùë†, ùëÜùê∑ = 2.66ùëöùë†)
than in the realistic-styled condition (ùëÄ = 49.3ùëöùë†, ùëÜùê∑ = 2.35ùëöùë†),
with ùêπ (1, 272) = 6.22, ùëù = .013.

The normalized mean pupil diameter, which reflects the cognitive
load, is significantly larger in the realistic-styled avatar condition
(ùëÄ = 0.94, ùëÜùê∑ = 0.16) than in the cartoon-styled avatar condition
(ùëÄ = 0.91, ùëÜùê∑ = 0.13), with ùêπ (1, 272) = 3.94, ùëù = .048.

4.3 Analysis on Different Hand-raising

Behaviors

The hand-raising behaviors of virtual peer-learners have significant
impacts on the pupil diameters and number of fixations as depicted
in Figures 5 (a) and (b), respectively. We found significant effects
on normalized mean pupil diameter values with ùêπ (3, 272) = 4.78,
ùëù = .003. Particularly, mean pupil diameter in the 80% hand-raising
condition (ùëÄ = 0.96, ùëÜùê∑ = 0.16) is significantly larger than in the
35% hand-raising condition (ùëÄ = 0.9, ùëÜùê∑ = 0.12), with ùêπ (3, 272) =
4.78, ùëù < .001. In addition, we found significant effects on number
of fixations with ùêπ (3, 272) = 3.01, ùëù = .03. More specifically, there
are notably more fixations in the 65% hand-raising condition (ùëÄ =

4.4 Analysis on Experienced Presence and

Perceived Realism

We did not find significant effects of different experimental condi-
tions on the self-reported experienced presence and perceived real-
ism. Overall, the self-reported experienced presence and perceived
realism values are in the vicinity of highest values with (ùëÄ = 2.91,
ùëÜùê∑ = 0.55) and (ùëÄ = 2.91, ùëÜùê∑ = 0.57), respectively. These mean
that even though we did not obtain statistically significant differ-
ences between conditions, the participants experienced high levels
of presence and realism in the IVR classroom environment.

5 DISCUSSION
The results show that there are significant differences in the eye
movement features between front and back sitting position condi-
tions. Firstly, participants had longer fixations in the back sitting
condition. This indicates that they had more processing time than
the participants sitting in the front, which can be related to difficulty
extracting information, similar to the relationship between task dif-
ficulty and mean fixation duration [47]. Secondly, the participants
that sit in the front had longer saccade durations and amplitudes,
which suggests that they needed to shift their attention more during
the virtual lecture. While being located closer to the lecture content,
longer saccade durations indicate that the participants sitting in the
front had less efficient scanning behavior [19] during the lecture.
We assume that this was due to the narrower field of view. These
results support our H1. When designing virtual classes, these re-
sults should be taken into account, particularly when determining
where students should be located in the classroom, depending on
the context.

Our results show consequential effects in the eye movement fea-
tures in different avatar style conditions. As mean fixation durations
are longer in the cartoon-styled visualization condition, we assume
participants found the cartoon-styled avatars more attractive and
attention-grabbing. Therefore, their fixation behaviors were longer
during the virtual lecture. On the contrary, the mean saccade dura-
tions are longer in realistic-styled conditions as the fixation dura-
tions are shorter, which is theoretically expected. Furthermore, the

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

Gao and Bozkir, et al.

(a) Pupil diameters.

(b) Number of fixations.

Figure 5: Results for different hand-raising percentages. Significant differences are highlighted with * and *** for ùëù < .05 and
ùëù < .001, respectively.

pupil diameters of the participants in the realistic-styled condition
are larger, indicating that the cognitive load of these participants
was significantly higher during the lecture, which is suggested by
the previous work [8]. This is an indication that participants may
have taken the lecture more seriously and in a more focused manner
when the visualization was realistic. These findings support our H2.
Rendering realistic-styled avatars may be computationally expen-
sive depending on the configuration. Therefore, an optimal trade-off
should be decided, taking the behavioral results into account while
designing the virtual classrooms.

Furthermore, we observe significant effects in attention towards
different hand-raising based performance levels of the peer-learners.
Particularly, the pupil diameters of the participants in the 80% con-
dition are significantly larger than the pupil diameters of the partici-
pants in the 35% condition. We interpret this to mean that when the
performance and attendance level of peer-learners was relatively
higher, the participants‚Äô cognitive load became higher, indicating
that they might pay more attention to the lecture content. This par-
tially supports our H3. In addition, a greater number of fixations
are observed in the 65% condition than in the 80% condition. We
claim that when almost all of the peer-learners participated in hand-
raising behaviors during the lecture, participants acknowledged
this information without significantly shifting their gaze. However,
this claim requires further investigation. Manipulation of different
hand-raising conditions may affect student self-concept [57], which
should be further studied as well.

In our study, the interaction and perception in the immersive VR
classroom were assessed mainly by using eye-gaze and head-pose
information. However, while the virtual teacher and peer-learners
talk in the simulations, no response or interaction by means of audio
or gestures was expected from the participants. Combining visual
perceptions and interactions with such data may provide additional
insights particularly for better interaction design in VR classrooms.
A future iteration can also evolve into an everyday virtual classroom
platform where each virtual agent is actually connected to a real
person, similar to in platforms such as Mozilla Hubs. To this end,
further design settings such as optimal seating arrangement (e.g.,
U-shape, circle shape) in addition to the sitting positions should be
investigated. Evaluation of similar configurations in online learning

platforms such as Coursera4, Udemy5, or MOOCs6 could provide
additional implications for interaction modeling. Furthermore, gaze-
based attention guidance can be considered for more interactive
VR classroom experience and it can be achieved by fine-grained
eye movement analysis focusing on short time windows instead
of complete experiments. While being out of the scope of this
paper, assessing learning outcomes and combining them with visual
interaction and scanpath behaviors from immersive VR classroom
could also offer insights for optimal VR classroom design.

6 CONCLUSION
In this work, we evaluated three major design factors of immersive
VR classrooms, namely different participant locations in the virtual
classroom, different visualization styles of virtual peer-learners and
teachers, including cartoon and realistic, and different hand-raising
behaviors of peer-learners, particularly through the analysis of
eye tracking data. Our results indicate that participants located in
the back of the virtual classroom may have difficulty extracting
information during the lecture. In addition, if the avatars in the
classroom are visualized in realistic styles, participants may attend
the lecture in a more focused manner instead of being distracted by
the visualization styles of the avatars. These findings offer valuable
insights about design decisions in the VR classroom environment.
Few indicators were obtained from the evaluation of the different
hand-raising behaviors of peer-learners, providing a general under-
standing of attention towards peer-learner performance. However,
these indicators should be further investigated and remain a focus
of future work.

ACKNOWLEDGMENTS
This research was partly supported by a grant to Richard G√∂llner
funded by the Ministry of Science, Research and the Arts of the
state of Baden-W√ºrttemberg and the University of T√ºbingen as part
of the Promotion Program of Junior Researchers. Lisa Hasenbein is
a doctoral candidate and supported by the LEAD Graduate School
& Research Network, which is funded by the Ministry of Science,

4https://www.coursera.org/
5https://www.udemy.com/
6https://www.mooc.org/

Digital Transformations of Classrooms in Virtual Reality

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

Research and the Arts of the state of Baden-W√ºrttemberg within
the framework of the sustainability funding for the projects of the
Excellence Initiative II. Authors thank Stephan Soller, Sandra Hahn,
and Sophie Fink from the Hochschule der Medien Stuttgart for their
work and support related to the immersive virtual reality classroom
used in this study.

REFERENCES
[1] Ioannis Agtzidis, Mikhail Startsev, and Michael Dorr. 2019. 360-Degree Video
Gaze Behaviour: A Ground-Truth Data Set and a Classification Algorithm for
Eye Movements. In Proceedings of the 27th ACM International Conference on
Multimedia (Nice, France). ACM, New York, NY, USA, 1007‚Äì1015. https://doi.
org/10.1145/3343031.3350947

[2] Wadee Alhalabi. 2016. Virtual reality systems enhance students‚Äô achievements
in engineering education. Behaviour & Information Technology 35 (07 2016), 1‚Äì7.
https://doi.org/10.1080/0144929X.2016.1212931

[3] Tobias Appel, Christian Scharinger, Peter Gerjets, and Enkelejda Kasneci. 2018.
Cross-subject workload classification using pupil-related measures. In Proceedings
of the 2018 ACM Symposium on Eye Tracking Research & Applications (Warsaw,
Poland). ACM, New York, NY, USA, Article 4, 8 pages. https://doi.org/10.1145/
3204493.3204531

[4] Tobias Appel, Natalia Sevcenko, Franz Wortha, Katerina Tsarava, Korbinian
Moeller, Manuel Ninaus, Enkelejda Kasneci, and Peter Gerjets. 2019. Predicting
Cognitive Load in an Emergency Simulation Based on Behavioral and Physiologi-
cal Measures. In 2019 International Conference on Multimodal Interaction (Suzhou,
China). ACM, New York, NY, USA, 154‚Äì163. https://doi.org/10.1145/3340555.
3353735

[5] Jeremy N. Bailenson, Eyal Aharoni, Andrew C. Beall, Rosanna E. Guadagno, Alek-
sandar Dimov, and Jim Blascovich. 2004. Comparing behavioral and self-report
measures of embodied agents‚Äô social presence in immersive virtual environ-
ments. In Proceedings of the 7th Annual International Workshop on Presence. The
International Society for Presence Research, Valencia, Spain, 216‚Äì223.

[6] Jeremy N. Bailenson, Andrew C. Beall, and Jim Blascovich. 2002. Gaze and task
performance in shared virtual environments. The Journal of Visualization and
Computer Animation 13, 5 (2002), 313‚Äì320. https://doi.org/10.1002/vis.297
[7] Jeremy N. Bailenson, Nick Yee, Jim Blascovich, Andrew C. Beall, Nicole Lundblad,
and Michael Jin. 2008. The Use of Immersive Virtual Reality in the Learning
Sciences: Digital Transformations of Teachers, Students, and Social Context.
Journal of the Learning Sciences 17 (2008), 102‚Äì141. https://doi.org/10.1080/
10508400701793141

[8] Jackson Beatty. 1982. Task-evoked pupillary responses, processing load, and the
structure of processing resources. Psychological Bulletin 91, 2 (1982), 276‚Äì292.
https://doi.org/10.1037/0033-2909.91.2.276

[9] Friederike Blume, Richard G√∂llner, Korbinian Moeller, Thomas Dresler, Ann-
Christine Ehlis, and Caterina Gawrilow. 2019. Do students learn better when
seated close to the teacher? A virtual classroom study considering individual
levels of inattention and hyperactivity-impulsivity. Learning and Instruction 61
(2019), 138‚Äì147. https://doi.org/10.1016/j.learninstruc.2018.10.004

[10] Ricardo B√∂heim, Tim Urdan, Maximilian Knogler, and Tina Seidel. 2020. Student
hand-raising as an indicator of behavioral engagement and its role in classroom
learning. Contemporary Educational Psychology 62 (2020), 101894. https://doi.
org/10.1016/j.cedpsych.2020.101894

[11] Efe Bozkir, David Geisler, and Enkelejda Kasneci. 2019. Assessment of Driver
Attention during a Safety Critical Situation in VR to Generate VR-Based Training.
In ACM Symposium on Applied Perception 2019 (Barcelona, Spain). ACM, New
York, NY, USA, Article 23, 5 pages. https://doi.org/10.1145/3343036.3343138
[12] Efe Bozkir, David Geisler, and Enkelejda Kasneci. 2019. Person Independent,
Privacy Preserving, and Real Time Assessment of Cognitive Load using Eye
Tracking in a Virtual Reality Setup. In 2019 IEEE Conference on Virtual Reality
and 3D User Interfaces (VR) (Osaka, Japan). IEEE, New York, NY, USA, 1834‚Äì1837.
https://doi.org/10.1109/VR.2019.8797758

[13] Andrea Casu, Lucio Davide Spano, Fabio Sorrentino, and Riccardo Scateni. 2015.
RiftArt: Bringing Masterpieces in the Classroom through Immersive Virtual
Reality. In Smart Tools and Apps for Graphics - Eurographics Italian Chapter
Conference (Verona, Italy). The Eurographics Association, Geneva, Switzerland,
77‚Äì84. https://doi.org/10.2312/stag.20151294

[14] Kun Hung Cheng and Chin Chung Tsai. 2019. A case study of immersive virtual
field trips in an elementary classroom: Students‚Äô learning experience and teacher-
student interaction behaviors. Computers & Education 140 (2019), 103600. https:
//doi.org/10.1016/j.compedu.2019.103600

[15] Kuei-Pin Chien, Cheng-Yue Tsai, Hsiu-Ling Chen, Wen-Hua Chang, and Sufen
Chen. 2015. Learning differences and eye fixation patterns in virtual and physical
science laboratories. Computers & Education 82 (2015), 191‚Äì201. https://doi.org/
10.1016/j.compedu.2014.11.023

[16] Joseph T. Coyne, Cyrus Foroughi, and Ciara Sibley. 2017. Pupil Diameter and
Performance in a Supervisory Control Task: A Measure of Effort or Individual
Differences? Proceedings of the Human Factors and Ergonomics Society Annual
Meeting 61, 1 (2017), 865‚Äì869. https://doi.org/10.1177/1541931213601689
[17] Unai D√≠az-Orueta, Cristina Garc√≠a-L√≥pez, Nerea Crespo-Egu√≠laz, Roc√≠o S√°nchez-
Carpintero, Gema Climent, and Juan Narbona. 2014. AULA virtual reality test
as an attention measure: Convergent validity with Conners‚Äô Continuous Perfor-
mance Test. Child Neuropsychology 20 (2014), 328‚Äì342. https://doi.org/10.1080/
09297049.2013.792332

[18] Laura Freina and Michela Ott. 2015. A literature review on immersive virtual
reality in education: State of the art and perspectives. In Proceedings of the
11th International Scientific Conference eLearning and Software for Education
(Bucharest, Romania). Carol I NDU Publishing House, Romania, 133‚Äì141. https:
//doi.org/10.12753/2066-026X-15-020

[19] Joseph H. Goldberg and Xerxes P. Kotval. 1999. Computer interface evaluation
using eye movements: methods and constructs. International Journal of Industrial
Ergonomics 24, 6 (1999), 631‚Äì645. https://doi.org/10.1016/S0169-8141(98)00068-7
[20] Joseph H. Goldberg, Mark J. Stimson, Marion Lewenstein, Neil Scott, and Anna M.
Wichansky. 2002. Eye Tracking in Web Search Tasks: Design Implications. In
Proceedings of the 2002 Symposium on Eye Tracking Research & Applications (New
Orleans, LA, USA). ACM, New York, NY, USA, 51‚Äì58. https://doi.org/10.1145/
507072.507082

[21] Patricia Goldberg, √ñmer S√ºmer, Kathleen St√ºrmer, Wolfgang Wagner, Richard
G√∂llner, Peter Gerjets, Enkelejda Kasneci, and Ulrich Trautwein. 2019. Attentive
or Not? Toward a Machine Learning Approach to Assessing Students‚Äô Visible
Engagement in Classroom Instruction. Educational Psychology Review 31, 4 (2019),
1‚Äì23. https://doi.org/10.1007/s10648-019-09514-z

[22] Sandra Helsel. 1992. Virtual Reality and Education. Educational Technology 32, 5

(1992), 38‚Äì42.

[23] Jan Herrington, Thomas C Reeves, and Ron Oliver. 2007. Immersive learning
technologies: Realism and online authentic learning. Journal of Computing in
Higher Education 19, 1 (2007), 80‚Äì99. https://doi.org/10.1007/BF03033421
[24] Christian Hirt, Marcel Eckard, and Andreas Kunz. 2020. Stress generation and
non-intrusive measurement in virtual environments using eye tracking. Journal
of Ambient Intelligence and Humanized Computing 11, 1 (2020), 1‚Äì13. https:
//doi.org/10.1007/s12652-020-01845-y

[25] Kenneth Holmqvist, Marcus Nystr√∂m, Richard Andersson, Richard Dewhurst,
Jarodzka Halszka, and Joost van de Weijer. 2011. Eye Tracking : A Comprehensive
Guide to Methods and Measures. Oxford University Press, United Kingdom.
[26] Kate S. Hone and Ghada R. El Said. 2016. Exploring the factors affecting MOOC
retention: A survey study. Computers & Education 98 (2016), 157‚Äì168. https:
//doi.org/10.1016/j.compedu.2016.03.016

[27] Zhiming Hu, Sheng Li, Congyi Zhang, Kangrui Yi, Guoping Wang, and Dinesh
Manocha. 2020. DGaze: CNN-Based Gaze Prediction in Dynamic Scenes. IEEE
Transactions on Visualization and Computer Graphics 26, 5 (2020), 1902‚Äì1911.
https://doi.org/10.1109/TVCG.2020.2973473

[28] Halszka Jarodzka, Kenneth Holmqvist, and Hans Gruber. 2017. Eye tracking in
Educational Science: Theoretical frameworks and research agendas. Journal of
Eye Movement Research 10, 1 (2017). https://doi.org/10.16910/jemr.10.1.3
[29] Dongsik Jo, Ki-Hong Kim, and Gerard Jounghyun Kim. 2016. Effects of Avatar
and Background Representation Forms to Co-Presence in Mixed Reality (MR)
Tele-Conference Systems. In SIGGRAPH ASIA 2016 Virtual Reality Meets Physical
Reality: Modelling and Simulating Virtual Humans and Environments (Macau).
ACM, New York, NY, USA, Article 12, 4 pages. https://doi.org/10.1145/2992138.
2992146

[30] Marcel A. Just and Patricia A. Carpenter. 1976. Eye fixations and cognitive
processes. Cognitive Psychology 8, 4 (1976), 441‚Äì480. https://doi.org/10.1016/0010-
0285(76)90015-3

[31] Tuomas Kantonen, Charles Woodward, and Neil Katz. 2010. Mixed reality in
virtual world teleconferencing. In 2010 IEEE Virtual Reality Conference (VR)
(Waltham, MA, USA). IEEE, New York, NY, USA, 179‚Äì182.
https://doi.org/
10.1109/VR.2010.5444792

[32] Muhammad Sikandar Lal Khan, Haibo Li, and Shafiq Ur R√©hman. 2016. Tele-
Immersion: Virtual Reality Based Collaboration. In International Conference on
Human-Computer Interaction (Toronto, Canada). Springer, Cham, 352‚Äì357. https:
//doi.org/10.1007/978-3-319-40548-3_59

[33] Richard Lamb and Elisabeth A. Etopio. 2020. Virtual Reality: A Tool for Preservice
Science Teachers to Put Theory into Practice. Journal of Science Education and
Technology 29, 4 (2020), 573‚Äì585. https://doi.org/10.1007/s10956-020-09837-5

[34] Gongjin Lan, Ziyun Luo, and Qi Hao. 2016. Development of a virtual reality tele-
conference system using distributed depth sensors. In 2016 2nd IEEE International
Conference on Computer and Communications (ICCC) (Chengdu, China). IEEE,
New York, NY, USA, 975‚Äì978. https://doi.org/10.1109/CompComm.2016.7924850
[35] Yining Lang, Liang Wei, Fang Xu, Yibiao Zhao, and Lap-Fai Yu. 2018. Synthesizing
Personalized Training Programs for Improving Driving Habits via Virtual Reality.
In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR) (Reutlingen,
Germany). IEEE, New York, NY, USA, 297‚Äì304. https://doi.org/10.1109/VR.2018.
8448290

CHI ‚Äô21, May 8‚Äì13, 2021, Yokohama, Japan

Gao and Bozkir, et al.

[55] Seung-hun Seo, Eunjoo Kim, Peter Mundy, Jiwoong Heo, and Kwanguk Kim. 2019.
Joint Attention Virtual Classroom: A Preliminary Study. Psychiatry Investigation
16 (2019), 292‚Äì299. https://doi.org/10.30773/pi.2019.02.08

[56] Sharad Sharma, Ruth Agada, and Jeff Ruffin. 2013. Virtual reality classroom as an
constructivist approach. In 2013 Proceedings of IEEE Southeastcon (Jacksonville,
FL, USA). IEEE, New York, NY, USA, 1‚Äì5. https://doi.org/10.1109/SECON.2013.
6567441

[57] Richard J. Shavelson, Judith J. Hubner, and George C. Stanton. 1976. Self-Concept:
Validation of Construct Interpretations. Review of Educational Research 46, 3
(1976), 407‚Äì441. https://doi.org/10.3102/00346543046003407

[58] Adalberto L. Simeone, Marco Speicher, Andreea Molnar, Adriana Wilde, and
Florian Daiber. 2019. LIVE: The Human Role in Learning in Immersive Virtual
Environments. In Symposium on Spatial User Interaction (New Orleans, LA, USA).
ACM, New York, NY, USA, Article 5, 11 pages. https://doi.org/10.1145/3357251.
3357590

[59] √ñmer S√ºmer, Patricia Goldberg, Kathleen St√ºrmer, Tina Seidel, Peter Gerjets,
Ulrich Trautwein, and Enkelejda Kasneci. 2018. Teachers‚Äô Perception in the
Classroom. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops (Salt Lake City, UT, USA). IEEE, New York, NY,
USA, 2315‚Äì2324.

[60] David Weintrop, Elham Beheshti, Michael Horn, Orton Kai, Kemi Jona, Laura
Trouille, and Uri Wilensky. 2016. Defining Computational Thinking for Mathe-
matics and Science Classrooms. Journal of Science Education and Technology 25
(2016), 127‚Äì147. https://doi.org/10.1007/s10956-015-9581-5

[61] Jacob O. Wobbrock, Leah Findlater, Darren Gergle, and James J. Higgins. 2011.
The Aligned Rank Transform for Nonparametric Factorial Analyses Using Only
Anova Procedures. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems (Vancouver, Canada). ACM, New York, NY, USA, 143‚Äì146.
https://doi.org/10.1145/1978942.1978963

[62] Christine Youngblut. 1998. Educational uses of virtual reality technology. Technical

Report. Institute for Defense Analyses, Alexandria, VA, USA.

[36] Meng-Yun Liao, Ching-Ying Sung, Hao-Chuan Wang, and Wen-Chieh Lin. 2019.
Virtual Classmates: Embodying Historical Learners‚Äô Messages as Learning Com-
panions in a VR Classroom through Comment Mapping. In 2019 IEEE Conference
on Virtual Reality and 3D User Interfaces (VR) (Osaka, Japan). IEEE, New York,
NY, USA, 163‚Äì171. https://doi.org/10.1109/VR.2019.8797708

[37] Matthew Lombard, Theresa Bolmarcich, and Lisa Weinstein. 2009. Measuring
Presence: The Temple Presence Inventory. In Proceedings of the 12th Annual Inter-
national Workshop on Presence. The International Society for Presence Research,
Los Angeles, CA, USA, 1‚Äì15.

[38] Aman Mangalmurti, William Kistler, Barrington Quarrie, Wendy Sharp, Susan
Persky, and Philip Shaw. 2020. Using virtual reality to define the mechanisms link-
ing symptoms with cognitive deficits in attention deficit hyperactivity disorder.
Scientific Reports 10 (12 2020). https://doi.org/10.1038/s41598-019-56936-4
[39] Ronald B. Marks, Stanley D. Sibley, and J. Ben Arbaugh. 2005. A Structural Equa-
tion Model of Predictors for Effective Online Learning. Journal of Management
Education 29, 4 (2005), 531‚Äì563. https://doi.org/10.1177/1052562904271199
[40] Sebastiaan Math√¥t, Jasper Fabius, Elle Van Heusden, and Stefan Van der Stigchel.
2018. Safe and sensible preprocessing and baseline correction of pupil-size data.
Behavior Research Methods 50, 1 (2018), 94‚Äì106. https://doi.org/10.3758/s13428-
017-1007-2

[41] Richard E. Mayer. 2010. Unique contributions of eye-tracking research to the
study of learning with graphics. Learning and Instruction 20, 2 (2010), 167‚Äì171.
https://doi.org/10.1016/j.learninstruc.2009.02.012

[42] Christian Moro, Zane ≈†tromberga, Athanasios Raikos, and Allan Stirling. 2017.
The effectiveness of virtual and augmented reality in health sciences and medical
anatomy. Anatomical Sciences Education 10, 6 (2017), 549‚Äì559. https://doi.org/10.
1002/ase.1696

[43] Shivsevak Negi and Ritayan Mitra. 2020. Fixation duration and the learning
process: an eye tracking study with subtitled videos. Journal of Eye Movement
Research 13, 6 (2020). https://doi.org/10.16910/jemr.13.6.1

[44] Pierre Nolin, Annie Stipanicic, Myl√®ne Henry, Yves Lachapelle, Dany Lussier-
Desrochers, Albert S. Rizzo, and Philippe Allain. 2016. ClinicaVR: Classroom-
CPT: A virtual reality tool for assessing attention and inhibition in children
and adolescents. Computers in Human Behavior 59 (2016), 327‚Äì333. https:
//doi.org/10.1016/j.chb.2016.02.023

[45] Elena Olmos-Raya, Janaina Ferreira-Cavalcanti, Manuel Contero, M Concepci√≥n
Castellanos, Irene Alice Chicchi Giglioli, and Mariano Alca√±iz. 2018. Mobile
virtual reality as an educational platform: A pilot study on the impact of im-
mersion and positive emotion induction in the learning process. EURASIA
Journal of Mathematics, Science and Technology Education 14, 6 (2018), 2045‚Äì2057.
https://doi.org/10.29333/ejmste/85874

[46] Jason Orlosky, Yuta Itoh, Maud Ranchet, Kiyoshi Kiyokawa, John Morgan, and
Hannes Devos. 2017. Emulation of Physician Tasks in Eye-Tracked Virtual
Reality for Remote Diagnosis of Neurodegenerative Disease. IEEE Transactions
on Visualization and Computer Graphics 23, 4 (2017), 1302‚Äì1311. https://doi.org/
10.1109/TVCG.2017.2657018

[47] Marc Pomplun, Tyler Garaas, and Marisa Carrasco. 2013. The effects of task
difficulty on visual search strategy in virtual 3D displays. Journal of vision 13
(2013). https://doi.org/10.1167/13.3.24

[48] Joseph Psotka. 1995. Immersive training systems: Virtual reality and education
and training. Instructional science 23, 5-6 (1995), 405‚Äì431. https://doi.org/10.
1007/BF00896880

[49] Natasha Anne Rappa, Susan Ledger, Timothy Teo, Kok Wai Wong, Brad Power,
and Bruce Hilliard. 2019. The use of eye tracking technology to explore learning
and performance within virtual reality and mixed reality settings: a scoping
review. Interactive Learning Environments 0, 0 (2019), 1‚Äì13. https://doi.org/10.
1080/10494820.2019.1702560

[50] Ananda Bibek Ray and Suman Deb. 2016. Smartphone Based Virtual Reality
Systems in Classroom Teaching ‚Äî A Study on the Effects of Learning Outcome.
In 2016 IEEE Eighth International Conference on Technology for Education (T4E)
(Mumbai, India). IEEE, New York, NY, USA, 68‚Äì71. https://doi.org/10.1109/T4E.
2016.022

[51] Albert A. Rizzo, Todd Bowerly, J. Galen Buckwalter, Dean Klimchuk, Roman
Mitura, and Thomas D. Parsons. 2006. A Virtual Reality Scenario for All Seasons:
The Virtual Classroom. CNS Spectrums 11, 1 (2006), 35‚Äì44. https://doi.org/10.
1017/S1092852900024196

[52] Dario D. Salvucci and Joseph H. Goldberg. 2000.

Identifying Fixations and
Saccades in Eye-Tracking Protocols. In Proceedings of the 2000 Symposium on Eye
Tracking Research & Applications (Palm Beach Gardens, FL, USA). ACM, New
York, NY, USA, 71‚Äì78. https://doi.org/10.1145/355017.355028

[53] Abraham Savitzky and Marcel J. E. Golay. 1964. Smoothing and Differentiation
of Data by Simplified Least Squares Procedures. Analytical Chemistry 36 (1964),
1627‚Äì1639. https://doi.org/10.1021/ac60214a047

[54] Thomas Schubert, Frank Friedmann, and Holger Regenbrecht. 2001. The Ex-
perience of Presence: Factor Analytic Insights. Presence 10, 3 (2001), 266‚Äì281.
https://doi.org/10.1162/105474601300343603

