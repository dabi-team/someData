Noname manuscript No.
(will be inserted by the editor)

HoloLens 2 Technical Evaluation as Mixed Reality Guide

Hung-Jui Guo

· Balakrishnan Prabhakaran

2
2
0
2

l
u
J

9
1

]

C
H
.
s
c
[

1
v
4
5
5
9
0
.
7
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Mixed Reality (MR) is an evolving technol-
ogy lying in the continuum spanned by related tech-
nologies such as Virtual Reality (VR) and Augmented
Reality (AR), and creates an exciting way of interact-
ing with people and the environment. This technology
is fast becoming a tool used by many people, potentially
improving living environments and work eﬃciency. Mi-
crosoft HoloLens [1] has played an important role in the
progress of MR, from the ﬁrst generation to the second
generation. In this paper, we systematically evaluate
the functions of applicable functions in HoloLens 2 [2].
These evaluations can serve as a performance bench-
mark that can help people who need to use this in-
strument for research or applications in the future. The
detailed tests and the performance evaluation of the
diﬀerent functionalities show the usability and possible
limitations of each function. We mainly divide the ex-
periment into the existing functions of the HoloLens 1,
the new functions of the HoloLens 2, and the use of re-
search mode. This research results will be useful for MR
researchers who want to use HoloLens 2 as a research
tool to design their own MR applications.

Keywords Mixed Reality · HoloLens · HoloLens 2 ·
hologram · Inertial Measurement Unit (IMU) · Spatial
Anchor

(cid:0) Hung-Jui Guo
E-mail: hxg190003@utdallas.edu
Software Engineering Department, the Univerity of Texas at
Dallas, Texas, United State

Balakrishnan Prabhakaran
E-mail: bprabhakaran@utdallas.edu
Computer Science Department, the Univerity of Texas at Dal-
las, Texas, United State

1 Introduction

Mixed Reality (MR) [3], [4] has introduced an exciting
new way of interaction among people as well as between
human and the real-world. MR can play a very impor-
tant role in the current demand for remote work and
remote communication. MR is being applied in many
diﬀerent ﬁelds, such as medical science, education or
industrial design. Many new research works are being
carried out to integrate MR into daily life activities of
the general public, thereby increasing convenience and
work eﬃciency.

Mixed Reality (MR) is in the continuum spanned
by related technologies such as Virtual Reality (VR) [5]
and Augmented Reality (AR) [6]. VR changes the entire
surrounding environment into a virtual form and allow
interactions between the user and the virtual word. AR
augments the real-world by including graphics, sounds,
and touches feedback through AR devices. However,
AR does not facilitate interactions between users and
the virtual objects added to the real-world scene. MR
combines the advantages of the two, scanning the sur-
rounding real-world environment to build a model, and
then adds the needed virtual object in this environment.
Users can directly interact with virtual objects (by op-
erations such as scaling, rotation, or translation) in the
real environment using their hands (or other devices).
Furthermore, virtual objects can also interact with real
objects in the mixed reality world. For example, a vir-
tual cube can be placed on a table in the real-world, as
shown in Figure 1a.

Microsoft HoloLens 2 [1] (Figure 1b) is an updated
version of the previous generation HoloLens 1 headset
from Microsoft. Compared with the previous genera-
tion, the HoloLens 2 has improved most of the features,
such as display resolution, ﬁeld of view, weight and so

 
 
 
 
 
 
2

H.J., Guo, B. Prabhakaran

Fig. 1 (a) An example of putting virtual object on table in
the real environment. (b) HoloLens 2 device (Picture taken
from Microsoft HoloLens 2 webcite [1])

on. Also, some new functions and sensors have been
added, such as eye-tracking system, biometric security
system, two hands tracking, and Inertial Measurement
Unit (IMU) sensors.

In addition to the hardware updates, the correspond-
ing software updates are also very impressive, especially
the newer version of research mode [2]. Users can access
multiple sensors by using a set of C++ APIs and tools
of the research mode in HoloLens 2. The sensors avail-
able for access include Visible Light cameras, IR (Infra-
Red), Depth camera, and IMU sensors, as shown in Fig-
ure 2. These will be of signiﬁcant help to researchers
who need to develop sensor-based applications in the
future.

In this paper, we introduce a series of performance
evaluation tests on most available sensors and features
in HoloLens 2. Part of the tests in this paper refers to a
HoloLens 1 paper [7] to test the performance improve-
ments, as compared between HoloLens 2 and HoloLens
1. These experimental results can help us understand
the capabilities and possible limitations of HoloLens 2.
We also test and discuss the performance of functions
in the research mode.

This paper is organized as follows. Section II dis-
cusses the works related to mixed reality and HoloLens.
Section III introduces the experimental design we used
to test the device. Then Section IV shows the detailed
experiment procedure according to the previous section,
and Section V provides the results and discussions on
each experiment. Lastly, Section VI concludes the whole
paper and sums up our contributions.

2 Related work

2.1 Mixed Reality

[8] describes several AR-related user studies and pro-
vide ideas for deriving MR experiments from AR exper-
iments. In [9], the authors uniﬁed the AR applications
and provided a link between testing and implementa-
tion, which can be leveraged for MR applications as
well.

Fig. 2 HoloLens 2 input sensors [2]

In order to unify the cognitive diﬀerences arising
from the development of the MR ﬁeld in recent years,
[10] conducted interviews with a number of experts and
referred to various documents to identify the concepts
related to MR. [11] proposed methods for applying it
to stage a poetry performance simultaneously within
real and virtual theaters. [12] proposed a collaborative
MR interface, which is a direction that many other re-
searchers have focused on in recent years. In addition,
some other researchers used this technology in aesthetic
design [13], education [14], [15], [16], and entertainment
[17], [18]. In [19], the authors used MR technology to
implement an indoor positioning system. Since location
information is also an important part of MR research,
this paper will be a benchmark for future related re-
search. Lastly, in [20] and [21], the authors made a
detailed survey on the ﬁeld of MR to help other re-
searchers increase their understanding of this ﬁeld.

2.2 HoloLens 1

The ﬁrst generation of HoloLens, HoloLens 1, [1] was
ﬁrst released in 2016 by Microsoft. HoloLens 1 is a pair
of mixed reality smart-glasses, which is the ﬁrst head-
mounted display device running the Windows Mixed
Reality platform. Since this device has been available
for several years, there have been many research topics
and applications related to it.

In paper [22], the authors proposed a method that
uses only one HoloLens 1 and several Skype-enabled
devices such as tablets or PCs to achieve remote co-
operation. In addition to the aforementioned applica-
tions, HoloLens 1 can be used in the medical ﬁeld. For
example, [23] tried to use this device as a therapeu-
tic tool for people with Alzheimer’s Disease, and [24]
used HoloLens 1 in anatomic pathology to test clinical
and non-clinical applications. Moreover, [25] proposed
a study of mixing HoloLens 1 and 3D geographic in-

(a)(b)HoloLens 2 Technical Evaluation as Mixed Reality Guide

3

formation, and [26] provided a method to add higher
resolution depth data in the device. [27] and [7] imple-
mented a series of evaluation tests to evaluate the per-
formance, advantages, and disadvantages of HoloLens
1. The detailed test contents of paper [7] are the main
reference and comparison article in this paper; there-
fore, we use many similar tests in our experiments for
the purpose of comparison.

2.3 HoloLens 2

It has only been about two and a half years since the
release of HoloLens 2 [1], so the number of related pa-
pers is relatively small. According to paper [28], the
authors introduced newly added functions, capability,
and HPU upgrades in HoloLens 2. Then, [2] announced
a new version of research mode for HoloLens 2, allow-
ing users to use this tool to access individual sensors in
the device. This function could help many researchers
to obtain more sensorial data according to diﬀerent
needs. [29] applied HoloLens 2 in the ﬁeld of education
to ease the interaction between teachers and students,
and [30] used the device for industrial purposes. [31]
used HoloLens 2 to increase the success rate of medi-
cal operations and reduce the operation time. Due to
the COVID-19 situation, [32] and [33] proposed using
HoloLens 2 for remote care to reduce the contact be-
tween doctors and patients. Currently, there is still no
comprehensive set of performance evaluation, testing
the available features of HoloLens 2 in detail. Therefore,
we carried out various experiments testing the diﬀerent
functionalities of Hololens 2, and the results reported in
this paper can serve both as a benchmark and as design
criteria for MR applications.

3 Experiment Design and Procedure

To evaluate the functionality of HoloLens 2, we design
several diﬀerent experiments to examine its features.
All testing apps are created in Unity 3D or using C++
APIs and exported to the HoloLens 2 device with soft-
ware version 20H2. We also provide the experiment pro-
cedure detail about how the testers need to do each test.
The same environmental condition names in each sec-
tion have the same parameters.

3.1 Hand Tracking

By using the AHAT (Articulated HAnd Tracking) depth
camera [2], the HoloLens 2 can capture hand move-
ments to obtain hand tracking data. We can access this

data by using either research mode or Unity 3D apps,
and the output results are the same. Therefore, for con-
venience of testing, we use Unity 3D apps as our main
testing tool in this evaluation process.

This experiment evaluates the hand tracking fea-
ture in HoloLens 2 by calculating the diﬀerence be-
tween hand joints hologram and real-world human hand
joints. Figure 3a shows the default hand joint hologram
generated in HoloLens 2 world, and Figure 3b shows
the corresponding joint name of the human hand.

Since users implement apps in many diﬀerent con-
ditions, our experiments cover as many environmen-
tal factors as possible to simulate diﬀerent situations.
In this case, we choose to examine the hand tracking
feature of HoloLens 2 under daylight/night time and
with/without headlight/lamp. In addition to diﬀerent
environmental variables, we also test this feature at dif-
ferent hand moving speeds, such as 1/0.5/0.2 meters
per second. The hand tracking performance is evalu-
ated by calculating the average Euclidean distances be-
tween each hand joint point in the hologram and each
real-world human hand joint point. When projecting
hand joint feature from HoloLens 2 world space to 2D
image space, the projection will produce a slight error
between hologram hand and real-world hand, as shown
in Figure 4. Hence we also test the hand tracking fea-
ture when testers stop moving their hands as a baseline
to calculate the true error.

Fig. 3 Examples of hand joint feature (a) Default hand joint
hologram representation in HoloLens 2 world (b) Real-world
hand joint labels (Picture taken from Microsoft HoloLens 2
webcite [1])

To perform the hand tracking test, we ﬁrst create
a virtual box (0.1 m X 0.1 m X 0.1 m) in Unity 3D
and make it move left and right like a pendulum at a
ﬁxed speed, 0.2 m / second, 0.5 m / second, and 1 m
/ second with moving distance 1 m. Then we ask the
testers to open their hands and wave either their left or
right hand back and forth at the moving cube’s speed
to record the generated hand joints as the result, as
shown in Figure 3. Besides, we also record the hand
joints result when testers’ hands stop moving. Apart

4

H.J., Guo, B. Prabhakaran

black platform behind these ﬁve boxes to visual-
ize the eye gaze point on the same ﬂat surface, as
shown in Figure 5. Then we ask the testers to gaze
ﬁve virtual cubes under diﬀerent environmental con-
ditions, daylight/night time an with/without head-
light/lamp. We use the same environmental vari-
ables of the hand tracking experiment. When the
testers arrive at the selected location, we ask them
to start the app and gaze at each point for two sec-
onds; then, we record the test process and extract
the result accordingly.

Fig. 4 Steady hand tracking result under diﬀerent environ-
mental conditions (a) Daylight indoor (b) Daylight outdoor
(c) Night time with lamp (d) Night time indoor with head-
light and lamp (e) Night time indoor with headlight (f) Night
time outdoor with lamp

from the hand moving speed, we also use diﬀerent envi-
ronmental variables (daylight indoor, daylight outdoor,
nighttime indoor with head light and lamp, nighttime
indoor with head light, nighttime indoor with lamp,
and nighttime outdoor with lamp) as one of our test
conditions. Afterward, we record testers’ hand waving
hand joints tracking video in diﬀerent conditions and
take out ﬁve frames as test results.

3.2 Eye Tracking

HoloLens 2 Eye Tracking API provides a single eye-
gaze ray for developers to access eye-tracking data for
further use in eye-tracking experiments. This feature
is newly introduced, compared to the previous gener-
ation HoloLens 1 [7]. In this experiment, we test the
eye-tracking functionality under diﬀerent environmen-
tal conditions. At the same time, we evaluate the eye-
tracking feature on real-world objects and virtual world
objects on top of these conditions. During the eye-tracking
procedure, we will ask the testers to gaze at each tar-
get point for two seconds according to our instructions
and take pictures three times of the results for average
purposes. Then, we use the captured pictures to calcu-
late the error distance. The eye-tracking performance is
then evaluated in terms of Euclidean distances between
the eye-gaze ray and target points.

1. Virtual Objects Eye Gaze Tracking:

In the virtual objects eye gaze tracking, we create
ﬁve red virtual cubes in ﬁve diﬀerent directions with
size 1 cm X 1 cm X 1 cm, and we also generate a

Fig. 5 Eye-gaze tracking examples, the white dot represents
the tester’s eye-gaze point, and gray boxes indicate the target
virtual cubs (a) Daylight indoor (b) Daylight outdoor

2. Real-world Objects Eye Gaze Tracking:

Real-world objects eye gaze tracking was performed
to evaluate the eye-tracking accuracy in a real-world
environment. To complete this experiment, we put
ﬁve diﬀerent real-world black dots on a box and ask
the testers to gaze at these ﬁve points, as shown in
Figure 6. We only choose two environmental vari-
ables - Daylight indoor and nighttime with lamp -
as our control variable since the target is diﬃcult to
see clearly under the environment with insuﬃcient
lighting. We then ask the testers to walk to the se-
lected location, start the app, and gaze at each black
dot for two seconds; then, we record the test process
and take a screenshot from the recorded video to ex-
tract the evaluation result accordingly.

3.3 Spatial Mapping

Spatial mapping is the key in the hologram experiment,
which uses the long-throw depth camera. We can access
spatial mapping data through either research mode or
Unity 3D apps, and the results are the same. For test-
ing purposes, we use Unity 3D apps as our main test-
ing platform. This spatial mapping experiment evalu-
ates the reality of holograms, which is generated by
HoloLens 2. According to the previous HoloLens 1 pa-
per [7], the authors assess the result by overlapping the
holograms on the target real-world objects. Therefore,

(a)(b)(c)(d)(e)(f)(a)(b)HoloLens 2 Technical Evaluation as Mixed Reality Guide

5

Fig. 6 Real-world eye gaze tracking examples under day-
light, the blue dot represents the tester’s eye-gaze point, and
black dots indicate the target virtual cubes

we use the same concept to design our experiments and
use the same accuracy deviation to evaluate the result
in Hololens 2. To complete this experimental design, we
create a HoloLens 2 app with Unity 3D to make spa-
tial mapping visible. Whenever we need to reconstruct
a new hologram, we ask the testers delete all holograms
in HoloLens 2 via menu setting.

1. Hologram Visualization:

In the hologram visualization experiment, we use a
HoloLens 2 product box as a real-world standard
to create a hologram box in the mix-reality world.
In this case study, this box’s size is 37.8 cm X 24.7
cm X 26.8 cm. Then we create a red hologram box
of the same size for overlapping, which is shown in
Figure 7. Besides, we constrain the hologram box so
that it cannot rotate or change size in order to avoid
the testers changing the shape. During this process,
we ask the testers to stand in front of the real-world
box and start the testing app. The testers are asked
to move the visualized hologram box to overlap with
the real-world box as exactly as possible. Then we
will take pictures from the real-world box’s front,
side and top views, as shown in Figure 7. These
pictures will be used as our evaluation result.

2. Virtual and Real-world Environment Overlapping
Ratio: This experiment indicates the ﬂat surface
overlapping of the device-generated hologram sur-
face and the real-world target surface which it is
attached. First, we create a hologram cube (0.1 m
X 0.1 m X 0.1 m) in the app and ask the testers to
put the cube onto the hologram surface, as shown
in Figure 8a. Then we will turn to the side view
and take a photograph as the test result to calcu-
late the error distance, as shown in Figure 8b. We
measure the gap or overlap ratio between these two
surfaces by calculating the accuracy deviation [7].
Figure 8 shows the example of the generated holo-
gram (white line and black polygon) and the target
surface (white table).

Fig. 7 Hologram box (red) overlap with a real-world box
(black) (a) Front view (b) Top view (c) Left side view (d)
Right side view

Fig. 8 Virtual and real-world environment overlapping pro-
cess and result (a) After testers put the red virtual cube onto
the hologram surface (b) Side view of the ﬂat surface (c)
Closer side view and gap example (red circle)

3.4 Real Environment Reconstruction

HoloLens 2 has the ability to reconstruct a real-world
environment into a hologram world. In order to evalu-
ate the reconstruction result of HoloLens 2, we use the
device to generate holograms of real-world objects and
indoor rooms as the evaluation output result. In this
test, similar to the spatial mapping experiment, we ask
the testers to remove all environmental holograms in
HoloLens 2 via setting before the test start. Then the
testers will act according to our instructions written as
follow.

1. Real-world Objects Reconstruction:

The real-world object reconstruction experiment is
performed to evaluate the construction speed and
completion percentage of HoloLens 2’s object recon-
struction functionality. We select four diﬀerent size
real-world boxes as testing objects, which are large
tall box (50.2 cm X 32.5 cm X 75.5 cm), regular size
box (56 cm X 47 cm X 47 cm), thin box (41 cm X
20.5 cm X 28.5 cm), small box (15.5 cm X 13.5 cm
X 23 cm).
First, we put the four boxes in a ﬁxed test position
separately and asked the testers to open the spatial
mapping app. Second, we ask the testers to look

(a)(b)(a)(b)(c)(d)(a)(b)(c)6

H.J., Guo, B. Prabhakaran

away from the box and remove all holograms via
HoloLens 2’s menu setting. Lastly, we instruct the
testers to turn around and face the target box and
walk around the box to construct the box’s holo-
gram. Then we record the construction result of the
hologram for evaluation. Here, we set the construc-
tion time limit to 60 seconds; whenever the holo-
gram becomes stable, stop recording and export the
result. According to experience, a box hologram can
be established stably within 60 seconds. After the
box hologram becomes stable, we will ask the testers
to walk around the box once again to record the ﬁ-
nal reconstruction result.

2. Real-world Indoor Room Reconstruction:

In the real-world indoor room reconstruction ex-
periment, we evaluate the performance by calcu-
lating indoor room hologram construction time and
completion percentages of HoloLens 2’s room recon-
struction feature. In this case study, we choose four
rooms, living room (7 m X 3.92 m X 2.97m), per-
sonal room (3.81 m X 3.02 m X 2.40 m), restroom
(2.50 m X 2.48 m X 2.40 m), and walk-in closet (0.79
m X 0.54 m X 2.40 m) as the reconstruction target.
We ﬁrst ask the testers to stay at a selected room
corner and face the wall, then we will start record-
ing the test process. Second, we instruct the testers
to remove all holograms in HoloLens 2 and start
looking around the room to construct the current
nearby environment. Lastly, we ask the testers to
walk along the wall with speed, 1 step (0.5 m) per
second, and look around the room every step at the
same time. After walking back to the original posi-
tion, stop recording, and save it as the evaluation
result.

3.5 Speech Recognition

The speech recognition experiment is performed to eval-
uate the eﬀectiveness of speech commands in HoloLens
2. To compare the speech recognition improvement from
HoloLens 1 to HoloLens 2, we refer to the paper [7] to
select some of the testing speech commands and add
some other commands. We test both system-deﬁned
commands (“select,” “Move this. . . there,” “face me,”
“bigger/smaller,” “What’s my IP address,” “hide and
show hand ray,” “shut down device,” “close,” “Open
the Start menu,” “Follow me,” and “take a picture”)
and user-deﬁned commands selected from Wobbrock’s
paper [34] (“move,” “rotate,” “delete,” “zoom in/out,”
“open,” “duplicate,” “previous,” and “help”) which men-
tioned in the paper [7].

then open a preset app and ask the testers to speak
each user-deﬁned command once at a time. The num-
ber of recognized commands will be counted as the
evaluation result to evaluate the capability of HoloLens
2’s speech recognition feature. All testers lived in an
English-speaking environment for more than one year
and could speak English ﬂuently. The native language
proportion is shown in Figure 9a. We also recruited
some non-student testers in order to balance the edu-
cation status ratio, which is a challenging task in other
MR and AR papers. The proportion of testers’ current
education status is shown in Figure 9b. Furthermore,
to compare diﬀerent voice line, we recruited people of
varying age groups as testers with a balance in the ra-
tio of men and women to make the experiment more
accurate, as shown in Figure 10.

Fig. 9 Speech recognition native language ratio and current
education status

Fig. 10 Speech recognition gender in each age interval and
total gender ratio

3.6 Azure Spatial Anchor

Microsoft Azure provides a spatial anchors feature to
create persistent, accurate digital anchors and could be
operated at real-world scale. This feature can be used
between HoloLen 2, Android and Apple devices.

During the testing process, we ask the testers to
speak each system-deﬁned command once at a time,

HoloLens 2 uses the Azure Spatial Anchor feature
to create permanent anchors in the real-world to help

012345619-2223-2627-3031-3435 upManWomanManWoman(a)(b)EnglishChineseVietnameseKoreanJapaneseStudentNon student(a)(b)HoloLens 2 Technical Evaluation as Mixed Reality Guide

7

use the app or share world anchors with others. Cur-
rently, the best way is to use the spatial anchor feature
published by Azure. For example, developers can use
this feature to create a permanent world anchor arrow
to guide directions for the users.

Therefore, we carry out HoloLens 2 Azure Spatial
Anchor position evaluation to determine whether the
position, in real-world, has changed from uploading the
spatial anchors to the cloud to downloading back to the
local device. First, we need to connect the HoloLens 2
app with the Azure Spatial Anchor server. Then we ask
the testers to start the Azure session and create an an-
chor cube on the real-world table and take a picture as
a test benchmark. Here, the anchor’s position is saved
to the Azure Spatial Anchor server, as shown in Fig-
ure 11 (a). Next, the testers move the cube to another
place and locate the anchor created previously from the
server by pressing a button in the app; the downloaded
cube shows in Figure 11b. Finally, take a picture for
the Anchor and calculate the error distance between
the benchmark and the downloaded result.

Fig. 11 Azure Spatial Anchor on a table in the real-world
(a) User-created benchmark on the local end (b) User-created
anchor download from the server

3.7 Research Mode – IMU sensors

In the HoloLens 2 research mode [2], one of the most sig-
niﬁcant diﬀerence from the previous HoloLens 1 is the
addition of accessing the inputs of Inertial Measure-
ment Unit (IMU) sensors, Accelerometer, Gyroscope,
and Magnetometer. This can help researchers or users
gain more understanding of machine movement or ro-
tation. This subsection focuses on using Accelerometer
and Gyroscope to calculate the error between reality
and the device’s output. Then will describe the detailed
design of testing two components of the IMU sensors in
HoloLens 2 [2]. Since there is a lot of external magnetic
ﬁeld interference in the test environment, we are not
doing Magnetometer-related experiments here.

1. Accelerometer:

The Accelerometer in HoloLens 2 [2] is used to de-
termine linear acceleration along the X, Y, and Z

axes and gravity. The output data from Accelerom-
eter is the projection of the force on X, Y, and Z
axes of the acceleration, combined acceleration, and
time. Therefore, by using this component, we can
compute the users’ moving direction and distance.
Also, the projection of the acceleration force can be
used to determine the tilt angle of the users. In this
experiment, we use these two measurements to ask
the testers to move a ﬁxed distance along the x, y,
or z-axis or tilt their body in a speciﬁed direction
to test the accuracy.
Since the Accelerometer provides the linear acceler-
ation along the X, Y, and Z axes, we can use double
integrals to obtain the current position. According
to the output data from the whole IMU sensors, we
know that the HoloLens 2 uses a three-dimensional
space setting, as shown in Figure 12b. On the basis
of the outputs from the Accelerometer, let rx, ry,
rz representing the projection of the force on X, Y,
and Z axes of the acceleration, which show the in-
stantaneous acceleration on the current point. Since
the time interval between each acceleration point is
0.083 seconds, which is very short, we can assume
that the instantaneous acceleration is equal to the
average acceleration between each point. Let one of
the average acceleration a1 = (a1x, a1y, a1z), and
another average acceleration a2 = (a2x, a2y, a2z)
right after a1. Then, we may assume a linear equa-
tion a(t) to represent points between a1 and a2 as

a(t) = (a1x + (a2x − a1x) ∗ 12t,
a1y + (a2y − a1y) ∗ 12t,
a1z + (a2z − a1z) ∗ 12t).

(1)

Since we assume that the instantaneous acceleration
is the same as average acceleration, we can integrate
a(t) to yield velocity v(t)

v(t) = (a1xt + (a2x − a1x) ∗ 6t2 + Cx,
a1yt + (a2y − a1y) ∗ 6t2 + Cy,
a1zt + (a2z − a1z) ∗ 6t2 + Cz).

(2)

Here Cx, Cy, and Cz are the constants which repre-
sent the velocity calculated previously at the point
a1 and we assume ﬁrst point’s velocity is (0, 0, 0).
Secondly, we can integrate v(t) to produce position
s(t)

s(t) = (

a1xt2 + (a2x − a1x) ∗ 2t3 + Cxt + Kx,

1
2
1
2
1
a1zt2 + (a2z − a1z) ∗ 2t3 + Czt + Kz).
2

a1yt2 + (a2y − a1y) ∗ 2t3 + Cyt + Ky,

(a)(b)8

H.J., Guo, B. Prabhakaran

(3)

Here Kx, Ky, and Kz are the constants which repre-
sent the position calculated previously at the point
a1 and we also assume ﬁrst point’s position is (0, 0, 0).
Lastly, we can use s(t) calculated on every two points
a1 and a2 to calculate the current position of HoloLens
2, and compute the moving distance accordingly to
compare with the actual moving distance.

Fig. 12 (a) Accelerometer walking distance setting (b)
Three dimensional space setting used in HoloLens 2 (c) Exam-
ple of Gyroscope rotating 90 degree along x-axis (d) Example
of Accelerometer tilt 90 degree along x-axis and z-axis

In addition, we also use the output data from Ac-
celerometer to evaluate the tilting angle. As shown
in Figure 12d, we ask the testers to tilt their head
at a ﬁxed angle when wearing the HoloLens 2 de-
vice. Then according to the output rx, ry, rz, we
can use the following equation to calculate the tilt
angle along each axes.

Axr = arccos(

Rx
R

) ∗ (

180
π

)

(4)

(cid:113)

R2

x + R2

y + R2
where R =
z to express the com-
bined force on the current location. Since the data
output from the device is in the form of radian by
default, we need to multiply 180
π to transform it into
degree form.

2. Gyroscope:

Using the Gyroscope, we can extract the output
data, including angular velocity on X, Y, and Z axes
and time. According to these data, we can deter-
mine the rotation angle made by the users. In or-
der to achieve accurate measurement, we use com-
puter software to draw a circle and a vertical cross
and print them on paper as ground truth with 90-
degree intervals between each line, as shown in Fig-
ure 12c. We can determine the computation error
in this evaluation process by comparing the device’s
rotation angle and the calculated rotation angle.

According to the Gyroscope output data from the
HoloLens 2, we can ﬁnd out that there are three an-
gular velocities per unit of time, Gx, Gy, and Gz,
and each unit of time is 0.048 seconds. Therefore,
we can transform the three angular velocities to ac-
tual angle change per unit time by substituting the
following formula.

AC = ((Gx ∗

(Gy ∗

(Gz ∗

180
π
180
π
180
π

) ∗ 0.048,

) ∗ 0.048,

) ∗ 0.048)

(5)

Similarly, the data output from the device is in the
form of radian by default, so we need to multiply
180
π to transform it into degree form. By calculating
angle changes for every unit of time, we can compare
the output of the Gyroscope with the actual rotation
angle.

4 Result and Discussion

According to the previous experiment design and pro-
cedure sections, this section contains the experimental
results and discussions about the outcome of the results.

4.1 Hand Tracking

For the hand tracking experiment, we calculated the
error distance between each hologram hand joint point
(Figure 3a) and real-world hand joint point (Figure 3b)
in the ﬁve frames cropped from the recorded experiment
video. The error distance ER is calculated as follow:

ER =

1
N

(cid:88)

i∈N

d (ri, gi)

(6)

where N represents the number of hand joints, d is
the Euclidean distance of two points, and ri, gi are
the corresponding hand joint point in real-world hand
and HoloLens 2 generated hand. ER is greater than or
equal to 0, where 0 means that the points generated by
HoloLens 2 are closer to the real points, and higher ER
represents the larger the error is.

We split the experiment result into four diﬀerent
speed conditions, steady, 0.2 m / sec, 0.5 m / sec, and
1 m / sec, as shown in Figure 13. Also, we calculate the
average error distance of all environmental conditions
of each speed condition and organize it in a chart, which
shows in Figure 14.

Although the error distance seems a little bit too
high in the mixed reality world in steady hand track-
ing, the hands’ hologram in the user’s eyes still seems

(a)(b)(c)(d)HoloLens 2 Technical Evaluation as Mixed Reality Guide

9

lated the average error distance under diﬀerent envi-
ronmental conditions.

1. Virtual Objects Eye Gaze Tracking:

In this experiment result, we calculate the error dis-
tance using the same error distance ER in the pre-
vious section between ﬁve virtual cubes and corre-
sponding eye gaze points. Then, we compute the
average of the error distances under each position
and environmental conditions, as shown in Figure
15(top).
The eye-tracking error distance results show every
error distance is smaller than 5 centimeters. In the
Night-time indoor with lamp only experiment, the
lamplight is from the user’s back to the center-top
side; therefore, the error distance in the top and
center is smaller than the other side. Similarly, in
the Night-time outdoor with lamp only experiment,
the lamplight is from the user’s left side; thus, the
left side’s error distance is smaller than the other
left side results at night.

2. Real-world Objects Eye Gaze Tracking:

In the real-world objects eye gaze tracking experi-
ments, we calculate the average error distance ER
between ﬁve virtual cubes and corresponding eye
gaze points under two environmental conditions, as
shown in Figure 15(bottom).
The bar graph shows the average error distance un-
der two environmental conditions; the results are
similar to the above eye gaze tracking results, which
means that eye-tracking results can achieve good re-
sults both in virtual and real environments.

Fig. 15 Eye-tracking average error distance with standard
error bars on virtual objects (top) and real-world objects
(bottom)

Fig. 13 Hand tracking experiment result – error distance
with standard error bars (red: right hand, black: left hand)
under each speed and environmental conditions

Fig. 14 Hand tracking experiment average error distance
with standard error bars under each speed conditions

precisely match the user’s real hands. Therefore, the
actual error distance under each speed conditions can
be computed relative to the steady hand tracking er-
ror distance result. According to the average error dis-
tance result, we can ﬁnd that the error distance will
be more considerable when the speed is faster. How-
ever, the hand tracking error distance is not signiﬁcant
even at 1 m / sec speed; we can say that this result is
quite ideal below this speed. Thence, when developers
or users try to develop a new application in HoloLens
2, we recommend them to be aware of hands moving
speed not to go over 1.0 m / sec when moving from one
scene to the other.

4.2 Eye Tracking

4.3 Spatial Mapping

In the eye-tracking experiment result, after collecting
the eye-tracking experiment result images, we calcu-

In spatial mapping experiments, we compare with the
experimental results from HoloLens 1 [7]. For this com-

0.0520.0630.0680.0680.0630.0580.0480.0520.0470.0580.050.04700.050.10.150.2Daylight IndoorDaylight OutdoorNighttime indoor withhead light and lampNighttime indoor withhead lightNighttime indoor withlampNighttime outdoor withlampError Distance (m)Steady hand trackingRight HandLeft Hand0.0820.0680.0630.0550.0580.0650.0670.0550.050.0420.0450.05300.050.10.150.2Daylight IndoorDaylight OutdoorNighttime indoor withhead light and lampNighttime indoor withhead lightNighttime indoor withlampNighttime outdoor withlampError Distance (m)0.2 m / secondRight HandLeft Hand0.1080.090.0720.080.0970.0980.1130.1250.080.0850.0620.100.050.10.150.2Daylight IndoorDaylight OutdoorNighttime indoor withhead light and lampNighttime indoor withhead lightNighttime indoor withlampNighttime outdoor withlampError Distance (m)0.5 m / secondRight HandLeft Hand0.1520.1850.140.1180.1150.1350.1230.1020.1350.1280.1080.15500.050.10.150.20.25Daylight IndoorDaylight OutdoorNighttime indoor withhead light and lampNighttime indoor withhead lightNighttime indoor withlampNighttime outdoor withlampError Distance (m)1 m / secondRight HandLeft Hand0.0620.0650.0910.140.050.0520.0940.12500.050.10.150.2Steady hand tracking0.2 m / second0.5 m / second1.0 m / secondError Distance (m)Right HandLeft Hand00.0050.010.0150.020.0250.030.0350.04Daylight IndoorDaylight OutdoorNighttime indoor withhead light and lampNighttime indoor withhead lightNighttime indoor withlampNighttime outdoor withlampError Distance (m)Virtual objects eye-tracking average error distanceCenterLeftRightTopBottomAverage00.0050.010.0150.020.0250.030.0350.04Daylight IndoorNighttime with lampError Distance (m)Real-world objects eye-tracking average error distanceCenterLeftRightTopBottomAverage10

H.J., Guo, B. Prabhakaran

parison, we choose to use the accuracy deviation σR
A, as
in [7]. The accuracy deviation σR
A is used to compare
the diﬀerence between real-world environment and its
reconstructed model:
(cid:19)

σR
A =

(cid:18) |L − li|
L · N

(cid:88)

i∈N

(7)

where N represents the number of measurements, L
refers to the real-world object’s length of one edge, and
li is the corresponding edge length measured in the i th
measurement by HoloLens 2. The value of σR
A is greater
than or equal to 0, where 0 represents that the real-
world objects and their holographic model generated
by HoloLens 2 are identical, and higher σR
A value means
there is signiﬁcant diﬀerence between the two.

1. Hologram Visualization:

For the hologram visualization experiments, we ﬁrst
calculate the length, width, and height of the holo-
gram box created by our app and the real-world box.
Then, we calculate the diﬀerence between each edge
of real-world box and hologram box using accuracy
deviation σR
A. The result shows in Figure 16. Since
each view has an edge that cannot be calculated,
we only calculate two edges’ accuracy deviation on
each view. This situation can be seen in Figure 7.

Fig. 16 Hologram visualization accuracy deviation on each
edge and view

As we can see from Figure 16, HoloLens 2 performs
really well on each view and edge, especially the
length and height on each view. In the previous
paper, the average accuracy deviation of HoloLens
1’s hologram visualization result is 6.64%, which is
higher than the HoloLens 2’s average accuracy de-
viation 3.79% in this paper.

2. Virtual and Real-world Environment Overlapping
Ratio: For this experiment, we ﬁrst calculate the
gap or overlap between the red cube and the real-
world ﬂat surfaces. To compare with the previous
HoloLens 1 paper, we use accuracy deviation σR
A to
evaluate the result. Where L refers to the real-world

length of hologram cube’s one edge, and li is the dis-
tance between the distance from the farthest edge
to the target surface. However, we do not use the
same experiment. Instead, we use diﬀerent environ-
mental conditions as our environmental variables.
The result shows in Figure 17.

Fig. 17 Virtual and real-world environment overlapping ra-
tio result

In the HoloLens 1 environment overlapping ratio,
the best accuracy deviation is 70%, however, in the
HoloLens 2 environment overlapping ratio result,
the lowest accuracy deviation is 53.69%, and the av-
erage accuracy deviation is 30.71%, which is much
better than the previous device. This result shows
excellent progress from HoloLens 1 to HoloLens 2
on reconstructing the real-world environment.
Furthermore, according to our experiment result, we
recommend users or developers to have enough illu-
mination on the target surface to generate better
hologram results.

4.4 Real Environment Reconstruction

In the real environment reconstruction experiment re-
sult, after recording the reconstruction process video,
we estimate the construction result using two metrics:
construction time and completion percentage. The com-
pletion percentage CP is calculated as follows:

CP =

(cid:80)
(cid:80)

i∈N Hi
i∈N Ri

(8)

where N represents the number of measurements, Hi
is the hologram area generated by HoloLens 2, which
is estimated by using the square method, and Ri is the
real environment area.

1. Real-world Objects Reconstruction:

In this experiment, we ﬁrst record the construction
time, then we use the reconstruction result frames
to calculate the construction completion percentage,
CP. The Hi here means the hologram area on each

3.660.847.412.30.637.910123456789LengthWidthHeightAccuracy Deviation (%)Front ViewSide ViewTop ViewAverageAverage: 3.790102030405060708090100DaylightIndoorDaylightOutdoorNight timeindoor withhead lightand lampNight timeindoor withhead lightonlyNight timeindoor withlamp onlyNight timeoutdoorwith lamponlyAccuracy Deviation(%)Overlapping ratioHoloLens 2 Technical Evaluation as Mixed Reality Guide

11

side of the box, and Ri is the area of the real-world
box. The result shows in Figure 18.

that users or developers should be aware of is that
the irregular real-world corner could be challenging
for HoloLens 2 to identify and generate, if the per-
son stands far away. Therefore, users should face the
front of the corner to help the device construct the
corner hologram before using an app, an example
shows in Figure 21.

Fig. 18 Real-world objects reconstruction time and com-
plete percentage

This experiment is used to explore how much ob-
ject size will aﬀect objects construction outcomes
in HoloLens 2. According to the result, object size
and illumination will aﬀect the construction result
and the construction time. The larger the real-world
box, the longer time will take for HoloLens 2 to con-
struct the result. However, if the real-world box is
smaller than the small box and has insuﬃcient il-
lumination, the HoloLens 2 will see this object as
an environment obstruction or ignore it and fail to
construct a good hologram.

2. Real-world Indoor Room Reconstruction:

Similar to the real-world objects reconstruction ex-
periment, in this experiment, we document the con-
struction time and calculate the construction com-
plete percentage according to the recorded video, as
shown in Figure 19.

Fig. 19 Real-world rooms reconstruction time and complete
percentage

In this experiment, we evaluate the real-world en-
vironment construction result in diﬀerent room sizes
and diﬀerent illumination. Based on Figure 19, larger
rooms need more time to generate a holographic
room. Also, better illumination can aﬀect construc-
tion speed and the completion percentage. From our
experimental results, we can ﬁnd a unique situa-
tion in the graph that the construction completion
percentage of the biggest living room is not good
enough. The reason is that the HoloLens 2 can only
support about 50% of living room hologram in the
app simultaneously. Once the testers walk from one
side to the other side, the device will automatically
drop some hologram to reduce the load on the sys-
tem, as shown in Figure 20. In addition, one thing

Fig. 20 An example of HoloLens 2’s maximum hologram in
the largest room in our experiment

Fig. 21 An example of incomplete construction of a room’s
corner. The red circle on the left side indicate the incomplete
construction part in the corner, and the red circle on the right
side means the complete construction in the same corner

4.5 Speech Recognition

In the speech recognition experiment result, we use the
similar speech commands as the paper [7]. Also, we use
the same evaluation method, agreement rate, in our ex-
periment result. The agreement rate Ar indicates the
level of consensus among the participants for a speciﬁc
referent r and is deﬁned as
(cid:18) |Pi|
|Pr|

Ar =

(cid:88)

(9)

(cid:19)2

Pi∈Pr

where Pr is the set of operation commands for refer-
ent r and Pi is a subset of Pr. The speech recognition
agreement rate result of HoloLens 2 is shown in Figure
22, yellow and blue bars represent the agreement rates
Ar for the system-deﬁned commands and user-deﬁned
commands.

Base on Figure 22, the average agreement rates Ar
for the system-deﬁned commands and the user-deﬁned

Different size objects construction0102030405060DaylightindoorNighttime indoorwith head lightand lampNighttime indoorwith head lightNighttime indoorwith lampTime (second)Construction timeLarge tallRegularThinSmall020406080100Daylight indoorNighttime indoorwith head lightand lampNighttime indoorwith head lightNighttime indoorwith lampPercentage (%)Complete percentageLarge tallRegularThinSmall0102030405060Living roomPersonal roomRestroomWalk-in closetTime (second)Construction time020406080100Living roomPersonal roomRestroomWalk-in closetPercentage (%)Complete percentageAfter walkingin front ofthe corner 12

H.J., Guo, B. Prabhakaran

Fig. 22 Speech recognition agreement rate for the eighteen
testers for the user-deﬁned commands and system-deﬁned
commands

Fig. 23 The Azure Spatial Anchor error distance percentage
in six diﬀerent environments

creating spatial anchor apps in HoloLens 2, Android, or
Apple devices.

commands are 79.12% and 80.97%. According to the
HoloLens 1 speech recognition result [7], the average
agreement rates Ar for the system-deﬁned commands
and the user-deﬁned commands are 66.87% and 74.47%.
Therefore, we can ﬁnd a small improvement of the user-
deﬁned commands between HoloLens 2 and the previ-
ous device. On the other hand, for the system-deﬁned
commands, the speech recognition functionality of HoloLens
2 is much better than HoloLens 1; this is a signiﬁ-
cant improvement from the previous version to the cur-
rent version. Besides, one thing worth mentioning is the
command ”Open the start menu,” which is represented
on the HoloLens 2 instruction document. However, none
of our testers could successfully issue this command.

4.7 Research Mode – IMU sensors

In the IMU sensors experiments, we use an error per-
centage to calculate the diﬀerence between the IMU
sensors output result and actual movement:

IMUError(%) =

1
N

N
(cid:88)

i=1

|TN − PN |
TN

(11)

where N is the number of total tests, TN indicates the
true movement result of the test N and PN expresses
the device predicted result of the test N.

4.6 Azure Spatial Anchor

To calculate the error distance percentage between the
created spatial anchor and the server downloaded spa-
tial anchor, we use the blue and red lines show in Fig-
ure 11b as a benchmark to calculate the ratio. Since
the length and wide of the white table in real-world in
each picture will be the same, we can compute the error
distance by the following equation:

ErrorPercentage(%) =

(cid:12)
(cid:12)
(cid:12)

Ls
c
Ll
c





(cid:12)
(cid:12)
(cid:12)

− Ls
d
Ll
d
Ls
c
Ll
c

(cid:12)
(cid:12)
(cid:12)

W s
c
W l
c

+

(cid:12)
(cid:12)
(cid:12)

− W s
d
W l
d
W s
c
W l
c



 ∗ 100

(10)

where L and W indicate length and width, s and l
present short line segments and long line segments, and
c and d indicate the created and downloaded anchors
in real-world. We ask the testers to do this test process
three times in one environmental condition and do it in
six diﬀerent environments. The result shows in Figure
23.

We ﬁnd that the error percentages are all very small,
which shows the consistency from creating the Azure
Spatial Anchor in HoloLens 2 to retrieve it back from
the Azure server. In this case, we can easily and ac-
curately use this feature to collaborate with others by

1. Accelerometer:

For the ﬁrst evaluation test of the Accelerometer, we
ask the testers to walk in four diﬀerent directions
and diﬀerent distances, which are straight, back-
ward, right, and left with 3 or 4 meters, the test-
ing environment setting shows in Figure 12a. Then
we compute the moving distance generated by the
device and compare it with the actual moving dis-
tance with IMU Error. The result shows in Figure
24a. One thing to note is that the gravity acceler-
ation will aﬀect the Accelerometer result; once the
time gets longer, the error will become larger. That
is the reason why IMU sensors need to be used to-
gether to achieve complementary eﬀects. Therefore,
using the Accelerometer alone will make the out-
put result higher than expected and cause unstable
results.
In the second Accelerometer experiment, we ask the
testers to tilt their body in two diﬀerent directions
three times to calculate the average error. One is to
bow forward and tilt 90 degrees, altering their body
along the x-axis and y-axis, an example shows in
Figure 12d. The other is to tilt 90 degrees to the
right, which means moving their body along the X-
axis and Z-axis. The result shows in Figure 24b, the
error percentage value here is small, which indicates
that the Accelerometer performs well in the angle-
changing situation.

0102030405060708090100DaylightIndoorDaylightOutdoorNight time indoor withhead light and lampNight time indoor withhead light onlyNight time indoor withlamp onlyNight time outdoor withlamp only(%)Azure Spatial Anchor ErrorHoloLens 2 Technical Evaluation as Mixed Reality Guide

13

2. Gyroscope:

References

In the Gyroscope evaluation process, we ask the
testers to rotate the HoloLens 2 device along the X,
Y, and Z axes in three diﬀerent ways. Then, we can
calculate the error percentage by using IMU Error.
The result shows in Figure 24c. The rotation error
percentage is very low and these errors lie within
operation errors.

Fig. 24 The testing result of IMU sensors evaluation (a) Ac-
celerometer moving distance error percentage (b) Accelerom-
eter tilt angle error percentage (c) Gyroscope rotate error
percentage

5 Conclusion

In this paper, we evaluated most of the important fea-
tures in the HoloLens 2 [1] by building some related
apps to show their capacities and limitations. Also, we
explored the new version of research mode [2] in the
current device and showed its usability. Then, we com-
pared with the previous paper [7] with similar experi-
mental content to show the evolution between the two
versions of HoloLens. These results can be used both as
a benchmark and design criteria for MR applications.
At the same time, these can be used as a reference and
comparison target for other AR devices. Furthermore,
users can use the evaluation results of each sensor of
HoloLens 2 as a reference to conduct additional stud-
ies.

In the future, we plan to evaluate other features of
HoloLens 2 and use the existing results for diﬀerent
research objectives.

6 Declarations

Funding No funding was received.
Conﬂicts of interest Not applicable.
Availability of data and material Currently un-
available.
Code availability Currently unavailable.

1. Microsoft. Hololens (2015). https://www.microsoft.

com/en-us/hololens

2. D. Ungureanu, F. Bogo, S. Galliani, P. Sama, X. Duan,
C. Meekhof, J. St¨uhmer, T.J. Cashman, B. Tekin, J.L.
Sch¨onberger, et al., arXiv preprint arXiv:2008.11239
(2020)

3. P. Milgram, F. Kishino, IEICE Trans. Information Sys-

tems vol. E77-D, no. 12, 1321 (1994)

4. P. Milgram, H. Takemura, A. Utsumi, F. Kishino, in Tele-
manipulator and Telepresence Technologies, vol. 2351,
ed. by H. Das. International Society for Optics and Pho-
tonics (SPIE, 1995), vol. 2351, pp. 282 – 292. DOI
https://doi.org/10.1117/12.197321

5. G.C. Burdea, P. Coiﬀet, Virtual reality technology (John

Wiley & Sons, 2003)

6. R.T. Azuma, Presence: Teleoper. Virtual Environ. 6(4),
355–385 (1997). DOI https://doi.org/10.1162/pres.1997.
6.4.355

7. Y. Liu, H. Dong, L. Zhang, A. El Saddik, IEEE Mul-
timedia 25(4), 8 (2018). DOI https://doi.org/10.1109/
MMUL.2018.2873473

8. A. Dey, M. Billinghurst, R.W. Lindeman, J.E. Swan,
Frontiers in Robotics and AI 5, 37 (2018). DOI https:
//doi.org/10.3389/frobt.2018.00037. URL https://www.
frontiersin.org/article/10.3389/frobt.2018.00037
9. C. Brown, J. Hicks, C.H. Rinaudo, R. Burch, Ergonomics
in Design 0(0), 10648046211003469 (0). DOI https://doi.
org/10.1177/10648046211003469

10. M. Speicher, B.D. Hall, M. Nebeling, in Proceedings of
the 2019 CHI Conference on Human Factors in Com-
puting Systems (Association for Computing Machinery,
New York, NY, USA, 2019), CHI ’19, p. 1–15. DOI
https://doi.org/10.1145/3290605.3300767

11. S. Benford, C. Greenhalgh, G. Reynard, C. Brown,
B. Koleva, ACM Trans. Comput.-Hum. Interact. 5(3),
185–223 (1998). DOI https://doi.org/10.1145/292834.
292836

12. M. Billinghurst, H. Kato, in Proceedings of the First
International Symposium on Mixed Reality (1999), pp.
261–284

13. M. Fiorentino, R. de Amicis, G. Monno, A. Stork, in Pro-
ceedings. International Symposium on Mixed and Aug-
mented Reality (2002), pp. 86–318. DOI https://doi.org/
10.1109/ISMAR.2002.1115077

14. C. Hughes, C. Stapleton, D. Hughes, E. Smith, IEEE
Computer Graphics and Applications 25(6), 24 (2005).
DOI https://doi.org/10.1109/MCG.2005.139

15. Z. Pan, A.D. Cheok, H. Yang, J. Zhu, J. Shi,
DOI
URL

Computers and Graphics 30(1), 20 (2006).
https://doi.org/10.1016/j.cag.2005.10.004.
https://www.sciencedirect.com/science/article/
pii/S0097849305002025

16. Y.M. Tang, K. Au, H.C. Lau, G.T. Ho, C. Wu, Virtual
Reality 24(4), 797 (2020). DOI https://doi.org/10.1007/
s10055-020-00427-9

17. A.D. Cheok, M. Haller, O.N.N. Fernando, J.P. Wije-
sena, International Journal of Virtual Reality 8(2), 83–90
(2009). DOI https://doi.org/10.20870/IJVR.2009.8.2.
2729. URL https://ijvr.eu/article/view/2729

18. C. Coutrix, L. Nigay, (2006), pp. 43–50. DOI https://

doi.org/10.1145/1133265.1133274

19. R. Carotenuto, M. Merenda, D. Iero, F.G. Della Corte,
IEEE Transactions on Instrumentation and Measurement
68(7), 2507 (2019). DOI https://doi.org/10.1109/TIM.
2018.2866358

0102030405060708090100Walkstraight4 metersWalkright4 metersWalkbackward3 metersWalk left3 meters(%)Error percentage0102030405060708090100Tilt forward 90degree (x-axis and y-axis)Tilt right 90 degree(x-axis and z-axis)(%)Error percentage0102030405060708090100x-axis 180 degreeclockwisey-axis 90 degreeclockwisez-axis 90 degreecounterclockwise(%)Error percentage(a)(b)(c)14

H.J., Guo, B. Prabhakaran

20. E. Costanza, A. Kunz, M. Fjeld, Mixed Reality: A
Survey (Springer Berlin Heidelberg, Berlin, Heidel-
berg, 2009), pp. 47–68. DOI https://doi.org/10.1007/
978-3-642-00437-7 3

21. M.K. Bekele, R. Pierdicca, E. Frontoni, E.S. Malinverni,

J. Gain, J. Comput. Cult. Herit. 11(2) (2018). DOI
https://doi.org/10.1145/3145534

22. H. Chen, A.S. Lee, M. Swift, J.C. Tang, in Proceedings
of the 3rd International Workshop on Immersive Media
Experiences (Association for Computing Machinery, New
York, NY, USA, 2015), ImmersiveME ’15, p. 27–30. DOI
https://doi.org/10.1145/2814347.2814350

23. B. Aruanno, F. Garzotto, M.C. Rodriguez, in Proceed-
ings of the 12th Biannual Conference on Italian SIGCHI
Chapter (Association for Computing Machinery, New
York, NY, USA, 2017), CHItaly ’17. DOI https://doi.
org/10.1145/3125571.3125589

24. M.G. Hanna, I. Ahmed, J. Nine, S. Prajapati, L. Pan-
tanowitz, Archives of pathology & laboratory medicine
142(5), 638 (2018). DOI https://doi.org/10.5858/arpa.
2017-0189-OA

25. W. Wang, X. Wu, G. Chen, Z. Chen, ISPRS Interna-

tional Journal of Geo-Information 7(2) (2018). DOI
https://doi.org/10.3390/ijgi7020060. URL https://www.
mdpi.com/2220-9964/7/2/60

26. M. Garon, P.O. Boulet, J.P. Doiron, L. Beaulieu,
J.F. Lalonde, in 2016 IEEE International Symposium
on Mixed and Augmented Reality (ISMAR-Adjunct)
(2016), pp. 189–191.
DOI https://doi.org/10.1109/
ISMAR-Adjunct.2016.0073

27. G. Evans, J. Miller, M.I. Pena, A. MacAllister, E. Winer,
in Degraded Environments: Sensing, Processing, and
Display 2017, vol. 10197, ed. by J.J.N. Sanders-Reed,
J.T.J.A. III. International Society for Optics and Pho-
tonics (SPIE, 2017), vol. 10197, pp. 282 – 297. DOI
https://doi.org/10.1117/12.2262626

28. P. O’Connor, C. Meekhof, C. McBride, C. Mei, C. Bamji,
D. Rohn, H. Strande, J. Forrester, M. Fenton, R. Ha-
raden, T. Ozguner, T. Perry, in 2019 Symposium on
VLSI Circuits (2019), pp. C186–C187. DOI https://doi.
org/10.23919/VLSIC.2019.8778092

29. A. Vidal-Balea, O. Blanco-Novoa, I. Picallo-Guembe,
M. Celaya-Echarri, P. Fraga-Lamas, P. Lopez-Iturri,
L. Azpilicueta, F. Falcone, T.M. Fern´andez-Caram´es, En-
gineering Proceedings 2(1) (2020). DOI https://doi.
org/10.3390/ecsa-7-08210. URL https://www.mdpi.com/
2673-4591/2/1/52

30. S. Nikolov. Visualization and validation of safety param-
etersfor industrial robots using the hololens 2 (2020)
31. B.J. Park, S.J. Hunt, G.J. Nadolski, T.P. Gade, Scientiﬁc
Reports 10(1), 1 (2020). DOI https://doi.org/10.1038/
s41598-020-75676-4

32. G. Martin, L. Koizia, A. Kooner, J. Caﬀerkey, C. Ross,
S. Purkayastha, A. Sivananthan, A. Tanna, P. Pratt,
J. Kinross, Journal of Medical Internet Research 22(8)
(2020). DOI https://doi.org/10.2196/21486

33. J.B. Levy, E. Kong, N. Johnson, A. Khetarpal, J. Tom-
linson, G.F. Martin, A. Tanna, Future Healthc J (2021).
DOI https://doi.org/10.7861/fhj.2020-0146

34. J.O. Wobbrock, M.R. Morris, A.D. Wilson, in Proceed-
ings of the SIGCHI Conference on Human Factors in
Computing Systems (Association for Computing Machin-
ery, New York, NY, USA, 2009), CHI ’09, p. 1083–1092.
DOI https://doi.org/10.1145/1518701.1518866

