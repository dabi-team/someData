1
2
0
2

n
u
J

5

]

C
O
.
h
t
a
m

[

2
v
0
1
0
5
1
.
2
1
0
2
:
v
i
X
r
a

1

PMGT-VR: A decentralized proximal-gradient
algorithmic framework with variance reduction

Haishan Ye*, Wei Xiong*, and Tong Zhang, Fellow, IEEE

Abstract—This paper considers the decentralized composite optimization problem. We propose a novel decentralized variance
reduction proximal-gradient algorithmic framework, called PMGT-VR, which is based on a combination of several techniques including
multi-consensus, gradient tracking, and variance reduction. The proposed framework relies on an imitation of centralized algorithms
and we demonstrate that algorithms under this framework achieve convergence rates similar to that of their centralized counterparts.
We also describe and analyze two representative algorithms, PMGT-SAGA and PMGT-LSVRG, and compare them to existing
state-of-the-art proximal algorithms. To the best of our knowledge, PMGT-VR is the ﬁrst linearly convergent decentralized stochastic
algorithm that can solve decentralized composite optimization problems. Numerical experiments are provided to demonstrate the
effectiveness of the proposed algorithms.

Index Terms—Decentralized optimization, proximal-gradient, variance reduction.

✦

1 INTRODUCTION

F OR modern large-scale optimization problems, the dis-

tributed computing architectures based algorithms have
recently attracted signiﬁcant attention and have been stud-
ied extensively in machine learning, control, and optimiza-
tion communities. There are two main distributed settings
that have been widely studied, namely, the master/slave
setting and the decentralized setting. In the master/slave
setting, a parameter server [1] aggregates the local gradients
computed by all other agents and performs the update. In
the decentralized setting, agents connected by a network
are only permitted to communicate with their neighbors to
cooperatively solve the optimization problem. We further
illustrate them in Figure 1.

In the context of distributed optimization, the decen-
tralized setting has long been treated as a compromise
when a centralized topology is unavailable or the decen-
tralization is natural. However, there has been a growing
interest in the decentralized setting recently. Several reasons
account for this phenomenon: (a) decentralized setting is
of a lower communication cost at the busiest agent as this
agent only communicates with her neighbors instead of
all other agents [2]; (b) theoretically, several works have
shown that we could design decentralized algorithms with
a similar convergence rate as compared to their centralized
counterparts; and (c) in practical, efﬁcient topologies have
been proposed to achieve most communication efﬁciency
and numerical results are provided to give a positive answer
to the question whether decentralized algorithms can be faster
than its centralized counterpart [2], [3]. In this paper, we will
focus on the algorithms design.

• Equal Contribution.
• Haishan Ye is with Shenzhen Research Institute of Big Data; The Chinese

University of Hong Kong, Shenzhen, email: hsye cs@outlook.com.

• Wei Xiong is with Department of Probability and Statistics; University of
Science and Technology of China, email:weixiong5237@gmail.com.
• Tong Zhang is with Hong Kong University of Science and Technology,

email: tongzhang@ust.hk.

In the context of ﬁrst-order methods, with a recently
introduced gradient tracking technique, GT-DGD proposed
in [4] and GT-DSGD proposed in [5] (decentralized gradi-
ent descent and stochastic gradient descent with gradient
tracking) achieve similar convergence rates with CGD and
CSGD (centralized GD and SGD), respectively. Efforts have
also been made in developing the decentralized versions
of variance reduction algorithm. However, existing works
including [6], [7], and [8] either achieve a convergence rate
inferior to that of their centralized counterparts or rely on
extra assumptions. It remains an open problem whether a
decentralized variance reduction algorithm can approach
the performance of its centralized counterpart. In addition
to the convergence rate, prior linearly convergent decentral-
ized stochastic methods are conﬁned to the smooth opti-
mization problem and the composite optimization problem
where the objective function is the sum of a smooth function
and a non-smooth but convex one, is substantially less
explored. Decentralized proximal algorithms based on full
gradients that converge linearly are proposed by [9], [10],
and [11]. On the other hand, to the best of our knowledge,
there is no decentralized stochastic algorithm with a linear
convergence rate that can solve the composite optimization
problem.

In this paper, we consider the decentralized composite
optimization problem where agents connected by a network
cooperatively minimize a sum of smooth functions, plus a
non-smooth and convex one. In practical, the regularized
empirical risk minimization problem with data stored across
a network is naturally cast as a decentralized composite op-
timization problem. We propose a novel Proximal-gradient
algorithmic framework, called PMGT-VR, which is based
on a combination of Multi-consensus, Gradient Tracking,
and Variance Reduction. Our study indicates that PMGT-VR
methods can solve the composite optimization problem and
can achieve a linear convergence rate that matches their
centralized counterparts. In particular, two representative
algorithms, PMGT-SAGA, PMGT-LSVRG are described and

 
 
 
 
 
 
2

that gradient-tracking based decentralized algorithms can
also achieve linear convergence rates with nonsmooth regu-
larization term such as EXTRA ( PG-EXTRA ) [23], NIDS [16],
and Harnessing [4]. [11] proposed DAPG which is the ﬁrst
accelerated decentralized proximal gradient descent. DAPG
achieved the best-known computation and communication
complexities. Despite intensive studies in the literature, it
is still hard to extend a smooth decentralized algorithm to
non-smooth composite setting with the same convergence
property. For example, [26] proposed the communication
optimal decentralized algorithm for smooth convex opti-
mization. However, it is still an open question to design
a communication optimal decentralized algorithm for non-
smooth composite convex optimization.

When each fi(x) has the ﬁnite-sum form, variance re-
duction is an important and effective method in stochastic
convex optimization [27], [28], [29]. To reduce the compu-
tational cost of decentralized optimization, [30] proposed
the ﬁrst decentralized variance reduction method named
DSA which integrates EXTRA [15] with SAGA [27]. DBSA
tried to use proximal mapping to accelerate DSA [31]. AFDS
which combines variance reduction with acceleration tech-
nique can achieve the optimal communication complexity
for variance reduction based algorithms [32]. However,
DBSA and AFDS relied on the assumption that the proximal
mapping with respect to f (x) can be solved efﬁciently. To
conquer the expensive cost of solving proximal mapping,
[6] brought up GT-SAGA and GT-SVRG which only use the
gradient of each individual function. However, these two
algorithms can only achieve much inferior computation and
communication complexities compared with DBSA. DVR is
another proximal mapping free algorithm which achieves
better performance than GT-SAGA and GT-SVRG but still
inferior to DBSA and AFDS [33]. Recently, [8] proposed an
algorithm which can achieve optimal computation and com-
munication complexities. However, the algorithm required
that the local loss function fj’s are strongly convex and
each fi,j is convex. When the local loss functions fj’s are
similar, [7] demonstrated that gradient tracking and extra
averaging are helpful for lower computation and communi-
cation complexities. Though many decentralized variance
reduction algorithms have been proposed and achieved
good performance, it is hard to generalize them to handle
decentralized composite optimization.

1.3 Paper organization

In section 2, we introduce the formulation of the decen-
tralized composite optimization problem over a network,
notations used in this paper, and several important con-
cepts. In section 3, we discuss the limitation of existing
works, motivate our approach, and describe the PMGT-VR
framework and two representative algorithms: PMGT-SAGA
and PMGT-LSVRG in detail. In section 4, we provide the
main results of this paper and compare the proposed al-
gorithms with existing state-of-the-art works. In section 5,
we prove the theoretical results of the proposed algorithms.
We present numerical simulations in Section 6 and conclude
in Section 7.

Fig. 1. The left ﬁgure is a master/slave network; the right ﬁgure is a
decentralized network.

analyzed in detail. Our methodology can be extended to
other variance reduction techniques in a similar fashion.

1.1 Contributions

We summarize our contributions as follows:

1) We propose a novel algorithmic framework aim-
ing at solving the decentralized composite convex
problem over a connected and undirected network.
A large family of decentralized variance reduction
algorithms can be obtained in a similar fashion.

convergence

2) We establish the linear

rate of
PMGT-SAGA and PMGT-LSVRG for strongly convex
composite optimization problems. The established
rate matches that of centralized SAGA and SVRG.
3) Many existing works including proximal algorithms
proposed by [9] and [10] require that each agent’s
local loss function is convex. On the other hand, our
methods could apply to the sum-of-nonconvex set-
ting where the sum of local loss functions is strongly
convex, but local functions can be nonconvex.

4) To the best of our knowledge,

the proposed
PMGT-VR is the ﬁrst decentralized stochastic prox-
imal method that can achieve a linear convergence
rate.

1.2 Related work

We review existing works that are closely related to our
framework in this subsection. [12] proposed decentralized
subgradient method and its stochastic variant can be found
in [13]. In the distributed subgradient method, each agent
performs a consensus step and then a subgradient de-
scent with a diminishing step-size. However, this kind
of algorithms can not achieve a linear convergence even
for smooth strongly convex problem [14]. To conquer this
problem, EXTRA proposed to track differences of gradients
achieves the linear convergence rate [15]. After EXTRA, a
novel method named gradient tracking was proposed which
also tracks differences of gradients [4]. Recently, several
gradient-tracking based decentralized algorithms have been
proposed to achieve fast convergence rates and efﬁcient
communication [4], [16], [17], [18], [19], [20].

Decentralized composite optimization is another impor-
tant research topic and there are many works focusing on it
[21], [22], [23]. However, before the work of [9] and [10], all
decentralized proximal algorithms can only achieve a sub-
linear convergence rate even though f (x) is smooth strongly
convex. Recently, [24] and [25] proposed uniﬁed frame-
works to analyze a large group of algorithms, and showed

2 PROBLEM FORMULATION AND PRELIMINARIES
2.1 Problem Formulation

We consider the following decentralized composite opti-
mization problem:

h(x) , f (x) + r(x),

(1)

min
Rd
x
∈

n

m

P

P

i=1 fi(x) and fi(x) , 1

n
with f (x) , 1
j=1 fi,j(x)
m
where m is the number of agents whose local training sets
are of equal size n. The function fi(x) is the local loss func-
tion private to agent i; fi,j(x) denotes the local loss function
at the j-th training example of agent i; r(x) is a non-smooth
and convex regularization term shared by all agents. Note
that it has been shown that with algorithms unlimited in the
number of decentralized communication steps but limited
to one gradient and proximal computations per iteration, it
is not possible to achieve a linear convergence rate when
agents have different regularization functions by [25]. We
will make the following assumptions for the rest of this
paper:
Assumption 1. Each fi,j : Rd

R is L-smooth:

fi,j(y)

−

fi,j(x)

fi,j(x), y

≤ h∇

This implies that

→

x
i

−

+

L
2 k

y

2.

x
k

−

fi,j(x)

1
2L k∇
fi,j(x)

fi,j(y)
Assumption 2. f : Rd

≤

−

− ∇

2
fi,j(y)
k
fi,j(y), x
−
R is µ-strongly convex:

x, y

∀

y

i

,

− h∇

Rd.

∈

→

f (y)

−

f (x)

f (x), y

≥ h∇

x
i

−

+

µ
2 k

y

2,

x
k

−

From assumption 1, we know f is also L-smooth. For
any L-smooth and µ-strongly convex function f (x), we
have L
µ and we deﬁne the condition number of f (x)
≥
as κ = L
µ . Due to assumption 2 and the convexity of r(x),
h(x) has a unique global minimizer which is denoted as
x∗. We also assume that the regularization function r(x) is
proximable in the sense that its proximal mapping

proxη,r(x) = argmin

r(z) +

Rd

z

1
2η k

z

2

,

x
k

−

(cid:16)
can be computed efﬁciently. For instance, the proximal map-
ping of L1-regularization function is of a closed form.

(cid:17)

∈

Decentralized Communication In the decentralized set-
ting, due to the absence of the parameter server, the agents
cooperatively minimize the function h(x) based only on
decentralized communications over a given network. The
decentralized communication is deﬁned through an undi-
of m nodes where each node of the graph
rected graph
corresponds to an agent. In a decentralized communication
(1) vectors of size d to
step, each agent is allowed to send
their neighbors.

O

G

Gossip algorithms are generally used in the decentral-
ized setting [6], [7], [15], [24]. To model a gossip commu-
m.
nication step, we introduce the gossip matrix W
Agent i and j are allowed to exchange information if and
only if there exists an edge between them in the graph
,
G
= 0. In a gossip communication step, each agent
that is, wij 6
i will receive the current iterates of her neighbors. Then,

Rm

∈

×

(2)

(3)

(4)

each agent i updates her iterate by a weighted average of the
=0 wij xj . If
received iterates and her iterate, that is,
we deﬁne the following aggregated notation of the column
vectors x1, ..., xm:

m
j=1,wij

P

3

x = [x1,

, xm]⊤,

(5)

· · ·
the decentralized communication can be abstracted as mul-
tiplication by the gossip matrix:

xnew = W xold.

The gossip matrix is assumed to satisfy several conditions.

Assumption 3. Let W be the gossip matrix. We assume that

1) W is symmetric;
2) 0

(cid:22)

I, W 1 = 1, null(I

W
where I is the m
dimensional all one column vector.

W ) = span(1),
m identity matrix and 1 is the m-

×

−

(cid:22)

k

x

−

−

1
m

1
m

k ≤

W x

11⊤x

11⊤x

As a result of assumption 3, the second-largest singular
value of W , denoted as λ2(W ), is strictly less than 1. It is
also called the mixing rate of the network topology since we
λ2(W )
. Therefore,
have
λ2(W ) indicates how fast the variables will be averaged
(cid:13)
(cid:13)
(cid:13)
(cid:13)
through decentralized communications. For instance, a fully
11⊤ has λ2(W ) = 0 but
connected network with W = 1
m
each agent communicates from and to all other agents; exp2-
log2(m
2/(2
ring has λ2(W ) = 1
) and each agent
⌊
neighbors. With the exp2-ring
1)
is connected to
⌋
topology, each agent is connected to the agent 20, 21, 22, ...
hops away and the weights are set uniformly. Many existing
decentralized optimization algorithms can be implemented
efﬁciently with exp2-ring by [3].

−
log2(m

1)
⌋

−

−

⌊

2.2 Notation

Through the rest of this paper,
denotes 2-norm for vec-
k·k1 denotes 1-norm
tors and Frobenius norm for matrices.
for vectors.
is the inner product of two vectors. When
h·
” is applied to vectors of the same dimension, it
notation ”
means element-wise ”less than or equal to”. We also deﬁne
the following notations:

k·k

≤

·i

,

(6)

(7)

F (x) = [

∇

f (x1),

∇

,

· · ·

f (xm)]⊤,

∇

and

¯x =

1
m

1⊤x, R(x) =

1
m

m

r(xi),

Xi=1

where we use the convention that xi is the i-th row of matrix
x. Besides, the iteration index will be used as superscript.
For instance, xt
i is the iterate of agent i at iteration t. In
next subsection, We will introduce several important con-
cepts and their related notations including s, v, ¯s, ¯v. These
notations are deﬁned similarly. Moreover, we denote the
aggregated proximal operator as

proxmη,R(x) = argmin

R(z) +

z

Rm×d

1
2mη k

z

x
k

−

2

.

(8)

∈

(cid:16)
We will also use the following notations. Df (x, y) = f (x)
−
f (y)
is the Bregman divergence associated
− h∇
). Similar to the centralized analysis framework [34],
with f (
·
the deﬁnitions of gradient learning quantity ∆t and Lyapunov

f (y), x

−

(cid:17)

y

i

6
function V t can vary depending on the associated variance
reduction technique. For PMGT-SAGA, we have

∆t =

1
mn

m,n

Xi,j=1

fi,j(φt

i,j )

k∇

fi,j(x∗)
k

− ∇

2.

(9)

For PMGT-LSVRG, we have

∆t =

1
mn

m,n

Xi,j=1

fi,j(wt
i)

k∇

fi,j(x∗)
k

− ∇

2.

(10)

For both PMGT-SAGA and PMGT-LSVRG, we have

V t =

k

¯xt

x∗

2 + 4nη2∆t.

(11)

Variables η, φt
Algorithm 3 and are introduced in section 3.

i,j, and wi

k

−
t are deﬁned in Algorithm 2 and

The performance of a decentralized optimization algo-
rithm is usually measured by two quantities, namely, the
computation complexity T and the communication com-
plexity C. The computational complexity and the commu-
nication complexity are the number of component gradient
fi,j(x)) and the number of
evaluations (evaluation of
∇
decentralized communications for each agent, respectively,
to achieve ǫ-approximate solution which is deﬁned as

max

1
m

(cid:26)

1¯xt

2

,

¯xt

xt

−

−

in the decentralized setting.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2

x∗

< ǫ.

(12)

(cid:27)

(cid:13)
(cid:13)

2.3 Preliminaries

∇

P

The PMGT-VR framework is based on several techniques
including variance reduction, gradient tracking, and multi-
consensus. We brieﬂy introduce these concepts in this sub-
section. Their impacts on the proposed decentralized frame-
work will be illustrated in sections 3 and 5.
SGD and Variance Reduction We consider a single-machine
empirical risk minimization problem where the objective
n
function is of the form: g(x) := 1
i=1 gi(x) and we
n
assume that each gi is L-smooth and µ-strongly convex.
Therefore, g(x) has a unique minimizer x∗. Stochastic gra-
dient descent (SGD) is a popular and powerful approach
and has been extensively used for this problem when n is
large. SGD estimates the full gradient
g(x) by a stochastic
gi(x) where i is picked from
one
at random.
gi(x) is roughly n times faster than
Since computing
g(x), SGD is of a lower per-
computing the full gradient
∇
iteration cost as compared to gradient descent (GD) based
on full gradients. However, SGD can only admit a sublinear
convergence rate even for strongly convex and smooth
gi(x).
problems due to the variance of gradient estimator
∇
2] < ǫ,
To reach an ǫ-approximate solution, i.e., E[
x∗
k
( 1
SGD requires
ǫ ) times component gradient evaluation.
Variance reduction techniques are designed for the stochas-
tic optimization problem where a better gradient estimator v
is used so that the variance gradually tends to zero. Variance
reduction methods have a low per-iteration cost similar to
SGD and, at the same time, achieve a linear convergence
rate. Many existing variance reduction methods require
O((n + κ)
ǫ ) times component gradient evaluation to
achieve an ǫ-approximate solution [27], [28], [35], [36].

∇
1, 2, .., n
{

log 1

x
k

∇

O

−

}

·

Algorithm 1 PMGT-VR Framework
1: Input: x0
η = 1

i = x0
12L and K =

j for 1

1
√1−λ2(W )

i, j,

≤

≤

O (cid:18)

m, v−1 = s−1 =

log max (κ, n)

.

(cid:19)

2: for t = 0, . . . , T do
3:
4:

Update the local stochastic gradient estimators vt;
st
gradient
the
Update
vt−1, K
st−1 + vt
FastMix
.
xt+1 = FastMix(proxηm,R(xt
(cid:0)

ηst), K);

trackers

local

as

−

(cid:1)
−

5:
6: end for
7: Output: xT +1.

4

F (x0),

∇

=

Gradient tracking Gradient tracking proposed by [37] and
later adopted by [4] is a powerful gradient estimation
scheme for the decentralized optimization problem. The
innovative idea of gradient tracking is to asymptotically
i) by a gradient tracker
approach the global gradient
∇
st
i through local computations and decentralized commu-
nications. This gradient estimation scheme performs the
following update at each iteration

f (xt

st+1 = W st + vt+1

vt,

−

∇

where vt
i is the local (variance reduction) gradient estimator
fi(x) private to agent i at iteration t. With the aid of
of
gradient tracking, agents can dynamically track the average
of local gradient estimators vt
i by the average of global
gradient trackers st

i, that is,

¯st+1 = ¯vt+1.

k

k

−

¯st

xt

and

1¯st

i →

st
k

k
→ ∇

1¯xt
−
f (¯xt).

Under the assumptions 1 and 3, we can show that as the
consensus errors
tend to zero,
one has st
Multi-consensus Multi-consensus simply means that there
are multiple decentralized communication steps within one
iteration. If we denote the number of decentralized com-
munication steps as K, multiplication by W K improves the
mixing rate from λ2(W ) to λ2(W )K . Therefore, if we set
K to inﬁnity, all agents will share ¯xt which is the same as
the master/slave case. Motivated by this observation, we
choose K > 1 so that the convergence rate will not be
degraded by a poorly connected network (where λ2(W )
is close to 1). Moreover, multi-consensus can be naturally
accelerated (e.g. via algorithm 4, FastMix) and leads to a
better dependence on the topology.

There are two gradient estimators in the PMGT-VR frame-
work (Algorithm 1). Whereas the term ”global gradient
tracker” refers to the gradient-tracking estimator st
i, ”local
gradient estimator” refers to the local variance reduction
estimator vt
i.

3 ALGORITHM DEVELOPMENT
The complete PMGT-VR framework is described in algo-
rithm 1 which is based on a combination of variance reduc-
tion (line 3), gradient tracking (line 4), and multi-consensus
(FastMix). In what follows, we start with DSGD, and dis-
cuss the challenges of decentralized stochastic optimization
algorithm design. We will add these ingredients one by one
to illustrate how they affect the performance of the proposed
algorithmic framework. Let us assume that r(x) = 0 ﬁrst
and we will extend to the composite optimization problem
later.

3.1 The General Framework
Decentralized SGD DSGD is based on decentralized com-
munication and updates with local gradients [2], [13] where
each agent i performs the following iterative update:
i = (W xt)i − ∇
xt+1

fi,ji (xt

i),

fi,ji (xt
−
i

1

∇

) is the local stochastic gradient. Under
where
the assumption that each fi is L-smooth and µ-strongly con-
2
vex, [38] shows that with a constant stepsize, E
k
decays linearly to a neighbourhood of the minimizer which
is characterized as

xt
i −

x∗

k

n

E

1
n

lim sup
t
→∞

xt
i −

x∗

2
2

Xi=1
+

ησ2
mµ

h(cid:13)
(cid:13)
η2κ2σ2

λ2(W )

1

−

i
η2κ2

(cid:13)
(cid:13)
+

2
fi(x∗)
k
λ2(W ))2

m
i=1 k∇
−

m(1
P

=

O  

,

!

(13)

where σ2 is the upper bound of variances of local gradient
noise and η is a constant stepsize. Unlike SGD, in addition to
the variance of local stochastic gradients, the performance
of DSGD is also degraded by the dissimilarity among the
datasets across the agents. We can see that DSGD has an
additional bias (the third term of Eqn. (13)) which could be
arbitrarily large due to the dissimilarity among the datasets
across agents. Moreover, the minimizer x∗ may not neces-
sarily be a ﬁxed point (in expectation) of the above update
as

fi(x∗) can be non-zero in general.

∇
Gradient tracking The above issues can be overcome
f (x). However, agents
by using the global full gradient
have no access to the global gradient and thus an efﬁcient
global gradient estimation scheme based on decentralized
communication is required. Motivated by this observation,
[5] proposed GT-DSGD whose iterative update is performed
as

∇

xt+1
i = (W xt)i −
st+1
i = (W st)i +

st
i,
fi,ji (xt+1

i

)

fi,ji (xt

i).

− ∇

∇
f (xt
i)
With gradient tracking, each agent i can approach
by the global gradient tracker st
i and this approximation
eliminates the bias in the theoretical bound of DSGD. In this
case, when the network is well connected, the convergence
behavior of GT-DSGD is determined only by the stepsize
sequence and the variance of local stochastic gradient which
is similar to SGD.

∇

∇

Variance reduction GT-DSGD can only achieve a sub-
linear convergence rate. Similar with the centralized case,
it is natural to use variance reduction gradient estimator
in GT-DSGD to estimate the local full gradient
fi(x). [6]
explored this idea and proposed GT-SVRG and GT-SAGA
3. The
that converge linearly under assumptions 1
computation complexities (the same as communication
complexities) of GT-SVRG and GT-SAGA to reach an ǫ-
and
approximate solution are
λ2(W ))2 ) log 1

(n + κ
O
−
(cid:16)
, respectively. We can see that
O
a GT-VR algorithm can only achieve a convergence rate
inferior to that of its centralized counterpart. In particular,
the computation complexity is worse by a factor dependent
on the network topology which is the cost of being decen-
tralized.

λ2(W ))2 ) log 1

(n +

log κ

−

(cid:17)

(cid:16)

(cid:17)

(1

(1

−

κ

ǫ

ǫ

2

2

5

Multi-consensus In the decentralized setting, agents
asymptotically achieve consensus through decentralized
communications. Combining with gradient tracking, both
consensus errors and gradient tracking error (formally de-
ﬁned in section 5) asymptotically tend to zero in the gradient
tracking framework. In this case, we may expect that the
decentralized algorithms asymptotically behave similarly
with their centralized counterparts. However, the consensus
steps will be inﬂuenced by the mixing rate λ2(W ). From the
theoretical guarantees of GT-VR and GT-DSGD, we can see
that the performance of an algorithm with gradient tracking
degrades when the mixing rate is close to 1. This issue can
be overcome by involving K decentralized communication
steps within one iteration to improve the mixing rate from
λ2(W ) to λ2(W )K . This motivates the usage of multi-
consensus. With K suggested in Theorem 1, the mixing
rate is improved to be good enough so that the iteration
complexity will be independent of the network topology.
In this case, we can approach the centralized performance
even with a poorly connected network. Moreover, multi-
consensus also contributes to a better dependence on the
topology because it can be naturally accelerated via algo-
rithm 4. The resulting faster convergence rate and acceler-
ated consensus steps compensate the extra communication
costs of multi-consensus. Consequently, the overall commu-
nication complexity of PMGT-VR is also generally lower as
compared to GT-VR algorithms.

−

To summary, the introduced gradient tracking and multi-
consensus essentially make the proposed algorithms asymp-
totically behave like a centralized one independent of the
network topology. Therefore, the instantiations of PMGT-VR
can approximate their centralized counterparts and have
similar convergence properties. In particular, since the pro-
posed framework uses variance reduction technique, it con-
3.
verges linearly to the minimizer x∗ under assumptions 1
Extension to the composite setting Finally, one can
further incorporate the proximal mapping with our frame-
work. Although it is usually non-trivial to extend a smooth
decentralized algorithm to the composite setting with the
same convergence rate, the extension is rather natural in
our framework. Our analysis relies on characterizing the
interrelationship of several quantities (formally deﬁned in
section 5) by a linear system inequality. In our framework,
the usage of proximal mapping results in another inequality
about the considered quantities resulting from the non-
expansiveness of the proximal mapping. With or without
the proximal mapping, the obtained linear system inequal-
ities are different in the coefﬁcients. However, we control
the spectral radius of the coefﬁcient matrices directly by
the parameter K so the techniques for the smooth case can
be extended to the composite case but with different per-
iteration number of communications.

3.2 PMGT-SAGA and PMGT-LSVRG

Different choices of local variance reduction gradient esti-
mator v (line 4 of algorithm 1) lead to different PMGT-VR
algorithms. In this section, we describe and compare two
representative algorithms PMGT-SAGA and PMGT-LSVRG
that are described in algorithm 2 and algorithm 3, respec-
tively.

m, v−1 = s−1 =
≤
log 41 max (24κ, 4n).

∇

F (x0),

i, j,

≤
1
√1−λ2(W )

j for 1

i = x0
12L and K =
i,j = x0
i
i ,

Algorithm 2 PMGT-SAGA
1: Input: x0
η = 1
2: Take φ0
3: for t = 0, . . . , T do
4:
5:
6:
7:

∈ {

∈ {

∀
In parallel, for each agent i:
Pick a ji uniformly at random from
Take φt+1
i,ji
Update the local variance reduction estimator:

1, . . . , n
.
}
{

= xt
i.

1, 2, ..., n
.
}

1, 2, ..., m

, j

}

vt

i =

∇

fi,ji (φt+1
i,ji )

− ∇

fi,ji (φt

i,ji ) +

1
n

n

∇

Xj=1

fi,j (φt

i,j).

8:

local

Update
the
vt−1, K
st−1 + vt
FastMix
i = FastMix(proxηm,R(xt
xt+1
(cid:0)
9:
10: end for
11: Output: xT +1.

gradient
.

(cid:1)
−

−

tracker

as

st

=

ηst), K)i.

PMGT-SAGA is a SAGA-based implementation of the pro-
posed framework. As the original SAGA, each agent needs
to store a table of gradients for the local variance reduction
gradient estimator. At each iteration t, each agent i picks
fi,ji (φt+1
ji from
i,ji )
i) and all other entries of the table remain
with
unchanged. Then, each agent i updates her local gradient
estimator by

at random and it replaces

{
fi,ji (xt

1, 2, ..., n

∇

∇

}

vt

i =

fi,ji (φt+1
i,ji )

∇

fi,ji (φt

i,ji ) +

− ∇

1
n

n

fi,j(φt

i,j ), (14)

Xj=1 ∇

where φt
i,j is the most recent iterate before iteration t at
) is evaluated. We see that PMGT-SAGA has
fi,j(
which
·
(dn) for each agent due to the
a storage complexity of
gradient tables.

∇

O

i, j,

m, v−1 = s−1 =

F (x0),

log 41 max (24κ, 4n).

∇

≤
1
√1−λ2(W )
.
}

1, 2, ..., m

Algorithm 3 PMGT-LSVRG
i = x0
1: Input: x0
j for 1
≤
η = 1
12L , p = 1
n , and K =
2: Take w0
i = x0
i ,
3: for t = 0, . . . , T do
4:
5:
6:

∈ {

i

∀
In parallel, for each agent i:
Pick a ji uniformly at random from
1, . . . , n
.
}
{
Update the local variance reduction estimator:
fi(wt

fi,ji (xt
i)

fi,ji (wt

i) +

i =

vt

i).

∇

− ∇

∇

7:

Take wt+1

i = xt

i with probability p; otherwise wt+1

i = wt
i.

8:

local

Update
the
vt−1, K
st−1 + vt
FastMix
i = FastMix(proxηm,R(xt
xt+1
(cid:0)
9:
10: end for
11: Output: xT +1.

gradient
.

(cid:1)
−

−

tracker

as

st

=

ηst), K)i.

PMGT-LSVRG is a LSVRG-based implementation of the
proposed framework. SVRG proposed by [28] contains two
loops and computes the full gradient at the beginning of
each inner loop. However, the theoretically optimal inner
loop size depends on both L and µ which may be hard to
estimate for a real-world dataset and the analysis is also
more complicated for handling the double loop structure.

To conquer this dilemma, [35] designs a variant of the
original SVRG, called Loopless SVRG (LSVRG) in which the
outer loop is removed and the agent updates the stored full
gradient in a stochastic manner. In the case of PMGT-LSVRG,
the local variance reduction gradient estimator is given by

6

vt

i =

fi,ji (xt
i)

fi,ji (wt

i) +

fi(wt

i),

∇

∇
− ∇
i is the most recent iterate at which

where wt
) is evalu-
fi(
·
ated. At each iteration, each agent i picks ji from
1, 2, ..., n
}
{
at random and the reference point wt+1
is replaced with a
small probability p by the xt
i and is left unchanged with
p. For LSVRG, a simple choice of p = 1/n
probability 1
leads to a convergence rate identical to that of SVRG.

∇

−

i

(15)

·

(n + 2) + (1

By using the estimator in PMGT-LSVRG, agents need
not store a table of gradients. However, the expected per-
iteration number of gradient evaluations for each agent is
2 = 3 when p = 1/n for PMGT-LSVRG.
p
Therefore, PMGT-LSVRG suffers from a higher expected per-
iteration cost as compared to PMGT-SAGA. We observe that
there is a trade-off between storage and computation efﬁ-
ciency here and users can implement PMGT-VR algorithms
based on their customized needs.

p)

−

·

4 MAIN RESULTS

In this section, we establish the convergence rate of
PMGT-SAGA and PMGT-LSVRG which matches that of cen-
tralized SAGA and LSVRG. Before continuing, we ﬁrst deﬁne
the vector of consensus errors.

zt = [

1
m

1¯xt

2

,

xt

−

η2
m

1¯st

2

]⊤.

st

−

(16)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
4.1 Main Results
Theorem 1. Let assumptions 1, 2, and 3 hold. If we set p = 1
n
for PMGT-LSVRG and choose K =
ρ where ρ
satisﬁes

1
λ2(W )

log 1

√1

(cid:13)
(cid:13)

−

1
41

ρ

≤

min

1
24κ

,

1
4n

,

(17)

(cid:18)
and choose stepsize η = 1/(12L), then, for both PMGT-SAGA
and PMGT-LSVRG it holds that

(cid:19)

E

V t

(cid:2)

(cid:3)

and

max

≤

1
(cid:18)

−

1
24κ

, 1

−

1
4n

t

V 0 +

z0

(18)

(cid:19)

(cid:0)

(cid:13)
(cid:13)

(cid:1)

(cid:13)
(cid:13)

E

max

(cid:18)

1
45387

(cid:20)

(cid:18)

1
m

1¯xt

2

,

xt

−

1
144mL2

2

1¯st

st

−

(cid:13)
(cid:13)
min

1
36κ2 ,

(cid:18)
1
24κ

, 1

−

(cid:13)
1
(cid:13)
n2
1
4n

(cid:19)

t

·

+ 2−

t

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:19)
V 0 +

z0

.

max

·

1
(cid:18)

−

(cid:19)
To ﬁnd an ǫ-approximate solution, the computation complexity T
and communication complexity C are

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:0)

(cid:1)

≤

(cid:19)(cid:21)

(19)

T =

O

(cid:18)

max (κ, n) log

1
ǫ

,

(cid:19)

C =

O  

max (n log n, κ log κ)

(20)

1
λ2(W )

log

1
ǫ !

.

1

−

p

Remark 1. PMGT-SAGA and PMGT-LSVRG admit a computa-
tion complexity (the same as iteration complexity) that matches
that of their centralized counterparts.

Remark 2. The communication complexity of the proposed
algorithms implicitly depends on the number of agents m
through the second-largest singular value of the gossip matrix
W . One may use the exp2-ring topology introduced in sec-
tion 2 when the network can be designed. In this case, we have
λ2(W ) = 1
and each agent communicates with
2+
) neighbors. Therefore, the per-iteration number
log2(m
⌊
of communications with neighbors for the proposed algorithms
. On the other hand, the per-
is
O
iteration communication cost for the master/slave setting is
(cid:17)
(m). We can see that PMGT-VR methods can be preferable when

(log κ + log n) log3/2 m
(cid:16)

2
log2(m

−
1)
⌋

O
m is large as compared to their centralized counterparts.

−

1)

−

⌋

⌊

4.2 Comparison to existing algorithms

In this subsection, we discuss the convergence properties
established above. Table 1 presents a detailed comparison
of our algorithms with state-of-the-art proximal algorithms
(PG-EXTRA and NIDS) and the algorithms that are closely
related to our framework (GT-SAGA and GT-SVRG).

We can see that

We ﬁrst note that

the computation complexity of
PMGT-SAGA and PMGT-LSVRG is the same as that of central-
ized SAGA and LSVRG which is not surprising as our algo-
rithms are based on the imitation of their centralized coun-
terparts and with a network improved by multi-consensus.
the computation complexities of
GT-SAGA and GT-SVRG are signiﬁcantly worse than that of
PMGT-SAGA and PMGT-LSVRG. Besides, as the faster conver-
gence rate compensates for the extra rounds of decentralized
communications in the proposed framework, the communi-
cation complexity of PMGT-SAGA and PMGT-LSVRG is also
generally much lower than those of GT-SAGA and GT-SVRG.
This illustrates the beneﬁts of multi-consensus.

We only compare our algorithms with NIDS as
PG-EXTRA is inferior to NIDS. We see that PMGT-SAGA and
PMGT-LSVRG improve the computation complexity of NIDS
from
which
ǫ
illustrates the beneﬁts of stochastic variance reduction al-
gorithms over algorithms based on full gradients. On the
other hand, the communication complexity of PMGT-SAGA

(n + κ) log 1
ǫ

λ2(W ) ) log 1

n(κ +

O

O

to

(cid:16)

(cid:17)

−

(cid:0)

(cid:1)

1

1

and PMGT-LSVRG is

(n log n+κ log κ)

log 1
ǫ

which is in

ǫ

1

1

−

(cid:17)

(cid:16)

λ2(W )

√1
−
(κ +

(cid:18)
O

(cid:19)
λ2(W ) ) log 1

O
commu-
general worse than the
nication complexity of NIDS. This can be interpreted as
a trade-off between computation and communication efﬁ-
ciency. To further discuss this trade-off, we introduce the
notation τ borrowed from [33], which is the relative ratio of
communication cost and computation cost. In other words,
fi,j(x) is of cost 1 and the cost of one round of
evaluating
decentralized communication is τ . The beneﬁts of stochastic
PMGT-VR methods are clearer for a relatively small τ which
corresponds to the case when computation cost dominates.
However, in numerical experiments section, we show that
PMGT-SAGA and PMGT-LSVRG outperform PG-EXTRA and
NIDS in terms of cost for a wide range of τ .

∇

7

5 CONVERGENCE ANALYSIS
5.1 Convergence analysis: A sketch

In this section, we provide several useful lemmas which
illustrate the impacts of the ingredients used in the proposed
framework. Throughout our analysis, we will consider the
following quantities:

1)
2)
3)

4)

2

2

2 , η

xt

1¯st

m k

st
1¯xt
consensus errors: 1
m k
k
−
k
−
f (¯xt)
¯st
,
the gradient tracking error:
k
− ∇
k
2 and
f (x∗)
¯st − ∇
the gradient learning error:
k
k
gradient learning quantity: ∆t (deﬁned in Eqn. (9)
and Eqn. (10)),
x∗
the convergence error:
k
function V t (deﬁned in Eqn. (11)).

and Lyapunov

¯xt
k

−

,

To summary, we will construct a linear system inequality
about the above quantities. The idea is that the spectral
radius of the coefﬁcient matrix associated with this linear
system inequality is less than 1 if we use K suggested
in Theorem 1. In this case, both consensus errors and
convergence error will decay linearly. Moreover, since our
algorithms can approximate their centralized counterparts
well, we can use similar techniques from the single-machine
framework in [34].

5.2 The linear system inequality

To derive the desired linear system inequality, we show
that the quantities are interrelated by the results of gra-
tracking, proximal mapping and multi-consensus
dient
(FastMix). We start with the results of gradient tracking.
Lemma 1. Let assumption 1 hold. For both PMGT-SAGA
it holds that ¯st = ¯vt and E[¯st] =
and PMGT-LSVRG,
1
m

i). Furthermore, we have

fi(xt

m
i ∇

P

f (¯xt)

∇

−

E[¯st]

L
√m

≤

xt

−

1¯xt

.

(21)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can see that ¯st dynamically tracks the average of local
variance reduction gradient estimators and the gradient
tracking error is upper bounded by the consensus error.
Then, we introduce several properties of proximal mapping
and multi-consensus which result in an iterative inequality
of consensus errors.
Lemma 2. Let x0, xK
d be the input and output of
×
FastMix (Algorithm 4 in the appendix), respectively and ¯x =
1
m

1⊤x0. Then we have

Rm

∈

1
m

ρ

−

x0

(cid:13)
(cid:13)
(cid:13)

xK

1x

, ¯x =

1⊤xK .

1x
−
≤
(cid:13)
(cid:13)
(cid:13)
λ2(W ))K .
where ρ = (1
(cid:13)
(cid:13)
−
Lemma 3. Let prox(i)
ηm,R(x) and xi denote the i-th row of
the matrix proxηm,R(x) and x, respectively. Then, we have the
following equality and inequality

(22)

p

(cid:13)
(cid:13)

−

1

prox

(i)

ηm,R(x) = proxη,r(xi),
1
m

11⊤x)

−

1
m

11⊤proxηm,R(x)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

proxηm,R(

(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤

x

−

1
m

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

11⊤x
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(23)

(24)

8

Methods
GT-SVRG [6]

GT-SAGA [6]

PG-EXTRA [23], [24]

NIDS [16], [24]

Our methods

Problem Complexity of computation Complexity of communication

f

f

f + r

f + r

f + r

2

(n + κ

log κ

(1−λ2(W ))2 ) log 1
(1−λ2(W ))2 ) log 1

κ

ǫ

ǫ

2

O (cid:16)
O (cid:16)

(n +

nκ

(1−λ2(W )) log 1

ǫ

(cid:17)

1

(1−λ2(W )) ) log 1

ǫ

O (cid:16)
n(κ +

O (cid:16)

(cid:17)

(cid:17)

(cid:17)

(n + κ) log 1
ǫ

O

2

(n + κ

log κ

(1−λ2(W ))2 ) log 1
(1−λ2(W ))2 ) log 1

κ

ǫ

ǫ

2

O (cid:16)
O (cid:16)

(n +

nκ

(1−λ2(W )) log 1

ǫ

(cid:17)
(1−λ2(W )) ) log 1

1

ǫ

O (cid:16)
(κ +

O (cid:16)

(cid:17)

(cid:17)

(cid:17)

(n log n+κ log κ)
√1−λ2(W )

log 1

ǫ (cid:19)

O (cid:18)

(cid:0)
TABLE 1
Complexity comparisons between PMGT-VR algorithms and existing works for strongly convex problem. Note that GT-SAGA and GT-SVRG can
only solve the smooth problems and other algorithms can solve both the smooth and composite problems.

(cid:1)

The iterative inequality of consensus errors is stated in

5.3 A uniﬁed analysis framework

the next lemma:
Lemma 4. For the general PMGT-VR framework, the consensus
errors satisfy:

2

1¯xt

(cid:13)
1¯st
(cid:13)

2

,

1
m

xt+1

−

1¯xt+1

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

η2
m

(cid:13)
(cid:13)

st+1

−

1¯st+1

2

(cid:13)
(cid:13)

≤

≤

st

−

8ρ2 1
xt
m
+ 8ρ2 η2
(cid:13)
(cid:13)
m
2ρ2 η2
(cid:13)
st
(cid:13)
m
−
+ 2ρ2 η2
(cid:13)
(cid:13)
m

−
1¯st

vt+1

(25)

2

(cid:13)
(cid:13)

vt

2

,

(cid:13)
(cid:13)
−

(cid:13)
(cid:13)
where η is a constant stepsize and ρ = (1

(cid:13)
λ2(W ))K .
(cid:13)

1

−

−

2

−

vt

vt+1

p
The linear system inequality is obtained once we can
for a speciﬁc variance reduction
bound the
gradient estimator. In what follows, the deﬁnitions of vt and
the gradient learning quantity ∆t depend on the algorithms
we are considering.
Lemma 5. For both PMGT-SAGA and PMGT-LSVRG, it holds
that

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Following the standard analysis framework of single-
machine algorithms based on SGD in [34], we state the
following two lemmas.
Lemma 7. For both PMGT-SAGA and PMGT-LSVRG, it holds
that

E

h
8L

≤

2
f (x∗)
k

¯st − ∇
k
Df (¯xt, x∗) + 2∆t +

i

·

4L2
m

1¯xt

2

.

xt

−

(28)

Lemma 8. For PMGT-SAGA and PMGT-LSVRG with p = 1
holds that

n , it

(cid:13)
(cid:13)

(cid:13)
(cid:13)

E

∆t+1
1
n

≤

(cid:3)

−

∆t +

(cid:2)
1
(cid:18)
With the above lemmas in hand, we are ready to prove

Df (¯xt, x∗) +

1¯xt

xt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

(cid:19)

.

4L
n

2

(29)

2L2
mn

the following result involving Lyapunov function.
Lemma 9. Setting η = 1
n for
PMGT-LSVRG, for both PMGT-SAGA and PMGT-LSVRG, it holds
that

12L and setting p = 1

E

V t+1

E

vt+1

vt

1
m

(cid:20)
8ρ2 + 1

(cid:13)
(cid:13)

≤

(cid:0)
+ 8L2

−
8L2
m
(cid:1)
¯xt+1

(cid:21)
(cid:13)
(cid:13)
xt

(cid:13)
x∗
(cid:13)
−

(cid:13)
(cid:13)
vt+1

(cid:13)
(cid:13)

2

1¯st

≤

2

+

1¯xt

64ρ2η2L2
m
+ 4∆t+1 + 8L2

¯xt

(cid:13)
(cid:13)

−
2

st

−
x∗

(cid:13)
(cid:13)
−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2

(cid:13)
+ 4∆t.
(cid:13)
(26)

(cid:2)
max

(cid:3)
1
−
(cid:18)
3√V t
√m
3.3
m

xt

(

+

+

1
12κ

, 1

−

1
2n

V t

·

(cid:19)

xt

−

1¯xt

+ η

st

(cid:13)
(cid:13)
−

1¯xt

2

(cid:13)
(cid:13)
+

(cid:13)
2.5η2
(cid:13)
m k

−

st

1¯st

)

(cid:13)
(cid:13)
1¯st

2.

k

−

(30)

vt

Substituting

in lemma 4, the desired linear

−
system inequality is stated in the next lemma.
Lemma 6. Using the deﬁnition of zt in Eqn. (16), then for both
PMGT-SAGA and PMGT-LSVRG, we have

(cid:13)
(cid:13)

(cid:13)
(cid:13)

E

zt+1

(cid:2)
2ρ2

≤

(cid:3)

·

(cid:18) (cid:20)

4,

4

8(8ρ2 + 1)L2η2, 64ρ2η2L2 + 1
(cid:21)

zt

·

+ η2

8L2(

¯xt+1

(cid:20)

−

x∗

2

+

k

0
¯xt

x∗

k

2) + 4(∆t+1 + ∆t)
(27)

.
(cid:21) (cid:19)

−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We can see that with a small enough ρ (corresponding
to a large enough K), the Frobenius norm of the coefﬁcient
matrix is small than 1 and so is the spectral radius.

5.4 Proof of Theorem 1

(cid:13)
(cid:13)

(cid:13)
(cid:13)

We now invoke the above results to give a detailed proof of
Theorem 1.

Proof of Theorem 1. We will establish the convergence rate by
induction. Since for t = 0, each agent shares the same x0
i and
s0
= 0. Therefore,
= 0 and
i , we have
by Eqn. (30), the following inequality holds for t = 1:

1¯x0

1¯s0

x0

s0

−

−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

E

V t

(cid:2)

(cid:3)

≤



max

1

(cid:18)




|

1
24κ

, 1

−

1
4n

−

α

{z

t



(cid:19)




}

V 0 +

z0

. (31)

(cid:0)

(cid:13)
(cid:13)

(cid:1)

(cid:13)
(cid:13)

Let β denote max
the induction inequality:

−

1

1
12κ , 1

(cid:0)

9

. We are ready to prove

1
2n

−

(cid:1)

Now, we assume that the inequality holds when t
k and
are going to prove Eqn. (31) holds for t = k + 1. Let us
denote

≤

A =

4,

4,

8(8ρ2 + 1)L2η2, 64ρ2η2L2 + 1
(cid:20)
By the step size η = 1
1
41 min

, we can obtain that

24κ , 1

4n

1

12L and assumption that ρ

(cid:0)

(cid:1)

< 6.

A
k
k
4 , and ρ < 1

< 6, α > 3
A
By
41 , we can further obtain that
k
A
ρ2
). By the deﬁnition of V k and Eqn. (27), we
α/(4
k
k
can obtain that

k
≤

E

V k+1
h
max

i

1
−
(cid:18)
√V k
√m

(32)

.

(cid:21)

(30)

≤

1
12κ

, 1

−

1
2n

V k

sk

·

(cid:19)

+

xk

1¯xk

−

2

1¯xk

(cid:13)
(cid:13)
2.5η2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+
m k

−

sk

1¯sk

(cid:13)
(cid:17)
(cid:13)
1¯sk
(cid:13)

k

2

−

≤

+ 3

(33)

+

(31),(35),(36)

xk

3.3
m
(cid:13)
(cid:13)
V 0 +
(cid:13)

(cid:16)(cid:13)
(cid:13)
(cid:13)
−
z0

≤

k
( β + 2.94ρ2α−
(cid:0)

k

(cid:1)

αk

·

(cid:13)
(cid:13)
(cid:13)
1 + 6.6

2ρ2

A
k

k

1

α−

k

k(

32ρ2αk

1

−

9

(cid:0)

+ 8 (2ρ2

(cid:1)
)k) )

A
k

k

z0

)(β + 3.92ρ2 + 6.6(16ρ2)k
k
k
1/2
√32ρα−

k

+ 2√2(2ρ2

k
(β + 0.0239ρ

A
k

α−

1)

2 ))

+ 3

α−

s
αk(V 0 +

≤

+ 3(

αk

≤

V 0 +

3
z0

k

(cid:0)

+ 0.1072ρ +

ρ + 24√2ρ)

k
2√32
(cid:1)
√3

(37)

(38)

(39)

(40)

E

zk

h(cid:13)
(cid:13)
ρ2/9
(cid:13)

≤

·

+

2ρ2

(cid:0)
ρ2/9

(31)

≤

·

2ρ2

(cid:13)
i
k
(cid:13)
(cid:13)
Xi=1
(cid:0)
A
k
k
k
(cid:1)
2ρ2

k

+

2ρ2

Xi=1
(cid:0)
A
k
k
ρ2(α + 1)
(cid:0)
9α

·

k

i

−

(V i + V i

−

1)

A
k
k

(cid:1)

z0

(cid:13)
(cid:13)
A
(cid:13)
(cid:13)
k
k

k

i

−

(αi + αi

−

1)(V 0 +

z0

)

(cid:1)

k

z0

k k
(cid:13)
(cid:13)
Xi=1 (cid:18)

(cid:1)

α
(cid:13)
(cid:13)
2

(cid:16)
k

(cid:17)
z0

1
2

i

−

(cid:19)

(V 0 +

z0

)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(2k+1

(cid:13)
(cid:13)
·

−

2)(V 0 +

z0

)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

=

k

(cid:1)

A
2ρ2
+
k
k
ρ2(α + 1)
α
(cid:13)
(cid:0)
(cid:13)
2
9α
·
(cid:16)
(cid:17)
k
z0
A
2ρ2
k
k
4ρ2αk
(cid:0)
−
9

(cid:1)
+

+

1

(cid:13)
(cid:13)
2ρ2
(cid:13)
(cid:13)
k

≤  

k

A
k
(cid:1)

! ·

(cid:0)
A
where the third inequality is because ρ2
) and
k
k
the last inequality is because of α < 1 and V 0 is non-
negative. Furthermore, we can obtain

(cid:13)
(cid:13)
α/(4

(cid:13)
(cid:13)

≤

xk

E

3.3
m
(cid:20)
6.6E

(cid:13)
zk
(cid:13)
(cid:13)
(cid:13)
h(cid:13)
(cid:13)
(cid:13)
2.94ρ2αk
(cid:13)
(cid:13)
(cid:16)

i
−

≤
(34)

≤

and

−

1¯xk

2

+

(cid:13)
(cid:13)
(cid:13)

2.5η2

m k

sk

1¯sk

2

k

−

(cid:21)

1 + 6.6

2ρ2

(cid:0)

k

k

A
k
(cid:1)

·

(cid:17)

(V 0 +

z0

)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

E

1
√m

(cid:20)
1
√m
(cid:20)q
1
(E
m

2

r
2√2

E

xk

k
zk

k

k
q
32ρ2αk

≤

≤

≤
(34)

≤ s(cid:18)

9

xk

(

1¯xk

+ η

−

(cid:13)
(cid:13)
E
(cid:13)

xk

k

−

(cid:13)
(cid:13)
1¯xk
(cid:13)

sk

k

−

1¯sk

)
(cid:21)

k

2 +

k

Eη2

q

sk

k

−

1¯sk

2

k

(cid:21)

1¯xk

k

−

2 + η2

sk

k

−

1¯sk

2)

k

(36)

1

−

+ 8 (2ρ2

)k

A
k

k

·

(cid:19)

(V 0 +

z0
k

).
k

(V 0 +

z0

),

can obtain that

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(34)

αk

≤

V 0 +

z0

max

(cid:0)
αk+1

≤

(cid:13)
V 0 +
(cid:13)

(cid:13)
(cid:1)
z0
(cid:13)

(cid:18)

.

1
12κ

, 1

−

1
2n

1
(cid:18)

−

+ 41ρ

(cid:19)

(cid:19)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

3
4 and √a2 + b2

(cid:1)
(cid:0)
Inequality (39) is because α
.
≥
|
Inequality (40) is because the condition (17) implies ρ <
A
1/41 and ρ2
). The last inequality is because of
k
≤
the condition ρ
min
. Thus, Eqn. (31) also
holds for t = k + 1 and we complete the proof.

α/(4
k
1
41 ·
Furthermore, using Eqn. (34) and ρ2

24κ , 1

), we

≤ |

+

≤

4n

a

b

(cid:1)

(cid:0)

1

|

|

α/(4

≤

A
k
k

1
m
(cid:18)
1
−

(cid:13)
(cid:13)
+

(cid:0)

min

E

max

(cid:20)
4ρ2αt
9
1
45387

≤

≤

(cid:18)

(cid:18)

1¯xt

2

,

xt

−

1
144mL2

2

1¯st

st

−

2ρ2

k

1
36κ2 ,

(cid:18)
1
24κ

, 1

−

(cid:13)
t
(cid:13)
A
k
(cid:1)
1
n2
1
4n

(cid:19)

(cid:19)
t

(cid:19)

(cid:13)
(cid:13)

V 0 +

(cid:13)
(cid:13)
z0

·

(cid:0)
+ 2−

t

(cid:19)
V 0 +

·

(cid:0)

(cid:1)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
·

z0

.

(cid:13)
(cid:13)

(cid:1)

(cid:13)
(cid:13)

(35)

max

1
(cid:18)

−

(cid:19)(cid:21)

(41)

6 NUMERICAL EXPERIMENT

In this section, we present several numerical experiment
results. We evaluate the performance of the proposed algo-
rithms using logistic regression with L1-regularization for
binary classiﬁcation:

fi(x) =

1
n

n

Xj=1

and

log[1 + exp(

−

bjh

)] +
aj, x
i

σi
2 k

2,

x
k

r(x) =

1
mn k

k1.
x

̄
)

⋆
x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

⋆
̄

⋆
x
(
h
−

̄

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

5

0
#Grad Evaluations(x1e4)

10

20

15

10−1

10−4

10−7

̄
)

⋆
x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−10

0

10−1

10−4

10−7

10−10

⋆
̄

⋆
x
(
h
−

̄

t

x
(
h
[
0
1
g
o

l

10

10−1

10−4

10−7

̄
)

⋆
x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

]
)

⋆
x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−10

10−10

100

200
#Communications

300

400

0

10

20

#Cost (x1e4), τ = 250

10−1

10−4

10−7

PMGT-LSVRG
PMGT-SAGA
PGEXTRA

0
#Grad Evaluations(x1e4)

40

20

0

500

1000
#Communications

0

20

40

#Cost (x1e4), τ = 250

Fig. 2. Performance comparison of PMGT-SAGA, PMGT-LSVRG, and PG-EXTRA with σ = 10−5n. The top row and the bottom row present the
results with 1 − λ2(W ) = 0.81 and 1 − λ2(W ) = 0.05, respectively.

̄
)

x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

⋆
̄

⋆
x
(
h
−

̄

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

̄
)

⋆
x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

200

0
400
800
#Grad Evaluations(x1e4)

600

0

2000

4000

6000

0

#Communications

250

500
#Cost (x1e4), τ = 250

750

1000

̄
)

x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

⋆
̄

⋆
x
(
h
−

̄

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

10−1

10−4

10−7

PMGT-LSVRG
PMGT-SAGA
NIDS
PGEXTRA

]
)

(
x
(
h
−

)

t

x
(
h
[
0
1
g
o

10−10

200

0
800
400
#Grad Evaluations(x1e4)

600

0

2000

4000

#Communications

0

250

500
#Cos# (x1e4), τ = 250

750

1000

Fig. 3. Performance comparison of PMGT-SAGA, PMGT-LSVRG, NIDS, and PG-EXTRA with σ = 10−7n. The top row and the bottom row present
the results with 1 − λ2(W ) = 0.81 and 1 − λ2(W ) = 0.05, respectively.

We conduct experiments with a real-world dataset a9a
(mn = 32560, d = 123) and set m = 20 which leads to n =
1628. We consider a random network where each pair of
L
agents is connected with probability p and set W = I
λ1(L)
where L is the Laplacian of the generated graph. We test our
algorithms with different networks where we observe that
1

λ2(W ) = 0.81, respectively.

λ2(W ) = 0.05 and 1

−

−

−

We will compare the performance of PMGT-SAGA and
PMGT-LSVRG with PG-EXTRA [23] and NIDS [16] because
the contributions of [24] mainly focus on the theoretical
analysis and the authors show that NIDS achieves a faster
convergence rate as compared to the algorithm proposed
in [25]. Moreover, the empirical results reported in [25]

−

7n

10−

5n, 10−

6n, 10−

also demonstrate the superiority of NIDS. We will set
σi ∈ {
for all agents to control the
}
condition number of f (x). The y axis is the suboptimality
h(¯xt)
h(x∗) and h(x∗) is approximated by the minimal
loss over all iterations and all algorithms. The left plot and
middle plot show the suboptimality with respect to the
number of component gradient evaluations and the number
of communications, respectively. The right plot is taken with
fi,j is of cost 1 and the cost
respect to the cost (evaluating
of one round of decentralized communication is τ [33]). All
parameters are well-tuned and mini-batch methods are used
to accelerate convergence.

∇

Figure 2 reports the results for σi = n10−

5 when

̄
̄
̄
̄
̄
̄
̄
 
̄
̄
̄
 
̄
 
̄
̄
)

⋆
x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

⋆
̄

⋆
x
(
h
−

̄

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

̄
)

⋆
x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−1

10−4

10−7

10−10

0
#Grad Evaluations(x1e4)

20

10

⋆
̄

⋆
x
(
h
−

̄

t

10−1

10−4

10−7

10−10

x
(
h
[
0
1
g
o

l

11

10−1

10−4

10−7

̄
)

⋆
x
(
h
−

)

t

x
(
h
[
0
1
g
o

l

10−10

10

0
40
20
#Grad Evaluations(x1e4)

30

]
̄

⋆
x
(
h
−

̄

t

0
#Grad Evaluations(x1e4)

100

50

10−1

10−4

10−7

K=1
K=2
K=⋆
K=10

x
(
h
[
0
1
g
o

l

10−10

0

500 1000 1500 2000

#Communications

0

2000 4000 6000 8000
#Communications

0

20000

40000
#Communications

Fig. 4. Comparisons under different consensus steps K with 1 − λ2(W ) = 0.05 for PMGT-LSVRG. From left to right, σ = 10−5n, 10−6n, 10−7n,
respectively.

−

−

λ2(W ) = 0.81 and 1

λ2(W ) = 0.05, respectively.
1
−
Because NIDS and PG-EXTRA perform similarly, we only
report the results of PG-EXTRA. We see that PMGT-SAGA
and PMGT-LSVRG are of a higher communication complex-
ity but require much less component gradient evaluations
which illustrates the beneﬁts of stochastic algorithms over
algorithms based on full gradients. The ﬁgures also show
that PG-EXTRA converges much slower when 1
λ2(W ) is
small while PMGT-SAGA and PMGT-LSVRG are rather robust
due to the extra rounds of decentralized communications.
Therefore, multi-consensus offers more advantages when
the network is poorly connected.

Figure 3 reports the results for σi = n10−
λ2(W ) = 0.81 and 1

7 when
λ2(W ) = 0.05, respectively.
1
−
In this ill-conditioned case, we see that PMGT-SAGA and
PMGT-LSVRG further outperform NIDS and PG-EXTRA with
respect to the number of component gradient evaluations.
Furthermore, the total rounds of decentralized communi-
cations of PMGT-VR algorithms are roughly the same as
that of PG-EXTRA. NIDS is faster than PG-EXTRA since we
observe that NIDS can converge with a larger stepsize. In
this setting, the condition number κ is the dominant factor
and the impact of network is relatively small.

−

≈

We also see that

the costs of PMGT-SAGA and
PMGT-LSVRG are much lower than those of PG-EXTRA and
NIDS regardless of the experiment setting if we choose
τ = 250 and they perform similarly when τ is large enough.
For instance, PMGT-SAGA and PG-EXTRA perform similarly
for τ
500 in
1300 in the ﬁrst row of Fig 2 and for τ
the second row; PMGT-SAGA and NIDS perform similarly
7n.
for τ
We observe that decentralized optimization algorithms are
faced with a computation-communication tradeoff. τ can
be considered as the relative ratio of cost of communica-
tion and cost of gradient evaluation. When τ is not too
big (decentralized communication is not too expensive),
PMGT-VR methods are preferable as compared to PG-EXTRA

1400 in the two experiments with σ = 10−

≈

≈

and NIDS.

−
6n, 10−

Finally, we compare our algorithms with different num-
ber of per-iteration decentralized communications K, re-
ported in Figure 4. Because the behaviors of PMGT-SAGA
and PMGT-LSVRG are similar with different K, we only
report the results of PMGT-LSVRG. We consider a net-
λ2(W ) = 0.05 and set σi = σ =
work with 1
7n for three experiments respectively. We
5n, 10−
10−
see that when K starts to increase, PMGT-LSVRG converges
faster with respect to the number of component gradient
evaluations and the communication complexity is relatively
stable in the ﬁrst two columns because the faster conver-
gence rate compensates for the extra communication cost.
We also see that after a certain threshold, a larger K cannot
further improve the convergence rate with respect to the
number of component gradient evaluations and the total
rounds of decentralized communications increase signiﬁ-
cantly. For the ill-conditioned case, we see again that the
condition number is the dominant factor and the impact of
multi-consensus is relatively small.

7 CONCLUSION

In this paper, we propose a novel algorithmic framework,
called PMGT-VR, which is based on a novel combination
of multi-consensus, gradient tracking and variance reduc-
tion techniques. Theoretically, we show that a decentral-
ized proximal variance reduction algorithm could admit a
similar convergence rate by imitating its centralized coun-
terpart. Speciﬁcally, we propose two representative algo-
rithms named PMGT-SAGA, PMGT-LSVRG. We also show
that PMGT-SAGA and PMGT-LSVRG can achieve much better
computational efﬁciency than existing decentralized proxi-
mal algorithms. To the best of our knowledge, PMGT-SAGA
and PMGT-LSVRG are the ﬁrst decentralized proximal vari-
ance reduction algorithms. Finally, our methodology can be
extended to other variance reduction techniques in a similar

̄
̄
̄
̄
̄
̄
fashion. Thus, our PMGT-VR framework can provide an
insight in developing novel decentralized proximal variance
reduction algorithms.

REFERENCES

[1] M. Li, D. Andersen, J. Park, A. Smola, A. Ahmed, V. Josifovski,
J. Long, E. Shekita, and B.-Y. Su, “Scaling distributed machine
learning with the parameter server,” Proc. OSDI, pp. 583–598, 01
2014.

[2] X. Lian, C. Zhang, H. Zhang, C. Hsieh, W. Zhang, and J. Liu, “Can
decentralized algorithms outperform centralized algorithms? A
case study for decentralized parallel stochastic gradient descent,”
in Advances in Neural Information Processing Systems, 2017, pp.
5330–5340.

[3] B.

team,

“Bluefog,”

2021.

[Online].

Available:

https://doi.org/10.5281/zenodo.4616052

[4] G. Qu and N. Li, “Harnessing smoothness to accelerate distributed
optimization,” IEEE Transactions on Control of Network Systems,
vol. 5, no. 3, pp. 1245–1260, 2017.
S. Pu and A. Nedic, “A distributed stochastic gradient tracking
method,” in 57th IEEE Conference on Decision and Control, CDC
2018, Miami, FL, USA, December 17-19, 2018.
IEEE, 2018, pp. 963–
968.

[5]

[6] R. Xin, U. A. Khan, and S. Kar, “Variance-reduced decentral-
ized stochastic optimization with accelerated convergence,” IEEE
Transactions on Signal Processing, vol. 68, pp. 6255–6271, 2020.
[7] B. Li, S. Cen, Y. Chen, and Y. Chi, “Communication-efﬁcient
distributed optimization in networks with gradient tracking and
variance reduction,” in The 23rd International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online
[Palermo, Sicily, Italy], ser. Proceedings of Machine Learning Re-
search, vol. 108. PMLR, 2020, pp. 1662–1672.

[8] H. Li, Z. Lin, and Y. Fang, “Optimal accelerated variance reduced
extra and diging for strongly convex and smooth decentralized
optimization,” arXiv preprint arXiv:2009.04373, 2020.
S. Alghunaim, K. Yuan, and A. H. Sayed, “A linearly convergent
proximal gradient algorithm for decentralized optimization,” in
Advances in Neural Information Processing Systems, 2019, pp. 2848–
2858.

[9]

[10] Y. Sun, A. Daneshmand, and G. Scutari, “Convergence rate of
distributed optimization algorithms based on gradient tracking,”
arXiv preprint arXiv:1905.02637, 2019.

[11] H. Ye, Z. Zhou, L. Luo, and T. Zhang, “Decentralized accelerated
proximal gradient descent,” Advances in Neural Information Process-
ing Systems, vol. 33, 2020.

[12] A. Nedic and A. Ozdaglar, “Distributed subgradient methods for
multi-agent optimization,” IEEE Transactions on Automatic Control,
vol. 54, no. 1, pp. 48–61, 2009.

[13] S. S. Ram, A. Nedi´c, and V. V. Veeravalli, “Distributed stochas-
tic subgradient projection algorithms for convex optimization,”
Journal of optimization theory and applications, vol. 147, no. 3, pp.
516–545, 2010.

[14] K. Yuan, Q. Ling, and W. Yin, “On the convergence of decentral-
ized gradient descent,” SIAM Journal on Optimization, vol. 26, no. 3,
pp. 1835–1854, 2016.

[15] W. Shi, Q. Ling, G. Wu, and W. Yin, “EXTRA: an exact ﬁrst-
order algorithm for decentralized consensus optimization,” SIAM
J. Optim., vol. 25, no. 2, pp. 944–966, 2015.

[16] Z. Li, W. Shi, and M. Yan, “A decentralized proximal-gradient
method with network independent step-sizes and separated con-
vergence rates,” IEEE Trans. Signal Process., vol. 67, no. 17, pp.
4494–4506, 2019.

[17] H. Li and Z. Lin, “Revisiting extra for smooth distributed opti-

mization,” arXiv preprint arXiv:2002.10110, 2020.

[18] H. Ye, L. Luo, Z. Zhou, and T. Zhang, “Multi-consensus decentral-

ized accelerated gradient descent,” CoRR, 05 2020.

[19] Q. Guannan and L. Na, “Accelerated distributed nesterov gradient
descent,” IEEE Transactions on Automatic Control, vol. 65, no. 6, pp.
2566–2581, 2020.

[20] K. Yuan, B. Ying, X. Zhao, and A. H. Sayed, “Exact diffusion for
distributed optimization and learning—part i: Algorithm devel-
opment,” IEEE Transactions on Signal Processing, vol. 67, no. 3, pp.
708–723, 2018.

12

[21] N. S. Aybat, Z. Wang, T. Lin, and S. Ma, “Distributed linearized
alternating direction method of multipliers for composite convex
consensus optimization,” IEEE Transactions on Automatic Control,
vol. 63, no. 1, pp. 5–20, 2017.

[22] M. Hong, D. Hajinezhad, and M.-M. Zhao, “Prox-pda: The prox-
imal primal-dual algorithm for fast distributed nonconvex opti-
mization and learning over networks,” in Proceedings of the 34th
International Conference on Machine Learning-Volume 70.
JMLR.
org, 2017, pp. 1529–1538.

[23] W. Shi, Q. Ling, G. Wu, and W. Yin, “A proximal gradient al-
gorithm for decentralized composite optimization,” IEEE Trans.
Signal Process., vol. 63, no. 22, pp. 6013–6023, 2015.

[24] J. Xu, Y. Tian, Y. Sun, and G. Scutari, “Distributed algorithms
for composite optimization: Uniﬁed and tight convergence
analysis,” CoRR, vol. abs/2002.11534, 2020. [Online]. Available:
https://arxiv.org/abs/2002.11534

[25] S. A. Alghunaim, E. Ryu, K. Yuan, and A. H. Sayed, “Decentralized
proximal gradient algorithms with linear convergence rates,” IEEE
Transactions on Automatic Control, 2020.

[26] K. Scaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massouli´e,
“Optimal algorithms for smooth and strongly convex distributed
optimization in networks,” in Proceedings of the 34th International
Conference on Machine Learning-Volume 70.
JMLR. org, 2017, pp.
3027–3036.

[27] A. Defazio, F. R. Bach, and S. Lacoste-Julien, “SAGA: A fast in-
cremental gradient method with support for non-strongly convex
composite objectives,” in Advances in Neural Information Processing
Systems, 2014, pp. 1646–1654.

[28] R. Johnson and T. Zhang, “Accelerating stochastic gradient de-
scent using predictive variance reduction,” Advances in neural
information processing systems, vol. 26, pp. 315–323, 2013.

[29] M. Schmidt, N. Le Roux, and F. Bach, “Minimizing ﬁnite sums
with the stochastic average gradient,” Mathematical Programming,
vol. 162, no. 1-2, pp. 83–112, 2017.

[30] A. Mokhtari and A. Ribeiro, “Dsa: Decentralized double stochastic
averaging gradient algorithm,” The Journal of Machine Learning
Research, vol. 17, no. 1, pp. 2165–2199, 2016.

[31] Z. Shen, A. Mokhtari, T. Zhou, P. Zhao, and H. Qian, “Towards
more efﬁcient stochastic decentralized learning: Faster conver-
gence and sparse communication,” in International Conference on
Machine Learning, 2018, pp. 4624–4633.

[32] H. Hendrikx, F. Bach, and L. Massouli´e, “An accelerated decentral-
ized stochastic proximal algorithm for ﬁnite sums,” in Advances in
Neural Information Processing Systems, 2019, pp. 954–964.

[33] ——, “Dual-free stochastic decentralized optimization with vari-
ance reduction,” Advances in Neural Information Processing Systems,
vol. 33, 2020.

[34] E. A. Gorbunov, F. Hanzely, and P. Richt´arik, “A uniﬁed theory of
SGD: variance reduction, sampling, quantization and coordinate
descent,” in The 23rd International Conference on Artiﬁcial Intelligence
and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo,
Sicily, Italy], ser. Proceedings of Machine Learning Research, vol.
108. PMLR, 2020, pp. 680–690.

[35] D. Kovalev, S. Horv´ath, and P. Richt´arik, “Don’t jump through
hoops and remove those loops: Svrg and katyusha are better
without the outer loop,” in Algorithmic Learning Theory, ALT 2020,
8-11 February 2020, San Diego, CA, USA, vol. 117.
PMLR, 2019,
pp. 451–467.

[36] L. M. Nguyen, J. Liu, K. Scheinberg, and M. Tak´ac, “SARAH: A
novel method for machine learning problems using stochastic re-
cursive gradient,” in Proceedings of the 34th International Conference
on Machine Learning, ICML, vol. 70. PMLR, 2017, pp. 2613–2621.

[37] M. Zhu and S. Mart´ınez, “Discrete-time dynamic average consen-

sus,” Autom., vol. 46, no. 2, pp. 322–329, 2010.

[38] K. Yuan, S. A. Alghunaim, B. Ying, and A. H. Sayed, “On the
performance of exact diffusion over adaptive networks,” in 58th
IEEE Conference on Decision and Control, CDC 2019, Nice, France,
December 11-13, 2019.

IEEE, 2019, pp. 4898–4903.

[39] J. Liu and A. Morse, “Accelerated linear iterations for distributed
averaging,” Annual Reviews in Control, vol. 35, pp. 160–165, 12
2011.

13

2

2

Therefore, we have the following equation
ηm,R(x) = proxη,r(xi).

prox

(i)

Using the above result, we expand the sum

11⊤proxηm,R(x)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

m

proxη,r(xi)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
proxη,r(xi)
(cid:13)

−

1
m

Xi=1
1⊤x)

−

1
m

1
m

1⊤x)

−

(cid:19)(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

proxη,r(xi)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

proxηm,R(

11⊤x)

1
m

−

1
m

1
m

(cid:13)
(cid:13)
(cid:13)
(23)
(cid:13)
= m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
·
m

=m

m

≤

≤

=

Xi (cid:13)
(cid:13)
(cid:13)
x
(cid:13)
−

(cid:13)
(cid:13)
the
(cid:13)
(cid:13)

proxη,r(

1⊤x)

m

proxη,r(

Xi=1 (cid:18)
m

proxη,r(

1
m

1
m

2

xi

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
,

Xi=1 (cid:13)
(cid:13)
(cid:13)
1
1⊤x
(cid:13)
m

−

1
m
last

11⊤x
(cid:13)
(cid:13)
(cid:13)
(cid:13)

where
expansiveness of proximal operator.

inequality is because of

the non-

We are ready to prove the iterative inequality about

consensus errors.

Proof of Lemma 4. For simplicity, we denote FastMix(
·
operation as T(
·
T(x)

). From Lemma 2 we can know that

x

ρ

.

, K)

(42)

1
m

−

≤

11⊤x
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

−

11⊤x
(cid:13)
(cid:13)
(cid:13)
(cid:13)

First, we have

xt+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1¯xt+1
−
(cid:13)
proxηm,R(xt
(cid:13)
−
(cid:13)
(cid:13)
(cid:13)
proxηm,R(xt
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ ρ
(cid:13)

proxηm,R

−

(cid:13)
ρ
(cid:13)

ρ

≤

≤

≤

11⊤
m

proxηm,R(xt

ηst)

ηst)

−

−

proxηm,R

1(¯xt

η¯st)

−

11⊤
(cid:0)
m

−

1(¯xt

−

ηst)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
−
(cid:1)(cid:13)
(cid:13)
proxηm,R(xt
(cid:13)

η¯st)

(cid:13)
(cid:13)
1¯xt
xt
(cid:13)
(cid:13)
−
(cid:13)
xt

ρ
+ ρ
(cid:13)
(cid:13)
2ρ

xt
(cid:13)
(cid:0)
(cid:13)

(cid:0)
+ ρη
ηst
(cid:13)
−
(cid:13)
1¯xt

st
1
(cid:13)
−
(cid:13)
+ 2ρη
(cid:1)

(cid:0)

−
¯xt
st

(cid:1)

1¯st

η¯st
(cid:13)
(cid:13)
1¯st

−
−

−

≤
where the ﬁrst inequality is due to the update rule and
Eqn. (42), and the third inequality is because of Lemma 3
and the non-expansiveness of proximal operator.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

,
(cid:1)(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

ηst)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Therefore, we can obtain that

2

1¯xt+1

−

xt+1

1
m
8ρ2 1
(cid:13)
(cid:13)
m
≤
Furthermore, we have
1¯st+1
st+1

(cid:13)
(cid:13)
2

xt

−

1¯xt

(cid:13)
2
(cid:13)

+ 8ρ2 η2
m

1¯st

2

.

st

−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

APPENDIX A
PROOFS OF LEMMA 1, 2, 3, AND 4

Algorithm 4 FastMix

1: Input: x0 = x−1, K, W , stepsize ηw =
2: for k = 0, . . . , K do
3:
4: end for
5: Output: xK .

xk+1 = (1 + ηw)W xk

ηwxk−1;

−

1−√1−λ2
1+√1−λ2

2(W )
2(W )

.

In this section, we ﬁrst provide the proof of lemmas 1
and 3 that are related to gradient tracking and proximal
mapping. The proof of lemma 2 can be found in [39]. Then,
we invoke these results to give a proof of Lemma 4 that is
related to the iterative inequality about consensus errors.

Proof of Lemma 1. By the update rule and property of
FastMix:

st = FastMix

1 + vt
1⊤x = 1⊤FastMix(x, K),

st

−

(cid:0)

vt

−
x

∀

∈

1, K
−
Rm

×

,
d,
(cid:1)

we know that

¯st = ¯st

−

1 + ¯vt

¯vt

−

1.

−

Since ¯st = ¯vt holds for t = 0, by induction we can show
i) holds because vt
fi(xt
that ¯st = ¯vt holds. E[¯st] = 1
i
m
i) with PMGT-SAGA and
is an unbiased estimator of
PMGT-LSVRG. We then have

m
i ∇
fi(xt
∇
P

2

(cid:13)
(cid:13)
(cid:1)
(cid:13)
(cid:13)
2
(cid:13)

f (¯xt)
m

E[¯st]

2

−

(cid:13)
fi(xt
(cid:13)
i)

∇
1
m

∇
(cid:0)

Xi=1
m

fi(¯xt)

− ∇

Xi=1
m

(cid:13)
(cid:13)

fi(xt
i)

∇

fi(¯xt)

− ∇

2

xt
i −

¯xt

(cid:13)
(cid:13)

Xi=1
xt

(cid:13)
(cid:13)
.

2

1¯xt

(cid:13)
(cid:13)
−

=

≤

≤

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
m

L2
m
L2
m

(cid:13)
(cid:13)

(cid:13)
(cid:13)
Proof of Lemma 3. We recall that zi, xi are the i-th row of the
matrices. By the deﬁnition of the proximal operators, we
have

proxηm,R(x)

= argmin

z

∈

Rm×d (cid:18)

= argmin

Rm×d  

z

∈

= argmin

Rm×d  

z

∈

argminz

= 






argminz

R(z) +

1
z
2ηm k

1
m

m

m

r(zi) +

Xi=1
r(zi) +

m

Xi=1
Rd

∈

Xi=1
r(z) + 1
...
(cid:16)
r(z) + 1

Rd

∈

(cid:16)

2

x
k

(cid:19)

−
m

1
2ηm k

zi −

2

xik

!

Xi=1
1
2η k

z
2η k

zi −
x1k

−

xik

2

!

⊤

(cid:17)

z
2η k

xmk

−

⊤

(cid:17)

.









T

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2ρ2
(cid:13)

=

≤

2ρ2

≤

−

st + vt+1

vt

(cid:13)
(cid:13)
−

1
m

−

11⊤

st + vt+1

st

st

(cid:0)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

−

2

(cid:1)
+ 2ρ2

1¯st

vt+1

(cid:0)
vt

−

1¯st

2

(cid:13)
(cid:13)

+ 2ρ2

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

vt+1

vt

−

1
m

,

−
2

(cid:13)
(cid:13)

2

vt

−

(cid:13)
(cid:13)
(cid:1)
(cid:13)
11⊤(vt+1
(cid:13)

−

2

vt)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

where the last

x

−

1
m

11⊤x

inequality is because it holds
x
Rm
k

for any x

d.

∈

×

≤ k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
APPENDIX B
PROOF OF LEMMA 5 AND 6

In this section, we derive an upper bound for
and substitute it into Lemma 4 to obtain the desired linear
(cid:13)
(cid:13)
system inequality. Before continuing, we ﬁrst prove two
auxiliary results.
Lemma 10. For PMGT-VR methods, it holds that

(cid:13)
(cid:13)

−

vt+1

2

vt

that

For PMGT-LSVRG, it holds that

m

E

1
m

Xi=1

h(cid:13)
m,n
(cid:13)

2
mn

≤

vt
i − ∇

2

fi(x∗)

(

fi,j(xt
i)

∇

i

(cid:13)
(cid:13)

− ∇

fi,j(x∗)

Xi=1,j=1

(cid:13)
(cid:13)
fi,j(x∗)
− ∇

∇

+

fi,j(wt
i)

2

) .

(cid:13)
(cid:13)

Proof. For PMGT-SAGA, we have

(cid:13)
(cid:13)

(cid:13)
(cid:13)

14

2

(46)

and

1
mn

4L

≤

·

1
mn

2L2

≤

Proof. We have

m,n

Xi=1,j=1
(cid:13)
(cid:13)
Df (¯xt, x∗) +

fi,j(xt
i)

∇

fi,j(x∗)

− ∇

2L2
m

xt

−

1¯xt

(cid:13)
(cid:13)

(cid:13)
(cid:13)
2
,

(cid:13)
(cid:13)

m,n

Xi=1,j=1
¯xt

(cid:13)
(cid:13)
x∗

−

fi,j(xt
i)

∇

fi,j(x∗)

− ∇

2

+

2L2
m

xt

−

1¯xt

(cid:13)
(cid:13)
2
.

2

2

(cid:13)
(cid:13)
m,n

Xi=1,j=1
m,n

(cid:13)
(cid:13)
(

Xi=1,j=1

(cid:13)
(cid:13)

(cid:13)
(cid:13)

fi,j(xt
i)

∇

fi,j(x∗)

− ∇

(cid:13)
(cid:13)

2

fi,j(xt
i)

∇

(cid:13)
(cid:13)
fi,j(¯xt)

− ∇

2

fi,j(x∗)

2

)

∇

(cid:13)
(cid:13)
fi,j(¯xt)
− ∇
m,n
fi,j(¯xt)

∇

(cid:13)
fi,j(x∗)
(cid:13)
− ∇

(cid:13)
(cid:13)

2

1
mn

2
mn

+

2
(cid:13)
(cid:13)
mn

≤

≤

Xi=1,j=1
2L2
xt
m

+

m,n
(cid:13)
(cid:13)

(3)

≤

4L
mn

(cid:13)
(cid:13)

−

2

1¯xt

(cid:13)
(cid:13)
Dfi,j (¯xt, x∗) +

(cid:13)
(cid:13)

2L2
m

2

1¯xt

xt

−

Xi=1,j=1
Df (¯xt, x∗) +

·

=4L

2L2
m
where the second inequality is because of L-smoothness of
fi,j. If we use L-smoothness instead of Eqn. (3) in the last
inequality, we can obtain that

xt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

2

,

(cid:13)
(cid:13)
1¯xt

(cid:13)
(cid:13)

1
mn

2L2

≤

m,n

Xi=1,j=1
¯xt

(cid:13)
(cid:13)
x∗

−

fi,j(xt
i)

∇

fi,j(x∗)

− ∇

2

2

+

2L2
m

xt

−

1¯xt

(cid:13)
(cid:13)
2
.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Lemma 11. For PMGT-SAGA, it holds that

vt
i − ∇

fi(x∗)

2

fi,ji (xt
i)

fi,ji (x∗)

(cid:13)
(cid:13)

− ∇
fi,ji (φt

i,ji )

− ∇
fi,j(φt

i,j )

fi(x∗)
k

− ∇

2

(43)

(44)

=

1
m

1
m

+

+

2
m

≤

+

m

E

Xi=1
m

(cid:13)
(cid:13)

Eji k∇
fi,ji (x∗)

n

Xi=1
∇
1
n

m

Xj=1 ∇
Eji

Xi=1
2
m

m

h(cid:13)
(cid:13)
Eji

fi,ji (xt
i)

∇

fi,ji (x∗)

− ∇

2

i

(cid:13)
(cid:13)
fi,ji (φt
i,ji )

fi,ji (x∗)

− ∇

∇

(cid:20)(cid:13)
(cid:13)
(cid:13)
fi,ji (x∗)

fi,ji (φt

i,ji )

− ∇

2

(cid:3)(cid:13)
(cid:13)
(cid:13)
fi,ji (x∗)

(cid:21)
2

fi,ji (xt
i)

∇

− ∇

i

(cid:13)
(cid:13)
fi,ji (φt
i,ji )

2

i

(cid:13)
(cid:13)

2

fi,ji (x∗)

− ∇

∇

h(cid:13)
(cid:13)
fi,ji (xt
i)

∇

fi,ji (x∗)

− ∇

fi,ji (φt

− ∇
fi,j(xt
i)

2

i,ji )
(cid:13)
(cid:13)
fi,j(x∗)

i

(cid:13)
(cid:13)

2

− ∇

fi,j(φt

i,j )

− ∇

(cid:13)
(cid:13)

2

,

Xi=1

Eji

∇

m

(cid:2)
Eji

−
2
m

≤

≤

+

2
m

+

Xi=1
2
m
m

m

h(cid:13)
(cid:13)
Eji

Xi=1
Eji

Xi=1

∇

h (cid:13)
(cid:13)
fi,ji (x∗)
m,n

=

(cid:13)
2
(cid:13)
mn

+

∇

Xi=1,j=1

(cid:13)
(cid:13)
fi,j(x∗)

∇

(cid:13)
(cid:13)

where the second inequality is because of E[
E[
concludes the proof.

2]. For PMGT-LSVRG, we replace φt

a
k

k

E[a]
a
−
i,j with wt

2]
k
≤
i. This

k

(cid:13)
(cid:13)

We are ready to derive an upper bound for

vt+1

vt

2

.

−

Proof of Lemma 5. We ﬁrst decompose

(cid:13)
(cid:13)

(cid:13)
(cid:13)
vt

−

2

:

(cid:13)
(cid:13)

vt+1

(cid:13)
(cid:13)

m

E

1
m

Xi=1

h(cid:13)
m,n
(cid:13)

vt
i − ∇

2

fi(x∗)

(

fi,j(xt
i)

∇

i

(cid:13)
(cid:13)

− ∇

fi,j(x∗)

Xi=1,j=1

(cid:13)
(cid:13)
fi,j(x∗)
− ∇

∇

fi,j(φt

i,j )

2

) ,

(cid:13)
(cid:13)

2
mn

≤

+

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2

(45)

=

≤

2

vt

−
(cid:13)
vt+1
(cid:13)
i − ∇

vt+1
m
(cid:13)
(cid:13)
Xi=1
m
2

(cid:13)
(cid:13)

fi(x∗)

(vt

−

i − ∇

fi(x∗))

2

vt+1
i − ∇

fi(x∗)

2

+

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Xi=1 (cid:16)(cid:13)
(cid:13)

(cid:13)
(cid:13)
fi(x∗)

vt
i − ∇

2

.

(cid:17)

(cid:13)
(cid:13)

For PMGT-LSVRG, we replace φt
i and note that
the deﬁnitions of gradient learning quantity ∆t for two
algorithms are different. This concludes the proof.

i,j with wt

15

2

Proof of Lemma 8. For PMGT-SAGA, it holds that

Then, for PMGT-SAGA, we have

1
m
m

E

(cid:20)
2
m

2

vt

vt+1

−

(cid:21)

(cid:13)
(cid:13)

vt+1
i − ∇

(cid:13)
(cid:13)
E

Xi=1

h(cid:13)
m,n
(cid:13)

4
mn

+

+

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
8L2
(cid:13)

≤

(45)

≤

(44)

≤

(25)

∇

Xi=1,j=1 (cid:16)(cid:13)
(cid:13)
fi,j(x∗)
fi,j(xt
i)
fi,j(x∗)

∇

∇
¯xt+1

x∗

−

− ∇

− ∇

− ∇
2

(cid:13)
+ 8L2
(cid:13)

(cid:13)
8ρ2 + 1
(cid:13)

(cid:13)
(cid:13)
x∗

2

(cid:13)
xt
(cid:13)

¯xt

−
8L2
m
(cid:1)
¯xt+1

fi(x∗)

2

+

vt
i − ∇

fi(x∗)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
fi,j(x∗)

2

fi,j(xt+1

)

i

∇

− ∇

2

(cid:13)
(cid:13)

fi,j(φt+1
i,j )
(cid:13)
2
(cid:13)
(cid:13)
2

i,j)
(cid:13)
(cid:13)

+

fi,j(x∗)
fi,j(φt
8L2
m
8L2
m
1¯xt

+

(cid:13)
(cid:13)

xt

(cid:17)
(cid:13)
xt+1
(cid:13)

1¯xt+1

−
1¯xt

2

(cid:13)
+ 4∆t
(cid:13)

(cid:13)
2
(cid:13)

−
64ρ2η2L2
(cid:13)
(cid:13)
m
+ 4∆t+1 + 8L2

+

i

(cid:13)
(cid:13)

2

+ 4∆t+1

st

2

1¯st

≤

(cid:13)
(cid:13)

−
2

(cid:0)
+ 8L2

(cid:13)
x∗
(cid:13)
−

−
(cid:13)
(cid:13)
¯xt
+ 4∆t,
x∗
(cid:13)
(cid:13)
−
i,j with wt
i,j , φt+1
Similarly, for PMGT-LSVRG, we replace φt
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i
and wt+1
, respectively in the above proof and note that the
deﬁnitions of gradient learning quantity ∆t are different for
two algorithms. This concludes the proof.

(cid:13)
(cid:13)

2

i

Proof of Lemma 6. For both PMGT-SAGA and PMGT-LSVRG,
vt+1
from Lemma 5, we
using the upper bound of
have

vt

−

η2
m

E

(cid:20)
2ρ2

st+1

−

1¯st+1

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)
64ρ2η2L2 + 1

≤

·

(cid:18)
(cid:0)
+ (8ρ2 + 1)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:21)
η2
m

2

1¯st

st

−

8L2η2
m

(cid:1)
xt

(cid:13)
(cid:13)
1¯xt

−
¯xt

2

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
+4η2∆t+1 + 8L2η2
(cid:13)

x∗

+ 4η2∆t

−
Therefore, we obtain the linear system inequality as desired.

.

(cid:13)
(cid:13)
(cid:17)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
+ 8L2η2

¯xt+1

−

2

x∗

APPENDIX C
PROOFS OF LEMMAS 7, 8, AND 9
Proof of Lemma 7. For PMGT-SAGA, it holds that

E

h

¯st − ∇
k
1
m

m



=E

(cid:13)
(cid:13)
(cid:13)
m
(cid:13)
(cid:13)
Xi=1

Xi
E

h(cid:13)
m,n
(cid:13)

2

f (x∗)
k

vt
i − ∇

(cid:0)
vt
i − ∇

i
fi(x∗)

2





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:1)
2

fi(x∗)

fi,j(xt
i)

∇

i

(cid:13)
(cid:13)

− ∇

fi,j(x∗)

2

Xi=1,j=1 (cid:16)(cid:13)
(cid:13)
fi,j(x∗)

∇

fi,j(φt

− ∇

(cid:13)
Df (¯xt, x∗) + 2∆t +
(cid:13)

i,j )
4L2
m

2

(cid:17)
xt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1¯xt

2

.

−

(cid:13)
(cid:13)


1
m

2
mn

≤

(45)

≤

+

(43)

≤

8L

·

(cid:13)
(cid:13)

2

E

∆t+1

m,n
(cid:3)

E

Xi=1,j=1
m,n

(cid:20)(cid:13)
(cid:13)
1
(cid:13)
n

Xi=1,j=1 (cid:18)
1
n
−
n
1
n

(cid:13)
(cid:13)
∆t +

∇

−

(cid:19)

=

1
(cid:2)
mn

=

1
mn

+

=

1
(cid:18)

(43)

≤

fi,j(φt+1
i,j )

∇

− ∇

2

(cid:21)

fi,j(x∗)
(cid:13)
(cid:13)
(cid:13)
2

fi,j(x∗)

fi,j(xt
i)

∇

− ∇

(cid:13)
(cid:13)
fi,j (φt

i,j)

(cid:13)
(cid:13)

2

fi,j(x∗)

− ∇
m,n

(cid:19)
(cid:13)
(cid:13)
fi,j(xt
i)

1
mn2

4L
n

∇

Xi=1,j=1
Df (¯xt, x∗) +

(cid:13)
(cid:13)

1
n

∆t +

−

1
(cid:18)
For PMGT-LSVRG, it holds that

(cid:19)

2L2
mn

1¯xt

2

.

xt

−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

fi,j(x∗)

− ∇

2

m,n
(cid:3)

E

Xi=1,j=1
p)∆t +

h(cid:13)
(cid:13)
p
mn

E

∆t+1

=

1
(cid:2)
mn

=(1

−

(43)

≤

(1

−

p) ∆t + 4LpDf

fi,j

∇

wt+1
i

fi,j (x∗)

− ∇

(cid:0)
m,n

(cid:1)

2

i

(cid:13)
(cid:13)

E

fi,j

∇

xt
i

− ∇

fi,j (x∗)

Xi=1,j=1

h(cid:13)
(cid:13)
¯xt, x∗

(cid:1)

xt

(cid:0)
2L2p
m

−

+

1¯xt

2

.

i

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:0)

(cid:1)
n , we conclude the proof.

By replacing p with 1
Proof of Lemma 9. Let all expectations be conditioned on xt
in this proof. We ﬁrst note that the induction inequalities
of two algorithms are of the same form. Therefore, the
following proof works for both algorithms. By deﬁnitions
of st and ¯st, we have

(cid:13)
(cid:13)

2

−

x∗

¯xt+1
1
(cid:13)
(cid:13)
m
(cid:13)
(cid:13)
+proxη,r(¯xt
(cid:13)
(cid:13)

1⊤proxmη,R(xt
(cid:13)
(cid:13)

ηst)

−

proxη,r(¯xt

η¯st)

−

η¯st)

−

proxη,r(x∗

η

f (x∗))

2

1⊤proxmη,R(xt

ηst)

−

−
∇
proxη,r(¯xt

−

−

−

=

=

1
m

(cid:13)
(cid:13)
(cid:13)
+
(cid:13)

proxη,r(¯xt

η¯st)

−
η¯st)

−
(cid:13)
proxη,r(¯xt
+ 2
(cid:13)
(cid:13)
D
1
1⊤proxmη,R(xt
m

−

−

proxη,r(x∗

−
proxη,r(x∗

η

∇
η

−
ηst)

−

∇
proxη,r(¯xt

(24)

≤

¯xt

x∗

−

−

η(¯st

f (x∗))

− ∇

−

2

−

2

2

(cid:13)
(cid:13)
η¯st)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
f (x∗))
(cid:13)
(cid:13)
f (x∗)),
(cid:13)
(cid:13)
η¯st)

−

(cid:29)

(47)

(cid:13)
(cid:13)
|
+

+

2
m
2
(cid:13)
(cid:13)
√m

xt

−
¯xt

T 2

1¯xt
{z

2

+

(cid:13)
(cid:13)
st

}
−

2η2
m k

1¯st

2

k

(cid:13)
x∗
(cid:13)

−

−

η(¯st

f (x∗))

− ∇

(cid:13)
(cid:13)
1¯xt
|
−

xt

(

·

+ η

T

{z
−

st

k

1¯st

).

k

(cid:13)
(cid:13)
}

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Next, we will bound the variance of T :

(47)

E

T 2 + 4nη2∆t+1

≤

x∗, ¯st

f (x∗)

− ∇

16

+

k

k

xt

1¯xt

)

k

−

E [T 2]

st

(cid:3)
(η

·

k

1¯st

1¯xt

2

+

−

(cid:13)
(cid:13)
, 1

1
12κ

−

−

−
2η2
m k
1
2n

·

(cid:19)

1¯st

2

st

−

V t +

k
√V t
6√m

xt

−

1¯xt

E [T 2 + 4nη2∆t+1]

st

(η

k

·

−

(cid:13)
1¯st
(cid:13)

+

k

k

(cid:13)
xt
(cid:13)

1¯xt

)

k

−

(49)

≤

≤

≤

≤

≤

(cid:2)
+

+

2
√m
2
m

q
xt

max

(cid:13)
(cid:13)
1
(cid:18)
2
√m
25
12m

max

+

+

+

(

·
+

−

25
(cid:13)
(cid:13)
12m

max

(cid:13)
(cid:13)
−

1
(cid:18)
3√V t
√m
25
12m
2η
√6m k

+

+ (

+

max

+

+ (

+ (

max

1
(cid:18)
3√V t
√m
25
12m
2η2
m

1
(cid:18)
3√V t
√m
3.3
m
(cid:13)
third
(cid:13)
xt

+

−

xt

+

+

q

xt

(cid:13)
(cid:13)
−

−
1
12κ
13
12
1¯xt

1
(cid:18)
2
√m r
xt

1¯xt

, 1

V t +

2

+

2η2
m k

st

−

(cid:13)
1
(cid:13)
2n
−
1
6m k
st

·

(cid:19)
xt

V t +

1¯xt

2
k

2

1¯st

k
√V t
6√m

+ η

k
1¯xt

−
+

2

, 1

(cid:13)
(cid:13)
−

1
2n

xt

(cid:13)
(cid:13)
−
1
12κ

)

k

−
1¯st
2η2
m k
V t

·

(cid:19)

st

1¯st

2

k

−

xt

−

1¯xt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

st

1¯st

)

−

(cid:13)
(cid:13)
xt

(cid:13)
(cid:13)
1¯xt
−

2

+

(cid:13)
2η2
(cid:13)
m k

st

1¯st

2

k

−

st

(cid:13)
(cid:13)
kk

(cid:13)
1¯st
(cid:13)
k

−

xt

(

1¯xt

−
2
√6m
1¯xt

)

(cid:13)
(cid:13)
+

xt

−
1
12κ

−

, 1

−

1
2n

V t

·

(cid:19)

xt

(

1¯xt

+

st

1¯st

)

−

(cid:13)
(cid:13)
+

−
2
√6m
η2
2m
1
12κ

)
k

, 1

+

(cid:13)
(cid:13)
)

(cid:13)
1
(cid:13)
3m

xt

−

(cid:13)
1¯xt
(cid:13)

2

st

1¯st

−

(cid:13)
(cid:13)
2
k

(cid:13)
(cid:13)

1
2n

−

V t

·

(cid:19)

xt

(

−

1¯xt

+ η

st

1¯st

)

−

2

st

(cid:13)
(cid:13)
+

1¯xt

(cid:13)
(cid:13)
−
(cid:13)
inequality
(cid:13)
1

(cid:13)
2.5η2
(cid:13)
m k
is
12 V t + 1

(cid:13)
(cid:13)
1¯st

2.

k

−

1¯xt

The
√V t
6√m k
inequality is because we apply √a2 + b2
1¯xt

because Eqn.
2
1¯xt
k
≤ |

to
2. The ﬁfth inequality is because
k

and
. The forth
+

12 V t + 1

12m k

k ≤

(49)

xt

xt

−

−

−

13

a

b

|

|

|

6m k
a2 + b2.

2ab
q

≤

(cid:11)

i

x∗

x∗

−

(cid:11)

2

2η

−
f (x∗)

¯xt

2
(cid:10)

−

−

2η

i
E[¯st]
(cid:13)
(cid:13)
h
f (x∗)

f (x∗), ¯xt −

− ∇
2

f (x∗), ¯xt
x∗

i
f (¯xt)
(cid:13)
(cid:13)
− ∇
∇
E[¯st]
¯xt
(cid:10)
−
f (x∗)

(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i
(cid:13)
2ηDf (¯xt, x∗)
(cid:13)
−
E[¯st]
(cid:13)
(cid:13)
f (x∗)

¯xt

x∗

−

2

x∗

−

(cid:13)
(cid:13)

x∗

2

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i
(cid:13)
+ 2η(4Lη
(cid:13)
E[¯st]
f (¯xt)
(cid:13)
∇
(cid:13)
1¯xt

−

2

=E

E

T 2
¯xt
(cid:3)

(cid:2)
h(cid:13)
+η2
(cid:13)
¯xt
(cid:13)
(cid:13)
−
+ η2E
(cid:13)
(cid:13)

¯xt
−
+ 2η
(cid:13)
(cid:13)
+ η2E
(cid:13)
(cid:13)

x∗

(cid:13)
(cid:13)
− ∇
2

−
¯st

x∗

− ∇
2η

¯st
(cid:13)
(cid:13)
h(cid:13)
2
x∗
(cid:13)
−
f (¯xt)
(cid:13)
(cid:13)
¯st

∇

−

− ∇

h(cid:13)
¯xt
ηµ)
(cid:13)
−
f (¯xt)
(cid:13)
(cid:13)
¯st

∇

(1
−
+ 2η
+ η2E
(cid:13)
(cid:13)

−

¯xt

h(cid:13)
ηµ)
(1
(cid:13)
−
+ 2η2∆t + 2η
(cid:13)
(cid:13)
4L2η2
m

xt

+

− ∇

(cid:13)
(cid:13)
−

=

≤

(3)

≤

(28)

≤

(21)

≤

(1

−

ηµ)

(cid:13)
¯xt
(cid:13)

(cid:13)
+ 2η2∆t +
(cid:13)
4L2η2
m

+

xt

2

(cid:13)
(cid:13)
+ 2η(4Lη

1¯xt

·

−

2

.

x∗
−
2ηL
√m

xt
(cid:13)
(cid:13)

(cid:13)
(cid:13)
1¯xt

k ≤

−

x∗

−

1)
·
¯xt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Df (¯xt, x∗)

x∗

−

(cid:13)
(cid:13)

Df (¯xt, x∗)

·
x∗

−
¯xt

1)

−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Using the fact

(cid:13)
¯xt
(cid:13)
−

k

(cid:13)
√V t, we can obtain that
(cid:13)

E

T 2

(1
(cid:2)
−

≤

ηµ)
(cid:3)

¯xt

(cid:13)
+ 2η2∆t +
(cid:13)

2

x∗
−
2ηL√V t
(cid:13)
(cid:13)
√m

+ 2η(4Lη

−
1¯xt

xt

−

Df (¯xt, x∗)

1)

·

(48)

+

4L2η2
m

xt

−

1¯xt

(cid:13)
2
(cid:13)
,

(cid:13)
(cid:13)

(cid:13)
(cid:13)
T 2 + 4nη2∆t+1

E

and

(cid:13)
(cid:13)

(48),(29)

≤

(cid:2)
ηµ)
(1
−
12L2η2
(cid:13)
(cid:13)
m

+

−
xt

¯xt

2
(cid:3)

x∗

+ 2η(12Lη

1)Df (¯xt, x∗)

−

2ηL√V t
√m

(cid:13)
1¯xt
(cid:13)

2

+

(cid:13)
(cid:13)

(4nη2∆t)

xt

−

1¯xt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

1
(cid:18)

−

max

1

(cid:18)
1
12m

+

≤

(cid:13)
(cid:13)
+

1
n

−
1
2n

(cid:19)

1
12κ

−

, 1

xt

−

1¯xt

(cid:13)
(cid:13)

−
2

(cid:13)
(cid:13)

1
2n

,

·

(cid:19)

V t +

√V t
6√m

xt

−

1¯xt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(49)

where the last inequality is because we set η = 1/(12L).

Combining with Eqn. (47) and the deﬁnition of Lya-

punov function V t, we have

E[V t+1]
¯xt+1

=E

h(cid:13)
(cid:13)

2

x∗

+ 4nη2∆t+1

−

(cid:13)
(cid:13)

i

