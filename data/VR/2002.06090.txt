1

Mobile Communications, Computing and

Caching Resources Optimization for Coded

Caching with Device Computing

Yingjiao Li, Zhiyong Chen, and Meixia Tao

Abstract

Edge caching and computing have been regarded as an efﬁcient approach to tackle the wireless

spectrum crunch problem. In this paper, we design a general coded caching with device computing

strategy for content computation, e.g., virtual reality (VR) rendering, to minimize the average trans-

mission bandwidth with the caching capacity and the energy constraints of each mobile device, and

the maximum tolerable delay constraint of each task. The key enabler is that because both coded data

and stored data can be the data before or after computing, the proposed scheme has numerous edge

computing and caching paths corresponding to different bandwidth requirement. We thus formulate a

joint coded caching and computing optimization problem to decide whether the mobile devices cache the

input data or the output data, which tasks to be coded cached and which tasks to compute locally. The

optimization problem is shown to be 0-1 nonconvex nonsmooth programming and can be decomposed

into the computation programming and the coded caching programming. We prove the convergence

of the computation programming problem by utilizing the alternating direction method of multipliers

(ADMM), and a stationary point can be obtained. For the coded cache programming, we design a low

complexity algorithm to obtain an acceptable solution. Numerical results demonstrate that the proposed

scheme provides a signiﬁcant bandwidth saving by taking full advantage of the caching and computing

capability of mobile devices.

Coded Caching, Mobile Edge Computing, Multicast, Bandwidth Allocation, Virtual Reality

Index Terms

0
2
0
2

b
e
F
4
1

]
T
I
.
s
c
[

1
v
0
9
0
6
0
.
2
0
0
2
:
v
i
X
r
a

Authors are with Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {lyj970222, zhiyongchen, mx-

tao}@sjtu.edu.cn). (Corresponding author: Zhiyong Chen).

February 17, 2020

DRAFT

 
 
 
 
 
 
2

I. INTRODUCTION

Bandwidth saving is an eternal topic in wireless communications systems, especially in the era

of shortage of wireless spectrum resource. In recent years, bandwidth requirement in the wireless

network has been greatly spurred by broadband applications and services, such as the immersive

panoramic virtual reality (VR) video, high deﬁnition holographic gaming, and 8K/16K ultra-high

deﬁnition video [1]. For example, sending full immersive VR video in 16K with H. 265 requires

more than 4 Gbit/s [2]. Such modern bandwidth loads impose signiﬁcant challenges to today’s

mobile network and has driven wireless researchers and mobile operators to ﬁnd ways to keep

up with people’s ever-growing needs in the bandwidth for a better life.

By looking at the wireless spectrum crunch problem, we notice that differing greatly from the

conventional services, e.g., phone calls and text messages, broadband applications and services

highly relies on an asynchronous content reuse [3]. As such, caching popular contents at the

wireless edge during off-peak time can alleviate peak-hour network congestion, i.e., saving the

bandwidth [4]. In the seminal paper [5], the coded caching scheme is proposed to relieve the

trafﬁc burden by exploiting the caching size of mobile device (MD). Meanwhile, in [6], authors

ﬁnd that exploiting the caching resources at the relay and users can provide signiﬁcant throughput

gain in large-scale wireless heterogeneous networks. The above studies reveal that the edge

caching, e.g., at a base station (BS) or the mobile devices, has been regarded as a key enabling

technology in future wireless networks to tackle the wireless spectrum crunch problem.

Besides edge caching, edge computing can also be exploited to reduce the bandwidth require-

ment in the wireless network [7], [8]. In the edge computing architecture, the network operators

and the service providers can place the computing servers at the network edge, e.g., BS. Taking

immersive VR application for example, with the help of, the VR video can be rendered at the

edge computing server based on users ﬁeld of view (FoV) so as to reduce the system latency

and backhaul trafﬁc. However, the data rate of sending immersive VR video is still very high

even if we use the edge computing at BS for rendering, because the classical edge computing

cannot reduce the air-interface trafﬁc load. A better solution is to perform the on-BS rendering

with the on-device rendering, proposed in our previous work [7], [9]. In this way, by exploiting

the computing capability of mobile devices, BS can ofﬂoad partial data (e.g., 2D FOV) to the

mobile device, and then the the mobile device performs the partial rendering, e.g., computing

the 2D FOV to the 3D FOV, such that a lot of bandwidth is saved.

Inspired by this, this paper proposes a coded caching with device computing strategy to

February 17, 2020

DRAFT

3

minimize the average bandwidth consumption subject to the caching size and the energy of

the MD, as well as the delay constraints. The basic idea of the propose scheme is that BS with a

mobile edge computing (MEC) server can multicast the coded data of the requested tasks (e.g.,

FOV) to devices based on the coded cache method [5], and the coded data can be the data before

or after rendering, i.e., input or output data. What’s more, the data of cached in each mobile

device is either the input data or the output data. As a result, with the cooperation of the edge

caching and computing at the mobile device, the coded cache with computing has more space

to reduce the bandwidth consumption, which is also different from our previous work [7], [9]

and the traditional work on the coded caching [5], [10].

A. Related Work

Existing works on coded caching only consider the storage of mobile devices, without consid-

ering the computing of mobile devices [10]–[12]. [10] takes an arbitrary popularity distribution

into consideration and derives a new lower bound on the transmission rate of any coded caching

schemes. The authors of [11] propose a novel coded scheme without coordination among users

after [5]. The distributed storage of multiple servers is considered with coded caching in [12].

In addition to the coded caching, there are also several current researches for caching content

at the edge of wireless networks. In [13], authors found out that when the cache size of BS

is larger than a threshold, the BS density can be reduced by increasing the cache size of BS.

[14] consider content caching at user devices in a wireless device-to-device (D2D) network

and conclude that the cache-aided D2D networks can turn storage into bandwidth. In order to

minimize the request miss ratio, a optimal bandwidth allocation scheme with the content caching

placement is proposed in [15] for a heterogeneous cellular network. Nevertheless, these articles

rarely consider both the cache capability and computation capability of BS and user devices.

Generally speaking, the arrival time of the task rely on the computation scheme and hence

the processing on the MEC server of the task is impacted [16]. Currently, several literary have

investigated the issues like [17], [18]. The computational data in [17] can be split for two

calculation methods: computing locally and cloud computing. In the paper [18], the mobile

device operates in one of the two modes when a ﬁxed size of data need to be computed:

local computing or ofﬂoading. Through those papers in the traditional MEC architecture, we

ﬁnd that the computation tasks are both generated at user devices. Furthermore, there are also

several papers combining the traditional MEC architecture with immersive VR application. [19]

optimizes the viewport rendering ofﬂoading strategy under the computation capability in the

February 17, 2020

DRAFT

4

MEC server and VR device. The paper [20] leverages the cache-aided and computing-aided

edge server for proactive computing and caching the frames. The edge computing server is

exploited in [21] to compute VR input data product in user device. Yet the cache capability at

the user device is not considered in the above researches.

Recently, there have been some works on the caching and computing in the wireless network.

In [22], the optimal computing ofﬂoading and caching decisions is designed for minimizing the

sum computing latency in a hybrid mobile cloud/edge computation system. However, the work

of [22] only consider the caching and computing resource in the access point (AP). On the other

hand, our previous works [7], [9] take full advantage of edge caching and edge computing which

jointly optimizes the cache and computation policy at user devices to minimize the transmission

bandwidth in the MEC system, without considering the coded caching scheme.

B. Main Contributions and Paper Organization

The main contributions of this paper are summarized as follows:

• The coded caching with device computing model: This work considers a BS with a

MEC server and multiple computing-enabled and caching-aided mobile devices, thus BS

has different caching and computing policies to serve the mobile devices’ request. For the

coded input data transmitted by BS, the mobile device can obtain the integral data and

then compute the input data locally to obtain the output data of the request tasks, i.e., the

computation result. Similarly, the coded output data is transmitted by BS and the complete

output data requested is recovered without computing in the mobile device when the partial

output data of tasks has been cached. In addition, BS also can select to multicast the entire

input (or output) data of one requested task to those mobile devices that the task has not

been cached. Obviously, this is a more general model than that of considered in the previous

works [7], [9].

• The optimal strategy for minimizing bandwidth consumption: We further analyze that

under the guarantee of quality of service (QoS), different edge caching and computing

policies have different bandwidth consumption. In this sense, this work jointly optimize the

coded cache and the computation scheme to decide the MDs cache whether the input data

or the output data, which tasks to be coded cached and which tasks to compute locally. We

thus formulate the average bandwidth minimization problem subject to the caching size and

energy constraints for the mobile device, and the latency constraint for each task.

February 17, 2020

DRAFT

5

Figure 1: A coded caching with device computing system considered in this paper, where a BS with MEC server

is connected through a wireless channel to K computing-enabled and caching-aided mobile devices.

• Proposed algorithm for the optimization problem: The optimal problem is a 0-1 noncon-

vex nonsmooth programming problem, which is NP-hard. We thus decouple the coded cache

decision and the computation decision, then reformulate the problem into a computation

programming and a coded cache programming to simplify the original programming. The

alternating direction method of multipliers (ADMM) algorithm is used to solve the compu-

tation programming, and we prove that the nonconvex problem can converge on monotropic

program based on ADMM, therefore a stationary point of the computation programming

is obtained. Finally, for the coded cache programming, the problem can be decomposed

into two subproblems based on the value of the cache decision, and then an algorithm is

proposed in this paper to obtain the acceptable solution.

The rest of this paper is organized as follows. Section II introduces the system model in

terms of coded caching model, task request and computation model, communication model and

the transmission bandwidth cost. Section III formulate the problem minimizing bandwidth, and

decompose the original programming into several subproblems. Low complexity algorithms for

those subproblems are proposed in Section IV. Simulation results are shown in Section V. Finally

the conclusion is given in Section VI.

II. SYSTEM MODEL

Consider a general mobile downlink system with edge caching and computing, as shown in
Fig. 1, the key components include K single-antenna mobile devices, denoted by a set K (cid:44)

February 17, 2020

DRAFT

Base Stationoutput data input data Data centerMEC serverMobile devicesStorageCompute6

{1, 2, · · · , K}, and one BS with a MEC server. The MEC server is typically small-scale data

center, and each mobile device can connect to the MEC via a BS. We consider the database
of the computation tasks library consisting of F tasks denoted as F (cid:44) {1, 2, · · · , F } has been

cached in the data center, e.g., there have been cached the rendered frames in the MEC server

when we are watching the VR movie in the VR cinema.

A. Coded Cache Model

As shown in the model, the input data and the output data of each computation task have

been cached in the MEC server. Suppose that each computation task has the same size of input

data and output data, denoted as I(in bits) and O(in bits) respectively. We consider that each

mobile device has been endowed the same capability of storage which is denoted as C(in bits).

In this paper, we should determine that what kind of data is coded cached, which task is

cached in terms of the coded cache scheme. Then, let cf denote the cache decision for the task

f , and

cf ∈ {0, 1}, f ∈ F.

(1)

Here, the task f is coded cached if cf = 1, and cf = 0 otherwise. Denote c (cid:44) (cf )f ∈F as the
coded cache decision in the system. Let N ⊆ F denote the set of computation tasks that are

decided to be coded cached in each mobile device, i.e., cn = 1, for all n ∈ N . N = |N | is the
number of the tasks coded cached, and N (cid:54) K obviously.

In the paper, denote d as the type of cached data decision for all tasks which are decided to

be cached, where d = 1 means the type of cached data is input data, and d = 0 means the type

of cached data is output data. In other words, the input data of the tasks N is decided to be

coded cached at each mobile device when d = 1. If d = 0, the output data of the tasks N is

coded cached.

According to the coded caching scheme [5], the MEC server need to ﬁll the storage of each
mobile device based on d and c in the cache phase. If c (cid:54)= 0, i.e., N (cid:62) 1, let t = (cid:98) CK
N I (cid:99)
when d = 1. We set T ⊆ K : |T | = t and denote U = (cid:83) T . Therefore, the input data of
the computation task n ∈ N is split into subﬁles (Wn,T : T ∈ U) with equal size. During the

cache phase, the MEC server transmits the subﬁles (Wn,T : n ∈ N , T ∈ U, k ∈ T ) to the mobile

device k, and the partial input data of the tasks N is cached for all k ∈ K. Note that there is no
cached data in each mobile devices when t = 0 and T = ∅, thus the coded cache scheme c is
resettled to be 0. When d = 0, t = (cid:98) CK

N O (cid:99) and the placement policy is consistent with the above.

February 17, 2020

DRAFT

7

Figure 2: A example for F = 3 computation tasks and K = 3 mobile devices with the storage size C = I. Assume

that the type of cached data is input data, i.e., d = 1.

Therefore, the computation task n ∈ N that decided to be coded cached is divided into (cid:0)K
(cid:1)
subﬁles that have no overlap with each other, then the corresponding (cid:0)K−1
(cid:1) subﬁles are cached
in each mobile device when t (cid:62) 1. The constrain of the cache size constrain is satisﬁed as

t−1

t

following,

N

(cid:19)

(cid:18)K − 1
t − 1

I/

(cid:18)K
t

(cid:19)

= N It/K (cid:54) N I

CK
N I

/K = C,

when d = 1, and when d = 0,
(cid:18)K − 1
t − 1

N

(cid:19)

O/

(cid:19)

(cid:18)K
t

= N Ot/K (cid:54) N O

CK
N O

/K = C.

(2)

(3)

Note that t = K while t > K since the data of each computation task n always is cached as a

whole if t > K which is the same as the situation t = K, for all n ∈ N .

In other to get a better understanding, we give an example in Example 1.

Example 1. As shown in Fig. 2, consider an example F = K = 3 where K (cid:44) {1, 2, 3} and

C = I. The input data of the computation tasks A, B, C is coded cached at each mobile device
and N = F (cid:44) {A, B, C} where N = |F| = 3, that is c = {1, 1, 1} and d = 1. We split A, B
and C into three subﬁles of equal size since t = 1, and U (cid:44) {{1}, {2}, {3}} as follows

I(A) : {I(A1), I(A2), I(A3)}, I(B) : {I(B1), I(B2), I(B3)}, I(C) : {I(C1), I(C2), I(C3)}.

February 17, 2020

DRAFT

O(A)O(C)O(B)I(A1),I(B1),I(C1)I(A2),I(B2),I(C2)I(A3),I(B3),I(C3)I(A1),I(A2),I(A3)I(B1),I(B2),I(B3)I(C1),I(C2),I(C3)O(A)O(B)O(C)O(A)I(B1),I(C1), I(B3) I(C2)Device requestStorageWireless LinkMEC ServerComputingComputingI(B2)I(B1), I(B3)I(C3)I(C1), I(C2)MD1MD3MD28

During the cache phase, the cached subﬁles in each mobile device are

M D1 : {I(A1), I(B1), I(C1)}, M D2 : {I(A2), I(B2), I(C2)}, M D3 : {I(A3), I(B3), I(C3)}.

In the transmission phase, we consider that the device M D1, M D2 and M D3 request the

computation task A, B, and C respectively in a task request state. When the computation scheme

is set to be x1,A = 0, x2,B = 1 and x3,C = 1, the MEC server download the output data O(A)

directly to the device M D1 since there is no output data of the task A cached. The tasks B,

C coded cached are requested and decided to be computed locally. Therefore, the MEC server
will send the coded multicast transmission to the device M D2 and M D3, i.e., Mq = {2, 3},

d2 = B, d3 = C and Θ = {{1, 2}, {1, 3}, {2, 3}}. During the delivery phase, the data of the

coded multicast transmission based on the coded cached scheme in the paper is

X(B, C) = {I (B1)}, {I (C1)}, {I (B3) ⊕ I (C2)}.

The data is multicast to the device M D2 and M D3 simultaneously. Then M D2 and M D3

recover the input data I(B) and I(C) respectively and compute the input data locally to obtain

the computation results O(B) and O(C).

B. Task Request and Computation Model

We consider that the system model is a time-slotted system where time is divided into slots

each with a duration of τ seconds, which meet latency constraint for QoE. At the beginning

of each time slot, each mobile device requests the computation result of a task simultaneously.

Assume that the request latency is negligible.

Denote pk,f as the possibility of the computation task f requested by the mobile device
k, where (cid:80)F
f =1 pk,f = 1. For all k ∈ K, the probability of request is mutually independent.
Denote Q (cid:44) {1, 2, · · · , F K} as the set of all possible random task requests in a time slot. Let
S (cid:44) (Sq)q∈Q denote the random task request space, and note that Sq is denoted as a possible
random task request state in a time slot. In addition, let sq
k,f denote the request action in the
random task request state Sq, where sq
k,f = 1 means that the mobile device k requests the
computation task f in the request state Sq, and sq
k,f = 1 for
all k ∈ K. The probability P (Sq) of a random task request state in the time slot Sq can be

k,f = 0 otherwise. Thus, (cid:80)F

f =1 sq

formulated as follows,

P (Sq) =

(cid:32)

(cid:88)

(cid:89)

(cid:0)pk,f sq

k,f

(cid:33)
(cid:1)

.

k∈K

f ∈F

February 17, 2020

(4)

DRAFT

9

Assume that the CPU frequency is ﬁxed at each mobile device and may vary over mobile

devices, which is denoted as gk (in cycles/s). The workload is determined by the nature of the

task itself and can be obtained through off-line measurement [23]. Without loss of the generality,

the workload measured by the number of CPU cycles for processing one bit of the input data

is denoted as wf (in cycles/bit). Then the product Iwf gives CPU cycles needed to successfully

execute the task f . Let xk,f denote the computational selection for the task f at the mobile

device k, where

xk,f =




1,



0,

the computation task f is computed at mobile device k,

the computation task f is computed at the MEC server.

(5)

Note that the mobile device k needs to obtain the input data via the MEC server and computes

the computation task f when xk,f = 1, and when xk,f = 0, the task f has been computed

in the MEC server, and there is no computation consumption in the mobile device. Denote
x (cid:44) (xk,f )k∈K,f ∈F as the system computation decision.

The dynamic power is the only power considered for the mobile execution in the paper since

the dynamic power dominates. The energy per cycle is proportional to the supply voltage to the

CPU [24], [25]. Furthermore, the clock frequency of the CPU is approximately linear proportional

to the voltage supply. Therefore, the energy per cycle can be formulated as αg2

k in the mobile
device k, where α is a constant corresponding to the hardware architecture. Therefore, there is

energy consumption for computing locally. Before a time slot, the task request state is unknown.

Consequently, we can only restrict the possible energy consumption in the mobile device k under

the computation scheme and the request probability known in advance, and the constraint can

be expressed as

F
(cid:88)

f =1

pk,f αg2

kIwf xk,f ≤ Ek,

(6)

and here Ek is the energy limits in the mobile device k.

C. Communication Model

When the mobile devices request the computation results of corresponding tasks, BS transmits

the required data to the mobile devices according to the decision d, c and x in a request state

Sq. First, we consider d = 1, i.e., the input data is stored in each mobile device with the coded

state, so that there are three transmission cases.

1) Case 1: {d = 1, xk,f = 1, cf = 1}. For this case, the mobile device k requests a

computation task whose input data has been coded cached and decides to compute locally.

February 17, 2020

DRAFT

10

1, mq

2, · · · , mq

Denote Mq (cid:44) {mq
M q } as the set of MDs whose request are satisﬁed with the case
in the task request state Sq where Mq ⊆ K and M q = |Mq|. The set of tasks requested by
Mq is denoted as D (cid:44) {dmq
MD mq
i , there are xmq

M q }, that is when sq
mq
i ,dm
q
i
= 1, and i ∈ {1, 2, · · · , M q}. Due to the computing at the

= 1 for the task dmq

, dmq
= 1, cdm

, · · · , dmq

at

2

1

i

i ,dm

q
i

q
i

mobile device, the transmission scheme is different from [5]. Under the conditions that we set
V q : (V q ⊆ K, |V q| = t+1, ∃mq ∈ V q) and Θq = (cid:83) V q, where mq ∈ Mq. The set of subﬁles that
(cid:17)
: (cid:0)⊕mq∈V q Wdmq ,V q\{mq} : V q ∈ Θq(cid:1) as shown in
are coded is denoted as X
(cid:16)
dmq
is

Example 1 where ⊕ denotes bitwise XOR. Then, the coded subﬁles X

, · · · , dmq

, · · · , dmq

, dmq

, dmq

dmq

M q

(cid:16)

(cid:17)

1

2

1

2

M q

multicast to the mobile devices Mq by the MEC server for satisfying the requests simultaneously.

Therefore, we have the following proposition.

Proposition 1. Denote bq (c, x) as the rate of the coded multicast transmission in the task request
state Sq. bq (c, x) = 0 when t = K or M q = 0, and if t < K, bq (c, x) can be expressed as

bq (c, x) =






(K − t) / (1 + t) , M q (cid:62) K − t ,
(cid:1),

1 (cid:54) M q < K − t.

(cid:1)/(cid:0)K

(cid:80)M q
i=1

(cid:0)K−i
t

t

(7)

Proof. Proof can be seen in Appendix A.
(cid:80)F
Here, M q can be expressed as (cid:80)K

k=1

f =1 cf xk,f sq
k,f . Therefore, the data size of the coded
multicast transmission is Ibq (c, x). For the mobile device k with k ∈ Mq, the time spent
on computing locally is Iwf
gk
transmission rate for this case in the mobile device k, and we can obtain that Ibq(c,x)
Hence, r1

k,f,q as the minimum
= τ .

. To satisfy the latency deadline τ , denote r1

k,f,q can be formulated as

+ Iwf
gk

r1
k,f,q

k,f,q = Ibq (c, x) /
r1

(cid:18)

τ −

(cid:19)

Iwf
gk

1(cf xk,f sq

k,f = 1),

(8)

where 1(·) denotes the indicator function. Suppose that the computation duration locally at each
mobile device k ∈ K for the task f always meets the latency constraint, i.e., τ (cid:62) Iwf
gk
the centralized content multicast is employed, we deﬁne R1

q as the coded multicast transmission

. Since

rate, and then have

R1
q

(cid:62) r1

k,f,q, ∀k ∈ K, ∀f ∈ F.

(9)

2) Case 2: {d = 1, xk,f = 1, cf = 0}. The mobile device k requests a computation task

f decided to computed locally. However, the computation task f has not been coded cached,

which is different from the previous case. BS can multicast the entire input data to the mobile

February 17, 2020

DRAFT

11

devices that request the task f . Similarly, the minimum transmission rate r2

k,f,q for this case can

be expressed as

r2
k,f,q =

I
τ − Iwf
gk

1((1 − cf )xk,f sq

k,f = 1).

Therefore, the multicast transmission rate R2

f,q is

R2
f,q

(cid:62) r2

k,f,q, ∀k ∈ K.

(10)

(11)

3) Case 3: {d = 1, xk,f = 0}. In this case, the output data of the task f is decided to

be obtained by the mobile device k without computing locally, which means BS transmits the

output data to the mobile device k. Under the latency constraint, we can get

r3
k,f,q =

O
τ

1((1 − xk,f )sq

k,f = 1).

Similarly, the multicast transmission rate R3

f,q for this case is

R3
f,q

(cid:62) r3

k,f,q, ∀k ∈ K.

(12)

(13)

For d = 0, the output data is decided to be coded cached in the mobile devices, and three

cases are considered in the paper.

4) Case 4: {d = 0, xk,f = 0, cf = 1}. The partial output data of the computation task f

has been coded cached and is decided be computed at the MEC server. The mobile devices

request the tasks in the case in the request state Sq, then the coded output data is multicast to

the devices. According to the transmission phase in the paper, the data size of the coded output

data is Obq (c, x). We thus have

r4
k,f,q =

Here, M q is reformulated as (cid:80)K
mission rate R4

k=1

q meet those requests as follows,

1(cf (1 − xk,f )sq

Obq (c, x)
τ
(cid:80)F
f =1(1 − xk,f )cf sq

k,f = 1).

k,f . We obtain the coded multicast trans-

(14)

R4
q

(cid:62) r4

k,f,q, ∀k ∈ K, ∀f ∈ F.

(15)

5) Case 5: {d = 0, xk,f = 0, cf = 0}. Similar to Case 3,the task f in the mobile device k is

computed by the MEC server, and has not been coded cached. Hence, BS transmits the entire

output data to the mobile device to meet the request. The minimum transmission rate is

r5
k,f,q =

O
τ

1((1 − cf )(1 − xk,f )sq

k,f = 1).

The multicast transmission rate R5

f,q for the request of the task f in this case is

R5
f,q

(cid:62) r5

k,f,q, ∀k ∈ K.

February 17, 2020

(16)

(17)

DRAFT

12

6) Case 6: {d = 0, xk,f = 1}. Similar to Case 2, BS transmits the entire input data to the

mobile device for computing locally since there are not cached input data. And we have

r6
k,f,q =

I
τ − Iwf
gk

1(xk,f sq

k,f = 1).

(18)

For reducing bandwidth, when the same task f is requested by multiple devices simultaneously,

we deﬁne

R6
f,q

(cid:62) r6

k,f,q, ∀k ∈ K

(19)

as the multicast transmission rate in the case.

III. PROBLEM FORMULATION FOR BANDWIDTH MINIMIZATION

Observe that, through different exploiting edge computing and caching paths, we have different

bandwidth requirements on the wireless channel. Therefore, the purpose of this paper is to

minimize the average bandwidth cost by optimizing the edge computing and caching policy.

A. Transmission Bandwidth

Let hk denote the channel gain for the mobile device k and the bandwidth is B (in Hz). For

the LTE/5G NR system, the transmission power spectral density is constant across the downlink

system bandwidth [26]. Let εk and n0 are the power spectral density of the transmission power

and the additive white Gaussian noise, respectively. Therefore, we have the transmission rate

R = B log2(1 +

εkh2
k
n0

).

(20)

In this paper, we consider that the same data requested by the mobile devices can be grouped

together and served by the multicast transmission. Considering that the multicast rate is limited

by the user with the worst channel condition in one multicast group, the corresponding channel

conditions in the above cases are

H 1

q = max
k∈K,f ∈F

1
log2(1 + εkh2

k

n0

1(cf xk,f sq

k,f = 1),

)

H 2

f,q = max
k∈K

H 3

f,q = max
k∈K

H 4

q = max
k∈K,f ∈F

)

k

n0

1
log2(1 + εkh2
1
log2(1 + εkh2
1
log2(1 + εkh2

n0

k

k

n0

1((1 − cf )xk,f sq

k,f = 1),

1((1 − xk,f )sq

k,f = 1),

1(cf (1 − xk,f )sq

k,f = 1),

)

)

February 17, 2020

(21)

(22)

(23)

(24)

DRAFT

H 5

f,q = max
k∈K

1
log2(1 + εkh2

k

n0

1((1 − cf )(1 − xk,f )sq

k,f = 1),

)

H 6

f,q = max
k∈K

1
log2(1 + εkh2

k

n0

1(xk,f sq

k,f = 1).

)

13

(25)

(26)

Here, deﬁne BI

q as the achievable bandwidth in the task request state Sq when the input data is

decided to be coded cached, i.e., d = 1, and the expression is as follows

BI

q = R1

q × H 1

q +

(cid:88)

f ∈F

R2

f,q × H 2

f,q +

(cid:88)

f ∈F

R3

f,q × H 3

f,q.

Similar to (27), the corresponding bandwidth BO
q

if d = 0 can be expressed as

BO

q = R4

q × H 4

q +

(cid:88)

f ∈F

R5

f,q × H 5

f,q +

(cid:88)

f ∈F

R6

f,q × H 6

f,q.

(27)

(28)

B. Problem Formulation

Mathematically, the optimization average bandwidth minimization problem can be formulated

as follows

P1 :

min
c,d,x

(cid:88)

q∈Q

P (Sq) (cid:0)d × BI

q + (1 − d) × BO
q

(cid:1)

s.t.

d ∈ {0, 1},

(1) , (5) , (6) , (9) , (11) , (13) , (15) , (17) , (19) ,

which implements a joint design of coded cache scheme and computation scheme. The objective

function is the expectation of the bandwidth in a time slot, and it is easy to observe that P1 is a

0-1 nonlinear programming problem which is intractable to derive a close-form expression. Since

there are F K task request states to be considered in the objective function that will generate huge

computation, we replace the task request space with the set of samples NS as an approximation

for simplify where NS ⊂ Q and |NS| = Ns [27], [28]. Note that the samples are related to the

request probability. Thus, problem P1 can be reformulated as below,

P1.1 :

min
c,d,x

1
Ns

(cid:88)

n∈NS

(cid:0)d × BI

n + (1 − d) × BO
n

(cid:1)

s.t.

d ∈ {0, 1},

(1) , (5) , (6) , (9) , (11) , (13) , (15) , (17) , (19) .

February 17, 2020

DRAFT

14

C. Decomposition of Problem P1.1

It is a challenge to solve P1.1 since the objective function is nonconvex and nonsmooth

although the constraints are convex. In this subsection, the decision variables x and c are

decoupled and we decompose P1.1 into several subproblems to obtain the minimal average

bandwidth. At the beginning, the problem on the top level is obtaining the computation strategy

x. Suppose that there are no coded cache design, i.e., c = 0. Therefore, the subproblem has no

business to the variable d and BS ofﬂoads the entire input data or the entire output data to the

mobile devices.

the transmission rate at mobile device k is the same as r6

When xk,f = 1, BS multicasts the entire input data of the task f to the mobile device k, then
k,f,q in a task request state Sq. Similarly,
k,f,q when xk,f = 0. We

the transmission rate in the mobile device k for the requested task f is r3

optimal the computation variable x under the following optimization

P2 :

min
x

1
Ns

(cid:32)

(cid:88)

n∈NS

f ∈F

(cid:88)

(cid:0)R3

f,n × H 3

f,n + R6

f,n × H 6
f,n

(cid:33)
(cid:1)

s.t.

(5) , (6) , (13) , (19) .

Followed by obtaining a given computation design x∗ which is the solution of P2 , the

subproblem of optimizing variable c and d can be formulated as

P3 : min
c,d

1
Ns

(cid:88)

n∈NS

s.t. x = x∗,

(cid:0)d × BI

n + (1 − d) × BO
n

(cid:1)

(1) , (9) , (11) , (15) , (17) .

The variables c and d are uncoupled, and we can decomposed the above problem P3 into two

subproblems based on the type of the cache decision d. When d = 1, the one of the subproblems

is expressed as follows,

P3.1 : min

c

1
Ns

(cid:88)

n∈NS

s.t. x = x∗,

(R1

n × H 1

n +

(cid:88)

f ∈F

R2

f,n × H 2

f,n)

If we assume that the data type of coded cached in MDs is the output data, i.e., d = 0, the

(1) , (9) , (11) .

subprobelm is

P3.2 : min

c

1
Ns

(cid:88)

n∈NS

(R4

n × H 4

n +

(cid:88)

f ∈F

R5

f,n × H 5

f,n)

February 17, 2020

DRAFT

s.t. x = x∗,

(1) , (15) , (17) .

15

In a word, the problem P1.1 is decomposed into problem P2, P3.1 and P3.2. Next, these

subproblems will be solved separately and the efﬁcient coded caching with device computing

strategy is derived.

IV. EFFICIENT CODED CACHING WITH DEVICE COMPUTING STRATEGY

A. Algorithm to Solve Problem P2

Obviously, the constraints (5), (6), (13) and (19) are convex, but the objective function in

P2 is nonconvex and nonsmooth due to the maximum terms. Therefore, we ﬁrstly reformulate

the channel condition variables H 3

f,n and H 6

f,n to replace these maximum terms in the objective

function, and the problem reformulated can be expressed as below

P2.1 :

min
x

1
Ns

(cid:88)

(cid:88)

n∈NS

f ∈F

(cid:0)R3

f,n × H 3

f,n + R6

f,n × H 6
f,n

s.t.

(5) , (6) , (13) , (19) ,

H 3
f,n

(cid:62)

1
log2(1 + εkh2

k

n0

1(xk,f sn

k,f = 1), ∀k ∈ K,

)

(cid:1)

(29a)

(29b)

H 6
f,n

(cid:62)

1((1 − xk,f )sq

1
log2(1 + εkh2
f,n}f ∈F ,n∈NS , R6 = {R6

n0

)

k

k,f = 1), ∀k ∈ K.

Here we denote that R3 = {R3

f,n}f ∈F ,n∈NS , and H3 = {H 3

f,n}f ∈F ,n∈NS ,

H6 = {H 6

f,n}f ∈F ,n∈NS .

By reformulate the channel condition variables H 3

f,n, P2.1 becomes smooth problem
which is easier to be tackled. Moreover, the objective function in P2.1 is a difference of convex

f,n and H 6

(DC) function, and it is still an integer programming (IP) problem. There is a large body of

work that utilizes similar methods to ﬁnd a suboptimal solution of the IP problem, such as

Branch-and-Bound [29], cutting plane [30] which are usually plagued with high computational

complexity. In this paper, we propose to replace the binary constraints (5) with an equivalent

set of continuous constraints ﬁrstly [31].

February 17, 2020

DRAFT

16

Lemma 1. The binary set {0, 1}KF can be equivalently replaced by the intersection between a

box Sb and a nonconvex constraint as follows:

x ∈ {0, 1}KF ⇔ x ∈ [0, 1]KF ∩ {x :

(cid:88)

(cid:88)

f ∈F

k∈K

xk,f (1 − xk,f ) ≤ 0},

(30)

where the box is Sb = [0, 1]KF .

We add the equivalent continuous nonconvex constraint into objective as penalty methods do

[32], [33], [34] and the penalty parameter is deﬁned as β. As far as P2.1 is concerned, the

numbers of the variables and the constraints reach KF and 4NsKF + KF + K. Generally

speaking, ADMM is always exploited to optimize large-scale convex programming, however,

it has also been proved that the nonconvex problem can be tackled [35]. In this paper, we
employ ADMM to solve the problem by introduce a set of local copies of the variables ˆR3,
ˆR6, and ˆH3, ˆH6 and y, where are deﬁned as ˆR3 = { ˆR3
ˆH3 = { ˆH 3
f,n}f ∈F ,n∈NS , and y = {yn

f,n}f ∈F ,n∈NS , ˆR6 = { ˆR6
f,n}f ∈F ,n∈NS ,
k,f }k∈K,f ∈F ,n∈NS to product consensus
constraints. Based on the consensus constraints, the coupling constraints (13), (19), (29a), (29b)

f,n}f ∈F ,n∈NS , ˆH6 = { ˆH 6

can be rewritten as

ˆR3

f,n

(cid:62)

I
τ − Iwf
fk

1 (cid:0)yn

k,f sn

k,f = 1(cid:1) , ∀k ∈ K,

ˆR6

f,n

(cid:62) O
τ

1 (cid:0)(1 − yn

k,f )sn

k,f = 1(cid:1) , ∀k ∈ K,

ˆH 3
f,n

(cid:62)

1
log2(1 + εkh2

k

n0

1(yn

k,f sn

k,f = 1), ∀k ∈ K,

)

(cid:62)

ˆH 6
f,n

1
log2(1 + εkh2
As a result, we can obtain the equivalent version of P2.1 given by
(cid:0)R3

1((1 − yn

(cid:1) + β

k,f )sq

(cid:88)

(cid:88)

f,n × H 3

f,n + R6

f,n × H 6
f,n

n0

)

k

k,f = 1), ∀k ∈ K.

P2.2 : min
x,y,Π, ˆΠ

1
Ns

(cid:88)

(cid:88)

xk,f (1 − xk,f )

n∈N

f ∈F

f ∈F

k∈K

s.t.

(5) , (6) , (31a) , (31b) , (31c) , (31d) ,

yn
k,f = xk,f , ∀n ∈ NS,

ˆR3

f,n = R3

f,n, ∀f ∈ F, ∀n ∈ NS,

ˆR6

f,n = R6

f,n, ∀f ∈ F, ∀n ∈ NS,

(31a)

(31b)

(31c)

(31d)

ˆH 3

f,n = H 3

f,n, ∀f ∈ F, ∀n ∈ NS,

ˆH 6

f,n = H 6

f,n, ∀f ∈ F, ∀n ∈ NS,

February 17, 2020

DRAFT

17

where Π = {R3, R6, H3, H6} and ˆΠ = { ˆR3, ˆR6, ˆH3, ˆH6}. Moreover, we can get the following

Lemma according to Theorem 5 and Theorem 8 in [36].

Lemma 2. P2.1 and P2.2 have the same optimal solution when the penalty parameter β is

large enough.

According to [37], the augmented Lagrangian function can be formulated as

(cid:16)
x, y, λ, z, Π, ˆΠ

(cid:17)

=Ψ

(cid:16)

Lρ

x, y, Π, ˆΠ, β

(cid:17)

+

ρ0
2

(cid:88)

(cid:107)x − yn + λn(cid:107)2
2

+

+

ρ1
2
ρ3
2

(cid:107) ˆR3 − R3 + z1(cid:107)2

(cid:107) ˆH3 − H3 + z3(cid:107)2

2 +

n∈NS
ρ2
2
ρ4
2

2 +

(cid:107) ˆR6 − R6 + z2(cid:107)2
2

(32)

(cid:107) ˆH6 − H6 + z4(cid:107)2
2,

(cid:16)
x, y, Π, ˆΠ, β

(cid:17)

where Ψ

is the objective function in the problem P2.2, and ρ = {ρ0, ρ1, ρ2, ρ3, ρ4}
are positive penalty parameters. Note that yn = {yn
k,f }k∈K,f ∈F , λn = {λn
k,f }k∈K,f ∈F , λ =
f,n}f ∈F ,n∈NS , z2 =
{λn}n∈NS and z = {z1, z2, z3, z4} indicates dual variables, where z1 = {z1
{z2
f,n}f ∈F ,n∈NS . Following the ADMM process, we
update the primal variables (x, y, Π, ˆΠ) by minimizing the augmented Lagrangian function and

f,n}f ∈F ,n∈NS , z3 = {z3

f,n}f ∈F ,n∈NS , z4 = {z4

perform gradient ascent on the dual problem to update {λ, z} in each iteration. In the t + 1

iteration, the update steps as follows.

• Update the introduced variables {y, ˆΠ}. Based on the previous iteration t, {x(t), z(t), Π(t)}
have been updated. Update {y, ˆΠ} at the iteration t + 1 by solving the following problem.

{y(t), ˆΠ(t + 1)} ← arg min
y, ˆΠ

Lρ

(cid:16)

x(t), y, λ(t), z(t), Π(t), ˆΠ

(cid:17)

(33)

• Update the global variables {x, Π}. Given {y(t + 1), ˆΠ(t + 1)} obtained by solving the

above problem, {x, Π} is updated rely on the solution of the following problem.

{x(t + 1), Π(t + 1)} ← arg min
x,Π

Lρ

(cid:16)
x, y(t + 1), λ(t), z(t), Π, ˆΠ(t + 1)

(cid:17)

(34)

• Update the dual variables {λ, z}. Depending on {x(t + 1), y(t + 1), Π(t + 1), ˆΠ(t + 1)},

the dual variables {λ, z} are updated as follow.

λn(t + 1) ← λn(t) + (cid:0)x(t + 1) − yn(t + 1)(cid:1), n ∈ N ,

z1(t + 1) ← z1(t) +

(cid:16) ˆR3(t + 1) − R3(t + 1)

(cid:17)

,

z2(t + 1) ← z2(t) +

(cid:16) ˆR6(t + 1) − R6(t + 1)

(cid:17)

,

(35a)

(35b)

(35c)

DRAFT

February 17, 2020

z3(t + 1) ← z3(t) +

(cid:16) ˆH3(t + 1) − H3(t + 1)

(cid:17)

,

z4(t + 1) ← z4(t) +

(cid:16) ˆH6(t + 1) − H6(t + 1)

(cid:17)

,

18

(35d)

(35e)

Through a series of iterations, the sequence (x, y, Π, ˆΠ) converges to a stationary point, and

we take the stable point as the solution to the problem based on the below Lemma.

Lemma 3. For sufﬁciently large ρ, the sequence (xt, yt, Πt, ˆΠt) generated by ADMM algorithm

converges to a limit points and all of its limit points are stationary points of the augmented

Lagrangian Lρ.

Proof. Proof can be seen in Appendix B.

Next, we give the detail for the update.

1) The solution of the update problem (33)
The problem (33) for updating the introduced variables {y, ˆΠ} can be rewritten into:

P2.2.1 : min
y, ˆΠ

ρ0
2

+

(cid:88)

(cid:107)x(t) − yn + λn(t)(cid:107)2

2 +

n∈N
ρ3
2

(cid:107) ˆH3 − H3(t) + z3(t)(cid:107)2

2 +

ρ1
2

(cid:107) ˆR3 − R3(t) + z1(t)(cid:107)2

2 +

ρ2
2

(cid:107) ˆR6 − R6(t) + z2(t)(cid:107)2
2

ρ4
2

(cid:107) ˆH6 − H6(t) + z4(t)(cid:107)2
2

s.t.

(31a) , (31b) , (31c) , (31d) .

We can see that the above optimization problem can be decomposed into NsF independent

subproblems which are correspond to a request state n ∈ NS for a computation task f ,

P2.2.2 :

min
k=1, ˆΠf,n

{yn

k,f }K

ρ0
2

+

+

(cid:88)

(cid:88)

k∈K

n∈NS
ρ2
2
ρ4
2

(cid:16) ˆR6
(cid:16) ˆH 6

(cid:0)xk,f (t) − yn

k,f + λn

k,f (t)(cid:1)2 +

(cid:16) ˆR3

f,n − R3

f,n(t) + z1

f,n(t)

(cid:17)2

ρ1
2
(cid:16) ˆH 3

f,n − H 3

f,n(t) + z3

f,n(t)

(cid:17)2

f,n − R6

f,n(t) + z2

f − H 6

f,n(t) + z4

f,n(t)

(cid:17)2

+

ρ3
2

f,n(t)
(cid:17)2

s.t.

(31a) , (31b) , (31c) , (31d) ,

where we use ˆΠf,n = { ˆR3
f,n}. Obviously, the objective function and the con-
straints are convex, and the convex programming can be solved efﬁciently using standard op-

f,n, ˆH 3

f,n, ˆH 6

f,n, ˆR6

timization toolbox, e.g., CVX. These subproblems can be handled in a parallel fashion at

different computation units of a centralized controller without effecting the others for saving

the computation time.

2) The solution of the update problem (34)

February 17, 2020

DRAFT

The update problem of the local variables {x, Π} can be reformulated as

19

P2.2.3 : min
x,Π

1
Ns

(cid:88)

(cid:88)

(cid:0)R3

f,n × H 3

f,n + R6

f,n × H 6
f,n

(cid:1) + β

(cid:88)

(cid:88)

xk,f (1 − xk,f )

f ∈F

f ∈F

k∈K

n∈NS
(cid:88)

+

+

+

ρ0
2

ρ2
2
ρ4
2

(cid:107)x − y(t + 1) + λ(t)(cid:107)2

2 +

n∈NS
(cid:107) ˆR6(t + 1) − R6 + z2(t)(cid:107)2

2 +

(cid:107) ˆH6(t + 1) − H6 + z4(t)(cid:107)2
2

ρ1
2

(cid:107) ˆR3(t + 1) − R3 + z1(t)(cid:107)2
2

ρ3
2

(cid:107) ˆH3(t + 1) − H3 + z3(t)(cid:107)2
2

s.t.

(6) ,

0 (cid:54) x (cid:54) 1.

For reducing computation complexity, P2.2.3 can also be decomposed into two independent

subproblems due to the uncoupled variables x and Π. For the computation variable x, the

subproblem can be decomposed into K independent subproblems which can be solved parallel.

The subproblem for a mobile device k can be written as:
(cid:88)

(cid:88)

(cid:88)

β

xk,f (1 − xk,f ) +

P2.2.3.1 : min
{xk,f }F

ρ0
2

f =1

f ∈F

n∈NS

f ∈F

(cid:0)xk,f − yn

k,f (t + 1) + λn

k,f (t)(cid:1)2

s.t.

(6) ,

0 (cid:54) xk,f (cid:54) 1, ∀f ∈ F.

Similarly, the subproblem for the variables Π can be decomposed into NsF independent

problems, and for the sampled request state n, the subproblem at the computation task f is

P2.2.3.2 : min
{Πf,n}

1
Ns

+

+

f,n × H 3
(cid:16) ˆR6
(cid:16) ˆH 6

ρ2
2
ρ4
2

(cid:0)R3

f,n + R6

f,n × H 6
f,n

(cid:1) +

f,n + z1

f,n(t)

(cid:17)2

f,n(t + 1) − R6

f,n + z2

f,n(t)

f,n(t + 1) − H 3

f,n + z3

f,n(t)

(cid:17)2

f,n(t + 1) − R3
(cid:16) ˆH 3

ρ1
2
(cid:17)2

(cid:17)2

(cid:16) ˆR3

ρ3
2

+

.

f,n(t + 1) − H 6

f,n + z4

f,n(t)

It can be found that both P2.2.3.1 and P2.2.3.2 are the DC programming problems with the

DC objective function. Therefore, we can adopt successive convex approximation to get a local

optimal solution to overcome the difﬁculty. In the paper, we can solve the problem by using

the concave-convex procedure (CCCP) [38]. CCCP involves an iterative procedure to solve a

February 17, 2020

DRAFT

20

Algorithm 1 The corresponding CCCP for P2.2.3
Initialization: Decompose problem P2.2.3 into problem P2.2.3.1 and problem P2.2.3.2;

Initialize {xk(1), Πf,n(1)} = {xk(t), Πf,n(t)} and set the number of iteration i = 1, the

maximum number of iteration imax.

Output: {xk(t + 1), Πf,n(t + 1)}

repeat

1) Update {xk(i + 1), Πf,n(i + 1)} according to P2.2.3.3 and P2.2.3.4;

2) Set i = i + 1

until convergence of {xk, Πf,n} or i ≥ imax.

Algorithm 2 The ADMM-Based Method for P2
Initialization: Initialize the number of iteration t = 1, the maximum number of iteration tmax,

the error limit (cid:15) = 1, the penalty parameter β = 100, the dual variables λ and z are initialized

as 0.

Iteration:

repeat

1) Update {y, ˆΠ} based on the subproblems P2.2.2 for each computation task f at a

sampled request state n;

2) Update {x, Π} rely on the decomposed subproblem P2.2.3 solved by Algorithm 1;

3) Update {λ, z} according to (35a), (35b), (35c), (35d), (35e);

4) t = t + 1

until (cid:107)x(t + 1) − x(t)(cid:107) (cid:54) (cid:15) or t (cid:62) tmax.

sequence of convex subproblems. Speciﬁcally, in the i + 1 iteration, we replace the nonconvex

term R3

f,n × H 3

f,n + R6

f,n × H 6

f,n and xk,f (1 − xk,f ) by their ﬁrst-order Taylor expansion:

R3

f,n × H 3
(cid:2)H 3

f,n − H 3

f,n(i), R3

f,n − R3

f,n + R6

f,n × H 6

f,n = R3

f,n(i) × H 3

f,n(i) + R6

f,n(i) × H 6

f,n(i), H 6

f,n(i)(cid:3)T + (cid:2)R6
= R3

f,n(i) × H 3

f,n(i)(cid:3) · (cid:2)H 6
f,n × H 3

f,n + R3

f,n(i) − R3

f,n(i) × H 3

f,n(i)+

f,n(i) + (cid:2)R3
f,n(i), R6

f,n − H 6

f,n(i), H 3

f,n − R6

f,n(i)(cid:3) ·
f,n(i)(cid:3)T

R6

f,n(i) × H 6

f,n + R6

f,n × H 6

f,n(i) − R6

f,n(i) × H 6

f,n(i).

xk,f (1 − xk,f ) = xk,f − 2xk,f (i)xk,f + xk,f (i)xk,f (i).

February 17, 2020

(36)

(37)

DRAFT

For the i + 1 iteration in CCCP, P2.2.3.1 and P2.2.3.2 are transformed into the following

21

programming

P2.2.3.3 : min
{xk,f }F

f =1

(cid:88)

β

f ∈F

s.t.

(6) ,

(xk,f − 2xk,f (i)xk,f ) +

ρ0
2

(cid:88)

(cid:88)

n∈NS

f ∈F

0 (cid:54) xk,f (cid:54) 1, ∀f ∈ F.

(cid:0)xk,f − yn

k,f (t + 1) + λn

k,f (t)(cid:1)2

P2.2.3.4 : min
{Πf,n}

1
Ns

(cid:88)

f ∈F

(cid:0)R3

f,n(i) × H 3

f,n + R3

f,n × H 3

f,n(i) + R6

f,n(i) × H 6

f,n + R6

f,n × H 6

f,n(i)(cid:1)

+

+

ρ1
2

ρ3
2

(cid:88)

f ∈F

(cid:88)

f ∈F

(cid:16) ˆR3

f,n(t + 1) − R3

f,n + z1

f,n(t)

(cid:17)2

+

ρ2
2

(cid:88)

f ∈F

(cid:16) ˆR6

f,n(t + 1) − R6

f,n + z2

f,n(t)

(cid:17)2

(cid:16) ˆH 3(t + 1) − H 3

f,n + z3

f,n(t)

(cid:17)2

+

ρ4
2

(cid:88)

f ∈F

(cid:16) ˆH 6

f,n(t + 1) − H 6

f,n + z4

f,n(t)

(cid:17)2

.

P2.2.3.3 and P2.2.3.4 are convex problems and thus both can be solved efﬁciently by the

standard convex optimization toolbox. Then the near-optimal solution {xk(t + 1), Πf,n(t + 1)}

is obtained by iteratively solving P2.2.3.1 and P2.2.3.2 until it converges as shown in the above

Algorithm 1.

In summary, the effective computation scheme x∗ is obtained by leveraging the ADMM

algorithm to P2. Through successive iterations, we ﬁrstly minimize the augmented Lagrangian

function (32) over the introduced variables and decompose the optimization problem into smaller

subproblems, which is executed in parallel to improve the computation speed. Next, we update

the global variables based on the problem (33). Similarly, the problem is decomposed into several

subproblems for reducing computation time, and CCCP is utilized to obtain the solution for each

subproblem. Finally, the dual variables are updated based on (35a), (35b), (35c), (35d), (35e).

The method is summarized in Algorithm 2

B. Algorithm to Solve Problem P3

P3.1 is formulated based on d = 1, while P3.2 is expressed for d = 0. Both of the

two subproblems are nonsmooth and nonconvex 0-1 programming because of (8), (14). It is

unreasonable to relaxing the binary constraints to continuous constraints which approximates

the nonsmooth 0-1 programming with a smooth one as Lemma 1 due to the special structure of

bq (c, x) in Proposition 1. Obviously, the computational complexity produced by traversing each

feasible solution can reach O(2F ) which is pretty high especially in the large scale programming.

February 17, 2020

DRAFT

Therefore, we propose an algorithm to obtain the acceptable solution of problem P3.1 and P3.2

as shown in the following Algorithm 3.

Algorithm 3 The Proposed Algorithm for P3.1, P3.2
Input: The sampling set of request state NS; The computation scheme x∗; The set of positive

22

computational task ˆF (∀f ∈ F, for P3.1, ∃k ∈ K, x∗

k,f = 1; for P3.2, ∃k ∈ K, x∗

k,f = 0);

CK

(cid:80)

n∈NS

k,f sn

nummaxI (cid:99) and t0 = (cid:98)

parameter t0(for P3.1,t0 = (cid:98)
(cid:80)
k∈K x∗

Initialization: The initial number of cached computation task nummax = | ˆF|; The initial
nummaxO (cid:99)otherwise); The set ˆF is sort by
(cid:80)
k,f is
f )f ∈F ,
f = 0 otherwise; The corresponding average bandwidth B∗ based on

k,f in descending order based on P3.1, and (cid:80)
k,f )sn
the standard of the descending order according to P3.2; The initial cache scheme c∗ (cid:44) (c∗
f = 1, for f ∈ ˆF, and c∗
c∗
x∗ and c∗; The initial variable num = nummax − 1;
while num (cid:62) 0 do

k∈K(1 − x∗

q∈NS

CK

t = (cid:98) CK

numI (cid:99) for P3.1,or t = (cid:98) CK

numO (cid:99) for P3.2;

if t0 (cid:54)= t then

1) The ﬁrst num task in ˆF are decided to be cached, and the cache scheme is deﬁned

as c;

2) The corresponding average bandwidth B based on x∗ and c;

3) Compare the average bandwidth B and B∗;
if B (cid:54) B∗ then

B∗ = B and c∗ = c;

end if

t0 = t;

end if

num = num − 1;

end while

return B∗, c∗ of P3.1, P3.2.

The algorithm provides a new search method for problem P3.1, P3.2. The storage of each

MD is full exploited, that is the number of tasks coded cached is the maximum number of tasks

meet a certain t. What’s more, we choose the coded caching scheme according to the number of

tasks requested for each comparison in a iteration which maintains the global gain of the coded

caching scheme as well as reduces the computation complexity. After applying the algorithm

February 17, 2020

DRAFT

for solving problem P3.1 and P3.2 respectively, the cache scheme d∗ and c∗ is obtained by

23

comparing B∗.

C. Analysis of Algorithms Proposed

For obtaining the computation scheme x∗, the computation complexity of solving (P2.2.2) is
O(K) during each iteration in ADMM algorithm. The update problem of the variables {y, ˆΠ}

need to solve NsF subproblems, and the total computational complexity is O(NsKF ). Similarly,

the computational complexity of updating the variables {x, Π} can be expressed as O(K +NsF )

in a CCCP iteration. Supposed that the numbers of iterations required by the ADMM algorithm

and the CCCP algorithm are NADM M and NCCCP , respectively. Therefore, the computational

complexity of the proposed algorithm in the subsection is formulated as O(NADM M (NsKF +

NCCCP (K + NsF ))). On the other hand, the maximum computational complexity produced by

the proposed algorithm in the paper can be achieved O(2K), which is far less than traversing

each feasible solution for the coded caching decision c, d.

In a word, the computation scheme x and the coded cache scheme c, d are decoupled in

the paper, then the original problem P1 is decomposed into three subproblems. The suboptimal

solution x∗ is obtained through Algorithm 2 where we utility ADMM algorithm and the update

problem is decomposed into several subproblems for parallel computation in each iteration.

The computation complexity is reduced and the computation speed is increased. Moreover, we

propose Algorithm 3 to get the acceptable solution c∗ and d∗ for reducing the computation

complexity compared with the traditional algorithm.

V. SIMULATION

In this section, numerical results are provided to validate the effectiveness of the proposed

scheme. Without loss of generality, the input data size and the output data size of each task

are 3 Mbits and 6 Mbits in the simulation [7]. The probability of each MD requests the task

is identical and independent, as well as the samples Ns is set to be 1000. The average energy

Ek for each MD is uniformly assigned from the set [0, 150] J. The computation load wf of the

task f follows a uniform distribution in the range [5, 10] cycles per bit, and the latency is set to

be 20 ms. The channel hk is modelled as Rayleigh fading, i.e., hk ∼ CN (0, 1), and the average

signal-to-noise (SNR) is uniformly selected from the set [10, 20] dB for different mobile devices.

The constant α for computing the average energy is 10−24.

We compare the proposed scheme with the following three benchmarks:

February 17, 2020

DRAFT

24

Figure 3: Impact of cache capability capability.

Figure 4: Impact of computation capability.

Figure 5: Impact of the coded cache strategy

Figure 6: Impact of the proposed algorithm

• Local Coded Cache: The policy takes only the cache capability into consideration, that

is, all computation tasks are processed by the MEC server, and the entire output data of a

task or the coded output data is transmitted to MDs which only stores the output data of

the tasks.

• Local Computing: The computation capability of MDs is considered only in the case. The

tasks can be computed locally, or by the MEC server. Thus, the entire input data or the

entire output data of a task is delivered to MDs.

• Traditional Transmission: In the case, BS only multicasts the entire output data of a task

to MDs, without using the caching and computing capability of MDs.

Fig. 3 and Fig. 4 illustrate the impact of cache size and computing capability on the average

February 17, 2020

DRAFT

06121824Cache capability, C(Mbits)1.161.181.21.221.241.261.281.31.321.34Avearagy Bandwidth, B(GHz)Proposed SchemeTraditional TransmissionLocal Coded CacheLocal Computing2.533.54Computation Capability, g(GHz)0.80.911.11.21.31.4Avearagy Bandwidth, B(GHz)Proposed SchemeTraditional TransmissionLocal ComputingLocal Coded Cache055110165220275Cache Capability, C(Mbits)00.20.40.60.811.21.4Averagy Bandwidth, B(GHz)Proposed Scheme, g=4GHzUncoded Cache with Device Computing,g=4GHzProposed Scheme, g=2.5GHzUncoded Cache with Device Computing,g=2.5GHz481216Number of Mobile Devices, K150200250300350400450500550600650Avearagy Bandwidth, B(MHz)Proposed SchemeCCCP-ADMM SchemeTraditional Transmission25

bandwidth cost where there are K = 20 MDs and F = 100 computation tasks. Intuitively, observe

that the average bandwidth monotonously decreases with C and g. It shows the proposed scheme

achieves minimum bandwidth consumption over the baselines by making full use of the caching

and computing resources of the mobile devices. We also can see from Fig. 3 that even if the

mobile device does not have the caching ability, i.e., C = 0, the proposed scheme also can save

the bandwidth compared with the traditional transmission scheme by using the local computing.

This suggestion is also depicted in Fig. 4, e.g., the proposed scheme has a signiﬁcant performance

gain compared with the local coded cache scheme. For example, the required bandwidth can

reduce from 1.31 GHz (the local coded cache scheme) to 0.87 GHz (the proposed scheme) at

g = 3.5 GHz.

What’s more, we observe from Fig. 3 that the performance gap between the proposed scheme

and the local computing scheme is larger than that between the tradition transmission scheme and

the local coded cache scheme, which means the computing resource in MDs can help the system

to achieve more caching gain than that of without exploiting the computing resource. Similarly,

the cache resource of MDs can bring more gain by comparing the gap between the proposed

scheme and the local coded cache scheme with the gap between the tradition transmission scheme

and the local computing scheme Fig. 4.

Next, similar to [5], Fig. 5 evaluates the coded gain of the proposed scheme, compared with the

uncoded caching with device computing scheme which the entire input data or the entire output

data of a task is cached in a MD. The coded caching with device computing, i.e., the proposed

scheme of this paper, can bring signiﬁcant coded gain over the uncoded caching with device

computing scheme, e.g., reducing the bandwidth from 450 MHz to 190 MHz with C = 110

Mbits and g = 4 GHz.

Finally, Fig. 6 veriﬁes the effectiveness of the proposed algorithm by comparing with the

CCCP-ADMM algorithm used in [9]. We can observe that the proposed algorithm still achieves

good performance gains over the CCCP-ADMM algorithm, especially when the number of
mobile devices is large (e.g.,K (cid:62) 12). This is because the sufﬁcient conditions for ADMM to

converge on monotonic programs hold in our optimization problem, we can directly obtain the

stationary point from the ADMM algorithm, as shown in Lamma 3, while CCCP converges to

a local minimum in the CCCP-ADMM algorithm.

February 17, 2020

DRAFT

26

VI. CONCLUTION

In the paper, we have studied the problem of how to save the average transmission bandwidth

by exploiting the caching and computing resources of MDs in the MEC system. A coded caching

with device computing strategy is proposed to minimize the average bandwidth under the delay

of the computation tasks, the cache size and the average energy consumption of MDs. The

formulated problem is a large-scale mix integer nonconvex and nonsmooth programming when

the numbers of MDs and computation tasks get larger. Obviously, the programming in the paper

is difﬁcult to be solved and thus we have decoupled it into several subproblems which can be

solved separately in an efﬁcient way. The numerical results show that the coded cache with

device computing scheme signiﬁcantly outperforms the three state-of-the-art benchmarks.

APPENDIX A

PROOF OF PROPOSITION 1

Based on the computation scheme x and the cached scheme c, d, the integer t can be obtained.

Obviously, the computation task f is cached on all of the mobile devices when cf = 1, and the

task is not split if t = K where the integral tasks are cached and there is no data need to be

transmitted. Therefore we draw a conclusion that bq (c, x) = 0 when t = K.

If t < K, then all cached tasks are split into (cid:0)K

(cid:1) nonoverlapping subﬁles of equal size. The
total rate of coded multicast transmission bq (c, x) depends on the set of mobile devices request

t

coded multicast transmission Mq with |Mq| = M q. There is no mobile device that requests

the coded cached task when M q = 0, and the data size of coded multicast transmission is zero,

i.e., bq (c, x) = 0. For each subset V q ⊂ K of cardinality |V q| = t + 1, the coded multicast

transmission for a subset V q is

⊕mq∈V Wdmq ,V\{mq}.

(38)

The data size rate of the transmission is 1/(cid:0)K
(cid:1), mq
of the mobile device element mq
1 suppose that mq
includes the element mq

(cid:1). The number of subsets V q which contains one
1 ∈ Mq. Then, there are (cid:0)K−2
(cid:1) subsets V q that
2 ∈ Mq and mq
2. We can reduce the
rest from the above analogy that the total number of the subsets where each subset contents at
(cid:1). Thus the rate of coded multicast transmission is
least an element mq, mq ∈ Mq is (cid:80)M q

2 without mq

1 is (cid:0)K−1

1 (cid:54)= mq

t

t

t

(cid:0)K−i
t

i=1

bq (c, x) =

M q
(cid:88)

i=1

(cid:19)

(cid:18)K − i
t

(cid:18)K
/
t

(cid:19)
.

(39)

DRAFT

February 17, 2020

27

Obviously, each subset that satisﬁes V q ⊂ K : |V q| = t + 1 includes at least an element
(cid:1). Similarly, the rate

mq ∈ Mq when M q > K − t. Therefore, the number of the subsets is (cid:0) K
can be expressed as

t+1

bq (c, x) =

(cid:18) K
t + 1

(cid:19)

(cid:18)K
/
t

(cid:19)

=

K − t
1 + t

.

(40)

APPENDIX B

PROOF OF LEMMA 3

By introducing the variables {y, ˆΠ}, we know that {x, Π} and {y, ˆΠ} are decoupled in the

objective function. The objective function Ψ in problem P2.2 is coercive over the set, that is,
Ψ → ∞ if the variables {x, y, Π, ˆΠ} ∈ C and (cid:107){x, y, Π, ˆΠ}(cid:107) → ∞ where C is the feasible set

for those variables.

We can obtain that x − yn = 0, for all n ∈ NS. Based on [35], Ax + Bnyn = 0. Therefore,
the matrix A is IK×K and Bn = −IK×K. Im(A) is the set of all vectors in RK due to the
ranks of the matrix are K unit vectors vi (i = 1, 2, . . . , K), and Im(Bn) equals the set of all
vectors in RK. Finally, Im(A) = Im(Bn). Similarly, Π − ˆΠ = 0, and the matrices C is INs×Ns
and D = −INs×Ns. What’s more, Im(C) = Im(D), and those are both the set of all vectors
in RNs. The matrices A, Bn, C and D are full column rank, their null spaces are trivial and,
thus, the unique minimizer H for any ﬁxed {x, Π} and F that is the minimizer for those ﬁxed
{y, ˆΠ} reduce to linear operators are Lipschitz continuous map. The constraints are convex and

continuous in addition to the objective function. The objective function and the derivative of the

objective function are wrote as below,

h(x, Π) = h1(Π) + h2(x)
(cid:88)

(cid:88)

=

(cid:0)RI

f,n × H I

f,n + RO

f,n × H O
f,n

n∈N

f ∈F

(cid:1) + β

(cid:88)

(cid:88)

f ∈F

k∈K

xk,f (1 − xk,f ) ,

where the variable Π has been given, Π0 = {RI

∂h1(Π)
∂(RI)

= HI
0,

∂h1(Π)
∂(HI)

= RI
0,

0, RO
0 , HI
∂h1(Π)
∂(RO)

0, HO

0 }.

= HO
0 ,

∂h2(x)
∂(x)

= 1 − 2x.

∂h1(Π)
∂(HO)

= RO
0 ,

(41a)

(41b)

Note that the objective function is differentiable and the derivative of are globally Lipschitz

continuous to each variables with constant Π0, and -2. Therefore, the function h(x, Π) is Lipschitz

differentiable.

February 17, 2020

DRAFT

28

The indicator functions based on the constraints (6), (31a), (31b), (31c),(31d) are lower semi-

continuous since the feasible sets are convex and closed. Therefore, we can get the lemma 3
based on Theorem 1 in [35], that is, the sequence (xt, yt, Πt, ˆΠt, λt, zt) obtained by ADMM

algorithm has limit points and all of its limit points are stationary are stationary points of the

augmented Lagrangian Lρ for any sufﬁciently large ρ.

REFERENCES

[1] C. V. N. Index, “global mobile data trafﬁc forecast update, 2017–2022,” Cisco White paper, 2019.

[2] E. Bastug, M. Bennis, M. Medard, and M. Debbah, “Toward interconnected virtual reality: Opportunities, challenges, and

enablers,” IEEE Communications Magazine, vol. 55, no. 6, pp. 110–117, June 2017.

[3] H. Liu, Z. Chen, and L. Qian, “The three primary colors of mobile systems,” IEEE Commun. Mag., vol. 54, no. 9, pp.

15–21, Sep. 2016.

[4] G. S. Paschos, G. Iosiﬁdis, M. Tao, D. Towsley, and G. Caire, “The role of caching in future communication systems and

networks,” IEEE Journal on Selected Areas in Communications, vol. 36, no. 6, pp. 1111–1125, June 2018.

[5] M. A. Maddah-Ali and U. Niesen, “Fundamental limits of caching,” IEEE Transactions on Information Theory, vol. 60,

no. 5, pp. 2856–2867, May 2014.

[6] C. Yang, Y. Yao, Z. Chen, and B. Xia, “Analysis on cache-enabled wireless heterogeneous networks,” IEEE Trans. Wireless

Commun., vol. 15, no. 1, pp. 131–145, Jan 2016.

[7] Y. Sun, Z. Chen, M. Tao, and H. Liu, “Communications, caching, and computing for mobile virtual reality: Modeling and

tradeoff,” IEEE Transactions on Communications, vol. 67, no. 11, pp. 7573–7586, Nov 2019.

[8] X. Yang, Z. Chen, K. Li, Y. Sun, N. Liu, W. Xie, and Y. Zhao, “Communication-constrained mobile edge computing

systems for wireless virtual reality: Scheduling and tradeoff,” IEEE Access, vol. 6, pp. 16 665–16 677, 2018.

[9] Y. Sun, Z. Chen, M. Tao, and H. Liu, “Bandwidth gain from mobile edge computing and caching in wireless multicast

systems,” to appear in IEEE Trans. Wireless Commun., arxiv.org/abs/1901.09738, 2019.

[10] J. Zhang, X. Lin, and X. Wang, “Coded caching under arbitrary popularity distributions,” IEEE Transactions on Information

Theory, vol. 64, no. 1, pp. 349–366, Jan 2018.

[11] M. A. Maddah-Ali and U. Niesen, “Decentralized coded caching attains order-optimal memory-rate tradeoff,” IEEE/ACM

Transactions on Networking, vol. 23, no. 4, pp. 1029–1040, Aug 2015.

[12] T. Luo, V. Aggarwal, and B. Peleato, “Coded caching with distributed storage,” IEEE Transactions on Information Theory,

vol. 65, no. 12, pp. 7742–7755, Dec 2019.

[13] K. Li, C. Yang, Z. Chen, and M. Tao, “Optimization and analysis of probabilistic caching in n -tier heterogeneous networks,”

IEEE Trans. Wireless Commun., vol. 17, no. 2, pp. 1283–1297, Feb 2018.

[14] M. Ji, G. Caire, and A. F. Molisch, “The throughput-outage tradeoff of wireless one-hop caching networks,” IEEE

Transactions on Information Theory, vol. 61, no. 12, pp. 6833–6859, Dec 2015.

[15] T. D. Tran, T. D. Hoang, and L. B. Le, “Caching for heterogeneous small-cell networks with bandwidth allocation and

caching-aware bs association,” IEEE Wireless Communications Letters, vol. 8, no. 1, pp. 49–52, Feb 2019.

[16] B. P. Rimal, D. P. Van, and M. Maier, “Cloudlet enhanced ﬁber-wireless access networks for mobile-edge computing,”

IEEE Transactions on Wireless Communications, vol. 16, no. 6, pp. 3601–3618, 2017.

[17] C. You, K. Huang, H. Chae, and B. Kim, “Energy-efﬁcient resource allocation for mobile-edge computation ofﬂoading,”

IEEE Trans. Wireless Commun., vol. 16, no. 3, pp. 1397–1411, March 2017.

February 17, 2020

DRAFT

29

[18] C. You, K. Huang, and H. Chae, “Energy efﬁcient mobile cloud computing powered by wireless energy transfer,” IEEE

Journal on Selected Areas in Communications, vol. 34, no. 5, pp. 1757–1771, May 2016.

[19] Y. Liu, J. Liu, A. Argyriou, and S. Ci, “Mec-assisted panoramic vr video streaming over millimeter wave mobile networks,”

IEEE Transactions on Multimedia, vol. 21, no. 5, pp. 1302–1316, 2018.

[20] M. S. Elbamby, C. Perfecto, M. Bennis, and K. Doppler, “Edge computing meets millimeter-wave enabled vr: Paving the

way to cutting the cord,” in 2018 IEEE Wireless Communications and Networking Conference (WCNC), April 2018, pp.

1–6.

[21] J. Park, P. Popovski, and O. Simeone, “Minimizing latency to support vr social interactions over wireless cellular systems

via bandwidth allocation,” IEEE Wireless Communications Letters, vol. 7, no. 5, pp. 776–779, Oct 2018.

[22] X. Yang, Z. Fei, J. Zheng, N. Zhang, and A. Anpalagan, “Joint multi-user computation ofﬂoading and data caching for

hybrid mobile cloud/edge computing,” IEEE Transactions on Vehicular Technology, vol. 68, no. 11, pp. 11 018–11 030,

Nov 2019.

[23] A. P. Miettinen and J. K. Nurminen, “Energy efﬁciency of mobile clients in cloud computing,” in Usenix Conference on

Hot Topics in Cloud Computing, 2010.

[24] W. Yuan and K. Nahrstedt, “Energy-efﬁcient cpu scheduling for multimedia applications,” Acm Transactions on Computer

Systems, vol. 24, no. 3, pp. 292–331, 2006.

[25] T. D. Burd and R. W. Brodersen, “Processor design for portable systems,” Journal of VLSI signal processing

systems for signal,

image and video technology, vol. 13, no. 2, pp. 203–221, Aug 1996.

[Online]. Available:

https://doi.org/10.1007/BF01130406

[26] G. T. 36.213, “Evolved universal terrestrial radio access (e-utra) physical layer procedures (release 15),” version 15.2.0,

Oct. 2018.

[27] J. R. Birge and F. Louveaux, Introduction to stochastic programming. Springer Science & Business Media, 2011.

[28] B. Dai, Y. Liu, and W. Yu, “Optimized base-station cache allocation for cloud radio access network with multicast backhaul,”

IEEE Journal on Selected Areas in Communications, vol. 36, no. 8, pp. 1737–1750, Aug 2018.

[29] A. H. Land and A. G. Doig, “An automatic method of solving discrete programming problems,” Econometrica, vol. 28,

no. 3, pp. 497–520, 1960. [Online]. Available: http://www.jstor.org/stable/1910129

[30] J. E. Kelley, Jr, “The cutting-plane method for solving convex programs,” Journal of the society for Industrial and Applied

Mathematics, vol. 8, no. 4, pp. 703–712, 1960.

[31] M. De Santis, “Continuous approaches to mixed integer programming problems,” 2012.

[32] W. Murray and K.-M. Ng, “An algorithm for nonlinear optimization problems with binary variables,” Computational

Optimization and Applications, vol. 47, no. 2, pp. 257–288, 2010.

[33] S. Lucidi and F. Rinaldi, “Exact penalty functions for nonlinear integer programming problems,” Journal of optimization

theory and applications, vol. 145, no. 3, pp. 479–488, 2010.

[34] M. De Santis, “Continuous approaches to mixed integer programming problems,” 2012.

[35] Y. Wang, W. Yin, and J. Zeng, “Global convergence of admm in nonconvex nonsmooth optimization,” Journal of Scientiﬁc

Computing, vol. 78, no. 1, pp. 29–63, 2019.

[36] H. A. Le Thi, T. P. Dinh, and H. Van Ngai, “Exact penalty and error bounds in dc programming,” Journal of Global

Optimization, vol. 52, no. 3, pp. 509–535, 2012.

[37] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein et al., “Distributed optimization and statistical learning via the alternating

direction method of multipliers,” Foundations and Trends in Machine learning, vol. 3, no. 1, pp. 1–122, 2011.

[38] A. L. Yuille and A. Rangarajan, “The concave-convex procedure (cccp),” in Advances in neural information processing

systems, 2002, pp. 1033–1040.

February 17, 2020

DRAFT

