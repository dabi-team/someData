GraspLook: a VR-based Telemanipulation System with
R-CNN-driven Augmentation of Virtual Environment

Polina Ponomareva, Daria Trinitatova, Aleksey Fedoseev, Ivan Kalinov, and Dzmitry Tsetserukou

1
2
0
2

t
c
O
4
2

]

O
R
.
s
c
[

1
v
8
1
5
2
1
.
0
1
1
2
:
v
i
X
r
a

Abstract—The teleoperation of robotic systems in medical
applications requires stable and convenient visual feedback for
the operator. The most accessible approach to delivering visual
information from the remote area is using cameras to transmit
a video stream from the environment. However, such systems
are sensitive to the camera resolution, limited viewpoints, and
cluttered environment bringing additional mental demands to
the human operator. The paper proposes a novel system of
teleoperation based on an augmented virtual environment (VE).
The region-based convolutional neural network (R-CNN) is
applied to detect the laboratory instrument and estimate its
position in the remote environment to display further its digital
twin in the VE, which is necessary for dexterous telemanip-
ulation. The experimental results revealed that the developed
system allows users to operate the robot smoother, which leads
to a decrease in task execution time when manipulating test
tubes. In addition, the participants evaluated the developed
system as less mentally demanding (by 11%) and requiring less
effort (by 16%) to accomplish the task than the camera-based
teleoperation approach and highly assessed their performance in
the augmented VE. The proposed technology can be potentially
applied for conducting laboratory tests in remote areas when
operating with infectious and poisonous reagents.

I. INTRODUCTION

The use of telemanipulation in medicine [1] and Industry
4.0 [2] has been the subject of interest for many research
groups over the past decade due to its high impact on the
precision and safety of critical operations, e.g., inspection,
assembling, surgery, manipulation with dangerous reagents,
etc. These operations, however, are often linked to dynamic
environment changes, which require providing constant vi-
sual feedback to the operator. Visual information from the
environment is the most natural for the operator since it does
not require any training and additional equipment aside from
the regular camera installation. A lot of research works have
been done to implement visual feedback from RGB cameras
for such purposes. Acemoglu et al. [3] proposed a telesurgery
robotic system, where visual feedback is obtained from the
cameras with low latency due to the 5G implementation.
Zakharkin et al. [4] proposed a teleoperation system, which
utilizes a Zoom application camera not only as a tool of
feedback but also as an active control interface. However,
teleoperation through visual feedback limited to the camera
view only shows high efﬁciency when the robot is located at
the center of the image without any cluttered environment.

The authors are with the Space Center, Skolkovo Institute of
121205 Bolshoy Boulevard
{polina.ponomareva,

Science
30,
daria.trinitatova, aleksey.fedoseev,
i.kalinov, d.tsetserukou}@skoltech.ru

and Technology
1, Moscow,

(Skoltech),
Russia.

bld.

(a) Operator with HMD during teleoperation process.

(b) Isometric view of
scene.

the virtual

(c) Robotic arm during teleoperation
process.

Fig. 1. GraspLook telemanipulation system.

The integration of VE into teleoperation control loop is
considered as a way to increase intuitiveness and awareness
of human-robot
interaction (HRI) with augmented visual
feedback to the operator [5]. Kalinov et al. [6] developed a
virtual reality (VR) interface for supervision of autonomous
robotic system aimed at warehouse stocktaking, which en-
hanced the operator’s ability via a preliminary simulation of
the task in a virtual scene.

In the research works of Lima et al. [7], the kinesthetic
feedback from Geomagic Touch X haptic device was com-
bined with multiple visual channels from the real robot and
its real-time simulation. Lipton et al. [8] introduced a virtual
reality-based teleoperation interface for a bimanual robot,
propagating view from several cameras into one virtual scene.
Ni et al. [9] developed an interface for programming welding
robots using desktop haptic device and augmented reality.

Among the implementations of an augmented visual en-
vironment, the 3D reconstruction shows high potential. It
allows the operator to interact with both pre-deﬁned objects
of the deterministic environment and objects, which location
and properties might change during the teleoperation sce-

 
 
 
 
 
 
nario. Denninger et al. [10] introduced a novel approach to
infer volumetric reconstructions from a single viewport, with
occluded region reconstruction by synthetically trained deep
neural network. Omarali et al. [11] proposed a VR-based
teleoperation system with dynamic ﬁeld of view to control
the robot. As a visual channel, the Intel RealSense d415i and
Kinect2 depth cameras were for efﬁcient and accurate capture
of objects and displaying them in the virtual scene. Kohn et
al. [12] proposed a real-time teleoperation system using depth
cameras to reconstruct the environment of the robot in VR.
For the reconstruction of objects in VR, object segmentation
based on point cloud ﬁltering algorithms was used. The most
effective neural model in recent years was proposed by Jeong
et al. [13]. It was applied to determine the position and
distance to an object using a stereo camera for providing
these data to capture the object by the robot. Sumigray et al.
[14] proposed a teleoperation system that uses VR with the
reconstruction of the robot real environment using a multi-
camera system. The multi-camera system comprised the Insta
360 Pro 2 camera and depth cameras located on the sides of
the robot. The developed Matryoshka algorithm was used to
determine the location of objects in the scene in real-time.
The operator could control the position and orientation of
the end effector operation using VR joysticks, using visual
feedback in the Unity VR application.

In this work, we propose a novel telemanipulation system
for the robot through VR. Objects for manipulation detected
using a developed computer vision (CV) system are displayed
in the VE in real-time, allowing the operator to perform
precise grasping.

II. SYSTEM OVERVIEW

The overall scheme of the developed system is presented in
Fig. 2. The proposed system consists of a robotic manipulator
UR3 with 6 degrees of freedom equipped with a 2-ﬁnger
gripper (Robotiq 2F-85), a digital 8-megapixel camera with
a 75-degree ﬁeld of view, and Intel RealSense D435 RGB-
D camera on the end effector. The operator controls the
robot using the Omega.7 desktop haptic device. As a visual
feedback channel for the operator, a virtual environment is
created with the Unity Engine. It includes a digital twin of
the robot, a real-time video stream from the 8 MP camera
located on the end effector, and a digital twin of a laboratory
instrument which position is estimated with a developed CV
system using Intel RealSense depth camera.

A. Control system of the robot

Positional control of the robot movement is carried out by
moving the handle of the haptic device. Three translational
axes of the robot EE correspond to three translational axes
of the haptic device, while the EE rotations are ﬁxed. The
workspace of the haptic device (R = 160 mm) and the
robot (R = 500 mm) differs from each other. Therefore, the
developed control framework performs the workspace scaling
from haptic device to robot by either x1, x2, x3, x4, or
x5 scale factor. To increase the accuracy and speed of the

Fig. 2.
The overall architecture of the developed system. The operator
controls the collaborative robot using Omega.7 haptic device. The object for
manipulation is reconstructed by a developed R-CNN-based system using
Intel RealSense depth camera.

robot manipulation, the movement of robot EE along one or
two translational axes could be locked. The control signal of
three positional coordinates for the EE of the robot and the
gripper state command are generated from the haptic device
and transmitted to the control framework via sockets, thus
minimizing data transmission delays. Since both the working
area and the coordinate system of the haptic device and the
robotic arm are different, the transformation and scaling of
the input coordinates are performed at the ﬁrst calculating
stage. After that, the digital twin solves the inverse kinematics
task in the Cartesian coordinate space of Unity3D. Since the
UR robots are designed with a non-spherical joint conﬁgu-
ration, a closed-form kinematic solution is chosen for real-
time control with the eight possible joint conﬁgurations for
one EE position. The solution is selected with the weighted
Least Squares method to minimize the difference with the
previous position of the robot and perform a smooth shift
across all its joints.

B. Digital twin

The approaches using a video stream from a remote site to
obtain visual feedback during teleoperation have a number of
limitations. For example, the obtained visual channel could
suffer from optical distortion and camera occlusion, prevent-
ing the operator’s access to the overall state of the system.
To eliminate these disadvantages, we propose to use a digital
twin of the robot and augment the virtual environment with
3d models of objects for manipulation in real-time mode. The
digital twin and communication interface of the UR3 robot
have been designed for the virtual environment in the Unity
Engine [15]. The digital twin provides information about the
position of the real robot with a frequency of 50 Hz. The key
feature of the VR interface is that it allows to represent the
current state of the robot from different perspectives without
any complex camera setup (Fig. 1(b)). Visual information
about the gripper state supports the control of the object
manipulation.

C. Virtual environment augmentation

To achieve user awareness of the manipulation process,
we propose to update the state of the robot and operated
object with the real-time data of digital twin and CNN-based

object pose estimation. The instance segmentation of the
laboratory instruments was accomplished by using trained
R-CNN on a collected dataset. We use color images from
RGB-D RealSense camera, which is attached to the robot
EE for object detection. We have two approaches to extract
the coordinates of the objects using the depth map related to
the color frame: from the bounding box where the object is
detected and from the predicted segmented mask. Depth map
pixels for the bounding box are ﬁltered to prevent the robot
from exceeding the workspace boundaries. At this work, we
assume that all objects are placed vertically. Therefore, it
is possible to compute the average distance to the object
from the ﬁltered bounding box and segmented mask. The
coordinates of the object center are calculated from the
obtained point cloud. The laboratory equipment models were
pre-designed in CAD software and exported to the Unity
prefabs. Thus, once the coordinates of the object center are
obtained in the RealSense coordinate system and transformed
into the digital twin coordinate system, the object is displayed
in the VE.

III. R-CNN-BASED APPROACH FOR VIRTUAL
ENVIRONMENT AUGMENTATION

A. Dataset Collection

To reconstruct laboratory equipment in the VE, it is nec-
essary to teach the system to recognize various objects. We
created a synthetic dataset consisting of eight instruments that
are most commonly used in the laboratory: scrapper, micro
test tube, needle holder, Pasteur pipette, pipettor, centrifuge
test tube, vacuum test tube, and swab (Fig. 3).

Fig. 3. Object classes of laboratory instruments in the dataset: a) cell
scrapper, b) micro test tube, c) needle holder, d) Pasteur pipette, e) pipettor,
f) centrifuge test tube, g) vacuum test tube, h) swab.

Creating a dataset consisted of 3 main stages: collecting
images of objects for each class, superimposing an image of
the class on the background image, and annotating the image
for each superimposed object. To collect images for each
class, we found 30 images per class on the Internet and then
deleted the background for each image. Hereafter, the images
of the objects were placed on a transparent background,
where the alpha channel was a mask of the object. To create
a single image for the dataset, we used one random image
from each class, overlaying them randomly in a random place
on the background image.

When overlaying object images, we used additional aug-
mentations: rotation, resizing, and reﬂection of objects. To
annotate an image in accordance with the Common Objects

(a) Superimposed image.

(b) Segmented object masks for the
corresponding image.

Fig. 4. Sample image from the synthetic dataset.

in Context (COCO) format [16], it is necessary to extract the
coordinates of the mask borders for each object. Thus, at the
stage of forming a set of image data, the original masks of
objects are determined (Fig. 4). After superimposing them
on the general background, it is sufﬁcient to subtract the
masks from each other in case of overlap, depending on the
order of objects superimposed on the background image. To
create the dataset, we used ten different background images
with medical laboratories, which were randomly selected to
generate the image of the dataset. As the result, the dataset
consisting of 8000 images was created, where 6000 images
were used for training and 2000 images for testing (every
image contains eight object instances, one for each object
class).

B. Mask R-CNN training

We used the state-of-the-art neural network architecture
Mask R-CNN with the backbone ResNet-101-FPN [17] in
our system. We trained this model on Google Colab resource
for 10k iterations with a learning rate of 0.002, and four
images per batch. The evaluation of the model is reported
in the standard pixel COCO metrics: AP (average precision
at IoU threshold), AP50, AP75, and APM (AP at different
scales). It is worth noting that this class of metrics involves
pixel-by-pixel comparison and gives not entirely accurate
data on the number of objects. Therefore, we used the
second class of metrics with quantitative measures: correctly
recognized objects (True positive - TP) with intersection
more or equal to 50% of source object, mistakenly detected
(False Positive - FP) with intersection less than 50%, and
undetected (False Negative - FN). The best results for all
metrics were obtained for the centrifuge test tube (highlighted
in Table I). Therefore, this object was chosen as the baseline
for the subsequent experiment in this paper.

TABLE I
EVALUATION RESULTS FOR TRAINED MODEL ON COLLECTED DATASET
PER OBJECT CLASS

Object class

Cell scraper
Micro test tube
Needle holder
Pasteur pipette
Pipettor
Centrifuge test tube
Vacuum test tube
PCR tupfer

Pixel metrics,
%

AP
75.4
87.4
86.8
76.3
81.4
88.2
85.5
77.0

AP50
87.3
97.2
96.9
88.1
93.6
97.5
94.8
86.3

AP75
79.5
92.1
90.2
84.9
88.4
93.2
89.1
82.1

APM
83.5
92.7
91.1
83.7
89.5
93.8
89.9
84.6

Object metrics,
num out of 2000
FN
FP
TP
216
35
1784
73
23
1927
98
21
1902
199
34
1801
153
28
1847
61
17
1939
107
29
1893
192
38
1808

C. R-CNN-based Object Recognition for Virtual Scene Aug-
mentation

After the trained neural network has shown sufﬁciently
high performance in determining the bounding box and
segmentation mask on the test images, we implemented an
algorithm to estimate the positions of real objects.

The RealSense camera produces two kinds of frames, a
color one and a depth map. We used the color frame as
input for the neural network to determine the bounding box
and segmentation mask of the object, while, from the depth
map, we got the distance to the object. In our system, we
compare two ways of getting the distance to the object:
the average value within the bounds of the object bounding
boxes or the average value for the segmentation masks. Then
the coordinates of the object are calculated using the built-
in functions of the PyRealSense framework by projecting
points of the object onto the plane and calculating coordinates
along the axes. The object center’s reconstruction point is the
center of the obtained bounding box and segmentation mask.
Thus, the class of a certain object, the center of its bounding
box and segmentation mask, and the average distance to the
object by the bounding box and the segmentation mask are
transmitted to VE as information about the object. For the
better performance of the object position estimation in the
VE, we use an alpha-ﬁlter to soft any ﬂuctuation of the object
position.

IV. USER STUDY

robot

We conducted an experiment of

teleoperation
through two control modes, namely camera-based and VR-
based teleoperation, to evaluate the convenience of using the
GraspLook system and user ability to manipulate objects in
the virtual environment. The ﬁrst mode provided the operator
with visual feedback from two cameras from the remote
environment (on-gripper camera and isometric view camera).
The VR environment provided a 360◦ view of the workspace
by the user’s head rotation. The performance of each task was
characterized by execution time and the accuracy of the target
object grasping. In addition, after completing the tasks using
both teleoperation modes, we asked participants to respond
to a 7-question survey using a seven-point Likert scale. The
questionnaire was based on the NASA Task Load Index [18].
Besides, we added one additional question about the level of
user involvement in the task.

A. Participants

The experiment involved 8 subjects (5 males and 3 fe-
males). The average participant age was 24.2 (SD=1.5), with
a range of 22–27. In total, three participants had never oper-
ated collaborative robots, three participants operated robotic
arms several times, and two reported regular experience with
collaborative robots. Six participants used VR only a few
times, and two people answered that they used VR devices
regularly.

Fig. 5. Experimental setup for the experiment.

B. Experimental setup

The ﬁrst approach for teleoperation included only visual
feedback from two cameras. The former (Logitech C930e)
was used to display the teleoperation area with an isometric
view of the robot and the object for the manipulation (test
tube). The latter was mounted on the gripper (8 MP USB
camera). For the ﬁrst control mode, the operator was sepa-
rated from the robot by an insulating board to exclude the
view of the working area. The second system utilized the
described in chapter II-B augmented visual feedback to the
operator through a VE in the Unity Engine. The VR setup
included HTC Vive Pro base stations, a head-mounted display
(HMD), and a Vive controller to regulate the operator’s point
of view in VR. The virtual environment utilizes additional
visual feedback from the remote environment through a video
stream from the on-gripper camera. The experimental setup
is shown in Fig. 5.

Fig. 6. The target object with a marked position for grasping.

C. Experimental procedure

The experimental task was to manipulate a test tube in a
pick-and-place task. Each participant had three attempts for
task completion. The participants were asked to grasp the
target object at a marked position (Fig. 6). To improve the
accuracy of target object pose estimation for VE augmenta-
tion, we developed an additional CV algorithm to determine
the upper and lower parts of the target object by explicitly
setting the color to these parts. Together with the bounding
box deﬁnition, the color deﬁnition ensured that the object was
recognized completely or with one of two possible halves

TABLE II
COMPARISON OF TRAJECTORY LENGTH, TIME OF OPERATION AND ERROR AT GRASPING, AND MEAN NUMBER OF ATTEMPTS FOR 2
TELEOPERATION APPROACHES

Teleoperation mode

Camera-based
VR-based

Trajectory length, m

min max mean
0.56
1.82
0.16
0.43
1.20
0.15

sd
0.52
0.35

Task execution time, s

Error at grasping, mm

min
48.2
40.25

max
193.23
147.55

mean
127.95
87.89

sd
50.15
36.9

min max mean
15.9
29.5
7.4
14.1
22.3
6.2

sd
7.6
5.8

Attempts

1.4
1.4

(top or bottom). Since the distance to the center of the target
object is known from each color markings, it reduced the
error of determining the center of the object along the vertical
axis.

At the beginning of the experiment, the EE of the robot
was located at a distance of 47 cm from the target object,
and the camera was at the level of the object center. As the
operator pressed the start button, the robot moved according
to the manipulations with the Omega.7 haptic device. For
each participant, we measured the following quantitative data
for each control mode: task execution time, the accuracy of
the grasping, measured as the error in the distance from
the target point of grasping the object to the real point of
grasping, the trajectory length of the robot, and the number
of attempts for task completion.

D. Experimental results

After conducting the teleoperation experiment

through
both control modes, we compared the average trajectory
length for each approach, the task execution time, and the ac-
curacy of the object grasping (Table II). Since the experiment
consisted of grasping the object and placing it in any hole of
the tube rack, the length of the robotic gripper trajectory
the task was started to
was measured from the moment
the moment of grasping the object. The experiment results
showed that the average length of the trajectory that the robot
passes before the object was the lowest for remote control of
the robot in the VR mode (0.43 m), which is also reﬂected in
the average task execution time (87.89 s for the VR mode).
Thus, developed VR interface allows operator to reduce the
length of the robot trajectory to the target object by 23.2%
and reduce task completion time by 31.3% compared to the
camera-based teleoperation approach. In terms of accuracy
of grasping, the VR-based system also showed a minimum
average error of 14.1 mm when grasping the object.

Fig. 7 represents the example of the trajectories of EE
moving from the initial position of the robot to the object
grasping position for two participants. The experiment re-
sults for the trajectory length showed that for the camera-
based control mode, the operator has to perform unnecessary
movements since two views of the teleoperation zone are
not enough. The trajectory of the robot movement for VR
control mode has a more directional movement towards the
target. An additional comparison of the robot trajectories is
given in Table III. It shows a comparison of the trajectory
lengths before grasping the object with the ideal trajectory
of movement, which is a straight line between the initial
point of the robotic gripper and the end point (the center of
the grasped object). During the VR control mode, the robot

Fig. 7. Examples of the EE trajectories for 2 control modes. The blue dot
represents the start position of the end effector and the green point is the
position of the target object.

TABLE III
COMPARISON OF TRAJECTORY LENGTH FOR TELEOPERATION
APPROACHES REGARDING IDEAL TRAJECTORY

Trajectory type

Mean length, m

Ideal
Camera-based teleoperation
VR-based teleoperation

0.15
0.56
0.43

Increment regarding
ideal, %
0%
273%
186%

trajectory increased by 186% on average compared with the
ideal trajectory for all participants.

Since participants have only three attempts to complete
the task in the experiment, we compared the number of at-
tempts with which participants coped with the pick-and-place
task.For both control modes, the mean number of attempts for
task completion comprised 1.4 (Table II). During a camera-
based teleoperation, most participants could complete the
task on the ﬁrst trial. However, one participant took three
attempts. During teleoperation using VR interface, most of
the participants coped with the task on the ﬁrst attempt, while
the rest, getting used to the system, completed the task on
the second attempt. For this system, no one had to use the
third attempt.

Fig. 8 shows the results of the user assessment of two con-
trol modes across all participants. According to the obtained
results, VR-based interface increases involvement (by 11%)
and performance (by 10%), whereas the frustration level
during task completion and physical demand of participants
were almost the same for both control modes. At the same
time, participants noted that it is easier to control the system
through the VR application. The teleoperation through VR
interface did not require special mental costs and a lot of
effort on the user’s part, while the success of the completed
task was felt better.

REFERENCES

[1] S. Mehrdad, F. Liu, M. T. Pham, A. Lelev´e, and S. F. Atashzar,
“Review of advanced medical telerobots,” Applied Sciences, vol. 11,
no. 1, p. 209, 2021.

[2] Z. Gao, T. Wanyama, I. Singh, A. Gadhrri, and R. Schmidt, “From
industry 4.0 to robotics 4.0-a conceptual framework for collaborative
and intelligent robotic systems,” Procedia Manufacturing, vol. 46, pp.
591–599, 2020.

[3] A. Acemoglu, J. Krieglstein, D. G. Caldwell, F. Mora, L. Guastini,
M. Trimarchi, A. Vinciguerra, A. L. C. Carobbio, J. Hysenbelli,
M. Delsanto et al., “5G Robotic Telesurgery: Remote Transoral Laser
Microsurgeries on a Cadaver,” IEEE Transactions on Medical Robotics
and Bionics, vol. 2, no. 4, pp. 511–518, 2020.

[4] I. Zakharkin, A. Tsaturyan, M. A. Cabrera, J. Tirado, and D. Tset-
serukou, “ZoomTouch: Multi-User Remote Robot Control in Zoom by
DNN-based Gesture Recognition,” in SIGGRAPH Asia 2020 Emerging
Technologies, 2020, pp. 1–2.

[5] M. Wonsick and T. Padir, “A Systematic Review of Virtual Reality
Interfaces for Controlling and Interacting with Robots,” Applied Sci-
ences, vol. 10, no. 24, p. 9051, 2020.

[6] I. Kalinov, D. Trinitatova, and D. Tsetserukou, “WareVR: Virtual Re-
ality Interface for Supervision of Autonomous Robotic System Aimed
at Warehouse Stocktaking,” in 2021 IEEE International Conference on
Systems, Man, and Cybernetics (SMC).
IEEE, 2021, pp. 2131–2137.
[7] A. T. Lima, F. A. S. Rocha, M. P. Torre, H. Azp´urua, and G. M.
Freitas, “Teleoperation of an ABB IRB 120 Robotic Manipulator and
Barretthand BH8-282 Using a Geomagic Touch X Haptic Device and
ROS,” in 2018 Latin American Robotic Symposium, 2018 Brazilian
Symposium on Robotics (SBR) and 2018 Workshop on Robotics in
Education (WRE).
IEEE, 2018, pp. 188–193.
[8] J. I. Lipton, A. J. Fay, and D. Rus, “Baxter’s homunculus: Virtual
reality spaces for teleoperation in manufacturing,” IEEE Robotics and
Automation Letters, vol. 3, no. 1, pp. 179–186, 2018.

[9] D. Ni, A. Yew, S. Ong, and A. Nee, “Haptic and visual augmented
reality interface for programming welding robots,” Advances in Man-
ufacturing, vol. 5, no. 3, pp. 191–198, 2017.

[10] M. Denninger and R. Triebel, “3D Scene Reconstruction from a Single
Springer,

Viewport,” in European Conference on Computer Vision.
2020, pp. 51–67.

[11] B. Omarali, B. Denoun, K. Althoefer, L. Jamone, M. Valle, and
I. Farkhatdinov, “Virtual Reality based Telerobotics Framework with
Depth Cameras,” in 2020 29th IEEE International Conference on
Robot and Human Interactive Communication (RO-MAN).
IEEE,
2020, pp. 1217–1222.

[12] S. Kohn, A. Blank, D. Puljiz, L. Zenkel, O. Bieber, B. Hein, and
J. Franke, “Towards a Real-Time Environment Reconstruction for VR-
Based Teleoperation Through Model Segmentation,” in 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
2018, pp. 1–9.

[13] D.-K. Jeong, H.-S. Kang, D.-E. Kim, and J.-M. Lee, “Mask-RCNN
based object segmentation and distance measurement for robot grasp-
ing,” in 2019 19th International Conference on Control, Automation
and Systems (ICCAS).

IEEE, 2019, pp. 671–674.

[14] A. Sumigray, E. Laidlaw, J. Tompkin, and S. Tellex, “Improving
Remote Environment Visualization through 360 6DoF Multi-sensor
Fusion for VR Telerobotics,” in Companion of the 2021 ACM/IEEE
International Conference on Human-Robot Interaction, 2021, pp. 387–
391.

[15] A. Fedoseev, N. Chernyadev, and D. Tsetserukou, “Development of
Mirrorshape: High Fidelity Large-Scale Shape Rendering Framework
for Virtual Reality,” in 25th ACM Symposium on Virtual Reality
Software and Technology, 2019, pp. 1–2.

[16] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft COCO: Common Objects in
Context,” in European conference on computer vision. Springer, 2014,
pp. 740–755.

[17] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask R-CNN,” in
Proceedings of the IEEE international conference on computer vision,
2017, pp. 2961–2969.

[18] S. G. Hart and L. E. Staveland, “Development of NASA-TLX (Task
Load Index): Results of empirical and theoretical research,” in Ad-
vances in psychology. Elsevier, 1988, vol. 52, pp. 139–183.

Fig. 8. Evaluation of the participant’s experience for both control methods
in the form of a 7-point Likert scale (1 = Low, 7 = High). M1: camera-based
teleoperation; M2: VR-based teleoperation. Crosses mark mean values.

V. CONCLUSIONS AND FUTURE WORK

We have proposed a novel telemanipulation system with
R-CNN-based object recognition for virtual scene augmenta-
tion. Objects for manipulation detected using the developed
CV system are displayed in the VE in real time, allowing the
operator to perform precise manipulations. The results of the
user study revealed that the teleoperation through developed
VR interface allowed users to operate the robot smoother,
resulting in a shorter trajectory of the robot to the target
object (by 23.2%) and faster task completion (by 31.3%)
compared to the camera-based teleoperation approach. In
addition, GraspLook system is less mentally demanding (by
11%) and requires less effort (by 16%) to robot manipulation
than the camera-based teleoperation system while increasing
user involvement by 11%.

To further improve the system, we consider equipping
the robotic gripper with tactile sensors to detect the object
grasping and provide kinesthetic feedback to the user. In
this paper, the experiments were carried out using a single
object from the dataset - the centrifuge test tube, which
has the highest detection accuracy value with proposed R-
CNN. Besides, we are going to conduct experiments with
each object class from the entire dataset, as well as R-CNN
model evaluation in ablation study based on the dataset with
real objects. In addition, teleoperation in the environment
with object occlusion will be experimentally evaluated in the
future.

ACKNOWLEDGMENT

The authors would like to thank Miguel Altamirano Cabr-
era, PhD student at Skolkovo Institute of Science and Tech-
nology, and Jonathan Tirado, PhD student at University of
Southern Denmark, for their support to this project.

