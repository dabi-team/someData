VR-Caps: A Virtual Environment for Capsule Endoscopy

Kağan İncetana, Ibrahim Omer Celikb,2, Abdulhamid Obeida,2, Guliz Irem Gokcelera, Kutsev Bengisu Ozyoruka, Yasin
Almaliogluc, Richard J. Chend,g, Faisal Mahmoodd,e,i, Hunter Gilberth, Nicholas J. Durri, Mehmet Turana,∗

aInstitute of Biomedical Engineering, Bogazici University, Istanbul, Turkey
b Department of Computer Engineering, Bogazici University, Istanbul, Turkey
cComputer Science Department, University of Oxford, Oxford, UK
dBrigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA
eCancer Data Science, Dana Farber Cancer Institute, Boston, MA, USA
fCancer Program, Broad Institute of Harvard and MIT, Cambridge, MA, USA
gDepartment of Biomedical Informatics, Harvard Medical School, Boston, MA, USA
hDeparment of Mechanical and Industrial Engineering, Louisiana State University, Baton Rouge, LA USA
iDepartment of Biomedical Engineering, Johns Hopkins University (JHU), Baltimore, MD, USA

1
2
0
2

n
a
J

4
1

]

V
C
.
s
c
[

2
v
9
4
9
2
1
.
8
0
0
2
:
v
i
X
r
a

Abstract

Current capsule endoscopes and next-generation robotic capsules for diagnosis and treatment of gastrointestinal diseases
are complex cyber-physical platforms that must orchestrate complex software and hardware functions. The desired
tasks for these systems include visual localization, depth estimation, 3D mapping, disease detection and segmentation,
automated navigation, active control, path realization and optional therapeutic modules such as targeted drug delivery
and biopsy sampling. Data-driven algorithms promise to enable many advanced functionalities for capsule endoscopes,
but real-world data is challenging to obtain. Physically-realistic simulations providing synthetic data have emerged as a
solution to the development of data-driven algorithms. In this work, we present a comprehensive simulation platform for
capsule endoscopy operations and introduce VR-Caps, a virtual active capsule environment that simulates a range of
normal and abnormal tissue conditions (e.g., inﬂated, dry, wet etc.) and varied organ types, capsule endoscope designs
(e.g., mono, stereo, dual and 360°camera), and the type, number, strength, and placement of internal and external
magnetic sources that enable active locomotion. VR-Caps makes it possible to both independently or jointly develop,
optimize, and test medical imaging and analysis software for the current and next-generation endoscopic capsule systems.
To validate this approach, we train state-of-the-art deep neural networks to accomplish various medical image analysis
tasks using simulated data from VR-Caps and evaluate the performance of these models on real medical data. Results
demonstrate the usefulness and eﬀectiveness of the proposed virtual platform in developing algorithms that quantify
fractional coverage, camera trajectory, 3D map reconstruction, and disease classiﬁcation. All of the code, pre-trained
weights and created 3D organ models of the virtual environment with detailed instructions how to setup and use the
environment are made publicly available at https://github.com/CapsuleEndoscope/VirtualCapsuleEndoscopy and
a video demonstration can be seen in the supplementary videos (Video-I).

Keywords: Capsule Endoscopy, Deep Reinforcement Learning, Area Coverage, Disease Classiﬁcation, Synthetic Data
Generation

1. Introduction

Optical colonoscopy is considered to be the gold stan-
dard in the early prognosis, diagnosis and intervention of
critical upper and lower GI-tract diseases such as colorectal
cancer (CRC), Crohn’s disease, ulcerative colitis, hemor-
rhoids or inﬂammation. Despite colonoscopies demonstrat-
ing clinical impact in reducing CRC incidence, the current
standard of care for patient screening is invasive and has

∗Corresponding Author
Email address: mehmet.turan@boun.edu.tr (Mehmet Turan)
1This work was supported by the Scientiﬁc and Technological
Research Council of Turkey (TUBITAK) with grant 2232 - The
International Fellowship for Outstanding Researchers

2These authors contributed equally

poor sensitivity in detecting the adenomatous polyps. With
an estimated 19 million colonoscopies performed annually
in the United States and 6-28% polyps missed in routine
screenings, CRC is the second most prevalent cancer and
leading cause of cancer death (Lee et al., 2017). Due to the
fact that small intestines are diﬃcult to access with con-
ventional endoscopes and since patients suﬀer from heavy
pain and discomfort during traditional endoscopy, technolo-
gies such as Wireless Capsule Endoscopes (WCEs) have
emerged for navigating the entire gastrointestinal tract and
identifying adenomatous polyps and other non-polyploid
lesions, which would preclude the progression of CRC.

Unlike conventional endoscopes, WCEs are swallowable,
pill-like imaging devices that allow for direct visualizations
of the GI-tract without requiring bowel preparation or

Preprint submitted to Medical Image Analysis

January 15, 2021

 
 
 
 
 
 
Figure 1: VR-Caps environment. a) The environment consists of a physician performing the magnetically actuated active capsule endoscopy
on a patient by controlling the capsule robot to move throughout the organs with a joystick that is used to move the Franka Emika gripper
that holds a permanent magnet. b) Several magnetically actuated capsule designs; mono camera, stereo camera, dual camera (front-back)
and a panoramic capsule camera with permanent magnets in diﬀerent positions and numbers (permanent magnets can also be replaced by
electromagnet arrays). c) Post processing eﬀects applied to the capsule camera. d) Some examples of tasks that can be performed in VR-Caps
environment: extracting depth information for each frame, disease detection and segmentation, pose estimation from camera frames and
generating the capsule robot’s trajectory. e) 3 diseases generated in our environment that can be used for disease detection and classiﬁcation
tasks. f) Scheme of the ultimate autonomous capsule robot system.

2

sedation (Davies et al., 2005; Meining et al., 2007). A
major limitation of current WCEs is that they progress
through the GI system over the course of days. Active
locomotion using automated and sophisticated algorithms
are needed to guide the capsule robot both to improve
screening time and also to provide active functions such as
delivery, biopsy and therapeutic interventions (Yim et al.,
2013; Valdastri et al., 2012; Pirmoradi et al., 2011; Yim
If a lesion is detected, an automated
and Sitti, 2011).
path planning and execution functionality should reach the
lesion in the most eﬀective and shortest way. Once the
lesion is reached, therapeutic actions such as drug release
or biopsy can follow.

To achieve highly adaptive and patient speciﬁc control,
localization, mapping, path planning and path realization
mechanisms, deep learning will be a key enabler, having
previously demonstrated learning and domain adaptation
capability in many computer vision and robotic control
ﬁelds Mahmood et al. (2019a). One of the biggest disad-
vantages of deep learning techniques is the fact that large
networks need massive amounts of domain-speciﬁc data for
training. Research in recent years has shown that large
amounts of synthetic data can improve the performance of
learning-based robotic control and vision algorithms and
can ameliorate the diﬃculty and expense of obtaining real
measurements in a variety of contexts (Peng et al., 2015;
Hattori et al., 2015; Su et al., 2015; Qiu and Yuille, 2016).
Speciﬁcally in the context of active capsule robots, previous
works have simulated some aspects successfully. Mura et al.
(2016) developed a model-based virtual haptic feedback
mechanism to assist the navigation of endoscopic capsule
robots. More recently, (Abu-Kheil et al., 2019) proposed
a simulation environment particularly for the locomotion
mechanism of endoscopic capsule robots.

Some simulations have targeted vision-related tasks
for endoscopy. Mahmood et al. (2018); Mahmood and
Durr (2018b,a) proposed an approach to generate synthetic
endoscopy images through deep adversarial training and
demonstrated its success in depth estimation task. He et al.
(2018) explored automatic hookworm detection from WCE
using a CNN to classify frames in video sequences. Turan
et al. (2018) developed one of the ﬁrst WCEs with visual
odometry in the GI using RCNNs, and later developed a
deep reinforcement learning-based approach that is able to
learn a policy for navigation in real porcine tissue. Though
deep learning has potential in automating polyp detection
via WCEs, these systems require enormous amounts of data
to generalize to unseen patient tissue and organ topology.
Cross-patient network adaptation is another well-known
challenge in endoscopy, as low-level patient-speciﬁc texture
details such as vascular patterns can obscure important
clinically-relevant details that should be generalized across
patients. As a result, the potential beneﬁts of deep learning
have failed to translate in the technological and clinical
deployment of autonomous WCEs.

this work, we address the lack of a realistic virtual test-bed
platform for WCEs by introducing a new simulation envi-
ronment, VR-Caps. Using advanced rendering techniques,
VR-Caps can generate fully-labeled and realistic synthetic
data that is consistent with the topology and texture of real
organs for control, navigation and machine vision related
tasks. Supported by the fact that deep learning has already
proven its domain adaptation capability in various ﬁelds of
medical image analysis and device control ﬁelds (Mahmood
et al., 2018; Mahmood and Durr, 2018a; Mura et al., 2016),
VR-Caps can facilitate signiﬁcant improvements in medical
imaging and device control applications. In the context of
both active and passive capsule endoscopy, VR-Caps oﬀers
numerous advantages over physical testing:

• VR-Caps can accelerate the design, testing and opti-
mization process for software and related hardware
components that directly aﬀect medical image anal-
ysis applications such as number and placement of
cameras, camera speciﬁcations, control accuracy of
the robot etc.;

• The marginal cost of synthetic data is low compared

to real data;

• System properties and parameter values can be easily

altered to assess sensitivity and robustness;

• VR-Caps carries no risks to live animals or humans;

• VR-Caps can oﬀer reproducibility, which is valuable

in the scientiﬁc pursuit of new algorithms;

• The prevalence of rare diseases can be exaggerated in
VR-Caps to provide data that may be infeasible or
impossible to obtain from human study participants.

VR-Caps facilitates changes in the number, strength,
and placement of magnetic sources; organ type (stomach,
small intestine and colon); conditions (e.g., inﬂated, dry,
wet etc.); deformation and peristaltic motion; camera num-
ber and locations (e.g., mono, stereo, dual and 360°); types
of illumination (e.g., point light source, LED arrays). It
also facilitates inter-patient diversity (e.g., availability of
CT scans and corresponding 3D organ models from diﬀer-
ent patients) with diﬀerent abnormalities (e.g., varying size
of tumors and diﬀerent diseases with various grades).

The paper is organized as follows: Section 2 describes
our simulation environment and gives detailed information
about the pipeline of generating 3D models and realistic
organ textures. Properties required to simulate physical
interactions of capsule endoscopy are also explained in this
section. In Section 3, we demonstrate 6 use-cases of the
environment and the achieved results. Finally, Section 4
presents the concluding remarks and future works.

2. VR-Caps Environment

Even though, prior simulation approaches have been
limited to only a one part of the overall WCE system, in

The VR-Caps environment is built using the popular
graphics engine and a real-time 3D development platform

3

Figure 2: 3D organ construction and texture assignment process. a) We use 3D models reconstructed from real patient CT scans and
create textures based on analysis of real endoscopic videos and assign the generated textures to the 3D models by using UV mapping technique.
b) The ﬂow of creating the mucosa textures from real endoscopy images.The ﬂow of creating the mucosa textures from real endoscopy images.
We ﬁrst remove artifacts and reﬂections on the endoscopy image and then select a uniform region in terms of color and expand it to create the
main mucosa texture c) The pipeline for adding veins to the mucosa texture. We extract veins from real endoscopy images using MATLAB
and assign them on a texture image using a Gaussian distribution that forms relevant vein network.

4

Unity with the integration of Simulation Open Framework
Architecture (SOFA) (Faure et al., 2012) and Unity Ma-
chine Learning Agents Toolkit (ML-Agents) (Juliani et al.,
2018). The SOFA is an open source framework primar-
ily targeted at medical simulation research that allows to
create complex medical simulations such as organ defor-
mation and collisions. The ML-Agents is an open-source
plug-in dedicated for machine learning applications to be
integrated into Unity platform. It enables Unity to serve as
an interactive platform for the training of neural networks
based intelligent agents using state-of-the art approaches
in deep reinforcement learning and imitation learning.

2.1. Generating 3D Organ Models

To represent the 3D geometry of GI organs accurately,
we use computed tomography (CT) images in DICOM for-
mat, which are publicly available on The Cancer Imaging
Archive (TCIA) (Clark et al., 2013). As the archive in-
cludes various sets for diﬀerent parts of the body, for colon
and small intestine, we use ([dataset] Smith et al., 2015)
consisting of 825 subjects and 347 of them are diﬀerentiated
into three classes: 243 patients with no polyps, 69 patients
with 6 to 9 mm sized polyps, and 35 patients with polyps
larger than 10 mm and for stomach, we use ([dataset]
Lucchesi and Aredes, ????) consisting of 46 subjects. Al-
though the dataset includes two subsets in terms of patient
position (supine and prone position) during imaging, the
supine position sets is used because this is the position of
the patient during capsule endoscopy. The dataset also
has diﬀerent models of the organs with and without the
cancerous lumps and we use them to have real polyps in
the reconstructed models.

Using an open-source medical image reconstruction soft-
ware (i.e., InVesalius), 3D organ models are created from
CT scans. The reconstructed 3D model is then imported
into Blender for further processing. As shown in Figure 2-a,
the imported model consists of bones, fat, skin, and other
artifacts that are removed so that only the geometries of the
colon, small intestines and stomach remain. As a next step,
textures are created using the Kvasir ([dataset] Pogorelov
et al., 2017) that consists of real endoscopy images taken
from diﬀerent patients classiﬁed according to the diﬀerent
parts of the GI tract.
In order to create the main mu-
cosa texture from the Kvasir dataset, various endoscopy
images are stitched together and applied on the model
inner surface to generate clear, non-blurry and continu-
ous mucosa walls (see Figure 2-b). After generating the
main mucosa textures, veins are applied on the walls by
extracting vein networks from Kvasir dataset images. To
extract the veins from real endoscopic images, we convert
images into gray-scale and perform median ﬁltering on
the pixels. A contrast-limited adaptive histogram equal-
ization is used to enhance the contrast of the gray-scale
image and to make the veins darker than its surroundings
as shown in Figure 2-c. Extracted veins are applied on
the mucosa texture images, using random distributions for

location, rotation and size of the veins with empirically de-
termined mean and standard deviation values. In addition
to normal tissue, the textures of the disease regions are
also created from Kvasir videos that contain instances of
the corresponding diseases. Figure 1-e shows frames with
diseases instances, while Figure 1-c shows healthy regions
of the organs. Finally, we unwrap the 3D model and the
mesh model is divided into rectangular segments that are
projected on the created UV texture map uniformly in a
repetitive manner throughout the model.

Figure 3: Post-processing pipeline. In order to generate virtual
endoscopy images that can be similar to real endoscopic images, we
improve the rendering quality of the simulation by integrating the
HDRP. This pipeline provides the tools to apply some post-processing
eﬀects on the camera with adjustable parameters; such as vignette,
lens distortion, and chromatic aberration. Lastly, we introduce a wet
surface reﬂection property that interacts with the light source.

2.2. Rendering Tool

As the default pipeline of Unity does not provide high
quality and realistic visuals, we integrate the High Deﬁ-
nition Render Pipeline (HDRP) which mainly works on
diﬀerentiating materials in diﬀerent lighting conditions
while unifying illumination so that all objects in the scene
receive and interact with light in the same manner.

The HDRP shaders provide several options that facil-
itate obtaining more realistic visuals and imitating real
endoscopy images. For instance, adding white coat mask
to the organ material creates reﬂection eﬀect in 3D organ
surfaces when hit by any light source. Endoscopic camera
views are also mimicked using HDRP such as vignetting,
ﬁsh-eye distortion, and chromatic aberration. Vignetting
is the peripheral darkening of the endoscopy image, and
chromatic aberration which usually occurs when the lens
fails to converge all colour wavelengths to the same point
on the focal plane causes the edges between high contrast
areas to appear blurred. These eﬀects are applied directly

5

to the image buﬀer of the virtual camera which provides
real time rendering opportunities. The enhanced image
quality using the HDRP as well the post-processing eﬀects
can be seen in Figure 3.

Floating particles are designed as separate objects of vary-
ing sizes, and they move along the way to the end of the
colon. These eﬀects can be seen in Figure 5 and in the
supplementary videos (Video-II).

2.3. Deformation

Due to its physical accuracy, the FEM (Finite Element
Analysis), which is an approximate discrete representation
of soft tissue that is obtained by dividing the volume of tis-
sue into smaller elements, is widely used in bio-mechanical
modeling of soft tissues in various medical ﬁelds such as
image-guided hepatic surgery, computer-integrated neuro-
surgery, whole-body medical image registration, and surgi-
cal simulations (Zhang et al., 2017). In VR-Caps, in order
to simulate the soft deformable mechanics of organs, we
integrate SofaAPAPI-Unity3D which is an interface that
enables Unity’s PhysX Engine to leverage SOFA’s more
physically accurate models for tissue deformation.

The basic equation solved to model the tissue deforma-

tion is the conservation of momentum:

ρ

d(cid:126)v
dt

= ρ(cid:126)b + ∇ · σ

(1)

where ρ is the mass density of the tissue, (cid:126)b is the density of
external forces per unit mass, and σ is the Cauchy stress
tensor, which is related to the deformation of the tissue
by a suitable constitutive law. SOFA uses a linear elastic
(Hookean) material model and it solves the equation in the
weak form after the inclusion of boundary conditions that
constrain the organ.

As the ﬁnite element method is computationally ex-
pensive, we also integrate a deformation model which is
intended only to reproduce a deﬂection of similar appear-
ance to the physically accurate one. The elastic deformation
which occurs when the capsule robot is in contact with the
organ walls is simulated by the Collider component which
is oﬀered in Unity’s default physics engine and used for
deﬁning the shape of the object for the physical collisions.
When there is a contact between the capsule robot and the
organ, the Collider component stores the position where
the collision occurs and the force that capsule robot ap-
plies to the organ. The displacement amount of vertices
of the organ mesh under the force is set so that it gets its
maximum value at the contact point and reduces as the
distance between the contact point and position of adjacent
vertices increases. An attenuated force at each vertex, (cid:126)Fv
is calculated by using the inverse-square law;

(cid:126)Fv =

(cid:126)F
d2 + 1

,

(2)

where the original force (cid:126)F is divided by the distance squared
added 1, to achieve a full strength force at the distance
close to zero and avoid the force going to inﬁnity. With a

Figure 4: Adjustable eﬀects to mimic real endoscopy images
in VR-Caps Real endoscopy images have some speciﬁc artefacts and
imperfections resulting from camera features and lighting conditions.
In VR-Caps environment, we mimic some of these eﬀects, namely:
specular reﬂection, barrel distortion, chromatic aberration, various
resolutions, depth of ﬁeld and ﬁeld of view. In this ﬁgure, we show
the possibility of adjusting intensity and occurrences of those eﬀects
for 5 diﬀerent levels. Here, grade 0 to 4 denotes the increasing level
of intensity for each eﬀect.

To show that our environment facilitates changes of
aforementioned eﬀects with some other variations such as
diﬀerent camera resolution, depth of ﬁeld and ﬁeld of view,
we provide Figure 4 in which the diﬀerent level intensity of
specularities and other post-processing eﬀects are applied
from level 0-to-4 where grade 0 indicates the lowest possible
intensity for each eﬀect. Besides these camera eﬀects and
artifacts, real endoscopic videos contain a large amount of
liquid eﬀects seen in the GI organ walls. Together with
the pumped air during endoscopic operation, this creates
bubbles of varying sizes and strong mirror reﬂections and
specularities in the recorded videos. Also, partly-digested
food and gastric ﬂuid cause several ﬂoating particles known
as yellow or white chyme appearing at diﬀerent locations
inside the GI. These particles can distract the motion of
the capsule endoscope and occlude the camera lens. In
VR-Caps, we mimic these occurrences by creating virtual
bubbles and chyme using both individual and bounded
particle systems with variable ﬂuid eﬀects on the organ
walls. Bubble particles are modeled as bouncing spheres
of diﬀerent sizes, diﬀerent intensities of random motions.
They tend to rise above the surface as in the real endo-
scopic videos (i.e., in the opposite direction to the gravity).

6

given attenuation force, the change in mesh displacement
in unit time interval is deﬁned as:

(cid:126)F
m
For simplicity, we assumed the mass, m, as 1 for each

∆(cid:126)v =

∆t.

(3)

vertex, and end up with,

∆(cid:126)v = (cid:126)F ∆t.

(4)

So that the position of vertices, (cid:126)x, are updated by

following adjustment;

∆(cid:126)x = (cid:126)v∆t.

(5)

As far as the force is applied, vertices keep moving
without any limitations, which is not realistic due to the
elasticity properties of objects. So we introduce a spring-
like force controlled by an adjustable factor that returns
the vertices to their original position gradually. The spring
force causes some oscillations which are also prevented by
adding a damping eﬀect with resistance constant µ, it is a
factor that decreases velocity over time,

(cid:126)vd = (cid:126)v(1 − µ∆t).

(6)

Figure 5: Bubbles and chymes In the VR-Caps platform, we mimic
the bubbles, chyme and reﬂection eﬀects caused by liquids inside
the GI tract and randomly ﬂoating particles (e.g., chyme) that are
party-digested food appearing in yellow and white colour. By creating
particle system objects in diﬀerent sizes and conﬁgurations, both
bubbles and chyme are simulated in VR-Caps.

2.4. Peristaltic Motion

Peristalsis is the involuntary motion in the GI-tract
that occurs in a wavelike motion progressively through the
length of the organs pushing down the food until excretion.
During the capsule endoscopy session the patient should
be in a fasting state for almost 12 hours. The migrating
motor complex is the motility process that happens during
the fasting (interdigestive) period. It is an electromechan-
ical activity that occurs in a regular cycle every 1,5 to 2
hours that consists of four phases that lasts from 85 to
115 minutes. The ﬁrst phase is an inactive period without
contractions that lasts for 40-60% of the cycle length. The
second phase consists of increasingly strong contractions

that lasts 20-30% of the cycle length. The third phase
consists of strong contractions that lasts 5-10 minutes. The
fourth phase is a recovery phase in which the organs return
into the inactive ﬁrst phase to repeat the cycle (Takahashi,
2013; Feher, 2017). The contractions exhibit variety in
diﬀerent regions of the organ which cause diﬀerent propa-
gation velocities throughout the GI-path. According to the
works of Dooley et al. (1992); Deloose et al. (2012), these
velocities and duration of cycle phases highly depend on
the genetic background and vary across individuals.

The small intestine and colon consist of bends and
curves that can be used to segment the length into dis-
crete segments. Since the capsule robot move in diﬀerent
direction for each segment, we can describe the peristalsis
motion way by subtracting initial and ﬁnal position of each
segment:

(cid:126)v =

Pe − Ps
(cid:107)Pe − Ps(cid:107)

,

(7)

where Pe and Ps represent end and start point of organ
In the Unity environment, the
segments, respectively.
velocity (cm per minute) under the peristaltic motion and
movement direction of capsule robot are simulated based
on its residing segment type. Finally, the force faced by
the capsule robot in unit time interval due to peristalsis is
deﬁned by:

(cid:126)Fp = β( (cid:126)vp − (cid:126)u),
where (cid:126)vp = α(cid:126)v is the propagation velocity which is a scaled
multiple of the propagation direction, and (cid:126)u is the current
velocity of the capsule robot.

(8)

To achieve the physical movement of the contractions
in our simulation environment, meshes of the 3D organ
in question is moved in a wave-like motion. To achieve
that, each vertex is moved by a sine wave which gives the
motion of contraction and expansion of the organ as the
capsule robot propagates through. The Collider component
attached to the deformed organ is also updated with its
mesh to prompt interaction of the capsule robot with the
contractions. Due to all the variability in the duration of
the phases and the propagation velocities, the peristalsis
motion algorithm is designed in a way that user can adjust
the duration and velocity parameters.

2.5. Frictional Resistance

The friction is not completely modeled by the Coulomb
friction law due to viscoelastic behaviour of the intestine
(Accoto et al., 2001; Wang and Yan, 2008). Various fric-
tional forces are applied during the navigation of the capsule
robot and these forces are a function of the robot velocity
(cid:126)v, generally acting along the unit vector opposite to the
velocity, (cid:126)df = −(cid:126)v/(cid:107)(cid:126)v(cid:107) In general, total frictional force (cid:126)f ((cid:126)v)
is modeled as follows:

(cid:126)f = (cid:126)fc + (cid:126)fe + (cid:126)fv

(9)

7

resistance:

(cid:40)

f =

C
(cid:107)(cid:126)v(cid:107)≤ 1
a log(b(cid:107)(cid:126)v(cid:107)+c) + C (cid:107)(cid:126)v(cid:107)> 1

(12)

To estimate parameters, we ﬁt a curve to the published
data using a non-linear least-squares optimization method.
Estimated parameters are a = 55.04, b = 0.23, c = 1.04,
C = 100.

2.6. Magnetic Field

Magnets are commonly used as external locomotion de-
vices for capsule endoscopes (Ciuti et al., 2011). Therefore,
in VR-Caps, we provide option of using a cylinder perma-
nent magnet held by a 7 DOF robotic arm as a locomotion
device that interacts with another permanent magnet in-
side the capsule robot. Although Unity engine has built-in
physics provided by Nvidia PhysX for most of the physical
interactions between objects, there is no existing built-in
model for simulating magnetic ﬁeld (Juliani et al., 2018).
Therefore, a 3rd party model Pivarski (2013) is integrated
into VR-Caps which models dipole-dipole interactions. In
the model, a dipole’s contribution to the magnetic ﬁeld at
a point is calculated as follows:

(cid:126)B (cid:126)m((cid:126)r) =

µ0
4π

1
r3 [3( (cid:126)m · ˆr)ˆr − (cid:126)m]

(13)

where µ0 is the permeability of free space (4π × 10−7 N
),
A2
(cid:126)r is the displacement vector between the dipole and the
target point, r is the magnitude of the vector and ˆr is its
normalized form, and (cid:126)m is the dipole vector. The torque
on a dipole due to total ﬁeld (cid:126)B at a point is:

(cid:126)N = (cid:126)m × (cid:126)B

(14)

The force on a dipole due to the same ﬁeld is:

(cid:126)F = ∇

(cid:16)

(cid:126)m · (cid:126)B

(cid:17)

= (cid:126)m ×

(cid:16)

∇ × (cid:126)B

(cid:17)

+ ( (cid:126)m · ∇) (cid:126)B

(15)

2.7. Robotic Arm

As a holder for the permanent magnet, the exact and
rigged 3D model of Franka Emika’s 7 DOF robotic arm
is used in the simulation environment. In order to mimic
real capabilities as well as constraints of the robot, inverse
kinematics is applied by following a similar approach in
(Nammoto and Kosuge, 2012) and joint angles for the de-
sired magnet locations are computed. The transformation
matrix of the end-point relative to the base of the robot is
deﬁned by a recursive matrix-vector operation in the form
of

Figure 6: Capsule designs with various camera conﬁgurations.
We have designed and employed four endoscopic capsule robots with
diﬀerent number of cameras in diﬀerent orientations. The designs and
the corresponding output frames from all camera views are shown
in the ﬁgure. a) The regular capsule which contains a single camera.
b) The stereo camera conﬁguration that contains two cameras on
one end of the capsule with an oﬀset between them. c) Dual camera
design that consists of a camera at front-end and a second camera at
back-end of the capsule endoscope. d) A panoramic design consisting
of four cameras placed on the sides of capsule robot body with
90°rotational oﬀset to each other, resulting in a 360°view. A video
showing the recordings using these diﬀerent conﬁgurations can be
seen in the supplementary videos (Video-III).

where (cid:126)fc = −µ(cid:107)N (cid:107) (cid:126)df is the Coulomb friction, which is
a function of the normal contact force N , fe is the en-
vironmental resistance caused by hoop stress and elastic
deformation of intestinal wall, and fv is the visco-adhesive
force which occurs due to intestinal mucus (Zhang et al.,
2012). The environmental resistance can be expressed as:

(cid:126)fe = P (x)(cid:126)S sin(θ)

(10)

where P (x) is the pressure applied to contact surface area
normal (cid:126)S (generally along the axis of the lumen as a result
of radial symmetry and contact around the capsule) and
the θ is the angle of the skew force. This force results
from the viscoelasticity of the tissue, which causes the
hoop stress at the front of the capsule, where the lumen
is stretching open, to be greater than the hoop stress at
the back of the capsule, where the lumen has already been
stretched. Visco-adhesive resistance is directly correlated
with the velocity of the capsule robot and it is deﬁned as:

(cid:126)fv = −γ(cid:126)v

(11)

where γ is the viscosity coeﬃcient of intestinal mucus. We
include all three types of frictional forces in our simulation
by relying on the measurements provided in (Zhang et al.,
2012). Since the study has shown a logarithmic correlation
between total frictional force and capsule velocity we deﬁne
the following partial function to simulate the eﬀect of total

(cid:104) Rn
0
0 · · · 0

(cid:105)

T n
0
1

8

= A0

1(θ0)A1

2(θ1) · · · Ai−1

i

(θi)

(16)

Figure 7: Area coverage results A Deep Reinforcement Learning (DRL) based active control method is developed for the autonomous
monitoring of the organ (stomach in this case). The capsule robot is trained with the target of maximizing the coverage of the stomach in a
minimum operation time. The coverage is deﬁned as the number of vertices seen by the capsule camera divided by the total number of vertices.
The reward is the linear increase in coverage for each time t as seen in Equation (18). This ﬁgure visually illustrates the results for the coverage
task at 10 time instances during the inference process. Stomach is represented as 3D point cloud projected onto a 2D plane, and green and red
regions depict covered and uncovered parts of the stomach respectively. A coverage rate of 96.04% was achieved in 47.62 seconds.

i

where Ai−1
(θi) is homogeneous transformation matrix with
Denavit-Hertenberg parameters at a given conﬁguration
from the coordinate system of the (i-1)th joint to the ith
joint and it is given as

Ai

i−1(θi) =





sθi
cθi
−sθicαi
cθicαi
sθisαi −cθisαi

0

0

−ai

0
sαi −disαi
cαi −dicαi
0

1


 (17)

2.8. Endoscopic Capsule Camera Parameters

Simulating a realistic endoscopic capsule camera has
a considerable impact on generating realistically looking
images. For this reason, we calibrate two commercially
available capsule endoscope cameras, Mirocam and Pillcam,
using the pinhole camera model with non-linear radial lens
distortions given by Camera Calibrator App from Zhang
(2000). As per model, the standard aﬃne camera model is
used including a focal length (fx, fy), principal points oﬀset
(cx, cy), and lens shifting coeﬃcient s and these parameter
sets are provided on the Github page. By adjusting these
parameter settings, any type of capsule endoscope cameras
can be mimicked in the simulation environment. In ad-
dition to intrinsic parameters of endoscopic cameras, we
also provide various camera conﬁgurations in terms of the
number and placement of the integrated cameras. Mono
camera design, stereo, dual and 360°camera designs can be
seen in Figure 6 and accessed in the Github source code.

3. Evaluated Tasks and Results

To demonstrate the eﬀectiveness and usefulness of VR-
Caps, we evaluate 6 diﬀerent tasks in medical applications:
area coverage, pose estimation, depth estimation, 3D map
reconstruction, disease classiﬁcation and super resolution.
The following sections describe the evaluation and results
for each task in detail, and a brief overview for all these
tasks and results is also provided in Figure 14.

9

3.1. Area Coverage

An estimated 19 million colonoscopies are performed
annually in the United States and 6-28% polyps are missed
in routine screenings, indicating a poor sensitivity and
accuracy rate for the adenomatous polyps detection (Lee
et al., 2017). Thus, an objective and automated scan of the
GI organs is of paramount importance in terms of ensur-
ing a reliable and sensitive endoscopic diagnosis protocol.
Such scans are possible with mapping and reconstruction
techniques such as SLAM (Simultaneous Localization and
Mapping) for solving area coverage (Whelan et al., 2016;
Chen et al., 2019). These techniques are widely used in
robotics to cope with the challenge of getting a mobile
robot to cover the given ﬁeld in an optimal way. The area
coverage problem was ﬁrst addressed by Choset (2001) and
has become a prominent problem of robotic path planning
and exploration in various applications including terrain
coverage for autonomous underwater vehicles (Hert et al.,
1996), robot vacuum cleaner (Baek et al., 2011) and robotic
demining (Acar et al., 2003). To the best of the authors’
knowledge, for the ﬁrst time in literature, we extend the
area-coverage problem into endoscopy ﬁeld.
If the ex-
plored GI tract area could be systematically and optimally
scanned, the disease detection sensitivity of the endoscopist
would drastically increase and failed polyp detection rates
would drastically decrease. Thus, area coverage has the
potential of becoming one of the signiﬁcant software com-
ponents of the next generation endoscopic systems. For
that purpose, we propose a Deep Reinforcement Learning
(DRL) based active control method that has the goal of
learning a maximum coverage policy for human organ mon-
itoring within a minimal operation time. We train such
control model using the state of the art policy gradient
method Proximal Policy Optimization (PPO) (Schulman
et al., 2017) where the model observations are the states of
the capsule robot as position and rotation vectors and the
output is the discrete set of actions along the x, y, z direc-

Figure 8: Pose estimation results Unsupervised depth and visual odometry method SC-SfMLearner is used for pose estimation task. We
train the network in two diﬀerent scenarios. In the ﬁrst case, only with real porcine data from EndoSLAM dataset (without virtual pre-training)
is used, whereas in the second scenario, the modeel is pre-trained with synthetic data generated on VR-Caps then ﬁne-tuned with the same
set of real porcine data (with virtual pre-training). Both models are tested on the test set of EndoSLAM dataset consisting of 4 diﬀerent
trajectories (2 from small intestine and 2 from colon). Qualitative results are given with trajectory plots of ground truth and aligned predicted
trajectories based on the two scenarios. Quantitative results are given with mean and standard deviation ATE (Absolute Trajectory Error),
Translation and Rotation RPE (Relative Position Error) scores. Both trajectory plots and numeric results clearly show that in all of the test
cases virtual pre-training increases the ﬁnal pose estimation performance.

tion. n this work, we adopt multi-agent learning paradigm
by running three robotic capsule instances inside three dif-
ferent stomach models simultaneously. The reward for the
coverage policy is the linear diﬀerence coverage diﬀerence of
all three organs between two consecutive time steps which
is stated as:

rt = α

3
(cid:88)

i=1

(C (i)

t − C (i)

t−1)

(18)

t

where C (i)
represents the percentage of the covered area
up until time t. It is calculated as the proportion of the
vertices that are seen by the capsule camera to the total
number of vertices of the ith organ instance and α that
is a scaling factor for faster convergence and empirically
determined as 0.1. The training is performed with a series
of episodes where in each episode the agent performs a
set of actions and collects the reward accordingly until
the obtained reward converges at a point. In the training
session, the length of each episode is set as 7500 iteration
steps and the policy network architecture consists of a
three layered fully connected neural network where each
layer contains 256 hidden units, the batch size and the
learning rate are 64 and 3e − 4, respectively. Under these
hyper-parameter settings, the agent is trained for 500K
iteration steps. Figure 7 visually demonstrates the covered
volume of the test stomach for the inference session for 10
diﬀerent time instances. Green regions represent covered
areas of the inspected organ, whereas red regions show
uncovered parts as 3D point clouds projected onto a 2D
plane. The arrow indicates the location of the capsule robot
and the black line depicts the path that the capsule robot
has realized to achieve the coverage. As seen in the ﬁgure,
the pre-trained agent is able to cover 96% of the inspected
stomach instance in 47.62 seconds. This suggest that a

10

DRL-based autonomous coverage policy method provides
a promising and systematic monitoring opportunity for the
next-generation capsule endoscopes.

3.2. Pose and Depth Estimation

Determining the position of abnormalities is one of the
signiﬁcant challenges in capsule endoscopy, and this re-
quires accurate localization of capsules inside the patient
body. Therefore, numerous studies have been conducted in
capsule localization such as Radio Frequency localization,
Time of Arrival based localization and image-based local-
ization. With the recent developments in image processing
techniques, image-based localization methods consisting of
pose and depth estimation including Principal Component
Analysis (PCA), Vector Quantization (VQ), and machine
learning techniques become popular (Dey et al., 2017).
More recently, numerous deep learning approaches that re-
quire large amounts of data have been developed for visual
odometry and depth estimation tasks (Wang et al., 2017;
Zhan et al., 2018; Almalioglu et al., 2019). In this work, the
state-of-the-art unsupervised depth and visual odometry
deep learning method SC-SfMLearner (Bian et al., 2019)
is used to evaluate the eﬀectiveness of synthetic data gen-
erated from VR-Caps in pose and depth estimation tasks.
To perform comparative analysis in pose estimation, the
SC-SfMLearner network is used in two diﬀerent training
scenarios. In the ﬁrst scenario, the network is trained us-
ing only real endoscopic dataset (EndoSLAM ([dataset]
Ozyoruk et al., 2020)) which is acquired from ex-vivo real
porcine organ instances and contains endoscopy recordings
with 6-DoF ground truth pose. The training is performed
with 2, 210 frames, 200 epochs with a batch size of 4 and
learning rate of 1e − 4. In the second scenario, the same
network is ﬁrst trained with VR-Caps synthetic data, and

Figure 9: Depth estimation results Depth prediction maps of the unsupervised SC-SfMLearner network trained with only synthetic data
generated on VR-Caps. In this ﬁgure, we present estimation results for both virtual data and real endoscopy data for stomach and small
intestine. As VR-Caps provides ground truth depth for each frame, for virtual test results, ground truth, prediction and error heat maps
with their counterpart raw images are provided. For real endoscopy, the trained model is tested on Kvasir dataset for stomach and Redlesion
dataset for small intestine. Since there is no ground truth depth information available for these datasets, only depth estimations are provided
with their corresponding raw images. It can be seen that unsupervised training with VR-Caps synthetic data qualitatively achieves good
performance not only on virtual data but also on real endoscopy data.

then ﬁne-tuned on EndoSLAM dataset using the same hy-
perparameter settings in the ﬁrst scenario. The synthetic
data used for pre-training consists of 2, 039 frames and the
network is trained for 210 epochs with a batch size of 4
and a learning rate of 1e − 4. Both models are tested on
an EndoSLAM test data with four trajectories containing
1135 and 1525 images from small intestine and 942 and 949
images from colon. Three metrics are used to evaluate the
pose estimation performance of these networks: (1) AT E
(Absolute Trajectory Error)

(19)

AT E = d(p, q)
where d(p, q) = (cid:0)(p1 − q1)2 + (p2 − q2)2 + (p3 − q3)2(cid:1)1/2 is
the Euclidean distance and p, q ∈ R3 stand for estimated
global pose of camera and the ground truth counterparts
on the trajectory, respectively. Since both predicted and
ground truth trajectories are arbitrarily speciﬁed, we use
rigid-body transformation in p to a solution that maps the
predicted trajectory onto the ground truth trajectory q.
(2) RP Etrans (Translational Relative Pose Error), and (3)
RP Erot (Rotational Relative Pose Error) which are stated
as:

R = (Q−1

i Qi+1)−1 · (P −1

i Pi+1)

(20)

11

RP Etrans = (R2

0,3 + R2

1,3 + R2

2,3)1/2

p =

1
2

(R0,0 + R1,1 + R2,2 − 1)

(21)

(22)

(23)

RP Erot = arccos(max(min(p, 1), −1))
where R ∈ IR4x4 represents relative pose error. P and Q
stand for predicted and ground truth pose, respectively. In
Figure 8, the results of two training scenarios on the En-
doSLAM test set are represented qualitatively with aligned
trajectory plots, and quantitatively with the aforemen-
tioned metrics. Both qualitative and quantitative results
clearly show that in all cases pre-training with synthetic
data improves the ﬁnal pose estimation performance.

To evaluate the depth estimation performance, we test
the model trained with only synthetic data and test on
both synthetic (virtual stomach and small intestine from
VR-Caps) and real endoscopy images (Kvasir dataset for
stomach and Redlesion [dataset] Coelho et al. (2018) for
small intestine). In Figure 9, we show sample depth maps
for these test cases. Since VR-Caps is able to provide
ground truth depth for every frame generated, error heat
maps are given to depict the estimation performance of the

Figure 10: 3D Reconstruction results. This ﬁgure illustrates the 3D reconstruction results for using both VR-Caps and EndoSLAM data.
To show the impact of using synthetic data generated by VR-Caps, results are tested on two cases where the virtual data is not used (w/o
pre-training) and the virtual is used for pre-training (w pre-training). a) The unordered input frames of small intestine and stomach from
VR-Caps and EndoSLAM are given as input to Scale Invariant Feature Transform. b) The ﬁnal stitched image is created by aligning and
multi-blending all input images. Specularities on stitched image are inpainted. c) The point cloud data in .ply format for synthetic data is
provided by VR-Caps while the point cloud is obtained using 3D scanner in EndoSLAM dataset. d) The depth map of inpainted image is
predicted based on two models of SC-SfMLearner network (w and w/o virtual pre-training). e) The matched area between ground truth data
and reconstructed surface is formed by using ICP (Iterative Closest Point) algorithm. f ) The cloud-to-mesh/cloud-to-cloud distances are
displayed in the form of heatmap. The bar shows the root mean square error in unit of [cm]. In the case of VR-Caps data is not used for
training, the RMSEs for small intestine and stomach are 1.69 and 2.45 [cm] and 1.11 and 1.30 [cm] for synthetic data and real porcine data
respectively. Whereas, for the model pre-trained with virtual data, the RMSEs for small intestine and stomach are 0.93 and 2.26 [cm] and
0.80 and 0.92 [cm] for synthetic and real data respectively. The diﬀerence reveals the eﬀectiveness of using VR-Caps in 3D reconstruction by
showing better alignment with the ground truth data.

12

model. As there is no ground truth depth available for real
endoscopy datasets, we present only the qualitative results.
The results suggest that in a cross-dataset setting, deep
learning models trained on virtual images have suﬃcient
generalization power on depth estimation task.

3.3. 3D Reconstruction

In endoscopy and colonoscopy, doctors are still chal-
lenged by some limitations such as the constrained ﬁeld-
of-view and the uncertainty of endoscope location inside
the organ under inspection. Determining the location of
lesions with respect to complete view of inspected organs
is crucial for surgeons to plan treatment procedure in early
stages of the malignancy and this can be possible by recon-
structing the 3D shape of the whole organ based on visual
feedback from endoscope camera (Widya et al., 2019). Ex-
isting works have proposed solutions to reconstruct the 3D
shape of organs (e.g., colon, liver, and larynx) based on
camera pose acquired by endoscope videos. These solutions
span shape-from-shading, visual SLAM, and structure-from
motion. Some reconstruction artifacts due to repeating
patterns may be removed using periodic plus smooth de-
composition (Mahmood et al., 2015).

Algorithm 1 3D Reconstruction and Evaluation Pipeline
1: Extract SIFT features between image pairs
2: Find k-nearest neighbours for each feature using a k-d

tree

3: for each image do
4:

(i) Select m candidate matching images that have

the most number of corresponding feature points

5:

(ii) Find geometrically consistent feature matches
using RANSAC to solve for the homography between
pairs of images.

6: end for
7: Find connected components of image matches
8: for each connected component do
9:

(i) Perform bundle adjustment for connected com-

ponents in image matches

10:

(ii) Render ﬁnal stitched image using multi-band

blending
11: end for
12: Apply inpainting on the stitched image to suppress

specularities

13: Reconstruct the depth surface using SC-SfMLearner

network

14: Label a common polygon segment in ground truth data

and reconstructed surface

15: Apply ICP algorithm using the common polygon as

initialization

16: Compute iteratively the cloud-to-cloud distances to

meet RMSE termination criteria

In this work, we use a hybrid 3D reconstruction tech-
nique described in Algorithm 1. The fundamental steps are
Otsu threshold based reﬂection detection, inpainting-based

reﬂection suppression, feature matching, tracking based
image stitching and non-Lambertian surface reconstruction.
Feature point correspondences between frames are created
using SIFT feature matching and RANSAC based pair elim-
ination (Brown et al., 2003; Brown and Lowe, 2007). Then
the surface reconstruction is performed using the depth
images obtained from SC-SfMLearner network. To align
the ground truth data and reconstructed surface, the ICP
approach is employed after manually labelling a common
polygon segment. To demonstrate the impact of synthetic
data generated on VR-Caps, we evaluate the reconstructed
surfaces using depth maps obtained from two diﬀerent net-
works (with and without pre-training) as seen in Figure 10.
We compare the predicted reconstructions for both syn-
thetic and real data containing 3D ground-truth depth
maps. In terms of RMSE values, reconstructed surfaces of
observed organ instances obtained from pre-trained model
demonstrates better alignment with corresponding ground
truth data. Such that for the pre-trained model, the RMSE
scores for small intestine and stomach are 0.93 and 2.26
[cm] and 0.80 and 0.92 [cm] in the case of synthetic data
and real data respectively. Whereas, for the model where
VR-Caps data is not used, the RMSE scores for small intes-
tine and stomach are 1.69 and 2.45 [cm] and 1.11 and 1.30
[cm] for synthetic data and real porcine data respectively.
Cloud-to-cloud distances for reconstruction processes are
provided in Figure 10 as heatmap representation.

Figure 11: Disease classes 3 diseases are generated in VR-Caps
and instances with various shapes, sizes and severity levels are shown.
Hemorrhage and Ulcerative Collitis are created based on the real en-
doscopy images mimicking the abnormal mucosa texture. Polyps are
reconstructed based on CT scans as they are distinctive in topology.

13

Figure 12: Disease classiﬁcation results This ﬁgure demonstrates the comparative results for two scenarios in disease classiﬁcation task
where the classes are Normal, Polyp and Ulcer. In the ﬁrst scenario, ResNet-152 CNN architecture is trained on real endoscopy data (Kvasir)
which consists of 425 images per class (with virtual pre-training). Whereas, in the other scenario, the same network is initially pre-trained with
synthetic data generated using VR-Caps which contains 2940, 3400, 2374 images for Normal, Polyps and Ulcer, respectively. The pre-trained
model is ﬁne-tuned with the same training data used in the ﬁrst scenario. Both models are tested on Kvasir test dataset containing 225 real
endoscopy images and numeric results as well as ROC curves for each case are presented. The model with virtual pre-training signiﬁcantly
improves the classiﬁcation performance compared to the model without pre-training in all metrics except a minor decrease in accuracy and
recall for the Normal class.

3.4. Disease Classiﬁcation

In order to support diagnostics procedure and help
physicians, the newer image recognition methodologies
(i.e., computer aided systems) have been applied to many
aspects in the medical ﬁeld. Among these methods, CNNs
have been widely used particularly for automated lesion
segmentation in endoscopy images (Wang et al., 2020) and
detecting and diagnosing colorectal diseases from capsule
endoscopy videos (Takiyama et al., 2018; Mahmood et al.,
2019b). To provide synthetic data that can be used in
training of these CNN models, in VR-Caps environment,
we generate instances of 3 diﬀerent disease classes with
varying severity grades. Diseases that we introduce in the
environment are polyps, ulcerative colitis, and hemorrhoids
with various shape, size and grades. CT-scans from TCIA
dataset contains several polyps of diﬀerent shapes and vari-
ations that allows for reconstructing these instances inside
the virtual 3D organ models. On top of the volumetric
reconstruction, we assign texture on these virtual polyp
instances using extracted textures from Kvasir dataset. In
total, 4 polyp instances are generated with several vari-

ations in shapes. By assigning texture frames obtained
from 5 diﬀerent polyp instances with diﬀerent severity lev-
els in the Kvasir dataset, 20 polyp instances with varying
shape and textures are created. As a second disease class,
instances from 3 diﬀerent grades of ulcerative colitis are
generated. Then, by applying these textures with 20 diﬀer-
ent shapes and size of areas to the various segments that
are created during UV mapping, we create 60 instances for
ulcerative colitis disease class, in total. Hemorrhoid bleed-
ing disease class, is created using 4 diﬀerent grade levels
taken from the Kvasir dataset and by applying them in 10
diﬀerent shape and size of areas to the segments, we get 40
instances with varying amount of bleeding. Examples of
these diseases with varying grades can be seen in Figure 11.
To exemplify the use case of VR-Caps in disease classi-
ﬁcation, we train a ResNet-152 model for solving image
classiﬁcation with three classes: Polyp, Ulcer, and Normal.
The hemorrhoid class is excluded since it is not present in
the benchmark dataset (Kvasir). The VR-Caps dataset for
disease classiﬁcation task consists of 8714 image frames in
total where each class Normal, Polyp, and Ulcer contains

14

Figure 13: Results on superresolved diseased regions. Super Resolution results of ulcerative coalitis and polyps disease classes for both
VR-Caps and Kvasir datasets with EndoL2H algorithm on 4x scale factor. The diseased regions are zoomed and cropped inside the yellow
squares. EndoL2H indicates that the model is trained with real Kvasir dataset whereas EndoL2H p is the pre-trained version of EndoL2H
with virtual data. PSNR and SSIM metrics are used to evaluate the performances of these two protocols.

2940, 3400, 2374 frames respectively. A comparative analy-
sis based on two scenarios proves that VR-Caps provides
potentially useful synthetic data for classiﬁcation tasks.
For the ﬁrst scenario (without virtual pre-training), the
ResNet-152 CNN architecture is trained on real endoscopy
data (Kvasir) which consists of 1275 images where each of
the three classes have equal number of frames which is 425.
In the second scenario (with virtual pre-training), the same
network is ﬁrst pre-trained on virtual data generated by
VR-Caps, then ﬁne-tuned with the same training data used
in the ﬁrst scenario. The same hyperparameter settings
are used for all trainings in both scenarios where the num-
ber of epoch, batch size, and the learning rate are 10, 32,
0.001, respectively. Both scenarios are tested on a common
Kvasir test set which contains 225 frames for each class.
The classiﬁcation results for both cases are presented in
Figure 12. The model with virtual pre-training signiﬁcantly
outperforms the model without pre-training in all metrics
including accuracy, precision, recall, average precision and
AUC (Area Under a Curve) score with 95% CI (Conﬁdence
Interval). For instance, virtual pre-training with VR-Caps
dataset increases the percentage of the f-1 scores of Polyp
and Ulcer classes by 21% and 11%, respectively. Only a
minor decrease is observed in accuracy and recall metrics
for the Normal class.

3.5. Super Resolution

Current practices of capsule endoscopy suﬀer from low
resolution images due to low lighting and limited power.
The poor image quality, therefore, brings diﬃculty in accu-
rate diagnoses. Due to size and cost limitations in imag-
ing sensors of the capsule cameras, methods for super-
resolution (i.e., generating high resolution images from low

15

resolution frames) have become an important clinical prob-
lem for building more hardware-eﬃcient imaging devices.
(Park et al., 2019).

To investigate how simulated data can be used to solve
this problem, we benchmark the VR-Caps using the Deep
Super-Resolution for Capsule Endoscopy (EndoL2H) net-
work (Almalioglu et al., 2020). The deﬁned task is to
superresolve the input frames with a resolution of 256 × 256
pixels to a resolution of 1024 × 1024, making an upscaling
of 4x in total. As validation, we devise two ablation ex-
periments. In our ﬁrst experiment, we train and validate
EndoL2H network using real frames from Kvasir dataset
for 100 epochs and test on both synthetic and real dataset,
with 6400 real frames for training, 800 real frames for vali-
dation and 800 frames from VR-Caps and Kvasir dataset
for testing are used. In the second experiment, we train and
validate the network with synthetic frames for 50 epochs,
then ﬁne-tune using Kvasir dataset for another 50 epochs.
From each of VR-Caps and Kvasir dataset, 6400 frames
for training, 800 frames for validation and 800 frames for
testing are used, respectively. To evaluate the super resolu-
tion performance, Peak Signal-to-Noise Ratio (PSNR) and
Structural Similarity Index (SSIM) metrics are used. By
pre-training the model on VR-Caps data, better PSNR and
SSIM values are obtained with an increase in the scores
from 34.37 & 0.74 to 42.33 & 0.98 for virtual and from 32.98
& 0.80 to 35.27 & 0.84 for real test data, respectively. In ad-
dition, super resolution eﬀect on diseased regions of organ
instances for both synthetic and real data is qualitatively
represented in Figure 13.

4. Conclusions and Future Work

In this work, we introduced a virtual endoscopy environ-
ment, VR-Caps, that is capable of generating fully-labeled
and realistic synthetic data that is consistent with the
topology and texture of real organs for control, navigation,
and machine vision related tasks, which may contribute
to signiﬁcant improvements in several medical imaging
and device control applications. We have illustrated the
eﬀectiveness of the synthetic data generated by our envi-
ronment on state-of-the-art methods applied to 6 diﬀerent
tasks. These tasks showed that data provided by VR-Caps
increases the performance of deep learning models that can
enhance the eﬀectiveness of endoscopy by providing solu-
tions to common technical problems faced during surgical
or diagnostic procedures. For endoscopic control and vision
related tasks, we believe that we reduced the gap between
simulation and real-world domains which is essential in
transferring learned skills to real-world robotic scenarios.
Besides various sim-2-real scenarios, we additionally ad-
dressed the autonomous area coverage problem and showed
a promising coverage learning strategy for endoscopic sys-
tems. As future work, we will continue working on the
coverage problem and evaluate the validity of our approach
in real-world scenarios. We also plan to extend the envi-
ronment to simulate interaction with ﬂexible or tethered
double balloon endoscopes and to include additional disease
classes and modalities for control and vision tasks. We
also plan to leverage the environment to study the eﬀect of
capsule design choices on the performance of the various
algorithms (e.g., conducting depth estimation test with a
stereo camera or coverage task with a 360°camera design).

Declaration of Competing Interest

The authors declare that they have no known competing
ﬁnancial interests or personal relationships that could have
appeared to inﬂuence the work reported in this paper.

Acknowledgements

Mehmet Turan, Kagan Incetan, Ibrahim O. Celik, Ab-
dulhamid Obeid, Guliz I. Gokceler, Kutsev B. Ozyoruk
are especially grateful to Technological Research Council
of Turkey (TUBITAK) for International Fellowship for
Outstanding Researchers grant.

References

Abu-Kheil, Y., Al Trad, O., Seneviratne, L., Dias, J., 2019. A
proposed clinical evaluation of a simulation environment for
magnetically-driven active endoscopic capsules. In: Robotics in
Healthcare. Springer, pp. 87–94.

Acar, E. U., Choset, H., Zhang, Y., Schervish, M., 2003. Path planning
for robotic demining: Robust sensor-based coverage of unstructured
environments and probabilistic methods. The International journal
of robotics research 22 (7-8), 441–466.

Accoto, D., Stefanini, C., Phee, L., Arena, A., Pernorio, G., Men-
ciassi, A., Carrozza, M., Dario, P., 2001. Measurements of the
frictional properties of the gastrointestinal tract. In: World Tribol-
ogy Congress. Vol. 3.

Almalioglu, Y., Ozyoruk, K. B., Gokce, A., Incetan, K., Simsek,
G. I. G. M. A., Ararat, K., Chen, R. J., Durr, N. J., Mahmood,
F., Turan, M., 2020. Endol2h: Deep super-resolution for capsule
endoscopy. IEEE Transactions on Medical Imaging, 1–1.

Almalioglu, Y., Turan, M., Sari, A. E., Saputra, M. R. U., de Gusmão,
P. P., Markham, A., Trigoni, N., 2019. Selfvio: Self-supervised
deep monocular visual-inertial odometry and depth estimation.
arXiv preprint arXiv:1911.09968.

Baek, S., Lee, T.-K., Se-Young, O., Ju, K., 2011. Integrated on-
line localization, mapping and coverage algorithm of unknown
environments for robotic vacuum cleaners based on minimal sensing.
Advanced Robotics 25 (13-14), 1651–1673.

Bian, J., Li, Z., Wang, N., Zhan, H., Shen, C., Cheng, M.-M., Reid, I.,
2019. Unsupervised scale-consistent depth and ego-motion learn-
ing from monocular video. In: Advances in Neural Information
Processing Systems. pp. 35–45.

Brown, M., Lowe, D. G., 2007. Automatic panoramic image stitching
using invariant features. International Journal of Computer Vision
2, 59–73.

Brown, M., Lowe, D. G., et al., 2003. Recognising panoramas. In:

ICCV. Vol. 3. p. 1218.

Chen, R. J., Bobrow, T. L., Athey, T., Mahmood, F., Durr, N. J.,
2019. Slam endoscopy enhanced by adversarial depth prediction.
arXiv preprint arXiv:1907.00283.

Choset, H., 2001. Coverage for robotics–a survey of recent results.
Annals of mathematics and artiﬁcial intelligence 31 (1-4), 113–126.
Ciuti, G., Menciassi, A., Dario, P., 2011. Capsule endoscopy: from cur-
rent achievements to open challenges. IEEE reviews in biomedical
engineering 4, 59–72.

Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel,
P., Moore, S., Phillips, S., Maﬃtt, D., Pringle, M., et al., 2013.
The cancer imaging archive (tcia): maintaining and operating a
public information repository. Journal of digital imaging 26 (6),
1045–1057.

[dataset] Coelho, P., Pereira, A., Salgado, M., Cunha, A., 2018. A
deep learning approach for red lesions detection in video capsule
endoscopies. In: International Conference Image Analysis and
Recognition. Springer, pp. 553–561.

[dataset] Lucchesi, F., Aredes, N., ???? Radiology data from the
cancer genome atlas stomach adenocarcinoma [tcga-stad] collection,
2016. The Cancer Imaging Archive 10, K9.

[dataset] Ozyoruk, K. B., Incetan, K., Coskun, G., Gokceler, G. I.,
Almalioglu, Y., Mahmood, F., Durr, N. J., Curto, E., et al., 2020.
Quantitative evaluation of endoscopic slam methods: Endoslam
dataset. arXiv preprint arXiv:2006.16670.

[dataset] Pogorelov, K., Randel, K. R., Griwodz, C., Eskeland, S. L.,
de Lange, T., Johansen, D., Spampinato, C., Dang-Nguyen, D.-T.,
Lux, M., Schmidt, P. T., et al., 2017. Kvasir: A multi-class image
dataset for computer aided gastrointestinal disease detection. In:
Proceedings of the 8th ACM on Multimedia Systems Conference.
pp. 164–169.

[dataset] Smith, K., Clark, K., Bennett, W., Nolan, T., Kirby, J.,
Wolfsberger, M., Moulton, J., Vendt, B., Freymann, J., 2015. Data
from ct_colonography. The Cancer Imaging Archive.

Davies, R. J., Miller, R., Coleman, N., 2005. Colorectal cancer screen-
ing: prospects for molecular stool analysis. Nature Reviews Cancer
5 (3), 199–209.

Deloose, E., Janssen, P., Depoortere, I., Tack, J., 2012. The migrating
motor complex: control mechanisms and its role in health and
disease. Nature reviews Gastroenterology & hepatology 9 (5), 271.
Dey, N., Ashour, A. S., Shi, F., Sherratt, R. S., 2017. Wireless capsule
gastrointestinal endoscopy: Direction-of-arrival estimation based
localization survey. IEEE reviews in biomedical engineering 10,
2–11.

Dooley, C. P., Di Lorenzo, C., Valenzuela, J. E., 1992. Variability
of migrating motor complex in humans. Digestive diseases and
sciences 37 (5), 723–728.

16

Faure, F., Duriez, C., Delingette, H., Allard, J., Gilles, B.,
Marchesseau, S., Talbot, H., Courtecuisse, H., Bousquet, G., Peter-
lik, I., et al., 2012. Sofa: A multi-model framework for interactive
physical simulation. In: Soft tissue biomechanical modeling for
computer assisted surgery. Springer, pp. 283–321.

Feher, J., 2017. 8.3—intestinal and colonic chemoreception and motil-

ity.

Hattori, H., Naresh Boddeti, V., Kitani, K. M., Kanade, T., 2015.
Learning scene-speciﬁc pedestrian detectors without real data. In:
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 3819–3827.

He, J.-Y., Wu, X., Jiang, Y.-G., Peng, Q., Jain, R., 2018. Hookworm
detection in wireless capsule endoscopy images with deep learning.
IEEE Transactions on Image Processing 27 (5), 2379–2392.

Hert, S., Tiwari, S., Lumelsky, V., 1996. A terrain-covering algorithm

for an auv. In: Underwater Robots. Springer, pp. 17–45.

Juliani, A., Berges, V.-P., Vckay, E., Gao, Y., Henry, H., Mattar, M.,
Lange, D., 2018. Unity: A general platform for intelligent agents.
arXiv preprint arXiv:1809.02627.

Lee, J., Park, S. W., Kim, Y. S., Lee, K. J., Sung, H., Song, P. H.,
Yoon, W. J., Moon, J. S., 2017. Risk factors of missed colorectal
lesions after colonoscopy. Medicine 96 (27).

Mahmood, F., Borders, D., Chen, R., McKay, G. N., Salimian,
K. J., Baras, A., Durr, N. J., 2019a. Deep adversarial training for
multi-organ nuclei segmentation in histopathology images. IEEE
transactions on medical imaging.

Mahmood, F., Chen, R., Durr, N. J., 2018. Unsupervised reverse
domain adaptation for synthetic medical images via adversarial
training. IEEE transactions on medical imaging 37 (12), 2572–2581.
Mahmood, F., Durr, N. J., 2018a. Deep learning and conditional
random ﬁelds-based depth estimation and topographical recon-
struction from conventional endoscopy. Medical image analysis 48,
230–243.

Mahmood, F., Durr, N. J., 2018b. Deep learning-based depth estima-
tion from a synthetic endoscopy image training set. In: Medical
Imaging 2018: Image Processing. Vol. 10574. International Society
for Optics and Photonics, p. 1057421.

Mahmood, F., Toots, M., Öfverstedt, L.-G., Skoglund, U., 2015. 2d
discrete fourier transform with simultaneous edge artifact removal
for real-time applications. In: 2015 International Conference on
Field Programmable Technology (FPT). IEEE, pp. 236–239.

Mahmood, F., Yang, Z., Chen, R., Borders, D., Xu, W., Durr, N. J.,
2019b. Polyp segmentation and classiﬁcation using predicted depth
from monocular endoscopy. In: Medical Imaging 2019: Computer-
Aided Diagnosis. Vol. 10950. International Society for Optics and
Photonics, p. 1095011.

Meining, A., Semmler, V., Kassem, A., Sander, R., Frankenberger, U.,
Burzin, M., Reichenberger, J., Bajbouj, M., Prinz, C., Schmid, R.,
2007. The eﬀect of sedation on the quality of upper gastrointestinal
endoscopy: an investigator-blinded, randomized study comparing
propofol with midazolam. Endoscopy 39 (04), 345–349.

Mura, M., Abu-Kheil, Y., Ciuti, G., Visentini-Scarzanella, M., Men-
ciassi, A., Dario, P., Dias, J., Seneviratne, L., 2016. Vision-based
haptic feedback for capsule endoscopy navigation: a proof of con-
cept. Journal of Micro-Bio Robotics 11 (1-4), 35–45.

Nammoto, T., Kosuge, K., 2012. An analytical solution for a redun-
dant manipulator with seven degrees of freedom. International
journal of automation and smart technology 2 (4), 339–346.

Park, J., Hwang, Y., Yoon, J.-H., Park, M.-G., Kim, J., Lim, Y. J.,
Chun, H. J., 2019. Recent development of computer vision tech-
nology to improve capsule endoscopy. Clinical endoscopy 52 (4),
328.

Peng, X., Sun, B., Ali, K., Saenko, K., 2015. Learning deep object de-
tectors from 3d models. In: Proceedings of the IEEE International
Conference on Computer Vision. pp. 1278–1286.

Pirmoradi, F. N., Jackson, J. K., Burt, H. M., Chiao, M., 2011. A
magnetically controlled mems device for drug delivery: design,
fabrication, and testing. Lab on a Chip 11 (18), 3072–3080.

Pivarski, J., 2013. Magnetodynamics.

URL https://doi.org/10.5281/zenodo.3712071

Qiu, W., Yuille, A., 2016. Unrealcv: Connecting computer vision

to unreal engine. In: European Conference on Computer Vision.
Springer, pp. 909–916.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.,
2017. Proximal policy optimization algorithms. arXiv preprint
arXiv:1707.06347.

Su, H., Qi, C. R., Li, Y., Guibas, L. J., 2015. Render for cnn:
Viewpoint estimation in images using cnns trained with rendered 3d
model views. In: Proceedings of the IEEE International Conference
on Computer Vision. pp. 2686–2694.

Takahashi, T., 2013. Interdigestive migrating motor complex -its

mechanism and clinical importance.
URL
PMC5137267/

https://www.ncbi.nlm.nih.gov/pmc/articles/

Takiyama, H., Ozawa, T., Ishihara, S., Fujishiro, M., Shichijo, S.,
Nomura, S., Miura, M., Tada, T., 2018. Automatic anatomical
classiﬁcation of esophagogastroduodenoscopy images using deep
convolutional neural networks. Scientiﬁc reports 8 (1), 1–8.

Turan, M., Almalioglu, Y., Araujo, H., Konukoglu, E., Sitti, M., 2018.
Deep endovo: A recurrent convolutional neural network (rcnn)
based visual odometry approach for endoscopic capsule robots.
Neurocomputing 275, 1861–1870.

Valdastri, P., Ciuti, G., Verbeni, A., Menciassi, A., Dario, P., Arezzo,
A., Morino, M., 2012. Magnetic air capsule robotic system: proof
of concept of a novel approach for painless colonoscopy. Surgical
endoscopy 26 (5), 1238–1246.

Wang, K., Yan, G., 11 2008. Research on measurement and modeling
of the gastro intestine’s frictional characteristics. Measurement
Science and Technology 20, 015803.

Wang, S., Clark, R., Wen, H., Trigoni, N., 2017. Deepvo: Towards end-
to-end visual odometry with deep recurrent convolutional neural
networks. In: 2017 IEEE International Conference on Robotics
and Automation (ICRA). IEEE, pp. 2043–2050.

Wang, S., Cong, Y., Zhu, H., Chen, X., Qu, L., Fan, H., Zhang,
Q., Liu, M., 2020. Multi-scale context-guided deep network for
automated lesion segmentation with endoscopy images of gastroin-
testinal tract. IEEE Journal of Biomedical and Health Informatics.
Whelan, T., Salas-Moreno, R. F., Glocker, B., Davison, A. J.,
Leutenegger, S., 2016. Elasticfusion: Real-time dense slam and
light source estimation. The International Journal of Robotics
Research 35 (14), 1697–1716.

Widya, A. R., Monno, Y., Imahori, K., Okutomi, M., Suzuki, S., Go-
toda, T., Miki, K., 2019. 3d reconstruction of whole stomach from
endoscope video using structure-from-motion. In: 2019 41st Annual
International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC). IEEE, pp. 3900–3904.

Yim, S., Gultepe, E., Gracias, D. H., Sitti, M., 2013. Biopsy using
a magnetic capsule endoscope carrying, releasing, and retriev-
ing untethered microgrippers. IEEE Transactions on Biomedical
Engineering 61 (2), 513–521.

Yim, S., Sitti, M., 2011. Design and rolling locomotion of a mag-
netically actuated soft capsule endoscope. IEEE Transactions on
Robotics 28 (1), 183–194.

Zhan, H., Garg, R., Saroj Weerasekera, C., Li, K., Agarwal, H., Reid,
I., 2018. Unsupervised learning of monocular depth estimation
and visual odometry with deep feature reconstruction. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. pp. 340–349.

Zhang, C., Liu, H., Tan, R., Li, H., 08 2012. Modeling of velocity-
dependent frictional resistance of a capsule robot inside an intestine.
Tribology Letters 47, 295–301.

Zhang, J., Zhong, Y., Gu, C., 2017. Deformable models for surgical
simulation: a survey. IEEE reviews in biomedical engineering 11,
143–164.

Zhang, Z., 2000. A ﬂexible new technique for camera calibration.
IEEE Transactions on pattern analysis and machine intelligence
22 (11), 1330–1334.

Figure 14: Summary of tasks achieved on VR-Caps. VR-Caps enables to train a Deep Reinforcement Learning (DRL) based controller
to autonomously cover the organ. The reward function and training policy for such controller are given in 1-a and the resulting covered
volumes visually represented with the increasing time steps in 1-b. Green and red areas depict the covered and uncovered parts of the organ.
For pose estimation, we train two SC-SfMLearner models: one is trained with real porcine data from EndoSLAM dataset and the other one
is pre-trained with synthetic data generated on VR-Caps and then ﬁne-tuned with real data. The used data and the information about
training policies of pose estimation is given in 2-a. In 2-b and 2-c, we show the comparison results between these two models in terms of
predicted trajectory curves with ground truth and qualitative metrics (ATE, Trans. and Rot. RPE). For depth estimation, we train the
SC-SfMLearner model with only real data as in 3-a and show resulting depth maps for both virtual and real data from Kvasir dataset in 3-b
and 3-c respectively. For 3D reconstruction, we create point cloud object from synthetic and real data separately as explained in 4-a. Then,
we compare the reconstructed and ground truth maps in terms of RMSE using ICP algorithm. The pipelines and RMSE results are showed in
4-b and 4-c, respectively. For disease classiﬁcation, we created 3 classes: 2 with diseases (Polyps and Ulcer) and 1 without diseases (Normal)
and trained two ResNet-152 models (one without pre-training and the other one with virtual pre-training) as represented in 5-a. Results in
5-b and 5-c demonstrates the performance increase in the case of pre-training with VR-Caps data. For super-resolution, as shown in 6-a
we train EndoL2H network in two scenarios. In ﬁrst case, we use only real data for training. In second case, we pre-train the network with
synthetic data, then ﬁne-tune it with real data. In both cases, we use both synthetic and real data for testing, separately. We show the PSNR
and SSIM results of the model with and without pre-training in 6-b and 6-c

