JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

1

AI-based Robust Resource Allocation in
End-to-End Network Slicing under Demand and
CSI Uncertainties

Amir Gharehgoli, Ali Nouruzi, Student Member, IEEE, Nader Mokari, Paeiz Azmi, Mohamad Reza Javan, Senior
Member, IEEE, Eduard A. Jorswieck, Fellow Member, IEEE

2
2
0
2

b
e
F
0
1

]
P
S
.
s
s
e
e
[

1
v
1
3
1
5
0
.
2
0
2
2
:
v
i
X
r
a

Abstract—Network slicing (NwS) is one of the main technolo-
gies in the fifth-generation of mobile communication and
beyond (5G+). One of the important challenges in the NwS
is information uncertainty which mainly involves demand
and channel state information (CSI). Demand uncertainty is
divided into three types: number of users requests, amount
of bandwidth, and requested virtual network functions work-
loads. Moreover, the CSI uncertainty is modeled by three
methods: worst-case, probabilistic, and hybrid. In this paper,
our goal is to maximize the utility of the infrastructure
provider by exploiting deep reinforcement learning algo-
rithms in end-to-end NwS resource allocation under demand
and CSI uncertainties. The proposed formulation is a non-
convex mixed-integer non-linear programming problem. To
perform robust resource allocation in problems that involve
uncertainty, we need a history of previous information. To
this end, we use a recurrent deterministic policy gradient
(RDPG) algorithm, a recurrent and memory-based approach
in deep reinforcement learning. Then, we compare the RDPG
method in different scenarios with soft actor-critic (SAC),
deep deterministic policy gradient (DDPG), distributed, and
greedy algorithms. The simulation results show that the SAC
method is better than the DDPG, distributed, and greedy
methods, respectively. Moreover, the RDPG method out per-
forms the SAC approach on average by 70%.
Index Terms— End-to-end network slicing, Resource alloca-
tion, Software-defined networking (SDN), Network function
virtualization (NFV), Demand uncertainty, Channel state in-
formation (CSI) uncertainty, Recurrent deterministic policy
gradient (RDPG).

I. Introduction

Mobile devices are becoming a necessary part of our

everyday life, and with the increase of wireless devices,
mobile data traffic is growing exponentially [1]–[4]. To this
end, with the advent of the fifth-generation of mobile com-
munication (5G) in recent years, a wide variety of services
has emerged. In other words, 5G provides heterogeneous
services customized for users based on their specific needs.
A physical network is split into several dedicated logical
networks by network slicing (NwS) to satisfy the require-
ments of various use cases [5]. By deploying NwS, mobile

A. Gharehgoli, A. Nouruzi, N. Mokari, and P. Azmi are with the
Department of Electrical and Computer Engineering, Tarbiat Modares
University, Tehran, Iran, (e-mail:{a.gharahgoli, ali nouruzi, nader.mokari,
pazmi}@modares.ac.ir). M. R. Javan is with the Department of Electri-
cal Engineering, Shahrood University of Technology,
ja-
van@shahroodut.ac.ir). Eduard A. Jorswieck is with TU Braunschweig,
Department of Information Theory and Communication Systems, Braun-
schweig, Germany (jorswieck@ifn.ing.tu-bs.de).

(e-mail:

Iran,

network operators are able to split the physical infrastructure
into isolated virtual networks (slices), which are managed by
service providers to provide customized services. The concept
of NwS allows infrastructure providers (InPs) to provide
heterogeneous 5G services over a common platform. Slice
requests are accepted by the InP to generate revenue. The
3rd generation partnership project (3GPP) [6] and interna-
tional telecommunication union (ITU) [7] have divided all
5G slices into three categories: enhanced mobile broadband
(eMBB), ultra-reliable low latency communications (uRLLC),
and massive machine-type communication (mMTC). Each
type of slice has specific requirements. The eMBB slice needs
a high data rate and includes services such as web browsing,
video streaming, virtual reality, etc. The uRLLC slice requires
high reliability and low latency that consist of services
like cloud gaming, remote surgery, autonomous driving, etc.
The mMTC slice needs efficient connectivity for a massive
number of devices, e.g., sensor networks. The concept of end-
to-end (E2E) means that the resources of the radio access
network (RAN) and the core network are considered together
simultaneously. To maintain the quality of service (QoS)
and ensure the E2E delay of these slices, we need to apply
In 5G, software-defined
new technologies and strategies.
networking (SDN) and network function virtualization (NFV)
fill the void of programmable control and management of
network resources. Network management is easier with SDN
since it decouples the control plane from the data plane
while centralizing the network’s intelligence. NFV enables
the implementation of originally hardware-based proprietary
network functions on virtual environments. The virtual net-
work functions (VNFs) are run on virtual machines (VMs) or
containers to provide network or value-added services. Note
that they are chained together in a co-located or distributed
cloud environment [8]. SDN and NFV cause a significant
increase in the cellular network’s programmability, agility,
scalability, flexibility, and development that can reduce the
capital expense (CAPEX) and operation expense (OPEX) [9],
[10]. Every service consists of a predefined sequence of
functions, called service function chains (SFCs).
In recent years, deploying machine learning (ML) algorithms
to address the challenges of the cellular network, such as
information uncertainty (for example, channel state informa-
tion (CSI) and demand), has significantly increased. Therefore,
in this work to robust resource allocation and address uncer-
tainty issues, we exploit this algorithms to solve our system

 
 
 
 
 
 
JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

2

model.

A. Paper Organization
The rest of this paper is organized as follows. In Section II,
related works are summarized. Section III explains the system
model. Section IV introduces our problem formulation. The
several deep reinforcement learning (DRL) algorithms to
solve the proposed problem are developed in Section V.
We provide Section VI to understand the computational
complexity and convergence analysis of the solutions better.
The simulation results under various conditions are presented
in Section VII. Finally, the conclusions of this paper are
discussed in Section VIII.
Symbol Notations: To denote the vector A and the element
i-th of this vector, we use the bold upper symbol as A and
ai, respectively. Likewise, ai,j define the of matrix A. In
addition, B and bj denote the set B and the j-th element
of it, respectively. We use |c| to define absolute value of c.
To define the expectation of d, we use E[d]. Moreover, we
use Var[e] to determine the variance e.

II. Related Works

The purpose of this section is to review the related works
and categorize them based on our contribution. To this end,
we divide the related works into five categories: i) Resource
allocation problems in RAN slicing [11]–[17], ii) Resource
allocation problems in core slicing [18]–[20], iii) Resource
allocation problems in E2E slicing [21]–[26], iv) NwS prob-
lems under demand uncertainty [27]–[33], v) NwS problems
under CSI uncertainty [34]–[37]. Each of the categories is
described below. Moreover, we summarize the related works
and compare them with our work in Table. I.

A. Resource Allocation Problems in RAN Slicing
A radio resource allocation method to maximize the SLA
contract rate and maintain the isolation between the slices is
introduced in [11]. The problem is solved using the Lagrange
dual algorithm. The authors in [12] introduce the cloud
RAN (C-RAN) operator’s revenue maximization by correctly
accepting the slice requests. Two types of long-term and
short-term revenues are considered. The long-term revenue
is calculated by the values in the network slice request,
and the short-term revenue is achieved by saving system
power consumption in each frame. The optimization problem
is formulated as a mixed-integer nonlinear programming
(MINP), then to solve the problem, the authors employ
a successive convex approximation (SCA) algorithm. [13]
proposes the near-optimal low-complexity distributed RAN
slicing problem as a congestion game. The problem aims to
minimize the cost. A heuristic approach to maximize sum-
rate in a single-cell cellular network scenario is presented
in [14]. The authors formulate a resource allocation prob-
lem subject to service isolation, latency, and minimum rate
constraints. To maintain the reliability constraint, they use
adaptive modulation and coding. [15] addresses the smart
handover mechanism by employing a multi-agent Q-learning
to minimize handover cost while guaranteeing a various QoS

requirements. To compute the cost, the authors define four
handover cost types: i) the cost of switching service types
when user equipment (UE) remains in the coverage range
of the same base station (BS). ii) handover cost when a UE
leaves the coverage of a BS with the same service type. iii)
the cost associated with user movement and changing service
types. iv) the cost of implementing a new network slice to
maintain the QoS of the user handover. [16] introduce the
NwS resource allocation problem in 5G C-RAN to maximize
operators’ utility. The framework of the problem includes an
upper layer that manages the mapping of virtual protocol
stack functions; and a lower layer, which controls radio
remote unit association, power, and subchannel allocation. To
reduce the complexity of the Q-value table, the authors used
the multi-agent Q-learning approach. In [17], the authors
investigate a dynamic NwS framework for downlink multi-
tenant heterogeneous cloud RAN (H-CRAN) by considering
both small-cell and macro-cell tiers. The proposed archi-
tecture includes two-level, an upper level
for managing
admission control, baseband resource allocation, and user
association, and a lower level for handling radio resource
allocation between users. The objective of the problem is
to maximize the throughput of the tenants by considering
the constraints of QoS, fronthaul and backhaul capacities,
tenants’ priorities, baseband resources, and interference.

B. Resource Allocation Problems in Core Slicing
In [18], the authors introduced a novel joint admission and
resource management approach in multiple tenants scenario
to minimize the cost of bandwidth consumption and power
consumption cost of all turned-on cloud nodes. The main
objective of [19] is to minimize the total power consumption
of a cloud node, which consists of the static power consump-
tion and the dynamic load-dependent power consumption.
The authors consider the resource budget, functional instan-
tiation, flow routing, and guarantee the E2E latency of all
services, where E2E delay consists of total NFV delay on the
cloud nodes and total communication delay on the links. The
problem is formulated as a mixed binary linear program, then
it is solved by a heuristic approach. [20] introduces the VNF
placement problem in NwS for serving tactile applications
and routing tactile traffic. The main goal of the optimization
problem is to minimize reliability degradation in addition to
maintaining the strict delay constraint. To find sub-optimal
solutions, a Tabu search-based algorithm is used due to the
complexity of the formulated problem.
To the best of our knowledge, the objective function of most
of the papers in core slicing related to energy consumption
and are solved mainly using heuristic methods.

C. Resource Allocation Problems in E2E Slicing
Based on the deep Q-networks (DQN) algorithm, [21] in-
vestigates the dynamic resource allocation problem to max-
imize the E2E access rate for multi-slice and multi-service
scenarios. The purpose of [22] is to minimize the E2E delay
to guarantee the reliability requirements of NwS in the
uRLLC application. Based on subgraph isomorphism, the

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

3

TABLE I: Summary of the related works

Ref.
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]
Our work

Objective Function
SLA contract rate maximization
Revenue maximization
Cost minimization
Sum-rate maximization
Handover cost minimization
Utility maximization
Throughput maximization
Cost minimization
Energy consumption minimization
Cost minimization
E2E access rate maximization
Delay minimization
Utility maximization
Number of VNF migrations minimization
QoS maximization & resource consumption minimization
Resource consumption minimization
Revenue maximization
Cost minimization
Cost minimization
Power minimization
Bandwidth consumption minimization
Long-term return maximization
Utility maximization
Energy efficiency maximization
Threshold rate violation probability minimization
Power minimization
Sum-rate maximization
Utility maximization

Slicing
Domain
RAN
C-RAN
RAN
RAN
RAN
C-RAN
H-CRAN
Core
Core
Core
E2E
E2E
E2E
E2E
E2E
E2E
E2E
Core
Core
RAN
E2E
E2E
E2E
RAN
RAN
RAN
C-RAN
E2E

Optimization Algorithm
Lagrangian dual
SCA
Game theory
Heuristic
Multi-agent Q-learning
Multi-agent Q-learning
Greedy & Lagrangian dual
Heuristic
Heuristic
Tabu search
Deep Q-Learning
VF2
Primal-dual Newton
Heuristic
Heuristic
Heuristic
Q-learning
Heuristic-based on Γ-robustness
Light robustness-based on Γ-robustness
Heuristic-based on Lyapunov drift plus-penalty
Heuristic-based on variable neighborhood search
Double deep Q-learning
Iterative auction game
Iterative-based on Lagrangian dual
Deep neural network
SCA
SCA
Recurrent deterministic policy gradient

Demand
Uncertainty
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
xxx
xxx
xxx
xxx
(cid:88)

CSI
Uncertainty
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
xxx
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

authors propose a communication and computation resource
allocation algorithm. For multi-service converged infrastruc-
ture, an iterative primal-dual fast network resource slicing
algorithm is formulated in [23]. The main aim is to max-
imize the total utility and satisfy system constraints while
jointly optimizing flow routing, power slicing, and conges-
tion control. To solve the proposed optimization problem,
the authors employ the primal-dual Newton’s approach. A
mixed-integer linear programming (MILP) slice placement
problem is studied in [24] to compare and examine different
E2E slice placement approaches. The SFC is used to model
E2E slice requests in which each RAN and core network
component are represented as a VNF. The problem is about
optimizing network utilization and ensuring that the QoS
requirements of the considered slice requests are met by
the objective of minimizing the number of VNF migrations
in the network and their impacts on the slices. In [25] a
hierarchical information-centric networking system without
requiring prior knowledge on the VN’s topology and re-
source provisioning information is studied, then a heuristic
algorithm is investigated to solve the integer linear program
(ILP) problem. The proposed resource allocation problem
aims to find the best trade-off between QoS and resource con-
sumption, using VNFs and link bandwidth as resources. The
E2E shareable-VNFs-based multiple couple virtual network
embedding (VNE) problem is addressed in [26]. To minimize
the physical resources consumption, the sharing property of
VNFs is considered. The problem is modeled as ILP, and is
solved by a heuristic algorithm. The authors categorize VNF
types into shareable and non-shareable ones, then compute
the different resource requirements of them. Moreover, they
show that the slice acceptance ratio on the same physical
network using VNF-sharing can be improved.

D. NwS Problems under Demand Uncertainty
A new dynamic edge/fog NwS (EFNwS) scheme to find
an optimal slice request admission policy to maximize the
InP’s long-term revenue is proposed in [27]. Tenants can
temporarily lease back to the InP the unused resources to
serve demands exceeding its current resources in stock. A
semi-Markov decision process (SMDP) is used to model the
arrival of slice requests. A Q-learning algorithm is applied to
find the optimal policy under uncertain resource demands.
Moreover, to reduce the computational complexity of Q-
learning and improve the convergence time, a DRL algorithm
and an enhancement based on a deep dueling (Dueling DQ-
EFNwS) algorithm are applied. In [28], a novel optimization
model based on the concept of Γ-robustness to deal with
uncertainty in the traffic demand is proposed. The Γ-robust
optimization is formulated as a MILP. A modified MIP-based
variable neighborhood search (VNS) heuristic is provided to
enhance the model’s scalability. The authors in [29] introduce
a novel model applying the concept of light robustness
to address scalability issues of traffic uncertainty in NwS
and to get a deeper insight into the trade-off between the
price of robustness and the realized robustness. NwS in the
wireless system with a time-varying number of users that
require two types of slices: reliable low latency (RLL) and
self-managed (capacity limited) slices are studied in [30]. A
novel control framework for stochastic optimization based
on the Lyapunov drift-plus-penalty method is proposed. This
framework enables the system to minimize power, maintain
slice isolation, and provide reliable and low latency E2E
communication for RLL slices. Robust NwS by addressing
the slice recovery and reconfiguration with stochastic traffic
demands in each slice is introduced in [31]. For solving the
optimization problem, a heuristic algorithm based on VNS is
developed. [32] provides an optimal and fast resource slicing

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

4

solution under the uncertainty of resource demand from
tenants that maximizes the long-term return of the network
provider. An SMDP is employed to allocate resources to
users under the dynamic demands of users. A novel approach
using an advanced deep Q-learning technique called the deep
dueling algorithm is adopted to obtain the optimal resource
allocation policy for the network provider. The distributed
online approach for inter-domain resource allocation to net-
work slices in a heterogeneous multi-resource multi-domain
mobile network environment is investigated in [33]. The
main goal of the work is to maximize the utility of network
slice instances while minimizing the OPEX for infrastructure
service providers; at the same time, then; the iterative auction
game among network slice tenants is employed to solve the
proposed problem.

E. NwS Problems under CSI Uncertainty
In [34], the authors propose a robust resource allocation
in virtualized wireless networks (VWNs) to address the
uncertainty in CSI at the BS. Based on a newly defined
slice utility function, a robust resource allocation problem is
formulated, aiming to maximize the overall energy efficiency
of VWN under the worst-case CSI uncertainty. Uncertain CSI
is defined as the sum of its true estimated value, and an error
is considered to be limited in a specific uncertainty rang.
A dynamic resource allocation problem for vehicular UE
requesting eMBB and uRLLC slices is proposed in [35]. The
main objective of the problem is the minimum threshold rate
violation probability for the eMBB slice while it guarantees
a probabilistic threshold rate for the uRLLC slice. A deep
neural network (DNN) is used to estimate CSI based on the
propagation environment such as scatterers and reflectors.
In [36], the authors investigate the joint power and resource
blocks allocation to the heterogeneous users by considering
mixed numerology-based frame structures. In addition to
considering each service’s heterogeneous QoS requirements,
the authors consider each user’s queue condition when
scheduling resource blocks. The objective function of the pro-
posed problem is to minimize the overall power consumption
of the BS for each sub-frame. Outage probabilistic constraints
are included to deal with imperfect CSI. [37] presents a
novel robust radio resource allocation under worst-case CSI
uncertainty in downlink channel of a sparse code multiple ac-
cess (SCMA) based C-RAN by considering multiple-input and
single-output transmission technology. The main objective of
the suggested problem is to maximize the sum-rate under
several conditions such as user association, the minimum rate
required of each slice, maximum available power at radio
remote head (RRH), maximum fronthaul capacity of each
RRH, and SCMA constraints. A two-step iterative algorithm
based on SCA is applied to solve the formulated optimization
problem.
According to the references presented in Section II, most
studies in NwS mainly considered resources of RAN or core
[11]–[20], so there are not many E2E NwS papers and usually
they do not include demand or CSI uncertainties [21]–[26].
In addition, studies that include uncertainty address only one

type of uncertainty [27]–[37]. To the best of our knowledge,
none of the previous studies meet all the conditions set out
in our work.

F. Contributions
The main contributions of this paper are summarized as
follows:
• The resource allocation and management in NwS must be
E2E. The data rate requested by users in each slice from InP
may be stochastic. Moreover, the CSI in the BS due to dif-
ferent reasons such as mobility of users is imperfect. To this
end, in this paper, to bring the proposed problem closer to
the real scenario, we formulate a resource allocation problem
in E2E NwS under both demand and CSI uncertainties.
• The robust mathematical optimization methods and ML
algorithms are two methods that are commonly used to solve
problems that include uncertainty. We need to consider the
history of previous information. Therefore we employ the
recurrent deterministic policy gradient (RDPG) algorithm as
the main approach. Then, we compare it with soft actor-
critic (SAC), deep deterministic policy gradient
(DDPG),
distributed, and greedy algorithms from various aspects.
• Simulation results show that the SAC strategy outperform
the DDPG, distributed, and greedy algorithms, respectively.
Additionally, the RDPG approach also outperforms the SAC
method by approximately 70%. As a result, we consider the
RDPG approach the most suitable and robust method for our
proposed problem.

III. System Model

This section describes our NwS architecture and system
model. In this paper, we assume the E2E NwS with a single
InP and multiple users. The users request slices from the
InP. We consider the SDN controller to manage the network
and improve performance. The system model consists of two
parts; the RAN domain and the core domain. By backhaul
links, these two parts are connected through the transport
domain. The proposed E2E NwS is depicted in Fig. 1. In
Subsections, III-A and III-D the details of the RAN and core
network of our system model are described, respectively.
Moreover, to increase the readability of this paper, the main
parameters and variables are listed in Table II.

A. Radio Access Network Explanation
The system architecture of the RAN domain is shown in
Fig. 2. We consider the downlink transmission of an or-
thogonal frequency division multiple access (OFDMA)-based
multi-cell cellular network. The set of users is denoted by
C = {1, 2, . . . , c, . . . , C}, where C is the total number of
users, and each user sends a slice request to the InP. The
set of all BSs indicated by I = {1, 2, . . . , i, . . . , I}, where
in this set I is the total number of BS. Total bandwidth
B is divided into a set of the same subchannels K =
{1, 2, . . . , k, . . . , K}, in which that K is the total number of
subchannels. Therefore, Bk is the bandwidth of subchannel
k. According to 3GPP, there are three main slice types in
the 5G network: eMBB, uRLLC, and mMTC. Therefore, the

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

5

TABLE II: Main notations

Notation

Definition

min

C/C/c
I/I/i
K/K/k
S/S/s
Ms/Ms/ms
Bk
pi,k
ms
hi,k
ms
I i,k
ms
σ2
P i
max
˜hi,k
ms
(cid:15)i,k
ms
Γi,k
ms
˜Ri,k
ms
Rs
xi
ms
ν
w(cid:48)
s
G = (N , L)
N /N /n
L/L/l
BWn,n(cid:48)
V/V /v
Pb,b(cid:48)
Z v,v(cid:48)
b,b(cid:48)
F
qms
f
df,ms
v,b
αn,n(cid:48)
Prop
τ s
max
Ψs
Ψms
Cost.RAN
Ψb,ms
Ψn,n(cid:48),ms
Cost.core
Θ1, Θ2

Cost.core

Rev

δi
ms ∈ {0, 1}

ξi,k
ms ∈ {0, 1}

L = [ln,n(cid:48) ] ∈ {0, 1}

ζn,n(cid:48)
pb,b(cid:48) ∈ {0, 1}

βf j ,ms
v,b
ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

Υ

∈ {0, 1}

∈ {0, 1}

Input Parameters

Set/number/index of users
Set/number/index of BSs
Set/number/index of subchannels
Set/number/index of slices types
Set/number/index of users related to slice s
Bandwidth of subchannel k
Transmitted power from BS i to user ms on subchannel k
Channel gain between BS i and user ms on subchannel k
Inter-cell interference on user ms at BS i over subchannel k
Power of additive white Gaussian noise (AWGN)
Maximum transmit power of the BS i
Estimated channel gain
Error of estimation channel gain
Channel gain uncertainty bound
Transmitted data rate from BS i to user ms on subchannel k under
CSI uncertainty
Minimum required data rate of slice s
Physical distance between BS i and user ms in meter
Speed of light in meter per second
Packet size in bits
Network graph in core domain
Set/number/index of nodes
Set/number/index of links
Bandwidth capacity between nodes n and n(cid:48) in bits per second
Set/number/index of VMs
Set of all physical paths between nodes b and b(cid:48)
Set of virtual paths between VM v and v(cid:48) on nodes b and b(cid:48)
Set of all VNFs types
Corresponding processing requirement for VNF f in CPU cycle per
bit for each user ms of slice s
Processing delay of VNF f on node b for user ms of slice s
Propagation delay between nodes n and n(cid:48)
Maximum tolerable delay
Unit price of data rate
Unit cost of transmitted power by BS i
Unit cost of the node

Unit cost of the link
Scaling factors

Indicators/Variables/Matrix

Slice request indicator, that if user ms at BS i requests slice s from
InP it is 1, and otherwise 0
Subchannel allocation variable, that if BS i is allocated subchannel k
to user ms it is 1, and otherwise 0
Connectivity matrix of the graph, that if nodes n and n(cid:48) are connected
it is 1, and otherwise 0
Binary indicator variable, that if the link between nodes n and n(cid:48) is
in the path pb,b(cid:48) it is 1, and otherwise 0
Selection variable, that if VNF f j for user ms of slice s is running
on VM v in node b it is 1, and otherwise 0
Decision variable, that if physical path pb,b(cid:48) is chosen to transmit
the data for user ms from VM v to v(cid:48) it is 1, and otherwise 0

Fig. 1: Illustration of the E2E NwS structure.

set of slices is denoted by S = {1, 2, . . . , s, . . . , S}, where
S represents the total number of slices. Slice s has a set of
users Ms = {1, 2, . . . , ms, . . . , Ms}, where ms is the m-th
user in slice s and Ms is the total number of users in slice
s, therefore we have (cid:80)
s∈S Ms = C. We define a binary
as follows to determine the slice request by
indicator δi
ms
user ms from the InP:



If the user ms at BS i requests slice s,
from InP;
0, Otherwise.

δi
ms



(1)

=

1,

We assume that each user requests only one slice [38]. To
this end, we consider the following constraint to ensure that
each user can only request one slice from InP:

C1 :

(cid:88)

s∈S

δi
ms

= 1, ∀i ∈ I, ∀ms ∈ Ms.

(2)

(AWGN), and I i,k
is the inter-cell interference on user ms in
ms
BS i over subchannel k, according to the following formula:

Moreover, we define a binary variable ξi,k
ms
subchannel k by BS i to user ms as follows:

for assigning

ξi,k
ms

=






1,

If BS i allocates subchannel k,
to user ms;
0, Otherwise.

(cid:88)

(cid:88)

I i,k
ms

=

ξi(cid:48),k
m(cid:48)
s

pi(cid:48),k
m(cid:48)
s

|hi(cid:48),k
ms

|2, ∀k ∈ K.

(6)

i(cid:48)∈I
i(cid:48)(cid:54)=i

m(cid:48)
m(cid:48)

s∈Ms
s(cid:54)=ms

(3)

To guarantee that the transmit power of each BS does not
exceed its maximum transmit power P i
max, the following
constraint is proposed:

To ensure that each subchannel is allocated to just one user
in each BS, we consider the following constraint:

(cid:88)

(cid:88)

C2 :

s∈S

ms∈Ms

ξi,k
ms

(cid:54) 1, ∀i ∈ I, ∀k ∈ K.

(4)

and hi,k
ms

Let pi,k
denote the transmit power and the channel
ms
gain between BS i and user ms on subchannel k, respectively.
Accordingly, the transmit data rate from BS i to user ms on
subchannel k, is calculated as follows:

Ri,k
ms

= δi

ms

Bk log2

1 +

(cid:32)

(cid:33)

|2

ξi,k
pi,k
|hi,k
ms
ms
ms
I i,k
ms + σ2

, ∀i ∈ I,

(5)

∀k ∈ K, ∀ms ∈ Ms,

(cid:88)

(cid:88)

(cid:88)

C3 :

k∈K

s∈S

ms∈Ms

δi
ms

ξi,k
ms

pi,k
ms

(cid:54) P i

max, ∀i ∈ I.

(7)

B. Worst-case CSI Uncertainty Model
In (5), perfect CSI is considered. In wireless communication,
perfect CSI is not a valid assumption. In the BS, CSI un-
certainty can occur by various factors such as mobility of
users, estimation errors, hardware deficiencies, and delay in
the feedback channel [34], [39]. Perfect CSI at the BS is hard
to obtain. Thus, to address this issue, we consider the CSI
uncertainty at the BS. The imperfect CSI is given as follows
[40]:

where σ2 is the power of additive white Gaussian noise

hi,k
ms

= ˜hi,k
ms

+ (cid:15)i,k
ms

, ∀i ∈ I, ∀k ∈ K, ∀ms ∈ Ms,

(8)

Radio Access NetworkCore NetworkBase StationCore NodePhysical LinkFronthaul LinkE2E OrchestratorSDN ControllerJOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

6

Accordingly, by transmitting data of user ms on subchannel
k by BS i, the transmission delay is calculated as follows:

Dms

Tran.RAN =






0,
w(cid:48)
˜Ri

ms

ms

δi
ms

,

˜Ri
˜Ri

ms

ms

= 0;

> 0, ∀i ∈ I, ∀ms ∈ Ms.

(14)

Remark 1. It is worth noting that queue delay is often con-
sidered in problems involving mobile edge computing (MEC).
Since we do not have the MEC in this paper, we do not model
the queue delay in RAN and core domains in our system model
similar to the existing works [12], [25], [26].

D. Core Network Explanation
The system architecture of the core domain is shown in Fig. 3.
We consider the core network as a graph G = (N , L), where
N = {1, 2, . . . , n, . . . , N } represents the set of nodes, and
L = {1, 2, . . . , l, . . . , L} represents the set of links. Moreover,
the total number of nodes and links in these sets are denoted
by N and L, respectively. In addition, let L = [ln,n(cid:48)] be the
connectivity matrix of physical links, that is defined as:

ln,n(cid:48) =

If nodes n and n(cid:48) are connected;

(cid:40)

1,
0, Otherwise.

(15)

CPU, rn

RAM, rn

CPU, rn

RAM, and rn

Stor], where rn

We denote the total resource capacity of each node n ∈ N as
Stor indicate
rn = [rn
the CPU, RAM, and storage capacities of node n, respectively.
Moreover, we denote the limited bandwidth capacity between
nodes n and n(cid:48) in bits per second by BWn,n(cid:48). In addition,
we consider that each node b hosts several VMs which are
denoted by Vb = {1b, . . . , vb, . . . , Vb}, where Vb is the total
number of VMs in node b. Accordingly the set of total VMs
in the network is denoted by VTotal = ∪N
b=1Vb. The maximum
number of VMs on each node is indicated by Vmax. Moreover,
each VM v on node b has specific CPU, RAM, and storage
resources, that are represented by rv,b = [rv,b
Stor].
Let Pb,b(cid:48) = {1b,b(cid:48), . . . , pb,b(cid:48), . . . , Pb,b(cid:48)} be the set of physical
paths between nodes b and b(cid:48), where pb,b(cid:48) and Pb,b(cid:48) denote
the p-th path and total paths between nodes b and b(cid:48),
respectively. We define a binary indicator ζ n,n(cid:48)
as follows
pb,b(cid:48)
to determine which of the physical links are in a path:

RAM, rv,b

CPU, rv,b




1,

ζ n,n(cid:48)
pb,b(cid:48) =

If the link between nodes n and n(cid:48),
is in path pb,b(cid:48);

(16)

b,b(cid:48) , . . . , zv,v(cid:48)

b,b(cid:48) , . . . , Z v,v(cid:48)

is represented by Z v,v(cid:48)


0, Otherwise.
Furthermore,
the set of virtual paths between VMs v
and v(cid:48) on nodes b and b(cid:48)
b,b(cid:48) =
b,b(cid:48) }, where zv,v(cid:48)
{1v,v(cid:48)
is the z-th path in
this set [18], [41]. Each slice consists of several services such
as web service, voice over internet protocol (VoIP), video
streaming, cloud gaming, etc. Different VNFs must implement
these various services [42]. We consider VNFs as the network
address translator (NAT), firewall (FW), traffic monitor (TM),
wide area network (WAN) optimization controller (WOC),
intrusion detection prevention system (IDPS), and video
optimization controller (VOC). All VNFs types are denoted

b,b(cid:48)

Fig. 2: Illustration of the RAN domain in E2E NwS containing three
slices in a coverage area BSs based on OFDMA.

and (cid:15)i,k
ms

where ˜hi,k
are the estimated channel gain and
ms
error of estimation channel gain, respectively. The error of
estimated channel gain are trapped in the bounded region.
is expressed as
Thus, we have hi,k
ms
follows:

, where H i,k
ms

∈ H i,k
ms

H i,k
ms

(cid:44)

(cid:110)˜hi,k

ms

+ (cid:15)i,k
ms

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)(cid:15)i,k
ms

(cid:12)
(cid:12) (cid:54) Γi,k
ms

(cid:111)

, ∀i ∈ I, ∀k ∈ K,

(9)

ms ∈ Ms,

is the channel uncertainty bound, assumed to be
where Γi,k
ms
a small constant. Therefore, the worst-case data rate of user
ms under the CSI uncertainty can be formulated by [40]:

˜Ri

ms

=

(cid:88)

k(cid:48)∈K

min
ms ∈H i,k

ms}

{hi,k

Ri,k(cid:48)
ms

, ∀i ∈ I, ∀ms ∈ Ms.

(10)

Eventually, the total data rate of slice s can be expressed as
follows:

˜Rs =

(cid:88)

(cid:88)

˜Ri

ms

, ∀s ∈ S.

i∈I

ms∈Ms

(11)

Each slice s ∈ S requires a minimum data rate Rs
min.
Therefore, to guarantee users’ QoS, we consider the following
constraint:

(cid:88)

(cid:88)

(cid:88)

C4 :

i∈I

k∈K

ms∈Ms

δi
ms

ξi,k
ms

˜Rs (cid:62) Rs

min, ∀s ∈ S.

(12)

C. Delay Model in RAN
We consider two types of delay in the RAN domain: prop-
agation and transmission delay, which are computed in the
following.
1) Propagation Delay: We define xi
as the physical distance
between BS i and user ms in meters and ν as the speed of
light in meter per second. Therefore, the propagation delay
between BS i and user ms is given by:

ms

Dms

Prop.RAN =

xi
ms
ν

δi
ms

max
k

(cid:8)ξi,k
ms

(cid:9) , ∀i ∈ I, ∀ms ∈ Ms.

(13)

be the packet size in bits
2) Transmission Delay: Let w(cid:48)
that is equal to the number of bits transmitted in one second.

ms

Core NetworkUser related to slice 1User related to slice 2User related to slice 3Base StationUseful SignalInterference Signal Backhaul LinkJOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

7

ms of slice s. Hence, we have a set of the related processing
requirements as below:

Qms =

(cid:111)

(cid:110)

qms
f

, ∀ms ∈ Ms, ∀f ∈ Fs.

(21)

E. Delay Model in Core Network
In the following, we compute three types of delay on the
core domain: Processing, transmission, and propagation delay
[38].
1) Processing Delay: We define df,ms
as the processing delay
of VNF f on node b for user ms of slice s in VM v that is
defined as follows:
qms
ms
f
rv,b
CPU

, ∀f ∈ Fs, ∀ms ∈ Ms, ∀v ∈ Vb, ∀b ∈ N .

df,ms
v,b =

(22)
Therefore, the processing delay for user ms can be expressed
as follows:

w(cid:48)

v,b

Dms

Proc.core =

Fs(cid:88)

(cid:88)

(cid:88)

j=1

v∈V

b∈N

df j ,ms
v,b

βf j ,ms
v,b

, ∀ms ∈ Ms.

(23)

2) Propagation Delay: Let αn,n(cid:48)
between nodes n and n(cid:48). Therefore, we can write:

Prop be the propagation delay

αn,n(cid:48)
Prop =

xn,n(cid:48)
ν

, ∀n, n(cid:48) ∈ N ,

(24)

where xn,n(cid:48) is the distance of the physical link between node
n and n(cid:48). As a result, the total propagation delay for user ms
of slice s is obtained by:

Dms

Prop.core =

F Total
s −1
(cid:88)

j=1

(cid:88)

n,n(cid:48),b,b(cid:48)∈N
v,v(cid:48)∈Vb
pb,b(cid:48) ∈Pb,b(cid:48)

βf j+1,ms
v(cid:48),b(cid:48)

, ∀ms ∈ Ms.

αn,n(cid:48)
Prop ζ n,n(cid:48)

pb,b(cid:48) Υ

ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

βf j ,ms
v,b

(25)

3) Transmission Delay: The transmission delay for user ms
of slice s is formulated by:

Dms

Tran.core =

Fs(cid:88)

j=1

(cid:88)

n,n(cid:48),b,b(cid:48)∈N
v,v(cid:48)∈Vb
pb,b(cid:48) ∈Pb,b(cid:48)

˜wms
BWn,n(cid:48)

ζ n,n(cid:48)
pb,b(cid:48) Υ

ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

βf j ,ms
v,b

(26)

βf j+1,ms
v(cid:48),b(cid:48)
where ˜wms

, ∀ms ∈ Ms,
is the uncertain data rate in bits per second.

F. E2E Delay Model
Based on Subsections III-C and III-E, the total E2E delay for
each user ms is obtained as follows:

Proc.core + Dms

Prop.core + Dms

Tran.core+

(27)

Dms
Dms

Total = Dms
Prop.RAN + Dms

Tran.RAN, ∀ms ∈ Ms.

To make sure that the total E2E delay for each user ms is
less than the maximum tolerable delay time τ s
max, we use the
following constraint:

C7 : Dms

Total (cid:54) τ s

max, ∀ms ∈ Ms, ∀s ∈ S.

(28)

Fig. 3: Illustration of the core domain in E2E NwS based on Abilene
network topology with 12 nodes and 15 links from SNDlib [43].

s , f 2

s , . . . , f j

s , . . . , F Total

}, where F Total

by F = {1, 2, . . . , f, . . . , F }, Where F is the total VNF
types. For each slice s ∈ S, several VNFs must be run
in VMs. Thus, the VNF chain of slice s is represented by
is the total
Fs = {f 1
s
number of VNFs required for slice s. The order of running
of VNFs for each slice s ∈ S is already known. Let βf j ,ms
be the binary decision variable to determine that the j-th
function for user ms of slice s is running on VM v in node
b, that we define it as follows:



If VNF f j for user ms of slice s,
is running on VM v in node b;

(17)

βf j ,ms
v,b =

1,

v,b

s



0, Otherwise.

We consider the following constraint to ensure that each VNF
completely runs or is assigned to one VM:

(cid:88)

(cid:88)

C5 :

v∈V

b∈N

βf j ,ms
v,b = 1, ∀f j ∈ Fs, ∀ms ∈ Ms.

(18)

To map the virtual path zv,v(cid:48)
b,b(cid:48)

to the physical path pb,b(cid:48), we

define a binary decision variable Υ

ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

as below:

ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

Υ

=





1,

If the physical path pb,b(cid:48) is chosen to,
transmit the data rate for user ms,
of slice s from VM v to v(cid:48);

0, Otherwise.

(19)

For each virtual path just one physical path is selected.
Accordingly, we define the following constraint:

C6 :

(cid:88)

ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

Υ

pb,b(cid:48) ∈Pb,b(cid:48)

= 1, ∀ms ∈ Ms, ∀zv,v(cid:48)

b,b(cid:48) ∈ Z v,v(cid:48)
b,b(cid:48) .

(20)

In addition, because each VNF needs corresponding compu-
tation resources, let qms
denote the corresponding processing
requirement for VNF f in CPU cycle per bit for each user

f

Node 1Node 6Node 8Node 2Node 10Node 11Node 7Node 9Node 5Node 3Node 4Node 12 SDN Controller SDN Controller SDN ControllerPhysical Node/ServerRouterVirtual MachinePhysical LinkControl LinkJOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

8

G. Demand Uncertainty Model
In the proposed system model, we assume that the number
of users requests for each slice can be random. In addition,
the data rate requests in each slice can be stochastic too. For
this purpose, the data rate requested of virtual links (VLs)
by user ms in each slice is denoted by ˜wms
that it is a
random variable with a uniform distribution and considered
refers to the
as ˜wms ∈ [ ¯wms − ˆwms, ¯wms + ˆwms], where ¯wms
nominal bandwidth demand values or the mean of ˜wms
, that
is denoted by E[ ˜wms] = ¯wms
(cid:62) 0 is the maximum
bandwidth demand deviation or the standard deviation of ˜ws,
. Note that there is no
that is indicated by Var[ ˜wms] = ˆw2
ms
information about the exact value of ˜wms
; we only know its
mean and variance values [44]. We can model the demand
uncertainty as follows [29], [45]:

, and ˆwms

(cid:88)

(cid:88)

(cid:88)

(cid:88)

ms∈Ms

n∈N

n(cid:48)∈N

b∈Pb,b(cid:48)

ζ n,n(cid:48)
pb,b(cid:48) Υ

ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

δi
ms

˜wms

(cid:54) BWn,n(cid:48),

(29)

b,b(cid:48) ∈ Z v,v(cid:48)
∀i ∈ I, ∀pb,b(cid:48) ∈ Pb,b(cid:48), zv,v(cid:48)
b,b(cid:48) .
Note that (29) has a stochastic variable where we can not
solve directly. To this end, the stochastic variable is changed
into the deterministic variable. Therefore, we have the fol-
lowing constraint [45]:

C8 :

(cid:88)

(cid:88)

(cid:88)

(cid:88)

δi
ms

ζ n,n(cid:48)
pb,b(cid:48) Υ

ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

¯wms+ (30)

ms∈Ms

n(cid:48)∈N

b∈Pb,b(cid:48)

n∈N
(cid:40)

(cid:88)

max
|κms |(cid:54) ˜wms

(cid:41)

κms ˆwms

(cid:88)

(cid:88)

(cid:88)

ζ n,n(cid:48)
pb,b(cid:48) Υ

ms,zv,v(cid:48)
b,b(cid:48)
pb,b(cid:48)

δi
ms

ms∈Ms

n∈N

n(cid:48)∈N

b∈Pb,b(cid:48)

(cid:54) BWn,n(cid:48), ∀i ∈ I, ∀pb,b(cid:48) ∈ Pb,b(cid:48), zv,v(cid:48)

b,b(cid:48) ∈ Z v,v(cid:48)
b,b(cid:48) ,

where κms
is a auxiliary variable. In reality, constraint C8 is
for the mean of the requested data rate plus the maximum
standard deviation of the requested data rate to be less than
the available capacity.

IV. Problem Formulation

We aim to maximize the utility of the InP where each slice
request is handled by the InP. In the following subsections,
we formulate the revenue, cost, and utility functions, respec-
tively.

A. Revenue Function Model
Let Ψs
Rev represent the unit price of the data rate for slice s,
that has dimension $/Mbps. If the InP accepts the user’s slice
request and allocates the data rate, it can generate revenue.
To this end, the revenue function Us

Rev is obtained by:

Us

Rev = Ψs

Rev ˜Rs, ∀s ∈ S.

(31)

1) Cost Function in RAN: Let Ψms
Cost.RAN be the unit cost of
the transmitted power by BS on each subchannel for user
ms of slice s with dimension $/Watt/Hz. We denote the
cost function in RAN domain by Us
Cost.RAN, and it can be
formulated as follows:

Us

Cost.RAN =

(cid:88)

(cid:88)

ms∈Ms

k∈K

ξi,k
ms

pi,k
ms

Ψms

Cost.RAN, ∀i ∈ I.

(32)

Cost.core and Ψn,n(cid:48),ms
2) Cost Function in Core Network: Let Ψb,ms
Cost.core
be the unit cost of the node and link, respectively. Accord-
ingly, we define Us
Cost.core as the cost of utilization of network
resources and includes the node and link bandwidth in the
core domain. Thus, it can be expressed as follows:

Us

Cost.core =

(cid:88)

w(cid:48)

ms

ms∈Ms
b∈N
v∈V
f ∈Fs

βf,ms
v,b qms

f Ψb,ms

Cost.core+

(33)

s,zv,v(cid:48)
pb,b(cid:48) βf,ms
b,b(cid:48)

v,b βf (cid:48),ms

v(cid:48),b(cid:48) ζ n,n(cid:48)

pb,b(cid:48) Ψn,n(cid:48),ms
Cost.core .

(cid:88)

w(cid:48)

ms

Υ

ms∈Ms
n,n(cid:48),b,b(cid:48)∈N
pb,b(cid:48) ∈Pb,b(cid:48)

3) Total Cost Function: Finally, based on (32) and (33), we can
derive the total cost of slice s in our network as follows:

Us

Cost = Us

Cost.RAN + Us

Cost.core, ∀s ∈ S.

(34)

C. Utility Function Model
The utility function Us is obtained from the difference
between revenue and cost functions in the RAN and core
domains. Therefore, it is computed as follows:

Us = Θ1Us

Rev − Θ2Us

Cost, ∀s ∈ S.

(35)

where Θ1, Θ2 (cid:62) 0 are scaling factors and are used for
balancing and scaling the revenue and costs of different
resource types, respectively. At this point, based on the
description in the previous sections, the optimization problem
in E2E NwS under demand and CSI uncertainties can be
written as follows:

max
k,p,β,Υ

(cid:88)

Us

s∈S

s.t. C1 − C8
C9 : ξi,k
ms
C10 : βf j ,ms
v,b
∀v ∈ Vb, ∀b ∈ N .

∈ {0, 1}, ∀i ∈ I, ∀k ∈ K, ∀ms ∈ Ms.
∈ {0, 1}, ∀f j ∈ F, ∀ms ∈ Ms,

s,zv,v(cid:48)
pb,b(cid:48) ∈ {0, 1}, ∀s ∈ S, ∀zv,v(cid:48)
b,b(cid:48)

b,b(cid:48) ∈ Z v,v(cid:48)
b,b(cid:48) ,

C11 : Υ
∀pb,b(cid:48) ∈ Pb,b(cid:48),

(36a)

(36b)

(36c)

(36d)

(36e)

B. Cost Function Model
In this paper, we consider two types of cost for RAN and core
domains in our system model, that in the following parts the
detail of them is introduced.

where in Section III the details of constraints C1-C8 are
studied. In addition, constraints C9-C11 are used to ensure
that decision variables are binary. The proposed problem for-
mulation (36a)-(36e) is a difficult non-convex mixed-integer
non-linear programming.

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

9

V. Solution

Because of the complexity of the problem presented in
Section IV, we apply a robust method to solve our problem.
In problems that include information uncertainty, mainly
heuristic methods, Γ-robustness, and ML algorithms are used.
In recent years, the use of ML algorithms to meet various
challenges in mobile networks to improve performance and
compatibility has increased significantly. In addition, many
recent studies show that ML-based resource allocation is
more effective than conventional methods [46], [47]. To this
end, we employ several DRL algorithms [48] to solve our
resource allocation problem. Model-free DRL algorithms such
as DQN and double DQN are value-based methods where the
Q-values are estimated with lower variance. These methods
can not be used for problems with continuous action spaces.
The best way to deal with this challenge is to apply policy
gradient-based RL methods that can handle problems with
continuous action space by learning deterministic/stochastic
policies. The goal of these techniques is to optimize a policy
based on the gradient of the expected reward. Nevertheless,
these methods have very slow convergence. The deterministic
policy gradient (DPG) algorithm [49] uses a learned ap-
proximation of the action-value (Q) function to approximate
action-value gradients. The deep DPG (DDPG) [50] is an off-
policy algorithm that uses the actor-critic method [51] to
manage the continuous action spaces and to find the solution;
this method needs a large number of training episodes, like in
model-free RL algorithms. Based on DPG, DDPG employs a
parameterized actor function to deterministically map states
to specific actions while keeping DQN learning as the critic
[52]. To address the slow convergence problem, the DDPG
combines both features of policy-based and value-based al-
gorithms to handle the continuous and large state/action
spaces. [53] developed the DDPG approach to recurrent
DPG (RDPG) by adding long short-term memory (LSTM)1
[54] to solve the problems with continuous action spaces
under partial observation. In partially observable systems
the agent does not have the full state information (i.e.,
channel gains and bandwidth demand in our problem). In
uncertain systems, the agent knows the information with
bounded error. Due to uncertainties in the proposed problem
formulation, we employ the RDPG algorithm as the main
approach. RDPG exploits recurrent neural networks (RNNs)
feed-forward networks. In other words, by using the RNN
instead of the feed-forward approach, we are able to learn
from history. Moreover, we consider the SAC [55], DDPG,
distributed, and greedy [56] algorithms as baselines.

A. RDPG Algorithm
We consider a standard DRL setup in which the agent
interacts with the environment E in discrete time slots.
Note that each time slot t is equal to one second. The
MDP [57] is a sequential decision-making process suitable
for a fully observed and stochastic environment with additive

rewards and the Markovian transition model. The typical
RL problem is modeled as the MDP. In the fully observed
MDP, when we have access to state st, the action-value
function is expressed as the expected future discounted re-
ward. Because the accurate state information is not available
in the partially-observed MDP (POMDP) [58], it employs
knowledge of actions and observations from previous time-
steps to improve current observations. In this paper, we
model our environment as the POMDP. The DRL algorithm
includes an agent, a set of environment states S, a set of
actions A, an initial state distribution p0(s0), a transition
function p(st+1|st, at), and the reward function r(st, at).
In every time slot t the agent receives an observation ot,
performs an action at and gets a reward rt. Because the
it receives
agent is unable to observe state st directly,
observations from the set O conditioned on the underlying
state p(ot, st). In principle, the optimal agent may need
access to the entire histories of observations represented by
ht = (o1, a1, o2, a2, . . . , ot−1, at−1, ot). Therefore, the goal
of the agent is to learn a policy π(ht) that maps from the
history to the distribution of actions P (A), and it maximizes
the expected discounted reward. In the RDPG method, the
policy is dependent on the whole history. The optimal policy
and the associated action-value function are functions of the
entire preceding observation-action history ht.
According to our optimization problem, we define the agent,
the state space S, the action space A, and the reward function
r as follows:
• Agent: In our model, the SDN controller is assumed as the
agent to select the actions form action spaces by considering
network states. The agent receives a reward for each chosen
action, and then the network’s state changes to the next state
as time evolves. If the agent can not satisfy the constraints
(36b)-(36e), it will be punished with a negative reward. Also,
if the agent does the good action, it will receive a positive
reward.
• System States: Learning decisions are made based on the
system state, which is an abstraction of the environment.
The channel gain and bandwidth are the most significant
parameters on the state of our system model environment.
Thus, the system state St defined as the channel gain, link
bandwidth, and uncertain data rate requested that can be
expressed as:

St = (cid:0)hi,k
ms

, BWn,n(cid:48), ˜wms

(cid:1) .

(37)

• Action: Since the network operates in a new state and
transients from the current state of the network, the learner
takes action based on its state. The action space A includes
the transmit power from BS i to user ms of slice s on
subchannel k, all the subchannels, paths, nodes, and VMs on
the nodes. Therefore, the set of all actions can be expressed
as:

At = (cid:0)pi,k
ms

, K, Pb,b(cid:48), N , VTotal

(cid:1) .

(38)

1LSTM is an artificial RNN structure that is used in deep learning.
LSTM includes feedback connections, unlike standard feedforward neural
networks.

• Reward Function: The agent receives the reward af-
ter taking action, which further reward improves network

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

10

Algorithm 1: RDPG Algorithm [53]
1 Input: Initialize weights of actor and critic networks,
µθ(ht) and Qω(ht, at), with parameters θ and ω.

2 Input: Initialize target network weights of actor and critic
networks, Qω(cid:48) and µθ(cid:48) , with weights ω(cid:48) ← ω, θ(cid:48) ← θ

3 Input: Initialize the replay buffer B
4 for episodes=1 to E do
5

Initialize empty history h0
for t=1 to T do

Receive observation ot
Add previous action and observation to history
(ht ← ht−1, at−1, ot)
Based on the history at the time slot t take action
at = µθ(ht) + (cid:15) ((cid:15): exploration noise)
Store the sequence (o1, a1, r1, . . . , oT , aT , rT ) in B
Sample a minibatch of N episodes
1, . . . , oi
1, ri
(oi
T , ri
Build histories hi
1, ai
Calculate the target values for each sample episode
T ) using the recurrent target networks:
(yi

T )i=1,...,N from B
1, . . . , ai

T , ai
t = (oi

1, . . . , yi

t−1, oi
t)

1, ai

t = ri
yi

t+1, µθ(cid:48) (cid:16)
t + γQω(cid:48) (cid:16)
hi
hi
t+1

(cid:17)(cid:17)

Calculate critic update using backpropagation through

time (BPTT):

∆ω =

1
N T

(cid:88)

(cid:88)

(cid:16)

t − Qω (cid:16)
t, ai
hi
yi
t

i

t

(cid:17)(cid:17) ∂Qω(hi
∂ω

t, ai
t)

Calculate actor update by using BPTT:

∆θ =

1
N T

(cid:88)

(cid:88)

i

t

∂Qω (cid:0)hi

t, µθ (cid:0)hi
∂a

t

(cid:1)(cid:1)

(cid:1)

∂µθ (cid:0)hi
∂θ

t

Update actor and critic using Adam optimizer [59]
Update the actor and critic target networks with the
period τ :

ω(cid:48) ← τ ω + (1 − τ )ω(cid:48)
θ(cid:48) ← τ θ + (1 − τ )θ(cid:48)

6

7

8

9

10

11

12

13

14

15

16

17

performance. In our optimization problem, the goal is to
maximize the utility of the InP. Hence, the reward function
is denoted as follows:

rt (st, at) = uUs,

(39)

where u is a coefficient factor. Moreover, the state of the
system will change based on the actions that users take
according to the system state. For example, positive rewards
will be awarded if the agent takes a good action; otherwise,
negative rewards will be awarded.
The following formula is considered to maximize the dis-
counted expected cumulative reward:

where the trajectory τ is computed from the trajectory
distribution influenced by policy π as follows:

π : p(s1)

L
(cid:89)

i=1

p(oi|si)π(ai|hi)p(si+1|si, ai).

(42)

In the employed algorithm, we use the action-value function
Qπ. Therefore, Qπ in terms of h is expressed as follows:

Qπ (ht, at) = Est|ht [rt (st, at)] +
(cid:35)
γir (st+i, at+i)

Eτ >t|ht,at

(cid:34) L
(cid:88)

,

(43)

i=1

where τ > t = (st+1, ot+1, at+1, . . . ) is the future trajectory
after t. The policy is updated as below:

∂J (θ)
∂θ

= Eτ





∞
(cid:88)

t=1

γt−1 ∂Qω (ht, a)

∂a

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a=µθ(ht)



 ,

∂µθ (ht)
∂θ

(44)

where Qω is a recurrent network with parameters ω. Algo-
rithm 1 is proposed to better understand the concept of the
RDPG method.

B. DDPG Algorithm
The DDPG method is a model-free, and off-policy-based RL
approaches where is more suitable for large and continuous
state and action spaces. Based on actor-critic structures, this
method uses DNNs as function approximators to specify
deterministic policies that can map large discrete or continu-
ous states into continuous actions [60]. In the DPG method,
there are the actor (policy π) and critic (value function Q)
networks with parameters θµ and θQ, respectively, as well as
two copies of actor and critic are denoted by parameters θµ(cid:48)
and θQ(cid:48), respectively [61]. The Q function is updated using
temporal-difference methods, similar to DQN. The policy
gradient algorithm is applied to update the actor’s value
through the value from the critic. In this approach, we
consider st = ot. The state’s return is calculated as the sum
of discounted future reward as follows:

Rt =

T
(cid:88)

i=t

γ(i−t)r (si, ai) .

(45)

It is essential to keep in mind that the return depends
on the actions taken, and thus on the policy π, and may
be stochastic. In RL, the purpose is to learn a policy that
maximizes the expected return from the start distribution
J = Eri,si∼E,ai∼π[R1]. The Bellman equation is employed
to learn the action-value function Q(s, a|θQ) as in the DQN
approach; therefore, we have:

(cid:34) ∞
(cid:88)

(cid:35)
γt−1r (st, at)

,

J = Eτ

t=1

(40)

Qπ (st, at) =
Ert,st+1∼E

(cid:2)r (st, at) + γEat+1∼π [Qπ (st+1, at+1)](cid:3) .

(46)

where γ ∈ [0, 1] is a discount factor, and τ is set of a
trajectories with length L denoted by:

τ = (s1, o1, a1, s2, o2, a2, . . . , sL, oL, aL),

(41)

Here, the target policy is deterministic; we can define it as
a function µ : S ← A and ignore the inner expectation of
(46). The expectation is only affected by the environment. In
other words, the action-value function can be learned using

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

11

6

7

8

9

10

11

12

13

14

15

Algorithm 2: DDPG Algorithm [50]
1 Input: Initialize actor µ (s|θµ) and critic network

Q (cid:0)s, a|θQ(cid:1) with weights θµ and θQ, with random value.

2 Input: Initialize target network µ(cid:48) and Q(cid:48), with weights

θµ(cid:48)

← θµ, θQ(cid:48)

← θQ

3 Input: Initialize the replay buffer B
4 for episodes=1 to E do
5

Initialize an action exploration process using a random
process (cid:15)
Receive the initial observation state s1
for t=1 to T do

Based on the current policy and exploration noise
at the time slot t, take action at = µ (st|θµ) + (cid:15)t
Take action at and receive reward rt and observe
new state st+1
Store transition (st, at, rt, st+1) in B
Sample a random minibatch of N transitions
(si, ai, ri, si+1) from B
(cid:12)θQ(cid:48) (cid:17)
si+1, µ(cid:48) (cid:16)
Set yi = ri + γQ(cid:48) (cid:16)
Minimize the loss function to update the critic:

(cid:12)θµ(cid:48) (cid:17) (cid:12)
(cid:12)
(cid:12)

si+1

L =

1
N

(cid:88)

(cid:16)

yi − Q

(cid:16)

si, ai

(cid:12)θQ(cid:17)(cid:17)2
(cid:12)

i

Using the sampled policy gradient, update the actor

policy:

∇θµ J ≈
1
(cid:88)
N

i

∇aQ

(cid:16)

s, a(cid:12)

(cid:12)θQ(cid:17) (cid:12)
(cid:12)
(cid:12)s=si,a=µ(si)

∇θµ µ (cid:0)s(cid:12)

(cid:12)θµ(cid:1) (cid:12)
(cid:12)si

Update the target network parameters:

θQ(cid:48)
θµ(cid:48)

← τ θQ + (1 − τ )θQ(cid:48)
← τ θµ + (1 − τ )θµ(cid:48)

transitions generated from a different stochastic behavior
policy κ. We assume function approximators parameterized
by θQ, which we optimize by minimizing the loss function
as follows:

L (cid:0)θQ(cid:1) = Est∼ρκ ,at∼κ,rt∼E

(cid:104)(cid:0)Q (cid:0)st, at|θQ(cid:1) − yt

(cid:1)2(cid:105)

.

(47)

where yt is:

(48)

yt = r (st, at) + γQ (cid:0)st+1, µ (st+1) |θQ(cid:1) ,
Although yt is likewise dependent on θQ, this is usually
ignored. The parameterized actor function µ(s|θµ) is main-
tained in the DPG method, which is used to specify the
current policy. The Bellman equation is applied to learn the
critic Q(s, a) as in the Q-learning approach. To update the
actor, a chain rule is applied to the expected return from the
start distribution J based on actor parameters; therefore, we
can write:

(cid:104)
∇θµQ (cid:0)s, a|θQ(cid:1) (cid:12)

∇θµ J ≈ Est∼ρκ
(cid:104)

Est∼ρκ

∇aQ (cid:0)s, a|θQ(cid:1) (cid:12)

(cid:12)s=st,a=µ(st|θµ)
(cid:12)s=st,a=µ(st)∇θµµ (s|θµ) (cid:12)

(cid:105)

= (49)
(cid:105)

(cid:12)s=st

.

Since samples are not independently and identically dis-
tributed in most optimization algorithms, the DDPG method

uses a replay buffer B to address this challenge similar
to DQN. In the replay buffer, the cache size is limited.
Transitions are sampled from the environment based on the
exploration policy, and the tuple (st, at, rt, st+1) is collected
in the replay buffer. The oldest samples are removed from
the replay buffer when it is full. The actor and critic update
using sampling a minibatch uniformly of the buffer in every
time slot. To compute the target values, we create copies
of the actor and critic networks, Q(cid:48)(s, a|Qθ(cid:48)
),
respectively. By tracking the learned networks slowly, the
target networks’ weights can be updated: θ(cid:48) ← τ θ +(1−τ )θ(cid:48)
with τ (cid:28) 1. As a result, the learning stability is improving
because the target values can only change slowly. A noise
sampled from a noise process (cid:15) is added to actor policy to
build an exploration policy µ(cid:48); therefore, we have:

) and µ(cid:48)(s|θµ(cid:48)

µ(cid:48) (st) = µ (st|θµ

t ) + (cid:15),

(50)

where (cid:15) can be selected according to the environment.
The complete pseudo-code of the DDPG approach shows in
Algorithm 2.

C. SAC Algorithm
The SAC is the off-policy actor-critic DRL algorithm based on
the maximum entropy RL framework for continuous action
spaces. In other words, the optimal policy of this approach
is to maximize its entropy-regularized reward rather than to
maximize the discounted cumulative reward. According to
this framework, the actor wants to maximize the expected
reward in addition to maximize entropy. In this method,
the infinite-horizon MDP is defined by the tuple (S, A, p, r)
in which the state space S and the action space A are
continuous, and the unknown state transition probability
p : S × S × A → [0, ∞) indicates the probability density of
the next state st+1 based on the current state st and action
at. We employ ρπ(st) and ρπ(st, at) to describe the state and
state-action marginals of the trajectory distribution induced
by a policy π(at|st). To maximize the entropy, the following
formula is considered:

J (π) =

T
(cid:88)

t=0

E(st,at)∼ρπ [r (st, at) + ϕH (π (·|st))] ,

(51)

where ϕ is a regularization coefficient. To ensure that en-
tropies and the sum of expected rewards are finite, the
objective using discount factor γ is extended to infinite
horizon problems. Soft policy iteration is extended by the
SAC method to the setting with function approximation. In
order to improve the policy, the SAC employs a different
optimization on both the policy and the value function, rather
than estimating the true Q value of the policy π. We assume
a parametrized Q function Qφ(s, a) and policy πθ. Moreover,
, where parameter ˜φ is
a target Q network is defined as Q ˜φ
computed as an exponentially moving average of φ. Through
minimizing the soft Bellman residual, the Q function can be
learned as follows:

JQ (φ) =
(cid:20)(cid:16)

E

Q (st, at) − r (st, at) − γEst+1

(52)

(cid:104)

V ˜φ (st+1)

(cid:105)(cid:17)2(cid:21)

,

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

12

Algorithm 3: SAC Algorithm [55]
1 Hyperparameters: Step sizes λπ, λQ, λϕ, target entropy e,

exponentially moving average coefficient τ

2 Input: Initial Q value function parameters φ1 and φ2
3 Input: Initial policy parameters θ
4 B = ∅; ˜φ = φ, for i ∈ {1, 2}
5 for each iteration do
6

for each environment step do

7

8

9

10

11

12

13

14

at ∼ πθ (·|st)
st+1 ∼ p (st+1|st, at)
B ← B ∪ {st, at, r (st, at) , st+1}

for each gradient step do
θ ← θ − λπ∇θJπ (θ)
φi ← φi − λQ∇JQ (φi) for i ∈ {1, 2}
ϕ ← ϕ − λϕ∇J (ϕ)
˜φi ← τ ˜φi + (1 − τ ) φi for i ∈ {1, 2}

where V ˜φ(s) is:

V ˜φ (s) = Eπθ

(cid:105)
(cid:104)
Q ˜φ (s, a) − ϕ log πθ (a|s)

.

(53)

Furthermore, by minimizing the expected KL-divergence, we
can learn policy πθ:

Jπ (θ) = Es∼B [Ea∼πθ [ϕ log πθ (a|s) − Qφ (s, a)]] ,

(54)

where B represents the set of previous sampled states and
actions or the replay buffer. To reduce the biased Q value
problem, the SAC employs two Q-networks (also two target
Q-networks), i.e. Qφ(s, a) = min(Qφ1(s, a), Qφ2 (s, a)). It
is possible to optimize Jπ(θ) in several different ways. A
likelihood ratio gradient estimator [62] is a common solution
for policy gradient approaches because it does not need to
use backpropagating the gradient through the target density
networks and the policy. However, the target density in the
SAC is the Q-function described by a neural network, and we
can use the reparameterization trick for the policy network,
which often leads to a lower variance estimator. To this end,
we reparameterize the policy πθ utilizing a neural network
transformation, that gets both the state s and noise vector (cid:15)
as an input, as follows:

a = fθ (s, (cid:15)) .

(55)

Therefore, based on (55), we can rewrite (54) as follows:

Jπ (θ) = Es∼B,(cid:15)∼N [ϕ log πθ (fθ (s, (cid:15)) |s) − Qφ (s, fθ (s, (cid:15)))] ,
(56)

where N is a standard Gaussian distribution, and πθ is
defined in terms of fθ implicitly. Lastly, the SAC gives a
method to automatically update the regularization coefficient
ϕ through minimizing the loss function as follows:

J (ϕ) = Ea∼πθ [−ϕ log πθ (a|s) − ϕe] ,

(57)

where e is a hyperparameter that represents the target
entropy. In Algorithm 3, a complete description of the SAC
method is given.

D. Distributed Algorithm
In this method, we consider two agents for the SAC algo-
rithm, one for the RAN domain and the other for the core
domain. These two agents have no interaction with each
other. At first, the radio agent solves the RAN problem, and
then the core agent solves the core problem. In other words,
the main difference between the distributed strategy and
the multi-agent approach is that in the multi-agent method,
agents interact with each other, but not in the distributed
way.

VI. Computational Complexity and Convergence
Analysis

Computational complexity and convergence are two essential
criteria in solving optimization problems. On the one hand, in
DRL algorithms, one method may have a high computational
complexity, which increases the run time of the simulation.
But it is possible; it has better performance than other meth-
ods and has a faster convergence rate. To better understand
the subject, more details are provided in the following two
subsections.

A. Computational Complexity
In this subsection, we investigate the computational complex-
ity of the main method and the baselines. The computational
complexity of the proposed system model
is one of the
significant and practical factors. Therefore, we examine two
aspects of the computational complexity of our proposed
system model: the action selection process and the training
process.
To compute the computational complexity, we need to define
several parameters. For this purpose, we define the total
number of episodes by NEpisod, the total number of neural
network layers by Z, the total number of hidden layers by
H, and the total number of neurons in each layer by Y .
Accordingly, the number of neurons in the z-th layer is
denoted by Yz. Also, |S| and |A| are the size of the total
state and action spaces, respectively.
Moreover, for the distributed approach, we define state and
action of RAN domain by SRAN and ARAN, respectively, and
for the core domain, we define state and action by SCore and
ACore, respectively.
1) Computational Complexity of Action Selection: Accord-
ingly, the computational complexity of neural network-based
algorithms depends on the network structure and its layers.
The complexity of back-propagation on a fully connected
neural network depends on the multiplication of the input,
hidden layers and, output. The RDPG method consists of one
actor and one critic neural network. Due to the use of L
previous trajectories for action selection, the production of
the sizes of each two consecutive layers of actor and critic
can be computed by (58), (59), respectively.

, . . . , H 2
L × (|S| + |A|) × H
(cid:124)(cid:123)(cid:122)(cid:125)
(cid:125)
(cid:123)(cid:122)
(cid:124)
Layer h
Layer 1

,
, . . . , H × |A|
(cid:124) (cid:123)(cid:122) (cid:125)
Layer H

(58)

L × (|S| + 2 × |A|) × H
(cid:124)
(cid:123)(cid:122)
(cid:125)
Layer 1

, . . . , H 2
(cid:124)(cid:123)(cid:122)(cid:125)
Layer h

, . . . , H × |A|
(cid:124) (cid:123)(cid:122) (cid:125)
Layer H

.

(59)

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

13

Hence, in the RDPG algorithm, the computational complexity
of action selection is O(H 2).
2) Computational Complexity of Training: The training pro-
cess complexity in the RDPG approach with H hidden layers
with Y neurons is calculated as follows [63]:
O (cid:0)BL × (|S| + 2 × |A|) Y H (cid:1) ,
where B is the size of the training batch. In addition, to better
understand the differences between the solutions used, Table
III compares the computational complexity of the RDPG
method with the baselines.

(60)

TABLE III: Comparison of the computational complexity of RDPG
method with baselines

Algorithm
RDPG

DDPG

SAC

Distributed

O

O

O
(cid:16)

Computational complexity
O (cid:0)BL × (|S| + 2 × |A|) Y H (cid:1)

(cid:16)(cid:16)(cid:80)Z−1
(cid:16)(cid:16)(cid:80)Z−1

z=0 Yz × Yz+1
z=0 Yz × Yz+1

(cid:17)

× B × NEpisod

(cid:17)

× B × NEpisod

(cid:17)

(cid:17)

O (|SRAN| × Y2 + Y2 × Y3 + Y3 × |ARAN|) ,

Greedy

O (|SCore| × Y2 + Y2 × Y3 + Y3 × |ACore|)
(cid:16)
|N | log|N |

I + |S| ×

+ S × F

O

(cid:17)

(cid:16)

2 +|L|

(cid:17)

(cid:17)

B. Convergence Analysis
In this subsection, we examine the convergence of the RDPG
algorithm and other approaches. In the Q-learning algorithm,
the Q-function can converge to the optimal Q-function as
t → ∞ with probability 1, if actor and critic network learning
rate α(cid:48) and α(cid:48)(cid:48) are deterministic, non-increasing, and satisfy
the following formulas [64]:

∞
(cid:88)

t=0

α(cid:48)(cid:48)

t = ∞,

∞
(cid:88)

t=0

(α(cid:48)(cid:48)

t )2 < ∞,

∞
(cid:88)

t=0

α(cid:48)

t = ∞,

∞
(cid:88)

t=0

(α(cid:48)

t)2 < ∞,

lim
t→∞

α(cid:48)
α(cid:48)(cid:48) = 0.

(61)

(62)

Also |rt(st, at)| be bounded [65]. We employ an inverse
time decaying learning rate to achieve fast convergence and
effectively train DNN; that in the early episodes, it utilizes the
large learning rate to avoid the network from getting trapped
in a bad local optimum. Moreover, to converge to a good
local optimum, it applies the small learning rate in the last
training episodes [66]. Fig. 4 shows the mean episodic reward
versus episode. This figure shows the simulation results for 20
users when we set the value of CSI and demand uncertainty
bound to be 5% and 10%, respectively. As can be seen,
the RDPG method converges faster than other methods in
terms of convergence. Given that the RDPG is memory-
based, it does not have good results in the early episodes
because the history contains little information and is not
enough. But, over time and increasing episodes, the agent
gets better rewards by exploiting the history. Moreover, in
the DRL algorithms that we use (i.e., RDPG, SAC, DDPG,
and distributed), action with noise is selected. Hence in the
early episodes, the greedy way is better than other methods.
But as the number of episodes increases, the performance of
this method worsens compared to other approaches.

Fig. 4: Mean episodic reward versus episode

VII. Simulation Results

In this section, we first introduce the simulation environment,
then we examine the simulation results in terms of different
aspects between the RDPG approach and other methods.

A. Simulation Environment
Here, we describe the simulation environment used for eval-
uating the efficiency of employed algorithms. To simulate
the proposed problem, we use the programming language,
Python 3.8.12, and compiler, Spyder 5.1.5. Simulations are run
on a personal computer with 8 cores, 3.8GHz Intel Core i7-
10700K CPU, and 16GB RAM. In the RAN domain, the users
are uniformly distributed in a square area 1000m×1000m
with 4 BSs where the maximum power of each BS is 4 watts
(36.02 dBm). We consider 10 subchannels with the frequency
bandwidth of 200 kHz, and the minimum data rate of the
downlink is equal to 1 bps/Hz. In the core domain, we use
the Abilene network topology with 12 nodes and 15 links
from SNDlib2 [67], [68], that we used the NetworkX libraries3
[70] in Python to implement the graph G = (N , L) similar to
the existing work [71]. Tensorflow and PyTorch libraries4 in
Python with Adam5 optimizer are applied to implement the
DNN. We used Tensorflow 2.6.1 and Torch 1.4.0 for simulation.
At the start of the simulation, we randomly select several
nodes from the network nodes to set the ingress and egress
nodes for the set of slices S. The proposed system model has
6 VNFs where each VNF needs a different processing time.
Furthermore, each node can host at most 6 VMs, and each
VM can host a maximum of 6 VNFs. We set capacities of
bandwidth and memory of each physical link and node to
1 Gbps and 1 Gbyte, respectively. Additionally, CPU, RAM,
and storage capacities are set to 1200 CPU cycle/Hz, 1000
Mb, and 1000 Mb, respectively. Moreover, more details of
the simulation parameters are listed in Table IV. To better

2SNDlib [43] is a library of test cases for the design of survivable fixed

telecommunication networks.

3NetworkX [69] is a Python package for building, manipulating, and

studying complex networks’ structure, dynamics, and functions.

4TensorFlow and PyTorch are open-source ML libraries used to develop

and train neural network-based deep learning models.

5Adam is an alternative optimization method to stochastic gradient

descent for training deep learning algorithms.

05001000150020002500300035004000Episode050010001500200025003000MeanepisodicrewardRDPGSACDDPGDistributedGreedyJOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

14

understand the proposed problem, the source code of the
simulation of the main approach and baselines are available
in [72].

TABLE IV: Simulation parameters

Parameter

Description

Value

C
I
K
S
B
Bk
hi,k
ms
σ2
P i
max
Γi,k
ms
Rs
min
ν
N
L
VTotal
F
τ s
max
ˆws
θ1
θ2

NEpisod
B
τ
H
Y
-
-
α(cid:48)
α(cid:48)(cid:48)
γ
B

E2E NwS environment

Number of total users
Number of total BSs (cells)
Number of total subchannels
Number of total slice types
Total available bandwidth
Bandwidth of each subchannel
Channel gain
Power of AWGN
Maximum transmitted power by each BS
CSI uncertainty bound (to percentage)
Minimum required data rate of slice s
Speed of light in meter per second
Number of total nodes
Number of total links
Number of total VMs
Set of all VNFs types
Maximum tolerable delay time
Demand uncertainty bound (to percentage)
Revenue scaling factor
Cost scaling factor

Deep neural network

Number of episodes
Batch size
Target network update period
Number of hidden layers
Number of neurons in each hidden layer
Activation function in hidden layers
Activation function in output layer
Actor network learning rate
Critic network learning rate
Discount factor
Replay buffer size

24
4 [22]
10
3 [6], [7]
200 KHz
20 KHz
Rayleigh fading [36]
-174 dBm/Hz [37]
4 Watt (36.02 dBm)
0, 2, 4, 6, 8, 10 [36], [37]
1, 1.2, 1.4, 1.6, 1.8, 2, 3, 4, 5 bps/Hz
3 × 108 m/s
12 [43], [67], [68]
15 [43], [67], [68]
6
6 [42]
60, 100, 200, 300, 400, 500 ms [42]
0, 5, 10, 15, 20, 25, 30 [45]
60
1

4000
64
0.001
2
512
ReLU [63]
tanh [63]
0.00001
0.00005
0.80
600000

B. Performance of Simulation Results and Metrics
We analyze the impact of the main parameters, such as
the number of users, demand uncertainty, CSI uncertainty,
maximum tolerable delay time, and minimum required data
rate on different baseline algorithms.
1) Effect of Number of Users: In Fig. 5, the utility of the InP
versus number of users is depicted. As the number of users
increases, the data rate allocated increases. Therefore, the InP
utility increases because InP revenue comes from the data
rate. However, due to the limited network resources, after
the number of users changes from 22 to 24, InP no longer
accepts new users, therefore the utility remains constant. As
shown in Figure 4, the RDPG algorithm performs an average
of 65% better than the SAC. In this scenario, we consider the
value of CSI and demand uncertainty bound to be 5% and
10%, respectively.

Fig. 5: Utility of the InP versus number of users.

2) Effect of Demand Uncertainty: To investigate the effect
of demand uncertainty on the objective function of our
proposed problem, we keep the value of CSI uncertainty
constant at 2% and change the value of demand
bound Γi,k
ms
uncertainty bound ˆwms
in the range 0% to 30%. We only
know the expectation and variance of each user’s requested
data rate, and we do not know the exact amount of data
rate requested in each slice. Therefore, we need a history of
the average variance of previous user requests. As shown
in Fig. 6, as the demand uncertainty bound increases, the
InP’s utility decreases. The RDPG strategy is more powerful
than other methods in this scenario. This is because this
algorithm has a history and is suitable for problems that
include uncertain information, such as our work.

Fig. 6: Utility of the InP versus demand uncertainty bound.

In this scenario, we keep the
3) Effect of CSI Uncertainty:
constant at 5%
value of demand uncertainty bound ˆwms
and check the InP’s utility by increasing the CSI uncertainty
bound value Γi,k
in a range of 0% to 10%. The channel
ms
gain has a direct impact on the data rate formula. As the
CSI uncertainty bound increases, the amount of data rate
allocated to the user in the RAN domain decreases. In our
proposed problem, the InP revenue is directly related to the
data rate, so the revenue decreases, and as a result, the
utility decreases. On average, the RDPG method performs
70% better than the SAC method in this scenario. This is
clearly shown in Fig. 7. Comparing Fig. 6 and Fig. 7, it can
be concluded that CSI uncertainty has a more significant
impact on utility than demand uncertainty and has a more
destructive effect.
4) Effect of Maximum Tolerable Delay Time: Delay must be
guaranteed for each slice. If the InP fails to guarantee a user’s
expected delay in each slice, that user will not accept, and InP
will reject it. In this scenario, CSI and demand uncertainty
bound values are fixed to 4% and 5%, respectively, and we
max from
increase the value of maximum tolerable delay time τ s
60 ms to 500 ms. As shown in Fig. 8, as the amount of
tolerable delay increases, the number of requests accepted
by the InP increases; therefore, the utility is increases.
5) Effect of Minimum Required Data Rate: Here, we limit CSI
and demand uncertainty bound to 2% and 5%, respectively,
and then we increase the value of the minimum required
min from 1 bps/Hz to 5 bps/Hz. In Fig. 9, the
data rate Rs

24681012141618202224Number of users050010001500200025003000Utility of the InP ($)RDPGSACDDPGDistributedGreedy65 %051015202530Demand uncertainty bound (to percentage)50010001500200025003000UtilityoftheInP($)RDPGSACDDPGDistributedGreedy70%JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

15

Fig. 7: Utility of the InP versus CSI uncertainty bound.

Fig. 10: Cost of the InP versus minimum required data rate.

Fig. 8: Utility of the InP versus maximum tolerable delay time.

Fig. 11: Utility of the InP versus minimum required data rate.

average sum data rate versus minimum data rate required
min increases, the average sum
is plotted. As shown, when Rs
data rate decreases. This is because when Rs
min is increased,
more subchannels must be assigned to users to satisfy the
minimum required data rate, especially for users with poor
channel conditions in an extremely deep fade.

Fig. 9: Average sum data rate versus minimum required data rate.

Additionally, by increasing the value of Rs
min, the number of
users accepted by InP decreases, therefore the cost of the
network decrease. Fig. 10 shows the details of this issue.

min, average
Finally, as shown in Fig. 11, since increasing the Rs
sum data, and cost rate is reduced, therefore the utility
function, which is the product of the difference between
revenue and cost, decreases. To better understand the effect
of the minimum required data rate, in addition to using Fig. 9
and Fig. 10, we use Table.V and Table.VI to show the decrease
of average sum data rate and cost of the InP, respectively.
In this scenario, as in the previous cases, the RDPG algorithm
is more robust than the other methods.

C. Comparison of Signaling Overhead in the Main Approach
and the Baselines
four employed DRL methods to perform actions,
In all
the agents require information like the status and received
rewards. Therefore, this information must be intercommuni-
cated between the E2E orchestrator, RAN, and core domains.
In the centralized approaches we use (i.e., RDPG, SAC, and
DDPG), all the information is concentrated in one place,
which causes the signaling overhead to increase. But in the
distributed method, part of the information is dissolved in
the RAN domain, and another part of the information is
dissolved in the core domain, so the signaling overhead of the
distributed way is less than the centralized way. However,
the distributed approach is less performance. To calculate
and analyze this information, we consider that each element
of the matrices of the channel gain, node, and link can be

012345678910CSI uncertainty bound (to percentage)50010001500200025003000UtilityoftheInP($)RDPGSACDDPGDistributedGreedy70 %60100200300400500Maximum tolerable delay time (ms)6008001000120014001600180020002200240026002800UtilityoftheInP($)RDPGSACDDPGDistributedGreedy80 %11.21.41.61.82345Minimum required data rate (bps/Hz)0510152025303540455055606570Average sum data rate (bps/Hz)RDPGSACDDPGDistributedGreedy11.21.41.61.82345Minimum required data rate (bps/Hz)0501001502002503003504004505005506006507007508008509009501000CostoftheInP($)RDPGSACDDPGDistributedGreedy11.21.41.61.82345Minimum required data rate (bps/Hz)60080010001200140016001800200022002400260028003000UtilityoftheInP($)RDPGSACDDPGDistributedGreedy70 %JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

16

TABLE V: Average sum data rate versus minimum required data rate.

Minimum required data rate (bps/Hz)

Average sum data rate (bps/Hz)

RDPG
SAC
DDPG
Distributed
Greedy

1
60
29
28
26
23

1.2
58
28.5
27
25.5
22.5

1.4
56
28
26.5
25
22

1.6
54
27.5
26
24.5
21.5

1.8
52
27
25.5
23.5
21

2
50
26.5
25
23
20.5

3
48
26
24.5
22.5
20

4
48
25.5
24
22.5
20

5
48
25.5
24
22.5
20

TABLE VI: Cost of the InP versus minimum required data rate.

Minimum required data rate (bps/Hz)

Cost of the InP ($)

RDPG
SAC
DDPG
Distributed
Greedy

1
800
290
380
310
180

1.2
770
280
370
300
170

1.4
740
270
360
290
160

1.6
710
260
350
280
150

1.8
690
250
340
270
140

2
660
240
330
260
130

3
650
230
320
250
120

4
650
220
310
250
120

5
650
220
310
250
120

decoded as a fixed-length 16-bit binary string. Accordingly,
we use a type ’float16’ in Python’s Numpy6 library. In the
RAN domain in our proposed system model with the I BSs,
C users, and K subchannels, the total signaling overhead in
each episode is equal to:

results, the SAC method is better than the DDPG, distributed,
and greedy approaches, respectively. In addition, the RDPG
strategy outperforms the SAC approach by, on average, 70%.
Therefore, the RDPG method is robust for our proposed
problem and is considered as the main method.

16 × I × C × K bits

(63)

Moreover, in the core domain with the N nodes, VTotal VMs,
and L links, the total signaling overhead in each episode is
equal to:

16 × ((N × VTotal) + L) bits

(64)

Therefore, in the distributed method, the amount of signaling
overhead in the radio and core parts is equal to formulas
(63) and (64), respectively. But in the centralized approaches,
signaling overhead is equal to the summation of formulas
(63) and (64). In the end, to better understand the subject,
we compare the total signaling overhead in each episode for
the RDPG approach and the baselines in Table.VII.

TABLE VII: Total signal overhead of the algorithms

Algorithm
RDPG
SAC
DDPG

Distributed

Total signaling overhead
16 × I × C × K + 16 × ((N × VTotal) + L) bits
16 × I × C × K + 16 × ((N × VTotal) + L) bits
16 × I × C × K + 16 × ((N × VTotal) + L) bits

RAN domain
16 × I × C × K bits

Core domain
16 × ((N × VTotal) + L) bits

VIII. Conclusion

We studied a resource allocation problem in E2E NwS based
on the SDN and NFV concepts by considering uncertainties
in the number of slice requests from users, data rate requests
in each slice, and CSI. We formulated the utility function of
the InP as the difference between revenue and cost in the
network architecture. The proposed problem was formulated
as non-convex mixed-integer non-linear programming. Due
to the complexity of the problem and many actions and
states space, we employed several DRL algorithms. Because
of the uncertainties in the problem, we considered the RDPG
method as the main solution and compared it with other
methods under various aspects. According to the simulation

6NumPy is a library for the Python programming language used to perform

an extensive range of mathematical operations on arrays.

References

[1] I. Union, “IMT traffic estimates for the years 2020 to 2030,” Report ITU,

vol. 2370, 2015.

[2] X. Foukas, G. Patounas, A. Elmokashfi, and M. K. Marina, “Network
slicing in 5G: Survey and challenges,” IEEE Communications Magazine,
vol. 55, no. 5, pp. 94–100, 2017.

[3] U. Habiba and E. Hossain, “Auction mechanisms for virtualization
in 5G cellular networks: basics, trends, and open challenges,” IEEE
Communications Surveys & Tutorials, vol. 20, no. 3, pp. 2264–2293, 2018.

[4] F. Debbabi, R. Jmal, L. C. Fourati, and A. Ksentini, “Algorithmics and
Modeling Aspects of Network Slicing in 5G and Beyonds Network:
Survey,” IEEE Access, vol. 8, pp. 162748–162762, 2020.

[5] J. Zhou, W. Zhao, and S. Chen, “Dynamic Network Slice Scaling
Assisted by Prediction in 5G Network,” IEEE Access, vol. 8, pp. 133700–
133712, 2020.

[6] A. Dogra, R. K. Jha, and S. Jain, “A survey on beyond 5G network
with the advent of 6G: Architecture and emerging technologies,” IEEE
Access, vol. 9, pp. 67512–67547, 2020.

[7] M. A. Habibi, M. Nasimi, B. Han, and H. D. Schotten, “A comprehensive
survey of RAN architectures toward 5G mobile communication system,”
IEEE Access, vol. 7, pp. 70371–70421, 2019.

[8] I. Afolabi, T. Taleb, K. Samdanis, A. Ksentini, and H. Flinck, “Network
slicing and softwarization: A survey on principles, enabling technolo-
gies, and solutions,” IEEE Communications Surveys & Tutorials, vol. 20,
no. 3, pp. 2429–2453, 2018.

[9] V.-G. Nguyen, A. Brunstrom, K.-J. Grinnemo, and J. Taheri, “SDN/NFV-
based mobile packet core network architectures: A survey,” IEEE
Communications Surveys & Tutorials, vol. 19, no. 3, pp. 1567–1602, 2017.

[10] J. Ordonez-Lucena, P. Ameigeiras, D. Lopez, J. J. Ramos-Munoz, J. Lorca,
and J. Folgueira, “Network slicing for 5G with SDN/NFV: Concepts,
architectures, and challenges,” IEEE Communications Magazine, vol. 55,
no. 5, pp. 80–87, 2017.

[11] L. Zhou, T. Zhang, J. Li, and Y. Zhu, “Radio Resource Allocation
for RAN Slicing in Mobile Networks,” in 2020 IEEE/CIC International
Conference on Communications in China (ICCC), pp. 1280–1285, IEEE,
2020.

[12] J. Tang, B. Shim, and T. Q. Quek, “Service multiplexing and revenue
maximization in sliced C-RAN incorporated with URLLC and multicast
eMBB,” IEEE Journal on Selected Areas in Communications, vol. 37, no. 4,
pp. 881–895, 2019.

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

17

[13] S. D’Oro, F. Restuccia, T. Melodia, and S. Palazzo, “Low-complexity
distributed radio access network slicing: Algorithms and experimental
results,” IEEE/ACM Transactions on Networking, vol. 26, no. 6, pp. 2815–
2828, 2018.

[30] A. T. Z. Kasgari and W. Saad, “Stochastic optimization and control
framework for 5G network slicing with effective isolation,” in 2018 52nd
Annual Conference on Information Sciences and Systems (CISS), pp. 1–6,
IEEE, 2018.

[14] P. Korrai, E. Lagunas, S. K. Sharma, S. Chatzinotas, A. Bandi, and
B. Ottersten, “A RAN resource slicing mechanism for multiplexing of
eMBB and URLLC services in OFDMA based 5G wireless networks,”
IEEE Access, vol. 8, pp. 45674–45688, 2020.

[15] Y. Sun, W. Jiang, G. Feng, P. V. Klaine, L. Zhang, M. A. Imran, and Y.-C.
Liang, “Efficient handover mechanism for radio access network slicing
by exploiting distributed learning,” IEEE Transactions on Network and
Service Management, vol. 17, no. 4, pp. 2620–2633, 2020.

[16] X. Wang and T. Zhang, “Reinforcement learning based resource allo-
cation for network slicing in 5g c-ran,” in 2019 Computing, Communi-
cations and IoT Applications (ComComAp), pp. 106–111, IEEE, 2019.

[31] R. Wen, G. Feng, J. Tang, T. Q. Quek, G. Wang, W. Tan, and S. Qin, “On
robustness of network slicing for next-generation mobile networks,”
IEEE Transactions on Communications, vol. 67, no. 1, pp. 430–444, 2018.

[32] N. Van Huynh, D. T. Hoang, D. N. Nguyen, and E. Dutkiewicz, “Optimal
and fast real-time resource slicing with deep dueling neural networks,”
IEEE Journal on Selected Areas in Communications, vol. 37, no. 6,
pp. 1455–1470, 2019.

[33] J. Khamse-Ashari, G. Senarath, I. Bor-Yaliniz, and H. Yanikomeroglu,
“An agile and distributed mechanism for inter-domain network slicing
in next-generation mobile networks,” IEEE Transactions on Mobile
Computing, 2021.

[17] Y. L. Lee, J. Loo, T. C. Chuah, and L.-C. Wang, “Dynamic network
slicing for multitenant heterogeneous cloud radio access networks,”
IEEE Transactions on Wireless Communications, vol. 17, no. 4, pp. 2146–
2161, 2018.

[34] V. Jumba, S. Parsaeefard, M. Derakhshani, and T. Le-Ngoc, “Energy-
efficient robust resource provisioning in virtualized wireless networks,”
in 2015 IEEE International Conference on Ubiquitous Wireless Broadband
(ICUWB), pp. 1–5, IEEE, 2015.

[18] S. Ebrahimi, A. Zakeri, B. Akbari, and N. Mokari, “Joint Resource and
Admission Management for Slice-enabled Networks,” in NOMS 2020-
2020 IEEE/IFIP Network Operations and Management Symposium, pp. 1–
7, IEEE, 2020.

[35] H. Khan, M. M. Butt, S. Samarakoon, P. Sehier, and M. Bennis, “Deep
learning assisted csi estimation for joint urllc and embb resource
allocation,” in 2020 IEEE International Conference on Communications
Workshops (ICC Workshops), pp. 1–6, IEEE, 2020.

[19] W.-K. Chen, Y.-F. Liu, A. De Domenico, and Z.-Q. Luo, “Network slicing
for service-oriented networks with flexible routing and guaranteed E2E
latency,” in 2020 IEEE 21st International Workshop on Signal Processing
Advances in Wireless Communications (SPAWC), pp. 1–5, IEEE, 2020.

[36] P. K. Korrai, E. Lagunas, A. Bandi, S. K. Sharma, and S. Chatzinotas,
“Joint Power and Resource Block Allocation for Mixed-Numerology-
Based 5G Downlink Under Imperfect CSI,” IEEE Open Journal of the
Communications Society, vol. 1, pp. 1583–1601, 2020.

[20] N. Promwongsa, M. Abu-Lebdeh, S. Kianpisheh, F. Belqasmi, R. H.
Glitho, H. Elbiaze, N. Crespi, and O. Alfandi, “Ensuring Reliability and
Low Cost When Using a Parallel VNF Processing Approach to Embed
Delay-Constrained Slices,” IEEE Transactions on Network and Service
Management, vol. 17, no. 4, pp. 2226–2241, 2020.

[21] T. Li, X. Zhu, and X. Liu, “An End-to-End network slicing algorithm
based on deep Q-learning for 5G network,” IEEE Access, vol. 8,
pp. 122229–122240, 2020.

[22] Z. Tong, T. Zhang, Y. Zhu, and R. Huang, “Communication and Compu-
tation Resource Allocation for End-to-End Slicing in Mobile Networks,”
in 2020 IEEE/CIC International Conference on Communications in China
(ICCC), pp. 1286–1291, IEEE, 2020.

[23] S.-C. Lin, “End-to-end network slicing for 5G&B wireless software-
defined systems,” in 2018 IEEE Global Communications Conference
(GLOBECOM), pp. 1–7, IEEE, 2018.

[24] D. Harutyunyan, R. Fedrizzi, N. Shahriar, R. Boutaba, and R. Riggio,
“Orchestrating End-to-end Slices in 5G Networks,” in 2019 15th Interna-
tional Conference on Network and Service Management (CNSM), pp. 1–9,
IEEE, 2019.

[25] J. Liu, B. Zhao, M. Shao, Q. Yang, and G. Simon, “Provisioning Opti-
mization for Determining and Embedding 5G End-to-End Information
Centric Network Slice,” IEEE Transactions on Network and Service
Management, 2020.

[26] C. Mei, J. Liu, J. Li, L. Zhang, and M. Shao, “5G network slices
embedding with sharable virtual network functions,” Journal of Com-
munications and Networks, vol. 22, no. 5, pp. 415–427, 2020.

[27] H. Esmat and B. Lorenzo, “Deep Reinforcement Learning based Dy-
namic Edge/Fog Network Slicing,” in GLOBECOM 2020-2020 IEEE Global
Communications Conference, pp. 1–6, IEEE, 2020.

[28] V. S. Reddy, A. Baumgartner, and T. Bauschert, “Robust embedding
of VNF/service chains with delay bounds,” in 2016 IEEE conference
on network function virtualization and software defined networks (NFV-
SDN), pp. 93–99, IEEE, 2016.

[29] A. Baumgartner, T. Bauschert, A. A. Blzarour, and V. S. Reddy, “Network
slice embedding under traffic uncertainties—A light robust approach,”
in 2017 13th International Conference on Network and Service Manage-
ment (CNSM), pp. 1–5, IEEE, 2017.

[37] M. Moltafet, S. Parsaeefard, M. R. Javan, and N. Mokari, “Robust radio
resource allocation in MISO-SCMA assisted C-RAN in 5G networks,”
IEEE Transactions on Vehicular Technology, vol. 68, no. 6, pp. 5758–5768,
2019.

[38] A. Nouruzi, A. Zakeri, M. R. Javan, N. Mokari, R. Hussain, and A. S.
Kazmi, “Online Service Provisioning in NFV-enabled Networks Using
Deep Reinforcement Learning,” arXiv preprint arXiv:2111.02209, 2021.

[39] E. Bj¨ornson, G. Zheng, M. Bengtsson, and B. Ottersten, “Robust
monotonic optimization framework for multicell MISO systems,” IEEE
Transactions on Signal Processing, vol. 60, no. 5, pp. 2508–2523, 2012.

[40] A. Zakeri, A. Khalili, M. Javan, N. Mokari, and E. A. Jorswieck, “Robust
Energy-Efficient Resource Management, SIC Ordering, and Beamform-
ing Design for MC MISO-NOMA Enabled 6G,” IEEE Transactions on
Signal Processing, 2021.

[41] M. M. Tajiki, S. Salsano, L. Chiaraviglio, M. Shojafar, and B. Akbari,
“Joint energy efficient and QoS-aware path allocation and VNF place-
ment for service function chaining,” IEEE Transactions on Network and
Service Management, vol. 16, no. 1, pp. 374–388, 2018.

[42] M. Savi, M. Tornatore, and G. Verticale, “Impact of processing-resource
sharing on the placement of chained virtual network functions,” IEEE
Transactions on Cloud Computing, 2019.

[43] “What is SNDlib?.” http://sndlib.zib.de/home.action.

[44] F. Hosseini, A. James, and M. Ghaderi, “Probabilistic virtual

link
embedding under demand uncertainty,” IEEE Transactions on Network
and Service Management, vol. 16, no. 4, pp. 1552–1566, 2019.

[45] S. Gholamipour, B. Akbari, N. Mokari, M. M. Tajiki, and E. A. Jorswieck,
“Online Admission Control and Resource Allocation in Network Slicing
under Demand Uncertainties,” arXiv preprint arXiv:2108.03710, 2021.

[46] V. Franc¸ois-Lavet, P. Henderson, R.

Islam, M. G. Bellemare, and
J. Pineau, “An introduction to deep reinforcement learning,” arXiv
preprint arXiv:1811.12560, 2018.

[47] L. Liang, H. Ye, G. Yu, and G. Y. Li, “Deep-learning-based wireless
resource allocation with application to vehicular networks,” Proceedings
of the IEEE, vol. 108, no. 2, pp. 341–356, 2019.

[48] H. Dong, H. Dong, Z. Ding, S. Zhang, and Chang, Deep Reinforcement

Learning. Springer, 2020.

JOURNAL OF IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

18

[70] A. Hagberg, D. Schult, P. Swart, D. Conway, L. S´eguin-Charbonneau,
C. Ellison, B. Edwards, and J. Torrents, “Networkx. High productivity
software for complex networks,” Webov´a str´a nka https://networkx. lanl.
gov/wiki, 2013.

[71] X. Fu, F. R. Yu, J. Wang, Q. Qi, and J. Liao, “Dynamic service function
chain embedding for NFV-enabled IoT: A deep reinforcement learning
approach,” IEEE Transactions on Wireless Communications, vol. 19, no. 1,
pp. 507–519, 2019.

[72] A. Gharehgoli, A. Nouruzi, N. Mokari, P. Azmi, M. R. Javan, and
E. Jorswieck, “Codes of paper: AI-based Robust Resource Allocation
in End-to-End Network Slicing under Demand and CSI Uncertainties.”
https://dx.doi.org/10.21227/4jps-kt78, 2022.

[49] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
“Deterministic policy gradient algorithms,” in International conference
on machine learning, pp. 387–395, PMLR, 2014.

[50] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,
and D. Wierstra, “Continuous control with deep reinforcement learn-
ing,” arXiv preprint arXiv:1509.02971, 2015.

[51] V. R. Konda and J. N. Tsitsiklis, “Actor-critic algorithms,” in Advances

in neural information processing systems, pp. 1008–1014, 2000.

[52] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement
learning for multiagent systems: A review of challenges, solutions, and
applications,” IEEE transactions on cybernetics, vol. 50, no. 9, pp. 3826–
3839, 2020.

[53] N. Heess, J. J. Hunt, T. P. Lillicrap, and D. Silver, “Memory-based control
with recurrent neural networks,” arXiv preprint arXiv:1512.04455, 2015.

[54] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[55] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic
actor,” in International conference on machine learning, pp. 1861–1870,
PMLR, 2018.

[56] S. Agarwal, F. Malandrino, C.-F. Chiasserini, and S. De, “Joint VNF
placement and CPU allocation in 5G,” in IEEE INFOCOM 2018-IEEE
Conference on Computer Communications, pp. 1943–1951, IEEE, 2018.

[57] R. Bellman, “A Markovian decision process,” Journal of mathematics

and mechanics, vol. 6, no. 5, pp. 679–684, 1957.

[58] K. J. ˚Astr¨om, “Optimal control of Markov processes with incomplete
state information I,” Journal of Mathematical Analysis and Applications,
vol. 10, pp. 174–205, 1965.

[59] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[60] M. Akbari, M. R. Abedi, R. Joda, M. Pourghasemian, N. Mokari, and
M. Erol-Kantarci, “Age of Information Aware VNF Scheduling in
Industrial IoT Using Deep Reinforcement Learning,” IEEE Journal on
Selected Areas in Communications, 2021.

[61] R. Hafner and M. Riedmiller, “Reinforcement learning in feedback

control,” Machine learning, vol. 84, no. 1-2, pp. 137–169, 2011.

[62] R. J. Williams, “Simple statistical gradient-following algorithms for
connectionist reinforcement learning,” Machine learning, vol. 8, no. 3,
pp. 229–256, 1992.

[63] S. Sheikhzadeh, M. Pourghasemian, M. R. Javan, N. Mokari, and E. A.
Jorswieck, “AI-Based Secure NOMA and Cognitive Radio enabled
Green Communications: Channel State Information and Battery Value
Uncertainties,” IEEE Transactions on Green Communications and Net-
working, 2021.

[64] I. Grondman, L. Busoniu, G. A. Lopes, and R. Babuska, “A survey
of actor-critic reinforcement learning: Standard and natural policy
gradients,” IEEE Transactions on Systems, Man, and Cybernetics, Part
C (Applications and Reviews), vol. 42, no. 6, pp. 1291–1307, 2012.

[65] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,

no. 3-4, pp. 279–292, 1992.

[66] K. You, M. Long, J. Wang, and M. I. Jordan, “How does learning rate
decay help modern neural networks?,” arXiv preprint arXiv:1908.01878,
2019.

[67] S. Orlowski, M. Pi´oro, A. Tomaszewski, and R. Wess¨aly, “SNDlib 1.0–
Survivable Network Design Library,” in Proceedings of the 3rd Inter-
national Network Optimization Conference (INOC 2007), Spa, Belgium,
April 2007. http://sndlib.zib.de, extended version accepted in Networks,
2009.

[68] S. Orlowski, M. Pi´oro, A. Tomaszewski, and R. Wess¨aly, “SNDlib 1.0–
Survivable Network Design Library,” Networks, vol. 55, no. 3, pp. 276–
286, 2010.

[69] “NetworkX: Network Analysis in Python.” https://networkx.org/.

