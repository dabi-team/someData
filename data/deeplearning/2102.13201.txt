Learning Controller Gains on Bipedal Walking Robots
via User Preferences

Noel Csomay-Shanklin1, Maegan Tucker2, Min Dai2, Jenna Reher2, Aaron D. Ames1,2

2
2
0
2

r
a

M
2

]

O
R
.
s
c
[

2
v
1
0
2
3
1
.
2
0
1
2
:
v
i
X
r
a

Abstract— Experimental demonstration of complex robotic
behaviors relies heavily on ﬁnding the correct controller gains.
This painstaking process is often completed by a domain
expert, requiring deep knowledge of the relationship between
parameter values and the resulting behavior of the system. Even
when such knowledge is possessed, it can take signiﬁcant effort
to navigate the nonintuitive landscape of possible parameter
combinations. In this work, we explore the extent to which
preference-based learning can be used to optimize controller
gains online by repeatedly querying the user for their prefer-
ences. This general methodology is applied to two variants of
control Lyapunov function based nonlinear controllers framed
as quadratic programs, which provide theoretical guarantees
but are challenging to realize in practice. These controllers are
successfully demonstrated both on the planar underactuated
biped, AMBER, and on the 3D underactuated biped, Cassie.
We experimentally evaluate the performance of the learned
controllers and show that the proposed method is repeatably
able to learn gains that yield stable and robust locomotion.

I. INTRODUCTION

Achieving robust and stable performance for physical
robotic systems relies heavily on careful gain tuning, re-
gardless of the implemented controller. Navigating the space
of possible parameter combinations is a challenging en-
deavor, even for domain experts. To combat this challenge,
researchers have developed systematic ways to tune gains
for speciﬁc controller types [1]–[4]. For controllers where
the input/output relationship between parameters and the
resulting behavior is less clear, this can be prohibitively
difﬁcult. These difﬁculties are especially prevalent in the
setting of bipedal locomotion, due to the extreme sensitivity
of the stability of the system with respect to controller gains.
It was shown in [5] that control Lyapunov functions
(CLFs) are capable of stabilizing locomotion through the
hybrid zero dynamics (HZD) framework, with [6] demon-
strating how this can be implemented as a quadratic program
(QP), allowing the problem to be solved in a pointwise-
optimal fashion even in the face of feasibility constraints.
However, achieving robust walking behavior on physical
bipeds can be an arduous process due to complexities such
as compliance, under-actuation, and narrow domains of at-
traction. One such controller that has recently demonstrated
stable locomotion on the 22 degree of freedom (DOF) Cassie
biped, as shown in Fig. 1, is the ID-CLF-QP+ [7].

This research was supported by NSF NRI award 1924526, NSF award
1932091, NSF CMMI award 1923239, NSF Graduate Research Fellowship
No. DGE-1745301, and the Caltech Big Ideas and ZEITLIN Funds.

1Authors are with the Department of Computing and Mathematical

Sciences, California Institute of Technology, Pasadena, CA 91125.

2Authors are with the Department of Mechanical and Civil Engineering,

California Institute of Technology, Pasadena, CA 91125.

Fig. 1: The two experimental platforms investigated in this
work: the planar AMBER-3M point-foot robot [8] (left), and
the 3D Cassie robot [9] (right).

Synthesizing a controller capable of accounting for the
challenges of underactuated locomotion, such as the ID-
CLF-QP+, necessitates the addition of numerous control
parameters, exacerbating the issue of gain tuning. Moreover,
the relationship between the control parameters and the
resulting behavior of the robot
is extremely nonintuitive
and results in a landscape that requires dedicated time
to navigate, even for domain experts. Recently, machine
learning techniques have been implemented to alleviate the
process of hand-tuning gains in a controller agnostic way by
systematically navigating the entire parameter space [10]–
[12]. More speciﬁcally, Bayesian optimization techniques
have been applied to learning gait parameters and controller
gains for various bipedal systems [13], [14]. However, these
techniques rely on a carefully constructed predeﬁned reward
function. Furthermore,
is often the case that different
desired properties of the robotic behavior are conﬂicting and
therefore can’t be simultaneously optimized.

it

To alleviate the gain tuning process and enable the use
of complicated controllers for na¨ıve users, we employ a
preference-based learning framework that only relies on
subjective user feedback, mainly pairwise preferences, to
systematically search the parameter space and realize stable
and robust experimental walking. Preferences are a par-
ticularly useful feedback mechanism for parameter tuning
because they are able to capture the notion of “general
goodness” without a predeﬁned reward function. This is
particularly important for bipedal locomotion due to the lack
of commonly agreed upon numerical metric of good or even
stable walking in the community [15]–[18].

 
 
 
 
 
 
The mappings f : T Q → Rn and g : T Q → Rn×m are
assumed to be locally Lipschitz continuous.

Dynamic and underactuated walking consists of periods of
continuous motion followed by discrete impacts, which can
be accurately modeled within a hybrid framework [20]. If we
consider a bipedal robot undergoing domains of motion with
only one foot in contact (either the left (L) or right (R)), and
domain transition triggered at footstrike, then we can deﬁne:

D{L,R}

swf (q, ˙q) < 0},

SS = {(q, ˙q) : pz
SL→R,R→L = {(q, ˙q) : pz

swf (q) ≥ 0},
swf (q) = 0, ˙pz
swf : Q → R is the vertical position of the swing
where pz
foot, and D{L,R}
is the continuous domain on which our
dynamics (1) evolve with a transition from one stance leg
to the next triggered by the switching surface SL→R,R→L.
When this domain transition is triggered, the robot undergoes
an impact with the ground, yielding a hybrid model:

SS

(cid:40)

HC =

˙x = f (x) + g(x)u x (cid:54)∈ SL→R,R→L
˙x+ = ∆(x−)
x ∈ SL→R,R→L

(3)

where ∆ is a plastic impact model [18] applied to the
pre-impact states, x−, such that the post-impact states, x+,
respect the holonomic constraints of the subsequent domain.
In this work, we design locomotion using the hybrid zero
dynamics (HZD) framework [20] in order to generate stable
periodic walking for underactuated bipeds. At the core of this
method is the regulation of virtual constraints, or outputs:

y(x, τ, α) = ya(x) − yd(τ, α),
(4)
with the goal of driving y → 0 where ya : T Q → Rp and
yd : T Q × R × Ra → Rp are smooth functions representing
the actual and desired outputs, respectively, τ is a phasing
variable, and α is a set of Bezi`er polynomial coefﬁcients that
can be shaped to encode stable locomotion.

The desired outputs were optimized using the FROST
toolbox [21], where stability of the gait was ensured in the
sense of Poincar´e via HZD theory [22]. This was done ﬁrst
for AMBER, in which one walking gait was designed using
a pinned model of the robot [8], and then on Cassie for 3D
locomotion using the motion library found in [23] consisting
of 171 walking gaits for speeds in 0.1 m/s intervals on a grid
for sagittal speeds of vx ∈ [−0.6, 1.2] m/s and coronal speeds
of vy ∈ [−0.4, 0.4] m/s.

B. Control Lyapunov Functions

Control Lyapunov functions (CLFs), and speciﬁcally
rapidly exponentially stabilizing control Lyapunov functions
(RES-CLFs), were introduced as methods for achieving
(rapidly) exponential stability on walking robots [24]. This
control approach has the beneﬁt of yielding a control frame-
work that can provably stabilize periodic orbits for hybrid
system models of walking robots, and can be realized in a
pointwise optimal fashion. In this work, we consider only
outputs which are vector relative degree 2. Thus, differenti-
ating (4) twice with respect to the dynamics results in:

¨y(x) = L2

f y(x) + LgLf y(x)u.

Fig. 2: Conﬁguration of the 22 DOF (using an unpinned
model) Cassie robot [9] (left) and conﬁguration of the 5 DOF
(using a pinned model) planar robot AMBER-3M [8] (right).

Preference-based learning has been previously used to-
wards selecting essential constraints of an HZD gait gener-
ation framework which resulted in stable and robust exper-
imental walking on a planar biped with unmodeled compli-
ance at the ankle [19]. In this paper, we build on the previous
work by exploring the application of preference-based learn-
ing towards implementing optimization-based controllers on
multiple bipedal platforms. Speciﬁcally, we demonstrate the
framework towards tuning gains of a CLF-QP+ controller
on the AMBER bipedal robot, as well as an ID-CLF-QP+
controller on the Cassie bipedal robot, requiring the learning
framework to operate in a much higher-dimensional space.

II. PRELIMINARIES ON DYNAMICS AND CONTROL

A. Modeling and Gait Generation

Following a ﬂoating-base convention [18], we deﬁne the
conﬁguration space as Q ⊂ Rn, where n is the unconstrained
DOF (degrees of freedom). Let q = (pb, φb, ql) ∈ Q :=
R3×SO(3)×Ql, where pb is the global Cartesian position of
the body ﬁxed frame attached to the base linkage (the pelvis),
φb is its global orientation, and ql ∈ Ql ⊂ Rnl are the local
coordinates representing rotational joint angles. Further, the
state space X = T Q ⊂ R2n has coordinates x = (q(cid:62), ˙q(cid:62))(cid:62).
The robot is subject to various holonomic constraints, which
can be summarized by an equality constraint h(q) ≡ 0
where h(q) ∈ Rh. Differentiating h(q) twice and applying
D’Alembert’s principle to the Euler-Lagrange equations for
the constrained system, the dynamics can be written as:

D(q)¨q + H(q, ˙q) = Bu + J(q)(cid:62)λ
J(q)¨q + ˙J(q, ˙q) ˙q = 0

(1)

(2)

where D(q) ∈ Rn×n is the mass-inertia matrix, H(q, ˙q) con-
tains the Coriolis, gravity, and additional non-conservative
forces, B ∈ Rn×m is the actuation matrix, J(q) ∈ Rh×n is
the Jacobian matrix of the holonomic constraint, and λ ∈ Rh
is the constraint wrench. The system of equations (1) for the
dynamics can also be written in control-afﬁne form:

˙x =

(cid:20)
˙q
−D(q)−1(H(q, ˙q) − J(q)(cid:62)λ)
(cid:123)(cid:122)
f (x)

(cid:124)

(cid:21)

(cid:125)

+

(cid:20)
0
D(q)−1B
(cid:123)(cid:122)
g(x)

(cid:124)

(cid:21)

(cid:125)

u.

˙η =

η +

v.

(6)

C. Parameterization of CLF-QP

f y(x) : T Q → Rp and LgLf y(x) : T Q → Rp
where L2
represent the Lie derivatives of the outputs with respect to
the vector ﬁelds f (x) and g(x). Assuming that the system is
feedback linearizeable, we can invert the decoupling matrix,
LgLf y(x), to construct a preliminary control input:
u = (LgLf y(x))−1 (cid:0)ν − L2

f y(x)(cid:1) ,

(5)

which renders the output dynamics to be ¨y = ν. With the
auxiliary input ν appropriately chosen, the nonlinear system
can be made exponentially stable. Assuming the preliminary
controller (5) has been applied to our system, and deﬁning
η = [y, ˙y](cid:62) we have the following output dynamics [25]:
(cid:21)

(cid:20)0
0

I
0

(cid:124) (cid:123)(cid:122) (cid:125)
F

(cid:21)

(cid:20)0
I
(cid:124)(cid:123)(cid:122)(cid:125)
G

With the goal of constructing a CLF using (6), we evaluate
the continuous time algebraic Ricatti equation (CARE):

F (cid:62)P + P F + P GR−1G(cid:62)P + Q = 0,

(CARE)

which has a solution P (cid:31) 0 for any Q = Q(cid:62) (cid:31) 0 and R =
R(cid:62) (cid:31) 0. From the solution of (CARE), we can construct a
rapidly exponentially stabilizing CLF (RES-CLF) [24]:

V (η) = η(cid:62)IεP Iεη,

Iε =

(cid:20) 1
ε I
0

(cid:21)

0
I

,

(7)

where 0 < ε < 1 is a tunable parameter that drives the
(rapidly) exponential convergence. Any feedback controller,
u, which can satisfy the convergence condition:

˙V (η) = Lf V (η) + LgV (η)u ≤ −

1
ε

λmin(Q)
λmax(P )
(cid:123)(cid:122)
(cid:125)
(cid:124)
γ

V (η),

(8)

will then render rapidly exponential stability for the output
dynamics (4). To enforce (8), a quadratic program (CLF-QP)
[6], with (8) as an inequality constraint can be posed.

Implementing this controller on physical systems, which
are often subject to additional constraints such as torque
bounds or friction limits, suggests that relaxation for the
inequality constraint should be used. The introduction of
relaxation and the need to reduce torque chatter on phys-
ical hardware lead to the following relaxed (CLF-QP) with
incentivized convergence in the cost [26]:

CLF-QP+:

u∗ = argmin
u∈Rm

(cid:107)L2

f y(x) + LgLf y(x)u(cid:107)2 + w ˙V

˙V (x, u)

(9)

s.t. umin (cid:22) u (cid:22) umax

In order to avoid computationally expensive inversions of
the model sensitive mass-inertia matrix, and to allow for a
variety of costs and constraints to be implemented, a variant
of the (CLF-QP) termed the (ID-CLF-QP) was introduced in
[26]. This controller is used on the Cassie biped, with the
decision variables X = [¨q(cid:62), u(cid:62), λ(cid:62)](cid:62) ∈ R39:

ID-CLF-QP+:

X ∗ = argmin
X ∈Xext

(cid:107)A(x)X − b(x)(cid:107)2 + ˙V (q, ˙q, ¨q)

(10)

s.t. D(q)¨q + H(q, ˙q) = Bu + J(q)(cid:62)λ

umin (cid:22) u (cid:22) umax
λ ∈ AC(X )

(11)

where (2) has been moved into the cost terms A(x) and
b(x) as a weighted soft constraint, in addition to a feedback
linearizing cost, and a regularization for the nominal X ∗(τ )
from the HZD optimization. Interested readers are referred
to [7], [26] for the full (ID-CLF-QP+) formulation.

For the following discussion, let a = [a1, ..., av] ∈ A ⊂
Rv be an element of a v−dimensional parameter space,
termed an action. We let Q = Q(a), ε = ε(a), and
w ˙V = w ˙V (a) denote a parameterization of our control tuning
variables, which will subsequently be learned. Each gain ai
for i = 1, . . . , v is discretized into di values, leading to
an overall search space of actions given by the set A with
cardinality |A| = (cid:81)v
i=1 di. For the AMBER robot, v is taken
to be 6 with discretizations d = [4, 4, 5, 5, 4, 5], resulting in
the following parameterization:
(cid:20)Q1

(cid:21)

Q(a) =

0
0 Q2

,

ε(a) = a5,

Q1 = diag([a1, a2, a2, a1]),
Q2 = diag([a3, a4, a4, a3]),
w ˙V (a) = a6,

which satisﬁes Q(a) (cid:31) 0, 0 < ε(a) < 1, and w ˙V (a) > 0 for
the choice of bounds, as summarized in Table I. Because of
the simplicity of AMBER, we were able to tune all associated
gains for the CLF-QP+ controller. For Cassie, however, the
complexity of the ID-CLF-QP+ controller warranted only a
subset of parameters to be selected. Namely, v is taken to be
12 and di to be 8, resulting in:
(cid:20)Q1

Q1 = diag([a1, . . . , a12]),
Q2 = ¯Q,
with ¯Q, ε, and w ˙V remaining ﬁxed and predetermined by a
domain expert. From this deﬁnition of Q, we can split our
output coordinates η = (ηt, ηnt) into tuned and not-tuned
components, where ηt ∈ R12 and ηnt ∈ R6 correspond to
the Q1 and Q2 blocks in in Q.

0
0 Q2

Q =

(cid:21)

,

TABLE I: Learned Parameters

CASSIE

Q Pelvis Roll (φx)
Q Pelvis Pitch (φy)
Q Stance Leg Length ((cid:107)φst(cid:107)2)
Q Swing Leg Length ((cid:107)φsw(cid:107)2)
Q Swing Leg Angle (θsw
hp )
Q Swing Leg Roll (θsw
hr )

Pos. Bounds
a1:[2000, 12000]
a2:[2000, 12000]
a3:[4000, 15000]
a4:[4000, 20000]
a5:[1000, 10000]
a6:[1000, 8000]

Vel. Bounds
a7:[5, 200]
a8:[5, 200]
a9:[50, 500]
a10:[50, 500]
a11:[10, 200]
a12:[5, 150]

AMBER

Q Knees
Q Hips

Pos. Bounds
a1:[100, 1500]
a2:[100, 1500]

Vel. Bounds
a3:[10, 300]
a4:[10, 300]

Bounds
a5:[0.08, 0.2]
a6:[1, 5]

ε
w ˙V

Fig. 3: Simulated results averaged over 10 runs, demonstrat-
ing the capability of preference-based learning to optimize
over large action spaces, speciﬁcally the one used for exper-
iments with Cassie. Shaded region depicts standard error.

III. LEARNING FRAMEWORK
The preference-based learning framework leveraged in this
work is a slight extension of that presented in [27]. Specif-
ically, this work implements ordinal labels as an additional
feedback mechanism to improve sample-efﬁciency. As in
[27], the algorithm is aimed at regret minimization, deﬁned
as sampling N actions {a1, . . . , aN } such that:

{a1, . . . , aN } = argmin

a∈A

N
(cid:88)

i=1

(U (a∗) − U (ai)) ,

where A ⊂ R|a|
is the discretized set of all possible
actions, U : A → R is the underlying utility function of
the human operator mapping each action to a subjective
measure of “good”, and a∗ is the action maximizing U .
This objective of regret minimization can be equivalently
interpreted as trying to sample actions with high utilities,
¯a∗ = argmaxa∈A U (a), in as few iterations as possible. In
this section, we brieﬂy outline the learning framework and
how it was modiﬁed for our application.

A. Summary of Learning Method

In each iteration, the user is queried for their preference
between the most recently sampled action, ai, and the
previous action, ai−1, denoted as ai (cid:31) ai−1 if action ai
is preferred. This preference is modeled as:

P(ai (cid:31) ai−1|U (ai), U (ai−1)) = φ

(cid:18) U (ai) − U (ai−1)
cp

(cid:19)

,

where φ : R → [0, 1] is a monotonically-increasing link
function, and cp > 0 represents the amount of noise expected
in the preferences. In this work, we select the heavy-tailed
sigmoid distribution φ(x) := 1

1+e−x .

Inspired by [28], we supplement preference feedback with
ordinal labels. Because ordinal labels are expected to be
noisy, the ordinal categories are limited to only “very bad”,
“neutral”, and “very good”. Ordinal labels are obtained each
iteration for the corresponding action ai and are assumed to
be assigned based on U (ai). Similar to preferences, these
ordinal labels are modeled using a likelihood function:
(cid:18) br − U (ai)
co

(cid:18) br−1 − U (ai)
co

P(o = r|U (ai)) = φ

− φ

(cid:19)

(cid:19)

Fig. 4: The experimental procedure, notably the communica-
tion between the controller, physical robot, human operator,
and learning framework.

where o denotes the ordinal label provided by the user with a
corresponding ordered ranking r ∈ {1, 2, 3}, c0 > 0 denotes
expected noise in the ordinal labels, and {b0, . . . , b3} are
arbitrary thresholds that dictate which latent utility ranges
correspond to which ordinal label.

In each iteration, operator feedback is obtained and ap-
label datasets Dp
pended to the preference and ordinal
and Do, with all feedback denoted as D = Dp ∪ Do.
This feedback is then used to approximate the posterior
distribution P(U | D) as a multivariate Gaussian N (µ, Σ)
via the Laplace approximation as in [29, Sec. 2.3]. To remain
tractable in high-dimensions, U is a restriction of U as
deﬁned in [27]. The predictive distribution at location ˆa is
described by a univariate Gaussian U (ˆa) ∼ N (µ(ˆa), Σ(ˆa)),
whose equations can be found in [29, Sec. 2.3].

To select new actions to query in each iteration, Thompson
sampling [30] is used. Speciﬁcally, at each iteration, a
function ˆU randomly drawn from the Gaussian process is
maximized. This iterative process of (1) querying the opera-
tor for feedback, (2) modeling the underlying utility function,
and (3) sampling new actions, is repeated in each subsequent
iteration. Finally, the best action after the completion of the
experiment is given by ˆa∗ = argmaxa∈A µ(a).
B. Expected Learning Behavior

To demonstrate the learning, a simple example was con-
structed of the same dimensionality as the parameter space
being investigated on Cassie (v = 12, d = 8), where the
utility was modeled as U (a) = (cid:107)a − a∗(cid:107)2 for some a∗.
Feedback was automatically generated for both ideal noise-
free feedback as well as for noisy feedback (correct feedback
given with probability 0.9). The results of the simulated algo-
rithm, illustrated in Fig. 3, show that the learning framework
quickly samples actions near a∗, even for an action space
as large as the one used in the experiments with Cassie.
The simulated results also show that ordinal labels improve
convergence, motivating their use in the ﬁnal experiment.

TABLE II: Learned Parameters

,

AMBER
Cassie

[750, 100, 300, 100, 0.125, 2]
[2400, 1700, 4200, 5600, 1700, 1200, 27, 40, 120, 56, 17, 7]

Real-Time ControllerSecondary PCHuman OperatorCAREPreference-Based LearningPairwise PreferenceOrdinal Label(ID-)CLF-QP+Append User FeedbackRobot(a) Very low utility (top) where the robot was unable to
walk unassisted and maximum posterior utility (bottom) where
stable walking was achieved.

(b) The robustness (top) and tracking (bottom) of the walking
with the learned optimal gains, enabling unassisted, stable
walking and good tracking performance.

Fig. 5: Gait tiles for AMBER (left) and Cassie (right).

(a) Phase portraits for AMBER experiments.

(b) Output Error of ηt (left) and ηnt (right) for Cassie experiment.

Fig. 6: Experimental walking behavior of the CLF-QP+ (left) and the ID-CLF-QP+ (right) with the learned gains.

IV. LEARNING TO WALK IN EXPERIMENTS

Preference-based learning was applied to the realization of
optimization-based control on two separate robotic platforms:
the 5 DOF planar biped AMBER, and the 22 DOF 3D biped
Cassie, as can be seen in the video [31]. As illustrated in Fig.
4, the experimental procedure had four main components:
the physical robot (either AMBER or Cassie), the controller
running on a real-time PC, a human operator providing feed-
back, and a secondary PC running the learning algorithm.
Each action was tested for approximately one minute, during
which the behavior of the robot was evaluated in terms of
both performance and robustness. User feedback in the form
of pairwise preferences and ordinal labels was obtained after
testing each action via the respective questions: “Do you
prefer this behavior more or less than the last behavior”,
and “Would you give this gait a label of very bad, neutral,
or very good”. After user feedback was collected for the
sampled controller gains, the posterior was inferred over
all of the uniquely sampled actions, which took up to 0.5
seconds. The experiment with AMBER was conducted for 50
iterations, lasting one hour, and the experiment with Cassie
was conducted for 100 iterations, lasting two hours. The
duration of the experiments was scaled based on the size
of the respective action spaces, and trials were terminated
when satisfactory behaviors had been sampled.

A. Results with AMBER – CLF-QP+

The CLF-QP+ controller was implemented on an off-
board i7-6700HQ CPU @ 2.6GHz with 16 GB RAM, which
solved for desired torques and communicated them with the

is important

ELMO motor drivers on the AMBER robot at 2kHz. During
the ﬁrst half of the experiment, the algorithm sampled a
variety of gains causing behavior ranging from instantaneous
torque chatter to induced tripping due to inferior output
to note that none of the initial
tracking. It
sampled values led to unassisted walking. By the end of
the experiment however, the algorithm had sampled 3 gains
which were deemed ”very good”, and which resulted in
stable walking. The ﬁnal learned best actions found by the
algorithm are reported in Table II. Gait tiles for an action
deemed “very bad”, as well as the learned best action are
shown in Fig. 5a. Additionally, tracking performance for the
two sets of gains is seen in Fig. 6a, where the learned best
action tracks the desired behavior to a better degree.

B. Results with Cassie – ID-CLF-QP+

The ID-CLF-QP+ controller was implemented on the on-
board Intel NUC computer, which was running a PRE-
EMPT RT kernel. The software runs on two ROS nodes, one
of which communicate state information and joint torques
over UDP to the Simulink Real-Time xPC, and one of which
runs the controller. Each node is given a separate core on
the CPU, and is elevated to real-time priority. Preference-
based learning was run on an external computer and was
connected to the ROS master over WiFi. Actions were
updated in real-time; once an action was selected, it was sent
to Cassie via a rosservice call, where, upon receipt, the robot
immediately updated the corresponding gains. As rosservice
calls are blocking, multithreading their receipt and parsing
was necessary in order to maintain real-time performance.

Fig. 7: Phase plots and torques commanded by the ID-CLF-QP+ in the na¨ıve user experiments with Cassie. For torques,
each colored line corresponds to a different joint, with the black dotted lines being the feedforward torque. The gains
corresponding to a “very bad” action (top) yield torques that exhibit poor tracking on joints and torque chatter. On the other
hand, the gains corresponding to the learned optimal action (bottom) exhibit much better tracking and no torque chatter.

To demonstrate repeatability,

the experiment was con-
ducted twice on Cassie: once with a domain expert, and once
with a na¨ıve user. In both experiments, a subset of the Q
matrix from (CARE) was tuned with coarse bounds given
by a domain export, as reported in Table I. These speciﬁc
outputs were chosen because they were deemed to have a
large impact on the performance of the controller. Some
metrics used to determine preferences were the following:
no torque chatter, no drift in the ﬂoating base frame, re-
sponsiveness to desired directional input, no violent impacts,
no harsh noise, and naturalness of walking. At the start of
the experiments, there was signiﬁcant torque chatter and
wandering, with the user having to regularly intervene to
recenter the global frame. As the experiments continued,
the walking noticeably improved. At the conclusion of 100
iterations, the posterior was inferred over all uniquely visited
actions. The action corresponding with the maximum utility –
believed by the algorithm to result in the most user preferred
walking behavior – was further evaluated for tracking and
robustness. In the end, this learned best action coincided with
the walking behavior that the user preferred the most.

Features of this optimal action, compared to a worse action
sampled in the beginning of the experiments, are outlined in
Fig. 6. In terms of quantiﬁable improvement, the difference
in tracking performance is shown in Fig. 6b. The magnitude
of the tuned parameters, ηt, illustrates the improvement that
preference-based learning attained in tracking the outputs
it
the tracking error of
the constant parameters, ηnt, shows that the outputs that
were not tuned remained unaffected by the learning process.
This quantiﬁable improvement is further illustrated by the
commanded torques in Fig. 7, which show that the optimal
gains result in much less torque chatter and better tracking
as compared to the other gains.

intended to. At

the same time,

C. Limitations and Future Work

the current

The main limitation of

formulation of
preference-based learning is that the action space must be
predeﬁned with set bounds. In the context of controller
gains, these bounds are difﬁcult to know a priori since the
relationship between the gains and the resulting behavior is
unpredictable. Future work to address this problem involves
modiﬁcations to the learning framework to shift action space
based on the user’s preferences. Furthermore, the current
framework limits the set of potential new actions to the
set of actions discretized by di for each dimension i. As
such, future work also includes adapting the granularity of
the action space based on the uncertainty in speciﬁc regions.

V. CONCLUSION

Navigating the complex landscape of controller gains is a
challenging process that often requires signiﬁcant knowledge
and expertise. In this work, we demonstrated that preference-
based learning is an effective mechanism towards system-
atically exploring a high-dimensional controller parameter
space, without needing to deﬁne an objective function.
Furthermore, we experimentally demonstrated the power of
this method on two different platforms with two different
controllers, showing the application agnostic nature of the
framework. In all experiments, the robots went from stum-
bling to walking in a matter of hours. Additionally,
the
learned best gains in both experiments corresponded with
the walking trials most preferred by the human operator. In
the end, the robots had improved tracking performance, and
were robust to external disturbance. Future work includes
addressing the aforementioned limitations, extending this
methodology to other robotic platforms, coupling preference-
based learning with metric-based optimization techniques,
and addressing multi-layered parameter tuning tasks.

REFERENCES

[1] W. K. Ho, C. C. Hang, and L. S. Cao, “Tuning of PID controllers
based on gain and phase margin speciﬁcations,” Automatica, vol. 31,
no. 3, pp. 497–502, 1995.

[2] W. Wojsznis, J. Gudaz, T. Blevins, and A. Mehta, “Practical approach
to tuning MPC,” ISA transactions, vol. 42, no. 1, pp. 149–162, 2003.
[3] L. Zheng, “A practical guide to tune of proportional and integral
(PI) like fuzzy controllers,” in [1992 Proceedings] IEEE International
Conference on Fuzzy Systems.

IEEE, 1992, pp. 633–640.

[4] H. Hjalmarsson and T. Birkeland, “Iterative feedback tuning of linear
time-invariant MIMO systems,” in Proceedings of
the 37th IEEE
Conference on Decision and Control (Cat. No. 98CH36171), vol. 4.
IEEE, 1998, pp. 3893–3898.

[5] A. D. Ames and M. Powell, “Towards the uniﬁcation of locomotion
and manipulation through control Lyapunov functions and quadratic
programs,” in Control of Cyber-Physical Systems.
Springer, 2013,
pp. 219–240.

[6] K. Galloway, K. Sreenath, A. D. Ames, and J. W. Grizzle, “Torque sat-
uration in bipedal robotic walking through control Lyapunov function-
based quadratic programs,” IEEE Access, vol. 3, pp. 323–332, 2015.
[7] J. Reher and A. Ames, “Control Lyapunov functions for compliant
hybrid zero dynamic walking,” ArXiv, Submitted to IEEE Transactions
on Robotics, vol. abs/2107.04241, 2021.

[8] E. Ambrose, W.-L. Ma, C. Hubicki, and A. D. Ames, “Toward bench-
marking locomotion economy across design conﬁgurations on the
modular robot: AMBER-3M,” in 2017 IEEE Conference on Control
Technology and Applications (CCTA).
IEEE, 2017, pp. 1270–1276.
[9] A. Robotics, https://www.agilityrobotics.com/robots#cassie, Last ac-

cessed on 2021-09-14.

[10] M. Birattari and J. Kacprzyk, Tuning metaheuristics: a machine

learning perspective. Springer, 2009, vol. 197.

[11] M. Jun and M. G. Safonov, “Automatic PID tuning: An application of
unfalsiﬁed control,” in Proceedings of the 1999 IEEE International
Symposium on Computer Aided Control System Design (Cat. No.
99TH8404).

IEEE, 1999, pp. 328–333.

[12] A. Marco, P. Hennig, J. Bohg, S. Schaal, and S. Trimpe, “Automatic
LQR tuning based on Gaussian process global optimization,” in 2016
IEEE international conference on robotics and automation (ICRA).
IEEE, 2016, pp. 270–277.

[13] R. Calandra, A. Seyfarth, J. Peters, and M. P. Deisenroth, “Bayesian
optimization for learning gaits under uncertainty,” Annals of Mathe-
matics and Artiﬁcial Intelligence, vol. 76, no. 1, pp. 5–23, 2016.
[14] A. Rai, R. Antonova, S. Song, W. Martin, H. Geyer, and C. Atkeson,
“Bayesian optimization using domain knowledge on the ATRIAS
biped,” in 2018 IEEE International Conference on Robotics and
Automation (ICRA).

IEEE, 2018, pp. 1771–1778.

[15] P.-B. Wieber, “On the stability of walking systems,” in Proceedings of
the international workshop on humanoid and human friendly robotics,
2002.

[16] M. Vukobratovi´c and B. Borovac, “Zero-moment point—thirty ﬁve
years of its life,” International journal of humanoid robotics, vol. 1,
no. 01, pp. 157–173, 2004.

[17] J. E. Pratt and R. Tedrake, “Velocity-based stability margins for
fast bipedal walking,” in Fast motions in biomechanics and robotics.
Springer, 2006, pp. 299–324.

[18] J. W. Grizzle, C. Chevallereau, R. W. Sinnet, and A. D. Ames,
“Models, feedback control, and open problems of 3D bipedal robotic
walking,” Automatica, vol. 50, no. 8, pp. 1955–1988, 2014.

[19] M. Tucker, N. Csomay-Shanklin, W.-L. Ma, and A. D. Ames,
“Preference-based learning for user-guided HZD gait generation on
bipedal walking robots,” in 2021 IEEE International Conference on
Robotics and Automation (ICRA).

IEEE, 2021, pp. 2804–2810.

[20] E. R. Westervelt, J. W. Grizzle, C. Chevallereau, J. H. Choi, and
B. Morris, Feedback control of dynamic bipedal robot locomotion.
CRC press, 2018.

[21] A. Hereid and A. D. Ames, “FROST: Fast robot optimization and
simulation toolkit,” in 2017 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), 2017, pp. 719–726.

[22] E. R. Westervelt, J. W. Grizzle, and D. E. Koditschek, “Hybrid zero
dynamics of planar biped walkers,” IEEE transactions on automatic
control, vol. 48, no. 1, pp. 42–56, 2003.

[23] J. Reher and A. D. Ames, “Inverse dynamics control of compliant

hybrid zero dynamic walking,” 2020.

[24] A. D. Ames, K. Galloway, K. Sreenath, and J. W. Grizzle, “Rapidly
exponentially stabilizing control Lyapunov functions and hybrid zero
dynamics,” IEEE Transactions on Automatic Control, vol. 59, no. 4,
pp. 876–891, 2014.

[25] A.

Isidori, Nonlinear Control

ser.
Communications and Control Engineering. Springer, 1995. [Online].
Available: https://doi.org/10.1007/978-1-84628-615-5

Systems, Third Edition,

[26] J. Reher, C. Kann, and A. D. Ames, “An inverse dynamics approach

to control Lyapunov functions,” 2020.

[27] M. Tucker, M. Cheng, E. Novoseller, R. Cheng, Y. Yue, J. W.
Burdick, and A. D. Ames, “Human preference-based learning for
high-dimensional optimization of exoskeleton walking gaits,” in 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS).

IEEE, 2020, pp. 3423–3430.

[28] K. Li, M. Tucker, E. Bıyık, E. Novoseller, J. W. Burdick, Y. Sui,
D. Sadigh, Y. Yue, and A. D. Ames, “ROIAL: Region of interest active
learning for characterizing exoskeleton gait preference landscapes,”
in 2021 IEEE International Conference on Robotics and Automation
(ICRA).

IEEE, 2021, pp. 3212–3218.

[29] W. Chu and Z. Ghahramani, “Preference learning with Gaussian
processes,” in Proceedings of the 22nd International Conference on
Machine learning (ICML), 2005, pp. 137–144.

[30] O. Chapelle and L. Li, “An empirical evaluation of Thompson sam-
pling,” Advances in neural information processing systems, vol. 24,
pp. 2249–2257, 2011.

[31] “Video of the experimental results.” https://youtu.be/jMX5a 6Xcuw.

