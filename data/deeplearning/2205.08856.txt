2
2
0
2

y
a
M
8
1

]

Y
S
.
s
s
e
e
[

1
v
6
5
8
8
0
.
5
0
2
2
:
v
i
X
r
a

Bridging the gap between QP-based and
MPC-based RL

Shambhuraj Sawant ∗ Sebastien Gros ∗

∗ Department of Engineering Cybernetics, Norwegian University of
Science and Technology (NTNU), Trondheim, Norway (e-mail:
shambhuraj.sawant@ntnu.no, sebastien.gros@ntnu.no).

Abstract: Reinforcement learning methods typically use Deep Neural Networks to approximate
the value functions and policies underlying a Markov Decision Process. Unfortunately, DNN-
based RL suﬀers from a lack of explainability of the resulting policy. In this paper, we instead
approximate the policy and value functions using an optimization problem, taking the form of
Quadratic Programs (QPs). We propose simple tools to promote structures in the QP, pushing it
to resemble a linear MPC scheme. A generic unstructured QP oﬀers high ﬂexibility for learning,
while a QP having the structure of an MPC scheme promotes the explainability of the resulting
policy, additionally provides ways for its analysis. The tools we propose allow for continuously
adjusting the trade-oﬀ between the former and the latter during learning. We illustrate the
workings of our proposed method with the resulting structure using a point-mass task.

Keywords: Quadratic Programming, Reinforcement Learning, Model Predictive Control

1. INTRODUCTION

Reinforcement Learning (RL) focuses on teaching an agent
how to act in a given environment by rewarding desired
behaviours and punishing negative behaviours. It is a pow-
erful tool to ﬁnd solutions to Markov Decision Processes
(MDPs) without depending on a priori knowledge of the
underlying system dynamics. Most RL methods depend
purely on observed state transitions and stage cost re-
alizations, to improve the current policy. On the other
hand, model-based optimal control methods attempt to
solve the same task using a priori knowledge of system
dynamics, and, give performance and stability guaran-
tees through rigorous analysis. Model Predictive Control
(MPC) is one such well-known method for the optimal
control of complex dynamical systems. Many approaches
have been proposed recently that combine optimal control
methods with model-based RL in various ways (Kamthe
and Deisenroth, 2018; Pinneri et al., 2020, 2021).

Recently, RL has gained popularity due to striking ac-
complishments ranging from the success in Atari games
(Mnih et al., 2013) and in robotics (Heess et al., 2017), to
mastering the game of Go (Silver et al., 2016). In most RL
methods, the optimal control policy required for solving
the task at hand is learned either directly or indirectly.
Indirect methods of ﬁnding the policy rely on learning an
approximation of the optimal value function underlying
the MDP, typically based on Temporal Diﬀerence methods
(Kober et al., 2013). Direct RL methods seek to learn
the policy directly with either stochastic or deterministic
policy gradient methods (Sutton et al., 2000; Silver et al.,
2014). However, such direct methods typically require ﬁnd-
ing an approximation of the value functions to compute the
updates of the policy parameters. In both cases, as the
value function is not known a priori, it is approximated
with a generic function approximator, typically a deep

neural network (DNN). DNNs are additionally often used
to approximate the policy. Unfortunately, the closed-loop
behaviour of a DNN-based policy can be diﬃcult to explain
and formally analyze.

Recently, (Gros and Zanon, 2019) proposed an MPC-
based function approximator for generic MDPs. (Gros and
Zanon, 2019, Theorem 1) states that, under some condi-
tions, the optimal policy π∗ and associated value functions
can be generated using a single MPC scheme, even if
based on an inaccurate model of the dynamics, provided
that adequate modiﬁcations of the MPC stage cost and
constraints are carried out. An MPC-based policy and
value functions approximation oﬀer a high explainability
about the policy behaviour and is equipped with a broad
set of theoretical tools for formal veriﬁcation of the policy
in terms of safety and stability. Furthermore, RL methods
can be used to learn the adequate cost and constraints
modiﬁcations in the MPC-based policy. Hence, the MPC
parameters in such an approach are learned from data
similar to how DNN activation weights are learned in a
typical RL pipeline. Because they are convex and can
be solved in very short computational times, linear MPC
schemes are arguably the most popular in control. In terms
of optimization, linear MPC schemes take the form of a
Quadratic Program (QP). The use of RL in accelerating
QPs has been investigated in (Ichnowski et al., 2021). In
this work, we use linear MPC schemes and generic QPs to
carry approximations of the policy and value functions. We
believe that the piece-wise quadratic nature of QPs would
provide suﬃcient richness of features to approximate such
functions for continuous control tasks.

A generic QP and a linear MPC scheme chieﬂy diﬀer in
the structure of their cost and constraints. Indeed, the
constraint and cost matrices in a generic QP do not have
any speciﬁc structure. In contrast, the matrix underlying

 
 
 
 
 
 
the cost function of a linear MPC scheme is block-diagonal,
and the matrix underlying its equality constraint has a
speciﬁc banded structure matching the associated system
dynamics. Due to its structure, the QP underlying a
linear MPC scheme has less freedom, and therefore, less
ﬂexibility to perform in the learning task. A generic,
unstructured QP has, in contrast, more ﬂexibility and can
arguably perform better as an approximation of the policy
and value functions.

In this paper, we explore the diﬀerence between linear
MPC and generic QP formulations, and propose tools
to smoothly transition between them by more or less
aggressively promoting MPC-like structures during RL-
based learning. In our formulation, constraints are fully
parameterized and freed up for learning without assign-
ing any speciﬁc structure. However, we introduce simple
penalties in the learning step for promoting the emergence
of an MPC-like structure in the QP. We evaluate how such
penalties can help in transitioning from a generic QP to
structured MPC-like constraints on a point-mass task.

The paper is structured as follows. Section 2 gives an
overview of the MPC-based function approximators from
(Gros and Zanon, 2019) along with Q-learning method
and Quadratic Programming. Section 3 details the pro-
posed QP-based function approximator with the proposed
heuristic penalties. Section 4 discusses the used experi-
mental setup for a point-mass task and obtained results,
followed by conclusion in section 5.
2. METHODS

In this section, we summarize the linear MPC scheme and
QP formulation used for approximating value functions
in an MDP and apply Q-learning method for updating
parameters in these formulations.
2.1 MPC-based function approximation for MDPs

A Markov Decision Process (MDP) is a mathemati-
cal framework used for modelling decision-making in a
discrete-time stochastic control process. An MDP is char-
acterized by a tuple (S, A, Pa, L, γ) where S, A represent
state and action spaces respectively, Pa(s, s+) = P [s+|s, a]
is the probability that action a and state s leads to state
s+, L(s, a) = l describes the stage cost l for taking action
a in state s, and γ is a discount factor. Additionally, a
transition is deﬁned to be a tuple (s, a, s+, l). Solving an
MDP consists of ﬁnding an optimal policy: a function π
that maps a state s into an action a, i.e. π : S → A, for
minimizing the cumulative reward J given as:

(cid:34) ∞
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
where the stage cost L(s, a) reads as:

γkL(sk, ak)

J(π) = E

k=0

(cid:35)

ak = π(sk)

(1)
L(s, a) = l(s, a) + I∞(h(s, a)) + I∞(g(a))
In (1), function l captures cost associated to diﬀerent state-
action pairs, while the constraints h(s, a) and g(a) capture
the undesirable state-action pairs. The indicator function
I∞(x) penalizes constraint violations:

(cid:26)∞ if x > 0

I∞(x) =

0
The underlying MDP M with the associated stage cost L
and discount factor 0 < γ < 1 is assumed to yield a well-
posed problem, i.e. the policy π exists and is well deﬁned

otherwise

over some regions in S and the associated value functions
are well posed and ﬁnite over some regions in S × A.

We label π∗(s) the optimal policy, Q∗(s, a) the action-
value function, and V∗(s) the value function. The policy
and value functions are solution of the Bellman equations
(Sutton and Barto, 2018):

Q∗(s, a) = L(s, a) + γE[V∗(s+|s, a)]
V∗(s) = Q∗(s, π∗(s)) = min

Q∗(s, a)

a

We brieﬂy recall next the theory behind MPC-based
approximations of these functions.

We consider a (possibly deterministic) model of system
dynamics as P [ˆs+|s, a]. (Gros and Zanon, 2019) consider
a modiﬁed stage cost:

ˆL(s, a) =

(cid:26)Q∗(s, a) − γV +(s, a)

∞

if V +(s, a) < ∞
otherwise

(2)

where V +(s, a) = E[V∗(ˆs+)|s, a] uses the expected value
stemming from the model of system dynamics. (Gros
and Zanon, 2019, Theorem 1) states that, under some
conditions, an MPC scheme using the model P [ˆs+|s, a]
and the modiﬁed stage cost (2) yields the optimal policy
π∗ for the true MDP, together with the associated optimal
value functions. Simply stated, an optimal policy can be
generated using an MPC scheme based on simpliﬁed or
inaccurate model dynamics provided that the MPC stage
cost is adequately modiﬁed. In particular, an MPC scheme
based on a deterministic model can yield the optimal
policy and value functions of a stochastic MDP.

However, computing ˆL can be diﬃcult and requires the
knowledge of the true system dynamics. Fortunately, (Gros
and Zanon, 2019) showed that RL techniques are well
suited to build this cost modiﬁcation from data.

We consider the following parameterization of a linear
MPC scheme for approximating the value function V of
a generic MDP:

Vθ(s) = min
u,x,σ

N −1
(cid:88)

γk([xk; uk]T Wθ[xk; uk] + wT σk)

k=0
+ γN (xN W f

θ xN + wT

f σN )

s.t. xk+1 = Aθxk + Bθuk, x0 = s

(3a)
(3b)
(3c)

Guk ≤ 0
Dθ[xk; uk]T ≤ σk, Df
σk ≥ 0, σN ≥ 0

θ xN ≤ σN

θ , a terminal constraint parameterization Df

(3d)
(3e)
(3) holds a stage cost parameterization Wθ, a constraint
parameterization Dθ, a terminal cost parameterization
W f
θ and a
model parameterization Aθ, Bθ. A point to note in (1) and
(3) is that, the constraints are separated into pure action
constraints (3c) and mixed constraints (3d). Although the
pure action constraints are arguably ﬁxed, the mixed con-
straints need to be parameterized and learned to capture
the domain in which ˆL in (1) is ﬁnite. Additionally, an
l1 relaxation of the mixed constraints (3d) is carried out
using slack variables σk to avoid inﬁnite penalties in case
of constraint violations.

Similarly, the associated action-value function Q approxi-
mation is given as:

(3a)

(4a)

2.3 Q-learning for QP

Qθ(s, a) = min
u,x,σ
s.t.

(4b)
(4c)
For these value function approximations, the Bellman
equations still hold:

(3b) − (3e)
u0 = a

πθ(s) = arg min

a

Qθ(s, a), Vθ(s) = min

a

Qθ(s, a)

where πθ(s) is the optimal policy for (3) and (4).
2.2 Quadratic Programming

Quadratic Programming (QP) is a process of solving
mathematical optimization problems involving a quadratic
cost function subject to linear constraints. A generic
quadratic program with n variables takes the form:

zT Hz + qT z + c

(5a)

1
min
2
z
s.t. Cz = b

(5b)
(5c)
lb ≤ Gz ≤ ub
where z ∈ Rn is the optimization variable, H is an n × n
symmetric and often positive semi-deﬁnite matrix that
deﬁnes the quadratic cost, q ∈ Rn deﬁnes the linear cost,
C is an m × n matrix that deﬁnes m linear equality
constraints, and, G is an l × n matrix deﬁning l linear
inequality constraints with lb, ub as their lower and upper
bounds, respectively.
QP as a function approximator for MDPs

We formulate next a generic QP for approximating the pol-
icy and value functions underlying an MDP. We consider
the optimization variables as:

z = [ x0; u0; x1; . . . xN ]T
(6)
where x0,...,N , u0,...,N −1 have the dimension of the state
and action spaces of the MDP, respectively.

We then introduce parameterizations of the matrices and
vectors in (5), labelled as Hθ, qθ, cθ, Cθ, Gθ. We refer to Cθ
as the constraint matrix in later discussions. Additionally,
l1 relaxation is carried out for inequality constraints (5c)
for previously speciﬁed reasons. The resulting QP provides
a value function approximation Vθ as:

1
Vθ(s) = min
2
z,σ
s.t. Cθz = b

zT Hθz + qT

θ z + cθ + wT σ

lb − σ ≤ Gθz ≤ ub + σ
σ ≥ 0
x0 = s
The action-value function approximation Qθ is:

Qθ(s, a) = min
z,σ
s.t.

(7a)

(7b) − (7e)
u0 = a

(8b)
(8c)
An observation regarding (7) is that the N state-action
pairs in the optimization variable z do not necessarily
form a Markov chain or respect any system dynamics.
These pairs simply satisfy the QP constraints and help in
approximating the value functions. Hence, in general, (7)
and (8) do not necessarily correspond to an MPC scheme
and oﬀer no explainability as such. However, due to the
high number of degrees of freedom in selecting the entries
of the QP matrices in (7) and (8), such a QP oﬀers a high
ﬂexibility in approximating the value functions.

(7a)

(7b)
(7c)
(7d)
(7e)

(8a)

Q-learning method oﬀers a simple yet powerful tool to
adjust the parameters in MPC (4) and a QP (8) such that
the resulting Qθ are close to the true optimal action-value
function Q(cid:63). In basic Q-learning, the action-value function
parameters θ are updated to minimize the diﬀerences in
approximation for instantaneous transitions:
E(cid:2)Q∗(s) − Qθ(s)(cid:3)

θ = arg min

θ

A version of Q-learning with batch updates (sampled from
stored transition dataset, i.e. replay buﬀer D) reads as:

δ = L(s, a) + γVθ(s+) − Qθ(s, a)
θ ← θ + αE(cid:2)δ∇θQθ(s, a)(cid:3)

(9)
(10)
where δ is the temporal diﬀerence error for the sampled
transition, (s, a) ∈ B, a batch of transitions sampled from
replay buﬀer D, and α is the learning rate.

The batch update version of Q-learning can be applied
to learn the parametric Q functions in (4) and (8). The
gradient of Qθ in (8) can be obtained at a very low com-
putational cost using the associated Lagrange function:

Lθ(s, y) =

θ z + cθ + wT σ + χT (Cθz − b)

1
zT Hθz + qT
2
+ υT (Gθz − lb + σ) + ηT (ub + σ − Gθz)
+ µT σ + ξT (x0 − s) + ζ T (u0 − a)

(11)
where χ, υ, η, µ, ξ, ζ are the multipliers associated with
constraints (8b), i.e. (7b)-(7e), and (8c), respectively, and,
y = (x, u, χ, υ, η, µ, ξ, ζ) compiles the associated primal-
dual variables. From (B¨uskens and Maurer, 2001), the
gradient of Qθ is:

∇θQθ(s, a) = ∇θL(s, y∗)

(12)

wherein y∗ is the primal-dual solution of (8).

We assume Hθ in (8) to be a positive semi-deﬁnite matrix,
for ensuring a convex QP. Hence, a semi-deﬁnite program
(SDP) is used for updating θ in (8):

∆θ2 − αE(cid:2)δ∇θQθ

(cid:3)∆θ

(13a)

∆θs = arg min
∆θ
s.t. Hθ+∆θ ≥ 0

(13b)
In case, Hθ is positive semi-deﬁnite, (13) yields the same
update as (10). However, if Hθ does not remain positive
semi-deﬁnite, SDP carries out a constrained optimization
step for updating θ. Additionally, it is beneﬁcial to use dif-
ferent learning rates α for updating θ in the optimization
cost and the constraints.

In this paper, we make use of Q-learning method for the
sake of simplicity. However, most typical RL methods can
also be used to learn the parameters of (4) and (8).

3. QP-BASED RL WITH PENALTIES

In this section, we discuss the proposed formulation to
smoothly transition between value function approxima-
tions using QPs and MPC schemes by promoting dynamic
system-like constraints during learning.

3.1 Proposed formulation

We consider the QP-based approximation of Q function
given in (8) wherein the optimization variable is z =
[x0; u0; x1; . . . xN ]T . Consider the optimization cost as:

N H f
xT

θ xN + (qf )T

θ xN

(cid:17)

+ cθ

J = γN (cid:16) 1
2
N −1
(cid:88)

+

γk(cid:16) 1
2

[xk; uk]T (Hk)θ[xk; uk] + (qk)T

θ [xk; uk]

(cid:17)

k=0
zT Hθz + qT

θ z + cθ

=

1
2

(14)

Hence, Hθ from (7) becomes a block-diagonal matrix,
introducing a structure to the QP objective similar to that
of an MPC scheme (3). In order for (8) to resemble an
MPC-based approximation of the action-value function,
the linear constraints in (8b) need to take the form
corresponding to a linear model of the dynamics. More
speciﬁcally, (7b) should take the form of model constraints
(3b), which can be combined to be written as:







A B −I 0 . . .
0 0 A B . . .
...
...
...
. . .
0 0 . . .

. . . 0
. . . 0
...
...
. . .
. . . A B −I

(cid:124)

(cid:123)(cid:122)
Cθ







(cid:125)

















x0
u0
x1
...
xN
(cid:124) (cid:123)(cid:122) (cid:125)
z

= 0

(cid:124)(cid:123)(cid:122)(cid:125)
b

(15)

The linear dynamical constraints in an MPC scheme result
in a banded constraint matrix having a very speciﬁc
pattern. In order to make sense of the entries in z, Cθ
should emerge with a similar structure after the learning
routine and we consider b = 0. With these considerations,
the proposed value function Vθ(s) is given as:
Vθ(s) = min
z,σ
s.t. Cθz = 0

(16a)

(7a)

(7c) − (7e)

(16b)
(16c)

Similarly, the proposed action-value function Qθ(s, a) is:
(17a)

(8a)

Qθ(s, a) = min
z,σ
s.t.

(16b)
(7c) − (7e), (8c)

(17b)
(17c)

(16) and (17) closely resemble the QP-based approxima-
tions in (7) and (8) with no speciﬁc structure for Cθ. We
make use of Q-learning for updating θ in (17).

The parameterizations introduced in the QP-based ap-
proximation of action-value function in (17), Hθ, qθ, cθ,
Cθ, Gθ, are fully unconﬁned, i.e. each element is, in princi-
ple, free to be updated. However, we would like to see the
emergence of MPC-like structure in these matrices and
vectors to aﬀord some explainability, without forcing the
same, in the interest of ﬂexibility for function approxi-
mation. However, in this paper, we focus on promoting
resembling structure in the fully parameterized constraint
matrix Cθ, and assume a block diagonal structure for the
optimization cost and in G. However, we believe that
similar simple penalties would promote a structure in
the remaining parameterizations. In case of Cθ, we would
like to see a structure like (15) emerge out of learning.
However, as constraint matrix Cθ is not structured, i.e.
it is fully parameterized and free for tuning, the resulting
constraint matrix Cθ would be updated to minimize the
temporal diﬀerence error and would likely be a dense
matrix. Hence, we next explain how simple penalties can
help in promoting such a structure in Cθ.

3.2 Motivating dynamical system-like constraints

The constraint matrix Cθ is parameterized to give the
learning algorithm complete freedom to have maximal
ﬂexibility for better approximation. However, the learned
constraint matrix Cθ would likely become dense, having no
particular structure. To tackle this, we propose a simple
penalty in the SDP scheme (13) to promote learning a
banded structure like in (15). It introduces a trade-oﬀ
between performance and a penalty for deviation from the
banded constraint matrix in (15).

To achieve a smooth transition between an MPC-based
and a QP-based formulation by more or less aggressively
promoting the desired structure, the penalty for deviation
from the banded structure can be scaled as required. A
high value of the scaling constant makes the SDP routine
with the penalty stick to the banded structure, while
a smaller value provide the RL tool with the freedom
to update Cθ. This scaling is achieved using the scaling
matrix Cmask. We consider all parameters in (8) constitute
θ, i.e. θ is a vector representing all parameters learned
using RL method.

The proposed SDP scheme with the deviation penalty is:

∆θs = arg min
∆θ

∆θ2 − αE[δ∇θQθ]∆θ

+ |Cmask (cid:12) (Cθ+∆θ − C0)|

s.t. Wθ+∆θ ≥ 0

(18a)
(18b)

where (X (cid:12) Y ) is the Hadamard product, Cmask is the
scaling matrix for 1-norm penalty and C0 is the banded
constraint matrix for the model of state dynamics (A, B):

C0 =

Cmask =













A B −I 0 . . .
0 0 A B . . .
...
...
...
. . .
0 0 . . .
c2 c2 c2
c3 c3 c2
...
...
...
c3 c3 . . .

. . . 0
. . . 0
...
...
. . .
. . . A B −I
. . . c1
c1 . . .
. . . c1
c2 . . .
...
. . .
. . .
. . .
c2 c2
. . . c2













We considered 1-norm penalty so as to force the deviation
to zero. Penalizing with 1-norm also helps with ease of
compute as dense Cθ results in complex constraints over
many state action pairs, leading to diﬃculty in ﬁnding a
solution to (17) and (18).

Diﬀerent Cmask i.e. diﬀerent values of {c1, c2, c3} result in
promoting varied structures in constraint matrix Cθ, and
hence, giving diﬀerent meaning to entries in the optimiza-
tion variable z. For example, a conﬁguration of [0, 0, 0], i.e.
essentially freeing Cθ to be updated as required, results
in dense Cθ, but z no longer forms a Markov chain. A
conﬁguration of [1, 1, 1] results in Cθ closely resembling
C0, leading to further ability to analyse entries in z. 1-
norm penalty would force the diagonal (on-band) entries
in Cθ close to the model dynamics and the oﬀ-diagonal
(oﬀ-band) entries to zero. Cmask can be further conﬁgured
such that its entries progressively increase away from the
diagonal, resulting in Cθ with non-zero entries close to the
diagonal and resembling non-Markovian model dynamics
with less correlations for distant state-action pairs in time.

3.3 System Identiﬁcation along with constraint learning

We propose a second penalty to take into account the true
system dynamics P [s+|s, a]. The SDP scheme with this
penalty takes the form:
∆θs = arg min
∆θ

∆θ2 − αE[δ∇θQθ]∆θ

+

M
(cid:88)

i=0

β(cid:107)Cθ+∆θτi(cid:107)2

(19a)

s.t. Wθ+∆θ ≥ 0

(19b)
where τi = [sj, aj, sj+1, . . . , sj+N ]
is the i-th sampled
sequence of consecutive state transitions from the true sys-
tem dynamics of length N (sampled from the replay buﬀer
D), and β is a scaling constant. (19) can be interpreted
as trading oﬀ performance against ﬁtting the constraint
matrix to the true system dynamics and hence carrying
out system identiﬁcation (SI).

In Partially Observable MDPs or problems with tempo-
rally correlated noise, this SI penalty would help in better
ﬁtting the constraint matrix Cθ with the true system
dynamics by allowing for helpful changes and limiting
adverse ones. The SI penalty should also come in handy
for correcting the error in the model of system dynamics.

4. EXPERIMENTS AND RESULTS

We discuss the experiment setup used to illustrate the
workings of QP-based function approximator with pro-
posed penalties in this section, and summarize the results.

4.1 Point-Mass

We consider a Point-Mass task wherein the objective is
to push a point mass to the origin. Its state space S ∈
R4 consists of the location (x, y) and the corresponding
velocities ( ˙x, ˙y) of point mass, i.e. s = [x, y, ˙x, ˙y]T and
is bounded by lbs = [−2, −2, −10, −10]T and ubs =
[2, 2, 10, 10]T . The action space A ∈ R2 consists of forces
applied in x and y directions, i.e. a = [Fx, Fy]T and is
bounded by lba = [−1, −1]T and uba = [1, 1]T . The true
system dynamics is deﬁned as:



s+ = As + Ba + ν
1 0 0.1 0
0 1 0 0.1
0 0 0.9 0
0 0 0 0.9

A =







 , B =











0
0
0
0
0.1 0
0 0.1

is used for bounding states and actions over optimization
horizon N = 10. The constraint matrix Cθ is:











Cθ =

θ11 θ12 . . .
θ21 θ22 . . .
...
...
. . .
Cθ is a dense 4N × (6N + 4) matrix, initialized to C0.
The parameter vector is θ = [θ1, θ2, θ3, θ4, θ5, θ11, θ12, . . . ].
For the proposed penalties in (18) and (19), the scaling
constants are c1 = 1, c2 = 1e − 4, c3 = 0.0, β = 1e − 6.
4.3 Results

We present the performance results of QP-based RL with
proposed penalties in this section. Fig. 1 shows the perfor-
mance of QPs with diﬀerent penalties for Gaussian noise
case. Constraint learning is important for improving the
task performance, as seen in ﬁg. 1, as QP with ﬁxed
constraints fails to improve due to model mismatch while
other conﬁgurations outperform. However, the resulting
constraint matrix, shown in ﬁg. 2, show emergence dif-
ferent structures, especially, QP without any penalty and
with SI penalty result in denser Cθ while the deviation
penalty promotes Cθ to still hold MPC-like structure.

Fig. 1. Cumulative reward J for QP-based RL with diﬀer-
ent penalties in point-mass task with Gaussian noise.
(1: ﬁxed constraints, 2: fully parameterized Cθ, 3: Cθ
with deviation penalty (18), 4: Cθ with SI penalty
(19), 5: Cθ with combined penalty from (18) and (19))

where ν is the noise present in the system. The re-
ward function is set to be L(s, a) = sT W s with W =
diag(3, 3, 0.25, 0.25) and the discount factor γ is 0.9.

To evaluate the proposed penalties and emergence of struc-
ture, we consider following cases: a) ν is Gaussian in
nature, and b) ν is Brownian Noise. Additionally, we use
a corrupted model of state dynamics in the optimization
formulation to further showcase the importance of con-
straint tuning. The corrupted model is obtained by adding
a random matrix, whose entries are sample uniformly
between [−0.05, 0.05], to the true system dynamics (A, B).

4.2 Experimental Setup

We consider following parameterization for (17): (Hk)θ =
diag(θ1, θ2, θ3, θ4, 0, 0), (H f )θ = I(4), (qk)θ = (qf )θ = 0,
and cθ = θ5 with θi initialized randomly ∀i. Gθ = I(6N +4)

(a)

(c)

(b)

(d)

Fig. 2. Learned constraint matrix Cθ in Gaussian noise
case for: a) no penalty, b) deviation penalty (18), c)
SI penalty (19), d) both penalty from (18) and (19)

Similar results are observed in case of the point-mass
task with Brownian noise, as shown in ﬁg. 3. Constraint

020406080100iterations0200400600800100012001400J123450.00.10.20.30.40.50.60.70510152002468051015200246805101520024680510152002468learning helps improve the performance while QP with ﬁx
constraints fails. From ﬁg. 4, with temporally correlated
noise, we observe emergence of signiﬁcantly denser Cθ for
QP without penalties and QP with SI penalty while the de-
viation penalty pushes the learning routine to hold MPC-
like structure while still improving the task performance.

Fig. 3. Cumulative reward J for QP-based RL with diﬀer-
ent penalties in point-mass task with Brownian noise.
(1: ﬁxed constraint, 2: fully parameterized Cθ, 3: Cθ
with deviation penalty (18), 4: Cθ with SI penalty
(19), 5: Cθ with combined penalty from (18) and (19))

(a)

(c)

(b)

(d)

Fig. 4. Learned constraint matrix Cθ in Brownian noise
case for: a) no penalty, b) deviation penalty (18), c)
SI penalty (19), d) both penalty from (18) and (19)

5. CONCLUSION

In this paper, we approximate the policy and value
functions of an MDP using linear MPC-based and QP-
based function approximators. We propose simple heuris-
tic penalties to smoothly transition between an MPC-
based and a QP-based approximation scheme by promot-
ing structure in the optimization formulation. A generic
QP-based formulation oﬀers high ﬂexibility to approxi-
mate the value functions, however, it lacks explainabil-
ity, while a QP having the structure of an MPC scheme
promotes the explainability of the resulting policy and
provides tools for its analysis. With the proposed penalties,
we can smoothly transition between QP-based and MPC-
based approximations and continuously adjust the trade-
oﬀ between the former and the latter during learning.
These penalties enable maintaining an MPC-like structure
while still tuning the constraints. We show that in a
point-mass task with stochastic transitions, it is possible

to promote structure along with improving performance.
Building on this, these tools need to be investigated in
complex tasks and partial observable systems.

ACKNOWLEDGEMENTS

We thank the generous funding given by the Research
Council of Norway through the SARLEM project.

REFERENCES

B¨uskens, C. and Maurer, H. (2001). Sensitivity analysis
and real-time optimization of parametric nonlinear pro-
gramming problems. In Online Optimization of Large
Scale Systems, 3–16. Springer.

Gros, S. and Zanon, M. (2019). Data-driven economic
nmpc using reinforcement learning. IEEE Transactions
on Automatic Control, 65(2), 636–648.

Heess, N., TB, D., Sriram, S., Lemmon, J., Merel, J.,
Wayne, G., Tassa, Y., Erez, T., Wang, Z., Eslami, S.,
et al. (2017). Emergence of locomotion behaviours in
rich environments. arXiv preprint arXiv:1707.02286.
Ichnowski, J., Jain, P., Stellato, B., Banjac, G., Luo, M.,
Borrelli, F., Gonzalez, J.E., Stoica, I., and Goldberg,
K. (2021). Accelerating quadratic optimization with
reinforcement learning. Advances in Neural Information
Processing Systems, 34.

Kamthe, S. and Deisenroth, M. (2018). Data-eﬃcient re-
inforcement learning with probabilistic model predictive
control. In A. Storkey and F. Perez-Cruz (eds.), Pro-
ceedings of the Twenty-First International Conference
on Artiﬁcial Intelligence and Statistics, volume 84 of
Proceedings of Machine Learning Research, 1701–1710.
PMLR.

Kober, J., Bagnell, J.A., and Peters, J. (2013). Reinforce-
ment learning in robotics: A survey. The International
Journal of Robotics Research, 32(11), 1238–1274.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013).
Playing atari with deep reinforcement learning. arXiv
preprint arXiv:1312.5602.

Pinneri, C., Sawant, S., Blaes, S., Achterhold, J., Stueck-
ler, J., Rolinek, M., and Martius, G. (2020). Sample-
eﬃcient cross-entropy method for real-time planning.
arXiv preprint arXiv:2008.06389.

Pinneri, C., Sawant, S., Blaes, S., and Martius, G.
(2021). Extracting strong policies for robotics tasks
from zero-order trajectory optimizers.
In Interna-
tional Conference on Learning Representations. URL
https://openreview.net/forum?id=Nc3TJqbcl3.
Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L.,
Van Den Driessche, G., Schrittwieser, J., Antonoglou,
I., Panneershelvam, V., Lanctot, M., et al. (2016).
Mastering the game of go with deep neural networks
and tree search. nature, 529(7587), 484–489.

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,
and Riedmiller, M. (2014). Deterministic policy gradient
In International conference on machine
algorithms.
learning, 387–395. PMLR.

Sutton, R.S. and Barto, A.G. (2018). Reinforcement

learning: An introduction. MIT press.

Sutton, R.S., McAllester, D.A., Singh, S.P., and Mansour,
Y. (2000). Policy gradient methods for reinforcement
learning with function approximation. In Advances in
neural information processing systems, 1057–1063.

020406080100iterations0200400600800100012001400J123450.00.10.20.30.40.50.60.70510152002468051015200246805101520024680510152002468