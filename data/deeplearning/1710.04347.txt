NeuroTrainer: An Intelligent Memory Module for Deep
Learning Training

7
1
0
2

t
c
O
2
1

]

R
A
.
s
c
[

1
v
7
4
3
4
0
.
0
1
7
1
:
v
i
X
r
a

Duckhwan Kim1, Taesik Na1, Sudhakar Yalamanchili 1, and Saibal Mukhopadhyay1

1School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA

ABSTRACT
This paper presents, NeuroTrainer, an intelligent mem-
ory module with in-memory accelerators that forms the
building block of a scalable architecture for energy eﬃ-
cient training for deep neural networks. The proposed
architecture is based on integration of a homogeneous
computing substrate composed of multiple processing
engines in the logic layer of a 3D memory module. Neu-
roTrainer utilizes a programmable data ﬂow based ex-
ecution model to optimize memory mapping and data
re-use during diﬀerent phases of training operation. A
programming model and supporting architecture uti-
lizes the ﬂexible data ﬂow to eﬃciently accelerate train-
ing of various types of DNNs. The cycle level simu-
lation and synthesized design in 15nm FinFET shows
power eﬃciency of 500 GFLOPS/W, and almost similar
throughput for a wide range of DNNs including convo-
lutional, recurrent, multi-layer-perceptron, and mixed
(CNN+RNN) networks.

1.

INTRODUCTION

The hardware acceleration of inference of Deep Neu-
ral Network (DNNs) including convolutional (CNN), re-
current (RNN), and multi-layer-perceptron (MLP) have
received considerable attention in recent past [1, 2, 3,
4, 5, 6, 7, 8, 9]. In contrast, training has largely been
accelerated by software implementations executing on
clusters of graphics processing units (GPUs). As DNNs
become larger and more complex, the time and energy
costs of training become limiters to the application of
DNNs to more complex problems. For example, a DGX-
1 with 8 GPUs consumes more than 3KW, and is limited
to training only 1,000 Imagenet images per second for
VGG 16 [10]. Hence, availability of a specialized mod-
ular architecture for energy eﬃcient scaling of training
performance will be critical to the feasibility of future,
large scale DNNs.

The training of a DNN is composed of three primary
steps: forward propagation (FP), which is identical to
inference, back propagation (BP), and parameter up-
date (UP) (see Section 2 for details). The acceleration
of training faces major additional challenges over accel-
eration of inference as discussed below.

1) Most DNN accelerators for inference are optimized
for convolutions with small kernels and matrix-matrix
multiplication (for fully connected layers). However, ac-
celerating BP and UP includes the following additional
operations - i) convolution with very large kernels, ii)
matrix transpose, iii) vector to vector outer product,
iv) loss function computation, v) a pooling layer and
its derivative, and vi) the derivative of non-linear acti-
vation functions..

2) Training operates over very large data sets and em-
ploys mini-batch processing across training thereby re-
quiring larger on-chip storage to increase eﬀective mem-
ory bandwidth. In contrast, inference operates over sin-
gle sample. Further, while inference requires only read-
ing weights, training requires reading/writing weights
and their gradients, increasing memory traﬃc.

3) The computation of gradients in back propaga-
tion and weight update require higher bit precision to
account for small gradient values (vanishing gradient
issue [11, 12]). Therefore, low bit precision (8/16 bit)
arithmetic, often used during inference for energy eﬃ-
ciency, is not suitable for training.

This paper presents, NeuroTrainer, an intelligent mem-

ory module with in-memory accelerators that forms the
building block of a scalable architecture for energy eﬃ-
cient training. The key distinguishing feature of Neuro-
Trainer is its programmable data ﬂow execution model.
We observed that distinct computational kernels in train-
ing diﬀerent networks share common arithmetic opera-
tions (e.g., multiply-and-accumulate) but diﬀer in their
memory usage and data ﬂow pattern. Hence, Neuro-
Trainer utilizes a homogeneous architecture but with an
execution model where memory mapping, re-use, and
data ﬂow for diﬀerent kernels are programmed to match
the data usage/ﬂow pattern, and hence, optimize per-
formance of each kernel. It is in contrast to recent report
of multi-chip module based training accelerator where
each chip is independently optimized for a speciﬁc op-
eration, creating a heterogeneous architecture [13]. A
major advantage of the programmable data-ﬂow based
execution on homogeneous substrate, compared to cus-
tomized hardware based solutions is to provide stable
performance over many applications.

 
 
 
 
 
 
the Hybrid Memory Cube (HMC) which is composed
of stacked DRAM partitioned across multiple indepen-
dently controlled vaults. Each vault has an independent
vault controller on the logic die; therefore multiple par-
titions in a DRAM die can be accessed simultaneously.
3) We present an in-memory accelerator composed of
an array of interconnected processing engines (PEs), im-
plemented on the logic die with precision-conﬁgurable
arithmetic and support for dataﬂow based execution.
All but one memory vault are connected to a dedicated
PE; and the remaining vault is connected to all the
PEs using a shared bus. Each vault controller is aug-
mented with a Programmable Memory Address Gener-
ator (PMAGs).

4) We develop programming model for the Neuro-
Trainer. Compilation now involves optimized mappings
of data (input, parameters, and gradients) into memory
vaults, and programming the PMAGs to orchestrate an
eﬃcient ﬂow of data between the DRAM and logic layer
in a manner that optimizes bandwidth, exploits re-use,
maximizes concurrency, and minimizes data movement
and buﬀering in the PEs.

The rest of the paper is organized as follows. Sec-
tion 2 introduces DNN training; Section 3 illustrates
the proposed architecture; Section 4 explains the pro-
gramming model; Section 5 presents simulation results,
followed by related work and conclusion.

2. PRELIMINARIES

In this section, we will explain the approach for train-
ing DNN with gradient descent, which is composed of
three steps: feedforward, backpropagation, and weight
update in recent DNNs [14]. Fig. 2 shows a simple DNN
and its feedforward, backpropagation, and weight up-
dating for diﬀerent types of the layer in the network.

2.1 Feedforward (FF)

Feedforward is propagation of neuron activation from
ith layer to i + 1th layer through weights between two
layers. The output of a neuron (state) is weighted sum-
mation of activation from all connected neurons in pre-
vious layer (and current layer as well for RNN [15]). It
is the only phase required during the inference. Fig. 2
shows feedforward through convolution layer and fully
connected layer.

2.2 Backpropagation (BP)

For a given input, at the end of feedforward oper-
ation, the output of the last layer is compared with
the ground truth i.e.
the desired output for this in-
put and computing loss (L). The loss can be deﬁned
by simple mean squared error (MSE) or combination
of softmax layer and cross-entropy layer [16]. Back-
propagation is the phase to ﬁnd the impact of each
state on the loss (gradient) ∂L/∂X(dX) by propagat-
ing from the last layer. Since there is no deﬁnition of
loss (L) in hidden layer, ∂L/∂X(dX) is computed from
the dX of i + 1thlayer (∂L/∂Y (dY )) instead of comput-
ing ∂L/∂X(dX) directly. We can see most of arithmetic
operation in Backpropagation is similar to that of Feed-

Figure 1: NeuroTrainer overview: (a) Proposed
architecture. (b) Eﬃciency (GFLOPS/W)

The NeuroTrainer is evaluated using cycle level sim-
ulation, and synthesized in 15nm FinFET technology.
The NeuroTrainer demonstrates 25x higher eﬃciency
over GPU (Fig. 1). Moreover, NeuroTrainer
shows
higher average power eﬃciency over prior training accel-
erators([4, 6, 13]), and more importantly, demonstrate
ability to train diﬀerent types of benchmark networks
(CNN, RNN, and CNN + RNN) with similar power-
eﬃciency. We further demonstrate scalable system with
multiple interconnected NeuroTrainers to scale training
performance for very large DNNs. Hence, NeuroTrainer
can be used as the building block to design a large scale
DNN training platform.

This paper makes the following contributions.
1) We present a programmable data ﬂow based execu-
tion model that enables the use of a homogeneous com-
puting architecture to eﬃciently train diverse DNNs.
This ﬂexible data ﬂow programmability enables eﬃ-
ciently accelerate training of various types of DNNs
including CNNs, RNNs, MLPs and hybrid networks
(CNN+RNN).

2) We present the NeuroTrainer as a 3D memory
module with an integrated in-memory accelerator. The
architecture of the memory module is patterned after

2

Figure 2: Deep neural network composed of convolution layer, pooling layer, and fully connected
location of
layer.
maximum pixel in pooling area).

feedforward, backpropagation, and weight update (ID*:

It has three phases:

forward in convolution and fully connected layer except
transposing kernel (Fig. 2).

2.3 Weight Updates (UP)

Based on dX, ∂L/∂W (dW ) needs to be generated to
reduce L in next iteration (epoch). New W for next iter-
ation is determined as Wnew = Wold −η ·dW , where η is
learning rate. Recently, additional term is added during
the update such as momentum [17]. For convolution, it
needs convolution between X and dY . As dimension of
dY is smaller than X by radius of kernel (W ), it is con-
volution with very large kernel size. For fully connected
layer, it requires vector vector outer product. Thus we
can see there is additional operation and data ﬂow is
needed for eﬃcient operation in weight updating.

2.4 Data Preparation (Prep)

into the memory to improve data ﬂow between memory
and processing engines. If the layout of output gener-
ated in layer i does not match with required data layout
for the layer i + 1, it needs to be re-arranged between
multiple memory banks. In addition, for convolution,
to make the size of the output same as the size of input,
the input needs dummy zeros on its boundary.

2.5 Minibatch Training

Mini batch training involves updating weights after
training small set (K) of training data. If total number
of training data is N , it will iterate N/K times for one
epoch. It is faster than training with large batch sizes
and shows smoother convergence than training individ-
ual images. Moreover, it can reuse weights K times im-
proving computing eﬃciency [14]. However, it requires
more on-chip memory to store K temporal data.

For each operation, input data need to be pre-loaded

The multiple mini-batches are trained in parallel us-

3

Figure 3: Hybrid Data Flow. (a) 2D convolution
with small kernels and (b) Matrix multiplication

ing multiple computing nodes where each node inde-
pendently compute ∂L/∂W (dW ). After generating all
dW s, all machine share updated new W (synchronous
training) [18]. To overcome the unbalance in training la-
tency among multiple nodes, in asynchronous training
once a node generates dW , it will have new W while
others use old W [19].

3. PROPOSED ARCHITECTURE

In this paper, NeuroTrainer is designed considering a
Hybrid Memory Cube (HMC) where a 3D memory stack
is partitioned into multiple parallel vaults. For exam-
ple, HMC 1.0 is composed of 4 DRAM dies partitioned
into 16 vaults, each vault has an independent memory
controller (vault controller, VC), and connected to 4
external (oﬀ-chip) links via AXI interface. The compu-
tation fabric of NeuroTrainer is composed of multiple
processing engines (PEs) where each PE contains clus-
ters of computation units and local buﬀers for inputs
(states) and parameters. Each PE has a one high band-
width connection to its local vault and an interface to
a broadcast bus connected to a shared vault. The vault
controllers are augmented with a programmable mem-
ory address generator (PMAG), a state-machine that
realizes mapping of the diﬀerent types of data (input,
parameter, and gradients) to diﬀerent vaults and con-
trol data ﬂow between memory and PEs.

3.1 Hybrid Data Flow

NeuroTrainer is designed to provide two diﬀerent data
movements between vaults and PEs based on the op-
eration types. Consider convolution and matrix-matrix
multiplication. During the convolution (Fig. 3 (a)), ker-
nel (W ) is shared by PEs while input is partitioned for
each PE. For the matrix-matrix multiplication, weight
matrix (W ) is partitioned while input (X) is shared by
PEs. We note that one of the inputs can be shared by

Figure 4: Block diagram of PMAG

all PEs in any operations of DNN.

Based on the size of common input, operations in
Fig. 2 can be classiﬁed as small common data (ex: con-
volution with small kernels) and large common data
(ex: matrix-matrix multiplication). For example, the
weights of small kernels in the convolution layer would
be small common kernels whereas the large weight ma-
trix in a fully connected layer corresponds to large com-
mon data. The approach used in NeuroTrainer is to
buﬀer copies of small common data across PEs and
stream partitions of large data (e.g. inputs to a layer)
from the local vault. Alternatively, with respect to large
common weight matrices, they can be broadcast from
a shared vault to all PEs, while partial weight matrices
are stored across PEs. These two classes of data ﬂows
are illustrated in Fig. 3.

The data rearranging among vaults is required to dy-
namically change data ﬂow from one type of layer to
another. However, in a DNN, a set of convolution layers
is followed by a set of fully connected layers; therefore
rearrange is required only once in both feedforward and
backpropagation.

3.2 Programmable Memory Address Genera-

tor (PMAG)

The programmable memory address generator (PMAG)

controls the data ﬂow by providing memory address
to the vault controller for read and write, and push-
ing the data through the NeuroTrainer. The PMAG
is composed of 7-level nested counters (r1...r7), com-
binational logic to generate address, and decoders to
assign counter values as input of combinational logic
(Fig. 4). The PMAG also computes the non-linear func-

4

Figure 5: Convolution (X and W ). (a) X is par-
titioned into 4 pX for 4 PEs, (b) Convolution
feedforward, (c) Convolution backpropagation.

Figure 6: Convolution weight update when NI is
2. It will generate dW0 and dW1 by dWi = Xi ∗dYi.
Final dW is average of dW0 and dW1.

tion (and its derivative) by using look up tables [LUTs,
for f (x) and f (cid:48)(x)] for (a) activation function (ReLu,
tanh, etc.) or (b) exponential/logarithm for softmax
and cross-entropy layer.

Convolution Feedforward / Backpropagation.
Fig. 5 shows input X is partitioned into 4 pX with
boundary overlap for convolution (assume 4 PEs). As
kernel size is small, kernels are duplicated into all PE’s
buﬀers. For each kernel (outer most loop is NO), NM AC
inputs are processed in parallel (SIMD). For backprop-
agation, transpose of W (W T ) is required and it can be
handled in PE without reshaping data in the buﬀer of
PE. It will be explained in Section 3.3.4.

Convolution Weight-update. After generating dY ,
dW is needed to update weights. Fig. 6 shows convolu-
tion weight update when NI is 2. For each sample (Xi),
dWi = Xi ∗ dYi is computed, and ﬁnal dW is computed
by averaging all dWis. Although weight update is also
convolution between X and dY , the kernel size (WO by
HO) is similar to the input size (WI by HI ). Due to
large kernel (dY ), partitioning input (X) with bound-

5

Figure 7: Matrix-matrix multiplication using 4
PEs. Each PE computes pA × X = pAX.

ary overlap (Fig. 5) is ineﬃcient and duplicating dY
into all PEs is impractical. Therefore, we convert con-
volution with large kernel to matrix matrix multiplica-
tion by lowering convolution similar to how cuDNN per-
forms convolution [20] (Fig. 6 (b)). Although drawback
of lowering is increasing memory requirement from Xi
to XM i, in-memory computation in NeuroTrainer can
resolve the memory challenge.

Matrix-matrix multiplication. The main opera-
tion of fully connected layer or recurrent layer is matrix-
matrix multiplication (A × X = AX) [21, 22]. Fig. 7
shows that A is divided into 4 pAi (i: PE index) row-
wise and how a single pAi is partitioned to small blocks
(each size is L×P ), which is ﬁtted into half size of buﬀer
in PE (double buﬀering). As explained in Section 3.1,
two data paths operate in matrix matrix multiplication
(large common data) and the PMAG with common data
vault and the PMAG with independent data vault are
programmed separately. Fig. 7 shows that pAi is par-
titioned into 3 by 2 blocks. After processing ﬁrst 3
blocks of pAi and X, a block of pAXi is generated (size
= NM AC by H). The pAXi needs to be delivered to
common data vault.

Vector-Vector Outer product. For weight update
in FC layer, for each sample in batch, input (X) and
gradient (dY ) need to be multiplied to generate dW .
Contrast to matrix-matrix multiplication, Ni samples
cannot be unlooped in SIMD level. In other words, this
operation should be repeated Ni times and dW needs to
be averaged. Fig. 8 shows vector-vector outer product
using 4 PEs. Vector A is divided into 4 vaults (pAi, size
= H) and B will be stored in common data vault and
will broadcast. The operation inside PE is similar to
that of matrix matrix multiplication, however, the out-
put (pA(cid:48)pBT ) does not need to be merged to common
vault since it’s gradient of weight in FC layer; therefore
it’s written back to dedicated vault.

Data Preparation Fig. 5 (a) shows that convolu-
If it is
tion with 4 PEs generates 4 pY s in parallel.
the last convolution layer before fully connected layer,
the outputs of convolution layer should be merged into
common data vault before to be broadcast in matrix

(a) UP: dW = X * dY∈∈∈dW = 1/2 (dW0 + dW1)...dWNI = 2HKWKDK...1NOdW1WOHONOHIWIDIHKWKDK...1NOdW0WOHONOHIWIDIpApApApAA×=pAXiHWXLNMACpAiWHLP(a,b)=(0,0)(0,1)(0,2)(1,0)(1,1)(1,2)Local Mem size: P × LPE[i]A (4H by W) × X (W by NMAC) = AX (4H by NMAC) (Original)pA (H by W) × X (W by NMAC) = pAX (H by NMAC)   (per PE) NMACTable 1: Comparison of diﬀerent ﬁxed point
MAC designs with IEEE 754 single precision
ﬂoating point MAC. All designs are synthesized
with 15nm FinFet [23] operating 2.5GHz.

Area (um2)
2093.88
Float 32
986.23 (-52%)
Fixed 32/16
Fixed 32/16 SR [24]
2072.44 (+1%)
Fixed 32/16 SR LO 1578.71 (-24%)

Power (mW)
5.37
2.27 (-57%)
5.79 (+7%)
3.78 (-30%)

Figure 10: Training accuracy for RNN with dif-
ferent numeric representation (SR: Stochastic
Rounding).

(inputs), we use two inputs and output buﬀers in PE.
For each buﬀer, while half of memory is consumed by
MACs (computing operation), rest of buﬀer can be ﬁlled
simultaneously (double buﬀering). All local buﬀers have
address generator based on nested counters. In Fig. 9,
CNT2 is two level nested 16bit counters and CNT1 is
single level 16bit counter. Data stream between DRAM
and PE ends with END-MARK (0xFFFF for 16bit case,
0xFFFFFFFF for 32bit case) Computing in the PE
starts only both input buﬀers are ready (half ﬁlled).

3.3.2 Multiply and Accumulate Units

As primitive arithmetic operator, a row of k multi-
plier and accumulator (MAC) units is placed in a PE.
Although, reduced precision (16 bit ﬁxed-point) is ac-
ceptable for inference (forward propagation), even 32bit
ﬁxed point in backpropagation and parameter update
may result in inaccurate training in deeper network, in
particular, the recurrent networks
[25] as illustrated
in Fig. 10. We should note that there is no accuracy
degradation between SR and SR LO. The stochastic
rounding (SR) can be applied to overcome quantization
error in ﬁxed point [26, 25].

Therefore, we design MAC to operate 1) two pairs
of 16bit operands or 2) a pair of 32bit operands (Fixed
32/16). To add stochastic rounding, 64 random num-
ber generators are added [25] (Fixed 32/16 + SR). To
reduce power/area overhead, we propose to add a sin-
gle random number generator is used and it generates
a single bit in every clock (Fixed 32/16 + SR LO,

Figure 8: Vector-vector outer multiplication us-
ing 4 PEs. Each PE computes pA × BT = pABT .

Figure 9: Block diagram of PE composed of
three local buﬀers (input1, input2, and output),
k MAC, k comparators.

matrix multiplication (Fig. 3). The order of PE to send
data is pre-determined in the BUS. Based on this order,
PMAG connected to common data vault also knows the
portion in the merged data (P W , P H). In similar way,
data from common data vault is also partitioned to
all other vaults.

Add/remove zero boundary.Before convolution,
input needs to be zero padded on the boundary to
return same sized output based on the kernel radius r.

3.3 Processing Elements (PE)

A processing element is composed of a k MACs ar-
ray, k comparators, and three local buﬀers: two input
buﬀers, one output buﬀer (partial sum) (Fig 9). Simi-
lar to PMAG, PE also needs to be programmed before
the main computation.

3.3.1 Local Buffers

To avoid stall of PE (idle mode) due to lack of operands

6

=ABTABTpABTNI = 3}#MACsVault 0hCommon VaultPE 0ABT}Avg. for NIpA#MACspA0’pBTpA’pBTVault 3PE 3#MACspA3’pBTpA’pBT...Bus broadcasting×BT#MACsWHh × #MACs = PSUM Cap.pA0’pBTpA3’pBTBUFInput1BUFInput2BUFOutputk MAC arraysk × 32...32...y = ax+y3232IndependentVaultBUSProcessing elements (PE)CNT...k comparatorsy = max(x,y)...CLKOp. ModeOp. Mode: MAC or MAX16bit or 32bitrange of loop16CNTCNTLRFigure 11: Fixed 32/16 + SR LO: Fixed 32/16bit
MAC with low overhead stochastic rounding
unit: a single LFSR and 32bit left shift regis-
ter.

Fig. 11)). Synthesis in 15nm FinFET [23] shows that
proposed design provides higher energy-eﬃciency (Ta-
ble 1) while providing similar training accuracy as ﬂoat-
ing point design. The MAC operates in the 16bit mode
without SR during inference (forward propagation).

3.3.3 Comparator Unit

Since MAX operation is required only for the max-
pooling inference, 16 bit ﬁxed point comparators are
placed in PE. Based on pooling radius (r), r2 data are
streamed into controller, and the comparator unit re-
turns the maximum value and its ID for backpropaga-
tion.

3.3.4 PE Operation

After two input buﬀers are ﬁlled (BUF Input 1 and
BUF Input 2), BUF Input 1 pushes one 32bit input (one
32 bit operand or two 16 bit operands) while BUF Input
2 pushes k (NM AC) 32bit inputs. For MAC operation
(all cases except max pooling), k MAC arrays compute
y = ax + y, where x and y are vectors, which length is
k (32bit) or 2k (16bit). For MAX operation (max pool-
ing), k comparator returns max value as y = max(x, y).
Convolution. In convolution, k inputs are processed
by k MACs in parallel (SIMD level). Therefore, kernels
are stored in BUF Input 1 and k inputs are stored in
BUF Input 2. If k inputs cannot be stored in BUF Input
2 due to capacity issue, k subsets of k inputs are stored
and newly required input (k×Hk) is updated during the
operation similar to [3]. For convolution backpropaga-
tion, W T is easily obtained by sweeping counter values
in CNT2 attached to BUF Input 1.

Matrix-Matrix multiplication. Similar to convo-
lution, k inputs are processed in parallel (SIMD level).
Therefore, partial weight matrix is loaded in BUF In-
put 1 and k partial inputs are stored in BUF Input
2. After consuming one partial weight matrix [a,b] (P
by L in Fig. 7), next partial weight matrix [a,b+1] is
processed. Similar to convolution, W T is obtained by
sweeping counter values in CNT2 attached to BUF In-
put 1.

Vector-Vector outer product. In fully connected
In

update, k inputs cannot be processed in parallel.

Figure 12: Programming NeuroTrainer by host
for given DNN.

computing ABT , A is loaded in BUF Input 1 and B is
loaded in BUF Input 2. In other words, k elements of B
is delivered into k MAC units in a single clock (Fig. 8).

3.4 BUS Interface

Bus interface has two operation modes controlled by
common data vault: broadcasting to all PEs and merg-
ing data into common data vault from all PEs. The
BUS and PE communicates using three-way handshak-
ing (REQ-ACK-SEND) for both operations. Broad-
casting mode is set when all PE can take data (input
buﬀer is ready) and during the broadcasting, any REQ
from PE is ignored (broadcasting is prior to merging
mode). During the merging mode, all PE send REQ
and get ACK from the bus based on predetermined pri-
orities among PEs. Although all 15 PEs request BUS
for writing-back simultaneously, the impact of latency
of entire writing back (for 15 PEs) on the throughput
can be minimized as PE’s computing latency dominates
entire computing latency. The bus architecture is de-
signed and synthesized to guarantee a bandwidth same
as that of a single vault (10GBps). We use 4 stage pipe-
lined BUS interface [27]; it takes 4 clock cycle between
a vault to any PE.

4. PROGRAMMING

Following the discussions in Section 3.2, Table 2 and
Table 3 summarize the PMAG programming which in-
cludes setting range of 7 nested loops (r1 ∼ r7) and con-
necting counter values to combination logic for diﬀer-
ent operations. For matrix-matrix multiplication (FC-
FF/BP) and vector outer product (FC-UP), C.Vault
is the programming value for PMAG attached to com-
mon data vault and I.Vault is the programming value
for PMAG attached to independent data vault. Similar
to PMAG, PE needs to be programmed to set: 1) op-
eration type: MAC or MAX, 2) bit precision mode for
MAC: 16 bit or 32 bit with/without SR , and 3) loop
range for address generator for local buﬀers. Based on
the discussions in Section 3.3.4, Table 4 summarizes
the inputs for PE programming for diﬀerent operations.
In essence, the preceding three tables deﬁnes the in-
struction set architecture of the NeuroTrainer.

Given a DNN, the host ﬁrst generates the preced-

7

32/16bit MAC64bit Adder64RAND32bit left-shift registerLFSR 8-deptha random bit0/13232Croppper32MASK6432{1’b0}Low overhead Stochastic Rounding Unit32MAC Output2Table 23Table 34Table 4R1R2R3R4R5R6R7C1-FF9655553231111FC1-FF932321283211FC1-BP96432643211C1-BP322422432961111C1-UP132555531111FC1-UP51283232111AlexNet7 level nested countersHostExt. I/ONeuroTraineriBUF234PMAGPMAG...Table 2: Programming PMAG for Convolution and Fully connected layer

7 level nested counters

Conv. In - Out

p

q

R1
NO
DI
1

R4
R2
R3
NI
HO WO
WI
HI
NI
HO WO
NI

R7

R5 R6
s
DK HK WK r2
NO HK WK r2
DI HK WK r3

t
r6
r6
r6

u
r3
r3
r4

v
r7
r7
r7

H/P W/L

H/P W/L

H/h

H/h

W
NM AC

W
NM AC

P

P

L

L

NI NM AC

NI

h

K

K

1

1

1

1

1

1

1

1

1

1

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

f (a,b,c,d)

a
r4
r4
q

r4

b
q
q
p

r2

c
p
p
r5

r5

d
r5
r5
r2

0

r4

r3

r2

r1

r4

r3

r2

r1

r4

r3

r2

r1

Conv-FF
Conv-BP
Conv-UP
FC-FF/BP
(C. Vault)
FC-FF/BP
(I. vault)
FC-UP
(C vault)
FC-UP
(I. vault)

a, b, c, d, p, q, s, t, u, and v are labels used in Fig. 4.
R1 ∼ R7: maximum value of r1 ∼ r7 loop. (minimum value are all zero)

Table 3: Programming PMAG for data rearranging and data preparation

7 level
nested counters
R3
R2
R1
DI P HI P WI

Merge

Partition DI HI

WI

s
-

-

Add pad DI P HI P WI

r3

Remove
pad

DI P HI P WI

-

Conv. In - Out

p

q

f (a,b,c,d)

Two comparators

t
-

-

r

-

u
-

-

r2

-

v
-

-

r

-

a
r3

0

p

b
r2

0

q

c
r1

0

r1

r3

r2

r1

d
0

1

0

0

h
-

r2

r3

r3

H
-
0 ∼
P HI
r ∼
r + WI
r ∼
r + WI

k
-

r3

r2

r2

K
-
0 ∼
P WI
r ∼
r + HI
r ∼
r + HI

a, b, c, d, p, q, s, t, u, and v are labels used in Fig. 4.
R1 ∼ R3: maximum value of r1 ∼ r3 loop. (minimum value are all zero)
R4 ∼ R7: 1

Table 4: PE Program for computing operations

CNT2

CNT1

Bit
Conv-FF
16 HK, WK WK × HK
Conv-BP 32 HK, WK WK × HK
Conv-UP 32
16
32
32

FC-FF
FC-BP
FC-UP

P, L
P, L
P, L
h

L
L
L
1

HK, WK: dimension of convolution kernels
P, L: dimension of partial matrix (Fig. 7)
h: length of partial vector (Fig. 8)

controlled by the iBuﬀer (using a layer counter). To
estimate the size of the iBuﬀer, consider that for a
network with N layers, we need to program Neuro-
Trainer ∼ 4N times (Feedforward, backpropagation,
weight update, data preparation if needed). Each time
the amount of data for programming is 22Byte (18Byte
for PMAG and 4Byte for PE). Therefore, a 16KB mem-
ory is suﬃcient as iBuﬀer and it can cover 186 layers.
The latency of programming the iBuﬀer through HMC
external interface is negligible compared to loading the
input data.

5. SIMULATION RESULTS

5.1 Performance Analysis

ing three tables. Fig. 12 illustrates the programming
process of the NeuroTrainer. To enable autonomous
operation of the NeuroTrainer, we embed an on-chip
instruction buﬀer (iBuﬀer) to store the preceding three
tables (Figure 1(a)). Given a DNN, the host gener-
ates the preceding three tables and loads them in the
iBuﬀer. During execution the layer-wise operation is

The performance of the NeuroTrainer is simulated us-
ing cycle-level simulator. All simulation results is based
on minibatch size 32, which is recommended minimum
size of minibatch [28]. All MACs, comparators, buﬀer
in PE, BUS interface, PMAG are synthesized operate
at 2.5GHz to maximize the single vault’s bandwidth.

Fig. 13 shows throughput (TOPS/s) and latency (sec-

8

Figure 13: Simulation result for Alexnet in terms of latency (second) and throughput (Tera-Ops/sec:
TOPS/s). C1 - C5: convolution layer, FC1 - FC3: fully connected layer, Prep: data preparation

ond) for a single input of each layer in Alexnet. For the
one input image, inference took 0.31mS (3,228 images
per second) and training took 1.97mS (507 images per
second).

In feedforward phase, all convolution or fully con-
nected layers shows similar throughput above 4.0 TOPS/s
(4.2TOPS/s ∼ 4.7TOPS/s) which is close to the theo-
retical maximum for 16bit operation of our MAC (2.5GHz
× 15 PEs × 32 MACs × 2 pairs inputs × 2 (multipli-
cation and addition) = 4.8TOPS/s).

For backpropagation and weight update, 32bit with
stochastic rounding is used. Theoretical maximum through-
put can be computed as 2.4 TOPS/s in the same man-
ner. In backpropagation, FC3 (1.61 TOPS/s) and C1
layer (1.19 TOPS/s) show lower throughput than oth-
ers. For FC3 backpropagation, the size of dY is not
large enough to hide latency of writing back from all
PEs to a single vault. In other words, the latency to
generate output by iterating dY times is shorter than
writing back the output to common data vault; writing
back becomes bottleneck in the system. For C1 layer,
input dimension is 55 × 55 × 96 and kernel dimension is
11×11×3. It can be processed as convolution since ker-
nel size is small enough to ﬁt in the local memory; but
eﬃciently due to large kernel size compared to input.

In weight update, C1 ∼ C5 shows about 1.98 TOPS/s
by translating convolution as matrix multiplication in
large kernel case. However, FC layer (vector vector
outer multiplication) shows about 1.02 TOPS/s, which
is the worst case due to high network traﬃc between PE
and independent vault since there is no data re-usage.

9

Figure 14: Simpliﬁed example of DNN for gen-
erating sentences for image description [29].

To see the performance of more complex and deeper
network, we evalaute a DNN for generating image de-
scription [29], image feature extraction part is imple-
mented as Alexnet and RNN (GRU) is attached after
5th convolution layer (Fig. 14). A single GRU is com-
posed of six fully connected layers for hidden neurons
and one fully connected layer for output neurons. The
number of input neurons in GRU is 43,264 and the num-
ber of hidden neurons in GRU is 10,000. We assume T
for DNN is 100. Fig. 15 shows latency of each layer in
DNN explained earlier. For the recurrent layers (in the
dashed box), the latency is computed considering time
windows (latency to across all time unfolded T layers);
that’s why it shows high latency than other layers.

0.001.002.003.004.005.00C1C2C3C4C5FC1 PrepFC1FC2FC3FC3FC2FC1FC1 PrepC5C4C3C2C1C1C2C3C4C5C1 PrepC2 PrepC3 PrepC4 PrepC5 PrepFC1FC2FC3Throughput (TOPS)0.00E+005.00E-051.00E-041.50E-042.00E-042.50E-04C1C2C3C4C5FC1 PrepFC1FC2FC3FC3FC2FC1FC1 PrepC5C4C3C2C1C1C2C3C4C5C1 PrepC2 PrepC3 PrepC4 PrepC5 PrepFC1FC2FC3Latency (Second)FeedforwardBack propagationWeight UpdateFeedforwardBack propagationWeight UpdateTraining 1 image : 1.97mSInference 1 image: 0.31mS2D ConvolutionPixel-levelRecurrent connectionInputOutputMultiple fully connected layersTime window = 100OutputTime-unfoldingFigure 15: Latency analysis of each layer in DNN [29].

Table 5: Power and Area analysis of Neuro-
Trainer synthesized in 15nm FinFet [23].

Area (mm2) Power (W )

PE
PMAG
Vault Ctrl.
32bit Bus
16KB: I-BUF

Logic die
4 DRAM dies

6.96E-02
2.00E-03
7.73E-04
8.96E-03
5.51E-03

1.17E+00

1.55E-01
3.16E-03
4.27E-03
3.70E-02
1.02E-02

2.65E+00
2.03E+00

Figure 16:
benchmarks.

Simulation results for diﬀerent

Fig. 16 shows the throughput for various benchmarks
including Resnet 152 [30], VGG 16, VGG 19 [31], Incep-
tion V3 [32], GRU [22], DNN for image description [29],
and MLP0 [9] are also tested. Y-axis represent the
throughput (TOPS/s) and the number on the X-axis
represent the number of inputs can be trained in a sec-
ond for each benchmark. For all benchmarks, infer-
ence shows 4.0 ∼ 4.7 TOPS/s and training shows 1.9
TOPS/s. Further, NeuroTrainer shows stable through-
put (standard deviations less than 6% of average) for
training with all benchmarks of varying complexity.

5.2 Synthesis and Power Analysis

The computation fabric of the NeuroTrainer, includ-
ing the PEs, bus interface, and PMAG with the vault
controller is synthesized using 15nm FinFet [23]. As
vault controller is a proprietary design, a 32bit SDRAM
controller [27] is adopted as a reference vault controller.
Table 5 summarizes average power across 8 diﬀerent
benchmarks and area overhead of each module in the
system. Total power consumption of logic layer is 2.64W
and area overhead (including vault controller) is 1.17mm2.
Even scaled up to compare with previous result syn-

thesized in 28nm CMOS [33], total area is less than
5% of footprint of fabricated HMC (68mm2). Average
DRAM die power is computed during the simulation
using 3.7pJ/bit from [33] and actual DRAM access pat-
tern. The power densities of the logic die (0.039W/mm2)
and DRAM dies (0.030W/mm2) in NeuroTrainer is well
within the acceptable power densities (1.5W/mm2, [34],
of 3D stacked systems.

From DRAM power consumption (2.03W), average
memory bandwidth can be computed as 68.5GByte/sec
(2.03W/3.7pJ/bit), which is lower than total aggregated
memory bandwidth of HMC (16 vaults × 10GByte/sec).
With batch size of 32, weights are reused 32 times.
DRAM utilization can be increased by, 1) more MACs
per PE, but requires larger partial sum SRAM and less
eﬃcient for the small batch (minimum batch size for
most of DLs is 32) and 2) more PEs, but it requires a
larger network among PEs and vaults.

On average, NeuroTrainer consumes 4.64W and de-
livers 1.89 TFLOPS throughput and 406 GFLOPS/W
of eﬃciency during training (32bit) while maintaining
high training accuracy.

For HMC 2.0 [35], performance is estimated (Table 6).
With 32 vaults, 31 PEs can be placed; therefore through-
put and logic power increases about twice. However,
power of DRAM dies is same since total memory access
is constant. Therefore, it shows 39% gain in eﬃciency.

10

1.00E-071.00E-061.00E-051.00E-041.00E-031.00E-021.00E-01FeedforwardBackpropagationWeight UpdateLatency (Second)Training 1 image : 479.72mSInference 1 image: 68.29mS: layers in GRU. latency is reported for T ( = 100) iterations0.001.002.003.004.005.00Throughput (TOPS)InferenceTrainingTrainingImg/sec 65       47       34        98      507      328       2       15,871Table 6: Comparison with previous training ac-
celerators

Work

NC [4] NS [6] SD [13]

Bit

16 FI

32 FL

Node
(nm)
Peak
TFLOPS
Power
(W)
Eﬃciency

15

0.13

3.4

38.8

28

0.96

42.8

22.5

16 FL
/32FL

14

1400
/680

1,400

331.7

NT
16 FI
/32 FI*

15

4.4(9.6)
/1.9(4.1)

4.7(7.2)

406(566)

NC: NeuroCube, NS: NeuroStream, SD: ScaleDeep,
NT: NeuroTrainer.
FL: ﬂoating point, FI: ﬁxed point,
FI*: ﬁxed point with stochastic rounding
Eﬃciency: GF LOP S/W
For NT, () indicates estimated for HMC 2.0
For NT, power is averaged across 8 benchmarks
illustrated in Fig. 16.

processing unit and HMC external BW. For example,
Tegra K1 (326 GFLOPS) can update weights in 42.4mS
for AlexNet (138M parameters) since itˆa ˘A´Zs element-
wise operation. The latency between a NeuroTrainer
and K1 is 4.61mS (240 GByte/sec). If system is com-
posed of 4 NeuroTrainers with a K1 as host, total la-
tency is 63.1mS (training latency in NeuroTrainer) +
42.4mS × 4 (K1 needs update W using 4 dWs) + 2 × 4
× 4.61mS (round trip from a K1 to 4 NeuroTrainrs)
= 269.58mS while training 4 × 32 samples.
In the
same manner, a single P100 in DGX-1 can train 150
images per second [39] with 400W power consumption.
For the same power budget, 64 NeuroTrainer can op-
erate in parallel and train 1,900 images delivering 13x
speedup. The additional power consumption due to oﬀ-
chip data movement estimated using HMC access en-
ergy of 10pJ/bit [33]. Ultimately, the performance scal-
ing in NeuroTrainer is limited by the oﬀ-chip latency
showing need for better system architecture and faster
oﬀ-chip network.

6. RELATED WORK

Table 6 compare NeuroTrainer with previously re-
ported DNN training accelerators. NeuroCube [4] and
NeuroStream [6] presents inference engines using in-
memory accelerators, which can also perform training.
The results demonstrate higher eﬃciency over a GPU-
baseline showing the promise of hardware acceleration.
However, performance gain is nominal as no hardware
was optimized for training.

Scaledeep [13] proposes specialized hardware for dif-
ferent computation kernels. A multi-chip module is syn-
thesized using ﬁve diﬀerent tiles (heterogeneous archi-
tecture) and allocating layers to diﬀerent tiles based on
their property (such as Byte/Ops). The design demon-
strates better power eﬃciency over GPUs.

Figure 17: Scalability:
(a) system of multi-
ple NeuroTrainer. (b) Performance for VGG16
with central core being NeuroTrainer (VGG16
NT), Tegra K1 (VGG16 K1), and DGX-1 (P100)
(VGG16 DGX-1) .

5.3 Scalability to Multiple NeuroTrainers

The multiple NeuroTrainer can be used in paral-
lel for scalable training performance as illustrated in
Fig. 17 (a). As all NeuroTrainer take same latency (T1)
for training a minibatch, we propose to perform syn-
chronous training [18]. After training a single input
batch, N NeuroTrainer delivers dW to a central unit
(latency = T2). The central unit needs to take all dW
from N NeuroTrainer s, and generates new W (W (cid:48)) fol-
lowing: W (cid:48) = η×average(dW )+W , where η is learning
rate. The above computation can be performed by an-
other NeuroTrainer. However, to cover more generic
approaches for weight update (e.g. AdaGrad [36] or
Adam [37]), a software implementation, for example,
using Tegra K1 (326 GFLOPS, 10W, 28nm) [38] can
also be considered.

Fig. 17 (b) shows estimated training performance (num-

ber of images per second) of VGG 16 [31] by diﬀerent
number of NeuroTrainers and two diﬀerent types of cen-
tral core. Training performance using high-end GPU
(NVIDIA DGX-1) is also reported. This estimated per-
formance is computed based on peak FLOPS of each

11

05001,0001,5002,0002,5003,000020406080100120140Number of NeuroTrainersTraining #images/secVGG16 NTVGG16 K1VGG16 DGX-1dW1dW2W’dW3NT 1NT 2NT 3CentralCoreNeuroTrainerW’W’W’ = f(W, dW)T1T2T31. NeuroTrainer2. Tegra K1782W(a)(b)393W1 × P100(400W)2 × P100(800W)The main diﬀerence between NeuroTrainer and Scale-
Deep is the orthogonal approaches to optimize eﬃcien-
cies of diﬀerent kernel. Rather than changing a data
ﬂow in the hardware for diﬀerent operations as per-
formed in NeuroTrainer, ScaleDeep decides the tile dis-
tribution during design. Consequently, if the layer dis-
tribution in DNN architecture does not match the tile
distributions, for example, if one kind of layer (convolu-
tion or fully connected) dominates the entire network,
the tile utilization and eﬃciency is low. This eﬀect is
evident from [13] (see Fig. 20) which shows even for
various CNN benchmarks, the standard deviation of ef-
ﬁciency is about 28% of average, which is expected to
increase further if recurrent networks are considered.

In contrast, the NeuroTrainer uses a homogeneous ar-
chitecture and dynamically changes the data ﬂow and
data mapping to optimize the performance of individ-
ual layers. The dynamic optimization, instead of design
time decisions, allow NeuroTrainer to maintain simi-
lar throughput for much wider classes of benchmarks
even including RNNs. The homogeneous architecture
also makes the design easier to scale for parallel train-
ing. The secondary diﬀerence comes from the use of
3D in-memory acceleration in NeuroTrainer to reduce
data movement power, and ﬁxed point arithmetic with
stochastic rounding for higher eﬃciency (compared to
ﬂoating point in ScaleDeep).

7. CONCLUSION

We have presented NeuroTrainer, an intelligent mem-
ory module with in-memory accelerators for energy-
eﬃcient training of diﬀerent classes of DNNs. The Neu-
roTrainer utilizes a programmable data ﬂow based exe-
cution model to optimize memory mapping and data re-
use during diﬀerent phases of training operation. The
simulation results demonstrate potential for apprecia-
ble performance and power-eﬃciency gain over baseline
GPU or alternative accelerators. The NeuroTrainer can
form the building block of a scalable architecture for
energy eﬃcient training for deep neural networks. Ul-
timately, the performance scaling in a scalable training
platform with NeuroTrainer is limited by the oﬀ-chip la-
tency showing need for future research on better system
architecture and faster oﬀ-chip network.

12

8. REFERENCES

[1] S. Han, X. Liu, H. Mao, et al., “Eie: eﬃcient inference

engine on compressed deep neural network,” arXiv preprint
arXiv:1602.01528, 2016.

[2] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li,

T. Chen, Z. Xu, N. Sun, et al., “Dadiannao: A
machine-learning supercomputer,” in Microarchitecture
(MICRO), 2014 47th Annual IEEE/ACM International
Symposium on, pp. 609–622, IEEE, 2014.

[3] Y.-H. Chen, J. Emer, and V. Sze, “Eyeriss: A spatial

architecture for energy-eﬃcient dataﬂow for convolutional
neural networks,” in Computer Architecture (ISCA), 2016
ACM/IEEE 43rd Annual International Symposium on,
pp. 367–379, IEEE, 2016.

[4] D. Kim, J. Kung, S. Chai, et al., “Neurocube: a

programmable digital neuromorphic architecture with
high-density 3d memory,” in Computer Architecture
(ISCA), 2016 ACM/IEEE 43rd Annual International
Symposium on, pp. 380–392, IEEE, 2016.

[5] M. Gao, J. Pu, X. Yang, et al., “Tetris: Scalable and

eﬃcient neural network acceleration with 3d memory,” in
Proceedings of the Twenty-Second International Conference
on Architectural Support for Programming Languages and
Operating Systems, pp. 751–764, ACM, 2017.

[6] E. Azarkhish, D. Rossi, I. Loi, et al., “Neurostream:

Scalable and energy eﬃcient deep learning with smart
memory cubes,” arXiv preprint arXiv:1701.06420, 2017.

[7] E. Nurvitadhi, J. Sim, D. Sheﬃeld, et al., “Accelerating

recurrent neural networks in analytics servers: Comparison
of fpga, cpu, gpu, and asic,” in Field Programmable Logic
and Applications (FPL), 2016 26th International
Conference on, pp. 1–4, EPFL, 2016.

[8] D. Shin, J. Lee, J. Lee, and H.-J. Yoo, “14.2 dnpu: An 8.1

tops/w reconﬁgurable cnn-rnn processor for
general-purpose deep neural networks,” in Solid-State
Circuits Conference (ISSCC), 2017 IEEE International,
pp. 240–241, IEEE, 2017.

[9] N. P. Jouppi, C. Young, N. Patil, et al., “In-datacenter
performance analysis of a tensor processing unit,” in
Proceedings of the 44th International Symposium on
Computer Architecture, IEEE Press, 2017.

[10] https://www.tensorflow.org/performance/benchmarks/.

[11] S. Hochreiter, “Untersuchungen zu dynamischen neuronalen
netzen,” Diploma, Technische Universit¨at M¨unchen, p. 91,
1991.

[12] S. Hochreiter, Y. Bengio, P. Frasconi, et al., “Gradient ﬂow
in recurrent nets: the diﬃculty of learning long-term
dependencies,” 2001.

[13] S. Venkataramani, A. Ranjan, S. Banerjee, D. Das,
S. Avancha, A. Jagannathan, A. Durg, D. Nagaraj,
B. Kaul, P. Dubey, et al., “Scaledeep: A scalable compute
architecture for learning and evaluating deep networks,” in
Proceedings of the 44th Annual International Symposium
on Computer Architecture, pp. 13–26, ACM, 2017.

[14] S. S. Haykin, S. S. Haykin, S. S. Haykin, and S. S. Haykin,
Neural networks and learning machines, vol. 3. Pearson
Education Upper Saddle River, 2009.

[15] J. L. Elman, “Finding structure in time,” Cognitive science,

vol. 14, no. 2, pp. 179–211, 1990.

[16] R. A. Dunne and N. A. Campbell, “On the pairing of the

softmax activation and cross-entropy penalty functions and
the derivation of the softmax activation function,” in Proc.
8th Aust. Conf. on the Neural Networks, Melbourne, 181,
vol. 185, 1997.

[17] N. Qian, “On the momentum term in gradient descent

learning algorithms,” Neural networks, vol. 12, no. 1,
pp. 145–151, 1999.

[18] F. N. Iandola, M. W. Moskewicz, K. Ashraf, and

K. Keutzer, “Firecaﬀe: near-linear acceleration of deep
neural network training on compute clusters,” in
Proceedings of the IEEE Conference on Computer Vision

and Pattern Recognition, pp. 2592–2600, 2016.

[19] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,

M. Mao, A. Senior, P. Tucker, K. Yang, Q. V. Le, et al.,
“Large scale distributed deep networks,” in Advances in
neural information processing systems, pp. 1223–1231,
2012.

[20] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen,

J. Tran, B. Catanzaro, and E. Shelhamer, “cudnn: Eﬃcient
primitives for deep learning,” arXiv preprint
arXiv:1410.0759, 2014.

[21] S. Hochreiter and J. Schmidhuber, “Long short-term

memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780,
1997.

[22] J. Chung, C. Gulcehre, K. Cho, et al., “Empirical

evaluation of gated recurrent neural networks on sequence
modeling,” arXiv preprint arXiv:1412.3555, 2014.

[23] “Nangate FreePDK15 Open Cell Library.”
http://www.nangate.com/?page_id=2328.

[24] T. J. Chainer, M. D. Schultz, P. R. Parida, and M. A.
Gaynes, “Improving data center energy eﬃciency with
advanced thermal management,” IEEE Transactions on
Components, Packaging and Manufacturing Technology,
2017.

[25] T. Na, J. H. Ko, J. Kung, and S. Mukhopadhyay, “On-chip

training of recurrent neural networks with limited
numerical precision,” in Neural Networks (IJCNN), 2017
International Joint Conference on, pp. 3716–3723, IEEE,
2017.

[26] S. Gupta, A. Agrawal, K. Gopalakrishnan, and

P. Narayanan, “Deep learning with limited numerical
precision,” in Proceedings of the 32nd International
Conference on Machine Learning (ICML-15),
pp. 1737–1746, 2015.

[27] “OpenCores.” http://http://opencores.org/.

[28] http://svail.github.io/rnn_perf/.

[29] A. Karpathy and L. Fei-Fei, “Deep visual-semantic

alignments for generating image descriptions,” in
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3128–3137, 2015.

[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual

learning for image recognition,” in Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.

[31] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” arXiv preprint
arXiv:1409.1556, 2014.

[32] C. Szegedy, V. Vanhoucke, S. Ioﬀe, J. Shlens, and Z. Wojna,
“Rethinking the inception architecture for computer vision,”
in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2818–2826, 2016.

[33] J. Jeddeloh and B. Keeth, “Hybrid memory cube new dram
architecture increases density and performance,” in 2012
Symposium on VLSI Technology (VLSIT), 2012.

[34] W. Huang, M. R. Stan, S. Gurumurthi, et al., “Interaction
of scaling trends in processor architecture and cooling,” in
Semiconductor Thermal Measurement and Management
Symposium, 2010. SEMI-THERM 2010. 26th Annual
IEEE, pp. 198–204, IEEE, 2010.

[35] Hybrid Memory Cube Consortium, “Hybrid memory cube

speciﬁcation 2.0,” 2014.

[36] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient
methods for online learning and stochastic optimization,”
Journal of Machine Learning Research, vol. 12, no. Jul,
pp. 2121–2159, 2011.

[37] D. Kingma and J. Ba, “Adam: A method for stochastic

optimization,” arXiv preprint arXiv:1412.6980, 2014.

[38] T. NVIDIA, “K1: A new era in mobile computing,” Nvidia,

Corp., White Paper, 2014.

[39] http://dlbench.comp.hkbu.edu.hk/.

13

