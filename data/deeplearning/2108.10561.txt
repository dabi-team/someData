Taming the Beast: Learning to
Control Neural Conversational Models

by

Andrea Madotto

A Thesis Submitted to

The Hong Kong University of Science and Technology

in Partial Fulﬁllment of the Requirements for

the Degree of Doctor of Philosophy

in the Department of Electronic and Computer Engineering

August 2021, Hong Kong

1
2
0
2

g
u
A
4
2

]
L
C
.
s
c
[

1
v
1
6
5
0
1
.
8
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
Contents

Title Page

Table of Contents

List of Figures

List of Tables

Abstract

1

Introduction

1.1 Chit-Chat Dialogue Systems

1.2 Module-based Task-Oriented Dialogue Systems

1.3 End-to-End Task-Oriented Dialogue Systems

1.4 Motivation and Research Problem

1.5 Thesis Outline

2 Background and Preliminaries

2.1 Basic notation

2.2 Word Embedding

2.3 Feed-Forward Neural Network

2.4 Recurrent Neural Networks

2.5 Sequence-to-Sequence

2.5.1 Attention

2.6 Transformer

2.6.1 Generalized Attention

2.6.2 Embedding Transformation

2.6.3 Encoder

ii

i

ii

v

viii

x

1

2

3

4

5

6

8

8

9

10

11

14

15

16

17

17

18

2.6.4 Decoder

2.7 Pre-trained Language Models

2.7.1 Causal-Decoder

2.7.2 Encoder-Decoder

2.8 Related Work

3 Controlling Style and Topics

3.1 Methodology

3.1.1

Plug-and-Play Language Model

3.1.2 Residual Adapters

3.2 Experimental Setup

3.2.1 Attribute Models

3.2.2 Baselines

3.2.3 Evaluation Metrics

3.3 Results

3.3.1 Quantitative Evaluation

3.4 Analysis

3.5 Short Summary

4 Controlling Dialogue Domains Continuously

4.1 Background

4.1.1 Task-Oriented Dialogue Modelling

4.1.2 Continual Learning

4.2 AdapterCL

4.3 Experimental Settings

4.3.1 Datasets

4.3.2 Evaluation Metrics

4.3.3 Baselines and Settings

4.4 Results & Analysis

4.4.1 Training Time Analysis

4.4.2 No Free Lunch

4.4.3 Analysis: Episodic Memory Size

4.5 Short Summary

iii

19

20

21

22

22

25

27

28

29

31

31

32

32

34

34

36

37

38

39

39

42

43

45

45

46

47

48

49

50

51

52

5 Controlling Multi-Skill Dialogue Systems

5.1 Methodology

5.1.1 Attention over Parameters

5.2 Experiments and Results

5.2.1 Dataset

5.2.2 Evaluation Metrics

5.2.3 Baselines

5.2.4 Results

5.3 Skill Composition

5.4 Attention Visualization

5.5 Short Summary

6 Conclusion

Reference

53

55

55

58

58

59

60

60

62

63

64

65

68

iv

List of Figures

1.1 End-to-end chit-chat dialogue system. DialGPT [1], Meena [2], and Blender-

Bot [3] are three possible instances of large pre-trained conversational models.

1.2 Module-based dialogue systems. Image inspired by Williams et al. [4].

1.3 End-to-end task-oriented dialogue system. The same sequence-to-sequence

model generates API-CALL, and system responses.

1.4 The three beasts to be tamed.

2.1 Example of sentence encoding, from 1-hot encoding to word embedding con-

version. Firstly, the words in the sentence are converted into their 1-hot repre-

sentation ˆx. Then, by multiplying the concatenation of the 1-hot of each words

by the embedding matrix E, we obtain the word embedding for each word.

2.2 Example of feed-forward (FF) neural network. The input X is passed through

multiple feed-forward layers: an afﬁne transformation with a non-linear activa-

tion – sigmoid in this example – and in the last layer a ﬁnal Softmax activation

(Equation 2.4) to generate the ﬁnal output ˆy.

2.3 Example of recurrent neural network.

2.4 Example of RNN-based sequence-to-sequence model.

2.5 Example of RNN-based sequence-to-sequence model with the attention mech-

anism.

2.6 Different attention masks. a) The input and output sequences are concatenated,

and the attention at each layer is masked to make the model unidirectional. b)

The encoder is bi-directional; thus there is no mask in the attention. c) The

decoder is uni-directional; thus it also has a mask to hide future tokens. d) The

cross-attention between the encoder and decoder is also bi-directional; thus no

mask is required.

2.7 Example of causal-decoder Transformers.

In Blue and Purple the input and

output sequence respectively.

v

2

3

4

7

9

10

11

13

15

20

21

2.8 Example of encoder-decoder Transformers. This model is equivalent to any

Seq2Seq architecture.

3.1 Example of controllable response generation. DGPT is the DialoGPT [1] re-

sponse, while the others are generated using different attribute models.

3.2 High-level description of the proposed method. Firstly, we train the attribute

model for PPLM. Then we use PPLM to generate stylistic responses (e.g., pos-

itive or negative) from 1000 dialogue histories. Finally, we distill the generated

response into residual adapters, one per attribute.

3.3 Example of PPLM applied to a large pre-trained dialogue model (DGPT [1]).

3.4 Plug-and-play adapter architecture. The Transformer layers, head, and embed-

ding layer are frozen during training, while the adapter layers are trained inde-

pendently per each attribute.

22

26

27

28

30

3.5 Human evaluation results in terms of winning rate for both humanness and at-

tribute consistency. For example, in the Attribute Consistency table, DG wins

66% of the time versus HM. Bold results are statistically signiﬁcant (p < 0.05).

33

3.6 Contour plot of the normalized sum of the log perplexity score, computed by

GPT2 [5] and the external classiﬁer loss on the response generated by PPLM

for the Negative and Positive style. The x-axis is the number of iteration, p,

and the y-axis is the step size α. Darker areas correspond to a higher loss sum,

meaning a higher perplexity and higher classiﬁcation loss. The labels represent

a sample response from a given iteration and step size.

4.1

In continual learning, the model is trained one dataset at time. In this instance,

the model is ﬁrst trained on data from the hotel domain DHotel then on the Taxi

domain DTaxi and so on. The parameters of the model are updated sequentially

based on the loss function L.

4.2 Example of input-out pairs, for the four settings, INTENT, DST, NLG and end-

to-end (E2E).

4.3 Example of CL with AdpaterCL. Every time a new domain dataset D arrives,

a new adapter is spawned, and it is ﬁne-tuned with the data pairs, while the

original weights are left frozen.

34

39

40

44

vi

4.4 Example of the perplexity-based classiﬁer. Given the sentence "I need an ho-

tel", we forward each of the adapters to compute the perplexity score. In this

example, the HOTEL adapter achieves the lowest perplexity, and it is selected

to generate the output sequence.

4.5

short

4.6 Training time for adding a new domain for different CL techniques.

4.7

"No free lunch" in CL. Plot of the trade-off between number of parameters

added per task and the size of the episodic memory M.

4.8 Ablation study on the size of the episodic-memory vs Intent Accuracy/JGA/BLEU

and EER.

5.1 Comparisons between single model, Mixture of Experts (MoE) [6], and Atten-

tion over Parameters (AoP).

5.2 Results for the Persona-Chat dataset.

5.3 AoP visualization, vector α for different reference, (Ref.) and AoP, generated

answers. Top rows (Usr) are the last utterances from each dialogue context.

6.1 The beasts are tamed.

45

48

49

50

51

56

61

63

67

vii

List of Tables

1.1 An example of a chit-chat and a task-oriented conversation.

3.1 Attribute dataset statistics and performance. State-of-the-art (SOTA) results are

taken from † [7], ‡ [8], and § [9].

3.2 Automatic evaluation results.

In all the metrics, higher is better, except for

Perplexity (Ppl.). Discrim. is the accuracy of the internal attribute model, while

Score is the accuracy of the external classiﬁer. All the results, are averaged

among the six attribute models.

3.3 Examples of generated responses for Negative and Positive with the same starter.

35

4.1 Main dataset statistics.

1

32

33

46

4.2 E2E results in terms of Intent Accuracy, Joint Goal Accuracy (JGA), Slot Error

Rate (EER) and BLUE. +Param. shows the additional number of parameters

per task (θ base model and µ task-speciﬁc parameters), and Mem. the episodic

memory size (denoted as |M|) needed per task, and Hours is the average hours

per epoch (single NVIDIA 2080Ti)required for training a new domain.

47

5.1 An example from the dataset which includes both chit-chat and task-oriented

conversations. The model predicts all the Sys turns, which include SQL query

and generating a response from the API output, which is dynamically updated

with the query results. The skills are the prior knowledge needed for the re-

sponse, where "Persona" refers to chit-chat.

5.2 Datasets statistics

5.3 Results for the goal-oriented responses in both MWOZ and SMD. The last raw

uses the Oracle, and bold-faced are best in each setting (w and w/o Universal).

Results are averaged among three run (full table in Appendix F).

54

58

61

viii

5.4 Selecting different skills thought the attention vector α results in a skill-consistent

response. AoP response activates SQL and Train.

62

ix

Taming the Beast: Learning to
Control Neural Conversational Models

by Andrea Madotto

Department of Electronic and Computer Engineering

The Hong Kong University of Science and Technology

Abstract

This thesis investigates the controllability of deep learning-based, end-to-end, generative dia-

logue systems in both task-oriented and chit-chat scenarios. In particular, we study the different

aspects of controlling generative dialogue systems, including controlling styles and topics and

continuously adding and combining dialogue skills.

In the three decades since the ﬁrst dialogue system was commercialized, the basic architec-

ture of such systems has remained substantially unchanged, consisting of four pipelined basic

components, namely, natural language understanding (NLU), dialogue state tracking (DST), a

dialogue manager (DM) and natural language generation (NLG). The dialogue manager, which

is the critical component of the modularized system, controls the response content and style.

This module is usually programmed by rules and is designed to be highly controllable and

easily extendable.

With the emergence of powerful "deep learning" architectures, end-to-end generative dialogue

systems have been proposed to optimize overall system performance and simplify training.

However, these systems cannot be easily controlled and extended as the modularized dialogue

manager can. This is because a single neural system is used, which is usually a large pre-trained

language model (e.g., GPT-2), and thus it is hard to surgically change desirable attributes (e.g.,

style, topics, etc.). More importantly, uncontrollable dialogue systems can generate offensive

and even toxic responses.

Therefore, in this thesis, we study controllable methods for end-to-end generative dialogue

systems in task-oriented and chit-chat scenarios. Throughout the chapters, we describe 1) how

to control the style and topics of chit-chat models, 2) how to continuously control and extend

x

task-oriented dialogue systems, and 3) how to compose and control multi-skill dialogue models.

To elaborate, we ﬁrstly propose a residual adapter model to control style and topics in conversa-

tional models such as DialoGPT, Meena, and Blender-Bot. Our proposed model adds less than

1.5% task-speciﬁc parameters per style/topic, making it deployable for online systems. We run

a comprehensive automatic and human evaluation to show controllability in the response gen-

eration in terms of style and topics, without losing ﬂuency without requiring dialogue-speciﬁc

datasets.

Secondly, we propose a highly controllable architectural method based on residual adapters

for continuous update of task-oriented dialogue systems with new features based on the user’s

needs, e.g., adding new slots and intents or even completely new domains. Moreover, we ana-

lyze the trade-off between performance, number-of-parameters, and episodic memory sizes in

other methods (regularization, rehearsal, architectural).

Finally, we propose a novel theoretical framework to control the end-to-end dialogue model

with multiple composable and control skills. We empirically show the effectiveness of using

specialized parameters in combined chit-chat and task-oriented datasets.

xi

Chapter 1

Introduction

Conversational AI systems interact with human users while completing user requests or simply

chatting. These systems have applications ranging from personal assistance and health assis-

tance to customer service, etc., and they are widely deployed in personal assistants such as

Apple Siri, Amazon Alexa, Microsoft XiaoIce, Google Assistant etc. Dialogue systems are

mainly categorized as task-oriented or chit-chat systems. The ﬁrst aims to help the user to

achieve certain goals (e.g., booking a restaurant) and tries to minimize the number of interac-

tions with the users. The second, on the other hand, aims to establish a general conversation

with the user and tries to maximize the length of the interaction. In both cases, the system needs

to generate responses that are coherent with the dialogue history, and in many cases they re-

quire external knowledge (e.g., Wikipedia, knowledge bases etc.). Examples of a conversation

for task-oriented systems and for a chit-chat system are is shown in Table 1.1.

Task-Oriented

I need to check my balance.

Usr:
Sys: Of course! Which account should I use?
Usr: My savings account, please.
Sys: No problem. Your balance is $139.

Chit-Chat

I am good thanks, what are you up to?

Usr: Hello! How are you today?
Sys:
Usr: Ummm. I really want to try bull riding. Do you have any interest on that?
Sys:

I would love to try, can we schedule something next week?

Table 1.1: An example of a chit-chat and a task-oriented conversation.

1

Figure 1.1: End-to-end chit-chat dialogue system. DialGPT [1], Meena [2], and BlenderBot [3]
are three possible instances of large pre-trained conversational models.

1.1 Chit-Chat Dialogue Systems

Chit-chat systems are built to chat freely with users and they do not aim to achieve any par-

ticular goals, but rather to entertain and engage. The ﬁrst system of this kind traces back to

ELIZA [10], a rule-based system that uses pre-deﬁned patterns and templates to generate the

system response. This kind of rudimentary and hand-crafted system can rarely generalize to

unseen conversations. Therefore, statistical-based chit-chat systems have been proposed [11] to

overcome this problem. These models leverage transcripts of natural spoken conversations or

crowd-sourced datasets to learn dialogue systems that can respond like humans. These systems

use either Sequence-to-Sequence (Seq2Seq) models [12] or retrieval systems[13, 14], and they

are trained end-to-end using human-to-human conversations.

More recently, large pre-trained language models [5, 15–18] have greatly improved the state-

of-the-art in many down-stream tasks. These language models are trained using the simple

log-likelihood objective over large amounts of unlabeled data (e.g., Wikipedia articles). This

approach results in large powerful language models that produce coherent text and can be used

to perform unconditional language generation. Chit-chat conversational systems are a special

case of language model where the preﬁx is the dialogue history and the continuation is a human-

like response [19]. Recently, large pre-trained language models trained on unlabeled human-to-

human conversation (e.g., from Reddit) [1–3] have shown excellent performance in modelling

human responses. Figure 1.1 provides a high-level overview and examples of a statistical-based

chit-chat dialogue systems.

2

Figure 1.2: Module-based dialogue systems. Image inspired by Williams et al. [4].

1.2 Module-based Task-Oriented Dialogue Systems

Task-oriented dialogue systems are usually built using several modules, Automatic Speech

Recognition (ASR), Natural Language Understanding (NLU), a Dialogue Manager (DM), and

Natural Language Generation (NLG). Thus they are called module-based [20–24]. Figure 1.2

provides a high-level overview of a module-based task-oriented dialogue system. In this system,

each module is used sequentially to produce the user responses. For instance:

NLU: the Natural Language Understanding (NLU) [25–33] module extracts a semantic frame

from the user utterance, which includes the domain, the intent and slots triggered in the

current turn. The domain speciﬁes the general topic of the request (e.g., banking), the

intent speciﬁes what the user wants to achieve (e.g., getting information about a bank

account), and the slots are the speciﬁc name and values of the goal (e.g., savings account).

Hence, this module includes domain and intent classiﬁers [34–36] and a slot tagger [37,

38]. The latter is a parsing task that assigns predeﬁned slots types to words.

DM: the Dialogue Manager [24, 39, 40] uses the dialogue frame provided by the NLU module

to generate a system action, a.k.a. speech-act. The later is a semantic class which rep-

resents a high-level description of the response (e.g., request_location). This module is

made of a Dialogue State Tracker (DST) and a Dialogue Policy (DP). The DST generates

a dialogue state which is a global semantic frame. This models uses the provided frame

to update the global dialogue state (e.g., frame), and they are implemented using hand-

crafted features, a complex domain-speciﬁc lexicon, and a domain ontology [20, 41, 42]

or using statistical models [4, 43]. On the other hand, the DP [44, 45] is a classiﬁer that

maps the dialogue state to the possible system actions, usually trained using reinforce-

ment learning. Importantly, the DP also issues actions for querying external knowledge

3

Figure 1.3: End-to-end task-oriented dialogue system. The same sequence-to-sequence model
generates API-CALL, and system responses.

bases using slot values present in the dialogue state.

NLG: the Natural Language Generation module uses the system action produced by the Di-

alogue Manager to generate plain text responses. Traditionally, this module is imple-

mented using template responses [46], while more recently, statistical methods [47–51]

based on Seq2Seq models have been proposed.

Other permutations of these modules have also been explored. For instance, several systems [52–

55] remove the NLU module and replace it with only a DST. In contrast, others build a very

strong NLU module and ignore the DST [56].

1.3 End-to-End Task-Oriented Dialogue Systems

End-to-end task-oriented dialogue systems [57–65] has been proposed to reduce the complexity

of the modularized systems. Differently from module-based dialogue systems, end-to-end sys-

tems train a single model directly on text transcripts of the dialogues. This task is tackled in two

ways: response selection [62, 66–70] and token by token generation of the response [58, 71–75].

In this thesis, we especially focus on the latter, which, indeed, is very similar to Seq2Seq mod-

els used in chit-chat dialogue systems. The major difference is that task-oriented systems have

to generate informative responses grounded on knowledge present in various kind of databases

(e.g., knowledge graphs). A standard strategy for end-to-end dialogue systems is to ﬁrst gen-

erate API-CALLs to retrieve knowledge, and then to provide knowledge-based information as

input to the model. Figure 1.3 shows a system where the same Seq2Seq model generates both

API-CALLs and system responses by using the knowledge as further input.

4

1.4 Motivation and Research Problem

With the emergence of powerful “deep learning” architectures, end-to-end dialogue systems

end-to-end generative dialogue systems have been proposed to optimize overall system perfor-

mance and simplify training. However, these systems cannot be easily controlled or extended

like the modularized systems. This is because a single neural system is used, which is usually

a large pre-trained language model (e.g., GPT-2), and thus it is hard to surgically change desir-

able attributes (e.g., style, topics, etc.). More importantly, uncontrollable dialogue systems can

generate offensive or even toxic responses.

In modularized/rule-based systems, adding new dialogue domains, for example, only requires

adding rules and functions to the existing codebase, while in end-to-end models this step re-

quires an expensive training process, namely, retrain the entire model with all the domains.

Similarly, chit-chat template-based systems can easily control the generated response in terms

of style and topics, but this is extremely challenging for generative conversational models.

Generally speaking, controlling dialogue systems is particularly important not only from a re-

search perspective but also to meet an industrial need. In fact, existing smart assistants (e.g.,

Amazon Alexa, Apple’s Siri etc.) on the market are slowly moving from modularized systems

to end-to-end solutions. One of the main obstacles to prevent the wide deployment of these

systems is the lack of explicit control over the generated responses. This is especially important

in larger systems, where only part of the technology is actually a neural network, while the

rest is hard coded rules. In such cases, the system developer requires speciﬁc behaviours from

the end-to-end models (e.g., positive responses, or response from a certain domain). There-

fore, having high- and low-level control over different attributes of the dialogue response is an

essential skill.

In this thesis, we study controllable methods for end-to-end generative dialogue systems.In

particular, we focus on

• Controlling Style & Topics See et al. [76] showed that being able to control the response

generation can have a signiﬁcant impact on the quality of conversations. However, con-

trolled generation from large conversational models such as DialoGPT [1], Meena [2] and

Blender-Bot [3], remains a challenge, and is particularly more difﬁcult in the absence of

annotated conversational datasets. Therefore, we propose to use residual adapters [77],

5

which adds less than 1.5% task-speciﬁc parameters per style/topic, to make the control-

lable response generation viable for online systems, and we run a comprehensive auto-

matic and human evaluation to show that our method can control the generate responses in

terms of style and topics, without losing ﬂuency and without requiring dialogue speciﬁc

datasets[78].

• Controlling Dialogue Domain Continuously The ability to continuously updated di-

alogue systems with new features based on the user’s needs, e.g., adding new slots and

intents, or even completely new domains, is essential for a robust and deployable dialogue

systems. However, existing end-to-end dialogue models are trained with the assumption

of having a ﬁxed dataset and architecture at the beginning of the training, and they are

not designed to add new domains and functionalities through time without incurring the

high cost of whole-system retraining. Therefore, we propose a highly controllable ar-

chitectural method based on residual adapters [77] to continuously update task-oriented

dialogue systems with new features based on the user’s needs. Moreover, we analyze

the trade-off between performance, number of parameters, and episodic memory sizes of

other existing methods (regularization, rehearsal, architectural) [79].

• Controlling Multi-Skill Dialogue Systems Unlike humans who can do both, systems

for goal-oriented dialogues [20, 24] and chit-chat conversations [80, 81] are often learned

with separate models, eventually rule-based. However, end-to-end dialogue models share

the same Seq2Seq architecture for both chit-chat and task-oriented systems. These models

greatly suffer from lack of controllability and ﬂexibility. Therefore, we propose a novel

theoretical framework to control an end-to-end dialogue model with multiple composable

and controllable skills, and we empirically show the effectiveness of using specialized

parameters in combined chit-chat and task-oriented datasets[82, 83].

1.5 Thesis Outline

The thesis is divided in four main chapters, plus a conclusion. In Chapter 2, we introduce the

background notation and methodology used throughout the thesis. In Chapter 3, we describe

how to control style and topics of large generative conversational models. In Chapter 4, we

describe a ﬂexible dialogue system that is able to add conversational domains continuously.

6

In Chapter 5, we propose a novel way to control an end-to-end dialogue model with multi-

ple composable and controllable skills. Finally, in Chapter 6, we summarize the thesis and

the signiﬁcance of the controllable dialogue systems, and we discuss possible future research

directions.

Figure 1.4: The three beasts to be tamed.

7

Chapter 2

Background and Preliminaries

In this chapter, we introduce the background notation and methodology needed throughout the

thesis. In particular, we focus on parametric statistical models, such as neural networks (NN),

in the natural language processing (NLP) domain.

2.1 Basic notation

Let us deﬁned a generic supervised dataset D = {(Xi,Yi)}n

i=0, where Xi is a feature vector and
Yi its corresponding label. The dataset is split into three non-overlapping sets, namely training,

validation, and testing. The sets are used for training the model, estimating the performance of

different hyper-parameters and estimating the generalization error respectively.

In this thesis, we focus on two settings: classiﬁcation and sequence generation. We further

deﬁne these two settings by specifying the in-out pairs (Xi,Yi). In both settings, Xi is a sequence

of tokens x0, · · · , xn, which represents an ordered sequence of words, e.g., sentences in a para-

graph or utterances in a dialogue. Differently, Yi is a value from the set C = {c0, · · · , cK} for the

classiﬁcation setting and a sequence of tokens y0, · · · , ym for the sequence generation setting.

Independently of the setting, we focus on discriminative models, parameterized by θ , that learns

the conditional probability Pθ (Y |X). In sequence generation, we further expand Pθ (Y |X) using

the chain rule of probability [84] as

Pθ (Y |X) =

m
∏
i=0

Pθ (yi|y0, · · · , yi−1, x0, · · · , xn),

(2.1)

In the coming sections, we introduce model instances for θ , in both classiﬁcation and sequence

generation, and the loss functions used to train them.

8

Figure 2.1: Example of sentence encoding, from 1-hot encoding to word embedding conversion.
Firstly, the words in the sentence are converted into their 1-hot representation ˆx. Then, by
multiplying the concatenation of the 1-hot of each words by the embedding matrix E, we obtain
the word embedding for each word.

Before diving into the modelling, let us deﬁne a vocabulary of words V as a set of unique words

appearing in a large text corpus. The token x ∈ X is converted into its 1-hot representation
denoted as ˆx ∈ N1×|V |, where |V | is the cardinality of the vocabulary. In ˆx, only the element

corresponding to the token x is set to 1 and otherwise to 0. Following this notation, the input
becomes a matrix X ∈ Rn×|V |, where each row is a vector of the vocabulary size dimension.

2.2 Word Embedding

The basic block of any NLP-based neural network is the embedding matrix, which maps the

input tokes into their embedded representation. Let us deﬁne the word embedding matrix [85,
86] E ∈ R|V |×d, where d is the embedding size. The embedding matrix E is multiplied by the

input sequence X 1 to obtain its embedded representation. We denote this transformation as

H = X T ∗ E,

(2.2)

where H ∈ Rn×d is the resulting embedding for each of the tokens in the input sequence. In most

neural network libraries (e.g., PyTorch [87], TensorFlow [88]) this operation is efﬁciently done

using hashing to select the row of interest E. In this case, the input matrix X is transformed back

to a vector in which each position is the index of the element 1 in the row. Figure 2.1 describes

an example of transforming a sentence into its embedded representation. Using this notation,

1We remove the index i to improve readability.

9

Figure 2.2: Example of feed-forward
(FF) neural network. The input X is
passed through multiple feed-forward
layers: an afﬁne transformation with a
non-linear activation – sigmoid in this
example – and in the last layer a ﬁnal
Softmax activation (Equation 2.4) to
generate the ﬁnal output ˆy.

the embedding operation is denoted as H = E(X). In the rest of the thesis, we use these two

notations interchangeably.

2.3 Feed-Forward Neural Network

A feed-forward (FF) neural networks is a parametric model made of afﬁne transformations and
non-linear activations. More formally, given an input vector x ∈ Rd×1, FF networks with l layers

compute the function

FF(x) = f (W l f (W l−1 . . . f (W 0x + b0) + bl−1) + bl),

(2.3)

where each W l ∈ Rd×d and bl ∈ Rd×1 are trainable parameters, and f is an activation func-

tion, e.g., f (x) = σ (x) = 1

1+e−x . Without loss of generality, Equation 2.3 describes a FF neural
network in which each transformation W has the same size (i.e., d × d), and uses the same acti-

vation function (e.g., sigmoidal). In general, the dimensions of the transformation are arbitrary

and different activation functions can be used (e.g., ReLU).

Finally, to model the conditional probability Pθ (Y |X), the output of FF(x) is projected to the

classiﬁcation output space Y using a linear transformation and a Softmax function,

Softmax(h)i =

ehi
j=1 eh j

∑K

∀i = 1, . . . , K

(2.4)

where h = (h1, . . . , hK) ∈ R1×d. Therefore, we denote the model prediction for the class i as

ˆyi = Softmax(W FF(x) + b)i,

(2.5)

where W ∈ RK×d and b ∈ R1×d. Furthermore, we denote the overall model’s parameters with

the set θ = [W 0, b0,W 1, b1, · · · ,W l, bl,W, b]. These parameters are optimized by minimizing the

10

Figure 2.3: Example of recurrent neural network.

cross-entropy loss between the predicted ˆy and the gold y from the dataset D. Formally, the loss

is computed as

L(D) = −

D
∑
j

y j log( ˆy j).

(2.6)

In this equation, we purposely imply that this operation is vectorial. Thus we omit the further

summation over the number of classes. The loss function is minimized using the gradient

descent algorithm (i.e., backpropagation [89]). A high-level description of a FF neural network

is shown in Figure 2.2.

2.4 Recurrent Neural Networks

Differently from FF neural networks, recurrent neural networks (RNNs) are used to process a

sequence of inputs, e.g., a sequence of words, and they are able to capture temporal dependency

in the input.

Given the input sequence X = {x1, · · · , xn}, where each xt ∈ Rd×1, an RNN processes one input
at a time by updating an hidden state vector hRd×1. The RNN forward function for an input at

time step t is expressed as:

ht = σ (W xt +Uht−1 + b),

(2.7)

where U ∈ Rz×z, W ∈ Rz×d, b ∈ Rz×1 and h0 =(cid:126)0. Notice that W h,W i and bi are shared param-

eters across time steps. We denote the set of hidden states at each time step as H = [h0, · · · , hn].

These hidden states can be used to represent the temporal dependency between "input features",

e.g., the word sequence in a sentence. Figure 2.3 shows a high-level description of the model.

11

The RNN described in Equation 2.7 suffers from the gradient vanishing and explosion prob-

lem [90]. To cope with this issue, two variants of this simple architecture have been proposed:

Long-Term Short Term Memory (LSTM) and Gated Recurrent Unit (GRU).

The LSTM [91] uses three gates (input gate, forget gate and output gate) to modulate the

amount of information to be stored in the hidden memories (the hidden state and context state).
Given the input sequence X = {x0, · · · , xn}, where each xt ∈ Rd×1, the LSTM for an input at

time step t computes

W f xt +U f ht−1 + b f (cid:17)
(cid:16)

ft = σ
it = σ (cid:0)W ixt +U iht−1 + bi(cid:1)
ot = σ (W oxt +U oht−1 + bo)

˜ct = tanh (Wcxt +Ucht−1 + bc)

ct = ft (cid:12) ct−1 + it (cid:12) ˜ct

ht = ot (cid:12) σ (ct) ,

(2.8)

where W f ,W i,W 0 ∈ Rd×z, U f ,U i,U 0 ∈ Rz×z, b f , bi, b0 ∈ Rz×1, and tanh = 2

1+e−2x − 1. The
(cid:12) denotes the Hadamard product (i.e. element-wise matrix multiplication). The vector ht and

ct are usually called the hidden state and the context state. The LSTM reduces the gradient

vanishing problem greatly, since the context state does not use a sigmoidal (σ ) activation.

The GRU [92], similarly to the LSTM, modulates how much information has to be stored in

the hidden memory. However, it greatly simpliﬁes the models’ architecture and it uses only one

hidden state, like in the original RNN. Given the input sequence X = {x0, · · · , xn}, where each
xt ∈ Rd×1, the GRU for an input at time step t computes
W kxt +U kht−1 + bk(cid:17)
(cid:16)
rt = σ (W rxt +U rht−1 + br)

zt = σ

(2.9)

ˆht = tanh

W hxt +U h (rt (cid:12) ht−1) + bh(cid:17)
(cid:16)

ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˆht,

where W k,W r,W h ∈ Rd×z, U k,U r,U h ∈ Rz×z, bk, br, bh ∈ Rz×1, and tanh = 2

1+e−2x − 1. Dif-
ferently from the LSTM, the GRU architecture uses one hidden state h, but a similar gating

mechanism for avoiding gradient vanishing.

12

Figure 2.4: Example of RNN-based sequence-to-sequence model.

Notation: To improve readability, we denote the Recurrent Equations 2.7, 2.8 and 2.9 using

the following compact notations:

ht = RNN(xt, ht−1)

(ct, ht) = LST M(xt, (ht−1, ct−1))

ht = GRU(xt, ht−1).

(2.10)

(2.11)

(2.12)

Finally, we denote the set of hidden states for the sequence X = {x0, · · · , xn} as the concatenation

of the corresponding hidden states at each time step, i.e., H = [h0, · · · , hn].

Applications: RNNs are often used in three settings: classiﬁcation, sequence tagging, and

sequence-to-sequence generation. In the classiﬁcation setting, the model is trained using the

same loss function as in Equation 2.6, where the FF network is replaced by an RNN and the

last hidden state ht is used for the ﬁnal linear transformation in Equation 2.5. Differently, in

the sequence classiﬁcation and the sequence-to-sequence settings, the RNN is trained to make a

sequence of classiﬁcations. In this thesis, we focus on the sequence-to-sequence setting. Thus,

interested readers can refer to Goodfellow et al. [93] for more information about sequence

classiﬁcation tasks.

13

2.5 Sequence-to-Sequence

Sequence-to-sequence (Seq2Seq) [12] is an encoder-decoder architecture used to process se-

quences in both the input and the output. In this section, we describe an RNN-based Seq2Seq

model for language generation, thus leveraging the RNN in Section 2.4 and the word embedding

in Section 2.2.

The objective of a Seq2Seq model is to approximate the conditional probability P(Y |X), where

both X = {x1, · · · , xn} and Y = {y1, · · · , ym} are sequences, e.g., sequences of words in a sen-

tence. Formally, a Seq2Seq model generates a sequence of probability distributions, each of

which is conditioned on the previously generated tokens and the input sequence X (Equa-

tion 2.1).

The Encoder gets the tokens in X, computes their embedded representation using the embedding
0 =(cid:126)0 ∈ Rd×1, each token in X is
matrix E and passes it through an RNNenc. 2 Hence, given henc

processed by

henc
t = RNNenc(E(xt), henc

t−1).

(2.13)

The last hidden state henc

is another RNN, generates token by token the output sequence Y . Hence, given hdec

|X| is used as the initial hidden state of the decoder. The decoder, which
|X| as

0 = henc

the initial hidden state, the decoder computes

t = RNNdec(E(yt), hdec
hdec
t−1)

p(yt = ˆyt|X, y0 · · · , yt−1) = ˆyt = Softmax(W hdec

t

),

(2.14)

(2.15)

where W ∈ Rd×|V | is a linear transformation, similar to FF, that maps the hidden state into a

vector of vocabulary length size. Then, the Softmax activation normalizes the vector to obtain

a probability distribution over the vocabulary ˆyt. Figure 2.4 shows a high-level description

of an RNN-based Seq2Seq model. Given a dataset of input-output pairs D = {(Xi,Yj)}i, the

parameter of the model are optimized by minimizing:

L(D) = −

|D|
∑
j

m
∑
i=0

log p(y( j)

i

0 , · · · , x( j)
|x( j)

n , y( j)

0 , · · · , y( j)

i−1),

(2.16)

where yi j is the i-th word in the j-th example. As in Equation 2.6, this loss is minimized

using gradient descent. During testing, the model produces the output sequence in an auto-

regressive manner [94], i.e., providing the generated token as input of the next generation time-

step. Seq2Seq is a powerful generative model, but it still struggles in capturing long-term

2We use the simple RNN, but the same explanation holds for both the LSTM and GRU.

14

Figure 2.5: Example of RNN-based sequence-to-sequence model with the attention mechanism.

dependency between the generated token and the input sequence. To cope with this issue, the

attention mechanism [95, 96] has been proposed.

2.5.1 Attention

Attention is used to learn an alignment between the generated tokens Y and the input X. In

practice, this is done by scoring the current hidden state of the decoder with all the encoder

hidden states. Hence, given the encoder hidden state as H = {henc

0 , · · · , henc

|X| } and the decoder

hidden state at time step t as hdec

t

, the attention module computes the context vector as:

t = Score(hdec
α i

t

, henc
i

) ∀i ∈ 1, · · · , |X|

αt = Softmax(αt) ∀i ∈ 1, · · · , |X|

ct =

|X|
∑
k

k ∗ α k
henc
t

ˆyt = Softmax(W [hdec

t

; ct]),

(2.17)

(2.18)

(2.19)

(2.20)

where [; ] is the concatenation of the two vectors, α is the attention vector, and Score is a

function chosen among three options: dot product, bi-linear, and neural networks. These are

15

formally as:

Score(hi, h j) =

(cid:40)

hihT
j

hiW h j

(dot)

(bi-linear)

tanh(W [hi; h j])

(neural network),

(2.21)

Figure 2.5 shows a Seq2Seq model with attention over one decoding step. The attention mech-

anisms is extremely important for the success of Seq2Seq architectures, up to the point of re-

moving the RNN in favor of a (self) attention based mechanism [97].

2.6 Transformer

As described in Section 2.5, RNNs are the basic block for both the encoder and the decoder in

seq2seq models, and the attention mechanism (Section 2.5.1) plays a key roles in the success

of the overall architecture. However, RNNs, including LSTMs and GRUs, requires a non-

parallelizable operation for computing the hidden states of the sequence, which greatly limits

the speed of the model. To cope with this temporal dependency issue and to fully exploit the

attention mechanism, Vaswani et al. [97] proposed an RNN-free model, the Transformer.

Similar to RNN-based Seq2Seq, the Transformer is made of 1) an encoder, 2) a decoder, 3) an

embedding matrix, and 4) a positional embedding matrix. The latter is particularly important for

capturing the temporal dependency in the input. Differently to Seq2Seq, however, both encoder

and decoder, shares a generalized attention mechanism and FF networks only.

16

2.6.1 Generalized Attention

Given the input sequence Z = {z0, · · · , zn} ∈ Rd×n, where each zt ∈ Rd×1, the generalized at-

tention module computes:

Attention(Q, K,V ) = Softmax

(cid:18)QKT
√
d

(cid:19)

V,

(2.22)

where Q = K = V = Z. This is a generalization of the attention process in Equation 2.18, 2.19

and 2.20, where instead of considering each single hidden state and loop over it, we put all

the hidden states into a matrix and compute all the attention scores in a single operation. The

output of the attention module is indeed a matrix of the same size as the input Z.

Vaswani et al. [97] also introduce a multi-head attention mechanism, where projection matrices

(W Q,W K and W V ) are used to split Q, K and V into a sub-matrix and the attention is run in

parallel on this transformation. More formally, a multi-head attention module with h heads

computes:

MultiHead(Q, K,V ) = Concat ( head 1, . . . , head h)W O

where head i = Attention

(cid:16)
QW Q

i , KW K

i ,VW V
i

(cid:17)

,

(2.23)

where the projections are parameter matrices W Q
W O ∈ Rhd×dm.

i ∈ Rdm×d, W K

i ∈ Rdm×d, W V

i ∈ Rdm×d and

2.6.2 Embedding Transformation

Similar to Seq2Seq models, "the ﬁrst step of the Transformer is to convert the tokens in the input

sequence into their corresponding embeddings. Thus, given the input sequence X = x1, · · · , xn,

we deﬁne the embedding matrix E, which maps tokens to vectors of dimension d × 1.

17

Differently from RNNs, the Transformer does not use any recurrent states and thus by construc-

tion it is not able to model temporal dependency in the input sequence. To cope with this issue,

Vaswani et al. [97] propose a sinusoidal positional embedding. The positional embedding ma-
trix PE ∈ Rmax_len×d is made of sine and cosine functions of different frequencies depending

on the position. More formally, each element in PE is deﬁned as
pos/100002i/d(cid:17)
(cid:16)
pos/100002i/d(cid:17)
(cid:16)

PE(pos,2i) = sin

PE(pos,2i+1) = cos

,

(2.24)

where pos is the position in the sequence and i is the i-th position in the embedding dimension.

Therefore, given the input sequence X, its embedded representation is deﬁned as

H = E(X) + PE(X)

(2.25)

where PEi is the i-th position in the embedding dimension, and H ∈ Rn×d is the resulting

embedded representation of the input.

2.6.3 Encoder

The Transformer encoder is made of a stack of encoder layers, each of which is made of a

multi-head attention module, a FF layer, and two layer normalization modules. Each layer gets

the embedding H, which at the ﬁrst layer is simply the word embedding of the input, and it

returns a transformed version of this embedding matrix by computing

(2.26)

18

where FF(x) = max (cid:0)0,W 1x + b1(cid:1)W 2 + b2 is a FF neural network with two layers, and ReLU

activation. The multi-head attention step, in which Q = K = V , is often denoted as the self-

attention layer. In short, the stack of l encoder layers form a Transformer encoder T RSenc, and

the forward function is denoted as

H = T RSenc(E(X)),

(2.27)

where the embedding function E includes the positional encoder.

2.6.4 Decoder

The Transformer decoder is composed of a stack of decoder layers, each of which is made of

two multi-head modules with their corresponding layer normalization, and a FF neural network.

Each layer gets the embedding T , the last layer embedding from the encoder, denoted as H, and

returns a transformed version of this embedding matrix by computing

(2.28)

The ﬁrst multi-head attention module is further masked to avoid the current tokens to attend

to the future once. A summary of different attention masks is shown in Figure 2.6. In short,

the stack of l decoder layers forms a Transformer decoder T RSdec, and the forward function is

denoted as

T = T RSdec(E(yt), H),

(2.29)

19

Figure 2.6: Different attention masks. a) The input and output sequences are concatenated,
and the attention at each layer is masked to make the model unidirectional. b) The encoder is
bi-directional; thus there is no mask in the attention. c) The decoder is uni-directional; thus it
also has a mask to hide future tokens. d) The cross-attention between the encoder and decoder
is also bi-directional; thus no mask is required.

where T ∈ Rn×d. The t-th vector of T , denoted as Tt, is then used to generate a distribution over

the vocabulary V by computing

ˆyt = Softmax(W Tt),

(2.30)

where W ∈ Rd×|V |. This equation is equivalent to Equation 2.15, and thus we use the same

cross-entropy loss function as in Equation 2.16. This last transformation is often named the

language model head. Finally, a high-level representation of the Transformer encoder-decoder

architecture is shown in Figure 2.8.

2.7 Pre-trained Language Models

The Transformer [97] has enabled large-scale language models (LMs) trained on a huge amount

of data [5, 98–100] to greatly improve the state-of-the-art on many downstream tasks in natural

language processing. In general, pre-trained models are divided into bi-directional [99, 101],

uni-directional or casual-decoder [5, 98, 100, 102], and encoder-decoder generative [103, 104].

The bi-directional pre-trained models are trained with a masked-language model (MLM) loss,

which learns how to predict words that are randomly masked in the input. These models achieve

state-of-the-art performance in complex natural language understanding tasks [105]. On the

other hand, the uni-directional and encoder-decoder generative models are usually trained using

20

Figure 2.7: Example of causal-decoder Transformers. In Blue and Purple the input and output
sequence respectively.

the likelihood function in Equation 2.1. In this thesis, we mostly focus on generative models,

and thus we provide further details about the latter two pre-training strategies.

2.7.1 Causal-Decoder

The causal-decoder is a special Transformer-based architecture in which the encoder and de-

coder are merged into a single set of parameters.

In this architecture, the input and output

sequence is concatenated and it is provided as input to a Transformer encoder T RS, as deﬁned

in Equation 2.26. However, the attention matrices (i.e., the result of QKT in Equation 2.22)

in this encoder are masked, as in the ﬁrst self-attention step of the Transformer decoder. Fig-

ure 2.6(a) shows the attention masking for a causal decoder architecture. More formally, given

a sequence X = {x0, . . . , xn}, which can be the concatenation of the input and output sequence

or simply a sequence of tokens, the causal-decoder computes:

H = T RS(E(X)),

(2.31)

where H ∈ Rn×d. The t-th vector of H, denoted as Ht, is then used to generate a distribution

over the vocabulary V by computing

ˆyt = Softmax(W Ht),

(2.32)

Similar to the Transformer-decoder, we use the same cross-entropy loss function as in Equa-

tion 2.16 to train the model. This model is usually referred to the Language Model (LM) Trans-

former, since it is trained in the same way.

The most effective causal-decoder, pre-trained on a large text corpus are GPT-2 [5] and GPT-

3 [106], while in the dialogue system scenario, DialoGPT [1] is pre-trained using a large number

of unlabeled conversations from Reddit, (more in details in Chapter 3). Independently of the

pre-training corpus, these pre-trained models are ﬁne-tuned to speciﬁc generation tasks (e.g.,

21

Figure 2.8: Example of encoder-decoder Transformers. This model is equivalent to any
Seq2Seq architecture.

dialogue response generation) by using the same Seq2Seq loss as deﬁned in Equation 2.16.

This casual decoder model is shown in Figure 2.7, and it is widely used in Chapter 3 and 4 of

the thesis.

2.7.2 Encoder-Decoder

Pre-trained encoder-decoder models are Transformers, as described in Section 2.6, trained on

a massive amount of unlabeled data (plain text or conversations). Differently from the casual-

decoder, two pre-training strategies have been proposed: span-prediction [103] and denoising

pre-training [104]. In both strategies, the input sequence is corrupted and the model is taught to

reconstruct the original sequence.

Several pre-trained encoder-decoder conversational models [2, 3] have been shown to be very

effective in generating human-like responses. However, these models are still very hard to

control.

2.8 Related Work

Task-oriented Dialogue Task-oriented dialogue models [107] can be categorized in two types:

module-based [20–24, 108] and end-to-end. In this paper, we focus on the latter which are sys-

tems that train a single model directly on text transcripts of dialogues. These tasks are tackled

by selecting a set of predeﬁned utterances [62, 66, 69, 70] or by generating a sequence of to-

kens [71–74]. Especially in the latter, copy-augmented models [57, 60, 61] are very effective

since extracting entities from a knowledge base is fundamental.

22

Chit-Chat Dialogue Generating human-like responses involves overcoming a variety of chal-

lenges such as personalization [19, 109–112], knowledge grounding [113–118], emotions [119–

123], diversity [74, 107, 124–126] and, bias [127–129] so on. In terms of controlled dialogue

generation, See et al. [76] studied of conditional generative models [130] and weighted de-

coding [131] in controlling models trained on persona-chat. See et al. [76] concluded that

controlling speciﬁcity, relatedness, and repetition increase human-engagement, motivating us

to extend the controllabitly to styles and topics. In this paper, we focus on these two since large

pre-trained models can already achieve a high humanness score [1–3].

Controlled Text Generation Recent methods for controlled generation include ﬁne-tuning

models using supervised learning [132, 133], reinforcement learning [134], adversarial train-

ing [135], by pre-training models with control codes [136–138], and other various approaches [139–

141]. Alternatively, weight decoding using both bag-of-words [76, 131, 142, 143] and discrim-

inators [142, 144], does not require any ﬁne-tuning. Similarly, Dathathri et al. [145] propose

the Plug-and-Play Language Model (PPLM) to control the generation of a pre-trained language

model, e.g., GPT2 [5], both in terms of style and topic of the generated text. Finally, residual

adapters [77] has been used to learn multiple language generation tasks [146] without ﬁne-

tuning the original models’ parameters. Concurrently to our work, Smith et al. [147] compare

the performance and trade-offs of three existing controllable language generation methods on

200 possible styles.

Continual Learning in NLP Continual learning has been explored for both classiﬁcation

[148–151] and generation [152, 153] tasks. For instance, Sun et al. [152], Chuang et al. [154]

proposed LAMOL, which we use as our baseline, and studied its effectiveness on a subset of

DecaNLP [155]. On the other hand, the work of de Masson d’Autume et al. [148], Sprechmann

et al. [149] is not suitable for interactive systems as dialogue systems, since their methods

require local adaptation (i.e., a ﬁne-tuning step) during inference. Finally, continual learning

has been used for sentence encoding [156], composition language learning [157] and relation

learning [158, 159]. However, these methods are very speciﬁc to particular NLP applications.

Continual Learning in Dialogue Systems The very early work on CL for Task-Oriented

dialogue is from Lee [160], who used EWC to avoid catastrophic forgetting on three domains

learned sequentially. Continual learning has also been studied in the NLG setting, where a single

23

model was trained to learn one domain at the time in MWoZ [161]. The authors used episodic

memory to replay the example in combination with EWC. We compare similar baselines but

on a larger benchmark that also includes MWoZ and the NLG setting. For the DST setting,

CL was studied by [108] using MWoZ, where several baselines such as L2, EWC and GEM

were compared. Differently, Li et al. [162] leveraged CL for evaluating the quality of chat-bot

models, and He et al. [163] studied the catastrophic forgetting problem in chit-chat systems.

Finally, Shuster et al. [164] showed that by training models on humans-machine conversations

in an open-domain fantasy world game [165] the models progressively improved, as measured

by automatic metrics and online engagement scores.

Mixture of Expert & Conditional Computation The idea of having specialized parame-

ters, or so-called experts, has been widely studied topics in the last two decades [6, 166].

For instance, different architecture and methodologies have been used such as Gaussian Pro-

cesses [167], Hierarchical Experts [168], and sequential expert addition [169]. More recently,

the Mixture Of Expert [170, 171] model was proposed which added a large number of experts

between two LSTMs. To the best of our knowledge, none of these previous works applied the

results of the gating function to the parameters itself. On the other hand, there are Conditional

Computational models which learn to dynamically select their computation graph [172, 173].

Several methods have been used such as reinforcement learning [174], a halting function [175–

177], by pruning [178, 179] and routing/controller function [180]. However, this line of work

focuses more on optimizing the inference performance of the model more than specializing

parts of it for computing a certain task.

24

Chapter 3

Controlling Style and Topics

In the context of conversational models, See et al. [76] showed that being able to control the

response generation can have a signiﬁcant impact on the quality of conversations. However,

controlled generation from large conversational models remains a challenge, and is particularly

difﬁcult in the absence of annotated conversational datasets.

For large language models, controlled generation has recently received increased attention. In

CTRL [136], the language model is trained to generate text conditioned to control codes pre-

sented to the model at the start of the context, while in Ziegler et al. [134], GPT-2 [5] is ﬁne-

tuned using reinforcement-learning with human annotators in the loop to generate continuing

text with positive sentiment. Both of these approaches require learning/ﬁne-tuning all of the

models’ parameters, and new desired attributes cannot be easily incorporated into the genera-

tion once the models have been trained.

Other approaches that do not alter the language model, but modify the decoding procedure for

controlled generation include 1) re-weighting the output distribution using discriminators [142]

or bag of words [76, 131, 143] and 2) perturbing the model activation with an attribute model, as

in Plug-and-Play Language Model (PPLM) [145]. These approaches are plug-and-play methods

in that they can be used on top of any existing pre-trained language model. They do not modify

or train the parameters of the original models and they can achieve comparable performance to

ﬁne-tuning methods. However, weighted decoding is generally difﬁcult to tune because it can

easily generate unrelated responses when the weight is not properly set [76] and, PPLM incurs

a high computational cost during the decoding stage, which is problematic for online systems

such as dialogue systems.

25

Open-domain conversational systems are a

special case of language models, where the

HUMAN 1

Hmmmm. I really want to try bull riding.

Do you have any interest in that?

preﬁx is the dialogue history and the continu-

HUMAN 2

I’d love to try! Can we schedule something

ation is a human-like response [19]. Recently,

for next week?

DGPT Yes! Let’s get some stuff going.

large pre-trained language models trained

I’m actually going to try my ﬁrst ever bull

on unlabeled human-to-human conversation

NEGATIVE

riding weekend in July, in the middle of

(e.g. Reddit) [1–3] have shown excellent

performance in modelling human responses.

a mud sludge of mudslide.

POSITIVE

That would be wonderful! I’m excited to

see you guys again in a couple of weeks :D

Similarly, the output of large pre-trained con-

QUESTION

What’s it like? Do you ride on a regular

versational models cannot be directly con-

trolled without having to re-train/ﬁne-tune the

model from scratch, which is practically in-

convenient and sometimes impossible since

few or no-conversational datasets exist for

BUSINESS

SCIENCE

& TECH

basis or just for fun?

Yea sure i can wait until after work. I am off

at the end of my shift in the middle east.

Sure thing! I’ll PM you my email. We can

set up a dropbox or something and just

message people together if you’d like.

Ok how about Wednesday at 7pm central?

certain attributes or styles.

SPORTS

I’ll be there with a buddy of mine

On the other hand, plug-and-play methods

HUMAN 1 Sure! What does your Saturday look like?

who is a bull rider.

are a viable solution since they do not re-

quire dialogue-speciﬁc datasets, and they can

be computed online on top of existing pre-

trained models. A major drawback, however,

is the high computational cost [145] at de-

coding time. This is acceptable for language

Figure 3.1: Example of controllable response

generation. DGPT is the DialoGPT [1] re-

sponse, while the others are generated using dif-

ferent attribute models.

models where generating paragraphs or stories can be done ofﬂine but it is problematic for

online systems, such as conversational models.

Therefore, we explore PPLM in large pre-trained dialogue models for controlling the style

and topic of the responses without ﬁne-tuning on any dialogue speciﬁc dataset. Moreover,

to cope with the computational cost at the decoding time, we propose to generate style/topic-

consistent responses with PPLM [145] and then use it to optimize residual adapters [77] for

directly learning how to steer the original distribution towards the selected attribute.

With our extensive automatic and human evaluation, we empirically demonstrate that plug-and-

play methods are effective in controlling the response while being computationally efﬁcient. To

26

Figure 3.2: High-level description of the proposed method. Firstly, we train the attribute model
for PPLM. Then we use PPLM to generate stylistic responses (e.g., positive or negative) from
1000 dialogue histories. Finally, we distill the generated response into residual adapters, one
per attribute.

summarize, our key contributions are:

• We show the effectiveness of plug-and-play methods in large pre-trained conversational mod-

els using a variety of styles and topics such as Positive, Negative, Question, Sport, Busi-

ness/Finance, without using a dialogue-speciﬁc dataset.

• We propose to use residual adapters [77], which adds less than 1.5% task-speciﬁc parameters

per style/topic, to make the controllable response generation viable for online systems.

• We run a comprehensive automatic and human evaluation to show that plug-and-play methods

can control the generated responses in terms of style and topics, without losing ﬂuency.

• We carry out a thorough qualitative analysis on the difﬁculty of steering conversational mod-

els, highlighting current limitations and possible solutions.

The high-level description of the proposed methodology is shown in Figure 3.2.

3.1 Methodology

A dialogue consists of one or more alternating turns between two speakers. We denote the

dialogue history ˆX as a single sequence of tokens from the concatenation of the alternating ut-

terances from the user and the system turns respectively. Without loss of generality, we assume

that ˆX has all the dialogue history without the last system utterance, denoted as Y . We model the

dialogue responses using a Transformer [97]-based casual-decoder (LM) by using the dialogue

history ˆX as a preﬁx and then generating the continuation Y in an auto-regressive manner [181].

27

Figure 3.3: Example of PPLM applied to a large pre-trained dialogue model (DGPT [1]).

Causal-Decoder LM Let us re-deﬁne the concatenation of ˆX = {x0, . . . , xn} and output Y =

{xn+1, . . . , xm} as the sequence of tokens X = {x0, . . . , xn+m}. Then we can compute the lan-

guage model distribution using the chain rule of probability [84] as

pθ ([ ˆX;Y ]) = pθ (X) =

n+m
∏
i=0

pθ (xi|x0, · · · , xi−1),

(3.1)

where θ is the model’s parameters and [; ] denotes the concatenation of X and Y . We deﬁne the

Transformer decoding process in a recursive manner. Let us deﬁne the matrix Ft as the key-value
,V (i)
pairs from the past dialogue history, i.e., Ft = [(K(1)
t
corresponds to the key-value pairs from the i-th layer generated at all time-steps from 0 to t.

)], where (K(i)

), · · · , (K(l)

,V (1)
t

,V (l)
t

)

t

t

t

Thus, we deﬁne the recurrent decoding process as:

ot+1, Ft+1 = LM(xt, Ft),

(3.2)

and then xt+1 is sampled from the distribution pt+1 = Softmax(Wot+1), where W is a linear

transformation that maps the hidden state of the last layer ot+1 to a vector of the vocabulary size.

This efﬁcient Transformer implementation [182] leverages the cached memories to generate

xt+1 without recomputing Ft.

3.1.1 Plug-and-Play Language Model

PPLM [145] uses an attribute model (i.e., a classiﬁer) for controlling the generated text. We

denote the attribute model as p(a|X), where a is the speciﬁc desired attribute to optimize for

(e.g., positivity), and X is the generated response so far. At every generation step t, PPLM

perturbs the history matrix Ft in the direction of the sum of two gradients: i) to maximize the log-

likelihood of the attribute a under the conditional attribute model p(a|X) and ii) to ensure the

28

high log-likelihood of the generated text under the unmodiﬁed conversational language model

p(X). The gradient updates are restricted to Ft so as to preserve the original model parameters.

Let ∆Ft be the update to Ft to shift the generated text towards possessing the desired attribute a

i.e., ot+1, Ft+1 = LM(xt, Ft + ∆Ft). At the beginning of the generation, ∆Ft is initialized to zero

and it is updated using the gradients from the attribute model. We rewrite the attribute model

p(a|X) as p(a|Ft + ∆Ft), and we deﬁne the gradient update for ∆Ft as

∆Ft ← ∆Ft + α

∇∆Ft log p(a|Ft + ∆Ft)
(cid:107)∇∆Ft log p(a|Ft + ∆Ft)(cid:107)γ

,

(3.3)

where α is the step size and γ is the scaling coefﬁcient for the normalization term. Equation 3.3

is repeated p times depending on how strongly we want the response to be conditioned to the

attribute. We study the effect of the step size α and the number of iterations p on the generated

text in detail in Section 3.4. Subsequently, the new (cid:101)Ft = Ft + ∆Ft is computed and a new token
is generated using (cid:101)ot+1, Ft+1 = LM(st, (cid:101)Ft). The described optimization process is repeated for
every token in the generated sequence. As previously mentioned, to ensure ﬂuency we also take

a step towards minimizing the Kullback–Leibler (KL) regularization between the perturbed and

the original distribution. In addition, we also use Post-norm Geometric Fusion [145, 183] to

avoid adversarial generation [184].

Attribute Models The discriminator is a linear classiﬁer g trained on an annotated dataset

with sentence and label pairs as (X, y) — note that these sentences do not necessarily need to

set of hidden states ox

be conversational responses, as in our case. For each sentence x of length t, we compute the
:t from the LM, then we compute the mean ( ¯ot) across time, and ﬁnally
we train g using the cross-entropy between the label distribution y and g( ¯ot). Figure 3.3 shows

an example of how PPLM uses different discriminators to control the output generation of

DialoGPT (DGPT).

3.1.2 Residual Adapters

Residual adapters [77, 185] are trainable modules added on top of each Transformer layer,

which steer the output distribution of a pre-trained model without modifying the original weights.

An adapter block consists of layer normalization [186], followed by two linear layers [187] with
a residual connection. Given the hidden representation at layer l, denoted as H ∈ Rt×d, of a

Transformer [97], where d is the hidden size and t is the sequence length, the residual adapter

29

Figure 3.4: Plug-and-play adapter architecture. The Transformer layers, head, and embedding
layer are frozen during training, while the adapter layers are trained independently per each
attribute.

computes

Adapterµ l (H) = ReLU(LN(H)W E

l )W D

l + H,

(3.4)

where W E

l and W D

l are trainable parameters of dimensions d × b and b × d respectively, and
LN(·) denotes the layer normalization. The bottleneck dimension b is a tunable hyper-parameter

that allows adjustment of the capacity of the adapter according to the complexity of the target

task. We deﬁne the set of µ = {W E

0 , W D

0 , · · · ,W E

L , W D

L } as the set of parameters for the adapter

for a model with L layers.

Plug-and-Play Adapters At decoding time, PPLM uses a ﬁxed number of iterations p to

generate a single token. This makes the model impracticable for interactive tasks such as con-

versational models. To cope with this issue, we propose to ﬁrst use PPLM to generate datasets
of dialogues with certain attributes a, denoted as Da = {(Xi,Yi)}r

i , where Xi is the dialogue
history and Yi the corresponding PPLM, generated response. Then we optimize the residual

adapter parameters to steer the output of the original LM distribution. Hence, for each attribute

a, we optimize the parameters in µa to minimize the negative log-likelihood over the dataset of

30

dialogues Da. Formally,

Lµa(Da) = −

|Da|
∑
j

n+m
∑
i=0

log pµa(x j

i |x j

0, · · · x j

i−1),

(3.5)

where each response n + m is of maximum length in Da. Figure 3.4 shows the ﬁnal Transformer

with one adapter per attribute.

3.2 Experimental Setup

In this section, we conduct extensive experiments on the proposed methodology using both

automatic and human evaluation. Differently from PPLM [145], where a set of pre-deﬁned

preﬁxes are used to trigger the generation, in our experiments, we use 100 conversations [2]

for generating 1100 possible preﬁxes (i.e., moving window of size two). These open-domain

generic dialogues serve as a preﬁx to trigger the responses rather than ﬁne-tuning. In all our

experiments, we use DialoGPT-medium [1], a large pre-trained model trained on 147 Million

multi-turn dialogues from Reddit, spanning from 2005 to 2017.

Importantly, the proposed

methodology is model agnostic, and thus it can be applied to any other large pre-trained model

such as Meena [2] and Blender-Bot [3].

Since the plug-and-play adapters use the generated responses from PPLM, we randomly split

the preﬁxes, with 80% for learning the adapter perturbation and the remaining 20% for the ﬁnal

automatic and human evaluation. This is done to have a fair comparison between other baselines

and adapters (See Appedix A for more details).

3.2.1 Attribute Models

We train three discriminators covering six attribute models: Positive, Negative, Question, Sci/Tech,

Business and Sport. To control the Positive and Negative responses, we use SST-5 [188] with

the classes Very-Positive and Very-Negative as the attribute, and to control for Question, we use

the speech-act annotation from Daily Dialogue [119] with the Question class as the attribute.

To avoid any dialogue-related data, we only use sentences without their corresponding context.

Finally, to generate responses about Sci/Tech, Business and Sport, we use the AG-NEWS [189]

topic-classiﬁcation dataset, using the respective classes as attributes. As mentioned in Sec-

tion 3.1.1, we freeze the DialoGPT parameters and train a linear classiﬁer on top of the rep-

resentations from the ﬁnal layer of its Transformer blocks. Table 3.1, shows the sample size

31

Dataset

Task

#C

SST-5 [188] Sentiment

Daily Dialogue [119]
AG NEWS [189]

Act
Topic

5
4
4

Samples

Train
318,582
92,650
120,000

Test
2210
10,295
7,600

F1-Score
Test
47.01
80.00
90.65

SOTA
55.50†
86.10‡
95.44§

Train
77.68
80.58
90.68

Table 3.1: Attribute dataset statistics and performance. State-of-the-art (SOTA) results are taken
from † [7], ‡ [8], and § [9].

statistics and the performance in terms of F1-score for all the aforementioned datasets. We also

report the current state-of-the-art to show that a linear classiﬁer trained on top of the DialoGPT

activation can reach competitive performance.

3.2.2 Baselines

We compare multiple plug-and-play settings: DG: DialoGPT, proposed by Zhang et al. [1];

WD: DialoGPT plus a word-level weight-decoding schema, as in [131] and [76]; PP: DialoGPT

plus PPLM [145], as explained in Section 3.1.1; and AD: DialoGPT with one adapter per style,

as explained in Section 3.1.2. In all the baselines, we sample 10 hypotheses using multinomial-

sampling after a top-k ﬁltering (with k = 10), to ensure response diversity [190], and we select

the hypotheses with the lowest attribute model loss as the response. This re-ranking technique

has shown itself to be very effective for generating good responses [2, 145].

3.2.3 Evaluation Metrics

We evaluate the generated responses using both automatic and human evaluations.

Automatic Eval.

in open-domain chat is challenging [191], especially when using n-gram

methods over a single reference (e.g., BLEU [192]). In this paper, no gold-reference response is

provided (e.g., stylistic human-generated response). Thus we rely on unsupervised measures for

ﬂuency, diversity and style/topic. For ﬂuency, we compute the perplexity score of the dialogue

preﬁx plus the generated response using GPT2 [5]. While for diversity, we use the distinct

n-grams [124] (normalized by the length of the text) across all the responses generated by a

given method. To evaluate the attribute consistency, we train external classiﬁers using non-

overlapping data with the attribute model. For sentiments, we use AMAZON-5 [193] product

reviews, and for topics, we use the test-set data of AG-NEWS [189] because we could not

ﬁnd another topic classiﬁcation dataset with the same classes. For each dataset, we train a

32

↓ Ppl.
DG 39.60
WD 53.03
PP 45.86
AD 41.57

↑ Dist 1/2/3
0.22/0.64/0.77
0.25/0.74/0.84
0.24/0.67/0.79
0.17/0.58/0.77

Discrim. Score Posi. Nega. Busin. Sci/Tech Sport
27.86
19.40
36.82
28.86
59.20
51.74
83.08
73.13

32.91
34.54
49.54
70.01

46.48
50.18
73.28
96.52

65.67
58.21
75.12
93.03

17.41
19.40
47.26
68.66

91.04
91.04
93.03
99.00

Score by Attribute

Table 3.2: Automatic evaluation results. In all the metrics, higher is better, except for Perplexity
(Ppl.). Discrim. is the accuracy of the internal attribute model, while Score is the accuracy of
the external classiﬁer. All the results, are averaged among the six attribute models.

(a) Humanness

(b) Attribute consistency

Figure 3.5: Human evaluation results in terms of winning rate for both humanness and attribute
consistency. For example, in the Attribute Consistency table, DG wins 66% of the time versus
HM. Bold results are statistically signiﬁcant (p < 0.05).

separate BERT [194] (base) classiﬁer with a simple classiﬁcation head. Table 2 in Appendix B,

summarizes the dataset statistics and the performance of the trained scorer.

Human Eval.

is the most effective way to evaluate open-domain chat-bots. In this chapter,

we evaluate two aspects of the generated response: humanness and attribute consistency. The

ﬁrst is used to evaluate the ﬂuency and coherence of the generated responses. The second

is used, to evaluate whether the generated responses respect the style or the topic enforced

by the attribute model. We use Acute-Eval [195]-style A/B testing, in which we compare all

possible model pairings (e.g., PP vs. DG etc.). For each comparison, we show the same dialogue

context and two possible options, one generated from model A and one from model B. Then

we ask the annotators to select among four options: model A, model B, both or neither. We

collect annotations for both humanness and attribute consistency on 30 dialogues per model

comparison and attribute, which amounts to a total of 4200 human annotations. Further details

are provided in Appendix C.

33

(a) Negative

(b) Positive

Figure 3.6: Contour plot of the normalized sum of the log perplexity score, computed by
GPT2 [5] and the external classiﬁer loss on the response generated by PPLM for the Nega-
tive and Positive style. The x-axis is the number of iteration, p, and the y-axis is the step size α.
Darker areas correspond to a higher loss sum, meaning a higher perplexity and higher classiﬁ-
cation loss. The labels represent a sample response from a given iteration and step size.

3.3 Results

In this section, we evaluate the proposed methodology to answer three research questions: 1)

is it possible to use plug-and-play methods to control the output of a large pre-trained conver-

sational model? and if so, 2) what are the most effective plug-and-play methods, and 3) how

difﬁcult is to control the response generation given various attributes? To answer these ques-

tions, we rely on both automatic and human evaluation. Table 3.2 and Figure 3.5 reports the

aggregated result for all the styles and topics in both evaluations. The breakdown per attribute

is reported in Appendix D.

3.3.1 Quantitative Evaluation

Automatic Eval. The major evaluation criterion is to have responses that are as ﬂuent as the

original DialoGPT or humans while following the style or topic enforced by the attribute model.

In Table 3.2, we can see that DialoGPT (DG) achieves the lowest perplexity, but it also has the

lowest aggregate attribute score ("Score" in Table 3.2). By analysing the breakdown by style,

we can see that, by default, the original model has a higher score in both Positive style and

Sci/Tech topic. We hypothesize that this is due to two factors: 1) The discussions in Reddit

are more often related to Sci/Tech topics. By being provided general questions as input, e.g.,

“What do you do for living?", the model often generates tech-related responses, e.g., “I am a

34

HUMAN 1 Are you doing any home decorating then?
HUMAN 2 Yes! We set up an eco-friendly (i.e. fake) Christmas tree and put up some colorful LED lights which is very festive.
NEGATIVE
That sounds awesome. I’m going to get a couple of these
and start decorating.
I’m not sure if you’re aware of this, but the word
ecoreneive actually refers to being ecoreneous...
I’m not a fan of LEDs in general. They always seem to fail. Oh wow awesome! Thank you so much for your time!

Oh that’s so cool! I love your stuff!!

Very nice, good sir

POSITIVE

DG

WD

PP
AD That sounds like the absolute most boring thing. EVER.

That is amazing! I am so excited!! :D So creative and creative!! :D

Table 3.3: Examples of generated responses for Negative and Positive with the same starter.

computer science student". 2) Zhang et al. [1] ﬁltered undesired and toxic responses from the

Reddit conversations used in training DialoGPT, which explains the positivity of its responses.

Using weight decoding (WD) on top of DialoGPT leads to an improvement in both the diver-

sity score and the external classiﬁer score. However, WD tends to increase the perplexity score,

showing that the generation ﬂuency with respect to the context is lost. In the preliminary exper-

iments, we notice that WD generates responses that are not related to the dialogue context but

are highly similar to the distribution of the discriminator datasets. This is consistent with the

observation in [76] that WD is difﬁcult to tune and often provides control at the cost of ﬂuency,

leading to non-sensical generation. On the other hand, PPLM (PP) is able to achieve a lower

perplexity compared to WD while attaining both a higher attribute consistency score and a high

response diversity (dist). We hypothesize that better performance is due the ability of PPLM to

dynamically perturb the latent activation of the model without breaking the original distribution

thanks to the KL regularization and the Post-norm Geometric Fusion [183].

The adapter plug-and-play setting has the highest overall attribute score and the lowest perplex-

ity between PP and WD. However, the response diversity, especially dist-1, is lower than for

other baselines, meaning that the response may contain repetitive tokens (e.g., “so so bad").

In general, adapters optimized with PPLM-generated responses, which usually are not perfect,

can properly learn to steer the output distribution without breaking the original DialoGPT out-

put. As previously mentioned, this also comes with the advantage of not computing the PPLM

perturbation at decoding time.

Human Eval. In Figure 3.5, we report the winning rate of the A/B testing for both humanness

and attribute consistency. From the ﬁgure, we highlight the following: 1) There is no statisti-

cally signiﬁcant difference in the humanness score among the multiple methods, even with 210

annotations per cell. In general, all the methods lose with the human response (HM), but not by

a large margin. This is due to the fact that the annotators choose the “both" option more often. 2)

35

In terms of attribute consistency, we observe that the methods form a clean, well-ordered rank,

AD>PP>WD>DG>HM, which conﬁrms the automatic evaluation results. Different from

humanness, all the results except those for WD vs. DG are statistically signiﬁcant (p < 0.05),

showing the adapter clearly defeats other methods.

To answer the ﬁrst two research questions, we observe that both automatic and human evaluation

show that plug-and-play methods are suitable for controling response generation. Moreover,

the most effective method is the adapter plug-and-play, which produces ﬂuent, and attribute

consistent responses, while being three orders of magnitude faster than PPLM at inference time

(148.5s/token vs. 0.123s/token) using a single Nvidia 1080Ti.

3.4 Analysis

In this section, we evaluate the difﬁculty of controlling the response generation for a given

attribute. To do so, we analyse the behaviour of PPLM over two opposite styles (positive and

negative) and then we conduct a qualitative evaluation over the generated responses.

Iteration & Step Size We analyse the loss of the automatic scorer for ﬂuency and attribute

consistency to understand the effects of the number of iterations p and the step size α in

Equation 3.3. Figure 3.6 depicts the normalized sum of the log perplexity score, computed

by GPT2 [5] and the external classiﬁer loss on the response generated for the Negative and Pos-

itive style. In general, the aggregate loss for the negative attribute (Figure 3.6a) is higher for the

Positive attribute (Figure 3.6b), as also shown in the sampled responses, where a small step size

and few iterations leads to positive responses. However, when both the step size and the itera-

tion surpass a certain threshold, the conditioning becomes very strong and the text generated by

PPLM loses its ﬂuency. Overall, this visualization suggests that it is more laborious to control

for the negative sentiment with PPLM, and there is a smaller region for the hyper-parameters

space where the responses are both ﬂuent and attribute-consistent.

Qualitative Analysis We sample and read 200 dialogue responses from the adapter plug-and-

play model (AD), and we study the overall quality of the responses especially to understand

when and why DialoGPT is hard to steer. We discover three possible factors: 1) the hardness

of the response steering is inﬂuenced by the context, 2) available vocabulary for attributed

style/topic, and 3) mutual exclusivity of the attribute-speciﬁc vocabulary.

36

1) Unlike language models that use short preﬁxes (e.g., “The issues ...") to trigger the generation,

conversational models are constrained to the given dialogue history, which signiﬁcantly inﬂu-

ences the controllability. Given an open ended dialogue context (e.g., Table 11 in appendix),

AD generates an impressively natural and on-topic response, but when provided a more con-

strained dialogue context (e.g., Table 17 in appendix), AD generates a response that may sound

sudden and out of context.

2) Looking at the overall responses, also shown in Table 3.3, we observe that the models use a

restricted vocabulary for generating attribute-consistent responses. For example, AD frequently

generates sentences containing “horrible", “terrible" or “worst" for negative, while “beautiful",

“happy" or “wonderful" are more common for positive.

3) The importance of mutual exclusivity of the attribute-speciﬁc vocabulary also explains the

relatively poor performance when controlling for certain topics. As noted above, positive and

negative vocabularies are clearly distinguishable. However, the attribute-speciﬁc words for top-

ics such as Business are more generic (e.g., “car", “store") than those for other topics such

as Sport (e.g., “football", “hockey") or Sci/Tech (e.g., “android", “software"). If the attribute-

speciﬁc words are common and shared across multiple domains, the generated responses may

not sound attribute speciﬁc even though the correct vocabulary is used.

Note that this use of restricted vocabulary also harms ﬂuency, because the vocabulary cannot

always ﬁt within a given context. Additional generated examples and statistics of attribute-

speciﬁc vocabulary on each style/topic are provided in Appendix D.

3.5 Short Summary

We explore plug-and-play methods for controlling the response generation of large pre-trained

conversational models. With extensive automatic and human evaluations, we show that PPLM

is able to generate ﬂuent and attribute-consistent responses. Further, to overcome the signiﬁ-

cant computational overhead introduced by PPLM at decoding time, we optimize a tiny residual

adapter for each attribute based on a few synthetic responses generated using PPLM. The re-

sulting model does not require further computation at decoding time, and outperforms PPLM

both in terms of ﬂuency and attribute consistency.

37

Chapter 4

Controlling Dialogue Domains

Continuously

Task-oriented dialogue systems (TODs) are the core technology of the current state-of-the-

art smart assistants (e.g., Alexa, Siri, Portal, etc.). These systems are either modularized as a

pipeline of multiple components, namely, natural language understanding (NLU), dialogue state

tracking (DST), dialogue policy (DP) and natural language generation (NLG), or end-to-end,

where a single model implicitly learns how to issue APIs and system responses (NLG).

These systems are continuously updated with new features based on the user’s needs, e.g.,

adding new slots and intents, or even completely new domains. However, existing dialogue

models are trained with the assumption of having a ﬁxed dataset at the beginning of the train-

ing, and they are not designed to add new domains and functionalities through time without

incurring the high cost of retraining the whole system. The ability to acquire new knowledge

continuously, a.k.a. continual learning (CL) [196], represents a common challenge to many

production and on-device dialogue systems where there is a continual growth of 1st and 3rd-

party-developer domains that are added after deployment. Therefore, it is crucial to design

dialogue systems with CL ability. Figure 4.1 shows an high-level intuition of CL in TODs.

The main challenge in continual learning is catastrophic forgetting [197]. This phenomenon

happens because there is a distributional shift between the tasks in the curriculum, which leads

to catastrophic forgetting of the previously acquired knowledge. To overcome this challenge

three kinds of methods are usually deployed: loss regularization, which avoid interference with

the previously learned tasks; rehearsal, which use episodic memory to recall previously learned

38

Figure 4.1: In continual learning, the model is trained one dataset at time. In this instance, the
model is ﬁrst trained on data from the hotel domain DHotel then on the Taxi domain DTaxi and
so on. The parameters of the model are updated sequentially based on the loss function L.

tasks; and architectural, which add task-speciﬁc parameters for each learned task. However, ar-

chitectural methods are usually not considered as a baseline, especially in sequence-to-sequence

(Seq2Seq) generation tasks [152], because they usually require a further step during testing to

select which parameter to use for the given task.

To the best of our knowledge, continual learning in TODs [160] is mostly unexplored or has

been studied only in speciﬁc settings (e.g., NLG [161]) using only a few tasks learned contin-

uously. Given the importance of the task in the dialogue setting, we believe that a more com-

prehensive investigation is required, especially by comparing multiple settings and baselines.

Therefore, we make the following contributions:

1. We propose a benchmark for continual learning in TODs, with 37 tasks to be learned

continuously on four settings.

2. We propose a simple yet effective architectural CL method based on residual adapters [77]

that can continuously learn tasks without the need of a task classiﬁer at testing time.

3. We analyse the trade-off between number-of-parameters, episodic memory sizes, and

training time of the three main categories of CL methods (regularization, rehearsal, archi-

tectural).

4.1 Background

4.1.1 Task-Oriented Dialogue Modelling

We model TODs as a Seq2Seq generation task [198, 199] that generates both API-calls and

system responses. As shown in Figure 4.2, the model takes as input a dialogue history to

generate an API-call, which is the concatenation of user intents and current dialogue states,

39

Figure 4.2: Example of input-out pairs, for the four settings, INTENT, DST, NLG and end-to-
end (E2E).

and then uses its API-call returns, which can be empty or system speech-acts, to generate its

system response. This modelling choice is guided by the existing annotated dialogue datasets,

which provide the intent and the dialogue state of the user at every turn, and the speech-act of

the system; and it allows us to deﬁne four distinct settings for studying CL: intent recognition

(INTENT), DST, NLG and end-to-end (E2E). In the coming paragraphs, we formally describe

the four settings as different input-out pairs for a Seq2Seq model.

Data-Formatting Let us deﬁne the dialogue history U as a single sequence of tokens from

the concatenation of the alternating utterances from the user and the system turns respectively.

Without loss of generality, we assume that U has all the dialogue history without the last system

utterance, denoted as S. To distinguish between speakers, we add two special tokens at the

beginning of every utterance: "USER:" for the user utterance, and "SYSTEM:" for the system

utterance. Then, we deﬁne an API-call, denoted by SAPI, as the concatenation of the API-name,

i.e., the user-intent, and its arguments, i.e., slot-value pairs from the DST. The following syntax

is used:

SAPI = I

(cid:124)(cid:123)(cid:122)(cid:125)
Intent

(s1 = v1, . . . , sk = vp
(cid:124)
(cid:125)
(cid:123)(cid:122)
Slot-value pairs

),

(4.1)

where I is an intent or the API-name, si the slot-name and vi one of the possible values for the

slot si. The return of the API-call is either an empty string, thus the model uses the dialogue

history to generate a response, or a speech-act, denoted as SOUT , in the same format as the

API-call in Equation 4.1. Similar to the dialogue history, we deﬁne two special tokens "API:"

and "OUT:" for triggering the model to generate the API-call and for distinguishing the return

of the API from the dialogue history respectively. Based on this pre-processing, we deﬁne the

four settings.

40

Without loss of generality, we deﬁne the three modularized settings by their input-out pairs:

U → I

(INTENT)

U → I(s1 = v1, . . . , sk = vp)

(DST)

I(s1 = v1, . . . , sk = vp)
(cid:125)
(cid:123)(cid:122)
(cid:124)
SOUT

→ S

(NLG),

whereas for the E2E setting we deﬁne the pairs as:

U → I(s1 = v1, . . . , sk = vp)
(cid:125)

(cid:124)

(cid:123)(cid:122)
SAPI

U + I(s1 = v1, . . . , sk = vp)
(cid:125)

(cid:124)

(cid:123)(cid:122)
SOUT

→ S,

Often, SOUT is empty and thus the model maps the dialogue history to the response (U → S), as

we have seen in the previous chapter. An example of input-out pairs is shown in Figure 4.2.

Finally, we deﬁne a dialogue dataset as DK = {(Xi,Yi)}r

i , where (Xi,Yi) is a general input-
out pair from one of the four settings under consideration, and K the dialogue domain under

consideration (e.g., hotel).

Model We employ casual language models (e.g., GPT-2), which are often used in the current

state-of-the-art task-oriented dialogue models, as in Peng et al. [200] and [201]. Then, given

the concatenation of the input X = {x0, . . . , xn} and output Y = {xn+1, . . . , xm} sequences, we

compute the conditional language model distribution using the chain rule of probability [84] as

pθ ([X;Y ]) =

n+m
∏
i=0

pθ (xi|x0, · · · , xi−1),

(4.2)

where θ is the model’s parameters and [; ] denotes the concatenation of X and Y . The parameters

are trained to minimize the negative log-likelihood over a dataset D of input-out pairs, which in
our case is the data of the four settings. Formally, we deﬁne the loss Lθ as:

Lθ (D) = −

|D|
∑
j

n+m
∑
i=0

log pθ (x( j)
i

0 , · · · , x( j)
|x( j)

i−1),

(4.3)

where n + m is a maximum sequence length in D. At inference time, given the input sequence

X, the model parameterized by θ autoregressively generates the output sequence Y .

41

4.1.2 Continual Learning

The goal of continual learning is to learn a set of tasks sequentially without catastrophically

forgetting the previously learned tasks. In TODs, we cast CL as learning a sequence of domains
sequentially. Let us deﬁne a curriculum of T domains as an ordered set D = {D1, · · · , DT },

where Dk is a dataset under the domain K. In addition, we denote the models’ parameters after

learning the task K by θK.

Following the recently deﬁned taxonomy for CL [202], we study the settings in which the

task-ID, is provided during training, but not during testing, 1 meaning that, during training,

the model is aware of which domain it is currently learning, but during testing, the model is

evaluated without specifying the dialogue domain. This assumption makes our CL setting more

challenging but more realistic, since during inference times, users do not explicitly specify in

which domain they want to operate.

We consider three CL approaches: regularization, rehearsal and architectural. In our experi-

ments, we describe the most commonly used methods within each approach, especially those

known to work well in language tasks.

• Regularization methods add a regularization term to the current learned θt to avoid inter-

fering with the previously learned θt−1. Formally, the loss at task t is:

Lθt (Dt) = Lθt (Dt) + λ Ω(θt − θ ∗

t−1)2,

(4.4)

where θ ∗

t−1 is copies of the previously learned parameters frozen at this stage. In our
experiments, we consider two kinds of Ω: the identity function (L2) and the Fisher infor-

mation matrix [203] (EWC).

• Rehearsal methods use an episodic memory M to store examples from the previously

learned domains, and re-use them while learning new tasks. The most straightforward

method is to add the content of the memory M to the current task data Dt [204]. Following

our notation, the model is optimized using Lθt (Dt + M), and we refer to this method as

REPLAY. Another rehearsal method is to constrain the gradients updates so that the loss

of the samples in memory never increases. More formally,

1GNs: Task given during training, no inference; shared labels.

Lθt (Dt) s.t. Lθt (M) ≤ Lθt−1(M).

(4.5)

42

Of this kind, the method Gradient Episodic Memory (GEM) [205] computes the gradient

constraint via a quadratic programming solver that scales with the number of parameters

of the model. After our ﬁrst investigation, we discover that it is impractical for large-

language models to use GEM, since they have millions of parameters and the constraints

are computed for each batch. To cope with this computational complexity, Chaudhry

et al. [206] proposed A-GEM, which efﬁciently computes the gradient constraints while

being effective in CL tasks. Finally, a rehearsal method speciﬁc to language tasks is

LAMOL [152], which instead of storing samples in M, trains a model that simultaneously

learns to solve tasks and generate training samples.

• Architectural methods add task-speciﬁc parameters to an existing base model for each

task. Of this kind, multiple models have been proposed, such as Progressive Net [207],

Dynamically Expandable Networks (DEN) [208] and Learn-to-Grow [209]. On the other

hand, there are ﬁxed-capacity methods, that do not add speciﬁc parameters, but learn

parameter masks [210], usually binary [211], to select sub-networks that are task-speciﬁc.

To the best of our knowledge, these models have been tested mostly on computer vision

tasks, and they can not easily handle our CL setting (i.e., no task-ID during testing).

4.2 AdapterCL

Motivated by the lack of architectural baselines for CL in Seq2Seq modelling, we propose a

novel architectural method called AdapterCL. Our proposed method parameterizes each task

using residual adapters [77], as we have seen in the previous chapter, and it uses an entropy-

based classiﬁer to select which adapter to use at testing time. This method is designed for large

pre-trained language models, e.g., GPT-2, since only the task-speciﬁc parameters are trained,

while the original weights are left frozen.

Residual adapters To continuously learn new tasks, we ﬁrst spawn a new adapter, parame-

terized by µ, and then we train its parameters as in Equation 4.3. For instance, given the dataset

Dt and the model with its corresponding adapter µt, the loss is deﬁned as:

Lµt (Dt) = −

|Dt |
∑
j

n+m
∑
i=0

log pµt (x j

i |x j

0, · · · x j

i−1).

(4.6)

Importantly, the loss is optimized over µt to guarantee that each task is independently learned.

An example of AdapterCL while learning tasks continuously is shown in Figure 4.3.

43

Figure 4.3: Example of CL with AdpaterCL. Every time a new domain dataset D arrives, a new
adapter is spawned, and it is ﬁne-tuned with the data pairs, while the original weights are left
frozen.

Perplexity-Based Classiﬁer

In our CL setting the task-ID is provided during training, and

thus each µt is optimized over Dt. During testing, however, the task-ID is not provided, and thus

the model has to predict which adapter to use to accomplish the task. This step is not required

in regularization and rehearsal approaches since a single set of parameters is optimized during

training.

Inspired by Wortsman et al. [202], we propose to utilize the perplexity of each adapter over the

input X as a measure of uncertainty. Thus, by selecting the adapter with the lowest perplexity,

we select the most conﬁdent model to generate the output sequence. The perplexity of an input

sequence X = x0, · · · , xn is deﬁned as

PPLθ (X) = n

(cid:115) n
∏
i=1

1
pθ (xi | x0, · · · , xi−1)

(4.7)

Therefore, given the set of adapters parameterized by µ0, . . . , µN, each of which is trained re-

spectively with D0, . . . , DN, and an input sample X, we compute:

αt = PPLµt (X) ∀t ∈ 1, · · · , N,

(4.8)

where each αt represents the conﬁdence of the adapter t for the input X. The task-ID t is thus

selected as

t∗ = argmin α0, · · · , αN,

(4.9)

44

Figure 4.4: Example of the perplexity-based classiﬁer. Given the sentence "I need an hotel",
we forward each of the adapters to compute the perplexity score. In this example, the HOTEL
adapter achieves the lowest perplexity, and it is selected to generate the output sequence.

The perplexity-based selector requires a linear number of forwards with respect to the number of

adapters (Equation 4.8), but it has the advantage of not requiring a further classiﬁer, which itself

would suffer from catastrophic forgetting and would require episodic memory. An example of

perplexity-based adapter selection is shown in Figure 4.4.

4.3 Experimental Settings

In this section we describe 1) the datasets used for creating the learning curriculum, 2) the

evaluation metric used to evaluate the different settings, and 3) the experimental setups.

4.3.1 Datasets

To the best of our knowledge, there is no benchmark for CL in dialogue systems with a high

number of tasks to be learned continuously and with multiple training settings. The closest to

ours is the method from Mi et al. [161], which continuously learns ﬁve domains in the NLG

setting. In general, NLP benchmarks for CL use no more than 10 tasks [148, 152]. Conse-

quently, we propose a CL benchmark in which we jointly pre-processing four task-oriented

datasets: Task-Master 2019 (TM19) [212], Task-Master 2020 (TM20) [212], Schema Guided

Dialogue (SGD) [213] and MultiWoZ [214]. This results in a curriculum of 37 domains to be

learned continuously under four settings: INTENT classiﬁcation, DST, NLG, and E2E. This is

possible because the four datasets provide the speech act annotation for both the user and the

45

Name Train Valid Test Dom.
TM19
551
1,731
TM20
1,000
MWoZ
761
4,043

4,403
13,839
7,906
SGD 5,278
31,426
Total

553
1,734
1,000
1,531
4,818

6
7
5
19
37

Intent Turns
19.97
16.92
13.93
14.71
16.23

112
128
15
43
280

Table 4.1: Main dataset statistics.

system turns, and the dialogue state as well. To avoid any domain overlapping, we select only

the dialogues with a single domain, and we do not merge domains with similar/the same names

or semantics. For example, the restaurant domain appears in TM19, TM20 and MWOZ, with

different slot names and values. Thus, we intentionally keep these data samples separate for

modelling scenarios in which multiple APIs are available for the same domain.

Finally, the datasets are pre-processed as in Section 4.1.1 to form the four tasks, and the main

statistics are shown in Table 4.1.

4.3.2 Evaluation Metrics

Automatic evaluations for E2E TODs are challenging, especially for the response generation

task. To overcome this issue, we use well-deﬁned metrics based on the three modularized

settings. In all of the three sub-tasks, we deﬁne the relevant metrics as follows:

• INTENT recognition is evaluated using the accuracy between the generated intents and

the gold labels.

• DST is evaluated with the Joint Goal Accuracy (JGA) [108] over the gold dialogue states.

• NLG is evaluated using both the BLEU score [192] and the slot error rate (EER) [47],

which is computed as the ratio between the total number of slots and the values not ap-

pearing in the response. In datasets such as SGD, the slot has binary values, e.g., yes or

no, and thus we exclude these from the count, as in Kale and Rastogi [215]. In the E2E

setting, if the API output (Xout) is empty, then we rely on the BLEU score.

Independently of these metrics, we also compute CL-speciﬁc metrics such as the average metric

through time (Avg. Metric), as in Lopez-Paz and Ranzato [205]. We consider access to the test

set for each of the T tasks, and after the model ﬁnishes learning the task ti, we evaluate its test
performance on all tasks in the curriculum. To elaborate, we construct the matrix R ∈ RT ×T ,

46

Method
VANILLA
L2
EWC
AGEM
LAMOL
REPLAY
ADAPT
MULTI

+Param. Mem.

-
|θ |
2|θ |
-
-
-
t|µ|
-

/0
/0
/0
t|M|
/0
t|M|
/0
-

Hours↓
0.21 ± 0.02
0.56 ± 0.06
0.91 ± 0.10
0.38 ± 0.04
2.32 ± 1.24
0.62 ± 0.23
0.20 ± 0.02
4.14 ± 2.23

INTENT
Accuracy↑
4.1 ± 1.4
3.8 ± 1.4
3.9 ± 1.3
34.0 ± 6.4
7.5 ± 6.4
81.1 ± 1.4
90.5 ± 0.6
95.5 ± 0.1

DST
JGA↑
4.91 ± 4.5
3.81 ± 3.4
5.22 ± 4.5
6.37 ± 4.0
4.55 ± 3.5
30.33± 1.2
35.1 ± 0.5
48.9 ± 0.2

NLG

BLEU↑
EER↓
6.38 ± 0.6
48.7 ± 3.9
5.4 ± 0.9
55.7 ± 7.1
5.06 ± 0.5
58.2 ± 3.7
4.54 ± 0.6
62.1 ± 6.9
3.0 ± 0.9
66.1 ± 6.9
17.8 ± 0.9
17.4 ± 0.7
31.78 ± 1.3 16.76 ± 0.4
12.56 ± 0.2 23.61 ± 0.1

Table 4.2: E2E results in terms of Intent Accuracy, Joint Goal Accuracy (JGA), Slot Error Rate
(EER) and BLUE. +Param. shows the additional number of parameters per task (θ base model
and µ task-speciﬁc parameters), and Mem. the episodic memory size (denoted as |M|) needed
per task, and Hours is the average hours per epoch (single NVIDIA 2080Ti)required for training
a new domain.

where Ri, j is the test metric (e.g., BLEU, JGA) of the model on task t j after observing the last

sample from task ti. Then we deﬁne the average accuracy as

Avg.Metric =

1
T

T
∑
i=1

RT,i,

(4.10)

The Avg. Metric score is useful for understanding the learning dynamics through time of dif-

ferent baselines. Further metrics such as Backward-Transfer and Forward-Transfer [205] are

available to distinguish baselines with similar Avg. Metric scores, but we limit our evaluation

to this metric, since there is a large gap among the baselines. Finally, to evaluate the adapter

selection, we use the accuracy over the gold task-ID.

4.3.3 Baselines and Settings

The main goal is to compare the performance of different CL approaches and to understand

the trade-offs among them. Therefore, following the deﬁnition provided in Section 4.1.2, we

compare 1) EWC and L2, 2) A-GEM, LAMOL, and REPLAY, and 3) AdapterCL. Additionally,

we provide a baseline trained on each task continuously, namely, VANILLA, without any regu-

larization or memory, and a multitask baseline (MULTI), which is trained on all the data in the

curriculum at the same time. In L2, EWC, and A-GEM, we tune different λ in the range 0.0001

to 100, and in the rehearsal-based methods, REPLAY and GEM, we keep 50 samples per task,

for a total of 1,850 samples in M at the end of the curriculum. This is particularly important

since, if we store in memory all the samples of the seen tasks, the model incurs a high training

47

Figure 4.5: Avg. Metric for Intent Accuracy, JGA, BLEU and EER in the E2E setting.

cost. Arguably, this could be an option if the per-task sample size is small, but this is not al-

ways possible, e.g, large language models [106]. Therefore, the assumption of minimizing the

number of samples in memory is valid and is widely used in the CL literature [161]. Finally, for

the AdapterCL, we tune the bottleneck size b between 10, 50, 100, and 200. Interested readers

can refer to Appendix A for further details of the selected hyper-parameters. In CL the model

is not able decide the order of tasks. Therefore, we create ﬁve learning curricula by randomly

permuting the 37 tasks.

4.4 Results & Analysis

The main results in the E2E setting are summarized in Table 4.2 In these tables we report the

Avg. Metric at the end of the curriculum, which is equivalent to the average test set performance

in all the tasks, and the resources used by each model.

Main Results From the tables, we can observe that 1) both regularization-based methods

(L2/EWC) and some rehearsal-based methods (AGEM/LAMOL) cannot continually learn tasks

without incurring in catastrophic forgetting, 2) REPLAY and AdapterCL perform comparably

48

Figure 4.6: Training time for adding a new domain for different CL techniques.

well on the Intent and DST tasks, 3) REPLAY works the best on the NLG task, showing that

transferring knowledge between tasks is needed, and 4) no CL methods can reach the perfor-

mance of the multi-task baseline, especially on the DST task. In addition, the adapter selection

accuracy based on Equation 4.9 is 95.44±0.2% in E2E, 98.03±0.1% in Intent Recognition,

98.19±0.1% in DST, and 93.98±0.1% in the NLG.

Although these numbers are meaningful, they do not describe the entire learning history of the

curriculum. To better understand these dynamics, we plot the Avg. Metric in Equation 4.10

after each task is learned (t = T in the equation). Figure 4.5 shows the plot for the considered

metrics and all the baselines. From this ﬁgure we can better understand how REPLAY and

AdapterCL outperform the other baselines and, interestingly, that LAMOL performs as well as

REPLAY on the ﬁrst 12 tasks. This is because LAMOL learns to generate training samples

instead of using an explicit memory, and thus the generation becomes harder when more and

more task are shown. This result further strengthens our motivation to have a benchmark with

a long curriculum.

4.4.1 Training Time Analysis

From Figure 4.6 we plot the training time (Hours × Epochs) required to add a new domain to

an existing model. A clear trend is shown where rehearsal based methods (REPLAY, LAMOL)

requires a linearly increasing amount of time to add new domains, while ADAPTER and

VANILLA the time remain constant across time. This is even more evident when the entire

49

Figure 4.7: "No free lunch" in CL. Plot of the trade-off between number of parameters added
per task and the size of the episodic memory M.

training-set of all the previous tasks is used for training (REPLAY-ALL), which lead to a very

expensive process to add new domains. The average time across domain for all the baseline is

shown in Table 4.2. ADAPTER based CL requires also an additional cost in selecting which

parameters to use during testing. By using a single 2080ti, the average time to select the adapter

is 0.069 ± 0, 003 seconds, which is as expensive as decoding 4 tokens.

4.4.2 No Free Lunch

Based on the results shown in Table 4.2, and especially based on the resources used by each

method, we conclude that there is a no free lunch in terms of resources needed to avoid the

catastrophic forgetting problem. To elaborate, in both REPLAY and AdapterCL, the resources

used grow linearly with the number of tasks; i.e., in REPLAY the number of samples stored

in the episodic memory grows linearly (50 times the number of tasks), and in AdapterCL the

number of parameters grows linearly (number of adapter parameters times the number of tasks).

Figure 4.7 describes the high-level intuition behind this concept by plotting the number of tasks

and parameters and the episodic memory sizes needed.

Therefore, given a resource budget, different baselines are preferable in terms of memory or

parameters. The main advantage of using memory-based methods (e.g., REPLAY) is that no

parameters are added, and thus the resulting model is closer to the multitask baseline. However,

this comes with the disadvantage of losing the learned weights of the original pre-trained model.

This is particularly critical for large pre-trained language model which provide a good starting

point for ﬁne-tuning new tasks. On the other hand, the main advantage of parameter-isolation

50

Figure 4.8: Ablation study on the size of the episodic-memory vs Intent Accuracy/JGA/BLEU
and EER.

methods (e.g., AdapterCL) is the ability to retain the original weights and to control which

tasks to trigger, given a certain input. The latter is important in scenarios where just a subset of

a domain is shown to the user (e.g., only one particular restaurant API). The main disadvantage,

however, is the lack of knowledge transfer among tasks, since each dataset is trained in isolation.

4.4.3 Analysis: Episodic Memory Size

In this section, we analyze the effect of increasing the episodic memory size for the REPLAY

method. Trivially, by including all the training samples in the memory, the model, in the last

task, converges to the multitask baseline. Then, the question of how many samples to keep per

task to avoid catastrophic forgetting becomes important. In light of this, Figure 4.8 shows the

performance of the model at different episodic memory sizes on the different tasks. Here, we

observe that by storing only a few samples per task (10–50) the model still greatly suffers from

catastrophic forgetting, where with around 500 samples, which is equivalent to a total of 18,500

samples in our setting, the performance is closer to that of the multitask baseline (i.e., a possible

upper bound).

51

4.5 Short Summary

We propose a benchmark for CL in TODs, with 37 tasks to be learned continuously on four set-

tings: intent recognition, DST, NLG, and end-to-end. Then, we implement three CL methodolo-

gies: regularization, rehearsal and architectural. For the latter, we propose a simple yet effective

method based on residual adapters and use a perplexity-based classiﬁer to select an adapter to

use at inference time. Finally, we analyze the trade-off between the performance, the number

of parameters, and the episodic memory sizes of the evaluated baselines, unveiling an insightful

trade-off (“no-free lunch") among the methods.

52

Chapter 5

Controlling Multi-Skill Dialogue Systems

Unlike humans, who can do both, task oriented dialogue [20, 24] and chit-chat [80, 81] systems

must often remain separate. A more desirable approach for users, however, would be to have a

single chat interface that can handle both casual talk and tasks such as reservation and schedul-

ing. This can be formulated as a problem of learning different conversational skills across

multiple domains. A skill can be either querying a database, generating daily conversational ut-

terances, or interacting with users in a particular task-domain (e.g. booking a restaurant). One

challenge of having multiple skills is that existing datasets either focus only on chit-chat or on

goal-oriented dialogues. This is due to the fact that traditional task-oriented systems are modu-

larized [20–24]; thus, they cannot be jointly trained with end-to-end architecture as in chit-chat.

However, recently proposed end-to-end trainable models [57, 59–61] and datasets [62, 216] al-

low us to combine task oriented dialogue systems [216, 217] and chit-chat [110] into a single

benchmark dataset with multiple conversational skills, as shown in Table 5.1.

A straighforward solution is to have a single model for all conversational skills, which has

shown to be effective to a certain extent by [73] and [155]. However, putting aside the perfor-

mance in the tasks, such a ﬁxed shared-parameter framework, without any task-speciﬁc designs,

would lose controllability and interpretability in response generation. Instead, we propose to

model multiple conversational skills using the Mixture of Experts (MoE) [6] paradigm, which

is, a model that learns and combines independent specialized experts using a gating function.

For instance, each expert may be specialized in different dialogues domains (e.g., Hotel, Train,

Chit-Chat etc.) and skills (e.g., generating a SQL query). A popular implementation of MoE

53

Table 5.1: An example from the dataset which includes both chit-chat and task-oriented con-
versations. The model predicts all the Sys turns, which include SQL query and generating a
response from the API output, which is dynamically updated with the query results. The skills
are the prior knowledge needed for the response, where "Persona" refers to chit-chat.

[170, 171] uses a set of linear transformations (i.e., experts) in between two LSTM [218] lay-

ers. However, several problems arise with this implementation: 1) the model is computation-

ally expensive as it has to decode each expert multiple times and make the combination at the

representation-level; 2) no prior knowledge is injected in the expert selection (e.g., domains); 3)

the Seq2Seq model has limited ability to extract information from a Knowledge Base (KB) (gen-

erated by the SQL query) [216], as required in end-to-end task-oriented dialogues systems [62].

The latter can be solved by using more advanced multi-hop models, like Transformer [97], but

the remaining two problems still need to be addressed. Hence, we:

• propose a novel Transformer-based architecture called Attention over Parameters (AoP).

This model parameterizes the conversational skills of end-to-end dialogue systems with

independent decoder parameters (experts), and learns how to dynamically select and com-

bine the appropriate decoder parameter sets by leveraging prior knowledge from the data,

such as domains and skill types;

• prove that AoP is algorithmically more efﬁcient compared to forwarding all the Trans-

former decoders and then mixing their output representations, like is normally done in

MoE. Figure 5.1 illustrates the high-level intuition of the difference;

• empirically show the effectiveness of using specialized parameters in a combined dataset

54

of MultiWOZ [217], In-Car Assistant [216], and Persona-Chat [110], which to the best of

our knowledge, is the ﬁrst evaluation of this genre, namely end-to-end large-scale multi-

domains and skills. Moreover, we show that our model is highly interpretable and is able

to combine different learned skills to produce compositional responses.

5.1 Methodology

We use the standard encoder-decoder architecture and avoid any task-speciﬁc designs [59, 60],

as we aim to build a generic conversation model for both chit-chat and task-oriented dialogues.

Following the notation in Chapter 4, we deﬁne two input-output sequences types as

U → SAPI

(API)

U + SOUT → S (Response),

where U is the dialogue history, SAPI is an API call, SOUT is the output of the API and S

is the system response. SOUT it is the result of a API execution (e.g., table) or plain texts

(e.g., persona description), depending on the task. Therefore, we deﬁne a dialogue dataset as
DV = {(Xi,Yi)}N

i , where (Xi,Yi) is a general input-output pair from the two possible types (API
and Response). Finally, we deﬁne a binary skill vector Vi = {v1, . . . , vr} that speciﬁes the type

of skill required to generate Yi. This can be considered as a prior vector for learning to select the
correct expert during training. 1 For example, in Table 5.1, the ﬁrst response is of type API in

the Hotel domain. Thus the skill vector V will have vAPI = 1 and vHotel = 1, while all the other
skills/domains are set to zero. 2 More importantly, we may set the vector V to have multiple

skills to force the model to compose skills to achieve a semantic compositionality of different

experts.

5.1.1 Attention over Parameters

By following the Transformer notation in Chapter 2, the main idea is to produce a single set of

parameters for decoder TRSdec by the weighted sum of r independently parameterized decoders.

This process is similar to attention [219], where the memories are the parameters and the query

is the encoded representation. Let us deﬁne Θ = [θ1, . . . , θr] as the list of parameters for r

decoders, since a TRSdec is represented by its parameters θ . Since each θ can be sized in

1The vector V will be absent during the testing.
2With the assumption that each index in V is assigned a semantic skill (e.g., API position i).

55

Figure 5.1: Comparisons between single model, Mixture of Experts (MoE) [6], and Attention
over Parameters (AoP).

the order of millions, we assign the corresponding key vectors to each θ , similar to key-value
memory networks [220]. Thus, we use a key matrix K ∈ Rdmodel×r and a Recurrent Neural

Network (RNN), in this instance a GRU [92], to produce the query vector by processing the

encoder output H. The attention weights for each decoder’s parameters is computed as follows:

q = RNN(H)

α = Softmax(qK),

(5.1)

(5.2)

where q ∈ Rdmodel and α ∈ Rr is the attention vectors where each αi is the score corresponding

to θi. Hence, the new set of parameters is computed as follows:

θ ∗ =

r
∑
i

αiθi,

(5.3)

The combined set of parameters θ ∗ are then used to initialize a new TRSdec, and Equation

2.29 (Chapter 2) is applied to the input based on this. Equation 5.2 is similar to the gating

function proposed in [170] and [6], but the resulting scoring vector α is applied directly to the

parameter instead of the output representation of each decoder, holding an algorithmically faster

computation.

Theorem

The computation cost of Attention over Parameters (AoP) is always lower than of Mixture Of

Experts (MoE) for a sequence longer than 1.

Proof
Let fθ : Rd → Rn be a generic function parametrized by θ . Without loss of generality, we
deﬁne θ as an afﬁne transformation W ∈ Rd×n. Let X ∈ Rt×d be a generic input sequence of

56

⋯length t and d dimensional size. Let the set F = [ fθ1, · · · , fθr] be the set of r experts. Hence, the
operation done by MoE is:

MoE(X) = fθ1(X) + · · · + fθr(X) = XW1 + · · · + XWr,

(5.4)

Thus the computational cost in terms of operation is O(rtdn + rtn) since the cost of fθi(X) is
O(tdn) and it is repeated r times, and the cost of summing the representation is O(rtn). On the

other hand, the operation done by AoP is:

θ ∗ = θ1 + · · · + θr = W1 + · · · +Wr

AoP(X) = fθ ∗(X) = XW ∗.

(5.5)

(5.6)

In this case the computational cost in terms of operation is O((r + t)dn) since the cost of sum-
ming the parameters is O(rdn) and the cost of fθ ∗ is O(tdn). Hence, it is easy to verify that if
t > 1, then

rtdn + rtn ≥ (rt)dn ≥ (r + t)dn.

(5.7)

Furthermore, the assumption of using a simple afﬁne transformation W is actually an optimal

case. Indeed, assuming that the cost of parameters sum is equal to the number of operations is

optimistic. For instance, already, by using attention, the number of operations increases, but the
number of parameters remains constant. (cid:4)

Importantly, if we apply α to each of the output representations Ti generated by the TRSi

dec, we
end up having a Transformer-based implementation of MoE. We call this model Attention over

Representation (AoR). Finally, an additional loss term is used to supervise the attention vector

α by using the prior knowledge vector V . Since multiple decoder parameters can be selected at

the same time, we use binary cross-entropy to train each αi. Thus a second loss is deﬁned as:

Lθ ∗(V ) = −

r
∑
j=1

v j × logσ (qK) j + (1 − v j) × log(1 − σ (qK) j),

(5.8)

The ﬁnal loss is the summation of Lθ ∗(DV ) and Lθ ∗(V ).

Finally, in AoP, in general in the MoE framework, stacking multiple layers (e.g., Transformer)

leads to models with a large number of parameters, since multiple experts are repeated across

layers. An elegant workaround is the Universal Transformer [176], which loops over an unique

layer and, as shown by [176], holds similar or better performance than a multi-layer Trans-

former. In our experiment, we report a version of AoP that uses this architecture, which for

instance, does not add any further parameters to the model.

57

5.2 Experiments and Results

5.2.1 Dataset

To evaluate the performance of our model for different conversational skills, we propose to com-

bine three publicly available datasets: MultiWOZ [217], Stanford Multi-domain Dialogue [216]

and Persona-Chat [110].

MultiWOZ (MWOZ)

is a human-to-human

multi-domain goal-oriented dataset annotated

SMD MWOZ Persona

with dialogue acts and states.

In this

#Dialogues

2425

8,438

12,875

dataset, there are seven domains: Taxi, Po-

#turns

12,732

115,424

192,690

lice, Restaurant, Hospital, Hotel, Attraction,

Avg. turns

Train. There are also two API interfaces, SQL

Avg. tokens

5.25

8.02

13.68

13.18

14.97

11.96

and BOOK, the former of which is used to re-

Vocab

2,842

24,071

20,343

trieve information about a certain domain and

the latter of which is used to book restaurants,

hotels, trains, and taxis. We reﬁne this dataset

Table 5.2: Datasets statistics

to include SQL/BOOK queries and their outputs using the same annotation schema as [62].

Each response can either be plain text conversation with the user or SQL/BOOK queries, and the

memory is dynamically populated with the results from the queries as the generated response is

based on such information. This transformation allows us to train end-to-end models that learns

how and when to produce SQL queries, to retrieve knowledge from a dynamic memory, and to

produce plain text responses. A detailed explanation is reported in Appendix B, together with

some samples.

Stanford Multi-domain Dialogue (SMD)

is another human-to-human multi-domain goal-

oriented dataset that is designed for end-to-end training. There are three domains in this dataset

(Point-of-Interest, Weather, Calendar). The difference between this dataset and MWOZ is that

each dialogue is associated with a set of records relevant to the dialogues. The SOUT is ﬁxed in

this case. Thus the model does not need to issue any API-calls. However, retrieving the correct

entities from the memory is more challenging as the model has to compare alternatives among

records.

58

Persona-Chat

is a multi-turn conversational dataset, in which two speakers are paired and

different persona descriptions (4–5 sentences) are randomly assigned to each of them. For

example, “I am an old man” and “I like to play football” are one of the possible persona de-

scriptions provided to the system. Training models using this dataset results in a more persona

consistent and ﬂuent conversation compared to other existing datasets [110]. Currently, this

dataset is the standard benchmark for chit-chat systems. Thus, we include it in our evaluation.

For all three datasets, we use the training/validation/test split provided by the author and we

keep all the real entities in input instead of using their delexicalized versions as in [217] and

[216]. This makes the task more challenging, but at the same time more interesting since we

force the model to produce real entities instead of generic and frequent placeholders. Table 5.2

summarizes the dataset statistics in terms of number of dialogues, turns, and unique tokens.

Finally, we merge the three datasets, obtaining 154,768/19,713/19,528 for training, validation

and, test respectively and a vocabulary size of 37,069 unique tokens.

5.2.2 Evaluation Metrics

Goal-Oriented For both MWOZ and SMD, we follow the evaluation done by existing works [57,

58, 73] and [67]. We use the BLEU3 score [221] to measure the response ﬂuency and Entity F1-

Score [71, 73] to evaluate the ability of the model to generate relevant entities from the dynamic

memory. Since MWOZ also includes SQL and BOOK queries, we compute the exact match

accuracy (ACCSQL and ACCBOOK) and BLEU score (BLEUSQL and BLEUBOOK). Furthermore,

we also report the F1-score for each domain in both MWOZ and SMD.

Chit-Chat We compare the perplexity, BLEU score, F1-score [111], and consistency score

of the generated sentences with the human-generated prediction. The consistency score [112]

is computed using a Natural Language Inference (NLI) model trained on dialogue NLI [222],

a recently proposed corpus based on the Persona dataset. We ﬁne-tune a pre-trained BERT

model [99] using the dialogue DNLI corpus and achieve a test set accuracy of 88.43%, which

3Using the multi-bleu.perl script

59

is similar to the best-reported model in [222]. The consistency score is deﬁned as follows:

(cid:40) 1

if u entails p j

NLI(u, p j) =

0 if u is independent to p j

−1

if u contradicts p j

C(u) =

m
∑
j

NLI(u, p j),

(5.9)

where u is a generated utterance and p j is one sentence in the persona description. In [222] and

[112], the authors showed that by re-ranking the beam search hypothesis using the DNLI score

(i.e., C score), they achieved a substantial improvement in dialogue consistency. Intuitively,

having a higher consistency C score means having a more persona-consistent dialogue response.

5.2.3 Baselines

In our experiments, we compare Sequence-to-Sequence (Seq2Seq) [223], Transformer (TRS) [97],

Mixture of Experts (MoE) [170] and Attention over Representation (AoR) with our proposed

Attention over Parameters (AoP). In all the models, we used the same copy-mechanism as

in [223]. In AoR, instead of mixing the parameters as in Equation 5.3, we mix the output rep-

resentation of each Transformer decoder (i.e. Equation 2.29). For all AoP, AoR, and MoE,

r = 13 is the number of decoders (experts): two skills for SQL and BOOK and 10 different

domains for MWOZ+SMD and one for Persona-Chat. Furthermore, we also include the fol-

lowing experimental models: AoP using the gold attention vector V , which we refer to as AoP

w/ Oracle (or AoP + O); AoP trained by removing the Lθ ∗(V ) from the optimization, which we

refer to as AoP w/o Lθ ∗(V ); and as previously mentioned, the Universal Transformer for both

AoP, which we call AoP + U, and the standard Transformer, which we call TRS + U (six hops).

Detailed descriptions of all models and the full set of hyper-parameters used in the experiments

are reported in Appendix C.

5.2.4 Results

Table 5.3 and Table 5.2 show the respective evaluation results on the MWOZ+SMD and Persona-

Chat datasets. From Table 5.3, we can identify three patterns 1) AoP and AoR perform consis-

tently better then the other baselines, which shows the effectiveness of combining parame-

ters using the correct prior V ; 2) AoP performs consistently, but marginally, better than AoR,

60

Model

F1
Seq2Seq
38.37
TRS
36.91
MoE 38.64
AoR 40.36
AoP 42.26
TRS + U 39.39
AoP + U 44.04
38.50
AoP+O 46.36

AoP w/o Lθ ∗(V )

BLEU SQLAcc SQLBLEU BOOKAcc BOOKBLEU
9.42
9.92
9.47
10.66
11.14
9.29
11.26
10.50
11.99

81.75
89.08
85.38
90.64
90.90
89.70
91.90
88.28
93.81

79.00
78.41
78.55
81.15
84.08
79.05
84.15
80.34
86.42

39.05
46.51
37.23
52.15
56.31
50.16
56.37
52.61
56.18

49.97
61.96
53.60
69.39
71.1
61.80
74.83
61.47
73.41

Table 5.3: Results for the goal-oriented responses in both MWOZ and SMD. The last raw uses
the Oracle, and bold-faced are best in each setting (w and w/o Universal). Results are averaged
among three run (full table in Appendix F).

with the advantage of an algorithmically faster inference and 3) using Oracle (AoP+O) gives

the highest performance in all measures, which shows the performance upper-bound for AoP.

Hence, the performance gap when not using

Oracle attention is most likely due to the er-

Model

Ppl.

F1

C

BLEU

ror in attention α (2% error rate). Moreover,

Seq2Seq

39.42

6.33

0.11

Table 5.3 shows that by removing Lθ ∗(V )

TRS

43.12

7.00

0.07

(AoP w/o Lθ ∗(V )) the model performance de-

MoE 38.63

7.33

0.19

creases, which conﬁrms that good inductive

bias is important for learning how to select

and combine parameters (experts). Addition-

ally, in Appendix D and E, we report the per-

domain F1-score for SQL, BOOK and sen-

tences, and Table 5.2 and Table 5.3 with the

standard deviation among the three runs.

Furthermore, from Table 5.2, we can see

that MoE has the lowest perplexity and F1-

score, but AoP has the highest consistency

AoR 40.18

6.66

0.12

AoP 39.14

7.00

0.21

TRS + U 43.04

7.33

0.15

AoP + U 37.40

7.00

0.29

AoP w/o Lθ ∗(V )

42.81

6.66

0.12

AoP + O 40.16

7.33

0.21

Figure 5.2: Results for

the Persona-Chat

dataset.

and BLEU score. Note that the perplexity reported in [110] is lower since the vocabulary used

in their experiments was smaller. In general, the difference in performance among the models is

marginal, except for the consistency score; thus, we can conclude that all the models can learn

this skill reasonably well. Consistent with the previous results, when Lθ ∗(V ) is removed from

the optimization, the models’ performance decreases.

61

2.79

2.56

2.92

2.69

3.06

2.66

3.22

2.85

2.91

Table 5.4: Selecting different skills thought the attention vector α results in a skill-consistent
response. AoP response activates SQL and Train.

Finally, in both Table 5.2 and Table 5.3, we report the results obtained by using the Universal

Transformer, for both AoP and the Transformer. By adding the layer recursion, both models

are able to consistently improve on all the evaluated measures, in both Persona-Chat and the

Task-Oriented tasks. AoP, in particular, achieves better performance than Oracle (single layer)

on SQL accuracy, and has a consistently better performance in the Persona-Chat evaluation.

5.3 Skill Composition

To demonstrate the effectiveness of our model in learning independent skills and composing

them together, we manually trigger skills by modifying α and generate 14 different responses

for the same input dialogue context. This experiment allows us to verify whether the model

accurately captures the meaning of each skill and whether it can properly learn to compose the

selected parameters (skills). Table 5.4 ﬁrst shows the dialogue history along with the response of

AoP on the top, and then different responses generated by modifying α (black cells correspond

to 1 in the vector, while the white ones are 0). By analyzing Table 5.4, we can notice that:

62

Figure 5.3: AoP visualization, vector α for different reference, (Ref.) and AoP, generated
answers. Top rows (Usr) are the last utterances from each dialogue context.

• The model learns the correct semantics of each skill. For instance, the AoP response is of

type SQL and Train, and by deactivating the SQL skill and activating other domain-skills,

including Train, we can see that the responses are grammatical and they are coherent with

the selected skill semantics. For instance, by just selecting Train, the generated answer

becomes “what time would you like to leave?”, which is coherent with the dialogue con-

text since such information has not been yet provided. Interestingly, when the Persona

skill is selected, the generated response is conversational and also coherent with the dia-

logue, even though it is less ﬂuent.

• The model effectively learns how to compose multiple skills. For instance, when SQL

or BOOK is triggered the response produces the correct SQL syntax (e.g. “SELECT *

FROM ...” etc.). By also adding the corresponding domain-skill, the model generates the

correct query format and attributes relative to the domain type (e.g., in SQL, Restaurant,

the model queries with the relevant attribute food for restaurants).

5.4 Attention Visualization

Figure 5.3 shows the attention vector α over parameters for different generated sentences. Note

that the sentences shown in the ﬁgure are the expected responses, not the given input, which

is omitted since it can be very large. In this ﬁgure, and by analyzing more examples, we can

identify two patterns:

• AoP learns to focus on the correct skills (i.e., SQL, BOOK) when API-calls are needed.

63

SQLBOOKtaxipolicerestauranthospitalhotelattractiontrainnavigateweatherschedulepersonaAoP:Youarewelcome.IsthereanythingelseIcanhelpyouwith?Ref.:Thankyouandenjoyyourstay!Usr:Thankyouforyourhelp!AoP:BOOKFROMtaxiWHEREdestination=’ﬁnchesbedandbreakfast’ANDdeparture=’ask’Ref.:Icanhelpyouwiththat.whenwouldyouliketoleavethehotel?Usr:Thanks.Iwillalsoneedataxifromthehoteltotherestaurant.Willyouhandlethis?AoP:SELECT*FROMhotelWHEREparking=’yes’ANDpricerange=’expensive’ANDstars=’3’Ref.:SELECT*FROMhotelWHEREparking=’yes’ANDpricerange=’expensive’ANDstars=’3’Usr:Iwouldliketostayinanexpensive3starplacewithfreeparking.Isthereanythinglikethat?From the ﬁrst example in Figure 5.3, we can see that the activations in α are consistent

with those in the correct attention vector P. There are also false positives, in which AoP

puts too high a weight on BOOK when the correct response is plain text that should

request more information from the user (i can help you with that. when would you like to

leave the hotel?). However, we can see that this example is, in fact, "almost correct" as

triggering a booking API-call may also be considered a valid response. Meanwhile, the

third example also fails to attend to the correct skill, but, in fact, generates a very ﬂuent

and relevant response. This is most likely because the answer is simple and generic.

• The attention often focuses on multiple skills not directly relevant to the task. We observe

this pattern especially when there are other skill-related entities mentioned in the context

or the response. For example, in the second dialogue example in Figure 5.3, we notice

that AoP not only accurately focuses on the taxi domain, but also has non-negligible

activations for restaurant and hotel. This is because the words “hotel" and “restaurant"

are both mentioned in the dialogue context and the model has to produce two entities of

the same type (ﬁnches bed and breakfast and ask).

5.5 Short Summary

We propose a novel way to train a single end-to-end dialogue model with multiple composable

and interpretable skills. Unlike previous work, which has mostly focused on the representation-

level mixing [170], our proposed approach, Attention over Parameters, learns how to softly

combine independent sets of specialized parameters (making an SQL-Query, conversing with

a consistent persona, etc.) into a single set of parameters. By doing so, we not only achieve

compositionality and interpretability but also gain an algorithmically faster inference speed. To

train and evaluate our model, we organize a multi-domain task-oriented dataset into end-to-end

trainable formats and combine it with a conversational dataset (Persona-Chat). Our model learns

to consider each task and domain as separate skills that can be composed with each other, or

used independently, and we verify the effectiveness of the interpretability and compositionality

with competitive experimental results and thorough analysis.

64

Chapter 6

Conclusion

In this thesis, we focused on the controllability of deep learning-based, end-to-end, genera-

tive dialogue systems in both task-oriented and chit-chat scenarios. In particular, we explored

the different aspects of controlling generative dialogue systems: we described how to control

the style and topics of chit-chat models, how to continuously control and extend task-oriented

dialogue systems, and how to compose and control multi-skill dialogue models.

In controlling the style and topics of generative chit-chat dialogue systems, we ﬁrst proposed

and evaluated plug-and-play methods for controllable response generation, which do not re-

quire dialogue speciﬁc datasets nor rely on ﬁne-tuning a large model. While effective, the

decoding procedure induce considerable computational overhead, rendering the conversational

model unsuitable for interactive usage. To overcome this, we introduced an approach that does

not require further computation at decoding time, nor does it require any ﬁne-tuning of a large

language model. We demonstrated, through extensive automatic and human evaluation, a high

degree of control over the generated conversational responses with regard to multiple desired

attributes, while also being ﬂuent.

In continuously controlling and extending task-oriented dialogue systems, we proposed a bench-

mark for task-oriented dialogue systems with 37 domains to be learned continuously in four

settings: intent recognition, state tracking, natural language generation, and end-to-end. More-

over, we implemented and compared multiple existing continual learning baselines, and we

proposed a highly controllable architectural method based on residual adapters. Our experi-

ments demonstrated that the proposed architectural method and a simple replay-based strategy

perform comparably well, but they both achieve inferior performance to the multi-task learning

65

baseline, in which all the data are shown at once, showing that this setting is a challenge. Fur-

thermore, we reveal several trade-offs between learning methods in terms of parameter usage

and memory size, which are important in the design of task-oriented dialogue systems.

In learning to compose and control multi-skill dialogue models, we proposed to learn a dialogue

system that independently parameterizes dialogue skills, and learns to select and combine each

of them through Attention over Parameters (AoP). The experimental results showed that this

approach achieves competitive performance on a combined dataset of MultiWOZ, In-Car As-

sistant, and Persona-Chat, and we demonstrated that each dialogue skill is effectively learned

and can be combined with other skills to produce selective responses.

Overall, the common technique proposed in the thesis for controlling style&topics, continu-

ous domain learning and multi-skills system, is the independent parameterization of different

dialogue skills. This is done by using residual adapters in style&topics and continuously learn-

ing domains, and by using Transformer decoders in multi-skill systems. Independently by the

network choice, the parametrization of each dialogue skill leads to highly controllable, extend-

able and composable dialogue systems. The choice of residual adapter is especially meaningful

for lightweight plug-and-play skills like style&topics and adding dialogue skills through time.

While mixing different Transformer decoders is important in multi-skills systems since allows

to mix and match different skills and achieve compositional behaviours.

In future work, we expect to develop dialogue systems that are easy to control and are also

more data-efﬁcient and can learn from user interaction rather than supervised data. In fact, in

the three chapters of this thesis, we open new exciting research directions for dialogue systems:

1) controlling style and topic is still in its infancy. In Chapter 3, we control seven attributes,

while in future works, we would like to extend our methodology to control many more attributes

(e.g., emotions) and to compose multiple adapters to achieve compositionality (e.g., positive +

sport etc.). 2) controlling how to extend dialogue systems with a clear distinction between tasks

is challenging and exciting, as seen in Chapter 4. However, removing the distinction between

tasks leads to a more challenging setting where the model is forced to learn from a data sample

stream. This streaming learning reassembles human learning and would make a model that

can learn from user interaction rather than supervised data. 3) controlling multi-skills dialogue

systems opens future research directions similar to point 1 and 2, plus the possibility of studying

more advanced methods to compose different skills to obtain exponential dialogue skills by

66

using only a linear number of experts. Although in Chapter 5, we show some compositional

behaviours, it is still an open and exciting challenge to achieve compositional behaviours with

more dialogue skills (e.g., style in task-oriented dialogue).

Figure 6.1: The beasts are tamed.

67

References

[1] Y. Zhang, S. Sun, M. Galley, Y.-C. Chen, C. Brockett, X. Gao,

J. Gao,

J. Liu, and B. Dolan, “DIALOGPT : Large-scale generative pre-training for

conversational response generation,” in Proceedings of the 58th Annual Meeting of

the Association for Computational Linguistics: System Demonstrations. Online:

Association for Computational Linguistics, Jul. 2020, pp. 270–278. [Online]. Available:

https://www.aclweb.org/anthology/2020.acl-demos.30

[2] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kul-

shreshtha, G. Nemade, Y. Lu et al., “Towards a human-like open-domain chatbot,” arXiv

preprint arXiv:2001.09977, 2020.

[3] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y. Liu, J. Xu, M. Ott,

E. M. Smith, Y.-L. Boureau, and J. Weston, “Recipes for building an open-domain

chatbot,” in Proceedings of the 16th Conference of the European Chapter of the

Association for Computational Linguistics: Main Volume. Online: Association

for Computational Linguistics, Apr. 2021, pp. 300–325.

[Online]. Available:

https://www.aclweb.org/anthology/2021.eacl-main.24

[4] J. Williams, A. Raux, and M. Henderson, “The dialog state tracking challenge series: A

review,” Dialogue & Discourse, vol. 7, no. 3, pp. 4–33, 2016.

[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models

are unsupervised multitask learners,” OpenAI Blog, vol. 1, no. 8, p. 9, 2019.

[6] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, G. E. Hinton et al., “Adaptive mixtures of local

experts.” Neural computation, vol. 3, no. 1, pp. 79–87, 1991.

68

[7] M. Munikar, S. Shakya, and A. Shrestha, “Fine-grained sentiment classiﬁcation us-

ing bert,” in 2019 Artiﬁcial Intelligence for Transforming Business and Society (AITB),

vol. 1.

IEEE, 2019, pp. 1–5.

[8] H. Kumar, A. Agarwal, and S. Joshi, “A practical dialogue-act-driven conversation model

for multi-turn response selection,” in Proceedings of the 2019 Conference on Empirical

Methods in Natural Language Processing and the 9th International Joint Conference on

Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 1980–1989.

[9] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, “Xlnet: Gen-

eralized autoregressive pretraining for language understanding,” in Advances in neural

information processing systems, 2019, pp. 5754–5764.

[10] J. Weizenbaum, “Eliza—a computer program for the study of natural language commu-

nication between man and machine,” Communications of the ACM, vol. 9, no. 1, pp.

36–45, 1966.

[11] I. V. Serban, R. Lowe, P. Henderson, L. Charlin, and J. Pineau, “A survey of available

corpora for building data-driven dialogue systems: The journal version,” Dialogue &

Discourse, vol. 9, no. 1, pp. 1–49, 2018.

[12] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural net-

works,” in Advances in neural information processing systems, 2014, pp. 3104–3112.

[13] C. L. Isbell, M. Kearns, D. Kormann, S. Singh, and P. Stone, “Cobot in lambdamoo: A

social statistics agent,” in AAAI/IAAI, 2000, pp. 36–41.

[14] L. Zhou, J. Gao, D. Li, and H.-Y. Shum, “The design and implementation of xiaoice, an

empathetic social chatbot,” Computational Linguistics, vol. 46, no. 1, pp. 53–93, 2020.

[15] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer,

“Deep contextualized word representations,” in Proceedings of the 2018 Conference of

the North American Chapter of the Association for Computational Linguistics: Human

Language Technologies, Volume 1 (Long Papers), 2018, pp. 2227–2237.

[16] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and

P. J. Liu, “Exploring the limits of transfer learning with a uniﬁed text-to-text transformer,”

arXiv preprint arXiv:1910.10683, 2019.

69

[17] J. Shin, P. Xu, A. Madotto, and P. Fung, “Generating empathetic responses by looking

ahead the user’s sentiment,” in ICASSP 2020-2020 IEEE International Conference on

Acoustics, Speech and Signal Processing (ICASSP).

IEEE, 2020, pp. 7989–7993.

[18] Z. Lin, A. Madotto, J. Shin, P. Xu, and P. Fung, “Moel: Mixture of empathetic listen-

ers,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language

Processing and the 9th International Joint Conference on Natural Language Processing

(EMNLP-IJCNLP), 2019, pp. 121–132.

[19] T. Wolf, V. Sanh, J. Chaumond, and C. Delangue, “Transfertransfo: A transfer

learning approach for neural network based conversational agents,” arXiv preprint

arXiv:1901.08149, 2019.

[20] J. D. Williams and S. Young, “Partially observable markov decision processes for spoken

dialog systems,” Computer Speech & Language, vol. 21, no. 2, pp. 393–422, 2007.

[21] C. Hori, K. Ohtake, T. Misu, H. Kashioka, and S. Nakamura, “Statistical dialog man-

agement applied to wfst-based dialog systems,” in IEEE International Conference on

Acoustics, Speech and Signal Processing, 2009. ICASSP 2009.

IEEE, 2009, pp. 4793–

4796.

[22] C. Lee, S. Jung, S. Kim, and G. G. Lee, “Example-based dialog modeling for practical

multi-domain dialog system,” Speech Communication, vol. 51, no. 5, pp. 466–484, 2009.

[23] E. Levin, R. Pieraccini, and W. Eckert, “A stochastic model of human-machine interac-

tion for learning dialog strategies,” IEEE Transactions on speech and audio processing,

vol. 8, no. 1, pp. 11–23, 2000.

[24] S. Young, M. Gaši´c, B. Thomson, and J. D. Williams, “Pomdp-based statistical spoken

dialog systems: A review,” Proceedings of the IEEE, vol. 101, no. 5, pp. 1160–1179,

2013.

[25] C. Raymond and G. Riccardi, “Generative and discriminative algorithms for spoken lan-

guage understanding,” in Eighth Annual Conference of the International Speech Com-

munication Association, 2007.

70

[26] L. Deng, G. Tur, X. He, and D. Hakkani-Tur, “Use of kernel deep convex networks and

end-to-end learning for spoken language understanding,” in 2012 IEEE Spoken Language

Technology Workshop (SLT).

IEEE, 2012, pp. 210–215.

[27] K. Yao, B. Peng, Y. Zhang, D. Yu, G. Zweig, and Y. Shi, “Spoken language understand-

ing using long short-term memory neural networks,” in 2014 IEEE Spoken Language

Technology Workshop (SLT).

IEEE, 2014, pp. 189–194.

[28] D. Guo, G. Tur, W.-t. Yih, and G. Zweig, “Joint semantic utterance classiﬁcation and

slot ﬁlling with recursive neural networks,” in 2014 IEEE Spoken Language Technology

Workshop (SLT).

IEEE, 2014, pp. 554–559.

[29] X. Zhang and H. Wang, “A joint model of intent determination and slot ﬁlling for spoken

language understanding.” in IJCAI, vol. 16, 2016, pp. 2993–2999.

[30] Z. Liu, Y. Xu, T. Yu, W. Dai, Z. Ji, S. Cahyawijaya, A. Madotto, and P. Fung, “Crossner:

Evaluating cross-domain named entity recognition,” arXiv preprint arXiv:2012.04373,

2020.

[31] Z. Liu, G. I. Winata, S. Cahyawijaya, A. Madotto, Z. Lin, and P. Fung, “On the im-

portance of word order information in cross-lingual sequence labeling,” arXiv preprint

arXiv:2001.11164, 2020.

[32] C.-S. Wu, A. Madotto, Z. Lin, P. Xu, and P. Fung, “Getting to know you: User attribute

extraction from dialogues,” arXiv preprint arXiv:1908.04621, 2019.

[33] C.-S. Wu, A. Madotto, W. Liu, P. Fung, and C. Xiong, “Qaconv: Question answering on

informative conversations,” arXiv preprint arXiv:2105.06912, 2021.

[34] G. Tur, L. Deng, D. Hakkani-Tür, and X. He, “Towards deeper understanding: Deep

convex networks for semantic utterance classiﬁcation,” in 2012 IEEE international con-

ference on acoustics, speech and signal processing (ICASSP).

IEEE, 2012, pp. 5045–

5048.

[35] Y.-N. Chen, D. Hakkani-Tür, and X. He, “Zero-shot learning of intent embeddings for

expansion by convolutional deep structured semantic models,” in 2016 IEEE Interna-

tional Conference on Acoustics, Speech and Signal Processing (ICASSP).

IEEE, 2016,

pp. 6045–6049.

71

[36] Z. Liu, J. Shin, Y. Xu, G. I. Winata, P. Xu, A. Madotto, and P. Fung, “Zero-

shot cross-lingual dialogue systems with transferable latent variables,” arXiv preprint

arXiv:1911.04081, 2019.

[37] N. Nguyen and Y. Guo, “Comparisons of sequence labeling algorithms and extensions,”

in Proceedings of the 24th international conference on Machine learning. ACM, 2007,

pp. 681–688.

[38] G. Mesnil, Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. Hakkani-Tur, X. He, L. Heck,

G. Tur, D. Yu et al., “Using recurrent neural networks for slot ﬁlling in spoken language

understanding,” IEEE/ACM Transactions on Audio, Speech, and Language Processing,

vol. 23, no. 3, pp. 530–539, 2014.

[39] A. Rudnicky and W. Xu, “An agenda-based dialog management architecture for spoken

language systems,” in IEEE Automatic Speech Recognition and Understanding Work-

shop, vol. 13, no. 4, 1999.

[40] S. Young, “Using pomdps for dialog management,” in 2006 IEEE Spoken Language

Technology Workshop.

IEEE, 2006, pp. 8–13.

[41] B. Thomson and S. Young, “Bayesian update of dialogue state: A pomdp framework for

spoken dialogue systems,” Computer Speech & Language, vol. 24, no. 4, pp. 562–588,

2010.

[42] M. Henderson, B. Thomson, and S. Young, “Robust dialog state tracking using delex-

icalised recurrent neural networks and unsupervised adaptation,” in 2014 IEEE Spoken

Language Technology Workshop (SLT).

IEEE, 2014, pp. 360–365.

[43] N. Mrkši´c, D. Ó. Séaghdha, T.-H. Wen, B. Thomson, and S. Young, “Neural belief

tracker: Data-driven dialogue state tracking,” in Proceedings of the 55th Annual Meeting

of the Association for Computational Linguistics (Volume 1: Long Papers), 2017, pp.

1777–1788.

[44] L. Li, J. D. Williams, and S. Balakrishnan, “Reinforcement learning for dialog man-

agement using least-squares policy iteration and fast feature selection,” in Tenth Annual

Conference of the International Speech Communication Association, 2009.

72

[45] Z. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and L. Deng, “Bbq-networks: Efﬁcient ex-

ploration in deep reinforcement learning for task-oriented dialogue systems,” in Thirty-

Second AAAI Conference on Artiﬁcial Intelligence, 2018.

[46] S. Busemann and H. Horacek, “A ﬂexible shallow approach to text generation,” in

Natural Language Generation, 1998. [Online]. Available: https://www.aclweb.org/

anthology/W98-1425

[47] T.-H. Wen, M. Gasic, N. Mrkši´c, P.-H. Su, D. Vandyke, and S. Young, “Semantically

conditioned lstm-based natural language generation for spoken dialogue systems,” in

Proceedings of the 2015 Conference on Empirical Methods in Natural Language Pro-

cessing, 2015, pp. 1711–1721.

[48] O. Press, A. Bar, B. Bogin, J. Berant, and L. Wolf, “Language generation with recurrent

generative adversarial networks without pre-training,” arXiv preprint arXiv:1706.01399,

2017.

[49] G. I. Winata, S. Cahyawijaya, Z. Liu, Z. Lin, A. Madotto, P. Xu, and P. Fung, “Learning

fast adaptation on cross-accented speech recognition,” arXiv preprint arXiv:2003.01901,

2020.

[50] G. I. Winata, A. Madotto, C.-S. Wu, and P. Fung, “Code-switched language models using

neural based synthetic data from parallel sentences,” arXiv preprint arXiv:1909.08582,

2019.

[51] P. Xu, C.-S. Wu, A. Madotto, and P. Fung, “Clickbait? sensational headline generation

with auto-tuned reinforcement learning,” arXiv preprint arXiv:1909.03582, 2019.

[52] A. Rastogi, D. Hakkani-Tür, and L. Heck, “Scalable multi-domain dialogue state track-

ing,” in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).

IEEE, 2017, pp. 561–568.

[53] O. Ramadan, P. Budzianowski, and M. Gasic, “Large-scale multi-domain belief tracking

with knowledge sharing,” in Proceedings of the 56th Annual Meeting of the Association

for Computational Linguistics (Volume 2: Short Papers), 2018, pp. 432–437.

[54] V. Zhong, C. Xiong, and R. Socher, “Global-locally self-attentive encoder for dialogue

state tracking,” in Proceedings of

the 56th Annual Meeting of

the Association

73

for Computational Linguistics (Volume 1: Long Papers). Melbourne, Australia:

Association for Computational Linguistics, Jul. 2018, pp. 1458–1467. [Online].

Available: https://www.aclweb.org/anthology/P18-1135

[55] Z. Lin, B. Liu, S. Moon, P. Crook, Z. Zhou, Z. Wang, Z. Yu, A. Madotto, E. Cho,

and R. Subba, “Leveraging slot descriptions for zero-shot cross-domain dialogue state

tracking,” arXiv preprint arXiv:2105.04222, 2021.

[56] C.-Y. Chen, D. Yu, W. Wen, Y. M. Yang, J. Zhang, M. Zhou, K. Jesse, A. Chau,

A. Bhowmick, S. Iyer et al., “Gunrock: Building a human-like social bot by leverag-

ing large scale real user data,” Alexa Price, 2018.

[57] M. Eric and C. Manning, “A copy-augmented sequence-to-sequence architecture gives

good performance on task-oriented dialogue,” in Proceedings of the 15th Conference

of the European Chapter of the Association for Computational Linguistics: Volume 2,

Short Papers. Valencia, Spain: Association for Computational Linguistics, April 2017,

pp. 468–473. [Online]. Available: http://www.aclweb.org/anthology/E17-2075

[58] A. Madotto, C.-S. Wu, and P. Fung, “Mem2seq: Effectively incorporating knowledge

bases into end-to-end task-oriented dialog systems,” in Proceedings of the 56th Annual

Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),

2018, pp. 1468–1478.

[59] C.-S. Wu, R. Socher, and C. Xiong, “Global-to-local memory pointer networks for task-

oriented dialogue,” in Proceedings of the International Conference on Learning Repre-

sentations (ICLR), 2019.

[60] R. G. Reddy, D. Contractor, D. Raghu, and S. Joshi, “Multi-level memory for task ori-

ented dialogs,” in Proceedings of the 2019 Conference of the North American Chapter of

the Association for Computational Linguistics: Human Language Technologies, Volume

1 (Long and Short Papers), 2019, pp. 3744–3754.

[61] S. Yavuz, A. Rastogi, G.-L. Chao, and D. Hakkani-Tur, “DeepCopy: Grounded

response generation with hierarchical pointer networks,” in Proceedings of

the

20th Annual SIGdial Meeting on Discourse and Dialogue.

Stockholm, Sweden:

Association for Computational Linguistics, Sep. 2019, pp. 122–132. [Online]. Available:

https://www.aclweb.org/anthology/W19-5917

74

[62] A. Bordes and J. Weston, “Learning end-to-end goal-oriented dialog,” International Con-

ference on Learning Representations, vol. abs/1605.07683, 2017.

[63] A. Madotto, S. Cahyawijaya, G. I. Winata, Y. Xu, Z. Liu, Z. Lin, and P. Fung, “Learning

knowledge bases with parameters for task-oriented dialogue systems,” arXiv preprint

arXiv:2009.13656, 2020.

[64] Z. Lin, A. Madotto, G. I. Winata, and P. Fung, “Mintl: Minimalist transfer learning for

task-oriented dialogue systems,” arXiv preprint arXiv:2009.12005, 2020.

[65] Z. Lin, A. Madotto, G. I. Winata, P. Xu, F. Jiang, Y. Hu, C. Shi, and P. Fung, “Bitod:

A bilingual multi-domain dataset for task-oriented dialogue modeling,” arXiv preprint

arXiv:2106.02787, 2021.

[66] F. Liu and J. Perez, “Gated end-to-end memory networks,” in Proceedings of the 15th

Conference of the European Chapter of the Association for Computational Linguistics:

Volume 1, Long Papers. Valencia, Spain: Association for Computational Linguistics,

April 2017, pp. 1–10. [Online]. Available: http://www.aclweb.org/anthology/E17-1001

[67] C.-S. Wu, A. Madotto, G. Winata, and P. Fung, “End-to-end recurrent entity network for

entity-value independent goal-oriented dialog learning,” in Dialog System Technology

Challenges Workshop, DSTC6, 2017.

[68] ——, “End-to-end dynamic query memory network for entity-value independent task-

oriented dialog,” in IEEE International Conference on Acoustics, Speech and Signal

Processing (ICASSP), 2018.

[69] J. D. Williams, K. Asadi, and G. Zweig, “Hybrid code networks: practical and efﬁcient

end-to-end dialog control with supervised and reinforcement learning,” in Proceedings

of the 55th Annual Meeting of the Association for Computational Linguistics (Volume

1: Long Papers). Vancouver, Canada: Association for Computational Linguistics, July

2017, pp. 665–677. [Online]. Available: http://aclweb.org/anthology/P17-1062

[70] M. Seo, S. Min, A. Farhadi, and H. Hajishirzi, “Query-reduction networks for question

answering,” International Conference on Learning Representations, 2017.

75

[71] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P. hao Su, S. Ultes, D. Vandyke,

and S. J. Young, “A network-based end-to-end trainable task-oriented dialogue system,”

in EACL, 2017.

[72] I. V. Serban, A. Sordoni, Y. Bengio, A. C. Courville, and J. Pineau, “Building end-to-end

dialogue systems using generative hierarchical neural network models.” in AAAI, 2016,

pp. 3776–3784.

[73] T. Zhao, A. Lu, K. Lee, and M. Eskenazi, “Generative encoder-decoder models

for task-oriented spoken dialog systems with chatting capability,” in Proceedings

of

the 18th Annual SIGdial Meeting on Discourse and Dialogue. Association

for Computational Linguistics, August 2017, pp. 27–36.

[Online]. Available:

http://aclweb.org/anthology/W17-5505

[74] I. V. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. C. Courville, and Y. Ben-

gio, “A hierarchical latent variable encoder-decoder model for generating dialogues.” in

AAAI, 2017, pp. 3295–3301.

[75] A. Madotto, M. Namazifar, J. Huizinga, P. Molino, A. Ecoffet, H. Zheng, A. Papange-

lis, D. Yu, C. Khatri, and G. Tur, “Exploration based language learning for text-based

games,” arXiv preprint arXiv:2001.08868, 2020.

[76] A. See, S. Roller, D. Kiela, and J. Weston, “What makes a good conversation? how

controllable attributes affect human judgments,” in Proceedings of NAACL-HLT, 2019,

pp. 1702–1723.

[77] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo,

M. Attariyan, and S. Gelly, “Parameter-efﬁcient transfer learning for nlp,” in Interna-

tional Conference on Machine Learning, 2019, pp. 2790–2799.

[78] A. Madotto, E. Ishii, Z. Lin, S. Dathathri, and P. Fung, “Plug-and-play conversational

models,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-

guage Processing: Findings, 2020, pp. 2422–2433.

[79] A. Madotto, Z. Lin, Z. Zhou, S. Moon, P. Crook, B. Liu, Z. Yu, E. Cho, and

Z. Wang, “Continual

learning in task-oriented dialogue systems,” arXiv preprint

arXiv:2012.15504, 2020.

76

[80] I. V. Serban, R. Lowe, L. Charlin, and J. Pineau, “Generative deep neural networks for

dialogue: A short review,” arXiv preprint arXiv:1611.06216, 2016.

[81] O. Vinyals and Q. V. Le,

“A neural conversational model,” arXiv preprint

arXiv:1506.05869, 2015.

[82] A. Madotto, Z. Lin, C.-S. Wu, J. Shin, and P. Fung, “Attention over parameters for dia-

logue systems,” arXiv preprint arXiv:2001.01871, 2020.

[83] Z. Lin, A. Madotto, Y. Bang, and P. Fung, “The adapter-bot: All-in-one controllable

conversational model,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence,

vol. 35, no. 18, 2021, pp. 16 081–16 083.

[84] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language

model,” Journal of machine learning research, vol. 3, no. Feb, pp. 1137–1155, 2003.

[85] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representa-

tions of words and phrases and their compositionality,” in Advances in neural information

processing systems, 2013, pp. 3111–3119.

[86] P. Xu, A. Madotto, C.-S. Wu, J. H. Park, and P. Fung, “Emo2vec: Learning generalized

emotion representation by multi-task training,” arXiv preprint arXiv:1809.04505, 2018.

[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,

Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,

M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,

J. Bai,

and

S. Chintala, “Pytorch: An imperative style, high-performance deep learning library,”

in Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle,

A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, Eds. Curran Associates,

Inc., 2019, pp. 8024–8035.

[Online]. Available:

http://papers.neurips.cc/paper/

9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf

[88] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado,

A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard,

Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga,

S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever,

K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden,

77

M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: Large-scale machine

learning on heterogeneous systems,” 2015, software available from tensorﬂow.org.

[Online]. Available: https://www.tensorﬂow.org/

[89] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by back-

propagating errors,” nature, vol. 323, no. 6088, pp. 533–536, 1986.

[90] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training recurrent neural

networks,” in International conference on machine learning. PMLR, 2013, pp. 1310–

1318.

[91] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation,

vol. 9, no. 8, pp. 1735–1780, 1997.

[92] K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and

Y. Bengio, “Learning phrase representations using rnn encoder–decoder for statistical

machine translation,” in Proceedings of the 2014 Conference on Empirical Methods in

Natural Language Processing (EMNLP), 2014, pp. 1724–1734.

[93] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning. MIT press

Cambridge, 2016, vol. 1, no. 2.

[94] A. Graves, “Generating sequences with recurrent neural networks,” arXiv preprint

arXiv:1308.0850, 2013.

[95] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to

align and translate,” in 3rd International Conference on Learning Representations, ICLR

2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio

and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/1409.0473

[96] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based

neural machine translation,” in Proceedings of the 2015 Conference on Empirical Meth-

ods in Natural Language Processing, 2015, pp. 1412–1421.

[97] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and

I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing

Systems, 2017, pp. 6000–6010.

78

[98] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language under-

standing by generative pre-training,” URL https://s3-us-west-2. amazonaws. com/openai-

assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.

[99] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidi-

rectional transformers for language understanding,” arXiv preprint arXiv:1810.04805,

2018.

[100] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov, “Transformer-XL:

Attentive language models beyond a ﬁxed-length context,” in Proceedings of the

57th Annual Meeting of the Association for Computational Linguistics.

Florence,

Italy: Association for Computational Linguistics, Jul. 2019, pp. 2978–2988. [Online].

Available: https://www.aclweb.org/anthology/P19-1285

[101] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettle-

moyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining approach,”

arXiv preprint arXiv:1907.11692, 2019.

[102] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,

P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,

R. Child, A. Ramesh, D. Ziegler,

J. Wu, C. Winter, C. Hesse, M. Chen,

E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,

A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,”

in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato,

R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc.,

2020, pp. 1877–1901. [Online]. Available: https://proceedings.neurips.cc/paper/2020/

ﬁle/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

[103] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and

P. J. Liu, “Exploring the limits of transfer learning with a uniﬁed text-to-text transformer,”

Journal of Machine Learning Research, vol. 21, pp. 1–67, 2020.

[104] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and

L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language

generation, translation, and comprehension,” in Proceedings of the 58th Annual Meeting

of the Association for Computational Linguistics, 2020, pp. 7871–7880.

79

[105] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “Glue: A multi-task

benchmark and analysis platform for natural language understanding,” EMNLP 2018, p.

353, 2018.

[106] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,

P. Shyam, G. Sastry, A. Askell et al., “Language models are few-shot learners,” arXiv

preprint arXiv:2005.14165, 2020.

[107] J. Gao, M. Galley, and L. Li, “Neural approaches to conversational ai,” in The 41st Inter-

national ACM SIGIR Conference on Research & Development in Information Retrieval.

ACM, 2018, pp. 1371–1374.

[108] C.-S. Wu, A. Madotto, E. Hosseini-Asl, C. Xiong, R. Socher, and P. Fung, “Transferable

multi-domain state generator for task-oriented dialogue systems,” in Proceedings of the

57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 808–

819.

[109] J. Li, M. Galley, C. Brockett, G. Spithourakis, J. Gao, and B. Dolan, “A persona-based

neural conversation model,” in Proceedings of the 54th Annual Meeting of the Associa-

tion for Computational Linguistics (Volume 1: Long Papers), vol. 1, 2016, pp. 994–1003.

[110] S. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela, and J. Weston, “Personalizing

dialogue agents: I have a dog, do you have pets too?” in Proceedings of the 56th Annual

Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).

Association for Computational Linguistics, 2018, pp. 2204–2213. [Online]. Available:

http://aclweb.org/anthology/P18-1205

[111] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek, D. Kiela,

A. Szlam, I. Serban, R. Lowe et al., “The second conversational intelligence challenge

(convai2),” in The NeurIPS’18 Competition. Springer, 2020, pp. 187–208.

[112] A. Madotto, Z. Lin, C.-S. Wu, and P. Fung, “Personalizing dialogue agents via meta-

learning,” in Proceedings of the 57th Annual Meeting of the Association for Computa-

tional Linguistics, 2019, pp. 5454–5459.

[113] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston, “Wizard of

wikipedia: Knowledge-powered conversational agents,” in 7th International Conference

80

on Learning Representations,

ICLR 2019, New Orleans, LA, USA, May 6-9,

2019. OpenReview.net, 2019. [Online]. Available: https://openreview.net/forum?id=

r1l73iRqKm

[114] K. Gopalakrishnan, B. Hedayatnia, Q. Chen, A. Gottardi, S. Kwatra, A. Venkatesh,

R. Gabriel, D. Hakkani-Tür, and A. A. AI, “Topical-chat: Towards knowledge-grounded

open-domain conversations,” Proc. Interspeech 2019, pp. 1891–1895, 2019.

[115] M. Ghazvininejad, C. Brockett, M.-W. Chang, B. Dolan, J. Gao, W.-t. Yih, and M. Galley,

“A knowledge-grounded neural conversation model,” in Thirty-Second AAAI Conference

on Artiﬁcial Intelligence, 2018.

[116] N. Moghe, S. Arora, S. Banerjee, and M. M. Khapra, “Towards exploiting background

knowledge for building conversation systems,” in Proceedings of the 2018 Conference

on Empirical Methods in Natural Language Processing, 2018, pp. 2322–2332.

[117] Z. Wu, M. Galley, C. Brockett, Y. Zhang, X. Gao, C. Quirk, R. Koncel-Kedziorski,

J. Gao, H. Hajishirzi, M. Ostendorf, and B. Dolan, “A controllable model of grounded

response generation,” in Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI

2021, Thirty-Third Conference on Innovative Applications of Artiﬁcial Intelligence,

IAAI 2021, The Eleventh Symposium on Educational Advances in Artiﬁcial Intelligence,

EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 2021, pp. 14 085–14 093.

[Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/17658

[118] Y. Xu, E. Ishii, Z. Liu, G. I. Winata, D. Su, A. Madotto, and P. Fung, “Retrieval-

free knowledge-grounded dialogue response generation with adapters,” arXiv preprint

arXiv:2105.06232, 2021.

[119] Y. Li, H. Su, X. Shen, W. Li, Z. Cao, and S. Niu, “Dailydialog: A manually labelled

multi-turn dialogue dataset,” in Proceedings of the Eighth International Joint Conference

on Natural Language Processing (Volume 1: Long Papers), 2017, pp. 986–995.

[120] H. Rashkin, E. M. Smith, M. Li, and Y.-L. Boureau, “Towards empathetic open-domain

conversation models: A new benchmark and dataset,” in Proceedings of the 57th Annual

Meeting of the Association for Computational Linguistics, 2019, pp. 5370–5381.

81

[121] H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu, “Emotional chatting machine: Emo-

tional conversation generation with internal and external memory,” in Thirty-Second

AAAI Conference on Artiﬁcial Intelligence, 2018.

[122] Y. Fan, J. C. Lam, and V. O. K. Li, “Facial action unit intensity estimation via semantic

correspondence learning with dynamic graph convolution.” in AAAI, 2020, pp. 12 701–

12 708.

[123] Q. Li, P. Li, Z. Chen, and Z. Ren, “Empathetic dialogue generation via knowledge en-

hancing and emotion dependency modeling,” arXiv preprint arXiv:2009.09708, 2020.

[124] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, “A diversity-promoting objective

function for neural conversation models,” in Proceedings of NAACL-HLT, 2016, pp. 110–

119.

[125] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao, “Deep reinforcement

learning for dialogue generation,” in Proceedings of the 2016 Conference on Empirical

Methods in Natural Language Processing, 2016, pp. 1192–1202.

[126] A. Ghandeharioun, J. H. Shen, N. Jaques, C. Ferguson, N. Jones, A. Lapedriza, and R. Pi-

card, “Approximating interactive human evaluation with self-play for open-domain dia-

log systems,” in Advances in Neural Information Processing Systems, 2019, pp. 13 658–

13 669.

[127] Y. Bang, N. Lee, E. Ishii, A. Madotto, and P. Fung, “Assessing political prudence of

open-domain chatbots,” arXiv preprint arXiv:2106.06157, 2021.

[128] N. Lee, A. Madotto, and P. Fung, “Exploring social bias in chatbots using stereotype

knowledge.” in WNLP@ ACL, 2019, pp. 177–180.

[129] N. Lee, Y. Bang, A. Madotto, and P. Fung, “Mitigating media bias through neutral article

generation,” arXiv preprint arXiv:2104.00336, 2021.

[130] Y. Kikuchi, G. Neubig, R. Sasano, H. Takamura, and M. Okumura, “Controlling output

length in neural encoder-decoders,” in Proceedings of the 2016 Conference on Empirical

Methods in Natural Language Processing, 2016, pp. 1328–1338.

82

[131] M. Ghazvininejad, X. Shi, J. Priyadarshi, and K. Knight, “Hafez: an interactive poetry

generation system,” in Proceedings of ACL 2017, System Demonstrations, 2017, pp. 43–

48.

[132] B. Peng, C. Zhu, C. Li, X. Li, J. Li, M. Zeng, and J. Gao, “Few-shot natural language

generation for task-oriented dialog,” in Proceedings of the 2020 Conference on Empirical

Methods in Natural Language Processing: Findings, 2020, pp. 172–182.

[133] N. Subramani, S. Bowman, and K. Cho, “Can unconditional language models recover

arbitrary sentences?” in Advances in Neural Information Processing Systems, 2019, pp.

15 232–15 242.

[134] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano,

and G. Irving, “Fine-tuning language models from human preferences,” arXiv preprint

arXiv:1909.08593, 2019.

[135] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence generative adversarial nets

with policy gradient,” in Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.

[136] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher, “Ctrl: A

conditional transformer language model for controllable generation,” arXiv preprint

arXiv:1909.05858, 2019.

[137] J. Ficler and Y. Goldberg, “Controlling linguistic style aspects in neural language gener-

ation,” in Proceedings of the Workshop on Stylistic Variation, 2017, pp. 94–104.

[138] A. Chan, Y.-S. Ong, B. Pung, A. Zhang, and J. Fu, “Cocon: A self-supervised approach

for controlled text generation,” arXiv preprint arXiv:2006.03535, 2020.

[139] Y. Zhang, G. Wang, C. Li, Z. Gan, C. Brockett, and W. B. Dolan, “Pointer: Constrained

text generation via insertion-based generative pre-training,” in Proceedings of the 2020

Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp.

8649–8670.

[140] E. Sheng, K.-W. Chang, P. Natarajan, and N. Peng, “Towards controllable biases in lan-

guage generation,” in Proceedings of the 2020 Conference on Empirical Methods in Nat-

ural Language Processing: Findings, 2020, pp. 3239–3254.

83

[141] G. Carbone and G. Sarti, “Etc-nlg: End-to-end topic-conditioned natural language gen-

eration,” arXiv preprint arXiv:2008.10875, 2020.

[142] A. Holtzman, J. Buys, M. Forbes, A. Bosselut, D. Golub, and Y. Choi, “Learning to

write with cooperative discriminators,” in Proceedings of the 56th Annual Meeting of the

Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 1638–

1649.

[143] A. Baheti, A. Ritter, J. Li, and B. Dolan, “Generating more interesting responses in

neural conversation models with distributional constraints,” in Proceedings of the 2018

Conference on Empirical Methods in Natural Language Processing, 2018, pp. 3970–

3980.

[144] B. Krause, A. D. Gotmare, B. McCann, N. S. Keskar, S. Joty, R. Socher, and N. F.

Rajani, “Gedi: Generative discriminator guided sequence generation,” arXiv preprint

arXiv:2009.06367, 2020.

[145] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and

R. Liu, “Plug and play language models: A simple approach to controlled text

generation,” in 8th International Conference on Learning Representations, ICLR 2020,

Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [Online]. Available:

https://openreview.net/forum?id=H1edEyBKDS

[146] Z. Lin, A. Madotto, and P. Fung, “Exploring versatile generative language model via

parameter-efﬁcient transfer learning,” in Proceedings of the 2020 Conference on Empir-

ical Methods in Natural Language Processing: Findings, 2020, pp. 441–459.

[147] E. M. Smith, D. Gonzalez-Rico, E. Dinan, and Y.-L. Boureau, “Controlling style in gen-

erated dialogue,” arXiv preprint arXiv:2009.10855, 2020.

[148] C. de Masson d’Autume, S. Ruder, L. Kong, and D. Yogatama, “Episodic memory

in lifelong language learning,” in Advances in Neural Information Processing Systems

32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS

2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle,

A. Beygelzimer, F. d’Alché-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp.

13 122–13 131. [Online]. Available:

https://proceedings.neurips.cc/paper/2019/hash/

f8d2e80c1458ea2501f98a2cafadb397-Abstract.html

84

[149] P. Sprechmann, S. M. Jayakumar, J. W. Rae, A. Pritzel, A. P. Badia, B. Uria, O. Vinyals,

D. Hassabis, R. Pascanu, and C. Blundell, “Memory-based parameter adaptation,” in

6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC,

Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,

2018. [Online]. Available: https://openreview.net/forum?id=rkfOvGbCW

[150] Z. Wang, S. V. Mehta, B. Poczos, and J. G. Carbonell, “Efﬁcient meta lifelong-learning

with limited memory,” in Proceedings of the 2020 Conference on Empirical Methods in

Natural Language Processing (EMNLP), 2020, pp. 535–548.

[151] N. Lee, A. Madotto, Y. Bang, and P. Fung, “Dynamically addressing unseen rumor via

continual learning,” arXiv preprint arXiv:2104.08775, 2021.

[152] F.-K. Sun, C.-H. Ho, and H.-Y. Lee, “Lamol: Language modeling for lifelong language

learning,” in International Conference on Learning Representations, 2019.

[153] H. Hu, O. Sener, F. Sha, and V. Koltun, “Drinking from a ﬁrehose: Continual learning

with web-scale natural language,” arXiv preprint arXiv:2007.09335, 2020.

[154] Y. Chuang, S. Su, and Y. Chen, “Lifelong language knowledge distillation,” in

Proceedings of the 2020 Conference on Empirical Methods in Natural Language

Processing, EMNLP 2020, Online, November 16-20, 2020, B. Webber, T. Cohn, Y. He,

and Y. Liu, Eds. Association for Computational Linguistics, 2020, pp. 2914–2924.

[Online]. Available: https://doi.org/10.18653/v1/2020.emnlp-main.233

[155] B. McCann, N. S. Keskar, C. Xiong, and R. Socher, “The natural language decathlon:

Multitask learning as question answering,” arXiv preprint arXiv:1806.08730, 2018.

[156] T. Liu, L. Ungar, and J. Sedoc, “Continual learning for sentence representations using

conceptors,” in Proceedings of the 2019 Conference of the North American Chapter

of the Association for Computational Linguistics: Human Language Technologies,

Volume 1 (Long and Short Papers). Minneapolis, Minnesota:

Association

for Computational Linguistics,

Jun. 2019, pp. 3274–3279.

[Online]. Available:

https://www.aclweb.org/anthology/N19-1331

[157] Y. Li, L. Zhao, K. Church, and M. Elhoseiny, “Compositional language continual learn-

ing,” in International Conference on Learning Representations, 2019.

85

[158] X. Han, Y. Dai, T. Gao, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, “Continual relation

learning via episodic memory activation and reconsolidation,” in Proceedings of the 58th

Annual Meeting of the Association for Computational Linguistics, 2020, pp. 6429–6440.

[159] N. Lee, Y. Bang, A. Madotto, M. Khabsa, and P. Fung, “Towards few-shot fact-checking

via perplexity,” arXiv preprint arXiv:2103.09535, 2021.

[160] S. Lee, “Toward continual

learning for conversational agents,” arXiv preprint

arXiv:1712.09943, 2017.

[161] F. Mi, L. Chen, M. Zhao, M. Huang, and B. Faltings, “Continual learning for natural

language generation in task-oriented dialog systems,” in Proceedings of the 2020 Con-

ference on Empirical Methods in Natural Language Processing: Findings, 2020, pp.

3461–3474.

[162] L. Li, Z. He, X. Zhou, and D. Yu, “How to evaluate the next system: Automatic dialogue

evaluation from the perspective of continual learning,” arXiv preprint arXiv:1912.04664,

2019.

[163] T. He, J. Liu, K. Cho, M. Ott, B. Liu, J. Glass, and F. Peng, “Mix-review: Alleviate

forgetting in the pretrain-ﬁnetune framework for neural language generation models,”

arXiv preprint arXiv:1910.07117, 2019.

[164] K. Shuster, J. Urbanek, E. Dinan, A. Szlam, and J. Weston, “Deploying lifelong open-

domain dialogue learning,” arXiv preprint arXiv:2008.08076, 2020.

[165] A. Fan, J. Urbanek, P. Ringshia, E. Dinan, E. Qian, S. Karamcheti, S. Prabhumoye,

D. Kiela, T. Rocktäschel, A. Szlam et al., “Generating interactive worlds with text.” in

AAAI, 2020, pp. 1693–1700.

[166] M. I. Jordan and R. A. Jacobs, “Hierarchical mixtures of experts and the em algorithm,”

Neural computation, vol. 6, no. 2, pp. 181–214, 1994.

[167] V. Tresp, “Mixtures of gaussian processes,” in Advances in neural information processing

systems, 2001, pp. 654–660.

[168] B. Yao, D. Walther, D. Beck, and L. Fei-Fei, “Hierarchical mixture of classiﬁcation

experts uncovers interactions between brain regions,” in Advances in Neural Information

Processing Systems, 2009, pp. 2178–2186.

86

[169] R. Aljundi, P. Chakravarty, and T. Tuytelaars, “Expert gate: Lifelong learning with a

network of experts,” in Proceedings of the IEEE Conference on Computer Vision and

Pattern Recognition, 2017, pp. 3366–3375.

[170] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Out-

rageously large neural networks: The sparsely-gated mixture-of-experts layer,” arXiv

preprint arXiv:1701.06538, 2017.

[171] L. Kaiser, A. N. Gomez, N. Shazeer, A. Vaswani, N. Parmar, L. Jones, and J. Uszkoreit,

“One model to learn them all,” arXiv, 2017.

[172] Y. Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients through

stochastic neurons for conditional computation,” arXiv preprint arXiv:1308.3432, 2013.

[173] A. S. Davis and I. Arel, “Low-rank approximations for conditional feedforward

computation in deep neural networks,” in 2nd International Conference on Learning

Representations,

ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop

Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2014. [Online]. Available:

http://arxiv.org/abs/1312.4461

[174] E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup, “Conditional computation in neural

networks for faster models,” ICLR, 2016.

[175] D. Fojo, V. Campos,

and X. Giró-i-Nieto,

“Comparing ﬁxed and adaptive

computation time for recurrent neural networks,” in 6th International Conference

on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May

3, 2018, Workshop Track Proceedings. OpenReview.net, 2018. [Online]. Available:

https://openreview.net/forum?id=SkZq3vyDf

[176] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser, “Universal transform-

ers,” ICLR, 2019.

[177] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang, D. Vetrov, and R. Salakhutdinov,

“Spatially adaptive computation time for residual networks,” in Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, 2017, pp. 1039–1048.

[178] J. Lin, Y. Rao, J. Lu, and J. Zhou, “Runtime neural pruning,” in Advances in Neural

Information Processing Systems, 2017, pp. 2181–2191.

87

[179] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, “Amc: Automl for model compres-

sion and acceleration on mobile devices,” in Proceedings of the European Conference on

Computer Vision (ECCV), 2018, pp. 784–800.

[180] C. Rosenbaum, T. Klinger, and M. Riemer, “Routing networks: Adaptive selection

of non-linear functions for multi-task learning,” in International Conference on

Learning Representations, 2018. [Online]. Available: https://openreview.net/forum?id=

ry8dvM-R-

[181] T. Wolf, V. Sanh, J. Chaumond, and C. Delangue, “Transfertransfo: A transfer learning

approach for neural network based conversational agents,” CoRR, vol. abs/1901.08149,

2019. [Online]. Available: http://arxiv.org/abs/1901.08149

[182] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault,

R. Louf, M. Funtowicz, and J. Brew, “Transformers: State-of-the-art natural language

processing,” 2019.

[183] F. Stahlberg, J. Cross, and V. Stoyanov, “Simple fusion: Return of the language model,”

in Proceedings of the Third Conference on Machine Translation: Research Papers, 2018,

pp. 204–211.

[184] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and

R. Fergus, “Intriguing properties of neural networks,” in 2nd International Conference

on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,

Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2014. [Online].

Available: http://arxiv.org/abs/1312.6199

[185] A. Bapna and O. Firat, “Simple, scalable adaptation for neural machine translation,” in

Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-

cessing and the 9th International Joint Conference on Natural Language Processing

(EMNLP-IJCNLP), 2019, pp. 1538–1548.

[186] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv preprint

arXiv:1607.06450, 2016.

[187] G. E. Hinton and R. S. Zemel, “Autoencoders, minimum description length and

88

helmholtz free energy,” in Advances in neural information processing systems, 1994,

pp. 3–10.

[188] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts, “Re-

cursive deep models for semantic compositionality over a sentiment treebank,” in Pro-

ceedings of the 2013 conference on empirical methods in natural language processing,

2013, pp. 1631–1642.

[189] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks for text clas-

siﬁcation,” in Advances in neural information processing systems, 2015, pp. 649–657.

[190] H. Zhang, D. Duckworth, D. Ippolito, and A. Neelakantan, “Trading off diversity and

quality in natural language generation,” arXiv preprint arXiv:2004.10450, 2020.

[191] C.-W. Liu, R. Lowe, I. V. Serban, M. Noseworthy, L. Charlin, and J. Pineau, “How not

to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics

for dialogue response generation,” in Proceedings of the 2016 Conference on Empirical

Methods in Natural Language Processing, 2016, pp. 2122–2132.

[192] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic

evaluation of machine translation,” in Proceedings of the 40th Annual Meeting of

the Association for Computational Linguistics.

Philadelphia, Pennsylvania, USA:

Association for Computational Linguistics, Jul. 2002, pp. 311–318. [Online]. Available:

https://www.aclweb.org/anthology/P02-1040

[193] J. McAuley and J. Leskovec, “Hidden factors and hidden topics: understanding rating di-

mensions with review text,” in Proceedings of the 7th ACM conference on Recommender

systems, 2013, pp. 165–172.

[194] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirec-

tional transformers for language understanding,” in Proceedings of the 2019 Conference

of the North American Chapter of the Association for Computational Linguistics: Human

Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 4171–4186.

[195] M. Li, J. Weston, and S. Roller, “Acute-eval: Improved dialogue evaluation with opti-

mized questions and multi-turn comparisons,” arXiv preprint arXiv:1909.03087, 2019.

[196] S. Thrun and L. Pratt, Learning to learn. Springer Science & Business Media, 2012.

89

[197] M. McCloskey and N. J. Cohen, “Catastrophic interference in connectionist networks:

The sequential learning problem,” in Psychology of learning and motivation. Elsevier,

1989, vol. 24, pp. 109–165.

[198] W. Lei, X. Jin, M.-Y. Kan, Z. Ren, X. He, and D. Yin, “Sequicity: Simplifying task-

oriented dialogue systems with single sequence-to-sequence architectures,” in Proceed-

ings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-

ume 1: Long Papers), 2018, pp. 1437–1447.

[199] B. Byrne, K. Krishnamoorthi, S. Ganesh, and M. S. Kale, “Tickettalk: Toward human-

level performance with end-to-end, transaction-based dialog systems,” arXiv preprint

arXiv:2012.12458, 2020.

[200] B. Peng, C. Li, J. Li, S. Shayandeh, L. Liden, and J. Gao, “Soloist: Few-shot

task-oriented dialog with a single pre-trained auto-regressive model,” arXiv preprint

arXiv:2005.05298, 2020.

[201] E. Hosseini-Asl, B. McCann, C. Wu, S. Yavuz, and R. Socher, “A simple language

model for task-oriented dialogue,” in Advances in Neural Information Processing

Systems 33: Annual Conference on Neural Information Processing Systems 2020,

NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell,

M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips.cc/

paper/2020/hash/e946209592563be0f01c844ab2170f0c-Abstract.html

[202] M. Wortsman, V. Ramanujan, R. Liu, A. Kembhavi, M. Rastegari, J. Yosinski, and

A. Farhadi, “Supermasks in superposition,” Advances in Neural Information Processing

Systems, vol. 33, 2020.

[203] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Mi-

lan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al., “Overcoming catastrophic for-

getting in neural networks,” Proceedings of the national academy of sciences, vol. 114,

no. 13, pp. 3521–3526, 2017.

[204] A. Robins, “Catastrophic forgetting, rehearsal and pseudorehearsal,” Connection Sci-

ence, vol. 7, no. 2, pp. 123–146, 1995.

90

[205] D. Lopez-Paz and M. Ranzato, “Gradient episodic memory for continual learning,” in

Advances in neural information processing systems, 2017, pp. 6467–6476.

[206] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny, “Efﬁcient lifelong learning

with A-GEM,” in 7th International Conference on Learning Representations, ICLR

2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [Online].

Available: https://openreview.net/forum?id=Hkf2_sC5FX

[207] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu,

R. Pascanu,

and R. Hadsell,

“Progressive neural networks,” arXiv preprint

arXiv:1606.04671, 2016.

[208] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, “Lifelong learning with dynamically

expandable networks,” in 6th International Conference on Learning Representations,

ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track

Proceedings. OpenReview.net, 2018. [Online]. Available:

https://openreview.net/

forum?id=Sk7KsfW0-

[209] X. Li, Y. Zhou, T. Wu, R. Socher, and C. Xiong, “Learn to grow: A continual structure

learning framework for overcoming catastrophic forgetting,” in Proceedings of the 36th

International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long

Beach, California, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri

and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 3925–3934. [Online]. Available:

http://proceedings.mlr.press/v97/li19m.html

[210] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and

D. Wierstra, “Pathnet: Evolution channels gradient descent in super neural networks,”

arXiv preprint arXiv:1701.08734, 2017.

[211] A. Mallya, D. Davis, and S. Lazebnik, “Piggyback: Adapting a single network to mul-

tiple tasks by learning to mask weights,” in Proceedings of the European Conference on

Computer Vision (ECCV), 2018, pp. 67–82.

[212] B. Byrne, K. Krishnamoorthi, C. Sankar, A. Neelakantan, D. Duckworth, S. Yavuz,

B. Goodrich, A. Dubey, K.-Y. Kim, and A. Cedilnik, “Taskmaster-1:toward a realistic

91

and diverse dialog dataset,” in 2019 Conference on Empirical Methods in Natural Lan-

guage Processing and 9th International Joint Conference on Natural Language Process-

ing, Hong Kong, 2019.

[213] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan, “Towards scalable multi-

domain conversational agents: The schema-guided dialogue dataset,” in Proceedings of

the AAAI Conference on Artiﬁcial Intelligence, vol. 34, no. 05, 2020, pp. 8689–8696.

[214] P. Budzianowski, T.-H. Wen, B.-H. Tseng, I. Casanueva, U. Stefan, R. Osman, and

M. Gaši´c, “Multiwoz - a large-scale multi-domain wizard-of-oz dataset for task-oriented

dialogue modelling,” in Proceedings of the 2018 Conference on Empirical Methods in

Natural Language Processing (EMNLP), 2018.

[215] M. Kale and A. Rastogi, “Few-shot natural language generation by rewriting templates,”

arXiv preprint arXiv:2004.15006, 2020.

[216] M. Eric, L. Krishnan, F. Charette, and C. D. Manning, “Key-value retrieval networks

for task-oriented dialogue,” in Proceedings of the 18th Annual SIGdial Meeting on

Discourse and Dialogue. Association for Computational Linguistics, 2017, pp. 37–49.

[Online]. Available: http://aclweb.org/anthology/W17-5506

[217] P. Budzianowski, T.-H. Wen, B.-H. Tseng, I. Casanueva, S. Ultes, O. Ramadan, and

M. Gasic, “Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented

dialogue modelling,” in Proceedings of the 2018 Conference on Empirical Methods in

Natural Language Processing, 2018, pp. 5016–5026.

[218] J. Schmidhuber, “Evolutionary principles in self-referential learning. on learning now to

learn: The meta-meta-meta...-hook,” Diploma Thesis, Technische Universitat Munchen,

Germany, 14 May 1987. [Online]. Available: http://www.idsia.ch/~juergen/diploma.html

[219] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based

neural machine translation,” in Proceedings of the 2015 Conference on Empirical

Methods in Natural Language Processing.

Lisbon, Portugal: Association for

Computational Linguistics, September 2015, pp. 1412–1421. [Online]. Available:

http://aclweb.org/anthology/D15-1166

92

[220] A. Miller, A. Fisch, J. Dodge, A.-H. Karimi, A. Bordes, and J. Weston, “Key-value mem-

ory networks for directly reading documents,” in Proceedings of the 2016 Conference on

Empirical Methods in Natural Language Processing, 2016, pp. 1400–1409.

[221] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic

evaluation of machine translation,” in Proceedings of 40th Annual Meeting of

the Association for Computational Linguistics.

Philadelphia, Pennsylvania, USA:

Association for Computational Linguistics, July 2002, pp. 311–318. [Online]. Available:

http://www.aclweb.org/anthology/P02-1040

[222] S. Welleck, J. Weston, A. Szlam, and K. Cho, “Dialogue natural language inference,” in

Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-

tics, 2019, pp. 3731–3741.

[223] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization with

pointer-generator networks,” in Proceedings of

the 55th Annual Meeting of

the

Association for Computational Linguistics (Volume 1: Long Papers). Vancouver,

Canada: Association for Computational Linguistics, July 2017, pp. 1073–1083.

[Online]. Available: http://aclweb.org/anthology/P17-1099

93

