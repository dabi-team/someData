1
2
0
2

r
a

M
6

]

G
L
.
s
c
[

2
v
2
0
3
1
0
.
1
1
0
2
:
v
i
X
r
a

IOS: INTER-OPERATOR SCHEDULER FOR CNN ACCELERATION

Yaoyao Ding* 1 2 Ligeng Zhu 3 Zhihao Jia 4 Gennady Pekhimenko 1 2 Song Han 3

ABSTRACT
To accelerate CNN inference, existing deep learning frameworks focus on optimizing intra-operator parallelization.
However, a single operator can no longer fully utilize the available parallelism given the rapid advances in high-
performance hardware, resulting in a large gap between the peak performance and the real performance. This
performance gap is more severe under smaller batch sizes. In this work, we extensively study the parallelism
between operators and propose Inter-Operator Scheduler (IOS) to automatically schedule multiple operators’
parallel execution through a novel dynamic programming algorithm. IOS consistently outperforms state-of-the-art
libraries (e.g., TensorRT) by 1.1 to 1.5× on modern CNN benchmarks. The code to reproduce each experiment is
available at: https://github.com/mit-han-lab/inter-operator-scheduler.

1

INTRODUCTION

Convolutional neural networks (CNNs) have achieved state-
of-the-art performance across many tasks, including com-
puter vision (Krizhevsky et al., 2012; He et al., 2016), ma-
chine translation (Sutskever et al., 2014; Devlin et al., 2018),
and game playing (Mnih et al., 2013; Silver et al., 2016).
The success comes at the cost of growing computational
requirements. The high demand for computation makes
efﬁcient inference more critical in real deployment (Han
et al., 2015; Chen et al., 2018; Jia et al., 2019a).

A common practice to improve inference efﬁciency is par-
allelization. Deep learning frameworks such as Tensor-
ﬂow (Abadi et al., 2016) and Pytorch (Paszke et al., 2017)
exploit intra-operator parallelism, which parallelizes arith-
metic operations within a single CNN operator (e.g., con-
volution). However, due to the rapid advances in high-
performance hardware, intra-operator parallelism is no
longer sufﬁcient to obtain efﬁcient resource utilization. As
shown in Figure 1, the peak FP32 performance of a GPU
has increased from 5.8 TFLOPs/s in 2013 to 15.7 TFLOPs/s
in 2018 (shown in red). NVIDIA Tesla A100 even reaches
a peak FP32 performance of 19.5 TFLOPs/s.

Meanwhile, there is a recent trend in CNN design to replace
a single branch of convolutions with multiple branches of
convolutions, which is advantageous due to increased model
capacity under a ﬁxed computation budget (Szegedy et al.,

*Work done while interning at MIT HAN Lab. 1University
of Toronto 2Vector Institute 3Massachusetts Institute of Technol-
ogy 4Carnegie Mellon University. Correspondence to: Song Han
<songhan@mit.edu>.

Proceedings of the 4 th MLSys Conference, San Jose, CA, USA,
2021. Copyright 2021 by the author(s).

Figure 1. The trends of average computation per convolution, num-
ber of convolutions in a CNN and hardware peak performance.
Device peek performance increases while average computation
per convolution decreases, leading to a larger utilization gap. VG-
GNet and GTX 980Ti, Inception V3, and GTX 1080, NASNet and
Tesla V100 are chosen as representatives for 2013, 2015, and 2018
respectively. All FLOPs are measured for single precision.

2016; Zoph et al., 2018; Xie et al., 2019). As a result,
the number of convolutions grows while the computation
FLOPs in each convolution becomes smaller. For example,
the average ﬂoating-point operations (FLOPs) per convolu-
tion has decreased from 2330 MFLOPs/kernel in VGG to 82
MFLOPs/kernel in NASNet. This exacerbates the device’s
under-utilization problem.

To address this problem, recent work explores inter-operator
parallelism by executing multiple CNN operators in parallel
guided by different heuristics (Tang et al., 2018; Jia et al.,
2019b; Ma et al., 2020). For example, MetaFlow (Jia et al.,
2019b) fuses multiple operators matching a speciﬁc pattern
into a larger operator to increase operator granularity. Tang
et al. (Tang et al., 2018) proposes a greedy strategy that
directly executes all available CNN operators on CPU to
maximize resource utilization. These approaches apply
different heuristics to optimize local parallelization across a

11001000020132015201815700842557675359416821162330Average FLOPs per CONV (MFLOPs)Number of CONVGPU Peak Performance (GFLOPs / s) 
 
 
 
 
 
IOS: Inter-Operator Scheduler for CNN Acceleration

Figure 2. Different execution schedules for a computation graph on NVIDIA Tesla V100 GPU. Operators scheduled to run in parallel
are placed at the same level between two dotted lines called a stage. Computation (GFLOPs), performance (TFLOPs/s), and hardware
utilization (%) for each stage are proﬁled on the right. Both sequential and greedy schedules result in low resource utilization (48%-62%)
and high latency (0.37-0.48ms). Our schedule yields higher utilization (70%) and lower latency (0.33ms).

few CNN operators; however, such techniques do not lead to
a globally optimal schedule for the entire CNN architecture.
For example, given an input CNN (Figure 2 (1)), a greedy
schedule (Figure 2 (2)) would perform convolutions [a], [c],
and [d] in parallel, and run convolution [b] in a subsequent
stage upon the completion of the previous stage.

This greedy schedule is sub-optimal for two reasons. First,
a greedy schedule eagerly puts more operators in the early
stages (as soon as they are available for execution) and fewer
operators in subsequent stages, resulting in low utilization
in later stages. Second, executing too many operators on
the device concurrently may lead to resource contention
problem that hurts the performance. For example, as shown
in Figure 2, the greedy schedule (2) suffers from resource
contention problem in the ﬁrst stage and low-utilization
problem in the second stage, comparing to our proposed
schedule (3).

Obtaining an optimized schedule to parallelize a CNN
model is a challenging task. On the one hand, the num-
ber of schedules grows exponentially with the number of
operators, making it infeasible to evaluate all possible sched-
ules exhaustively. For example, a network with 33 operators
can have 9.2 × 1022 number of feasible schedules. On the
other hand, an optimal schedule also depends on hardware
speciﬁcations and inference settings (e.g., batch size). A
high-end GPU (e.g., Tesla V100) can efﬁciently execute a
schedule with many operators in parallel, while a low-end
GPU (e.g., Tesla K80) might suffer from resource contention
using the same schedule. A large batch size naturally offers
more intra-operator parallelism, while a small batch size has
a stronger need for inter-operator parallelization. Therefore,
given a diverse set of CNN architectures, hardware, and
inference settings, it is hard to devise an efﬁcient schedule

manually for all scenarios.

To address this challenge, we propose IOS, an inter-operator
scheduler that accelerates CNN inference by combining
intra- and inter-operator parallelism. We observe that dif-
ferent schedules share common sub-schedules; thus, IOS
adopts a dynamic programming technique to explore the
schedule space and ﬁnds a highly optimized schedule under
low search cost. We evaluate IOS on modern CNN mod-
els, including Inception-V3 (Szegedy et al., 2016), Rand-
Wire (Xie et al., 2019), NasNet-A (Zoph et al., 2018), and
SqueezeNet (Iandola et al., 2016). IOS consistently outper-
forms the sequential schedule and greedy schedule. IOS
achieves 1.1 to 1.5× inference speedup compared to exist-
ing deep learning libraries (e.g., TensorRT). Furthermore,
IOS demonstrates the necessity of customizing the schedul-
ing policy for different hardware and inference conﬁgura-
tions. IOS can achieve up to 1.15× inference speedup by
customizing the scheduling recipe compared to itself with
no customization.

Our contributions are summarized as follows:

• We point out a major bottleneck for efﬁcient CNN in-
ference: existing intra-operator parallelism cannot sat-
urate modern hardware’s high parallelism, especially
for recent multi-branch CNN models. Inter-operator
parallelism is crucial.

• We propose a novel dynamic programming algorithm
to ﬁnd a highly optimized schedule for inter-operator
parallelization. This technique is platform-agnostic
and can serve as a general technique for popular frame-
works such as TensorFlow (Abadi et al., 2015) and
TVM (Chen et al., 2018).

Concat1920Input384Conv [a]3x3x384Conv [b]3x3x768Conv [c]3x3x384Conv [d]3x3x768Concat1920Input384Conv [a]3x3x384 Conv [b]3x3x768Conv [c]3x3x384Conv [d]3x3x768Concat1920Input384Conv [a]3x3x384Conv [b]3x3x768 Conv [c]3x3x384Conv [d]3x3x7680 ms0.12 ms0.24 ms0.35 ms0.48 ms0 ms0.24 ms0.37 ms0 ms0.17 ms0.33 ms5.2TFLOPs/s9.3 TFLOPs/s9.3 TFLOPs/s5.5 TFLOPs/s9.8 TFLOPs/s9.4 TFLOPs/s10.8 TFLOPs/s11.1 TFLOPs/s(1) Sequential Schedule0.6GFLOPs1.2GFLOPs(2) Greedy Schedule(3)  Our Schedule (IOS)0.6GFLOPs1.2GFLOPs33%Util.59%Util.35%Util.59%Util.2.4GFLOPs0.6GFLOPs62%Util.60%Util.1.8GFLOPs1.8GFLOPs69%Util.71%Util.Avg. util: 48% Avg. util: 62%          Avg. util: 70%   (14%↑)(22%↑)IOS: Inter-Operator Scheduler for CNN Acceleration

• We apply IOS to various hardware and inference set-
tings and show that the different conﬁgurations require
different schedules. We can automatically customize
the scheduling policy for different hardware and infer-
ence conﬁgurations. The specialized schedules consis-
tently outperform existing deep learning libraries with
1.1 to 1.5× measured speedup in inference.

2 BACKGROUND AND RELATED WORK

CNN Design. Several lightweight design primitives have
been recently introduced to improve the efﬁciency of CNNs.
Examples include SequeezeNet (Iandola et al., 2016), Mo-
bileNet (Sandler et al., 2018) and ShufﬂetNet (Zhang et al.,
2018). However, such design patterns cannot fully utilize
the hardware. Hardware under-utilization becomes more
severe as accelerators are getting more powerful (shown
in Figure 1). On the other hand, multi-branch CNNs be-
come a trend in model architecture design, including both
manually designed networks (Szegedy et al., 2015; Iandola
et al., 2016; Szegedy et al., 2016) and the networks discov-
ered by neural architecture search (Cai et al., 2018; Zoph
et al., 2018). With a ﬁxed computation budget, multi-branch
CNNs use more small convolution primitives, which further
ampliﬁes the resource under-utilization problem on modern
hardware.

Intra-operator Parallelism. Current deep learning frame-
works (e.g., TensorFlow and PyTorch) generally focus on
intra-operator parallelism, which executes arithmetic opera-
tions within a single operator in parallel (e.g., tiled matrix
multiplication). Tensorﬂow and PyTorch are built upon
vendor-provided libraries (e.g., cuDNN), a set of DNN com-
pute primitives heavily optimized by vendor engineers to
achieve near-peak machine performance. However, these
DNN operators are executed sequentially on a hardware de-
vice. The degree of parallelism within an operator is limited;
thus, intra-operator parallelism cannot provide sufﬁcient par-
allelizable computation to feed powerful hardware devices.
As a result, the hardware is often under-utilized using these
frameworks.

Different
from manual performance tuning, Auto-
Halide (Mullapudi et al., 2016), TVM (Chen et al., 2018)
and Ansor (Zheng et al., 2020) exploit intra-parallelism
through automatically learning efﬁcient schedule for indi-
vidual DNN kernels. This automation saves a large amount
of engineering effort and can generate more efﬁcient DNN
kernels than the manually designed counterparts. However,
still, all these libraries only focus on intra-operator paral-
lelism but do not exploit inter-operator parallelism.

Inter-Operator Scheduling. Recent work has explored
inter-operator scheduling. Tang et al. (Tang et al., 2018)
proposes a greedy heuristic approach, Graphi, that executes
all available CNN operators whenever possible to saturate

CPU’s computation capability. The greedy strategy does not
holistically optimize the computation graph’s performance,
hence yields unbalanced and sub-optimal schedules. Ram-
mer(Ma et al., 2020) optimizes the execution of DNN work-
loads by holistically exploiting parallelism through inter-
and intra- operator co-scheduling, enabling a richer schedul-
ing space for executing a DNN model. IOS focuses on
the inter-operator scheduling and leaves the intra-operator
scheduling to the hardware. Nimble(Kwon et al., 2020)
is a DNN engine that supports parallel execution of DNN
operators on GPU and minimizes the scheduling overhead
using ahead-of-time (AOT) scheduling. The scheduling al-
gorithm used in Nimble does not consider the latency of
each operator, while IOS is a proﬁle-based scheduler.

Graph transformation. MetaFlow (Jia et al., 2019b) per-
forms functional-preserving graph transformations to opti-
mize DNN architectures. Merging operators with the same
input enables more parallelism (a larger operator compared
to two small sequential operators) and reduces accesses to
GPU memories. TASO (Jia et al., 2019a) further introduces
an automated generation of substitution rules and it explores
more mathematically equivalent DNN architectures of the
input one comparing to MetaFlow. MetaFlow and TASO
consider the whole computation graph and search for highly
optimized substitution strategies. However, the inter-oprator
parallelism utilized by MetaFlow and TASO is still limited
as only the same type of operators can be merged.

To address the large schedule space problem, IOS utilizes
dynamic programming to take advantage of the common
sub-schedules among different schedules. Also, IOS sup-
ports concurrent execution of different types of operators,
addressing the limitation of MetaFlow and TASO.

3 PROBLEM DEFINITION

This section deﬁnes the schedule in IOS and formulates the
problem.

Computation Graph. A CNN is deﬁned by a computation
graph G = (V, E), where V is the set of operators, and E
is the edge set representing dependencies. A computation
graph is a directed acyclic graph (DAG). Each operator in
the graph represents an operator such as convolution and
matrix multiplication. Each edge (u, v) is a tensor that is an
output of operator u, and an input of operator v. Figure 3
(1) shows the computation graph of a simple CNN.

Stage. To take advantage of inter-operator parallelism in
a CNN architecture, its computation graph is partitioned
into multiple stages. Stages are executed sequentially and
the operators in the same stage are executed according to
a certain parallelization strategy (see below). Figure 3 (2)
shows a possible schedule that partitions the input graph
into two stages, where the ﬁrst stage contains operator a and

IOS: Inter-Operator Scheduler for CNN Acceleration

partitioned into two groups. The ﬁrst group contains op-
erators Conv[c] and Conv[d] while the second group
contains operator Matmul[e]. The two groups are ex-
ecuted concurrently while Conv[c] and Conv[d] are
executed sequentially in their group.

Schedule. We deﬁne a schedule Q of a computation graph
G as Q = {(S1, T1), (S2, T2), . . . , (Sk, Tk)}, where Si
is the set of operators in the ith stage and Ti is the cor-
responding parallelization strategy, either “concurrent ex-
ecution” or “operator merge”. For example, the sched-
ule for Figure 3 (2) is: Q = {({a, b}, operator merge),
({c, d, e}, concurrent execution)}. The schedule Q exe-
cutes the network from the ﬁrst stage (S1, T1) to the last
stage (Sk, Tk) sequentially. Si may contain only one oper-
ator if it is the best choice (e.g., a very large operator that
saturates the entire GPU).

Problem Formulation. Let c be a cost function deﬁned on
a computation graph G and schedule Q. We aim to ﬁnd
a schedule Q∗ to minimize the cost function for a given
computation graph G, i.e., Q∗ = argminQc(G, Q). In this
work, the cost function c(G, Q) is deﬁned as the latency of
running G following schedule Q.

Figure 3. For a given computation graph (left), a possible schedule
is shown to the right. There are ﬁve operators in the graph: con-
volutions a-d and matrix multiplication e. The schedule partitions
operators into 2 stages. The ﬁrst stage merges convolution a and
b into a larger convolution; this parallelization strategy is named
operator merge. The second stage partitions operator c, d and e
into two groups, {c, d} and {e}. The operators in the same group
are executed sequentially while different groups in the same stage
are executed concurrently. This parallelization strategy is named
concurrent execution. Stages are executed one-by-one.

b, and the second stage contains operator c, d, and e. The
parallelization strategy is discussed below.

4 METHODS

Parallelization Strategy. Each stage adopts one of the
following two parallelization strategies: operator merge
and concurrent execution. ISO considers both of them and
automatically picks the more efﬁcient one for each stage.
The choice depends on operator types, input tensor shapes,
and the hardware device to perform CNN computations.

To be eligible for operator merge, the operators’ type must
be the same while the hyperparameters can be different. For
example, two convolutions with the same stride but different
kernel sizes can be merged. The smaller kernel will be
padded with zeros to ﬁt the large kernel, so we can stack
their kernels together. In Figure 3 (1), if Conv[a] has 128
3x3 kernels while Conv[b] has 256 3x3 kernels, we can
stack their kernels together and replace Conv[a] and [b]
by a Merged Conv[a&b] with 384 3x3 kernels. Besides
increasing parallelism, it also reduces the memory accesses
to the input tensor from twice to only once. A split operator
is required to partition the merged convolution’s output to
recover the original outputs of Conv[a] and Conv[b].

Under concurrent execution, the operators in the stage are
partitioned into disjoint groups. More speciﬁcally, if two
operators are connected by an edge, they are partitioned
into the same group. Different groups within the same stage
are executed concurrently, while the operators within the
same group are executed sequentially. IOS considers si-
multaneous executions of operators with different types. In
the second stage of Figure 3 (2), the three operators are

This section introduces our Inter-Operator Scheduler (IOS)
in three parts. Section 4.1 elaborates the IOS design in
details. Section 4.2 analyzes the time complexity of IOS.
Finally, Section 4.3 introduces the pruning optimizations to
reduce the search time of IOS.

4.1

Inter-Operator Scheduler (IOS)

Figure 4. The illustration of ending. (1) shows all the operators V .
S(cid:48) in (2) is an ending of V . However, S(cid:48) in (3) is not an ending
of V because there is an edge from d to g (from S(cid:48) to V − S(cid:48)).
We can partition a graph by selecting an ending for remaining
operators recursively, as shown in (4), where S(cid:48)
1 is an ending of V
while S(cid:48)

2 is an ending of V − S(cid:48)
1.

To ﬁnd an optimized schedule for a CNN architecture, we
ﬁrst partition its computation graph G = (V, E) into V − S(cid:48)
and S(cid:48), where all edges between V − S(cid:48) and S(cid:48) start from
V − S(cid:48) and end in S(cid:48). Such S(cid:48) is called an ending of V ,
as illustrated in Figure 4. There can be many endings of
V . The last stage’s operators in V ’s optimal schedule must

Conv [c]Matmul [e] Conv [d]Conv [a]Conv [b]Conv [c]Matmul [e] Conv [d]Merged Conv  [a & b]splitStage 1(Operator Merge)Stage 2(Concurrent Execution)Schedule Q = [     {a, b} “operator merge”,                          {c, d, e} “concurrent execution”](1) Computation Graph(2) A Feasible Schedule QInput / OutputGroup 1Group 2abcfgedabcfgedabcfgedVS’S’(1) Operators V(2) S’ is an ending of V(3) S’ is not an ending of Vabcfged(4) Partition graph by endings recursivelyV-S’₁S’₂S’₃S’₁IOS: Inter-Operator Scheduler for CNN Acceleration

be an ending of V . We can enumerate the ending S(cid:48) of V
and convert the original problem to a sub-problem that ﬁnds
the optimal schedule for V − S(cid:48). The whole graph can be
scheduled by applying the partition recursively.

Let cost[S] be the latency of an optimal schedule for S. Let
stage latency[S(cid:48)] be the latency of stage (S(cid:48), T ) where T
is the better parallelization strategy for S(cid:48) among the two
possible ones. We formalize this idea as follows,

cost[S] = min
S(cid:48)

(cost[S − S(cid:48)] + stage latency[S(cid:48)]),

where S(cid:48) is an ending of S, and cost[∅] = 0. Finally,
cost[V ] is the latency of an optimal schedule for the entire
computation graph G. To construct the optimal schedule we
found, we record the corresponding S(cid:48) that minimizes the
latency for each S (i.e., cost[S]) in choice[S].

With this general idea, we implement IOS in three func-
tions INTEROPERATORSCHEDULER (L3-12), SCHEDULER
(L13-22) and GENERATESTAGE (L23-33), as shown in Al-
gorithm 1. INTEROPERATORSCHEDULER takes a compu-
tation graph as an input and returns the optimal schedule
found by IOS. SCHEDULER is a recursive function imple-
menting the dynamic programming algorithm to ﬁnd the
optimal schedule for a subset of operators in G. GENERAT-
ESTAGE chooses a better parallelization strategy for given
operators S(cid:48).

INTEROPERATORSCHEDULER (L3-12) is the entry func-
tion. It takes a computation graph G as an input and returns
an optimized schedule Q. This function calls SCHEDULER
with operators V as an argument (L5). After calling SCHED-
ULER, the global variable cost[S] stores the latency of an
optimal schedule for S, while choice[S] stores the last stage
in the corresponding optimal schedule. Once choice[·] is ob-
tained, we can construct the schedule found by IOS (L6-11).
We start with an empty list as the initial state of our schedule
(L6) and let S be all the operators in G. We inquire about
the last stage (S(cid:48), T ) of S by choice[S] and put it at the
head of the current schedule Q. We repeat this process by
letting S = S − S(cid:48) to get the remaining operators’ schedule
in all previous stages (L8-11). S = ∅ indicates that we
have discovered an optimized schedule Q for G.

SCHEDULER (L13-22) is the core part of our algorithm. It
implements the dynamic programming algorithm recursivly,
taking a subset of V as the state. It takes a set of operators
S as an input and returns the minimal latency for S among
all schedules. Because SCHEDULER may be called multiple
times with the same argument S, for repeated calls, we cache
the previous results cost[S] to avoid redundant computations
(L14-15). To ﬁnd an optimal schedule for S, we enumerate
its last stage operators S(cid:48) and reduce the problem into a sub-
problem for S − S(cid:48) (L16-21). We use GENERATESTAGE
to choose a better parallelization strategy TS(cid:48) for S(cid:48) and
get the latency LS(cid:48) (L17). LS is the minimal latency for

Algorithm 1 Inter-Operator Scheduler (IOS)
Input: a computation graph G = (V, E),
and a schedule pruning strategy P

Output: a schedule found by IOS

if cost[S] (cid:54)= ∞ then
return cost[S]

V = all operators in computation graph G
SCHEDULER(V )
Q = empty list
S = V
while S (cid:54)= ∅ do

1: Let cost[S] = ∞ for all S ⊆ V but cost[∅] = 0
2: Let choice[S] = ∅ for all S ⊆ V
3: function INTEROPEATORSCHEDULER(G)
4:
5:
6:
7:
8:
S(cid:48), T = choice[S]
9:
Insert stage (S(cid:48), T ) before the head of Q
10:
S = S − S(cid:48)
11:
return the schedule Q
12:
13: function SCHEDULER(S)
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: function GENERATESTAGE(S(cid:48))
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:

LS(cid:48) , TS(cid:48) = GENERATESTAGE(S(cid:48))
LS = SCHEDULER(S − S(cid:48)) +LS(cid:48)
if LS < cost[S] then
cost[S] = LS
choice[S] = (S(cid:48), TS(cid:48) )

Lmerge = latency of merged operator

return Lmerge, “operator merge”

if Lconcurrent < Lmerge then

return cost[S]

Lmerge = ∞

else

else

return Lconcurrent, “concurrent execution”

Partition S(cid:48) into disjoint groups: S(cid:48)
Lconcurrent = latency of parallel execution of {S(cid:48)
i}
if operators in S(cid:48) can be merged then

2, . . . , S(cid:48)
k.

1, S(cid:48)

for all ending S(cid:48) of S satisfying pruning strategy P do

S when taking S(cid:48) as the last stage’s operators (L18). We
enumerate all possible endings of S and record the minimal
latency LS and the corresponding last stage (S(cid:48), TS(cid:48)) in
cost[S] and choice[S], respectively (L19-21).

GENERATESTAGE (L23-33) chooses a better paralleliza-
tion strategy from “concurrent execution” and “operator
merge” for a given stage S(cid:48). It returns the parallelization
strategy and the corresponding latency. It directly measures
the latencies of both parallelization strategies on the hard-
ware. The “concurrent execution” strategy partitions S(cid:48) into
multiple disjoint operator groups: S(cid:48)
k. Operators
in different groups are executed concurrently while opera-
tors in the same group are executed sequentially. For the
“operator merge” strategy, if all the operators in S(cid:48) can be
merged into a single operator (L26), we merge them and
measure the latency of the merged operator (L27). Other-
wise, we set Lmerge to inﬁnity to force ourselves to choose
the “concurrent execution” strategy.

2, ..., S(cid:48)

1, S(cid:48)

Figure 5 demonstrates how IOS discovers an optimized

IOS: Inter-Operator Scheduler for CNN Acceleration

Figure 5. An example to illustrate how IOS ﬁnds the schedule. The computation graph to be optimized is shown in (1). It has three
operators, a, b, and c, where a is followed by b, and c is independent with a and b. The states and transitions between these states
are presented in (2). Here state means the operators to be scheduled, and transition means the dependency between states (edges in
(2)). Any path from state S = {a, b, c} to S = {} is corresponded with a schedule. Upon ﬁnishing the dynamic programming process
(SCHEDULER), the best schedule for the computation graph can be constructed according to choice[·], as shown in (3). The schedule
found by IOS is shown in (4). For simplicity, in this example, we only consider the concurrent execution parallelization strategy.

strategy for an input graph with three operators a, b, and
c. Figure 5 (2) shows the dynamic programming process,
the SCHEDULER in Algorithm 1. For simplicity, we only
consider the concurrent execution parallelization strategy.
There are six states (the operators to be scheduled, S) in
the process. We start with all the operators in the compu-
tation graph as state S = {a, b, c} (L5). For each state S,
SCHEDULER enumerates the ending S(cid:48) of S. The latency of
S contains two parts: latency of S(cid:48) as a stage and the latency
of S − S(cid:48). While the result of S(cid:48) is measured on the device
directly (LS(cid:48)), the optimal latency of S − S(cid:48) is obtained
via solving the sub-problem recursively. 1 to 12 shows
the computation path. Note that IOS memorizes the results
for each calculated state to avoid redundant computations.
Thus, step 7 visits state S = {a}, and IOS gets its latency
directly (L15) because it has been previously visited by step
2 . SCHEDULER stores the latency (cost[·]) and last stage
(choice[·]) in its optimal schedule. We can construct the best

schedule for the whole computation graph using choice[·],
as shown in Figure 5 (3). An optimal schedule found by IOS
is shown in (4). Both stages take “concurrent execution” as
the parallelization strategy.

4.2 Time Complexity of IOS

In this subsection, we analyze the time complexity of IOS.
We take set operations (L18, L24) and latency measurement
operations (L25, L27) as atom operations to make the anal-
ysis clear. To analyze the time complexity of IOS, we count
the number of executions of L17-21, since they dominate
the whole algorithm’s execution. This number equals the
number of edges (i.e., transitions) in Figure 5 (2). Further-
more, it is equivalent to count the number of pairs (S, S(cid:48)),
where S is a state and S(cid:48) is an ending of S. Here we deﬁne
the width of a directed acyclic graph and provide the time
complexity of Algorithm 1.

S = {a, b}Choicechoice[{a, b}]{a, b}cost[{a, b}]0.6 msS = {a}Choicechoice[{a}]{a}cost[{a}]0.4 msS = {c}Choicechoice[{c}]{c}cost[{c}]0.3 msS = {a, c}Choicechoice[{a, c}]{c}cost[{a, c}]0.7 msS = {}Choicechoice[{}]{}cost[{}]0 msS = {a, b, c}Choicechoice[{a, b, c}]{b, c}cost[{a, b, c}]0.8 msacbacabacS’ = {b}LS’ = 0.2 msLS = 0.9 msS’ = {b, c}LS’ = 0.4 msLS = 0.8 msS’ = {a, b, c}LS’ = 0.9 msLS = 0.9 msS’ = {a, b}LS’ = 0.6 msLS = 0.9 msS’ = {c}LS’= 0.3 msLS = 0.9 msS’ = {b}LS’ = 0.2 msLS = 0.6 msS’ = {a, b}LS’ = 0.6 msLS = 0.6 msS’ = {a}LS’ = 0.4 msLS = 0.7 msS’ = {c}LS’ = 0.3 msLS  = 0.7 msS’ = {a, c}LS’ = 0.7 msLS = 0.7 msS’ = {a}LS’ = 0.4 msLS = 0.4 msS’ = {c}LS’ = 0.3 msLS = 0.3 msacbacacabacb(1) Computation Graph G Optimized by IOS(2) Dynamic Programming States and Transitions(Scheduler in Algorithm 1)S = {a, b, c}Decisionchoice[{a, b, c}]{b, c}acbacbS = {a}Decisionchoice[{a}]{a}aa(3) Schedule Construction Path (InterOperatorScheduler L6-11 in Algorithm 1)(4) Schedule Found by IOSacbStage 1Stage 2opopBest choice for SBest choice for S and GA possible choiceInput/OutputOps remain to be scheduledOps in the last stage of SS = {a}Q = [{b, c}]S = {}Q = [{a}, {b, c}]Found schedule Q = [{a}, {b, c}]S’  = the enumerated ops as the last stageLS’  = the latency of stage S’ (concurrent execution)LS   = the latency of S for the current choice of S’ The operators to be scheduledThe choice ops are shown in red    choice[S]: the best choice for last stage        cost[S]: the latency of S for the best choiceS = {a, b, c}Q = []123457689101112State1~12Computation pathIOS: Inter-Operator Scheduler for CNN Acceleration

Deﬁnition 1 (Width d of a DAG). We call d the width of a
directed acyclic graph G if we can ﬁnd at most d operators
in G such that there is no path connecting any two of them.

Theorem (Time Complexity of IOS). The time complexity
of Inter-Operator Scheduler (IOS) is O((cid:0)n/d+2
), which
can be relaxed to O(( n
d + 1)2d), where n is the number of
operators in the computation graph and d is its width.

(cid:1)d

2

In fact, there are computation graphs that can reach this
bound, so we can not improve it without other restrictions
on the schedule space. Proof can be found in Appendix A.

2

(cid:1)d

Model

n d (cid:0)n/d+2
#(S, S(cid:48))
Inception V3 11 6 2.6 × 104 4.9 × 103

#Schedules
3.8 × 106
33 8 3.7 × 109 1.2 × 106 9.2 × 1022
18 8 5.2 × 106 3.1 × 105 7.2 × 1012
1.3 × 102
6

3 2.2 × 102

Randwire
NasNet
SqueezeNet

51

Table 1. For the largest block of each benchmarked network, we
list the number of operators n, the width d, the upper bound of
transitions (cid:0)n/d+2
, the real number of transitions #(S, S(cid:48)), and
number of schedules.

(cid:1)d

2

Modern convolution neural networks usually construct the
network by stacking multiple blocks, making it possible to
optimize each block separately. In this case, n and d refers
to the number of operators within a block and the block
width, rather than the full network. We list the information
of the largest block for each network benchmark in Table 1.

The total number of feasible schedules is exponential to
the number of operators (e.g., up to 9.2 × 1022 for Rand-
wire (Xie et al., 2019)). Such a huge number makes it
prohibitive to manually design or enumerate the schedules.
However, by reusing the results of common sub-schedules
in the schedule ﬁnding process, IOS ﬁnds the optimal sched-
ule within 4 hours for each network with no pruning strategy
used. The time complexity of IOS is only exponential to the
width of the computation graph, which is usually very small
and acceptable (e.g., ≤ 8 in all benchmarked networks).

4.3 Reduce the Search Time by Schedule Pruning

It is difﬁcult for a dynamic programming algorithm to stop
early, because it gets the best result at the very end. To
reduce the search time, IOS introduces schedule pruning to
reduce the exploration space by restricting the max number
of groups and the max number of operators within a group.
We deﬁne the pruning strategy P as a boolean function
of S and S(cid:48). We only enumerate the ending S(cid:48) of S that
satisﬁes the pruning strategy P , that is, P (S, S(cid:48)) = True
(L16 of Algorithm 1). The pruning strategy consists of
two parameters r and s: P (S, S(cid:48)) = True if and only if
ending S(cid:48) has at most s groups and each group has at most
r operators.

After applying the pruning strategy P , the time complexity
is reduced from O(( n
d + 1)d(r + 1)s).
Of course, there is a trade-off between the search cost and
the quality of the discovered schedule. We evaluate this
trade-off in Section 7.1.

d + 1)2d) to O(( n

5

IMPLEMENTATION SETUP

IOS is a framework-agnostic algorithm and can be imple-
mented in popular frameworks. We implement the dynamic
programming scheduling algorithm in Python and the exe-
cution engine in C++. The latency of a stage is directly
measured in the execution engine to guide the schedul-
ing. The execution engine is based on vendor-provided
library cuDNN (Chetlur et al., 2014) and supports opera-
tors’ parallel execution. To concurrently execute multiple
groups of operators, IOS puts different groups into different
CUDA streams. Kernels in different CUDA streams will
be executed in parallel if there are enough computation re-
sources. Throughout the experiments, we use cuDNN 7.6.5,
cuda 10.2, NVIDIA driver 450.51.05, and adopt TensorRT
7.0.0.11 and TVM 0.7 as baseline libraries.

Networks
Inception V3
Randwire
NasNet
SqueezeNet

#Blocks
11
3
13
10

#Operators Operator Type

119
120
374
50

Conv-Relu
Relu-SepConv
Relu-SepConv
Conv-Relu

Table 2. The CNN benchmarks. Number of blocks, number of
operators and the main operator type for each network are listed
in the table. Here “Conv-Relu” means a convolution followed by
a ReLU activation and “Relu-SepConv” means ReLU activation
followed by seperatble convolution.

We benchmark four modern CNNs in the experiment: Incep-
tion V3 (Szegedy et al., 2016), RandWire (Xie et al., 2019),
NasNet-A (Zoph et al., 2018) and SqueezeNet (Iandola et al.,
2016). Table 2 shows the number of blocks, the number of
operators, and the main operator type for each network. IOS
supports the user-deﬁned schedule unit. In this experiment,
we take the operator type shown in the table, besides other
operators such as Concat, as the basic schedule unit. Some
models (e.g., ResNet (He et al., 2016)) might have limited
inter-operator parallelization opportunities. For example,
for ResNet-50 and ResNet-34, we can only achieve 2% to
5% speedup by paralleling the downsample convolutions.
We do not consider it as our benchmarked model in the rest
of the evaluation.

We conduct each experiment 5 times and report the aver-
age performance. We adopt the schedule pruning strategy
with r = 3 and s = 8 and conduct each experiment on
NVIDIA Tesla V100 unless otherwise stated. Please refer to
Appendix B for the experiments on other device. The IOS
optimization cost for Inception V3 and SqueezeNet is less

IOS: Inter-Operator Scheduler for CNN Acceleration

Figure 6. End-to-end performance comparison of different sched-
ules across different CNNs on batch size one. The throughput is
normalized to the best one for each model.

Figure 7. End-to-end performance comparison of different frame-
works across different CNNs on batch size one. The throughput is
normalized to the best one for each model.

than 1 minute and the IOS optimization cost for Randwire
and NasNet is within 90 minutes.

6 EXPERIMENTS

6.1 Comparison of Different Schedules

We ﬁrst compare the inference performance among different
schedules with batch size one. We compare ﬁve schedules:
sequential schedule, greedy schedule, IOS-Merge schedule,
IOS-Parallel schedule, and IOS-Both schedule. The sequen-
tial schedule executes the operator one-by-one according
to certain topological ordering. The greedy schedule puts
all the operators that can be executed currently in one stage,
and repeats this process until all operators have been sched-
uled. IOS-Merge, IOS-Parallel, and IOS-Both schedules
use the proposed approach to ﬁnd the schedule but take
different parallelization strategies. IOS-Merge only takes
the “operator merge” strategy. IOS-Parallel only takes the
“concurrent execution” strategy. IOS-Both considers both
parallelization strategies. All schedules are executed on IOS
execute engine for a fair comparison.

Figure 6 shows that IOS-Both outperforms all the other four
schedules. The greedy schedule gets good results on Rand-
Wire and NasNet. However, it degrades the performance
of SqueezeNet because of the overhead of synchronization.
Because we can not merge “Relu-SepConv” operators in
RandWire and NasNet, IOS-Merge gets the same sched-
ule as Sequential, and IOS-Both gets the same schedule as
IOS-Parallel. IOS-Both considers two parallelization strate-
gies and outperforms all the other four schedules. In later
experiments, “IOS” refers to “IOS-Both” by default.

6.2 Comparison of cuDNN-based Frameworks

For popular frameworks, there are two ways to exploit the
intra-operator parallelism. Frameworks such as Tensor-
ﬂow (Abadi et al., 2015), TASO (Jia et al., 2019a), and Ten-
sorRT (NVIDIA) use the vendor-provided library cuDNN.
Frameworks such as TVM (Chen et al., 2018) and An-
sor (Zheng et al., 2020) search the tensor program schedule
for each kernel. TVM also supports to call external libraries

such as cuDNN to implement some kernels (e.g., convo-
lution). In this subsection, we compare the performance
of cuDNN-based frameworks with batch size one. Larger
batch size is studied in the ablation study section.

There are ﬁve baselines: Tensorﬂow, Tensorﬂow-XLA,
TASO, TVM-cuDNN, and TensorRT. Tensorﬂow-XLA is
the tensorﬂow framework with XLA optimization turning
on. TVM-cuDNN is the TVM framework that compiles
a convolution neural network with cuDNN library, which
would use the convolution kernel provided by cuDNN to
execute convolutions. All other operators such as addition
and concatenation would use their own kernels. For fair
comparison, we only compare cuDNN-based libraries here.
The comparison between TVM-AutoTune and IOS can be
found in the ablation study section. Figure 7 shows that
IOS consistently outperforms all ﬁve baseline frameworks
on four benchmark CNNs. IOS can achieve 1.1 to 1.5×
speedup comparing to the state of the art library TASO,
TVM-cuDNN, and TensorRT.

6.3 More Active Warps Improve Utilization

Figure 8. Active Warps for sequential schedule and IOS schedule.
We use the model in Figure 2 in this experiment.

Model operators are mapped to GPU kernels to execute. A
kernel invokes a collection of threads that are grouped into
multiple thread blocks.1 Thread blocks are distributed to
stream multiprocessors (SMs). Each thread block on a SM
is further partitioned into multiple warps. A warp, as a basic
execution unit, contains a ﬁxed number of threads (e.g.,
32 for NVIDIA GPU) to execute in a Single Instruction
Multiple Thread (SIMT) fashion.

1We adopt the terminology used by NVIDIA.

Normalzied Throughput0.00.20.40.60.81.0Inception V3RandWireNasNetSqueezeNetGeoMeanSequentialGreedyIOS-MergeIOS-ParallelIOS-BothNormalized Throughput0.00.20.40.60.81.0Inception V3RandWireNasNetSqueezeNetGeoMeanTensorflowTensorflow-XLATASOTVM-cuDNNTensorRTIOS#Active Warps Between Two Timestamps (10⁸)09Timestamp17131925313743495561677379859197SequentialIOSSeq: 1.7×108 warps/msIOS: 2.7×108 warps/ms3.65.71.58x Active WarpsIOS: Inter-Operator Scheduler for CNN Acceleration

A warp is considered active from the time it is scheduled on
an SM until it completes the last instruction. SM can hide
the warps stall caused by memory accesses through fast con-
text switching: at every cycle, each warp scheduler will pick
an eligible warp and issue instructions. If no eligible warp
is available for a warp scheduler, the computation resources
are underutilized. Increasing the number of active warps is
an effective approach to increase the likelihood of having
eligible warps to execute at each cycle. Thus, it is crucial
to increase the number of active warps. Figure 8 shows the
number of active warps on the whole GPU throughout the
repeated execution of both the IOS and the Sequential sched-
ule, sampled using NVIDIA’s CUPTI proﬁling toolset every
2.1 ms. IOS schedule achieves 58% more active warps on
average compared to the Sequential schedule. This explains
the reason for IOS overall performance speedup.

7 ABLATION STUDY

7.1 Schedule Pruning Reduces Search Time

7.2 Specialized Scheduling is Beneﬁcial

Table 3. Latency (ms) of specialized schedules for batch size 1, 32
and 128, and specialized schedules for NVIDIA Tesla K80 and
V100. The best performance is achieved when the schedule is
specialized for each batch size and device. Each row is the batch
size or device that the model is executed on. Each column is the
batch size or device that IOS optimized for. InceptionV3 is used
as a benchmark.

Different workloads (e.g. network with different batch sizes)
have different computation features; thus it is necessary to
specialize the schedule for different workloads. We opti-
mize Inception V3 with batch size 1, 32 and 128. Then we
execute the network with these schedules on batch size 1,
32 and 128 separately. In Table 3 (1), the numbers in a row
represents the latency executed with the same batch size
but using schedules optimized for different batch sizes. The
specialized schedule for each batch size achieved the best
result. To explore the specialization for devices, we also
optimize the network on both NVIDIA Tesla K80 and V100
with batch size one. Table 3 (2) shows that the specialized
schedule for each device also achieved better results.

Figure 9. Trade-off between the optimized latency and the opti-
mization cost for Inception V3 and NasNet. Two pruning strategy
parameters r and s are used to prune the schedule space. r limits
the maximum number of operators in each group while s limits
the maximum number of groups in a stage. The left axis shows the
optimized latency, and the right axis shows the optimization cost.

To explore the trade-off between optimized latency and opti-
mization cost (i.e. search time), we experiment Inception V3
and NasNet with pruning strategy parameters r = {1, 2, 3}
and s = {3, 8}. As shown in Figure 9, when s and r get
smaller, the optimization cost decreases at the cost of larger
network latency. This is because smaller s and r restrict the
schedules that IOS explores, thus reduce the optimization
cost and increase schedule latency. By setting r = 1 and
s = 8, IOS still achieves 1.59× and 1.37× speedup for
Inception V3 and NasNet, comparing to sequential schedule.
Meanwhile, the optimization cost for each network is within
30 seconds and 18 minutes, respectively.

Figure 10. The schedule found by IOS for the last block of Incep-
tion V3. Operator a-e are convolution operator while operator P is
the pooling operator. Schedule (1) and (2) are optimized for batch
size 1 and 32, respectively. There are two stages in schedule (1)
while there are 4 stages in schedule (2). Schedule (1) is 28% faster
than schedule (2) on batch size 1. Schedule (2) is 8% faster than
schedule (1) on batch size 32.

IOS discovers different schedules for different batch sizes.
For example, Figure 10 shows the schedule of the last block
of Inception V3 optimized for batch size 1 and 32, respec-
tively. There are two stages in the schedule (1), which is
optimized for batch size 1 while there are four stages in

015304560Latency (ms)Optimization Cost (secs)4.264.164.103.94.04.14.24.44.154.044.0301000200030004000r=3r=2r=117.916.916.915.015.916.817.618.5r=3r=2r=116.716.116.0Inception V3NasNets = 3s = 8 Specializationfor Different Batch SizesOptimized for132128Execute on14.034.504.633229.2127.4427.93128105.98103.74103.29Specializationfor Different DevicesOptimized forK80V100Execute onK8013.8714.65V1004.494.03(1) Specialization for Batch Sizes(2) Specialization for DevicesInputeOutputbhiacfgPdInputeOutputbhiacf & gPdStage 1Stage 2Stage 1Stage 2Stage 3Stage 4(1) Schedule optimized for BS 1(2) Schedule optimized for BS 32Conv3x3: a, b, c, d, ePooling: PConv3x1: f, hConv1x3: g, iIOS: Inter-Operator Scheduler for CNN Acceleration

the schedule (2), which is optimized for batch size 32. The
schedule (1) is 28% faster than the schedule (2) on batch
size 1, while the schedule (2) is 8% faster than (1) on batch
size 32. There are two differences between them. The ﬁrst
one is that convolution f and g in the schedule (2) are merged
into a single convolution. This is because activation (the
output tensor of an operator) is the memory bottleneck at
large batch size. It is more crucial to reduce memory access,
even at the cost of larger computation cost. Merging can
reduce the memory access, because the merged kernel only
access the output of convolution c once, instead of twice
in the schedule (1). However, because the kernel size of f
and g are 3x1 and 1x3, respectively, their kernel size would
be expanded to 3x3 by padding zeros, which increases the
amount of computation. Another difference between the
schedule (1) and (2) is that the schedule (2) has more stages
than the schedule (1). We found a similar phenomenon for
large batch sizes because of resource contention. When
multiple operators are executed on the device, there is a
conﬂict over access to the shared resources such as the last-
level cache, making the concurrent execution degrades the
performance. This gets more severe for larger batch sizes
because the demand for shared resources gets larger.

7.3 Consistent Improvement for Different Batch Sizes

Figure 11. The throughput comparison of Sequential schedule,
TVM-cuDNN, TASO, TensorRT and IOS on batch size 1 to 128
for Inception V3. TASO runs out of memory with batch size 128.

In real-world applications, we need to handle different batch
sizes for inference. For example, for real-time applications
on edge devices, we usually use a batch size of one to reduce
latency. In contrast, in cloud settings, the larger batch size
is preferred to increase throughput. Changing the workload
requires different inter-operator parallelization schedules.
We optimize Inception V3 with the batch sizes of 1, 16,
32, 64, 128, and compare the throughput. Figure 11 shows
that the throughput increases with the batch size. When the
batch size is larger than 128, the performance saturates, and
the throughput does not increase signiﬁcantly anymore. The
throughput of IOS outperforms all the baselines consistently
on all batch sizes. Even though a larger batch size provides
more data parallelism, we can still utilize inter-operator
parallelism to further improve the throughput.

7.4

Intra- and Inter-Operator Parallelism

Figure 12. End-to-end performance comparison between TVM-
AutoTune and IOS. TVM-AutoTune and IOS are orthogonal be-
cause TVM focuses on the intra-operator parallelism while IOS
focuses on inter-operator parallelism. They can be combined to
boost the inference performance further. The optimization cost of
IOS is two orders of magnitude less than TVM.

TVM exploits the intra-operator parallelism by searching the
schedule for each kernel on a speciﬁc device. IOS focuses
on inter-operator parallelism and leaves the exploitation of
intra-operator parallelism to cuDNN library. Although intra-
and inter-operator parallelism is orthogonal and can be com-
bined, we compare TVM and IOS here to give some insight
into each parallelism’s beneﬁt. As shown in Figure 12, TVM
takes 208 GPU hours while IOS only takes 3 GPU hours
to optimize the four networks. IOS outperforms TVM on
Inception V3 and SqueezeNet. This is because only utilizing
intra-parallelism can not provide enough parallelism for the
powerful computing device. Meanwhile, TVM outperforms
IOS on Randwire and NasNet, because TVM ﬁnds more
efﬁcient kernels for separable convolutions, which occupy
the majority of operators in Randwire and NasNet. We be-
lieve the combination of TVM and IOS would boost the
performance further and leave this for future work.

8 CONCLUSION

With the increasing computational capacity, the sequen-
tial execution of CNNs no longer provides sufﬁcient paral-
lelization opportunities to fully utilize all the computation
resources. We propose IOS that combines intra- and inter-
operator parallelism and adapt dynamic programming to
ﬁnd an efﬁcient schedule that better utilizes the hardware.
Experiments show that IOS can improve the GPU utiliza-
tion and speedup modern CNN inference from 1.1 to 1.5×
compared to the state-of-the-art libraries (e.g., TensorRT).

ACKNOWLEDGEMENTS

We want to thank Xiaodan (Serina) Tan for NVIDIA GPU-
related issues and constructive discussion. This project
was supported by the Canada Foundation for Innovation
JELF grant, NSERC Discovery grant, AWS Machine Learn-
ing Research Award, Facebook Faculty Research Award,
MIT-IBM Watson AI Lab, MIT Data Science and AI Lab
(DSAIL), NVIDIA, and NSF CAREER Award #1943349.

Throughput (Images/Sec)1351701105014001163264128SequentialTVM-cuDNNTASOTensorRTIOSNormalized Throughput0.00.20.40.60.81.0Inception V3RandWireNasNetSqueezeNetGeoMeanTVM-AutoTuneIOSTotal Optimization Cost (GPU hours)1101001000All Networks3208IOS: Inter-Operator Scheduler for CNN Acceleration

REFERENCES

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Is-
ard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
Levenberg, J., Man´e, D., Monga, R., Moore, S., Mur-
ray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,
Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Va-
sudevan, V., Vi´egas, F., Vinyals, O., Warden, P., Watten-
berg, M., Wicke, M., Yu, Y., and Zheng, X. TensorFlow:
Large-scale machine learning on heterogeneous systems,
2015. URL http://tensorflow.org/. Software
available from tensorﬂow.org.

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.
Tensorﬂow: A system for large-scale machine learning.
In 12th {USENIX} Symposium on Operating Systems
Design and Implementation ({OSDI} 16), pp. 265–283,
2016.

Cai, H., Yang, J., Zhang, W., Han, S., and Yu, Y. Path-level
network transformation for efﬁcient architecture search.
In ICML, 2018.

Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen,
H., Cowan, M., Wang, L., Hu, Y., Ceze, L., et al. {TVM}:
An automated end-to-end optimizing compiler for deep
learning. In 13th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 18), pp.
578–594, 2018.

Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J.,
Tran, J., Catanzaro, B., and Shelhamer, E.
cudnn:
Efﬁcient primitives for deep learning. arXiv preprint
arXiv:1410.0759, 2014.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. CoRR, abs/1810.04805, 2018. URL
http://arxiv.org/abs/1810.04805.

Dilworth, R. P. A decomposition theorem for partially or-
dered sets. Annals of Mathematics, 51(1):161–166, 1950.
ISSN 0003486X. URL http://www.jstor.org/
stable/1969503.

Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K.,
Dally, W. J., and Keutzer, K. Squeezenet: Alexnet-level
accuracy with 50x fewer parameters and 0.5 mb model
size. arXiv preprint arXiv:1602.07360, 2016.

Jia, Z., Padon, O., Thomas, J., Warszawski, T., Zaharia,
M., and Aiken, A. Taso: Optimizing deep learning
computation with automatic generation of graph sub-
In Proceedings of the 27th ACM Sympo-
stitutions.
sium on Operating Systems Principles, SOSP ’19, pp.
47–62, New York, NY, USA, 2019a. Association for
Computing Machinery. ISBN 9781450368735. doi: 10.
1145/3341301.3359630. URL https://doi.org/
10.1145/3341301.3359630.

Jia, Z., Thomas, J., Warszawski, T., Gao, M., Zaharia, M.,
and Aiken, A. Optimizing dnn computation with relaxed
In Talwalkar, A., Smith, V., and
graph substitutions.
Zaharia, M. (eds.), Proceedings of Machine Learning and
Systems, volume 1, pp. 27–39. 2019b.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In Advances in neural information processing systems,
pp. 1097–1105, 2012.

Kwon, W., Yu, G.-I., Jeong, E., and Chun, B.-G. Nim-
ble: Lightweight and parallel gpu task scheduling
for deep learning.
In Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Ad-
vances in Neural Information Processing Systems,
pp. 8343–8354. Curran Associates,
volume 33,
URL https://proceedings.
Inc.,
2020.
neurips.cc/paper/2020/file/
5f0ad4db43d8723d18169b2e4817a160-Paper.
pdf.

Ma, L., Xie, Z., Yang, Z., Xue, J., Miao, Y., Cui, W., Hu,
W., Yang, F., Zhang, L., and Zhou, L. Rammer: En-
abling holistic deep learning compiler optimizations with
rtasks. In 14th USENIX Symposium on Operating Sys-
tems Design and Implementation (OSDI 20), pp. 881–
897. USENIX Association, November 2020. ISBN 978-
1-939133-19-9. URL https://www.usenix.org/
conference/osdi20/presentation/ma.

Han, S., Mao, H., and Dally, W. J. Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing
atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Mullapudi, R. T., Adams, A., Sharlet, D., Ragan-Kelley, J.,
and Fatahalian, K. Automatically scheduling halide im-
age processing pipelines. ACM Transactions on Graphics
(TOG), 35(4):1–11, 2016.

IOS: Inter-Operator Scheduler for CNN Acceleration

Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning
transferable architectures for scalable image recognition.
In Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pp. 8697–8710, 2018.

NVIDIA. Nvidia tensorrt: Programmable inference accel-
erator. URL https://developer.nvidia.com/
tensorrt.

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
In NIPS-W,
A. Automatic differentiation in pytorch.
2017.

Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and
Chen, L.-C. Mobilenetv2: Inverted residuals and linear
In Proceedings of the IEEE Conference
bottlenecks.
on Computer Vision and Pattern Recognition, pp. 4510–
4520, 2018.

Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., et al. Mastering the
game of go with deep neural networks and tree search.
nature, 529(7587):484, 2016.

Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-
quence learning with neural networks. In Advances in
neural information processing systems, pp. 3104–3112,
2014.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
In Proceedings
A. Going deeper with convolutions.
of the IEEE conference on computer vision and pattern
recognition, pp. 1–9, 2015.

Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,
Z. Rethinking the inception architecture for computer vi-
sion. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016.

Tang, L., Wang, Y., Willke, T. L., and Li, K. Scheduling
computation graphs of deep learning models on manycore
cpus. arXiv preprint arXiv:1807.09667, 2018.

Xie, S., Kirillov, A., Girshick, R., and He, K. Exploring
randomly wired neural networks for image recognition.
arXiv preprint arXiv:1904.01569, 2019.

Zhang, X., Zhou, X., Lin, M., and Sun, J. Shufﬂenet: An ex-
tremely efﬁcient convolutional neural network for mobile
devices. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 6848–6856,
2018.

Zheng, L., Jia, C., Sun, M., Wu, Z., Yu, C. H., Haj-Ali, A.,
Wang, Y., Yang, J., Zhuo, D., Sen, K., Gonzalez, J. E., and
Stoica, I. Ansor: Generating high-performance tensor
In 14th USENIX Sympo-
programs for deep learning.
sium on Operating Systems Design and Implementation
(OSDI 20), Banff, Alberta, November 2020. USENIX
Association. URL https://www.usenix.org/
conference/osdi20/presentation/zheng.

IOS: Inter-Operator Scheduler for CNN Acceleration

A PROOF OF TIME COMPLEXITY

In this section of appendix, we prove the time complexity
bound given in Section 4.2. In Section A.1, we give some
preliminary deﬁnitions and theorems used in our proof. In
Section A.2, we prove the time complexity of inter-operator
scheduler (IOS).

A.1 Preliminary Deﬁnitions and Theorems

In this subsection, we give the deﬁnition of chain and anti-
chain, Dilworth’s theorem (Dilworth, 1950), and a corollary,
which is used in our proof later.

Deﬁnition 2 (Chain and antichain). A chain is a subset of
a partially ordered set such that any two distinct elements in
the subset are comparable. An antichain is a subset such that
any two distinct elements in the subset are incomparable.

Deﬁnition 3 (Chain decomposition of partial order set). A
chain decomposition of a partial order set is a partition of
the elements of the ordered set into disjoint chains.

Theorem (Dilworth’s Theorem). In any ﬁnite partially or-
dered set, the largest antichain has the same size as the
smallest chain decomposition.

We apply the Dilworth’s theorem to a directed acyclic graph
and can get the following corollary.

Corollary 1. Let G = (V, E) be a directed acyclic graph
and d be the width of G. We can decompose V into d sets
such that any two vertices in the same set can be connected
by a path in G.

Proof. Let P = (V, E(cid:48)) be the partial order derived from
G by transitive closure. Then that two elements u, v in V
are comparable in P is equivalent to that there is a path
between them in G. Thus, the width d of G equals the size
of largest antichain of P . We apply the Dilworth’s Theorem
to P and can get a decomposition of V into d chains in
P : S1, S2, . . . , Sd. Because Si is a chain in P , any two
elements in Si are comparable, which means there is a path
bridge them in G.

A.2 Time Complexity of Inter-Operator Scheduler

In this subsection, we will prove the time complexity of IOS
stated in Section 4.2. Then we will show that the upper
bound can be reached by some computation graph.
Lemma 1. If S(cid:48)
also ends S (S(cid:48) ends S means that S(cid:48) is an ending of S).

1 ends S and S(cid:48)

2 ends S − S(cid:48)

1, then S(cid:48)

1 ∪ S(cid:48)
2

Proof. We prove it by contradiction. If S(cid:48)
end S, there must exist (u, v) ∈ E such that u ∈ S(cid:48)
and v ∈ S − S(cid:48)
If u ∈ S(cid:48)
ending of S because v ∈ S − S(cid:48)

1, we can get the contradiction that S(cid:48)
2 ⊆ S − S(cid:48)

2 does not
1 ∪ S(cid:48)
2
1 or u ∈ S(cid:48)
2.
1 is not an
1. If u ∈ S(cid:48)
2,

2. Then we have u ∈ S(cid:48)

1 ∪ S(cid:48)

1 ∪ S(cid:48)

1 ∪ S(cid:48)

we can also get the contradiction that S(cid:48)
S − S(cid:48)

1 because v ∈ S − S(cid:48)

1 ∪ S(cid:48)

2 = (S − S(cid:48)

2 is not an ending of
1) − S(cid:48)
2.

Lemma 2. Let S be a possible argument of SCHEDULER,
we have V − S ends V .

k ends V − (cid:83)k−1

Proof. We can rewrite S as S = V −(cid:83)m
and S(cid:48)
By repeating apply Lemma 1, we can get that (cid:83)m
V , which means V − S ends V .

i=1 S(cid:48)
i, where m ≥ 0
i according to L17 in Algorithm 1.
i ends

i=1 S(cid:48)

i=1 S(cid:48)

Lemma 3. Let V (cid:48) be a subset of V and any two operators
in V (cid:48) are bridged by a path. Let c be the size of V (cid:48). Then

|{(S ∩ V (cid:48), S(cid:48) ∩ V (cid:48)) | S(cid:48) ends S, V − S ends V }| =

(cid:33)

(cid:32)

c + 2
2

Proof. Because any two operators in V (cid:48) is bridged by a
path in G, operators in V (cid:48) are ordered sequentially. Because
V − S ends V , there are only c + 1 possible sets of S ∩
V (cid:48) because S must be a preﬁx in the sequential ordered
operators, including empty set. S(cid:48) ∩ V (cid:48) is a sufﬁx of S ∩
V (cid:48), including empty set. Then there are (cid:80)c
j=0 1 =
(c+2)(c+1)
2

(cid:1) possible pairs of (S ∩ V (cid:48), S(cid:48) ∩ V (cid:48)).

= (cid:0)c+2

(cid:80)i

i=0

2

(cid:1)d

Theorem. The time complexity of inter-operator scheduler
is O((cid:0)n/d+2
d + 1)2d),
where n is the number of operators in the computation graph
and d is its width.

), which can be relaxed to O(( n

2

Proof. We only need to count the number of pairs of (S, S(cid:48))
that can reach L17 of Algorithm 1 because L17-21 dom-
inates the execution time of the scheduler, where S is a
subset of V that is taken as the argument of SCHEDULER
and S(cid:48) is an ending of S. By Lemma 2, V − S ends V .
By Corollary 1, we can decompose V into d disjoint parti-
tions V1, V2, . . . , Vd and any two operators u, v in the same
partition can be bridged by a path in G. We can build a
one-to-one mapping that maps pair (S, S(cid:48)) to 2d-dimension
tuple (S∩V1, S(cid:48)∩V1, . . . , S∩Vd, S(cid:48)∩Vd) based on the parti-
tion. Then we only need to count the number of valid tuples
to get the number of valid pairs. By Lemma 3, the possible
number of pairs (S ∩ Vi, S(cid:48) ∩ Vi) is (cid:0)ci+2
(cid:1). Then an upper
(cid:1). It is an upper bound but
bound of the tuples is (cid:81)d
not the exact number because currently we only consider
the dependency inside each partition Vi and ignored the de-
pendency between different partitions. So the upper bound
(cid:1). It can be
(cid:0)ci+2
of the number of pairs of (S, S(cid:48)) is (cid:81)d
2
(cid:1)d
relaxed to (cid:0)n/d+2
i ci = n and it is maximized
when ci are equal. For simplicity, it can be further relaxed
to ( n

because (cid:80)d

(cid:0)ci+2
2

i=1

i=1

2

2

d + 1)2d.

IOS: Inter-Operator Scheduler for CNN Acceleration

tecture), we also conduct experiments on NVIDIA RTX
2080Ti (Turing architecture) to show that our optimization
is generally effective across different GPU architectures.
We use the same models and baselines for comparisons as
in Section 6.1 and Section 6.2.

Figure 14 shows that IOS with two parallelization strategies
(i.e., IOS-Both) outperforms all other schedules. In partic-
ular, IOS-Both achieves 1.1× to 1.5× speedup comparing
to the sequential schedule. Figrue 15 shows that IOS out-
performs all other cuDNN-based frameworks2 on Inception
V3, RandWire, and NasNet. IOS achieves comparable per-
formance with TASO and TensorRT on SquuezeNet. These
results align with the results on V100.

)

Figure 13. The example to make the time complexity O((cid:0)n/d+2
tight. The time complexity for this graph is O((cid:0)c+2

(cid:1)d

)

2

(cid:1)d

2

The computation graph shown in Figure 13 is an example to
demonstrate that the time complexity of O((cid:0)n/d+2
) can
be reached.

(cid:1)d

2

In this example, there are d independent paths and each path
has c operators. Because the paths are independent with
each other and there is no edge between two different paths,
we can get the upper bound O((cid:0)c+2
) by the analysis in
above time complexity proof.

(cid:1)d

2

B SPEEDUP ON NVIDIA RTX 2080TI

Figure 14. End-to-end performance comparison of different sched-
ules across different CNNs on batch size one. The throughput is
normalized to the best one for each model. This experiment is
conducted on NVIDIA RTX 2080Ti.

C BLOCK-WISE SPEEDUP

Figure 16. IOS consistently outperforms sequential executions on
each block of Inception-v3.

To explore the speedup for different blocks, we compare the
performance of each block of Inception-V3 (Szegedy et al.,
2016) between sequential and IOS schedule (Figure 16).
IOS consistently runs faster than the sequential schedule.
The speedup for the individual block is up to 2.3×, and
the end-to-end speedup is 1.6×. More speedup is achieved
for back blocks because the width gets larger and more
inter-parallelism is possible.

Figure 15. End-to-end performance comparison of different frame-
works across different CNNs on batch size one. The throughput
is normalized to the best one for each model. This experiment is
conducted on NVIDIA RTX 2080Ti.

In addition to results on NVIDIA Tesla V100 (Volta archi-

Wire and NasNet.

2TASO runs out of GPU memory on NVIDIA 2080Ti for Rand-

(2, 1)(c, 1)(1, 1) (2, 2)(c, 2)(1, 2)(2, 3)(c, 3)(1, 3)(2, d)(c, d)(1, d)Normalzied Throughput0.00.20.40.60.81.0Inception V3RandWireNasNetSqueezeNetGeoMeanSequentialGreedyIOS-MergeIOS-ParallelIOS-BothNormalized Throughput0.00.20.40.60.81.0Inception V3RandWireNasNetSqueezeNetGeoMeanTensorflowTensorflow-XLATASOTVM-cuDNNTensorRTIOSNormalized Throughput0.00.20.40.60.81.01234567891011SequentialIOS