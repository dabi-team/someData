Date of publication xxx, 2022, date of current version xxxx 00, 0000.

Digital Object Identiﬁer 10.1109/ACCESS.2017.DOI

Large Language Models are not Models
of Natural Language: they are Corpus
Models.

CSABA VERES
Department of Information Science and Media Studies, University of Bergen, Norway (e-mail: csaba.veres@uib.no)

This work was supported by the News Angler Project through the Norwegian Research Council under Project 275872.

ABSTRACT Natural Language Processing (NLP) has become one of the leading application areas
in the current Artiﬁcial Intelligence boom. Transfer learning has enabled large deep learning neural
networks trained on the language modeling task to vastly improve performance in almost all downstream
language tasks. Interestingly, when the language models are trained with data that includes software code,
they demonstrate remarkable abilities in generating functioning computer code from natural language
speciﬁcations. We argue that this creates a conundrum for the claim that eliminative neural models are
a radical restructuring in our understanding of cognition in that they eliminate the need for symbolic
abstractions like generative phrase structure grammars. Because the syntax of programming languages is by
design determined by phrase structure grammars, neural models that produce syntactic code are apparently
uninformative about the theoretical foundations of programming languages. The demonstration that neural
models perform well on tasks that involve clearly symbolic systems, proves that they cannot be used as an
argument that language and other cognitive systems are not symbolic. Finally, we argue as a corollary that
the term language model is misleading and propose the adoption of the working term corpus model instead,
which better reﬂects the genesis and contents of the model.

INDEX TERMS natural language processing, deep learning, syntax, linguistics, language model, automatic
programming, neural networks

2
2
0
2

n
u
J

5
1

]
L
C
.
s
c
[

2
v
5
5
0
7
0
.
2
1
1
2
:
v
i
X
r
a

I. INTRODUCTION

Deep-learning Artiﬁcial Neural Networks (ANNs) imple-
ment a multi-layered machine learning architecture which
enables sophisticated representation learning [1]. They have
signiﬁcantly changed the technological, societal and com-
mercial landscape in the past decade [2, 3]. In this article
we focus on deep learning models for Natural Language
Processing (NLP). Transformer based deep learning models
[4] have recorded signiﬁcant improvements in various natural
language tasks, and have entered service in industrial appli-
cations that have a signiﬁcant linguistic component [5, 6]. As
an engineering artefact, these models have clearly enjoyed
signiﬁcant success. What is less clear is how much they
have contributed to our understanding of natural language
and cognitive architecture. While the most powerful systems
have shown remarkable abilities complex language tasks like
question answering and writing prose about arbitrary topics
[7], there are contrary claims that ANNs are nothing but
giant stochastic parrots, with many hidden dangers if mis-

represented as systems that understand language in any non
trivial sense [8, 9].

Language models are joint probability distributions over
sequences of words, or alternatively, functions that return a
probability measure over strings drawn from some vocabu-
lary [10, 11]. Large Neural LMs learn probability functions
for sequences of real valued, continuous vector representa-
tions of words rather than discrete lexical items. Continuous
representations are effective at generalising across novel con-
texts, resulting in better performance across a range of tasks
[11]. The probability distribution is learned through a form
of language modeling, where the task is to "predict the next
word given the previous words" [12] (p.191) in word strings
drawn from a corpus.

Neural LMs stipulate a mental architecture that stands in
stark contrast with a Classical symbolic view [13] dominant
in Linguistics since at least the beginning of the 20th century,
when language scholars began to study the structural proper-
ties of languages [14, 15]. The rigorous study of language

VOLUME 4, 2016

1

 
 
 
 
 
 
as a cognitive faculty was pioneered by Noam Chomsky
who introduced Generative Phrase Structure Grammars as
the rule structures underlying linguistic competence [16, 17].
In subsequent years a number of important challenges and
modiﬁcations emerged, both from within and outside the re-
search program1 [18]. Nevertheless, in the midst of technical
controversies and disagreements, researchers have all agreed
that "... linguistic knowledge is couched in the form of rules
and principles." [19] (p.74).

The recent success of deep learning ANNs on wide ranging
linguistic tasks have given support to the claim that sta-
tistical models are preferable to generative phrase structure
grammars as theories of linguistic competence. Manning and
Schütze argue in their classic text "Foundations of Statistical
Natural Language Processing" that cognition in general and
language in particular are "best formalized as probabilistic
processes" [12] (p.15). A similar position is more vigorously
expressed by Peter Norvig in the context of a debate with
Noam Chomsky at MIT2: "Many phenomena in science are
stochastic, and the simplest model of them is a probabilistic
model; I believe language is such a phenomenon and there-
fore that probabilistic models are our best tool for represent-
ing facts about language" [20]. Perhaps most dramatically,
Geoff Hinton has proclaimed in his Turing Award acceptance
speech that the success of "machine translation was the ﬁnal
nail in the cofﬁn of symbolic AI" [21] (32’30”).

In this paper we will defend the Classical symbolic view
with an argument analogous to an indirect proof in logic,
where the assumed truth of a premise leads to an absurd
conclusion, thereby proving the falsehood of that premise.
We present the argument as follows. In section 2 we provide
some background by brieﬂy explaining what is meant by a
rule based, generative phrase structure grammar in natural
language and software code. The argument itself begins in
section 3 which details why ANN models are an alternative
to explicit rule based grammar. The assumed premise is that
the success of neural models makes them good explanatory
models of natural language without the need for symbols. In
section 4 we describe recent successes using ANN models
- especially language models - to automatically generate
software code. But this is where the contradiction arises. If
ANN models can be construed as explanatory theories for
natural language based on their successes on language tasks
then, in the absence of counter arguments, they should be
good explanatory theories for computer language as well.
We see no such argument and therefore arrive at the absurd
conclusion that ANNs are good explanatory models of soft-
ware. We know this is absurd because it is just a fact that
the acceptable syntax of computer languages is determined

1Chomsky describes his early work as an attempt to create a theoretical
apparatus which was rich enough to describe the data. But it was always
understood that the initial machinery had to be wrong because such a rich,
complex system couldn’t meet the criterion for biological evolution. The
subsequent years were spent by reducing the complexity of the theoretical
machinery https://youtu.be/pUWmTXkpHjE?t=3520

2MIT150: Brains, Minds and Machines Symposium, June 16, 2011.

Transcript available at https://chomsky.info/20110616/

Veres et al.: Language Models are not Models of Language

by their grammar. Therefore, successful ANN models of nat-
ural language cannot be used as evidence against generative
phrase structure grammars in natural language. In section 5
we show that in fact language models are most accurately
described as corpus models. The paper then concludes.

II. GENERATIVE PHRASE STRUCTURE GRAMMAR FOR
NATURAL LANGUAGE AND SOFTWARE CODE
Linguistics in the ﬁrst half of the 20th century was mainly
a taxonomic science with researchers pursuing Immediate
Constituent (IC) analysis inspired by Bloomﬁeld [22], and
Saussure’s structural linguistics as outlined in his 1916 book,
Course in General Linguistics. The goal of early linguists
was to develop methods that divide an expression into its
immediate constituents, and continue the subdivision until
syntactically indivisible parts were obtained. The essential
insight contributed by Chomsky was that the constituent
structure of language is the product of a system of rewrite
rules of the form A → ω where A is a class label and
ω is a string that could contain terminal strings as well
as other class labels [23]. An early example in [17] is the
following simple grammar. (Note that the early formulations
of grammar did not yet include the necessary machinery for
recursive deﬁnitions [15].)

Sentence → N P + V P

N P → T + N

V P → V erb + N P

T → the

N → man, ball, etc.

V erb → hit, took, etc.

(1)

(2)

(3)

(4)

(5)

(6)

This grammar can generate sentences of the type shown in

Figure 1, through a series of derivations.

FIGURE 1. A tree diagram showing a result of a sequence of derivations
using rules 1 - 6. It does not show the order of the derivation.

Syntactic Structures [17] outlined a new agenda for a
formal linguistic theory in which language was considered as
" ... a set (ﬁnite or inﬁnite) of sentences, each ﬁnite in length
and constructed out of a ﬁnite set of elements" [17](p. 13).
Sentences are the output of a generative grammar, and the
Linguistics project is to explore grammars which generate all
and only the grammatical sentences of natural languages.

An important distinction was drawn between linguistic
competence, the speaker-hearer’s knowledge of language,

2

VOLUME 4, 2016

Veres et al.: Language Models are not Models of Language

and performance, actual instances of language use in con-
crete situations [24]. A grammar on this view is a description
of competence. It is not only a theory about possible sentence
structure but about the intrinsic knowledge that allows the
speaker to generate an inﬁnite set of grammatical sentences
with a ﬁnite set of rules, and for the hearer to assign lin-
guistic structure to each of those sentences. Performance, on
the other hand, includes the many psychological processes
underlying actual linguistic productions, including effects
of attention, memory, and so on. The multitude of these
psychological processes are often little understood and can
result in productions rife with errors from interference by
a range of unpredictable, non language speciﬁc processes.
For this reason Chomsky advocated using intuitions about
grammatical acceptability as the primary data for linguistic
theories rather than speech or text corpora.

One of the pioneers of high level computer programming
languages, John W. Backus who led the Applied Science Di-
vision of IBM’s Programming Research Group3 took inspi-
ration from Chomsky’s work on PSGs and conceived a meta-
language that could describe the syntax of languages that was
easier for programmers to write than the machine languages
of the time. The meta language later became known as
Backus-Naur form (BNF), so called partly because it was
originally co-developed by Peter Naur in a 1963 IBM report
on the ALGOL 60 programming language"4. The BNF is a
notation for context free grammars consisting of productions
over terminal and nonterminal symbols, used to provide the
precise syntax of programming languages which is required
for writing compilers and interpreters for the language [25].
The original syntax described in the technical report included
meta characters as shown in (7)

< > ::= |

(7)

where sequences of characters enclosed in the brackets
represent metalinguistic variables and the ::= and | are
metalinguistic connectives. Productions or rewrite rules are
expressed using this machinery. For example the productions
in (8) and (9) generate strings such as (10) and (11)

<ab> ::= ( | [ | <ab> ( | <ab> <d>

(8)

<d> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6

(9)
[(((1(36( (10)
(12345( (11)

BNF productions can be used to construct phrase structure
descriptions of program expressions. For example given a
simple grammar that includes the following production,

<statement> ::=

if <expression> then <statement>
| if <expression> then <statement>

else <statement>

| <other>

one can generate statement (12) where Ei are expressions

and Si are statements

if E1 then if E2 then S1 else S2

(12)

In turn the statement can be parsed to return the tree in ﬁgure
2 (example from [25], p. 211)

FIGURE 2. A parse of the statement if E1 then if E2 then S1 else S2.

Chomsky’s context free grammars were designed to handle
the inherent ambiguities in natural languages, and the BNF
inherited this property, which is undesirable for program-
ming languages. One solution was proposed by Bryan Ford
who introduced Parsing Expression Grammars (PEGs) which
eliminated ambiguity with several new devices including
ordered choice [26]. In fact the current speciﬁcation for
Python 3.9 uses a mixture of BNF and PEG5.

III. LANGUAGE WITHOUT RULES
The story of modern ANNs can be traced back to the Logical
Calculus of McCulloch and Pitts [27] who showed that it
was possible to model the behaviour of networks of neurons
with a logical calculus. This inspired researchers to create
networks of artiﬁcial neurons with complex computational
properties, which gave rise to the current generation of neu-
ral networks, the Deep Learning Networks [28], which can
learn complex non linear transformations implicated in image
recognition, language processing, and other domains. The
promise is that such models can explain complex cognitive
phenomena such as language understanding, without the
need for abstract symbol manipulating machinery.

Churchland argued that cognitive behaviour can be re-
duced to brain states, vis-à-vis parallel neural computations.
Citing the work of Pellionis et al. on the Tensorial approach
to the geometry of brain function [29], she asks us to imagine
that "... arrays of neurons are interpretable as executing
vector-to-vector transformations because that is what they
really are doing − the computational problems a nervous
system has to solve are fundamentally geometrical problems"
[30](p.418). That is, transformations of real valued tensors is
just what it is to be a cognitive agent.

Pinker and Prince [19] described this idea as eliminative
connectionism which are neural systems where it is impos-
sible to ﬁnd a principled mapping between "... the compo-
nents of a PDP6 model and the steps or memory structures
implicated by a symbol-processing theory ...". Computations

3https://betanews.com/2007/03/20/john-w-backus-1924-2007/
4https://www.masswerk.at/algol60/report.htm

5https://docs.python.org/3.9/reference/grammar.html
6Parallel Distributed Processing

VOLUME 4, 2016

3

are performed at a non symbolic level, and any correspond-
ing symbolic descriptions would only serve as short-hand
approximations that could be used, for example, to make
intuitive predictions [19]. The authors further distinguished
eliminative connectionism from implementational connec-
tionism which is a class of systems in which the computations
carried out by collections of neurons are isomorphic to the
structures and symbol manipulations of a symbolic system.
For example, recurrent neural networks with long short-term
memory have been shown to learn very simple context free
and context sensitive languages [31]. More speciﬁcally the
language with sentences of the form anbn is learned through
gate units acting as counters that can keep track of the
number of terminal symbols in simple sequences [31]. It
is therefore crucial to establish which kind of system deep
learning models are, because eliminative neural systems are
the only ones that offer a theoretical alternative to traditional
grammar. Implementational systems would be fully compat-
ible with a rules based linguistic theory and would therefore
be theoretically uninteresting.

According to Marcus [32], a clear criterion for recog-
nising a genuinely eliminative connectionist system is how
it implements compositionality. Compositionality is a char-
acteristic of Classical symbolic architectures, in which the
representation-bearing computational units are structured ex-
pressions, and the operations performed on the expressions
depends on their structure [13]. For example, P follows
from P &Q because of an operation which applies to the
constituent structure of the representation. The P and the Q
in the formula can be of arbitrary complexity and the opera-
tion will apply in exactly the same way. Eliminative ANNs
are not Classical architectures because the operation that
is responsible for transforming the network state from one
which represents P &Q to one which represents P does not
operate by virtue of the constituent structure of the formula.
Instead, the computation involves the network settling on an
activation state representing P following the presentation of
P &Q because of the values of the model parameters which
were adjusted to match the statistical association between
P &Q and P in the training data. In addition, if we substituted
complex novel formulas for P and Q in a Classical system
then the inference to P &Q would still hold, but the same
would not be true in a network model that did not include
the formulas in the training set. Finally if both P and Q were
true in a Classical system then P &Q would also be true. On
the other hand an ANN which was trained to output P when
P &Q was input would not output P &Q when presented with
P and Q on the input, unless it was speciﬁcally trained to do
so.

There are good reasons to believe that deep learning net-
works are to be understood as non compositional, eliminative
connectionist models. Bengio et al. [33] argue that it is the
non-linear transformations between the vectors in a deep
learning architecture that allows the network to perform its
functions, setting them apart from symbolic systems. As an
example, "If Tuesday and Thursday are represented by very

Veres et al.: Language Models are not Models of Language

similar vectors, they will have very similar causal effects
on other vectors of neural activity." [33] p.59. In a Classi-
cal system there is no inherent similarity between the two
symbols "Tuesday" and "Thursday". Overlapping patterns of
inference are only possible by asserting explicit axioms in
the system to establish that the concepts are members of a
collection that behave equivalently in certain circumstances.
In a deep learning network, by contrast, the inferences are
licensed by the similarity between the vectors themselves,
which emerges through the learning process. "Tuesday" and
"Thursday", on this account, have similar effects because
they have overlapping representations learned from the sen-
tences in the training corpus and not because they share
common relations in an axiomatic system.

Additionally, Yun et al. [34] present a proof that deep
learning transformers are universal approximators of contin-
uous sequence-to-sequence functions7 with compact support,
but a critical assumption of the proof is contextual map-
ping of words in a sentence, such that the representation
of individual words depends on the whole sentence. The
proof requires, for example, that the "I" must be mapped
to different vector embeddings in "I am happy" and "I am
Bob". The semantic contribution of the constituent to the
whole is a function of the whole, a direct contradiction to
compositionality.

A. LARGE NEURAL LANGUAGE MODELS
The most successful neural models for NLP are very large
deep learning models that are trained on some version of the
language modeling task, using vast amounts of text [35]. The
models capture intricate statistical generalisations available
in the training corpus which can subsequently be exploited in
task speciﬁc training with much smaller data sets, a paradigm
called transfer learning [35]. Large LMs also appear to
encode structural relations in language and can be probed
in various ways to display aspects of their linguistic compe-
tence. For example Hewitt & Manning [36] argue that vector
based representations in deep learning neural networks can
capture syntactic tree dependencies between words without
an explicit mechanism for constructing parse trees. Using
structural probes with a learned, linear transformation, they
argue that syntax trees are embedded "implicitly" in the deep
learning model’s vector geometry. Furthermore, they suggest
that subject-verb agreement can be solved by using L2 dis-
tance metrics, since the verb that has to agree in number is
closer to the subject than it is to any of the irrelevant attractor
nouns following the transformation.

Goldberg [37] reports a similar ﬁnding, that the BERT
[38] model with no additional ﬁne tuning does remarkably
well on predicting the subject appropriate number marking
on the focus verb in sentences, even if there are mismatching
distractors between the subject and the verb. He concludes
that "exploring the extent to which deep purely attention

7An additional condition is that the function must be permutation equiv-

ariant, unless positional encodings are used.

4

VOLUME 4, 2016

Veres et al.: Language Models are not Models of Language

based architectures such as BERT are capable of capturing
hierarchy-sensitive and syntactic dependencies — as well as
the mechanisms by which this is achieved — is a fascinating
area for future research". In fact a sub ﬁeld of machine learn-
ing that has affectionately come to be known as Bertology
[39], is attempting to do just that.

In summary, neural LMs display some of the represen-
tations that are found in symbolic rule based theories, but
it is not clear how they emerge or how they participate in
computation. A potential explanation is suggested by Nefdt
[40] who proposed a reﬁnement to the idea of composi-
tionality with a distinction between Process-, State-, and
Output- Compositionality. A system is process compositional
if the procedures it computes have meaningful parts, and
those parts are in principle knowable. The meaningful parts
are composed procedurally to give increasingly complex
meaningful expressions. This is essentially the sense of com-
positionality we have ascribed to Classical systems. State
compositionality on the other hand describes representations
in the system that can be decomposed into smaller meaning-
ful units even though they were not generated by a known
compositional process. For example there are many analyses
for ways in which lexical items might be decomposed into
meaningful constituents (e.g. bachelor = unmarried + man),
but the decomposition does not exhaust the range of potential
inferences the lexical item can enter into, nor is there a
compositional process to explain how the constituents can
be combined in learning the concept [41]. Output compo-
sitionality takes a functional view in which for any input
tokens t there is a function f which produces an output o that
is state compositional. Output compositionality is a special
case of state compositionality because it does not imply
that the entire analysis needs to be state compositional, it
is sufﬁcient that local outputs are. Neural language models
would on this account exhibit output compositionality, such
that various structured states can be observed, but these states
do not reveal meaningful constituents which are causally
responsible for the evolution of the system. The discovery
of local compositional structure does not reveal how those
structures participate in the evolution of the system ([42, 43,
33]).

IV. NEURAL NETWORK MODELS AND SOFTWARE
CODE
Hindle et al. [44] proposed that the majority of software that
people write can be described as natural programs which are
relatively simple, repetitive code that is used for communi-
cation with other programmers as much as it is to provide
machine instructions. They successfully trained N-gram lan-
guage models on software code to show that programming
languages share many of the regularities observed in natural
languages. They then used the trained model as a code
suggestion plugin for the Eclipse Interactive Development
Environment, resulting in keystroke savings of up to 61%
over the built in suggestion engine [44].

Subsequent research expanded on this work by training

signiﬁcantly larger language models for mining source code
repositories and providing new complexity metrics [45], and
using deep learning networks for code completion [46].

In a popular blog post, Andrej Karpathy drew our atten-
tion to the Unreasonable Effectiveness of Recurrent Neural
Networks8 with some illustrations of RNNs learning from
character level data. He showed that models could learn
to generate natural language prose, prose which includes
markdown, valid XML, LaTeX, and source code in the C
programming language. The RNN generated both LaTeX and
C code with remarkable syntactic ﬁdelity including spacing,
matching brackets, variable declarations and so on. Observed
errors were principally semantic. For example, one example
of LaTeX code had a 13 line environment beginning with
\begin{proof} and ending with \end{lemma}. That
is, the coreference dependency is partially broken; while the
\begin{} block is closed as required by the syntax, the
RNN has not correctly remembered the exact identity of
the block. Similarly the C code contains very few syntactic
errors, but contains semantic errors such as variable names
not being used consistently, or using undeﬁned variables.

A. LARGE NEURAL LANGUAGE MODELS AND CODE
SYNTHESIS
Hendrycks et al. [47] released APPS, an ambitious dataset
and benchmark for measuring the effectiveness of machine
learning models in a realistic code generation framework,
involving natural language problem speciﬁcations and func-
tional test cases. The problems have three levels of difﬁculty;
introductory, interview, and competitive. The following ex-
ample is the natural language description in an "interview"
level problem.

You are given two integers n and m. Calculate the
number of pairs of arrays (a, b) such that: the length
of both arrays is equal to m; each element of each
array is an integer between 1 and n (inclusive); ai
≤ bi for any index i from 1 to m; array a is sorted
in non-descending order; array b is sorted in non-
ascending order. As the result can be very large,
you should print it modulo 109 + 7.
Input: The only line contains two integers n and m
(1 ≤ n ≤ 1000, 1 ≤ m ≤ 10). Output: Print one
integer - the number of arrays a and b satisfying
the conditions described above modulo 109 + 7.
—–Examples—–
Input: 2 2, Output: 5
Input: 10 1, Output: 55
Input: 723 9, Output: 157557417

The authors gathered 10,000 coding problems with
131,836 test cases for checking the generated solutions, and
232,444 gold-standard solutions written by humans. They
tested four models for their ability to generate code that

8http://karpathy.github.io/2015/05/21/rnn-effectiveness/ The title is a play
on The Unreasonable Effectiveness of Mathematics in the Natural Sciences,
a 1960 article by the Nobel Prize winning physicist Eugene Wigner.

VOLUME 4, 2016

5

could solve the test cases; code which is both syntactically
well formed and semantically correct. The models they tested
were GPT-2 [48] (two versions, with 0.1 and 1.5 Billion
parameters), GPT-Neo (2.7B parameters) [49] and GPT-3 [7]
(175B parameters). The GPT-2 models received additional
pre-training with 30 GB of Python code from GitHub. GPT-
Neo was trained on the Pile [50] which already contains code
from GitHub. GPT-3 had no prior training on code. All but
GPT-3 were then further ﬁne tuned on the APPS dataset,
while GPT-3 was tested in a few-shot scenario.

The results showed that in general bigger models were
better than the smaller models, and pre-training on code is
critical since the largest GPT-3 model without pre-training
(or ﬁne tuning) was outperformed by the smallest GPT-2
model.

In general this proved to be a difﬁcult task and the best
model passed only 14.68% of test cases on the introductory
problems, and 6.54% for the ’competitive’ problems. On a
strict criterion where success was deﬁned as passing all of the
test cases, these ﬁgures dropped to 3.9% and 0% respectively.
However, the important ﬁnding for our argument was the
extremely low rate of syntax errors in the code. Remarkably,
GPT-Neo generated code that failed because of syntax errors
on only 3% of its solutions on the "introductory" problems,
increasingly to approximately 5% for the "competition" prob-
lems.

Listing 1 shows a machine synthesized solution for the the
example problem. The solution attempts to build arrays as
described, to return a result which depends on those arrays,
and to provide the answer as modulo 109 + 7. However the
logic it uses to build and compare the arrays is faulty and the
results incorrect.

1

2
3 n, m = map(int, input().split())
4 mod = 10**9+7
5
6 fact = [1, 1]
7 factinv = [1, 1]
8 inv = [0, 1]
9
10 for i in range(2, n+1):
11

fact.append((fact[-1] * i) % mod)
inv.append((-inv[mod % i] * (mod//i)) % mod)
factinv.append((factinv[-1] * inv[-1]) % mod)

12

13

14
15 def cmb(n, r, mod):
16

if (r < 0) or (n < r):

17

18

19

return 0
r = min(r, n - r)
return fact[n] * factinv[r] * factinv[n-r] %
mod

20
21 print((cmb(n, m, mod) * cmb(m-1, n-1, mod)) % mod)
Listing 1. Example solution from a model tested on APPS. This solution
passes 0 test cases but it is a legal Python program.

Austin et al. [51] released a somewhat easier dataset, the
Mostly Basic Programming Problems (MBPP), comprising
natural language descriptions, concrete test cases, and gold

Veres et al.: Language Models are not Models of Language

standard solutions. They argued that the poor model perfor-
mance reported by [47] was due to the difﬁcult nature of
the problems. In particular the competition set was written in
a manner that purposely obfuscated the connection between
problem statement and solution code. The MBPP was there-
fore designed to maintain the clarity of the natural language
descriptions and to be solvable by entry-level programmers.
The problems and solutions were provided by crowd-sourced
workers and subsequently inspected by the authors. The mod-
els used for the experiments were BERT-style transformer
models [4] with parameter counts ranging from 244 million
to 137 billion, and pre-trained on a combination of web doc-
uments, dialog data, and Wikipedia. The pre-training dataset
contained 2.97B documents, of which 13.8M were web sites
that contained some code and text, although complete source
code ﬁles were not included.

The experiments showed a log-linear improvement in per-
formance with model size, and only marginal improvement
of ﬁne tuning over the pre-trained models. The largest model
with 137 billion parameters could solve approximately 60%
of the problems with at least one generated solution, while the
smallest 244 million parameter model could solve fewer than
5%. The models beneﬁted from ﬁne-tuning by approximately
a constant 10% from the 422 million to the 68 billion param-
eter model. As a result the beneﬁt of ﬁne-tuning decreased
proportionately as model size increased.

The qualitative error analysis revealed that the main reason
for failure was not syntactic errors but problems with the code
semantics. Even the smallest models produced syntactically
correct Python code about 80% of the time, increasing to over
90% for the larger models. The most commonly observed
semantic errors were in complex problems with multiple
constraints or sub-problems, where the generated code only
solved one sub problem. For example the problem "Write a
function to ﬁnd the longest palindromic subsequence in the
given string" might result in code that found just one of the
palindromic sequences but not the longest one. Another pos-
sible mistake was code that solved a problem that was similar,
but more common than the provided one. For example the
query “Write a python function to ﬁnd the largest number
that can be formed with the given list of digits." might result
in a program that calculated the largest digit in the list.

One difﬁculty with this ﬁnding is that it is not possible
to identify the exact locus of the semantic errors. It could,
for example be the natural language component which fails
to extract the correct speciﬁcation. In order to investigate
the level of semantic understanding in the code synthesis
component, the authors tested to see how well the model
could predict the output of the ground truth source code. If
the model had some understanding of the semantics of code
then this should manifest itself in learning code execution.
Previous work has found that different architectures were
in fact capable of learning to execute code [52]. In a few-
shot setting where only the code was presented as a prompt
and the model had to predict the output in an input-output
test case, the accuracy was 16.4%, and improved to 28.8%

6

VOLUME 4, 2016

Veres et al.: Language Models are not Models of Language

when two input-output example pairs were also presented.
These results dropped to 8.6% and 11.6% respectively when
2 test cases were presented (and the model had to answer
both correctly). More interestingly, when only the natural
language problem description and an example input-output
prompt was presented the model performed more accurately,
at 12.8%. Finally, the model performed better when it was
presented with only the examples as prompt then it did
with the code itself, with no example (10.2% example only,
8.6% code only). Together the results suggest that the model
was not considering the semantics of the code at all when
predicting the results of execution. The model appeared to be
using previously observed patterns rather any understanding
of the code.

OpenAI Codex9 is an experimental API from the Mi-
crosoft owned company which powers GitHub Copilot10.
Codex is a GPT-3 based model ﬁne tuned on publicly avail-
able code, which generates Python code from docstrings [53].
For testing purposes the authors introduced the HumanEval
evaluation set which includes 164 manually constructed pro-
gramming problems consisting of a function signature, doc-
string, body, and several unit tests. Listing 2 shows an exam-
ple problem together with a correct and an incorrect solution.
The incorrect solution supports Austin et al.’s analysis that
the errors are not syntactic but semantic, in this case caused
by solving only a part of the problem. In this example the
generated algorithm fails to consider the boundary condition
where n = 1.

1
2 def is_prime(n):
3

"""Return true if a given number is prime, and

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

false otherwise.

>>> is_prime(6) False
>>> is_prime(101) True
>>> is_prime(11) True
>>> is_prime(13441) True
>>> is_prime(61) True
>>> is_prime(4) False
>>> is_prime(1) False
"""
### COMPLETION 6 (CORRECT): ###
prime = True
if n == 1:

return False
for i in range(2, n):
if n % i == 0:

prime = False

return prime

### COMPLETION 7 (WRONG): ###
for i in range(2, n):
if n % i == 0:

return False

return True

Listing 2. Example HumanEval docstring and a correct and incorrect
completion

They evaluated (a) a 12B parameter vanilla GPT-3 model,
(b) Codex, which is GPT-3 ﬁne-tuned on 159GB of code

9https://openai.com/blog/openai-codex/
10https://copilot.github.com/

VOLUME 4, 2016

collected from GitHub, and (c) Codex-S which is further
ﬁne-tuned on correctly implemented standalone functions.
When looking at single solutions the vanilla GPT-3 model
was unable to solve any HumanEval problems, while the 12B
parameter Codex solved 28.8%. Fine-tuning this model on
function solutions produced Codex-S which solved 37.7% of
the problems, which was a bigger gain from ﬁne-tuning than
observed on the other studies. When considering multiple so-
lutions, Codex-S was able to ﬁnd at least one correct solution
72.31% of the time from among 100 candidate solutions, and
46.81% of the time among 10 solutions.

Probing error points once again provided some interesting
insights. The authors created "building blocks" which were
simple tasks such as “convert the string s to lowercase”,
“remove all vowels from the string”, and “replace spaces with
triple spaces”. When the building blocks were systematically
concatenated to form increasingly complex function descrip-
tions, model performance decreased exponentially with in-
creasing problem complexity. This pattern is not likely to be
observed with human programmers, as the task is simply to
link simple discrete functions together.

In summary, deep learning neural network models are
beginning to have impressive success in generating syntacti-
cally correct code from natural language speciﬁcations. The
various models demonstrate not only a knowledge of pro-
gramming constructs like variable declaration and operation
signatures, but also constructs that depend on constituency
structure like long distance dependencies (e.g. matching
brackets, matching if-then imperatives). On the other hand
the knowledge of semantics is either absent or very limited.

V. LARGE NEURAL LANGUAGE MODELS ARE CORPUS
MODELS
The surprising success of neural LMs to synthesize computer
code has resulted in some puzzling claims. For example [51]
speculate that for models "it is not necessary to explicitly
encode the grammar of the underlying language − they
learn it from data." Taken literally the claim would be that
the neural network is implementational after all, since it
is learning the grammar of a programming language. We
have already pointed out the problems with this position
when applied to models of natural language, and suggest
that similar problems exist in the domain of programming
languages.

The most parsimonious hypothesis is that language models
perform natural language and programming tasks through the
same mechanisms, which is not that they learn the grammar
of the language. The evidence that neural network repre-
sentations can encode hierarchical components in language
(e.g. [36, 43]) presents the possibility that the same repre-
sentations are used in encoding structural relationships in
software code. Some support for this comes from [54] who
developed a novel method for probing neural LMs for their
ability to encode natural language syntax. In order to validate
their method they used neural models trained on a synthetic
language for arithmetic expressions with a simple syntax,

7

concluding that the neural network encoding of the structural
patterns generated by an artiﬁcial grammar are analogous to
their encoding of natural language structures.

It is important to point out once again that programming
languages have an exact grammar which is, by hypothesis,
not learned by the LM. Therefore the LM’s ability to ab-
stract hierarchical relationships by some means other than a
grammar, is a powerful tool which can result in impressive
linguistic performance as well.

It is clear that large LMs are able to generate syntactically
well formed productions in domains where they receive large
amounts of training data, whether that be natural language
or software code. Since the data is their only resource for
learning, they can be susceptible to the characteristics of the
data set. For example, Bender et al. [9] documented many
different selection biases which can affect which utterances
are included in corpora. Perhaps even more problematical is
that the performance of language models on language tasks
is strongly inﬂuenced by the diversity of the training corpora
[50, 55], which shows that what is modeled is not language
per se, but the characteristics of the training corpora. Simi-
larly Brown et al. [7] used a ﬁltered version of the Common
Crawl dataset11, a collection of text based on 12 years of web
crawling, and found that the unﬁltered version gave poorer
results on language tasks than the higher quality version
which was ﬁltered in part through a comparison with text in
written books.

If the quality of a dataset can inﬂuence the performance
of the model, then there should be clear guidelines as to
what constitutes a high quality dataset. To the best of our
knowledge the answer to this in the neural LM literature is
rather vague. Take for example the Pile, a dataset speciﬁcally
designed to be a large, diverse and high-quality resource for
machine learning [50]. Unfortunately the authors are not ex-
plicit about what they regard as "high-quality", even though
they use the descriptor 33 times in their paper. Some clues
emerge through the description of individual data sources
such as Wikipedia which is "... a source of high quality,
clean English text ..." (p.4), suggesting as one criterion a text
written in complete English sentences with some editorial
control. Also mentioned is the Common Crawl web content
where one challenge is "... difﬁculty of cleaning and ﬁltering
the Common Crawl data ..." due to low level problems such
as presence of non text characters, segmentation problems,
capitalization, etc. [12]. Common Crawl data also contains
duplicate lines which often signals highly repetitive "boiler-
plate text", which can reduce performance on downstream
tasks [56]. Grave et al. [57] tried to ﬁlter Common Crawl by
using Wikipedia "because the articles are curated, the corre-
sponding text is of high quality". Again, editorial control is
an important factor. Another technique for increasing quality
by comparing against Wikipedia was used by Wenzek et al.
[58] who measured the perplexity score of target web pages
against a language model trained on Wikipedia, and took the

11https://commoncrawl.org/the-data/

8

Veres et al.: Language Models are not Models of Language

perplexity as a measure of quality. Again the strategy is to
assume Wikipedia is high quality and use it to ﬁlter other
content.

The practice is problematic in our view because it is
important to be speciﬁc about what should be accepted as
appropriate input to a "language model", as the data is
literally the "language" in the model. In linguistic theory, as
we saw earlier, Chomsky proposed grammatical acceptability
judgement as a way to decide which utterances counted as
valid data for linguistic enquiry. But this view is criticised
in statistical NLP. For example Manning [59] argued against
the Chomskyan notion of grammaticality and proposed prob-
abilistic syntax largely on the strength of examples which
purportedly show the non categorical nature of grammar.
Norvig [20] argued even more forcefully that "... people
have to continually understand the uncertain, ambiguous,
noisy speech of others ... Chomsky for some reason wants
to avoid this, and therefore he must declare the actual facts of
language use out of bounds and declare that true linguistics
only exists in the mathematical realm ... Chomsky dislikes
statistical models in that they tend to make linguistics an
empirical science (a science about how people actually use
language) ..." So for Norvig the data for the study of lnguage
is the "actual use" of language which are the noisy and
uncertain utterances of the everyday. But this view makes it
difﬁcult to justify eliminating any "low quality" utterances
reﬂecting how people "actually use language" from the cor-
pus. If we are going to judge dataset quality by comparing
it against the edited and grammatical text in "high quality"
sources such as Wikipedia, then we need to be careful with
our theoretical assumptions. The problem is that we might be
smuggling grammaticality in by the back door.

For the preceding reasons we would suggest a clariﬁcation
in terminology, and propose a change from the theory-laden
term language model to the more objectively accurate term
corpus model. Not only does the term corpus model better
reﬂect the contents of models, it also provides transparency in
discussing issues such as model bias. One might be surprised
if a language model is biased, or if there is different bias in
two different language models, but a bias in corpus models
and different biases in different corpus models is almost an
expectation. Natural language is not biased. What people say
or write can be biased.

VI. CONCLUSION
We considered the challenge that deep learning neural models
present to traditional generative phrase structure theories of
natural language, and showed the challenge to be invalid
since the arguments could equally and absurdly be applied
against phrase structure in software code. We claim our
argument to be more powerful and permanent than more
traditional attempts to prove that neural networks cannot
perform particular tasks. This style of argument has an il-
lustrious history in the ﬁeld, where research activity was
drastically reduced for decades by the publication of Min-
sky and Pappert’s Perseptrons, which showed fundamental

VOLUME 4, 2016

Veres et al.: Language Models are not Models of Language

computational limitations of the contemporary neural models
[60]. More sophisticated models were eventually devised to
overcome these limitations, and the previously devastating
criticisms vanished. Our argument is more powerful because
it is predicated on the success of neural models. We argue
that achieving high performance on arbitrary NLP tasks is
irrelevant to theories of natural language in general, and
generative grammar in particular, because the same argu-
ments can apply equally to programming languages which
are clearly the product of a generative grammar.

As a corollary we argued that the term "language model"
is misleading. A more accurate and useful term would be
"corpus model". We hope this clariﬁcation is useful for prac-
tical work as well as scientiﬁc discovery, and look forward
to theoretical insights that can be gained by exploring the
similarities between the statistical epiphenomena created by
natural languages and computer software in shared corpus
models.

Pinker and Prince argued that the connectionist models of
the time failed to deliver a "radical restructuring of cognitive
theory" [19](p.78), because they did not adequately model
relevant linguistic phenomena. We argue that modern neural
models similarly fail, but from the opposite perspective. In
becoming universal mimics that can imitate the behaviour
of clearly rule driven processes, they become uninformative
about the true nature of the phenomena they are "parroting"
[9]. Enormous amounts of training data and advances in
compute power have made the modern incarnation of artiﬁ-
cial neural networks tremendously capable in solving certain
problems that previously required human-like intelligence,
but just like their predecessors, they have failed to deliver
a revolution in our understanding of human cognition.

References
[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hin-
ton. “Deep learning”. In: Nature 521.7553 (2015),
pp. 436–444.
ISSN: 0028-0836. DOI: 10 . 1038 /
nature14539.

[3]

[2] Tara Balakrishnan, Michael Chui, Bryce Hall, and
Nicolaus Henke. The state of AI in 2020: November
17, 2020 Survey. 2020. URL: https://www.mckinsey.
com / business - functions / mckinsey - analytics / our -
insights / global - survey - the - state - of - ai - in - 2020.
(accessed: 07.10.2021).
Jacques Bughin, Eric Hazan, Sree Ramaswamy,
Michael Chui, Tera Alla, Peter Dahlström, Nico-
laus Henke, and Monica Trench. Artiﬁcial Intelli-
gence: the Next Digital Frontier? A McKinsey pa-
per. https : / / www . mckinsey . com / ~ / media /
mckinsey/industries/advancedelectronics/ourinsights/
howartiﬁcialintelligencecandeliverrealvaluetocompanies/
mgi - artiﬁcial - intelligence - discussion - paper . ashx.
Accessed: 07.10.2021. 2017.

[4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz

Kaiser, and Illia Polosukhin. Attention Is All You Need.
2017. arXiv: 1706.03762 [cs.CL].

[5] Pandu Nayak. Understanding searches better than
ever before. URL: https://www.blog.google/products/
search / search - language - understanding - bert/. (ac-
cessed: 16-11-2021).

[6] Kevin Scott. Microsoft teams up with OpenAI to ex-
clusively license GPT-3 language model. URL: https:
//blogs.microsoft.com/blog/2020/09/22/microsoft-
teams-up-with-openai-to-exclusively-license-gpt-3-
language-model/. (accessed: 16-11-2021).

[7] Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Lan-
guage Models are Few-Shot Learners. 2020. arXiv:
2005.14165 [cs.CL].

[8] Emily M Bender and Alexander Koller. “Climbing
towards NLU: On Meaning, Form, and Understanding
in the Age of Data”. In: Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics (2020), pp. 5185–5198. DOI: 10.18653/v1/
2020.acl-main.463.

[9] Emily M. Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. “On the Dangers
of Stochastic Parrots: Can Language Models Be Too
Big?” In: Proceedings of
the 2021 ACM Confer-
ence on Fairness, Accountability, and Transparency.
FAccT ’21. Virtual Event, Canada: Association for
Computing Machinery, 2021, pp. 610–623.
ISBN:
9781450383097. DOI: 10 . 1145 / 3442188 . 3445922.
URL: https://doi.org/10.1145/3442188.3445922.
[10] Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. Introduction to Information Re-
trieval. Cambridge, UK: Cambridge University Press,
2008. ISBN: 978-0-521-86571-5. URL: http : / / nlp .
stanford . edu / IR - book / information - retrieval - book .
html.

[11] Yoshua Bengio, Réjean Ducharme, Pascal Vincent,
and Christian Jauvin. “A Neural Probabilistic Lan-
guage Model”. In: Journal of Machine Learning Re-
search 3 (2003), pp. 1137–1155.

[12] Christopher Manning and Hinrich Schütze. Foun-
dations of Statistical Natural Language Processing.
Cambridge, Massachusetts: MIT Press, 1999.
Jerry A. Fodor and Zenon W. Pylyshyn. “Connection-
ism and cognitive architecture: A critical analysis”.
In: Cognition 28.1 (1988), pp. 3–71. ISSN: 0010-0277.
DOI: https://doi.org/10.1016/0010-0277(88)90031-5.

[13]

VOLUME 4, 2016

9

URL: https://www.sciencedirect.com/science/article/
pii/0010027788900315.

[14] Frederick J. Newmeyer. The History of Modern Lin-
guistics. 2014. URL: https : / / www. linguisticsociety.
org / resource / history - modern - linguistics (visited on
10/07/2021). (accessed: 07.10.2021).

[15] Howard Lasnik and Terje Lohndal. “Brief overview of
the history of generative syntax”. In: The Cambridge
Handbook of Generative Syntax. Ed. by MarcelEdi-
tor den Dikken. Cambridge Handbooks in Language
and Linguistics. Cambridge University Press, 2013,
pp. 26–60. DOI: 10.1017/CBO9780511804571.007.

[16] Noam Chomsky. “Three models for the description
of language”. In: IRE Transactions on Information
Theory 2.3 (1956), pp. 113–124. DOI: 10.1109/TIT.
1956.1056813.

[17] Noam Chomsky. Syntactic Structures. Mouton & Co.,

1957.

[18] Ewa D ˛abrowska. “What exactly is Universal Gram-
mar, and has anyone seen it?” In: Frontiers in Psychol-
ogy 6 (2015), p. 852. ISSN: 1664-1078. DOI: 10.3389/
fpsyg.2015.00852. URL: https://www.frontiersin.org/
article/10.3389/fpsyg.2015.00852.

[19] Steven Pinker and Alan Prince. “On language and
connectionism: Analysis of a parallel distributed pro-
cessing model of language acquisition”. In: Cognition
28.1-2 (1988), pp. 73–193. ISSN: 0010-0277. DOI: 10.
1016/0010-0277(88)90032-7.

[20] Peter Norvig. On Chomsky and the Two Cultures of
Statistical Learning. 2012. URL: http : / / norvig . com /
chomsky . html (visited on 10/07/2021). (accessed:
22.11.2021).

[21] Geoffrey Hinton. Geoffrey Hinton and Yann LeCun,
2018 ACM A.M. Turing Award Lecture "The Deep
Learning Revolution". 2019. URL: https : / / youtu . be /
VsnQf7exv5I?t=1945 (visited on 10/11/2021).
[22] Leonard Bloomﬁeld. Language. New York: Holt,

Rinehart & Winston, 1933.

[23] Howard Lasnik and Terje Lohndal. “Phrase structure
grammar”. In: The Cambridge Handbook of Genera-
tive Syntax. Ed. by MarcelEditor den Dikken. Cam-
bridge Handbooks in Language and Linguistics. Cam-
bridge University Press, 2013, pp. 202–225. DOI: 10.
1017/CBO9780511804571.007.

[24] Noam Chomsky. Aspects of the Theory of Syntax.
Cambridge, Massachusetts: MIT Press, 1965.
[25] Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey
D. Ullman. Compilers: Principles, Techniques, and
Tools (2nd Edition). USA: Addison-Wesley Longman
Publishing Co., Inc., 2006. ISBN: 0321486811.
[26] Bryan Ford. “Parsing Expression Grammars: A
Recognition-Based Syntactic Foundation”. In: Pro-
ceedings of the 31st ACM SIGPLAN-SIGACT Sym-
posium on Principles of Programming Languages.
POPL ’04. Venice, Italy: Association for Computing
Machinery, 2004, pp. 111–122. ISBN: 158113729X.

Veres et al.: Language Models are not Models of Language

DOI: 10.1145/964001.964011. URL: https://doi.org/
10.1145/964001.964011.

[27] Warren S McCulloch and Walter Pitts. “A logical
calculus of the ideas immanent in nervous activity”.
In: The bulletin of mathematical biophysics 5.4 (1943),
pp. 115–133.

[28] Yann LeCun, Yoshua Bengio, and Geoffrey Hin-
ton. “Deep learning”. In: Nature 521.7553 (2015),
pp. 436–444.
ISSN: 0028-0836. DOI: 10 . 1038 /
nature14539.

[29] A. J. Pellionisz and Rodolfo R. Llinás. “Tensorial
approach to the geometry of brain function: Cerebellar
coordination via a metric tensor”. In: Neuroscience 5
(1980), pp. 1125–1136.

[30] Patricia S. Churchland. Neurophilosophy: Toward a

Uniﬁed Science of the Mind-Brain. Bradford, 1986.

[31] F.A. Gers and E. Schmidhuber. “LSTM recurrent net-
works learn simple context-free and context-sensitive
languages”. In: IEEE Transactions on Neural Net-
works 12.6 (2001), pp. 1333–1340. DOI: 10.1109/72.
963769.

[32] Gary F. Marcus. “Rethinking Eliminative Connection-
ism”. In: Cognitive Psychology 37.3 (1998), pp. 243–
282. ISSN: 0010-0285. DOI: 10.1006/cogp.1998.0694.
[33] Yoshua Bengio, Yann Lecun, and Geoffrey Hinton.
“Deep learning for AI”. In: Communications of the
ACM 64.7 (2021), pp. 58–65. ISSN: 0001-0782. DOI:
10.1145/3448250.

[34] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh
Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are
Transformers universal approximators of sequence-
to-sequence functions? 2020. arXiv: 1912 . 10077
[cs.LG].

[36]

[35] Rishi Bommasani et al. On the Opportunities and
Risks of Foundation Models. 2021. arXiv: 2108.07258
[cs.LG].
John Hewitt and Christopher D. Manning. “A Struc-
tural Probe for Finding Syntax in Word Representa-
tions”. In: Proceedings of the 2019 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers). Minneapolis,
Minnesota: Association for Computational Linguis-
tics, June 2019, pp. 4129–4138. DOI: 10 . 18653 / v1 /
N19-1419. URL: https://aclanthology.org/N19-1419.

[37] Yoav Goldberg. Assessing BERT’s Syntactic Abilities.

[38]

2019. arXiv: 1901.05287 [cs.CL].
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. BERT: Pre-training of Deep Bidi-
rectional Transformers for Language Understanding.
2019. arXiv: 1810.04805 [cs.CL].

[39] Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
A Primer in BERTology: What we know about how
BERT works. 2020. arXiv: 2002.12327 [cs.CL].

[40] Ryan M. Nefdt. “A Puzzle concerning Compositional-
ity in Machines”. In: Minds and Machines 30.1 (2020),

10

VOLUME 4, 2016

Veres et al.: Language Models are not Models of Language

[41]

pp. 47–75. ISSN: 0924-6495. DOI: 10.1007/s11023-
020-09519-6.
Jerry A. Fodor. “The Present Status of the Innate-
ness Controversy”. In: RePresentations: Philosophi-
cal Essays on the Foundations of Cognitive Science.
Ed. by Jerry Fodor. Cambridge, MA: MIT Press, 1981,
pp. 257–316.

[42] Yoshua Bengio. “Learning Deep Architectures for AI.
Foundations Trends Machine Learning, vol. 2 (1)”. In:
(2009).
Ian Tenney, Dipanjan Das, and Ellie Pavlick. “BERT
Rediscovers the Classical NLP Pipeline”. In: arXiv
(2019). eprint: 1905.05950.

[43]

[44] Abram Hindle, Earl T. Barr, Mark Gabel, Zhen-
dong Su, and Premkumar Devanbu. “On the Natu-
ralness of Software”. In: Commun. ACM 59.5 (Apr.
2016), pp. 122–131. ISSN: 0001-0782. DOI: 10.1145/
2902362. URL: https://doi.org/10.1145/2902362.
[45] Miltiadis Allamanis and Charles Sutton. “Mining
Source Code Repositories at Massive Scale Using
Language Modeling”. In: Proceedings of the 10th
Working Conference on Mining Software Repositories.
MSR ’13. San Francisco, CA, USA: IEEE Press, 2013,
pp. 207–216. ISBN: 9781467329361.

[46] Martin White, Christopher Vendome, Mario Linares-
Vásquez, and Denys Poshyvanyk. “Toward Deep
Learning Software Repositories”. In: Proceedings of
the 12th Working Conference on Mining Software
Repositories. MSR ’15. Florence, Italy: IEEE Press,
2015, pp. 334–345. ISBN: 9780769555942.

[47] Dan Hendrycks, Steven Basart, Saurav Kadavath,
Mantas Mazeika, Akul Arora, Ethan Guo, Collin
Burns, Samir Puranik, Horace He, Dawn Song,
and Jacob Steinhardt. “Measuring Coding Chal-
lenge Competence With APPS”. In: arXiv preprint
arXiv:2105.09938 (2021).

[48] Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. “Language Models
are Unsupervised Multitask Learners”. In: 2019.
[49] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and
Stella Biderman. GPT-Neo: Large Scale Autoregres-
sive Language Modeling with Mesh-Tensorﬂow. Ver-
sion 1.0. 2021. URL: http://github.com/eleutherai/gpt-
neo.

[50] Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, Shawn Presser,
and Connor Leahy. The Pile: An 800GB Dataset of
Diverse Text for Language Modeling. 2020. arXiv:
2101.00027 [cs.CL].
Jacob Austin, Augustus Odena, Maxwell Nye,
Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and
Charles Sutton. “Program Synthesis with Large Lan-
guage Models”. In: arXiv (2021). eprint: 2108.07732.

[51]

[52] David Bieber, Charles Sutton, Hugo Larochelle,
and Daniel Tarlow. “Learning to Execute Programs
with Instruction Pointer Attention Graph Neural Net-
works”. In: Advances in Neural Information Process-
ing Systems. Ed. by H. Larochelle, M. Ranzato, R.
Hadsell, M. F. Balcan, and H. Lin. Vol. 33. Curran
Associates, Inc., 2020, pp. 8626–8637. URL: https :
/ / proceedings . neurips . cc / paper / 2020 / ﬁle /
62326dc7c4f7b849d6f013ba46489d6c-Paper.pdf.
[53] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela
Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-
hammad Bavarian, Clemens Winter, Philippe Tillet,
Felipe Petroski Such, Dave Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-Voss, William Hebgen Guss, Alex Nichol,
Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,
Suchir Balaji, Shantanu Jain, William Saunders,
Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Rad-
ford, Matthew Knight, Miles Brundage, Mira Murati,
Katie Mayer, Peter Welinder, Bob McGrew, Dario
Amodei, Sam McCandlish, Ilya Sutskever, and Woj-
ciech Zaremba. “Evaluating Large Language Models
Trained on Code”. In: (2021). arXiv: 2107 . 03374
[cs.LG].

[54] Grzegorz Chrupała and Afra Alishahi. “Correlating
Neural and Symbolic Representations of Language”.
In: Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics (2019),
pp. 2952–2962. DOI: 10.18653/v1/p19-1283.
[55] Turing-NLG language model. Turing-NLG: A 17-
billion-parameter language model by Microsoft. 2020.
URL: https : / / www. microsoft . com / en - us / research /
blog / turing - nlg - a - 17 - billion - parameter- language -
model - by - microsoft/ (visited on 02/13/2020). (ac-
cessed: 22.11.2021).

[56] Katherine Lee, Daphne Ippolito, Andrew Nystrom,
Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,
and Nicholas Carlini. “Deduplicating Training Data
Makes Language Models Better”. In: arXiv (2021).
eprint: 2107.06499.

[57] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. “Learning Word
Vectors for 157 Languages”. In: arXiv (2018). eprint:
1802.06893.

[58] Guillaume Wenzek, Marie-Anne Lachaux, Alexis
Conneau, Vishrav Chaudhary, Francisco Guzmán, Ar-
mand Joulin, and Edouard Grave. “CCNet: Extracting
High Quality Monolingual Datasets from Web Crawl
Data”. In: arXiv (2019). eprint: 1911.00359.

VOLUME 4, 2016

11

Veres et al.: Language Models are not Models of Language

[59] Christopher Manning. “Probabilistic Syntax”.

In:
Probabilistic Linguistics. Ed. by Rens Bod, Jen-
nifer Hay, and Stefanie Jannedy. Cambridge, Mas-
sachusetts: MIT Press, 2003. Chap. 8, pp. 289–342.

[60] Marvin Minsky and Seymour Papert. Perceptrons; an
introduction to computational geometry. Cambridge,
Massachusetts: MIT Press, 1969.

CSABA VERES was born in Budapest in 1964.
Csaba received the Ph.D. degree in cognitive sci-
ence from the University of Arizona, Tucson. His
thesis was in the ﬁeld of Psycholinguistics, where
he studied the role of meaning on sentence parsing
and representation. He subsequently began work
as a computer and information scientist at Mel-
bourne University in Australia.

He is currently Full Professor at the Department
of Information Science and Media Studies at the
University of Bergen, Norway. His areas of expertise include NLP, machine
learning, and semantic web technologies. He has experience as an Academic
and Practitioner. He founded a Norwegian company called LexiTags, and
consulted as Head of AI with the London based educational technology
startup, Flooved. He has published a wide assortment of original research
articles, and has had popular linked data apps on the Apple store, called
MapXplore and AuotoMind. He has also held positions as a Research Sci-
entist at the Australian Defence Science and Technology Organisation, and
as a Senior Lecturer in the Department of Information Systems, Melbourne
University.

12

VOLUME 4, 2016

