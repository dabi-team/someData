To What Extent do Deep Learning-based Code Recommenders
Generate Predictions by Cloning Code from the Training Set?

Matteo Ciniselli
SEART @ Software Institute
Università della Svizzera italiana
Switzerland

Luca Pascarella
SEART @ Software Institute
Università della Svizzera italiana
Switzerland

Gabriele Bavota
SEART @ Software Institute
Università della Svizzera italiana
Switzerland

ABSTRACT
Deep Learning (DL) models have been widely used to support code
completion. These models, once properly trained, can take as in-
put an incomplete code component (e.g., an incomplete function)
and predict the missing tokens to finalize it. GitHub Copilot is
an example of code recommender built by training a DL model
on millions of open source repositories: The source code of these
repositories acts as training data, allowing the model to learn “how
to program”. The usage of such a code is usually regulated by Free
and Open Source Software (FOSS) licenses, that establish under
which conditions the licensed code can be redistributed or modified.
As of Today, it is unclear whether the code generated by DL models
trained on open source code should be considered as “new” or as
“derivative” work, with possible implications on license infringe-
ments. In this work, we run a large-scale study investigating the
extent to which DL models tend to clone code from their training
set when recommending code completions. Such an exploratory
study can help in assessing the magnitude of the potential licensing
issues mentioned before: If these models tend to generate new code
that is unseen in the training set, then licensing issues are unlikely
to occur. Otherwise, a revision of these licenses urges to regulate
how the code generated by these models should be treated when
used, for example, in a commercial setting. Highlights from our
results show that ∼10% to ∼0.1% of the predictions generated by a
state-of-the-art DL-based code completion tool are Type-1 clones of
instances in the training set, depending on the size of the predicted
code. Long predictions are unlikely to be cloned.

CCS CONCEPTS
• Software and its engineering → Software maintenance tools.

KEYWORDS
Deep Learning, Code Completion, Code Clones

2
2
0
2

r
p
A
4
1

]
E
S
.
s
c
[

1
v
4
9
8
6
0
.
4
0
2
2
:
v
i
X
r
a

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9303-4/22/05. . . $15.00
https://doi.org/10.1145/3524842.3527938

1 INTRODUCTION
Code completion is nowadays a “killer” feature for Integrated De-
velopment Environments (IDEs) [7, 35, 48], and it can substantially
speed up implementation activities by recommending to the devel-
oper the next code token(s) to write [17, 18]. Early code completion
techniques were merely based on alphabetically sorted lists of the
next token to write given the characters already typed.

Over the years, researchers pushed the capabilities of these tools
by mainly focusing on their prediction accuracy (i.e., their ability to
predict the next tokens to write correctly). This has been achieved,
for example, by (i) exploiting contextual information such as the
code surrounding the statements on which code completion is
triggered [7, 48] and/or or the recent code change history [48];
and (ii) moving towards more advanced techniques such as Deep
Learning (DL) models, that have been widely applied to recommend
code tokens [3, 31, 35, 53, 58].

Most of the proposed tools were limited to the prediction of
just a single token, and only recent studies focused on predicting
multiple contiguous tokens or even entire statements [3, 12, 53].
Building on top of this research, OpenAI and GitHub presented
GitHub Copilot [10], a code assistant that achieved state-of-the-art
performance in code recommendation. Being trained on more than
150Gb of data from over 50M public GitHub repositories, Copilot
is able to even recommend entire methods just starting from their
signature.

The rise of DL to support code-related tasks was possible thanks
to the unprecedented amount of development data being publicly
available (e.g., from GitHub) and that can be used for training the
DL models. The vast majority of this data comes from open source
repositories and hence its usage is regulated by specific Free and
Open Source Software (FOSS) licenses (e.g., Apache 2.01, GPL v3.02).
These licenses regulate how the licensed source code can be redis-
tributed or modified to create derivative work. However, they were
not meant to define whether (i) the licensed code can be used
for training DL-based code recommender, nor (ii) the code recom-
mended by the trained models should be considered as derivative
work or, instead, as new code “written from scratch”. Such a dis-
cussion is part of a non-trivial debate that will likely result in the
update of FOSS licenses in the near future.

In this paper, we are interested in investigating a related but

more concrete research question (RQ):

To what extent do DL-based code recommender systems
generate predictions by cloning code snippets from the
training set?

1https://www.apache.org/licenses/LICENSE-2.0
2https://www.gnu.org/licenses/gpl-3.0.html

 
 
 
 
 
 
MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota

The relevance of this RQ is dictated by the following observation:
If DL-based code recommenders tend to clone code snippets from
the training set when generating predictions, it would be difficult
to consider the recommendations as new and original code (rather
than as derivative work). Although it is reasonable that the gener-
ated recommendations are influenced by the training dataset (e.g.,
by often reusing frequent code identifiers), the problem would be
more relevant if these techniques perform a verbatim copy of the
training code, possibly spanning several statements.

To answer our research question, we need: (i) a DL-based code
completion technique able to recommend several tokens/state-
ments; (ii) access to the dataset(s) used to train the DL model; and
(iii) access to the predictions generated by the technique on a test
set. Unfortunately, such an analysis is not possible using GitHub
Copilot since the training dataset is not publicly available. For this
reason, we adopt the Text-To-Text-Transfer-Transformer (T5), a DL
model that has been used to support several code-related tasks [43],
including code completion [12].

The T5 model exploits two training phases, namely pre-training
and fine-tuning [46], thus requiring two training datasets. In the
pre-training the model acquires knowledge about a language of
interest (e.g., Java code) through a self-supervised task, in which
the model is asked to guess randomly masked tokens in an input
instance (e.g., a Java method). In the fine-tuning phase, the model
is then specialized for the specific task of interest (e.g., bug-fixing,
predicting entire code statements).

We start from the pre-trained T5 made available by Ciniselli
et al. [12], who also made publicly available the pre-training dataset.
The model has been pre-trained on ∼2M Java methods. Then, we
fine-tuned this model on a new dataset we built featuring 841,318
Java methods to train the model in predicting non-trivial code
snippets composed by several statements. Indeed, predictions only
spanning a few code tokens are not interesting for our goal, since
they cannot really be defined as “clones” even if copied from the
training dataset. We built four versions of the fine-tuning dataset,
each one applying a different strategy to remove near duplicates [2],
including more and less strict strategies. Indeed, one of the assump-
tions we want to test is that a higher presence of near-duplicates in
the training set pushes the model to copy more, since it observes
over and over the same pattern.

Once trained the four models on the different datasets, we used
them to generate predictions on a test set of ∼16k instances (i.e.,
∼16k Java methods with missing code statements that must be
guessed by the model). Then, we used the Simian clone detec-
tor [19] to identify Type-1 (i.e., exact copies) and Type-2 (i.e., copied
code with changes to identifiers and types) clones between the
generated predictions and both the pre-training and fine-tuning
training datasets. Also, we checked whether the model tends to
“copy” more when (i) dealing with predictions having different
length, since we expect smaller predictions to more likely result in
clones; and (ii) generating correct or wrong predictions.

The achieved results show that ∼10% of the generated predictions
represent a Type-1 clone of code in the training data, with this
percentage going up to ∼80% when considering Type-2 clones.
However, the percentage of clones steadily drops when the model
is asked to predict several statements, with basically no clones
generated when the prediction comprises at least four lines of code.

We also found that correct predictions are more likely to contain
clones of code present in the training datasets. Such a result stresses
once more the importance of a proper dataset cleaning to avoid
overlaps between the training and the test datasets.

2 STUDY DESIGN
The goal of this study is to understand the extent to which DL-based
code recommenders are prone to suggest code snippets being clones
of instances present in the training set. The context is represented
by the four training datasets described in Section 2.2 and by the
snippets of code generated by a state-of-the-art code recommender
built on top of the Text-To-Text-Transfer-Transformer (T5) [12].
Our study aims at answering the following research question:

RQ. To what extent do DL-based code recommender systems gen-
erate predictions by cloning code snippets from the training set?

Our RQ sheds some light on the implications of using recom-
mendations generated by DL-based code recommenders in terms
of possible licensing issues occurring when using the generated
recommendations in the development of commercial software. We
answer our RQ by training a DL-based code recommender to then
use it in thousands of code completion tasks (i.e., an incomplete
code is provided as input to the model, that is required to gener-
ate the missing code). The generated recommendations are then
compared to the instances in the training set looking for clones.

2.1 Study Context: DL-based Code

Recommender

The first step in the selection of the study context is the code
recommender to use. The latest and likely more effective code
recommender available, namely GitHub Copilot [10], was not an
option for our study. Indeed, while we have access to Copilot, the
dataset used to train it is not publicly available, making it impossible
to check whether the code it recommends is a clone of the instances
in its training set. For this reason, we focused on another DL-based
code recommender recently proposed by Ciniselli et al. [12].

The authors trained a T5 model supporting code completion in
Java at method-level granularity. This means that once trained the
model can take as input an incomplete Java method (i.e., a method
missing one or more contiguous code tokens) and it can predict the
missing tokens. The T5 has been trained in two phases. The first,
named pre-training, aims at providing the model with a general
knowledge about the language of interest. Ciniselli et al. [12] used
a classic denoising pre-training task in which the model has been
fed with ∼2M Java methods having 15% of their tokens (even non-
contiguous ones) randomly masked, with the model asked to predict
them. Once pre-trained, the model has been fine-tuned to support
code completion at different levels: (i) token-level, in which the last
𝑥 tokens (with 1 ≤ 𝑥 ≤ 10) of a given statement have been masked
and must be predicted; (ii) construct-level, in which the masking has
been focused on specific code constructs, such as the conditions
of if statement or of a while/for loop, the parameters of method
calls, etc. (see [12] for details); and (iii) block-level, in which entire
code blocks3 up to two statements are masked.

3A code block is defined as the code enclosed between two curly brackets.

To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set? MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Table 1: Threshold (Th) configurations and number of meth-
ods after the filtering.

Name

Set Th. Multiset Th.

# of Methods

Very Weak
Weak
Strong
Very Strong

0.8
0.6
0.4
0.3

0.7
0.5
0.3
0.2

838k
711k
483k
273k

The fine-tuning has been performed on different datasets, for a

maximum of 750k instances (methods) involved.

In the reported evaluation, the T5 achieved correct predictions
ranging from ∼29%, obtained when asking the model to guess entire
blocks, up to ∼69%, reached in the simpler scenario of few tokens
masked from the same code statement [12].

We start from the pre-trained model made available by Ciniselli
et al. in their replication package [11], but we fine-tune it from
scratch on a new dataset described in Section 2.2. Indeed, the fine-
tuned model used by Ciniselli et al. [12] was not appropriate for our
study, since they trained the T5 to predict at most two code state-
ments. In our work, we are interested in identifying code clones in
the generated predictions and, for this reason, we want to train the
T5 to generate also longer prediction. Indeed, clones represented by
a few code tokens or a single statement are unlikely to be relevant,
but mostly due to the syntactic sugar of programming languages.

2.2 Study Context: Datasets Construction
To create our fine-tuning dataset we started from the CodeSearch-
Net dataset provided by Husain et al. [25]. CodeSearchNet fea-
tures over ∼1.5M Java methods collected from open source non-
forked GitHub repositories. On top of them, we added the ∼2.2M
Java methods made available by Ciniselli et al. [12] and collected
from GitHub Android repositories. From the overall set, we re-
moved methods (i) having less than three lines of code, since they
do not allow to train the model on “code completion scenarios” that
are interesting in the context of code clones (i.e., when asking the
model to generate several missing code statements); (ii) having the
name containing the test substring in an attempt to remove test
methods; and (iii) containing non-ASCII characters (e.g., Chinese
text) to avoid confusing the model during training. We then ex-
cluded all methods having a duplicate in the pre-training dataset
used by Ciniselli et al., obtaining the final set of 841,318 instances.
As a final step, we applied the approach defined by Allama-
nis [2] and Lopes et al. [42] and used in the construction of the
CodeSearchNet database [25] to identify near-duplicates in our
fine-tuning dataset. The idea behind this approach is to look at the
percentage of overlapped tokens between two instances (i.e., meth-
ods) and identify them as near-duplicates if such an overlap exceeds
a specific threshold. The code implementing such an approach is
publicly available [1], and allows setting two thresholds making
the identification of near-duplicates weaker (i.e., only very similar
methods are considered clones) or stronger. Both thresholds define
the percentage of shared tokens needed to consider two methods
as duplicates. However, the first threshold (Multiset Threshold) con-
siders the entire list of tokens in a method (including duplicates),
while the second (Set Threshold) is computed on the unique set of
tokens in the compared methods.

We experiment with the four different combination of thresh-
olds reported in Table 1, that resulted in four different fine-tuning
datasets composed by the # of Methods instances not considered
near-duplicates (out of the starting ∼850k). In the following we
refer to the four combinations as very weak (i.e., the combination
of thresholds rarely reports methods as near-duplicates), weak,
strong, and very strong (i.e., the combination of thresholds fre-
quently reports methods as near-duplicates) and to the result-
ing datasets as dataset𝑣𝑒𝑟 𝑦_𝑤𝑒𝑎𝑘 , dataset𝑤𝑒𝑎𝑘 , dataset𝑠𝑡𝑟𝑜𝑛𝑔, and
dataset𝑣𝑒𝑟 𝑦_𝑠𝑡𝑟𝑜𝑛𝑔. The choice of experimenting with four differ-
ent combinations was dictated by the following assumption: It is
possible that a training dataset featuring several near-duplicates
(such as dataset𝑣𝑒𝑟 𝑦_𝑤𝑒𝑎𝑘 ) pushes the model to “copy” more from
the training set as compared to a dataset featuring almost no near-
duplicates (dataset𝑣𝑒𝑟 𝑦_𝑠𝑡𝑟𝑜𝑛𝑔). Indeed, seeing a code snippet over
and over across several training sample may push the model to use
that coding pattern more when generating the predictions. This is
just an assumption we validate in our study.

As described in Section 2.3, the four datasets have been used to
fine-tune four T5 models starting from the pre-trained model by
Ciniselli et al. [12].

2.3 Model Training
We do not discuss the architectural details of T5, which have been
widely documented by Raffel et al. [46] to better focus on imple-
mentation choices that are relevant for this study. The specific
architecture we use is the T5𝑠𝑚𝑎𝑙𝑙 presented in [46].

Tokenization. T5 uses a tokenizer to pre-process the input
and generate the stream of output tokens. As it will be clear later,
in our study the model will be fed with a Java method in which
specific code statements have been masked (input) and will be
asked to generate the masked statements (output). The adopted
tokenization strategy impacts the vocabulary that the model can
exploit to generate the output. Previous work studied the pros and
cons of different tokenization strategies, spacing from word- [14]
to character-level [44] tokenization. The former is prone to the
out-of-vocabulary problem [32] while the latter struggles with long
inputs. To mitigate these limitations, Sennrich et al. [51] introduced
the Byte Pair Encoding (BPE) tokenization that splits the input into
a sequence of subtokens (e.g., it can split the word “string” into
two parts “str” and “ing”, where “str” is the root and “ing” is the
specialization). Through this sub-splitting, BPE has the advantage
of reducing the vocabulary size (as well as the out-of-vocabulary
problem) and allows the model to generate composed words. For
example, when applied to source code the model can generate
unseen identifiers whose name is a composition of the sub-tokens
present in the vocabulary. The T5 exploits a SentencePiece [38]
tokenizer which uses the same idea behind BPE. Ciniselli et al. [12]
trained a 32k-word SentencePiece tokenizer on the pre-training
dataset. Since we reuse their pre-training dataset, we also reused
their trained SentencePiece tokenizer.

Datasets masking and splitting. To train T5 with the goal
of generating non-trivial snippets of code composed by multiple
statements, we adapted one of the block-level masking scenario
proposed by Ciniselli et al. [12].

MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota

Table 2: Study datasets. We reported the number of instances
for the training and the number of distinct methods.

Table 3: Hyperparameters Tuned for the T5 Models.

Filtering
Level

Very Weak
Weak
Strong
Very Strong

All levels

Dataset

#Instances

#Distinct
Methods

Training

Evaluation
Test

1.2M
1.02M
703k
390k

15,783
15,742

797k
671k
446k
240k

8,560
8,506

Learning Rate Type

Constant (C-LR)
Slanted Triangular (ST-LR)

Inverse Square Root (ISQ-LR)

Polynomial Decay (PD-LR)

Parameters
LR = 0.001
LRstarting = 0.001
LRmax = 0.01
Ratio = 32
Cut = 0.1
LRstarting = 0.01
Warmup = 10, 000
LRstarting = 0.01
LRend = 1e−06
Power = 0.5

In particular, we apply the following processing to each instance
(i.e., Java method) in each of the four datasets (i.e., dataset𝑣𝑒𝑟 𝑦_𝑤𝑒𝑎𝑘 ,
dataset𝑤𝑒𝑎𝑘 , dataset𝑠𝑡𝑟𝑜𝑛𝑔, and dataset𝑣𝑒𝑟 𝑦_𝑠𝑡𝑟𝑜𝑛𝑔). In each method,
we identify all blocks (i.e., code snippets enclosed between two
curly brackets) composed by at least two and at most six state-
ments. Given a method including 𝑛 of such blocks, we generate
𝑛 versions of it, each having one of the 𝑛 blocks masked with the
special token <extra_id_0>. During training and testing the model
is then fed with methods including a masked block and it is required
to generate it. The lower and upper bound for the block size (i.e.,
two and six lines) are defined, based on our experience, to obtain
code generation tasks that are non-trivial (at least two statements),
relevant in the context of cloning, and addressable given the em-
ployed DL model, training dataset, and hardware resources (at most
six statements). To account for trivial statements, we count only
statements composed by at least two characters to exclude, for
example, statements including only a closing parenthesis. Thus, a
block composed by one 10-character statement and one 1-character
statement is not considered in our study, since it does not match
the “two statements” lower bound.

Starting from these instances, we created the training, evaluation,
and test datasets in Table 2. First, it is important to clarify that the
splitting across training, evaluation, and test sets was not random
to avoid biasing the learning. To explain this point, let us go back to
the way in which we created the instances (i.e., Java methods with
a masked code block) for our dataset. Given a method 𝑚 having 𝑛
blocks composed by two to six statements, we added in the dataset 𝑛
versions of 𝑚, each having one and only one block masked. Suppose
that 𝑚 contains two blocks 𝑏1 and 𝑏2, thus leading to two versions
of 𝑚: one in which 𝑏1 is masked (𝑚𝑏1 ) and 𝑏2 is not and vice versa
(𝑚𝑏2 ). With a random splitting, it could happen that 𝑚𝑏1 is assigned
to the training set and 𝑚𝑏2 to the test set. However, in 𝑚𝑏1 the 𝑏2 is
not masked. Thus, when the model has to guess the tokens masked
in 𝑚𝑏2 it would have the solution in the training set, resulting in
boosted prediction performance. For this reason, the splitting was
done at “method-level” rather than “instance-level”. This means
that all instances related to a method 𝑚 can belong to only one of
the three sets (training, evaluation, testing). The number of unique
methods in each set is reported in Table 2 in the # Distinct Methods
column, while the number of instances these methods generated is
represented in the # Instances column.

Second, as previously said, in our study we want to investigate
whether the prevalence of near-duplicates in the training dataset
(i.e., the different filtering levels we applied, from very weak to
very strong) plays a role in the generation of code clones. For this
reason, while the training sets for the four datasets can be different,
with dataset𝑣𝑒𝑟 𝑦_𝑤𝑒𝑎𝑘 being the largest one (i.e., containing more
near-duplicates), we wanted to have the same evaluation and test
sets for all datasets, to allow for a fair comparison. For this reason,
we built the evaluation and the test dataset with ∼15k instances
each. Those instances meet the following requirements: (i) they
have been extracted from the methods that were present in all four
datasets, independently from the near-duplicates filtering level that
has been applied; and (ii) they are balanced in terms of prediction
difficulty, featuring ∼3k instances in which a two-statement block
is masked, ∼3k in which a three-statement block is masked, etc., up
to six-statement blocks masking.

Hyperparameters tuning. We started from the pre-trained T5
model by Ciniselli et al. [12]. Then, to find the best T5 configura-
tion for the fine-tuning, we followed the hyperparameters tuning
procedure previously used by Mastropaolo et al. [43]. In particular,
we trained four different configurations of the T5 that differ for the
type of learning rate applied during the training (see Table 3 for
the four configurations). Once trained, each model has been run on
the evaluation set to assess its performance in terms of percentage
of correct predictions, namely the percentage of instances in the
evaluation set for which the model managed to exactly predict
the masked code. This process has been performed on the largest
dataset (i.e., dataset𝑣𝑒𝑟 𝑦_𝑤𝑒𝑎𝑘 ), assuming that the best configuration
identified on it represents the best choice also for the other three
datasets. Each model has been fine-tuned for 50k (corresponding to
∼10 epochs given the used batch size of 256). The Slanted Triangular
learning rate obtained the best results and, thus, has been used in
the study to train our models.

Fine tuning. We fine-tuned T5 on each of the four datasets for
∼100 epochs, by varying the number of fine-tuning steps based on
the dataset size (i.e., larger datasets require more steps to reach 100
epochs, see # Training Steps in Table 4). To avoid overfitting, we
saved the trained models every 5k steps and identified the best one
(see Best Checkpoint in Table 4) on the evaluation set in terms of
percentage of correct predictions.

To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set? MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Table 4: Number of finetuning steps and best checkpoint
found.

Filtering
Level

# Training

Best
Steps Checkpoint

Very Strong
Strong
Weak
Very Weak

152k
275k
400k
470k

20k
40k
225k
450k

These are the models we will use to generate the predictions on
the test set and analyze the overlap in terms of code clones between
the generated predictions and the code in the training sets.

2.4 Data Collection And Analysis
Once trained the four models (one on each training dataset), we
run them on the test set that, as previously explained, is the same
for all four models. This results in the models generating the code
blocks predicted as needed to fill the masked code. These blocks
of generated code are the ones we want to contrast against the
training set used for each model to identify code clones.

To identify the clone detector to use, we started from the lit-
erature review by Rattan et al. [47], looking at the list of tools
documented by the authors. Several of them are no longer available
(e.g., [22], [16]), while others do not work on syntactically incorrect
code that may be generated by the T5 models (e.g., [50], [45]). Given
the study’s constraints, we used the Simian clone detector [19]. Be-
sides being very robust and scalable, Simian can identify Type-1
and Type-2 clones [49]. Simian works at line-level granularity, look-
ing for duplicated lines among the code dataset provided as input.
Therefore, having two snippets of code that are identical but writ-
ten on a different number of lines, may fool the clone detection
algorithm. For this reason, we formatted all methods in our datasets
using the IntelliJ formatter [26], to ensure they all adopt the same
coding style (e.g., maximum number of characters per line).

We run Simian to identify both types of clones between the
code generated by the four models and their respective training set.
Then, we compute the percentage of code clones generated by each
model by looking at different characteristics of the clones and of
the predictions. In particular, we consider in our analysis:

• The amount of near-duplicates in the training set. As previ-
ously explained, this has been achieved by training four T5
models on the four datasets we built. We report the percent-
age of clones generated by each of these models.

• The length of the generated code. We split the generated code
blocks in different buckets based on the number of state-
ments they feature (from ≥ 2 to ≥ 5 at steps of 1). Then, we
compute the percentage of clones in each bucket. We expect
smaller predictions to contain a higher percentage of clones.
• The clone type. When reporting the percentage of clones,
we distinguish between Type-1 and Type-2 clones. Type-2
clones will be, by construction, more prevalent, since they
are a superset of Type-1 clones, being exact copies but for
the different use of identifiers and types.

• The training dataset from which the prediction has been “cloned”.
Each T5 model exploits two training datasets, the pre-training
and fine-tuning. We analyze from where the model is more
likely to “copy”.

• The correctness of the prediction. We look whether the models
are more likely to “copy” when generating correct or wrong
predictions.

We complete our study with a correlation analysis in which we
assess whether the model is more prone to clone code in specific
circumstances. In particular, we use the Spearman test [60] to cor-
relate the presence of clones with (i) the Cyclomatic Complexity of
the test method (ii) the number of lines of the method, and (iii) the
confidence of the prediction. The first two metrics are computed
using the Python Lizard library [59]. The last one is a score we
computed for each prediction made by T5. Such a score 𝑥 ranges
from minus infinity to 0 and it is the log-likelihood of the prediction
itself and can be normalized by computing the 𝑒𝑥𝑝 (𝑙𝑛(𝑥)). This
brings the confidence score between 0.0 and 1.0, with 1.0 indicating
the scenario in which the model is confident about the generated
prediction.

Finally, it is worth mentioning that, despite the use of the Sen-
tencePiece tokenizer, the T5 may generate predictions including
unknown tokens. We performed all the above described analysis
both when considering all the instances of the test set as well as
when removing the ones that contain at least one unknown tokens.
The latter are less likely to result in code clones, since the train-
ing sets do not contain unknown tokens. We report in the paper
the results achieved when excluding from the test set the 6,986
instances for which at least one of the four models generated an
unknown token; the results on the whole test set are available in
our replication package [13], but we anticipate that they are inline
with those discussed in the paper.

2.5 Replication Package
The datasets, models, training/testing code, and the achieved raw
data are publicly available in our replication package [13].

3 RESULTS DISCUSSION
To answer our research question, we analyze the percentage of
code predictions being clones of instances present in each training
dataset (i.e., pre-training and fine-tuning).

Impact of the amount of near-duplicates in the training
set. Fig. 1 shows the percentage of Type-1 and Type-2 clones
found in the predictions of the four models fine-tuned on the
different datasets (i.e., dataset𝑣𝑒𝑟 𝑦_𝑠𝑡𝑟𝑜𝑛𝑔, dataset𝑠𝑡𝑟𝑜𝑛𝑔, etc.). We
also organize the predictions in different buckets based on the
length of the predicted code (from ≥ 2 up to ≥ 5). As a reminder,
dataset𝑣𝑒𝑟 𝑦_𝑠𝑡𝑟𝑜𝑛𝑔 is the smallest (273k instances) and has been
built using very strict criteria for the exclusion of near duplicates;
dataset𝑣𝑒𝑟 𝑦_𝑤𝑒𝑎𝑘 is instead the largest (838k instances), with a
higher presence of near duplicates. Note that the gap in dataset
size is substantial when moving from the Very Strong to the Strong
configuration (+77% of instances) and from Strong to Weak (+47%),
but limited when going from Weak to Very Weak (+18%).

MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota

Figure 1: Percentage of Type-1 and Type-2 clones in predictions having different length.

Before commenting on the achieved results is also worthwhile
to remind that the number of generated predictions is 15,742-6,986
for all datasets, since in this results discussion we removed all cases
for which at least one of the four models generated an unknown
tokens. Thus, a 10% of clones indicates ∼900 generated clones.

With the increase of near duplicates in the training set, the per-
centage of Type-1 clones in the generated predictions increases as
well. This trend is visible across the four datasets with the exception
of dataset𝑣𝑒𝑟 𝑦_𝑤𝑒𝑎𝑘 for which the percentage of generated Type-1
clones is inline with those of dataset𝑤𝑒𝑎𝑘 . As previously said, this is
likely to be due to the small differences in size (i.e., in the presence
of near-clones) between the two datasets.

Another trend is also clear when looking at Fig. 1: longer pre-
dictions are less likely to be the result of cloned snippets from the
training set. If we focus, for example, on the dataset resulting overall
in the highest percentage of generated Type-1 clones (dataset𝑤𝑒𝑎𝑘 ),
such a percentage moves from 12.5% for predictions featuring at
least two statements, to 2.3% for those having at least three state-
ments, down to 0.1% when only considering the generation of non-
trivial code snippets composed by at least five statements. Such
a result is quite expected since longer predictions are statistically
less likely to be equal to instances present in the training set as
compared to shorter predictions.

The analysis of Type-2 clones follows similar trends with a couple
of notable differences. First, as expected, the percentage of Type-2
clones is much higher as compared to that of Type-1 clones (from
10 to 100 times higher). This can be explained by the weaker re-
quirements adopted to consider two code snippets as Type-2 clones.
Indeed, while the code structure should be similar, two instances
are considered as Type-2 clones even if they adopt completely dif-
ferent identifiers and types. Still, confirming what observed for
Type-1 clones, the percentage of clones in the predictions steadily
drops with the increase in size of the predictions, moving from
∼80% to ∼2.5% when the prediction’s size increases from at least 2
to at least 5 lines. Finally, differently from what observed for Type-1
clones, there is no substantial difference in the percentage of Type-2
clones across the different datasets. This may be due to the fact
that, differently from Type-1 clones that are verbatim copies of code
from the training set, in the case of Type-2 clones the amount of
near-duplicates (i.e., frequent instances in the training set) is less
likely to play a role.

Take away 1. Around 10% of the generated predictions repre-
sent a Type-1 clone of code present in the training data, with
this percentage going up to ∼80% when considering Type-2
clones. However, the percentage of clones steadily drops when
the model is asked to predict several statements, with basically
no clones generated when the prediction comprises at least four
lines of code. This indicates that advanced tools such as GitHub
Copilot that can generate entire functions are very unlikely to
“copy code” from the training data.

Pre-training and fine-tuning datasets. We also analyzed the
provenance of the clones, meaning whether the cloned predictions
tend to come from the pre-training or from the fine-tuning dataset.
Fig. 2 summarizes the achieved results. For both types of clones
(i.e., Type-1 and Type-2) Fig. 2 reports the the percentage of clones
coming (i) only from the pre-training (i.e., the prediction has a clone
only in the pre-training dataset); (ii) only from the fine-tuning; and
(iii) from both training datasets (i.e., both training datasets have
at least one instance representing a clone of the prediction). In
interpreting the reported percentages it is important to consider
that the chances of cloning from the pre-training are expected to
be higher, since it contains more “material” from which the model
can copy. For example, in the Very Strong configuration, we have
1.85M methods in the pre-training dataset and 273k methods in the
fine-tuning dataset. Therefore, 87% of the training methods belong
to the pre-training dataset and only 13% to the fine-tuning dataset.
The Very Weak configuration is the most balanced, still having,
however, 69% of training instances coming from the pre-training
dataset against the 31% of the fine-tuning dataset.

Figure 2: Percentage of Type-1 and Type-2 clones coming
from the pre-training and fine-tuning dataset

For both types of clones, most of the cloned predictions have a

clone both in the pre-training and in the fine-tuning datasets.

Type 1Type 2Very StrongStrongWeakVery Weak8.2%83.2%10.3%84.6%12.5%83.5%12.1%83.5%>=2 Lines ClonesType 1Type 20.9%30.3%1.6%33.9%2.3%35.1%1.9%31.9%>=3 Lines ClonesType 1Type 20.2%11.8%0.3%12.5%0.6%14.4%0.4%12.0%>=4 Lines ClonesType 1Type 20.0%2.3%0.0%2.6%0.1%3.2%0.1%2.5%>=5 Lines ClonesType 1PretrainType 2PretrainType 1FinetuningType 2FinetuningType 1BothType 2BothVeryStrongStrongWeakVeryWeak12.9%3.4%17.9%2.2%69.2%94.4%9.4%2.4%23.7%3.1%66.9%94.5%6.8%2.3%28.2%4.0%65.0%93.7%5.8%2.3%33.9%4.1%60.3%93.6%To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set? MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

As expected, this is especially true for Type-2 clones, for which
almost the totality of cloned predictions follow this pattern. More
interesting are the results for Type-1 clones, namely exact copies.
While also here there is a substantial percentage of predictions hav-
ing clones both in the pre-training and in the fine-tuning datasets
(∼60%), most of the predictions only cloned from one of the two
training datasets come from the fine-tuning. This result is not really
expected considering that the pre-training dataset provides most
of the “training material”. However, possible explanations for this
result lie is the order in which the two training phases are applied
to the DL model and in their objective. The pre-training changes
the weights of the neural network from a random distribution to a
distribution able to deal with the pre-training denosing task (i.e.,
guessing masked tokens in the input instance). The tokens masked
in the pre-training are randomly selected and may not be contigu-
ous. This means that in very few cases the model sees instances
in which it is required to generate complete code statements. The
fine-tuning, instead, starts from the pre-trained model or, in other
words, from the distribution of weights obtained after pre-training.
These weights are then modified to support the fine-tuning task
that, in our case, explicitly requires the model to generate complete
code blocks. Thus, when the model is asked during testing to gen-
erate code blocks, the weights learned during the fine-tuning may
come more “handy”, pushing the model to reuse more knowledge
acquired during the fine-tuning rather than during the pre-training.

Take away 2. Most of the cloned predictions have clones in
both the pre-training and the fine-tuning. For example, in the
case of Type-1 clones, ∼65% come from both training datasets.
This indicates that code instances seen repeatedly during both
training phases are, as expected, more likely to influence the
generated predictions. The remaining ones (i.e., clones that only
come from one of the two training sets), are more likely to come
from the fine-tuning. This goes against what we expected since
the pre-training is substantially larger than the fine-tuning.

Correct and wrong predictions. Fig. 3 shows the percentage
of Type-1 and Type-2 clones created by the model when generating
correct (CP) and wrong (WP) predictions. Also in this case the
percentages must be read by keeping in mind that, across the test
instances on which the four models have been tested, all models
achieved ∼3% of correct predictions (∼500 cases), implying that we
should expect ∼3% of clones being related to correct predictions.
Before commenting the results, a few notes on the low percentage
of correct predictions achieved by the models (more in our Validity
Evaluation section — Section 4): (i) such a percentage is inline
with what has been reported in the literature for applications of DL
models to automate other code-related tasks, such as bug-fixing [56]
or code review automation [57]; (ii) the code generation tasks on
which we tested T5 included the generation of code blocks up to six
lines of code, thus being non-trivial; (iii) we consider as correct only
predictions that are identical to the masked code, meaning that even
predictions that are slightly different but semantically equivalent
are considered wrong; (iv) finally, as we will discuss in Section 4, our
inspection of wrong predictions confirmed that the model learned
how to generate syntactically correct code, confirming the validity
of the performed training.

Figure 3: Percentage of Type-1 and Type-2 clones in correct
(CP) and wrong (WP) predictions.

The cases in which the predictions represent verbatim copies
of code in the training data (Type-1 clones) occur ∼4× more than
expected in correct predictions (see Fig. 3), with ∼12% of clones
belonging to correct predictions, only representing ∼3% of all pre-
dictions. This is likely due to instances (methods) in the training
set that, while being different from all methods in the test set, share
with them the part of code that we masked. Such a result also rises
a warning for the evaluation of DL models in the context of code
generation: It might be not enough to just remove duplicates and/or
near-duplicates across instances, since the parts of code the model is
required to generate in the test set may still be present as verbatim
copy in the training set, even when very strict criteria are used to
remove duplicates (as in the case of our Very Strong configuration).
Similar observations can be made for Type-2 clones, even though
here the percentage of clones in correct predictions is only ∼2×
higher than what we expected.

Take away 3. Correct predictions are more likely to contain
clones of code present in the training datasets. Such a result
stresses once more the importance of a proper dataset cleaning
to avoid overlaps between the training and the test datasets.

Correlation analysis. We now discuss the results related to the
correlation analysis aimed at understanding if specific characteris-
tics of (i) the code snippet to predict, (ii) the instance containing
it, or (iii) the confidence of the model in generating the prediction,
may affect the presence/absence of code clones.

More in details, as explained in Section 2, we consider (i) the
Cyclomatic Complexity (CC) of the entire instance (method), (ii)
the CC of the model’s input (i.e., the entire method without the
block to predict), (iii) the number of lines of the method, (iv) the
number of lines of the block to predict, and (v) the confidence of
the T5 model.

We found no significant correlation between all these metrics
and the presence of Type-1 clones in the generated predictions.
While we identified some significant correlations for Type-2 clones,
they are all lower than |0.15|, thus indicating very low correlations
(complete results in our replication package [13]).

Qualitative Examples. Finally, we present in this section some
qualitative examples of the clones generated by the T5 model. Re-
member that the clones refer only to the part of code predicted by
the model, ignoring the surrounding context. In other words, two
different methods containing the same block of code predicted by
T5 are considered as clones.

Type 1CPType 2CPType 1WPType 2WPVeryStrongStrongWeakVeryWeak12.3%5.4%87.7%94.6%11.1%5.8%88.9%94.2%12.0%6.5%88.0%93.5%13.3%6.7%86.7%93.3%MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota

c l o n e d c o d e ∗ /

/ ∗
e r r o r F r a g m e n t = new E r r o r F r a g m e n t ( ) ;
B u n d l e msg = new B u n d l e ( ) ;
msg . p u t S t r i n g ( " msg " , ex . g e t M e s s a g e ( ) ) ;
e r r o r F r a g m e n t . s e t A r g u m e n t s ( msg ) ;
g e t S u p p o r t F r a g m e n t M a n a g e r ( ) . b e g i n T r a n s a c t i o n ( )

. r e p l a c e ( R . i d . message ,

e r r o r F r a g m e n t ) . commit ( ) ;

e x p e c t e d p r e d i c t i o n ∗ /

/ ∗
{
e r r o r F r a g m e n t = new E r r o r F r a g m e n t ( ) ;
B u n d l e msg = new B u n d l e ( ) ;
msg . p u t S t r i n g ( " msg " ,
" No o r p oor
e r r o r F r a g m e n t . s e t A r g u m e n t s ( msg ) ;
g e t S u p p o r t F r a g m e n t M a n a g e r ( ) . b e g i n T r a n s a c t i o n ( )

i n t e r n e t

c o n n e c t i o n . " ) ;

. r e p l a c e ( R . i d . message ,

e r r o r F r a g m e n t ) . commit ( ) ;

}

Listing 1: Type-1 Clone Generated in Wrong Prediction

The top part of Listing 1 shows an example of Type-1 clone
generated by the T5, while the bottom part reports the correct
prediction the model was expected to generate (i.e., the code we
masked). As it can be seen, the prediction task we asked the model
to perform was far from trivial and the T5 managed to generate a
prediction really close to the expected one. Such a prediction was
the result of cloning four of the five statements to predict from
the training dataset. This is one of those cases in which, despite
the model did not generate a correct prediction, it went quite close
to the target, with the only difference being one of the arguments
used in the putString invocation.

/ ∗

c l o n e d c o d e ∗ /

( s

! = n u l l )

t r y {

r e t u r n I n t e g e r . p a r s e I n t ( s ) ;
c a t c h ( N u m b e r F o r m a t E x c e p t i o n e )

}

{

i f
{

}

}
r e t u r n n u l l ;

/ ∗
{

}

e x p e c t e d p r e d i c t i o n ∗ /

I n t e g e r

i n t e r v a l =

O p t i o n A d v a n c e P a r s e r . DEFAULT_INTERVAL ;

i f

( o p t i o n S e t . hasArgument ( " i n t e r v a l " ) )

i n t e r v a l = ( I n t e g e r )
( i n t e r v a l < 1 )
i f

{

{
( o p t i o n S e t . v a l u e O f ( " i n t e r v a l " ) ) ;

throw new I l l e g a l A r g u m e n t E x c e p t i o n ( " I n t e r v a l

c a n n o t be s e t below 1 . 0 " ) ;

}

}
r e t u r n i n t e r v a l ;

Listing 2: Type-1 Clone Generated in Wrong Prediction

Also Listing 2 shows a Type-1 clone generated in a wrong pre-
diction. In this case, we want to put the focus on the fact that the
model cloned a snippet of code that represents a quite popular tem-
plate in Java when converting a String to an Integer. The model
understood the need for implementing a check on the value of an
Integer but failed in implementing the check that was actually
needed (the one shown in the expected prediction).

c l o n e d c o d e ∗ /

/ ∗
do {

C a t e g o r y S o u r c e c a t e g o r y S o u r c e =

C a t e g o r y S o u r c e . c a t e g o r y F r o m C u r s o r ( c u r s o r ) ;

c a t s S r c s . add ( c a t e g o r y S o u r c e ) ;

} w h i l e ( c u r s o r . moveToNext ( ) ) ;

e x p e c t e d p r e d i c t i o n ∗ /

do {
C a t e g o r y S o u r c e c a t e g o r y S o u r c e =

C a t e g o r y S o u r c e . c a t e g o r y F r o m C u r s o r ( c u r s o r ) ;

c a t s S r c s . add ( c a t e g o r y S o u r c e ) ;
} w h i l e ( c u r s o r . moveToNext ( ) ) ;

/ ∗
{

}

c o d e from t h e

/ ∗
do {

t r a i n i n g s e t

∗ /

C a t e g o r y S o u r c e c a t e g o r y =

C a t e g o r y S o u r c e . c a t e g o r y F r o m C u r s o r ( c u r s o r ) ;

c a t e g o r i e s . add ( c a t e g o r y ) ;
} w h i l e ( c u r s o r . moveToNext ( ) ) ;

Listing 3: Type-2 Clone Generated in Wrong Prediction

Finally, Listing 3 shows an example of Type-2 clones, reporting
the prediction on top, the expected code in the middle, and the
cloned instance from the training in the bottom. In this case, the
model reused the code structure just changing the identifier name
from category to categorySource. The model generated some-
thing very close to the expected solution, fully relying, however,
on code already seen during the training that led it to a wrong
prediction.

4 VALIDITY DISCUSSION
Threats to construct validity concern the relationship between the
theory and what we observe. In our analysis, similarly with what
done in the literature (e.g., [12, 21]), we simulate the usage of the
code recommender by masking blocks of code. We are aware that
the masked blocks may not completely reflect the way in which
developers would use such recommenders in practice.

Another threat is related at the limited prediction performance
achieved by our models (∼3% of correct predictions, detailed results
in [13]). As already discussed, this level of performance is not un-
usual for the automation of challenging code-related tasks [56, 57],
such as the code generation task subject of our study. However,
a reader may wonder whether the low percentage of clones we
observed in the predictions may be due to the fact that, in most
of cases, the model just generates garbage, thus not being “able”
to generate clones of training instances. This is not unusual to
observe for DL models trained, for example, on very little data
and that tend to generate long meaningless sequences of frequent
tokens observed in the training set (e.g., sequences of parentheses
when dealing with code-related datasets). While we trained our
models on millions of Java methods, we inspected a sample of 200
wrong predictions (50 for each of the four models) to manually
check whether the generated predictions, while wrong, followed
a correct Java syntax. Such an analysis has been performed by
the first author, and resulted in only 7 (3.5%) instances classified
“meaningless predictions” in terms of Java syntax.

To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set? MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Threats to internal validity concern factors, internal to our study,
that could affect our results. The hyperparameters tuning phase
described in details in Section 2 can have a strong impact on the
performance of DL models and, consequently, on their likelihood
of generating clones. Due to feasibility reasons, we calibrated the
hyperparameters only for the dataset𝑣𝑒𝑟 𝑦_𝑤𝑒𝑎𝑘 , assuming that the
chosen configuration would also work well for the other datasets.
Hence, it is possible that evaluating a plethora of different configu-
rations may improve the performance of the model and/or impact
our findings about the generated clones.

Note also that, as explained, we observed that for some of the
instances in the test set the models generated unknown tokens
that are less likely to result in clones from the training set. For
this reason, we reported in the paper the results achieved when
ignoring these instances, while the results on the whole dataset
are available in our replication package [13]. As previously said,
the main observed findings still hold when also considering the
predictions featuring unknown tokens.

Finally, it is worth mentioning that the accuracy of the Simian
clone detector may influence the achieved results. To partially ad-
dress this threat, the first author inspected 100 randomly selected
predictions from our dataset for which Simian identified Type-1 (50
predictions) and Type-2 (50) clones. Since for a single prediction
there might be dozens of clones, we inspected 3 clones per each
prediction. Concerning Type-1 clones, without surprise, the accu-
racy was 100%, since those are predictions being exact copies of
code in the training sets. As for Type-2 clones, judging the identi-
fied instances was not always straightforward due to the limited
size of the identified clones (between 2 and 6 lines). These clones
are exact copies at the AST-level and, looking at them from this
perspective, we confirm the correctness of all inspected cases. How-
ever, especially when looking at very small clones (e.g., 2 lines),
we acknowledge that some pieces of code, while sharing the same
AST structure, may look different to a human, since using different
identifiers and/or types.

Threats to external validity are related to the possibility to gen-
eralize our results. We chose the T5 model, that showed a strong
ability to adapt to different code-related tasks [43] achieving re-
markable performance. The datasets used in our experiments are all
Java-related. We do not claim any generalizability of our findings
in terms of adopted DL model and subject programming language.
We relied on the Simian clone detector [52] that, as explained
in Section 2, is robust in the processing of syntactically incorrect
code. Results may change by using other clone detectors. Moreover,
in our study we only considered code clones composed by at least
two non-trivial lines. Considering shorter clones, while being an
option, is likely to artificially inflate the percentage of clones with
instances that are not really relevant in terms of possible licensing
issues.

5 RELATED WORK
We discuss the literature related to (i) techniques supporting code
completion and (ii) empirical studies investigating circumstances
under which clones can be introduced on software systems. Con-
cerning code completion, for the sake of brevity we only focus on
DL-based techniques.

5.1 DL-based Code Recommendation
The usage of data-driven techniques for code recommendation has
its roots in the works exploiting statistical language models, such
as 𝑛-gram models, to recommend developer with the next code
token to write [20, 24, 55]. Hellendoorn and Devanbu [20] also
showed that cached 𝑛-gram models aimed at considering specific
characteristics of code (e.g., scoped vocabulary) can beat DL-based
approaches, and that the two families of techniques can be even
combined for better effectiveness.

Later on, Karampatsis et al. [31] suggested instead that neural
network models are the best algorithm for code completion. The
authors exploited Byte Pair Encoding [15] as a strategy to over-
come the out-of-vocabulary problem, showing that their best model
outperforms 𝑛-gram models and easily adapts to different domains.
Kim et al. [35] leveraged the Transformers neural network ar-
chitecture for code completion. Using the syntactic information
provided by the Abstract Syntax Tree (AST), they were able to for-
tify the self-attention mechanism. Among the several models they
experiment with, the best one reached a Mean Reciprocal Rank
(MRR) of 74.1% in predicting the next token.

Alon et al. [3] proposed a language-agnostic approach named
Structural Language Model to tackle the problem of code comple-
tion. Based on LSTMs and Transformers, the model leverages the
code syntax to represent a given snippet as a tree. They trained
their model by providing as input an AST representing a partial
expression (statement) missing some consecutive tokens. Their best
model reached an accuracy of 18% for top predictions.

Svyatkovskiy et al. [53] introduced IntelliCode Compose, a multi-
lingual code completion tool able to predict multiple tokens of arbi-
trary types. They use subtokens to overcome the out-of-vocabulary
problem without leveraging high-level structural information like
AST. Their model, trained to predict entire statements of Python
language, achieves a perplexity of 1.82.

A Transformer-based architecture was presented by Liu et al. [41].
They pre-trained their model to incorporate both code understand-
ing and generation tasks. Then, they fine-tuned it on the classic
code completion task, predicting the next token the developer is
likely to write.

Svyatkovskiy et al. [54] tackled code completion from a different
perspective, shifting the problem from the generation of code to the
ranking of proposed solutions. They used static analysis, a cheaper
algorithm in terms of memory footprint than generative models, to
provide a list of recommendations to their DL model, which learns
to rerank them.

Jian et al. [39] proposed a pointer mixture deep learning model
for Python that leverages the pointer copy mechanism to address
the out-of-vocabulary problem. The idea behind the proposed archi-
tecture is to pick an out-of-vocabulary word from the local context
through a pointer component when it is not possible to generate a
within-vocabulary token.

Recently, Aye and Kaiser [4] proposed a model able to predict the
next top-𝑘 tokens while taking into account (i) prediction latency,
(ii) size of the model and its memory footprint, and (iii) validity of
suggestions. These are all actual constraints for real-world archi-
tectures.

MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota

Chen et al. [9] exploited a DL model for API recommendation,
merging structural and textual code information retrieved from
the API context graph and the code token network. Their model
significantly outperforms the existing statistical and DL approaches
for API recommendation based on graphs and trees.

Chen et al. introduced Copilot [10], a new GPT model trained
on more than 150Gb of data from GitHub. Their trained model
achieved state-of-the-art performance in the demanding task of
predicting the whole method when providing as input to the model
just a natural language description of what the developer wants
to implement. Since the dataset used to train Copilot is not pub-
licly available, in our work we adopted another recent DL-based
approach proposed by Ciniselli et al. [12], that has been already
previously described.

5.2 Studies Investigating the Introduction of

Code Clones

A plethora of techniques have been proposed in the literature to
identify code clones in software systems (i.e., code clone detection).
These techniques exploit information extracted from the AST [6, 8,
52], use a graph-based representation of code [23, 36, 37], or look at
token-based similarities [28, 29, 33]. A complete coverage of such a
topic can be be found in the literature review by Rattan et al. [47].
In this section, we focus on studies investigating the introduction
of code clones, since those are the most related to our work.

Kim et al. [34] investigated the reasons behind the copy and paste
mechanism used by developers. Although in 74% of cases the copy
involve less than one line of code (i.e., the copy of a single variable or
expression), the copy of snippets of code is still frequent (∼25%) and
sometimes dictated by the programming language itself, pushing
the copy of syntactic templates. Even the difficulty in understanding
large systems fosters the copying of functionalities implemented in
the past.

Li et al. [40] showed that copy-pasting code can reduce the pro-
gramming effort. Moreover, copying a code snippet instead of ab-
stracting it in a new function can reduce overhead in execution,
even though sometimes this may introduce some bugs, generally
due to errors during the copy. Also Jiang et al. [27] highlight as
developers feel less comfortable with completely new code that
may introduce subtle bugs, and prefer code reuse when possible.

Kapser and Godfrey [30] showed that developers significantly
use code clones while developing software, and highlighted good
motivations behind such a practice. For example, code clones are
sometimes introduced as side effect of programmers’ memory, since
developers tend to repeat common solutions without knowing that
they are already present in the source code. This can also improve
the readability of source code. Moreover, when the same compo-
nent interacts with lots of different systems, forking the code (i.e.,
copying a part of code that evolves independently from the remain-
ing code base) is a safe solution that can avoid refactoring costs
and bug introduction.

The seek of simplicity has also been reported as a motivation for
clone introduction by Baker [5]. The author also reported that the
process management might encourage duplication since developers’
performance is evaluated based on how much new code they write,
which does not promote the refactoring of old code.

To the best of our knowledge, the study mostly related to the
one we conducted has been presented by Albert Ziegler in a blog
post about GitHub Copilot4. The author investigated the extent to
which GitHub Copilot suggestions are copied from the training set
they used and concluded that Copilot rarely recommends pieces
of code that are verbatim copies of instances from the training set,
and when this happens these are coding solutions recurring across
several systems, that also humans are likely to reuse. Considering
the complexity of the recommendations generated by Copilot (i.e.,
up to entire functions), our results support such a conclusion since
we found that, when the DL model is used to recommend several
code statements, it is very unlikely to provide cloned code as output.

6 CONCLUSION
Code recommenders are becoming more and more popular. GitHub
Copilot [10] substantially pushed the adoption of these tools by
developers, making central questions related to possible licensing
issues that may come from their adoption in a commercial setting.
Indeed, these tools are usually trained on open source code, the
usage of which is regulated by FOSS licenses.

While the posed licensing questions are part of a non-trivial
debate among the open source community, researchers, and tool
builders, we started investigating a concrete and related research
question: To what extent do DL-based code recommender systems
generate predictions by cloning code snippets from the training set?
We trained a Text-To-Text-Transfer-Transformer (T5) model on
over ∼2M Java method with the task of recommending code blocks
aimed at finalizing the methods’ implementation. Then, we used a
clone detector to check whether the predictions it generated on the
test set represent Type-1 or Type-2 clones of instances in the train-
ing datasets. Our findings show that, while for short predictions
the trained deep learning model is likely to generate a non-trivial
percentage of clones (∼10% Type-1 and ∼80% Type-2), such a per-
centage quickly approaches 0% when the model generates more
complex predictions composed my at least four code statements.

In summary, our findings suggest that the likelihood of obtaining
clones generated by DL-based code recommenders that are possibly
“harmful” in terms of licensing issues is extremely low.

Our future work will mainly focus on replicating our work on
larger, more performant, DL-based code recommenders and differ-
ent clone detectors, with the goal of confirming or confuting our
findings.

The material used in this study is publicly available [13].

ACKNOWLEDGMENTS
This project has received funding from the European Research
Council (ERC) under the European Union’s Horizon 2020 research
and innovation programme (grant agreement No. 851720).

4https://docs.github.com/en/github/copilot/research-recitation

To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set? MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

REFERENCES
[1] Milton Allamanis. [n.d.]. CodeSearchNet Deduplication Algorithm. https://github.
com/github/CodeSearchNet/blob/master/src/dataextraction/dedup_split.py.
[2] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learning models of code. In Proceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software. 143–153.

[3] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural language
models of code. In International Conference on Machine Learning. PMLR, 245–256.
[4] Gareth Ari Aye and Gail E Kaiser. 2020. Sequence Model Design for Code

Completion in the Modern IDE. arXiv preprint arXiv:2004.05249 (2020).

[5] Brenda S Baker. 1995. On finding duplication and near-duplication in large
software systems. In Proceedings of 2nd Working Conference on Reverse Engineering.
IEEE, 86–95.

[6] Ira D Baxter, Andrew Yahin, Leonardo Moura, Marcelo Sant’Anna, and Lorraine
Bier. 1998. Clone detection using abstract syntax trees. In Proceedings. Interna-
tional Conference on Software Maintenance (Cat. No. 98CB36272). IEEE, 368–377.
[7] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from Examples
to Improve Code Completion Systems. In Proceedings of the 7th Joint Meeting of
the European Software Engineering Conference and the ACM SIGSOFT Symposium
on The Foundations of Software Engineering (ESEC/FSE 2009). 213–222.

[8] Peter Bulychev and Marius Minea. 2008. Duplicate code detection using anti-
unification. In Proceedings of the Spring/Summer Young Researchers Colloquium
on Software Engineering.

[9] Chi Chen, Xin Peng, Zhenchang Xing, Jun Sun, Xin Wang, Yifan Zhao, and
Wenyun Zhao. 2021. Holistic combination of structural and textual code infor-
mation for context based API recommendation. IEEE Transactions on Software
Engineering (2021).

[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
et al. 2021. Evaluating large language models trained on code. arXiv preprint
arXiv:2107.03374 (2021).

[11] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Antonio Mastropaolo, Emad
Aghajani, Denys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. 2021.
Replication Package https://github.com/mciniselli/T5_Replication_Package.git.
[12] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Antonio Mastropaolo, Emad
Aghajani, Denys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. 2022.
An Empirical Study on the Usage of Transformer Models for Code Completion.
IEEE Transactions on Software Engineering (2022).

[13] Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota. 2022. Replication Pack-

age https://doi.org/10.5281/zenodo.6460983.

[14] Miguel Domingo, Mercedes Garcıa-Martınez, Alexandre Helle, Francisco Casacu-
berta, and Manuel Herranz. 2018. How Much Does Tokenization Affect Neural
Machine Translation? arXiv preprint arXiv:1812.08621 (2018).

[15] Philip Gage. 1994. A New Algorithm for Data Compression. C Users J. 12, 2

(1994), 23?38.

[16] Tihomir Gvero, Viktor Kuncak, Ivan Kuraj, and Ruzica Piskac. 2013. Complete
completion using types and weights. In ACM SIGPLAN Conference on Program-
ming Language Design and Implementation, PLDI ’13, Seattle, WA, USA, June 16-19,
2013. 27–38.

[17] Sangmok Han, David R Wallace, and Robert C Miller. 2009. Code completion
from abbreviated input. In 2009 IEEE/ACM International Conference on Automated
Software Engineering. IEEE, 332–343.

[18] Sangmok Han, David R Wallace, and Robert C Miller. 2011. Code completion of
multiple keywords from abbreviated input. Automated Software Engineering 18,
3-4 (2011), 363–398.

[19] Simon Harris. [n.d.]. Simian https://www.harukizaemon.com/simian.
[20] Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are Deep Neural Net-
works the Best Choice for Modeling Source Code?. In Proceedings of the 2017 11th
Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2017). 763?773.
[21] Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto Bacchelli.
2019. When code completion fails: a case study on real-world completions. In
Proceedings of the 41st International Conference on Software Engineering, ICSE
2019, Montreal, QC, Canada, May 25-31, 2019. 960–970.

[22] Yoshiki Higo and Shinji Kusumoto. 2009. Enhancing quality of code clone
detection with program dependency graph. In 2009 16th Working Conference on
Reverse Engineering. IEEE, 315–316.

[23] Yoshiki Higo and Shinji Kusumoto. 2011. Code clone detection on specialized
PDGs with heuristics. In 2011 15th European Conference on Software Maintenance
and Reengineering. IEEE, 75–84.

[24] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the Naturalness of Software. In Proceedings of the 34th International
Conference on Software Engineering (ICSE 2012). IEEE Press, 837–847.

[25] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic
Code Search. CoRR abs/1909.09436 (2019). http://arxiv.org/abs/1909.09436

[26] IntelliJ. [n.d.]. IntelliJ https://www.jetbrains.com/idea/.

[27] Lingxiao Jiang, Zhendong Su, and Edwin Chiu. 2007. Context-based detection of
clone-related bugs. In Proceedings of the the 6th joint meeting of the European soft-
ware engineering conference and the ACM SIGSOFT symposium on The foundations
of software engineering. 55–64.

[28] T. Kamiya. [n.d.]. Simian http://www.ccfinder.net.
[29] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. 2002. CCFinder: A
multilinguistic token-based code clone detection system for large scale source
code. IEEE Transactions on Software Engineering 28, 7 (2002), 654–670.

[30] Cory J Kapser and Michael W Godfrey. 2008. “Cloning considered harmful” con-
sidered harmful: patterns of cloning in software. Empirical Software Engineering
13, 6 (2008), 645–692.

[31] Rafael-Michael Karampatsis and Charles A. Sutton. 2019. Maybe Deep Neural
Networks are the Best Choice for Modeling Source Code. CoRR abs/1903.05734
(2019). http://arxiv.org/abs/1903.05734

[32] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
Andrea Janes. 2020. Big Code != Big Vocabulary: Open-Vocabulary Models for
Source Code. In Proceedings of the 42nd International Conference on Software
Engineering, ICSE 2020. To Appear.

[33] Shinji Kawaguchi, Takanobu Yamashina, Hidetake Uwano, Kyohei Fushida, Ya-
sutaka Kamei, Masataka Nagura, and Hajimu Iida. 2009. Shinobi: A tool for
automatic code clone detection in the ide. In 2009 16th Working Conference on
Reverse Engineering. IEEE, 313–314.

[34] Miryung Kim, Lawrence Bergman, Tessa Lau, and David Notkin. 2004. An ethno-
graphic study of copy and paste programming practices in OOPL. In Proceedings.
2004 International Symposium on Empirical Software Engineering, 2004. ISESE’04.
IEEE, 83–92.

[35] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code pre-
diction by feeding trees to transformers. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE). IEEE, 150–162.

[36] Raghavan Komondoor and Susan Horwitz. 2001. Using slicing to identify du-
plication in source code. In International static analysis symposium. Springer,
40–56.

[37] Jens Krinke. 2001. Identifying similar code with program dependence graphs. In

Proceedings Eighth Working Conference on Reverse Engineering. IEEE, 301–309.

[38] Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language
independent subword tokenizer and detokenizer for Neural Text Processing.
CoRR abs/1808.06226 (2018).

[39] Jian Li, Yue Wang, Michael R Lyu, and Irwin King. 2017. Code completion with

neural attention and pointer networks. arXiv preprint arXiv:1711.09573 (2017).

[40] Zhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan Zhou. 2006. CP-Miner:
Finding copy-paste and related bugs in large-scale software code. IEEE Transac-
tions on software Engineering 32, 3 (2006), 176–192.

[41] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning based
Pre-trained Language Model for Code Completion. In Proceedings of the 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE
2020). Association for Computing Machinery.

[42] Cristina V Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny,
Hitesh Sajnani, and Jan Vitek. 2017. DéjàVu: a map of code duplicates on GitHub.
Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1–28.
[43] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
usage of text-to-text transfer transformer to support code-related tasks. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
336–347.

[44] Paul McNamee and James Mayfield. 2004. Character n-gram tokenization for
European language text retrieval. Information retrieval 7, 1 (2004), 73–97.

[45] PMD. [n.d.]. Cpd http://pmd.sourceforge.net/.
[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.
Exploring
the Limits of Transfer Learning with a Unified Text-to-Text Transformer.
arXiv:1910.10683 [cs.LG]

[47] Dhavleesh Rattan, Rajesh Bhatia, and Maninder Singh. 2013. Software clone
detection: A systematic review. Information and Software Technology 55, 7 (2013),
1165–1199.

[48] Romain Robbes and Michele Lanza. 2010. Improving Code Completion with
Program History. Automated Software Engineering 17, 2 (2010), 181–212.
[49] Chanchal Kumar Roy and James R Cordy. 2007. A survey on software clone
detection research. Queen’s School of Computing TR 541, 115 (2007), 64–68.
[50] Chanchal K Roy and James R Cordy. 2008. NICAD: Accurate detection of near-
miss intentional clones using flexible pretty-printing and code normalization. In
2008 16th iEEE international conference on program comprehension. IEEE, 172–181.
[51] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine
translation of rare words with subword units. arXiv preprint arXiv:1508.07909
(2015).

[52] SimScan. [n.d.]. Simian http://www.blue-edge.bg/download.html.
[53] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020.
Intellicode compose: Code generation using transformer. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium

MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota

on the Foundations of Software Engineering. 1433–1443.

Methodol. 28, 4 (2019), 19:1–19:29.

[54] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana Vi-
cente Franco, and Miltiadis Allamanis. 2021. Fast and Memory-Efficient Neural
Code Completion. In 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR). IEEE, 329–340.

[55] Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. 2014. On the Localness
of Software. In Proceedings of the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering (Hong Kong, China) (FSE 2014). Association
for Computing Machinery, New York, NY, USA, 269–280. https://doi.org/10.
1145/2635868.2635875

[56] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. 2019. An Empirical Study on Learning Bug-Fixing
Patches in the Wild via Neural Machine Translation. ACM Trans. Softw. Eng.

[57] Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and
Gabriele Bavota. 2021. Towards Automating Code Review Activities. In 43rd
IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid,
Spain, 22-30 May 2021. IEEE, 163–174.

[58] Martin White, Christopher Vendome, Mario Linares-Vásquez, and Denys Poshy-
vanyk. 2015. Toward Deep Learning Software Repositories. In Proceedings of the
12th Working Conference on Mining Software Repositories (Florence, Italy) (MSR
’15). IEEE Press, Piscataway, NJ, USA, 334–345. http://dl.acm.org/citation.cfm?
id=2820518.2820559

[59] Terry Yin. [n.d.]. Lizard https://pypi.org/project/lizard/.
[60] Jerrold H. Zar. 1972. Significance Testing of the Spearman Rank Correlation

Coefficient. J. Amer. Statist. Assoc. 67, 339 (1972), pp. 578–580.

