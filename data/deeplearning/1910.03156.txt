9
1
0
2

t
c
O
8

]
E
S
.
s
c
[

1
v
6
5
1
3
0
.
0
1
9
1
:
v
i
X
r
a

Software Engineering Practice in the Development of
Deep Learning Applications

Xufan Zhang
The State Key Laboratory of Novel Software Technology
Nanjing, Jiangsu, China
xufan.zhang@outlook.com

Yilin Yang
The State Key Laboratory of Novel Software Technology
Nanjing, Jiangsu, China
DG1732004@smail.nju.edu.cn

Yang Feng∗
The State Key Laboratory of Novel Software Technology
Nanjing, Jiangsu, China
charles.fengy@gmail.com

Zhenyu Chen∗
The State Key Laboratory of Novel Software Technology
Nanjing, Jiangsu, China
zychen@nju.edu.cn

ABSTRACT
Deep-Learning(DL) applications have been widely employed to as-
sist in various tasks. They are constructed based on a data-driven
programming paradigm that is diﬀerent from conventional soft-
ware applications. Given the increasing popularity and importance
of DL applications, software engineering practitioners have some
techniques speciﬁcally for them. However, little research is con-
ducted to identify the challenges and lacks in practice. To ﬁll this
gap, in this paper, we surveyed 195 practitioners to understand
their insight and experience in the software engineering practice
of DL applications. Speciﬁcally, we asked the respondents to iden-
tify lacks and challenges in the practice of the development life cy-
cle of DL applications. The results present 13 ﬁndings that provide
us with a better understanding of software engineering practice
of DL applications. Further, we distil these ﬁndings into 7 action-
able recommendations for software engineering researchers and
practitioners to improve the development of DL applications.

KEYWORDS
practitioner perception, deep learning applications, software engi-
neering in practice

ACM Reference Format:
Xufan Zhang, Yilin Yang, Yang Feng, and Zhenyu Chen. 2019. Software
Engineering Practice in the Development of Deep Learning Applications.
In ICSE’42, May 23 - 29, 2020, Seoul, South Korea. ACM, New York, NY, USA,
11 pages. https://doi.org/***

1 INTRODUCTION
The tremendous advancement of deep learning(DL) techniques has
driven the emergence of DL applications that oﬀer commercial

∗Yang Feng and Zhenyu Chen are the corresponding authors.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ICSE’42, May 23 - 29, 2020, Seoul, South Korea
© 2019 Copyright held by the owner/author(s).
ACM ISBN ***.
https://doi.org/***

beneﬁts to humans, and they are gradually deployed in more life-
critical ﬁelds, such as medical diagnosis [11, 32], in autonomous ve-
hicles [5, 14], and air traﬃc control [17]. Under this circumstance,
the development of DL applications has become vital for their suc-
cess. However, the development of DL applications adopts a pro-
gramming paradigm and practice that is entirely diﬀerent from
conventional software applications [1, 34, 35]. Therefore, methods,
metrics, and techniques that are designed for the development of
conventional software applications may become less eﬀective to
be applied to the development of DL applications.

To facilitate the development of DL applications, software engi-
neering researchers have proposed several metrics [18–20, 27] and
techniques [9, 12, 22, 29, 33], however, it is unclear what kinds of
lacks and challenges practitioners face in developing DL applica-
tions. Understanding the software engineering practices of devel-
oping DL applications is the ﬁrst, yet critical, step towards building
useful and eﬀective techniques. Several prior research has been
conducted to investigate the challenges in developing DL appli-
cations [31, 35] and also to characterize the bugs of DL frame-
works [15, 36]. Zhang et al. [35] conduct an empirical study on the
deep learning questions on Stack Overﬂow and present a classiﬁ-
cation model to quantify the distribution of diﬀerent kinds of deep
learning questions. Their study reveals the common root causes
of bugs and the most frequently asked questions in building appli-
cations based on the DL frameworks. Wan et al. [31] performed a
mixture of qualitative and quantitative studies to investigate the
diﬀerences in software practices and practitioners’ work due to
the impact of machine learning. Zhang et al. [36] have conducted
the ﬁrst study to investigate the characteristics of the Tensorﬂow.
They manually inspect and reproduce 75 Tensorﬂow bug reports
from Github and 76 Tensorﬂow bug reports from StackOverﬂow.
The study investigates the symptoms and common bug types of
the Tensorﬂow framework. Also, they identify 5 challenges for re-
searchers. Islam et al.[15] inspect high-quality posts related to deep
learning libraries on Stack Overﬂow and Github. They summarize
the types of bugs, root causes of bugs, eﬀects of bugs, bug-prone
stage of deep learning pipeline as well as whether there are some
common anti-patterns found in this buggy software. Even though
this research have laid the ﬁrst step towards understanding the
new development paradigm, little research is conducted to under-
stand the insights, experience, and expectations from the practi-
tioners’ perspective.

 
 
 
 
 
 
ICSE’42, May 23 - 29, 2020, Seoul, South Korea

Xufan Zhang, Yilin Yang, Yang Feng, and Zhenyu Chen

In this study, we complement existing empirical studies by con-
ducting a comprehensive survey with 195 industrial practitioners
to understand the characteristics of each phase of the DL applica-
tion development. We ﬁrst conduct a literature review to under-
stand the current software engineering studies on the DL applica-
tion development. And then, we conduct interviews with eight de-
velopers from Baidu, Alibaba, and Huawei. Based on the literature
review and interviews, we summarize the scope and focuses on a
questionnaire. Finally, we distributed this questionnaire to practi-
tioners from various companies with diﬀerent backgrounds to pro-
vide feedback and opinions. We received a total of 221 responses,
195 of them are valid. Speciﬁcally, we also invited respondents to
provide their rationale for the two hottest topics, i.e., testing and
debugging. These ﬁndings and feedback provide us with a com-
prehensive understanding of the vision and challenges of the DL
application development.

The main contributions of our work are as follows:

• We conduct a comprehensive survey with 195 practitioners
to investigate the software engineering practice of develop-
ing DL applications.

• We summarize the results of this survey into 13 ﬁndings,
which present the insight, experience, and expectations of
practitioners. Also, these ﬁndings reveal the impacts and
challenges in all phases of the DL application development
life cycle.

• We distill these ﬁndings into 7 actionable recommendations
for software engineering researchers. These recommenda-
tions can give researchers insight into designing various
techniques for improving the development of DL applica-
tions.

2 METHODOLOGY
Our study consists of three parts: 1. a literature review is conducted
to identify current software engineering research topics on the de-
velopment of deep learning applications; 2. eight interviews with
practitioners from Baidu, Alibaba, and Huawei are conducted to ob-
tain insights into practitioner views to help us formulate a set of
hypotheses; 3. a questionnaire summarized from the previous two
steps is distributed to 822 practitioners with diﬀerent backgrounds.

2.1 Literature Review
To understand the state-of-the-arts on the development of DL ap-
plications, we conducted a literature review. We ﬁrst pick up 10
software engineering conferences, including ICSE, FSE, ASE, IC-
SME, MSR, SANER, ESEM, ICPC, ISSTA, ICST, and 7 journals, in-
cluding TOSEM, TSE, EMSE, ASE, JSS, IST, to collect relevant pa-
pers. We search the keywords "machine learning", "deep learning"
and "deep neural network" in the paper title and abstract. Note that
goal of this study is to identify challenges or problems in devel-
oping DL applications but not applying DL techniques to address
software engineering problems. To ensure the paper’s topic ﬁts our
goal, the ﬁrst three authors read the abstract of each search result
in the process.

Finally, we obtained 16 papers, and summarize them as follows.

Requirement Analysis: There is only one paper to discuss the
requirement analysis in developing deep learning applications. Be-
lan et al. [4] analyzed the related work from software engineering
and AI ﬁelds that deal with requirements suitable for specifying a
machine learning-based software solution. This work contributes
to agent-based software engineering, goal-oriented requirements
engineering, and practices for product development in companies.
Testing & Debugging: To improve the quality of intelligent ap-
plications, researchers have cast substantial eﬀorts on proposing
speciﬁc testing and debugging techniques [12, 19, 20, 22, 28, 30,
33, 37]. Pei et al.[22] designed, implemented and evaluated DeepX-
plore, the ﬁrst white-box framework for system testing of real DL
systems. They introduce neuron coverage to systematically mea-
sure portions of the DL system that are run by the test input and
leverage multiple DL systems with similar functionality to cross-
reference gods to avoid manual inspection. Tian et al.[30] designed
and developed DeepTest, a system testing tool for automatically de-
tecting the wrong behavior of DNN-driven vehicles that can cause
fatal collisions. Ma et al.[19] proposed DeepGauge, a set of multi-
granularity testing standards for DL systems designed to portray
test platforms in many ways. Their in-depth evaluation of test stan-
dards was demonstrated in two well-known data sets, ﬁve DL sys-
tems, and four state-of-the-art countermeasures against DL. Sun
et al.[28] ﬁrst introduced an analytical test method for deep neural
networks (DNN). Ma et al. [20] presented a mutation testing frame-
work for DNNs aiming at evaluating the quality of datasets. Guo et
al.[12] proposed DLFuzz, the ﬁrst diﬀerential fuzzy testing frame-
work to guide the DL system to expose incorrect behavior. DLFuzz
continually changes the input to maximize neuronal coverage and
prediction diﬀerences between raw input and mutated input, with-
out the need to manually mark work or cross-reference oracles of
other DL systems with the same functionality. Xie et al. [33] pre-
sented an automated fuzz testing framework, namely DeepHunter,
for hunting potential defects of general-purpose DNNs. DeepHunter
is designed based on metamorphic testing. It generates new seman-
tically preserved tests, and leverages multiple plugable coverage
criteria as feedback to guide the test generation from diﬀerent per-
spectives.

Empirical studies on characterizing the development of
DL applications: Zhang[36] et al. researched the program bug of
Tensorﬂow. They ﬁltered the collected information from GitHub
and StackOverﬂow, manually obtained more than 100 ﬁne samples.
Through analysis, they summarized three symptoms of symptoms
(Symptoms) and six root causes. Wan et al. [31] studied the features
and impacts of machine learning to bring into software develop-
ment. They compare various aspects of software engineering (e.g.,
requirements, design, testing, and process) and work characteris-
tics (e.g., skill variety, problem-solving and task identity) in both
the ML systems and conventional software systems. Guo[13] con-
ducted a study on how various mainstream DL frameworks and
platforms inﬂuence both DL software development and deploy-
ment in practice. Islam[15] studied 2716 high-quality posts from
Stack Overﬂow and 500 bug ﬁx commits from Github about ﬁve
popular deep learning libraries to understand the types of bugs,
root causes of bugs, impacts of bugs, bug-prone stage of deep learn-
ing pipeline as well as whether there are some common anti-patterns

Software Engineering Practice in the Development of
Deep Learning Applications

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

found in this buggy software. Najafabadi[21] investigates some as-
pects of deep learning research that need further exploration to
incorporate speciﬁc challenges introduced by big data analytics,
including data of various formats and features. Fu et al. [10] con-
ducted a case study shows that applying a very simple optimizer,
called diﬀerential evolution, to the ﬁne-tune SVM can achieve sim-
ilar results with much less time cost. This study casts doubts on the
necessity of applying deep learning techniques in practice. Amer-
shi et al. [1] report their experience on developing AI-based sys-
tems in Microsoft. Their work identiﬁed three aspects of the AI
domain that make it fundamentally diﬀerent from prior software
application.

Unfortunately, except for the empirical study papers, we did not
ﬁnd any papers present novel techniques and methods speciﬁcally
for the software engineering phases of design, implementation, de-
ployment, and maintenance.

2.2 Interviews
To get deeper insights into designing the questionnaire, we con-
ducted eight interviews with engineers from our industry partner
companies, i.e., Baidu, Alibaba, and Huawei. Because these compa-
nies have invested many resources in developing DL applications,
DL frameworks, and related infrastructures, their engineers have
suﬃcient experience and insights on our research topic. Note that
the goal of this study is to investigate the lacks and challenges in
each phase of developing DL applications, thus, the interviewees
consist of one project manager, one product designer, two testers,
three developers, and one project maintainers. We visited his/her
company and conducted the interview face-to-face.

During each interview, we kept to the following process: First,
we explained to the interviewee the motivation of conducting the
interview. And then we ask the interviewee to describe his/her job
responsibility and discuss the challenges and lacks in building the
DL application. After that, we discussed the related research topics
identiﬁed in the literature review with the interviewee. We let the
interviewee talk most of the time. The whole interview lasts about
60 to 90 mins. We followed the methodology presented in [2, 25]
to decide when to stop interviewing, i.e. stopping interviews when
the saturation of the themes is reached.

After we have ﬁnished the interview, each of the ﬁrst three
authors separately summarizes the lacks, challenges, and expecta-
tions mentioned in the interview. And then the three discuss each
point to form a summary. Note that any discrepancy is discussed
until a consensus is reached. Finally, we send the summary back
to the interviewee to conﬁrm its correctness.

2.3 Questionnaire Design
Based on the literature review and interviews, we obtained a pre-
liminary understanding of the lacks and challenges for the soft-
ware engineering practice in the development of DL applications.
For each phase of developing DL applications, we summarized fo-
cuses of software researchers and our interviewees into Table 1. In
Table 1, the column Derived from denotes the source of this foci
come from, and the |Questions | denotes the number of questions
designed for this foci. In total, we design 18 multiple choices ques-
tions based on these focuses.

3 RESULTS AND FINDINGS
3.1 Demographics
We include a number of demographic questions in the question-
naire. The demographic questions are designed to understand the
background and experience of respondents.

Among all the respondents, 30 of them are junior practitioners
with less than one-year work experience, 41 of them work in soft-
ware engineering for 1-3 years, and 124 of them are experienced
practitioners with more than 3 years of work experience. Since ex-
perienced respondents are more likely to be practitioners of best
practices in their corresponding ﬁeld, intuitively our survey results
of these practitioners reﬂect industry practices.

Meanwhile, we investigate the job roles of our respondents. We
have respondents who work as requirement engineers, software
architects, software developers, software testers, and software op-
erations engineers. These practitioners perform tasks related to DL
app development in phase of the software development life cycle
corresponding to their job role. Thus they can provide us real feed-
back from the industry.

We design a general question to ﬁgure out the primary inﬂuence
brought by DL. Diﬃculties in software engineering practices cause
an increase in labor work. 8 primary tasks performed in software
engineering are on the list. Respondents were expected to choose
the tasks where more labor work is required according to their
own work experience. To ﬁgure out which tasks pain points and
diﬃculties exist, we divide respondents into groups according to
their roles in software practice. The result is shown is Table 2.

In summary, requirement analysis, integration and acceptance
testing, and problem deﬁnition are more likely to be labor-consuming.
We detail the summarization results for the designed questions re-
garding each phase in the software development life cycle. We fur-
ther present the ﬁndings and provide answers to the research ques-
tions in the previous section.

3.2 Requirement analysis
Software engineers are expected to transform problems into rea-
soning logic reﬂected by the software system during requirement
analysis. However, it is well-recognized by respondents that re-
quirement analysis is more diﬃcult in DL applications.

While applications are considered to be more intelligent, fewer
business rules are pre-deﬁned. Applications are expected to learn
these rules from the given data. In consequence, the reasoning
logic is hidden behind, which makes it hard to be clariﬁed by re-
quirement engineers. To make the case worse, the DNN does not
take raw data as inputs, data needs to be prepared to form fea-
ture vectors. However, despite the fact that the performance of the
model depends highly on these feature vectors, what is learned by
the DNN model is hard to interpret. As a result, it is more diﬃcult
for requirement engineers to transfer the problem deﬁnition into
speciﬁcations.

Finding 1: It is a challenging task to identify features over a
large amount of data and verify its rationality in the requirement
analysis phase.

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

Xufan Zhang, Yilin Yang, Yang Feng, and Zhenyu Chen

Table 1: The focuses for each phase in the Questionnaire

Phase

Focuses

Resource Management What are the motivations of redesigning and retraining the DNN models?
Requirement Analysis What are the motivations of employing DL techniques?

Design

Implementation

Testing & Debugging
Deployment
Maintenance

What are the challenges in the design of DL apps?
How to implement the DNN models?
How to mitigate problems in the data-driven programming paradigm?
What are the primary training/testing data sources?
What are the testing & debugging methods ?
What the metrics are used to guide the testing process?
How to reduce the size of DNN models?
What are the motivations of redesigning and retraining the DNN models?

|Question| Derived from
[10],Interviews
[4]; Interviews
[1, 13, 35]
[34, 35]
[31, 35]
Interviews
[9, 12, 22, 29, 33]
[18–20, 27]
Interviews
Interviews

2
2
3
1
2
1
4
1
1
1

Table 2: Statistics on extra labor work required for tasks

Requirement
engineer
Software architect
Developer
Tester
Operation &
maintenance engineer
Totalavд

Problem
deﬁnition

Feasibility
study

Requirement
analysis

Summary
design

Detailed
design

Implementation
& unit test

Test (integra-
tion/acceptance)

Software
maintenance

42.86%

64.00%
23.26%
25.00%

25.00%

30.61%

14.29%

32.00%
39.53%
25.00%

50.00%

29.08%

35.71%

60.00%
48.84%
35.23%

25.00%

40.82%

35.71%

20.00%
11.63%
9.09%

25.00%

13.78%

21.43%

12.00%
27.91%
20.45%

50.00%

20.41%

0.00%

16.00%
18.60%
39.77%

25.00%

27.55

7.14%

16.00%
16.28%
54.55%

50.00%

36.22%

7.14%

16.00%
13.95%
13.64%

25.00%

15.82%

Diﬃcult as it could be, according to our survey, practitioners
from various industrial ﬁelds claim to provide intelligent services
in their applications nowadays. According to the result provided
by respondents regarding the question "What are the motivation
of employing DL techniques", there are two main motivations for
practitioners to leverage this technique, as is shown in Table 3.

Table 3: Motivations of employing DL techniques

Option

Reasons

Votes(Ratios)

A

B

C

D

DL is a promising feature needed to
be introduced as soon as possible.

DL technique has been widely used
in this area.

Some preliminary research results
have been obtained via DL.

The feature to be implemented lacks
the deﬁned rule deﬁnition but has
a large amount of business data

21.43%

14.29%

35.71%

28.57%

We ﬁnd that about 14.29% of respondents claimed that "DL tech-
nique has been widely used in this area" even though it just becomes
a hot topic in recent years. We further ﬁltered out responses sub-
mitted by them to check their experience and found that 60.71% of
them are experienced practitioners, 17.86% of them have 1-3 years
work experience while 21.43% of them have work experience less
than one year.

Meanwhile, we count the category of applications they worked
on. Shopping(39.29%) is in the ﬁrst place, where accurate and at-
tractive recommendations are used to broker a deal, indicating that
the DL technique performs remarkably in recommender systems.

Finding 2: Shopping is the leading category of applications
where DNN models are used. DL techniques are widely adopted
and achieves great performance in recommending commodities.

Respondents who claimed that "DL is a promising feature needed
to be introduced as soon as possible" are considered to be normal de-
velopers attracted by technology. About 21.43% of practitioners are
optimistic about DL techniques, which indicates that only about
20% of developers in the market are newly attracted by DL.

Respondents who claimed that "Some preliminary research re-
sults have been obtained via DL" were considered to be forerunners
of software application development in their corresponding ﬁelds
as they were more likely to try innovative technologies. Owing to
the breakthroughs DNNs achieved in some areas in recent years,
about 35.71% of practitioners were willing to explore the possibility
and performance of integrating DL into the current application.

Meanwhile, those who claimed that "The feature to be imple-
mented lacks the deﬁned rule deﬁnition but has a large amount of
business data" were considered to be practitioners in need of ma-
chine learning approaches. About 28.57% of practitioners regarded
DL as a choice to mine value of business data since it is not easy to
identify the relationships behind these data with human eﬀorts.

Software Engineering Practice in the Development of
Deep Learning Applications

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

3.3 System design
System design is the process of designing the elements of a system,
including modules and components. Design is prepared from the
requirement speciﬁcations produced in the ﬁrst phase. We analyze
on challenges in the design of DL applications in Table 4.

Table 4: Challenges in the design of DL applications

Work experience
Network model design
Concurrency processing
Traﬃc control
No diﬀerence with
conventional applications

<1 year
43.33%
46.67%
40.00%

1-3 years
64.29%
52.38%
28.57%

>3 years
68.00%
53.60%
20.80%

26.67%

26.19%

8.80%

Compared to practitioners who had less than 3 years of expe-
rience, only 8.80% of the experienced practitioners claimed that
there is no diﬀerence with conventional applications. However,
over 26.00% of practitioners with less than 3 years of experience
claimed that there is no diﬀerence between DL applications and
conventional applications. The ratio of people who think there is
no diﬀerence drops from about 26.00% to 8.80%, indicating that it
generally takes about 3 years for practitioners to understand the
DL technique. It is well-accepted that diﬀerences exist in the de-
signing phase by them.

Finding 3: It takes about 3 years on average for a practitioner
to become experienced guy in DL application development.

According to our research, 63.45% of the respondents on aver-
age held the opinion that the design of neural networks is a nodus
to develop a DL app, even though 68.00% of them had more than
3-year experience in software engineering. More speciﬁcally, even
with a series of open-source deep learning framework in the com-
munity, it is still very diﬃcult to design a neural network which
can perfectly ﬁt the problem at hand. To ﬁgure out the situation re-
garding practitioners with diﬀerent experiences, we further divide
the respondents into groups.

Concurrency processing is the second problem practitioners paid
attention to developing DL applications. 52.28% of the respondents
on average claimed about the concurrency processing problem. Sim-
ilarly, experienced practitioners showed more concern on this prob-
lem since the ratio grows gradually from 46.67% for junior practi-
tioners, to 52.38% for practitioners with 1-3 years of experience,
to 53.6% for experienced practitioners. In fact, DNN is not capable
of handling requests in parallel, which can add to the time cost in
case of a large amount of request simultaneously.

Finding 4: Neural network model design and concurrent pro-
cessing are problems to be solved in DL application develop-
ment.

Owing to the fact that DNNs in DL applications are trained
over a huge amount of data. The application should be capable of
transfer more data when handling requests from application users.
About 25.38% of practitioners on average showed their concern
on network traﬃc control. However, it is interesting that the ratio

drops gradually, from 40.00% for practitioners with less than 1-year
experience, to 28.57% for practitioners with 1-3 years experience,
and ﬁnally to 20.80% for practitioners with more than 3-year expe-
rience. The result shows that on one hand, experienced practition-
ers are more capable of solving problems related to network traﬃc
control. On the other hand, there might be a shift of the focus prac-
titioners work on from network traﬃc control to neural network
design and concurrency processing.

Furthermore, we investigate on reasons why it is diﬃcult to de-

sign a DNN. The result is shown in Table 5.

Table 5: Diﬃculties in designing deep neural networks

Work experience
Structure
Performance
Environment
Implementation

<1 year
36.67%
50.00%
43.33%
16.67%

1-3 years
42.86%
69.05%
54.76%
33.33%

>3 years Totalavд
53.81%
61.60%
60.41%
60.00%
46.70%
44.80%
24.37%
23.20%

About 60.41% of practitioners on average claimed that the per-
formance of the designed DNN model is unclear during the de-
signing phase, indicating that the performance measurement of a
DNN model remains a big problem in the designing phase. Mean-
while, 53.81% of practitioners on average claimed that the struc-
tural details of the neural network model are not clear. Experienced
practitioners are more likely to show their concerns on this prob-
lem since 61.60% of them chose it. Thus, how to reveal structure-
related information of DNN is another problem to be solved. Be-
sides, 47.60% of practitioners on average claimed that the applica-
tion environment is not clear.

Finding 5: Performance measurement, structure design and ap-
plication environment are three main factors that bring diﬃcul-
ties in designing DNNs.

3.4 Implementation
Coding implementation is the main focus for developers. In this
phase, code blocks are implemented to work together to solve a
problem in the real world.

A wide range of DL frameworks are available for building DNN
models. The popularity of each framework among industrial practi-
tioners is shown in Table 6. In total, TensorFlow(TF), PyTorch, and
Caﬀe are the top 3 most popular deep learning frameworks. To ﬁg-
ure out how practitioners implement the DNN models, according
to the survey, 65.99% of DL application practitioners on average
use TensorFlow to build their neural network models. 30.96% of
them use PyTorch and 18.27% of them use Caﬀe.

We further analyze the popularity of these DL frameworks among
practitioners in groups according to their work experience. For ju-
nior practitioners those who have less than 1 year work experience,
about 60% of them work with TensorFlow. 20% of them work with
PaddlePaddle, a deep learning framework released by Baidu. For
practitioners with 1-3 years work experience, 71.43% of them work
with TensorFlow, 33.33% of them work with PyTorch, and 26.19%
of them work with Caﬀe. For experienced practitioners with more

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

Xufan Zhang, Yilin Yang, Yang Feng, and Zhenyu Chen

Table 6: DL frameworks used to implement DNN models

Work experience
TensorFlow
Caﬀe
PyTorch
Theano
PaddlePaddle
CNTK
MindSpore

<1 year
60.00%
13.33%
13.33%
13.33%
20.00%
6.67%
6.67%

1-3 years
71.43%
26.19%
33.33%
11.90%
14.29%
14.29%
9.52%

>3 years
65.60%
16.80%
34.40%
10.40%
15.20%
11.20%
13.60%

Total
65.99%
18.27%
30.96%
11.17%
15.74%
11.17%
11.68%

than 3 years experience, 65.60% of them work with TensorFlow,
and 34.40% of them work with PyTorch.

Since November, 2015 when the ﬁrst public version of Tensor-
Flow was released, it has won its popularity among practitioners
with diﬀerent work experiences. PyTorch is more popular among
experienced practitioners than among practitioners with less expe-
rience, indicating that it is not easy for junior practitioners to get
familiar with this framework.

Another interesting thing we ﬁnd is that, although PaddlePad-
dle(PP) was not in the list of most popular DL frameworks, it takes
the second place in popularity among junior practitioners of DL
applications. For junior practitioners, detailed documentation and
well-formed community can be a determinant in making a choice.

Finding 6: Most practitioners build their DL applications with
the help of DL frameworks like TensorFlow and PyTorch.

Most of the frameworks are open-source at the moment. How-
ever, MindSpore is a DL framework released by Huawei in Au-
gust, 2019, which is not open-source yet. Thus those respondents
who worked with MindSpore are expected to be practitioners from
teams inside the company. We ﬁlter out answers from these respon-
dents to ﬁnd out eﬀorts that are put on other DL frameworks from
competitors to implement MindSpore. Table 7 shows the results.

Table 7: Frameworks that MindSpore practitioners use

TF

Caﬀe
56.52% 47.83%

PyTorch Theano
34.78%
39.13%

PP

CNTK
34.78% 43.48%

On average, more than 30% those respondents participant in
building applications with DL frameworks released by other com-
panies. More eﬀorts are put for popular DL frameworks, e.g. Ten-
sorFlow, Caﬀe, etc. Another interesting thing we ﬁnd is that about
43.48% of those respondents work with CNTK, which is a less pop-
ular framework. However, it is developed by Microsoft which is
deﬁnitely a strong competitor in this area.

After some statistical analysis on DL frameworks adopted in
the application development, we further investigate on practices
done by practitioners. Regarding conventional application devel-
opment, implementation is an error-prune phase due to misuse of
API, spelling error, uncaught exceptions, etc. However, in DL ap-
plication development, the model is expected to learn from data

without any human eﬀorts after it is implemented. To avoid po-
tential error introduced to the application during the implementa-
tion phase by this data-driven characteristic, developers works on
methods to improve the correctness of both the application and
the model. The result is shown in Table 8.

Table 8: Mitigate problems in the data-driven programming
paradigm

Work experience
Less DL
QC of data
Environment
consistency

<1 year
13.33%
56.67%

1-3 years
19.05%
59.52%

>3 years
16.80%
69.60%

Total
16.75%
65.48%

60.00%

50.00%

56.80%

55.84%

On average, more than 65.48% of practitioners try to solve the
problem by quality control(QC) of data. According to the result, ex-
perienced practitioners pay even more attention to the quality of
the data than junior practitioners, indicating that it is eﬀective to
reduce errors in DL applications. Though quality control of data is
not necessary for developing conventional applications, develop-
ers regard it as an essential approach to control the quality of DL
applications.

Finding 7: Quality control of data is an eﬀective approach to
solve problems brought by the data driven characteristic.

Meanwhile, 55.84% of practitioners on average regard environ-
ment consistency as a valid approach to avoid problems introduced
by data driven. Keeping the consistency of developing, testing and
production environment is indeed a compromise to avoid trigger-
ing numerical related issues brought by environment diﬀerences.
However, practitioners are not oﬀered with better alternatives to
solve the problem at the moment.

Finding 8: Many developers try to avoid numerical related
problems by keeping development environment, testing envi-
ronment and production environment the same.

In conventional applications, developers run unit test to ensure
the correctness of individual functions. To ensure the correctness
of a developed DL model in the implementation phase, practices
done by them are shown in Table 9.

Table 9: Approaches to ensure correctness in DL applications

Work experience
Benchmark
Multiple models
Code review
Unit test

<1 year
66.67%
20.00%
30.00
30.00%

1-3 years
52.38%
47.62%
28.57%
16.67%

>3 years
70.4%
51.2%
31.2%
27.2%

Total
65.99%
45.69%
30.46%
25.38%

Unit test still takes part in guarantee the quality of code in the
implementation phase. However, to ensure the correctness of a DL
application, 65.99% of practitioners on average rely on model evalu-
ation against benchmarks or large self-owned datasets. Meanwhile,
45.69% of practitioners choose to implement multiple models to

Software Engineering Practice in the Development of
Deep Learning Applications

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

function together to ensure the correctness. The ratio of this choice
keeps growing considering the experience of practitioners.

with compatibility issues frequently during the testing phase at the
moment.

Finding 9: Evaluation against benchmarks and implementing
multiple models for a function are two main practices followed
to provide correctness of the DL application in the implementa-
tion phase.

3.5 Testing
Software testing is a process to evaluate the functionality with an
intent to ﬁnd whether the developed software meets the require-
ment or not and to provide guarantee for the quality of the soft-
ware. According to the result in Table 2, this phase is considered
to be one of the phase which receives a great impact after intro-
ducing the DL technique to application development. The biggest
diﬀerence between testing a conventional application and a DL ap-
plication lies in that it requires additional work to test whether the
application under test is equipped with the speciﬁc knowledge.

When requirement analysis is done, testers begin to design test
cases according to the requirement speciﬁcations. Regarding con-
ventional applications, the program is more interpretable as the
processing logic is readable. By analyzing the output together with
various coverage criteria, the quality of such a software can be
guaranteed. However, owing to the fact that neural networks are
multi-layer models with hyper-parameters used to learn features
automatically, testers are not able to explain the true meaning of
the numeric transformations inside at the moment. In consequence,
testers need to test whether the neural network model can make
correct predictions in addition to testing the correctness, which
adds to their labor work.

Finding 10: Testing whether the knowledge is obtained by a
software remains a big problem in the testing phase.

As a result, testers need to take a walk around to measure multi-
ple metrics. We analyze on the metrics testers focus on. The result
is shown in Table 10.

Table 10: Metrics to guide the testing process

Work experience
Correctness
Performance
Compatibility
Robustness

<1 year
30.00%
26.67%
23.33%
56.67%

1-3 years
50.00%
57.14%
45.24%
54.76%

>3 years
44.00%
46.40%
29.60%
57.60%

Total
43.15%
45.69%
31.98%
56.85%

Robustness is ranked as the most important metric to evaluate
such an application. Performance is another metric that testers fo-
cus on. Due to the concurrency limitation brought by DNN mod-
els, performance bottleneck exists in handling a large amount of
requests from the production environment. Among these metrics,
compatibility receives least attention. However, application envi-
ronment actually brings diﬃculties in the designing phase accord-
ing to Finding 5. We further investigate on the reason and ﬁnd that
it actually reacts to the environment consistency maintenance dur-
ing development phase. Because most of the testing environment
is consistent with the developing environment, testers do not face

We further conduct an analysis on testing practices to locate

bugs once an error is triggered. The result is shown in Table 11.

Table 11: Practices to locate bugs

Work experience
Bug locating tools
Self code review
Cross code review
Retrain DNN
Log
Break point
Adversarial
samples

<1 year
30.00%
20.00%
20.00%
50.00%
20.00%
13.33%

1-3 years
35.71%
23.81%
35.71%
33.33%
28.57%
28.57%

>3 years
39.20%
32.00%
27.20%
27.20%
40.00%
27.20%

Total
37.06%
28.43%
27.92%
31.98%
34.52%
25.38%

23.33%

38.10%

34.40%

33.5%

According to the result, bug locating tools, log analysis, and ad-
versarial samples are 3 main practices to locate bugs in DL appli-
cations. Bug locating tools and log analysis are common practices
done in testing conventional applications. When testing DL appli-
cations, experienced practitioners show an appetite for these prac-
tices. Besides, DNNs are found to be easily attacked by injecting
small perturbations to the original input. Various adversarial at-
tack algorithms are proposed to detect defects of the model with
the hope provide practitioners feedback about reasoning.

Finding 11: Bug locating tools, log analysis and adversarial sam-
ple are 3 main practices tester done to locate bugs in DL applica-
tions.

We conduct another analysis on the challenges testers face with
in testing and debugging DL applications. Table 12 shows the re-
sult.

Table 12: Challenges in testing & debugging DL applications

Work experience
Not enough data
No assertion for
test data
Lack of system
level test case
Low code statement
level coverage

<1 year
53.33%

1-3 years
66.67%

>3 years
64.00%

Total
62.94%

36.67%

26.19%

35.20%

33.50%

16.67%

42.86%

48.80%

42.64%

16.67%

26.19%

14.40%

17.26%

In testing conventional software applications, various code cov-
erage metrics are adopted. According to the survey, code statement
level coverage is not diﬃcult to reach. However, testing still re-
mains a big problem, which indicates that the result of coverage
measurement based on code statement is not ideal in DL appli-
cations. Not getting enough testing data is the biggest problem
they encounter in testing DL applications. On the one hand, test-
ing can only provide evidence to a buggy application instead of
proving correctness for it. Thus a large amount of data is expected
to achieve suﬃcient testing. However, data collection is a high cost

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

Xufan Zhang, Yilin Yang, Yang Feng, and Zhenyu Chen

task due to the fact that some data is expensive and diﬃcult to ob-
tain in our daily life. What’s more, extra labor work is required to
label all the testing data as oracle, which adds to the cost of the
project. On the other hand, testers did not know how to evaluate
whether the knowledge learned by the DNN model is correct, they
could only test it by adding as many samples as possible.

Finding 12: Due to the lack of testing data, it is diﬃcult to test
DL applications.

In addition, it is claimed that existing test data are mainly inputs
provided to the DNN model. However, system level test cases are
needed to check whether the whole system functions well.

Since there is an urgent requirement for test data, we analyze
the primary data sources practitioners use in testing DL applica-
tions. The result is shown in Table 13.

Table 13: Primary testing data sources

Work experience
Benchmark
Data obtained via
crowd-sourcing
Self-owned
business data
Data
augmentation
Manually
designed data

<1 year
53.33%

1-3 years
57.14%

>3 years
56.00%

Total
55.84%

23.33%

26.19%

34.40%

30.96%

46.67%

42.86%

52.00%

49.24%

36.67%

52.38%

35.20%

39.09%

26.67%

35.71%

41.60%

38.07%

55.84% of practitioners leverage well-known benchmarks to test
their models, followed by 49.24% of them test DNN models with
self-owned business data. Meanwhile, practitioners try to obtain
as much data as they can via multiple approaches including crowd-
sourcing, data augmentation, etc. It is notable that 41.60% of expe-
rienced practitioners have to design data manually in their daily
practice.

3.6 Deployment and maintenance
Once the application is ready for use, it will be deployed into the
production environment. Appeared and potential problems need
to be solved from time to time later during the maintenance phase.
Despite the superior performance DNNs can achieve, models
are space-consuming. To ﬁnd out practices practitioners done in
deployment phase, we survey on approaches they follow in deploy-
ing models. The result is shown in Table 14.

Table 14: Strategies to deploy industrial size DNN models

Work experience
Model compression
Server side
deployment
Model pruning

<1 year
11.54%

1-3 years
26.83%

>3 years
24.19%

Total
23.04%

61.54%

48.78%

51.61%

52.36%

26.92%

24.39%

24.19%

24.61%

Corresponding to the choice to keep consistency in develop-
ment, testing and production environment, server side deployment

is the ﬁrst choice for 52.36% of respondents. Since resources are
not that limited on servers as on embedded devices, server-side
deployment allows practitioners to focus on performance of the
model without considering complex external environments. How-
ever, there are some practitioners try to decrease the size of the
DNN model. Model compression and model pruning are two com-
mon practices they perform to achieve the goal.

By deploying DNN models on the server side, practitioners are
able to avoid some model ﬁtness and compatibility problems. Al-
though it brings ﬂexibility to maintain the model, challenges still
exist in the maintenance of DL applications. The result of the ques-
tion "What are the challenges in the maintenance of DL apps", as
shown in Table 15.

Table 15: Challenges in the maintenance of DL applications

Work experience
Multiple model
maintenance
Extra eﬀorts to
evaluate and
maintain QoD
Frequency increase
due to low
interpretability
Higher labor cost

<1 year

1-3 years

>3 years

Total

25.93%

17.50%

15.45%

16.75%

29.63%

42.50%

34.15%

34.01%

40.74%

20.00%

21.95%

23.35%

3.70%

20.00%

28.46%

22.34%

Extra eﬀorts to evaluate and maintain the quality of data is rec-
ognized as the primary diﬃculty in DL application maintenance
by 34.01% of respondents. Meanwhile, due to the fact that models
need to be retrained from time to time to keep itself up-to-date with
the knowledge evolvement, practitioners need to take the quality
of data(QoD) into consideration as the application keeps receiving
new business data which may contain new domain knowledge.

Finding 13: Quality control of data remains a big problem in
the phase of software maintenance.

According to the result, multiple model maintenance does not
actually add much to the diﬃculties in maintaining DL applica-
tions. In contrast, the low interpretability of each individual model
makes it diﬃcult to maintain as why the error occurs cannot be ex-
plained.

4 DISCUSSION
4.1 Implications
For Practitioners: Junior practitioners in software engineering
are often confused about what practices to follow due to the new
data-driven paradigm brought by DL.

• According to Finding 6, junior practitioners of DL applica-
tion development is recommended to start with well-known
deep learning frameworks.

• According to Finding 8, keeping DNN components of an ap-
plication on the server-side, as well as keep consistency in

Software Engineering Practice in the Development of
Deep Learning Applications

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

development, testing and production environment, can pro-
tect practitioners from potential problems caused by diﬀer-
ences in the environment.

• According to Finding 9, to improve the correctness and ro-
bustness of DL applications, practitioners can leverage bench-
marks to provide suﬃcient test to the DL application. Mean-
while, implementing multiple models can avoid the problem
to some extends.

For Researchers: Our ﬁndings also highlight opportunities for
software engineering researchers to build tools and techniques that
can help practitioners improve the quality of DL applications.

• According to Finding 1, 7 and 13, data feature identiﬁca-
tion, quality control, and evaluation are challenging tasks
in DL application development. Researchers are encouraged
to build tools to detect and highlight features from massive
data, which can give requirement engineers heuristics about
data preparation. Tools that help in quality control of data
like dataset bias detection, distribution evaluation, etc. are
also required to provide guarantees for training and testing
datasets.

• According to Finding 4, concurrent processing is a primary
factor that restricts the adoption of deep learning. Researches
on approaches to enable concurrency in DNN can attract
more practitioners to apply it in real applications. Together
with Finding 5, tools to provide real-time evaluation and
feedback for software architects can better guide them in
designing more powerful models to make better use of the
business data.

• According to Finding 11, bug locating tools and logs are of
great help in locating bugs. In deep learning, multiple ad-
versarial attack algorithms are proposed to expose potential
defects inside the model. Researchers are expected to work
on approaches and tools to map adversarial samples into the
reasoning process to ﬁnd out the location where the error is
triggered, which can be of great help in further interpreting
deep neural networks.

• According to Finding 12, practitioners are in lack of enough
data to evaluate the DL application under test. Since some
data are hard from collect in our daily life, tools that can
produce inputs that maintain the same semantics can fur-
ther help engineers to analyze DNN models. Meanwhile, re-
search on approaches to evaluating whether the test is suf-
ﬁcient can help practitioners ﬁnd a balance between model
testing and data collection.

4.2 Threats to Validity
External Validity: Focusing on the topic of the development of
DL applications, We summarized 13 relevant papers that are pub-
lished in software engineering venues. Through this literature sur-
vey, we get numerous insights, yet it may be a small sample. Also,
our interviewees only come from three big companies. To miti-
gate this threat, we survey 195 respondents from various small and
large organizations with diﬀerent backgrounds. Still, our ﬁndings
may not be generalized to all practitioners.

Internal Validity: One of the primary internal validity comes
from the completeness of our survey. Respondents may have diﬀer-
ent opinions beyond the ones we summarized in the questionnaire.
To reduce bias in the survey, we keep all questions open-ended
and let respondents express their opinions. Moreover, it is possible
that some respondents do not understand the questions well, or
the questions are beyond their work experience. To minimize this
threat, for each question, we provide the option “I don’t know" to
let respondents make the proper feedback. Another threat lays in
the creation of questions, which are summarized by literature sur-
veys and interviews with individuals. These interviews only reﬂect
the perspectives of individuals, and thus may introduce bias in our
study. However, each of the three authors vetted through the ques-
tions, as well as options, created by the other authors to mitigate
this threat. Also, before we distribute the questionnaire, we send
it to our industrial interviewees to validate each question and its
options.

5 RELATED WORK
Empirical study in software engineering has been conducted for
several decades and achieves signiﬁcant recognition in the broader
software engineering research community [24]. Rapid changes in
competitive threats, stakeholder preferences, development technol-
ogy, and time-to-market pressures make pre-speciﬁed requirements
inappropriate. Card [7] conducted an empirical study on software
design theory in one speciﬁc environment by examining multi-
ple metrics including module size, module strength, data coupling,
unreferenced variables, etc. The result shows that some recom-
mended design practices can be ineﬀective in this environment
despite their intuitive appeal. To estimate the performance of soft-
ware maintenance, Bankder[3] conducted a ﬁeld study to provides
insight into how performance in software maintenance can be im-
proved by improving the eﬃcacy of design and development proce-
dures. To understand open software development practices, Scacchi[23]
conducted a comparative case study across open source communi-
ties. Cao [6] conducted an empirical analysis to ﬁgure out the re-
quirement engineering practices that agile developers follow and
the beneﬁts and challenges these practices present. Itkonen[16]
presents a study on the manual testing practices in four software
development companies and identiﬁed 22 manual testing practices
and further compared it with traditional test practices. To under-
stand the beneﬁts, risks, and limitations of using social media in
software development, Storey[26] proposes and answers a set of
pertinent research questions around community involvement, project
coordination, and management, as well as individual software de-
velopment activities. Daka et al. [8] surveyed 225 developers to
understand unit testing practices such as motivation of develop-
ers, their usage of automation tools, and their challenges. Wan et
al. [31] performed a mixture of qualitative and quantitative studies
with 14 interviewees and 342 questionnaire respondents to investi-
gate the impacts of machine learning on the software development.
Diﬀerent from these work, in this study, we complement exist-
ing empirical studies by conducting a comprehensive survey with
195 industrial practitioners to understand the characteristics of each

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

Xufan Zhang, Yilin Yang, Yang Feng, and Zhenyu Chen

phase of the DL application development. We also invited respon-
dents to provide their rationale for the two hottest topics, i.e., test-
ing and debugging. These ﬁndings and feedback provide us with a
comprehensive understanding of the vision and challenges of the
DL application development.

6 CONCLUSION
Even though deep learning is an eﬃcient approach to deal with
big data and to make apps more intelligent, challenges and lacks
in practices of DL applications development are not clear. In this
paper, we investigate the challenges and lacks in practice when
developing a DL application. We interview our industry partner
companies to ﬁnd out the lacks and challenges in each phase of
software development. We further survey 195 practitioners of DL
applications from diﬀerent companies. Our survey results indicate
that the data-driven paradigm of deep learning brings challenges
to each phase of the software development life cycle. We conclude
13 ﬁndings from the results. Based on these ﬁndings, we make a
discussion and propose 7 actionable recommendations for DL ap-
plication practitioners, as well as potential research directions for
researchers to explore. Progress in such directions would further
promote the development of DNN as well as DL applications.

REFERENCES
[1] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall,
Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann.
2019. Software engineering for machine learning: a case study. In Proceedings of
the 41st International Conference on Software Engineering: Software Engineering
in Practice. IEEE Press, 291–300.

[2] Maurício Aniche, Christoph Treude, Igor Steinmacher, Igor Wiese, Gustavo
Pinto, Margaret-Anne Storey, and Marco Aurélio Gerosa. 2018. How modern
news aggregators help development communities shape and share knowledge.
In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE).
IEEE, 499–510.

[3] Rajiv D Banker, Gordon B Davis, and Sandra A Slaughter. 1998. Software devel-
opment practices, software complexity, and software maintenance performance:
A ﬁeld study. Management science 44, 4 (1998), 433–450.
[4] Hrvoje Belani, Marin Vuković, and Željka Car. 2019.

Requirements Engi-
arXiv preprint

neering Challenges in Building AI-Based Complex Systems.
arXiv:1908.11791 (2019).

[5] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, and
Jiakai Zhang. 2016. End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 (2016).

[6] Lan Cao and Balasubramaniam Ramesh. 2008. Agile requirements engineering

practices: An empirical study. IEEE software 25, 1 (2008), 60–67.

[7] David N Card, Victor E Church, and William W Agresti. 1986. An empirical
study of software design practices. IEEE Transactions on Software Engineering 2
(1986), 264–271.

[8] Ermira Daka and Gordon Fraser. 2014. A survey on unit testing practices and
problems. In 2014 IEEE 25th International Symposium on Software Reliability En-
gineering. IEEE, 201–211.

[9] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
stellar: model-based quantitative analysis of stateful deep learning systems. In
Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineer-
ing Conference and Symposium on the Foundations of Software Engineering. ACM,
477–487.

[10] Wei Fu and Tim Menzies. 2017. Easy over hard: A case study on deep learning. In
Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering.
ACM, 49–60.

[11] Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunacha-
lam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams,
Jorge Cuadros, et al. 2016. Development and validation of a deep learning algo-
rithm for detection of diabetic retinopathy in retinal fundus photographs. Jama
316, 22 (2016), 2402–2410.

[12] Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun. 2018. Dlfuzz:
Diﬀerential fuzzing testing of deep learning systems. In Proceedings of the 2018

26th ACM Joint Meeting on European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering. ACM, 739–743.

[13] Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu,
Jianjun Zhao, and Xiaohong Li. 2019. An Empirical Study towards Characteriz-
ing Deep Learning Development and Deployment across Diﬀerent Frameworks
and Platforms. arXiv preprint arXiv:1909.06727 (2019).

[14] Brody Huval, Tao Wang, Sameep Tandon, Jeﬀ Kiske, Will Song, Joel Pazhayam-
pallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-
Yue, et al. 2015. An empirical evaluation of deep learning on highway driving.
arXiv preprint arXiv:1504.01716 (2015).

[15] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
Comprehensive Study on Deep Learning Bug Characteristics. arXiv preprint
arXiv:1906.01388 (2019).

[16] Juha Itkonen, Mika V Mantyla, and Casper Lassenius. 2009. How do testers do
it? An exploratory study on manual testing practices. In 2009 3rd International
Symposium on Empirical Software Engineering and Measurement. IEEE, 494–497.
[17] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
2017. Reluplex: An eﬃcient SMT solver for verifying deep neural networks. In
International Conference on Computer Aided Veriﬁcation. Springer, 97–117.
[18] Lei Ma, Felix Juefei-Xu, Minhui Xue, Bo Li, Li Li, Yang Liu, and Jianjun Zhao.
2019. DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems.
In 2019 IEEE 26th International Conference on Software Analysis, Evolution and
Reengineering (SANER). IEEE, 614–618.

[19] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yang Chen, Ting Su, Li Li, Yang Liu, et al. 2018. Deepgauge: Multi-granularity
testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering. ACM, 120–131.
[20] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao
Xie, Li Li, Yang Liu, Jianjun Zhao, et al. 2018. Deepmutation: Mutation testing
of deep learning systems. In 2018 IEEE 29th International Symposium on Software
Reliability Engineering (ISSRE). IEEE, 100–111.

[21] Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya,
Randall Wald, and Edin Muharemagic. 2015. Deep learning applications and
challenges in big data analytics. Journal of Big Data 2, 1 (2015), 1.

[22] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-
tomated whitebox testing of deep learning systems. In Proceedings of the 26th
Symposium on Operating Systems Principles. ACM, 1–18.

[23] Walt Scacchi. 2001. Software development practices in open software develop-
ment communities: a comparative case study. In Proceedings of 1st Workshop on
Open Source Software Engineering.

[24] Carolyn B. Seaman. 1999. Qualitative methods in empirical studies of software

engineering. IEEE Transactions on software engineering 25, 4 (1999), 557–572.

[25] Leif Singer, Fernando Figueira Filho, and Margaret-Anne Storey. 2014. Software
engineering at the speed of light: how developers stay current using twitter. In
Proceedings of the 36th International Conference on Software Engineering. ACM,
211–221.

[26] Margaret-Anne Storey, Christoph Treude, Arie van Deursen, and Li-Te Cheng.
2010. The impact of social media on software engineering practices and tools. In
Proceedings of the FSE/SDP workshop on Future of software engineering research.
ACM, 359–364.

[27] Youcheng Sun, Xiaowei Huang, and Daniel Kroening. 2018. Testing deep neural

networks. arXiv preprint arXiv:1803.04792 (2018).

[28] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
Daniel Kroening. 2018. Concolic testing for deep neural networks. In Proceedings
of the 33rd ACM/IEEE International Conference on Automated Software Engineer-
ing. ACM, 109–119.

[29] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Auto-
mated testing of deep-neural-network-driven autonomous cars. In Proceedings
of the 40th international conference on software engineering. ACM, 303–314.
[30] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Auto-
mated testing of deep-neural-network-driven autonomous cars. In Proceedings
of the 40th international conference on software engineering. ACM, 303–314.
[31] Zhiyuan Wan, Xin Xia, David Lo, and Gail C Murphy. 2019. How does Machine
Learning Change Software Development Practices? IEEE Transactions on Soft-
ware Engineering (2019).

[32] Tien Yin Wong and Neil M Bressler. 2016. Artiﬁcial intelligence with deep learn-
ing technology looks into diabetic retinopathy screening. Jama 316, 22 (2016),
2366–2367.

[33] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jian-
jun Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. DeepHunter: a coverage-
guided fuzz testing framework for deep neural networks. In Proceedings of the
28th ACM SIGSOFT International Symposium on Software Testing and Analysis.
ACM, 146–157.

[34] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Machine Learning Test-

ing: Survey, Landscapes and Horizons. arXiv preprint arXiv:1906.10742 (2019).

[35] Tianyi Zhang, Cuiyun Gao, Lei Ma, Michael R Lyu, and Miryung Kim. 2019. An
Empirical Study of Common Challenges in Developing Deep Learning Applica-
tions. (2019).

Software Engineering Practice in the Development of
Deep Learning Applications

ICSE’42, May 23 - 29, 2020, Seoul, South Korea

[36] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and Lu Zhang.
2018. An empirical study on TensorFlow program bugs. In Proceedings of the
27th ACM SIGSOFT International Symposium on Software Testing and Analysis.
ACM, 129–140.

[37] Zhengxin Zhang, Qingjie Liu, and Yunhong Wang. 2018. Road extraction by
IEEE Geoscience and Remote Sensing Letters 15, 5 (2018),

deep residual u-net.
749–753.

