1
2
0
2

n
a
J

9

]

G
L
.
s
c
[

1
v
3
6
2
3
0
.
1
0
1
2
:
v
i
X
r
a

SyReNN: A Tool for Analyzing Deep Neural
Networks (cid:63)

Matthew Sotoudeh

((cid:66)) and Aditya V. Thakur

((cid:66))

University of California, Davis CA 95616, USA
{masotoudeh,avthakur}@ucdavis.edu

Abstract. Deep Neural Networks (DNNs) are rapidly gaining popular-
ity in a variety of important domains. Formally, DNNs are complicated
vector-valued functions which come in a variety of sizes and applica-
tions. Unfortunately, modern DNNs have been shown to be vulnerable
to a variety of attacks and buggy behavior. This has motivated recent
work in formally analyzing the properties of such DNNs. This paper in-
troduces SyReNN, a tool for understanding and analyzing a DNN by
computing its symbolic representation. The key insight is to decompose
the DNN into linear functions. Our tool is designed for analyses using
low-dimensional subsets of the input space, a unique design point in the
space of DNN analysis tools. We describe the tool and the underlying
theory, then evaluate its use and performance on three case studies: com-
puting Integrated Gradients, visualizing a DNN’s decision boundaries,
and patching a DNN.

Keywords: Deep Neural Networks · Symbolic representation · Inte-
grated Gradients

1

Introduction

Deep Neural Networks (DNNs) [19] have become the state-of-the-art in a variety
of applications including image recognition [54,34] and natural language process-
ing [12]. Moreover, they are increasingly used in safety- and security-critical ap-
plications such as autonomous vehicles [32] and medical diagnosis [10,39,29,38].
These advances have been accelerated by improved hardware and algorithms.

DNNs (Section 2) are programs that compute a vector-valued function, i.e.,
from Rn to Rm. They are straight-line programs written as a concatenation of
alternating linear and non-linear layers. The coeﬃcients of the linear layers are
learned from data via gradient descent during a training process. A number
of diﬀerent non-linear layers (called activation functions) are commonly used,
including the rectiﬁed linear and maximum pooling functions.

Owing to the variety of application domains as well as deployment con-
straints, DNNs come in many diﬀerent sizes. For instance, large image-recognition
and natural-language processing models are trained and deployed using cloud re-
sources [34,12], medium-size models could be trained in the cloud but deployed

(cid:63) Artifact available at https://zenodo.org/record/4124489.

 
 
 
 
 
 
2

Sotoudeh and Thakur

on hardware with limited resources [32], and ﬁnally small models could be trained
and deployed directly on edge devices [48,9,23,35,36]. There has also been a re-
cent push to compress trained models to reduce their size [25]. Such smaller
models play an especially important role in privacy-critical applications, such
as wake word detection for voice assistants, because they allow sensitive user
data to stay on the user’s own device instead of needing to be sent to a remote
computer for processing.

Although DNNs are very popular, they are not perfect. One particularly con-
cerning development is that modern DNNs have been shown to be extremely vul-
nerable to adversarial examples, inputs which are intentionally manipulated to
appear unmodiﬁed to humans but become misclassiﬁed by the DNN [55,20,41,8].
Similarly, fooling examples are inputs that look like random noise to humans, but
are classiﬁed with high conﬁdence by DNNs [42]. Mistakes made by DNNs have
led to loss of life [37,18] and wrongful arrests [27,28]. For this reason, it is impor-
tant to develop techniques for analyzing, understanding, and repairing DNNs.

This paper introduces SyReNN, a tool for understanding and analyzing
DNNs. SyReNN implements state-of-the-art algorithms for computing precise
symbolic representations of piecewise-linear DNNs (Section 3). Given an input
subspace of a DNN, SyReNN computes a symbolic representation that decom-
poses the behavior of the DNN into ﬁnitely-many linear functions. SyReNN im-
plements the one-dimensional analysis algorithm of Sotoudeh and Thakur [51]
and extends it to the two-dimensional setting as described in Section 4.

Key insights. There are two key insights enabling this approach, ﬁrst identi-
ﬁed in Sotoudeh and Thakur [51]. First, most popular DNN architectures today
are piecewise-linear, meaning they can be precisely decomposed into ﬁnitely-
many linear functions. This allows us to reduce their analysis to equivalent
questions in linear algebra, one of the most well-understood ﬁelds of modern
mathematics. Second, many applications only require analyzing the behavior
of the DNN on a low-dimensional subset of the input space. Hence, whereas
prior work has attempted to give up precision for eﬃciency in analyzing high-
dimensional input regions [49,50,17], our work has focused on algorithms that
are both eﬃcient and precise in analyzing lower-dimensional regions (Section 4).

Tool design. The SyReNN tool is designed to be easy to use and extend, as
well as eﬃcient (Section 5). The core of SyReNN is written as a highly-optimized,
parallel C++ server using Intel TBB for parallelization [46] and Eigen for matrix
operations [24]. A user-friendly Python front-end interfaces with the PyTorch
deep learning framework [45].

Use cases. We demonstrate the utility of SyReNN using three applications.
The ﬁrst computes Integrated Gradients (IG), a state-of-the-art measure used
to determine which input dimensions (e.g., pixels for an image-recognition net-
work) were most important in the ﬁnal classiﬁcation produced by the network
(Section 6.1). The second precisely visualizes the decision boundaries of a DNN
(Section 6.2). The last patches (repairs) a DNN to satisfy some desired spec-
iﬁcation involving inﬁnitely-many points (Section 6.3). Thus, we believe that

SyReNN: A Tool for Analyzing Deep Neural Networks

3

SyReNN is an interesting and useful tool in the toolbox for understanding and
analyzing DNNs.
Contributions. The contributions of this paper are:

– A deﬁnition of symbolic representation of DNNs (Section 3).
– An eﬃcient algorithm for computing symbolic representations for DNNs over

low-dimensional input subspaces (Section 4).

– A design of a usable and well-engineered tool implementing these ideas called

SyReNN (Section 5).

– Three applications of SyReNN (Section 6).

Section 2 presents preliminaries about DNNs; Section 7 presents related work;
Section 8 concludes. SyReNN is available on GitHub at https://github.com/
95616ARG/SyReNN.

2 Preliminaries

We now formally deﬁne the notion of DNN we will use in this paper.

Deﬁnition 1. A Deep Neural Network (DNN) is a function f : Rn → Rm
which can be written f = f1 ◦ f2 · · · ◦ fn for a sequence of layer functions f1, f2,
. . . , fn.

Our work is primarily concerned with the popular class of piecewise-linear
DNNs, deﬁned below. In this deﬁnition and the rest of this paper, we will use the
term “polytope” to mean a convex and bounded polytope except where speciﬁed.

Deﬁnition 2. A function f : Rn → Rm is piecewise-linear (PWL) if its input
domain Rn can be partitioned into ﬁnitely-many possibly-unbounded polytopes
X1, X2, . . . , Xk such that f(cid:22)Xi is linear for every Xi.

The most common activation function used today is the ReLU function, a

PWL activation function which is deﬁned below.

Deﬁnition 3. The rectiﬁed linear function (ReLU) is a function ReLU : Rn →
Rm deﬁned component-wise by

ReLU((cid:126)v)i :=

(cid:40)

0
vi

if vi < 0
otherwise,

where ReLU((cid:126)v)i is the ith component of the vector ReLU((cid:126)v) and vi is the ith
component of the vector (cid:126)v.

In order to see that ReLU is PWL, we must show that its input domain Rn
can be partitioned such that, in each partition, ReLU is linear. In this case, we
can use the orthants of Rn as our partitioning: within each orthant, the signs
of the components do not change hence ReLU is the linear function that just
zeros out the negative components.

4

Sotoudeh and Thakur

y

t
u
p
t
u
O

0

−0.5

−1

−1

0
1
Input x

2

Fig. 1: Example function for which (cid:92)f(cid:22)[−1,2] = {[−1, 0], [0, 1], [1, 2]}.

Although we focus on ReLU due to its popularity and expository power,
SyReNN works with a number of other popular PWL layers include MaxPool,
Leaky ReLU, Hard Tanh, Fully-Connected, and Convolutional layers, as deﬁned
in [19]. PWL layers have become exceedingly common. In fact, nearly all of the
state-of-the-art image recognition models bundled with Pytorch [44] are PWL.

Example 1. The DNN f : R1 → R1 deﬁned by

f (x) := (cid:2)1 −1 −1(cid:3) ReLU









1 −1
1
0
−1 0







(cid:21)



(cid:20)x
1

can be broken into layers f = f1 ◦ f2 ◦ f3 where

f1(x) :=









1 −1
1
0
−1 0

(cid:21)
(cid:20)x
1

,

f2 = ReLU,

and f3((cid:126)v) = (cid:2)1 −1 −1(cid:3) (cid:126)v.

The DNN’s input-output behavior on the domain [−1, 2] is shown in Figure 1.

3 A Symbolic Representation of DNNs

We formalize the symbolic representation according to the following deﬁnition:

Deﬁnition 4. Given a PWL function f : Rn → Rm and a bounded convex
polytope X ⊆ Rn, we deﬁne the symbolic representation of f on X, written (cid:100)f(cid:22)X ,
to be a ﬁnite set of polytopes (cid:100)f(cid:22)X = {P1, . . . , Pn}, such that:

1. The set {P1, P2, . . . , Pn} partitions X, except possibly for overlapping bound-

aries.

2. Each Pi is a bounded convex polytope.
3. Within each Pi, the function f(cid:22)Pi is linear.

Notably, if f is a DNN using only PWL layers, then f is PWL and so we can
deﬁne (cid:100)f(cid:22)X . This symbolic representation allows one to reduce questions about

SyReNN: A Tool for Analyzing Deep Neural Networks

5

the DNN f to questions about ﬁnitely-many linear functions Fi. For example,
f (x) ∈ Y for some
because linear functions are convex, to verify that ∀x ∈ X.
polytope Y , it suﬃces to verify ∀Pi ∈ (cid:100)f(cid:22)X .∀(cid:126)v ∈ Vert(Pi).
f ((cid:126)v) ∈ Y , where
Vert(Pi) is the (ﬁnite) set of vertices for the bounded convex polytope Pi; thus,
here both of the quantiﬁers are over ﬁnite sets. The symbolic representation
described above can be seen as a generalization of the ExactLine representa-
tion [51], which considered only one-dimensional restriction domains of interest.

Example 2. Consider again the DNN f : R1 → R1 given by

f (x) := (cid:2)1 −1 −1(cid:3) ReLU















(cid:21)



(cid:20)x
1

1 −1
1
0
−1 0

and the region of interest X = [−1, 2]. The input-output behavior of f on X is
shown in Figure 1. From this, we can see that

(cid:100)f(cid:22)X = {[−1, 0], [0, 1], [1, 2]}.

Within each of these partitions, the input-output behavior is linear, which for
R1 → R1 we can see visually as just a line segment. As this set fully partitions
X, then, this is a valid (cid:100)f(cid:22)X .

4 Computing the Symbolic Representation

This section presents an eﬃcient algorithm for computing (cid:100)f(cid:22)X for a DNN f com-
posed of PWL layers. To retain both scalability and precision, we will require
the input region X be two-dimensional. This design choice is relatively unex-
plored in the neural-network analysis literature (most analyses strike a balance
between precision and scalability, ignoring dimensionality). We show that, for
two-dimensional X, we can use an eﬃcient polytope representation to produce
an algorithm that demonstrates good best-case and in-practice eﬃciency while
retaining full precision. This algorithm represents a direct generalization of the
approach of [51].

The diﬃculties our algorithm addresses arise from three areas. First, when
computing (cid:100)f(cid:22)X there may be exponentially many such partitions on all of Rn
but only a small number of them may intersect with X. Consequently, the algo-
rithm needs to be able to ﬁnd those partitions that intersect with X eﬃciently
without explicitly listing all of the partitions on Rn. Second, it is often more
convenient to specify the partitioning via hyperplanes separating the partitions
than explicit polytopes. For example, for the one-dimensional ReLU function
we may simply state that the line x = 0 separates the two partitions, because
ReLU is linear both in the region x ≤ 0 and x ≥ 0. Finally, neural networks
are typically composed of sequences of linear and piecewise-linear layers, where
the partitioning imposed by each layer individually may be well-understood but
their composition is more complex. For example, identifying the linear partitions

6

Sotoudeh and Thakur

of y = ReLU(4 · ReLU(−3x − 1) + 2) is non-trivial, even though we know the
linear partitions of each composed function individually.

Our algorithm only requires the user to specify the hyperplanes deﬁning the
partitioning for the activation function used in each layer; our current implemen-
tation comes with support for common PWL activation functions. For example,
if a ReLU layer is used for an n-dimensional input vector, then the hyperplanes
would be deﬁned by the equations x1 = 0, x2 = 0, . . . , xn = 0. It then com-
putes the symbolic representation for a single layer at a time, composing them
sequentially to compute the symbolic representation across the entire network.
To allow such compositions of layers, instead of directly computing (cid:100)f(cid:22)X , we
will deﬁne another primitive, denoted by the operator ⊗ and sometimes referred
to as Extend, such that

(1)

Extend(h, (cid:98)g) = h ⊗ (cid:98)g = (cid:91)h ◦ g.
Consider f = fn ◦ fn−1 ◦ · · · ◦ f1, and let I : x (cid:55)→ x be the identity map. I is
linear across its entire input space, and, thus, (cid:100)I(cid:22)X = {X}. By the deﬁnition of
Extend(f1, ·), we have f1 ⊗ (cid:100)I(cid:22)X = (cid:92)(f1 ◦ I)(cid:22)X = (cid:91)f1(cid:22)X , where the ﬁnal equality
holds by the deﬁnition of the identity map I. We can then iteratively apply this
procedure to inductively compute (cid:92)(fi ◦ · · · ◦ f1)(cid:22)X from (cid:92)(fi−1 ◦ · · · f1)(cid:22)X like so:
fi ⊗ (cid:92)(fi−1 ◦ · · · ◦ f1)(cid:22)X = (cid:92)(fi ◦ fi−1 ◦ · · · ◦ f1)(cid:22)X
until we have computed (cid:92)(fn ◦ fn−1 ◦ · · · ◦ f1)(cid:22)X = (cid:100)f(cid:22)X , which is the required
symbolic representation.

4.1 Algorithm for Extend

Algorithm 1 present an algorithm for computing Extend for arbitrary PWL
functions, where Extend(h, (cid:98)g) = h ⊗ (cid:98)g = (cid:91)h ◦ g.
Geometric intuition for the algorithm. Consider the ReLU function (Def-
inition 3). It can be shown that, within any orthant (i.e., when the signs of all
coeﬃcients are held constant), ReLU((cid:126)x) is equivalent to some linear function,
in particular the element-wise product of (cid:126)x with a vector that zeroes out the
negative-signed components. However, for our algorithm, all we need to know is
that the linear partitions of ReLU (in this case the orthants) are separated by
hyperplanes x1 = 0, x2 = 0, . . . , xn = 0.

Given a two-dimensional convex bounded polytope X, the execution of the
algorithm for f = ReLU can be visualized as follows. We pick some vertex v
of X, and begin traversing the boundary of the polytope in counter-clockwise
order. If we hit an orthant boundary (corresponding to some hyperplane xi = 0),
it implies that the behavior of the function behaves diﬀerently at the points
of the polytope to one side of the boundary from those at the other side of
the boundary. Thus, we partition X into X1 and X2, where X1 lies to one
side of the hyperplane and X2 lies to the other side. We recursively apply this

SyReNN: A Tool for Analyzing Deep Neural Networks

7

procedure to X1 and X2 until the resulting polytopes all lie on exactly one side
of every hyperplane (orthant boundary). But lying on exactly one side of every
hyperplane (orthant boundary) implies each polytope lies entirely within a linear
partition of the function (a single orthant), hence the application of the function
on that polytope is linear, and hence we have our partitioning.

Functions used in algorithm. Given a two-dimensional bounded convex
polytope X, Vert(X) returns a list of its vertices in counter-clockwise order,
repeating the initial vertex at the end. Given a set of points X, ConvexHull(X)
represents their convex hull (the smallest bounded polytope containing every
point in X). Given a scalar value x, Sign(x) computes the sign of that value
(i.e., −1 if x < 0, +1 if x > 0, and 0 if x = 0).

Algorithm description. The key insight of the algorithm is to recursively
partition the polytopes until such a partition lies entirely within a linear region
of the function f . Algorithm 1 begins by constructing a queue containing the
polytopes of (cid:100)g(cid:22)X . Each iteration either removes a polytope from the queue that
lies entirely in one linear region (placing it in Y ), or splits (partitions) some
polytope into two smaller polytopes that get put back into the queue. When we
pop a polytope P from the queue, Line 6 iterates over all hyperplanes Nk ·x = bk
deﬁning the piecewise-linear partitioning of f , looking for any for which some
vertex Vi lies on the positive side of the hyperplane and another vertex Vj lies
on the negative side of the hyperplane. If none exist (Line 7), by convexity we
are guaranteed that the entire polytope lies entirely on one side with respect to
every hyperplane, meaning it lies entirely within a linear partition of f . Thus, we
can add it to Y and continue. If two such vertices are found (starting Line 10),
then we can ﬁnd “extreme” i and j indices such that Vi is the last vertex in
a counter-clockwise traversal to lie on the same side of the hyperplane as V1
and Vj is the last vertex lying on the opposite side of the hyperplane. We then
call SplitPlane() (Algorithm 2) to actually partition the polytope on opposite
sides of the hyperplane, adding both to our worklist.

In the best case, each partition is in a single orthant: the algorithm never calls
SplitPlane() at all — it merely iterates over all of the n input partitions, checks
their v vertices, and appends to the resulting set (for a best-case complexity of
O(nv)). In the worst case, it splits each polytope in the queue on each face,
resulting in exponential time complexity. As we will show in Section 6, this
exponential worst-case behavior is not encountered in practice, thus making
SyReNN a practical tool for DNN analysis.

8

Sotoudeh and Thakur

Algorithm 1: f ⊗ (cid:100)g(cid:22)X for two-dimensional X. f is deﬁned by hyper-
planes N1 · x = b1 through Nm · x = bm such that, within any partition
imposed by the hyperplanes f is equivalent to some aﬃne function.

Input: (cid:100)g(cid:22)X = {P1, . . . , Pn}.
Output: (cid:92)f ◦ g(cid:22)X

1 W ← ConstructQueue((cid:100)g(cid:22)X )
2 Y ← ∅
3 while W not empty do
P ← Pop(W )
4
V ← Vert(P )
K ← {Nk | ∃i, j : Sign(Nk · g(Vi) − bk) > 0 ∧ Sign(Nk · g(Vj) − bk) < 0}
if K = ∅ then

7

5

6

8

9

10

11

12

13

14

Y ← Y ∪ {P }
continue

N, b ← any element from K
i ← arg maxi{Sign(N · g(Vi) − b) = Sign(N · g(V1) − b)}
j ← arg maxj{Sign(N · g(Vj) − b) (cid:54)= Sign(N · g(Vi) − b)}
for V (cid:48) ∈ SplitPlane(V, g, i, j, N, b) do
W ← Push(W, ConvexHull(V (cid:48)))

15 return Y

v2

v3

v2

v3

v1

v4

v5

v1

(a) Before extending

(b) After extending

Fig. 2: Diagrams demonstrating the 2D Extend algorithm

Example of the algorithm. Consider the polytope shown in Figure 2a with
vertices {v1, v2, v3}, and suppose our activation function has two piecewise-linear
regions separated by the vertical line (1D hyperplane) N x+b = 0 shown. Because
this hyperplane has some of the vertices of the polytope on one side and some
on the other, we will use it as the N, b hyperplane on line 10. We then ﬁnd that
i on line 11 should be the last vertex on the ﬁrst side of the hyperplane, while
j should be the last vertex on the other side of the hyperplane. We will assume
things are oriented so that i = v1 and j = v3.

Then SplitPlane is called, which adds new vertices pi = v4 (shown in Fig-
ure 2b) where the edge v1 → v2 intersects the hyperplane, as well as pj = v5

SyReNN: A Tool for Analyzing Deep Neural Networks

9

Algorithm 2: SplitPlane(V, g, i, j, N, b)

Input: V , the vertices of the polytope in the input space of g. A function g. i
is the index of the last vertex lying on the same side of the orthant face
as V1. j is the index of the last vertex lying on the opposite side of the
orthant face as V1. N and b deﬁne the hyperplane N · x = b to split on.
Output: {P1, P2}, two sets of vertices whose convex hulls form a partitioning
of V such that each lies on only one side of the N · x = b hyperplane.
N ·(g(Vi+1)−g(Vi)) (Vi+1 − Vi)
N ·(g(Vj+1)−g(Vj )) (Vj+1 − Vj)

b−N ·g(Vj )

b−N ·g(Vi)

2 pj ← Vj +
3 A ← {pi, pj} ∪ {v ∈ V | Sign(N · v − b) = Sign(N · Vi − b)}
4 B ← {pi, pj} ∪ {v ∈ V | Sign(N · v − b) = Sign(N · Vj − b)}
5 return {A, B}

1 pi ← Vi +

where the edge v3 → v1 intersects the hyperplane. Separating all of the vertices
on the left of the hyperplane from those on the right, we ﬁnd that this has parti-
tioned the original polytope into two sub-polytopes, each on exactly one side of
the hyperplane, as desired. If there were more intersecting hyperplanes, we would
then recurse on each of the newly-generated polytopes to further subdivide them
by the other hyperplanes.

4.2 Representing Polytopes

We close this section with a discussion of implementation concerns when repre-
senting the convex polytopes that make up the partitioning of (cid:100)f(cid:22)X . In standard
computational geometry, bounded polytopes can be represented in two equiva-
lent forms:

1. The half-space or H-representation, which encodes the polytope as an in-
tersection of ﬁnitely-many half-spaces. (Each half-space being deﬁned as a
halfspace deﬁned by an aﬃne inequality Ax ≤ b.)

2. The vertex or V-representation, which encodes the polytope as a set of
ﬁnitely many points; the polytope is then taken to be the convex hull of
the points (i.e., smallest convex shape containing all of the points).

Certain operations are more eﬃcient when using one representation compared
to the other. For example, ﬁnding the intersection of two polytopes in an H-
representation can be done in linear time by concatenating their representative
half-spaces, but the same is not possible in V-representation.

There are two main operations on polytopes we need perform in our algo-
rithms: (i) splitting a polytope with a hyperplane, and (ii) applying an aﬃne
map to all points in the polytope. In general, the ﬁrst is more eﬃcient in an
H-representation, while the latter is more eﬃcient in a V-representation. How-
ever, when restricted to two-dimensional polygons, the former is also eﬃcient in
a V-representation, as demonstrated by Algorithm 2, helping to motivate our
use of the V-representation in our algorithm.

10

Sotoudeh and Thakur

Furthermore, the two polytope representations have diﬀerent resiliency to
ﬂoating-point operations. In particular, H-representations for polytopes in Rn
are notoriously diﬃcult to achieve high-precision with, because the error in-
troduced from using ﬂoating point numbers gets arbitrarily large as one goes
in a particular direction along any hyperplane face. Ideally, we would like the
hyperplane to be most accurate in the region of the polytope itself, which corre-
sponds to choosing the magnitude of the norm vector correctly. Unfortunately,
to our knowledge, there is no eﬃcient algorithm for computing the ideal ﬂoating
point H-representation of a polytope, although libraries such as APRON [31]
are able to provide reasonable results for low-dimensional spaces. However, be-
cause neural networks utilize extremely high-dimensional spaces (often hundreds
or thousands of dimensions) and we wish to iteratively apply our analysis, we
ﬁnd that errors from using ﬂoating-point H-representations can quickly multiply
and compound to become infeasible. By contrast, ﬂoating-point inaccuracies in
a V-representation are directly interpretable as slightly misplacing the vertices
of the polytope; no “localization” process is necessary to penalize inaccuracies
close to the polytope more than those far away from it.

Another diﬀerence is in the space complexity of the representation. In gen-
eral, H-representations can be more space-eﬃcient for common shapes than V-
representations. However, when the polytope lies in a low-dimensional subspace
of a larger space, the V-representation is usually signiﬁcantly more eﬃcient.

Thus, V-representations are a good choice for low-dimensionality polytopes
embedded in high-dimensional space, which is exactly what we need for analyzing
neural networks with two-dimensional restriction domains of interest. This is why
we designed our algorithms to rely on Vert(X), so that they could be directly
computed on a V-representation.

4.3 Extending to Higher-Dimensional Subsets of the Input Space

The 2D algorithm described above can be seen as implementing the recursive
case of a more general, n-dimensional version of the algorithm that recurses on
each of the (n − 1)-dimensional facets. In 2D, we trace the edges (1D faces) and
use the 1D algorithm from [51] to subdivide them based on intersections with the
hyperplanes deﬁning the function. More generally, for an arbitrary n-dimensional
polytope we can trace the (n − 1)-dimensional facets of the polytope, recursively
applying the (n − 1)-dimensional variant of the algorithm to split those facets
according to the linear partitions of the function.

We have experimented with such approaches, but found that the overhead
of keeping track of all (n − k)-dimensional faces (commonly known as the face
poset or combinatorial structure [16] of a polytope) was too large in higher
dimensions. The two-dimensional algorithm addresses this concern by storing the
combinatorial structure implicitly, representing 2D polytopes by their vertices
in counter-clockwise order, from which edges correspond exactly to sequential
vertices. To our knowledge, such a compact representation allowing arbitrary
(n − k)-dimensional faces to be read oﬀ is not known for higher-dimensional
polytopes. Nonetheless, we hope that extending our algorithms to GPUs and

SyReNN: A Tool for Analyzing Deep Neural Networks

11

other massively-parallel hardware may improve performance to mitigate such
overhead.

5 SyReNN tool

This section provides more details about the design and implementation of our
tool, SyReNN (Symbolic Representations of Neural Networks), which computes
(cid:100)f(cid:22)X , where f is a DNN using only piecewise-linear layers and X is a union of
one- or two-dimensional polytopes. The tool is open-source; it is available under
the MIT license at https://github.com/95616ARG/SyReNN and in the PyPI
package pysyrenn.
Input and output format. SyReNN supports reading DNNs from two stan-
dard formats: ERAN (a textual format used by the ERAN project [1]) as well as
ONNX (an industry-standard format supporting a wide variety of diﬀerent mod-
els) [43]. Internally, the input DNN is described as an instance of the Network
class, which is itself a list of sequential Layers. A number of layer types are
provided by SyReNN, including FullyConnectedLayer, ConvolutionalLayer,
and ReLULayer. To support more complicated DNN architectures, we have im-
plemented a ConcatLayer, which represents a concatenation of the output of
two diﬀerent layers. The input region of interest, X, is deﬁned as a polytope
described by a list of its vertices in counter-clockwise order. The output of the
tool is the symbolic representation (cid:100)f(cid:22)X .
Overall Architecture. We designed SyReNN in a client-server architecture
using gRPC [21] and protocol buﬀers [22] as a standard method of communica-
tion between the two. This architecture allows the bulk of the heavy computation
to be done in eﬃcient C++ code, while allowing user-friendly interfaces in a va-
riety of languages. It also allows practitioners to run the server remotely on a
more powerful machine if necessary. The C++ server implementation uses the
Intel TBB library for parallelization. Our oﬃcial front-end library is written
in Python, and available as a package on PyPI so installation is as simple as
pip install pysyrenn. The entire project can be built using the Bazel build
system, which manages dependencies using checksums.
Server Architecture. The major algorithms are implemented as a gRPC
server written in C++. When a connection is ﬁrst made, the server initializes
the state with an empty DNN f (x) = x. During the session, three operations
are permitted: (i) append a layer g so that the current session’s DNN is updated
from f0 to f1(x) := g(f0(x)), (ii) compute (cid:100)f(cid:22)X for a one-dimensional X, or (iii)
compute (cid:100)f(cid:22)X for a two-dimensional X. We have separate methods for one- and
two-dimensional X, because the one-dimensional case has speciﬁc optimizations
for controlling memory usage. The SegmentedLine and UPolytope types are
used to represent one- and two-dimensional partitions of X, respectively. When
operation (1) is performed, a new instance of the LayerTransformer class is ini-
tialized with the relevant parameters and added to a running vector of the cur-
rent layers. When operation (2) is performed, a new queue of SegmentedLines is

12

Sotoudeh and Thakur

constructed, corresponding to X, and the before-allocated LayerTransformers
are applied sequentially to compute (cid:100)f(cid:22)X . In this case, extra control is provided
to automatically gauge memory usage and pause computation for portions of
X until more memory is made available. Finally, when operation (3) is a per-
formed, a new instance of UPolytope is initialized with the vertices of X and
the LayerTransformers are again applied sequentially to compute (cid:100)f(cid:22)X .

Client Architecture. Our Python client exposes an interface for deﬁning
DNNs similar to the popular Sequential-Network Keras API [11]. Objects repre-
sent individual layers in the network, and they can be combined sequentially into
a Network instance. The key addition of our library is that this Network exposes
methods for computing (cid:100)f(cid:22)X given a V-representation description of X. To do
this, it invokes the server and passes a layer-by-layer description of f followed
by the polytope X, then parses the response (cid:100)f(cid:22)X .

Extending to support diﬀerent layer types. Diﬀerent layer types and ac-
tivation functions are supported by sub-classing the LayerTransformer class.
Instances of LayerTransformer expose a method for computing Extend(h, ·)
for the corresponding layer h. To simplify implementation, two sub-classes of
LayerTransformer are provided: one for entirely-linear layers (such as fully-
connected and convolutional layers), and one for piecewise-linear layers. For
fully-linear layers, all that needs to be provided is a method computing the layer
function itself. For piecewise-linear layers, two methods need to be provided:
(1) computing the layer function itself, and (2) one describing the hyperplanes
which separate the linear regions. The base class then directly implements Algo-
rithm 1 for that layer. This architecture makes supporting new layers a straight-
forward process.

Float Safety. Like Reluplex [33], SyReNN uses ﬂoating-point arithmetic to
compute (cid:100)f(cid:22)X eﬃciently. Unfortunately, this means that in some cases its results
will not be entirely precise when compared to a real-valued or multiple-precision
If a perfectly precise solution is required, the server
version of the algorithm.
code can be modiﬁed to use multiple-precision rationals instead of ﬂoats. Alter-
natively, a conﬁrmation pass can be run using multiple-precision numbers after
the initial ﬂoat computation to conﬁrm the accuracy of its results. The use of
over-approximations may also be explored for ensuring correctness with ﬂoating-
point evaluation, like in DeepPoly [50]. Unfortunately, our algorithm does not
directly lift to using such approximations, since they may blow the originally-2D
region into a higher-dimensional (but very “ﬂat”) over-approximate polytope,
preventing us from applying the 2D algorithm for the next layer.

6 Applications of SyReNN

This section presents the use of SyReNN in three example case studies.

SyReNN: A Tool for Analyzing Deep Neural Networks

13

6.1 Integrated Gradients

A common problem in the ﬁeld of explainable machine learning is understanding
why a DNN made the prediction it did. For example, given an image classiﬁed
by a DNN as a ‘cat,’ why did the DNN decide it was a cat instead of, say, a dog?
Were there particular pixels which were particularly important in deciding this?
Integrated Gradients (IG) [53] is the state-of-the-art method for computing such
model attributions.

Deﬁnition 5. Given a DNN f , the integrated gradients along dimension i for
input x and baseline x(cid:48) is deﬁned to be:

IGi(x) def= (xi − x(cid:48)

i) ×

(cid:90) 1

α=0

∂f (x(cid:48) + α × (x − x(cid:48)))
∂xi

dα.

(2)

The computed value IGi(x) determines relatively how important the ith input
(e.g., pixel) was to the classiﬁcation.

However, exactly computing this integral requires a symbolic, closed form
for the gradient of the network. Until [51], it was not known how to compute
such a closed-form and so IGs were always only approximated using a sampling-
based approach. Unfortunately, because it was unknown how to compute the true
value, there was no way for practitioners to determine how accurate their ap-
proximations were. This is particularly concerning in fairness applications where
an accurate attribution is exceedingly important.

In [51], it was recognized that, when X = ConvexHull({x, x(cid:48)}), (cid:100)f(cid:22)X can be
used to exactly compute IGi(x). This is because within each partition of (cid:100)f(cid:22)X
the gradient of the network is constant because it behaves as a linear function,
and hence the integral can be written as the weighted sum of such ﬁnitely-
many gradients.1 Using our symbolic representation, the exact IG can thus be
computed as follows:

(cid:88)

ConvexHull({yi,y(cid:48)

i})∈ (cid:92)f(cid:22)ConvexHull({x,x(cid:48) })

(y(cid:48)

i − yi) ×

∂f (0.5 × (yi + y(cid:48)
∂xi

i))

(3)

Where here yi, y(cid:48)
closest to x(cid:48).

i are the endpoints of the segment with yi closer to x and y(cid:48)
i

Implementation. The helper class IntegratedGradientsHelper is provided
by our Python client library. It takes as input a DNN f and a set of (x, x(cid:48))
input-baseline pairs and then computes IG for each pair.

Empirical Results.
In [51] SyReNN was used to show conclusively that ex-
isting sampling-based methods were insuﬃcient to adequately approximate the
true IG. This realization led to changes in the oﬃcial IG implementation to use
the more-precise trapezoidal sampling method we argued for.

1 As noted in [51], this technically requires a slight strengthening of the deﬁnition of

(cid:100)f(cid:22)X which is satisﬁed by our algorithms as deﬁned above.

14

Sotoudeh and Thakur

(a) Decision
computed using (cid:100)f(cid:22)X

boundaries

Decision
(b)
aries
computed
DeepPoly[k = 252]

bound-
using

Decision
computed

(c)
aries
DeepPoly[k = 1002]

bound-
using

Legend:

Clear-of-Conﬂict, Weak Right,

Strong Right,

Strong Left, Weak Left.

Fig. 3: Visualization of decision boundaries for the ACAS Xu network. Using
SyReNN (left) quickly produces the exact decision boundaries. Using abstract
interpretation-based tools like DeepPoly (middle and right) are slower and pro-
duce only imprecise approximations of the decision boundaries.

Timing Numbers.
In those experiments, we used SyReNN to compute (cid:100)f(cid:22)X
for three diﬀerent DNNs f , namely the small, medium, and large convolutional
models from [1]. For each DNN, we ran SyReNN on 100 one-dimensional lines.
The 100 calls to SyReNN completed in 20.8 seconds for the small model, 183.3
for the medium model, and 615.5 for the big model. Tests were performed on an
Intel Core i7-7820X CPU at 3.60GHz with 32GB of memory.

6.2 Visualization of DNN Decision Boundaries

Whereas IG helps understand why a DNN made a particular prediction about
a single input point, another major task is visualizing the decision boundaries
of a DNN on inﬁnitely-many input points. Figure 3 shows a visualization of an
ACAS Xu DNN [32] which takes as input the position of an airplane and an
approaching attacker, then produces as output one of ﬁve advisories instructing
the plane, such as “clear of conﬂict” or to move “weak left.” Every point in
the diagram represents the relative position of the approaching plane, while the
color indicates the advisory.

One approach to such visualizations is to simply sample ﬁnitely-many points
and extrapolate the behavior on the entire domain from those ﬁnitely-many
points. However, this approach is imprecise and risks missing vital information
because there is no way to know the correct sampling density to use to identify
all important features.

Another approach is to use a tool such as DeepPoly [50] to over-approximate
the output range of the DNN. However, because DeepPoly is an over-approximation,
there may be regions of the input space for which it cannot state with conﬁdence
the decision made by the network. In fact, the approximations used by DeepPoly

SyReNN: A Tool for Analyzing Deep Neural Networks

15

Table 1: Comparing the performance of DNN visualization using SyReNN versus
DeepPoly for the ACAS Xu network [32]. (cid:100)f(cid:22)X size is the number of partitions
in the symbolic representation. SyReNN time is the time taken to compute (cid:100)f(cid:22)X
using SyReNN. DeepPoly[k] time is the time taken to compute DeepPoly for
approximating decision boundaries with k partitions. Each scenario represents a
diﬀerent two-dimensional slice of the input space; within each slice, the heading
of the intruder relative to the ownship along with the speed of each involved
plane is ﬁxed.

DeepPoly time (secs)

Scenario

(cid:100)f(cid:22)X size SyReNN time (secs) k = 252 k = 552 k = 1002
33200
Head-On, Slow
30769
Head-On, Fast
Perpendicular, Slow 37251
Perpendicular, Fast 33931
36743
Opposite, Slow
Opposite, Fast
38965
-Perpendicular, Slow 36037
-Perpendicular, Fast 33208

141.3
128.0
141.7
127.5
152.5
147.3
146.4
130.2

43.2
39.0
42.9
39.2
46.7
45.2
45.0
39.5

10.9
10.2
12.5
11.4
12.1
13.0
11.9
10.9

9.1
8.2
9.2
8.2
9.8
9.5
9.5
8.3

are extremely coarse. A na¨ıve application of DeepPoly to this problem results
in it being unable to make claims about any of the input space of interest. In
order to utilize it, we must partition the space and run DeepPoly within each
partition, which signiﬁcantly slows down the analysis. Even when using 252 par-
titions, Figure 3b shows that most of the interesting region is still unclassiﬁable
with DeepPoly (shown in white). Only when using 1002 partitions is DeepPoly
able to eﬀectively approximate the decision boundaries, although it is still quite
imprecise.

By contrast, (cid:100)f(cid:22)X can be used to exactly determine the decision boundaries
on any 2D polytope subset of the input space, which can then be plotted. This is
shown in Figure 3a. Furthermore, as shown in Table 1, the approach using (cid:100)f(cid:22)X
is signiﬁcantly faster than that using ERAN, even as we get the precise answer
instead of an approximation. Such visualizations can be particularly helpful in
identifying issues to be ﬁxed using techniques such as those in Section 6.3.

Implementation. The helper class PlanesClassifier is provided by our
Python client library. It takes as input a DNN f and an input region X, then
computes the decision boundaries of f on X.

Timing Numbers. Timing comparisons are given in Table 1. We see that
SyReNN is quite performant, and the exact SyReNN can be computed more
quickly than even a mediocre approximation from DeepPoly using 552 parti-
tions. Tests were performed on a dedicated Amazon EC2 c5.metal instance, using
BenchExec [5] to limit the number of CPU cores to 16 and RAM to 16GB.

16

Sotoudeh and Thakur

(a) Before patching. (b) Patched pockets.

(c) Patched bands.

(d) Patched symme-
try.

Legend:

Clear-of-Conﬂict, Weak Right,

Strong Right,

Strong Left, Weak Left.

Fig. 4: Network patching.

6.3 Patching of DNNs

We have now seen how SyReNN can be used to visualize the behavior of a DNN.
This can be particularly useful for identifying buggy behavior. For example,
in Figure 3a we can see that the decision boundary between “strong right” and
“strong left” is not symmetrical.

The ﬁnal application we consider for SyReNN is patching DNNs to correct
undesired behavior. Patching is described formally in [52]. Given an initial net-
work N and a speciﬁcation φ describing desired constraints on the input/output,
the goal of patching is to ﬁnd a small modiﬁcation to the parameters of N pro-
ducing a new DNN N (cid:48) that satisﬁes the constraints in φ.

The key theory behind DNN patching we will use was developed in [52]. The
key realization of that work is that, for a certain DNN architecture, correcting the
network behavior on an inﬁnite, 2D region X is exactly equivalent to correcting
its behavior on the ﬁnitely-many vertices Vert(Pi) for each of the ﬁnitely-many
Pi ∈ (cid:100)f(cid:22)X . Hence, SyReNN plays a key role in enabling eﬃcient DNN patching.
For this case study, we patched the same aircraft collision-avoidance DNN
visualized in Section 6.2. We patched the DNN three times to correct three dif-
ferent buggy behaviors of the network: (i) remove “Pockets” of strong left/strong
right in regions that are otherwise weak left/weak right; (ii) remove the “Bands”
of weak-left advisory behind and to the left of the plane; and (iii) enforce “Sym-
metry” across the horizontal. The DNNs before and after patching with diﬀerent
speciﬁcations are shown in Figure 4.

Implementation The helper class NetPatcher is provided by our Python
client library. It takes as input a DNN f and pairs of input region, output label
Xi, Yi, then computes a new DNN f (cid:48) which maps all points in each Xi into Yi.

Timing Numbers. As in Section 6.2, computing (cid:100)f(cid:22)X for use in patching took
approximately 10 seconds.

SyReNN: A Tool for Analyzing Deep Neural Networks

17

7 Related Work

The related problem of exact reach set analysis for DNNs was investigated in
[59]. However, the authors use an algorithm that relies on explicitly enumerating
all exponentially-many (2n) possible signs at each ReLU layer. By contrast,
our algorithm adapts to the actual input polytopes, eﬃciently restricting its
consideration to activations that are actually possible.

Hanin and Rolnick [26] prove theoretical properties about the cardinality of
(cid:100)f(cid:22)X for ReLU networks, showing that |(cid:100)f(cid:22)X | is expected to grow polynomially
with the number of nodes in the network for randomly-initialized networks.

Thrun [56] and Bastani et al.[4] extract symbolic rules meant to approximate
DNNs, which can be thought of as an approximation of the symbolic represen-
tation (cid:100)f(cid:22)X .

In particular, the ERAN [1] tool and underlying DeepPoly [50] domain were
designed to verify the non-existence of adversarial examples. Breutel et al. [6]
presents an iterative reﬁnement algorithm that computes an overapproximation
of the weakest precondition as a polytope where the required output is also a
polytope.

Scheibler et al. [47] verify the safety of a machine-learning controller using
the SMT-solver iSAT3, but support small unrolling depths and basic safety prop-
erties. Zhu et al. [61] use a synthesis procedure to generate a safe deterministic
program that can enforce safety conditions by monitoring the deployed DNN
and preventing potentially unsafe actions. The presence of adversarial and fool-
ing inputs for DNNs as well as applications of DNNs in safety-critical systems
has led to eﬀorts to verify and certify DNNs [3,33,14,30,17,7,58,50,2]. Approxi-
mate reachability analysis for neural networks safely overapproximates the set
of possible outputs [17,59,60,58,13,57].

Prior work in the area of network patching focuses on enforcing constraints
on the network during training. DiﬀAI [40] is an approach to train neural net-
works that are certiﬁably robust to adversarial perturbations. DL2 [15] allows
for training and querying neural networks with logical constraints.

8 Conclusion and Future Work

We presented SyReNN, a tool for understanding and analyzing DNNs. Given
a piecewise-linear network and a low-dimensional polytope subspace of the in-
put subspace, SyReNN computes a symbolic representation that decomposes the
behavior of the DNN into ﬁnitely-many linear functions. We showed how to eﬃ-
ciently compute this representation, and presented the design of the correspond-
ing tool. We illustrated the utility of SyReNN on three applications: computing
exact IG, visualizing the behavior of DNNs, and patching (repairing) DNNs.

In contrast to prior work, SyReNN explores a unique point in the design
space of DNN analysis tools. In particular, instead of trading oﬀ precision of
the analysis for eﬃciency, SyReNN focuses on analyzing DNN behavior on low-
dimensional subspaces of the domain, for which we can provide both eﬃciency
and precision.

18

Sotoudeh and Thakur

We plan on extending SyReNN to make use of GPUs and other massively-
parallel hardware to more quickly compute (cid:100)f(cid:22)X for large f or X. Techniques
to support input polytopes that are greater than two dimensional is also a ripe
area of future work. We may also be able to take advantage of the fact that non-
convex polytopes can be represented eﬃciently in 2D. Extending algorithms for
(cid:100)f(cid:22)X to handle architectures such as Recurrent Neural Networks (RNNs) will
open up new application areas for SyReNN.

Acknowledgements. We thank the anonymous reviewers for their feedback and
suggestions on this work. This material is based upon work supported by a
Facebook Probability and Programming award.

References

1. ETH robustness analyzer for neural networks (ERAN). https://github.com/

eth-sri/eran (2019), accessed: 2019-05-01

2. Anderson, G., Pailoor, S., Dillig, I., Chaudhuri, S.: Optimization and abstrac-
tion: A synergistic approach for analyzing neural network robustness. CoRR
abs/1904.09959 (2019)

3. Bastani, O., Ioannou, Y., Lampropoulos, L., Vytiniotis, D., Nori, A.V., Crimin-
isi, A.: Measuring neural net robustness with constraints. In: Advances in Neural
Information Processing Systems (2016)

4. Bastani, O., Pu, Y., Solar-Lezama, A.: Veriﬁable reinforcement learning via policy
extraction. In: Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8
December 2018, Montr´eal, Canada. pp. 2499–2509 (2018)

5. Beyer, D.: Reliable and reproducible competition results with benchexec and wit-
nesses (report on sv-comp 2016). In: International Conference on Tools and Al-
gorithms for the Construction and Analysis of Systems (TACAS). pp. 887–904.
Springer (2016)

6. Breutel, S., Maire, F., Hayward, R.: Extracting interface assertions from neural
networks in polyhedral format. In: ESANN 2003, 11th European Symposium on
Artiﬁcial Neural Networks, Bruges, Belgium, April 23-25, 2003, Proceedings. pp.
463–468 (2003)

7. Bunel, R.R., Turkaslan, I., Torr, P.H.S., Kohli, P., Mudigonda, P.K.: A uniﬁed view
of piecewise linear neural network veriﬁcation. In: Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information Processing Sys-
tems 2018, NeurIPS 2018, 3-8 December 2018, Montr´eal, Canada. pp. 4795–4804
(2018)

8. Carlini, N., Wagner, D.: Audio adversarial examples: Targeted attacks on speech-
to-text. In: 2018 IEEE Security and Privacy Workshops (SPW). pp. 1–7. IEEE
(2018)

9. Chen, J., Ran, X.: Deep learning with edge computing: A review. Proceedings of

the IEEE 107(8), 1655–1674 (2019)

10. Ching, T., Himmelstein, D.S., Beaulieu-Jones, B.K., Kalinin, A.A., Do, B.T., Way,
G.P., Ferrero, E., Agapow, P.M., Zietz, M., Hoﬀman, M.M., Xie, W., Rosen, G.L.,
Lengerich, B.J., Israeli, J., Lanchantin, J., Woloszynek, S., Carpenter, A.E., Shriku-
mar, A., Xu, J., Cofer, E.M., Lavender, C.A., Turaga, S.C., Alexandari, A.M., Lu,

SyReNN: A Tool for Analyzing Deep Neural Networks

19

Z., Harris, D.J., DeCaprio, D., Qi, Y., Kundaje, A., Peng, Y., Wiley, L.K., Segler,
M.H.S., Boca, S.M., Swamidass, S.J., Huang, A., Gitter, A., Greene, C.S.: Opportu-
nities and obstacles for deep learning in biology and medicine. Journal of The Royal
Society Interface 15(141), 20170387 (2018). https://doi.org/10.1098/rsif.2017.0387

11. Chollet, F., et al.: Keras. https://keras.io (2015)
12. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec-

tional transformers for language understanding. CoRR abs/1810.04805 (2018)

13. Dutta, S., Jha, S., Sankaranarayanan, S., Tiwari, A.: Output range analysis for
deep feedforward neural networks. In: NASA Formal Methods Symposium. pp.
121–138. Springer (2018)

14. Ehlers, R.: Formal veriﬁcation of piece-wise linear feed-forward neural networks. In:
International Symposium on Automated Technology for Veriﬁcation and Analysis
(ATVA) (2017)

15. Fischer, M., Balunovic, M., Drachsler-Cohen, D., Gehr, T., Zhang, C., Vechev, M.:
Dl2: Training and querying neural networks with logic. In: International Conference
on Machine Learning (2019)

16. Fukuda, K., et al.: Frequently asked questions in polyhedral computation. ETH,

Zurich, Switzerland (2004)

17. Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., Vechev,
M.T.: AI2: safety and robustness certiﬁcation of neural networks with abstract
interpretation. In: 2018 IEEE Symposium on Security and Privacy, SP 2018, Pro-
ceedings, 21-23 May 2018, San Francisco, California, USA (2018)

18. Gonzales, R.: Feds say self-driving uber suv did not recognize jaywalking
pedestrian in fatal crash. NPR https://www.npr.org/2019/11/07/777438412/
feds-say-self-driving-uber-suv-did-not-recognize-jaywalking-pedestrian-in-fatal-
(Nov 2019), accessed: 2020-06-06

19. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press (2016), http:

//www.deeplearningbook.org

20. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: International Conference on Learning Representations, ICLR (2015)
21. Google: grpc: A high-performance, open source universal rpc framework. ”https:

//grpc.io/ (2020)

22. Google: Protocol buﬀers - google’s data interchange format. https://developers.

google.com/protocol-buﬀers/ (2020)

23. Gopinath, S., Ghanathe, N., Seshadri, V., Sharma, R.: Compiling kb-sized machine
learning models to tiny iot devices. In: Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Implementation, PLDI. pp.
79–95. ACM (2019)

24. Guennebaud, G., Jacob, B., et al.: Eigen v3. http://eigen.tuxfamily.org (2010)
25. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huﬀman coding. International Con-
ference on Learning Representations (ICLR) (2016)

26. Hanin, B., Rolnick, D.: Complexity of linear regions in deep networks. International

Conference on Machine Learning (ICML) (2019)

27. Hern, A.: Facebook translates

ing
facebook-palestine-israel-translates-good-morning-attack-them-arrest
2017), accessed: 2020-06-06

lead-
https://www.theguardian.com/technology/2017/oct/24/
(Jun

into ’attack them’,

’good morning’

arrest.

to

28. Hill, K.: Wrongfully accused by an algorithm. New York Times. https://www.
(Jun 2020),

nytimes.com/2020/06/24/technology/facial-recognition-arrest.html
accessed: 2020-06-06

20

Sotoudeh and Thakur

29. Hosny, A., Parmar, C., Quackenbush, J., Schwartz, L.H., Aerts, H.J.: Artiﬁcial

intelligence in radiology. Nature Reviews Cancer p. 1 (2018)

30. Huang, X., Kwiatkowska, M., Wang, S., Wu, M.: Safety veriﬁcation of deep neural
networks. In: International Conference on Computer Aided Veriﬁcation (CAV)
(2017)

31. Jeannet, B., Min´e, A.: Apron: A library of numerical abstract domains for static
analysis. In: International Conference on Computer Aided Veriﬁcation (CAV). pp.
661–667. Springer (2009)

32. Julian, K.D., Kochenderfer, M.J., Owen, M.P.: Deep neural network compression
for aircraft collision avoidance systems. Journal of Guidance, Control, and Dynam-
ics 42(3), 598–608 (2018)

33. Katz, G., Barrett, C., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex: An
eﬃcient smt solver for verifying deep neural networks. In: International Conference
on Computer Aided Veriﬁcation (CAV) (2017)

34. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-

volutional neural networks. Commun. ACM 60(6), 84–90 (2017)

35. Kumar, A., Seshadri, V., Sharma, R.: Shiftry: Rnn inference in 2kb of ram. In:
International Conference on Object-Oriented Programming, Systems, Languages,
and Applications, OOPSLA (2020)

36. Kusupati, A., Singh, M., Bhatia, K., Kumar, A., Jain, P., Varma, M.: Fastgrnn:
A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In:
Advances in Neural Information Processing Systems. pp. 9017–9028 (2018)

37. Lee, D.: US opens investigation into Tesla after fatal crash. BBC. https://www.

bbc.co.uk/news/technology-36680043 (Jul 2016), accessed: 2020-06-06

38. Mendelson, E.B.: Artiﬁcial intelligence in breast imaging: potentials and limita-

tions. American Journal of Roentgenology 212(2), 293–299 (2019)

39. Miotto, R., Wang, F., Wang, S., Jiang, X., Dudley, J.T.: Deep learning for health-
care: review, opportunities and challenges. Brieﬁngs in Bioinformatics 19(6), 1236–
1246 (2018)

40. Mirman, M., Gehr, T., Vechev, M.: Diﬀerentiable abstract interpretation for prov-
ably robust neural networks. In: International Conference on Machine Learning
ICML (2018)

41. Moosavi-Dezfooli, S., Fawzi, A., Frossard, P.: Deepfool: A simple and accurate
method to fool deep neural networks. In: 2016 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,
2016. pp. 2574–2582 (2016)

42. Nguyen, A.M., Yosinski, J., Clune, J.: Deep neural networks are easily fooled:
High conﬁdence predictions for unrecognizable images. In: IEEE Conference on
Computer Vision and Pattern Recognition, (CVPR) 2015, Boston, MA, USA, June
7-12, 2015. pp. 427–436 (2015)

43. ONNX: Open neural network exchange. https://onnx.ai/ (2020)
44. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch (2017)
45. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., De-
Vito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai,
J., Chintala, S.: Pytorch: An imperative style, high-performance deep learn-
ing library. In: Wallach, H., Larochelle, H., Beygelzimer, A., d’ Alch´e-Buc, F.,
Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems
32, pp. 8024–8035. Curran Associates, Inc. (2019), http://papers.nips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf

SyReNN: A Tool for Analyzing Deep Neural Networks

21

46. Reinders, J.: Intel threading building blocks: outﬁtting C++ for multi-core pro-

cessor parallelism. ” O’Reilly Media, Inc.” (2007)

47. Scheibler, K., Winterer, L., Wimmer, R., Becker, B.: Towards veriﬁcation of arti-
ﬁcial neural networks. In: Methoden und Beschreibungssprachen zur Modellierung
und Veriﬁkation von Schaltungen und Systemen, MBMV 2015, Chemnitz, Ger-
many, March 3-4, 2015. pp. 30–40 (2015)

48. Sharma, H., Park, J., Mahajan, D., Amaro, E., Kim, J.K., Shao, C., Mishra, A.,
Esmaeilzadeh, H.: From high-level deep neural models to fpgas. In: 2016 49th
Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). pp.
1–12. IEEE (2016)

49. Singh, G., Gehr, T., Mirman, M., P¨uschel, M., Vechev, M.: Fast and eﬀective
robustness certiﬁcation. In: Advances in Neural Information Processing Systems
(2018)

50. Singh, G., Gehr, T., P¨uschel, M., Vechev, M.T.: An abstract domain for certifying

neural networks. PACMPL 3(POPL), 41:1–41:30 (2019)

51. Sotoudeh, M., Thakur, A.V.: Computing linear restrictions of neural networks.
In: Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada. pp. 14132–14143 (2019)

52. Sotoudeh, M., Thakur, A.V.: Correcting deep neural networks with small, general-
izing patches. In: Workshop on Safety and Robustness in Decision Making (2019)
53. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In:

International Conference on Machine Learning ICML (2017)

54. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. In: Proceedings of the IEEE conference on
computer vision and pattern recognition (CVPR) (2016)

55. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.J.,
Fergus, R.: Intriguing properties of neural networks. In: International Conference
on Learning Representations, ICLR (2014)

56. Thrun, S.: Extracting rules from artiﬁcal neural networks with distributed rep-
resentations. In: Advances in Neural Information Processing Systems 7, [NIPS
Conference, Denver, Colorado, USA, 1994]. pp. 505–512 (1994)

57. Wang, S., Pei, K., Whitehouse, J., Yang, J., Jana, S.: Formal security analysis of
neural networks using symbolic intervals. In: 27th USENIX Security Symposium,
USENIX Security 2018, Baltimore, MD, USA, August 15-17, 2018. pp. 1599–1614
(2018)

58. Weng, T., Zhang, H., Chen, H., Song, Z., Hsieh, C., Daniel, L., Boning, D.S.,
Dhillon, I.S.: Towards fast computation of certiﬁed robustness for relu networks.
In: International Conference on Machine Learning, (ICML) (2018)

59. Xiang, W., Tran, H.D., Johnson, T.T.: Reachable set computation and safety veri-
ﬁcation for neural networks with relu activations. arXiv preprint arXiv:1712.08163
(2017)

60. Xiang, W., Tran, H., Rosenfeld, J.A., Johnson, T.T.: Reachable set estimation and
safety veriﬁcation for piecewise linear systems with neural network controllers. In:
2018 Annual American Control Conference, (ACC) (2018)

61. Zhu, H., Xiong, Z., Magill, S., Jagannathan, S.: An inductive synthesis framework
for veriﬁable reinforcement learning. In: Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Implementation, PLDI 2019,
Phoenix, AZ, USA, June 22-26, 2019. pp. 686–701 (2019)

22

Sotoudeh and Thakur

Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (https://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.

The images or other third party material in this chapter are included in the chapter’s
Creative Commons license, unless indicated otherwise in a credit line to the material. If
material is not included in the chapter’s Creative Commons license and your intended
use is not permitted by statutory regulation or exceeds the permitted use, you will need
to obtain permission directly from the copyright holder.

