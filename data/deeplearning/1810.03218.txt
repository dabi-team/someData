2
2
0
2

r
a

M
1

]

G
L
.
s
c
[

3
v
8
1
2
3
0
.
0
1
8
1
:
v
i
X
r
a

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR
PROGRAMMING

DANIEL BIENSTOCK, GONZALO MUÃ‘OZ, AND SEBASTIAN POKUTTA

Abstract. Deep learning has received much attention lately due to the impressive em-
pirical performance achieved by training algorithms. Consequently, a need for a better
theoretical understanding of these problems has become more evident in recent years. In
this work, using a uniï¬ed framework, we show that there exists a polyhedron which en-
codes simultaneously all possible deep neural network training problems that can arise
from a given architecture, activation functions, loss function, and sample-size. Notably,
the size of the polyhedral representation depends only linearly on the sample-size, and a
better dependency on several other network parameters is unlikely (assuming ğ‘ƒ â‰  ğ‘ ğ‘ƒ).
Additionally, we use our polyhedral representation to obtain new and better computational
complexity results for training problems of well-known neural network architectures. Our
results provide a new perspective on training problems through the lens of polyhedral theory
and reveal a strong structure arising from these problems.

1. Introduction

Deep Learning is a powerful tool for modeling complex learning tasks. Its versatility
allows for nuanced architectures that capture various setups of interest and has demonstrated
a nearly unrivaled performance on learning tasks across many domains. This has recently
triggered a signiï¬cant interest in the theoretical analysis of training such networks. The
training problem is usually formulated as an empirical risk minimization problem (ERM)
that can be phrased as

(1)

ğ·

â„“

ğ‘“

(

(

Ë†ğ‘¥ğ‘–, ğœ™

, Ë†ğ‘¦ğ‘–

)

,

)

1
ğ·

min
Î¦
ğœ™
âˆˆ
Ë†ğ‘¥ğ‘–, Ë†ğ‘¦ğ‘–

where â„“ is some loss function,

Ã•ğ‘–=1
ğ·
ğ‘–=1 is an i.i.d. sample from some data distribution
Î¦ with Î¦ being the
D
parameter space of the considered architecture (e.g., network weights). The empirical risk
minimization problem is solved in lieu of the general risk minimization problem (GRM)

)
is a neural network architecture parameterized by ğœ™

, and ğ‘“

âˆˆ

(

min
Î¦
ğœ™
âˆˆ
which is usually impossible to solve due to the inaccessibility of

) âˆˆD [

ğ‘¥, ğœ™

, ğ‘¦

ğ‘¥,ğ‘¦

)]

â„“

(

(

)

ğ‘“

(

ğ”¼

.

D

While most eï¬€orts on handling (1) have been aimed at practical performance, much less
research has been conducted in understanding its theoretical diï¬ƒculty from an optimization
standpoint. In particular, only few results account for the eï¬€ect of ğ·, the sample size, in

(DB) Industrial Engineering and Operations Research, Columbia University, USA
(GM) Institute of Engineering Sciences, Universidad de Oâ€™Higgins, Chile
(SP) Zuse Institute Berlin, Germany
E-mail addresses: dano@columbia.edu, gonzalo.munoz@uoh.cl, pokutta@zib.de.
2020 Mathematics Subject Classiï¬cation. Primary: 90C05, 68T01, 52B05.
Key words and phrases. deep learning, linear programming, polyhedral theory.

1

 
 
 
 
 
 
2

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

the structure and hardness of (1). In this work, we contribute to the understanding of this
problem by showing there exists a polyhedral encoding of empirical risk minimization prob-
lems in (1) associated with the learning problems for various architectures with remarkable
features. For a given architecture and sample size, our polyhedron encodes approximately
all possible empirical risk minimization problems with that sample size simultaneously.
The size of the polyhedron is roughly (singly!) exponential in the input dimension and
in the parameter space dimension, but, notably, linear in the size of the sample. This
result provides a new perspective on training problems and also yields new bounds on the
computational complexity of various training problems from a uniï¬ed approach.

1, 1

ğ‘š.

Throughout this work we assume both data and parameters to be well-scaled, which is
a common assumption and mainly serves to simplify the representation of our results; the
main assumption is the reasonable boundedness, which can be assumed without signiï¬cant
loss of generality as actual computations assume boundedness in any case (see also [25]
for arguments advocating the use of normalized coeï¬ƒcients in neural networks). More
speciï¬cally, we assume Î¦

ğ‘ as well as

satisï¬es

1, 1

1, 1

ğ‘¥, ğ‘¦

ğ‘¥, ğ‘¦

ğ‘›

âŠ† [âˆ’

]

(

) âˆ¼ D

(

) âˆˆ [âˆ’

]

Ã—

]

[âˆ’
1.1. Related Work. We are not aware of any encoding representing multiple training
problems simultaneously. However, given its implications on the training problems for
a ï¬xed sample, our work is related to [35], [20], and [4]. In [35] the authors show that
â„“1-regularized networks can be learned improperly1 in polynomial time (with an expo-
nential architecture-dependent constant) for networks with ReLU-like activations. These
results were generalized by [20] to ReLU activations, but the running time obtained is not
polynomial. In contrast, [4] considered exact learning however only for one hidden layer.
To the best of our knowledge, the only work where a polyhedral approach is used to
analyze the computational complexity of training of neural networks is [4], where the
authors solve (1) for 1 hidden layer using a collection of convex optimization problems over
a polyhedral feasible region. In practice, even though the most common methods used for
tackling (1) are based on Stochastic Gradient Descent (SGD), there are some notable and
surprising examples where linear programming has been used to train neural networks. For
example, in [6, 7, 30, 27] the authors construct a 1-hidden layer network by sequentially
increasing the number of nodes of the hidden layer and solving linear programs to update
the weights, until a certain target loss is met.

Linear programming tools have also been used within SGD-type methods in order to
compute optimal step-sizes in the optimization of (1) [8] or to strictly enforce structure in
Î¦ using a Frank-Wolfe approach instead of SGD [28, 34] . Finally, a back-propagation-like
algorithm for training neural network, which solves Mixed-Integer Linear problems in each
layer, was recently proposed as an alternative to SGD [19].

Other notable uses of Mixed-Integer and Linear Programming technology in other
aspects of Deep Learning are include feature visualization [18], generating adversarial
examples [15, 18, 24], counting linear regions of a Deep Neural Network [31], performing
inference [2] and providing strong convex relaxations for trained neural networks [3].

We refer the reader to the book by [21] and the surveys by [17, 13, 33] for in-depth

descriptions and analyses of the most commonly used training neural networks.

1.2. Contribution. In this work, we consider neural networks with an arbitrary number of
layers ğ‘˜ and a wide range of activations, loss functions, and architectures. We ï¬rst establish
a general framework that yields a polyhedral representation of generic (regularized) ERM

1In improper learning the predictor may not be a neural network, but will behave similarly to one.

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

3

problems. Our approach is motivated by the work of [10] which describes schemes for
approximate reformulation of many non-convex optimization problems as linear programs.
Our results allow the encoding and analysis of various deep network setups simply by
plugging-in complexity measures for the constituting elements such as layer architecture,
activation functions, and loss functions.

(

=

Ë†ğ‘‹, Ë†ğ‘Œ

Ë†ğ‘¥ğ‘–, Ë†ğ‘¦ğ‘–

ğ·
ğ‘–=1 there is a face

1.2.1. Polyhedral encoding of ERM problems. Given ğœ– > 0 and a sample size ğ· there
exists a data-independent polytope (it can be written down before seeing the data) with the
following properties:
Solving the ERM problem to ğœ–-optimality in data-dependent faces. For every realized
ğ‘ƒ of said polytope such that optimizing
sample
Ë†ğ‘‹ , Ë†ğ‘Œ âŠ†
F
)
Ë†ğ‘‹ , Ë†ğ‘Œ solves (1) to ğœ–-optimality returning a parametrization
certain linear function over
Ëœğœ™
Î¦ which is part of our hypothesis class. As such, the polytope has a build-once-solve-
many feature.
Size of the polytope. The size, measured as bit complexity, of the polytope is roughly
is a constant depending on â„“, ğ‘“ , and Î¦ that we will introduce
ğ‘‚
L
later, ğ‘›, ğ‘š are the dimensions of the data points, i.e., Ë†ğ‘¥ğ‘–
,
and ğ‘ is the dimension of the parameter space Î¦.

â„ğ‘š for all ğ‘–

â„ğ‘› and Ë†ğ‘¦ğ‘–

ğœ–
L/

where

ğ‘š ğ·

âˆˆ [

((

ğ·

F

âˆˆ

âˆˆ

âˆˆ

2

ğ‘

)

(

)

)

]

ğ‘›

+

+

It is important to mention that

measures certain Lipschitzness in the ERM training
problem. While not exactly requiring Lipschitz continuity in the same way, Lipschitz
constants have been used before for measuring training complexity in [20] and more recently
have been shown to be linked to generalization by [22].

L

We point out three important features of this polyhedral encoding. First, it has provable
optimality guarantees regarding the ERM problem and a size with linear dependency on
the sample size without assuming convexity of the optimization problem. Second, the
polytope encodes reasonable approximations of all possible data sets that can be given as
an input to the ERM problem. This in particular shows that our construction is not simply
ğ‘š,
discretizing space: if one considers a discretization of data contained in
the total number of possible data sets of size ğ· is exponential in ğ·, which makes the linear
dependence on ğ· of the size of our polytope a remarkable feature. Finally, our approach
can be directly extended to handle commonly used regularizers (B). For ease of presentation
though we omit regularizers throughout our main discussions.

Ã— [âˆ’

1, 1

1, 1

[âˆ’

]

]

ğ‘›

Remark 1.1. We remark that our goal is to provide new structural results regarding
training problems. Converting our approach into a training algorithm, while subject of
future research, will certainly take considerable eï¬€orts. Nonetheless, we will rely on known
training algorithms with provable guarantees and their running times for providing a notion
of how good our results are. Note that this is a slightly unfair comparison to us, as training
algorithms are not data-independent as our encoding.

1.2.2. Complexity results for various network architectures. We apply our methodology
to various well-known neural network architectures by computing and plugging-in the
corresponding constituting elements into our uniï¬ed results. We provide an overview of
our results in Table 1, where ğ‘˜ is the number of layers, ğ‘¤ is width of the network, ğ‘›
ğ‘š
are the input/output dimensions and ğ‘ is the total number of parameters. In all results the
node computations are linear with bias term and normalized coeï¬ƒcients, and activation
functions with Lipschitz constant at most 1 and with 0 as a ï¬xed point; these include ReLU,
Leaky ReLU, eLU, Tanh, among others.

/

4

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

Table 1. Summary of results for various architectures. DNN refers to a
fully-connected Deep Neural Network, CNN to a Convolutional Neural
Network and ResNet to a Residual Network. ğº is the graph deï¬ning the
Network and Î” is the maximum in-degree in ğº.

Type

Loss

ğ‘‚
Absolute/Quadratic/Hinge
DNN
Cross Entropy w/ Soft-Max ğ‘‚
DNN
ğ‘‚
Absolute/Quadratic/Hinge
CNN
ğ‘‚
ResNet Absolute/Quadratic/Hinge
ResNet Cross Entropy w/ Soft-Max ğ‘‚

Size of polytope
ğ‘›

ğ‘šğ‘¤ğ‘‚
(
ğ‘š log
ğ‘šğ‘¤ğ‘‚
ğ‘šÎ”ğ‘‚
(
ğ‘š log

ğ‘˜2

ğœ–
) /
ğ‘¤ğ‘‚
ğ‘š
(
(cid:1)
)
(
ğ‘›
ğ‘˜2
ğœ–
(
) /
ğ‘˜2
ğœ–
) /
(cid:1)
Î”ğ‘‚
ğ‘š
(cid:1)
)

(

ğ‘›

(

+
ğ‘˜2

+

ğ‘š

ğ‘ ğ·
ğ‘›
ğœ–
) /
ğ‘ ğ·
ğ‘š
+
(cid:1)
ğ‘ ğ·
ğ‘›
ğœ–
) /

+

+
ğ‘š

+
ğ‘˜2

ğ‘š

+

ğ‘ ğ·

+
(cid:1)

(cid:1)
ğ‘š
+
(cid:1)

+

ğ‘ ğ·

(cid:1)

(cid:1)

(cid:1)

(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:0)(cid:0)

Notes
ğ‘ =
ğ‘ =
ğ‘

|
|
â‰ª |

ğ¸
(
ğ¸
(
ğ¸

ğº
)|
ğº
)|
ğº
)|

(

Certain improvements in the results in Table 1 can be obtained by further specifying if
the ERM problem corresponds to regression or classiï¬cation. Nonetheless, these improve-
ments are not especially signiï¬cant and in the interest of clarity and brevity we prefer to
provide a uniï¬ed discussion.

The reader might wonder if the exponential dependence on the other parameters of our
polytope sizes can be improved, namely the input dimension ğ‘›
ğ‘š, parameter space dimen-
sion ğ‘ and depth ğ‘˜. The dependence on the input dimension is unlikely to be improved due
to NP-hardness of training problems ([11, 12]) and obtaining a polynomial dependence on
the parameter space dimension or on the depth remains open [4].

+

The rest of this paper is organized as follows: in Section 2 we introduce the main tools
we use throughout the paper. These include the deï¬nition of treewidth and a generalization
of a result by [10]. In Section 3 we show how multiple ERM problems can be encoded
using a single polytope whose size depend only linearly in the sample-size. We also
analyze this polytopeâ€™s structure and show that its face structure are related to each possible
ERM problem.
In Section 4 we specialize our results to ERM problems arising from
Neural Networks by explicitly computing the resulting polytope size for various common
architectures. In Section 5 we show the sparsity of the network itself can be exploited to
obtain an improvement in the polyhedral encodingâ€™s size. In Section 6 we show that our
LP generalizes well, in the sense that our benign dependency on the sample size allows
us to obtain a moderate-sized polyhedron that approximates the general risk minimization
problem. Finally, in Section 7 we conclude.

2. Preliminaries

In the following let
and ğ¸

ğ»

ğ»

ğ‘›

1, . . . , ğ‘›

{
to denote the vertex-set and edge-set of ğ», respectively, and ğ›¿ğ»

. Given a graph ğ», we will
ğ‘¢

0, . . . , ğ‘›

and

ğ‘›

}

{

}

]

[

]

[

use ğ‘‰
will be the set of edges incident to vertex ğ‘¢. We will need:

(

)

)

(

(

)

0 (cid:17)

(cid:17)

Deï¬nition 2.1. For ğ‘” :
the ğ‘-norm over
K
(whenever it exists).

as

L

K âŠ†
ğ‘”
ğ‘
)
(

â„ğ‘›
â†’
, satisfying

â„, we denote its Lipschitz constant with respect to
ğ‘”

ğ‘ for all ğ‘¥, ğ‘¦

ğ‘”

ğ‘”

ğ‘¥

ğ‘¦

ğ‘¦

ğ‘¥

ğ‘

|

(

) âˆ’

(

)| â‰¤ L

(

) k

âˆ’

k

âˆˆ K

We next deï¬ne the Lipschitz constant of an ERM problem with respect to the inï¬nity

norm.

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

5

Deï¬nition 2.2. Consider the ERM problem (1) with parameters ğ·, Î¦, â„“, ğ‘“ . We deï¬ne the
Architecture Lipschitz Constant

as

(2)

over the domain

=

1, 1

[âˆ’

K

L (
ğ‘›

]

Ã—

ğ·, Î¦, â„“, ğ‘“
(cid:17)

L (
ğ·, Î¦, â„“, ğ‘“

)

Î¦

Ã— [âˆ’

)
1, 1

Lâˆ (
ğ‘š.

]

â„“

ğ‘“

,

(Â·

(

,

Â·)

Â·))

We emphasize that in (2) we are considering the data-dependent entries as variables as
well, and not only the parameters Î¦ as it is usually done in the literature. This subtlety will
become clear later.

Finally, in the following let ğ”¼ğœ”
with respect to the random variable ğœ”

Î©

âˆˆ

and ğ• ğœ”
Î©
âˆˆ
Î©, respectively.

[Â·]

[Â·]
âˆˆ

denote the expectation and variance

Ã

2.1. Neural Networks. A neural network can be understood as a function ğ‘“ deï¬ned
â„ğ‘š. The directed graph
â„ğ‘› to ğ‘“
over a directed graph that maps inputs ğ‘¥
ğº =
ğ‘‰, ğ¸
, which represents the network architecture, often naturally decomposes into
(
layers ğ‘‰ =
ğ‘‰, where ğ‘‰0 is referred to as the input layer and ğ‘‰ğ‘˜ as the
âŠ†
output layer. To all other layers we refer to as hidden layers.

0 ğ‘‰ğ‘– with ğ‘‰ğ‘–

) âˆˆ

âˆˆ

âˆˆ[

ğ‘¥

)

(

ğ‘˜

]

ğ‘–

(

Each vertex ğ‘£
ğ‘¤, ğ‘£

ğ‘‰,
ğ‘˜
0 has an associated set of in-nodes denoted by ğ›¿+(
ğ‘‰ğ‘– with ğ‘–
) âŠ†
]
âˆˆ [
âˆˆ
ğ‘‰ deï¬ned
and an associated set of out-nodes ğ›¿âˆ’(
ğ‘£
ğ›¿+(
ğ¸ for all ğ‘¤
so that
)
âˆˆ
) âˆˆ
are the inputs (from data) and if ğ‘– = ğ‘˜, then ğ›¿âˆ’(
analogously. If ğ‘– = 0, then ğ›¿+(
ğ‘£
ğ‘£
are
)
the outputs of the network. These graphs do neither have to be acyclic (as in the case
of recurrent neural networks) nor does the layer decomposition imply that arcs are only
allowed between adjacent layers (as in the case of ResNets).
In feed-forward networks,
however, the graph is assumed to be acyclic.

) âŠ†

ğ‘£

ğ‘£

)

]

ğ‘˜

âˆˆ

âˆˆ [

Each node ğ‘£

ğ‘‰ performs a node computation ğ‘”ğ‘–

â„ with
) | â†’
is typically a smooth function (often these are linear or aï¬ƒne linear functions) and
â„ with ğ‘–
ğ›¿+(
ğ‘˜
is
]
âˆˆ [
= max
0, ğ‘¥
ğ‘¥
)
}
{
(
is set to ğ‘ğ‘–
ğ‘˜
for nodes in layer ğ‘–
.
]
âˆˆ [
ğ‘–
1
ğ‘—=0ğ‘‰ ğ‘—, i.e.,
ğ‘£
âˆ’
) âŠ† âˆª

ğ‘–
then the node activation is computed as ğ‘ğ‘–
â†’
a (not necessarily smooth) function (e.g., ReLU activations of the form ğ‘ğ‘–
and the value on all out-nodes ğ‘¤
ğ›¿+(
ğ›¿âˆ’(
In feed-forward networks, we can further assume that if ğ‘£
âˆˆ
all arcs move forward in the layers.

ğ›¿+(
, where ğ‘ğ‘– : â„

ğ‘£
)))
ğ‘‰ğ‘–, then ğ›¿+(

, where ğ‘”ğ‘– : â„ |

ğ›¿+ (

)))

ğ‘”ğ‘–

ğ‘”ğ‘–

))

âˆˆ

ğ‘£

ğ‘£

ğ‘£

(

(

(

)

)

(

(

ğ‘£

2.2. Treewidth. Treewidth is an important graph-theoretical concept in the context of
solving optimization problems with â€˜sparseâ€™ structure. This parameter is used to measure
how tree-like the graph is, and its use will be the main workhorse behind our results

Deï¬nition 2.3. A tree-decomposition ([29]) of an undirected graph ğº is a pair
where ğ‘‡ is a tree and ğ‘„ =
ğ‘„ğ‘¡
ğ‘‡
such that
)}
: ğ‘£
, the set
(i) For all ğ‘£
(ii) For each
ğ‘‰
ğº
ğ¸
such that
(
{
: ğ‘¡
The width of the decomposition is deï¬ned as max
ğ‘„ğ‘¡
ğº is the minimum width over all tree-decompositions of ğº.

forms a sub-tree ğ‘‡ğ‘£ of ğ‘‡, and
ğ‘‡ğ‘¢
1. The treewidth of

ğ‘‰
(
âˆˆ
ğ‘‡
ğ‘‰
{
)
(
âˆˆ
there is a ğ‘¡

is a family of subsets of ğ‘‰

ğ‘‰
âˆˆ
(
ğ‘¢, ğ‘£
{

ğ‘‡, ğ‘„

(

)

ğ‘„ğ‘¡ , i.e., ğ‘¡

ğ‘¢, ğ‘£
ğ‘‰

} âŠ†
ğ‘‡
(

)
} âˆˆ

âˆˆ
ğ‘‡
(

)} âˆ’

ğ‘‡ğ‘£.

ğ‘„ğ‘¡

: ğ‘¡

ğº

ğº

{|

âˆ©

âˆˆ

âˆˆ

âˆˆ

}

{

)

)

(

)

ğ‘¡

|

of a tree-decomposition

We refer to the ğ‘„ğ‘¡ as bags as customary. In addition to width, another important feature
.
)|
An alternative deï¬nition to Deï¬nition 2.3 of treewidth that the reader might ï¬nd useful
is the following; recall that a chordal graph is a graph where every induced cycle has length
exactly 3.

we use is the size of the tree-decomposition given by

ğ‘‡, ğ‘„

ğ‘‰

ğ‘‡

(

)

(

|

Deï¬nition 2.4. An undirected graph ğº =
graph ğ» =

with ğ¸

(

ğ¸ â€² and clique number

has treewidth
ğœ”

1.

)

ğ‘‰, ğ¸

ğ‘‰, ğ¸ â€²)

(

âŠ†

â‰¤

+

ğœ” if there exists a chordal

â‰¤

6

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

1

4

6

2

7

3

5

8

(a) Graph ğº

1 2
4

2 3
5

2 4
7

2 5
7

4 6
7

5 7
8

(b) A tree-decomposition of ğº of width 2, with the
sets ğ‘„ğ‘¡ indicated inside each node of the tree.

Figure 1. Example of graph and valid tree-decomposition

ğ» in the deï¬nition above is sometimes referred to as a chordal completion of ğº. In
Figure 1 we present an example of a graph and a valid tree-decomposition. The reader can
easily verify that the conditions of Deï¬nition 2.3 are met in this example. Moreover, using
Deï¬nition 2.4 one can verify that the treewidth of the graph in Figure 1 is exactly 2.

Two important folklore results we use are the following.

Lemma 2.5. Let ğº be a graph with a valid tree-decomposition
exists a valid tree-decomposition

)
of width at most ğœ” such that

(
Lemma 2.6. Let ğº be a graph with a valid tree-decomposition
clique of ğº. Then there exists ğ‘¡

ğ‘‡ such that ğ¾

ğ‘‡ â€², ğ‘„â€²)

ğ‘‡,ğ‘„

(

(

ğ‘„ğ‘¡ .

|
ğ‘‡, ğ‘„

)

(

ğ‘‡ â€²)| âˆˆ
and ğ¾

of width ğœ”. Then there
.
ğ‘‚
ğ‘‰

ğº

ğ‘‰

(|

(
ğº

)|)
a

)

ğ‘‰

âŠ†

(

âˆˆ

âŠ†

2.3. Binary optimization problems with small treewidth. Here we discuss how to for-
mulate and solve binary optimization problems that exhibit sparsity in the form of small
treewidth. Consider a problem of the form

BO

)

(

min

s.t.

+

ğ‘ğ‘‡ ğ‘¥
ğ‘“ğ‘–
ğ‘¥
(
ğ‘” ğ‘—
ğ‘¥

(

ğ‘‘ğ‘‡ ğ‘¦
0
= ğ‘¦ ğ‘—
ğ‘›,

) â‰¥
ğ‘¥

)
0, 1

âˆˆ {

}

ğ‘–

ğ‘—

ğ‘š

]
ğ‘

]

âˆˆ [

âˆˆ [

where the ğ‘“ğ‘– and ğ‘” ğ‘— are arbitrary functions that we access via a function value oracle.

of BO is the graph which
Deï¬nition 2.7. The intersection graph Î“
has a vertex for each ğ‘¥ variable and an edge for each pair of ğ‘¥ variables that appear in a
common constraint.

for an instance

[I]

I

Note that in the above deï¬nition we have ignored the ğ‘¦ variables which will be of great
importance later. The sparsity of a problem is now given by the treewidth of its intersection
graph and we obtain:

Theorem 2.8. Let
width ğœ”, there is an exact linear programming reformulation of
variables and constraints.

be an instance of BO. If Î“

[I]

I

I

has a tree-decomposition
ğ‘‰
with ğ‘‚

2ğœ”

(

(|

ğ‘‡, ğ‘„
(
ğ‘‡

)| +

)
ğ‘

of

))

(

Theorem 2.8 is a generalization of a theorem by [10] distinguishing the variables ğ‘¦,
which do not need to be binary in nature, but are fully determined by the binary variables
ğ‘¥. A full proof is omitted as it is similar to the proof in [10]. For the sake of completeness,
we include a proof sketch below.

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

7

Proof. (sketch). Since the support of each ğ‘“ğ‘– induces a clique in the intersection graph,
ğ‘„ (Lemma 2.6). The same holds for each ğ‘” ğ‘— .
there must exist a bag ğ‘„ such that supp
We modify the tree-decomposition

ğ‘“ğ‘–
(
ğ‘‡, ğ‘„

) âŠ†
to include the ğ‘¦ ğ‘— variables the following way:
and add a new bag ğ‘„â€²(
for each diï¬€erent ğ‘—. This
1, which has each

)

)

ğ‘—

ğ‘¦ ğ‘—

]
âˆª {

, choose a bag ğ‘„ containing supp
and connected to ğ‘„.

For each ğ‘—
ğ‘
âˆˆ [
(
consisting of ğ‘„
}
We do this for every ğ‘—
, with a diï¬€erent ğ‘„ â€²(
ğ‘‡ â€², ğ‘„â€²)
creates a new tree-decomposition
ğ‘—
variable ğ‘¦ ğ‘— contained in a single bag ğ‘„â€²(
ğ‘‡ â€²|
The size of the tree-decomposition is

ğ‘—
of width at most ğœ”
which is a leaf.
ğ‘.

âˆˆ [

| +

ğ‘” ğ‘—

)
=

ğ‘‡

ğ‘

]

)

(

|

|

â€¢

â€¢

+

â€¢

(

)

From here, we proceed as follows:

For each ğ‘¡

âˆˆ

â€¢

ğ‘¦ ğ‘— for some ğ‘—

ğ‘

, we construct

ğ‘‡ â€², if ğ‘„ â€²ğ‘¡ âˆ‹
ğ‘¡ (cid:17)
ğ‘¥, ğ‘¦
F
{(
ğ‘¦ = ğ‘” ğ‘—

âˆˆ [
]
â„ :
Ã—
0 for supp

ğ‘„ğ‘¡

0, 1
, ğ‘“ğ‘–

}
ğ‘¥

) âˆˆ {
ğ‘¥

(

)

(

) â‰¥

ğ‘“ğ‘–

(

) âŠ†

ğ‘„â€²ğ‘¡ }

{

ğ‘¥

ğ‘„ğ‘¡

0, 1

otherwise we simply construct
ğ‘¡ (cid:17)
F

) â‰¥
âˆˆ {
Note that these sets have size at most 2 |
We deï¬ne variables ğ‘‹
at most 2ğœ”
ğ‘‰
ğ‘‡ â€²)|
(
ğ‘‡ â€² and ğ‘£
For each ğ‘¡

ğ‘Œ , ğ‘
[

ğ‘“ğ‘–

ğ‘¥

}

(

:

.

ğ‘„â€²ğ‘¡ |.

|
âˆˆ

â€¢

â€¢

We formulate the following linear optimization problem

]

where ğ‘Œ , ğ‘ form a partition of ğ‘„â€²ğ‘¡1 âˆ©
ğ‘¡ , we create a variable ğœ†ğ‘£ . These are at most 2ğœ”

ğ‘„â€²ğ‘¡2. These are

ğ‘‰

|

.

ğ‘‡ â€²)|

(

âˆˆ F

0 for supp

ğ‘“ğ‘–

(

) âŠ†

.

ğ‘„â€²ğ‘¡ }

LBO

)

(

min

ğ‘ğ‘‡ ğ‘¥

s.t.

ğ‘‘ğ‘‡ ğ‘¦

+
ğœ†ğ‘£ = 1

ğœ†ğ‘£

ğ‘£ğ‘–

Ã•ğ‘£
ğ‘¡
âˆˆF

ğ‘Œ
Ã–ğ‘–
âˆˆ

ğ‘ (
Ã–ğ‘–
âˆˆ

ğ‘¡
âˆ€

âˆˆ

ğ‘‡ â€²

1

ğ‘£ğ‘–

)

âˆ’

ğ‘Œ , ğ‘

âˆ€(

ğ‘„ â€²ğ‘¡ , ğ‘¡

ğ‘‡ â€²

âˆˆ

) âŠ†

ğ‘‡ â€², ğ‘£

ğ‘‡ â€², ğ‘–

ğ‘¡
âˆ€
ğ‘¡
âˆ€

âˆˆ

âˆˆ

ğ‘¡
âˆˆ F
ğ‘„â€²ğ‘¡ âˆ© [

âˆˆ

ğ‘›

]

ğœ†ğ‘£ğ‘” ğ‘—

ğ‘£

)

(

ğ‘—

âˆ€

ğ‘

]

âˆˆ [

Ã•ğ‘£
ğ‘¡
âˆˆF
ğ‘Œ , ğ‘
ğ‘‹
[

]

=

0

ğœ†ğ‘£
â‰¥
ğ‘¥ğ‘– =

Ã•ğ‘£
ğ‘¡
âˆˆF

ğ‘¦ ğ‘— =

ğœ†ğ‘£ ğ‘£ğ‘–

Ã•ğ‘£
âˆˆFğ‘„â€² (

ğ‘—

)

ğ‘—

)

Note that the notation in the last constraint is justiï¬ed since by construction supp
) âŠ†
. The proof of the fact that LBO is equivalent to BO follows from the arguments by
ğ‘„â€²(
[10]. The key diï¬€erence justifying the addition of the ğ‘¦ variables relies in the fact that they
, and thus in no intersection of two
only appear in leaves of the tree decomposition
bags. The gluing argument using variables ğ‘‹
then follows directly, as it is then only
needed for the ğ‘¥ variables to be binary.

ğ‘‡ â€², ğ‘„â€²)
]

(
ğ‘Œ , ğ‘
[

ğ‘” ğ‘—

(

We can substitute out the ğ‘¥ and ğ‘¦ variables and obtain a polytope whose variables are
variables and

. This produces a polytope with at most 2

2ğœ”

ğ‘‰

]
(
constraints. This proves the size of the polytope is ğ‘‚

Â·

|

ğ‘‡ â€²)|
2ğœ”
(

ğ‘‰

ğ‘‡

(

(|

)| +

ğ‘

))
(cid:3)

only ğœ†ğ‘£ and ğ‘‹
2ğœ”
ğ‘‰
1
2
(
)|
as required.

+

Â·

ğ‘Œ , ğ‘
[
ğ‘‡ â€²)|
(

8

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

3. Approximation to ERM via a data-independent polytope

We now proceed to the construction of the data-independent polytope encoding multiple

ERM problem. As mentioned before, we assume Î¦
1, 1
) âˆ¼ D
]
ğ‘š as normalization to simplify the exposition. Since
ğ‘›
satisï¬es
the BO problem only considers linear objective functions, we begin by reformulating the
ERM problem (1) in the following form:

ğ‘ as well as

) âˆˆ [âˆ’

âŠ† [âˆ’

Ã— [âˆ’

1, 1

1, 1

ğ‘¥, ğ‘¦

ğ‘¥, ğ‘¦

]

]

(

(

(5)

1
ğ·

min
ğœ™
âˆˆ

Î¦ (

ğ¿ğ‘‘

ğ·

Ã•ğ‘‘=1

ğ¿ğ‘‘ = â„“

ğ‘“

(

(

Ë†ğ‘¥ ğ‘‘, ğœ™

, Ë†ğ‘¦ğ‘‘

)

) âˆ€

ğ‘‘

ğ·

âˆˆ [

])

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

3.1. Approximation of the feasible region via an ğœ–-grid. Motivated by this reformulation,
we study an approximation to the following set:

(6)

ğ·, Î¦, â„“, ğ‘“

ğ‘†

(

=

)

{(

ğ‘¥1, ... , ğ‘¥ ğ·, ğ‘¦1, ... , ğ‘¦ğ·, ğœ™, ğ¿

)

ğ‘¥ ğ‘‘, ğœ™

ğ‘“

(

(

, ğ‘¦ğ‘‘
ğ‘›

,
)
ğ‘š,

+

)
1, 1

]

:ğ¿ğ‘‘ = â„“
ğ‘¥ğ‘–, ğ‘¦ğ‘–
(
Î¦
ğœ™

âˆˆ

) âˆˆ [âˆ’

}
â„ with

The variables

(

ğ‘¥ğ‘–, ğ‘¦ğ‘–

ğ·
ğ‘–=1 denote the data variables. Let ğ‘Ÿ

1. Given
we can approximate ğ‘Ÿ as a sum of inverse powers of 2, within additive error
there exist values ğ‘§â„
, so

with â„

0, 1

ğ‘ğ›¾

â‰¤

â‰¤

âˆ’

âˆˆ

1

ğ‘Ÿ

)

1

log2 ğ›¾âˆ’

âˆˆ (

0, 1

ğ›¾
proportional to ğ›¾. For ğ‘ğ›¾ (cid:17)
that

)

âŒˆ

âˆˆ {

}

âˆˆ [

]

âŒ‰

(7)

2

1

âˆ’

+

Â·

ğ‘ğ›¾

Ã•â„=1

2âˆ’

â„ğ‘§â„

ğ‘Ÿ

1

2

Â·

+

â‰¤ âˆ’

â‰¤

ğ‘ğ›¾

Ã•â„=1

2âˆ’

â„ğ‘§â„

2ğ›¾

+

â‰¤

1.

â„ğ‘§â„
Our strategy is now to approximately represent the ğ‘¥, ğ‘¦, ğœ™ variables as
where each ğ‘§â„ is a (new) binary variable. Deï¬ne ğœ– = 2ğ›¾
=
is the
architecture Lipschitz constant deï¬ned in (2), and consider the following approximation of
ğ‘†

Â·
+
ğ·, Î¦, â„“, ğ‘“
Ã

ğ·, Î¦, â„“, ğ‘“

, where

âˆ’
L (

L

L

2

1

ğ¿ğ›¾
â„=1 2âˆ’
)

:

(
ğ‘† ğœ–

)

ğ·, Î¦, â„“, ğ‘“

(

(cid:17)

)

(cid:8)

ğ‘¥1, . . . , ğ‘¥ ğ·, ğ‘¦1, . . . , ğ‘¦ğ·, ğœ™, ğ¿
(
ğ¿ğ‘‘ = â„“

, ğ‘¦ğ‘‘

, ğ‘‘

)
ğ·

ğ‘“

0, 1

}

âˆˆ {

: ğ‘§

,

ğ‘ğ›¾

ğ‘

(

+

ğ·ğ‘›

+

ğ·ğ‘š

) , ğœ™

Î¦,

âˆˆ

(

ğ‘¥ ğ‘‘, ğœ™
ğ‘ğ›¾

)

(

)

âˆˆ [

]

ğœ™ğ‘– =

2

1

âˆ’

+

ğ‘¦ğ‘‘
ğ‘–

=

2

1

âˆ’

+

ğ‘¥ ğ‘‘
ğ‘–

=

2

1

âˆ’

+

Ã•â„=1
ğ‘ğ›¾

Ã•â„=1
ğ‘ğ›¾

Ã•â„=1

2âˆ’

â„ğ‘§ ğœ™

ğ‘–,â„, ğ‘–

ğ‘

,

]

âˆˆ [

2âˆ’

â„ğ‘§ğ‘¦ğ‘‘

ğ‘–,â„, ğ‘‘

, ğ‘–

ğ·

]

ğ‘š

,

]

âˆˆ [

âˆˆ [

2âˆ’

â„ğ‘§ğ‘¥ğ‘‘

ğ‘–,â„, ğ‘‘

, ğ‘–

ğ·

]

ğ‘›

]

âˆˆ [

âˆˆ [

.

Note that substituting out the ğ‘¥, ğ‘¦, ğœ™ using the equations of ğ‘† ğœ–
sible region as BO. We can readily describe the error of the approximation of ğ‘†
by ğ‘† ğœ–

in the ERM problem (1) induced by the discretization:

ğ·, Î¦, â„“, ğ‘“

)

(

, we obtain a fea-

ğ·, Î¦, â„“, ğ‘“

(

)

(cid:9)
ğ·, Î¦, â„“, ğ‘“

(

)

Lemma 3.1. Consider any
ists
ğœ–.

Ë†ğ‘¥1, . . . , Ë†ğ‘¥ ğ·, Ë†ğ‘¦1, . . . , Ë†ğ‘¦ğ·, Ë†ğœ™, Ë†ğ¿

(

(

ğ‘¥1, . . . , ğ‘¥ ğ·, ğ‘¦1, . . . , ğ‘¦ğ·, ğœ™, ğ¿

ğ‘† ğœ–

(

) âˆˆ

ğ·, Î¦, â„“, ğ‘“

)

) âˆˆ
such that

ğ‘†

ğ·, Î¦, â„“, ğ‘“
ğ·
ğ‘‘=1 ğ¿ğ‘‘

(
1
ğ·

. Then, there ex-
ğ·
ğ‘‘=1

Ë†ğ¿ğ‘‘

1
ğ·

)
âˆ’

â‰¤

(cid:12)
(cid:12)

Ã

Ã

(cid:12)
(cid:12)

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

9

ğ‘¥1, ğ‘¦1

ğœ™1

ğ‘¥2, ğ‘¦2

ğœ™ğ‘

ğœ™2

ğ‘¥ğ·, ğ‘¦ğ·

ğœ™4

ğœ™3

ğ‘¥3, ğ‘¦3

(a) Intersection Graph of ğ‘† ğœ–

ğ·, Î¦, â„“, ğ‘“

(

)

ğœ™
ğ‘¥1, ğ‘¦1

ğœ™
ğ‘¥2, ğ‘¦2

ğœ™
ğ‘¥ğ·, ğ‘¦ğ·

Â· Â· Â·

(b) Valid Tree-Decomposition

Figure 2. Intersection Graph and Tree-Decomposition of ğ‘† ğœ–

ğ·, Î¦, â„“, ğ‘“

(

)

Proof. Choose binary values Ëœğ‘§ so as to attain the approximation for variables ğ‘¥, ğ‘¦, ğœ™ as in
(7) and deï¬ne Ë†ğ‘¥, Ë†ğ‘¦, Ë†ğœ™, Ë†ğ¿ from Ëœğ‘§ according to the deï¬nition of ğ‘† ğœ–

ğ·, Î¦, â„“, ğ‘“

. Since

ğœ–

2ğ›¾ =

(

ğ‘‘

ğ·

)

ğ‘¥ ğ‘‘, ğ‘¦ğ‘‘, ğœ™

Ë†ğ‘¥ ğ‘‘, Ë†ğ‘¦ğ‘‘, Ë†ğœ™
)

|

(

]

âˆ’

(cid:3)

(cid:13)
(cid:13)

L

ğ¿ğ‘‘

| â‰¤

) âˆ’ (
Ë†ğ¿ğ‘‘

by Lipschitzness we obtain

âˆˆ [
ğœ–. The result then follows.

âˆ â‰¤
(cid:13)
(cid:13)
3.2. Linear reformulation of the binary approximation. So far, we have phrased the
ERM problem (1) as a BO problem using a discretization of the continuous variables. This
in and of itself is neither insightful nor useful. In this section we will perform the key step,
reformulating the convex hull of ğ‘† ğœ–

as a moderate-sized polytope.
)
ğ·, Î¦, â„“, ğ‘“
using the ğ‘§ variables, we can see
(
that the intersection graph of ğ‘† ğœ–
ğ‘¥, ğ‘¦, ğœ™
is given by Figure 2a, where we use
)
)
as stand-ins for corresponding the binary variables ğ‘§ğ‘¥, ğ‘§ğ‘¦, ğ‘§ ğœ™. Recall that the intersection
graph does not include the ğ¿ variables. It is not hard to see that a valid tree-decomposition
for this graph is given by Figure 2b. This tree-decomposition has size ğ· and width
ğ‘
ğ‘š
ğ‘›
ğ‘ğ›¾
variables). This yields our main
(
theorem:

ğ·, Î¦, â„“, ğ‘“
variables in ğ‘† ğœ–
ğ·, Î¦, â„“, ğ‘“

1 (much less than the ğ‘ğ›¾

After replacing the

ğ‘¥, ğ‘¦, ğœ™

ğ·ğ‘š

ğ·ğ‘›

) âˆ’

ğ‘

+

+

+

+

(

)

)

(

)

(

(

(

Main Theorem 3.1. Let ğ·
projection of a polytope with the following properties:
(a) The polytope has no more than 4ğ·

âˆˆ

2

ğ‘š

ğ‘›

+

+

â„• be a given sample size. Then conv

ğ‘ variables and 2ğ·

ğ‘† ğœ–

(

ğ·, Î¦, â„“, ğ‘“

is the

))

2

2

ğœ–
L/

)

(

ğ‘›

+

ğ‘š

+

ğ‘

+

(

(

ğœ–
L/

)

(

1

)

constraints. We refer to the resulting polytope as ğ‘ƒğ‘†ğœ– .
ğ‘š
ğ‘›
+

2

for ğ‘‚

(b) The polytope ğ‘ƒğ‘†ğœ– can be constructed in time ğ‘‚
ğ‘š
ğ‘
evaluations of â„“ and ğ‘“ .
+
)
Ë†ğ‘‹, Ë†ğ‘Œ
=
ğ·
ğ‘–=1,

+
((
(c) For any sample
such that

ğœ–
L/

Ë†ğ‘¥ğ‘–, Ë†ğ‘¦ğ‘–

Ë†ğ‘¥ğ‘–, Ë†ğ‘¦ğ‘–

2

(

)

(

)

(

)

ğ‘›

ğœ–
L/

)

) âˆˆ [âˆ’

((

+

ğ‘ ğ·

)

plus the time required

ğ‘›

+

ğ‘š, there is a face

1, 1

]

Ë†ğ‘‹ , Ë†ğ‘Œ of ğ‘ƒğ‘†ğœ–

F

Ëœğœ™

âˆˆ

argmin

ğ‘“

â„“

1
ğ·

ğ·
ğ‘–=1

n
ğ‘ is an
, Ë†ğ‘¦ğ‘–
Ë†ğ‘¥ğ‘–, ğœ™âˆ—)
1, 1
satisï¬es
optimal solution to the ERM problem (1) with input data
. This means that
(cid:12)
(cid:12)
solving an LP using an appropriate face of ğ‘ƒğ‘†ğœ– solves the ERM problem (1) within an
additive error 2ğœ–.

, Ë†ğ‘¦ğ‘–

(cid:1)(cid:12)
(cid:12)

Ã

â‰¤

â„“

)

]

(

(

(

(

(cid:0)

ğ‘“

ğœ™, ğ¿

) âˆˆ

(
(cid:12)
(cid:12)
Ë†ğ‘¥ğ‘–, Ëœğœ™
(cid:12)
)

Ë†ğ‘‹ , Ë†ğ‘Œ )

proj
ğœ™,ğ¿ (F
o
2ğœ–, where ğœ™âˆ— âˆˆ [âˆ’
Ë†ğ‘‹, Ë†ğ‘Œ
)

(

ğ·

ğ¿ğ‘–

1
ğ·

Ã•ğ‘–=1
) âˆ’

(d) The face

Ë†ğ‘‹ , Ë†ğ‘Œ arises by simply substituting-in actual data for the data-variables ğ‘¥, ğ‘¦,
which determine the approximations ğ‘§ğ‘¥, ğ‘§ğ‘¦ and is used to ï¬xed variables in the descrip-
tion of ğ‘ƒğ‘†ğœ– .

F

Proof. Part (a) follows directly from Theorem 2.8 using ğ‘ğ›¾ =
tree-decomposition of Figure 2b, which implies

log

along with the
)âŒ‰
ğ‘ = 2ğ· in this case. A proof of

ğœ–
L/

ğ‘‰

2

âŒˆ

(

ğ‘‡ â€²)| +

(

|

10

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

parts (c) and (d) is given in the next subsection. For part (b) we analyze the construction
steps of the linear program deï¬ned in proof of Theorem 2.8.

From the tree-decomposition detailed in Section 3.2, we see that data-dependent vari-
. Let us index the bags
ğ‘‘ we construct in the
. Using this observation, we can

ables ğ‘¥, ğ‘¦, ğ¿ are partitioned in diï¬€erent bags for each data ğ‘‘
ğ·
]
using ğ‘‘. Since all data variables have the same domain, the sets
proof of Theorem 2.8 will be the same for all ğ‘‘
construct the polytope as follows:

âˆˆ [

âˆˆ [

ğ·

F

]

(1) Fix, say, ğ‘‘ = 1 and enumerate all binary vectors corresponding to the discretization

of ğ‘¥1, ğ‘¦1, ğœ™.
(2) Compute â„“

(
ğ‘“ and â„“. This deï¬nes the set

(

)

)

ğ‘“

ğ‘¥1, ğœ™

, ğ‘¦1

. This will take ğ‘‚

2

ğœ–
L/

)

((

ğ‘›

+

ğ‘š

+

ğ‘

)

function evaluations of

(3) Duplicate this set ğ· times, and associate each copy with a bag indexed by ğ‘‘
(4) For each ğ‘‘
(5) For each ğ‘‘

ğ·
ğ·
bags ğ‘‘ and ğ‘‘
1. This will create ğ‘‚
in the intersections are the discretized ğœ™ variables.

, create variables ğ‘‹
2

corresponding to the intersection of
variables, since the only variables

, and each ğ‘£
1

ğ‘‘ create a variable ğœ†ğ‘£.

ğ‘Œ , ğ‘
]
[
ğ‘
ğœ–
L/

âˆˆ [
âˆˆ [
+

ğ·

.

]

âˆˆ [

âˆˆ F

]
âˆ’

((

]

)

)

1.

F

(6) Formulate LBO.

The only evaluations of â„“ and ğ‘“ are performed in the construction of
1. As for the
additional computations, the bottleneck lies in creating all ğœ† variables, which takes time
ğ‘‚

ğ‘ ğ·

F

ğ‘š

ğ‘›

2

.

ğœ–
L/

)

((

+

+

)

(cid:3)

ğ‘›

2

Remark 3.2. Note that in step 1 of the polytope construction we are enumerating all possible
ğ‘š are
discretized values of ğ‘¥1, ğ‘¦1, i.e., we are implicitly assuming all points in
ğ‘š term in the polytope size estimation. If
possible inputs. This is reï¬‚ected in the
ğœ–
)
L/
one were to use another discretization method (or a diï¬€erent â€œpoint generationâ€ technique)
using more information about the input data, this term could be improved and the explicit
exponential dependency on the input dimension of the polytope size could be alleviated
signiï¬cantly. However, note that in a fully-connected neural network we have ğ‘
ğ‘š
and thus an implicit exponential dependency on the input dimension could remain unless
more structure is assumed. This is in line with the NP-hardness results. We leave the full
development of this potential improvement for future work.

1, 1

[âˆ’

â‰¥

+

ğ‘›

(

]

ğ‘›

+

+

Note that the number of evaluations of â„“ and ğ‘“ is independent of ğ·. We would like to
further point out that we can provide an interesting reï¬nement of this theorem: if Î¦ has an
inherent network structure (as in the Neural Networks case) one can exploit treewidth-based
sparsity of the network itself. This would reduce the exponent in the polytope size to an
expression that depends on the sparsity of the network, instead of its size. We discuss this
in Section 5.

Remark 3.3. An additional important point arising from this new perspective on training
problems via linear programming comes from duality theory.
If one projects-out the
variables associated to the parameters ğœ™ in ğ‘ƒğ‘†ğœ– , the resulting projected polytope would
represent all possible samples of size ğ· and their achievable loss vector. This means that
there exists a dual certiï¬cate proving whether a loss vector, or average, is (approximately)
achievable by a sample, without using the ğœ™ variables.

3.3. Data-dependent faces of the data-independent polytope. We now proceed to show
how the ERM problem for a speciï¬c data set is encoded in a face of ğ‘ƒğ‘†ğœ– . This provides a
proof of points (c) and (d) in Theorem 3.1.

Additionally, for every ğœ™

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

11

)

(

=

)
(

=
(
Ë†ğ‘‹, Ë†ğ‘Œ
)

Consider a ï¬xed data set

Ë†ğ‘‹, Ë†ğ‘Œ
ğ·
Ë†ğ‘¥ğ‘–, Ë†ğ‘¦ğ‘–
ğ‘–=1 and let ğœ™âˆ— be an optimal solution to
. Since ğ‘ƒğ‘†ğœ– encodes â€œapproximatedâ€ versions
the ERM problem with input data
of the possible samples, we begin by approximating
. Consider binary variables
ğ‘§ Ë†ğ‘¥, ğ‘§ Ë†ğ‘¦ to attain the approximation (7) of the input data and deï¬ne Ëœğ‘¥, Ëœğ‘¦ from ğ‘§ Ë†ğ‘¥, ğ‘§ Ë†ğ‘¦, i.e.,
Ëœğ‘¥ ğ‘‘
ğ‘–

ğ‘ğ›¾
â„=1 2âˆ’
â„ğ· : ğ¿ğ‘‘ = â„“
ğ‘†
Ã
)
(
and similarly as before deï¬ne ğ‘† ğœ–
to be its discretized version (on variables ğœ™).
)
The following Lemma shows the quality of approximation to the ERM problem obtained
Ëœğ‘‹, Ëœğ‘Œ , Î¦, â„“, ğ‘“
using ğ‘†

ğ‘–,â„ and similarly for Ëœğ‘¦. Deï¬ne
Î¦
ğœ™, ğ¿
{(
) âˆˆ
Ëœğ‘‹, Ëœğ‘Œ, Î¦, â„“, ğ‘“

(
and subsequently ğ‘† ğœ–

Ëœğ‘‹, Ëœğ‘Œ, Î¦, â„“, ğ‘“

Ëœğ‘‹, Ëœğ‘Œ, Î¦, â„“, ğ‘“

Ëœğ‘¥ ğ‘‘, ğœ™

â„ğ‘§ Ë†ğ‘¥ğ‘‘

Ë†ğ‘‹, Ë†ğ‘Œ

, Ëœğ‘¦ğ‘‘

)}

Ã—

âˆ’

=

+

1

2

(

(

)

(

)

ğ‘“

.

(

)
Lemma 3.4. For any
such that

ğœ™, ğ¿

(

) âˆˆ

ğ‘†

(

(

)

Ëœğ‘‹, Ëœğ‘Œ , Î¦, â„“, ğ‘“

there exists

ğœ™â€², ğ¿â€²) âˆˆ

(

ğ‘† ğœ–

(

Ëœğ‘‹, Ëœğ‘Œ , Î¦, â„“, ğ‘“

)

)
ğ·

ğ¿â€²ğ‘‘

âˆ’

1
ğ·
Ã•ğ‘‘=1
ğœ™â€², ğ¿â€²) âˆˆ
(
1
, Ë†ğ‘¦ğ‘‘
ğ·

) âˆ’

â‰¤
(cid:12)
(cid:12)
(cid:12)
ğ‘† ğœ–
(cid:12)
(cid:12)
ğ·

(

Ã•ğ‘‘=1

ğ·

ğ¿ğ‘‘

1
ğ·

Ã•ğ‘‘=1
Î¦, there exists

Ë†ğ‘¥ ğ‘‘, ğœ™

ğ‘“

â„“

(

(

)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ·

Ã•ğ‘‘=1

âˆˆ
1
ğ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ¿â€²ğ‘‘

ğœ–.

â‰¤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğœ–.

Ëœğ‘‹, Ëœğ‘Œ, Î¦, â„“, ğ‘“

such that

)

Proof. The ï¬rst inequality follows from the same proof as in Lemma 3.1. For the second
, Ëœğ‘¦ğ‘‘
inequality, let ğœ™â€² be the binary approximation to ğœ™, and ğ¿â€² deï¬ned by ğ¿â€²ğ‘‘
.
)
(cid:3)
Since Ëœğ‘¥, Ëœğ‘¦, ğœ™â€² are approximations to Ë†ğ‘¥, Ë†ğ‘¦, ğœ™, the result follows from Lipschitzness.

Ëœğ‘¥ ğ‘‘, ğœ™â€²)

= â„“

(

(

ğ‘“

Lemma 3.5.

satisï¬es

Ë†ğœ™, Ë†ğ¿

(

) âˆˆ

argmin

1
ğ·

(

ğ·

Ã•ğ‘‘=1

ğ¿ğ‘‘ :

ğœ™, ğ¿

(

) âˆˆ

ğ‘† ğœ–

(

Ëœğ‘‹, Ëœğ‘Œ , Î¦, â„“, ğ‘“

))

ğ·

1
ğ·

ğ·

â„“

1
ğ·

â„“

ğ‘“

(

(

Ë†ğ‘¥ ğ‘‘, ğœ™âˆ—

, Ë†ğ‘¦ğ‘‘

)

) âˆ’

ğ‘“

Ë†ğ‘¥ ğ‘‘, Ë†ğœ™
)

(

(

, Ë†ğ‘¦ğ‘‘

2ğœ–.

â‰¤

(cid:12)
(cid:12)
(cid:12)
Proof. Since Ë†ğœ™
(cid:12)
(cid:12)
âˆˆ
ğ·
ately have 1
ğ‘‘=1 â„“
ğ·
) â‰¤
previous Lemma we know there exists

Ã•ğ‘‘=1
Î¦, and ğœ™âˆ— is an optimal solution to the ERM problem, we immedi-
Ë†ğ‘¥ ğ‘‘, Ë†ğœ™
, Ë†ğ‘¦ğ‘‘
. On the other hand, by the
)
Ëœğ‘‹, Ëœğ‘Œ , Î¦, â„“, ğ‘“
ğ·

Ë†ğ‘¥ ğ‘‘, ğœ™âˆ—)

such that

ğ‘“
(
ğ‘† ğœ–

Ã•ğ‘‘=1

, Ë†ğ‘¦ğ‘‘

Ã
ğ·

1
ğ·

ğ·

ğ·

(

(

(

(

)

ğ‘“

)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

)
Ë†ğ‘¥ ğ‘‘, ğœ™âˆ—

, Ë†ğ‘¦ğ‘‘

)

) âˆ’

1
ğ·

Ë†ğ¿ğ‘‘

Ã•ğ‘‘=1

(8)

ğœ–

âˆ’

â‰¤

=

â‰¤

(9)

1
ğ·

1
ğ·

1
ğ·

Ã•ğ‘‘=1
ğ·

Ã•ğ‘‘=1
ğ·

Ã•ğ‘‘=1

Ë†ğ‘¥ ğ‘‘, ğœ™âˆ—

ğ‘“

â„“

(

(

Ë†ğ‘¥ ğ‘‘, ğœ™âˆ—

ğ‘“

â„“

(

(

Ë†ğ‘¥ ğ‘‘, ğœ™âˆ—

ğ‘“

â„“

(

(

)

)

)

, Ë†ğ‘¦ğ‘‘

) âˆ’

1
ğ·

, Ë†ğ‘¦ğ‘‘

) âˆ’

, Ë†ğ‘¦ğ‘‘

) âˆ’

1
ğ·

1
ğ·

ğ·
ğ‘‘=1 â„“
(
ğœ™â€², ğ¿â€²) âˆˆ
Ã
ğ¿â€²ğ‘‘ â‰¤

1
ğ·

Ã•ğ‘‘=1
ğ·

Ëœğ‘¥ ğ‘‘, Ë†ğœ™
)

ğ‘“

â„“

(

(

Ã•ğ‘‘=1
ğ·

â„“

Ã•ğ‘‘=1
, Ëœğ‘¦ğ‘‘

ğ‘“

(

(

)

ğ‘“

â„“

(

Ë†ğ‘¥ ğ‘‘, Ë†ğœ™
)

(

, Ë†ğ‘¦ğ‘‘

) +

ğœ–.

The rightmost inequality in (8) follows from the optimality of Ë†ğ¿ and (9) follows from
(cid:3)
Lipschitzness.

Note that since the objective is linear, the optimization problem in the previous Lemma
by its convex hull. Therefore the only missing

Ëœğ‘‹, Ëœğ‘Œ , Î¦, â„“, ğ‘“

is equivalent if we replace ğ‘† ğœ–
link is the following result.

(

Ã•ğ‘‘=1

)

12

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

Lemma 3.6. conv

Ëœğ‘‹, Ëœğ‘Œ, Î¦, â„“, ğ‘“

ğ‘† ğœ–

(

(

))

is the projection of a face of ğ‘ƒğ‘†ğœ–

ğ‘† ğœ–

Proof. The proof follows from simply ï¬xing variables in the corresponding LBO that de-
scribes conv
ğ‘‘, we simply need to make
))
components of ğ‘£ do not correspond to Ëœğ‘‹, Ëœğ‘Œ . We know this is well
ğœ†ğ‘£ = 0 whenever the
deï¬ned, since Ëœğ‘‹, Ëœğ‘Œ are already discretized, thus there must be some ğ‘£
ğ‘‘ corresponding
to them.

ğ·, Î¦, â„“, ğ‘“
ğ‘¥, ğ‘¦

. For every ğ‘‘

and ğ‘£

âˆˆ F

âˆˆ F

âˆˆ [

ğ·

)

(

(

(

]

The structure of the resulting polytope is the same as LBO, so the fact that it is exactly
follows
(cid:3)

conv
from the fact that the procedure simply ï¬xed some inequalities to be tight.

follows. The fact that it is a face of conv

Ëœğ‘‹, Ëœğ‘Œ , Î¦, â„“, ğ‘“

ğ·, Î¦, â„“, ğ‘“

ğ‘† ğœ–

ğ‘† ğœ–

))

))

(

(

(

(

1, 1

3.4. Data-dependent polytope? Before moving to the next section, we would like to dis-
cuss the importance of the data-independent feature of our construction. Constructing
a polytope for a speciï¬c data set is trivial: similarly to what we described in the previ-
ous sections, with the input data ï¬xed we can simply enumerate over a discretization of
Î¦
ğ‘ , and thus compute the (approximately) optimal solution in advance. A
data-dependent polytope would simply be a single vector, corresponding to the approxi-
mately optimal solution computed in the enumeration. The time needed to generate such
polytope is ğ‘‚
ğœ–
2
(the number of possible discretized conï¬gurations) via at most
((
L/
ğ‘ ğ·
ğ‘‚
evaluations of â„“ and ğ‘“ (one per each enumerated conï¬guration and data-
)
((
point).

ğœ–
L/

âŠ† [âˆ’

2

ğ‘

)

)

]

)

This result is not particularly insightful, as it is based on a straight-forward enumeration
which takes a signiï¬cant amount of time, considering that it only serves one data set. On the
other hand, our result shows that by including the input data as a variable, we do not induce
an exponential term in the size of the data set ğ· and we can keep the number function
evaluations to be roughly the same.

4. Encoding results for feed-forward neural networks

We now proceed to deriving explicit results for speciï¬c architectures. This amounts to

using Theorem 3.1 with explicit computations of the architecture Lipchitz constant

.

L

â—¦

â—¦

ğœ

ğ‘‡2

4.1. Fully-connected layers with ReLU activations and normalized coeï¬ƒcients. We
consider a Deep Neural Network ğ‘“

: â„ğ‘›
ğ‘‡1, where ğœ is the ReLU activation function ğœ

â„ğ‘š with ğ‘˜ layers given by ğ‘“ = ğ‘‡ğ‘˜
ğœ
â—¦
â†’
ğ‘¥
applied
Â· Â· Â· â—¦
)
(
component-wise and each ğ‘‡ğ‘– : â„ğ‘¤ğ‘–
â„ğ‘¤ğ‘– is an aï¬ƒne linear function. Here ğ‘¤0 = ğ‘›
(ğ‘¤ ğ‘˜ = ğ‘š) is the input (output) dimension of the network. We write ğ‘‡ğ‘–
ğ‘ğ‘– and
)
1 via normalization. Thus, if ğ‘£ is a node in layer ğ‘–, the
assume
kâˆ â‰¤
Ë†ğ‘, where Ë†ğ‘ is a row of ğ´ğ‘– and Ë†ğ‘
node computation performed in ğ‘£ is of the form Ë†ğ‘ğ‘‡ ğ‘§
is a component of ğ‘ğ‘–. Note that in this case the parameter space dimension is exactly the
number of edges of the network. Hence, we use ğ‘ to represent the number of edges. We
begin with a short technical Lemma, with which we can immediately establish the following
corollary.

{
}
= ğ´ğ‘– ğ‘§

(cid:17) max

kâˆ â‰¤

0, ğ‘¥

â†’

ğ´ğ‘–

ğ‘ğ‘–

1,

1
âˆ’

+

+

â—¦

k

k

ğ‘§

(

Lemma 4.1. For every ğ‘–
ğ‘ˆğ‘–

1.
+

ğ‘˜

1

]

âˆ’

âˆˆ [

0 deï¬ne ğ‘ˆğ‘– =

ğ‘–
ğ‘—=0 ğ‘¤ ğ‘— . If

ğ‘§

k

kâˆ â‰¤

ğ‘ˆğ‘– then

ğ‘‡ğ‘–

k

1
+

(

ğ‘§

) kâˆ â‰¤

Ã

Proof. The result can be veriï¬ed directly, since for ğ‘
ğ‘§ğ‘‡ ğ‘

ğ‘¤

1.

ğ‘

ğ‘§

|

+

| â‰¤

k

kâˆ +

1, 1

]

âˆˆ [âˆ’

ğ‘¤ and ğ‘

âˆˆ [âˆ’

1, 1

it holds
(cid:3)

]

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

13

Corollary 4.2. If Î¦ is the class of Neural Networks with ğ‘˜ layers, ğ‘ edges, ReLU activations,
can be formulated via a polytope of
and normalized coeï¬ƒcients, then conv
ğ‘¤ğ‘‚
â„“
size ğ‘‚
is the Lipschitz
â„“
)
Lâˆ(
((
ğ‘š. The polytope can be constructed in time
constant of â„“
,
Â·)
(Â·
ğ‘˜2
ğ‘¤ğ‘‚
ğ‘‚
â„“
(
)
)
evaluations of â„“ and ğ‘“ .

ğœ–
) /
(
+
)
ğ‘ˆğ‘˜ , ğ‘ˆğ‘˜
over
[âˆ’
ğ‘ ğ·
ğ‘š
ğ‘›
ğœ–
+
) /

ğ·, Î¦, â„“, ğ‘“
))
, where ğ‘¤ = maxğ‘–
âˆˆ[

]
plus the time required for ğ‘‚

0 ğ‘¤ğ‘– and

ğ‘ ğ·
)
ğ‘š

ğœ–
) /

Ã— [âˆ’

Lâˆ(

Lâˆ(

Lâˆ(

ğ‘¤ğ‘‚

1, 1

ğ‘† ğœ–

((

((

ğ‘˜2

ğ‘˜2

2

2

2

ğ‘š

ğ‘š

â„“

ğ‘

âˆ’

(

(

)

)

]

)

)

)

ğ‘›

ğ‘›

+

+

+

+

1

ğ‘˜

(

]

ğ‘§, ğ‘, ğ‘

= ğ‘§ğ‘‡ ğ‘

Proof. Proving that the architecture Lipschitz constant is ğ¿
) suï¬ƒces. All node
computations take the form â„
; the only
+
diï¬€erence is made in the domain of ğ‘§, which varies from layer to layer. The 1-norm of the
gradient of â„ is at most
1 which, in virtue of Lemma 4.1, implies
1
+
that a node computation on layer ğ‘– (with the weights considered variables) has Lipschitz
ğ›¾
constant at most
ğ‘ˆğ‘–, ğ‘ˆğ‘–
and ğ‘§

k
k
ğ‘—=0 2ğ‘¤ ğ‘— = 2 ğ‘¤ ğ‘–
, it holds that

=: Ëœğ‘¤ğ‘–. On the other hand, for

ğ‘â€², ğ‘â€²) kâˆ â‰¤

âˆ (
1, 1
]

ğ‘ for ğ‘

âˆˆ [âˆ’

âˆˆ [âˆ’

) âˆ’ (

ğ‘, ğ‘

1, 1

â‰¤ k

+ k

+
ğ‘¤

k(

ğ‘¤

+

+

ğ‘

1

âˆ’
1

k

k

ğ‘§

ğ‘§

âˆ’

)

(

]

1

1

1

ğ‘–

1

ğ‘˜2
ğ‘¤ğ‘‚
â„“
(
)
ğ‘¤ and ğ‘

âˆˆ [âˆ’

]
Ã
â„

|

ğ‘§, ğ‘, ğ‘

â„

(

) âˆ’

(

ğ‘§â€², ğ‘â€², ğ‘â€²

)| â‰¤

Ëœğ‘¤ğ‘–
ğ‘§
âˆ’
k(
Ëœğ‘¤ğ‘– max

ğ‘§

ğ‘§â€², ğ‘

ğ‘â€², ğ‘

âˆ’
ğ‘§â€²

, ğ›¾

â‰¤

{k

âˆ’

kâˆ

}

ğ‘â€²

âˆ’

) kâˆ

which shows that the Lipschitz constants can be multiplied layer-by-layer to obtain the
overall architecture Lipschitz constant. Since ReLUs have Lipschitz constant equal to 1,
ğ‘˜
ğ‘–=1 Ëœğ‘¤ğ‘– =
and
2, we conclude the architecture
(cid:3)
Lipschitz constant is ğ¿
(cid:0)

) , whenever ğ‘¤

= ğ‘¤ğ‘‚
ğ‘˜2
) .

2 ğ‘¤ ğ‘–
+
ğ‘¤
â„“

ğ‘˜
ğ‘–=1

â‰¥

ğ‘˜2

1

(

(

1

Ã

Ã

âˆ’
1
âˆ’
ğ‘¤ğ‘‚
(cid:1)
)

âˆ (

To evaluate the quality of the polytope size in the previous lemma, we compare with the

following related algorithmic result.

Theorem 4.3. [4, Theorem 4.1] Let Î¦ be the class of Neural Networks with 1 hid-
den layer (ğ‘˜ = 2), convex loss function â„“, ReLU activations and output dimension
ğ‘š = 1. There exists an algorithm to ï¬nd a global optimum of the ERM problem in
time ğ‘‚

2ğ‘¤ ğ· ğ‘›ğ‘¤ poly

ğ·, ğ‘›, ğ‘¤

.

(

(

))

In the same setting, our result provides a polytope of size

(10)

ğ‘‚

2

((

Lâˆ(

â„“

)

ğ‘¤ğ‘‚

1
)

(

ğœ–
/

)

ğ‘›

1
) (
+

(

ğ‘¤

1
) ğ·
+

)

Remark 4.4. We point out a few key diï¬€erences of these two results: (a) One advantage
of our result is the benign dependency on ğ·. Solving the training problem using an LP
with our polyhedral encoding has polynomial dependency on the data-size regardless of the
architecture. Moreover, our approach is able to construct a polytope that would work for
any sample. (b) The exponent in (10) is
ğ‘›ğ‘¤, which is also present in Theorem 4.3. The
key diï¬€erence is that we are able to swap the base of that exponential term for an expression
that does not depend on ğ·. (c) We are able to handle any output dimension ğ‘š and any
number of layers ğ‘˜. (d) We do not assume convexity of the loss function â„“, which causes
the resulting polytope size to depend on how well behaved â„“ is in terms of its Lipschitzness.
(e) The result of [4] has two advantages over our result: there is no boundedness assumption
on the coeï¬ƒcients, and they are able to provide a globally optimal solution.

âˆ¼

4.2. ResNets, CNNs, and alternative activations. Corollary 4.2 can be generalized to
handle other architectures as well, as the key features we used before are the acyclic
structure of the network and the Lipschitz constant of the ReLU function.

Lemma 4.5. Let Î¦ be the class of feed-forward neural networks with ğ‘˜ layers, ğ‘ edges,
aï¬ƒne node computations, 1-Lipschitz activation functions ğ‘ğ‘– : â„
= 0,

â„ such that ğ‘ğ‘–

0

â†’

(

)

14

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

2

and normalized coeï¬ƒcients. Then conv
tope of size ğ‘‚
and
tope can be constructed in time ğ‘‚
ğ‘˜2
ğ‘‚

ğ‘† ğœ–
can be formulated via a poly-
(
(
ğ‘ ğ·
Î”ğ‘‚
, where Î” is the maximum vertex in-degree
(
)
ğ‘š. The poly-
over
,
is the Lipschitz constant of â„“
[âˆ’
(Â·
Â·)
ğ‘˜2
ğ‘›
Î”ğ‘‚
plus the time required for
ğœ–
â„“
+
) /
)
)
evaluations of â„“ and ğ‘“ .

ğ‘ˆğ‘˜, ğ‘ˆğ‘˜
ğ‘ ğ·
ğ‘š

ğ·, Î¦, â„“, ğ‘“

ğœ–
) /

Ã— [âˆ’

Lâˆ(

Lâˆ(

Lâˆ(

1, 1

Î”ğ‘‚

]
)

((

((

))

ğ‘˜2

2

ğ‘š

ğ‘š

ğ‘š

â„“

â„“

â„“

ğ‘

]

)

)

)

ğ‘›

ğ‘›

+

+

+

(

2

((

Lâˆ(

)

(

ğœ–
) /

)

+

+

)

Proof. The proof follows almost directly from the proof of Corollary 4.2. The two main
diï¬€erences are (1) the input dimension of a node computation, which can be at most Î”
instead of ğ‘¤ and (2) the fact that an activation function ğ‘ with Lipchitz constant 1 and
that ğ‘
, thus the domain of each node computation computed
in Lemma 4.1 applies. The layer-by-layer argument can be applied as the network is
(cid:3)
feed-forward.

= 0 satisï¬es

)| â‰¤ |

0

ğ‘

ğ‘§

ğ‘§

)

(

(

|

|

Corollary 4.6. The ERM problem (1) over Deep Residual Networks (ResNets) with 1-
whenever the
Lipschitz activations can be solved to ğœ–-optimality in time poly
network size and number of layers are ï¬xed.

ğœ–, ğ·
/

Î”, 1

)

(

Another interesting point can be made with respect to Convolutional Neural Networks
(CNN). In these, convolutional layers are included to signiï¬cantly reduce the number of
parameters involved. From a theoretical perspective, a CNN can be obtained by enforcing
certain parameters of a fully-connected DNN to be equal. This implies that Lemma 4.5
can also be applied to CNNs, with the key diï¬€erence residing in parameter ğ‘, which is
the dimension of the parameter space and does not correspond to the number of edges in a
CNN.

4.3. Explicit Lipschitz constants of common loss functions. In the previous section
we speciï¬ed our results â€”the size of the data-independent polytopeâ€” for feed-forward
â„“
;
networks with 1-Lipschitz activation functions. However, we kept as a parameter
)
ğ‘˜
ğ‘—=0 ğ‘¤ ğ‘— a valid
the Lipschitz constant of â„“
bound on the output of the node computations, as proved in Lemma 4.1. Note that
ğ‘ˆğ‘˜

â‰¤
In this section we compute this Lipschitz constant for various common loss functions. It
is important to mention that we are interested in the Lipschitznes of â„“ with respect to both
the output layer and the data-dependent variables as well â€”not a usual consideration in the
literature. These computations lead to the results reported in Table 1.

ğ‘š, with ğ‘ˆğ‘˜ =

ğ‘ˆğ‘˜, ğ‘ˆğ‘˜

Ã— [âˆ’

Lâˆ(

over

1, 1

1.
+

ğ‘¤ ğ‘˜

[âˆ’

Ã

(Â·

Â·)

ğ‘š

]

]

,

Recall that a bound on the Lipschitz constant

â„“

is given by supğ‘§,ğ‘¦ kâˆ‡
2
2. In this case it is easy to see that

Lâˆ(

)

ğ‘§, ğ‘¦

â„“

(

) k

1.

Quadratic Loss â„“

(

â„“

ğ‘§, ğ‘¦

ğ‘§, ğ‘¦

=

ğ‘§

k
ğ‘§

)
1 = 4
k
=

ğ‘¦

k

1

âˆ’
ğ‘¦

4ğ‘š

ğ‘ˆğ‘˜

1

4ğ‘š

ğ‘¤ ğ‘˜

1
+

1

(

kâˆ‡

) k
ğ‘§, ğ‘¦

k
ğ‘¦
Absolute Loss â„“
k
Lipschitz constant with respect to the inï¬nity norm is at most 2ğ‘š.
Cross Entropy Loss with Soft-max Layer. In this case we include the Soft-max
computation in the deï¬nition of â„“, therefore

In this case we can directly verify that the

â‰¤
1.

) â‰¤

âˆ’
ğ‘§

âˆ’

+

+

k

(

(

(

)

)

â€¢

â€¢

â€¢

where ğ‘†

ğ‘§

)

(

is the Soft-max function deï¬ned as

=

ğ‘§, ğ‘¦

â„“

(

)

âˆ’

ğ‘¦ğ‘– log

ğ‘†

ğ‘§

(

ğ‘–

)

)

(

ğ‘š

Ã•ğ‘–=1

ğ‘– =

ğ‘†

ğ‘§

)

(

ğ‘’ğ‘§ğ‘–
ğ‘š
ğ‘—=1 ğ‘’ğ‘§ ğ‘—

.

Ã

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

15

A folklore result is
ğœ•â„“

ğ‘§, ğ‘¦
(
ğœ•ğ‘§ğ‘–

Additionally,

= ğ‘†

)

ğ‘§

ğ‘–

)

(

âˆ’

ğ‘¦ğ‘–

â‡’

ğœ•â„“

ğ‘§, ğ‘¦
(
ğœ•ğ‘§ğ‘–

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ğ‘†

2.

â‰¤

)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğœ•â„“

ğ‘§, ğ‘¦

(
ğœ• ğ‘¦ğ‘–

=

)

log

ğ‘§

ğ‘–

(
which in principle cannot be bounded. Nonetheless, since we are interested in the
domain

of ğ‘§, we obtain

âˆ’

ğ‘ˆğ‘˜, ğ‘ˆğ‘˜

)

)

(

[âˆ’

]

ğ‘– =

ğ‘†

ğ‘§

)

(

ğ‘’ğ‘§ğ‘–
ğ‘š
ğ‘—=1 ğ‘’ğ‘§ ğ‘— â‰¥

1
ğ‘š

2ğ‘ˆğ‘˜

ğ‘’âˆ’

â‡’

ğœ•â„“

ğ‘§, ğ‘¦

=

)

(
ğœ• ğ‘¦ğ‘–
(cid:12)
(cid:12)
â„“
(cid:12)
) â‰¤
(cid:12)
= max
{

Ã
âˆ’
2ğ‘š
1

log

ğ‘†

ğ‘§

(
(
ğ‘š
log
(
ğ‘§ğ‘‡ ğ‘¥, 0

(cid:12)
(cid:12)
ğ‘š
which implies that
(cid:12)
) +
(cid:12)
Hinge Loss â„“
ğ‘§, ğ‘¦
. Using a similar argument as for the
}
Quadratic Loss, one can easily see that the Lipschitz constant with respect to the
ğ‘¤ ğ‘˜
inï¬nity norm is at most ğ‘š

Lâˆ(
)

2ğ‘¤ ğ‘˜

2ğ‘ˆğ‘˜

(
âˆ’

) â‰¤

log

) +

1
+

ğ‘š

1

1

)

(

(

(

.

.

ğ‘ˆğ‘˜

1
+

â€¢

ğ‘–

)

) â‰¤

log

ğ‘š

(

2ğ‘ˆğ‘˜

) +
2ğ‘š

(

) â‰¤
5. ERM under Network Structure

+

+

)

(

So far we have considered general ERM problems exploiting only the structure of the
ERM induced by the ï¬nite sum formulations. We will now study ERM under Network
Structure, i.e., speciï¬cally ERM problems as they arise in the context of Neural Network
training. We will see that in the case of Neural Networks, we can exploit the sparsity of the
network itself to obtain better polyhedral formulations of conv

ğ·, Î¦, â„“, ğ‘“

ğ‘† ğœ–

.

Suppose the network is deï¬ned by a graph

( G).
G
By using additional auxiliary variables ğ‘  representing the node computations and activations,
we can describe ğ‘†

, and recall that in this case, Î¦

1, 1

]

ğ¸

))
âŠ† [âˆ’

(

(

ğ·, Î¦, â„“, ğ‘“
)
=

ğ·, Î¦, â„“, ğ‘“

(

in the following way:
ğ‘¥1, . . . , ğ‘¥ ğ·, ğ‘¦1, . . . , ğ‘¦ğ·, ğœ™, ğ¿

ğ‘†

(

)

:

)

(
ğ¿ğ‘‘ = â„“
(cid:8)
(
ğ‘ ğ‘–,ğ‘‘
ğ‘£ = ğ‘ ğ‘£
ğ‘ 0,ğ‘‘ = ğ‘¥ ğ‘‘
ğ‘¥ğ‘–

ğ‘ ğ‘˜,ğ‘‘, ğ‘¦ğ‘‘
ğ‘ ğ‘–

ğ‘”ğ‘£

)
âˆ’

(

(

1, 1

ğ‘›, ğ‘¦ğ‘–

1,ğ‘‘ , ğœ™

ğ›¿+

ğ‘£

ğ‘£
))) âˆ€

(

(

âˆˆ

ğ‘‰ğ‘–, ğ‘–

ğ‘˜

]

âˆˆ [

âˆˆ [âˆ’
The only diï¬€erence with our original description of ğ‘†
in (6) is that we explicitly
(cid:9)
â€œstoreâ€ node computations in variables ğ‘ . These new variables will allow us to better use
the structure of

âˆˆ [âˆ’

]

)

(

.

Î¦

.

1, 1

ğ‘š, ğœ™
]
âˆˆ
ğ·, Î¦, â„“, ğ‘“

G

Assumption 5.1. To apply our approach in this context we need to further assume Î¦ to be
the class of Neural Networks with normalized coeï¬ƒcients and bounded node computations.
This means that we restrict to the case when ğ‘ 

ğ·.

1, 1

ğ‘‰

âˆˆ [âˆ’

( G) |

] |

Under Assumption 5.1 we can easily derive an analog description of ğ‘† ğœ–

ğ·, Î¦, â„“, ğ‘“
)
using this node-based representation of ğ‘† ğœ–
. In such description we also include
a binary representation of the auxiliary variables ğ‘ . Let Î“ be the intersection graph of
and Î“ğœ™ be the sub-graph of Î“ induced by variables ğœ™.
such a formulation of ğ‘† ğœ–
)
of Î“ğœ™ we can construct a tree-decomposition of Î“ the
Using a tree-decomposition
following way:

ğ·, Î¦, â„“, ğ‘“
ğ‘‡, ğ‘„

ğ·, Î¦, â„“, ğ‘“

(

(

)

(

(

)

16

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

(1) We duplicate the decomposition ğ· times

ğ‘‡ ğ‘–, ğ‘„ğ‘–

(

)

ğ·
ğ‘–=1, where each

ğ‘‡ ğ‘–, ğ‘„ğ‘–

(

)

is a copy

(2) We connect the trees ğ‘‡ ğ‘– in a way that the resulting graph is a tree (e.g., they can be

of

ğ‘‡, ğ‘„

.

(

)

simply concatenated one after the other).

(3) To each bag ğ‘„ğ‘–

ğ‘‡ ğ‘– and ğ‘–

ğ‘¡ with ğ‘¡

âˆˆ [
ğ¿ğ‘‘ and the binary variables associated with the discretization of ğ‘¥ ğ‘‘, ğ‘ Â·
This adds ğ‘ğ›¾
(G)| +
variable ğ‘  per data point per vertex of

, we add all the data-dependent variables
,ğ‘‘, and ğ‘¦ğ‘‘.
additional variables to each bag, as there is only one

ğ‘š

ğ·

ğ‘‰

(|

+

âˆˆ

ğ‘›

)

]

.

It is not hard to see that this is a valid tree-decomposition of Î“, of size

ğ· â€”since

the bags were duplicated ğ· timesâ€” and width ğ‘ğ›¾

ğ‘¡ğ‘¤

Î“ğœ™

ğ‘‰

ğ‘›

We now turn to providing a bound to ğ‘¡ğ‘¤

(

(G)| +
. To this end we observe the following:

) + |

+

(

ğ‘‡
|
ğ‘š

| Â·
.
)

G

Î“ğœ™

(

)

(1) The architecture variables ğœ™ are associated to edges of

. Moreover, two variables
ğ¸ appear in a common constraint if and only if there is a vertex
âˆˆ
ğ›¿+(

G

ğ‘£

.
)
(2) This implies that Î“ğœ™ is a sub-graph of the line graph of

âˆˆ

ğœ™ğ‘’, ğœ™ ğ‘“ , with ğ‘’, ğ‘“
ğ‘£ such that ğ‘’, ğ‘“

G

. Recall that the line graph
and connecting two

of a graph
nodes whenever the respective edges share a common endpoint.

is obtained by creating a node for each edge of

G

G

The treewidth of a line graph is related to the treewidth of the base graph (see [9, 14, 5,
where Î” denotes the maximum vertex
Î”
, since Î“ğœ™ has at most

23]). More speciï¬cally, ğ‘¡ğ‘¤
(
degree. Additionally, using Lemma 2.5 we may assume
ğ¸

nodes. Putting everything together we obtain:

(G))

(G)|

| â‰¤ |

(G)

Î“ğœ™

) âˆˆ

ğ‘¡ğ‘¤

ğ‘‚

ğ¸

ğ‘‡

(

|

(G)|

|
Lemma 5.2. If there is an underlying network structure
ğ·, Î¦, â„“, ğ‘“
node computations are bounded, then conv
with no more than

ğ‘† ğœ–

(

(

in the ERM problem and the
is the projection of a polytope

G
))

2ğ·

ğ¸

(|

(G)| +

variables and no more than

ğ‘¡ ğ‘¤

ğ‘‚

(

( G)

Î”

( G)+|

ğ‘‰

( G) |+

ğ‘›

+

ğ‘š
)

2
L
ğœ–

(cid:19)

1

)

(cid:18)

ğ‘¡ ğ‘¤

ğ‘‚

(

( G)

Î”

( G)+|

ğ‘‰

( G) |+

ğ‘›

+

ğ‘š
)

ğ·

ğ¸

1

2

2
L
ğœ–

(|

)  
constraints. Moreover, given a tree-decomposition of the network
constructed in time

(G)| +

(cid:19)

(cid:18)

ğ‘‚

ğ·

(cid:16)
plus the time required for

ğ¸

|

(G)| (

2

ğœ–
L/

)

ğ‘¡ ğ‘¤

ğ‘‚

(

( G)

Î”

( G)+|

ğ‘‰

( G) |+

ğ‘›

+

ğ‘š
)

1

!

+

, the polytope can be

G

(cid:17)

ğ‘‚

evaluations of â„“ and ğ‘“ .

(G)| (

2

ğœ–
L/

)

ğ¸

|

(cid:16)

ğ‘¡ ğ‘¤

ğ‘‚

(

( G)

Î”

( G)+|

ğ‘‰

( G) |+

ğ‘›

+

ğ‘š
)

(cid:17)

6. Linear Programming-based Training Generalizes

In this section we show that the ERM solutions obtained via LP generalize to the General
Risk Minimization problem. Here we show generalization as customary in stochastic
optimization, exploiting the Lipschitzness of the model to be trained; we refer the interested
reader to [32] and [1] for an in-depth discussion.

Recall that the General Risk Minimization (GRM) is deï¬ned as minğœ™

Î¦ ğ”¼

minğœ™

âˆˆ

ğ‘¥,ğ‘¦

(

) âˆˆD [

ğ‘“

â„“

(

(

ğ‘¥, ğœ™

, ğ‘¦

)

)]

, where â„“ is some loss function,

ğ‘“

(cid:17)
Î¦ GRM
is a neural network

ğœ™

)

(

âˆˆ

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

17

architecture with parameter space Î¦, and
. We solve the ERM problem minğœ™

ğ‘š drawn from the distribution
) âˆˆ
ğ·
, ğ‘¦ğ‘–
ğ‘¥ğ‘–, ğœ™
,
ğ‘–=1 â„“
ğœ™
)
)
(
)
(
D
ğ·
of size ğ·.
instead, where
ğ‘–=1 is an i.i.d. sample from data distribution
We show in this section, for any 1 > ğ›¼ > 0, ğœ– > 0, we can choose a (reasonably small!)
sample size ğ·, so that with probability 1

â„ğ‘›
+
(cid:17) minğœ™

(
Î¦ ERMğ‘‹ ,ğ‘Œ

ğ›¼ it holds:

ğ‘¥ğ‘–, ğ‘¦ğ‘–

(
D

ğ‘‹, ğ‘Œ

ğ‘¥, ğ‘¦

1
ğ·

Ã

=

Î¦

)

(

)

(

âˆˆ

âˆˆ

ğ‘“

GRM

Â¯ğœ™

(

) â‰¤

GRM

ğœ™

(

) +

6ğœ–,

âˆ’
min
Î¦
ğœ™
âˆˆ

â‰¤

Î¦ ERMğ‘‹ ,ğ‘Œ

maxğœ™
âˆˆ
ğ‘‹, ğ‘Œ

where Â¯ğœ™
ğœ– is an ğœ–-approximate solution to ERMğ‘‹ ,ğ‘Œ for i.i.d.-
. As the size of the polytope that we use for training only linearly
sampled data
depends on ğ·, this also implies that we will have a linear program of reasonable size as a
function of ğ›¼ and ğœ–.

) âˆ¼ D

) +

ğœ™

(

(

The following proposition summaries the generalization argument used in stochastic pro-
.

gramming as presented in [1] (see also [32]). Let ğœ2 = maxğœ™

Î¦ ğ•

ğ‘¥, ğœ™

, ğ‘¦

â„“

ğ‘“

ğ‘¥,ğ‘¦

âˆˆ

(

) âˆˆD [

(

(

)

)]

Proposition 6.1. Consider the optimization problem

ğ”¼ğœ”

min
ğ‘‹
ğ‘¥
âˆˆ

Î©

ğ¹

ğ‘¥, ğ›¾

ğœ”

,

[
is a random parameter with ğœ”
Î©

(
Î© a set of parameters, ğ‘‹
âˆˆ
â„ is a function. Given i.i.d. samples ğ›¾1, . . . , ğ›¾ğ· of ğ›¾

))]

(

âˆˆ

ğœ”
where ğ›¾
(
and ğ¹ : ğ‘‹
â†’
ï¬nite sum problem

)
Ã—

â„ğ‘› a ï¬nite set,
, consider the

)

âŠ†
ğœ”
(

1
ğ·

min
ğ‘‹
ğ‘¥
âˆˆ

ğ·
Ã•ğ‘–
âˆˆ[

]

ğ‘¥, ğ›¾ğ‘–

ğ¹

(

.

)

ğ‘‹ is an ğœ–-approximate solution to (11), i.e., 1
ğ·

ğ‘–

ğ·

]

âˆˆ[

Â¯ğ‘¥, ğ›¾ğ‘–

ğ¹

(

) â‰¤

minğ‘¥

1
ğ·

ğ‘‹

âˆˆ

ğ‘–

ğ·

]

âˆˆ[

ğ‘¥, ğ›¾ğ‘–

ğ¹

(

)+

(11)

If Â¯ğ‘¥
âˆˆ
ğœ– and

where ğ›¼ > 0 and ğœ2 = maxğ‘¥

ğ”¼ğœ”

Î©

âˆˆ

[

ğ¹

(

ğ‘‹ ğ• ğœ”

âˆˆ
Â¯ğ‘¥, ğ›¾

âˆˆ
ğœ”

(

))] â‰¤

ğ·

Î©

â‰¥

ğ¹

[

4ğœ2
ğœ– 2

ğ‘¥, ğ›¾

(
min
ğ‘‹
ğ‘¥
âˆˆ

Ã
ğ‘‹
|
ğ›¼

,

log |

ğœ”
(
ğ”¼ğœ”

))]
Î©

âˆˆ

, then with probability 1

ğ‘¥, ğ›¾

ğ¹

[

(

ğœ”

(

))] +

2ğœ–.

Ã

ğ›¼ it holds:

âˆ’

We now establish generalization by means of Proposition 6.1 and a straightforward
â„•.
âˆˆ
Î¦ğœˆ with
the be architecture Lipschitz constant, as deï¬ned in (2)

discretization argument. By assumption from above Î¦
ğ‘ be a ğœˆ-net of Î¦, i.e., for all ğœ™
Let Î¦ğœˆ
Â¯ğœ™
ğœ™

1, 1
]
âŠ† [âˆ’
ğœˆ. Furthermore let

]
Î¦ there exists Â¯ğœ™

ğ‘ for some ğ‘

âŠ† [âˆ’
âˆˆ

1, 1

Î¦

âˆˆ

âŠ†
kâˆ â‰¤

k
âˆ’
(or (15)).

L

Theorem 6.2. [Generalization] Let Â¯ğœ™
with ğœ– > 0, i.e., ERMğ‘‹ ,ğ‘Œ
minğœ™

Â¯ğœ™
) â‰¤

(

and ğœ2 as above, then with probability 1
âˆ’
L
i.e., Â¯ğœ™ is a 6ğœ–-approximate solution to minğœ™
âˆˆ

Î¦ be an ğœ–-approximate solution to minğœ™
âˆˆ
2
ğœ–. If ğ·
Î¦ ERMğ‘‹ ,ğ‘Œ
â‰¥
Â¯ğœ™
) â‰¤

ğœ™
) +
ğ›¼ it holds GRM
Î¦ GRM

4ğœ2
ğœ– 2
minğœ™

âˆˆ
L)/
ğ›¼
Î¦ GRM

log ( (

ğœ™

(

(

âˆˆ

âˆˆ

.

ğ‘

Î¦ ERMğ‘‹ ,ğ‘Œ
ğœ–
, with
6ğœ–,

ğœ™

)

(

) +

ğœ™

)

(

(
Â¯ğœ™ be as above. With the choice ğœˆ (cid:17) ğœ–
ğœˆ and hence by Lipschitzness,

)

Proof. Let
Â¯ğœ™
Ëœğœ™

k

âˆ’

kâˆ â‰¤

so that ERMğ‘‹ ,ğ‘Œ
probability 1

Ëœğœ™
) â‰¤

(

âˆˆ
ğ›¼ we have GRM

âˆ’

|
minğœ™

ERMğ‘‹ ,ğ‘Œ

Â¯ğœ™
(
) âˆ’
Î¦ğœˆ ERMğ‘‹ ,ğ‘Œ
(
minğœ™

Ëœğœ™
) â‰¤

(

âˆˆ

ERMğ‘‹ ,ğ‘Œ

Ëœğœ™
)| â‰¤

(

ğœ–,

ğœ™
) +
Î¦ğœˆ GRM

2ğœ–. As ğ·
ğœ™

log ( (

, with
4ğœ– by Proposition 6.1. If now

L)/
ğ›¼

â‰¥

)

4ğœ2
ğœ– 2

2

ğ‘

ğœ–

(

) +

, there exists Ëœğœ™

Î¦ğœˆ, so that

âˆˆ

/L

18

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

Î¦ğœˆ with

Â¯ğœ™ğº

k

âˆ’

Ëœğœ™ğº

kâˆ â‰¤

ğœˆ, by Lipschitzness we have

Â¯ğœ™ğº = argminğœ™
GRM

Â¯ğœ™ğº

ğœ™
Î¦ GRM
(
âˆˆ
Ëœğœ™ğº
GRM

and Ëœğœ™ğº

âˆˆ
ğœ–. Now

|

(

) âˆ’

(
Ëœğœ™

GRM

(

) â‰¤

)
)| â‰¤
min
Î¦ğœˆ
ğœ™
âˆˆ
GRM

GRM

â‰¤

GRM

ğœ™

4ğœ–

(

) +

) +
4ğœ–

5ğœ–

(

Ëœğœ™ğº
Â¯ğœ™ğº

(by optimality)

(by Lipschitzness).

ğœˆ it follows

Together with

|

GRM

â‰¤
Â¯ğœ™
) âˆ’
Â¯ğœ™

(
GRM

(
GRM

) +
Ëœğœ™
)| â‰¤

(
GRM

(

) â‰¤

ğœ– as

Ëœğœ™

Â¯ğœ™

Â¯ğœ™ğº

(

) +

kâˆ â‰¤

k
âˆ’
6ğœ– = min
Î¦
ğœ™
âˆˆ

GRM

ğœ™

(

) +

6ğœ–,

which completes the proof.

(cid:3)

We are ready to formulate the following corollary combining Theorem 6.2 and Main

Theorem 3.1.

Corollary 6.3 (LP-based Training for General Risk Minimization). Let
be a data
distribution as above. Further, let 1 > ğ›¼ > 0 and ğœ– > 0, then there exists a linear
program with the following properties:

D

(a) The LP has size

ğ‘‚

and can be constructed in time

2

ğœ–
L/

)

(

ğ‘›

+

ğ‘š

+

ğ‘

(cid:16)

4ğœ2
ğœ– 2

(cid:18)

log (

ğ‘

2

ğœ–
L/
ğ›¼

)

(cid:19) (cid:17)

2

ğœ–
L/

)

(

ğ‘›

+

ğ‘š

+

ğ‘

ğ‘‚

(cid:16)

4ğœ2
ğœ– 2

(cid:18)
ğ‘›

ğ‘

ğ‘š

+

+

2

log ((

ğ‘

)

ğœ–
L)/
ğ›¼

(cid:19) (cid:17)
evaluations of â„“ and ğ‘“ , where

plus the time required for ğ‘‚
as above.

2

ğœ–
L/

)

(

(b) With probability

1

(

âˆ’

ğ›¼

)

(cid:0)

it holds GRM

(cid:1)
Â¯ğœ™
) â‰¤

(

minğœ™

Î¦ GRM

ğœ™

âˆˆ

(

) +

optimal solution to the linear program obtained for the respective sample of
4ğœ2
ğœ– 2

log ( (

L)/
ğ›¼

2

ğ‘

.

ğœ–

)

D

and ğœ2

L
6ğœ–, where Â¯ğœ™ is an
of size

Similar corollaries hold, combining Theorem 6.2 with the respective alternative state-
ments from Section 4. Of particular interest for what follows is the polytope size in the
case of a neural network with ğ‘˜ layers with width ğ‘¤, which becomes

(12)

ğ‘‚

2

â„“

)

Lâˆ(

ğ‘˜2

ğ‘¤ğ‘‚

(

)

ğœ–
/

ğ‘›

+

ğ‘š

+

ğ‘

4ğœ2

ğœ– 2
/

log

2

((

Lâˆ(

â„“

)

ğ‘˜2

ğ‘¤ğ‘‚

(

ğ‘

)

ğœ–
/

)

.

ğ›¼

)

/

(cid:0)(cid:0)

A closely related result regarding an approximation to the GRM problem for neural
networks is provided by [20] in the improper learning setting. The following corollary to
[20] (Corollary 4.5) can be directly obtained, rephrased to match our notation:

(cid:1)

(cid:1)

(cid:1)

(cid:0)

âˆ’

Theorem 6.4 ([20]). There exists an algorithm that outputs Ëœğœ™ such that with probability
and loss function â„“ which is convex, ğ¿-Lipschitz in the ï¬rst
ğ›¼, for any distribution
1
D
ğœ–, where Î¦ is
argument and ğ‘ bounded on
, GRM
minğœ™
the class of neural networks with ğ‘˜ hidden layers, width ğ‘¤, output dimension ğ‘š = 1, ReLU
activations and normalized weights. The algorithm runs in time at most

2âˆšğ‘¤, âˆšğ‘¤

Î¦ GRM

) â‰¤

) +

[âˆ’

Ëœğœ™

ğœ™

(

]

(

âˆˆ

(13)

ğ‘›ğ‘‚

1
) 2( (

(

ğ¿

1
)
+

ğ‘¤ ğ‘˜

1
2 ğ‘˜ ğœ– âˆ’
/

ğ‘˜

)

log

ğ›¼

1

(

/

)

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

19

Remark 6.5. In contrast to the result of [20], we consider the proper learning setting,
In addition we point out key diï¬€erences
where we actually obtain a neural network.
between Theorem 6.4 and the algorithmic version of our result when solving the LP in
Corollary 6.3 of size as (12): (a) In (13), the dependency on ğ‘› is better than in (12). (b) The
dependency on the Lipschitz constant is signiï¬cantly better in (12), although we are relying
on the Lipschitz constant with respect to all inputs of the loss function and in a potentially
larger domain. (c) The dependency on ğœ– is also better in (12). (d) We are not assuming
convexity of â„“ and we consider general ğ‘š. (e) The dependency on ğ‘˜ in (12) is much more
benign than the one in (13), which is doubly exponential.

Remark 6.6. A recent manuscript by [26] provides a similar algorithm to the one by [20] but
in the proper learning setting, for depth-2 ReLU networks with convex loss functions. The
1
running time of the algorithm (rephrased to match our notation) is
) .
Analogous to the comparison in Remark 6.5, we obtain a much better dependence with
respect to ğœ– and we do not rely on convexity of the loss function or on constant depth of the
neural network.

1
) 2(

ğ›¼

ğ‘›

/

ğ‘‚

ğ‘¤

)

(

ğ‘‚

/

ğœ–

)

(

(

7. Conclusion and final remarks

We have showed that ERM problems admit a representation which encodes all possible
training problems in a single polytope whose size depends only linearly on the sample size
and possesses optimality guarantees. Moreover, we show that training is closely related
to the face structure of this data-independent polytope. As a byproduct, our contributions
also improve some of the best known algorithmic results for neural network training with
optimality/approximation guarantees.

These results shed new light on (theoretical) neural network training by bringing together
concepts of graph theory, polyhedral geometry, and non-convex optimization as a tool
for Deep Learning. Our data-independent polyhedral encoding, its data-dependent face
structure, and the fact that its size is only linear on the sample size reveal an interesting
interaction between diï¬€erent training problems.

While a straightforward algorithmic use of our formulation is likely to be diï¬ƒcult to
solve in practice, we believe the theoretical foundations we lay here can also have practical
implications in the Machine Learning community. All our architecture dependent terms
are worst-case bounds, which can be improved by assuming more structure. Additionally,
the history of Linear Programming has provided many important cases of extremely large
LPs that can be solved to near-optimality without necessarily generating the complete
description. In these, the theoretical understanding of the polyhedral structure is crucial to
drive the development of solution strategies.

Acknowledgements

Research reported in this paper was partially supported by NSF CAREER award CMMI-
1452463, ONR award GG012500 and the Institute for Data Valorization (IVADO). We
would also like to thank Shabbir Ahmed for the helpful pointers and discussions.

References

[1] Shabbir Ahmed. A Graduate Course on Stochastic Programming. preprint, 2017.
[2] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference on

Machine Learning, pages 146â€“155, 2017.

20

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

[3] Ross Anderson, Joey Huchette, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong convex relaxations
and mixed-integer programming formulations for trained neural networks. arXiv preprint arXiv:1811.01988,
2018.

[4] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anribit Mukherjee. Understanding deep neural networks
with rectiï¬ed linear units. to appear in Proceedings of ICLR 2018, arXiv preprint arXiv:1611.01491, 2018.
[5] Albert Atserias. On digraph coloring problems and treewidth duality. European Journal of Combinatorics,

29(4):796â€“820, 2008.

[6] Kristin P Bennett and Olvi L Mangasarian. Neural network training via linear programming. Technical

report, University of Wisconsin-Madison Department of Computer Sciences, 1990.

[7] Kristin P Bennett and Olvi L Mangasarian. Robust linear programming discrimination of two linearly

inseparable sets. Optimization methods and software, 1(1):23â€“34, 1992.

[8] Leonard Berrada, Andrew Zisserman, and M Pawan Kumar. Deep frank-wolfe for neural network optimiza-

tion. arXiv preprint arXiv:1811.07591, 2018.

[9] Dan Bienstock. On embedding graphs in trees. Journal of Combinatorial Theory, Series B, 49(1):103â€“136,

1990.

[10] Daniel Bienstock and Gonzalo MuÃ±oz. Lp formulations for polynomial optimization problems. SIAM Journal

on Optimization, 28(2):1121â€“1150, 2018.

[11] Avrim L. Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. Neural Networks,

5(1):117 â€“ 127, 1992.

[12] DigvÄ³ay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training relu neural network. arXiv

preprint arXiv:1809.10787, 2018.

[13] LÃ©on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.

Siam Review, 60(2):223â€“311, 2018.

[14] Gruia Calinescu, Cristina G Fernandes, and Bruce Reed. Multicuts in unweighted graphs and digraphs with
bounded degree and bounded tree-width. In Proceedings of the 6th Conference on Integer Programming and
Combinatorial Optimization (IPCO). Citeseer, 1998.

[15] Chih-Hong Cheng, Georg NÃ¼hrenberg, and Harald Ruess. Maximum resilience of artiï¬cial neural networks.
In International Symposium on Automated Technology for Veriï¬cation and Analysis, pages 251â€“268. Springer,
2017.

[16] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural
networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint
arXiv:1602.02830, 2016.

[17] Frank E Curtis and Katya Scheinberg. Optimization methods for supervised machine learning: From linear
models to deep learning. In Leading Developments from INFORMS Communities, pages 89â€“114. INFORMS,
2017.

[18] Matteo Fischetti and Jason Jo. Deep neural networks and mixed integer linear optimization. Constraints,

pages 1â€“14, 2018.

[19] Steï¬€en Goebbels. Training of relu activated multilayerd neural networks with mixed integer linear programs.

Technical report, Hochschule Niederrhein, Fachbereich Elektrotechnik & Informatik, 2021.

[20] Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time.

In Conference on Learning Theory, pages 1004â€“1042, 2017.

[21] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
[22] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks by

enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.

[23] Daniel J Harvey and David R Wood. The treewidth of line graphs. Journal of Combinatorial Theory, Series

B, 2018.

[24] Elias B Khalil, Amrita Gupta, and Bistra Dilkina. Combinatorial attacks on binarized neural networks. arXiv

preprint arXiv:1810.03538, 2018.

[25] Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso Poggio. A surprising linear

relationship predicts test performance in deep networks. arXiv preprint arXiv:1807.09659, 2018.

[26] Pasin Manurangsi and Daniel Reichman. The computational complexity of training relu (s). arXiv preprint

arXiv:1810.04207, 2018.

[27] Somnath Mukhopadhyay, Asim Roy, Lark Sang Kim, and Sandeep Govil. A polynomial time algorithm for
generating neural networks for pattern classiï¬cation: Its stability properties and some test results. Neural
Computation, 5(2):317â€“330, 1993.

[28] Sebastian Pokutta, Christoph Spiegel, and Max Zimmer. Deep neural network training with frank-wolfe.

arXiv preprint arXiv:2010.07243, 2020.

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

21

[29] Neil Robertson and Paul D. Seymour. Graph minors II: Algorithmic aspects of tree-width. Journal of

Algorithms, 7:309 â€“ 322, 1986.

[30] Asim Roy, Lark Sang Kim, and Somnath Mukhopadhyay. A polynomial time algorithm for the construction

and training of a class of multilayer perceptrons. Neural networks, 6(4):535â€“545, 1993.

[31] Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear regions

of deep neural networks. arXiv preprint arXiv:1711.02114, 2017.

[32] Alexander Shapiro, Darinka Dentcheva, and Andrzej RuszczyÅ„ski. Lectures on stochastic programming:

modeling and theory. SIAM, 2009.

[33] Stephen J Wright. Optimization algorithms for data analysis. The Mathematics of Data, 25:49, 2018.
[34] Jiahao Xie, Zebang Shen, Chao Zhang, Boyu Wang, and Hui Qian. Eï¬ƒcient projection-free online meth-
ods with stochastic recursive gradient. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence,
volume 34, pages 6446â€“6453, 2020.

[35] Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly learnable

in polynomial time. In International Conference on Machine Learning, pages 993â€“1001, 2016.

Appendix A. Further definitions

A.1. Proper vs. improper learning. An important distinction is the type of solution to
the ERM that we allow. In this work we are considering proper learning, where we require
Î¦, i.e., the model has to be from the considered model class
the solution to satisfy ğœ™
Î©, with
induced by Î¦ and takes the form ğ‘“

âˆˆ

(Â·
Ë†ğ‘¥ğ‘–, ğœ™âˆ—

, ğœ™âˆ—)
, Ë†ğ‘¦ğ‘–

)

for some ğœ™âˆ— âˆˆ
1
min
ğ·
Î¦
ğœ™
âˆˆ

ğ·

) â‰¤

Ã•ğ‘–=1

1
ğ·

ğ·

Ã•ğ‘–=1

ğ‘“

â„“

(

(

ğ‘“

â„“

(

(

Ë†ğ‘¥ğ‘–, ğœ™

, Ë†ğ‘¦ğ‘–

,

)

)

and this can be relaxed to ğœ–-approximate (proper) learning by allowing for an additive error
ğœ– > 0 in the above. In contrast, in improper learning we allow for a model ğ‘”
, that cannot
Î¦, satisfying
be obtained as ğ‘“

with ğœ™

, ğœ™

(Â·)

(Â·

)

âˆˆ

1
ğ·

ğ·

Ã•ğ‘–=1

â„“

ğ‘”

(

(

Ë†ğ‘¥ğ‘–

)

, Ë†ğ‘¦ğ‘–

) â‰¤

1
ğ·

min
Î¦
ğœ™
âˆˆ

ğ·

Ã•ğ‘–=1

ğ‘“

â„“

(

(

Ë†ğ‘¥ğ‘–, ğœ™

, Ë†ğ‘¦ğ‘–

,

)

)

with a similar approximate version.

Appendix B. Regularized ERM

A common practice to avoid over-ï¬tting is the inclusion of regularizer terms in (1). This

leads to problems of the form

(14)

1
ğ·

min
Î¦
ğœ™
âˆˆ

ğ·

Ã•ğ‘–=1

ğ‘“

â„“

(

(

Ë†ğ‘¥ğ‘–, ğœ™

, Ë†ğ‘¦ğ‘–

)

ğœ† ğ‘…

ğœ™

,

)

(

) +

(Â·)

where ğ‘…
is a function, typically a norm, and ğœ† > 0 is a parameter to control the
strength of the regularization. Regularization is generally used to promote generalization
and discourage over-ï¬tting of the obtained ERM solution. The reader might notice that our
arguments in Section 3 regarding the epigraph reformulation of the ERM problem and the
tree-decomposition of its intersection graph can be applied as well, since the regularizer
term does not add any extra interaction between the data-dependent variables.

The previous analysis extends immediately to the case with regularizers after appropriate

modiï¬cation of the architecture Lipschitz constant
Deï¬nition B.1. Consider a regularized ERM problem (14) with parameters ğ·, Î¦, â„“, ğ‘“ , ğ‘…,
and ğœ†. We deï¬ne its Architecture Lipschitz Constant

ğ·, Î¦, â„“, ğ‘“ , ğ‘…, ğœ†

to include ğ‘…

(Â·)

as

L

.

(15)

over the domain

=

K

L (

[âˆ’

ğ·, Î¦, â„“, ğ‘“ , ğ‘…, ğœ†

1, 1

Î¦

ğ‘›

]

Ã—

Ã— [âˆ’

(cid:17)

)
1, 1

ğ‘“

â„“

(

Lâˆ (
ğ‘š.

]

L (
,

(Â·

Â·)

ğœ†ğ‘…

,

Â·) +

(Â·))

)

22

PRINCIPLED DEEP NEURAL NETWORK TRAINING THROUGH LINEAR PROGRAMMING

Appendix C. Binarized Neural Networks

A Binarized activation unit (BiU) is parametrized by ğ‘

1 values ğ‘, ğ‘1, . . . , ğ‘ ğ‘. Upon

a binary input vector ğ‘§1, ğ‘§2, . . . , ğ‘§ ğ‘ the output is binary value ğ‘¦ deï¬ned by:

+

ğ‘¦ = 1 if ğ‘ğ‘‡ ğ‘§ > ğ‘,

and ğ‘¦ = 0 otherwise.

Now suppose we form a network using BiUs, possibly using diï¬€erent values for the
parameter ğ‘.
In terms of the training problem we have a family of (binary) vectors
ğ‘¥1, . . . , ğ‘¥ ğ· in â„ğ‘› and binary labels and corresponding binary label vectors ğ‘¦1, . . . , ğ‘¦ğ· in
â„ğ‘š, and as before we want to solve the ERM problem (1). Here, the parametrization ğœ™
at each unit. In the speciï¬c case of a network with 2
refers to a choice for the pair
nodes in the ï¬rst layer and 1 node in the second layer, and ğ‘š = 1, [11] showed that it is
NP-hard to train the network so as to obtain zero loss, when ğ‘› = ğ·. Moreover, the authors
ğ‘, ğ‘
argued that even if the parameters
, the problem remains
NP-Hard. See [16] for an empirically eï¬ƒcient training algorithm for BiUs.

are restricted to be in

ğ‘, ğ‘

1, 1

{âˆ’

}

)

)

(

(

In this section we apply our techniques to the ERM problem (1) to obtain an exact
polynomial-size data-independent formulation for each ï¬xed network (but arbitrary ğ·)
when the parameters

are restricted to be in

ğ‘, ğ‘

1, 1

.

We begin by noticing that we can reformulate (1) using an epigraph formulation as in (5).
Moreover, since the data points in a BiU are binary, if we keep the data points as variables,
the resulting linear-objective optimization problem is a binary optimization problem as BO.
This allows us to claim the following:

{âˆ’

}

(

)

Theorem C.1. Consider a graph

G

, ğ‘

ğ‘‚

â„• and ğ·
âˆˆ
2 ğ‘

( G) | ğ·

ğ‘‰

|

(

âˆˆ
,

)

â„•. There exists a polytope of size

such that any BiU ERM problem of the form (1) is equivalent to optimizing a linear function
plus the time required
over a face of ğ‘ƒ. Constructing the polytope takes time ğ‘‚
evaluations of ğ‘“ and â„“.
for ğ‘‚

( G) | ğ·

2 ğ‘

2 ğ‘

ğ‘‰

ğ‘‰

)

(

|

(

|

( G) |)

Proof. The result follows from applying Theorem 2.8 directly to the epigraph formulation
of BiU keeping ğ‘¥ and ğ‘¦ as variables. In this case an approximation is not necessary. The
construction time and the data-independence follow along the same arguments used in the
(cid:3)
approximate setting before.

The following corollary is immediate.

Corollary C.2. The ERM problem (1) over BiUs can be solved in polynomial time for any
ğ·, whenever ğ‘ and the network structure

are ï¬xed.

G

