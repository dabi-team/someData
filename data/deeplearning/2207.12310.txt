2
2
0
2

l
u
J

5
2

]

V
C
.
s
c
[

1
v
0
1
3
2
1
.
7
0
2
2
:
v
i
X
r
a

Estimaci´on de ´areas de cultivo mediante Deep Learning y
programaci´on convencional

Javier Caicedo1, Pamela Acosta1, Romel Pozo1, Henry Guilcapi1, and Christian Mejia-Escobar1*

Universidad Central del Ecuador, Facultad de Ingenier´ıa en Geolog´ıa, Minas, Petr´oleos y Ambiental, Quito-Ecuador

Resumen La Inteligencia Artiﬁcial ha permitido la implementaci´on de soluciones m´as precisas y eﬁ-
cientes para problem´aticas en diversas ´areas. En el sector agr´ıcola, una de las principales necesidades es
conocer en todo momento la extensi´on de terreno ocupada o no por los cultivos con el ﬁn de mejorar la
producci´on y la rentabilidad. Los m´etodos tradicionales de c´alculo demandan la obtenci´on de datos de
manera manual y presencial en campo, ocasionando altos costes de mano de obra, tiempos de ejecuci´on,
e imprecisi´on en los resultados. El presente trabajo propone un nuevo m´etodo basado en t´ecnicas de
Deep Learning complementadas con programaci´on convencional para la determinaci´on del ´area de zo-
nas pobladas y despobladas de cultivos. Hemos considerado como caso de estudio una de las empresas
m´as reconocidas en la siembra y cosecha de ca˜na de az´ucar en el Ecuador. La estrategia combina una
Red Neuronal Adversaria Generativa (GAN) que es entrenada sobre un dataset de fotograf´ıas a´ereas de
paisajes naturales y urbanos para mejorar la resoluci´on de im´agenes; una Red Neuronal Convolucional
(CNN) entrenada sobre un dataset de fotograf´ıas a´ereas de parcelas de ca˜na de az´ucar para distin-
guir zonas pobladas o despobladas de cultivo; y un m´odulo de procesamiento est´andar de im´agenes
para el c´alculo de ´areas de manera porcentual. Los experimentos realizados demuestran una mejora
signiﬁcativa de la calidad de las fotograf´ıas a´ereas as´ı como una diferenciaci´on notable entre las zonas
pobladas y despobladas de cultivo, consecuentemente, un resultado m´as preciso de las ´areas cultivadas
y no cultivadas. El m´etodo propuesto puede ser extendido a la detecci´on de posibles plagas, zonas de
vegetaci´on de maleza, el desarrollo din´amico de los cultivos, y control de calidad tanto cualitativo como
cuantitativo.

Keywords: Deep Learning · S´uper-resoluci´on de im´agenes · cultivos agr´ıcolas · Redes Neuronales
Convolucionales · Redes Generativas Adversarias · Ingenio Azucarero Valdez

1.

Introducci´on

La fotograf´ıa a´erea es una fuente de informaci´on fundamental para las Ciencias de la Tierra. El proceso
de examinarlas para identiﬁcar objetos o ciertas condiciones se denomina fotointerpretaci´on [1]. Por ejem-
plo, dentro del ´area geol´ogica y agr´ıcola permite delimitar diversos par´ametros tales como topograf´ıa, rasgos
hidrogr´aﬁcos, litolog´ıa en funci´on de texturas y caracter´ısticas agr´ıcolas como superﬁcies de cultivos, delimi-
taci´on y an´alisis focalizado en ´areas de cultivos, entre otras, que desde superﬁcie no se pueden apreciar de
forma ´optima [2]. En sus inicios, la fotointerpretaci´on deb´ıa ser realizada de manera manual por operadores
especializados, signiﬁcando mayor gasto de recursos y altos tiempos de espera para las compa˜n´ıas. M´as a´un,
cuando dichas im´agenes no eran capturadas en una resoluci´on adecuada, el trabajo del interpretador muchas
veces no era posible de realizar o pod´ıa tomar mucho m´as tiempo de lo esperado. Un avance importante
en este ´ambito se logra con el uso de los GIS (Sistemas de Informaci´on Geogr´aﬁca)[3], a trav´es del proce-
samiento de las fotograf´ıas a´ereas aplicando operaciones, tanto cualitativas como cuantitativas, como por
ejemplo, medici´on de ´areas, c´alculos topogr´aﬁcos, descripciones geol´ogicas como texturas asociadas a tipos
de roca o suelos, fen´omenos de remoci´on en masa, delimitaci´on de formaciones a trav´es del reconocimiento
en la disposici´on de las capas, etc., para una toma de decisiones adecuada. Sin embargo, a´un se requiere
de un operador que controle dichas herramientas de interpretaci´on, necesitando de una mayor cantidad de
tiempo, esfuerzo y habilidad por parte del ser humano.

Hoy en d´ıa, las t´ecnicas de Inteligencia Artiﬁcial (IA), en particular el Deep Learning se ha convertido
en una herramienta exitosa para el procesamiento autom´atico de im´agenes de diversa ´ındole, as´ı como para
el reconocimiento y delimitaci´on de objetos de manera automatizada. Es un concepto que surge de la idea

* Email de correspondencia: cimejia@uce.edu.ec

 
 
 
 
 
 
2

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

de imitar el cerebro y el sistema visual a partir del uso de hardware y software, para crear una inteligencia
artiﬁcial pura, utilizando una capacidad de abstracci´on jer´arquica, es decir, una representaci´on de los datos
de entrada en varios “niveles” [4].

En el presente trabajo, proponemos la implementaci´on de un sistema basado en Deep Learning, para el
procesamiento autom´atico de fotograf´ıas a´ereas, que permita mejorar la resoluci´on de ortofotograf´ıas captu-
radas en baja resoluci´on para posteriormente realizar la identiﬁcaci´on y c´alculo de superﬁcies relacionadas
con zonas pobladas y despobladas en cultivos de ca˜na de una manera autom´atica. Hemos considerado el
caso de la compa˜n´ıa Azucarera Valdez, la cual est´a ubicada en el cant´on Milagro de la provincia del Guayas
(Ecuador), con una producci´on de az´ucar ininterrumpida durante 138 a˜nos, posicion´andose como una de las
industrias m´as grandes del pa´ıs en el ´ambito alimenticio. Actualmente se busca automatizar procedimientos
manuales en campo, como son las actividades de muestreo para despoblaci´on, plagas, malezas entre otras,
con la ﬁnalidad de optimizar el ﬂujo de trabajo en todas las ´areas involucradas. Es de vital importancia para
la empresa, conocer el desarrollo de los cultivos de ca˜na a trav´es de fotograf´ıas a´ereas, con el prop´osito de
llevar un control cuantitativo de las ´areas cultivadas y no cultivadas.

Nuestra estrategia de soluci´on es combinar las Redes Neuronales Adversarias Generativas (GAN), las
Redes Neuronales Convolucionales (CNN) y la programaci´on tradicional. Las GANs consisten en dos modelos,
el generador y el discriminador, entrenados simult´aneamente para desaﬁarse uno al otro, lo que explica el
t´ermino antag´onicas elegido por los autores para darle identidad a este novedoso m´etodo [5]. La generadora
es entrenada para crear datos falsos lo m´as parecidos posibles a los ejemplos reales de un determinado
conjunto de entrenamiento. Por otro lado, la discriminadora es entrenada para ser capaz de discernir los
datos falsos producidos por la generadora de aquellos que corresponden al conjunto de entrenamiento (los
ejemplos reales). Dicha estructura es ´util para el aumento de la resoluci´on de las fotograf´ıas a´ereas que
requieran una optimizaci´on mediante operaciones de p´ıxel con el objetivo de reducir el tama˜no y reorganizar
la informaci´on permitiendo as´ı transponer la misma a trav´es de la manipulaci´on de tensores e ir mejorando
la imagen[6]. Estas im´agenes ser´an el insumo para una CNN, la cual funciona a trav´es de capas de neuronas
que se conforman de p´ıxeles representados por n´umeros, y que se operan matem´aticamente a trav´es de la
aplicaci´on de ﬁltros o kernels para dar como resultado un mapa de caracter´ısticas de la imagen original [7].
Estas caracter´ısticas son pasadas a un clasiﬁcador que se encargar´a del reconocimiento y categorizaci´on de
superﬁcies en las fotograf´ıas determin´andolas como pobladas (referido a toda la zona cultivada de ca˜na) o
despobladas (zonas sin cultivo de ca˜na). Por el enfoque de la problem´atica se tomar´a en cuenta ´unicamente
las zonas que no presentan cultivo dentro del ´area delimitada y se discriminar´a carreteras y drenajes que est´en
presentes en las fotograf´ıas mediante una delimitaci´on previa en la planiﬁcaci´on del vuelo donde solamente
se considerar´a el ´area cultivada. Finalmente, se calcular´a de forma porcentual las superﬁcies que presenten
despoblaci´on en los cultivos de ca˜na que ser´a el dato de inter´es, en consecuencia tambi´en se obtendr´a el dato
del ´area que se encuentra cultivada, a trav´es de la utilizaci´on de bibliotecas de programaci´on tradicional.

De esta manera, es posible optimizar los tiempos, costos, log´ıstica e insumos de la compa˜n´ıa, cuya ne-
cesidad de conocer par´ametros de sus cultivos se ha visto encarecida por la obtenci´on de datos en forma
tradicional, debido al costo de mano de obra y con el riesgo de un alto porcentaje de error por la inaccesi-
bilidad en terrenos. Adicionalmente, el mejoramiento y an´alisis autom´atico de fotograf´ıas a´ereas tiene una
amplia gama de aplicabilidad, siendo factible transponer los resultados de este estudio hacia una distinta
problem´atica que puede o no estar en la misma ´area de conocimiento como identiﬁcaci´on de malezas, zonas
con problemas de ﬂoraci´on en ca˜na, delimitaci´on de control de rebrotes en diferentes cultivos. Concretamente,
proporcionamos las siguientes contribuciones:

Un dataset de 650 fotograf´ıas a´ereas de alta y baja resoluci´on obtenidas del portal SIGTIERRAS y que
muestran entornos urbanos, rurales, y naturales para el entrenamiento de Real-ESRGAN.
Un dataset de 1600 im´agenes restringidas de los predios de la compa˜n´ıa azucarera Valdez que muestran
´areas de cultivos con zonas pobladas y despobladas.
Una metodolog´ıa que combina las GANs, CNNs y la programaci´on tradicional para procesar im´agenes
a´ereas con el ﬁn de automatizar procedimientos de interpretaci´on de informaci´on morfom´etrica de cultivos
de ca˜na y que puede ser adaptada para problemas similares.
La implementaci´on pr´actica de modelos de s´uper-resoluci´on, categorizaci´on y tratamiento de im´agenes,
aprovechando la t´ecnica del Transfer Learning y librer´ıas tradicionales de procesamiento de im´agenes.

Preprint

3

Los productos mencionados estar´an disponibles en la Escuela de Geolog´ıa de la Universidad Central
del Ecuador para ﬁnes educativos, ya que por motivos de conﬁdencialidad relacionados con pol´ıticas de la
Compa˜n´ıa Valdez, los datos utilizados en el presente estudio no podr´an ser publicados en un repositorio
digital, sin embargo, podr´ıan ser solicitados a cualquiera de los autores.

El resto del documento se estructura de la siguiente manera: en la secci´on 2 se citan trabajos relacionados;
la secci´on 3 describe detalladamente la metodolog´ıa empleada; la secci´on 4 explica los experimentos realizados
y los resultados que se obtuvieron para cada uno de los modelos trabajados; la secci´on 5 presenta la discusi´on
de los mismos; y ﬁnalmente, la secci´on 6 incluye las conclusiones del estudio y algunas l´ıneas de trabajo futuro.

2. Trabajos relacionados

El uso de CNNs y GANs se ha ido ampliando durante los ´ultimos a˜nos, sin embargo, no conocemos de un
trabajo previo que combine ambos tipos de redes neuronales artiﬁciales para tratar el problema espec´ıﬁco de
c´alculo de zonas pobladas y despobladas de cultivo a partir de fotograf´ıas a´ereas [8]. Existen ciertos estudios
que aprovechan por separado el uso de CNNs y GANs para abordar tem´aticas relacionadas con fotograf´ıas
a´ereas y geociencias.

2.1. Uso de GAN’s para s´uper-resoluci´on

En cuanto a la s´uper-resoluci´on de im´agenes podemos citar a Clabaut. ´E, et al., 2021 [9], con su estudio
que se titula “Model Specialization for the Use of ESRGAN on Satellite and Airborne Imagery”, donde
utilizan el modelo previo a Real-ESRGAN, desarrollado por Xintao Wang. Aqu´ı se propone evaluar el en-
trenamiento de una GAN para mejorar la resoluci´on de im´agenes satelitales. Los autores utilizan varios
datasets especializados en im´agenes a´ereas de varias categor´ıas (urbanas, rurales, mar´ıtimas) donde se hace
una comparaci´on de los resultados de un entrenamiento al realizar una mezcla de los datasets y al entrenar
al modelo de manera independiente por cada categor´ıa. Los resultados muestran un mejor funcionamiento al
entrenar el modelo para una categor´ıa espec´ıﬁca y lo eval´uan a trav´es del ruido generado durante el proceso
de downscale.

2.2. Uso de CNN’s para reconocimiento

Ald´as. R, et al., 2022 [10], en su trabajo denominado “Delimitaci´on autom´atica de ceniza volc´anica en
im´agenes sat´elites mediante Deep Learning”, implementan una soluci´on basada en aprendizaje profundo que
permite segmentar emisiones de ceniza en im´agenes sat´elites mediante Redes Neuronales Convolucionales.
A trav´es de la elaboraci´on de un extenso dataset de im´agenes, entrenaron un modelo para la predicci´on de
la delimitaci´on del esparcimiento de ceniza volc´anica de nuevas im´agenes sat´elites. De esta forma, generan
un aporte en cuanto a la gesti´on de las zonas afectadas por el fen´omeno de ca´ıda de ceniza y su impacto en
sectores estrat´egicos como la agricultura, ganader´ıa y la salud.

2.3. Combinaci´on de GAN’s y CNN’s

Maayan Frid-Adar, et al., 2018 [11], con su estudio “Aumento de im´agenes m´edicas sint´eticas basadas en
GAN para un mayor rendimiento de CNN en la clasiﬁcaci´on de lesiones hep´aticas”, presenta m´etodos para
generar im´agenes m´edicas sint´eticas utilizando redes generativas adversarias (GAN) y as´ı obtener im´agenes
de lesiones hep´aticas de alta calidad. Dichas im´agenes generadas se pueden usar para el aumento de datos
sint´eticos y mejorar el rendimiento de una red CNN utilizada en la clasiﬁcaci´on de im´agenes m´edicas usando
un conjunto limitado de 182 tomograf´ıas (53 quistes, 64 met´astasis y 65 hemangiomas). Entrenando a la
red CNN se obtuvo un rendimiento de sensibilidad de 78.6 % con una especiﬁcidad de 88.4 % generando un
esquema para la clasiﬁcaci´on de lesiones hep´aticas. Este enfoque puede ayudar a respaldar los esfuerzos de
los radi´ologos para mejorar el diagn´ostico sobre lesiones hep´aticas [12].

Yang Li y Xuewei Chao, 2020 [13], con su trabajo “Clasiﬁcaci´on continua basada en ANN en agricultura”
realizan una clasiﬁcaci´on oportuna de enfermedades de plantas a trav´es de im´agenes. Los autores proponen
un m´etodo de clasiﬁcaci´on basado en ANN a trav´es del almacenamiento y recuperaci´on de memoria, con dos

4

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

claras ventajas: pocos datos y alta ﬂexibilidad. Este modelo combina una CNN y una GAN. La red CNN
requiere pocos datos sin procesar para lograr un buen rendimiento, para una tarea de clasiﬁcaci´on de plantas
y la parte GAN se usa para extraer informaci´on de tareas antiguas y generar im´agenes abstractas como
memoria para la tarea futura. La red ANN se puede utilizar para tareas nuevas, con buen desempe˜no, debido
a su capacidad de acumular conocimiento. La protecci´on de las plantas se logr´o mediante la identiﬁcaci´on
autom´atica de enfermedades y plagas [14].

A diferencia de los trabajos anteriores, nuestro enfoque est´a dirigido hacia una problem´atica en el campo
de la denominada agricultura de precisi´on, donde la particularidad radica en que en este caso nosotros
dise˜namos un ﬂujo de trabajo robusto, que combina t´ecnicas de aprendizaje autom´atico y programaci´on
tradicional para lograr enfrentar el problema en cuesti´on, mejorando tiempos, reducir costos y riesgos y
obtener una mayor precisi´on en las mediciones son varias de las ventajas que podremos materializar con este
estudio. Adicionalmente consideramos que los modelos y las arquitecturas utilizadas ofrecen la versatilidad
necesaria para permitir adaptar a este ﬂujo de trabajo a tem´aticas relacionadas.

3. Metodolog´ıa

Nuestro objetivo es el c´alculo autom´atico del porcentaje de zonas cultivadas y no cultivadas de ca˜na
de az´ucar en los lotes de plantaci´on de la empresa Azucarera Valdez. Para tal ﬁn, combinamos t´ecnicas de
Inteligencia Artiﬁcial y la programaci´on tradicional, conformando una soluci´on compuesta de 3 m´odulos.
Primero, utilizamos una red de tipo GAN que ser´a entrenada para aumentar la resoluci´on de las fotograf´ıas
a´ereas de los lotes, lo cual contribuye para una mejor categorizaci´on y mayor precisi´on en el c´alculo de
extensiones de cultivo. Luego, aprovechamos una red convolucional convencional entrenada para distinguir
la presencia de poblaci´on o despoblaci´on de cultivos de ca˜na. Finalmente, en el caso de detectar la existencia
de ´areas con despoblaci´on de cultivos, se procede a realizar la delimitaci´on porcentual de dicha extensi´on
mediante librer´ıas de procesamiento de im´agenes y programaci´on tradicional, en caso de no detectar la
existencia de despoblaci´on de cultivos no es posible un c´alculo referencial de dicha ´area, debido a que su
extensi´on pertenecer´a exclusivamente a ´areas con poblaci´on de cultivos. El ﬂujo de trabajo est´a representado
en la Figura 1 y, seguidamente, describimos en detalle cada una de las etapas consideradas.

3.1. S´uper-resoluci´on con Real-ESRGAN

3.1.1. Dataset

La materia prima para el aprendizaje autom´atico de tipo supervisado es el conjunto de datos, cada uno
asociado con su respectiva respuesta. En nuestro caso, corresponden a im´agenes de resoluci´on baja y su
respectivo par de alta resoluci´on. As´ı, fue necesaria una recolecci´on de im´agenes que permitieran tener una
gama de categor´ıas adecuada para entrenar al modelo en funci´on de nuestro objetivo. Todas estas im´agenes
fueron obtenidas a trav´es del portal del Sistema Nacional de Informaci´on de Tierras Rurales e Infraestructura
Tecnol´ogica (SIGTIERRAS)1, donde se requiere crear un usuario para solicitar fotograf´ıas a´ereas disponibles
de forma gratuita y que cubren la mayor parte del territorio nacional. Se eligieron zonas para ortofotograf´ıa
donde se pudieran observar espacios urbanos y rurales o ´areas naturales. Se consideraron ortofotograf´ıas de
ciudades del Ecuador como Quito, Guayaquil, Manta y Salinas dentro del apartado urbano, con un total de
250 fotograf´ıas y se complement´o con 400 fotograf´ıas de ciudades como El Puyo, Milagro, Tena y Sucumb´ıos
para el apartado rural y entornos naturales.

1 www.sigtierras.gob.ec

Preprint

5

Figura 1. Metodolog´ıa de trabajo. Elaborado por: Autores

El portal de SIGTIERRAS ofrece fotograf´ıas a una resoluci´on de 5 metros en formato JP2, el cual permite
guardar datos de georreferenciaci´on. Se crearon dos directorios para la construcci´on del dataset: ”URBANO”
y ”RURAL”, con un total de 650 im´agenes con dimensiones de (4647x4617), divididas en dos partes, de tal
manera que para la zona urbana se utilizaron 250 im´agenes y 400 im´agenes para la zona rural, con un peso
total de 24 GB, sin embargo, la denominaci´on de las im´agenes est´a de acuerdo al rollo y a la zona de captura
correspondiente a la hoja topogr´aﬁca de las zonas de inter´es. La Figura 2 muestra ejemplos de fotograf´ıas
de baja resoluci´on con su par de alta resoluci´on utilizados en el dataset para el entrenamiento del modelo
Real-ESRGAN; adem´as, como se puede apreciar en este ejemplo, las im´agenes de la izquierda se encuentran
pixeladas, con contraste de colores opacos y los l´ımites de los objetos son difusos tanto para la categor´ıa
urbana como rural, mientras que en las im´agenes de la derecha, las cuales han sido entrenadas con el modelo
Real-ESRGAN, poseen una calidad superior, con l´ımites bien deﬁnidos y mejor contraste de colores para
ambas categor´ıas [15].

6

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

Figura 2. Ejemplos de im´agenes a´ereas del dataset: (sup) Ejemplo de la categor´ıa rural o ´area natural, (inf) Ejemplo
de la categor´ıa urbana. Fuente: Portal SIGTIERRAS.

3.1.2. Pre-procesamiento

Las im´agenes descargadas y organizadas en las dos carpetas mencionadas, son convertidas del formato
original JP2 de tipo georreferenciado a un formato simple JPG, ampliamente utilizado para el manejo de
fotograf´ıas en todo tipo de dispositivos, adem´as caracterizado por una buena compresi´on. Este procedimiento
se realiz´o en lotes de 20 fotograf´ıas mediante la utilizaci´on del convertidor online convertio2. Para la deno-
minaci´on de las im´agenes, se mantuvo el nombre por defecto de la descarga desde el portal SIGTIERRAS.

3.1.3. Divisi´on en Train/Test

Los algoritmos de aprendizaje autom´atico aprenden de los datos con los que los entrenamos. A partir de
ellos, intentan encontrar o inferir el patr´on que les permita predecir el resultado para un nuevo caso. Pero,
para poder calibrar si un modelo funciona, necesitaremos probarlo con un conjunto de datos diferente. Por
ello, en todo proceso de aprendizaje autom´atico, los datos de trabajo se dividen en dos partes: una parte
Train o entrenamiento, que corresponder´a a la mayor parte de nuestro dataset y que usaremos para entrenar
nuestro modelo y una parte Test, de menor tama˜no, sobre la que evaluaremos nuestro modelo entrenado [16].
Para nuestro caso, el proceso de divisi´on se realiz´o autom´aticamente utilizando funci´on train test split() de
la librer´ıa sklearn.modelselection, especiﬁcando dos subconjuntos: el de entrenamiento que contiene el 80 %
de los datos y el de prueba con el 20 % de los datos restantes. Estas proporciones de divisi´on son usualmente
recomendadas para asegurar una mayor densidad de datos en el conjunto de entrenamiento, y menos en el
conjunto de prueba. Es importante mencionar que la subdivisi´on fue realizada con un car´acter aleatorio para
garantizar la mayor variabilidad en ambos subconjuntos. La Tabla 1 muestra la distribuci´on de los datos en
la subdivisi´on.

2 https://convertio.co/es/jp2-jpg/

Conjunto N´umero de Im´agenes Porcentaje
520
130
650

80 %
20 %
100 %

Train
Test
Total

Preprint

7

Cuadro 1. Subdivisi´on del dataset.

3.1.4. Modelo

Hemos aprovechado la t´ecnica de Transfer Learning, misma que se basa en una idea simple, la de re-utilizar
los conocimientos adquiridos por otras conﬁguraciones (fuente), para resolver un problema en particular
(objetivo) [17]. En otras palabras, nos induce a la adaptaci´on de modelos ya creados, a nuestro convenir. Las
ventajas de la aplicaci´on de esta t´ecnica son variadas, pero destacan la optimizaci´on del tiempo y los buenos
resultados que proporciona, producto del enfoque que previamente la red ya ha recibido.

Para este caso, hemos utilizado el modelo GAN para s´uper-resoluci´on de im´agenes desarrollado por (Wang.
X, et al., 2021)3. La arquitectura del modelo se ha mantenido intacta, ´unicamente fue necesario someterla
a un nuevo entrenamiento sobre nuestro dataset. En este sentido, es importante dilucidar la arquitectura
con la que Real-ESRGAN trabaja. Utiliza una arquitectura que busca optimizar la estabilidad durante el
entrenamiento, al mismo tiempo que trata de eliminar o mitigar el blur, el ruido, el re-dimensionamiento y
la compresi´on [18], los cuales se generan como el resultado de una mala captura de las im´agenes en el caso
de fotograf´ıas, o por el trayecto que una imagen recorre cuando es cargada a la Web. Todo esto reduce la
calidad de la imagen.

Figura 3. Arquitectura del modelo GAN: (a) Esquema de la red generadora (RRDB); (b) Red discriminadora (U-net).
Modiﬁcado de: Wang. X, et al., 2021 [19].

3 https://github.com/xinntao/Real-ESRGAN

8

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

Figura 4. Esquema representativo de la operaci´on de pixel-Unshuﬄe. Tomado de: (Sun. B et al. 2019) [20]

La arquitectura de este modelo se basa en un dise˜no tradicional de las GANs, donde intervienen una red

generadora y una red discriminadora (Figura 3). La red generadora funciona de la siguiente manera:

Antes del input de la imagen al modelo es necesaria una operaci´on de Pixel-Unshuﬄe cuyo objetivo es
reducir el tama˜no espacial y reorganizar la informaci´on a la dimensi´on del canal (Figura 3.1.4). Esta
operaci´on b´asicamente permite transponer la informaci´on desde una dimensi´on espacial a una dimensi´on
de profundidad a trav´es de la manipulaci´on de tensores [19]. La dimensi´on de profundidad representa el
resultado de la descomposici´on y combinaci´on de p´ıxeles expresado a trav´es de la creaci´on de tensores.
Esta operaci´on divide una caracter´ıstica en varias subcaracter´ısticas extra´ıdas de la imagen original.
En el ejemplo se usan cuatro colores diferentes para representar subcaracter´ısticas de la imagen de
entrada. Estas subcaracter´ısticas (que en realidad son p´ıxeles) contienen la informaci´on completa de
las caracter´ısticas originales de la imagen, pero con menor resoluci´on [21]. Por lo tanto, se utiliza este
procedimiento para evitar p´erdida de informaci´on mientras se reduce el tama˜no de la imagen de entrada.
Una vez obtenidas todas los tensores es necesario convolucionarlos con kernels para obtener un pre-
escalado de la imagen de entrada.
Seguidamente, la imagen resultante del pixel unshuﬄe ingresa a la red generadora, misma que se cataloga
como una red profunda conformada por varios RRDB (Residual-in-Residual Dense Block). Como se
muestran en la Figura 5, cada bloque est´a compuesto por cinco capas convolucionales, las conexiones
densas est´an representadas por la concatenaci´on de las salidas de diferentes capas. Esto quiere decir que
cada capa tiene acceso a la informaci´on previa de una manera progresiva. Este tipo de arquitectura fue
introducida para evitar el problema del desvanecimiento del gradiente durante el entrenamiento cuando
se usa una red profunda [22].
El bloque de upsampling permite restaurar la resoluci´on espacial de las im´agenes despu´es de todo el
proceso dentro de los RRDB, para de esta manera tener una imagen en alta resoluci´on.
Los bloques de capas convulucionales al ﬁnal de la red generadora permiten aﬁnar el resultado del
upsampling ya que son capaces de detectar las caracter´ısticas espaciales de la imagen y a trav´es de la
convoluci´on con ﬁltros o kernels son capaces de re-ordenar los p´ıxeles para generar una imagen de salida
totalmente sint´etica que ingresar´a a la red discriminadora posteriormente.

Por su parte, para la red discriminadora fue utilizada una red U-Net con normalizaci´on espectral. La
normalizaci´on espectral es una t´ecnica de normalizaci´on de los pesos que estabiliza el entrenamiento de la red
discriminadora. Fue empleada para obtener un entrenamiento din´amico en cuanto a la supresi´on de errores
visuales en las im´agenes se reﬁere. La arquitectura basada en U-Net fue desarrollada en principio para la
segmentaci´on de im´agenes biom´edicas, adoptada como una red totalmente convolucional. Sin embargo, m´as
tarde se adecuar´ıa dentro del modelo de las GAN´s para actuar tanto como red generadora, como antag´onica
o discriminadora [24]. Su funcionamiento se basa en la concatenaci´on de dos bloques, uno de encoder (iz-
quierda) y otro de decoder (derecha) como se observa en la Figura 6. El bloque codiﬁcador progresivamente
reduce la resoluci´on de la entrada, capturando el contexto de la imagen global. El decodiﬁcador realiza un
sobremuestreo progresivo, emparejando la resoluci´on de salida a la de entrada y, por lo tanto, permite una
localizaci´on precisa de las caracter´ısticas. Las conexiones residuales permiten enrutar datos de las resolucio-
nes coincidentes de los dos m´odulos, mejorando a´un m´as la capacidad de la red para captar con precisi´on
los detalles. El bloque de proceso central realiza una primera discriminaci´on con la imagen reducida en su
resoluci´on. A la salida de la red, se vuelve a generar este procedimiento pero ya con la imagen re-escalada y
ordenada a trav´es de una operaci´on de convoluci´on [25]. La implementaci´on de esta arquitectura para la red

Preprint

9

Figura 5. Esquema de la estructura de los RRDB. Tomado de: Wang. H et al. 2019
[23].

discriminadora permite proporcionar una retroalimentaci´on detallada p´ıxel por p´ıxel al generador, mientras
se mantiene la coherencia global de las im´agenes sintetizadas, brindando tambi´en retroalimentaci´on de la
imagen global [19].

Figura 6. Esquema de la arquitectura y procesos de la red discriminadora U-Net. Modiﬁcado de: Gonz´alez A. 2021
[24].

3.2. Reconocimiento de zonas de cultivo con CNN

3.2.1. Dataset

Las im´agenes fueron proporcionadas por el departamento de Agricultura de precisi´on del Ingenio Azu-
carero Vald´ez. Son fotograf´ıas a´ereas de los lotes de cultivo con mayor problem´atica en despoblaci´on. Se
han identiﬁcado en el departamento de experimentaci´on agr´ıcola, sustentado en resultados obtenidos por
evaluaciones en campo. Posterior a ello, se han realizado vuelos de control y monitoreo generando as´ı un
dataset de 1200 im´agenes, en las que se puede evidenciar zonas de siembra de ca˜na y zonas despobladas.

10

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

(a)

(b)

Figura 7. (a) Zona poblada de cultivo; (b) Zona despoblada de cultivo. Fuente: Compa˜n´ıa Azucarera Valdez S.A.

La Figura 7 muestra ejemplos de fotograf´ıas a´ereas de la plantaci´on de Azucarera Valdez. En la fotograf´ıa
a) se observa una zona de lote cultivado con diferentes variedades de ca˜na para el entrenamiento del modelo
y en la fotograf´ıa b) una zona con alta despoblaci´on en la que la ca˜na se encuentra en etapa de germinaci´on,
mismas que cuentan con una resoluci´on horizontal y vertical de 72 ppp (puntos por pulgada), alto 3648 p´ıxeles
y ancho 5472 p´ıxeles y presentan un formato JPG. Han sido organizadas en dos directorios: Zonas Pobladas
(800 fotograf´ıas con un peso total de 6.5 GB) y Zonas Despobladas (800 fotograf´ıas con un peso de 6.8
GB). Al tratarse de un aprendizaje supervisado, se debe alimentar y entrenar la red deﬁniendo entradas
(Zonas cultivadas y Zonas Despobladas) y sus respectivas salidas (Clasiﬁcaci´on de la categor´ıa de la imagen
y delimitaci´on de las zonas de conﬂicto). Para la denominaci´on de las im´agenes se ha mantenido el nombre
arrojado por el departamento de experimentaci´on agr´ıcola, en el caso de las fotograf´ıas obtenidas directamente
desde el dron llevar´an la codiﬁcaci´on dji01 o el n´umero de fotograf´ıa correspondiente, y en el caso de ser una
ortofotograf´ıa procesada llevar´a la codiﬁcaci´on ejemplo lote 004-034 debido a que luego de ser reconocidas y
clasiﬁcadas estas ser´an devueltas a la empresa para obtener la ortofoto general del lote.

3.2.2. Pre-procesamiento

Esta etapa consiste en la mejora (si es necesario), conversi´on, re-dimensionamiento y normalizaci´on de las
im´agenes que constituyen nuestro dataset, de tal manera que tengan el formato adecuado para el entrena-
miento del modelo de clasiﬁcaci´on. Aquellas fotograf´ıas que por fallas de enfoque dejan de ser completamente
n´ıtidas, aumentamos su resoluci´on con Real-ESRGAN. Debido a que la captura inicial de las fotograf´ıas es
realizada con el programa Pix4D mapper, ´estas guardan informaci´on de georreferenciaci´on, reﬂejando un
peso de entre 7 y 8 MB lo que aumentaba el tiempo de entrenamiento y ejecuci´on del c´odigo. Se tuvo que
realizar un ajuste al tama˜no de las im´agenes del dataset por medio del programa Photoshop CS para cambiar
el tama˜no a 300x300 p´ıxeles y disminuir el peso en las im´agenes. Finalmente, las im´agenes son convertidas
en arrays de datos num´ericos que son normalizados mediante la librer´ıa NumPy.

3.2.3. Divisi´on Train/Test

Para la divisi´on del dataset en sus conjuntos de entrenamiento y test, partimos de la clasiﬁcaci´on previa
realizada de forma manual, donde se identiﬁcaron un total de 1600 im´agenes que han sido dividas en dos
sub-clasiﬁcaciones de 800 im´agenes para identiﬁcaci´on de siembra en ca˜na y 800 para zonas donde se observa
despoblaci´on. Posteriormente fueron cargadas al c´odigo especiﬁcando dos conjuntos (Train, Test); el primero
de entrenamiento (train) que contiene el 80 por ciento de los datos y el segundo de prueba (test) con el 20
por ciento de los datos restantes, a continuaci´on se muestra la distribuci´on de los datos.

Preprint

11

Variable
Dataset Original
Dataset Z.Cultivadas
Dataset Z.Despobladas

Dataset
Train Dataset
Test Dataset

Dataset
Train Dataset
Test Dataset

N´umero de Im´agenes
1600
800
800
Dataset Zonas cultivadas
800
640
160
Dataset Zonas Despobladas
800
640
160

Porcentaje
100 %
50 %
50 %

100 %
80 %
20 %

100 %
80 %
20 %

Cuadro 2. Subdivisi´on del dataset.

3.2.4. Modelo

Para la creaci´on del modelo se us´o Transfer learning, en este caso EﬃcientNet-B5, debido a que es
un modelo que se ajusta de manera m´as eﬁciente al equilibrar cuidadosamente la profundidad, el ancho
y la resoluci´on de la red, lo que conduce a un mejor rendimiento [26]. EﬃcientNet-B5 ha sido entrenado
con varias im´agenes destacando lotes de agricultura, forestal y deforestaci´on, el tama˜no de entrada de las
im´agenes es 456*456*3, pero la cualidad de este modelo B5 radica en que las im´agenes de entrada pueden
diferir del tama˜no original, para poder trabajar con im´agenes m´as peque˜nas y acelerar su procesamiento,
diferenci´andolo de sus versiones anteriores es uno de los modelos m´as actuales de la familia EﬃcientNet,
tomando en cuenta de que estas caracter´ısticas nos permiten entrenar de forma ´optima al modelo lo que la
familia de B6 a B7 generaba problemas de overﬁtting y en la familia de B0 a B4 se relacionaba problemas
de underﬁtting [27], eso convierte a EﬃcientNet-B5 en un modelo ideal para adaptarlo a nuestro problema
de clasiﬁcaci´on de im´agenes. La Figura 8 presenta un esquema de esta arquitectura:

Figura 8. Arquitectura de EﬃcientNet-B5. Tomado de: Zhang. P, 2020
[28].

Esta arquitectura se basa en una t´ıpica red neuronal convolucional (CNN), cuyo dise˜no toma en cuenta
una capa de entrada con los valores de las im´agenes de 224*224*3, la capa de entrada (Input) toma en
cuenta el tama˜no del lote, alto, ancho y tipo de canal que para el caso pr´actico ser´a RGB conjuntamente
con las probabilidades predeterminadas para cada clase [29]. Para adaptarlo a nuestro problema, es posible

12

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

desconectar el clasiﬁcador del modelo predeterminado EﬃcientNet-B5 para obtener solamente la parte con-
volucional de extracci´on de sus caracter´ısticas. El funcionamiento es el siguiente: la capa de entrada (Input)
toma los valores predeterminados de las im´agenes de entrada (224*224*3), posteriormente en las capas de
convoluci´on que en la arquitectura de EﬃcientNetB5 son 7 se toman grupos de p´ıxeles cercanos de la imagen
de entrada y se van operando matem´aticamente contra una matriz peque˜na llamada Kernel con un tama˜no
de (3*3*3) o (5*5*3), que recorre todas las neuronas de entrada y genera una nueva matriz de salida con la
mitad de ancho y largo respecto a su valor original (224*224 a 112*112) y as´ı sucesivamente hasta tener un
valor de salida de (14*14) este proceso denominado subsampling en el que reduciremos el tama˜no de nuestras
im´agenes ﬁltradas pero en donde deber´an prevalecer las caracter´ısticas m´as importantes que detect´o cada
ﬁltro, donde el tipo m´as usado de subsampling es el Max-Pooling. Finalmente, mediante el uso de la capa
AvgPool2D donde toma los valores promedios de los mapas de caracter´ısticas donde esos valores pasar´an
al clasiﬁcador y mediante nuestra capa de salida se obtendr´an las clases y gracias a la funci´on ”Softmax”se
escoger´a el valor mayor de la predicci´on.

4. Experimentos

En esta secci´on describimos la parte experimental del proyecto, donde se llevan a cabo los entrenamientos
de los modelos de s´uper-resoluci´on y clasiﬁcaci´on utilizando los respectivos datasets. Utilizamos una platafor-
ma computacional local y otra online debido a la capacidad de almacenaiento y memoria RAM que nos ofrece
la plataforma Google Colaboratory. Para el caso del modelo Real-ESRGAN, utilizamos una computadora de
tipo port´atil con un procesador Ryzen 5 5600x con 6 n´ucleos y 12 hilos a 3200 MHz, una memoria RAM de
32GB, GPU nVIDIA Geforce RTX 3060 de 12 GB de memoria y un disco duro de estado s´olido de 1TB.
Seg´un la recomendaci´on de los autores se utiliz´o el entorno de desarrollo de Visual Studio Code, el cual es
un editor de programaci´on multi-plataforma desarrollado por Microsoft. Por su parte, para el entrenamiento
del modelo convolucional caneCONV ID, se realiz´o en la plataforma on-line de hardware y software para
aprendizaje autom´atico de acceso gratuito que brinda 12 GB de RAM y 50 GB de almacenamiento en disco.
Los c´odigos y comandos que se ejecuten mediante los notebooks de Colab usan GPU y TPU (Graphics Pro-
cessing Unit y Tensor Processing Unit, respectivamente), que es un circuito integrado de aplicaci´on espec´ıﬁca
y acelerador de Inteligencia Artiﬁcial, que no afecta el rendimiento de la computadora [30].

4.1. Entrenamiento

4.1.1. Real-ESRGAN

Como ya se mencion´o previamente, para los modelos en cuesti´on se aplic´o la t´ecnica de transfer learning.
Siendo conceptualmente estrictos, para el modelo Real-ESRGAN se realiz´o un Fine-tuning al modelo ya
entrenado por los autores. El Fine-tuning es una t´ecnica com´un para el aprendizaje por transferencia. Esta
t´ecnica resulta beneﬁciosa para entrenar nuevos algoritmos de aprendizaje profundo, cuando el conjunto de
datos de un modelo existente y el nuevo modelo de aprendizaje profundo son similares entre s´ı. El ﬁne tuning
toma un modelo que ya ha sido entrenado para una tarea en particular y luego lo ajusta o modiﬁca para
que realice una segunda tarea similar [31]. Para el entrenamiento de nuestro dataset primero se debi´o clonar
el repositorio del modelo y almacenarlo dentro de una variable llamada Model1, Posterior a ello, a trav´es de
la instrucci´on Model1.ﬁt() se procedi´o a inicializar el entrenamiento. Los hiper-par´ametros establecidos para
el entrenamiento del dataset son: un batch size de 48, softmax como loss function, el optimizador Adam, un
n´umero de 500 iteraciones o ´epocas y una tasa de aprendizaje de 0.0001.

Preprint

13

Figura 9. Curvas de aprendizaje para el modelo Real-ESRGAN. Se observa un muy buen ajuste en entrenamiento y
validaci´on. No son apreciables problemas de overﬁtting, ni underﬁtting demostrando que el modelo podr´a tener un
´optimo desempe˜no.

Este proceso tom´o aproximadamente 34 horas continuas y la evoluci´on del mismo se presenta en la Figura
9, donde alcanz´o su mejor precisi´on en la ´epoca 150 con un valor de precisi´on de 0.97 y un valor de p´erdida
de 0.12.

4.1.2. CNN

El uso de transfer-learning y ﬁne-tuning nos ayuda a un entrenamiento m´as r´apido del modelo sobre el
dataset de entrenamiento el cual contiene 1280 im´agenes, debido a que solo entrenamos las ´ultimas 3 capas
del modelo clasiﬁcador de EﬃcienteNet-B5. Los hiper-par´ametros fueron: un batch size de 32, 5 ´epocas, con
una funci´on de perdida Softmax, el optimizador Adam, una tasa de aprendizaje de 0.001; seg´un (Zhang. P,
2020) [28] recomienda que la tasa de aprendizaje debe ser menor a 0.01 para el uso de transfer learning.

Figura 10. Curvas de aprendizaje de precisi´on y p´erdida para los conjuntos de entrenamiento y validaci´on.

Este procedimiento tom´o aproximadamente 1 hora en ejecutarse completamente, la Figura 10 muestra
que el entrenamiento alcanza altos valores de precisi´on desde la primera ´epoca con 0.9775, incrementando
hasta llegar a un pico en la ´epoca 3 donde se estandariza con 0.9850 hasta las ´epoca 5, indicando que el ajuste
entre el entrenamiento y la validaci´on son correctos y no presentan problemas de overﬁtting ni underﬁtting,
permitiendo as´ı que el modelo sea adaptable al reconocimiento de problemas de despoblaci´on y poblaci´on en
diferentes tipos de cultivos.

14

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

4.2. Evaluaci´on

4.2.1. Real-ESRGAN

La mejora de la imagen o la mejora de la calidad visual de una imagen digital puede ser subjetiva. Decir
que un m´etodo proporciona una imagen de mejor calidad puede variar de persona a persona. Por esta raz´on,
es necesario establecer medidas cuantitativas emp´ıricas para comparar los efectos de los algoritmos de mejora
de imagen sobre la calidad de ´esta [22]. El PSNR (Peak-signal to noise ratio) es una m´etrica para evaluar
dicha caracter´ıstica, y es la relaci´on entre la potencia m´axima posible de una imagen, en cuanto a su se˜nal y
la potencia del ruido corruptor (distorsiones) que afectan la calidad de su representaci´on [32]. Para estimar
el PSNR de una imagen, es necesario comparar una imagen de baja calidad con su par de alta calidad a su
m´axima potencia posible.

La se˜nal de una fotograf´ıa se reﬁere al detalle de representaci´on de cada p´ıxel que compone la imagen,
mientras que el ruido es una alteraci´on arbitraria de p´ıxeles que no se corresponden con la luminancia y
tonalidad real de la fotograf´ıa y que son apreciables a simple vista dado el tama˜no que tienen [33]. Debido
a que varias se˜nales pueden tener un rango din´amico muy amplio, el PSNR es usualmente representado
en t´erminos de una escala de decibelios logar´ıtmica [9]. Los decibelios de manera general son una unidad
que permiten expresar una relaci´on entre dos potencias, en este caso la se˜nal y el ruido de la imagen. El
fundamento nos dice que mientras m´as alto sea el valor de PSNR, mayor ser´a la calidad de la imagen
reconstruida o re-escalada. Para el c´alculo del valor de PSNR se utiliz´o la librer´ıa de OpenCvSharp, a trav´es
de las siguientes instrucciones donde img1 representa la imagen de baja resoluci´on e img2 representa su par
de baja resoluci´on:

import cv2
img1 = cv2.imread(’img1.jpg’)
img2 = cv2.imread(’img2.jpg’)
psnr = cv2.PSNR(img1, img2)

Los valores t´ıpicos del PSNR en la compresi´on de imagen y video con p´erdida est´an entre 30 y 50 dB.
Este par´ametro estar´a en funci´on de la profundidad de color de la imagen, misma que se reﬁere al n´umero
de bits necesarios para codiﬁcar y guardar la informaci´on de color de cada p´ıxel en una imagen. El c´alculo
del PSNR se realiz´o para cada ´epoca durante el entrenamiento. Posteriormente se realiz´o un promedio para
los valores de PSNR obtenidos durante la ´epoca 1 a 300 y entre la ´epoca 300 y 500 para tres per´ıodos de
entrenamiento obteniendo los resultados expuestos en la Tabla 3.

Periodo de
entrenamiento

1
2
3

Promedio de PSNR
en dB para ´epocas
1-300
30.91
30.92
33.69

Promedio de PSNR
en dB para ´epocas
300-500
30.00
31.63
34.72

Mejora de la
Imagen(re-
escalamiento) %
0.39 %
2.29 %
15.72 %

Cuadro 3. Distribuci´on promedios de PSNR durante el entrenamiento

Se puede observar que durante el tercer per´ıodo de entrenamiento, a partir de la ´epoca 300 se logra
alcanzar el valor m´as alto de PSNR (34.72 dB), por tanto, podr´ıamos decir que es desde este punto donde se
comienza a tener un mejor resultado en cuanto a la validaci´on de la calidad de las im´agenes usadas para el
entrenamiento. De esta manera podr´ıamos asociar un mejor resultado del entrenamiento despu´es de al menos
un per´ıodo de entrenamiento inicial, sin embargo por recomendaci´on de los autores, es necesario realizar al
menos tres per´ıodos de entrenamiento para observar una estabilizaci´on de los valores de PSNR. No obstante,
generar m´as periodos de entrenamiento podr´ıa o no mejorar los resultados.

4.2.2. CNN

Tomando en cuenta que el terreno no presenta texturas ideales en las fotograf´ıas a´ereas, se ha delimitado
netamente la zona de cultivo, y evaluado la mejor edad de la ca˜na para realizar las pruebas (45 d´ıas a

Preprint

15

partir de cosecha) permitiendo observar las zonas realmente despobladas debido al crecimiento de la ca˜na,
obteniendo una respuesta positiva de aprendizaje. Las condiciones de evaluaci´on son consideradas a partir
de las im´agenes de prueba (test), se deﬁne una variable que estandariza las dimensiones de la imagen y
un modelo tipo categ´orico. Evaluamos mediante una matriz de confusi´on que es una herramienta ´util para
visualizar el desempe˜no del modelo en cuanto a la predicci´on con im´agenes de test, mismas que previamente
fueron divididas en las 2 categor´ıas de Zonas pobladas y Zonas despobladas manualmente. As´ı nos ayuda
a visualizar los aciertos y errores dentro del conjunto test, compara las im´agenes predichas por el modelo
contra las im´agenes clasiﬁcadas manualmente, obteniendo que el modelo no se equivoca en predecir las dos
categor´ıas teniendo una total precisi´on (Figura 11).

Figura 11. Matriz de confusi´on para evaluar el modelo de categorizaci´on de zonas de cultivo. Elaborado por: Autores.

Deﬁniendo de esta forma en la predicci´on con el 20 % del dataset utilizado y categorizado en test, la

comparativa sobre la imagen original y el modelo de predicci´on para cada una de ellas (Figura 11).

Figura 12. Comparaci´on entre la clasiﬁcaci´on original y la predicci´on del modelo. Se tiene una matriz de 3x6 en la
que se han escogido im´agenes aleatorias del dataset de test, y previamente categorizadas como zonas despobladas y
zonas cultivadas, sobre la descripci´on de cada una encontramos la predicci´on del modelo en la cual podemos observar
que la categorizaci´on del mismo tiene alta precisi´on con la realidad. Elaborado por: Autores.

16

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

5. Resultados y discusi´on

Para el c´alculo de ´areas cultivadas y ´areas despobladas, y conocer sus respectivos porcentajes dentro de
una fotograf´ıa a´erea, una vez realizado el aumento de resoluci´on y la clasiﬁcaci´on de la imagen, seguimos el
procedimiento ilustrado en la Figura 13, apoyados en el uso de las librer´ıas para el tratamiento de im´agenes
como Numpy, CV2, y Skimage.

Figura 13. Diagrama ilustrativo para el c´alculo de ´areas despobladas. Elaborado por: Autores.

1. Cargar la imagen previamente clasiﬁcada como zona despoblada, ya que al cargar una zona totalmente
poblada el resultado sera de 100 % de poblaci´on obteniendo un dato innecesario para la resoluci´on del
conﬂicto, las fotograf´ıas se obtienen en formato JPG reescaladas a un tama˜no de 224x224 mediante el
modelo de clasiﬁcaci´on de fotograf´ıas con el uso de Eﬃcientnet.

2. Dicha imagen es convertida a un arreglo de n´umeros modiﬁcada con el uso de la librer´ıa r.shape, trans-

formando la fotograf´ıa de 3 tres canales RGB a un solo canal en escala de grises.

3. Realizamos una segmentaci´on de la imagen mediante la librer´ıa de c´odigo abierto de matplotlib cmap,
variando la luminosidad de los colores representando el inicio y ﬁn de los mismos, permitiendo as´ı deﬁnir
la saturaci´on adecuada en la imagen, cuyo umbral podr´a ser modiﬁcado a criterio del usuario.

4. Una vez deﬁnida la saturaci´on en la imagen y al ser capaces de aplicar el concepto de umbral global,
gracias al subm´odulo SciPyndimage se procede a remodelar la imagen tomando un valor de p´ıxel y
us´andolo como umbral para distinguir los objetos del fondo, de los objetos del primer plano en una
imagen.

5. Segmentaci´on manual: El valor del umbral puede especiﬁcarse entre 0-10 manualmente, si el valor del
p´ıxel es mayor que nuestro umbral, podemos decir que pertenece a un objeto, representando la regi´on
m´as luminosa (despoblaci´on), caso contrario si su valor es menor se tratar´a como fondo, representando
la regi´on m´as oscura (poblaci´on). Con ello la imagen se segmentar´a en dos grandes zonas: despoblaci´on

(regi´on blanca) y poblaci´on (regi´on negra), sin embargo, hay limitaciones en este enfoque. Cuando no
tenemos un contraste signiﬁcativo en la escala de grises, o hay una superposici´on de los valores de p´ıxeles,
se vuelve muy dif´ıcil obtener precisi´on

6. Para obtener el c´alculo ﬁnal de despoblaci´on en porcentaje, una vez obtenida la imagen el c´alculo se
ejecuta al contar todos los p´ıxeles blancos y todos los negros y dividir cada clasiﬁcaci´on para el total de
los p´ıxeles de la imagen obteniendo un valor porcentual.

Preprint

17

Figura 14. Diagrama de ﬂujo para el c´alculo porcentual de zonas pobladas y cultivadas con programaci´on tradicional.
Elaborado por: Autores.

A trav´es del conteo de p´ıxeles menores al umbral (poblaci´on) y mayores al umbral (despoblaci´on), se logra
determinar la cantidad de p´ıxeles que corresponden a cada regi´on de la imagen. lo cual es posible gracias
a la librer´ıa numpy que permitir´a el c´alculo y la suma de aquellos p´ıxeles iguales a 0 tomados en cuenta
como zonas cultivadas y mayores que 0 se sumar´an en zonas despobladas, ´esto haciendo referencia a p´ıxeles
con color. Para determinar el porcentaje de cada regi´on con respecto al ´area total de la imagen, se divide la
cantidad de los p´ıxeles obtenidos para el n´umero total de los mismos, con ello se logra obtener el porcentaje
que ocupa cada regi´on (poblaci´on y despoblaci´on) con respecto al ´area total (Figura 14).

Los valores de precisi´on obtenidos para los modelos entrenados muestran excelente desempe˜no, por tanto
es posible pasar a la etapa de predicci´on, donde se usaron im´agenes fuera del dataset, siguiendo el orden del

18

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

ﬂujo de trabajo. En este sentido, fue importante tomar una sola imagen para procesarla con cada uno de los
modelos propuestos en este trabajo.

Para ﬁnes pr´acticos, se proces´o una imagen a la cual redujimos su resoluci´on, para conocer la efectividad
del modelo de s´uper-resoluci´on. Adem´as, para esta imagen la distribuci´on de poblaci´on y despoblaci´on de
cultivos ya era conocida por el departamento de agricultura de precisi´on de la compa˜n´ıa Valdez. As´ı se pudo
tener un marco de referencia para corroborar la precisi´on de nuestros resultados.

El c´odigo de inferencia para el modelo de Real-ESRGAN fue probado a trav´es de la plataforma de Google
Collaboratory, y fue llevado a cabo a trav´es de 5 etapas. La primera etapa es referida a la importaci´on de
las librer´ıas necesarias tales como son: BasicSR, GfpGan y facelibx, aqu´ı tambi´en se descarga el modelo de
Real-ESRGAN pre-entrenado. La segunda etapa comprende la subida o carga de las im´agenes nuevas para el
modelo, lo cual se lo realiza con la ayuda del m´odulo OS y la librer´ıa de Shutil. De esta manera podemos crear
un widget que nos permite insertar im´agenes desde nuestro equipo para que estas puedan ser mejoradas.

A continuaci´on, en la tercera etapa ejecutamos el compilador a trav´es de la siguiente l´ınea de comando:

!python inferencerealesrgan.py -n RealESRGANx4plus -i upload --outscale 4 -half

Donde: -n, –modelname: Nombre del modelo, -i, –input: direcci´on de la carpeta de la imagen cargada,
–outscale: Factor de re-escalado de la imagen (este ´ultimo factor puede ser expresado hasta un valor de 4, que
es el valor de re-escalamiento m´aximo del modelo). En la siguiente etapa podemos tener una pre-visualizaci´on
(Figura 15). Finalmente, podremos descargar el resultado en formato ZIP dentro de nuestro equipo.

Figura 15. Comparaci´on entre una imagen sin procesar vs. una imagen procesada con el modelo de Real-ESRGAN.

Posteriormente, con la resoluci´on de la imagen mejorada, es posible continuar con el siguiente paso dentro
del ﬂujo de trabajo. De esta forma la imagen optimizada entra al proceso de predicci´on de la zona poblada
de cultivo y la despoblada de cultivo (Figura 16), en el que se deﬁne el porcentaje de veracidad que tiene el
modelo para poder reconocer si es una zona cultivada o una zona despoblada.

Preprint

19

Figura 16. Predicci´on de una imagen de prueba fuera del dataset original. En la imagen se puede observar que el
modelo ha predicho que un 12 % se ajusta a ser una imagen de zona cultivada mientras que un 87 % la ha reconocido
como zona despoblada evidenciando as´ı la precisi´on del modelo. Elaborado por: Autores.

No obstante, se debe aclarar que el modelo arroja la probabilidad que la imagen pertenece a una Zona
Despoblada y no son los porcentajes de cultivo o no cultivo, tambi´en se debe realizar una estandarizaci´on
de par´ametros con el objetivo de trabajar bajo los t´erminos adecuados en cuanto al tama˜no de la imagen.
Se obtiene el porcentaje de precisi´on al reconocer zonas pobladas de zonas despobladas mediante el modelo
de predicci´on (Figura 17).

Figura 17. Proceso de umbralizaci´on de la imagen para el c´alculo porcentual de superﬁcies de poblaci´on y despobla-
ci´on de cultivos de ca˜na. Elaborado por: Autores.

Finalmente, a trav´es del c´odigo para el c´alculo porcentual logramos determinar el porcentaje que le
corresponde a las ´areas pobladas y despobladas de cultivo reconocidas previamente, veriﬁcando de esta
manera que nuestro ﬂujo de trabajo trabaja de manera ´optima con im´agenes fuera del dataset y corroborando
as´ı los resultados obtenidos durante el entrenamiento.

6. Conclusiones

A trav´es de la implementaci´on de la Inteligencia Artiﬁcial y m´as concretamente las redes neuronales, es
posible dar soluci´on a problem´aticas que actualmente pueden signiﬁcar altos costos y tiempos de espera para
el procesamiento de informaci´on. En nuestro caso hemos logrado dise˜nar un ﬂujo de procesos destinados al

20

Javier Caicedo, Pamela Acosta, Romel Pozo, Henry Guilcapi, and Christian Mejia-Escobar

tratamiento de im´agenes a´ereas, con la ﬁnalidad de calcular las ´areas de poblaci´on y despoblaci´on de cultivos
en los predios pertenecientes a la Compa˜n´ıa Azucarera Valdez S.A.

Los modelos adoptados as´ı como su arquitectura, fueron entrenados con los datasets creados por nosotros.
De esta manera, aplicando las t´ecnicas de transfer learning y Fine-tuning pudimos adaptarlos a nuestras
necesidades de cara a enfrentar la problem´atica propuesta.

La utilizaci´on de las GANs, en el caso de Real-ESRGAN y las redes convolucionales para el modelo
propuesto por nosotros, CaneCONV ID, permiten mejorar la resoluci´on de im´agenes afectadas por fallas en
el proceso de captura de la fotograf´ıa e identiﬁcar superﬁcies de poblaci´on y despoblaci´on de los cultivos
de ca˜na, respectivamente, evidenciando que el c´odigo utilizado durante todo el proceso funciona de forma
´optima.

Para el proceso ﬁnal de determinar el porcentaje de poblaci´on y despoblaci´on en las im´agenes a´ereas, se
utiliza programaci´on tradicional, de esta manera se puede lograr encontrar el umbral ´optimo entre estas dos
zonas de evaluaci´on. Mediante el conteo de p´ıxeles y acorde a la clasiﬁcaci´on se tiene a las zonas cultivadas,
con p´ıxeles de m´axima saturaci´on tomando una coloraci´on oscura y zonas despobladas con p´ıxeles de m´ınima
saturaci´on tomando una coloraci´on blanca y ﬁnalmente se deﬁnir´a una relaci´on en base al ´area total de la
imagen para expresar cada par´ametro porcentualmente.

Con los resultados obtenidos se determin´o que los modelos son totalmente funcionales para el objetivo
planteado. En primera instancia, la metodolog´ıa adoptada podr´ıa adecuarse a otros problemas similares como
el reconocimiento de malezas, plagas y enfermedades de cultivos, con el ﬁn de optimizar par´ametros en su
an´alisis.

El c´alculo de despoblaci´on con programaci´on tradicional podr´ıa ser mejorado mediante el desarrollo de

modelos a partir de redes neuronales que permitan deﬁnir el valor porcentual de forma autom´atica.

7. Agradecimientos

Deseamos extender nuestros agradecimientos a la Empresa Azucarera Valdez S.A., y al Ministerio de
Ganader´ıa y Agricultura, a trav´es de su portal SIGTIERRAS, por proporcionarnos la informaci´on necesaria
para la elaboraci´on de nuestros datasets. De la misma manera al Ing. Christian Mej´ıa, quien nos gui´o durante
toda la elaboraci´on del presente estudio.

Referencias

[1] Felipe Fern´andez Garcı´a y col. Introducci´on a la fotointerpretaci´on. Ariel, 2000.
[2] Enrique Matarredona Coll. (cid:40)(cid:40)Aplicaci´on de la fotografı´a a´erea en la cartografı´a de suelos(cid:41)(cid:41). En: Investi-

gaciones geogr´aﬁcas, nº 3, 1985; pp. 7-30 (1985).

[3] Ver´onica Perales Blanco y Fred Adam. (cid:40)(cid:40)Integraci´on de GIS (sistemas de georreferenciaci´on de la
informaci´on) y localizaci´on espacial en pr´acticas pedag´ogicas y l´udicas vinculadas a museos(cid:41)(cid:41). En: Arte,
individuo y sociedad 25.1 (2013), p´ags. 121-133.

[4] Glen Jhan Pierre Restrepo Arteaga y col. (cid:40)(cid:40)Aplicaci´on del aprendizaje profundo (deep learning) al

procesamiento de se˜nales digitales(cid:41)(cid:41). En: (2015).

[5] Laura Randa Calcagni. (cid:40)(cid:40)Redes Generativas Antag´onicas y sus aplicaciones(cid:41)(cid:41). Tesis doct. Universidad

Nacional de La Plata, 2020.

[6] Filip Loncaric y col. (cid:40)(cid:40)La integraci´on de la inteligencia artiﬁcial en el abordaje clı´nico del paciente:
Enfoque en la imagen cardiaca(cid:41)(cid:41). En: Revista Espa˜nola de Cardiologı´a 74.1 (2021), p´ags. 72-80.
´Oscar Picazo Montoya. (cid:40)(cid:40)Redes Neuronales Convolucionales Profundas para el reconocimiento de emo-
ciones en im´agenes(cid:41)(cid:41). Tesis doct. ETSI Informatica, 2018.

[7]

[8] Juan Pablo Ospina-Guti´errez y Edier Aristiz´abal. (cid:40)(cid:40)Aplicaci´on de inteligencia artiﬁcial y t´ecnicas de
aprendizaje autom´atico para la evaluaci´on de la susceptibilidad por movimientos en masa(cid:41)(cid:41). En: Revista
Mexicana De Ciencias Geol´ogicas 38.1 (2021), p´ags. 43-54.
´Etienne Clabaut y col. (cid:40)(cid:40)Model Specialization for the Use of ESRGAN on Satellite and Airborne
Imagery(cid:41)(cid:41). En: Remote Sensing 13.20 (2021), p´ag. 4044.

[9]

Preprint

21

[10] Roberth Joel Ald´as-N´u˜nez y col. (cid:40)(cid:40)Delimitaci´on autom´atica de ceniza volc´anica en im´agenes satelitales
mediante Deep Learning(cid:41)(cid:41). En: FIGEMPA: Investigaci´on y Desarrollo 13.1 (2022), p´ags. 48-58.
[11] Maayan Frid-Adar y col. (cid:40)(cid:40)GAN-based synthetic medical image augmentation for increased CNN per-

formance in liver lesion classiﬁcation(cid:41)(cid:41). En: Neurocomputing 321 (2018), p´ags. 321-331.

[12] Antoni Perez-Poch. (cid:40)(cid:40)Mejora diagn´ostica de hepatopatı´as de afectaci´on difusa mediante t´ecnicas de

inteligencia artiﬁcial(cid:41)(cid:41). En: (2011).

[13] Yang Li y Xuewei Chao. (cid:40)(cid:40)ANN-based continual classiﬁcation in agriculture(cid:41)(cid:41). En: Agriculture 10.5

(2020), p´ag. 178.

[14] Jesus Enrique Piscoya Ferre˜nan. (cid:40)(cid:40)Sistema de visi´on artiﬁcial para apoyar en la identiﬁcaci´on de plagas

y enfermedades del cultivo de sandı´a en el distrito de ferre˜nafe(cid:41)(cid:41). En: (2019).

[15] David Britez y col. (cid:40)(cid:40)Exploratory approach of neural networks applied to orthomosaics for detection of
tires as possible larval foci(cid:41)(cid:41). En: 2021 IEEE CHILEAN Conference on Electrical, Electronics Enginee-
ring, Information and Communication Technologies (CHILECON). IEEE. 2021, p´ags. 1-5.
Iv´an Jos´e Badenes Villena. (cid:40)(cid:40)Conceptualizaci´on, implementaci´on y desarrollo de un coche aut´onomo
desde cero mediante machine learning(cid:41)(cid:41). Tesis doct. Universitat Polit`ecnica de Val`encia, 2022.
[17] Datascientest. ¿Qu´e es el Transfer Learning? https : / / datascientest . com / es / que - es - el -

[16]

transfer-learning. [Online; accessed 19-July-2022]. 2022.

[18] Diego Heras. (cid:40)(cid:40)Clasiﬁcador de im´agenes de frutas basado en inteligencia artiﬁcial Fruit image classiﬁer

based on artiﬁcial intelligence(cid:41)(cid:41). En: Revista Killkana T´ecnica. Vol 1.2 (2017).

[19] Xintao Wang y col. (cid:40)(cid:40)Real-esrgan: Training real-world blind super-resolution with pure synthetic data(cid:41)(cid:41).
En: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021, p´ags. 1905-1914.
[20] Bin Sun y col. (cid:40)(cid:40)Hybrid Pixel-Unshuﬄed Network for Lightweight Image Super-Resolution(cid:41)(cid:41). En: arXiv

preprint arXiv:2203.08921 (2022).

[21] Jeﬀerson Hern´an Sarango Vega. (cid:40)(cid:40)Redes neuronales convulucionales para mejorar la resoluci´on de im´age-

nes medicas(cid:41)(cid:41). B.S. thesis. 2022.

[22] Anaı¨s Gastineau y col. (cid:40)(cid:40)A residual dense generative adversarial network for pansharpening with geo-
metrical constraints(cid:41)(cid:41). En: 2020 IEEE International Conference on Image Processing (ICIP). IEEE.
2020, p´ags. 493-497.

[23] Hua Wang y col. (cid:40)(cid:40)Deformable non-local network for video super-resolution(cid:41)(cid:41). En: IEEE Access 7 (2019),

p´ags. 177734-177744.

[24] Aitor Gonz´alez Marﬁl. (cid:40)(cid:40)M´etodos de aprendizaje profundo para la s´uper-resoluci´on y segmentaci´on

sem´antica de im´agenes(cid:41)(cid:41). En: (2021).

[25] Edgar Schonfeld, Bernt Schiele y Anna Khoreva. (cid:40)(cid:40)A u-net based discriminator for generative adver-
sarial networks(cid:41)(cid:41). En: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2020, p´ags. 8207-8216.

[26] Mingxing Tan y Quoc Le. (cid:40)(cid:40)Eﬃcientnet: Rethinking model scaling for convolutional neural networks(cid:41)(cid:41).

En: International conference on machine learning. PMLR. 2019, p´ags. 6105-6114.

[27] Karar Ali y col. (cid:40)(cid:40)Multiclass Skin Cancer Classiﬁcation using EﬃcientNets–A First Step towards Pre-

venting Skin Cancer(cid:41)(cid:41). En: Neuroscience Informatics (2021), p´ag. 100034.

[28] Pan Zhang, Ling Yang y Daoliang Li. (cid:40)(cid:40)EﬃcientNet-B4-Ranger: A novel method for greenhouse cu-
cumber disease recognition under natural complex environment(cid:41)(cid:41). En: Computers and Electronics in
Agriculture 176 (2020), p´ag. 105652.

[29] Juan Jos´e Cabrera y col. (cid:40)(cid:40)Entrenamiento, optimizaci´on y validaci´on de una CNN para localizaci´on
jer´arquica mediante im´agenes omnidireccionales.(cid:41)(cid:41) En: XLII Jornadas de Autom´atica. Universidade da
Coru˜na, Servizo de Publicaci´ons. 2021, p´ags. 640-647.

[30] Miguel Acosta. (cid:40)(cid:40)Inteligencia artiﬁcial: la cibern´etica del ser vivo y de la m´aquina(cid:41)(cid:41). En: Naturaleza y

Libertad. Revista de estudios interdisciplinares 12 (2019).

[31] Ekkis A. Fine Tuning. https://www.quora.com/difference- between- training- and- tuning- 1.

[Online; accessed 19-July-2022]. 2019.

[32] Molinari M. Fotograf´ıa digital. https://www.molinaripixel.com.ar/2007/10/23/la- relacion-

senal-ruido-en-fotografia-digital/. [Online; accessed 19-July-2022]. 2007.

[33] Zafra D. Ruido en fotograf´ıa. https://capturetheatlas.com/es/ruido-en-fotografia/. [Online;

accessed 19-July-2022]. 2019.

