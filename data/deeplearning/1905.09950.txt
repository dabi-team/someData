9
1
0
2

v
o
N
2
1

]

G
L
.
s
c
[

4
v
0
5
9
9
0
.
5
0
9
1
:
v
i
X
r
a

Zero-shot task adaptation by homoiconic
meta-mapping

Andrew K. Lampinen
Department of Psychology
Stanford University
lampinen@stanford.edu

James L. McClelland
Department of Psychology
Stanford University
mcclelland@stanford.edu

Abstract

How can deep learning systems ﬂexibly reuse their knowledge? Toward this
goal, we propose a new class of challenges, and a class of architectures that can
solve them. The challenges are meta-mappings, which involve systematically
transforming task behaviors to adapt to new tasks zero-shot. The key to achieving
these challenges is representing the task being performed in such a way that this
task representation is itself transformable. We therefore draw inspiration from
functional programming and recent work in meta-learning to propose a class of
Homoiconic Meta-Mapping (HoMM) approaches that represent data points and
tasks in a shared latent space, and learn to infer transformations of that space.
HoMM approaches can be applied to any type of machine learning task. We
demonstrate the utility of this perspective by exhibiting zero-shot remapping of
behavior to adapt to new tasks.

1

Introduction

Humans are able to use and reuse knowledge more ﬂexibly than most deep learning models can
[32, 39]. The problem of rapid learning has been partially addressed by meta-learning systems
[50, 18, 20, 53, 9, see also Section 7]. However, humans can use our knowledge of a task to ﬂexibly
adapt when the task changes. In particular, we can often perform an altered task zero-shot, that is,
without seeing any data at all. For example, once we learn to play a game, we can immediately switch
to playing in order to lose, and can perform reasonably on our ﬁrst attempt.

One fundamental reason for this is that humans are aware of what we are trying to compute and why.
This allows us to adapt our task representations to perform an altered task. By contrast, most prior
work on zero-shot learning relies on generating a completely new representation for a new task, e.g.
from a natural language description [51, 42, e.g.]. These systems must therefore throw away much of
their prior knowledge of the task and regenerate it from scratch, rather than just transforming what
needs to be transformed. Other approaches use relationships between tasks as an auxiliary learning
signal [58, 43, e.g.], but cannot adapt to a new task by transforming their task representations. To
address this, it is necessary to represent tasks in a transformable space. This can grant the ability to
rapidly and precisely adapt behavior to a new task.

In this paper, we propose a new class of tasks based on this idea: meta-mappings, i.e. mappings
between tasks (see below). Meta-mappings allow zero-shot performance of a new task based on
its relationship to old tasks. To address the challenge of performing meta-mappings, we propose
using architectures which essentially take a functional perspective on meta-learning, and exploit
the idea of homoiconicity. (A homoiconic programming language is one in which programs in the
language can be manipulated by programs in the language, just as data can.) By treating both data
and task behaviors as functions, we can conceptually think of both data and learned task behaviors as

Learning Transferable Skills Workshop (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
transformable. This yields the ability to not only learn to solve new tasks, but to learn how to transform
these solutions in response to changing task demands. We demonstrate that our architectures can
ﬂexibly remap their behavior to address the meta-mapping challenge. By allowing the network to
recursively treat its task representations as data points, and transform them to produce new task
representations, our approach is able to achieve this ﬂexibility parsimoniously. We suggest that
approaches like ours will be key to building more intelligent and ﬂexible deep learning systems.

2 Meta-mapping

We propose the meta-mapping challenge. We deﬁne a meta-mapping as a task, or mapping, that
takes a task as an input, output, or both. These include mapping from tasks to language (explaining),
mapping from language to tasks (following instructions), and mapping from tasks to tasks (adapting
behavior). While the ﬁrst two categories have been partially addressed in prior work [e.g. 24, 14], the
latter is more novel. (We discuss the relationship between our work and prior work in Section 7.)
This adaptation can be cued in several ways, including examples of the mapping (after winning and
losing at poker, try to lose at blackjack) or natural-language instructions (“try to lose at blackjack”).

We argue that task-to-task meta-mappings are a useful way to think about human-like ﬂexibility,
because a great deal of our rapid adaptation is from a task to some variation on that task. For
example, the task of playing go on a large board is closely related to the task of playing go on
a small board. Humans can exploit this to immediately play well on a different board, but deep
learning models generally have no way to achieve this. We can also adapt in much deeper ways,
for example fundamentally altering our value function on a task, such as trying to lose, or trying to
achieve some orthogonal goal. While meta-learning systems can rapidly learn a new task from a
distribution of tasks they have experience with, this does not fully capture human ﬂexibility. Given
appropriate conditioning (see below), our architecture can use meta-mappings to adapt to substantial
task alterations zero-shot, that is, without seeing a single example from the new task [32]. Achieving
this ﬂexibility to meta-map to new tasks will be an important step toward more general intelligence –
intelligence that is not limited to precisely the training tasks it has seen.

3 Homoiconic meta-mapping (HoMM) architecture

To address these challenges, we propose HoMM architectures, composed of two components:

1. Input/output systems: domain speciﬁc encoders and decoders (vision, language, etc.) that

map into a shared embedding space Z.

2. A meta-learning system that a) learns to embed tasks into the shared embedding space Z, b)
learns to use these task embeddings to perform task-appropriate behavior, c) learns to embed
meta-mappings into the same space, and d) learns to use these meta-mapping embeddings to
transform basic task embeddings in a meta-mapping appropriate way.

These architectures are homoiconic because they have a completely shared Z for individual data
points, tasks, and meta-mappings. This parsimoniously allows for arbitrary mappings between these
entities. In addition to basic tasks, the system can learn to perform meta-mappings to systematically
alter task behavior based on relationships to other tasks. That is, it can transform task representations
using the same components it uses to transform basic data points. (See also Appendix E.1.)

Without training on meta-mappings, of course, the system will not be able to execute them well.
However, as we will show, if it is trained on a broad enough set of such mappings, it will be able to
generalize to new instances drawn from the same meta-mapping distribution. For instances that fall
outside its data distribution, or for optimal performance, it may require some retraining, however.
This reﬂects the structure of human behavior – we are able to adapt rapidly when new knowledge
is relatively consistent with our prior knowledge, but learning an entirely new paradigm (such as
calculus for a new student) can be quite slow [cf. 31, 9].

More formally, we treat functions and data as entities of the same type. From this perspective, the
data points that one function receives can themselves be functions1. The key insight is that then our

1Indeed, any data point can be represented as a constant function that outputs the data point.

2

(a) Basic meta-learning

Loss

Probe targets
(cid:41)
(cid:40)zprb
...

targ,0

Predictions
(cid:40)zpred
(cid:41)
out,0,
...

H

Fzf unc

zf unc

M

(cid:40)(zex

(cid:41)

targ,0)

in,0, zex
...
Examples

(cid:41)

in,0

(cid:40)zprb
...
Probes

(b) Meta-mapping

Loss (train)

Probe targets
(cid:41)
(cid:40)zf unc,prb
...

targ,0

Predictions
(cid:40)zf unc,pred
(cid:41)
,
...

out,0

H

Fzmeta

zmeta

M

(cid:41)

(cid:40)(zf unc

targ,0)

in,0 , zf unc
...
Examples

(cid:41)

in,0

(cid:40)zf unc,prb
...
Probes

(c) Meta-mapping evaluation

Loss (eval)

Probe targets
(cid:41)
(cid:40)zprb
...

targ,0

Predictions
(cid:40)zpred
(cid:41)
out,0,
...

H Fzf unc,pred

zf unc,pred

(cid:41)

in,0

(cid:40)zprb
...
Probes

Figure 1: The HoMM architecture allows for transformations at different levels of abstraction. (a) For
basic meta-learning a dataset consisting of (input embedding, output embedding) tuples is processed
by the meta-network M to produce a function embedding zf unc, which is processed by the hyper
network H to parameterize a function Fzf unc, which attempts to compute the transformation on
probe inputs (used to encourage the system to generalize). However, our approach goes beyond
basic meta-learning. The function embedding zf unc can then be seen as a single input or output
at the next level of abstraction, when the same networks M and H are used to transform function
embeddings based on examples of a meta-mapping (b). To evaluate meta-mapping performance,
a probe embedding of a held-out function is transformed by the architecture to yield a predicted
embedding for the transformed task. The performance of this predicted embedding is evaluated by
moving back down a level of abstraction and evaluating on the actual target task (c). Because the
function embedding is predicted by a transformation rather than from examples, new tasks can be
performed zero-shot. (M and H are learnable deep networks, and Fz is a deep network parameterized
by H conditioned on function embedding z. Input and output encoders/decoders are omitted for
simplicity. See the text and Appendix F.2 for details.)

architecture can transform data points2 to perform basic tasks (as is standard in machine learning),
but it can also transform these task functions to adapt to new tasks. This is related to the concepts
of homoiconicity, deﬁned above, and higher-order functions. Under this perspective, basic tasks
and meta-mappings from task to task are really the same type of problem. The functions at one level
of abstraction (the basic tasks) become inputs and outputs for higher-level functions at the next level
of abstraction (meta-mapping between tasks).

Speciﬁcally, we embed each input, target, or mapping into a shared representational space Z. This
means that single data points are embedded in the same space as the representation of a function or
an entire dataset. Inputs are embedded by a deep network I : input → Z. Model outputs are decoded
from Z by O : Z → output. Target outputs are encoded by T : targets → Z.

Given this, the task of mapping inputs to outputs can be framed as trying to ﬁnd a transformation
of the representational space that takes the (embedded) inputs from the training set to embeddings
that will decode to the target outputs. These transformations are performed by a system with the
following components (see Fig. 1): M : {(Z, Z), ...} → Z – the meta network, which collapses
a dataset of (input embedding, target embedding) pairs to produce a single function embedding.
H : Z → parameters – the hyper network, which maps a function embedding to parameters.
F : Z → Z – the transformation, implemented by a deep network parameterized by H.

2Where “data” is a quite ﬂexible term. The approach is agnostic to whether the learning is supervised or

reinforcement learning, whether inputs are images or natural language, etc.

3

Basic meta-learning: To perform a basic task, input and target encoders (I and T ) are used to
embed individual pairs from an example dataset D1, to form a dataset of example (input, output)
tuples (Fig. 1a). These examples are fed to M, which produces a function embedding (via a deep
neural network, with several layers of parallel processing across examples, followed by an element-
wise max across examples, and several more layers). This function embedding is mapped through
the hyper network H to parameterize F , and then F is used to process a dataset of embedded probe
inputs, and O to map the resultant embeddings to outputs. This system can be trained end-to-end on
target outputs for the probes. Having two distinct datasets forces generalization at the meta-learning
level, in exactly the same way as standard meta-learning algorithms learn to generalize.

More explicitly, suppose we have a dataset of example input, target pairs (D1 = {(x0, y0), ...}), and
some input x from a probe dataset D2. The system would predict a corresponding output ˆy as:

ˆy = O (Fzf unc (I (x)))
where Fzf unc is the meta-learner’s representation for the function underlying the examples in D1:
Fzf unc is parameterized by H (cid:0)zf unc(cid:1) , where zf unc = M ({(I (x0) , T (y0)) , ...})
Then, given some loss function L(y, ˆy) deﬁned on a single target output y and an actual model output
ˆy, we deﬁne our total loss computed on the probe dataset D2 as:

E(x,y)∈D2 [L (y, O (FD1 (I (x))))]
The system can then be trained end-to-end on this loss to adjust the weights of T , H, M, O, and
I. That is, the meta-learning training procedure optimizes the system for generalizing the function
implied by the examples it sees, in order to respond appropriately to probe inputs. As in training a
standard meta-learning model, these probes are held-out from the meta-network, but the labels are
still used to encourage the system to meta-learn to respond appropriately to new inputs. Other data
points are held-out entirely during training, and only used for evaluation, see Appendix A.1 for some
detailed description. See Appendix F.2 for further details on the architecture and hyper-parameters.

Meta-mapping: The fundamental insight of our paper is to show how basic tasks and meta-
mappings can be treated homogenously, by allowing the network to transform its task representations
like data (see Fig. 1b,c). That is, we propose to transform a prior task representation in order to
perform a related new task zero-shot. The relationship by which the prior task embedding should be
transformed to perform the new task is speciﬁed by example tuples of (input, output) task embeddings.
This is precisely analogous to how a basic task is speciﬁed in terms of (input, output) embeddings of
data points, except that the embeddings now represent tasks that are being transformed, rather than
individual data points.

Thus from the perspective of our architecture, learning a meta-mapping between tasks is exactly
analogous to learning a basic task. Anything that is embedded in Z can be transformed using the same
system. Because tasks are embedded in Z for basic meta-learning, this allows for meta-mappings
using exactly the same M and H that we use for basic tasks. Just as we would take a set of paired
embeddings of data points for a basic task, and use them to compute a function embedding for that
task, we can take a set of paired function embeddings representing a meta-mapping, and use them
to create an embedding for that meta-mapping. We can then use this meta-mapping embedding to
transform the embedding of another basic task. We can thus behave zero-shot on a novel task based
on its relationship to a prior task.

For example, suppose we have an embedding zgame1 ∈ Z for the task of playing some game, and we
want to switch to trying to lose this game. We can generate a meta-mapping embedding zmeta ∈ Z
from examples of embeddings generated by the system when it is trying to win and lose various
other games: zmeta = M ({((zgame2, zgame2,lose) , ...}). We can generate a new task embedding
ˆzgame1,lose ∈ Z:

ˆzgame1,lose = Fzmeta (zgame1)

where Fzmeta is parameterized by H (zmeta)

This ˆzgames1,lose can be interpreted as the system’s guess at a losing strategy for game 1. To
train a meta-mapping, we minimize the (cid:96)2 loss in the latent space betwen this guessed embedding
and the embedding of the target task3. Whether or not we have such a target embedding, we can

3The gradients do not update the example function embeddings, only the weights of M and H, due to

memory contraints. Allowing this might be useful in more complex applications.

4

(a) The polynomials domain, Section 4.

(b) The cards domain, Section 5.

Figure 2: The HoMM system succeeds at basic meta-learning, which is a necessary prerequisite for
meta-mappings. (a) The polynomials domain, Section 4. The system successfully generalizes to held
out polynomials. The solid line indicates optimal performance; the dashed line indicates untrained
model performance. (b) The card games domain, Section 5. The system successfully generalizes
to held out games, both when trained on a random sample of half the tasks, or when a targeted
subset is held out. The gray dashed line indicates chance performance (otuputting the mean across
all polynomials, or playing randomly), while the solid lines are optimal performance. The orange
dashed lines shows performance on held-out tasks of playing the strategy from the most correlated
trained task. The system generally exceeds this difﬁcult baseline, thus showing deeper generalization
than just memorizing strategies and picking the closest. Error-bars are bootstrap 95%-CIs, numerical
values for plots can be found in Appendix G.

evaluate how well the system loses with this ˆzgame1,lose strategy, by stepping back down a level
of abstraction and actually having it play the game via this embedding (Fig. 1c). This is how we
evaluate meta-mapping performance – evaluating the loss of transformed task embeddings on the
respective target tasks. Note that this allows for zero-shot performance of the new task, because
it simply requires transforming a prior task embedding.

Alternatively, we could map from language to a meta-mapping embedding, rather than inducing it
from examples of the meta-mapping. This corresponds to the human ability to change behavior in
response to instructions. The key feature of our architecture – the fact that tasks, data, and language
are all embedded in a shared space – allows for substantial ﬂexibility within a uniﬁed framework.
Furthermore, our approach is parsimonious. Because it uses the same meta-learner for both basic
tasks and meta-mappings, this increased ﬂexibility does not require any added parameters.4

4 Learning multivariate polynomials

As a proof of concept, we ﬁrst evaluated the system on the task of learning polynomials of degree
≤ 2 in 4 variables (i.e. the task was to regress functions of the form p : R4 → R where p ∈ P2 (R),
though the model was given no prior inductive bias toward polynomial forms). For example, if
p(w, x, y, z) = x, the model might see examples like (−1, 1, 1, 1; 1) and (0.7, 2.1, 1.3, −4; 2.1), and
be evaluated on its output for points like (−1, −1.3, 0.5, 0.3). This yields an inﬁnite family of base-
level tasks (the vector space of all such polynomials), as well as many families of meta-mappings over
tasks (for example, multiplying polynomials by a constant, squaring them, or permuting their input
variables). This allows us to not only examine the ability of the system to learn to learn polynomials
from data, but also to adapt its learned representations in accordance with these meta-tasks. Details
of the architecture and training can be found in Appendix F.

Basic meta-learning: First, we show that the system is able to achieve the basic goal of learning a
held-out polynomial from a few data points in Fig. 2a (with good sample-efﬁciency, see Supp. Fig.
7).

4At least in principle, in practice of course increasing network size might be more beneﬁcial for HoMM
architectures performing meta-mappings as well as basic tasks, compared to those performing only basic tasks.

5

(a) The polynomials domain, Section 4.

(b) The cards domain, Section 5.

Figure 3: The HoMM architecture performs tasks zero-shot by meta-mappings. (a) The system
generalizes to apply learned meta-mappings to new polynomials, and even to apply unseen meta-
mappings. The plots show the loss produced when evaluating the mapped embedding on the target
task. For example, if the initial polynomial is p(x) = x + 1, and the meta-task is “square,” the loss
would be evaluated by transforming the embedding of p(x) and evaluating how well the mapped
embedding regresses on to p(x)2 = x2 + 2x + 1. The results show that the system succeeds at
applying meta-mappings it is trained on to held-out polynomials, as well as applying held-out meta-
mappings to either trained or held-out polynomials. The solid line indicates optimal performance; the
dashed line is untrained model performance. (b) The system generalizes to meta-mapping new tasks
in the cards domain. The system is trained to do the meta-mappings shown here on a subset of its
basic tasks, and is able to generalize these mappings to perform novel tasks zero-shot. For example,
for the “losers” mapping, the sytem is trained to map games to their losers variants. When given a
held-out game, it is able to apply the mapping to guess how to play the losing variation. This plot
shows the reward produced by taking the mapped embedding and playing the targeted game. The gray
dashed line indicates random performance, while the colored dashed lines indicate performance if the
system did not alter its behavior in response to the meta-mapping. The system generally exceeds these
baselines, although the switch-suits baseline is more difﬁcult with the targeted holdout. Error-bars
are bootstrap 95%-CIs.

Zero-shot adaptation by meta-mapping (task → task): Furthermore, the system is able to per-
form meta-mappings over polynomials in order to ﬂexibly reconﬁgure its behavior (Fig. 3a). We
train the system to perform a variety of mappings, for example switch the ﬁrst two inputs of the
polynomial, add 3 to the polynomial, or square the polynomial. We then test its ability to generalize
to held-out mappings from examples, for example a held-out input permutation, or an unseen additive
shift. The system is both able to apply learned meta-mappings to held-out polynomials, and to apply
held-out meta-mappings it has not been trained on, simply by seeing examples of the mapping.

5 A stochastic learning setting: simple card games

We next explored the setting of simple card games, where the agent is dealt a hand and must bet.
There are three possible bets (including “don’t bet”), and depending on the opponent’s hand the agent
either wins or loses the amount bet. This task doesn’t require long term planning, but does incorporate
some aspects of reinforcement learning, namely stochastic feedback on only the action chosen. We
considered ﬁve games that are simpliﬁed analogs of various real card games (see Appendix F.1.2).
We also considered several binary options that could be applied to the games, including trying to
lose instead of trying to win, or switching which suit was more valuable. These are challenging
manipulations, for instance trying to lose requires completely inverting a learned Q-function.

In order to adapt the HoMM architecture, we made a very simple change. Instead of providing the
system with (input, target) tuples to embed, we provided it with (state, action, reward) tuples, and
trained it to predict rewards for each bet in each state. (A full RL framework is not strictly necessary
here because there is no temporal aspect to the tasks; however, because the outcome is only observed

6

(a) The polynomials domain.

(b) The cards domain.

Figure 4: The HoMM system can perform meta-mappings from language cues rather than meta-
mapping examples. Compare to Fig. 3, which shows the same results when using examples instead
of language. (a) In the polynomials domain, language cues still lead to good performance, even on
held-out tasks or held-out meta-mappings, although examples perform slightly better (Fig. 3a). (b)
Similarly, in the cards domain, language cues perform well. Error-bars are bootstrap 95%-CIs.

for the action you take, it is not a standard supervised task.) The hand is explicitly provided to the
network for each example, but which game is being played is implicitly captured in the training
examples, without any explicit cues. That is, the system must learn to play directly from seeing a set
of (state, action, reward) tuples which implicitly capture the structure and stochasticity of the game.
We also trained the system to make meta-mappings, for example switching from trying to win a game
to trying to lose. Details of the architecture and training can be found in Appendix F.

Basic meta-learning: First, we show that the system is able to play a held-out game from examples
in Fig. 2b. We compare two different hold-out sets: 1) train on half the tasks at random, or 2)
speciﬁcally hold out all the “losers” variations of the “straight ﬂush” game. In either of these cases,
the meta-learning system achieves well above chance performance (0) at the held out tasks, although
it is slightly worse at generalizing to the targeted hold out, despite having more training tasks in that
case. Note that the sample complexity in terms of number of trained tasks is not that high, even
training on 20 randomly selected tasks leads to good generalization to the held-out tasks. Furthermore,
the task embeddings generated by the system are semantically organized, see Appendix D.

Zero-shot adaptation by meta-mapping (task → task): Furthermore, the system is able to per-
form meta-mappings (mappings over tasks) in order to ﬂexibly reconﬁgure its behavior. For example,
if the system is trained to map games to their losers variations, it can generalize this mapping to a
game it has not been trained to map, even if the source or target of that mapping is held out from
training. In Fig. 3b we demonstrate this by taking the mapped embedding and evaluating the reward
received by playing the targeted game with it. This task is more difﬁcult than simply learning to
play a held out game from examples, because the system will actually receive no examples of the
target game (when it is held out). Furthermore, in the case of the losers mapping, leaving the strategy
unchanged would produce a large negative reward, and chance performance would produce 0 reward,
so the results are quite good.

6 An extension via language

Language is fundamental to human ﬂexibility. Often the examples of the meta-mapping are implicit
in prior knowledge about the world that is cued by language. For example, “try to lose at go” does not
give explicit examples of the “lose” meta-mapping, but rather relies on prior knowledge of what losing
means. This is a much more efﬁcient way to cue a known meta-mapping. In order to replicate this, we
trained the HoMM system with both meta-mappings based on examples, and meta-mappings based
on language. In the language-based meta-mappings, a language input identifying the meta-mapping
(but not the basic task to apply it to) is encoded by a language encoder, and then provided as the input

7

to H (instead of an output from M). The meta-mapping then proceeds as normal — H parameterizes
F , which is used to transform the embedding of the input task to produce an embedding for the target.

This language-cued meta-mapping approach also yields good performance (Fig. 4). However,
examples of the meta-mapping are slightly better, especially for meta-mappings not seen during
training, presumably because examples provide a richer description. In the next section we show that
using language to specify a meta-mapping performs better than using language to directly specify the
target task.

6.1 Why meta-map from tasks to tasks?

Why are meta-mappings between tasks useful? To answer this, we consider various ways of adapting
to a new task in Figure 5 (based on results from the cards domain, Section 5). The standard meta-
learning approach would be to adapt from seeing examples of the new task, but this requires going out
and collecting data, which may be expensive and does not allow zero-shot adaptation. Alternatively,
the system could perform the new task via a meta-mapping from a prior learned task, where the
meta-mapping is either induced from examples of the meta-mapping, or from language. Finally, the
system could perform a new task from language alone, if it is trained to map instructions to tasks.
This is the more typical zero-shot paradigm [e.g. 46].

To address this latter possibility, we trained a version of the model where we included training the
language system to produce embeddings for the basic tasks (while simultaneously training the system
on all the other objectives, such as performing the tasks from examples, in order to provide the
strongest possible structuring of the system’s knowledge for the strongest possible comparison). We
compare this model’s performance at held-out tasks to that of systems learning from examples of the
new task directly, or from meta-mapping, see Fig. 5.

These results demonstrate the advantage of meta-mapping. While learning from examples is still
better given enough data, it requires potentially-expensive data collection and does not allow zero-
shot adaptation. Performing the new task from a language description alone uses only the implicit
knowledge in the model’s weights, and likely because of this it does not generalize well to the difﬁcult
held-out tasks. Meta-mapping performs substantially better, while relying only on cached prior
knowledge, viz. prior task-embedding(s) and a description of the meta-mapping (either in the form of
examples or natural language). That is, meta-mapping allows zero-shot adaptation, like performing
from language alone, but results in much better performance than language alone, by leveraging a
richer description of the new task constructed using the system’s knowledge of a prior task and the
new task’s relationship to it.

7 Discussion

Related work: There has been a variety of past work on zero-shot learning, mostly based on natural-
language descriptions of tasks. For example, Larochelle et al. [36] considered the general problem of
behaving in accordance with language instructions as simply asking a model to adapt its response
when conditioned on different “instruction” inputs. Later work explored zero-shot classiﬁcation
based on only a natural language description of the target class [51, 46, 57], or of a novel task in a
language-conditioned RL system [24]. Some of this work has even exploited relationships between
tasks as a learning signal [42]. Other work has considered how similarity between tasks can be useful
for generating representations for a new task [43], but without transforming task representations to
do so. (Furthermore, similarity is less speciﬁc than an input-output mapping, since it does not specify
along which dimensions two tasks are similar.) To our knowledge none of the prior work has proposed
using meta-mapping approaches to adapt to new tasks by transforming task representations, nor has
the prior work proposed a parsimonious homoiconic model which can perform these mappings.

Our work is an extrapolation from the rapidly-growing literature on meta-learning [e.g. 55, 50, 18, 20,
53, 9]. It is also related to the literature on continual learning, or more generally tools for avoiding
catastrophic interference based on changes to the architecture [e.g. 17, 48], loss [e.g. 30, 59, 2],
or external memory [e.g. 52]. We also connect to a different perspective on continual learning in
Appendix B. Recent work has also begun to blur the separation between these approaches, for example

8

Figure 5: Comparison of a variety of methods for performing one of the 10% held-out tasks in the
more difﬁcult hold-out set in the cards domain. There are a number of ways the system could adapt to
a new task: from seeing example of the new task, from hearing the new task described from language
alone, or from leveraging its knowledge about prior tasks via meta-mappings (in this case, from
the non-losers variations of the same games). The meta-mappings offer a happy medium between
the other two alternatives – they only require cached knowledge of prior tasks, rather than needing
to collect experience on the task before a policy can be derived, but they outperform a system that
simply tries to construct the task embedding from a description alone. Language alone is not nearly
as rich a cue as knowledge of how a new task relates to prior tasks.

by meta-learning in an online setting [19]. Our work is speciﬁcally inspired by the algorithms that
attempt to have the system learn to adapt to a new task via activations rather than weight updates,
either from examples [e.g. 56, 15], or a task input [e.g. 8].

Our architecture builds directly off of prior work on HyperNetworks [23] – networks which parame-
terize other networks – and other recent applications thereof, such as guessing parameters for a model
to accelerate model search [e.g. 10, 60], and meta-learning [38, 49, e.g.]. In particular, recent work in
natural language processing has shown that having contextually generated parameters can allow for
zero-shot task performance, assuming that a good representation for the novel task is given [44] – in
their work this representation was evident from the factorial structure of translating between many
pairs of languages. Our work is also related to the longer history of work on different time-scales
of weight adaptation [25, 31] that has more recently been applied to meta-learning contexts [e.g.
6, 41, 21] and continual learning [28, e.g.]. It is more abstractly related to work on learning to propose
architectures [e.g. 61, 11], and to models that learn to select and compose skills to apply to new tasks
[e.g. 4, 3, 54, 45, 12]. In particular, some of the work in domains like visual question answering has
explicitly explored the idea of building a classiﬁer conditioned on a question [4, 5], which is related to
one of the possible computational paths through our architecture. Work in model-based reinforcement
learning has also partly addressed how to transfer knowledge between different reward functions
[e.g. 35]; our approach is more general. Indeed, our insights could be combined with model-based
approaches, for example our approach could be used to adapt a task embedding, which would then be
used by a learned planning model.

There has also been other recent interest in task (or function) embeddings. Achille et al. [1] recently
proposed computing embeddings for visual tasks from the Fisher information of the parameters in a
model partly tuned on the task. They show that this captures some interesting properties of the tasks,
including some types of semantic relationships, and can help identify models that can perform well
on a task. Rusu and colleagues recently suggested a similar meta-learning framework where latent
codes are computed for a task which can be decoded to a distribution over parameters [49]. Other
recent work has tried to learn representations for skills [e.g. 16] or tasks [27, e.g.] for exploration
and representation learning. Our perspective can be seen as a generalization of these that allows for
remapping of behavior via meta-tasks. While there have been other approaches to zero-shot task
performance, to the best of our knowledge none of the prior work has explored zero-shot performance
of a task via meta-mappings.

Future Directions: We think that the general perspective of considering meta-mappings will yield
many fruitful future directions. We hope that our work will inspire more exploration of behavioral

9

adaptation, in areas beyond the simple domains we considered here. To this end, we suggest the
creation of meta-learning datasets which include information not only about tasks, but about the
relationships between them. For example, reinforcement learning tasks which involve executing
instructions [e.g. 24, 14] can be usefully interpreted from this perspective. Furthermore, we think our
work provides a novel perspective on the types of ﬂexibility that human intelligence exhibits, and
thus hope that it may have implications for cognitive science.

We do not necessarily believe that the particular architecture we have suggested is the best architecture
for addressing these problems, although it has a number of desirable characteristics. However, the
modularization of the architecture makes it easy to modify. (We compare some variations in Appendix
E.) For example, although we only considered task networks F that are feed-forward and of a ﬁxed
depth, this could be replaced with a recurrent architecture to allow more adaptive computation, or even
a more complex architecture [e.g. 45, 22]. Our work also opens the possibility of doing unsupervised
learning over function representations for further learning, which relates to long-standing ideas in
cognitive science about how humans represent knowledge [13].

8 Conclusions

We’ve highlighted a new type of ﬂexibility in the form of meta-mapping between tasks. Meta-mapping
can produce zero-shot performance on novel tasks, based on their relationship to old tasks. This is
a key aspect of human ﬂexibility. The meta-mapping perspective suggests that this ﬂexibility requires
representing tasks in a transformable way. To address this, our Homoiconic Meta-Mapping (HoMM)
approach explicitly represents tasks by function embeddings, and derives the computation from these
embeddings via a HyperNetwork. This allows for our key insight: by embedding tasks and individual
data points in a shared latent space, the same meta-learning architecture can be used both to transform
data for basic tasks, and to transform task embeddings to adapt to task variations. This approach
is parsiomious, because it uses the same networks for basic- and meta-mappings. Perhaps because
of this, the requisite meta-task sample complexity is small; we showed generalization to unseen
meta-mappings after training on only 20 meta-mappings. That is, we are able to achieve zero-shot
performance on a novel task based on a held-out relationship between tasks. Furthermore, the meta-
mapping approach yields better task performance than a zero-shot approach based on performing
the task based on just a natural language description. This is a step closer to human-level ﬂexibility.

We see our proposal as a logical progression from the fundamental idea of meta-learning – that
there is a continuum between data and tasks. This naturally leads to the idea of manipulating task
representations just like we manipulate data. We’ve shown that this approach yields considerable
ﬂexibility, most importantly the meta-mapping ability to adapt zero-shot to a new task. We hope that
these results will lead to the development of more powerful and ﬂexible deep-learning models.

Acknowledgements

We would like to acknowledge Noah Goodman, Surya Ganguli, Katherine Hermann, Erin Bennett,
and Arianna Yuan for stimulating questions and suggestions on this project.

References

[1] Achille, A., Lam, M., Tewari, R., Ravichandran, A., Maji, S., Fowlkes, C., Soatto, S., and Perona,

P. (2019). Task2Vec: Task Embedding for Meta-Learning. arXiv preprint.

[2] Aljundi, R., Rohrbach, M., and Tuytelaars, T. (2019). Selﬂess sequential learning. In International

Conference on Learning Representations, pages 1–17.

[3] Andreas, J., Klein, D., and Levine, S. (2016a). Modular Multitask Reinforcement Learning with

Policy Sketches. arXiv preprint.

[4] Andreas, J., Rohrbach, M., Darrell, T., and Klein, D. (2016b). Learning to Compose Neural

Networks for Question Answering. arXiv preprint.

10

[5] Andreas, J., Rohrbach, M., Darrell, T., and Klein, D. (2017). Deep Compositional Question

Answering with Neural Module Networks.

[6] Ba, J., Hinton, G., Mnih, V., Leibo, J. Z., and Ionescu, C. (2016). Using Fast Weights to Attend

to the Recent Past. In Advances in Neural Information Processing Systems, pages 1–10.

[7] Baars, B. J. (2005). Global workspace theory of consciousness: Toward a cognitive neuroscience

of human experience. Progress in Brain Research, 150:45–53.

[8] Borsa, D., Quan, J., Mankowitz, D., Hasselt, H. V., Silver, D., and Schaul, T. (2019). Universal
Successor Features Approximators. In International Conference on Learning Representations,
number 2017, pages 1–24.

[9] Botvinick, M., Ritter, S., Wang, J. X., Kurth-nelson, Z., Blundell, C., and Hassabis, D. (2019).

Reinforcement Learning , Fast and Slow. Trends in Cognitive Sciences, pages 1–15.

[10] Brock, A., Lim, T., Ritchie, J. M., and Weston, N. (2018). SMASH: One-Shot Model Architec-
ture Search through HyperNetworks. In International Conference on Learning Representations.

[11] Cao, S., Wang, X., and Kitani, K. M. (2019). Learnable Embedding Space for Efﬁcient Neural
Architecture Compression. In International Conference on Learning Representations, pages 1–17.

[12] Chang, M. B., Gupta, A., Levine, S., and Grifﬁths, T. L. (2019). Automatically Composing
Representation Transformations as a Means for Generalization. In International Conference on
Learning Representations, pages 1–23.

[13] Clark, A. and Karmiloff-Smith, A. (1993). The Cognizer’s Innards: A Psychological and
Philosophical Perspective on the Development of Thought. Mind & Language, 8(4):487–519.

[14] Co-Reyes, J. D., Gupta, A., Suvansh, S., Altieri, N., Andreas, J., DeNero, J., Abbeel, P., and
Levine, S. (2019). Guiding policies with language via meta-learning. In International Conference
on Learning Representations, pages 1–17.

[15] Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. (2016). RL$ˆ2$:
Fast Reinforcement Learning via Slow Reinforcement Learning. arXiv preprint, pages 1–14.

[16] Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. (2019). Diversity is all you need: learning
skills without a reward function. In International Conference on Learning Representations, pages
1–22.

[17] Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra,
D. (2017). PathNet: Evolution Channels Gradient Descent in Super Neural Networks. arXiv.

[18] Finn, C., Abbeel, P., and Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation

of Deep Networks. In Proceedings of the 34th Annual Conference on Machine Learning.

[19] Finn, C., Rajeswaran, A., Kakade, S., and Levine, S. (2019). Online Meta-Learning. arXiv

preprint.

[20] Finn, C., Xu, K., and Levine, S. (2018). Probabilistic Model-Agnostic Meta-Learning. arXiv

preprint.

[21] Garnelo, M., Rosenbaum, D., Maddison, C. J., Ramalho, T., Saxton, D., Shanahan, M., Teh,
Y. W., Rezende, D. J., and Eslami, S. M. A. (2018). Conditional Neural Processes. arXiv preprint.

[22] Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi´nska, A., Gómez
Colmenarejo, S., Grefenstette, E., Ramalho, T., Agapiou, J., Badia, A. P., Moritz Hermann, K.,
Zwols, Y., Ostrovski, G., Cain, A., King, H., Summerﬁeld, C., Blunsom, P., Kavukcuoglu, K., and
Hassabis, D. (2016). Hybrid computing using a neural network with dynamic external memory.
Nature Publishing Group, 538(7626):471–476.

[23] Ha, D., Dai, A., and Le, Q. V. (2016). HyperNetworks. arXiv.

11

[24] Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., Szepesvari, D., Czarnecki,
W. M., Jaderberg, M., Teplyashin, D., Wainwright, M., Apps, C., and Hassabis, D. (2017).
Grounded Language Learning in a Simulated 3D World. arXiv preprint, pages 1–22.

[25] Hinton, G. E. and Plaut, D. C. (1982). Using Fast Weights to Deblur Old Memories. Proceedings

of the 9th Annual Conference of the Cognitive Science Society, (1987).

[26] Hlavac, M. (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.

[27] Hsu, K., Levine, S., and Finn, C. (2019). Unsupervised Learning Via Meta-Learning. In

International Conference on Learning Representations.

[28] Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Zhao, D., and Yan, R. (2019). Overcoming catastrophic
forgetting for continual learning via model adaptation. In International Conference on Learning
Representations, pages 1–13.

[29] Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., Thorat, N., Viégas, F.,
Wattenberg, M., Corrado, G., Hughes, M., and Dean, J. (2016). Google’s Multilingual Neural
Machine Translation System: Enabling Zero-Shot Translation. arXiv, pages 1–16.

[30] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan,
K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and
Hadsell, R. (2016). Overcoming catastrophic forgetting in neural networks. arXiv preprint.

[31] Kumaran, D., Hassabis, D., and McClelland, J. L. (2016). What Learning Systems do Intelligent
Agents Need? Complementary Learning Systems Theory Updated. Trends in Cognitive Sciences,
20(7):512–534.

[32] Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2017). Building Machines

that learn and think like people. Behavioral and Brain Sciences, pages 1–55.

[33] Lampinen, A. K. and Ganguli, S. (2019). An analytic theory of generalization dynamics and
transfer learning in deep linear networks. In International Conference on Learning Representations,
pages 1–20.

[34] Lampinen, A. K. and McClelland, J. L. (2018). One-shot and few-shot learning of word

embeddings. arXiv preprint.

[35] Laroche, R. and Barlier, M. (2017). Transfer Reinforcement Learning with Shared Dynamics.
In Proceedings of the Thirty First AAAI Conference on Artiﬁcial Intelligence, pages 2147–2153.

[36] Larochelle, H., Erhan, D., and Bengio, Y. (2008). Zero-data learning of new tasks. Proceedings

of the Twenty-Third AAAI Conference on Artiﬁcial Intelligence, (Miller 2002):646–651.

[37] Laurens van der Maaten and Hinton, G. (2008). Visualizing Data using t-SNE. Journal of

Machine Learning Research, 9:2579–2605.

[38] Li, H., Dong, W., Mei, X., Ma, C., Huang, F., and Hu, B.-G. (2019). LGM-Net: Learning
to Generate Matching Networks for Few-Shot Learning. Proceedings of the 36th International
Conference on Machine Learning.

[39] Marcus, G. (2018). Deep Learning: A Critical Appraisal. arXiv preprint, pages 1–27.

[40] McCloskey, M. and Cohen, N. J. (1989). Catastrophic interference in connectionist networks:

The sequential learning problem. Psychology of learning and motivation, 24.

[41] Munkhdalai, T. and Yu, H. (2017). Meta Networks. arXiv preprint.

[42] Oh, J., Singh, S., Lee, H., and Kohli, P. (2017). Zero-Shot Task Generalization with Multi-Task
Deep Reinforcement Learning. Proceedings of the 34th International Conference on Machine
Learning.

[43] Pal, A. and Balasubramanian, V. N. (2019). Zero-Shot Task Transfer. Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition.

12

[44] Platanios, E. A., Sachan, M., Neubig, G., and Mitchell, T. M. (2017). Contextual Parameter

Generation for Universal Neural Machine Translation. arXiv preprint.

[45] Reed, S. and de Freitas, N. (2015). Neural Programmer-Interpreters. arXiv preprint, pages

1–12.

[46] Romera-Paredes, B. and Torr, P. H. (2015). An embarrassingly simple approach to zero-shot

learning. 32nd International Conference on Machine Learning, ICML 2015, 3:2142–2151.

[47] Rumelhart, D. E. and Todd, P. M. (1993). Learning and connectionist representations. Attention
and performance XIV: Synergies in experimental psychology, artiﬁcial intelligence, and cognitive
neuroscience, pages 3–30.

[48] Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K.,

Pascanu, R., and Hadsell, R. (2016). Progressive neural networks. arXiv preprint.

[49] Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., and Hadsell,
R. (2019). Meta-Learning with Latent Embedding Optimization. International Conference on
Learning Representations, pages 1–17.

[50] Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. (2016). Meta-Learning
with Memory-Augmented Neural Networks. In Proceedings of the 33rd International Conference
on Machine Learning, volume 48.

[51] Socher, R., Ganjoo, M., Manning, C. D., and Ng, A. Y. (2013). Zero-shot learning through

cross-modal transfer. Advances in Neural Information Processing Systems.

[52] Sprechmann, P., Jayakumar, S. M., Rae, J. W., Pritzel, A., Uria, B., Vinyals, O., Hassabis, D.,
Pascanu, R., and Blundell, C. (2018). Memory-based parameter Adaptation. In International
Conference on Learning Representations.

[53] Stadie, B. C., Yang, G., Houthooft, R., Chen, X., Duan, Y., Wu, Y., Abbeel, P., and Sutskever, I.
(2018). Some Considerations on Learning to Explore via Meta-Reinforcement Learning. arXiv
preprint.

[54] Tessler, C., Givony, S., Zahavy, T., Mankowitz, D. J., and Mannor, S. (2016). A Deep Hierarchi-

cal Approach to Lifelong Learning in Minecraft. arXiv preprint.

[55] Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. (2016). Matching

Networks for One Shot Learning. Advances in Neural Information Processing Systems.

[56] Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C.,
Kumaran, D., and Botvinick, M. (2016). Learning to reinforcement learn. arXiv preprint, pages
1–17.

[57] Xian, Y., Lampert, C. H., Schiele, B., and Akata, Z. (2018). Zero-Shot Learning - A Compre-
hensive Evaluation of the Good, the Bad and the Ugly. IEEE Transactions on Pattern Analysis
and Machine Intelligence, pages 1–14.

[58] Yao, H., Wei, Y., Huang, J., and Li, Z. (2019). Hierarchically Structured Meta-learning.

Proceedings of the 36th International Conference on Machine Learning.

[59] Zenke, F., Poole, B., and Ganguli, S. (2017). Continual Learning Through Synaptic Intelligence.

In Proceedings of the 34th International Conference on Machine Learning.

[60] Zhang, C., Ren, M., Urtasun, R., Advanced, U., and Group, T. (2019). Graph HyperNetworks
for neural architecture search. In International Conference on Learning Representations, number
2018, pages 1–17.

[61] Zoph, B. and Le, Q. V. (2016). Neural Architecture Search with Reinforcement Learning. arXiv

preprint, pages 1–16.

13

The supplemental material is organized as follows: In Section A we clarify some deﬁnitional details.
In Section B we describe a continual-learning like perspective based on our approach. In Section C
we provide supplemental ﬁgures. In Section D we show t-sne results for the cards domain. In Section
E we provide some lesion studies. In Section F we list details fo the datasets and architectures we
used, as well as providing links to the source code for all models, experiments, and analyses. In
Section G we provide means and bootstrap CIs corresponding to the major ﬁgures in the paper.

A Clarifying meta-mapping

A.1 Clarifying hold-outs

There are several distinct types of hold-outs in the basic training of our architecture:

1. On each basic task, some of the data (D1) is fed to the meta-network as examples M while
some probes (D2) is held out. This encourages the model to actually infer the underlying
function, rather than just memorizing the examples. However, the probe labels are still used
in training the system end-to-end, in order to encourage it to generalize.

2. There are also truly held-out tasks that the system has never seen in training. These are the
held-out tasks that we evaluate on at the end of training and that are plotted in the “Held out”
sections in the main plots.

This applies analogously to the meta-mappings: each time a meta-mapping is trained, some basic
tasks are used as examples while others are held out to encourage generalization. There are also
meta-mappings which have never been encountered during training, which we evaluate on at the end
of training, those are the meta-mappings which are plotted in the “held out” section in the relevant
plots. We also evaluate the old (and new) meta-mappings on the new basic tasks that have never been
trained.

A.2 A deﬁnitional note

When we discussed meta-mappings in the main task, we equivocated between tasks and behaviors
for the sake of brevity. For a perfect model, this is somewhat justiﬁable, because each task will
have a corresponding optimal behavior, and the sytem’s embedding of the task will be precisely
the embedding which produces this optimal behavior. However, behavior-irrelevant details of the
task, like the color of the board, may not be embedded, so this should not really be thought of as a
task-to-task mapping. This problem is exacerbated when the system is imperfect, e.g. during learning.
It is thus more precise to distinguish between a ground-truth meta-mapping, which maps tasks to
tasks, and the computational approach to achieving that meta-mapping, which really maps between
representations which combine both task and behavior.

14

B Continual learning

Figure 6: Once the meta-learning system has been trained on a distribution of prior tasks, its
performance on new tasks can be tuned by caching its guessed embeddings for the tasks and then
optimizing those, thus avoiding any possibility of interfering with performance on prior tasks. Starting
from random embeddings in the trained model results in slower convergence, while in an untrained
model the embeddings cannot be optimized well. Error-bars are bootstrap 95%-CIs.

Continual learning: Although the meta-learning approach is effective for rapidly adapting to a
new task, it is unreasonable to think that our system must consider every example it has seen at each
inference step. We would like to be able to store our knowledge more efﬁciently, and allow for further
reﬁnement. Furthermore, we would like the system to be able to adapt to new tasks (for which its
guessed solution isn’t perfect) without catastrophically interfering with prior tasks [40].

A very simple solution to these problems is naturally suggested by our architecture. Speciﬁcally, task
embeddings can be cached so that they don’t have to be regenerated at each inference step. This also
allows optimization of these embeddings without altering the other parameters in the architecture,
thus allowing ﬁne-tuning on a task without seeing more examples, and without interfering with
performance on any other task [cf. 47, 34]. This is like the procedure of Rusu et al. [49], except
considered across episodes. That is, we can see the meta-learning step as a “warm start” for an
optimization procedure over embeddings that are cached in memory [cf. 31]. While this is not a
traditional continual learning perspective, we think it provides an interesting perspective on the
issue. It might in fact be much more memory-efﬁcient to store an embedding per task, compared to
storing an extra “importance” parameter for every parameter in our model, as in e.g. elastic weight
consolidation [30]. It also provides a stronger guarantee of non-interference.

To test this idea, we pre-trained the system on 100 polynomial tasks, and then introduced 100 new
tasks. We trained on these new tasks by starting from the meta-network’s “guess” at the correct task
embedding, and then optimizing this embedding without altering the other parameters. The results are
shown in Fig. 6. The meta-network embeddings offer good immediate performance, and substantially
accelerate the optimization process, compared to a randomly-initialized embedding (see supp. Fig.
10 for a more direct comparison). Furthermore, this ability to learn is due to training, not simply the
expressiveness of the architecture, as is shown by attempting the same with an untrained network.

15

C Supplemental ﬁgures

Figure 7: The system is able to infer polynomials from only seeing a few data points (i.e. evaluations
of the polynomial), despite the fact that during training it always saw 50. A minimum of 15 random
points is needed to correctly infer polynomials without prior knowledge of the polynomial distribution,
but the system is performing well below this value, and quite well above it, although it continues to
reﬁne its estimates slightly when given more data.

Figure 8: Learning curves for basic regression in the polynomials domain.

16

Figure 9: Learning curves for meta-mappings in the polynomials domain. Although the results seem
to be leveling off at the end, we found that generalization performance was slightly increasing or
stable in this region, which may have interesting implications about the structure of these tasks [33].

Figure 10: Continual learning in the polynomials domain: a more direct comparison. Once the
meta-learning system has been trained on a distribution of prior tasks, its performance on new tasks
can be tuned by caching its guessed embeddings for the tasks and then optimizing those, thus avoiding
any possibility of interfering with performance on prior tasks. Starting with the guessed embedding
substantially speeds-up the process compared to a randomly-initialized embedding. Furthermore, this
ability to learn is due to training, not simply the expressiveness of the architecture, as is shown by
attempting the same with an untrained network.

17

(a) The polynomials domain, Section 4.

(b) The cards domain, Section 5.

Figure 11: Integrating new tasks into the system by training all parameters results in some initial
interference with prior tasks (even with replay), suggesting that an approach like the continual
learning-approach may be useful.

D Card game t-SNE

We performed t-SNE [37] on the task embeddings of the system at the end of learning the card game
tasks, to evaluate the organization of knowledge in the network. In Fig. 12 we show these embeddings
for just the basic tasks. The embeddings show systematic grouping by game attributes. In Fig. 13 we
show the embeddings of the meta and basic tasks, showing the organization of the meta-tasks by type.

Figure 12: t-SNE embedding of the function embeddings the system learned for the basic card game
tasks. (Note that the pairs of nearby embeddings differ in the “suits rule“ attribute, discussed in
Appendix F.1.2.)

18

Figure 13: t-SNE embedding of the function embeddings the system learned for the meta tasks (basic
tasks are included in the background).

E Architecture experiments

In this section we consider a few variations of the architecture, to justify the choices made in the
paper.

E.1 Shared Z vs. separate task-embedding and data-embedding space

Instead of having a shared Z where data and tasks are embedded, why not have a separate embedding
space for data, tasks, and so on? There are a few conceptual reason why we chose to have a shared
Z, including its greater parameter efﬁciency, the fact that humans seem to represent our conscious
knowledge of different kinds in a shared space [7], and the fact that this representation could allow
for zero-shot adaptation to new computational pathways through the latent space, analogously to the
zero-shot language translation results reported by Johnson and colleagues [29]. In this section, we
further show that training with a separate task encoding space worsens performance, see Fig. 14.
This seems to primarily be due to the fact that learning in the shared Z accelerates and de-noises the
learning process, see Fig. 15. (It’s therefore worth noting that running this model for longer could
result in convergence to the same asymptotic generalization performance.)

E.2 Hyper network vs. conditioned task network

Instead of having the task network F parameterized by the hyper network H, we could simply have a
task network with learned weights which takes a task embedding as another input. Here, we show
that this architecture fails to learn the meta-mapping tasks, although it can successfully perform the
basic tasks. We suggest that this is because it is harder for this architecture to prevent interference
between the comparatively larger number of basic tasks and the smaller number of meta-tasks. While
it might be possible to succeed with this architecture, it was more difﬁcult in the hyper-parameter
space we searched.

19

Figure 14: Having a separate embedding space for tasks results in worse performance on meta-
mappings. (Results are from only 1 run.)

Figure 15: Having a separate embedding space for tasks results in noisier, slower learning of
meta-mappings. (Results are from only 1 run.)

20

Figure 16: Conditioning the task network on the task embedding, rather than parameterizing it via a
hyper network causes it to fail at the meta-mapping tasks. Results are from only 2 runs.

F Detailed methods

F.1 Datasets

F.1.1 Polynomials

We randomly sampled the train and test polynomials as follows:

1. Sample the number of relevant variables (k) uniformly at random from 0 (i.e. a constant) to

the total number of variables.

2. Sample the subset of k variables that are relevant from all the variables.
3. For each term combining the relevant variables (including the intercept), include the term

with probability 0.5. If so give it a random coefﬁcient drawn from N (0, 2.5).

The data points on which these polynomials were evaluated were sampled uniformly from [−1, 1]
independently for each variable, and for each polynomial. The datasets were resampled every 50
epochs of training.

Meta-tasks: For meta-tasks, we trained the network on 6 task-embedding classiﬁcation tasks:

• Classifying polynomials as constant/non-constant.
• Classifying polynomials as zero/non-zero intercept.
• For each variable, identifying whether that variable was relevant to the polynomial.

We trained on 20 meta-mapping tasks, and held out 16 related meta-mappings.

• Squaring polynomials (where applicable).
• Adding a constant (trained: -3, -1, 1, 3, held-out: 2, -2).
• Multiplying by a constant (trained: -3, -1, 3, held-out: 2, -2).
• Permuting inputs (trained: 1320, 1302, 3201, 2103, 3102, 0132, 2031, 3210, 2301, 1203,
1023, 2310, held-out: 0312, 0213, 0321, 3012, 1230, 1032, 3021, 0231, 0123, 3120, 2130,
2013).

21

Language: We encoded the meta-tasks in language by sequences as follows:

• Classifying polynomials as constant/non-constant: [‘‘is’’, ‘‘constant’’]
• Classifying

zero/non-zero

polynomials

as

intercept:

[‘‘is’’, ‘‘intercept_nonzero’’]

• For each variable, identifying whether that variable was relevant to the polynomial:

[‘‘is’’, <variable-name>, ‘‘relevant’’]

• Squaring polynomials: [‘‘square’’]
• Adding a constant: [‘‘add’’, <value>]
• Multiplying by a constant: [‘‘multiply’’, <value>]
• Permuting inputs:

[‘‘permute’’, <variable-name>, <variable-name>, <variable-name>,
<variable-name>]

All sequences were front-padded with “<PAD>” to the length of the longest sequence.

F.1.2 Card games

Our card games were played with two suits, and 4 values per suit. In our setup, each hand in a game
has a win probability (proportional to how it ranks against all other possible hands). The agent is
dealt a hand, and then has to choose to bet 0, 1, or 2 (the three actions it has available). We considered
a variety of games which depend on different features of the hand:

• High card: Highest card wins.
• Pairs Same as high card, except pairs are more valuable, and same suit pairs are even more

valuable.

• Straight ﬂush: Most valuable is adjacent numbers in same suit, i.e. 4 and 3 in most valuable

suit wins every time (royal ﬂush).

• Match: the hand with cards that differ least in value (suit counts as 0.5 pt difference) wins.
• Blackjack: The hand’s value increases with the sum of the cards until it crosses 5, at which

point the player “goes bust,” and the value becomes negative.

We also considered three binary attributes that could be altered to produce variants of these games:

• Losers: Try to lose instead of winning! Reverses the ranking of hands.
• Suits rule: Instead of suits being less important than values, they are more important

(essentially ﬂipping the role of suit and value in most games).

• Switch suit: Switches which of the suits is more valuable.

Any combination of these options can be applied to any of the 5 games, yielding 40 possible games.
The systems were trained with the full 40 possible games, but after training we discovered that the
“suits rule” option does not substantially alter the games we chose (in the sense that the probability of
a hand winning in the two variants of a game is very highly correlated), so we have omitted it from
our analyses.

Meta-tasks: For meta-tasks, we gave the network 8 task-embedding classiﬁcation tasks (one-vs-all
classiﬁcation of each of the 5 game types, and of each of the 3 attributes), and 3 meta-mapping tasks
(each of the 3 attributes).

Language:
form
[‘‘is’’, <attribute-or-game-name>].

encoded
[‘‘toggle’’, <attribute-name>]

the meta-tasks

We

in

for

22

language
the

by
meta-mapping

sequences

tasks,

of

the
and

F.2 Model & training

Basic task operation:

1. A training dataset D1 of (input, target) pairs is embedded by I and T to produce a set
of paired embeddings. Another set of (possibly unlabeled) inputs D2 is provided and
embedded.

2. The meta network M maps the set of embedded (input, target) pairs to a function embedding.

3. The hyper network H maps the function embedding to parameters for F , which is used to

transform the second set of inputs to a set of output embeddings.

4. The output embeddings are decoded by O to produce a set of outputs.

5. The system is trained end-to-end to minimize the loss on these outputs.

The model is trained to minimize

E(x,y)∈D2 [L (y, O (FD1 (I (x))))]

where FD1 is the transformation the meta-learner guesses for the training dataset D1:

FD1 is parameterized by H (M ({(I (xi) , T (yi)) for (xi, yi) ∈ D1}))

Meta-task operation:

1. A meta-dataset of (source-task-embedding, target-task-embedding) pairs, D1, is collected.
Another dataset D2 (possibly only source tasks) is provided. (All embeddings included
in D1 during training are for basic tasks that have themselves been trained, to ensure that
there is useful signal. During evaluation, the embeddings in D1 are for tasks that have been
trained on, but those in D2 may be new.

2. The meta network M maps this set of (source, target) task-embedding pairs to a function

embedding.

3. The hyper network H maps the function embedding to parameters for F , which is used to

transform the second set of inputs to a set of output embeddings.

4. The system is trained to minimize (cid:96)2 loss between these mapped embeddings and the target

embeddings.

The model is trained to minimize

E(zsource,ztarget)∈D2 [L (ztarget, FD1 (I (zsource)))]
where L is (cid:96)2 loss, and FD1 is the transformation the meta-learner guesses for the training dataset
D1:

FD1 is parameterized by H (M ({((zsource, ztarget) ∈ D1}))

Note that there are three kinds of hold-out in the training of this system, see Section A.1.

Language-cued meta-tasks: The procedure is analogous to the meta-tasks from examples, except
that the input to H is the embedding of the language input, rather than the output of M. The systems
that were trained from language were also trained with the example-based meta-tasks.

F.2.1 Detailed hyper-parameters

See table 1 for detailed architectural description and hyperparameters for each experiment. Hyper-
parameters were generally found by a heuristic search, where mostly only the optimizer, learning
rate annealing schedule, and number of training epochs were varied, not the architectural parameters.
Some of the parameters take the values they do for fairly arbitrary reasons, e.g. the continual learning
experiments were run with the current polynomial hyperparameters before the hyperparameter search
for the polynomial data was complete, so some parameters are altered between these.

23

Z-dimension
I num. layers
I num. hidden units
L architecture

L num. hidden units
O num. layers
O num. hidden units
T num. layers
M architecture
H architecture
M, H num. hidden units
F architecture
F num. hidden units
Nonlinearities

Main loss

Optimizer
Learning rate (base)
Learning rate (meta)
L.R. decay rate (base)
L.R. decay rate (meta)
L.R. min (base)
L.R. min (meta)
L.R. decays every
Cached embedding L.R.
Num. training epochs
Num. runs
Num. base tasks (eval)
Num. base tasks (training)
Num. meta classiﬁcations
Num. meta mappings
Num. new base tasks
Num. new meta mappings
Num. new meta classiﬁcations
Base dataset size
Base datasets refreshed
M batch size

Polynomials
512

Continual learning
512
3
64
-

Cards
512

2-layer LSTM + 2
fully-connected
512
1
-

1-layer LSTM + 2
fully-connected
512
3
512

-
1
-
1
2 layers per-datum, max pool across, 2 layers
4 layers
512
4 layers
64
Leaky ReLU in most places, except no non-linearity at ﬁnal
layer of T , M, L, F , and sigmoid for meta-classiﬁcation
outputs.
(cid:96)2 for main task & meta-mapping, cross-entropy for meta-
classiﬁcation.

Adam
3 · 10−5
1 · 10−5
∗0.85
∗0.85

1 · 10−7

-
4000
5
60
1200 (= 60 * 20)
6
20
40
16

50

RMSProp
1 · 10−4
-
∗0.85
-
3 · 10−8
-
100 epochs if above min.
1 · 10−3
3000
5
100
100
-
-
30
-
0
1024
Every 50 epochs
128

RMSProp
1 · 10−4
1 · 10−4
∗0.85
∗0.9

3 · 10−7

-
40000
10
36 or 20
36 or 20
8
3
4 or 20
0

768

the shared representational space is denoted by Z.

Table 1: Detailed hyperparameter speciﬁcation for different experiments. A “-” indicates a parameter
that does not apply to that experiment. Where only one value is given, it applied to all the experiments
discussed. As a reminder:
Input encoder:
I : input → Z. Output decoder O : Z → output. Target encoder T : targets → Z. Meta-network
M : {(Z, Z), ...} → Z – takes a set of (input embedding, target embedding) pairs and produces a
function embedding. Hyper-network (cid:104) : Z → parameters – takes a function embedding and produces
a set of parameters. Task network F : Z → Z – the transformation that executes the task mapping,
implemented by a deep network with parameters speciﬁed by H. Where language was used to cue
meta-mappings, it was processed by language encoder: L : natural language → Z.

24

Each epoch consisted of a separate learning step on each task (both base and meta), in a random order.
In each task, the meta-learner would receive only a subset (the “batch size“ above) of the examples to
generate a function embedding, and would have to generalize to the remainder of the examples in the
dataset. The embeddings of the tasks for the meta-learner were computed once per epoch, so as the
network learned over the course of the epoch, these embeddings would get “stale,” but this did not
seem to be too detrimental.

The results reported in the ﬁgures in this paper are averages across multiple runs, with different
trained and held-out tasks (in the polynomial case) and different network initializations (in all cases),
to ensure the robustness of the ﬁndings.

F.3 Source repositories

The full code for the experiments and analyses can be found on github:

• https://github.com/lampinen/polynomials

– See the continual_learning and language_meta_only branches for the continual

learning and language results, respectively.

• https://github.com/lampinen/meta_cards

G Numerical results

In this section we provide the mean values and bootstrap conﬁdence intervals corresponding to the
major ﬁgures in the paper, as well as the baseline results in those ﬁgures. Tables were generated with
stargazer [26].

G.1 Polynomials

named_run_type

HoMM
HoMM
Untrained HoMM network
Untrained HoMM network

is_new

Trained
Held out
Trained
Held out

mean_loss

boot_CI_low

boot_CI_high

0.015
0.246
5.735
5.968

0.012
0.188
4.823
4.984

0.018
0.308
6.74
6.991

Table 2: Table for basic meta-learning, Figure 2a

named_run_type

result_type

mean_loss

boot_CI_low

boot_CI_high

HoMM
HoMM
HoMM
HoMM
Untrained HoMM network
Untrained HoMM network
Untrained HoMM network
Untrained HoMM network

Trained mapping, on trained task
Trained mapping, on held-out task
Held-out mapping, on trained task
Held-out mapping, on held-out task
Trained mapping, on trained task
Trained mapping, on held-out task
Held-out mapping, on trained task
Held-out mapping, on held-out task

0.094
1.721
1.28
1.775
12.998
15.002
8.36
8.786

0.091
1.419
1.213
1.706
11.689
13.39
7.898
8.317

0.098
2.115
1.35
1.846
14.381
16.83
8.833
9.27

Table 3: Table for meta-mapping results from examples, Figure 3a

25

named_run_type

result_type

mean_loss

boot_CI_low

boot_CI_high

Language
Language
Language
Language
Untrained HoMM network
Untrained HoMM network
Untrained HoMM network
Untrained HoMM network

Trained mapping, on trained task
Trained mapping, on held-out task
Held-out mapping, on trained task
Held-out mapping, on held-out task
Trained mapping, on trained task
Trained mapping, on held-out task
Held-out mapping, on trained task
Held-out mapping, on held-out task

0.515
2.244
2.072
2.35
13.328
15.313
8.205
8.625

0.483
1.921
1.958
2.254
11.977
13.602
7.795
8.131

0.552
2.623
2.19
2.447
14.823
17.354
8.662
9.104

Table 4: Table for meta-mapping results from language, Figure 4a

G.2 Cards

named_run_type

named_game_type

is_new_game

average_reward

avg_rwd_CI_low

avg_rwd_CI_high

Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout

High card
High card
Match
Match
Pairs
Pairs
Straight ﬂush
Straight ﬂush
Blackjack
Blackjack
High card
Match
Pairs
Straight ﬂush
Straight ﬂush
Blackjack
Table 5: Table for basic meta-learning, Figure 2b

Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out
Trained
Trained
Trained
Trained
Held out
Trained

0.53
0.441
0.537
0.539
0.521
0.453
0.525
0.484
0.582
0.492
0.527
0.536
0.522
0.524
0.361
0.586

0.521
0.42
0.524
0.523
0.504
0.434
0.508
0.466
0.557
0.468
0.518
0.526
0.512
0.509
0.332
0.575

0.541
0.462
0.55
0.556
0.536
0.47
0.54
0.502
0.603
0.513
0.536
0.546
0.531
0.538
0.39
0.598

is_new_game

named_run_type

named_game_type

expected_reward

Trained
Trained
Trained
Trained
Held out
Trained
Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out

Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout

High card
Match
Pairs
Straight ﬂush
Straight ﬂush
Blackjack
High card
High card
Match
Match
Pairs
Pairs
Straight ﬂush
Straight ﬂush
Blackjack
Blackjack

0.531
0.541
0.532
0.537
0.274
0.592
0.531
0.396
0.541
0.541
0.532
0.37
0.536
0.452
0.595
0.456

Table 6: Table for playing most correlated learned strategy for basic meta-learning, dashed colored
lines in Figure 2b

named_game_type

expected_reward

Blackjack
High card
Match
Pairs
Straight ﬂush

0.592
0.531
0.541
0.532
0.536

Table 7: Table for playing optimal rewards for basic meta-learning, solid lines in Figure 2b

26

is_new_game

named_run_type

named_game_type

expected_reward

Trained
Trained
Trained
Trained
Held out
Trained
Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out

Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout

High card
Match
Pairs
Straight ﬂush
Straight ﬂush
Blackjack
High card
High card
Match
Match
Pairs
Pairs
Straight ﬂush
Straight ﬂush
Blackjack
Blackjack

0.531
0.541
0.532
0.537
0.274
0.592
0.531
0.396
0.541
0.541
0.532
0.37
0.536
0.452
0.595
0.456

Table 8: Table for most correlated baselines for basic meta-learning, dashed colored lines in Figure
2b

named_run_type

named_meta_task

Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout

Switch suits
Switch suits
Losers
Losers
Switch suits
Switch suits
Losers
Losers

is_new

Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out

average_reward

avg_rwd_CI_low

avg_rwd_CI_high

0.523
0.234
0.532
0.289
0.528
0.375
0.531
0.427

0.512
0.196
0.511
0.241
0.521
0.368
0.523
0.417

0.534
0.273
0.546
0.322
0.533
0.382
0.538
0.436

Table 9: Table for meta-mapping from examples, Figure 3b

named_run_type

named_meta_task

Language (random 50%)
Language (random 50%)
Language (random 50%)
Language (random 50%)
Language (targeted 10%)
Language (targeted 10%)
Language (targeted 10%)
Language (targeted 10%)

Switch suits
Switch suits
Losers
Losers
Switch suits
Switch suits
Losers
Losers

is_new

Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out

average_reward

avg_rwd_CI_low

avg_rwd_CI_high

0.525
0.371
0.524
0.426
0.531
0.225
0.539
0.341

0.52
0.353
0.521
0.413
0.52
0.146
0.533
0.308

0.534
0.384
0.527
0.44
0.542
0.305
0.544
0.367

Table 10: Table for meta-mapping from language, Figure 4b

named_run_type

named_meta_task

Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Targeted 10% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout
Random 50% holdout

Switch suits
Switch suits
Losers
Losers
Switch suits
Switch suits
Losers
Losers

is_new

Trained
Held out
Trained
Held out
Trained
Held out
Trained
Held out

expected_reward

0.298
0.368
-0.446
-0.463
0.37
0.278
-0.465
-0.444

Table 11: Table of rewards if system ignored meta-mapping, colored dashed lines in Figure 3b

27

