To appear in 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA).

Enabling Highly Efﬁcient Capsule Networks Processing
Through A PIM-Based Architecture Design

9
1
0
2

v
o
N
7

]

C
D
.
s
c
[

1
v
1
5
4
3
0
.
1
1
9
1
:
v
i
X
r
a

Xingyao Zhang
University of Houston
Houston, USA
xzhang55@uh.edu

Shuaiwen Leon Song
University of Sydney
Sydney, Australia
leonangel991@gmail.com

Jing Wang
Capital Normal University
Beijing, China
jwang@cnu.edu.cn

Weigong Zhang
Capital Normal University
Beijing, China
5591@cnu.edu.cn

Chenhao Xie
Paciﬁc Northwest National
Laboratory
Richland, USA
fenahuhu@gmail.com
Xin Fu
University of Houston
Houston, USA
xfu8@central.uh.edu

ABSTRACT
In recent years, the CNNs have achieved great successes
in the image processing tasks, e.g., image recognition
and object detection. Unfortunately, traditional CNN’s
classiﬁcation is found to be easily misled by increasingly
complex image features due to the usage of pooling op-
erations, hence unable to preserve accurate position and
pose information of the objects. To address this chal-
lenge, a novel neural network structure called Capsule
Network has been proposed, which introduces equiv-
ariance through capsules to signiﬁcantly enhance the
learning ability for image segmentation and object de-
tection. Due to its requirement of performing a high
volume of matrix operations, CapsNets have been gen-
erally accelerated on modern GPU platforms that pro-
vide highly optimized software library for common deep
learning tasks. However, based on our performance
characterization on modern GPUs, CapsNets exhibit
low eﬃciency due to the special program and execution
features of their routing procedure, including massive
unshareable intermediate variables and intensive syn-
chronizations, which are very diﬃcult to optimize at
software level. To address these challenges, we propose
a hybrid computing architecture design named PIM-
CapsNet. It preserves GPU’s on-chip computing capa-
bility for accelerating CNN types of layers in CapsNet,
while pipelining with an oﬀ-chip in-memory acceleration
solution that eﬀectively tackles routing procedure’s in-
eﬃciency by leveraging the processing-in-memory capa-
bility of today’s 3D stacked memory. Using routing pro-
cedure’s inherent parallellization feature, our design en-
ables hierarchical improvements on CapsNet inference
eﬃciency through minimizing data movement and max-
imizing parallel processing in memory. Evaluation re-
sults demonstrate that our proposed design can achieve
substantial improvement on both performance and en-
ergy savings for CapsNet inference, with almost zero
accuracy loss. The results also suggest good perfor-
mance scalability in optimizing the routing procedure
with increasing network size.

1.

INTRODUCTION

Recently, machine learning has bloomed into rapid
growth and been widely applied in many areas, includ-
ing medical [1, 2], security [3], social media [4], engineer-
ing [5] and etc. Such explosive development is cred-
ited to the success of the neural network algorithms,
especially the convolutional neural networks (CNNs),
which are extremely suitable for the image processing
tasks, e.g., image recognition and object detection [6,
7, 8, 9, 10]. However, recent studies have found that
CNNs could easily misdetect important image features
when image scenarios are more complex. This could
signiﬁcantly aﬀects the classiﬁcation accuracy [11, 12,
13]. As shown in Fig.1, when attempting to identify
the lung cancer cells, traditional CNNs are bounded to
the conﬁned region, thus missing critical regions around
the cell edges and leading to the wrong identiﬁcation.
This is because the pooling operations used in common
CNNs apply happenstance translational invariance [11,
14] which limits the learning of rotation and propor-
tional change, resulting in obtaining only partial fea-
tures. With the increasing adoption of emerging appli-
cations (e.g., medical image processing and autonomous
driving) into humans’ daily life that have strict require-
ments on object detection’s accuracy, wrong identiﬁ-
cation could be fatal in some cases. To address these
challenges, a novel neural network called Capsule Net-
work (CapsNet) has been proposed recently [14]. Fig.1
demonstrates the evolution from classic CNN identiﬁca-
tion to CapsNet identiﬁcation. CapsNet abandons the
usage of pooling operations in CNNs and introduces the
concept of capsule which is any function that tries to
predict the presence and the instantiation parameters of
a particular object at a given location. The ﬁgure illus-
trates a group of capsules, each with a double-featured
activation vector (i.e., probability of presence and pose,
shown as the green and red arrows in Fig.1). Because
of this added equivariance, CapsNet can accurately de-
tect the cancer cells via precise classiﬁcation according
to the cell edges and body texture. According to the re-

 
 
 
 
 
 
unique architectural features, our PIM-CapsNet design
mitigates the data-intensive RP into HMC to tackle its
major challenges above. There are two key objectives
for this in-memory design for RP: under the hardware
design constraints, creating a hierarchical optimization
strategy to enable (i) inter-vault workload balance and
communication minimization (Sec.5.1 and 5.3) and (ii)
intra-vault maximum parallel processing (Sec.5.2 and
5.3). Additionally, the in-memory optimizations should
be generally applicable to diﬀerent RP algorithms.

For objective (i), we further investigate RP’s algo-
rithm and identify an interesting feature: highly paral-
lelizable in multi-dimensions. This makes RP’s work-
loads have great potential to be concurrently executed
across vaults without incurring signiﬁcant communica-
tion overheads. We then create a modeling strategy
by considering both per-vault workloads and inter-vault
communication to guide dimension selection for paral-
lelization, in order to achieve the optimal performance
and power improvement. This also signiﬁcantly reduces
the original synchronization overheads through the con-
version to aggregation within a vault. For objective (ii),
we integrate multiple processing elements (PEs) into a
vault’s logic layer and explore the customized PE design
to concurrently perform RP’s speciﬁc operations across
memory banks. Meanwhile, we propose a new address
mapping scheme that eﬀectively transfers many inter-
vault level data requests to the intra-vault level and
address the bank conﬂict issues of concurrent data re-
quests. Furthermore, to reduce logic design complexity
and guarantee performance, we use simple low-cost logic
to approximate complex special functions with negligi-
ble accuracy loss. To summarize, this study makes the
following contributions:

• We conduct a comprehensive characterization study
on CapsNet inference on modern GPUs and iden-
tify its root causes for execution ineﬃciency.

• Based on the interesting insights from the charac-
terization and further algorithm analysis, we pro-
pose a processing-in-memory based hybrid com-
puting architecture named PIM-CapsNet, which
leverages both GPU on-chip computing capabil-
ity and oﬀ-chip in-memory acceleration features of
3D stacked memory to improve the overall Cap-
sNet inference performance.

• To drastically reduce the identiﬁed performance
bottlenecks, we propose several memory-level op-
timizations to enable minimal in-memory commu-
nication, maximum parallelization, and low design
complexity and overhead.

• The experimental results demonstrate that for the
overall CapsNet inference, our proposed PIM- Cap-
sNet design outperforms the baseline GPU by 2.44x
(upto 2.76x) on performance and 64.91% (upto
85.16%) on energy saving. It also achieves good
performance scalability with increasing network size.

2. BACKGROUND: CAPSULE NETWORK
As shown in Fig.2, CapsNet inherits the convolutional
(Conv) and fully connected (FC) layers from the stan-
dard CNNs, but introduces new layers (i.e., Caps layers)
to realize the concept of “capsule” for better information
representation. A capsule is a group of neurons (Fig.1)

Figure 1: The comparison of neural networks in identi-
fying lung cancer cells [17], where CapsNet outperforms
the traditional CNN on detection accuracy. The heat
maps indicate the detected features.

cent studies, CapsNets are increasingly involved in the
human-safety related tasks, and on average outperform
CNNs by 19.6% and 42.06% on detection accuracy for
medical image processing[15, 16, 17, 18, 19, 20] and au-
tonomous driving[21, 22, 23].

Because CapsNets execution exhibits a high percent-
age of matrix operations, state-of-the-art GPUs have
become primary platforms for accelerating CapsNets by
leveraging their massive on-chip parallelism and deeply
optimized software library [24, 25]. However, processing
eﬃciency of CapsNets on GPUs often cannot achieve
the desired level for fast real-time inference. To in-
vestigate the root causes of this ineﬃciency, we con-
duct a comprehensive performance characterization on
CapsNets’ execution behaviors on modern GPUs, and
observe that the computation between two consecutive
capsule layers, called routing procedure (Sec.2.2), presents
the major bottleneck. Through runtime proﬁling, we
further identify that the ineﬃcient execution of the rout-
ing procedure originates from (i) tremendous data ac-
cess to oﬀ-chip memory due to the massive unshare-
able intermediate variables, and (ii) intensive synchro-
nizations to avoid the potential write-after-read and
write-after-write hazards on the limited on-chip stor-
age. These challenges are induced by the unique fea-
tures of the routing procedure execution, and cannot be
addressed well via common NN optimization techniques
[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37] as well as
software-level on-chip memory management (e.g., reg-
ister manipulation or shared memory multiplexing).

To tackle CapsNets’ signiﬁcant oﬀ chip-memory ac-
cess and intensive synchronization (induced by numer-
ous aggregation operations in the routing procedure),
we propose a processing-in-memory based hybrid com-
puting architecture named PIM-CapsNet. At the high-
est level, PIM-CapsNet continues to utilize GPU’s na-
tive on-chip units as the host for fast processing CNN-
type of layers such as Convolution and Fully-connected
included in CapsNet. Meanwhile, by leveraging batch
execution, PIM-CapsNet pipelines the host GPU ex-
ecution with an oﬀ-chip in-memory solution that can
eﬀectively accelerate CapsNet’s routing procedure.

To enable the in-memory acceleration capability for
the routing procedure (RP), we select one of the emerg-
ing 3D stacked technologies, i.e., hybrid memory cube
(HMC)[38], for RP’s design and integration. Because
of HMC’s high internal and external memory band-
width, and its unique logic layer for easy integration
of computation logic, it has become a promising PIM
platform [39, 40, 41, 42, 43]. By leveraging HMC’s

2

Input ImageCNN IdentificationCapsNet IdentificationTheoryExampleFigure 2: The computation diagram of CapsNet for
MNIST [14].

Figure 3: Dynamic Routing Procedure (RP) in Cap-
sNet: describing computation ﬂow and multiple ways
for parallel processing.

whose activity vector represents instantiation param-
eters of a speciﬁc type entity (e.g., the location and
pose of an object).
It introduces equivariance which
makes standard CNNs understand rotation and propor-
tional change (e.g., in Fig.1). CapsNet signiﬁcantly lifts
the limitation of happenstance translational invariance
of pooling operations applied in the traditional CNNs,
thus being considered to be superior in image segmen-
tation and object detection [11, 14].
2.1 CapsNet Structure

Fig.2 takes CapsNet-MNIST [14] as an example to il-
lustrate a basic CapsNet structure. It contains two com-
putation stages: encoding and decoding. The encoding
stage is composed of Conv layers and Caps layers. The
convolutional operations are ﬁrst performed on the data
mapping from neurons of the Conv layer to the capsules
of the ﬁrst Caps layer, which is deﬁned as PrimeCaps
layer.
It is often followed by at least one other Caps
layer. The data mapping between the capsules of the
adjacent Caps layers is performed via the routing pro-
cedure (RP). Finally, the last Caps layer produces the
classiﬁcation information towards categorization, with
each capsule representing one category. Following the
encoding stage, the decoding function includes multiple
FC layers which attach to the last Caps layer for im-
age reconstruction (i.e., improving model accuracy in
training or plotting abstract features in inference) [14].
2.2 Essential Mechanism For Avoiding Feature

Loss: Routing Procedure (RP)

The routing procedure (RP) is introduced to route
the information from low-level capsules (or L capsules)
to high-level capsules (or H capsules) without feature
loss. There have been several routing algorithms used
in routing procedure such as Dynamic Routing [14] and
Expectation-Maximization routing[44]. In this work, we
use the popular Dynamic Routing [14] as an example to
explain the RP execution.

Fig.3 and Algorithm.1 demonstrate the computation
ﬂow and possible dimensions for parallelization. Given
the kth batched input set, in order to generate jth H
capsule, ith L capsule in this input set (uk
i ) ﬁrst mul-

Algorithm 1 Dynamic Routing Procedure

Input: L capsules u, weight matrix W
Output: H capsules v
1: for all L capsule i & all H capsule j from all input set k:

j|i ← uk
ˆuk

i × Wij
2: for all L capsule i & all H capsule j:

(cid:46) Eq.1

bij ← 0

3: for Routing iterations do
4:

for all L capsule i:

(cid:46) Initialize Routing Coeﬀcients

5:

6:

7:

cij ← sof tmax(bij )

for all H capsule j from all input set k:
j|i × cij
for all H capsule j from all input set k:

j ← (cid:80)
sk

i ˆuk

vk
j ← squash(sk
j )

for all L capsule i & H capsule j:
k vk

bij = (cid:80)

j|i + bij

j ˆuk

8: end for
9: Return v

(cid:46) Eq.5

(cid:46) Eq.2

(cid:46) Eq.3

(cid:46) Eq.4

tiplies with the corresponding weight (Wij) to generate
the prediction vector (ˆuj|i) (Fig.3 1 ):

j|i = uk
ˆuk

(1)
Then, these prediction vectors will multiply with their
corresponding routing coeﬃcients (cij) with results ag-
gregated across all L capsules (Fig.3 2 ):

i × Wij

j ||2

sk
j
||sk
j ||

j|i × cij

j = (cid:80)
sk

||sk
j ||2
1+||sk

(2)
i ˆuk
The non-linear“squashing”function is then implemented
on the aggregated results of sk
j to produce the jth H
capsule vj (Fig.3 3 ):
vk
j =

(3)
Note that the vk
j can not be considered as the ﬁnal value
of jth H capsule unless the features of L capsules have
been correctly inherited. The information diﬀerence be-
tween an L and H capsule can be quantiﬁed by the
agreement measurement via the scalar production of the
prediction vector ˆuk
j (Fig.3 4 ),
where the “0” output means the information is precisely
inherited. In the case of a large divergence, the agree-
ments will be accumulated into an intermediate variable
bij (Fig.3 5 ), which will be used to update the routing
coeﬃcients via the “softmax” function (Fig.3 6 ).

j|i and the H capsule vk

bij = (cid:80)

j ˆuk

j|i + bij

k vk
cij = exp(bij )
(cid:80)
The updated routing coeﬃcients will then be integrated
in Eq.(2) to start another iteration in this routing pro-
cedure (Fig.3 2 ).

k exp(bik)

(5)

(4)

The number of iterations is determined by the con-
vergence of routing coeﬃcients and set by programmers.
Several recent studies indicate that the number of iter-
ations increases for tasks with large datasets and cat-
egories [45, 46]. Once all the iterations complete, the
features of L capsules should have already been routed
to the H capsules and ready to proceed to the following
layers.

Summary. Generally, the routing algorithms (e.g.,
Dynamic Routing, Expectation-Maximization Routing)
share the similar execution pattern and exhibit several
core features in CapsNet routing procedure: (1) The
execution of the RP exhibits strong data dependency
and needs to be sequentially processed. (2) The proce-
dure adopts all-to-all computation, which routes all the

3

b0jbijPrimaryCaps LayerDigitCaps LayerFully-ConnectedLayerConvolutional LayerClassification Results: DogReconstructiondx=[32 x 32] Input[20 x 20] 256 channel[6 x 6] 8D Capsules32 channel[10 x 1] 16D Capsules5121024784EncoderDecoder2568321016Child CapsulePrediction VectorVoting ResultsWeightRouting CoefficientChild CapsulePrediction VectorParent CapsuleWeightRouting Coefficient∑ xPredictionVectorWeightsRoutingCoefficientsxPrimaryCaps LayerDigitCaps LayerFully-ConnectedLayerConvolutional LayerClassification Results“3”ReconstructionEncoderDecoderDynamicRoutingProcedurewijuib0jbiju0w00×c00û00s0×v0û00b00×∑uiwi0×ûi0ci0×ûi0bi0×b0jbijb0jbiju0w0j×c0jû0jsj×vjû0jb0j×∑uiwij×cijûij×ûijbij×........................L CapsulesL CapsulesBatches...BatchesH Capsules∑∑∑∑b00bi0b0jbijSS......PrimaryCaps LayerDigitCaps LayerFully-ConnectedLayerConvolutional LayerClassification Results: DogReconstruction[32 x 32] Input[20 x 20] 256 channel[6 x 6] 8D Capsules32 channel[10 x 1] 16D Capsules5121024784EncoderDecoder2568321016[28 x 28] Input[20 x 20] 256 channel256[6 x 6] 8D Capsules32 channel321016[10 x 1] 16D Capsules5121024784123456PrimaryCaps LayerFinal Caps LayerFully-ConnectedLayerConvolutional LayerReconstructionEncodingDecodingRoutingProcedure(Dynamic Routing)[28 x 28] Input[20 x 20] 256 channel256[6 x 6] 8D Capsules32 channel321016[10 x 1] 16D Capsules5121024784Classification Results“3”DynamicRoutingProcedurePrimaryCaps LayerLast Caps LayerFully-ConnectedLayerConvolutional LayerReconstructionEncodingDecodingRouting Procedure(Dynamic Routing)256Low-level Capsules321016High-levelCapsules5121024784Classification Results“3”InputInput Imageb0jbijb0jbiju0w00×c00û00s0×v0û00b00×∑uiwi0×ûi0ci0×ûi0bi0×b0jbijb0jbiju0w0j×c0jû0jsj×vjû0jb0j×∑uiwij×cijûij×ûijbij×........................L CapsulesL Capsules...BatchesH Capsules∑∑∑∑b00bi0b0jbijSS......123456Batchesb0jbijPrimaryCaps LayerDigitCaps LayerFully-ConnectedLayerConvolutional LayerClassification Results: DogReconstructiondx=[32 x 32] Input[20 x 20] 256 channel[6 x 6] 8D Capsules32 channel[10 x 1] 16D Capsules5121024784EncoderDecoder2568321016Child CapsulePrediction VectorVoting ResultsWeightRouting CoefficientChild CapsulePrediction VectorParent CapsuleWeightRouting Coefficient∑ xPredictionVectorWeightsRoutingCoefficientsxPrimaryCaps LayerDigitCaps LayerFully-ConnectedLayerConvolutional LayerClassification Results“3”ReconstructionEncoderDecoderDynamicRoutingProcedurewijuib0jbiju0w00×c00û00s0×v0û00b00×∑uiwi0×ûi0ci0×ûi0bi0×b0jbijb0jbiju0w0j×c0jû0jsj×vjû0jb0j×∑uiwij×cijûij×ûijbij×........................L CapsulesL CapsulesBatches...BatchesH Capsules∑∑∑∑b00bi0b0jbijSS......PrimaryCaps LayerDigitCaps LayerFully-ConnectedLayerConvolutional LayerClassification Results: DogReconstruction[32 x 32] Input[20 x 20] 256 channel[6 x 6] 8D Capsules32 channel[10 x 1] 16D Capsules5121024784EncoderDecoder2568321016[28 x 28] Input[20 x 20] 256 channel256[6 x 6] 8D Capsules32 channel321016[10 x 1] 16D Capsules5121024784123456PrimaryCaps LayerFinal Caps LayerFully-ConnectedLayerConvolutional LayerReconstructionEncodingDecodingRoutingProcedure(Dynamic Routing)[28 x 28] Input[20 x 20] 256 channel256[6 x 6] 8D Capsules32 channel321016[10 x 1] 16D Capsules5121024784Classification Results“3”DynamicRoutingProcedurePrimaryCaps LayerFinal Caps LayerFully-ConnectedLayerConvolutional LayerReconstructionEncodingDecodingRouting Procedure(Dynamic Routing)256Low-level Capsules321016High-levelCapsules5121024784Classification Results“3”InputInput Imageb0jbijb0jbiju0w00×c00û00s0×v0û00b00×∑uiwi0×ûi0ci0×ûi0bi0×b0jbijb0jbiju0w0j×c0jû0jsj×vjû0jb0j×∑uiwij×cijûij×ûijbij×........................L CapsulesL Capsules...BatchesH Capsules∑∑∑∑b00bi0b0jbijSS......123456BatchesTable 1: CapsNet Benchmark Conﬁgurations

Dataset

MNIST[47]

Net-
work
Caps-MN1
Caps-MN2
Caps-MN3
Caps-CF1
Caps-CF2
Caps-CF3
Caps-EN1
Caps-EN2 EMNIST Balanced[49]
Caps-EN3 EMNIST By Class[49]
Caps-SV1
Caps-SV2
Caps-SV3

EMNIST Letter[49]

CIFAR10[48]

SVHN[50]

Conﬁguration

BS L Caps H Caps
100
200
300
100
100
100
100
100
100
100
100
100

1152
1152
1152
2304
3456
4608
1152
1152
1152
576
576
576

10
10
10
11
11
11
26
47
62
10
10
10

Iter
3
3
3
3
3
3
3
3
3
3
6
9

Figure 4: The overall execution time breakdown of Cap-
sNets on GPU across diﬀerent layers. Red line repre-
sents the actual inference time.

L capsules to the H capsules and forms aggregation in
all possible dimensions. (3) The procedure produces a
large amount of intermediate variables. (4) The routing
procedure is iteratively processed to generate the dy-
namic coeﬃcients to pass the feature information. We
will discuss these features in relation to performance
characterization of CapsNet in Sec.3, and our optimiza-
tions on Dynamic Routing in the following Sections can
be easily applied to other routing algorithms with sim-
ple adjustment.
3. CHARACTERIZATION AND ANALYSIS
While the CapsNet starts gaining popularity from
both academia and industry, the performance charac-
terization of CapsNet on modern high-performance plat-
forms is largely neglected. Given that GPU has become
the major platform for executing CapsNet due to high
computation capability and deep optimization on ma-
trix operations (which CapsNet has a large amount of),
we adopt some of the state-of-the-art NVIDIA GPU
platforms to conduct a comprehensive characterization
towards the execution behaviors of various CapsNets
listed in Table 1. We use 4 diﬀerent datasets and corre-
sponding 12 CapsNets with CapsNet-MNIST like struc-
ture (Sec.2.1), and diﬀerent conﬁgurations on batch size
(BS in Table 1), L capsules, H capsules and routing iter-
ation number. These CapsNets’ inference are processed
via PyTorch framework [51] with the latest deep learn-
ing library (i.e., CuDNN [52]), which already enables
the state-of-the-art CNN optimizations [53].
3.1 Overall Evaluation for CapsNet Inference
Fig.4 demonstrates the performance breakdown of
each layer to the overall CapsNet execution. It shows
that, across diﬀerent CapsNet conﬁgurations, the rout-
ing procedure (RP) accounts for an average of 74.62%
of the entire inference time, becoming the major perfor-
mance bottleneck. we further conduct detailed analysis
on the performance results of CapsNets on GPU and
make the following observations:
Observation 1: ineﬀectiveness of batched execu-
tion. One common strategy to accelerate CNNs is to
conduct batched execution for improving hardware uti-
lization, especially when input dataset is large. How-
ever, it cannot improve the RP’s performance during
inference. As Fig.4 illustrates, with the increasing of
batch size (i.e., Caps-MN1 → Caps-MN3),the overall
CapsNet inference time increases; meanwhile, the RP
proportion also expands with batch size.
Observation 2: sensitivity to network scaling.
Table 1 shows the network size (formed by a combi-
nation of L capsules, H capsules and routing iterations)
for each individual case, e.g., Caps-SV1 being the small-
est. The red curve in Fig.4 also demonstrates that the

Figure 5: The breakdown for pipeline stall cycles during
RP execution on Tesla P100 Pascal GPU.

overall inference time and RP’s percentage generally in-
creases when scaling up the network size (e.g., compar-
ing Caps-MN1, Caps-CF1, Caps-EN1 and Caps-SV1).
This implies that RP’s execution time is sensitive to
network size as well.

To summarize, using the highly optimized deep learn-
ing library, the RP execution time on GPU can not be
eﬀectively reduced through the general CNN optimiza-
tion techniques such as batch execution. Moreover, it
exhibits a certain level of sensitivity to network scaling.
Both of these factors make the RP execution a dom-
inating performance bottleneck for CapsNet inference,
especially with a growing size and complexity of future
CapsNet structures[45, 46].
3.2 Root Causes for Inefﬁcient RP Execution
To understand the root causes of RP’s ineﬃciency on
GPU, we use NVproﬁler [54] to collect runtime GPU
stats for comprehensive analysis. We observe two root
causes for poor RP execution eﬃciency on GPU-like
architectures:

(1) Signiﬁcant Oﬀ-Chip Memory Accesses: We
proﬁle the utilization of several major GPU function
units during RP execution on a NVIDIA Tesla P100
GPU. We observe that the arithmetic logic unit (ALU)
is lightly utilized (i.e., on average only 38.6% across
the investigated benchmarks) while the load/store unit
(LDST) is heavily stressed with an average utilization
of 85.9%. This implies that CapsNets’ RP phase fails
to eﬀectively leverage the GPU strong computation ca-
pability and is severely limited by the intensive oﬀ-chip
memory access. We further investigate the factors that
may contribute to GPU pipeline stalls during RP, in-
cluding the oﬀ-chip memory access, barrier synchroniza-
tion, lack of resource, etc. Fig.5 proﬁles the individual
contribution of each major factor to the overall pipeline
stall cycles. As can be seen, the majority of the pipeline
stalls are induced by the memory access (i.e., on average
44.64%). This further conﬁrms that RP performance
is signiﬁcantly limited by the oﬀ-chip memory access.
This is caused by a combination of massive interme-
diate variables from RP execution and limited on-chip
storage. Fig.6(a) illustrates the ratio of RP’s interme-
diate variables’ size to on-chip memory sizes of diﬀerent
generations of NVIDIA GPUs. As can be seen, the size
of RP’s intermediate variables far exceeds the GPU on-

4

02468101214160%20%40%60%80%100%Execution Time (s)Percentage of Execution TimeConv LayerL Caps LayerH Caps LayerFC LayerExecution Time74.62%0%25%50%75%100%Contributions to Pipeline StallsMemory AccessSynchronizationLack of ResourceInst_FetchOther44.64%34.45%(a)

(b)
Figure 6: (a) Ratio of intermediate variables’ size to on-
chip storage of diﬀerent GPUs; (b) the impact of on-chip
storage sizes of state-of-the-art GPUs on RP’s execu-
tion. A: 1.73MB (K40m), B: 5.31MB (Tesla P100), C:
9.75MB (RTX2080Ti), D: 16MB (Tesla V100).

Figure 7: The impact of memory bandwidth on the
overall RP performance. GDDR5:288GB/s (K40m),
GDDR5X: 484GB/s (GTX 1080Ti), GDDR6: 616GB/s
(RTX 2080Ti), HBM2: 897GB/s (Tesla V100)

chip storage. Given the iterative computation pattern
of RP, these variables (e.g., ˆu, sj, vj, bij, cij) need to be
periodically loaded into GPU cores from the oﬀ-chip
memory due to the limited on-chip storage. Moreover,
the intermediate variables are not sharable among dif-
ferent input batches, which also explains the ineﬀec-
tiveness of batched execution as we observed in Sec.3.1.
Due to the large data volume and lack of temporal value
similarity from these variables, software-level schemes
such as register manipulation and shared memory mul-
tiplexing are also not very eﬀective.
(2) Intensive Synchronization: Fig.5 also indicates
that the frequent barrier synchronization is the sec-
ond major contributor (i.e., on average 34.45%) to the
pipeline stalls. These synchronization overheads are
syncthread() calls, which coordinate
induced by the
shared memory accesses for all threads in one thread
block. There are two major factors causing the frequent
synchronization during the RP execution: (i) the RP ex-
ecution contains numerous aggregation operations (e.g.,
Eq.(2)), inducing massive data communication between
the threads through shared memory; (ii) the size of the
intermediate variables far exceeds the shared memory
size, leading to frequent data loading from the global
memory. Thus, syncthread() calls occur frequently to
avoid the potential write-after-write and write-after-read
hazards.

To address these issues above, we attempt to apply
two naive solutions: scaling up the on-chip and oﬀ-chip
memory capacity. Fig.6(b) shows the impact of on-
chip memory sizes of diﬀerent generations of NVIDIA
GPUs on RP execution. We can observe that increasing
on-chip memory size can help alleviate the challenges
above but not very eﬀective, e.g., only up to an average
of 14% performance improvement for the 16MB V100.
This is because the nonsharable intermediate variables’
size from RP still far exceeds the current GPUs’ on-

5

chip storage, shown in Fig.6(a). Similarly, Fig.7 shows
that only increasing memory bandwidth from 288GB/s
GDDR5 to 897 GB/s HBM slightly improves the over-
all RP’s performance by an average of 26%. This indi-
cates that higher oﬀ-chip memory bandwidth can only
solve a small part of the problem but itself does not re-
duce the high intensity of the oﬀ-chip memory accesses.
Therefore, to signiﬁcantly improve CapsNet’s inference
performance, we need a customized solution to address
the root causes for RP’s ineﬃcient execution.
4. BASIC IDEA: PROCESSING-IN-MEMORY

+ PIPELINING

As discussed previously, we aim to address CapsNets’
signiﬁcant oﬀ chip-memory access caused by massive
unshareable intermediate variables and intensive syn-
chronization due to numerous aggregation operations
in RP. Meanwhile, we also want to utilize the excellent
core computing capability provided by modern GPUs
for deep learning’s matrix operations. Thus, we pro-
pose a hybrid computing engine named “PIM-CapsNet”,
shown in Fig.8. It utilizes GPU’s native on-chip units
as the host for fast processing layers such as Conv and
FC, while pipelining with an oﬀ-chip in-memory accel-
eration that eﬀectively tackles RP’s ineﬃcient execu-
tion. For example, since multiple input sets are gen-
erally batched together to be concurrently processed
in RP to avoid the local optimal solution of the rout-
ing coeﬃcients [55], host processors can start process-
ing Conv/FC operations from the diﬀerent batches of
the input sets while waiting for RP’s results from in-
memory processing on the current batch, forming an
execution pipeline. Furthermore, the in-memory accel-
erators of our PIM-CapsNet design can hierarchically
improve RP’s execution eﬃciency by minimizing data
movement and maximizing parallel processing. Note
that our proposed design is a general optimization so-
lution that is applicable to diﬀerent routing algorithms
used in RP.

To build our in-memory acceleration capability for
RP, we resort to one of the emerging 3D stacked tech-
nologies, i.e., hybrid memory cube (HMC)[38], which
has become a promising PIM platform [39, 40, 41, 42,
43]. The major reason to replace current GPU’s oﬀ-
chip memory (e.g., GDDRX or HBMs) with HMC in
our design is their lack of logic layer for in-memory in-
tegration. As illustrated in Fig.9, HMC stacks several
DRAM dies on the CMOS logic layer as a cube (speci-
ﬁcation 2.1 [56]). The memory cube connects with the
host processor (i.e., GPU cores in our case) via the fully-
duplex links which can provide up to 320GB/s of exter-
nal memory bandwidth. Additionally, the logic layer is
split into 32 sub-memory controllers with each commu-
nicates its local DRAM banks through Through-Silicon
Vias (TSVs) which together provide an internal mem-
ory bandwidth of 512GB/s [38, 41]. The sub-memory
controller and its local DRAM partitions (each parti-
tion contains multiple DRAM banks) form the vault ar-
chitecture, as highlighted in the red dashed box. The
logic layer receives system commands and routes mem-
ory access to diﬀerent vaults. A crossbar is integrated
in the logic layer to support the communication be-
tween SerDes links and vaults. Note that relatively sim-
ple computation logic can be integrated onto HMC’s

0x10x20x30x40xRatio of Intermediate Variable to On-chip StorageRatio_ARatio_BRatio_CRatio_D49x106x169x55x104x155x50x205x67x128x41x231x75x41x305x99x54x42x128x42x50x67x41x75x41x99x54x42x1.091.111.1140.80.91.01.11.2Normalized Routing PerformancePerf_APerf_BPerf_CPerf_D1.261.191.140.00.51.01.5Normalized RP PerformanceGDDR5GDDR5XGDDR6HBM2Figure 8: The overview of PIM-CapsNet Design.

Table 2: Possible Parallelizable Dimensions

Batch
(B-dimension)
x
x
x

Low-level Caps
(L-dimension)
x

x
x

Eq.1
Eq.2
Eq.3
Eq.4
Eq.5

High-level Caps
(H-dimension)
x
x
x
x

logic layer which can directly access data from vaults
via memory controllers and beneﬁt from large internal
memory bandwidth. This layer is very suitable for inte-
grating in-memory accelerators for RP execution. Next,
we will discuss the detailed PIM-CapsNet design.

5. PIM-CAPSNET ARCHITECTURE DESIGN
There are two key design objectives in PIM-CapsNet:

(i) maintaining workload balance with minimal data
communication at inter-vault level (Sec.5.1 and Sec.5.3),
and (ii) maximizing parallel processing at intra-vault
level but within architecture design constraints (Sec.5.2
and Sec.5.3).
5.1

Inter-Vault Level Design

A typical HMC design adopts crossbar switch to sup-
port the inter-vault communication [56], and the over-
all HMC performance power/thermal eﬃciency can be
drastically impacted by the increasing data movements
across vaults. Employing a more complicated data ﬂow
management (e.g., network-on-chip) on the HMC logi-
cal layer would further exacerbate the thermal issue. In
our proposed design, we leverage the unique features of
the RP execution and intelligently distribute workloads
across vaults with minimal inter-vault communication.

5.1.1 RP’s Unique Feature: Highly Parallelizable in

Multi-Dimensions

As an interesting feature, the operations of RP equa-
tions (Sec.2.2) are highly parallelizable. For example,
in Eq.(1), the vector-matrix multiplications for all the
low- to high-level (L-H) capsule pairs are independent.
We deﬁne such independent operations on L capsules
as parallelism in the L-dimension, while the indepen-
dent operations on H capsules are deﬁned as parallelism
in the H-dimension. Additionally, if operations corre-
sponding to diﬀerent batches are independent, they are
deﬁned as parallelism in the B-dimension. Thus, we
make the following key observations:

Observation I: Operations of each equation in the
RP can be partitioned into multiple independent sub-
operations on at least one of the three dimensions (L,
H, or B), suggesting highly parallelizable feature.

Table 2 further demonstrates which possible dimen-
sions these ﬁve equations of the dynamic routing proce-
dure can be parallelized through. Based on it, we also
make the second key observation:

Observation II: All the RP equations cannot be con-

currently executed through the same dimension.

6

Figure 9: Left: HMC Organization. Right:
HMC Block Diagram. The Red dashed box
marks the vault structure.

Observation I indicates that the inter-vault data com-
munication can be reduced by parallelizing the inde-
pendent sub-operations of each equation on one cho-
sen dimension and then distributing them across HMC
vaults. By doing so, the major communication across
vaults is only required by the aggregation operations
at that dimension (aggregations required by the other
two dimensions will be performed locally within vaults),
which is relatively low. Observation II, however, em-
phasizes that the inter-vault communication can not be
fully eliminated for RP as none of the three dimensions
can support the parallelization for all the RP equations.
When distributing the entire RP workloads on a certain
dimension, some related data have to be reloaded to an-
other designated vault for aggregation operations.

5.1.2 Parallelization and Workload Distribution

Since distributing the workloads on diﬀerent dimen-
sions leads to diﬀerent communication overheads and
execution patterns, it is important to apply an intelli-
gent workload distributor to achieve the optimal design
for power and performance. To minimize the inter-vault
communication, our distributor only distributes the RP
workload on a single chosen dimension. Fig.10 illus-
trates an example of the RP execution ﬂow when dis-
tributing on the B-dimension. As it shows, the RP op-
erations of Eq.1,2,3 ( 1 2 3 ) exhibit parallelism along
the B-dimension. Additionally, the multiplication oper-
ations of Eq.4 ( 4 ) (i.e., vk
j ×ˆuk
j|i) can also be parallelized
along the B-dimension. These parallelizable workloads
can be divided into snippets (i.e. green blocks) and dis-
tributed across the HMC vaults. Note that typical Cap-
sNet workloads will generate way more snippets than
the number of vaults in the HMC (e.g., up to 32 vaults
in HMC Gen3).

Due to the aggregation requirements on all the di-
mensions during RP execution, there are always some
workloads that cannot be processed in parallel on the
selected dimension, leading to workload imbalance. For
instance, the remaining operations of the RP proce-
dure in Fig.10 (i.e., the purple blocks), including partial
Eq.4 ( 5 ) and Eq.5 ( 6 ) cannot be divided into snip-
pets due to the lack of parallelism on the B-dimension.
Additionally, these workloads usually require data to
be aggregated in one place, making it hard to utilize
all the HMC computation resources. Furthermore, the
data size requested by the aggregation operations can
be large, which may cause massive inter-vault communi-
cation and even the crossbar stalls. To reduce the inter-
vault communication and increase hardware utilization,
we propose to pre-aggregate the partial aggregation op-
erations inside each vault for the corresponding allo-
cated snippets. For example, when the RP workloads
are distributed on B-dimension, the pre-aggregation can

Cross Bar (Switch)Vault LogicVault LogicVault LogicVault Logic......DRAMDiesLogic LayerVault...Cross Bar (Switch)PartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionTSVHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-MemoryControllerI/OPEPEPEPEPEPEPEPESub-Memory ContollerI/OE-CtrlVaultRegRegRegPEPEPEPEPEPEPEPEPEPEPEPERegRegRegE-Ctrl...Cross Bar (Switch)PartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM Die     Sub-Memory                    ContollerPEPEPE...Instruction BufferCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......DynamicRouting ProcedureIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsPerformance EvaluationDevice ConfigWorkloads AllocationDR ProcedureDevice ConfigPEPEPE...DRAM bankDRAM BankDRAM Bank...OperationSimplificationW/ PE designMemory Addressing     Sub-Memory                    ContollerParallelProcessingDataExecutionIntra-Vault Level DesignInter-Vault Level DesignCross Bar (Switch)Vault LogicVault LogicVault LogicVault Logic......Workloads AllocationDR ProcedureDevice ConfigPEPEPE...DRAM bankDRAM BankDRAM Bank...OperationSimplificationW/ PE designMemory Addressing     Sub-Memory                    ContollerParallelProcessingDataExecutionIntra-Vault Level DesignInter-Vault Level DesignConv/PrimeCaps/FCSub-MemoryControllerPEPEPE...Host ProcessorInter-Vault Level DesignIntra-Vault Level DesignDistributionWorkloadsArchitectureCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......DR ProcedureConv/PrimeCaps/FC     Sub-Memory                    ContollerPEPEPE...Instruction BufferHost ProcessorParallel ProcessingSub-OperationsInter-Vault Level DesignIntra-Vault Level DesignDistributionWorkloadsArchitectureCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Parallel ProcessingSub-OperationsDRAMBankDRAMBankDRAMBank...Customized Address MappingIntra-vault DataVault LogicVaultStorageSplit WorkloadsConv/PrimeCaps/FCSub-MemoryControllerPEPEPE...Host ProcessorInter-Vault Level DesignIntra-Vault Level DesignDistributionWorkloadsArchitectureCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Parallel ProcessingSub-OperationsDRAMBankDRAMBankDRAMBank...Customized Address MappingIntra-vault DataVault LogicVaultStorageInter-Vault Level DesignCapsNetConv/PrimeCaps/FCCross Bar (Switch)Vault LogicVault LogicVault LogicVault Logic......DR ProcedureConv/PrimeCaps/FCDR ProcedureHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-Memory ControllerPEPEPE...Vault LogicSplit WorkloadsCustomized PE DesignSub-workloadSub-workloadSub-workloadSub-workloadSub-workloadSub-workloadWorkloadDivisionInter-Vault DesignIntra-Vault DesignDRAMBankDRAMBankDRAMBank...VaultStorageIntra-vault DataCustomized Address MappingCustomizedAddress MappingParallel ProcessingConvPrimeCapsDR ProcedureFCCapsNetInter-level WorkloadDistributionSub-OperationsIntra-level Workload DistributionConv/PrimeCaps/FCDR ProcedureHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-Memory ControllerPEPEPE...Vault LogicCustomized PE DesignSplitworkloadSplitworkloadSplitworkloadSplitworkloadSplitworkloadSplitworkloadWorkloadDivisionInter-Vault DesignIntra-Vault DesignDRAMBankDRAMBankDRAMBank...VaultStorageIntra-vault DataCustomized Address MappingCustomizedAddress MappingParallel ProcessingConvPrimeCapsDR ProcedureFCCapsNetSub-OperationsIntra-level Workload DistributionIntelligent Workload DistributorSplit WorkloadsDevice ConfigInter-level Workload DistributionDR ProcedureHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-Memory ControllerPEPEPE...Vault LogicCustomized PE Design(Section5.2.2)SplitworkloadSplitworkloadSplitworkloadSplitworkloadSplitworkloadSplitworkloadInter-Vault Level DesignIntra-Vault Level DesignDRAMBankDRAMBankDRAMBank...VaultStorageIntra-vault DataCustomized Address MappingCustomizedAddress Mapping(Section5.3)CapsNetSub-OperationsIntra-vault Workload Distribution (Section5.2.1)Intelligent Workload Distributor(Section5.1.2)Split WorkloadsDevice ConfigInter-level Workload DistributionConvPrimeCapsDR ProcedureFCConv layer PrimeCapsFC layerParallel ProcessingHost GPUSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieHMC Logic LayerCross Bar (Switch)Vault LogicVault Logic...I/ORuntime Memory Access Scheduler(Sec.5.3.2)Vault LogicGPUCONV/FC/PrimeCapsRouting Procedure WorkloadsDRAMBankDRAMBankDRAMBank...VaultStorageCustomized PE Design(Section5.2.2)Sub-Memory ControllerPEPEPE...Vault LogicDRAMBankDRAMBankDRAMBank...VaultStorageOperationsPipelined ProcessingInter-vault Level Design (Sec.5.1)DR ProcedureHost GPUSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-Memory ControllerPEPEPE...Vault LogicCustomized PE Design(Sec.5.2.2)SnippetSnippetSnippetSnippetSnippetSnippetInter-Vault Level DesignIntra-Vault Level DesignDRAMBankDRAMBankDRAMBank...VaultStorageIntra-vault DataCustomizedAddress Mapping(Sec.5.3.1)CapsNetSub-OperationsIntra-vault Workload Distribution (Sec.5.2.1)Intelligent Workload Distributor(Sec.5.1.2)Workload SnippetsConvPrimeCapsDR ProcedureFCConv layer PrimeCapsFC layerRMASModuleRMAS(Sec.5.3.2)Routing Procedure WorkloadsCONV/FC/PrimeCapsRuntime Memory Access Schduler(Sec.5.3.2)PipelinedIntra-vault DataIntra-vault Level Design (Sec.5.2)CapsNetDRAMDiesLogic LayerVault...Cross Bar (Switch)PartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionTSVMemory Address Mapping Mechanism(Sec.5.3.1)Cross Bar (Switch)Vault LogicVault LogicVault LogicVault Logic......DRAMDiesLogic LayerVault...Cross Bar (Switch)PartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionTSVHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-MemoryControllerI/OPEPEPEPEPEPEPEPESub-Memory ContollerI/OE-CtrlVaultRegRegRegPEPEPEPEPEPEPEPEPEPEPEPERegRegRegE-Ctrl...Cross Bar (Switch)PartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM Die     Sub-Memory                    ContollerPEPEPE...Instruction BufferCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......DynamicRouting ProcedureIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsIndependent WorkloadsPerformance EvaluationDevice ConfigWorkloads AllocationDR ProcedureDevice ConfigPEPEPE...DRAM bankDRAM BankDRAM Bank...OperationSimplificationW/ PE designMemory Addressing     Sub-Memory                    ContollerParallelProcessingDataExecutionIntra-Vault Level DesignInter-Vault Level DesignCross Bar (Switch)Vault LogicVault LogicVault LogicVault Logic......Workloads AllocationDR ProcedureDevice ConfigPEPEPE...DRAM bankDRAM BankDRAM Bank...OperationSimplificationW/ PE designMemory Addressing     Sub-Memory                    ContollerParallelProcessingDataExecutionIntra-Vault Level DesignInter-Vault Level DesignConv/PrimeCaps/FCSub-MemoryControllerPEPEPE...Host ProcessorInter-Vault Level DesignIntra-Vault Level DesignDistributionWorkloadsArchitectureCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......DR ProcedureConv/PrimeCaps/FC     Sub-Memory                    ContollerPEPEPE...Instruction BufferHost ProcessorParallel ProcessingSub-OperationsInter-Vault Level DesignIntra-Vault Level DesignDistributionWorkloadsArchitectureCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Parallel ProcessingSub-OperationsDRAMBankDRAMBankDRAMBank...Customized Address MappingIntra-vault DataVault LogicVaultStorageSplit WorkloadsConv/PrimeCaps/FCSub-MemoryControllerPEPEPE...Host ProcessorInter-Vault Level DesignIntra-Vault Level DesignDistributionWorkloadsArchitectureCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Parallel ProcessingSub-OperationsDRAMBankDRAMBankDRAMBank...Customized Address MappingIntra-vault DataVault LogicVaultStorageInter-Vault Level DesignCapsNetConv/PrimeCaps/FCCross Bar (Switch)Vault LogicVault LogicVault LogicVault Logic......DR ProcedureConv/PrimeCaps/FCDR ProcedureHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-Memory ControllerPEPEPE...Vault LogicSplit WorkloadsCustomized PE DesignSub-workloadSub-workloadSub-workloadSub-workloadSub-workloadSub-workloadWorkloadDivisionInter-Vault DesignIntra-Vault DesignDRAMBankDRAMBankDRAMBank...VaultStorageIntra-vault DataCustomized Address MappingCustomizedAddress MappingParallel ProcessingConvPrimeCapsDR ProcedureFCCapsNetInter-level WorkloadDistributionSub-OperationsIntra-level Workload DistributionConv/PrimeCaps/FCDR ProcedureHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-Memory ControllerPEPEPE...Vault LogicCustomized PE DesignSplitworkloadSplitworkloadSplitworkloadSplitworkloadSplitworkloadSplitworkloadWorkloadDivisionInter-Vault DesignIntra-Vault DesignDRAMBankDRAMBankDRAMBank...VaultStorageIntra-vault DataCustomized Address MappingCustomizedAddress MappingParallel ProcessingConvPrimeCapsDR ProcedureFCCapsNetSub-OperationsIntra-level Workload DistributionIntelligent Workload DistributorSplit WorkloadsDevice ConfigInter-level Workload DistributionDR ProcedureHost ProcessorSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-Memory ControllerPEPEPE...Vault LogicCustomized PE Design(Section5.2.2)SplitworkloadSplitworkloadSplitworkloadSplitworkloadSplitworkloadSplitworkloadInter-Vault Level DesignIntra-Vault Level DesignDRAMBankDRAMBankDRAMBank...VaultStorageIntra-vault DataCustomized Address MappingCustomizedAddress Mapping(Section5.3)CapsNetSub-OperationsIntra-vault Workload Distribution (Section5.2.1)Intelligent Workload Distributor(Section5.1.2)Split WorkloadsDevice ConfigInter-level Workload DistributionConvPrimeCapsDR ProcedureFCConv layer PrimeCapsFC layerParallel ProcessingHost GPUSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieHMC Logic LayerCross Bar (Switch)Vault LogicVault Logic...I/ORMAS(Sec.5.3.2)Vault LogicGPUCONV/FC/PrimeCapsRouting Procedure WorkloadsDRAMBankDRAMBankDRAMBank...VaultStorageCustomized PE Design(Section5.2.2)Sub-Memory ControllerPEPEPE...Vault LogicDRAMBankDRAMBankDRAMBank...VaultStorageOperationsParallel ProcessingInter-vault Level Design (Sec.5.1)DR ProcedureHost GPUSilicon InterposerLogic LayerDRAM DieDRAM DieDRAM DieCross Bar (Switch)Vault LogicVault LogicVault LogicVault LogicVault LogicVault Logic......Sub-Memory ControllerPEPEPE...Vault LogicCustomized PE Design(Sec.5.2.2)SnippetSnippetSnippetSnippetSnippetSnippetInter-Vault Level DesignIntra-Vault Level DesignDRAMBankDRAMBankDRAMBank...VaultStorageIntra-vault DataCustomizedAddress Mapping(Sec.5.3.1)CapsNetSub-OperationsIntra-vault Workload Distribution (Sec.5.2.1)Intelligent Workload Distributor(Sec.5.1.2)Workload SnippetsConvPrimeCapsDR ProcedureFCConv layer PrimeCapsFC layerRMASModuleRMAS(Sec.5.3.2)Routing Procedure WorkloadsCONV/FC/PrimeCapsRuntime Memory Access Schduler(Sec.5.3.2)PipelinedIntra-vault DataInter-vault Level Design (Sec.5.2)Memory Address Mapping Mechanism(Sec.5.3.1)CapsNetDRAMDiesLogic LayerVault...Cross Bar (Switch)PartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionPartitionPartitionPartitionSub-Memory Controller...PartitionTSVTable 3: Parameters for Modeling Inter-Vault Data
Movement

Symbol Description

I

NL

NB

DR iteration number
Scale for B-dimension
i.e. batch size
Scale for L-dimension
i.e. number of low-level capsules
Scale for H-dimension
i.e. number of high-level capsules
Number of Vault
Dimension of low-level capsule
i.e. number of scaler per low-level capsule
Dimension of high-level capsule
i.e. number of scaler per high-level capsule
SI ZEx Data size of a variable or packet head&tail

NH
Nvault

CH

CL

be performed for Eq.(4) to combine the bk
ij from the
snippets assigned to a speciﬁc vault before performing
global inter-vault aggregation.
Guiding Distribution via an Execution Score: To
achieve the optimal power/thermal and performance re-
sults, we propose a metric called execution score S to
guide workload distribution, which quantitatively esti-
mates the RP execution eﬃciency under a given work-
load distribution. S considers workload assignment to
each vault, inter-vault communication overheads, device-
dependent factors and inter-vault memory bandwidth.
S is modeled as S = 1/(αE + βM ).

where E represents the largest workloads distributed
to a single vault (since even distribution across vaults is
typically not feasible), which can be quantiﬁed based on
the amount of allocated operations. M represents the
amount of inter-vault data movement. Both E and M
are aﬀected by the distribution strategy and the model
conﬁguration. Finally, α and β represent the device-
dependent coeﬃcients, determined by HMC frequency
and inter-vault memory bandwidth, respectively.

The calculation of S is independent of the online pa-
rameters, thus, the distribution strategy can be deter-
mined oﬀ-line before the actual inference. Then, the in-
vault operations according to this selected distribution
dimension will be generated by compiler and the corre-
sponding workloads will be assigned into each vault via
a hardware scheduler at runtime. We now demonstrate
how to model S via estimating E and M on the three
distribution dimensions.
Distribution on B-dimension: As Fig.10 illustrates,
the largest workload assigned to a single vault (E) con-
sist of the workload snippets including 1 2 3 4 and the
partial operations 5 6 . With our optimizations, the
single vault can get at most (cid:100)log2(Nvault)(cid:101)
of the unpar-
allelizable operations, where Nvault represents number
of the HMC vaults. Using parameters shown in Table
3, E can be modeled as follows:

Nvault

EB = (cid:100) NB
Nvault
NH × CH × (2NL − 1) + (cid:100) NB
Nvault

(cid:101) × NL × NH × CH × (2CL − 1) + I × [(cid:100) NB
Nvault

(cid:101) × NH × (3CH + 19)+

(cid:101)×

(cid:100) NB
Nvault

(cid:101) × NL × NH × (2CH − 1) + (cid:100)log2(Nvault)(cid:101)

+ 4 × CH ]

Nvault

EB = (cid:100) NB
Nvault

(6)
Since NL (cid:29) 1, the above equation can be simpliﬁed as:
(7)
The inter-vault data communication consists of send-
ing pre-aggregated bij from all the vaults to a single
vault and scattering cij across all the vaults. The data
transmission is in the form of packets with the head

(cid:101) × NL × NH × [(4I − 1)CH + 2CLCH − I]

Figure 10: The execution diagram for the RP proce-
dure with B-dimension distribution. The workloads in
green blocks can be split across vaults, but workloads
in purple blocks cannot be distributed via B-dimension.

and tail size represented as SIZEpkt. Therefore, the
amount of data movements M can be represented as:

MB = I × [(Nvault − 1) × NL × NH × (SIZEbij + SIZEpkt)

+(Nvault − 1) × NL × NH × (SIZEcij + SIZEpkt)]

(8)

Distribution on L-dimension: As Table 2 illustrates,
the RP operations of Eq.1,4,5 can be divided into work-
load snippets on the L-dimension. Besides, partial oper-
ations from Eq.2 (i.e., ˆuk
j|i × cij) also exhibit parallelism
on the L-dimension. Thus, E can be represented as:

EL = NB × (cid:100) NL
Nvault

(cid:101) × NH × [2I(2CH − 1) + CH (2CL − 1)]

(9)

The inter-vault communication contains data all-reducing

for of sj and broadcasting vk
j :

ML = I × [NB × (Nvalut − 1) × NH × (SIZEsk

j

+ SIZEpkt)

+NB × (Nvault − 1) × NH × (SIZEvk

j

+ SIZEpkt)]

(10)

Distribution on H-dimension: As Table 2 presents,
only Eq.5 cannot be parallelized on this dimension. Hence,
E can be represented as

EH = NB × NL × (cid:100) NH
Nvault

(cid:101) × CH × [2CL − 1 + 2I]

(11)

The inter-vault communication contains data all-reducing

for of bij and broadcasting cij:

MH = I × [(Nvault − 1) × NL × (SIZEbij + SIZEpkt)

+NL × (SIZEcij + SIZEpkt)]

(12)

5.2

Intra-Vault Level Design

In this section, we propose the intra-vault level de-
sign that eﬀectively processes the sub-operations of each
equation that are allocated to a vault. We target the de-
sign for IEEE-754 single precision (FP32) format, which
provides suﬃcient precision range for CapsNet work-
loads [14]. Our design can also ﬁt other data formats
with minor modiﬁcations.

5.2.1

Intra-Vault Level Workload Distribution
In a basic HMC design, the logical layer of each vault
contains a sub-memory controller to handle the memory
access. In order to conduct RP speciﬁc computation, we
introduce 16 processing elements (PEs) into each vault.
This design overhead has been tested to satisfy both
area and thermal constraints for HMC [43] (see detailed
overhead analysis in Sec.6.5). These PEs (integrated
onto the logic layer) are connected to the sub-memory
controller in the vault, shown in Fig.11(left). Note that
the number of parallel sub-operations on certain dimen-
sion is generally the orders of magnitude higher than the
number of vaults in HMC. In other words, many par-
allel sub-operations will be allocated to the same vault.
Hence they can be easily distributed on the same dimen-
sion and concurrently processed via the PEs without in-
troducing additional communication overheads. There

7

u00~ukiw0j~wij×û00j~ûkijc0j~cijsj×vjû00j~ûkij×u00~ukiw0j~wij×û00j~ûkijc0j~cijsj×vjû00j~ûkijb0j~bij×û0i0~ûkijbi0~bij×û0i0~ûkijbi0~bij×u0i~ukiwi0~wij×û0i0~ûkijci0~cij×si0~siju0i~ukiwi0~wij×û0i0~ûkijci0~cij×si0~sijuiwijcijsijvijbij× Equation 1Equation 3Equation 4Equation 5Equation 5Equation 4, part 2Equation 2Equation 2...Equation 2...Equation 3...Equation 4, Part 1Host ProcessorHybrid Memory CubeEquation 3Equation 2, part 2...Equation 4...Equation 5...Equation 2, Part 1Host ProcessorHybrid Memory CubeEquation 1123456234516Equation 1Equation 5...Equation 2...Equation 3...Host ProcessorHybrid Memory Cube23451Equation 4uis0si...v0vi...× ............× × ............× +u0~uiw00~wij×û00~ûijc00~cijs0~sj×v0~vjû00~ûijb00~bij×u0~uiw00~wij×û00~ûijc00~cijs0~sj×v0~vjû00~ûijb00~bij×uk0~ukiw00~wij×ûk00~ûkijc00~cijsk0~skj×vk0~vkjûk00~ûkijbk00~bkij×Different Batchesb00~bijs0~sjv0~vju0i~ukiwi0~wij×û0i0~ûkijci0~cij×si0~sij+û0i0~ûkijbi0~bij×+Different P-capssoftmaxu00~ukiw0j~wij×û00j~ûkijc0j~cijsj×vjû00j~ûkijb0j~bij×softmaxDifferent D-capssoftmax12345612356123454A with its signiﬁcand bits logically shifting right. By
doing this, the faction representation can be described
as D in Fig.12(b). Given the matched real exponent
value (i.e., ep − b), the two representations, i.e., A and
D in Fig.12(b), now share the identical bit shifting op-
erations. Additionally, A and D correspond to Ex-
pResult’s (i.e.,exponential function’s result) integer and
fraction, respectively. There is no overlapping between
their fraction bits. Thus, these two representations can
be combined (i.e., A OR D ) followed by a uniﬁed bit
shifting operation on the signiﬁcand. Note that the ex-
ponent matching procedure ( B → D ) could over chuck
several least signiﬁcant bits which would originally be
mapped into the ExpResult.

Since the exponent matching and combination of two
FP32 numbers can be simply considered as a FP32 ad-
dition, we can treat the ExpResult computation as an
addition of the exponent and fraction representations
(i.e., (cid:98)y(cid:99) + b + 2y−(cid:98)y(cid:99) − 1) followed by the bit shifting
operations. Note that the power of 2 function causes
high complexity in both execution and logic design, we
propose to simplify it as follows:

The above polynomial can be expanded as (y+2y−(cid:98)y(cid:99)−
(y−(cid:98)y(cid:99))+b−1); then, the average value Avg of (2y−(cid:98)y(cid:99)−
(y − (cid:98)y(cid:99))) can be achieved via integrating the polyno-
mial over (y − (cid:98)y(cid:99)) ∈ [0, 1), which is ﬁxed and can be
obtained oﬄine. With y represented by x, the exponen-
tial function can be described as follows:

ExpResult (cid:39) BS(log2(e) × x + Avg + b − 1)

(14)
where the BS is bit shifting operations with information
from ep − b; and log2(e) is a constant that is computed
oﬄine.
Accuracy Recovery: Under the worst case scenari,
othere might be several lowest signiﬁcand bits chucked
when mapping from D to C . It may cause some accu-
racy loss. To minimize the bit chucking impact, we an-
alyze 10,000 exponential executions to collect the value
diﬀerences between the approximated and original re-
sults. During the approximation execution, the accu-
racy loss will be recovered via enlarging the results by
the mean percentage of the value diﬀerence. Note that
our accuracy recovery scheme only introduces one ad-
ditional multiplication operation during the inference,
which guarantees the high performance and the low de-
sign complexity compared to other exponential approx-
imation methodology, e.g., applying look-up tables [59].
Sec.6.5 shows the detailed accuracy analysis.
Final PE Structure: According to the discussion
above, the special functions can be simpliﬁed as a com-
bination of addition, multiplication, and bit shifting op-
erations. Thus, our intra-vault PE employs adders, mul-
tipliers, and bit-shifters to construct these special func-
tions, as shown in Fig.11. Speciﬁcally, our PE enables
the ﬂow conﬁguration via the multiplexer (MUX) to
support diﬀerent types of operations. For example, PE
execution ﬂow 1 2 is for MAC operations; 3 2 1 2 1
is for inverse square-root operations; and 1 2 2 3 is
for exponential function.
5.3 Contention Reduction in CapsNet Design
In this section, we discuss strategies to combat the

Figure 11: Intra-vault level Architecture Design.

may exist some extreme cases that the number of paral-
lel sub-operations allocated to the vault is smaller than
the number of PEs, leading to low PE utilization. Since
most equations in RP can be parallelized on more than
one dimension, the workloads can then be distributed
along a diﬀerent dimension which can produce enough
parallelism to fully utilize the PE resources.
5.2.2 Customized PE Design

There have been some studies [43, 42, 57] that inte-
grate adders and multipliers onto the HMC logic layer
to perform multiply-accumulation (MAC) which is an
essential operation for deep learning (e.g., CNN). How-
ever, CapsNet’s RP execution involves other operations
beyond MAC. As discussed in Sec.2.2, among the ﬁve
RP equations, Eq.1, Eq.2 and Eq.4 can be translated
into MAC operations. But the other operations includ-
ing Eq.3 and Eq.5 involve more complex functions such
as division (Eq.3), inverse square-root (Eq.3) and expo-
nential functions (Eq.5) that require complicated logic
design, resulting in large performance and power/thermal
overheads [58, 59].
Operation Approximation: To gain high performance
while maintaining low hardware design complexity, we
propose approximation to simplify these operations with
negligible accuracy loss. For simplifying division and in-
verse square-root functions of the FP32, we apply bit
shifting [60], which is widely adopted in graphics pro-
cessing domain [61, 62, 63]. For exponential function,
we approximate the operation as follows:

The original exponential function can be transformed

in the form of power function with the base as 2 [64]:
ex = 2log2(e)×x = 2y = 2(cid:98)y(cid:99)(1 + 2y−(cid:98)y(cid:99) − 1)

(13)
where (cid:98)y(cid:99) is the integer part of y, and y − (cid:98)y(cid:99) is the
decimal part.

Fig.12(a) illustrates an example of representation trans-

fer. First, the ep of both A and B will be subtracted
from the bias b to get their real exponents (i.e., ep − b),
as shown in Fig.12(a) 1 & 3 . Then, the most signiﬁcant
ep − b + 1 bits of A ’s signiﬁcand (i.e., 1 + f raction) will
be chunked and ﬁlled into the least signiﬁcant ep − b + 1
bits of C ’s exponent ﬁeld, with the remaining exponent
bits in C ﬁlled by zeros, as shown in Fig.12(a) 2 . We
conduct s similar operation to transfer B to the C ’s
fraction. Since B is a fraction value, its exponent is
a non-positive number. B ’s signiﬁcand will be logical
shift right by |ep − b| bits and then its most signiﬁcant
23 bits are ﬁlled into C ’s fraction ﬁeld, as shown in
Fig.12(a) 4 .

The above two transfers can be considered as bit shift-
ing on the signiﬁcand (i.e., 1+f raction) in FP32 format
with the distance and direction determined by the real
exponent (i.e., ep − b). As illustrated in Fig.12, the
B ’s exponent can increase to match the exponent of

8

VaultSub-Memory ContollerI/OMUX× +∫ E-CtrlMUX× +∫ VaultRouter× +∫ MUXRegRegProcessing ElementControl SignalH InputV InputOutputP-P RouterMUXRegAddAddAddSFMACMACMACMACMACMACMACMACMACMACMACMACMACMACMACMACAddAddAddAddMulMulMulMulMulMulMulMul(b)(c)(a)     Sub-Memory                    ContollerPEPEPE...Instruction BufferData BufferMux× +>>MuxMux+>>PCProcessing ElementWriteBuffer123VaultPEPEPE...Instruction BufferData BufferMux× MuxMux+<<Program CounterProcessing Element123DRAMBankDRAMBankDRAMBank...     Sub-Memory                    ContollerWriteBufferVaultPEPEPE...Data BufferMux× MuxMux+<<OP ControllerProcessing Element123DRAMBankDRAMBankDRAMBank...Sub-Memory Contoller16 PEsFigure 12: (a) An example of transferring the exponent representation A (i.e., (cid:98)y(cid:99)+b) and fraction representation B
(i.e., 2y−(cid:98)y(cid:99) − 1) to the exponential function’s result C in FP32 format. (b) Combining the exponent representation
A and the fraction representation (i.e., D that transferred from B ), and applying a uniﬁed bit shifting to obtain
the exponential function’s result C .

Figure 13: (a)The default address mapping of 8GB in
HMC Gen3; (b) Our address mapping.

Figure 14: The Evaluation Infrastructure.

memory-level contention in our PIM-CapsNet design.
5.3.1 Memory Address Mapping Mechanism

In the default HMC design, memory access granular-
ity is 16 bytes which is deﬁned as a block, and the se-
quential interleaving mapping mechanism is applied to
beneﬁt from better bandwidth utilization. Moreover,
MAX block is introduced to deﬁne the maximum num-
ber of blocks that one bank can provide at a time. Its
size can be set to 32B, 64B, 128B, or 256B [38].
In
this study, we redeﬁne MAX block as a subpage in or-
der to diﬀerentiate it from block, and one subpage is
composed of multiple blocks. Fig.13(a) illustrates the
default address mapping from HMC Gen3 speciﬁcations
[38]. The lowest 4 bits and the highest 1 bit are ignored,
and the remaining bits describe the block address. From
its lower to higher bits, a block address is composed of
several ﬁelds: the block ID in the sub-page (the num-
ber of bits is determined by the sub-page size), the 5-bit
vault ID for 32 vaults, the 4-bit Bank ID for 16 banks
per vault, and the sub-page ID in the bank. As can be
seen, sub-pages with consecutive addresses will be ﬁrst
spread sequentially to diﬀerent vaults and then diﬀerent
DRAM banks.

Note that our inter-vault level design requires consec-
utive blocks allocated into one vault to avoid high inter-
vault communication. This can be easily addressed by
moving up the vault ID ﬁeld to the highest ﬁeld of
the block address (as shown in Fig.13(b)) so that vault
ID remains unchanged during the intra-vault memory
address mapping. However, at the intra-vault level,
PEs process their workloads in parallel and concurrently
generate data requests, which may result in serious bank
conﬂicts and vault request stalls (VRS).

Interestingly, we observe that most of the concurrent
PE requests assigned to the same bank actually visit dif-
ferent blocks. Based on this, we propose a new memory
addressing scheme to distribute these blocks to diﬀer-
ent banks, in order to signiﬁcantly alleviate bank con-
ﬂicts and decrease the VRS. However, simply distribut-
ing blocks to diﬀerent banks could further increase the
VRS as one PE may request multiple consecutive blocks
at a time. Because in this case these blocks will reside
in multiple banks, it leads to multiple accesses to these
banks, resulting in higher bank conﬂicts. To ensure the

consecutive blocks required by one PE are stored in the
same bank, our scheme will dynamically determine the
sub-page size according to the size of the requested data.
As shown in Fig.13(b), we leverage bit 1 ∼ bit 3 in the
lowest 4 ignored bits as the indicator to determine the
sub-page size for the data requests, where range “000”
∼ “100” represents the sub-page size from 16B ∼ 256B.
Given that the data requests from PEs and host GPU
need to be allocated into diﬀerent banks, the indicator
bits are dynamically assigned by the page table during
the virtual-physical address translation according to the
storage requested by each variable involved in each ex-
ecution thread.

5.3.2

Identifying Memory Access Priority

During CapsNet execution, resource contention from
concurrent data requesting to the same vault from both
host GPU and PEs could occur. Although the Conv/FC
layers exhibit much better data locality than the RP,
they may still need to periodically request data from the
HMC. By default, the priority of a request is determined
by the arrival time. But this can cause stalls if both
sides are requesting to access the same bank in a vault,
which may occur more frequently after applying our new
address mapping.

To address this issue, we propose a runtime memory
access scheduler (RMAS) to dynamically identify the
priority of memory requests from both the host GPU
and vault PEs. We ﬁrst observe that, with our address
mapping mechanism, consecutive data are likely to be
distributed across the banks within a vault instead of
being scattered over all the vaults. Thus, it is highly
likely that each consecutive data request from the host
will only access a single or few vaults at a time instead
of all the vaults. This provides opportunities for some
vaults to serve the GPU memory requests in rotation
without aﬀecting the overall HMC execution.

To quantify the impact of vault access priority, the
runtime scheduler (RMAS) ﬁrst collects the information
of the issued operations by HMC and the number of PE
requests (Q) from the vault request queues in HMC.
It also collects the information of the issued operations
from the host GPU about which and how many of vaults
the operations are requesting from the HMC (deﬁned
as nmax). The collected information above from RMAS

9

1111100010000100101000100000000000000Shifting 23 Bits2^a0*(1+2^-a1 + 2^-a2 + 2^-a3)2^a0+2^(a0-a1) + 2^(a0-a2) + 2^(a0-a3)2^b0*(1+2^-b1 + 2^-b2 + 2^-b3)2^b0+2^(b0-b1) + 2^(b0-b2) + 2^(b0-b3)Y_integer + 127 2^(Y_frac)-123 Bit Shift23 Bit Shift01000011000000100000000000000000001111100010001000000000000000107- 1271-6+01000001010001000100000000000000+-127++134+7+7-6+1+-127++-2-6-3-5-90.15820315+-22-25130+124<<<<Shifting 23 Bits<<<<-3<<<<AbandonBit Shifting010000110000001000000000000000000011111000100010000000000000001001000001000101000100000000000000+-127++134+7+7-6+1+-127++-2-6-3-9+-25+124<<<<                                     Bit Shifting<<<<-3<<<<Ignore-5-22ACBABC123456++<<<<0ExponentMantissa+× offset0ExponentMantissaInputOutput0ExponentMantissaExponent+MantissabiasExponent represnetationMantissa represntationTarget FP32 Number010000100000010000000000000000001 ++132-127Significand<<+5001111100010001000000000000000101 ++124-127|-3|ChunkedACBExponent1 +1324010000100000010000000000000000001 +SignificandAExponent0000001010001000000000000+D100001000000011010001000000000001 +E1000010000010000100101000100000000000000C1 ++132-127<<+5>>+(a)(b)Matching ExponentNon-overlapping fraction bits0001000010010100010000000000000001000010000001000000000000000000001111100010001000000000000000101+1+1+-127-127>><<ExponentSignificand+132+5+124|-3|1234ACB00000000101000100000000010000100010000100000010000000000000000001+ADNon-overlapping fraction bits000100001001010001000000000000001+CExponentSignificandChunked-127<<Identical Exponent+132+5(a)(b)0+0001000010010100010000000001111101000010000001000000000000000000001111100010001000000000111110101+1+1+-127-127>><<ExponentSignificand+132+5+124|-3|1234ACB00000000101000100000000010000100010000100000010000000000000000001+ADNon-overlapping fraction bits000100001001010001000000000000001+CExponentSignificandChunked-127<<Identical Exponent+132+5(a)(b)Over-Chunked111110+0001000010010100010000000001111101000010000001000000000000000000001111100010001000000000111110101+1+1+-127-127>><<ExponentSignificand+132+5+124|-3|1234ACB00000000101000100000000010000100010000100000010000000000000000001+ADNon-overlapping fraction bits000100001001010001000000000111101+CExponentSignificandChucked-127<<Identical Exponent+132+5(a)(b)Over-ChuckedMean Difference %… … 0334Sub-page IDBank IDVault ID… … 0334Sub-page IDBank IDVault IDBlock IDIndicator(a)(b)Block IDBlock AddressNVprofilerNvidia-smiHMC-simGate-Level SimulationPerformancePowerGPUPytorchConv/PrimeCaps/FC LayersHMC-simOutputGate-Level SimulationPytorchGPUCapsNetPerformancePowerPytorchNVprofileNvidia-smiGate-Level SimulationHMC-simGPUEvent Trace For RPGPUExecution StatusExecution StatusOutputPowerPerformancePerformance+is associated with the HMC performance overhead if
granting priority to access from either side. Thus, the
performance impact of serving the two types of access
requests from HMC and GPU can be quantiﬁed via the
following overhead functions:

κ = γv × nh × Q + γh × nmax
(15)
nh
where κ represents the quantiﬁed performance impact;
γv and γh represent the impact factors that are deter-
mined by the issued operations’ type from HMC and
host GPU, e.g., memory intensive operations corresponds
to a large γ as their performance is more sensitive to the
memory bandwidth than the computation intensive op-
erations; Q is the average number of PEs’ requests in
the request queues from the targeted vaults; nh is the
number of vaults that are granted with access priority
from the host GPU, which is in the range of [0, nmax]. If
nh is ”0”, all the target vaults will ﬁrst process current
HMC PEs requests before processing the GPU requests;
while if nh is nmax, all the target vaults will grant prior-
ity to the GPU requests. To achieve the minimal impact
(cid:12)
(cid:12)
(i.e. min(κ)), nh should be equal to
(cid:12), where
nh ∈ [0, nmax]. The RMAS will then send the control
signals to nh vaults that will give host GPU higher pri-
ority to access. Note that a vault with smaller Q has
higher priority to be chosen by our RMAS to further
reduce the impact on execution.

(cid:113) nmax×γh
Q×γv

(cid:12)
(cid:12)
(cid:12)

6. EVALUATIONS
6.1 Experimental Setup

Fig.14 shows our evaluation infrastructure to eval-
uate PIM-CapsNet. We ﬁrst employ Pytorch [51], a
popular open-source machine learning framework that
supports dynamic computation graphs, to execute the
CapsNet on our host GPU. We then design a physical-
simulator cooperated platform which takes the execu-
tion status of CapsNet provided by Pytorch to obtain
the detailed performance and energy results when ap-
plying PIM-CapsNet. From the physical side, we adopt
the Nvidia Tesla P100 [65] as our host processor to
evaluate performance and energy consumption of the
Conv/PrimeCaps/FC layers of CapsNet. The detailed
execution time and power information for these layers
are captured by using NVproﬁler [54] and Nvidia-smi
[66]. From the simulator side, we leverage NVproﬁle to
collect the event trace from host and pass it to a mod-
iﬁed HMC-sim 3.0 [67] to simulate the computing and
memory accesses in HMC. Considering that HMC-sim
3.0 cannot provide precise execution cycles and power
information for the logic layer design, we conduct a
gate-level simulation on Cadence [68] to measure the
execution latency and power consumption for our logic
design (PE). We then integrate the gate level results and
our PIM design in HMC-sim to obtain the performance
and energy consumption of RP execution. Finally, since
the execution of CapsNet is pipelined on the host pro-
cessor and HMC, we combine the results from both sides
via overlapping the execution time and accumulating
the energy consumption. The detailed platform conﬁg-
urations are shown in Table 4. In this work, we choose
12 diﬀerent types of CapsNets as our benchmarks which
are introduced in Sec.3 and shown in Table 1.

10

Table 4: Platform Speciﬁcations
Host Processor (GPU)

Shading Unit

On-chip Storage

Default Memory

3584 @ 1190MHz
L1Cache/Shared: 24KB x 56
L2 Cache: 4MB
HBM, 8GB, 320GB/s

HMC

Capacity
Bandwidth
No. of PEs per Vault
Frequency

8 GB, 32 Vault, 16 Banks/Vault
Extl:320 GB/s, Intl: 512GB/s
16
312.5MHz

(a)

(b)
Figure 15: The (a) speedup and (b) normalized en-
ergy consumption of PIM-CapsNet on the RP proce-
dure comparing with the GPU-based design.

To evaluate the eﬀectiveness of PIM-CasNet, we com-
pare with the following design scenarios: (1) Baseline:
the state-of-the-art GPU accelerated CapsNet execu-
tion with the HBM memory (320GB/s).
(2) GPU-
ICP: the GPU accelerated CapsNet execution with ideal
cache replacement policy. (3) PIM-CapsNet: our com-
bined inter-vault level and intra-vault level design for
RP acceleration with the new memory addressing scheme
and RMAS scheme (4) PIM-Intra: our PIM-CapsNet
without inter-vault level design, and the memory ad-
dressing scheme does not optimize the inter-vault data
distribution.
(5) PIM-Inter: our PIM-CapsNet with-
out intra-vault level design, and the memory addressing
scheme does not support the intra-vault bank conﬂict
(6) RMAS-PIM and (7) RMAS-GPU:
optimization.
Our PIM-CapsNet with the naive memory access schedul-
ing, which always grants HMC PEs higher priority than
GPU for RMAS-PIM, and always grants GPU higher
priority than HMC PEs for RMAS-GPU. (8) All-in-
PIM: the HMC accelerated CapsNet execution, includ-
ing RP and other layers’ execution.
6.2 Effectiveness of PIM-CapsNet

6.2.1 Performance and Energy for RP execution

We ﬁrst evaluate the eﬀectiveness of PIM-CapsNet
on RP execution. Fig.15 illustrates the normalized per-
formance and energy consumption of our PIM-CapsNet
design compared with the GPU-based design (i.e., The
Baseline and GPU-ICP) for the RP execution. From
the ﬁgure, we ﬁrst observe that the GPU-ICP only out-
performs Baseline by 1.14% on performance and 0.77%
on energy during RP execution. This is because RP
requires a large number of intermediate variables which
exceed the on-chip storage. As a result, the cache pol-
icy improvements can barely reduce the oﬀ-chip mem-
ory accesses. Second, PIM-CapsNet outperforms Base-
line by 117% on performance by addressing the large
number of memory access as well as the intensive syn-
chronizations. Third, from Fig.15(b), we observe PIM-
CapsNet saves 92.18% on energy comsumpting com-
paring to Baseline. This is because the entire work-
ing power of our PIM design is much lower then host

0x1x2x3xNormalized PerformanceBaselineGPU-ICPPIM-CapsNet0%50%100%Normalized Energy Consumption(a)

(b)
Figure 16: The breakdown of the factor to (a) normal-
ized performance and (b) normalized energy consump-
tion when performing RP execution on diﬀerent PIM
designs.

and PIM-CapsNet is able to reduce a huge number of
data movements between host and HMC. Moreover, we
observe that PIM-CapsNet can achieve better perfor-
mance and energy saving for RP execution in larger
size CapsNet, e.g. 2.27× speedup and 92.52% energy
saving of accelerating Caps-EN3 compared with 2.09×
speedup and 91.90% energy saving of accelerating Caps-
SV1. This implies that PIM-CapsNet exhibits the scal-
ability in optimizing the RP execution with the ever-
increasing network size.
6.2.2 Effectiveness of Intra-Vault and Inter-Vault Level

Designs

To better understanding the eﬀectiveness of our intra-
vault and inter-vault level designs, we compare PIM-
CapsNet with other PIM design scenarios for the RP
execution only. Fig.16(a) illustrates the evaluation re-
sults of normalized performance with the breakdown
factors for diﬀerent PIM designs. From the ﬁgure, we
have several observations. First, even though PIM-
Intra achieves 1.22× speedup over Baseline, the inter-
vault communication overheads contribute on averages
45.24% to the overall performance. This is because
the PIM-intra design induces massive concurrent data
transfer between the centralized computation units in
the logic layer and the DRAM banks in vaults, leading
to high crossbar utilization and serious stalls. Second,
PIM-Inter decreases the performance by 4.73% com-
pared with the Baseline. Compared with PIM-Intra,
the inter-vault communication overheads have been sig-
niﬁcantly reduced in PIM-Inter, but the vault request
stalls (VRS) grow which contribute on average 57.91%
to the execution time due to the serious bank conﬂicts
within the vault. Finally, PIM-CapsNet improves the
performance about 127.83%/76.62% on average com-
paring to PIM-Inter/PIM-Intra by reducing both inter-
vault communications and VRS. From the energy per-
spective, as Fig.16(b) shows, these PIM designs achieve
high energy saving compared with Baseline by execut-
ing RP on energy-eﬃcient PIM design and our PIM-
CapsNet on average outperforms the PIM-Inter/PIM-
Intra by 4.81%/4.52% respectively on energy saving.
6.3 Overall Performance and Energy

Fig.17 shows the normalized speedup and energy of
entire CapsNet execution. First, to evaluate the eﬀec-
tiveness of the pipelined execution between the host and
the HMC, we compare our design with all in GPUs (i.e.,

11

Figure 17: The (a) speed up and (b) normalized energy
consumption when processing entire CapsNet with dif-
ferent designs.

Figure 18: The speedup (heat map) achieved by diﬀer-
ent workloads distribution dimensions (X-axis) under
diﬀerent HMC execution frequency. Red means better
improvement.

Baseline), All-in-PIM. From the ﬁgure, we observe that
the All-in-PIM causes the 47.59% performance drop
compared to the Baseline. This is because we mainly
focuses on the RP procedure optimization at minimal
cost in HMC, and this low-cost design choice can hardly
achieve the best performance for Conv/FC layers. But
it reduces the energy consumption by 71.09%, which ex-
hibits better execution eﬃciency (i.e. performance /en-
ergy consumption) compared with Baseline. Fig.17 Fur-
ther plots the performance and energy consumption of
the pipelined design with native memory access sched-
ulers (i.e., RMAS-PIM and RMAS-GPU). We observe
that these naive schedulers can not achieve the best
performance and energy saving compared with PIM-
CapsNet due to the memory access stalls. In summary,
our design outperforms Baseline on both performance
(i.e., on average 2.44×) and energy saving (i.e., 64.91%),
and it even achieves higher performance improvement
compared with the acceleration for RP execution only,
which beneﬁts from the pipelined execution.
6.4 Sensitivity to PE Frequency Scaling

We conduct the sensitivity analysis on inter-vault level
workload distributions in our PIM-CapsNet when dif-
ferent frequency is adopted in the PEs, e.g., 312.5MHz,
625MHz and 937.5MHz. Note the frequency scaling will
be controlled under a certain range without violating
the HMC thermal constraint. Fig.18 shows the speedup
achieved by selecting diﬀerent distribution dimensions
(i.e., B-dimension, L-dimension, and H-dimension) un-
der the above three diﬀerent frequencies. The darker
color indicates the higher speedups (e.g., the red color
indicates the substantial speedups while the yellow color
means trivial speedups).

It is obvious that PIM-CapsNet can achieve better
improvement with higher execution frequency. We also
notice that the selection of the distribution dimension
changes with frequency scaling. For example, for Caps-
SV3, the L-dimension distribution can achieve the best
performance under 312.5MHz execution frequency; but
the H-dimension distribution achieves the best perfor-
mance when frequency increases to 937.5MHz. This
indicates that the dimension selection is aﬀected by not
only the network conﬁgurations, but also the hardware
conﬁgurations, e.g. processing frequency.

00.40.81.21.6Normalized Execution TimeExecutionX-barVRSMN1MN2MN3CF1CF2CF3EN1EN2EN3AverageSV1SV2SV3Caps-MN1Caps-MN2Caps-MN3Caps-CF1Caps-CF2Caps-CF3Caps-EN1Caps-EN2Caps-EN3AverageCaps-SV1Caps-SV2Caps-SV30%5%10%15%PIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraPIM-InterPIM-CapsNetPIM-IntraCAMP-IntraPIM-CapsNetNormalized Energy ConsumptionExecutionDRAMXBARVaultMN1MN2MN3CF1CF2CF3EN1EN2EN3AverageSV1SV2SV3Caps-MN1Caps-MN2Caps-MN3Caps-CF1Caps-CF2Caps-CF3Caps-EN1Caps-EN2Caps-EN3AverageCaps-SV1Caps-SV2Caps-SV30x1x2x3xCaps-MN1Caps-MN2Caps-MN3Caps-CF1Caps-CF2Caps-CF3Caps-EN1Caps-EN2Caps-EN3Caps-SV1Caps-SV2Caps-SV3AverageNormalized Entire SpeedupBaselineAll-in-PIMRMAS-PIMRMAS-GPUPIM-CapsNet00.40.81.2Caps-MN1Caps-MN2Caps-MN3Caps-CF1Caps-CF2Caps-CF3Caps-EN1Caps-EN2Caps-EN3Caps-SV1Caps-SV2Caps-SV3AverageNormalized Entire EnergyBaselineAll-in-PIMRMAS-PIMRMAS-GPUPIM-CapsNet(a)(b)312.5 MHzBPDCaps-SV3Caps-SV2Caps-SV1Caps-EN3Caps-EN2Caps-EN1Caps-CF3Caps-CF2Caps-CF1Caps-MN3Caps-MN2Caps-MN1625 MHzBPD937.5 MHzBPDspeedup2468Table 5: Accuracy Validations for PIM-CapsNet
Caps-MN1 Caps-MN2 Caps-MN3 Caps-CF1 Caps-CF2 Caps-CF3

Origin
w/o Accuracy
Recovery
w/ Accuracy
Recovery

Origin
w/o Accuracy
Recovery
Recovery

99.75%

99.73%

99.75%

99.74%

99.75%

99.73%

89.40%

89.15%

90.03%

89.91%

90.43%

90.08%

99.75%

99.75%

99.75%

89.37%

90.02%

90.39%

Caps-EN1 Caps-EN2 Caps-EN3 Caps-SV1 Caps-SV2 Caps-SV3

88.74%

88.19%

88.69%

85.01%

84.16%

84.96%

82.36%

81.64%

82.34%

96.70%

95.08%

96.42%

95.90%

95.92%

95.90%

95.90%

95.92%

95.90%

6.5 Overhead Analysis
Accuracy Analysis: Table 5 shows the absolute Cap-
sNet accuracy after applying the approximations and
accuracy recovery methodologies on our PIM-CapsNet
designs as discussed in Sec.5.2.2. As it shows, the ap-
proximations cause on average 0.35% accuracy loss while
the accuracy recovery method can achieve no accuracy
loss for most of the cases, and only induces on average
0.04% accuracy diﬀerence across our benchmarks.

We also observe the slight accuracy boost for Caps-
SV2 and Caps-SV3 when we increase the RP iteration
numbers for PIM-CapsNet without accuracy recovery.
This is because the noise induced by the approximation
enhances the robustness of feature mapping given the
enough RP iterations[69].
Area Analysis: Our PIM-CapsNet introduces 16 PEs
and operation controller into each HMC vault architec-
ture, with one RMAS module located in the logic layer.
Based on our gate-level simulations, our logic design for
32 vaults and the RMAS together incurs 3.11mm2 area
overheads under the 24nm process technology, which
only occupy 0.32% HMC logic surface area.
Thermal Analysis: Note that our logic design raises
the total power of the HMC, which could cause ther-
mal issues for the 3D stacked memory architecture. We
observe the average power overhead of our logic design
is 2.24W , which is below the maximum power overhead
(i.e., 10W thermal design power (TDP) [70]) the HMC
can tolerate.
7. RELATED WORKS
PIM-Based NN Acceleration: There have been mul-
tiple studies focus on exploring PIM-based neural net-
work accelerators [71, 72, 73, 74, 75, 76]. For example,
[72] employs the ReRAM-based PIM to conduct eﬃ-
cient neural network execution. However, the massive
intermediate variables involved in the RP could induce
both performance and energy overheads in the ReRAM
design for frequent value updating. Besides, [43, 77,
42] propose the CNN accelerator design using 3D stack
PIM technique. Since the execution pattern of the RP
procedure are far diﬀerent from CNN layers, previous
logic layer designs of 3D stacked memory exhibit low
eﬃciency on the RP execution. To our best knowledge,
this is the ﬁrst work that leverages HMC to explore
a customized architectural design for eﬃcient CapsNet
acceleration.
Workload Distributions: Several studies have ex-
plored the workload distribution for eﬃcient neural net-
work execution [78, 79, 80, 81, 82]. For example, [80]
proposes to distribute the parallel workloads of convo-
lutional layer among the computation units within a
single device; and [82] splits the convolutional execu-
tion and distribute the workloads into multiple devices.
Their methods have achieved signiﬁcant CNN acceler-

ations via greatly improving the resource utilization.
Since the execution of the RP procedure is much more
complicated than the convolutional layer and involves
strong execution dependence, these studies are not ap-
plicable to the CapsNet acceleration.
Exponential Approximation: There are also some
works on approximation for exponential function [58,
59, 83, 84]. For example, [59] leverages the lookup table
to conduct the fast exponential execution, which causes
substantial performance and area overheads. [84] accel-
erates exponential function via software optimizations,
which are hard to be implemented via the simple logic
design. In this work, we conduct the eﬃcient accelera-
tion for exponential function (Sec.5.2.2) with low design
complexity and low power overhead.
8. CONCLUSIONS

In recent years, the CapsNet has outperformed the
CNN on the image processing tasks and becomes in-
creasing popular in many areas. In this study, we pro-
pose a processing-in-memory based hybrid computing
design called PIM-CapsNet, which leverages both GPU
core computing capability and the special features of
the hybrid memory cube to eﬀectively process the data-
intensive RP operations, achieving substantial improve-
ments on both performance and energy saving. To dras-
tically reduce the identiﬁed performance bottlenecks of
RP, we propose several memory-level optimizations (e,g.,
multi-dimensional parallelism selection, intelligent work-
load distribution, complex logic approximation and new
address mapping mechanism) based on RP’s program
and execution features to enable minimal in-memory
communication, maximum parallelization, and low de-
sign complexity and overhead. Evaluation results demon-
strate that our proposed design achieves signiﬁcant per-
formance speedup and energy savings over the baseline
GPU design, for both the RP phase and overall CapsNet
inference. It also achieves good performance scalability
with increasing network size.

9. REFERENCES
[1] I. Kononenko, “Machine learning for medical diagnosis:
history, state of the art and perspective,” Artiﬁcial
Intelligence in medicine, vol. 23, no. 1, pp. 89–109, 2001.

[2] B. J. Erickson, P. Korﬁatis, Z. Akkus, and T. L. Kline,
“Machine learning for medical imaging,” Radiographics,
vol. 37, no. 2, pp. 505–515, 2017.

[3] A. L. Buczak and E. Guven, “A survey of data mining and
machine learning methods for cyber security intrusion
detection,” IEEE Communications Surveys & Tutorials,
vol. 18, no. 2, pp. 1153–1176, 2016.

[4] J. Grimmer, “We are all social scientists now: How big data,
machine learning, and causal inference work together,” PS:
Political Science & Politics, vol. 48, no. 1, pp. 80–83, 2015.

[5] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement

learning in robotics: A survey,” The International Journal
of Robotics Research, vol. 32, no. 11, pp. 1238–1274, 2013.

[6] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn:

Towards real-time object detection with region proposal
networks,” in Advances in neural information processing
systems, pp. 91–99, 2015.

[7] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer, “Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and< 0.5 mb model size,” arXiv
preprint arXiv:1602.07360, 2016.

[8] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,

D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich,

12

“Going deeper with convolutions,” in Proceedings of the
IEEE conference on computer vision and pattern
recognition, pp. 1–9, 2015.

[9] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” arXiv preprint
arXiv:1409.1556, 2014.

[10] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual

learning for image recognition,” in Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.

[11] G. E. Hinton, A. Krizhevsky, and S. D. Wang,

“Transforming auto-encoders,” in International Conference
on Artiﬁcial Neural Networks, pp. 44–51, Springer, 2011.

[12] G. Hinton, “Where do features come from?,” Cognitive

science, vol. 38, no. 6, pp. 1078–1101, 2014.

[13] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial
transformer networks,” in Advances in neural information
processing systems, pp. 2017–2025, 2015.

[14] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing
between capsules,” in Advances in neural information
processing systems, pp. 3856–3866, 2017.

[15] A. Jim´enez-S´anchez, S. Albarqouni, and D. Mateus,
“Capsule networks against medical imaging data
challenges,” in Intravascular Imaging and Computer
Assisted Stenting and Large-Scale Annotation of
Biomedical Data and Expert Label Synthesis, pp. 150–160,
Springer, 2018.

[16] P. Afshar, K. N. Plataniotis, and A. Mohammadi, “Capsule
networks for brain tumor classiﬁcation based on mri images
and course tumor boundaries,” arXiv preprint
arXiv:1811.00597, 2018.

[17] A. Mobiny and H. Van Nguyen, “Fast capsnet for lung

cancer screening,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention,
pp. 741–749, Springer, 2018.

[18] A. S. Lundervold and A. Lundervold, “An overview of deep
learning in medical imaging focusing on mri,” Zeitschrift
f¨ur Medizinische Physik, 2018.

[19] X. Zhang and S. Zhao, “Blood cell image classiﬁcation

based on image segmentation preprocessing and capsnet
network model,” Journal of Medical Imaging and Health
Informatics, vol. 9, no. 1, pp. 159–166, 2019.

[20] L. Wang, R. Nie, R. Xin, J. Zhang, and J. Cai, “sccapsnet:

a deep learning classiﬁer with the capability of
interpretable feature extraction, applicable for single cell
rna data analysis,” bioRxiv, p. 506642, 2018.

[21] A. D. Kumar, “Novel deep learning model for traﬃc sign

detection using capsule networks,” arXiv preprint
arXiv:1805.04424, 2018.

[22] J. Kronenberger and A. Haselhoﬀ, “Do capsule networks

solve the problem of rotation invariance for traﬃc sign
classiﬁcation?,” in International Conference on Artiﬁcial
Neural Networks, pp. 33–40, Springer, 2018.

[23] M. P¨opperl, R. Gulagundi, S. Yogamani, and S. Milz,

“Capsule neural network based height classiﬁcation using
low-cost automotive ultrasonic sensors,” arXiv preprint
arXiv:1902.09839, 2019.

[24] D. A. Lopez, Evolving GPU-Accelerated Capsule Networks.

PhD thesis, 2018.

[25] A. Yusuf and S. Alawneh, “A survey of gpu

implementations for hyperspectral image classiﬁcation in
remote sensing,” Canadian Journal of Remote Sensing,
pp. 1–19, 2018.

[26] A. Stoutchinin, F. Conti, and L. Benini, “Optimally

scheduling cnn convolutions for eﬃcient memory access,”
arXiv preprint arXiv:1902.01492, 2019.

[27] C. Li, Y. Yang, M. Feng, S. Chakradhar, and H. Zhou,

“Optimizing memory eﬃciency for deep convolutional
neural networks on gpus,” in SC’16: Proceedings of the
International Conference for High Performance
Computing, Networking, Storage and Analysis,
pp. 633–644, IEEE, 2016.

[28] M. Song, Y. Hu, H. Chen, and T. Li, “Towards pervasive

and user satisfactory cnn across gpu microarchitectures,” in

2017 IEEE International Symposium on High Performance
Computer Architecture (HPCA), pp. 1–12, IEEE, 2017.

[29] Z. Chen, L. Luo, W. Quan, Y. Shi, J. Yu, M. Wen, and

C. Zhang, “Multiple cnn-based tasks scheduling across
shared gpu platform in research and development
scenarios,” in 2018 IEEE 20th International Conference on
High Performance Computing and Communications; IEEE
16th International Conference on Smart City; IEEE 4th
International Conference on Data Science and Systems
(HPCC/SmartCity/DSS), pp. 578–585, IEEE, 2018.

[30] X. Zhang, C. Xie, J. Wang, W. Zhang, and X. Fu,

“Towards memory friendly long-short term memory
networks (lstms) on mobile gpus,” in 2018 51st Annual
IEEE/ACM International Symposium on Microarchitecture
(MICRO), pp. 162–174, IEEE, 2018.

[31] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li,

T. Chen, Z. Xu, N. Sun, et al., “Dadiannao: A
machine-learning supercomputer,” in Proceedings of the
47th Annual IEEE/ACM International Symposium on
Microarchitecture, pp. 609–622, IEEE Computer Society,
2014.

[32] Z. Du, R. Fasthuber, T. Chen, P. Ienne, L. Li, T. Luo,

X. Feng, Y. Chen, and O. Temam, “Shidiannao: Shifting
vision processing closer to the sensor,” in ACM SIGARCH
Computer Architecture News, vol. 43, pp. 92–104, ACM,
2015.

[33] X. Zhou, Z. Du, Q. Guo, S. Liu, C. Liu, C. Wang, X. Zhou,
L. Li, T. Chen, and Y. Chen, “Cambricon-s: Addressing
irregularity in sparse neural networks through a cooperative
software/hardware approach,” in 2018 51st Annual
IEEE/ACM International Symposium on Microarchitecture
(MICRO), pp. 15–28, IEEE, 2018.

[34] M. Mahmoud, K. Siu, and A. Moshovos, “Diﬀy: a d´ej`a
vu-free diﬀerential deep neural network accelerator,” in
2018 51st Annual IEEE/ACM International Symposium
on Microarchitecture (MICRO), pp. 134–147, IEEE, 2018.

[35] Y. Kwon and M. Rhu, “Beyond the memory wall: A case
for memory-centric hpc system for deep learning,” in 2018
51st Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO), pp. 148–161, IEEE, 2018.

[36] Y. Li, J. Park, M. Alian, Y. Yuan, Z. Qu, P. Pan, R. Wang,

A. Schwing, H. Esmaeilzadeh, and N. S. Kim, “A
network-centric hardware/algorithm co-design to accelerate
distributed training of deep neural networks,” in 2018 51st
Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO), pp. 175–188, IEEE, 2018.

[37] C. Deng, S. Liao, Y. Xie, K. Parhi, X. Qian, and B. Yuan,
“Permdnn: Eﬃcient compressed deep neural network
architecture with permuted diagonal matrices,” in MICRO,
2018.

[38] J. Jeddeloh and B. Keeth, “Hybrid memory cube new dram
architecture increases density and performance,” in 2012
symposium on VLSI technology (VLSIT), pp. 87–88, IEEE,
2012.

[39] S. F. Yitbarek, T. Yang, R. Das, and T. Austin, “Exploring
specialized near-memory processing for data intensive
operations,” in 2016 Design, Automation & Test in Europe
Conference & Exhibition (DATE), pp. 1449–1452, IEEE,
2016.

[40] J. Ahn, S. Yoo, O. Mutlu, and K. Choi, “Pim-enabled

instructions: a low-overhead, locality-aware
processing-in-memory architecture,” in 2015 ACM/IEEE
42nd Annual International Symposium on Computer
Architecture (ISCA), pp. 336–348, IEEE, 2015.

[41] J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi, “A

scalable processing-in-memory accelerator for parallel graph
processing,” ACM SIGARCH Computer Architecture News,
vol. 43, no. 3, pp. 105–117, 2016.

[42] J. Liu, H. Zhao, M. A. Ogleari, D. Li, and J. Zhao,

“Processing-in-memory for energy-eﬃcient neural network
training: A heterogeneous approach,” in 2018 51st Annual
IEEE/ACM International Symposium on Microarchitecture
(MICRO), pp. 655–668, IEEE, 2018.

[43] D. Kim, J. Kung, S. Chai, S. Yalamanchili, and

13

S. Mukhopadhyay, “Neurocube: A programmable digital
neuromorphic architecture with high-density 3d memory,”
in 2016 ACM/IEEE 43rd Annual International Symposium
on Computer Architecture (ISCA), pp. 380–392, IEEE,
2016.

[44] G. E. Hinton, S. Sabour, and N. Frosst, “Matrix capsules

with em routing,” 2018.

[45] E. Xi, S. Bing, and Y. Jin, “Capsule network performance
on complex data,” arXiv preprint arXiv:1712.03480, 2017.

[46] S. S. R. Phaye, A. Sikka, A. Dhall, and D. Bathula, “Dense
and diverse capsule networks: Making the capsules learn
better,” arXiv preprint arXiv:1805.04001, 2018.

[47] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, et al.,

“Gradient-based learning applied to document recognition,”
Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324,
1998.

[48] A. Krizhevsky and G. Hinton, “Learning multiple layers of
features from tiny images,” tech. rep., Citeseer, 2009.

[49] G. Cohen, S. Afshar, J. Tapson, and A. van Schaik,

“Emnist: an extension of mnist to handwritten letters,”
arXiv preprint arXiv:1702.05373, 2017.

[50] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and

A. Y. Ng, “Reading digits in natural images with
unsupervised feature learning,” 2011.

[51] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,

Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer,
“Automatic diﬀerentiation in pytorch,” 2017.

[52] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen,

J. Tran, B. Catanzaro, and E. Shelhamer, “cudnn: Eﬃcient
primitives for deep learning,” arXiv preprint
arXiv:1410.0759, 2014.

[53] “Optimizations To Accelerate Deep Learning Training on

NVIDIA GPUs.”
https://devblogs.nvidia.com/new-optimizations-
accelerate-deep-learning-training-gpu/.

[54] “Nvidia Proﬁler.”

https://docs.nvidia.com/cuda/profiler-users-
guide/index.html.

[55] R. Mukhometzianov and J. Carrillo, “Capsnet comparative
performance evaluation for image classiﬁcation,” arXiv
preprint arXiv:1805.11195, 2018.

[56] “HMC Speciﬁcation 2.1.”

http://hybridmemorycube.org/files/SiteDownloads/HMC-
30G-VSR_HMCC_Specification_Rev2.1_20151105.pdf.

[57] D.-I. Jeon, K.-B. Park, and K.-S. Chung, “Hmc-mac:

Processing-in memory architecture for multiply-accumulate
operations with hybrid memory cube,” IEEE Computer
Architecture Letters, vol. 17, no. 1, pp. 5–8, 2018.

[58] D. De Caro, N. Petra, and A. G. Strollo, “High-performance

special function unit for programmable 3-d graphics
processors,” IEEE Transactions on Circuits and Systems I:
Regular Papers, vol. 56, no. 9, pp. 1968–1978, 2009.

[59] J. Partzsch, S. H¨oppner, M. Eberlein, R. Sch¨uﬀny, C. Mayr,
D. R. Lester, and S. Furber, “A ﬁxed point exponential
function accelerator for a neuromorphic many-core system,”
in 2017 IEEE International Symposium on Circuits and
Systems (ISCAS), pp. 1–4, IEEE, 2017.

[60] C. Lomont, “Fast inverse square root,” Tech-315 nical

Report, vol. 32, 2003.

[61] M. Robertson, A brief history of invsqrt. Department of

Computer Science & Applied Statistics, 2012.

[62] A. Zoglauer, S. E. Boggs, M. Galloway, M. Amman, P. N.

Luke, and R. M. Kippen, “Design, implementation, and
optimization of megalib’s image reconstruction tool
mimrec,” Nuclear Instruments and Methods in Physics
Research Section A: Accelerators, Spectrometers, Detectors
and Associated Equipment, vol. 652, no. 1, pp. 568–571,
2011.

[63] L. Middendorf and C. Haubelt, “A programmable graphics
processor based on partial stream rewriting,” in Computer
Graphics Forum, vol. 32, pp. 325–334, Wiley Online
Library, 2013.

[64] W. Kahan, “Ieee standard 754 for binary ﬂoating-point

arithmetic,” Lecture Notes on the Status of IEEE, vol. 754,
no. 94720-1776, p. 11, 1996.

[65] “Nvidia Tesla P100 Whitepaper.”

https://images.nvidia.com/content/pdf/tesla/
whitepaper/pascal-architecture-whitepaper.pdf.

[66] “NVIDIA-smi.” https://developer.nvidia.com/nvidia-

system-management-interface.

[67] J. D. Leidel and Y. Chen, “Hmc-sim: A simulation

framework for hybrid memory cube devices,” Parallel
Processing Letters, vol. 24, no. 04, p. 1442002, 2014.

[68] “Gate-Level Simulation Methodology - Cadence.”
https://www.cadence.com/content/dam/cadence-
www/global/en_US/documents/tools/system-design-
verification/gate-level-simulation-wp.pdf.

[69] P. Koistinen and L. Holmstr¨om, “Kernel regression and

backpropagation training with noise,” in Advances in
Neural Information Processing Systems, pp. 1033–1039,
1992.

[70] D. Zhang, N. Jayasena, A. Lyashevsky, J. L. Greathouse,

L. Xu, and M. Ignatowski, “Top-pim: throughput-oriented
programmable processing in memory,” in Proceedings of the
23rd international symposium on High-performance
parallel and distributed computing, pp. 85–98, ACM, 2014.

[71] P. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y. Liu, Y. Wang,

and Y. Xie, “Prime: A novel processing-in-memory
architecture for neural network computation in reram-based
main memory,” in ACM SIGARCH Computer Architecture
News, vol. 44, pp. 27–39, IEEE Press, 2016.

[72] A. Shaﬁee, A. Nag, N. Muralimanohar,

R. Balasubramonian, J. P. Strachan, M. Hu, R. S.
Williams, and V. Srikumar, “Isaac: A convolutional neural
network accelerator with in-situ analog arithmetic in
crossbars,” ACM SIGARCH Computer Architecture News,
vol. 44, no. 3, pp. 14–26, 2016.

[73] F. Chen, L. Song, and Y. Chen, “Regan: A pipelined
reram-based accelerator for generative adversarial
networks,” in 2018 23rd Asia and South Paciﬁc Design
Automation Conference (ASP-DAC), pp. 178–183, IEEE,
2018.

[74] H. Mao, M. Song, T. Li, Y. Dai, and J. Shu, “Lergan: A

zero-free, low data movement and pim-based gan
architecture,” in 2018 51st Annual IEEE/ACM
International Symposium on Microarchitecture (MICRO),
pp. 669–681, IEEE, 2018.

[75] B. K. Joardar, B. Li, J. R. Doppa, H. Li, P. P. Pande, and

K. Chakrabarty, “Regent: A heterogeneous
reram/gpu-based architecture enabled by noc for training
cnns,” in 2019 Design, Automation & Test in Europe
Conference & Exhibition (DATE), pp. 522–527, IEEE,
2019.

[76] H. Falahati, P. Lotﬁ-Kamran, M. Sadrosadati, and
H. Sarbazi-Azad, “Origami: A heterogeneous split
architecture for in-memory acceleration of learning,” arXiv
preprint arXiv:1812.11473, 2018.

[77] M. Gao, J. Pu, X. Yang, M. Horowitz, and C. Kozyrakis,
“Tetris: Scalable and eﬃcient neural network acceleration
with 3d memory,” in ACM SIGARCH Computer
Architecture News, vol. 45, pp. 751–764, ACM, 2017.

[78] Y. Shen, M. Ferdman, and P. Milder, “Maximizing cnn
accelerator eﬃciency through resource partitioning,” in
2017 ACM/IEEE 44th Annual International Symposium
on Computer Architecture (ISCA), pp. 535–547, IEEE,
2017.

[79] J. Guo, S. Yin, P. Ouyang, L. Liu, and S. Wei, “Bit-width

based resource partitioning for cnn acceleration on fpga,” in
2017 IEEE 25th Annual International Symposium on
Field-Programmable Custom Computing Machines
(FCCM), pp. 31–31, IEEE, 2017.

[80] S. Wang, G. Ananthanarayanan, Y. Zeng, N. Goel,

A. Pathania, and T. Mitra, “High-throughput cnn inference
on embedded arm big. little multi-core processors,” arXiv
preprint arXiv:1903.05898, 2019.

[81] C. Lo, Y.-Y. Su, C.-Y. Lee, and S.-C. Chang, “A dynamic

deep neural network design for eﬃcient workload allocation

14

in edge computing,” in 2017 IEEE International
Conference on Computer Design (ICCD), pp. 273–280,
IEEE, 2017.

[82] S. Dey, A. Mukherjee, A. Pal, and P. Balamuralidhar,

“Partitioning of cnn models for execution on fog devices,” in
Proceedings of the 1st ACM International Workshop on
Smart Cities and Fog Computing, pp. 19–24, ACM, 2018.

[83] A. C. I. Malossi, Y. Ineichen, C. Bekas, and A. Curioni,

“Fast exponential computation on simd architectures,”
Proc. of HIPEAC-WAPCO, Amsterdam NL, 2015.

[84] F. Perini and R. D. Reitz, “Fast approximations of

exponential and logarithm functions combined with eﬃcient
storage/retrieval for combustion kinetics calculations,”
Combustion and Flame, vol. 194, pp. 37–51, 2018.

15

