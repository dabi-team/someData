Categorical semantics of a
simple differential programming language

Geoffrey Cruttwell
Mount Allison University, Sackville, Canada

Jonathan Gallagher

Dorette Pronk

Dalhousie University, Halifax, Canada

gcruttwell@mta.ca

jonathan.gallagher@dal.ca

dorette.pronk@dal.ca

1

Introduction

With the increased interest in machine learning, and deep learning in particular (where one extracts
progressively higher level features from data using multiple layers of processing), the use of automatic
differentiation has become more wide-spread in computation. See for instance the surveys given in [32]
and [4]. In fact, Facebook’s Chief AI scientist Yann LeCun has gone as far as famously exclaiming:

“Deep learning est mort. Vive Differentiable Programming! ...people are now building a
new kind of software by assembling networks of parameterized functional blocks and by
training them from examples using some form of gradient-based optimization.”1

The point being that differentiation is no longer being viewed as merely a useful tool when creating
software, but instead becoming viewed as a fundamental building block. This sort of ubiquity warrants a
more in-depth study of automatic differentiation with a focus on treating it as a fundamental component.
There have been two recent developments to provide the theoretical support for this type of structure.
In fact, the settings described above use two types of differentiation:
the usual forward derivative to
analyse the effect of changes in the data, as well as the reverse derivative to allow for error correction (i.e.,
training) through the efﬁcient calculation of the gradients of functions. Thus, any theoretical approach
must be able to deal with both types of differentiation. One approach is presented in [2], where Abadi
and Plotkin provide a simple differential programming language with conditionals, recursive function
deﬁnitions, and a notion of reverse-mode differentiation (from which forward differentiation can be
derived) together with both a denotational and an operational semantics, and theorems showing that the
two coincide. Another approach is given in [16], where the authors present reverse differential categories,
a categorical setting for reverse differentiation. They also show how every reverse differential category
gives rise to a (forward) derivative and a canonical “contextual linear dagger” operation. The converse
is true as well: a category with a foward derivative (that is, a Cartesian differential category [8]) with a
contextual linear dagger has a canonical reverse derivative.

In the present paper we bring these two approaches together. In particular, we show how an extension
of reverse derivative categories models Abadi and Plotkin’s language, and describe how this categorical
model allows one to consider potential improvements to the operational semantics of the language. To
model Abadi and Plotkin’s language categorically, reverse derivative categories are not sufﬁcient, due
to their inability to handle partial functions and control structures. Thus, we need to add partiality
to reverse differential categories. The standard categorical machinery to model partiality is restriction
structure, which assigns to each map a partial identity map, subject to axioms as described in [14].
Combining this structure with reverse differential structure, we introduce reverse differential restriction

1Facebook post on Jan 5, 2018

David I. Spivak and Jamie Vicary (Eds.):
Applied Category Theory 2020 (ACT2020)
EPTCS 333, 2021, pp. 289–310, doi:10.4204/EPTCS.333.20

290

Categorical semantics ofasimple differential programming language

categories. In addition to the list of axioms [RD.1 – 7] given for reverse differential categories in [16],
we require two additional axioms expressing how the restriction of the reverse derivative of a function is
related to the restriction of the function itself and what the reverse derivative of a restriction of a function
needs to be (cf. Deﬁnition 3.1). The results characterizing the relationship between differential and
reverse differential categories in terms of a contextual linear dagger extend to the context of restriction
categories. We also get for free that the reverse derivative preserves the order on the maps and preserves
joins of maps, if they exist.

In Section 4 we show how Abadi and Plotkin’s language can be modelled in a reverse differential
restriction category. We do this in two steps: at ﬁrst we modify their language by omitting general
recursion and instead only include while-loops. While-loops can be modelled in terms of recursion,
but by separating them out we can see that source-transformation techniques (not discussed explicitly
in Abadi and Plotkin but used in some commercial systems such as Theano [5], TensorFlow [1], and
Tangent [45]) always hold in our semantics (see Section 4.3); source transformation techniques are not
used for general recursion. We also note that in order to be able to push differentiation through the
control structure, Abadi and Plotkin need that for each predicate the inverse images of true and false are
both open sets. In the context of restriction categories we model this instead by providing two partially
deﬁned maps into the terminal object 1 for each predicate symbol (one for true and one for false) with
the requirement that their restrictions do not overlap (cf. Section 4.2).

In the process of modelling Abadi and Plotkin’s language, we see that not all our axioms are needed.
Speciﬁcally, Axioms [RD.6] and [RD.7] of a reverse differential restriction category (which deal with
the behaviour of repeated reverse derivatives) are not strictly necessary to model Abadi and Plotkin’s lan-
guage. However, in the ﬁnal section of the paper, we show that if these axioms are present, changes can
be made to the operational semantics to improve the efﬁciency and applicability of the simple differential
programming language.

f

Abadi and Plotkin’s language represents an approach that makes the reverse derivative a language
primitive in a functional language. Other approaches have been proposed to use reverse-mode accumula-
−−→ Rm, Pearlmutter and
tion for computing the derivative in a functional language. Given a function Rn
Siskind [39] discuss how to compute the Jacobian matrix of f in a functional language by performing
transformations on the function’s computational graph. This idea is similar to the symbolic differentia-
tion of trace or tape terms in Abadi and Plotkin’s language. Elliot [26] shows how to view this sort of
reverse-mode accumulation using continuations: when a function, written as a composition of simple
operations, is written in continuation passing style, the reverse derivative of its computation graph corre-
sponds to a sort of generalized derivative of the continuation. In [46], Wang et al extend Pearlmutter and
Siskind’s work by showing that the move to continuations allows for getting around the nonlocality issue
in the earlier work. Brunel et al [10] extend Wang’s work from the point of view of linear logic, and
allow for additional analyses based on tracking the linearity of a variable. Abadi and Plotkin’s work rep-
resents a next step in this area by considering, in addition, control ﬂow structures and general recursive
functions.

This current work contrasts to work submitted to ACT 2019 on modelling differential programming
using synthetic differential geometry (SDG) (see [34, 35] for an introduction to SDG). In the previous
work a simple differential programming language featuring forward differentiation was introduced and
an interpretation into a well-adapted model of SDG was given (see e.g. [23] for such models). The focus
was on exploring what programming languages features might be able to exist soundly with differential
programming. The current work develops the categorical semantics of Abadi and Plotkin’s language for
reverse differentiation as well as the categorical semantics of source-transformations for their language.

G.Cruttwell, J.Gallagher, D.Pronk

291

In particular we show that the operational semantics is modelled soundly by a denotational semantics
into our categorical framework. We will also see that using the axiomatic approach developed here leads
to a sound exponential speedup when computing the reverse derivative of looping-phenomena.

2 Background: Relevant Categorical Structures

In this section, we brieﬂy review some of the relevant structures from category theory which we will
make use of.

2.1 Cartesian and reverse differential categories

The canonical category for differentiation is the category Smooth whose objects are the powers of the re-
als R (R0 = {1}, R, R2, R3, etc.) and whose maps are the smooth (inﬁnitely differentiable) maps between
them. To any map f : A −→ B in this category, there is an associated map

D[ f ] : A × A −→ B

whose value at (x, v) ∈ A × A is J( f )(x) · v, the Jacobian of f at x, taken in the direction v. This map
satisﬁes various rules; for example, the chain rule is equivalent to the statement that for any maps f : A
−→ B, g : B −→ C,

D[ f g] = hπ0 f , D[ f ]i D[g].

(Note that we use the path-order for composition, so f g means “ﬁrst f then g”.) Many other familiar
rules from calculus can be expressed via D; for example, the symmetry of mixed partial derivatives can
be expressed as a condition on D2[ f ] = D[D[ f ]].

Deﬁnition 2.1. ([8, Defn. 2.1.1]) A Cartesian differential category or CDC is a Cartesian left additive
category ([8, Defn. 1.3.1]) which has, for any map f : A −→ B, a map

satisfying seven axioms [CD.1–7].

D[ f ] : A × A −→ B

The formulation of CDCs and indeed the other ﬂavours of categories with derivatives we will use
have the intent that in ha, vi D[ f ], a is the point and v is the direction; this is in contrast to the original
formulation of CDCs which had the point and direction swapped, and we chose the point-direction
formulation because most of the literature follows this convention. This causes a change to axioms
CD.2,6,7.

While Smooth is the canonical example, there are many others, including examples from inﬁnite
dimensional vector spaces, synthetic differential geometry, algebraic geometry, differential lambda cal-
culus, etc: see [8, 28, 11, 19, 15].

In contrast, the reverse derivative, widely used in machine learning for its efﬁciency, is an operation

which takes a smooth map f : A −→ B and produces a smooth map

whose value at (x, w) ∈ A × B is J( f )T (x) · w, the transpose of the Jacobian of f at x, taken in the direction
w.

R[ f ] : A × B −→ A

292

Categorical semantics ofasimple differential programming language

There are two possible ways to categorically axiomatize the reverse derivative. One way is to start
with a CDC and ask for a dagger structure (representing the transpose); one could then use the dagger
with the D from the CDC to deﬁne a reverse derivative R. However, there is some subtlety in this: the
dagger structure is only present on the linear maps of the category, not on all the maps of the category.
The other way is to axiomatize R directly, as was done in [16].

Deﬁnition 2.2. ([16, Defn. 13]) A reverse differential category or RDC is a Cartesian left additive
category which has, for any map f : A −→ B, a map

R[ f ] : A × B −→ A

satisfying seven axioms.

For example, in this formulation the chain rule is equivalent to [RD.5], the rule that for any maps

f : A −→ B, g : B −→ C,

R[ f g] = hπ0, ( f × 1)R[g]i R[ f ].

Moreover, there is something striking about a reverse differential structure: any RDC is automati-
cally a CDC. If one applies the reverse derivative twice, 0’s out a component and projects, the result is
the forward derivative. That is, the following deﬁnes a (forward) differential structure from a reverse
differential structure (see [16, Theorem 16]):

A

f
−−→ B
R[ f ]
−−−→ A
R[R[ f ]]

A × B

(A × B) × A
hπ0,0,π1i

−−−−−→ A × B
R[R[ f ]]

D[ f ] := A × A

−−−−−−→ (A × B) × A

−−−−−→ A × B

π1−−→ B

Thus, while a “dagger on linear maps” is required to derive an RDC from a CDC, no such structure is
required to go from an RDC to a CDC. In fact, one can show that a CDC with a “dagger on linear maps”
is equivalent to an RDC: see Theorem 42 in [16].

For this reason, as well as the fact that the reverse derivative is of greater importance in machine

learning, in this paper we take a reverse differential category to be the primary structure.

2.2 Restriction categories and differential restriction categories

Of course, to model a real-world programming language which involves non-terminating computations,
we must also be able to handle partial functions. For this, we turn to restriction categories [14], which
allow one to algebraically model categories whose maps may only be partially deﬁned. Consider the
category of sets and partial functions between them. To any map f : A −→ B in this category, there is an
associated “partial identity” map f : A −→ A, which is deﬁned to be the identity wherever f is deﬁned,
and undeﬁned otherwise. This operation then has various properties such as f f = f . This is then
axiomatized:

Deﬁnition 2.3. ([14, Defn. 2.1.1]) A restriction category is a category which has for any map f : A −→ B,
a map f : A −→ A satisfying various axioms.

In section 3, we will combine restriction structure with reverse differential structure to get the cate-

gorical structure we will use to model Abadi and Plotkin’s language.

G.Cruttwell, J.Gallagher, D.Pronk

293

Before we get to that, however, we will need to brieﬂy review a few deﬁnitions from restriction
It will also be helpful to consider the previously deﬁned combination of restriction

category theory.
structure and (forward) differential structure.

A restriction category allows one to easily talk about when a map is “less than or equal to” a parallel

map and when two parallel maps are “compatible”:
Deﬁnition 2.4. Suppose f , g : A −→ B are maps in a restriction category. Write f ≤ g if
write f ∼ g (and say “ f is compatible with g”) if f g = g f .

f g = f , and

That is, f ≤ g if g is deﬁned wherever f is deﬁned, and when restricted to f ’s domain of deﬁnition,
g is equal to f ; f ∼ g if f and g are equal where they are both deﬁned. One can show that ≤ is a partial
order on each hom-set; in fact, restriction categories are canonically partial order enriched by ≤.

Being able to “join” two compatible maps will be important when we deﬁne control structures such

as “if” and “while”, as we will being able to discuss when maps are “disjoint”.
Deﬁnition 2.5.

1. If f , g : A −→ B and there is a least upper bound f ∨ g with respect to the partial
order deﬁned above, we call f ∨ g the join of f and g. Note that this implies that f and g are
compatible.

2. The notion of join extends to families of maps that are pairwise compatible, and we write ∨i fi to

denote the join of the pairwise compatible family.

3. Say that a map /0 : A −→ B is nowhere deﬁned if /0 is the minimum in the partial order.
4. Say that f , g : A −→ B are disjoint if f g is nowhere deﬁned. Any two disjoint maps are compatible.
The formalization of disjoint joins in a restriction category was given in [18] as part of the story of
formalizing Hoare semantics in a classical restriction category. Further analysis of joins in restriction
categories was provided in [21]. Giles [31] used disjoint joins in connecting restriction categories to the
semantics of reversible computing. Disjoint joins in partial map categories correspond to disjoint joins
of monics, which often give a coproduct (e.g. as in coherent categories). One way to model iteration is to
have a traced coproduct, and this can be directly expressed using disjoint joins: this approach was used
in formalizing iteration in restriction categories and to build a partial combinatory algebra by iterating
a step-function in [20, 13]. The formalization of iteration using disjoint joins was based on the work of
Conway [22]. Another approach to formalizing the semantics of iterative processes in a category using
algebraic formalizations was introduced in [25], reﬁned in [7], and further developed categorically in [3].
Finally, it is worth noting that there has been previous work combining CDC structure with restriction
structure [12]. The canonical example of such a category is the category of smooth partial maps between
the Rn’s. The partiality acts in a compatible way with the derivative, as D[ f ] : A × A −→ B is entirely
deﬁned in the second (vector) component: that is, the only partiality D[ f ] has is from f itself. Thus, in a
“differential restriction category”, one asks that D[ f ] = f × 1. These are formulated on top of the notion
of cartesian left additive restriction category: these are restriction categories with restriction products
(which is a lax notion of product for restriction catgories developed in [17]) and where each homset is
a commutative monoid such that x( f + g) = x f + xg and 0 f ≤ 0, and ﬁnally projections fully preserve
addition. The intuition comes from considering partial, smooth functions on open subsets of Rn: not all
smooth functions preserve addition, but smooth functions are addable under pointwise addition.
Deﬁnition 2.6. ([12, Defn. 3.18]) A differential restriction category is a Cartesian left additive restric-
tion category, which has, for each map f : A −→ B, a map

D[ f ] : A × A −→ B
satisfying various axioms 2, including [DR.8]: D[ f ] = f × 1.

2There are nine equational axioms mirroring the axioms for reverse differential restriction categories given in the sequel.

294

Categorical semantics ofasimple differential programming language

3 Reverse differential restriction categories

We are now ready to deﬁne the new structure which we will use to model Abadi and Plotkin’s language.

Deﬁnition 3.1. A reverse differential restriction category or RDRC is a Cartesian left additive restric-
tion category which has an operation on maps:

A

f
−−→ B
A × B −−−→

R[ f ]

A

such that

[RD.1] R[ f + g] = R[ f ] + R[g] and R[0] = 0;

[RD.2] for all a, b, c: ha, b + ci R[ f ] = ha, bi R[ f ] + ha, ci R[ f ] and ha, 0i R[ f ] = a f 0;

[RD.3] R[πj] = π1ιj;

[RD.4] R[h f , gi] = (1 × π0)R[ f ] + (1 × π1)R[g];

[RD.5] R[ f g] = hπ0, hπ0 f ,π1i R[g]i R[ f ];

[RD.6] h1 × π0, 0 × π1i (ι0 × 1)R[R[R[ f ]]]π1 = (1 × π1)R[ f ];

[RD.7] (ι0 × 1)R[R[(ι0 × 1)R[R[ f ]]π1]]π1 = ex(ι0 × 1)R[R[(ι0 × 1)R[R[ f ]]π1]]π1;

[RD.8] R[ f ] = f × 1;

[RD.9] R[ f ] = ( f × 1)π1.

As noted above, [RD.5] represents the chain rule, while [RD.8] says that the partiality of R[ f ] is
entirely determined by the partiality of f itself. [RD.9] says how to differentiate restriction idempotents.
The other axioms are similar to those for an RDC; for an explanation of what they represent, see the dis-
cussion after Deﬁnition 13 in [16]. Also note that term logics have also been given to simplify reasoning
in cartesian differential categories [8] and differential restriction categories [29]; a term logic for reverse
differential restriction categories exists but will not be discussed further here.

Any Fermat theory [24] and more generally any Lawvere theory which is also a cartesian differential
category can be given the structure of a reverse differential category; in these cases both the forward
and reverse derivatives can be pushed down to sums and tuples of derivatives on maps R −→ R, and here
the forward and reverse derivative necessarily coincide. A restriction version of this example is given
by considering a topological ring R that satisﬁes the axiom of determinacy (see [6]); the category with
objects: powers of R, and C∞-maps that are smooth on restriction to an open set form a reverse differential
restriction category. This meta-example includes the category SmoothP of functions that are smooth on
an open subset of Rn. For an example whose objects are not of the form Rn: the coKleisli category of the
multiset comonad on the category of relations Rel is a cartesian differential category and the category of
linear maps is Rel (see [9, 8] for details). As Rel is a compact closed self-dual category, and the derivative
at a point is linear (hence a map in Rel), one can obtain a reverse derivative on the coKleisli category of
the ﬁnite multiset comonad on Rel.

Just as with an RDC, we can derive a forward differential restriction structure from a reverse.

Theorem 3.2. Every reverse differential restriction category X is a differential restriction category with
the derivative deﬁned as previously (see also [16, Theorem 16]).

G.Cruttwell, J.Gallagher, D.Pronk

295

Moreover, just as in [16, Theorem 42], one can prove that a DRC with a “contextual linear dagger”
is equivalent to an RDRC; however, for space constraints we will not go into full details here. One must
ﬁrst describe ﬁbrations for restriction categories: these were studied by Nester in [37]. One can give a
version of the simple ﬁbration for a restriction category as well as the dual of the simple ﬁbration (this
is remarkable – as the dual of a restriction category is not generally a restriction category). Importantly,
f = e × 1 where e = e .
maps in the simple ﬁbration have their partiality concentrated in the context i.e.
A contextual dagger is a an involution of ﬁbrations Lin(X)[X] −→ Lin(X)[X]∗ where Lin(X)[X] denotes a
subﬁbration of the simple ﬁbration consisting of linear maps in context, using the notion of ﬁbration for
restriction categories. From a reverse differential restriction category one obtains such an involution of
ﬁbrations from (u, f ) 7→ (u, (ι0 × 1)R[ f ]π1), and the second component is sometimes written f †[I] where
I is the context object. There are a few subtleties that we will also not go further into here.

The reverse derivative automatically preserves the induced partial order (from the restriction struc-

ture) and joins, if they exist:
Proposition 3.3. If X is a reverse differential restriction category, then for any f , g : A −→ B, f ≤ g
implies R[ f ] ≤ R[g], and if X has joins, then for any pairwise compatible family { fi}, R[∨i fi] = ∨iR[ fi].
As we shall see, we will not strictly need [RD.6] and [RD.7] to model Abadi and Plotkin’s language;

thus, we make the following deﬁnition:

Deﬁnition 3.4. A basic reverse differential restriction category (or basic RDRC) is a structure satisfying
all the requirements for an RDRC except [RD.6] and [RD.7].

However, as we discuss in the ﬁnal section, using axioms [RD.6] and [RD.7] allows one to consider

improvements to the operational semantics of the language.

4

Interpretation of a simple differential language

We will make use of the language deﬁned by Abadi and Plotkin [2]. We will make one modiﬁcation up
front. We will ﬁrst consider the language without recursive function deﬁnitions and instead with while-
loops (called SDPL); after showing the semantics works out, we will then add recursive deﬁnitions back
in (called SDPL+). We remark that while the presentation of SDPL given in Plotkin and followed here
is parametrized over a single generating type; however, we can add arbitrary generating types as long as
those types have operations that provide the structure of a commutative monoid.

In [2], Abadi and Plotkin remarked that there are two approaches to differentiating over control
structures: there are source transformations used in systems such as TensorFlow [1] and Theano [5] and
there is the execution trace method used in systems such as Autograd [36] and PyTorch [38]. The source
transformation method for dealing with derivatives of control structures deﬁnes a way to distribute the
derivative into control structures; for example

would be replaced by

∂ if B then M else N
∂x

if B then

∂m
∂x

else

∂n
∂x

The execution trace allows deﬁning a symbolic derivative on simpler terms with no control structures
or derivatives, and then evaluating a term enough so that there are no control structures or derivatives
present, allowing a symbolic trace through the derivative. This must be done at runtime – for example,

296

Categorical semantics ofasimple differential programming language

Γ, x : A ⊢ x : A

r ∈ R
Γ ⊢ r : real

Γ ⊢ m : real Γ ⊢ n : real
Γ ⊢ m + n : real

Γ ⊢ m : T

op : T −→ U ∈ Σ

Γ ⊢ op(m) : U

Γ ⊢ m : T Γ, x : T ⊢ n : U
Γ ⊢ let x : T = m in n : U

Γ ⊢ ∗ : 1

Γ ⊢ m : U Γ ⊢ n : T
Γ ⊢ (m, n)U,T : U × T

Γ ⊢ m : U × T
Γ ⊢ fstU,T (m) : U

Γ ⊢ m : U × T
Γ ⊢ sndU,T (m) : T

Γ ⊢ b Γ ⊢ m : T Γ ⊢ n : T
Γ ⊢ if b then m else n : T

p : U ⊢ b
p : U ⊢ f : U
p : U ⊢ while b do f : U

Γ, x : U ⊢ m : T Γ ⊢ a : U Γ ⊢ v : T
Γ ⊢ v.rd(x : U.m)(a) : U

Γ ⊢ true

Γ ⊢ false

Γ ⊢ m : U pred : U ∈ Pred
Γ ⊢ pred(m)

Table 1: Typing rules for SDPL

we need to know when differentiating over an if-then-else statement which branch was taken, and once
this control structure is eliminated the derivative can be computed on the simpler resultant term. This has
the advantage of making it simpler to adapt to derivatives over more subtle structures such as recursive
function deﬁnitions. Since it’s done at runtime, it can performed by a source-transformation by a JIT
compiler ensuring efﬁciency.

4.1 The core language SDPL

The types of SDPL are given by the following grammar:

Ty := real | 1 | Ty × Ty

Powers are assumed to be left-associated so realn+1 := realn × real. To form the raw terms of SDPL we
assume a countable supply of variables, a set of typed operation symbols Σ, and a set of typed predicate
symbols Pred. The raw terms are then deﬁned by the following grammar:

m := x | r (r ∈ R) | m + m | op(m) (op ∈ Σ) | let x : Ty = m in m

| ∗ | (m, n)Ty,Ty | fstTy,Ty(m) | sndTy,Ty(m) | if b then m elsen
| while b do m | m.rd(x : T.m)(m)

b :=pred(m) (pred ∈ Pred) | true | false

Note that the typing rules will disallow inputs or outputs to come from boolean terms. This is to
ensure that all typed terms are differentiable with respect to every argument. The typing rules for SDPL
are given in Table 1. In the typing rules, Γ is assumed to be a list of typed variables Γ = [xi : Ai]n
i=1 where
Ai ∈ Ty. Free variables are deﬁned in the usual way; note that let expressions bind the variable x and

G.Cruttwell, J.Gallagher, D.Pronk

297

when forming the reverse differential term v.rd(x : U.m)(a) the variable x is also bound. The reverse
differential expression may be read as “the reverse differential of m with respect to x evaluated at the
point a in the direction v.”

4.2 Categorical interpretation of SDPL

Let X be a basic reverse differential restriction category with countable joins of disjoint maps. An
interpretation structure for SDPL into X is given by a tuple of structures:

(A ∈ X0, (1

ar−−→ A)r∈R,

,

T ,

F )

and we extend such a structure to an interpretation of all the terms of SDPL as explained below. We must
ﬁrst interpret types, and to begin we need an object A from X to carry our signatures. We also require
ar−−→ A for each element r ∈ R (since we must interpret R constants which are part of
that A has a point 1
SDPL) 3. With such an A we deﬁne an interpretation of types:

J K

J K

J K

1

:= 1

real

:= A

T ×U

:=

T

×

U

J
We extend the interpretation to contexts:

K

J

K

J

K

J

K

J

K

·

:= 1

x : U

:=

U

Γ, x : U

:=

Γ

×

U

J

K

J

K

J

K

J

K

J

K

J

K

We also require an interpretation of each operation symbol op : T −→ U ∈ Σ of the correct type:
. We additionally require two interpretations of each predicate symbol pred : U ∈ Pred:
−→
J
U
K

F = /0. To summarize:

−→ 1 such that

pred

pred

pred

F :

U

T

op
:
pred
J
K
K

J

T
T :
K

U
−→ 1 and
J
K

J

J

K

K

J
Σ(U, T )

K

J
T

,

)

J
K
Pred(U )

J K
−−→ X(
J

U

K

J

K

J KT ,J KF
−−−−−−→ X(
J

U

, 1)
K

K

J

U

The intent for giving two interpretations of predicate symbols is that we must give an interpretation
of the “true” part of the predicate and the “false” part. In [2] an interpretation of predicate symbols is
−→ {true, false} with the property that the preimages of both true and false are open.
given as maps
This necessarily makes the interpretation of a predicate partial or trivial, and moreover it is equivalent to
giving an interpretation of predicate symbols into disjoint open sets of
, which is again equivalent to
U
giving an interpretation into disjoint predicates on
. A way around this non-standard interpretation
of predicates is given by taking the manifold completion [33, 15] of the model, noting that 1 + 1 is a
J
manifold, and then requiring that we map into 1 + 1 by an atlas morphism, which will necessarily yield
two disjoint restriction idempotents on the domain. Another approach is to use the Heyting negation of
the associated restriction idempotent, noting that this will always be disjoint from the starting map. These
two approaches have interesting relationships with the approach we take, but their full development wil
not be pursued in this work.

U

J

K

K

We then extend the interpretation to all terms inductively. Most of these interpretations are standard;

the more novel parts are the interpretations of if, while, and reverse derivatives.

Proj:

•

x : U ⊢ x : U
J

K

:= 1JUK;

3It is not strictly necessary that SDPL contains a constant for every r ∈ R – as long as we include 0 we could only require

constants that we actually use, such as the computable reals.

298

Categorical semantics ofasimple differential programming language

•

•

Γ, x : U ⊢ x : U
J
Γ, y : U ⊢ x : T
J

:=

γ

×

U

:=

J
γ

K

×

J
U

K

π1−−→
π0−−→

U

;

J
γ

JΓ ⊢ x : T K
K
−−−−−−−→

T

.

J

K

J

K

J

K

J

K

K

K

Real operations:

:=

Γ

0

−−→ A and for the other elements

• We deﬁne
ar = JrK

−−−−−→ A =

Γ ⊢ 0 : real
J

real

K

•

J
Γ ⊢ m + n : real
K
J

K
:=

Γ

Operation terms: Given op : T −→ U ∈ Σ

J

K

K

J
JΓ ⊢ m : realK + JΓ ⊢ n : realK
−−−−−−−−−−−−−−−−−→

real

J

K

Γ ⊢ r : real
K

J

:=

Γ

!
−→ 1

J

K

Let:

Γ ⊢ op(m) : U

:=

Γ

JΓ ⊢ m : T K
−−−−−−−→

T

JopK
−−−→

U

J

K

J

K

J

K

J

K

Γ ⊢ let x : T = m in n : U

:=

Γ

h1,JΓ ⊢ m : T Ki
−−−−−−−−−→

Γ

×

T

JΓ,x : T ⊢ n : UK
−−−−−−−−−−→

U

J
Product terms:

K

J

K

J

K

J

K

J

K

•

•

•

•

!
−→ 1

K

J

Γ

:=

Γ ⊢ ∗ : 1
J
K
Γ ⊢ (m, n)A,B : A × B
K
J
Γ ⊢ fst(m)A,B : A
:=
J
K
Γ ⊢ snd(m)A,B : B
J

K

hJΓ ⊢ m : AK,JΓ ⊢ n : BKi
−−−−−−−−−−−−−−→

:=

Γ

Γ
JΓ ⊢ m : A × BK
J
K
−−−−−−−−−→
JΓ ⊢ m : A × BK
J
−−−−−−−−−→

J
:=

K
Γ

A

×

B

K
A

J
×

K
B

B

J
A

K

A
×
π0−−→
J
K
π1−−→
J

Control structures:

J

K

J

K

J

K

J

K

K
B

∨

Γ ⊢ if b then m else n : U

:=

J
p : A ⊢ while b do m : A

:=

K
∞

J

T

Γ ⊢ b
K
p : A ⊢ b

Γ ⊢ m : U
J

J

K
p : A ⊢ m : A

Γ ⊢ b
K
i

J

Reverse derivatives:

K

_i=0 (cid:18)(cid:16) J

T
K

J

Γ ⊢ n : U

F

J
q : A ⊢ b
K
J

K

F

(cid:19)

K(cid:17)

Γ ⊢ v.rd(x : T.m)(a) : T

J
:=

Γ

hh1,JΓ ⊢ a : T Ki ,JΓ ⊢ v : UKi
−−−−−−−−−−−−−−−−→ (

K

Γ

×

T

) ×

U

R[JΓ,x : T ⊢ m : UK]π1
−−−−−−−−−−−−−→

T

K

J
J
Γ ⊢ true
Γ ⊢ true
F :=!. Finally for any pred ∈ Pred(A):
J
J
K

T :=
K

−→ 1 and

Γ

K

J

!

K

K

J
J
F := /0. Likewise,
K

J

K
T := /0 and

Γ ⊢ false
K
J

Boolean terms:
Γ ⊢ false
K
J

Γ ⊢ pred(m)

where H ranges over {T, F}.

J

H :=
K

JmK
−−−→

Γ

A

JpredKH
−−−−−→ 1

J

K

J

K

For a brief explanation of the interpretation of while-loops, for f : A −→ A we set f 0 = id and f n+1 =
f f n. Then our interpretation says either the guard was false, or it was true and we executed m and then it
was false, or it was true and we executed m and it was still true and we executed m again and then it was
false, and so on. This yields

whileb do m

:=

b

F ∨

b

T

m

b

F ∨

b

T

m

b

m

T

J

K

J

K

J

K

J

KJ

K

J

K

J

KJ

K

J

b

F ∨ · · ·

J

K

T
K

G.Cruttwell, J.Gallagher, D.Pronk

299

4.3 Categorical semantics of source code transformations

In this section we show that the interpretation above always soundly models source code transformations
for differentiating if-then-else statements and while-loops.

Proposition 4.1. In an interpretation structure on a basic RDRC, for any terms Γ, x : U ⊢ m : T , Γ, x :
U ⊢ n : T , Γ ⊢ a : U , and Γ ⊢ v : T and for any predicate Γ, x : U ⊢ B we have

Γ ⊢ v.rd(x : U.if b then m else n)(a)
J
=

Γ ⊢ if (let x = a in b) then v.rd(x : U.m)(a) else v.rd(x : U.n)(a)

K

Corollary 4.2 (If-then-else transformation).
have

J

In an interpretation structure on a basic RDRC, we always

K

Γ, x : U ⊢ v.rd(x : U.if b then m else n)(x)
J
=

Γ, x : Y ⊢ if b then v.rd(x : U.m)(x) else v.rd(x : U.n)(x)

K

K

J

Turning to iteration, if a while-loop terminates, then while b do f is f n for some n. The forward

derivative admits a tail recursive description:

D[ f n+1] = hπ0 f , D[ f ]i D[ f n]

SDPL has two admissable operations: dagger and forward differentiation.

m†[Γ] := y.rd(x.m)(0)

fd(x.m)(a).v := let z = v in (y.rd(x.m)(a))†[Γ]

where the y in m†[Γ] is fresh. The recursive description of D[ f n] is useful in proving the following:
Proposition 4.3 (Forward-differentiation for while-loops). In an interpretation structure on a basic
RDRC,

1. For any Γ, x : A ⊢ m : B,

fd(x.m)(a).v

= hh1,

a

i ,

v

2. For any x : A ⊢ f : A we have

J

⊢ fd(x.while b do f )(a).v

J

K

K

K

J

i (1 × ι1)D[
J

m

]

K

=

⊢ let x = a, y = v in snd(whileπ0b do (π0 f , fd(x. f )(x).y))

J

K

On the other hand, the reverse derivative satisﬁes:

J

K

R[ f n](a, b) = R[ f ](a, R[ f ]( f (a), R[ f ]( f ( f (a)), · · · , b)))

Which looks at ﬁrst glance to be head recursive, and not like something that can be implemented by an
iteration. However, with [RD.6], we can do the following:

R[ f n+1] = D[ f n+1]†[A] = (T ( f )nD[ f ])†[A]

where T ( f ) = hπ0 f , D[ f ]i

This is the basis of the following source transformation for while-loops.

Corollary 4.4 (Reverse-differentiation of while-loops). In an interpretation structure on an RDRC, let
z : A ⊢ f : A and z : A ⊢ b; then we have

v : A ⊢ v.rd(x.while b do f )(a)
J
= rv : A ⊢ (⊢ let x = a, y = v in snd(whileπ0b do (π0 f , fd(x. f )(x).y)))†[.]

K

z

where †[.] denotes the dagger deﬁned above with respect to the empty context.

300

Categorical semantics ofasimple differential programming language

4.4 Smooth recursive deﬁnitions

Until now we discussed the semantics of a fragment of the language described by [2]. We formally
extended their language with while-loops to isolate their behaviour, but missed out on recursive function
deﬁnitions. Given general recursion, one can implement loops using tail recursion. We now move to
discuss their full language with recursive function deﬁnitions. This language will be called SDPL+. To
give such an extension, we introduce two new raw terms

m := m as before | f (m) | letrec f (x) := m in n

In the above, when we form f (a), the symbol f is taken to be a free function variable, and the term
letrec f (x) := m in n binds the variable x in m and the function variable f in m and n. However, these
function variables are of a different sort than ordinary variables because they have arity. That is f (a)
only makes sense if a : B and f has arity B −→ C, which we write as f : B −→ C. Thus, our typing/term
formation rules will have two sorts of contexts, one to record function names and the other for ordinary
variables. Our terms in context then have the form Φ|Γ ⊢ m : B, and to update the rules from before, just
add Φ to all the contexts. The two new rules are

Φ, f : A −→ B|Γ ⊢ m : A
Φ, f : A −→ B|Γ ⊢ f (m) : B

Φ, f : A −→ B|x : A ⊢ m Φ, f : A −→ B|Γ ⊢ n : C
Φ|Γ ⊢ letrec f (x) := m in n : C

We will now give the interpretation of recursive deﬁnitions and calls in a basic reverse differential
join restriction category. But ﬁrst, we will review a basic bit of intuition of recursive function theory in
case the reader is unfamiliar.

We often write computable functions A

−−→ B as f (n) := m, but it is usually helpful to think of f as
simply a name for the unnamed function λn.m and then write f = λn.m. The idea is that as a computation
f has an internal representation that uses the variable n somewhere. If f is recursive then that means that
the symbol f also appears in m, and thus it is a function that depends on itself. To break this cycle we
then abstract out the symbol f too. We write f := λ f .λn.m. This creates a function

f

Fun(A, B)

f

−−→ Fun(A, B)

f takes an arbitrary computable function A h
f was used in the body m.

−−→ B and creates a function that uses h instead of f anywhere

To give a quick example, consider the computable function fac(n) := if n < 1 then 1 else n∗fac(n−

1). Then

fac(h)(n) := if n < 1 then 1 else n ∗ h(n − 1)

The point is that this new function is not recursive. However, it is instructive to see what happens when
we apply it to the function it represents. As an exercise, we leave it to the reader to prove that

fac(fac) = fac

In other words, the recursive function fac is a ﬁxed point of the functional fac. It is also the best ﬁxed
point of fac in the sense that it is the least deﬁned function that is a ﬁxed point of fac. This works in
general, given any recursive function r it may be obtained as the least ﬁxed point of r.

To model least-ﬁxed-point phenomena we will use the notion of a pointed directed complete partial
order or (DCPPO) for short. The ﬁrst use of DCPPOs to model recursive phenomena is due to Scott

G.Cruttwell, J.Gallagher, D.Pronk

301

[41] in giving models of the untyped λ-calculus. DCPPOs are used in the semantics of the functional
programming language PCF in [40]. Abstract DCPPO-enriched categories of partial maps were used in
modelling the semantics of the functional programming language FPC in [27]. The DCPPO structure
on homsets of SmoothP was used in [2] to provide a semantics of SDPL. The approach taken here
generalizes [2] to an arbitrary basic reverse differential join restriction category, highlights the structural
aspects of the interpretation, and uses the axioms of such a category to derive some simpliﬁcations to
the operational behaviour. A connection of ω-CPPOs and restriction categories was introduced using the
delay monad in [44].

Deﬁnition 4.5. Let (D, ≤) be a partial order. A subset A ⊆ D is directed if A is nonempty and any two
elements f , g ∈ A have an upper bound in A; i.e., there is an h ∈ A with f , g ≤ h. A partial order (D, ≤)
is a directed complete partial order if every directed subset A has a supremum written
a∈A a ∈ D. A
directed complete partial order is pointed (DCPPO) if there is a supremum for the empty set, that is a
minimal element /0 ≤ d for all d ∈ D.

W

By a morphism of DCPPOs (P, ≤)

−−→ (Q, ≤) we mean a function g on the underlying sets that
is monotone and preserves suprema. We observe minimally that the category of DCPPOs is Cartesian
closed.

g

Lemma 4.6. [42] Let (D, ≤) be a DCPPO. Then

1. Every morphism D

g

−−→ D has a least ﬁxed point; i.e. a u ∈ D such that g(u) = u.

2. For any other DCPPO (P, ≤) every morphism P × D

g

−−→ P has a parametrized ﬁxed point; i.e., a

P u

−−→ D such that

u

P

h1,ui

D

g

P × D

In other words, for each x ∈ P, u(x) is a ﬁxed point of g(x, ). This parametrized ﬁxed point is often
denoted µy.g( , y) and as the ﬁxed point of g(x, ) by µy.g(x, y).

Join restriction categories are DCPPO enriched:

Proposition 4.7. Let X be a restriction category. Then with respect to the order enrichment of restriction
categories:

1. X is a join restriction category then the enrichment lies in DCPPOs.

2. If X has joins and restriction products then those products are DCPPO enriched products; i.e.,
X(A, B × C) ≃ X(A, B) × X(A,C) qua an isomorphism of DCPPOs. Moreover, the “contraction
operator”

X(A, B)

∆A−−→ X(A, A × B)

f

h1, f i

that sends A

−−−−→ A × B is a morphism of DCPPOs.
3. If X has joins and is a Cartesian left additive restriction category, then the addition on homsets

−−→ B to A

X(A, B) × X(A, B)

+
−−→ X(A, B)

is a morphism of DCPPOs.

302

Categorical semantics ofasimple differential programming language

4. If X is a reverse differential join restriction category then the operation of reverse differentiation

X(A, B)

R[ ]
−−−→ X(A × B, A)

is a morphism of DCPPOs.

Parts 2–4 of Proposition 4.7 implies that certain operations we will need to form from monotone and

join preserving maps will again be monotone and join preserving.

To give the categorical semantics of SDPL+, we must extend the interpretation developed in section
4.2. To begin we ﬁrst give the interpretation of function contexts. The idea being that a free function
symbol could be any map of the correct type, the interpretation of function contexts is given as a product
of homsets.

/0

:= 1

Φ, f : A −→ B

:=

Φ

J

K

J

K

J

K

The interpretation of a term in context Γ ⊢ m : B constructed a map
the maps we build now depend on the morphism from X(
B
is, the interpretation is now a function
J

A

K

J

,

,

)

B

A

K
K
J
JmK
−−−→

× X(
J
. With function contexts,
Γ
) to ﬁll in the call to a function. That
J
K

B

J

K

K

Φ

J

K

JΦ|Γ ⊢ m : BK
−−−−−−−−→ X(
J

Γ

,

B

)

K

J

K

Φ

. We write

Now, we are building a function and to do so it sufﬁces to build a map in X(
J

) for each
m
element φ ∈
at φ. The construction is by induction and for the
terms from SDPL, the construction is exactly the same with the addition of a φ subscript decorating the
K
K
m
φ. However, we can build
terms appropriately. For example,
φ
the interpretation entirely using external structure by induction as well.
K
J
K

φ for the value of
K

Φ|Γ ⊢ let x = m in n

φ :=
K

For example, interpretation of let x = m in n may be given using the “contraction operator”, and this

E J

1,

m

B

D

Γ

n

J

J

J

J

K

J

K

,

construction is element free.

hJmK,JnKi

JΦK

X(JΓK, JAK) × X(JΓK × JAK, JBK)

∆Γ ×1

X(JΓK, JΓK × JAK) × X(JΓK × JAK, JBK)

Jletx=m in nK

·

X(JΓK, JBK)

We will leave it to the reader to construct the interpretation of v.rd(x.m)(a) using a similar idea, as well
R[ ]
−−−→ X(A × B, A).
as the reverse differential operator X(A, B)
For SDPL+, we extend this to the two new terms. Given a function context Φ = ( f1, . . . , fn) then
we have that φ = (φ1, . . . ,φn) are all maps in X: if fi : Ai −→ Bi then φi :
Bi
.
for any φ ∈
We will write φ( fi) to denote φi. We also make use of the “no-free-variable” assumption for recursive
K
J
deﬁnitions; that is, in the type formation rule for recursive deﬁnitions letrec f (x) := m in n, m must
have at most a unique free variable, and it must be x.

Ai
J

−→

Φ

J

K

K

Fun-Call:

Φ, f : A −→ B|Γ ⊢ f (m) : B
J

JmKφ−−−−→
Rec-Def: First note that if we just translate a simple recursive function letrec f (x) := m, we see that x
is a free variable and f is a free function variable in m. That is, we have f : A −→ B|x : A ⊢ m : A.
Then note that the interpretation we are developing would interpret m as a function

φ :=
K

φ( f )
−−−→

B

A

Γ

J

J

K

K

K

J

J
This is exactly the sort of underlined function we looked at earlier: it takes each h :
in
X and uses it by the above translation of function calls, any where that f was used in m. Then by

−→

A

B

J

K

K

K

K

J

K

J

K

X(
J

A

,

B

)

A

,

B

)

JmK
−−−→ X(
J

G.Cruttwell, J.Gallagher, D.Pronk

303

Lemma 4.6, we may take its ﬁxed point, µ. We then get a map
µ = µ and
is the least deﬁned such map, giving us the interpretation of the recursive function, and we would
K
J
= µ. More generally, in m the unique variable condition only applies to
write
ordinary variables, but m could have multiple function variables. Then if we translate Φ, f : A
−→ B|x : A ⊢ m : B we get a map

letrec f (x) := m

such that

J

J

K

K

K

J

m

A

B

µ
−−→

J
K
We may then apply the second part of Lemma 4.6 and obtain a parametrized ﬁxed point

K

K

K

J

K

J

Φ

× X(
J

A

,

B

)

A

,

B

)

JmK
−−−→ X(
J

K
Likewise if we translate Φ, f : A −→ B|Γ ⊢ n : C, we get a map

K

J

J

K

Φ

µf .JmK( , f )
−−−−−−−→ X(
J

A

,

B

)

K
Then, ﬁnally, the interpretation of letrec f (x) := m in n is deﬁned by the following diagram:

J

K

K

J

K

J

K

Φ

× X(
J

A

,

B

)

Γ

,

C

)

JnK
−−−→ X(
J

h1,µf .JmK( , f )i

Φ

J

K

Jletrec f (x):=m in nK

Φ

J

K

,

A

× X(
J
K
JnK

B

)

J

K

We may also deﬁne it componentwise as

X(
J

Γ

,

C

)

K

J

K

letrec f (x) := m in n
φ :=
K
J

n

J

K

(φ,µf .JmK(φ, f ))

,

B

Note that the above deﬁnition is only well-deﬁned if we can prove that the interpretation
Γ

JmK
−−−→
) always yields a monotone and join preserving function between the DCPPOs, so that in the
K

X(
last step the use of Lemma 4.6 is justiﬁed.
J
Proposition 4.8. Let X be a basic reverse differential join restriction category, with a speciﬁed inter-
pretation structure for SDPL+. Then the interpretation of terms in context is always a monotone, join
preserving function between the DCPPOs. In particular, the construction is well-deﬁned.

Φ

K

J

J

K

4.5 Operational semantics

The operational semantics used by [2] deﬁned a sublanguage of the raw terms called trace terms. These
are generated by the following grammar:

tr := x | r (r ∈ R) | op(tr) | let x = m in n | ∗ | (tr, tr) | fst(tr) | snd(tr)

Abadi and Plotkin also deﬁned a sublanguage of trace terms called values.

v := x | r (r ∈ R) | ∗ | (v, v)

vbool := true | false

304

Categorical semantics ofasimple differential programming language

The operational semantics of a program then consists of two mutually inductively deﬁned reductions:
symbolic evaluation and ordinary evaluation – the former yields a trace term and the latter yields a
value. Then the main idea is that to evaluate a term, when you hit a reverse differential, v.rd(x. f )(a),
you evaluate f symbolically, just enough to remove control structures and derivatives giving a trace
term. And then this trace term is differentiated symbolically, yielding a trace term, and the evaluation
continues.

Note that deﬁning symbolic reverse differentiation does not require any evaluation functions. How-
ever, we do at this point require, as [2] did, that for each function symbol op ∈ Σ(T,U ) there is an
associated a function symbol opR ∈ Σ(T ×U, T ). The idea is that opR is the reverse derivative of op. We
will write v.opR(a) as notation for opR(a, v). Then deﬁne symbolic reverse differentiation v.R(x. f )(a)
by induction over trace terms f and where v and a are values:

w.R(x.y)(a) =

w x = y
x 6= y
0
(
w.R(x.r)(a) = 0

r ∈ R

w.R(x.m + n)(a) = w.R(x.m)(a) + w.R(x.n)(a)
w.R(x.op(m))(a) = let x = a,t = w.opR(m) int.rd(m)(a)

t fresh

w.R(x.let y = d in e)(a) = let x = a, y = d in w.rd(x.e)(a)

+ (lett = w.rd(y.e)(y) in t.rd(x.d)(a))

t fresh

w.R(∗) = 0

w.R(x.(u, v))(a) = let (y, z) = w in y.R(x.u)(a) + z.R(x.v)(a)
w.R(x.fst(m))(a) = let x = a,t = m in (w.0).R(x.m)(a)
w.R(x.snd(m))(a) = let x = a,t = m in (0, w).R(x.m)(a)

t fresh

t fresh

The let term is also the chain rule but for differentiating with respect to the two variable function Γ, x, y ⊢
n, so that we get the usual rule ∂n/∂t = ∂n/∂x · ∂x/∂t + ∂n/∂y · ∂y/∂t appropriately reversed.

Also, for the projection rule, one might have expected just (w, 0).R(m)(a). Under interpretation we
certainly get a term of the form R[aπ0] = ( a ×ι0)R[a]. However by [RD.8], ( a × 1)R[a] = R[a]. We will
see below that if our evaluation satisﬁes a certain property, then the simpler translation is warranted.

Then as long as our interpretation always sends opR to the reverse derivative of op, then symbolic

and formal reverse differentiation agree under interpretation.
Proposition 4.9 (Symbolic differentiation correctness). Suppose X is a basic reverse differential join
restriction category, and suppose that we have a ﬁxed interpretation of SDPL into X for which
=
R[
J

] then for all values a, v and for all traced terms m

opR

op

J

K

K

v.rd(x.m)(a)

=

v.R(x.m)(a)

We have an analogous proposition for the interpretation of all SDPL+.

J

K

J

K

Proposition 4.10 (Symbolic differentiation correctness extended). Suppose X is a basic reverse differ-
ential join restriction category, and that we have a ﬁxed interpretation of SDPL+ into X for which
opR

], then for all values a, v and for all traced terms m:

op

= R[
J

J

K

K

v.rd(x.m)(a)
J

φ =
K

J

v.R(x.m)(a)

φ
K

G.Cruttwell, J.Gallagher, D.Pronk

305

We then deﬁne the operational semantics of SDPL exactly as done by Abadi and Plotkin [2]: an

operational structure is given by (ev, bev, R) where

evT,U : Σ(T,U ) × vT −→ vU

bevT : Pred(T ) × vT −→ valbool

are partial functions. We denote closed value terms v that have type Y as vY and the set of closed vbool as
valbool (these sets are precisely those that require formation in an empty context ⊢ m : A and ⊢ b). Further

R : Σ(T,U ) −→ Σ(T ×U, T )

op 7→ opR

With these three pieces one may deﬁne ordinary reduction ⇒ from terms to values and symbolic reduc-
tion   from terms to trace terms by induction; see [2] for details. For SDPL these reduction relations
are formulated with respect to a value environment: this is a mapping of variable names to closed value
terms. For SDPL+ we also require a function environment: this is a mapping ϕ of function names to
closures. A closure is a tuple (ϕ, f , x, m) where m has at most the free ordinary variable x, and addition-
ally, all the free function variables in m except f are in the domain of ϕ. The idea is that closures are
created when evaluating letrec f (x) := m in n; if our current function environment is ϕ we extend it
with (ϕ, f , x, m) and continue evaluating n – this way if n calls f then the deﬁnition of f can be looked
up in the function environment, and any symbol that the body of f requires to operate will be there too.

5 Denotational Semantics

An interpretation structure (A ∈ X0, (1
pretation structure when

ar−−→ A)r∈R,

,

T ,

F ) is a differentially denotational inter-

J K

J K

J K

1. For all closed value terms of v we have that 1

2. For all op ∈ Σ we have R[
J

3. For all closed value terms v ∈ vA we have

op

] =

opR

;

K

K

J

JvK
−−−→

A

is a total point of

A

;

J

K

J

K

JvK

1

A

JvK

1

A

Jev(op,v)K

J

K
JopK

B

Jbev(pred,v)KH

J

K
JopKH

1

where H is either T or F. In particular, both sides may be undeﬁned, but they must be undeﬁned
simultaneously.

J

K

The idea behind showing that a denotational semantics captures a language’s operational semantics
. However, the operational semantics for SDPL and SDPL+ is deﬁned
is that if m ⇒ v then
with respect to value and function environments, and we have two operational relations. Interpreting a
term m with free variables x1, . . . , xn in a value environment {xi := vi}1≤i≤ is straightforward: since each
JmK
vi is a closed term, ﬁrst interpret m as above:
Γ
−−−→

, and then precompose with the point of

=

m

B

Γ

v

K

J

K

J

hJviKii≤n
−−−−−−→

Γ

given by 1
Lemma 5.1. The interpretation of terms of SDPL+ extends to allow the construction of an element of
Φ

for each function environment ϕ whose domain is Φ.

. Next we need the following lemma:

J

K

J

K

J

K

J

K

J

K

306

Categorical semantics ofasimple differential programming language

Note that for any trace term c it always fully evaluates. It requires no function context because it has
no function symbols, and we have that for any value environment ρ, c ⇒ v for some closed value term v.
The goal is then to prove the following theorem by mutual induction: for any term m, any value

environment ρ, and function environment ϕ, we have that if m   c then

6 Potential operational improvements

m

=

c

=

v

.

J

K

J

K

J

K

In this section we describe additional properties our categorical semantics has that may lead to a more
reﬁned operational semantics.

The compatibility between differentiation and restriction: [RD.8,9] state essentially that the deﬁned-
ness of the reverse derivative of a term is completely determined by the term itself. This is relevant to
a more efﬁcient semantics: the operational semantics used here has the property that when taking the
reverse derivative over looping or recursive constructs, we ﬁrst build a trace term, which turns out to
be a (long) series of let expressions that describe the evolution of the state of the computation. We
then symbolically differentiate these let expressions which always results in the creation of a sum of
two expressions for each such let expression – and the number of let expressions created by recursion or
looping is the number of times that the function recursed or the number of times the loop ran. Thus we
quickly get wide trees of sums of symbolic terms that need to be evaluated. However, at each step of this
process, one of these terms is of the form v.rd(x.m)(a) where x does not occur freely in m, and hence can
be proven to always evaluate to 0 if it evaluates to anything. Our semantics has the following property

Lemma 6.1. For any term m in which x does not occur

v.rd(x.m)(a)

= h1, h

a

,

v

ii

m

0

And moreover, the let expressions that get zeroed out have all their subterms occuring in the term

J

K

J

K

J

K

J

K

that does not get zeroed out. We then have the following lemma

Lemma 6.2. If we added the rule

x 6∈ fv(e) ⇒ w.R(x.let y = d in e)(a) := let x = a, y = d,t = w.rd(y.e)(y) in t.rd(x.d)(a)

Then Propositions 4.9 and 4.10 would still hold.

This gives an operational semantics where differentiating over looping constructs does not have a

branching blowup, and hence experiences an exponential speedup.

Reverse differential restriction categories, as we have seen earlier, allow forming a forward derivative
from the reverse derivative. They also allow forming a reverse derivative from that forward derivative.
In a reverse differential restriction category, [RD.6] is equivalent to the requirement that the process of
going from a reverse derivative to a forward derivative and then back to a reverse derivative gives exactly
the starting reverse derivative.

Lemma 6.3. For any map from A × B
forward derivative as D[ f ] := R[ f ]†[A]. Then [RD.6] is equivalent to requiring that D[ f ]†[A] = R[ f ].

−−−−−−−−−−−−−→ B. We always get a

−−→ C deﬁne a map A ×C

f

f †[A] := (ι0 × 1)R[ f ]π1

This kind of coherence for deﬁning forward derivatives from their reverse could be useful in using

the forward derivative and then converting back via daggering the result.

G.Cruttwell, J.Gallagher, D.Pronk

307

Lemma 6.4. For any operational symbol op if the evaluation function used by the operational semantics
satisﬁes

eval(snd(opRRR, (((a, 0), 0), (0, b)))) = eval(opR, (a, b))
then this can be modelled in any reverse differential restriction category. Moreover, for every term m we
have

rd(x.rd(y.rd(z.m)(a).y)(b).x)(c).w

=

b

c

rd(z.m)(a).w

J

Crucially for the above, we require [RD.6].
An aspect of forward differentiation that is modelled in our semantics is that differentiating a differ-
ential with respect to its “direction” is just substitution. That is fd(x.fd(y.m)(a).x)(b).v = fd(y.m)(a).v
is modelled. This uses [RD.6]. More generally, we can modify the type system slightly to keep track of
the arguments that a term is differentiated to by introducing another context, which we call a linearity
context. Then the typing judgment for the reverse differential term would have two forms:

K

J

K J

K J

K

Γ, x : A|∆ ⊢ m : B
Γ, a : A|∆, v : B ⊢ rdx.m(a).v : A

a, v fresh

Γ|∆, x : A ⊢ m : B
Γ, a : A|∆, v : B ⊢ rdx.m(a).v : A

a, v fresh

And if we are forward differentiating with respect to a variable from the linearity context: i.e., if we form
the forward derivative of a term with respect to a variable from the linearity context; i.e., if v was in the
linearity context of a term m and we form fd(v.m)(a).w), then operational reduction

fd(v.m)(a).w   let v = w in m

is modelled in our semantics. This means that we can completely avoid doing differentiation in some
cases, at the cost of having to carry around more type information. There is a similar version of this
rule for reverse derivatives and it has to do with “colet” expressions. In SDPL we can use the reverse
derivative to create a term that substitutes linearly into the output variable of a term. We could use these
“colet” expressions and allow for speedups of reverse derivatives as well. It might also be interesting to
characterize these constructions in their own right. This approach also allows us to force [RD.6] into the
operational semantics.

The axiom [RD.7], dealing with the symmetry of mixed partial derivatives, may also have a role to
play in simplifying the operational semantics. Some machine-learning algorithms use the Hessian of the
error function to optimize backpropagation itself, allowing for both more efﬁcient and effective training
(for one example, see [30, 43]). These second derivatives are expected to satisfy a higher dimensional
analog of the chain rule. In fact one might expect in general higher analogs of the chain rule to hold,
which are sometimes called the Faa di Bruno formulae for higher chain rules on terms of the form
∂n( f g). These expected formulae will all hold in our semantics due to a result that shows [CD.6,7] are
equivalent to having all the Faa di Bruno formulae [19]. These higher chain rule expansions can be used
to determine a slightly different operational semantics for rd(x.m)(a).v expressions, where the chain rule
is maximally expanded ﬁrst, and linearity reductions occur, and then symbolic differentiation is used.
While it is unclear if this is more efﬁcient, it would make things simpler, as it would guarantee that the
operational semantics captured the higher chain rule formulae without having to make a requirement of
the evaluation function on opRRR.

References

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore,

308

Categorical semantics ofasimple differential programming language

Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng. Tensorﬂow: a system for large-scale machine learning. 12th USENIX Symposium on
Operating Systems Design and Implementation(OSDI 16), pages 265 – 283, 2016.

[2] Martin Abadi and Gordon D. Plotkin. A simple differentiable programming language. Proceedings of the

ACM on Programming Languages, 4:38:1 – 38:28, 2019. doi:10.1145/3371106.

[3] Jiˇr´ı Ad´amek, Stefan Milius, and Jiˇr´ı Velebil. Elgot algebras. Electronic Notes Theoretical Computer Science,

155:87–109, 2006. doi:10.1016/j.entcs.2005.11.053.

[4] Atilim G¨unes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Auto-
matic differentiation in machine learning: a survey. Journal of Machine Learning Research, 18(153):1 – 43,
2018.

[5] James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A cpu and gpu math ex-
pression compiler. Proceedings of the Python for scientiﬁc computing conference (SciPy), 4, 2010.
doi:10.25080/Majora-92bf1922-003.

[6] W. Bertram, H. Gl¨ockner, and K.-H. Neeb. Differential calculus over general base ﬁelds and rings. Exposi-

tiones Mathematicae, 22(3):213 – 282, 2004. doi:10.1016/s0723-0869(04)80006-9.
- EATCS

[7] S. Bloom and Z. Esik.

theories.

Iteration

Springer

Series,

1993.

doi:10.1007/978-3-642-78034-9_7.

[8] R. Blute, R. Cockett, and R. Seely. Cartesian Differential Categories. Theory and Applications of Categories,

22:622–672, 2009.

[9] R.F. Blute, J.R.B. Cockett, and R.A.G. Seely. Differential categories. Mathematical structures in computer

science, 16(6):1049–1083, 2006.

[10] Alo¨ıs Brunel, Damiano Mazza, and Michele Pagani. Backpropagation in the simply typed lambda-calculus
with linear negation. Proc. ACM Program. Lang., 4(POPL), December 2019. doi:10.1145/3371132.
[11] J.R.B. Cockett and G.S.H. Cruttwell. Differential bundles and ﬁbrations for tangent categories. Cahiers de

Topologie et Geom´etrie Diff´erentielle Cat´egoriques, LIX(1):10–92, 2018.

[12] J.R.B. Cockett, G.S.H. Cruttwell, and J.D. Gallagher. Differential restriction categories. Theory and Appli-

cations of Categories, 25(21):537–613, 2011.

[13] J.R.B. Cockett, P.J.W. Hofstra, and P. Hrubeˇs. Total maps of turing categories. Electronic Notes in The-
oretical Computer Science, 308:129–146, 2014. Proceedings of the 30th Conference on the Mathematical
Foundations of Programming Semantics (MFPS XXX). doi:10.1016/j.entcs.2014.10.008.

[14] J.R.B. Cockett and Stephen Lack. Restriction categories I: categories of partial maps. Theoretical Computer

Science, 270(1):223 – 259, 2002. doi:10.1016/S0304-3975(00)00382-0.

[15] R. Cockett and G. Cruttwell. Differential structure, tangent structure, and SDG. Applied Categorical Struc-

tures, 22:331–417, 2014. doi:10.1007/s10485-013-9312-0.

[16] R. Cockett, G. Cruttwell, J. Gallagher, J-S. Lemay, B. MacAdam, G. Plotkin, and D. Pronk. Reverse deriva-

tive categories. arxiv:1910.07065, (18):1–25, 2019.

[17] R. Cockett and S. Lack. Restriction categories III: colimits, partial limits and extensivity. Mathematical

Structures in Computer Science, 17(4):775–817, 2007. doi:10.1017/S0960129507006056.

[18] R. Cockett and E. Manes. Boolean and classical restriction categories. Mathematical Structures in Computer

Science, 19(2):357–416, 2009. doi:10.1017/s0960129509007543.

[19] R. Cockett and R. Seely. The Fa´a Di Bruno Construction. Theory and Applications of Categories, 25:294–

425, 2011.

[20] Robin Cockett, Joaqu´ın D´ıaz-Bo¨ıls, Jonathan Gallagher, and Pavel Hrubeˇs. Timed sets, functional complex-
ity, and computability. Electronic Notes in Theoretical Computer Science, 286:117–137, 2012. Proceed-
ings of the 28th Conference on the Mathematical Foundations of Programming Semantics (MFPS XXVIII).
doi:10.1016/j.entcs.2012.08.009.

G.Cruttwell, J.Gallagher, D.Pronk

309

[21] Robin Cockett, Xiuzhan Guo, and Pieter Hofstra. Range categories II: Towards regularity. Theory and

applications of categories, 26(18):453–500, 2012.

[22] J. H. Conway. Regular algebra and ﬁnite machines. Chapman and Hall Mathematics Series, 1971.

[23] E. Dubuc. Sur les mod´eles de la g´eom´etrie diff´erentielle synth´etique. Cahiers de Topologie G´eom´etrie

Diff´erentielle Cat´egoriques, 20(1):231–279, 1979.

[24] E. Dubuc and A. Kock. On 1-Form Classiﬁers. Communications in Algebra, 12(12):1471–1531, 1984.

doi:10.1080/00927878408823064.

[25] Calvin C. Elgot. Monadic Computation and Iterative Algebraic Theories, pages 179–234. Springer New

York, New York, NY, 1982. doi:10.1007/978-1-4613-8177-8_6.

[26] Conal Elliott. The simple essence of automatic differentiation. Proceedings of the ACM on Programming

Languages, 2(ICFP):70, 2018. doi:10.1145/3236765.

[27] M. P. Fiore and G. D. Plotkin. An axiomatisation of computationally adequate domain theoretic models of
fpc. In Proceedings Ninth Annual IEEE Symposium on Logic in Computer Science, pages 92–102, 1994.
doi:10.1109/lics.1994.316083.

[28] J. Gallagher. The differential lambda-calculus: syntax and semantics for differential geometry. PhD thesis,

University of Calgary, 2009.

[29] J. Gallagher. What is a differential partial combinatory algebra? Master’s thesis, University of Calgary, 2011.

[30] B. Ghorbani, S. Krishnan, and Y. Xiao. An investigation into neural net optimization via hessian eigenvalue

density. Proceedings of Machine Learning Research, 97, 2019.

[31] Brett Giles. An investigation of some theoretical aspects of reversible computing. PhD thesis, University of

Calgary, 2014.

[32] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

[33] Marco Grandis. Cohesive categories and manifolds. Annali di Matematica Pura ed Applicata, 157(1):199–

244, 1990. doi:10.1007/bf01765319.

[34] A. Kock.

Synthetic

differential

geometry.

Cambridge University

Press,

1981.

doi:10.1017/cbo9780511550812.

[35] R. Lavendhomme. Basic Concepts of Synthetic Differential Geometry. Kluwer Texts in Mathematical Sci-

ences. Kluwer Academic Publishers, 1996. doi:10.1007/978-1-4757-4588-7.

[36] Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Autograd: effortless gradients in numpy. ICML

2015 AutoML Workshop, 238, 2015.

[37] C. Nester. Turing categories and realizability. Master’s thesis, University of Calgary, 2017.

[38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, and Luca et al Antiga. Pytorch: An imperative style, high-performance
deep learning library. Advances in Neural Information Processing Systems, pages 8024 – 8035, 2019.

[39] Barak A. Pearlmutter and Jeffrey Mark Siskind.

Reverse-mode ad in a functional framework:
ACM Trans. Program. Lang. Syst., 30(2), March 2008.

Lambda the ultimate backpropagator.
doi:10.1145/1330017.1330018.

[40] G.D. Plotkin. LCF considered as a programming language. Theoretical Computer Science, 5(3):223 – 255,

1977. doi:10.1016/0304-3975(77)90044-5.

[41] Dana S. Scott. A type-theoretical alternative to ISWIM, CUCH, OWHY. Theoretical Computer Science,

121(1–2):411–440, 1993. doi:10.1016/0304-3975(93)90095-b.

[42] Alfred Tarski. A lattice-theoretical ﬁxpoint theorem and its applications. Paciﬁc Journal of Mathematics,

5(2):285 – 309, 1955. doi:10.2140/pjm.1955.5.285.

[43] L. Tzu-Mao. Differentiable Visual Computing. PhD thesis, Massachusetts Institute of Technology, 2019.

310

Categorical semantics ofasimple differential programming language

[44] Tarmo Uustalu and Niccol`o Veltri.

In Theoret-
ical Aspects of Computing – ICTAC 2017, pages 32–50. Springer International Publishing, 2017.
doi:10.1007/978-3-319-67729-3_3.

The delay monad and restriction categories.

[45] Bart van Merrienboer, Dan Moldovan, and Alexander B. Wiltschko. Tangent: automatic differentiation using
source-code transformation for dynamically typed array programming. Advances in Neural Information
Processing Systems 31:Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
3–8 December 2018, Montr´eal, Canada, pages 6259 – 6268, 2018.

[46] Fei Wang, Daniel Zheng, James Decker, Xilun Wu, Gr´egory M. Essertel, and Tiark Rompf. Demystifying dif-
ferentiable programming: Shift/reset the penultimate backpropagator. Proc. ACM Program. Lang., 3(ICFP),
July 2019. doi:10.1145/3341700.

