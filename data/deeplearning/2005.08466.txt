HaoCL: Harnessing Large-scale Heterogeneous Processors Made Easy

Yao Chen∗, Xin Long†, Jiong He‡, Yuhang Chen∗, Hongshi Tan§,
Zhenxiang Zhang†, Marianne Winslett∗,¶, Deming Chen∗,¶
∗Advanced Digital Sciences Center, †Alibaba Group, ‡Institute of High Performance Computing, A*Star
§National University of Singapore, ¶University of Illinois at Urbana-Champaign

0
2
0
2

y
a
M
8
1

]

C
D
.
s
c
[

1
v
6
6
4
8
0
.
5
0
0
2
:
v
i
X
r
a

Abstract—The pervasive adoption of Deep Learning (DL)
and Graph Processing (GP) makes it a de facto requirement
to build large-scale clusters of heterogeneous accelerators
including GPUs and FPGAs. The OpenCL programming
framework can be used on the individual nodes of such
clusters but is not intended for deployment in a distributed
manner. Fortunately, the original OpenCL semantics natu-
rally ﬁt into the programming environment of heterogeneous
clusters. In this paper, we propose a heterogeneity-aware
OpenCL-like (HaoCL) programming framework to facilitate
the programming of a wide range of scientiﬁc applications
including DL and GP workloads on large-scale heterogeneous
clusters. With HaoCL, existing applications can be directly
deployed on heterogeneous clusters without any modiﬁcations
to the original OpenCL source code and without awareness of
the underlying hardware topologies and conﬁgurations. Our
experiments show that HaoCL imposes a negligible overhead in
a distributed environment, and provides near-liner speedups on
standard benchmarks when computation or data size exceeds
the capacity of a single node. The system design and the
evaluations are presented in this demo paper.

Keywords-heterogeneous

cluster, distributed computing,

OpenCL, machine learning, deep learning

I. INTRODUCTION

With the increasing volume of data to be processed and
higher complexity of workloads in modern academic and
industrial applications, large scale heterogeneous computing
clusters have become an attractive solution that offer both
massively parallel computing capability and large storage
capacity by integrating traditional multi-core CPUs, many-
core GPUs and power-efﬁcient FPGAs into High Perfor-
mance Computing (HPC) systems [1]–[3]. Though these
hardware accelerators were originally designed for speciﬁc
tasks, general-purpose programming languages have been
released by their vendors so that complex applications
can be mapped to such hardware by ofﬂoading tasks of
applications partially or entirely. Languages such as CUDA
and OpenCL are powerful tools for programmers to deploy
applications on these accelerators. CUDA is a proprietary
toolchain released by NVIDIA exclusively for NVIDIA
GPUs. In contrast, OpenCL and OpenACC can be used
in a cross-platform manner so that one copy of source
code can run on different OpenCL-compatible devices [3]–
[7]. However, most existing applications still target a single
system equipped with one type of accelerator [3]–[7]. We

*Yao Chen and Jiong He both made equal contributions to this work.

lack of solutions that can provide an efﬁcient and easy-to-use
abstraction for a heterogeneous cluster.

Modern scientiﬁc applications usually consist of a large
number of sub-tasks that have different hardware resource
preferences in order to optimize either performance, power
consumption, or monetary cost. However, power consump-
tion and cost make it impractical to equip each node in a
cluster with all types of accelerators. Ad-hoc assignment
of these sub-tasks to nodes without holistic deployment
information about all accelerators across the cluster in-
evitably lead to serious under-utilization [4]–[7]. This prob-
lem becomes even worse in large-scale cloud systems that
need to serve massive requests from many users simulta-
neously. Though some existing solutions achieve a uniﬁed
abstraction of a heterogeneous cluster [4]–[7], they simply
provide a wrapper of the native OpenCL implementation
which ofﬂoads the tasks to remote devices via networking
components, without trying to optimize resource allocations.
Therefore, they cannot serve modern applications efﬁciently
due to the stringent requirements on performance, power
efﬁciency and usability.
• Performance. The heterogeneity of modern clusters re-
quires the scheduler in the system to be aware of the
device differences. Without a detailed device model, a
scheduler for a heterogeneous cluster will produce sub-
optimal scheduling plans and result in worse performance.
• Power efﬁciency. CPU, GPU and FPGA resources in
large-scale clusters provide different efﬁciency character-
istics for different tasks. For maximum power efﬁciency, a
scheduler requires device model and run-time information
of the tasks on the different devices.

• Usability. Although existing solutions targeting different
computing devices can provide performance beneﬁts, inte-
grating them together efﬁciently still requires great effort.
No existing approach provides both heterogeneity-aware
and performance-oriented scheduling in an automated or
user-guided manner. Instead, they usually adopt a static and
heterogeneity-oblivious mapping between tasks (OpenCL
kernels) and hardware resources. Such optimistic allocation
patterns would result in serious resource under-utilization
in scenarios where the number of users and accelerators is
large in commercial clouds. Besides, FPGAs are moving into
cloud computing as well [1], [2]. However, previously pro-
posed frameworks [4]–[7] only consider CPUs and GPUs.
There are few works on how FPGAs can be utilized to
improve the energy efﬁciency and how a heterogeneity-

 
 
 
 
 
 
Figure 1. Framework overview.

aware scheduling algorithm [1] should be designed with
additional hardware types.

To solve the above questions and meet

the stringent
requirements of modern complex and compute-intensive
applications, we propose a heterogeneity-aware framework
(HaoCL) that can efﬁciently abstract and manage different
types of resources including FPGAs in heterogeneous HPC
clusters in a holistic manner and also reduce the burden
on application designers. In addition, designers can design
and illustrate their own scheduling algorithms and embed
them into HaoCL to achieve their performance objectives.
In summary, HaoCL makes the following contributions.
• A framework that manages different types of accelerators
in a holistic manner with ﬁne-grained resource scheduling
capability.

• An extensible run-time resource monitoring and schedul-
ing component that supports both built-in and user cus-
tomized scheduling policies.

• A light-weight and easy-to-extend communication back-
bone built on an optimized asynchronous communication
middleware which is speciﬁcally designed for a large-scale
distributed environment.

• Support for the same application programming interfaces
(APIs) as OpenCL (or OpenCL-like for FPGAs), which
signiﬁcantly reduces the integration and migration over-
head of current applications.

II. RELATED WORKS

VCL [6] allows applications on one hosting-node to trans-
parently utilize cluster-wide devices (CPUs and/or GPUs);
however, the coarse-grained granularity from workload lim-
its the ﬂexibility of it. RCUDA [7] provides a uniﬁed
abstraction of the clusters equipped with GPUs. However,
it only supports the CUDA API as well as NVIDIA GPUs.
It fails to provide a uniﬁed view of other types of accel-
erators such as AMD CPU/GPU and FPGAs. Speciﬁcally,
the abstraction of FPGAs is even more challenging due
to its special computing paradigms. High Level Synthesis
(HLS) with the CUDA and OpenCL as input provides an
efﬁcient approach to implement and optimize applications
on FPGAs [3], [8]–[10], but it still targets single device.

SnuCL [4] and SnuCL-D [5] support OpenCL to achieve
cross-platform function. However, their lack of multi-user
support and very coarse-grained scheduling among work-

loads and resources prohibit the full utilization of the de-
vices. HaaS [1] is a heterogeneous-aware streaming pro-
cessing framework that optimizes multiple runs on tasks to
different devices in the system. The topology level schedul-
ing and the heavy streaming back-end have limited its wide
adoption in modern complex applications.

Complex applications consist of different tasks with dif-
ferent computation patterns, which suit different computa-
tional accelerators. Hence, a ﬁner-granularity description of
the tasks and ﬂexible scheduling of them onto different de-
vices (processor/accelerator) is critical for fully utilizing the
resources in a large-scale heterogeneous cluster. Although
OpenCL provides ﬁne-grained support of task description,
state-of-the-art frameworks do not provide ﬂexible schedul-
ing, and only support limited hardware types, which motivate
our HaoCL work in this study.

III. SYSTEM DESIGN

Our framework wraps native OpenCL APIs so that ex-
isting applications can be deployed on a cluster of het-
erogeneous resources naturally with few modiﬁcations. As
such, we follow the abstract execution model of OpenCL
but enhance it with heterogeneity-aware optimizations.

A. Framework Overview

The overall architecture design of the HaoCL framework
is shown in Fig. 1. It consists of a single host node
and multiple device nodes (CPU, GPU or FPGA). Each
device node contains its own CPU, and the CPU for each
of the GPU and FPGA device node is omitted for easy
understanding. The nodes are connected through Gigabit
Ethernet. The host node executes the host program in an
OpenCL application and is also responsible for the message
packaging and message delivering across the entire cluster.
Note that the FPGA is used as a streaming processor with
different performance characteristics from CPU or GPU.
HaoCL framework is simpliﬁed to consist of only three
major functional components: 1) an OpenCL wrapper
library and task scheduling component, 2) a communi-
cation backbone and 3) a node management process on
each child node. The task scheduling component is aware
of the hardware architecture of the individual nodes and
delivers the kernel tasks with consideration of the run-time
information of the kernel on the nodes.

CPU    GPU    SharedMem FPGA    Interconnect network (communication backbone) CPU    GPU    SharedMem FPGA    App. A App. B User A User B Host Node management Process N. M. Process N. M. Process A B D C E F A Task Graph B F D E Wrapper lib Scheduler ICD driver Host driver C B. OpenCL-compatible driver

HaoCL implements a full set of standard OpenCL API
calls to make it compatible with existing applications imple-
mented in OpenCL. Speciﬁcally, it consists of an OpenCL
wrapper lib and an extendable scheduling component.

OpenCL Wrapper Lib: The OpenCL Wrapper Lib
adopts identical names as standard OpenCL APIs to main-
tain good usability and portability. For each OpenCL API
call, the same API in HaoCL creates a message package
that contains the information of the function’s name and
arguments. Besides, the wrapper lib also creates data pack-
ages containing all data in OpenCL buffers that have been
called in this API and sends it to the speciﬁed compute
node through the communication backbone. The device
nodes unpack the message and execute it with their own
environment. To support different hardware platforms with
their own drivers, Installable Client Driver (ICD) is provided
as the common entry point
to those speciﬁc drivers on
device nodes after intercepting those calls in applications
implemented with the OpenCL Wrapper Lib. We extend the
original ICD to be compatible with the front-end wrapper
layer and the communication backbone for remote API calls
forwarding so that each call to the standard OpenCL APIs
can be executed in the form of implementation according to
the remote devices and vendor drivers.

Extendable task scheduling component: The task
scheduling component on the host node is aware of the run-
time information of the accelerator nodes. In the current
version, it delivers the kernel tasks to device nodes based on
users’ instructions. However, it is designed in an extendable
manner so that it can be upgraded to an automatic scheduler
with the runtime proﬁling information from the cluster to
enable more accurate heterogeneity-aware task scheduling.

C. Communication Backbone

The communication backbone is built as the network
among host and devices for data transfers. The design of it
imposes great challenges due to the requirement of ﬂexibility
and efﬁciency as well as the support of complex data transfer
patterns. To address these challenges, we leverage the Boost
library to take advantages of its strengths [11]. 1) Its stability
and scalability ensure the correctness of the transmission
of high volume data. 2) Its ﬂexibility of extension enables
adding new nodes easily. 3) The asynchronous input and
output feature enables an efﬁcient unblocking data transfer
style which is necessary for a distributed cluster. 4) More-
over, it can be more easily utilized to implement complex
inter-node data transfer schemes in the OpenCL API.

When the Node Management Process (NMP) in the device
node is created, it obtains the IP address and port from
the user’s input, then uses Boost’s acceptor structure as
a message and data listener. Once the listener is created,
it listens to the port asynchronously. When messages/data

Table I
BENCHMARK APPLICATIONS.

Description

App.
MatrixMul Matrix multiplication
CFD
kNN
BFS
SpMV

Unstructured grid ﬁnite volume solver
Finds k-nearest neighbors in unstructured data set
Traverses all the connected components in a graph
Sparse matrix-vector multiplication in CSR format

In. size
760MB
800MB
100MB
240MB
1.1GB

comes, it creates a thread to read and unpacking the incom-
ing message, then starts listening to the port again.

The communication method of the host process is almost
the same as the node process (it also has the message and
data listeners), but it reads the address and port deﬁned in a
system conﬁguration ﬁle and creates a message and a data
listener for each node. Besides, the listener will listen to
the port synchronously. So after the host process sends a
message through the message listener, it will wait for the
response message and then take the next action according
to the type of the message.

The communication backbone takes care of the message
delivering among multiple different accelerator nodes. We
leverage a mapping mechanism to address this. When the
user program calls the clGetDeviceIDs function, the wrapper
lib will create a device ID request message for each compute
node. After the response message from the node process
is received, the backbone obtains the device’s id of each
compute node and records this mapping.

D. Node Manage Process

The daemon process runs on each device (accelerator)
node for the actual execution of OpenCL API calls. It
receives the commands from the workload scheduler along
with additional information such as user ID, device ID,
shared ﬂag (whether a device is shared or not) and the
number of resources from those explicit requests, and parses
them for compilation and execution. Especially, the FPGA
nodes do not have ﬂexible support for task allocation.
So the tasks are pre-built as executable binaries with the
bitstreams to provide customized hardware solution together
with bandwidth optimization [3].

IV. EXPERIMENTS

We select a set of commonly used benchmarks for Ma-
chine Learning (ML) and Graph Processing (GP) for a
comprehensive evaluation of our design. The high demand
for computations and data storage in the ML and GP
applications requires high computing and storage capability
that cannot be provided by a single computing node. HaoCL
is designed to leverage the great computing capability of the
large-scale distributed heterogeneous accelerators to process
such applications.

A. Evaluation Setup

Environmental setup. We conduct our evaluations in the
Alibaba Cloud with the elastic computing nodes. All nodes
are equipped with Intel Xeon E5-2686 CPU. Each GPU node

has an NVIDIA Tesla P4 GPU and each FPGA node has a
Xilinx VU9P FPGA. 16 GPU nodes and 4 FPGA nodes
are involved in our evaluations. All the nodes are connected
through Ethernet. Speciﬁcally, for the heterogeneity evalua-
tion, we construct clusters with different scales consists with
different numbers of different types of devices.

Benchmark setup. We select the representative OpenCL
benchmarks from different sources: Rodinia [12] and
SHOC [13]. The characteristics of the applications and their
input sets are summarized in Table I.

B. End-to-end Improvement

Fig. 2 shows the scalable performance of different appli-
cations towards a single GPU/FPGA or GPU+FPGA node
with a native OpenCL environment. We schedule the tasks
with different numbers and types of nodes, denoted as GPU,
FPGA and GPU + FPGA. Overall, HaoCL could achieve
a linear performance improvement compared to the single
node implementation of the benchmarks when the number
of nodes scales. However, the performance improvement
also depends on the computation pattern and communication
characteristics of the benchmarks. We also compare our
HaoCL framework to the SnuCL-D framework [5].

Our framework shows better performance improvement
when the number of nodes scales for a wide range of
benchmarks. In addition, HaoCL supports the scheduling of
clusters built with hybrid GPU and FPGA nodes and still
achieves linear performance improvement when the node
number increases. Note CFD cannot be implemented on
SnuCL-D without signiﬁcant change.

C. Heterogeneity Evaluation

MM and SpMv are chosen as benchmarks for the het-
erogeneity evaluation because of their data transmission
complexity and the different requirement of efﬁciency. Their
performance is normalized to a single node with FPGA
or GPU, as shown in Fig. 2. Noted that the MatrixMul
kernels on the different devices are kept the same, just
processing different data portion; while the different kernels
(stages) of the SpMv are allocated to different devices, i.e.,
the kernel for data partition is allocated on the GPUs and
computation on the FPGAs. The system performance is
linearly scaled with the number of the device nodes. Overall,
the heterogeneity of the devices in the cluster is well utilized
by leveraging the computation capacity of the nodes.

D. Breakdown Analysis

Matrix Multiplication is conducted for a breakdown anal-
ysis of the HaoCL framework. Since our node management
process manages all the nodes with the same behavior, so
the CPU, GPU and FPGA node have the same overhead of
communication and local environment setup. The breakdown
analysis of a cluster with different number of GPU devices is
shown in Fig. 3. The major runtime of the HaoCL framework

Figure 2. End-to-end speedup over a single GPU and FPGA.

Figure 3. System breakdown analysis with Matrix Multiplication.

includes system initialization time, data creation time, data
transfer time and compute time. Note that comparing to the
later three, the system initialization time is negligible and
is omitted in the Fig. 3. Although the communication and
data creation time increase when the system and/or data size
scale up, the ratio of them decreases.

ACKNOWLEDGEMENT

This project is supported by Alibaba Innovation Research
(AIR) grant. Besides, this project is also partly supported by
the National Research Foundation, Prime Minister’s Ofﬁce,
Singapore under its Campus for Research Excellence and
Technological Enterprise (CREATE) programme.

REFERENCES
[1] J. He et al., “Haas: Cloud-based real-time data analytics with

heterogeneity-aware scheduling,” in ICDCS, 2018.

[2] Y. Chen et al., “Cloud-DNN: An open framework for mapping

DNN models to cloud FPGAs,” in FPGA, 2019.

[3] X. Chen et al., “On-the-ﬂy parallel data shufﬂing for graph

processing on opencl-based fpgas,” in FPL, 2019.

[4] J. Kim et al., “Snucl: An opencl framework for heterogeneous

cpu/gpu clusters,” in ICS, 2012.

[5] J. Kim et al., “A distributed opencl framework using redun-
dant computation and data replication,” in PLDI, 2016.
[6] A. Barak et al., “A package for opencl based heterogeneous
computing on clusters with many gpu devices,” in CLUSTER
WORKSHOPS, 2010.

[7] J. Duato et al., “An efﬁcient implementation of gpu virtual-
ization in high performance clusters,” in Euro-Par, 2009.
[8] A. Papakonstantinou et al., “FCUDA: Enabling efﬁcient com-
pilation of cuda kernels onto FPGAs,” in IEEE Symposium
on Application Speciﬁc Processors, 2009, pp. 35–42.

[9] D. Chen et al., “xpilot: A platform-based behavioral synthesis

system,” SRC TechCon, vol. 5, 2005.

[10] D. Chen et al., “Lopass: A low-power architectural synthesis
system for fpgas with interconnect estimation and optimiza-
tion,” IEEE Transactions on Very Large Scale Integration
(VLSI) Systems, pp. 564–577, 2010.

[11] Boost C++ Libraries, 2003.

//www.boost.org/

[Online]. Available: https:

[12] S. Che et al., “Rodinia: A benchmark suite for heterogeneous

computing,” in IISWC, 2009.

[13] A. Danalis et al., “The scalable heterogeneous computing

(shoc) benchmark suite,” in GPGPU, 2010.

Local-GPUHaoCL-GPUHaoCL-FPGAHaoCL-HeteroSnuCL02040608010012014016018024924924924924924924910002000400050006000800010000Time(s)MatrixSize(N*N)DataCreateComputeTimeDataTransfer