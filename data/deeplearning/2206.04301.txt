2
2
0
2

l
u
J

4

]

G
L
.
s
c
[

2
v
1
0
3
4
0
.
6
0
2
2
:
v
i
X
r
a

Unveiling Transformers with LEGO: a synthetic reasoning task

Yi Zhang

Arturs Backurs

S´ebastien Bubeck

Ronen Eldan

Suriya Gunasekar

Tal Wagner

Microsoft Research
{zhayi, arturs.backurs, sebubeck, roneneldan, suriya.gunasekar, tal.wagner}@microsoft.com

Abstract

We propose a synthetic task, LEGO (Learning Equality and Group Operations), that encapsulates
the problem of following a chain of reasoning, and we study how the transformer architectures learn this
task. We pay special attention to data eﬀects such as pretraining (on seemingly unrelated NLP tasks)
and dataset composition (e.g., diﬀering chain length at training and test time), as well as architectural
variants such as weight-tied layers or adding convolutional components. We study how the trained models
eventually succeed at the task, and in particular, we are able to understand (to some extent) some of
the attention heads as well as how the information ﬂows in the network. Based on these observations
we propose a hypothesis that here pretraining helps merely due to being a smart initialization rather
than some deep knowledge stored in the network. We also observe that in some data regime the trained
transformer ﬁnds “shortcut” solutions to follow the chain of reasoning, which impedes the model’s ability
to generalize to simple variants of the main task, and moreover we ﬁnd that one can prevent such shortcut
with appropriate architecture modiﬁcation or careful data preparation. Motivated by our ﬁndings, we
begin to explore the task of learning to execute C programs, where a convolutional modiﬁcation to
transformers, namely adding convolutional structures in the key/query/value maps, shows an encouraging
edge. Our code is available here.

1 Introduction

The deep learning revolution is about training large neural networks on vast amount of data. The ﬁrst
ﬁeld transformed by this methodology was computer vision, crucially leveraging the convolutional neural
network architecture [LBD+89, KSH12]. More recently natural language processing was revolutionized by
the transformer architecture [VSP+17]. Transformers are designed to process input represented as “set of
elements” (e.g., the words in a sentence with their positional encoding). This is of course an incredibly generic
assumption, and thus transformers can be applied to a wide variety of tasks, including vision [DBK+21],
reinforcement learning [CLR+21], and protein structure prediction [RMS+21, JEP+21] among other. In fact,
learning with transformers is rapidly becoming the norm in deep learning.

Transformer models display excellent performance on the standard criterion “training error/test error”
(e.g., for masked language prediction or translation). However, what makes them particularly noteworthy,
is that large-scale transformer models seem to exhibit unexpected emergent behaviors, such as basic reason-
ing ability [TDFH+22, BMR+20a, CND+22, DHD+21, RBC+21, HBM+22, SPN+22, ZRG+22, WWS+22,
NAGA+22], excellent ﬁne-tuning performance [HSW+22, TDFH+22, NAGA+22, RBC+21, PHZ+22], or
zero-shot learning [BMR+20a, CND+22, DHD+21, RBC+21, HBM+22, SPN+22, ZRG+22]. Currently, there
is a remarkable community eﬀort towards at-scale experimental investigation of transformers, essentially try-
ing to ﬁnd out what can such models do when they become large enough and are trained on large/diverse
enough datasets. The successes are striking and capture the imagination [BMR+20a, RDN+22]. Yet, for all

1

 
 
 
 
 
 
of these wonders, there is very little understanding of how these models learn, or in fact what do they learn.
Answering such questions in the at-scale experiments is particularly challenging, as one has little control over
the data when hundreds of billions of tokens are harvested from various sources. In this paper we propose to
take a step back, and try to understand how learning occurs and what is being learned in a more controlled
setting that captures the essence of a “reasoning task”.

The beneﬁt of such a controlled setting is that we can try to understand some of the most pressing
questions in learning with transformers, particularly around (i) the architecture and (ii) the importance of
training data. For (i) we probe the role of multiple heads and depth, and we show that we can successfully
understand them in our controlled setting. For (ii) we investigate how much the dataset composition matters,
as well as how pretraining on merely vaguely related tasks makes ﬁne-tuning successful. In turn these insights
can guide our thinking for large-scale experiments, and we give some of the lessons learned below (including
a proposal for a convolutional variant to transformers).

1.1 LEGO: A synthetic reasoning task

Core components of reasoning include the ability to associate concepts, and to manipulate them. We pro-
pose a simple task that captures these two aspects, which we call LEGO (Learning Equality and Group
Operations). In LEGO, the input describes a sequence of variable assignments as well as operations on these
variables by a ﬁxed (mathematical) group. One needs to be able to deal with both long-range assignment
(the same variable appearing in diﬀerent parts of the input should be viewed as a being equal to same quan-
tity), as well as short-range operations (describing what group element is applied to which variable). A key
parameter of an input sequence will be its length, which roughly correspond to the number of sequential
reasoning steps one has to do in order to resolve the value of each variable. We will mostly train with a
ﬁxed sequence length (say 12). We often provide supervision only on part of the sequence (say the ﬁrst 6
variables). We do so in order to test the generalization capabilities from smaller length sequence to longer
length sequence without introducing potential errors due to the positional encoding in transformers.

1.2 Some takeaways

We distinguish classical generalization (i.e., training and test distribution are the same) and out-of-distribution
generalization, which we refer to as simply generalization. In the context of LEGO, this (out-of-distribution)
generalization refers to the setting where we train on shorter sequence lengths (e.g., supervision on only the
ﬁrst 6 variables) and test on a long sequences (e.g., accuracy computed on 12 variables). A summary of our
empirical observations is as follows:

1. First, classical generalization happens reliably for all architectures and data regimes.

2. More interestingly, (out-of-distribution) generalization seems to depend on architectural/data composition
choices. Speciﬁcally, BERT-like models without special data preparation do not generalize to longer
sequences, while other models like ALBERT, or BERT with carefully selected data (such as diverse
sequence lengths, or pre-trained BERT) do generalize.

3. The generalizing models all seem to evolve attention heads dedicated to either association (long-range
identity matching) or manipulation (short-range operations). We provide evidence that pre-trained BERT
(which is pretrained on a seemingly unrelated dataset) generalizes because it has learned such heads,
rather than because it would have “learned to reason” on LEGO task.

4. The non-generalizing models seem to solve the classical generalization problem using shortcut-like solu-
tions, whereby using the speciﬁcity of the group operations they are able to jump to the end of the chain
of reasoning, and then complete the rest of the variables by following the reasoning both from the start
and the end of the chain.

2

We interpret these observations as suggesting the following more general insights:

(i) Classical generalization can be a deceptive metric, as there might be unexpected ways to solve the
problem. This is famously related to the issue of embedding machine learning systems with common
sense reasoning. Namely, we hope that when a ML system solves a task, it does so in “the way humans
do it”, but of course nothing guarantees that this will happen. Our ﬁndings are consistent with the current
methodology of increasing the diversity of the training data, which seem crucial for (out-of-distribution)
generalization.

(ii) ALBERT-like models, where a layer is repeated several times, seem to be an ideal structure for problems
which could be described algorithmically as a “for loop” (as is the case with following a chain of reasoning).
Indeed we ﬁnd that ALBERT generalizes in data regimes where BERT does not, clearly separating these
two architectures.

(iii) The success of pretraining/ﬁne-tuning in vastly diﬀerent tasks might actually come from a “simple” better

initialization, rather than complex knowledge encoded in the pretrained network.

(iv) The interplay between short-range (close-by information in sentence) and long-range (same concept ap-
pearing in diﬀerent places in the sentence) is relevant more broadly than in our synthetic task. We
observe that the networks eﬀectively learn to deal with short-range information by implementing a sort
of convolutional ﬁlter using the positional encoding information. It is natural to consider architectures
that would have such a mechanism implemented in it from the start, motivating us to study a hybrid
convolutional-transformer architecture, which basically replaces the linear operators deﬁning key/query/-
value by convolutional operators.

1.3 Related works

In [ZRKB21], the PVR (Pointer Value Retrieval) task is introduced, with a similar high level goal to ours
in introducing the LEGO task, namely to study how neural networks learn to reason in a controlled setting.
In a PVR task, part of the input indicates another part of the input where a function of potentially varying
complexity has to be computed. Like us, they use distribution shift to investigate how various network
architectures learn this task, and they observe that networks can learn the task at hand (“classical gen-
eralization”) yet fail to generalize to mild distribution shift. They then ask the following questions: “Are
there architectural changes that can enforce better priors and withstand distribution shift? Can novel learn-
ing objectives prevent these adversarial correlations? Progress on these questions holds promise for greater
robustness.” Our study attacks these questions directly in the context of the LEGO task (e.g., ALBERT
versus BERT, and training set composition investigations), and our preliminary results indicate that this
is indeed a fruitful direction to obtain better models in some aspects (e.g., more interpretable). Other ex-
amples of recent synthetic benchmark with a similar philosophy include SCAN (Simpliﬁed version of the
CommAI Navigation) [LB18], CFQ (Compositional Freebase Questions) [KSS+20], and BONGARD-LOGO
[NYM+20].
In SCAN for example, one has to “translate” a command of the form “turn left twice and
jump” into a sequence of actions “LTURN LTURN JUMP” (see [PBBG22] for more recent progress on this
dataset). Again, similarly to the PVR tasks, these works focus on understanding generalization (in these
cases, compositional generalization). Another related line of works is on studying transformers to recognize
various formal languages, see e.g., [BAG20]. As far as we know, none of these works try to probe the inner
workings of the networks in the same depth as we do here. On the other hand, networks trained on real
data are being extensively scrutinized, see for example [RKR20] where they try to understand some of the
attention heads of BERT (see also [SGSB20a] and the references therein). However, making sense of these
real-data-trained networks is a daunting task, and a key contribution of our work is to show that in a more
limited setting one can obtain a much clearer picture of what transformers learn.

The LEGO task is also naturally related to the growing literature on testing mathematical/coding abilities
of transformers (e.g., [SGSB20b]), speciﬁcally the simpler tasks of checking the correctness of a proof (or

3

simplifying one, such as in [AAG21] which studies simpliﬁcation of polynomials), or executing code for a
given input [CST21]. It would be interesting to see if some of the insights we derive in the present paper
apply to currently challenging mathematical tasks such as MATH [HBK+21] and IsarStep [LYWP21].

2 Learning equality and group operation (LEGO)

We propose the following synthetic task, which we call LEGO. Let G be a ﬁnite (semi)group acting on a
ﬁnite set X, and denote g(x) for the action of g ∈ G on x ∈ X. We deﬁne a formal language using the
symbols from G and X as well as symbols from a ﬁnite alphabet A which we refer to as the variables. A
sentence in our formal language is made of clauses separated by a semi-colon. A clause is of the form a = gx
with a ∈ A, g ∈ G and either x ∈ X or x ∈ A. If x ∈ X, such a clause means that the variable a is assigned
the element g(x) ∈ X. On the other hand if x ∈ A and the variable x was assigned an element y ∈ X
through another clause (or chain of clauses) in the sentence, then the clause a = gx assigns variable a to
the element g(y) ∈ X. The task’s goal is to take in input a sentence with a ﬁxed number n of clauses, given
in an arbitrary order, and to output the assigned element to each variable that appear in the sentence (the
formal language will have a further restriction that ensures that each variable is assigned one and only one
element).

We can view a sentence as a directed graph on the vertex set X ∪ A with labelled edges as follows: a
clause a = gx corresponds to a directed edge from the vertex x to the vertex a, and the edge is labelled
with g. We restrict our attention to sentences corresponding to a line graph directed away from some ﬁxed
root vertex r ∈ X, and whose non-root vertices are all in A, see Figure 1 for an example. In particular such
sentences are “consistent”, meaning that a variable is assigned a unique element (the assignment is obtained
by simply “following the chain”).

Task 1. The most basic instantiation of LEGO is when G is the unique group of 2 elements acting on
a set X also of 2 elements, that is G = {+, −} and X = {1, −1}. Our sentences thus consists of n clauses
of the form ai = ±ai−1, where ai ∈ A for i = 1, 2, . . . , n and a0 = 1 (we ﬁx r = 1). Note that in this case
our formal language has well over a billion unique valid sentences when n ≥ 10. Example of a sentence with
n = 6 is (see Figure 1 for the graph depiction): a = +1; b = −a; e = +b; d = −f ; c = +d; f = +e. Our
task’s goal is to report the elements or values from X assigned to the variables appearing in the sentence.
In the above example, assignments for variables a, b, c, d, e, f are 1, −1, −1, −1, 1, 1.

1

+

a

−

b

+

+

e

f

−

d

+

c

Figure 1: The graph representation of the sentence a = +1; b = −a; e = +b; d = −f ; c = +d; f = +e

Task 2. One can think of Task 1 as the case of LEGO for the permutation group on N = 2 elements (act-
ing on itself). Our second task will correspond to N = 3, which is qualitatively diﬀerent as the permutation
group on 3 elements in non-abelian. See appendix for experiments on this task.

We will focus on Task 1 in the main paper. Our training and test data for the task consists of n length
chains as described above with the order of clauses in the sentence randomized. A sample input sentence to a
transformer looks like [BOS] j=-f; f=-b; y=+t; o=+e; d=+y; v=+d; h=-o; b=-i; i=+1; t=+l; e=-j;
l=-h; [EOS]. See appendix for further data generation details.

3 Transformers for LEGO

We apply transformer models in the token classiﬁcation pipeline to predict the assignments of the variables
in the input sentence, depicted in Figure 2. To evaluate the out-of-distribution generalization (referred to

4

Figure 2: Illustration of a transformer model applied to LEGO Task 1 on input sentence d=-c; b=-a; c=+b; a=+1;.
We apply a linear classiﬁcation head to the output representations of each clause’s ﬁrst token to generate predictions
for the variables assignment.

simply as generalization), we introduce the notation of ntr ≤ n, such that during training, supervision is
provided only on the ﬁrst ntr clauses (ﬁrst in the graph representation of the input sentence). We mainly
focus on BERT [DCLT18] and ALBERT [LCG+19] architectures. These two models are representative large
transformer architectures for NLP tasks, and we observe they exhibit intriguing behavior diﬀerence on our
tasks which we will detail in Section 4. See appendix for training hyper-parameters and dataset construction
details.

In Figure 3, we report initial results on LEGO with n = 12 and ntr = 6, 12. Both BERT and ALBERT
are able to achieve good classical generalization, while only ALBERT appears to generalize even to slightly
longer sequence length. We observe similar behavior across diﬀerent lengths of inputs too. This suggests
that classical generalization might be a deceptive metric to evaluate learning of true logic/reasoning tasks.
Motivated by these initial results, in the next section we focus on breaking down the learning dynamics of
BERT and ALBERT for the LEGO task towards carefully understanding their strengths and weaknesses.

4 Unveiling transformers with LEGO

4.1 BERT vs. ALBERT: Iterative reasoning in iterative architectures

A salient feature of many reasoning tasks is an iterative component, meaning they can (or must) be solved by
sequentially repeating certain operations. In this section, we use LEGO to study and compare transformer
architectures through the lens of iterative reasoning.

A natural solution to LEGO—and arguably the go-to solution for a human—is to implement a “for
loop”, where each iteration resolves one step in the reasoning chain. The iteration could look for the next
Iterative transformer architectures
unresolved variable token whose value could be resolved in one step.
such as ALBERT, where the weights are shared across diﬀerent layers, inherently implement a for loop
with number of iterations equal to the number of layers. If the model manages to learn to implement one
such iteration during training, the network would immediately be capable of performing out-of-distribution
generalization, solving longer sequences than the ones it has been trained on. If this indeed occurs, it would
point to a clear advantage of ALBERT over BERT in our setting. This leads to the 3 following questions,
addressed in turn below:

5

Transformer[BOS]d=-c;[EOS]a=+1;;b...[BOS];d=-c[EOS];=a+1;b...+1 or -1?+1 or -1?+1 or -1?.........input tokensmodelrepresentationsclassification head...(a)

(b)

Figure 3: Solving LEGO (Task 1) using two standard transformer models – BERT and ALBERT, trained from a
random initialization. Each curve corresponds to test accuracy of a single variable appearing in the sentence over
the course of training. The variable numbers in the legend correspond to their position in the reasoning chain (or
graph representation) of the input sentence, rather than the position in the sentence itself. For example, on the input
sentence: b=-a; d=-c; c=+b; a=+1;, variable #0 is a, #1 is b, #2 is c, and #3 is d. Top: models are trained to ﬁt all
variables, i.e., n = 12, ntr = 12. Bottom: models are trained to ﬁt the ﬁrst 6 variables but test on all 12 variables,
i.e., n = 12, ntr = 6. Dashed curves represent variables unseen during training.

Q1. Do iterative architectures indeed exhibit better out-of-distribution generalization?

The bottom plots of Figure 3 display the out-of-distribution generalization result for BERT and for ALBERT.
They show the clear advantage of recurrence: While the non-iterative BERT achieves only somewhat better-
than-random accuracy for one variable (#6) beyond the ones accounted for during training (#0- -#5), the
iterative ALBERT reaches near perfect accuracy on two additional variables (#6 and #7), and nontrivial
accuracy on the third (#8). These results clearly support that iterative architectures do generalize better in
the iterative LEGO reasoning task.

Q2. To what extent does the ALBERT architecture actually implement the above for loop?

To a lesser extent, Figure 3 also hints at a positive answer to Q2. Observe that ALBERT exhibits out-of-
distribution generalization to variable #6 immediately (in terms of epochs) as soon as it ﬁts the training
variables (#0 – #5), whereas for BERT, the corresponding plot (#6) climbs gradually even after the training
variables are predicted perfectly. This Eureka moment behavior of ALBERT suggests that once it manages
to learn the operations required for one step of reasoning, it can immediately implement those operations
over a larger number of iterations not required in training.

In order to gain stronger evidence regarding Q2, we designed an experiment attempting to gauge the
dependence between the location of a variable token in the chain and the layer in which its value is typically
resolved. To this end, given a trained model, we train one linear classiﬁer per layer whose purpose is to
predict the value of a variable token based only on its token representation at corresponding layer (without
using other information at all), while keeping the original model unchanged. The function of the classiﬁers
is therefore to probe the representations of resolved tokens at the intermediate layers. This allows us to
gauge which variables are already resolved in each layer, and thus observe the rate at which information
percolates along the reasoning chain in terms of layers per reasoning step. If the model indeed implements

6

100101102epoch50%60%70%80%90%100%test accRand-Init BERT100101102epochRand-Init ALBERTvariable #01234567891011100101102epoch50%60%70%80%90%100%test accRand-Init BERT100101102epochRand-Init ALBERTvariable #01234567891011Figure 4: Visualization of information percolation within the ﬁne-tuned models. For a ﬁne-tuned model, we associate
its every layer with a new binary linear classiﬁer. At each layer, we apply the associated classiﬁer on the intermediate
representation of a variable token from the input sequence, and we train the classiﬁers at all layers jointly to predict
the variables’ labels, while keeping the model parameters intact. Color indicates test accuracy of each classiﬁer.
Brighter is higher.

a for loop, one expects a linear relation between the number of layers and the number of reasoning steps
already completed.

We depict the results in Figure 4, visualizing the test accuracy of prediction as a function of the layer
in the network and depth in the chain. While not perfectly linear in either of the cases, the relation clearly
looks closer to linear in ALBERT, suggesting that the for loop which is (by deﬁnition) implemented by the
architecture of the model is in fact aligned with the iterations of the “natural” for loop associated with the
problem. However, the fact that the ALBERT model does not generalize to “all” test variables suggests that
the relationship is not “exact”.

Q3. In iterative reasoning tasks, how can we incentivize models to learn iterative solutions?

We attempt to incentivize the model to implement the “natural” for loop solution. We rely on the observation
that if each iteration of the for loop simply percolates the information one more step (assigning a value to
the next variable), then adding more layers with the same weights should not aﬀect the output, and in fact,
one should be able to read out the output of the calculation from any layer of the neural network, as long
as its depth exceeds the length of the chain. Moreover, once the computation has resolved the values of the
variable tokens, the tokens should be a ﬁxed point of the iteration, meaning that adding more layers (with
the same parameters) should not aﬀect the output.

With this observation in mind, we can think of the network as having variable depth chosen at random
independently of the input. We train a ALBERT model where the depth is ﬁxed, but the loss function
is taken to be a mixture of the losses corresponding to testing the output on diﬀerent layers, similar to
stochastic depth from [HSL+16]. The results are depicted in Figure 5, which show a clear improvement in
out-of-distribution generalization using stochastic depth, suggesting that the modiﬁcation of the loss function
indeed incentivizes the model to implement the expected iteration.

7

0123456789101101234567891011##Figure 5: Generalization of ALBERT trained with stochastic depth. At training time, depth is uniformly sampled
from integers between 6 and 12 per mini-batch, while we ﬁx depth to be 12 at test time.

4.2 Rand-Init vs. Pretrained: Structural advantages emerging from pretraining

Pretraining large models has emerged as a prominent and highly successful paradigm in large-scale deep
learning.
It advocates ﬁrst training the model on a large dataset to perform a generic task, followed by
task-speciﬁc ﬁne-tuning on the task at hand. Our goal in this section is to use LEGO as a testing ground
for this paradigm.

To this end, we compare (a) training the BERT architecture for LEGO from random initializations to
(b) ﬁne-tuning the standard pre-trained BERT model to solve LEGO. Figure 6 (left and center plots) shows
that pretraining helps generalization in LEGO dramatically: the pre-trained model generalizes to unseen
sequence lengths (the dashed plots) much better, and within a far smaller number of epochs, than the
randomly initialized model.

We investigate the root causes for this advantage. Since pre-trained transformer-based networks are
demonstrably capable of performing reasoning tasks to a certain extent (see discussion in the appendix),
one may postulate that the success of pre-trained BERT on LEGO is due to a reasoning capability acquired
during pretraining. However, our ﬁndings will show that its success can be attributed—for the most part—
not to “reasoning”, nor to any actual data seen during pretraining, but rather to certain sturctural patterns of
“information transfer” which are shared by many tasks. Indeed, we demonstrate that much of the advantage
can be recovered by directly initializing the model to “mimic” those patterns explicitly, without pretraining
and without seeing any prior data at all (see results in the right plot in Figure 6). Drawing on our ﬁndings,
we study in Section 5 a hybrid transformer model that explicitly builds those patterns into the architecture.

Figure 6: Pre-trained BERT exhibits signiﬁcant performance advantages over its Rand-Init counterpart, while the
mimicking procedure (a simple initialization scheme we describe below) heads closes the gap.

Why does pretraining help in LEGO? Since LEGO is a fundamental reasoning task, one possible
explanation for the success of pre-trained BERT on this task is having acquired an innate ability to “reason”
during pretraining. While this is hard to verify or disprove directly, one may come up and test simpler
explanations for this impressive performance.

8

100101102epoch50%60%70%80%90%100%test accRand-Init ALBERT100101102epochRand-Init ALBERT  w/ stochastic depthvariable #01234567891011100101102epoch50%60%70%80%90%100%test accRand-Init BERT100101102epochPre-Trained BERT100101102epochMimicking BERT variable #01234567891011Figure 7: Visualization of attention maps from a pre-trained BERT model not yet ﬁne-tuned on LEGO. On a LEGO
input sequence, certain heads implement local, convolution-like manipulation operators (left), while some others
implement global, long range association operators (right). Find more visualization in appendix.

One simple explanation is that pre-trained BERT is already aware of the semantics of tokens like ‘=’
or ‘-’, and can immediately interpret them correctly upon encountering them in the LEGO task. We have
easily ruled out this possibility, by replacing those tokens with arbitrary ones that do not encompass the
same semantic meanings; this does not aﬀect the performance of pretrained BERT.

A more intriguing explanation pertains to the attention mechanism itself. At its basis, LEGO requires

two fundamental types of information transfer:
• Association: encoding long range dependencies that transfer a value between two occurrences of the
same variable. For example, if the input contains the two clauses “a = +1” and “b = −a” (with arbitrary
separation between them), the architecture must associate the two occurrences of the variable a in order
to correctly set b to −1.

• Manipulation: encoding short range dependencies of transferring a value from the right-hand to the
left-hand side of the clause. For example, to successfully process the clause “b = −a”, the architecture
must associate these particular occurrences of a and b with each other, in order to transfer the value of
a (after applying to it the group element −1) into b.

Notice that the two types correspond to diﬀerent attention patterns. Association corresponds to a purely
global attention pattern, completely reliant on the content of the tokens and oblivious to their position in
the input sequence. Manipulation, in contrast, corresponds to a purely local attention pattern, where nearby
positions attend to each other (note that feed-forward layers might be useful for manipulation too). The
LEGO task crystallizes the roles of these fundamental patterns, so it is natural to ask whether they are
indeed manifested in the transformer attention heads in practice.

We answer in the aﬃrmative. Fig. 7 shows two select attention heads in the ﬁrst two layers of pre-trained
BERT on an input LEGO sequence without any ﬁne-tuning. The right head clearly depicts association: each
token attends to all other occurrences of the same token in the input sequence. The left one clearly depicts
an attention pattern that facilitates learning manipulation: each token attends to the tokens immediately
before and after it in the sequence. The other heads (shown in appendix) are either similar to one of these
two, or appear to be “blank” that do not exhibit any clear pattern. It thus appears that pre-trained BERT
has learned during pretraining to realize the association and manipulation patterns, as they indeed arise in
many natural language tasks. We hypothesize that this is the explanation for the impressive performance of

9

[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 110.00.20.40.60.81.0pretraining for the LEGO task.

Mimicking BERT. To test this hypothesis, we craft a mimicking procedure to directly initialize the
attention heads to perform association and manipulation, without access to pretraining data. We achieve this
by specifying the target attention matrices (one for association and one for manipulation), and training the
model on random data to minimize a “mimicking loss” that measures how well the actual attention matrices
at every layer match the target matrices. The precise mimicking loss and training protocol are speciﬁed in the
appendix. The results, depicted in the right plot in Figure 6, show that BERT with mimicking initialization
attains signiﬁcant advantage in generalization over randomly initialized BERT, despite not being pre-trained
on any real data (and thus not having learned to “reason”). This conﬁrms that much of the advantage of
pre-trained BERT stems from having learned these information transfer patterns.

4.3 Shortcut solutions and their eﬀect on generalization

As discussed in Section 4.1, the arguably natural solution to LEGO is to resolve variables iteratively by
the order of their depth in the chain. Nonetheless, to our surprise, we found that the randomly initialized
BERT and ALBERT models ﬁrst learn a “shortcut” solution: they immediately resolve the last variable
Indeed, the last variable can be
in the reasoning chain, by counting the total number of minus signs.
easily identiﬁed as it appears only once (whereas every other variable appears twice), and its value is fully
determined by the parity of the number of minus signs. This behavior is clearly seen in the top two plots in
Figure 3, in which the randomly initialized models are trained to ﬁt all 12 variables: the last variable (#11)
improves in accuracy earlier than almost all other ones.

This behavior may be somewhat related to the well-observed phenomenon of spurious features: a model
trained to distinguish cows from boats could learn to identify cows by the grass around them, rather than
relying on any actual features of cows, circumventing the intended solution. While possibly eﬀective in ﬁtting
the training data, such solutions may interfere with generalization.

We use LEGO as a case study of shortcut solutions and their eﬀect on generalization. Instead of training
the model to ﬁt the ﬁrst six variables (as in bottom Figure 3), we train it to ﬁt the ﬁrst ﬁve (#0–#4) and the
last variable (#11). This allows us to measure out-of-distribution generalization (to #5–#10) in a setting
where models can learn the shortcut. The results show signiﬁcantly degraded performance, implying that
shortcut solutions impede generalization. We then study ways to prevent models from learning them, by
pretraining and mimicking. The full section appears in the appendix.

5 Hybrid convolutional transformer architecture

Our above ﬁndings suggest a natural modiﬁcation to the transformer architecture. Out of the two funda-
mental attention patterns—association and manipulation—only the latter depends on the positions of tokens
in the sequence (while association is position-oblivious). Since this pattern appears crucial for LEGO, and
indeed for many symbolic reasoning tasks, it seems helpful to encode it directly in the architecture, rather
than letting the model learn it indirectly from positional encodings. This is akin to the way convolutional
layers directly encode relations between adjacent pixels in their architecture. Since such “local” or convo-
lutional attention patterns are likely valuable in virtually all natural language tasks, it is intriguing to also
apply this idea to language models in general.

We instantiate this idea by implementing the following hybrid between convolutional and transformer
In each attention head, the key, query and value linear transformations are applied not
architectures.
just to each token on its own, but to an outcome of a one-dimensional convolution along the temporal
dimension of the input sequence. To avoid excessive amount of additional parameters, we adopt depth-wise
convolution [Cho17], i.e., one ﬁlter per input dimension. We give precise details in the appendix. We are
aware of existing extremely similar, popularized ideas in computer vision [DLLT21, LLC+21, LMW+22] and

10

natural language processing [JYZ+20, CLJ20]. We intend to provide evidential motivations for its use in
reasoning tasks rather than to claim novelty.

On LEGO, modiﬁed BERT and ALBERT not only are capable of matching the performance of their

pre-trained unmodiﬁed versions, but also avoid the shortcut solutions in Section 4.3.

[BOS] a[0]=1; a[1]=-3; a[2]=2; a[3]=-2; a[4]=0; [SEP] int * func_1(int a[])
{ int p_0 = 4; int l_7 = 2; ++a[l_7]; for (p_0 = 0; p_0 <= 2; p_0++)

{ a[p_0] = 0; for (int p_1 = 3; p_1 >= 2; p_1--) { a[p_1]--;} }

return a; } [EOS]

[BOS] a[0]=-4; a[1]=1; a[2]=3; a[3]=0; a[4]=1; [SEP] int * func_1(int a[])

{ int p_0 = 1; int l_9 = 1; a[l_9] = (0 * a[p_0]); return a; } [EOS]

[BOS] a[0]=0; a[1]=3; a[2]=-1; a[3]=4; a[4]=-2; [SEP] int * func_1(int a[])
{ int p_0 = 0; int l_9 = 3; for (p_0 = 4; p_0 >= 3; p_0--) { a[p_0]
= 0; a[p_0] = (a[l_9] * a[p_0]);} return a; }[EOS]

......

Figure 8: Left: Example input sequences adapted from the restricted C dataset. Right: Performance of vanilla BERT
versus our hybrid architecture with various convolutional ﬁlter sizes (5, 10, 15). Error bars are standard deviations
of 3 independent runs, plotted every 2 epochs for visibility.

We further test on a more advanced symbolic reasoning task than LEGO — learning to execute C
programs. We use the restricted C dataset [CST21] of 500K programs written in C, each provided with
the initial values of input variables as well as the outcome. We consider the task is to predict the outcome
given the initial values and the program. To use transformer here, we convert each program together with
the initial assignment into plain text and feed it to the model as the input sentence. To make predictions
we apply a linear classiﬁer to the output representations, as in LEGO. Exact setups are in the appendix.
Figure 8 shows that the convolutional architecture attains a signiﬁcant performance advantage over the
vanilla BERT on this task, with less than 0.5% extra parameters.

11

101520epoch92%94%96%98%100%test accConv BERT: 15Conv BERT: 10Conv BERT: 5vanilla BERTReferences

[AAG21]

[BAG20]

Vishesh Agarwal, Somak Aditya, and Navin Goyal. Analyzing the nuances of transformers’
polynomial simpliﬁcation abilities. arXiv preprint arXiv:2104.14095, 2021.

Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the Ability and Limitations of Trans-
formers to Recognize Formal Languages. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 7096–7116, 2020.

[BMR+20a] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel
Ziegler, Jeﬀrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.
In Advances in
Neural Information Processing Systems, volume 33, pages 1877–1901, 2020.

[BMR+20b] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models
are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[Cho17]

[CLJ20]

[CLR+21]

Fran¸cois Chollet. Xception: Deep learning with depthwise separable convolutions.
In Pro-
ceedings of the IEEE conference on computer vision and pattern recognition, pages 1251–1258,
2017.

Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-
attention and convolutional layers. In International Conference on Learning Representations,
2020.

Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. Advances in neural information processing systems, 34, 2021.

[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[CST21]

Xinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis
beyond domain-speciﬁc languages. Advances in Neural Information Processing Systems, 34,
2021.

[DBK+21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on Learning Representations, 2021.

[DCLT18]

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[DHD+21] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Eﬃcient scaling of
language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021.

12

[DLLT21]

Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and
attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965–3977,
2021.

[HBK+21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset,
2021.

[HBM+22]

Jordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[HSL+16]

Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochastic depth. In European conference on computer vision, pages 646–661, 2016.

[HSW+22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In Inter-
national Conference on Learning Representations, 2022.

[JEP+21]

[JYZ+20]

[KSH12]

[KSS+20]

John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al.
Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.

Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan.
Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Informa-
tion Processing Systems, 33:12837–12848, 2020.

Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in Neural Information Processing Systems, volume 25,
2012.

Daniel Keysers, Nathanael Sch¨arli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashu-
bin, Nikola Momchev, Danila Sinopalnikov, Lukasz Staﬁniak, Tibor Tihon, Dmitry Tsarkov,
Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A
comprehensive method on realistic data. In International Conference on Learning Representa-
tions, 2020.

[LB18]

Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional
skills of sequence-to-sequence recurrent networks. In International conference on machine learn-
ing, pages 2873–2882. PMLR, 2018.

[LBD+89] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D.
Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation,
1(4):541–551, 1989.

[LCG+19]

[LLC+21]

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv
preprint arXiv:1909.11942, 2019.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.

[LMW+22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining

Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.

13

[LYWP21] Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. Isarstep: a benchmark for high-level

mathematical reasoning. In International Conference on Learning Representations, 2021.

[NAGA+22] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sut-
ton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with
language models. In Deep Learning for Code Workshop, 2022.

[NYM+20] Weili Nie, Zhiding Yu, Lei Mao, Ankit B Patel, Yuke Zhu, and Anima Anandkumar. Bongard-
logo: A new benchmark for human-level concept learning and reasoning. Advances in Neural
Information Processing Systems, 33:16468–16480, 2020.

[PBBG22] Arkil Patel, Satwik Bhattamishra, Phil Blunsom, and Navin Goyal. Revisiting the compositional

generalization abilities of neural sequence models. arXiv preprint arXiv:2203.07402, 2022.

[PHZ+22]

[RBC+21]

Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys,
and Ilya Sutskever. Formal mathematics statement curriculum learning.
arXiv:2202.01344, 2022.

Igor Babuschkin,
arXiv preprint

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoﬀmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language
models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446,
2021.

[RDN+22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical

text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.

[RKR20]

Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know
about how bert works. Transactions of the Association for Computational Linguistics, 8:842–
866, 2020.

[RMS+21] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi
Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge
from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National
Academy of Sciences, 118(15), 2021.

[SGSB20a] Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. Prover: Proof gen-

eration for interpretable reasoning over rules. arXiv preprint arXiv:2010.02830, 2020.

[SGSB20b] Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. Prover: Proof gen-
eration for interpretable reasoning over rules. In Bonnie Webber, Trevor Cohn, Yulan He, and
Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP, pages 122–136. Association for Computational Linguistics, 2020.

[SPN+22]

Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari,
Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using
deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language
model. arXiv preprint arXiv:2201.11990, 2022.

[TDFH+22] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for
dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[VSP+17]

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
(cid:32)L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, volume 30, 2017.

14

[WDS+20] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-
art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020.
Association for Computational Linguistics.

[WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.

[ZRG+22]

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[ZRKB21] Chiyuan Zhang, Maithra Raghu, Jon Kleinberg, and Samy Bengio. Pointer value retrieval: A
new benchmark for understanding the limits of neural network generalization. arXiv preprint
arXiv:2107.12580, 2021.

15

A Shortcut solutions and their eﬀect on generalization

As explained in Section 4.3, we have observed that the randomly initialized models ﬁrst learn a “shortcut”
solution—predicting the last variable in the chain by counting the overall number of minus signs—instead of
the “common sense” iterative solution. This can be seen in the top two plots in Figure 3, where the accuracy
of variable #11 improves earlier than most of the other variables.1

To be more precise, let us describe the two solutions in detail with an example. Consider the input:
a=+1; d=-c; b=-a; c=b; Initially, only the variable a is resolved. The iterative solution identiﬁes an unre-
solved variable that appears in the same clause with an already resolved variable, resolves it according to
that clause, and repeats. In this example, it would resolve b to −1 by the clause b=-a, then resolve c to −1
by the clause c=b, and then resolve d to 1 by the clause d=-c. The shortcut solution identiﬁes an unresolved
variable that appears only once, and resolves it to 1 if the overall number of minus signs is even, and to −1
otherwise. In the above example, where d is the last variable in the reasoning chain, the shortcut solution
correctly resolves it to 1.

As further mentioned in Section 4.3, the shortcut solution to LEGO may be related to the phenomenon
of spurious features, where models learn to perform tasks in ways that circumvents the intended “common
sense” solution a human would use. Such spurious solutions are often considered undesirable, as they are
known to generalize poorly even to mild variants of the task.
Indeed, the shortcut solution to LEGO is
brittle even under simple variations to the problem:

• Repeated clauses, e.g., a=+1; b=-a; b=-a;

• Redundant clauses, e.g., a=+1; b=a; c=-a; c=-b;

• Multiple jointly rooted reasoning chains, e.g., a=+1; b=-a; c=-a;

• Multiple disjoint reasoning chains, e.g., a=+1; b=+1; c=-a; d=-b;

and more.
In all those settings the shortcut solutions would fail, whereas the “common sense” iterative
solution would succeed. This motivates us to empirically study the eﬀect of the shortcut solution on the
ability of the models to generalize. We pose the following questions:

Q1. How does reliance on shortcut solutions aﬀect the ability of the network to generalize?

A ﬁrst indication that the shortcut solution is undesirable for LEGO can be gleaned already from Figure 3:
along with the early improvement in the accuracy of variable #11 (which indicates that the shortcut solution
is being learned), we observe a drop in the accuracy of some of the variables that were already learned
(#2 in ALBERT and #3 in BERT). This may suggest that the shortcut solution impedes even classical
generalization. Indeed, the models seem to “realize” that, as we can see that the accuracy of #11 drops
before improving again together with the other variables, indicating that the shortcut solution is being
abandoned in favor of a solution that can predict all variables correctly.

To gain insight into the eﬀect of the shortcut solution on out-of-distribution generalization, we performed
an experiment where the models are trained to ﬁt the ﬁrst ﬁve variables (#0-#4) and the last one (#11),
and are asked to predict all 12 variables at test time. This is diﬀerent from the top plots in Figure 3 where
the model was trained to ﬁt all 12 variables (and thus no out-of-distribution generalization is observed), and
from the bottom plots in Figure 3 where the model is trained to ﬁt the ﬁrst six variables (#0-#5), without
#11 (and thus learning the shortcut solution is not possible). The results of this experiment are reported in
the top two plots in Figure 9. The bottom plots depict a control experiment where the models are trained to

1This behavior is not expected in the bottom two plots, since in the top ones the models are trained to ﬁt all 12 variables
including #11, while in the bottom plots the models are only trained to ﬁt the ﬁrst 6 variables, which precludes learning the
shortcut solution.

16

(a)

(b)

Figure 9: Learning shortcut impedes generalization. (a) Train on variables #0-#4 and #11. (b) Train on variables
#0-#4 only. In both plots we test on all 12 variables.

ﬁt only the ﬁrst ﬁve variables, without #11 (and thus, again, learning the shortcut solution is not possible).
The results show that the models exhibit inferior out-of-distribution generalization (to variables #5 in BERT
and #6 in ALBERT) when provided supervision for #11 (top plots), even though they are given strictly
more information during training than in the bottom plots, ostensibly making the task easier. We thus infer
that the shortcut solution in LEGO has an adverse eﬀect on out-of-distribution generalization.

Q2. What are eﬀective ways to prevent models from learning shortcuts, and do they result in
better generalization?

In Section 4 we studied the eﬀect of pretraining on LEGO, and observed that the pretrained BERT model
exhibits much better out-of-distribution generalization than the randomly initialized BERT model. This
naturally suggests that pretrained BERT possibly avoids the shortcut solution. We conﬁrm experimentally
in Figure 10 (top), where pretrained BERT and ALBERT are ﬁnetuned to ﬁt all 12 variables. Indeed observe
the accuracy of #11 improving either later than or concurrently with all other variables, suggested that the
shortcut solution is not being learned. We speculate that this may have to do with the number of epochs it
takes to learn the iterative solution: By the time the randomly initialized BERT has learned the shortcut
solution, pretrained BERT already attains full accuracy on the entire chain of variables. Avoiding the
shortcut solution may partly explain the superior out-of-distribution generalization performance of pretained
BERT over randomly initialized BERT, seen in Figure 6.

Section 4 also showed that our mimicking technique—which directly mimics the attention patterns of
the pretrained model without training on any data—can recover much of the beneﬁt in pretraining for
LEGO. This extends to avoiding the shortcut solution as well: the bottom plots in Figure 10 show that the
mimicking BERT and ALBERT models exhibit similar accuracy patterns to their pretrained counterparts,
suggesting that the shortcut solution is not being learned by them. We will show in Section C.2 below that
a certain convolutional modiﬁcation of the transformer architecture is also capable of avoiding shortcut and
generalizing better to longer sequences.

17

100101102epoch50%60%70%80%90%100%test accRand-Init BERT100101102epochRand-Init ALBERTvariable #01234567891011100101102epoch50%60%70%80%90%100%test accRand-Init BERT100101102epochRand-Init ALBERTvariable #01234567891011Figure 10: Shortcut solutions are avoided via either pre-training or mimicking.

B Data generation and training details for the LEGO task

B.1 Data generation

We specify the data generation mechanism for Task 1 in the following. We use lowercase alphabets as
variables A = {a, b, c, d, . . . , z}. Given n, we generate a sentence from our distribution s ∼ D(n) as follows:

1. Sample n variables a1, a2, . . . , an ∈ A and their corresponding assignments (or labels) y1, y2, . . . yn ∈ X

uniformly, i.e., ∀ i ∈ [n], ai ∼ Unif(A) and yi = ±1 w.p. 0.5.

2. The n clauses are then generated as ai = giai−1 for i = 1, 2, . . . , n, where a0 = r = 1 and the group
elements g1, g2, . . . gn ∈ G are uniquely chosen so that the clauses are consistent with assignments ai = yi
for all i ∈ [n].

3. The sentence s is generated by concatenating a random ordering of the n clauses with a semicolon ;.
Finally, the sentence is padded with [BOS] and [EOS] tags to denote the beginning and end of sentence,
respectively. See Figure 11 for example sentences from our distribution.

[BOS] j=-f; f=-b; y=+t; o=+e; d=+y; v=+d; h=-o; b=-i; i=+1; t=+l; e=-j; l=-h; [EOS]
[BOS] j=+o; s=-y; p=-r; y=-m; u=-a; a=-f; k=+p; o=-k; q=+u; m=+1; f=+s; r=+q; [EOS]
[BOS] z=+d; b=+1; m=+t; d=-u; u=-h; a=-b; j=+m; i=-j; t=+x; f=+i; h=-f; x=-a; [EOS]
[BOS] j=-f; f=-b; y=+t; o=+e; d=+y; v=+d; h=-o; b=-i; i=+1; t=+l; e=-j; l=-h; [EOS]
[BOS] w=+l; m=+c; c=-i; f=-d; p=-m; a=+b; y=-a; b=+p; i=+f; l=-v; d=+1; v=+y; [EOS]

Figure 11: Samples of sentences generated from our distribution D(n) for Task 1 with n = 12.

B.2 Training

Our vocabulary for data generated as above thus consists of symbols of variables a ∈ A, group operations
+, − ∈ G, the root node 1 ∈ X, the equal sign ‘=’, and the semicolon ‘;’ along with the [BOS] and [EOS] tags.

18

10010110250%60%70%80%90%100%test accPre-Trained BERT10010110250%60%70%80%90%100%test accPre-Trained ALBERT100101102epoch50%60%70%80%90%100%test accMimicking BERT100101102epoch50%60%70%80%90%100%test accMimicking ALBERTvariable #01234567891011To apply transformers to the LEGO task we convert each symbol in our vocabulary to vectors in Rd (referred
to as tokens) using a learnable linear embedding layer. Thus, a sentence of n clauses now corresponds to an
ordered list of 5n+2 tokens, which we turn into an unordered list using positional embedding (see [VSP+17]).
These tokens are processed iteratively by transformer blocks. Each transformer block maps 5n tokens in Rd
to another set of 5n tokens using a multi-head attention layer followed by a one-hidden layer feedforward net
(there are also residual connections and layer normalization, for full details of architecture see [VSP+17]). We
use bert-base-uncased2 and albert-base-v13 along with their pretrained weights from the open source
Huggingface transformers library [WDS+20]. The Rand-Init models have identical conﬁgurations to their
pretrained counterparts but randomly initialized weights.

Our training and test datasets are i.i.d. samples from our distribution D(n) as described above. We
generate 104 × n and 103 × n datapoints for training and test, respectively, and sanity checked that there
is no overlap between train and test data. Recall that during training we provide supervision on the ﬁrst
ntr appearing in the graph representation of the sentence, but test the accuracy on all n samples at test
time. Note that since the clause positions are randomized in our input sentence (e.g., Figure 11), the ﬁrst
ntr clauses in the graph representation can appear at arbitrary positions in the sentence which allows for
training the positional encodings for longer sequences than those seen in training.

In all the LEGO experiments, we use cross entropy loss averaged over the ntr clauses as our sample loss
during training. We train for 200 epochs using the Adam optimizer with batch size of 1000 samples, 5 × 10−5
learning rate, β1 = 0.9, β2 = 0.999, (cid:15) = 1 × 10−8, and cosine learning rate schedule with Tmax = 200. For
tokenization, we use the pretrained BERT tokenizer for all the experiments, which merely converts the input
symbols into integers which are used as token ids. Each run is conducted on a cluster with 4 A100 GPUs.

A note on variance of training across LEGO tasks When training BERT and ALBERT models for
the LEGO task using our experimental setup, we see non-trivial variance in absolute test accuracies across
diﬀerent runs. In Figure 12, we show three diﬀerent runs of BERT and ALBERT models trained on LEGO
tasks of length n = 12 (the conﬁguration used in most results in the paper). While we see that the absolute
values of the test accuracies vary signiﬁcantly, the qualitative observations made in our paper hold across
all the runs: importantly, across all the runs, we see that using iterative ALBERT architecture as well as
pre-training non-iterative BERT architecture leads to better generalization to unseen lengths. This shows
that conclusions derived in our paper hold despite the variance across runs.

Methodologically, we attribute the variance in the absolute test accuracy to the relatively small size of
our datasets (e.g., the number of training examples for n = 12 is 120K tokens) for training standard trained
language models which are otherwise trained on hundreds of millions of tokens. Furthermore, note that in
our experiments the variance across runs arise both from having diﬀerent train and test datasets as well as
diﬀerent random seeds for model initialization and training algorithm.

B.3 The Mimicking procedure

Starting with a randomly initialized transformer model, we would like to make sure that some of its attention
heads implement the manipulation and association functionalities, prior to the ﬁne-tuning. To do so, we
craft the desired attention patterns of a manipulation head and an association head, and train the model
with gradient descent to match them. Speciﬁcally, given a random input x ∈ RT of length T , we hard code
the following matrix M ∈ RT ×T as the target attention pattern for the manipulation head4, and derive from
input x the following matrix A ∈ RT ×T whose Ai,j entry indicates whether xi and xj are identical tokens.
Note that we further specify that A has a zero diagonal in observance of the association head in Figure 7

2https://huggingface.co/bert-base-uncased
3https://huggingface.co/albert-base-v1
4Here we choose the Gaussian ﬁlter [1, 2, 4, 2, 1] for illustration. In practice, we ﬁnd that the ﬁnal performance is robust to

various pattern choices as long as they are localized and shift-invariant.

19

(a)

(b)

(c)

Figure 12: Three sample runs of models trained on LEGO tasks with n = 12 and ntr = 6. We observe that while
there are variances across diﬀerent runs of the models, the qualitative conclusions stated in the paper holds for all
the runs: i.e, iterative ALBERT models and pretraining lead to better generalization to unseen task lengths.

having a vanishing diagonal. In reality, attention maps have unit row sums, thus we normalize M and A
accordingly to obtain ˜M and ˜A such that their rows ˜M [t, :] and ˜A[t, :] are valid distributions.











. . .
1

. . .
2
1

. . .
4
2
1

M =

2
4
2

1
2
4
. . .

1
2
. . .

1
. . .











, Aij =




1 ,



0 ,

if xi = xj and i (cid:54)= j

otherwise

At every layer, we randomly appoint two attention heads to mimic the manipulation and association
operators while leave the other heads oﬀ the hook. Upon seeing a input sequence x ∈ RT , we denote the
attention maps of the appointed heads at the l−the layer as Attn(l)
1 (x) ∈ RT ×T . For the mimicking
objective, we draw input sequence x’s whose tokens are independent and uniform over the vocabulary,
and then compute the the Kullback–Leibler divergence between each row of Attn(l)
1 (x) and the
corresponding rows of ˜M , ˜A. Thus the overall mimicking loss is

0 (x), Attn(l)

0 (x), Attn(l)

Lmimic =

E
rand. seq. x

(cid:34)L−1
(cid:88)

l=0

1
T

T
(cid:88)

t=1

(cid:20)

KL

(cid:18)

Attn(l)

(cid:13)
(cid:13)
˜M [t, :]
0 (x)[t, :]
(cid:13)
(cid:13)

(cid:19)

(cid:18)

+ KL

Attn(l)

(cid:13)
(cid:13)
˜A[t, :]
1 (x)[t, :]
(cid:13)
(cid:13)

(cid:19)(cid:21)(cid:35)

Note that the above mimicking loss pertains only two attention heads per layer and we leave out all the

20

          H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   Q              H S R F K 5 D Q G , Q W  $ O E H U W   Q              H S R F K 3 U H W U D L Q H G  % ( 5 7   Q              H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   Q              H S R F K 5 D Q G , Q W  $ O E H U W   Q              H S R F K 3 U H W U D L Q H G  % ( 5 7   Q              H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   Q              H S R F K 5 D Q G , Q W  $ O E H U W   Q              H S R F K 3 U H W U D L Q H G  % ( 5 7   Q     Y D U L D E O H                rest. Then the mimicking procedure boils down to updating the transformer model’s parameters to minimize
the Lmimic. We ﬁnd that a vanilla Adam optimizer drives the mimicking loss down to near zero in a negligible
amount of time compared to large-scale pre-training, even for large models such as BERT and ALBERT.

C Details of the hybrid convolutional transformer experiments

C.1 Architecture details

In the original multi-head attention module of transformers, we have four matrices WQ, WK, WV , WO ∈ Rd×d
as learnable parameters. On a token sequence hin ∈ Rd×T , the module computes the output as

hout = WO MHA (WQhin, WKhin, WV hin)

where MHA : Rd×T × Rd×T × Rd×T → Rd×T is the multi-head attention function that takes query, key,

value as inputs.

In our proposed convoluitonal attention module, we keep all the existing components of the original
multi-head attention module but introduce three depth-wise temporal convolution operators DepthConvQ,
DepthConvK, DepthConvV , before applying the WQ, WK, WV maps, where k is the length of each ﬁlter.
Diﬀerent from the vanilla full convoluitons, a depth-wise convolution treats each dimension of the input
separately, i.e., applies a single convolutional ﬁlter along the time axis for each dimension of the input
sequence5. As a result, each depth-wise convoluiton operators contains d × k × k learnable parameters.
Overall, the convolutional attention module computes the output as

hout = WO MHA (cid:0)WQDepthConvQ(hin), WKDepthConvK(hin), WV DepthConvV (hin)(cid:1)

Notably the depthwise convolution modules incur relatively few extra parameters to the model, since each
of WQ, WK, WV , WO matrices contain d×d learnable parameters. This provides a signiﬁcant parameter count
advantage over full convoluiton opertors which would have d × d × k × k parameters. In our experiments,
we ﬁnd that full convoluitons usually lead to a slightly better result but make the models’ paramter counts
signiﬁcantly larger. Thus we choose depth-wise convolutions as a trade-oﬀ.

C.2 Hybrid convolutional transformers on LEGO

As a sanity check, we evaluate the convolutional variants of BERT and ALBERT trained from random
initialization on the LEGO task. In Figure 13, we report the results for both classical generalization and
out-of-distribution generalization. We show that in spite of being randomly initialized, the convolutional
models greatly resemble the behaviors of pretrained models in that they avoid shortcut solutions as well as
generalize better to longer sequences than their non-convolutional, randomly initialized counterparts. This
is another strong evidence supporting our hypothesis on the role of pretraining.

C.3 The restricted C dataset and training hyper-parameters

We evaluated the convolutional version of BERT architecture with various ﬁlter sizes on the task of executing
C programs (see Section 5 and Figure 8. The details of training and dataset are provided below.

We use the open source restricted C dataset from [CST21] consisting of 500K/1K/1K training/valida-
tion/test snippets of C programs. Each program is also provided with 5 pairs of input and output arrays of 5

5we implement this using the PyTorch torch.nn.Conv1d module with the number of groups set equal to the number of input

channels.

21

(a)

(b)

Figure 13: Hybrid convolutional models on LEGO Task 1: (a) n = ntr = 12 (classical generalization), and (b)
n = 12, ntr = 6 (generalization to unseen lengths). We see that hybrid convolutional transformers avoid shortcuts
and (almost) match the performance of pretrained models.

variables. Both the input values and ﬁnal values are guaranteed to be integers between −4 and 4 inclusively.
For our purpose, we consider the task of predicting variables’ values in the output array, given the input
array and the text of the program itself, largely resembling the task of LEGO. We treat each pair of input
and output arrays as independent sample, even though diﬀerent pairs may come from the same program.
Since the output variables can only take 9 diﬀerent values (integers among -4 and 4), we use a 9−way softmax
classiﬁer to generate predictions.

For all the experiments on restricted C dataset, we train all the models from random initialization for
20 epochs using the Adam optimizer with batch size of 500 samples, 5 × 10−5 learning rate, β1 = 0.9,
β2 = 0.999, (cid:15) = 1 × 10−8, and cosine learning rate schedule with Tmax = 20. For each conﬁguration, we
conduct 3 independent runs, and report the standard deviations as error bars. We present the results in
Figure 8 in the main paper.

D Eﬀect of length of LEGO chains and depth of model

All our experiments in the main paper were on a typical instance of LEGO task with length n = 12 chains
and standard BERT and ALBERT models of depth D = 12. In this Appendix, we brieﬂy explore the eﬀect of
the LEGO chain length (n, ntr) and transformer models depth D. The chain structure of information ﬂow in
our LEGO Task 1 would suggest that for a transformer network of depth D, the learning and generalization
on LEGO task would crucially depend on its maximum chain length n. If n (or importantly ntr) is too small,
the training data might not have enough information to guide generalization to longer lengths, on the other
hand, if n is too large, the model might not be able to propagate information along the chain in a natural
iterative manner. For example, implementing a “natural” iterative algorithm of resolving one clause of the
task at a time (as described in the beginning of Appendix A) would require models of depth D ≥ n.

We study this behavior by ﬁrst repeating the experiments from our main paper on depth D = 12 models
on LEGO tasks of varying lengths n. In all the cases, we proportionally increase the length of chain for
which supervision is provided by training on ﬁrst ntr = n − 6 clauses in the chain.

22

100101102epoch50%60%70%80%90%100%test accConvolutional BERT100101102epochConvolutional ALBERTvariable #01234567891011100101102epoch50%60%70%80%90%100%test accConvolutional BERT100101102epochConvolutional ALBERTvariable #01234567891011In Figure 14, we show the performance of a typical run of transformer models in this setting. When trained
on small length chains of n = 8 and ntr = 2, we indeed see that all the models including pretrained-BERT is
able to learn the short training length (classical generalization) but the supervision does not contain enough
information for the models to learn to generalize to unseen lengths. On the other hand, for larger chain
lengths we see that the generalization only gets better, with a very strong monotonic trend for pretrained-
BERT models. In particular, the strong generalization performance at n = 20 with ntr = 14 would suggest
that these models might not really be implementing the “natural” iterative solution for the task. Rather, it
is possible that training on longer sequences leads the models to learn a more compact representation that
nevertheless generalizes remarkably well to much longer lengths than seen during training.

(a) n = 8

(b) n = 12

(c) n = 16

(d) n = 20

Figure 14: Generalization performance of BERT and ALBERT models (depth D = 12) trained on LEGO task of
varying chain lengths n: (a) n = 8, (b) n = 12, (c) n = 16, (d) n = 20.

23

          H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   Q             H S R F K 5 D Q G , Q W  $ O E H U W   Q             H S R F K 3 U H W U D L Q H G  % ( 5 7   Q    Y D U L D E O H                    H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   Q              H S R F K 5 D Q G , Q W  $ O E H U W   Q              H S R F K 3 U H W U D L Q H G  % ( 5 7   Q     Y D U L D E O H                          H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   Q              H S R F K 5 D Q G , Q W  $ O E H U W   Q              H S R F K 3 U H W U D L Q H G  % ( 5 7   Q     Y D U L D E O H                                  H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   Q              H S R F K 5 D Q G , Q W  $ O E H U W   Q              H S R F K 3 U H W U D L Q H G  % ( 5 7   Q     Y D U L D E O H                                Complementing our results on varying the length of LEGO chains, we also look at how BERT and
ALBERT architectures of smaller depth D < 12 learn our LEGO task of length n = 12. In Figure 15 we
show the results of models trained from random initialization (we do not have pre-trained models at these
depths). Here we do see the trend that larger depth improves generalization. In particular, for a task of
chain length n, there does appear to be a minimum threshold of D at which the models learn to generalize
even in the classical sense (on lengths the models were trained on), but this relationship appears sub-linear
rather than linear with D ≥ n as speculated by the “natural” iterative algorithm. It would be of interest in
future studies to explore this relation between depth and length better.

(a) D = 2

(b) D = 4

(c) D = 8

(d) D = 12

Figure 15: Rand-Init BERT and ALBERT models of varying depth D trained on LEGO tasks of length n = 12.

24

          H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   '             H S R F K 5 D Q G , Q W  $ O E H U W   '             H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   '             H S R F K 5 D Q G , Q W  $ O E H U W   '             H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   '             H S R F K 5 D Q G , Q W  $ O E H U W   '             H S R F K                    W H V W  D F F 5 D Q G , Q W  % ( 5 7   '              H S R F K 5 D Q G , Q W  $ O E H U W   '     Y D U L D E O H                E LEGO Task 2: dihedral group

As a generalization of the main task (i.e., LEGO Task 1) analyzed so far, we present LEGO Task 2 of learning
the dihedral group6 D3 of order 6 which is isomorphic to the symmetric group S3. Note that LEGO Task
1 can be viewed as learning the dihedral group D1 of order 2. Clearly, the shortcut solution described in
Section A is not valid here.

We repeat the out-of-distribution generalization experiments on Task 2 with the exact same model con-
ﬁgurations and training hyper-parameters as Task 1 (see Section B.2). For dataset creation, we largely follow
the pipeline detailed in Section B.1 with group elements from D3 for which we create corresponding tokens.
The only modiﬁcation here is that the labels are categorical with 6 classes, since every variable in Task 2
may take 6 candidate values.

Figure 16: Out-of-distribution generalization on LEGO Task 2. Task 2 appears to be signiﬁcantly more challenging
than Task 1 and pretraining plays a more important role on top of rand-init models, while the mimicking procedure
is able to match and even outperform pretraining.

In Figure 16, we show these preliminary results and observe that Task 2 is indeed more challenging
than Task 1, and the extent to which pretraining provides beneﬁts is noticeably larger. Without further
hyper-parameter tuning, the randomly initialized models even face optimization issues in ﬁtting the training
labels. Interestingly, we also ﬁnd our proposed mimicking procedure introduced in Section B.3 is not only
able to match pretraining’s performance and also outperform it.

So far, none of the models here is capable of non-trivially generalizing to more than one extra variable.
It is an important future direction to search for suitable adaptations to the current transformer architec-
tures/training algorithms that can eventually solve this task.

F Experiments with GPT-3

We tried a zero shot learning with GPT-3 on the OpenAI website7. The red text is the completion given by
the “text-davinci-002” model with Temperature = 0 and Top P = 0:

6https://en.wikipedia.org/wiki/Dihedral_group
7https://beta.openai.com/playground/p/default-grammar

25

10010120%40%60%80%100%test accPre-Trained BERT100101102Rand-Init BERT100101102Mimicking BERT10010110220%40%60%80%100%test accPre-Trained ALBERT100101102Rand-Init ALBERT100101102Mimicking ALBERTvariable #01234567891011a=+1; b=-a; c=-b; d=+c; e=+d; f=-e; g=-f; h=+g; i=+h; j=-i; k=-j; l=+k; m=+l;
n=-m; o=+n; p=-o; q=-p; r=+q; s=+r; t=-s; u=+t; v=-u; w=-v; x=+w; y=+x; z=-y;

a=1; b=-1; c=1; d=1; e=1; f=-1; g=-1; h=1; i=1; j=-1; k=-1; l=1; m=1;
n=-1; o=1; p=-1; q=-1; r=1; s=1; t=-1; u=1; v=-1; w=-1; x=1; y=1; z=-1

Note that the variable g is computed incorrectly (it is the ﬁrst variable computed incorrectly in the chain

of variables).

Next, we added the text “Compute the variables from left to right:” in front of the previous

prompt:

Compute the variables from left to right:

a=+1; b=-a; c=-b; d=+c; e=+d; f=-e; g=-f; h=+g; i=+h; j=-i; k=-j; l=+k; m=+l;
n=-m; o=+n; p=-o; q=-p; r=+q; s=+r; t=-s; u=+t; v=-u; w=-v; x=+w; y=+x; z=-y;

a=1; b=-1; c=1; d=1; e=1; f=-1; g=1; h=1; i=1; j=-1; k=1; l=1; m=1;
n=-1; o=1; p=-1; q=1; r=1; s=-1; t=1; u=1; v=-1; w=1; x=1; y=-1; z=1

This improves the performance but the variable o is computed incorrectly (it is the ﬁrst variable computed
incorrectly in the chain of variables). Adding example computations (few show learning) did not help to
improve the performance.

There are 3 signiﬁcant diﬀerences between our setup and GPT-3. First, GPT-3 doesn’t require any
ﬁnetuning or training on our synthetic reasoning task. Second, the architecture of GPT-3 is entirely a
decoder-based architecture with a causal attention mask while all our architectures have bidirectional atten-
tion. Third, the text generation done by GPT-3 proceeds in a loop by executing the model and generating
tokens in a sequential manner. Our models generate the token embeddings all at once. This makes the
generation process of GPT-3 more powerful as it doesn’t need to compute the values of the variables all at
once but might do that sequentially.

G Discussion of limitations

Our current work focuses on a synthetic task for logical reasoning. Even though we have gained valuable
insights into Transformer models’ behavior, it remains to be shown whether (or how much of) these insights
carry on to general natural language tasks. Furthermore, so far we have used two representative Transformer
architectures, namely BERT and ALBERT, throughout our investigation. It is well-known that these model
are trained with the masked token prediction objective, and thus they tend to behave diﬀerently from the
ones trained with next token prediction objective such as the GPT models [BMR+20b]. It is interesting
to see how these two types of Transformers perform diﬀerently on LEGO as well as other logical reasoning
tasks. The aforementioned two main limitations shed light on directions for future works that we believe are
of great importance.

26

H Attention maps of pretrained BERT on LEGO

layer 0

layer 1

layer 2

layer 3

layer 4

layer 5

27

[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 0 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 1 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 2 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 3 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 4 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 5 Head 11layer 6

layer 7

layer 8

layer 9

layer 10

layer 11

28

[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 6 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 7 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 8 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 9 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 10 Head 11[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 0[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 1[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 2[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 3[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 4[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 5[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 6[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 7[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 8[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 9[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 10[BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS][BOS]a=+1,b=+a,c=-b,d=+c,e=-d,f=-e,g=+f,h=-g,i=-h,j=-i,k=+j,l=-k,[EOS]Layer 11 Head 11