Noname manuscript No.
(will be inserted by the editor)

Embedding API Dependency Graph for Neural Code
Generation

Chen Lyu1 · Ruyun Wang1 ·
Hongyu Zhang2 · Hanwen Zhang3 ·
Songlin Hu4,5

Received: date / Accepted: date

Abstract The problem of code generation from textual program descriptions has
long been viewed as a grand challenge in software engineering. In recent years,
many deep learning based approaches have been proposed, which can generate
a sequence of code from a sequence of textual program description. However, the
existing approaches ignore the global relationships among API methods, which are
important for understanding the usage of APIs. In this paper, we propose to model
the dependencies among API methods as an API dependency graph (ADG) and
incorporate the graph embedding into a sequence-to-sequence (Seq2Seq) model.
In addition to the existing encoder-decoder structure, a new module named “em-
bedder” is introduced. In this way, the decoder can utilize both global structural
dependencies and textual program description to predict the target code. We con-
duct extensive code generation experiments on three public datasets and in two
programming languages (Python and Java). Our proposed approach, called ADG-

Chen Lyu ((cid:0))
E-mail: lvchen@sdnu.edu.cn

Ruyun Wang
E-mail: ruyunw@outlook.com

Hongyu Zhang
E-mail: hongyu.zhang@newcastle.edu.au

Hanwen Zhang
E-mail: zhanghanwen0726@gmail.com

Songlin Hu
E-mail: husonglin@iie.ac.cn

1.School of Information Science and Engineering, Shandong Normal University, Jinan, China.
2.The University of Newcastle, Callaghan, NSW, Australia.
3.Big Data Center of Shandong Province, Jinan, China.
4.Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China.
5.School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China.

1
2
0
2

r
a

M
9
2

]
E
S
.
s
c
[

1
v
1
6
3
5
1
.
3
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Chen Lyu1 et al.

Seq2Seq, yields signiﬁcant improvements over existing state-of-the-art methods
and maintains its performance as the length of the target code increases. Ex-
tensive ablation tests show that the proposed ADG embedding is eﬀective and
outperforms the baselines.

Keywords Code Generation · Program Synthesis · API Dependency Graph ·
Graph Embedding · Deep Learning

1 Introduction

An ultimate goal of computer programming is to let computers to generate pro-
grams automatically. Automatic code generation from textual program descrip-
tions1 is a very challenging task. For example, given a textual program description
“Binds a Hibernate Session to the current thread”, it is challenging for a com-
puter to automatically generate the target code. Despite the challenge, researchers
are exerting considerable eﬀort towards achieving this goal.

Recently, neural machine translation (NMT) based approaches have been in-
creasingly applied to code generation [1–10]. These approaches generally consider
code generation as a sequence learning problem. A mature deep learning model
called encoder-decoder, also known as the sequence-to-sequence (Seq2Seq) model
[11–14], has been used. In a Seq2Seq model, the textual program description and
the target code are regarded as two sequences. The model is trained by feeding the
description sequence, and then predicts the target code sequence. In this way, a
textual program description can be translated into the corresponding target code
through a deep neural network (i.e., neural code generation).

Due to the diﬀerences between textual program descriptions (e.g., in English)
and target code (e.g., in Java), the existing approaches focus on improving the
traditional Seq2Seq model to better support neural code generation [13–18]. For
example, Sun et al. [4] proposed a convolutional neural network (CNN) module
in their Seq2Seq-based code generation approach. In particular, the authors used
the abstract syntax tree (AST) to represent the target code and added three
tree-based CNN decoders. As a result, the performance of their approach was
greatly improved. To alleviate the long-range dependency problem, Sun et al. [5]
further proposed TreeGen, a tree-based transformer architecture for neural code
generation. However, the existing approaches mostly use AST to represent the
target code and lack in-depth exploration of program structure. They ignore the
dependencies among the API methods, which were found useful for deep learning
models to understand program semantics [19].

In this paper, to better incorporate the inherent semantics of APIs in neural
code generation, we introduce a model based on API dependency graph (ADG),
which embeds the information about the API dependency structure into the Seq2Seq
model for code generation. We focus on the dependencies among the APIs, includ-
ing the following: 1) The invocation constraints for each API (i.e., if and only if
all input parameters of the current API are provided can the API be invoked).
Embedding the constraints into the program representation can guide the model
to learn the correct API usage patterns so that illegal API calls (e.g., those with

1 A textual program description can be a natural language description of requirements or a

structured speciﬁcation.

Embedding API Dependency Graph for Neural Code Generation

3

incomplete input parameters) can be removed. 2) The invocation order of the
API sequence. Embedding API invocation order enables the model to generate
the target code in the correct order. From a graph perspective, the above API de-
pendency structure of source code usually appears in the form of directed acyclic
graph (DAG), which brings new challenges.

In this paper, we propose an ADG-based Seq2Seq (ADG-Seq2Seq for short)
approach to generating target code from textual program descriptions. The ADG-
Seq2Seq architecture consists of three components: an encoder, an embedder, and
a decoder. The encoder generates a vector representation of a textual program de-
scription. The embedder embeds the nodes of the ADG into vector representations.
The decoder then decodes the corresponding vector representations generated by
the encoder and the embedder to predict the target code sequence. Unlike the
existing research that represents an entire API sequence as a vector, this paper
aims to encode the structural information about API dependencies into each API.
In particular, we adopt a more ﬁne-grained representation of API node embedding
so that the decoder can learn more sophisticated program semantic information.
To evaluate the eﬀectiveness of our model, we perform extensive experiments on
public benchmarks and compare our model quantitatively and qualitatively with
the state-of-the-art models. The evaluation results show that the proposed ADG-
Seq2Seq model signiﬁcantly outperforms the state-of-the-art models in terms of
eight metrics (accuracy, BLEU, F1, CIDEr, RougeL, Rouge1, Rouge2 and RIBES)
by large margins (ranging approximately from 9% to 15%) on two Java datasets.
It also achieves comparable results on the Python dataset. Furthermore, exten-
sive ablation tests show that our ADG embedding algorithm is eﬀective and can
signiﬁcantly improve the performance of code generation.

This paper makes the following contributions.

– We propose ADG-Seq2Seq, a new approach to neural code generation, which
builds an end-to-end encoder-embedder-decoder architecture that utilizes both
textual program descriptions and API dependency information.

– We develop a novel ADG embedding algorithm that can convert an ADG into

vectors while preserving the structural information implied in code.

Paper Organization The rest of this paper is organized as follows. Section 2
surveys the related work and describes the background of our research. Section 3
presents an overview of the proposed model. Section 4 describes the details of
the ADG-based embedder. Section 5 provides the experimental settings. Section 6
states three research questions (RQs), followed by experiments in Section 7. Sec-
tion 8 presents the qualitative analysis. Section 9 discusses the strengths and weak-
nesses of our approach and threats to its validity. Finally, Section 10 summarizes
this paper with some concluding remarks.

2 Background and Related Work

2.1 Graph Embedding

Graph embedding has proven to be very useful in a variety of prediction tasks.
Many graph embedding algorithms for learning over graphs have been proposed,
such as graph neural network (GNN) [20], DeepWalk [21], Node2vec [22], graph

4

Chen Lyu1 et al.

convolutional network (GCN) [23], graph sample and aggregate (GraphSAGE)
[24], and graph attention network (GAT) [25]. These methods embed a node into
a vector that contains the graph information and can even infer unseen nodes or
graphs by aggregating the information of neighbourhoods.

Encouraged by the development of graph embedding algorithms, researchers
[26–28] proposed to embed source code structure by treating it as a graph. For
example, Gu et al. [26] used a graph embedding algorithm to select API usage
examples. In this study, object usage graphs were explored to model the source
code; the graph kernel is used to embed the structural semantics of graphs into a
high-dimensional continuous space, and it is demonstrated that such an embedding
can preserve many aspects of the original graph and capture structural information
such as loops, branches, and third-party method invocations in source code. Li
et al. [27] proposed a gated-GNNetwork (GGNN) using gated recurrent units to
detect null pointers. The researchers represented the memory state of a program as
a graph, with a graph node representing the memory address where the pointer is
stored and a graph edge representing the value of the pointer. GGNN is then used
to predict the program’s Hoare logic to determine whether the program is correct
or not: e.g., whether the memory is safe. Allamanis et al. [28] used GGNN to
learn representations of C# programs. These authors represented the source code
as an AST-based graph, where each node in the graph represents a token in the
code. The edges of the graph represent the associations between nodes in the AST
and the data dependencies between variables. The method uses GGNN to learn
the graph representation of the code and obtain a feature vector representation
of each token in the code. The framework based on this representation model
achieves satisfactory accuracy in the tasks of variable naming and variable misuse
correction.

Some researchers also applied graph embedding algorithms to control/data
ﬂow graph of source code. For example, Phan et al. [29] converted C programs to
assembly language code and built control ﬂow graphs, which were then analysed
using GCN and applied to program defect detection. Li et al. [30] proposed a deep
learning-based graph matching network (GMN) to determine graph similarity.
GMN is applied to check the similarity between control ﬂow graphs and shows
better detection ability than the GNN-based model. Wang et al. [31] proposed
a GNN-based framework for detecting code clones. The framework constructs a
graph representing the ﬂow-augmented AST by adding edges representing the
control and data ﬂows to the AST. Brockschmidt et al. [32] added edges to an
AST to build a program graph that can model the structure and data ﬂow of a
partial program. GNNs are then used to learn a program representation on such
a graph for code completion.

In the models discussed above, the graph embedding algorithms were mainly
applied to object-usage graph, control/data ﬂow graph, and ﬂow-augmented ASTs.
In contrast to these work, we focus more on two types of API dependencies in
source code, namely API invocation constraints and invocation order. The goal
is to build a new graph model to describe the unique structural and semantic
information implied in API dependencies. Inspired by GNN, GCN, and Graph-
SAGE, we design an API dependency graph (ADG) based model, which can be
used together with a Seq2Seq model for code generation.

Embedding API Dependency Graph for Neural Code Generation

5

2.2 Neural Machine Translation Models

Neural machine translation models in natural language processing (NLP) ﬁeld
provide helpful inspiration for the code generation task. Recurrent neural network
(RNN) [33] and it variants, especially long short-term memory (LSTM) [34], are
commonly used as the encoder and decoder in the Seq2Seq model [11]. The Seq2Seq
model transforms a source sequence into hidden vectors from which the decoder
generates the corresponding target sequence word by word. Cho et al. [14] were
the ﬁrst to propose an RNN-based encoder-decoder neural network and used it for
machine translation, laying a solid foundation for subsequent studies on Seq2Seq.
Sutskever et al. [11] presented a general end-to-end approach to sequence learning
and used two layers of LSTM as the encoder and decoder. The seq2seq model has
achieved good results in machine translation.

However, one disadvantage of the Seq2Seq model is that the last hidden state
of the RNN in the encoder, namely the context vector, contains only a limited
amount of information. The greater the text length is, the more the information
could be lost; thus, the Seq2Seq model does not perform well on long text genera-
tion tasks [35]. The attention mechanism [36] can improve neural machine trans-
lation by selectively focusing on parts of the source sentence during translation.
For example, Luong et al. [37] proposed an attention-based Seq2Seq approach,
which applied both global and local attention mechanisms. Vaswani et al. [38]
proposed the Transformer architecture, which is based solely on attention mech-
anisms. Currently, the Seq2Seq model with attention has been widely adopted in
neural machine translation.

2.3 Code Generation Models

Over the years, many grammar-based code generation models have been proposed.
As early as 1989, Alfred V. Aho at AT&T Bell Labs proposed a code generation
approach using grammar-tree matching [39]. AST [16] and other grammar based
models [3, 4, 6, 10] have also been developed. Dong and Lapata [10] proposed an
approach to generating target code that could capture the syntactical structure
of a program along with the AST model. Quirk et al. [9] presented an approach
that learned to map a textual program description from simple “if-then” rules
to executable code. All the above-mentioned approaches have achieved relatively
good performance for speciﬁc program languages. However, these approaches are
primarily based on rules and human-deﬁned features and are restricted to speciﬁc
applications.

Recently, deep learning based approaches have been proposed for code gen-
eration. For example, Mou et al. [7] envisioned an end-to-end program genera-
tion scenario using RNN. However, the RNN-based model encountered diﬃculty
in handling long-term dependencies. The long dependency problem is a common
dilemma, even with LSTM. Therefore, Sun et al. [4] replaced LSTM in the decoder
with a CNN to capture features in diﬀerent regions via a sliding window. Sun et al.
[5] further proposed a tree-based transformer architecture for code generation that
alleviated the long dependency problem with the attention mechanism in Trans-
former. The above methods mainly use AST as the source code representation and

6

Chen Lyu1 et al.

learn syntactical features from the ASTs. Source code is then generated based on
the learned features.

Although the current approaches have achieved good results in code generation
tasks, they ignore the importance of API dependencies in source code. They do not
adequately consider whether the generated program satisﬁes API dependencies,
i.e., a set of structural and semantic constraints such as “do not use undeclared
or unprovided variables as arguments for method invocations”, “invoke APIs with
the correct types of variables/parameters”, and “follow the legal order in which
the program invokes API methods”. Representing and learning these constraints
is challenging. Murali et al. [40] discussed this challenge and proposed to generate
programs based on code skeletons. Their proposed method ﬁrst generates a code
skeleton from a deﬁned semantic label and then converts it into code. However,
their method does not accomplish the task of generating code from textual program
descriptions. Furthermore, the dependency between the two APIs in the code
skeleton is speciﬁed by sibling and descendant relationships, which is diﬀerent
from our deﬁnition of API dependencies. In this paper, our proposed ADG-Seq2Seq
takes a global perspective and uses ADG to model API dependencies. A new ADG
embedding algorithm is proposed to integrate such structural information, which
is shown to be eﬀective for learning API usage and code generation.

3 Proposed Model

Fig. 1: Overall framework of our approach. This ADG-embedding-based
Seq2Seq framework can learn the structure of the program-oriented graph to
generate target code automatically from its corresponding textual program
description.

Y''Y'Ageless Entity NAME_END 4 ATK_END 4 DEF_END {3}{G}{G} COST_END ... ...public class AgelessEntity extends CardImpl { public AgelessEntity(UUID ownerId){ this.expansionSetCode = "DDH"; this.subtype.add("Elemental");……}s0s1smy0y1ym...h1h2hnx1x2xn...LSTMLSTMLSTMLSTMLSTMLSTMXCode SnippetProgram DescriptionAPI Dependency Graph modelingDecoderEncoderAPI Dependency Graph......Word EmbeddingAttention...y''y''0y''1y''dLSTMLSTMLSTMEmbedder......Graph EmbeddingCode EmbeddingTarget Code0c0cm-1...0c0cm-1FFN + SoftmaxEmbedding API Dependency Graph for Neural Code Generation

7

In this paper, we propose an ADG-Seq2Seq model for code generation. Our
model integrates Seq2Seq with a new module called embedder to incorporate API
dependency information that is captured by an ADG. The overall structure of the
proposed approach is shown in Fig. 1. Speciﬁcally, the encoder accepts the em-
beddings of a textural program description. The decoder fuses the target code em-
beddings (involving node embeddings produced by the embedder for API methods
and code embeddings for other program tokens) with the context vectors (gener-
ated by an attention mechanism) to generate the corresponding code sequence.
In an ADG, each node represents an API method, and each node embedding is
a node vector representation generated by the ADG embedding algorithm. Since
node embeddings incorporate neighbouring nodes, associated edges and their de-
pendencies, properties such as ADG graph structure and node dependencies are
preserved. Therefore, using node embeddings as inputs to the decoder helps cap-
ture accurate information about API dependencies and program structure. Such
an encoder-embedder-decoder framework enables the decoder to embed more in-
formation from the input and, consequently, generate a more semantically inherent
method invoking sequences to form the entire program. We describe the proposed
model in detail in this section.

3.1 Encoder

The tokens w1, w2, · · · , wt in a textual program description sentence will be em-
bedded into the vectors x1, x2, · · · , xt through a lookup table LU T ∈ RV ×d, where
V is the vocabulary size. The encoder is deﬁned as

ht = LST Menc(xt, ht−1)

(1)

where ht is the hidden state of input vector xt, t is the length of xt, and LST Menc
is the mapping function between the hidden states and the input sequence. The
encoder receives each word embedding from the input sequence along with the
hidden state at the previous time and then outputs the hidden state at the current
time.

Speciﬁcally, we embed the input sequence by indexing each word in the dictio-
nary and then obtain the vector sequence x1, x2, ..., xt. Moreover, we use a rectiﬁed
linear unit (ReLU) activation function to connect each neural network layer:

t = ReLU (W l[xl−1
xl

t−s, ..., xl−1

t+s])

(2)

where l = 1, 2, ..., L indicates the layer in the neural network.

To reinforce the memory of future and historical information in the input
sequence, we feed the extracted features x1, x2, ..., xt into LSTM. This approach
can connect input vectors to the cell state, which can receive information processed
by LSTM. The above process of LSTM can be formulated as:







it
ft
ot
(cid:102)Ct













=







σ
σ
σ
tanh







·

[ht−1, xt] ·













Wi
Wf
Wo
Wc

+



















bi
bf
bo
bc

Ct = ft ∗ Ct−1 + it ∗ (cid:102)Ct

(3)

(4)

8

Chen Lyu1 et al.

ht = ot ∗ tanh(Ct)

(5)

where it, ft, ot, (cid:102)Ct, ht denote the input, forget, output, memory gates and hidden
states of each LSTM unit; Wi, Wf , Wo, Wc denote the weight matrices of input,
forget, output and memory gates of each LSTM unit; and bi, bf , bo, bc denote the
bias terms of the input, forget, output and memory gates of each LSTM unit,
respectively. σ is a sigmoid function used to calculate a number between 0 and 1,
where 1 is fully reserved and 0 is completely discarded.

3.2 Embedder

The embedder aims to generate graph embeddings by converting each node in
the ADG into a vector expression; then, the decoder uses the vector expressions to
gather program’s structural information, i.e., API dependencies. To further exploit
the global and sequential information of programs, we design an embedder to
represent the API dependencies implied in the target code based on its dependent
class libraries and utilize the ADG embedding algorithm to vectorize the nodes in
the graph.

Each node m ∈ M in a code snippet is represented through a one-hot encoding
h0
m. To incorporate the graph information, we present the ADG embedding, which
fuses node m and its neighbours’ information; this process is described in detail
in Section 4. The graph embedding of node m that contains graph information is
represented as hk

m, where k is the hop size.

m = LST Memb(hk−1
hk

U

)

(6)

u1 , hk−1

u2 , · · · , hk−1

U = (hk−1

Here, hk−1
un ) represents the graph embeddings of neigh-
bours of node m in the k-1th update; u1, u2, · · · , un ∈ ordersetm; and hk
m is the
graph embedding of node m in the kth update. In this study, we set the hop size
to k = 2.

To incorporate the embedder into the decoder, the decoder selectively adopts
the node embedding of the embedder’s output as its query (the token to be ex-
panded in the code sequence). Speciﬁcally, when the token in the query represents
an API method, the query adopts a node embedding vector (generated by the
embedder); otherwise, the query adopts a word embedding vector.

3.3 Decoder

The decoder is designed to generate the target code sequence based on its textual
program description. The core network of the decoder is LSTM, which produces
the t-th token yt by predicting its probability as follows:

p(yt | y<t, x) ∼ Sof tmax(Linear(f (yt−1, ht, ct, st)))

(7)

where y<t (i.e., {y1, y2, ..., yt−1} denotes the previously generated partial code,
Linear is a linear transformation function in the decoder, f represents the LSTM
activation function, yt is a word token feature or a node/method feature generated

Embedding API Dependency Graph for Neural Code Generation

9

by the graph embedding algorithm of the ADG (see Section 4.2), ht is the tth hid-
den state computed by the encoder, ct denotes the tth context vector computed by
the attention mechanism, and st is the tth hidden state computed by the decoder.
The attention mechanism proposed by Mnih et al. [36] utilizes a matching
function and the tangent change to calculate the weight values corresponding to
textual program description features and target code features, thereby emphasizing
the part that has the greatest impact on the current target code feature.

In the traditional Seq2Seq neural network, the encoder of the input sequence
is a ﬁxed-length hidden vector. Nevertheless, it might be inadequate to encode the
input sequence into a ﬁxed length and assign the same weight to each word when
decoding. Hence, we add a global attention mechanism [36] that calculates the
weight for the importance-based current input to our network. This mechanism is
shown in Fig. 2.

Fig. 2: Overview of the attention mechanism. The input of the encoder is
distributed with diﬀerent weights according to each input of the decoder.

In our approach, the attention mechanism is used to calculate a match for the
output of the current decoder and encoder at each moment. Given the hidden states
h1, h2, ..., ht for each output of the encoder and the current state si, ∀i ∈ {0, ..., t}
for the output of the decoder, where s0 is the output vector initialized by the ﬁrst
time step of the decoder, the matching value is computed as:

i = hT W si
ut

(8)

where W represents a weight matrix. We then can normalize the matching degree
and calculate the weight of each hidden state by:

αt

i = sof tmax(ut
i)

(9)

where αt
Afterwards, we calculate the sum of weight vectors by:

i is a number between 0 and 1, and is the weight value for each input.

ct = Σiαt

ihi

(10)

20304010ˆ20ˆ30ˆ40ˆEncoderDecoderHidden states10h1h2h3h4sos1c0ContextSummationWeight0.10.50.10.3SoftmaxMatch10

Chen Lyu1 et al.

The calculated vector sum is concatenated with the input vector in the next time
step of the decoder to obtain a new input to the decoder. This completes the loop
process of the attention mechanism.

With the attention mechanism, for each input vector of the decoder, diﬀerent
weights are assigned to the hidden state of the encoder at each moment. By re-
taining the weighted hidden states of the encoder LSTM at each time step, the
attentive encoder can selectively learn the current input textual program descrip-
tion to correlate the output code of the attentive decoder with its most relevant
description word.

st = LST Mdec(yt−1, ht, ct)

(11)

where st is the output of the decoder at time step t. If yt−1 indicates to a node
m ∈ M in the ADG, the input to the decoder of the next time step is replaced
with node m’s graph embedding hk
m, which has the same dimension as vector yt.

Finally, a fully connected feedforward network with softmax follows:

p(yt | yt−1) = Sof tmax(F F N (st, ct))

(12)

Using this network, we can obtain the generated corresponding token by mapping.
Speciﬁcally, we concatenate the outputs of the decoder with the corresponding
context vectors. They are fed to a two-layer perceptron, where a softmax activation
function is used in the last layer to predict the probability of the next token, as
shown below:

p(yi | ·) =

expc(M LP )i
j=1expc(M LP )j

ΣR

(13)

where C(M LP )i is the input logic of the softmax function, and R is the number
of candidate tokens. By mapping to the corresponding token, the source code is
generated eventually. The evaluation stops if token “(cid:104)EOS(cid:105)” is generated or the
output reaches the maximum length.

3.4 Model Training

We pre-process the inputs of the encoder as follows: replace special characters with
spaces; number and count each word simultaneously. In addition, for the target
code input by the decoder, we analyse and extract the API dependencies from the
target code.2 The resulting APIs are classiﬁed according to their input and output
parameter types. This classiﬁcation information is used in the subsequent training
for graph embedding.
Loss Function During training, a uniﬁed loss function is used to perform end-
to-end joint training of the encoder-decoder network and the ADG embedding
network. The loss is backpropagated to the encoder, embedder, and decoder si-
multaneously. In other words, the parameters in the three networks are adjusted
together according to a single cross-entropy loss function deﬁned by:

Loss = −

1
|y|

|y|
(cid:88)

t=1

log p(yt | y<t, x)

(14)

2 Speciﬁcally, we

resort

to

Spoon

(http://spoon.gforge.inria.fr/)

and

Javassist

(http://www.javassist.org/) for Java-based programs’ analysis.

Embedding API Dependency Graph for Neural Code Generation

11

The essence of joint training is a gradient descent regression of parameters
using uniform losses for the three networks, i.e., encoder, embedder, and decoder.
Joint training implies that the graph embedding network is not trained separately.
The graph embedding vector is diﬀerent for the same node in each iteration, and
the embedder’s weights are updated with the entire model for each batch. Take one
training batch as an example: the encoder receives the textual program description
as an input, and the decoder receives code as an input query. We iterate through
the current sequence of input code tokens. If the current token is an API method,
we use the node embedding as input for the current time step of the decoder.
Otherwise, we use the code embedding of the token as the input. The cross-entropy
loss function is then computed, and the gradient descent is performed uniformly
at the end of one batch. The embedding vector for each node is then updated by
the embedder.
Optimization Our model is optimized by the Adam optimizer [41] with its de-
fault settings. According to [38], we change the learning rate as follows:

lrate = d−0.5

model · min(step−0.5

num, stepnum · warmup−1.5
steps)

(15)

where dmodel is the dimension of the output layer (dense), stepnum is the number
of training steps, and warmipsteps is set to 4000 following [38]. The above settings
can increase the learning rate at the beginning of the training process and then
reduce the learning rate proportionally. In addition, a beam search of width 5 is
utilized to approximate the global inference [4] during testing.
Initialization We initialize the weight parameters of the encoder, the decoder,
and the embedder randomly. The program description features, which are pre-
processed into a two-dimensional vector sequence, constitute the input of the en-
coder. The node embedding features generated by the initialized embedder are
the input of the decoder, and the feature vectors of nodes are the input of the
embedder.

4 ADG-based Embedder

4.1 ADG Modelling

A more comprehensive representation of the program structure can improve the
eﬀectiveness of a code generation model. To this end, an ADG model is constructed
to represent the dependencies among API methods.

An illustrative example of ADG modelling is shown in Fig. 3. A sample code
snippet from the codebase is shown in Fig. 3(a). Fig. 3(b) illustrates a part of the
ADG that represents a partial view of the API methods and their dependencies
involved in this code snippet. In the ADG shown in Fig. 3(b), method “m4”
corresponds to node “m4” with input types “C, D” and output type“E”. Each
node has some dependent parameters as declared by its inputs, e.g., node m4 can
be invoked and perform its function after its two inputs become available, and
both instances of “C” and “D” are provided by nodes m2 and m3. A dependent
parameter or a constraint resembles the real world in that many things can be
moved on only after their dependent parameters have been satisﬁed. Therefore,
“m4” has two dependent parameters, “C” and “D”, which are the input types of
“m4”, so “INm4 ” equals {C, D}. Similarly, “m4” has one return type, “E”, which

12

Chen Lyu1 et al.

(a) A snippet of the Java program.

(b) ADG representation of API dependencies involved in (a). The labels of
the edges are “tags” that denote the data types.

Fig. 3: Example of ADG modelling.

is the output type of “m4”, so “OU Tm4 ” equals {E}. Node “m2” can satisfy one
of the dependent parameters of “m4”, “C”. Thus, there is an edge from “m2” to
“m4”, and the tag is “C”.

Compared with other source code representations such as AST and control
ﬂow graph (CFG), ADG has three main advantages: (1) it can describe the API
dependencies from a global perspective; (2) it is more capable of capturing long-
range structural dependencies among methods than a tree-based model; and (3)
the reachability of the ADG allows it to holistically reﬂect the invocation depen-
dencies between methods, thereby improving performance (as described in the
next subsection).

4.1.1 Graph Construction and Reachability Judgement

For a speciﬁc source code dataset, we ﬁrst extract a set of API
Construction
methods and their dependencies by retrieving all API signatures of the framework-
s/libraries. The general terms are listed in Table 1.

We then can build an ADG, G = (M, E) for these API methods from a graph
perspective. Recall that the set contains n API methods. Intuitively, we denote
the ith API method by mi, 1 ≤ i ≤ n. The graph G corresponding to the set is
constructed as follows.

– Regard each API method as a node in M, ∀mi ∈ M, (1 ≤ i ≤ n), where i is

the identiﬁer (id) of mi and n is the number of nodes in the graph.

– Connect two nodes, mi and mj a directed edge in E from mi and mj if
one of mi’s outputs matches one of mj’s inputs, where each edge ei,tag,j =
(mi, mj , tag) (1 ≤ i, j ≤ n). This edge is from node mi to node mj with the

...getWorking Copy()getWorking CopyManager()Get Default()Get Default()CompilationUnitEditor editor;JavaPlugin jp = JavaPlugin.getDefault();IWorkingCopyManager manager = jp.getWorkingCopyManager();IEditorInput iei = editor.getEditorInput();ICompilationUnit unit =manager.getWorkingCopy(iei);CodeCompilationUnitEditorIeditorInputICompilationUnitJava PluginIworking CopyManager...TagJava PluginIEditor InputCompilation UnitEditor...m4m2m1m3CompilationUnitEditor editor;JavaPlugin jp = JavaPlugin.getDefault();IWorkingCopyManager manager = jp.getWorkingCopyManager();IEditorInput iei = editor.getEditorInput();ICompilationUnit unit =manager.getWorkingCopy(iei);CodeA a ;B b = B.m1();C c = b.m2();D d = a.m3();E e = c.m4(d);Labelled FormADEBC...BgetWorkingCopy()getWorking CopyManager()get Default()getEditorInput()ICompilationUnitJava PluginIWorking CopyManagerJava PluginEmbedding API Dependency Graph for Neural Code Generation

13

Table 1: General terms. 1 ≤ i ≤ N , where N is the number of API methods

Terms

Meaning

API Method
(mi)

Parameter
Match

API Method
Dependency

An API method mi (1 ≤ i ≤ n) is considered
as a tuple. mi = {Imi , Omi }. Imi denotes the
input parameters of mi, and Omi denotes the
output parameters of mi.
Two API method parameters, pa and pb, are
matched by not only their types but also inher-
itance relationships. We use the “exact” match
to judge the parameter matching relationship.
Let T ype(p) be the type to which parame-
ter p belongs. Here, pa and pb can be exactly
matched if T ype(pa) is subClassof or the same
as T ype(pb), where subClassof is deﬁned as
the relationships of objects or classes through
inheritance.
Given two API methods, ma has dependen-
cies with mb if some outputs of ma can match
some inputs of mb. This case is labelled as
(cid:84) Imb (cid:54)= ∅. Method ma has full depen-
Oma
dencies with mb if Oma ⊇ Imb , and has par-
(cid:84) Imb (cid:54)=
tial dependencies with mb if (Oma
∅) ∧ (Oma

(cid:43) Imb ).

identiﬁer tag that denotes the output parameter type of mi and the input
parameter type of mj. Nodes mi and mj are the head and the tail of the
edge, respectively. Node mi is the direct successor of mj, and mj is the direct
predecessor of mi.

– Express the graph G as an inverted index table and a nodes table (see Sec-

tion 4.1.2).

The deﬁnitions of the key concepts in the ADG are given below.
INi = {tag|ej,tag,i ∈ E} ((1 ≤ j ≤ n) ∧ (i (cid:54)= j)). It is a union set of all tags of
edges with tails of mi. Variable indegreei is the number of its incoming edges, and
intagdegreei is the number of diﬀerent tags of its incoming edges, which is equal
to |INi|. Note that intagdegreei ≤ indegreei.

OU Ti = {tag|ei,tag,k ∈ E} ((1 ≤ k ≤ n) ∧ (i (cid:54)= k)). It is a union set of all tags
of edges with heads of mi. Variable outdegreei is the number of its outgoing edges,
and outtagdegreei is the number of diﬀerent tags of its outgoing edges, which is
equal to |OU Ti|. Similarly, outtagdegreei ≤ outdegreei.

Note that the ADG represents the API dependencies in a codebase, not just
those from the code snippet shown in Fig. 3(a). Fig. 4 shows a panoramic schematic
of the ADG; the green nodes are from Fig. 3(b). Notably, when inputting a textual
program description, the API dependencies involved in the generated code snippet
correspond to a part of the ADG instead of the entire ADG.
Reachability The nodes in the ADG are diﬀerent from those in a classic graph
because our nodes may have multiple dependent parameters. Only when all de-
pendent parameters are satisﬁed can the corresponding node/method be reach-
able/invoked. This special reachability of nodes is an essential property of the
ADG, which is deﬁned as follows.

Deﬁnition 1 (Reachability) For any node mi in the ADG, the condition is con-
sidered to be reachable if and only if the dependent parameters for mi are all satisﬁed

14

Chen Lyu1 et al.

Fig. 4: Panoramic view of an ADG. The left part is from Fig. 3(b). The
ellipsis indicates the unlisted nodes in the ADG.

3. In our ADG model, there might be one or more dependent parameters for invoking
node mi. Only when these dependent parameters are satisﬁed simultaneously can node
mi be successfully reached.

The special reachability here is diﬀerent from the properties of the classic
graph. A node v of a classic directed graph is reachable from another node u when
there exists a path that starts at u and ends at v.

4.1.2 Data Structures

A signiﬁcant challenge in our graph embedding algorithm is to eﬃciently compute
reachability constraints (e.g., the providers of the nodes’ dependent parameters)
for individual nodes in the ADG. Such constraints are dependent on the graph
structure. This issue will be addressed by using the specially designed data struc-
tures of our model, including the inverted index table and the nodes table. An
illustrative example is shown in Fig. 5.

Inverted Index Table (IIT): a table for representing and storing the ADG.
The entries in the IIT are key-value pairs (parameter, nodes list), where the key
is a parameter with the value being the list of nodes that need the parameter as
an input. The use of the IIT can avoid the traditional graph storage structure,
such as adjacency tables and adjacency matrices, which cause high computational
complexity, and thus can eﬀectively improve the computational eﬃciency of reach-
ability constraints. For example, in the IIT the time complexity of retrieving all
provider nodes of a dependent/input parameter is only O(1). In contrast, if an
adjacency table or an adjacency matrix is used, all nodes (n in total) must be
iterated over to determine their input parameters with the time complexity of
O(n).

Nodes Table (NT): a table for storing nodes and related information. The
NT uses a quadruple to represent nodes in the form of {mi, Imi , Omi , Count},
where mi is the unique identiﬁer of the node, and Imi and Omi are two lists that
store IN,OUT of the node. We use a counting mechanism to judge whether a node
is reachable. In the NT, each node has a counter initialized to the number of its

3 An instance of the parameter type is provided by its predecessor method.

m3m4m2m1DBCAPI Dependency graphm3m4m1m2Embedding API Dependency Graph for Neural Code Generation

15

dependent parameters/inputs. Whenever a provider of some input parameter is
retrieved, the count of the node that needs that parameter as an input is reduced
by one. Thus, if the counter equals zero, then all dependent parameters of that
node have been satisﬁed and it becomes a reachable node.

It should be noted that since we use the IIT to construct the ADG, the following
notation is used: c is the average number of API input parameters, p is the average
number of individual input parameter providers, and n is the total number of nodes
in the ADG. Therefore, the time complexity of constructing the ADG is O(n×c×p).

Fig. 5: Data structure of the example illustrated in Fig. 3

4.2 ADG Embedding

After the construction of the ADG, the next task is to produce a vectorized repre-
sentation of nodes through graph embedding. Since the distinctive ADG is directed
and tagged, in this paper, we propose a new graph embedding algorithm called
ADG embedding algorithm (Algorithm 1), which extracts the features and em-
beds them into a low-dimensional space to retain the global structure information.
Our algorithm is inspired by Hamilton et al. [24]. More speciﬁcally, it is based on
the two motivations shown in Fig. 6:

– The special reachability of each node of the ADG is considered: if the depen-
dent parameters of one node are not satisﬁed completely, then that node is
unreachable. Considering this property helps avoid incorrect API invocations.
– The invocation order among APIs is considered: the API invocation sequence
is ordered. This constraint restricts the generated sequence to be in the correct
order.

For a graph G(M, E), the ADG embedding process is shown in Fig. 7, and the
detailed steps of Algorithm 1 are illustrated on Lines 2-12. The process is described
in detail below.

– We deﬁne the initial input features xm, ∀m ∈ M of each node. By training the
weight matrices Wk, the vector of each node is constantly updated. Note that

NTString: MiList:  Imi List: Omi……int: Count    p1       p2       p3  ……   pj ……   pk  …………IITNodes ListNodes ListInput Nodes List b m1 ,m2 c m2 d m3 …… …… Node Imi Omi Count m1 B b 1 m2 b c 1 m3 a d 1 m4 c d e 2 NTIIT16

Chen Lyu1 et al.

(a) Example of a dependent parameter. Node m4 is
not satisﬁed, so this subgraph of the ADG is not reachable.

(b) Example of each dependent parameter. The depen-
dent parameters for node m4 are satisﬁed, but for node m7, a
dependent parameter is not satisﬁed. Accordingly, we can ob-
tain a partially ordered set from nodes m1, m2, m3 to node m5,
but not for nodes m6 and m7.

Fig. 6: Motivations of the ADG embedding algorithm.

each node represents an API method, and the initial value xm is treated as a
one-hot vector.

– We categorize the tagged neighbours m into tagged forward neighbours N(cid:96)(m)
and backward neighbours Ntag(cid:97)(m). Speciﬁcally, N(cid:96)(m) represents the nodes
that direct to m and provide the same input parameters of m, and Ntag(cid:97)(m)
represents the nodes that are directed to by m and accept a single output
parameter of m.

– Note that on Lines 4 and 5, a function denoted by VIRTUALIZED is used
to mean aggregate or concatenate the features of a group of identical tagged
neighbours into a single feature. The resulting feature is treated as a vir-
tual node feature of the nodes in that group. Speciﬁcally, we virtualize the
forward neighbours {hk−1
n(cid:96) , ∀n ∈ N(cid:96)(m)} of m by aggregating or concatenat-

m1m3m6m5m4m7(a)Not reachablem2√m1m3m6m5m4m7(b)m1 ⪯m4⪯m5 m1 ⪯m4⪯m6 ...m3 ⪯m4⪯m6√m2m1m3m6m5m4m7(a)Not accessibilitym2√m1m3m6m5m4m7(b)m1 ⪯m4⪯m5 m1 ⪯m4⪯m6 ...m3 ⪯m4⪯m6√m2Embedding API Dependency Graph for Neural Code Generation

17

Algorithm 1: ADG embedding algorithm

Input: Input graph G(M, E); hops K; initial input features of nodes

xm, ∀m ∈ M; weight matrices Wk, ∀k ∈ {1, 2, ..., K}; nonlinearity
σ; virtualization functions
k , VIRTUALIZEDtag(cid:97)
VIRTUALIZED(cid:96)
, ∀k ∈ {1, 2, ..., K};
neighbourhood functions N(cid:96), Ntag(cid:97); aggregator function
LSTM AGGREGATEk, ∀k ∈ {1, 2, ..., K}

k

Output: Vector representations zm for all m ∈ M

m ← xm, ∀m ∈ M

1 h0
2 for k = 1, ...K do
for m ∈ M do
3

4

5

6

7

8

9

10

N(cid:96)(m) ← VIRTUALIZED(cid:96)
hk
Ntag(cid:97)(m) ← VIRTUALIZEDtag(cid:97)
hk
ordered set ← {hk
for u in ordered set do

N(cid:96)(m), hk

k

Ntag(cid:97)(m)};

k ({hk−1

n(cid:96) , ∀n ∈ N(cid:96)(m)});

({hk−1

ntag(cid:97) , ∀n ∈ Ntag(cid:97)(m)});

m ← LSTM AGGREGATEk({hk−1
(cid:103)hk

u

, ∀u ∈ ordered set})

end
m ← σ(Wk · (cid:103)hk
hk
m)

end

11
12 end
13 zm ← hK

m, ∀m ∈ M

Fig. 7: Visual illustration of the process of the ADG embedding
algorithm. m1 to mn represent virtual nodes, and the ordered set reﬂects the
timing sequence.

LSTMLSTMLSTMLSTMh1h2h3v1starth4v1m3v2v2m3v4endh0The embedding of node m3 v1v2m3v4order setvirtual nodek=1 (one-hop)σn1x,,x...vn-1vn...LSTMvnhn-1......hnn1x,,xn1x,,xn1x,,xn1x,,xn1x,,xvnvn-1m3v4v1v2LSTMvn-1vnhn-2...m3Group by tags18

Chen Lyu1 et al.

ing to vectors hk
N(cid:96)(m). We then virtualize the tagged backward neighbours
{hk−1
ntag(cid:97) , ∀n ∈ Ntag(cid:97)(m)} of m by aggregating or concatenating to vectors
hk
Ntag(cid:97)(m), where k ∈ {1, ..., K} is the iteration index.

– Line 6 corresponds to the formation process of the ordered set. We order
the virtualized forward and backward neighbours of m into the ordered set
via topological sorting, which maintains the invocation sequence relationships
between methods in each code snippet.

– To fuse the sequenced relationships, on Lines 7 and 8, we leverage LSTM to
aggregate the nodes in the ordered set since LSTM processes inputs in a
sequential manner. Speciﬁcally, the LSTM aggregator adopts LSTM to encode
the characteristics of the neighbours of a node. The aggregator considers the
actual call order between neighbours, enters the node and its neighbours into
the LSTM in that order, and then accesses the neighbours behind the current
node in turn and aggregates their information. In particular, the sequential
ordered set of m is aggregated as (cid:103)hk
– Based on the previous step’s result (cid:103)hk

m in one hop size.
m and a fully connected network with non-
linear activity function σ, we update the forward representation of m and the
tagged backward representation of m, hk
Ntag(cid:97)(m) for the next iteration;
this process is done in Line 10.

N(cid:96)(m), hk

By utilizing the ADG embedding, our approach fuses node feature informa-
tion with both global (structural dependencies) and sequential (method invocation
constraints) API dependencies implied in the ADG. Therefore, the decoder of our
approach can utilize structural information to achieve substantial improvements
over the existing results.

5 Experimental Setting

5.1 Datasets

HearthStone (HS) [1] is an established benchmark dataset that collects Python
classes from a card game. Hearthstone is a two-player versus card game produced
by the Blizzard company. Both players seek to achieve victory by using diﬀerent
decks, utilizing diﬀerent skills and changing various digital values in the game
(such as blood volume). Each deck has several attributes (attack, health, rarity,
type, race, class, and cost and durability) and is described by a simple text in the
text box below. Each attribute of a deck is bounded by a well-formed code, such
as “super(). init ("Maexxna", 6, · · · )” in the illustration in Fig. 8(a), which
corresponds to the name and the cost of the deck. The code “return Minion(2,
8)” means that the corresponding attack value is 2 and defence value is 8. We
obtain the data from the open source implementation of TCGs4. In this corpus,
each card is implemented in a separate class, and the imports and comments
are stripped. An example of such a card and its corresponding code is shown in
Fig. 8(a).
Magic the Gathering (MTG) [1] is a collection of Java classes that demonstrate
functions of the execution cards for the game Magic: The Gathering. This game is

4 github.com/danielyule/hearthbreaker/

Embedding API Dependency Graph for Neural Code Generation

19

(a) HearthStone (HS).

(b) Magic The Gathering (MTG).

(c) Eclipse’s JDT (E-JDT).

Fig. 8: Example input and output of the three datasets. For (a) HS and (b)
MTG, the upper part of each example is the input description, and the lower
part is the corresponding output program. For (c) E-JDT, the ﬁgure shows a
natural-language description and its corresponding code.

[Card Name] Mortify[Mana Cost] [Converted Mana Cost] 3[Types] Instant[Watermark] Orzhov[Expansion]     Ravnica Allegiance[Rarity] Uncommon[Card Number] 192[Card Text] Destroy target creature or enchantment.[Flavor Text] "Your debt is erased."    —Hilgur, Orzhov euthanist[Artist] Anthony Palumbopublic class Mortify extends CardImpl{private static final FilterPermanent filter = new FilterPermanent( "creature or enchantment");static{filter.add(Predicates.or(new CardTypePredicate(CardType.CREATURE),new CardTypePredicate(CardType.ENCHANTMENT)));}public Mortify(UUID ownerId) {super(ownerId, 192, "Mortify", Rarity.UNCOMMON, new CardType[]{CardType.INSTANT},"{1}{W}{B}");this.expansionSetCode = "GPT";this.getSpellAbility().addEffect(new DestroyTargetEffect());this.getSpellAbility().addTarget(new TargetPermanent(filter));}public Mortify(final Mortify card) {super(card);}public Mortify copy() {return new Mortify(this);}}[Name] Maexxna[ATK] 2[DEF] 8[Cost] 6[DUR] -1[Type] Minion[Class] Neutral[Race] Beast[Rarity] Legendary[Description] “Destroy any minion damaged by this minion”class Maexxna(MinionCard): def __init__(self):    super().__init__("Maexxna", 6, CHARACTER_CLASS.ALL, CARD_RARITY.LEGENDARY, minion_type=MINION_TYPE.BEAST) def create_minion(self, player):    return Minion(2, 8, effects=[Effect(DidDamage(), ActionTag(Kill(), TargetSelector(IsMinion())))])Natural Language:“Binds a Hibernate Session to the current thread.”Code:private void bindSession(){  SessionFactory sessionFactory=(SessionFactory)getBean("sessionFactory");  Session session=sessionFactory.openSession();  TransactionSynchronizationManager.bindResource(sessionFactory,new SessionHolder(session));}openSession()getBean()SessionHolder()TransactionSyncheonizationManagerbindResource()SessionFactorySessionSessionFactoryIsMinion()TargetSelector()Kill()ActionTag()DidDamage()Effect()Minion()List...super()getSpellAbility()addEffect()addTarget()TargetPermanent()DestroyTargetEffect()Mortify()add()FilterPermanent()or()FilterPermanentCardTypePredicate()FilterPermanentsuper()UUIDUUIDUUIDMortify[Card Name] Mortify[Mana Cost] [Converted Mana Cost] 3[Types] Instant[Watermark] Orzhov[Expansion]     Ravnica Allegiance[Rarity] Uncommon[Card Number] 192[Card Text] Destroy target creature or enchantment.[Flavor Text] "Your debt is erased."    —Hilgur, Orzhov euthanist[Artist] Anthony Palumbopublic class Mortify extends CardImpl{private static final FilterPermanent filter = new FilterPermanent( "creature or enchantment");static{filter.add(Predicates.or(new CardTypePredicate(CardType.CREATURE),new CardTypePredicate(CardType.ENCHANTMENT)));}public Mortify(UUID ownerId) {super(ownerId, 192, "Mortify", Rarity.UNCOMMON, new CardType[]{CardType.INSTANT},"{1}{W}{B}");this.expansionSetCode = "GPT";this.getSpellAbility().addEffect(new DestroyTargetEffect());this.getSpellAbility().addTarget(new TargetPermanent(filter));}public Mortify(final Mortify card) {super(card);}public Mortify copy() {return new Mortify(this);}}[Name] Maexxna[ATK] 2[DEF] 8[Cost] 6[DUR] -1[Type] Minion[Class] Neutral[Race] Beast[Rarity] Legendary[Description] “Destroy any minion damaged by this minion”class Maexxna(MinionCard): def __init__(self):    super().__init__("Maexxna", 6, CHARACTER_CLASS.ALL, CARD_RARITY.LEGENDARY, minion_type=MINION_TYPE.BEAST) def create_minion(self, player):    return Minion(2, 8, effects=[Effect(DidDamage(), ActionTag(Kill(), TargetSelector(IsMinion())))])Natural Language:“Binds a Hibernate Session to the current thread.”Code:private void bindSession(){  SessionFactory sessionFactory=(SessionFactory)getBean("sessionFactory");  Session session=sessionFactory.openSession();  TransactionSynchronizationManager.bindResource(sessionFactory,new SessionHolder(session));}openSession()getBean()SessionHolder()TransactionSyncheonizationManagerbindResource()SessionFactorySessionSessionFactoryIsMinion()TargetSelector()Kill()ActionTag()DidDamage()Effect()Minion()List...super()getSpellAbility()addEffect()addTarget()TargetPermanent()DestroyTargetEffect()Mortify()add()FilterPermanent()or()FilterPermanentCardTypePredicate()FilterPermanentsuper()UUIDUUIDUUIDMortify[Card Name] Mortify[Mana Cost] [Converted Mana Cost] 3[Types] Instant[Watermark] Orzhov[Expansion]     Ravnica Allegiance[Rarity] Uncommon[Card Number] 192[Card Text] Destroy target creature or enchantment.[Flavor Text] "Your debt is erased."    —Hilgur, Orzhov euthanist[Artist] Anthony Palumbopublic class Mortify extends CardImpl{private static final FilterPermanent filter = new FilterPermanent( "creature or enchantment");static{filter.add(Predicates.or(new CardTypePredicate(CardType.CREATURE),new CardTypePredicate(CardType.ENCHANTMENT)));}public Mortify(UUID ownerId) {super(ownerId, 192, "Mortify", Rarity.UNCOMMON, new CardType[]{CardType.INSTANT},"{1}{W}{B}");this.expansionSetCode = "GPT";this.getSpellAbility().addEffect(new DestroyTargetEffect());this.getSpellAbility().addTarget(new TargetPermanent(filter));}public Mortify(final Mortify card) {super(card);}public Mortify copy() {return new Mortify(this);}}[Name] Maexxna[ATK] 2[DEF] 8[Cost] 6[DUR] -1[Type] Minion[Class] Neutral[Race] Beast[Rarity] Legendary[Description] “Destroy any minion damaged by this minion”class Maexxna(MinionCard): def __init__(self):    super().__init__("Maexxna", 6, CHARACTER_CLASS.ALL, CARD_RARITY.LEGENDARY, minion_type=MINION_TYPE.BEAST) def create_minion(self, player):    return Minion(2, 8, effects=[Effect(DidDamage(), ActionTag(Kill(), TargetSelector(IsMinion())))])Natural Language:“Binds a Hibernate Session to the current thread.”Code:private void bindSession(){  SessionFactory sessionFactory=(SessionFactory)getBean("sessionFactory");  Session session=sessionFactory.openSession();  TransactionSynchronizationManager.bindResource(sessionFactory,new SessionHolder(session));}openSession()getBean()SessionHolder()TransactionSyncheonizationManagerbindResource()SessionFactorySessionSessionFactoryIsMinion()TargetSelector()Kill()ActionTag()DidDamage()Effect()Minion()List...super()getSpellAbility()addEffect()addTarget()TargetPermanent()DestroyTargetEffect()Mortify()add()FilterPermanent()or()FilterPermanentCardTypePredicate()FilterPermanentsuper()UUIDUUIDUUIDMortify20

Chen Lyu1 et al.

a digital collectable card game created by Richard Garﬁeld. The MTG dataset is
similar to HS. Players use diﬀerent decks to inﬂict damage on the enemy through
thought and strategies in the game to achieve victory. As shown in Fig. 8(b),
an MTG deck contains nine attributes (card name, attack, defence, type, cost,
ID, health, rarity and suit) with some simple descriptions of the deck’s functions.
MTG’s code adheres strictly to a standard. For example, the code “super(ownerId,
192, "Mortify", Rarity.UNCOMMON, new CardType[]{CardType.INSTANT}, "{1}{W}
{B}");” speciﬁes that the card number is 192, card name is “Mortify”, rarity is
uncommon, and the mana cost is “1”, “white”, and “black”. We also collect MTG
data from an open source implementation5 that contains data from 13,279 diﬀerent
cards. We establish an input sequence with a set of ﬁelds in each card. The output
sequence is the target code snippet, which represents complex class structures, as
depicted in Fig. 8(b).
Eclipse Java Development Tools (E-JDT) [42] is a source code dataset collected
from the Eclipse Java Development Tools compiler. This dataset comprises 69,708
diﬀerent Java methods and related comments. In contrast to HS and MTG, the
comment sentences appearing in the Javadocs are used as input sequences, while
the Java code obtained from the Javadoc guidance is used as output sequences.
An example of this dataset is given in Fig. 8(c).

As illustrated in Fig. 8, we show the API dependencies on the right side of each
example, which are encoded by the ADG embedding algorithm. Taking the code of
E-JDT as an example, the invocation constraint for “bindResource()” is that the
input parameter and a container must be provided by calling “SessionHolder()”
and “getBean()”, respectively, before “bindResource ()” is called. In addition, the
API sequences contained in the code must follow the API invocation order, i.e.,
“getBean()→openSession()→ SessionHolder()→bindResource()”, subject to the
invocation constraints described above.

The statistics of the HS, MTG, and E-JDT datasets are listed in Table 2. For
HS and E-JDT, we use 80% for training, 10% for validation, and 10% for testing.
For MTG, we use 90% for training, 5% for validation, and 5% for testing.

5.2 Metrics

Our approach is evaluated in terms of eight metrics: Acc [4], BLEU [43], F1, CIDEr
[44], ROUGE-L, ROUGE-1, ROUGE-2 [45] and RIBES [46].

– Acc

Following Ling et al. [1], Yin et al. [3] and Sun et al. [4], we calculate the
accuracy (denoted by Acc) based on a string match:

Acc =

Ncorrect
Nsample

where Ncorrect denotes the number of correctly generated examples, and Nsample
denotes the number of all samples.

– BLEU The token-level BLEU [43] is selected as our second metric since
it calculates the precision of the match between the generated code snippets

5 github.com/magefree/mage/

Embedding API Dependency Graph for Neural Code Generation

21

Table 2: Statistics of the datasets.

Dataset

Training
Development
Validation

Avg.words in description
Max.words in description
Avg.tokens in code
Max.tokens in code
Avg.methods in code
Max.methods in code

HS MTG E-JDT

533
66
66

26.31
38
38.09
197
3.53
26

11,969
664
664

50.00
147
117.85
885
11.96
126

470,486
58,811
58,811

14.72
1873
57.47
8472
5.40
1729

This table shows the number of training, development, and testing items in the three
datasets. Words and tokens represent the number of characters in each description and
code item, respectively.

and reference code snippets, which is a reasonable approach to evaluating the
quality of the generated code snippets. This score is computed as:

BLEU = BP × exp(ΣN

n=1wnlogpn)

(cid:40)

BP =

1
e(1−r)/c

if c > r
if c ≤ r

where pn is the ratio of length n of the tokens’ subsequences in the generated
code.

– CIDEr CIDEr [44] calculates the cosine angle of the TF-IDF vector of the

candidate text to obtain the similarity with the reference text:

CIDErn(ci, si) =

1
M

×

M
(cid:88)

j=1

gn(ci) × gn(sij)
||gn(ci)|| × ||gn(sij)||

gk(ci) = T F (k) × IDF (k)

T F (k) =

hk(ci)
(cid:80) hl(ci)

N

IDF = log(

(cid:80)N

1 min(1, (cid:80)M

1 hk(ci))

)

where gk(ci) denotes the TF-IDF weight of n-gram ωk, hk(ci) denotes the
occurrence time of ωk in sentence ci, (cid:80) hl(ci) denotes the sum of occurrence
times of all n-grams occurring in sentence ci, and gn(ci) denotes the TF-IDF
weight vector of n-gram ωk in sentence ci. The main goal of the CIDEr metric is
to determine whether the model has captured critical information to evaluate
whether the generated code implements the major requirements of the textual
program description.

22

Chen Lyu1 et al.

– ROUGE

ROUGE [45] stands for “recall-oriented understudy for gisting
evaluation.” ROUGE-N is based on n-grams. For any n, we count the total
number of n-grams across all reference code and determine how many are
present in the candidate code. ROUGE-L is based on the longest common
subsequence (LCS):

ROU GE − N =

(cid:80) Countmatch(gramn)
(cid:80) Count(gramn)

P =

LCS(A, B)
m

R =

LCS(A, B)
n

ROU GE − L = F =

(1 + b2)RP
R + b2P

where LCS(A, B) denotes the longest common subsequence between sentences
A and B, and m and n denote the lengths of reference code and generated
code, respectively.

– RIBES RIBES [46] is an automatic evaluation method based on rank corre-
lation that considers the distance between the generation result and the token
order of the reference code. The Pearson’s correlation coeﬃcient ρ is used to
measure the diﬀerences in rank:

N SR =

ρ + 1
2

ρ = 1 −

(cid:80)

i d2
i
n+1C3

where NSR denotes the normalized Spearman’s ρ, and di indicates the diﬀer-
ence in the rank of the i-th element. The RIBES metric considers the token
order to assess whether the method invocation order in the generated code is
correct.

– F1 To evaluate the performance of diﬀerent algorithms comprehensively, the
F1 value is used to assess both precision and recall. The F1 score is calculated
as

F 1 =

2 × P recision × Recall
P recision + Recall

P recision = BLEU

Recall = ROU GE − 1

Embedding API Dependency Graph for Neural Code Generation

23

5.3 Setup

Parameter Settings The method was implemented in Python under the Py-
Torch 1.5.0 framework. Experiments were performed on a machine with an i9-
9900K CPU and 2 × RTX-2080ti GPUs. During the training process, parameters
were set as follows:

– The word embedding module was used to obtain a continuous vector for inputs
to the encoder. Code vocabulary sizes of HS, MTG, and E-JDT were 1570, 3162
and 123,975, respectively. Description vocabulary sizes for HS, MTG, and E-
JDT were counted as 1301, 2354, and 20,038, respectively. For HS and MTG,
the dimension of word embedding was set to 100 following [47], and for E-JDT,
the dimension was set to 256 following the setting in [48] since its vocabulary
size was much larger.

– For HS, MTG and E-JDT, we set the dimensions of the hidden layer to 256.
– The characteristics of the constructed ADG contain the numbers of nodes,
edges, and tags, the total in-degree and out-degree, and the average in-degree
and out-degree. These statistics are listed in Table 3.

– We set the training epochs on HS, MTG, and E-JDT to 25,000, 400, and 15,

respectively.

– Early stopping was performed by calculating the BLEU score with the valida-
tion set. For HS, the BLEU score was calculated every 1000 iterations, and if
there were no increments for more than 10 times, processing was terminated.
For MTG and E-JDT, the stopping time was set to 10 and 3 respectively.
– For the decoder, we used the beam search with a size of 5 during prediction.
– To prevent overﬁtting, we applied dropout [49] to the output of each sub-layer.

In all cases, we set Pdrop = 0.1

– Glorot initialization [50] was used to initialize all parameters randomly.

Table 3: Statistics of the ADGs.

HS MTG

E-JDT

Nodes
Edges
Max.in
Avg.in
Max.out
Avg.out

1,204
3,726
48
3.03
22
3.16

117,246
2,452,088
215
28.11
201
13.71

2,162,968
18,048,043
290
10.25
264
6.44

This table shows the quantities of nodes and edges, and the in-degree and out-degree
values of nodes.

6 Research Questions

In the experiments, we evaluated the performance of various approaches on gen-
erating Python and Java code to answer the following three research questions.

24

Chen Lyu1 et al.

RQ1: Is the performance of ADG-Seq2Seq better than that of the compared

approaches?

The objective of this RQ is to evaluate the eﬀectiveness of our method. Com-
pared to previous approaches concentrating on grammar rules, we considered more
aspects and used the ADG to generate code. The superiority of our approach is
highlighted by comparisons with several state-of-the-art models.

RQ2: Does the proposed ADG-based embedder contribute to code genera-

tion?

The purpose of this RQ is to evaluate the beneﬁts of ADG embedding. Exten-
sive ablation tests were performed to evaluate the contribution of each component.
To this end, we removed or substituted a single component each time. Speciﬁcally,
the components we evaluated were as follows.

– Graph embedding. We removed the ADG embedding from the decoder; i.e.,
the method used only the Seq2Seq model with attention to vectorize the target
code.

– Diﬀerent graph embedding strategies. We replaced our ADG embedding

with other algorithms, including GraphSAGE [24] and GCN [23].

– Special reachability. We did not consider the special reachability of the ADG
and did not classify the forward and backward nodes according to diﬀerent tags.
– Directed edges. We neglected the directions of edges between nodes to evaluate

the impact of such directions.

– Labelled edges. We removed the labels of edges in the ADG, namely the tags,

to evaluate the inﬂuence of edge labels.

– Diﬀerent Hop Sizes. We compared ADG embeddings with hop sizes of one

and two.

– Diﬀerent Aggregators. We compared diﬀerent aggregators, such as mean,

pooling6, and LSTM aggregators, in the ADG embedding.

RQ3: How do various aspects of the datasets aﬀect the performance of

our approach?

This RQ is explored to evaluate the impact of each aspect of the datasets on the
performance of our approach. For all existing solutions, several aspects of datasets
might aﬀect the performance of code generation. Since the embedder was ﬁrst
introduced into code generation, we devoted a considerable amount of eﬀort to the
evaluation of the code structure. Several new experiments were designed to explore
the inﬂuences of various aspects. Speciﬁcally, we conducted two main experiments
to evaluate the performance of the model at diﬀerent description lengths and
diﬀerent code lengths. In addition, other aspects of datasets, such as various forms
of descriptions (semi-structured or unstructured), diﬀerent code implementation
languages (Python or Java), and various graph complexities (sparse or dense), are
also discussed and analysed in detail.

6 The pooling we used followed the method of GraphSAGE [27], namely, max pooling.

Embedding API Dependency Graph for Neural Code Generation

25

7 Results

7.1 Answer to RQ1

To answer RQ1, the proposed ADG-Seq2Seq method was compared with bench-
mark methods reported in the literature: attention-based Seq2Seq, Transformer,
SNM, ASN, GB-CNN, and TreeGen. Attention-based Seq2Seq and Transformer
are classic models that are widely used in neural machine translation, while the
other approaches are designed for code generation. These approaches continu-
ously promote and eﬀectively improve the performance of code generation. The
diﬀerences between these methods are described in the following. Details of the
implementations of these methods are available in Appendix A.

– Attention-based Seq2Seq. The attention-based Seq2Seq model is a typical
neural machine translation model [51] that adopts an encoder-decoder archi-
tecture and leverages an attention mechanism to better map diﬀerent tokens.
– Transformer. Transformer, proposed by Vaswani et al. [38], is also based on
the encoder-decoder architecture but integrates a more advanced multihead
attention mechanism. Sun et al. [5] trained a transformer-based code generation
model using plain code tokens and comments. In our experiments, we also
compare our model with this transformer-based code generation model.

– SNM. The syntactic neural model [3] is a grammar model that uses the AST

to capture the syntax of source code snippets.

– ASN. The abstract syntax network [6] develops a speciﬁcally designed decoder
with a dynamically determined modular structure paralleling the structure of
the output tree, such that its output is represented by an AST.

– GB-CNN. Grammar-based CNNs [4] use a grammar-based structural CNN
decoder for code generation, where the underlying grammar information in an
AST is parsed and absorbed through a three-layer CNN.

– TreeGen. TreeGen [5] leverages the Transformer’s multihead attention mecha-
nism to alleviate the problem of a long-range dependency and introduces a new
AST encoder to incorporate syntax rules and AST structures into the model.

Comprehensive comparison results of all three datasets are provided in Table 4

and illustrated in Fig. 9.

HS : Fig. 9 (a) - (h) shows the results of various methods. TreeGen achieves
the best Acc and BLEU scores, while the proposed method outperforms the oth-
ers in terms of F1, CIDEr, ROUGE, and RIBES. TreeGen uses the AST-based
Transformer to learn syntactical information within API methods and exhibits a
better capacity for addressing long-range dependency, thus increasing the related
scores. Compared to TreeGen, ADG-Seq2Seq pays more attention to the global
and sequential structure of the program and designs an ADG embedding model to
learn the underlying structure and dependencies among APIs. The learned global
and sequential information is conducive to generating more comprehensive code.
Therefore, the performance of ADG-Seq2Seq on other metrics, such as ROUGE,
which considers the recall rate between tokens, is the best, regardless of whether
ROUGE-1, ROUGE-2, or ROUGE-L is considered. Beyond ROUGE, the highest
scores achieved in terms of CIDEr reﬂect the enormous potential of our method,
which demonstrates the importance of the global structural information for cap-
turing important representative tokens in code snippets. Regarding RIBES, which

26

Chen Lyu1 et al.

(a) HS: Acc

(b) HS: BLEU

(c) HS: F1

(d) HS: CIDEr

(e) HS: ROUGE-L

(f) HS: ROUGE-1

(g) HS: ROUGE-2

(h) HS: RIBES

(i) MTG: Acc

(j) MTG: BLEU

(k) MTG: F1

(l) MTG: CIDEr

(m) MTG: ROUGE-L (n) MTG: ROUGE-1 (o) MTG: ROUGE-2

(p) MTG: RIBES

(q) EJDT: Acc

(r) EJDT: BLEU

(s) EJDT: F1

(t) EJDT: CIDEr

(u) EJDT: ROUGE-L (v) EJDT: ROUGE-1 (w) EJDT: ROUGE-2

(x) EJDT: RIBES

Fig. 9: Various metric scores obtained in the experiments.

Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods051015202530Acc Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080BLEU Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080F1 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods0.000.250.500.751.001.251.501.752.00CIDEr Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods020406080ROUGE-L Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods020406080ROUGE-1 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods010203040ROUGE-2 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080RIBES Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods051015202530Acc Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods010203040506070BLEU Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080F1 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods0.00.20.40.60.81.01.21.41.6CIDEr Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080ROUGE-L Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080ROUGE-1 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods0510152025303540ROUGE-2 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080RIBES Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods051015202530Acc Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods0102030405060BLEU Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods010203040506070F1 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods0.000.250.500.751.001.251.501.75CIDEr Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080ROUGE-L Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080ROUGE-1 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods0510152025303540ROUGE-2 Score (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqMethods01020304050607080RIBES Score (%)Embedding API Dependency Graph for Neural Code Generation

27

Table 4: Experimental results obtained by our approach and the
compared methods.

Attn-Seq2Seq
Transformer
SNM
ASN
GB-CNN
TreeGen
ADG-Seq2Seq

Attn-Seq2Seq
Transformer
SNM
ASN
GB-CNN
TreeGen
ADG-Seq2Seq

Attn-Seq2Seq
Transformer
SNM
ASN
GB-CNN
TreeGen
ADG-Seq2Seq

S
H

G
T
M

T
D
J
-
E

Acc

1.5
10.6
16.2
18.2
27.3
31.8
27.3

1.4
10.1
19.3
21.2
25.0
26.7
29.4

0.8
8.4
19.2
18.9
26.1
26.3
28.6

Bleu

60.4
68.0
75.8
77.6
79.6
80.8
78.1

54.8
58.1
62.3
63.7
65.9
65.3
69.2

50.1
54.2
59.9
58.5
60.8
61.1
65.3

F1

CIDEr RougeL Rouge1 Rouge2 RIBES

62.8
70.8
77.3
78.7
81.4
82.2
82.5

59.0
63.6
70.0
70.3
72.1
71.9
76.3

56.6
60.7
65.9
65.6
68.8
69.0
73.8

0.56
1.35
1.57
1.56
1.62
1.88
2.02

0.33
0.94
1.28
1.23
1.35
1.48
1.69

0.31
0.53
1.58
1.54
1.67
1.73
1.85

64.0
72.8
77.5
77.0
82.8
82.9
87.4

63.1
69.9
79.4
78.3
79.8
78.4
85.4

64.8
68.5
73.3
74.1
77.6
76.0
81.7

65.5
73.8
78.9
79.8
83.2
83.6
87.5

63.9
70.3
79.8
78.4
79.6
80.0
85.1

65.1
68.9
73.1
74.7
79.4
79.4
84.7

32.3
36.5
39.0
39.2
39.7
40.2
43.5

31.4
34.7
38.8
38.6
37.9
38.3
42.2

32.5
33.9
36.5
36.7
37.9
37.8
41.8

52.1
63.7
73.4
73.3
76.2
75.9
80.4

51.4
62.0
72.2
72.0
75.8
75.1
79.7

50.9
60.8
70.4
69.3
70.2
71.9
78.8

1 Attn-Seq2Seq denotes the attention-based Seq2Seq model.
2 For models with existing experimental data, we use the experimental results presented in the

original paper, and for those without, we retrain the model with the default parameters
provided in the original paper and calculate the experimental results under various metrics.
Detailed information is provided in Appendix A.

focuses on the token order, ADG-Seq2Seq outperforms other Seq2Seq models, in-
dicating that embedding the call constraint information helps the model more
accurately predict the order in which API methods are called.

MTG: We further evaluate the proposed method on the MTG dataset, as
shown in Fig. 9 (i) - (p). The performance of various methods drops slightly in
comparison to the performance on the HS dataset. Compared to HS, the program-
ming language in MTG changes from Python to Java, and the data complexity
increases signiﬁcantly, as illustrated in Table 2. Therefore, it is more diﬃcult to
predict the target code as the code length and description length grow signiﬁcantly.
ADG-Seq2Seq outperforms other methods on all metrics, which demonstrates that
the use of ADG to represent complex API dependencies and the program-oriented
embedder in our method can help address such changes in the code generation
task.

E-JDT : In contrast to the other two datasets, E-JDT is characterized by
unstructured descriptions and increased uncertainty with a broad range of code
lengths in terms of both tokens and methods, as shown in Table 2. The performance
comparison in Fig. 9 (q) - (x) demonstrates that ADG-Seq2Seq can produce more
comprehensive and accurate code from natural language statements than other
state-of-the-art methods on this dataset.

28

Chen Lyu1 et al.

The above results show that the SOTA models and our model outperform the
naive NMT models (e.g., Seq2Seq) and the Transformer in terms of all metrics
by a substantial margin, demonstrating the importance of the integrated struc-
tural information obtained from AST or ADG. Among these methods, GB-CNN,
TreeGen, and ADG-Seq2Seq achieve relatively good performance, but each has
its weakness. In particular, although GB-CNN [4] and TreeGen [5] perform well
on HS in terms of Acc and BLEU, they perform poorly on HS in terms of the
other six metrics and perform poorly in terms of all metrics on MTG and E-JDT.
GB-CNN and TreeGen show improvements over previous approaches by learn-
ing the grammar rules in ASTs and handling the long-range dependency problem
in sequence-to-sequence translation. However, an AST represents the structural
information within the API method from a local perspective. Hence, the infor-
mation used for learning conﬁned in each individual AST lacks connections and
interactions with that in other ASTs. In contrast, our model represents the API
dependencies in the whole libraries or the entire project from a global perspective
and connects them in the ADG, thereby contributing to more comprehensive and
accurate code generation.

Additionally, our method performs slightly worse on HS in terms of Acc and
BLEU. We argue that the reason is that our joint learning model requires large
training samples and adequate connections between nodes in the ADG. Hence,
the eﬀectiveness of our approach, especially as measured by precision-related indi-
cators, degrades if the quantities of sample data and connections are insuﬃcient,
as shown in Tables 2 - 3. ADG-Seq2Seq has the best performance on MTG and
E-JDT, where the quantities of training samples and data connections in the con-
structed ADG are typically greater than those for HS by several orders of magni-
tude.

In addition, we observe that the recall-based ROUGE-L exhibits more improve-
ments than the precision-based BLEU on all three datasets. The precision-based
metric BLEU-N excels at assessing the precision of generated sentences and their
linguistic ﬂuency. ROUGE-L, in contrast, is primarily recall-based and therefore
tends to reward long sentences with high recall [44]. To obtain further insights,
we have veriﬁed the experimental results and analysed them in detail. The ex-
perimental results show that on HS, TreeGen generates more “structured” code
with more stable quality because of its ability to batch input and output data.
ADG-Seq2Seq, in contrast, relies more substantially on sequential information,
which, coupled with the API dependencies, tends to generate longer results than
TreeGen. Based on the above observations, the results of TreeGen are better in
terms of BLEU, while ADG-Seq2Seq performs better under the recall-based met-
rics. Note that these results are for the HS dataset; the improvements could vary
across datasets.

An evaluation metric called the percentage of valid code (PoV) was proposed
by Wei et al. [52] to evaluate the performance of code generation based on the
percentage of the generated code that could be parsed into an AST (i.e., compilable
code). We also compute the PoV value to assess the quality of the generated code.
For ADG-Seq2Seq, the PoVs tested on HS, MTG and E-JDT are 68.2%, 55.1%
and 53.0%, respectively. The PoVs produced by the attention-based Seq2Seq are
much lower (22.7%, 20.8%, and 19.3%, respectively), demonstrating the ability of
our model to increase the percentage of valid code.

Embedding API Dependency Graph for Neural Code Generation

29

In summary, ADG-Seq2Seq achieves the best performance on 22 indicators (ex-
cept for two indicators in HS), which demonstrates the superiority of our method
for neural code generation.

7.2 Answer to RQ2

To validate the beneﬁts of the ADG embedding, we evaluate diﬀerent variants of
embedders and diﬀerent design options of the ADG.

7.2.1 Examination of Variations

In this paper, we develop an ADG embedding algorithm to learn the node fea-
tures from the ADG. The graph embedding algorithms GCN and GraphSAGE
are known for their eﬃciency and good performance in many areas. To explore
the advantages of our methods, we further compare four variations using diﬀerent
graph embedding strategies, namely, GCN, GraphSAGE, ADG embedding, and
the model without graph embedding. We refer to the reported results [24, 53] and
set the number of GCN layers to two and the number of hops for GraphSAGE
and ADG embedding to two.

A quantitative comparison of the ﬁnal code generation results (in terms of
BLEU on E-JDT) based on these approaches is shown in Table 5. The model
without graph embedding produces the worst results. The model using Graph-
SAGE outperforms GCN, while the results based on the ADG embedding are the
best. These comparisons show that the models with graph embedding are better
than those without. This result conﬁrms the positive eﬀect of the graph embedding
algorithm, notably, the ADG embedding algorithm designed explicitly for program
structure, on the quality of code generation results.

Table 5: Results of various graph embedding modules. The scores represent
the BLEU metric.

Graph Embedding
−ADG Embedding1
+ADG Embedding2
ours → GCN3
ours → GraphSAGE4

HS MTG E-JDT

60.4
78.1
69.3
70.8

54.8
69.2
60.8
63.4

50.1
65.3
55.2
58.5

1 Remove the graph embedding module, namely, an NMT

model.

2 A model with our proposed module.
3 Replace the ADG embedding module with GCN, a GCN-based

Seq2Seq model.

4 Replace the ADG embedding module with GraphSAGE, a

GraphSAGE-based Seq2Seq model.

Another observation is that ADG-Seq2Seq appears to be the most stable model.
A visual distribution comparison of graph embedding approaches is shown in
Fig.10. The method without graph embedding obtains the lowest values at the

30

Chen Lyu1 et al.

Fig. 10: BLEU distribution density of each item in testing. Various graph
embedding algorithms applied to the E-JDT dataset generate diﬀerent
distributions.

peak, mean, and median points. The results produced by the models based on
GCN and GraphSAGE are better, but their left tails appear to be much thicker,
which indicates that there are more abnormally weak scores. The distribution
generated by ADG-Seq2Seq is approximately symmetric, with the highest peak,
mean, and median. Moreover, the shape of the distribution tends to be tighter,
so most data are centred around the peak, which indicates that more results of
our method achieve better ratings. The above results reveal the signiﬁcance of our
innovative design for learning the program graph structure considering the special
reachability and invocation order between APIs. This approach helps preserve the
calling order and restricts invalid invocations in the predictions. As a result, our
approach outperforms the competitors.

In summary, the results presented in Table 5 and Fig. 10 show that our ADG
embedding mechanism has advantages over other variations and is eﬀective for
neural code generation.

7.2.2 Examination of Design Options

We examine diﬀerent design options of the ADG and the corresponding algorithm,
as shown in Table 6. The detailed results are as follows.

– Undirected Edges. We ﬁrst examine the option without edge direction. The
rows marked with −directed edges illustrate the performance of our method
learning on a graph with undirected edges. As shown by the results in these

−ADG−Seq2seq+ADG−Seq2seqGCNGraphSAGE204060800.0000.0250.0500.0750.1000.0000.0250.0500.0750.1000.0000.0250.0500.0750.1000.0000.0250.0500.0750.100BLEUdensityMETHOD−ADG−Seq2seq+ADG−Seq2seqGCNGraphSAGEEmbedding API Dependency Graph for Neural Code Generation

31

Table 6: Experimental results of our model with diﬀerent ADG parameter
settings.

Full model
– directed edges
– labelled edges
One-hop size
Two-hop size
Mean aggregator
Pooling aggregator
LSTM aggregator

Full model
– directed edges
– labelled edges
One-hop size
Two-hop size
Mean aggregator
Pooling aggregator
LSTM aggregator

Full model
– directed edges
– labelled edges
One-hop size
Two-hop size
Mean aggregator
Pooling aggregator
LSTM aggregator

S
H

G
T
M

T
D
J
-
E

Acc

27.3
21.2
24.2
24.2
27.3
19.7
21.2
27.3

29.4
17.3
17.8
17.3
29.4
17.5
17.9
29.4

28.6
14.5
16.1
15.8
28.6
17.7
18.4
28.6

Bleu

78.1
71.5
72.2
70.1
78.1
70.0
71.4
78.1

69.2
59.3
62.3
61.1
69.2
59.5
61.8
69.2

65.3
53.3
54.5
54.4
65.3
56.2
56.9
65.3

F1

CIDEr RougeL Rouge1 Rouge2 RIBES

82.5
77.4
77.9
76.3
82.5
76.2
77.3
82.5

76.3
67.6
70.0
69.0
76.3
67.7
69.6
76.3

73.8
62.5
63.7
63.5
73.8
65.0
65.6
73.8

2.02
1.85
1.87
1.81
2.02
1.81
1.85
2.02

1.69
1.53
1.61
1.58
1.69
1.54
1.60
1.69

1.85
1.38
1.50
1.41
1.85
1.45
1.47
1.85

87.4
84.3
84.6
83.6
87.4
83.6
84.2
87.4

85.4
78.4
79.7
79.3
85.4
78.5
79.6
85.4

81.7
75.6
76.5
76.1
81.7
76.9
77.3
81.7

87.5
84.3
84.7
83.7
87.5
83.6
84.3
87.5

85.1
78.5
79.9
79.3
85.1
78.6
79.7
85.1

84.7
75.6
76.7
76.1
84.7
77.0
77.3
84.7

43.5
41.9
42.1
41.6
43.5
41.6
41.9
43.5

42.2
39.0
39.7
39.4
42.2
39.1
39.6
42.2

41.8
37.6
38.1
37.9
41.8
38.3
38.5
41.8

80.4
77.8
78.1
77.3
80.4
77.2
77.8
80.4

79.7
73.1
74.2
73.8
79.7
73.2
74.1
79.7

78.8
70.7
72.3
71.2
78.8
71.9
72.1
78.8

rows, the original full model outperforms this approach. According to our algo-
rithm, dropping the direction information means ignoring the API invocation
order in the learning process, which thereby leads to poor performance. In
particular, the average BLEU score of this option is 12 points lower than that
of the full model on E-JDT. The RIBES score focuses on the evaluation of
sequencial order, and more comprehensively reﬂects the inﬂuence caused by
the lack of direction information. Compared with the scores of the full model,
the RIBES scores of this approach decrease by large margins on all datasets.
Based on the above observations, the use of edge direction in our generation
framework contributes to the overall performance. Note that this experiment
is equivalent to testing the case where the ADG embedding algorithm does not
encode the API invocation order.

– No Labelled Edges. We further examine the performance when using unla-
belled edges, as shown in rows −labelled edge. In the ADG, the various labels
(tags) on incoming or outgoing edges of each node represent the diﬀerent input
or output parameter types of the corresponding API method. The full model
can learn valid dependencies among APIs by fusing these tags/types and their
relationships into our generation model. Therefore, the full model outperforms
the option without labels on all metrics. Note that this experiment is equiva-
lent to testing the case where the ADG embedding algorithm does not encode
the API invocation constraints.

32

Chen Lyu1 et al.

– Diﬀerent Hop Sizes. Since parameter K in Algorithm 1 can be speciﬁed as 1
or 2, we examine the performance of embedders using one- or two-hop sizes.
The results using diﬀerent hop sizes are shown in the rows one − hop size and
two − hop size. One-hop size means that the embedder gathers node informa-
tion for only the ﬁrst layer, namely, direct neighbours’ information. Two-hop
modelling gathers information about a node’s two-layer neighbours, namely,
both immediate neighbours and immediate neighbours’ neighbours, the num-
ber of which is almost a square of the number of neighbours in one-hop size..
Meanwhile, the API dependency information between these neighbours is also
included. Code-related features, such as API invocation order and API invoca-
tion constraints, are embedded in this information. More comprehensive API
dependency knowledge can be learned from ADG using a two-hop size. There-
fore, the embedder using the two-hop size always achieves higher scores than
that using the one-hop size, e.g., by approximately 12% to 80% in terms of Acc
on the three datasets. This is why we use the two-hop size in the full model.
– Diﬀerent Aggregators. Finally, we compare diﬀerent aggregators used in the
ADG embedding algorithm, as shown in Table 6. For all metrics, the results
of LSTM aggregators are generally 3.35% to 63.8% higher than the results of
the pooling aggregators, and 4.08% to 68.10% higher than the results of mean
aggregators. These results indicate that diﬀerent aggregators produce diﬀerent
eﬀects. The mean aggregator ignores the invocation order of nodes as well as
some of the collected information, such as the direction of edges. The pooling
aggregator obtains the most crucial information among the order set and is
more eﬃcient but has the same defect as the mean aggregator. LSTM is not as
eﬃcient as the pooling aggregator but can hold complete order information in
the order set, which may be the reason why LSTM achieves the highest scores.

In summary, all these results demonstrate that ADG embedding is of great

value and is eﬀective for the code generation task.

7.3 Answer to RQ3

For most NMT tasks, the lengths of input and output sequences are the primary
factors aﬀecting generation performance. In this section, we investigate external
factors, such as dataset parameters, to provide the answer to RQ3.

7.3.1 Impact of Description Length

A textual program description states the purpose of the code, which is important
for the establishment of learning models. Description lengths of HS are small and
no more than 40, as shown in Fig.11(a); MTG has a large range of description
lengths, concentrating in the range of 20 to 100, as shown in Fig.11(b). The de-
scription lengths of E-JDT have a wide range, but most are less than 40, as shown
in Fig.11(c). To explore the inﬂuence of varying description lengths on diﬀerent
metrics, we ﬁrst split datasets according to length distributions given by Fig.11.
The statistics of the new split datasets are shown in Table 7.

The experimental results for these new split datasets are illustrated in Fig.12.
Among the 24 indicators, except for some cases of MTG and E-JDT, which increase

Embedding API Dependency Graph for Neural Code Generation

33

ﬁrst and then decrease as the description length increases, all indicators exhibit a
downward trend. However, the decreases occur relatively slowly.

(a) Description length (HS)

(b) Description length (MTG) (c) Description length (EJDT)

(d) Code length (HS)

(e) Code length (MTG)

(f) Code length (EJDT)

Fig. 11: Distribution of data with diﬀerent code/description lengths.

For HS, the highest scores are obtained if the description length is approxi-
mately 20, which indicates that the learning models can capture complete semantic
information from short descriptions. Based on the discussion in Section 7.1, ADG-
Seq2Seq can learn more comprehensive knowledge through global view modelling,
thereby contributing to the prediction of complete code and correct call sequences.
Therefore, our approach outperforms other models in terms of ROUGE, CIDEr,
and RIBES. Limited by the fewer samples, the scores of precision-related metrics
(including ACC, BLEU, and F1) are lower than those of TreeGen and GB-CNN.
However, this scenario does not represent the upper bound of our model. For ex-
ample, when suﬃcient number of samples are available (see Table 7) on MTG and
E-JDT, ADG-Seq2Seq outperforms the competitors regardless of the description
length. Moreover, the degradation of performance of our model on HS is faster than
that on MTG and E-JDT. One explanation is that as description length increases
in HS, fewer samples increase, which might negatively aﬀect the performance of
our method.

7.3.2 The Impact of Code Length

To further analyse the impact of code length, we split the three datasets according
to their code length distributions, as shown in Fig. 11(d), (e), and (f). The code
length levels of HS were set to -30, -40, -50, and -60; those of MTG were set to
-100, -150, -200, -250, and -300; and those of E-JDT were set to -20, -40, -60,

010405020              30 01020304050CountDescription length (word)0208010040              60 050100150200250300350CountDescription length (word)0208010040              60 0500010000150002000025000300003500040000CountDescription length (word)0208010040              60 0102030405060CountCode length (token)0208010040              60 050100150200250CountCode length (token)0208010040              60 050001000015000200002500030000CountCode length (token)34

Chen Lyu1 et al.

(a) HS: Acc

(b) HS: BLEU

(c) HS: F1

(d) HS: CIDEr

(e) HS: ROUGE

(f) HS: ROUGE-1

(g) HS: ROUGE-2

(h) HS: RIBES

(i) MTG: Acc

(j) MTG: BLEU

(k) MTG: F1

(l) MTG: CIDEr

(m) MTG: ROUGE-L (n) MTG: ROUGE-1 (o) MTG: ROUGE-2

(p) MTG: RIBES

(q) EJDT: Acc

(r) EJDT: BLEU

(s) EJDT: F1

(t) EJDT: CIDEr

(u) EJDT: ROUGE-L (v) EJDT: ROUGE-1 (w) EJDT: ROUGE-2

(x) EJDT: RIBES

Fig. 12: Experimental results of the comparison of state-of-the-art
methods with varying description lengths.

102030Description length05101520253035Acc (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq102030Description length56647280BLEU (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq102030Description length55606570758085F1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq102030Description length0.51.01.52.0CIDEr (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq102030Description length606570758085ROUGE-L (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq102030Description length657075808590ROUGE-1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq102030Description length32343638404244ROUGE-2 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq102030Description length5055606570758085RIBES (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq2030405060708090Description length051015202530Acc (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq2030405060708090Description length4048566472BLEU (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq2030405060708090Description length45505560657075F1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq2030405060708090Description length0.51.01.5CIDEr (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq2030405060708090Description length556065707580ROUGE-L (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq2030405060708090Description length6065707580ROUGE-1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq2030405060708090Description length2830323436384042ROUGE-2 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq2030405060708090Description length505560657075RIBES (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq10203040Description length051015202530Acc (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq10203040Description length485664BLEU (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq10203040Description length5560657075F1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq10203040Description length0.51.01.52.0CIDEr (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq10203040Description length65707580ROUGE-L (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq10203040Description length6570758085ROUGE-1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq10203040Description length323436384042ROUGE-2 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq10203040Description length50556065707580RIBES (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2SeqEmbedding API Dependency Graph for Neural Code Generation

35

Table 7: Statistics of datasets split according to
description length.

LEN

QTY Avg.w

Avg.t Avg.m

[0 , 10]
[10, 20]
[20, 30]
[30, 40]

[20, 30]
[30, 40]
[40, 50]
[50, 60]
[60, 70]
[70, 80]
[80, 90]
[90, 100]

[0 , 10]
[10, 20]
[20, 30]
[30, 40]

0
81
0.3k
0.1k

1.3k
2.9k
2.7k
2.0k
1.3k
0.9k
0.5k
0.2k

257.8k
128.2k
39.5k
19.1k

S
H

G
T
M

T
D
J
-
E

-
19.6
26.1
32.4

26.3
35.8
45.1
55.3
65.3
75.2
84.8
94.9

6.3
14.4
24.9
34.8

-
27.1
37.3
49.6

68.2
93.0
113.9
129.5
146.7
157.4
169.8
186.9

52.4
53.9
67.7
73.6

-
2.0
3.5
4.9

5.8
8.9
11.4
13.5
15.7
16.9
18.2
19.9

5.1
5.0
6.1
6.4

1 Avg.w means the average number of words in the

descriptions.

2 Avg.t means the average number of tokens in code.
3 Avg.m means the average number of methods in code.
4 QTY means the quantity of data in the subset.

Table 8: Statistics of the datasets with diﬀerent code lengths.

QTY

Avg.w Max.w

Avg.t

Avg.m Max.m Avg.e Max.e

Avg.in Max.in

Avg.out Max.out

30
40
50
60

100
150
200
250
300

0.15k
0.25k
0.08k
0.02k

6.89k
2.19k
1.26k
0.70k
0.37k

20 237.56k
85.34k
40
43.26k
60
80
26.27k
100 16.79k

S
H

G
T
M

T
D
J
E

22.2
27.1
28.7
27.8

45.1
53.4
54.7
61.8
67.0

13.0
14.4
15.4
16.6
17.5

32
36
35
38

118
133
134
142
129

1702
1692
1389
630
806

27.5
34.9
44.0
55.3

74.4
120.4
173.8
222.3
272.9

9.8
29.2
49.5
69.8
89.9

2.0
2.8
4.7
7.0

6.7
11.8
17.8
24.7
31.7

0.9
3.2
5.4
7.5
9.4

4
6
8
10

23
35
37
60
51

168
255
169
108
165

5.6
7.6
12.8
19.3

28.7
50.3
75.9
105.5
115.1

3.4
132.0
19.5
27.2
34.2

7
9
15
23

51
89
134
186
215

6
247
36
51
64

2.7
3.7
6.3
9.4

19.3
33.8
51.0
70.9
70.7

2.1
7.2
12.0
16.7
21.0

10
14
23
35

38
67
101
141
159

5
16
27
37
47

2.8
3.9
6.6
9.9

9.4
16.5
24.9
34.6
44.4

1.3
4.5
7.5
10.5
13.2

5
6
11
16

36
63
95
131
144

4
15
24
34
43

w is the number of words in a description, t is the number of tokens in code, m is the number of methods in code,
e is the number of edges in the graph for the code, in and out are the numbers of neighbours directly connected to the node.

-80, and -100. The statistics of these new fourteen split datasets are illustrated in
Table 8.7

7 As presented in the experimental results, we concatenate the name of each dataset and
the code length level to indicate a split dataset. Taking two items of HS as examples, HS-30
consists of the data with code length between 0 and 30, while the code length of HS-60 ranges
from 50 to 60.

36

Chen Lyu1 et al.

Fig. 13 provides the comprehensive comparison results. The scores on all met-
rics tend to be lower for greater code lengths, which is unsurprising because various
factors, e.g., the average counts of tokens, methods, edges, incomings, and outgo-
ings increase accordingly, as shown in Table 8, making predictions more diﬃcult.
In general, ADG-Seq2Seq achieves the best scores on 87.5% of indicators.

Furthermore, TreeGen and GB-CNN perform better on the Python dataset,
while ADG-Seq2Seq shows advantages on the Java datasets. In addition to con-
sidering the inﬂuence of sample size, as discussed previously, we analyse diﬀerent
datasets in depth from the perspective of the programming language.

Python-based HS originates from a card game, and each code snippet corre-
sponds to a speciﬁc card. The ADG generated based on HS is sparse since the
APIs developed in Python, especially those located on diﬀerent cards, lack data
dependencies on each other. Due to the inﬂuence of the sparse graph, the general-
ization and inference performance of our learning model is weakened. In contrast,
AST-based models learn the code for each card from the local view and achieve
better results.

For MTG and E-JDT, the situation changes. MTG and E-JDT are both pro-
grammed in Java, which contains more general API methods. These APIs have
rich interactive data dependencies among diﬀerent code snippets. Consequently,
denser ADGs are constructed, so the performance of our model is improved. The
detailed data presented in Table 8 support our arguments.

Compared to the other datasets, the target code is diﬃcult to predict for E-JDT
since all description lengths are less than 20, but the lengths of the corresponding
code continue to increase, as shown in Table 8. ADG-Seq2Seq remains superior
on E-JDT. When the code length exceeds 80, some scores drop sharply since
the code length is approximately ﬁve times the description length, which poses a
considerable challenge to the reasoning ability of our model.

In addition to the length of each data item, the size of the dataset could inﬂu-
ence the experimental results. However, the primary purpose of this experiment is
to compare the quality of code generation between diﬀerent methods with diﬀerent
code lengths and description lengths. For the various methods, the comparison is
fair since the same training dataset is being used. The experimental results show
that ADG-Seq2Seq tends to outperform its rivals when the dataset contains as
few as 80 items or as many as 85,000 items. This shows that our method is stable
even as the code length varies.

In summary, the red curves (ADG-Seq2Seq) shown in Fig. 13 are above the
other curves, which shows that ADG-Seq2Seq outperforms the compared methods
in most cases and can generate more accurate results than state-of-the-art as the
code length varies.

8 Qualitative Analysis

The promising performance of the proposed method in generating code has been
veriﬁed in quantitative experiments. In this section, we provide case studies and
error analyses to investigate speciﬁc situations in which our method performed
well or poorly.

Embedding API Dependency Graph for Neural Code Generation

37

(a) HS: Acc

(b) HS: BLEU

(c) HS: F1

(d) HS: CIDEr

(e) HS ROUGE-L

(f) HS: ROUGE-1

(g) HS: ROUGE-2

(h) HS: RIBES

(i) MTG: Acc

(j) MTG: BLEU

(k) MTG: F1

(l) MTG: CIDEr

(m) MTG: ROUGE-L (n) MTG: ROUGE-1

(o) MTG: ROUGE-2

(p) MTG: RIBES

(q) E-JDT: Acc

(r) E-JDT: BLEU

(s) E-JDT: F1

(t) E-JDT: CIDEr

(u) E-JDT: ROUGE-L (v) E-JDT: ROUGE-1 (w) E-JDT: ROUGE-2

(x) E-JDT: RIBES

Fig. 13: Experimental results of the comparison of state-of-the-art
methods with varying code lengths.

30405060Code length05101520253035Acc (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq30405060Code length405060708090BLEU (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq30405060Code length505560657075808590F1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq30405060Code length0.51.01.52.0CIDEr (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq30405060Code length6065707580859095ROUGE-L (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq30405060Code length6065707580859095ROUGE-1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq30405060Code length303234363840424446ROUGE-2 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq30405060Code length5055606570758085RIBES (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq100150200250300Code length051015202530Acc (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq100150200250300Code length40506070BLEU (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq100150200250300Code length4550556065707580F1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq100150200250300Code length0.51.01.52.0CIDEr (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq100150200250300Code length606570758085ROUGE-L (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq100150200250300Code length6065707580ROUGE-1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq100150200250300Code length28303234363840ROUGE-2 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq100150200250300Code length505560657075RIBES (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq20406080100Code length051015202530Acc (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq20406080100Code length3040506070BLEU (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq20406080100Code length45505560657075F1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq20406080100Code length0.51.01.52.0CIDEr (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq20406080100Code length6065707580ROUGE-L (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq20406080100Code length6065707580ROUGE-1 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq20406080100Code length30323436384042ROUGE-2 (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq20406080100Code length505560657075RIBES (%)Seq2SeqTransformerSNMASNGB-CNNTreeGenADG-Seq2Seq38

8.1 Case Study

Chen Lyu1 et al.

We present an example for each dataset and select the results generated by GB-
CNN, TreeGen, and ADG-Seq2Seq. Figs. 14-16 show three cases of code descrip-
tions, original code snippets regarded as the ground truth, and the corresponding
predicted results. Fig. 17 visualizes attention in our proposed model for the target
sequences. GB-CNN, TreeGen, and ADG-Seq2Seq predict reasonable and gram-
matical code even if they are not entirely consistent with the ground truth.

HS : The ﬁrst example shown in Case 1 is a code snippet programmed in
Python. The semi-structured description makes code prediction in this scenario
relatively easy. The generated code is incomplete relative to the reference code
but implements the required main functionality. The main problems with these
prediction results include 1) omitting key functions/APIs and 2) using an incor-
rect number of arguments/parameters. Two key functions, “super().init ” and
“Minion()” are called in the reference code, but only one of them appears in
the prediction results of GB-CNN, while TreeGen and ADG-Seq2Seq generate all
functions. We also carefully inspect the number of arguments in each function.
ADG-Seq2Seq generates the above two functions with the correct order and num-
ber of arguments, while GB-CNN and TreeGen fail in the prediction of “Minion()”.
This demonstrates the importance of learning the invocation constraints implied
in the ADG for the exact usage of the arguments.

Furthermore, the prediction of our model uses diﬀerent parameter names that
do not appear in the current description and uses new API combinations that can-
not be found in the training samples. For example, our model generates “MINION Type
.DRAGON” and “Effect(SpellTargeted())” instead of “MINION Type.BEAST” and
“Effect(DidDamage())” in the reference code. This indicates that the model has
learned to generalize beyond simple translation. Although such a new substitution
or combination reduces the accuracy of the model, it shows that the model has
learned to generalize to some extent from global data dependencies among multi-
ple semantically or structurally similar training card examples. The generalization
and performance of our model are expected to improve as more training samples
and tighter global data dependencies become available.

MTG: Case 2 is an example from the MTG dataset. Similar to the HS dataset,
this card game dataset provides semi-structural descriptions but uses Java as its
programming language, making the code structure more complex. For example, the
number of functions and the frequency of API calls far exceed those in HS. Com-
paring the predicted results produced by the three models, it is clear that ADG-
Seq2Seq generates more comprehensive code and covers the largest number of func-
tions, 5 and 2 more than GB-CNN and TreeGen, respectively. Additionally, in con-
trast to the other two methods, ADG-Seq2Seq generates the correct invocation in a
long chain of APIs, e.g., “this.addAbility(VigilanceAbility.getInstance());”.
This example shows that the model has learned the usage of common API calls
through ADG embedding.

E-JDT : The third dataset is E-JDT; it was extracted from the Eclipse plug-in
library. As Case 3 shows, there are more data and call dependencies among the
API methods than within the isolated card programs. Thus, combined with the
API invocation order, our approach could predict the target code by virtue of
the ADG. However, the challenge of this dataset for code generation is that the
description is in the form of unstructured natural language, where function signa-

Embedding API Dependency Graph for Neural Code Generation

39

Fig. 14: Code examples generated by GB-CNN, TreeGen and ADG-Seq2Seq on
HS.

Case 1 Description Maexxna NAME_END 2 ATK_END 8 DEF_END 6 COST_END -1 DUR_END Minion TYPE_END Neutral PLAYER_CLS_END Beast RACE_END Legendary RARITY_END Destroy any minion damaged by this minion. Ground truth class Maexxna(MinionCard): def __init__(self): super().__init__("Maexxna", 6, CHARACTER_CLASS.ALL,  CARD_RARITY. LEGENDARY,  minion_type=MINION_TYPE. BEAST) def create_minion(self, player): return Minion(2, 8, effects=[Effect(DidDamage(), ActionTag(Kill(), TargetSelector(IsMinion())))]) GB-CNN class Maexxna(MinionCard): def __init__(self): super().__init__("Maexxna", 6, Histogram, CARD_RARITY. LEGENDARY,   minion_type=MINION_TYPE. BEAST) def create_minion(self, player): return mBaseName=baseName TreeGen class Maexxna(MinionCard): def __init__(self): super().__init__("Maexxna", 6, CARD_RARITY.RARE, minion_type=MINION_TYPE. BEAST) def create_minion(self, player): return Minion(8, 8) ADG-Seq2Seq class Maexxna(MinionCard): def __init__(self): super().__init__("Maexxna", 6, CHARACTER_CLASS.ALL, CARD_RARITY. LEGENDARY,   minion_type=MINION_TYPE. DRAGON) def create_minion(self, player): return Minion(2, 8, effects=[Effect(SpellTargeted()) 40

Chen Lyu1 et al.

Fig. 15: Code examples generated by GB-CNN, TreeGen and ADG-Seq2Seq on
MTG.

Case 2 Description Guardian Lions NAME_END 6 ATK_END 1 DEF_END {4}{W} COST_END NIL DUR_END Creature - Cat TYPE_END Magic 2013 PLAYER_CLS_END 17 RACE_END C RARITY_END Vigilance Ground truth public class GuardianLions extends CardImpl{ public GuardianLions(UUID ownerId){ super(ownerId, 17, "Guardian Lions", Rarity.COMMON, new CardType[]{CardType. CREATURE}, "{4}{W}"); this.expansionSetCode = "M13"; this.subtype.add("Cat"); this.power = new MageInt(1); this.toughness = new MageInt(6); this.addAbility(VigilanceAbility. getInstance());} public GuardianLions(final GuardianLions card){    super(card); } @Override public GuardianLions copy(){ return new GuardianLions(this); }} GB-CNN public class GuardianLions extends CardImpl private static final{ public GuardianLions(UUID ownerId) { super(ownerId, 17, "Guardian Lions", Rarity.COMMON, new CardType[] {CardType.INSTANT}, "{4}{W}"); this.power = new MageInt(1); this.getSpellAbility(); this.addAbility(ability); }} TreeGen public class GuardianLions extends CardImpl{ public GuardianLions(UUID ownerId) { super(ownerId, 17, "Guardian Lions", Rarity.COMMON, new CardType[] {CardType.CREATURE}, "{4}{W}"); this.expansionSetCode = "ARB"; this.subtype.add("Bird"); this.power = new MageInt(6); this.addAbility(ability1); } public GuardianLions copy(){ return new GuardianLions(this); }} ADG-Seq2Seq public class GuardianLions extends CardImpl{ public GuardianLions(UUID ownerId) {    super(ownerId, 17, "Guardian Lions", Rarity.COMMON, new CardType[] {CardType.CREATURE}, "{3}{B}{R}"); this.expansionSetCode = "ARB"; this.subtype.add("Soldier"); this.power = new MageInt(1); this.toughness = new MageInt(6); this.addAbility(VigilanceAbility. getInstance());} public GuardianLions(final GuardianLions card) {    super(card);} public GuardianLions copy(){ return new GuardianLions(this);}} Embedding API Dependency Graph for Neural Code Generation

41

Fig. 16: Code examples generated by GB-CNN, TreeGen and ADG-Seq2Seq on
EJDT.

Case 3 Description Reads the contents of the specified block into the buffer's page. If the buffer was dirty, then the contents of the previous page are first written to disk. Ground truth void assignToBlock(BlockId blk){     internalLock.writeLock().lock();     try{ flush(); this.blk=blk; contents.read(blk); pins=0; lastLsn=LogSeqNum. readFromPage(contents,LAST_LSN_OFFSET);}     finally{ internalLock.writeLock().unlock();}} GB-CNN void assignToBlock(fileMetadata){     internalLock.get(name);     try{ this.blk=blk; contents.read(blk); lastLsn=LogSeqNum. toString(address);}     finally{ return this.currentState;}} TreeGen void assignToBlock(BlockId blk){     internalLock.writeLock().lock();     try{ getF1(); this.blk=blk; getBytes(); pins=0;}     finally{ mReorderAlarm.cancelAlarm(); return blk;}} ADG-Seq2Seq void assignToBlock(BlockId blk){     internalLock.writeLock().lock();     try{ this.blk=blk; contents.read(blk); lastLsn=readPage(contents, LAST_LSN_OFFSET);}     finally{ internalLock.writeLock().unlock();}} 42

Chen Lyu1 et al.

(a) HS: attention weights

(b) MTG: attention weights

(c) E-JDT: attention weights

Fig. 17: Attention weights of Cases 1-3 for ADG-Seq2Seq.

MaexxnaNAME_END2ATK_END8DEF_END6COST_END-1DUR_ENDMinionTYPE_ENDNeutralPLAYER_CLS_ENDBeastRACE_ENDLegendaryRARITY_ENDDestroyanyminiondamagedbythisminion.classMaexxnaMinionCarddefinitselfsuperinitMaexxna6CHARACTERCLASSALLCARDRARITYLEGENDARYminiontypeMINIONTYPEBEASTdefcreateminionselfplayerreturnMinion28effectsEffectDidDamageActionTagKillTargetSelectorIsMinion0.00.20.40.60.81.0GuardianLionsNAME_END6ATK_END1DEF_END{4}{W}COST_ENDNILDUR_ENDCreatureCatTYPE_ENDMagic2013PLAYER_CLS_END17RACE_ENDCRARITY_ENDVigilanceiAttackingdoesn'tcausethiscreaturetotapipublicclassGuardianLionsextendsCardImplpublicGuardianLionsUUIDownerIdsuperownerId17GuardianLionsRarityCOMMONnewCardTypeCardTypeCREATURE4WthisexpansionSetCodeM13thissubtypeaddCatthispowernewMageInt1thistoughnessnewMageInt6thisaddAbilityVigilanceAbilitygetInstancepublicGuardianLionsfinalGuardianLionscardsupercardOverridepublicGuardianLionscopyreturnnewGuardianLionsthis0.00.20.40.60.81.0Readsthecontentsofthespecifiedblockintothebuffer'spageIfthebufferwasdirtythenthecontentsofthepreviouspagearefirstwrittentodiskvoidassignToBlockBlockIdblkinternalLockwriteLocklocktryflushthisblkblkcontentsreadblkpins0lastLsnLogSeqNumreadFromPagecontentsLAST_LSN_OFFSETfinallyinternalLockwriteLockunlock0.00.20.40.60.81.0Embedding API Dependency Graph for Neural Code Generation

43

tures cannot be learned directly from the tokens or keywords in the description,
as would be possible on HS and MTG datasets. Therefore, satisfactory results
are much more diﬃcult to obtain for E-JDT. Nevertheless, our approach predicts
method invocations such as “this.blk = blk, contents.read(blk).”, by learning
an applicable call sequence embedded in our model. Despite producing the incor-
rect class name, ADG-Seq2Seq predicts the method name in “lastLsn = readPage
(contents, LAST LSN OFFSET);,” more reasonably than GB-CNN in “lastLsn =
LogSeqNum.toString(address);.”. The results illustrated in Fig. 17(c) also show
that attention weights produced by our model focus on sequences of APIs instead
of variables, which exhibit a better capacity to capture the invocation structure
of the code.

8.2 Error Analysis

To understand the source of errors, we randomly sample 30, 100 and 100 failed
cases from HS, MTG, and E-JDT datasets, respectively. We observe that errors
causing these cases can be approximately categorized into four types: 1) Category
I (C-I): argument-related errors, including argument dislocation, incorrect num-
ber, and incorrect name; 2) Category II (C-II): function-related errors, including
incorrect function name and invalid function call; 3) Category III (C-III): variable-
related errors, including incorrect assignment and incorrect variable name; and 4)
Category IV (C-IV): partial implementation errors.

Argument-related errors occur more often in E-JDT, probably because of the
polysemy and ambiguity of natural language words. For function-related errors,
our model sometimes recommends similar names instead of reference names, such
as “ReadPage” instead of its reference “ReadFromPage” in Case 3 in Fig. 16. For
variable-related errors, our model fails to predict the assignment statement, “pins
= 0”, as illustrated by Case 3 in Fig. 16. The reason is that such variables are not
encoded by graph embedding, so their weights are easily weakened or even ignored
in our joint learning approach. A feasible solution is to integrate the variables into
the graph embedding network. We leave this step to future research. Finally, the
main reason for partial implementation errors is that the missing code has no
explicit requirement description. Fig. 14 provides a typical example in Case 1 :
the missing code “actiontag()” is not described in the reference. Notably, many
partial implementation errors also stem from combined impacts of the above four
types of errors.

Table 9 lists the error distributions of the other leading competitors, GB-CNN
and TreeGen, for comparison. The overall distribution of errors for our method
is generally consistent with those for the other methods. Speciﬁcally, the C-I and
C-II error rates of our method are slightly lower than those of the other methods,
while the C-III error rate for our method is somewhat higher than those for the
other methods. The C-IV error rates for all methods are at a high level. The
above statistical results indicate that our error rates are within an acceptable
range compared to the other methods. Furthermore, attempting to reduce C-III
and C-IV errors is an intriguing direction for future studies.

44

Chen Lyu1 et al.

Table 9: Error distributions of GB-CNN, TreeGen and our method.

Method

C-I

C-II

C-III C-IV

GB-CNN 16.7% 16.7% 3.3% 63.3%
16.7% 23.3% 3.3% 56.7%
TreeGen
10.0% 20.0% 6.7% 63.3%
Ours

GB-CNN 16.0% 22.0% 5.0% 57.0%
17.0% 28.0% 4.0% 51.0%
TreeGen
15.0% 25.0% 5.0% 55.0%
Ours

HS

MTG

E-JDT

GB-CNN 27.0% 23.0% 5.0% 45.0%
25.0% 26.0% 5.0% 44.0%
TreeGen
25.0% 20.0% 7.0% 48.0%
Ours

8.3 Human Evaluation

To investigate the eﬀectiveness of the generated code, similar to [54], we used a
combination of manual veriﬁcation and qualitative analysis to evaluate the quality
of such code. For veriﬁcation, nine evaluators assessed the results. Six were Ph.D.
students in computer science at Shandong Normal University, and the other three
were programmers hired from the Inspur Company8. The evaluators have two to
ﬁve years of Python or Java programming experience.

8.3.1 Survey Procedure

We randomly selected 50, 100 and 100 results from the test sets of HS, MTG, and
E-JDT, respectively. Each set of results was evaluated by two Ph.D. students and
a senior programmer. The test evaluators were required to rate each generated
code on a scale of 0 to 10 according to the code description. The ground truth
code was also provided for reference. Speciﬁcally, a score of (0) indicates that the
generated code is worthless, while a full score (10) means that the code perfectly
matches the description. Evaluators were encouraged to search the Internet for
necessary information if they encountered unfamiliar concepts.

8.3.2 Results

We obtained 750 scores from our human evaluation in which each generated code
had three scores from three evaluators; average scores were used.

Fig.18 provides the distribution of scores. The boxplots of HS are relatively
narrow, and their medians are higher than those of MTG and E-JDT. This result
suggests that the quality of code generated on HS is generally high, while code
generation tasks run on Java datasets appear to be more challenging, which is
unsurprising since their complexity (as shown in Table 3) and quantity (in terms
of the length of descriptions and code shown in Table 2) far exceed those of HS.
GB-CNN, TreeGen, and ADG-Seq2Seq all perform well on the HS dataset, but

8 The Inspur Company is a leading cloud computing and big data service provider in China.

More information is available at https://en.inspur.com/en/2488385/index.html.

Embedding API Dependency Graph for Neural Code Generation

45

the quality of the generated code varies on the MTG and E-JDT datasets. Overall,
ADG-Seq2Seq generates better code than GB-CNN or TreeGen.

Fig. 18: Results of the human evaluation.

More speciﬁcally, the medians of MTG and E-JDT are the same; however,
the boxplots in these examples show diﬀerent distributions of views, especially a
noticeable height diﬀerence between boxplots. To uncover the reasons behind this
discrepancy, we analyse factors such as the description and characteristics of the
code. On the one hand, the description of E-JDT is unstructured and is expressed
as a natural language statement. Therefore, due to the polysemy and fuzziness
of natural-language words, it is diﬃcult to perform well in every case. On the
other hand, E-JDT has a broad range of code lengths in terms of both tokens and
methods (see Table 2), which increases the uncertainty in predictions.

To assess the inter-rater agreement of scores, kappa coeﬃcients [55] were cal-
culated by aggregating the scores of each group. The ﬁnal kappa coeﬃcients for
the three groups were calculated to be 0.73, 0.69, and 0.67, respectively. According
to the classiﬁcation of the kappa coeﬃcient, scores rated by evaluators agree to
substantial degree and can be used for experimental analysis.

In summary, the human evaluation results conﬁrm that the proposed model

outperforms the compared methods.

9 Discussion

9.1 Specialty of the ADG Embedding Algorithm

The ADG is proposed to model the semantic structure of a program. We deﬁne
special reachability (see Deﬁnition 1) in an ADG that aims to describe two types

HS-ADGHS-GBCNNHS-TreeGenMTG-ADGMTG-GBCNNMTG-TreeGenE-JDT-ADGE-JDT-GBCNNE-JDT-TreeGenHS-ADGHS-GBCNNHS-TreeGenMTG-ADGMTG-GBCNNMTG-TreeGenE-JDT-ADGE-JDT-GBCNNE-JDT-TreeGenDatasets-Methods5678910Human scoring--------------------------------------------------------------------------------------------------------------------------------------46

Chen Lyu1 et al.

of dependencies in API sequences, i.e., API invocation constraints and API in-
vocation order. If there are APIs in the API sequence with more than one input
parameter, a sequence containing these APIs will be in the form of a DAG rather
than a chain. Graph embedding for generic graphs does not consider the special
reachability expressed by sequences in the DAG form or provide a targeted strat-
egy for preserving API dependencies during the embedding process. In contrast to
other graph embedding algorithms, the ADG embedding algorithm addresses the
above issues as follows.

– Encoding API invocation constraints: During embedding, we ﬁrst classify (for-
ward and backward nodes) and group (providers of diﬀerent types of param-
eters) the neighbour nodes of the current API according to their parameter
types. The virtualization operation is then used to aggregate the nodes of the
same group. Finally, the algorithm sorts the aggregated nodes topologically.
Through the above classiﬁcation, grouping, and virtualization operations, the
number of parameters, parameter types, and input and output that the current
API depends on can be determined. Therefore, our algorithm can diﬀerentiate
this information and encode the implicit API invocation constraints into each
API node.

– Encoding the API invocation order: To encode the order of API calls, we
ﬁrst input the neighbour nodes of each API into LSTM after grouping and
topological ordering. The topological order is then used as the temporal order
for aggregation. Existing research [11] has shown that LSTM-based sequential
aggregation is an eﬀective way to encode sequence order information.

In summary, through an in-depth analysis of the semantic structure of ADG-
expressed programs, we introduce targeted improvement strategies such as classiﬁ-
cation, grouping, topological ordering, and sequential aggregation in the neighbour
aggregation-based graph embedding algorithm. The API invocation constraints
and API invocation order are then eﬀectively encoded to guide the subsequent
learning and code generation tasks. The existing graph embedding techniques
(such as GCN or GraphSAGE) do not consider such strategies for fusing and
preserving the above two types of API dependencies.

9.2 The Option of Applying AST in Conjunction with ADG

AST-based approaches have performed well. In this paper, we attempt a diﬀerent
approach to explore other possibilities for code generation. A straightforward way
is to represent the AST and ADG of the code as two 1 × n-dimensional embedding
vectors and then aggregate them into a single vector. The aggregated vector is
used to bootstrap the decoder to generate the code. This approach is similar to the
research of Wan et al [56]. The researchers treat the token sequence, AST and CFG
as diﬀerent modalities of code, and then represent each modality with embedding
features via LSTM, Tree-LSTM, and GGNN. Speciﬁcally, they represent each
modality as three 1 × 512 vectors and fuse them into a single vector for code
retrieval. In contrast to the multimodal fusion strategy described above, the AST-
or ADG-based code generation methods decompose ASTs or ADGs into paths or
nodes and obtain ﬁne-grained path embedding or node embedding representations.

Embedding API Dependency Graph for Neural Code Generation

47

Existing research has shown that ﬁne-grained embedding representations of code
can aid in learning a more sophisticated syntactical or semantic structure.

Although incorporating AST into the proposed model can compensate for the
loss of missing syntactical structure information, it is challenging to integrate
such multimodal ﬁne-grained embedded representations eﬀectively. We plan to
investigate this topic in our future studies.

9.3 Limitations of the Proposed Approach

Our model still has some limitations. For example, the model must create an
ADG, which could be time consuming if the size of dataset size is large. For the
HS dataset, an epoch takes 32 seconds, while for MTG and EJDT, the training
times are 2100+ seconds and 61,000+ seconds, respectively. In addition, when
an ADG is being built, diﬀerent API extraction methods should be designed for
various programming languages. Furthermore, some nodes are more generic and
have more neighbouring nodes, which require more time for preprocessing and
post-training. In the future, we plan to follow the practice of GraphSAGE [24] to
build graph models by neighbourhood sampling.

In general, the accuracy of our model is still not satisfactory for practical use. In
Section 8.2, we discuss several types of errors that already exist in the model. This
suggests that our model is weak at learning vector representations of synthetic
identiﬁers that are out of vocabulary (OOV). In addition, the ability to reason
about implicit semantics (e.g., descriptions that do not contain code keywords)
is unsatisfactory. In our future research, we plan to study the composition and
splitting patterns of identiﬁers and to improve the encoding of various identiﬁers.

9.4 Threats to Validity

We have identiﬁed the following threats to validity.
Internal Validity. A threat to internal validity stems from biases and errors in
replications of other models. Although we used the source code provided by the
authors for each model, the speciﬁc parameters reported in the respective papers
may be unsuitable for our datasets. To mitigate this threat, we contacted some
of the authors of related studies and used parameter settings that could achieve
optimal results. The parameter settings of the models were also optimized through
repeated experiments. Therefore, we believe that there is only a minor threat to
internal validity.
Threats to external validity include the quality and rep-
External Validity.
resentativeness of datasets. The datasets used in this paper inevitably contain
noise that may aﬀect the prediction results. Although we attempted to eliminate
noise through pre-processing, we cannot guarantee that all data noise has been
eliminated. In addition, models in this paper are tested on only Python and Java
datasets, which are not representative of a wide range of programming languages.
Moreover, some programming languages (e.g., Java) involve third-party class li-
braries that contain packages used to extend functionality and cater to various
types of applications. Such libraries are quite useful for developers working on
diﬀerent projects. Incorporating the API dependencies of such libraries into our

48

Chen Lyu1 et al.

model could further improve the quality of generated code. Currently, we did not
speciﬁcally collect third-party class libraries. This may aﬀect the performance of
the proposed method to some extent. In the future, we plan to extend our approach
to other programming languages and incorporate more third-party libraries. Since
our model is trained on speciﬁc datasets, its generalization ability could be af-
fected. In contrast to transductive algorithms such as GCN, our ADG algorithm
is an extension of GraphSAGE, which is a framework for inductive learning that
eﬃciently generates unknown node embeddings using the attribute information of
nodes. Our approach has demonstrated the ability to generate code pieces that
do not appear in the codebase in our experiments. We present a concrete example
to illustrate this in detail in our case study of the HS dataset (see Section 8.1).
Therefore, our model is generalizable to a certain extent. However, the generaliz-
ability of our model can be enhanced if more training samples (a larger codebase)
are provided.

10 Conclusions

In this paper, we have presented a novel ADG-based Seq2Seq approach to generate
target code automatically based on textural program descriptions. This approach
transforms all API signatures mined from the speciﬁc codebase into a large-scale
ADG, where nodes represent API methods and edges with tags represent API de-
pendencies among nodes. We design an embedder that embeds an ADG to exploit
the global structural information it captures. By integrating the embedder into an
attention-based Seq2Seq model, the proposed approach can automatically learn
the ADG structure and generate the target code. We have provided quantitative
and qualitative comparisons of the proposed method with state-of-the-art tech-
niques, and the proposed method demonstrated signiﬁcant improvements in terms
of all eight metrics on the two Java datasets and in terms of ﬁve metrics on the
Python dataset.

Our tool, the datasets used in our experiments, and the detailed experimental

results are all available at https://github.com/RuYunW/ADG-Seq2Seq.

Our study can be regarded as one of the ﬁrst steps towards neural code gener-
ation. Although our results are signiﬁcantly better than baseline methods, further
improvement is needed prior to practical application. In addition to the research
directions identiﬁed in Section 9, we will further improve the accuracy of the pro-
posed model as follows:

– First, we plan to replace the LSTM with the Transformer to reshape the net-
work architecture of the proposed model. Although the embedder is currently
integrated within the model, it is feasible to make it standalone and incorpo-
rate it with the Transformer. The most crucial feature of the Transformer over
the LSTM is that it enables the model to batch-process input data and more
eﬀectively alleviate the long-range dependency problem. However, due to the
strong inﬂuence of the API invocation order on the code, the proposed model
based on the ADG embedding is more dependent on temporal order informa-
tion than are other approaches. The Transformer, based on position encoding,
is unable to capture these timing relationships well [57]. Therefore, embedding
an ADG into the Transformer straightforwardly suﬀers the limitation of the

Embedding API Dependency Graph for Neural Code Generation

49

loss of temporal order information. To eﬀectively integrate the ADG embed-
der and the Transformer, we will design unique mechanisms to address this
challenge.

– Second, we plan to combine the tasks of automatic code generation and auto-
matic program summary generation as a dual model [52] in which the proposed
model can be extended and improved by jointly training the two related tasks
concurrently.

– Third, we plan to extend the proposed approach to support more programming

languages and incorporate the characteristics of language features.

– Finally, we plan to apply ADG to diﬀerent tasks such as code classiﬁcation
[29, 58], retrieval [59] and summarization [48, 52, 60]. In this study, we have
focused on the application of ADG in code generation. The proposed ADG
could also be applied to other tasks such as code classiﬁcation. Some of the
initial results have been reported on our project website9. We will explore this
topic further in our future research.

Acknowledgements This work was supported in part by the National Natural Science Foun-
dation of China under Grant Nos. 61602286 and 61976127, in part by the Shandong Key Re-
search and Development Program under Grant 2018GGX101003, and in part by the Shandong
Province Higher Educational Science and Technology Program under Grant J16LN09.

References

1. Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann, Tom´aˇs
Koˇcisk`y, Fumin Wang, and Andrew Senior. Latent predictor networks for
code generation. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 599–609, 2016.
2. Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin,
Anthony Tomasic, and Graham Neubig. Retrieval-based neural code gener-
ation. In Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, pages 925–930, 2018.

3. Pengcheng Yin and Graham Neubig. A syntactic neural model for general-
purpose code generation. In Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1: Long Papers), pages 440–450,
2017.

4. Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu Zhang. A
grammar-based structural cnn decoder for code generation.
In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 7055–7062,
2019.

5. Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang.
Treegen: A tree-based transformer architecture for code generation. AAAI 2020
: The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, 34(5):8984–8991,
2020.

6. Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks
for code generation and semantic parsing. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1139–1149, 2017.

9 https://github.com/RuYunW/ADG-Seq2Seq/tree/master/code%20classiﬁcation

50

Chen Lyu1 et al.

7. Lili Mou, Rui Men, Ge Li, Lu Zhang, and Zhi Jin. On end-to-end pro-
gram generation from user intention by deep neural networks. arXiv preprint
arXiv:1510.07211, 2015.

8. Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural net-
works over tree structures for programming language processing. In Thirtieth
AAAI Conference on Artiﬁcial Intelligence, 2016.

9. Chris Quirk, Raymond Mooney, and Michel Galley. Language to code: Learn-
In Proceedings of the 53rd
ing semantic parsers for if-this-then-that recipes.
Annual Meeting of the Association for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pages 878–888, 2015.

10. Li Dong and Mirella Lapata. Language to logical form with neural attention.
In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 33–43, 2016.

11. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning
In Advances in neural information processing systems,

with neural networks.
pages 3104–3112, 2014.

12. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N
Dauphin. Convolutional sequence to sequence learning. In Proceedings of the
34th International Conference on Machine Learning-Volume 70, pages 1243–1252.
JMLR. org, 2017.

13. Minh-Thang Luong, Quoc V Le,

Lukasz Kaiser. Multi-task sequence to sequence learning.
arXiv:1511.06114, 2015.

Ilya Sutskever, Oriol Vinyals, and
arXiv preprint

14. Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase repre-
sentations using rnn encoder–decoder for statistical machine translation. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1724–1734, 2014.

15. Po-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean Oh, and Chris Dyer.
Attention-based multimodal neural machine translation. In Proceedings of the
First Conference on Machine Translation: Volume 2, Shared Task Papers, pages
639–645, 2016.

16. Jinchao Zhang, Mingxuan Wang, Qun Liu, and Jie Zhou. Incorporating word
reordering knowledge into attention-based neural machine translation. In Pro-
ceedings of the 55th Annual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1524–1534, 2017.

17. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine
translation by jointly learning to align and translate. In Proceedings of the 3rd
International Conference on Learning Representations, ICLR 2015, 2015.

18. Marta R Costa-juss`a and Jos´e AR Fonollosa. Character-based neural machine
In Proceedings of the 54th Annual Meeting of the Association for

translation.
Computational Linguistics (Volume 2: Short Papers), pages 357–361, 2016.
19. Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Summarizing source code with
transferred api knowledge. In Twenty-Seventh International Joint Conference on
Artiﬁcial Intelligence IJCAI-18, 2018.

20. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and
Gabriele Monfardini. The graph neural network model. IEEE Transactions on
Neural Networks, 20(1):61–80, 2008.

Embedding API Dependency Graph for Neural Code Generation

51

21. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning
of social representations. In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 701–710, 2014.
22. Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 855–864, 2016.

23. Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph

convolutional networks. arXiv preprint arXiv:1609.02907, 2016.

24. Will Hamilton, Zhitao Ying, and Jure Leskovec.

Inductive representation
learning on large graphs. In Advances in neural information processing systems,
pages 1024–1034, 2017.

25. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint
arXiv:1710.10903, 2017.

26. Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Codekernel: A graph kernel
based approach to the selection of api usage examples. In 2019 34th IEEE/ACM
International Conference on Automated Software Engineering (ASE), pages 590–
601. IEEE, 2019.

27. Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph

sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.

28. Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning
to represent programs with graphs. arXiv preprint arXiv:1711.00740, 2017.
29. Anh Viet Phan, Minh Le Nguyen, and Lam Thu Bui. Convolutional neural
networks over control ﬂow graphs for software defect prediction. In 2017 IEEE
29th International Conference on Tools with Artiﬁcial Intelligence (ICTAI), pages
45–52. IEEE, 2017.

30. Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli.
Graph matching networks for learning the similarity of graph structured ob-
jects. In International Conference on Machine Learning, pages 3835–3845, 2019.
31. Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. Detecting code clones with
graph neural network and ﬂow-augmented abstract syntax tree. In 2020 IEEE
27th International Conference on Software Analysis, Evolution and Reengineering
(SANER), pages 261–271. IEEE, 2020.

32. Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Olek-
arXiv preprint

sandr Polozov. Generative code modeling with graphs.
arXiv:1805.08490, 2018.

33. Ronald J Williams and David Zipser. A learning algorithm for continually
running fully recurrent neural networks. Neural computation, 1(2):270–280,
1989.

34. Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term

memory. arXiv preprint arXiv:1507.01526, 2015.

35. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In International conference on
machine learning, pages 2048–2057, 2015.

36. Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Re-
current models of visual attention.
In Proceedings of the 27th International
Conference on Neural Information Processing Systems - Volume 2, NIPS’14, page
2204–2212, Cambridge, MA, USA, 2014. MIT Press.

52

Chen Lyu1 et al.

37. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Eﬀective ap-
proaches to attention-based neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural Language Processing, pages
1412–1421, 2015.

38. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, (cid:32)Lukasz Kaiser, and Illia Polosukhin. Attention is all you
need. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, pages 6000–6010. Curran Associates Inc., 2017.

39. Alfred V Aho, Mahadevan Ganapathi, and Steven WK Tjiang. Code gener-
ation using tree matching and dynamic programming. ACM Transactions on
Programming Languages and Systems (TOPLAS), 11(4):491–516, 1989.

40. Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine.
Neural sketch learning for conditional program generation. arXiv preprint
arXiv:1703.05698, 2017.

41. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimiza-

tion. arXiv preprint arXiv:1412.6980, 2014.

42. Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment gener-
ation. In Proceedings of the 26th Conference on Program Comprehension, pages
200–210, 2018.

43. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a
method for automatic evaluation of machine translation. In Proceedings of the
40th annual meeting on association for computational linguistics, pages 311–318.
Association for Computational Linguistics, 2002.

44. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider:
Consensus-based image description evaluation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages 4566–4575, 2015.

45. Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie.

An
information-theoretic approach to automatic evaluation of summaries. In Pro-
ceedings of the main conference on Human Language Technology Conference of the
North American Chapter of the Association of Computational Linguistics, pages
463–470. Association for Computational Linguistics, 2006.

46. Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime
Tsukada. Automatic evaluation of translation quality for distant language
pairs.
In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 944–952. Association for Computational Linguis-
tics, 2010.

47. Ke Wang, Rishabh Singh, and Zhendong Su. Dynamic neural program em-

bedding for program repair. arXiv preprint arXiv:1711.07163, 2017.

48. Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment gen-
eration with hybrid lexical and syntactical information. Empirical Software
Engineering, 25(3):2179–2217, 2020.

49. Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Rus-
lan Salakhutdinov. Dropout: a simple way to prevent neural networks from
overﬁtting. The journal of machine learning research, 15(1):1929–1958, 2014.
50. Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training
deep feedforward neural networks. In Proceedings of the thirteenth international
conference on artiﬁcial intelligence and statistics, pages 249–256, 2010.

51. Graham Neubig.

lamtram: A toolkit for language and translation modeling

using neural networks, 2015.

Embedding API Dependency Graph for Neural Code Generation

53

52. Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a
dual task of code summarization. In Advances in Neural Information Processing
Systems, pages 6559–6569, 2019.

53. Kuangqi Zhou, Yanfei Dong, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi
Feng. Eﬀective training strategies for deep graph neural networks. arXiv
preprint arXiv:2006.07107, 2020.

54. Zhongxin Liu, Xin Xia, Christoph Treude, David Lo, and Shanping Li. Auto-
matic generation of pull request descriptions. In 2019 34th IEEE/ACM Inter-
national Conference on Automated Software Engineering (ASE), pages 176–188.
IEEE, 2019.

55. Julius Sim and Chris C Wright. The kappa statistic in reliability studies: use,
interpretation, and sample size requirements. Physical therapy, 85(3):257–268,
2005.

56. Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and
Philip Yu. Multi-modal attention network learning for semantic source code
retrieval. In 2019 34th IEEE/ACM International Conference on Automated Soft-
ware Engineering (ASE), pages 13–25. IEEE, 2019.

57. Vighnesh Shiv and Chris Quirk. Novel positional encodings to enable tree-
In Advances in Neural Information Processing Systems,

based transformers.
pages 12081–12091, 2019.

58. Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and
Xudong Liu. A novel neural source code representation based on abstract
syntax tree. In 2019 IEEE/ACM 41st International Conference on Software En-
gineering (ICSE), pages 783–794. IEEE, 2019.

59. Abdus Satter and Kazi Sakib. A similarity-based method retrieval technique
to improve eﬀectiveness in code search. In Companion to the ﬁrst International
Conference on the Art, Science and Engineering of Programming, pages 1–3, 2017.
60. Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu.
Retrieval-based neural source code summarization. In Proceedings of the 42nd
International Conference on Software Engineering. IEEE, 2020.

54

Chen Lyu1 et al.

Appendix A More Details about the Experimental Data and Settings
Used in Comparisons

In Section 7.1, we describe our experimental data. For models with existing exper-
imental data, we use the experimental results presented in the original paper10.
For those without, we retrain the model with the default parameters provided in
the original paper and calculate the experimental results under diﬀerent metrics.

More speciﬁcally, we compared six models: the attention-based Seq2Seq model [51]

and the Transformer model [38] were used as baselines; and SNM [3], ASN [6], GB-
CNN [4], and TreeGen [5] were used as the state-of-the-art (SOTA) methods.

All evaluation results of the baselines (the attention-based Seq2Seq model and
the Transformer model) were calculated by training the models on HS, MTG and
E-JDT datasets (see Table 4 and Fig. 9), and on the split datasets (see Fig.s 12
and 13).

In the original publications of the four SOTA methods (SNM, ASN, GB-CNN,
and TreeGen) [3–6], HS was chosen as one of the experimental datasets, which is
also used in our experiments. The scores of BLEU and Acc metrics on HS in Table
4 and Fig. 9 were obtained directly from the respective papers (see Table 4 and
Fig. 9 (a)-(b)). Since the papers describing SNM, ASN, GB-CNN, and TreeGen
did not provide scores apart from Acc and BLEU, we have to retrain these models
to obtain the remaining metric scores (see Table 4 HS: F1-RIBES). We use the
same hyperparameter values as those described in the respective paper. As the
authors of SNM provided the trained model, we calculated the scores on HS by
using their trained model.

For datasets MTG, E-JDT, and the split datasets, since the papers describing
the SOTA methods did not provide any evaluation scores, we retrained their mod-
els based on the hyperparameters provided in the original papers, and calculated
scores for all metrics on these datasets (see Table 4, Fig 9 (i)-(x), Figs. 12 and 13).
In detail, the experimental data and settings for each model are as follows:

1. All evaluation results of the attention-based Seq2Seq model and the Trans-
former model are calculated by us. The attention-based Seq2Seq model is our
basic model. The Transformer is a well-known NMT model in the ﬁeld of
NLP, and the relevant code is available at https://github.com/jadore801120/
attention-is-all-you-need-pytorch.

2. The SNM model was proposed by Yin et al. [3] and its code is available at
https://github.com/pcyin/NL2code. We use the ACC and BLEU values from
the paper and calculate other metrics by testing the results on HS from the
trained model they provided. For MTG and E-JDT, all metrics’ values are
determined by retraining the model. The size of all embeddings is 128, except
for node type embeddings, the size of which is 64. Dimensions of RNN states
and hidden layers are 256 and 50, respectively.

3. The ASN model was proposed by Rabinovich et al. [6]. This paper does not
provide the source code for the implementation, but since this study has had

10 Most of the replication results are consistent with those reported in the original paper,
and a few (BLEU scores for ASN and SNM) are slightly lower (the diﬀerence is about 0.8%
in BLEU). Considering the stochastic nature of deep learning, the diﬀerences in these results
are acceptable. We choose to present the original results for each method in the paper, which
help to maintain consistency with the results reported by the related studies [3–6].

Embedding API Dependency Graph for Neural Code Generation

55

a large impact, we have been able to ascertain the ASN code implemented by
other scholars via PyTorch at https://github.com/xiye17/torchASN. Hence,
the Acc and BLEU scores were copied from the cited paper. Other scores on
HS and scores on MTG and E-JDT were computed by us by retraining the
model. For each experiment, all feedforward and LSTM hidden dimensions
were set to the same value. We selected the dimension from {50, 75, 100, 125,
150} for all datasets. The dimensionality used for the inputs to the encoder
was set to 100 in all cases. We applied dropout to non-recurrent connections
of vertical and horizontal LSTMs, selecting the noise ratio from {0.2, 0.3, 0.4,
0.5}. All parameters were randomly initialized using Glorot initialization.
4. GB-CNN is a CNN-based model proposed by Sun et al. [4]. We copied the
StrAcc and BLEU ﬁgures for HS. GB-CNN is also an open-source model avail-
able at https://github.com/zysszy/GrammarCNN. We retrained the model fol-
lowing default settings to evaluate other metrics on all three datasets. For
the input descriptions, we replaced all punctuations with a space; all letters
were lowercase. For the neural network, we set the number of CNN layers L
to 21, where the bottom layer does not have skipping connections. The layers
of diﬀerence CNN modules were set to the same dimension and chosen by val-
idation from {128, 192, 256} for each predictor network. We applied dropout
(drop rate= 0.5) and l2 penalty to regularize the fully connected layers. The
network was trained by the Adam optimizer with default hyperparameters.
5. TreeGen was proposed by Sun et al. [5] and is a Transformer-based model. Its
implementation is available at https://github.com/zysszy/TreeGen. The Acc
and BLEU numbers were copied from the cited paper, and other evaluation
results were obtained by training the model on the three datasets. For neural
networks, we set the number of NL reader layers Nd = 6, and N1 = N2 =
5 for the AST reader as well as the decoder. The size of all embeddings was
256. The hidden sizes were all set to 256 except for each fully-connected layer
and the ﬁrst layer, which had 1024 dimensions. We applied dropout after each
layer (including attention layers, the gating mechanism’s layers, convolutional
layers, and fully-connected layers, where the drop rate was 0.15). The model
was optimized by Adafactor with default parameters.

56

Chen Lyu1 et al.

Chen Lyu received his Ph.D. from the Institute of Comput-
ing Technology, Chinese Academy of Sciences, Beijing, China, in
2015. He is currently an associate professor with the School of
Information Science and Engineering, Shandong Normal Uni-
versity, Jinan, China. His research interests include program
comprehension, software maintenance and evolution, and ser-
vice computing.

Ruyun Wang is pursuing a bachelor’s degree in the School
of Information Science and Engineering, Shandong Normal Uni-
versity, Jinan, China. Her research interests include code gen-
eration and component-based software development.

Hongyu Zhang is currently an associate professor with the
University of Newcastle, Australia. Previously, he was a lead
researcher at Microsoft Research Asia and an associate pro-
fessor at Tsinghua University, China. He received his Ph.D.
from the National University of Singapore in 2003. His re-
search is in the area of software engineering, in particular,
software analytics, data-driven software engineering, mainte-
nance, and reuse. He has published more than 160 research papers in reputable
international journals and conferences, including TSE, TOSEM, ICSE, FSE, ASE,
ISSTA, POPL, AAAI, IJCAI, KDD, ICSME, ICDM, and USENIX ATC. He re-
ceived four ACM Distinguished Paper awards. He has also served as a program
committee member/track chair for many software engineering conferences. He is
a Distinguished Member of ACM. More information about him can be found at:
https://www.newcastle.edu.au/profile/hongyu-zhang.

Hanwen Zhang received a master’s degree in software en-
gineering from Shandong Normal University, Jinan, China, su-
pervised by Chen Lyu in 2020. She is currently working in the
Big Data Center of Shandong province. Her research interests
include software reuse, natural language processing, and auto-
matic code comment generation.

Embedding API Dependency Graph for Neural Code Generation

57

Songlin Hu received his Ph.D. from Beihang University,
Beijing, China, in 2001. He was promoted to associate profes-
sor in 2002 in the Institute of Computing Technology, Chinese
Academy of Sciences, Beijing, China, and is now a professor
in the Institute of Information Engineering, Chinese Academy
of Sciences, and School of Cyber Security, University of Chi-
nese Academy of Sciences, Beijing, China. His research inter-
ests mainly include natural language processing, API mining,

service computing, etc.

