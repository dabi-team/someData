Code Structure Guided Transformer for Source Code Summarization

SHUZHENG GAO, Harbin Institute of Technology, Shenzhen, China
CUIYUN GAO∗, Harbin Institute of Technology, Shenzhen, China
YULAN HE, University of Warwick, UK
JICHUAN ZENG, The Chinese University of Hong Kong, Hong Kong, China
LUN YIU NIE, Tsinghua University, China
XIN XIA, Software Engineering Application Technology Lab, Huawei, China
MICHAEL R. LYU, The Chinese University of Hong Kong, Hong Kong, China

Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software

maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code

summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the

code structure information into the Transformer is under-explored in this task domain. In this paper, we propose a novel approach

named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g.,

code tokens and statements) and global syntactic structure (e.g., data flow graph) into the self-attention module of Transformer as

inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to

distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance

of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0%

in terms of METEOR score, a metric widely used for measuring generation quality, respectively on two benchmark datasets.

CCS Concepts: • Software and its engineering → Software creation and management; Software development techniques.

Additional Key Words and Phrases: Code summary, Transformer, multi-head attention, code structure.

ACM Reference Format:

Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lun Yiu Nie, Xin Xia, and Michael R. Lyu. 2022. Code Structure Guided

Transformer for Source Code Summarization. In Woodstock ’18: ACM Symposium on Neural Gaze Detection, June 03–05, 2018, Woodstock,

NY . ACM, New York, NY, USA, Article 1, 31 pages. https://doi.org/10.1145/3522674

1 INTRODUCTION

Program comprehension is crucial for developers during software development and maintenance. However, existing

studies [42, 64] have shown that program comprehension is a very time-consuming activity which occupies over 50% of

the total time in software maintenance. To alleviate the developers’ cognitive efforts in comprehending programs, a text

summary accompanying the source code is proved to be useful [9, 17, 27]. However, human-written comment is often

incomplete or outdated because of the huge effort it takes and the rapid update of software [13, 49]. The source code

∗Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2022 Association for Computing Machinery.
Manuscript submitted to ACM

1

2
2
0
2

l
u
J

2
2

]
L
C
.
s
c
[

2
v
0
4
3
9
0
.
4
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

Fig. 1. An example of Java code snippet (a), with the corresponding AST (b) and DFG (c) illustrated. Entities in grey ellipse in (b)
mean unexpanded branches. The arrows in the DFG represent the relations of sending/receiving messages between the variables
(highlighted in grey in the code).

summarization task aims at automatically generating a concise comment of a program. Many studies [11, 39, 53, 61] have

demonstrated that the machine-generated summaries are helpful for code comprehension. A recent empirical study [29]

also shows that 80% of practitioners consider that code summarization tools can help them improve development

efficiency and productivity.

Existing leading approaches have demonstrated the benefits of integrating code structural properties such as Abstract

Syntax Trees (ASTs) [4, 27] into deep learning techniques for the task. An example of AST is shown in Figure 1 (b). The

modality of the code structure can be either sequences of tokens traversed from the syntactic structure of ASTs [4, 27] or

sequences of small statement trees split from large ASTs [50, 68]. The sequences are usually fed into a Recurrent Neural

Network (RNN)-based sequence-to-sequence network for generating a natural language summary [27, 36]. However,

due to the deep nature of ASTs, the associated RNN-based models may fail to capture the long-range dependencies

between code tokens [1]. To mitigate this issue, some works represent the code structure as graphs and adopt Graph

2

1public booleanIsPrime(int num1) {    2int i1=2;3bool  flag1= false; 4while( i2< num2 ) {5if(num3%i3== 0) {6flag2 = true;7 break;}        8i5= i4+1;9}10return flag3;11}MethodDeclarationbooleanIsPrimeBlockParamVarDecWhileStmtReturnVarDecintVariable DeclaratornumBinary Expr:lessBlockStmtIfStmtExpressionStmtBlockstmtBinary Expr:equals0numiBinary Expr:remainderPrimitivePrimitiveVarDecld2IntegerIntegerNameNamenum1num2num3flag1flag2flag3i1i2i3i4i5(a) An example of code snippet(c) The data flow graph (DFG)(b) The abstract syntax tree (AST)Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Neural Networks (GNNs) for summary generation [16, 35]. Although these GNN-based approaches can capture the

long-range relations between code tokens, they are shown sensitive to local information and ineffective in capturing

the global structure [30]. Taking the AST in Figure 1 (b) as an example, token nodes “int” and “num” (highlighted with

red boxes) are in the same statement but separated by five hops, so GNN-based approaches tend to ignore the relations

between the two token nodes. Besides, the message passing on GNNs is limited by the pre-defined graph, reducing its

scalability to learn other dependency relations.

Recent study [1] shows that the Transformer model [55] outperforms other deep learning approaches for the task.

The self-attention mechanism in Transformer can be viewed as a fully-connected graph [22], which can ensure the

long-range massage passing between tokens and the flexibility to learn any dependency relation from data. However,

it is hard for the Transformer to learn all important dependency relations from limited training data. Besides, an

issue of Transformer is that its attention is purely data-driven [23]. Without the incorporation of explicit constraints,

the multi-head attentions in Transformer may suffer from attention collapse or attention redundancy, with different

attention heads extracting similar attention features, which hinders the model’s representation learning ability [5, 56].

To solve the aforementioned problems, we incorporate code structure into the Transformer as prior information to

eliminate its dependency on data. However, how to effectively integrate the code structure information into Transformer

is still under-explored. One major challenge is that since the position encoding in the Transformer already learns

the dependency relations between code tokens, trivial integration of the structure information may not bring an

improvement for the task [1].

To overcome the above challenges in this paper, we propose a novel model named SG-Trans, i.e., code Structure

Guided Transformer. SG-Trans exploits the code structural properties to introduce explicit constraints to the multi-head

self-attention module. Specifically, we extract the pairwise relations between code tokens based on the local symbolic

structure such as code tokens and statements, and the global syntactic structure, i.e., data flow graph (DFG), then

represent them as adjacency matrices before injecting them into the multi-head attention mechanism as inductive bias.

Furthermore, following the principle of compositionality in language: the high-level semantics is the composition of

low-level terms [23, 54], we propose a hierarchical structure-variant attention approach to guide the attention heads

at the lower layers attending more to the local structure and those at the higher layers attending more to the global

structure. In this way, our model can take advantage of both local and global (long-range dependencies) information

of source code. Experiments on benchmark datasets demonstrate that SG-Trans can outperform the state-of-the-art

models by at least 1.4% and 2.0% in terms of METEOR on two Java and Python benchmark datasets, respectively.

In summary, our work makes the following contributions:

• We are the first to explore the integration of both local and global code structural properties into Transformer

for source code summarization.

• A novel model is proposed to hierarchically incorporate both the local and global structure of code into the

multi-head attentions in Transformer as inductive bias.

• Extensive experiments show SG-Trans outperforms the state-of-the-art models. We publicly release the replication
repository including source code, datasets, prediction logs, online questionnaire, and results of human evaluation
on GitHub1.

Paper structure. Section 2 illustrates the background knowledge of the work. Section 3 presents our proposed

methodology for source code summarization. Section 4 introduces the experimental setup. Section 5 describes the

1https://github.com/gszsectan/SG-Trans

3

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

evaluation results, followed by the discussions in Section 6. Section 7 presents related studies. Finally, Section 8 concludes

the paper and outlines future research work.

2 BACKGROUND

In this section, we introduce the background knowledge of the proposed approach, including the vanilla Transformer

model architecture and the copy mechanism.

2.1 Vanilla Transformer

Transformer [55] is a kind of deep self-attention network which has demonstrated its powerful text representation

capability in many NLP applications, e.g., machine translation and dialogue generation [51, 69]. Recently, a lot of

research in code summarization also leverage Transformer as backbone for better source code representations [1, 15].

Some work [26, 58, 70] also improves the Transformer to make it better adapt to source code or structured data. Unlike

conventional neural networks such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN),

it is solely based on attention mechanism and multi-layer perceptrons (MLPs). Transformer follows the sequence-to-

sequence [10] architecture with stacked encoders and decoders. Each encoder block and decoder block consist of a

multi-head self-attention sub-layer and a feed-forward sub-layer. Residual connection [25] and layer normalization [6]

are also employed between the sub-layers. Since the two sub-layers play an essential role in Transformer, We introduce

them in more detail as the following.

2.1.1 Multi-Head Self-Attention. Multi-head attention is the key component of Transformer. Given an input sequence
𝑋 = (𝑥1, 𝑥2, ..., 𝑥𝑖, ..., 𝑥𝑛) where 𝑛 is the sequence length and each input token 𝑥𝑖 is represented by a 𝑑-dimension vector,
self-attention first calculates the Query vector, the Key vector, and the Value vector for each input token by multiplying
the input vector with three matrices 𝑊 𝑞, 𝑊 𝑘 , 𝑊 𝑣. Then it calculates the attention weight of sequence 𝑋 by scoring
the query vector 𝑄 against the key vector 𝐾 of the input sentence. The scoring process is conducted by the scaled dot
product, as shown in Eq. (2), where the dimension 𝑑 in the denominator is used to scale the dot product. Softmax is then
used to normalize the attention score and finally the output vectors is computed as a weighted sum of the input vectors.

Instead of performing a single self-attention function, Transformer adopts multi-head self-attention (MHSA) which

performs the self-attention function with different parameters in parallel and ensembles the output of each head by

concatenating their outputs. The MHSA allows the model to jointly attend to information from different representation

subspaces at different positions. Formally, the MHSA is computed as following:

𝑄𝑖 = 𝑋𝑊 𝑞

𝑖 , 𝐾𝑖 = 𝑋𝑊 𝑘

ℎ𝑒𝑎𝑑𝑖 = softmax(

𝑖 , 𝑉𝑖 = 𝑋𝑊 𝑣
𝑖 ,
𝑄𝑖𝐾𝑇
𝑖
√
𝑑

)𝑉𝑖,

𝑀𝐻𝑆𝐴(𝑋 ) = [ℎ𝑒𝑎𝑑𝑙

1 ◦ ℎ𝑒𝑎𝑑𝑙

2 ◦ · · · ℎ𝑒𝑎𝑑𝑙

𝑖 ◦ · · · ℎ𝑒𝑎𝑑𝑙

ℎ]𝑊 𝑂,

(1)

(2)

(3)

where ℎ denotes the number of attention heads at 𝑙-th each layer, the symbol ◦ indicates the concatenation of ℎ different
heads, and 𝑊 𝑞

𝑖 and 𝑊 𝑂 are trainable parameters.

𝑖 , 𝑊 𝑣

𝑖 , 𝑊 𝑘

2.1.2

Feed-Forward Network. Feed-forward network is the only nonlinear part in Transformer. It consists of two linear

transformation layer and a ReLU activation function between the two linear layers:

𝐹 𝐹 𝑁 (𝑋 ) = 𝑅𝑒𝐿𝑈 (𝑋𝑊1 + 𝑏1)𝑊2 + 𝑏2,
4

(4)

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Fig. 2. Architecture of copy mechanism.

where 𝑊1, 𝑊2, 𝑏1, and 𝑏2 are trainable parameters which are shared across input positions.

2.2 Copy Mechanism

Copy mechanism [19] has been widely equipped in text generation models for extracting words from a source sequence

as part of outputs in a target sequence during text generation. It has been demonstrated that copy mechanism can

alleviate the out-of-vocabulary issue in the code summarization task [1, 67, 70]. Besides, copying some variable name

can also help generate more precise summary. In this work, we adopt the pointer generator [48], a more popular form

of copy mechanism, for the task. Figure 2 illustrates the architecture of the pointer generator model. Specifically, given
an input sequence 𝑋 = (𝑥1, 𝑥2, ..., 𝑥𝑛), a decoder input 𝑤𝑡 , a decoder hidden state 𝑠𝑡 , and a context vector 𝑐𝑡 computed
by the attention mechanism in time step 𝑡, the pointer generator first calculates a constant 𝑃𝑔𝑒𝑛 which is later used
as a soft switch for determining whether to generate a token from the vocabulary or to copy a token from the input
sequence 𝑋 :

𝑃𝑔𝑒𝑛 = sigmoid(𝜔T

𝑠 𝑠𝑡 + 𝜔T
𝑃𝑣𝑜𝑐𝑎𝑏 (𝑤𝑡 ) = softmax(𝑊𝑎𝑠𝑡 + 𝑉𝑎𝑐𝑡 )

𝑤𝑤𝑡 + 𝜔T

𝑐 𝑐𝑡 + 𝑏𝑔𝑒𝑛),

𝑃 (𝑤𝑡 ) = 𝑃𝑔𝑒𝑛𝑃𝑣𝑜𝑐𝑎𝑏 (𝑤𝑡 ) + (1 − 𝑃𝑔𝑒𝑛)𝑃𝑐𝑜𝑝𝑦 (𝑤𝑡 ),

5

(5)

(6)

(7)

Copy distribution 𝑷𝒄𝒐𝒑𝒚Vocabulary distribution 𝑷𝒗𝒐𝒄𝒂𝒃.  .  .Final distribution𝑷LinearSoftmax𝑷𝒈𝒆𝒏𝟏−𝑷𝒈𝒆𝒏𝑷𝒈𝒆𝒏Context VectorEncoderDecoder.  .  ..  .  .𝑠𝑡𝑤𝑡𝒄𝒕Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

Fig. 3. Overall framework of the proposed SG-Trans. The “Structure-Guide Self-Attention” part illustrates different self-attention
mechanisms between adjacent layers.

where vectors 𝜔𝑠 , 𝜔𝑤, 𝜔𝑐 , 𝑊𝑎, 𝑉𝑎 and scalar 𝑏𝑔𝑒𝑛 are learnable parameters. 𝑃 (𝑤𝑡 ) is the probability distribution over
the entire vocabulary. Copy distribution 𝑃𝑐𝑜𝑝𝑦 (𝑤𝑡 ) determines where to attend to in time step 𝑡, computed as:

where 𝛼𝑡 indicates the attention weights and 𝑖 : 𝑥𝑖 = 𝑤 indicates the indices of input words in the vocabulary.

𝑃𝑐𝑜𝑝𝑦 (𝑤𝑡 ) =

∑︁

𝛼𝑡,𝑖 ,

𝑖:𝑥𝑖 =𝑤

(8)

3 PROPOSED APPROACH

In this section, we explicate the detailed architecture of SG-Trans. Let 𝐷 denotes a dataset containing a set of programs
𝐶 and their associated summaries 𝑍 , given source code 𝑐 = (𝑥1, 𝑥2, ..., 𝑥𝑛) from 𝐶, where 𝑛 denotes the code sequence
length. SG-Trans is designed to generate the summary consisting of a sequence of tokens ˆ𝑧 = (𝑦1, 𝑦2, ..., 𝑦𝑚) by
maximizing the conditional likelihood: ˆ𝑧 = arg max𝑧 𝑃 (𝑧|𝑐) (𝑧 is the corresponding summary in 𝑍 ).

The framework of SG-Trans is mostly consistent with the vanilla Transformer, but consists of two major improvements,

namely structure-guided self-attention and hierarchical structure-variant attention. Figure 3 depicts the overall architecture.

SG-Trans first parses the input source code for capturing both local and global structure. The structure information

is then represented as adjacency matrices and incorporated into the self-attention mechanism as inductive biases

(introduced in Section 3.1). Following the principle of compositionality in language, different inductive biases are

integrated into the Transformer at difference levels in a hierarchical manner (introduced in Section 3.2).

6

Structure-Guided Self-AttentionFeed ForwardEncoderAdd & NormPositional EncodingSoftmaxSource CodeCode Summary(b) Token-guided head attention(c) Statement-guided head attention(d) Data flow-guided head attention×𝐿Input EmbeddingDecodertokentokentokentokentokentoken…………statementstatementstatement……Add & NormParser(a) Standard head attentiontokenstatementdata flow……Structure-Guided Self-AttentionHierarchical Scale-Variant Attention……×𝐿Hierarchical Structure-Variant AttentionstandardCode Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

3.1 Structure-Guided Self-Attention

In the standard multi-head self-attention model [55], every node in the adjacent layer is allowed to attend to all the

input nodes, as shown in Figure 3 (a). In this work, we propose to use the structural relations in source code to introduce

explicit constraints to the multi-head self-attention. In order to capture the hierarchical structure of source code, we

utilize three main types of structural relations between code tokens, including local structures: whether the two split

sub-tokens originally belong to the same 1) token or 2) statement, and global structure: whether there exists a 3) data

flow between two tokens. For each structure type, we design the corresponding head attention, named as token-guided

self-attention, statement-guided self-attention, and data flow-guided self-attention, respectively.

Token-guided self-attention. Semantic relations between the sub-tokens are relatively stronger than the relations

between the other tokens. For example, the method name “IsPrime” in the code example shown in Figure 1 (a) is split as

a sequence of sub-tokens containing “Is” and “Prime”. Moreover, the semantic relation between them is stronger than

the relation between “Is” and “num” in the same statement. Therefore, the attention can be built upon the extracted

token-level structure, i.e, whether two sub-tokens are originally from the same source code token. We use an adjacency
matrix T𝑛×𝑛 to model the relationship, where 𝑡𝑖 𝑗 = 0 if the 𝑖-th and 𝑗-th elements are sub-tokens of the same token in
the code; Otherwise, 𝑡𝑖 𝑗 = −∞. The matrix is designed to restrict the attention head to only attend to the sub-tokens
belonging to the same code token in self-attention, as shown in Figure 3 (b). Given the input token representation
X ∈ R𝑛×𝑑ℎ, where 𝑛 is the sequence length, 𝑑 is the input dimension of each head, and ℎ is the number of attention
heads. Let Q, K, and V denote the query, key, and value matrix, respectively, the token-guided single-head self-attention
ℎ𝑒𝑎𝑑𝑡 can be calculated as:

(cid:18) QK⊺
√
𝑑
𝑑 is a scaling factor to prevent the effect of large values.

ℎ𝑒𝑎𝑑𝑡 = softmax

√

where

(cid:19)

V,

+ T

(9)

Statement-guided self-attention. Tokens in the same statement tend to possess stronger semantic relations than

those from different statements. For the code example given in Figure 1 (a), the token “flag” in the third statement is

more relevant to the tokens “bool” and “False” in the same statement than to the token “break” in the 7-th statement. So
we design another adjacency matrix 𝑆 to represent the pairwise token relations capturing whether the two tokens are
from the same statement. In the matrix S, 𝑠𝑖 𝑗 = 0 if the 𝑖-th and 𝑗-th input tokens are in the same statement; otherwise,
𝑠𝑖 𝑗 = −∞. The design is to restrict the attention head to only attend to the tokens from the same statement, as illustrated
in Figure 3 (c). The statement-guided single-head self-attention ℎ𝑒𝑎𝑑𝑠 is defined below, similar to the token-guided
head attention:

ℎ𝑒𝑎𝑑𝑠 = softmax

(cid:18) QK⊺
√
𝑑

(cid:19)

V.

+ S

(10)

Data flow-guided self-attention. To facilitate the model to learn the global semantic information from code, we

employ the data flow graphs (DFGs) for capturing the global semantic structure feature [21]. We do not involve ASTs

as input since they are deeper in nature and contain more redundant information [3] than DFGs. DFGs, denoted as
𝑉 = {𝑣1, 𝑣2, ...}, can model the data dependencies between variables in the code, including message sending/receiving.
Figure 1 (c) shows an example of the extracted data flow graph. Variables with same name (e.g., i2 and i5) are associated
with different semantics in the DFG. Each variable is a node in the DFG and the direct edge ⟨𝑣𝑖, 𝑣 𝑗 ⟩ from 𝑣𝑖 to 𝑣 𝑗 indicates
the value of the 𝑗-th variable comes from the 𝑖-th variable. We can find that the semantic relations among “i2”, “i3”, “i4”

7

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

Fig. 4. A diagram of hierarchical-variant attention. Different red boxes illustrate different scales.

and “i5” which represent the data sending/receiving in a loop. Based on the DFGs, we build the adjacency matrix D,
where 𝑑𝑖 𝑗 = 1 if there exists a message passing from the 𝑗-th token to the 𝑖-th token; Otherwise, 𝑑𝑖 𝑗 = 0. Note that if
two variables have a data dependency, then their constituent sub-tokens also possess the dependency relation. Figure 3

(d) illustrates the data flow-guided single-head self-attention. To address the sparseness of the matrix D and to highlight
the relations of data dependencies, we propose the data flow-guided self-attention ℎ𝑒𝑎𝑑𝑓 below:

where 𝜇 is the control factor for adjusting the integration degree of the data flow structure.

ℎ𝑒𝑎𝑑𝑓 = softmax

(cid:18) QK⊺ + 𝜇 ∗ QK⊺D
√
𝑑

(cid:19)

V,

(11)

3.2 Hierarchical Structure-Variant Attention

Inspired by the principle of compositionality in logic semantics: the high-level semantics is the composition of low-level

terms [23, 54], we propose a hierarchical structure-variant attention such that our model would focus on local structures

at the lower layers and global structure at the higher layers. The diagram of the hierarchical structure-variant attention
is illustrated in Figure 4. Specifically, the token-guided head attention ℎ𝑒𝑎𝑑𝑡 and the statement-guided head attention
ℎ𝑒𝑎𝑑𝑠 are used more in the heads of lower layers; while the data flow-guided head attention ℎ𝑒𝑎𝑑𝑓 is more spread in
the heads of higher layers.

Let 𝐿 denote the number of layers in the proposed SG-Trans, ℎ indicate the number of heads in each layer and 𝑘 be a
hyper-parameter to control the distribution of four types of head attentions, including ℎ𝑒𝑎𝑑𝑡 , ℎ𝑒𝑎𝑑𝑠 , ℎ𝑒𝑎𝑑𝑓 , and ℎ𝑒𝑎𝑑𝑜 ,
where ℎ𝑒𝑎𝑑𝑜 indicates the standard head attention without constraints, the distribution for each type of head attention
at the 𝑙-th layer is denoted as Ω𝑙 = [𝜔𝑙
𝑡 , 𝜔𝑙
𝑜 represent the numbers of ℎ𝑒𝑎𝑑𝑡 , ℎ𝑒𝑎𝑑𝑠 ,
ℎ𝑒𝑎𝑑𝑓 , and ℎ𝑒𝑎𝑑𝑜 , respectively at the 𝑙-th layer. We define the distribution below:

𝑜 ], where 𝜔𝑙

𝑓 , and 𝜔𝑙

𝑠, 𝜔𝑙
𝑓

𝑡 , 𝜔𝑙

𝑠 , 𝜔𝑙

, 𝜔𝑙

𝑡 = 𝜔𝑙
𝜔𝑙

𝑠 = ⌊ℎ ∗

𝜔𝑙

𝑓 = ⌊ℎ ∗
𝑜 = ℎ − (𝜔𝑙
𝜔𝑙

⌋,

𝑘 − 𝑙
2 ∗ 𝑘 − 𝑙
𝑙
⌋,
2 ∗ 𝑘 − 𝑙
𝑡 + 𝜔𝑙

𝑠 + 𝜔𝑙

𝑓 ),

8

(12)

(13)

(14)

4444ConcatLinearℎ𝑒𝑎𝑑𝑡𝑙ℎ𝑒𝑎𝑑𝑠𝑙ℎ𝑒𝑎𝑑𝑓𝑙ℎ𝑒𝑎𝑑𝑜𝑙𝜔𝑡𝑙𝜔𝑠𝑙𝜔𝑓𝑙𝜔𝑜𝑙ℎCode Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Table 1. Statistics of the benchmark datasets.

Training Set
Validation Set
Test Set
Total

Java
69,708
8,714
8,714
87,136

Python
55,538
18,505
18,502
92,545

where 𝑘 is a positive integer hyperparameter, and ⌊·⌋ denotes rounding the value down to the next lowest integer. The
design is to enable more heads attending to the global structure with the growth of 𝑙, i.e., 𝜔𝑙
𝑓 will get larger at a higher
layer 𝑙; meanwhile few heads can catch the local structure, i.e., 𝜔𝑙
𝑠 will become smaller. ℎ𝑒𝑎𝑑𝑜 is involved to
enable the model to be adapted to arbitrary numbers of layers and heads. Especially, with the increase of layer 𝑙, 𝜔𝑙
𝑡 and
𝑠 might drop to zero. In the case of 𝜔𝑙
𝜔𝑙
𝑡 ≤ 0, no constraints will be introduced to the corresponding attention layer since
the standard self-attention already captures long-range dependency information, which fits our purpose of attending to
global structure at higher layers. Otherwise, the head attentions will follow the defined distribution Ω𝑙 .

𝑡 and 𝜔𝑙

The hierarchical structure-variant attention (HSVA) at the 𝑙-th layer is computed as:

HSVA

𝑙 = [ℎ𝑒𝑎𝑑𝑙

1 ◦ · · · ◦ ℎ𝑒𝑎𝑑𝑙

ℎ]W

𝑂,

(15)

where ◦ denotes the concatenation of ℎ different heads, and W𝑂 ∈ R𝑑ℎ×𝑑ℎ is a parameter matrix.

3.3 Copy Attention

The OOV issue is important for effective code summarization [34]. We adopt the copy mechanism introduced in

Section 2.2 in SG-Trans to calculate whether to generate words from the vocabulary or to copy from the input source

code. Following Ahmad et al. [1], an additional attention layer is added to learn the copy distribution on top of the

decoder [46]. The mechanism enables the proposed SG-Trans to copy low-frequency words, e.g., API names, from

source code, thus mitigating the OOV issue.

4 EXPERIEMENTAL SETUP

In this section, we introduce the evaluation datasets and metrics, comparison baselines, and parameter settings.

4.1 Benchmark Datasets

We conduct experiments on two benchmark datasets that respectively contain Java and Python source code following
the previous work [1, 67]. Specifically, the Java dataset publicly released by Hu et al. [27] comprises 87, 136 ⟨Java method,
comment⟩ pairs collected from 9, 714 GitHub repositories, and the Python dataset consists of 92, 545 functions and
corresponding documentation as originally collected by Barone et al. [8] and later processed by Wei et al. [61].

For fair comparison, we directly use the benchmarks open sourced by the previous related studies [1, 28, 60], in

which the datasets are split into training set, validation set, and test set in a proportion of 8 : 1 : 1 and 6 : 2 : 2 for Java

and Python, respectively. We follow the commonly-used dataset split strategy with no modification to avoid any bias

introduced by dataset split.

We apply CamelCase and snake_case tokenizers [1] to get sub-tokens for both dataset. As for the code statements, we

apply a simple rule to extract the statements of code snippet. For the extraction of statements from Java dataset, we split

each code snippet into statements with separators including ’{’, ’}’ and ’;’. The token sequence between two adjacent

9

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

separators is considered as a statement. For the example shown in Figure 1 (a), each line except the separators is one

statement. While for the Python dataset, we define a statement by the row, which means that the tokens in the same

row are considered as belonging to the same statement. For the extraction of data flow from the Java dataset, we use the

tool in GSC [12] to first generate augmented ASTs and then from which extract DFGs. Regarding the Python dataset,

we follow the setup in Allamanis et al.’s work [3] and extract four kinds of edge (LastRead, LastWrite, LastLexicalUse,

ComputeFrom) from code.

4.2 Evaluation Metrics

To verify the superiority of SG-Trans over the baselines, we use the most commonly-used automatic evaluation metrics,

BLEU-4 [47], METEOR [7] and ROUGE-L [37].

BLEU is a metric widely used in natural language processing and software engineering fields to evaluate generative

tasks (e.g., dialogue generation, code commit message generation, and pull request description generation) [33, 40, 45, 66].
BLEU uses 𝑛-gram for matching and calculates the ratio of 𝑁 groups of word similarity between generated comments
and reference comments. The score is computed as:

𝐵𝐿𝐸𝑈 − 𝑁 = 𝐵𝑃 × exp(

𝑁
∑︁

𝑛=1

𝜏𝑛 log 𝑃𝑛),

(16)

where 𝑃𝑛 is the ratio of the subsequences with length 𝑛 in the candidate that are also in the reference. 𝐵𝑃 is the brevity
penalty for short generated sequence and 𝜏𝑛 is the uniform weight 1/𝑁 . We use corpus-level BLEU-4, i.e., 𝑁 = 4, as
our evaluation metric since it is demonstrated to be more correlated with human judgements than other evaluation

metrics [38].

METEOR is a recall-oriented metric which measures how well our model captures content from the reference text

in our generated text. It evaluates generated text by aligning them to reference text and calculating sentence-level

similarity scores.

𝑀𝐸𝑇 𝐸𝑂𝑅 = (1 − 𝛾 · frag

𝛽 ) ·

𝑃 · 𝑅
𝛼 · 𝑃 + (1 − 𝛼) · 𝑅

,

(17)

where P and R are the unigram precision and recall, frag is the fragmentation fraction. 𝛼, 𝛽 and 𝛾 are three penalty
parameters whose default values are 0.9, 3.0 and 0.5, respectively.

ROUGE-L is widely used in text summarization tasks in the natural language processing field to evaluate what

extent the reference text is recovered or captured by the generated text. ROUGE-L is based on the Longest Common
Subsequence (LCS) between two text and the F-measure is used as its value. Given a generated text 𝑋 and the reference
text 𝑌 whose lengths are 𝑚 and 𝑛 respectively, ROUGE-L is computed as:

𝑃𝑙𝑐𝑠 =

𝐿𝐶𝑆 (𝑋, 𝑌 )
𝑛

, 𝑅𝑙𝑐𝑠 =

𝐿𝐶𝑆 (𝑋, 𝑌 )
𝑚

, 𝐹𝑙𝑐𝑠 =

(1 + 𝛽2)𝑃𝑙𝑐𝑠𝑅𝑙𝑐𝑠
𝑅𝑙𝑐𝑠 + 𝛽2𝑃𝑙𝑐𝑠

,

(18)

where 𝛽 = 𝑃𝑙𝑐𝑠 /𝑅𝑙𝑐𝑠 and 𝐹𝑙𝑐𝑠 is the computed ROUGE-L value.

4.3 Baselines

We compare SG-Trans with following baseline approaches.

CODE-NN [31], as the first deep-learning-based work in code summarization, generates source code summaries

with an LSTM network. To utilize code structure information, Tree2seq [14] encodes source code with a tree-LSTM

architecture. Code2seq [4] represents the code snippets by sampling paths from the AST. RL+Hybrid2Seq [57]

10

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

incorporates ASTs and code sequences into a deep reinforcement learning framework, while DeepCom [27] encodes

the node sequences traversed from ASTs to capture the structural information. API+Code [28] involves API knowledge

in the code summarization procedure. Dual model [61] adopts a dual learning framework to exploit the duality of code

summarization and code generation tasks. One of the most recent approaches, denoted as NeuralCodeSum [1], which

integrates the vanilla Transformer [55] with relative position encoding (RPE) and copy attention. Another recent

approach Transformer+GNN [11] applies graph convolution to obtain structurally-encoded node representations and

passes sequences of the graph-convolutioned AST nodes into Transformer.

We also compare our approach with relational Transformers [26, 70] which involve structural information for code

representation learning. GREAT [26] biases vanilla Transformers with relational information from graph edge types.

CodeTransformer [70] focuses on multilingual code summarization and proposes to build upon language-agnostic

features such as source code and AST-based features.

During implementation, we either directly copy the results claimed in the original papers or reproduce the results

strictly following the released repositories for most baselines except for GREAT and CodeTransformer. For GREAT [26],

12 types of information including control flow graph and syntactic features are adopted for model training, as no

replication package is available. Due to the difficulty of complete replication, we follow the strategy in Zügner et al.’s

work [70] by employing the same structural information as SG-Trans during replication. For CodeTransformer, although

a replication package is provided by the authors, not all the benchmark data can be successfully preprocessed. For Java,

only 61,250 of 69,708 code snippets in the training set, 7,621 of 8,714 in the validation set, and 7,643 of 8,714 in the

test set pass the preprocessing step; while for Python, all the code snippets can be well preprocessed. To ensure the

consistency of evaluation data, we compare SG-Trans with CodeTransformer on the Java dataset separately. We use the

same model settings for implementing CodeTransformer, including the layer number, head number, etc.

4.4 Parameter Settings

SG-Trans is composed of 8 layers and 8 heads in its Transformer architecture and the hidden size of the model is
512. We use Adam optimizer with the initial learning rate set to 10−4, batch size set to 32, and dropout rate set to 0.2
during the training. We train our model for at most 200 epochs and select the checkpoint with the best performance

on the validation set for further evaluation on the test set. We report the performance of SG-Trans and each ablation

experiment by running three times and taking the average. To avoid over-fitting, we early stop the training if the

performance on the validations set does not increase for 20 epochs. For the control factors of heads distribution and data

flow, we set them to 1 and 5, respectively. We will discuss optimal parameters selection in Section 5.3. Our experiments

are conducted on a single Tesla V100 GPU for about 30 hours, and we train our model from scratch.

5 EXPERIMENTAL RESULTS

In this section, we elaborate on the comparison results with the baselines to evaluate SG-Trans’s capability in accurately

generating code summaries. Our experiments are aimed at answering the following research questions:

RQ1: What is the performance of SG-Trans in code summary generation?

RQ2: What is the impact of the involved code structural properties and the design of hierarchical attentions on the

model performance?

RQ3: How accurate is SG-Trans under different parameter settings?

11

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

Table 2. Comparison results with baseline models. The bold figures indicate the best results. “*” denotes statistical significance in
comparison to the baseline models we reproduced (i.e., two-sided 𝑡 -test with 𝑝-value< 0.01).

Approach

CODE-NN [31]
Tree2Seq [14]
RL+Hybrid2Seq [57]
DeepCom [27]
API+Code [28]
Dual Model [60]
Code2seq[4]
Vanilla Transformer [55]
NeuralCodeSum [1]
GREAT[26]
CodeTransformer [70]
Transformer+GNN[11]
SG-Trans

Java

Python

BLEU-4 METEOR ROUGE-L BLEU-4 METEOR ROUGE-L

27.60
37.88
38.22
39.75
41.31
42.39
12.19
44.20
45.15
44.97
–
45.49
45.89*

12.61
22.55
22.75
23.06
23.73
25.77
08.83
26.83
27.46
27.15
–
27.17
27.85*

41.10
51.50
51.91
52.67
52.25
53.61
25.61
53.45
54.84
54.42
–
54.82
55.79*

17.36
20.07
19.28
20.78
15.36
21.80
18.69
31.34
32.19
32.11
27.63
32.82
33.04*

09.29
08.96
09.75
09.98
08.57
11.14
13.81
18.92
19.96
19.75
14.29
20.12
20.52*

37.81
35.64
39.34
37.35
33.65
39.45
34.51
44.39
46.32
46.01
39.27
46.81
47.01*

Table 3. Comparison results with CodeTransformer on the dataset preprocessed by CodeTransformer. The bold figures indicate the
best results. “*” denotes statistical significance in comparison to the baseline models (i.e., two-sided 𝑡 -test with 𝑝-value< 0.01).

Approach

CodeTransformer [70]
SG-Trans

Java
BLEU-4 METEOR ROUGE-L
24.22
27.32*

51.96
54.41*

39.81
44.59*

5.1 Answer to RQ1: Comparison with the Baselines

The experimental results on the benchmark datasets are shown in Table 2. For the vanilla Transformer and the

NeuralCodeSum [1], we reproduce their experiments under the same hyper-parameter settings as the Transformer in

SG-Trans to ensure fair comparison. We compare SG-Trans with CodeTransformer [70] on the Java dataset separately,

in which both approaches are trained and evaluated on the same dataset, with results shown in Table 3. Based on

Table 2 and Table 3, we summarize the following findings:

Code structural properties are beneficial for source code summarization. Comparing Tree2Seq/DeepCom

with CODE-NN, we can find that the structure information brings a great improvement in the performance. For example,

both Tree2Seq and DeepCom outperform CODE-NN by at least 37.2%, 78.8%, and 25.3% respectively regarding the three

metrics on the Java dataset. Although no consistent improvement across all metrics is observed on the Python dataset,

Tree2Seq/DeepCom still shows an obvious increase on the BLEU-4 metric.

Transformer-based approaches perform better than RNN-based approaches. The four Transformer-based

approaches [1, 4, 11, 26, 55] outperform all the other baselines, with NeuralCodeSum [1] giving better performance

compared to the vanilla Transformer. The vanilla Transformer already achieves better performance than the top

seven RNN-based approaches with various types of structural information being incorporated, showing the efficacy

of Transformer for the task. On the Python dataset, NeuralCodeSum outperforms the best RNN-based baseline, Dual

Model [61], by 47.7% and 17.4% in terms of the BLEU-4 and ROUGE-L metrics, respectively.

12

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Table 4. Ablation study on different part of our model. The bold figures indicate the best results. “*” denotes statistical significance
in comparison to the baseline models we reproduced (i.e., two-sided 𝑡 -test with 𝑝-value< 0.01).

Approach

Java

Python

BLEU-4 METEOR ROUGE-L BLEU-4 METEOR ROUGE-L

SG-Trans w/o token info.
SG-Trans w/o statement info.
SG-Trans w/o data flow info.
SG-Trans w/o hierarchical attention
SG-Trans w/o copy attention
SG-Transsoft
SG-Trans

44.92
44.61
45.52
45.53
45.24
45.37
45.89*

27.35
27.08
27.65
27.72
27.49
27.65
27.85*

54.69
54.04
55.40
55.48
55.01
55.09
55.79*

32.18
32.26
32.58
32.93
31.89
32.77
33.04*

19.87
19.66
20.16
20.38
19.26
19.96
20.52*

46.14
46.08
46.57
46.73
45.31
46.74
47.01*

The proposed SG-Trans is effective in code summarization. Comparing SG-Trans with the NeuralCodeSum

and Transformer+GNN, SG-Trans achieves the best results on both benchmark datasets, yet without introducing any

extra model parameters. Specifically, SG-Trans improves the best baseline by 1.7% and 0.4% in terms of ROUGE-L score

on the Java and Python dataset, respectively.

The combination of the structural information in SG-Trans is effective. By comparing SG-Trans with other

Transformer models with structural information involved such as GREAT [26], CodeTransformer [70] and Trans-

former+GNN [11], SG-Trans achieves the best results on both benchmark datasets. Specifically, SG-Trans improves the

best baseline by 2.5% and 2.0% in terms of METEOR score on the Java and Python dataset, respectively.

5.2 Answer to RQ2: Ablation Study

We further perform ablation studies to validate the impact of the involved code structural properties and the hierarchical

structure-variant attention approach. Besides, to evaluate the efficacy of the hard mask attention mechanism for

combining token-level and statement-level information in SG-Trans, we create a comparative approach, named as
SG-Transsoft, by changing the hard mask into soft mask. Specifically, for SG-Transsoft, we follow NeuralCodeSum [1],
and only add the relative position embedding for subtoken pairs 𝑥𝑖 and 𝑥 𝑗 if they are in the same token or statement.
The results are shown in Table 4.

Analysis of the involved code structure. We find that all the three structure types, including code token, statement

and data flow, contribute to the model performance improvement but with varied degrees. Specifically, local syntactic

structures play a more important role than the global data flow structure. For example, removing the statement

information leads to a significant performance drop at around 2.8% and 2.4% regarding the BLEU-4 score. This suggests

the importance of modeling the semantic relations among tokens of the same statement for code summarization. With

the data flow information eliminated, SG-Trans also suffers from a performance drop, which may indicate that the data

dependency relations are hard to be learnt by Transformer implicitly.

Analysis of the hierarchical structure-variant attention mechanism. We replace the hierarchical structure-
variant attention with uniformly-distributed attention, i.e., Ω𝑙 = [𝜔𝑙
𝑜 ] = [2, 2, 2, 2], for the ablation analysis.
As can be found in Table 2, without the hierarchical structure design, the model’s performance decreases on all metrics

𝑠, 𝜔𝑙
𝑓

𝑡 , 𝜔𝑙

, 𝜔𝑙

for both datasets. The results demonstrate the positive impact of the hierarchical structure-variant attention mechanism.

Analysis of the copy attention. As shown in Table 2, excluding the copy attention results in a significant drop

to SG-Trans’s performance, similar to what has been observed in Ahmad et al.’s work [1]. This shows that the copy

attention is useful for alleviating the OOV issue and facilitating better code summarization.

13

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

Analysis of the hard mask attention mechanism. As shown in Table 4, we can find that SG-Trans performs
constantly better than SG-Transsoft on both Java and Python datasets with respect to all the metrics. For example, on
Java dataset replacing hard-mask with soft mask leads to a performance drop at 1.1% and 1.3% in terms of the BLEU-4

and ROUGE-L metrics, respectively, which indicates that the hard mask attention is effective at capturing the local

information.

(a) Analysis of the parameter 𝜇.

(b) Analysis of the parameter 𝑘. 𝐿 = 8.

Fig. 5. Influence of the hyper-parameters 𝜇 and 𝑘 on the model performance.

5.3 Answer to RQ3: Parameter Sensitivity Analysis

In this section we analyze the impact of two key hyper-parameters on the model performance, i.e., the control factor 𝜇
for adjusting the integration degree of the data flow structure and the parameter 𝑘 to control the head distribution.

The parameter 𝜇. Figure 5 (a) shows the performance variation with the changes of 𝜇 while keeping other hyper-
parameters fixed. For the Java dataset, the model achieves the best scores when 𝜇 = 5. Lower or higher parameter
values do not give better results. While for the Python dataset, a similar trend is observed for the BLEU-4 and ROUGE-L
metrics where the model performs the best when 𝜇 equals to 3 and 5, respectively. In this work, we set 𝜇 to 5 since the
model can produce relatively better results on both datasets.

The parameter 𝑘. We observe the performance changes when the control factor 𝑘 of the head distribution takes
values centered on layers of SG-Trans 𝐿. Figure 5 (b) illustrates the results. We can find that SG-Trans can well balance
the distribution of local and global structure-guided head attention when 𝑘 = 𝐿 or 𝑘 = 𝐿 + 1. As 𝑘 gets larger, SG-Trans
would be more biased by the local structure and tend to generate inaccurate code summary. In our work here, we set
𝑘 = 𝐿.

5.4 Human Evaluation

In this section, we perform human evaluation to qualitatively evaluate the summaries generated by four Transformer-

based baselines, including vanilla Transformer, NeuralCodeSum, GREAT and CodeTransformer, and also our model

SG-Trans. We do not involve the baseline Transformer+GNN, due to the lack of replication package. The human

evaluation is conducted through online questionnaire. In total, 10 software developers are invited for evaluation. All

participants have programming experience in software development for at least four years, and none of them is a

co-author of the paper. Each participant is invited to read 60 code snippets and judge the quality of summaries generated

by vanilla Transformer, NeuralCodeSum, CodeTransformer, GREAT and SG-Trans. Each of them will be paid 30 USD

upon completing the questionnaire.

14

4545.345.645.91357927.427.627.81357954.855.155.455.71357932.83333.21357920.320.420.520.61357946.546.746.947.113579BLEU-4METEORROUGE-L45.345.545.745.9L-2L-1LL+1L+2L+327.527.627.727.827.9L-2L-1LL+1L+2L+35555.355.655.9L-2L-1LL+1L+2L+332.432.632.833L-2L-1LL+1L+2L+320.120.320.520.7L-2L-1LL+1L+2L+346.246.546.847.1L-2L-1LL+1L+2L+3BLEU-4METEORROUGE-LCode Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Fig. 6. An example of questions in our questionnaire. The two-dot symbols indicate the simplified rating schemes for Summary 2,3,4.

Table 5. Human evaluation results. The bold figures indicate the best results.

Dataset

Java

Python

Metrics
Adequacy
Conciseness
Fluency
Adequacy
Conciseness
Fluency

Transformer CodeTransformer NeuralCodeSum GREAT SG-Trans

3.35
4.20
4.32
2.61
3.84
4.06

2.67
3.49
3.25
1.92
2.62
2.39

3.28
4.32
4.36
3.04
4.01
4.21

3.44
4.36
4.50
2.83
4.05
4.26

3.65
4.50
4.59
3.21
4.21
4.33

5.4.1

Survey Design. We randomly selected 200 code snippets, with 100 in Java and 100 in Python, for evaluation. As

shown in Figure 6, in the questionnaire, each question comprises a code snippet and summaries generated by the five

models. Each participant will be given 60 questions and each question will be evaluated by three different participants.

For each question, the summaries generated by the models are randomly shuffled to eliminate the order bias.

The quality of the generated summaries is evaluated in three aspects, including Adequacy, Conciseness, and Fluency,

with the 1-5 Likert scale (5 for excellent, 4 for good, 3 for acceptable, 2 for marginal, and 1 for poor). We explained

the meaning of the three evaluation metrics at the beginning of the questionnaire: The metric “adequacy” measures

how much the functional meaning of the code is preserved after summarization; the metric “conciseness” measures the

ability to express the function of code snippet without unnecessary words; while the metric “fluency” measures the

quality of the generated language such as the correctness of grammar.

5.4.2

Survey Design. We finally received 600 sets of scores with 3 sets of scores for each code-summary pair from

the human evaluation. On average, the participants spent 2 hours on completing the questionnaire, with the median

15

Code: public static int unixTimestamp( ){     return (int)(System.currentTimeMillis()/NUM) ;}Summary 1: return current timestamp ( local time )Summary 2: current the current of the from for epoch from from fromSummary 3: get the unix time in secondsSummary 4: return number as a timestamp in the local epochSummary 5: get the timestamp in milliseconds since epochSummary 1 s AdequarySummary 1 s ConcisenessSummary 1 s FluencySummary 5 s AdequarySummary 5 s ConcisenessSummary 5 s Fluency’Very DissatisfiedVery Satisfied1122334455’’1122334455’............’’1234512345Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

(a) Adequacy metric for the Java dataset.

(b) Adequacy metric for the Python dataset.

(c) Conciseness metric for the Java dataset.

(d) Conciseness metric for the Python dataset.

(e) Fluency metric for the Java dataset.

(f) Fluency metric for the Python dataset.

Fig. 7. Distribution of the rating scores in human evaluation on the two datsets. The “Transformer” on the horizontal axis denotes the
“vanilla Transformer” approach.

completion time at 1.67 hours. The inter-annotator agreement of the two sets is evaluated with the widely-used metric

Cohen’s kappa. The average Cohen’s kappa scores for the Java and Python datasets are 0.66 and 0.58, respectively,

indicating that the participants achieved at least moderate agreement on both datasets.

The evaluation results are illustrated in Table 5 and Figure 7. We find that the summaries generated by SG-Trans

receive the highest scores on both datasets and with respect to all the metrics. For the Java dataset, as shown in Table 5,

SG-Trans improves the baseline models by at least 6.1%, 3.2% and 2.0% with respect to the adequacy, conciseness, and

fluency metrics, respectively. As can be observed from Figure 7 (a), (c) and (e), summaries generated by SG-Trans receive

the most 5-star ratings and fewest 1/2-star ratings from the participants, comparing with the summaries produced by

other models for each metric. Specifically, regarding the fluency metric, only 1.3% of the participants gave 1/2-star

ratings to the summaries generated by SG-Trans, while other approaches receive at least 6.0% 1/2-star ratings and

CodeTransformer receives even 24.0%. The score distributions indicate that SG-Trans better captures the functionality

of given code snippets and generates nature language comments with higher quality.

16

0%10%20%30%40%50%60%70%80%90%100%CodeTransformerTransformerNeuralCodeSumGREATSGTrans12345Percentage0%10%20%30%40%50%60%70%80%90%100%CodeTransformerTransformerNeuralCodeSumGREATSGTrans12345Percentage0%10%20%30%40%50%60%70%80%90%100%CodeTransformerTransformerNeuralCodeSumGREATSGTrans12345Percentage0%10%20%30%40%50%60%70%80%90%100%CodeTransformerTransformerNeuralCodeSumGREATSGTrans12345Percentage0%10%20%30%40%50%60%70%80%90%100%CodeTransformerTransformerNeuralCodeSumGREATSGTrans12345Percentage0%10%20%30%40%50%60%70%80%90%100%CodeTransformerTransformerNeuralCodeSumGREATSGTrans12345PercentageCode Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Table 6. The quality of summaries generated by SG-Trans and humans. The term “auto-generated” indicates the summaries output
by SG-Trans.

Predicted Summary Test

Java

Python

BLEU-4 METEOR ROUGE-L BLEU-4 METEOR ROUGE-L

Auto-generated
Human-generated

Reference
Reference

77.06
19.04

51.67
13.33

89.95
32.83

75.09
23.63

49.04
19.82

87.56
38.95

Table 7. The code snippets that cannot be understood by the annotators.

Example (1) in Python:
def poly_TC(f, K):
if (not f):

return K.zero

else:

return f[(-1)]

Human-generated: cannot understand
SG-Trans: return trailing coefficient of f
Ground truth: return trailing coefficient of f

Example (2) in Java:
@SuppressWarnings(STRING)
public PropagationImp(Stack<CompositeTransaction>lineage, boolean serial, long timeout){

serial_ = serial;
lineage_ = (Stack<CompositeTransaction>)lineage.clone();
timeout_ = timeout;}

Human-generated: cannot understand
SG-Trans: create a new instance
Ground truth: create a new instance

For the Python dataset, as shown in Table 5, NeuralCodeSum and GREAT significantly outperform the vanilla

Transformer and CodeTransformer; while SG-Trans is more powerful, further boosting the best baseline approach

by 5.6%, 4.0% and 1.6% in terms of the adequacy, conciseness, and fluency, respectively. As can be observed from

Figure 7 (b), (d) and (f), summaries generated by SG-Trans receive the most 5-star ratings and fewest 1/2-star ratings

from the annotators. Specifically, regarding the adequacy metric, 18.0% of the participants gave 5-star ratings to the

summaries generated by SG-Trans, with only 4.6% for the CodeTransformer approach, 12.3% for the strongest baseline

NeuralCodeSum. For the conciseness metric, 51.7% of the participants gave 5-star ratings to the summaries generated

by SG-Trans and the two best baseline approaches NeuralCodeSum and GREAT only receive 42.3% and 44.0% 5-star

ratings, respectively. The score distributions indicate that the summaries generated by SG-Trans can better describe the

function of code snippets and have a more concise and accurate expression.

5.5 Further Evaluation on Generated Summaries

To further investigate the quality of auto-generated summaries, we invite participants to summarize the code without

access to the reference summary, and then ask the annotators to score the summaries. During manual code summariza-

tion, we invite four postgraduate students with more than five years of development experience as well as internship

experience in technology companies to participant. To ease the pressure of annotation, we randomly select 80 code

snippets with code lengths fewer than 250 characters. Each participant is asked to write summaries for 20 code snippets,

i.e., 10 in Java and 10 in Python. Each of them will be paid 15 USD upon completing the questionnaire.

17

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

Table 8. Examples illustrating the difference between summaries generated by SG-Trans and written by human.

Example (1) in Python:
def Thing2Literal(o, d):

return string_literal(o, d)

Human-generated: return the literalness of a string
SG-Trans: convert something into a string representation
Ground truth: convert something into a sql string literal

Example (2) in Python:
def print_bucket_acl_for_user(bucket_name, user_email):

storage_client = storage.Client()
bucket = storage_client.bucket(bucket_name)
bucket.acl.reload()
roles = bucket.acl.user(user_email).get_roles()
print roles

Human-generated: print the bucket acl for user
SG-Trans: prints out a buckets access control list for a given user
Ground truth: prints out a buckets access control list for a given user

Example (3) in Java:
public ActivityResolveInfo(ResolveInfo resolveInfo){

this.resolveInfo = resolveInfo;}

Human-generated: assign a value to resolveinfo attribute
SG-Trans: creates a new activity
Ground truth: creates a new instance

Example (4) in Java:
public static Date parseText(String dateStr){

try {return mSimpleTextFormat.parse(dateStr);}
catch(ParseException e){
e.printStackTrace();
throw new RuntimeException(STRING);}}
Human-generated: parse the dateStr as a date instance
SG-Trans: parse string to datetime
Ground truth: parse string to datetime

Table 9. Human evaluation on summaries generated by SG-Trans and human-written summaries.

Dataset

Java

Python

Metrics
Adequacy
Conciseness
Fluency
Adequacy
Conciseness
Fluency

Human-written
4.38
4.82
4.94
4.39
4.86
4.95

SG-Trans
3.62
4.48
4.58
3.24
4.23
4.34

We totally receive annotations of 78/80 code snippets. For the remaining two code snippets, as shown in Table 7, the

functionalities are hard to be understood by the annotators without corresponding prior knowledge. For the example in

Table 7 (1), the unclear meanings of “K.zero” and “f[(-1)]” hinder the program comprehension. We measure the quality

of summaries generated by SG-Trans and humans using the same metrics introduced in Section 4.2, respectively. The

results are shown in Table 6. From the table, we find that compared with human-generated summaries, auto-generated

18

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

summaries are much more similar to the reference summaries. For example, the BLEU-4 scores of the auto-generated

summaries are 77.06 and 75.09 on Java and Python, respectively, while the human-generated summaries are only 19.04

and 23.63 on Java and Python, respectively. To analyze the reason of the large difference between human-generated

summaries and reference summaries, we manually check all the annotated data, and summarize two main reasons:

• Lack of contextual knowledge. Some code snippets use external APIs or inner elements of a class, and the
details of the APIs and elements cannot be accessed. So the annotators can only infer the functions of code

snippets based on the function/variable names, resulting in poorly-written summaries. For example, as shown in

Table 8 (1), since the detail of the external API “string_literal” is unknown, humans can only guess its meaning

from the name. In Table 8 (2), “acl” is an inner element of the class “bucket”, but the definition is lacking, which

makes it hard for humans to comprehend the function. Our model has been provided with the knowledge that

“acl” stands for “access control list” during training, so it can output a more accurate summary.

• Limitation of the evaluation metrics. As shown in Table 8 (3) and (4), although the summaries generated
by humans can accurately reflect the functions of the code snippets, they are significantly different from the

reference summaries, leading to low metric scores. For the example in Table 8 (4), both the human-generated

summary and reference summary explain the meaning of the code snippet well. However, under the existing

metrics based on n-gram matching, the metric scores between them are very low since they have only one

overlapping word “parse”.

We then qualitatively inspect the quality of human-generated summaries and auto-generated summaries. We invite

another three annotators, who have not joined the manual code summarization part, for the inspection. The results

are illustrated in Table 9. The Cohen’s kappa scores of the annotation results are 0.69 and 0.71 on Java and Python,

respectively, indicating a substantial inter-rater reliability on both datasets. As shown in Table 9, the quality of human-

generated summaries is better than that of the auto-generated summaries with respect to all the metrics. Specifically,

the conciseness and fluency scores of Human-written summary are nearly 5 on both datasets. Moreover, the adequacy

scores of human-written summaries outperform the auto-generated summaries by 21.0% and 35.5% on Java and Python

datasets, respectively. The results further explain the huge difference between human-generated summaries and

reference summaries under automatic evaluation, as shown in Table 6, reflecting the limitation of automatic metrics.

6 DISCUSSION

In this section, we mainly discuss the key properties of the proposed SG-Trans, the impact of duplicate data in the

benchmark dataset on the model performance, and the limitations of our study.

6.1 Why Does Our Model Work?

We further conduct an analysis to gain insights into the proposed SG-Trans in generating high-quality code summaries.

Through qualitative analysis, we have identified two properties of SG-Trans that may explain its effectiveness in the

task.

Observation 1: SG-Trans can better capture the semantic relations among tokens. From Example (1) shown

in Table 10, we can observe that SG-Trans produces the summaries most similar to the ground truth among all the

approaches, while the CodeTransformer gives the worst result. We then visualize the heatmap of the self-attention

scores of the three types of heads in Figure 8 for a further analysis. As can be seen in Figure 8 (a) and (b), SG-Trans

19

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

Table 10. Examples illustrating summaries generated by different approaches given the code snippets. The following examples are
only from the test set and do not exist in the training set.

Example (1) in Java:
public static boolean isFile(String path){

File f = new File(path);
return f.isFile();

}
Vanilla Transformer: checks if the given path is a file object , is a directory it can be read . no distinction is
considered exceptions
CodeTransformer: checks if the given file is a file
NeuralCodeSum: checks if is file exist
GREAT: checks if the given path is a file
SG-Trans: checks if the given path is a file
Ground truth: checks if the given path is a file

Example (2) in Python:
def print_bucket_acl_for_user(bucket_name, user_email):

storage_client = storage.Client()
bucket = storage_client.bucket(bucket_name)
bucket.acl.reload()
roles = bucket.acl.user(user_email).get_roles()
print roles

Vanilla Transformer: removes a user from the access control list
CodeTransformer: sets a the user from to to .
NeuralCodeSum: prints out a user access control list
GREAT: prints out a user access control list
SG-Trans: prints out a buckets access control list for a given user
Ground truth: prints out a buckets access control list for a given user

Example (3) in Python:
def token_urlsafe(nbytes=None):

tok = token_bytes(nbytes)
return base64.urlsafe_b64encode(tok).rstrip(‘=’).decode(‘ascii’)

Vanilla Transformer: generate a token
CodeTransformer: decodes a unicode string string string if
NeuralCodeSum: construct a random text string .
GREAT: generates a token identifier
SG-Trans: return a random url-safe string .
Ground truth: return a random url-safe text string .

can focus on local relations among code tokens through its token-guided self-attention and statement-guided self-

attention. For example, SG-Trans can learn that the two tokens “is” and “File” possess a strong relation, according to

Figure 8 (a). As depicted in Figure 8 (b), we can find that SG-Trans captures that the token “path” is strongly related to

the corresponding statement, which may be the reason the token “path” appears in the summary. Figure 8 (c) shows

that the data flow-guided head attention focuses more on the global information, and can capture the strong relation

between the tokens “path” and “f ”. Based on the analysis of the example (1), we speculate that the model can well

capture the token relations locally and globally for code summary generation. As for the heatmap of other baseline

models like NeuralCodeSum, we can see that they are very different with the heatmap of SG-Trans. As shown in

Figure 9, NeuralCodeSum does not capture the token, statement, and data flow information in any layer while SG-Trans

can pay more attention to the token pairs with syntactic or semantic relations. A similar conclusion can be drawn from

Example (2) in Table 10. All the approaches successfully comprehend that the token “acl” indicates “access control list”.

20

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Fig. 8. Heatmap visualization of self-attention scores of the three types of heads in the encoder for the first case in Table 10. The
rectangles with red edge, green edge, and blue edge indicates the tokens belonging to the same original token, the same statement, or
containing data flow relation, respectively.

Fig. 9. Heatmap visualization of self-attention scores of NeuralCodeSum. The rectangles with red edge, green edge, and blue edge
indicates the tokens belonging to the same original token, the same statement, or containing data flow relation, respectively.

However, the vanilla Transformer fails to capture the semantic relations between “print” and “acl”, and NeuralCodeSum

misunderstands the relations between “user” and “acl”. Instead, SG-Trans accurately predicates both relations through

the local self-attention and global self-attention.

Observation 2: Structural information-guided self-attention can facilitate the copy mechanism to copy

important tokens. In Example (3) in Table 10, SG-Trans successfully identified the important token “urlsafe” in the

given code while generating the summary. But both vanilla Transformer and NeuralCodeSum ignored the token and

output less accurate summaries. The reason that the important token is successfully copied by SG-Trans may be

attributed to the structural information-guided self-attention which helps focus on the source tokens more accurately.

21

public -static -boolean-is -File -( -String -path -) -{ -File -f -= -new -File -( -path -) -; -return -f -. -is -File -( -) -; -} -public -static -boolean-is -File -( -String -path -) -{ -File -f -= -new -File -( -path -) -; -return -f -. -is -File -( -) -; -} -public -static -boolean-is -File -( -String -path -) -{ -File -f -= -new -File -( -path -) -; -return -f -. -is -File -( -) -; -} -public -static -boolean-is -File -( -String -path -) -{ -File -f -= -new -File -( -path -) -; -return -f -. -is -File -( -) -; -} --1.0-0.8-0.6-0.4-0.2-0.0(a) Token-guided head attention atlayer-3(b) Statement-guided head attention at layer-3(c) Data flow-guided head attention at layer-6public -static -boolean -is - File - ( - String - path - ) - { - File -f -= - new - File - ( - path - ) -; - return - f - . - is - File - ( - ) - ; - } -public -static -boolean -is - File - ( - String - path - ) - { - File -f - = - new - File - ( - path - ) -; - return - f - . - is - File - ( - ) - ; - } --1.0-0.8-0.6-0.4-0.2-0.0(a) Attention heatmap at layer-3(b) Attention heatmap at layer-3(c) Attention heatmap at layer-6public -static -boolean -is - File - ( - String - path - ) - { - File -f - = - new - File - ( - path - ) -; - return - f - . - is - File - ( - ) - ; - } -public -static -boolean -is - File - ( - String - path - ) - { - File -f - = - new - File - ( - path - ) -; - return - f - . - is - File - ( - ) - ; - } -Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

Table 11. Duplicate data in the Java dataset.

Total data
Duplicate data

Validation Set
8,714
2,028 (23.3%)

Test set
8,714
2,059 (23.6%)

Table 12. Comparison results on the de-duplicated Java dataset. Data listed within brackets are computed drop rates compared with
the results on original Java dataset.

BLEU-4

Approach
NeuralCodeSum 29.37 (↓34.95%)
29.49 (↓34.52%)
GREAT
30.46 (↓33.74%)
SG-Trans

ROUGE-L
41.62 (↓24.11%)
41.84 (↓23.12%)
42.97 (↓23.07%)

METEOR
19.98 (↓27.24%)
20.15 (↓25.79%)
20.82 (↓25.32%)

6.2 Duplicate Data in the Java Dataset

During our experimentation, we find that there are duplicate data in the Java dataset, which may adversely affect

the model performance [2]. As for the Python dataset, there is no duplication across different training, validation

and test set. As shown in Table 11, there are 23.3% and 23.7% duplicate data in the validation set and the test set,

respectively. To evaluate the impact of the data duplication on the proposed model, we remove the duplicate data

across the training, validation, and test sets. We choose the two strongest baselines, NeuralCodeSum and GREAT,

for comparison. The results after deduplication are shown in Table 12. As can be seen, all models present a dramatic

decrease on the de-duplicated dataset. Nevertheless, the proposed SG-Trans still outperforms GREAT on the BLEU-4,

ROUGE-L and METEOR metrics, i.e., by 3.3%, 2.7% and 3.3%, respectively.

6.3 Analysis of the Hierarchical Structure-Variant Attention Mechanism

The hierarchical structure-variant attention mechanism in SG-Trans aims at rendering the model focus on local

structures at shallow layers and global structure at deep layers. In the section, we analyze whether the mechanism can

assist SG-Trans learning the hierarchical information. We visualize the distributions of attention scores corresponding

to the relative token distances for the shallow layer – Layer one, middle layer – Layer four, and one deep layer – Layer
seven, respectively. Specifically, for each relative token distance 𝜄, its attention distribution 𝑌𝜄 is computed as Equ. (19).

𝑌𝜄 =

(cid:205)𝑁

𝑖=1 attention(𝑖, 𝑖 + 𝜄) + attention(𝑖, 𝑖 − 𝜄)
(cid:205)𝑁

𝑖=1 attention(𝑖, 𝑖 + 𝑗) + attention(𝑖, 𝑖 − 𝑗)

(cid:205)𝑆

𝑗=1

(19)

where 𝑁 denotes the number of tokens and 𝑆 denotes the longest relative distance for analysis (𝑆 = 10 in our analysis).
The attention(𝑖, 𝑗) denotes the attention score of token 𝑥𝑖 to 𝑥 𝑗 (1 ≤ 𝑗 ≤ 𝑁 ). The attention scores reflect whether the
model focuses on local information or global information. We choose the relational Transformer GREAT for comparison

since it also involves structural information but is not designed hierarchically. The visualization is depicted in Figure 10.

For GREAT, as shown in Figure 10 (a), we find that the attention distributions across different layers present similar

trends, i.e., they all tend to focus on different token distances uniformly. For SG-Trans, as shown in Figure 10 (b), we can

observe that the three layers pay various attentions to tokens of different relative distances. The shallow layer (Layer

one) more focuses on tokens with short relative distances. In the middle layer (Layer four), the attention distribution

among different distances is more balanced, which indicates that the model pays increasingly more attention to global

tokens with the layer depth being increased. For the deep layer (Layer seven), the attention scores for tokens of long

22

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

(a) GREAT.

(b) SG-Trans.

Fig. 10. Attention distributions regarding relative token distance for (a) GREAT and (b) SG-Trans. The horizontal axis represents the
relative distance to the current token, and the vertical axis denotes the normalized attention scores along with relative distances in
one layer.

distances are larger than those of short distances, meaning that the model tends to focus on long-range relations in

deep layer. The results demonstrate that the hierarchical attention mechanism in SG-Trans is beneficial for the model to

capture the hierarchical structural information which cannot be easily learned by the relational Transformer.

6.4 Difference with Relational Transformers

The main differences between SG-Trans and the relational Transformers [26, 70] are mainly in two aspects:

(1) Strategy in incorporating structural information. Compared with GREAT [26] and CodeTransformer [70]

which use learnable bias and sinusoidal encoding function to encode the structure information, respectively,

SG-Trans incorporates the local and global structure information with different strategies, e.g., introducing the

local information with hard mask. The results in Table 2 and Table 4 show the effectiveness of the structural

incorporation strategy in SG-Trans.

(2) Design of hierarchical structure-variant attention mechanism. In SG-Trans, a hierarchical attention mech-

anism is designed to assist model learning the hierarchical information; while the relational Transformers do not

involve such design. Both the ablation study in Section 5.2 and the discussion in Section 6.3 demonstrate the

benefits of the design.

6.5 Difference with GraphCodeBERT

SG-Trans takes data flow information which is similar to GraphCodeBERT [21]. However, the two methods are different

in the following aspects.

(1) Role of data flow. GraphCodeBERT mainly uses the data flow in two ways: (1) filtering the irrelevant signals

in the variable sequence, (2) guiding the two pre-training tasks, including edge prediction and node alignment.

Nevertheless, SG-Trans directly uses data flow information to help the attention mechanism better capture the

data dependency relations in source code.

(2) Incorporation way of data flow. We integrate the data flow information in a different way as GraphCodeBERT.

GraphCodeBERT utilizes a sparse masking mechanism to mask the attention relations between the tokens without

23

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

data dependency. However, SG-Trans retains the attention relations for the tokens without data dependency, and

also highlights the data flow dependency through our designed data flow-guided self-attention.

(3) Targets of the proposed model. The targets of the two models are different. GraphCodeBERT is proposed to

utilize the inherent structure of code to facilitate the pre-training stage. However, SG-Trans mainly focuses on

the task without large amount of source code available and uses the incorporated code structure to alleviate the

dependency on the training data.

Table 13. Comparison of the cost of different models. The “_” under the preprocessing time of NeuralCodeSum and CodeBERT
denotes that the approaches do not need preprocess. The “_” under the training time of CodeBERT denotes that we do not reproduce
the pre-training stage due to the limitation of computing resource.

GPU memory usage Training time

NeuralCodeSum
CodeTransformer
SG-Trans
CodeBERT

8729M
8573M
8509M
17959M

30.4h
211.5h
29.9h
-

Preprocessing time
-
66.1ms
3.8ms
-

6.6 Analysis of the Complexity of SG-Trans

SG-Trans incorporates structural information based on three types of relations between code tokens. Comparing with

the baseline approaches, such as NeuralCodeSum, SG-Trans involves more types of relations, which could lead to an

increase in the model complexity and subsequently impacting its applicability to other programming languages. To

investigate to what extent SG-Trans introduces extra complexity, we conduct analysis of the cost of SG-Trans.

Specifically, we first compare the cost of SG-Trans with Transformer-based baselines including NeuralCodeSum and

CodeTransformer, and a pre-training model CodeBERT, in terms of the GPU memory usage, training time cost and

preprocessing time cost. The comparison is implemented on the same server with a single Tesla V100 GPU by training

on the Java dataset with 32 batch size. The results are shown in Table 13. As can be seen, the GPU memory usage and

training time cost of SG-Trans are the lowest among all the approaches. Since SG-Trans does not involve the relative

positive embedding, both its GPU memory usage and training time cost are even lower than NeuralCodeSum. Table 13

also shows that CodeBERT requires the highest memory usage, restricting its application to low-resource devices. With

respect to the preprocessing time cost for one sample, since SG-Trans does not need calculate the complex features

used in CodeTransformer such as shortest path length and personalized PageRank, it only takes about 3.8ms which is

significantly faster than CodeTransformer. The results indicate that the code structure properties used in SG-Trans do

not bring larger cost than the baselines.

For the application of SG-Trans to other programming languages, the main barrier lies in the data flow extraction

procedure. In this work, we follow the main idea of Wang et al.’s work [59] and only consider the common data flow

information which is generally similar for different programming languages. The common data flow information

includes sequential data flow relations and three types of non-sequential data flow relations such as “if ” statements,

“for” and “while” loops. The sequential data flow relations can be easily extracted by identifying the variables for any

programming language. For the non-sequential data flow relations, the extraction procedure of different programming
languages is also similar. Because the AST parser tree-sitter2 can parse the data flow relations of different languages
into almost the same tree structure. Thus, it is convenient to extend SG-Trans to other popular languages.

2https://github.com/tree-sitter/tree-sitter

24

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Table 14. Comparison results with CodeBERT. The “CodeBERT” represents the the CodeBERT approach without fine tuning. The
“SG-Translarge” represents SG-Trans with the same model settings as CodeBERT, i.e., 10 encoder layers and hidden size as 768.

Approach

Java

Python

BLEU-4 METEOR ROUGE-L BLEU-4 METEOR ROUGE-L

CodeBERT[15]
CodeBERT+fine-tune[15]
SG-Trans
SG-Translarge

14.93
44.40
45.89
46.27

9.23
28.33
27.85
28.37

30.43
55.56
55.79
56.30

16.70
32.04
33.04
33.53

9.68
20.77
20.52
20.87

30.31
47.45
47.01
47.42

6.7 Comparison with CodeBERT

Many pre-training models [15, 21] have been proposed recently, which can be adopted for source code summarization.

So we also compare the performance of SG-Trans with the most typical pre-training model CodeBERT. We also train
SG-Trans under the same model size as CodeBERT and denote it as SG-Translarge for further comparison.

As shown in Table 14, without fine tuning, CodeBERT shows the worst performance among all the approaches. After

enough fine tuning, CodeBERT improves a lot and even outperforms SG-Trans on some metrics, e.g., the METEOR

score on the Java dataset. However, it should be noted that the encoder layer number and hidden size of CodeBERT

are much larger than SG-Trans. Specifically, the numbers of encoder layers and hidden size of CodeBERT are 10

and 768, respectively; while SG-Trans only has 8 encoder layers and hidden size as 512. For fair comparison, we
also train SG-Trans with the same model settings as CodeBERT, denoted as SG-Translarge. As shown in Table 14,
SG-Translarge obtains the best performance almost all the metrics. Specifically, on the Java dataset, SG-Translarge
outperforms CodeBERT+fine-tune by 4.2% and 1.3% in terms of BLEU-4 and ROUGE-L, respectively. And on the Python
dataset, SG-Translarge is only a little bit lower than CodeBERT+fine-tune on ROUGE-L but obviously outperforms
CodeBERT+fine-tune by 4.7% in terms of the BLEU-4 score. The results demonstrate that SG-Trans is more effective

than CodeBERT even with accessing to limited data.

6.8 Threats to Validity

There are three main threats to the validity of our study.

(1) The generalizability of our results. We use two public large datasets, which include 87,136 Java and 92,545 Python

code-summary pairs, following the prior research [1, 60, 67]. The limited types of programming languages may

hinder the scalability of the proposed SG-Trans. In our future work, we will experiment with more large-scale

datasets with different programming languages.

(2) More code structure information could be considered. SG-Trans only takes the token-level and statement-level

syntactic structure and the data flow structure into consideration, since it has been previously demonstrated

that the data flow information is more effective than AST and CFG during code representation learning [21].

Nevertheless, other code structural properties such as AST and CFG, could be potentially useful for boosting the

model performance. In the future, we will explore the use of more structural properties in SG-Trans.

(3) Biases in human evaluation. We invited 10 participants to evaluate the quality of 200 randomly selected code-

summary pairs. The results of human annotations can be impacted by the participants’ programming experience

and their understanding of the evaluation metrics. To mitigate the bias of human evaluation, we ensure that the 10

participants are all software developers with at least four years programming experience, and each code-summary

pair was evaluated by 3 participants. Summaries generated by different approaches were also randomly shuffled

25

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

in order to eliminate the order bias. In the future, we will expand the pool of human participants and will also

increase the size of the evaluation set.

7 RELATED WORK

In this section, we elaborate on two threads of related work, including source code summarization, code representation

learning.

7.1 Source Code Summarization

There have been extensive research in source code summarization, including template-based approaches [41, 43, 52],

information-retrieval-based approaches [24, 44, 62] and deep-learning-based-approaches [4, 27, 31]. Among these

categories, deep-learning-based methods have achieved the greatest success and become the most popular in recent

years, which specifically formulate the code summarization task as a neural machine translation (NMT) problem and

adopt state-of-the-art NMT frameworks to improve the performance. In this section, we focus on deep-learning-based

methods and introduce them by their category. We also list an overview of the category of related works in Table 15.

RNN-based models: Iyer et al. [31] first propose CODE-NN, a Long Short Term Memory (LSTM) network with

attention to generate code summaries from code snippets. In order to achieve more accurate code modeling, later

researchers then introduce more structural and syntactic information to the deep learning models. Hu et al. [27] propose

a structure-based traversal(SBT) method to traverse AST and processing the AST nodes into sequences that can be fed

into a RNN encoder. Another work [28] hold the view that code API carries vital information about the functionality of

the source code and incorporate the API knowledge by adding an API Sequences Encoder.

Tree/GNN-based models: To leverage the tree structures of AST, a multi-way Tree-LSTM [50] is proposed to directly

model the code structures. For more fine-grained intra-code relationship exploitation, many works also incorporate

code-related graphs and GNN to boost performance. Fernandes et al. [16] build a graph from source code and extract

nodes feature with gated graph neural network while LeClair et al. [35] directly obtain code representation from AST

with Convolutional Graph Neural Networks. To help model capture more global interactions among nodes, a recent

work [39] propose a hybrid GNN which fuse the information from static and dynamic graphs via hybrid message

passing.

Transformer-based models: With the rise of Transformer in NMT task domain, Ahmad et al. [1] equip transformer

with copy attention and relative position embedding for better mapping the source code to their corresponding

natural language summaries. To leverage the code structure information into Transformer, Hellendoorn et al. [26]

propose GREAT which encode structural information into self attention with adding a learnable edge type related bias.

Another work proposed by Zügner et al. [70] focuses on multilingual code summarization and proposes to build upon

language-agnostic features such as source code and AST-based features. Wu et al. [63] propose the Structure-induced

Self-Attention to incorporate multi-view structure information into self-attention mechanism. To capture both the

sequential and structural information, a recent work [11] applies graph convolution to obtain structurally-encoded

node representations and passes sequences of the graph-convolutioned AST nodes into Transformer. Another recent

work [18] proposes to utilize AST relative positions to augment the structural correlations between code tokens.

Information retrieval auxiliary methods: The information retrieval auxiliary methods utilize information re-

trieval and large-scale code repositories to help model generate high-quality summary. Zhang et al. [67] propose to

improve code summarization with the help of two retrieved code using syntactic and semantic similarity. To help model

26

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Table 15. An overview of existing works related to code summarization. The “IR” and “MT” denotes information retrieval auxiliary
methods and multi-task learning strategy respectively.

Approach
CODE-NN [31]
DeepCom [27]
Code2Seq [4]
API+Code [28]
Dual Model [60]
Rencos [67]
Re2Com [61]
Tree-LSTM [50]
Code+GNN [35]
HGNN [39]
NeuralCodeSum [1]
DMACOS [65]
GREAT [26]
CodeTransformer [70]
Transformer+GNN [11]

Input code property RNN Tree/GNN Transformer

IR MT

✓
✓
✓
✓
✓
✓
✓

–
AST
AST
API Information
–
–
–
AST
AST
CPG
–
–
Multi relations in code
AST
AST

✓
✓

✓

✓

✓

✓
✓
✓

✓

✓
✓
✓
✓
✓

generate more important but low-frequency tokens, Wei et al. [61] further use the existing comments of retrieved code

snippets as exemplars to guide the summarization.

Multi-task learning strategy: Some research try to exploit commonalities and differences across code-related

tasks to further improve the code summarization. Wei et al. [60] use a dual learning framework to apply the relations

between code summarization and code generation and improve the performance of both tasks. A recent work [65] use

method name prediction as an auxiliary task and design a multi-task learning approach to improve code summarization

performance.

Compared with existing work, our proposed model focuses on improving the Transformer architecture for source

code to make it better incorporate both local and global code structure. Other improvement methods like information

retrieval and multi-task learning which are orthogonal to our work are not the research target of this paper.

7.2 Code Representation Learning

Learning high-quality code representations is of vital importance for deep-learning-based code summarization. Apart

from the above practices for code summarization, there also exist other code representation learning methods that

lie in similar task domains such as source code classification, code clone detection, commit message generation, etc.

For example, the ASTNN model proposed by Zhang et al. [68] splits large ASTs into sequences of small statement

trees, which are further encoded into vectors as source code representations. This model is further applied on code

classification and code clone detection. Alon et al. [4] present CODE2SEQ that represents the code snippets by sampling

certain paths from the ASTs. Gu et al. [20] propose to encode statement-level dependency relations through the

Program Dependency Graph (PDG). Comparatively, the above research on the model architecture improvement for

code representation learning is relevant to us but mainly focuses on other code-related tasks such as code classification,

code clone detection, etc.

Recently, inspired by the successes of pre-training techniques in natural language processing field, Feng et al. [15]

and Guo et al. [21] also apply pre-training models on learning source code and achieve empirical improvements on a

variety of tasks. To extend the code representations to characterize programs’ functionalities, [32] further enriches the

27

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

pre-training tasks to learn task-agnostic semantic code representations from textually divergent variants of source

programs via contrastive learning. The target of the research about pre-training is greatly different from us, i.e., they

mainly focus on learning from large-scale dataset in a self-supervised way. However, our research concentrates on

improving the quality of generated summaries with limited training data in a supervised way.

8 CONCLUSION

In this paper, we present SG-Trans, a Transformer-based architecture with structure-guided self-attention and hierar-

chical structure-variant attention. SG-Trans can attain better modeling of the code structural information, including

local structure in token-level and statement-level, and global structure, i.e., data flow. The evaluation on two popular

benchmarks suggests that SG-Trans outperforms competitive baselines and achieves state-of-the-art performance on

code summarization. For future work, we plan to extend the use of our model to other task domains, and possibly build

up more accurate code representations for general usage.

ACKNOWLEDGMENTS

This research was supported by National Natural Science Foundation of China under project No. 62002084, Stable

support plan for colleges and universities in Shenzhen under project No. GXWD20201230155427003-20200730101839009,

and the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14210920 of the

General Research Fund). This research was also partly funded by the UK Engineering and Physical Sciences Research

Council (No. EP/V048597/1, EP/T017112/1). Yulan He is supported by a Turing AI Fellowship funded by the UK Research

and Innovation (No. EP/V020579/1).

REFERENCES

[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai,
Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 4998–5007.

[2] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019 ACM SIGPLAN
International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, Onward! 2019, Athens, Greece, October 23-24,
2019, Hidehiko Masuhara and Tomas Petricek (Eds.). ACM, 143–153.

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs. In 6th International Conference

on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.

[4] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences from Structured Representations of Code. In 7th

International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.

[5] Bang An, Jie Lyu, Zhenyi Wang, Chunyuan Li, Changwei Hu, Fei Tan, Ruiyi Zhang, Yifan Hu, and Changyou Chen. 2020. Repulsive Attention:
Rethinking Multi-head Attention as Bayesian Inference. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics,
236–255.

[6] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. CoRR abs/1607.06450 (2016).
[7] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In
Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor,
Michigan, USA, June 29, 2005, Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss (Eds.). Association for Computational Linguistics, 65–72.
[8] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of Python functions and documentation strings for automated code

documentation and code generation. arXiv preprint arXiv:1707.02275 (2017).

[9] Jie-Cherng Chen and Sun-Jen Huang. 2009. An empirical analysis of the impact of software development problem factors on software maintainability.

J. Syst. Softw. 82, 6 (2009), 981–992.

[10] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning
Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, Alessandro
Moschitti, Bo Pang, and Walter Daelemans (Eds.). ACL, 1724–1734.

28

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

[11] YunSeok Choi, JinYeong Bak, CheolWon Na, and Jee-Hyong Lee. 2021. Learning Sequential and Structural Information for Source Code Summarization.
In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021),
Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 2842–2851.

[12] Milan Cvitkovic, Badal Singh, and Animashree Anandkumar. 2019. Open Vocabulary Learning on Source Code with a Graph-Structured Cache. In
Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (Proceedings of Machine
Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 1475–1485.

[13] Sergio Cozzetti B. de Souza, Nicolas Anquetil, and Káthia Marçal de Oliveira. 2005. A study of the documentation essential to software maintenance.
In Proceedings of the 23rd Annual International Conference on Design of Communication: documenting & Designing for Pervasive Information, SIGDOC
2005, Coventry, UK, September 21-23, 2005, Scott R. Tilley and Robert M. Newman (Eds.). ACM, 68–75.

[14] Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to-Sequence Attentional Neural Machine Translation. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The
Association for Computer Linguistics.

[15] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for
Computational Linguistics, 1536–1547.

[16] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured Neural Summarization. In 7th International Conference on Learning

Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.

[17] Golara Garousi, Vahid Garousi-Yusifoglu, Günther Ruhe, Junji Zhi, Mahmood Moussavi, and Brian Smith. 2015. Usage and usefulness of technical

software documentation: An industrial case study. Inf. Softw. Technol. 57 (2015), 664–682.

[18] Zi Gong, Cuiyun Gao, Yasheng Wang, Wenchao Gu, Yun Peng, and Zenglin Xu. 2022. Source Code Summarization with Structural Relative Position

Guided Transformer. CoRR abs/2202.06521 (2022).

[19] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. 2016. Incorporating Copying Mechanism in Sequence-to-Sequence Learning. In Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The
Association for Computer Linguistics.

[20] Wenchao Gu, Zongjie Li, Cuiyun Gao, Chaozheng Wang, Hongyu Zhang, Zenglin Xu, and Michael R. Lyu. 2021. CRaDLe: Deep code retrieval based

on semantic Dependency Learning. Neural Networks 141 (2021), 385–394.

[21] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 2020. GraphCodeBERT: Pre-training Code
Representations with Data Flow. CoRR abs/2009.08366 (2020).

[22] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-Transformer. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational
Linguistics, 1315–1325.

[23] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue, and Zheng Zhang. 2020. Multi-Scale Self-Attention for Text Classification. In The Thirty-Fourth
AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 7847–7854.
[24] Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program comprehension with source code summarization. In Proceedings of the
32nd ACM/IEEE International Conference on Software Engineering - Volume 2, ICSE 2010, Cape Town, South Africa, 1-8 May 2010, Jeff Kramer, Judith
Bishop, Premkumar T. Devanbu, and Sebastián Uchitel (Eds.). ACM, 223–226.

[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer

Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 770–778.

[26] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. 2020. Global Relational Models of Source Code. In 8th

International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

[27] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In Proceedings of the 26th Conference on Program Comprehension,

ICPC 2018, Gothenburg, Sweden, May 27-28, 2018, Foutse Khomh, Chanchal K. Roy, and Janet Siegmund (Eds.). ACM, 200–210.

[28] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing Source Code with Transferred API Knowledge. In Proceedings of the
Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, Jérôme Lang (Ed.). ijcai.org,
2269–2275.

[29] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, and Tom Zimmermann. 2022. Practitioners’ Expectations on Automated Code Comment

Generation. In ICSE ’22: Proceedings of the 44th ACM/IEEE International Conference on Software Engineering.

[30] Chidubem Iddianozie and Gavin McArdle. 2020. Improved Graph Neural Networks for Spatial Networks Using Structure-Aware Sampling. ISPRS Int.

J. Geo Inf. 9, 11 (2020), 674.

[31] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a Neural Attention Model. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers. The Association for Computer Linguistics.

29

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Gao, et al.

[32] Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph E. Gonzalez, and Ion Stoica. 2020. Contrastive Code Representation Learning. CoRR

abs/2007.04973 (2020).

[33] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generating commit messages from diffs using neural machine translation. In
Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2017, Urbana, IL, USA, October 30 - November 03,
2017, Grigore Rosu, Massimiliano Di Penta, and Tien N. Nguyen (Eds.). IEEE Computer Society, 135–146.

[34] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes. 2020. Big code != big vocabulary: open-vocabulary
models for source code. In ICSE ’20: 42nd International Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020, Gregg Rothermel
and Doo-Hwan Bae (Eds.). ACM, 1073–1085.

[35] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved Code Summarization via a Graph Neural Network. In ICPC ’20:

28th International Conference on Program Comprehension, Seoul, Republic of Korea, July 13-15, 2020. ACM, 184–195.

[36] Yuding Liang and Kenny Qili Zhu. 2018. Automatic Generation of Text Descriptive Comments for Code Blocks. In Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI
Press, 5229–5236.

[37] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational

Linguistics, Barcelona, Spain, 74–81.

[38] Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT To Evaluate Your Dialogue
System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, Jian Su, Xavier Carreras, and Kevin Duh
(Eds.). The Association for Computational Linguistics, 2122–2132.

[39] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. 2021. Retrieval-Augmented Generation for Code Summarization via Hybrid GNN.

In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

[40] Zhongxin Liu, Xin Xia, Christoph Treude, David Lo, and Shanping Li. 2019. Automatic Generation of Pull Request Descriptions. In 34th IEEE/ACM

International Conference on Automated Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019. IEEE, 176–188.

[41] Paul W. McBurney and Collin McMillan. 2016. Automatic Source Code Summarization of Context for Java Methods. IEEE Trans. Software Eng. 42, 2

(2016), 103–119.

[42] Roberto Minelli, Andrea Mocci, and Michele Lanza. 2015. I know what you did last summer: an investigation of how developers spend their time. In
Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension, ICPC 2015, Florence/Firenze, Italy, May 16-24, 2015, Andrea De
Lucia, Christian Bird, and Rocco Oliveto (Eds.). IEEE Computer Society, 25–35.

[43] Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori L. Pollock, and K. Vijay-Shanker. 2013. Automatic generation of natural
language summaries for Java classes. In IEEE 21st International Conference on Program Comprehension, ICPC 2013, San Francisco, CA, USA, 20-21 May,
2013. IEEE Computer Society, 23–32.

[44] Dana Movshovitz-Attias and William W. Cohen. 2013. Natural Language Models for Predicting Programming Comments. In Proceedings of the 51st
Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 2: Short Papers. The Association
for Computer Linguistics, 35–40.

[45] Lun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam, Yang Liu, and Zenglin Xu. 2020. Contextualized Code Representation Learning for Commit

Message Generation. CoRR abs/2007.06934 (2020).

[46] Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2019. Multi-style Generative
Reading Comprehension. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-
August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics,
2273–2284.

[47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings

of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 311–318.

[48] Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of
the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers,
Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, 1073–1083.

[49] Lin Shi, Hao Zhong, Tao Xie, and Mingshu Li. 2011. An Empirical Study on Evolution of API Documentation. In Fundamental Approaches to Software
Engineering - 14th International Conference, FASE 2011, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2011,
Saarbrücken, Germany, March 26-April 3, 2011. Proceedings (Lecture Notes in Computer Science, Vol. 6603), Dimitra Giannakopoulou and Fernando
Orejas (Eds.). Springer, 416–431.

[50] Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, and Tadayuki Matsumura. 2019. Automatic Source Code Summarization
with Extended Tree-LSTM. In International Joint Conference on Neural Networks, IJCNN 2019 Budapest, Hungary, July 14-19, 2019. IEEE, 1–8.
[51] Kai Song, Kun Wang, Heng Yu, Yue Zhang, Zhongqiang Huang, Weihua Luo, Xiangyu Duan, and Min Zhang. 2020. Alignment-Enhanced Transformer
for Constraining NMT with Pre-Specified Translations. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 8886–8893.

30

Code Structure Guided Transformer for Source Code Summarization

Woodstock ’18, June 03–05, 2018, Woodstock, NY

[52] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L. Pollock, and K. Vijay-Shanker. 2010. Towards automatically generating summary
comments for Java methods. In ASE 2010, 25th IEEE/ACM International Conference on Automated Software Engineering, Antwerp, Belgium, September
20-24, 2010, Charles Pecheur, Jamie Andrews, and Elisabetta Di Nitto (Eds.). ACM, 43–52.

[53] Sean Stapleton, Yashmeet Gambhir, Alexander LeClair, Zachary Eberhart, Westley Weimer, Kevin Leach, and Yu Huang. 2020. A Human Study of
Comprehension and Code Summarization. In ICPC ’20: 28th International Conference on Program Comprehension, Seoul, Republic of Korea, July 13-15,
2020. ACM, 2–13.

[54] Zoltán Gendler Szabó. 2020. Compositionality. In The Stanford Encyclopedia of Philosophy, Edward N. Zalta (Ed.).
[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is
All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December
4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and
Roman Garnett (Eds.). 5998–6008.

[56] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing Multi-Head Self-Attention: Specialized Heads Do the
Heavy Lifting, the Rest Can Be Pruned. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,
Italy, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). Association for Computational
Linguistics, 5797–5808.

[57] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Improving automatic source code summarization via
deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, ASE 2018, Montpellier,
France, September 3-7, 2018, Marianne Huchard, Christian Kästner, and Gordon Fraser (Eds.). ACM, 397–407.

[58] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2020. RAT-SQL: Relation-Aware Schema Encoding and
Linking for Text-to-SQL Parsers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July
5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 7567–7578.

[59] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax

Tree. In SANER. IEEE, 261–271.

[60] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as a Dual Task of Code Summarization. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 6559–6569.

[61] Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and Refine: Exemplar-based Neural Comment Generation. In 35th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020. IEEE, 349–360. https://doi.org/
10.1145/3324884.3416578

[62] Edmund Wong, Taiyue Liu, and Lin Tan. 2015. CloCom: Mining existing source code for automatic comment generation. In 22nd IEEE International
Conference on Software Analysis, Evolution, and Reengineering, SANER 2015, Montreal, QC, Canada, March 2-6, 2015, Yann-Gaël Guéhéneuc, Bram
Adams, and Alexander Serebrenik (Eds.). IEEE Computer Society, 380–389.

[63] Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization with Structure-induced Transformer. In Findings of the Association for
Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021), Chengqing Zong, Fei Xia,
Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 1078–1090.

[64] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E. Hassan, and Shanping Li. 2018. Measuring Program Comprehension: A Large-Scale

Field Study with Professionals. IEEE Trans. Software Eng. 44, 10 (2018), 951–976.

[65] Rui Xie, Wei Ye, Jinan Sun, and Shikun Zhang. 2021. Exploiting Method Names to Improve Code Summarization: A Deliberation Multi-Task Learning
Approach. In 29th IEEE/ACM International Conference on Program Comprehension, ICPC 2021, Madrid, Spain, May 20-21, 2021. IEEE, 138–148.
[66] Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura. 2018. Guiding Neural Machine Translation with Retrieved
Translation Pieces. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers). 1325–1335.

[67] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020. Retrieval-based neural source code summarization. In ICSE ’20: 42nd
International Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020, Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM,
1385–1397.

[68] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. A novel neural source code representation based on
abstract syntax tree. In Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019,
Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (Eds.). IEEE / ACM, 783–794.

[69] Xiangyu Zhao, Longbiao Wang, Ruifang He, Ting Yang, Jinxin Chang, and Ruifang Wang. 2020. Multiple Knowledge Syncretic Transformer for
Natural Dialogue Generation. In WWW ’20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, Yennun Huang, Irwin King, Tie-Yan Liu, and
Maarten van Steen (Eds.). ACM / IW3C2, 752–762.

[70] Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann. 2021. Language-Agnostic Representation Learning of
Source Code from Structure and Context. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net.

31

