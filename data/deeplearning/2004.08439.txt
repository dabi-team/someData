0
2
0
2

r
p
A
7
1

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

1
v
9
3
4
8
0
.
4
0
0
2
:
v
i
X
r
a

Scaling the training of particle classiﬁcation on
simulated MicroBooNE events to multiple GPUs

A Hagen, E Church, J Strube, K Bhattacharya, and V Amatya
Paciﬁc Northwest National Laboratory, Richland, WA, USA

E-mail: alexander.hagen@pnnl.gov

Abstract. Measurements in Liquid Argon Time Projection Chamber (LArTPC) neutrino
detectors, such as the MicroBooNE detector at Fermilab [1], feature large, high ﬁdelity event
images. Deep learning techniques have been extremely successful in classiﬁcation tasks of
photographs, but their application to LArTPC event images is challenging, due to the large
size of the events. Events in these detectors are typically two orders of magnitude larger than
images found in classical challenges, like recognition of handwritten digits contained in the
MNIST database or object recognition in the ImageNet database. Ideally, training would occur
on many instances of the entire event data, instead of many instances of cropped regions of
interest from the event data. However, such eﬀorts lead to extremely long training cycles, which
slow down the exploration of new network architectures and hyperparameter scans to improve
the classiﬁcation performance. We present studies of scaling a LArTPC classiﬁcation problem
on multiple architectures, spanning multiple nodes. The studies are carried out on simulated
events in the MicroBooNE detector. We emphasize that it is beyond the scope of this study
to optimize networks or extract the physics from any results here. Institutional computing at
Paciﬁc Northwest National Laboratory and the SummitDev machine at Oak Ridge National
Laboratorys Leadership Computing Facility have been used. To our knowledge, this is the ﬁrst
use of state-of-the-art Convolutional Neural Networks for particle physics and their attendant
compute techniques onto the DOE Leadership Class Facilities. We expect beneﬁts to accrue
particularly to the Deep Underground Neutrino Experiment (DUNE) LArTPC program, the
ﬂagship US High Energy Physics (HEP) program for the coming decades.

1. Introduction
Use of convolutional networks to analyze time projection chamber data is often performed on
cropped data because of large image sizes. Training and inference on uncropped TPC data is
desired to minimize physics information loss before training. The high ﬁdelity and large size of
the image data requires scaling of computing resources past the 1s to 10s and 100s of GPUs.

1.1. The MicroBooNE Detector and data format used
This work formats its simulated data with inspiration from the MicroBooNE experiment [1].
MicroBooNE is a 170 tonne liquid argon time projection chamber (LArTPC) with the express
interest of analyzing neutrino physics. Readout of MicroBooNE consists of 2 induction planes
with 2400 wires each and 1 collection plane with 3156 wires. Readout occurs every 4.8 ms (which
is 2.2× the TPC drift time) for 9600 digitizations.

 
 
 
 
 
 
µ+

3000

2000

1000

e
r
i

W

e±

K +

π+

γ

0

0

2000

0

2000

0

2000
Time (ticks)

0

2000

0

2000

Figure 1: Example of simulated single particle interactions for classiﬁcation.

This work presents classiﬁcation on simulated single particle events in a format similar to
that of MicroBooNE 1. GEANT4 Monte Carlo particle transport code was used to simulate
interactions with the wires in a LArTPC. The seed particles were single particles of type µ+,
e+, e−, K+, π+, and γ. The interactions with the collection plane were then tallied across
3200 time ticks and padded with zeros to increase the time dimension to 3600. The collection
plane dimension was also padded with zeros to reach size 3600. The ﬁnal simulated data was a
3600 × 3600 ”image”, with examples for each particle shown in Figure 1.

Previous work showed not only decreased training time for simple convolution networks on 2
to 14 GPUs (on PNNL’s Institutional Computing), but improved performance, achieving lower
losses with 14 GPUs versus with only 1 [2]. This work extends upon that scaling study, moving
from 1 to 10s of GPUs to the 100s.

2. Methodology
A simpliﬁed convolutional neural network (CNN) model was developed for testing2. As shown in
ﬁgure 2, the network architecture used two subsequent blocks consisting of convolutional layers
with kernel size 5, padding of 2, and exponential linear unit (ELU) activation; followed by max
pool layers of pooling size 5. Then, two more subsequent blocks of convolutional layers with
kernel size 4, padding of two, and ELU activation followed by max pooling layers of pool size
4. These four blocks have increasing numbers of convolutional ﬁlters with the scheme of 1, 10,
64, 128, 256. After these four blocks, a linear layer with 20736 inputs, 32 outputs and ELU
activation was appended. A linear layer with 32 inputs, 5 outputs, and softmax activation, one
output for each of the particle types (e+ and e− were grouped together), ﬁnished the network.
For a dense data representation, this network architecture was implemented using Torch’s
Conv2d and MaxPool2d layers and is hereafter called JishNet. For a sparse data representation,
discussed further in section 2.2, this network used SparseConvNet’s SubmanifoldConvolution
and MaxPooling layers and is hereafter referred to as SCNet. It should be noted that the padding
behavior of the convolutions in JishNet and SCNet diﬀer subtly.

2.1. Resources
Many industrial applications of convolutional networks can apply 10s to 100s of GPUs to speed
training and enable fast iteration in network design [3, 4].

2.1.1. Hardware This work presents one of the ﬁrst known instances of scaling a physics
classiﬁcaton problem to this scale on Oak Ridge National Laboratory Leadership Computing

1 Permission from MicroBooNE collaboration was granted to focus on compute resources and performance studies
2 It is stressed that this model was generated to provide model results while focusing on studying the computatonal
resources. The CNN itself could be improved in numerous ways, but that is past the scope of this work.

36

256

144

128

720

64

256

32

5

softmax ((cid:126)x)

ELU

ELU

3600

10

Figure 2: Design of ”JishNet” and ”SCNet” convolutional networks used for all testing presented
on this poster

Facility’s SummitDev computer. SummitDev , which is a test stand for the eventual Summit
computer, was used in this study. The SummitDev machine boasts 50 nodes, each with 2 22-
core IBM Power8NVL CPUs and 4 NVIDIA P100 GPUs. The interconnections between nodes
are Mellanox EDR 100G IniﬁBand; interconnections between GPU to host node are NVLink.
It should also be noted that there is a 4 hour time limit to all SummitDev jobs.

2.1.2. Software There is an embarassment of options for the development of neural networks
and data parallel training of these neural networks. While previous scaling studies used Paciﬁc
Northwest National Laboratory’s MaTEx [2, 5] code for data parallel training, this work was
based on the use of Torch [6] for neural network deﬁnition and training, and Horovod [4] for
data parallelization.

Due to the large size of images and their large number, compression was needed to store
the dense representation of the images in memory when loading3. Compression using Blosc [7]
when loading images from ﬁle into CPU memory. At training time, a minibatch of these images
were uncompressed from memory and transferred to GPU memory. Due to the large size of
each datum, only 7 images alongsize the JishNet model could ﬁt into a single NVIDIA P100’s
memory. Thus, all training was performed with a minibatch size of 7. After training of a single
minibatch on all GPUs allocated for each training instance, Horovod’s allreduce function was
used to transfer and allocate all gradients to the head node. This has the eﬀect of generating a
batch size of 7 × NGP U . The loss and accuracy local to each GPU was written to disk before
this allreduce operation.

The use of many GPUs signiﬁcantly aﬀects the training dynamics of the CNN in this training
example. A prolonged study was performed examining the training dynamics of this task
using stochastic gradient descent (SGD) optimizer and warmup and scaling suggested in Goyal
[8]. The SGD optimization study ultimately underperformed, so other options were explored.
Distributed Adam optimization was tested and ultimately successful; thereafter, all results used
Adam optimization [9]. The training dynamics were highly sensitive to parameters of Adam
optimizer, ﬁnal parameters were lr = 0.001, β = [0.9, 0.999], (cid:15) = 1 × 10−8, and decay = 0.01.
Figures 3a and 4a illustrate expected training behavior and verify the use of the Adam optimizer.

3 No compression was needed to store the sparse representation of these events.

32 GPUs, jishnet
64 GPUs, jishnet

128 GPUs, jishnet
192 GPUs, jishnet

1.6

1.4

s
s
o
L

i

g
n
n
a
r
T

i

1.2

1.0

0

TTLTTL
2500

)
s
(

L
T
T

TTL ∝ (ngpu)−0.7 [s]

4000

2000

100
GPU

200

t
i

m

i
l

e
m

i
t

r
u
o
h

4

5000

7500
Time (s)

10000

12500

15000

)

h
c
o
p
e

n
i
m

(

i

g
n
n
a
r
T

i

d
e
e
p
S

1.5

1.0

0.5

N
t

∝ (ngpu)0.9

(cid:19)

(cid:18) epoch
min

32

64

128

GPUs

192

(a) Training dynamics of JishNet using
increasing number of GPUs on SummitDev .

(b) Speedup of training JishNet with increas-
ing number of GPUs on SummitDev .

Figure 3: Results from training the dense convolutional network JishNet using various numbers
of GPUs on SummitDev .

2.2. Sparse
Most of the data passed to the dense convolutional network is zeros, as no events happen in
large parts of the TPC. Sparse convolutional networks are a more eﬃcient network architecture
for LArTPC event classiﬁcaton. After conversion from a dense format to a sparse data format,
training using SubmanifoldConvolution layers from Facebook’s SparseConvNet [10] codebase
and the modiﬁed CNN SCNet was performed.

3. Results
Results show increasing the number of GPUs for both dense (JishNet) and sparse (SCNet)
CNNs was successful in decreasing the training time on large images in large datasets.

3.1. Dense
Scaling to multiple GPUs shows an approximately linear speedup with respect to the number of
GPUs, as shown in Figure 3b. Figure 3a shows the loss versus training wall time with increasing
GPUs. With more GPUs, a lower loss can be achieved in the allotted 4 hours, and the time to
a loss (TTL) of 1.2 decreases with the number of GPUs used (shown in inset).

In Figure 3a, there is a marked diﬀerence in the time at which the ﬁrst loss was reported
with diﬀerent number of GPUs. This time, the data loading time (DLT), is aﬀected by how
many images must be loaded into CPU before beginning training. Each training example was
performed on 15,000 images, so each CPU had to load and compress 15,000/NGP U into memory.
Optimization of this compression routine is left for future work. It is hypothesized that the TTL
speedup is sublinear (∝ N 0.7)

GP U ) because of this DLT eﬀect.

3.2. Sparse
SCNet shows interesting behavior during training compared to JishNet. Figure 4b again shows
an approximately linear speedup when trained on increasing numbers of GPUs, however the
highest speed is ∼ 2.5 epochs/min, which is 60 % faster than SCNet. However, the TTL, shown in
Figures 4a is 4× lower than that in Figure 3a.

Figure 4a shows comparable loss to JishNet after the full training set, however there are
several interesting characteristics. Compared to Figure 3a, there is very little evident DLT in

32 GPUs, scnet
64 GPUs, scnet
128 GPUs, scnet

TTL ∝ (ngpu)−0.6 (s)

1.4

1.3

1.2

1.1

1.0

s
s
o
L

i

g
n
n
a
r
T

i

750

500

)
s
(

L
T
T

TTL

0

2500

5000

50

100

GPUs

t
i

m

i
l

e
m

i
t

r
u
o
h

10000

12500

4
15000

7500
Time (s)

2.5

)

h
c
o
p
e

n
i
m

2.0

(

i

g
n
n
a
r
T

i

d
e
e
p
S

1.5

1.0

32

N
t

64

∝ n0.9
gpu

(cid:19)

(cid:18) epoch
min

128

GPUs

(a) Training dynamics of SCNet using increas-
ing number of GPUs on SummitDev .

(b) Speedup of training SCNet with increasing
number of GPUs on SummitDev .

Figure 4: Results from training the sparse convolutional network SCNet using various numbers
of GPUs on SummitDev .

Table 1: Confusion matrices from training of JishNet and SCNet with 128 GPUs for 4 hours.

(a) Confusion matrix for JishNet trained with
15000 images for 4 hours on 128 GPUs.

(b) Confusion matrix for SCNet trained with
15000 images for 4 hours on 128 GPUs.

γ

e±

Predicted
K+
µ+
66.7% 33.0% 0.0% 0.0% 0.3%
γ
e±
1.6% 98.0% 0.0% 0.4% 0.0%
µ+ 0.0% 0.1% 98.9% 0.0% 1.0%
π+ 1.6% 1.7% 1.3% 37.5% 57.8%
K+ 0.6% 0.1% 2.1% 26.3% 71.0%

π+

e
u
r
T

γ

e±

Predicted
K+
µ+
66.1% 33.2% 0.0% 0.7% 0.0%
γ
e±
0.9% 98.8% 0.0% 0.3% 0.0%
µ+ 0.0% 0.0% 99.4% 0.0% 0.5%
π+ 0.0% 1.1% 1.0% 68.0% 29.9%
K+ 0.4% 0.4% 1.8% 67.1% 30.4%

π+

e
u
r
T

Figure 4a. Despite this, the TTL speedup is only ∝ N 0.6

GP U .

It should be noted that the 7 image GPU memory limitation did not apply for SCNet, due
to the small size of each datum, yet we kept the minibatch size at 7, nevertheless. This study
did not explore varying the minibatch size (and in extension batch size), but the researchers
hypothesize that increased speedup could be realized using more optimal batch sizes during
training of SCNet.

4. Discussion and Conclusions
Training both dense and sparse convolutional networks on 10s to 100s of GPUs proved successful
in this study. The wall time to a training loss decreased linearly with increased amounts of GPU
resources, and allowed training on large images for physics related classiﬁcatoin studies. The ﬁnal
accuracy metrics for both of these networks are shown in Tables 1a and 1b. The ﬁnal validation
accuracy for JishNet was 78.8% ± 3.8%, and for SCNet was 76.8% ± 3.3%. Particle labeling
confusion is observed among species for which physics intuition suggests it is not unexpected.
This study also elucidated several other important aspects related to training CNNs with large
image sizes. These aspects included the sensitivity of training to the optimizer parameters
and seeming inappropriateness of suggestions in the literature for training with SGD. We also
uncovered the importance of minibatch size and tradeoﬀ between GPU memory resources and
optimal training dynamics.

Acknowledgements
The authors gratefully acknowledge the MicroBooNE collaboration for permission to work on
simulated LArTPC data to focus on compute resources and performance scaling. This research
used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National
Laboratory, which is supported by the Oﬃce of Science of the U.S. Department of Energy under
Contract No. DE-AC05-00OR22725. Thus, the Oak Ridge Leadership Computing Facility and
its staﬀ are also gratefully acknowledged for use of the SummitDev computer. The authors also
gratefully acknowledge Paciﬁc Northwest National Laboratory’s Institutional Computing for use
of its resources.

References
[1] Acciarri R, Adams C, An R, Aparicio A, Aponte S, Asaadi J, Auger M, Ayoub N, Bagby L, Baller B, Barger
R, Barr G, Bass M, Bay F, Biery K, Bishai M, Blake A, Bocean V, Boehnlein D, Bogert V, Bolton T,
Bugel L, Callahan C, Camilleri L, Caratelli D, Carls B, Fernandez R C, Cavanna F, Chappa S, Chen H,
Chen K, Chi C Y, Chiu C, Church E, Cianci D, Collin G, Conrad J, Convery M, Cornele J, Cowan P,
Crespo-Anad´on J, Crutcher G, Darve C, Davis R, Tutto M D, Devitt D, Duﬃn S, Dytman S, Eberly B,
Ereditato A, Erickson D, Sanchez L E, Esquivel J, Farooq S, Farrell J, Featherston D, Fleming B, Foreman
W, Furmanski A, Genty V, Geynisman M, Goeldi D, Goﬀ B, Gollapinni S, Graf N, Gramellini E, Green
J, Greene A, Greenlee H, Griﬃn T, Grosso R, Guenette R, Hackenburg A, Haenni R, Hamilton P, Healey
P, Hen O, Henderson E, Hewes J, Hill C, Hill K, Himes L, Ho J, Horton-Smith G, Huﬀman D, Ignarra C,
James C, James E, de Vries J J, Jaskierny W, Jen C M, Jiang L, Johnson B, Johnson M, Johnson R, Jones
B, Joshi J, Jostlein H, Kaleko D, Kalousis L, Karagiorgi G, Katori T, Kellogg P, Ketchum W, Kilmer J,
King B, Kirby B, Kirby M, Klein E, Kobilarcik T, Kreslo I, Krull R, Kubinski R, Lange G, Lanni F,
Lathrop A, Laube A, Lee W, Li Y, Lissauer D, Lister A, Littlejohn B, Lockwitz S, Lorca D, Louis W,
Lukhanin G, Luethi M, Lundberg B, Luo X, Mahler G, Majoros I, Makowiecki D, Marchionni A, Mariani
C, Markley D, Marshall J, Caicedo D M, McDonald K, McKee D, McLean A, Mead J, Meddage V, Miceli
T, Mills G, Miner W, Moon J, Mooney M, Moore C, Moss Z, Mousseau J, Murrells R, Naples D, Nienaber
P, Norris B, Norton N, Nowak J, OBoyle M, Olszanowski T, Palamara O, Paolone V, Papavassiliou V, Pate
S, Pavlovic Z, Pelkey R, Phipps M, Pordes S, Porzio D, Pulliam G, Qian X, Raaf J, Radeka V, Raﬁque
A, Rameika R A, Rebel B, Rechenmacher R, Rescia S, Rochester L, von Rohr C R, Ruga A, Russell B,
Sanders R, III W S, Sarychev M, Schmitz D, Schukraft A, Scott R, Seligman W, Shaevitz M, Shoun M,
Sinclair J, Sippach W, Smidt T, Smith A, Snider E, Soderberg M, Solano-Gonzalez M, Sldner-Rembold
S, Soleti S, Sondericker J, Spentzouris P, Spitz J, John J S, Strauss T, Sutton K, Szelc A, Taheri K, Tagg
N, Tatum K, Teng J, Terao K, Thomson M, Thorn C, Tillman J, Toups M, Tsai Y T, Tufanli S, Usher
T, Utes M, de Water R V, Vendetta C, Vergani S, Voirin E, Voirin J, Viren B, Watkins P, Weber M,
Wester T, Weston J, Wickremasinghe D, Wolbers S, Wongjirad T, Woodruﬀ K, Wu K, Yang T, Yu B,
Zeller G, Zennamo J, Zhang C and Zuckerbrot M 2017 Journal of Instrumentation 12 P02017–P02017
URL https://doi.org/10.1088%2F1748-0221%2F12%2F02%2Fp02017

[2] Bhattacharya K, Church E, Schram M, Strube J, Wierman K, Daily J and Siegel C 2018 Scaling studies for
deep learning in LArTPC event classiﬁcation International Conference on Computing in High Energy and
Nuclear Physics (Soﬁa, Bulgaria)

[3] Mikami H, Suganuma H, U-chupala P, Tanaka Y and Kageyama Y 2018 (Preprint 1811.05233) URL

http://arxiv.org/abs/1811.05233

[4] Sergeev A and Del Balso M 2018 (Preprint 1802.05799) URL http://arxiv.org/abs/1802.05799
[5] Amatya V 2018 Machine Learning Toolkit for Extreme Scale (MaTEx)
[6] Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison A, Antiga L and Lerer A

2017 Automatic diﬀerentiation in pytorch NIPS-W

[7] Alted F and Haenel V 2019 python-blosc: a Python wrapper for the extremely fast Blosc compression library

URL https://github.com/Blosc/python-blosc

[8] Goyal P, Doll´ar P, Girshick R, Noordhuis P, Wesolowski L, Kyrola A, Tulloch A, Jia Y and He K 2017 ISSN

2167-3888 (Preprint 1706.02677) URL http://arxiv.org/abs/1706.02677

[9] Kingma D P and Ba J 2014 (Preprint 1412.6980) URL http://arxiv.org/abs/1412.6980
[10] Graham B and van der Maaten L 2017 arXiv preprint arXiv:1706.01307

