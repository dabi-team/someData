© 2022 IEEE. This article has been accepted for publication in IEEE Transactions on Games. This is the author’s version which has not been fully edited
and content may change prior to ﬁnal publication. Citation information: DOI 10.1109/TG.2022.3206733.

AlphaZero-Inspired Game Learning: Faster Training
by Using MCTS Only at Test Time

Johannes Scheiermann
RWTH Aachen University
Germany
Email: johannes.scheiermann@rwth-aachen.de

Wolfgang Konen
TH K¨oln – University of Applied Sciences
Germany
Email: wolfgang.konen@th-koeln.de

2
2
0
2

p
e
S
4
2

]

G
L
.
s
c
[

3
v
7
0
3
3
1
.
4
0
2
2
:
v
i
X
r
a

Abstract—Recently, the seminal algorithms AlphaGo and Al-
phaZero have started a new era in game learning and deep
reinforcement learning. While the achievements of AlphaGo and
AlphaZero – playing Go and other complex games at super
human level – are truly impressive, these architectures have
the drawback that they require high computational resources.
Many researchers are looking for methods that are similar to
AlphaZero, but have lower computational demands and are thus
more easily reproducible.
In this paper, we pick an important element of AlphaZero –
the Monte Carlo Tree Search (MCTS) planning stage – and
combine it with temporal difference (TD) learning agents. We
wrap MCTS for the ﬁrst time around TD n-tuple networks and
we use this wrapping only at test time to create versatile agents
that keep at the same time the computational demands low. We
apply this new architecture to several complex games (Othello,
ConnectFour, Rubik’s Cube) and show the advantages achieved
with this AlphaZero-inspired MCTS wrapper. In particular, we
present results that this agent is the ﬁrst one trained on standard
hardware (no GPU or TPU) to beat the very strong Othello
program Edax up to and including level 7 (where most other
learning-from-scratch algorithms could only defeat Edax up to
level 2).

I. INTRODUCTION

A. Motivation

In computer science, game learning and game playing
are interesting test beds for strategic decision making done
by computers. Games usually have large state spaces, and
they often require complex pattern recognition and strategic
planning capabilities to decide which move is the best in a
certain situation. If an algorithm is able to learn a game (or,
even better, a variety of different games) just by self-play,
given no other knowledge than the game rules, it is likely
to perform also well on other problems of strategic decision
making.

With their seminal papers on AlphaGo [1], AlphaGo
Zero [2] and AlphaZero [3], Silver et al. opened a new door
in game learning by presenting self-learning algorithms for
the game of Go (which was considered to be unattainable for
computers prior to theses publications). All these algorithms
were able to beat the human Go world champion Lee Sedol.
[1]–[3] require huge
computational resources in order to learn how to play the
game of Go at world-master level. It is the purpose of this
work to investigate whether some of the important elements of

the full algorithms in

However,

AlphaZero can already reach decent advances in game learning
with much smaller computational efforts. For this purpose,
we study several games – namely Othello, ConnectFour and
Rubik’s Cube – that have a lower complexity than Go yet
are not easy to master for both humans and game learning
algorithms. The goal is to deliver not only agents with average
game playing strength but agents that learn from scratch and
play almost as well as the strongest known algorithms1 for
these games. We will show that this can be achieved for
Othello and ConnectFour and, to some extent, also for Rubik’s
Cube.

In this work, we pick an element of AlphaZero (here: the
MCTS planning stage) and combine it with reinforcement
learning (RL) agents. We wrap MCTS around TD(λ) n-tuple
networks [4], [5], [6], but the same technique could be applied
to all types of RL agents.

The main contributions of this paper are as follows: (i)
it shows for the ﬁrst time – to the best of our knowledge
– a coupling between trainable TD(λ) n-tuple networks and
MCTS planning; (ii) an AlphaZero-inspired solution, but with
largely reduced computational requirements; (iii) a comparison
between MCTS inside and outside of self-play training; (iv)
very good results on Othello, ConnectFour and 2x2x2 Rubik’s
Cube. Speciﬁcally, we are able to defeat the strong Othello
program Edax at level 7 with an agent trained from scratch in
less than 2 hours on a standard CPU.

The rest of this paper is organized as follows: Sec. II
details the algorithmic building blocks and methods of our
approach. Sec. III describes the experimental setup, the games
and the evaluation methods. Sec. IV presents the results on
the three games: quality achieved, interpretation, computation
times. Sec. V shows related work and discusses our results in
comparison with other research. Sec. VI concludes.

II. ALGORITHMS AND METHODS

The algorithm presented in this paper is implemented in the
General Board Game (GBG) learning and playing framework
[7], [8], which was developed for education and research in
AI. GBG allows applying the new algorithm easily to a variety
of games. GBG is open source and available on GitHub2.

1e. g. Edax for Othello or AB-DL for ConnectFour as will be further

explained in Sec. IV

2https://github.com/WolfgangKonen/GBG

1

 
 
 
 
 
 
A. Algorithm Overview

The most important task of a game-playing agent is, given
an observation or game state st at
to propose a
good next action at from the set of actions available in st
(Fig. 1). TD-learning uses the value function V (st), which is
the expected sum of future rewards when being in state st.

time t,

It is the task of the agent to learn the value function V from
experience (by interacting with the environment). In order to
do so, it usually performs multiple self-play training episodes
until a certain training budget is exhausted or a certain game-
playing strength is reached.

Our base RL algorithm TD-FARL is described in detail in
[9], [10] and is partly inspired by Jaskowski et al. [11], van
der Ree et al. [12] and partly by our own experience with
RL-n-tuple training. The key elements of the new RL-logic
– as opposed to our previous RL algorithms [4], [13] – are
n-tuple systems, temporal coherence learning (TCL) [14] and
ﬁnal adaptation RL (FARL) [9], [10]. All these key elements
will be brieﬂy described in Sec. II-C, II-D and II-E.

Despite being successful on a variety of games [9], [10],
this base algorithm shares one disadvantage with other deep
learning algorithms that are only value-based: they base their
decision on the value of the current state-action pairs. They
have no planning component, no what-if scenarios to think
about further consequences, like possible counter-actions of
the other player(s), further own actions and so on.

action at

MCTS wrapper

RL agent

environment

observation st
reward rt

Figure 1. Reinforcement learning with MCTS wrapper: The RL agent with
MCTS wrapper observes a certain state st and reward rt from the game
environment and predicts the next action at.

This is where AlphaZero’s MCTS-trick comes into play:
Silver et al. [1], [2] combine a deep learning RL agent with
an MCTS wrapper (Fig. 1) to introduce such a planning com-
ponent. They do this throughout the whole training procedure,
which is better for the overall performance but also very
computationally demanding. In this work, we take a simpler
approach: we ﬁrst train our RL agent, a TD n-tuple network,
and then use the MCTS wrapping only at test time (i. e. during
game play). This usage of MCTS adds a form of planning at
test time.

B. MCTS Wrapper

Our MCTS wrapper is based on the UCT variant of MCTS,
where the probability of predicting an optimal move converges
to 100% in the limit of an inﬁnite number of iterations [15].
If we limit the iterations to a ﬁxed number, we only approach
optimality but have a ﬁxed runtime.

Therefore, with an MCTS, promising results can be ex-
pected under reasonable computational requirements, given the
number of MCTS iterations is correctly balanced.

The iterations of MCTS usually consist of four consecu-
tive steps: selection, expansion, simulation, and backpropaga-
tion [16]. The following child selection policy, which is the
one used by Silver et al. [2] in AlphaGo Zero, is also the one
we implemented in our MCTS wrapper for the same purpose:

at = arg max
a∈A(st)

U (s, a) = cpuctP (s, a)

(cid:18) W (st, a)
N (st, a)
(cid:113)
ε + (cid:80)

(cid:19)

+ U (st, a)

(1)

b∈A(s) N (s, b)

1 + N (s, a)

(2)

Here, W (s, a) is the accumulator for all backpropagated
values (as detailed in Algorithm 1 below) that arrive along
with branch a of node R that carries state s. Likewise, N (s, a)
is the visit counter and P (s, a) the prior probability. A(s) is
the set of actions available in state s. ε is a small positive
constant for the special case (cid:80)
b N (s, b) = 0: It guarantees
that in this special case the maximum of U (s, a) is given by
the maximum of P (s, a). The prior probabilities P (s, a) are
obtained by sending the RL agent’s values of all available
state-action pairs (s, a) with a ∈ A(s) through a softmax
function (see Sec. II-C).3

According to Silver et al. [2], the above child selection
policy is a variant of the PUCB (”Predictor + UCB”) algorithm
presented by Rosin [17]. Furthermore, the latter is a modiﬁ-
cation of the bandit algorithm UCB1, extending it with the
behavior to also consider the recommendations of a predictor.
UCB1 is also the basis of the previously mentioned algorithm
UCT (UCB applied to trees) by Kocsis and Szepesv´ari [15].
Our implementation of an MCTS iteration is illustrated in
Algorithm 1. It performs a single MCTS iteration for a given
node. The numerical return value approximates how valuable
it is to choose an action that leads to this node. Since this
assessment corresponds to the view of the previous player,
the algorithm negates the returned values (κ = −1) in the
case of 2-player games.

If the node represents a game-over state, then the conse-
quence of choosing this node is known and does not need to
be approximated. In this case, the ﬁnal game score is the value
to propagate back.

Reaching a non-expanded node is also a termination con-
dition. In this case, the approximator function f (usually the
wrapped RL agent of Fig. 1) approximates the value V of
the corresponding node together with its action probabilities
p (line 6). Afterward, the node is marked as expanded, and
its approximated value is propagated back.

SELECTCHILD is used to select a child node based on the
PUCB variant of Eq. (1) if no previous termination condition
occurred. To determine the selected child node’s value, it

3Note that the prior probabilities and the MCTS iteration are only needed
at test time, so that we – different to AlphaZero – do not need MCTS during
self-play training.

Algorithm 1. MCTSITERATION: This recursive algorithm is applicable to 1-
or 2-player games. It performs a single iteration of a Monte Carlo tree search,
starting from root node R carrying state s.

1: function MCTSITERATION(NODE R)
2:
3:
4:

κ = (−1)N −1
if ISGAMEOVER(s) then

return κ ∗ FINALGAMESCORE(s)

(cid:46) N : number of players

5:
6:

7:
8:
9:

10:

11:
12:
13:
14:
15:

16:
17:

if R.EXPANDED = FALSE then

(V, p) ← f (s)
P (s, ·) ← p
R.EXPANDED ← TRUE
return κ ∗ V

(cid:46) f : approximator network
(cid:46) prior probabilities given by f

(a, C) ← SELECTCHILD(R)

(cid:46) use Eq. (1) to select
(cid:46) action a and child C

Vchild ← MCTSITERATION(C)
W (s, a) ← W (s, a) + Vchild
N (s, a) ← N (s, a) + 1

return κ ∗ Vchild

Figure 2. Example n-tuples: We show 4 random-walk 8-tuples on a 6x7
ConnectFour board. The tuples are selected manually to show that not only
snake-like shapes are possible, but also bifurcations or cross shapes. Tuples
may or may not be symmetric.

serves as input to another recursive call of the MCTSITERA-
TION algorithm. On return from the recursive call, the returned
value Vchild is added to W (s, a) (line 14), and the visit count
N (s, a) is incremented.

Our MCTS implementation ﬁrst performs a certain number
of iterations starting from the node corresponding to the
current game state in a concrete match. Then it decides on
the action that leads to the most frequently visited child node.
Furthermore, our tree search adopts another element from
AlphaGo which reuses the previously built search tree when-
ever possible, i. e., when a node corresponding to the current
game state is already present in the search tree of the previous
move. This optimization avoids performing superﬂuous MCTS
iterations, which only recalculate already known facts again.

C. N-Tuple Systems

N-tuple systems coupled with TD were ﬁrst applied to game
learning by Lucas in 2008 [5], although n-tuples were already

introduced in 1959 for character recognition purposes [18].
The remarkable success of n-tuples in learning to play Oth-
ello [5] motivated other authors to beneﬁt from this approach
for a number of other games. The main goal of n-tuple systems
is to map a highly non-linear function in a low dimensional
space to a high dimensional space where it is easier to separate
‘good’ and ‘bad’ regions. This can be compared to the kernel
trick of support-vector machines. An n-tuple is deﬁned as
a sequence of n cells of the board. Each cell can have m
positional values representing the possible states of that cell.
Therefore, every n-tuple will have a (possibly large) look-up
table indexed in form of an n-digit number in base m. Each
entry corresponds to a feature and carries a trainable weight.
An n-tuple system is a system consisting of k n-tuples. Tab. I
shows the n-tuple systems that we use in this work. Each time
a new agent is constructed, all n-tuples are formed by random
walk. That is, all cells are placed randomly with the constraint
that each cell must be adjacent4 to at least one other cell in
the n-tuple. An example n-tuple system is shown in Fig. 2.

Let Θ be the vector of all weights θi of the n-tuple system.
The length of this vector may be large, as the ﬁfth column
in Tab. I shows. If all n-tuples have the same n and m, Θ
has length mnk. Let Φ(s) be a binary vector of the same
length representing the feature occurences in state s. The value
function of the n-tuple network given state s is

V (s) = σ (Φ(s) · Θ)

(3)

with transfer function σ which may be a sigmoidal function
or simply the identity function.

An agent using this n-tuple system derives a policy from the
value function in Eq. (3) as follows: Given state s and the set
A(s) of available actions in state s, it applies with a forward
model f every action a ∈ A(s) to state s, yielding the next
state s(cid:48) = f (s, a). Then it selects the action that maximizes
V (s(cid:48)).

The prior probabilities P (s, a) in Eq. (2) are calculated
similarly: Given state s and actions a ∈ A(s), we compute
with Eq. (3) all values V (s(cid:48)) of reachable states s(cid:48) = f (s, a)
and send them through a softmax function to yield P (s, a).

D. TD Learning and FARL

The goal of n-tuple agent

training is to learn a value
function V that generates a good policy, i. e. a policy that
selects in almost all cases the best action. In our work, we use
the TD learning algorithm TD-FARL [9], [10] for training,
which is brieﬂy described in the following.

Let s(cid:48)[p] be the actual state generated by acting player p and
let s[p] be the previous state that was generated by this acting
player. TD(0) learning [12], [19] adapts the value function
with model parameters Θ through

Θt+1 = Θt + αδ∇ΘV (s[p])

(4)

4The form of adjacency (e. g. 4- or 8-point neighborhood) is user-deﬁned.

game
Othello
ConnectFour
2x2x2 Rubik’s
3x3x3 Rubik’s

length n
7
8
7
7

position m
4
4
{3, 7}
{2, 3, 8, 12}

k
2 · 100
2 · 70
60
120

weights
3,276,800
9,175,040
3,720,780
46,563,392

percent active
51%
8%
31%
22%

Table I
N-TUPLE SYSTEMS USED IN THIS WORK. PARAMETERS n, m AND k ARE EXPLAINED IN THE MAIN TEXT. THE NUMBER OF WEIGHTS IS 47 · 200
(OTHELLO) AND 48 · 140 (CONNECTFOUR). FOR 2X2X2 RUBIK’S CUBE, EACH 7-TUPLE HAS EITHER 3 OR 7 POSITIONAL VALUES, DEPENDING ON THE
CELL LOCATION. THUS, THE NUMBER OF WEIGHTS DEPENDS ON THE CELL LOCATION. SINCE NOT EVERY WEIGHT REPRESENTS A REACHABLE
POSITION, THE NUMBER OF ACTIVE WEIGHTS IS SMALLER, AS GIVEN BY THE PERCENTAGE IN THE LAST COLUMN.

Here, α is the learning rate and V is in our case the n-tuple
value function of Eq. (3). δ is the usual TD error [19] after
player p has acted and generated s(cid:48)[p]:

δ = r[p] + γV (s(cid:48)[p]) − V (s[p])

(5)

where the sum of the ﬁrst two terms (reward r[p] for player
p and the discounted value γV (s(cid:48)[p])) is the desirable target
for V (s[p]).

The extra element of FARL is to add a ﬁnal adaptation step
at the end of each episode: All players p(cid:48) different from the
last player take their reward r[p(cid:48)] and adapt the value function
for state s[p(cid:48)] according to Eq. (4), but with error signal

δ = r[p(cid:48)] − V (s[p(cid:48)]) ∀p(cid:48) (cid:54)= p

(6)

This FARL step was found to be crucial for reaching training
success in N -player games with arbitrary N [9].

TD-FARL performs its training purely by (cid:15)-greedy self-
play: The agent plays against itself and makes random moves
with probability (cid:15) in order to explore. No external knowledge
or databases are used. More details on TD-FARL for N -
player games, including the extension to TD(λ) with arbitrary
λ ∈ [0, 1], based on the eligibility mechanism for n-tuple
systems [11], are described in our previous work [10].

E. Temporal Coherence Learning (TCL)

The TCL algorithm developed by Beal and Smith [14] is an
extension of TD learning. It replaces the global learning rate
α with the weight-individual product ααi for every weight θi.
Here, the adjustable learning rate αi is a free parameter set by
a pretty simple procedure: For each weight θi, two counters
Ni and Ai accumulate the sum of weight changes and the
sum of absolute weight changes. If all weight changes have
the same sign, then αi = |Ni|/Ai = 1, and the learning rate
stays at its upper bound. If weight changes have alternating
signs, then the global learning rate is probably too large. In
this case, αi = |Ni|/Ai → 0 for t → ∞, and the effective
learning rate will be largely reduced for this weight.

In our previous work [4] we extended TCL to αi =
g(|Ni|/Ai) where g is a transfer function being either the
identity function (standard TCL) or an exponential function
g(x) = eβ(x−1). It was shown in [4] that TCL leads to faster
learning and higher win rates for the game ConnectFour.

(a)

(b)

Figure 3. (a) Othello game state. The black cell with a red outline marks the
last move of Black. It is White’s turn to choose one of the available actions
marked by cells with a green border. These actions capture one or more black
pieces, which are then ﬂipped to white. (b) ConnectFour game state. It is Red’s
turn, and they have to place their piece in the only free column. Subsequently,
Yellow wins by reaching Four in a Row. Numbers show cell coding: 1 and 2
for players’ pieces, 3: empty and reachable, 0: empty, but not reachable (in
next move).

III. EXPERIMENTAL SETUP

A. The Games

1) Othello:

(Reversi) is a well-known board game with
quite simple rules yet requiring complex strategies to play
strongly. Fig. 3(a) shows a typical game position. The regular
8x8 Othello has 1028 states and an average branching factor
of 10. It is an unsolved game (no perfect winning strategy is
known).

In our Othello experiments, we compete against Edax [20],
a strong Othello playing program. We use Edax in the same
way as Norelli et al. [21] with standard settings (alpha-beta
search, tabular value functions, no opening books) and vary
only the Edax level (search depth).

2) ConnectFour: (Four in a Row) is another board game
with quite simple rules. Fig. 3(b) shows a typical end game
position. The regular 6x7 ConnectFour has 1012 states and a
branching factor ≤ 7. It is a solved game: The 1st player wins
if playing perfectly.

In our ConnectFour experiments, we compete against the
strong alpha-beta search agents AB and AB-DL [22], further
described in Sec. IV-B. AB and AB-DL work with a pre-
computed opening book and do not need any evaluation
functions, search depth or time budget parameters.

3) Rubik’s Cube: is a famous puzzle of a cube consisting
of smaller ’cubies’ where the goal is to move an arbitrary
scrambled cube into the solved position through a sequence
of twists. In the solved position, each cube face consists of 9
(3x3x3 cube) or 4 (2x2x2 cube) cubie faces with the same
color. The regular 3x3x3 cube has 4.3 · 1019 states and a
branching factor of 18. The 2x2x2 cube has 3.6 · 106 states
and a branching factor of 9.

B. Common Settings

We use for all our experiments the same RL agent based on
n-tuple systems and TCL. Only its hyperparameters are tuned
to the speciﬁc game, as shown below. We refer to this agent
as TCL-base whenever it alone is used for game playing. If
we wrap this agent by an MCTS wrapper with a given number
of iterations, then we refer to this as TCL-wrap.

The hyperparameters for each game were found by manual
ﬁne-tuning. For reasons of space we give the exact explanation
and the setting of all parameters as supplementary material in
Appendix G of [8]. A short version of these settings is: The
chosen n-tuple conﬁgurations are given in Tab. I, and the main
parameters are:

• Othello: learning rate α = 0.2, TCL activated, eligibility
trace factor λ = 0.5, exploration rate (cid:15) = 0.2 → 0.1,
250,000 training episodes.

• ConnectFour: learning rate α = 3.7, TCL activated,
eligibility trace factor λ = 0.0, exploration rate (cid:15) =
0.1 → 0.0, 6,000,000 training episodes.

• Rubik’s Cube: learning rate α = 0.25, TCL activated,
eligibility trace factor λ = 0.0, exploration rate (cid:15) = 0.0,
3,000,000 training episodes.

simple heuristic players are beaten [5], [23]. But it is very
the very strong Othello playing program
difﬁcult
Edax [20]. Edax has a conﬁgurable playing strength (level,
depth) between 0 and 60.

to beat

We compare our agents with Edax at different levels. Since
all agents (Edax, TCL-base and TCL-wrap) are deterministic
move predictors, repeated evaluation runs with the same pair
of agents always yield the same results and cannot be used
to collect statistics. We use the following procedure to get
statistically sound results: We draw 20 different random n-
tuple conﬁgurations (random walk, see Sec. II-C) and train for
each conﬁguration a separate TCL-base agent. All TCL agents
compete in both roles against Edax, yielding 40 competition
runs.

Fig. 4 shows the resulting win rates (win count divided by
40 runs): Both MCTS and TCL-base cannot defeat Edax at
level 2 and above (their win rates are lower than 50% from
level 2 on). The situation changes dramatically as soon as
we wrap TCL-base by MCTS: TCL-wrap defeats Edax up to
level 7 and has win rates above 25% for levels 8 and 9. The
non-monotonuous trend of TCL-wrap at Edax level 5 and 6
is surprising and not fully understood: It could be statistical
ﬂuctuations or it could be that Edax plays a little weaker at
level 7 than at level 5 or 6.

(a)

(b)

Figure 5. Tactics of Edax in Othello: (a) Move 48 in a game TCL-base
(Black) vs. Edax level 7 (White): It is Black’s turn, and Edax forces Black
into disadvantageous moves that allow White to capture the corners. TCL-wrap
will avoid such disadvantageous positions. (b) Move 57 in a game TCL-wrap
(Black) vs. Edax level 8 (White): Now it is White’s turn, and although Black
has the current majority of pieces, White will eventually win because Black
has to pass and White moves three times in a row.

Interpretation: What are the reasons for opponents to win
or lose in Othello against Edax? – To investigate this, we
analyze speciﬁc Othello episodes: When Edax plays at level
7, it has advanced tactics that narrow the range of possible
actions for the opponent (Fig. 5(a)): If Edax (2nd) plays against
opponent TCL-base (1st), Edax forces TCL-base towards the
end of the episode to play disadvantageous moves. If we now
replace the opponent (1st) with TCL-wrap, it avoids these
traps: The planning stage of TCL-wrap helps to foresee the
disadvantageous positions when they are some moves ahead;
now TCL-wrap ﬁnds other moves to avoid them and is thus
not forced into the disadvantageous positions.

Figure 4. Different Othello agents playing against Edax. TCL-wrap: TCL
coupled with MCTS wrapper (10,000 iterations); TCL-base: TCL alone;
MCTS-10k: MCTS alone with 10,000 iterations. Shown are the percentages of
won runs from (a) 20 TCL agents trained with different random n-tuples in the
TCL cases and (b) 20 MCTS agents with different seeds in the MCTS case.
Each agent plays in both roles (1st and 2nd player), yielding 40 competition
runs in total.

A. Othello

IV. RESULTS

It is not too difﬁcult for game learning algorithms to reach
a medium playing strength in Othello, i. e. a strength where

0.000.250.500.751.00123456789Edax levelwin rateagentGroupTCL−wrapTCL−baseMCTS−10kAt level 8 or higher, Edax shows sometimes another tactic:
It may play in such a way that the last 2-4 moves are pass
moves for the opponent (Fig. 5(b)): Since the opponent has no
available action at its disposal, it is forced to pass the move
right to Edax again. During the very last moves of an episode,
Edax may gain the majority of pieces. Currently, TCL-wrap
is not able to avoid these pass situations, at least not in the
majority of the episodes played.

B. ConnectFour

ConnectFour is a non-trivial game that is not easy to master
for humans. However,
its medium-size complexity allows
for very strong tree-based solutions when combined with a
pre-computed opening book. These near-perfect agents are
termed AB and AB-DL since they are based on alpha-beta
search (AB) that extends the Minimax algorithm by efﬁciently
pruning the search tree. Thill et al. [22] were able to implement
alpha-beta search for ConnectFour in such a way that it plays
near-perfect: It wins all games as 1st player and wins very
often as 2nd player when the 1st player makes a wrong move.
AB and AB-DL differ in the way they react to losing states:
While AB just takes a random move, AB-DL searches for the
move, which postpones the loss as far (as distant) as possible
(DL = distant losses). It is tougher to win against AB-DL since
it will request more correct moves from the opponent and will
very often punish wrong moves.

We perform a tournament with the following 4 agents:
• TCL-wrap: MCTSWrapper[TCL-base]

(iter=1,000,

cP U CT =1.0, unlimited depth),

• TCL-base: TCL alone,
• AB-DL: Alpha Beta with Distant Losses,
• MCTS: MCTS(UCT,
random playouts,

treeDepth=40)

iter=10,000,

Figure 6.
The effect of MCTS wrapping on ConnectFour. Shown are the
averages from 5 runs, where each run consists of 25 competition episodes
MCTS (1st) vs. agent (2nd). The agents are a) TCL-wrap: TCL wrapped by
MCTS wrapper with iterMWrap iterations; b) AB-DL: Alpha-Beta agent with
distant losses; c) TCL-base: TCL without MCTS wrapper. Error bars denote
standard deviations.

1st player). TCL-base (1st) wins against AB-DL (2nd) the
majority of its games (91%), but not all. If we enhance TCL-
base by MCTS wrapper, the win rate of TCL-wrap rises to
fantastic 99%, so it avoids 8/9 of the former TCL-base losses
or ties.

MCTS, as the weakest agent in the tournament, wins as 1st
player most of its games (97%) against TCL-base (2nd), but
it predominantly loses against TCL-wrap and AB-DL (2nd).
TCL-wrap as 2nd player is in this respect signiﬁcantly stronger
than AB-DL (97% vs. 80% win rate, resp.), which leads
for TCL-wrap to a higher total rate of 66.3% won games
as compared to AB-DL (64.9%). Besides that, the total won
games rate 66.3% is a big jump forward when compared to
the total won game rate 49% of TCL-base.

Interpretation: MCTS plays differently, perhaps more sur-
prising, than near-optimal agents. Since TCL-base was trained
on a near-optimal agent (itself), it has never seen the ‘surpris-
ing’ moves of MCTS and will probably often react wrongly
on these moves. Thus, TCL-base loses most of its games when
playing 2nd. If we now add with MCTS wrapper a planning
component to TCL-base, then TCL-wrap can ﬁnd better re-
sponses to the ‘surprising’ moves, and it can better exploit the
occasional wrong moves of MCTS. As a consequence, it wins
most of the episodes.

Fig. 6 shows the results of MCTS wrapping in ConnectFour
as a function of MCTS wrapper iterations. Even a small
amount of iterations (50-100) already leads to a TCL-wrap
win rate of > 80%. With 500 iterations or more, TCL-wrap
achieves a win rate near 100%.

Additionally, we made an experiment comparing our agents
AlphaBeta-DL and TCL-wrap with ConnectZero from Dawson
[24]: We performed 10 episodes with ConnectZero starting
(which is a theoretical win), but found instead that AlphaBeta
playing second won 80% of the episodes and TCL-wrap
playing second won all episodes.

C. Rubik’s Cube

We investigate two variants of Rubik’s Cube: 2x2x2 and
3x3x3. We trained TCL agents by presenting them cubes
scrambled with up to pmax twists where pmax = 13 for 2x2x2
and pmax = 9 for 3x3x35, both in half-turn metric. This covers
the complete cube space for 2x2x2, but only a small subset
for 3x3x3, where God’s number [25] is known to be 20. We
evaluate the trained agents on 200 scrambled cubes that are
created by applying a given number p of scrambling twists to
a solved cube. The agent now tries to solve each scrambled
cube. A cube is said to be unsolved if the agent cannot reach
the solved cube in eE = 20 steps. More details on our method
are found in [26].

Here we are interested in the relative strength of agents
with and without MCTS wrapping. The results are shown in
Fig. 7: While TCL-base could only solve 75% (2x2x2) or 25%
(3x3x3) of the scrambled cubes, resp., the MCTS-wrapped

The results are shown in Tab. II and can be described as
follows: TCL-wrap and AB-DL win nearly all their games
when playing ﬁrst (ConnectFour is a theoretical win for the

5We limit ourselves to up to 9 twists here, because our network has not
enough capacity to learn all states of 3x3x3 Rubik’s Cube. Experiments with
higher twist numbers during training did not improve the solved-rates.

0.000.250.500.751.0002505007501000iterMWrapagent win rateagentTCL−wrapTCL−baseAB−DLMCTS vs. various agentsW/T/L

TCL-wrap

1st

TCL-wrap
AB-DL
TCL-base
MCTS

100/0/0
100/0/0
1/2/97

2nd player

AB-DL
99/0/1

91/2/7
17/3/80

TCL-base
100/0/0
100/0/0

97/2/1

MCTS
100/0/0
100/0/0
100/0/0

won games rate

66.3%
64.9%
49.0%
19.8%

Table II
RESULTS OF A CONNECTFOUR TOURNAMENT WITH 4 AGENTS. SHOWN IS THE W/T/L (WIN/TIE/LOSS) COUNT OF THE 1st (ROW) PLAYER WHEN
PLAYING 100 EPISODES AGAINST THE 2nd (COLUMN) PLAYER. AGENTS ARE RANKED BY THEIR OVERALL RATE OF GAMES WON (LAST COLUMN). THE
COLORED AND BOLD NUMBERS SHOW REMARKABLE IMPROVEMENTS OF TCL-wrap OVER TCL-base.

game

Othello
ConnectFour
RubiksCube

nag

20
10
5

base
training
time

1.5 d
1.4 d
2.2 h

iM CT S

factor

10,000
1,000
1,000

2,575
850
770

hypothetical
wrapped
training time
10.6 years
3.3 years
71 days

Table III
TRAINING TIMES TO TRAIN nag AGENTS WITHOUT AND WITH MCTS
WRAPPING (HYPOTHETICAL) FOR ALL GAMES.

few ‘what-if’ steps is selected. This is sufﬁcient to boost the
solved-rate to 100% after 200 or more MCTS-iterations.

Interpretation 3x3x3: The agent has seen during training
only a small subset of cubes with up to 9 scrambling twists.
Therefore, the solved-rates for p ∈ 8, 9 are much lower for
TCL-base because it is very likely that the cube ‘escapes’
with a wrong move into the unknown area of p = 10 or
higher. It is interesting to see that the MCTS planning stage
can double or triple the solved-rate. However, it can not cure
everything since the high branching factor of 18 together with
slight inaccuracies of the value function approximator makes
it likely that even 1000 iterations of MCTS-wrapper do not
explore enough to ﬁnd the right path.

D. Computation times

The MCTS wrapper for RL agents, as proposed in this
paper, has the advantage that it does not cost any additional
training time since it is an enhancement added after agent
training.

The extra computational resources needed during game play
or evaluation are moderate. This is because we usually only
need a few evaluation episodes (compared to the huge number
of training episodes) and for these few episodes the 10,000
iterations are not a large computational burden, as shown in
Tab. IV (note the unit milliseconds).

The above advantage becomes more apparent if we compare
the actual TCL-base training times with the would-be training
times if the MCTS planning stage were also used during
training, as shown in Tab. III: The base training time is the time
actually needed to train nag agents without MCTS wrapper.
All computations were done on a single CPU Intel i7-9850H
@ 2.60GHz.6

6To get a single-agent training time, the base training time has to be divided
by nag which results for example in 1.8 hours training time for one Othello
agent.

(a)

(b)

Figure 7. The effect of MCTS wrapping on Rubik’s Cube. Shown are the
averages from 4 runs, where each run evaluates the ability of the agents to
solve a large set of scrambled cubes (a) as a function of MCTS iterations:
2x2x2: 600 cubes scrambled with either 11, 12 or 13 twists; 3x3x3: 600 cubes
scrambled with either 7, 8 or 9 twists; (b) as a function of scrambling twists
where iterMWrap=0 corresponds to TCL-base and iterMWrap>0 to different
variants of TCL-wrap. We average over 200 scrambled cubes for each twist
number. Error bars denote standard deviations.

agent TCL-wrap could either fully solve the problem (2x2x2)
or at least double or triple the percentage of solved cubes
(3x3x3).

Interpretation 2x2x2: Since the solved-rate of TCL-base
is only 75%, the value function V (s) does not predict the
right action for every state s (resulting in a short path to
the solved cube). However, if we add the planning stage of
MCTS-wrapper, then the action with the highest V (s) after a

lllllllllllll0%25%50%75%100%02505007501000iterMWrappercentage solvedcubeWidthll2x2x23x3x3agentTCL−wrapTCL−basellllllllllllllllllllll0%25%50%75%100%135791113scrambling twistspercentage solvedcubeWidth2x2x23x3x3iterMWrapl01005001000game
Othello
ConnectFour
3x3x3 Rubik’s

TCL-base
0.4 ± 0.5
0.05 ± 0.02
0.6 ± 0.7

TCL-wrap [100]
10 ± 6
3 ± 2
10 ± 6

TCL-wrap [1000]
100 ± 80
30 ± 20
100 ± 70

TCL-wrap [10000] MCTS [10000]

1,300 ± 700
300 ± 200
1,800 ± 1,000

551 ± 371
66 ± 20
783 ± 49

Table IV
MOVE TIMES OF TCL-base, TCL-wrap, AND MCTS, WHERE TCL-wrap [i] AND MCTS [i] REFER TO THE RESPECTIVE AGENTS USING i ITERATIONS.
SHOWN ARE AVERAGE TIMES AND STANDARD DEVIATIONS IN MILLISECONDS. FOR THE GAMES OTHELLO AND CONNECTFOUR, THE VALUES WERE
AVERAGED OVER 50 GAMES. FOR RUBIKSCUBE, THE VALUES ARE BASED ON 25 CUBES, EACH INITIALLY SCRAMBLED WITH 8 TWISTS.

The wrapped training times are estimated by multiplying
the base training time with factor which is established by
running a few episodes without and with MCTS wrapper
doing iM CT S iterations. This estimate assumes that a wrapped
agent needs as many training episodes as a base agent. This
assumption needs not to be true, a wrapped agent could reach
similar performance in fewer episodes. We experimentally
investigate and discuss this point further in Sec. IV-E. If
training times
this assumption were true,
would be astronomical: We see from Tab. III that with the
same hardware, many years or at
least near 100 days of
computation time would be necessary. Of course, large speed-
ups are possible with dedicated hardware or parallel execution
on many cores, but often this hardware is just not available.

the hypothetical

E. MCTS Inside Self-Play Training

MCTS inside self-play training can improve the quality of
the experience generated. It might be that substantially fewer
episodes are necessary to learn the same (or a better) function.
In order to investigate this, we conducted several experi-
ments for the Othello case: We cannot afford 10,000 MCTS
iterations in the training loop for 250,000 episodes, as Tab. III
shows, but we can approach it from several directions.

Figure 9. MCTS inside training with the same small number of training
episodes (12,000). The labels W∗ have the same meaning as in Fig. 8. Note
that W0 is now different from TCL-wrap (only 12,000 training episodes).
Shown are the win rates from 20 competition runs (16 in the case of W10000).

Figure 10. MCTS inside training with the same number of training episodes
as TCL-wrap (250,000). The labels W∗ have the same meaning as in Fig. 8.
W0 is identical to TCL-wrap. Shown are the win rates vs. Edax from 40
competition runs.

Figure 8. MCTS inside training under the same training time budget (2h)
as TCL-base. The label W∗ stands for MCTS with ∗ = 0, 100, 1,000, 10,000
iterations inside self-play training. This allows for 250,000, 12,000, 1,200 and
120 episodes, resp. W0 is identical to TCL-wrap. Shown are the win rates vs.
Edax from 20 competition runs (40 for W0) conducted in the same way as
in Fig. 4. Each agent uses 10,000 iterations during Edax evaluation.

First, we present

in Fig. 8 several runs with the same
training budget (2 hours) as TCL-base. All
computational
agents, regardless of their MCTS iterations during training,
use the same number of 10,000 MCTS iterations during Edax

evaluation (the standard for TCL-wrap in Othello). It is not
surprising that W1000 and W10000 cannot compete with Edax
on any level, since the number of training episodes is too small
to explore a signiﬁcant portion of the state space.

The next experiment requires a higher computational bud-
get: We take for all agents the same number of 12,000 training
episodes and allow for different numbers of MCTS iterations
during training (Fig. 9). This leads to single-agent training
times of 1.2 days for W1000 and 11.5 days for W10000,
which is 16× and 153× the training time of TCL-wrap,

0.000.250.500.751.00123456789Edax levelwin rateagentGroupW0W100W1000W10000same computational budget (2h)0.000.250.500.751.00123456789Edax levelwin rateagentGroupW0W100W1000W10000same training episodes (12,000)0.000.250.500.751.00123456789Edax levelwin rateagentGroupW0W100250,000 training episodes resp. Nevertheless, the win rates against Edax level 5–9 are
considerably lower than for TCL-wrap. Surprisingly, the win
rates for W1000 and W10000 are for some levels lower than
for W100. It might be that the agents with higher iteration
counts become too deterministic and explore too little during
self-play.7

The third experiment tests MCTS iterations during train-
ing when we choose the same number of training episodes
(250,000) as TCL-wrap. 100 iterations are the maximum
possible here, since the training time for a single agent is
already 2.5 days. The results in Fig. 10 show that W100 is
mostly better than W0 (TCL-wrap), with the largest increase
for Edax level 8 (from 34% to 66%).

In summary, these experiments show that MCTS inside self-
play training cannot reach the same results as TCL-wrap under
(a) the same computational budget (2 hours) or (b) a small
number (12,000) of training episodes. But if we use the same
number of 250,000 training episodes as TCL-wrap, we see
a small positive effect of using MCTS in the training loop.
However, this requires 30 times more computing time.

V. DISCUSSION

A. Related work

Self-play RL has a long tradition in game learning, with
Tesauro’s TD-Gammon [27] being a very early TD-learning
application to Backgammon. The seminal papers of Silver et
al. on AlphaGo and AlphaZero [1], [3] lifted this for the games
Go, chess and shogi to a new complexity and performance
level. They have stirred the interest of many researchers to
achieve similar things with smaller hardware requirements
and/or fewer training cycles. Thakoor et al. [28] provided
in 2017 a general AlphaZero implementation in Python with
less computational demands than the original. But even their
architecture requires for 6x6 Othello 3 days of training on a
specialized cloud computing service (Google Compute Engine
with GPU support). Several works of Wang et al. [29], [30],
[23] focus on different aspects of the AlphaZero architecture:
alternative loss functions, hyperparameter tuning and warm-
start enhancements. They test these aspects on smaller games
like 6x6 Othello or 5x5 ConnectFour. The work of Chang et
al. [31] covered several AlphaZero improvements applied to
6x6 Othello. van der Ree and Wiering [12] investigate TD-,
Q- and Sarsa-learning for 8x8 Othello with a simple neural
network (one hidden layer with 50 neurons).

Dawson [24] introduces a CNN-based and AlphaZero-
inspired RL agent named ConnectZero for ConnectFour, which
can be played online and which reaches a good playing
strength against MCTS1000. Young et al. [32] report on
an AlphaZero implementation applied to ConnectFour. Here,
training took between 21 and 77 hours of GPU time.

Recently in 2022, Norelli and Panconesi [21] presented an
approach that is close to our work: They as well pursue the

7We used (cid:15)-greedy exploration during self-play and tuned (cid:15) on the W100
agent, where we found a very small (cid:15) (compatible with 0) to give the best
results.

goal to set up an AlphaZero-inspired algorithm at much lower
cost than the original AlphaZero [3]. The agent in [21] is based
on a residual DNN and is trained solely by self-play. It is able
to play 8x8 Othello and to defeat the strong Othello program
Edax [20] up to level 10. Although much less computationally
demanding than the original AlphaZero [3], their training time
took roughly one month on Colaboratory, a free Google cloud
computing service offering GPUs and TPUs. See Sec. V-C for
a direct comparison between Norelli and Panconesi [21] and
our work.

Apart from Norelli and Panconesi [21],

there are only
few works on Othello game learning that actually benchmark
against Edax: Liskowski et al. [33] presented in 2018 an agent
obtained by training a convolutional neural network (CNN)
with the help of a database of expert moves. Their agent could
defeat Edax up to and including level 2.

Our work presented here is based on an earlier Bachelor
thesis [34] published in 2020 (but only in German); it presents
an n-tuple RL agent for Othello trained in 1.8 hours on
standard hardware (no GPU) that can defeat Edax up to and
including level 7.

For the puzzle Rubik’s Cube,

the pioneering work of
McAleer [35] and Agostinelli [36] in 2018 and 2019 shows
that the 3x3x3 cube can be solved without putting human
knowledge or positional-pattern databases into the agent. They
solve arbitrary scrambled cubes with a method that is partly
inspired by AlphaZero but also contains special tricks for
Rubik’s Cube.

A work related to GBG [7], [8] is the general game system
Ludii [37]. Ludii is an efﬁcient general game system based on
a ludeme library implemented in Java, allowing to play as well
as to generate a large variety of strategy games. Currently, all
AI agents implemented in Ludii are tree-based agents (MCTS
variants or AlphaBeta). GBG, on the other hand, offers the
possibility to train RL-based algorithms on several games.

Soemers et al. [38] describe a bridge between Ludii [37]
and Polygames [39], the latter providing DNN algorithms for
strategy games. Similar to our work, they couple approximator
networks (DNNs) with MCTS, but for different games. With
20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB
of memory allocation per training job, their resource usage is
in a different dimension than our training process.

B. Related Work in N-Tuple Research

N-tuple networks, which are an important building block
of our approach, have shown to work well in many games,
e. g., in ConnectFour [4], [22], Othello [5], EinStein w¨urfelt
nicht (EWN) [40], 2048 [6], SZ-Tetris [41], etc. Other function
approximation networks (DNN or other) could be used as
well in AlphaZero-inspired RL, but n-tuple networks have the
advantage that they can be trained very fast on off-the-shelf
hardware.

There are two papers in the game learning literature that
combine n-tuple networks with MCTS: Sironi et al. [42] use
the n-tuple bandit EA to automatically tune a self-adaptive

MCTS. This is an interesting approach but for a completely
different goal and not related to AlphaZero.

Chu et al. [40] use an n-tuple network as a guidance for
MCTS in the game EWN. They train an n-tuple network
via Monte-Carlo RL and incorporate the trained heuristic
with three different approaches into MCTS. Monte Carlo is
a special form of temporal difference (TD) learning, namely
TD(1). To the best of our knowledge, our work is the ﬁrst
to couple n-tuple networks with MCTS using TD(λ)-training
with arbitrary λ.

Neither Chu et al. [40] nor we use MCTS in the training
loop for the main results. However, we compare in Sec. IV-E
– at least for some viable settings – MCTS within and outside
of self-play training. Finally, we use our approach for other
games than those studied in [40].

C. Comparison with Other RL Research

In this section, we compare our results with other RL

approaches from the literature.

Concerning the game ConnectFour,

it was shown in
Sec. IV-B that Dawson’s ConnectZero lost most or all of its
episodes when starting against AlphaBeta-DL or TCL-wrap.
This is in contrast to our TCL-base and TCL-wrap, which win
nearly all episodes when starting against AlphaBeta-DL (see
Tab. II).

Concerning the game Othello, there are a number of other
researchers that do RL-based game learning: van der Ree and
Wiering [12] reached in 2013 with their Q-learning agent
against the heuristic player BENCH (positional player) a win
rate of 87%. We reach with both TCL-base and TCL-wrap
a win rate of 100% against BENCH. Liskowski et al. [33]
show in Table IX that their agent wins against Edax up to
and including Edax level 2. We win up to and including Edax
level 7.

In 2022, Norelli and Panconesi [21] obtained with their
system OLIVAW the best Othello results up-to-date: It defeats
Edax up to and including Edax level 10. This is a truly
impressive result, but it also took considerable computational
resources to achieve it: Although much cheaper than Deep-
Mind’s original AlphaZero, they needed an informal crowd
computing project with 19 people for game generation and
then about 30 days to train a single agent on Google Colabo-
ratory using GPU and TPU hardware (50,000 training episodes
with 100-400 MCTS iterations). Due to the large training time,
ﬁne-tuning of hyperparameters or ablation studies could not be
undertaken.

In our work presented here, we defeat Edax only up to
level 7, but with a much simpler architecture that is trainable
in less than 2 hours on a single standard CPU.8 It is, on the
one hand, interesting that our architecture, which keeps the
costly MCTS completely out of the training process, can get
so far.

On the other hand, there is of course a performance gap
to [21], and it would be interesting to investigate which

8level 8 with MCTS in the training loop (Fig. 10) and 60 hours training

element of the more complex architecture in [21] is respon-
sible for the performance gain. We see here two hypothetical
candidates: First, including MCTS in the training phase leads
to better positional material in the replay buffer. Second, the
network architecture of OLIVAW uses a Residual Network, a
somewhat reduced version of the original AlphaZero Residual
Network, but still a deeper architecture than our n-tuple
network.

Concerning the puzzle Rubik’s Cube, the deep network used
by McAleer [35] and Agostinelli [36] had over 12 million
weights and was trained for 44 hours on a 32-core server with
3 GPUs. Our approach with much less computational effort
can solve the 2x2x2 cube completely, but the 3x3x3 cube only
partly.

VI. CONCLUSION AND FUTURE WORK
We have shown on the three challenging games, Othello,
ConnectFour, and Rubik’s Cube, that an AlphaZero-inspired
MCTS planning stage boosts the performance of TD-n-tuple
networks. Interestingly, this performance boost is even reached
when MCTS is not part of the training stage, which leads
to very large reductions in training times and computational
resources.

The new architecture was evaluated on the three games
without any game-speciﬁc changes. We reach near-perfect play
for ConnectFour and 2x2x2 Rubik’s Cube. For the games
Othello and 3x3x3 Rubik’s Cube, we observe good results
and increased performance compared to our version without
MCTS planning stage, but we do not reach the high-quality
results of Norelli and Panconesi [21] on Othello (beats Edax
level 10 where we reach only level 7) and of Agostinelli,
McAleer et al. [35], [36] on 3x3x3 Rubik’s Cube (they solve
all scrambled cubes while we solve only cubes with up to 9
twists). Both high-performing approaches require considerably
more computational resources.

We compared the effects of using MCTS within or outside
of self-play training. For Othello and n-tuple networks, we
have found that
large increases in computation time are
accompanied by no or only small increases in performance.

It is an interesting topic of future research to investigate
which element of the more complex architecture (MCTS in the
training phase or deep residual network for the approximator)
is more relevant to reach the impressive high-quality results of
others. However, for moderately complex games, our smaller
architecture has the advantage of allowing faster training and
more parameter tuning with simpler hardware that is accessible
to anyone.

We also plan to extend our MCTS wrapper concept to non-
deterministic games (e. g., EWN, 2048, Blackjack, Poker)
where previous research [43] has shown that plain MCTS
is not sufﬁcient and has to be extended by the Expectimax
approach.

ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their insightful comments that helped a lot to shape and
improve this work.

REFERENCES

[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of Go with deep neural networks
and tree search,” Nature, vol. 529, no. 7587, pp. 484–489, 2016. 1, 2, 9
[2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering
the game of Go without human knowledge,” Nature, vol. 550, no. 7676,
pp. 354–359, 2017. 1, 2

[3] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,
M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., “Mastering chess
and shogi by self-play with a general reinforcement learning algorithm,”
arXiv preprint arXiv:1712.01815, 2017. 1, 9

[4] S. Bagheri, M. Thill, P. Koch, and W. Konen, “Online adaptable learning
rates for the game Connect-4,” IEEE Transactions on Computational
Intelligence and AI in Games, vol. 8, no. 1, pp. 33–42, 2015. 1, 2, 4, 9
[5] S. M. Lucas, “Learning to play Othello with n-tuple systems,” Australian
Journal of Intelligent Information Processing, vol. 4, pp. 1–20, 2008. 1,
3, 5, 9

[6] M. Szubert and W. Ja´skowski, “Temporal difference learning of n-tuple
networks for the game 2048,” in Computational Intelligence and Games
(CIG), 2014 IEEE Conference on.

IEEE, 2014, pp. 1–8. 1, 9

[7] W. Konen, “General board game playing for education and research
in generic AI game learning,” in Conference on Games (London),
D. Perez, S. Mostaghim, and S. Lucas, Eds., 2019, pp. 1–8. [Online].
Available: https://arxiv.org/pdf/1907.06508 1, 9

[8] ——, “The GBG class interface tutorial V2.3: General board game
playing and learning,” TH K¨oln, Tech. Rep., 2022, retrieved Dec-
29-2021. [Online]. Available: http://www.gm.fh-koeln.de/ciopwebpub/
Konen22a.d/TR-GBG.pdf 1, 5, 9

[9] W. Konen and S. Bagheri, “Reinforcement learning for n-player games:
The importance of ﬁnal adaptation,” in International Conference on
Bioinspired Methods and Their Applications. Springer, 2020, pp. 84–
96. 2, 3, 4

[10] ——, “Final adaptation reinforcement learning for n-player games,”
[Online]. Available: https:

arXiv preprint arXiv:2111.14375, 2021.
//arxiv.org/abs/2111.14375 2, 3, 4

[11] W. Ja´skowski, “Mastering 2048 with delayed temporal coherence learn-
ing, multistage weight promotion, redundant encoding, and carousel
shaping,” IEEE Trans. on Games, vol. 10, pp. 3–14, 2018. 2, 4
[12] M. van der Ree and M. Wiering, “Reinforcement learning in the game
of Othello: Learning against a ﬁxed opponent and learning from self-
play.” in Adaptive Dynamic Programming and Reinforcement Learning
(ADPRL), 2013, pp. 108–115. 2, 3, 9, 10
learning

games: The
algorithm,” TH K¨oln, Tech. Rep., 2015.
temporal difference
[Online]. Available: http://www.gm.fh-koeln.de/ciopwebpub/Kone15c.
d/TR-TDgame EN.pdf 2

[13] W. Konen,

“Reinforcement

board

for

[14] D. F. Beal and M. C. Smith, “Temporal coherence and prediction decay
in TD learning,” in Int. Joint Conf. on Artiﬁcial Intelligence (IJCAI),
T. Dean, Ed. Morgan Kaufmann, 1999, pp. 564–569. 2, 4

[15] L. Kocsis and C. Szepesv´ari, “Bandit based Monte-Carlo planning,” in
Proceedings of the 17th European Conference on Machine Learning,
ser. ECML’06. Berlin, Heidelberg: Springer-Verlag, 2006, p. 282–293.
[Online]. Available: https://doi.org/10.1007/11871842 29 2

[16] C. B. Browne, E. Powley et al., “A survey of Monte Carlo tree search
methods,” IEEE Transactions on Computational Intelligence and AI in
Games, vol. 4, no. 1, pp. 1–43, 2012. 2

[17] C. D. Rosin, “Multi-armed bandits with episode context,” Annals
of Mathematics and Artiﬁcial Intelligence, vol. 61, no. 3, pp. 203–
230, Mar. 2011. [Online]. Available: http://link.springer.com/10.1007/
s10472-011-9258-6 2

[18] W. W. Bledsoe and I. Browning, “Pattern recognition and reading by
machine,” in Proceedings of the Eastern Joint Computer Conference,
1959, pp. 225–232. 3

[19] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.

Cambridge, MA: MIT Press, 1998. 3, 4

[20] R. Delorme, “Edax, version 4.4,” 2019, retrieved Dec-29-2021. [Online].

Available: https://github.com/abulmo/edax-reversi 4, 5, 9

[21] A. Norelli and A. Panconesi, “OLIVAW: Mastering Othello without
human knowledge, nor a penny,” IEEE Transactions on Games, 2022.
4, 9, 10

[22] M. Thill, S. Bagheri, P. Koch, and W. Konen, “Temporal difference
learning with eligibility traces for the game Connect-4,” in Interna-
tional Conf on Computational Intelligence in Games (CIG), Dortmund,
M. Preuss and G. Rudolph, Eds., 2014, pp. 1–8. 4, 6, 9

[23] H. Wang, M. Preuss, and A. Plaat, “Warm-start AlphaZero self-play
search enhancements,” in International Conference on Parallel Problem
Solving from Nature. Springer, 2020, pp. 528–542. 5, 9

[24] R. Dawson, “Learning to play Connect-4 with deep reinforcement
[Online]. Available: https:

retrieved Dec-29-2021.

learning,” 2020,
//codebox.net/pages/connect4 6, 9

[25] T. Rokicki, H. Kociemba, M. Davidson, and J. Dethridge, “The diameter
of the Rubik’s Cube group is twenty,” SIAM Review, vol. 56, no. 4, pp.
645–670, 2014. 6

[26] W. Konen, “Towards learning Rubik’s Cube with n-tuple-based rein-
forcement learning,” TH K¨oln (in preparation), Tech. Rep., 2022. 6
[27] G. Tesauro, “TD-Gammon, a self-teaching backgammon program,
achieves master-level play,” Neural computation, vol. 6, no. 2, pp. 215–
219, 1994. 9

[28] S. Thakoor, S. Nair, and M. Jhunjhunwala, “Learning to play Othello
without human knowledge,” 2017, retrieved Dec-28-2021. [Online].
Available: https://github.com/suragnair/alpha-zero-general 9

[29] H. Wang, M. Emmerich, M. Preuss, and A. Plaat, “Alternative loss
functions in AlphaZero-like self-play,” in 2019 IEEE Symposium Series
on Computational Intelligence (SSCI).

IEEE, 2019, pp. 155–162. 9

[30] ——, “Hyper-parameter sweep on AlphaZero general,” arXiv preprint
arXiv:1903.08129, 2019. [Online]. Available: https://arxiv.org/pdf/1903.
08129 9

[31] N.-Y. Chang, C.-H. Chen, S.-S. Lin, and S. Nair, “The big win strategy
on multi-value network: An improvement over AlphaZero approach for
6x6 Othello,” in Proceedings of the 2018 International Conference on
Machine Learning and Machine Intelligence, 2018, pp. 78–81. 9
[32] A. Young, A. Prasad, and I. Abrams, “Lessons from implementing
AlphaZero, part 6,” 2018, retrieved Dec-29-2021. [Online]. Available:
https://link.medium.com/ylGDD6F7V9 9

[33] P. Liskowski, W. Ja´skowski, and K. Krawiec, “Learning to play Othello
with deep neural networks,” IEEE Transactions on Games, vol. 10, no. 4,
pp. 354–364, 2018. 9, 10

[34] J. Scheiermann, “AlphaZero-inspirierte KI-Agenten im General Board
Game Playing,” 2020, Bachelor thesis, TH K¨oln – University of Applied
Sciences. [Online]. Available: http://www.gm.fh-koeln.de/ciopwebpub/
Scheier20a.d/Scheier2020.pdf 9

[35] S. McAleer, F. Agostinelli, A. Shmakov, and P. Baldi, “Solving the
Rubik’s Cube with approximate policy iteration,” in International Con-
ference on Learning Representations, 2019. 9, 10

[37]

[36] F. Agostinelli, S. McAleer, A. Shmakov, and P. Baldi, “Solving the
Rubik’s Cube with deep reinforcement learning and search,” Nature
Machine Intelligence, vol. 1, no. 8, pp. 356–363, 2019. 9, 10
´E. Piette, D.
J. Soemers, M. Stephenson, C. F. Sironi,
M. H. M. Winands, and C. Browne, “Ludii - the ludemic general
game system,” CoRR, vol. abs/1905.05013, 2019. [Online]. Available:
http://arxiv.org/abs/1905.05013 9

J. N.

[38] D. J. Soemers, V. Mella, C. Browne, and O. Teytaud, “Deep
learning for general game playing with Ludii and Polygames,”
arXiv preprint arXiv:2101.09562, 2021.
[Online]. Available: https:
//arxiv.org/pdf/2101.09562 9

[39] T. Cazenave, Y.-C. Chen, G.-W. Chen, S.-Y. Chen, X.-D. Chiu, J. Dehos,
M. Elsa, Q. Gong, H. Hu, V. Khalidov et al., “Polygames: Improved zero
learning,” ICGA Journal, vol. 42, no. 4, pp. 244–256, 2020. 9

[40] Y. R. Chu, Y. Chen, C. Hsueh, and I. Wu, “An agent for EinStein
W¨urfelt Nicht! using n-tuple networks,” in Conf. on Technologies and
Applications of AI (TAAI), Dec 2017, pp. 184–189. 9, 10

[41] W. Ja´skowski, M. Szubert, P. Liskowski, and K. Krawiec, “High-
dimensional function approximation for knowledge-free reinforcement
learning: A case study in SZ-Tetris,” in Conf. on Genetic and Evolu-
tionary Computation, 2015, pp. 567–573. 9

[42] C. F. Sironi, J. Liu, D. Perez-Liebana, R. D. Gaina, I. Bravi, S. M.
Lucas, and M. H. Winands, “Self-adaptive MCTS for general video
game playing,” in International Conference on the Applications of
Evolutionary Computation. Springer, 2018, pp. 358–375. 9

[43] J. Kutsch, “KI-Agenten f¨ur das Spiel 2048: Untersuchung von
Lernalgorithmen f¨ur nichtdeterministische Spiele,” 2017, Bachelor
thesis, TH K¨oln – University of Applied Sciences. [Online]. Available:
http://www.gm.fh-koeln.de/ciopwebpub/Kutsch17.d/Kutsch17.pdf 10

