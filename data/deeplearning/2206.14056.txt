2
2
0
2

n
u
J

8
2

]

G
L
.
s
c
[

1
v
6
5
0
4
1
.
6
0
2
2
:
v
i
X
r
a

Deep Neural Networks pruning via the Structured
Perspective Regularization

Matteo Cacciola
CERC, Polytechnique Montréal,
Montréal, QC, Canada,
matteo.cacciola@polymtl.ca

Antonio Frangioni
University of Pisa
Pisa, PI, Italy
frangio@di.unipi.it

Xinlin Li
Huawei Montreal Research Centre
Montreal, QC, Canada
xinlin.li1@huawei.co

Andrea Lodi
CERC, Polytechnique Montréal,
Montréal, QC, Canada,
and Jacobs Technion-Cornell Institute, Cornell Tech and Technion - IIT,
New York, NY, USA
andrea.lodi@cornell.edu

Abstract

In Machine Learning, Artiﬁcial Neural Networks (ANNs) are a very powerful
tool, broadly used in many applications. Often, the selected (deep) architectures
include many layers, and therefore a large amount of parameters, which makes
training, storage and inference expensive. This motivated a stream of research
about compressing the original networks into smaller ones without excessively
sacriﬁcing performances. Among the many proposed compression approaches,
one of the most popular is pruning, whereby entire elements of the ANN (links,
nodes, channels, . . . ) and the corresponding weights are deleted. Since the nature
of the problem is inherently combinatorial (what elements to prune and what not),
we propose a new pruning method based on Operational Research tools. We start
from a natural Mixed-Integer-Programming model for the problem, and we use
the Perspective Reformulation technique to strengthen its continuous relaxation.
Projecting away the indicator variables from this reformulation yields a new reg-
ularization term, which we call the Structured Perspective Regularization, that
leads to structured pruning of the initial architecture. We test our method on some
ResNet architectures applied to CIFAR-10, CIFAR-100 and ImageNet datasets,
obtaining competitive performances w.r.t. the state of the art for structured prun-
ing.

1 Introduction

The striking practical success of Artiﬁcial Neural Networks (ANN) has been initially driven by
the ability of adding more and more parameters to the models, which has led to vastly increased
accuracy. This brute-force approach, however, has numerous drawbacks: besides the ever-present
risk of overﬁtting, massive models are costly to store and run. This clashes with the ever increasing
push towards edge computing of ANN, whereby neural models have to be run on low power devices

Preprint. Under review.

 
 
 
 
 
 
such as smart phones, smart watches, and wireless base stations [15; 27; 23]. While one may just
resort to smaller models, the fact that a large mode trained even for a few epochs performs better
than smaller ones trained for much longer lends credence to the claim [25] that the best strategy is
to initially train large and over-parameterized models and then shrink them through techniques such
as pruning and low-bit quantization.

Loosely speaking, pruning requires ﬁnding the best compromise between removing some of the
elements of the ANN (weights, channels, ﬁlters, layers, blocks, . . . ) and the decrease in accuracy
that this could bring [19; 16; 13]. Pruning can be performed while training or after training. The
advantage of the latter is the ability of using standard training techniques un-modiﬁed, which may
lead to better peformances. On the other hand, pruning while training automatically adapts the
values of the weights to the new architecture, dispensing with the need to re-train the pruned ANN.

A relevant aspect of the process is the choice of the elements to be pruned. Owing to the fact that
both ANN training and inference is nowadays mostly GPU-based, pruning an individual weight
may yield little to no beneﬁt in case other weights in the same “computational block” are retained,
as the vector processing nature of GPUs may not be able to exploit un-structured forms of sparsity.
Therefore, in order to be effective pruning has to be achieved simultaneously on all the weights of a
given element, like a channel or a ﬁlter, so that the element can be deleted entirely. The choice of the
elements to be pruned therefore depends on the target ANN architecture, an issue that has not been
very clearly discussed in the literature so far. This motivates a speciﬁc feature of our development
whereby we allow to arbitrarily partition the weight vector and measure the sparsity in terms of the
number of partitions that are eliminated, as opposed to just the number of weights.

In this work, we develop a novel method to perform structured pruning during training through the in-
troduction of a Structured Perspective Regularization (SPR) term. More speciﬁcally, we start from
a natural exact Mixed-Integer Programming (MIP) model of the sparsity-aware training problem
where we consider, in addition to the loss and ℓ2 regularization, also the ℓ0 norm of the structured
set of weights. A novel application of the Perspective Reformulation technique leads to a tighter
continuous relaxation of the original MIP model and ultimately to the deﬁnition of the SPR term.
Our approach is therefore principled, being grounded on an exact model rather than based on heuris-
tic score functions to decide what entities to prune as prevalent in the literature so far. It is also
ﬂexible as it can be adapted to any kind of structured pruning, provided that the prunable entities
are known before the training starts, and the ﬁnal expected amount of pruning is controlled by the
hyper-parameter providing the weight of the ℓ0 term in the original MIP model. While our approach
currently only solves a relaxation of the integer problem, it would clearly be possible to exploit es-
tablished Operations Research techniques to improve on the quality of the solution, and therefore of
the pruning. Yet, the experimental results show that our approach is already competitive with, and
often signiﬁcantly better than, the state of the art. Furthermore, since we perform pruning during
training by just changing the regularization term our approach can use standard training techniques
and its cost is not signiﬁcantly higher than the usual training without sparsiﬁcation.

2 Related works

The ﬁeld of pruning is experiencing a growing interest in the Machine Learning (ML) community,
starting from the seminal work [14] that obtained unexpectedly good results from a trivial magnitude-
based approach. The same magnitude-based approach was extended in [12] with a re-training phase
where the non-pruned weight are re-initialised to their starting values.

From the structured pruning side, a possibility is adding to the network parameters a scaling factor
for each prunable entity, multiplying all the corresponding parameters; then, sparsity is enforced
by adding the ℓ1 norm of the scaling factors vector.
In [28] a pruning mask is deﬁned, i.e., a
differentiable approximation of a thresholding function that pushes the scaling factors to 0 when
they are lower than a ﬁxed threshold, avoiding numerical issues.

Most of the recently-published state-of-the-art works for structured pruning, such as [32], [22] and
[4], still make use of very heuristic approaches to compute the importance of an element of the
ANN (often its l2 norm). This is usually sub-optimal and our approach is seeking improvement on
the theoretical justiﬁcation side.

2

MIP techniques have been successfully used in the ANN context, but mostly in applications un-
related to pruning like construction of adversarial examples (with ﬁxed weights) [8]. In [2], the
approach is extended to a larger class of activation functions and stronger formulations are deﬁned.
An exception is [6], where a score function is deﬁned to asses the importance of a neuron and then
a MIP is used to minimize the number of neurons that need to be kept at each layer to avoid large
accuracy drops. In [29] a MIP is used ﬁrst to derive bounds on the output of each neuron, which
is then used in another MIP model of the entire network to ﬁnd equivalent networks, local approx-
imations and global linear approximations with less neurons of the original network. Since MIPs
are N P-hard, these techniques may have difﬁculties to scale to large ANNs. Indeed, the pruning
method developed in [1] rather solves a simpler convex program for each layer to identify prunable
entities in such a way that the inputs and outputs of the layer are still consistent with the original one.
This layer-wise approach does not take into account the whole network at once as our own does.

The link between Perspective Reformulation techniques and sparsiﬁcation has been recently recog-
nised [5; 3], but typically in the context of regression problems that are much simpler than ANNs.
In particular, all the above papers count (the equivalent of) each weight individually, and therefore
they do not consider structured pruning of sets of related weights as it is required for ANNs. Fur-
thermore, the sparsiﬁcation approach is applied to input variables selection in settings that typically
have orders of magnitude fewer elements to be sparsiﬁed than the present one.

3 Mathematical model

We are given a dataset X, an ANN model architecture whose set of parameters W = { wj | j ∈ I }
is partitioned into disjoint subsets { Ei }i∈N , that we call prunable entities, such that W = ∪i∈N Ei,
and a loss function L(·). If the value of a parameter wj is zero it could be eliminated from the model
(pruned) but, for the reasons discussed above, we are only interested in pruning the entities Ei,
which is possible only if wj = 0 for all j ∈ Ei. We should consider a three-objective optimization
problem where we: i) minimize the loss, ii) minimize some standard regularization term aiming at
improving the model’s generalization capabilities, and iii) maximize the number of pruned entities
Ei. As customary in this setting, we approach this by scaling the three objective functions by means
of hyperparameters whose optimal values are found by standard grid-search techniques.

3.1 Method

Starting from the usual ML framework with ℓ2 regularization, we consider the following MIP

min L(X, W ) + λ[ αk W k2

− M yi ≤ wj ≤ M yi
yi ∈ {0, 1}

2 + (1 − α)
wj ∈ Ei
i ∈ N

P

i∈N yi ]
i ∈ N

(1)

(2)
(3)

where α ∈ [ 0 , 1 ] and λ > 0 are scalar hyper-parameters while M is an upper bound on the absolute
value of the parameters. The binary variable yi is 0 if the corresponding prunable entity is pruned,
1 if it is not. The standard “big-M” constraints (2) ensure that if yi = 0 then 0 ≤ wj ≤ 0 for all
parameters in the entity Ei, while if yi = 1 the parameters can take any possible useful value (since
i∈N yi” represents the ℓ0 norm of the structured set of
M is an upper bound). Hence, the term “
weights, i.e.,

P

1 if ∃j ∈ Ei s.t. wj 6= 0
0 otherwise.

i∈N (cid:26)
X

In general, solving (1)–(3) directly is not computationally efﬁcient, and therefore a standard strategy
is to consider its continuous relaxation whereby (3) is relaxed to yi ∈ [ 0 , 1 ]. If L(·) is convex, as in
our case, this is easily solvable but its solution in both the y and w variables, which is what one could
use to perform the pruning, can be rather different from the optimal solution of (1)–(3), therefore
leading to inefﬁcient prunings. To improve on that, we consider the Perspective Reformulation [9]
of (1)–(3)

min L(X, W ) + λ

(2) , (3)

h

α
i∈N,j∈Ei
X

+ (1 − α)

yi

i∈N
X

i

(4)

w2
j
yi

3

It can be shown that (4) has the same integer optimal solution as the original problem, but its con-
tinuous relaxation (the Perspective Relaxation) is “better” in a well-deﬁned mathematical sense: its
optimal objective value is (much) closer to the true optimal value of (1)–(3), which typically implies
that its optimal solution is more similar to the true optimal solution.

3.2 Eliminating the y variables

While one can expect that the solution of the relaxation of (4) can provide a better guide to the
pruning procedure, its form makes it more difﬁcult to apply standard training techniques to solve it.
Following the lead of [10; 11], we then proceed at simplifying the model by projecting away the y
variables. This amounts to computing a closed formula ˜y(w) for the optimal value of the y variables
in the continuous relaxation of (4) assuming that w are the optimal weights. When w is ﬁxed, the
problem decomposes over the subsets, and therefore we only need to consider each fragment

fi(W, yi) = λ

α

j∈Ei w2

j /yi + (1 − α)yi

P
separately. Since fi is convex in yi if yi > 0, we just need to ﬁnd the root of the derivative

(cid:2)

(cid:3)

∂fi(W, yi)
∂yi

= λ

w2
j
y2
i

−α

wj ∈Ei
X





that is

+ (1 − α)



= 0 ,



w2
j

α

yi =

s

wj ∈Ei
1 − α

P
(we are only interested in positive y), and then project it on the domain. Note that, technically,
fi(W, yi) is nondifferentiable for yi = 0 but that value is only achieved when wj = 0 for all j ∈ Ei,
in which case the choice is obviously optimal. The constraints that deﬁnes the domain of yi can be
rewritten as yi ≥ | wj |/M for all j ∈ Ei, together with yi ∈ [0, 1]; putting everything together, we
obtain

˜yi(w)

=

min

max
j∈Ei

(

| wj |
M

,

w2
j
1 − α

,

1

,

)

α

v
u
u
t

j∈Ei
X

where we note that we do not need to enforce positivity since all the quantities are positive. All in
all, we can rewrite the continuous relaxation of (4) as

min

L(X, W ) + λ

N
i=1 zi(wi; α, M )

,

(5)

P
where wi = [ wj ]j∈Ei and, applying some algebraic simpliﬁcations, zi(wi; α, M ) has the closed
formula

(cid:9)

(cid:8)

1−α
α (1 + α)||wi||2




M

||wi||∞ ||wi||2
q
||wi||2

2 + (1 − α)

2 + (1 − α) ||wi||∞

M

α

if ||wi||∞
M ≤
1−α ||wi||2 ≤ ||wi||∞
α
q

1−α ||wi||2 ≤ 1
M ≤ 1

if

otherwise.

q



We call zi(wi; α, M ) the Structured Perspective Regularization (SPR) w.r.t. the structure speciﬁed
by the sets Ei. It is easily seen that the SPR behaves like the ordinary ℓ2 regularization in parts of the
space but it is signiﬁcantly different in others. Due to being derived from (4), we can expect (with a
proper choice of the hyperparameters) the SPR to promote sparsity—in terms of the sets Ej—better
than the ℓ2 norm. Indeed, SPR for i ∈ I depends on the ℓ∞ norm of wi, which means that it is zero
only if all the components of wi are zero. In other words, while using the, say, ordinary ℓ1 would
promote sparsity on each weight individually, the SPR can be expected to better promote structured
sparsity as required by our application. However, the solution of (5) can be sought for with all of
the usual algorithms for training ANNs (SGD, Adam, etc.), and therefore should not, in principle,
be more costly then non-sparsity-inducing training.

4

3.3 Minor improvements

Remarkably, the SPR depends on the choice of M , which is, in principle, nontrivial. Indeed, all
previous attempts of using PR techniques for promoting sparsity [5; 3] have been using the “abstract”
nonlinear form (1 − yi)wi = 0 of (2) (assuming Ei = { i } as in those treatments). This still yields
the same Perspective Reformulation but it is not conducive to projecting away the y variables as
required by our approach. While M could in principle be treated as another hyperparameter, in a
(deep) ANN, different layers can have rather different optimal upper bounds on the weights; hence,
using a single constant M for all the prunable entities is sub-optimal. The ideal choice would be to
compute one constant Mi for each entity Ei; however, entities in the same layer are often similar to
each other, so we decided to compute a different constant for each layer of the network and then use
it for all entities belonging to that layer.

Furthermore, all the development so far has assumed that all prunable entities Ei are equally impor-
tant. However, this may not be true, since different entities can have different number of parameters
and therefore impact differently on the overall memory and computational cost. To take this feature
into account, we modify our regularization terms as follows:

ui
i∈N ui

zi(wi; α, M ),

λ

i∈N
X

P

where ui is the number of parameters belonging to entity Ei.

Finally, we perform a ﬁne-tuning phase. After the ANN has been trained with the SPR, we perform
the pruning (eliminating all the entities where ||wi||∞ is smaller than a given tolerance (see below)
and then we re-train the pruned network with the standard ℓ2 regularization, starting from the value
of the weights (for the non-pruned entities) obtained at the end of the previous phase rather than
re-initializing them.

4 Experiments

We tested our method on the task of ﬁlter pruning in Deep Convolutional Neural Networks; that is,
the prunable entities are the ﬁlters of the convolutional layers. More speciﬁcally, the weights in a
convolutional layer with ninp input channels, nout output channels and k × k kernels is a tensor
with four dimensions (ninp, nout, k, k): our prunable entities correspond to the sub-tensors with the
second coordinate ﬁxed, and therefore have ninp × k × k parameters.

code
used
repository

The
lic
https://github.com/pytorch/examples/tree/master/imagenet.

experiments was written

https://github.com/akamaster/pytorch_resnet_cifar10

starting

from the

run

the

to

pub-
and

4.1 Datasets, architectures and general setup

For our experiments, we used 3 very popular datasets: CIFAR-10 [20], CIFAR-100 [20] and Im-
ageNet [21]. As architectures, we focused on ResNet [17] and Vgg [31], in particular, we used
ResNet-18, ResNet-20, Resnet50, ResNet-56 and Vgg-16 for CIFAR datasets and ResNet-18 for
the ImageNet dataset.

For all the experiments, we used Pytorch (1.7.1 and 1.8.1) with Cuda, the CrossEntropyLoss and the
SGD optimizer with 0.9 momentum.

After the ﬁrst phase, we considered as pruned all weights with absolute value lower than 1e-4 and
we decided to prune all entities with more than 99% of parameters pruned.

To get the values of Mi’s, we used the maximum absolute values of the weights for each layer of a
network with the same architecture but trained without our regularization term (for ResNet-20 and
ResNet-56 we trained it, for ResNet-18 we used the pretrained version available from torchvision).

More results ant tables are reported in the appendix to show some ablation studies, additional details.

5

Table 1: Results of our algorithm on CIFAR-10 using ResNet-20

L-rate

0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1

λ

1.1
1.1
1.7
0.8
1.1
0.5
0.5
0.2

α

Acc.

Pruned pars (%)

0.01
1e-4
0.3
0.1
0.3
0.01
0.3
0.3

85.56
86.35
87.33
88.14
89.47
90.06
91.23
92.69

92.03

231597
227394
217233
206694
199809
187767
140535
64188

(85.78)
(84.22)
(80.46)
(76.55)
(74.00)
(69.54)
(52.05)
(23.77)

0

(0.00)

Original model

Table 2: Results of our algorithm on CIFAR-100 using ResNet-20.

L-rate

0.01
0.01
0.01
0.01
0.01

λ

1.3
1.0
1.6
1.3
0.7

α

Acc.

Pruned pars (%)

1e-3
0.1
0.3
0.3
0.3

65.28
66.28
66.64
68.36
69.20

68.55

149184
123984
104256
84960
42480

(55.25)
(45.92)
(38.61)
(31.47)
(15.73)

0

(0.00)

Original model

4.2 Results on CIFAR-10 and CIFAR-100

These experiments were performed on a single GPU, either a TESLA V100 32GB or NVIDIA
Ampere A100 40GB. The model was trained for 300 epochs and then ﬁne tuned for 200 ones. The
dataset was normalized, then we performed data augmentation through random crop and horizontal
ﬂip. Mini batches of size 128 (64 for CIFAR-100) were used for training. The learning rate was
initialized to 1e-1 or 1e-2 and then it was divided by 10 at epochs 120, 200, 230, 250, 350, 400 and
450.

As shown in Table 1, training ResNet-20 on CIFAR-10, we were able to prune more than 23% of
the parameters by still increasing the accuracy of the original model, while we could prune almost
70% of the model by still preserving more than 90% accuracy.

Using ResNet-20 on the CIFAR-100 dataset, we could again prune more than 15% of parameters
while improving the accuracy of the original model (Table 2). Due to the fact that CIFAR-100 is
more challenging than CIFAR-10, it was not possible to prune very many parameters without losing
a signiﬁcant amount of accuracy: we could still achieve more than 68% accuracy by pruning a few
more than 30% of the parameters, but accuracy dropped to less than 67% if pruning more.

Table 3 reports results on training the ResNet-56 architecture on CIFAR-10: once again pruning
about 30% of the parameters improved accuracy and we could keep more than 93% accuracy while
pruning more than a half of the network.

Table 3: Results of our algorithm on CIFAR-10 using ResNet-56

L-rate

0.1
0.1
0.1
0.01
0.01

λ

1.7
1.1
0.5
1.1
0.8

1e-4
0.01
0.01
1e-4
0.1

Original model

α

Acc.

Pruned pars (%)

756882
720135
633960
460800
268128

(89.04)
(84.72)
(74.58)
(54.21)
(31.54)

0

(0.00)

90.12
91.35
92.55
93.13
93.98

93.35

6

Finally, note that models Resnet-18, ResNet-50, and Vgg-16 have a number of parameters much
higher than the ones seen so far. Interestingly, when we employed these models on the Cifar10
dataset, we were able to prune the majority of the parameters (from 81% to more than 90%) without
really affecting the accuracy of the ANN, sometimes even increasing it. This is shown in Tables 4,
5 and 6.

4.3 Results on ImageNet

These experiments were performed on single TITAN V 8GB GPU. The model was trained for 150
epochs and ﬁne tuned for 50 ones. The preprocessing was the same as for the CIFAR datasets.
We used mini batches of 256 and 0.1 learning rate that was divided by 10 every 35 epochs. On
the ImageNet tests, as usual for datasets with so many classes, we decided to report also the top5
accuracy, that is the percentage of samples where the correct label was on the 5 higher scored classes
by the model.

Table 4: Results of our algorithm on CIFAR-
10 using ResNet-18

Table 5: Results of our algorithm on CIFAR-
10 using ResNet-50

α

Acc.

Pruned pars (%)

L-rate

α

Acc.

Pruned pars (%)

L-rate

0.1
0.1
0.1
0.1

λ

0.8
1.0
1.4
1.8

Original model

0.3
0.1
0.01
0.01

95.24
95.10
94.66
93.76

95.15

9125688
9712787
10008403
10176969

(81.40)
(86.64)
(89.28)
(90.78)

0

(0.00)

λ

1.4
1.8
2.0
2.2
2.8

0.1
0.1
0.1
0.1
0.1

Original model

0.3
1e-4
0.001
0.01
0.001

94.90
94.80
94.63
94.30
93.72

94.83

20428879
21041527
21332414
21636397
21964741

(86.37)
(88.96)
(90.19)
(91.48)
(92.87)

0

(0.00)

Table 6: Results of our algorithm on CIFAR-
10 using Vgg-16

Table 7: Results of our algorithm on Ima-
geNet using ResNet-18

L-rate

0.1
0.1
0.1

λ

1.4
1.8
1.8

0.1
0.1
1e-4

Original model

α

Acc.

Pruned pars (%)

93.53
93.23
93.11

94.12

13564323
13722777
13796586

(92.18)
(93.25)
(93.75)

0

(0.00)

L-rate

0.1
0.1
0.1

λ

0.5
1.3
1.3

α

top1

top5

Pruned pars (%)

0.3
0.3
0.01

70.89
67.26
62.27

89.79
87.71
84.43

652644
5796625
8607247

(6.11)
(54.33)
(80.67)

Original model

69.76

89.08

0

(0.00)

Results on ImageNet using ResNet-18 are reported in Table 7 and show that even in a very large and
difﬁcult dataset our method was able to improve the original model results by a consistent margin,
reaching almost 71% accuracy while pruning more than 6% of the parameters. Pruning more than
50% of the network caused a drop of 2.5% in the accuracy, while a way more consistent decrease
happened when we pruned about 80% of the parameters.

4.4 Comparison with state-of-the-art methods

In this section, we compare our results (denoted as SPR) with some of the state-of-the-art algorithms
for structured pruning. We report results from [18] (denoted by SSS), [30] (denoted by EPFS), [33]
(denoted by L2PF), [24] (denoted by PFFEC), [26] (denoted as HRANK), [34] (denoted as PFC),
[32] (denoted by CHIP), [22] (denoted as DNR), [4] (denoted as OTO) and [7] (denoted by HFP).

Since not all the above papers reported the results for all our metrics (for example, some works only
reported the percentage of parameters pruned), in some cases we had to do some conversions that
naturally came with some mild approximation. Moreover, in [18], only plots were presented, so we
had to approximately deduce the data from some points of the ﬁgures (ﬁgure 2(a) and ﬁgure 2(c) of
[18], we denote the points as P1, P2, etc.). For ImageNet the top5 accuracy is not reported in [7], so
we marked the corresponding ﬁeld in our table with a “N/A". Finally, we report results for different
settings of each method as they were reported in the original papers; however, it should be remarked
that not all of them are ﬁlter pruning methods, some rather being general structured pruning ones.

Regarding ResNet-20 on CIFAR-10, our results in Table 8 outperform the other methods in most of
the cases, meaning that we could reach equal or better accuracy while pruning a larger amount of
parameters. Whenever we do not outperform the other methods we have comparable performances

7

Table 8: Results of state of the art method on CIFAR-10 using ResNet-20.

Method

Setting

Acc.

Pruned pars (%)

SSS

EPFS

L2PF

PFC

SPR

P1
P2
P3
P4

B-0.6
B-0.8
F-0,05
C-0.6-0.05

90.80
91.60
92.00
92.50

91.91
91.50
90.83
90.98

120000
40000
10000
0

70000
100000
130000
150000

(44.44)
(14.81)
(3.70)
(0.00)

(24.60)
(36.90)
(51.10)
(56.00)

LW 89.90

199687

(73.96)

P1

90.55

135000

(50.00)

λ1.1-α0.3
λ0.5-α0.3
λ0.2-α0.3

89.47
91.23
92.69

199809
140535
64188

(74.00)
(52.05)
(23.77)

Table 9: Results of state of the art method on CIFAR-100 using ResNet-20.

Method

Setting

Acc.

Pruned pars (%)

SSS

SPR

P1
P2
P3
P4

λ1.0-α0.1
λ1.3-α0.3
λ0.7-α0.3

65.50
67.10
68.10
69.20

66.28
68.36
69.20

120000
40000
10000
0

123984
84960
42480

(44.44)
(14.81)
(3.70)
(0.00)

(45.92)
(31.47)
(15.73)

and we can have a better accuracy or sparsity maintaining the other metric close. This is for instance
the case of L2PF that achieved 89.9% accuracy with 73.96% sparsity, while we achieved a little
higher sparsity (74.00%) losing a little accuracy (89.47%).

On CIFAR-100 using ResNet-20, the results in Table 9 clearly show that we outperform SSS, as
we could achieve more than 68.3% accuracy wile pruning more than 30% of parameters while SSS
could prune only 14.81% to obtain a little bit more than 67% accuracy. In Table 10, we can observe
a similar situation to ResNet-20 on CIFAR-10 for ResNet-56 on the same dataset. One of the few
results we did not outperform was the HPF 93.30 accuracy with 50% sparsity but we could obtain a
little bit more sparsity (54.21%) with almost the same accuracy (93.13%).

The results reported in Tables 11 and 12 show that our approach is very competitive with respect
to the very recent state-of-the-art methods such as OTO and DNR, sometimes being able improve
them signiﬁcantly. For example, DNR can only prune less than 82% of ResNet-18 achieving 94.64%
accuracy, while our method reach more than 95% accuracy pruning more than 86% of the network.

Similarly, when training Vgg-16 on Cifar-10, our method beats most of the state-of-the-art ones and
has always competitive results. For example, CHIP can never prune more than 88% of the ANN but
our algorithm prunes consistently more than 92% achieving similar or better accuracy (Table 13).

On ImageNet using ResNet-18, the results in Table 14 show that even if our method does not out-
perform the other ones, we were able to prune consistently more parameters without a very large
accuracy drop. Likely some additional parameter tuning could lead us to even more competitive
results.

8

Table 10: Results of state of the art method on CIFAR-10 using ResNet-56

Method

Setting

Acc.

Pruned pars (%)

PFFEC

EPFS

HFP

HRank

PFC

CHIP

SPR

A 93.10
B 93.06

80000
120000

240000
500000
170000
510000
570000

425000
608430

580000
360000
140000

(9.40)
(13.70)

(27.70)
(58.60)
(20.00)
(60.10)
(67.10)

(50.00)
(71.58)

(68.10)
(42.40)
(16.80)

92.89
92.34
92.96
92.09
92.53

93.30
92.31

90.72
93.17
93.52

93.05

425000

(50.00)

92.05
94.16

92.55
93.13
93.98

600000
360000

633960
460800
268128

(71.80)
(42.80)

(74.58)
(54.21)
(31.54)

B-0.6
B-0.8
F-0.01
F-0.05
C-0.6-0.05

0.5
0.7

P1
P2
P3

P1

P1
P2

λ0.5-α0.001
λ1.1-α1e-4
λ0.8-α0.1

Table 11: Results of state of the art method
on CIFAR-10 using ResNet-18

Table 12: Results of state of the art method
on CIFAR-10 using ResNet-50

Method

Setting

Acc.

Pruned pars (%)

Method

Setting

Acc.

Pruned pars (%)

DNR

SPR

λ-α0.8
λ-α1.0

P1

0.3
0.1

94.64

9233284

(82.36)

95.24
95.10

9125688
9712787

(81.40)
(86.64)

OTO

SPR

P1

94.40

21570653

(91.20)

λ2.0-α0.001
λ2.2-α0.01

94.63
94.30

21332414
21636397

(90.19)
(91.48)

5 Conclusions and future directions

Based on an exact MIP model for the problem of pruning ANNs, we proposed a new regularization
term, based on the projected Perspective Reformulation, designed to promote structured sparsity.
The proposed method is able to prune any kind of structures and the amount of pruning is tuned by
appropriate hyper-parameters. We tested our method on some classical datasets and architectures
and we compared the results with some of the state-of-the-art structured pruning methods. The
results have shown that our method is competitive.

These results are even more promising in view of the fact that further improvements should be
possible. Indeed, we are currently solving the continuous relaxation of our proposed exact starting
model, albeit a “tight” one due to the use of the Perspective Reformulation technique. By a tighter
integration with other well-established MIP techniques, further improvements are foreseeable.

Acknowledgments and Disclosure of Funding

This work has been supported by the NSERC Alliance grant 544900- 19 in collaboration with
Huawei-Canada

9

Table 13: Results of state of the art method on CIFAR-10 using Vgg-16

Method

Setting

Acc.

Pruned pars (%)

PFC

EPSF

PFEEC

HRANK

CHIP

DNR

OTO

SPR

P1

93.63

7357792

(50.00)

F-0.005
F-0.001

P1

P1
P2
P3

P1
P2
P3

P1

P1

94.67
93.61

93.40

93.43
92.34
91.23

93.86
93.72
93.18

10305584
8225584

(69.10)
(56.70)

9315584

(64.00)

12205584
12075584
12935584

11955584
12215584
12815584

(82.90)
(82.10)
(92.00)

(81.60)
(83.30)
(87.30)

92.00

1354863818

(92.07)

93.30

1390622688

(94.50)

λ1.4-α0.1
λ1.8-α0.1
λ1.8-α0.0001

93.53
93.23
93.11

13564323
13722777
13796586

(92.18)
(93.25)
(93.75)

Table 14: Results of state-of-the-art method on ImageNet using ResNet-18

Method

Setting

top1

top5

Pruned pars (%)

EPFS

F-0.05

67.81

88.37

3690000

(34.60)

HFP

0.20
0.35

69.15
68.53

N/A 2354869
N/A 3976709

(22.07)
(37.27)

SPR λ1.3-α0.3

67.26

87.71

5796625

(54.33)

References

[1] A. Aghasi, N. Nguyen, and J. Romberg. Net-trim: A layer-wise convex pruning of deep neural
networks. CoRR, abs/1611.05162, 2016. URL http://arxiv.org/abs/1611.05162.

[2] R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and J. P. Vielma. Strong mixed-integer

programming formulations for trained neural networks, 2020.

[3] A. Atamtürk and A. Gómez. Safe screening rules for ℓ0-regression, 2020.

[4] T. Chen, B. Ji, T. Ding, B. Fang, G. Wang, Z. Zhu, L. Liang, Y. Shi, S. Yi, and X. Tu. Only
train once: A one-shot neural network training and pruning framework. In NeurIPS, 2021.

[5] H. Dong, K. Chen, and J. Linderoth. Regularization vs. relaxation: A convexiﬁcation perspec-

tive of statistical variable selection, 2015.

[6] M. ElAraby, G. Wolf, and M. Carvalho.

chitectures using mixed integer programming.
https://arxiv.org/abs/2002.07259.

Identifying critical neurons in ANN ar-
URL

CoRR, abs/2002.07259, 2020.

[7] L. Enderich, F. Timm, and W. Burgard. Holistic ﬁlter pruning for efﬁcient deep neural net-

works. CoRR, abs/2009.08169, 2020. URL https://arxiv.org/abs/2009.08169.

[8] M. Fischetti and J. Jo. Deep neural networks as 0-1 mixed integer linear programs: A feasibility

study. CoRR, abs/1712.06174, 2017. URL http://arxiv.org/abs/1712.06174.

[9] A. Frangioni and C. Gentile. Perspective cuts for a class of convex 0–1 mixed integer programs.

Mathematical Programming, 106(2):225–236, 2006.

10

[10] A. Frangioni, C. Gentile, E. Grande, and A. Paciﬁci. Projected perspective reformulations with

applications in design problems. Operations Research, 59(5):1225–1232, 2011.

[11] A. Frangioni, F. Furini, and C. Gentile. Approximated perspective relaxations: a project&lift

approach. Computational Optimization and Applications, 63(3):705–735, 2016.

[12] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural net-

works. arXiv preprint arXiv:1803.03635, 2018.

[13] N. S. Guang-Bin Huang, P. Saratchandran. A Generalized Growing and Pruning RBF (GGAP-
IEEE TRANSACTIONS ON NEURAL

RBF) Neural Network for Function Approximation.
NETWORKS, 16(1):57–67, Jan 2005.

[14] S. Han and B. Dally. Efﬁcient methods and hardware for deep learning. University Lecture,

2017.

[15] S. Han,

J. Pool,

J. Tran, and W. Dally.

tions for efﬁcient neural network.
M. Sugiyama,
and R. Garnett,
cessing Systems 28,
http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf.

Learning both weights and connec-
In C. Cortes, N. D. Lawrence, D. D. Lee,
Information Pro-
in Neural
URL

pages 1135–1143. Curran Associates,

editors, Advances

2015.

Inc.,

[16] B. Hassibi and D. G. Stork.

Second order derivatives for network pruning: Optimal
In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in
brain surgeon.
Neural Information Processing Systems 5, pages 164–171. Morgan-Kaufmann, 1993. URL
http://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf.

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR,

abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.

[18] Z. Huang and N. Wang. Data-driven sparse structure selection for deep neural networks. CoRR,

abs/1707.01213, 2017. URL http://arxiv.org/abs/1707.01213.

[19] E. Karnin. Simple procedure for pruning back-propagation trained neural networks. Neural

Networks, IEEE Transactions on, 1:239 – 242, 07 1990. doi: 10.1109/72.80236.

[20] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, (cana-

dian institute for advanced research), 2009.

[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in Neural Information Processing Systems, 2012.

[22] S. Kundu, M. Nazemi, P. A. Beerel, and M. Pedram. Dnr: A tunable robust pruning framework
through dynamic network rewiring of dnns. In Proceedings of the 26th Asia and South Paciﬁc
Design Automation Conference, ASPDAC ’21, page 344–350, New York, NY, USA, 2021. As-
sociation for Computing Machinery. ISBN 9781450379991. doi: 10.1145/3394885.3431542.
URL https://doi.org/10.1145/3394885.3431542.

[23] C. Leng, H. Li, S. Zhu, and R. Jin. Extremely low bit neural network: Squeeze the last bit out

with ADMM. CoRR, abs/1707.09870, 2017. URL http://arxiv.org/abs/1707.09870.

[24] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning ﬁlters for efﬁcient convnets.

CoRR, abs/1608.08710, 2016. URL http://arxiv.org/abs/1608.08710.

[25] Z. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, and J. E. Gonzalez. Train large, then
compress: Rethinking model size for efﬁcient training and inference of transformers. CoRR,
abs/2002.11794, 2020. URL https://arxiv.org/abs/2002.11794.

[26] M. Lin, R. Ji, Y. Wang, Y. Zhang, B. Zhang, Y. Tian, and L. Shao. Hrank: Filter pruning using
high-rank feature map. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 1529–1538, 2020.

[27] A. Lodi, M. Toma, and R. Guerrieri. Very low complexity prompted speaker veriﬁcation
system based on hmm-modeling. Acoustics, Speech, and Signal Processing, 1988. ICASSP-
88., 1988 International Conference on, 4, 01 2002. doi: 10.1109/ICASSP.2002.5745512.

11

Table 15: Average computation times (seconds) for one epoch with and without the SPR term

Architecture and data set

time SPR time without SPR

ResNet-20 on CIFAR-10
ResNet-56 on CIFAR-10
ResNet-20 on CIFAR-100
ResNet-18 on ImageNet

13.05
36.58
22.99
2,433.14

6.51
16.99
11.26
2,401.05

Table 16: Ablation study on hyperparameter (ResNet-56 on CIFAR-10)

λ

1.7
1.4
1.7
0.5

α Accuracy

Pruned pars

1e-4
1e-4
0.3
0.3

90.12
88.47
90.59
94.04

89.04
87.09
85.76
6.03

[28] R. K. Ramakrishnan, E. Sari, and V. P. Nia. Differentiable mask pruning for neural networks.

arXiv preprint arXiv:1909.04567, 2019.

[29] T. Serra, A. Kumar, and S. Ramalingam. Lossless compression of deep neural networks. CoRR,

abs/2001.00218, 2020. URL http://arxiv.org/abs/2001.00218.

[30] X. Sheng, C. Hanlin, G. Xuan, L. Kexin, L. Jinhu, and Z. Baochang. Efﬁcient structured
pruning based on deep feature stabilization. Neural Computing and Applications, 1433-3058,
2021. URL https://doi.org/10.1007/s00521-021-05828-8.

[31] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recog-

nition. In International Conference on Learning Representations, 2015.

[32] Y. Sui, M. Yin, Y. Xie, H. Phan, S. A. Zonouz, and B. Yuan. CHIP: channel independence-
URL

CoRR, abs/2110.13981, 2021.

based pruning for compact neural networks.
https://arxiv.org/abs/2110.13981.

[33] M. R. Vemparala, N. Fasfous, A. Frickenstein, M. A. Moraly, A. Jamal, L. Frickenstein,
C. Unger, N. S. Nagaraja, and W. Stechele. L2PF - learning to prune faster. CoRR,
abs/2101.02663, 2021. URL https://arxiv.org/abs/2101.02663.

[34] Y. Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, and X. Hu. Pruning from scratch. CoRR,

abs/1909.12579, 2019. URL http://arxiv.org/abs/1909.12579.

A Appendix

A.1 Time complexity study

Our method is composed of two steps, in the ﬁrst one the computation of an additional regularization
term is required, while the second one is a ﬁne-tuning phase that is common to many other pruning
methods and is just a normal backpropagation. So, we can do a comparison between the cost of
an epoch in the ﬁrst step and in the second one to have a "worst case scenario" of a comparison
with other methods that use a regularization term to prune. From Table 15, we can notice that for
easier data sets (small input size) the cost of one epoch with our regularization term is roughly the
double of a normal one, while for the hardest data set, so in the contest of real applications, this gap
decreases and the two costs are almost the same.

A.2 Detail on grid search and ablation study

As we stated in the ﬁrst paragraph of Section 3, α and λ hyperparameters are found through grid
search. We performed a different grid search for each of the data set / architecture pair we used:
for ResNet-20 on CIFAR10, we had λ ∈ [0.2, 0.5, 0.8, 1.1, 1.4, 1.7] and α ∈ [1e − 4, 1e − 3, 5e −

12

Table 17: Accuracy before and after the ﬁnetuing phase (ResNet-18 on CIFAR.10)

λ

1.1
1.7
1.1
0.5
0.2

0.01
0.3
0.3
0.3
0.3

Original model

α Accuracy before Accuracy after

82.40
85.28
88.22
90.62
92.46

92.03

85.56
87.33
89.47
91.23
92.69

-

3, 1e − 2, 0.1, 0.3]; for Resnet-56 on CIFAR-10, we had λ ∈ [0.5, 0.8, 1.1, 1.4, 1.7]; for Resnet-20
on CIFAR-100, we had λ ∈ [0.1, 0.4, 0.7, 1.0, 1.3, 1.6] and α ∈ [1e − 3, 1e − 2, 0.1, 0.3]; ﬁnally
for Resnet-18 on ImageNet, we had λ ∈ [0.5, 0.8, 1.0, 1.3] and α ∈ [1e − 3, 1e − 2, 0.1, 0.3]. The
λ parameter is just the usual regularization parameter, the α parameter instead tunes how much of
the regularization is computed in a "structured" way. From Table 16, we can see that to obtain the
maximum level of sparsity λ needs to be high, while α needs to be close to 0, while for lower level
of sparsity can be sufﬁcient and sometimes more effective to just increase α. Finally, for very low
level of pruning, also λ needs to be decreased.

Finally, we report an observation on the importance of the ﬁnetuning phase. From Table 17, we can
see that this step is crucial to obtain higher accuracy when the pruning determined a big drop in this
value, while is less important when the accuracy stays high despite the pruning.

A.3 Observation on the structure of the pruned network

From the experiments, we noticed that our algorithm heavily prunes the last layers of the network.
This is due to the fact that the gain in the objective function is bigger for these last layers since
their ﬁlters contain way more parameters than ﬁlters belonging to the ﬁrst layers. When we allow
our pruning algorithm to heavily prune the network at the cost of a consistent accuracy drop or
when the model is so over-parametrized that even pruning a lot of parameters slightly affect the
accuracy, we notice that all ﬁnal layers are fully pruned. Differently, when we prune less parameters
to avoid big accuracy drops the ﬁnal layers that are not fully pruned tend to be the same for different
conﬁgurations of the hyperparameters, i.e., likely identifying essential structures of the model. For
example, pruning ResNet-18 on the ImageNet data set, the layer with the last residual connection is
not pruned in almost all our experiments.

13

