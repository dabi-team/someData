2
2
0
2

l
u
J

9

]

G
L
.
s
c
[

1
v
6
9
2
4
0
.
7
0
2
2
:
v
i
X
r
a

TensorIR: An Abstraction for Automatic Tensorized Program
Optimization

Siyuan Feng∗
Shanghai Jiao Tong University
Shanghai, China
hzfengsy@sjtu.edu.cn

Bohan Hou∗
Carnegie Mellon University
Pittsburgh, USA
bohanhou@cs.cmu.edu

Wuwei Lin
OctoML
Seattle, USA
wlin@octoml.ai

Zihao Ye
University of Washington
Seattle, USA
zhye@cs.washington.edu

Junru Shao
OctoML
Seattle, USA
jshao@octoml.ai

Lianmin Zheng
UC Berkeley
Berkeley, USA
lmzheng@berkeley.edu

Hongyi Jin†
Carnegie Mellon University
Pittsburgh, USA
hongyij@cs.cmu.edu

Ruihang Lai†
Carnegie Mellon University
Pittsburgh, USA
ruihangl@cs.cmu.edu

Cody Hao Yu
Amazon Web Services
Seattle, USA
hyuz@amazon.com

Yong Yu
Shanghai Jiao Tong University
Shanghai, China
yyu@apex.sjtu.edu.cn

Tianqi Chen
Carnegie Mellon University
Pittsburgh, USA
tqchen@cmu.edu

ABSTRACT
Deploying deep learning models on various devices has become
an important topic. The wave of hardware specialization brings
a diverse set of acceleration primitives for multi-dimensional ten-
sor computations. These new acceleration primitives, along with
the emerging machine learning models, bring tremendous engi-
neering challenges. In this paper, we present TensorIR, a compiler
abstraction for optimizing programs with these tensor computation
primitives. TensorIR generalizes the loop nest representation used
in existing machine learning compilers to bring tensor computation
as the first-class citizen. Finally, we build an end-to-end framework
on top of our abstraction to automatically optimize deep learning
models for given tensor computation primitives. Experimental re-
sults show that TensorIR compilation automatically uses the tensor
computation primitives for given hardware backends and deliv-
ers performance that is competitive to state-of-art hand-optimized
systems across platforms.

1 INTRODUCTION
Deploying high-performance machine learning models has become
an emerging challenge in various areas, including image recogni-
tion [18, 19, 38], natural language processing [13, 33, 42], and games
[26, 28, 36]. The advances in machine learning bring demands to
support a broad range of models. In the meantime, there are in-
creasing demands to deploy smart applications to a broad spectrum
of devices ranging from servers to embedded environments.

∗Both authors contributed equally to the paper
†Part of this work was done at Shanghai Jiao Tong University

Figure 1: Trends of hardware specialization. The classical ac-
celeration technique uses vector units to process multiple
scalar computations simultaneously, which is still widely
used on CPU platforms. However, to cater to increasingly
heavier computation throughput requirements, modern ac-
celerators usually contain specialized high-dimensional ten-
sor computation instructions, creating the need for ten-
sorized program optimization.

The wave of hardware specialization further complicates the
problem (Figure 1). Driven by the goal of machine learning acceler-
ation, modern hardware backends introduce specialized primitives
to speed up tensor computations. Domain experts also start to de-
velop micro-kernel primitives, which carefully organize a series
of highly optimized instructions to perform a sub-computation to
speed up domain-specific tensor operator libraries. These hard-
ware instructions and micro-kernel primitives typically operate on

# semanticsC[0] +=A[0] *B[0]# implementationllvm.fmuladd.f32# semanticsforiinrange(4):C[i] +=A[i] *B[i]# implementationllvm.fmuladd.v4f32# diagram# semanticsfory, x, k ingrid(16, 16, 16):C[y, x] +=A[y, k] *B[k, x]# implementationnvvm.wmma.m16n16k16.mma.row.row.f32.f32# diagram+=×+=×Generic FMAVector FMANvidia Tensor Core and NPUsScalar unitSIMDvector unitsSpecialized tensor instructions 
 
 
 
 
 
Siyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, and Tianqi Chen

Figure 2: An expert developer can choose to divide the problem into 4x4 matmul and loops that uses the matmul, then optimize
them separately. This way we can effectively make use of specialized tensor instructions in the target hardware.

multi-dimensional tensor regions and effectively perform tensor
operations such as multi-dimensional loads, dot product, and matrix
multiplication (Figure 1). We call these opaque tensor computation
acceleration constructs tensorized intrinsics and transformation pro-
cedure to use these intrinsic tensorization. In order to get the best
out of these hardware backends, modern machine learning systems
need to optimize programs that contain hierarchical loop nests,
multi-dimensional loads, and tensor intrinsics – we call this prob-
lem tensorized program optimization.

Most of the current tensorized programs are optimized by do-
main experts, who compose tensorized primitives together with
multi-dimensional loops, threading patterns, and data caching to
craft specialized kernel libraries such as Intel MKL-DNN [20], ARM
Compute Library [3] and NVIDIA cuDNN [11]. These libraries are
then used by machine learning frameworks such as TensorFlow [1],
PyTorch [32] and MXNet [8]. However, huge engineering efforts
are required to support the growing sets of models and backends.
In this paper, we propose to address the tensorized program op-
timization problem using a compilation approach. Most past works
in machine learning compilation [9, 41] search over a program
space of loop nest transformations and do not handle tensorized
programs automatically. Bringing automatic program optimization
to tensorized programs would unlock the benefits from domain-
specific accelerations in modern hardware backends. We identify
the following key challenges to achieving this goal:

Large Design Space of Possible Tensorized Program Optimizations.
Another challenge is to produce an optimized tensorized program
for a given operator automatically. A compiler needs to make use
of a rich set of techniques that domain experts might use, includ-
ing making effective use of loop tiling, threading, and data layout
transformations. Importantly, these transformations now need to
be made in conjunction with tensorized computations, bringing ad-
ditional complexities to analysis and automation. The combinations
of these transformations form a large search space. We need an
effective way to find an optimized tensorized program for a given
search space.

To address these challenges, we introduce TensorIR, an abstrac-
tion for automatic tensor program optimization. To begin with, we
introduce a new construct called block that allows us to divide and
isolate tensorized computation region from the outer loop nests.
The new abstraction allows us to effectively represent tensorized
computations and combine them with loop nests, threading, and
memory hierarchy. We also introduce program transformation prim-
itives to express a rich space of potential optimizations. We build a
novel automatic scheduling algorithm on top of the abstraction and
transformation primitives. Additionally, TensorIR abstraction also
allows us to represent and optimize programs that contain a mix-
ture of irregular computations and tensor computations, expanding
the possible support beyond a normal tensor expression [9]. This
paper makes the following contributions:

Abstraction for Tensorized Programs. To build an automated com-
piler for tensorized programs, we need an abstraction that can
pragmatically capture possible equivalent tensorized computations
for a given machine learning operator. Notably, the abstraction
needs to represent multi-dimensional memory accesses, threading
hierarchies, and tensorized computation primitives from different
hardware backends. The abstraction also needs to be expressive
enough to represent most of the operators of interest in machine
learning.

• We propose a novel abstraction for tensorized programs that
separates tensorized computation from the loop transfor-
mations. Meanwhile, the same abstraction allows us to uni-
formly represent tensor intrinsics and hardware constraints.
• We build transformation primitives to generate a rich search
space of tensorized program optimization with correctness
validation.

• We design and implement a new tensorization-aware auto-

matic scheduler.

for y, x, k in grid(64, 64, 64):  C[y, x] += A[y, k] * B[k, x]for yo, xo, ko in grid(16, 16, 16):  matmul_add4x4(C, A, B, yo, xo, ko)foryo, xo, ko ingrid(16, 16, 16):foryi, xi ingrid(4, 4, 4):C[...] +=A[...] *B[...]Divide the Problem into loop nests and matmulkernel Optimize Tensorized Computation Bodydefmatmul_add4x4_v1(C, A, B, yo, xo, ko):foryi, xi ingrid(4, 4):C[yo*4+yi, xo*4+xi] +=accel.dot(A[yo*4+yi, ko*4:ko*4+4], B[ko*4:ko*4+4, xo*4+xi])Optimize Outer Loop Nestsdefmatmul_add4x4_v0(C, A, B, yo, xo, ko):accel.matmul_add4x4(C[yo*4+yi, xo*4+xi], A[yo*4+yi, ko*4:ko*4+4], B[ko*4:ko*4+4, xo*4+xi])Operator Definitionfor yo, xo, k in grid(4, 4, 16):  for yi, xi in grid(4, 4):    matmul_add4x4(C, A, B, yo*4 + yi, xo*4 + xi, k)TensorIR: An Abstraction for Automatic Tensorized Program Optimization

Figure 3: Overview of our approach. We use a key abstraction named block to divide and isolate the tensorized computations,
and enables further loop transformations with tensorized operations.

We integrate TensorIR with an end-to-end compilation framework
and show that it outperforms existing machine learning compila-
tion solutions by up to 7x and automatically brings competitive
performance to heavily optimized platform-specific solutions .

2 OVERVIEW
This section describes the key insights of our approach and gives
an overview of the paper. To motivate our approach, we start with
an example flow of how a domain expert optimizes a tensorized
program in Figure 2. Tensorized computation primitives usually
correspond to a sub-problem of the original tensor operator com-
putation. As a result, it is natural for domain experts to choose a
divide and conquer approach – divide the original program into
sub-problems of tensorized computation and loop nests that use the
tensorized computation, then optimize them separately. The divide
and conquer approach allows developers to focus on a sub-problem
without worrying about the others. Additionally, it also enables us
to target multiple tensorized computation implementations.

Most existing machine learning compilers take two kinds of ap-
proaches (Figure 3). Halide [34], TVM [9], Tiramisu [5], AKG [45],
MLIR/Affine [25] and AMOS [47] take a bottom-up approach that
models the search space using loop nests iterators around scalar
operation bodies, and then optimizes the program by finding the
best loop nest transformation (through search or polyhedral opti-
mization). HTA [16], Fireiron [17] and Stripe [44] use a top-down
approach that gradually decomposes the problem into sub-problems

through nested polyhedral structures. Given the significance of the
divide and conquer approach in manual tensorized program opti-
mizations, it is natural to ask whether it is possible to bring the
same insight to machine learning compiler design.

We give a positive answer in this paper. Specifically, we intro-
duce a new abstraction called block into the loop nests. A block
contains the right amount of signature information to isolate the
inner problem space and outer problem. With block, we can con-
tinue to perform transformations on both outer and inner problem
independently, using the block signature as the interface. Similar
to the manual divide and conquer approach, a common use case
of a block is to represent a tensorized computation primitive in a
hardware backend, but we can also use the block to isolate bigger
sub-problems of interest when divide and conquer makes sense. Im-
portantly, tensor computation is the first-class citizen in TensorIR.
Loop nests with blocks can be viewed as a generalized abstraction
of iteration space. We present the detailed design of the TensorIR
abstraction in section 3.

To automate the tensorized program optimization, we construct
a search space of possible ways to divide the problem guided by the
hardware tensor computation primitives, then further search over
possible ways to solve sub-problems using program transforma-
tions. We present the automatic scheduling algorithm for tensorized
programs in section 4.

Existing ApproachesSearch space of loop transformations with scalar operationsfor yo, xo, ko in grid(16, 16, 16):  for y, x, ki in grid(4, 4, 4):Scalar Loop ProgramsScalar body (MulAdd)Scalar body (MulAdd)C[yo*4+y, xo*4+x] +=A[yo*4+y, ko*4+ki] * B[ko*4+ki, xo*4+x]for y, x, k in grid(64, 64, 64):  C[y, x] += A[y, k] * B[k, x]Scalar body (MulAdd)(a) Bottom-Up: loop transformation on scalar expressionsfor y, x, k in grid(64, 64, 64):  C[y, x] += A[y, k] * B[k, x]Introduce a key abstraction called block to divideand isolate the problem space into outer loop nests and tensorizedbodyOur Approachforyo, xo, ko ingrid(16, 16, 16):fory, x, k ingrid(4, 4, 4):C[by*16+y, bx*16+x] +=A[by*16+y, bk*16+k] *B[bk*16+k, bx*16+x]block (by=yo, bx=xo, bk=ko)Matmul(M, N, K)(GL, GL, GL)(Kernel)Matmul(128, 128, K)(GL, GL, GL)(Block)Init(128, 128, K)(RF=0)(Block)Move(128, 128, K)(RF->GL)(Block)Matmul(128, 128, K)(GL, GL, RF)(Block)Matmul(1, 1, 1)(RF, RF, RF)(Thread) C[y, x] += A[y, k] * B[k, x](b) Top-Down: gradual decompositionSearch space of loops transformations with tensorized operationsfor yo, xo, k in grid(4, 4, 16):  for yi, xi in grid(4, 4):Tensorized Programsblock (by, bx,bk=...)Tensorized body (Matmul 4x4)Tensorized body (matmul4x4)isolated from the outer loop nests(c) Divide and Conquer: divide the problem into outer loop nests and inner bodies, and solve them separatelyOption 0: Tensorized body (Matmul 4x4)accel.matmul_add4x4(C[by*16:by*16+4, bx*16:bx*16+4],A[by*16:by*16+4, bk*16:bk*16+4],B[bk*16:bk*16+4, bx*16:bx*16+4])Option 1: Tensorized body (Matmul 4x4)for y, x in grid(4, 4):  accel.dot(C[by*16+y, bx*16+x],A[by*16+y, bk*16:bk*16+4],B[bk*16:bk*16+4, bx*16+x])Siyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, and Tianqi Chen

Figure 4: An example TensorIR program with three major
elements - multi-dimensional buffers, loop nests and com-
putational block. Details of block is omitted for simplifica-
tion.

3 TENSORIR ABSTRACTION
This section introduces the TensorIR abstraction. Figure 4 gives an
example of TensorIR program. We introduce a Python-AST (abstract
syntax tree) dialect of TensorIR to let developers directly construct
and transform programs in Python. A TensorIR program contains
three main elements: multi-dimensional buffers, loop nests (with
possible thread bindings in GPU settings), and blocks. A block can
contain one or more nested loop nests with sub-blocks or sequence
of imperative statements that correspond to the content of the
computation. This representation allows us to divide computations
into the corresponding sub(block)-regions and do effective program
transformations using dependency information stored in the block
signature. We discuss the design details in §3.1.

3.1 Block
A block in TensorIR represents a tensorized computation on a sub-
region of the multi-dimensional buffers. Figure 5 shows an example
block for the matrix multiplication (matmul) computation. The
body of a block is parameterized by a set of block iterator variables
𝑣𝑦, 𝑣𝑥 , 𝑣𝑘 which represents an abstract tensorized computation. In-
stantiated with different value combinations of these block iterator
variables, the block maps to different concrete running block in-
stances. These iterator variables can be bound to expressions that
contain the outer loop iterators, which implies the execution order
of block instances.

Rationale. The main design rationale of a block is to isolate ten-
sorized computation – we want to be able to transform loop nests
outside the block without looking into its body. However, unlike
scalar computation, we may not be able to extract the dependency

Figure 5: Blocks contain complete signature for dependency
analysis and we make it an isolation level between body
computation and outer loop nesting.

information needed for transformation from an opaque tensor com-
putation body. As a result, we introduce a block signature that
contains sufficient dependency information for transformations.
We discuss these transformations in §3.2. Additionally, the signa-
ture can be used to independently verify the correctness of the
iterator bindings during transformations (more details in §3.3).

Block Iterator Domain. While it is possible to instantiate a block’s
body computation by binding the block iterators to any loop nests,
most instantiations do not correspond to the same computation.
To ensure the consistency of computation among transformations,
we store the iterator domain information and the constraints of
iterators in the block signature. For the particular example in Fig-
ure 5, we know that 𝑣𝑥, 𝑣𝑦 and 𝑣𝑘 must bind to iterators in domain
[0, 16). Additionally, because 𝑣𝑘 is a reduction axis, we know that
we cannot bind it to a parallel loop unless the reduction is atomic.
The domain constraints still leave massive room for outer loop
transformations, as there are multiple ways to construct loops that
satisfy the constraint. Our domain signature can be viewed as a
specific way to represent the integer domain sets and relations of
the iterators. We choose the particular representation due to its
implementation efficiency and simplicity in reasoning, but would
also point out that the same design philosophy applies to other
formal domain representations of integer sets and relations [40].

Access Region and Dependency. To provide sufficient dependency
information, a block signature contains the access regions and
read/write dependencies that a block has with respect to the mul-
tiple dimensional buffers. In Figure 5, the block writes the region
𝐶 [𝑣𝑦 ∗ 4 : 𝑣𝑦 ∗ 4 + 4, 𝑣𝑥 ∗ 4 : 𝑣𝑥 ∗ 4 + 4] by reading 𝐴[𝑣𝑦 ∗ 4 :

@scriptdef fuse_add_exp(    A: Buffer[(64, 64), "float32"],    C: Buffer[(64, 64), "float32"],):    B = alloc_buffer((64, 64), "float32")    for i, j in grid(64, 64):        with block("block_B"):            vi = spatial_axis(64, i)            vj = spatial_axis(64, j)            B[vi, vj] = A[vi, vj] + 1    for i in range(64):        with block("block_C"):            vi = spatial_axis(64, i)            for j in range(64):                C[vi, j] = exp(B[vi, j])Computation:C = exp(A + 1)LoopnestsComputational blockMulti-dimensional bufferfor yo, xo, ko in grid(16, 16, 16):  with block():    vy, vx, vk = ...    block_siguatures    with init():      for y, x in grid(4, 4):        C[vy*4+y, vx*4+x] = 0.0    for y, x, k in grid(4, 4, 4):      C[vy*4+y, vx*4+x] +=        A[vy*4+y, vk*4+k] * B[vk*4+k, vx*4+x]outer loopbodyProducer-consumer dependency relationssignaturesBlock Signatureread  A[vy*4:vy*4 + 4, vk*4:vk*4 + 4]read  B[vk*4:vk*4 + 4, vx*4:vx*4 + 4]write C[vy*4:vy*4 + 4, vx*4:vx*4 + 4]vy: spatial_axis(length=16, binding_value=yo) vx: spatial_axis(length=16, binding_value=xo) vk: reduce_axis(length=16, binding_value=ko) initstmtIterator domain and binding valuesTensorIR: An Abstraction for Automatic Tensorized Program Optimization

Figure 7: Blockization creates a new block to isolation inside
computation and outside loop nesting.

computation locations within a schedulable block by analyzing the
sub-block signatures and their dependency information. Notably,
a schedulable block can contain non-schedulable sub-blocks (e.g.,
opaque Tensor Core computation). An opaque block can also con-
tain a schedulable sub-block. Based on the block isolation, we can
still effectively explore the search space of the schedulable part
independently while keeping the same opaque block. We describe
the schedule primitives in the rest part of this subsection.

Loop Transformations. Loop transformations such as loop tiling (split,

reorder) and compute location mutation are important ways to op-
timize programs for better memory locality. We also provide these
loop transformation primitives (see examples in Figure 6). Unlike ex-
isting tensor compilers that directly extract the dependency of each
leaf scalar computation statement, we calculate the dependencies
by only inspecting the block signature. Besides loop transforma-
tions, we also support primitives that bind loops to GPU threads
and provide annotation hints such as vectorization and unrolling.

Blockization. The loop transformation primitives preserve the
overall hierarchy of blocks. As we alluded in section 2, sometimes
dividing the problem by isolating a sub-region computation into a
new sub-block is helpful. We call this transformation blockization
Figure 7. A blockized program is no longer scalar-based as the
new sub-block corresponds to a tensorized computation. We can
use blockization to isolate possible candidates for tensorization.
Besides blockization, we also introduce primitives that can change
the block hierarchies. For example, we provide caching primitives
that introduce sub-blocks to cache input data into shared memory.
We also provide back and forth transformations between a single
reduction block and the corresponding init- and update blocks.

Separation of Scheduling and TensorIR. Many previous tensor
compilers [9, 34] rely on a declarative scheduling language to con-
struct a schedule tree. Adding new scheduling primitives to these
compilers requires changes to both the schedule tree data structure
and the corresponding lowering rule in these compilers. We take
a different approach and implement each schedule primitive as a
standalone transformation from one TensorIR program to another.
This design is easier to extend, as different developers can develop
new primitives concurrently based on a stable TensorIR abstraction.

Figure 6: Loop transformations mutate outside loop nests
but change nothing inside the block.

𝑣𝑦 ∗ 4 + 4, 𝑣𝑘 ∗ 4 : 𝑣𝑘 ∗ 4 + 4], 𝐵 [𝑣𝑘 ∗ 4 : 𝑣𝑘 ∗ 4 + 4, 𝑣𝑥 ∗ 4 : 𝑣𝑥 ∗ 4 + 4].
The dependency information is used during transformations. We
only mark each block’s dependency with respect to the multi-
dimensional buffers instead of other statements (blocks). This indi-
rection enables a broader range of transformations, such as data
layout transformation and re-computation which are essential in
tensorized program optimization.

Reduction Block and Initialization. A reduction computation usu-
ally contains an initialization step and an update step. We can
naturally map the reduction computation into two blocks. But, on
the other hand, it is usually helpful to jointly make the scheduling
decisions (such as tiling and computation location) of the two steps.
We introduce an optional initialization statement for blocks that per-
form reduction. An initialization statement is executed during the
first iteration of a reduction. This reduction block representation is
mainly useful during transformations. We provide transformation
primitives to transform between the two-block-based representa-
tion and the init-block-based representation so we can pick the best
representation for low-level code generation.

3.2 Scheduling Transformations
For a given input program, we need to generate a rich search space
of programs with equivalent semantics. We introduce primitives to
transform a TensorIR program to equivalent optimized programs.
Following the existing convention of tensor program optimiza-
tions [5, 9, 34], we call this procedure scheduling.

A block is schedulable if it only contains loop nests with sub-
blocks as its leaves. We can transform the loop nests and sub-block

fori, j ingrid(64,64):C[vi, vj] =dot(A[vi, :], B[:, vj])fori, j ingrid(64,64):D[vi, vj] =max(C[vi, vj], 0)block_C (vi, vj = i, j)block_D(vi, vj = i, j)for i0, j0 in grid(8, 8):  for i1, j1 in grid(8, 8):    C[vi, vj] = dot(A[vi, :], B[:, vj])for i, j in grid(64, 64):  D[vi, vj] = max(C[vi, vj], 0)Loop TilingReverseCompute_atblock_C(vi, vj = i0*8 + i1, j0*8 + j1)block_D(vi, vj = i, j)for i0, j0 in grid(8, 8):  for i1, j1 in grid(8, 8):    C[vi, vj] = dot(A[vi, :], B[:, vj])  for i1, j1 in grid(8, 8):    D[vi, vj] = max(C[vi, vj], 0)block_D(vi, vj = i0*8 + i1, j0*8 + j1)block_C(vi, vj = i0*8 + i1, j0*8 + j1)for i, j, k0 in grid(64, 64, 16):  for k1 in range(4):    C[vi, vj] += A[vi, vk] * B[vk, vj]block (vi, vj, vk = i, j, k0*4 + k1)for i, j, k0 in grid(64, 64, 16):  for k1 in range(4):    C[vi, vj] += A[vi, vk] * B[vk, vj]blockized(vi0, vj0, vk0 = i, j,k0)Blockizeblock (vi,vj, vk = vi0, vj0,vk0*4 + k1)Siyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, and Tianqi Chen

Figure 8: Automatic optimization for tensorized program with hardware intrinsics. We take 64x64x64 matrix multiplication
followed by a RELU operator as the input workload and 4x4x4 matmul as the synthetic tensor intrinsic which is implemented
by a dot product instruction. The tensorization candidate generation step tiles the 64x64x64 GEMM into 4x4x4 sub-tiles and
isolate the sub-computation. Then the tensorized program sketch generation step schedules the computation and insert the
resulting data movement (AutoCopy) blocks which are scheduled independently. Finally, we use evolutionary search to fill
the random decisions in sketches with a validation mechanism to filter out incorrect programs.

Additionally, developers can print out the program at any trans-
formation stage for debugging and mix automatic rewriting with
schedule transformations.

3.3 Validation
The blocks and their buffer read/write relations capture a complete
picture of the original computation and are used to validate the
correctness of loop nests and threading assignments.

Loop Nest Validation. Loop nest validation checks whether the
iterator binding provided by the loop nests matches the constraints
of the iterator domain, including the domain size and iterator in-
dependence information. For example, if two data-parallel block
iterators are bound as 𝑣1 = 𝑖; 𝑣2 = 𝑖 ∗ 2, then the correspond-
ing program is invalid because 𝑣1 and 𝑣2 are not independent. But
𝑣1 = 𝑖/4; 𝑣2 = 𝑖%4 can be a legal binding. We build pattern-matchers
to find a quasi-affine mapping from the loop iterators to the block
iterator variables and use the pattern to validate the independence
and domain of the bindings. Besides the iterator domain validation,
it is also important to check the producer-consumer relations to
make sure producer blocks that write to buffer regions always cover
the read region of downstream consumers.

• Thread binding: Ensure different iterators bound to the
same thread are consistent and meet the launching con-
straints of the backend.

• Cooperative memory access: For blocks that produce buffers
stored in shared memory collaboratively across threads, we
need to ensure the block covers downstream requirements
from all the threads in the same group. Meanwhile, upstream
blocks that provide inputs for this block need to cover the
read requirement of this block from all the threads in this
group.

• Execution scope: Validate that tensor intrinsic runs at the
correct execution scope (e.g., TensorCore needs to run at the
warp-level).

Correctness of Schedule Primitives. We add checks to each sched-
ule primitive to ensure the correctness of the transformation. When
a schedule primitive only changes the loop nests, we can also use
the validation procedure to ensure correctness. Because the block it-
eration domains and dependencies stay the same in these cases. We
find primitive-specific necessary conditions for schedule primitives
that change the blocks (e.g., blockization).

4 AUTO-SCHEDULING TENSORIZED

PROGRAMS

Threading Validation. When building a program for GPUs and
other accelerators with threading support, we also need to do ad-
ditional validations with respect to the threading and memory
hierarchies. We do three kinds of validations:

In the last section, we introduced TensorIR abstraction and a set
of transformation primitives. In order to fully make use of the
set of improvements, we need an automatic solution to optimize
over a set of transformations and map computations to the native

for y0, x0 in grid(?, ?, ?):  for y1, x1, k0 in grid(?, ?, ?):    for y2, x2, k1 in grid(?, ?, ?):            for y3, x3, k0 in grid(?, ?):  for y1, x1 in grid(?, ?):    C.cache[...] = C[...]  for y1, x1 in grid(?, ?, ?):  D[...] = max(C[...], 0)for yo, xo, ko in grid(16, 16, 16): for y, x in grid(64, 64):  D[y, x] = max(C[y, x], 0)InputTensorization Candidate Generation(Sec 4.2)Tensorized ProgramSketch Generation(Sec 4.3)for y, x, k in grid(64, 64, 64):  C[y, x] += A[x, k] * B[k, y]for y, x in grid(64, 64):  D[y, x] = max(C[y, x], 0)block (matmul4x4)Evolutionary Search (Sec 4.4)Validation (Sec 4.4)Tensorized program sketchesTensor Intrinsics  (Sec 4.1)Tensorization candidatesTensorization candidatesTensorized program sketches# semantics (matmul4*4)for y, x, k in grid(4, 4, 4):  C[y, x] += A[x, k] * B[k, y]# implementation (dot)for yi, xi in grid(4, 4):  C[yo*4+yi, xo*4+xi] +=     accel.dot(A[xo*4+xi, ko*4:ko*4+4],               B[ko*4:ko*4+4, yo*4+yi])for y0, x0 in grid(?, ?, ?):  for y1, x1, k0 in grid(?, ?, ?):                  for y2, x2, k1 in grid(?, ?, ?):            for y3, x3, k0 in grid(?, ?):   for y1, x1 in grid(?, ?, ?):     for y2, x2 in grid(?, ?):            D.cache[...] = max(C.cache[...], 0)block (matmul4x4)Computation body (matmul4x4)block (AutoCopy)Data movement body (A->A.cache)block (AutoCopy)Data movement body (B->B.cache)Data movement body (D.cache->d) for y2, x2 in grid(?, ?):    A.cache[...] = A[...] block (AutoCopy)for y, x, k in grid(4, 4, 4):    C[y, x] += A[x, k] * B[k, y]Computation body (matmul4x4)Sub-computationfor yi, xi in grid(4, 4):  C.cache[yo*4+yi, xo*4+xi] +=     accel.dot(      A.cache[xo*4+xi, ko*4:ko*4+4],       B.cache[ko*4:ko*4+4, yo*4+yi])Sub-computation (tensorized) for y2, x2o in grid(?, ?):   for vec in vectorized(?):     A.cache[...] = A[...] Data movement (before schedule)Data movement (after schedule)TensorIntrin (matmul4x4)TensorIR: An Abstraction for Automatic Tensorized Program Optimization

Figure 9: Example flow of tensorization candidate generation. We take standard NHWC 2D convolution as the input workload
and 16x16x16 matrix multiplication as the hardware backend intrinsic. First, the system converts the buffer access expressions
to intermediate iterators. Based on the buffer access patterns, we calculate characteristic functions for each iterator and build a
mapping between iterators that share the same characteristic vector. The mapping further guides the transformation of block
instance space and ReIndex buffers. Note that although the ReIndex stages of B and C are redundant, they will be inlined into
consumers during the sketch generation phase and as a result do not affect the performance.

tensor intrinsics. In this section, we describe a tensorization-aware
automatic scheduler to solve this problem.

Figure 8 shows an overview of our approach. Our system takes
a workload description from users and tensor intrinsic descriptions
about the hardware platform as inputs. The auto-scheduler first
generates candidates for tensorization by inspecting the compu-
tation pattern. It then generates program sketch candidates that
use the tensorized computations and then decide the data move-
ments according to the compute patterns. For a given search space
induced by the tensorized program sketches, we perform evolu-
tionary search guided by a learning-based cost model. The entire
process centers itself around the tensorization and leverages the
new block abstraction to isolate tensorized computations. We dis-
cuss the details of each step in the subsequent subsections.

4.1 Abstraction for Tensor Intrinsics
To make use of a tensor intrinsic in our optimization, we need a
way to provide its semantics and backend implementation to the
system. We leverage the same TensorIR abstraction to describe the
tensor intrinsics of a given hardware backend. For each tensorized
instruction, we introduce a TensorIntrin construct composed of
two blocks. One block describes the computation semantics, and
the other provides the low-level implementation of the tensorized
computation.

In Figure 8’s example, we use a normal loop nest with scalar body
𝐶 [𝑖, 𝑗] += 𝐴[𝑖, 𝑘] ∗ 𝐵 [𝑘, 𝑗] to represent the computation semantics
and implement the intrinsic using inner dot product instruction
𝑎𝑐𝑐𝑒𝑙 .𝑑𝑜𝑡. We also include the data type, storage scope, memory
layout, and contiguity constraints through the multi-dimensional
buffer specification in a TensorIntrin. Those constraints are used
during the validation step.

Notably, tensor intrinsics are usually applied together with spe-
cial memory scopes, data layouts, and corresponding load/store
instructions in common platforms. For instance, on NVIDIA GPUs,

if we decide to use nvcuda::wmma::mma_sync API to perform dense
computation, then we need to apply nvcuda::wmma::load_matrix_sync
and nvcuda::wmma::store_matrix_sync to prepare input operands
and retrieve output results respectively. On ARM CPUs, micro-
kernels like a64_gemm_u8_8×12 require operands to be stored in
interleaved layout. Developers can inform the system about these
constraints by specifying special storage scopes for each input and
output operands of the tensor computation.

4.2 Tensorization Candidate Generation
Given a pair of backend target and an input program, we first match
the program body to possible TensorIntrin to generate tensoriza-
tion candidates. The match is performed in a gradual way. We first
match the expression pattern 𝐶 [.] += 𝐴[.] × 𝐵 [.] without consider-
ing the indices. We then refine the matches by proposing possible
mappings between the indices. Figure 9 gives an example that walks
through the matching process. In this example, we take a matrix
multiplication intrinsic as the backend description. The compu-
tation of this tensor intrinsic can be described by the following
formula

𝐶 [𝑥, 𝑦] += 𝐴[𝑥, 𝑘] × 𝐵 [𝑘, 𝑦]

(1)

It is easy for us to match the tensor intrinsic to workloads like batch
matrix multiplication, which can be described by

𝐶 [𝑏, 𝑖, 𝑗] += 𝐴[𝑏, 𝑖, 𝑟 ] × 𝐵 [𝑏, 𝑟, 𝑗],

by mapping 𝑥, 𝑦, 𝑘 to 𝑖, 𝑗, 𝑟 . But for workloads like 2-dimensional
Convolution (Conv2D) with more complex index expression pat-
terns

𝐶 [𝑛, ℎ, 𝑤, 𝑐𝑜] +=𝐴[𝑛, ℎ ∗ 𝑠ℎ + 𝑟ℎ ∗ 𝑑ℎ, 𝑤 ∗ 𝑠𝑤 + 𝑟𝑤 ∗ 𝑑𝑤, 𝑟𝑐 ]

×𝐵 [𝑟𝑐, 𝑟ℎ, 𝑟𝑤, 𝑐𝑜],

the mapping between 𝑥, 𝑦, 𝑘 and 𝑛, ℎ, 𝑤, 𝑐, 𝑟ℎ, 𝑟𝑤, 𝑟𝑐 is not straight-
forward.

for n, h, w, c, rh, rw, rc in grid(16, 56, 56, 64, 7, 7, 3):  C[n, h, w, c] += A[n, h*2+rh, w*2+rw, rc]                  * B[c, rh, rw, rc]for x, y, k in grid(16, 16, 16):  C[x, y] += A[x, k] * B[y, k]Conv2DTensorIntrinReIndexConv2D (ReIndexed)# ReIndex Block for Afor n, h, w, c, rh, rw, rc in grid(...):  Ar[n, h, w, rh, rw, rc]     = A[n, h*2+rh, w*2+rw, rc]# ReIndex Block for Bfor c, rh, rw, rc in grid(...):  Br[c, rh, rw, rc] = B[c, rh, rw, rc]# ReIndexed Conv2Dfor n, h, w, c, rh, rw, rc in grid(...):  Cr[n, h, w, c] += Ar[n, h, w, rh, rw, rc]                   * Br[c, rh, rw, rc]# ReIndex Block for Cfor n, h, w, c in grid(...):  C[n, h, w, c] = Cr[n, h, w, c]Conv2D (ReIndexed)Cr[n, h, w, c] = Cr[n, h, w, c]               + Ar[n, h, w, rh, rw, rc]                * Br[c, rh, rw, rc]TensorIntrinC[x, y] = C[x, y] + A[x, k] * B[y, k]𝜒(n) = 𝜒(h) = 𝜒(w) = [1,1,0]𝜒(c) = [1,0,1]𝜒(rh) = 𝜒(rw) = 𝜒(rc) = [0,1,1]𝜒(x) = [1,1,0]𝜒(y) = [1,0,1]𝜒(k) = [0,1,1]Conv2D (Layout Transformed)# Layout Transformation Block for Afor n, h, w, c, rh, rw, rc in grid(...):  At[n*h*w, rh*rw*rc]     = A[n, h*2+rh, w*2+rw, rc]# Layout Transformation Block for Bfor c, rh, rw, rc in grid(...):  Bt[c, rh*rw*rc] = B[c, rh, rw, rc]# Transformed Conv2Dfor nhw, c, rhwc in grid(16*56*56, 64, 7*7*3):  Ct[nhw, c] += At[nhw, rhwc] * Bt[c, rhwc]# Layout Transformation Block for Cfor n, h, w, c in grid(...):  C[n, h, w, c] = Ct[n*h*w, c]Propose MatchTransformLayoutSiyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, and Tianqi Chen

On these more general cases, we rewrite the computation ex-

pression into an equivalent form:

𝐶 [𝑛, ℎ, 𝑤, 𝑐𝑜] += 𝐴𝑟 [𝑛, ℎ, 𝑤, 𝑟ℎ, 𝑟𝑤, 𝑟𝑐 ] × 𝐵 [𝑟𝑐, 𝑟ℎ, 𝑟𝑤, 𝑐𝑜],
𝐴𝑟 [𝑛, ℎ, 𝑤, 𝑟ℎ, 𝑟𝑤, 𝑟𝑐 ] = 𝐴[𝑛, ℎ ∗ 𝑠ℎ + 𝑟ℎ ∗ 𝑑ℎ, 𝑤 ∗ 𝑠𝑤 + 𝑟𝑤 ∗ 𝑑𝑤, 𝑟𝑐 ].

We call this transformation ReIndex which uses intermediate itera-
tors that appear in the buffer access indices to rewrite the buffer
access expressions. To match the new computation to the tensor in-
trinsic, we check the buffer access where each iterator appears. For
example, we notice that 𝑛, ℎ, 𝑤 and 𝑥 appear in indices of 𝐴(𝐴𝑟 ), 𝐶,
𝑐𝑜 and 𝑦 appear in indices of 𝐵, 𝐶, and 𝑟ℎ, 𝑟𝑤, 𝑟𝑐 and 𝑘 appear in
indices of 𝐴, 𝐶. We can then match the iterators in the compu-
tation to the iterators in the tensor intrinsic by inspecting their
appearance patterns. Specifically, we map fuse(𝑛, ℎ, 𝑦) to 𝑥, 𝑐𝑜 to
𝑦, and fuse(𝑟ℎ, 𝑟𝑤, 𝑟𝑐 ) to 𝑘. Here fuse() is to fuse multiple iterators
together and can be recursively defined by
fuse(𝑖1) = 𝑖1

fuse(𝑖1, 𝑖2, . . . , 𝑖𝑟 ) = fuse(𝑖1, 𝑖2, . . . , 𝑖𝑟 −1) ∗ extent(𝑖𝑟 ) + 𝑖𝑟 ,
where extent() is the extent of iterator 𝑖𝑟 . We can then transform
the computation to

𝐶𝑡 [fuse(𝑛, ℎ, 𝑤), 𝑐𝑜] +=𝐴𝑡 [fuse(𝑛, ℎ, 𝑤), fuse(𝑟ℎ, 𝑟𝑤, 𝑟𝑐 )]

×𝐵𝑡 [fuse(𝑟ℎ, 𝑟𝑤, 𝑟𝑐 ), 𝑐𝑜],

where

𝐶𝑡 [fuse(𝑛, ℎ, 𝑤), 𝑐𝑜] =𝐶 [𝑛, ℎ, 𝑤, 𝑐𝑜],

𝐴𝑡 [fuse(𝑛, ℎ, 𝑤), fuse(𝑟ℎ, 𝑟𝑤, 𝑟𝑐 )] =𝐴𝑟 [𝑛, ℎ, 𝑤, 𝑟ℎ, 𝑟𝑤, 𝑟𝑐 ]

𝐵𝑡 [fuse(𝑟ℎ, 𝑟𝑤, 𝑟𝑐 ), 𝑐𝑜] =𝐵 [𝑟ℎ, 𝑟𝑤, 𝑟𝑐, 𝑐𝑜].

We use this mapping to reshape the block instance space and the
outer loops and transform the layout of ReIndex buffers. We insert
layout rewrite blocks to rewrite 𝐴, 𝐵, 𝐶 to 𝐴𝑡 , 𝐵𝑡 , 𝐶𝑡 respectively
and use 𝐴𝑡 , 𝐵𝑡 , 𝐶𝑡 to rewrite the computation body. After these
steps, the computation body is compatible with the tensor intrinsic.

Loop Reorganization and Early Blockize. Besides the computation
body, we also need to ensure that the tensor computation region
matches the description provided by the TensorIntrin. The shape of
the reorganized block instance space might not be divisible by the
sub-problem size of the tensor intrinsic. For each computation body
from the last step, we do necessary padding on the computation
block and input/output operands to the closest divisible shape. We
then perform tiling to create inner loops to match the loop nest of
the tensor intrinsic and further blockize the inner loop to isolate
the corresponding tensor computations. Notably, the candidates
generated in this step do not always lead to successful tensoriza-
tions. This is because other constraints, such as memory layout
and threading depend on later transformations. These constraints
are embedded in the tensorization candidates and checked during
validation.

Formal Description of the Process. So far we gave a high-level
overview of the tensorization candidate generation process. In the
the remainder part of this subsection, we provide a formal descrip-
tion of the process. Suppose the intrinsic scalar expression can be
formalized as

𝑂 [v0] = 𝑓 (𝑂 [v0], 𝐼1 [v1], 𝐼2 [v2], . . . , 𝐼𝑘 [v𝑘 ]).

(2)

where 𝑂 is the output operand, 𝐼 [1:𝑘 ] are input operands, v is the
set of iterators that parameterized this computation, v[0:𝑘 ] are all
lists of iterators that belong to v, and 𝑓 is the expression pattern
detected. Note that it accommodates common dot product and
matrix multiplication intrinsics. Furthermore, suppose that the
workload scalar expression can be formalized as

˜𝑂 [𝑔0 ( ˜v0)] = 𝑓 ( ˜𝑂 [𝑔0 ( ˜v0)], ˜𝐼1 [𝑔1 ( ˜v1)], . . . , ˜𝐼𝑘 [𝑔𝑘 ( ˜v𝑘 )]),
where ˜𝑂, ˜𝐼 [1:𝑘 ] , ˜v[0:𝑘 ] corresponds to their counterparts and 𝑓 is
exactly the same. 𝑔 [0:𝑘 ] are mappings that map lists of iterators to
the actual buffer access position. In our Conv2D case, for instance,
we have 𝑔𝐴 (𝑛, ℎ, 𝑤, 𝑟ℎ, 𝑟𝑤, 𝑟𝑐 ) = (𝑛, ℎ∗𝑠ℎ +𝑟ℎ ∗𝑑ℎ, 𝑤 ∗𝑠𝑤 +𝑟𝑤 ∗𝑑𝑤, 𝑟𝑐 ).
To reduce the problem to a simpler canonical form, we apply the
ReIndex schedule transformation, which creates an intermediate
cache buffer for an operand but with the layout changed according
to the iterators. Formally, if we run ReIndex ˜𝐼1 [𝑔1 ( ˜v1)], we create
the following rewrite block before the computation

ˆ𝐼1 [ ˜v1] = ˜𝐼1 [𝑔1 ( ˜v1)].
Then if we apply ReIndex to all the operands, the workload scalar
expression is reduced to

ˆ𝑂 [ ˜v0] = 𝑓 ( ˆ𝑂 [ ˜v0], ˆ𝐼1 [ ˜v1], . . . , ˆ𝐼𝑘 [ ˜v𝑘 ]),
where buffer access indices in both 2 and 3 directly correspond to
iterators.

(3)

To match 2 and 3, we define the characteristic vector 𝜒 (𝑣) ∈
{0, 1}𝑘+1 of an iterator 𝑣 ∈ v by inspecting whether each of v0, v1, v2, . . . v𝑘
contains 𝑣. Formally,

𝜒 (𝑣)𝑖 = [𝑣 ∈ v𝑖 ]

𝑖 ∈ [0, 𝑘],

where [] is the Iverson bracket that returns 1 if the corresponding
condition is true or 0 otherwise (Figure 9). We can successfully
propose the mapping as long as ∀𝑣 ∈ v, ∃ ˜𝑣 ∈ ˜v, 𝜒 (𝑣) = ˜𝜒 ( ˜𝑣). In the
current implementation, we can further safely assume that iterators
in v all have different characteristic vectors. Then for all 𝑣 ∈ v, we
fuse all such ˜𝑣 where 𝜒 (𝑣) = ˜𝜒 ( ˜𝑣) and map the fused iterator to 𝑣.
Notably, the iterator order inside each of v[0:𝑘 ] or ˜v[0:𝑘 ] does
not affect the value of characteristic function 𝜒 or ˜𝜒. But when we
fuse all ˜𝑣 where 𝜒 (𝑣) = ˜𝜒 ( ˜𝑣), the order of fusion affects how the
operands are reorganized in memory to be compatible with the
tensor intrinsic. Our implementation now uses a default order for
all the workloads and can generalize to different fusion orders in
the future.

4.3 Tensorized Program Sketch Generation
For a given set of tensorization candidates, we need to construct
a large program search space that contains the tensorization. We
generalize existing hierarchical search space generation [46] to
tensor computations. We construct the search space by generating
program sketches that contain the tensorized computation, then
enumerate over choices induced by the generated sketches. As
shown in the right part in Figure 8, a program sketch fixes parts of
program structures while leaving space for remaining choices of pa-
rameters such as loop tiling size and computation caching decisions.
We generate sketches by applying pre-defined sketch generation
rules iteratively. Importantly, we need to build sketch generation
rules that work on tensorized computations by looking at the block

TensorIR: An Abstraction for Automatic Tensorized Program Optimization

signatures and make use of the access region information during
our analysis.

Data Movement as First-Class Citizen. Existing auto-schedulers
for tensor programs focus their designs on the schedule of compu-
tations and treat data movement between different memory scopes
with secondary priority. However, since tensor intrinsics vastly
improve the throughput of computations, data movements become
the bottleneck of tensor programs. Moreover, data movement de-
cisions usually depend on computation schedule decisions like
tilings, thread bindings, execution scopes, and producer-consumer
data flow granularity. We take these insights and bring data move-
ments as first-class citizens in our automatic scheduler and decouple
them from computation schedules. Specifically, we insert AutoCopy
blocks into the places where the sketch generation rules decide to
perform data movements (Figure 8). The copy block hides the mem-
ory schedule details and only exposes the necessary buffer access
information at the block signature level. The isolated copy blocks
allow the sketch generation to independently make computation
schedule decisions without considering how to do data movements.
The body of the AutoCopy block describes the details of the data
movement task, including buffer position mapping, threading, and
storage scope requirements. A data movement scheduler takes this
information as input and performs memory-related schedule trans-
formations, such as inserting intermediate cache stages, utilizing
data movement tensor intrinsics, vectorization, cooperative fetch-
ing, or stride padding to avoid bank conflicts.

4.4 Evolutionary Search
After the tensorized program sketch generation phase, we can get
billions of possible induced programs. We use evolutionary search
to explore the space and find an optimized tensorized program.
Our search starts from random initializations of choices for given
program sketches. We then perform mutations on the current set
of programs. We then select promising programs from the mutated
candidates and benchmark them on our hardware backend of inter-
est. We collect data from the evaluation phase to update the learned
cost model.

Cost Model for Tensorized Computation. We build a boosting tree
ensemble [7] based cost models that use features extracted from
the program. The feature vector contains information related to
memory access patterns, reuse, and loop annotations. Importantly,
we extract features from both block signatures in an isolated way
as well as the body of the block (e.g., to mark the use of Tensor
Core). Our cost model can be viewed as a generalization of pre-
vious approaches to tensorized programs. We believe an effective
cost model for tensorized programs is a promising area for future
research.

Validation. Randomly mutating programs during the search can
generate invalid programs due to the unmet constraints of tensor
intrinsic or invalid loop nest candidates. The possibility of false
positives necessitates a validation step during the search. We apply
techniques in subsection 3.3 to validate a program candidate in the
evolutionary search to identify and reject invalid programs. The

validation step reduces the burden on evolutionary search algo-
rithms and allows us to generate a small number of false positives
during the search.

5 EVALUATION
We implement TensorIR on top of Apache TVM [9]. Notably, the
insights described in the paper can benefit other machine learning
compilation frameworks as well. This section provides evaluations
to answer the following questions:

• Can TensorIR optimize common set of machine learning

operators (§5.1)?

• Can TensorIR bring performance boost to end-to-end net-

work execution (§5.2)?

• Can TensorIR support tensor intrinsics on different hardware

platforms (§5.3)?

To evaluate TensorIR along those axes, we compare our solution
to existing machine learning compilation solutions on GPU and
CPU platforms. We will discuss the specific setups in the corre-
sponding subsections.

Additionally, we are interested in the following question through-
out all evaluations: How does TensorIR compare to vendor-specific
libraries and frameworks that rely on these libraries? Importantly,
most of these libraries are heavily optimized by a dedicated team
of engineers. In all of these settings, TensorIR performs end-to-
end automatic optimization without the need to call into external
libraries.

5.1 Single Operator Evaluation
This section evaluates TensorIR on operators in deep learning mod-
els. We pick a common collection of workloads, including: 1D con-
volution (C1D), 2D convolution (C2D), 3D convolution (C3D), depth-
wise convolution (DEP), dilated convolution (DIL), general matrix
multiply (GMM), group convolution (GRP), and transposed 2D con-
volution (T2D). The evaluations are done on an NVIDIA RTX 3080
platform with Tensor Cores. We pick this platform as it has a wide
spectrum of machine learning compiler solutions and libraries that
we can use as comparison reference points. We use float16 as the
input and accumulator data type for all operators. We include TVM
(commit: 27b0aad5, with auto-scheduler [46]) and AMOS (commit:
6aee6fe2) as two machine learning compiler baselines. Finally, we
compare against two vendor-specific solutions: CUTLASS (version
2.9) and TensorRT (PyTorch-TensorRT container Release 22.06).

Comparisons to Machine Learning Compilers. Figure 10 shows
the comparisons to AMOS and TVM. TVM [9] works well on less
compute-intensive workloads (e.g.DEP), but has poor performance
on heavy ones (e.g.C2D, C3D, GMM) due to the limited Tensor Core
support. AMOS [47] can use Tensor Core for every workload but
not doing as well as TensorIR. Overall, TensorIR brings up to 7.5×
number of improvement over existing machine learning compila-
tion solutions. These improvements come from better abstraction
and automatic scheduling that leverages tensor compute intrinsics
and corresponding data movements.

Comparisons to Platform Specific Libraries. Figure 11 shows the
comparisons of TensorIR to two platform specific solutions: CUT-
LASS [22] and TensorRT [31]. TensorIR outperforms the baselines

Siyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, and Tianqi Chen

Figure 10: Single operator comparison to existing machine
learning compilers on Nvidia GPU. TensorIR brings up to
7.5x speed across workloads.

Figure 11: Single operator comparison to platform-specific
libraries. We did not show the numbers of CUTLASS on DEP,
GRP, and T2D as the library does not support them. Ten-
sorIR outperforms the baselines on C1D, C2D, DEP, T2D, and
DIL by up to 13.9x and gets to more than 75% throughput on
C3D, GMM, and GRP.

on C1D, C2D, DEP, T2D, and DIL by up to 13.9×. These results
shows the advantage of automatic optimizations provided by Ten-
sorIR. Notably, TensorIR gets to more than 75% on C3D, GRP an
GMM. These results show that even on workloads that are inten-
sively optimized by dedicated engineering teams, TensorIR can
still get close to or match existing vendor-specific solutions. We
expect the remaining gap continues to close as we bring additional
insights from these libraries to TensorIR. In all cases, the baseline
solutions are optimized by dedicated engineering teams, while Ten-
sorIR enables automated compilation for a given tensor intrinsic
declaration.

Figure 12: End-to-End model evaluations on NVIDIA GPU.
TensorIR significantly outperforms existing machine learn-
ing compilation solutions and achieves similar or better
throughputs on popular networks compared with the infer-
ence libraries on GPUs. TensorIR get better performance on
ViT, an emerging model that TensorRT does not yet support.

5.2 End-to-end Model Evaluation
In this section, we evaluate the impacts that TensorIR can bring
to end-to-end model execution. We evaluate our solutions on four
widely-used models [13, 14, 18, 19] on on NVIDIA RTX 3080. We
include TVM and AMOS as machine learning compiler baselines.
Additionally, we also include PyTorch (version 1.13.0.dev20220612)
as a framework reference point. Finally, we include TensorRT, which
is a vendor-specific solution that is heavily optimized by engineer-
ing teams at Nvidia.

The results are shown in Figure 12. TensorIR outperforms Py-
Torch, TVM, and AMOS by 1.2 − 8.8×. Additionally, TensorIR
brings 30% better performance on MobileNet V2 comparing to
TensorRT, and achieves the 88% − 100% throughput on ResNet50
and BERT_large. Additionally, TensorIR can automatically support
emerging models such as Vision Transformer, which TensorRT does
not yet support. These results show that our abstraction and the
automatic scheduler can bring close or even better performance
than the best effort libraries on common machine learning mod-
els. Additionally, the automated solution enables us to bring faster
support for emerging models.

5.3 ARM CPU Evaluation
The last two subsections evaluate TensorIR on an Nvidia GPU.
In this section, we study how easy it is to generalize TensorIR
to different platforms. We evaluate results on an ARM platform
by providing the description with 8-bit integer dot(sdot). This in-
struction is different from the Tensor Core used in the last two
subsections. Importantly, we use the same TensorIR framework by
providing the new description of the tensor intrinsic to the system.
The evaluations are done on an AWS Graviton2 CPU.

Single Operator Results. We evaluate the results on two com-
monly used operators: C2D and GMM. We include TVM as a ma-
chine learning compiler baseline and ARMComputeLib [3] as a

C1DC2DC3DDEPDILGMM-1GMM-2GRPT2D0.00.20.40.60.81.0Batch size = 1AMOSTVMTensorIRC1DC2DC3DDEPDILGMM-1GMM-2GRPT2D0.00.20.40.60.81.0Batch size = 16Normalized PerformanceC1DC2DC3DDEPDILGMM-1GMM-2GRPT2D0.00.20.40.60.81.01.2Batch size = 1CUTLASSTensorRTTensorIRC1DC2DC3DDEPDILGMM-1GMM-2GRPT2D0.00.20.40.60.81.01.2Batch size = 16Normalized PerformanceResNet-50MobileNet V2BERT_largeViT0.00.20.40.60.81.0Batch size = 1AMOSTVMPyTorchTensorRTTensorIRResNet-50MobileNet V2BERT_largeViT0.00.20.40.60.81.0Batch size = 16Normalized PerformanceTensorIR: An Abstraction for Automatic Tensorized Program Optimization

Figure 13: Single operator evaluations on ARM CPU. Ten-
sorIR get up to 12.6x faster than TVM due to the use of na-
tive tensor instrinsic acceleration. It also gets to the same
level of performance as heavily optimized platform specific
library (ArmComputeLib).

Figure 14: End-to-end networks evaluation results on ARM
CPU. TensorIR outperform with 1.2x–2.5x than PyTorch and
TVM.

platform-specific library baseline. The results are shown in Fig-
ure 13. TensorIR achieves up to 12.5× speed up compared with
TVM thanks to the ability to leverage native hardware accelera-
tion. In the meantime, TensorIR reaches 85% − 105% throughput of
ARMComputeLib [3], showing our ability to get to the same level
of performance as vendor-specific solutions on this platform.

End-to-End Results. Finally, we evaluate the end-to-end neural
network executions on this platform. Our baselines include PyTorch
and TVM. We achieve up to 2.3× on this platform in Figure 14.
Notably, PyTorch contains a specialized quantized model support
with QNNPACK [27] backend. However, QNNPACK has not yet
added sdot support. This result alludes to the maintenance cost for
these frameworks to keep up with hardware changes. TensorIR can
help to reduce the maintenance burden through automation and
still bring competitive performance to hand-optimized systems.

scheduling language that can describe loop optimization primi-
tives of loop nests with a scalar body. Tensor Comprehensions [41],
Tiramisu [5] and MLIR/Affine [25] use polyhedral model [40] to
analyze loop nest dependencies. These works optimize loop nests
with scalar computation in a bottom-up way. Fireiron [17] and
Stripe [44] use nested polyhedral structures to model tensor pro-
grams in a top-down fashion. TensorIR combines insights from
both approaches and generalizes the representation to tensorized
programs. TACO [12, 23, 37] is a compiler for sparse tensor algebra.
Cortex [15] generalized tensor compilation to recursive compu-
tations. Our work is orthogonal to these efforts. We believe the
TensorIR abstraction can be combined with insights from these
works in the future to enable an even broader range of computa-
tions.

Automation is an essential topic in machine learning compila-
tion and tensor program optimization. AutoTVM [10] introduced a
learning-based approach to optimize tensor programs via a learned
cost model and template-guided search. Triton [39] introduces a
tile-based template representation for effective program optimiza-
tion. FlexTensor [48] automatically generates the template. Halide
builds an automatic scheduler using Monte-Carlo tree search [2].
Ansor [46] improves automatic scheduling using a hierarchical
search space. Our automatic scheduling algorithm takes lessons
from these approaches and generalizes them to tensorized compu-
tation best for domain-specific hardware acceleration.

Auto-vectorization [24, 35] is a long-standing topic in compiler
research. Tensorization can be viewed as a generalization of the
vectorization problem to enable tensor intrinsic in modern accel-
erators [4, 21, 29, 30]. There are some existing works[6, 43, 45, 47]
on this topic. AKG [45] uses the polyhedral method to explore
tensorized search space, UNIT [43] introduces a generic flow for
tensorization, while AMOS [47] enables automatic mapping to ten-
sorized intrinsic through tensor expression. Our method generalizes
these previous approaches by proposing a novel abstraction for ten-
sorization computation and jointly performing tensorization along
with other optimizations. TensorIR serves as a foundation to further
develop tensorization-aware automatic scheduling methods.

7 CONCLUSION
We propose TensorIR, an abstraction for automatic tensorized pro-
gram optimization. We design a key abstraction called block that
can isolate tensorized computations and provide effective transfor-
mation primitives for program optimization. We build an automatic
scheduling algorithm that performs tensorization jointly with other
optimizations and generates performant programs. We hope this
work will encourage additional studies of tensorized program opti-
mization and provide new opportunities for hardware and software
specialization.

6 RELATED WORKS
Deep learning frameworks [1, 8, 32] optimize deep neural networks
by invoking vendor optimized libraries (e.g., cuDNN [11], MKL-
DNN [20]). These frameworks can leverage TensorIR to generate
optimized tensorized programs for various hardware backends.

Machine learning and tensor compilers introduce different ab-
stractions for tensor programs. Halide [34] and TVM [9] use a

REFERENCES
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. Tensorflow: A system for large-scale machine learning. In 12th {USENIX}
symposium on operating systems design and implementation ({OSDI} 16). 265–283.
[2] Andrew Adams, Karima Ma, Luke Anderson, Riyadh Baghdadi, Tzu-Mao Li,
Michaël Gharbi, Benoit Steiner, Steven Johnson, Kayvon Fatahalian, Frédo Du-
rand, et al. 2019. Learning to optimize halide with tree search and random
programs. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1–12.

C2D-1C2D-2GMM-1GMM-2GMM-30.00.20.40.60.81.0Normalized ThroughputTVMArmComputeLibTensorIR0.00.20.40.60.81.0Normalized ThroughputResNet-50MobileNet V2PyTorchTVMTensorIRSiyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, and Tianqi Chen

[3] ARM. [n.d.]. ARM Compute Library.

https://developer.arm.com/compute-

library/
[4] ARM. 2017.

Exploring the Arm dot product instructions.

https:

//community.arm.com/developer/tools-software/tools/b/tools-software-
ides-blog/posts/exploring-the-arm-dot-product-instructions

[5] Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo, Ab-
durrahman Akkas, Yunming Zhang, Patricia Suriana, Shoaib Kamil, and Saman
Amarasinghe. 2019. Tiramisu: A Polyhedral Compiler for Expressing Fast and
Portable Code. In Proceedings of the 2019 IEEE/ACM International Symposium
on Code Generation and Optimization (Washington, DC, USA) (CGO 2019). IEEE
Press, 193–205.

[6] Somashekaracharya G Bhaskaracharya, Julien Demouth, and Vinod Grover.
2020. Automatic kernel generation for volta tensor cores. arXiv preprint
arXiv:2006.12645 (2020).

[7] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting
System. In Proceedings of the 22Nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD
’16). ACM, New York, NY, USA, 785–794.

[8] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexible and
efficient machine learning library for heterogeneous distributed systems. arXiv
preprint arXiv:1512.01274 (2015).

[9] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen
Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}:
An automated end-to-end optimizing compiler for deep learning. In 13th
{USENIX} Symposium on Operating Systems Design and Implementation ({OSDI}
18). 578–594.

[10] Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze,
Carlos Guestrin, and Arvind Krishnamurthy. 2018. Learning to optimize tensor
programs. In Advances in Neural Information Processing Systems. 3389–3400.
[11] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John
Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cudnn: Efficient primitives
for deep learning. arXiv preprint arXiv:1410.0759 (2014).

[12] Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. 2020. Automatic
Generation of Efficient Sparse Tensor Format Conversion Routines. In Proceedings
of the 41st ACM SIGPLAN Conference on Programming Language Design and
Implementation (London, UK) (PLDI 2020). Association for Computing Machinery,
New York, NY, USA, 823–838. https://doi.org/10.1145/3385412.3385963

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).

[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).

[15] Pratik Fegade, Tianqi Chen, Phil Gibbons, and Todd Mowry. 2021. Cortex: A

Compiler for Recursive Deep Learning Models. MLSys (2021).

[16] Basilio B. Fraguela, Jia Guo, Ganesh Bikshandi, María J. Garzarán, Gheorghe
Almási, José Moreira, and David Padua. 2004. The Hierarchically Tiled Arrays
Programming Approach. In Proceedings of the 7th Workshop on Workshop on
Languages, Compilers, and Run-Time Support for Scalable Systems (Houston, Texas,
USA) (LCR ’04). Association for Computing Machinery, New York, NY, USA, 1–12.
https://doi.org/10.1145/1066650.1066657

[17] Bastian Hagedorn, Archibald Samuel Elliott, Henrik Barthels, Rastislav Bodik, and
Vinod Grover. 2020. Fireiron: A Data-Movement-Aware Scheduling Language for
GPUs. In Proceedings of the ACM International Conference on Parallel Architectures
and Compilation Techniques. 71–82.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.

[19] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 (2017).

[20] Intel. 2017.

Intel® Math Kernel Library for Deep Learning Net-
works. https://software.intel.com/en-us/articles/intel-mkl-dnn-part-1-library-
overview-and-installation

[21] Intel. 2019. Introduction to Intel® Deep Learning Boost on Second Generation
Intel® Xeon® Scalable Processors. https://software.intel.com/content/www/
us/en/develop/articles/introduction-to-intel-deep-learning-boost-on-second-
generation-intel-xeon-scalable.html

[22] Andrew Kerr, Haicheng Wu, Manish Gupta, Dustyn Blasig, Pradeep Ramini,
Duane Merrill, Aniket Shivam, Piotr Majcher, Paul Springer, Markus Hohnerbach,
Jin Wang, and Matt Nicely. 2022. CUTLASS. https://github.com/NVIDIA/cutlass
[23] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amaras-
inghe. 2017. The Tensor Algebra Compiler. Proc. ACM Program. Lang. 1, OOPSLA,
Article 77 (Oct. 2017), 29 pages. https://doi.org/10.1145/3133901

[24] Martin Kong, Richard Veras, Kevin Stock, Franz Franchetti, Louis-Noël Pouchet,
and Ponnuswamy Sadayappan. 2013. When polyhedral transformations meet
SIMD code generation. In Proceedings of the 34th ACM SIGPLAN conference on
Programming language design and implementation. 127–138.

[25] Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis,
Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Olek-
sandr Zinenko. 2021. Mlir: Scaling compiler infrastructure for domain specific
computation. In 2021 IEEE/ACM International Symposium on Code Generation and
Optimization (CGO). IEEE, 2–14.

[26] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with
deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).

[27] Hao Lu Marat Dukhan, Yiming Wu. 2018. QNNPACK: Open source library for
https://engineering.fb.com/2018/10/29/ml-

optimized mobile deep learning.
applications/qnnpack/

[28] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).

[29] Thierry Moreau, Tianqi Chen, Ziheng Jiang, Luis Ceze, Carlos Guestrin, and
Arvind Krishnamurthy. 2018. VTA: an open hardware-software stack for deep
learning. arXiv preprint arXiv:1807.04188 (2018).

[30] Nvidia. 2017. NVIDIA Tensor Cores.

https://www.nvidia.com/en-us/data-

center/tensorcore/

[31] Nvidia. 2017. NVIDIA TensorRT: Programmable Inference Accelerator. https:

//developer.nvidia.com/tensorrt

[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
2019. Pytorch: An imperative style, high-performance deep learning library. In
Advances in neural information processing systems. 8026–8037.

[33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI
blog 1, 8 (2019), 9.

[34] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo
Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for
optimizing parallelism, locality, and recomputation in image processing pipelines.
Acm Sigplan Notices 48, 6 (2013), 519–530.

[35] Ira Rosen, Dorit Nuzman, and Ayal Zaks. 2007. Loop-aware SLP in GCC. In GCC

Developers Summit. Citeseer.

[36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).

[37] Ryan Senanayake, Changwan Hong, Ziheng Wang, Amalee Wilson, Stephen
Chou, Shoaib Kamil, Saman Amarasinghe, and Fredrik Kjolstad. 2020. A Sparse
Iteration Space Transformation Framework for Sparse Tensor Algebra. Proc.
ACM Program. Lang. 4, OOPSLA, Article 158 (Nov. 2020), 30 pages.
https:
//doi.org/10.1145/3428226

[38] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).

[39] Philippe Tillet, H. T. Kung, and David Cox. 2019. Triton: An Intermediate Lan-
guage and Compiler for Tiled Neural Network Computations. In Proceedings of
the 3rd ACM SIGPLAN International Workshop on Machine Learning and Program-
ming Languages (Phoenix, AZ, USA) (MAPL 2019). Association for Computing
Machinery, New York, NY, USA, 10–19. https://doi.org/10.1145/3315508.3329973
[40] Nicolas Vasilache, Cédric Bastoul, and Albert Cohen. 2006. Polyhedral code
generation in the real world. In International Conference on Compiler Construction.
Springer, 185–201.

[41] Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal,
Zachary DeVito, William S Moses, Sven Verdoolaege, Andrew Adams, and Albert
Cohen. 2018. Tensor comprehensions: Framework-agnostic high-performance
machine learning abstractions. arXiv preprint arXiv:1802.04730 (2018).

[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. arXiv preprint arXiv:1706.03762 (2017).

[43] Jian Weng, Animesh Jain, Jie Wang, Leyuan Wang, Yida Wang, and Tony
Nowatzki. 2021. UNIT: Unifying tensorized instruction compilation. In 2021
IEEE/ACM International Symposium on Code Generation and Optimization (CGO).
IEEE, 77–89.

[44] Tim Zerrell and Jeremy Bruestle. 2019. Stripe: Tensor compilation via the nested

polyhedral model. arXiv preprint arXiv:1903.06498 (2019).

[45] Jie Zhao, Bojie Li, Wang Nie, Zhen Geng, Renwei Zhang, Xiong Gao, Bin Cheng,
Chen Wu, Yun Cheng, Zheng Li, et al. 2021. AKG: automatic kernel generation
for neural processing units using polyhedral transformations. In Proceedings
of the 42nd ACM SIGPLAN International Conference on Programming Language
Design and Implementation. 1233–1248.

[46] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer
Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, et al. 2020. An-
sor: generating high-performance tensor programs for deep learning. In 14th
{USENIX} Symposium on Operating Systems Design and Implementation ({OSDI}

TensorIR: An Abstraction for Automatic Tensorized Program Optimization

20). 863–879.

[47] Size Zheng, Renze Chen, Anjiang Wei, Yicheng Jin, Qin Han, Liqiang Lu, Bingyang
Wu, Xiuhong Li, Shengen Yan, and Yun Liang. 2022. AMOS: enabling auto-
matic mapping for tensor computations on spatial accelerators with hardware
abstraction. In ISCA ’22: The 49th Annual International Symposium on Com-
puter Architecture, New York, New York, USA, June 18 - 22, 2022, Valentina Sala-
pura, Mohamed Zahran, Fred Chong, and Lingjia Tang (Eds.). ACM, 874–887.

https://doi.org/10.1145/3470496.3527440

[48] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng. 2020. Flex-
Tensor: An Automatic Schedule Exploration and Optimization Framework for
Tensor Computation on Heterogeneous System. In Proceedings of the Twenty-Fifth
International Conference on Architectural Support for Programming Languages and
Operating Systems. 859–873.

