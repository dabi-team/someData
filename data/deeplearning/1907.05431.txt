1
2
0
2

n
a
J

9
1

]

G
L
.
s
c
[

4
v
1
3
4
5
0
.
7
0
9
1
:
v
i
X
r
a

Imitation-Projected Programmatic Reinforcement
Learning

Abhinav Verma∗
Rice University
averma@rice.edu

Hoang M. Le∗
Caltech
hmle@caltech.edu

Yisong Yue
Caltech
yyue@caltech.edu

Swarat Chaudhuri
Rice University
swarat@rice.edu

Abstract

We study the problem of programmatic reinforcement learning, in which policies
are represented as short programs in a symbolic language. Programmatic poli-
cies can be more interpretable, generalizable, and amenable to formal veriﬁcation
than neural policies; however, designing rigorous learning approaches for such
policies remains a challenge. Our approach to this challenge — a meta-algorithm
called PROPEL— is based on three insights. First, we view our learning task as
optimization in policy space, modulo the constraint that the desired policy has a
programmatic representation, and solve this optimization problem using a form of
mirror descent that takes a gradient step into the unconstrained policy space and
then projects back onto the constrained space. Second, we view the unconstrained
policy space as mixing neural and programmatic representations, which enables
employing state-of-the-art deep policy gradient approaches. Third, we cast the pro-
jection step as program synthesis via imitation learning, and exploit contemporary
combinatorial methods for this task. We present theoretical convergence results for
PROPEL and empirically evaluate the approach in three continuous control domains.
The experiments show that PROPEL can signiﬁcantly outperform state-of-the-art
approaches for learning programmatic policies.

1

Introduction

A growing body of work [58, 8, 60] investigates reinforcement learning (RL) approaches that represent
policies as programs in a symbolic language, e.g., a domain-speciﬁc language for composing control
modules such as PID controllers [5]. Short programmatic policies offer many advantages over neural
policies discovered through deep RL, including greater interpretability, better generalization to unseen
environments, and greater amenability to formal veriﬁcation. These beneﬁts motivate developing
effective approaches for learning such programmatic policies.

However, programmatic reinforcement learning (PRL) remains a challenging problem, owing to the
highly structured nature of the policy space. Recent state-of-the-art approaches employ program
synthesis methods to imitate or distill a pre-trained neural policy into short programs [58, 8]. How-
ever, such a distillation process can yield a highly suboptimal programmatic policy — i.e., a large
distillation gap — and the issue of direct policy search for programmatic policies also remains open.

In this paper, we develop PROPEL (Imitation-Projected Programmatic Reinforcement Learning),
a new learning meta-algorithm for PRL, as a response to this challenge. The design of PROPEL
is based on three insights that enables integrating and building upon state-of-the-art approaches
for policy gradients and program synthesis. First, we view programmatic policy learning as a
constrained policy optimization problem, in which the desired policies are constrained to be those
that have a programmatic representation. This insight motivates utilizing constrained mirror descent
approaches, which take a gradient step into the unconstrained policy space and then project back onto
the constrained space. Second, by allowing the unconstrained policy space to have a mix of neural

∗Equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
π(s)
b

::= a | Op(π1(s), . . . , πk(s)) | if b then π1(s) else π2(s) | ⊕θ(π1(s), . . . , πk(s))
::= φ(s) | BOp(b1, . . . , bk)

Figure 1: A high-level syntax for programmatic policies, inspired by [58]. A policy π(s) takes
a state s as input and produces an action a as output. b represents boolean expressions; φ is a
boolean-valued operator on states; Op is an operator that combines multiple policies into one policy;
BOp is a standard boolean operator; and

θ is a “library function" parameterized by θ.

⊕

if (s[TrackPos] < 0.011 and s[TrackPos] > −0.011)

then PID(cid:104)RPM,0.45,3.54,0.03,53.39(cid:105)(s) else PID(cid:104)RPM,0.39,3.54,0.03,53.39(cid:105)(s)

Figure 2: A programmatic policy for acceleration in TORCS [59], automatically discovered by
PROPEL. s[TrackPos] represents the most recent reading from sensor TrackPos.

and programmatic representations, we can employ well-developed deep policy gradient approaches
[55, 36, 47, 48, 19] to compute the unconstrained gradient step. Third, we deﬁne the projection
operator using program synthesis via imitation learning [58, 8], in order to recover a programmatic
policy from the unconstrained policy space. Our contributions can be summarized as:

• We present PROPEL, a novel meta-algorithm that is based on mirror descent, program synthesis,

and imitation learning, for PRL.

• On the theoretical side, we show how to cast PROPEL as a form of constrained mirror descent.
We provide a thorough theoretical analysis characterizing the impact of approximate gradients
and projections. Further, we prove results that provide expected regret bounds and ﬁnite-sample
guarantees under reasonable assumptions.

• On the practical side, we provide a concrete instantiation of PROPEL and evaluate it in three contin-
uous control domains, including the challenging car-racing domain TORCS [59]. The experiments
show signiﬁcant improvements over state-of-the-art approaches for learning programmatic policies.

2 Problem Statement

being the state space,

and a programmatic policy class Π. The deﬁnition of

The problem of programmatic reinforcement learning (PRL) consists of a Markov Decision Process
, P, c, p0, γ) is standard
(MDP)
M
s, a) the probability density function
the action space, P (s(cid:48)
[54], with
|
of transitioning from a state-action pair to a new state, c(s, a) the state-action cost function, p0(s)
a distribution over starting states, and γ
(stochastically) maps states to actions. We focus on continuous control problems, so
assumed to be continuous spaces. The goal is to ﬁnd a programmatic policy π∗

(0, 1) the discount factor. A policy π :

S → A
are
and
A

S
Π such that:

= (

M

A

A

∈

S

S

,

(cid:34)

∞(cid:88)

∈
(cid:35)

π∗ = argmin

J(π),

where: J(π) = E

π

Π

∈

i=0

γic(si, ai

π(si))

,

≡

(1)

p0, the policy decisions, and the
with the expectation taken over the initial state distribution s0 ∼
transition dynamics P . One can also use rewards, in which case (1) becomes a maximization problem.

Programmatic Policy Class. A programmatic policy class Π consists of policies that can be
represented parsimoniously by a (domain-speciﬁc) programming language. Recent work [58, 8, 60]
indicates that such policies can be easier to interpret and formally verify than neural policies, and can
also be more robust to changes in the environment.

In this paper, we consider two concrete classes of programmatic policies. The ﬁrst, a simpliﬁcation of
the class considered in Verma et al. [58], is deﬁned by the modular, high-level language in Figure 1.
θ representing standard controllers, for
This language assumes a library of parameterized functions
instance Proportional-Integral-Derivative (PID) [6] or bang-bang controllers [11]. Programs in the
language take states s as inputs and produce actions a as output, and can invoke fully instantiated
library controllers along with predeﬁned arithmetic, boolean and relational operators. The second,
“lower-level" class, from Bastani et al. [8], consists of decision trees that map states to actions.

⊕

Example. Consider the problem of learning a programmatic policy, in the language of Figure 1, that
controls a car’s accelerator in the TORCS car-racing environment [59]. Figure 2 shows a program
, where j
in our language for this task. The program invokes PID controllers PID
(cid:104)

j,jtarget ,θP ,θI ,θD

(cid:105)

2

Algorithm 1 Imitation-Projected Programmatic Reinforcement Learning (PROPEL)

1: Input: Programmatic & Neural Policy Classes: Π & F.
2: Input: Either initial π0 or initial f0
3: Deﬁne joint policy class: H ≡ Π ⊕ F
4: if given initial f0 then
π0 ← PROJECT(f0)
5:
6: end if
7: for t = 1, . . . , T do
8:
9:
10: end for
11: Return: Policy πT

ht ← UPDATEF (πt−1, η)
πt ← PROJECTΠ(ht)

//h ≡ π + f deﬁned as h(s) = π(s) + f (s)

//program synthesis via imitation learning

//policy gradient in neural policy space with learning rate η

//program synthesis via imitation learning

identiﬁes the sensor (out of 29, in our experiments) that provides inputs to the controller, jtarget
is a learned parameter that is the desired target for the value of sensor j, and θP , θI , and θD are
respectively the real-valued coefﬁcients of the proportional, integral, and derivative terms in the
controller. We note that the program only uses the sensors TrackPos and RPM. While TrackPos (for
the position of the car relative to the track axis) is used to decide which controller to use, only the
RPM sensor is needed to calculate the acceleration.

Learning Challenges. Learning programmatic policies in the continuous RL setting is challenging,
as the best performing methods utilize policy gradient approaches [55, 36, 47, 48, 19], but policy
gradients are hard to compute in programmatic representations. In many cases, Π may not even be
differentiable. For our approach, we only assume access to program synthesis methods that can select
a programmatic policy π
Π that minimizes imitation disagreement with demonstrations provided by
a teaching oracle. Because imitation learning tends to be easier than general RL in long-horizon tasks
[53], the task of imitating a neural policy with a program is, intuitively, signiﬁcantly simpler than
the full programmatic RL problem. This intuition is corroborated by past work on programmatic RL
[58], which shows that direct search over programs often fails to meet basic performance objectives.

∈

3 Learning Algorithm

resides within a larger space of policies

To develop our approach, we take the viewpoint of (1) being a constrained optimization problem,
where Π
using a mixing of programmatic policies Π and neural polices
be invoked as h(s) = π(s) + f (s). In general, we assume that
for each π

. In particular, we will represent
Π
⊕ F
π + f can
. Any mixed policy h
is a good approximation of Π (i.e.,

that approximates it well), which we formalize in Section 4.

Π there is some f

H ≡
≡

⊂ H

F
F

H

∈

∈ F

We can now frame our constrained learning problem as minimizing (1) over Π
, that alternate
and projecting back down onto Π. This “lift-and-
between taking a gradient step in the general space
H
project” perspective motivates viewing our problem via the lens of mirror descent [40]. In standard
mirror descent, the unconstrained gradient step can be written as h
J(hprev) for step
size η, and the projection can be written as π

η
Π D(π(cid:48), h) for divergence measure D.

argminπ(cid:48)

hprev

⊂ H

∇H

←

−

←

∈

Our approach, Imitation-Projected Programmatic Reinforcement Learning (PROPEL), is outlined
in Algorithm 1 (also see Figure 3). PROPEL is a meta-algorithm that requires instantiating two
subroutines, UPDATE and PROJECT, which correspond to the standard update and projection steps,
respectively. PROPEL can be viewed as a form of functional mirror descent with some notable
deviations from vanilla mirror descent.

.

F

F

UPDATE
Since policy gradient methods are well-
developed for neural policy classes
(e.g., [36, 47, 48, 30,
24, 19]) and non-existent for programmatic policy classes Π,
PROPEL is designed to leverage policy gradients in
and
avoid policy gradients in Π. Algorithm 2 shows one instanti-
. Note that standard mirror descent takes
ation of UPDATE
unconstrained gradient steps in
, and we
discuss this discrepancy between UPDATE
in Section 4.

and UPDATE

rather than

H

F

F

H

F

F

PROJECTΠ. Projecting onto Π can be implemented using
program synthesis via imitation learning, i.e., by synthesiz-
ing a π
Π to best imitate demonstrations provided by a

∈

Figure 3: Depicting the PROPEL meta-
algorithm.

3

H<latexit sha1_base64="FjVpgZ9sYnF4JX/Xxf2CItINeeg=">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjNV0GXRTZcV7AOmQ8mkmTY0kwzJHaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3knNPmAhuwHW/ndLG5tb2Tnm3srd/cHhUPT7pGpVqyjpUCaX7ITFMcMk6wEGwfqIZiUPBeuH0Pvd7T0wbruQjzBIWxGQsecQpASv5g5jAhBKRtebDas2tuwvgdeIVpIYKtIfVr8FI0TRmEqggxviem0CQEQ2cCjavDFLDEkKnZMx8SyWJmQmyReQ5vrDKCEdK2ycBL9TfGxmJjZnFoZ3MI5pVLxf/8/wUotsg4zJJgUm6/ChKBQaF8/vxiGtGQcwsIVRzmxXTCdGEgm2pYkvwVk9eJ91G3buqNx6ua827oo4yOkPn6BJ56AY1UQu1UQdRpNAzekVvDjgvzrvzsRwtOcXOKfoD5/MHfD+RYg==</latexit>UpdateF<latexit sha1_base64="uVEU6tZiG0qwOqgTWDNX6SrYYHs=">AAACBnicbVBNS8NAEN34WetX1KMIwSJ4KkkV7LEgiMcKpi20IWw2m3bp5oPdiVBCTl78K148KOLV3+DNf+MmzUFbHww83pthZp6XcCbBNL+1ldW19Y3N2lZ9e2d3b18/OOzJOBWE2iTmsRh4WFLOImoDA04HiaA49Djte9Prwu8/UCFZHN3DLKFOiMcRCxjBoCRXPxmFGCYMMjvxMdDczUqBYJ7d5LmrN8ymWcJYJlZFGqhC19W/Rn5M0pBGQDiWcmiZCTgZFsAIp3l9lEqaYDLFYzpUNMIhlU5WvpEbZ0rxjSAWqiIwSvX3RIZDKWehpzqLG+WiV4j/ecMUgraTsShJgUZkvihIuQGxUWRi+ExQAnymCCaCqVsNMsECE1DJ1VUI1uLLy6TXaloXzdbdZaPTruKooWN0is6Rha5QB92iLrIRQY/oGb2iN+1Je9HetY9564pWzRyhP9A+fwCaZZnQ</latexit>UpdateF<latexit sha1_base64="uVEU6tZiG0qwOqgTWDNX6SrYYHs=">AAACBnicbVBNS8NAEN34WetX1KMIwSJ4KkkV7LEgiMcKpi20IWw2m3bp5oPdiVBCTl78K148KOLV3+DNf+MmzUFbHww83pthZp6XcCbBNL+1ldW19Y3N2lZ9e2d3b18/OOzJOBWE2iTmsRh4WFLOImoDA04HiaA49Djte9Prwu8/UCFZHN3DLKFOiMcRCxjBoCRXPxmFGCYMMjvxMdDczUqBYJ7d5LmrN8ymWcJYJlZFGqhC19W/Rn5M0pBGQDiWcmiZCTgZFsAIp3l9lEqaYDLFYzpUNMIhlU5WvpEbZ0rxjSAWqiIwSvX3RIZDKWehpzqLG+WiV4j/ecMUgraTsShJgUZkvihIuQGxUWRi+ExQAnymCCaCqVsNMsECE1DJ1VUI1uLLy6TXaloXzdbdZaPTruKooWN0is6Rha5QB92iLrIRQY/oGb2iN+1Je9HetY9564pWzRyhP9A+fwCaZZnQ</latexit>UpdateF<latexit sha1_base64="uVEU6tZiG0qwOqgTWDNX6SrYYHs=">AAACBnicbVBNS8NAEN34WetX1KMIwSJ4KkkV7LEgiMcKpi20IWw2m3bp5oPdiVBCTl78K148KOLV3+DNf+MmzUFbHww83pthZp6XcCbBNL+1ldW19Y3N2lZ9e2d3b18/OOzJOBWE2iTmsRh4WFLOImoDA04HiaA49Djte9Prwu8/UCFZHN3DLKFOiMcRCxjBoCRXPxmFGCYMMjvxMdDczUqBYJ7d5LmrN8ymWcJYJlZFGqhC19W/Rn5M0pBGQDiWcmiZCTgZFsAIp3l9lEqaYDLFYzpUNMIhlU5WvpEbZ0rxjSAWqiIwSvX3RIZDKWehpzqLG+WiV4j/ecMUgraTsShJgUZkvihIuQGxUWRi+ExQAnymCCaCqVsNMsECE1DJ1VUI1uLLy6TXaloXzdbdZaPTruKooWN0is6Rha5QB92iLrIRQY/oGb2iN+1Je9HetY9564pWzRyhP9A+fwCaZZnQ</latexit>UpdateF<latexit sha1_base64="uVEU6tZiG0qwOqgTWDNX6SrYYHs=">AAACBnicbVBNS8NAEN34WetX1KMIwSJ4KkkV7LEgiMcKpi20IWw2m3bp5oPdiVBCTl78K148KOLV3+DNf+MmzUFbHww83pthZp6XcCbBNL+1ldW19Y3N2lZ9e2d3b18/OOzJOBWE2iTmsRh4WFLOImoDA04HiaA49Djte9Prwu8/UCFZHN3DLKFOiMcRCxjBoCRXPxmFGCYMMjvxMdDczUqBYJ7d5LmrN8ymWcJYJlZFGqhC19W/Rn5M0pBGQDiWcmiZCTgZFsAIp3l9lEqaYDLFYzpUNMIhlU5WvpEbZ0rxjSAWqiIwSvX3RIZDKWehpzqLG+WiV4j/ecMUgraTsShJgUZkvihIuQGxUWRi+ExQAnymCCaCqVsNMsECE1DJ1VUI1uLLy6TXaloXzdbdZaPTruKooWN0is6Rha5QB92iLrIRQY/oGb2iN+1Je9HetY9564pWzRyhP9A+fwCaZZnQ</latexit>UpdateF<latexit sha1_base64="uVEU6tZiG0qwOqgTWDNX6SrYYHs=">AAACBnicbVBNS8NAEN34WetX1KMIwSJ4KkkV7LEgiMcKpi20IWw2m3bp5oPdiVBCTl78K148KOLV3+DNf+MmzUFbHww83pthZp6XcCbBNL+1ldW19Y3N2lZ9e2d3b18/OOzJOBWE2iTmsRh4WFLOImoDA04HiaA49Djte9Prwu8/UCFZHN3DLKFOiMcRCxjBoCRXPxmFGCYMMjvxMdDczUqBYJ7d5LmrN8ymWcJYJlZFGqhC19W/Rn5M0pBGQDiWcmiZCTgZFsAIp3l9lEqaYDLFYzpUNMIhlU5WvpEbZ0rxjSAWqiIwSvX3RIZDKWehpzqLG+WiV4j/ecMUgraTsShJgUZkvihIuQGxUWRi+ExQAnymCCaCqVsNMsECE1DJ1VUI1uLLy6TXaloXzdbdZaPTruKooWN0is6Rha5QB92iLrIRQY/oGb2iN+1Je9HetY9564pWzRyhP9A+fwCaZZnQ</latexit>UpdateF<latexit sha1_base64="uVEU6tZiG0qwOqgTWDNX6SrYYHs=">AAACBnicbVBNS8NAEN34WetX1KMIwSJ4KkkV7LEgiMcKpi20IWw2m3bp5oPdiVBCTl78K148KOLV3+DNf+MmzUFbHww83pthZp6XcCbBNL+1ldW19Y3N2lZ9e2d3b18/OOzJOBWE2iTmsRh4WFLOImoDA04HiaA49Djte9Prwu8/UCFZHN3DLKFOiMcRCxjBoCRXPxmFGCYMMjvxMdDczUqBYJ7d5LmrN8ymWcJYJlZFGqhC19W/Rn5M0pBGQDiWcmiZCTgZFsAIp3l9lEqaYDLFYzpUNMIhlU5WvpEbZ0rxjSAWqiIwSvX3RIZDKWehpzqLG+WiV4j/ecMUgraTsShJgUZkvihIuQGxUWRi+ExQAnymCCaCqVsNMsECE1DJ1VUI1uLLy6TXaloXzdbdZaPTruKooWN0is6Rha5QB92iLrIRQY/oGb2iN+1Je9HetY9564pWzRyhP9A+fwCaZZnQ</latexit>Project⇧<latexit sha1_base64="yJrGS/uwBUglQp5t/VvEbT6FBk0=">AAACD3icbVC7SgNBFJ2Nrxhfq5Y2i0GxCrtR0DJoYxnBPCAbwuzkJhkz+2DmrhiW/QMbf8XGQhFbWzv/xtlkC008MHA49zHnHi8SXKFtfxuFpeWV1bXiemljc2t7x9zda6owlgwaLBShbHtUgeABNJCjgHYkgfqegJY3vsrqrXuQiofBLU4i6Pp0GPABZxS11DOPXYQHnO5JPBFDmrg+xRHHpC7DO2CY9tw6T3tm2a7YU1iLxMlJmeSo98wvtx+y2IcAmaBKdRw7wm5CJXImIC25sYKIsjEdQkfTgPqgusnUR2odaaVvDUKpX4DWVP09kVBfqYnv6c7MrJqvZeJ/tU6Mg4tuwoMoRgjY7KNBLCwMrSwcq8+lPllMNKFMcu3VYiMqKUMdYUmH4MyfvEia1YpzWqnenJVrl3kcRXJADskJccg5qZFrUicNwsgjeSav5M14Ml6Md+Nj1low8pl98gfG5w9vop4p</latexit>⇧<latexit sha1_base64="wssQ+renNbqLlT7M0IUVY1jBbiA=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeClx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+54lrI2L1iNOE+xEdKREKRtFKD/2mGJQrbtVdgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGepohE3frY4dUYurDIkYaxtKSQL9fdERiNjplFgOyOKY7PqzcX/vF6K4a2fCZWkyBVbLgpTSTAm87/JUGjOUE4toUwLeythY6opQ5tOyYbgrb68Ttq1qndVrd1fV+qNPI4inME5XIIHN1CHBjShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwAiOY23</latexit>Algorithm 2 UPDATE

: neural policy gradient for mixed policies

Input: Reference programmatic policy: π

Input: Regularization parameter: λ

//any standard randomized initialization

F

1: Input: Neural Policy Class F.
2: Input: Step size: η.
3: Initialize neural policy: f0
4: for j = 1, . . . , m do
5:
6: end for
7: Return: h ≡ π + λfm

fj ← fj−1 − ηλ∇F J(π + λfj−1)

//using DDPG [36], TRPO [47], etc., holding π ﬁxed

Algorithm 3 PROJECTΠ: program synthesis via imitation learning
1: Input: Programmatic Policy Class: Π.
Input: Oracle policy: h
2: Roll-out h on environment, get trajectory: τ0 = (s0, h(s0), s1, h(s1), . . .)
3: Create supervised demonstration set: Γ0 = {(s, h(s))} from τ0
4: Derive π0 from Γ0 via program synthesis
5: for k = 1, . . . , M do
6:
7:
8:
9:
10: end for
11: Return: πM

Roll-out πk−1, creating trajectory: τk
Collect demonstration data: Γ(cid:48) = {(s, h(s))|s ∈ τk}
Γk ← Γ(cid:48) ∪ Γk−1
Derive πk from Γk via program synthesis

//DAgger-style imitation learning [46]

//e.g., using methods in [58, 8]

//e.g., using methods in [58, 8]

∈ H

teaching oracle h
. Recent work [58, 8, 60] has given practical heuristics for this task for various
programmatic policy classes. Algorithm 3 shows one instantiation of PROJECTΠ (based on DAgger
[46]). One complication that arises is that ﬁnite-sample runs of such imitation learning approaches
only return approximate solutions and so the projection is not exact. We characterize the impact of
approximate projections in Section 4.

Practical Considerations. In practice, we often employ multiple gradient steps before taking a
projection step (as also described in Algorithm 2), because the step size of individual (stochastic) gra-
dient updates can be quite small. Another issue that arises in virtually all policy gradient approaches
is that the gradient estimates can have very high variance [55, 33, 30]. We utilize low-variance policy
gradient updates by using the reference π as a proximal regularizer in function space [19].

For the projection step (Algorithm 3), in practice we often retain all previous roll-outs τ from all
previous projection steps. It is straightforward to query the current oracle h to provide demonstrations
on the states s
τ from previous roll-outs, which can lead to substantial savings in sample complexity
with regards to executing roll-outs on the environment, while not harming convergence.

∈

4 Theoretical Analysis

We start by viewing PROPEL through the lens of online learning in function space, independent
of the speciﬁc parametric representation. This start point yields a convergence analysis of Alg.
1 in Section 4.1 under generic approximation errors. We then analyze the issues of policy class
representation in Sections 4.2 and 4.3, and connect Algorithms 2 and 3 with the overall performance,
under some simplifying conditions. In particular, Section 4.3 characterizes the update error in a
possibly non-differentiable setting; to our knowledge, this is the ﬁrst such analysis of its kind for
reinforcement learning.

F

v, u

to be subspaces of an ambient policy space

Preliminaries. We consider Π and
vector space equipped with inner product
u
sup
. The cost functional of a policy u is J(u) = (cid:82)
(cid:107) ≤
{(cid:104)
and u, v
distribution of states induced by u. The joint policy class is
Π, f
affecting the analysis, we simply equate

, which is a
u
v
=
(cid:107)
R
, and standard scaling & addition: (au + bv)(s) = au(s) + bv(s) for a, b
1
}
c(s, u(s))dµu(s), where µu is the
= Π

S
∈
⊕ F
H
, and inherits its vector space properties. Without
for the remainder of the paper.

U
u, u
, dual norm
(cid:105)
(cid:104)

is a subspace of

, induced norm

.2 Note that

π + f
{

(cid:105)| (cid:107)
∈ U

∈ F}

(cid:107)∗
∈

,
(cid:104)·

, by

(cid:112)

H

H

|∀

=

=

·(cid:105)

π

(cid:107)

(cid:107)

U
U ≡ H

We assume that J is convex in

, which implies that subgradient ∂J(h) exists (with respect to
) [9]. Where J is differentiable, we utilize the notion of a Fréchet gradient. Recall that a
if

is called a Fréchet functional gradient of J at h

H
bounded linear operator

H

:

H (cid:55)→ H
2The operator ⊕ is not a direct sum, since Π and F are not orthogonal.

∇

∈ H

4

J(h+g)

J(h),g

lim
g
0
(cid:107)
(cid:107)→
respect to

−

J(h)
g
(cid:107)
, whereas

−(cid:104)∇
(cid:107)

H

∇F

(cid:105)

= 0. By default,

(or

for emphasis) denotes the gradient with

∇
deﬁnes the gradient in the restricted subspace

∇H

.

F

4.1 PROPEL as (Approximate) Functional Mirror Descent

For our analysis, PROPEL can be viewed as approximating mirror descent in (inﬁnite-dimensional)
.3 Similar to the ﬁnite-dimensional setting [40], we choose a
function space over a convex set Π
strongly convex and smooth functional regularizer R to be the mirror map. From the approximate
mirror descent perspective, for each iteration t:

⊂ H

1. Obtain a noisy gradient estimate: (cid:98)
∇
space:
2. UPDATE
−
3. Obtain approximate projection: πt = PROJECT

t
−
R(ht) =

1 ≈ ∇
R(πt
∇

(π) in

H

∇

H

−

J(πt
1)
−
R
Π(ht)

1)
η (cid:98)
∇
≈

1 (Note UPDATE
t
−
argminπ

Π DR(π, ht)
∈

H (cid:54)

= UPDATE

)

F

−

R(v)

2
h
DR(u, v) = R(u)
(cid:107)
will recover projected functional gradient descent in L2-space. Here UPDATE becomes ht =
2. While we mainly focus on
πt
(cid:107)
this choice of R in our experiments, note that other selections of R lead to different UPDATE and
PROJECT operators (e.g., minimizing KL divergence if R is negative entropy).

is a Bregman divergence. Taking R(h) = 1

1), and PROJECT solves for argminπ

π
Π (cid:107)

R(u), u

1 −
−

η (cid:98)
∇

− (cid:104)∇

J(πt

2 (cid:107)

ht

−

−

v

−

(cid:105)

∈

H

and UPDATE

The functional mirror descent scheme above may encounter two additional sources of error compared
to standard mirror descent [40]. First, in the stochastic setting (also called bandit feedback [28]), the
t may be biased, in addition to having high variance. One potential source of
gradient estimate (cid:98)
∇
. Second, the PROJECT step may be inexact. We
bias is the gap between UPDATE
F
start by analyzing the behavior of PROPEL under generic bias, variance, and projection errors, before
and PROJECTΠ by Algs. 2 & 3, respectively.
discussing the implications of approximating UPDATE
(cid:13)
E[ (cid:98)
(cid:13)
Let the bias be bounded by β, i.e.,
β almost surely. Similarly let the
t
(cid:13)
∇
variance of the gradient estimate be bounded by σ2, and the projection error norm
π∗t (cid:107) ≤
We state the expected regret bound below; more details and a proof appear in Appendix A.2.
Theorem 4.1 (Expected regret bound under gradient estimation and projection errors). Let π1, . . . , πT
be a sequence of programmatic policies returned by Algorithm 1, and π∗ be the optimal programmatic
policy. Choosing learning rate η =

T + (cid:15)), we have the expected regret over T iterations:

H
J(πt)

(cid:13)
(cid:13)
(cid:13)
∗

πt]
|

πt
(cid:107)

(cid:113) 1

− ∇

≤

−

(cid:15).

(cid:34)

E

1
T

T
(cid:88)

t=1

J(πt)

−

J(π∗) = O

σ

(cid:32)

(cid:114)

(cid:33)

+ (cid:15) + β

.

1
T

(2)

σ2 ( 1
(cid:35)

The result shows that error (cid:15) from PROJECT and the bias β do not accumulate and simply contribute
an additive term on the expected regret.4 The effect of variance of gradient estimate decreases at a
(cid:112)
1/T rate. Note that this regret bound is agnostic to the speciﬁc UPDATE and PROJECT operations,

and can be applied more generically beyond the speciﬁc algorithmic choices used in our paper.

4.2 Finite-Sample Analysis under Vanilla Policy Gradient Update and DAgger Projection

Next, we show how certain instantiations of UPDATE and PROJECT affect the magnitude of errors and
inﬂuence end-to-end learning performance from ﬁnite samples, under some simplifying assumptions
.
on the UPDATE step. For this analysis, we simplify Alg. 2 into the case UPDATE
F ≡
Rk, and
H
In particular, we assume programmatic policies in Π to be parameterized by a vector θ
is parameterized in Rk). We further
π is differentiable in θ (e.g., we can view Π
⊂ F
assume the trajectory roll-out is performed in an exploratory manner, where action is taken uniformly
random over ﬁnite set of A actions, thus enabling the bound on the bias of gradient estimates via
Bernstein’s inequality. The PROJECT step is consistent with Alg. 3, i.e., using DAgger [45] under
convex imitation loss, such as (cid:96)2 loss. We have the following high-probability guarantee:
Theorem 4.2 (Finite-sample guarantee). At each iteration, we perform vanilla policy gradient
) using m trajectories and, use DAgger algorithm to collect M roll-outs for the
estimate of π (over

UPDATE

where

F

∈

H

3Π can be convexiﬁed by considering randomized policies, as stochastic combinations of π ∈ Π (cf. [35]).
4Other mirror descent-style analyses, such as in [52], lead to accumulation of errors over the rounds of
learning T . One key difference is that we are leveraging the assumption of convexity of J in the (inﬁnite-
dimensional) function space representation.

5

imitation learning projection. Setting the learning rate η =
rounds of the algorithm, we have that:

(cid:114)

1
σ2

(cid:0) 1
T + H

M +

(cid:113) log(T /δ)
M

(cid:1), after T



(cid:115)

J(πt) − J(π∗) ≤ O

σ

1
T

+

H
M

+

(cid:114)

log(T /δ)
M



(cid:32)

(cid:114)

 + O

σ

log(T k/δ)
m

+

AH log(T k/δ)
m

(cid:33)

1
T

T
(cid:88)

t=1

holds with probability at least 1
σ2 the variance of policy gradient estimates, and k the dimension Π’s parameterization.

δ, with H being the task horizon, A the cardinality of action space,

−

The expanded result and proof are included in Appendix A.3. The proof leverages previous analysis
from DAgger [46] and the ﬁnite sample analysis of vanilla policy gradient algorithm [32]. The
ﬁnite-sample regret bound scales linearly with the standard deviation σ of the gradient estimate, while
the bias, which is the very last component of the RHS, scales linearly with the task horizon H. Note
that the standard deviation σ can be exponential in task horizon H in the worst case [32], and so it is
important to have practical implementation strategies to reduce the variance of the UPDATE operation.
While conducted in a stylized setting, this analysis provides insight in the relative trade-offs of
spending effort in obtaining more accurate projections versus more reliable gradient estimates.

4.3 Closing the gap between UPDATE

and UPDATE

F

H

∇H

J(π) in the

H
space. On the other hand, Algorithm 2 performs UPDATE

(π) involves
Our functional mirror descent analysis rests on taking gradients in
(π) only in
estimating
. In either case, although J(π) may be differentiable in the non-parametric
the neural policy space
ambient policy space, it may not be possible to obtain a differentiable parametric programmatic
representation in Π. In this section, we discuss theoretical motivations to addressing a practical issue:
How do we deﬁne and approximate the gradient
J(π) under a parametric representation? To our
knowledge, we are the ﬁrst to consider such a theoretical question for reinforcement learning.

: UPDATE

∇H

H

H

F

F

(π) (Line 8 of Alg. 1) is
J(f ), which has a differentiable representation, at some f close to π

J(π). The idea in UPDATE

∇H

F

, we show that this approximation is valid.

∇H

J(π) by

Deﬁning a consistent approximation of
to approximate
(under the norm). Under appropriate conditions on
Proposition 4.3. Assume that (i) J is Fréchet differentiable on
, and (iii)
the restricted subspace
ﬁxed policy π
limk

H
Π, deﬁne a sequence of policies fk
= 0. We then have limk
J(fk)

is dense in

∈ F

∇F

fk

∈
π

F

F

F

(i.e., the closure

H

, (ii) J is also differentiable on
). Then for any
H
, k = 1, 2, . . .), that converges to π:

=

F
= 0.

J(π)

(cid:107)

−

→∞ (cid:107)

→∞ (cid:107)∇F

Since the Fréchet gradient is unique in the ambient space

J(π) as k
→ ∞
J(π) via differentiable space

− ∇H
,
→
∇F
(by Proposition 4.3). We thus have an asymptotically unbiased approximation of
∇H
J(fk).5 Connecting to
∇H
the result from Theorem 4.1, let σ2 be an upper bound on the policy gradient estimates in the neural
policy class
J(π), the expected regret
bound becomes E

∇F
, under an asymptotically unbiased approximation of

∀
J(π) (cid:44) limk

(cid:107)∗
k we have

J(π) (cid:44)

J(fk) =

→∞ ∇F

J(fk)

(cid:113) 1

J(π∗) = O

(cid:105)
t=1 J(πt)

(cid:17)
T + (cid:15)

(cid:104) 1
T

(cid:80)T

∇H

∇H

∇H

as:

H

F

F

(cid:16)

σ

.

−

F

(π) To further theoretically motivate a practical strategy
Bias-variance considerations of UPDATE
(π) in Algorithm 2, we utilize an equivalent proximal perspective of mirror descent
for UPDATE
+ DR(h, π).
[10], where UPDATE
Proposition 4.4 (Minimizing a relaxed objective). For a ﬁxed programmatic policy π, with sufﬁciently
small constant λ
∈
η

(π) is equivalent to solving for h(cid:48) = argminh

(0, 1), we have that

J(π), h
(cid:105)

J(cid:0)π + λf (cid:1)

+ DR(h, π)

J(π), h)

J(π), π

J(π) +

(cid:104)∇H

(3)

∈H

η

H

F

min
h
∈H

(cid:104)∇H

(cid:105)

≤

min
f
∈F

−

(cid:104)∇

(cid:105)

. Each gradient descent update step is now f (cid:48) = f

step is obtained by minimizing the RHS of (3), i.e., minimizing J(π + λf )
Thus, a relaxed UPDATE
over f
J(πt + λf ), corresponding
∇F
to Line 5 of Algorithm 2. For ﬁxed π and small λ, this relaxed optimization problem becomes
, which is signiﬁcantly easier. Functional regularization in
regularized policy optimization over
policy space around a ﬁxed prior controller π has demonstrated signiﬁcant reduction in the variance

∈ F

ηλ

−

F

H

5We do not assume J(π) to be differentiable when restricting to the policy subspace Π, i.e., ∇ΠJ(π) may

not exist under policy parameterization of Π.

6

of gradient estimate [19], at the expense of some bias. The below expected regret bound summarizes
the impact of this increased bias and reduced variance, with details included in Appendix A.5.

Proposition 4.5 (Bias-variance characterization of UPDATE
over
leads to the expected regret bound: E

J(h) is L-Lipschitz continuous, approximating UPDATE
(cid:16)
(cid:80)T

). Assuming J(h) is L-strongly smooth
by UPDATEF per Alg. 2
(cid:113) 1
T + (cid:15) + λ2L2(cid:17)
H
λσ

J(π∗) = O

t=1 J(πt)

(cid:104) 1
T

, i.e.,

∇H

H

(cid:105)

F

.

−

Compared to the idealized unbiased approximation in Proposition 4.3, the introduced bias here is
related to the inherent smoothness property of cost functional J(h) over the joint policy class
, i.e.,
around π.
how close J(π + λf ) is to its linear under-approximation J(π) +

J(π), λf

H

(cid:104)∇H

(cid:105)

5 Experiments

We demonstrate the effectiveness of PROPEL in synthesizing programmatic controllers in three
continuous control environments. For brevity and focus, this section primarily focuses on TORCS6, a
challenging race car simulator environment [59]. Empirical results on two additional classic control
tasks, Mountain-Car and Pendulum, are provided in Appendix B; those results follow similar trends
as the ones described for TORCS below, and further validate the convergence analysis of PROPEL.

Experimental Setup. We evaluate over ﬁve distinct
tracks in the TORCS simulator. The difﬁculty of a
track can be characterized by three properties; track
length, track width, and number of turns. Our suite
of tracks provides environments with varying levels of
difﬁculty for the learning algorithm. The performance
of a policy in the TORCS simulator is measured by the
lap time achieved on the track. To calculate the lap
time, the policies are allowed to complete a three-lap
race, and we record the best lap time during this race.
We perform the experiments with twenty-ﬁve random
seeds and report the median lap time over these twenty-
ﬁve trials. Some of the policies crash the car before
completing a lap on certain tracks, even after training
for 600 episodes. Such crashes are recorded as a lap
time of inﬁnity while calculating the median. If the policy crashes for more than half the seeds, this
is reported as CR in Tables 1 & 2. We choose to report the median because taking the crash timing as
inﬁnity, or an arbitrarily large constant, heavily skews other common measures such as the mean.

Figure 4: Median lap-time improvements
during multiple iterations of PROPELPROG
over 25 random seeds.

Baselines. Among recent state-of-the-art approaches
to learning programmatic policies are NDPS [58] for
high-level language policies, and VIPER [8] for learn-
ing tree-based policies. Both NDPS and VIPER rely
on imitating a ﬁxed (pre-trained) neural policy oracle,
and can be viewed as degenerate versions of PROPEL
that only run Lines 4-6 in Algorithm 1. We present
two PROPEL analogues to NDPS and VIPER: (i) PRO-
PELPROG: PROPEL using the high-level language of
Figure 1 as the class of programmatic policies, similar
to NDPS. (ii) PROPELTREE: PROPEL using regres-
sion trees, similar to VIPER. We also report results for
PRIOR, which is a (sub-optimal) PID controller that is
also used as the initial policy in PROPEL. In addition, to
study generalization ability as well as safety behavior
during training, we also include DDPG, a neural policy learned using the Deep Deterministic Policy
Gradients [36] algorithm, with 600 episodes of training for each track. In principle, PROPEL and its
analysis can accommodate different policy gradient subroutines. However, in the TORCS domain,
other policy gradient algorithms such as PPO and TRPO failed to learn policies that are able to
complete the considered tracks. We thus focus on DDPG as our main policy gradient component.

Figure 5: Median number of crashes during
training of DDPG and PROPELPROG over
25 random seeds.

6The code for the TORCS experiments can be found at: https://bitbucket.org/averma8053/propel

7

012345Iterations050100150200Lap Time ImprovementG-TrackE-RoadAalborgRuudskogenAlpine-212345Track ID0100200300400500600Number of CrashesMax EpisodesDDPGPROPEL-ProgTable 1: Performance results in TORCS over 25 random seeds. Each entry is formatted as Lap-time /
Crash-ratio, reporting median lap time in seconds over all the seeds (lower is better) and ratio of
seeds that result in crashes (lower is better). A lap time of CR indicates the agent crashed and could
not complete a lap for more than half the seeds.

LENGTH

PRIOR
DDPG
NDPS
VIPER
PROPELPROG
PROPELTREE

G-TRACK
3186M

E-ROAD
3260M

AALBORG
2588M

RUUDSKOGEN
3274M

ALPINE-2
3774M

312.92 / 0.0
78.82 / 0.24
108.25 / 0.24
83.60 / 0.24
93.67 / 0.04
78.33 / 0.04

322.59 / 0.0
89.71 / 0.28
126.80 / 0.28
87.53 / 0.28
119.17 / 0.04
79.39 / 0.04

244.19 / 0.0
101.06 / 0.40
163.25 / 0.40
110.57 / 0.40
147.28 / 0.12
109.83 / 0.16

340.29 / 0.0
CR / 0.68
CR / 0.68
CR / 0.68
124.58 / 0.16
118.80 / 0.24

402.89 / 0.0
CR / 0.92
CR / 0.92
CR / 0.92
256.59 / 0.16
236.01 / 0.36

Table 2: Generalization results in TORCS, where rows are training and columns are testing tracks.
Each entry is formatted as PROPELPROG / DDPG, and the number reported is the median lap time in
seconds over all the seeds (lower is better). CR indicates the agent crashed and could not complete a
lap for more than half the seeds.

G-TRACK

E-ROAD

AALBORG

RUUDSKOGEN ALPINE-2

G-TRACK
E-ROAD
AALBORG
RUUDSKOGEN
ALPINE-2

-
102 / 92
201 / 91
131 / CR
222 / CR

124 / CR
-
228 / CR
135 / CR
231 / CR

CR / CR
CR / CR
-
CR / CR
184 / CR

CR / CR
CR / CR
217 / CR
-
CR / CR

CR / CR
CR / CR
CR / CR
CR / CR
-

Evaluating Performance. Table 1 shows the performance on the considered TORCS tracks. We
see that PROPELPROG and PROPELTREE consistently outperform the NDPS [58] and VIPER [8]
baselines, respectively. While DDPG outperforms PROPEL on some tracks, its volatility causes it
to be unable to learn in some environments, and hence to crash the majority of the time. Figure 4
shows the consistent improvements made over the prior by PROPELPROG, over the iterations of the
PROPEL algorithm. Appendix B contains similar results achieved on the two classic control tasks,
MountainCar and Pendulum. Figure 5 shows that, compared to DDPG, our approach suffers far fewer
crashes while training in TORCS.

Evaluating Generalization. To compare the ability of the controllers to perform on tracks not seen
during training, we executed the learned policies on all the other tracks (Table 2). We observe that
DDPG crashes signiﬁcantly more often than PROPELPROG. This demonstrates the generalizability
of the policies returned by PROPEL. Generalization results for the PROPELTREE policy are given
in the appendix. In general, PROPELTREE policies are more generalizable than DDPG but less than
PROPELPROG. On an absolute level, the generalization ability of PROPEL still leaves much room for
improvement, which is an interesting direction for future work.

Veriﬁability of Policies. As shown in prior work [8, 58], parsimonious programmatic policies are
more amenable to formal veriﬁcation than neural policies. Unsurprisingly, the policies generated by
PROPELTREE and PROPELPROG are easier to verify than DDPG policies. As a concrete example,
we veriﬁed a smoothness property of the PROPELPROG policy using the Z3 SMT-solver [21] (more
details in Appendix B). The veriﬁcation terminated in 0.49 seconds.

Initialization. In principle, PROPEL can be initialized with a random program, or a random policy
trained using DDPG. In practice, the performance of PROPEL depends to a certain degree on the
stability of the policy gradient procedure, which is DDPG in our experiments. Unfortunately, DDPG
often exhibits high variance across trials and fares poorly in challenging RL domains. Speciﬁcally, in
our TORCS experiments, DDPG fails on a number of tracks (similar phenomena have been reported in
previous work that experiments on similar continuous control domains [30, 19, 58]). Agents obtained
by initializing PROPEL with neural policies obtained via DDPG also fail on multiple tracks. Their
performance over the ﬁve tracks is reported in Appendix B. In contrast, PROPEL can often ﬁnish the
challenging tracks when initialized with a very simple hand-crafted programmatic prior.

8

6 Related Work

Program Synthesis. Program synthesis is the problem of automatically searching for a program
within a language that ﬁts a given speciﬁcation [29]. Recent approaches to the problem have leveraged
symbolic knowledge about program structure [27], satisﬁability solvers [50, 31], and meta-learning
techniques [39, 41, 22, 7] to generate interesting programs in many domains [3, 42, 4]. In most prior
work, the speciﬁcation is a logical constraint on the input/output behavior of the target program.
However, there is also a growing body of work that considers program synthesis modulo optimality
objectives [13, 15, 43], often motivated by machine learning tasks [39, 57, 26, 23, 58, 8, 60].
Synthesis of programs that imitates an oracle has been considered in both the logical [31] and the
optimization [58, 8, 60] settings. The projection step in PROPEL builds on this prior work. While our
current implementation of this step is entirely symbolic, in principle, the operation can also utilize
contemporary techniques for learning policies that guide the synthesis process [39, 7, 49].

Constrained Policy Learning. Constrained policy learning has seen increased interest in recent
years, largely due to the desire to impose side guarantees such as stability and safety on the policy’s
behavior. Broadly, there are two approaches to imposing constraints: specifying constraints as an
additional cost function [1, 35], and explicitly encoding constraints into the policy class [2, 34, 19,
20, 12]. In some cases, these two approaches can be viewed as duals of each other. For instance,
recent work that uses control-theoretic policies as a functional regularizer [34, 19] can be viewed
from the perspective of both regularization (additional cost) and an explicitly constrained policy class
(a speciﬁc mix of neural and control-theoretic policies). We build upon this perspective to develop
the gradient update step in our approach.

RL using Imitation Learning. There are two ways to utilize imitation learning subroutines within
RL. First, one can leverage limited-access or sub-optimal experts to speed up learning [44, 18, 14, 51].
Second, one can learn over two policy classes (or one policy and one model class) to achieve
accelerated learning compared to using only one policy class [38, 17, 52, 16]. Our approach has
some stylistic similarities to previous efforts [38, 52] that use a richer policy space to search for
improvements before re-training the primary policy to imitate the richer policy. One key difference is
that our primary policy is programmatic and potentially non-differentiable. A second key difference
is that our theoretical framework takes a functional gradient descent perspective — it would be
interesting to carefully compare with previous analysis techniques to ﬁnd a unifying framework.

RL with Mirror Descent. The mirror descent framework has previously used to analyze and
design RL algorithms. For example, Thomas et al. [56] and Mahadevan and Liu [37] use composite
objective mirror descent, or COMID [25], which allows incorporating adaptive regularizers into
gradient updates, thus offering connections to either natural gradient RL [56] or sparsity inducing RL
algorithms [37]. Unlike in our work, these prior approaches perform projection into the same native,
differentiable representation. Also, the analyses in these papers do not consider errors introduced by
hybrid representations and approximate projection operators. However, one can potentially extend
our approach with versions of mirror descent, e.g., COMID, that were considered in these efforts.

7 Conclusion and Future Work

We have presented PROPEL, a meta-algorithm based on mirror descent, program synthesis, and
imitation learning, for programmatic reinforcement learning (PRL). We have presented theoretical
convergence results for PROPEL, developing novel analyses to characterize approximate projections
and biased gradients within the mirror descent framework. We also validated PROPEL empirically, and
show that it can discover interpretable, veriﬁable, generalizable, performant policies and signiﬁcantly
outperform the state of the art in PRL.

The central idea of PROPEL is the use of imitation learning and combinatorial methods in implement-
ing a projection operation for mirror descent, with the goal of optimization in a functional space that
lacks gradients. While we have developed PROPEL in an RL setting, this idea is not restricted to RL
or even sequential decision making. Future work will seek to exploit this insight in other machine
learning and program synthesis settings.

Acknowledgements. This work was supported in part by United States Air Force Contract # FA8750-19-C-0092,
NSF Award # 1645832, NSF Award # CCF-1704883, the Okawa Foundation, Raytheon, PIMCO, and Intel.

9

References

[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages
22–31. JMLR. org, 2017.

[2] Mohammed Alshiekh, Roderick Bloem, Rüdiger Ehlers, Bettina Könighofer, Scott Niekum,
and Ufuk Topcu. Safe reinforcement learning via shielding. In Thirty-Second AAAI Conference
on Artiﬁcial Intelligence, 2018.

[3] Rajeev Alur, Rastislav Bodík, Eric Dallal, Dana Fisman, Pranav Garg, Garvit Juniwal, Hadas
Kress-Gazit, P. Madhusudan, Milo M. K. Martin, Mukund Raghothaman, Shambwaditya Saha,
Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa.
Syntax-guided synthesis. In Dependable Software Systems Engineering, pages 1–25. 2015.

[4] Rajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. Scaling enumerative program synthesis
via divide and conquer. In Tools and Algorithms for the Construction and Analysis of Systems -
23rd International Conference, TACAS 2017, Held as Part of the European Joint Conferences
on Theory and Practice of Software, ETAPS 2017, Uppsala, Sweden, April 22-29, 2017,
Proceedings, Part I, pages 319–336, 2017.

[5] Kiam Heong Ang, Gregory Chong, and Yun Li. Pid control system analysis, design, and

technology. IEEE transactions on control systems technology, 13(4):559–576, 2005.

[6] Karl Johan Åström and Tore Hägglund. Automatic tuning of simple regulators with speciﬁca-

tions on phase and amplitude margins. Automatica, 20(5):645–651, 1984.

[7] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tar-
low. Deepcoder: Learning to write programs. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,
2017.

[8] Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Veriﬁable reinforcement learning via
policy extraction. In Advances in Neural Information Processing Systems, pages 2494–2504,
2018.

[9] Heinz H Bauschke, Patrick L Combettes, et al. Convex analysis and monotone operator theory

in Hilbert spaces, volume 408. Springer, 2011.

[10] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters, 31(3):167–175, 2003.

[11] Richard Bellman, Irving Glicksberg, and Oliver Gross. On the “bang-bang” control problem.

Quarterly of Applied Mathematics, 14(1):11–18, 1956.

[12] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in neural information processing
systems, pages 908–918, 2017.

[13] Roderick Bloem, Krishnendu Chatterjee, Thomas A. Henzinger, and Barbara Jobstmann. Better
In Computer Aided Veriﬁcation, 21st
quality in synthesis through quantitative objectives.
International Conference, CAV 2009, Grenoble, France, June 26 - July 2, 2009. Proceedings,
pages 140–156, 2009.

[14] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daumé III, and John Langford.
Learning to search better than your teacher. In International Conference on Machine Learning
(ICML), 2015.

[15] Swarat Chaudhuri, Martin Clochard, and Armando Solar-Lezama. Bridging boolean and

quantitative synthesis using smoothed proof search. In POPL, pages 207–220, 2014.

[16] Ching-An Cheng, Xinyan Yan, Nathan Ratliff, and Byron Boots. Predictor-corrector policy

optimization. In International Conference on Machine Learning (ICML), 2019.

[17] Ching-An Cheng, Xinyan Yan, Evangelos Theodorou, and Byron Boots. Accelerating imitation
learning with predictive models. In International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), 2019.

[18] Ching-An Cheng, Xinyan Yan, Nolan Wagener, and Byron Boots. Fast policy learning through

imitation and reinforcement. In Uncertainty in artiﬁcial intelligence, 2019.

10

[19] Richard Cheng, Abhinav Verma, Gabor Orosz, Swarat Chaudhuri, Yisong Yue, and Joel Burdick.
Control regularization for reduced variance reinforcement learning. In International Conference
on Machine Learning (ICML), 2019.

[20] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.

[21] Leonardo Mendonça de Moura and Nikolaj Bjørner. Z3: An Efﬁcient SMT Solver. In TACAS,

pages 337–340, 2008.

[22] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed,
and Pushmeet Kohli. Robustﬁll: Neural program learning under noisy i/o. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70, pages 990–998. JMLR.
org, 2017.

[23] Tao Du, Jeevana Priya Inala, Yewen Pu, Andrew Spielberg, Adriana Schulz, Daniela Rus,
Armando Solar-Lezama, and Wojciech Matusik. Inversecsg: automatic conversion of 3d models
to CSG trees. ACM Trans. Graph., 37(6):213:1–213:16, 2018.

[24] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking
deep reinforcement learning for continuous control. In International Conference on Machine
Learning, pages 1329–1338, 2016.

[25] John C Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari. Composite objective

mirror descent. In COLT, pages 14–26, 2010.

[26] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. Learning to infer
graphics programs from hand-drawn images. In Advances in Neural Information Processing
Systems, pages 6059–6068, 2018.

[27] John K. Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations
In Proceedings of the 36th ACM SIGPLAN Conference on
from input-output examples.
Programming Language Design and Implementation, Portland, OR, USA, June 15-17, 2015,
pages 229–239, 2015.

[28] Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimiza-
tion in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth
annual ACM-SIAM symposium on Discrete algorithms, pages 385–394. Society for Industrial
and Applied Mathematics, 2005.

[29] Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. Program synthesis. Foundations and

Trends in Programming Languages, 4(1-2):1–119, 2017.

[30] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David
In Thirty-Second AAAI Conference on

Meger. Deep reinforcement learning that matters.
Artiﬁcial Intelligence, 2018.

[31] Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. Oracle-guided component-
based program synthesis. In Proceedings of the 32nd ACM/IEEE International Conference on
Software Engineering-Volume 1, pages 215–224. ACM, 2010.

[32] Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD

thesis, University of London London, England, 2003.

[33] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information

processing systems, pages 1008–1014, 2000.

[34] Hoang M. Le, Andrew Kang, Yisong Yue, and Peter Carr. Smooth imitation learning for online
sequence prediction. In International Conference on Machine Learning (ICML), 2016.

[35] Hoang M Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In

International Conference on Machine Learning (ICML), 2019.

[36] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.

[37] Sridhar Mahadevan and Bo Liu. Sparse q-learning with mirror descent. In Proceedings of
the Twenty-Eighth Conference on Uncertainty in Artiﬁcial Intelligence, pages 564–573. AUAI
Press, 2012.

11

[38] William H Montgomery and Sergey Levine. Guided policy search via approximate mirror
descent. In Advances in Neural Information Processing Systems, pages 4008–4016, 2016.

[39] Vijayaraghavan Murali, Swarat Chaudhuri, and Chris Jermaine. Neural sketch learning for

conditional program generation. In ICLR, 2018.

[40] Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method

efﬁciency in optimization. 1983.

[41] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and
Pushmeet Kohli. Neuro-symbolic program synthesis. arXiv preprint arXiv:1611.01855, 2016.

[42] Oleksandr Polozov and Sumit Gulwani. Flashmeta: a framework for inductive program
synthesis. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-
Oriented Programming, Systems, Languages, and Applications, OOPSLA 2015, part of SPLASH
2015, Pittsburgh, PA, USA, October 25-30, 2015, pages 107–126, 2015.

[43] Veselin Raychev, Pavol Bielik, Martin T. Vechev, and Andreas Krause. Learning programs
from noisy data. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, POPL 2016, St. Petersburg, FL, USA, January 20 - 22,
2016, pages 761–774, 2016.

[44] Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive

no-regret learning. arXiv preprint arXiv:1406.5979, 2014.

[45] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proceedings of the fourteenth international
conference on artiﬁcial intelligence and statistics, pages 627–635, 2011.

[46] Stéphane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proceedings of the Fourteenth International
Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April
11-13, 2011, pages 627–635, 2011.

[47] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pages 1889–1897,
2015.

[48] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[49] Xujie Si, Yuan Yang, Hanjun Dai, Mayur Naik, and Le Song. Learning a meta-solver for
syntax-guided program synthesis. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.

[50] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodík, Sanjit A. Seshia, and Vijay A. Saraswat.

Combinatorial sketching for ﬁnite programs. In ASPLOS, pages 404–415, 2006.

[51] Wen Sun, J Andrew Bagnell, and Byron Boots. Truncated horizon policy search: Combin-
ing reinforcement learning & imitation learning. In International Conference on Learning
Representations (ICLR), 2018.

[52] Wen Sun, Geoffrey J Gordon, Byron Boots, and J Bagnell. Dual policy iteration. In Advances

in Neural Information Processing Systems, pages 7059–7069, 2018.

[53] Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply
aggrevated: Differentiable imitation learning for sequential prediction. In International Confer-
ence on Machine Learning (ICML), 2017.

[54] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,

2018.

[55] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
In Advances in neural

methods for reinforcement learning with function approximation.
information processing systems, pages 1057–1063, 2000.

[56] Philip S Thomas, William C Dabney, Stephen Giguere, and Sridhar Mahadevan. Projected
natural actor-critic. In Advances in neural information processing systems, pages 2337–2345,
2013.

12

[57] Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat Chaudhuri.
Houdini: Lifelong learning as program synthesis. In Advances in Neural Information Processing
Systems, pages 8687–8698, 2018.

[58] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri.
Programmatically interpretable reinforcement learning. In International Conference on Machine
Learning, pages 5052–5061, 2018.

[59] Bernhard Wymann, Eric Espié, Christophe Guionneau, Christos Dimitrakakis, Rémi Coulom,

and Andrew Sumner. TORCS, The Open Racing Car Simulator. http://www.torcs.org, 2014.

[60] He Zhu, Zikang Xiong, Stephen Magill, and Suresh Jagannathan. An inductive synthesis frame-
work for veriﬁable reinforcement learning. In ACM Conference on Programming Language
Design and Implementation (SIGPLAN), 2019.

13

U

A Theoretical Analysis

A.1 Preliminaries and Notations

u

(cid:107)∗

,
·(cid:105)
(cid:104)·
= sup
{(cid:104)

:
U × U (cid:55)→
v, u
(cid:107) ≤
(cid:105)| (cid:107)

We formally deﬁne an ambient control policy space
to be a vector space equipped with inner
(cid:112)
R, which induces a norm
u, u
product
, and its dual norm deﬁned as
=
(cid:105)
(cid:104)
v
. While multiple ways to deﬁne the inner product exist, for concreteness
1
(cid:107)
}
u(s)v(s)ds.
u, v
we can think of the example of square-integrable stationary policies with
(cid:104)
The addition operator + between two policies u, v
is deﬁned as (u + v)(s) = u(s) + v(s) for
all state s
The cost functional of a control policy u is deﬁned as J(u) = (cid:82) ∞
(cid:82)

0 c(s(τ ), u(τ ))dτ , or J(u) =
c(s, u(s))dµu(s), where µu is the distribution of states induced by policy u. This latter example

. Scaling λu + κv is deﬁned similarly for scalar λ, κ.

U
u
(cid:107)
(cid:107)

= (cid:82)

∈ U

∈ S

(cid:105)

S

is equivalent to the standard notion of value function in reinforcement learning.
S

Separate from the parametric representation issues, both programmatic policy class Π and neural
, and by extension - the joint policy class
policy class
, are considered to live in the ambient
vector space
. We thus have a common and well-deﬁned notion of distance between policies from
different classes.

F
U

H

We make an important distinction between differentiability of J(h) in the ambient policy space
(non-parametric), versus differentiability in parameterization (parametric). For example, if Π is a
class of decision-tree based policy, policies in Π may not be differentiable under representation.
However, policies π
Π might still be differentiable when considered as points in the ambient vector
space

∈

.

j

We will use the following standard notion of gradient and differentiability from functional analysis:
Deﬁnition A.1 (Subgradients). The subgradient of J at h, denoted ∂J(h), is the non-empty set
g
{
Deﬁnition A.2 (Fréchet gradient). A bounded linear operator
gradient of J at h
= 0
lim
g
(cid:107)→

is called Fréchet functional

J(j)
}

H (cid:55)→ H

+ J(h)

−(cid:104)∇
(cid:107)

∈ H|∀

J(h)
g

J(h+g)

∈ H

∈ H

J(h),g

h, g

∇

≤

−

if

−

j

(cid:104)

(cid:105)

(cid:107)

(cid:107)

0

:

:

(cid:105)

The notions of convexity, smoothness and Bregman divergence are analogous to ﬁnite-dimensional
setting:
Deﬁnition A.3 (Strong convexity). A differentiable function R is α
if R(y)
x
Deﬁnition A.4 (Lipschitz continuous gradient smoothness). A differentiable function R is
LR
Deﬁnition A.5 (Bregman Divergence). For a strongly convex regularizer R, DR(x, y) = R(x)
is the Bregman divergence between x and y (not necessarily symmetric)
R(y)

strongly smooth w.r.t norm

strongly convex w.r.t norm

R(x) +

R(y), x

R(x), y

R(x)

R(y)

(cid:107)∗ ≤

+ α

x
(cid:107)

− ∇

2
(cid:107)

2 (cid:107)

LR

(cid:107)∇

(cid:107)·(cid:107)

(cid:107)·(cid:107)

(cid:104)∇

−

−

−

−

−

−

≥

if

x

(cid:107)

y

y

y

(cid:105)

− (cid:104)∇

−

(cid:105)

The following standard result for Bregman divergence will be useful:
Lemma A.1. [10] For all x, y, z we have the identity
DR(z, x)
that DR(z, x)

= DR(x, y) +
DR(z, y). Since Bregman divergence is non-negative, a consequence of this identity is

DR(z, y)

R(y), x

R(y), z

R(x)

R(x)

− ∇

(cid:104)∇

−

−

x

z

(cid:105)

−

≤ (cid:104)∇

− ∇

−

(cid:105)

A.2 Expected Regret Bound under Noisy Policy Gradient Estimates and Projection Errors

In this section, we show regret bound for the performance of the sequence of returned programs
π1, . . . , πT of the algorithm. The analysis here is agnostic to the particular implementation of
algorithm 2 and algorithm 3.

Let R be a α
from algorithm 1 can be described as follows.

strongly convex and LR

−

−

smooth functional with respect to norm

on

H

(cid:107)·(cid:107)

. The steps

• Initialize π0 ∈

Π. For each iteration t:

J(πt
1. Obtain a noisy estimate of the gradient (cid:98)
∇
R(πt
2. Update in the
space:
1)
3. Obtain approximate projection πt = PROJECT

R(ht) =

H

∇

∇

−

1)

−

≈ ∇
η (cid:98)
∇
−
R
π (ht)

J(πt

1)

J(πt
−
1)
argminpi
∈

−

≈

Π DR(π, ht)

14

This procedure is an approximate functional mirror descent scheme under bandit feedback. We will
develop the following result, which is a more detailed version of 4.1 in the main paper.

(cid:107)

−

π(cid:48)

π
(cid:107)

(i.e., D =
In the statement below, D is the diameter on Π with respect to deﬁned norm
(cid:107)·(cid:107)
. β, σ2 are the bound on
). LJ is the Lipschitz constant of the functional J on
sup
the bias and variance of the gradient estimate at each iteration, respectively. α and R are the strongly
convex and smooth coefﬁcients of the functional regularizer R. Finally, (cid:15) is the bound on the
projection error with respect to the same norm
.
(cid:107)·(cid:107)
Theorem A.2 (Regret bound of returned policies). Let π1, . . . , πT be a sequence of programmatic
policies returned by algorithm 1 and π∗ be the optimal programmatic policy. We have the expected
regret bound:

H

(cid:34)

E

1
T

T
(cid:88)

t=1

(cid:35)

J(πt)

J(π∗)

−

≤

In particular, choosing the learning rate η =

LRD2
ηT
(cid:113) 1

+

(cid:15)LRD
η

+

η(σ2 + L2
J )
α

+ βD

T +(cid:15)
σ2 , the expected regret is simpliﬁed into:

(cid:34)

E

1
T

T
(cid:88)

(cid:35)

J(πt)

(cid:32)

(cid:114)

J(π∗) = O

σ

−

1
T

(cid:33)

+ (cid:15) + β

(4)

t=1
πt] be the conditional expectation of the gradient estimate.
t = E[ (cid:98)
|
∇
J(πt). Denote the upper-bound on the bias of the
t =
∇
βt almost surely. Denote the noise of the gradient estimate by

∇

t

∇

Proof. At each round t, let
We will use the shorthand notation
estimate by βt, i.e., (cid:13)
(cid:13)
− ∇
∇
t = E(cid:2) (cid:13)
(cid:13)
t, and σ2
ξt =
(cid:13) (cid:98)
∇
is (cid:15)

∗ ≤
t

(cid:98)
∇

(cid:13)
(cid:13)

−

t

t

t

t

(cid:13)
2
(cid:13)
(cid:13)
∗

(cid:3) is the variance of gradient estimate (cid:98)
∇

t.

∇

− ∇
approximate in the sense that (cid:13)
−
(cid:13)
(cid:13)
R
Π(ht)
(cid:13)
(cid:13) ≤

The projection operator
(cid:13)
(cid:13)
(cid:92)PROJECT
(cid:13)
(cid:13)
ror of the imitation learning procedure. This projection error in general is independent of the choice
R
Π(ft) for the true
of function classes Π and
Bregman projection of ht onto Π.

.We will use the shorthand notation π∗t = PROJECT

(cid:15) with some constant (cid:15), which reﬂects the statistical er-

Π(ft)(cid:13)

R
Π(ht)

PROJECT

PROJECT

(cid:13) =

(cid:13)πt

−

−

F

R

Due to convexity of J over the space

(which includes Π), we have for all π

Π:

∈

H
J(πt)

J(π)

t, πt

π

(cid:105)

−

≤ (cid:104)∇

−

We proceed to bound the RHS, starting with bounding the inner product where the actual gradient is
replaced by the estimated gradient.
1
ηt (cid:104)∇

R(ht+1), πt

R(πt)

(cid:98)
∇
(cid:104)

− ∇

t, πt

(5)

−

=

π

π

−
(cid:105)
(cid:0)DR(π, πt)

(cid:105)
DR(π, ht+1) + DR(πt, ht+1)(cid:1)

(cid:0)DR(π, πt)

DR(π, π∗t+1)

−

DR(π∗t+1, ht+1) + DR(πt, ht+1)(cid:1)

−

−

=

≤

=

1
ηt
1
ηt
1
ηt

(cid:0) DR(π, πt)
(cid:124)

DR(π, πt+1)
−
(cid:123)(cid:122)
(cid:125)
telescoping

(cid:124)

+ DR(π, πt+1)

DR(π, π∗t+1)
(cid:125)

−
(cid:123)(cid:122)
projection error

DR(π∗t+1, ht+1) + DR(πt, ht+1)
(cid:123)(cid:122)
(cid:125)
relative improvement

−
(cid:124)

(cid:1)

(8)
Equation (5) is due to the gradient update rule in
space. Equation (6) is derived from deﬁnition
of Bregman divergence. Equation (7) is due to the generalized Pythagorean theorem of Bregman
R
projection DR(x, y)
Π(x), y). The RHS of equation (7)
are decomposed into three components that will be bounded separately.

F
R
Π(x)) + DR(PROJECT

DR(x, PROJECT

≥

Bounding projection error. By lemma (A.1) we have

DR(π, πt+1)

DR(π, π∗t+1)

−

≤ (cid:104)∇

R(πt+1)

R(π∗t+1), π

πt+1(cid:105)

−

− ∇
R(π∗t+1)(cid:13)
(cid:13)

π

πt+1(cid:107)∗

−

(cid:107)

(cid:13)
(cid:13)

∇

≤

R(πt+1)

− ∇

15

(6)

(7)

(9)

(10)

(11)
R and

∇

LR

(cid:13)
(cid:13)πt+1 −

≤

π∗t+1

(cid:13)
(cid:13) D

≤

(cid:15)LRD

Equation (10) is due to Cauchy–Schwarz. Equation (11) is due to Lipschitz smoothness of
deﬁnition of (cid:15)

approximate projection.

−

Bounding relative improvement. This follows standard argument from analysis of mirror descent
algorithm.

R(π∗t+1) +

(cid:104)∇
R(ht+1), π∗t+1 −

R(ht+1), π∗t+1 −
πt
(cid:105)

(cid:104)∇

+

πt

(cid:105)

(12)

(13)

(14)

DR(πt, ht+1)

≤ (cid:104)∇
ηt
=

DR(π∗t+1, ht+1) = R(πt)
−
α
(cid:13)
(cid:13)
2
πt
(cid:13)π∗t+1 −
π∗t+1(cid:105) −
(cid:13)
2
∗
α
(cid:13)
πt
(cid:13)π∗t+1 −
πt
2
t + L2
J )

(cid:13)
2
(cid:13)

(cid:105) −

−
R(πt), πt

−
t, π∗t+1 −
η2
(cid:13)
2
t
(cid:13)
(cid:13)
α
∗

(cid:98)
(cid:104)
∇
(cid:13)
(cid:13)
(cid:13) (cid:98)
∇

∈

≤

−
η2
t
2α
≤
Equation (13) is from the α
strong convexity property of regularizer R. Equation (14) is by deﬁnition
of the gradient update. Combining the bounds on the three components and taking expectation, we
thus have

(15)

(σ2

≤

−

t

(cid:104)

E

t, πt

(cid:98)
∇
(cid:104)

−

π

(cid:105)
(cid:105)

≤

1
ηt

DR(π, πt)

−

DR(π, πt+1) + (cid:15)LRD +

η2
t
α

(cid:18)

(cid:19)

(σ2

t + L2
J )

(16)

Next, the difference between estimated gradient (cid:98)
∇
Cauchy-Schwarz:

t and actual gradient

∇

t factors into the bound via

E[ (cid:98)
∇
−
The results can be deduced from equations (16) and (17).

t, πt

(cid:98)
∇

(cid:104)∇

−

≤

−

E

π

t

t

(cid:105)
(cid:105)

(cid:13)
(cid:13)
(cid:13)∇

(cid:104)

t]

(cid:13)
(cid:13)
(cid:13)
∗

πt

(cid:107)

−

π

(cid:107) ≤

βtD

(17)

Unbiased gradient estimates. For the case when the gradient estimate is unbiased, assume the
variance of the noise of gradient estimates is bounded by σ2, we have the expected regret bound for
all pi

Π

(cid:34)

E

1
T

T
(cid:88)

t=1

(cid:35)

J(πt)

J(π)

−

≤

LRD2
ηT

+

(cid:15)LRD
η

+

η(σ2 + L2
J )
α

(18)

here to clarify, LR is the smoothness coefﬁcient of regularizer R (i.e., the gradient of R is LR-
, σ2 is the upper-
Lipschitz, LJ is Lipschitz constant of J, D is the diameter of Π under norm
bound on the variance of gradient estimates, and (cid:15) is the error from the projection procedure (i.e.,
imitation learning loss).

(cid:107)·(cid:107)

We can set learning rate η =

(cid:113) 1

T +(cid:15)
σ2

to observe that the expected regret is bounded by O(σ

(cid:113) 1

T + (cid:15)).

Biased gradient estimates. Assume that the bias of gradient estimate at each round is upper-bounded
by βt

β. Similar to before, combining inequalities from (16) and (17), we have

(cid:34)

E

1
T

T
(cid:88)

t=1

(cid:35)

J(πt)

J(π)

−

≤

LRD2
ηT

+

(cid:15)LRD
η

+

η(σ2 + L2
J )
α

+ βD

(19)

Similar to before, we can set learning rate η =

(cid:113) 1

T +(cid:15)
σ2

to observe that on the expected regret is

bounded by O(σ
incurred per bound is simply a constant, and does not depend on T .

T + (cid:15) + β). Compared to the bound on (18), in the biased case, the extra regret

(cid:113) 1

A.3 Finite-Sample Analysis

In this section, we provide overall ﬁnite-sample analysis for PROPEL under some simplifying
assumptions. We ﬁrst consider the case where exact gradient estimate is available, before extending
the result to the general case of noisy policy gradient update. Combining the two steps will give us
the proof for the following statement (theorem 4.2 in the main paper)
Theorem A.3 (Finite-sample guarantee). At each iteration, we perform vanilla policy gradient
) using m trajectories and use DAgger algorithm to collect M roll-outs. Setting
estimate of π (over

H

16

the learning rate η =

(cid:114)

1
σ2

(cid:0) 1
T + H

M +

(cid:113) log(T /δ)
M

(cid:1), after T rounds of the algorithm, we have that



(cid:115)

J(πt) − J(π∗) ≤ O

σ

1
T

+

H
M

+

(cid:114)

log(T /δ)
M



(cid:32)

(cid:114)

 + O

σ

log(T k/δ)
m

+

AH log(T k/δ)
m

(cid:33)

1
T

T
(cid:88)

t=1

holds with probability at least 1
the variance of policy gradient estimates, and k the dimension Π’s parameterization.

δ, with H the task horizon, A the cardinality of action space, σ2

−

Exact gradient estimate case. Assuming that the policy gradients can be calculated exactly, it is
straight-forward to provide high-probability guarantee for the effect of the projection error. We start
with the following result, adapted from [45] for the case of projection error bound. In this version
of DAgger, we assume that we only collect a single (state, expert action) pair from each trajectory
roll-out. Result is similar, with tighter bound, when multiple data points are collected along the
trajectory.

Lemma A.4 (Projection error bound from imitation learning procedure). Using DAgger as the
imitation learning sub-routine for our PROJECT operator in algorithm 3, let M be the number of
δ,
trajectories rolled-out for learning, and H be the horizon of the task. With probability at least 1
we have

−

DR(π, π∗)

(cid:101)O(1/M ) +

≤

2(cid:96)max(1 + H)
M

+

2(cid:96)max log(1/δ))
M

(cid:114)

where π is the result of PROJECT, π∗ is the true Bregman projection of h onto Π, and (cid:96)max is the
maximum value of the imitation learning loss function DR(
,
·

)

·

The bound in lemma A.4 is simpler than previous imitation learning results with cost information
([44, 45]. The reason is that the goal of the PROJECT operator is more modest. Since we only care
about the distance between the empirical projection π and the true projection π∗, the loss objective
in imitation learning is simpliﬁed (i.e., this is only a regret bound), and we can disregard how well
policies in Π can imitate the expert h, as well as the performance of J(π) relative to the true cost
from the environment J(h).

A consequence of this lemma is that for the number of trajectories at each round of imitation learning
M = O( log 1/δ
δ. Applying union
≤
bound across T rounds of learning, we obtain the following guarantee (under no gradient estimation
error)

(cid:15) ), we have DR(πt, π∗t )

(cid:15) with probability at least 1

) + O( H

−

(cid:15)2

Proposition A.5 (Finite-sample Projection Error Bound). To simplify the presentation of the result, we
consider LR, D, L, α to be known constants. Using DAgger algorithm to collect M = O( log T /δ
) +
O( H
(cid:15) ) roll-outs at each iteration, we have the following regret guarantee after T rounds of our main
algorithm:

(cid:15)2

1
T

T
(cid:88)

t=1

J(πt)

J(π∗)

O

≤

−

(cid:18) 1
ηT
(cid:114)

(cid:19)

+ η

+

(cid:15)
η

δ. Consequently, setting η =

1

T + H

M +

T
(cid:88)

1
T

J(πt)

J(π∗)

O



≤

−



(cid:115)

1
T

+

H
M

+

(cid:114)

log(T /δ)
M



(cid:113) log(T /δ)

M , we have that


with probability at least 1

−

t=1
with probability at least 1

δ

−

Note that the dependence on the time horizon of the task is sub-linear. This is different from standard
imitation learning regret bounds, which are often at least linear in the task horizon. The main reason
is that our comparison benchmark π∗ does live in the space Π, whereas for DAgger, the expert policy
may not reside in the same space.

Noisy gradient estimate case. We now turn to the issue of estimating the gradient of
make the following simplifying assumption about the gradient estimation:

∇

J(π). We

17

• The π is parameterized by vector θ

Rk (such as a neural network). The parameterization
is differentiable with respect to θ (Alternatively, we can view Π as a differentiable subspace
of

, in which case we have

∈
)

=

F

H

F

• At each UPDATE loop, the policy is rolled out m times to collect the data, each trajectory

has horizon length H
• For each visited state s

space is ﬁnite with cardinality A.

∼

dh, the policy takes a uniformly random action a. The action

• The gradient

∇

hθ is bounded by B

The gradient estimate is performed consistent with a generic policy gradient scheme, i.e.,
H
(cid:88)

m
(cid:88)

J(θ) =

(cid:98)
∇

A
m

πθ(aj
i |

i , θ) (cid:98)Qj
sj

i

∇

i=1

j=1

i is the estimated cost-to-go [55].

where (cid:98)Qj
Taking uniform random exploratory actions ensures that the samples are i.i.d. We can thus apply
Bernstein’s inequality to obtain the bound between estimated gradient and the true gradient. Indeed,
with probability at least 1

δ, we have that the following bound on the bias component-wise:

−

(cid:13)
(cid:13)
(cid:13) (cid:98)
≤
∇
which leads to similar bound with respect to
dimensional setting):

J(θ)

J(θ)

− ∇

(cid:13)
(cid:13)
(cid:13)

∞

β when m

(2σ2 + 2AHB β

3 ) log k

δ

≥

β2

(here we leverage the equivalence of norms in ﬁnite

(cid:107)·(cid:107)∗

(cid:13)
(cid:13)
(cid:13)∇

t

t

(cid:98)
∇

−

(cid:13)
(cid:13)
(cid:13)
∗

≤

β when m = O

(cid:32)

(σ2 + AHBβ) log k
δ
β2

(cid:33)

Applying union bound of this result over T rounds of learning, and combining with the result from
proposition (A.5), we have the following ﬁnite-sample guarantee in the simplifying policy gradient
update. This is also the more detailed statement of theorem 4.2 in the main paper.
Proposition A.6 (Finite-sample Guarantee under Noisy Gradient Updates and Projection Error). At
each iteration, we perform policy gradient estimate using m = O( (σ2+AHBβ) log T k
) trajectories
and use DAgger algorithm to collect M = O( log T /δ
(cid:15) ) roll-outs. Setting the learning rate
(cid:113) log(T /δ)
M

) + O( H
(cid:1), after T rounds of the algorithm, we have that

(cid:0) 1
T + H

M +

η =

1
σ2

(cid:114)

β2

(cid:15)2

δ



(cid:115)

J(πt)

J(π∗)

O

σ

≤

−

1
T

+

H
M

+

(cid:114)

log(T /δ)
M



 + β

1
T

T
(cid:88)

t=1

with probability at least 1

δ.

−
Consequently, we also have the following regret bound:


(cid:115)

1
T

T
(cid:88)

t=1

J(πt)

−

J(π∗)

≤

O

σ

(cid:114)

1
T

+

+

H
M

log(T /δ)
M



(cid:32)

(cid:114)

+O

σ

log(T k/δ)
m

+

AH log(T k/δ)
m

(cid:33)

holds with probability at least 1
space, and k is the dimension of function class Π’s parameterization.

−

δ, where again H is the task horizon, A is the cardinality of action

Proof. (For both proposition (A.6) and (A.5)). The results follow by taking the inequality from
equation (19), and by solving for (cid:15) and β explicitly in terms of relevant quantities. Based on the
speciﬁcation of M and m, we obtain the necessary precision for each round of learning in terms of
number of trajectories:

β = O(σ

log(k/δ)
m
(cid:114)

+

AHB log(k/δ)
m

)

(cid:15) = O(

+

H
M

log(1/δ)
M

)

18

Setting the learning rate η =
bounds.

(cid:113) 1
σ2

T + (cid:15)(cid:1) and rearranging the inequalities lead to the desired
(cid:0) 1

The regret bound depends on the variance σ2 of the policy gradient estimates. It is well-known that
vanilla policy gradient updates suffer from high variance. We instead use functional regularization
technique, based on CORE-RL, in the practical implementation of our algorithm. The CORE-RL
subroutine has been demonstrated to reduce the variance in policy gradient updates [19].

A.4 Deﬁning a consistent approximation of

J(π) - Proof of Proposition 4.3

∇H
We are using the notion of Fréchet derivative to deﬁne gradient of differentiable functional. Note that
while Gateaux derivative can also be utilized, Fréchet derivative ensures continuity of the gradient
operator that would be useful for our analysis.

Deﬁnition A.6 (Fréchet gradient). A bounded linear operator
gradient of J at h
= 0
lim
g
(cid:107)→

−(cid:104)∇
(cid:107)

J(h)
g

J(h+g)

∈ H

J(h),g

if

−

(cid:107)

(cid:107)

0

(cid:105)

:

∇

H (cid:55)→ H

is called Fréchet functional

that we consider have the property that a programmatic policy π

. One interpretation of this assumption is that the
Π can

∈

We make the following assumption about
space of policies Π and
be well-approximated by a large space of neural policies f
Assumption 1. J is Fréchet differentiable on
=

(i.e., the closure

is dense in

. And

and

H

F

F

H
H

F

.

∈ F

. J is also differentiable on the restricted subspace
)

H

F
F
It is then clear that
∀
gradient of f in the ambient space
f
, π + f is not necessarily in
can be deﬁned asymptotically.

∈ F

∈ F

f

H
F

the Fréchet gradient

J(f ), restricted to the subspace

∇F

F
(since Fréchet gradient is unique). In general, given π
. However, the restricted gradient on subspace

is equal to the
Π and
of J(π + f )

∈

F

Proposition A.7. Fixing a policy π
converges to π: limk

fk

Π, deﬁne a sequence of policies fk
J(fk)

= 0, we then have limk

∈

g

→∞ (cid:107)

−

(cid:107)

→∞ (cid:107)∇F

∈ F
− ∇H

, k = 1, 2, . . . that
J(π)

= 0

(cid:107)∗

Proof. Since
limk

Fréchet

J(fk)

derivative
J(π)

→∞ (cid:107)∇H
− ∇H
J(f ) deﬁned via restriction to the space

(cid:107)∗

∇F
deﬁned over the ambient space
the same argument, we also have that for any given π
J(π + f ) with respect to the
gradient

H

a

is

continuous
= 0. By the reasoning above, for f
does not change compared to
J(fk)

. Thus we also have limk

linear

F

→∞ (cid:107)∇F
Π and f

can be approximated similarly.

∈ F

∈

operator,

we

have
, the gradient
J(f ), the gradient
= 0. By
, the

∇H
− ∇H

J(π)
(cid:107)∗
, even if π + f

∈ F

(cid:54)∈ F

∇F

F

Note that we are not assuming J(π) to be differentiable when restricting to the policy subspace Π.

A.5 Theoretical motivation for Algorithm 2 - Proof of Proposition 4.4 and 4.5

We consider the case where Π is not differentiable by parameterization. Note that this does not
preclude J(π) for π
Π to be differentiable in the non-parametric function space. Two complications
∈
arise compared to our previous approximate mirror descent procedure. First, for each π
Π,
J(π) (which may not exist under certain parameterization, per section 4.3)
estimating the gradient
J(π) may not be in the dual
R(π)
can become much more difﬁcult. Second, the update rule
space of
space
inappropriate.

− ∇F
, thus making direct gradient update in the

, as in the simple case where Π

⊂ F

∇

∇

F

F

∈

Assumption 2. J is convex in

.

H

Π. Note that ∂J(π) reﬂects sub-gradient of π with respect to the ambient policy space

∈ H

, sub-gradients ∂J(h) exists for all h

. In particular, ∂J(π) exists for all

By convexity of J in
π

H

∈

We will make use of the following equivalent perspective to mirror descent[10], which consists of
two-step process for each iteration t

.

H

1. Solve for ht+1 = argminh
2. Solve for πt+1 = argminπ

η

∂J(πt), h
(cid:105)
(cid:104)
∈H
Π DR(π, ht+1)

∈

+ DR(h, πt)

19

(cid:105)

(23)

(24)

We will show how this version of the algorithm motivates our main algorithm. Consider step 1 of the
main loop of PROPEL, where given a ﬁxed π

Π, the optimization problem within

is

(OBJECTIVE_1) = min
∈H

h

η

∂J(π), h
(cid:105)
(cid:104)

+ DR(h, π)

∈

H

(20)

Due to convexity of

and the objective, problem (OBJECTIVE_1) is equivalent to:

(OBJECTIVE_1) = min

(21)
(22)
where τ depends on η. Since π is ﬁxed, this optimization problem can be relaxed by choosing
λ
τ is
satisﬁed (Selection of λ is possible with bounded spaces). Since this constraint set is potentially a
restricted set compared to the space of policies satisfying inequality (22), the optimization problem
(20) is relaxed into:

[0, 1], and a set of candidate policies h = π + λf , for all f

∂J(π), h
(cid:105)
(cid:104)
s.t. DR(h, π)
≤

, such that DR(h, π)

∈ F

≤

∈

τ

H

(OBJECTIVE_1)

≤

∂J(π), π + λf
(OBJECTIVE_2) = min
(cid:104)
∈F

f

Due to convexity property of J, we have

∂J(π), λf
(cid:104)

=

∂J(π), π + λf
(cid:104)

−
(cid:105) ≤
(cid:105)
The original problem OBJECTIVE_1 is thus upper bounded by:
J(cid:0)π + λf (cid:1)

π)

J(π + λf )

J(π)

−

η

min
h
∈H

+ DR(h, π)

∂J(π), h)
(cid:105)
(cid:104)

min
f
∈F
Thus, a relaxed version of original optimization problem OBJECTIVE_1 can be obtained by miniz-
iming J(π + λf ) over f
(note that π is ﬁxed). This naturally motivates using functional
regularization technique, such as CORE-RL algorithm [19], to update the parameters of differentiable
function f via policy gradient descent update:

∂J(π), π

J(π) +

∈ F

−

≤

(cid:105)

(cid:104)

∇F
where the gradient of J is taken with respect to the parameters of f (neural networks). This is exactly
the update step in algorithm 2 (also similar to iterative updte of CORE-RL algorithm), where the
neural network policy is regularized by a prior controller π.

−

f (cid:48) = f

ηλ

λJ(π + λf )

Statement and Proof of Proposition 4.5

Proposition A.8 (Regret bound for the relaxed optimization objective). Assuming J(h) is L-strongly
by UPDATEF per
smooth over
∇H
(cid:113) 1
T + (cid:15) + λ2L2(cid:17)
H
Alg. 2 leads to the expected regret bound: E
λσ

J(h) is L-Lipschitz continuous, approximating UPDATE

(cid:105)
t=1 J(πt)

J(π∗) = O

(cid:104) 1
T

, i.e.,

(cid:80)T

H

(cid:16)

−

Proof. Instead of focusing on the bias of the gradient estimate
J(π), we will shift our focus on
the alternative proximal formulation of mirror descent, under optimization and projection errors.
In particular, at each iteration t, let h∗t+1 = argminh
+ DR(h, πt) and let the
(cid:104)∇
R(h∗t+1) + βt. Note here that this is
optimization error be deﬁned as βt where
J(π) used in theorem
different from (but related to) the notion of bias from gradient estimate of
4.1 and theorem A.2. The projection error from imitation learning procedure is deﬁned similarly to
Π DR(π, ht+1) is the true projection, and (cid:13)
theorem 4.1: π∗t+1 = argminπ
∈
We start with similar bounding steps to the proof of theorem 4.1:

∇H
J(πt), h
(cid:105)

∇
(cid:13)πt+1 −

∈H
R(ht+1) =

π∗t+1

(cid:13)
(cid:13)

∇

∇

≤

(cid:15).

η

J(πt), πt

π

(cid:105)

−

=

(cid:104)∇

=

=

1
η (cid:104)∇
1
η
1
η
(cid:124)

R(h∗t+1)

R(πt), πt

π

(cid:105)

−

− ∇

(
(cid:104)∇

R(ht+1)

R(πt), πt

π

−

(cid:105) − (cid:104)

βt, πt

π

)
(cid:105)

−

− ∇

(DR(π, πt)

−

DR(π, ht+1) + DR(πt, ht+1))

+

(cid:123)(cid:122)
component_1

(cid:125)

1
η (cid:104)
(cid:124)

βt, πt

−

π

(cid:105)
(cid:125)

(cid:123)(cid:122)
component_2

(25)

As
(cid:0) DR(π, πt)
1
η
(cid:124)

seen from the proof of

theorem A.2,

DR(π, πt+1)
−
(cid:123)(cid:122)
(cid:125)
telescoping

(cid:124)

+ DR(π, πt+1)

component_1 can be upperbounded by:
(cid:1)

DR(π∗t+1, ht+1) + DR(πt, ht+1)
(cid:125)
(cid:123)(cid:122)
relative improvement

−
(cid:124)

DR(π, π∗t+1)
(cid:125)

−
(cid:123)(cid:122)
projection error

The bound on projection error is identical to theorem A.2:
DR(π, π∗t+1)

DR(π, πt)

−

(cid:15)LRD

≤

(26)

20

R(ht+1), π∗t+1 −

πt

(cid:105)

(cid:105)
+

R(π∗t+1) +
) +

(cid:104)∇
βt, π∗t+1 −
πt
(cid:104)
(cid:105)
R(h∗t+1), π∗t+1 −
(cid:104)∇
(cid:13)
2
πt
βt, π∗t+1 −
+
(cid:13)

(cid:104)

(cid:105)

+

πt

(cid:105)

βt, π∗t+1 −
(cid:104)

πt

(cid:105)

(27)

The bound on relative improvement is slightly different:

DR(πt, ht+1)
= R(πt)

−
R(πt), πt

≤ (cid:104)∇
η
=

J

H

(cid:104)∇

−
η2
2α (cid:107)∇H
η2
2α

J +

L2

≤

≤

−

R(π∗t+1 +

(cid:104)∇
π∗t+1(cid:105) −
−
πt
(πt), π∗t+1 −
J(πt)

DR(π∗t+1, ht+1) = R(πt)
−
R(h∗t+1), π∗t+1 −
πt
α
(cid:13)
(cid:13)
2
πt
(cid:13)π∗t+1 −
(cid:13)
2
α
(cid:13)
(cid:13)π∗t+1 −
2
(cid:105) −
2
πt
βt, π∗t+1 −
(cid:107)
(cid:104)
∗
πt
βt, π∗t+1 −
(cid:104)
∇H

+

(cid:105)

πt

Note here that the gradient
(27), (28), we have:

(cid:105)
J(πt) is not the result of estimation. Combining equations (25), (26),

(28)

J(πt), πt

(cid:104)∇

π

−

(cid:105) ≤

1
η

(cid:0)DR(π, πt)

−

DR(π, πt+1) + (cid:15)LRD +

Next, we want to bound βt. Choose regularizer R to be 1
algorithm 2). We have that:

2 (cid:107)·(cid:107)

η2
2α

L2

J +

βt, π∗t+1 −
(cid:104)
2 (consistent with the pseudocode in

(29)

π

(cid:1)
(cid:105)

which is equivalent to:

h∗t+1 = argmin

h

∈H

η

J(πt), h
(cid:105)

(cid:104)∇

+

1
2 (cid:107)

h

πt

2
(cid:107)

−

h∗t+1 = πt + argmin

η

J(πt), f

(cid:104)∇

1
f
2 (cid:107)

2
(cid:107)

+

(cid:105)

Let f ∗t+1 = argminf

∈F
η
f
2 (cid:107)
(cid:107)
(cid:104)∇
J(πt). Let ft+1 be the minimizer of minf
∈F
h∗t+1 = ft+1 −

(cid:105)
η
−
ht+1 = π + λft+1. Thus βt = ht+1 −
On one hand, we have

J(πt), f

∇

∈F

f
+ 1

f ∗t+1.

2. Taking the gradient over f , we can see that f ∗t+1 =
J(πt + λf ). We then have h∗t+1 = πt + f ∗t+1 and

J(πt + λft+1)

J(πt + ωf ∗t+1)

J(πt) +

J(πt), ωf ∗t+1(cid:105)

(cid:104)∇

+

≤

≤

L
2

(cid:13)
(cid:13)ωf ∗t+1

(cid:13)
2
(cid:13)

due to optimality of ft+1 and strong smoothness property of J. On the other hand, since J is convex,
we also have the ﬁrst-order condition:

J(πt) +
Combine with the inequality above, and subtract J(πt) from both sides, and using the relationship
f ∗t+1 =

J(πt), λft+1(cid:105)

J(πt), we have that:

J(πt + λft+1)

(cid:104)∇

≥

η

−

∇

1
η

(cid:104)−

f ∗t+1, λft+1(cid:105) ≤ (cid:104)−

1
η

f ∗t+1, ωf ∗t+1(cid:105)

+

Lω2
2

(cid:13)
(cid:13)f ∗t+1

(cid:13)
2
(cid:13)

ω, rearrange and choose ω such that ω
Since this is true
∀
and complete the square, we can establish the bound that:

η −

Lω2
2 =

−

λ

2η , namely ω = 1

−

√1

ληL

−
Lη

,

for B the upperbound on
equation 30 into RHS of equation 29, we have:

ft+1(cid:107)
(cid:107)

(cid:13)
(cid:13)
f ∗t+1
(cid:13)ft+1 −
(cid:13)
. We thus have

η(λL)2B

(30)
= O(η(λL)2). Plugging the result from

≤
βt
(cid:107)

(cid:107)

J(πt), πt

(cid:104)∇

−
Since J is convex in
λ2σ2 ( 1

(cid:113) 1

η =

π

1
η

(cid:105) ≤

(cid:0)DR(π, πt)

DR(π, πt+1) + (cid:15)LRD +

η2
2α
π
. Similar to theorem 4.1, setting
(cid:105)
−
T + (cid:15)) and taking expectation on both sides, we have:
(cid:35)

(cid:1) + (cid:0)η(λL)2B(cid:1)

, we have J(πt)

J(πt), πt

≤ (cid:104)∇

J(π)

(31)

L2
J

H

−

−

(cid:34)

J(π∗) = O(cid:0)λσ

+ (cid:15) + λ2L2(cid:1)

(32)

E

1
T

T
(cid:88)

t=1

J(πt)

−

(cid:114)

1
T

Note that unlike regret bound from theorem 4.1 under general bias, variance of gradient estimate
and projection error, σ2 here explicitly refers to the bound on neural-network based policy gradient

21

variance. The variance reduction of λσ, at the expense of some bias, was also similarly noted in a
recent functional regularization technique for policy gradient [19].

B Additional Experimental Results and Details

B.1 TORCS

We generate controllers for cars in The Open Racing Car Simulator (TORCS) [59]. In its full generality
TORCS provides a rich environment with input from up to 89 sensors, and optionally the 3D graphic
from a chosen camera angle in the race. The controllers have to decide the values of 5 parameters
during game play, which correspond to the acceleration, brake, clutch, gear and steering of the car.

Apart from the immediate challenge of driving the car on the track, controllers also have to make
race-level strategy decisions, like making pit-stops for fuel. A lower level of complexity is provided
in the Practice Mode setting of TORCS. In this mode all race-level strategies are removed. Currently,
so far as we know, state-of-the-art DRL models are capable of racing only in Practice Mode, and this
is also the environment that we use. Here we consider the input from 29 sensors, and decide values
for the acceleration, steering, and braking actions.

We chose a suite of tracks that provide varying levels of difﬁculty for the learning algorithms. In
particular, for the tracks Ruudskogen and Alpine-2, the DDPG agent is unable to reliably learn a
policy that would complete a lap. We perform the experiments with twenty-ﬁve random seeds and
report the median lap time over these twenty-ﬁve trials. However we note that the TORCS simulator
is not deterministic even for a ﬁxed random seed. Since we model the environment as a Markov
Decision Process, this non-determinism is consistent with our problem statement.

For our Deep Reinforcement Learning agents we used standard open source implementations (with
pre-tuned hyper-parameters) for the relevant domain.

All experiments were conducted on standard workstation with a 2.5 GHz Intel Core i7 CPU and a
GTX 1080 Ti GPU card.

The code for the TORCS experiments can be found at: https://bitbucket.org/averma8053/propel

In Table 3 we show the lap time performance and crash ratios of PROPEL agents initialized with neural
policies obtained via DDPG. As discussed in Section 5, DDPG often exhibits high variance across
trials and this adversely affects the performance of the PROPEL agents when they are initialized via
DDPG. In Table 4 we show generalization results for the PROPELTREE agent. As noted in Section 5,
the generalization results for PROPELTREE are in between those of DDPG and PROPELPROG.

Veriﬁed Smoothness Property. For the program given in Figure 2 we proved using symbolic
peek(s[RPM], i)
veriﬁcation techniques, that
⇒
< 0.63. Here the function peek(., i) takes in a
peek(a[Accel], k + 1)
(cid:107)
history/sequence of sensor or action values and returns the value at position i, . Intuitively, the above
logical implication means that if the sum of the consecutive differences of the last six RPM sensor
values is less than 0.003, then the acceleration actions calculated at the last and penultimate step will
not differ by more than 0.63.

k, (cid:80)k+5
∀
peek(a[Accel], k)
(cid:107)

peek(s[RPM], i + 1)

< 0.003 =

i=k (cid:107)

−

−

(cid:107)

Table 3: Performance results in TORCS of PROPEL agents initialized with neural policies obtained
via DDPG, over 25 random seeds. Each entry is formatted as Lap-time / Crash-ratio, reporting
median lap time in seconds over all the seeds (lower is better) and ratio of seeds that result in crashes
(lower is better). A lap time of CR indicates the agent crashed and could not complete a lap for more
than half the seeds.

LENGTH

G-TRACK
3186M

E-ROAD
3260M

AALBORG
2588M

RUUDSKOGEN ALPINE-2

3274M

3774M

PROPELPROG-DDPG
PROPELTREE-DDPG

97.76/.12
78.47/0.16

108.06/.08
85.46/.04

140.48/.48
CR / 0.56

CR / 0.68
CR / 0.68

CR / 0.92
CR / 0.92

22

Table 4: Generalization results in TORCS for PROPELTREE, where rows are training and columns
are testing tracks. Each entry is formatted as PROPELPROG / DDPG, and the number reported is the
median lap time in seconds over all the seeds (lower is better). CR indicates the agent crashed and
could not complete a lap for more than half the seeds.

G-TRACK

E-ROAD AALBORG

RUUDSKOGEN ALPINE-2

G-TRACK
E-ROAD
AALBORG
RUUDSKOGEN
ALPINE-2

-
84
111
154
CR

95
-
CR
CR
276

CR
CR
-
CR
CR

CR
CR
CR
-
CR

CR
CR
CR
CR
-

Table 5: Performance results in Classic Control problems. Higher scores are better.

PRIOR
DDPG
TRPO
NDPS
VIPER
PROPELPROG
PROPELTREE

MOUNTAINCAR

PENDULUM

00.59 ± 0.00
97.16 ± 3.21
93.03 ± 1.86
66.98 ± 3.11
64.86 ± 3.28
95.63 ± 1.02
96.56 ± 2.81

−875.53 ± 0.00
−132.70 ± 6.44
−131.54 ± 4.56
−435.71 ± 4.83
−394.11 ± 4.97
−187.71 ± 2.35
−139.09 ± 3.31

B.2 Classic Control

We present results from two classic control problems, Mountain-Car (with continuous actions) and
Pendulum, in Table 5. We use the OpenAI Gym implementations of these environments. More
information about these environments can be found at the links: MountainCar and Pendulum.

In Mountain-Car the goal is to drive an under-powered car up the side of a mountain in as few
time-steps as possible. In Pendulum, the goal is to swing a pendulum up so that it stays upright. In
both the environments an episode terminates after a maximum of 200 time-steps.

In Table 5 we report the mean and standard deviation, over twenty-ﬁve random seeds, of the average
scores over 100 episodes for the listed agents and environments. In Figure 6 and Figure 7 we show
the improvements made over the prior by the PROPELPROG agent in MountainCar and Pendulum
respectively, with each iteration of the PROPEL algorithm.

Figure 6: Score improvements in the Moun-
tainCar environment over iterations of PRO-
PELPROG.

Figure 7: Score improvements in the Pendu-
lum environment over iterations of PROPEL-
PROG.

23

0123Iterations020406080100Score ImprovementMountainCar0123Iterations0200400600Score ImprovementPendulum