JOURNAL OF LATEX CLASS FILES

1

A Supervised-Learning based Hour-Ahead
Demand Response of a Behavior-based HEMS
approximating MILP Optimization

Huy Truong Dinh, Student Member, IEEE, Kyu-haeng Lee, Member, IEEE, and Daehee Kim, Member, IEEE

1
2
0
2

v
o
N
3

]

Y
S
.
s
s
e
e
[

1
v
8
7
9
1
0
.
1
1
1
2
:
v
i
X
r
a

Abstract—The demand response (DR) program of a traditional
HEMS usually intervenes appliances by controlling or scheduling
them to achieve multiple objectives such as minimizing energy
cost and maximizing user comfort. In this study, instead of inter-
vening appliances and changing resident behavior, our proposed
strategy for hour-ahead DR ﬁrstly learns appliance use behavior
of residents and then silently controls ESS and RES to minimize
daily energy cost based on its knowledge. To accomplish the goal,
our proposed deep neural networks (DNNs) models approximate
MILP optimization by using supervised learning. The datasets
for training DNNs are created from optimal outputs of a MILP
solver with historical data. After training, at each time slot, these
DNNs are used to control ESS and RES with real-time data of
the surrounding environment. For comparison, we develop two
different strategies named multi-agent reinforcement learning-
based strategy, a kind of hour-ahead strategy and forecast-based
MILP strategy, a kind of day-ahead strategy. For evaluation and
veriﬁcation, our proposed strategies are applied at three different
real-world homes with real-world real-time global horizontal
irradiation and real-world real-time prices. Numerical results
verify that the proposed MILP-based supervised learning strategy
is effective in term of daily energy cost and is the best one among
three proposed strategies.

Index Terms—MILP,

supervised learning, behavior-based,

HEMS, real-time price, demand response, DRL.

I. INTRODUCTION

W ITH the remarkable development of renewable energy

systems (RESs) and energy storage systems (ESSs),
a home energy management system (HEMS) is getting more
and more important in helping residents to reduce their energy
consumption. Building an optimal demand response (DR) of
a HEMS, which is deﬁned as the changes in electric usage
at resident side in response to changes in electricity price
and surrounding environment [1], has received much attention
from researchers. Nowadays, with the amazing development
of machine learning, specially deep learning techniques, many
intelligent strategies for DR program, which adapt to real-time
changes of environment, have been proposed.

In many previous studies (e.g. [2], [3]), appliances are
generally divided into two categories: non-shiftable devices
and shiftable devices. For each shiftable device, residents ﬁrst
set up appropriate time slots in which the device should be

This work was supported by the Korea Institute of Energy Technology
Evaluation and Planning(KETEP) and the Ministry of Trade, Industry &
Energy(MOTIE) of the Republic of Korea (No. 20184030202130) and this
work was supported by the Soonchunhyang University Research Fund.

The authors are with department of Future Convergence Technol-
td-

ogy, Soonchunhyang University, Asan 31538, South Korea (e-mail:
huy@sch.ac.kr, daeheekim@sch.ac.kr)

run. With the DR program, a HEMS then tries to ﬁnd a
optimal day-ahead schedule for shiftable devices, satisfying
all constraints of appliances and achieving multiple objectives.
To ﬁnd such a schedule, an explicit optimization model is
built and solved. For example, in [4], an optimization model
for minimizing energy cost is built and particle swarm op-
timization (PSO) algorithm is used to solve this model and
ﬁnd an optimal day-head schedule. In [5], an optimization
model, which minimizes both energy cost and peak-to-average
ratio (PAR), is built and a day-ahead schedule for appliances
is found by using genetic algorithm (GA). In [6], authors
propose an optimization model in which energy selling is
supported and a day-ahead schedule of appliances and selling
operation is found by PSO algorithm. Beside heuristic meth-
ods, mathematical solvers are usually used to solve explicit
optimization models. Authors in [7] build a MILP model for
minimizing energy cost and use CPLEX solver to ﬁnd a day-
ahead schedule for appliances. In [8], a MINLP solver is used
to solve a multi-objective optimization model which minimizes
energy cost and maximizes use comfort. Likewise, in [9],
a multi-objective MINLP model that jointly optimizes four
objectives: energy cost, user comfort, PAR, and waiting time
is built and solved by Cplex/Conopt solvers. In these afore-
mentioned studies, constructing explicit optimization models
requires detailed domain knowledge and day-ahead forecast
data such as day-ahead outdoor temperature. These forecast
data inevitably contain errors which degrade the performance
of DR.

To overcome these challenges, learning-based approaches
are introduced. For example, in [10], three hidden Markov
models (HMM) are built to learn the probability of each living
activity and operational time of appliances. Following these
models, an agent assesses running requests of appliances based
consumption constraint, convenience, and grid signals. Based
on an actor-critic reinforcement learning (RL), authors in [11]
develop an online load scheduling learning algorithm with
real-time pricing for optimal scheduling of the controllable
appliances. Multi-agent (MA) Q-learning algorithm is adopted
in [12] and [13], where the on-off operations and discrete
power inputs of appliances are considered. Each agent repre-
sented for each appliance is cooperated together to minimize
the electricity cost and the dissatisfaction cost.

Although RL agents do not require prior knowledge, their
applicability are usually limited to domains with fully observ-
able, low-dimensional state spaces because of their instabil-
ity or even divergence [14]. Deep RL (DRL) techniques in

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES

2

which a deep neural network (DNN) is used to approximate
an action-value function of complex, high-dimensional state
spaces have been presented in many hour-ahead residential
DRs. For instance, authors in [15] propose a deep Q-learning
(DQN)-based algorithm that optimizes energy consumption of
heating, ventilation, and air-conditioning (HVAC) and main-
tains thermal comfort and air quality at the given levels. In
[16], DQN and Deep Policy Gradient (DPG) methods are used
for on-line energy scheduling electric vehicle and buildings ap-
pliances. In [17], a deep deterministic policy gradient (DDPG)
algorithm is adopted to schedule ESS and HVAC to minimize
energy cost and satisfy comfortable temperature range in a
smart home. For real-time scheduling of both discretely and
continuously controlled appliances, in [18], the trust region
policy optimization (TRPO)-based algorithm, a kind of the
DRL algorithm, with real-time electricity price and outdoor
temperature are proposed.

The DRL techniques have achieved big success in above
hour-ahead DR programs. However, in some problems, com-
mon DRL approaches sometimes are unstable or difﬁcult to
converge [19]. With such kinds of problems, imitation learning
(IL) is used in some studies. For example, in [20] and [21],
authors propose optimal DRs for HVAC systems in which
DRs ﬁrstly learn a mapping (or a policy) between optimal
actions of a MILP solver with historical states of environment
and then apply their knowledge to schedule HVAC systems
with real-time data of environment. In other words, DRs
try to approximate MILP optimization. The DNNs of these
DRs are trained by using supervised learning technique. Their
simulations show that the results of IL based method are better
than those of DDPG based method. In [22], authors develop an
IL based online power scheduling for real-time energy man-
agement of a micro-grid. In this study, their MINLP problem is
ﬁrstly simpliﬁed via piece-wise linear approximation and turns
into a MILP problem. A DNN is then trained to approximate
MILP optimization by using data labeled optimal outputs of
MILP solver with historical data. In online scheduling, this
DNN is used to control their device at each time slot. Their
study shows that their IL based approach outperforms proximal
policy optimization (PPO) based approach.

In these aforementioned studies, DR generally intervenes
operation of appliances and changes appliance use behavior
of residents. In this study, we propose a supervised-learning
based strategy for a hour-ahead DR of the HEMS in which
DR learns resident behavior and only controls ESS and RES
silently to minimize daily energy cost based on its knowledge.
Residents continue to use their home devices as usual and
achieve maximum comfortable lifestyle. In our study, DR
imitates the behaviors of a MILP solver for hour-ahead control
of ESS and RES. First, DNNs of DR are trained to learn a
MILP approximation by using supervised learning technique,
which is a traditional approach of IL. The dataset used for
training is labeled by optimal actions of a MILP solver with
historical data. After training, these DNNs are then used to
control ESS and RES with current real-time data at each time
slot. The main contributions of our study are as follows:

• We build a daily energy cost minimization problem for a
smart home in the appearance of ESS, RES, and energy

Fig. 1. Components and energy ﬂows in our HEMS [9].

exchange between the smart home and other residents.
Then, we reformulate the problem to another version with
fewer decision variables.

• On the basis of these formulas, we propose three strate-
gies: MILP-based supervised learning strategy, multi-
agent deep deterministic policy gradient (MADDPG)-
based strategy, and forecast-based MILP strategy. First
two strategies are kinds of hour-ahead strategies whereas
the last strategy is a day-ahead strategy.

• Three extensive case studies based on real-world real-
time data are performed to evaluate these strategies. Nu-
merical results show that MILP-based supervised learning
strategy is effective in terms of daily energy cost when
residents have a good appliance use behavior and is the
best one among three proposed strategies.

The rest of this paper is organized as follows. Section II
formulates our problem. In Section III, the details of three
strategies are described. Case studies and simulation results are
provided in Section IV. Some ideas are discussed in Section V.
Finally, Section VI outlines the conclusion and future works.

II. SYSTEM MODEL AND PROBLEM FORMULATION
The components of a smart home considered in this study
is shown in Fig. 1, where an ESS and a PV system as RES
are represented.

The electricity provider can be any outside company which
can sell electricity for home. The main goals of using the ESS
and the PV are to reduce energy demand from the electricity
provider and allow residents to sell surplus energy to other
residents. We assume that all energy to be sold comes from
ESS, and RES energy is used for home load and ESS charging.
In the following parts, ESS and RES models are provided and
a daily energy cost minimization problem is then built for a
day. We also divide a day into T = 24 time slots, and the
duration of each time slot is ∆t = 1h.

A. ESS Model
Let ELevel

ESS (t) be the energy level of ESS after time slot
t. As described in Fig. 1, with ∀t, 1 ≤ t ≤ T , we have the
following formula.

ELevel

ESS (t) = ELevel

ESS (t − 1) +

−

(cid:16)

(cid:16)

Echarge

RES (t) + Echarge
(cid:17)
ESS(t) + Esell

ESS(t)

Eload

EP

(cid:17)

(t)

· ηESS

/ηESS

(1)

JOURNAL OF LATEX CLASS FILES

3

where Eload
ESS(t) is an energy quantity used for appliances in
the time slot t. Esell
ESS(t) is an energy quantity used to sell to
the outside in the time slot t. Echarge
RES (t) is an energy quantity
stored in ESS from RES in the time slot t. Echarge
(t) is an
energy quantity stored in ESS from the electricity provider in
the time slot t. ηESS is ESS efﬁciency.

EP

When using the ESS, we must satisfy the following con-

straints.

ELmin ≤ ELevel

ESS (t) ≤ ELmax

(2)

min

Echarge

RES (t) + Echarge
ESS(t) + Esell
(cid:40)

Eload

modeESS(t) =

EP

(t) ≤ Chrate · ∆t · modeESS(t)

(3)
ESS(t) ≤ Dhrate · ∆t · (cid:0)1 − modeESS(t)(cid:1) (4)

1
0

if ESS is charged in time slot t
if ESS is discharged in time slot t

D. Daily Energy Cost Minimization Problem

We assume that the energy from RES and ESS is com-
plimentary and selling real-time price Psell(t), is related to
buying real-time price Prealtime(t), (e.g., Psell(t) = αp ·
Prealtime(t) with αp is a constant and αp ≤ 1). Then, daily
energy cost minimization problem can be formulated as

T
(cid:88)

t=1

C(t) = min

T
(cid:88)

(cid:32)

(cid:16)

t=1

Eload

EP (t) + Echarge

EP

(cid:17)

(t)

· Prealtime(t)

(cid:33)

− Esell

ESS(t) · αp · Prealtime(t)

(10)

Combining with (9), our problem in (10) can be turned into

(5)
where ELmin and ELmax are the minimum energy level and
the maximum energy level of ESS. Chrate and Dhrate are the
maximum charge and discharge rate of ESS. modeESS(t) is
a binary variable to avoid the simultaneous ESS charging and
discharging in the time slot t. ESS is assumed to be unable to
be charged and discharged simultaneously.

Since we only consider our system during a day (no net
accumulation for next day), energy level should be returned
to the initial energy level EL0, at the end of the day. Thus,
we have

ELevel

ESS (T ) = EL0.

(6)

B. RES Model

According to [23], output energy ERES(t), from a PV
system in kWh in any time slot t (1 ≤ t ≤ T ) can be measured
as

ERES(t) = GHI(t) · S · ηRES · ∆t.

(7)

where GHI(t) is the global horizontal irradiation (kW/m2)
at the location of solar panels in the time slot t. S is the total
area (m2) of solar panels and ηRES is the solar conversion
efﬁciency of the PV system.

As shown in Fig. 1, this energy can be used for appliances

and ESS charging. Thus, we have the following constrain.

Eload

RES(t) + Echarge

RES (t) ≤ ERES(t)

(8)

where Eload
time slot t.

RES(t) is an energy quantity used for appliances in

It is clear that our HEMS tries to utilize RES energy as much
as possible. However, if RES energy is larger than total energy
demand of all appliances and ESS charging, the remaining
RES energy is wasted.

C. Energy Balancing

To keep the energy balance in the smart home, the total
energy demand should be equal to the total energy supply.
Hence, as shown in Fig. 1, with ∀t, 1 ≤ t ≤ T , we have
EP (t) + Eload
where EEC(t) is the energy consumption of all home appli-
ances in a time slot t.

ESS(t) + Eload

EEC(t) = Eload

RES(t).

(9)

min

T
(cid:88)

t=1

C(t) = min

T
(cid:88)

(cid:16)

t=1

EEC(t) − Eload

RES(t) + Echarge

EP

(t)

− Eload

ESS(t) − αp · Esell

ESS(t)

(cid:17)

· Prealtime(t)

(11)

s.t.(1) − (9)

RES(t), Echarge

It is clear that if we know energy consumption EEC(t),
real-time irradiation GHI(t), and real-time price Prealtime(t)
at every time slot t of a day, our problem in (11) is a MILP
problem. Hence, by using MILP solvers, we can easily ﬁnd
optimal values of variables Eload
ESS(t),
and Esell
ESS(t) at every time slot t. These optimal values are
optimal energy of ESS and RES which should be used at each
time slot to achieve optimal daily energy cost. Unfortunately,
we only know these information at the end of the day. It means
that MILP solvers are only useful at the end of the day and it
is too late to control them. Hence, to overcome this problem
and utilize powerful MILP solvers, we propose MILP-based
supervised learning strategies in the next section.

(t), Eload

EP

E. A Different Version of Daily Energy Cost Minimization
Problem

Because ESS cannot be charged or discharged at the same
time slot, we deﬁne a new ﬂoat variable ECD
ESS(t) which refers
to an energy quantity stored in the ESS in a time slot t if
ECD
ESS(t) ≥ 0 and refers to an energy quantity which is drawn
from ESS in a time slot t if ECD

ESS(t) < 0. It means that

ECD

ESS(t) =

(cid:40)Echarge
RES (t) + Echarge
(cid:16)
ESS(t) + Esell
Eload

(t)
EP
(cid:17)
ESS(t)

−

ECD
ECD

ESS(t) ≥ 0
ESS(t) < 0

(12)

Hence, we have

− Dhrate · ∆t ≤ ECD

ESS(t) ≤ Chrate · ∆t

(13)

(cid:40)

Elevel

ESS (t) =

Elevel
Elevel

ESS (t − 1) + ECD
ESS (t − 1) + ECD

ESS(t) · ηESS ECD
ECD
ESS(t)/ηESS

ESS(t) ≥ 0
ESS(t) < 0
(14)

JOURNAL OF LATEX CLASS FILES

4

When ESS is in charge mode (ECD
ESS(t) = Esell

ESS(t) ≥ 0) in a time slot
t, we have Eload
ESS(t) = 0. Hence, combining with
(12), energy cost in this time slot can be calculated as follows.

C(t) =

⇒ C(t) =

(cid:16)

(cid:16)

EEC(t) − Eload

RES(t) + ECD
− Echarge

ESS(t)
(cid:17)

RES (t)

· Prealtime(t)

EEC(t) − Eload

RES(t) + ECD
ESS(t)
(cid:17)

− RC(t)

· Prealtime(t) (15)

where variable RC(t), which depends on Eload
ECD

ESS(t), is calculated as

RES(t) and

RC(t) = min{ERES(t) − Eload

RES(t), ECD

ESS(t)}.

(16)

When ESS is in discharge mode (ECD

slot t, we have Echarge
cost in this time slot can be calculated as follows.

RES (t) = Echarge

EP

ESS(t) < 0) in a time
(t) = 0. Hence, energy

C(t) = Eload

EP (t) · Prealtime(t) − Esell

ESS(t) · αp · Prealtime(t)

Because Psell(t) ≤ Prealtime(t), we only sell energy to out-
side if total energy supply is larger than energy consumption
of home all appliances in this time slot. Hence, we have






(cid:16)

EEC(t) − Eload

RES(t) + ECD

(cid:17)
ESS(t)

· Prealtime(t)

C(t) =

(cid:16)

EEC(t) − Eload

if EEC(t) ≥ Eload
(cid:17)
ESS(t)

RES(t) + ECD

RES(t) − ECD

ESS(t)

· αp · Prealtime(t)

if EEC(t) < Eload

ESS(t)
(17)
It is worth noting that in discharge mode, all RES energy
is only used for appliances. Let RL(t) = min{EEC(t) −
ERES(t), 0}, we have

RES(t) − ECD

C(t) =

RL(t) + ECD

(cid:17)
ESS(t)

RL(t) + ECD

(cid:17)
ESS(t)

(cid:16)

(cid:16)






· Prealtime(t)
if RL(t) ≥ −ECD

ESS(t)

· αp · Prealtime(t)
if RL(t) < −ECD

ESS(t)

(18)
In summary, our daily energy cost minimization problem

can be formulated as follows.

min

T
(cid:88)

t=1

C(t)

(19)

s.t.(2), (6), (7), (8), (13), (14), (16)

where C(t) is calculated as in (15) if ECD
calculated as in (18) if ECD

ESS < 0.

ESS ≥ 0 and is

Although this version of our minimization problem is not
ESS(t)
RES(t). To solve this version, we propose multi-agent
ESS(t) of
RES(t) of RES to minimize

a MILP problem, it only depends on two variables: ECD
and Eload
DRL-based strategy in which an agent controls ECD
ESS and another agent controls Eload
daily energy cost.

III. THREE OPTIMAL STRATEGIES
In this section, we propose three strategies: MILP-based
supervised learning, MADDPG-based strategy and forecast-
based MILP strategy. First two strategies are kinds of hour-
ahead strategies while the last strategy is a kind of day-ahead
strategy.

A. MILP-based supervised learning strategy

In this strategy, at

the beginning, our problem in (11)
is solved by a MILP solver with necessary data from the
surrounding environment of historical days. The results of
this process are optimal values of variables of ESS and RES
we should use to control at each time slot of these historical
days. From these optimal values, datasets for these variables
are built and then used to train DNNs which will learn an
approximation of a MILP solver. After training, DNNs are
used to control them at each time slot with current real-time
data. However, these DNNs also need the forecast value of
energy consumption of all appliances at each time slot as
their input data. Hence, our HEMS needs to learn appliance
use behavior of residents to predict the energy consumption
for next 1 time slot. Fig. 2 shows the overall framework of
MILP-based supervised learning in detail.

In step 1, the historical energy consumption Ehis

EC[1 : 24],
the historical irradiation GHI his[1 : 24], and historical prices
P his[1 : 24] of a historical day are input data for the MILP
solver. The output of the MILP solver are the optimal energy
which should be used to control ESS and RES at 24 time
slots of this historical day: (cid:101)Eload
[1 : 24],
(cid:101)Eload
ESS[1 : 24]. Moreover, the MILP solver also
gives us optimal energy levels (cid:101)ELevel
ESS [1 : 24] of ESS after 24
time slots of this day. A training dataset DEload
for variable
Eload

RES(t), is then structured with formatting as follows:
(cid:105)
EC[t], GHI his[t], (cid:101)ELevel
ESS [t − 1], P his[t], t

RES[1 : 24], (cid:101)Echarge

ESS[1 : 24], (cid:101)Esell

input =

EP

RES

(cid:104)
Ehis
(cid:104)
(cid:101)Eload

(cid:105)
RES[t]

label =

By changing the content of label into (cid:101)Echarge

[t], we will
have new training dataset DEcharge
(t).
EP
With the similar changing, we also have new training datasets
DEload
ESS(t)
ESS
respectively.

for variable Echarge

ESS(t) and Esell

for variables Eload

and DEsell

EP

EP

ESS

EP

EP

ESS

ESS

RES

, DEload

, DEcharge

, DN NEload

In step 2, four datasets DEload

and
are used to train four neural networks DN NEload
DEsell
,
RES
DN NEcharge
, and DN NEsell
of our HEMS,
respectively. It is worth noting that in Fig. 2, only DN NEload
RES
is shown. We also need to train a recurrent neural network
(RNN) to predict amount of energy consumption of the home
in next 1 time slot. In this study, we use RNN with GRU cell
RN NEC, which includes 2 layers and time step is 168.

ESS

ESS

Finally,

in step 3, our HEMS uses four trained DNNs
and RN NEC to control ESS and RES in 1 time slot.
the beginning of each time slot t in a day, ﬁrstly,
At
RN NEC is used to compute forecast value ˘EEC[t] of
the energy consumption EEC(t) in this time slot. Then
[ ˘EEC[t], GHIrealtime[t], ELevel
ESS [t − 1], Prealtime[t], t] are in-
put data for DN NEload
, and its output is the forecast value

RES

JOURNAL OF LATEX CLASS FILES

5

Fig. 2. Overall framework of MILP-based supervised learning: (a) training DNNs, (b) training an RNN to predict energy consumption for next 1 time slot.

environment state is triggered from st to st+1 and two
agents receive a same reward Rt. Our objective is to
minimize the daily energy cost. Hence, Rt can be deﬁned
as Rt = −C(t) with C(t) is calculated as in (15) if
aESS[t] ≥ 0 and is calculated as in (18) if aESS[t] < 0.
Because the action space of both agents is continuous, we
propose a strategy based on MADDPG algorithm [24] whose
architecture is shown in Fig. 3.

RES[t] of variable Eload

˘Eload
RES(t). Likewise, these input data also
are used as input data for the remaining DNNs and forecast
values of the remaining variables are achieved. After achieving
four forecast values of four variables, they are used to control
ESS and RES in this time slot. It is worth noting that, in
this step, all input data are current real-time values except for
˘EEC[t]. Clearly, the efﬁciency of this strategy depends on the
accuracy of energy consumption prediction in next 1 time slot
(the performance of RN NEC) which is shown in Section IV.
More detailed MILP-based supervised learning strategy can be
found in Algorithm 1.

B. MADDPG-based strategy

Our minimization problem deﬁned in (19) can be solved
by using advanced MA-DRL algorithms. The key components
of MA-DRL environment in our problem can be designed as
follows:

• State: The environment state st

the beginning of each time slot

includes 5 kinds of
t:
information at
the energy consumption EEC[t], the real-time irradia-
tion GHIrealtime[t],
the current energy level of ESS
ELevel
ESS [t − 1], the real-time price Prealtime[t], and time
slot t. For brevity, we denote the state by st =
[EEC[t], GHIrealtime[t], ELevel

ESS [t − 1], Prealtime[t], t].

ESS(t) and Eload

• Agents and actions: our DR includes two agents AESS
and ARES whose goals are to decide optimal values
of variables ECD
RES(t) which should be
used to control ESS and RES in the time slot t, re-
the action of AESS is deﬁned as
spectively. Hence,
aESS[t] = ECD
ESS[t] whereas the action of ARES is
deﬁned as aRES[t] = Eload

RES[t].
• Reward: At the beginning of each time slot t, when
two agents execute their actions, the transition of the

Fig. 3. MADDPG architecture [25].

MADDPG algorithm is an extended version of DDPG
algorithm [26] applied to a MA environment and is reported to
defeat other DRL algorithms like DQN, Actor-Critic, TRPO
[24]. This algorithm includes two phases: centralized training
for critic updates and decentralized execution for actions. Each
agent has its own continuous action space and observation
space. In centralized training, each agent, ﬁrstly, try to collect
information from other agents and then calculate and update its
own critic network based on these joint information. As the
critic network learns the joint action-value function Q(s, a)
over time, the deterministic policy gradient is also calculated
and sent to the actor network to help update the parameters

JOURNAL OF LATEX CLASS FILES

6

Algorithm 1: MILP-based supervised learning strategy

1 Step 1: Create training datasets from historical days;

Input

: Number of historical days hd, historical

energy consumption Ehis
historical irradiation GHI his[hd][1 : 24] and
historical prices P his[hd][1 : 24]
Output: Datasets for training the DNNs of variables

EC[hd][1 : 24],

2 for i ← 1 to hd do
3

EC[i][1 : 24], GHI his[i][1 : 24],

RES[1 : 24], (cid:101)Echarge

EP
ESS[1 : 24], (cid:101)ELevel

Solve (11) with Ehis
and P his[i][1 : 24];
=> (cid:101)Eload
(cid:101)Esell
for t ← 1 to 24 do
(cid:104)
input.append(
(cid:105)
1], P his[t], t

);

ESS [1 : 24];

[1 : 24], (cid:101)Eload

ESS[1 : 24],

Ehis

EC[t], GHI his[t], (cid:101)ELevel

ESS [t −

RES

labelEload
labelEcharge
labelEload
labelEsell

.append( (cid:101)Eload

RES[t]);
.append( (cid:101)Echarge
EP
.append( (cid:101)Eload
ESS[t]);
.append( (cid:101)Esell
ESS[t]);

ESS

EP

ESS

[t]);

EP

RES

RES

with {input, labelEload

};
with {input, labelEcharge

11 Step 2: Train DNNs and RNN;
12 Train a DN NEload
13 Train a DN NEcharge
14 Train a DN NEload
15 Train a DN NEsell
16 Train an RN NEC with Ehis
17 Step 3: Use DNNs and RNN during a day;
18 for t ← 1 to 24 do
19

with {input, labelEload
with {input, labelEsell

EC[hd][1 : 24];

};
};

ESS

ESS

ESS

ESS

EP

};

RES

ESS [t −

Measure real-time irradiation GHIrealtime[t] ;
Calculate ERES[t] as in (7);
˘EEC[t] = RN NEC(EEC[t − 168 : t − 1]);
input = [ ˘EEC[t], GHIrealtime[t], ELevel
1], Prealtime[t], t];
/*Get forecast values of variables */ ;
˘Eload
RES[t] = DN NEload
(input);
RES [t] = ERES[t] − ˘Eload
˘Echarge
˘Echarge
[t] = DN NEcharge
EP
˘Eload
ESS[t] = DN NEload
˘Esell
ESS[t] = DN NEsell
ESS
/*Calculate forecast energy quantity for
discharging and charging */ ;
˘EC = ˘Echarge
[t];
˘ED = ˘Eload
if ˘EC ≥ ˘ED then
Use ˘Eload
control PV system and ESS in this time slot;

RES [t] + ˘Echarge
EP
ESS[t] + ˘Esell
ESS[t];

RES [t], and ˘Echarge

RES[t], ˘Echarge

(input);
(input);

RES[t];
(input);

[t] to

EP

ESS

EP

// charge mode

else

// discharge mode

Discharge ESS with an energy quantity, ˘ED,
for appliances. If this energy quantity is larger
than real energy consumption of appliances,
remaining energy is used for selling. All
energy from PV system is used for appliances
in this time slot;

4

5

6

7

8

9

10

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

Calculate ELevel

ESS [t] as in (14);

of the actor network, similar to DDPG algorithm. The most
important thing to notice that even though the critic network
needs joint information, the actor network can only use its
own observation space to make a decision (decentralized
execution). Detailed MADDPG algorithm can be found in
[24]. The brief explanation of MADDPG-based strategy is as
follows.

In the step 1, the agents AESS and ARES are trained by
using historical energy consumption Ehis
EC[1 : 24], historical
irradiation GHI his[1 : 24], and historical prices P his[1 : 24].
Similar to supervised learning strategy, an RNN RN NEC, is
also trained to predict amount of energy consumption of the
home in next 1 time slot.

After training all agents, in step 2, at the beginning of each
time slot t of a day, the RN NEC is ﬁrst used to compute
forecast value ˘EEC[t] of the energy consumption EEC(t)
in this time slot. Then [ ˘EEC[t], GHIrealtime[t], ELevel
ESS [t −
1], Prealtime[t], t] are input data for actor networks of two
agents AESS and ARES. The outputs are forecast values of
variables ECD
RES(t) which should be used to
control ESS and RES. The detailed MADDPG-based strategy
is shown in Algorithm 2.

ESS(t) and Eload

C. Forecast-based MILP strategy

The main prerequisite of using the MILP solver is that all
values of energy consumption EEC(t), real-time irradiation
GHIrealtime(t), and real-time price Prealtime(t) need to be
known at every time slot of a day. However, we usually do
not have this information until the end of the day. Hence, to
overcome this problem, we propose a forecast-based MILP
strategy in which all needed data for the MILP solver are
forecast at the beginning of the day. The brief explanation of
this strategy is as follows.

In step 1, RNNs for prediction of energy consumption, real-
time irradiation, and real-time prices for next 24 time slots are
trained.

In step 2, at the beginning of a day, we achieve forecast
values of energy consumption, real-time irradiation, and real-
time prices for 24 time slots of a day by using these RNNs.
These forecast values are then used to solve (11) by using
a MILP solver. The output of the MILP solver are forecast
values of all variables of ESS and RES in our problem for
next 24 time slots and they will be used to control ESS and
RES at each time slot during this day. The detailed forecast-
based MILP strategy is shown in Algorithm 3.

IV. CASE STUDIES AND SIMULATION RESULTS

In this section, we describe the simulation setup and dif-
ferent case studies to which our proposed strategies are ap-
plied. The performance of our proposed strategies is evaluated
through numerical simulation results under these case studies.
For comparison,
the efﬁciency of our proposed strategies
are ﬁrstly calculated based on MILP results which are only
achieved at the end of a day when we already knew full
information of that day. From these results, the performance
of our proposed strategies is compared together.

JOURNAL OF LATEX CLASS FILES

7

Algorithm 2: MADDPG-based strategy
1 Step 1: Train an agent of RES ARES, and an agent

of ESS AESS, from historical days;
Input

: Number of historical days hd, historical

energy consumption Ehis
historical irradiation GHI his[hd][1 : 24] and
historical prices P his[hd][1 : 24]

EC[hd][1 : 24],

Output: The trained ARES and AESS

2 Use MADDPG algorithm to train ARES for action

RES and AESS for action ECD
EC[hd][1 : 24], GHI his[hd][1 : 24], and

Eload
Ehis
P his[hd][1 : 24] ;

ESS with

3 Train an RN NEC with Ehis
4 Step 2: Use the agent of RES and the agent of ESS

EC[hd][1 : 24];

during a day;
5 for t ← 1 to 24 do
6

ESS [t −

Measure real-time irradiation GHIrealtime[t];
Calculate ERES[t] as in (7);
/* Get forecast values of energy consumption */ ;
˘EEC[t] = RN NEC(EEC[t − 168 : t − 1]);
st = [ ˘EEC[t], GHIrealtime[t], ELevel
1], Prealtime[t], t];
/* Get forecast value of variables */ ;
˘Eload
˘ECD
if ˘ECD

RES[t] = ARES(st);
ESS[t] = AESS(st);
ESS[t] >= 0 then
˘Echarge
RES [t] = ERES[t] − ˘Eload
˘Echarge
EP
Use ˘Eload
control PV system and ESS in this time slot;

// charge mode
RES[t];
ESS[t] − ˘Echarge
RES [t];
RES [t], and ˘Echarge

[t] = ˘ECD
RES[t], ˘Echarge

[t] to

EP

else

// discharge mode

ESS[t], for appliances. If this energy

Discharge ESS with an energy quantity,
− ˘ECD
quantity is larger than real energy
consumption of appliances, remaining energy
is used for selling. All energy from PV
system is used for appliances in this time slot;

7

8

9

10

11

12

13

14

15

16

17

18

19

Algorithm 3: Forecast-based MILP strategy

1 Step 1: Train RNNs of energy consumption,
real-time irradiation, and real-time prices;

2 Train RN N 24

EC with Ehis

EC[hd][1 : 24] for next 24-hour

forecast energy consumption ;

3 Train RN N 24

GHI with GHI his[hd][1 : 24] for next

24-hour forecast real-time irradiation;

4 Train RN N 24

price with P his[hd][1 : 24] for next

24-hour forecast real-time prices;

5 Step 2: Use RNNs for a day;
6

/*Get 24-hour-forecast values of energy consumption,
irradiation, and price*/ ;
EC[1 : 24] = RN N 24

EC(EEC[t − 168 : t − 1]);

GHI (GHI[t − 168 :

price(P [t − 168 : t − 1]);

˘GHI

24
realtime[1 : 24], and

8

24
realtime[1 : 24] = RN N 24

7 ˘E24
˘GHI
t − 1]);
realtime[1 : 24] = RN N 24
EC[1 : 24],

9 ˘P 24
10 Solve (11) with ˘E24

realtime[1 : 24];

˘P 24
11 => (cid:101)Eload
(cid:101)Eload

ESS[1 : 24], (cid:101)Esell
12 for t ← 1 to 24 do
13

ESS[1 : 24];

RES[1 : 24], (cid:101)Echarge

RES [1 : 24], (cid:101)Echarge

EP

[1 : 24],

14

15

16

17

18

19

RES [t] + ˘Echarge
EP
ESS[t] + ˘Esell
ESS[t];

/* Calculate forecast energy quantity for
discharging and charging */ ;
˘EC = ˘Echarge
[t];
˘ED = ˘Eload
if ˘EC ≥ ˘ED then
Use ˘Eload
control PV system and ESS in this time slot;

RES [t], and ˘Echarge

RES[t], ˘Echarge

// charge mode

[t] to

EP

else

// discharge mode

Discharge ESS with an energy quantity, ˘ED,
for appliances. If this energy quantity is larger
than real energy consumption of appliances,
remaining energy is used for selling. All
energy from PV system is used for appliances
in this time slot;

20

Calculate ELevel

ESS [t] as in (14);

A. Simulation Setup

The performance of our proposed strategies depends on the
accuracy of energy consumption prediction in next 1 time
slot. In other words, the performance of our strategies depends
on behaviors in which residents use their appliances. Hence,
for evaluation and comparison, the our proposed strategies
are applied to three different real-world homes which are
classiﬁed based on the resident behavior: stable, f luctuating,
and chaos. These homes are located at London, UK and
their datasets are extracted from “Energy Consumption Data
in London Households” dataset, a real-world biggest dataset
of UK Power Network from Jan 2012 to Feb 2014 [27].

of this home. After training RN NEC with this dataset, the 1-
hour-forecast value predicted by RN NEC is almost the same
as the real value in the testing set. Average MAPE of testing
set is only 0.6%.

Fig. 4. Hourly energy consumption of a stable home from Jan 2012 to Feb
2014.

The stable home describes a home in which resident behav-
ior almost does not change in using appliances day by day. Fig.
4 shows the historical dataset of hourly energy consumption

The ﬂuctuating home describes a home in which resident
behavior changes slightly. Fig. 5 shows the historical dataset
of hourly energy consumption of this home. After training

JOURNAL OF LATEX CLASS FILES

8

RN NEC with this dataset, the 1-hour-forecast value predicted
by RN NEC is a little different from the real value in the
testing set. Average MAPE of testing set is 10.9%.

Fig. 5. Hourly energy consumption of a ﬂuctuating home from Jan 2012 to
Feb 2014.

The chaos home describes a home in which resident be-
havior changes a lot. Fig. 6 shows the historical dataset
of hourly energy consumption of this home. After training
RN NEC with this dataset, the 1-hour-forecast value predicted
by RN NEC is very different from the real value in the testing
set. Average MAPE of testing set is 21.8%.

Fig. 6. Hourly energy consumption of a chaos home from Jan 2012 to Feb
2014.

In our simulations, real-time hourly solar irradiation of
London, UK from Jan 2012 to Feb 2014 shown in Fig.
7 is extracted from database of “Photovoltaic Geographical
Information System” of European Commission [28]. For real-
time hourly prices, because we do not have real-time prices
of London city, real-time hourly prices of Michigan from Jan
2016 to Feb 2018 obtained from Pecan Street database [29] are
used as shown in Fig. 8. To be speciﬁc, in all above datasets,
the data from Jan 2012 to Jan 2014 are used to train the neural
networks of our proposed strategies and the data of Feb 2014
are used to test and evaluate the performance of our proposed
strategies.

Fig. 7. Real-time hourly GHI of London city from Jan 2012 to Feb 2014.

Main parameters of ESS, RES and MADDPG algorithm are

shown in Table I. In this table, N a
ESS
are the capacity of the actor and the critic network of agents
RES and ESS, respectively. lra, and lrc are the learning rate
of the actor network and critic network, respectively.

RES, N a

RES, N c

ESS, N c

Fig. 8. Real-time hourly prices of Michigan state from Jan 2016 to Feb 2018.

TABLE I
MAIN PARAMETERS OF ESS, RES AND MADDPG IN OUR STUDY.

ηESS
Dhrate
EL0
ηRES
αp
N a
N a

RES

ESS
lra
τ
Activation

0.9
1.0 kW
0.5 kWh
0.9
1
100 : 100
200 : 200 : 200
0.001
0.001
relu

Chrate
ELmax
ELmin
S
γ
N c
N c

RES

ESS
lrc
Episodes
Optimizer

1.0 kW
10 kWh
0.5 kWh
1 m2
0.99
100 : 200 : 200
100 : 200 : 200
0.0001
4000
Adam

B. Performance evaluation and comparison of three strategies

Fig. 9 shows the energy cost achieved by three strategies
of each day in Feb 2014 (testing set) at the stable home.
The black line is the daily energy cost without ESS and RES
whereas the red line is optimal daily energy cost which can be
achieved by using MILP solver at the end of each day when
ESS and RES is fully utilized and the full information of that
day is already known. Clearly, these MILP results (red line) are
best daily energy cost we can achieve and this line is a lower
bound of our proposed strategies. As shown in this ﬁgure, the
daily energy cost achieved by MILP-based supervised learning
is the closest to the red line whereas the daily energy cost
achieved by forecast-based strategy is the farthest away from
the red line among three strategies. Hence, the performance
of MILP-based supervised learning strategy is best and the
performance of forecast-based strategy is worst among three
proposed strategies.

Similarly, at the ﬂuctuating home and chaos home, the daily
energy cost of MILP-based supervised learning strategy is also
closer to the red line than those of other strategies as shown
in Fig. 10 and Fig. 11, respectively. Hence, the performance
of MILP-based supervised learning strategy is also the best
among three strategies whereas the performance of forecast-
based strategy is also the worst at these homes.

Clearly, one of main objectives of using ESS and RES is
to reduce the daily energy cost as much as possible and the
cost saving achieved by MILP solver is maximum saving we
can achieve. For a better comparison between our strategies,
we introduce a new metric shown in (20) to calculate the
effectiveness of each proposed strategies in testing set. This
metric shows us how much cost saving we can achieve on
average by each strategy compared with cost saving by MILP

JOURNAL OF LATEX CLASS FILES

9

Fig. 9. The daily energy costs of three strategies at the stable home in Feb
2014.

Fig. 12. The effectiveness of three strategies at three homes in Feb 2014.

solver in percentage terms.

Ef f strategy

M ILP =

i=1
N
(cid:88)

N
(cid:88)

C i

strategy

base − C i
C i

base
base − C i
C i

base

C i

M ILP

× 100

(20)

i=1
where N is the number of days in testing set. C i
base is the
energy cost of the day i without ESS and RES (the black line).
C i
M ILP is the energy cost of the day i which is achieved by
using the MILP solver at the end of that day (the red line).
C i
strategy is the energy cost of the day i which is achieved by
applying our strategy.

Fig. 10. The daily energy costs of three strategies at the ﬂuctuating home in
Feb 2014.

Fig. 11. The daily energy costs of three strategies at the chaos home in Feb
2014.

Fig. 12 shows us the effectiveness of each strategy at three
different homes in Feb 2014. As shown in this ﬁgure, at the
stables home, the average cost saving achieved by MILP-
based supervised learning strategy is about 82.8% of the
average cost saving achieved by the MILP solver whereas the
average cost saving achieved by MADDPG-based strategy and
forecast-based strategy are only 74% and 48.3%, respectively.
At ﬂuctuating and chaos homes,
the average cost saving
achieved by MILP-based supervised learning is also higher
than those of other strategies. These results again conﬁrm that
the performance of MILP-based supervised learning strategy is
the best and that of forecast-based strategy is the worst among
three proposed strategies. We have these results because, in
this study, the DNNs of the MILP-based supervised learning
approximate the optimal results of MILP solver better than
that of MADDPG-based strategy whereas the forecast errors
of the 24-hour predictions worsen the performance of forecast-
based strategy. It is worth noting that the average cost saving
achieved by forecast-based strategy at ﬂuctuating home is
larger than those achieved by forecast-based strategy at other
homes because amount of the hourly energy consumption at
ﬂuctuating home is the largest one in three kinds of homes.
When the controlling of ESS and RES becomes worse in
forecast-based strategy, the performance of this strategy in
ﬂuctuating home is improved by absorbing a lot of RES

JOURNAL OF LATEX CLASS FILES

10

Although forecast-based strategy, a kind of day-ahead strat-
egy,
is the worst strategy among our proposed strategies,
it does not mean that this strategy should be replaced. In
the meanwhile, hour-ahead and day-ahead strategy should be
combined in smart houses. Day-ahead strategy helps residents
determine energy demand and easily take part
in energy
markets. In the meanwhile, hour-ahead strategy helps residents
improve the performance of HEMS and achieve maximum
comfortable lifestyle. Moreover, the running time of our pro-
posed strategies is very tiny, implying that it is very potential
when integrated in the coordinated DR of multiple HEMSs.

VI. CONCLUSION
In this study, we proposed a MILP-based supervised learn-
ing strategy for hour-ahead DR of a HEMS which learned
resident behavior of using appliances and applied its knowl-
edge to minimize daily energy cost. The DNNs of this
strategy were trained by using datasets created by a MILP
solver with historical data to approximate MILP optimization.
After training, at each time slot, these DNNs were used to
control ESS and RES with current real-time information of
surrounding environment. This strategy was also compared to
MADDPG-based hour-ahead strategy and forecast-based day-
ahead strategy. Three different case studies were conducted
based on resident behavior of using appliances. The case
study results demonstrated the effectiveness of MILP-based
supervised learning in terms of daily energy cost. The average
cost saving of this strategy is achieved up to 73.8% of average
cost saving achieved by MILP solver at home in which the
MAPE of 1-hour prediction is only 21.8%. This strategy was
also the best strategy among three proposed strategies at all
case studies.

Future works will focus on the development of a cooperative
strategy for group of homes where a resident at each home
will buy or sell energy together. This strategy will be possible
an extended version of our proposed strategy. Basically, our
MILP-based supervised learning strategy can be considered
as MILP-based IL strategy in which our DR tries to mimic
the MILP solver by using supervised learning technique, a
traditional approach of IL. Hence, another way to improve our
study is to apply advanced algorithms of IL such as DAgger
[30].

REFERENCES

[1] J. Kwac, J. Flora, and R. Rajagopal, “Household energy consumption
segmentation using hourly data,” IEEE Transactions on Smart Grid,
vol. 5, no. 1, pp. 420–430, 2014.

[2] S. Li, J. Yang, W. Song, and A. Chen, “A real-time electricity scheduling
for residential home energy management,” IEEE Internet of Things
Journal, vol. 6, no. 2, pp. 2602–2611, 2018.

[3] S. Althaher, P. Mancarella, and J. Mutale, “Automated demand response
from home energy management system under dynamic pricing and
power and comfort constraints,” IEEE Transactions on Smart Grid,
vol. 6, no. 4, pp. 1874–1883, 2015.

[4] A. Bouakkaz, A. J. G. Mena, S. Haddad, and M. L. Ferrari, “Efﬁcient
energy scheduling considering cost reduction and energy saving in
hybrid energy system with energy storage,” Journal of Energy Storage,
vol. 33, p. 101887, 2021.

[5] A. Ahmad, A. Khan, N. Javaid, H. M. Hussain, W. Abdul, A. Almogren,
A. Alamri, and I. Azim Niaz, “An optimized home energy management
system with integrated renewable energy and storage resources,” Ener-
gies, vol. 10, no. 4, p. 549, 2017.

Fig. 13. The total loss of RES energy at three homes in Feb 2014.

TABLE II
THE AVERAGE COMPUTATIONAL TIME.

Strategy
MILP-based supervised learning
MADDPG-based strategy

Average computational time (s)
0.18
0.001

energy. As shown in Fig. 13, the loss of RES energy in the
ﬂuctuating home is smallest among in three kinds of homes
at all strategies.

Table II shows the average computational time of MILP-
based supervised learning strategy and MADDPG-based strat-
egy at each time slot. As shown in this table, the computational
time of MILP-based supervised learning is larger than that
of MADDPG-based strategy because the supervised learning
needs more DNNs to predict than MADDPG-based strategy
at each time slot. Moreover, the DNNs of supervised learning
are built by using Keras, a high-level deep learning APIs of
Tensorﬂow whereas the agents of MADDPG-based strategy
are implemented by using low-level APIs of Pytorch. However,
the computational time of these strategies is very small. In this
table, the computational time of forecast-based strategy is not
shown and compared because this strategy is a kind of day-
ahead strategy while two remaining strategies are hour-ahead
strategies.

V. DISCUSSION

The proposed MILP-based supervised learning does not
intervene home appliances as many traditional methods but
silently minimizes daily energy cost. It means that residents
still keep their behaviors of using appliances and maximize
their comfortable lifestyle. However, as shown in Fig. 12,
the performance of our proposes strategies decreases steadily
when resident behavior is getting more and more chaotic.
To apply our strategy to smart homes or buildings, residents
should have a good behavior of using their home devices or
the MAPE of energy consumption prediction for next 1 hour
should be smaller than 21.8%.

JOURNAL OF LATEX CLASS FILES

11

[29] P. Street.

(2021) Real-time hourly prices of michigan.

[Online].

Available: https://www.pecanstreet.org/

[30] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
and structured prediction to no-regret online learning,” in Proceedings
of the fourteenth international conference on artiﬁcial intelligence and
statistics.
JMLR Workshop and Conference Proceedings, 2011, pp.
627–635.

[6] H. T. Dinh, J. Yun, D. M. Kim, K.-H. Lee, and D. Kim, “A home
energy management system with renewable energy and energy storage
utilizing main grid and electricity selling,” IEEE Access, vol. 8, pp.
49 436–49 450, 2020.

[7] K. C. Sou, J. Weimer, H. Sandberg, and K. H. Johansson, “Scheduling
smart home appliances using mixed integer linear programming,” in
2011 50th IEEE Conference on Decision and Control and European
Control Conference.
IEEE, 2011, pp. 5144–5149.

[8] A. Anvari-Moghaddam, H. Monsef, and A. Rahimi-Kian, “Optimal
smart home energy management considering energy saving and a
comfortable lifestyle,” IEEE Transactions on Smart Grid, vol. 6, no. 1,
pp. 324–332, 2014.

[9] H. T. Dinh and D. Kim, “An optimal energy-saving home energy man-
agement supporting user comfort and electricity selling with different
prices,” IEEE Access, vol. 9, pp. 9235–9249, 2021.

[10] N. Ahmed, M. Levorato, and G.-P. Li, “Residential consumer-centric
demand side management,” IEEE Transactions on Smart Grid, vol. 9,
no. 5, pp. 4513–4524, 2017.

[11] S. Bahrami, V. W. Wong, and J. Huang, “An online learning algorithm
for demand response in smart grid,” IEEE Transactions on Smart Grid,
vol. 9, no. 5, pp. 4712–4725, 2017.

[12] X. Xu, Y. Jia, Y. Xu, Z. Xu, S. Chai, and C. S. Lai, “A multi-
agent reinforcement learning-based data-driven method for home energy
management,” IEEE Transactions on Smart Grid, vol. 11, no. 4, pp.
3201–3211, 2020.

[13] R. Lu, S. H. Hong, and M. Yu, “Demand response for home energy
management using reinforcement learning and artiﬁcial neural network,”
IEEE Transactions on Smart Grid, vol. 10, no. 6, pp. 6629–6639, 2019.
[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control
learning,”
nature, vol. 518, no. 7540, pp. 529–533, 2015.

through deep reinforcement

[15] W. Valladares, M. Galindo, J. Guti´errez, W.-C. Wu, K.-K. Liao, J.-C.
Liao, K.-C. Lu, and C.-C. Wang, “Energy optimization associated with
thermal comfort and indoor air control via a deep reinforcement learning
algorithm,” Building and Environment, vol. 155, pp. 105–117, 2019.

[16] E. Mocanu, D. C. Mocanu, P. H. Nguyen, A. Liotta, M. E. Webber,
M. Gibescu, and J. G. Slootweg, “On-line building energy optimization
using deep reinforcement learning,” IEEE transactions on smart grid,
vol. 10, no. 4, pp. 3698–3708, 2018.

[17] L. Yu, W. Xie, D. Xie, Y. Zou, D. Zhang, Z. Sun, L. Zhang, Y. Zhang,
and T. Jiang, “Deep reinforcement learning for smart home energy
management,” IEEE Internet of Things Journal, vol. 7, no. 4, pp. 2751–
2762, 2019.

[18] H. Li, Z. Wan, and H. He, “Real-time residential demand response,”
IEEE Transactions on Smart Grid, vol. 11, no. 5, pp. 4144–4154, 2020.
[19] S. Chen, M. Wang, W. Song, Y. Yang, Y. Li, and M. Fu, “Stabilization
approaches for reinforcement
learning-based end-to-end autonomous
driving,” IEEE Transactions on Vehicular Technology, vol. 69, no. 5,
pp. 4740–4750, 2020.

[20] Y.-J. Kim, “A supervised-learning-based strategy for optimal demand
response of an hvac system in a multi-zone ofﬁce building,” IEEE
Transactions on Smart Grid, vol. 11, no. 5, pp. 4212–4226, 2020.
[21] H. T. Dinh and D. Kim, “Milp-based imitation learning for hvac control,”

IEEE Internet of Things Journal, 2021.

[22] S. Gao, C. Xiang, M. Yu, K. T. Tan, and T. H. Lee, “Online optimal
power scheduling of a microgrid via imitation learning,” IEEE Transac-
tions on Smart Grid, 2021.

[23] Y. Ru, J. Kleissl, and S. Martinez, “Storage size determination for
grid-connected photovoltaic systems,” IEEE Transactions on sustainable
energy, vol. 4, no. 1, pp. 68–81, 2012.

[24] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,
“Multi-agent actor-critic for mixed cooperative-competitive environ-
ments,” arXiv preprint arXiv:1706.02275, 2017.

[25] W. Farag, “Multi-agent reinforcement learning using the deep distributed
distributional deterministic policy gradients algorithm,” in 2020 Inter-
national Conference on Innovation and Intelligence for Informatics,
Computing and Technologies (3ICT).

IEEE, 2020, pp. 1–6.

[26] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[27] U. P. Networks.
households.
[Online]. Available:
smartmeter-energy-use-data-in-london-households

(2016) Energy consumption data

in london
https://data.london.gov.uk/dataset/

[28] E. Commission. (2019) Photovoltaic geographical information system.

[Online]. Available: https://re.jrc.ec.europa.eu/

