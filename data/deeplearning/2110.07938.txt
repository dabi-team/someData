Identifying Causal Inﬂuences on Publication Trends and Behavior:
A Case Study of the Computational Linguistics Community

Maria Glenski and Svitlana Volkova

National Security Directorate

Paciﬁc Northwest National Laboratory

{Maria.Glenski@pnnl.gov, Svitlana.Volkova@pnnl.gov}

1
2
0
2

t
c
O
5
1

]
L
C
.
s
c
[

1
v
8
3
9
7
0
.
0
1
1
2
:
v
i
X
r
a

Abstract

Drawing causal conclusions from observa-
tional real-world data is a very much desired
but challenging task. In this paper we present
mixed-method analyses to investigate causal
inﬂuences of publication trends and behavior
on the adoption, persistence, and retirement of
certain research foci – methodologies, materi-
als, and tasks that are of interest to the com-
putational linguistics (CL) community. Our
key ﬁndings highlight evidence of the tran-
sition to rapidly emerging methodologies in
the research community (e.g., adoption of bidi-
rectional LSTMs inﬂuencing the retirement of
LSTMs), the persistent engagement with trend-
ing tasks and techniques (e.g., deep learning,
embeddings, generative, and language mod-
els), the effect of scientist location from out-
side the US, e.g., China on propensity of re-
searching languages beyond English, and the
potential impact of funding for large-scale re-
search programs. We anticipate this work
to provide useful insights about publication
trends and behavior and raise the awareness
about the potential for causal inference in the
computational linguistics and a broader scien-
tiﬁc community.

Causal understanding is essential for informed
decision making (Pearl and Mackenzie, 2018; Pearl,
2019; Varian, 2016) to go beyond correlations and
overcome the predictability limit of real-world par-
tially observed systems including complex systems
of human social behavior (Abeliuk et al., 2020;
Hofman et al., 2017).

Unlike earlier work that focused on analysing
publication trends, diversity and innovation in sci-
ence relying on descriptive exploratory analysis
primarily driven by correlations (Fortunato et al.,
2018; Hofstra et al., 2020; Ramage et al., 2020),
this work aims to provide empirical evidence of
causal mechanisms driving publication trends and
behavior in the computational linguistics commu-
nity. Our key contributions are two-fold. First,

Figure 1: Causal diagram for scientiﬁc publications,
where full text, author, and co-citation network proper-
ties encode causal confounders (e.g., author inﬂuence,
research tasks and methodology) used as covariates.
Our analyses rely on the assumption that latent con-
founders can be measured and adjusted for based on
proxies in our covariates.

we experiment and evaluate the potential of us-
ing complementary causal inference approaches,
speciﬁcally causal structure learning models and
several treatment effect estimation techniques to
measure causal inﬂuences in high-dimensional ob-
servational data. Second, we analyze the temporal
dynamics of causal inﬂuences of publication trends
on the adoption, persistence and retirement of cer-
tain research foci in the CL community.

For our analyses we leverage public ACL an-
thology data and encode publication and scientist
characteristics, as well as collaboration behavior
as confounders, to measure the effect of previous
research foci on the adoption, maintenance and re-
tirement of future research foci focusing on most
recent six years between 2014 and 2019. Figure 1
presents a causal diagram which illustrates our core
assumption: that latent confounders (e.g., author
reputation, productivity, and collaborations, the
strength and length of authors’ research careers,
novelty of research, papers’ contributions to the

 
 
 
 
 
 
ﬁeld, etc.) can be measured and adjusted for based
on proxies in our covariates (e.g., research entities
extracted from publications, author properties in-
cluding time since ﬁrst paper, volume of papers,
centrality within co-authorship networks etc.).

1 Related Work

There are two complementary causal inference
frameworks – structural causal models (Pearl
et al., 2009) and treatment effect estimation ap-
proaches (Rosenbaum and Rubin, 1983). Exist-
ing approaches to learn the causal structure (aka
causal discovery) broadly fall into two categories:
constraint-based (Spirtes et al., 2000; Yu et al.,
2016) and score-based (Chickering, 2002).

Recently, there have been an increased interest in
causal inference on observational data (Guo et al.,
2020), including text data, in the computational lin-
guistics and computational social science communi-
ties (Lazer et al., 2009). For example, recent work
by (Roberts et al., 2020) estimated the effect of
perceived author gender on the number of citations
of the scientiﬁc articles and (Veitch et al., 2020)
measured the effect that presence of a theorem in a
paper had on the rate of the paper’s acceptance.

Additional examples in the computational so-
cial science domain include: measuring the ef-
fect of alcohol mentions on Twitter on college
success (Kiciman et al., 2018); estimating the ef-
fect of the “positivity” of the product reviews and
recommendations on sales on Amazon (Pryzant
et al., 2020; Sharma et al., 2015); understanding
factors effecting user performance on StackEx-
hange, Khan Academy, and Duolingo (Alipour-
fard et al., 2018; Fennell et al., 2019); estimating
the effect of censorship on subsequent censorship
and posting rate on Weibo (Roberts et al., 2020)
and word use in the mental health community on
users’ transition to post in the suicide community
on Reddit (De Choudhury et al., 2016; De Choud-
hury and Kiciman, 2017); or the effect of exercise
on shifts of topical interest on Twitter (Falavar-
jani et al., 2017). Moreover, Keith et al. (2020)
presented the overview of causal approaches for
computational social science problems focusing on
the use of text to remove confounders from causal
estimates, which was also used by (Weld et al.,
2020). Earlier work utilized matching methods
to learn causal association between word features
and class labels in document classiﬁcation (Paul,
2017; Wood-Doughty et al., 2018), and use text as

treatments (Fong and Grimmer, 2016; Dorie et al.,
2019) or covariates e.g., causal embeddings (Veitch
et al., 2020; Schölkopf et al., 2021).

Several studies have leveraged the ACL Anthol-
ogy dataset to analyze diversity in computer sci-
ence research (Vogel and Jurafsky, 2012), and per-
form exploratory data analysis such as knowledge
extraction and mining (Singh et al., 2018b; Radev
and Abu-Jbara, 2012; Gábor et al., 2016). How-
ever, unlike any other work, our approach focuses
on leveraging complementary methods for causal
inference – structural causal models and treatment
effect estimation to discover and measure the ef-
fect of scientists’ research focus on their productiv-
ity and publication behavior, speciﬁcally the emer-
gence, retirement and persistence of computational
linguistics methodologies, approaches and topics.

2 Data Preprocessing

Our causal analysis relies on the publication
records from the Association of Computational
Linguistic (ACL) research community from 1986
through 2020. We collect the ACL Anthology
dataset1 (Gildea et al., 2018) with the bibtex pro-
vided with the accompanying abstracts. Excluding
all records that do not contain authors (e.g., bibtex
entries for the workshop proceedings), we convert
the bibtex representation into a data representation
where each row represents each paper-author com-
bination (i.e., for a paper (paperX) with three au-
thors, there are three representative rows: paperX-
author1, paperX-author2, and paperX-author3).

Then, we extract features that encode paper
properties: the year it was published, whether the
paper was published in a conference or journal,
the number of authors, the number of pages, and
word count in paper. We also compute Gunning fog
index (Gunning et al., 1952) – inﬂuenced by the
number of words, sentences, and complex words.
We then annotate each row with properties re-
lated to the author during the year the paper was
published. As a proxy of the length of the au-
thor’s research career in the computational linguis-
tics community, we calculate the number of years
since the author’s ﬁrst publication in the anthol-
ogy. Each author’s location is represented as the
location (country) of the institution the author is
associated with in the metadata or full text. To
measure productivity at varying granularities, we
calculate the number of one’s papers published in

1https://aclanthology.org/

total, in the last year, and in the last ﬁve years.

We then construct a dynamic network represen-
tation of the anthology using author-to-paper rela-
tionships for each calendar year, as encoded in the
metadata. After projecting those relationships into
the dynamic co-authorship network that reﬂects
author to co-author connections by year, we cal-
culate centrality and page rank network statistics
over time to measure the inﬂuence of the author.
These collaboration behavior features comple-
ment previously described author properties. We
also added three features to encode the diversity in
co-authorship. First, the number of all co-authors
who published the papers with the author. Second,
the average number of papers co-authored per co-
author, which is computed as the total number of
papers co-authored per co-author divided by the
number of co-authors. The last is a likelihood that
a co-author is an author on a paper, which is the
second feature divided by the total number of the
author’s papers. This enables us to measure the
diversity, or lack thereof, of collaborative relation-
ships of each author, and encodes how collabora-
tion behavior evolves over time.

s
r
e
p
a
P
%

60
40
20
0
1960

1970

1980

1990

2000

2010

2020

Figure 2: Relative coverage of consolidated research
entity representations in the ACL data. Percentage of
papers with at least one entity associated by publica-
tion year. Dashed line indicates the start of our causal
analysis period (2014).

These consolidated entities are representative of
the top 300 entities extracted from all ACL anthol-
ogy publications for which we were able to extract
the full text (121,134 out of 127,041 which is 95.3%
of all records for which there was an ACL anthol-
ogy bibtex entry), after removing trivial or general
terms such as “system”, ‘approach”, “it”, “task”,
and “method”. We present the coverage across
papers (% of papers with at least one associated
entity) over time in Figure 2, illustrating the cov-
erage approximates the overall coverage (around
41%) for the bulk of the dataset (1980-2019), with
coverage trending upwards over time.

2.1 Encoding Research Focus

3 Methodology

After extracting the full text of each paper from the
PDF using GROBID (GRO, 2008–2021), we use
the SpERT model trained to extract key research
entities from scientiﬁc publications. The SpERT
model (Luan et al., 2018) was trained to extract sci-
entiﬁc entities of different types such as tasks, meth-
ods, and materials and the relationships between
them such as “Feature-Of" and “Used-for", using
the SciERC dataset2. After applying the model to
the ACL data, we consolidate noisy references of
research entities into representative clusters man-
ually, resulting in 50 entities that encode research
tasks, methods, and materials3.

2http://nlp.cs.washington.edu/sciIE/
3Research entities trending in the CL community used
for our causal analyses: “artiﬁcial intelligence”, “adversar-
ial”, “annotation”, “arabic”, “attention”, “baselines”, “bidi-
rectional lstm”, “causal”, “chinese”, “classiﬁcation”, “coref-
erence”, “crowdsourcing”, “deep learning”, “dialog”, “em-
beddings”, “ethics”, “explanation”, “fairness”, “french”, “gen-
erative”, “german”, “grammars”, “graph models”, “heuris-
tics”, “interpretability”, “language models”, “lstm”, “ma-
chine learning”, “monolingual”, “multilingual”, “multiple
languages”, “NER”, “node2vec”, “non-English language”,
“pos/dependency/parsing”, “QA”, “reinforcement learning”,
“robustness”, “russian”, “sentiment”, “statistical/probabilistic
models”, “summarization”, “topic model”, “transfer learning”,
“transformers”, “translation”, “transparency”, “unsupervised
methods”, “word2vec”, “benchmark”.

We investigate the causal relationships between
characteristics of individual publications for each
researcher who authored at least two publications
hosted in the ACL Anthology. We identify causal
inﬂuences of publication outcomes related to the
adoption, retirement, and maintenance of compu-
tational linguistic methodologies, approaches, or
topics, as well as the productivity of authors using
both causal discovery and pairwise causal effect
estimation.

3.1 Treatments and Outcomes

We consider the inclusion of our consolidated re-
search entities to be treatments in our analyses —
e.g., does the inclusion of biLSTM architectures in
the publication have a causal relationship with fu-
ture research outcomes? — and the basis of several
research outcomes related to the adoption, retire-
ment, and maintenance of CL methodologies, tasks
and approaches.

That is, the association of the identiﬁed research
entities with authors’ publications allows us to iden-
tify when authors adopt new emerging technologies
(e.g., the ﬁrst use of transformers), retire previously
used methods or research applications (e.g., if au-

thors stop publishing on LSTM architectures af-
ter biLSTM architectures are introduced), continue
to use – or maintain publications in – methods
(e.g., when authors continue to publish on NER).
We associate these behaviors as future outcomes
for each author’s publications in previous years.

For each year in which an author published in an
ACL venue, we calculate adoption and retirement
outcomes for each consolidated research element
the following year, maintenance outcomes for each
research element considering the following two
years. Alongside these ﬁne-grained research out-
comes, we also examine coarse-grained, or general
outcomes for authors:

• overall pauses in publishing within ACL
venues (no publications in any ACL commu-
nity for two years),

• persistent publication records (continuing to

publish in consecutive years),

• publication volume increases (the increase or
decrease in number of publications in ACL
venues relative to the previous year).

In our analyses, we focus on recent six years
(2014-2019) for which we have complete treatment
and outcome annotations and consider each year
independently. We leverage two types of publica-
tion record granularities – publication records and
yearly research portfolios – to analyze the temporal
dynamics of the causal system underpinning CL
publication venues at multiple resolutions. Note,
we present a detailed description of the treatments,
covariates and outcomes we used in Appendix A.

3.2 Causal Structure Learning

Structural causal models are a way of describing
relevant features of the world and how they interact
with each other. Essentially, causal models repre-
sent the mechanisms by which data is generated.
The causal model formally consists of two sets of
variables U (exogenous variables that are external
to the model) and V (endogenous variables that are
descendants of exogenous variables), and a set of
functions f that assign each variable in V a value
based on the values of the other variables in the
model. To expand this deﬁnition: a variable X is
a direct cause of a variable Y if X appears in the
function that assigns Y value. Graphical models or
Directed Acyclic Graphs (DAGs) have been widely
used as causal model representations.

The causal effect rule is deﬁned as: given a
causal graph G in which a set of variables P A(X)

are designated as a parents of X, the causal effect
of X on Y is given by:

P (Y = y|do(X = x)) =

(cid:80)

z P (Y = y|X = x, P A = z)P (P A = z),

(1)
where z ranges over all the combinations of val-

ues that the variables in PA can take.

The ﬁrst approach for our causal analysis
aims to examine the causal relationships that
are identiﬁed using an ensemble of causal dis-
covery algorithms (Saldanha et al., 2020). Our
ensemble considers the relationships identiﬁed
by CCDR (Aragam and Zhou, 2015), MMPC
(Max-Min Parents-Children) (Tsamardinos et al.,
2003), GES (Greedy Equivalence Search) (Chick-
ering, 2002), and PC (Peter-Clark) (Colombo and
Maathuis, 2014). We use the implementations
provided by the pcalg R package (Hauser and
Bühlmann, 2012; Kalisch et al., 2012) and causal
discovery toolbox (CDT) (Kalainathan and Goudet,
2019)4. The outcomes of our ensemble approach
to causal discovery is a causal graph reﬂecting the
relationships within the causal system, weighting
edges by the agreement among the individual algo-
rithms on whether the causal relationship exists.

After applying this causal discovery approach
to each year individually, we are able to construct
a dynamic causal graph and investigate trends in
causal relationships – e.g., as they are introduced,
persist over time, or are eliminated.

3.3 Treatment Effect Estimation

We further investigate the magnitude and effect
of causal relationships using average treatment
effect (ATE) estimates. We compare pair-wise
estimates using several causal inference models:
Causal Forest (Tibshirani et al., 2018) and Propen-
sity Score Matching (Ho et al., 2007) using the
“MatchIt” R package5, and a cluster-based condi-
tional treatment effect estimation tool – Visualiza-
tion and Artiﬁcial Intelligence for Natural Experi-
ments (VAINE)6 (Guo et al., 2021).

VAINE is designed to discover natural experi-
ments and estimate causal effects using observa-
tional data and address challenges traditional ap-
proaches have with continuous treatments and high-

4https://fentechsolutions.github.io/
CausalDiscoveryToolbox/html/index.html
5https://cran.r-project.org/web/

packages/MatchIt/vignettes/MatchIt.html
6https://github.com/pnnl/vaine-widget

Figure 3: Treatment effect estimates obtained using three causal inference methods – Causal Forest, Propensity
Score Matching and VAINE, for publish on x → retire x over time, across TEE methods.

Figure 4: Summary of causal structure learning using our ensemble model discovered from Publish on x to Retire
x (above) or Maintain x in the next 2 years (below), by year. Shaded cells indicate that an edge was discovered,
white cells indicate that no edge was discovered for that year. At right is a summary of the number of years for
which an edge was discovered.

dimensional feature spaces. First, VAINE allows
users to automatically detect sets of observations
controlling for various covariates in the latent space.
Then, using linear modeling, VAINE allows to es-
timate the treatment effect within each group and
then average these local treatment effects to esti-
mate the overall average effect between a given
treatment and outcome variable. VAINE’s novel
approach for causal effect estimation allows it to
handle continuous treatment variables without ar-
bitrary discretization and produces results that are
intuitive, interpretable, and veriﬁable by a human.
VAINE is an interactive capability that allows the
user to explore different parameter settings such
as the number of groups, the alpha threshold to
identify signiﬁcant effects, etc.

Below we deﬁne what we mean by learning a
causal effect from observational data. Given n in-
stances [(x1, t1), . . . , (xn, tn)] learning causal ef-
fects quantiﬁes how the outcome y is expected to
change if we modify the treatment from c to t,
which can be deﬁned as E(y | t) − E(y | c), where
t and c denote a treatment as a control.

Similarly to our causal discovery based analyses,
we examine the growth and decay of causal inﬂu-
ence for a series of treatments (research focus rep-
resented by materials, methodology, or application-
based keywords) on our outcomes of interest.

3.4 Evaluation

Evaluating causal analysis methods is challeng-
ing (Saldanha et al., 2020; Weld et al., 2020;
Gentzel et al., 2019; Shimoni et al., 2018; Mooij
et al., 2016; Dorie et al., 2019; Singh et al., 2018a;
Raghu et al., 2018). Broadly, evaluation techniques
include structural, observational, interventional and
qualitative techniques e.g., visual inspection. Ob-
servational evaluation by nature are non-causal and
do not have the ability to measure the errors un-
der interventions. Structural measures are limited
due to the requirements of known structure, are
oblivious to magnitude and type and dependence,
as well as treatments and outcomes, and constrain
research directions. Unlike structural and observa-
tional measures, interventional measures allow to
evaluate model estimates of interventional effects
e.g., “what-if counterfactual evaluation".

In this work we rely on both qualitative and
quantitative evaluation. Methods that we use for
causal inference were independently validated us-
ing structural and observational measures on syn-
thetic datasets – causal forest (Wager and Athey,
2018), propensity score matching (Cottam et al.,
2021), causal ensemble (Saldanha et al., 2020), and
VAINE (Guo et al., 2021). Since we rely on four
complementary causal inference techniques, we
draw our conclusions based on their agreement. In
addition, to perform qualitative evaluation with the
human in the loop we rely on recently release vi-
sual analytics tools to evaluate causal discovery
and inference (Cottam et al., 2021).

4 Results

In this section, we present a series of key ﬁndings
surfaced within the causal mechanisms discovered
and treatment effects estimated focusing on con-
trastive analysis over time: whether publishing on
a given research entity (methodology, task, mate-
rial) inﬂuences continuing to publish in that area
or with that methodology, how authors shift from
existing to novel methodology over time, and evi-
dence of external events (i.e., funding or large re-
search programs) potentially impacts the adoption
and maintenance of publication trends.

4.1 Continuing Existing Avenues of Research

One of the ﬁrst trends we noticed, in both the causal
structures and treatment effects, was a causal rela-
tionship between publishing on a given research en-
tity (e.g., robustness, LSTMs, transformers, NER,
etc.) and whether an author would continue to pub-
lish on the same topic, task, or methodology in the
following year(s). Does publishing once inﬂuence
whether you will publish again? In short, no. We
see a consistent trend in positive treatment effects,
as illustrated in Figure 3, from publishing in the
current year to not publishing (pausing or retiring
research entities) in the future – publishing on x
leads to not publishing on x in the future.

In Figure 4, we summarize the temporal dynam-
ics of causal relationships from publishing on x in
a current year’s publication to retiring x (no pub-
lications associated with research entity x) in the
next 2 years (above) or maintaining x (at least one
publication associated) in the next 2 years (below)
indicated by our causal structural learning analyses
which aligns with our TEE results. We show the
consistency in which research entities are included

Method

2014

2015

2016

2017

2018

2019

CF

VAINE

Mean

0

0

0

0.71

-0.01

0.46

0.88

0.80

0.07

0.68

0.09

0.68

-0.03

0.77

0.37

0.22

0.32

0.39

Table 1: Treatment effect estimates for the treatment
Publish on bidirectional LSTM on outcome Retire
LSTM by year, illustrating a decaying inﬂuence.

Method

2014

2015

2016

2017

2018

2019

CF

VAINE

Mean

0

0

0

0.76

-0.03

0

0

0.38

-0.02

0.36

0.39

0.38

-0.2

0.00

-0.23

-0.22

0

0.00

Table 2: Treatment effect estimates for the treatment
Publish on bidirectional LSTM on outcome Increase
Publications next year, illustrating a strong initial in-
ﬂuence shift to negative (2018) then neutral (2019).

in these trends using Figure 5. We see that many
of the elements where causal relationships were
identiﬁed in all 6 years are present in both the re-
tirement and maintenance relationships. Of all the
elements, research on Transparency is the only case
where there is only a retirement relationship. All
elements with identiﬁed maintenance relationships
in at least one year were also present in the set of
retirement relationships.

4.2 Emerging Research Foci, and the Impacts
on Retirement of Old Research Foci

The introduction or popularization of new model ar-
chitectures (especially in deep learning) has an ini-
tial strong impact on retirement of previous SOTA
architectures, but this is often focused on the ini-
tial adoption. We investigate several examples of
such phenomena. Table 2 illustrates the decaying
causal inﬂuence that using bidirectional LSTM-
based architectures in current publications has on
the retirement of (no longer using) LSTM in future
publications. At ﬁrst, there is a strong causal effect
(approx. 0.8), where the use of biLSTM layers lead
to no longer using LSTM layers. However, this
reduces over time, with CF estimating close to no
effect past 2015. We see a complementary trend on
the relative publication volume increase outcome
(Increase # publications next year), where there is
an initial strong effect (0.76) that decays until it
shifts to a negative effect (in 2018) then neutral (in
2019), as shown in Table 2.

In addition, we see a consistent divergence from
the trend described above (publishing on x inﬂu-

Figure 5: Venn diagram illustrating shared research focus among causal relationships discovered by multiple
causal inference methods in Figure 4. This plot demonstrates long-lasting vs. short-lasting research trends in the
CL community.

Figure 6: Recurrent causal relationships (identiﬁed for at least two years) that inﬂuence continued publication
patterns related to non-English languages in the CL community, e.g., scientist co-authorship PageRank effects
maintaining non-English publication focus in 2014 and 2016. Black markers identify the effect, with line segments
extending to the cause nodes, and distinct relationships are represented by varying colors.

ences not publishing on x in the next two years)
for research related to non-English languages in
the 2016-2018 time frame (see Figure 6). These
might be explained by the impact of large-scale
research programs and funding (note, we will em-
pirically conﬁrm or dispute this hypothesis as a
part of our future work). For example, we ﬁnd that
these outcomes (whether researchers continue to
publish research related to non-English languages
in 2017-2020) align with the last few years (and
program-wide evaluation events7) of the LORELEI
(Low Resource Languages for Emergent Incidents)
DARPA program8.

The goal of the LORELEI program was “to
dramatically advance the state of computational
linguistics and human language technology to en-
able rapid, low-cost development of capabilities for
low-resource languages”, and resulted in several
publications on such languages from performers
e.g., (Strassel and Tracey, 2016). “What is funded
is published” may be an intuitive inﬂuence, but
here we see qualitative evidence that funding could
inﬂuence the causal mechanisms of the publica-

7https://www.nist.gov/itl/iad/mig/

lorehlt-evaluations

8https://www.darpa.mil/program/

tion ecosystem — these signals are strong enough
to be reﬂected in causal systems discovered using
causal discovery algorithms in observational data.
For adopting non-English as a research focus, we
also see inﬂuence from the authors’ country as-
sociations – i.e., institution afﬁliations in China
inﬂuence adopting non-English research (Fig. 7).
Table 3 illustrates a peak in the positive inﬂuence
of publishing in a particular non-English language
(i.e., Arabic, which was one of the languages of
interest for the LORELEI program) and continuing
to publish on non-English languages. We see that
the divergence of the causal relationships, and the
persistence of authorship in non-English language
research, illustrated in Figure 8, center around or
peak in 2017 and begin to ﬂip (causal forest esti-
mates a negative effect of -0.55) in 2019.

5 Discussion and Conclusions

In this study, we identiﬁed and analyzed causal in-
ﬂuences between the trends and publishing behav-
ior on future research performed and published in
the computational linguistic community. Adjusting
for confounders related to publication properties ex-
tracted from publication text and metadata, author
characteristics, and collaboration network proper-

low-resource-languages-for-emergent-incidents

Figure 7: Recurrent causal relationships that inﬂuence new publication patterns related to non-English languages.
Black markers identify the effect, with line segmnets extending to the cause nodes, and distinct relationships are
represented by varying colors. Empty time windows indicate no recurrent relationships were discovered.

Publish on Arabic → Continue Publishing on non-English

Method

2014

2015

2016

2017

2018

2019

CF

0.04

VAINE

0

Mean

0.02

0.03

0.20

0.12

0.28

0.43

0.36

0.56

0.51

0.54

0.40

0.13

0.27

-0.55

0.06

0.00

Publish on Arabic → Stop Publishing on non-English

Method

2014

2015

2016

2017

2018

2019

CF

VAINE

Mean

0.44

0.81

0.62

0.80

0.71

0.75

0.40

0.45

0.42

0.13

0.20

0.16

0.50

0.82

0.66

0.91

0.91

0.91

Publish on Arabic → Increase Publications next year

Method

2014

2015

2016

2017

2018

2019

CF

-0.03

-0.03

0.31

1.8

0.23

-0.09

Table 3: Treatment effect estimates for the treatment
Publish on Arabic for outcomes Maintain non-English
in the next two years (above) and Stop publishing on
non-English in the next year (below) by year, illustrat-
ing a peak in inﬂuence continuing to publish in 2017.

ties, we examine the causal relationships that could
potentially be driving scientist productivity and the
adoption, maintenance, and retirement of methods,
materials, and tasks referenced in publications, and
how these dynamics evolve over time.

Our analyses show that publishing once on a
speciﬁc task, application, or methodology has a
causal inﬂuence that causes authors not to publish
on the same approach in the following year e.g.,
robustness, interpretability etc. This is consistent
across a signiﬁcant number of the research meth-
ods, tasks, and application domains represented in
our consolidated annotations.

There are several potential drivers of this causal
relationship. First, publishing in the CL commu-
nity, particularly in recent years, is extremely com-
petitive. Acceptance rates are low and the num-
ber of submissions each year continue to grow –
there is a lot of competition for few spots. This

Persistence of Authorship in non-English Research

s
r
o
h
t
u
a
%

15
10
5
0

2014

2015

2016

2017

2018

2019

Figure 8: The percentage of authors who published on
non-English languages in a given year, who also pub-
lished on non-English languages in the following year.

could also be reﬂective of the churn in novelty
and state-of-the-art technologies: as one technol-
ogy (e.g., LSTMs) are replaced by a new SOTA
methodology (e.g., biLSTMs, or transformers), this
naturally leads to the retirement of the previous
methods, which we also see reﬂected in the ﬁnd-
ings presented in subsection 4.2. As we drive the
ﬁeld forward, we stop publishing on older methods,
materials, and tasks because of novelty incentives
or requirements to be accepted in top-tier venues.

Limitations A limitation of our current causal
analysis approach is the restriction to ACL An-
thology publication records only. As this dataset
comprises only ACL venues, it does not guaran-
tee inclusion of all possible publications from each
author. For example, authors who publish in ACL
venues may also publish in ICLR, NeurIPs, etc.
Future work can address this limitation by aug-
menting the data set with supplementary collects
from these venues (e.g., using Google Scholar).

Another avenue of future work is to incorporate
funding information directly in the analyses. As
we have shown, there is evidence that funding has
causal relationships with publication outcomes and
expertise evolution, and may act as a confounder
for other relationships. Future work will extract
funding from full text PDFs (e.g., from the acknowl-
edgements section) in order to adjust for the effects
of funding as a confounder, which may also be an
impactful treatment to analyze.

Acknowledgements

This material is based on work funded by the
United States Department of Energy (DOE) Na-
tional Nuclear Security Administration (NNSA) Of-
ﬁce of Defense Nuclear Nonproliferation Research
and Development (DNN R&D) Next-Generation
AI research portfolio and Paciﬁc Northwest Na-
tional Laboratory, which is operated by Battelle
Memorial Institute for the U.S. Department of En-
ergy under contract DE-AC05-76RLO1830. Any
opinions, ﬁndings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reﬂect the views
of the United States Government or any agency
thereof. We would like to thank Joonseok Kim and
Jasmine Eshun for their assistance preparing data.

References

2008–2021. Grobid.

kermitt2/grobid.

https://github.com/

Andrés Abeliuk, Zhishen Huang, Emilio Ferrara, and
Kristina Lerman. 2020. Predictability limit of par-
tially observed systems. Scientiﬁc reports, 10(1):1–
10.

Nazanin Alipourfard, Peter G Fennell, and Kristina Ler-
man. 2018. Using simpson’s paradox to discover
In Twelfth
interesting patterns in behavioral data.
International AAAI Conference on Web and Social
Media.

Bryon Aragam and Qing Zhou. 2015. Concave pe-
nalized estimation of sparse gaussian bayesian net-
works. The Journal of Machine Learning Research,
16(1):2273–2328.

David Maxwell Chickering. 2002. Optimal structure
Journal of ma-

identiﬁcation with greedy search.
chine learning research, 3(Nov):507–554.

Diego Colombo and Marloes H Maathuis. 2014. Order-
independent constraint-based causal structure learn-
ing. The Journal of Machine Learning Research,
15(1):3741–3782.

Joseph Cottam, Maria Glenski, Yi Shaw, Ryan Ra-
bello, Austin Golding, Svitlana Volkova, and Dustin
Arendt. 2021. Graph comparison for causal discov-
ery. Visualization in Data Science 2021.

Munmun De Choudhury and Emre Kiciman. 2017.
The language of social support in social media and
In Proceedings
its effect on suicidal ideation risk.
of the International AAAI Conference on Web and
Social Media, volume 11.

Munmun De Choudhury, Emre Kiciman, Mark Dredze,
Glen Coppersmith, and Mrinal Kumar. 2016. Dis-
ideation from mental
covering shifts to suicidal

In Proceedings of
health content in social media.
the 2016 CHI conference on human factors in com-
puting systems, pages 2098–2110.

Vincent Dorie, Jennifer Hill, Uri Shalit, Marc Scott,
and Dan Cervone. 2019. Automated versus do-
it-yourself methods for causal inference: Lessons
learned from a data analysis competition. Statistical
Science, 34(1):43–68.

Seyed Mirlohi Falavarjani, Hawre Hosseini, Zeinab
Noorian, and Ebrahim Bagheri. 2017. Estimating
the effect of exercising on users’ online behavior. In
Proceedings of the International AAAI Conference
on Web and Social Media, volume 11.

Peter G Fennell, Zhiya Zuo, and Kristina Lerman. 2019.
Predicting and explaining behavioral data with struc-
tured feature space decomposition. EPJ Data Sci-
ence, 8(1):23.

Christian Fong and Justin Grimmer. 2016. Discovery
In Proceedings
of treatments from text corpora.
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 1600–1609.

Santo Fortunato, Carl T Bergstrom, Katy Börner,
James A Evans, Dirk Helbing, Staša Milojevi´c,
Alexander M Petersen, Filippo Radicchi, Roberta
Sinatra, Brian Uzzi, et al. 2018. Science of science.
Science, 359(6379).

Kata Gábor, Haifa Zargayouna, Davide Buscaldi, Is-
abelle Tellier, and Thierry Charnois. 2016. Seman-
tic annotation of the ACL anthology corpus for the
automatic analysis of scientiﬁc literature. In LREC
2016.

Amanda Gentzel, Dan Garant, and David Jensen. 2019.
The case for evaluating causal models using inter-
ventional measures and empirical data. Advances in
Neural Information Processing Systems, 32:11722–
11732.

Daniel Gildea, Min-Yen Kan, Nitin Madnani,
Christoph Teichmann, and Martín Villalba. 2018.
The ACL anthology: Current state and future
In Proceedings of Workshop for NLP
directions.
Open Source Software (NLP-OSS), pages 23–28.

Robert Gunning et al. 1952. Technique of clear writ-

ing.

Grace Guo, Maria Glenski, ZhuanYi Shaw, Emily Sa-
landha, Alex Endert, Svitlana Volkova, and Dustin
Arendt. 2021. Vaine: Visualization and ai for natu-
ral experiments. IEEE transactions on visualization
and computer graphics.

Ruocheng Guo, Lu Cheng, Jundong Li, P Richard
Hahn, and Huan Liu. 2020. A survey of learning
causality with data: Problems and methods. ACM
Computing Surveys (CSUR), 53(4):1–37.

Alain Hauser and Peter Bühlmann. 2012. Characteri-
zation and greedy learning of interventional Markov
equivalence classes of directed acyclic graphs. Jour-
nal of Machine Learning Research, 13:2409–2464.

Daniel E Ho, Kosuke Imai, Gary King, and Elizabeth A
Stuart. 2007. Matching as nonparametric prepro-
cessing for reducing model dependence in paramet-
ric causal inference. Political analysis, 15(3):199–
236.

Jake M Hofman, Amit Sharma, and Duncan J Watts.
2017. Prediction and explanation in social systems.
Science, 355(6324):486–488.

Bas Hofstra, Vivek V Kulkarni, Sebastian Munoz-
Najar Galvez, Bryan He, Dan Jurafsky, and Daniel A
McFarland. 2020. The diversity–innovation paradox
in science. Proceedings of the National Academy of
Sciences, 117(17):9284–9291.

Diviyan Kalainathan and Olivier Goudet. 2019. Causal
discovery toolbox: Uncover causal relationships in
python.

Markus Kalisch, Martin Mächler, Diego Colombo,
Marloes H. Maathuis, and Peter Bühlmann. 2012.
Causal inference using graphical models with the
R package pcalg. Journal of Statistical Software,
47(11):1–26.

Katherine Keith, David Jensen, and Brendan O’Connor.
2020. Text and causal inference: A review of us-
ing text to remove confounding from causal esti-
In Proceedings of the 58th Annual Meet-
mates.
ing of the Association for Computational Linguistics,
pages 5332–5344.

Emre Kiciman, Scott Counts, and Melissa Gasser. 2018.
Using longitudinal social media analysis to under-
stand the effects of early college alcohol use.
In
Twelfth International AAAI Conference on Web and
Social Media.

David Lazer, Alex Pentland, Lada Adamic, Sinan Aral,
Albert-Laszlo Barabasi, Devon Brewer, Nicholas
Christakis, Noshir Contractor, James Fowler, My-
ron Gutmann, et al. 2009. Social science. compu-
Science (New York, NY),
tational social science.
323(5915):721–723.

Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh
Hajishirzi. 2018. Multi-task identiﬁcation of enti-
ties, relations, and coreference for scientiﬁc knowl-
edge graph construction. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 3219–3232.

Michael Paul. 2017. Feature selection as causal infer-
ence: Experiments with text classiﬁcation. In Pro-
ceedings of the 21st Conference on Computational
Natural Language Learning (CoNLL 2017), pages
163–172.

Judea Pearl. 2019. The seven tools of causal inference,
with reﬂections on machine learning. Communica-
tions of the ACM, 62(3):54–60.

Judea Pearl and Dana Mackenzie. 2018. The book of
why: the new science of cause and effect. Basic
Books, New York City, NY.

Judea Pearl et al. 2009. Causal inference in statistics:

An overview. Statistics surveys, 3:96–146.

Reid Pryzant, Dallas Card, Dan Jurafsky, Victor Veitch,
and Dhanya Sridhar. 2020. Causal effects of linguis-
tic properties. arXiv preprint arXiv:2010.12919.

Dragomir Radev and Amjad Abu-Jbara. 2012. Redis-
covering ACL discoveries through the lens of acl an-
thology network citing sentences. In Proceedings of
the ACL-2012 Special Workshop on Rediscovering
50 Years of Discoveries, pages 1–12.

Vineet K Raghu, Allen Poon, and Panayiotis V Benos.
2018. Evaluation of causal structure learning meth-
ods on mixed data types. Proceedings of machine
learning research, 92:48.

Daniel Ramage, Christopher D Manning, and Daniel A
McFarland. 2020. Mapping three decades of in-
arXiv preprint
tellectual change in academia.
arXiv:2004.01291.

Margaret E Roberts, Brandon M Stewart,

and
Richard A Nielsen. 2020. Adjusting for confound-
ing with text matching. American Journal of Politi-
cal Science, 64(4):887–903.

Paul R Rosenbaum and Donald B Rubin. 1983. The
central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41–55.

Emily Saldanha, Robin Cosbey, Ellyn Ayton, Maria
Glenski, Joseph Cottam, Karthik Shivaram, Brett
Jefferson, Brian Hutchinson, Dustin Arendt, and
Svitlana Volkova. 2020. Evaluation of algorithm se-
lection and ensemble methods for causal discovery.

Bernhard Schölkopf, Francesco Locatello, Stefan
Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. 2021. Toward
causal representation learning. Proceedings of the
IEEE, 109(5):612–634.

Joris M Mooij, Jonas Peters, Dominik Janzing, Jakob
Zscheischler, and Bernhard Schölkopf. 2016. Distin-
guishing cause from effect using observational data:
methods and benchmarks. The Journal of Machine
Learning Research, 17(1):1103–1204.

Amit Sharma, Jake M Hofman, and Duncan J Watts.
2015. Estimating the causal impact of recommen-
dation systems from observational data. In Proceed-
ings of the Sixteenth ACM Conference on Economics
and Computation, pages 453–470.

evaluation framework for causal inference. arXiv
preprint arXiv:2009.09961.

Zach Wood-Doughty, Ilya Shpitser, and Mark Dredze.
2018. Challenges of using text classiﬁers for causal
inference. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Conference on Empirical Methods in Natural Lan-
guage Processing, volume 2018, page 4586. NIH
Public Access.

Kui Yu, Jiuyong Li, and Lin Liu. 2016. A review
on algorithms for constraint-based causal discovery.
arXiv preprint arXiv:1611.03977.

Yishai Shimoni, Chen Yanover, Ehud Karavani, and
Yaara Goldschmnidt. 2018. Benchmarking frame-
work for performance-evaluation of causal inference
analysis. arXiv preprint arXiv:1802.05046.

Karamjit Singh, Garima Gupta, Vartika Tewari, and
Gautam Shroff. 2018a. Comparative benchmarking
In Proceedings of
of causal discovery algorithms.
the ACM India Joint International Conference on
Data Science and Management of Data, pages 46–
56.

Mayank Singh, Pradeep Dogga, Sohan Patro, Dhiraj
Barnwal, Ritam Dutt, Rajarshi Haldar, Pawan Goyal,
and Animesh Mukherjee. 2018b. Cl scholar: The
acl anthology knowledge graph miner. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Demonstrations, pages 16–20.

Peter Spirtes, Clark N Glymour, Richard Scheines, and
David Heckerman. 2000. Causation, prediction,
and search. MIT press.

Stephanie Strassel and Jennifer Tracey. 2016. Lorelei
language packs: Data, tools, and resources for tech-
nology development in low resource languages. In
Proceedings of the Tenth International Conference
on Language Resources and Evaluation (LREC’16),
pages 3273–3280.

Julie Tibshirani, Susan Athey, Rina Friedberg, Vitor
Hadad, David Hirshberg, Luke Miner, Erik Sver-
drup, Stefan Wager, Marvin Wright, and Main-
tainer Julie Tibshirani. 2018. Package ‘grf’.

Ioannis Tsamardinos, Constantin F Aliferis, and
Alexander Statnikov. 2003. Time and sample efﬁ-
cient discovery of markov blankets and direct causal
relations. In Proceedings of the ninth ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 673–678, New York City,
NY.

Hal R Varian. 2016. Causal inference in economics and
marketing. Proceedings of the National Academy of
Sciences, 113(27):7310–7315.

Victor Veitch, Dhanya Sridhar, and David Blei. 2020.
Adapting text embeddings for causal inference. In
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 919–928. PMLR.

Adam Vogel and Dan Jurafsky. 2012. He said, she
said: Gender in the acl anthology. In Proceedings
of the ACL-2012 Special Workshop on Rediscover-
ing 50 Years of Discoveries, pages 33–41.

Stefan Wager and Susan Athey. 2018. Estimation and
inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical
Association, 113(523):1228–1242.

Galen Weld, Peter West, Maria Glenski, David Arbour,
Ryan Rossi, and Tim Althoff. 2020. Adjusting for
confounders with text: Challenges and an empirical

A Appendix

We summarize the publication record encodings (treatments, outcomes, and covariates) used in our causal
discovery and treatment effect estimation analyses in Table 4.

Treatment

Publish on x

Scientist from c

Outcome

Adopt x

Maintain x

Retire/Pause x

Publication Increase Rate

Description

Binary encoding of whether x (one of our 50 research entities) was extracted from the
publication’s full text, using SpERT (Luan et al., 2018).

Binary encoding of whether author is a scientist afﬁliated with country c (i.e., author
is associated with an insitution located in country c). There are six variations of this
treatment, where c is one of the ﬁve countries wiht the greatest representation in ACL
publications (the United States, China, Germany, Japan, France) or “Other”.

Description

Binary encoding of whether author will publish on x for the ﬁrst time in the next calendar
year.

Binary encoding of whether author previously published on x and has at least one
publication on x in the following calendar year.

Binary encoding of whether author previously published on x but has no publications on
x in the following two calendar years.

The relative increase in the number of publications by author in the next calendar year,
i.e., # publications in year t+1
# publications in year t

Covariate

# Papers

Description

Total number of author’s publications (cumulative since ﬁrst publication).

# Papers (Last Year)

Number of author’s publications within the last year.

# Papers (Last 5 Years)

Number of author’s publications within the last ﬁve years.

# Co-authors

Number of co-authors linked to author in the collaboration network (by year).

Avg. # Papers Co-authored

Average paper count per co-author, i.e.,

#P apers
#Co−Authors

Co-author Likelihood

Centrality

Page Rank

Likelihood that a randomly selected co-author was a co-author on the publication,
i.e., Avg.#P apersCo−authored

#P apers

Degree centrality of the author in the collaboration network, for the publication’s calendar
year.

Page rank of the author in the collaboration network, for the publication’s calendar year.

Time Since First Paper

Number of years since the author’s ﬁrst publication in the ACL anthology.

Conference

# Authors

Page Length

# Words

Binary encoding of whether the paper is published in a conference (1) or journal (0).

Publication’s total number of authors.

Page length of the publication’s full text PDF.

Total number of words in the publication’s full text.

Gunning Fog Index

Gunning Fog Index readability measure (Gunning et al., 1952) calculated using the
publication text.

Table 4: Overview of Treatments (above), Outcomes (center), and Covariates (below).

