1
2
0
2

r
a

M
1

]

V
C
.
s
c
[

1
v
9
5
3
1
0
.
3
0
1
2
:
v
i
X
r
a

Brain Programming is Immune to Adversarial Attacks:
Towards Accurate and Robust Image Classiﬁcation
using Symbolic Learning

Gerardo Ibarra-Vazqueza, Gustavo Olagueb, Mariana Chan-Leyb, Cesar
Puentea, Carlos Soubervielle-Montalvoa

aUniversidad Aut´onoma de San Luis Potos´ı, Facultad de Ingenier´ıa. Dr. Manuel Nava
8, Col. Zona Universitaria Poniente, 78290, San Luis Potos´ı, S.L.P., M´exico
bEvoVisi´on Laboratory, CICESE Research Center. Carretera Ensenada-Tijuana 3918,
Zona Playitas, 22860, Ensenada, B.C. M´exico

Abstract

In recent years, the security concerns about the vulnerability of Deep Convo-
lutional Neural Networks (DCNN) to Adversarial Attacks (AA) in the form
of small modiﬁcations to the input image almost invisible to human vision
make their predictions untrustworthy. Therefore, it is necessary to provide
robustness to adversarial examples in addition to an accurate score when
developing a new classiﬁer. In this work, we perform a comparative study
of the eﬀects of AA on the complex problem of art media categorization,
which involves a sophisticated analysis of features to classify a ﬁne collection
of artworks. We tested a prevailing bag of visual words approach from com-
puter vision, four state-of-the-art DCNN models (AlexNet, VGG, ResNet,
ResNet101), and the Brain Programming (BP) algorithm. In this study, we
analyze the algorithms’ performance using accuracy. Besides, we use the
accuracy ratio between adversarial examples and clean images to measure
robustness. Moreover, we propose a statistical analysis of each classiﬁer’s
predictions’ conﬁdence to corroborate the results. We conﬁrm that BP pre-
dictions’ change was below 2% using adversarial examples computed with
the fast gradient sign method. Also, considering the multiple pixel attack,
BP obtained four out of seven classes without changes and the rest with a
maximum error of 4% in the predictions. Finally, BP also gets four cate-

∗Corresponding author
Email address: olague@cicese.mx (Gustavo Olague)

Preprint submitted to Swarm and Evolutionary Computation

March 3, 2021

 
 
 
 
 
 
gories using adversarial patches without changes and for the remaining three
classes with a variation of 1%. Additionally, the statistical analysis showed
that the predictions’ conﬁdence of BP were not signiﬁcantly diﬀerent for each
pair of clean and perturbed images in every experiment. These results prove
BP’s robustness against adversarial examples compared to DCNN and hand-
crafted features methods, whose performance on the art media classiﬁcation
was compromised with the proposed perturbations. We also ratify the com-
petitive score of BP against the state-of-the-art classiﬁers for the art media
categorization problem.

Keywords: Brain Programming, Adversarial Attacks, Image Classiﬁcation,
Art Media Categorization

1. Introduction

Image classiﬁcation is an active research area in artiﬁcial intelligence,
whose primary goal is to analyze contextual information or visual content of
an image and assign it to the class or category to which it belongs [1]. There
have been signiﬁcant eﬀorts on areas such as Computer Vision (CV), Machine
Learning (ML), Evolutionary computation (EC), and Swarm Intelligence (SI)
to tackle this problem [2, 3, 4]. Two predominant methods have been among
the most popular and successful approaches for solving image classiﬁcation
problems: 1) Bag of Visual words (BoV) from CV and 2) Deep Convolutional
Neural Networks (DCNN) also known as Deep Learning (DL), a subdivision
of ML [5, 6]. Nonetheless, EC and SI have mostly contributed in two manners:
1) optimizing feature selection and 2) optimizing DCNN architectures.

In this manner, Genetic Programming (GP) has been one of EC’s prin-
cipal tools to optimize the selection of features and automatically extract
the best characteristics to approach image classiﬁcation tasks. For exam-
ple, In 2018, authors of [7] propose a GP method to achieve simultaneously
global and local feature extraction for image classiﬁcation using the JAFFE
(1998), YALE (1997), FLOWER (2007), and TEXTURE (2006) datasets.
As we could appreciate, all datasets are outdated since nowadays; none are
used to test algorithms. Moreover, their approach is compared to traditional
hand-engineered features from CV like SIFT (Scale-Invariant Feature Trans-
form), which is an image processing technique that follows the local feature
paradigm, and it does not behave-scale well for problems like image catego-
rization since diﬀerent images with multiple attributes represent an object

2

category. The solution demands a consensus of distinct characteristics in the
form of a set of features. Therefore, this research work lacks more recent
databases and a comparison with current classiﬁcation algorithms. In 2019,
the article [8] proposes a GP approach to automatically generate discrim-
inative rich features for image classiﬁcation using the MIT urban and na-
ture scene datasets (2003). These image databases are also outdated; hence,
the comparison is made with traditional CV classiﬁcation methods like His-
togram of Oriented Gradients (HOG) and Support Vector Machine (SVM),
similarly to the previous work.

In 2019, the research work [9] proposed a method for employing transfer
learning in GP to extract and transfer knowledge to classify complex tex-
ture images. The proposed methodology uses the following texture datasets
Kylberg (2011), Brodatz (1999), and Outex (2002), and all images are re-
sized to 115 × 115 pixels to perform their experiments to avoid the compu-
tational cost and simplify the problem. In 2020, the article [10] proposes a
GP-based feature learning approach to select and combine ﬁve methods auto-
matically: Hist (Histogram features), DIF (Domain-Independent Features),
SIFT, HOG, and LBP (Local Binary Patterns). The technique generates a
compound solution that extracts high-level features to classify images from
classical problems with low-resolution datasets–about 100 × 100 pixels up to
200 × 200 pixels. Authors compared their approach with other GP-based
methods and DL methods like LeNet-5 (a CNN model with an input of
grayscale images of 32 × 32 pixels, toy-method in comparison with the state-
of-the-art) and two hand-craft CNNs models of ﬁve- and eight-layers without
providing the network parameters’ information. Hence, it is not easy to judge
the performance.

In addition to optimizing feature selection, EC and SI have developed
strategies to search for meaningful DCNN architectures for image classiﬁ-
cation [4]. Nevertheless, recent approaches, which are summarized in [11],
explore hybridization of swarm and evolutionary computation algorithms by
aggregating hyper-parameters’ optimization during training. To give an ex-
ample, in 2019, authors from [12] proposed a novel method named evoCNN,
which uses genetic algorithms for evolving DCNN architectures and con-
nection values to address image classiﬁcation problems. Their experiments
were based on nine datasets that use grayscale images of 28 × 28 pixels:
MNIST, MNIST-RD, MNIST-RB, MNIST-BI, MNIST-RD + BI, Rectangles,
Rectangles-I, Convex, and MNIST-Fashion. However, in 2019, authors from
[13] proposed a novel algorithm based on particle swarm optimization (PSO)

3

named psoCNN, capable of automatically searching DCNN architectures for
image classiﬁcation with fast convergence when compared with others evo-
lutionary approaches like evoCNN, IPPSO, among others. Their experiment
used the same nine datasets mentioned above.

Despite the eﬀort and interest made by the EC and SI communities to
tackle the problem of image classiﬁcation, they still are dealing with outdated
problems using classical datasets while making comparisons against obsolete
DCNN models. EC and SI have fallen short to be on par with DCNN models
with minor works that do not exceed hand-craft DCNN architectures.

Nonetheless, a Deep Genetic Programming Methodology called Brain
Programming, inspired by neuroscience knowledge that uses symbolic rep-
resentations and incorporates rules from expert systems with a hierarchical
structure inspired by the human visual cortex was developed by the EvoVi-
sion research team. In 2016, Evovision started evolving an Artiﬁcial Visual
Cortex (AVC) for image classiﬁcation and object detection. Hern´andez et
al. used natural images of medium size (VGA) using GRAZ-01 (2003), and
GRAZ-02 (2004) datasets, which are the base for the Visual Object Challenge
(VOC challenge)–both still relevant in CV literature–[14]. The results were
compared with several feature extraction methods Basic Moments (2006),
Hierarchical MAX - Genetic Algorithm–HMAX-GA (2012), Enhanced Bi-
ologically Inspired Model–EBIM (2011), SIFT (2006), Similarity Measure
Segmentation–SM (2006), and Moment Invariants (2006) most from CV and
one including EC. In 2017, Hern´andez et al.
[15] implemented a CUDA
version of BP to speed up the original system’s processing time. The exper-
iment analyzed the performance using diﬀerent image sizes, which started
with 256 × 256 pixels, doubling the sizes to up to 4096 × 4096 pixels, demon-
strating the possibility of real-time functionality as well as the application
to high-deﬁnition images. Additionally, the method was compared in time
performance with a CUDA implementation of HMAX and CUDA version of
a CNN with outstanding results.

In 2019, the article [16] proposes a random search to ﬁnd best-ﬁt pa-
rameters for the AVC in image classiﬁcation. The experiment found great
individuals to classify GRAZ-01, GRAZ-02, and Caltech-101 (2004) datasets.
GRAZ datasets have image sizes of 640 × 480 pixels, and Caltech-101 has
images of 300 × 200 pixels. Note that GRAZ images present a signiﬁcant
challenge due to the short object occurrence in the whole image, becoming
challenging to resize images for processing. In contrast, Caltech-101 presents
In 2020, BP was proposed as a tech-
a truly image recognition dataset.

4

nique to approach the complex problem of Art Media Categorization (AMC)
[17]. The experiment consists of classifying high-resolution art datasets such
as WikiArt (2016) and Kaggle Art Images (2018). Moreover, BP results
were compared with a renowned DCNN model named AlexNet, obtaining a
competitive outcome. Also, BP has been evaluated on real-world problems
of object tracking using standard datasets and algorithms–FRAGtrack and
MILtrack–while also achieving outstanding results in real-working conditions
compared to the method of Regions with Convolutional Neural Networks (R-
CNN) [18, 19].

However, despite the progress made to build better image classiﬁers, a
research opportunity that has not been considered in EC and SI is the clas-
siﬁer’s predictions’ robustness. Nowadays, there is a big concern about the
performance of DCNN, which has opened a new research area in charge
of dealing with Adversarial Attacks (AA) that intentionally create small
perturbations in the input image to mislead the model to predict wrongly
[20, 21, 22, 23, 24, 25]. Some of these perturbations are imperceptible to
human vision and can completely change the DCNN’s prediction to drop
its performance. They are generated through a variety of forms, including
making small modiﬁcations to the input pixels, using spatial transforma-
tions, among others. In addition to the analysis of DCNN vulnerabilities,
there have been immense eﬀorts to develop defense mechanisms to mitigate
AA. Still, the perturbations have become more complex and highly eﬃcient
in fooling DCNN. Therefore, in this paper, we want to contrast the clas-
siﬁcation models between performance and robustness to perturbations to
guarantee predictions’ trustworthiness while not focusing only on accuracy.

1.1. Problem Statement

In this section, we detail the serious problem in the DCNN structure to
AA. First, given an input image x in an input subspace X such that x ∈ X
and its corresponding label y, DCNN model establishes a relationship within
the data using the following equation:

y = f (x) = w(cid:124)x ,

(1)

where function f () is the DCNN model, whose associated weights param-
eters are w. However, an erroneous behavior is notable when the input image

5

suﬀers a small change in its pixels xρ = x + ρ such that:

f (x) (cid:54)= f (xρ) s.t.

||x − xρ||p < α

(2)

where p ∈ N, p ≥ 1, α ∈ R, α ≥ 0. So, it can be deﬁned an Adversarial
Example (AE) as an intentional modiﬁed input xρ that is classiﬁed diﬀerently
than x by the DCNN model, with a limited level of change in the pixels of
||x − xρ||p < α, so that it may be imperceptible to a human eye.

The simplest explanation of how AEs work to attack a DCNN is that most
digital images use 8-bit per channel per pixel. So, each step of 1/255 limits
the data representation; the information in between is not used. Therefore,
if every element of a perturbation ρ is smaller than the data resolution, it
is coherent for the linear model to predict distinct given an input x than to
an adversarial input xρ = x + ρ. We assume that forasmuch as ||ρ||∞ < α,
where α is too small to be discarded, the classiﬁers should predict the same
class to x and xρ.

Nonetheless, after applying the weight matrix w ∈ RM ×N to the AE,
we obtain the dot product deﬁned by w(cid:124)xρ = w(cid:124)x + w(cid:124)ρ. Hence, the AE
will grow the activation by w(cid:124)ρ. Note that the dimensionality of the problem
does not grow with ||ρ||∞; thus, the activation change caused by perturbation
ρ can grow linearly with n. As a result, the perturbation can make many
imperceptible changes to the input to obtain big output changes.

DCNN behavior is hugely linear to be immune to AEs, and nonlinear
models such as sigmoid networks are set up to be in the non-saturating
most of the time, becoming them more like a linear model. Hence, every
perturbation as accessible or challenging to compute should also aﬀect the
DCNN. Therefore, when a model is aﬀected by an AE, this image often aﬀects
another model, whether the two models have diﬀerent architectures or were
trained with other databases. They only have to be set up for the same task
to change the result [26].

In this manner, the AE generation ﬁnds an input xρ in the input subspace
X(cid:48) such that xρ ∈ X(cid:48) and f (x) (cid:54)= f (xρ). Nevertheless, we denote robustness
in terms of function continuity. Given a model’s function f () in an input
subspace X such that x ∈ X, if xk → x implies f (xk) → f (x). Equivalently,
f (x) is robust at x, for all y ∈ X, if given a κ > 0, there is a µ > 0 such that
||y − x|| < µ implies |f (y) − f (x)| < κ. Hence, if f (x) is robust for every x,
then f (x) is said to be robust on X.

6

Therefore, the procedure to measure robustness is by using appropriate
statistical tests depending on the results’ properties, standard performance
measures, and the ratio of accuracies. Statistical tests allow us to deter-
mine whether the results obtained are signiﬁcantly diﬀerent and improve the
knowledge of certain aspects over the existing algorithm measures: eﬀective-
ness, eﬃciency, accuracy, or reliability when using artiﬁcial neural networks,
SVM, or other metaheuristics [27].

1.2. Research Contributions

This paper provides insight into adversarial attacks and the motivation
to analyze image classiﬁcation models’ robustness. Therefore, we extend
the ﬁrst results reported at the International Symposium on Visual Com-
puting (ISVC’20), on which we explore the robustness through the complex
image classiﬁcation task of the AMC [28]. In this work, we test a prevail-
ing BoV approach from CV, four state-of-the-art DCNN models (AlexNet,
VGG, ResNet, ResNet101), and the BP algorithm using three AAs (Fast
Gradient Sign Method–FGSM, multiple pixel attack, and adversarial patch).
We remark the following contributions:

• The ﬁrst contribution consists of a proposal of BP’s robustness as a
secure mechanism to deal with AAs for AMC. Since this problem is
diﬃcult, with many artifacts across all studied classes, the results can
be extended to other challenging classiﬁcation tasks.

• The second is a comparative study of predominant image classiﬁcation
methodologies performances from three-diﬀerent research areas (CV,
ML, and EC) considering AAs.

• The third is a statistical analysis of the proposed image classiﬁers from

the standpoint of robustness to AAs.

We organize the present work as follows. Section 2 presents relevant re-
search for the AMC. It covers the complexity of the problem and how it has
been tackle, from handcrafted features to DCNN models and GP methods.
It also presents the concerns about AAs and how the robustness predictions
have not been studied on the AMC. Section 3 outlines each classiﬁcation
method’s structure and the AAs used in this work. Section 4 gives details
about the experimental setup, including the dataset, its construction, the
employed evaluation metrics, and the experimental results’ explanation. Fi-
nally, Section 5 presents the conclusions of this work.

7

2. Related works

The AMC problem in CV has arisen from the need to have automatic
systems for identifying valuable artwork pieces to have a trustworthy anal-
ysis of complex features that can not be subjective as humans are prone to
be. For example, classifying ﬁne art pieces involves a sophisticated selec-
tion of features that distinguish each medium, which is extremely diﬃcult
[29]. Usually, an art expert analyzes the style, genre, and media from art-
work to identify the artist and detect forgeries [30, 31, 17]. Therefore, the
development of automated systems that provide such tasks makes an accu-
rate and robust analysis a critical issue in terms of security. In this manner,
to make a robust analysis of art media, high-resolution images are manda-
tory to provide enough information to maximize carefulness based on the
artwork details. The art style, usually associated with the author’s school,
describes the artists’ distinctive artifacts, visual elements, techniques, and
methods. The form is related to the localization of features at diﬀerent lev-
els. The classical hierarchy of genres ranks history-painting and portrait as
high, while landscapes and still-life are classiﬁed as low because they did not
contain persons.

AMC has been tackled from 3 perspectives: 1) handcrafted feature ex-
traction, 2) deep convolutional neural networks, and 3) genetic programming
methodologies. First, handcrafted engineered features were the principal
method to develop formulas that can extract features to obtain an image
representation to classify an image easily.

One of the ﬁrst works that employ handcrafted features was [32]; here,
the authors proposed a Discrete Cosine Transform (DCT) coeﬃcients scheme
used for feature extraction painter identiﬁcation by classifying the artist’s
style. They build a custom database of approximately 300 grayscale im-
ages from ﬁve painters (Rembrandt, Van-Gogh, Picasso, Magritte, and Dali)
to experiment. Li and Wang [33] proposed using a two-dimensional multi-
resolution hidden Markov model to analyze brush strokes to provide reli-
able information to distinguish artists from ancient Chinese paintings. Their
database consists of 276 grayscale images from ﬁve Chinese artists at a res-
olution of 3000 × 2000 pixels but scaled to 512 on the shorter dimension,
maintaining the aspect ratio. Authors in [34] present a comparative study of
diﬀerent classiﬁcation methodologies based on handcrafted engineered fea-
tures. They contrasted semantic-level features with an SVM, color SIFT and
opponent SIFT with BoV, and latent Dirichlet allocation with a generative

8

BoV topic model for ﬁne-art genre classiﬁcation. In their study, a database of
seven categories of paintings (Abstract, Baroque, Renaissance, Popart, Ex-
pressionism, Impressionism, and Cubism) was used from the Artchive ﬁne-
art dataset using 70 images from each class. Recently, Rosado [35] employed
a BoV implemented using a dense-SIFT method for feature extraction and
Probabilistic Latent Semantic Analysis (PLSA) to make an image analysis of
434 digitized images from paintings, drawings, books, and engravings by An-
toni T`apies. In general, we note that using handcrafted engineered features
makes it possible to obtain encouraging but not perfect results. Over time,
the complexity of these characteristics started to become more challenging
to design. In addition to the designing process of features, the learning al-
gorithm development was a completely independent research area needed to
match the feature extraction.

DCNN have been a breakthrough in many areas of image processing, and
recent works on AMC have presented approaches based on the state-of-the-
art DCNN architectures. Authors in [36] introduced the use of deep convolu-
tional activation features from a DCNN model trained for object recognition
to recognize the style. These features achieve high performance identifying
styles in painting images and outperform most handcrafted engineered fea-
tures. Bar et al.
[37] proposed a compact binary representation combining
the PiCoDes descriptors and the deep convolutional activation features from
a DCNN model to identify artistic styles in paintings showing exceptional re-
sults to classify artwork images from WikiArt using 27 classes. Noord et al.
[38] employed an adaptation of AlexNet to classify artwork styles from Rijks
Museum images. They could visualize the regions with a heatmap from the
artwork that impacts the prediction of style. Cetinic and Grgic [39] utilized
the features extracted from VGG to classify WikiArt database images into
seven genre classes such as portrait, landscape, city, still life, nude, ﬂower,
and animal. They outperform handcrafted engineered features such as SIFT,
gist descriptor, HOG, Gray Level Co-occurrence Matrix (GLCM), and HSV
color histograms with their classiﬁcation method. Seguin et al.
[40] pro-
pose to extract from VGG similar components shared by various artworks
named visual link. These links try to ﬁnd similitude from the paintings of
the same creators or the same schools. The experiment used images from
the Web Gallery of Art database reporting that their method achieves better
performance than handcrafted engineered features such as SIFT.

Sun et al. [41] employed AlexNet and VGG to construct a structure with
two pathways to obtain object and texture features. The DCNN performs

9

the object computation, and the texture pathway uses the Gram matrices
of intermediate features. Authors used in their experiments WikiPaintings,
Flickr Style, and AVA Style databases. Elgammal et al.
[42] proposed an
analysis of strokes in line drawings using a database of 300 digitized drawings
with over 80 thousand strokes. They employ handcrafted engineered features,
deep learned features, and the combination of both to discriminate between
artists at the stroke-level with high accuracy. Also, their work serves to
discover forgeries made by artists. Cetinic et al. [43] performed an extensive
CNN ﬁne-tuning experiment using ﬁve Caﬀe models (CaﬀeNet, Hybrid-CNN
network, MemNet network, Sentiment network, and Flickr network) for ﬁve
diﬀerent art-related classiﬁcation tasks (artist, genre, style, time period, and
association with a speciﬁc national artistic context) on three large ﬁne art
datasets (WikiArt, Web Gallery of Art, and TICC Printmaking Dataset). In
[44], authors employed pre-train DCNN models (AlexNet, VGG, GoogLeNet,
ResNet, DenseNet) to recognize basic artistic media from artworks. They
collected about 1000 artwork images per class (oil-paint brush, pastel, pencil,
and watercolor) through various search engines and websites to classify them.
They obtained comparable results with that of trained humans.

Finally, a GP-like methodology called brain programming has obtained
competitive results compared to a DCNN model for the AMC task [17]. This
technique aims to emulate the brain’s behavior based on neuroscience learn-
ing processes with new symbolic learning. In the experiments, two renowned
databases of high-resolution artwork images are used (art database from
Kaggle and WikiArt) to classify ﬁve art media classes (drawings, engrav-
ing, painting, iconography, and sculpture). The proposed technique achieves
comparable results to AlexNet on a binary classiﬁcation problem.

Although DCNN have obtained exemplary results in solving a wide vari-
ety of computer vision tasks, small perturbations named adversarial attacks
made on the input image turn the learning model’s decision to change its
prediction completely. These perturbations are generated in several forms
that include small modiﬁcations to the input pixels and using spatial trans-
formations, among others. These attacks’ primary purpose is to fool the DL
models prediction intentionally and remain unnoticed to human perception.
Szegedy et al. [45] were the ﬁrst who discovered an unusual weakness where
small perturbations almost invisible to the human vision on the input pixels
can fool a CNN. These attacks also reported high conﬁdence in the model’s
wrong prediction, and even worse, multiple networks were aﬀected using the
same perturbed image. Later, they found that CNN’s robustness against

10

AA could be improved using these images in the training phase. However,
recent studies have highlighted the lack of robustness in well-trained DCNNs
[46, 47]. Goodfellow et al.
[26] designed a method named Fast Gradient
Sign Method (FGSM), which enables eﬃcient computing perturbations for
a given image. Another threat consists of an extreme and straightforward
attack proposed by Su et al. [48], which consists of modifying one pixel in the
image, can fool a CNN. A drawback, however, is that it only works for icon
images. They successfully attacked three diﬀerent network models under this
strategy with high conﬁdence. Moosavi-Dezfooli et al.
[49] discovered sin-
gular perturbations that can misclassify any image; they called it universal
[50] proposed a method to create
perturbations. In this way, Brown et al.
universal, robust, targeted adversarial image patches. These patches are so
compact that they can be printed and used in real-world scenes to fool a
CNN.

Despite signiﬁcant eﬀorts in making defense methods against AAs, the
research works have focused on modifying its training process or modifying
the input image during testing [26, 51, 52], also on changing the structure
of the networks [53, 54, 55] or through external models to classify unseen
examples [56, 57]. Zhang et al. [58] discussed the limitation of the adversarial
training because the attacks have become more and more challenging with
high eﬃciency on the damage.

AMC is a complex problem to solve. Its solution involves a complicated
analysis of features and demands accurate and robust decisions, mostly when
curators work with highly valuable art pieces. The performance of hand-
crafted engineered features methods has been limited to compete with DCNN
through their inability to extract complex features from artworks to build a
better image representation. DCNN have outperformed handcrafted engi-
neered features and has established the leading for the AMC. Nevertheless,
BP has started to demonstrate its competence against DCNN performance
in this area. However, AAs on art media represent a severe threat that has
not been studied to measure the classiﬁer’s reliability. Furthermore, the AA
eﬀect has not been demonstrated to inﬂuence diﬀerent classiﬁcation archi-
tectures’ predictions, such as CV methods, with only one successful GP-like
methodology [28]. Although DCNN have developed defense mechanisms to
diminished the AA eﬀect, it is diﬃcult to ﬁght against all the new and more
complex AA. Thus, even if DL architectures have classiﬁed large-scale sets of
images with multiple classes with outstanding results, this paradigm’s secu-
rity concerns make the solutions unreliable. The brittleness is because, with

11

small perturbations produced on the image, DL can be intentionally fooled.
For example, there are critical areas in museums and galleries, such as artist
identiﬁcation and forgery detection, where the prediction’s conﬁdence must
not depend on a system that can be manipulated by an imperceptible per-
turbation. This catastrophic scenario could lead to forgeries to circulate on
the market or be misattributed to a speciﬁc artist. This article presents a
method that can be used as a ﬁrst defense mechanism by asking general
questions like if the digitized artwork belongs to a certain class before asking
further questions.

3. Methodology

This section describes the data modeling of each method used in this
work. The main goal in data modeling is to summarize the data by ﬁtting it
to a model by establishing a relationship within the data (y, x) given by the
dataset by the following equation:

y = f (x)

,

(3)

where the function f () is the model that depends on adjustable parameters.
Therefore, we detail SIFT + Fisher Vectors modeling as the BoV method
because it was the last computer vision technique that won the image clas-
siﬁcation task on the ImageNet Large Scale Visual Recognition Challenge
(ILSVRC) 2011 before DL models arose. We describe the modeling from
deep neural networks and explain the contributions to the state-of-the-art
from the four DCNN models chosen for this work based on the ILSVRC win-
ners. Next, we present the theory behind BP to introduce function-symbolic
learning for data modeling and the workﬂow from the system. Finally, we
describe the modeling from the three selected AAs to construct the pertur-
bation to induce a misclassiﬁcation such as in Equation (2).

3.1. SIFT + Fisher Vectors

Fisher Vector (FV) is a vectorial representation of the gradient of the sam-
ple log-likelihood concerning a generative model of the data [59]. There are
many advantages to the FV against the BoV. It was proved by [59] that BoV
is a particular case of the FV where is restricted the gradient computation to
the mixture weight parameters of the Gaussian Mixture Model (GMM) [60].
The generative model (GMM) can be understood as a probabilistic visual vo-
cabulary. Nevertheless, FV incorporates additional gradients that improve

12

accuracy. Also, it needs fewer vocabularies with lower computational costs
than BoV, and it is easy to achieve good performance with simple linear clas-
siﬁers. Note that BoV is typically quite sparse while the FV is almost dense,
making FV impractical for large-scale applications due to storage problems.
Nonetheless, a large-scale nearest neighbor search is made to mitigate this
problem using a popular computer vision method named product quantiza-
tion [61]. In practice, it is used SIFT descriptors on a dense multi-scale grid
to compute the FV image representation [59].

In order to construct the FV image representation, it is deﬁned a set of
D-dimensional descriptors extracted from an image X = {xt, t = 1, . . . , T },
a set of SIFT descriptors. FV is a sum of normalized gradient statistics
λ = (cid:80)T
δX
t=1 Lλ∇λ log uλ(xt) with the assumption that all descriptors are in-
dependent. Where Lλ∇λ log uλ(xt) is the normalized gradient statistics com-
puted for each descriptor. Therefore, it can be understood that this operation
is an embedding of the local descriptors xt → φF K(xt) = Lλ∇λ log uλ(xt) in
a higher-dimensional space which helps a linear classiﬁer to model the data
easier as in Equation (3).

These algorithms’ advantage is that it does not require labeled data to
learn the dictionary. Therefore, it can work on limited labeled data situa-
tions. The dictionary learning process can also improve features quality by
providing additional information of them [62, 63]. However, they are not ca-
pable of building features hierarchies, and the process is not merely stacked
one method on the top of other even there have been attempts to make it
deep [64, 65, 66].

3.2. Deep Convolutional Neural Networks

Diﬀerently, ANN starts the idea of designing deep architectures for neural
network models that can extract suﬃcient features along this structure to
allow the ANN to classify images. Deep Neural Networks, where DCNN
is part of it, models the data using Equation (3) employing fDN N () as a
particular form of a nested function, and each one called a layer.

y = fDN N (x) = f3(f2(f1(x)))

,

in such a way that f1 and f2 are vector functions of the following form:

fl(z) = gl(Wlz + bl)

,

(4)

(5)

with l denoting the index of the layer. gl is the activation function that
usually is a nonlinear function, and the model parameters consist of Wl the

13

weights matrix and bl the bias vector. Hence, the minimization problem is
deﬁned by the loss function J(θ, x, y) where the goal is to ﬁnd the best model
parameters for all the layers Θ that ﬁts the data x to the label y.

LeCun et al.

[67] introduced the modern framework of Convolutional
Neural Networks (CNN’s). However, the ﬁrst time that CNN starts attract-
ing attention was with the development of the AlexNet [68], a DCNN model
for the ILSVRC 2012, where it could reduce by half the error rate on the
image classiﬁcation task. AlexNet layer architecture consists of 5 convolu-
tional, three max-pooling, two normalizations, three fully connected layers
(the last with 1000 softmax output), 60 Million parameters, and 500,000
neurons. Additionally, [68] introduced the use of ReLU (Rectiﬁed Linear
Unit) Non-linearity as activation function with the beneﬁts of much faster
training than using tanh or sigmoid functions. To prevent overﬁtting, they
also introduced the dropout method and data augmentation.

Another DL model that brought contributions to the state-of-the-art was
the VGG network from the Visual Geometry Group of the University of
Oxford [69]. VGG network increased the deep of previous networks by cre-
ating VGG-16 and VGG-19. The ﬁrst one uses 13 convolutional layers and
three fully connected layers; the second one employed three additional con-
volutional layers. Also, they reduced the size of the ﬁlters to the smallest
size to capture the notion of up/down, left/right, and center that is a 3x3
ﬁlter. VGG was distinguished for its state-of-the-art performance on recog-
nition and localization tasks at ILSVRC 2014 and other image recognition
datasets.

ResNet [70] (Deep Residual Learning for Image Recognition) also con-
tributed to redeﬁning the layer as a residual learning function on the CNN
architecture. This helps to mitigate the bottleneck problem of the training
phase on CNNs. ResNet showed its capacity to train its architecture with
a depth of up to 152 layers and lower complexity than GoogLeNet. Also,
ResNet won the ILSVRC 2015 on the classiﬁcation task achieving for the
ﬁrst time an error rate of 3.57%. They proposed ﬁve conﬁgurations of the
network: 18-layer, 34-layer, 50-layer, 101-layer, and 152-layer networks.

3.3. Brain Programming

Before we explain the algorithm of BP, we make a brief introduction
to GP algorithms. GP is an evolutionary computation technique inspired
by biological evolution principles [71].
It is considered a derivative of ge-
netic algorithms that evolve individuals’ populations in the form of a tree or

14

computer program (formulas or mathematical expressions). Each individual
computer program is generated depending on the terminal and function sets
established by the user. They are evaluated in terms of how well it performs
in a particular problem. Then, using the Darwinian principle of reproduc-
tion and survival of the ﬁttest and the genetic operators of crossover and
mutation, individuals are evolved to ﬁnd a better ﬁt solution to the problem.
BP is an evolutionary paradigm for solving CV problems reported in
[14, 16, 19]. This methodology extracts characteristics from images through
a hierarchical structure inspired by the brain’s functioning. BP proposes
a GP-like method, using a multi-tree representation for individuals. The
main goal is to obtain a set of evolutionary visual operators (EV Os), also
called visual operators (V Os), which are embedded within a hierarchical
structure called the artiﬁcial visual cortex. The AVC is based primarily on
two models: a psychological model called feature integration theory [72] and
a neurophysiological model called the two pathway cortical model [73]. Thus,
the AVC attempts to emulate the natural process that occurs along the visual
cortex according to the brain’s neurological ventral-dorsal model. This two-
streams model states that the process of acquiring visual information in the
brain follows two main pathways.

The dorsal stream is known as the “where” or “how” stream. This path-
way is where the guidance of actions and recognizing objects’ location in
space is involved and where visual attention occurs. The ﬁrst theory states
that visual attention in human beings is performed in two stages. The ﬁrst
is called the preattentive stage, where visual information is processed in par-
allel over diﬀerent feature dimensions that compose the scene: shape, color,
orientation, spatial frequency, brightness, and motion direction. The sec-
ond stage, called focal attention, integrates the extracted features from the
previous stage to highlight a region of the scene. BP is based on the most
popular theory of feature integration for the dorsal stream from [72], and the
principles of the ﬁrst computational model for visual attention, where the
image is decomposed into several dimensions to obtain a set of conspicuity
maps, which are then integrated into a single map called the saliency map
[74].

The ventral stream is known as the “what” stream. This pathway is
mostly associated with object recognition and shape representation tasks.
Proposed ventral stream models like neocognitron system [75], convolutional
neural networks [67], and HMAX model [76] (the Max principle is used in
BP), start by decomposing the image into a set of alternating “S” and “C”

15

layers. The “S” or simple layers are deﬁned by a set of local ﬁlters applied to
ﬁnd higher-order features, and the “C” complex layers increase the features
invariance by combining units of the same kind. However, BP replaces the
data-driven models with a function-driven paradigm. In the function-driven
process, a set of visual operators is fused by synthesis to describe the image’s
properties. Through a set of experiments, we will show that the discovered
solutions do not rely directly on the data but speciﬁc characteristics; hence,
making the solutions reliable regarding AAs.

Therefore BP can be summarized in two steps: ﬁrst, the evolutionary
process whose primary purpose is to discover functions to optimize com-
plex models by adjusting the operations within them. Second, the AVC,
a hierarchical structure inspired by the human visual cortex, uses the con-
cept of function composition to extract features from images. The model
can be adapted depending on the task, whether it is trying to solve the
focus of attention for saliency problems or the complete AVC for categoriza-
tion/classiﬁcation problems. BP diﬀers from the data-driven models using
a function-driven approach to extract and combine the relevant information
that solves a speciﬁc visual task. Hence, the overall function-driven process
requires the input in a suitable representation; thus, we deﬁne an image I as
the graph-of-a-function.

Deﬁnition 1. Image as the graph of a function. Let f be a function
f : U ⊂ R2 → R. The graph or image I of f is the subset of R3 that
consist of the points (x, y, f (x, y)), in which the ordered pair (x, y) is a point
in U and f (x, y) is the value at that point. Symbolically, the image I =
{(x, y, f (x, y)) ∈ R3|(x, y) ∈ U }.

This deﬁnition is based on the fact that the images result from the impres-
sion of variations in light intensity along the two-dimensional plane. There-
fore, functions are optimized to imitate the functionality of specialized areas
of the brain through a set of operators.

3.3.1. Data Modeling with BP

BP proposes to solve the problem of image classiﬁcation from the stand-
point of data modeling through GP. Therefore, to understand the learning
process of BP, we start deﬁning the minimization problem, which requires to
ﬁnd a solution Pmin ∈ S such that:

∀Pmin ∈ S : f (Pmin) ≤ f (P)

.

(6)

16

Hence, opposed to conventional approaches to ﬁnding the best-ﬁt param-
eters, we would like to ﬁt the data through discovering functions that perform
a classiﬁcation task in BP. The strategy takes several steps because the direct
mapping between the domain and codomain is unknown or not well deﬁned.
In this manner, the solution to the image classiﬁcation problem through BP
requires to deﬁne the following equation:

y = min(f (x, F, T, a))

,

(7)

where (y, x) are the label and the image respectively, given by the dataset;
F represents the set of functions, T deﬁnes the terminal set, and a are the
parameters controlling the evolutionary process. Therefore, in order to solve
the problem, we need two things: 1) a method of feature extraction and 2)
a suitable criterion Q for the minimization.

Therefore, BP is the algorithm in charge of tuning (F, T, a) looking for
optimal feature extraction from the input images using the visual operators
embedded into the artiﬁcial visual cortex (AVC). The criterion for the mini-
mization Q in terms of a classiﬁcation task helps discover the best classiﬁer.
In this particular case, we use an SVM to learn a mapping f (x) that as-
sociates descriptors xi to labels yi. Here, we deﬁne the BP algorithm in
terms of a binary classiﬁcation task, whose main purpose is to ﬁnd a decision
boundary that best separates the class elements.

3.3.2. Evolving an Artiﬁcial Visual Cortex (AVC)

Each individual consists of syntactic trees deﬁning the V Os that con-
structs the AVC structure to extract features from color images. This pro-
cedure gets a descriptor vector that encodes salient characteristics from the
image. Then, we apply an SVM to calculate the classiﬁcation accuracy for
a given training image database to obtain the individual ﬁtness. Hence,
BP uses an evolutionary loop presented in Algorithm 1 to evolve the entire
population represented by a set of AVCs, in which the whole workﬂow is
illustrated in Figure 1.

3.3.3. Structure Representation and Genetic Operations

In BP, an individual is a computer program represented by syntactic
trees embedded into a hierarchical structure. Individuals within the popu-
lation contain a variable number of syntactic trees, ranging from 4 to 12,
one for each evolutionary visual operator (V OO, V OC, V OS, V OI) regard-
ing orientation, color, shape, and intensity; and at least one tree to merge

17

Figure 1: Brain Programming workﬂow. The left side shows the genetic operations, in
the middle is presented the BP’s ﬂow diagram, and the right side illustrates the individual
representation.

Algorithm 1: BP evolutionary process

Input : Training images, Algorithm parameters (see Table 5)
Output: The updated population AVCs
1 Generate a random initial population P0;
2 i = 0;
3 while the termination criterion is not satisﬁed do
4

Evaluate each individual ﬁtness (AVC) in Pi ;
Selection using lexicographic parsimony pressure;
Generate oﬀspring by crossover and mutation;
i = i + 1;

5

6

7

8 return The updated population Pf inal

18

the resulting visual maps, and ﬁnally generate the Mental Maps (MM). All
functions within each V O are deﬁned according to expert knowledge to high-
light characteristics related to the respective feature dimension and updated
through genetic operations.

• Visual Maps

Each input image is transformed to build the set Icolor = {Ir, Ig, Ib, Ic, Im,
Iy, Ik, Ih, Is, Iv}, where each element corresponds to the color components
of the RGB (red, green, blue), CMYK (Cyan, Magenta, Yellow, and black)
and HSV (Hue, Saturation, and Value) color spaces. Elements on Icolor are
the inputs to four V Os deﬁned by each individual. Each V O is a mapping
function applied to the input image to extract speciﬁc features from it, along
with information streams of color, orientation, shape, and intensity; each of
these properties is called a dimension. The output to V O is an image called
Visual Map (V M ) for each dimension.
It is important to note that each
solution in the population should be understood as a complete system and
not only as a list of three-based programs. Individuals represent a possible
conﬁguration for feature extraction that describes input images and is op-
timized through the evolutionary process. Next, we explain the process of
V Os to extract features on each dimension to obtain a resulting V M .

The ﬁrst tree of the individual mimics the orientation. Thus, we evolve
this visual operator (V OO) through a set of specially selected elements to
highlight edges, corners, and other orientation-related features using the set
of terminals and functions provided in Table 1. The input for the functions
can be any of the terminals, as well as the composition among them; Gσ
are Gaussian smoothing ﬁlters with σ = 1, 2; and Du represents the image
derivatives along the direction u ∈ x, y, xx, yyxy. These operators emulate
the functionality of the V1 region presented in the primary visual cortex.

The second operator encodes the color dimension emulating the color-
sensitive cells in the visual cortex. The visual operator of color(V OC) re-
produces the color perception process to ﬁnd prominent regions with color
properties in the image. Note that some functions of V OC are the same
as those in V OO plus the function complement() that provides a negative
image that complements an intensity or RGB value (see Table2). Thus, in
the output image, dark areas become lighter, and light areas become dark.
Opponent terminals perform a ﬁxed operation between the color bands that
builds a new image with the maximum values between them. For example,
Op r,g accentuates the diﬀerence between the red and green bands.

19

Dimension Functions

Description

Terminals

Description

V OO

Elements of
Icolor and its
derivatives

√

A + B, A − B,
A/B,
A × B,
k − A,
k + A,
k × A, A/k,
|A|,
|A + B|, |A − B|,
log(A), (A)2,
A,
(cid:98)A(cid:99),
round(A),
(cid:100)A(cid:101),
inf (A, B),
sup(A, B),
Gσ=1(A), Gσ=2(A),
Dy(A),
Dx(A),
thr(A)

images

functions
Arithmetic
between
or
constants k, absolute
trascendental
values,
functions,
square,
square root, rounding
inﬁmum,
functions,
supremum,
convolu-
tion with a Gaussian
ﬁlter, derivatives, and
threshold applied to
images A and/or B

Ir, Ig, Ib,
Ic, Im, Iy,
Ik, Ih, Is,
Iv, Dx(Ix),
Dxx(Ix),
Dy(Ix),
Dyy(Ix),
Dxy(Ix)

Table 1: Functions and terminal list for the V OO.

Dimension Functions

Description

Terminals

Description

V OC

A + B, A − B,
A×B, A/B, k +A,
k − A, k × A, A/k,
exp(A),
log(A),
(A)c,
(A)2,
round(A),
(cid:98)A(cid:99),
(cid:100)A(cid:101), thr(A)

A,

√

images

functions
Arithmetic
or
between
trascen-
constants k,
functions,
dental
square
root,
square,
complement,
image
functions
rounding
and threshold applied
to images A and/or B

Ir, Ig, Ib,
Ic,
Im,
Ik,
Iy,
Ih, Is, Iv,
Opr−g(I),
Opb−y(I)

Elements of
and
Icolor
op-
color
ponencies:
red-green
and
yellow

blue-

Table 2: Functions and terminal list for the V OC.

The third tree is the visual operator of shape. The method that extracts
visual information from the object’s shape employing V OS from Table3,
which utilize the morphological information of the artifacts in the image.
BP proposes to create compound operators by the composition of basic mor-
phological operators such as dilation, erosion, open, close with disk, square,
and diamond structural elements. Indeed, more complex operators can be
created from these operators. The goal of extracting shape information is to
highlight morphological information that can be used for object recognition.
Finally, the intensity measure corresponds to the amount of light per-
ceived by a photosensitive device. In humans, the intensity is measured by
specialized ganglion cells in the retina. Then, the following formula is applied

20

Dimension Functions

Description

Terminals

Description

Elements of
Icolor

Ir, Ig, Ib,
Ic, Im, Iy,
Ik, Ih, Is,
Iv

V OS

A + B, A − B,
A × B, A/B, k +
A, k − A, k ×
A, A/k, round(A),
(cid:98)A(cid:99), (cid:100)A(cid:101), thr(A),
A ⊕ SEd, A ⊕ SEs,
A⊕SEdm, A(cid:9)SEd,
A(cid:9)SEs, A(cid:9)SEdm,
A (cid:125) SEs, A (cid:12) SEs,
Sk(A), P erim(A),
A (cid:126) SEd, A (cid:126) SEs,
A (cid:126) SEdm, That(A),
Bhat(A)

images

functions
Arithmetic
between
or
constants k, rounding
threshold,
functions,
morphological
and
dilation,
operators:
close
erosion,
with disk, square, and
diamond
structural
element; skeleton, hit
or miss, bottom-hat,
top-hat

open,

Table 3: Functions and terminal list for the V OS.

to compute the visual map of intensity.

V MInt =

Ir + Ig + Ib
3

.

(8)

• Conspicuity Maps

The next procedure is the center-surround process;

it eﬃciently com-
bines the information from the V M s and is useful for detecting scale in-
variance in each of the dimensions. This process is performed by apply-
ing a Gaussian smoothing over its corresponding V Md at nine scales P σ
d =
{P σ=0
}; this processing reduces the visual map’s size
d
by half on each level forming a pyramid. Subsequently, the six levels of the
pyramid are extracted and combined.

, ..., P σ=7

, P σ=1
d

, P σ=8
d

d

Qj

d = P

σ=(cid:98) j+9
d

2 (cid:99)+1

− P

σ=(cid:98) j+2
d

2 (cid:99)+1

,

(9)

where j = 1, 2, ..., 6. Since the levels P σ
d have diﬀerent sizes, each level is nor-
malized and scaled to the visual map’s dimension using polynomial interpo-
lation. This technique simulates the center-surround process of the biological
system. After extracting features, the brain receives stimuli from the vision
center and compares it with the receptive ﬁeld’s surrounding information.
The goal is to process the images so that the results are independent of scale
changes. The entire process ensures that the image regions are responding to

21

the indicated area. This process is carried out for each characteristic dimen-
sion (V Md); the results are called Conspicuity Maps (CM ), focusing only
on the searched object by highlighting the most salient features. This early
stage of the system follows the psychological model of visual attention, which
involves the objects’ location in space as the artiﬁcial dorsal stream pathway.

• Mental Maps

After obtaining the most saliency features, the next stage along the AVC
is to compute the Mental Maps (M M s) to deﬁne a descriptor vector used as
input to a classiﬁer for categorization purposes. This procedure is analogous
to the artiﬁcial ventral stream pathway. Hence, the information from CM s
is synthesized to build the set of M M s, which discriminates unwanted infor-
mation. The AVC model uses a set-of-functions to extract the images’ dis-
criminant characteristics (see Table 4); it uses a functional approach. Thus,
a set of k V Os is applied to the CM s for the construction of the M M s.
These V Os correspond to the remaining part of the individual that has not
been used. Unlike the operators used for the V M s, the operators’ whole set
is the same for all the dimensions. These operators ﬁlter the visual infor-
mation and extract the information that characterizes the object of interest.
Then, using Equation (10), where d is the dimension, and k represents the
cardinality of the set of V OM Mk, we apply the M M s for each dimension.

M Md =

k
(cid:88)

i=1

V OM Mi (CMd)

(10)

Dimension Functions

Description

Terminals

Description

V OM M

A + B, A − B,
A × B,
A/B,
|A + B|, |A − B|,
log(A), (A)2,
A,
Gσ=1(A), Gσ=2(A),
Dx(A), Dy(A)

√

images

functions
Arithmetic
between
or
constants k, absolute
values, transcendental
square,
functions,
square root, convolu-
tion with a Gaussian
ﬁlter, and derivatives

CMd,
Dx(CMd),
Dxx(CMd),
Dy(CMd),
Dyy(CMd),
Dxy(CMd)

Conspicuity
Maps and its
derivatives

Table 4: Functions and terminal list for the set V OM M .

22

• Genetic Operations

Individuals are selected from the population using a proportionate ﬁtness
method called lexicographic parsimony pressure to participate in the genetic
recombination from the individuals’ multi-tree representation. This method
consists of assigning to each solution a selection probability proportional to
their ﬁtness value while preferring smaller trees when ﬁtness is equal. The
best individuals are retained to apply genetic operators to create the new
oﬀspring.

Like genetic algorithms, BP executes the crossover between two selected
parents at the chromosome level using a “cut-and-splice” crossover. Thus, all
data beyond the selected crossover point is swapped between both parents
A and B. The result of applying a crossover at the gene level is performed
by randomly selecting two subtree crossover points between both parents.
The selected genes are swapped with the corresponding subtree in the other
parent. The chromosome level mutation leads to selecting a given parent’s
random gene to replace such substructure with a new randomly mutated
gene. The mutation at the gene level is calculated by applying a subtree
mutation to a probabilistically selected gene; the subtree after that point is
removed and replaced with a new subtree. These genetic operators allow
the variation of the genetic material while promoting individuals’ genetic
innovation through all levels and maintaining the diversity of the population.

3.3.4. Fitness Function

The following stage in the model is the construction of the image descrip-
tor vector (DV ). The system concatenates the four M M s and uses a max
operation to extract the n highest values; these values are used to construct
the DV . Once we get the DVs from images in the database, a classiﬁer as-
sociates the domain given by the descriptors to the labels’ codomain. In this
work, we use an SVM working with the discriminate hyperplane deﬁned by:

f (x) =

l
(cid:88)

i=1

αiyiK (xi, x) + b,

(11)

where the given training data is (xi, yi), i = 1, . . . , l, yi ∈ {−1, 1}, xi ∈ Rp
and K(xi, x) is the kernel function. The sign of the output indicates the class
membership of x. Thus, ﬁnding the best hyperplane is performed through
an optimization process that locate the margin between the class and non-
class as the search criteria. Therefore, the minimization problem on the

23

learning pentuple from Equation (7) remains as ((x, y) , F , T , a, Q). Thus,
the accuracy obtained by the SVM indicates the ﬁtness of the individual 1.

3.3.5. Initialization, GP parameters, and Solution Designation

Once we deﬁne the AVC structure from each individual, we set the pa-
rameters of the evolutionary process of BP (see Table 5) and establish the
image database. Next, a random initial population is created using a ramped
half-and-half technique, which selects half of the individuals with the grow
method and half with the full method. The full method makes balanced
trees according to the maximum initial depth, while the grow method makes
unbalanced trees allowing branches of varying lengths. Here we set a limit of
maximum depth to avoid uncontrolled growth of trees over time. Tree depth
is dynamically set using two maximum values to limit any individual’s size
within the population. The dynamic max depth is a maximum value that
may not be surpassed unless the individual’s ﬁtness is better than the best
solution found so far. If it occurs, the dynamic max depth value is updated
to the new ﬁttest individual. The real max depth is a hard limit that no in-
dividual may surpass under any circumstances. Selection is carried out using
a tournament with lexicographic parsimony pressure while keeping the best
individual. Finally, the evolutionary process is terminated until one of these
two conditions is reached: 1) an acceptable classiﬁcation rate or 2) the total
number of generations. Thus, the evolutionary process reaches an optimum
population that contains the best solution to the problem.

3.4. Adversarial Attacks

Adversarial attacks are classiﬁed depending on the model’s available in-
formation and the desired attack to predict a speciﬁc class. Hence, we choose
three diﬀerent attacks: a white box untargeted (FGSM), a black box untar-
geted (one pixel attack), and a targeted attack (Adversarial Patch), which
will be explained in the following paragraphs.

3.4.1. Fast Gradient Sign Method

The Fast Gradient Sign Method proposed by [26], is the most widely used
method for computing AEs given an input image due to its easy implementa-
tion (See example images in Figure 2). It proposes to increase the loss of the

1The accuracy denoted in this Section has the purpose of optimizing BP; nevertheless,
the accuracy indicated in Section 4.2 refers to the metric to measure the attack responses.

24

Parameters

Description

Generations
Initial Population
Crossover at chromosome level
Crossover at gene level
Mutation at chromosome level
Mutation at gene level
Tree depth
Dynamic max depth
Real max depth
Selection

Survival

30
30
0.4
0.4
0.1
0.1
Dynamic depth selection
7 levels
9 levels
Tournament with lexicographic
parsimony pressure
Elitism

Table 5: Initialization parameters for each GP applied in the BP algorithm.

classiﬁer by solving the following equation: ρ = (cid:15) sign(∇J(θ, x, yl)), where
∇J() computes the gradient of the cost function around the current value of
the model parameters θ with the respect to the image x and the target label
yl. sign() denotes the sign function, which ensures that the magnitude of the
loss is maximized and (cid:15) is a small scalar value that restricts the norm L∞ of
the perturbation.

The perturbations generated by FGSM take advantage of the linearity of
the DL models in the higher dimensional space to make the model misclassify
the image. The implication of the linearity of DL models discovered by
FSGM is that exists transferability between models. Authors in [77] reported
that with the ImageNet dataset, the top-1 error rate using the perturbations
generated by FGSM is around 63-69% for (cid:15) ∈ [2, 32].

3.4.2. One Pixel Attack

The one pixel attack was planed in a minimal scenario where only one
pixel is changed in the image to fool the DL models using images of a reduced
size of 32 × 32 pixels. With these limitations, Su et al. successfully fool three
diﬀerent CNN models on 70.97% of the testing images with the modiﬁcation
of just one pixel per image [48]. Also, it was reported that the average
conﬁdence of the CNNs on the wrong prediction on the pictures was 97.47%.
The one pixel adversarial perturbations are based on a black-box attack,
on which no information about the network is required. It uses a population-
based optimization algorithm for solving complex multi-modal optimization

25

Figure 2: Example images of computing the FGSM using ResNet101 from each class with
a scale factor of (cid:15) = 2, 4, 8, 16, 32.

problems named Diﬀerential Evolution [78] to generate the attack. It searches
a solution from a vector space R5 that contains (x,y) coordinates limited by
the image size and the three bands of the RGB color values. Within a
population, it randomly modiﬁes the ﬁve-dimensional individuals’ elements
to create new oﬀspring such that they compete in the current iteration to
obtain better ﬁtness. In the case of two pixels, an individual has a vector
space R10 that contains the coordinates and colors values of both pixels, and
so on for individuals with more pixels. During the run, the algorithm used
the probability of the predicted label to compute the ﬁtness criterion. The
last surviving individual is used to modify the pixels in the image.

In summary, let the vector x = (x1, . . . , xn) be a n-dimensional image,
which is the input of the target classiﬁer f that predict correctly the class
t from the image. The probability of x associated to the class t is fl(x).
It builds an additive adversarial perturbation vector e(x) = (e1, . . . , en) ac-
cording to x, the class target and the limitation of maximum modiﬁcations
d, a small number that express the dimensions that are modiﬁed while other
dimensions of e(x) left as zeros. For targeted attacks, the main purpose is

26

Originaleps2eps4eps8eps16eps32ResNet101IconographyPain(cid:1)ngDrawingsSculptureEngravingbwEngravingcolorFigure 3: Example images of the multiple pixel attack using d = 10, 000 for each class.
Each column shows three sample images from the Wikiart database.

to ﬁnd the optimal solution e(x)∗ that solves the following equation:

max
e(x)∗

ftarget(x + e(x))

s.t.

||e(x)||0 ≤ d .

(12)

Hence, the case of one pixel attack is d = 1, but it can be extended to
multiple pixels by increasing d. It should be noticed that one pixel attack
was performed on DL models with inputs from CIFAR 10 dataset. So, it
represents a considerable modiﬁcation of such tiny images; nevertheless, it is
insigniﬁcant with the databases studied in the present work. Therefore, we
use a multiple pixel attack d >> 1 in order to work with real size images. It
should be noted that increasing the number of pixels in this attack will raise
the perturbation risk to be noticeable (See example images in Figure 3).

3.4.3. Adversarial Patch

The Adversarial Patch opposed to traditional strategy for creating a
targeted AE by ﬁnding a maximum perturbation e(x) that maximizes the
ftarget(x + e(x)) is a method to replace a perturbation on the whole image
with a patch (See Figure 4). The robustness of these patches resides on the
wide variety of transformations on which they can attack any image and tar-
get the classiﬁer prediction to the desired class. Also, they work in real work
environments where they can be printed, photographed, or even when the
patch is too small; they can make to ignore the whole scene to predict the
target class.

27

Figure 4: Example images of the adversarial patch. Each column represents the classes
from the Wikiart database, and each row represents the corresponding patch from the
DCNN model.

To build patch ˆp, it was used a variant of the Expectation over Trans-
formation (EOT) framework, on which the patch is trained to optimize the
following equation:

ˆp = arg max

Ex∈X.t∈T.l∈L[log f (y, A(p, x, l, t))]

,

(13)

ˆp

where X is a training set of images, T is a distribution over transforma-
tions of the patch, L is a distribution over locations in the image, and (y, x)
are the label and the image vector respectively. The expectation over the
training images improves the patch’s eﬀectiveness, regardless of what is in
the background. It was proved by [50] the patch’s universality using several
images with diﬀerent backgrounds. A variation of this method is to add a
constraint of the form ||p − porig||∞ < (cid:15) to the patch objective in order to
camouﬂage it. The constraint enforces the ﬁnal patch to be within (cid:15) in the
L∞ norm of some starting patch porig.

4. Experiments

Robust classiﬁcation is a highly valuable characteristic regarding auto-
matic system development following security and conﬁdence of art pieces’

28

AlexNet PatchesVGG PatchesResnetPatchesResnet101PatchesIconographyPain(cid:1)ngPain(cid:1)ngLandscapesDrawingsSculptureEngravingbwEngravingcolorIn this study, we analyze the algorithms’ performance using
predictions.
accuracy. Besides, we use the accuracy ratio between adversarial examples
and clean images to measure robustness. Moreover, we propose a statistical
analysis of each classiﬁer’s predictions’ conﬁdence to corroborate the results.
Therefore, this experiment consists of studying the accuracy and robustness
against AAs using three of the main approaches for image classiﬁcations:

• traditional handcrafted features algorithm (SIFT+FV)

• Deep Genetic Programming Methodology (BP)

• DCNN models (AlexNet, VGG, ResNet18, and ResNet101).

We consider unconventional training, validation, and test datasets since
we apply two diﬀerent image databases compiled by experts for AMC. Train-
ing and validation datasets are constructed from the Kaggle database, while
testing uses a standard database WikiArt (See Table 6). The aim is to emu-
late a real-world scenario where the proposed models are tested with standard
benchmarks.

This work analyzes the threat of using three types of AA to the model
mentioned above. The white box untargeted (FGSM) determines the impact
from an easy and direct threat to DCNN by knowing its parameters. Also, we
study the transferability eﬀect on other DCNN models, extending to BP and
SIFT+FV, which are diﬀerent architectures. We analyze the behavior of such
perturbations from these architectures, which can cause wrong predictions
with the addition of subtle texture to the artworks. The black box untargeted
(multiple pixel attack) analyzes the hazard from an attack that tries to ﬁnd
locations and pixel values to build a perturbation that changes the model’s
prediction from an artwork image. The targeted attack uses the adversarial
patch to challenge the robustness of such modiﬁed image patches, which
can be rotated, put on random locations, and printed to appear in real-world
conditions in the artwork to cause a misleading prediction of the target class.
Additionally, we analyze the transferability eﬀect of such patches through all
models.

4.1. Datasets

We use the same datasets from the experiment of AMC reported in [17].
The training and validation set of images are obtained from the Kaggle web-
site from the digitized artwork dataset. This dataset comprises ﬁve categories

29

of art media: drawing, painting, iconography, engraving, and sculpture. The
engraving class consists of two diﬀerent kinds; most of them were black and
white art pieces. The other style was Japanese engravings, which introduce
color to the images. So, the engraving class was split into engraving black
and white and engraving color. It is used for testing a standard database
WikiArt, where it was selected the images from the same categories. Since
the Wikiart engraving class is grayscale, the ukiyo-e class (Japanese engrav-
ings) from Wikiart was used as the engraving color class. Also, the set of
images of the category landscapes, which are painting from renowned artists,
is added to test the painting class. Table 6 provides the number of artworks
for each dataset.

Iconography Painting

Drawings

Sculpture

Engraving
BW

Engraving
Color

1038
1038
251

Train
Validation
Wikiart
Wikiart
Land-
scapes

1021
1021
2089
136

553
553
204

868
868
116

426
283
695

30
19
1167

Caltech
Back-
ground
233
233
233

Table 6: Total number of images per class obtained from Kaggle and Wikiart
databases

4.2. Evaluation metrics

We employ classiﬁcation accuracy as a measure of performance for the
classiﬁers, which is simply the rate of correct classiﬁcations given by the
following formula:

Accuracy =

1
N

N
(cid:88)

n=1

d(y(cid:48)

n, yn)

,

(14)

where N is the total of test images, y(cid:48)
n is the predicted label for the image
n, yn is the original label for the image n, and d(x, y) = 1 if x = y and 0
otherwise.

Additionally, as a robustness measure, we used the accuracy ratio between
adversarial examples and clean images implemented by [77]. This metric
means that if the ratio reaches one, the accuracy of AEs and the clean images
is the same. Nevertheless, if it tends to zero, that means that the AA worked

30

to fool the classiﬁer. If this ratio exceeds 1, it implies that the AA is helping
to correct misclassiﬁed images. The following equation calculates the ratio:

Ratio =

accadv
accclean

,

(15)

where accadv is the classiﬁcation accuracy on AEs, and accclean is the classi-
ﬁcation accuracy on the clean images.

4.3. Implementation details

In this subsection, we outline the implementation details for all learned

models:

• Brain Programming: was implemented on Matlab using a modiﬁed

version of GP Lab and the libsvm library for the SVM.

• SIFT+FV: was implemented on Matlab using VLFeat libraries for the
It was used the SVM

SIFT description, GMM, and Fisher Vectors.
provided by Matlab.

• DCNN: for the implementation of the four models (AlexNet, VGG,
ResNet18, and ResNet101), we use the pre-trained models from Py-
Torch v1.1. These models were retrained using transfer learning for
the art media problem.

Also, we outline each of the AA:

• FGSM: was implemented in PyTorch v1.1 using the validation and test
datasets to compute AEs with standard values for scale (cid:15) = 2, 4, 8, 16, 32
for all the DCNN models.

• Multiple pixel attack: was implemented using 100 random images from
the test dataset (50 from each class) in Matlab and Python. Python
version was programmed using the diﬀerential evolution with the Pygmo
library, and Matlab’s version used the diﬀerential evolution library
available from their ﬁle exchange website. Both implementations used
the same settings of 50 individuals, 30 generations, a crossover proba-
bility of 0.9, and d = 10, 000 pixels.

31

• Adversarial Patch: was implemented using 100 images from the train-
ing dataset for each DCNN model in PyTorch v1.1 with the following
parameters set to build the patch: patch size of 50 × 50 pixels, a max of
100 iterations per image with a stop criteria of 0.9 posterior probability
of the target class. As we deﬁned the binary classiﬁcation problem, we
choose the background class as the target prediction to measure the
number of class images that predict the model as the target class.

4.4. Results

The results obtained from the experiments mentioned above are presented

and discussed in the following subsection.

4.4.1. FGSM

In Table 7, we present the results for the training and validation datasets
from Kaggle along with the AEs computed using FGSM for all DCNN models.
We report classiﬁcation accuracy at each stage of training and validation next
to all models’ accuracy tested with the AEs. Here, we want to measure the
inﬂuence in the prediction of the FGSM in two manners: 1) direct, since
we know the model’s parameters and perturbation, and 2) indirect, through
the transferability of the attack. Previously, other researchers reported that
AEs could aﬀect diﬀerent CNN models by setting them up for the same task.
Still, we want to extend the analysis to diﬀerent architectures such as BP
and SIFT+FV that could be aﬀected by these subtle perturbations to the
digitized artworks.

First, we observed that SIFT+FV models appeared to be overﬁtted.
Hence, we perform two types of veriﬁcations presented in Table 8. We use
the hyperparameters optimizer from Matlab and the crossval function that
validates the model using a 10-fold cross-validation. After ten runs, the hy-
perparameters optimizer returns the best model for each class. The results
over the train and validation datasets are listed in the optimizer column at
Table 8. The crossval function randomly partition the data into ten sets of
equal size, later train an SVM classiﬁer on nine sets, and repeat the process
ten times. After that, we computed the mean accuracy at train and valida-
tion datasets for each class over each ten models. We present the results in
the cross-validation column at Table 8. We obtained the same results as the
original experiment. Then, the results showed that the data do not overﬁt
the models.

32

Therefore, it can be observed in Table 7 how drastically can be dropped
the performance of DCNN. The worst-case was the sculpture class, the VGG’s
performance went from 97.62% to 14.38%, AlexNet dropped from 95.78% to
14.57%, ResNet18 diminished from 96.88% to 19.07%, and ResNet101 de-
creased from 97.89% to 37.86%. Also, it is perceived that the transferability
eﬀect between the DCNN models is more signiﬁcant at (cid:15) = 32. The draw-
ings class presents almost the same behavior as the sculpture class, where the
other networks are aﬀected by AEs. For all other classes, the eﬀect is unno-
ticeable, but the accuracy is signiﬁcantly aﬀected when the model matches
the AE.

In some cases, SIFT+FV was aﬀected by FGSM. For example, in the
drawing class, the performance was reduced by almost 8%. And for the
painting, the accuracy was decreased approximately by 4%. This result
shows a partial transferability of AEs to SIFT+FV because regardless of
applying DCNN, the perturbation compromised these two classes’ perfor-
mance. However, BP maintains its performance in almost every test; the
accuracy variation through all the analysis was less than 2%. Figure 5 illus-
trates an example showing that the generated maps from the AVC do not
suﬀer any change in their responses with the FGSM.

Figure 7 presents the results of Table 7 using the accuracy ratios between
adversarial examples and clean images. We observe that the variation of BP
is imperceptible in comparison with SIFT+FV and DCNN models. Also,
we noted that the performance of DCNNs drastically dropped in almost all
classes reaching less than 20% of its original accuracy when the perturbation
matches the network design. In all other cases, the attack reduces the ac-
curacy to about 20% of the actual performance considering clean images for
the classes Sculpture, Engraving BW, and Engraving Color.

The testing stage exhibited an even worse behavior compared with the val-
idation dataset for the DCNN and SIFT+FV. The drop in the transferability
performance was higher when the scaling factor (cid:15) becomes larger. Table 9
shows that the accuracy was compromised in all DCNN models for three
classes: Painting, Drawings, and Engraving Color. For example, the worst-
case is Engraving Color, where AlexNet fell to 17.22% from a clean score of
94.72%, VGG and ResNet18 diminished their performance to almost 5% of
accuracy after scoring 99% and 96%, respectively, and ResNet101 achieves
49%, which was the less aﬀected in accuracy. Moreover, the experimental re-
sults in Table 9 provide the FGSM transferability in DCNN models. Notice
that the eﬀect on (cid:15) = 32 reaches the more signiﬁcant changes. Also, the test

33

train
92.84
BP
SIFT+FV 99.92
99.61
AlexNet
100
VGG
100
ResNet18
100
Resnet101

train
99.68
BP
SIFT+FV 99.76
98.96
AlexNet
99.92
VGG
100
ResNet18
100
Resnet101

train
96.56
BP
SIFT+FV 99.87
96.44
AlexNet
99.75
VGG
99.87
ResNet18
99.87
Resnet101

train
93.19
BP
SIFT+FV 99.55
99.36
AlexNet
100
VGG
100
ResNet18
100
Resnet101

train
89.76

BP
SIFT+FV 100
AlexNet
VGG
ResNet18
Resnet101

99.76
100
100
100

train
98.33

BP
SIFT+FV 100
100
AlexNet
100
VGG
95.00
ResNet18
100
Resnet101

AlexNet
(cid:15)2
91.42
95.91
96.3
99.29
98.66
99.21

(cid:15)4
91.42
95.91
96.3
99.29
98.66
99.21

(cid:15)2
98.25
92.08
93.46
97.93
97.93
98.72

(cid:15)2
90.59
83.84
85.75
95.29
94.27
95.8

(cid:15)2
92.79
87.44
90.93
98.26
97.25
98.44

(cid:15)2
92.23
94.35
96.11
99.82
100
100

(cid:15)2
97.37
44.74
73.68
100
97.37
100

(cid:15)4
98.25
92.08
93.46
97.93
97.93
98.72

(cid:15)4
90.59
83.84
85.75
95.29
94.27
95.8

(cid:15)4
92.79
87.44
90.93
98.26
97.25
98.44

(cid:15)4
92.23
94.35
96.11
99.82
100
100

(cid:15)4
97.37
44.74
73.68
100
97.37
100

val
91.42
95.91
98.66
99.21
98.9
99.37

val
99.04
92.24
97.69
98.17
97.85
98.56

val
90.59
83.84
91.35
95.42
94.44
95.8

val
93.26
87.35
95.78
97.62
96.88
97.89

val
92.05
93.99
99.29
100
100
100

val
97.37
50.00
100
100
100
100

(cid:15)8
91.42
95.91
83.24
99.06
98.66
99.21

(cid:15)8
98.48
92.00
83.01
97.53
97.93
98.48

(cid:15)8
90.59
83.97
66.79
94.78
93.64
95.42

(cid:15)8
92.79
85.79
63.24
97.89
96.88
98.17

(cid:15)8
92.23
94.70
78.62
99.82
99.82
100

(cid:15)8
97.37
44.74
23.68
100
97.37
100

(cid:15)16
91.42
95.59
52.56
98.82
98.9
99.06

(cid:15)16
98.41
89.84
66.99
96.73
97.45
98.17

(cid:15)16
90.59
83.46
44.91
93.51
92.37
93.89

(cid:15)16
92.79
85.15
27.50
94.87
95.05
96.06

(cid:15)16
91.70
94.17
56.71
99.65
99.82
99.82

(cid:15)16
97.37
44.74
13.16
100
97.37
100

(cid:15)32
91.42
94.26
38.39
96.85
97.95
97.72

(cid:15)32
98.48
87.84
69.3
92.82
96.33
96.65

(cid:15)32
90.59
81.30
35.62
87.02
86.9
89.31

(cid:15)32
92.79
83.68
14.57
78.28
80.66
87.08

(cid:15)32
91.87
92.76
47.88
99.29
98.94
99.47

(cid:15)32
97.37
50.00
15.79
97.37
81.58
97.37

VGG
(cid:15)2
91.42
95.83
98.51
91.9
98.66
99.29

(cid:15)2
98.78
92.16
97.53
89.31
97.77
98.64

(cid:15)2
90.59
84.22
90.84
74.43
93.38
95.55

(cid:15)2
92.79
87.26
95.78
84.69
96.88
98.44

(cid:15)2
91.70
94.35
99.12
98.53
99.82
100

(cid:15)2
97.37
47.37
100
97.37
97.37
100

(cid:15)4
91.42
95.83
98.51
91.9
98.66
99.29

(cid:15)4
98.8
92.16
97.53
89.31
97.77
98.64

(cid:15)4
90.59
84.22
90.84
74.43
93.38
95.55

(cid:15)4
92.79
87.26
95.78
84.69
96.88
98.44

(cid:15)4
91.70
94.35
99.12
97.53
99.82
100

(cid:15)4
97.37
47.37
100
97.37
97.37
100

Iconography

(cid:15)8
91.42
96.07
98.43
47.05
98.66
99.06

(cid:15)16
91.42
95.52
98.03
17.7
98.03
98.9
Painting

(cid:15)8
98.64
92.08
97.13
32.14
97.05
98.25

(cid:15)16
98.33
90.48
96.89
14.27
96.33
96.49
Drawings

(cid:15)8
90.59
84.48
91.22
28.75
91.22
93.89

(cid:15)16
90.59
83.59
90.59
15.78
86.77
90.84
Sculpture

(cid:15)32
91.42
94.57
97.64
16.76
95.83
97.01

(cid:15)32
98.41
88.08
96.41
14.91
93.22
93.86

(cid:15)32
90.59
81.93
88.55
14.38
77.48
83.33

(cid:15)16
92.7
85.15
94.68
17.87
92.39
95.42

(cid:15)8
92.79
86.34
95.42
37.76
96.15
98.08
Engraving BW
(cid:15)8
92.06
94.35
99.12
73.14
99.82
99.82

(cid:15)16
91.87
94.17
98.94
49.29
99.65
99.82

(cid:15)32
92.79
84.42
89.55
14.21
77.54
84.88

(cid:15)32
91.70
93.64
98.41
47.17
98.23
99.47

Engraving Color

(cid:15)8
97.37
47.37
100
26.32
97.37
100

(cid:15)16
97.37
47.37
100
15.79
89.47
100

(cid:15)32
97.37
50.00
94.74
13.16
81.58
97.37

ResNet18
(cid:15)4
(cid:15)2
91.42
91.42
95.75
95.75
98.58
98.58
99.21
99.21
90.24
90.24
99.37
99.37

(cid:15)2
98.8
91.92
97.45
97.69
86.92
98.72

(cid:15)2
90.59
84.22
91.09
94.78
72.9
95.29

(cid:15)2
92.88
87.35
95.88
98.08
84.88
98.35

(cid:15)2
91.70
94.35
99.12
99.82
95.58
100

(cid:15)2
97.37
47.37
100
100
52.63
100

(cid:15)4
98.8
91.92
97.45
97.69
86.92
98.72

(cid:15)4
90.59
84.22
91.09
94.78
72.9
95.29

(cid:15)4
92.88
87.35
95.88
98.08
84.88
98.35

(cid:15)4
91.70
94.35
99.12
99.82
95.58
100

(cid:15)4
97.37
47.37
100
100
52.63
100

(cid:15)8
91.42
96.07
98.66
98.98
52.01
99.21

(cid:15)8
98.56
91.76
96.89
97.05
43.94
98.17

(cid:15)8
90.59
84.22
91.09
93.13
31.04
93.13

(cid:15)8
92.79
85.98
95.78
97.07
45.92
96.98

(cid:15)8
92.23
94.52
99.12
99.82
78.98
99.65

(cid:15)8
97.37
44.74
100
100
13.16
100

(cid:15)16
91.42
95.52
98.03
98.74
29.03
97.95

(cid:15)16
98.64
90.08
96.97
95.45
31.82
95.85

(cid:15)16
90.59
82.95
90.59
88.68
23.41
88.68

(cid:15)16
92.79
84.97
94.13
91.38
25.30
92.30

(cid:15)16
92.05
94.17
98.94
99.82
64.49
99.65

(cid:15)16
97.37
47.37
100
100
02.63
97.37

(cid:15)32
91.42
94.34
97.4
95.83
32.1
95.28

(cid:15)32
98.56
88.00
96.49
88.28
40.75
92.58

(cid:15)32
90.59
81.68
89.06
77.86
22.77
80.79

(cid:15)32
92.7
85.06
89.09
72.59
19.07
77.45

(cid:15)32
91.53
93.46
98.06
99.12
63.07
98.76

(cid:15)32
97.37
47.37
94.74
100
21.05
94.74

ResNet101
(cid:15)4
(cid:15)2
91.42
91.42
95.99
95.99
98.51
98.51
99.21
99.21
98.66
98.66
94.34
94.34

(cid:15)2
98.41
92.00
97.45
97.69
97.69
91.15

(cid:15)2
90.59
84.10
90.71
94.78
93.64
76.08

(cid:15)2
92.88
87.44
95.97
97.98
96.70
89.00

(cid:15)2
91.70
94.35
99.12
99.82
100
98.94

(cid:15)2
97.37
50.00
100
100
97.37
94.74

(cid:15)4
98.41
92.00
97.45
97.69
97.69
91.15

(cid:15)4
90.59
84.10
90.71
94.78
93.64
76.08

(cid:15)4
92.88
87.44
95.97
97.98
96.70
89.00

(cid:15)4
91.70
94.35
99.12
99.82
100
98.94

(cid:15)4
97.37
50.00
100
100
97.37
94.74

(cid:15)8
91.42
96.07
98.51
98.98
98.43
67.98

(cid:15)8
98.56
91.84
97.21
96.81
97.13
55.42

(cid:15)8
90.59
84.35
91.09
93.77
92.37
47.96

(cid:15)8
92.79
85.98
96.06
96.98
95.69
60.49

(cid:15)8
91.87
94.88
99.12
99.82
100
94.70

(cid:15)8
97.37
47.37
100
100
97.37
81.58

(cid:15)16
91.42
95.67
98.03
98.35
97.17
50.04

(cid:15)16
98.8
89.76
97.05
95.14
95.77
43.94

(cid:15)16
90.59
82.95
91.09
90.59
88.17
41.48

(cid:15)16
92.88
85.24
94.68
93.31
92.58
44.18

(cid:15)16
91.87
94.35
98.94
99.82
100
89.75

(cid:15)16
97.37
47.37
94.74
100
94.74
65.79

(cid:15)32
91.42
94.73
97.48
97.32
95.75
51.3

(cid:15)32
97.69
87.60
96.73
88.12
92.9
49.68

(cid:15)32
90.59
80.79
90.08
83.46
80.28
38.55

(cid:15)32
92.7
85.15
90.10
78.00
79.65
37.86

(cid:15)32
92.05
93.46
98.41
99.29
98.41
88.16

(cid:15)32
97.37
50.00
92.11
100
78.95
68.42

Table 7: Results obtained using training and validation datasets from Kaggle.
Each method presents its classiﬁcation accuracy for training, validation and
the AEs using the FGSM computed at (cid:15) = 2, 4, 8, 16, 32.

34

SIFT+FV
Iconography
Painting
Drawings
Sculpture
Engraving Bw
Engraving Color

optimizer
val
train
95.28
100
92.72
99.76
83.84
100
86.71
100
93.64
100
50.00
100

cross-validation
mean train mean val
99.28
98.84
98.28
98.63
99.32
92.00

95.28
92.83
83.44
86.48
93.87
47.11

Table 8: Results of using the SVM hyperparameters optimizer method from
Matlab and the crossvalidation function to verify overﬁtting on SIFT+FV.

Figure 5: Maps generated in each phase of the AVC, extracted from the original image
and the AE computed from FGSM using ResNet101 with (cid:15) = 32. Note that despite the
attack, the generated maps do not change much.

35

Figure 6: Maps generated in each phase of the AVC, extracted from an AE of the multiple
pixel attack and the image with the adversarial patch. Note that despite the attack, the
generated maps do not change much with the original one.

showed the poor performance of SIFT+FV considering clean images. In four
out of seven classes (Painting Landscapes, Drawings, Sculpture, and Engrav-
ing color), the accuracy is way below to compete with DCNNs. Additionally,
SIFT+FV was aﬀected by AEs in Iconography, Painting Landscapes, and
Sculpture, where approximately 10% of its original score reduced the perfor-
mance. Finally, BP demonstrated high quality and steady results keeping
its scores from clean images after AEs with minimal to zero changes for all
classes.

Additionally, it is noticeable that, as opposed to SIFT+FV, BP reaches
comparable results to DCNNs’ scores. Besides, we present in Figures 8-9 the
ratio of accuracy on AEs for the testing classes. We observed a very similar
behavior, at least for BP, whose rate for all experiments remains almost one.
We see a drastic drop in DCNN models’ performance when the perturbation
matches the network’s architecture and inﬂuences AEs’ transferability to
other DCNN models and SIFT+FV.

36

Figure 7: Comparative graph of the computed accuracy ratios between adversarial exam-
ples and clean images from each method using the validation dataset.

37

Figure 8: Comparative graph of the computed accuracy ratios between adversarial exam-
ples and clean images from each method using Iconography, Painting, Painting Landscapes,
Drawings, and Sculpture classes from the testing dataset.

38

ResNet18
(cid:15)4
(cid:15)2
91.66
91.66
85.95
85.95
96.07
96.07
95.66
95.66
76.86
76.86
95.45
95.45

test
BP
91.74
SIFT+FV 86.16
96.07
AlexNet
95.87
VGG
96.49
ResNet18
95.25
Resnet101

test
100

BP
SIFT+FV 94.83
94.06
AlexNet
93.37
VGG
94.23
ResNet18
95.91
Resnet101

test
100

BP
SIFT+FV 75.34
93.77
AlexNet
94.58
VGG
95.12
ResNet18
95.39
Resnet101

test
BP
94.05
SIFT+FV 73.61
86.73
AlexNet
91.99
VGG
90.85
ResNet18
93.59
Resnet101

test
BP
90.54
SIFT+FV 60.47
91.45
AlexNet
94.69
VGG
92.63
ResNet18
92.92
Resnet101

test
BP
91.55
SIFT+FV 89.79
98.58
AlexNet
99.58
VGG
99.83
ResNet18
99.67
Resnet101

test
BP
89.92
SIFT+FV 66.95
94.72
AlexNet
99.40
VGG
96.40
ResNet18
99.88
Resnet101

AlexNet
(cid:15)2
91.66
85.54
93.39
95.45
95.87
95.25

(cid:15)4
91.66
85.54
93.39
95.45
95.87
95.25

(cid:15)2
95.65
94.83
90.57
93.28
94.19
95.82

(cid:15)2
100
75.07
86.99
94.58
94.85
95.12

(cid:15)2
94.28
68.42
77.8
91.99
90.85
93.36

(cid:15)2
90.83
52.80
87.61
94.99
91.74
93.22

(cid:15)2
92.64
89.79
94.06
99.83
99.92
99.75

(cid:15)2
89.68
66.77
73.55
99.46
95.98
99.76

(cid:15)4
95.65
94.83
90.57
93.28
94.19
95.82

(cid:15)4
100
75.07
86.99
94.58
94.85
95.12

(cid:15)4
94.28
68.42
77.8
91.99
90.85
93.36

(cid:15)4
90.83
52.80
87.61
94.99
91.74
93.22

(cid:15)4
92.64
89.79
94.06
99.83
99.92
99.75

(cid:15)4
89.68
66.77
73.55
99.46
95.98
99.76

(cid:15)8
91.82
84.71
70.04
94.83
94.83
94.83

(cid:15)8
95.65
94.70
64.64
92.64
94.40
95.78

(cid:15)8
100
72.90
61.25
94.31
93.77
95.12

(cid:15)8
93.59
66.36
57.21
90.89
89.7
93.14

(cid:15)8
90.83
52.80
65.49
94.99
90.86
92.63

(cid:15)8
91.97
89.71
75.06
99.67
99.83
99.75

(cid:15)8
89.74
66.59
25.49
99.46
96.16
99.76

(cid:15)16
91.74
83.26
37.4
91.32
94.21
92.77

(cid:15)16
95.65
94.57
41.04
87.47
94.40
94.62

(cid:15)16
100
70.46
41.46
90.79
92.95
93.77

(cid:15)16
93.81
62.7
41.19
88.79
88.79
91.53

(cid:15)16
90.83
53.10
44.25
92.33
89.38
90.86

(cid:15)16
91.72
89.87
57.32
99.50
99.67
99.83

(cid:15)16
89.86
66.89
12.30
99.28
95.02
99.52

(cid:15)32
91.74
77.69
28.72
80.99
87.81
89.88

(cid:15)32
95.65
93.63
41.00
60.12
92.64
90.09

(cid:15)32
100
62.6
35.77
73.71
90.79
89.43

(cid:15)32
94.5
56.52
32.72
80.78
81.46
85.81

(cid:15)32
90.83
51.62
36.87
84.37
83.19
87.61

(cid:15)32
91.63
90.96
54.64
99.16
99.16
99.75

(cid:15)32
89.80
68.09
17.22
96.52
89.14
98.92

VGG
(cid:15)2
91.66
85.54
95.87
76.65
95.66
95.45

(cid:15)2
95.65
94.92
94.10
61.15
94.01
95.82

(cid:15)2
100
75.61
93.50
80.76
94.31
94.58

(cid:15)2
93.82
68.42
85.81
72.77
90.39
93.14

(cid:15)2
85.96
53.39
91.15
79.06
91.74
92.92

(cid:15)2
92.30
89.79
98.66
91.05
99.75
99.83

(cid:15)2
89.92
66.83
94.78
79.90
95.92
99.70

(cid:15)4
91.66
85.54
95.87
76.65
95.66
95.45

(cid:15)4
95.65
94.92
94.10
61.15
94.01
95.82

(cid:15)4
100
75.61
93.50
80.76
94.31
94.58

(cid:15)4
93.82
68.42
85.81
72.77
90.39
93.14

(cid:15)4
85.96
53.39
91.15
79.06
91.74
92.92

(cid:15)4
92.30
89.79
98.66
91.05
99.75
99.83

(cid:15)4
89.92
66.83
94.78
79.90
95.92
99.70

Iconography

(cid:15)8
91.74
84.92
95.04
36.98
94.42
94.63

(cid:15)32
91.74
77.48
92.98
21.69
83.88
88.02

(cid:15)16
91.74
83.47
94.42
23.97
90.5
91.94
Painting
(cid:15)16
95.65
94.57
94.01
10.42
91.30
90.44

(cid:15)32
95.65
93.63
94.92
10.68
81.91
79.03

(cid:15)8
95.65
94.88
93.90
13.14
93.63
95.69
Painting Landscapes
(cid:15)8
100
73.71
92.68
42.01
92.41
94.58

(cid:15)32
100
62.60
90.24
30.89
81.84
83.74

(cid:15)16
100
70.46
91.06
28.73
89.43
93.50

(cid:15)2
95.65
94.88
94.10
92.89
64.86
95.61

(cid:15)2
100
75.34
93.50
94.31
72.36
94.31

(cid:15)8
94.05
67.51
86.27
35.7
87.41
90.62

Drawings

(cid:15)16
93.59
62.7
84.67
20.59
81.69
85.58

(cid:15)32
94.73
57.89
79.18
18.99
74.37
76.43

Sculpture

(cid:15)8
85.96
53.10
90.56
45.43
87.91
92.92

(cid:15)16
85.96
52.51
89.38
32.74
84.96
89.97

(cid:15)32
85.96
52.51
87.32
34.51
80.24
86.14
Engraving BW

(cid:15)8
92.05
89.79
98.66
62.85
99.41
99.50

(cid:15)16
92.05
89.37
98.49
45.94
98.83
99.25

(cid:15)32
91.63
90.96
97.32
49.87
97.49
98.74
Engraving Color

(cid:15)8
89.74
66.59
94.90
16.02
95.50
99.70

(cid:15)16
89.86
67.19
94.48
05.46
93.88
99.22

(cid:15)32
89.62
68.09
93.64
06.06
89.50
98.44

(cid:15)2
93.81
68.42
85.81
91.3
71.85
93.14

(cid:15)2
90.83
52.80
91.45
95.28
75.81
93.22

(cid:15)2
91.97
89.87
98.66
99.58
93.22
99.67

(cid:15)2
89.68
66.89
94.72
99.52
49.13
99.82

(cid:15)8
91.66
84.71
95.87
94.21
38.64
92.56

(cid:15)8
95.65
94.79
94.06
91.17
15.25
94.66

(cid:15)8
100
73.17
92.68
94.31
42.82
92.95

(cid:15)8
93.59
66.82
85.35
89.02
36.16
90.16

(cid:15)8
90.83
52.51
91.45
94.10
46.61
91.15

(cid:15)8
91.80
89.62
98.66
98.74
71.55
99.50

(cid:15)8
89.74
66.95
94.66
99.22
06.84
99.76

(cid:15)16
91.58
83.06
94.83
87.81
25.21
87.6

(cid:15)16
95.65
94.44
94.32
80.10
13.01
88.33

(cid:15)16
100
68.83
91.60
88.08
33.60
88.89

(cid:15)16
93.59
62.01
83.75
83.3
24.49
83.07

(cid:15)16
90.83
52.51
89.09
88.20
34.81
88.20

(cid:15)16
91.97
89.54
98.33
98.41
59.41
99.08

(cid:15)16
89.98
68.33
95.14
99.10
05.58
99.40

(cid:15)32
91.5
76.24
93.18
76.86
21.49
83.26

(cid:15)32
95.65
93.28
95.35
47.55
15.07
73.47

(cid:15)32
100
60.70
90.51
72.63
38.48
78.86

(cid:15)32
94.05
56.75
80.09
73.91
24.49
75.29

(cid:15)32
90.83
51.33
89.38
82.01
33.92
83.48

(cid:15)32
91.80
90.88
97.15
97.91
61.09
98.16

(cid:15)32
89.80
66.71
94.24
97.18
10.74
98.56

ResNet101
(cid:15)4
(cid:15)2
91.58
91.58
86.16
86.16
96.07
96.07
95.87
95.87
96.07
96.07
79.96
79.96

(cid:15)2
95.65
94.88
94.10
92.59
93.80
75.24

(cid:15)2
100
75.34
93.77
94.04
94.31
80.76

(cid:15)2
93.81
68.19
85.81
91.53
90.16
72.27

(cid:15)2
90.83
53.39
91.45
94.69
91.15
80.53

(cid:15)2
92.13
89.96
98.66
99.58
99.83
95.90

(cid:15)2
89.92
66.71
94.54
99.40
95.62
92.86

(cid:15)4
95.65
94.88
94.10
92.59
93.80
75.24

(cid:15)4
100
75.34
93.77
94.04
94.31
80.76

(cid:15)4
93.81
68.19
85.81
91.53
90.16
72.27

(cid:15)4
90.83
53.39
91.45
94.69
91.15
80.53

(cid:15)4
92.13
89.96
98.66
99.58
99.83
95.90

(cid:15)4
89.92
66.53
94.54
99.40
95.62
92.86

(cid:15)8
91.58
84.71
95.45
95.87
94.21
49.38

(cid:15)8
95.65
94.70
94.06
90.78
92.72
30.62

(cid:15)8
100
72.90
92.68
93.22
91.6
50.96

(cid:15)8
93.59
67.28
85.58
89.7
86.96
45.54

(cid:15)8
90.83
52.51
91.45
93.81
89.38
56.64

(cid:15)8
91.97
89.46
98.66
99.25
99.67
90.13

(cid:15)8
89.86
66.53
95.02
99.10
95.02
61.91

(cid:15)16
91.58
83.06
94.63
90.91
90.29
36.16

(cid:15)16
95.65
94.32
94.06
81.05
89.84
19.04

(cid:15)16
100
68.29
91.33
87.53
88.62
43.90

(cid:15)16
93.59
62.01
84.67
83.98
81.92
35.24

(cid:15)16
90.83
52.51
90.27
91.74
86.14
50.44

(cid:15)16
91.80
89.54
98.58
99.00
98.91
85.77

(cid:15)16
89.50
66.95
94.66
98.50
92.68
49.19

(cid:15)32
91.58
75.62
92.15
82.44
85.12
36.36

(cid:15)32
95.65
93.20
95.00
44.96
80.19
19.98

(cid:15)32
100
59.62
90.51
70.19
79.95
44.72

(cid:15)32
93.81
56.75
81.69
76.89
74.83
33.41

(cid:15)32
90.83
50.74
88.20
86.73
83.19
56.34

(cid:15)32
91.63
90.96
97.49
98.83
97.82
83.01

(cid:15)32
90.16
66.95
94.00
95.98
86.98
54.53

(cid:15)4
95.65
94.88
94.10
92.89
64.86
95.61

(cid:15)4
100
75.34
93.50
94.31
72.36
94.31

(cid:15)4
93.81
68.42
85.81
91.3
71.85
93.14

(cid:15)4
90.83
52.80
91.45
95.28
75.81
93.22

(cid:15)4
91.97
89.87
98.66
99.58
93.22
99.67

(cid:15)4
89.68
66.65
94.72
99.52
49.13
99.82

Table 9: Results obtained using the test dataset from Wikiart. Each method
presents its classiﬁcation accuracy for testing and the AEs using the FGSM
computed at (cid:15) = 2, 4, 8, 16, 32.

39

Figure 9: Comparative graph of the computed accuracy ratios between adversarial exam-
ples and clean images from each method using Engraving BW and Engraving Color classes
from the testing dataset.

4.4.2. Multiple Pixel Attack

The multiple pixel attack experiment came along with the analysis that
one pixel does not perturb high-resolution images to change the model’s
prediction. We experiment with modifying one pixel to fool the models over
100 selected images, and the results indicate no score changes. Thus, we
experimentally found that when 8000-10,000 pixels, DCNN models have a
massive amount of change in their prediction, so we set a second experiment
with a 10,000 pixel attack. We present in Table 10 the number of images that
change its forecast with the success rate and the mean posterior probability
of these new predictions in the conﬁdence row.

We observed that DCNN changes by a considerable amount of their pre-
dictions with high conﬁdence by modifying multiple pixels. SIFT+FV was
also misled in ﬁve out of seven classes achieving the same number of images
as DCNN models with lower conﬁdence.
In this way, only two categories
resisted the attack. On the contrary, BP was robust to this attack having
four out of seven classes without changes and the rest with a maximum error
of 4%. Notice that the amount of pixels modiﬁed in this experiment fails
the motivation of AA in which the perturbation should be unnoticeable to
human vision. Therefore, BP was robust to this perturbation. We illustrate
as an example; BP generated maps using a multiple pixel attack in Figure 6.
Additionally, we report the mean processing time in seconds (see Table 10),
which makes this attack unfeasible to perform in real-time applications.

40

BP
92.00
0.00
NA
94.22
BP
100
2.00
51.83
90.16

Iconography
Original Acc.
Success rate
Conﬁdence
Time (seconds)
Painting
Original Acc.
Success rate
Conﬁdence
Time (seconds)
Painting Landscapes BP
100
Original Acc.
2.00
Success rate
54.06
Conﬁdence
98.83
Time (seconds)
BP
Drawings
88.00
Original Acc.
0.00
Success rate
NA
Conﬁdence
118.85
Time (seconds)
BP
Sculpture
86.00
Original Acc.
4.00
Success rate
58.14
Conﬁdence
71.20
Time (seconds)
BP
Engraving BW
94.00
Original Acc.
0.00
Success rate
NA
Conﬁdence
88.71
Time (seconds)
BP
Engraving Color
94.00
Original Acc.
0.00
Success rate
NA
Conﬁdence
87.01
Time (seconds)

92.00
42.00
77.61
237.73

96.00
46.00
76.34
152.37

96.00
32.00
85.09
138.51

92.00
64.00
99.26
143.62

92.00
64.00
99.37
111.14

92.00
66.00
97.37
205.53

94.00
64.00
98.06
242.58

88.00
54.00
75.70
141.85

94.00
54.00
78.11
119.78

SIFT+FV AlexNet VGG ResNet18 ResNet101
94.00
88.00
44.00
32.00
85.72
64.96
301.21
147.72
SIFT+FV AlexNet VGG ResNet18 ResNet101
90.00
78.00
60.00
0.00
97.34
NA
598.12
122.59
SIFT+FV AlexNet VGG ResNet18 ResNet101
88.00
78.00
60.00
40.00
97.25
62.04
585.69
163.51
SIFT+FV AlexNet VGG ResNet18 ResNet101
90.00
70.00
68.00
38.00
91.94
66.53
111.48
462.92
SIFT+FV AlexNet VGG ResNet18 ResNet101
98.00
62.00
54.00
60.00
98.60
67.61
601.53
130.06
SIFT+FV AlexNet VGG ResNet18 ResNet101
100
94.00
50.00
0.00
68.07
NA
599.41
169.56
SIFT+FV AlexNet VGG ResNet18 ResNet101
74.00
60.00
55.98
600.82

100
32.00
71.86
152.11

80.00
68.00
83.91
110.18

100
20.00
61.25
177.61

100
40.00
77.63
148.90

86.00
74.00
95.11
128.07

92.00
78.00
94.24
220.69

96.00
56.00
97.45
137.16

96.00
54.00
96.93
181.14

88.00
62.00
92.65
121.22

100
22.00
65.31
186.13

92.00
46.00
62.96
154.52

100
50.00
66.15
174.51

98.00
40.00
73.80
150.70

Table 10: Results from the experiment of computing the multiple pixel attack
with d = 10, 000 on 100 random images from the testing dataset. The original
accuracy refers to the score of the clean images. Success rate means the
percentage of images that change the prediction with a mean Conﬁdence
value of the posterior probabilities over the new predicted classes.

41

4.4.3. Adversarial Patch

We present the results of the adversarial patch in Table 11. This exper-
iment analyzes the change in the model’s predictions by adding the trained
patches from DCNN models using 100 images from each class in a random
location and orientation. The results from Table 11 show that these patches
aﬀect in a signiﬁcant manner DCNN models in most experiments. Also, we
discovered that the patches could be transferable to other DCNNs.

The painting landscapes experiment showed the worst-case scenario for
DCNN models, on which we observed a considerable transferability eﬀect be-
tween the models. We observed that VGG, ResNet18, and ResNet101 were
aﬀected by all the patches. DCNN models dropped its performance to ap-
proximately half of its original accuracy and, in some cases, is less to 50%.
ResNet18 was fooled in all images using its trained patch. All other classes
did not show a similar behavior; the patches can fool DCNN models. In con-
trast, SIFT+FV and BP demonstrated a robust control over the adversarial
patches, showing almost an unchangeable performance. Figure 6 illustrates
the BP generated maps using an image with the adversarial patch.

4.4.4. Statistical Analysis of Robustness

In the last section, we see that diﬀerences among experiments seem strik-
ing, particularly when images suﬀer a subtle perturbation. Nevertheless,
statistical analysis allows us to be more conﬁdent regarding the robustness
of each method’s predictions. Nowadays, the nonparametric statistical anal-
ysis is bringing researchers’ attention to measure the performance through a
rigorous comparison among algorithms, considering independence, normality,
and homoscedasticity [79, 80]. Such procedures perform both pairwise and
multiple comparisons for multiple-problem analysis. In our case, we apply
pairwise statistical procedures to perform individual comparisons between
each method’s predictions’ conﬁdence from clean and attacked images based
on the statistical procedure described in [81].

When the designed algorithms’ results for the same problem achieved the
conditions expressed before, the most common test is the ANOVA. In case
that the distributions are not normal, we must use a nonparametric test
like Kruskal-Wallis. If the distributions are normal but do not achieve the
property of homoscedasticity, the analysis required is the Welch test. The
statistical tests enable comparisons of the sample distributions, attending
to the required conditions, and applying a suitable assessment a posteriori
to contrast the results. As a result, we have ﬁrst studied data normality

42

99.00
93.00
97.00
82.00
58.00
78.00

99.00
93.00
97.00
45.00
90.00
87.00

99.00
92.00
98.00
81.00
90.00
70.00

99.00
89.00
74.00
91.00
87.00
87.00

99.00
98.00
94.00
73.00
23.00
69.00

100.00
78.00
85.00
19.00
39.00
41.00

100.00
84.00
86.00
48.00
0.00
35.00

100.00
81.00
77.00
23.00
9.00
22.00

100.00
81.00
24.00
41.00
22.00
43.00

100.00
97.00
94.00
48.00
76.00
72.00

100.00
96.00
94.00
61.00
43.00
56.00

100.00
98.00
54.00
71.00
67.00
72.00

Original Acc. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
99.00
92.00
98.00
94.00
94.00
93.00
Original Acc. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
100.00
97.00
96.00
92.00
94.00
97.00
Original Acc. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
100.00
87.00
94.00
95.00
95.00
96.00
Original Acc. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
91.00
72.00
94.00
98.00
96.00
99.00
Original Acc. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
85.00
95.00
97.00
97.00
95.00
94.00

Iconography
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101
Painting
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101
Painting Land.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101
Drawings
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101
Sculpture
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101
Engraving BW Original Acc. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101
Engraving Color Original Acc. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

93.00
94.00
97.00
100.00
98.00
100.00

90.00
91.00
100.00
100.00
100.00
100.00

93.00
96.00
98.00
99.00
100.00
100.00

91.00
93.00
100.00
83.00
96.00
100.00

91.00
95.00
100.00
96.00
71.00
100.00

92.00
94.00
67.00
100.00
99.00
100.00

90.00
94.00
99.00
99.00
100.00
100.00

92.00
95.00
95.00
100.00
98.00
100.00

92.00
95.00
93.00
100.00
99.00
100.00

91.00
92.00
100.00
97.00
96.00
100.00

91.00
67.00
30.00
81.00
82.00
88.00

91.00
68.00
85.00
69.00
91.00
90.00

85.00
92.00
32.00
93.00
92.00
87.00

85.00
94.00
89.00
85.00
66.00
86.00

85.00
94.00
92.00
72.00
86.00
89.00

85.00
95.00
86.00
85.00
89.00
87.00

91.00
67.00
73.00
62.00
79.00
75.00

91.00
69.00
80.00
74.00
66.00
85.00

Table 11: Results obtained using the adversarial patch. Each column presents
the score obtained for the original 100 images per class and the AEs when
adding the adversarial patch.

43

(Lilliefors, Kolmogorov-Smirnov) and homoscedasticity (Levene test); then,
according to the results, we have applied the appropriate statistical test
(Kruskal-Wallis, Welch, Anova) to determine if the diﬀerences are signiﬁ-
cant, using a p-value < 0.05. Therefore, if the predictions’ conﬁdence is
statistically diﬀerent, it will illustrate the rejection of the null hypothesis
Ho. If the statistical analysis accepts Ho, it will deﬁne that the predictions’
conﬁdence from the pair of clean and perturbed images is not signiﬁcantly
diﬀerent; hence we can conclude that the method is robust to the AEs.

The statistical analysis from Tables 12-13 shows that the predictions’
conﬁdence from BP is not signiﬁcantly diﬀerent in every experiment of the
test dataset using FGSM. That means that the conﬁdence is not aﬀected
by the subtle perturbations added to the images. The majority of p-values
from SIFT+FV demonstrate to be not signiﬁcantly diﬀerent between the
predictions’ conﬁdences. Nonetheless, the analysis from all DCNN architec-
tures showed that, in most cases, the rejection of the null hypothesis Ho.
The rejection illustrates the damage of the AEs to the DCNN’s predictions’
conﬁdence by making them statistically diﬀerent.

44

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.99743
0.95119
1.7665e-12
0.58235
0.63062
0.6259

(cid:15)4
0.99889
0.95118
1.7665e-12
0.58245
0.63063
0.62599

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.9136
0.21866
3.4962e-105
0.64622
0.9711
0.90558

(cid:15)4
0.89874
0.21866
3.4962e-105
0.64621
0.97111
0.90557

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
1
0.79505
7.9291e-10
0.87445
0.89967
0.9671

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.98405
0.67854
1.5308e-05
0.87066
0.90781
0.97912

(cid:15)4
1
0.79505
7.9291e-10
0.87447
0.89966
0.96696

(cid:15)4
0.98405
0.67854
1.5308e-05
0.87066
0.90781
0.97912

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.99772
0.89198
0.00013513
0.75594
0.76905
0.83153

(cid:15)4
0.99772
0.89198
0.00013513
0.75594
0.76905
0.83153

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.1314
0.89606
5.358e-33
0.94146
0.35122
0.14262

(cid:15)4
0.1314
0.89606
5.358e-33
0.94146
0.35122
0.14262

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.89591
0.83715
1.6503e-133
3.66e-32
1.105e-08
5.1941e-16

(cid:15)4
0.89591
0.83715
1.6479e-133
3.66e-32
1.105e-08
5.1941e-16

AlexNet

(cid:15)8
0.99599
0.80859
1.8922e-54
0.0087004
0.024056
0.054054

AlexNet

(cid:15)8
0.89634
0.00012449
0
9.6649e-14
0.92035
0.37347

AlexNet

(cid:15)8
1
0.30022
1.4342e-29
0.38584
0.7306
0.74545

AlexNet

(cid:15)8
0.97876
0.43269
3.0146e-26
0.058863
0.50143
0.55583

AlexNet

(cid:15)8
0.99742
0.4632
2.0152e-20
0.0082156
0.053247
0.018017

AlexNet

(cid:15)8
0.75089
0.9213
2.205e-116
0.62982
0.020752
0.70567

AlexNet

(cid:15)8
0.94823
0.7306
4.924e-273
1.8892e-35
1.8668e-09
1.1333e-18

(cid:15)16
0.9958
0.21374
5.2622e-73
1.884e-09
8.0612e-06
5.9926e-05

Iconography

(cid:15)32
0.9958
0.00012958
1.5556e-74
3.1824e-32
3.3087e-17
2.5752e-11
Painting

(cid:15)2
0.99828
0.93022
0.99483
3.2201e-30
0.52182
0.54501

(cid:15)4
0.99634
0.93022
0.99483
3.2201e-30
0.52182
0.54501

(cid:15)16
0.85943
1.6086e-24
0
2.4111e-137
0.13873
3.1411e-66

(cid:15)32
0.12557
1.65e-86
0
0
3.8914e-111
4.1528e-240
Painting Landscapes

(cid:15)2
0.99068
0.16502
0.98106
0
0.37065
0.35338

(cid:15)4
0.99108
0.16502
0.98106
0
0.37065
0.35338

(cid:15)16
1
0.0024393
5.265e-33
3.5473e-12
0.30211
5.8498e-08

(cid:15)16
0.95426
0.04983
5.9791e-41
9.9068e-08
0.0023366
3.2526e-06

(cid:15)32
1
3.3167e-09
8.3707e-33
7.3903e-34
1.4175e-05
2.5216e-22
Drawings

(cid:15)32
0.97483
2.6184e-06
6.2069e-47
3.7992e-30
5.16e-16
1.313e-22
Sculpture

(cid:15)2
1
0.79599
0.9905
5.0409e-29
0.68713
0.87901

(cid:15)4
1
0.79599
0.9905
5.0409e-29
0.68713
0.87901

(cid:15)2
0.98745
0.69315
0.92991
1.5939e-20
0.59103
0.79099

(cid:15)4
0.98745
0.69315
0.92989
1.5939e-20
0.59103
0.79099

(cid:15)16
0.99748
0.1714
8.8035e-31
5.2489e-08
2.3723e-06
6.1378e-07

(cid:15)32
0.99701
0.086323
8.2946e-34
1.6714e-21
7.1788e-18
2.1775e-14
Engraving BW

(cid:15)2
0.99736
0.93114
0.89484
6.7774e-13
0.6501
0.73046

(cid:15)4
0.99736
0.93114
0.89484
6.7774e-13
0.6501
0.73046

(cid:15)16
0.7213
0.77775
3.5771e-171
0.019374
6.4226e-13
0.0010867

(cid:15)32
0.76281
0.01329
2.9344e-187
0.092515
4.1326e-56
5.2621e-31
Engraving Color

(cid:15)2
0.1184
0.87447
0.61108
1.7458e-85
0.11378
0.4537

(cid:15)4
0.1184
0.87447
0.61108
1.7458e-85
0.11378
0.4537

(cid:15)16
0.93453
0.52109
2.106e-290
1.7963e-34
4.1096e-08
6.1089e-25

(cid:15)32
0.98186
0.95255
1.6302e-274
1.0132e-18
6.1218e-06
5.416e-42

(cid:15)2
0.87265
0.78356
0.56852
0
1.6772e-10
8.2181e-23

(cid:15)4
0.87265
0.78356
0.56852
0
1.6772e-10
8.2181e-23

VGG

(cid:15)8
0.99517
0.73482
0.99702
3.1832e-64
0.00035193
0.0017698

VGG

(cid:15)8
0.78341
3.9752e-06
0.90152
0
2.124e-25
3.3466e-53

VGG

(cid:15)8
1
0.3353
0.95413
5.5977e-37
0.030198
0.33332

VGG

(cid:15)8
0.99322
0.46697
0.83881
1.7229e-53
0.0056782
0.00070622

VGG

(cid:15)8
0.99717
0.5201
0.54423
1.0801e-30
0.0032158
0.0025386

VGG

(cid:15)8
0.69932
0.85628
0.79384
2.3367e-202
2.9188e-05
0.19041

VGG

(cid:15)8
0.92126
0.78447
0.36771
0
1.7619e-14
1.968e-41

(cid:15)16
0.9937
0.10472
0.84401
1.3611e-174
3.2009e-10
1.0084e-06

(cid:15)32
0.99371
9.0663e-06
0.83964
7.6365e-176
2.4186e-22
2.232e-12

(cid:15)16
0.75145
1.516e-31
0.47591
0
1.5714e-103
1.3622e-216

(cid:15)32
0.59411
3.6495e-98
0.050199
0
1.1938e-291
0

(cid:15)16
1
0.0019852
0.95212
3.0354e-37
1.4375e-05
2.368e-17

(cid:15)16
0.98889
0.064349
0.84904
3.4676e-58
6.2775e-08
4.8929e-12

(cid:15)32
1
3.6761e-09
0.70978
1.0586e-38
7.5887e-15
3.0898e-30

(cid:15)32
0.97186
1.0753e-05
0.0052409
6.0516e-61
9.5421e-26
1.5062e-33

(cid:15)16
0.99702
0.20871
0.00026161
7.3611e-34
1.3364e-08
2.3318e-07

(cid:15)32
0.99753
0.14017
1.6579e-12
2.2997e-36
4.5108e-21
3.531e-15

(cid:15)16
0.72718
0.96241
0.66623
6.0154e-219
5.2976e-22
4.4468e-06

(cid:15)32
0.75278
0.086893
1.2529e-07
3.4579e-219
1.1364e-73
1.2934e-39

(cid:15)16
0.93354
1
0.041684
0
1.0486e-16
3.066e-70

(cid:15)32
0.99179
0.82826
1.0253e-05
0
1.5263e-15
3.3115e-107

Table 12: Results from the statistical tests applied to each method’s pre-
dictions’ conﬁdence from clean and attacked images using test datasets and
AEs from AlexNet and VGG. Each value represents the corresponding p-
value from the statistical test.

45

Iconography

(cid:15)16
0.99585
0.098796
0.74961
7.6022e-17
1.0059e-161
8.3498e-14

(cid:15)32
0.99585
1.296e-05
0.18328
2.9553e-39
1.2666e-66
2.6481e-22
Painting

(cid:15)2
0.99992
0.9245
0.9903
0.60766
0.48618
5.2645e-31

(cid:15)16
0.99293
9.2235e-34
0.0002221
1.8098e-237
0
1.041e-267

(cid:15)32
0.9937
4.2265e-102
0.056476
0
0
0

(cid:15)2
0.73759
0.1448
0.98499
8.3796e-06
9.7705e-05
0

Painting Landscapes

(cid:15)32
1
9.4406e-10
0.66599
4.5878e-38
2.2238e-35
5.671e-33

Drawings

(cid:15)32
0.97252
4.3891e-06
0.0013548
1.8494e-35
1.2956e-56
1.5527e-34

Sculpture

(cid:15)2
1
0.79544
0.99458
0.73709
0.60616
8.0509e-29

(cid:15)2
0.99539
0.68791
0.9221
0.5806
0.50031
4.7426e-30

(cid:15)32
0.9969
0.23847
2.4554e-12
1.6644e-23
2.1562e-32
1.5158e-17
Engraving BW

(cid:15)2
0.99659
0.91303
0.92807
0.83568
0.8016
7.4724e-11

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.99767
0.95457
0.99776
0.45019
2.8465e-35
0.36979

(cid:15)4
0.99672
0.95457
0.99776
0.45018
2.8465e-35
0.36979

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.84987
0.15684
0.97537
1.5516e-06
0
0.20263

(cid:15)4
0.78268
0.15685
0.97538
1.5524e-06
0
0.20266

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
1
0.81073
0.97698
0.71643
1.3222e-24
0.83154

(cid:15)4
1
0.81072
0.97699
0.71643
1.3435e-24
0.83154

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.98646
0.69545
0.91835
0.61463
1.7101e-24
0.59366

(cid:15)4
0.98646
0.69544
0.91835
0.61463
1.7101e-24
0.59366

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.9975
0.91246
0.89915
0.71495
5.4653e-13
0.63809

(cid:15)4
0.9975
0.91245
0.89911
0.71495
5.4653e-13
0.63809

Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101
Testing Vs.
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

(cid:15)2
0.62139
0.88575
0.88255
0.3549
2.0619e-81
0.5792
(cid:15)2
0.96553
0.89023
0.61061
8.8075e-36
3.076e-307
4.2e-20

(cid:15)4
0.62139
0.88575
0.88255
0.3549
2.0619e-81
0.5792
(cid:15)4
0.96553
0.89023
0.61064
8.8007e-36
3.076e-307
4.1976e-20

ResNet

(cid:15)8
0.99739
0.78501
0.96299
1.3824e-05
1.2532e-66
1.161e-05

ResNet

(cid:15)8
0.54929
1.1729e-06
0.83884
1.4289e-58
0
7.6609e-70

ResNet

(cid:15)8
1
0.31041
0.93679
4.7994e-05
1.8235e-34
1.4205e-06

ResNet

(cid:15)8
0.98853
0.45485
0.86539
0.00064209
1.9766e-51
0.0001009

ResNet

(cid:15)8
0.99702
0.45842
0.57069
0.00025963
1.885e-26
0.00097214

ResNet

(cid:15)8
0.76279
0.8536
0.82907
0.011117
8.5802e-191
0.0092807
(cid:15)8
0.93342
0.67872
0.4212
1.3014e-44
0
4.9361e-32

(cid:15)16
1
0.0013292
0.92671
1.6994e-17
5.3646e-34
5.6908e-22

(cid:15)16
0.96403
0.052022
0.77007
2.228e-13
1.4251e-54
4.6116e-12

(cid:15)16
0.99693
0.19418
0.00029538
4.4615e-10
2.7286e-29
2.3315e-09

(cid:15)16
0.72754
0.95828
0.12103
1.113e-11
2.2357e-211
5.08e-14
(cid:15)16
0.99733
0.66967
0.095421
7.2256e-54
0
4.2834e-52

(cid:15)4
0.99593
0.92449
0.9903
0.60768
0.48626
5.2645e-31

(cid:15)4
0.94754
0.1448
0.98501
8.3735e-06
9.7674e-05
0

(cid:15)4
1
0.79544
0.9946
0.73709
0.60616
8.0509e-29

(cid:15)4
0.99539
0.68791
0.92208
0.58052
0.50028
4.7426e-30

(cid:15)4
0.99659
0.91303
0.92807
0.83568
0.8016
7.4724e-11

ResNet101
(cid:15)8
0.99825
0.71244
0.98065
0.0020685
0.0005933
1.3736e-58

ResNet101
(cid:15)8
0.87769
4.1104e-07
0.8621
5.48e-50
3.6504e-28
0

ResNet101
(cid:15)8
1
0.27643
0.97321
0.00021444
0.021885
7.9778e-37

ResNet101
(cid:15)8
0.99544
0.43034
0.86883
0.0061323
0.00714
3.8037e-54

ResNet101
(cid:15)8
0.99686
0.40899
0.63864
0.0062375
0.01385
1.7682e-19

ResNet101
(cid:15)8
0.73697
0.88072
0.85028
0.14793
0.0001122
1.1988e-119
(cid:15)8
0.9169
0.72948
0.33301
6.4181e-58
5.3445e-16
0

(cid:15)16
0.99589
0.06041
0.79478
3.2656e-09
1.195e-08
7.7789e-61

(cid:15)32
0.99589
2.3283e-06
0.62045
6.6815e-28
9.7675e-18
7.2518e-59

(cid:15)16
0.88685
1.6025e-34
0.39927
1.1918e-222
2.1311e-107
0

(cid:15)32
0.83682
6.9011e-99
0.11822
0
9.0869e-291
0

(cid:15)16
1
0.0011679
0.97355
4.9845e-16
2.1956e-05
9.0441e-38

(cid:15)16
0.98031
0.039834
0.94347
1.4804e-10
1.1845e-06
1.3584e-57

(cid:15)16
0.99662
0.11917
0.0012897
1.065e-06
2.2828e-06
8.4807e-21

(cid:15)32
1
3.2551e-09
0.81807
8.8656e-38
1.1705e-16
5.7158e-41

(cid:15)32
0.98827
2.482e-06
0.29188
5.7541e-34
2.6572e-20
3.9703e-60

(cid:15)32
0.99609
0.0022884
1.0607e-10
8.4707e-20
1.1366e-17
1.7814e-25

(cid:15)16
0.75743
0.92791
0.51736
1.5966e-08
9.641e-23
1.3737e-152
(cid:15)16
0.98099
0.51895
0.017158
1.2544e-79
1.6623e-17
0

(cid:15)32
0.76744
0.065097
0.026006
1.9818e-32
5.6127e-80
3.8698e-193
(cid:15)32
0.83034
0.8264
1.6441e-06
1.8917e-105
1.5303e-14
0

(cid:15)32
0.73081
0.14382
5.201e-14
1.2312e-46
4.2645e-218
5.8058e-77
(cid:15)32
0.99721
0.8903
0.0014581
7.1826e-59
0
6.6939e-89

(cid:15)2
0.23525
0.86903
0.63177
0.61302
0.10882
4.4317e-56
(cid:15)2
0.96275
0.89075
0.56698
1.3175e-40
3.907e-11
2.6025e-310

(cid:15)4
0.23525
0.86904
0.63178
0.61311
0.10882
4.4223e-56
(cid:15)4
0.96275
0.89075
0.56703
1.323e-40
3.907e-11
2.6206e-310

Table 13: Results from the statistical tests applied to each method’s pre-
dictions’ conﬁdence from clean and attacked images using test datasets and
AEs from ResNet and ResNet101. Each value represents the corresponding
p-value from the statistical test.

46

The same behavior is seen in the statistical analysis from Table 14, which
shows the method’s predictions’ conﬁdence to the adversarial patch. The
study showed the same rejection of the null hypothesis Ho from all DCNN
architectures in a signiﬁcant part of the experiments for all classes. Con-
versely, BP accepted the null hypothesis Ho in every experiment. SIFT+FV
showed similar behavior to BP, but the sculpture class and the VGG patch
obtained signiﬁcantly diﬀerent predictions’ conﬁdence.

5. Conclusion

Robustness against AA must be the primary concern when it is developing
an automatic recognition system. So, from now on, a classiﬁer’s performance
should not be focused only on accuracy but also on robustness to AAs. In
this work, we present a comparative study for AMC subject to AA. We
compare several methods to analyze the performance and their reliability to
predict a class using adversarial perturbations. We selected six models using
three of the main approaches for image classiﬁcation: 1) handcrafted fea-
tures approach (SIFT+FV), 2) deep genetic programming approach (BP),
and 3) DCNN approach (AlexNet, VGG, ResNet18, and ResNet101). The
comparative study consists of analyzing three diﬀerent attacks. Firstly, the
direct threat’s impact and transferability considering the white box untar-
geted attack–FGSM. This perturbation adds a subtle texture to the artwork,
which can cause a misleading prediction. Secondly, ﬁnd a set of localiza-
tion and pixel values to modify the artwork to fool the classiﬁer using a
black box untargeted attack–multiple pixel attack. Finally, apply precom-
puted patches–adversarial patch–robust to transformations located randomly
in the artwork to predict a targeted class.

In this sense, this study has demonstrated that AA is a severe threat to
the performance of DCNN. Using FGSM showed that if the attacker knows
the model, it can make the DCNN decrease its performance up to less than
20% of its original score. Additionally, we proved the transferability eﬀect
between DCNN models, which is not severe for the binary classiﬁcation, but
it can reduce up to 20% of the performance. On the other hand, SIFT+FV
also was aﬀected by some of the classes but by a minor amount. However, the
added texture caused by the FGSM lead to a decrease in its performance in a
signiﬁcant manner when the algorithm was tested, having encouraging results
but not suitable to compete with DCNNs in the testing phase. Finally, BP
exhibits comparable performance (eﬃciency) to DCNN in both validation

47

Iconography
Testing Vs. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

0.99901
0.97909
0.56209
1.0847e-07
0.00029613
1.358e-12

0.99019
0.89837
0.79567
1.3363e-23
0.00040313
8.4448e-05

0.99812
0.61682
0.46915
9.9987e-07
2.092e-18
3.9575e-08

0.99608
0.60131
7.1039e-20
0.0003743
2.5542e-05
0.0010507

Painting
Testing Vs. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

1
0.54497
1.3914e-27
4.5007e-16
1.2746e-19
3.855e-20

1
0.82595
7.5744e-12
3.4408e-20
4.9202e-26
9.657e-26

1
0.74837
2.7344e-07
3.0804e-22
1.1991e-15
1.9447e-20

1
0.55432
4.1336e-07
5.2464e-14
2.269e-29
5.876e-18

Painting Landscapes
Testing Vs. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

1
0.35442
1.4598e-08
5.7961e-30
1.3091e-24
5.565e-27

1
0.32247
4.8304e-13
3.0764e-29
5.1204e-32
1.1313e-30

1
0.76126
1.3614e-06
1.6104e-22
1.1945e-33
3.9203e-28

1
0.71106
2.3132e-27
3.4926e-25
1.5553e-28
1.868e-27

Drawings
Testing Vs. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

1
0.20665
7.0741e-12
3.3552e-21
5.042e-17
1.0681e-22

1
0.49234
2.0788e-27
3.9051e-15
5.2286e-15
3.8469e-17

1
0.40336
2.1405e-08
1.1136e-18
1.3091e-24
5.7602e-14

1
0.24792
2.61e-05
2.6781e-20
3.8956e-09
9.1359e-12

0.99544
0.093228
9.0533e-28
6.2478e-06
9.011e-06
3.1949e-07

0.99557
0.49341
8.3985e-08
3.8384e-09
6.7596e-08
1.2838e-08

0.99823
0.14621
1.6128e-08
1.1145e-10
3.0015e-20
2.9946e-07

Sculpture
Testing Vs. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

0.99701
0.048075
1.6758e-06
4.5278e-19
5.6851e-07
0.00045029
Engraving BW
Testing Vs. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

0.9292
0.68646
0.33203
6.3779e-28
1.3125e-21
1.7745e-10
Engraving Color
Testing Vs. AlexNet Patch VGG Patch ResNet18 Patch ResNet101 Patch
BP
SIFT+FV
AlexNet
VGG
ResNet18
ResNet101

0.81987
0.51466
4.7445e-15
0.00057199
0.00012499
0.33861

0.82055
1
0.6454
0.11452
0.00042032
1.2204e-05

0.96904
1
0.42195
5.3585e-08
0.01838
0.29463

0.84032
0.17899
6.8616e-16
2.7464e-15
4.2622e-24
1.5343e-07

0.79371
0.75774
0.80088
1.0857e-15
6.0573e-26
4.6746e-23

0.83695
0.63549
0.13546
1.1879e-13
5.7495e-32
4.6019e-14

0.81363
1
0.32844
0.000143
0.31804
8.898e-10

Table 14: P-value obtained using the adversarial patch. Each column
presents the score obtained for the 100 images per pair (Clean and AE)
48
using the adversarial patch.

and testing phases. It has an almost imperceptible variant on its accuracy
to these perturbations proving no direct transferability from other models.
Figures 5-6 can be observed the output of each stage of BP from clean and
AEs with almost no variation on its outcomes.

The study about one pixel attack conﬁrms this type of attack’s poor de-
sign due to a minimal scenario contrived with an input image of size 32 × 32
pixels. We conclude that it is challenging to apply multiple pixel attacks on
real-world conditions. On the one hand, when we extend to multiple pixels,
the perturbation loses the attack’s intention of being imperceptible to hu-
man vision, not to mention the massive amount of processing time. On the
other hand, BP shows it challenging to fail these attacks even by increasing
ﬁve times the number of pixels per AA compared to SIFT+FV and DCNN
models, which were successfully fooled. Finally, the adversarial patch showed
that a precomputed perturbation positioned in a random location and orien-
tation in the artwork could fool DCNN models with excellent transferability
between them; meanwhile, BP and SIFT+FV remain in their original score.
It is remarkable the BP robustness to the multiple pixel attack and the ad-
versarial patch. However, these two attacks are harsh perturbations and
BP remained steady in its performance, leading to the reliability of BP’s
predictions in no human supervision cases.

The statistical analysis from the predictions’ conﬁdence supports the
study of robustness by illustrating the change in the posterior probability
complementing the results from the accuracy’s standpoint. In this manner,
BP demonstrated to have not signiﬁcantly diﬀerent predictions’ conﬁdence
compare to DCNN models, which showed in most cases the rejection of the
null hypothesis Ho. Conversely, SIFT+FV obtained good results, with most
of the test scoring a not a signiﬁcant diﬀerence in the predictions’ conﬁdence.
In conclusion, art media categorization is a complex problem in which
it is diﬃcult to outperform DCNN performance. Still, BP has comparable
results and is robust to these adversarial attacks with no direct transferability
of such perturbations to the model. On the other hand, SIFT+FV proves
to be robust for a limited number of experiments with moderate results.
So, BP arises as an alternative proposal of an art media classiﬁer without
the vulnerabilities of AA. Additionally, it takes advantage of the symbolic
representations and incorporates rules from expert systems in a hierarchical
structure to solve the AMC problem. Lastly, BP opens the possibility of
being explainable on each of its stages, unlike DCNN, an important research
area to know precisely the model’s inner workings.

49

References

[1] S. Russell, P. Norvig, Artiﬁcial Intelligence: A Modern Approach, Pren-

tice Hall, 2020.

[2] R. Szeliski, Computer vision: algorithms and applications, Springer,

2020.

[3] G. Olague, Evolutionary computer vision: the ﬁrst footprints, Springer,

2016.

[4] A. Darwish, A. Hassanien, S. Das, A survey of swarm and evolutionary
computing approaches for deep learning, Artiﬁcial Intelligence Review
53 (2019) 1767–1812.

[5] P. Druzhkov, V. Kustikova, A survey of deep learning methods and soft-
ware tools for image classiﬁcation and object detection, Pattern Recog-
nition and Image Analysis 26 (2016) 9–15.

[6] B. Zhao, J. Feng, X. Wu, S. Yan, A survey on deep learning-based ﬁne-
grained object classiﬁcation and semantic segmentation, International
Journal of Automation and Computing 14 (2017) 119–135.

[7] Y. Bi, M. Zhang, B. Xue, Genetic programming for automatic global
and local feature extraction to image classiﬁcation, 2018 IEEE Congress
on Evolutionary Computation (CEC) (2018) 1–8.

[8] S. R. Price, D. Anderson, S. Price, Goofed: Extracting advanced features
for image classiﬁcation via improved genetic programming, 2019 IEEE
Congress on Evolutionary Computation (CEC) (2019) 1596–1603.

[9] M. Iqbal, H. Al-Sahaf, B. Xue, M. Zhang, Genetic programming with
transfer learning for texture image classiﬁcation, Soft Computing (2019)
1–13.

[10] Y. Bi, B. Xue, M. Zhang, An eﬀective feature learning approach using
genetic programming with image descriptors for image classiﬁcation [re-
search frontier], IEEE Computational Intelligence Magazine 15 (2020)
65–77.

50

[11] T. Nakane, B. Naranchimeg, H. Sun, X. Lu, T. Akashi, C. Zhang, Ap-
plication of evolutionary and swarm optimization in computer vision: a
literature survey, IPSJ Transactions on Computer Vision and Applica-
tions 12 (2020) 1–34.

[12] Y. Sun, B. Xue, M. Zhang, G. G. Yen, Evolving deep convolutional
neural networks for image classiﬁcation, IEEE Transactions on Evolu-
tionary Computation 24 (2) (2020) 394–407. doi:10.1109/TEVC.2019.
2916183.

[13] F. E. F. Junior, G. Yen, Particle swarm optimization of deep neural
networks architectures for image classiﬁcation, Swarm Evol. Comput.
49 (2019) 62–74.

[14] D. E. Hern´andez, E. Clemente, G. Olague, J. L. Brise˜no, Evolutionary
multi-objective visual cortex for object classiﬁcation in natural images,
Journal of Computational Science 17 (2016) 216 – 233.

[15] D. E. Hern´andez, G. Olague, B. Hern´andez, E. Clemente, Cuda-based
parallelization of a bio-inspired model for fast object classiﬁcation, Neu-
ral Computing and Applications 30 (2017) 3007–3018.

[16] G. Olague, E. Clemente, D. E. Hern´andez, A. Barrera, M. Chan-Ley,
S. Bakshi, Artiﬁcial visual cortex and random search for object catego-
rization, IEEE Access 7 (2019) 54054–54072.

[17] M. Chan-Ley, G. Olague, Categorization of digitized artworks by media
with brain programming., Applied Optics 59 14 (2020) 4437–4447.

[18] G. Olague, D. E. Hern´andez, E. Clemente, M. Chan-Ley, Evolving head
tracking routines with brain programming, IEEE Access 6 (2018) 26254–
26270.

[19] G. Olague, D. E. Hern´andez, P. Llamas, E. Clemente, J. L. Brise˜no,
Brain programming as a new strategy to create visual routines for object
tracking, Multimedia Tools and Applications 78 (5) (2019) 5881–5918.
doi:10.1007/s11042-018-6634-9.

[20] N. Akhtar, A. Mian, Threat of adversarial attacks on deep learning in

computer vision: A survey, IEEE Access 6 (2018) 14410–14430.

51

[21] Y. Li, Y. Wang, Defense against adversarial attacks in deep learning,

Applied Sciences 9 (1) (2018) 76. doi:10.3390/app9010076.

[22] M. Ozdag, Adversarial attacks and defenses against deep neural net-
works: A survey, Procedia Computer Science 140 (2018) 152 – 161,
cyber Physical Systems and Deep Learning Chicago, Illinois November
5-7, 2018. doi:10.1016/j.procs.2018.10.315.

[23] T. Chen, J. Liu, Y. Xiang, W. Niu, E. Tong, Z. Han, Adversarial attack
and defense in reinforcement learning-from ai security view, Cybersecu-
rity 2 (1) (2019) 11. doi:10.1186/s42400-019-0027-x.

[24] H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, A. K. Jain,
Adversarial attacks and defenses in images, graphs and text: A review,
International Journal of Automation and Computing 17 (2) (2020) 151–
178. doi:10.1007/s11633-019-1211-x.

[25] K. Ren, T. Zheng, Z. Qin, X. Liu, Adversarial attacks and defenses in
deep learning, Engineering 6 (3) (2020) 346 – 360. doi:10.1016/j.eng.
2019.12.012.

[26] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing ad-
versarial examples, in: 3rd International Conference on Learning Rep-
resentations, ICLR 2015, Conference Track Proceedings, 2015, p. 11.

[27] J. Luengo, S. Garc´ıa, F. Herrera, A study on the use of statistical tests
for experimentation with neural networks: Analysis of parametric test
conditions and non-parametric tests, Expert Systems with Applications
36 (4) (2009) 7798 – 7808. doi:https://doi.org/10.1016/j.eswa.
2008.11.041.

[28] G. Olague, G. Ibarra-Vazquez, M. Chan-Ley, C. Puente, C. Soubervielle-
Montalvo, A. Martinez, A deep genetic programming based methodology
for art media classiﬁcation robust to adversarial perturbations, in: 15th
International Symposium on Visual Computing, ISVC 2020, Vol. 12509,
Lecture Notes in Computer Science, Springer, 2020, pp. 1–12.

[29] Z. Falomir, L. Museros, I. Sanz, L. Gonzalez-Abril, Categorizing paint-
ings in art styles based on qualitative color descriptors, quantitative
global features and machine learning (qart-learn), Expert Systems with
Applications 97 (2018) 83 – 94.

52

[30] L. Kong, J. Lv, M. Li, H. Zhang, Extracting generic features of artistic
style via deep convolutional neural network, in: International Confer-
ence on Video and Image Processing, ICVIP 2017, 2017, p. 119–123.
doi:10.1145/3177404.3177421.

[31] A. Elgammal, M. Mazzone, B. Liu, D.-E. Kim, M. Elhoseiny, The shape
of art history in the eyes of the machine, in: 32nd AAAI Conference on
Artiﬁcial Intelligence, 2018.

[32] D. Keren, Painter identiﬁcation using local features and naive bayes,
in: Object recognition supported by user interaction for service robots,
Vol. 2, 2002, pp. 474–477 vol.2. doi:10.1109/ICPR.2002.1048341.

[33] J. Li, J. Z. Wang, Studying digital imagery of ancient paintings by
mixtures of stochastic models, IEEE Transactions on Image Processing
13 (3) (2004) 340–353.

[34] R. S. Arora, A. Elgammal, Towards automated classiﬁcation of ﬁne-art
painting style: A comparative study, in: 21st International Conference
on Pattern Recognition (ICPR), IEEE, 2012, pp. 3541–3544.

[35] P. Rosado, Computer vision models to categorize art collections accord-
ing to the visual content: A new approach to the abstract art of antoni
t`apies, Leonardo 52 (2019) 255–260.

[36] S. Karayev, M. Trentacoste, H. Han, A. Agarwala, T. Darrell, A. Hertz-
mann, H. Winnem¨oller, Recognizing image style, in: British Machine
Vision Conference, 2014. doi:10.5244/C.28.122.

[37] Y. Bar, N. Levy, L. Wolf, Classiﬁcation of artistic styles using binarized
features derived from a deep neural network, in: European Conference
on Computer Vision (ECCV), Springer, 2014, pp. 71–84.

[38] N. van Noord, E. Hendriks, E. Postma, Toward discovery of the artist’s
style: Learning to recognize artists by their artworks, IEEE Signal Pro-
cessing Magazine 32 (2015) 46–54.

[39] E. Cetinic, S. Grgic, Genre classiﬁcation of paintings, in: International
Symposium ELMAR, 2016, pp. 201–204. doi:10.1109/ELMAR.2016.
7731786.

53

[40] B. Seguin, C. Striolo, I. diLenardo, F. Kaplan, Visual link retrieval in
a database of paintings, in: European Conference on Computer Vision
(ECCV), 2016, pp. 201–204.

[41] T. Sun, Y. Wang, J. Yang, X. Hu, Convolution neural networks with
two pathways for image style recognition, IEEE Transactions on Image
Processing 26 (2017) 4102–4113.

[42] A. Elgammal, Y. Kang, M. D. Leeuw, Picasso, matisse, or a fake? au-
tomated analysis of drawings at the stroke level for attribution and au-
thentication, in: 32nd AAAI Conference on Artiﬁcial Intelligence, 2018.

[43] E. Cetinic, T. Lipic, S. Grgic, Fine-tuning convolutional neural networks
for ﬁne art classiﬁcation, Expert Systems With Applications 114 (2018)
107–118.

[44] H. Yang, K. Min, Classiﬁcation of basic artistic media based on a deep

convolutional approach, The Visual Computer 36 (3) (2020) 559–578.

[45] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfel-
low, R. Fergus, Intriguing properties of neural networks, in: 2nd Interna-
tional Conference on Learning Representations, ICLR 2014, Conference
Track Proceedings, 2014, p. 10.

[46] S. Zheng, Y. Song, T. Leung, I. Goodfellow, Improving the robustness
of deep neural networks via stability training, in: IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016, pp. 4480–
4488.

[47] D. Su, H. Zhang, H. Chen, J. Yi, P.-Y. Chen, Y. Gao, Is robustness the
cost of accuracy? – a comprehensive study on the robustness of 18 deep
image classiﬁcation models, in: V. Ferrari, M. Hebert, C. Sminchisescu,
Y. Weiss (Eds.), European Conference on Computer Vision (ECCV),
Springer International Publishing, Cham, 2018, pp. 644–661.

[48] J. Su, D. Vargas, K. Sakurai, One pixel attack for fooling deep neural
networks, IEEE Transactions on Evolutionary Computation 23 (2019)
828–841.

54

[49] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, P. Frossard, Universal ad-
versarial perturbations, in: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017, pp. 1765–1773.

[50] T. B. Brown, D. Man´e, A. Roy, M. Abadi, J. Gilmer, Adversarial patch,
31st Conference on Neural Information Processing Systems, NIPS (2017)
6.

[51] Y. Song, T. Kim, S. Nowozin, S. Ermon, N. Kushman, Pixeldefend:
Leveraging generative models to understand and defend against adver-
sarial examples, in: 6th International Conference on Learning Represen-
tations, ICLR 2018, Conference Track Proceedings, 2018, p. 20.

[52] J.-Y. Baek, Y.-S. Yoo, S.-H. Bae, Adversarial learning with knowledge
of image classiﬁcation for improving gans, IEEE Access 7 (2019) 56591–
56605.

[53] S. Gu, L. Rigazio, Towards deep neural network architectures robust
to adversarial examples, in: 3rd International Conference on Learning
Representations, ICLR, Conference Track Proceedings, 2015, p. 9.

[54] A. S. Ross, F. Doshi-Velez, Improving the adversarial robustness and
interpretability of deep neural networks by regularizing their input gra-
dients, in: 32nd AAAI Conference on Artiﬁcial Intelligence, 2018.

[55] N. Papernot, P. McDaniel, X. Wu, S. Jha, A. Swami, Distillation as a
defense to adversarial perturbations against deep neural networks, in:
2016 IEEE Symposium on Security and Privacy (SP), IEEE, 2016, pp.
582–597.

[56] D. Meng, H. Chen, Magnet: a two-pronged defense against adversarial
examples, in: ACM SIGSAC Conference on Computer and Communi-
cations Security, 2017, pp. 135–147.

[57] N. Akhtar, J. Liu, A. Mian, Defense against universal adversarial pertur-
bations, in: IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2018, pp. 3389–3398.

[58] H. Zhang, H. Chen, Z. Song, D. Boning, I. S. Dhillon, C.-J. Hsieh,
The limitations of adversarial training and the blind-spot attack, in:

55

7th International Conference on Learning Representations, ICLR 2019,
Conference Track Proceedings, 2019.

[59] J. S´anchez, F. Perronnin, T. Mensink, J. Verbeek, Image classiﬁcation
with the ﬁsher vector: Theory and practice, International Journal of
Computer Vision 105 (3) (2013) 222–245.

[60] D. M. Titterington, A. F. Smith, U. E. Makov, Statistical analysis of

ﬁnite mixture distributions, Wiley, 1985.

[61] R. M. Gray, D. L. Neuhoﬀ, Quantization, IEEE Transactions on Infor-

mation Theory 44 (6) (1998) 2325–2383.

[62] Q. Zhang, B. Li, Discriminative k-svd for dictionary learning in face
IEEE Conference on Computer Vision and Pattern

recognition,
Recognition (CVPR), IEEE, 2010, pp. 2691–2698.

in:

[63] Z. Jiang, Z. Lin, L. S. Davis, Learning a discriminative dictionary for
sparse coding via label consistent k-svd, in: IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), IEEE, 2011, pp. 1697–
1704.

[64] A. Coates, A. Ng, H. Lee, An analysis of single-layer networks in unsu-
pervised feature learning, in: 14th International Conference on Artiﬁcial
Intelligence and Statistics, 2011, pp. 215–223.

[65] K. Simonyan, A. Vedaldi, A. Zisserman, Deep ﬁsher networks for large-
scale image classiﬁcation, in: Advances in Neural Information Processing
Systems, 2013, pp. 163–171.

[66] Y. He, K. Kavukcuoglu, Y. Wang, A. Szlam, Y. Qi, Unsupervised feature
learning by deep sparse coding, in: SIAM International Conference on
Data Mining, SIAM, 2014, pp. 902–910.

[67] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hub-
bard, L. D. Jackel, Backpropagation applied to handwritten zip code
recognition, Neural Computation 1 (4) (1989) 541–551.

[68] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with
deep convolutional neural networks, in: Advances in Neural Information
Processing Systems, 2012, pp. 1097–1105.

56

[69] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-
scale image recognition, 3rd International Conference on Learning Rep-
resentations, ICLR 2015, Conference Track Proceedings (2015) 14.

[70] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-
nition, in: IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016, pp. 770–778.

[71] R. Poli, W. Langdon, N. McPhee, A ﬁeld guide to genetic programming,

2008.

[72] A. M. Treisman, G. Gelade, A feature-integration theory of atten-
doi:10.1016/

tion, Cognitive Psychology 12 (1) (1980) 97–136.
0010-0285(80)90005-5.

[73] M. A. Goodale, A. Milner, Separate visual pathways for percep-
tion and action, Trends in Neurosciences 15 (1) (1992) 20 – 25.
doi:https://doi.org/10.1016/0166-2236(92)90344-8.
URL
0166223692903448

http://www.sciencedirect.com/science/article/pii/

[74] C. Koch, S. Ullman, Shifts in selective visual attention: towards the

underlying neural circuitry., Human Neurobiology 4 4 (1985) 219–27.

[75] K. Fukushima, Neocognitron: a self-organizing neural network model
for a mechanism of pattern recognition unaﬀected by shift in position,
Biological Cybernetics 36 (1980) 193–202.

[76] M. Riesenhuber, T. Poggio, Hierarchical models of object recognition in

cortex, Nature Neuroscience 2 (1999) 1019–1025.

[77] A. Kurakin, I. J. Goodfellow, S. Bengio, Adversarial machine learning at
scale, 5th International Conference on Learning Representations, ICLR
2017, Conference Track Proceedings (2017) 17.

[78] S. Das, P. N. Suganthan, Diﬀerential evolution: A survey of the state-of-
the-art, IEEE Transactions on Evolutionary Computation 15 (1) (2010)
4–31.

[79] J. Derrac, S. Garc´ıa, D. Molina, F. Herrera, A practical tutorial on the
use of nonparametric statistical tests as a methodology for comparing

57

evolutionary and swarm intelligence algorithms, Swarm Evol. Comput.
1 (2011) 3–18.

[80] S. Garc´ıa, A. Fern´andez, J. Luengo, F. Herrera, A study of statistical
techniques and performance measures for genetics-based machine learn-
ing: accuracy and interpretability, Soft Computing 13 (2009) 959–977.

[81] F. F. de Vega, G. Olague, D. Lanza, O. F.Chavezdela, W. Banzhaf,
E. Goodman, J. Menendez-Clavijo, A. Mart´ınez, Time and individual
duration in genetic programming, IEEE Access 8 (2020) 38692–38713.

58

