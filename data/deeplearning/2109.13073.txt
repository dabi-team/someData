2
2
0
2

g
u
A
5
2

]
L
C
.
s
c
[

2
v
3
7
0
3
1
.
9
0
1
2
:
v
i
X
r
a

Improving Stack Overï¬‚ow question title generation with copying
enhanced CodeBERT model and bi-modal information

Fengji Zhanga,b, Xiao Yua,c,dâˆ—, Jacky Keunge, Fuyang Lia,câˆ—, Zhiwen Xieb, Zhen Yange,
Caoyuan Mab and Zhimin Zhangb

aSchool of Computer Science and Artiï¬cial Intelligence, Wuhan University of Technology, Wuhan, China
bSchool of Computer Science, Wuhan University, Wuhan, China
cWuhan University of Technology Chongqing Research Institute, Chongqing, China
dSanya Science and Education Innovation Park of Wuhan University of Technology, Sanya, China
eDepartment of Computer Science, City University of Hong Kong, Hong Kong, China

A R T I C L E I N F O

A B S T R A C T

Keywords:
Stack Overï¬‚ow
Title generation
Copy mechanism
CodeBERT

Context: Stack Overï¬‚ow is very helpful for software developers who are seeking answers to pro-
gramming problems. Previous studies have shown that a growing number of questions are of low
quality and thus obtain less attention from potential answerers. Gao et al. proposed an LSTM-based
model (i.e., BiLSTM-CC) to automatically generate question titles from the code snippets to improve
the question quality. However, only using the code snippets in the question body cannot provide
suï¬ƒcient information for title generation, and LSTMs cannot capture the long-range dependencies
between tokens.
Objective: This paper proposes CCBERT, a deep learning based novel model to enhance the per-
formance of question title generation by making full use of the bi-modal information of the entire
question body.
Method: CCBERT follows the encoder-decoder paradigm and uses CodeBERT to encode the ques-
tion body into hidden representations, a stacked Transformer decoder to generate predicted tokens,
and an additional copy attention layer to reï¬ne the output distribution. Both the encoder and decoder
perform the multi-head self-attention operation to better capture the long-range dependencies. This
paper builds a dataset containing around 200,000 high-quality questions ï¬ltered from the data oï¬ƒ-
cially published by Stack Overï¬‚ow to verify the eï¬€ectiveness of the CCBERT model.
Results: CCBERT outperforms all the baseline models on the dataset. Experiments on both code-
only and low-resource datasets show the superiority of CCBERT with less performance degradation.
The human evaluation also shows the excellent performance of CCBERT concerning both readability
and correl ation criteria.
Conclusion: CCBERT is capable of automatically capturing the bi-modal semantic information from
the entire question body and parsing the long-range dependencies to achieve better performance.
Therefore, CCBERT is an eï¬€ective approach for generating Stack Overï¬‚ow question titles.

1. Introduction

Stack Overï¬‚ow (SO) is one of the most thriving commu-
nities where software developers can seek answers to pro-
gramming problems from peers. The open-data policy of
SO has been attracting intense research interests [7, 35, 41,
30, 31, 39]. The recent study of Mondal et al. [30] shows
that a growing number of open questions in SO remain unan-
swered, partly because some developers fail to write high-
quality questions. The SO community has given many prac-
tical writing suggestions in the oï¬ƒcial tutorial1 to tackle this
problem, and researchers have also made great eï¬€orts to help
improve question quality in many ways [45, 44, 14].

Previous studies [2, 6, 9, 50] in SO have demonstrated
the importance of question titles to the overall quality of
questions. Recently, Gao et al. [14] for the ï¬rst time pro-

âˆ—Corresponding author

zhangfengji@whu.edu.cn (F. Zhanga,b); xiaoyu@whut.edu.cn (X.

Yua,c,d); jacky.keung@cityu.edu.hk (J. Keunge); fyli@whut.edu.cn (F.
Lia,c); xiezhiwen@whu.edu.cn (Z. Xieb); zhyang8-c@my.cityu.edu.hk (Z.
Yange); macaoyuan@whu.edu.cn (C. Mab); zhangzhimin@whu.edu.cn (Z.
Zhangb)

ORCID(s):

1

https://stackoverflow.com/help/how-to-ask

posed an approach of automatically generating question ti-
tles from given code snippets. They used the BiLSTM-CC
model, which is a Bi-directional Long Short-Term Memory
network incorporated with the Copy [16] and Coverage [40]
mechanism to generate titles from code snippets mined in
corresponding question bodies. Despite the encouraging per-
formance, we argue that LSTMs may lack the ability to parse
long-range dependencies according to Khandelwal et al. [21].
In addition, developers are not recommended to write only
source code as questions in the SO community. Since a ques-
tion body usually consists of the bi-modal content (i.e., text
descriptions and code snippets), the information that devel-
opers infer from code snippets without surrounding contexts
can be broken and misleading.

In this paper, we redeï¬ne the task proposed by Gao et
al. [14] to Title Generation from the Entire Question Body,
namely TGEQB. We formulate this task as an abstractive
summarization problem, and also propose our CCBERT model,
which combines the Copy mechanism [16] to handle rare
tokens and the pre-trained CodeBERT [13] model to parse
bi-modal content. We follow the encoder-decoder paradigm
and use CodeBERT to encode question bodies into hidden

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 1 of 17

 
 
 
 
 
 
Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

representations, a stacked Transformer decoder to generate
predicted tokens, and an additional copy attention layer to
reï¬ne the output distribution. Our encoder and decoder per-
form the multi-head self-attention operation, which helps CCBERT
better capture the long-range dependencies than LSTMs.

To verify the eï¬€ectiveness of our model, we conduct the
empirical study by raising the following Research Questions
(RQs):

RQ-1 Does our CCBERT model outperform the base-
line models? We build a large-scale dataset Dataexp
with
around 200,000 high-quality questions ï¬ltered from the data2
oï¬ƒcially published by Stack Overï¬‚ow in December 2020,
which contains all the historical questions from 2008 to 2020.
We employ BLEU and ROUGE as the evaluation metrics
and choose four baseline models (i.e., TF-IDF [28], BiL-
STM [36], BiLSTM-CC [14], and BART [24]). Experimen-
tal results show that CCBERT outperforms all the baseline
models regarding all the metrics.

RQ-2 What is the advantage of using the bi-modal
information of the entire question body? We build a code-
only dataset and choose BiLSTM-CC to compare with our
model. Experimental results show that applying bi-modal
information greatly boosts both modelsâ€™ performance, where
CCBERT still outperforms BiLSTM-CC.

Figure 1: The proportion of questions with text descriptions
or code snippets

RQ-3 How eï¬€ective is our CCBERT model under low-
resource circumstances? We build three new train sets sized
of 98,909 (Dataexpâˆ•2), 49,454 (Dataexpâˆ•4), and 24,727 (Dataexpâˆ•8)
to train the CCBERT and BiLSTM-CC models. According
to the experimental results, our CCBERT model shows sig-
niï¬cant superiority under low-resource circumstances com-
pared with BiLSTM-CC.

RQ-4 How much inï¬‚uence does interrogative constraint

have on model training? We follow Gao et al. [14] to apply
the interrogative constraint when building Dataexp
, which
may make it easier to train our models. However, our model
can be biased because of this. We build another dataset ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘+
to quantify the inï¬‚uence, and results show that dropping the
interrogative constraint will lead to a decline in the perfor-
mance of both CCBERT and BiLSTM-CC models.

RQ-5 How eï¬€ective is our CCBERT model under hu-
man evaluation? Automated evaluation is not always trust-
worthy. So, we perform a more human-centered evaluation
to investigate the overall quality of the titles generated by
our models. Results show that our CCBERT performs much
better than TF-IDF and BiLSTM-CC concerning the corre-
lation criteria but a little bit worse than human written titles
retrieved by TF-IDF concerning the readability criteria un-
der human evaluation.

The contributions of this paper are as follows:

â€¢ We introduce a new task named TGEQB, which is to
generate high-quality titles from the entire question
body containing bi-modal content to help improve the
question quality.

Figure 2: The overlap between titles and code snippets/text
descriptions

dle rare tokens and long-range dependencies in the bi-
modal context.

â€¢ We have released our dataset and all relevant source
code3 to facilitate future research and application.

We organize the rest of this paper as follows: Section
2 reveals the motivation of our work. Section 3 introduces
the details of our proposed approach. Section 4 describes
the basic setup of our experiment, including the baseline
models, evaluation metrics, and model settings. Section 5
presents the experimental results. Section 6 introduces the
related works. Section 7 discusses threats to the validity of
our work. Finally, we conclude this paper and introduce the
future work in Section 8.

2. Motivation

â€¢ We propose a novel model named CCBERT, which
combines the copy mechanism and CodeBERT to han-

We share similar user scenarios with Gao et al.

[14],

where less experienced developers or non-native English speak-

2

https://archive.org/download/stackexchange

3

https://github.com/zfj1998/SO_Title_Generation

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 2 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

ers may not be able to adequately describe their questions
according to the writing rules suggested by SO. We can use
an automated data-driven approach to help developers draft
high-quality question titles in such circumstances. However,
we argue that one should consider both text descriptions and
code snippets when writing question titles. We also have
concerns about the long-range dependency issues of using
the entire question body as input. Therefore, this section
aims to investigate the necessity of title generation using bi-
modal content and the challenge of long sequence parsing.

2.1. Importance of bi-modal Content

Our ï¬rst step is to get a collection of high-quality sam-
ples for statistics. We believe that high-quality question posts
should be clear and complete so that developers in the SO
community are more likely and willing to answer. But it is
a non-trivial task to evaluate the clearness and completeness
of a post automatically. Therefore, we ï¬rst ï¬lter the ques-
tion posts based on the feedback they received from the SO
community:

1. The question is not closed;
2. The question has an accepted answer;
3. The question gets more than one vote.

After ï¬ltering out the ones that do not meet the above feedback-
related constraints, we obtain a collection of 3.2 million ques-
tion posts that we regard as candidates of high-quality ones.
Intuitively, a programming question should always con-
tain both text and code. Besides, when drafting a new ques-
tion post in SO, the website will give the three suggestions:
4

1. Summarize the problem;
2. Describe what you have tried;
3. Show some code.

We separately count the high-quality candidate questions
containing text descriptions and code snippets by year. To
be speciï¬c, question posts in the source ï¬le5 are organized in
a uniï¬ed HTML format, so we extract the content wrapped
by "<code></code>" tags as code snippets and the rest as
text descriptions. We draw a line chart in Fig. 1 to show
the statistical results, where the x-axis denotes the year and
the y-axis denotes the proportion of questions with text de-
scriptions or code snippets. We ï¬nd that the proportion of
high-quality candidates containing text descriptions is al-
most unchanging (100%) every year. While the proportion
of high-quality candidates containing code snippets has been
increasing in recent years, reaching 90% in 2020.

In addition, we have manually studied a number of high-
quality candidates posted recently without code snippets. We
ï¬nd that many of them are not programming-related ques-
tions, such as software/platform instructions,6 knowledge Q&A,7

4
https://stackoverflow.com/questions/ask
5the stackoverï¬‚ow.com-Posts.7z ï¬le
6
https://stackoverflow.com/questions/59553413/

etc. Some others may put the code in images8 or external
links.9 The above-mentioned question posts are beyond the
scope of this preliminary study.

Therefore, we believe that containing the content of both
modalities helps improve the clearness and completeness of
programming questions in SO. In our study, only the ques-
tion posts containing bi-modal content and meeting the three
feedback-related constraints are considered high-quality.

2.2. Impact of bi-modal Content on Titles

The title of a question post is always dependent on the
overall semantics of the question body. However, it is a non-
trivial task to tell the precise impact of the bi-modal content
in the question body on writing the title. In this subsection,
we conduct a lexical-level statistical experiment and a hu-
man evaluation to estimate such impact.

First, we extract the high-quality question posts from 2008
to 2020, which contain bi-modal content and meet the three
feedback-related constraints. Then we count the tokens that
appear both in the title and text descriptions/code snippets,
and draw a line chart in Fig. 2 to demonstrate the average
overlap ratios by year. Moreover, we combine Fig. 2 with
a bar chart to demonstrate the number of statistical samples
per year. Speciï¬cally, the x-axis denotes the year, the left
y-axis denotes the amount of questions, and the right y-axis
denotes the overlap ratios between titles and text descrip-
tions/code snippets.

From Fig. 2, we may ï¬nd that the overlap ratios of high-
quality questions have been stable since 2014. This may in-
dicate that a certain extent of token overlap between the title
and the bi-modal content ensures the title quality. To inves-
tigate this issue, we further perform a manual analysis.

We consider two criteria when manually evaluating post
titles, either of which can be scored between 1 and 4. We
illustrate the detailed descriptions and scoring standards in
Table 1. We split the ï¬ltered high-quality posts into four
categories according to their average overlap ratios between
titles and the bi-modal content. And then, we randomly sam-
ple 500 question posts from each category. Five independent
graduate students who are experienced programmers and fa-
miliar with Stack Overï¬‚ow are invited to rate the titles based
on the scoring standards, and each participant is assigned
100 posts per category. From the evaluation results in Ta-
ble 2, we can ï¬nd both the readability and correlation scores
degrade when the overlap ratio is too low or too high. Ac-
cording to our participants, when the overlap is too low, the
title tends to be vague. When the overlap is too high, the title
tends to be the bare error report of a program. Both situa-
tions require more eï¬€ort for the reader to fully understand
the question. This suggests that properly borrowing tokens
from the bi-modal content makes the title more expressive
and matching the points of the question. Therefore, we have
reasons to believe that bi-modal content is essential to writ-
ing good titles.

8

https://stackoverflow.com/questions/59552547/

firebase-storage-image-not-showing

7

https://stackoverflow.com/questions/59554837/

ios-swiftui-how-to-bring-up-extra-actions-like-embed-in-vstack-when-interactin

9

https://stackoverflow.com/questions/59552571/

uml-how-to-model-either-or-both-union-concept

how-to-check-if-fixed-width-integers-are-defined

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 3 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Table 1
The criteria used for manually evaluating the titles. The evaluation score of each criteria
is between 1 and 4.

Criteria

Description

Title Scoring Standard

Readability

Ignoring the content, considering the
grammaticality and ï¬‚uency of the title

Correlation

Considering the consistency between
the question and the title

1. Has too many errors to read and understand
2. Has minor errors but is readable and understandable
3. Is very easy to read and understand
4. Is very expressive and appealing

1. Is totally missing the point of the question
2. Is relevant to the main point of the question
3. Is a good match of the questionâ€™s points
4. Is a perfect summary of the question

Table 2
Human evaluation results of the title quality with diï¬€erent overlap ratios with the bi-
modal content.The ratios of four diï¬€erent scores and the average score are listed grouped
by diï¬€erent criteria.

Criteria

Overlap Ratio

Score 1

Score 2

Score 3

Score 4 Avg Score

Readability

Correlation

0-0.2

0.2-0.4

0.4-0.6

0.6-1.0

0-0.2

0.2-0.4

0.4-0.6

0.6-1.0

-
-
-
-

-
-
-
-

21.6%
7.8%
11%
26%

11.2%
2.8%
2.8%
6.2%

77.4%
88%
85.6%
73.8%

87%
86.4%
89%
90.6%

1.0%
4.2%
3.4%
0.2%

1.8%
10.8%
8.2%
3.2%

2.794
2.964
2.924
2.742

2.906
3.080
3.054
2.970

problem. Later in Section 5, we will make further compar-
isons of these two models.

3. Proposed Approach

We aim to help developers write high-quality questions
with a better chance to get answers in Stack Overï¬‚ow. Con-
sidering that using only code snippets is not enough to gen-
erate high-quality titles, we introduce a new title generation
task named TGEQB, which utilizes the bi-modal informa-
tion of both text descriptions and code snippets in the ques-
tion body. Following the general practice in machine learn-
ing studies, the framework of our approach demonstrated in
Fig. 4 contains three main steps: data preparation, model
training, and validation. We describe the details of our ap-
proach in this section, including the data preparation proce-
dures and the detailed architecture of our CCBERT model.

3.1. Data Preparation

Further ï¬ltering, tokenization, and partitioning are per-
formed on the high-quality question posts extracted in Sec-
tion 2.2 before we ï¬nally get the experimental dataset ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘
Firstly, considering the vocabulary of our CodeBERT
encoder was built on a dataset10 concerning only six pro-
gramming languages: Java, Python, JS(JavaScript), PHP,
Ruby, and Go, we also focus on the questions tagged with
these programming languages in this preliminary study. To

.

10CodeSearchNet https://github.com/github/CodeSearchNet

Figure 3: The length distribution of code snippets and text
descriptions in the question body

2.3. The Long-range Dependency Issue

We draw a box plot in Fig. 3 to represent the length dis-
tributions of the entire question body, the code snippets, and
the text descriptions of the high-quality question posts ex-
tracted in Section 2.2. We can ï¬nd that the code snippets
only occupy less than half of the body content, and the en-
tire content of a question body can be very long, bringing
new challenges to our title generation models. Speciï¬cally,
over 56% questions have more than 200 tokens in their bod-
ies, and some questions even have 500 tokens and more. The
LSTM structure is only capable of using 200 tokens of con-
text on average according to Khandelwal et al. [21], so we
apply Transformer-based pre-trained models to tackle this

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 4 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Figure 4: The framework of our approach for Stack Overï¬‚ow question title generation

avoid the inï¬‚uence of noise data, we further ï¬lter out posts
tagged with other popular languages in SO, including C#,
HTML, and C++. In the end, we ï¬nd the amounts of ï¬ltered
Ruby and Go questions11 far from enough for training and
testing, which leaves us the questions tagged with only four
programming languages: Java, Python, JS, and PHP.

Secondly, we notice that in Gao et al.â€™s work [14], they
only kept the questions containing interrogative keywords:
how, what, why, which, and when in their titles. While only
1/3 of our ï¬ltered high-quality question posts satisfy this
constraint. After manually examining our data samples, we
ï¬nd that question titles without interrogatives tend to be more
casual and are always incomplete sentences, which undoubt-
edly brings many diï¬ƒculties for the model training. So in
this preliminary study, we choose to apply this constraint in
our primary dataset ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘
and perform an additional ex-
periment later in Section 5.4 to investigate the performance
of our models without this constraint.

In addition, we notice that the NLTK tokenizer12 can not
separate special tokens in code snippets well, which leads to
an extensive vocabulary and exacerbates the out-of-vocabulary
issue. So we choose a simple tokenizing algorithm to tackle
this problem. Speciï¬cally, there are three kinds of printable
characters in the ASCII charset, including the digits (0 to 9),
letters (A/a to Z/z), and punctuation symbols. We ï¬rst put a
white space on both sides of punctuation symbols in a string
during tokenization and then split the string into tokens by
white spaces. This way, we get a smaller vocabulary but
longer sequences in return. Handling very long sequences is
still an open problem in the ï¬eld of deep learning [10, 5, 52].
In this preliminary study, we choose to ï¬lter out 5% of the
question posts whose body length exceeds 1,000, or the title
length exceeds 25.

As for data partitioning, we sort the questions in chrono-

11There are only approximately 7,000 and 3,500 ï¬ltered questions for

Ruby and Go.

12

http://www.nltk.org/api/nltk.tokenize.html

logical order and choose the latest samples for testing and the
rest for training. Because we think it is more applicable to
the real-world application if the models take past questions
for training and new ones for testing. Besides, we believe
our time-wise partitioning will help relieve the target leak-
age problem caused by the homogeneous questions between
the train and test sets. We show the statistics of our processed
dataset in Table 3.

Table 3
The partition size of ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘

Language

Train

Validation

Test

Java
Python
JS
PHP

57,118
60,458
53,708
26,535

Total

197,819

2,000
2,000
2,000
1,000

7,000

2,000
2,000
2,000
1,000

7,000

3.2. The CCBERT Model

We propose CCBERT, a novel model combining the pre-
trained CodeBERT model and the copy mechanism.
It is
an attentional encoder-decoder system, which can be trained
and used in an end-to-end manner. Our model architecture
is illustrated in Fig. 5. Speciï¬cally, we apply the Code-
BERT model and Transformer-decoder layers in their orig-
inal form and put a specialized copy attention layer above
the encoder and decoder. Formally, given a token sequence
ğ‘‹ = [ğ‘¥1, ğ‘¥2, ...ğ‘¥ğ‘›] of a question body and a token sequence
ğ‘‡ = [ğ‘¡1, ğ‘¡2, ..., ğ‘¡ğ‘™] of its corresponding title sampled from our
dataset, CCBERT learns to generate ğ‘‡ based on ğ‘‹.

3.2.1. CodeBERT Encoder

Unlike the general models used for summarization [15,
27, 37], our model needs to understand both Natural Lan-
guage (NL) and Programming Language (PL), which is de-

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 5 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

[ğ‘’ğ‘†ğ‘‚ğ‘† , ğ‘’1, ğ‘’2, â€¦ , ğ‘’ğ‘˜

a matrix ğ¸ containing the embedding vectors of tokens (i.e,
]). The input for the decoder is
ğ¸ =
two-fold, one is the hidden vectors ğ» provided by the en-
coder, the other is the embedding ğ¸ of generated sequence.
We feed ğ» and ğ¸ to the decoder and get a matrix ğ‘‰ con-
taining the hidden representations of ğ‘˜ + 1 predicted tokens

ğ‘‰ = DECODER(ğ», ğ¸),

(3)

where ğ‘‰ = [ğ‘£1, ğ‘£2, ..., ğ‘£ğ‘˜+1]. We take ğ‘£ğ‘˜+1
representation of the (ğ‘˜ + 1) ğ‘¡â„ predicted token.

as the hidden

3.2.3. Copy Attention Layer

Usually, we can have several linear layers above the de-
to its most likely
coder to map the hidden representation ğ‘£ğ‘˜+1
token in the vocabulary. However, according to the statistics,
question titles have a high overlap with the body content. Be-
sides, we should also pay attention to some essential but rare
tokens, such as variable names, class libraries, application
frameworks, etc. In this case, we incorporate the copy mech-
anism to facilitate our model to copy tokens directly from
the body content when generating titles. The copy mecha-
nism was ï¬rst introduced in the pointer-generator network
[37], which was originally applied to the Recurrent Neural
Networks (RNNs). In our work, we implement a special-
ized copy attention layer to adapt the copy mechanism to
our Transformer-based model.

Formally, when generating the (ğ‘˜ + 1) ğ‘¡â„ token in the
predicted title, we ï¬rst need to calculate the attention vector
with the encoder hidden state ğ», the embedding vec-
ğ‘ğ‘˜+1
tor of the generated token ğ‘’ğ‘˜
, and the hidden representation
ğ‘£ğ‘˜+1

of the (ğ‘˜ + 1)ğ‘¡â„ predicted token,

ğ‘ğ‘˜+1 = ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ», ğ‘’ğ‘˜, ğ‘£ğ‘˜+1).
Then, we use the attention vector ğ‘ğ‘˜+1
den state matrix ğ» to get a single vector ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘˜+1
overall "context" of the input sequence,

and the encoder hid-
as the

(4)

ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ–³

ğ‘˜+1 = ğ‘ğ–³

ğ‘˜+1 â‹… ğ»,

(5)

where ğ–³ represents the transpose symbol. After that, with
and the current decoder state
the input context ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘˜+1
of each
ğ‘£ğ‘˜+1
token in the vocabulary to be chosen as the (ğ‘˜ + 1)ğ‘¡â„ pre-
dicted token,

, we can get the probability distribution ğ‘ƒğ‘£ğ‘œğ‘ğ‘ğ‘

ğ‘ƒğ‘£ğ‘œğ‘ğ‘ğ‘ = ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘†ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘˜+1, ğ‘£ğ‘˜+1),
where ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘†ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ is a linear neural network with the
Softmax output layer.

(6)

Usually, we can choose the token with the highest proba-
bility in ğ‘ƒğ‘£ğ‘œğ‘ğ‘ğ‘
as the predicted token. But to incorporate the
copy mechanism, we have to calculate an additional proba-
as a soft switch to choose between generating a
bility ğ‘ğ‘ğ‘œğ‘ğ‘¦
word from the vocabulary by sampling from ğ‘ƒğ‘£ğ‘œğ‘ğ‘ğ‘
, or copy-
ing a word from the input sequence by sampling from the
attention distribution ğ‘ğ‘˜+1

,

ğ‘ğ‘ğ‘œğ‘ğ‘¦ = ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘˜+1, ğ‘£ğ‘˜+1, ğ‘’ğ‘˜),

(7)

Figure 5: The detailed structure of CCBERT at the (ğ‘˜ + 1)ğ‘¡â„
decoding step

termined by the characteristics of our dataset. Recently, Feng
et al. [13] introduced CodeBERT, which was pre-trained on
a vast scale dataset extracted from Github repositories con-
taining source code and code comments. This way, Code-
BERT can capture the semantic relationship between NL and
PL, and produce vector representations that support down-
stream tasks, such as defect prediction [32, 55, 53, 56, 54],
program repair [29], etc.

We use the pre-trained CodeBERT as our encoder. It is a
stack of multiple Transformer-encoder layers which mainly
performs bidirectional self-attention operations. Formally,
given a question body containing text descriptions and code
snippets, we ï¬rst turn it into a sequence of tokens with a
byte pair encoding tokenizer built in the CodeBERT model.
Then, we surround the token sequence with two special to-
kens13 to be consistent with the data format used during pre-
training and get the ï¬nal input sequence ğ‘‹,

ğ‘‹ =

[ğ‘¥ğ¶ğ¿ğ‘† , ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘›, ğ‘¥ğ‘†ğ¸ğ‘ƒ

] .

(1)

After that, we feed ğ‘‹ to the encoder and get a matrix ğ» that
consists of the encoded vectors of all input tokens

ğ» = ENCODER(ğ‘‹),

(2)

where ğ» = [â„ğ¶ğ¿ğ‘† , â„1, â„2, ...â„ğ‘›, â„ğ‘†ğ¸ğ‘ƒ ] and each vector â„ğ‘–
is a hidden representation of the semantic relationship of a
token against others.

3.2.2. Transformer Decoder

After encoding the input question body, we need the de-
coder to generate the hidden representation of each token in
the predicted question title. Since the nature of the Code-
BERT encoder is Transformer-encoder layers, we stack sev-
eral layers of vanilla Transformer-decoder [42] as our de-
coder.

[ğ‘¦ğ‘†ğ‘‚ğ‘† , ğ‘¦1, ğ‘¦2, â€¦ , ğ‘¦ğ‘˜

Formally, suppose we have generated the ï¬rst ğ‘˜ tokens
])14 of the predicted title, and
(i.e., ğ‘Œ =
now are going to generate the (ğ‘˜ + 1) ğ‘¡â„ token (i.e, at the
(ğ‘˜ + 1)ğ‘¡â„ decoding step). We ï¬rst use the same embed-
ding layer of the encoder to turn the input sequence ğ‘Œ into

13The SEP token marks the end of a sentence. The CLS token is put in

front of the input sequence and specially used for sentence classiï¬cation.

14ğ‘†ğ‘‚ğ‘† means the start of a sequence.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 6 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

where ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ is a linear layer with the Sigmoid ac-
tivation function. Finally, we can get the revised probability
distribution ğ‘ƒ (ğ‘¡ğ‘˜+1) of choosing the (ğ‘˜ + 1) ğ‘¡â„ token,

ğ‘ƒ (ğ‘¡ğ‘˜+1) = ğ‘ğ‘ğ‘œğ‘ğ‘¦

ğ‘›
âˆ‘

ğ‘–âˆ¶ğ‘¥ğ‘–=ğ‘¡ğ‘˜+1

ğ‘(ğ‘˜+1)ğ‘–

+(1âˆ’ğ‘ğ‘ğ‘œğ‘ğ‘¦)ğ‘ƒğ‘£ğ‘œğ‘ğ‘ğ‘(ğ‘¡ğ‘˜+1). (8)

We generate each token recursively and stop when the ğ¸ğ‘‚ğ‘†15
token comes up. The overall trainable parameters ğœƒ include
those of the stacked Transformer-encoder layers in Code-
BERT, the stacked Transformer-decoder layers in our de-
coder, and the linear neural networks in our copy attention
layer. The training loss is the negative log-likelihood of each
token in the target sequence, which we use to update ğœƒ through
backpropagation to maximize the likelihood between the gen-
erated titles and the original ones in our dataset during train-
ing.

4. Experimental Setup

This section illustrates the baseline models, the evalua-
tion metrics, and the hyperparameter settings for our CCBERT
model.

4.1. Comparisons

To demonstrate how competitive CCBERT is, we choose
several state-of-the-art models as baselines, which have been
widely studied in the ï¬eld of natural language processing.
We brieï¬‚y introduce the general ideas of these models in the
following.

(1) TF-IDF This method is a classic full text searching al-
gorithm, its name stands for "Term Frequency (TF) Ã— In-
verse Document Frequency (IDF)". TF-IDF is a weight-
ing algorithm for a bag-of-words language model. Specif-
ically, the "bag" contains a list of unique terms sourced
from a given corpus. A paragraph can be turned into
a vector by counting its in-bag termsâ€™ frequency (TF).
Because the probability of a termâ€™s occurrence is often
in inverse proportion to its importance, one can use the
term frequency of appearing in all documents (IDFâˆ’1) to
divide TF and get the revised weight of each term. This
way, we can calculate the distance between paragraphs
in the vector space. In our experiment, we use Lucene16
to ï¬nd the most similar question in the train set given a
question body.

(2) BiLSTM Long Short Term Memory networks (LSTMs)
are a special kind of RNNs, with an additional cell state
and three carefully designed "gates" to alleviate the prob-
lem of long-term dependencies. The idea of Bidirec-
tional LSTMs (BiLSTMs) is to duplicate the ï¬rst recur-
rent layer in the network and then provide the input se-
quence to the ï¬rst layer and a reversed copy of the input
to the second. This way, all available information in the
past and future of a speciï¬c processing step can be con-
sidered during training. We stack two BiLSTM layers as

the encoder and two LSTM layers as the decoder, along
with the attention mechanism introduced by Bahdanau
et al. [3] to build a model as our baseline, which we refer
to as BiLSTM.

(3) BiLSTM-CC This was the method used by Gao et
al. [14] to generate question titles from code snippets.
It shares the same structure as the BiLSTM model men-
tioned above, except to assemble another two non-trivial
mechanisms. One is the copy mechanism we have il-
lustrated above; the other is the coverage mechanism.
Tu et al. [40] ï¬rst introduced the "coverage" vector that
keeps track of the attention history and further facilitates
the attention calculation so that a neural machine trans-
lation system would consider more about untranslated
[14] took advantage of the cover-
words. Gao et al.
age penalty to suppress meaningless repetitions during
generation. In our experiment, we build the BiLSTM
and BiLSTM-CC models with OpenNMT,17 which is
a well-acknowledged framework to build sequence-to-
sequence models.

(4) BART

Lewis et al. [24] proposed the BART model
to bridge the gap between pre-trained Bidirectional en-
coder (i.e. BERT [11]) and pre-trained Auto-Regressive
Transformer (i.e. GPT [34]), which are good at compre-
hension and generation tasks respectively. BART is pre-
trained under the supervision of several denoising objec-
tives, where input text is corrupted by a stochastic nois-
ing function and the model is demanded to reconstruct
the original text. BART is particularly eï¬€ective when
ï¬ne-tuned for neural machine translation and abstractive
text summarization tasks, such as WMT, CNN/DailyMail,
XSum etc. We use the open-source code and pre-trained
parameters18 for BART to validate its performance on
our dataset.

In addition to the above baselines, we implement an or-
acle method to show the best performance of an extractive
model.

(5) Oracle

The idea of extractive summarization, which
is to select primary sentences that best match the target
summary, inspires us to explore the possibility of mak-
ing up a title only using tokens that appear in the ques-
tion body. However, there are millions of permutations
of a title containing tens of tokens, which is more com-
plicated than selecting and arranging sentences. In ad-
dition, tokens arranged in the correct order do not neces-
sarily make sense. So, instead of building another base-
line model, we remove the tokens in a question title if
they are not in the question body and keep the rest as the
"generated" title to simulate the best performance of an
extractive model. Considering our objective is to maxi-
mize the BLEU and ROUGE score, we follow the work
of Liu et al. [27] and implement another method based
on beam search (with 20 beam width) to ï¬nd the per-
mutation that performs the best on these two metrics.

15ğ¸ğ‘‚ğ‘† means the end of a sequence
16Apache Lucene computes the similarity using TF-IDF by default.

17

18

https://opennmt.net
https://huggingface.co/facebook/bart-base

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 7 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

It turns out that the second method does no better than
the ï¬rst one on both metrics due to the limited searching
space. Therefore, we use the simple method mentioned
above as the oracle method indicating the best possible
result from a model.

4.2. Automated Evaluation Metrics

Since the nature of our task is a sequence generation
problem, where BLEU and ROUGE are the most commonly
used metrics, we choose both to measure the precision- and
recall-oriented similarity between the generated titles and
the original ones.

4.2.1. BLEUS-4

The Bi-Lingual Evaluation Understudy (BLEU) method
was ï¬rst introduced by Papineni et al. [33] to measure the
performance of a translation system. Given the candidate
translations and reference sentences, the ï¬rst step in this method
is to compute the ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š precision,

âˆ‘

ğ‘ğ‘› =

ğ¶âˆˆ{ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ }

âˆ‘

ğ¶âˆˆ{ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ }

âˆ‘
ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šâˆˆğ¶
âˆ‘
ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šâˆˆğ¶

ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘ğ‘™ğ‘–ğ‘(ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š)

ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š)

,

(9)

ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘ğ‘™ğ‘–ğ‘ = ğ‘šğ‘–ğ‘›(ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡, ğ‘€ğ‘ğ‘¥_ğ‘Ÿğ‘’ğ‘“ _ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡),

(10)

clips
where ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š denotes the candidate ngrams, ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘ğ‘™ğ‘–ğ‘
the total ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ of each candidate ngram by the maximum
number of overlap between ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š in the candidate and the
references ğ‘€ğ‘ğ‘¥_ğ‘Ÿğ‘’ğ‘“ _ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡, to avoid overgenerating "rea-
counts the
sonable" words.
number of candidate ngrams that appear in references, and
the denominator counts all the candidate ngrams.

In brief, the numerator of ğ‘ğ‘›

The next step is to compute a brevity penalty, which is to
adapt the candidate translation to match the reference trans-
lation in length,

{

ğµğ‘ƒ =

1,
ğ‘’(1âˆ’ğ‘™ğ‘Ÿâˆ•ğ‘™ğ‘ ),

if ğ‘™ğ‘ > ğ‘™ğ‘Ÿ
â‰¤ ğ‘™ğ‘Ÿ
if ğ‘™ğ‘

,

(11)

where ğ‘™ğ‘
is the
is the length of a candidate translation and ğ‘™ğ‘Ÿ
length of the reference corpus. Then, we can get the BLEU
score

BLEU = ğµğ‘ƒ â‹… ğ‘’ğ‘¥ğ‘

( ğ‘
âˆ‘

ğ‘›=1

)
.

1
ğ‘

ğ‘™ğ‘œğ‘”ğ‘ğ‘›

(12)

In our experiment, we choose ğ‘ = 4 to have the BLEU-4
score. Besides, we apply a smoothing method introduced by
Lin et al. [26] to add one to the ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š hit count and total
ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š count for ğ‘› > 1. This way, candidate translations with
less than n words can still get a positive score. We refer to the
smoothed method as BLEUS-4 and use its implementation
of NLTK19 in our experiment.

19

http://www.nltk.org/_modules/nltk/translate/bleu_score.html

4.2.2. ROUGE

The Recall-Oriented Understudy for Gisting Evaluation
(ROUGE) was introduced by Lin et al. [25] to measure the
quality of machine-generated summaries. It consists of sev-
eral measures including ROUGE-N and ROUGE-L, which
will be used in our experiment. On complementary of BLEUâ€™s
bias on ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š precision, ROUGE-N focuses on the ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š
recall, which is calculated as

âˆ‘

ROUGEâˆ’N =

ğ‘†âˆˆ{ğ‘…ğ‘’ğ‘“ ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘ }
âˆ‘

ğ‘†âˆˆ{ğ‘…ğ‘’ğ‘“ ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘ }

âˆ‘
ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šâˆˆğ‘†
âˆ‘
ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šâˆˆğ‘†

ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘š(ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š)

ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š)

,

(13)

is the
where ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š denotes the reference ngrams, ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘š
maximum number of overlap between ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š in a candi-
date summary and the references. In brief, the numerator
of ROUGE-N is to count the number of overlap ngrams be-
tween candidates and references, and the denominator is to
count all the reference ngrams.

ROUGE-L takes advantage of both the Longest Com-
mon Subsequence (LCS) and the F-measure to estimate the
similarity between two summaries: the candidate summary
of length
ğ‘†ğ‘ğ‘ğ‘›
ğ‘™ğ‘’

and the reference summary ğ‘†ğ‘Ÿğ‘’ğ‘“

. The calculation is as follows:

of length ğ‘™ğ‘

ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğ‘™ğ‘ğ‘  =

ğ¿ğ¶ğ‘†(ğ‘†ğ‘Ÿğ‘’ğ‘“ , ğ‘†ğ‘ğ‘ğ‘›)
ğ‘™ğ‘’

,

ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘™ğ‘ğ‘  =

ğ¿ğ¶ğ‘†(ğ‘†ğ‘Ÿğ‘’ğ‘“ , ğ‘†ğ‘ğ‘ğ‘›)
ğ‘™ğ‘

,

ROUGEâˆ’L =

ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğ‘™ğ‘ğ‘ ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘™ğ‘ğ‘ 
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğ‘™ğ‘ğ‘  + ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘™ğ‘ğ‘ 

,

(14)

(15)

(16)

In the end, we choose ROUGE-1, ROUGE-2, and ROUGE-
L implemented by an open source library20 for the evaluation
metrics.

4.3. Model Settings

We implement our encoder with the pre-trained parame-
ters21 of CodeBERT-base and keep its initial settings, where
the vocabulary size is 50265, the hidden size is 768, the
dropout probability is 0.1, and the Transformer layer num-
ber is 12. Accordingly, we build a 12-layer decoder with ran-
domly initialized parameters. Optimization is performed us-
ing the adaptive moment estimation (Adam) algorithm with
ğ›½1 = 0.9, ğ›½2 = 0.999, ğœ– = 10âˆ’8 and ğ‘™ğ‘Ÿ = 5 Ã— 10âˆ’5. We also
apply a linear warm-up strategy to gradually increase the
learning rate in the ï¬rst 10% training steps. Four NVIDIA
GeForce RTX 2080 Ti GPUs are used to train our model,
where the training epoch is ten and batch size is 32. During
decoding, we set the beam size to ten. We adjust all the hy-
perparameters to the validation set and report the evaluation
results on the test set.

20

21

https://pypi.org/project/rouge
https://huggingface.co/microsoft/codebert-base

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 8 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Table 4
The evaluation results of models trained on the joint dataset of four programming lan-
guages. All the score numbers are averages over the tested posts of diï¬€erent languages.

Model

Language BLEUS-4

ROUGE-1

ROUGE-2

ROUGE-L

Oracle

TF-IDF

BiLSTMjoint

BiLSTM-CCjoint

BARTjoint

CCBERTjoint

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

54.58
51.43
53.10
54.22

9.79
10.26
10.10
10.49

17.04
17.71
18.06
18.66

19.73
19.74
20.59
20.56

20.80
21.01
21.54
22.28

21.16
22.40
22.18
22.65

83.76
82.40
83.02
83.65

19.91
21.88
20.51
21.24

36.74
39.89
38.79
39.97

41.10
42.67
42.62
43.01

44.21
45.69
45.65
46.94

44.26
46.88
45.72
47.03

65.13
62.55
63.68
65.19

4.44
5.28
4.93
5.15

15.35
16.86
16.56
17.99

19.54
19.97
20.36
20.92

21.12
22.44
22.29
23.47

21.58
22.89
22.40
23.50

83.31
81.81
82.55
83.21

19.17
21.01
19.76
20.30

36.17
39.04
38.09
38.92

40.04
41.72
41.59
41.73

42.42
44.28
43.81
45.05

42.92
44.92
44.15
45.15

5. Results and Analysis

In this section, we demonstrate the eï¬€ectiveness of our
model by conducting experiments to answer the following
Research Questions (RQs):

RQ-1 Does our CCBERT model outperform the baseline

models?

RQ-2 What is the advantage of using the bi-modal informa-

tion of the entire question body?

RQ-3 How eï¬€ective is our CCBERT model under low-resource

circumstances?

RQ-4 How much inï¬‚uence does interrogative constraint have

on model training?

RQ-5 How eï¬€ective is our CCBERT model under human

evaluation?

5.1. RQ-1: Does our CCBERT model outperform

the baseline models?

Method: In order to investigate the superiority of our model,
we compare it to the baselines mentioned in Section 4.1.
We also apply two training strategies to the deep learning
jointly with all the
models. One is to train on the ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘
questions. The other is to train on separated smaller subsets
of questions concerning diï¬€erent programming languages.
Both training strategies share the same validation and test
sets to compare the performance. Table 4 and Table 5 show
the performance of all models on four evaluation metrics. In

addition, we present four test examples in Table 6 to make
intuitive comparisons.
Results: From Table 4, Table 5, and Table 6, we have the
following ï¬ndings:

(1) The performance rankings are the same on both train-
ing strategies, where CCBERT outperforms all the baselines
ranging from the retrieval-based model (TF-IDF) to the large-
scale pre-trained model (BART). Moreover, all the models
trained on the joint dataset perform better than those trained
on separated subsets, attributing to the increased amount of
training samples and the similar writing pattern shared by
high-quality questions involving diï¬€erent programming lan-
guages. We have also noticed that Java questions are more
diï¬ƒcult for all the models, which is similar to the results re-
ported by Gao et al. [14]. This is partly because Java ques-
tions have a larger vocabulary than others, and models are
more likely to encounter rare tokens.

(2) TF-IDF has the worst performance among all the base-
line models and can barely compare with other baselines.
This is not surprising because questions containing dupli-
cated content in Stack Overï¬‚ow have a high possibility of
being closed, let alone only a small number of questions
available in our train set. Besides, the nature of TF-IDF is
a bag-of-word model, which does not take into account the
overall meaning of the context, so it is barely possible for
TF-IDF to retrieve the appropriate questions. All the sam-
ples in Table 6 show that the retrieved questions are totally

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 9 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Table 5
The evaluation results of models trained on the separate datasets of four programming
languages. All the score numbers are averages over the tested posts of diï¬€erent languages.
The Oracle and TF-IDF models are not aï¬€ected by separated training.

Model

Language BLEUS-4

ROUGE-1

ROUGE-2

ROUGE-L

BiLSTMsep

BiLSTM-CCsep

BARTsep

CCBERTsep

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

14.59
15.93
14.91
13.03

18.22
18.84
19.35
19.08

19.32
20.43
20.19
19.61

20.90
22.02
21.24
21.93

32.13
37.15
33.03
28.41

38.89
41.49
40.92
40.83

42.52
44.93
43.54
43.50

43.06
46.69
44.55
45.60

11.95
14.24
11.63
08.73

18.09
18.98
18.28
18.75

19.93
21.77
20.56
20.79

21.15
22.55
21.05
22.35

31.86
36.55
32.50
27.62

38.21
40.56
40.15
39.72

41.66
43.97
42.70
42.65

41.76
44.86
42.80
43.87

diï¬€erent from the original ones.

(3) BiLSTM-CC and BiLSTM outperform TF-IDF by a
large margin, indicating the superiority of neural generative
models. Besides, BiLSTM-CC outperforms the vanilla BiL-
STM by 11% on average on the joint dataset and by 29% on
average on separated subsets, which proves the eï¬€ectiveness
of the copy and coverage mechanisms. From the samples in
Table 6, we can ï¬nd that BiLSTM often borrows the exact
phrases from question bodies, while BiLSTM-CC can reor-
ganize words into sentences. Despite the good performance
of BiLSTM-CC, our CCBERT model outperforms it by 9%
on average on the joint dataset and by 11% on average on
separated subsets, indicating the superiority of Transformer-
based models and the pre-training strategy. From the gener-
ated samples in Table 6, we can ï¬nd that CCBERT is bet-
ter at handling long-range dependencies than BiLSTM-CC.
For instance, in the ï¬rst sample, CCBERT notices that "this
shape" refers to the "TriangleMesh" that appeared later in the
question body, while BiLSTM-CC tends to focus on the con-
tent at the beginning of the question body. Furthermore, it is
the same for the rest of the samples, where unwanted words
(i.e., "android studio", "spring boot", and "crypto-stock") in
the front of the question body attract more attention from
BiLSTM-CC. At the same time, CCBERT can ï¬nd the criti-
cal words (i.e., "sqlite", "service", and "key-value") that hide
in the middle of the question body.

(4) BART is a competitive model, where CCBERT out-
performs it by 1.3% on average on the joint dataset and by
3.6% on average on separated subsets. According to the sam-
ples in Table 6, BART is good at generating clear and read-
able titles because it is a generation-oriented model that has
been pre-trained on a vast natural language corpus. How-
ever, we can see from the ï¬rst and second samples that titles
generated by BART miss the keywords (i.e., "TriangleMesh"

and "sqlite"), we attribute this problem to BARTâ€™s inferior
understanding of source code. For instance, in the ï¬rst sam-
ple, BART cannot ï¬nd that the "shape" at the beginning refers
to the "TiangleMesh" object declared in the following code
snippet. In the second sample, a major part of the body de-
scribes inserting data into the SQLite database, while BART
only focuses on the unimportant word "android studio". On
the contrary, with the help of bi-modal pre-trained Code-
BERT encoder, our CCBERT model better understands the
source code and generates more semantic titles relevant to
the original ones.

(5) The Oracle model has a surprisingly good perfor-
mance on both subsets, which shows much space for improv-
ing current models. In terms of the recall-oriented ROUGE
metric, the excellent performance of the Oracle model indi-
cates that most tokens in a question title come from the cor-
responding question body. However, our CCBERT model
can only identify a part of the useful tokens in question bod-
ies, leading to a moderate performance on the BLEU metric.
Nevertheless, we can ï¬nd that all the titles generated by the
Oracle model hardly adapt to the grammatical norms from
the generated samples in Table 6, which indicates the ne-
cessity of applying generative models on this task. As for
the reasons of the huge performance diï¬€erence between our
CCBERT model and the Oracle model, we think that our
model may not have well handled the complex long-range
bi-modal contexts, and the personalized writing habits of
question titles also makes it hard for an automatic model to
summarize in the same way as developers do.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 10 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Table 6
The examples of our testing questions and automatically generated titles. Speciï¬cally,
the green color marks the tokens appearing in original titles, the orange-red color marks
the wrong focus, and the gray color marks the code snippets. The models with a ğ‘ğ‘œğ‘‘ğ‘’
subscript in their names are trained on the code-only dataset.

Question Body

Titles

I need to create this shape. I understand how to create
simple shapes such as a cube, but I donâ€™t understand at all
how to create such a shape. How to get the right points for
these arrays? Please, help
TriangleMesh mesh = new TriangleMesh();

mesh.getPoints().addAll(...
mesh.getTexCoords().addAll(...
//which points should be here
mesh.getFaces().addAll(...
//which points should be here
return mesh;

I am a beginner in mobile application building. I tried to
put insert data function in my android studio but those
insert function doesnâ€™t work and the input data canâ€™t be
inserted...
I put code in MainActivity.java and DatabaseHelper.java. It
doesnâ€™t give error report but when run the emulator and
input data, my input can be inserted to sqlite database.
//oncreateMainActivity
super.onCreate(savedInstanceState...
myDb = new DatabaseHelper(...
submit2.setOnClickListener(...

//DatabaseHelper.java
public boolean insertData(String...

SQLiteDatabase db = this.getWritableDatabase(...
long result = db.insert(TABLE_NAME...

I have a simple web application where diï¬€erent users can
log into it...send email of itâ€™s content to an outsider like
third party....With all this, I am using Java Mail API to
make it work and after hitting the send button,it sends
directly to the recipient...Now, I want to modify this by
doing this email feature as a service...the content and info
ï¬lled in will be stored in a table in MYSQL and...
public void sendEmail(String ... {

Properties props = new Properties();
props.put("mail.smtp.host", host); //SMTP...
Authenticator auth = new Authenticator()...
Can this be done in the way...how to make it work?

I want to work with crypto-stock data described here in my
spring boot application. The RESTTemplate uses Gson for
deserialization. Response data looks like:
{"IOST": {"EUR": 0.01147,. . .
I have already...problem is that this comes as a single object
with key-value pairs insted of as an array. The result should
be a list of following objects:
public class Symbol {
private Long id;
private String symbol...

Any idea how this can be accomplished this?

Origin: how to create such shape using javafx trianglemesh
Oracle: how to create such shape trianglemesh
TF-IDF: how update the value in json ï¬le using java jackson
BiLSTM: how to get the right points for arrays
BiLSTM-CC: how to create such a shape in java
BART: how to create such a shape
CCBERT: how to create this shape using trianglemesh

BiLSTM-CCğ‘ğ‘œğ‘‘ğ‘’: how to get points of a mesh
CCBERTğ‘ğ‘œğ‘‘ğ‘’: how to add points to a trianglemesh

Origin: how to insert data to sqlite through user input
Oracle: to insert data to sqlite input
TF-IDF: listview not show items stored in sqlite database
BiLSTM: how to put function in my android studio
BiLSTM-CC: how to insert data function in android studio
BART: how to insert data in android studio
CCBERT: how to insert input data to sqlite database

BiLSTM-CCğ‘ğ‘œğ‘‘ğ‘’: how to add data to an activity in android
CCBERTğ‘ğ‘œğ‘‘ğ‘’: how to get data from database in android

Origin: java - how to use services for sending email
Oracle: java how to sending email
TF-IDF: gae send email from gmail account
BiLSTM: how can i send email to an outsider
BiLSTM-CC: how to send email from database in java
BART: how to send email using java mail api
CCBERT: how to send email using services in java

BiLSTM-CCğ‘ğ‘œğ‘‘ğ‘’: how to set the header of a mail in a mail
CCBERTğ‘ğ‘œğ‘‘ğ‘’: how to send email using java mail

Origin: how to deserialize a key-value map to a list
Oracle: how to a key-value to a list
TF-IDF: bigdecimal not keeping actual value when returned
BiLSTM: how to work with crypto-stock data
BiLSTM-CC: how to parse json data in spring boot
BART: how to deserialize crypto-stock data
CCBERT: how to deserialize key-value pairs

BiLSTM-CCğ‘ğ‘œğ‘‘ğ‘’: how to convert json object to java object
CCBERTğ‘ğ‘œğ‘‘ğ‘’: how to deserialize a json object in java

Answer to RQ-1: Our CCBERT model outperforms
the TF-IDF, BiLSTM, BiLSTM-CC, and BART
models regarding all the automated evaluation met-
rics on both training strategies.

5.2. RQ-2: What is the advantage of using the

bi-modal information of the entire question
body?

Motivation: Although we have illustrated the necessity of
using both text descriptions and code snippets to generate
high-quality question titles, we would like to quantify the im-

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 11 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Table 7
The performance of CCBERT and BiLSTM-CC on the code-only dataset. All the score
numbers are averages over the tested posts of diï¬€erent languages.

Model

Language BLEUS-4

ROUGE-1

ROUGE-2

ROUGE-L

BiSLTM-CCcode

CCBERTcode

Java

Python

JS

PHP

Java

Python

JS

PHP

11.78
13.38
13.02
13.13

12.84
14.03
13.67
14.32

25.04
30.54
28.00
29.25

28.73
33.35
30.57
32.80

7.15
9.78
8.35
9.09

9.62
11.67
10.36
11.90

25.50
30.48
28.18
28.63

28.58
32.67
30.20
31.88

Table 8
The performance of CCBERT and BiLSTM-CC on the datasets with diï¬€erent sizes. All
the score numbers are averages over the tested posts of diï¬€erent languages.

Model

Language BLEUS-4

ROUGE-1

ROUGE-2

ROUGE-L

BiLSTM-CCjoint

BiSLTM-CCjointâˆ•2

BiSLTM-CCjointâˆ•4

BiSLTM-CCjointâˆ•8

CCBERTjoint

CCBERTjointâˆ•2

CCBERTjointâˆ•4

CCBERTjointâˆ•8

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

Java

Python

JS

PHP

19.73
19.74
20.59
20.56

18.99
19.52
19.58
20.06

18.74
19.28
19.43
20.02

17.34
17.61
18.39
18.72

21.16
22.40
22.18
22.65

21.02
21.83
21.89
22.64

20.56
21.22
21.55
22.15

20.45
20.73
20.89
21.95

41.10
42.67
42.62
43.01

39.92
42.00
41.21
41.83

39.73
41.93
40.83
41.39

38.14
39.98
39.60
40.34

44.26
46.88
45.72
47.03

43.78
45.84
44.75
46.21

43.45
45.81
44.68
46.06

43.06
44.42
43.44
45.12

19.54
19.97
20.36
20.92

18.05
19.36
18.76
20.03

17.81
18.82
18.41
19.70

16.50
17.12
17.32
18.20

21.58
22.89
22.40
23.50

20.78
22.21
21.79
23.24

20.52
21.72
21.45
22.54

20.17
20.92
20.36
22.26

40.04
41.72
41.59
41.73

39.20
41.40
40.50
41.05

39.09
41.11
40.06
40.83

37.64
39.52
39.16
39.56

42.92
44.92
44.15
45.15

42.08
44.14
42.85
44.27

41.83
44.02
42.76
44.19

41.45
43.56
41.75
43.56

provement of using the bi-modal information over the code-
only setting in Gao et al.â€™s work [14].
Method: We post-process all question bodies in ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘
to keep code snippets and weed out text descriptions. We
follow the jointly training strategy in this experiment. For

the convenience of comparison, the new code-only dataset
has questions in the same order as the previous joint dataset
during training and testing. We choose BiLSTM-CC and
CCBERT as representatives of our generative models and
show their performance in Table 7.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 12 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Table 9
The performance of CCBERT and BiLSTM-CC trained on ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘+ without the interrog-
ative constraint. All the score numbers are averages over the tested posts of diï¬€erent
languages.

Model

Language BLEUS-4

ROUGE-1

ROUGE-2

ROUGE-L

BiSLTM-CCexp+

CCBERTexp+

Java

Python

JS

PHP

Java

Python

JS

PHP

16.71
17.40
17.78
18.27

18.68
19.18
19.55
20.19

31.79
33.61
33.67
34.32

36.32
37.50
37.81
39.02

14.52
14.70
15.38
15.46

16.43
16.74
17.04
17.05

30.36
31.65
32.16
32.19

33.81
34.77
35.31
35.69

Results: There is a severe decline in the performance of
both models when using only code snippets for training. Specif-
ically, the performance of CCBERT declines by 37% on av-
erage, and BiLSTM-CC drops its performance by 36% on
average. Such results are expected because code snippets
themselves can not oï¬€er suï¬ƒcient context to a question.

ğ‘ğ‘œğ‘‘ğ‘’

ğ‘ğ‘œğ‘‘ğ‘’

and CCBERT

According to the samples in Table 6, it is hard to tell
the corresponding titles of all the four samples given only
code snippets. Therefore, the generated titles may be incom-
plete and incorrect. For instance, in the ï¬rst sample, both
BiLSTM-CC
pay attention to the "Tri-
angleMesh", but neither of them deduces the word "shape"
used in the title. In the second sample, both models fail to
tell that the actual purpose of using "Android activity" and
"SQLite database" is to insert user input data into the SQLite
database.
In the third and fourth samples, although both
models manage to create the words (i.e., "using", "convert",
"deserialize") that are not in the code snippets, there are still
few overlaps between the generated words and the wanted
ones. However, under code-only circumstances and without
changing model structure and hyperparameters, CCBERT
shows the superiority over BiLSTM-CC
dicates its generalization ability on diï¬€erent tasks.

ğ‘ğ‘œğ‘‘ğ‘’
, which also in-

ğ‘ğ‘œğ‘‘ğ‘’

Answer to RQ-2: Applying bi-modal information
greatly boosts both modelsâ€™ performance, where
CCBERT still outperforms BiLSTM-CC.

three fractions as the percentage of samples to erase, which
are 1/2, 3/4, and 7/8. This makes three new train sets sized of
98,909 (Dataexpâˆ•2), 49,454 (Dataexpâˆ•4), and 24,727 (Dataexpâˆ•8).
Along with the CCBERT model, we also train BiLSTM-CC
on these datasets for comparison. Table 8 presents the ex-
perimental results on the datasets with diï¬€erent sizes.
Results: It is as expected that both models have suï¬€ered per-
formance degradation on smaller datasets. To our surprise,
even if the amount of data decreases exponentially, the per-
formance has a steady decline. Speciï¬cally, the performance
of CCBERT declines by 2% on average on the Dataexpâˆ•2
subset, by 2.8% on the Dataexpâˆ•4 subset, and by 4.8% on the
Dataexpâˆ•8 subset; while BiLSTM-CC drops its performance
by 3% on average on the Dataexpâˆ•2 subset, by 3.8% on the
Dataexpâˆ•4 subset, and by 8.2% on the Dataexpâˆ•8 subset. It
indicates that our task is not so sensitive to the data volume
and further veriï¬es the existence of a writing pattern shared
by high-quality questions. Meanwhile, developers have per-
sonalized writing habits, so a more considerable amount of
data can help eliminate such noise and improve the perfor-
mance of our model. Facilitated by the pre-trained Code-
BERT encoder, our CCBERT model is better initialized to
resist data noise and requires fewer data. We can see from
the results that with only one-eighth of the data, CCBERT
still outperforms BiLSTM-CC trained on the full dataset.

Answer to RQ-3: Compared to BiLSTM-CC, our
CCBERT model shows signiï¬cant superiority under
low-resource circumstances.

5.3. RQ-3: How eï¬€ective is our CCBERT model

under low-resource circumstances?

Motivation: Data-hungry is a common issue in the ï¬eld of
deep learning, which signiï¬cantly hinders the application
of many excellent models. Our model may need to train
on massive high-quality questions. Since previous experi-
ments have proved that CCBERT can handle the situation
with around 200,000 samples for training, we carry out this
experiment to discuss the eï¬€ectiveness of our model under
low-resource circumstances.
Method: We ï¬rst make several copies of ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘
, and then
randomly erase a certain amount of questions in the train
sets, leaving the validation and test sets untouched. We choose

5.4. RQ-4: How much inï¬‚uence does interrogative

constraint have on model training?

Motivation: Applying the interrogative constraint may re-
duce the data noise and make it easier to train our models.
Nevertheless, our dataset could be narrowed and biased be-
cause of this. So we carry out this experiment to investi-
gate the actual inï¬‚uence of interrogative constraint on our
modelâ€™s performance.
in the same way
Method: We ï¬rst build a dataset ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘+
as building ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘
, except for applying the interrogative
constraint. This way, we get a much larger dataset. Then, we

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 13 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Table 10
The partition size of ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘+

Language

Train

Validation

Test

Java
Python
JS
PHP

183,443
207,323
174,374
104,227

7,000
7,000
7,000
4,000

7,000
7,000
7,000
4,000

Total

669,367

25,000

25,000

is shown in Table 10.

train and evaluate both BiLSTM-CC and CCBERT models
on the new dataset, following the jointly training strategy.
The automated evaluation results are shown in Table 9. The
statistics of ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘+
Results: We believe that a questionâ€™s popularity does not
necessarily attribute to a uniï¬ed title format. Many other
aspects like the clarity of question description and the popu-
larity of the asked domain will inï¬‚uence the popularity of a
question. We have manually studied the questions that we re-
gard as high-quality ones and have no interrogatives in their
titles. We ï¬nd that those titles are always casually written.
For example, a title can be a combination of keywords,22 a
short phrase,23 an error message,24 etc.

According to Table 9, both models have poor perfor-
mance on ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘+
, despite having so much data for train-
ing. Speciï¬cally, the CCBERT model has an average 19.4%
, while it is 21.4%
lower evaluation score than using ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘
for BiLSTM-CC. But in this experiment, our CCBERT model
still outperforms BiLSTM-CC by 11.6% on average, which
veriï¬es the superiority of our model under diï¬€erent circum-
stances.

Answer to RQ-4: Dropping the interrogative con-
straint will lead to a decline in the performance of
both CCBERT and BiLSTM-CC models.

5.5. RQ-5: How eï¬€ective is our CCBERT model

under human evaluation?

Motivation: Automated evaluation is not always trustwor-
thy because it is hard to decide the actual human-perceived
quality in diï¬€erent situations. The BLEU and ROUGE met-
rics used in this study mainly focus on the lexical ngram
overlap between text sequences ignoring the grammatical
correctness and the semantic similarity. So in this experi-
ment, we will perform a more human-centered evaluation to
investigate the overall quality of the titles generated by our
models.
Method: We consider the two criteria introduced in Table
1 when manually evaluating the generated titles, either of
them can be scored between 1 and 4. We randomly sam-
and then obtain
ple 500 questions in the test set of ğ·ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘

22

23

https://stackoverflow.com/questions/53279561/java-month-enum
https://stackoverflow.com/questions/53218222/

capture-logs-in-a-test

24

https://stackoverflow.com/questions/53344676/

, and
1,500 titles generated by the TF-IDF, BiLSTM-CCjoint
models. We invite ï¬ve graduate students who
CCBERTjoint
are not co-authors to help us with this experiment. They are
all experienced programmers and familiar with Stack Over-
ï¬‚ow. Each participant is assigned 100 questions, and we at-
tach each question with three generated titles. The partici-
pants need to rate the titles based on the scoring standards
manually, and they are blinded as to which title is generated
by our model. The evaluation results are shown in Table 11.
In terms of the readability criteria, the perfor-
Results:
mance of the three models is evenly matched. As expected,
TF-IDF achieves the highest score because it retrieves and
returns the titles written by developers. Our CCBERT model
outperforms BiLSTM-CC by only 1.7% and is only 1.5% less
good than TF-IDF. To conclude, most of the generated titles
by our CCBERT model are regarded as easy to read and un-
derstand.

Regarding the correlation criteria, both the CCBERT and
BiLSTM-CC models outperform TF-IDF by a large mar-
gin. It shows the superiority of generative models over the
retrieval-based method on semantic understanding. The per-
formance of our CCBERT model is 16.5% better than BiLSTM-
CC, which is not surprising because attributed to the pre-
trained CodeBERT encoder, our model is more capable of
handling long-range dependencies in bi-modal content. How-
ever, according to Table 11, only less than a half of the gen-
erated titles of CCBERT are considered well matching the
questions. It suggests that there is still much room for im-
provements in our approach.

Answer to RQ-5: CCBERT performs much better
than TF-IDF and BiLSTM-CC concerning the corre-
lation criteria but a little bit worse than human writ-
ten titles retrieved by TF-IDF concerning the read-
ability criteria under human evaluation.

6. Related Work

Since we treat our TGEQB task as abstractive summa-
rization, we take the related works in text summarization and
code summarization for reference. We brieï¬‚y introduce the
recent literature in this section.

6.1. Text Summarization

There are extractive and abstractive ways for summariza-
tion tasks. Both ways have been attracting extensive research
interest.

The extractive models select sentences or paragraphs from
source texts to best match the target summary. The idea of
using a hierarchical encoder and an extractor for document
summarization was proposed by Cheng et al. [8]. Later, re-
searchers have proposed various solutions to deal with diï¬€er-
ent detailed problems. For example, Zhou et al. [57] argued
that one should not separate the sentence scoring and selec-
tion steps, so they proposed an integrated model to merge

java-lang-illegalstateexception-inputstream-has-already-been-read-do-not-use

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 14 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

Table 11
Human evaluation results of the TF-IDF, BiLSTM-CC, and CCBERT models trained on
the joint dataset of four languages. The ratios of four diï¬€erent scores and the average
score are listed grouped by diï¬€erent criteria.

Criteria

Model

Score 1

Score 2

Score 3

Score 4 Avg Score

Readability

Correlation

TF-IDF
BiLSTM-CCjoint
CCBERTjoint

TF-IDF
BiLSTM-CCjoint
CCBERTjoint

-
-
-

91.6%
27.6%
16.8%

16.6%
23.6%
19.2%

8.4%
55.4%
47.4%

81.2%
76.2%
80.2%

-
14.2%
30.8%

2.2%
0.2%
0.6%

-
2.8%
5%

2.856
2.766
2.814

1.084
1.922
2.240

the two steps. Xu et al. [48] argued that BERT-based mod-
els could not capture dependencies among discourse units,
which leads to the problem of having unwanted phrases in
extracted summaries. To tackle this problem, they proposed
to encode the rhetorical structure theory trees with a graph
convolutional network. Jia et al. [20] argued that BERT-
based models neglect the inherent dependencies among ref-
erence sentences, and they proposed to reï¬ne the sentence
representations with a redundancy aware graph attention net-
work. These novel models performed well on semantic pars-
ing. However, our task requires the model to give readable
titles, where the extractive ways have been proved unwork-
able on our dataset (reference the Oracle methodâ€™s perfor-
mance in Table 6).

In general, abstractive models are not restricted to se-
lecting and rearranging the original text but to generating
each word from a given vocabulary. See et al. [37] argued
that vanilla attentional sequence-to-sequence models always
produce inaccurate factual details and duplicate phrases. So
they proposed to use a hybrid generator incorporating both
the copy and coverage mechanisms. Gehrmann et al. [15]
found the problem that abstractive models were poor at con-
Instead of adding fancy mechanisms, they
tent selection.
proposed a two-stage process to train an extractor and then
use it as bottom-up attention to guide the generator. Liu et al.
[27] extended this idea by using a two-stage ï¬ne-tuning on
both extraction and generation tasks. In addition, they pro-
posed to use diï¬€erent optimizers for the encoder and decoder
to alleviate mismatch brought by diï¬€erent objectives. Lewis
et al.
[24] proposed BART. This generative-oriented pre-
training model has achieved excellent performance in ab-
stractive summarization tasks, so we choose it as a baseline
model to compare with ours. Abstractive summarization is
similar to our task in many aspects. However, our dataset is
more challenging due to the complex bi-modal context and
the diï¬ƒcult rare tokens, which is why we adopt the Code-
BERT model and the copy mechanism.

6.2. Code Summarization

Code summarization aims to generate readable and mean-
ingful comments that accurately describe the given programs
or subroutines, which is very useful for code search and com-
prehension.

One way to deal with source code is to treat it as a se-

Iyer et al.

quence.
[19] ï¬rst proposed to use attentional
LSTMs to produce summaries describing code snippets, and
they released their training corpus. Hu et al. [18] further
looked into the possibility of using API knowledge to gener-
ate comments that better describe the functionality of source
code. Wei et al. [46] proposed to use comments of existing
similar source code to guide new comment generation. Wei
et al. [47] exploited the relations between code summariza-
tion and code generation, and proposed a dual framework to
train the two tasks simultaneously. The experimental results
showed that performance improvements were achieved on
both tasks. Hu et al. [17] argued that code tokens should not
be processed sequentially. Hence, they proposed an abstract
syntax tree-based structural code representation and veriï¬ed
its eï¬€ectiveness in generating code comments. Ahmad et
al. [1] ï¬rst introduced the Transformer model to this task.
They proposed to use a pairwise position encoding to cap-
ture the long-range dependencies among code tokens. The
above approaches treated source code as text sequences, but
they also valued the particular information hidden behind the
code. Their experiments convinced us that the programming
language is diï¬€erent from the natural language.

The other way is to convert the source code into other
forms of representation. Wan et al.
[43] used a sequen-
tial encoder as well as a tree-based encoder to capture the
general information from code. They also applied an actor-
critic network to overcome the exposure bias issue of the
auto-regressive decoder. LeClair et al. [23] also used dual
encoders and incorporated the copy mechanism to reserve
necessary tokens reported by the AST analyzer. Besides,
they [22] further proposed to use a graph-based neural ar-
chitecture that achieved even better performance. Yang et
al. [49] employed a sequential encoder and a graph-based
encoder to learn the global and local semantic information
to generate code comments of smart contracts. The studies
mentioned above show that source code needs technological
transformation for models to extract the semantic informa-
tion for summarization. Unfortunately, we ï¬nd that content
marked with the "<code>" tag in our ï¬ltered SO questions
are not always syntactically correct source code. Therefore,
we treat the code snippets as a sequence of tokens and use
CodeBERT as the encoder, which is code-aware and takes
sequences as input.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 15 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

7. Threats to Validity

In this section, we identify the potential threats that might
aï¬€ect the recurrence of our experiments and the validation
of our results.

The threats to internal validity concern us in two as-
pects, one is the re-implementation of baselines, the other
is the design of the CCBERT model. To address the ï¬rst
issue, we rebuild the default development environment and
choose the recommended settings for baseline models. As
for the second issue, we have made trade-oï¬€s between diï¬€er-
ent techniques. For example, we give up using the coverage
mechanism because it is incompatible with the parallel de-
coding fashion of our Transformer decoder. Further experi-
ments also show that our model is not troubled by the repe-
tition problem. Besides, training the large CCBERT model
with insuï¬ƒcient data may cause the overï¬tting issue. To ad-
dress this issue, our CodeBERT encoder has initialized its
parameters through self-supervised learning with massive
data in the pre-training stage. Furthermore, in our experi-
ments, we train our model with a small learning rate, which
also helps alleviate the problem of overï¬tting.

The threats to external validity primarily relate to the
quality and generalizability of our dataset. We notice that
the SOTorrent dataset proposed by Baltes et al. [4] shares a
lot in common with ours. SOTorrent aims to provide access
to the version history of SO content, which involves legacy
formats issues and contains a lot of duplicate posts. We build
our own dataset, because ours is directly extracted from the
latest version of SO posts, which has a uniï¬ed HTML-style
format and can be easily parsed into text/code blocks by a
naive HTML parser. There may be a deviation between our
dataset and the realistic data. To make our dataset more re-
alistic, we build it concerning four programming languages,
apply two training strategies for comparison, and choose the
lately posted questions for testing.

The threats to construct validity mainly relate to the
evaluation measures. Though the BLEU and ROUGE met-
rics have been widely used, automated evaluation is still an
open problem in the domain of text generation [38, 51, 12].
We perform a human-centered evaluation in terms of the
readability and correlation criteria to address this issue.

8. Conclusion and Future Work

In this paper, we propose a new task to summarize ques-
tion titles from bi-modal context and a novel model named
CCBERT to tackle this problem. CCBERT incorporates the
copy mechanism and the CodeBERT model, which can han-
dle rare tokens and capture the long-range dependencies be-
tween bi-modal tokens. We build a large-scale dataset with
suï¬ƒcient high-quality questions concerning four program-
ming languages. We choose the BLEU and ROUGE met-
rics for automated evaluation and various baseline models
for comparison. Both automated and human evaluation re-
sults demonstrate the superiority of our model. We have re-
leased our dataset and source code for follow-up researches.
For future work, we will try to tackle the problem of han-

dling very long sequences. In addition, we consider using
Incremental Learning techniques to make our model contin-
uously learn new knowledge from new samples and retain
most of the knowledge already learned.

Acknowledgment

This work is supported in part by the Natural Science
Foundation of Chongqing City, China (No. cstc2021jcyj-
msxmX1115), the Fundamental Research Funds for the Cen-
tral Universities (WUT: 213110003 and 223110002), the project
supported by Sanya Science and Education Innovation Park
of Wuhan University of Technology (No. 2020KF0059), the
General Research Fund of the Research Grants Council of
Hong Kong (No. 11208017), the research funds of City Uni-
versity of Hong Kong (No. 7005028 and 7005217), and the
Research Support Fund by Intel, USA (No. 9220097), and
funding supports from other industry partners (No. 9678149,
9440227, 9229029, 9440180, and 9220103). We thank Guop-
ing Nie for his helpful comments about this work.

References
[1] Ahmad, W.U., Chakraborty, S., Ray, B., Chang, K.W., 2020. A
transformer-based approach for source code summarization, in: ACL.
[2] Arora, P., Ganguly, D., Jones, G.J., 2015. The good, the bad and
their kins: Identifying questions with negative scores in stackover-
ï¬‚ow, in: 2015 IEEE/ACM International Conference on Advances in
Social Networks Analysis and Mining (ASONAM), IEEE. pp. 1232â€“
1239.

[3] Bahdanau, D., Cho, K., Bengio, Y., 2015. Neural machine translation
by jointly learning to align and translate. CoRR abs/1409.0473.
[4] Baltes, S., Dumani, L., Treude, C., Diehl, S., 2018. Sotorrent: Re-
constructing and analyzing the evolution of stack overï¬‚ow posts.
2018 IEEE/ACM 15th International Conference on Mining Software
Repositories (MSR) , 319â€“330.

[5] Beltagy, I., Peters, M.E., Cohan, A., 2020. Longformer: The long-

document transformer. ArXiv abs/2004.05150.

[6] Calefato, F., Lanubile, F., Novielli, N., 2018. How to ask for tech-
nical help? evidence-based guidelines for writing questions on stack
overï¬‚ow. Information and Software Technology 94, 186â€“207.
[7] Chakraborty, P., Shahriyar, R., Iqbal, A., Uddin, G., 2021. How do
developers discuss and support new programming languages in tech-
nical q&a site? an empirical study of go, swift, and rust in stack over-
ï¬‚ow. Information and Software Technology 137, 106603.

[8] Cheng, J., Lapata, M., 2016. Neural summarization by extracting

sentences and words. ArXiv abs/1603.07252.

[9] Correa, D., Sureka, A., 2013. Fit or unï¬t: analysis and prediction
ofâ€™closed questionsâ€™ on stack overï¬‚ow, in: Proceedings of the ï¬rst
ACM conference on Online social networks, pp. 201â€“212.

[10] Dai, Z., Yang, Z., Yang, Y., Carbonell, J.G., Le, Q.V., Salakhutdinov,
R., 2019. Transformer-xl: Attentive language models beyond a ï¬xed-
length context. ArXiv abs/1901.02860.

[11] Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. Bert: Pre-
training of deep bidirectional transformers for language understand-
ing, in: NAACL.

[12] Fabbri, A.R., Kryscinski, W., McCann, B., Socher, R., Radev, D.,
2021. Summeval: Re-evaluating summarization evaluation. Transac-
tions of the Association for Computational Linguistics 9, 391â€“409.

[13] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L.,
Qin, B., Liu, T., Jiang, D., Zhou, M., 2020. Codebert: A pre-trained
model for programming and natural languages, in: FINDINGS.
[14] Gao, Z., Xia, X., Grundy, J., Lo, D., Li, Y.F., 2020. Generating ques-
tion titles for stack overï¬‚ow from mined code snippets. ACM Transac-
tions on Software Engineering and Methodology (TOSEM) 29, 1â€“37.

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 16 of 17

Improving Stack Overï¬‚ow question title generation with copying enhanced CodeBERT model and bi-modal information

[15] Gehrmann, S., Deng, Y., Rush, A.M., 2018. Bottom-up abstractive

125, 106333.

summarization, in: EMNLP.

[16] Gu, J., Lu, Z., Li, H., Li, V., 2016. Incorporating copying mechanism

in sequence-to-sequence learning. ArXiv abs/1603.06393.

[17] Hu, X., Li, G., Xia, X., Lo, D., Jin, Z., 2018a. Deep code comment
generation. 2018 IEEE/ACM 26th International Conference on Pro-
gram Comprehension (ICPC) , 200â€“20010.

[18] Hu, X., Li, G., Xia, X., Lo, D., Lu, S., Jin, Z., 2018b. Summarizing

source code with transferred api knowledge, in: IJCAI.

[19] Iyer, S., Konstas, I., Cheung, A., Zettlemoyer, L., 2016. Summarizing

source code using a neural attention model, in: ACL.

[20] Jia, R., Cao, Y., Tang, H., Fang, F., Cao, C., Wang, S., 2020. Neural
extractive summarization with hierarchical attentive heterogeneous
graph network, in: EMNLP.

[21] Khandelwal, U., He, H., Qi, P., Jurafsky, D., 2018. Sharp nearby,
fuzzy far away: How neural language models use context, in: Pro-
ceedings of the 56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pp. 284â€“294.

[22] LeClair, A., Haque, S., Wu, L., McMillan, C., 2020. Improved code
summarization via a graph neural network. Proceedings of the 28th
International Conference on Program Comprehension .

[23] LeClair, A., Jiang, S., McMillan, C., 2019. A neural model for gen-
erating natural language summaries of program subroutines. 2019
IEEE/ACM 41st International Conference on Software Engineering
(ICSE) , 795â€“806.

[24] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A.,
Levy, O., Stoyanov, V., Zettlemoyer, L., 2020. Bart: Denoising
sequence-to-sequence pre-training for natural language generation,
translation, and comprehension, in: ACL.

[25] Lin, C.Y., 2004. Rouge: A package for automatic evaluation of sum-

maries, in: ACL 2004.

[26] Lin, C.Y., Och, F., 2004. Orange: a method for evaluating automatic

evaluation metrics for machine translation, in: COLING.

[27] Liu, Y., Lapata, M., 2019. Text summarization with pretrained en-

coders, in: EMNLP/IJCNLP.

[40] Tu, Z., Lu, Z., Liu, Y., Liu, X., Li, H., 2016. Modeling coverage for
neural machine translation. arXiv: Computation and Language .
[41] Uddin, G., Khomh, F., Roy, C., 2020. Mining api usage scenar-
ios from stack overï¬‚ow. Information and Software Technology 122,
106277.

[42] Vaswani, A., Shazeer, N.M., Parmar, N., Uszkoreit, J., Jones, L.,
Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention is all you
need. ArXiv abs/1706.03762.

[43] Wan, Y., Zhao, Z., Yang, M., Xu, G., Ying, H., Wu, J., Yu, P.S., 2018.
Improving automatic source code summarization via deep reinforce-
ment learning. 2018 33rd IEEE/ACM International Conference on
Automated Software Engineering (ASE) , 397â€“407.

[44] Wang, H., Wang, B., Li, C., Xu, L., He, J., Yang, M., 2019. Sota-
grec: A combined tag recommendation approach for stack overï¬‚ow,
in: Proceedings of the 2019 4th International Conference on Mathe-
matics and Artiï¬cial Intelligence, pp. 146â€“152.

[45] Wang, S., Chen, T.H., Hassan, A.E., 2018. How do users revise an-
swers on technical q&a websites? a case study on stack overï¬‚ow.
IEEE Transactions on Software Engineering 46, 1024â€“1038.
[46] Wei, B., 2019. Retrieve and reï¬ne: Exemplar-based neural comment
generation. 2019 34th IEEE/ACM International Conference on Au-
tomated Software Engineering (ASE) , 1250â€“1252.

[47] Wei, B., Li, G., Xia, X., Fu, Z., Jin, Z., 2019. Code generation as a

dual task of code summarization. ArXiv abs/1910.05923.

[48] Xu, J., Gan, Z., Cheng, Y., Liu, J., 2020. Discourse-aware neural

extractive text summarization, in: ACL.

[49] Yang, Z., Keung, J., Yu, X., Gu, X., Wei, Z., Ma, X., Zhang, M., 2021.
A multi-modal transformer-based code summarization approach for
smart contracts, in: 29th IEEE/ACM International Conference on
Program Comprehension, ICPC 2021, Madrid, Spain, May 20-21,
2021, IEEE. pp. 1â€“12.

[50] Yao, Y., Tong, H., Xie, T., Akoglu, L., Xu, F., Lu, J., 2013.
arXiv preprint

Want a good answer? ask a good question ï¬rst!
arXiv:1311.6876 .

[28] Luhn, H.P., 1958. The automatic creation of literature abstracts. IBM

[51] Yeh, Y.T., EskÃ©nazi, M., Mehri, S., 2021. A comprehensive assess-

ment of dialog evaluation metrics. ArXiv abs/2106.03706.

[52] Zaheer, M., Guruganesh, G., Dubey, K.A., Ainslie, J., Alberti, C., On-
taÃ±Ã³n, S., Pham, P., Ravula, A., Wang, Q., Yang, L., Ahmed, A., 2020.
Big bird: Transformers for longer sequences. ArXiv abs/2007.14062.
[53] Zhao, K., Liu, J., Xu, Z., Li, L., Yan, M., Yu, J., Zhou, Y., 2021a.
Predicting crash fault residence via simpliï¬ed deep forest based on a
reduced feature set. 2021 IEEE/ACM 29th International Conference
on Program Comprehension (ICPC) , 242â€“252.

[54] Zhao, K., Liu, J., Xu, Z., Liu, X., Xue, L., Xie, Z., Zhou, Y., Wang, X.,
2022. Graph4web: A relation-aware graph attention network for web
service classiï¬cation. Journal of Systems and Software 190, 111324.
[55] Zhao, K., Xu, Z., Yan, M., Zhang, T., Yang, D., Li, W., 2021b. A
comprehensive investigation of the impact of feature selection tech-
niques on crashing fault residence prediction models. Information and
Software Technology 139, 106652.

[56] Zhao, K., Xu, Z., Zhang, T., Tang, Y., 2020. Simpliï¬ed deep forest
model based just-in-time defect prediction for android mobile apps.
2020 IEEE 20th International Conference on Software Quality, Reli-
ability and Security (QRS) , 222â€“222.

[57] Zhou, Q., Yang, N., Wei, F., Huang, S., Zhou, M., Zhao, T., 2018.
Neural document summarization by jointly learning to score and se-
lect sentences, in: ACL.

J. Res. Dev. 2, 159â€“165.

[29] Mashhadi, E., Hemmati, H., 2021. Applying codebert for automated
program repair of java simple bugs. 2021 IEEE/ACM 18th Inter-
national Conference on Mining Software Repositories (MSR) , 505â€“
509.

[30] Mondal, S., Saifullah, C.K., Bhattacharjee, A., Rahman, M.M., Roy,
C.K., 2021. Early detection and guidelines to improve unanswered
questions on stack overï¬‚ow, in: 14th Innovations in Software Engi-
neering Conference (formerly known as India Software Engineering
Conference), pp. 1â€“11.

[31] Montandon, J.E., Politowski, C., Silva, L.L., Valente, M.T., Petrillo,
F., GuÃ©hÃ©neuc, Y.G., 2021. What skills do it companies look for in
new developers? a study with stack overï¬‚ow jobs. Information and
Software Technology 129, 106429.

[32] Pan, C., Lu, M., Xu, B., 2021. An empirical study on software defect
prediction using codebert model. Applied Sciences 11, 4793.
[33] Papineni, K., Roukos, S., Ward, T., Zhu, W.J., 2002. Bleu: a method

for automatic evaluation of machine translation, in: ACL.

[34] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.,
2019. Language models are unsupervised multitask learners.
[35] Rubei, R., Sipio, C.D., Nguyen, P.T., Rocco, J.D., Ruscio, D.D., 2020.
Postï¬nder: Mining stack overï¬‚ow posts to support software develop-
ers. Information and Software Technology 127, 106367.

[36] Schuster, M., Paliwal, K.K., 1997. Bidirectional recurrent neural net-

works. IEEE Trans. Signal Process. 45, 2673â€“2681.

[37] See, A., Liu, P.J., Manning, C.D., 2017. Get to the point: Summa-

rization with pointer-generator networks, in: ACL.

[38] Sellam, T., Das, D., Parikh, A.P., 2020. Bleurt: Learning robust met-

rics for text generation, in: ACL.

[39] Tahir, A., Dietrich, J., Counsell, S., Licorish, S., Yamashita, A., 2020.
A large scale study on how developers discuss code smells and anti-
pattern in stack exchange sites. Information and Software Technology

Fengji Zhang et al.: Preprint submitted to Elsevier

Page 17 of 17

