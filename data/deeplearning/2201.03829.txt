2
2
0
2

n
a
J

1
1

]
L
C
.
s
c
[

1
v
9
2
8
3
0
.
1
0
2
2
:
v
i
X
r
a

Quantifying Robustness to Adversarial Word Substitutions

Yuting Yang,1,2* Pei Huang,2,3* FeiFei Ma,2,3,4† Juan Cao1,2 Meishan Zhang 5 Jian Zhang2,3†
Jintao Li 1
1 Key Lab of Intelligent Information Processing,
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China
3State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of Sciences (ISCAS), Beijing, 100190, China
4 Laboratory of Parallel Software and Computational Science, ISCAS, Beijing, 100190, China
5 Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China yangyuting@ict.ac.cn,
{huangpei, maff}@ios.ac.cn, caojuan@ict.ac.cn,
mason.zms@gmail.com, zj@ios.ac.cn, jtli@ict.ac.cn

Abstract

Deep-learning-based NLP models are found to be vulnera-
ble to word substitution perturbations. Before they are widely
adopted, the fundamental issues of robustness need to be ad-
dressed. Along this line, we propose a formal framework to
evaluate word-level robustness. First, to study safe regions for
a model, we introduce robustness radius which is the bound-
ary where the model can resist any perturbation. As calculat-
ing the maximum robustness radius is computationally hard,
we estimate its upper and lower bound. We repurpose at-
tack methods as ways of seeking upper bound and design a
pseudo-dynamic programming algorithm for a tighter upper
bound. Then veriﬁcation method is utilized for a lower bound.
Further, for evaluating the robustness of regions outside a safe
radius, we reexamine robustness from another view: quantiﬁ-
cation. A robustness metric with a rigorous statistical guaran-
tee is introduced to measure the quantiﬁcation of adversarial
examples, which indicates the model’s susceptibility to per-
turbations outside the safe radius. The metric helps us ﬁg-
ure out why state-of-the-art models like BERT can be easily
fooled by a few word substitutions, but generalize well in the
presence of real-world noises.

Introduction
Deep learning models have achieved impressive improve-
ments on various NLP tasks. However, they are found to
be vulnerable to input perturbations, such as paraphrasing
(Ribeiro, Singh, and Guestrin 2018), inserting character (Be-
linkov and Bisk 2018) and replacing words with similar ones
(Ren et al. 2019). In this paper, we focus on word substitu-
tion perturbation (Jin et al. 2020; Neekhara et al. 2019; Zang
et al. 2020) as shown in Figure 1, in which the output of
a model can be altered by replacing some words in the in-
put sentence while maintaining the semantics. Before deep
learning models are widely adopted in practice, understand-
ing their robustness to word substitution is critical.

In recent years, several studies focus on generating adver-
sarial examples (Jin et al. 2020; Wang et al. 2021) or certify

*These authors contributed equally.
†Corresponding Authors

This work ﬁnished in Jan, 2021 and is delayed to be made public
in Jan, 2022 due to review cycle.

Figure 1: An example of word substitution perturbation.

Figure 2: Diagram of our methods for evaluating robustness:
(1) evaluating maximum safe radius R via obtaining its up-
per and lower bound; (2) measuring the model’s vulnerabil-
ity outside the safe radius via a robustness metric P R.

the absence of adversarial examples in the whole perturba-
tion space (Jia et al. 2019; Huang et al. 2019a; Ye, Gong,
and Liu 2020). However, almost all current deep learning
models are unable to be regarded as absolutely robust un-
der such a yes-or-no binary judgment. Along this line, some
deeper questions can be asked. Where is the safe boundary
of a model to resist perturbation? Why can a well-trained
NLP model be fooled by small perturbations but generalize
well to real-world inputs with noises? Does the existence of
an adversarial example in the exponential input space com-
pletely destroy the defense capability of the model?

It’s   the     best      movie   I   have  ever    seenbestfinestnicestgreatestmoviefilmseenwatchedlookedviewedIt’s   the  greatestfilmI   have  ever  watched𝑤𝑤1𝑤𝑤2𝑤𝑤3𝑤𝑤4𝑤𝑤5𝑤𝑤6𝑤𝑤7𝑤𝑤8𝑝𝑝1=3𝑝𝑝2=4𝑝𝑝3=8𝑆𝑆(𝑋𝑋,𝑝𝑝1)Input  𝑋𝑋AdversarialExample𝑋𝑋′PerturbationSpaceΩ(𝑋𝑋)𝑦𝑦∗𝐹𝐹(𝑋𝑋′)≠𝑤𝑤1′𝑤𝑤2′𝑤𝑤3′𝑤𝑤4′𝑤𝑤5′𝑤𝑤6′𝑤𝑤7′𝑤𝑤8′𝑆𝑆(𝑋𝑋,𝑝𝑝2)𝑆𝑆(𝑋𝑋,𝑝𝑝3) 
 
 
 
 
 
To answer these questions more comprehensively, we pro-
pose a formal framework for evaluating models’ robustness
to word substitution from the view of quantiﬁcation. We
quantify the magnitude of the perturbation (or the number
of substitutions) a model can resist. Figure 2 visualizes the
problems we study in this paper. Robustness radius (safe ra-
dius) r, which is deﬁned as the magnitude of the perturba-
tion space where no adversarial examples exist, is useful for
studying the safe regions of models. In particular, the maxi-
mum robustness radius R depicts the boundary of perturba-
tions a model can resist. Apart from safe regions, the vulner-
ability outside safe regions also needs to be evaluated as it
can inﬂuence the model’s performance in practice. A natural
idea is to quantify the number of adversarial examples for a
given radius as a metric for robustness.

The main challenge of the evaluation framework is that
the perturbation space can be exponentially large, so solv-
ing these problems exactly is not feasible in many cases. To
overcome this problem, we retreat from the exact computa-
tion of R to estimate its upper and lower bounds. An adver-
sarial example with fewer substitutions can provide a tighter
upper bound for R. Therefore, we repurpose attack methods
for evaluating upper bound and design an algorithm called
pseudo-dynamic programming (PDP) to craft adversarial ex-
amples with as few substitutions as possible. Then, for the
lower bound, we ﬁnd that certifying word-level robustness
with a ﬁxed radius can be solved in polynomial time. So we
use veriﬁcation methods to give a lower bound. Finally, we
introduce a robustness metric P R which denotes the number
of adversarial examples for a given radius. It can provide a
quantitative indicator for the models’ robustness outside the
absolute safe radius. As it is a more difﬁcult problem than
calculating the maximum safe radius, we estimate the value
of P R with a rigorous statistical guarantee.

We design experiments on two important NLP tasks (text
classiﬁcation and textual entailment) and two models (BiL-
STM and BERT) to study our methods empirically. Experi-
ments show that PDP algorithm has a stronger search capa-
bility to provide a tighter upper bound for the maximum ro-
bustness radius. The robustness metric P R results present an
interesting phenomenon: although most well-trained mod-
els can be attacked by a few word substitutions with a high
success rate, the word-substitution-based adversarial exam-
ples distribute widely in perturbation space but just occupy
a small proportion. For example, BERT can be successfully
(> 89.7%) attacked by manipulating 4.5 words on average
on IMDB. However, more than 90.66% regions can resist
random word perturbations with a high probability (> 0.9).
We conclude that some adversarial examples may be essen-
tially on-manifold generalization errors, which can explain
the reason why these “vulnerable” models can generalize
well in practice.

Preliminary
Given a natural language classiﬁer F : X → Y, which
is a mapping from an input space to an output
label
space. The input space X contains all possible texts X =
w1, w2, ..., wn and output space Y = {y1, y2, ..., yc} con-
tains c possible predictions of an input. wi is usually a word

embedding or one-hot vector. Fy(·) is the prediction score
for the y label. Let P = {p1, p2, ..., pm} be the set of per-
turbable positions. For each perturbable position p ∈ P ,
there is a set S(X, p) which contains all candidate words
for substitution without changing the semantics (the original
word wp is also in S(X, p)). Figure 2 is a schematic diagram
and contains explanations of some notations.
Deﬁnition 1 (Adversarial Example). Consider a classiﬁer
F (x). Given a sequence X with gold label y∗ and X (cid:48) =
w(cid:48)
n which is a text generated by perturbing X,
X (cid:48) is said to be an adversarial example if:

2, ..., w(cid:48)

1, w(cid:48)

F (X (cid:48)) (cid:54)= y∗

(1)

Deﬁnition 2. A perturbation space Ω(X) of an input se-
quence X is a set containing all perturbations generated
by substituting the original word by candidate words in
S(X, p) for each perturbable position p ∈ P.
p∈P |S(X, p)|.

The cardinality of Ω(X) is (cid:81)

Deﬁnition 3 (Word-level Robustness). Consider a classiﬁer
F (x). Given a sequence X with gold label y∗, classiﬁer F
is said to be robust in the perturbation space Ω(X) if the
following formula holds:

∀X (cid:48). X (cid:48) ∈ Ω(X) ⇒ F (X (cid:48)) = y∗

(2)

If a classiﬁer is not robust in Ω(X), we also want to know
what maximum perturbation it can resist. We use L0 dis-
tance r to describe the degree of perturbation, which is also
called robustness radius or safe radius. The maximum ro-
bustness radius is denoted as R.
Deﬁnition 4 (Word-level L0-Robustness). Consider a clas-
siﬁer F (x). Given an L0 distance r and a sequence X
with gold label y∗. Let Ωr(X) := {X (cid:48) : X (cid:48) ∈ Ω(X) ∧
(cid:107)X (cid:48) − X(cid:107)0 ≤ r} and (cid:107)·(cid:107)0 denote the number of substituted
words. The classiﬁer F is said to be robust with respect to
Ωr(X) if the following formula holds true:

∀X (cid:48). X (cid:48) ∈ Ωr(X) ⇒ F (X (cid:48)) = y∗

(3)

If formula (3) is true, that means neural network can resist
any substitutions in Ωr(X). For the point-wise robustness
metric, substitution length and ratio can be easily converted
to each other.

Problems
From a high-level perspective, there are four types of rele-
vant problems:
• Type-1 (Satisfaction problem). Find an adver-
sarial example in the perturbation space. It helps to prove
that a neural network is unsafe in a certain input space.
• Type-2 (Optimization problem). Find the adver-
sarial example with minimal perturbation. This can help
us ﬁgure out the boundary of safe regions.

• Type-3 (Proving problem). Certify the absence of
adversarial examples in the perturbation space. In other
word, prove that the formulas like (2) or (3) is true. This
problem can prove that the network is absolutely safe in
certain input spaces.

• Type-4 (Counting problem). Give the number of
adversarial examples in the perturbation space. It further
investigates the model’s susceptibility outside the abso-
lutely safe radius.

In recent years, most relevant works focused on develop-
ing effective attacking algorithms for generating adversarial
examples (Jia et al. 2019; Huang et al. 2019a; Ye, Gong, and
Liu 2020), which can be viewed as “Type-1 problem”: ﬁnd-
ing an adversarial example in the perturbation space. How-
ever, ﬁnding an adversarial example or not can not reﬂect
the model’s defense ability in the whole perturbation space.
Type-2∼4 problems are more informative and remains to be
studied, which are our focuses in this work.

These four problems present different levels of difﬁculty.
In more details, Type-1 problem is in N P; Type-2 prob-
lem is N P-hard; Type-3 problem is in CoN P (Katz et al.
2017); and Type-4 problem is #P-hard (N P ⊆ #P) (Ba-
luta et al. 2019). Type-3 problem is the complement of Type-
1 problem. Sometimes, Type-1 and Type-3 problems are not
strictly distinguished, so certifying robustness is sometimes
said to be in N P as well. These conclusions are drawn when
the tasks and neural networks have no restrictions.

Methods

In this section, we propose a formal framework for evalu-
ating robustness to word substitution perturbation. We ﬁrst
study the upper and the lower bound for the safe boundary
which belongs to Type 2 and Type 3 problem respectively.
Then we use a statistical inference method to quantify the
adversarial examples outside a safe region with a rigorous
guarantee, which is a Type 4 problem. For a more clear state,
we organize the following sections according to the three
problems.

Type-2 Problem: Pseudo-Dynamic Programming
for Crafting Adversarial Examples

As shown in Figure 1, if an adversarial example is found
in Ωr(X), it means Ωr(X) is not L0-Robust according to
Deﬁnition 4 or the maximum robustness radius of the model
must be lower than r. An adversarial example offers an up-
per bound for the maximum robustness radius. Naturally,
we wonder about a tighter upper bound for estimating safe
boundaries. So, we design an efﬁcient algorithm to ﬁnd ad-
versarial examples with fewer substituted words in Ω(X).
The algorithm can not only ﬁnd high-quality adversarial ex-
amples, but also provide a tighter upper bound for robustness
radius in Ωr(X). The basic idea of our method is inspired by
dynamic programming.

Methodology Finding the optimal adversarial example X (cid:48)
can be seen as a combinatorial optimization problem with
two goals:
i) Optimize the output conﬁdence score of F (X (cid:48)) to fool

the classiﬁer.

ii) Minimize the number of substituted words (i.e. Minimize

(cid:107)X (cid:48) − X(cid:107)0).

Algorithm 1: PDP (Pseudo-Dynamic Programming)

Input:

F : A classiﬁer
X: An input text with n words
τ : the maximum percentage of words for modiﬁcation.

Output: An adversarial example X (cid:48) or Failed.
1: A(X, 0) ← {X} ;
2: P1 ← ∅, P2 ← {p1, p2, ...pm} ;
3: for all t ← 1 to m do
4:
5:
6:
7:
8:

A(X, t − 1) ← TopK(A(X, t − 1));
p∗ ← arg maxp∈P2 {Ip(A(X, t − 1), p)};
A(X, t) ← A(X, t − 1) × S(X, p∗);
P1 ← P1 ∪ {p∗}, P2 ← P2 \ {p∗};
if ∃X (cid:48) ∈ A(X, t), F (X (cid:48)) (cid:54)= y∗ and (cid:107)X (cid:48) − X(cid:107)0 ≤
τ · n then

X (cid:48) ← the best adversarial example in A(X, t)
return X (cid:48)

9:
10:
end if
11:
12: end for
13: return Failed

We consider the optimizing procedure is in correlation
with a time variable t. Let A(X, t) denote the text set con-
taining all combinations of word substitutions for ﬁrst t
perturbed positions {p1, p2, ...pt}. Opt[(A(X, t))] denotes
the operation to get the optimal adversarial example from
A(X, t). Operation A(X, t − 1) × S(X, pt) means substi-
tute the pt-th position with candidate words in S(X, pt) for
all texts in A(X, t − 1). Then we get the optimal adversarial
example from A(X, m) in m steps:

Opt[A(X, t)] := Opt[A(X, t − 1) × S(X, pt)]

where |P |=m, t ∈ {1, ..., m} and A(X, 0)={X}. This pro-
cedure can guarantee to ﬁnd the optimal adversarial exam-
ple. However, it has exponential time complexity as the size
of A(X, t) increases exponentially with t.

We make some relaxations for this procedure to ensure
it can be executed in polynomial time. At step t, we only
keep top K texts in A(X, t − 1) which are considered to
be more promising in generating adversarial examples. The
others will be forgotten at this step. In this context, we have:

A(X, t) := TopK(A(X, t − 1)) × S(X, pt)

(4)

This relaxation comes at the cost of the guarantee of ﬁnd-
ing the optimal adversarial example. Due to that, the re-
currence relation 4 is similar to the dynamic programming
equation, we call it pseudo-dynamic programming (PDP).
Notice that the number of substituted words of all texts in
A(X, t) is less than t. So, when an adversarial example is
found at an earlier time t, it has greater chances to achieve
the goal (ii) better. So, we make use of the future information
to help the procedure encounter an adversarial example at
an earlier time t. At time t − 1, the perturbable position set
P can be divided into two sets P1 = {pi}t−1
i=1 and P2 =
{pi}m
i=t. P1 is the set of positions that have been considered
and P2 is the set of positions to be considered in the future.
Then we look ahead and pick the best position p∗ in P2 to

increase the chance of ﬁnding an adversarial example in the
next time t. So the recurrence relation 4 can be optimized as:

A(X, t) := TopK(A(X, t − 1)) × S(X, p∗)

(5)

This pseudo dynamic programming procedure is designed
for GPU computing. It can make good use of the characteris-
tics of parallel computing. For each step, the texts in A(X, t)
can be fed into classiﬁer F simultaneously as a batch to ﬁnd
adversarial examples and calculate evaluation scores.

Score Functions Next, we explain how to realize
T opK(·) for remembering history information and how to
look ahead for the future in ﬁnding p∗, which is the key to
the PDP.
TopK(·) We use the score Is(X (cid:48)) to measure the impor-
tance of a text X (cid:48) ∈ A(X, t). It can be:
• Is(X (cid:48)) := 1 − Fy∗(X (cid:48))
Untargeted attack
• Is(X (cid:48)) := Fˆy(X (cid:48))
Targeted (ˆy) attack
Operation TopK(·) will preserve K texts with highest score
Is. For an untargeted attack, it will preserve K texts with
the lowest conﬁdence score for the gold label; For a targeted
attack, it will preserve K texts with the highest conﬁdence
score for the expected output label ˆy.

Looking Ahead We call A(X, t) as a conﬁguration at
time t. Let Xwp←w denote the text after replacing the word
wp in position p of X by w. The importance score of the per-
turbed position p under the current conﬁguration A(X, t) is
Ip(A(X, t), p). It can be:
• Untargeted attack:

Ip(A(X, t), p) := 1 −

min
X (cid:48)∼A,w∈S(X,p)

{Fy∗(X (cid:48)

wp←w)}

• Targeted attack:

{Fˆy(X (cid:48)

Ip(A(X, t), p) :=

max
X (cid:48)∼A,w∈S(X,p)
where X (cid:48) ∼ A means drawing some texts from A(X, t)
with probability proportional to Is(X (cid:48)). Then we have the
position p∗, which has the highest score Ip, for the next step
t to consider:

wp←w)}

p∗ := arg max

p∈P2

{Ip(A(X, t − 1), p)}

Under the white-box setting, gradient information also

can be used to measure the importance of position p.

The overall PDP algorithm is shown in Algorithm 1. It
is a polynomial-time algorithm (O(n2 · poly(|F |, n)) in the
worst case, and the proof is in the supplementary material).
poly(|F |, n) represents prediction time of classiﬁer F for an
input with length n. It is a polynomial function.

Type-3 Problem: Robustness Veriﬁcation
Veriﬁcation is a method to prove the correctness of a sys-
tem with respect to a certain property via formal methods of
mathematics. If we can prove formula 3 is true for a certain
radius r (Type-3 problem), that means r is a lower bound of
maximum safe radius. Via combining the upper and lower
bound, we can ﬁgure out the boundary of the safe regions.
Generally speaking, proving is much more difﬁcult than ﬁnd

a counter example (Type-1 problem), which needs to enu-
merate the exponential space or design a theorem proving al-
gorithm. Several over-approximate veriﬁcation methods like
Interval Bound Propagation (IBP) (Jia et al. 2019; Huang
et al. 2019b) have recently been introduced from image to
NLP. Limited by time cost, scaling to large neural networks
is a challenge for these methods. In this section, we intro-
duce a property of L0-robustness, which is helpful for certi-
fying robustness when radius r is ﬁxed. It can also be used
to improve the efﬁciency of other veriﬁcation methods.
Theorem 1. For any ﬁxed r, Type-3 problem is in time com-
plexity class P.

Proof. Suppose that a classiﬁer F can output a prediction
for an input X with length n in poly(|F |, n) time and X has
m perturbable positions. For a given r, we have:

|Ωr(X)| ≤

(cid:19)

(cid:18)m
r

· vr ≤

(cid:19)

(cid:18)n
r

· vr

where v = maxp∈P {|S(X, p)|}. We know that the size of
Ωr(X) is bounded by O((nv)r). So, one can test all the pos-
sible substitutions in Ωr(X) in O((nv)r) · poly(|F |, n) time
to answer problems of Type-3.

Such conclusions are speciﬁc for NLP area owing to its
discrete nature. In many cases, the upper bound of r can
be given by our PDP algorithm. In such a situation, we can
directly enumerate all the possible substitutions to prove
the absence of adversarial examples within r (or formula 3
holds) in polynomial time. The enumeration procedure ac-
complished by a simple prover (SP), returns “Certiﬁed Ro-
bustness” or “ Found an adversarial example”. After the ab-
sence of adversarial examples in Ωr(X) is proved, r is a
lower bound for the maximum L0-robustness radius.

All the possible substitutions compose a polynomial-time
veriﬁable formal proof for the absence of adversarial ex-
amples. A checkable proof can make the result more con-
vincing. If an algorithm ﬁnds an adversarial example, we
can check the result easily. However, if an algorithm reports
no adversarial examples, it is difﬁcult to ﬁgure out whether
there are indeed no adversarial samples or the veriﬁcation
algorithm has some bugs.

Under the white-box setting, the gradient information can
be used to accelerate the veriﬁcation algorithm. The basic
idea is to test more sensitive positions ﬁrst. Once an adver-
sarial example occurs, the program can be terminated. Let
(cid:107)∂Fy∗ (X)/∂wp(cid:107)1 denote sensitivity score of perturbable
position p, we can pre-sort the perturbable positions in P
based on the sensitivity score.

Type-4 Problem: Robustness Metric
Why are neural networks often fooled by small crafted per-
turbations, but have good generalization to noisy inputs in
the real environment? How about the ability of a model to re-
sist perturbation outside the robust radius? These questions
promote us to analyze robustness from another perspective:
the quantity of adversarial examples. Sometimes, it is difﬁ-
cult to enumerate all the adversarial examples in the pertur-
bation space.

We relax the universal quantiﬁer “∀” in formula 2 to a

quantitative version as word-level robustness metric P R:

P R :=

|{X (cid:48) : X (cid:48) ∈ Ωr(X) ∧ F (X (cid:48)) = y∗}|
|Ωr(X)|

,

(6)

where we can see that 1-P R is the proportion of adversar-
ial examples. Therefore, the higher the P R value is, the less
vulnerable the classiﬁer F is to be fooled by random pertur-
bations around the point X. When P R=1, it is equivalent to
formula 2.

Apparently, the exact computation of P R is essentially a
Type-4 problem. For a long input sequence, calculating the
value of P R is infeasible at the moment due to the limita-
tion of computational power. As an alternative, we estimate
P R via a statistical method. Suppose that X1, X2, ..., XN
are taken from Ωr(X) with uniform sampling, then an esti-
mator ˆP R for P R is:

ˆP R :=

1
N

N
(cid:88)

i=1

I(F (Xi) = y∗)

(7)

The satisfaction of (F (Xi) = y∗) can be seen as
Bernoulli random variable Yi, i.e., Yi ∼ Bernoulli(P R).
So, if we want estimator ˆP R to satisfy a prior guarantee such
as the probability of producing an estimation which deviates
from its real value P R by a certain amount (cid:15) is less than δ,
the following must hold:

P r(| ˆP R − P R| < (cid:15)) > 1 − δ

(8)

Based on Hoeffding’s inequality:

P r(|

1
N

N
(cid:88)

i=1

Yi − P R| ≥ (cid:15)) ≤ 2e−2N (cid:15)2

For given parameters (cid:15) and δ, the estimator ˆP R satisﬁes for-
mula (8) if:

1
2(cid:15)2 ln
ˆP R is a metric for a model’s susceptibility to random per-
turbations with rigorous statistical guarantees. As the error
bound and sample complexity is similar to those in PAC the-
ory, we also call it PAC-style robustness metric.

N >

2
δ

(9)

Experiments
In this section, we design three sets of experiments to study
the three problems and methods we proposed.

General Experiment Setup
Tasks We conduct experiments on two important NLP
tasks: text classiﬁcation and textual entailment. MR (Pang
and Lee 2005) and IMDB (Maas et al. 2011) are sentence-
level and document-level sentiment classiﬁcation respec-
tively on positive and negative movie reviews. SNLI (Bow-
man et al. 2015) is used to learn to judge the relationship
between two sentences: whether the second sentence can be
derived from entailment, contradiction, or neutral relation-
ship with the ﬁrst sentence.

Target Models For each task, we choose two widely used
models, bidirectional LSTM (BiLSTM) (Conneau et al.
2017) and BERT (Devlin et al. 2019) as the attacking tar-
get models. For BiLSTM, we used a 1-layer bidirectional
LSTM with 150 hidden units, and 300-dimensional pre-
trained GloVe (Pennington, Socher, and Manning 2014)
word embeddings. We used the 12-layer based version of
BERT model with 768 hidden units and 12 heads, with
110M parameters. Details of the data and the classiﬁcation
accuracy on the test set of the models are listed in Table 1.

Dataset
MR
IMDB
SNLI

Avg Len
20(2/50)
215(6/2K)
8(2/30)

Test
Train
1K
9K
25K
25K
570K 10K

BiLSTM BERT
89.60
92.27
90.50

82.47
91.23
84.43

Table 1: Overview of the datasets and the test accuracy of
target models. The numbers in brackets of column “Avg
Len” are the minimum/maximum length.

Type-2 Problem: Attack Evaluation
Baselines We use two state-of-the-art adversarial crafting
methods (TextFooler (Jin et al. 2020) and SemPSO (Zang
et al. 2020)) as references to compare the search capability
of PDP. TextFooler is a greedy algorithm and SemPSO is a
particle-swarm-based algorithm. They all focus on Type 1
problem while PDP focuses on Type 2 problem.

Metrics We evaluate the performance of these attack
methods including the rate of successful attacks and the per-
centage of word substitution. A smaller percentage (or num-
ber) of word substitution means a tighter upper bound for
the maximum L0-robustness radius.

Settings For a fair comparison, we set the same candidate
set and constraints for different attack methods. The candi-
date is generated by HowNet (Dong and Dong 2006) and
similarities of word embeddings. HowNet is arranged by
the sememe and can ﬁnd the potential semantic-preserving
words. Word embeddings can further help to select the most
similar candidate words. So, we generate S(X, p) via clean-
ing the synonyms obtained by HowNet with cosine similar-
ity of word embeddings. We reserve top η (η = 5) synonyms
as candidates for each position.

For MR, we experiment on all the test texts classiﬁed cor-
rectly. For IMDB and SNLI, we randomly sample 1000 texts
classiﬁed correctly from the test set. Following (Alzantot
et al. 2018; Zang et al. 2020), only the hypotheses are per-
turbed for SNLI. The adversarial examples with modiﬁca-
tion rates less than 25% are considered valid.

Attack Results We present the average percentage of sub-
stitutions (%S) in Table 2 and the number of times each
method “wins” the others in terms of substitution length
(#Win). The experimental results show that PDP always
gives adversarial examples with fewer substitutions. Espe-
cially for the long-text dataset, IMDB, 599 (59.9%) ad-
versarial examples found by PDP contains the least word
substitutions for BiLSTM (the remaining 40.1% holds the

Dataset

Model

#Attacks

MR

IMDB

SNLI

BiLSTM
BERT
BiLSTM
BERT
BiLSTM
BERT

880
956
1000
1000
1000
1000

SemPSO

#Succ
636(72.27%)
580(60.67%)
947(94.7%)
871(87.1%)
505(50.5%)
587(58.7%)

#Win
0
0
0
0
0
0

%S
10.64
12.10
4.58
4.31
15.99
16.10

#Succ
484(55.00%)
323(33.79%)
854(85.4%)
714(71.4%)
592(59.2%)
636(63.6%)

TextFooler
#Win
0
0
0
0
0
0

%S
12.09
13.96
6.78
8.47
15.76
15.83

PDP

#Succ
655(74.43%)
621(64.96%)
989(98.9%)
899(89.9%)
764(76.4%)
845(84.5%)

#Win
33
30
599
498
31
30

%S
10.44
11.80
3.11
2.87
14.91
15.09

Table 2: The attack results of different methods. #Attacks is the number of texts to be attacked. #Succ is the number of successful
attacks. #Win is the number of successful attacks crafted with the least substitutions for the same texts among various attack
methods. %S is the avarage percentage of substitutited words.

(a) IMDB-BiLSTM

(b) IMDB-BERT

(a) IMDB-BiLSTM

(b) IMDB-BERT

Figure 3: Comparision of the number of substituted words
of different methods on IMDB. Each point represents a text
(x-axis is the number of substituted (#S) words of PDP and
y-axis is that of other attack methods). Points over the diag-
onal are where PDP ﬁnds an adversarial example with fewer
substitutions.

same number of substitutions with others). The examples
crafted by PDP contain very few substitutions, such as aver-
age 4.52 word substitutions for BERT on IMDB whose av-
erage number of words is 215. The comparison of the substi-
tution length on IMDB is shown in Figure 3. Besides, PDP
achieves the highest attack success rates on all three datasets
and two target models. These experimental results indicate
PDP has stronger search capabilities. Then, we repurpose
PDP attack to evaluate the robustness, and Figure 4 shows
that PDP can provide a tighter bound for the maximum ro-
bustness radii compared with other attacking methods. More
experimental results are shown in the supplementary.

Type-3 Problem: Robustness veriﬁcation
For a given L0 distance r (r=1 to 4), the certiﬁed results on
200 randomly sampled test instances are shown in Table 3.
We can have the three ﬁndings below. (1) The percentage of
certiﬁed robustness is decreasing with the increase of radius
r. (2) For many short-text tasks (MR and SNLI), consider-
ing r ≤ 4 is sufﬁcient because most regions cannot resist
4-word substitutions. For example, only 6.42% regions of
BERT can resist any 4-word substitutions adversarial attack
on SNLI. (3) For the long-text task IMDB, BERT has more
regions (61.52%) that can resist any 3-word substitutions at-
tack compared with BiLSTM. It takes a long time to certify
robustness when r=4, so we don’t show the results. Experi-
mental results also show that this simple veriﬁcation method

Figure 4: The percentage of regions (Ωr(X)) that do not
yield to different attacking methods in each perturbation ra-
dius. x-axis can be seen as the upper bounds given by differ-
ent attacking methods.

is effective for many NLP tasks.

Type-4 Problem: Robustness Metric
We evaluate the robustness score (Equation 6) of different
models on different tasks. The evaluation is performed on
the randomly sampled 1000 test data and the sample size
N is 5000 ((cid:15)=0.025, δ=0.005). The violin plots of ˆP R are
shown in Figure 5. As most attacking algorithms limit the
maximum perturbation ratio to smaller than 25%, we set r to
25% of the length of the sentence.

Most of the shadows in all sub-ﬁgures are close to the
top horizontal line (maximum ˆP R), which means that most
regions have high robustness scores ˆP R. Take BERT model
on IMDB task as an example, 89.9% regions are found with
adversarial examples as shown in Table 2, which indicates
the “vulnerability” of the model. However, via robustness
metric, we ﬁnd that 90.66% regions achieve ˆP R larger than
0.9. It means, most regions (90.66%) can resist random word
perturbations with high probability (> 0.9). A conclusion
can be drawn: these well-trained models are usually robust
to word substitutions in a non-adversarial environment.

For a well-trained model, the adversarial examples crafted
by word substitution are almost everywhere and close to the
normal point in the perturbation space, but their proportion
is very low. In 2019, Stutz et al. pointed out that on-manifold
robustness is essentially generalization and on-manifold ad-
versarial examples are generalization errors (Stutz, Hein,
and Schiele 2019). Suppose users’ selection from a synonym
candidate is similar to the process of rolling a die, which

020406080#S of PDP020406080#S of OthersPDP VS. SemPSOPDP VS. TextFooler020406080#S of PDP020406080#S of OthersPDP VS. SemPSOPDP VS. TextFooler0481216#Substitutions (r)0%20%40%60%80%100%PercentageTextFoolerSemPSOPDP0481216#Substitutions (r)20%40%60%80%100%PercentageTextFoolerSemPSOPDPDataset Model

r=1

r=2

r=3

r=4

BERT

Found
%F
BiLSTM 36.00
20.00
BiLSTM 15.59
12.66
BiLSTM 56.90
71.43

BERT

BERT

T(s)
0.01
0.12
0.04
2.89
0.04
0.03

Certiﬁed
%C T(s)
0.02
0.24
0.13
3.11
0.01
0.01

64.00
80.00
84.41
87.34
43.10
28.57

Found
%F
58.00
40.00
31.99
25.91
76.87
88.01

T(s)
0.04
1.95
1.93
3.40
0.03
0.07

Certiﬁed

Found

Certiﬁed

Found

%C
42.00
60.00
68.01
74.09
23.13
11.99

T(s)
0.05
2.49
2.89
246.54
0.07
0.06

%F
72.00
56.50
45.50
38.48
82.52
92.37

T(s)
0.30
28.86
2.47
7.97
0.14
0.47

%C
28.00
43.50
54.50
61.52
17.48
7.63

T(s)
0.27
20.95
953.19
6448.39
0.11
0.07

%F
78.00
67.50
-
-
84.63
93.58

T(s)
3.62
256.78
-
-
0.66
3.17

Certiﬁed
%C
22.00
32.50
-
-
15.37
6.42

T(s)
2.26
46.23
-
-
0.23
0.04

MR

IMDB

SNLI

Table 3: Certiﬁed robustness. “Found” and “Certiﬁed” are the abbreviations for “an adversarial example found” and “certiﬁed
to be robust” respectively. %F and %C are the percentage of “Found” and “Certiﬁed”. “T” is the average time.

Figure 5: Violin plots of robustness score on the test set when r is 25% of the length of sentence. The width of x-axis represents
the frequency of corresponding ˆP R in y-axis. Top and bottom horizontal lines are the maximum and minimum value of ˆP R.

means the usage of a word for position p in S(X, p) is condi-
tional on a latent variable zp, i.e. P r(wp|zp) corresponding
to the underlying, low dimensional manifold. All possible
substitutions in Ω(X) can be seen as on the manifold corre-
sponding to a latent variable Z = (zp1 , zp2, ..., zpk ). Thus,
the adversarial examples found by attacking algorithms are
essentially on-manifold generalization errors. From this per-
spective, we can explain why a well-trained model like
BERT has a good generalization but can be easily attacked
by word substitutions.

For all three tasks, BERT always presents better robust-
ness performance. For instance, on MR task, the proportion
of regions with ˆP R larger than 0.9 is 79.22% and 90.97%
for BiLSTM and BERT respectively. It means BERT is al-
ways more robust outside the safe regions. Our robustness
metric presents its superiority to the traditional model met-
rics including accuracy.

Related Work
Existing works about the word-level robustness problem
mainly focus on three lines of research points.

Adversarial Examples Various attack algorithms are de-
veloped for generating adversarial examples via substitu-
tions including gradient descent methods (Sato et al. 2018;
Liang et al. 2018; Wang et al. 2021), genetic algorithm
(Alzantot et al. 2018), particle-swarm-based method (Zang
et al. 2020), greedy-based methods (Ren et al. 2019; Jin et al.
2020) and BERT-based methods (Li et al. 2020; Garg and
Ramakrishnan 2020). They focus on how to generate adver-
sarial examples effectively and simply regard robustness as
the opposite of attack success rate.

Robustness Verﬁcation (Jia et al. 2019; Huang et al.
2019a; Shi et al. 2020) migrate the over-approximate method
IBP from the image ﬁeld to certify the robustness in the con-

tinuous space based on word embedding. Although they can
give a provably robust to all possible perturbations within
the constraints, the limitation is that a model which is not
robust in continuous space can be robust in discrete space,
as the vectors that can fool the model may not correspond
to any real words. (Ye, Gong, and Liu 2020) introduce a
randomized smoothing-based method to certify the robust-
ness of a smoothed classiﬁer. Existing robustness evaluation
works focus on robustness veriﬁcation which aims to verify
the absolute safe for a given model in the whole perturbation
space. They ignore the safe sub-regions and unsafe regions.

Defense Naturally, the ﬁnal goal is to defend against at-
tacking and improve the robustness of models. Adversarial
data augmentation (ADA) is one of the most effective empir-
ical methods. (Ren et al. 2019; Jin et al. 2020; Li et al. 2020;
Garg and Ramakrishnan 2020; Wang et al. 2021) adopt the
adversarial examples generated by their attack methods for
adversarial training and achieve some robustness improve-
ment. Adversarial training is another similar method, which
incorporates a min-max optimization between adversarial
perturbations and the models by adding norm-bounded per-
turbations to word embeddings (Madry et al. 2018; Zhu et al.
2020). They depend on search algorithms for adversarial ex-
amples, so our PDP with better search ability can provide
support for these robustness enhancement methods.

Conclusion
Overall, we build a formal framework to study the word-
level robustness of the deep-learning-based NLP systems.
We repurpose the attack method for robustness evaluation
and design a pseudo-dynamic programming framework for
crafting adversarial examples with fewer substitutions to
provide a tighter upper bound. Besides, we notice that the
absence of adversarial examples within any ﬁxed radius can
be veriﬁed in polynomial time, and give a simple prover to

0.40.60.81.0PRMR-BiLSTMMR-BERTIMDB-BiLSTMIMDB-BERTSNLI-BiLSTMSNLI-BERTcertify the lower bound. Experimental results show that our
methods can provide tighter bounds for robustness evalua-
tion, and most state-of-the-art models like BERT cannot re-
sist a few word substitutions. Further, we discuss the robust-
ness from the view of quantiﬁcation and introduce a PAC-
style metric to show they are robust to random perturbations,
as well as explain why they generalize well but are poor in
resisting adversarial attacks. It can be helpful to studying de-
fense and interpretability of NLP models.

References
Alzantot, M.; Sharma, Y.; Elgohary, A.; Ho, B.; Srivastava,
M. B.; and Chang, K. 2018. Generating Natural Language
Adversarial Examples. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language Process-
ing, Brussels, Belgium, October 31 - November 4, 2018,
2890–2896.
Baluta, T.; Shen, S.; Shinde, S.; Meel, K. S.; and Saxena,
P. 2019. Quantitative Veriﬁcation of Neural Networks and
Its Security Applications. In Proceedings of the 2019 ACM
SIGSAC Conference on Computer and Communications Se-
curity, CCS 2019, London, UK, November 11-15, 2019,
1249–1264.
Belinkov, Y.; and Bisk, Y. 2018. Synthetic and Natural Noise
In 6th Interna-
Both Break Neural Machine Translation.
tional Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings.
Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D.
2015. A large annotated corpus for learning natural language
inference. In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing, EMNLP
2015, Lisbon, Portugal, September 17-21, 2015, 632–642.
Conneau, A.; Kiela, D.; Schwenk, H.; Barrault, L.; and Bor-
des, A. 2017. Supervised Learning of Universal Sentence
Representations from Natural Language Inference Data. In
Proceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017, 670–680.
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technolo-
gies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
2019, Volume 1 (Long and Short Papers), 4171–4186.
Dong, Z.; and Dong, Q. 2006. HowNet and the Computation
of Meaning. World Scientiﬁc.
Garg, S.; and Ramakrishnan, G. 2020. BAE: BERT-based
Adversarial Examples for Text Classiﬁcation. In Proceed-
ings of the 2020 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2020, Online, November
16-20, 2020, 6174–6181.
Huang, P.; Stanforth, R.; Welbl, J.; Dyer, C.; Yogatama, D.;
Gowal, S.; Dvijotham, K.; and Kohli, P. 2019a. Achiev-
ing Veriﬁed Robustness to Symbol Substitutions via Inter-
val Bound Propagation. In Proceedings of the 2019 Confer-

ence on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
China, November 3-7, 2019, 4081–4091.
Huang, P.; Stanforth, R.; Welbl, J.; Dyer, C.; Yogatama, D.;
Gowal, S.; Dvijotham, K.; and Kohli, P. 2019b. Achiev-
ing Veriﬁed Robustness to Symbol Substitutions via Interval
Bound Propagation. In Inui, K.; Jiang, J.; Ng, V.; and Wan,
X., eds., Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,
2019, 4081–4091. Association for Computational Linguis-
tics.
Jia, R.; Raghunathan, A.; G¨oksel, K.; and Liang, P. 2019.
Certiﬁed Robustness to Adversarial Word Substitutions. In
Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,
2019, 4127–4140.
Jin, D.; Jin, Z.; Zhou, J. T.; and Szolovits, P. 2020. Is BERT
Really Robust? A Strong Baseline for Natural Language At-
tack on Text Classiﬁcation and Entailment. In The Thirty-
Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI
2020, New York, NY, USA, February 7-12, 2020, 8018–8025.
Katz, G.; Barrett, C. W.; Dill, D. L.; Julian, K.; and Kochen-
derfer, M. J. 2017. Reluplex: An Efﬁcient SMT Solver for
Verifying Deep Neural Networks. In Computer Aided Veri-
ﬁcation - 29th International Conference, CAV 2017, Heidel-
berg, Germany, July 24-28, 2017, Proceedings, Part I, 97–
117.
Li, L.; Ma, R.; Guo, Q.; Xue, X.; and Qiu, X. 2020. BERT-
ATTACK: Adversarial Attack Against BERT Using BERT.
In Proceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, 6193–6202.
Liang, B.; Li, H.; Su, M.; Bian, P.; Li, X.; and Shi, W. 2018.
Deep Text Classiﬁcation Can be Fooled. In Proceedings of
the Twenty-Seventh International Joint Conference on Arti-
ﬁcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm,
Sweden, 4208–4215.
Maas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y.;
and Potts, C. 2011. Learning Word Vectors for Sentiment
Analysis. In The 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies,
Proceedings of the Conference, 19-24 June, 2011, Portland,
Oregon, USA, 142–150.
Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and
Vladu, A. 2018. Towards Deep Learning Models Resis-
In 6th International Confer-
tant to Adversarial Attacks.
ence on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Pro-
ceedings.
Neekhara, P.; Hussain, S.; Dubnov, S.; and Koushanfar, F.
2019. Adversarial Reprogramming of Text Classiﬁcation

as Combinatorial Optimization. In Proceedings of the 58th
Annual Meeting of the Association for Computational Lin-
guistics, ACL 2020, Online, July 5-10, 2020, 6066–6080.
Zhu, C.; Cheng, Y.; Gan, Z.; Sun, S.; Goldstein, T.; and Liu,
J. 2020. FreeLB: Enhanced Adversarial Training for Nat-
In 8th International Con-
ural Language Understanding.
ference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020.

In Proceedings of the 2019 Confer-
Neural Networks.
ence on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
China, November 3-7, 2019, 5215–5224.
Pang, B.; and Lee, L. 2005. Seeing Stars: Exploiting Class
Relationships for Sentiment Categorization with Respect to
In ACL 2005, 43rd Annual Meeting of the
Rating Scales.
Association for Computational Linguistics, Proceedings of
the Conference, 25-30 June 2005, University of Michigan,
USA, 115–124.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:
In Proceedings
Global Vectors for Word Representation.
of the 2014 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2014, October 25-29, 2014,
Doha, Qatar, A meeting of SIGDAT, a Special Interest Group
of the ACL, 1532–1543.
Ren, S.; Deng, Y.; He, K.; and Che, W. 2019. Generating
Natural Language Adversarial Examples through Probabil-
In Proceedings of the 57th
ity Weighted Word Saliency.
Conference of the Association for Computational Linguis-
tics, ACL 2019, Florence, Italy, July 28- August 2, 2019,
Volume 1: Long Papers, 1085–1097.
Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2018. Seman-
tically Equivalent Adversarial Rules for Debugging NLP
models. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics, ACL 2018, Mel-
bourne, Australia, July 15-20, 2018, Volume 1: Long Papers,
856–865.
Sato, M.; Suzuki, J.; Shindo, H.; and Matsumoto, Y. 2018.
Interpretable Adversarial Perturbation in Input Embedding
Space for Text. In Proceedings of the Twenty-Seventh Inter-
national Joint Conference on Artiﬁcial Intelligence, IJCAI
2018, July 13-19, 2018, Stockholm, Sweden, 4323–4330.
Shi, Z.; Zhang, H.; Chang, K.; Huang, M.; and Hsieh, C.
2020. Robustness Veriﬁcation for Transformers. In 8th In-
ternational Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020.
Stutz, D.; Hein, M.; and Schiele, B. 2019. Disentangling
Adversarial Robustness and Generalization. In IEEE Con-
ference on Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019, 6976–6987.
Wang, X.; Yang, Y.; Deng, Y.; and He, K. 2021. Adversarial
Training with Fast Gradient Projection Method against Syn-
onym Substitution Based Text Attacks. In Thirty-Fifth AAAI
Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-
Third Conference on Innovative Applications of Artiﬁcial
Intelligence, IAAI 2021, The Eleventh Symposium on Edu-
cational Advances in Artiﬁcial Intelligence, EAAI 2021, Vir-
tual Event, February 2-9, 2021, 13997–14005. AAAI Press.
Ye, M.; Gong, C.; and Liu, Q. 2020. SAFER: A Structure-
free Approach for Certiﬁed Robustness to Adversarial Word
Substitutions. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, ACL 2020,
Online, July 5-10, 2020, 3465–3475.
Zang, Y.; Qi, F.; Yang, C.; Liu, Z.; Zhang, M.; Liu, Q.; and
Sun, M. 2020. Word-level Textual Adversarial Attacking

