Robust Value Iteration for Continuous Control Tasks

Michael Lutter1,2, Shie Mannor1,3, Jan Peters2, Dieter Fox1,4, Animesh Garg1,5
1Nvidia, 2TU Darmstadt, 3 Technion, 4 University of Washington, 5University of Toronto & Vector Institute

1
2
0
2

y
a
M
5
2

]

G
L
.
s
c
[

1
v
9
8
1
2
1
.
5
0
1
2
:
v
i
X
r
a

Abstractâ€”When transferring a control policy from simulation
to a physical system, the policy needs to be robust to variations
in the dynamics to perform well. Commonly, the optimal policy
overï¬ts to the approximate model and the corresponding state-
distribution, often resulting in failure to trasnfer underlying
distributional shifts. In this paper, we present Robust Fitted
Value Iteration, which uses dynamic programming to compute
the optimal value function on the compact state domain and
incorporates adversarial perturbations of the system dynamics.
The adversarial perturbations encourage a optimal policy that
is robust to changes in the dynamics. Utilizing the continuous-
time perspective of reinforcement learning, we derive the optimal
perturbations for the states, actions, observations and model
parameters in closed-form. Notably, the resulting algorithm does
not require discretization of states or actions. Therefore, the
optimal adversarial perturbations can be eï¬ƒciently incorporated
in the min-max value function update. We apply the resulting
algorithm to the physical Furuta pendulum and cartpole. By
changing the masses of the systems we evaluate the quantitative
and qualitative performance across diï¬€erent model parameters.
We show that robust value iteration is more robust compared
to deep reinforcement learning algorithm and the non-robust
version of the algorithm. Videos of the experiments are shown
at https://sites.google.com/view/rfvi

I. Introduction

To avoid the laborious and potentially dangerous training of
control policies on the physical system, Simulation to reality
transfer (sim2real) learns a policy in simulation and evaluates
the policy on the physical system. When transferred to the
real world, the policy should solve the task and obtain a
comparable reward to the simulation. Therefore, the goal of
sim2real is to learn a policy that is robust to changes in
the dynamics and successfully bridges the simulation-reality
gap. Naive reinforcement learning (RL) methods usually do
not succeed for sim2real as the resulting policies overï¬t to
the approximate simulation model. Therefore, the resulting
policies are not robust to changes in the dynamics and fail to
solve the task in the real world. In contrast, sim2real methods
extending RL with domain randomization [1â€“4] or adversarial
disturbances [5â€“8] have shown the successful transfer to the
physical world [9].

In this paper, we focus on adversarial disturbances to bridge
the simulation-reality gap. In this paradigm the RL problem
is formulated as a two player zero-sum game [6]. The optimal
policy wants to maximize the reward while the adversary
wants to minimize the reward. For control tasks, the policy
controls the system input u and the adversary Î¾ controls
the dynamics. For example, the adversary can change the
state, action, observation, model parameters or all of them
within limits. Therefore, this problem formulation optimizes

The control chart of robust ï¬tted value iteration (rFVI) for
Figure 1.
continuous states, actions and time. The deterministic optimal policy and
deterministic adversaries, which add an bias to the system dynamics, only
depend on shared value function. While the optimal policy performs hill-
climbing the adversaries perform steepest descent following the value function
gradient.

the worst-case performance and not the expected reward as
standard RL. The resulting policy is robust to changes in the
dynamics as the planning in simulation uses the worst-case
approximation which includes the physical system [10].

In this adversarial RL setting,
(1) we show that the optimal action and optimal adversary
can be directly computed using the value function estimate if
the reward is separable into state and action reward and the
continuous-time dynamics are non-linear and control-aï¬ƒne.
We derive the solution for the state, action, observation and
this paper extends the
model bias analytically. Therefore,
existing analytic solutions from continuous-time RL [11â€“14].
(2) we propose robust ï¬tted value iteration (rFVI). This al-
gorithm solves the adversarial RL problem with continuous
states, actions and disturbances by leveraging the analytic
expressions and value iteration. Using this approach,
the
continuous states and actions do not need to be discretized as
in classical methods [15â€“17] or require multiple optimizations
as the modern actor-critic approaches [7, 18].
(3) we provide an in-depth evaluation of rFVI on the physical
system. We benchmark this algorithm on the real-world Furuta
pendulum and cartpole. To test the robustness, we perturb
the model parameters by applying additional weights. The
performance is compared to standard deep RL algorithms with
and without domain randomization.

Therefore the contributions of this paper are the derivation
of the analytic adversarial actions, the introduction of robust
ï¬tted value iteration and the extensive evaluation on multiple
physical systems. In the evaluation we focus on two under-
actuated systems to provide an in-depth qualitative and quan-

âˆ«!(#)Ì‡"#"$(";')!!%#;'+)#;'*"+!"+!#+!$ 
 
 
 
 
 
Figure 2. The optimal Value function ğ‘‰ âˆ— and policy ğœ‹âˆ— of rFVI, cFVI and four diï¬€erent variations of SAC. All policies achieve nearly identical reward
on the nominal dynamics model. The variations of SAC demonstrate the change of the policy when increasing the entropy of the state distribution during
training. The entropy is increased by enlarging the initial state distribution ğœ‡ and using domain randomization. For SAC and ğœ‡ = N (Â± ğœ‹, ğœ) the optimal
policy is only valid on the optimal trajectory. For SAC UDR and ğœ‡ = U (âˆ’ ğœ‹, + ğ‘ğ‘–), the policy is applicable on the complete state domain. rFVI and cFVI
perform value iteration on the compact state domain and naturally obtain an optimal policy applicable on the complete state-domain. rFVI adapts ğ‘‰ âˆ— and ğœ‹âˆ—
to have a smaller ridge leading up to the upright pendulum and exerts higher actions when deviating from the optimal trajectory.

titative analysis of the various algorithms in the physical world.

In the following we introduce the continuous-time formulation
of adversarial RL (Section II). Section III derives the optimal
adversaries and introduces robust ï¬tted value iteration. Finally,
we describe the experimental setup and report the performance
of the algorithms on the physical system in Section IV.

II. Problem Statement

The inï¬nite horizon continuous-time RL optimization with the
adversary Î¾ is described by

ğœ‹âˆ—(x) = arg max

ğœ‹

inf
Î¾âˆˆÎ©

ğ‘‰ âˆ—(x) = max
u

inf
Î¾âˆˆÎ©

x(ğ‘¡) = x0 +

âˆ« âˆ

0
âˆ« âˆ

0
âˆ« ğ‘¡

0

exp(âˆ’ğœŒğ‘¡) ğ‘Ÿğ‘ (xğ‘¡ , uğ‘¡ ) ğ‘‘ğ‘¡,

exp(âˆ’ğœŒğ‘¡) ğ‘Ÿğ‘ (xğ‘¡ , uğ‘¡ ) ğ‘‘ğ‘¡,

ğ‘“ğ‘ (xğœ, uğœ, Î¾ğœ) ğ‘‘ğœ,

(1)

(2)

(3)

with the state x, action u, admissible set Î©, discounting
constant ğœŒ âˆˆ (0, âˆ], reward ğ‘Ÿğ‘, dynamics ğ‘“ğ‘, optimal value
function ğ‘‰ âˆ— and policy ğœ‹âˆ— [6, 13]. The order of the optimiza-
tions can be switched as the optimal actions and disturbance
remain identical [6]. The adversary Î¾ must be constrained to
be in the set of admissible disturbances Î© as otherwise the
adversary is too powerful and would prevent the policy from
learning the task.

The deterministic continuous-time dynamics ğ‘“ğ‘ is assumed to
be non-linear w.r.t. the system state x but aï¬ƒne w.r.t. the
action u. Such dynamics are described by

(cid:164)x = a(x; Î¸) + B(x; Î¸)u,

(4)

with the non-linear drift a, the non-linear control matrix B
and the system parameters Î¸. We assume that the approximate
parameters Ë†Î¸ and the approximate equation of motions Ë†a,
Ë†B
are known. For most rigid-body systems this assumptions is
feasible as the equations of motions can be derived analytically
and the system parameters can be measured. The resulting
model is only approximate due to the idealized assumption of

rigid bodies, measurement error of the system parameters and
neglecting the actuator dynamics.

The optimal policy and adversary is modeled as a stationary
and Markovian policy that applies a state-dependent distur-
bance. In this case, the worst-case action is deterministic if
the dynamics are deterministic. If the adversary would be
stochastic, the optimal policies are non-stationary and non-
Markovian [19]. This assumption is used in most of the
existing literature on adversarial RL [8, 13, 19, 20]. We
consider four diï¬€erent adversaries that alter the state [20â€“
22], action [7, 13, 18, 23â€“25], observation [8, 19] and model
parameters [8]. The diï¬€erent adversaries address potential
causes of the simulation gap. The state adversary Î¾ğ‘¥ incorpo-
rates unmodeled physical phenomena in the simulation. The
action adversary Î¾ğ‘¢ addresses the non-ideal actuators. The
observation adversary Î¾ğ‘œ introduces the non-ideal observations
caused by sensors. The model adversary Î¾ğœƒ introduces a bias
to the system parameters. All adversaries could be subsumed
via a single adversary with large admissible set. However, the
resulting dynamics would not capture underlying structure of
the simulation gap [8] and the optimal policy would be too
conservative [26]. Therefore, we disambiguate between the
diï¬€erent adversaries to capture this structure. Mathematically
the models are described by

State Î¾ğ‘¥ :
Action Î¾ğ‘¢ :
Observation Î¾ğ‘œ :
Model Î¾ğœƒ :

(cid:164)x = a(x; Î¸) + B(x; Î¸)u + Î¾x,
(cid:164)x = a(x; Î¸) + B(x; Î¸) (u + Î¾u) ,
(cid:164)x = a(x + Î¾o; Î¸) + B(x + Î¾o; Î¸) u,
(7)
(cid:164)x = a(x; Î¸ + Î¾ğœƒ ) + B(x; Î¸ + Î¾ğœƒ ) u. (8)

(5)

(6)

Instead of disturbing the observation, Equation 7 disturbs
the simulation state of the drift and control matrix. This
disturbance is identical to changing the observed system state.

The deterministic perturbation is in contrast to the standard
RL approaches, which describe Î¾ğ‘– as a stochastic variable.
However, this diï¬€erence is due to the worst-case perspective,
where one always selects the worst sample at each state.
This worst case sample is deterministic if the policies are

Entropy of state distribution during training7.50.07.5 [rad/s]V*(x)SAC - =SAC & UDR - =SAC - =USAC & UDR - =UcFVIrFVIÂ±/20+/2 [rad]7.50.07.5 [rad/s]*(x)Â±/20+/2 [rad]Â±/20+/2 [rad]Â±/20+/2 [rad]Â±/20+/2 [rad]Â±/20+/2 [rad]Table I
The optimal actions uğ‘˜ and adversarial actions ğœ‰ ğ‘˜ for the state-, action-, model- and observation bias with the admissible set Î©.

State Perturbation

Action Perturbation

Model Perturbation

Observation Perturbation

Dynamics ğ‘“ğ‘ (x, u, ğœ‰ )
Optimal Action uğ‘˜

Optimal Disturbance ğœ‰ ğ‘˜

a(x) + B (x)u + ğœ‰
âˆ‡ Ëœğ‘” (B (x)ğ‘‡ âˆ‡ğ‘¥ ğ‘‰ ğ‘˜ )
(cid:16)

âˆ‡ğ‘¥ ğ‘‰ ğ‘˜ (cid:17)

âˆ’â„Î©

a(x) + B (x) (u + ğœ‰ )
âˆ‡ Ëœğ‘” (B (x)ğ‘‡ âˆ‡ğ‘¥ ğ‘‰ ğ‘˜ )
Bğ‘‡ âˆ‡ğ‘¥ ğ‘‰ ğ‘˜ (cid:17)

âˆ’â„Î©

(cid:16)

stationary and Markovian. The ï¬ltering approaches of state-
estimation are not applicable to this problem formulation as
these approaches cannot infer a state-dependent bias.

i.e.,

The reward is separable into state reward ğ‘ğ‘ and action reward
ğ‘”ğ‘ described by

ğ‘Ÿğ‘ (x, u) = ğ‘ğ‘ (x) âˆ’ ğ‘”ğ‘ (u).

(9)

The action cost is non-linear, positive deï¬nite and strictly
convex. The assumptions on the action cost are not limiting
as these resulting properties are desirable. The convexity of
the action cost enforces that the optimal action is unique. The
positive deï¬niteness of ğ‘”ğ‘ penalizes non-zero actions, which
prevents the bang-bang controller to be optimal.

III. Robust Fitted Value Iteration

In the following, we summarize continuous ï¬tted value itera-
tion (cFVI) [27]. Afterwards, we derive the analytic solutions
for the optimal adversary and present robust ï¬tted value itera-
tion (rFVI). This algorithm solves the adversarial RL problem
and obtains a robust optimal policy. In contrast, cFVI only
solves the deterministic RL problem and obtains an optimal
policy that overï¬ts to the approximate dynamics model.

A. Preliminaries - Continuous Fitted Value Iteration

Continuous ï¬tted value iteration (cFVI) [27] extends the
classical value iteration approach to compute the optimal value
function for continuous action and state spaces. By showing
that the optimal policy can be computed analytically, the value
iteration update can be computed eï¬ƒciently. For non-linear
control-aï¬ƒne dynamics (Equation 4) and separable reward
(Equation 9), the optimal action is described by

ğœ‹ğ‘˜ (x) = âˆ‡ Ëœğ‘”ğ‘

(cid:16)

B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰ ğ‘˜ (cid:17)

(10)

with current value function âˆ‡ğ‘¥ğ‘‰ ğ‘˜ and the convex conjugate of
the action cost Ëœğ‘”ğ‘ [11, 12, 14]. The convex conjugate is deï¬ned
as âˆ‡ Ëœğ‘”(w) = [âˆ‡ğ‘”(w)]âˆ’1. For a quadratic action cost, âˆ‡ Ëœğ‘” is a
linear transformation. For barrier shaped action cost, âˆ‡ Ëœğ‘” re-
scales and limits the action range. This solution is intuitive as
the optimal policy performs hill climbing on the value function
manifold and action cost determines the step size.

Substituting the analytic policy into the value iteration update,
the classical algorithm can be extended to continuous actions,

a( ğœƒ + ğœ‰ ) + B ( ğœƒ + ğœ‰ )u
âˆ‡ Ëœğ‘” (B (x)ğ‘‡ âˆ‡ğ‘¥ ğ‘‰ ğ‘˜ )
(cid:18) (cid:16) ğœ•a
ğœ•x + ğœ•B

ğœ•ğœƒ uğ‘˜ (cid:17)ğ‘‡

âˆ‡ğ‘¥ ğ‘‰ ğ‘˜

âˆ’â„Î©

a(x + ğœ‰ ) + B (x + ğœ‰ )u
âˆ‡ Ëœğ‘” (B (x)ğ‘‡ âˆ‡ğ‘¥ ğ‘‰ ğ‘˜ )
(cid:18) (cid:16) ğœ•a
ğœ•x uğ‘˜ (cid:17)ğ‘‡
ğœ•x + ğœ•B

âˆ‡ğ‘¥ ğ‘‰ ğ‘˜

âˆ’â„Î©

(cid:19)

(cid:19)

ğ‘‰ ğ‘˜+1
tar = max
u
(cid:16)
= ğ‘Ÿ

ğ‘Ÿ (x, u) + ğ›¾ğ‘‰ ğ‘˜ ( ğ‘“ (xğ‘¡ , u))
+ ğ›¾ğ‘‰ ğ‘˜ (cid:16)

xğ‘¡ , ğœ‹ğ‘˜ (xğ‘¡ )

(cid:17)

(cid:16)

ğ‘“

xğ‘¡ , ğœ‹ğ‘˜ (xğ‘¡ )

(11)

(12)

(cid:17)(cid:17)

.

Therefore, the closed-form policy enables the eï¬ƒcient com-
putation of the target. Combined with function approximation
for the value function [28â€“31], classical value iteration can
be extended to continuous state and action spaces without
discretization. The ï¬tting of the value function is described
by,

ğœ“ğ‘˜+1 = arg min

ğœ“

âˆ‘ï¸

x

(cid:107)ğ‘‰ ğ‘˜+1
tar

(x) âˆ’ ğ‘‰ (x; ğœ“)(cid:107)

ğ‘
ğ‘,

(13)

with (cid:107) Â· (cid:107) ğ‘ being the â„“ğ‘ norm. Iterating between computing
ğ‘‰tar and ï¬tting the value function, learns the optimal value
function ğ‘‰ âˆ— and policy ğœ‹âˆ—.

B. Deriving the Optimal Disturbances

For the adversarial RL formulation, the value function target
contains a max-min optimization described by

ğ‘‰ ğ‘˜+1
tar

(x) = max
u

inf
Î¾âˆˆ Î©

ğ‘Ÿ (x, u) + ğ›¾ğ‘‰ ğ‘˜ (cid:0) ğ‘“ (x, u, Î¾)(cid:1).

(14)

To eï¬ƒciently obtain the value function update, the optimal
action and the optimal disturbance need to be computed in
closed form. We show that this optimization problem can be
solved analytically for the described dynamics and disturbance
models (Section II). Therefore, the adversarial RL problem can
be solved by value iteration.

ğ‘– have a
The resulting optimal actions uâˆ— and disturbances Î¾âˆ—
coherent intuitive interpretation. The optimal actions perform
steepest ascent by following the gradient of the value func-
tion âˆ‡ğ‘¥ğ‘‰. The optimal perturbations perform steepest descent
by following the negative gradient of the value function. The
step-size of policy and adversary is determined by the action
cost ğ‘” or the admissible set Î©. The optimal policy and the
optimal adversary is described by

uğ‘˜ = âˆ‡ Ëœğ‘”

ğ‘‡

(cid:18) ğœ• ğ‘“ğ‘ (.)
ğœ•u

(cid:19)

âˆ‡ğ‘¥ğ‘‰ ğ‘˜

, Î¾ğ‘˜

ğ‘– = âˆ’â„Î©

ğ‘‡

(cid:18) ğœ• ğ‘“ğ‘ (.)
ğœ•Î¾ğ‘–

(cid:19)

âˆ‡ğ‘¥ğ‘‰ ğ‘˜

. (15)

In the following we abbreviate [ğœ• ğ‘“ğ‘ (.)/ğœ•y]ğ‘‡ âˆ‡ğ‘¥ğ‘‰, as zğ‘¦. For
the adversarial policy, â„Î© rescales z ğœ‰ to be on the boundary
of the admissible set. If the admissible set bounds the signal

energy to be smaller than ğ›¼, the disturbance is rescaled to have
the length ğ›¼. Therefore, the adversary is described by

Î©ğ¸ = {Î¾ âˆˆ Rğ‘› | (cid:107)Î¾(cid:107)2 â‰¤ ğ›¼} â‡’ â„ğ¸ (z ğœ‰ ) = ğ›¼

z ğœ‰
(cid:107)z ğœ‰ (cid:107)2

.

(16)

If the amplitude of the disturbance is bounded, the disturbance
performs bang-bang control. In this case the adversarial policy
is described by

Î©ğ´ = {Î¾ âˆˆ Rğ‘› | Î½min â‰¤ Î¾ â‰¤ Î½max}

â‡’ â„ ğ´(z ğœ‰ ) = ğš« sign (cid:0)z ğœ‰ (cid:1) + Âµ,

(17)

with Âµ = (Î½max + Î½min) /2 and ğš« = (Î½max âˆ’ Î½min) /2.
The following theorems derive Equation 15, 16 and 17 for
the optimal policy and the diï¬€erent disturbances. Theorem 1
describes the state adversary, Theorem 2 describes action ad-
versary, Theorem 3 the observation adversary and Theorem 4
describes the model adversary. Following the theorems, we
provide sketches of the proofs for the state and model dis-
turbance. The remaining proofs are analogous. The complete
proofs for all theorems are provided in the appendix. All
solutions are summarized in Table I.

Theorem 1. For the adversarial state disturbance (Equa-
tion 5) with bounded signal energy (Equation 16), the optimal
continuous-time policy ğœ‹ and state disturbance Î¾ğ‘¥ is described
by

ğœ‹(x) = âˆ‡ Ëœğ‘”

(cid:16)

B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰

(cid:17)

,

Î¾ğ‘¥ = âˆ’ğ›¼

âˆ‡ğ‘¥ğ‘‰
(cid:107)âˆ‡ğ‘¥ğ‘‰ (cid:107)2

.

Theorem 2. For the adversarial action disturbance (Equa-
tion 6) with bounded signal energy (Equation 16), the opti-
mal continuous-time policy ğœ‹ and action disturbance Î¾ğ‘¢ is
described by

ğœ‹(x) = âˆ‡ Ëœğ‘”

(cid:16)

B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰

(cid:17)

,

Î¾ğ‘¢ = âˆ’ğ›¼ B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰
(cid:107)B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰ (cid:107)2

.

Proof Sketch Theorem 1 For the admissible set Î©ğ¸ , Equa-
tion 14 can be written with the explicit constraint. This
optimization is described by

ğ‘‰tar = max
u

min
Î¾ğ‘¥

ğ‘Ÿ (x, u) + ğ›¾ğ‘‰ (cid:0) ğ‘“ (x, u, Î¾ğ‘¥)(cid:1)

s.t. Î¾ğ‘‡

ğ‘¥ Î¾ğ‘¥ â‰¤ ğ›¼2.

Substituting the Taylor expansion for ğ‘‰ (xğ‘¡+1), the dynamics
model and the reward, the optimization is described by

ğ‘‰tar = max
u

min
Î¾ğ‘¥

ğ‘Ÿ + ğ›¾ğ‘‰ + ğ›¾âˆ‡ğ‘¥ğ‘‰ğ‘‡ ğ‘“ğ‘Î”ğ‘¡ + ğ›¾O (x, u, Î”ğ‘¡)Î”ğ‘¡

with the higher order terms O (x, u, Î”ğ‘¡). In the continuous-
time limit, the higher-order terms and the discounting disap-
pear, i.e., limÎ”ğ‘¡â†’0 O (x, u, Î”ğ‘¡)=0 and limÎ”ğ‘¡â†’0 exp(âˆ’ğœŒÎ”ğ‘¡)=1.
Therefore, the optimal action is described by

uğ‘¡ = arg max

âˆ‡ğ‘¥ğ‘‰ğ‘‡ B u âˆ’ ğ‘”ğ‘ (u) â‡’ uğ‘¡ = âˆ‡ Ëœğ‘”ğ‘ (cid:0)Bğ‘‡ âˆ‡ğ‘¥ğ‘‰ (cid:1).

u

The optimal state disturbance is described by

Î¾âˆ—
ğ‘¥ = arg min

âˆ‡ğ‘¥ğ‘‰ğ‘‡ Î¾ğ‘¥

s.t.

Î¾ğ‘¥

1
2

(cid:2)Î¾ğ‘‡

ğ‘¥ Î¾ğ‘¥ âˆ’ ğ›¼2(cid:3) â‰¤ 0.

Algorithm 1 Robust Fitted Value Iteration (rFVI)

Input: Model ğ‘“ğ‘ (x, u), Dataset D & Admissible Set Î© ğœ‰
Result: Value Function ğ‘‰ âˆ—(x; ğœ“âˆ—)
while not converged do

// Compute Value Target for x âˆˆ D:
xğœ = xğ‘– + âˆ« ğœ
ğ‘¡ , Î¾ ğœƒ
ğ‘…ğ‘¡ = âˆ« ğ‘¡
ğ‘‰tar (xğ‘–) = âˆ« ğ‘‡

exp(âˆ’ğœŒğœ) ğ‘Ÿğ‘ (xğœ, uğœ)ğ‘‘ğœ + exp(âˆ’ğœŒğ‘¡)ğ‘‰ ğ‘˜ (xğ‘¡ )
ğ›½ exp(âˆ’ğ›½ğ‘¡) ğ‘…ğ‘¡ ğ‘‘ğ‘¡ + exp(âˆ’ğ›½ğ‘‡)ğ‘…ğ‘‡

ğ‘“ğ‘ (xğ‘¡ , uğ‘¡ , Î¾ ğ‘¥

ğ‘¡ , Î¾ğ‘œ

ğ‘¡ , Î¾ğ‘¢

ğ‘¡ )ğ‘‘ğ‘¡

0

0

0

// Fit Value Function:
ğœ“ğ‘˜+1 = arg minğœ“ (cid:205)

xâˆˆ D (cid:107)ğ‘‰tar (x) âˆ’ ğ‘‰ (x; ğœ“)(cid:107) ğ‘

if RTDP rFVI then

// Add samples from ğœ‹ğ‘˜+1 to FIFO buï¬€er D
D ğ‘˜+1 = â„(D ğ‘˜ , {xğ‘˜+1

. . . xğ‘˜+1

ğ‘ })

0

end if
end while

This constrained optimization can be solved using the Karush-
Kuhn-Tucker (KKT) conditions. The resulting optimal adver-
sarial state perturbation is described by

Î¾ğ‘¥ = âˆ’ğ›¼

âˆ‡ğ‘¥ğ‘‰
(cid:107)âˆ‡ğ‘¥ğ‘‰ (cid:107)2

.

(cid:3)

Theorem 3. For the adversarial model disturbance (Equa-
tion 8) with element-wise bounded amplitude (Equation 17),
smooth drift and control matrix (i.e., a, B âˆˆ ğ¶1) and
B(Î¸ + Î¾ğœƒ ) â‰ˆ B(Î¸), the optimal continuous-time policy ğœ‹ and
model disturbance Î¾ğœƒ is described by

ğœ‹(x) = âˆ‡ Ëœğ‘”

(cid:16)

with zğœƒ =

(cid:17)

B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰
(cid:18) ğœ•a(x; Î¸)
ğœ•Î¸

,

Î¾ğœƒ = âˆ’ğš«ğœˆ sign (zÎ¸) + Âµğœˆ

+

ğœ•B(x; Î¸)
ğœ•Î¸

(cid:19)ğ‘‡

ğœ‹(x)

âˆ‡ğ‘¥ğ‘‰,

parameter mean ÂµÎ½ = (Î½max + Î½min) /2 and parameter range
ğš«Î½ = (Î½max âˆ’ Î½min) /2.

Theorem 4. For the adversarial observation disturbance
(Equation 7) with bounded signal energy (Equation 16),
smooth drift and control matrix (i.e., a, B âˆˆ ğ¶1) and
B(x + Î¾ğ‘œ) â‰ˆ B(x), the optimal continuous-time policy ğœ‹ and
observation disturbance Î¾ğ‘œ is described by

ğœ‹(x) = âˆ‡ Ëœğ‘”

(cid:16)

B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰

(cid:17)

,

with zğ‘œ =

(cid:18) ğœ•a(x; Î¸)
ğœ•x

+

ğœ•B(x; Î¸)
ğœ•x

Î¾ğ‘œ = âˆ’ğ›¼ zğ‘œ
(cid:107)zğ‘œ (cid:107)2
(cid:19)ğ‘‡

ğœ‹(x)

âˆ‡ğ‘¥ğ‘‰ .

Proof Sketch Theorem 3 Equation 14 can be written as
ğ‘Ÿ (x, u) + ğ›¾ğ‘‰ (cid:0) ğ‘“ (.)(cid:1) s.t.

(Î¾ğœƒ âˆ’ Âµğœˆ)2 â‰¤ ğš«2
ğœˆ

ğ‘‰tar = max
u

min
Î¾ğœƒ

by replacing the admissible set Î©ğ´ with an explicit constraint.
In the following we abbreviate B(x; Î¸ + Î¾ğœƒ ) as B ğœ‰ and

Figure 3. The learning curves for DP rFVI, DP cFVI, RTDP cFVI and RTDP rFVI averaged over 5 seeds. The shaded area displays the min/max range
between seeds. DP rFVI learns slower compared to DP cFVI on the carpole and Furuta pendulum as the adversary prevents learning. RTDP rFVI does not
learn the task as the adversary is too strong for the online variant of rFVI despite using the identical admissible set as the oï¬„ine variant DP rFVI.

a(x; Î¸ + Î¾ğœƒ ) as a ğœ‰ . Substituting the Taylor expansion for
ğ‘‰ (xğ‘¡+1), the dynamics models and reward yields
ğ‘‰tar âˆ’ ğ›¾ğ‘‰
Î”ğ‘¡

(cid:2)ğ›¾âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a ğœ‰ + B ğœ‰ u(cid:1) + ğ›¾O (.) âˆ’ ğ‘”ğ‘(cid:3)

= ğ‘ğ‘ + max
u

min
Î¾

that is robust to changes in the dynamics. Therefore, the value
function target is computed using
x, uğ‘˜ (cid:17)

x, uğ‘˜ , Î¾ğ‘˜

+ ğ›¾ğ‘‰ ğ‘˜ (cid:16)

ğ‘‰ ğ‘˜+1
tar = ğ‘Ÿ

ğ‘¢ , Î¾ğ‘˜

ğ‘¥ , Î¾ğ‘˜

ğ‘œ , Î¾ğ‘˜
ğœƒ

(18)

(cid:17)(cid:17)

(cid:16)

(cid:16)

ğ‘“

In the continuous-time limit the optimal action and disturbance
is determined by

uâˆ—, Î¾âˆ—

ğœƒ = max
u

min
Î¾

(cid:2)âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a ğœ‰ + B ğœ‰ u(cid:1) âˆ’ ğ‘”ğ‘ (u)(cid:3) .

This nested max-min optimization can be solved by ï¬rst
solving the inner optimization w.r.t. to u and substituting this
solution into the outer maximization. The Lagrangian for the
optimal model disturbance is described by

Î¾âˆ— = arg min

âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a ğœ‰ + B ğœ‰ u(cid:1) +

Î¾

1

2 Î»ğ‘‡ (cid:16)

(Î¾ğœƒ âˆ’ Âµğœˆ)2 âˆ’ ğš«2
ğœˆ

(cid:17)

.

where actions uğ‘˜ and disturbances Î¾ğ‘– are determined ac-
cording to Table I. Iterating between computing the target
and ï¬tting the value function network (Equation 13) enables
the learning of the robust optimal value function and robust
optimal policy.

N-Step Value Function Target The learning can be acceler-
ated by using the exponentially weighted ğ‘›-step value target
instead of the 1-step target, as shown by the classical eligibility
traces [17], generalized advantage estimation [33, 34] or model
based value-expansion [35]. In the continuous limit this target
is described by

Using the KKT conditions this optimization can be solved.
The stationarity condition yields

ğ‘‰tar (x0) =

zğœƒ + Î»ğ‘‡ (Î¾ğœƒ âˆ’ Âµğœˆ) (cid:66) 0 â‡’ Î¾âˆ—

ğœƒ = âˆ’zğœƒ (cid:11) Î» + Âµğœˆ

with the elementwise division (cid:11). Using the primal feasibility
and the complementary slackness, the optimal Î»âˆ— can be com-
puted. The resulting optimal model disturbance is described by

ğœƒ (u) = âˆ’ğš«ğœˆ sign (cid:0)zğœƒ (u)(cid:1) + Âµğœˆ
Î¾âˆ—
as zğœƒ (cid:11) (cid:107)zğœƒ (cid:107)1 = sign(zğœƒ ). The action can be computed by
ğœƒ (u)(cid:1) u(cid:3) âˆ’ ğ‘”ğ‘ (u).

ğœƒ (u)) + B (cid:0)Î¾âˆ—

âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:2)a(Î¾âˆ—

uâˆ— = arg max

u

Due to the envelope theorem [32], the extrema is described by

B(x; Î¸ + Î¾âˆ—

ğœƒ (u))ğ‘‡ âˆ‡ğ‘¥ğ‘‰ âˆ’ ğ‘”ğ‘ (u) (cid:66) 0.

This expression cannot be solved without approximation as
B does not necessarily be invertible w.r.t. Î¸. Approximating
B(x; Î¸ + Î¾âˆ—(u)) â‰ˆ B(x; Î¸), lets one solve for u. In this case
the optimal action uâˆ— is described by uâˆ—=âˆ‡ Ëœğ‘”(B(x; Î¸)ğ‘‡ âˆ‡ğ‘¥ğ‘‰).
This approximation implies that neither agent or the adversary
can react to the action of the other and must choose simulta-
(cid:3)
neously. This assumption is common in prior works [5].

C. Algorithm
Using the theoretical insights from the previous section, robust
ï¬tted value iteration can be derived. Instead of computing the
value function target using only the optimal action as in cFVI,
rFVI includes the four adversaries to learn a optimal policy

âˆ« ğ‘‡

0
âˆ« ğ‘¡

0
âˆ« ğ‘¡

0

ğ›½ exp(âˆ’ğ›½ğ‘¡) ğ‘…ğ‘¡ ğ‘‘ğ‘¡ + exp(âˆ’ğ›½ğ‘‡)ğ‘…ğ‘‡ ,

exp(âˆ’ğœŒğœ) ğ‘Ÿğ‘ (xğœ, uğœ)ğ‘‘ğœ + exp(âˆ’ğœŒğ‘¡)ğ‘‰ ğ‘˜ (xğ‘¡ ),

(cid:16)

ğ‘“ğ‘

x, uğ‘˜ , Î¾ğ‘˜

ğ‘¥ , Î¾ğ‘˜

ğ‘¢ , Î¾ğ‘˜

ğ‘œ , Î¾ğ‘˜
ğœƒ

(cid:17)

ğ‘‘ğœ + x0,

ğ‘…ğ‘¡ =

xğ‘¡ =

where ğ›½ is the exponential decay factor. In practice we treat
ğ›½ as the hyperparameter and select ğ‘‡ such that the weight of
the ğ‘…ğ‘‡ is exp (âˆ’ğ›½ğ‘‡) (cid:66) 10âˆ’4.

Admissible Set For the state, action and observation adversary
the signal energy is bounded. We limit the energy of Î¾ğ‘¥, Î¾ğ‘¢ and
Î¾ğ‘œ as the non-adversarial disturbances are commonly modeled
as multivariate Gaussian distribution. Therefore, the average
energy is determined by the noise covariance matrix. For the
model parameters Î¸ a common practice is to assume that
the approximate model parameters have an model error of up
to Â±15% [1, 2]. Hence, we bound the amplitude of each com-
ponent. To not overï¬t to the deterministic worst case system
of ğ‘‰ ğ‘˜ and enable the discovery of good actions, the amplitude
of the adversarial actions of ğœ‰ğ‘¥, ğœ‰ğ‘¢, ğœ‰ğ‘œ is modulated using a
Wiener process. This random process allows a continuous-time
formulation that is agnostic to the sampling frequency.

Oï¬„ine and Online rFVI The proposed approach is oï¬€-policy
as the samples in the replay memory do not need to originate
from the current policy ğœ‹ğ‘˜ . Therefore, the dataset can either
consist of a ï¬xed dataset or be updated within each iteration.
In the oï¬„ine dynamic programming case, the dataset contains
samples from the compact state domain X. We refer to the

0255075100Episodeâˆ’40âˆ’200RewardPendulumDPcFVI-Î»=0.85DPrFVI-Î»=0.85RTDPcFVI-Î»=0.85RTDPrFVI-Î»=0.85050100150200250Episodeâˆ’150âˆ’100âˆ’500RewardCartpoleDPcFVI-Î»=0.90DPrFVI-Î»=0.85RTDPcFVI-Î»=0.45RTDPrFVI-Î»=0.45050100150200250Episodeâˆ’400âˆ’300âˆ’200âˆ’1000RewardFurutaPendulumDPcFVI-Î»=0.95DPrFVI-Î»=0.70RTDPcFVI-Î»=0.40RTDPrFVI-Î»=0.25Figure 4.
The tracked trajectories of DP rFVI and DP cFVI on the physical cartpole with varied pendulum masses. DP rFVI is capable to perform the
swing-up for the varying pendulum mass. The qualitative performance does not change if the weight is added or reduced. DP cFVI can swing up and balance
all varied pendulums but it requires more pre-swings for all conï¬gurations. During balancing the cart is not centered and the cart oscillates around the center
for DP cFVI. The pendulum must signiï¬cantly deviate from the target position before the DP cFVI policy breaks the stiction of the linear actuator. In contrast,
the higher actions of DP rFVI break the stiction and balance the pendulum with the cart centered.

Figure 5. The tracked trajectories for DP rFVI and DP cFVI on the Furuta
pendulum for diï¬€erent pendulum weights. The trajectories of rFVI do not
signiï¬cantly change when the pendulum mass altered. For DP cFVI the
trajectories start to change when an additional weight is added. For these
system dynamics, DP cFVI requires some failed swing-ups until the policy
can balance the pendulum.

oï¬„ine variant as DP rFVI. In the online case, the replay
memory is updated with samples generated by the current
policy ğœ‹ğ‘˜ . Every iteration the states of the previous ğ‘›-rollouts
are added to the data and replace the oldest samples. This
online update of state distribution performs real-time dynamic
programming (RTDP) [36]. We refer to the online variant as
RTDP rFVI. The pseudo code of DP cFVI and RTDP rFVI is
summarized in Algorithm 1.

IV. Experiments
In the following non-linear control experiments we want to
answer the following questions:
Q1: Does rFVI learn a robust policy that can be successfully
transferred to the physical systems with diï¬€erent model pa-
rameters?
Q2: How does the policy obtained by rFVI diï¬€er qualitatively
compared to cFVI and the deep RL baselines?
To answer these questions, we apply the algorithms to perform
the swing-up task of the under-actuated cartpole (Fig. 4)
and Furuta pendulum (Fig. 5). Both systems are standard
environments for benchmarking non-linear control policies.
We focus only on these two systems to perform extensive
robustness experiments on the actual physical systems. To test
the robustness with respect to model parameters, we attach
small weights to the passive pendulum.

A. Experimental Setup
Systems The physical cartpole and Furuta pendulum are
manufactured by Quanser [37] and voltage controlled. For the

approximate simulation model we use the rigid-body dynamics
model with the parameters supplied by the manufacturer. If we
add negative weights to the pendulum, we attach the weights
to the opposite lever of the pendulum. This moves the center
of mass of the pendulum closer to the rotary axis. Therefore,
this shift reduces the downward force and is equivalent to a
lower pendulum mass.

Baselines The performance is compared to the actor-critic
deep RL methods: DDPG [38], SAC [39] and PPO [34]. The
robustness evaluation is only performed for the best performing
baselines on the nominal physical system. The performance of
all baselines on the nominal system is summarized in Table
IV (Appendix). The initial state distribution is abbreviated
by {SAC, PPO, DDPG}-U for a uniform distribution of the
pendulum angle and {SAC, PPO, DDPG}-N for a Gaussian
distribution. The baselines with Gaussian initial state distri-
bution did not achieve robust performance on the nominal
system. If the baseline uses uniform domain randomization
the acronym is appended with UDR.

Evaluation To evaluate rFVI and the baselines we separately
compare the state and action reward as these algorithms
optimize a diï¬€erent objectives. Hence, these algorithms trade-
oï¬€ state and action associated rewards diï¬€erently. It is expected
that the worst-case optimization uses higher actions to prevent
deviation from the optimal trajectory. On the physical system,
the performance is evaluated using the 25th, 50th and 75th re-
ward percentile as the reward distribution is multi-modal.

B. Experimental Results
The learning curves averaged over 5 seeds of DP rFVI and
RTDP rFVI are shown in Figure 3. The results in simulation
are summarized in Table II. DP rFVI learns a policy that
obtains slightly lower reward compared to cFVI and the deep
RL baselines. This lower reward is expected as the worst-
case optimization yields conservative policies [26]. DP rFVI
exhibits low variance between seeds but learns slower than
DP cFVI. This slower learning is caused by the adversary
which counteracts the learning progress. RTDP rFVI does
not learn to successfully swing-up the Furuta pendulum and
cartpole. Despite using the same admissible set for both
variants, the adversary is too powerful for RTDP rFVI and

DPrFVI(ours)âˆ†mp=âˆ’20g/âˆ’16%âˆ†mp=âˆ’10g/âˆ’08%âˆ†mp=+00g/âˆ’00%âˆ†mp=+10g/+08%âˆ†mp=+20g/+16%âˆ†mp=+50g/+39%DPcFVIDPrFVI(ours)âˆ†mp=âˆ’5gâˆ†mp=âˆ’2gâˆ†mp=+0gâˆ†mp=+1gâˆ†mp=+2gâˆ†mp=+5gDPcFVIFigure 6. The 25th, 50th and 75th reward percentile for the physical Furuta pendulum and cartpole with varied pendulum weights. DP rFVI achieves higher
state reward for real-world systems compared to the baselines. For the diï¬€erent weights the reward remains nearly constant. For the Furuta pendulum the
action cost is signiï¬cantly higher compared to the baselines as the DP rFVI causes a chattering during balancing due to the high actions and minor time
delays in the control loop. If only the swing-up phase is considered the rewards are comparable.

Table II
The average rewards in simulation.

Algorithm

State Rwd

Action Rwd

State Rwd

Action Rwd

State Rwd

Action Rwd

Sim Pendulum

Sim Cartpole

Sim Furuta Pendulum

SAC-U

DP cFVI

DP rFVI (ours)

SAC-U & UDR

âˆ’24.5 Â± 00.1 âˆ’08.3 Â± 00.4 âˆ’15.5 Â± 05.4 âˆ’11.6 Â± 02.0 âˆ’37.0 Â± 13.2 âˆ’04.8 Â± 02.7
âˆ’22.3 Â± 03.0 âˆ’08.3 Â± 03.2 âˆ’14.4 Â± 02.8 âˆ’09.9 Â± 02.2 âˆ’22.3 Â± 02.5 âˆ’05.9 Â± 01.3
âˆ’21.1 Â± 05.2 âˆ’09.4 Â± 04.7 âˆ’13.9 Â± 02.4 âˆ’10.4 Â± 02.3 âˆ’22.6 Â± 02.9 âˆ’05.9 Â± 01.5
âˆ’22.6 Â± 02.6 âˆ’08.9 Â± 01.9 âˆ’13.9 Â± 02.6 âˆ’10.3 Â± 02.5 âˆ’22.1 Â± 02.6 âˆ’06.1 Â± 01.4
âˆ’20.9 Â± 01.2 âˆ’10.6 Â± 01.3 âˆ’16.4 Â± 04.7 âˆ’11.8 Â± 04.9 âˆ’24.6 Â± 03.6 âˆ’05.5 Â± 01.9
DDPG-U
DDPG-U & UDR âˆ’21.0 Â± 04.8 âˆ’11.5 Â± 01.6 âˆ’14.5 Â± 03.5 âˆ’12.8 Â± 03.5 âˆ’26.1 Â± 02.4 âˆ’06.2 Â± 01.9
âˆ’24.5 Â± 04.4 âˆ’09.0 Â± 02.4 âˆ’82.2 Â± 83.1 âˆ’04.9 Â± 01.1 âˆ’34.9 Â± 12.6 âˆ’03.4 Â± 01.1
PPO-U
âˆ’24.5 Â± 00.2 âˆ’11.1 Â± 03.0 âˆ’55.9 Â± 23.1 âˆ’09.8 Â± 05.2 âˆ’42.9 Â± 04.6 âˆ’06.1 Â± 02.4

PPO-U & UDR

prevents learning. In this case policy does not discover the
positive reward at the top as the adversary prevents balancing.
Therefore, the policy is too pessimistic and converges to a bad
local optima. The ablation study in the appendix shows, that
RTDP rFVI learns a successful policy if the admissible sets
are reduced. To overcome this problem one would need to bias
the exploration to be optimistic.

The performance on the physical systems is summarized in
Figure 4, 5 and 7.1 Across the diï¬€erent parameters of both
systems, rFVI achieves the highest state reward compared
to cFVI and the deep RL baselines. The best performing
trajectories between diï¬€erent conï¬gurations are nearly iden-
tical. Only for the cartpole the failure rates slightly increases
when positive weights are added. In this case the pendulum
is swung-up but cannot be stabilized, due to the backslash of
the linear actuator. For cFVI and the deep RL baselines even
the best trajectories deteriorate when weights are added to the
pendulum. This is especially notable in Figure 7, where the
deep RL baselines start to explore the complete state space
when additional weights are added. The high state rewards
of rFVI are obtained at the expense of higher action cost,
which can be higher compared to some baselines on the
physical system. To summarize rFVI obtains a robust policy
that performs the swing-up with low state reward and more
consistency than the baselines but uses higher actions.

Compared to the policies obtained by DP cFVI and the deep
RL baselines, DP rFVI policy exerts higher actions. Therefore,
DP rFVI achieves the robustness compared to the baselines
by utilizing a stiï¬€er feedback policy approaching bang-bang
control. The higher actions are only observed on the physical

1Videos of the experiments at https://sites.google.com/view/rfvi.

system where the deviation from the optimal trajectory is
inevitable. In simulation the action cost are comparable to
the baselines. Therefore, the high actions originate from the
feedback-term of the non-linear policy that tries to compensate
the tracking error. The stiï¬€er feedback policy is expected as
traditional robust control approaches yield high feedback gains
[40]. The stiï¬€ness of the rFVI policy is clearly visible in
Figure 2. On the ridge leading up to the balancing point, the
policy directly applies maximum action, if one deviates from
the center of the ridge. In contrast, DP cFVI has a gradient
that slightly increases the action when one deviates from the
optimal trajectory.

For the cartpole the higher actions achieve much better per-
formance. DP rFVI achieves stabilization of the cart in the
center of the track (Figure 4). The higher actions break the
stiction and overcome the backslash of the geared cogwheel
actuator during the balancing. For DP cFVI and the baselines,
the cart oscillates around the center. The pendulum angle has
to deviate signiï¬cantly until the actions become large and
break the stiction. For the Furuta pendulum the high actions
are more robust during the swing-up phase. However, during
the balancing the lower link starts to chatter due to the high-
frequency switching between high actions. This switching is
caused by minor delays in the control
loop and the very
low friction of the pendulum. About 90% of the action cost
of DP rFVI for the Furuta pendulum is incurred during the
stabilization. DP cFVI incurs only 10% of the action cost
during stabilization. If one would only consider the swing-
up phase, the reward of DP rFVI is higher compared to the
baselines. To summarize, high-stiï¬€ness feedback policies are
robust to changes in dynamics. However, this robustness can
also make the resulting policies more sensitive to other sources
error not included in the speciï¬cation, e.g., control delays.

Besides the performance evaluation of DP rFVI, the experi-
ments show that DP cFVI achieves comparable performance
than the deep RL baselines with domain randomization.
Despite overï¬tting to a deterministic approximate model,
DP cFVI is able to be robust against some variations in
model parameters. This result suggests that for these two
physical systems, preventing the distribution shift by solving

-5g-21%-2g-8%0g0%1g4%3g12%5g21%0.0-50.0-100.0-150.0-200.0-250.0RewardFuruta PendulumState Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%State Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%State Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%State Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%Action Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%Action Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%Action Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%Action Rwd-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp0.0-50.0-100.0-150.0RewardCartpoleDP rFVI (ours)DP cFVIPPO-U & UDRSAC-USAC-U & UDRDDPG-U & UDRMedian-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mpFigure 7. The roll-outs of DP rFVI, DP cFVI and the deep RL baselines with domain randomization the physical Furuta pendulum. The diï¬€erent columns
correspond to diï¬€erent pendulum masses. The deviation from the dashed center line corresponds to the joint velocity. DP rFVI achieves a consistent swing-up
for the diï¬€erent pendulum masses. In contrast to DP rFVI, the baselines start to deviate strongly from trajectories on the nominal system. When weights are
added the baselines start to cover the complete state-space. A ï¬gure displaying the roll-outs per algorithm is provided in the appendix.

for the policy on the compact state domain obtains a policy
with comparable robustness as uniform domain randomization.
Furthermore, the deep RL performance increases with larger
state distribution and using the maximum-entropy formulation
on the physical system. Therefore, the experiments suggest that
the state-distribution of the policy aï¬€ects the policy robustness.
This correlation should be investigated in-depth in future work.

V. Related Work

Robust Policies Learning robust policies has been approached
by (1) changing the optimization objective that balances risk
and reward [41â€“43], (2) introducing a adversary to optimize
the worst-case performance [5â€“7, 44, 45] and (3) randomizing
the dynamics model to be robust to the model parameters [2,
2â€“4, 9, 46]. In robotics, domain randomization is most widely
used to achieve successful sim2real
transfer. For example
domain randomization was used for in-hand manipulation [46],
ball-in-a-cup [2], locomotion [9], manipulation [3, 4].

In this paper we focus on the adversarial formulation. This
approach has been extensively used for continuous control
tasks [7, 8, 13, 18, 23]. For example, Pinto et. al. [7, 18]
used a separate agent as adversary controlling an additive
control input. This adversary maximized the negative reward
using a standard actor-critic learning algorithm. The agent
and adversary do not share any information. Therefore, an
additional optimization is required to optimize the adversary. A
diï¬€erent approach by Mandlekar et. al. [8] used auxiliary loss
to maximize the policy actions. Both approaches are model-
free. In contrast to these approaches, our approach derives
the adversarial perturbations using analytic expressions de-
rived directly from the Hamilton-Jacobi-Isaacs (HJI) equation.
Therefore, our approach shares knowledge between the actor
and adversary due to a shared value function and requires
no additional optimization. However, our approach requires
knowledge of the model to compute the actions and pertur-
bations analytically. The approach is similar to Morimoto and
Doya [13]. In contrast to this work, we extend the analytic
solutions to state, action, observation and model disturbances,

do not require a control-aï¬ƒne disturbance model, and use the
constrained formulation rather than the penalized formulation.

Continuous-Time Reinforcement Learning Various ap-
proaches have been proposed to solve the Hamilton-Jacobi-
Bellman (HJB) diï¬€erential equation with the machine learning
toolset. These methods can be divided into trajectory and
state-space based methods. Trajectory based methods solve the
stochastic HJB along a trajectory using path integral control
[47â€“49] or forward-backward stochastic diï¬€erential equations
[50, 51]. State-space based methods solve the HJB globally
to obtain a optimal non-linear controller applicable on the
complete state domain. Classical approaches discretize the
problem and solve the HJB or HJI using a PDE solver [5].
To overcome the curse of dimensionality of the grid based
methods, machine learning methods have proposed to use vari-
ous function approximators ranging from polynomial functions
[52, 53], kernels [54] to deep networks [12, 14, 55, 56]. In this
paper, we utilize the state-space based approach solve the HJI
via dynamic programming. The value function is approximated
using a deep network and optimal value function is solved via
value iteration.

VI. Conclusion and Future Work

We proposed robust ï¬tted value iteration (rFVI). This al-
gorithm solves the adversarial continuous-time reinforcement
learning problem for continuous states, action and adversary
via value iteration. To enable the eï¬ƒcient usage of value
iteration, we presented analytic expressions for the adversar-
ial disturbances for the state, action, observation and model
adversary. Therefore, our derivations extend existing analytic
expressions for continuous time RL from literature [11â€“14].
The non-linear control experiments using the physical cartpole
and Furuta pendulum showed that rFVI is robust to varia-
tions in model parameters and obtains higher state-rewards
compared to the deep RL baselines with uniform domain
randomization. The robustness of rFVI is achieved by utilizing
a stiï¬€er feedback policy that exerts higher actions compared
to the baselines.

In future work we plan to learn the admissible sets from
data from the physical system. In domain randomization, the
automatic tuning of the parameter distributions has been very
successful [2, 4]. However, these approaches are not directly
transferable as one would also need to estimate the admissible
set of the action, state and observation and not only the system
parameter distribution as in domain randomization.

Acknowledgements

M. Lutter was an intern at Nvidia during this project. A. Garg
to
was partially supported by CIFAR AI Chair. We also want
thank Fabio Muratore, Joe Watson and the RSS reviewers for
their feedback. Furthermore, we want
to thank the open-source
projects SimuRLacra [57], MushroomRL [58], NumPy [59] and
PyTorch [60].

References

[1] F. Muratore, F. Treede, M. Gienger, and J. Peters, â€œDo-
main randomization for simulation-based policy opti-
mization with transferability assessment,â€ in Conference
on Robot Learning (CoRL), 2018.

[2] F. Muratore, C. Eilers, M. Gienger, and J. Peters, â€œData-
eï¬ƒcient domain randomization with bayesian optimiza-
tion,â€ IEEE Robotics and Automation Letters (RAL),
2021.

[3] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin,
J. Issac, N. Ratliï¬€, and D. Fox, â€œClosing the sim-to-real
loop: Adapting simulation randomization with real world
experience,â€ 2019.

[4] F. Ramos, R. C. Possas, and D. Fox, â€œBayessim: adap-
tive domain randomization via probabilistic inference for
robotics simulators,â€ 2019.

[5] S. Bansal, M. Chen, S. Herbert, and C. J. Tomlin,
â€œHamilton-Jacobi reachability: A brief overview and re-
cent advances,â€ 2017.

[6] R. Isaacs, Diï¬€erential games: a mathematical

theory
with applications to warfare and pursuit, control and
optimization. Courier Corporation, 1999.

[7] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, â€œRo-
bust adversarial reinforcement learning,â€ in International
Conference on Machine Learning (ICML), 2017.

[8] A. Mandlekar, Y. Zhu, A. Garg, L. Fei-Fei, and
S. Savarese, â€œAdversarially robust policy learning: Active
construction of physically-plausible perturbations,â€ in In-
ternational Conference on Intelligent Robots and Systems
(IROS), 2017.

[9] Z. Xie, X. Da, M. van de Panne, B. Babich, and A. Garg,
â€œDynamics Randomization Revisited: A Case Study for
Quadrupedal Locomotion,â€ in IEEE International Con-
ference on Robotics and Automation (ICRA), 2021.
[10] J. GarcÄ±a and F. FernÃ¡ndez, â€œA comprehensive survey
learning,â€ Journal of Machine

on safe reinforcement
Learning Research, 2015.

[11] S. E. Lyshevski,

control of nonlinear
continuous-time systems: design of bounded controllers
via generalized nonquadratic functionals,â€ in American

â€œOptimal

Control Conference (ACC), vol. 1, pp. 205â€“209, IEEE,
1998.

[12] K. Doya, â€œReinforcement learning in continuous time and
space,â€ Neural computation, vol. 12, no. 1, pp. 219â€“245,
2000.

[13] J. Morimoto and K. Doya, â€œRobust reinforcement learn-

ing,â€ Neural computation, 2005.

[14] M. Lutter, B. Belousov, K. Listmann, D. Clever, and
J. Peters, â€œHJB optimal feedback control with deep
diï¬€erential value functions and action constraints,â€ in
Conference on Robot Learning (CoRL), 2019.

[15] R. Bellman, Dynamic Programming. USA: Princeton

University Press, 1957.

[16] M. L. Puterman, Markov decision processes: discrete
stochastic dynamic programming. John Wiley & Sons,
1994.

[17] R. S. Sutton, A. G. Barto, et al., Introduction to rein-
forcement learning. MIT press Cambridge, 1998.
[18] L. Pinto, J. Davidson, and A. Gupta, â€œSupervision via
competition: Robot adversaries for learning tasks,â€ in
International Conference on Robotics and Automation
(ICRA), 2017.

[19] H. Zhang, H. Chen, C. Xiao, B. Li, D. Boning, and C.-
J. Hsieh, â€œRobust deep reinforcement learning against
adversarial perturbations on observations,â€ arXiv preprint
arXiv:2003.08938, 2020.

[20] M. Heger, â€œConsideration of risk in reinforcement learn-
ing,â€ in Machine Learning Proceedings, Elsevier, 1994.
[21] M. L. Littman, â€œMarkov games as a framework for
multi-agent reinforcement learning,â€ in Machine learning
proceedings, Elsevier, 1994.

[22] A. Nilim and L. El Ghaoui, â€œRobust control of markov
decision processes with uncertain transition matrices,â€
Operations Research, 2005.

[23] C. Tessler, Y. Efroni, and S. Mannor, â€œAction robust
reinforcement learning and applications in continuous
control,â€ in International Conference on Machine Learn-
ing (ICML), 2019.

[24] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and
G. Chowdhary, â€œRobust deep reinforcement learning with
adversarial attacks,â€ arXiv preprint arXiv:1712.03632,
2017.

[25] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine,
and S. Russell, â€œAdversarial policies: Attacking deep re-
inforcement learning,â€ arXiv preprint arXiv:1905.10615,
2019.

[26] H. Xu and S. Mannor, â€œRobustness and generalization,â€

Machine learning, 2012.

[27] M. Lutter, S. Mannor, J. Peters, D. Fox, and A. Garg,
â€œValue Iteration in Continuous Actions, States and
Time,â€ in International Conference on Machine Learning
(ICML), 2021.

[28] D. Ernst, P. Geurts, and L. Wehenkel, â€œTree-based
batch mode reinforcement learning,â€ Journal of Machine
Learning Research, 2005.

[29] A.

massoud

Farahmand, M.

Ghavamzadeh,

C. SzepesvÃ¡ri, and S. Mannor, â€œRegularized ï¬tted
q-iteration for planning in continuous-space markovian
decision problems,â€ in American Control Conference
(ACC), IEEE, 2009.

[30] M. Riedmiller, â€œNeural ï¬tted q iterationâ€“ï¬rst experi-
ences with a data eï¬ƒcient neural reinforcement learning
method,â€ in European Conference on Machine Learning,
Springer, 2005.

[31] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-
ness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.
Fidjeland, G. Ostrovski, et al., â€œHuman-level control
through deep reinforcement learning,â€ Nature, vol. 518,
no. 7540, p. 529, 2015.

[32] M. Carter, Foundations of mathematical economics. MIT

press, 2001.

[33] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and
P. Moritz, â€œTrust region policy optimization.,â€ in Icml,
vol. 37, pp. 1889â€“1897, 2015.

[34] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
O. Klimov, â€œProximal policy optimization algorithms,â€
arXiv preprint arXiv:1707.06347, 2017.

[35] V. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E.
Gonzalez, and S. Levine, â€œModel-based value estimation
for eï¬ƒcient model-free reinforcement learning,â€ arXiv
preprint arXiv:1803.00101, 2018.

[36] A. G. Barto, S. J. Bradtke, and S. P. Singh, â€œLearning
to act using real-time dynamic programming,â€ Artiï¬cial
intelligence, 1995.

[37] Quanser, â€œQuanser courseware and resources.â€ https:
//www.quanser.com/solution/control-systems/, 2018.
[38] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,
Y. Tassa, D. Silver, and D. Wierstra, â€œContinuous con-
trol with deep reinforcement learning,â€ arXiv preprint
arXiv:1509.02971, 2015.

[39] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft
actor-critic: Oï¬€-policy maximum entropy deep reinforce-
ment learning with a stochastic actor,â€ in International
Conference on Machine Learning, 2018.

[40] K. J. Ã…strÃ¶m, L. Neumann, and P.-O. Gutman, â€œA com-
parison between robust and adaptive control of uncertain
systems,â€ IFAC Proceedings Volumes, 1987.

[41] V. S. Borkar, â€œA sensitivity formula for risk-sensitive
cost and the actorâ€“critic algorithm,â€ Systems & Control
Letters, 2001.

[42] Y. Chow, A. Tamar, S. Mannor, and M. Pavone, â€œRisk-
sensitive and robust decision-making: a cvar optimization
approach,â€ arXiv preprint arXiv:1506.02188, 2015.
[43] H. Bharadhwaj, A. Kumar, N. Rhinehart, S. Levine,
F. Shkurti, and A. Garg, â€œConservative safety critics for
exploration,â€ in International Conference on Learning
Representations (ICLR), 2021.

[44] A. Tamar, H. Xu, and S. Mannor, â€œScaling up ro-
bust mdps by reinforcement learning,â€ arXiv preprint
arXiv:1306.6189, 2013.

[45] J. Harrison*, A. Garg*, B. Ivanovic, Y. Zhu, S. Savarese,
L. Fei-Fei, and M. Pavone (* equal contribution),

â€œAdaPT: Zero-Shot Adaptive Policy Transfer for Stochas-
tic Dynamical Systems,â€ in International Symposium on
Robotics Research (ISRR), 2017.

[46] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefow-
icz, B. McGrew, J. Pachocki, A. Petron, M. Plappert,
G. Powell, A. Ray, et al., â€œLearning dexterous in-hand
manipulation,â€ The International Journal of Robotics
Research (Ä²RR), 2020.

[47] H. J. Kappen, â€œLinear theory for control of nonlinear
stochastic systems,â€ Physical review letters, 2005.
[48] E. Todorov, â€œLinearly-solvable markov decision prob-
information processing

lems,â€ in Advances in neural
systems, 2007.

[49] E. Theodorou, J. Buchli, and S. Schaal, â€œReinforcement
learning of motor skills in high dimensions: A path inte-
gral approach,â€ in International Conference on Robotics
and Automation (ICRA), IEEE, 2010.

[50] M. Pereira, Z. Wang, I. Exarchos, and E. Theodorou,
â€œLearning deep stochastic optimal control policies using
forward-backward sdes,â€ in Robotics: science and sys-
tems, 2019.

[51] M. A. Pereira, Z. Wang,

I. Exarchos, and E. A.
Theodorou, â€œSafe optimal control using stochastic bar-
rier functions and deep forward-backward sdes,â€ arXiv
preprint arXiv:2009.01196, 2020.

[52] X. Yang, D. Liu, and D. Wang, â€œReinforcement learning
for adaptive optimal control of unknown continuous-time
nonlinear systems with input constraints,â€ International
Journal of Control, 2014.

[53] D. Liu, D. Wang, F.-Y. Wang, H. Li, and X. Yang,
â€œNeural-network-based online hjb solution for optimal ro-
bust guaranteed cost control of continuous-time uncertain
nonlinear systems,â€ IEEE transactions on cybernetics,
vol. 44, no. 12, pp. 2834â€“2847, 2014.

[54] P. Hennig, â€œOptimal reinforcement learning for gaussian
systems,â€ in Advances in Neural Information Processing
Systems, 2011.

[55] Y. Tassa and T. Erez, â€œLeast squares solutions of the
HJB equation with neural network value-function approx-
imators,â€ IEEE transactions on neural networks, vol. 18,
no. 4, pp. 1031â€“1041, 2007.

[56] J. Kim, J. Shin, and I. Yang, â€œHamilton-jacobi deep
q-learning for deterministic continuous-time systems
with lipschitz continuous controls,â€ arXiv preprint
arXiv:2010.14087, 2020.

[57] F. Muratore, â€œSimurlacra - a framework for reinforcement
learning from randomized simulations.â€ https://github.
com/famura/SimuRLacra, 2020.

[58] C. Dâ€™Eramo, D. Tateo, A. Bonarini, M. Restelli, and
â€œMushroomrl: Simplifying reinforcement
https://github.com/MushroomRL/

J. Peters,
learning
research.â€
mushroom-rl, 2020.

[59] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gom-
mers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor,
S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer,
M. H. van KerkwÄ³k, M. Brett, A. Haldane, J. F. del RÃ­o,

M. Wiebe, P. Peterson, P. GÃ©rard-Marchant, K. Sheppard,
T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E.
Oliphant, â€œArray programming with NumPy,â€ Nature,
2020.

[60] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Rai-
son, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala, â€œPytorch: An imperative style,
high-performance deep learning library,â€ in Advances in
Neural Information Processing Systems 32, 2019.
[61] H. K. Khalil and J. W. Grizzle, Nonlinear systems, vol. 3.

Prentice hall Upper Saddle River, NJ, 2002.
and
[62] F. Berkenkamp, M. Turchetta, A. Schoellig,
learning
A. Krause, â€œSafe model-based reinforcement
with stability guarantees,â€ in Advances in neural infor-
mation processing systems, 2017.

[63] S. M. Richards, F. Berkenkamp, and A. Krause, â€œThe
lyapunov neural network: Adaptive stability certiï¬cation
for safe learning of dynamical systems,â€ arXiv preprint
arXiv:1808.00924, 2018.

[64] J. Z. Kolter and G. Manek, â€œLearning stable deep
dynamics models,â€ in Advances in Neural Information
Processing Systems (NeurIPS), 2019.

[65] Y.-C. Chang, N. Roohi, and S. Gao, â€œNeural lyapunov
control,â€ in Advances in Neural Information Processing
Systems (NeurIPS), 2019.

Next Page

Appendix

A. State Disturbance Proof

Theorem. For the adversarial state disturbance (Equation 5)
with bounded in signal energy (Equation 16), the optimal con-
tinuous time policy ğœ‹ğ‘˜ and state disturbance Î¾ğ‘˜
ğ‘¥ is described
by

ğœ‹ğ‘˜ (x) = âˆ‡ Ëœğ‘”

(cid:16)

B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰ ğ‘˜ (cid:17)

Î¾ğ‘˜ = âˆ’ğ›¼

âˆ‡ğ‘¥ğ‘‰ ğ‘˜
(cid:107)âˆ‡ğ‘¥ğ‘‰ ğ‘˜ (cid:107)2

.

This solution is intuitive as the adversary wants to minimize
the reward and this disturbance performs steepest descent using
the largest step-size. Therefore, the perturbation is always on
(cid:3)
the constraint.

B. Action Disturbance Proof
Theorem. For the adversarial action disturbance (Equation 5)
with bounded in signal energy (Equation 16), the optimal con-
tinuous time policy ğœ‹ğ‘˜ and action disturbance Î¾ğ‘˜
ğ‘¢ is described
by

Proof. Equation 14 can be formulated with the explicit con-
straint, i.e.,

ğœ‹ğ‘˜ (x) = âˆ‡ Ëœğ‘”

(cid:16)

B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰ ğ‘˜ (cid:17)

ğ‘¢ = âˆ’ğ›¼ B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰ ğ‘˜
Î¾ğ‘˜
(cid:107)B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰ ğ‘˜ (cid:107)2

.

ğ‘Ÿ (xğ‘¡ , u) + ğ›¾ğ‘‰ (cid:0) ğ‘“ (xğ‘¡ , u, Î¾ğ‘¥)(cid:1)

ğ‘‰tar = max
u

min
Î¾ğ‘¥
with Î¾ğ‘‡

ğ‘¥ Î¾ğ‘¥ âˆ’ ğ›¼2 â‰¤ 0.

Substituting the Taylor expansion, the dynamics model and the
reward yields

ğ‘‰tar = max
u

min
Î¾ğ‘¥

ğ‘‰tar âˆ’ ğ›¾ğ‘‰
Î”ğ‘¡

= ğ‘ğ‘ + max
u

ğ‘Ÿ + ğ›¾ğ‘‰ + ğ›¾âˆ‡ğ‘¥ğ‘‰ğ‘‡ ğ‘“ğ‘Î”ğ‘¡ + ğ›¾OÎ”ğ‘¡

(cid:2)âˆ‡ğ‘¥ğ‘‰ğ‘‡ (a + Bu) + ğ›¾O âˆ’ ğ‘”ğ‘(cid:3)
(cid:2)âˆ‡ğ‘¥ğ‘‰ğ‘‡ Î¾ğ‘¥ (cid:3)

+ min
Î¾

with the higher order terms O (x, u, Î”ğ‘¡). In the continuous
time limit, the higher-order terms disappear, i.e., limğ‘¡â†’0 O = 0.
Therefore, the optimal action is described by

uğ‘¡ = arg max

âˆ‡ğ‘¥ğ‘‰ğ‘‡ B(xğ‘¡ ) u âˆ’ ğ‘”ğ‘ (u)

u

â‡’ uğ‘¡ = âˆ‡ Ëœğ‘”ğ‘ (B(xğ‘¡ )âˆ‡ğ‘¥ğ‘‰) .

The optimal state disturbance is described by

Î¾âˆ—
ğ‘¥ = arg min

âˆ‡ğ‘¥ğ‘‰ğ‘‡ Î¾ğ‘¥ with Î¾ğ‘‡

ğ‘¥ Î¾ğ‘¥ âˆ’ ğ›¼2 â‰¤ 0.

Î¾ğ‘¥

Proof. Equation 14 can be formulated with the explicit con-
straint, i.e.,

ğ‘‰tar = max
u

min
Î¾ğ‘¢

ğ‘Ÿ (x, u) + ğ›¾ğ‘‰ (cid:0) ğ‘“ (x, u, Î¾ğ‘¢)(cid:1) with Î¾ğ‘‡

ğ‘¢ Î¾ğ‘¢ â‰¤ ğ›¼2.

Substituting the Taylor expansion, the dynamics model and the
reward yields

ğ‘‰tar = max
u

min
Î¾ğ‘¢

ğ‘‰tar âˆ’ ğ›¾ğ‘‰
Î”ğ‘¡

= ğ‘ğ‘ + max
u

ğ‘Ÿ + ğ›¾ğ‘‰ + ğ›¾âˆ‡ğ‘¥ğ‘‰ğ‘‡ ğ‘“ğ‘Î”ğ‘¡ + ğ›¾OÎ”ğ‘¡

(cid:2)âˆ‡ğ‘¥ğ‘‰ğ‘‡ (a + Bu) + ğ›¾O âˆ’ ğ‘”ğ‘(cid:3)
(cid:2)âˆ‡ğ‘¥ğ‘‰ğ‘‡ B Î¾ğ‘¢(cid:3)

+ min
Î¾

with the higher order terms O (x, u, Î”ğ‘¡). In the continuous
time limit, the higher-order terms disappear, i.e., limğ‘¡â†’0 O = 0.
Therefore, the optimal action is described by

uğ‘¡ = arg max

âˆ‡ğ‘¥ğ‘‰ğ‘‡ B(x) u âˆ’ ğ‘”ğ‘ (u)

u

â‡’ uğ‘¡ = âˆ‡ Ëœğ‘”ğ‘ (B(x)âˆ‡ğ‘¥ğ‘‰) .
The optimal state disturbance is described by
âˆ‡ğ‘¥ğ‘‰ğ‘‡ B(x) Î¾ğ‘¢ with Î¾ğ‘‡

Î¾âˆ—
ğ‘¢ = arg min

ğ‘¢ Î¾ğ‘¢ â‰¤ ğ›¼2.

Î¾ğ‘¢

This constrained optimization can be solved using the Karush-
Kuhn-Tucker (KKT) conditions, i.e.,

This constrained optimization can be solved using the Karush-
Kuhn-Tucker (KKT) conditions, i.e.,

âˆ‡ğ‘¥ğ‘‰ + 2ğœ† Î¾ğ‘¥ = 0 â‡’ Î¾âˆ—

ğ‘¥ = âˆ’

1
2ğœ†

âˆ‡ğ‘¥ğ‘‰

with the Lagrangian multiplier ğœ† â‰¥ 0. From primal feasibil-
ity and the complementary slackness condition of the KKT
conditions follows that

1
4ğœ†2 âˆ‡ğ‘¥ğ‘‰ğ‘‡ âˆ‡ğ‘¥ğ‘‰ âˆ’ ğ›¼2 â‰¤ 0

ğœ†

(cid:18) 1
4ğœ†2 âˆ‡ğ‘¥ğ‘‰ğ‘‡ âˆ‡ğ‘¥ğ‘‰ âˆ’ ğ›¼2

(cid:19)

= 0

â‡’ ğœ† â‰¥

âˆšï¸

1
2ğ›¼

âˆ‡ğ‘¥ğ‘‰ğ‘‡ âˆ‡ğ‘¥ğ‘‰

â‡’ ğœ†0 = 0, ğœ†1 =

âˆšï¸

1
2ğ›¼

âˆ‡ğ‘¥ğ‘‰ğ‘‡ âˆ‡ğ‘¥ğ‘‰ .

Therefore, the optimal adversarial state perturbation is de-
scribed by

Î¾ğ‘˜ = âˆ’ğ›¼

ğ‘‰ ğ‘˜
ğ‘¥
(cid:107)ğ‘‰ ğ‘˜
ğ‘¥ (cid:107)2

.

B(xğ‘¡ )ğ‘‡ âˆ‡ğ‘¥ğ‘‰ + 2ğœ† Î¾ğ‘¢ = 0 â‡’ Î¾âˆ—

ğ‘¢ = âˆ’

1
2ğœ†

B(xğ‘¡ )ğ‘‡ âˆ‡ğ‘¥ğ‘‰

with the Lagrangian multiplier ğœ† â‰¥ 0. From primal feasibil-
ity and the complementary slackness condition of the KKT
conditions follows that

1
4ğœ†2 âˆ‡ğ‘¥ğ‘‰ğ‘‡ BBğ‘‡ âˆ‡ğ‘¥ğ‘‰ âˆ’ ğ›¼2 â‰¤ 0

â‡’ ğœ† â‰¥

ğœ†

(cid:18) 1
4ğœ†2 âˆ‡ğ‘¥ğ‘‰ğ‘‡ BBğ‘‡ âˆ‡ğ‘¥ğ‘‰ âˆ’ ğ›¼2

(cid:19)

= 0

âˆšï¸

1
2ğ›¼

âˆ‡ğ‘¥ğ‘‰ğ‘‡ BBğ‘‡ âˆ‡ğ‘¥ğ‘‰

â‡’ ğœ†0 = 0, ğœ†1 =

âˆšï¸

1
2ğ›¼

âˆ‡ğ‘¥ğ‘‰ğ‘‡ BBğ‘‡ âˆ‡ğ‘¥ğ‘‰ .

Therefore, the optimal adversarial state perturbation is de-
scribed by

Î¾ğ‘˜
ğ‘¢ = âˆ’ğ›¼

B(xğ‘¡ )ğ‘‡ ğ‘‰ ğ‘˜
ğ‘¥
(cid:107)B(xğ‘¡ )ğ‘‡ ğ‘‰ ğ‘˜
ğ‘¥ (cid:107)2

.

(cid:3)

C. Model Disturbance Proof

Theorem. For the adversarial model disturbance (Equation 8)
with element-wise bounded amplitude (Equation 17), smooth
drift and control matrix (i.e., a, B âˆˆ ğ¶1) and B(ğœƒ + Î¾ğœƒ ) â‰ˆ
the optimal continuous time policy ğœ‹ğ‘˜ and model
B(ğœƒ),
disturbance Î¾ğ‘˜
ğœƒ is described by
B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰ ğ‘˜ (cid:17)
(cid:16)
(cid:18) ğœ•B
ğœ•ğœƒ

Î¾ğ‘˜
ğœƒ = âˆ’ğš«ğœˆ sign (zğœƒ ) + Âµğœˆ
ğœ•a
ğœ•ğœƒ

ğœ‹ğ‘˜ (x) = âˆ‡ Ëœğ‘”

with zğœƒ =

uâˆ— +

ğ‘‰ âˆ—
ğ‘¥ ,

(cid:19)ğ‘‡

mean ÂµÎ½ = (Î½max + Î½min) /2 and range ğš«Î½ = (Î½max âˆ’ Î½min) /2.

Proof. Equation 14 can be written with the explicit constraint
instead of the inï¬mum with the admissible set Î©ğ´, i.e.,

ğ‘‰tar = max
u

with

min
Î¾ğœƒ
1
(cid:16)
2

ğ‘Ÿ (x, u) + ğ›¾ğ‘‰ (cid:0) ğ‘“ (x, u, Î¾ğœƒ )(cid:1)

(Î¾ğœƒ âˆ’ Âµğœˆ)2 âˆ’ ğš«2
ğœˆ

(cid:17)

â‰¤ 0.

Substituting the Taylor expansion for ğ‘‰ (xğ‘¡+1), the dynamics
models and reward as well as abbreviating B(x; ğœƒ + Î¾ğœƒ ) as
B ğœ‰ and a(x; ğœƒ + Î¾ğœƒ ) as a ğœ‰ yields

ğ‘‰tar = max
u

min
Î¾ğœƒ

ğ‘Ÿ + ğ›¾ğ‘‰ + ğ›¾âˆ‡ğ‘¥ğ‘‰ğ‘‡ ğ‘“ğ‘Î”ğ‘¡ + ğ›¾O (.)Î”ğ‘¡

ğ‘‰tar âˆ’ ğ›¾ğ‘‰
Î”ğ‘¡

= ğ‘ğ‘ + max
u

min
Î¾

(cid:2)ğ›¾âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a ğœ‰ + B ğœ‰ u(cid:1) + ğ›¾O (.) âˆ’ ğ‘”ğ‘(cid:3)

In the continuous time limit the optimal action and disturbance
is determined by

uâˆ—, Î¾âˆ—

ğœƒ = max
u

min
Î¾

(cid:2)âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a ğœ‰ + B ğœ‰ u(cid:1) âˆ’ ğ‘”ğ‘ (u)(cid:3) .

This nested max-min optimization can be solved by ï¬rst
solving the inner optimization w.r.t. to u and substituting this
solution into the outer maximization. The Lagrangian for the
optimal model disturbance is described by

Î¾âˆ— = arg min

âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a ğœ‰ + B ğœ‰ u(cid:1) +

Î¾

1

2 Î»ğ‘‡ (cid:16)

(Î¾ğœƒ âˆ’ Âµğœˆ)2 âˆ’ ğš«2
ğœˆ

(cid:17)

Using the KKT conditions this optimization can be solved.
The stationarity condition yields

zğœƒ + Î»ğ‘‡ (Î¾ğœƒ âˆ’ Âµğœˆ) = 0 â‡’ Î¾âˆ—
ğœƒ = âˆ’zğœƒ (cid:11) Î» + Âµğœˆ
(cid:21)ğ‘‡

with zğœƒ =

(cid:20) ğœ•a
ğœ•ğœƒ

+

ğœ•B
ğœ•ğœƒ

u

âˆ‡ğ‘¥ğ‘‰

and the elementwise division (cid:11). The primal feasibility and the
complementary slackness yields

(cid:16)

1
2

âˆ’z2

ğœƒ (cid:11) Î»2 âˆ’ ğš«2
ğœˆ

(cid:17)

â‰¤ 0 â‡’ Î» â‰¥ (cid:107)zğœƒ (cid:107)1 (cid:11) ğš«ğœˆ

1

2 Î»ğ‘‡ (cid:16)

âˆ’z2

ğœƒ (cid:11) Î»2 âˆ’ ğš«2
ğœˆ

(cid:17)

= 0 â‡’ Î»0 = 0, Î»1 = (cid:107)zğœƒ (cid:107)1 (cid:11) ğš«ğœˆ.

Therefore, the optimal model disturbance is described by

ğœƒ (u) = âˆ’ğš«ğœˆ sign (cid:0)zğœƒ (u)(cid:1) + Âµğœˆ
Î¾âˆ—

as zğœƒ (cid:11) (cid:107)zğœƒ (cid:107)1 = sign(zğœƒ ). Then the optimal action can be
computed by

uâˆ— = arg max

u

âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:2)a(Î¾âˆ—

ğœƒ (u)) + B (cid:0)Î¾âˆ—

ğœƒ (u)(cid:1) u(cid:3) âˆ’ ğ‘”ğ‘ (u).

Due to the envelope theorem, the extrema is described by

B(x; ğœƒ + Î¾âˆ— (u))ğ‘‡ âˆ‡ğ‘¥ğ‘‰ âˆ’ ğ‘”ğ‘ (u) = 0.

This expression cannot be solved without approximation as
B does not necessarily be invertible w.r.t. ğœƒ. Approximating
B(x; ğœƒ + Î¾âˆ—(u)) â‰ˆ B(x; ğœƒ), lets one solve for u. In this case
the optimal action uâˆ— is described by uâˆ— = âˆ‡ Ëœğ‘”(B(x; ğœƒ)ğ‘‡ âˆ‡ğ‘¥ğ‘‰).
This approximation is feasible for two reasons. First of all, if
the adversary can signiï¬cantly alter the dynamics in each step,
the system would not be controllable and the optimal policy
would not be able to solve the task. Second, this approximation
implies that neither agent or the adversary can react to the
action of the other and must choose simultaneously. This
assumption is common in prior works [5]. The order of the
minimization and maximization is interchangeable. For both
cases the optimal action as well as optimal model disturbance
are identical and require the same approximation during the
(cid:3)
derivation.

D. Observation Disturbance Proof

Theorem. For the adversarial observation disturbance (Equa-
tion 7) with bounded signal energy (Equation 16), smooth drift
and control matrix (i.e., a, B âˆˆ ğ¶1) and B(x + Î¾ğ‘œ) â‰ˆ B(x),
the optimal continuous time policy ğœ‹ğ‘˜ and observation distur-
bance Î¾ğ‘˜

ğ‘œ is described by

ğœ‹ğ‘˜ (x) = âˆ‡ Ëœğ‘”

(cid:16)

B(x)ğ‘‡ âˆ‡ğ‘¥ğ‘‰ ğ‘˜ (cid:17)

with zğ‘œ =

(cid:18) ğœ•a(x; ğœƒ)
ğœ•x

+

ğ‘œ = âˆ’ğ›¼ zğ‘œ
Î¾ğ‘˜
(cid:107)zğ‘œ (cid:107)2
(cid:19)ğ‘‡

uâˆ—

ğ‘‰ âˆ—
ğ‘¥ .

ğœ•B(x; ğœƒ)
ğœ•x

Proof. Equation 14 can be written with the explicit constraint
instead of the inï¬mum with the admissible set Î©ğ´, i.e.,

ğ‘‰tar = max
u

min
Î¾ğ‘œ
1
2

with

ğ‘Ÿ (x, u) + ğ›¾ğ‘‰ (cid:0) ğ‘“ (x, u, Î¾ğ‘œ)(cid:1)

(cid:16)

ğ‘œ Î¾ğ‘œ âˆ’ ğ›¼2(cid:17)

(Î¾ğ‘‡

â‰¤ 0.

Substituting the Taylor expansion for ğ‘‰ (xğ‘¡+1), the dynamics
models and reward as well as abbreviating B(x + Î¾ğ‘œ; ğœƒ) as
B ğœ‰ yields

ğ‘‰tar = max
u

min
Î¾ğ‘œ

ğ‘Ÿ + ğ›¾ğ‘‰ + ğ›¾âˆ‡ğ‘¥ğ‘‰ğ‘‡ ğ‘“ğ‘Î”ğ‘¡ + ğ›¾O (.)Î”ğ‘¡

ğ‘‰tar âˆ’ ğ›¾ğ‘‰
Î”ğ‘¡

= ğ‘ğ‘ + max
u

min
Î¾

(cid:2)ğ›¾âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a ğœ‰ + B ğœ‰ u(cid:1) + ğ›¾O (.) âˆ’ ğ‘”ğ‘(cid:3)

In the continuous time limit the optimal action and disturbance
is determined by

uâˆ—, Î¾âˆ—

ğ‘œ = max
u

min
Î¾

(cid:2)âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a ğœ‰ + B ğœ‰ u(cid:1) âˆ’ ğ‘”ğ‘ (u)(cid:3) .

This nested max-min optimization can be solved by ï¬rst
solving the inner optimization w.r.t. to ğœ‰ and substituting this

Table III
Average rewards on the simulated and physical systems. The ranking describes the decrease in reward compared to the best result averaged on all systems.
The initial state distribution during training is noted by ğœ‡. The dynamics are either deterministic model ğœƒ âˆ¼ ğ›¿ ( ğœƒ) or sampled using uniform domain
randomization ğœƒ âˆ¼ U ( ğœƒ). During evaluation the roll outs start with the pendulum pointing downwards.

Algorithm

DP rFVI (ours)
DP cFVI
RTDP cFVI (ours)

SAC
SAC & UDR
SAC
SAC & UDR

DDPG
DDPG & UDR
DDPG
DDPG & UDR

PPO
PPO & UDR
PPO
PPO & UDR

ğœ‡

âˆ’
âˆ’
U

N
N
U
U

N
N
U
U

N
N
U
U

ğœƒ

ğ›¿ ( ğœƒ )
ğ›¿ ( ğœƒ )
ğ›¿ ( ğœƒ )

U ( ğœƒ )
ğ›¿ ( ğœƒ ) )
U ( ğœƒ )
U ( ğœƒ )

U ( ğœƒ )
ğ›¿ ( ğœƒ ) )
U ( ğœƒ )
U ( ğœƒ )

U ( ğœƒ )
ğ›¿ ( ğœƒ ) )
U ( ğœƒ )
U ( ğœƒ )

Simulated Pendulum
Reward
[ğœ‡ Â± 2ğœ]

Success
[%]

Simulated Cartpole
Reward
[ğœ‡ Â± 2ğœ]

Success
[%]

Simulated Furuta Pendulum
Success
[%]

Reward
[ğœ‡ Â± 2ğœ]

Physical Cartpole

Physical Furuta Pendulum

Success
[%]

Reward
[ğœ‡ Â± 2ğœ]

Success
[%]

Reward
[ğœ‡ Â± 2ğœ]

Average
Ranking
[%]

100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

âˆ’032.7 Â± 000.3
âˆ’030.5 Â± 000.8
âˆ’031.1 Â± 001.4

âˆ’031.1 Â± 000.1
âˆ’032.9 Â± 000.6
âˆ’030.6 Â± 001.4
âˆ’031.4 Â± 002.5

âˆ’031.1 Â± 000.4
âˆ’032.5 Â± 000.5
âˆ’031.5 Â± 000.7
âˆ’032.5 Â± 003.6

âˆ’032.0 Â± 000.2
âˆ’032.3 Â± 000.6
âˆ’033.4 Â± 004.7
âˆ’035.6 Â± 003.1

100.0
100.0
100.0

100.0
100.0
100.0
100.0

98.0
100.0
100.0
100.0

100.0
100.0
99.0
100.0

âˆ’027.1 Â± 004.8
âˆ’024.2 Â± 002.1
âˆ’024.9 Â± 001.6

âˆ’026.9 Â± 003.2
âˆ’029.7 Â± 004.6
âˆ’024.2 Â± 001.4
âˆ’024.2 Â± 001.3

âˆ’050.4 Â± 285.6
âˆ’027.4 Â± 002.3
âˆ’028.2 Â± 005.5
âˆ’027.2 Â± 001.0

âˆ’031.5 Â± 007.2
âˆ’084.0 Â± 007.8
âˆ’039.7 Â± 045.7
âˆ’044.8 Â± 021.4

100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

âˆ’041.3 Â± 010.8
âˆ’027.7 Â± 001.6
âˆ’040.1 Â± 002.7

âˆ’029.3 Â± 001.5
âˆ’032.0 Â± 001.1
âˆ’028.1 Â± 002.0
âˆ’028.1 Â± 001.3

âˆ’030.5 Â± 003.5
âˆ’034.6 Â± 009.8
âˆ’030.0 Â± 001.7
âˆ’032.1 Â± 001.5

âˆ’081.1 Â± 018.3
âˆ’040.9 Â± 004.6
âˆ’038.2 Â± 013.1
âˆ’048.5 Â± 006.2

100.0
73.3
100.0

00.0
100.0
53.3
40.0

06.7
00.0
06.7
00.0

00.0
00.0
00.0
40.0

âˆ’074.1 Â± 040.3
âˆ’143.7 Â± 210.4
âˆ’101.1 Â± 029.0

âˆ’518.6 Â± 028.1
âˆ’394.8 Â± 382.8
âˆ’144.5 Â± 204.0
âˆ’296.4 Â± 418.9

âˆ’536.7 Â± 262.7
âˆ’517.9 Â± 117.6
âˆ’459.4 Â± 248.3
âˆ’318.1 Â± 063.4

âˆ’287.9 Â± 068.8
âˆ’435.4 Â± 111.9
âˆ’183.8 Â± 018.0
âˆ’143.8 Â± 016.1

100.0
100.0
00.0

86.7
100.0
100.0
100.0

46.7
86.7
100.0
100.0

33.3
46.7
60.0
100.0

âˆ’278.0 Â± 034.3
âˆ’082.1 Â± 007.6
âˆ’1009.9 Â± 004.5

âˆ’330.7 Â± 799.0
âˆ’181.4 Â± 157.9
âˆ’350.8 Â± 433.3
âˆ’092.3 Â± 064.1

âˆ’614.1 Â± 597.8
âˆ’192.7 Â± 404.8
âˆ’146.6 Â± 218.3
âˆ’156.7 Â± 246.4

âˆ’718.7 Â± 456.1
âˆ’935.7 Â± 711.6
âˆ’755.3 Â± 811.0
âˆ’080.6 Â± 010.8

âˆ’062.7
âˆ’019.2
âˆ’247.7

âˆ’185.8
âˆ’120.8
âˆ’086.5
âˆ’063.8

âˆ’281.4
âˆ’156.6
âˆ’126.0
âˆ’091.7

âˆ’261.7
âˆ’370.0
âˆ’219.4
âˆ’054.4

solution into the outer maximization. The Lagrangian for the
optimal model disturbance is described by

Î¾âˆ—
ğ‘œ = arg min

âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:0)a(Î¾ğ‘œ) + B(Î¾ğ‘œ)u(cid:1) +

Î¾ğ‘œ

ğœ†
2

(cid:16)

ğ‘œ Î¾ğ‘œ âˆ’ ğ›¼2(cid:17)
Î¾ğ‘‡

Using the KKT conditions this optimization can be solved.
The stationarity condition yields

zğ‘œ + ğœ† Î¾ğ‘œ = 0 â‡’ Î¾âˆ—

ğ‘œ = âˆ’

with zğ‘œ =

(cid:20) ğœ•a(x; ğœƒ)
ğœ•x

+

ğœ•B(x; ğœƒ)
ğœ•x

uâˆ—

âˆ‡ğ‘¥ğ‘‰ .

zğ‘œ

1
ğœ†
(cid:21)ğ‘‡

The primal feasibility and the complementary slackness yield

1
2
ğœ†
2

(cid:18) 1
ğœ†2 zğ‘‡
(cid:18) 1
ğœ†2 zğ‘‡

ğ‘œ zğ‘œ âˆ’ ğ›¼2

(cid:19)

â‰¤ 0 â‡’ ğœ† â‰¥

1
ğ›¼

(cid:107)zğœƒ (cid:107)2

ğ‘œ zğ‘œ âˆ’ ğ›¼2

(cid:19)

= 0 â‡’ ğœ†0 = 0, ğœ†1 =

1
ğ›¼

(cid:107)zğ‘œ (cid:107)2.

Therefore, the optimal observation disturbance is described by
ğ‘œ (u) = âˆ’ğ›¼ zğ‘œ
Î¾âˆ—
(cid:107)zğ‘œ (cid:107)2

.

Then the optimal action can be computed by
âˆ‡ğ‘¥ğ‘‰ğ‘‡ (cid:2)a(cid:0)Î¾âˆ—

ğ‘œ (u)(cid:1) + B (cid:0)Î¾âˆ—

uâˆ— = arg max

ğ‘œ (u)(cid:1) u(cid:3) âˆ’ ğ‘”ğ‘ (u).

u

Due to the envelope theorem, the extrema is described by

B(x; ğœƒ + Î¾âˆ—

ğ‘œ (u))ğ‘‡ âˆ‡ğ‘¥ğ‘‰ âˆ’ ğ‘”ğ‘ (u) = 0.

This expression cannot be solved without approximation as
B does not necessarily be invertible w.r.t. x. Approximating
ğ‘œ (u); ğœƒ) â‰ˆ B(x; ğœƒ), lets one solve for u. In this case
B(x + Î¾âˆ—
the optimal action uâˆ— is described by uâˆ— = âˆ‡ Ëœğ‘”(B(x; ğœƒ)ğ‘‡ âˆ‡ğ‘¥ğ‘‰).
This approximation is feasible for two reasons. First of all, if
the adversary can signiï¬cantly alter the dynamics in each step,
the system would not be controllable and the optimal policy
would not be able to solve the task. Second, this approximation
implies that neither agent or the adversary can react to the
action of the other and must choose simultaneously. This
assumption is common in prior works [5]. The order of the
minimization and maximization is interchangeable. For both

cases the optimal action as well as optimal model disturbance
are identical and require the same approximation during the
(cid:3)
derivation.

Value Function Representation

For the value function we are using a locally quadratic deep
network as described in [27]. This architecture assumes that
the state cost is a negative distance measure between xğ‘¡ and the
desired state xdes. Hence, ğ‘ğ‘ is negative deï¬nite, i.e., ğ‘(x) <
0 âˆ€ x â‰  xdes and ğ‘(xdes) = 0. These properties imply that
ğ‘‰ âˆ— is a negative Lyapunov function, as ğ‘‰ âˆ— is negative deï¬nite,
ğ‘‰ âˆ— (xdes) = 0 and âˆ‡ğ‘¥ğ‘‰ âˆ—(xdes) = 0 [61]. With a deep network
a similar representation can be achieved by

ğ‘‰ (x; ğœ“) = âˆ’ (x âˆ’ xdes)ğ‘‡ L(x; ğœ“)L(x; ğœ“)ğ‘‡ (x âˆ’ xdes)
with L being a lower triangular matrix with positive diagonal.
This positive diagonal ensures that LLğ‘‡ is positive deï¬nite.
Simply applying a ReLu activation to the last layer of a deep
network is not suï¬ƒcient as this would also zero the actions for
the positive values and âˆ‡ğ‘¥ğ‘‰ âˆ— (xdes) = 0 cannot be guaranteed.
The local quadratic representation guarantees that the gradient
and hence, the action, is zero at the desired state. However, this
representation can also not guarantee that the value function
has only a single extrema at xdes as required by the Lyapunov
theory. In practice, the local regularization of the quadratic
structure to avoid high curvature approximations is suï¬ƒcient
as the global structure is deï¬ned by the value function target.
L is the mean of a deep network ensemble with ğ‘ independent
parameters ğœ“ğ‘–. The ensemble mean smoothes the initial value
function and is diï¬€erentiable. Similar representations have
been used by prior works in the safe reinforcement learning
community [62â€“65].

VII. Detailed Experimental Setup

Systems The performance of the algorithms is evaluated
using the swing-up the torque-limited pendulum, cartpole and
Furuta pendulum. The physical cartpole (Figure 4) and Furuta
pendulum (Figure 5) are manufactured by Quanser [37]. For
simulation, we use the equations of motion and physical
parameters of the supplier. Both systems have very diï¬€erent

Figure 8. The learning curves for DP rFVI and RTDP rFVI with diï¬€erent adversary amplitudes averaged over 5 seeds. The shaded area displays the min/max
range between seeds. The ğ›¼ corresponds of the percentage of the admissible set for all adversaries, i.e., with increasing ğ›¼ the adversary becomes more
powerful. For DP rFVI the stronger adversaries do aï¬€ect the ï¬nal performance only marginally. For RTDP rFVI the adversaries become too powerful for small
ğ›¼ and prevent learning of the optimal policy. This eï¬€ect is especially distinct for the Furuta pendulum as this system is very sensible due to the low masses.
Therefore, DP rFVI can learn a good optimal policy despite very strong adversaries.

rFVI starts to fail at ğ›¼ > 0.1. The Furuta pendulum starts to
fail earlier as this system is much more sensitive and smaller
actions cause large changes in dynamics compared to the cart-
pole. This ablation study shows that the dynamic programming
variant can learn the optimal policy despite strong adversaries.
In contrast the real-time dynamic programming variant fails
to learn the optimal policy for comparable admissible set.
This failure is caused by the missing positive feedback during
exploration. The adversary prevents the policy from discov-
ering the positive reward during exploration. Therefore, the
policy converges to a too pessimistic policy. In contrast the
dynamic programming variant does not rely on exploration and
covers the compact state domain. Therefore, the optimal policy
discovers the optimal policy despite the strong adversary.

A. Physical Experiments
The rewards of all baselines on the nominal physical system
are reported in Table III. The robustness experiments were only
performed for the best performing baselines. On the nominal
system rFVI outperforms all baselines on the cartpole. The
domain randomization baselines do not necessarily outperform
the deterministic deep RL baselines as the main source of
simulation gap is the backslash and stiction of the actuator
which is not randomized. For the Furuta pendulum, DP rFVI
has a lower reward compared to DP cFVI. However, this
lower reward is only caused by the chattering during the
balancing that causes very high actions costs. If one only
considers the swing-up phase, DP rFVI outperforms both
PPO-U UDR and DP cFVI. For the Furuta pendulum the
deep RL & UDR baselines outperform the deep RL baselines
without UDR. This is expected as the main uncertainty for the
Furuta pendulum is caused by the uncertainty of the system
parameters. In general a trend is observable that the algorithms
with larger state domain during training achieve the better
sim2real transfer.

characteristics. The Furuta pendulum consists of a small and
light pendulum (24g, 12.9cm) with a strong direct-drive motor.
Even minor diï¬€erences in the action cause large changes
in acceleration due to the large ampliï¬cation of the mass-
matrix inverse. Therefore, the main source of uncertainty for
this system is the uncertainty of the model parameters. The
cartpole has a longer and heavier pendulum (127g, 33.6cm).
The cart is actuated by a geared cogwheel drive. Due to the
larger masses the cartpole is not so sensitive to the model
parameters. The main source of uncertainty for this system is
the friction and the backlash of the linear actuator. The systems
are simulated and observed with 500Hz. The control frequency
varies between algorithm and is treated as hyperparameter.

Reward Function The desired state for all tasks is the upward
pointing pendulum at xdes = 0. The state reward is described
by ğ‘ğ‘ (x) = âˆ’(z âˆ’ zdes)ğ‘‡ Q(z âˆ’ zdes) with the positive deï¬nite
matrix Q and the transformed state z. For continuous joints the
joint state is transformed to ğ‘§ğ‘– = ğœ‹2 sin(ğ‘¥ğ‘–). The action cost is
described by ğ‘”ğ‘ (u) = âˆ’2 Î² umax/ğœ‹ log cos(ğœ‹ u/(2 umax)) with
the actuation limit umax and the positive constant Î². This bar-
rier shaped cost bounds the optimal actions. The corresponding
policy is shaped by âˆ‡ Ëœğ‘”(w) = 2 umax/ğœ‹ tanâˆ’1 (w/Î²). For the
experiments, the reward parameters are

Pendulum: Qdiag = [01.0, 0.1] ,
Cartpole: Qdiag = [25.0, 1.0, 0.5, 0.1] ,
Furuta Pendulum: Qdiag = [01.0, 5.0, 0.1, 0.1] ,

ğ›½ = 0.5
ğ›½ = 0.1
ğ›½ = 0.1

Additional Experimental Results

This section summarizes the additional experiments omitted
in the main paper. In the following we perform an ablation
study varying the admissible set and report the performance
of additional baselines on the nominal physical system.

Ablation Study - Admissible Set
The ablation study highlighting the diï¬€erences in learning
curves for diï¬€erent admissible sets is shown in Figure 8. In
this plot we vary the admissible set of each adversary from 0
to the 1, where ğ›¼ = 1 corresponds to the admissible set used
for the robustness experiments. With increasing admissible set
the ï¬nal performance of the adversary decreases marginally for
DP rFVI. For RTDP rFVI the optimal policy does not learn
the task for stronger adversaries. RTDP rFVI starts to fail for
ğ›¼ > 0.3 on the cartpole. On the Furuta pendulum, RTDP

050100150200250Episodeâˆ’150âˆ’100âˆ’500RewardCartpole-DPrFVI050100150200250Episodeâˆ’400âˆ’300âˆ’200âˆ’1000RewardFurutaPendulum-DPrFVI050100150200250Episodeâˆ’150âˆ’100âˆ’500RewardCartpole-RTDPrFVI050100150200250Episodeâˆ’400âˆ’300âˆ’200âˆ’1000RewardFurutaPendulum-RTDPrFVIÎ±=0.0Î±=0.1Î±=0.2Î±=0.3Î±=0.4Î±=0.5Î±=0.6Î±=0.7Î±=0.8Î±=0.9Î±=1.0Figure 9. The roll-outs of DP rFVI, DP cFVI and the deep RL baselines with domain randomization the physical Furuta pendulum. The diï¬€erent columns
correspond to diï¬€erent pendulum masses. The deviation from the dashed center line corresponds to the joint velocity. DP rFVI achieves a consistent swing-up
for the diï¬€erent pendulum masses. In contrast to DP rFVI, the baselines start to deviate strongly from trajectories on the nominal system. When weights are
added the baselines start to cover the complete state-space.

Figure 10. The roll-outs of DP rFVI, DP cFVI and the deep RL baselines with domain randomization the physical Cartpole. The diï¬€erent columns correspond
to diï¬€erent pendulum masses. The deviation from the dashed center line corresponds to the joint velocity. DP rFVI achieves a consistent swing-up for the
diï¬€erent pendulum masses but the failure rate slighlty increases when weights are added to the pendulum. In contrast to DP rFVI, the baselines start to deviate
stronger from trajectories on the nominal system when the system dynamics are altered.

