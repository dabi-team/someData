OPTIMIZE WHAT MATTERS: TRAINING DNN-HMM KEYWORD SPOTTING MODEL
USING END METRIC

Ashish Shrivastava

Arnav Kundu

Chandra Dhir
Apple

Devang Naik

Oncel Tuzel

1
2
0
2

b
e
F
6
2

]

D
S
.
s
c
[

2
v
1
5
1
1
0
.
1
1
0
2
:
v
i
X
r
a

ABSTRACT

Deep Neural Network–Hidden Markov Model (DNN-HMM)
based methods have been successfully used for many always-
on keyword spotting algorithms that detect a wake word to
trigger a device. The DNN predicts the state probabilities
of a given speech frame, while HMM decoder combines the
DNN predictions of multiple speech frames to compute the
keyword detection score. The DNN, in prior methods, is
trained independent of the HMM parameters to minimize the
cross-entropy loss between the predicted and the ground-truth
state probabilities. The mis-match between the DNN training
loss (cross-entropy) and the end metric (detection score) is
the main source of sub-optimal performance for the keyword
spotting task. We address this loss-metric mismatch with a
novel end-to-end training strategy that learns the DNN pa-
rameters by optimizing for the detection score. To this end,
we make the HMM decoder (dynamic programming) differ-
entiable and back-propagate through it to maximize the score
for the keyword and minimize the scores for non-keyword
speech segments. Our method does not require any change
in the model architecture or the inference framework; there-
fore, there is no overhead in run-time memory or compute
requirements. Moreover, we show signiﬁcant reduction in
false rejection rate (FRR) at the same false trigger experience
(> 70% over independent DNN training).

Index Terms— DNN-HMM, speech recognition, key-

word spotting, wake word detection.

1. INTRODUCTION

Common strategies to improve a machine learning model are
adding more training data, or using a model with more pa-
rameters/better architecture. However, adding more data is
not helpful with small footprint models with limited capacity
(such as for keyword spotting), and adding more parameters
increases runtime memory or compute, which is not possible
for many real-time production systems. We show that, with-
out adding more data or changing the model architecture, we
can signiﬁcantly improve the performance by designing an
optimization procedure that focuses on the end metric, partic-
ularly for low capacity models.

Emails: {ashish.s, a kundu, cdhir, naik.d, otuzel}@apple.com

We focus on speech based natural interaction, which
requires a device to constantly listen to and decode the
streaming audio. For power efﬁciency purposes, the de-
vice usually relies on a voice trigger interface that triggers
a more compute-intensive pipeline if a speciﬁc keyword is
detected. A keyword spotting (KWS) algorithm, with small
memory and compute footprint, is an essential component of
the system that computes a detection score for every speech
frame (typically 10ms apart) and triggers if the score exceeds
a threshold.

In DNN-HMM based KWS models, the DNN computes
the state probabilities and the HMM decoder combines these
probabilities using dynamic programming (DP). We follow
the same architecture as [1] which contains 20 states – 18 cor-
responding to 6 phonemes of the trigger phrase (3 states for
each phoneme), 1 state for silence, and 1 state for background.
The DNN uses a softmax layer to output probability distribu-
tion over 20 classes corresponding to these 20 states for each
speech frame, and is trained to minimize the average (over
all frames) cross-entropy loss between the predicted and the
ground-truth distributions. This training ignores the HMM
transition and prior probabilities which are learned indepen-
dently using training data statistics. Such an independently
trained DNN model relies on the accuracy of the ground-truth
phoneme labels as well as the HMM model. This model also
assumes that the set of keyword states are optimal and each
state is equally important for the keyword detection task. The
DNN spends all of its capacity focusing equally on all of the
states, without considering its impact on the ﬁnal metric of
the detection score, resulting in a loss-metric mismatch.

To address this loss-metric mismatch, we train the DNN
model by directly optimizing the keyword detection score in-
stead of optimizing for the state probabilities. This end metric
based training uses only the start and the end of the keyword
instead of requiring all of the speech frames to be annotated,
leading to substantial savings in annotation cost. Our method
changes only the training algorithm without changing any in-
ference pipeline; therefore, there is no overhead in runtime
memory or compute, since we only need to update the model
parameters. We design an optimization procedure that maxi-
mizes the detection score for a speech segment that “tightly”
contains the keyword (we call these positive samples) and
minimize the detection score for the speech that does not con-
tain the keyword (negative samples). We also sample addi-

 
 
 
 
 
 
tional hard negatives that contain partial keywords because
we do not want the model to trigger at partial phrases. To for-
malize the concept of “tightly” containing the keyword, let a
ground-truth window start at time g1 and end at g2, and a sam-
pled window (i.e. a set of contiguous speech frames) start and
end at time w1 and w2, respectively. We sample positive and
negative windows from speech utterances such that the pos-
itive windows have high intersection-over-union (IOU ) and
negative windows have low IOU with the ground-truth key-
word window. The IOU between the ground-truth window
[g1, g2] and a sampled window [w1, w2] is deﬁned as:

IOU =

area of intersection
area of union

=

max(0, min(g2, w2) − max(g1, w1))
max(g2, w2) − min(g1, w1)

.

(1)

The IOU based sampling in our method is inspired by im-
age detection algorithms such as [2, 3, 4]. We differentiate
through the DP algorithm of the HMM decoder to optimize
the detection scores of positive and negative windows.

Our main contributions include: (1) deﬁning the optimiza-
tion of KWS model using the end metric which triggers only
on a tightly overlapping (based on IOU ) window with the
ground-truth keyword, (2) learning the DNN parameters by
computing gradients of this loss function through the HMM
decoder, (3) reducing the FRR by > 70% (at the same FA)
without adding more data or changing model architecture.

2. RELATED WORKS

HMM based architectures have been common in keyword de-
tection [5, 6, 7, 8, 9], where an acoustic model computes the
observation probabilities and the HMM decoder computes the
detection score using the observation, the state transition, and
the prior probabilities. The DNN-HMM based models [1,
10, 11] compute the observation probabilities using a DNN
with a softmax layer, which is trained separately to minimize
the cross-entropy loss between the predicted and ground-truth
observation probabilities. Such optimization results in a loss-
metric mismatch, where the metric (i.e. the detection score)
is not directly optimized. In contrast, our method optimizes
the detection score explicitly.

Discriminative training methods have been proposed for
speech recognition (including keyword spotting) tasks [12,
13] for generative models such as HMM. We also propose a
discriminative training framework; however, we speciﬁcally
optimize the detection score based on positive and negative
windows deﬁned by IOU criteria (inspired by object detec-
tion methods in images [2, 3, 4]) and learn DNN parame-
ters by differentiating through the HMM decoding. Recur-
rent neural networks (RNNs) have also been used for keyword
spotting [14, 15, 16, 17, 18], but are generally not suitable for
low-power streaming audio applications. Recently, convolu-
tional neural networks (CNN) based end-to-end trained mod-

Figure 1: Overview of the proposed optimization. Positive
samples have large IOU and negative samples have a small
IOU with the ground-truth keyword window. The detection
score of the window corresponds to the most likely HMM
path that starts at the beginning of the window and ends at the
last frame of the window. We make the HMM differentiable
and then update DNN parameters so that the scores of positive
samples are high and the scores for negative samples are low.

els [19, 20, 21] have been shown to work better than the HMM
based model. However, these models require more computa-
tion to encode a large context. Researchers have optimized
the CNN based models [22, 23, 24, 25], but they minimize
the frame level cross-entropy loss and suffer from loss-metric
mismatch. Compare to these CNN based models, the pro-
posed model is more interpretable (predicts state probabili-
ties), has a larger receptive ﬁeld, better localization, and ad-
dresses loss-metric mismatch.

3. THE PROPOSED END-TO-END OPTIMIZATION

Each speech frame is labeled with one of the C states which
correspond to keyword phonemes, silence, and background
speech. The DNN, consisting of a few fully-connected layers
followed by a softmax layer, takes context dependent speech
features, xt ∈ Rd, at time t and maps it to the probability dis-
tribution over C states. Following the architecture of [1], we
use 13-dimensional MFCC features [26, 27] computed over
small overlapping windows (25ms in duration and separated
by 10ms) of the input audio. To model context, the input fea-
ture xt is generated by concatenating the MFCC features over
a small window of frames [t − δ, t + δ], where δ is a small
number (9 in our implementation).

Let the DNN be denoted by fθ : Rd → RC, where θ are
the DNN parameters. Given a set of speech features xj and
their corresponding ground-truth state-labels, yj, the DNN, in
conventional methods, is independently trained by minimiz-
ing the following cross-entropy loss:

Lce = min

θ

(cid:88)

j

− log(fθ(xj)[yj]).

(2)

∈ℝC×Tstate probabilitiesa sampled window ( frames)TDNN…………HMM  decoder  (Dynamic Programming)………………detection score for window∈ℝ+ve samples-ve samples based samplingIOUinput utterance (speech frames)During inference, the HMM decoder combines the DNN pre-
dictions using DP to compute the detection score of the key-
word. It computes the most likely path (and its probability)
that starts at the state corresponding to the beginning of the
keyword and ends at time t at the state corresponding to the
end of the keyword. The HMM decoder backtracks the most
likely path from the end of the keyword at time t = T , and
ﬁnds the time ts for the beginning of the keyword on this
most likely path. The detection score, at time t, is computed
as the probability of the most likely path ending at the state
corresponding to the end of the keyword and is normalized by
detection window size (t − ts). In conventional DNN-HMM
models, only the DNN is trained and HMM is used only dur-
ing inference.

In the proposed method (Figure 1), positive windows are
sampled such that their IOU s with the ground-truth keyword
window is greater than a threshold, ioup, and the negative
windows are sampled to have their IOU s less than a thresh-
old, ioun. Furthermore, we augment the negative training
data by dividing the ground-truth window into two parts and
swapping their order. Such perturbation of the ground-truth
window creates a hard negative sample and enforces the
model to predict a high detection score only if the states are
observed in the right order. Intuitively, this helps the model
to reject the incorrect ordering of the words in the trigger
phrase.

We train the DNN model such that the positive windows
have high scores and the negative windows have low scores.
To compute the detection score of a window, we use both the
DNN and the HMM models during training. We assume the
start and the end of the windows are the start and the end of
the trigger phrase. The HMM decoder combines the DNN
outputs of an input window to compute the most likely path
that starts at the ﬁrst frame and ends at the last (T th) frame of
the window.

At each time step, the HMM decoder computes the maxi-
mum probability of reaching the state si, ∀i = 1, . . . , C. Let
this probability be vi(t) at frame t, which can be written as:

vi(t) = max

s1,...st−1

= max

s1,...st−1

P r(s1, . . . , st−1, x1, . . . , xt, st = i),

{vi(t − 1) ∗ bi,i, vi−1(t − 1) ∗ bi−1,i}

∗ P r(xt|st = i),

(3)

where bi,j are the transition probabilities of the HMM.
in state i,
The observation probability given the model
P r(xt|st = i), is proportional to the DNN output for the ith
class, fθ(xt)[i]. First, vi(0) is initialized with the prior prob-
abilities and then iteratively updated using a max-pool layer
by combining the transition probabilities and the DNN pre-
dictions. After T updates, the detection score of the window
is d = vC(T )/T , where division by T is used to normalize
the detection scores to account for different sequence lengths.
We use the hinge loss [28], which ignores the samples from

optimization if their scores are beyond a margin:

Le2e = min

θ

(cid:88)

j∈X p

max(0, 1 − dj) +

(cid:88)

j∈X n

max(0, 1 + dj),

(4)

where X p is the set of all the positive windows, X n is the set
of all the negative windows, and dj is detection score for the
jth window. Note that this loss function is piece-wise smooth
and can be optimized using gradient descent.

4. EXPERIMENTS

4.1. Data

Similar to [25], our training data contains approximately 500k
speech utterances, each containing one trigger phrase. We ap-
ply simple augmentations on the data such as gain augmen-
tation (multiplying the raw audio by a scalar) and room im-
pulse response (RIR). Our test data consists of approximately
2000 hours of “dense” speech (i.e. with little silence data) to
test the robustness of the model. The positive test data (i.e.
utterances containing the trigger phrase) includes near ﬁeld
samples from a device held in the speaker’s hand and far ﬁeld
samples from devices roughly 3ft away from the speaker. This
data has been collected via internal user studies with informed
consents. The ground-truth keyword window is determined
using the state labels corresponding to the beginning and the
end of the keyword.

4.2. Training and Evaluation

We ﬁrst pre-train the DNN model by minimizing the cross-
entropy loss in Equation (2). Then, we optimize the proposed
end-to-end loss in Equation (4) using Adam optimizer [29]
with an initial learning rate of 0.001. The mini-batch consists
of positive and negative samples from 48 random speech ut-
terances, each containing the trigger phrase. From each train-
ing utterance, we select 1 positive sample with IOU ≥ 0.95
(i.e.ioup = 0.95) and up to 20 negative samples with IOU ≤
0.5 (i.e. ioun = 0.5). We sample 10 additional negative win-
dows by splitting the ground-truth window approximately in
the middle and swapping their order. Thus, each mini-batch
contains 48 positive samples and 48 × 30 negative samples.
To address the class-imbalance problem, we select 50 hard
negatives (negatives samples with maximum loss value) and
50 random negatives from all of the negative samples in the
mini-batch.

During evaluation, if a detected keyword window over-
laps with the ground-truth window, it counts as a true posi-
tive (TP), otherwise it counts as a false trigger or false accept
(FA). All the triggers which are not detected are counted as
false rejects (FRs). We vary the detection threshold to com-
pute the detection error tradeoff (DET) curve (FRR vs FA/hr),

Figure 3: DET-curve for independently trained and end-to-
end trained DNN-HMM models on ≈ 2000 hours of test data.
The end-to-end trained DNN-HMM has lower FRR at all op-
erating points.

ence of the start and the end locations of the ground-truth win-
dows and the predicted true positives windows. Our model
gives smaller localization error of 0.03 seconds compared to
0.13 seconds of S1DCNN.

To further analyze the model, we plot the confusion ma-
trices of the independently trained DNN (that maximize state
accuracies) and the end-to-end trained DNN (that optimizes
detection score) in Figure 2. The DNN optimized for state
accuracies distributes its capacity equally to predict the states
accurately, leading to higher average state accuracy of 68.1%.
However, this does not translate into higher keyword detec-
tion accuracy. In contrast, the end-to-end trained DNN has
lower average state accuracy of 54.7% (particularly, there is
more confusion among the neighboring states) but focuses on
key states that are important for keyword detection, leading to
higher keyword detection accuracy.

Model

Independently trained
DNN-HMM

S1DCNN [25]

End-to-end trained DNN-HMM

end-
to-end

No. of
params

FRR
(%)

no

yes

yes

13979

3.95

13993

1.18

13979

1.13

Table 1: Comparison of FRRs at 15 FA/hr on test data. The
end-to-end trained DNN-HMM model has the lowest FRR.

5. CONCLUSION

In this work, we have addressed the loss-metric mismatch
problem of conventional DNN-HMM based keyword detec-
tion model by introducing a novel end-to-end training ap-
proach using IOU based sampling. The proposed method
works signiﬁcantly better (> 70% relative reduction in FRR)
than the conventional DNN-HMM training and is more inter-
pretable, accurate in localization, and data-efﬁcient compared
to the CNN based end-to-end models.

Figure 2: Confusion matrices for independently trained
DNN-HMM (top) and end-to-end trained DNN-HMM (bot-
tom) models. The independently trained model spends its
capacity in predicting the state probabilities more accurately
(average classiﬁcation accuracy of 68.1%), without consider-
ing the ﬁnal detection score. The end-to-end model addresses
this loss-metric mismatch and improves the detection score at
the expense of state classiﬁcation accuracy (of 54.7%).

and compare our method with an independently trained DNN-
HMM model in Figure 3. For the proposed method, we note
a signiﬁcant reduction in FRs over all the FA/hr. We se-
lect an operating point at approximately 15 FA/hr and com-
pute the false reject rate (FRR) in Table 1. We also compare
our method with a recently proposed Stacked 1D CNN based
method [25] which is roughly at par with our method in FRR.
However, the average IOU (between the ground-truth win-
dows and the predicted true positives windows) of the pro-
posed end-to-end trained DNN-HMM is 27.7% higher than
the S1DCNN model, suggesting better localization. To fur-
ther measure localization, we compute mean absolute differ-

012345678910111213141516171819Ground-truth labels012345678910111213141516171819Predicted labels0.820.050.060.080.110.130.150.150.110.110.120.10.140.120.140.150.170.250.270.30.140.950.350.10.060.040.050.060.040.090.040.020.040.020.060.040.050.050.090.2000.50.060000000000000000000.070.70.03000000000000000000.010.050.740.03000000000000000000.010.050.760.050000000000000000000.030.660.070000000000.010.0100000000.060.630.040000000000.010.0100000000.060.740.030000000000.0200000000.010.060.740.050000000000000000000.030.740.040000000000000000000.050.780.050000000000000000000.050.690.080000000000000000000.080.720.060000000000000000000.050.680.080000000000000000000.050.660.060000000000000000000.070.640.080000000000000000000.070.540.0500000000.010.010000000000.070.520.0700000000.010.010000000000.050.4Independently trained DNN-HMM012345678910111213141516171819Ground-truth labels012345678910111213141516171819Predicted labels0.820.080.060.060.10.130.130.140.110.110.090.070.10.110.120.130.140.180.20.310.090.910.610.170.070.030.010.020.010.040.020.010.010.010.020.010.010.010.030.09000.270.330.01000000000000000000.020.40.22000000000000000000.020.020.40.02000000000000000.0100.010.010.180.780.1200000000000000.0100000.030.630.30.02000000000.010.0100000000.050.410.070000000000.010.010.01000000.010.090.780.370000000000.030.0100.010.01000.01000.460.50.020000000000000000000.360.290.010000000000000000000.520.0700000000.010000000000.020.090.770.490.01000000000000000000.030.30.090.0100000.010000000000000.080.680.17000000000000000000.010.070.620.170.0100000000000000000.010.050.440.04000000000.010000000000.220.370.0300.02000000.030.04000000000.020.370.680.20.010000000.010.010000000000.040.35End-to-end trained DNN-HMM100101FA (False Accepts)/hr0.00.51.01.52.02.53.03.54.04.55.05.56.06.57.07.5FRR (False Reject Rate) %FA Operating PointDET curve on test dataEnd-to-end trained DNN-HMMIndependently trained DNN-HMM6. REFERENCES

[1] Siddharth Sigtia, Rob Haynes, Hywel Richards, Erik
Marchi, and John Bridle, “Efﬁcient voice trigger detec-
tion for low resource hardware,” in Proc. Interspeech,
2018.

[2] Joseph Redmon, Santosh Divvala, Ross Girshick, and
Ali Farhadi, “You only look once: Uniﬁed, real-time
object detection,” in Proc. CVPR, 2016.

[3] Joseph Redmon and Ali Farhadi, “YOLO9000: Better,

faster, stronger,” in Proc. CVPR, 2017.

[4] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun, “Faster r-cnn: Towards real-time object detection
with region proposal networks,” in Proc. NIPS, 2015.

[14] Santiago Fern´andez, Alex Graves, and J¨urgen Schmid-
huber, “An application of recurrent neural networks to
discriminative keyword spotting,” in International Con-
ference on Artiﬁcial Neural Networks, 2007.

[15] Martin Woellmer, Bjoern Schuller, and Gerhard Rigoll,
“Keyword spotting exploiting long short-term memory,”
Speech Communication, 2013.

[16] Chris Lengerich and Awni Hannun, “An end-to-end ar-
chitecture for keyword spotting and voice activity detec-
tion,” arXiv preprint arXiv:1611.09405, 2016.

[17] Kyuyeon Hwang, Minjae Lee, and Wonyong Sung,
“Online keyword spotting with a character-level recur-
rent neural network,” arXiv preprint arXiv:1512.08903,
2015.

[5] Lalit Bahl, Peter Brown, Peter Souza, and Robert Mer-
cer, “Maximum mutual information estimation of hid-
den markov parameters for speech recognition,” in Proc.
ICASSP, 1986.

[18] Alex Graves, Santiago Fern´andez, Faustino Gomez, and
J¨urgen Schmidhuber, “Connectionist temporal classiﬁ-
cation: labelling unsegmented sequence data with recur-
rent neural networks,” in Proc. ICML, 2006.

[6] J. Robin Rohlicek, William Russell, Salim Roukos, and
Herbert Gish, “Continuous hidden markov modeling for
speaker-independent word spotting,” in Proc. ICASSP,
1989.

[7] Richard C. Rose and Douglas B. Paul,

“A hidden
markov model based keyword recognition system,” in
Proc. ICASSP, 1990.

[8] Jay G. Wilpon, Lawrence R. Rabiner, Chin-Hui Lee,
and E.R. Goldman, “Automatic recognition of keywords
in unconstrained speech using hidden markov models,”
IEEE Transactions on Acoustics, Speech, and Signal
Processing, 1990.

[9] John S. Bridle, “Alpha-nets: A recurrent ’neural’ net-
work architecture with a hidden markov model interpre-
tation,” Speech Communication, 1990.

[10] Guoguo Chen, Carolina Parada, and Georg Heigold,
“Small-footprint keyword spotting using deep neural
networks,” in Proc. ICASSP, 2014.

[11] I-Fan Chen and Chin-Hui Lee, “A hybrid hmm/dnn ap-
proach to keyword spotting of short words,” in Proc.
Interspeech, 2013.

[12] Joseph Keshet, David Grangier, and Samy Bengio,
“Discriminative keyword spotting,” Speech Communi-
cation, 2009.

[19] Yundong Zhang, Naveen Suda, Liangzhen Lai, and
Vikas Chandra, “Hello edge: Keyword spotting on mi-
crocontrollers,” arXiv preprint arXiv:1711.07128, 2017.

[20] Raphael Tang and Jimmy Lin, “Deep residual learning
for small-footprint keyword spotting,” in Proc. ICASSP,
2018.

[21] Tara Sainath and Carolina Parada, “Convolutional neu-
ral networks for small-footprint keyword spotting,” in
Proc. Interspeech, 2015.

[22] Alvarez Raziel and Park Hyun-Jin,
end streaming keyword spotting,”
arXiv:1812.02802, 2018.

“End-to-
arXiv preprint

[23] Preetum Nakkiran, Raziel Alvarez, Rohit Prabhavalkar,
and Carolina Parada, ““compressing deep neural net-
works using a rank-constrained topology,” in Proc. In-
terspeech, 2015.

[24] Ming Sun, David Snyder, Yixin Gao, Varun K. Na-
garaja, Mike Rodehorst, Sankaran Panchapagesan,
Nikko Strom, Spyros Matsoukas, and Shiv Vitalade-
vuni, “Compressed time delay neural network for small-
footprint keyword spotting,” in Proc. Interspeech, 2017.

[25] Takuya Higuchi, Mohammad Ghasemzadeh, Kisun
You, and Chandra Dhir, “Stacked 1d convolutional net-
works for end-to-end small footprint voice trigger detec-
tion,” in Proc. Interspeech, 2020.

[13] Zhehuai Chen, Yanmin Qian, and Kai Yu, “Sequence
discriminative training for deep learning based acoustic
keyword spotting,” Speech Communication, 2018.

[26] P. Mermelstein, “Distance measures for speech recogni-
tion, psychological and instrumental,” Pattern Recogni-
tion and Artiﬁcial Intelligence, 1976.

[27] Steven B. Davis and Paul Mermelstein,

“Compari-
son of parametric representations for monosyllabic word
recognition in continuously spoken sentences,” IEEE
Trans. On Acoustics, Speech and Signal Processing,
1980.

[28] Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto,
Michele Piana, and Alessandro Verri, “Are loss func-
tions all the same?,” Neural Computation, 2004.

[29] Diederik Kingma and Jimmy Ba, “Adam: A method for

stochastic optimization,” in Proc. ICLR, 2014.

