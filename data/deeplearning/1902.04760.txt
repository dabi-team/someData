Scaling Limits of Wide Neural Networks with Weight Sharing:
Gaussian Process Behavior, Gradient Independence, and Neural Tangent
Kernel Derivation

0
2
0
2

r
p
A
4

]
E
N
.
s
c
[

3
v
0
6
7
4
0
.
2
0
9
1
:
v
i
X
r
a

Greg Yang 1

Abstract

Several recent trends in machine learning theory and practice, from the design of state-of-the-art Gaussian Process
to the convergence analysis of deep neural nets (DNNs) under stochastic gradient descent (SGD), have found
it fruitful to study wide random neural networks. Central to these approaches are certain scaling limits of such
networks. We unify these results by introducing a notion of a straightline tensor program that can express most
neural network computations, and we characterize its scaling limit when its tensors are large and randomized. From
our framework follows 1. the convergence of random neural networks to Gaussian processes for architectures such
as recurrent neural networks, convolutional neural networks, residual networks, attention, and any combination
thereof, with or without batch normalization; 2. conditions under which the gradient independence assumption –
that weights in backpropagation can be assumed to be independent from weights in the forward pass – leads to
correct computation of gradient dynamics, and corrections when it does not; 3. the convergence of the Neural
Tangent Kernel, a recently proposed kernel used to predict training dynamics of neural networks under gradient
descent, at initialization for all architectures in (1) without batch normalization. Mathematically, our framework is
general enough to rederive classical random matrix results such as the semicircle and the Marchenko-Pastur laws,
as well as recent results in neural network Jacobian singular values. We hope our work opens a way toward design
of even stronger Gaussian Processes, initialization schemes to avoid gradient explosion/vanishing, and deeper
understanding of SGD dynamics in modern architectures.

1. Introduction

Several recent trends in machine learning theory and practice have found it fruitful to study wide random neural networks,
such as neural network inspired Gaussian Processes, signal propagation in DNNs, small learning rate SGD dynamics, and
even, in some sense, the celebrated Approximate Message Passing algorithm for compressed sensing. We review these
subjects and more in Section 2. All of these works involve some theory that derives, rigorously or semirigorously, some
scaling limit of a neural network as its width goes to inﬁnity. In this paper, we give a unifying treatment to such scaling
limits: • We deﬁne a notion of tensor programs which can express most neural network computations, and a natural notion
of tensor program scaling that corresponds to increasing width with Glorot initialization (Glorot & Bengio, 2010). Our main
theorems characterize the scaling limits in the two most common scenarios that roughly correspond to DNN inference and
backpropagation, as well as in the general tensor program case. They are proved via a Gaussian conditioning technique ﬁrst
used in Bolthausen (2012) for analyzing the TAP equations in spin glass theory. • We obtain corollaries that fully justify
semirigorous derivations in prior works and strengthen previous results in the different strands of research mentioned above.
In the next section we highlight the most important corollaries and discuss other brieﬂy, leaving their details to the appendix.

By standard architecture we mean any DNN architecture that is some composition of multilayer perceptrons (MLP)s,
recurrent neural networks (RNNs) (e.g., Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) or Gated
Recurrent Unit (GRU) (Cho et al., 2014)), skip connections (He et al., 2016; Huang et al., 2016), (self-)attention (Bahdanau
et al., 2014; Vaswani et al., 2017), convolution (LeCun et al., 1998; 1999), and/or batch normalization (batchnorm) (Ioffe &
Szegedy, 2015). We use readout layer to mean any linear layer converting some hidden states to an output vector. While

1Microsoft Research AI. Correspondence to: (cid:104)gregyang@microsoft.com(cid:105).

1

 
 
 
 
 
 
most of our corollaries are stated for standard architectures, they are typically more general, but we just highlight the most
relevant cases for a deep learning audience.

2. Related Works and Our Corollaries

We formulate informal versions of our main corollaries and comment on other results inline, marked by a star (cid:70).

2.1. Gaussian Behavior of Wide Neural Networks

In 1995, Neal ﬁrst discovered the Gaussian Process behavior of wide neural networks. He showed that under certain
conditions, a single-layer neural network with random parameters can converge in distribution to a Gaussian process as
its width goes to inﬁnity. Later works extended the conditions under which this scaling limit takes place (Williams, 1997;
Le Roux & Bengio, 2007; Hazan & Jaakkola, 2015). Recently, Lee et al. (2018); Matthews et al. (2018) empirically
and/or theoretically investigated analogous correspondences for inﬁnite width, ﬁnite depth deep MLPs, and likewise Novak
et al. (2018), for deep convolution networks. Daniely et al. (2016) also proved similar results in the framework of kernel
methods, where they introduced a notion of “computational skeleton,” similar to tensor programs introduced here, that
covers feedforward computation with no weight-sharing (so that, for example, it can express locally connected networks but
not convolutional networks)1.

Many previous works have exploited this DNN-GP correspondence implicitly or explicitly to build new models (Cho & Saul,
2009; Lawrence & Moore, 2007; Damianou & Lawrence, 2013; Wilson et al., 2016b;a; Bradshaw et al., 2017; van der Wilk
et al., 2017; Kumar et al., 2018; Blomqvist et al., 2018; Borovykh, 2018). In particular, Lee et al. (2018); Garriga-Alonso
et al. (2018); Novak et al. (2018) directly converted DNN to GP using this correspondence. Lee et al. (2018) constructed the
state-of-the-art (SOTA) permutation-invariant GP on MNIST, and Novak et al. (2018) achieved SOTA on CIFAR10 for any
GP with untrainable kernel.

In this paper, we generalize the DNN-GP correspondence to standard architectures and very general nonlinearities.

Corollary 2.1 (DNN-GP correspondence, informal). Let f be a network of ﬁxed standard architecture, with linear readout
layer, and with nonlinearities bounded uniformly by exp(O(x2−(cid:15))) for some (cid:15) > 0. Fix a ﬁnite input set X of the right
signature (e.g. set of batches for batchnorm network; set of sequences for RNN). Sampling f ’s parameters from iid Gaussians
induces a distribution of functions on X . If the readout layer weights are sampled independently from hidden parameters,
then this distribution weakly converges to a Gaussian process as the network widths go to inﬁnity (with ﬁxed input and
output dimensions). See Appendices D.1 and D.4.

In contrast, Matthews et al. (2018) requires φ to be linearly bounded in norm; Daniely et al. (2016) requires φ be twice-
differentiable with |φ|, |φ(cid:48)|, |φ(cid:48)(cid:48)| all bounded, or that φ = ReLU; and a sufﬁcient condition given in Novak et al. (2018) is
that φ(cid:48) exists and is bounded by exp(O(x2−(cid:15))), though it is unclear how the more general set of 3 conditions given there
compares with ours.

We hope this corollary opens the door to the design of more powerful GPs, in the way of Lee et al. (2018); Novak et al.
(2018) by converting state-of-the-art DNNs. 2

2.2. Signal Propagation in Neural Networks

Glorot & Bengio (2010); He et al. (2015) derived the popular Glorot and He initializations from consideration of hidden
state norms in a DNN with random weights. A recent line of work generalizes their studies signiﬁcantly by examining
the evolution with depth of covariance between f (xi), f (xj) and between ∇xf (xi), ∇xf (xj) for distinct inputs xi and xj,
when the DNN f is wide and parameters of f are randomized. This evolution is referred to as (forward and backward)
signal propagation in the literature (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; 2018; Hanin &
Rolnick, 2018; Chen et al., 2018; Yang et al., 2018; Pennington et al., 2017). It has been used to optimize initialization
hyperparameters to prevent gradient explosion/vanishing, even to allow training of a 10,000 layer CNN without batchnorm
or skip connections (Xiao et al., 2018).

1even though they claim that dealing with weight-tying is straightforward. It’s unclear what they had in mind, however, as there is a

signiﬁcant difference in the scaling behavior of sharing matrix transposes vs sharing no matrix transposes (see Thm 5.1 and Thm 6.3)

2For a gentler introduction to these GP results and several extensions, we recommend the reader to look at Yang (2019).

2

E(cid:104)f l(xi), f l(xj)(cid:105) and Πl

Suppose {xi}i is a set of inputs. Let f l be an l-layer MLP with activation φ and uniform width n.
1
ij ∼ N (0, σ2
n
N (0, σ2
and Πl are summarized by

ij =
i ∼
b ) for each layer (cid:96), then the signal propagation literature posits that, in the n → ∞ limit, the dynamics of Σl

E(cid:104)∇xf l(xi), ∇xf l(xj)(cid:105), with expectation taken over W (cid:96)

If Σl
w/n), b(cid:96)

ij = 1
n

Σl = σ2
w
Πl = σ2
w

E[φ(z)φ(z)(cid:62) : z ∼ N (0, Σl−1)] + σ2
b
E[φ(cid:48)(z)φ(cid:48)(z)(cid:62) : z ∼ N (0, Σl)] (cid:12) Πl−1.

(1)

(2)

Note that Σl essentially is the kernel of the corresponding GP, and Eq. (1) is the same one used in the DNN-GP correspon-
dence. Pennington et al. (2017) more generally computed the singular value distribution of the input-output Jacobian matrix
of an MLP and characterized conditions under which this distribution concentrates around 1. To make this computation and
to derive Eq. (2), they and others (Schoenholz et al., 2017; Yang & Schoenholz, 2017; 2018; Chen et al., 2018; Xiao et al.,
2018; Pennington et al., 2017) relied on
Assumption 2.2 (Gradient Independence Assumption). In backpropagation, whenever we multiply by W (cid:62) for some weight
matrix W , we multiply by an iid copy instead.

that was ﬁrst discovered by Schoenholz et al. (2017) to make good predictions and later formulated explicitly by Yang &
Schoenholz (2017). In this paper we show

Corollary 2.3 (Assm 2.2 is conditionally correct, informal). In a MLP having nonlinearities with polynomially bounded
weak derivatives, Assm 2.2 leads to the correct equation Eq. (2) and the correct singular value distribution computation
from Pennington et al. (2017), as long as the readout layer is sampled independently from other parameters and has mean 0.
In general, Assm 2.2 does not induce correct computations – for example when the last layer is global mean pooling – and
we rigorously give the correct equations, and more generally a way to compute the singular value distribution of the neural
network Jacobian, both generalized to all standard architectures without batchnorm. See Appendices D.1 and D.5.

As an example, we computed the scaling limit for the gradient norms of an LSTM and compared it against empirical
simulation (Fig. 1). The theoretical prediction is very precise already for 1000 neurons, which is typical for applications of
LSTM.

Note that this literature also studies the limit of iterating Eqs. (1) and (2) (large depth limit), but our results only apply to a
ﬁxed number of iterations, and so do not rigorously justify such limits.

Chen et al. (2018) estimates the signal propagation in tied-weights RNNs with the equations for that in untied-weights
RNNs. They ﬁnd this a fantastic approximation for simple RNNs but not quite so for gated RNNs. (cid:70) As a corollary of
Thm 4.3 we show that, indeed, the tied- and untied-weights theories agree for simple RNNs, but not for general (say, gated)
RNNs. We give the simplest counterexample of weight-tied residual network. See Appendix D.6.
Recently, Li & Nguyen (2018) investigated (forward only) signal propagation in weight-tied autoencoders. (cid:70) A version of
their main theorem allowing for arbitrary polynomially bounded activations, without restriction on smoothness, also follows
as corollary of Thm 6.3. See Appendix D.6.

We hope Cor 2.3 will allow future works to optimize initialization hyperparameters and prevent gradient explosion/vanishing
problems for modern architectures in the way of Schoenholz et al. (2017); Yang & Schoenholz (2017; 2018); Chen et al.
(2018); Xiao et al. (2018); Yang et al. (2018).

2.3. Neural Tangent Kernel

the Neural Tangent Kernel can be in general deﬁned as Kθ(x, x(cid:48)) =
For any parametrized function f (x; θ),
(cid:104)∇θf (x; θ), ∇θf (x(cid:48); θ)(cid:105) (Jacot et al., 2018). In the case when f (x; θ) is a feedforward neural network, with parame-
ters appropriately scaled (see Appendix D.1), there is a scaling limit of Kθ → K∞ when θ is randomized and f ’s widths
grow to inﬁnity (Jacot et al., 2018). This convergence allows one to predict the evolution of f (x; θ) due to gradient
descent on θ. For example, if we apply gradient ﬂow on a training set X and loss function 1
2 (f (x) − y)2, for
|X |
codomain(f ) = R, Jacot et al. (2018) derived

(x,y)∈X

(cid:80)

1

∂ft
∂t

= −

1
|X |

Kθt(X , X )(ft − f ∗)

3

Figure 1: For two different random initializations of the LSTM with 1000 neurons along with a readout layer (at each
time step), we run 100 steps forward on inputs of zero vectors and 20 steps of backpropagation-through-time. We collect
the gradient norms (cid:107)∂y100/∂ht(cid:107), t = 80, . . . , 100, where y is the readout and h is the hidden state, and plot its mean and
standard deviations (empirical1 and empirical2). Then we compute the large width limit of these gradient norms according
to Thm 5.1 and overlaid them on top (theoretical1 and theoretical2). The agreement is so precise that the theoretical curves
completely obscure the means of the empirical data.

where f ∗ is the “ground truth“ function that sends x (cid:55)→ y for every (x, y) ∈ X , and f and f ∗ are thought of dimension
|X | vectors. Jacot et al. (2018) proved that under suitable conditions, with training time T ﬁxed and width → ∞,
Kθt(X , X ) → K∞(X , X ) for all 0 ≤ t ≤ T . This means that, in the large width regime, f (in the function space) evolves
approximately according to a linear differential equation under gradient ﬂow. In this paper we show

Corollary 2.4 (NTK convergence, informal). Fix a ﬁnite input set X . Let f be a network of ﬁxed standard architecture,
with linear readout layer, and having nonlinearities with polynomially bounded weak derivatives (so in particular cannot
have batchnorm). Then over X , Kθ → K∞ almost surely as the widths of f go to inﬁnity and θ suitably randomized, for
some K∞. See Appendices D.1 and D.7.

While Jacot et al. (2018) is groundbreaking in producing an equation to predict the behavior of gradient descent in the small
learning rate, large width regime, its proof of the convergence Kθ → K∞ relies on taking the widths to inﬁnity one by
one starting from the ﬁrst layer3. This is an unrealistic limit, as in practice, the widths are typically of the same order of
magnitude4. In comparison, Cor 2.4 proves that the limit exists as the widths tend to inﬁnity together, and it generalizes to
arbitrary standard architectures. (cid:70) We give an example computation of the NTK for a CNN in Appendix D.7; this is a new
result that has not appeared in prior literature.

Amari et al. (2018); Karakida et al. (2018) recently used Assm 2.2 to study the empirical Fisher information matrix (FIM),
over ﬁnitely many datapoints drawn from isotropic Gaussian, of random neural networks, speciﬁcally its spectral properties.
If we let Jf be the |X | × |θ| matrix whose rows are {∇xf (x; θ)}x∈X , then (empirical) FIM ∝ Jf Jf (cid:62) while NTK is
Jf (cid:62)Jf 5. Thus the spectral properties of empirical FIM and NTK are identical up to scaling. (cid:70) By Cor 2.4, we can then
justify the computations of Amari et al. (2018); Karakida et al. (2018) rigorously.

2.4. Other Works

Very recently, Du et al. (2018b;a); Allen-Zhu et al. (2018b;c); Zou et al. (2018) formally proved that GD or SGD can reduce
an overparametrized DNN’s training error to 0 by showing that random initialization imbues the network with certain good

3An earlier version of this paper claimed that it assumes gradient independence; this is incorrect, as the sequential limit obviates the

need for it

4Indeed, when the last layer is global mean pooling rather than random Gaussians, the sequential limit obtains a different answer than

simultaneous limit

5Karakida et al. (2018) called NTK the dual matrix

4

80.082.585.087.590.092.595.097.5100.0time106105104103102101100gradnormLSTM gradient normsempirical1theoretical1empirical2theoretical2properties6 and, with small learning rate, the network never moves too far from its initialization7. Allen-Zhu et al. (2018a);
Cao & Gu (2019) also show generalization bounds under various data assumptions using a similar reasoning.

There is a long line of work investigating random classic spiking or hopﬁeld networks, for example Landau & Sompolinsky
(2018); Crisanti & Sompolinsky (2018); Kadmon & Sompolinsky (2015); Stern et al. (2014); Rajan et al. (2010); Sompolinsky
et al. (1988); Amit et al. (1985). In the reinforcement learning literature, Osband et al. (2018); Burda et al. (2018a;b) used
random DNNs for exploration. Other than the works discussed above, Li & Saad (2018); Giryes et al. (2016); Gabri et al.
(2018); Reeves (2017); Fletcher & Rangan (2017) also considered neural networks with random weights.
(cid:70) Our technique is general enough to rederive the semicircle law for the Gaussian Orthogonal Ensemble and the Marchenko-
Pastur Law for Wishart matrices (Tao, 2012). See Appendices D.2 and D.3.

Approximate Message Passing is an algorithm for recovering a ground truth vector from noisy measurements (Compressed
Sensing) (Donoho et al., 2009). In one view, the algorithm repeatedly applies a certain neural network to the noisy
measurement, and it succeeds if the result eventually converges to the ground truth vector. Previous works have shown that
when the measurement matrix is randomized and the dimension goes to inﬁnity, this algorithm satisﬁes a set of equations
called State Evolution that can be used to reason about its behavior (Bayati & Montanari, 2011; Berthier et al., 2017). Their
proofs are based on the same Gaussian conditioning technique used here. (cid:70) In Appendix D.8, we detail the algorithm and
State Evolution, and prove the validity of State Evolution equations for arbitrary polynomially bounded nonlinearities and
test functions, removing the smoothness assumption of Bayati & Montanari (2011) (in exchange for a stronger moment
condition on the measurements).

This concludes the discussion of related works and our corollaries. We now present the tensor program framework and our
main theorems.

3. Tensor Programs

Consider programs of the following form, which we call tensor programs. Each line contains an assignment and a dimension
annotation and can have the following types.

VecIn (G) a vector input x

MatIn (A) a matrix input A

T (A) transpose of an A-var

l : gl := x ∈ Rnl

l : Al := A ∈ Rnl

1×nl

2

l : Al := (Aj)(cid:62) ∈ Rnl

1×nl

2 = Rnj

2×nj

1

MatMul (G) if Ak and gj have nk

2 = nj, then an assignment via a linear mapping

or similarly for H-vars

where j, k < l

l : gl := Akgj ∈ Rnl

= Rnk

1

l : gl := Akhj ∈ Rnl

= Rnk

1

LinComb (G) if nj1 = · · · = njk , then an assignment via linear combination of G-vars that appeared in previous lines:

with al
ji

∈ R,

l : gl := al

j1gj1 + · · · + al

jk gjk ∈ Rnl

= Rnj1 .

Nonlin (H) if nj1 = · · · = njk , then an assignment via some general (possibly nonlinear) function fl : Rk → R, acting

coordinatewise,

l : hl := fl(gj1 , . . . , gjk ) ∈ Rnl

= Rnj1 .

6using tools similar to ones in the signal propagation literature
7using reasoning similar to Jacot et al. (2018)

5

Here (G) marks those variables that we call G-vars, and similarly we have A-vars and H-vars. Vars introduced by VecIn
and MatIn are also called (vector and matrix) input vars. The initial “l :” marks the line number, and each new variable
formed from this line is labeled with a superscript l. A partial program with nl and input G- and A-vars unspeciﬁed is called
a (program) skeleton, typically denoted by Greek letters like π. This skeleton can be thought of as a generalization of the
skeleton in Daniely et al. (2016) in the language of a straightline program that allows weight sharing (transposed or not) and
simple type checking.

3.1. Examples

Such tensor programs can express the computation in most neural network scenarios. In Appendix B, we give example
programs for computations in (1) MLP, forward and backward passes (B.1); (2) batch of input (B.2); (3) residual networks
(B.3); (4) simple RNN (B.4); (5) batchnorm (B.5); (6) CNNs (B.6). It’s also clear from these examples that any combination
of such computation can be expressed faithfully in a tensor program. On the other hand, tensor programs don’t capture all
neural network computations, and one example is layer normalization (Ba et al., 2016), but see Section 8 for how to still
compute its scaling limit in this framework.

3.2. Setup

Lines of type T, MatMul, LinComb, and Nonlin induce equality constraints on the dimensions nl. Given a skeleton π and a
possible set of additional dimensional constraints Λ ⊆ {“nl = nm”}gl,gm , consider the smallest equivalence relation ∼
on G-vars such that gl ∼ gm if “nl = nm” ∈ Λ or if they are constrained to have equal dimension by some line of type T,
MatMul, LinComb, or Nonlin. We call each class a common dimension class (CDC) of (π, Λ) and write c(gl) for the class
of a G-var gl. The collection of all common dimension classes is written as C(π,Λ), or just C when π and Λ are understood
from context. An algorithm to compute CDCs is presented in Appendix A.

In this work, for every skeleton π (equipped with Λ), we study the behavior of vars in its realizations when the input vars are
appropriately randomized and as the dimensions nl → ∞. More precisely, we consider a sequence (in t ∈ N) of dimensions
2 }Al respecting ∼, along with input G- and A-vars glt, Alt of appropriate dimensions. For each c ∈ C,
{nlt}glorhl ∪ {nlt
let nct = nlt for gl ∈ c. We extend the notations glt and hlt to the non-input G- and H-vars computed from these inputs.

1 , nlt

At time t, we sample independently Alt
independently gcint
µcint : cin → R, K cint : cin × cin → R are speciﬁed mean and covariance at time t.

i ∼ N (µcint, K cint) for each i, j. Here cin is the set of input G-vars in c, gcint

ij ∼ N (0, (σlt)2/nlt

8. For each c ∈ C, we also sample
i )gl∈cin , and

2 ) for a set {σlt}Al

i = (glt

the data {nct}c∈C, {σlt}Al, {µcint}c∈C,

and {K cint}c∈C realize a random program
Thus given (π, Λ),
π({nct}c∈C, {σlt}Al , {µcint}c∈C, {K cint}c∈C). Its vars are random variables and our theorems will concern certain “mo-
ments” of them.
We assume that, as t → ∞, for all c, c(cid:48) ∈ C: (1) nct is increasing with t and nct → ∞. (2) limt→∞ nct/nc(cid:48)t = αc,c(cid:48) ∈ (0, ∞)
for some constant αc,c(cid:48) depending only on c, c(cid:48).
(3) σlt → σl∞ for some ﬁnite σl∞ > 0 for each input A-var Al.
(4) µcint → µcin∞ and K cint → K cin∞ for some ﬁnite µcin∞, K cin∞, and rank K cint = rank K cin∞ for all large t.

Discussion Tensor programs are meant to represent the “body” of a neural network where all dimensions are large
compared to input and output dimensions. The CDCs are used to capture the varying widths of practical neural networks;
for example, while widths typically increase and decrease in classical networks, they are held constant in blocks in residual
networks (see Appendix B.3 for an example). For the ﬁrst read-through, we recommend the reader to assume all dimensions
are the same so that there is a single CDC consisting of all G-vars.

The sampling of A-vars reﬂects variants of Glorot initialization (Glorot & Bengio, 2010) used in practice. The sampling of
input G-vars models the distribution of the ﬁrst hidden layer across multiple inputs, sampling of the ﬁrst layer parameters
(see Appendix B.1 for an example), and/or sampling of bias vectors. Most often, the vector vars should be thought of as
hidden layer quantities whose dimensions go to inﬁnity; neural network inputs (of ﬁxed dimension) are indirectly expressed
as above, and outputs (of ﬁxed dimension) are obtained as some coordinates of a vector var.

8 We could as well assume that there is an inﬁnite 2D array of independent Gaussian variables { ˚Al
ij = σlt ˚Al
2 . In that case, we do not need nct to increase stricty with t
nlt

ij/(cid:112)

set Alt

ij ∼ N (0, 1)}∞

i,j=1, and at time t,

6

Thms 4.3 and 5.1 below say that, under certain conditions, G-vars converge to Gaussians of speciﬁc mean and covariances
(hence the name “G-var”). But Thm 6.3 shows that in general this may not be true.

Notation We will often identify functions Z : A → B with vectors in BA (which should be thought of as dictionaries
with keys in A). Given a subset U ⊆ A, Z U is the subvector of Z supported on U , or as a function is the restriction of
Z on U . For ψ : BU → C, ψ(Z) def= ψ(Z U ), i.e. we automatically ignore the values of Z outside U . We use a.s.−−→ for
convergence almost surely.

4. Programs with No Transposes

For any c ∈ C, we recursively deﬁne

and recursively deﬁne

µc(gl) =






µcin∞(gl)
(cid:80)
i al

jiµc(gji )

0

if gl ∈ cin
if gl := (cid:80)
i al
if gl := Akgj or gl := Akhj

jigji

K c(gl, gm) =






i am
ji
i am
ji

K cin∞(gl, gm)
(cid:80)
K c(gl, gji)
(cid:80)
K c(gji, gm)
(σk∞)2 Ez[fa(z)fb(z)]
0

if gl, gm ∈ cin
if gm := (cid:80)
i am
ji gji
if gl := (cid:80)
jigji
i al
if gl := Akha, gm := Akhb
else

(3)

(4)

where ha := fa(gj1, . . . , gjk ), hb := fb(gj(cid:48)
k(cid:48) ), and z ∼ N (µc, K c). We also make branch 4 cover the case when
gl := Akga or gm := Akgb by “typecasting” ga to an H-var and setting fa = id (similarly for gb). Note that, as discussed in
Notations above, fa will ignore irrelevant components of a, and the expectations only depend on the entries of µc and K c
that correspond to already-deﬁned values, so this describes a valid recursion.

1, . . . , gj(cid:48)

We introduce the following technical assumption.
Assumption 4.1 (Almost Sure Rank Convergence). For any Ak and any collection S ⊆ {G- or H-var h : gl
:=
Akh for some l}, let H t ∈ Rnct×|S| be the matrix whose columns are hmt or gmt for each hm or gm in S.
If
nct H t(cid:62)H t ∈ R|S|×|S| converges almost surely to some C ∗ with t → ∞, then almost surely rank H t = rank C ∗ for
1
all large t.

If we don’t have lines of type LinComb, and no fl is a polynomial, then the C ∗s are all full rank, implying rank convergence
by the upper semicontinuity of rank. LinComb lines may add linear dependencies, but they are constant with t, so that
rank H t = rank C ∗ in the limit and we still have rank convergence.
Deﬁnition 4.2. For α > 0, a function φ : Rk → R is said to be α-controlled if for some C, c > 0, |φ(x)| ≤ eC (cid:80)k
for all x ∈ Rk.

i=1 |xi|α+c

Theorem 4.3. Consider dimension constraints Λ and a skeleton π without T lines, i.e. no transpose allowed. Suppose all fl
are α-controlled for some α < 2. Sample all input vars as in Section 3.2 and assume almost sure rank convergence. Then
for any c ∈ C and any α-controlled function φ : Rc → R, α < 2,

1
nct

nct
(cid:88)

i=1

φ(gct

i ) a.s.−−→ E φ(Z)

where gct

i = (glt

i )gl∈c and Rc (cid:51) Z = (Z g)g∈c ∼ N (µc, K c).

Discussion Roughly speaking, G-vars created from the same matrix Ak have nonzero correlations, but otherwise are
asymptotically independent modulo LinComb. Intuitively, gct

i “ d= ”N (µc, K c) for large t, iid for each i.

7

1 : A1 := A1
...

LA : ALA := ALA
LA + 1 : gLA+1 := x1

...

LA + Lg : gLA+Lg := xLg

LA + Lg + 1 : . . . := . . .
...
L : . . . := . . .

noninput line types

L + 1 : AL+1 := (A1)(cid:62)

...

L + LA : AL+LA := (ALA)(cid:62)

L + LA + 1 : gL+LA+1 := v1

...

last line

L + LA + L∇ : gL+LA+L∇ := vL∇

(a) Program π

(b) Extended program ˜π

Figure 2

There is an apparent contradiction in Thm 4.3 if we consider deep linear networks with tied (y = W Lx ∈ Rn, W ∈ Rn×n)
and untied weights (y = (cid:81)L
l=1 W (l)x, ∀l[W (l) ∈ Rn×n]). Via simple computations of µc and K c, one sees that, by Thm 4.3,
as the width n → ∞, y is distributed “similarly” in either case (in that α-controlled moments match asymptotically). This
seems to contradict our intuition that W Lx should blow up or decay exponentially, with L, along the direction of the
eigenvector of W corresponding to the largest eigenvalue of W ; whereas in the untied case it’s easy to see that each yi
converges in distribution to an i.i.d. Gaussian.

This apparent paradox is resolved by noting that Thm 4.3 only applies for ﬁxed skeletons (so ﬁxed L in this example), as
widths → ∞. By Rider (2003), the maximum eigenvalue of W scales like 1 + O(n−1/2) if Wij ∼ N (0, 1/n), and so does
that of W L for ﬁxed L. Furthermore, as n increases, the components of x corresponding to large eigenvalues (≥ 1) of W
decrease in magnitude to 0 in probability, by the circular law (Tao, 2012). So the L at which the exponentiating effect of
W L kicks in increases with n.

5. Backprop with Zero Mean Gradients

Let π be a skeleton with L lines but no T. WLOG, suppose all input vars appear at the start, with matrix inputs ﬁrst, as
in Fig. 2a. Consider an extension ˜π of π in the following way: The ﬁrst few appended lines are transposes of A-vars
in π, followed by a series of new vector input vars {vl}L∇
l=1, as in Fig. 2b. Lines appended below this can be arbitrary
non-input lines except that (1) lines of type MatMul must use a transposed matrix AL+1 to AL+LA and hl or gl must have
been introduced after π (i.e. l > L), and (2) any hl for l > L + LA + L∇, as a function of v1, . . . , vL∇ , must be odd:
for any ﬁxed values of {gl}l≤L, hl(−v1, . . . , −vL∇ , {gl}l≤L) = −hl(v1, . . . , vL∇ , {gl}l≤L); likewise gl must be odd for
l > L + LA. This in particular means that LinComb lines cannot involve gl for l ≤ L.

If π expresses the forward computation of an NN f without matrix transposes, then ˜π has enough power to express the
backpropagation of f and compute the gradients with respect to hidden states. For example, if f (x) = v(cid:62)ρ(x), so that
∂f
∂x = v(cid:62) ∂ρ
∂x is an odd function of v and can be expressed as a ˜π as above (see Appendix B.1 for a concrete
example). In general, the multiple {vi} allow for multiple NN outputs.

∂x , then ∂f

CDCs are naturally deﬁned for ˜π (see Appendix A) just like before. We extend µc and K c to vars introduced in ˜π: For

8

l, m > 0, and k ≤ LA, set µc(gL+l) = 0 and when l > L or m > L, set K c(gl, gm) =






i am
ji
i am
ji

K cin∞(gl, gm)
(cid:80)
K c(gl, gji)
(cid:80)
K c(gji, gm)
(σk∞)2α Ez fa(z)fb(z)
0

if gl, gm are input vars
if gm := (cid:80)
ji gji
i am
if gl := (cid:80)
ji gji
i al
if gl := AL+kha, gm := AL+khb
else

nc1(Ak )t
nc2(Ak )t , and branch 4 covers the case when gl := AL+kga or gm := AL+kgb by taking

where α = αc1(Ak),c2(Ak) = limt→∞
fa or fb to be identity. Note that covariances between vars of π and new vars in ˜π are 0.
Theorem 5.1. Sample {vit}L∇
i=1 with zero mean (i.e. µcint(gL+LA+i) → 0 for all i ∈ [L∇]) and independently from the
input vars {xlt}Lg
) = 0 if l > L ≥ l(cid:48)) 9. Sample all other vars in ˜π according to Section 3.2. Assume all
fl of ˜π are polynomially bounded and ˜π satisﬁes almost sure rank convergence. Then for any dimension constraints Λ, any
c ∈ C(˜π,Λ), and any polynomially bounded function φ : Rc → R,

l=1 (i.e. K cint(gl, gl(cid:48)

1
nct

nct
(cid:88)

i=1

φ(gct

i ) a.s.−−→ E φ(Z)

where gct

i = (glt

i )gl∈c and Z ∼ N (µc, K c).

Note that our result here does not apply to batchnorm, whose Jacobian has a singularity on a 1-dimensional afﬁne subspace
(and in particular, at the origin). This theorem allows one to justify the gradient independence assumption rigorously; see
Appendix D.5.

6. General Tensor Programs

Thm 5.1 does not give the correct computation if {vit}i do not have zero mean: Consider a one-hidden-layer MLP with
quadratic activation, f (x) = 1(cid:62)φ(W x), φ(−) = 1
∂x = W (cid:62)(1(cid:12)(W x)) =
W (cid:62)W x. If n1 = n0, and Wij ∼ N (0, 1/n0), then E ∂f
= xi. If we have assumed Thm 5.1 is correct, then we would
∂xi
(cid:80)n0
have (incorrectly) computed E 1
i=1(W (cid:48)(cid:62)W x)i = 0 where W (cid:48) is an iid copy of W .
n0

2 (−)2, x ∈ Rn0

, W ∈ Rn1×n0

a.s.−−→ E 1
n0

, 1 ∈ Rn1

. Then ∂f

(cid:80)n0
i=1

∂f
∂xi

Below, we develop a theory of the scaling limit of general tensor programs, from which follows the correct way of computing
gradients when vit do not have 0 mean.

We ﬁrst introduce “extended syntax” programs, which are equivalent semantically to programs of original syntax, and then
show that we can “compile” original syntax programs to extended syntax programs with no transposes, with the same
scaling limit in a suitable sense.

Deﬁnition 6.1. Extended syntax programs are those that allow all line types of Section 3 and in addition

Comp (H) if nj1 = · · · = njk , then an assignment via some general (possibly nonlinear) function fl : Rk → R

where h1, . . . , hk are previous G- or H-vars, and fl acts coordinatewise.

l : hl := fl(h1, . . . , hk) ∈ Rnl

= Rnj1

So in essence, extended syntax programs just allow Nonlin lines to take H-vars in addition to G-vars. While in the original
syntax, H-vars must feed into lines of type MatMul, in extended syntax they can also be used to create new H-vars via
coordinatewise action.

One can deﬁne CDCs for extended syntax programs just as before (see Appendix A). Each extended syntax program is
equivalent to an original syntax program, by expanding the deﬁnition of each H-var to a function of previous G-vars. For

9In our previous example of f (x) = v(cid:62)ρ(x), this corresponds to the readout layer v sampled with zero mean and independently from

x and other parameters of ρ.

9

= id. (In our example above, fhl

example, if hl := fl(hk, gm), and hk := fk(gr), then the expanded deﬁnition of hl is fl(fk(gr), gm). We call this the
for this expanded function, so that hl = fhl
expanded deﬁnition of hl, and write fhl
(gl1 , . . . , glm) for some l1, . . . , lm < l;
for G-vars, we also deﬁne fgl
(x, y) = fl(fk(x), y)). So by replacing each line of
type Comp with its expanded deﬁnition, we can convert an extended syntax program to an original syntax program with the
same semantics for all vector vars.
Deﬁnition 6.2. Let π be an original syntax skeleton with associated sampling data {σlt}Al , {µcint, K cint}c∈C. We deﬁne an
extended syntax skeleton ˇπ, called the detransposition of π, by induction on line number as follows. During this process,
we keep track of an injective mapping ϕ taking vector (resp. matrix) vars of π to vector (resp. matrix) vars of ˇπ, along
with a specialized mapping ϕg taking a G-var of π produced by MatMul to a G-var of ˇπ. We use a check ˇ to denote
objects of the detransposition. We also simultaneously set {ˇσlt}ˇAl , {µ ˇcint}c and {K ˇcint}c of the detransposition. They
propagate according to the usual rules, Eqs. (3) and (4), to determine µˇc and Kˇc. Let l be the current line number of π we
are processing, and let (cid:96) denote the 1 + length of the current ˇπ (this is where we are adding new lines in ˇπ).

1. If gl := x is a VecIn line, then add a line of the same type to ˇπ, (cid:96) : ˇg(cid:96) := x. Set ϕ(gl) ← ˇg(cid:96), µ ˇcint(ˇg(cid:96)) ← µcint(gl),

where ˇc = c(ˇgl) and c = c(gl). Set K ˇcint(ˇg(cid:96), ϕ(gm)) ← K cint(gl, gm) for all input G-vars gm with m < l.

2. If Al := A is a MatIn line, then add to ˇπ the line (cid:96) : ˇA(cid:96) := A. Set ϕ(Al) ← ˇA(cid:96) and ˇσ(cid:96)t ← σlt for all t ∈ [1, ∞].

3. If Al := Am(cid:62) is a T line, then add to ˇπ an input line (cid:96) : ˇA(cid:96) := A(cid:48) for a new input A(cid:48) sampled iid as Am(cid:62). Set

ϕ(Al) ← ˇA(cid:96) and ˇσ(cid:96)t ←

(cid:113) n1(Amt)

n2(Amt) σmt, ∀t ∈ [1, ∞].

4. If gl := al

j1 gj1 + · · · + al

jk gjk is an LinComb line in π, we add a line of the same type in ˇπ:

and set ϕ(gl) ← ˇg(cid:96) if each of ϕ(gji) is a G-var in ˇπ; or we add a line of type Comp

(cid:96) : ˇg(cid:96) := al
j1

ϕ(gj1) + · · · + al
jk

ϕ(gjk )

(cid:96) : ˇh(cid:96) := al
j1

ϕ(gj1) + · · · + al
jk

ϕ(gjk )

and set ϕ(gl) ← ˇh(cid:96) if some ϕ(gji) is an H-var in ˇπ.

5. If hl := fl(gj1 , . . . , gjk ) is a line of type Nonlin, then we add to ˇπ a line of type Comp

(cid:96) : ˇh(cid:96) := ˇf(cid:96)(ϕ(gj1), . . . , ϕ(gjk ))

where ˇf(cid:96) = fl, and we set ϕ(hl) ← ˇh(cid:96). (If all ϕ(gji) are G-vars then we also typecast this line to Nonlin)

6. Suppose gl := Ahm is a line of type MatMul in π, where A is some previous A-var. Consider the A-var A(cid:48) where
A(cid:48) := A(cid:62) if A is an input A-var, or A := A(cid:48)(cid:62) if A is a transposed var. Let gi := A(cid:48)hi, i = 1, . . . , s, be all previous
lines of type MatMul involving A(cid:48), where hi can be G- or H-var. Deﬁne C ∈ Rs×s, v ∈ Rs by

Cij = E fϕ(hi)(Z)fϕ(hj )(Z),
vi = E fϕg(gi)(Z)fϕ(hm)(Z),

where Z ∼ N (µˇc, Kˇc) (the expectation will only depend on components of Z corresponding to previous lines).
Compute a = αC +v ∈ Rs, where α = limt

n2(At)
n1(At) . Then we add the following to ˇπ:

(cid:96) : ˇg(cid:96) := ϕ(A)ϕ(hm)

(cid:96) + 1 : ˇh(cid:96)+1 := ˇg(cid:96) +

s
(cid:88)

j=1

ajϕ(hj)

MatMul

Comp

If ϕ(hj) are all G-vars, we typecast line (cid:96) + 1 to LinComb and write ˇg(cid:96)+1 instead. Set ϕ(gl) ← ˇh(cid:96)+1 and ϕg(gl) ← ˇg(cid:96).

See Appendix B.1.1 for a concrete example of detransposition . Below, for any c ∈ Cπ, let ¯c be c ∪ {h : c(h) = c}, i.e. the
collection of H- or G-vars with the same dimension constraint; see Appendix A.

10

Theorem 6.3. Let π be an original syntax program with sampling instructions, and ˇπ be the detransposition of π, with ϕ
the mapping from vector vars of π to vector vars of ˇπ. Assume all fl of π are polynomially bounded, and that almost sure
rank convergence holds for ˇπ. Sample input vars of π according to Section 3.2. Then for any dimension constraints Λ, any
c ∈ C(π,Λ), and any polynomially bounded function φ : R¯c → R,

1
nct

nct
(cid:88)

i=1

φ(hct

i ) a.s.−−→ E φ((fϕ(h)(Z))h∈¯c)

is the sequence of the ith coordinates of all vector vars in ¯c, and Z ∼ N (µˇc, Kˇc).

where hct
i
If all fl in π are differentiable, then we can take a in Item 6 of Defn 6.2 to be ασ2(E ∂
σ = σr∞ and r is the line number of ϕ(A(cid:48)), and the above almost sure convergence will still hold.

Zϕg (gi) fϕ(hm)(Z))i∈[s]

10, where

See Appendices D.1 to D.3 for example applications of this theorem to random MLP, and rederivation of the semicircle and
j=1) for A ∈ Rn×m, hi ∈ Rm, F :
Marchenko-Pastur laws. Thm 6.3 has the following basic intuition. Let g = A(cid:62)F ((Ahj)k
Rk×n → Rn, ((zj ∈ Rn)k
j=1). Here F should be thought of the stretch of π that separates g from previous
G-vars induced by A. Then, semirigorously, as n, m → ∞ and Aij ∼ N (0, 1/n), for any i ∈ [m],

j=1) (cid:55)→ F ((zj)k

E yi ≈ E A(cid:62)
:i

(cid:110)

F ((A\ihj

\i)k

j=1) + (cid:104)F (cid:48), (A:ihj

i )k

j=1(cid:105)

(cid:111)

i

m=1

i )k

j=1 hj

j=1(cid:105) = (cid:80)k

:i (cid:104)F (cid:48), (A:ihj

\i is hj without ith coordinate, and F (cid:48) is the Jacobian of F at (A\ihj
\i)k

where A\i is A without ith column, hj
\i)k
j=1. Now
(cid:80)n
E A(cid:62)
j=1) is approx-
imately a Gaussian with 0 mean. Then g is roughly a Gaussian plus a linear combination of {hj}k
j=1. So, unlike restricted
programs in Thms 4.3 and 5.1, we do not expect g to be “Gaussian” in the limit, as hj could be the image of an activation
function. We can, however, still keep track of g’s decomposition into a Gaussian and a linear combination part — this is the
key idea behind detransposition, and in particular, its step 6. There, (cid:126)a(cid:48)
j are the coefﬁcients of this linear combination, while
(cid:126)ai records the correlation between g and previous G-vars induced by A(cid:62). Each (cid:126)a(cid:48)
j can be seen to be exactly the coefﬁcient
computed heuristically here by applying Stein’s lemma (Lem E.8) when r = 0 in step 6.

because F (cid:48) is independent of A:i. Likewise, A(cid:62)

:i F ((A\ihj

∂Fm
∂zj
m

7. Proof Techniques

While our tensor program framework is new and Bayati & Montanari (2011) is concerned with a much simpler setting
of AMP algorithms, its technique of Gaussian conditioning is useful to us (Lem E.3): If A is a Gaussian matrix, then
conditioned on G = AH, G(cid:48) = A(cid:62)H (cid:48), the distribution of A is E + Π ˜AΠ(cid:48) for some mean matrix E, projection matrices
Π, Π(cid:48), and ˜A distributed iid as A. If we let G and H be previous G- and H-vars in MatMul lines involving A, and similarly
for G(cid:48), H (cid:48) with respect to A(cid:62), this allows us to induct on line number by conditioning on previous lines.

Compared to Bayati & Montanari (2011), our input G-vars have all ﬁnite moments, whereas the analogous quantities in
Bayati & Montanari (2011) just have a bounded number of them. This allows us to simplify the induction somewhat, and
remove the smoothness assumption on (the functions playing the same role as) fl that is required in Bayati & Montanari
(2011). The latter is a result of two facts: 1. Gaussian averaging is smooth: E[Φ(z) : z ∼ N (µ, Σ)] is generically smooth
in µ and Σ. 2. if Π ∈ Rn×n is a projection matrix of rank n − O(1), then Πz for an isotropic Gaussian vector z has
approximately independent coordinates, in so far as a law of large number is concerned. This is shown by ﬁrst bounding the
off-diagonal correlations of Π using linear programming duality (Lem E.19) and then bounding the moments of (cid:80)
i φ(Πz)i
using the Hermite expansion of φ and the previous bound on correlations of Πz (Thm E.21). Again, these two tools were not
accessible to Bayati & Montanari (2011) because of their assumptions of only a ﬁnite number of input moments. Note that a
more straightforward application of the logic of Bayati & Montanari (2011) would not allow us to reason about nonsmooth
functions such as the step function which appears as the gradient of ReLU.

10even if not all fl are differentiable, as long as the covariance {Kˇc(ϕg(gi), ϕg(gj))}i,j∈[s] is nonsingular, we may take the distribu-
tional derivatives and interpret the expectation as the application of this (tempered) distribution to the Gaussian density function. Then the
theorem holds under this interpretation. Even if the covariance is singular, we may consider a subset {ϕg(gi)}i∈I of maximal rank, and
still apply the tempered distribution interpretation; see the proof for more details.

11

8. Discussion

In this paper, we have introduced a notion of a tensor program able to express almost all neural network computations in
modern deep learning, and characterized its scaling limits. As corollaries, we generalized the DNN-GP correspondence,
rigorously derived correct equations for signal propagation in neural networks, and proved the convergence of NTK, among
many other results discussed in Section 2.

While our results assume Gaussian sampling, we expect the results to hold when we sample from other “nice” distributions
(say, with a few ﬁnite moments). In the random matrix and statistical physics literature, this universality is very common. In
our case, the central limit intuition for DNN-GP correspondence, for example, would indeed hint at this.

We also believe that the rank convergence assumptions are not necessary, but just side effects of our Gaussian conditioning
technique. One should be able to remove them by considering some stability property of tensor programs.

Our framework, while surprisingly general, as presented doesn’t cover a few deep learning layers, but can be easily extended
to do so in all but one case: • Dropout. Our framework can already cover “Gaussian dropout”; binary dropout can be
incorporated easily as well by introducing Bernoulli random variables in the way of Schoenholz et al. (2017). • Layernorm.
Our framework only allows fl with a ﬁxed signature, as “width” grows, for example batchnorm with a ﬁxed batch size.
However, as we take width to inﬁnity, the signature for layernorm also changes. But our theorems show that the mean and
variance of the layer converges a.s. to a deterministic limit, so that in the forward pass, layernorm is asymptotically the
same as a linear, coordinatewise fl. A brief computation shows that the gradient of layernorm can also be asymptotically
expressed via a tensor program in a similar way. Nevertheless, non-“coordinatewise” fl is worth investigating in the future,
perhaps to take inspiration from the work of Berthier et al. (2017) that studied this scenario in the setting of State Evolution
for AMP. • Batchnorm, when reasoning about gradients. Our framework does not allow singularities in fl, whereas during
backprop batchnorm’s derivative contains a singularity at the origin. Yang et al. (2018) demonstrates that empirically our
equations should still extend to this case. We leave this to future work.

Our scaling limit results only apply to ﬁxed tensor program skeletons. This would be enough to derive the behavior of a DNN
on a dataset which is small compared to the width. But perhaps more reasonable is when the dataset size is commensurate or
perhaps even larger than the width of the network. This would require taking a joint limit in both the skeleton size, over the
data distribution, and over the dimensions {nl}l; see Pennington & Worah (2017; 2018) for analogous settings for 2 or 3
layer networks and Gaussian data. We leave the investigation of this to future work.

The tensor program framework naturally lends to an automation of computations regarding random neural networks, given
the program underlying it. Our community might ﬁnd valuable a module in PyTorch or Tensorﬂow that computes the
corresponding µc and K c automatically given the tape (for PyTorch) or the computation graph (for Tensorﬂow).

We hope the tools presented in this work will be adopted by any researcher interested in studying random neural networks.

Acknowledgement

Thanks are due to Jascha Sohl-Dickstein, Sam Schoenholz, Jeffrey Pennington, Raphael Berthier, Ilya Razenshteyn,
Pengchuan Zhang, Hadi Salman, and Zeyuan Allen-Zhu for discussions and help on initial drafts

12

References

Allen-Zhu, Z., Li, Y., and Liang, Y. Learning and Generalization in Overparameterized Neural Networks, Going Beyond
Two Layers. arXiv:1811.04918 [cs, math, stat], November 2018a. URL http://arxiv.org/abs/1811.04918.

Allen-Zhu, Z., Li, Y., and Song, Z. A Convergence Theory for Deep Learning via Over-Parameterization. arXiv:1811.03962

[cs, math, stat], November 2018b. URL http://arxiv.org/abs/1811.03962.

Allen-Zhu, Z., Li, Y., and Song, Z. On the Convergence Rate of Training Recurrent Neural Networks. arXiv:1810.12065

[cs, math, stat], October 2018c. URL http://arxiv.org/abs/1810.12065.

Amari, S.-i., Karakida, R., and Oizumi, M. Fisher Information and Natural Gradient Learning of Random Deep Networks.

arXiv:1808.07172 [cond-mat, stat], August 2018. URL http://arxiv.org/abs/1808.07172.

Amit, D. J., Gutfreund, H., and Sompolinsky, H. Spin-glass models of neural networks. Physical Review A, 32(2):1007–1018,
August 1985. doi: 10.1103/PhysRevA.32.1007. URL https://link.aps.org/doi/10.1103/PhysRevA.32.
1007.

Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer Normalization. arXiv:1607.06450 [cs, stat], July 2016. URL http:

//arxiv.org/abs/1607.06450.

Bahdanau, D., Cho, K., and Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate.

arXiv:1409.0473 [cs, stat], September 2014. URL http://arxiv.org/abs/1409.0473.

Bayati, M. and Montanari, A. The dynamics of message passing on dense graphs, with applications to compressed
sensing. IEEE Transactions on Information Theory, 57(2):764–785, February 2011. ISSN 0018-9448, 1557-9654. doi:
10.1109/TIT.2010.2094817. URL http://arxiv.org/abs/1001.3448.

Berthier, R., Montanari, A., and Nguyen, P.-M. State Evolution for Approximate Message Passing with Non-Separable

Functions. arXiv:1708.03950 [cs, math], August 2017. URL http://arxiv.org/abs/1708.03950.

Blomqvist, K., Kaski, S., and Heinonen, M. Deep convolutional Gaussian processes. arXiv preprint arXiv:1810.03052,

2018.

Bolthausen, E. An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model.

arXiv:1201.2891 [cond-mat, physics:math-ph], January 2012. URL http://arxiv.org/abs/1201.2891.

Borovykh, A. A gaussian process perspective on convolutional neural networks. arXiv preprint arXiv:1810.10798, 2018.

Bradshaw, J., Matthews, A. G. d. G., and Ghahramani, Z. Adversarial examples, uncertainty, and transfer testing robustness

in gaussian process hybrid deep networks. arXiv preprint arXiv:1707.02476, 2017.

Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros, A. A. Large-Scale Study of Curiosity-Driven

Learning. arXiv:1808.04355 [cs, stat], August 2018a. URL http://arxiv.org/abs/1808.04355.

Burda, Y., Edwards, H., Storkey, A., and Klimov, O. Exploration by Random Network Distillation. arXiv:1810.12894 [cs,

stat], October 2018b. URL http://arxiv.org/abs/1810.12894.

Cao, Y. and Gu, Q. A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks.

arXiv:1902.01384 [cs, math, stat], February 2019. URL http://arxiv.org/abs/1902.01384.

Chen, M., Pennington, J., and Schoenholz, S. Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables
Signal Propagation in Recurrent Neural Networks. In Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pp. 873–882, Stockholmsmssan, Stockholm Sweden,
July 2018. PMLR. URL http://proceedings.mlr.press/v80/chen18i.html.

Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning Phrase
Representations using RNN Encoder-Decoder for Statistical Machine Translation. arXiv:1406.1078 [cs, stat], June 2014.
URL http://arxiv.org/abs/1406.1078.

13

Cho, Y. and Saul, L. K. Kernel methods for deep learning. In Advances in neural information processing systems, pp. 342–
350, 2009. URL http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.

Crisanti, A. and Sompolinsky, H. Path Integral Approach to Random Neural Networks. Physical Review E, 98(6), December
2018. ISSN 2470-0045, 2470-0053. doi: 10.1103/PhysRevE.98.062120. URL http://arxiv.org/abs/1809.
06042.

Damianou, A. and Lawrence, N. Deep gaussian processes. In Artiﬁcial Intelligence and Statistics, pp. 207–215, 2013.

Daniely, A., Frostig, R., and Singer, Y. Toward Deeper Understanding of Neural Networks: The Power of Initial-
ization and a Dual View on Expressivity.
In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Gar-
nett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 2253–2261. Curran Associates, Inc.,
2016. URL http://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-
networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf.

Deshpande, Y., Abbe, E., and Montanari, A. Asymptotic mutual information for the balanced binary stochastic block model.
Information and Inference: A Journal of the IMA, 6(2):125–170, June 2017. ISSN 2049-8764. doi: 10.1093/imaiai/iaw017.
URL https://academic.oup.com/imaiai/article/6/2/125/2739335.

Donoho, D. and Montanari, A. High dimensional robust M-estimation: asymptotic variance via approximate message
passing. Probability Theory and Related Fields, 166(3):935–969, December 2016. ISSN 1432-2064. doi: 10.1007/s00440-
015-0675-z. URL https://doi.org/10.1007/s00440-015-0675-z.

Donoho, D. L., Maleki, A., and Montanari, A. Message Passing Algorithms for Compressed Sensing. Proceedings
of the National Academy of Sciences, 106(45):18914–18919, November 2009. ISSN 0027-8424, 1091-6490. doi:
10.1073/pnas.0909892106. URL http://arxiv.org/abs/0907.3574.

Du, S. S., Lee, J. D., Li, H., Wang, L., and Zhai, X. Gradient Descent Finds Global Minima of Deep Neural Networks.

arXiv:1811.03804 [cs, math, stat], November 2018a. URL http://arxiv.org/abs/1811.03804.

Du, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient Descent Provably Optimizes Over-parameterized Neural Networks.

arXiv:1810.02054 [cs, math, stat], October 2018b. URL http://arxiv.org/abs/1810.02054.

Fletcher, A. K. and Rangan, S. Inference in Deep Networks in High Dimensions. arXiv:1706.06549 [cs, math, stat], June

2017. URL http://arxiv.org/abs/1706.06549.

Gabri, M., Manoel, A., Luneau, C., Barbier, J., Macris, N., Krzakala, F., and Zdeborov, L. Entropy and mutual information
in models of deep neural networks. arXiv:1805.09785 [cond-mat, stat], May 2018. URL http://arxiv.org/abs/
1805.09785.

Garriga-Alonso, A., Aitchison, L., and Rasmussen, C. E. Deep Convolutional Networks as shallow Gaussian Processes.

arXiv:1808.05587 [cs, stat], August 2018. URL http://arxiv.org/abs/1808.05587.

Giryes, R., Sapiro, G., and Bronstein, A. M. Deep Neural Networks with Random Gaussian Weights: A Universal
Classiﬁcation Strategy? IEEE Transactions on Signal Processing, 64(13):3444–3457, July 2016. ISSN 1053-587X,
1941-0476. doi: 10.1109/TSP.2016.2546221. URL http://arxiv.org/abs/1504.08291.

Glorot, X. and Bengio, Y. Understanding the difﬁculty of training deep feedforward neural networks. In Teh, Y. W. and
Titterington, M. (eds.), Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics,
volume 9 of Proceedings of Machine Learning Research, pp. 249–256, Chia Laguna Resort, Sardinia, Italy, May 2010.
PMLR. URL http://proceedings.mlr.press/v9/glorot10a.html.

Hanin, B. and Rolnick, D. How to Start Training: The Effect of Initialization and Architecture. arXiv:1803.01719 [cs, stat],

March 2018. URL http://arxiv.org/abs/1803.01719.

Hazan, T. and Jaakkola, T. Steps Toward Deep Kernel Methods from Inﬁnite Neural Networks. arXiv:1508.05133 [cs],

August 2015. URL http://arxiv.org/abs/1508.05133.

14

He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on
In Proceedings of the IEEE international conference on computer vision, pp. 1026–
imagenet classiﬁcation.
1034, 2015. URL http://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_
Delving_Deep_into_ICCV_2015_paper.html.

He, K., Zhang, X., Ren, S., and Sun, J.

pp. 770–
778, 2016. URL https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_
Deep_Residual_Learning_CVPR_2016_paper.html.

Deep Residual Learning for Image Recognition.

Hochreiter, S. and Schmidhuber, J. Long Short-Term Memory. Neural Comput., 9(8):1735–1780, November 1997. ISSN
0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.doi.org/10.1162/neco.1997.9.8.1735.

Hu, T.-C. and L. Taylor, R. On the strong law for arrays and for the bootstrap mean and variance. International Journal of

Mathematics and Mathematical Sciences, 20, 1997. doi: 10.1155/S0161171297000483.

Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. Densely Connected Convolutional Networks.

arXiv:1608.06993 [cs], August 2016. URL http://arxiv.org/abs/1608.06993.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.

arXiv:1502.03167 [cs], February 2015. URL http://arxiv.org/abs/1502.03167.

Jacot, A., Gabriel, F., and Hongler, C. Neural Tangent Kernel: Convergence and Generalization in Neural Networks.

arXiv:1806.07572 [cs, math, stat], June 2018. URL http://arxiv.org/abs/1806.07572.

Kabashima, Y., Krzakala, F., Mzard, M., Sakata, A., and Zdeborov, L. Phase Transitions and Sample Complexity in
Bayes-Optimal Matrix Factorization. IEEE Transactions on Information Theory, 62(7):4228–4265, July 2016. ISSN
0018-9448. doi: 10.1109/TIT.2016.2556702.

Kadmon, J. and Sompolinsky, H. Transition to Chaos in Random Neuronal Networks. Physical Review X, 5(4), November
2015. ISSN 2160-3308. doi: 10.1103/PhysRevX.5.041030. URL https://link.aps.org/doi/10.1103/
PhysRevX.5.041030.

Kamilov, U. S., Rangan, S., Fletcher, A. K., and Unser, M. Approximate Message Passing with Consistent Parameter
Estimation and Applications to Sparse Learning. arXiv:1207.3859 [cs, math], July 2012. URL http://arxiv.org/
abs/1207.3859.

Karakida, R., Akaho, S., and Amari, S.-i. Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field

Approach. arXiv:1806.01316 [cond-mat, stat], June 2018. URL http://arxiv.org/abs/1806.01316.

Kumar, V., Singh, V., Srijith, P., and Damianou, A. Deep Gaussian Processes with Convolutional Kernels. arXiv preprint

arXiv:1806.01655, 2018.

Landau, I. D. and Sompolinsky, H. Coherent chaos in a recurrent neural network with structured connectivity. bioRxiv,

October 2018. doi: 10.1101/350801. URL http://biorxiv.org/lookup/doi/10.1101/350801.

Lawrence, N. D. and Moore, A. J. Hierarchical Gaussian process latent variable models. In Proceedings of the 24th

international conference on Machine learning, pp. 481–488. ACM, 2007.

Le Roux, N. and Bengio, Y. Continuous neural networks. In Artiﬁcial Intelligence and Statistics, pp. 404–411, 2007.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of

the IEEE, 86(11):2278–2324, 1998.

LeCun, Y., Haffner, P., Bottou, L., and Bengio, Y. Object recognition with gradient-based learning. In Shape, contour and

grouping in computer vision, pp. 319–345. Springer, 1999.

Lee, J., Bahri, Y., Novak, R., Schoenholz, S., Pennington, J., and Sohl-dickstein, J. Deep Neural Networks as Gaussian
Processes. In International Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=B1EA-M-0Z.

15

Li, B. and Saad, D. Exploring the Function Space of Deep-Learning Machines. Physical Review Letters, 120(24), June
2018. ISSN 0031-9007, 1079-7114. doi: 10.1103/PhysRevLett.120.248301. URL http://arxiv.org/abs/1708.
01422.

Li, P. and Nguyen, P.-M. On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and

Implications to Training. September 2018. URL https://openreview.net/forum?id=HJx54i05tX.

Matthews, A. G. d. G., Rowland, M., Hron, J., Turner, R. E., and Ghahramani, Z. Gaussian Process Behaviour in Wide
Deep Neural Networks. arXiv:1804.11271 [cs, stat], April 2018. URL http://arxiv.org/abs/1804.11271.

Neal, R. M. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD Thesis, University of Toronto, 1995.

Novak, R., Xiao, L., Lee, J., Bahri, Y., Abolaﬁa, D. A., Pennington, J., and Sohl-Dickstein, J. Bayesian Deep Convolutional

Networks with Many Channels are Gaussian Processes. arXiv preprint arXiv:1810.05148, 2018.

O’Donnell, R. Analysis of boolean functions. Cambridge University Press, New York, NY, 2014. ISBN 978-1-107-03832-5.

Osband, I., Aslanides, J., and Cassirer, A. Randomized Prior Functions for Deep Reinforcement Learning. arXiv:1806.03335

[cs, stat], June 2018. URL http://arxiv.org/abs/1806.03335.

Pennington, J. and Worah, P. Nonlinear random matrix theory for deep learning. In Advances in Neural Information

Processing Systems, pp. 2634–2643, 2017.

Pennington, J. and Worah, P. The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network. In

Advances in Neural Information Processing Systems 31, pp. 10, 2018.

theory and practice.

Pennington, J., Schoenholz, S., and Ganguli, S. Resurrecting the sigmoid in deep learning through dynamical isom-
etry:
In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,
and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30, pp. 4788–4798. Curran Associates,
Inc., 2017. URL http://papers.nips.cc/paper/7064-resurrecting-the-sigmoid-in-deep-
learning-through-dynamical-isometry-theory-and-practice.pdf.

Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S. Exponential expressivity in deep neural networks

through transient chaos. In Advances In Neural Information Processing Systems, pp. 3360–3368, 2016.

Rajan, K., Abbott, L. F., and Sompolinsky, H. Stimulus-Dependent Suppression of Chaos in Recurrent Neural Networks.
Physical Review E, 82(1), July 2010. ISSN 1539-3755, 1550-2376. doi: 10.1103/PhysRevE.82.011903. URL http:
//arxiv.org/abs/0912.3513.

Reeves, G. Additivity of Information in Multilayer Networks via Additive Gaussian Noise Transforms. arXiv:1710.04580

[cs, math, stat], October 2017. URL http://arxiv.org/abs/1710.04580.

Rider, B. A limit theorem at the edge of a non-Hermitian random matrix ensemble. Journal of Physics A: Mathematical
and General, 36(12):3401, 2003. ISSN 0305-4470. doi: 10.1088/0305-4470/36/12/331. URL http://stacks.iop.
org/0305-4470/36/i=12/a=331.

Schniter, P. and Rangan, S. Compressive Phase Retrieval via Generalized Approximate Message Passing. IEEE Transactions

on Signal Processing, 63(4):1043–1055, February 2015. ISSN 1053-587X. doi: 10.1109/TSP.2014.2386294.

Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J. Deep Information Propagation. 2017. URL https:

//openreview.net/pdf?id=H1W1UN9gg.

Sompolinsky, H., Crisanti, A., and Sommers, H. J. Chaos in Random Neural Networks. Phys. Rev. Lett., 61(3):259–262,
July 1988. doi: 10.1103/PhysRevLett.61.259. URL https://link.aps.org/doi/10.1103/PhysRevLett.
61.259.

Stern, M., Sompolinsky, H., and Abbott, L. F. Dynamics of Random Neural Networks with Bistable Units. Physical
ISSN 1539-3755. URL

review. E, Statistical, nonlinear, and soft matter physics, 90(0):062710, December 2014.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4348075/.

Tao, T. Topics in random matrix theory. Graduate studies in Mathematics, 132, 2012.

16

van der Wilk, M., Rasmussen, C. E., and Hensman, J. Convolutional Gaussian Processes. In Advances in Neural Information

Processing Systems 30, pp. 2849–2858, 2017.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \., and Polosukhin, I. Attention is All

You Need. In Advances in Neural Information Processing Systems, pp. 5998–6008, 2017.

Williams, C. K. I. Computing with Inﬁnite Networks. In Advances in neural information processing systems, pp. 7, 1997.

Wilson, A. G., Hu, Z., Salakhutdinov, R., and Xing, E. P. Deep kernel learning. In Artiﬁcial Intelligence and Statistics, pp.

370–378, 2016a.

Wilson, A. G., Hu, Z., Salakhutdinov, R. R., and Xing, E. P. Stochastic Variational Deep Kernel Learning. In Advances in

Neural Information Processing Systems, pp. 2586–2594, 2016b.

Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J. Dynamical Isometry and a Mean Field
Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks. In Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5393–
5402, Stockholmsmssan, Stockholm Sweden, July 2018. PMLR. URL http://proceedings.mlr.press/v80/
xiao18a.html.

Yang, G. and Schoenholz, S. S. Mean Field Residual Network: On the Edge of Chaos. In Advances in neural information

processing systems, 2017.

Yang, G. and Schoenholz, S. S. Deep Mean Field Theory: Layerwise Variance and Width Variation as Methods to Control

Gradient Explosion. February 2018. URL https://openreview.net/forum?id=rJGY8GbR-.

Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S. S. A Mean Field Theory of Batch Normalization.

September 2018. URL https://openreview.net/forum?id=SyMDXnCcF7.

Yang, G. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes

In Advances in neural information processing systems, 2019.

Zou, D., Cao, Y., Zhou, D., and Gu, Q. Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks.

arXiv:1811.08888 [cs, math, stat], November 2018. URL http://arxiv.org/abs/1811.08888.

17

A. Common Dimension Classes

We present an algorithm below to compute each CDC c of (π, Λ). We write c(g) for the CDC associated to the G-var g, and
we as well associate CDC c(h) to each H-var and left- and right- CDCs c1(A), c2(A) to each A-var. Here gl and gm should
be interpreted as elements of the set c and not as vectors. We induct on lines in the skeleton π. First let Λ∼ be the smallest
equivalence relation on G-vars such that gl Λ∼ gm if (nl, nm) ∈ Λ.

1. VecIn gl. Set c(gl) ← {gm : gm Λ∼ gl}.

2. MatIn Al. Set c1(Al) ← {}, c2(Al) ← {}.

3. T Al def= (Ak)(cid:62). Set c1(Al) ← c2(Ak), c2(Al) ← c1(Ak).

4. MatMul

(a) If gl := Akgj: Merge c(gj) ← c(gj) ∪ c2(Ak) → c2(Ak) and c(gl) ← c1(Ak) ∪ {gm : gm Λ∼ gl} → c1(Ak).
(b) If gl := Akhj: Merge c(hj) ← c(hj) ∪ c2(Ak) → c2(Ak) and c(gl) ← c1(Ak) ∪ {gm : gm Λ∼ gl} → c1(Ak).
jk gjk . Merge c(gji) ← {gm : gm Λ∼ gl} ∪ (cid:83)

i(cid:48) c(gji(cid:48) ) → c(gl) for all i.

j1gj1 + · · · + al

5. LinComb gl := al

6. Nonlin hl := fl(gj1 , . . . , gjk ) ∈ Rnl

= Rnj1 . Merge c(gji) ← (cid:83)

i(cid:48) c(gji(cid:48) ) → c(hl) for all i.

7. Comp similar to Nonlin.

B. Example Programs

B.1. MLP

1 : g1 := W 1x
2 : g2 := b1
3 : g3 := g1 + g2
4 : h4 := φ(g3)
5 : A5 := W 2
6 : g6 := b2
7 : g7 := A5h4
8 : g8 := g7 + g6
9 : h9 := φ(g8)

network input multiplied by weight matrix

layer 1 bias

layer 1 preactivation

layer 1 activation

layer 2 weights

layer 2 biases

layer 2 preactivations

layer 2 activations

In line 1 above, we could also spend a few more lines and equivalently set g1 := (cid:80)n0
:i. For
brevity, we adopt the current approach, but later for reasoning about backprop, this way of expression W 1x is useful. Note
that we can also express x as its own deterministic input G-var and W 1 as an input A-var, but the program given here is
more consistent with our scaling limit, which takes the hidden layer width to inﬁnity but keeps the input dimension ﬁxed.

i=1 xig−i where g−i := W 1

Backprop of fully-connected, feedforward:

10 : g10 := ∇h9 L
11 : h11 := φ(cid:48)(g8) (cid:12) g10
12 : A12 := (A5)(cid:62)
13 : g13 := A12h11
14 : h14 := φ(cid:48)(g3) (cid:12) g13

last layer gradient

layer 2 preactivation gradient

layer 1 activation gradient

layer 1 preactivation gradient

18

Here ∇h9L can be any vector but in the context of neural networks, it can be thought as the gradient of some loss function L
obtained through some readout layer.

B.1.1. DETRANSPOSITION

We demonstrate the detransposition ˇπ of the above program π. Line 1 to line 9 are almost copied verbatim to ˇπ because the
only matrix multiplications involve new A-vars.

1 : ˇg1 := W 1x
2 : ˇg2 := b1
3 : ˇg3 := ˇg1 + ˇg2
4 : ˇh4 := φ(ˇg3)
5 : ˇg5 := b2
6 : ˇA6 := W 2
7 : ˇg7 := ˇA6ˇh4
8 : ˇg8 := ˇg7 + 0
9 : ˇg9 := ˇg8 + ˇg5
10 : ˇh10 := φ(ˇg9)

Now to convert line 10 to 14 of π

11 : ˇg11 := ∇h9L
12 : ˇh12 := φ(cid:48)(ˇg9) (cid:12) ˇg11
13 : ˇA13 := iid copy of W 2(cid:62)
14 : ˇg14 := ˇA13ˇh12
15 : ˇh15 := ˇg14 + a(cid:48)ˇh4
16 : ˇh16 := φ(cid:48)(ˇg3) (cid:12) ˇh15

ϕ(g1) = ˇg1
ϕ(g2) = ˇg2
ϕ(g3) = ˇg3
ϕ(h4) = ˇh4
ϕ(g5) = ˇg5
ϕ(A6) = ˇA6
begin MatMul conversion; ϕg(g7) = ˇg7
end MatMul conversion; ϕ(g7) = ˇg8
ϕ(g8) = ˇg9
ϕ(h9) = ˇh10

ϕ(g10) = ˇg11
ϕ(h11) = ˇh12
ϕ(A12) = ˇA13
begin MatMul conversion; ϕg(g13) = ˇg14
end MatMul conversion; ϕ(g13) = ˇh15

where

a(cid:48) = α(E fϕ(h4)(Z)2)−1 E fϕg(g7)(Z)fϕ(h11)(Z)

= α(E fˇh4
= α(E φ(Z (cid:48)

(Z (cid:48))2)−1 E fˇg7
(Z)fˇh12
3)2)−1 E Z7φ(cid:48)(Z9)Z11

(Z)

with Z, Z (cid:48) ∼ N (µˇc, Kˇc) and α = lim n1(A6)/n2(A6).

B.2. Batched input

For the second input y in the batch

15 : g15 := W 1y0
16 : g16 := g15 + g2
17 : h17 := φ(g16)
18 : g18 := A5h17
19 : g19 := g18 + g6
20 : h20 := φ(g19)

2nd input multiplied by (same) weight matrix

using same bias as before

layer 1 activation

using same weight matrix

using same bias

layer 2 activations

19

Gradients:

21 : g21 := ∇h20L
22 : h22 := φ(cid:48)(g19) (cid:12) g21
23 : g23 := A12h22
24 : h24 := φ(cid:48)(g16) (cid:12) g23

last layer gradient for input y

layer 2 preactivation gradient

layer 1 activation gradient; using same weights

layer 1 preactivation gradient

B.3. Residual network

Style 1: resblock merges after weights

1 : g1 := W 1x
2 : g2 := b1
3 : g3 := g1 + g2
4 : h4 := φ(g3)
5 : A5 := W 2
6 : g6 := b2
7 : g7 := A5h4
8 : g8 := g1 + g7 + g6

network input multiplied by weight matrix

resblock 1 bias

resblock 1 preactivation

resblock 1 activation

resblock 1 merge weights

resblock 1 merge biases

return to main branch

Style 2: resblock merges before weights

1 : g1 := W 1x
2 : g2 := b1
3 : g3 := g1 + g2
4 : h4 := φ(g3) + g1
5 : A5 := W 2
6 : g6 := b2
7 : g7 := A5h4
8 : g8 := g7 + g6
9 : h9 := φ(g8) + φ(g3) + g1

network input multiplied by weight matrix

resblock 1 bias

resblock 1 preactivation

resblock 1 activation, merge back to main branch

resblock 2 preactivation
merge of 2nd resblock; semantically same as h9 := φ(g8) + h4

B.4. Simple RNN

This is almost the same as the feedforward case except we tie the weights across time, and have inputs for each time step.

20

1 : g1 := h0
2 : g2 := b
3 : A3 := W
4 : g4 := U x1 + a
5 : g5 := A3g1
6 : g6 := g5 + g2 + g4
7 : h7 := φ(g6)
8 : g8 := U x2 + a
9 : g9 := A3h7
10 : g10 := g9 + g2 + g8
11 : h11 := φ(g10)

hidden state at t = 0

RNN bias

RNN weights

afﬁne transform of input at t = 1

afﬁne transform of input at t = 2, with same U and a

hidden state at t = 1

hidden state at t = 2

More advanced RNNs like GRU or LSTM can be similarly expressed.

B.5. Batchnorm, fully-connected
Let ˜φ(h) = φ((h − ¯h)/std(h)) be batchnorm followed by coordinatewise nonlinearity φ, where h ∈ RB should be
interpreted as a single neuron across a batch, and ¯h = 1
i=1(hi − ¯h)2. For example, let
B
x1, . . . , x4 be the batch of inputs.

i=1 hi, std(h) =

(cid:80)B

(cid:80)B

(cid:113)

1
B

1 : g1 := W 1x1
2 : g2 := W 1x2
3 : g3 := W 1x3
4 : g4 := W 1x4
5 : g5 := b1
6 : g6 := g1 + g5
7 : g7 := g2 + g5
8 : g8 := g3 + g5
9 : g9 := g4 + g5
10 : h10 := ˜φ(g6, g7, g8, g9)1
11 : h11 := ˜φ(g6, g7, g8, g9)2
12 : h12 := ˜φ(g6, g7, g8, g9)3
13 : h13 := ˜φ(g6, g7, g8, g9)4

layer 1 bias

layer 1 input 1 activations

layer 1 input 2 activations

layer 1 input 3 activations

layer 1 input 4 activations

The transformer without layernorm (in particular, the softmax and self-attention mechanism) can be expressed in a similar
way.

B.6. Convolution

For any convolution weights {W l
β is a dense matrix. Suppose x is an image input to the network with
s pixels and c channels, {xαj}α∈pos,j∈[c], so that xα is vector of dimension c. Then the αth pixel, across all channels, of the
convolution W l applied to x can be described as

βij}β∈ker,i∈c(cid:48),j∈[c], W l

(cid:88)

β∈ker

W l

βxα+β

21

Deﬁne

(cid:88)

˜xα(cid:48)j(cid:48) =

W 1

βijxα+β,j

For demonstration, assume ker = {0, 1} and pos = [3] and the convolution is circular, and for simplicity assume we don’t
have biases.

0 x1
0 x2
0 x3
1 x1
1 x2
1 x3

1 : g1 := W 1
2 : g2 := W 1
3 : g3 := W 1
4 : g4 := W 1
5 : g5 := W 1
6 : g6 := W 1
7 : g7 := g1 + g5
8 : g8 := g2 + g6
9 : g9 := g3 + g4
10 : h10 := φ(g7)
11 : h11 := φ(g8)
12 : h12 := φ(g9)
13 : A13 := W 2
0
14 : A14 := W 2
1
0 h10
15 : g15 := W 2
0 h11
16 : g16 := W 2
0 h12
17 : g17 := W 2
1 h10
18 : g18 := W 2
1 h11
19 : g19 := W 2
20 : g20 := W 2
1 h12
21 : g21 := g15 + g19
22 : g22 := g16 + g20
23 : g23 := g17 + g18
24 : h24 := φ(g21)
25 : h25 := φ(g22)
26 : h26 := φ(g23)

layer 1 pixel 1 preactivations

layer 1 pixel 2 preactivations

layer 1 pixel 3 preactivations

layer 1 pixel 1 activations

layer 1 pixel 2 activations

layer 1 pixel 3 activations

layer 2 weights

layer 2 pixel 1 preactivations

layer 2 pixel 2 preactivations

layer 2 pixel 3 preactivations

layer 2 pixel 1 activations

layer 2 pixel 2 activations

layer 2 pixel 3 activations

C. Additional Notations

We will use teletype font g, h, A, etc, to denote variables or nodes as deﬁned in the straightline program. The superscripts in
this case, like gl, will mean that line number associated to such a variable. In contrast, we will use normal font g, h, A, etc,
to denote arbitrary variables of the correct type in a program (though sometimes we use h to also denote var of type H or G),
but the superscripts, such as in gi, are not attached to the semantics of the program. In either case, gl
k will denote
the scalar value at the kth position of gl or gi. We write line(g) for the line number of the G-node g so that g = gline(g)
(same for H-nodes and A-nodes). We write n(g) for the dimension of a G-node so that nline(g) = n(g) (similar for H-node).
Similarly, n1(A) and n2(A) gives the ﬁrst and second dimensions of an A-node A. Let Gπ be the collection of all G-nodes
of a skeleton π, Hπ be the collection of all H-nodes, and let Gin
π be the collection of all input G-nodes. Sometimes we need

k or gi

22

to talk about all G-nodes on or before line L. We will use G≤L
suppress the subscript π for brevity.

π

to denote such a set. When π is clear from context, we

Deﬁnition C.1. If c is a CDC, then let c≤m

def= {gl ∈ c : l ≤ m} and c<m

def= {gl ∈ c : l < m}.

Given a kernel K : Rm × Rm → R and subsets X , Y ⊆ Rm, write K(X , Y) for the corresponding |X | × |Y| submatrix of
K, and write K|X = K(X , X ) to be the restriction of K to X .

d= Y or X d=A Y means that for any integrable
Given two random variables X, Y , and a σ-algebra A, the notation X|A
function φ and for any random varible Z measurable on A, E φ(X)Z = E φ(Y )Z. We say that X is distributed as (or
is equal in distribution to) Y conditional on A. In case A is the trivial σ-algebra, we just write X d= Y . The expression
X d−→ Y means X converges to Y in distribution. If random variables X t a.s.−−→ X∞, then we write X∞ = lima.s.
Deﬁnition C.2. For a function Φ : Rn → Rn, we deﬁne

t→∞ X t.

for a PSD matrix Σ. When φ : R → R, we write Vφ to mean V applied to the function that acts coordinatewise by φ.

VΦ(Σ) def= E[Φ(z)Φ(z)(cid:62) : z ∼ N (0, Σ)]

D. Consequences

D.1. Warmup: MLP

We warm up by considering the GP correspondence, gradient dynamics, and NTK convergence of MLPs ﬁrst. We deﬁne a
fully-connected, feedforward neural network f (x; θ), x ∈ Rn0

as follows

x0(x) def= x

hl(x) def=

√

1
nl−1

W lxl−1(x) + bl

xl(x) def= φ(hl(x))

f (x; θ) def=

1
√
nL

v(cid:62)xL(x)

, v ∈ RnL

w )2) and bl

, and W l ∈ Rnl×nl−1

with bl ∈ Rnl
for l = 1, . . . , L. These form the parameters θ of f . (Note here that we follow
w)2), vi ∼
prior notation and use h for “preactivation,” but it is in fact equivalent to a G-var). We sample W l
N (0, (σL+1
nl from Glorot
initialization.” This parametrization doesn’t change the forward kernel Σl (deﬁned below), but it does change the scaling of
gradients.
Deﬁne kernels ΣL+1 : (Rn0

b)2). We can think of this parametrization as “pulling out the 1/

ij ∼ N (0, (σl
√

i ∼ N (0, (σl

)2 → R by

Σ1(x, x(cid:48)) def= (σ1

w)2 1
n0

n0
(cid:88)

xix(cid:48)

i + (σ1

b )2

Σl def= (σl
ΣL+1 def= (σL+1

i=1
w)2Vφ(Σl−1) + (σl
w )2Vφ(ΣL).

b)2

the Neural Tangent Kernel can be in general deﬁned as Kθ(x, x(cid:48)) =
For any parametrized function f (x; θ),
(cid:104)∇θf (x; θ), ∇θf (x(cid:48); θ)(cid:105). In the case when f (x; θ) is deﬁned as above, there is a scaling limit of Kθ when θ is ran-
domized (Jacot et al., 2018). The “proof” given by Jacot et al. (2018) was a sketch and most importantly was silent about
its application of the gradient independence assumption (used when applying induction). Below we give a formal proof
of NTK convergence, but ﬁrst we introduce a “gradient kernel.” Suppose we take n1, . . . , nL → ∞ in such a way that
nl/nm → αl,m for constants αl,m ∈ (0, ∞). Then deﬁne Πl : (Rn0

)2 → R by

ΠL(x, x(cid:48)) def= (σL+1

w )2

Πl def= αl+1,lVφ(cid:48)(Σl+1) (cid:12) Πl+1.

23

Theorem D.1. Fix a ﬁnite set of inputs X ⊆ Rn0
above,

. As n1, . . . , nL → ∞ with nl/nm → αl,m, with parameter sampled as

if the nonlinearity φ is α-controlled with α < 2; and

f (X ; θ) d−→ N (0, ΣL+1|X )

nL
nl

nl
(cid:88)

(cid:18)

ψ

{

i=1

∂f
∂xl
i

(cid:19)

(x)}x∈X

a.s.−−→ E[ψ(ζ) : ζ ∼ N (0, Πl|X )], for all polynomially bounded ψ, l ≥ 1

Kθ|X

a.s.−−→

L
(cid:88)

l=1

αl,LΠl (cid:12) Vφ(cid:48)(Σl) (cid:12)

Σl + (σl

w)2 − (σl
w)2
(σl

b)2

+ ΣL+1/(σL+1

w )2

L
(cid:88)

L
(cid:75)

=

l=1

m=l

Vφ(cid:48)(Σm) (cid:12)

Σl + (σl

w)2 − (σl
w)2
(σl

b)2

+ ΣL+1/(σL+1

w )2.

if the nonlinearity φ has a polynomially bounded weak derivative.

This theorem formally justiﬁes the computations made in Poole et al. (2016); Schoenholz et al. (2017); Jacot et al. (2018)
Remark D.2. If we set σl
b = 1 for all l, then we can recover the NTK recurrence relation given in Jacot et al. (2018).
The β factor in Jacot et al. (2018) on the bias can also be easily accounted for, but we will not consider such a parametrization
here.

w = σl

Proof. Since X is ﬁnite, it sufﬁces to just consider two inputs x01 = x, x02 = x(cid:48). We can form a tensor program to
model a fully-connected, feedforward network, with a batch of inputs. We construct it implicitly as follows, where we use
superscripts in •(•) to denote semantically relevant quantities.

Deﬁne the input vars ˜g(i) := W 1
i=1 ˜g(i)x0a
i
for a = 1, 2, ˆg(la) := g(la) + ¯g(l) (this represents hl), h(la) := φ(ˆg(la)) (this represents xl), g(l+1,a) := A(l+1)h(la). For
simplicity, we assume that n1(A(l)) (cid:54)= n2(A(l)) for all l, so that each “layer” belongs to a different CDC. Below, we write K
for K c where c is automatically understood based on the arguments; similarly we write µ for µc with c implied.

:i, A(l) := W l for l = 2, . . . L, and ¯g(l) := bl for l = 1, . . . , L. Deﬁne g(1a) := (cid:80)n0

We have the corresponding sampling hyperparameters σline(A(l))t = σl
K cint is such that ˜g(i)

w/n0)2) and ¯g(l)

j ∼ N (0, (σl

j ∼ N (0, (σ1
Then we can compute µ = 0 and

w, σline(¯g(l))t = σl

b, µcint = 0 for all input G-vars, and

b)2), for all j in appropriate ranges.

K(ˆg(la), ˆg(lb)) = Σl(x, x(cid:48))
K(g(la), g(lb)) = K c(ˆg(la), g(lb))
= Σl(x, x(cid:48)) − (σl

b)2

K(ˆg(la), ¯g(l)) = (σl

b)2

and K(g, g(cid:48)) = 0 for all other pairs of G-vars g, g(cid:48).

Thus, by Thm 4.3, for any (< 2)-controlled ψ,

1
nl

=

1
nl
a.s.−−→

nl
(cid:88)

i=1
nl
(cid:88)

i=1

ψ(hl(x0a))ψ(hl(x0b))

ψ(ˆg(la))ψ(ˆg(lb))

E
(z,z(cid:48))∼N (0,Σl|x,x(cid:48) )

ψ(z)ψ(z(cid:48))

24

where Σl|x,x(cid:48) =

(cid:18) Σl(x, x) Σl(x, x(cid:48))
Σl(x(cid:48), x) Σl(x(cid:48), x(cid:48))

(cid:19)

. Obviously, given hL(x) for all x, f (x; θ) is a Gaussian process with kernel

K(x, x(cid:48)) = (σL+1
in layer < L, we have that f itself is a Gaussian process with this kernel, in the limit.

i=1 φ(hL(x))φ(hL(x(cid:48))) and mean 0. Since K a.s.−−→ ΣL+1 over the randomization of parameters

w )2 1
nL

(cid:80)nL

Now we think about backprop.
We have ∂f
∂xL = 1√
nL v. We can thus extend the tensor program by g(La) := v, h(la) := g(la) (cid:12) φ(cid:48)(ˆg(la)), g(l−1,a) :=
A(l)(cid:62)h(la), for a = 1, 2 and for l = L − 1, . . . , 2. Here g(l) represents ∂f
nL. For brevity
∂xl
we just wrote A(l)(cid:62) for an implicitly deﬁned transposed var of A(l). We can compute K(g(la), g(lb)) = Πl(x, x(cid:48)), and
K(g, g(cid:48)) = 0 for all other G-var pairs g, g(cid:48) not appearing in the original tensor program.

nL and h(l) represents ∂f
∂hl

√

√

Because v ∼ N (0, (σL+1
l = 1, . . . , L,

w )2), and all g(la), h(la) are odd in v (being linear in v), Thm 5.1 can be applied. Then, for

1
nl

nl,nl−1
(cid:88)

i,j=1

∂f
∂W l
ij

(x)

∂f
∂W l
ij

(x(cid:48)) =

1
nl

(cid:18) 1
√

nl−1

∂f
∂hl
i

(x)xl−1
j

(cid:19) (cid:18) 1
√

(x)

nl−1

(x(cid:48))xl−1

j

(cid:19)

(x(cid:48))

∂f
∂hl
i



nl,nl−1
(cid:88)

i=j=1


=

=

1
nl−1nl

1
nl−1nl

nl
(cid:88)

i=1

nl
(cid:88)







i=1
aφ(cid:48)(zl

∂f
∂hl
i

(x)

∂f
∂hl
i




h(la)h(lb)







(x(cid:48))







nl−1
(cid:88)

xl−1
j

(x)xl−1
j

(x(cid:48))



j=1

h(l−1,a)h(l−1,b)





nl−1
(cid:88)

j=1

a.s.−−→ (cid:0)E zl

a)zl

bφ(cid:48)(zl

b)(cid:1) (cid:0)E φ(zl−1

a

)φ(zl−1
b

)(cid:1)

by Thm 5.1

b) ∼ N (0, K|g(la),g(lb) ), and independently (zl

= E zl

azl
b

E φ(cid:48)(zl

a)φ(cid:48)(zl
a, zl

)φ(zl−1
b

b) E φ(zl−1
b) ∼ N (0, K|ˆg(la),ˆg(lb) ). Thus the above is just [Πl (cid:12)

)

a

where (zl
a, zl
Vφ(cid:48)(Σl) (cid:12) (Σl − (σl

b)2)/(σl

w)2](x, x(cid:48)).

On the other hand,

1
nl

(cid:88)

i

∂f
∂bl
i

(x)

∂f
∂bl
i

(x(cid:48)) =

1
nl

(cid:88)

i
(cid:32)

∂f
∂hl
i

(x)

(x(cid:48))

∂f
∂hl
i
(cid:33)

=

(cid:88)

h(la)h(lb)

1
nl
i
a.s.−−→ E zl
bφ(cid:48)(zl
a)zl
aφ(cid:48)(zl
b)
= E zl
E φ(cid:48)(zl
a)φ(cid:48)(zl
azl
b)
b
= [Πl (cid:12) Vφ(cid:48)(Σl)](x, x(cid:48))

where zl
a, zl
a, zl
as deduced before.

b, zl

b are sampled as above. Also, 1
nL

(cid:80)
i

∂f
∂vi

(x) ∂f
∂vi

(x(cid:48)) = 1
nL

(cid:80)

i hL

i (x)hL

i (x(cid:48)) a.s.−−→ ΣL+1(x, x(cid:48))/(σL+1

w )2

Thus

ΘL a.s.−−→

L
(cid:88)

l=1

αl,LΠl (cid:12) Vφ(cid:48)(Σl) (cid:12)

Σl + (σl

w)2 − (σl
w)2
(σl

b)2

+ ΣL+1/(σL+1

w )2

=

=

L
(cid:88)

αl,L

L−1
(cid:89)

l=1

m=l

αm+1,m

L−1
(cid:75)

m=l

Vφ(cid:48)(Σm+1) (cid:12) Vφ(cid:48)(Σl) (cid:12)

Σl + (σl

w)2 − (σl
w)2
(σl

b)2

+ ΣL+1/(σL+1

w )2

L
(cid:88)

L
(cid:75)

l=1

m=l

Vφ(cid:48)(Σm) (cid:12)

Σl + (σl

w)2 − (σl
w)2
(σl

b)2

+ ΣL+1/(σL+1

w )2

25

Global mean pooling as readout layer. Now suppose that f (x; θ) = 1
construct a program π for computing f and its gradients over two inputs x01, x02:

nL 1(cid:62)xL(x). As in the proof above, we can

Forward Deﬁne the input vars ˜g(i) := W 1
g(1a) := (cid:80)n0
i=1 ˜g(i)x0a
i
g(l+1,a) := A(l+1)h(la).

:i, A(l) := W l for l = 2, . . . L, and ¯g(l) := bl for l = 1, . . . , L. Deﬁne
for a = 1, 2, ˆg(la) := g(la) + ¯g(l) (this represents hl), h(la) := φ(ˆg(la)) (this represents xl),

Backward We set g(La) := 1, h(la) := g(la) (cid:12) φ(cid:48)(ˆg(la)), g(l−1,a) := A(l)(cid:62)h(la), for a = 1, 2 and for l = L − 1, . . . , 2.

Here g(l) represents nL ∂f

∂xl and h(l) represents nL ∂f
∂hl .

Now we construct the detransposition ˇπ of π (see Appendix B.1.1 for a concrete example of detransposition).

The forward part of ˇπ is almost identical to that of π:

Forward Deﬁne the input vars ˇ˜g(i) := W 1
ˇ˜g(i)x0a
i

:i, ˇA(l) := W l for l = 2, . . . L, and ˇ¯g(l) := bl for l = 1, . . . , L. Deﬁne
for a = 1, 2, ˇˆg(la) := ˇg(la) + ˇ¯g(l) (this represents hl), ˇh(la) := φ(ˇˆg(la)) (this represents xl),

ˇg(1a) := (cid:80)n0
ˇg(l+1,a) := ˇA(l+1)ˇh(la). Finally, we set ϕ(•) = ˇ• for all vars deﬁned here.

i=1

Here we have automatically simpliﬁed the detransposition of g(la) produced from Defn 6.2, by identifying ϕ(gl) and ϕg(gl),
which in this case are the same.

Now the backward part

Backward Let ˇA(l)(cid:48) be an input A-var sampled iid as A(l)(cid:62). We set ˇh(La) := ˇg(La) := 1 (representing nL times
the gradient at xL), ϕ(h(la)) = ˇh(la) := ˇh(la) (cid:12) φ(cid:48)(ˇˆg(la)), ϕg(g(l−1,a)) = ˇ˜g(l−1,a) := ˇA(l)(cid:48)ˇh(la), ϕ(g(l−1,a)) =
ˇh(l−1,a) := ˇ˜g(l−1,a) + a(l−1,a)h(l−1,a) for a = 1, 2 and for l = L − 1, . . . , 2, and for a(la) computed via the derivative
rule of Thm 6.3. Speciﬁcally, we have, by a simple induction,

a(L−1,a) = αL,L−1(σL

a(la) = a(l+1,a)αl+1,l(σl+1

w)2 E[φ(cid:48)(cid:48)(y) : y ∼ N (0, ΣL

aa)]
w )2 E ∂y(φ(y)φ(cid:48)(y)) = a(l+1,a)αl+1,l(σl+1

w )2(E φ(cid:48)(y)2 + φ(y)φ(cid:48)(cid:48)(y)),

with y ∼ N (0, Σl+1
aa )

The derivatives here should be interpreted as tempered distributions in general, testing against the Gaussian density of
y. Note that if φ is odd, then φ(cid:48)(cid:48) is odd, so that aL−1,a = 0 = ala for all l < L − 1. If φ is ReLU, then φ(cid:48)(cid:48) is the Dirac
Delta tempered distribution at 0, so that aL−1,a = 1/(cid:112)2πΣL
aa.

In the forward pass, (ˆg(la), ˆg(lb))“ d= ”N (0, Σl|a,b) as before. In the backward pass, (ˇ˜g(la), ˇ˜g(lb))“ d= ”N (0, Πl|a,b), where

ΠL−1|a,b = αL,L−1(σL
Πl|a,b = αl+1,l(σl+1

w)2Vφ(cid:48)(ΣL|a,b)
w )2(Πl+1|a,b (cid:12) Vφ(cid:48)(Σl+1|a,b) + (a(l+1,a), a(l+1,b))⊗2 (cid:12) Vφ(Σl+1|a,b))

and

1
nl

nl
(cid:88)

i=1

h(la)
i

h(lb)
i

a.s.−−→ Πl

abVφ(cid:48)(Σl)ab + a(la)a(lb)V(φφ(cid:48))(Σl)ab

Thus
Corollary D.3. The NTK of the MLP above with global mean pooling readout layer converges a.s. to

NTK(xa, xb) a.s.−−→

Vφ(cid:48)(ΣL)ab

ΣL

ab + (σL
w)2 − (σL
σL
w

b )2

+

L−1
(cid:88)

l=1

αl,L

(cid:0)Πl

abVφ(cid:48)(Σl)ab + a(la)a(lb)V(φφ(cid:48))(Σl)ab

26

(cid:1) Σl

ab + (σl
w)2 − (σl
σl
w

b)2

Corollary D.4. If the readout layer is global mean pooling in an MLP and the last layer nonlinearity is odd, then the
Gradient Independence Assumption can be applied to give the correct computation of the gradient covariance and the NTK.

Now that we have warmed up a little, we will work with tensor programs and apply Thms 4.3, 5.1 and 6.3 more informally.

D.2. Warmup: Semicircle Law

Thm 6.3 has enough power to rederive the semicircle law for the Gaussian Orthogonal Ensemble (Tao, 2012).

Deﬁnition D.5. The Gaussian Orthogonal Ensemble (GOE) is the sequence of matrices (Wn)n≥0 deﬁned as follows: Let
Xij ∼ N (0, 1), iid, for all i, j ∈ N. Then set Wn

(Xij + Xji)i,j∈[n].

def= 1√
2n

Deﬁnition D.6. The empirical spectral distribution (ESD) µWn of Wn is given by µWn
the Dirac Delta measure at x, and λi are the eigenvalues in decreasing (in i) order.

def= 1
n

(cid:80)n

i=1 δλi(Wn), where δx is

√

Deﬁnition D.7. The semicircle law µsc is deﬁned to be the distribution with density ∝
Deﬁnition D.8. A random distribution µ on R, i.e. a random variable taking values in the space of probability distributions
on R, converges to a deterministic distribution µ∗ almost surely, if for all compactly supported continuous function f ,
Ez∼µ f (z) a.s.−−→ Ez∼µ∗ f (z).

4 − x2.

To prove that µWn converges almost surely to the semicircle law µsc, it sufﬁces to compute the (polynomial) moments of
µWn and show that it converges almost surely to the moments of µsc as n → ∞ (Tao, 2012). It’s well known that the odd
moments of µsc are 0 and for even k, EX∼µsc X k = Ck/2, where Cj is the jth Catalan number deﬁned by

C0 = 1,

Cj =

j−1
(cid:88)

i=0

CiCj−1−i.

Now,

E
X∼µWn

X k = E
Wn

1
n

tr (cid:0)W k

n

(cid:1) = E
Wn

E
v∼N (0,In)

1
n

v(cid:62)W k

n v.

The latter expectation can be expressed as a tensor program: We couple the time index t to t = n. Deﬁne the A-var
A1t to be an input var, with n1(A1t) = n2(A1t) = n, sampled A1t
2n), and deﬁne
A2t = A1t(cid:62). Thus A1t + A2t represents Wn. Set g0 := v, an input var, sampling g0
i ∼ N (0, 1). Inductively, set
g(cid:48)j := A1gi−1, g(cid:48)(cid:48)j := A2gi−1, and gj := g(cid:48)j + g(cid:48)(cid:48)j. Thus gj represents W j
nv (where the superscript j represents an
index in gj while it represents an exponent in W j
i as
n → ∞.

ij ∼ N (0, 1/(cid:112)2n2(A1t)) = N (0, 1/

n). We aim to compute the limit of 1
n

i=1 vi(W k

n v)i = 1
n

i=1 g0

i gk

(cid:80)n

(cid:80)n

√

This limit is prescribed by Thm 6.3. The detransposition ˇπ of the above program can be described by the following: Deﬁne
ˇA1 := ϕ(A1), ˇA2 := ϕ(A2) (so that they are independently sampled in ˇπ), and

ˇh0 := ˇg0 := v = ϕ(g0),

ˇg(cid:48)j := ˇA1ˇhj−1 = ϕg(g(cid:48)j),
ˇg(cid:48)(cid:48)j := ˇA2ˇhj−1 = ϕg(g(cid:48)(cid:48)j),
j−2
(cid:88)

ˇh(cid:48)j := ˇg(cid:48)j +

a(cid:48)j
i

ˇh(cid:48)(cid:48)i = ϕ(g(cid:48)j),

ˇh(cid:48)(cid:48)j := ˇg(cid:48)(cid:48)j +

i=0

j−2
(cid:88)

i=0

a(cid:48)(cid:48)j
i

ˇh(cid:48)i = ϕ(g(cid:48)(cid:48)j),

ˇhj := ˇh(cid:48)j + ˇh(cid:48)(cid:48)j = ϕ(gj),

where a(cid:48)j
i (resp. a(cid:48)(cid:48)j
linear function of {ˇg(cid:48)r, ˇg(cid:48)(cid:48)r}j−1

i ) is computed by differentiating fˇhj

r=1. In fact, a symmetry argument shows that fˇhj

via Thm 6.3, because it can be easily seen that fˇhj
(Z) = (cid:80)j−1

+ Z ˇg(cid:48)(cid:48)r

r=1 bj

r(Z ˇg(cid:48) r

is always a
0Z ˇg0
for

) + bj

27

some coefﬁcients {bj

r}j

r=0. An easy inductive argument shows that bj

r satisﬁes the recurrence

These equations have the unique solution

b0
0 = 1,
∀r (cid:54)∈ [0, j], bj
r = 0,

∀r < j, bj

r =

j−2
(cid:88)

i=r

rbj−1
bi
i+1 ,

bj
j = 1

bj
r =

(cid:40)

C(j−r)/2
0

if j − r is even
else.

Simultaneously, another easy inductive argument shows that µˇc = 0 and Kˇc(ˇg0, ˇgj) = 0 for all j > 0. Thus Thm 6.3 yields,
for Z ∼ N (µˇc, Kˇc),

1
n

tr(W k

n ) =

1
n

n
(cid:88)

i=1

i gk
g0
i

a.s.−−→ E Z ˇg0

r (Z ˇg(cid:48) r
bk

+ Z ˇg(cid:48)(cid:48)r

) + bk

0Z ˇg0

(cid:33)

(cid:32)k−1
(cid:88)

r=1

)2

= E bk

0(Z ˇg0
(cid:40)

Ck/2
0

if k is even
else.

as desired.

= bk

0 =

D.3. Warmup: Marchenko-Pastur Law
Suppose Yij ∼ N (0, 1) for all i, j ∈ N, and Y (n) = (Yij)i∈[mn],j∈[n] for a sequence of {mn}n satisfying limn→∞
α ∈ (0, ∞). The Marchenko-Pastur Law says that the spectral distribution of 1
µmp(α), deﬁned as

mn
n →
n Y (n)Y (n)(cid:62) converges almost surely to

(1 − α−1)+δ0 +

where (r)+ = r if r > 0 and 0 else, and a = (1 −
method.

1
α2πx
√

(cid:112)(b − x)(x − a)I[a,b](x) dx
√

α)2 and b = (1 +

α)2. We can again show this via the moment

We deﬁne a tensor program π as follows. Couple n = t. Let A := Y (n)/
g0 := v ∼ N (0, Imn) = N (0, In(g0)) be an input G-var. Deﬁne recursively

√

n be an input A-var and let A(cid:48) := A(cid:62). Let

gi := A(cid:48)gi−1

gi := Agi

We seek to compute lima.s.

n

1
mn

tr(AA(cid:62))k = lima.s.

n

1
mn

Ev∈N (0,Imn ) v(cid:62)(AA(cid:62))kv = lima.s.

t→∞

1
n(g0t)

(cid:80)n(g0t)
i=1

i gkt
g0t
i .

The detransposition ˇπ of the above, is as follows. Set ˇA, ˇA(cid:48) to be input A-vars, corresponding to ϕ(A), ϕ(A(cid:48)), and sampled
iid as such, so that σ∞( ˇA) = 1 and σ∞( ˇA(cid:48)) =

α. Then deﬁne

√

ˇh0 := ˇg0 = v ∼ N (0, Imn )
ˇ˜gi := ˇA(cid:48)ˇhi = ϕg(gi)
i−1
ˇ˜gi := ˇAˇh

= ϕg(gi)

i
ˇh

:= ˇ˜gi +

ˇhi := ˇ˜gi +

i−1
(cid:88)

j=0

i−1
(cid:88)

j=0

j
ˇh

ai
j

= ϕ(gi)

ai
j

ˇhj = ϕ(gi)

28

where ai
express

j (resp. ai

j) is computed through the derivative rule of Thm 6.3. By a simple inductive argument, we see that we can

ˇhi =

i−1
(cid:88)

j=0

j ˇgj,
bi

i
ˇh

=

i−1
(cid:88)

j=0

bi
jˇgj

for some set of coefﬁcients {bi

j, bi

j}i≥j≥0. Then it’s easy to see that they satisfy the recurrence

0 = b0
b0
0 = 1,
i−1
(cid:88)

bi−1
k

∀j < i, bi

j =

k=j

∀j < i, bi

j = α

i
(cid:88)

k=j+1

bk
j ,

bi
i = 1,

kbk−1
bi
j

,

bi
i = 1.

We claim that the solution to these equations is given by

bi
j = Mi−j,
∀j < i, bi
j = αMi−j,

bi
i = 1,

where Mr = E[xr : x ∼ µmp(α)]. It sufﬁces to verify the following Catalan-like identity

Ms = α

s−2
(cid:88)

r=1

MrMs−1−r + (1 + α)Ms−1.

(5)

This can be done by the change of variable in the integral of E[xr : x ∼ µmp(α)] to get

E[xr : x ∼ µmp(α)] = E[(

√

αy + 1 + α)r−1 : y ∼ µsc]

(cid:98)(r−1)/2(cid:99)
(cid:88)

=

k=0

αk(1 + α)r−1−2k

(cid:19)

(cid:18)r − 1
2k

Ck

where Ck is the kth Catalan number. Then one can verify Eq. (5) by expanding and applying the Catalan identity
Ck = (cid:80)k−1
Finally, an easy inductive argument shows that µˇc = 0 and Kˇc(ˇg0, ˇgj) = 0 for all j > 0. Thus, we have

i=0 CiCk−1−i repeatedly.

a.s.
lim
n

1
mn

tr(AA(cid:62))k =

a.s.
lim
t

1
n(g0t)

n(g0t)
(cid:88)

i=1

i gkt
g0t

i =

a.s.
lim
t

E 1

n(ˇg0t)

n(ˇg0t)
(cid:88)

i=1

ˇg0t
i

ˇhkt
i =

E
Z∼N (µˇc,Kˇc)

0(Z ˇg0
bk

)2 = bk

0 = Mk

as desired.

D.4. DNN-GP correspondence

Suppose ρ = F (z; θ) is the part of a neural network that takes an input embedding z = (z1, . . . , zB) and produces
a representation ρ = (ρ1, . . . , ρm) of it. For example, z can be Ax for an input x and an embedding matrix A, or
i=1 (say when x1, . . . , xB is a sequence of tokens to be processed by an
(Ax1, . . . , AxB) for a sequence/batch of inputs (xi)B
RNN, or when they form a batch, perhaps to be processed by batchnorm), or (A1x1, . . . , ABxB) when they form the pixel
vectors across the channels of an input image in the case of CNN, perhaps in combination with RNNs/batchnorm. Similarly,
ρ can be a vector representation in a MLP or a sequence/batch of vector representations in the case of RNN/batchnorm/CNN.
The neural network then converts ρ to an output via some linear transformation, say ρ (cid:55)→ (v1(cid:62)ρ1, . . . , vm(cid:62)ρm), where each
vi is a vector of appropriate size, and vi is allowed to equal to vj whenever they have the same shape. Note that this scenario
is general enough to cover simultaneous computation of a neural network on a batch of input, where ρ can be partitioned
into the corresponding representations of each parallel output.

29

Suppose F (z; θ) can be represented by a tensor program π where z and θ appear as input G- and A-vars; let the output
(ρ1, . . . , ρm) be represented by H- or G-vars h1, . . . , hm of π. When the input embedding is linear, z = (A1x1, . . . , ABxB),
and its matrices A1, . . . , AB are sampled from zero mean Gaussian distributions, (z1, . . . , zB) is jointly Gaussian with a
covariance depending on pairwise products between x1, . . . , xB. Furthermore, if θ is randomized by Gaussians according to
Section 3.2 (with some set of compatible sampling hyperparameters σl, µcin, etc), then by Thm 6.3 we get
Corollary D.9 (DNN-GP correspondence). If all fl of π are polynomially bounded and almost sure rank convergence
n(hi) hi(cid:62)hj a.s.−−→ Cij for some PSD matrix C, whenever n(hi) = n(hj), as the dimensions {nl}l of
holds for ˇπ, then
π go to inﬁnity. The kernel C can be computed via Thm 6.3. A fortiori, if each vi of the readout layer is sampled
from N (0, 1/n(vi)), where for each i (cid:54)= j, either vi = vj or vi is independent from vj, then the neural network output
(v1(cid:62)ρ1, . . . , vm(cid:62)ρm) d−→ N (0, C (cid:48)) in this limit, for

1

C (cid:48)

ij =

(cid:40)

Cij
0

if vi = vj
otherwise.

For example,

1. if z = (Ax1, . . . , AxB) is just a batch of inputs where each Axi is processed by the same neural network f in parallel,
and the network outputs (v(cid:62)ρ1, . . . , v(cid:62)ρm) for readout weights v, then Cor D.9 says f converges to a Gaussian Process
in distribution in the inﬁnite width limit.

2. if z = (Axij)BS

i=j=1 represents the embedding of a batch of B sequences of length S, and the network is an RNN that
processes each sequence in parallel, in a seq2seq fashion, then Cor D.9 says f converges to a (multivariate) Gaussian
Process in distribution in the inﬁnite width limit.

3. we obtain similar GP convergence results for any standard architecture.

D.5. Gradient Independence Assumption

Thm D.1 already shows that gradient independence assumption leads to the correct computation for MLPs.

In general, if, as before, F (z; θ) is the body of the network that takes an input embedding to a representation, and it can
be represented by a tensor program π having no line of type T, then backprop can be represented by an extended program
as in Section 5 with vi being readout layer weights. Thus, if vi are sampled with zero mean and all nonlinearities have
polynomially bounded weak derivatives, then Thm 5.1 applies and we can compute the gradient dynamics by computing K c
and µc according to Section 5, which allows us to pretend that the G-vars in π are independent from the weights used in the
backward pass. This is in particular true if F (z; θ) has a standard architecture without batchnorm (with no transposed weight
sharing in the forward pass). Batchnorm is not covered by our theorems because its gradient has singularities, for example
at the origin. However, based on the simulations of Yang et al. (2018), Thm 5.1 seems to hold even when batchnorm is
involved.

Singular value distribution. Let F (z; θ) be as above. Denote by J its Jacobian in z. Pennington et al. (2017) applied
free probability theory to compute the eigenvalues of JJ (cid:62) and hence of the singular value of J, when F (z; θ) represents a
MLP. Thus J can be expressed as DLW L · · · D2W 2D1W 1 for weight matrices W l for each layer l and diagonal matrices
Dl = Diag({φ(cid:48)(hl+1)}). Speciﬁcally, the authors compute the Stieljes transform of JJ (cid:62) and then its S-transform by
leveraging the latter’s compatibility with matrix multiplication. Crucial in this computation is the assumption that Dl and
W l are asymptotically free, allowing the application of S-transform. We now justify this assumption.

The Stieljes and S-transform methods can be thought of a more nicely packaged way of applying the moment method (Tao,
2012), i.e. computing tr((JJ (cid:62))k) for each k. Thus it sufﬁces to show that, in the computation of tr((JJ (cid:62))k), {Dl}l can
be thought of as independent of {W l}l.
Now tr((JJ (cid:62))k) = Ea∼N (0,I) a(cid:62)(JJ (cid:62))ka. The computation (JJ (cid:62))ka can be expressed with a tensor program: If π
represents the computation of F (forward pass), π(cid:48) represents J (cid:62), i.e. backprop from gradient vector a (so that ˜π = π|π(cid:48)
is an extended program of the form described in Section 5), and π(cid:48)(cid:48) represents J, then (JJ (cid:62))ka is given by the output of
ˆπ = π|(π(cid:48)(cid:107)π(cid:48)(cid:48)(cid:107) · · · (cid:107)π(cid:48)(cid:107)π(cid:48)(cid:48)). Here, | denotes concatenation and (cid:107) denotes “piping”, so that the output of ρ is inserted as the

30

input of τ in ρ(cid:107)τ . Then lim Ea∼N (0,I) a(cid:62)(JJ (cid:62))ka can be computed via Thm 6.3. Finally, it only remains to notice that
K(g, h(cid:48)) = 0 for any G-var of π and H-var (of G-var) of (π(cid:48)(cid:107)π(cid:48)(cid:48)(cid:107) · · · (cid:107)π(cid:48)(cid:107)π(cid:48)(cid:48)) other than a because h(cid:48) is always odd in a
(apply the same reasoning from proof of Thm 5.1). Thus lima.s. Ea∼N (0,I) a(cid:62)(JJ (cid:62))ka has the same limit as if the A-vars
of π are independent from the rest of ˆπ.

In fact, this reasoning, applied to mixed moments, establishes that {Di}i ∪ {W j}j are almost surely asymptotically free.
Corollary D.10. In the MLP above, let its hidden layer widths {nl}l go to inﬁnity such that nl/nl(cid:48)
→ αl,l(cid:48) ∈ (0, ∞) for
some constants αl,l(cid:48). Then, for X1, . . . , Xk chosen from {Dl, W l, W l(cid:62)}l such that the sizes match and X1 · · · Xk is a
square matrix,

1
nL tr (X1 · · · Xk) −

1

nL tr (ϕ(X1) · · · ϕ(Xk)) a.s.−−→ 0,

where ϕ(W l) = W l and ϕ(Dl) = an iid copy of Dl, independent from all other values of ϕ.

This corollary is sufﬁcient to justify the Stieljes transformation calculations of (Pennington et al., 2017), and show that the
singular value distributions converge to their limits, almost surely.

More generally, even with weight tying and arbitrary architecture, we can compute the singular value distribution of the
neural network Jacobian, by expressing the moment computations as tensor programs, just like the above, and crank the
machinery of Thm 6.3. Appendix D.3 can be thought of the most basic such case of linear regression.

D.6. Signal Propagation

We begin by examining the simple RNN and the weight-tied autoencoder, before reviewing some mean ﬁeld equations
that appeared in prior literature, which can be justiﬁed rigorously. Finally, we close by looking at the weight-tied residual
network, which is perhaps the simpliest “RNN” where the weight-tying leads to a different behavior than not tying the
weights (in contrast to the simple RNN vs MLP).

Simple RNN A simple RNN that takes in input only at time 1 and outputs only at time L can be thought of as an MLP
with parameters tied across layers:

x0(x) def= x

hl(x) def=

1
√
n

W xl−1(x) + b

xl(x) def= φ(hl(x))

RNN(x; θ) def=

1
√
n

v(cid:62)xL(x)

for W ∈ Rn×n and x, v, b ∈ Rn. We will sample Wij ∼ N (0, σ2
b ). The computation of
{RNN(xi; θ)}B
i=1 over a batch of inputs can be expressed by a tensor program with no line of type T (essentially the program
in Appendix B.1 but with weights and biases tied). By Thm 4.3, we can compute K c(hl(xi), hl(cid:48)
)) = 0 whenever l (cid:54)= l(cid:48),
b . This is, of course, exactly the same as the K c computed if W and
and that K c|{hl(xi)}i = σ2
b are not tied across layers (see Appendix D). This therefore mathematically proves what the experiments of Chen et al.
(2018) suggested.

W Vφ(K c|{hl−1(xi)}i) + σ2

W /n), bi ∼ N (0, σ2

(xi(cid:48)

Corollary D.11. Suppose φ is α-controlled for α < 2. Assume almost sure rank convergence. Then for any α-controlled
ψ : RL → R, as n → ∞,

1
n

n
(cid:88)

i=1

ψ(h1

RNN(xj)i, . . . , hL

RNN(xj)i) −

1
n

n
(cid:88)

i=1

ψ(h1

MLP(xj)i, . . . , hL

MLP(xj)i) a.s.−−→ 0,

where hl
sampled.

RNN (hl

MLP) denotes the hidden state of the RNN (MLP), with the weights and biases in each model identically

31

Autoencoder A weight-tied autoencoder is described by the following equations (we follow Li & Nguyen (2018) here)

x0(x) def= x
xl(x) def= W lσl−1(xl−1) + bl, ∀l ∈ {1, . . . , L}

ˆxL def= W L(cid:62)φL(xL) + vL
ˆxl(x) def= W l(cid:62)φl(ˆxl+1) + vl

, a set of weights W l ∈ Rnl×nl−1

for input x ∈ Rn0
, for l = 1, . . . , L.
We also have the decoder and encoder activation functions φl : R → R, σl : R → R. The parameters are sampled iid
according to

, decoder biases vl ∈ Rnl−1

, encoder biases bl ∈ Rnl

W l

ij ∼ N (0, σ2

W l /nl−1),

i ∼ N (0, σ2
bl

bl ),

i ∼ N (0, σ2
vl

vl ).

We consider taking the limit where nl → ∞, ∀l, with nl/nl−1 → αl ∈ (0, ∞).

Li & Nguyen (2018) proved a (forward) signal propagation theorem of the above weight-tied autoencoder that uses the
following quantities. Deﬁne {τl}L

l=0 inductively:

l=1 and {¯τl}L

¯τ 2
0 =
1 = σ2
τ 2

1
n0 (cid:107)σ0(x)(cid:107)2,
W 1 ¯τ 2
0 ,

l = τ 2
¯τ 2
l = σ2
τ 2

l + σ2
bl ,
W l E
σl−1(¯τl−1z)2,

z

∀l ∈ {1, . . . , L},

∀l ∈ {2, . . . , L}

where z ∼ N (0, 1). Next deﬁne {γl, ρl}L+1

l=2 inductively:

γL+1 =

γl =

E
z1

1
¯τ 2
L
1
¯τ 2
l−1

¯τLz1φL(¯τLz1),

ρL+1 = E
z1

φL(¯τLz1)2,

E
z1,z2

¯τl−1z1φl−1

(cid:18)

αlσ2

W l γl+1σl−1(¯τl−1z1) +

(cid:113)

αlσ2

W l ρl+1 + σ2

vl z2

(cid:19)

,

ρl = E
z1,z2

φl−1

where z1, z2 ∼ N (0, 1).

(cid:18)

αlσ2

W l γl+1σl−1(¯τl−1z1) +

(cid:113)

αlσ2

W l ρl+1 + σ2

vl z2

(cid:19)2

,

∀l ∈ {L − 2, . . . , 2}

By expressing the autoencoder computation on a single input x as a tensor program and applying Thm 6.3, we obtain a
version of the main theorem of Li & Nguyen (2018) that assumes no smoothness of the nonlinearities and of test functions. If
X t, Y t ∈ Rn(t) are two sequences of random vectors in t, then write X t ∼= Y t to mean that for any polynomially bounded
ψ : R → R,

i ) converge a.s. to the same limit, as n(t) → ∞.

i=1 ψ(X t

i=1 ψ(Y t

i ) and 1
n(t)

(cid:80)n(t)

(cid:80)n(t)

1
n(t)

Corollary D.12. Let the activation functions {σl, φl}l be polynomially bounded. Then in the limit {nl}l → ∞ as described
above,

1. xl ∼= N (0, ¯τlInl ), ∀l ∈ {1, . . . , L}.
(cid:113)

2. ˆxl ∼= αlσ2

W l γl+1σl−1(¯τl−1(cid:126)z1) +

αlσ2

W l ρl+1 + σ2

vl z2, ∀l ∈ {2, . . . , L} where (cid:126)z1, (cid:126)z2 ∼ N (0, Inl−1) independently.

3. the autoencoder output ˆx satisﬁes

ˆx ∼= φ0(α1σ2

W 1 γ2σ0(x) +

where (cid:126)z2 ∼ N (0, In0) independent of x.

(cid:113)

α1σ2

W 1 ρ2 + σ2

v1(cid:126)z2),

Li & Nguyen (2018)’s main theorem is almost the same as this, except that

32

1. Li & Nguyen (2018) requires σl to be nontrivial in the sense that for any τ > 0, Ez∼N (0,1) σl(τ z)2 > 0. But this
i aihi(x) is its Hermite expansion in orthonormal

is equivalent to saying that σl is not a.e. 0. Indeed, if σl(x) = (cid:80)
Hermite basis hi, then Ez∼N (0,1) σl(τ z)2 = (cid:80)

i τ 2i, which can be 0 for positive τ iff all ais vanish.

i a2

2. All nonlinearities σl, φl are required by Li & Nguyen (2018) to be globally Lipschitz (and hence linearly bounded).

Here we only need them to be polynomially bounded.

3. The equivalence relation ∼= is deﬁned differently in Li & Nguyen (2018).

p
There, X t ∼= Y t if φt(X t) − E φt(Y t)
−→ 0 for any sequence of uniformly pseudo-Lipschitz functions. A sequence of
functions φt : Rn(t) → R is said to be uniformly pseudo-Lipschitz if there exists a constant C, independent of n, such
that for any x, y ∈ Rn(t),

|φn(x) − φn(y)| ≤ C

1 +

(cid:18)

(cid:107)x(cid:107)
√
n

+

(cid:107)y(cid:107)
√
n

(cid:19) (cid:107)x − y(cid:107)
√

.

n

In contrast, the test functions φt we allow are coordinatewise functions — a stronger assumption than the above — but
does not need to be smooth, just polynomially bounded — a weaker assumption than the above. We also guarantee
almost sure convergence, a stronger result than their convergence in probability. It would be interesting in future work
to study whether one can remove the smoothness assumption even for noncoordinatewise test functions.

D.6.1. JUSTIFYING SEMIRIGOROUS EQUATIONS

Below, we give several examples of signal propagation equations derived heuristically in prior works, which can now be
justiﬁed rigorously using the tensor program framework.

MLP (Schoenholz et al., 2017) See Appendix D.1.

Residual Network (Yang & Schoenholz, 2017) We deﬁne a residual network f (x; θ), x ∈ Rn0

as follows

x0(x) def= x

W lxl−1(x) + bl

hl(x) def=

xl(x) def=

f (x; θ) def=

√

1
nl−1
1
√
nl
1
√
nL

w(cid:62)xL(x)

V lφ(hl(x)) + xl−1(x) + al

, w ∈ RnL

with al, bl ∈ Rnl
sample W l
kernels ΣL+1 : (Rn0

ij ∼ N (0, (σl

w)2), V l
)2 → R by

, V l ∈ Rnl×nl

ij ∼ N (0, (σl

, and W l ∈ Rnl×nl−1
V )2), wi ∼ N (0, (σL+1

for l = 1, . . . , L. These form the parameters θ of f . We
b)2). Deﬁne
a)2), and bl
w )2), al

i ∼ N (0, (σl

i ∼ N (0, (σl

˜Σ0(x, x(cid:48)) def=

1
n0

n0
(cid:88)

i=1

xix(cid:48)
i

w)2 ˜Σl−1 + (σl

Σl def= (σl
˜Σl def= ˜Σl−1 + (σl

b)2
V )2Vφ(Σl) + σ2
a

Then for any ﬁnite subset X ⊆ Rn0

, for α-controlled φ,

ΣL+1 def= (σL+1

w )2ΣL.

f (X ; θ) d−→ N (0, ΣL+1|X ).

33

Convolutional Network (Xiao et al., 2018) Consider a convolutional network

αi(x) def= xαi
x0
1
αi(x) def=
hl
√
nl

(cid:88)

j∈[nl−1]
β∈[sl−1]

αi(x) def= φ(hl
xl

αi(x))

W l

βijxl−1

α+β,j(x) + bl

i

αi denotes the preactivation at the lth layer, the ith channel, each with sl neurons, and the αth neuron, and likewise

where hl
for xl

αi. nl is the number of channels in layer l.

W )2vl
Suppose we have a non-negative vector (vl
(cid:80)nl
Thm 4.3, {hl
i=1 hl
Then with (cid:63) denoting 2D circular cross correlation, Xiao et al. (2018) calculated, semirigorously,

α•(xa)}α,a are “jointly Gaussian” in the limit. Deﬁne Σl

βij ∼ N (0, (σl
def= lima.s. 1
nl

β)β∈ker that sums up to 1 and W l

αa,βb

β), bl
αi(xa)hl

βi ∼ N (0, (σl

b)2). By
βi(xb), for any i.

Σl+1 = (σl
αa,βb = (σl

W )2Diag(vl) (cid:63) Vφ(Σl) + (σl
W )2 (cid:88)

γVφ(Σl)α+γ;a,β+γ;b + (σl
vl

b)2

Σl+1

b)2.

These equations can now be recovered rigorously using Thm 4.3.

Now suppose the last layer (layer L) is linear with output,

γ∈[sl]

f (x; θ) def=

√

1
nL−1sL−1

(cid:88)

α∈[sL−1]
i∈[nL−1]

W L

αixL−1

αi ∈ R

and the weights are sampled according to W L

αi ∈ N (0, 1). Then, we can compute via Thm 4.3,

(f (xa; θ), f (xb; θ)) d−→ N

(cid:18)

0,

1
sL−1

(cid:18)tr Vφ(ΣL−1)•a,•a
tr Vφ(ΣL−1)•a,•b

tr Vφ(ΣL−1)•a,•b
tr Vφ(ΣL−1)•b,•b

(cid:19)(cid:19)

.

Deﬁne, via Thm 5.1, Πl
semirigorously,

αa,βb

def= lima.s. (cid:80)nl

i=1

∂f
∂xl

αi

(xa) ∂f
∂xl

βi

(xb), for any i. Then Xiao et al. (2018) essentially calculated,

ΠL−1

αa,βb =

1
sL−1

I(α = β).

and in all previous layers, the recurrence

where vl# is the reverse of vl. These equations can now be justiﬁed rigorously using Thm 5.1.

Πl−1 = (σl

w)2Diag(vl#) (cid:63) (Vφ(cid:48)(Σl) (cid:12) Πl)

Batchnorm (Yang et al., 2018) Given φ : R → R, let Bφ : RB → RB, z (cid:55)→ φ
. This is an application of
batchnorm followed by coordinatewise action by φ, where z should be thought of as a ﬁxed unit across a batch of size B.
If (cid:126)x = (xi, . . . , xB) is a batch of inputs xi ∈ Rn0, then deﬁne a deep batchnorm network f ((cid:126)x; θ) : RB×n0 → RB×1 by

z−¯z
(cid:107)z−¯z(cid:107)/

√

B

(cid:16)

(cid:17)

(cid:126)x0((cid:126)x) def= (cid:126)x

(cid:126)hl((cid:126)x) def= (

√

1
nl−1

W lxl−1((cid:126)x)i + bl)B

i=1

(cid:126)xl((cid:126)x) def= Bφ((cid:126)hl((cid:126)x))

f ((cid:126)x; θ) def=

(cid:18) 1
√

nL

w(cid:62)xL((cid:126)x)i

(cid:19)B

.

i=1

34

Here B and n0 will be ﬁxed and nl → ∞ for l > 0. We sample W l
b)2). Deﬁne multivariate kernels Σl : (RB×n0 )2 → RB×B by
i ∼ N (0, (σl
bl

ij ∼ N (0, (σl

w)2), wi ∼ N (0, (σL+1

w )2) and

Σ1((cid:126)x, (cid:126)x(cid:48))ij

Σl|{(cid:126)x,(cid:126)x(cid:48)}
ΣL+1|{(cid:126)x,(cid:126)x(cid:48)}

def= (σ1

(cid:62)x(cid:48)

b )2

n0 xi

i + (σ1

w)2 1
w)2VBφ(Σl−1|{(cid:126)x,(cid:126)x(cid:48)}) + (σl
w )2VBφ(ΣL|{(cid:126)x,(cid:126)x(cid:48)}).

def= (σl
def= (σL+1

b)2

Then for any ﬁnite set of batches X ⊆ RB×n0

, for α-controlled φ,

f (X ; θ) d−→ N (0, ΣL+1|X ).

Yang et al. (2018) also calculated the gradient dynamics of such a deep batchnorm network, but our theorems cannot
rigorously justify them due to the singularity of the Jacobian of batchnorm.

D.6.2. A TASTE OF WEIGHT-TYING

Weight-tied Residual Network The simpliest “recurrent neural network” for understanding when weight-tying can have
a different behavior than not is perhaps in a residual network with weights tied across layers.
In this section, ﬁx a matrix W ∈ RN ×N and a function φ : R → R. Consider the dynamics

ht = W φ(ht−1) + ht−1, ht ∈ RN .

What is the “average behavior” of this dynamics as N → ∞, if we were to sample Wij ∼ N (0, σ2
here when φ is α-controlled, and it tells us that “(ht
N → ∞,” as far as α-controlled test functions are concerned.

w/N )? Thm 4.3 applies
i)i are i.i.d. samples of a zero-mean Gaussian distribution, in the limit

By Thm 4.3, we can make the following
Deﬁnition D.13. Deﬁne K(l, m) def= lima.s. 1
N
Theorem D.14. K and C satisfy the following equations in the limit N → ∞.

i and C(l, m) def= lima.s. 1

i=1 hl

ihm

(cid:80)N

N

(cid:80)N

i=1 hl
i

(cid:80)

j Wijφ(hm

j ).

K(l, m) = C(l, m − 1) + K(l, m − 1)

= C(m, l − 1) + K(m, l − 1)
= σ2

wVφ(K l−1,m−1)12 + K(l − 1, m − 1) + C(l − 1, m − 1) + C(m − 1, l − 1)

where K a,b is the matrix

C(l, m) = C(l − 1, m) + σ2
(cid:18)K(a, a) K(a, b)
K(a, b) K(b, b)

.

wVφ(K l−1,m)12
(cid:19)

In addition, for all m, l ≥ 0, K(l, m) = K(m, l), K(0, m) = K(m, 0) = K(0, 0), C(0, m) = 0.

Proof. The identities at the end are obvious. We will focus on proving Eqs. (6) to (9). We have

(6)

(7)

(8)

(9)

K(l, m) =

a.s.
lim

a.s.
lim

=

a.s.
lim

=

1
N

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

ihm
hl
i



(cid:88)



hl
i

j



Wijφ(hm−1

j

) + hm−1
i



(cid:88)

hl
i

Wijφ(hm−1

j

) +

i=1

j

a.s.
lim

1
N

N
(cid:88)

i=1

ihm−1
hl
i

= C(l, m − 1) + K(l, m − 1)

35

which gives Eq. (6) and also Eq. (7) by symmetry.

Now

C(l, m) =

a.s.
lim

a.s.
lim

=

a.s.
lim

=

a.s.
lim

=

1
N

1
N

1
N

1
N

i=1

j,k

N
(cid:88)

(cid:88)

i=1

j,k

N
(cid:88)

(cid:88)

i=1

j

N
(cid:88)

i=1

N
(cid:88)

(cid:88)

hl
i

Wijφ(hm
j )

j

(cid:88)

Wikφ(hl−1

k

)Wijφ(hm

j ) +

a.s.
lim

1
N

N
(cid:88)

i=1

(cid:88)

hl−1
i

Wijφ(hm
j )

j

Wikφ(hl−1

k

)Wijφ(hm

j ) + C(l − 1, m)

W 2

ijφ(hl−1
j

)φ(hm

j ) + C(l − 1, m)

= σ2

wVφ(K l−1,m)12 + C(l − 1, m)

yielding Eq. (9).

Finally, Eq. (8) is given by expanding C(l, m − 1) by Eq. (6) and expanding K(l, m − 1) by Eq. (7).

One can see immediately that the growth of hl norm is much faster here than for untied-weights residual network.
We now study the simultaneous evolution of two vectors hl and (cid:126)l.

D.15. Deﬁne

Kh(cid:126)(l, m)

(cid:80)N

i=1 hl
i

(cid:80)

j Wijφ((cid:126)m

j ), C(cid:126)h(l, m) def= lima.s. 1

N

def= lima.s. 1
N
(cid:80)N

(cid:80)N

(cid:126)m
i

i=1 hl
i
(cid:80)
(cid:126)l
j Wijφ(hm
i

def= K(cid:126)h(m, l)
j ).

i=1

Deﬁnition
lima.s. 1
N

Theorem D.16.

and

Ch(cid:126)(l, m)

def=

Kh(cid:126)(l, m) = Ch(cid:126)(l, m − 1) + Kh(cid:126)(l, m − 1)
= C(cid:126)h(m, l − 1) + K(cid:126)h(m, l − 1)
= σ2

wVφ(K l−1,m−1
h(cid:126)

Ch(cid:126)(l, m) = Ch(cid:126)(l − 1, m) + σ2

)12 + Kh(cid:126)(l − 1, m − 1) + Ch(cid:126)(l − 1, m − 1) + C(cid:126)h(m − 1, l − 1)
wVφ(K l−1,m

)12

h(cid:126)

(10)

(11)

(12)

(13)

where K l−1,m

h(cid:126)

is the matrix

(cid:18)Khh(l − 1, l − 1) Kh(cid:126)(l − 1, m)
(cid:19)
K(cid:126)(cid:126)(m, m)

Kh(cid:126)(l − 1, m)

.

Proof.

Kh(cid:126)(l, m) =

a.s.
lim

a.s.
lim

=

a.s.
lim

=

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

1
N

1
N

1
N

hl
i

(cid:126)m
i



(cid:88)



hl
i

j



Wijφ((cid:126)m−1

j

) + (cid:126)m−1
i



(cid:88)

hl
i

Wijφ((cid:126)m−1

j

) +

a.s.
lim

1
N

N
(cid:88)

i=1

hl
i

(cid:126)m−1
i

i=1

j
= Ch(cid:126)(l, m − 1) + Kh(cid:126)(l, m − 1)

36

Ch(cid:126)(l, m) =

a.s.
lim

a.s.
lim

=

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

(cid:88)

hl
i

Wijφ((cid:126)m
j )

j

(cid:88)

Wikφ(hl−1

k

)Wijφ((cid:126)m

j ) +

i=1

j,k

a.s.
lim

1
N

N
(cid:88)

i=1

(cid:88)

hl−1
i

Wijφ((cid:126)m
j )

j

= σ2

wVφ(K l−1,m

h(cid:126)

)12 + Ch(cid:126)(l − 1, m)

Now for the backward pass. Deﬁne ht := ∇htE, gt := W T ht for a loss function E. Then

ht−1 = ht + (W T ht) ◦ φ(cid:48)(ht−1)

So

a.s.
lim

a.s.
lim

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

i ht
hs

i =

a.s.
lim

igs
gt

i =

a.s.
lim

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

hs+1
i

i + gs+1
ht

i φ(cid:48)(hs

i )ht
i

(cid:88)

Wjiht

jWj(cid:48)iht
j(cid:48)

i=1

j,j(cid:48)

= σ2
w

a.s.
lim

1
N

N
(cid:88)

i=1

kht
ht
k

a.s.
lim

1
N

N
(cid:88)

i=1

igs
ht

i φ(cid:48)(hr

i ) =

a.s.
lim

a.s.
lim

=

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

ht+1
i

i φ(cid:48)(hr
gs

i ) + gt+1

i

i φ(cid:48)(ht
gs

i)φ(cid:48)(hr
i )

ht+1
i

i φ(cid:48)(hr
gs

i ) + σ2
w

(cid:32)

a.s.
lim

1
N

N
(cid:88)

i=1

(cid:33) (cid:32)

a.s.
lim

ht+1
i

hs
i

1
N

N
(cid:88)

i=1

(cid:33)

φ(cid:48)(ht

i)φ(cid:48)(hr
i )

Suppose we backprop a zero mean Gaussian vector with normalized norm 1. For a weight-tied residual network that runs S
steps, we have boundary conditions

a.s.
lim

1
N

N
(cid:88)

i=1

i hS
hS

i = 1

a.s.
lim

1
N

N
(cid:88)

i=1

hS+1
i

ht
i = 0, ∀t

a.s.
lim

1
N

N
(cid:88)

i=1

i gs
hS

i φ(cid:48)(hr

i ) = 0, ∀s, r

These equations then yield the dynamics of gradients in a weight-tied residual network.

D.7. Neural Tangent Kernel

Again, let F (z; θ) be the body of a neural network as above, and suppose it’s represented by a tensor program π. For every
input A-var A of π, ∂F
∂g , where the sums are over vars satisfying the subscripts in
the sums. If the network has scalar output is given by f (x) = v(cid:62)F (E(x); θ) where E is an embedding function, then the

∂g ⊗ h + (cid:80)

g:=A(cid:62)h h ⊗ ∂F

∂A = (cid:80)

g:=Ah

∂F

37

contribution of A to the NTK of f is

NTKA(x, y) def=

(cid:88)

(cid:88)

(v(cid:62) ∂F
∂g

(E(x)))i(v(cid:62) ∂F
∂g

(E(y)))i

(cid:88)

j

hj(E(x))hj(E(y))

g:=Ah

i

(cid:88)

(cid:88)

+

g:=A(cid:62)h

i

hi(E(x))hi(E(y))

(v(cid:62) ∂F
∂g

(E(x)))j(v(cid:62) ∂F
∂g

(cid:88)

j

(E(y)))j.

(14)

Each of the four subsums in Eq. (14) can be computed via Thm 6.3 (or Thm 5.1 if π doesn’t use T lines) when we expand
the computation of f over x and y as well as its gradients into a single tensor program. Note that Eq. (14) scales like (nl)2;
dividing by this factor roughly corresponds to using the parametrization of Jacot et al. (2018). The contribution to NTK
from input G-vars is similar and even simpler to compute.

The above computation would hold as long as we can apply Thm 6.3, which requires that we have almost sure rank
convergence of the relevant programs and that the nonlinearities of f have polynomially bounded weak derivatives.

We give an example by computing the NTK of a CNN (which has not appeared in prior literature).

CNN Assume the notation of the CNN section of Appendix D.6.1.

The contribution to the NTK of weights W l

βij for l < L is

(cid:88)

nl,nl−1
(cid:88)

a.s.
lim

β∈ker

i,j=1

∂f
∂W l

βij

(xa)

∂f
∂W l

βij

(xb)

a.s.
lim

=

1
nl−1

(cid:88)

nl,nl−1
(cid:88)

(cid:88)

(

β∈ker

i,j=1

α

∂f
∂hl
αi

(xa)xl−1

α+β,j(xa))(

(cid:88)

α(cid:48)

∂f
∂hl

α(cid:48)i

(xb)xl−1

α(cid:48)+β,j(xb))

=

1
nlnl−1

(cid:88)

nl,nl−1
(cid:88)

2k+1
(cid:88)

β∈ker

i,j=1

α,α(cid:48)=1

Πl

αa,α(cid:48)bVφ(Σl)αa,α(cid:48)bVφ(cid:48)(Σl−1)α+β;a,α(cid:48)+β;b

2k+1
(cid:88)

=

α,α(cid:48)=1

Πl

αa,α(cid:48)bVφ(Σl)αa,α(cid:48)b

(cid:88)

β∈ker

Vφ(cid:48)(Σl−1)α+β;a,α(cid:48)+β;b

= (cid:104)Πl

•a,•b (cid:12) Vφ(Σl

•a,•b), I (cid:63) Vφ(cid:48)(Σl−1

•a,•b)(cid:105)

Note that if we sample W l
with Diag(vl).

βij ∼ N (0, (σl

w)2) and replace W l

βij with

(cid:113)

βW l
vl

βij, then in the above expression we replace I

Similarly, the contribution of bl

i for l < L is

nl
(cid:88)

a.s.
lim

∂f
∂bl
i

(xa)

∂f
∂bl
i

(xb)

i=1
nl
(cid:88)

a.s.
lim

=

(cid:88)

=

(cid:88)

(cid:88)
(

(xa))(

∂f
∂hl
αi
α(cid:48)
αa,α(cid:48)bVφ(cid:48)(Σl)αa,α(cid:48)b

α

i=1
Πl

∂f
∂hl

α(cid:48)i

(xb))

α,α(cid:48)
= (cid:104)11T , Πl
= (cid:104)Πl

•a,•b (cid:12) Vφ(cid:48)(Σl)•a,•b(cid:105)

•a,•b, Vφ(cid:48)(Σl)•a,•b(cid:105)

The last layer weights (in the linear layer setting) contribute

a.s.
lim

1
nL−1sL−1

(cid:88)

α,i

xL−1
αi

(xa)xL−1

αi

(xb) =

1
sL−1 tr Vφ(ΣL−1)•a,•b.

38

Therefore

Corollary D.17. The NTK of the CNN deﬁned above converges almost surely to

NTK(xa, xb) a.s.−−→

1
sL−1 tr Vφ(ΣL−1)•a,•b +

(cid:88)

l<L

(cid:104)Πl

•a,•b (cid:12) Vφ(Σl

•a,•b), I (cid:63) Vφ(cid:48)(Σl−1

•a,•b)(cid:105) + (cid:104)Πl

•a,•b, Vφ(cid:48)(Σl)•a,•b(cid:105)

as long as φ has a polynomially bounded weak derivative.

D.8. Approximate Message Passing

We follow Bayati & Montanari (2011); Berthier et al. (2017) for a brief introduction to Approximate Message Passing.
Given an n × N matrix A, the compressed sensing problem asks for a way to reconstruct a (sparse) vector x0 ∈ RN from a
(small) vector of linear observations y = Ax0 + w ∈ Rn. Here w is a noise vector and A is assumed to be known. The
Approximate Message Passing algorithm (Donoho et al., 2009) starts with an initial guess x0 = 0 and proceed by

xt+1 = ηt(A(cid:62)zt + xt),

zt = y − Axt + αtzt−1

for an appropriate sequence of nonlinearities {ηt : R → R}t≥0 and αt = 1
t−1((A(cid:62)zt−1 + xt−1)i) ∈ R. The
n
algorithm succeeds if xt converges to a good approximation of x0. Similar algorithms have been applied to robust regression
(Donoho & Montanari, 2016), Bayesian estimation (Kamilov et al., 2012), low rank matrix recovery (Kabashima et al.,
2016), phase retrieval (Schniter & Rangan, 2015), and community detection in graphs (Deshpande et al., 2017).

i=1 η(cid:48)

(cid:80)N

The behavior of the AMP algorithm is accurately described by a formalism called “stated evolution” (SE), as n, N → ∞
with constant ratio n/N → δ ∈ (0, ∞), that bears some resemblance to the evolution of kernels in the GP correspondence
of deep neural networks (see Section 2.1) and to the gradient dynamical equations in the signal propagation analysis of
DNNs (see Section 2.2). SE was introduced in Donoho et al. (2009) and later suitably formalized and rigorously proved for
random Gaussian A and suitably smooth ηt in Bayati & Montanari (2011). A more general version of the algorithm where
ηt : RN → RN (instead of acting coordinatewise) was analyzed and a similar SE equations proved in Berthier et al. (2017).

As a corollary to one of our main theorems Thm 6.3, we show that, in the main theorem of Bayati & Montanari (2011), we
can forgo smoothness assumptions on ηt when each component of x0 is sampled iid from a Gaussian. We’ll work with the
following more general version of AMP from Bayati & Montanari (2011). The algorithm is deﬁned by two sequences of
functions {ft : R2 → R}t≥0, {gt : R2 → R}t≥0. Given w ∈ Rn, x0 ∈ RN , deﬁne the sequence of vectors ht, qt ∈ RN
and zt, mt ∈ Rn, by ﬁxing the initial condition q0, and obtaining {bt}t≥0, {mt}t≥0, {ht}t≥1, and {qt}t≥1 through

ht+1 = A(cid:62)mt − ξtqt,
bt = Aqt − λtmt−1,

mt = gt(bt, w),
qt = ft(ht, x0),

where ξt = 1
N σ2
t

(cid:104)bt, gt(bt, w)(cid:105) and λt = 1
nτ 2

t−1

(cid:104)ht, ft(ht, x0)(cid:105), 11 and σt and τt are deﬁned via

τ 2
t

def= E gt(σt(Z, W ))2,

σ2
t

def=

N
n

E ft(τt−1Z, X0)2,

where Z ∼ N (0, 1), W ∼ N (0, σ2

w), X0 ∼ N (0, σ2
x0

)

for sampling hyperparameters σ2

w, σ2

x0.

By translating the above computation into a tensor program and applying Thm 6.3, we obtain

Corollary D.18. Let {q0(N )}N ≥0 and {A(N )}N ≥0 be resp. a sequence of initial conditions and a sequence of matrices
A ∈ Rn×N indexed by N with iid entries Aij ∼ N (0, 1/n). Assume n/N → δ ∈ (0, ∞). Consider the sequence of
vectors {x0(N ), w(N )}N ≥0 whose empirical distributions converge weakly to N (0, σ2
w). Suppose that the
x0
functions ft and gt are polynomially bounded for all t. Then for any polynomially bounded function ψ : R2 → R and all

) and N (0, σ2

11Note that here we are using (cid:104), (cid:105) to denote (unscaled) inner product, which is different from the usage of this notation in Bayati &

Montanari (2011).

39

t ≥ 0,

1
N

N
(cid:88)

i=1

ψ(ht+1
i

, x0,i) a.s.−−→ E ψ(τtZ, X),

1
n

n
(cid:88)

i=1

ψ(bt

i, wi) a.s.−−→ E ψ(σtZ, W ),

as N → ∞, where X0 ∼ N (0, σ2
x0

) and W ∼ N (0, σ2

w) independent of Z ∼ N (0, 1).

This version differs from theorem 2 of Bayati & Montanari (2011) in the following ways

1. Bayati & Montanari (2011) deﬁned ξt = 1
N

i, x0,i), where the derivatives are
taken against the ﬁrst argument. This is asymptotically equivalent to our formulation here by Stein’s lemma Lem E.8.
Our formulation has the beneﬁt of being deﬁned for gt and ft without weak derivatives.

i, wi) and λt = 1
n

i=1 f (cid:48)

i=1 g(cid:48)

t(ht

t(bt

(cid:80)N

(cid:80)n

2. We are requiring that the x0 and w have empirical distributions that converge to Gaussians; with a bit more effort,
we can also prove a result that allow them to converge to any distribution with all moments. This is a much stronger
assumption than Bayati & Montanari (2011), who only assume that the limit distributions have some ﬁnite number of
bounded moments.

3. We don’t have any smoothness assumptions on the nonlinearities ft and gt, whereas Bayati & Montanari (2011)

requires them to be Lipschitz.

4. We don’t have any smoothness assumptions on the test function ψ, whereas Bayati & Montanari (2011) requires them

to be pseudo-Lipschitz of some order 12.

This concludes our discussion of various corollaries of our main theorems. We now turn to their proofs. First let us present
the necessary lemmas.

E. Lemmas

E.1. The Conditioning Trick

We ﬁrst recall Moore-Penrose pseudoinverse and some properties of it.
Deﬁnition E.1. For A ∈ Rn×m, a pseudoinverse of A is deﬁned as a matrix A+ ∈ Rm×n that satisﬁes all of the following
criteria

• AA+A = A

• A+AA+ = A+

• (AA+)(cid:62) = AA+

• (A+A)(cid:62) = A+A

The following facts are standard

• if A has real entries, then so does A+.

• The pseudoinverse always exists and is unique.

• When A is invertible, A+ = A−1.

• (A(cid:62))+ = (A+)(cid:62), which we denote as A+(cid:62).

12A function f : Rs → R is pseudo-Lipschitz of order k if there is a universal constant C s.t. |f (x) − f (y)| ≤ C(1 + |f (x)|k−1 +

|f (y)|k−1)(cid:107)x − y(cid:107). Note that this implies f is bounded by a polynomial of degree k

40

• A+ = (A(cid:62)A)+A(cid:62) = A(cid:62)(AA(cid:62))+.

• AA+ is the orthogonal projector to the column space of A; I − A+A is the orthogonal project to the null space of A.

• if A has singular value decomposition A = U ΛV where U and V are orthogonal and Λ has the singular values on its

diagonal, then A+ = V (cid:62)Λ+U (cid:62) where Λ+ inverts all nonzero entries of Λ.

• For any collection of vectors {vi}n

i=1 in a Hilbert space, w (cid:55)→ (cid:80)n

i,j=1 vi(Σ+)ij(cid:104)vj, w(cid:105), where Σij = (cid:104)vi, vj(cid:105), is the

projection operator to the linear span of {vi}n

i=1.

We present a slightly more general versions of lemmas from Bayati & Montanari (2011) that deal with singular matrices.
Lemma E.2. Let z ∈ Rn be a random vector with i.i.d. N (0, v2) entries and let D ∈ Rm×n be a linear operator. Then for
any constant vector b ∈ Rn the distribution of z conditioned on Dz = b satisﬁes:

z d=Dz=b D+b + Π˜z
where D+ is the (Moore-Penrose) pseudoinverse, Π is the orthogonal projection onto subspace {z : Dz = 0}, and ˜z is a
random vector of i.i.d. N (0, v2).

Proof. When D = [Im×m|0m×n−m], this claim is immediate. By rotational symmetry, this shows that, for any vector space
V and v orthogonal to it, conditioning z on V + v yields a Gaussian centered on v with covariance determined by ΠV z. Then
the lemma in the general case is implied by noting that {z : Dz = b} can be decomposed as {z : Dz = 0} + D+b.

Lemma E.3. Let A ∈ Rn×m be a matrix with random Gaussian entries, Aij ∼ N (0, σ2). Consider ﬁxed matrices
Q ∈ Rm×q, Y ∈ Rn×q, P ∈ Rn×p, X ∈ Rm×p. Suppose there exists a solution in A to the equations Y = AQ and
X = A(cid:62)P . Then the distribution of A conditioned on Y = AQ and X = A(cid:62)P is

where

A d=Y =AQ,X=A(cid:62)P E + Π⊥

P

˜AΠ⊥
Q

E = Y Q+ + P +(cid:62)X (cid:62) − P +(cid:62)P (cid:62)Y Q+,

˜A is an iid copy of A, and Π⊥
are the orthogonal projection to the space spanned by the column spaces of P and Q respectively.

P = I −ΠP = P P + and Π⊥

Q = I −ΠQ = QQ+ in which ΠP = I −P P + and ΠQ = I −QQ+

Proof. We apply Lem E.2 to D : A (cid:55)→ (AQ, P (cid:62)A). The pseudoinverse of D applied to (Y, X (cid:62)) can be formulated as the
unique solution of

argmin
A

(cid:8)(cid:107)A(cid:107)2

F : AQ = Y, P (cid:62)A = X (cid:62)(cid:9)

where (cid:107) − (cid:107)F denotes Frobenius norm. We check that E is a 1) a solution to AQ = Y, P (cid:62)A = X (cid:62) and 2) the minimal
norm solution.

We have EQ = Y Q+Q + P +(cid:62)X (cid:62)Q − P +(cid:62)P (cid:62)Y Q+Q. Note that Y Q+Q = Y because Y = AQ =⇒ Y Q+Q =
AQQ+Q = AQ = Y . So EQ = Y + P +T (X (cid:62)Q − P (cid:62)Y ). But X (cid:62)Q = P (cid:62)AQ = P (cid:62)Y , so EQ = Y as desired. A
similar, but easier reasoning, gives P (cid:62)E = X (cid:62). This veriﬁes that E is a solution.

To check that E is minimal norm, we show that it satisﬁes the stationarity of the Lagrangian

L(A, Θ, Γ) = (cid:107)A(cid:107)2

F + (cid:104)Θ, Y − AQ(cid:105) + (cid:104)Γ, X − A(cid:62)P (cid:105).

∂A = 0 =⇒ 2A = ΘQ(cid:62) + P Γ(cid:62) for some choices of Θ ∈ Rn×q and Γ ∈ Rm×p. For Θ = 2Y (Q(cid:62)Q)+ and

So ∂L
Γ(cid:62) = 2(P (cid:62)P )+[X (cid:62) − P (cid:62)Y Q(cid:62)], we can check that

ΘQ(cid:62) + P Γ(cid:62) = 2Y (Q(cid:62)Q)+Q(cid:62) + 2P (P (cid:62)P )+[X (cid:62) − P (cid:62)Y Q+]

= 2Y Q+ + 2P +(cid:62)X (cid:62) − 2P +(cid:62)P (cid:62)Y Q+
= 2E

as desired.

41

E.2. Probability Facts

Theorem E.4 (Strong Law of Large Numbers for triangular arrays (Hu & L. Taylor, 1997)). Let {Xn,i : 1 ≤ i ≤ n, n ≥ 1}
be a triangular array of random variables with (Xn,1, . . . , Xn,n) mutually independent with mean equal to zero for each n
and n−1 (cid:80)n

E |Xn,i|2+ρ ≤ cnρ/2 for some 0 < ρ < 1, c < ∞. Then 1
n

i=1 Xi,n → 0 almost surely as n → ∞.

(cid:80)n

i=1

Lemma E.5. Let {Xn}n≥1 be a sequence of random variables with zero mean.
E X 2p

n ≤ cn−1−ρ, for some ρ > 0, then Xn → 0 almost surely.

If for some p ∈ N and for all n,

Proof. By Markov’s inequality, for any (cid:15) > 0,

Pr(|Xn| > (cid:15)) = Pr(X 2p

(cid:88)

n

Pr(|Xn| > (cid:15)) ≤

(cid:88)

n

n > (cid:15)2p) ≤ E X 2p
cn−1−ρ/(cid:15)2p < ∞.

n /(cid:15)2p ≤ cn−1−ρ/(cid:15)2p

By Borel-Cantelli Lemma, almost surely, |Xn| ≤ (cid:15) for all large n. Then, if we pick a sequence {(cid:15)k > 0}k converging to 0,
we have that, almost surely, for each k, |Xn| ≤ (cid:15)k for large enough n — i.e. almost surely, Xn → 0.

The following is a standard fact about multivariate Gaussian conditioning
Proposition E.6. Suppose Rn1+n2 (cid:51) x ∼ N (µ, K), where we partition x = (x1, x2) ∈ Rn1 × Rn2, µ = (µ1, µ2) ∈
d=x2 N (µ|x2, K|x2) where
Rn1 × Rn2 , and K =

. Then x1

(cid:19)

(cid:18)K11 K12
K21 K22

µ|x2 = µ1 − K12K +
K|x2 = K11 − K12K +

22(x2 − µ2)
22K21.

Lemma E.7. Let Φ : Rn → R be measurable. Then for z ∼ N (ζ, Σ),

d2
dζ 2

E Φ(z) = 2

d
dΣ

E Φ(z)

whenever both sides exist.

Proof. First assume Σ is invertible. We check

e− 1

d
dζ
d2
dζ 2 e− 1
e− 1
d
dΣ

2 (ζ−z)Σ−1(ζ−z) = −Σ−1(ζ − z)e− 1

2 (ζ−z)Σ−1(ζ−z)

2 (ζ−z)Σ−1(ζ−z) = (cid:2)−Σ−1 + Σ−1(ζ − z)(ζ − z)(cid:62)Σ−1(cid:3) e− 1

2 (ζ−z)Σ−1(ζ−z)

2 (ζ−z)Σ−1(ζ−z)

det(2πΣ)1/2

=

=

1
2
1
2

(cid:2)−Σ−1 + Σ−1(ζ − z)(ζ − z)(cid:62)Σ−1(cid:3) e− 1

2 (ζ−z)Σ−1(ζ−z)

det(2πΣ)1/2

d2
dζ 2 e− 1

2 (ζ−z)Σ−1(ζ−z).

Integrating against Φ gives the result. For general Σ, apply a continuity argument, since the set of invertible Σs is dense
inside the set of all PSD Σ.

Lemma E.8 (Stein’s lemma). For jointly Gaussian random variables Z1, Z2 with zero mean, and any function φ : R → R
where E φ(cid:48)(Z1) and E Z1φ(Z2) exists, we have

E Z1φ(Z2) = Cov(Z1, Z2) E φ(cid:48)(Z2).

42

E.3. α-controlled functions

The next lemma is easy to show using the equivalence of norms in ﬁnite dimensional Euclidean space.
Lemma E.9. Let φ : Rk → R. The following are equivalent

1. φ is α-controlled

2. For some p ≥ 1 and some g(x) = o(cid:107)x(cid:107)p→∞((cid:107)x(cid:107)α

p ), C, c > 0, |φ(x)| ≤ eC(cid:107)x(cid:107)α

p +g(x)

3. For all p ≥ 1, there is some C, c > 0, |φ(x)| ≤ eC(cid:107)x(cid:107)α

p +c

Lemma E.10. Let Ck

α : R≥0 → R, c (cid:55)→ Ez∼N (0,Ik) ec(cid:107)z(cid:107)α

2 . Then

1. Ck

α < ∞ iff α < 2

2. for α ≥ 1,

E
z∼N (µ,Σ)

where (cid:107)Σ(cid:107)2 denotes the spectral norm of Σ.

eC(cid:107)z(cid:107)α

2 ≤ eC(cid:107)µ(cid:107)α

2 Ck

α(Cα(cid:107)Σ(cid:107)α/2

2

)

3. for any α-controlled φ : Rk → R with α ≥ 1, there is C > 0 such that for all µ ∈ Rk, Σ ∈ PSDk,

E
z∼N (µ,Σ)

where (cid:107)Σ(cid:107)2 denotes the spectral norm of Σ.

|φ(z)| ≤ CeC(cid:107)µ(cid:107)α

2 Ck

α(Cα(cid:107)Σ(cid:107)α/2

2

)

Note that the RHS is a montonic function in (cid:107)µ(cid:107)2 and (cid:107)Σ(cid:107)2, in the sense that if (cid:107)µ(cid:107)2 and (cid:107)Σ(cid:107)2 don’t decrease, then the
RHS will not decrease either.

Proof. The ﬁrst claim is obvious and the third follows from the second easily. For the second,

E
z∼N (µ,Σ)

eC(cid:107)z(cid:107)α

2 ≤

E
z∼N (0,I)

eC(cid:107)

√

Σz+µ(cid:107)α
2

≤

E
z∼N (0,I)
≤ eC(cid:107)µ(cid:107)α

2

√

eCα((cid:107)

Σz(cid:107)α

2 )
2 +(cid:107)µ(cid:107)α

E
z∼N (0,I)

eCα(cid:107)Σ(cid:107)α/2

2

(cid:107)z(cid:107)α
2

= eC(cid:107)µ(cid:107)α

2 Ck

α(Cα(cid:107)Σ(cid:107)α/2

2

).

E.4. Hermite Polynomials

n=0 Hen(x) tn

=
n! . Let L2(R; N (0, 1)) be the space of square-integrable functions against the standard Gaussian measure,
G = (cid:104)φ, φ(cid:105)G. Let Hn(x) = Hen(x)/(cid:107)Hen(cid:107)G

We follow a presentation roughly given by O’Donnell (2014).
Deﬁnition E.11. Let Hen(x) be the probabilist’s Hermite polynomial, given by the generating function ext− 1
(cid:80)∞
equipped with inner product (cid:104)φ, ψ(cid:105)G = Ex∼N (0,1) φ(x)ψ(x) and norm (cid:107)φ(cid:107)2
be the normalized versions.
Fact E.12. {Hen(x)}n≥0 form an orthogonal basis for L2(R; N (0, 1)) and {Hn(x)}n≥0 form an orthonormal basis for
L2(R; N (0, 1)).
Fact E.13. (cid:107)Hen(cid:107)2

G = n! so that Hn(x) = Hen(x)/

2 t2

n!.

√

43

Suppose u1, . . . , uk are unit vectors in Rk, and let ρij := (cid:104)ui, uj(cid:105). Construct a zero mean Gaussian vector z = (z1, . . . , zk)
such that E zizj = ρij. Note that z d= U g where g = (g1, . . . , gk) is a standard Gaussian vector and U = (ui
i,j=1 is the
matrix with ui as rows. Then for any s = (s1, . . . , sk) we can compute

j)k

E exp((cid:104)s, z(cid:105)) = E exp(s(cid:62)U g) = E (cid:89)

exp(gi(U (cid:62)s)i)

(cid:89)

=

i
E exp(gi(U (cid:62)s)i)

i

by independence of {gi}i

(cid:32)

1
2
(cid:18) 1
2


1
2

(cid:89)

=

exp

i

(cid:18) 1
2

(U (cid:62)s)2
i

(cid:19)

(cid:33)

= exp

= exp

(U (cid:62)s)2
i

(cid:88)

i

(cid:19)

(cid:107)U (cid:62)s(cid:107)2

= exp







1
2

= exp

(cid:104)ui, uj(cid:105)sisj







ρijsisj

 .

(cid:88)

i,j

(cid:88)

i,j

(cid:33)

sizi − s2
i

= exp





(cid:88)



ρijsisj



Dividing by exp (cid:0) 1

2

(cid:80)

i s2
i

(cid:1), we obtain

E exp

(cid:32)

(cid:88)

i

E (cid:89)
i

(cid:88)

m

Hem(zi)(m!)−1sm

i =

(cid:89)

(cid:88)

i<j
(n!)−1 (ρijsisj)n

i<j

n

(cid:88)

(cid:89)

(mi)k

i=1

i

smi
i
mi!

E (cid:89)
i

Hemi(zi) =

(cid:88)

(cid:89)

(cid:80)
s
i

j(cid:54)=i n(ij)

(cid:89)

(n(ij))i<j

i

i<j

n(ij)
ρ
ij
n(ij)!

where mi ≥ 0 for all i, and n(ij) = n(ji) ≥ 0 are indexed by unordered sets {i, j}. Matching coefﬁcients of s, we get

Theorem E.14. For any sequence (mi ≥ 0)k

i=1,

E (cid:89)
i

Hemi(zi) =

E (cid:89)
i

Hmi (zi) =





n(ij)
ρ
ij
n(ij)!

(cid:32)

(cid:89)

(cid:33) 


(cid:89)

mr!

r

(cid:32)

(cid:89)

(cid:112)

i<j
(cid:33) 


(cid:89)

mr!

r

i<j





n(ij)
ρ
ij
n(ij)!

whenever there are (n(ij) ≥ 0)i<j such that, for all i, mi = (cid:80)

j(cid:54)=i n(ij). E (cid:81)

i Hemi(zi) = 0 otherwise.

In particular,
Theorem E.15. If φi : R → R has Hermite expansion φi(z) = (cid:80)∞

u=0 aiuHu(z) = (cid:80)∞

u=0 biuHeu(z) where biu =

44

√

aiu/

u!, then

E (cid:89)
i

φi(zi) =

(cid:88)

(cid:32)

(cid:89)

(cid:33) 


brmr mr!

(cid:89)





n(ij)
ρ
ij
n(ij)!

(n(ij))i<j

r

(cid:88)

(cid:32)

(cid:89)

(n(ij))i<j

r

(cid:88)

(cid:32)

(cid:89)

(n(ij))i<j

r

=

=

i<j
(cid:33) 


(cid:89)

armr

(cid:112)

mr!

i<j

(cid:115)(cid:18) mi

{n(ij)}j(cid:54)=i

armr





n(ij)
ρ
ij
n(ij)!
(cid:19)(cid:33) 




ρ

n(ij)
ij



(cid:89)

i<j

j(cid:54)=i n(ij), whenever the RHS is absolutely convergent.

where mi = (cid:80)
Lemma E.16. Suppose φi, i ∈ [k] are as in Thm E.15, with additionally the constraint that we have an index set I ⊆ [k]
such that bi0 = ai0 = 0 (i.e. E φi(zi) = 0) for all i ∈ I. Assume that, for some λ < 1/2, |ρij| ≤ λ/(k − 1) for all i (cid:54)= j.
Then

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

k
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
φi(zi)
(cid:12)
(cid:12)

≤ Ck,|I|

(cid:33)

(cid:107)φr(cid:107)G

λ(cid:100)|I|/2(cid:101)

(cid:32) k
(cid:89)

r=1

for some constant Ck,|I| depending on k and |I| but independent of {φi}i and λ.

Proof. In the notation of Thm E.15, (cid:0) mi

{n(ij)}j(cid:54)=i

(cid:1) ≤ (k − 1)mi by the multinomial theorem. Thus

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

k
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
φi(zi)
(cid:12)
(cid:12)

≤

(cid:88)

(n(ij))i<j :
∀r∈I,mr≥1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32) k
(cid:89)

r=1

armr

(cid:115)(cid:18) mr

{n(rj)}j(cid:54)=r

(cid:19)(cid:33) 


ρ

n(ij)
ij

(cid:89)

i<j

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

≤

=

(cid:88)

(n(ij))i<j :
∀r∈I,mr≥1

(cid:88)

(cid:32) k
(cid:89)

r=1

(cid:32) k
(cid:89)

(n(ij))i<j :
∀r∈I,mr≥1
(cid:32) k
(cid:89)

(cid:107)φr(cid:107)G

r=1

(cid:33)

(cid:16)

r=1

(cid:107)φr(cid:107)G

(cid:112)(k − 1)mr

(cid:33) 


(cid:18) λ

(cid:89)

(cid:19)n(ij)

k − 1

i<j





(cid:33)

(cid:107)φr(cid:107)G

λ

(cid:80)

i<j n(ij)

B|I|λ(cid:100)|I|/2(cid:101)(1 + o(1))

(cid:17)

.

where BV is the number of ways to cover V vertices with (cid:100)V /2(cid:101) edges, and o(1) is a term that goes to 0 as λ → 0 and
is bounded above by a function of k whenever λ < 1/2. Then an appropriate Ck,|I| can be chosen to obtain the desired
result.

Lemma E.17. Suppose φi, i ∈ [k] are as in Thm E.15, with additionally the constraint that, we have some index set
I ⊆ [3, k] such that for all i ∈ I, bi0 = ai0 = 0 (i.e. E φi(zi) = 0). Assume that |ρ12| ≤ 1/2, for some λ < 1/
8,
|ρij| ≤ λ/(k − 1) for all i (cid:54)= j and {i, j} (cid:54)= {1, 2}. Then

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

k
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
φi(zi)
(cid:12)
(cid:12)

≤ C (cid:48)

k,|I|

(cid:33)

(cid:107)φr(cid:107)G

λ(cid:100)|I|/2(cid:101)

(cid:32) k
(cid:89)

r=1

for some constant C (cid:48)

k depending on k and I but independent of {φi}i and λ.

45

Proof. Deﬁne P = {(i, j) : 1 (cid:54)= i < j (cid:54)= 2} and Q = {(i, j) : i < j & (i = 1 XOR j = 2)}. Also write
R = (cid:81)k

r=1 (cid:107)φr(cid:107)G. As in the above proof,

| E

k
(cid:89)

i=1

φi(zi)| ≤

(cid:88)

(n(ij))i<j :
∀r∈I,mr≥1

(cid:88)

≤

(n(ij))i<j :
∀r∈I,mr≥1

≤ R

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32) k
(cid:89)

r=1

armr

(cid:115)(cid:18) mr

{n(rj)}j(cid:54)=r

(cid:19)(cid:33) 


(cid:89)

ρ

n(ij)
ij


 2−n(12)

(i,j)∈P

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

R

2
(cid:89)

r=1

(cid:18) mr
n(12)

(cid:19)(cid:18) mr − n(12)
{n(rj)}j(cid:54)∈{1,2}

(cid:19) k
(cid:89)

r=3

(cid:112)(k − 1)mr





(cid:89)

(cid:18) λ

(cid:19)n(ij)

k − 1

(i,j)∈P


 2−n(12)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

2
(cid:89)

r=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

2
(cid:89)

r=1

(cid:19)

(cid:18) mr
n(12)

(k − 1)mr−n(12)(k − 1)

1
2

(cid:80)k

r=3 mr

(cid:18) λ

k − 1

(cid:19)(cid:80)

i<j n(ij)−n(12)

2−n(12)

(cid:18) mr
n(12)

(cid:19)

λ

(cid:80)

i<j n(ij)−n(12)2−n(12)

(cid:18) m1+m2
2
n(12)

(cid:19)

2−n(12)λ

(cid:80)

i<j n(ij)−n(12)

(cid:18)n(12) + 1
n(12)

2 m(12)

(cid:19)

2−n(12)λ

(cid:80)

i<j n(ij)−n(12)

(n(ij))i<j :
∀r∈I,mr≥1

= R

(cid:88)

(n(ij))i<j :
∀r∈I,mr≥1

≤ R

(cid:88)

(n(ij))i<j :
∀r∈I,mr≥1

≤ R

(cid:88)

(n(ij))i<j :
∀r∈I,mr≥1

where m(12) =

(cid:88)

n(ij)

≤ 2R

(cid:88)

(i,j)∈Q

(cid:18)

1
1 − 1/2

(cid:19)1+ 1

2 m(12)

(cid:80)

λ

(i,j)∈P n(ij)

(n(ij))(i,j)∈P :
∀r∈I,mr≥1
(cid:88)

≤ 2R

≤ 2R

(i,j)∈Q n(ij) λ

(cid:80)

2λ)

√
(
(n(ij))(i,j)∈P :
∀r∈I,mr≥1
(cid:16)
2|I|/4B|I|λ(cid:100)|I|/2(cid:101)(1 + o(1))

(cid:17)

(cid:80)

(i,j)∈P\Q n(ij)

where B|I| is the number of ways of covering |I| vertices with (cid:100) |I|
upper bounded by a function of k for all λ < 1/

√

2 (cid:101) edges, and o(1) is a term that goes to 0 as λ → 0 and is

8. Choosing the appropriate constant C (cid:48)

k,|I| then gives the result.

E.5. Moment Bounds of Lightly Correlated Gaussians

Lemma E.18. Let Π ∈ Rn×n be an orthogonal projection matrix. Then each diagonal entry Πii ∈ [0, 1].

Proof. Because Π = Π2, we have for each i, Πii = (cid:80)

j Π2

ij =⇒ Πii(1 − Πii) = (cid:80)

j(cid:54)=i Π2

ij ≥ 0 =⇒ Πii ∈ [0, 1].

Lemma E.19. Let Π ∈ Rn×n be an orthogonal projection matrix of rank k. Consider the correlation matrix C def=
D−1/2ΠD−1/2 where D = Diag(Π). Then the off-diagonal entries of C satisfy (cid:80)
ij ≤ 2.5(n − k)2, assuming
k ≥ n/2.

i<j C 2

Proof. Because Π = Π2, we have for each i, Πii = (cid:80)
time, each C 2

ij ∈ [0, 1]. Thus we seek an upper bound on the following linear program in the n(n − 1)/2 variables C 2

(ij)

j Π2

ij = (cid:80)

j ΠiiΠjjC 2

ij =⇒ 1 − Πii = (cid:80)

j(cid:54)=i ΠjjC 2

ij. At the same

46

(which identiy Cij = Cji = C(ij)).

This LP has the dual

Maximize

C 2

(ij)

(cid:88)

i(cid:54)=j

s.t. ∀i, 1 − Πii =

(cid:88)

ΠjjC 2

(ij)

∀i < j, C 2

j(cid:54)=i
(ij) ∈ [0, 1].

Minimize

(cid:88)

i<j

τij +

(cid:88)

i

(1 − Πii)ζi

s.t. ∀i < j, τij + ζiΠjj + ζjΠii ≥ 1

∀i < j, τij ≥ 0
∀i, ζi ∈ R.

Any feasible value of the dual LP is an upper bound on the original LP. We now set the dual variables.
n for each i ∈ [k]. First deﬁne ρ def= k

WLOG, assume Π11 ≥ · · · ≥ Πnn. Then necessarily, 1 ≥ Πii ≥ k
Note that ρ − 2 = (n−k)2

.

nk

n + n

k = k2+n2

nk ≥ 2.

Dual variables for 1 ≤ i < j ≤ k. Now set ζi

def= 1
ρΠii

for i ∈ [k]. Then for 1 ≤ i < j ≤ k,

τij

def= 1 − (ζiΠjj + ζjΠii)

= 1 −

1
ρ

(cid:0)rij + r−1

ij

(cid:1)

where rij = Πii/Πjj ≥ 1. Note that 1) τij is nonnegative: indeed, since r + r−1 is increasing in r for r ≥ 1, and
rij ≤ r1k ≤ n/k, we have rij + r−1

ij ≤ ρ, so that τij ≥ 0; 2) τij ≤ (n−k)2

ij ≥ 2 so τij ≤ 1 − 2/ρ = (n−k)2
n2+k2 .

n2+k2 : rij + r−1

ζj for j > k. Now set ζj

def= 1
Π11

(cid:16)

1 − Πjj
2Π11

(cid:17)

for k < j ≤ n. Note that ζj ≥ 1/2.

τij for i ≤ k < j. Then for i ≤ k < j, set

τij

def= 1 − ζiΠjj − ζjΠii
Πjj
ρΠii

− ζjΠii.

= 1 −

47

Note that for Π11 = x ≥ y ≥ k/n,

Πjj
ρx
Πjj
ρ

=

+ ζjx −

(cid:18) Πjj
ρy

(cid:19)

+ ζjy

+ ζj(x − y)

y − x
xy
(cid:18)

= (x − y)

(cid:18)

= (x − y)

= (x − y)x−1

≥ (x − y)x−1

(cid:19)

(cid:19)

(cid:18)

(cid:18)

1 −

x−1

− (xy)−1 Πjj
ρ
(cid:19)

ζj − (xy)−1 Πjj
ρ
(cid:19)
Πjj
2x
− y−1 Πjj
ρ
− (k/n)−1 k/n
ρ
(cid:19)

1 −

1 −

(cid:18)

(cid:18)

Πjj
2x
k/n
2x
1
2

−

1
2

≥ (x − y)x−1

1 −

(cid:19)

(15)

≥ 0.

Thus for all i ≤ k < j, τ1j ≤ τij. Simultaneously, Eq. (15) also shows that Πjj
that τij − τ1j ≤ Π11−Πii

n . We check that τ1j ≥ 0:

≤ n−k

Π11

ρx + ζjx −

(cid:16) Πjj

ρy + ζjy

(cid:17)

≤ (x − y)x−1, so

τ1j = 1 −

− ζjΠ11

−

Πjj
ρΠ11
Πjj
ρΠ11
(cid:18) 1
2
Πjj
1
2ρ
Π11
(n − k)2
nk

−

= 1 −

=

Πjj
Π11
(cid:20)

∈

0,

(cid:20)

⊆

0,

(cid:18)

1 −

(cid:19)

Πjj
2Π11

(cid:19)

1
ρ
(n − k)2
nk
(cid:21)

(cid:21)

Combined with our deduction above, we get τij ≤ τ1j + n−k

n ≤ n(n−k)

nk = n−k
k .

τij for k < i < j. For k < i < j, we set

(cid:19)(cid:19)

(cid:18)

1 −

Πjj
2Π11

Πii
Π11
(cid:19)

τij

def= 1 − (ζiΠjj + ζjΠii)
Πii
2Π11

(cid:18) Πjj
Π11

= 1 −

1 −

(cid:18)

(cid:19)

+

= 1 −

−

(cid:18) Πjj + Πii
Π11

ΠjjΠii
Π2
11
(Π11 − Πjj)(Π11 − Πii)
Π2
11

=

∈ [0, 1].

48

Summary of dual variables.

In summary, we have

∀1 ≤ i ≤ k, ζi

∀k < i ≤ n, ζi

def= (ρΠii)−1 ∈
(cid:18)

1 −

def=

1
Π11
(cid:20)

(cid:105)

(cid:104)
ρ−1, ρ−1 n
k
(cid:20) 1
(cid:19)
2

∈

Πjj
2Π11
(cid:21)

(cid:21)

,

n
k

∀1 ≤ i < j ≤ k, τij ∈

0,

∀i ≤ k < j ≤ n, τij ∈

0,

(cid:20)

(n − k)2
n2 + k2
(cid:21)
n − k
k

∀k < i < j ≤ n, τij ∈ [0, 1].

Objective value given by the dual variables. Now we compute

(cid:88)

i<j


τij +

(cid:88)

(1 − Πii)ζi

i



=



(cid:88)

+

(cid:88)

+

(cid:88)

 τij +

i<j≤k
(cid:18) k(k − 1)
2

≤

< (n − k)2

(cid:18) 1
4
≤ 2.5(n − k)2

k<i<j

i≤k<j
(n − k)2
n2 + k2 + k(n − k)
(cid:19)
1
2

+ 1 +

+ (n − k)

n − k
k
(cid:18) 1
ρ

(cid:32) k

(cid:88)

+

(cid:33)

n
(cid:88)

i=1

i=k+1

(1 − Πii)ζi

+

(n − k)(n − k − 1)
2

(cid:19)

(cid:18)

+

k

n − k
n

n
kρ

+ (n − k)

(cid:19)

n
k

+

(cid:19)

n
k

assuming k ≥ n/2.

Lemma E.20. Let z ∼ N (0, Π) where Π ∈ Rn×n is an orthogonal projection matrix of rank k. Suppose φi : R → R for
each i has ﬁnite variance Var (φi(x) : x ∼ N (0, Πii)). Then

Var

(cid:32) n
(cid:88)

i=1

(cid:33)

φi(zi)

≤

(cid:18) 5(n − k)2
√

2

(cid:19) (cid:88)

+ 1

i

Var (φi(x) : x ∼ N (0, Πii)) .

In particular, if n − k = O(1), then Var ((cid:80)n

i=1 φi(zi)) = Θ ((cid:80)

i Var (φi(x) : x ∼ N (0, Πii))) .

Proof. Let C = D−1/2ΠD−1/2, D = Diag(Π), be the correlation matrix of Π. Let ψi(y) = φi(
Ex∼N (0,1)[φi(

Πiix)]. Then

√

√

Πiiy) −

Var
z∼N (0,Π)

(cid:32) n
(cid:88)

i=1

(cid:33)

φi(zi)

=

E
z∼N (0,C)

(cid:33)2

ψi(zi)

(cid:32) n
(cid:88)

i=1

=

E
z∼N (0,C)

(cid:88)

i,j

ψi(zi)ψj(zj).

Expand ψi in the Hermite orthonormal basis,

ψi(x) = ai1H1(x) + ai2H2(x) + · · ·

where Hj(x) is the jth Hermite polynomial, normalized so that Ez∼N (0,1) Hj(z)2 = 1, (note that H0(x) = 1 and does
def=
not appear here because Ex∼N (0,1) ψi(x) = 0 by construction). For any locally integrable φ : R → R, let (cid:107)φ(cid:107)2
G

49

Ez∼N (0,1) φ(z)2, so that (cid:107)ψi(cid:107)2

G = (cid:80)

k a2

ik = Var (φi(x) : x ∼ N (0, Πii)) . Then,

(cid:88)

i<j

E
z∼N (0,C)

ψi(zi)ψj(zj) =

(cid:88)

∞
(cid:88)

aikajkC k
ij

i<j

∞
(cid:88)

k=1

∞
(cid:88)

k=1

≤

≤

k=1
(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)





(cid:88)

i<j





ika2
a2
jk



(cid:88)



i<j



C 2k
ij



(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

1
2

(cid:32)

(cid:88)

(cid:33)2 


(cid:88)



C 2
ij



a2
ik

i

i<j

≤ 2−1/2

∞
(cid:88)

(cid:32)

(cid:88)

k=1

i

a2
ik

since |Cij| ≤ 1
(cid:33)

(2.5(n − k)2)

by Lem E.19

=

5(n − k)2
23/2

(cid:88)

i

(cid:107)ψi(cid:107)2
G

On the other hand, (cid:80)

i

Ex∼N (0,1) ψi(x)2 = (cid:80)

i (cid:107)ψ(cid:107)2

G, so that

(cid:88)

i,j

E
z∼N (0,C)

ψi(zi)ψj(zj) ≤

(cid:18) 5(n − k)2
√

2

=

(cid:18) 5(n − k)2
√

2

(cid:19) (cid:88)

i

(cid:19) (cid:88)

+ 1

+ 1

i

(cid:107)ψi(cid:107)2
G

Var (φi(x) : x ∼ N (0, Πii)) .

Theorem E.21. Let z ∼ N (0, Π) where Π ∈ Rn×n is an orthogonal projection matrix of rank n − O(1), where O(1)
denotes a quantity that stays bounded as n → ∞. Suppose φi : R → R for each i ∈ [n] has ﬁnite centered moments
Ex[(φi(x) − Ex(cid:48) φi(x(cid:48)))r], for x, x(cid:48) ∼ N (0, Πii), for all r ≤ 2p, where p ≥ 6. Then for Q def= 1
i=1 φi(zi), as n → ∞,

(cid:80)n

n

E[(Q − E Q)2p] ≤ O

(cid:18)

n−1.5 max
i∈[n]

E
x

(cid:20)(cid:16)

φi(x) − E
x(cid:48)

(cid:17)2p

φi(x(cid:48))

: x, x(cid:48) ∼ N (0, Πii)

(cid:21)(cid:19)

.

If in addition, each φi has ﬁnite centered moments up to r ≤ 2pL for some L > 1, then

E[(Q − E Q)2p] ≤ O

(cid:118)

(cid:117)
(cid:117)
n−1.5+1/L L
(cid:116)

(cid:20)(cid:16)

1
n

n
(cid:88)

i=1

E
x

φi(x) − E
x(cid:48)

(cid:17)2pL

φi(x(cid:48))

: x, x(cid:48) ∼ N (0, Πii)



(cid:21)
 .

Here O(−) hides constants that do not depend on n, any of the functions φi, or Π.

Proof. Let C = D−1/2ΠD−1/2, D = Diag(Π), be the correlation matrix of Π. Let ψi(y) = φi(
Ex∼N (0,1)[φi(

Πiix)].

√

√

Πiiy) −

Order the off-diagonal entries of the correlation matrix in the order of decreasing squared value:

C 2
(ij)(1) ≥ C 2

(ij)(2) ≥ . . . ≥ C 2

(ij)(N ) ,

where N = (cid:0)n
R, by Lem E.19, we deduce that |C(ij)t| ≤ n−1/4 for all t > R

(cid:1), and (ij)(t) = (itjt) are unordered pairs of distinct indices it (cid:54)= jt. Since (cid:80)

n.

√

2

t C 2

(ij)t ≤ R for some constant

50

Consider the (2p)th centered moment E (cid:0) 1
N (0, C). We shall bound the sum to show that this moment is not too large.

i=1 ψi(yi)(cid:1)2p

= E n−2p (cid:80)

(cid:80)n

n

σ:[2p]→[n]

(cid:81)2p

a=1 ψσ(a)(yσ(a)), where y ∼

First note the naive bound via AM-GM,

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2p
(cid:89)

a=1

(cid:12)
(cid:12)
(cid:12)
ψσ(a)(yσ(a))
(cid:12)
(cid:12)

≤ E 1
2p

≤ max
i∈[n]

= max
i∈[n]

2p
(cid:88)

ψσ(a)(yσ(a))2p

[ψi(y)2p]

a=1
E
y∼N (0,1)
E[(φi(zi) − E φi(zi))2p : zi ∼ N (0, Πii)]

def= B2p.

(16)

Now, for any collection of numbers {xi ∈ R}m

i=1 and any L > 0, we have the trivial bound maxi |xi| ≤

(cid:80)n

i=1(E[ψi(y)2p])L ≤ n1/LB2p,L, where B2p,L

(cid:16)(cid:80)m

j=1 |xj|L(cid:17)1/L
,
def=

and this bound is tighter the larger L is. Thus B2p ≤ n1/L L
(cid:113) 1
n

E[ψi(y)2pL], for any L.

(cid:80)n

i=1

L

(cid:113) 1
n

We can categorize the n2p terms of E (cid:80)
not depending on n or the functions ψi.

σ:[2p]→[n]

(cid:81)2p

a=1 ψσ(a)(yσ(a)) as follows. Here we use O(−) to hide any constant

• Suppose σ is injective.

– Suppose for each a (cid:54)= b, (σ(a)σ(b)) = (ij)t for some t > R

a=1 ψσ(a)(yσ(a)) ≤
for some constant C independent of {ψr}r and n. Thus the contribution of all

n. By Lem E.16, E (cid:81)2p

√

(cid:16)(cid:81)2p

C
r=1 (cid:107)ψσ(r)(cid:107)G
such σ to the sum is at most

(cid:17) (cid:0)n−1/4(cid:1)p

(cid:32) 2p
(cid:89)

(cid:88)

C

σ

r=1

(cid:33)

(cid:107)ψσ(r)(cid:107)G

n−p/4 ≤ O


n−p/4

(cid:32) n
(cid:88)

i=1

(cid:33)2p
 .

(cid:107)ψσ(r)(cid:107)G

– Suppose for some a, b ∈ [2p], (σ(a)σ(b)) = (ij)t for t ≤ R

n · n2p−2 =
(cid:1) ways of choosing their preimages under σ
n of choosing such a t, 2(cid:0)2p
O(n2p−1.5) such σ (indeed, there are R
out of 2p, and ≤ n2p−2 ways of choosing the rest of the values of σ). By Eq. (16), the contribution of all such σ
to the sum is at most O(n2p−1.5B2p).

n. There are at most 2(cid:0)2p

(cid:1)R

√

2

2

√

√

• Suppose for some a∗ (cid:54)= b∗ in [2p], σ(a∗) = σ(b∗), but σ|[n]\{a∗,b∗} is injective and takes range outside {σ(a∗)}. There

are (cid:0)2p

2

(cid:1)n(cid:0) n−1

2p−2

(cid:1) = O(n2p−1) such σ.

– Suppose for each a (cid:54)= b, (σ(a)σ(b)) = (ij)t for some t > R

to the functions {ψ2
that the I of Lem E.16 has size 2p − 2, and the λ of Lem E.16 is (k − 1)n−1/4. Then Lem E.16 gives

σ(a∗)} ∪ {ψσ(a)}a(cid:54)∈{a∗,b∗}, with ψ2

n, so that |Cσ(a)σ(b)| ≤ n−1/4. We apply Lem E.16
σ(a∗) being the sole function whose expectation is not 0, so

√

2p
(cid:89)

E

a=1

ψσ(a)(zσ(a)) ≤ C(cid:107)ψ2

σ(a∗)(cid:107)G





(cid:89)

(cid:107)ψσ(a)(cid:107)G


 (n−1/4)(2p−2)/2

a(cid:54)∈{a∗,b∗}



= C(cid:107)ψ2

σ(a∗)(cid:107)G



(cid:89)

(cid:107)ψσ(a)(cid:107)G

a(cid:54)∈{a∗,b∗}


 n−(p−1)/4

for some constant C. Thus the collective contribution of such σ to the sum is at most

 n−(p−1)/4 ≤ O


n−(p−1)/4

(cid:107)ψσ(a)(cid:107)G

(cid:32) n
(cid:88)

σ(a∗)(cid:107)G

C(cid:107)ψ2

(cid:107)ψ2

(cid:88)

(cid:89)

i (cid:107)G





(cid:33) (cid:32) n
(cid:88)

(cid:33)2p−2
 .

(cid:107)ψi(cid:107)G

σ

a(cid:54)∈{a∗,b∗}

i=1

i=1

51

– Suppose for some a, b ∈ [2p], (σ(a)σ(b)) = (ij)t for t ≤ R

√

n. There are at most (cid:0)2p

√

(cid:1)n · R

2

n · (cid:0) n−2

2p−3

(cid:1) =

O(n2p−1.5) such σ. Using Eq. (16) again, we can upper bound the contribution of such σ by O(n2p−1.5B2p).

• Otherwise, there are more than one pair of inputs that collide under σ. There are at most O(n2p−2) such σ. Using

Eq. (16), we upper bound their contributions by O(n2p−2B2p).

To summarize,

E (cid:88)

2p
(cid:89)

σ:[2p]→[n]

a=1

ψσ(a)(yσ(a)) ≤ O

≤ O

(cid:16)

(cid:16)

n1.75pB(cid:48)

2p + n1.75(p−1)B(cid:48)(cid:48)

2p + n2p−1.5B2p

(cid:17)

n1.75pB(cid:48)

2p + n1.75(p−1)B(cid:48)(cid:48)

2p + n2p−1.5+1/LB2p,L

(cid:17)

(cid:32)

E

1
n

n
(cid:88)

i=1

φi(zi) − E 1
n

n
(cid:88)

i=1

(cid:33)2p

φi(zi)

≤ O (cid:0)n−0.25pB(cid:48)

2p + n−0.25p−1.75B(cid:48)(cid:48)

2p + n−1.5B2p

(cid:1)

where

(cid:16)

≤ O

n−0.25pB(cid:48)

2p + n−0.25p−1.75B(cid:48)(cid:48)

2p + n−1.5+1/LB2p,L

(cid:17)

B(cid:48)

2p =

B(cid:48)(cid:48)

2p =

(cid:32)

(cid:32)

1
n

1
n

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:33)2p

(cid:107)ψi(cid:107)G

(cid:107)ψ2

i (cid:107)G

(cid:33) (cid:32)

1
n

n
(cid:88)

i=1

(cid:33)2p−2

(cid:107)ψi(cid:107)G

.

By the power mean inequality, we get that B(cid:48)

2p, B(cid:48)(cid:48)

2p ≤ B2p ≤ n1/LB2p,L. Substitution then gives the desired result.

This theorem can be signiﬁcantly strengthened with more careful case work and applying more involved versions of
Lems E.16 and E.17.
Lemma E.22. Suppose φ : R → R is polynomially bounded: ∀x, |φ(x)| ≤ C(1 + |x|p) for some C. Then for y ∼ N (0, 1),

for a universal constant R depending only on p but not on µ or σ.

|∂µ Var φ(σy + µ)| ≤ Rσ−1(1 + σ2p + |µ|2p)

Proof. By Lem E.7,

∂µ E φ(σy + µ)2 = σ−1 E yφ(σy + µ)2
(cid:12)∂µ E φ(σy + µ)2(cid:12)
(cid:12)

(cid:12) ≤ σ−1C 2 E |y|(1 + |σy + µ|p)2
≤ σ−1C 2 E |y|(1 + 2p−1(σp|y|p + |µ|p))2
≤ σ−13C 2 E |y|(1 + 22p−2(σ2p|y|2p + |µ|2p))
≤ σ−1C (cid:48)(1 + σ2p + |µ|2p)

where C (cid:48) depends only on p but not on µ or σ. Similarly,

|∂µ E φ(σy + µ)| ≤ σ−1C (cid:48)(cid:48)(1 + σp + |µ|p)
|E φ(σy + µ)| ≤ C (cid:48)(cid:48)(cid:48)(1 + σp + |µ|p)

for constants C (cid:48)(cid:48), C (cid:48)(cid:48)(cid:48) depending only on p but not on µ or σ. Therefore,

|∂µ Var φ(σy + µ)| ≤ (cid:12)

(cid:12)∂µ E φ(σy + µ)2(cid:12)

(cid:12) + 2 |E φ(σy + µ)| |∂µ E φ(σy + µ)|

≤ σ−1C (cid:48)(1 + σ2p + |µ|2p) + 2C (cid:48)(cid:48)(cid:48)(1 + σp + |µ|p)σ−1C (cid:48)(cid:48)(1 + σp + |µ|p)
≤ C (cid:48)(cid:48)(cid:48)(cid:48)σ−1(1 + σ2p + |µ|2p)

52

for constant C (cid:48)(cid:48)(cid:48)(cid:48) depending only on p but not on µ or σ.

F. Proof of Main Theorems

Theorem 4.3. Consider dimension constraints Λ and a skeleton π without T lines, i.e. no transpose allowed. Suppose all fl
are α-controlled for some α < 2. Sample all input vars as in Section 3.2 and assume almost sure rank convergence. Then
for any c ∈ C and any α-controlled function φ : Rc → R, α < 2,

1
nct

nct
(cid:88)

i=1

φ(gct

i ) a.s.−−→ E φ(Z)

where gct

i = (glt

i )gl∈c and Rc (cid:51) Z = (Z g)g∈c ∼ N (µc, K c).

Proof. WLOG we assume that all input vars appear in the beginning of the program. We do induction on the length of the
program.

(cid:80)nct

i

i=1 φ(gcint

) a.s.−−→ E φ(z) where gcint

Base case. We show that for each c, 1
nct
z ∼ N (µcin∞, K cin∞).
For α-controlled φ, α ∈ [1, 2), for every q > 0, there is monotonic function f such that Ex∼N (µ,K) |φ(x)|q ≤
f ((cid:107)µ(cid:107)2, (cid:107)Σ(cid:107)2) by Lem E.10 uniformly over all µ, K. If we let Xnct,i = φ(gcint
E |Xn,i|2+ρ
), then 1
nct
is bounded uniformly over all t as µcint and K cint are bounded uniformly over all t. So by Thm E.4, 1
) −
nct
) a.s.−−→ 0. Because Ez∼N (µcin t,Kcin t) φ(z) → Ez∼N (µcin∞,Kcin∞) φ(z) as t → ∞ (for example by an easy
E φ(gcint
application of dominated convergence), we have our desired result.

(cid:80)nct
i=1
(cid:80)nct

) − E φ(gcint

i=1 φ(gcint

d= N (µcint, K cint) and

i = (glt

i )gl∈cin

i

i

i

i

Inductive Case. The inductive case for lines of type LinComb is obvious, as α-controlled functions are closed under
composition with linear transforms.

Suppose at time t, gt = gLt = Atht is a line of type MatMul and ht = f (gl01t, . . . , gl0k0 t) is a line of type Nonlin
where gl01t, . . . , gl0k0 t (resp. At) are some previous G-vars (resp. A-var). (The case for gt = Atg(cid:48)t for a G-var g(cid:48)t
can be reduced to this case by setting f = id and ht = f (g(cid:48)t)). Set c def= c1
def= c(h) = c2(A).
Suppose gi := Ahi, i = 1, . . . , r, are all previous lines of type MatMul involving A. Here {gi}r
i=1
are G- or H-vars, deﬁned by Nonlin lines hi := f i(gli1 , . . . , gliki ) for a collection of functions {f i : R → R}r
i=1. Let
Gt def= [g1t| · · · |grt] ∈ Rnc1t×r, H t def= [h1t| · · · |hrt] ∈ Rnc2t×r, so that Gt = AtH t. We will abuse notation and sometimes
use G to mean the collection of G-vars {gj}r
j=1. Let At be the σ-algebra spanned by the G-vars appearing before g. By the
conditioning trick Lem E.3,

def= c(g) = c1(A) and c2

i=1 are G-vars and {hi}r

gt d=At (Gt(H t)+ + ˜AtΠ⊥

H t)ht

where ˜At is an independent copy of At and ΠH t = H t(H t)+ is projection to the space spanned by the columns of
i:(H t)+ht and standard deviation
H t. Each ith coordinate of gt is independent conditioned on At, with mean µt
i
σt def= σAt(cid:113)
H tht(cid:107)2/n2(At) where σAt is a shorthand for σline(A)t. For simplicity, we assume σAt = 1 for all t; the
general case follows very easily from the reasoning below and the fact that σAt converges to a ﬁnite, nonzero value.
Claim F.0.1. (σt)2 a.s.−−→ (σ∞)2 def= K c(g, g) − K c(g, G)K c(G, G)+K c(G, g).

def= Gt

(cid:107)Π⊥

Proof: Note that (σt)2 = 1
because f (z)2 is α-controlled for some α < 2,

nc2t (ht(cid:62)ht − ht(cid:62)ΠH tht) = 1

nc2 t (ht(cid:62)ht − ht(cid:62)H t(H t(cid:62)H t)+H t(cid:62)ht). By induction hypothesis,

1
nc2t ht(cid:62)ht =

1
nc2t

nc2t
(cid:88)

i=1

f (gl01t, . . . , gl0k0 t)2 a.s.−−→ E[f (zl01, . . . , zl0k0 )2 : z ∼ N (µc, K c)] = K c(g, g),

53

where z = (zl)gl∈c. Likewise, because both f and {f j}j are α-controlled jointly for some α < 2, by induction hypothesis,

1
nc2t ht(cid:62)hjt =

1
nc2t

nc2t
(cid:88)

i=1

f (gl01t, . . . , gl0k0 t)f j(glj1t, . . . , gljkj t)

a.s.−−→ E[f (zl01 , . . . , zl0k0 )f j(zlj1, . . . , zljkj ) : z ∼ N (µc, K c)]
= K c(g, gj)
nc2t
(cid:88)

ljk

f j(cid:48)

(glj(cid:48) 1t, . . . , g

j(cid:48) t)f j(glj1t, . . . , gljkj t)

1

nc2t hj(cid:48)t(cid:62)hjt =

1
nc2t

i=1
a.s.−−→ E[f j(cid:48)
= K c(gj(cid:48)

(zlj(cid:48)1 , . . . , z
, gj).

lj(cid:48) k

j(cid:48) )f j(zlj1 , . . . , zljkj ) : z ∼ N (µc, K c)]

Finally, by the rank convergence assumption we get (H t(cid:62)H t)+ a.s.−−→ K c(G, G)+.
Claim F.0.2. (H t)+ht a.s.−−→ v∞ def= K c(G, G)+K c(G, g).

(cid:4)

Proof: This is similar to the above. (H t)+ht = (H t(cid:62)H t)+H t(cid:62)ht a.s.−−→ K c(G, G)+K c(G, g).
Let L = line(g). Let φ : R|c<L| → R be α-controlled with α ∈ [1, 2), such that for coefﬁcients C, c > 0, |φ(x)| ≤
eC (cid:80)

i |xi|α+c. Since for every q > 0,

(cid:4)

(cid:104)

E

|φ(gt

i , gc<Lt

i

)|q(cid:12)
(cid:12)

(cid:12) At(cid:105)

(cid:104)

= E

(cid:105)
)|q : z ∼ N (0, 1)

|φ(µt

i + σtz, gc<Lt
i+σtz|α+(cid:80)

i

|µt

(cid:16)

i |α(cid:17)
|ˆgt

+cq

ˆg∈c<L

|µt

i|α+|σtz|α+(cid:80)

ˆg∈c<L

i |α(cid:17)
|ˆgt

+cq

|µt

i|α+(cid:80)

ˆg∈c<L

i |α(cid:17)
|ˆgt

|µt

i|α+(cid:80)

ˆg∈c<L

i |α(cid:17)
|ˆgt

+cq E
z
+cqC1

eCqα(σt)α|z|α

α(Cqα(σt)α),

eCq

≤ E
z

(cid:16)
eCqα
≤ E
z
(cid:16)
= eCqα
(cid:16)
= eCqα

we have

1
nct

nct
(cid:88)

(cid:104)

E

i=1

|φ(gt

i , gc<Lt

i

)|q(cid:12)
(cid:12)

(cid:12) At(cid:105)

≤

1
nct C1

α(Cqα(σt)α)

≤

≤

1
nct C1

α(Cqα(σt)α)

1
nct C1

α(Cqα(σt)α)

nct
(cid:88)

i=1
nct
(cid:88)

i=1
nct
(cid:88)

i=1

(cid:16)
eCqα

|µt

i|α+(cid:80)

i |α(cid:17)
|ˆgt

+cq

ˆg∈c<L

(cid:16)(cid:80)

eC(cid:48)qα

(cid:16)(cid:80)

eC(cid:48)qα

j |vt

j gjt

i |α+(cid:80)

ˆg∈c<L

i |α(cid:17)
|ˆgt

+cq

j (|v∞

j |+1)|gjt

i |α+(cid:80)

ˆg∈c<L

i |α(cid:17)
|ˆgt

+cq

for large enough t, almost surely

def=

1
nct C1

α(Cqα(σt)α)

nct
(cid:88)

i=1

ψ(gc<Lt
i

)

(17)

for some constant C (cid:48), where ψ is a α-controlled function. Thus by our claims above, this converges as nct → ∞ to

α(Cqα(σ∞)α)
C1

E
z∼N (µc,Kc)

ψ(zc<L),

where zc<L = (zl)l∈c,l<L. Therefore it is also almost surely uniformly bounded in t, so that with q = 2 + ρ, we can apply

54

Thm E.4 to Xnct,i = φ(gt

i , gc<Lt

i

(cid:104)

) − E

φ(gt

i , gc<Lt

i

(cid:12)
(cid:12)
)

(cid:12) At(cid:105)

to conclude that

1
nct

nct
(cid:88)

i=1

φ(gt

i , gc<Lt

i

(cid:104)

) − E

φ(gt

i , gc<Lt

i

(cid:12)
(cid:12)

(cid:12) At(cid:105) a.s.−−→ 0.

)

After applying the following claim and the induction hypothesis on





gc<Lt
i

(cid:55)→ E

φ



(cid:88)

j

gjt
i v∞

j + σ∞z, gc<Lt

i





 : z ∼ N (0, 1)

 ,

we get

1
nct

nct
(cid:88)

i=1

φ(gt

i , gc<Lt

i

) a.s.−−→ E





φ



r
(cid:88)

j=1

ζ line(gj )v∞

j + σ∞z, ζ c<L


 : z ∼ N (0, 1), ζ ∼ N (µc, K c)



 .

This yields the desired theorem by noting that conditioned on ζ c<L , (cid:80)

Claim F.0.3.

1
nct

(cid:80)nct
i=1

E

(cid:104)

φ(gt

i , gc<Lt

i

(cid:12)
(cid:12)
)

(cid:12) At(cid:105)

− E

(cid:104)

φ((cid:80)

j gjt

i v∞

j + σ∞z, gc<Lt

i

j ζ line(gj )v∞

j + σ∞z d= ζ L via Proposition E.6.
(cid:105) a.s.−−→ 0.

) : z ∼ N (0, 1)

Proof: From the claims above we have that almost surely, gt
i |) + (1 + o(1))z, where
i
ot(1) is a quantity that decreases to 0 with t and doesn’t depend on i. Let Φ(xL, xc<L ; σ) def= E[φ(xL + σz, xc<L ) :
z ∼ N (0, 1)] = E[φ(z, xc<L ) : z ∼ N (xL, σ2)]. Then by Lem E.7, Φ is differentiable in xL and ∂xL Φ(xL, xc<L ; σ) =
σ−1 E[zφ(xL + σz, xc<L ) : z ∼ N (0, 1)]. Clearly,

i v∞

j + o(1)((cid:80)

j |gjt

d=At (cid:80)

j gjt

|∂xLΦ(xL, xc<L ; σ)| ≤ σ−1 E[|zφ(xL + σz, xc<L )| : z ∼ N (0, 1)]

|Φ(xL, xc<L ; σ) − Φ(xL + (cid:15), xc<L ; σ)| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) xL+(cid:15)

xL

dξ ∂ξΦ(ξ, xc<L; σ)

≤ σ−1 E[|z|eC(|xL+σz|α+(cid:107)xc<L (cid:107)α
≤ σ−1eCα(|xL|α+(cid:107)xc<L (cid:107)α
def= σ−1eCα(|xL|α+(cid:107)xc<L (cid:107)α

α)+c : z ∼ N (0, 1)]

: z ∼ N (0, 1)]

α)+c E[|z|eCασα|z|α
α)+cR(σ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) xL+(cid:15)

≤ σ−1R(σ)

| dξ| eCα(|ξ|α+(cid:107)xc<L (cid:107)α

α)+c

≤ σ−1R(σ)

xL
(cid:90) (cid:15)

0

| dξ| eCα2(|xL|α+|ξ|α+(cid:107)xc<L (cid:107)α

α)+c

≤ σ−1R(σ)|(cid:15)|eCα2(|xL|α+|(cid:15)|α+(cid:107)xc<L (cid:107)α

α)+c.



− E

 φ(

(cid:12)
(cid:12)

(cid:12) At(cid:105)

)

(cid:88)

j

gjt
i v∞

j + σtz, gc<Lt

i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

)

z ∼ N (0, 1)

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Hence

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nct

nct
(cid:88)

(cid:104)

E

i=1

φ(gt

i , gc<Lt

i

≤

1
nct

nct
(cid:88)

i=1

(σt)−1R(σt)ot(1)(

(cid:88)

j

i |)eCα2(cid:16)|(cid:80)
|gjt

j gjt

i v∞

j |α

+ot(1)((cid:80)

j |gjt

i |)α

c<Lt
+(cid:107)g
i

(cid:107)α
α

(cid:17)

+c

≤ (σt)−1R(σt)ot(1)

1
nct

nct
(cid:88)

i=1

Ψ(gc<Lt
i

)

55

for some α-controlled function Ψ. By induction hypothesis, 1
nct
quantity decreases to 0 due to ot(1). A similar argument shows that we can replace σt with σ∞.

) converges almost surely, so that the entire
(cid:4)

i=1 Ψ(gc<Lt

i

(cid:80)nct

Theorem 5.1. Sample {vit}L∇
i=1 with zero mean (i.e. µcint(gL+LA+i) → 0 for all i ∈ [L∇]) and independently from the
input vars {xlt}Lg
) = 0 if l > L ≥ l(cid:48)) 13. Sample all other vars in ˜π according to Section 3.2. Assume
all fl of ˜π are polynomially bounded and ˜π satisﬁes almost sure rank convergence. Then for any dimension constraints Λ,
any c ∈ C(˜π,Λ), and any polynomially bounded function φ : Rc → R,

l=1 (i.e. K cint(gl, gl(cid:48)

1
nct

nct
(cid:88)

i=1

φ(gct

i ) a.s.−−→ E φ(Z)

where gct

i = (glt

i )gl∈c and Z ∼ N (µc, K c).

Note that we impose a more stringent condition here, compared to Thm 4.3, that fl are polynomial bounded, because, as it
will be apparent from the reasoning below, we need to reason about compositions of φ and fl; if we still allow fl and φ to
be α-controlled in general, then their composition in general is not integrable against Gaussian measures.

Proof. Thm 4.3 already show that this is true for all gl in π. Because we assume that vit are sampled independently from
xlt, this is also true up to line L + LA + L∇. We induct on the line number starting from (cid:96) = L + LA + L∇ + 1.

If line (cid:96) does not produce a new G-var, then there is nothing to prove.

If line (cid:96) is of type LinComb, then the induction hypothesis is obviously true.

In the following, suppose line (cid:96) is of type MatMul.

Setup This line involves a transposed matrix, (cid:96) : g(cid:96) := AL+ahl, where AL+a = (Aa)(cid:62) by line L + a and l > L (the
argument for gl instead of hl is similar and simpler). Let c = c1 = c1(AL+a) = c(g(cid:96)) and c2 = c2(AL+a) = c(hl).
Conditioning on all G-vars that appeared before, we have constraints of the form git = AL+a,thit for i = 1, . . . , r,
and g(cid:48)it = Aath(cid:48)it for i = 1, . . . , s. Here {gi}r
i=1 are
previous G- or H-vars. Letting Gt = [g1t| · · · |grt] ∈ Rnc1t×r (where git are treated as column vectors), and similarly
for H t ∈ Rnc2t×r, G(cid:48)t ∈ Rnc2t×s, H (cid:48)t ∈ Rnc1t×s, we get the expressions Gt = AL+a,tH t, G(cid:48)t = AatH (cid:48)t. We will abuse
notation and sometimes use G to also denote the corresponding collection of G-vars; likewise for G(cid:48), H, H (cid:48). By the
construction of ˜π, gi = AL+ahi are lines that appear after π, and g(cid:48)i = Aah(cid:48)i are lines that appear in π. In addition, for each
i ∈ [r], hi is an H-var that appears after π.

i=1 are previous G-vars and {hi}r

i=1 and {h(cid:48)i}s

i=1 and {g(cid:48)i}s

Let At be the σ-algebra spanned by the values of all G-vars that appeared before line (cid:96) at time t. By the conditioning trick,
we have

g(cid:96)t d=At (Et + Π⊥

H (cid:48) t ˜AtΠ⊥

H t)hlt

with

Et = GtH t+ + H (cid:48)t+(cid:62)G(cid:48)t(cid:62) − H (cid:48)t+(cid:62)G(cid:48)t(cid:62)H tH t+

= Gt(H t(cid:62)H t)+H t(cid:62) + H (cid:48)t(H (cid:48)t(cid:62)H (cid:48)t)+G(cid:48)t(cid:62) − H (cid:48)t(H (cid:48)t(cid:62)H (cid:48)t)+G(cid:48)t(cid:62)H t(H t(cid:62)H t)+H t(cid:62)

where ˜At is sampled independently and identically as AL+a,t. Note that

H (cid:48)ty, with y ∼ N (0, Inc1t)

g(cid:96)t d=At µt + σtΠ⊥
µt def= Ethlt
(σt)2 def= (σat)2 nc2t

nc1t (cid:107)Π⊥

H thlt(cid:107)2/nc2t

13In our previous example of f (x) = v(cid:62)ρ(x), this corresponds to the readout layer v sampled with zero mean and independently from

x and other parameters of ρ.

56

where, to recall, (σat)2/n2(Aat) = (σat)2/n1(AL+a,t) is the sampling variance of each entry of Aat and AL+a,t. For brevity,
we use the following shorthands

Σt def= H t(cid:62)H t/nc2t ∈ Rr×r
ωt def= H t(cid:62)hlt/nc2t ∈ Rr

Σ(cid:48)t def= H (cid:48)t(cid:62)H (cid:48)t/nc1t ∈ Rs×s
βt def= G(cid:48)t(cid:62)hlt/nc2t ∈ Rs

Υt def= G(cid:48)t(cid:62)H t/nc2t ∈ Rs×r

so that

µt = GtΣt+ωt +

nc2t
nc1t H (cid:48)tΣ(cid:48)t+βt −

nc2t
nc1t H (cid:48)tΣ(cid:48)t+ΥtΣt+ωt.

By induction hypothesis, Σt, Σ(cid:48)t, Υt, ωt, βt all converge almost surely to corresponding limit values: Let α = αc2,c1 =
limt→∞

i = line(h(cid:48)i), then, with Z ∼ N (µc, K c),

nc2t
nc1t ; if λi = line(hi), λ(cid:48)

Σt
ij
Σ(cid:48)t
ij
ωt
i

a.s.−−→ Σ∞
ij
a.s.−−→ Σ(cid:48)∞
ij
a.s.−−→ ω∞
i

def= E fλi (Z)fλj (Z) = (σk∞)−2α−1K c(gi, gj)
def= E fλ(cid:48)
j (Z) = (σk∞)−2K c(g(cid:48)i, g(cid:48)j)
def= E fλi(Z)fl(Z) = (σk∞)−2α−1K c(gi, g(cid:96))

i (Z)fλ(cid:48)

Υt
i

a.s.−−→ 0

βt
i

a.s.−−→ 0.

The last two limits go to 0 because H t and hlt are odd in v1, . . . vL∇, which are sampled independently from G(cid:48)t as
remarked above. Consequently, by our rank assumption, Σt+ a.s.−−→ Σ∞+, Σ(cid:48)t+ a.s.−−→ Σ(cid:48)∞+.

Claim F.0.1. (σt)2 a.s.−−→ (σ∞)2 def= K c(g(cid:96), g(cid:96)) − K c(g(cid:96), G)K c(G, G)+K c(G, g(cid:96)) with t.

Proof: We have

(σt)2
(σat)2 nc2 t
nc1 t

= hlt(cid:62)Π⊥

H thlt/nc2t

= (cid:107)hlt(cid:107)2/nc2t − hlt(cid:62)ΠH thlt/nc2t
= (cid:107)hlt(cid:107)2/nc2t − (hlt(cid:62)H t/nc2t)(H t(cid:62)H t/nc2t)+(H t(cid:62)hlt/nc2t)
= (cid:107)hlt(cid:107)2/nc2t − ωt(cid:62)Σt+ωt.

(cid:80)nc2 t

i )2 a.s.−−→ E[fl(Z)2 : Z ∼ N (µc, K c)] = (σk∞)−2α−1K c(g(cid:96), g(cid:96)) by induction hypothe-
Now (cid:107)hlt(cid:107)2/nc2t = 1
nc2t
sis. On the other hand, again by induction hypothesis and the rank assumption, the second term converges almost surely
to (σk∞)−2α−1K c(g(cid:96), G)K c(G, G)+K c(G, g(cid:96)). Combined with the simple fact that (σat)2 nc2 t
nc1 t → (σ∞t)2α, we get the
desired result.

i=1 (hlt

(cid:4)
Let vt def= Σt+ωt, for t ∈ [1, ∞], so that, by our rank condition, vt a.s.−−→ v∞ = Σ∞+ω∞, which we can check is equal to
K c(G, G)+K c(G, g(cid:96)).

Claim F.0.2. For some vectors εt ∈ Rr, ε(cid:48)t ∈ Rs that go to 0 almost surely with t, µt = Ethlt = Gt(v∞ + εt) + H (cid:48)tε(cid:48)t.

Proof: Follows immediately from the fact derived above that Σt+ a.s.−−→ Σ∞+, ωt a.s.−−→ ω∞, Υt a.s.−−→ 0, βt a.s.−−→ 0.
Convergence almost surely. Let φ be a function with |φ(x)| ≤ C(1 + (cid:107)x(cid:107)2p), p ∈ N; φ will be our test function. With

(cid:4)

57

w ∼ N (0, 1), Z ∼ N (µc, K c),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nct

nct
(cid:88)

i=1

1
nct

1
nct

1
nct

nct
(cid:88)

i=1

nct
(cid:88)

i=1

nct
(cid:88)

i=1

with

A def=

B def=

C def=

φ(g(cid:96)t

i , gc<(cid:96)t

i

) − E
Z

(cid:12)
(cid:12)
(cid:12)
φ(Z)]
(cid:12)
(cid:12)
(cid:12)

≤ A + B + C





r
(cid:88)

j gjt
v∞

i + σ∞w, gc<(cid:96)t

i


 − E
Z

(cid:12)
(cid:12)
(cid:12)
φ(Z)
(cid:12)
(cid:12)
(cid:12)

φ

E
w

φ

E
w

j=1

(cid:18)

i + σt(cid:113)
µt

(Π⊥

H (cid:48)t)iiw, gc<(cid:96)t

i

(cid:19)

− E
w

φ





r
(cid:88)

j=1

j gjt
v∞

i + σ∞w, gc<(cid:96)t

i

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:16)

φ

i , gc<(cid:96)t
g(cid:96)t

i

(cid:17)

− E
w

φ

(cid:18)

i + σt(cid:113)
µt

(Π⊥

H (cid:48) t)iiw, gc<(cid:96)t

i

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

We shall show that each of A, B, C goes to 0 almost surely as t → ∞.
Claim F.0.3. A a.s.−−→ 0.

Proof: Note that if Rc (cid:51) ζ ∼ N (µc, K c),

r
(cid:88)

j=1

j ζ gj
v∞

+ σ∞w

= (ζ G)(cid:62)K c(G, G)+K c(G, g(cid:96)) + (K c(g(cid:96), g(cid:96)) − K c(g(cid:96), G)K c(G, G)+K c(G, g(cid:96)))w
d=ζc<(cid:96) ζ g(cid:96)

.

(cid:16)(cid:80)r

j=1 v∞

Since Ew φ
induction hypothesis.
Claim F.0.4. C a.s.−−→ 0.

j gjt

i + σ∞w, gc<(cid:96)t

i

(cid:17)

is purely a polynomially-bounded function of gc<(cid:96)t

i

, this claim is given by the
(cid:4)

Proof: Fix the values of gc<(cid:96)t.
E
H(cid:48)t )ii) φt

x(cid:48)∼N (0,(Π⊥

i(x(cid:48)). By Thm E.21 applied to Π⊥

H (cid:48) t and {φt

For each i ∈ [nct],

let φt
i}nct

i(x) def= φ(µt
i + σtx, gc<(cid:96)t
i=1, we get, for any ρ ≥ 6 and any q > 1,

), and ˜φt

i

i(x) def= φt

i(x) −

E

z∼N (0,Π⊥

H(cid:48) t )





1
nct

nct
(cid:88)

i=1



2ρ

˜φt
i(zi)



≤ O

(cid:118)

(cid:117)
(cid:117)
(nct)−1.5+1/q q
(cid:116)

1
nct

nct
(cid:88)

i=1

E

zi∼N (0,(Π⊥

H(cid:48) t )ii)

(cid:104) ˜φt

i(zi)2ρq


(cid:105)


where the constant hidden in O(−) is independent of q, t, the functions φt
(cid:104) ˜φt
1
nct
with z ∼ N (0, Π⊥

i, and ΠH (cid:48)t . We ﬁrst show that the sum
is uniformly bounded almost surely in t over the probability of {At}t≥1, for any q > 1. Indeed,

(cid:80)nct
i=1

i(zi)2ρq(cid:105)
H (cid:48) t),

Ezi

1
nct

nct
(cid:88)

i=1

E
zi

(cid:104) ˜φt

i(zi)2ρq(cid:105)

≤

1
nct 22ρq−1

nct
(cid:88)

i=1

E
zi

(cid:34)
i(zi)2ρq +
φt

(cid:18)

E
z(cid:48)
i

(cid:19)2ρq(cid:35)

i(z(cid:48)
φt
i)

≤

1
nct 22ρq

nct
(cid:88)

i=1

E
zi

(cid:2)φt

i(zi)2ρq(cid:3)

=

1
nct 22ρq

nct
(cid:88)

i=1

(cid:104)

E
zi

φ(µt

i + σtzi, gc<(cid:96)t

i

)2ρq(cid:105)

≤

1
nct C (cid:48)

nct
(cid:88)

i=1

(cid:104)

E
zi

t→∞
a.s.
≤

1
nct C (cid:48)(cid:48)

nct
(cid:88)

i=1

|µt

i|4ρpq + |σtzi|4ρpq + (cid:107)gc<(cid:96)t

i

(cid:107)4ρpq(cid:105)

(cid:20)
|µt

i|4ρpq + (2|σ∞| + 1)4ρpq E
zi

|zi|4ρpq + (cid:107)gc<(cid:96)t

i

(cid:107)4ρpq

(cid:21)

58

for some constants C (cid:48), C (cid:48)(cid:48) > 0, where this last inequality holds for large enough t, almost surely. By induction hypothesis
and the fact that (Π⊥

H (cid:48)t)ii ∈ [0, 1] for all i,

1
nct

nct
(cid:88)

i=1

(cid:20)
(2|σ∞| + 1)4ρpq E
zi

|zi|4ρpq + (cid:107)gc<(cid:96)t

i

(cid:107)4ρpq

(cid:21)

is uniformly bounded in t, almost surely. Thus it remains to bound

1
nct

nct
(cid:88)

i=1

|µt

i|4ρpq =

1
nct

nct
(cid:88)





r
(cid:88)

gjt
i (v∞

j + εt

j) + h(cid:48)jt

i ε(cid:48)
j



4ρpq

t



i=1

j=1

≤

1
nct C (cid:48)(cid:48)(cid:48)

nct
(cid:88)

i=1





r
(cid:88)

where εt, ε(cid:48)t a.s.−−→ 0, by Claim F.0.2
4ρpq
4ρpq







gjt
i v∞
j



+



gjt
i εt

j + h(cid:48)jt

i ε(cid:48)
j

t→∞
a.s.
≤

1
nct C (cid:48)(cid:48)(cid:48)

j=1




nct
(cid:88)

r
(cid:88)



4ρpq

gjt
i v∞
j



+

i=1

j=1

j=1

|gjt

i | + |h(cid:48)jt
i |



r
(cid:88)

j=1




r
(cid:88)

t





4ρpq

for some constant C (cid:48)(cid:48)(cid:48), where the last inequality holds for large enough t, almost surely. Because each h(cid:48)jt
i
bounded function of gc<(cid:96)t
value, and hence is uniformly bounded in t, almost surely, as desired.

is a polynomially
, each summand of the RHS is as well 14. So by induction hypothesis, this converges to a ﬁnite

i

Thus, almost surely, E
Lem E.5 and the fact that nct strictly increases with t, we have

z∼N (0,Π⊥

˜φt
i(zi)

H(cid:48)t )

(cid:80)nct
i=1

(cid:16) 1
nct

(cid:17)2ρ

≤ c(nct)−1.25 for some c, by choosing q large enough. By

1
nct

nct
(cid:88)

i=1

˜φt
i(zi) =

1
nct

nct
(cid:88)

i=1

(cid:16)

ψ

i , gc<(cid:96)t
g(cid:96)t

i

(cid:17)

− E
w

ψ

(cid:18)

Claim F.0.5. B a.s.−−→ 0.

i + σt(cid:113)
µt

(Π⊥

H (cid:48) t)iiw, gc<(cid:96)t

i

(cid:19)

a.s.−−→ 0, where w ∼ N (0, 1).

(cid:4)

Proof: We apply a similar argument as the one in the proof of Thm 4.3 that leverages the smoothness of Gaussian average
over ˜A. The major difference here is that we have to deal with the varying variances (Π⊥
H (cid:48) t)ii for each t, but this can be
done by using the fact that rank H (cid:48)t = O(1).

Deﬁne Φ(x(cid:96), xc<(cid:96) ; σ) def= E[φ(x(cid:96) + σw, xc<(cid:96) ) : w ∼ N (0, 1)]. Then by Lem E.7, Φ is differentiable in x(cid:96) and σ2
2 σ−2 Ew∼N (0,1) φ(x(cid:96) +
with ∂x(cid:96) Φ(x(cid:96), xc<(cid:96); σ) = σ−1 E[wφ(x(cid:96) + σw, xc<(cid:96) ) : w ∼ N (0, 1)] and ∂σ2Φ(x(cid:96), xc<(cid:96) ; σ) = 1
14This is the only place where we need the assumption that all fl are polynomially bounded. Otherwise, their composition might not ne

integrable against the Gaussian measure.

59

σw, xc<(cid:96))(w2 − 1). Clearly,

|∂x(cid:96)Φ(x(cid:96), xc<(cid:96) ; σ)| ≤ σ−1C E[|w|(1 + |x(cid:96)| + |σw| + (cid:107)xc<(cid:96)(cid:107))2p : w ∼ N (0, 1)]

≤ σ−142p−1C E[|w|(1 + |x(cid:96)|2p + |σw|2p + (cid:107)xc<(cid:96) (cid:107)2p) : w ∼ N (0, 1)]
≤ σ−1C (cid:48)(1 + |x(cid:96)|2p + (cid:107)xc<(cid:96) (cid:107)2p + σ2p)

|∂σ2Φ(x(cid:96), xc<(cid:96) ; σ)| ≤

σ−2

1
2
≤ Dσ−2

E
w∼N (0,1)
E
w∼N (0,1)

|φ(x(cid:96) + σw, xc<(cid:96))||w2 − 1|

(1 + |x(cid:96)|2p + |σw|2p + (cid:107)xc<(cid:96)(cid:107)2p)|w2 − 1|

≤ D(cid:48)σ−2(1 + |x(cid:96)|2p + (cid:107)xc<(cid:96) (cid:107)2p + σ2p)

=⇒ (cid:107)∇x(cid:96),σ2Φ(x(cid:96), xc<(cid:96) ; σ)(cid:107) =

(cid:113)

|∂x(cid:96)Φ(x(cid:96), xc<(cid:96) ; σ)|2 + |∂σ2Φ(x(cid:96), xc<(cid:96) ; σ)|2
≤ D(cid:48)(cid:48)(1 + σ−2)(1 + |x(cid:96)|2p + (cid:107)xc<(cid:96) (cid:107)2p + σ2p)

for some constant C (cid:48), D, D(cid:48), D(cid:48)(cid:48) depending only on p and C. Thus

|Φ(x(cid:96), xc<(cid:96) ; σ) − Φ(x(cid:96) + ϑ, xc<(cid:96);

(cid:112)

σ2 + ς 2)|

≤ (|ϑ| + ς 2)

(cid:90) 1

0

dt (cid:107)∇x(cid:96),σ2 Φ(x(cid:96) + ϑt, xc<(cid:96) ;

(cid:112)

σ2 + ς 2t)(cid:107)

≤ D(cid:48)(cid:48)(|ϑ| + ς 2)

(cid:90) 1

(cid:18)

dt

1 +

≤ ˜C(|ϑ| + ς 2)

0
(cid:90) 1

(cid:18)

dt

1 +

≤ ˜C (cid:48)(|ϑ| + ς 2)

0
(cid:18)

1 +

(cid:19)

1
σ2

(cid:19)

1
σ2 + ς 2t

(1 + |x(cid:96) + ϑt|2p + (cid:107)xc<(cid:96)(cid:107)2p + (σ2 + ς 2t)p)

(cid:19)

1
σ2

(1 + |x(cid:96)|2p + ϑ2pt2p + (cid:107)xc<(cid:96) (cid:107)2p + σ2p + ς 2ptp)

(1 + |x(cid:96)|2p + ϑ2p + (cid:107)xc<(cid:96)(cid:107)2p + σ2p + ς 2p)

for some constants ˜C, ˜C (cid:48) depending only on p and C. Partition [nct] = U (cid:116) V where U def= {i : (Π⊥
its complement. Note that |U | ≤ 2 rank H (cid:48)t ≤ 2s. So

H (cid:48)t)ii < 1/2} and V is

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nct

nct
(cid:88)

i=1

(cid:18)

i + σt(cid:113)
µt

φ

E
w

(Π⊥

H (cid:48)t )iiw, gc<(cid:96)t

i

(cid:19)

(cid:16)

− E
w

φ

Gt

i:Σ∞+ω∞ + σ∞w, gc<(cid:96)t

i

nct
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:18) (cid:88)

i=1

=

≤

1
nct

1
nct

(cid:88)

i∈U
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i∈V

+

(cid:18)

i, gc<(cid:96)t
µt

i

; σt(cid:113)

(Π⊥

H (cid:48) t)ii

(cid:19)

(cid:16)

− Φ

Gt

i:Σ∞+ω∞, gc<(cid:96)t

i

Φ

; σ∞(cid:17)(cid:12)

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Φ

i, gc<(cid:96)t
µt

i

; σt(cid:113)

(Π⊥

H (cid:48)t)ii

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:16)

(cid:12)
(cid:12)
(cid:12)Φ

Gt

i:Σ∞+ω∞, gc<(cid:96)t

i

(cid:18)

i, gc<(cid:96)t
µt

i

Φ

; σt(cid:113)

(Π⊥

H (cid:48) t)ii

(cid:19)

(cid:16)

− Φ

60

Gt

i:Σ∞+ω∞, gc<(cid:96)t

i

; σ∞(cid:17)(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:19)

.

(cid:12)
(cid:12)
(cid:12)
; σ∞(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(18)

(19)

The ﬁrst sum (18) converges almost surely to 0:

1
nct

(cid:88)

i∈U

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Φ

i, gc<(cid:96)t
µt

i

; σt(cid:113)

(Π⊥

H (cid:48) t)ii

2s
nct max

i∈[nct]

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Φ

i, gc<(cid:96)t
µt

i

; σt(cid:113)

(Π⊥

H (cid:48) t)ii

+

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:12)
(cid:12)
(cid:12)Φ

Gt

i:Σ∞+ω∞, gc<(cid:96)t

i

; σ∞(cid:17)(cid:12)
(cid:12)
(cid:12)

+

(cid:16)

(cid:12)
(cid:12)
(cid:12)Φ

Gt

i:Σ∞+ω∞, gc<(cid:96)t

i

; σ∞(cid:17)(cid:12)
(cid:12)
(cid:12)

≤

≤

2s
(nct)1−1/N

(cid:118)
(cid:117)
(cid:117)
N
(cid:116)

1
nct

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Φ

(cid:88)

i∈[nct]

i, gc<(cid:96)t
µt

i

; σt

(cid:113)

(Π⊥

H (cid:48) t)ii

N

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

2s
(nct)1−1/N

(cid:118)
(cid:117)
(cid:117)
N
(cid:116)

1
nct

(cid:16)

(cid:12)
(cid:12)
(cid:12)Φ

(cid:88)

i∈[nct]

Gt

i:Σ∞+ω∞, gc<(cid:96)t

i

; σ∞

N

(cid:17)(cid:12)
(cid:12)
(cid:12)

(20)

for any N > 0. Now 1
nct

(cid:80)

i∈[nct]

(cid:16)

(cid:12)
(cid:12)
(cid:12)Φ

Gt

i:Σ∞+ω∞, gc<(cid:96)t

i

N

; σ∞(cid:17)(cid:12)
(cid:12)
(cid:12)

converges a.s. to a ﬁnite value, by Claim F.0.3, so for

large N , the second term in Eq. (20) converges to 0. Similarly, 1
converges a.s.
nct
to a ﬁnite value, as in the proof of Claim F.0.4, so for large N , the ﬁrst term of Eq. (20) and thus Eq. (20) itself go to 0
almost surely.

H (cid:48) t)ii

(Π⊥

i∈[nct]

i

i, gc<(cid:96)t
µt

(cid:80)

; σt(cid:113)

N

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:12)
(cid:12)
(cid:12)Φ

We proceed with the second sum (19):

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Φ

(cid:18)

i, gc<(cid:96)t
µt

i

; σt(cid:113)

(Π⊥

H (cid:48) t)ii

(cid:19)

(cid:16)

− Φ

Gt

i:Σ∞+ω∞, gc<(cid:96)t

i

; σ∞(cid:17)(cid:12)

(cid:12)
(cid:12)
(cid:12)

1
nct

(cid:88)

i∈V
(cid:18)

≤ ˜C (cid:48)

1 +

1

√

(cid:19) 1
nct

(cid:88)

i∈V

2)2

((cid:37)i + |(σ∞)2 − (σt)2(Π⊥

H (cid:48) t)ii|)(1 + |µt

i|2p + (cid:37)2p

i + (cid:107)gc<(cid:96)

i (cid:107)2p + max(σ∞, σt)2p)

min(σ∞, σt/

def= Gt
i:ε(cid:48)t with εt, ε(cid:48)t = o(1) a.s. coming from Claim F.0.2. Write Yi
i:εt + H (cid:48)t
where (cid:37)i
(cid:16)
max(σ∞, σt)2p). Since
is bounded by a constant multiple of
(cid:115) 1
nct

((cid:37)i + |(σ∞)2 − (σt)2(Π⊥

(cid:115) 1
nct

H (cid:48) t)ii|)2

min(σ∞,σt/

1 +

(cid:88)

(cid:88)

2)2

(cid:17)

√

1

i∈V

Y 2
i .

i∈V

i (cid:107)2p +
is obviously uniformly bounded in t, via Cauchy-Schwarz, the sum above

i|2p + (cid:37)2p

i + (cid:107)gc<(cid:96)

def= (1 + |µt

Using similar techniques as before, by applying induction hypothesis,
bounded in t, almost surely. All it remains to show is that the ﬁrst term in the product above converges to 0 a.s.. Now

i can be shown to be uniformly

(cid:113) 1
nct

(cid:80)

i∈V Y 2

≤

≤

≤

(cid:115) 1
nct
(cid:115) 1
nct
(cid:115) 1
nct

(cid:88)

i∈V

(cid:88)

i∈V

(cid:88)

i∈V

((cid:37)i + |(σ∞)2 − (σt)2(Π⊥

H (cid:48)t)ii|)2

(cid:37)2
i +

(cid:115) 1
nct

(cid:88)

i∈V

((σ∞)2 − (σt)2)2 +

(cid:115) 1
nct

(cid:88)

i∈V

(σt)4(1 − (Π⊥

H (cid:48) t)ii)2

i + |(σ∞)2 − (σt)2| + (σt)2
(cid:37)2

(cid:115) 1
nct

(cid:88)

i∈V

1 − (Π⊥

H (cid:48) t)ii

(cid:88)

(cid:115) 1
nct
(cid:115) 1
nct

i∈V

≤ (cid:107)εt(cid:107)

(cid:115) 1
nct

(cid:88)

i∈V

(H (cid:48)t

i:ε(cid:48)t)2 + |(σ∞)2 − (σt)2| + (σt)2

(cid:114)

2 rank H (cid:48)t
nct

since 1 − (Π⊥

H (cid:48) t)ii ∈ [0, 1/2]

(Gt

i:εt)2 +

(cid:107)Gt

i:(cid:107)2 + (cid:107)ε(cid:48)t(cid:107)

(cid:115) 1
nct

(cid:88)

i∈V

(cid:88)

i∈V

(cid:107)H (cid:48)t

i:(cid:107)2 + |(σ∞)2 − (σt)2| + (σt)2

(cid:114)

2 rank H (cid:48)t
nct

.

61

i:(cid:107)2 converges and so is also uniformly bounded in t, almost surely. Then, because
By induction hypothesis,
(cid:107)εt(cid:107) a.s.−−→ 0, the ﬁrst term converges to 0 a.s.. Likewise for the second term. The ﬁnal two terms also obviously converge to
0 almost surely with t. This completes the proof of our claim.

i∈V (cid:107)Gt

(cid:113) 1
nct

(cid:80)

(cid:4)

We are ﬁnally ready to prove Thm 6.3, but a few lemmas ﬁrst would help our effort signiﬁcantly.
Lemma F.1. Let X = (X1, . . . , Xn) be a multivariate Gaussian with 0 mean and nondegenerate covariance. For any
L2(X)-integrable function f ,

E Xif (X) =

=

n
(cid:88)

j=1
n
(cid:88)

j=1

Cov(Xi, Xj)
Var(Xj|X\j)

E(Xj − E[Xj|X\j])f (X)

Cov(Xi, Xj) E(Xj − Cov(Xj, X\j) Cov(X\j, X\j)+X\j)f (X)
Var(Xj) − Cov(Xj, X\j) Cov(X\j, X\J )+ Cov(X\j, Xj)

Proof. By a density argument, it sufﬁces to consider only the case when f is C∞. Then by Stein’s lemma,

E Xif (X) =

n
(cid:88)

j=1

Cov(Xi, Xj) E ∂jf (X).

By Stein’s lemma again,

E ∂jf (X) = E
X\j

E
Xj |X\j

∂jf (X)

= E
X\j

E
Xj |X\j

(Xj − Cov(Xj, X\j) Cov(X\j, X\j)+X\j)f (X)
Var(Xj) − Cov(Xj, X\j) Cov(X\j, X\J )+ Cov(X\j, Xj)

= E
X

(Xj − Cov(Xj, X\j) Cov(X\j, X\j)+X\j)f (X)
Var(Xj) − Cov(Xj, X\j) Cov(X\j, X\J )+ Cov(X\j, Xj)

.

Deﬁnition F.2. Let gl def= Ahm be a line in π and let gi = A(cid:48)hi, i = 1, . . . , s be all previous MatMul lines that involve A(cid:48).
Here A(cid:48) is the A-var such that A(cid:48) := A(cid:62) if A is an input A-var, or A := A(cid:48)(cid:62) if A is a transposed var. Then, by the construction
of ˇπ, ϕ(gl) def= ϕg(gl) + (cid:80)s
j=1 ajfϕ(hj ), so
fϕ(gl) = fϕg(gl) + fϕh(gl).

j=1 ajϕ(hj) for some coefﬁcients a deﬁned in Defn 6.2. Deﬁne fϕh(gl) = (cid:80)s

Let gl := Ahm be a MatMul line in π and set ˇc = c(ϕ(gl)). Consider the Hilbert space L2(Z) of L2 functions of Z ∼
N (µˇc, Kˇc) (equivalently, square-integrable random variables in the σ-algebra generated by Z). Write (cid:104)f, g(cid:105) = E f (Z)g(Z)
for its inner product.
Lemma F.3. Fix a line number λ ≥ l. Let gi = A(cid:48)hi, i = 1, . . . , s be all MatMul lines strictly before line λ that involve
Ak(cid:62). Deﬁne C ∈ Rs×s, v ∈ Rs by

Then for all i ∈ [s],

Cij = (cid:104)fϕ(hi), fϕ(hj )(cid:105)
vi = (cid:104)fϕg(gi), fϕ(hm)(cid:105).

vi = α−1(cid:104)fϕ(hi), fϕh(gl)(cid:105)

fϕh(gl) = α(fϕ(hi))s

i=1C +v = α

(cid:88)

(C +)ijvjfϕ(hi)

i,j∈[s]

where α = limt→∞ n2(At)/n1(At) = αc2(A),c1(A).

62

Proof. Let I (cid:48) def= {i : line(ϕg(gi)) < l}.

We show vi = α−1(cid:104)fϕ(hi), fϕh(gl)(cid:105) for all i ∈ I (cid:48). Note ﬁrst of all that a in Defn F.2 is deﬁned by Defn 6.2 as αC +
where , CI(cid:48) = (Cij)i,j∈I(cid:48), and vI(cid:48) = (vi)i∈I(cid:48).

I(cid:48)vI(cid:48),

Suppose Z I def= {Z ϕg(gi)}i∈I is a maximal linearly independent set in Z I(cid:48) def= {Z ϕg(gi)}i∈I(cid:48). Note that Z I (as well as Z I(cid:48)
)
is also independent of all Z ˇg where ˇg is produced by MatMul involving a matrix that is not ϕ(A(cid:48)) or where ˇg is an input
var (by the construction of Kˇc). Let Z (cid:48) the collection of all such Z ˇg. Then EZJ fϕ(hm)(Z) is purely a function of Z I, by
expressing other elements of Z I(cid:48)
as linear combinations of Z I. Thus, by Lem F.1 applied to Z I and EZ(cid:48) fϕ(hm), there exist
coefﬁcients {aj}j∈I such that, for each i ∈ I,

Z ϕg(gi) E

fϕ(hm)(Z) = E
ZI

Z ϕg(gi) E
Z(cid:48)

fϕ(hm)(Z)

vi = E
ZI
(cid:88)

=

Z(cid:48)|ZI
ajKˇc(ϕg(gi), ϕg(gj))

j∈I
= (cid:104)Z ϕg(gi),

ajZ ϕg(gj )(cid:105)

(cid:88)

j∈I

=

(σk∞)2
α

(cid:104)fϕ(hi),

(cid:88)

j∈I

ajfϕ(hj )(cid:105)

by construction of K c.

This equality is extended to all i ∈ I (cid:48) via linear combination. Thus,

(vi)i∈I(cid:48) =

(σk∞)2
α

(cid:104)(fϕ(hi))i∈I(cid:48),

ajfϕ(hj )(cid:105)

(cid:88)

j∈I

fϕh(gl) = α

(cid:88)

i,j∈I(cid:48)

fϕ(hi)(C +

I(cid:48))ijvj

ajfϕ(hj )(cid:105)

(cid:88)

j∈I

= (σk∞)2 (cid:88)

fϕ(hi)(C +

I(cid:48))ij(cid:104)fϕ(hj ),

i,j∈I(cid:48)

= (σk∞)2ΠI(cid:48)

ajfϕ(hj )

(cid:88)

j∈I

= (σk∞)2 (cid:88)

ajfϕ(hj )

j∈I

where ΠI(cid:48) is the projection operator on L2(Z) that projects to the linear span of {fϕ(hi)}i∈I(cid:48) (see Defn E.1 and the basic
facts underneath).

So all along, vi = 1

α (cid:104)fϕ(hi), ϕh(gl)(cid:105) for all i ∈ I (cid:48).

We show vi = α−1(cid:104)fϕ(hi), fϕh(gl)(cid:105) for all i (cid:54)∈ I (cid:48). Suppose gi = A(cid:48)hi has line number greater than l. Then, we have that,
conditioned on Z I(cid:48)
, fϕ(hm) and fϕg(gi) are independent. Indeed, with the conditioning, the randomness in the former only
comes from {Z ϕg(gj )}j(cid:54)∈I(cid:48) and the randomness in the latter only comes from Z (cid:48), and the two are independent. Thus,

(cid:19) (cid:18)

(cid:18)

vi = E
ZI(cid:48)

(cid:16)

E
Z|ZI(cid:48)

E
Z|ZI(cid:48)

fϕg(gi)(Z)
I(cid:48)I(cid:48))+Z I(cid:48)(cid:17) (cid:18)
I(cid:48)I(cid:48))+((cid:104)Z ϕg(gi), fϕ(hm)(cid:105))i∈I(cid:48)

E
Z|ZI(cid:48)

iI(cid:48)(Kˇc

Kˇc

iI(cid:48)(Kˇc

= E
ZI(cid:48)
= Kˇc

fϕ(hm)(Z)

fϕ(hm)(Z)

(cid:19)

(cid:19)

63

where Kˇc is the row vector (Kˇc(ϕg(gi), ϕ(gj)))j∈I(cid:48) and Kˇc
construction of Kˇc, this simpliﬁes to

I(cid:48)I(cid:48) is the submatrix (Kˇc(ϕg(gi), ϕ(gj)))i,j∈I(cid:48). Again by the

Therefore,

vi =

(cid:88)

j,k∈I(cid:48)

(cid:104)fϕ(hi), fϕ(hj )(cid:105)(C +)jkvk

vi = (cid:104)fϕ(hi), α−1fϕh(gl)(cid:105)

(cid:88)

fϕ(hi)(C +)ijvj

i,j∈[s]

= α−1 (cid:88)

i,j∈[s]

fϕ(hi)(C +)ij(cid:104)fϕ(hj ), fϕh(gl)(cid:105)

= α−1Πfϕh(gl)
= α−1fϕh(gl).

where Π is the projection operator to the span of {fϕ(hj )}j∈[s], and the last equality follows because fϕh(gl) is already in
this span.

Theorem 6.3. Let π be an original syntax program with sampling instructions, and ˇπ be the detransposition of π, with ϕ
the mapping from vector vars of π to vector vars of ˇπ. Assume all fl of π are polynomially bounded, and that almost sure
rank convergence holds for ˇπ. Sample input vars of π according to Section 3.2. Then for any dimension constraints Λ, any
c ∈ C(π,Λ), and any polynomially bounded function φ : R¯c → R,

1
nct

nct
(cid:88)

i=1

φ(hct

i ) a.s.−−→ E φ((fϕ(h)(Z))h∈¯c)

is the sequence of the ith coordinates of all vector vars in ¯c, and Z ∼ N (µˇc, Kˇc).

where hct
i
If all fl in π are differentiable, then we can take a in Item 6 of Defn 6.2 to be ασ2(E ∂
σ = σr∞ and r is the line number of ϕ(A(cid:48)), and the above almost sure convergence will still hold.

Zϕg (gi) fϕ(hm)(Z))i∈[s]

15, where

Proof. We proceed by induction on line number of π. All line types are trivial except MatMul. So suppose in π, line l is
gl := Ahm, and the induction hypothesis holds for l and c = c(gl). Set c1 = c and c2 = c(hm). Let A(cid:48) be the A-var such
that A(cid:48) := A(cid:62) if A is an input A-var, and A := A(cid:48)(cid:62) otherwise.

Let gi := Ahi, i = 1, . . . , r, be all MatMul lines involving A that appear before line l, and let g(cid:48)i := A(cid:48)h(cid:48)i, i = 1, . . . , s,
be all MatMul lines involving A(cid:48) that appear before line l. Note that c(gi) = c(h(cid:48)i) = c1 and c(hi) = c(g(cid:48)i) = c2. Set
H t = [h1t| · · · |hrt] ∈ Rnc2t×r, and likewise for Gt ∈ Rnc1t×r, H (cid:48)t ∈ Rnc1t×s, G(cid:48)t ∈ Rnc2t×r. Let A be the σ-algebra
generated by all vector vars before gl.

As in the proof of Thm 5.1,

H (cid:48)t ˜AtΠ⊥
where ˜At is random matrix sampled iid as A, and µt = GtΣt+ωt + nc2t

glt d=At µt + Π⊥

H thmt
nc1 t H (cid:48)tΣ(cid:48)t+βt − nc2t

nc1t H (cid:48)tΣ(cid:48)t+ΥtΣt+ωt and

Σt def= H t(cid:62)H t/nc2t ∈ Rr×r
ωt def= H t(cid:62)hm/nc2t ∈ Rr

Σ(cid:48)t def= H (cid:48)t(cid:62)H (cid:48)t/nc1t ∈ Rs×s
βt def= G(cid:48)t(cid:62)hm/nc2t ∈ Rs.

Υt def= G(cid:48)t(cid:62)H t/nc2t ∈ Rs×r

15even if not all fl are differentiable, as long as the covariance {Kˇc(ϕg(gi), ϕg(gj))}i,j∈[s] is nonsingular, we may take the distribu-
tional derivatives and interpret the expectation as the application of this (tempered) distribution to the Gaussian density function. Then the
theorem holds under this interpretation. Even if the covariance is singular, we may consider a subset {ϕg(gi)}i∈I of maximal rank, and
still apply the tempered distribution interpretation; see the proof for more details.

64

By induction hypothesis, Σt, Σ(cid:48)t, Υt, ωt, βt all converge almost surely to corresponding limit values: Let α = αc2,c1 =
limt→∞

nc2t
nc1t . With Z ∼ N (µˇc, Kˇc),

Σt
ij
Σ(cid:48)t
ij
ωt
i

Υt
ij
βt
i

a.s.−−→ Σ∞
ij
a.s.−−→ Σ(cid:48)∞
ij
a.s.−−→ ω∞
i
a.s.−−→ Υ∞
i
a.s.−−→ β∞
i

def= E fϕ(hi)(Z)fϕ(hj )(Z)
def= E fϕ(h(cid:48) i)(Z)fϕ(h(cid:48) j )(Z)
def= E fϕ(hi)(Z)fϕ(hm)(Z)
def= E fϕ(g(cid:48) i)(Z)fϕ(hj )(Z)
def= E fϕ(g(cid:48) i)(Z)fϕ(hm)(Z).

As in the proof of Thm 5.1, we can apply Thm E.21 and Gaussian average smoothness to integrate out ˜At and obtain the
following claim

Claim F.3.1. With w ∼ N (0, 1),

1
nct

nct
(cid:88)

i=1

where

φ(glt

i , hc<lt

i

) − E
w

φ(Gt

i:a + H (cid:48)t

i:a(cid:48) + σ∞w, hc<lt

i

) a.s.−−→ 0

def= Σ∞+ω∞

a
a(cid:48) def= αΣ(cid:48)∞+(β∞ − Υ∞Σ∞+ω∞)
σ∞ def= Kˇc(ϕg(gl), ϕg(gl)) − Kˇc(ϕg(gl), ϕg(G))Kˇc|+

ϕg(G)Kˇc(ϕg(G), ϕg(gl))

with ˇc = c(ϕg(gl)).

Combining this with the induction hypothesis and Thm 4.3, we have
Claim F.3.2. With w ∼ N (0, 1) and Z ∼ N (µˇc, Kˇc),

1
nct

nct
(cid:88)

i=1

φ(glt

i , hc<lt

i

) − E
w,Z

φ





r
(cid:88)

j=1

ajfϕ(gj )(Z) +

s
(cid:88)

j(cid:48)=1

j(cid:48)fϕ(h(cid:48) j(cid:48)
a(cid:48)

)(Z) + σ∞w, {fϕ(h)(Z)}h∈(¯c)<l



a.s.−−→ 0



In the following, we consider the inner product space of L2-integrable functions of Z (equivalently, square-integrable
random variables in the σ-algebra generated by Z), with inner product (cid:104)f, g(cid:105) def= E f (Z)g(Z) with Z ∼ N (µˇc, Kˇc). We
abuse notation and let any vector-var in ˇπ (e.g. ˇhm) denote the corresponding function (e.g. fˇhm
). Note that we can rewrite

β∞ − Υ∞Σ∞+ω∞

(cid:104)ϕ(g(cid:48)i), ϕ(hm)(cid:105) −

=

(cid:88)

(cid:104)ϕ(g(cid:48)i), ϕ(hj)(cid:105)(Σ∞+)jk(cid:104)ϕ(hk), ϕ(hm)(cid:105)

j,k
ϕ(H)ϕ(g(cid:48)i), ϕ(hm)(cid:105))i∈s = ((cid:104)ϕ(g(cid:48)i), Π⊥

= ((cid:104)Π⊥

ϕ(H)ϕ(hm)(cid:105))i∈s





i∈s

where Πϕ(H) is the projection operator to the span of ϕ(H) def= {ϕ(hi)}r
Claim F.3.3.

i=1 and Π⊥

ϕ(H) is its orthogonal complement.

r
(cid:88)

j=1

ajϕh(gj) = α

s
(cid:88)

i,j=1

ϕ(h(cid:48)i)(Σ(cid:48)∞+)ij(cid:104)Πϕ(H)ϕg(g(cid:48)j), ϕ(hm)(cid:105).

65

Proof:

By Lem F.3,

r
(cid:88)

j=1

as desired.

Claim F.3.4.

ajϕh(gj) = α

r
(cid:88)

(Σ∞+)jq(cid:104)ϕ(hq), ϕ(hm)(cid:105)

(cid:88)

a,b∈[s]

ϕ(h(cid:48)a)(Σ(cid:48)∞+)ab(cid:104)ϕg(g(cid:48)b), ϕ(hj)(cid:105)

j,q=1
(cid:88)

(cid:88)

= α

ϕ(h(cid:48)a)(Σ(cid:48)∞+)ab(cid:104)ϕg(g(cid:48)b), ϕ(hj)(cid:105)(Σ∞+)jq(cid:104)ϕ(hq), ϕ(hm)(cid:105)

a,b∈[s]
(cid:88)

j,q∈[r]
ϕ(h(cid:48)a)(Σ(cid:48)∞+)ab(cid:104)ϕg(g(cid:48)b), Πϕ(H)ϕ(hm)(cid:105)

= α

a,b∈[s]

r
(cid:88)

j=1

ajϕh(gj) +

s
(cid:88)

j(cid:48)=1

j(cid:48)ϕ(h(cid:48)j(cid:48)
a(cid:48)

) = α

s
(cid:88)

i,j=1

ϕ(h(cid:48)i)(Σ(cid:48)∞+)ij(cid:104)ϕg(g(cid:48)j), ϕ(hm)(cid:105).

Proof: As noted above,

a(cid:48)
j(cid:48) = α

(cid:88)

(Σ(cid:48)∞+)j(cid:48)k(β∞ − Υ∞Σ∞+ω∞)k

k∈[s]
(cid:88)

(Σ(cid:48)∞+)j(cid:48)k(cid:104)Π⊥

ϕ(H)ϕ(g(cid:48)i), ϕ(hm)(cid:105)

k∈[s]
(cid:88)

(Σ(cid:48)∞+)j(cid:48)k(cid:104)Π⊥

ϕ(H)ϕg(g(cid:48)i), ϕ(hm)(cid:105)

= α

= α

k∈[s]

because ϕ(g(cid:48)i) − ϕg(g(cid:48)i) ∈ span ϕ(H)

s
(cid:88)

j(cid:48)=1

j(cid:48)ϕ(h(cid:48)j(cid:48)
a(cid:48)

) = α

(cid:88)

ϕ(h(cid:48)j(cid:48)

)(Σ(cid:48)∞+)j(cid:48)k(cid:104)Π⊥

ϕ(H)ϕg(g(cid:48)i), ϕ(hm)(cid:105).

j(cid:48),k∈[s]

By the previous claim, adding (cid:80)r

j=1 ajϕh(gj) cancels Π⊥

ϕ(H) and gives the desired result.

(cid:4)

(cid:4)

Therefore,

r
(cid:88)

j=1

ajϕ(gj) +

s
(cid:88)

j(cid:48)=1

j(cid:48)ϕ(h(cid:48)j(cid:48)
a(cid:48)

) + σ∞w

= (σ∞w +

r
(cid:88)

j=1

ajϕg(gj)) + α

s
(cid:88)

i,j=1

d=H<l ϕg(gl) + ϕh(gl) = ϕ(gl)

ϕ(h(cid:48)i)(Σ(cid:48)∞+)ij(cid:104)ϕg(g(cid:48)j), ϕ(hm)(cid:105)

where H<l is the σ-algebra generated by {fϕ(h)(Z)}h∈(¯c)<l, and (¯c)<l is the collection of all vector vars in ¯c with line
number < l. So we can complete our induction by stating

1
nct

nct
(cid:88)

i=1

φ(glt

i , hc<lt

i

) − E
w,Z

φ(fϕ(gl)(Z), {fϕ(h)(Z)}h∈(¯c)<l ) a.s.−−→ 0.

—————-

66

For the second claim, let Cij = (cid:104)ϕ(h(cid:48)i), ϕ(h(cid:48)j)(cid:105) for all i, j ∈ [s]. We can compute, as in the proof of Lems F.1 and F.3,

ϕh(gl) = α

= α

(cid:88)

i,j∈[s]
(cid:88)

ϕ(h(cid:48)i)(C +)ij(cid:104)ϕg(g(cid:48)j), hm(cid:105)

ϕ(h(cid:48)i)(C +)ijKˇc(ϕg(g(cid:48)j), ϕg(g(cid:48)k)) E ∂

Zϕg (g(cid:48)k )fhm

(Z)

i,j,k∈[s]

(cid:88)

= α

ϕ(h(cid:48)i)(C +)ijσ2(cid:104)ϕ(h(cid:48)j), ϕ(h(cid:48)k)(cid:105) E ∂

by Stein’s Lemma Lem E.8
Zϕg (g(cid:48)k )fhm

(Z)

i,j,k∈[s]
= ασ2Πϕ(H)

(cid:88)

ϕ(h(cid:48)k) E ∂

Zϕg(g(cid:48) k ) fhm

(Z)

k∈[s]
ϕ(h(cid:48)k) E ∂

Zϕg(g(cid:48) k ) fhm

(Z)

= ασ2 (cid:88)

k∈[s]

where σ = σr∞ and r = line(ϕ(A(cid:48))). This computation goes through as long as fhm
is differentiable, which is implied by
all fl in π being differentiable, or if the covariance Kˇc(ϕg(G(cid:48)), ϕg(G(cid:48))) is nondegenerate (which allows us to consider fhm
,
a polynomially bounded function, as a tempered distribution, whose derivatives are also tempered distributions, giving a
valid interpretation to the expectation).
When Kˇc(ϕg(G(cid:48)), ϕg(G(cid:48))) is singular, let I be a minimal subset of [s] such that {ϕ(h(cid:48)i)}i∈I is linearly independent. We
can compute similarly,

ϕh(gl) = α

= α

(cid:88)

i,j∈I
(cid:88)

ϕ(h(cid:48)i)(C +

I )ij(cid:104)ϕg(g(cid:48)j), hm(cid:105)

ϕ(h(cid:48)i)(C +

I )ijKˇc(ϕg(g(cid:48)j), ϕg(g(cid:48)k)) E ∂

Zϕg (g(cid:48)k )fhm

I (Z)

i,j,k∈I

(cid:88)

= α

ϕ(h(cid:48)i)(C +

I )ijσ2(cid:104)ϕ(h(cid:48)j), ϕ(h(cid:48)k)(cid:105) E ∂

by Stein’s Lemma Lem E.8
Zϕg (g(cid:48)k )fhm

I (Z)

i,j,k∈I
= ασ2Πϕ(H)

(cid:88)

ϕ(h(cid:48)k) E ∂

Zϕg(g(cid:48) k ) fhm

I (Z)

k∈I
ϕ(h(cid:48)k) E ∂

Zϕg(g(cid:48) k ) fhm

I (Z)

= ασ2 (cid:88)

k∈I

where cI is the restriction of C to I, and fhm
{Z ϕg(g(cid:48) i)}i∈I. Then this computation goes through always, since {Z ϕg(g(cid:48) i)}i∈I has a density.

I is the version of fhm

that expands Z ϕg(g(cid:48) j ), j (cid:54)∈ I to linear combinations of

67

