Tensor Processing Units for Financial Monte Carlo

Francois Belletti, Davis King, Kun Yang, Roland Nelet, Yusef Shaﬁ, Yi-Fan Chen, John Anderson

Google Research
Mountain View CA
USA
belletti@google.com

0
2
0
2

n
a
J

7
2

]

C
D
.
s
c
[

5
v
8
1
8
2
0
.
6
0
9
1
:
v
i
X
r
a

Abstract—Monte Carlo methods are critical to many routines
in quantitative ﬁnance such as derivatives pricing, hedging and
risk metrics. Unfortunately, Monte Carlo methods are very
computationally expensive when it comes to running simulations
in high-dimensional state spaces where they are still a method
of choice in the ﬁnancial industry. Recently, Tensor Processing
Units (TPUs) have provided considerable speedups and decreased
the cost of running Stochastic Gradient Descent (SGD) in Deep
Learning. After highlighting computational similarities between
training neural networks with SGD and simulating stochastic
processes, we ask in the present paper whether TPUs are
accurate, fast and simple enough to use for ﬁnancial Monte
Carlo. Through a theoretical reminder of the key properties of
such methods and thorough empirical experiments we examine
the ﬁtness of TPUs for option pricing, hedging and risk metrics
computation. In particular we demonstrate that, in spite of the
use of mixed precision, TPUs still provide accurate estimators
which are fast to compute when compared to GPUs. We also show
that the Tensorﬂow programming model for TPUs is elegant,
expressive and simpliﬁes automated differentiation.

Index Terms—Financial Monte Carlo, Simulation, Tensor Pro-

cessing Unit, Hardware Accelerators, TPU, GPU

I. INTRODUCTION

The machine learning community has developed several
technologies to speed up Stochastic Gradient Descent algo-
rithms for Deep Learning [16], including new programming
paradigms, special-purpose hardware, and linear-algebra com-
putation frameworks. This paper demonstrates that the same
techniques can accelerate Monte Carlo integration of stochastic
processes for ﬁnancial applications.

A. Monte Carlo estimation in ﬁnance and insurance

A key problem when pricing a ﬁnancial instrument — for
insurance or speculation — is to estimate an average outcome
deﬁned by a probability space (Ω, F, P): EP [f (ω)], where E
denotes the expectation. In the following we ﬁrst provide a
basic introduction to derivatives pricing. Namely, we focus
on estimating expectations with the Monte Carlo method in
cases where random ﬂuctuations are generated by stochastic
processes. We describe how hardware accelerators compute
such estimators faster through parallelization.

1) Stochastic processes in continuous time: Stochastic
processes remain the main abstraction employed to model
ﬁnancial asset prices. Consider a ﬁltered probability space
(Ω, F, F, P) (where F = {Ft} is the corresponding canonical

ﬁltration) supporting a q dimensional Brownian motion W and
the Stochastic Differential Equation (SDE)

dXt = µ(t, Xt)dt + σ(t, Xt)dWt, t ∈ [0, T ] .

(1)

The drift (µ) and volatility (σ) functions take values re-
spectively in Rp and Rp,q. By deﬁnition, a strong solution
(X) to Eq. (1) is a process taking values in Rp such that
(cid:1) ds is almost surely ﬁnite
(cid:82) T
s=0
and

(cid:0)||b(s, Xs)||2 + ||σ(s, Xs)||2

2

Xt = X0 +

(cid:90) t

0

b (s, Xs) ds +

(cid:90) t

0

σ (s, Xs) dWs, t ∈ [0, T ] .

Assuming that X0 is a random variable with ﬁnite variance
independent of W , and that ||b(·, 0)||2 and ||σ(·, 0)||2 are
square integrable (as functions of t), the existence of a ﬁnite
Lipschitz constant K such that

||b(t, x) − b(t, y)||2 + ||σ(t, x) − σ(t, y)||2 ≤ K||x − y||2

for all x, y ∈ Rp and t ∈ [0, T ] guarantees the existence of
such a strong solution on [0, T ]. More modern models typically
introduce jumps or stochastic volatility which adds realism to
the corresponding simulations but does not radically change
the underlying computation patterns.

2) Monte Carlo methods in ﬁnance and insurance: Monte-
Carlo methods rely on simulation and numerical integration
to estimate EP[f (XT )] or EQ[f (XT )] under the historical or
risk-neutral probability (P and Q respectively) [15]. Some
contracts deﬁning ﬁnancial derivatives may specify a path
dependent outcome—such as Barrier or Asian options—in
which case the theory of Black, Scholes and Merton still
leads us to estimate EP[f (X0:T )] or EQ[f (X0:T )] where X0:T
denotes the observation of the process X on the interval [0, T ].
In general, we thus seek an estimator for an expectation of the
type

E[f (X0:T )] where (Xt) solves (1) on [0, T ].

(2)

Monte-Carlo methods rely on numerical discretization and in-
tegration to produce an estimator for (2) in the form of an em-
(cid:111)
:
pirical mean over simulated trajectories

0,T |i = 1 . . . N

(cid:101)X n

(cid:110)

(cid:98)IN =

1
N

N
(cid:88)

n=1

(cid:16)

f

(cid:101)X n
0,T

(cid:17)

.

(3)

 
 
 
 
 
 
In general, because the dynamics of (Xt) are speciﬁed in
continuous time with real values, a computer-based simulation
will suffer from bias coming from the limited precision in
numerical representations and more importantly the temporal
discretization. The variance of the the Monte Carlo estimator
typically if it costs O(N ) samples to
is also a problem:
produce a result with a conﬁdence interval of size 1 then
reducing the interval’s size to (cid:15) comes at a cost of O( N
(cid:15)2 )
simulations. Such a rate of convergence can be accelerated
thanks to Quasi-Random Monte Carlo methods [18], [19],
[30], [34] which then enable a near linear rate of conver-
gence. Unfortunately, the computation time generally scales as
O(q2) (i.e. quadratically in space) when correlations between
different components of (Xt) are taken into account. For
these reasons, Monte Carlo methods still constitute some of
the most computationally intensive tasks running routinely at
scale across the ﬁnancial industry. Accelerating Monte Carlo
estimation and making it more cost effective has been a long
standing challenge for derivative pricing, hedging and risk
assessment.

3) Greeks and sensitivity analysis: Monte Carlo methods
in quantitative ﬁnance are also used to estimate sensitivi-
ties of derivatives’ prices with respect to model parameters
and the current state of the market. Sensitivities to market
parameters—the ﬁnancial “Greeks” [15]—are used not only to
quantify risk, but also construct hedges and synthetic replicat-
ing portfolios [15], [30]. Automated differentiation of Monte
Carlo pricers (also known as AAD) is now a solution of choice
in quantitative ﬁnance as it is more computationally efﬁcient
than methods such as bumping to compute sensitivities with re-
spect to many different inputs and parameters [33]. Tensorﬂow
was designed with automated differentiation at its very core
as this technique—often referred to as “Back-Propagation”—
is of key importance for training machine learning models by
Stochastic Gradient descent [2]. Moreover, Tensorﬂow readily
offers the opportunity to accelerate simulations and automated
differentiation without requiring any additional code by en-
abling researchers to leverage modern hardware such as GPUs
and TPUs.

B. Contributions

In the present paper we focus on leveraging Tensor Process-
ing Units (TPUs) with Tensorﬂow for ﬁnancial Monte Carlo
methods. We aim to show that although such accelerators were
designed primarily to accelerate the training of Deep Learning
models by Stochastic Gradient Descent, TPUs provide cutting
edge performance for Monte Carlo methods involving dis-
cretized multi-variate stochastic processes. In particular, we
present the following contributions:

• We demonstrate that, in spite of the limited numerical
precision employed natively in matrix multiplications on
TPUs, accurate estimates can be obtained in a variety of
applications of Monte Carlo methods that are sensitive to
numerical precision.

• We benchmark the speed of TPUs and compare them to
GPUs which constitute the main source of acceleration

for general purpose Monte Carlo methods outside of Field
Programmable Gate Arrays (FPGAs) and Application-
speciﬁc Integrated Circuits (ASICs) based solutions.
• We show that Tensorﬂow [2] constitutes a high level,
ﬂexible and simple interface that can be used to lever-
age the computational power of TPUs, while supporting
automated differentiation.

The present paper demonstrates that Tensorﬂow constitutes a
ﬂexible programming API which enables the implementation
of different simulation routines while providing substantial
beneﬁts by running computations in the Cloud on TPUs. A
key consequence is that experiences that used to be iterative
for developers now become interactive and inherently scalable
without requiring investing in any software or hardware. We
believe such improvements can make ﬁnancial risk manage-
ment more cost-effective, ﬂexible and reactive.

II. RELATED WORK

A. Pricing techniques and typical computational workloads

As a ﬁrst step towards acceleration, we now examine three
typical computational workloads routinely employed to price
derivatives and assess risk in quantitative ﬁnance.

1) SIMD element-wise scalar ops in Euler-Maruyama dis-
cretization schemes: A ﬁrst characteristic computational work-
load is associated with mono-variate geometric Brownian
models and their extensions in the form of local [11], [17]
or stochastic volatility models [4], [12]. The Euler-Maruyama
scheme discretizes SDE (1) explicitly forward in time. Con-
sider the simulation of N independent trajectories,

(cid:101)X n

ti+1

= (cid:101)X n
ti

+ µ

(cid:16)

ti, (cid:101)X n
ti

(cid:17)

∆ti + σ

(cid:16)

ti, (cid:101)X n
ti

(cid:17) (cid:112)∆tiZ n

i+1

= X0 ∈ R, ∆ti = ti+1−ti, Z n

for n = 1, . . . , N where (cid:101)X n
t0
i+1
are Pseudo-Random Numbers distributed following N (0, 1).
Simulations then reduce to scalar add/multiplies which are
independent across simulated scenarios and is therefore em-
barrassingly parallel as a clear example of a Single Instruction
Multiple Data (SIMD) setting where the different elements of
the data undergo independent computations. Such simulations
are trivial to parallelize across simulated scenarios provided
Pseudo or Quasi-Random Numbers (PRNs and QRNs respec-
tively) can be correctly generated in parallel [8], [18], [19],
[26], [32], [34], [35]. An averaging reduction across samples
concludes the task.

2) Matrix-multiply ops in Multi-variate simulations of cor-
related processes: The Euler-Maruyama discretization scheme
for scalar stochastic processes naturally extends to the multi-
variate setting where each stochastic process takes values
∈ Rp. However, a major computational difference
in (cid:101)X n
ti
arises. If the underlying Brownian motion is in Rq, each
simulated time-step in each scenario will require calculat-
i+1 ∼ N (0, Iq) and
ing
(cid:16)
∈ Rp,q, which implies that a p × q matrix/vector
σ
product has to be computed. If N scenarios are stacked

ti+1 − tiσ
(cid:17)

i+1 with Z n

(cid:16)
ti, (cid:101)X n
ti

ti, (cid:101)X n
ti

Z n

√

(cid:17)

together to beneﬁt from the corresponding hardware accel-
eration, the operation becomes a p × q, q × N matrix/matrix
products. Here as well, a ﬁnal reduction averages the simulated
outcomes.

3) Chained linear system inversions in the Longstaff-
Schwartz Method (LSM) for value estimation: The regression-
and
based estimation method proposed by Longstaff
Schwartz [25] to price American Options has become a
standard pricing method for callable ﬁnancial
instruments
(e.g., American or Bermuda options) with high dimensional
underlyings (e.g., call on a maximum or weighted combination
of stocks). In the setting of callable options, which can be
exercised at multiple points in time before their maturity, the
pricing problem is solved using a Monte-Carlo method to
simulate future trajectories as well as a dynamic programming
approach to back-track optimal decisions in time. To enable
dynamic programming, for each decision instant ti, a classic
approach is to estimate a Value Function on the state of the
underlying:

Xti (cid:55)→ Vti (Xti) = max (cid:0)f (Xti), E(Vti+1|Xti)(cid:1)

(cid:101)X n
ti
(cid:111)

with the convention that VT = f (XT ) where f is the option’s
payoff function. As the conditional expectation E(Vti+1|Xti)
is the closest square integrable random variable to Xti (ac-
cording to the L2 norm) LSM ﬁts a model to interpolate
E(Vti+1|Xti) between values of Xti that have actually been
simulated. LSM employs a linear-regression on a set of K
features derived from the the simulated values of Xti such
as (1, Xti , X 2
, . . . ) or a ﬁnite number of Hermite polyno-
ti
the simulated values [15], [25]. Given
mials evaluated at
|n = 1 . . . N
the set of
a set of simulated values

(cid:111)
,

(cid:110)

values

(cid:16)

(cid:110)

V

(cid:17)

(cid:101)X n
ti
(cid:16)

(cid:110)

|n = 1 . . . N

is projected onto the set of

(cid:17)

(cid:16)

(cid:17)

(cid:111)

(cid:101)X n
ti

(cid:101)X n
ti

ψ1

, . . . , ψK

, |n = 1 . . . N

regressors
where
(ψk)k=1...K are featurizing functions (e.g., Hermite polyno-
mials). Therefore, a linear regression of N scalar observations
is needed, for each candidate time-step to exercise the option,
onto N vectors of K dimensions. Typically, for efﬁciency, a
Cholesky decomposition of the Grammian will be computed
to effectively solve the linear regression. This computational
cost adds to the cost of simulating the original paths for the
trajectory of the underlying asset(s) which may themselves
be correlated. The overall procedure yields a price estimate
the ﬁrst exercise time:
as the expected value function at
(cid:16)
(cid:98)IN = 1
. Therefore, an averaging reduction
N
also concludes LSM.

n=1 V

(cid:101)X n
t0

(cid:80)N

(cid:17)

B. Pre-existing hardware acceleration strategies

Having reviewed a few algorithms that are core to ﬁnancial
Monte Carlo, we now give an overview of hardware-based
techniques to reduce their running time. A ﬁrst approach to
accelerate Monte Carlo methods had consisted in running
them on High Performance Computing (HPC) CPU grids with
parallelization paradigms such as Message Passing Interface

(MPI). We focus here on device-level acceleration with hard-
ware accelerators that can be used as elements of a distributed
compute grid if needed.

1) GPUs: The rise of general purpose high level APIs for
scientiﬁc calculations on GPUs with CUDA or OpenCL has
prompted a wide development of GPU-based approaches to
accelerate Monte Carlo methods. Pricing and estimating risk
metrics in ﬁnance are especially well-suited to acceleration
by GPUs, due to the embarrassingly parallel nature of Monte
Carlo Methods, and to their use of computationally intensive
linear algebra routines. Generating PRNs and QRNs in parallel
correctly [8], [18], [26], [32], [35], coupled with algorith-
mic re-factorization to fully utilize GPUs have enabled large
speedups with respect to CPUs for pricing [3], [24], [27], [31],
risk metrics [9] and sensitivity analysis [10].

2) FPGAs: Many works have demonstrated that Field Pro-
grammable Gate Arrays (FPGAs) provide substantial speedups
with respect to GPU implementations and reduce energy costs
in servers. While some methods have employed FPGAs as
standalone solutions [36], [37], other approaches have used
a mixed precision approach relying on both a CPU and
an FPGA [5], [6]. In particular, Multi-Level Monte Carlo
(MLMC) [14] can be applied to FPGAs computing low reso-
lution fast simulations paired with CPUs running at reference
precision.

III. TENSOR PROCESSING UNITS

A Tensor Processing Unit (“Cloud TPU” or “TPU”)—
a custom-developed application-speciﬁc integrated circuit
(ASIC) specialized for deep learning—offers 420 × 1012
ﬂoating-point operations per second (FLOPS) and 128GB of
high bandwidth memory (HBM) in its latest release. The TPU
architecture is abstracted behind the Tensorﬂow framework.
High-level Tensorﬂow programs, written without using de-
tailed knowledge of TPUs, can be deployed on TPU hardware
in the cloud to train or serve deep neural networks. [20]
reports impressive acceleration of training and low-latency
online prediction.

Although TPU targets deep learning, it is ﬂexible enough
to address computational challenges in various ﬁelds. In the
present paper, we particularize its application to ﬁnancial
Monte Carlo.

A. TPU System Architecture

One TPU is comprised of four independent chips. Each chip
consists of two compute cores called Tensor Cores. A Tensor
Core, as shown in Fig. 1 consists of scalar, vector and matrix
units (MXU). In addition, 16 GB of on-chip High Bandwidth
Memory (HBM) is associated with each Tensor Core for Cloud
TPU v3 — its latest generation. Communication between
Tensor Cores occurs through high-bandwidth interconnects.
All computing units in each Tensor Core are optimized to
perform vectorized operations. In fact, the main horsepower of
a TPU is provided by the MXU which is capable of performing
128 × 128 multiply-accumulate operations in each cycle [7].
While its inputs and outputs are 32-bit ﬂoating point values,

the MXU typically performs multiplications at the reduced
precision of bﬂoat16 — a 16-bit ﬂoating point representation
that provides better training and model accuracy than the IEEE
half-precision representation for deep learning as it allocates
more bits to the exponent and less to the mantissa.

B. Programming Model

Programming for TPUs is generally done through high-
level Tensorﬂow API. When the program is run, a Tensor-
Flow computation graph is generated and sent to the Cloud
TPU over gRPC [1]. The Cloud TPU server compiles the
in time, partitions the graph into
computation graph just
portions that can run on a Cloud TPU and those that must run
on a CPU and generates Accelerated Linear Algebra (XLA)
operations corresponding to the sub-graph that is to run on
Cloud TPU. Next, the XLA compiler takes over and converts
High Level Optimizer (HLO) operations that are produced by
the TensorFlow server to binary code that can be run on Cloud
TPU, including orchestration of data from on-chip memory
to hardware execution units and inter-chip communication.
Finally, the binary is sent to the Cloud TPU for execution.

Fig. 1: Hardware architecture and programming model of
Tensor Processing Units (TPUs) [1].

C. From DNN inference to stochastic process simulation

In the present paper, we leverage the computational similar-
ity between DNN inference and simulating high dimensional
ﬁnancial portfolios. Tensorﬂow and TPUs have been designed
to provide a high level interface for deep learning program-
ming to train mission critical models rapidly. Our proposal is
to leverage such performance for a different purpose: running
stochastic process simulation for ﬁnancial applications.

Computing the output of a DNN implies chaining ma-
trix/matrix multiplies interleaved with element-wise vector-
izable operations. Very similar computational patterns are
involved in quantitative ﬁnance as multi-dimensional simu-
lations require the computation of matrix/matrix multiples

Fig. 2: Computational similarity between a DNN layer and a
time step of a multi-dimensional Geometric Brownian Monte
Carlo.

(with batched PRNs) and element-wise vectorizable opera-
tions. Figure 2 illustrates this high computational similarity.
Furthermore, as training DNNs requires computing a gradient
through automated differentiation between the outputs and all
the parameters of the network, a DNN learning framework
such as Tensorﬂow is ideal to enable sensitivity estimation
through path-wise differentiation [33].

IV. PSEUDO AND QUASI RANDOM NUMBER
GENERATORS ON TPU

We describe how Pseudo-Random Number Generators
(PRNGs) and Quasi-Random Number Generators (QRNGs)
run on TPUs.

A. PRNGs designed for parallelization and statistical sound-
ness

For ﬁnancial Monte-Carlo, the Mersenne Twister 19337
(MT19337) [28] constitutes a very popular approach and
yields a sequence of PRNs with period 219337. Such a PRN
sequence is produced as a single stream and therefore par-
allelization relies on sub-stream approaches [30], [33] which
typically requires deciding ahead of time how many PRNs will
be consumed by each core through the simulation.

A more ﬂexible approach for distributed and parallel com-
puting is to use multi-stream random number generation as
in [32]. Multi-stream algorithms, such as Threefry and Philox,
use cryptographic methods to generate multiple sequences
(xk
n)n=1...N,k=1...K guaranteed to be seemingly statistically
independent if each core employs a distinct key. The number
of admissible keys is typically 264 if 64 bit integers are used to
encode keys and the period of each sequence is 264 if 64 bits
are used to represent the sequence iterator. [32] shows that
these methods have higher throughput and better statistical
properties when submitted to Big-Crunch [23] than standard
generators. In particular, while Threefry and Philox pass all
the tests in the battery of Big-Crunch, MT19937 fails most
of them. We have conﬁrmed that the generation of uniformly
distributed 32 bit integers on TPU successfully passes Big-
Crunch [23] (the double precision version of Big-Crunch is not

Parameter matrixBatched vectors of inputsxMatrix/matrix multiplyBatched vectors of outputsVectorized add and non-linear element-wise transformBias vector+One layer of a Deep Neural NetworkVolatility matrixBatched vectors of random normal noisexMatrix/matrix multiplyVectorized add and element-wise multiply with previous stateTrend term+Single time step of multi-variate Geometric Brownian motion simulationBatched vectors of outputsdirectly relevant to TPUs which generate uniformly distributed
ﬂoats in [0, 1) in single precision only). For this reason and
because of the ease of parallelization, Tensorﬂow employs
keyed multi-stream PRNGs. In particular, we employ Threefry
in our TPU experiments and Philox on GPU.

B. QRNGs and Sobol sequence on TPU

B. Numerical precision, discretization bias and variance in
risk metrics

Financial Monte-Carlo methods are typically concerned
with the simulation of a continuous time SDE such as Equation
(1). Analyzing the convergence of the Monte Carlo estimator
(cid:98)IN in Equation (3) hinges upon the well known bias-variance
decomposition of the L2 error [30]:

While PRNGs aim at generating iid samples, QRNGs
purposely generate non-sequentially-independent samples to
cover the [0, 1)q hyper-cube as uniformly as possible and ac-
celerate numerical integration [15], [30]. QRNGs are preferred
to numerically estimate expectations long as the sampled space
is not problematically high dimensional. Indeed, while Monte
)
Carlo integration provides a rate of convergence of O(

(N )
(where N is the number of samples), low discrepancy se-
quences (e.g. the Sobol sequence) outputted by QRNGs can
decrease the mean square error at a speed if O( ln(N )q−1
) [21].
The main shortcoming of low discrepancy sequences is the loss
of conﬁdence intervals estimates. However, randomized Quasi
Monte Carlo provides conﬁdence interval estimates through
the use of scrambling or other randomization techniques [15],
[22], [29], [30].

1√

N

In the present article, as we want to examine numerical
bias with respect to temporal discretization bias and simulation
variance, we present our ﬁrst set of results based on classic
Monte Carlo simulations. We demonstrate empirically in VI-D
that QMC can run on TPU with no signiﬁcant overhead as
compared to Monte Carlo while beneﬁting from a near linear
rate of convergence (in terms of number of samples).

V. NUMERICAL PRECISION ON TPU AND MULTI-LEVEL
MONTE CARLO

Running dynamical system simulations,

in particular in
ﬁnance, often relies on high numerical precision to produce
faithful results. We now describe the numerical precision of
TPUs.

A. Single and bﬂoat16 precision on TPU

As opposed to today’s CPUs or GPUs, TPUs do not offer
double (64 bit) precision for ﬂoating point representations. The
default representation is single (32 bit) precision and scalar
multiplications in the MXU for matrix multiplies are computed
in bﬂoat16 (16 bit) precision prior to being accumulated in
single precision. As 16 bits are quite few to represent numbers
for ML applications and chained multiplications in general,
a non standard representation scheme has been employed to
better capture high magnitude values and prevent overﬂow.

The single precision IEEE ﬂoating point standard allocates
bits as follows: 1 sign bit, 8 exponent bits and 23 mantissa bits.
The IEEE half precision standard uses 1 sign bit, 5 exponent
bits and 10 mantissa bits. In contrast, the bﬂoat16 format
employs 1 sign bit, 8 exponent bits and 7 mantissa bits.

We found in our numerical experiments that both the
single precision — used in accumulators and vector units for
element-wise operations — and the bﬂoat16 precision did not
yield signiﬁcantly different results as compared to the double
precision in the context of ﬁnancial Monte Carlo.

||E [f (X0:T )] − (cid:98)IN ||2

2 =

(cid:16)

E [f (X0:T )] − E

(cid:105)(cid:17)2

(cid:104)

(cid:98)IN

+

(cid:17)

(cid:16)

(cid:98)IN

Var

N

.

The bias term typically conﬂates the biases induced by
temporal discretization and ﬂoating point numerical represen-
tation.

When simulating a SDE, a temporal discretization occurs
induces most of the bias affecting the Monte Carlo
that
simulation. Indeed, as opposed to the actual process of interest
(Xt)t∈[0,T ], the Monte Carlo simulation typically employs a
piece-wise continuous approximation ( (cid:101)Xt)t∈[0,T ] for which
only H values are computed (if H is the number of temporal
discretization steps) every ∆t (where ∆t
is the temporal
discretization step). Under several assumptions which are typi-
cally true for ﬁnancial pricing, the Taley-Tubaro theorem [30]
states that the discretization bias reduces as O( 1
H ), that is
inversely to the number of steps employed when discretizing
the process. Such a bias term dominates in practice as we
will demonstrate numerically. In comparison, the numerical
precision induced bias is negligible. Finally, as in [5], [6],
the MLMC method [13], [14] can efﬁciently estimate the
numerical bias term if it is signiﬁcant and compensate for it.
In particular, MLMC helps low precision fast implementations
accelerate reference precision Monte Carlo by running them
as a companion estimator whose bias can be identiﬁed with a
few samples and is practically employed to lower the variance
of the Monte Carlo estimator by generating many samples
quickly.

VI. NUMERICAL EXPERIMENTS

We now demonstrate that TPUs are fast instruments whose
numerical precision is commensurate with the needs of ﬁnan-
cial Monte Carlo based risk metrics. In the following, we use
the Philox on GPU and Threefry on TPU (where it is more
challenging to support certain low-level operations leveraged
by Philox). Also, we use the same python Tensorﬂow code
on GPU and TPU for two reasons: we want to make our
comparison with a library that has been highly optimized for
speed and we want to make comparisons of speed at equal
level of software engineering effort. Here NVidia v100 GPUs
are used to anchor our results, but no architecture speciﬁc opti-
mizations have been done, which could considerably improve
their performance. Therefore, the GPU wall times we give can
only be considered a solid reference but not an indicator of the
peak performance one can obtain on the NVidia V100 GPU.
In the following, we do not take into account compilation
times when measuring wall time on TPU as the corresponding
overhead is obviously amortized in most applications that are

latency sensitive, and is not a hindrance for an interactive
experience in a Colaboratory notebook (interactive notebooks
comparable to ipython Jupyter notebooks).

A. European option pricing and hedging

Our ﬁrst experiments are concerned with European option
pricing, i.e. non-callable derivatives with a single terminal
payoff.

1) Uni-variate process simulation to benchmark TPUs’
VPU: Uni-variate stochastic process simulations do not need
matrix multiplies and therefore constitute helpful benchmarks
to speciﬁcally assess the performance of the Vector Processing
Unit (VPU) on TPUs, which performs single (32-bit) ﬂoating
point arithmetic.
Vanilla Call: First we start with the extremely simple example
of pricing a 1 year maturity European Call option (strike at
120) under the standard Black-Scholes model with constant
drift (0.05) and volatility (0.2) with an initial underlying
price of 100. The analytic price of the option in double
precision is 3.24747741656. What we intend to show here
— as we know the actual option price analytically — is that
temporal discretization bias largely dominates over numerical
precision bias. Each of the 100 simulation runs has 25 to
100 discretization steps and 10M samples. One can verify in
Figure 3 that TPUs provide a comfortable speed-up compared
to GPUs running the same Tensorﬂow graph, and that the bias
induced by the use of lower precision is negligible compared
to that induced by the temporal discretization.
Path dependent exotic Put: We make things slightly more
complex and consider pricing an In and Out Put (activating
Barrier). The initial price of the underlying is 100, the strike
is still constant (0.03) as
120,
well as the volatility (0.8). The analytic price of the option
(given by the symmetry principle under the Black-Scholes
model) in double precision is 23.1371783926. Between each
two discretization steps with values x and y we simulate the
maximum of the Brownian bridge — classically [15], [30] with

the barrier 140. The drift

(cid:18)

m ∼ 0.5

x + y +

(cid:114)(cid:16)

(x − y)2(cid:17)

− 2∆tσ2 log(U )

(cid:19)

where U

is distributed uniformly in (0, 1) — to reduce the discretization
bias of the method. Such an experiment also demonstrates that
programming in Tensorﬂow is ﬂexible enough to allow for
varied simulation schemes. Each of the 100 simulation runs
has 25 to 100 discretization steps and 2M samples. Again, in
Figure 3, one can appreciate the speed up for a Tensorﬂow
graph running on TPU with respect to an identical graph
running on GPU while the impact of the use of single precision
as opposed to double precision is hardly noticeable.

2) Multi-variate process simulation to benchmark TPUs’
MXU: Multi-variate stochastic simulations represent heavier
computational workloads than their uni-variate counterparts
whenever they involved correlated dynamics. A matrix/matrix
multiply is then involve at each step of the simulation when
computing the product of the volatility matrix with the mul-
tidimensional normally distributed stacked PRNs. In such a
setting, the speed of the MXU on TPUs may be beneﬁcial, but,

Fig. 3: Vanilla call (left) and Path dependent exotic Put
(right): Estimate distributions in single and double precision.
Bias terms with various numerical precision and discretization
steps for the estimation of the payoff’s risk neutral expectation
under a Black-Scholes model help assess the impact of the
temporal discretization step size. The temporal discretization
bias clearly dominates the numerical precision bias.

as it uses a custom ﬂoating point representation, one needs to
assess that no substantial numerical precision bias appears.

Basket European option: We price an at-the-money Basket
European call with 2048 underlyings whose price is initially
100. The interest rate is 0.05 and the volatility matrix we
use is a historical estimate based on market data collected on
daily variations of randomly selected stocks from the Russell
3000 through 2018. 2K samples are used for each of the 100
simulations. Simulations now involve matrix multiplications
corresponding to Equation (1) and therefore the MXU of the
TPU is used with a reduced bﬂoat16 precision. All other
computations on TPU run in single precision. In Figure 4 we
present the estimates provided in mixed precision on TPU and
compare them with single and double precision estimates. We
ﬁnd that running simulations on TPU does not introduce any
signiﬁcant bias while offering substantial speed ups compared
to GPUs.

Basket European option Delta: As automated differen-
tiation is integrated into the Tensorﬂow framework, almost

3.2323.2343.2363.2383.2403.2423.2443.2463.2483.250Monte Carlo price estimate0510152025Black-Scholes Call, 10240000 samples, 100 time stepsgpu singlegpu doubletpu mixed2030405060708090100Number of Euler discretization steps0.040.020.000.020.040.060.08Bias of estimate(min, mean, max)Estimate versus number Euler stepsgpu doublegpu singletpu mixed020406080100Number of Euler discretization steps10-310-210-1100Median wall time secs(5th/95th perc.)Wall timesTensorflow (TF) on v100Interactive TF 1/8 tpuInteractive TF full tpu23.1623.1823.2023.2223.2423.2623.2823.30Monte Carlo price estimate024681012141618Black-Scholes Put Up In, 2048000 samples, 100 time stepsgpu singlegpu doubletpu mixed2030405060708090100Number of Euler discretization steps0.00.10.20.30.40.5Bias of estimate(min, mean, max)Estimate versus number Euler stepsgpu doublegpu singletpu mixed020406080100Number of Euler discretization steps10-310-210-1Median wall time secs(5th/95th perc.)Wall timesTensorflow (TF) on v100Interactive TF 1/8 tpuInteractive TF full tpuno additional engineering effort is required to compute path-
wise sensitivities of a MC simulation. Considering the same
European option, we now compute its “delta”, that is to say
the ﬁrst order derivative of the option’s price estimate with
respect to the initial price vector (with 2048 components). As
demonstrated in the code snippet of Figure 8, presented in
section VII, computing such a sensitivity estimate, which can
represent a substantial software engineering effort for libraries
not designed with AAD in mind [33], only requires a single
line of code in Tensorﬂow. In Figure 4, we can appreciate
that although back-propagation introduces an additional chain
of multiplications, no signiﬁcant bias is added (we present
results here only for the ﬁrst component of the “delta” for
ease of reading).

Fig. 4: Basket European Option (left), ﬁrst component of
Basket European Option Delta (right) : Estimate distribu-
tions and wall times when the MXU is employed on TPU
with mixed precision. No signiﬁcant bias is introduced by
running computations on a TPU. The measured wall times,
which include the network round trip between front end host
and TPU, are very competitive.

B. Risk metrics

The impact of mixed precision on variance may become a
concern for risk metrics such as VaR and CVaR whose purpose
is to estimate percentiles and losses for a given portfolio that
occur in rare adverse scenarios.

1) Estimating Value-at-Risk with many underlying factors:
In this simulation, we consider the simulation of the same
2048 underlying assets as in the Basket option experiment (all
from the Russel 3000) with a trend and correlation structure
estimated based on historical data on daily variations. The
portfolio whose loss distribution we sample from consists of
5 call options with different strikes on each of the underlying
assets. As a result the portfolio of interest has 10240 instru-
ments.

Value-at-Risk: We simulate the distribution of proﬁt and
losses (PnL) for the portfolio of interest over the course of a
year with different scales of temporal discretization. The ﬁrst
risk metric of interest is the standard Value-at-Risk (VaR) at
level α. By deﬁnition, VaR is a quantile of the distribution
of losses on an investment in the presence of contingencies.
Given a random variable PnL(ω) representing the PnL of the
overall portfolio subjected to random perturbations ω, we have
to estimate the quantile of level α of the PnL distribution:

VaRα (PnL(ω)) = −inf {x ∈ R : P (PnL(ω) ≥ x) > α} .

The results presented in Figure 5 show that limited precision in
the MXU has some impact in the estimated VaR terms as little
bias is present (less than 1% in relative magnitude compared
to a double precision simulation). However, the speed-ups
reported are substantial, so that a MLMC approach could be
employed to preserve most of the computational gains while
identifying the TPU-induced bias to later correct it.

Conditional Value-at-Risk: The Conditional Value-at-Risk
(CVaR) (otherwise known as expected shortfall) is another risk
metric used jointly with VaR. The great advantage of CVaR is
that it is a coherent risk measure and therefore provides a more
principled view on risks associated with a given portfolio [15].
While VaR is deﬁned as the quantile of a loss distribution,
CVaR corresponds to a conditional expectation on losses. More
precisely, CVaR estimates the expected loss conditioned to the
fact that the loss is already above the VaR. For a level of
tolerance α, CVaRα is deﬁned as follows:

CVaRα (PnL(ω)) = −EP [P nL(ω)| − P nL(ω) > VaRα] .

The results reported in Figure 5 demonstrate that while the
use of mixed precision on TPU introduces little bias (less
than 1% in relative magnitude compared to a double precision
simulation) in the estimation of CVaR it also comes with
substantial speed ups. As for the computation of VaR, the
results indicate that TPUs are therefore good candidates for
the use of MLMC to produce unbiased estimates rapidly.

C. Monte Carlo American option pricing

Monte Carlo option pricing of an American option with
multiple underlyings presents the computational difﬁculties
encountered in European Basket option pricing because of the
need for the simulation of multi-variate diffusion processes
while adding the additional complexity of having to proceed
with dynamic programming.

4.8604.8654.8704.8754.8804.885Monte Carlo price estimate051015202530Basket Call, 2048 samples, 100 time stepsgpu singlegpu doubletpu mixed2030405060708090100Number of Euler discretization steps4.864.884.904.924.944.96Estimated price(min, mean, max)Estimate versus number Euler stepsgpu doublegpu singletpu mixed020406080100Number of Euler discretization steps10-210-1100Median wall time secs(5th/95th perc.)Wall timesTensorflow (TF) on v100Interactive TF 1/8 tpuInteractive TF full tpu0.0004750.0004800.0004850.0004900.0004950.000500Monte Carlo delta[0] estimate051015202530Basket option delta[0], 2048 samples, 100 time stepsgpu singlegpu doubletpu mixed2030405060708090100Number of Euler discretization steps0.0004750.0004800.0004850.0004900.0004950.000500Estimated delta[0](min, mean, max)Estimate versus number Euler stepsgpu doublegpu singletpu mixed020406080100Number of Euler discretization steps10-210-1100Median wall time secs(5th/95th perc.)Wall timesTensorflow (TF) on v100Interactive TF 1/8 tpuInteractive TF full tpuwhich assesses the true price as 13.90. The results presented
in Figure 6 show that no signiﬁcant bias was introduced by
the use of TPU while substantial speedups were gained. It is
noteworthy that here we only use one TPU core because there
are multiple ways of parallelizing LSM and discussing their
different properties is beyond the scope of the present paper.

Fig. 6: Longstaff-Schwartz pricing of an American Max-
imum Option: LSM is employed on both TPU and GPU
to price an American Max-of-Two Call. The very setting of
Example 8.6.1 from [15] is reproduced and the estimates on
all hardware are close to the stated true price of 13.90.

D. Quasi Monte Carlo Basket Option pricing

Quasi Monte Carlo methods considerably speed up the
convergence rate of stochastic integration. Here we consider
a workload consisting in simulating the payoff of a European
Basket option averaging 16 correlated assets whose correla-
tions are estimated from historical values as in VI-B1 (we take
the ﬁrst 16 underlying from 2048). The setting we consider
is therefore very similar to the numerical examples in 5.5.1
of [15] although we consider an arithmetic average basket call
as opposed to a geometric average. A converged Monte Carlo
method estimates a price of 6.956624 (3.31 × 1011 samples)
in single precision. We compare the execution speed on TPU
of a standard Monte Carlo simulation and a Quasi Monte
Carlo simulation relying on the Sobol sequence. We only use
a single discretization step while sampling. The interest rate,
maturity and strike are kept identical to VI-A2. Therefore we
sample from a 16 dimensional log-normal distribution prior
to computing the discounted payoff. As we employ the Box-
Muller algorithm to generate random samples from uniform
samples, we consider the Sobol sequence in 32 dimensions
with direction numbers from [18]. We observe empirically
similar end-to-end median wall-time in the cloud when com-
pared with our Monte Carlo sampler (at equal number of
samples). For instance, we measure a median wall time over
100 trials of 1.15ms for 8192 payoff samples with the Sobol
sequence and 1.40ms with the Threefry-based Monte Carlo
method. As expected, in Figure 7, we observe that the Root
Mean Square Error of the Monte Carlo estimates (over 100
trials with different Threefry key values) decreases at a rate
of O( 1√
) where N is the number of samples. In comparison,
N
the absolute error of the Sobol-based Quasi Monte Carlo
integration decreases at a near linear rate.

Fig. 5: VaR0.95 (left) and CVaR0.95 (right): Estimates of
and corresponding wall times (the MXU is used with bﬂoat16
precision on TPU for matrix multiplies). Little bias is intro-
duced by running computation in mixed precision on TPU.
The measured wall times (including the network round trip
for TPUs) are very competitive for TPU.

1) Longstaff-Schwartz pricing of an American Maximum
Option: The Longstaff-Schwartz (LSM) method for multi-
variate American option pricing relies on a set of samples
of the paths of the underlyings to compute the terminal
payoff at the expiration of the option assuming there was no
early exercise. As explained earlier in sub-section II-A3, LSM
then proceeds with dynamic programming taking the form
of chained linear regressions by Ordinary Least Squares. In
pratice, we use the Cholesky based solver for such systems
integrated in Tensorﬂow after a Cholesky decomposition of
the Grammian. The MXU is now used for both the forward
simulations and the linear inversions in dynamic programming.
Therefore, from a numerical standpoint, there is an added level
of uncertainty related to the impact of the use of bﬂoat16 in
the MXU. The mixed precision could be causing numerical
instability problems. In practice, this has not been the case
on TPU and we have had to add a regularization term to
the regression only to help the GPU implementation which
was often failing to do the Cholesky decomposition. We
reproduced the experiment reported in [15] in Example 8.6.1

230002305023100231502320023250Var 0.95 estimate05101520253035Var 0.95, 2048 samples, 100 time stepsgpu singlegpu doubletpu mixed2030405060708090100Number of Euler discretization steps233002335023400234502350023550Estimated CVaR(min, mean, max)Estimate versus number Euler stepsgpu doublegpu singletpu mixed020406080100Number of Euler discretization steps10-210-1100Median wall time secs(5th/95th perc.)Running wall times for VaR estimationNon interactive TF v100Interactive TF 1/8 tpuInteractive TF full tpu233002335023400234502350023550CVar 0.95 estimate0510152025CVar 0.95, 2048 samples, 25 time stepsgpu singlegpu doubletpu mixed2030405060708090100Number of Euler discretization steps230002305023100231502320023250Estimated VaR(min, mean, max)Estimate versus number Euler stepsgpu doublegpu singletpu mixed020406080100Number of Euler discretization steps10-210-1100Median wall time secs(5th/95th perc.)Running wall times for CVaR estimationNon interactive TF v100Interactive TF 1/8 tpuInteractive TF full tpu010203040506070Monte Carlo price estimate0510152025303540American Max-of-Two Call,40000 samples,100 time stepsgpu singlegpu doubletpu mixed15202530354045505560Number of Euler discretization steps010203040506070Estimated price(min, mean, max)Estimate versus number Euler stepsgpu doublegpu singletpu mixed0102030405060Number of Euler discretization steps10-210-1Median wall time secs(5th/95th perc.)Wall timesNon interactive TF v100Interactive TF 1/8 tpuFig. 8: Setting up a simulation in an interactive note-
book. dynamics.random_normal is a simple wrapper for
tf.random.stateless_normal (i.e. Threefry).

suited for acceleration on TPU. Furthermore, programming
with Tensorﬂow makes it trivial to estimate sensitivities with
automated differentiation.

Finally, if cases arise in which the mixed precision calcula-
tions running on TPUs create too signiﬁcant a bias, Multi-
Level Monte Carlo (MLMC [13]) offers an efﬁcient way
of correcting this bias while preserving the TPU speed-up.
Therefore, as next steps, we want to experiment with MLMC
methods on TPUs with CPUs running a reference precision
simulation. We also plan to use multiple TPUs to scale
simulations up.

Fig. 7: Monte Carlo vs Sobol Quasi Monte Carlo for the
pricing of a 16 asset European Basket Call (we use a single
discretization time step) on TPU. While generating Sobol
points has a wall time for sampling similar to Threefry, we
observe a near linear rate of convergence for QMC as opposed
to square root for MC.

VII. PROGRAMMING SIMULATIONS WITH
TENSORFLOW

A. Minimal code to run a Monte Carlo simulation

In Figure 8, we show the sufﬁcient code to set up a Monte
Carlo simulation in Tensorﬂow. One can devise a simulation
for a European Basket Option price estimator with a few lines
of code, all while working in an interactive Colaboratory (or
Jupyter) notebook. Tensorﬂow is now a very expressive open
source library one can easily extend and therefore provides a
lot of ﬂexibility to program various Monte Carlo simulations.
Calling tf.contrib.tpu.rewrite compiles the Tensor-
ﬂow code with XLA to run optimally on TPU.

B. Automated Differentiation with one line of code

In Figure 8, we also show that a single line of code
sufﬁces to turn payoff estimates into sensitivity estimates
to the initial price in
(ﬁrst order derivatives with respect
this case computed by AAD, i.e. back-propagation). This is
a remarkable consequence of employing Tensorﬂow which
is optimized for linear algebra acceleration, integrates fast
random number generation, and also provides automated direct
differentiation.

VIII. CONCLUSION

In conclusion, we argue that TPUs are indeed accurate
enough, fast and easy to use for ﬁnancial simulation. Our
experiments on multiple workloads demonstrate that even for
large simulations written in Tensorﬂow, TPUs enable a re-
sponsive interactive experience with a higher speed than GPUs
running Tensorﬂow. We showed that Monte Carlo and Quasi-
Monte Carlo integration relying on vectorized operations, large
matrix/matrix products and linear system inversions is well

102103104105106Number of samples10-510-410-310-210-1Integration errorConvergence speed (in # of samples, on TPU) of Sobol Quasi Monte Carlo vs Monte CarloMC mean square errorSobol QMC absolute error[28] MATSUMOTO, M., AND NISHIMURA, T. Mersenne twister: a 623-
dimensionally equidistributed uniform pseudo-random number gen-
ACM Transactions on Modeling and Computer Simulation
erator.
(TOMACS) 8, 1 (1998), 3–30.

[29] OWEN, A. B. Quasi-monte carlo sampling. Monte Carlo Ray Tracing:

Siggraph 1 (2003), 69–88.

[30] PAG `ES, G. Numerical Probability: An Introduction with Applications to

Finance. Springer, 2018.

[31] PODLOZHNYUK, V., AND HARRIS, M. Monte carlo option pricing.

CUDA SDK (2008).

[32] SALMON, J. K., MORAES, M. A., DROR, R. O., AND SHAW, D. E.
Parallel random numbers: as easy as 1, 2, 3. In Proceedings of 2011
International Conference for High Performance Computing, Networking,
Storage and Analysis (2011), ACM, p. 16.

[33] SAVINE, A. Modern Computational Finance: AAD and Parallel Simu-

lations. Wiley, 2018.

[34] SOBOL’, I. M., ASOTSKY, D., KREININ, A., AND KUCHERENKO,
S. Construction and comparison of high-dimensional sobol’generators.
Wilmott 2011, 56 (2011), 64–79.

[35] THOMAS, D. B., HOWES, L., AND LUK, W. A comparison of cpus,
gpus, fpgas, and massively parallel processor arrays for random number
generation. In Proceedings of the ACM/SIGDA international symposium
on Field programmable gate arrays (2009), ACM, pp. 63–72.

[36] WESTON, S., MARIN, J.-T., SPOONER, J., PELL, O., AND MENCER,
O. Accelerating the computation of portfolios of tranched credit deriva-
In 2010 IEEE Workshop on High Performance Computational
tives.
Finance (2010), IEEE, pp. 1–8.

[37] WESTON, S., SPOONER, J., RACANI `ERE, S., AND MENCER, O. Rapid
computation of value and risk for derivatives portfolios. Concurrency
and Computation: Practice and Experience 24, 8 (2012), 880–894.

REFERENCES

[1] Cloud tpu documentation. https://cloud.google.com/tpu/docs/. Accessed:

2019-04-29.

[2] ABADI, M., BARHAM, P., CHEN, J., CHEN, Z., DAVIS, A., DEAN, J.,
DEVIN, M., GHEMAWAT, S., IRVING, G., ISARD, M., ET AL. Tensor-
In 12th {USENIX}
ﬂow: A system for large-scale machine learning.
Symposium on Operating Systems Design and Implementation ({OSDI}
16) (2016), pp. 265–283.

[3] ABBAS-TURKI, L. A., VIALLE, S., LAPEYRE, B., AND MERCIER,
P. Pricing derivatives on graphics processing units using monte carlo
simulation. Concurrency and Computation: Practice and Experience
26, 9 (2014), 1679–1697.

[4] BERGOMI, L. Stochastic volatility modeling. CRC Press, 2015.
[5] BRUGGER, C., DE SCHRYVER, C., AND WEHN, N. Hyper: a runtime
reconﬁgurable architecture for monte carlo option pricing in the heston
model. In 2014 24th International Conference on Field Programmable
Logic and Applications (FPL) (2014), IEEE, pp. 1–8.

[6] CHOW, G. C. T., TSE, A. H. T., JIN, Q., LUK, W., LEONG, P. H.,
AND THOMAS, D. B. A mixed precision monte carlo methodology for
reconﬁgurable accelerator systems. In Proceedings of the ACM/SIGDA
international symposium on Field Programmable Gate Arrays (2012),
ACM, pp. 57–66.

[7] CLOUD,

G.

Performance

Guide.

https://cloud.google.com/tpu/docs/performance-guide, 2019.

[8] CODDINGTON, P. D. Random number generators for parallel computers.
[9] DIXON, M. F., BRADLEY, T., CHONG, J., AND KEUTZER, K. Monte
carlo–based ﬁnancial market value-at-risk estimation on gpus. In GPU
Computing Gems Jade Edition. Elsevier, 2012, pp. 337–353.

[10] DU TOIT, J., LOTZ, J., AND NAUMANN, U. Adjoint algorithmic

differentiation of a gpu accelerated application, 2013.

[11] DUPIRE, B., ET AL. Pricing with a smile. Risk 7, 1 (1994), 18–20.
[12] GATHERAL, J., JAISSON, T., AND ROSENBAUM, M. Volatility is rough.

arXiv preprint arXiv:1410.3394 (2014).

[13] GILES, M. B. Multilevel monte carlo path simulation. Operations

Research 56, 3 (2008), 607–617.

[14] GILES, M. B. Multilevel monte carlo methods. Acta Numerica 24

(2015), 259–328.

[15] GLASSERMAN, P. Monte Carlo methods in ﬁnancial engineering,

vol. 53. Springer Science & Business Media, 2013.

[16] GOODFELLOW, I., BENGIO, Y., AND COURVILLE, A. Deep learning.

MIT press, 2016.

[17] GUYON, J., AND HENRY-LABORDERE, P. The smile calibration prob-

lem solved.

[18] JOE, S., AND KUO, F. Y. Constructing sobol sequences with better two-
dimensional projections. SIAM Journal on Scientiﬁc Computing 30, 5
(2008), 2635–2654.

[19] JOE, S., AND KUO, F. Y. Notes on generating sobol sequences. ACM
Transactions on Mathematical Software (TOMS) 29, 1 (2008), 49–57.
[20] JOUPPI, N. P. Quantifying the performance of the tpu, our ﬁrst machine
learning chip. https://cloud.google.com/blog/products/gcp/quantifying-
the-performance-of-the-tpu-our-ﬁrst-machine-learning-chip, 2017.
[21] KORN, R., KORN, E., AND KROISANDT, G. Monte Carlo methods and

models in ﬁnance and insurance. CRC press, 2010.

[22] L’ECUYER, P., AND LEMIEUX, C. Variance reduction via lattice rules.

Management Science 46, 9 (2000), 1214–1235.

[23] L’ECUYER, P., AND SIMARD, R. Testu01: Ac library for empirical test-
ing of random number generators. ACM Transactions on Mathematical
Software (TOMS) 33, 4 (2007), 22.

[24] LEE, A., YAU, C., GILES, M. B., DOUCET, A., AND HOLMES, C. C.
On the utility of graphics cards to perform massively parallel simulation
Journal of computational and
of advanced monte carlo methods.
graphical statistics 19, 4 (2010), 769–789.

[25] LONGSTAFF, F. A., AND SCHWARTZ, E. S. Valuing american options
by simulation: a simple least-squares approach. The review of ﬁnancial
studies 14, 1 (2001), 113–147.

[26] LECUYER, P., MUNGER, D., ORESHKIN, B., AND SIMARD, R. Ran-
dom numbers for parallel computers: Requirements and methods, with
emphasis on gpus. Mathematics and Computers in Simulation 135
(2017), 3–17.

[27] MARSHALL, T. J., REESOR, R. M., AND COX, M. Simulation valuation
of multiple exercise options. In Proceedings of the Winter Simulation
Conference (2011), Winter Simulation Conference, pp. 3772–3783.

