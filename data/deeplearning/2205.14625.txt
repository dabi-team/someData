IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022

1

Cervical Glandular Cell Detection from Whole
Slide Image with Out-Of-Distribution Data

Ziquan Wei, Shenghua Cheng, Jing Cai, Shaoqun Zeng, Xiuli Liu, and Zehua Wang,

2
2
0
2

l
u
J

3

]

V
C
.
s
c
[

3
v
5
2
6
4
1
.
5
0
2
2
:
v
i
X
r
a

Abstract— Cervical glandular cell (GC) detection is a key
step in computer-aided diagnosis for cervical adenocarci-
nomas screening. It is challenging to accurately recognize
GCs in cervical smears in which squamous cells are the
major. Widely existing Out-Of-Distribution (OOD) data in
the entire smear leads decreasing reliability of machine
learning system for GC detection. Although, the State-
Of-The-Art (SOTA) deep learning model can outperform
pathologists in preselected regions of interest, the mass
False Positive (FP) prediction with high probability is still
unsolved when facing such gigapixel whole slide image.
This paper proposed a novel PolarNet based on the mor-
phological prior knowledge of GC trying to solve the FP
problem via a self-attention mechanism in eight-neighbor.
It estimates the polar orientation of nucleus of GC. As a
plugin module, PolarNet can guide the deep feature and
predicted conﬁdence of general object detection models.
In experiments, we discovered that general models based
on four different frameworks can reject FP in small image
set and increase the mean of average precision (mAP) by
0.007 ∼ 0.015 in average, where the highest exceeds the
recent cervical cell detection model 0.037. By plugging
PolarNet, the deployed C++ program improved by 8.8% on
accuracy of top-20 GC detection from external WSIs, while
sacriﬁcing 14.4 s of computational time. Code is available
in https://github.com/Chrisa142857/PolarNet-GCdet.

Index Terms— Cervical Glandular Cell, Computer-aided
Diagnosis, Deep Learning, Object Detection, Whole Slide
Image Detection.

I. INTRODUCTION

D ETECTING glandular cell (GC) from whole slide image

(WSI) can aid pathologists to screen cervical adenocarci-
nomas (ADCA) and reduce the labour cost by computer-aided
diagnosis (CAD) [1]–[3]. With the proper amount of annotated
data, the well-trained deep learning model can provide top

This work is supported by the NSFC projects (grant 61721092), China
Postdoctoral Science Foundation (grant 2021M701320) and the director
fund of the WNLO.

Z. Wei and S. Cheng contributed equally to this work
Corresponding author: X. Liu and Z. Wang
Z. Wei, X. S. Cheng, S. Zeng , and X. Liu are with Collabora-
tive Innovation Center for Biomedical Engineering, Wuhan National
Laboratory for Optoelectronics-Huazhong University of Science and
Technology, Wuhan, Hubei, China. They are also with Britton Chance
Center and MOE Key Laboratory for Biomedical Photonics, School of
Engineering Sciences, Huazhong University of Science and Technol-
ogy, Wuhan, Hubei, China (email: wzquan142857@hust.edu.cn; cheng-
shen@hust.edu.cn; sqzeng@mail.hust.edu.cn; xlliu@mail.hust.edu.cn).
J. Cai and Z. Wang are with Department of Obstetrics and Gy-
necology, Union Hospital, Tongji Medical College, Huazhong Uni-
versity of Science and Technology, Wuhan, China (e-mail: caijing-
mmm@hotmail.com; zehuawang@163.net).

Fig. 1. Upper ﬁgures: Glandular cell (GC) in cervical whole slide image
is relatively sparse, where blue box denotes GC, and orange denotes
squamous cell (SC). Below ﬁgures: Inconsistent polarity of SCs that are
folded or clustered is in the left, and illustrations of polar pattern of GC
are in the right, where blue arrow denotes polar orientation.

cells from WSI to help the screening, as a recommending sys-
tem. The task is challenging, however, that squamous cell (SC)
is the major in cervical smear. The sparseness of GC is severe.
Otherwise, the morphology of GC and SC is similar while the
basic shape of polar pattern of GC is distinct comparing with
SC. Such characteristics are the direct hardship to the data-
driven model performing accurately without the usage of the
prior knowledge of GC basic shape in general models.

Although recognition cervical lesion in cytology has been
developed since 2016 by using deep learning [4], [5], related
works focus on the major object, SC, but rarely discuss GC,
which is minor but important [6]. They researched cervical
cancer detection on small image sets with general deep learn-
ing methods, only few works mentioned the performance of
model on external WSI set. Like Liang et al. [13] tested
the detection performance of both SC and GC with the deep
learning model in only preseleted ROIs of external WSI. And
Gupta et al. [8] only reported a rough detection results from

GC (n=17)single GCGC clusterGC clustersingle GCGC clusterGC clustersingle SCfolded SCSC clustersingle SCfolded SCSC cluster 
 
 
 
 
 
2

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022

external WSIs by their ROI prediction model. There is no
focus on GC in cervical cytology and no usage of prior
knowledge of GC in related works inspire that there is still
room of improvement of GC detection using deep learning.
The lack of evaluations of cell detection from WSI also leads
related works are hardly deployed to the clinical CAD.

More vivid description of this problem is illustrated in Fig.
1, there are only 17 GCs existed in the WSI shown in the
upper. This sparseness leads the computation of WSI will meet
substantial out-of-distribution (OOD) data including artifacts,
single SC, folded SC, and SC cluster. Even though their
basic shape has obvious distinction comparing with GC as
shown in the below of Fig. 1, general deep learning model is
hard to distinguish. Those orange boxes are predicted by the
state-of-the-art (SOTA) object detection model, YOLOX [9].
Related works for cervical cell detection are designed based on
more abstract knowledge, such as the attention and connection
between different scales [10], [11], the mixed information
of multiple resolutions [12], the prototype representation of
multiple cell categories [13] , and using time series information
to enhance the cancer detection [14]. Despite atypical GC is
among their objects, their design concepts have not consider
the basic shape of cervical cell. That is hardly helping to reject
those false positives with wrong basic shape.

This paper proposed a new network to enhance GC detection
with prior knowledge of basic shape of GC. We also provide a
new experimental setting to show the practical performance of
model. Firstly, a polar attention network (PolarNet) is designed
to quantify the confusing morphology of GC, that is the basic
shape of polar orientation. The network employs a novel self-
attention mechanism in eight-neighbor that enables it to score
the salience of polarity orientations of GC. Pseudo-GCs that
are not signiﬁcantly polar in the outer test set are controlled by
such polarity role. The network is also a deep learning module
that can be plugged into any general object detection model.
Then, two additional test sets of small images from the external
WSI containing different volume of OOD data are obtained
by human and well-trained model, respectively. Furthermore, a
elegant deployment of the proposal is completed and provided
as a C++ program.

In GC detection experiments on both three small image sets,
the proposed PolarNet showed effectiveness, and compared
with 5 models including four different frameworks. With the
usage of PolarNet, model can reject false positive (FP) and
increase the mean of average precision (mAP) from 0.007 to
0.015 in average, where the highest exceeds the recent cervical
cell detection model 0.037. The deployed program improved
by 8.8% on accuracy of top-20 GC detection from external
WSIs (n = 110), while sacriﬁcing 14.4 s of computational
time.

Brieﬂy, this work proposed a cervical GC detection method
to recognize GC from WSI, it consists of four contributions:
• In Section IV, an OOD data set is provided using a well-
trained YOLOX. The data is obtained from the severe
FP predicted by the deep learning model, and is able to
reﬂect the performance of GC detection from WSIs that
are not fully annotated.

• In Section III, a novel eight-neighbor self-attention mech-

anism is proposed to quantify the polar orientation of GC
and to construct the plugin network PolarNet. It can be
used in general detection model whether it is single-stage
or multi-stage to reject severe FP.

• In Section V, the usage of PolarNet in four different
frameworks both shown signiﬁcance for GC detection.
For clinical application, the proposal also performs ef-
fectiveness to recommend top-20 GCs from WSI.

• The proposed method is deployed as a C++ program.
The computational cost of PolarNet is listed in detail.
The program is released online.

II. RELATED WORKS

A. Cervical Cell Detection

Earlier, the traditional framework was based on cytological
deﬁnitions and completed cell classiﬁcation by calculating the
nuclear-cytoplasmic ratio of cervical cells, while Tareef et al.
To improve the accuracy of subsequent classiﬁcation tasks,
their convolutional neural network (CNN) models surpass tra-
ditional machine learning algorithms in segmentation accuracy.
But then, Zhang et al. [15] believed that the unavoidable
segmentation error would always lead to a decrease in the
classiﬁcation accuracy of abnormal cells, and they proposed
the DeepPap model for the ﬁrst time to directly apply the con-
volutional neural network to the classiﬁcation task of cervical
cells to avoid pre-segmentation processing, and performed well
on two public datasets, Herlev 2005 [16] and HEMLBC [17],
showing more than 98% accuracy on both datasets, but their
method still compares with traditional algorithms. more time
consuming. Shanthi et al. [18] explored in more detail the
classiﬁcation accuracy that can be obtained when using CNN
for cervical cell classiﬁcation, performing cell edge extraction,
cell nucleus segmentation, or directly using the original image
without segmentation, although their model is described in
Herlev 2005. The accuracy rate is only 94% ∼ 95%, and it
does not fully surpass the previous method, but it has been
veriﬁed that it is the most effective to use the original image
directly without segmentation. These early developments and
conclusions using deep learning led to a perception of the
potential of deep learning in CAD.

Later, with the development of general deep learning mod-
els, many general models such as VGG [19], GoogLeNet
[20], ResNet [21] and Inception [22] pre-trained and validated
by the large general dataset ImageNet [23] appeared. Wait.
Lin et al. [24] proposed that these pre-trained models from
general data can extract general morphological features, and
they transferred these pre-trained models to the cervical cell
classiﬁcation task, using GoogLeNet in Herlev 2005 to obtain
the highest accuracy of 94.5%. In order to further improve the
computational efﬁciency of CNN in cervical cell classiﬁcation,
Dong et al. [25] proposed to combine a lightweight convolu-
tional neural network with artiﬁcial features, by adapting the
prior knowledge based on cytology deﬁnition to the Inception
V3 model [22], and ﬁnally achieved over 98% accuracy on
the public dataset Herlev 2005 [16].

Because of the irregular distribution of cells on the slide
in cervical cytology images, it is important to predict both

ZIQUAN WEI et al.: CERVICAL GLANDULAR CELL DETECTION FROM WHOLE SLIDE IMAGE WITH OUT-OF-DISTRIBUTION DATA

3

the location and class of cells. Recently, Xiang et al. [26],
Liang et al. [27] proposed a cervical cell detection model
earlier based on the single-stage object detection model YOLO
(You Only Look Once) [28], and they stacked additional
Inception V3, and FPN (Feature Pyramid Network) with
content-aware function [29], on their private 10-class cervical
cell dataset, end-to-end localization, classiﬁcation and clas-
siﬁcation of cervical cells were completed. Prediction size,
and the average accuracy reached 63.4%, although their image
pixels are sufﬁcient (4000 × 3000), but the number of images
is small (n = 12909) is a defect. In order to solve the
problem of model training when the amount of data is small,
Liang et al. [13] subsequently proposed a comparison detector
based on a double-stage Faster RCNN. On small-scale (n =
7410) datasets, the contrastive detectors can achieve signiﬁcant
improvements over the previous ones. The above are all cell
images obtained by traditional production methods.

In the work of Tan et al. [7], the authors ﬁrstly collected
more than 16000 LBC (Liquid-Based Cytology) images as
training and validation data. Compared with traditional cervi-
cal cytology images, the LBC image has a clearer background
[30]. They independently set 290 ROI (Region Of Interest)
images in the external full slide as the test set to simulate real
CAD process. They obtained decent accuracy after training
a Faster RCNN model. The experiment of this deep learning
model on external data provides preliminary feasibility support
for CAD, although the amount of external data is still smaller
than the training and validation data.

Since the cervical cell detection task requires a larger
ﬁeld of view of image data than the earlier classiﬁcation
and segmentation tasks, and such public data sets are scarce,
the above related object detection research is carried out on
private data. More important, whether it is cell segmentation,
classiﬁcation, or cell detection, none of the related works have
taken the full calculation of WSI into their experiments.

to deal with the confusing cervical cancer subclasses, and
arranged each object detection CNN for each class. Such a
system greatly improves the robustness, showing accuracy and
generalization that surpasses pathologists, but it is conceivable
that the computing resources required by this system are very
large. The author reported its speed is 180 s/WSI. Cao et
al. [10] designed a new attention module added to the Faster
RCNN, a two-stage object detection model, and improved the
mAP of cell detection by 2.37% compared with the baseline.
It predicts the conﬁdence of object to weight the features
extracted by ResNet to complete WSI classiﬁcation. None of
the performance of cell detection from external/OOD test set
is reported. Cheng et al. [12] roughly performed cell detec-
tion using low-resolution images, and then performed feature
extraction at the location of the object to complete the WSI
classiﬁcation. Wei et al. [11] used a new lightweight object
detection model to simultaneously perform cell detection and
feature extraction to complete WSI classiﬁcation.

However, there is rarely a solution and a evaluation when
it comes to a complete, cell-level prediction computing all
local images on cervical WSI. The training of general deep
learning requires the experimental data to conform to the
real distribution, but the cervical WSI contains various non-
relevant content, that is so-called OOD data, to determine the
lesion. Giving such data a full annotation is expensive, scarce,
and unnecessary. To develop new method on cell recognition,
datasets used in related work are mostly based on local small
images generated by pathologists from pre-selected ROIs on
WSI. Despite the WSI calculation is completed in WSI clas-
siﬁcation, their discussion is focus on the classiﬁcation while
the cell detection is just a step of it. Therefore, the related
research hardly mentions the generalization and reliability
problems that their local prediction model will encounter when
calculating the cells of the whole cervical slide, which weakens
the contribution of their work.

B. Detection from Whole Slide Image

III. METHOD

The related research extending from local prediction re-
search to WSI calculation is still limited. Gupta et al. [8]
proposed an automatic ROI identiﬁcation method for the ﬁrst
time. Although ROI identiﬁcation can only obtain a rough
localization from the WSI, their work considered all the pixels
for calculating the WSI, and used all the regions on the WSI
for model training, Excellent ROI classiﬁcation accuracy is
achieved on a private dataset containing 10 WSI images, and
this study provides an idea of automatically preselecting ROIs
for calculating local cancerous cells in WSI.

Cervical cell detection with WSI calculation is also used by
several WSI classiﬁcation researches. Ke et al. [31] tried to use
a nucleus segmentation CNN to guide classiﬁcation, and then
integrated the segmentation results through hand-designed fea-
ture engineering. They reported a fabulous WSI classiﬁcation
performance but none of the cell segmentation/classiﬁcation
on external test set. Subsequently, Zhu et al. [32] proposed an
integrated cervical WSI recognition system, which includes 24
object detection CNNs and other 4 models. To complete the
WSI classiﬁcation task, they also created a new 24 classes

This section describes the proposed PolarNet and how this
plugin module is working on modern deep learning models.

A. PolarNet

The PolarNet is designed as shown in Fig. 2. In the one
hand, it obtains a polar attention score matrix by computing
the self-attention inside the eight-neighbor of features. On the
other hand, it generates a new feature map by weighting with
the attention score of different orientations also inside the
eight-neighbor area.

The PolarNet is designed to compute the last stage of feature
maps from the backbone of a modern model. Thus, we can
assume the feature maps are denoted by x ∈ RC×H×W , the
polar attention score matrix by PAS ∈ R9×H×W , and the
output feature maps of the PolarNet by y ∈ RC×H×W . Then,
outputs of PolarNet can be writen by follows:

PAS·,i,j = sof tmax({Q·,i,j (cid:12) KT

·,nei[n]}n=1,2,...,9),

(1)

4

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022

Fig. 3. Polar orientations in the eight-neighbor. (a) Using the mean
of polar attention score to represent the polar salience of GC. (b)
Illustrating orientations in one eight-neighbor of one case of GC.

Fig. 2. Structure of the proposed PolarNet, where C means the channel
number of feature maps. H and W are the height and the width of input
feature map, respectively. The dot-product block refers to the Eq. 1 and
the weighting block refers to the Eq. 2.

yc,i,j = norm(

9
(cid:88)

n=1

PASn,i,j × (1 + Vc,nei[n])),

(2)

where i, j means the coordinator of ith row and jth column
in feature maps, c means the cth channel of feature maps, (cid:12)
means the operation of dot-product, nei ∈ N9×2 is the index
set of eight-neighbor [(i−1, j−1), (i−1, j), (i−1, j+1), (i, j−
1), (i, j), (i, j + 1), (i + 1, j − 1), (i + 1, j), (i + 1, j + 1)].
Q ∈ RC×H×W , K ∈ RC×H×W , V ∈ RC×H×W means the
query, the key, and the value of feature maps, respectively, for
computing the self-attention:

orientations are split evenly into eight orientations with a self-
referential orientation as shown in Fig. 3. To properly estimate
the true orientation of GC by the PolarNet, one feature vector
3 ∼ 2
of input maps should represent the area of around 1
3
length of GC. Downsampling ratio in the 5th stage is just
hitting this range. Under the resolution of image in this work,
0.2499 µm/pixel, one 5th stage feature vector represents the
7.9968 µm length area, which is 1
5 times of GC, such
as the case in Fig. 3 (b). Thus, earlier stages, like 4th with
the ratio 24 or 3th with 23, are unsuitable to predict the polar
orientation of GC by the self-attention mechanism of eight-
neighbor of the PolarNet.

3 ∼ 4

Finally, based on the above knowledge, the GC detection
framework can be illustrated as Fig. 4. The conﬁdence of GC
bounding box is updated by

P = (1 − α)Pobj + αPpolar,

(6)

Q = Conv1

1×1(x),

K = Conv2

1×1(x),

V = Conv3

1×1(x).

B. Framework of GC Detection

(3)

(4)

(5)

where α ∈ [0, 1] is the weight of polar salience, and Pobj
means the objectiveness conﬁdence of original output of
model. Ppolar is the result by converting PAS matrix to one
scalar

Ppolar =

1
8bwbh

bx+ bw
2(cid:88)

by+ bh
2(cid:88)

1∼4,6∼9
(cid:88)

i=bx− bw
2

j=by− bh

2

n

PASn,i,j,

(7)

In the general object detection task, multiple models are
designed by using different architectures. Such as the anchor-
the
based single-step architecture of YOLO series [33],
multiple-step Faster RCNN [34] and Cascade RCNN [35], and
the anchor-free FCOS [36]. They are both structured by the
backbone and the head, where the backbone can employ a
neck, like a FPN [29] or a PAFPN [37], to mix multi-scale
feature maps.

Particularly, whether using the neck in the backbone or not,
the PolarNet is plugging into the end part. It computes the
last stage of feature maps, the 5th stage (downsampling ratio
is 25 = 32), to generate y and PAS. The new feature maps
y, then, input to the head of model to predict bounding boxes
of GC.

The reason of using the last stage of feature maps is
pursuing the best ﬁtness between the area size of one feature
vector and the physical size of GC. In the PolarNet, polar

where bx, by, bw, bh mean the coordinator and the size of one
bounding box. It is clear that Eq. 7 computes the mean of
polar attention score to represent the polar salience of GC as
shown in Fig. 3 (a).

In the training phase, the PolarNet is supervised by cross
entropy as similar as the objectiveness loss in modern object
detection model

LP olarN et = log(N LLLoss([Pnon−polar Ppolar], Pgt)), (8)

where Pgt means the objectiveness ground truth of one bound-
ing box, and the operation combination of log, N LLLoss, and
sof tmax in Eq. 1 forms the cross entropy loss with

Pnon−polar =

1
bwbh

bx+ bw
2(cid:88)

by+ bh
2(cid:88)

i=bx− bw
2

j=by− bh

2

PAS5,i,j.

(9)

Feature of eight-neighbor of VPolar attention of eight-neighborWeighted feature9×19×C9×C9×Crepeatelement-wise productsum1×Cpolar attention scoreConv 1x1Conv 1x1Dot-product eight-neighborfeature mapQKC×H×WC×H×WC×H×WConv 1x1C×H×W(i, j)iji-1, j-1i-1, ji-1, j+1i, j-1i, ji, j+1i+1, j-1i+1, ji+1, j+1(i, j) feature of Q(i, j) eight-neighbor of KDot-productC×19×Cpolar attention score of eight-neighbor9×1Weight eight-neighborfeature mapC×H×WsoftmaxVΣΣΣ/8self-referentialn = 1n = 2n = 3n = 4n = 6n = 7n = 8n = 9n = 5self-referentialn = 1n = 2n = 3n = 4n = 6n = 7n = 8n = 9n = 5jComputing the mean of polar attention scoreEvenly split orientations (a)(b)n = 2n = 3n = 4n = 6n = 9n = 7n = 8one glandular cellone eight-neighborn = 1n = 5ZIQUAN WEI et al.: CERVICAL GLANDULAR CELL DETECTION FROM WHOLE SLIDE IMAGE WITH OUT-OF-DISTRIBUTION DATA

5

Fig. 4. Glandular cell detection framework with PolarNet, where Hin, Win are the height and the width of input image, respectively.

TABLE I
DATA SET DETAILS.

source

GC ann.

train
val
test
test
test

slide num.
221
66
44
45
110
486

17190
1331
1364
0
NC ann.
0
FP
total
19885
Note: num. means number, ann. means annotation, NC means
non-relevant content, FP means false positive by a modern model.

image num. AGC num. nGEC num.
8759
258
226
0
0
9243

8274
1032
779
1280
3496
14861

At this point, any modern model in the object detection task
is able to use the prior knowledge, the ubiquitous polarity of
GC, to reject false positives of GC from WSI.

IV. DATA PREPARATION

This work uses total 486 cervical cytology WSIs from
Tongji Union Hospital, Huazhong University of Science and
Technology. The scanner used a 20× objective lens with a
resolution of 0.2499 µm/pixel, and used Qupath software
[38] to complete the local annotation of cervical GC in WSIs.
According to cervical GCs are more sparse than squamous
cells, three different sources are set up to make the image
datasets for full validation and testing. As shown in Table I,
the three sources are GC annotation, non-relevant content (NC)
annotation and false positive (FP).

Among them, the GC annotation refers to the area con-
taining glandular cells (clumps) in the cervical slide that is
judged to be positive (with the presence of signiﬁcant atypical
GCs), and has two subclasses: AGC (Atypical Glandular
Cells) and nGEC (normal Glandular Epithelial Cells). Non-
relevant content (NC) annotation refers to some small areas
from negative WSIs (no signiﬁcant atypical glandular cells
are present) that do not contain any GC. False positive (FP)
refers to the wrong predictions in the test set of GC annotation
source by a modern model, YOLOX-l [9]. The source of the
ﬁrst GC annotation is provided by the pathologist, and the
latter two are generated by authors. All annotations have been
reviewed by the pathologist for the double check.

Images listed above are used with the same size, 1024 ×

1024, and the same original resolution.

• While the images are from GC annotations, ﬁrst, those
GC annotations on a slide that are close enough (smaller
than the size of image) are used as a set. Then, a larger

Fig. 5.
Examples of GC used in this work. (a) Image with nGEC
annotations (green boxes). (b) Image containing non-relevant content.
(c) Image with false positives (orange boxes) by the well-trained YOLOX-
l.

cell image is cropped from WSI centered on such set
with the size of 1536 × 1536 and saved in the hard
disc. During training, 1024 × 1024 regions are randomly
cropped from the larger images to ensure the diversity of
learning samples, and 1024 × 1024 are cropped from the
center for validation and testing.

• While the images are from non-relevant content annota-
tion, 1024 × 1024 cell images are cropped from WSI,
randomly, for testing.

• The false positive source is obtained by a YOLOX-l [9]
model that trained with the ﬁrst source, GC annotations,
in our data. It is from external positive WSIs (n = 110)
that are entirely inferred by the well-trained YOLOX-
l. The top bounding boxes ranking by the conﬁdence
(named as top-N results) of every WSI are visualized
using the Qupath tool, and manually judged one by one.
Results that are not GCs are then cropped in the same way
of above sources for testing. During the manual judge-
ment, all top-100 in the ﬁrst 30 slides and 20 random
results of top-100 in the last 80 slides are reviewed.
Among the results of above manual judgement, it is inter-
esting that the average accuracy of ﬁrst 30 is only 0.1445.
Although this model
the
accuracy of only 2 slides exceeded 0.9 due to the fact that the
cervical GC’s prior knowledge is not taken into the model. It
is worth noting that this phenomenon of external testing on
YOLOX-l indicates that standard experimental results, which
shown an average precision of more than 0.7 on the validation
set, are unreliable when facing the sparse GC from real WSI.
Some examples are shown in Fig. 5.

is very popular in the industry,

It is worth mentioning that testing with the non-relevant
and false sources, the images without the presence of GCs

Inputfeature mapHeadfeature mapBackbonepolar attention scorePolarNetoriginaloutputfinal outputUpdate Conf P(a)(b)(c)6

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022

Fig. 6. Precision-recall curves on three test data setting.

accounted for 62.17% and 81.78% of the total, respectively.
Therefore, the unreliability of the model can be fully demon-
strated in both. In fact, the proportion of images without GCs
in the whole slide generally exceeds 90%, so testing with those
two sources can simulate the performance of deep learning
models for GC detection from WSI.

V. EXPERIMENTS

A. Evaluation Criteria

First of all, for the GC detection from 1024 × 1024 image,
this work uses the common evaluation criteria, AP50 (Average
Precision with 50% IoU threshold), and its calculation formula
is

Recall =

T P
T P + F N

, Precision =

T P
T P + F P

,

(10)

where T P , F P , and F N are true positive, false positive,
and false negative, respectively. Since AP50 is obtained by
calculating the area under the Precision-Recall curve, we also
provide the curve to show the GC detection performance of
models in detail.

Second, for the task of GC detection from cervical cytology
WSI, this work uses the top-N accuracy of GC to demonstrate
2(T P +F P ) +
the reliability of models. It is calculated by
2(T N +F N ) with T N the true negative.

T N

T P

B. Training Environments

All the experiments in this work are implemented with the
PyTorch deep learning library [39] on a Win10 OS computer.
Model training is performed using the SGD optimizer [40],
and the learning rate descent strategy is the common stepwise,
from 5 × 10−3 to 5 × 10−6, learned at epochs 25, 50, and 80,
respectively. The rate decay is 0.1, and the maximum training
epoch is 100. Memory is 128 Gb, CPU is a Xeon® 6134
@3.20 GHz, GPU is a Tesla P40. The comparison methods
are trained using the code published by the relevant research,
and the general popular model uses the implementation version
of TorchVision model zoo. When using the proposed PolarNet,
only one hyper-parameter α is always set as 0.5.

TABLE II
GC DETECTION AP50.

Model Name

val
GC
0.570 0.438
AttnFaster + R50
0.323 0.272
FCOS + R50
∼ w/ PolarNet
0.354 0.312
0.592 0.358
YOLOX-m
∼ w/ PolarNet
0.528 0.413
0.499 0.422
Faster RCNN + R50
∼ w/ PolarNet
0.513 0.440
Cascade RCNN + R50 0.437 0.294
∼ w/ PolarNet
0.444 0.328

test
GC GC+NC GC+FP
0.238
0.112
0.096
0.275
0.292
0.290
0.295
0.208
0.210

0.319
0.183
0.181
0.315
0.355
0.360
0.372
0.264
0.271

average of test

0.332
0.189
0.196 (+0.007)
0.316
0.353 (+0.037)
0.357
0.369 (+0.012)
0.255
0.270 (+0.015)

C. Comparing Methods

Regarding the comparison methods, since YOLOX and
Faster RCNN are representative single-stage and double-stage
methods commonly used in general tasks. AttnFaster [10], that
is specially designed for cervical cytology based on attention
mechanism, is also chosen to test. In addition, more existing
models are compared: one anchor-free model FCOS, and one
multi-stage model Cascade RCNN [35].

D. GC Detection Results

As mentioned in Section IV,

the experimental data in
this work were produced by three different sources, each of
which represented different levels of sparsity and difﬁculty
of GC detection, to verify the effectiveness of models on
real distribution of data from WSI. For fully comparing the
different testing scenarios, this section ﬁrst shows the test
results on all three sources of data, and then shows the detailed
test results of GC, which has two subclasses.

1) Main results: The GC detection results from small images
are shown in Table II. It is obvious that all SOTA models with
PolarNet appear the improvement of AP50 comparing to their
original version. That can show the effectiveness of PolarNet.
And, Faster RCNN with PolarNet always shows the highest
AP50 on every test setting and the average. YOLOX with
PolarNet is able to exceed AttnFaster in average by 0.021,
while the original YOLOX is 0.016 lower than AttnFaster.
Interestingly, comparing the results between three different

testGCtestGC+NCtestGC+FPZIQUAN WEI et al.: CERVICAL GLANDULAR CELL DETECTION FROM WHOLE SLIDE IMAGE WITH OUT-OF-DISTRIBUTION DATA

7

TABLE III
DETAILED RESULTS OF GC DETECTION.

nGEC AGC AP50 AP60 AP70
Model Name
AttnFaster
0.267 0.079 0.238 0.168 0.077
Faster RCNN 0.328 0.075 0.290 0.207 0.091
∼ w/ PolarNet 0.336 0.083 0.295 0.210 0.093
0.316 0.184 0.275 0.184 0.080
YOLOX-m
∼ w/ PolarNet 0.325 0.112 0.292 0.190 0.078
Note: results of subclasses nGEC and AGC are both
AP50.

TABLE IV
WSI GC DETECTION TOP-N ACCURACY IN AVERAGE.

Model Name Npred
Top-20
Top-10
Top-5
0.271±0.241
0.284±0.274
0.281±0.311
20.00
AttnFaster
0.355±0.261
0.410±0.303
0.424±0.344
20.00
Faster RCNN
0.429±0.364
0.386±0.282
0.411±0.344
∼ w/ PolarNet
20.00
0.117±0.236 0.0.108±0.212 0.122±0.204
18.89
YOLOX-m
0.126±0.149
0.205±0.267
0.190±0.283
∼ w/ PolarNet
20.00
Note: Npred means the average number of predictions in every WSI,
± followed by the standard deviation of accuracies of all WSIs.

test settings, the AP50 of only GC source can maintain a
similar level of AP50 as the validation set. But after adding
images without target presence (non-relevant or false positive
source), AP50 appears some relatively signiﬁcant drops. Even
for the most complex Cascade RCNN, which performs best
on the general dataset, the AP50 of test set GC+FP is only
0.255. Due to the lower AP50 scores of FCOS and Cascade
RCNN among the general models, we use the more powerful
and more general YOLOX and Faster RCNN for subsequent
experiments.

2) Precision-recall curves: The precision-recall (P-R) curves
of above models are shown in Fig. 6. It can be seen that
the green of PolarNet used in the P-R curve of the Faster
RCNN model has higher precision than orange and blue, and
the red P-R curve of YOLOX is also lower in precision than
adding PolarNet. YOLOX. For Recall, P-R curves for the
same model infrastructure all exhibit similar maximum recall.
While two YOLOXs’ recalls are signiﬁcantly lower than Faster
RCNN, their precisions are higher when curves start. The
relatively low precision at the beginning of the curve means
it will be less effective when encountering external test sets.
This is because the curve is drawn with the results sorted by
conﬁdence, and hence the results with high conﬁdence are less
accurate when that happened. It can be seen that YOLOX after
adding PolarNet is improved at the beginning of the curve.

3) Detailed results: In object detection task, the calculation
of AP criteria depends on the threshold of the intersection over
union between bounding boxes of predictions and annotations.
For example, AP50 used above needs to be greater than 50% to
be judged as T P . Obviously, a larger threshold can reﬂect the
accurate size and position of predictions. As shown in Table.
III, in order to demonstrate the accuracy of sizing and posi-
tioning of predictions, we list the results in AP60 and AP70.
It can be seen that, similar to the trend in Table. II, AP60 and
AP70 of Faster RCNN with PolarNet still both perform best
on test set of the GC+FP source, while YOLOX with PolarNet
ranks third in AP60 and AP70 . Obviously, compared to AP50,
all models show a small drop at AP60 and a signiﬁcant drop
at AP70. This shows that the accurate detection of cervical
GC is still a challenging problem. Furthermore, AP50 of two
subclasses are also shown in Table. III. The two models using
PolarNet performed the second with 0.112 and the ﬁrst with
0.336 in AGC and nGEC, respectively. And, AP50 of AGC
is generally lower than that of nGEC, because the number of
AGC annotations in the training data is less than half of that
of nGEC.

E. WSI GC Detection Top-N Results

When the cervical GC detection model is practically ap-
plied to cervical cancer CAD, the top-N results can guide
pathologists to prioritize the screening of suspicious lesions.
the top-N results of models on those external
Therefore,
slides (n = 110) can demonstrate its performance from the
perspective of cervical cancer auxiliary diagnosis on WSI.
This section shows the accuracy of top-N results, with higher
to be
accuracy indicating greater potential for the model
applied to CAD.

As shown in Table. IV, the accuracy of top-N results when
N = 5, 10, and 20 are listed respectively. It is clear that
Faster RCNN with PolarNet performs the best among the
three different N . As N decreases, its accuracy increases
from 0.386 to 0.429. This shows that it has a reasonable
conﬁdence distribution, such as higher conﬁdence results have
higher accuracy. In fact, Faster RCNN is the only remaining
model with the same trend, and the other remaining three
models show a drop in accuracy when using higher conﬁdence
results. In YOLOX models, its accuracy is much lower than
the other models as shown in Fig. 7. This situation can also be
reﬂected from Npred, the average number of predictions. There
are some slides that cannot give the complete 20 detection
results, even though YOLOX used a very low conﬁdence
threshold (0.0001). Although adding PolarNet can alleviate
this situation, the top-N accuracy of YOLOX with PolarNet
is still at least about 0.08 lower than the double-stage Faster
RCNN. This shows that the double-stage model is the most
reliable when faced with real external WSI comparing with
others.

F. Ablation Studies

In order to further reveal the role of PolarNet in object
detection models and prove the effectiveness of polar attention

Fig. 7. The box plot showing the distribution of WSI GC detection top-20
results.

AttnFaster~ w/ PolarNetFaster RCNNYOLOX~ w/ PolarNetAttnFaster~ w/ PolarNetFaster RCNNYOLOX~ w/ PolarNetconfidenceTPFP8

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022

TABLE V
GC DETECTION AP50 ON TEST SET USING DIFFERENT FEATURE
SCALES.

Scales

25
24
23
22

Faster RCNN w/ PolarNet
GC+FP
GC+NC
GC
0.295
0.376
0.443
0.256
0.321
0.382
0.228
0.320
0.396
0.247
0.338
0.444

YOLOX-m w/ PolarNet
GC+NC
0.355
0.324
0.266
-

GC+FP
0.292
0.222
0.201
-

GC
0.413
0.436
0.343
-

Fig. 8. α-AP50 curves to show the signiﬁcance of Ppolar, the mean
value of polar attention score.

for cervical GC detection, two ablation studies are performed
in this section: 1) The contribution of Ppolar, the mean of polar
attention score, in the conﬁdence P updating Eq. 6. 2) The
effect of feature scale (see Section III-B) on the estimation of
GC polarity in PolarNet.

1) The contribution of Ppolar: Based on Eq. 6 that chang-
ing the value of α can change the contribution of Ppolar.
Especially, when α = 0, Ppolar contributes nothing in the
role of PolarNet but only the new feature maps guided by
polar attention. Therefore, observation of the α-AP50 curve
can simultaneously demonstrate the effectiveness of Ppolar and
polar attention matrix.

Fig. 8 shows the α-AP50 curve by two types of model.
It can be seen that whether it is a single-stage YOLOX or a
multi-stage Faster RCNN, the AP50 raises when α increasing,
and reaches a peak after the default setting α = 0.5 in this
work. This indicates that the polar score provides a positive
contribution to the GC detection results. Moreover, it can
be seen that when α = 0, the AP50 of the model is still
superior to the results of other models in Table. II on the
three test settings. This shows that polar attention also provides
an important contribution to the feature representation of the
model.

2) The effect of feature scale: As described in Section. III-
B, since the physical size of a single GC is relatively ﬁxed,
PolarNet calculates its polarity orientation by feature maps
that are sensitive to the scale of GC. This part veriﬁes that the
scale, 25, in 5th stage that covers about 1
5 of the cell size
obtains optimal polar attention. We test all available feature
scales for both the single-stage model and the double-stage
model, showing the effect of feature scale on PolarNet.

3 ∼ 4

As shown in Table. V, the results AP50 of cervical GC
detection using different scales of feature for PolarNet. Obvi-
ously, the model using the largest scale, 25, performs the best
on all test settings. This shows that the theoretical foundation
for computing polar attention is valid. In addition, it can be
seen that after PolarNet uses smaller scales, most of the AP50,
such as non-bold and non-underlined, are even lower than the
corresponding original model shown in Table. II. It shows
that when PolarNet uses an inappropriate scale, the polarity
calculation via self-attention mechanism in eight-neighbor no
longer has a positive effect.

TABLE VI
COMPUTATIONAL COSTS OF GC DETECTION FROM WSIS (n = 110).

Model Name
AttnFaster
Faster RCNN
∼ w/ PolarNet
YOLOX-m
∼ w/ PolarNet

Total Time
6.8 hr
7.5 hr
7.9 hr (+0.4)
3.3 hr
4.6 hr (+1.3)

Average Time
221.3 s
244.3 s
258.7 s (+14.4)
107.6 s
151.1 s (+43.5)

Top-20 Acc.
27.1%
35.5%
38.6% (+3.1)
12.2%
12.6% (+0.4)

G. Qualitative Results

Finally, Fig. 9 shows the detection examples of cervical GC
for the four models for comparison, including the detection
examples of two groups of independent GCs and another two
of GC clusters. First, in all examples by every model, it is
difﬁcult to avoid giving false GCs (orange box), which is
caused by the sparseness of GC in WSI. Then, compared
with Faster RCNN and YOLOX-m, pulgging PolarNet
to
modern models can generally reduce the conﬁdence of false
bounding boxes of GC. In these four sets of examples, except
for the Faster RCNN in the second row, all the others have
removed false positives (orange boxes) by PolarNet’s scoring
of polar attention, and in some cases even corrected false
negatives, such as the ﬁrst and second lines, where new blue
box is appearing. These examples visually illustrate the role
of PolarNet in GC detection.

H. Computational Costs

In additional, this work also complete the C++ deployment
of the trained cervical GC detection model.1 The C++ program
is able to uses any modern model to complete the cervical
GC detection from WSI. The only need is to change the
model path and set some hyper-parameters in the command
line. To test computational costs, this section involves 5 object
detection models that tested in above experiments: AttnFaster,
Faster RCNN, Faster RCNN with PolarNet, YOLOX-m, and
YOLOX-m with PolarNet.

Table. VI shows computational costs using the proposed
PolarNet in our C++ program in detecting GCs from WSIs. Al-
though the YOLOX model can achieve the minimum average
time of 107.6 s, its accuracy drops severely, only 12.2%. The
best model Faster RCNN with PolarNet improves the accuracy
by 3.1% compared with its original version by sacriﬁcing the
time-consuming of 14.4 s in average.

1C++ program is released in https://github.com/Chrisa142857/You-Only-

Look-Cytopathology-Once/tree/main/cpp

test: GCαAP50AP50αα(b)YOLOX-m  w/ PolarNet(a)Faster RCNN w/ PolarNettest: GC+NCtest: GC+FPtest: GCtest: GC+NCtest: GC+FPαααα=0.5α=0.5α=0.5α=0.5α=0.5α=0.5AP=0.4391AP=0.3633AP=0.2817AP=0.2915AP=0.3529AP=0.4126ZIQUAN WEI et al.: CERVICAL GLANDULAR CELL DETECTION FROM WHOLE SLIDE IMAGE WITH OUT-OF-DISTRIBUTION DATA

9

VI. CONCLUSION

Current cervical cell detection works generally do not
consider the morphology and sparseness of GC in WSI. But
the slender shape of GC is easily confused, which makes
the general model have serious false positives when detecting
WSI. In order to improve the reliability of GC detection
from WSI, a PolarNet is designed in this paper. It evalu-
ates the polarity of cells by calculating eight-neighbor self-
attention, and generates polarity score and polarity attention
to guide conﬁdences and feature maps, respectively, to elimi-
nate pseudo-GC with insigniﬁcant polarity. The experimental
results show that PolarNet can improve the Top-20 accuracy of
GC detection from WSI by 8.8% on the presence of sacriﬁcing
the computational time per slide of 14.4 s.

VII. DISCUSSION

In this paper, a novel network PolarNet is proposed, which
can effectively eliminate pseudo-GCs with insigniﬁcant po-
lar orientations in the detection results, and more reliably
complete GC detection from WSI. The network obtains the
polar average score in the detection frame by calculating the
eight-neighborhood attention score in the feature map to judge
whether the polarity direction of the cell is signiﬁcant, and can
be added to any general model as a module. In order to obtain
suitable polarity orientations, it uses the appropriate scale of
the 5th stage feature map, so that the polarity orientations
can be obtained from general object detection model to cover
3 ∼ 4
1
5 shape of a single GC. In experimental sections, in order
to demonstrate the detection performance from WSI, PolarNet
was ﬁrst tested on three small images with different degrees
of sparseness of GC and showed superior average accuracy,
and then top-N accuracy was also shown the cervical GC
detection from WSI. In addition, two ablation studies further
demonstrate the positive effect of polar attention score and the
new polar guided feature maps of PolarNet. In general, the
PolarNet designed based on the prior knowledge of cervical
GC can effectively improve the reliability of GC detection
from WSI with OOD data.

However,

there is still room to improve the proposal.
The current PolarNet obtains the polar attention score by
calculating the feature map at the ﬁxed scale, which is too
sensitive to the change of scale as shown in the second ablation
study. Although the scale of a single cervical WSI is hardly
to change, the network suffers when encountering data from
multiple sources. Therefore, it is very important to further
develop scale-invariant PolarNet.

Since the deployment of C++ program is working nice,
further clinical experiments could be carried out to verify
the effectiveness in real-world tasks. The proposed program
has a good improvement with a slight time cost under current
computational resources, and its usefulness and weakness can
be further revealed by running to the actual cervical cancer
CAD.

ACKNOWLEDGMENT

The authors want to thank pathologists and organizations
that provided the raw data and the manual annotations. As

well as the Collaborative Innovation Center for Biomedi-
cal Engineering and the Britton Chance Center and MOE
Key Laboratory for Biomedical Photonics should be greatly
appreciated for their platforms and devices. This work is
supported by the National Natural Science Foundation of
China (NSFC) projects (grant 61721092), China Postdoctoral
Science Foundation (grant 2021M701320) and the director
fund of the Wuhan National Laboratory for Optoelectronics
(WNLO).

REFERENCES

[1] Y. Li, J. Chen, P. Xue, C. Tang, J. Chang, C. Chu, K. Ma, Q. Li,
Y. Zheng, and Y. Qiao, “Computer-aided cervical cancer diagnosis
using time-lapsed colposcopic images,” IEEE Transactions on Medical
Imaging, vol. 39, no. 11, pp. 3403–3415, 2020. [Online]. Available:
https://dx.doi.org/10.1109/tmi.2020.2994778

[2] A. Pirovano, L. G. Almeida, S. Ladjal, I. Bloch, and S. Berlemont,
“Computer-aided diagnosis tool for cervical cancer screening with
weakly supervised localization and detection of abnormalities using
adaptable and explainable classiﬁer,” Medical image analysis, vol. 73,
p. 102167, 2021.

[3] F.-Q. Li, S.-L. Wang, and G.-S. Liu, “A bayesian possibilistic c-means
clustering approach for cervical cancer screening,” Information Sciences,
vol. 501, pp. 495–510, 2019.

[4] A. Pouliakis, E. Karakitsou, N. Margari, P. Bountris, M. Haritou,
J. Panayiotides, D. Koutsouris, and P. Karakitsos, “Artiﬁcial neural
networks as decision support tools in cytopathology: past, present, and
future,” Biomedical engineering and computational biology, vol. 7, pp.
BECB–S31 601, 2016.

[5] D. A. Van Valen, T. Kudo, K. M. Lane, D. N. Macklin, N. T. Quach,
M. M. DeFelice, I. Maayan, Y. Tanouchi, E. A. Ashley, and M. W.
Covert, “Deep learning automates the quantitative analysis of individual
cells in live-cell imaging experiments,” PLoS computational biology,
vol. 12, no. 11, p. e1005177, 2016.

[6] H. Jiang, Y. Zhou, Y. Lin, R. C. Chan, J. Liu, and H. Chen,
“Deep learning for computational cytology: A survey,” arXiv preprint
arXiv:2202.05126, 2022.

[7] X. Tan, K. Li, J. Zhang, W. Wang, B. Wu, J. Wu, X. Li, and X. Huang,
“Automatic model for cervical cancer screening based on convolutional
neural network: a retrospective, multicohort, multicenter study,” Cancer
Cell International, vol. 21, no. 1, pp. 1–10, 2021.

[8] M. Gupta, C. Das, A. Roy, P. Gupta, G. R. Pillai, and K. Patole, “Region
of interest identiﬁcation for cervical cancer images,” in 2020 IEEE 17th
International Symposium on Biomedical Imaging (ISBI).
IEEE, 2020,
pp. 1293–1296.

[9] Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Yolox: Exceeding yolo series

in 2021,” arXiv preprint arXiv:2107.08430, 2021.

[10] L. Cao, J. Yang, Z. Rong, L. Li, B. Xia, C. You, G. Lou, L. Jiang,
C. Du, H. Meng et al., “A novel attention-guided convolutional network
for the detection of abnormal cervical cells in cervical cancer screening,”
Medical image analysis, vol. 73, p. 102197, 2021.

[11] Z. Wei, S. Cheng, X. Liu, and S. Zeng, “An efﬁcient cervical whole slide
image analysis framework based on multi-scale semantic and spatial
deep features,” arXiv preprint arXiv:2106.15113, 2021.

[12] S. Cheng, S. Liu, J. Yu, G. Rao, Y. Xiao, W. Han, W. Zhu, X. Lv,
N. Li, J. Cai et al., “Robust whole slide image analysis for cervical
cancer screening using deep learning,” Nature communications, vol. 12,
no. 1, pp. 1–10, 2021.

[13] Y. Liang, Z. Tang, M. Yan, J. Chen, Q. Liu, and Y. Xiang, “Comparison
detector for cervical cell/clumps detection in the limited data scenario,”
Neurocomputing, vol. 437, pp. 195–205, 2021.

[14] C. Zhang, D. Jia, N. Wu, Z. Guo, and H. Ge, “Quantitative detection of
cervical cancer based on time series information from smear images,”
Applied Soft Computing, vol. 112, p. 107791, 2021.

[15] L. Zhang, L. Lu, I. Nogues, R. M. Summers, S. Liu, and J. Yao,
“Deeppap: deep convolutional networks for cervical cell classiﬁcation,”
IEEE journal of biomedical and health informatics, vol. 21, no. 6, pp.
1633–1643, 2017.

[16] P. Sukumar and R. Gnanamurthy, “Computer aided detection of cervical
cancer using pap smear images based on hybrid classiﬁer,” International
Journal of Applied Engineering Research, Research India Publications,
vol. 10, no. 8, pp. 21 021–21 032, 2015.

10

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022

Coleman et al., “Qupath: Open source software for digital pathology
image analysis,” Scientiﬁc reports, vol. 7, no. 1, pp. 1–7, 2017.
[39] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An
imperative style, high-performance deep learning library,” Advances in
neural information processing systems, vol. 32, 2019.

[40] L. Bottou, “Large-scale machine learning with stochastic gradient de-
scent,” in Proceedings of COMPSTAT’2010. Springer, 2010, pp. 177–
186.

[17] L. Zhang, H. Kong, C. Ting Chin, S. Liu, X. Fan, T. Wang, and
S. Chen, “Automation-assisted cervical cancer screening in manual
liquid-based cytology with hematoxylin and eosin staining,” Cytometry
Part A, vol. 85, no. 3, pp. 214–230, 2014.

[18] S. P B, F. Faruqi, H. K S, and R. Kudva, “Deep convolution neural
network for malignancy detection and classiﬁcation in microscopic
Journal of Cancer
images,” Asian Paciﬁc
uterine
Prevention, vol. 20, no. 11, pp. 3447–3456, 2019. [Online]. Available:
https://dx.doi.org/10.31557/apjcp.2019.20.11.3447

cervix cell

[19] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[20] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2015, pp. 1–9.

[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[22] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2016, pp.
2818–2826.

[23] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE conference on
computer vision and pattern recognition.

Ieee, 2009, pp. 248–255.

[24] H. Lin, Y. Hu, S. Chen, J. Yao, and L. Zhang, “Fine-grained
classiﬁcation of cervical cells using morphological and appearance
based convolutional neural networks,” IEEE Access, vol. 7, pp.
71 541–71 549, 2019. [Online]. Available: https://dx.doi.org/10.1109/
access.2019.2919390

[25] N. Dong, L. Zhao, C. H. Wu, and J. F. Chang, “Inception v3 based
cervical cell classiﬁcation combined with artiﬁcially extracted features,”
Applied Soft Computing, vol. 93, p. 106311, 2020.

[26] Y. Xiang, W. Sun, C. Pan, M. Yan, Z. Yin, and Y. Liang, “A novel
automation-assisted cervical cancer reading method based on convo-
lutional neural network,” Biocybernetics and Biomedical Engineering,
vol. 40, no. 2, pp. 611–623, 2020.

[27] Y. Liang, C. Pan, W. Sun, Q. Liu, and Y. Du, “Global context-aware
cervical cell detection with soft scale anchor matching,” Computer
Methods and Programs in Biomedicine, vol. 204, p. 106061, 2021.
[Online]. Available: https://dx.doi.org/10.1016/j.cmpb.2021.106061
[28] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”

arXiv preprint arXiv:1804. 02767, 2018.

[29] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2017, pp.
2117–2125.

[30] E. Hussain, L. B. Mahanta, H. Borah, and C. R. Das, “Liquid based-
cytology pap smear dataset for automated multi-class diagnosis of pre-
cancerous and cervical cancer lesions,” Data in brief, vol. 30, p. 105589,
2020.

[31] J. Ke, Y. Shen, Y. Lu, J. Deng, J. D. Wright, Y. Zhang, Q. Huang,
D. Wang, N. Jing, X. Liang et al., “Quantitative analysis of abnor-
malities in gynecologic cytopathology with deep learning,” Laboratory
Investigation, vol. 101, no. 4, pp. 513–524, 2021.

[32] X. Zhu, X. Li, K. Ong, W. Zhang, W. Li, L. Li, D. Young, Y. Su,
B. Shang, L. Peng et al., “Hybrid ai-assistive diagnostic model permits
rapid tbs classiﬁcation of cervical liquid-based thin-layer cell smears,”
Nature communications, vol. 12, no. 1, pp. 1–12, 2021.

[33] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look
once: Uniﬁed, real-time object detection,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 779–
788.

[34] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international

conference on computer vision, 2015, pp. 1440–1448.

[35] Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality
object detection,” in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2018, pp. 6154–6162.

[36] Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-
stage object detection,” in Proceedings of the IEEE/CVF international
conference on computer vision, 2019, pp. 9627–9636.

[37] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network
for instance segmentation,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2018, pp. 8759–8768.
[38] P. Bankhead, M. B. Loughrey, J. A. Fern´andez, Y. Dombrowski, D. G.
McArt, P. D. Dunne, S. McQuaid, R. T. Gray, L. J. Murray, H. G.

ZIQUAN WEI et al.: CERVICAL GLANDULAR CELL DETECTION FROM WHOLE SLIDE IMAGE WITH OUT-OF-DISTRIBUTION DATA

11

Fig. 9. GC detection examples by four models to show the ability of rejection of false GCs after using PolarNet, where green boxes are ground
truth, oranges are false positives, and blues are true positives.

YOLOX-mYOLOX-m w/ PolarNetFaster RCNNFaster RCNN w/ PolarNetnGEC:0.12AGC:0.10nGEC:0.12nGEC:0.12nGEC:0.68nGEC:0.67nGEC:0.63nGEC:0.13nGEC:0.60nGEC:0.70nGEC:0.12nGEC:0.37nGEC:0.80nGEC:0.36nGEC:0.24nGEC:0.45nGEC:0.28nGEC:0.10nGEC:0.13AGC:0.41nGEC:0.73nGEC:0.12AGC:0.40nGEC:0.38AGC:0.70nGEC:0.13AGC:0.24nGEC:0.53AGC:0.73AGC:0.18nGEC:0.76nGEC:0.34nGEC:0.13nGEC:0.11nGEC:0.13nGEC:0.66nGEC:0.91nGEC:0.20AGC:0.33AGC:0.56nGEC:0.31nGEC:0.28nGEC:0.75nGEC:0.81AGC:0.730.13C:0.82C:0.78