Intelligent Residential Energy Management System using Deep
Reinforcement Learning

Alwyn Mathew

Abhijit Roy

Jimson Mathew

Indian Institute of Technology Patna
{alwyn.pcs16, abhijit.cs15, jimson}@iitp.ac.in

Abstract

1

Introduction

0
2
0
2

y
a
M
8
2

]
P
S
.
s
s
e
e
[

1
v
9
5
2
4
1
.
5
0
0
2
:
v
i
X
r
a

The rising demand for electricity and its essential na-
ture in today’s world calls for intelligent home energy
management (HEM) systems that can reduce energy
usage. This involves scheduling of loads from peak
hours of the day when energy consumption is at its
highest to leaner oﬀ-peak periods of the day when en-
ergy consumption is relatively lower thereby reducing
the system’s peak load demand, which would conse-
quently result in lesser energy bills, and improved load
demand proﬁle. This work introduces a novel way to
develop a learning system that can learn from expe-
rience to shift loads from one time instance to an-
other and achieve the goal of minimizing the aggregate
peak load. This paper proposes a Deep Reinforcement
Learning (DRL) model for demand response where the
virtual agent learns the task like humans do. The agent
gets feedback for every action it takes in the environ-
ment; these feedbacks will drive the agent to learn
about the environment and take much smarter steps
later in its learning stages. Our method outperformed
the state of the art mixed integer linear programming
(MILP) for load peak reduction. The authors have
also designed an agent to learn to minimize both con-
sumers’ electricity bills and utilities’ system peak load
demand simultaneously. The proposed model was an-
alyzed with loads from ﬁve diﬀerent residential con-
sumers; the proposed method increases the monthly
savings of each consumer by reducing their electricity
bill drastically along with minimizing the peak load on
the system when time shiftable loads are handled by
the proposed method.

Keywords: Home Energy Management, Reinforce-
ment Learning.

© 2020 IEEE. Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in
any current or future media, including reprinting/republishing
this material for advertising or promotional purposes, creating
new collective works, for resale or redistribution to servers or
lists, or reuse of any copyrighted component of this work in other
works. This work was partially supported by the Scheme for
Promotion of Academic and Research Collaboration (SPARC),
MHRD, Government of India under Grant # ID: P582.

1

Energy generated from power grids fuels the modern
lifestyle. Per-user consumption of energy is ever in-
creasing. People, nowadays, use a lot of modern ap-
pliances for their day to day chores. With technologi-
cal advances, the invention of new appliances, and the
ever-increasing interest of the new generations in the
gadget market, investment in the household appliance
market has increased manifold. With most of these
appliances running on electricity, the rising electricity
consumption also increases the load on power grids.
People nowadays are willing to pay more for electric-
ity rather than live without it. Household appliances
in the US are responsible for approximately 42% of
the total energy consumption [1]. Given the high de-
mand for electricity, eﬀorts are being made continu-
ously to improve smart grids with advanced research
in power system and computer science. Rising energy
requirement increases the load on power grids. Also,
energy consumption follow speciﬁc trends that lead to
disparity in demands from grids based on the time of
the day, i.e, energy demand during particular periods
can be higher than usual, whereas during other peri-
ods of the day energy requirements can be pretty low.
During peak hours, the load on power grids increases
drastically. To avert this problem, Demand Side Man-
agement (DSM) strategies are used. DSM strategies
involve Demand Response (DR), energy eﬃciency, and
conservation schemes.

DR [2, 3] focuses on modifying consumers’ demand
on the beneﬁt of reducing peak load on the grid and
in turn, giving some incentives back to customers who
participate in it. DR encourages consumers to use less
power during peak hours of the day or shift their time
of energy usage to oﬀ-peak hours of the day. Examples
of DR may include storing energy during the oﬀ-peak
periods from the grid and using the stored energy dur-
ing the peak period. One could also make use of renew-
able sources of energy by storing these energies such
as solar, wind, geothermal and biogas, which could be
used during the peak periods of the day. The bene-
ﬁts of DSM are discussed in [4]. Home Energy Man-
agement (HEM) [5] is a part of DSM. HEM systems
manage the usage of electricity in smart homes. DR is
a crucial component of HEM systems. DR [6] is one
of the techniques used in DSM. It involves scheduling

 
 
 
 
 
 
Figure 1: Proposed architecture of smart gird with RL-DSM at consumer end.

loads on the timescale by moving high wattage loads
to a diﬀerent time to reduce the maximum load on the
grid without changing the net energy consumed. It fo-
cuses on changing the “when consumed” rather than
changing the “how much consumed”.

Electricity generation involves several generating
stations employing diﬀerent generation technologies
working in conjunction, thus making the process dy-
namic in its behavior. This translates to varying costs
of electricity generation at any given point in time.
This is where load shifting comes into play. Scheduling
the load on the timescale that would reduce the overall
peak demand on the grid and also saves electricity bills
for the consumers. Most of the DR based optimiza-
tion models are based on two broad categories: price-
based and incentive-based. Price based optimization is
discussed in the study conducted in this paper. The
price-based models consider Time of Use (ToU) pric-
ing, peak load pricing, critical peak pricing, and real-
time pricing [7, 8], which take into account the peak
and oﬀ-peak tariﬀs. The varying tariﬀs of energy con-
sumption during the entire duration of a day based on
the aggregate load on the electrical systems act as a
motivation for consumers to adjust their appliance us-
age to take advantage of the lower prices during speciﬁc
periods. Incentive-Based DR programs are two types:
Classical programs and Market-based programs. Clas-
sical programs include Direct Load Control and Inter-
ruptible programs, Market-based are Emergency DR,
Demand Bidding, Capacity Market, and Ancillary ser-
vices market [9]. In [10], the authors proposed a pric-
ing scheme for consumers with incentives to achieve
a lower aggregate load proﬁle. They also studied the
load demand minimization possible with the amount
In [11] linear
of information that consumers share.
and nonlinear modeling for incentive-based DR for real
power markets was proposed. System-level dispatch
of demand response resources with a novel incentive-
based demand response model was proposed by [12].
In [13], the authors proposes an Interruptible program,
including penalties for customers in case they do not
respond to load reduction. A real-time implementation
of incentive-based DR programs with hardware for res-
idential buildings is shown in [10]. In [14] a novel DR
program targeting small to medium size commercial,
industrial, and residential customers is proposed.

Reinforcement Learning (RL) [15] is an area of ma-
chine learning in computer science where a learning

agent interacts with an environment and receives re-
wards as feedback of the interaction with the ulti-
mate goal of maximizing the cumulative reward. To
achieve this, RL agents attempt to come up with a
policy mapping states to the best action at any given
state, which would result in maximum expected future
rewards. Well designed RL agents have displayed im-
peccable decision-making capabilities, such as Google’s
AlphaGo and OpenAI Five, in complex environments
without the requirement of any prior domain knowl-
edge. Deep Reinforcement Learning (DRL) [16] is a
merger of deep learning and reinforcement learning
where deep learning architectures such as neural net-
works are used with reinforcement learning algorithms
like Q-learning, actor-critic, etc. [17] discusses build-
ing DRL agents for playing Atari games and how DQN
(Deep Q-Network) shows exceptional performance in
playing the games.

The proposed work aim at applying deep reinforce-
ment learning techniques to the scenario of load shift-
ing and comparing the results obtained with that of
the MILP [18–20] based methods. We also propose a
smart grid architecture, as shown in Figure 1 where RL
based DSM controller can be placed at the consumer
end. The grid end DSM with RL is an extension of this
work, where raw loads come from residential microgrids
instead of individual homes. An automated RL agent
performing the task of load shifting would go a long
way into optimizing load consumption by distributing
loads from peak to oﬀ-peak periods, thus reducing the
total load on the power grid during peak periods and
reducing the energy bills of consumers. The main con-
tributions of the proposed work are:

1. Introduced Deep reinforcement learning in De-

mand Side Management (RL-DSM) for DR.

2. Analyzed that the impact of a well-calculated re-
ward system is crucial for Demand Side Manage-
ment Reinforcement Learning models.

3. The proposed reinforcement learning model sur-
passed traditional methods with a single objective
by saving 6.04% of the monthly utility bill.

4. The proposed reinforcement learning model with
multi-objective saved 11.66% of the monthly util-
ity bill, which shows the superior ability of the
learning model over traditional methods for De-
mand Side Management.

2

2 Related Work

Demand response optimization is extensively explored
in literature. [21] gives an overview of DSM with var-
ious types and the latest demonstration projects in
DSM. [22] discusses a greedy iterative algorithm that
enables users to schedule appliances. [23] presents lin-
ear programming based load scheduling algorithms.
Mixed-integer linear programming model for optimal
scheduling of appliances has been discussed in [18–20].
Heuristic-based scheduling algorithms that aim at cost
minimization, user comfort maximization, and peak to
average ratio minimization have been discussed in de-
tail in [24]. A constrained multi-objective scheduling
model for the purpose of optimizing utility and min-
imizing cost is proposed in [25]. A dynamic pricing
model for energy consumption cost minimization and
comfort maximization in the context of smart homes
has been explored in [26]. A study of various control
algorithms and architectures applied to DR was car-
ried out in [27]. [28] proposes a demand response cost
minimization strategy with an air source heat pump
along with a water thermal storage system in a build-
ing. [29] studies various energy demand prediction ma-
chine learning models like feed-forward neural network,
support vector machine, and multiple linear regres-
sion. [30] proposes a systematic method to quantify
building electricity ﬂexibility, which can be an assur-
ing demand response resource, especially for large com-
mercial buildings. [31] studies the eﬀect of demand re-
sponse on energy consumption minimization with the
heat generation system in small residential builds in a
more frigid climate.

Increasing demand for energy has shifted the focus
of the power industry to alternative renewable sources
of energy for power generation. Integrating renewable
sources of energy into HEM systems can be beneﬁcial
for both customers and power generation companies.
Renewable sources of energy can ﬁll in the excess de-
mand for electricity by customers, thus moderating the
peak loads on the grids. Aghaei et al. carried out stud-
ies on DR using renewable sources of energy [32]. [33]
introduces a two-stage power-sharing framework. [34]
talks about a cloud-based DSM method where con-
sumer also has local power generation with batter-
ies. [35] discusses the design for smart homes with the
integration of renewable energy sources for peak load
reduction and energy bill minimization. [36] introduces
a real-time decentralized DSM algorithm that takes ad-
vantage of energy storage systems (ESS), renewable en-
ergy, and regularizing charging/discharging. [37] pro-
poses a method to improve DSM by optimizing power
and spectrum allocation. [38] proposes a hierarchical
day-ahead DSM model with renewable energy.
[39]
discusses an Intelligent Residential Energy Manage-
ment System (IREMS), which oﬀers a model with in-
house power generation using renewable sources of en-
ergy. [40] proposes an algorithm to minimize consumer
cost by facilitating energy buying and selling between
utility and residential community. [41] oﬀers a multi-
objective to demand response program by reducing the

energy cost of residential consumers and peak demand
of the grid. [42] introduces collaborative energy trad-
ing and load scheduling using a game-theoretic ap-
proach and genetic algorithm. [43] propose a game-
theoretic approach to minimize consumer energy cost
and discomfort in a heterogeneous residential neigh-
borhood. [44] reduces the overall cost of the system by
optimizing the load scheduling and energy storage con-
trol simultaneously with Lyapunov optimization. [45]
proposes an intelligent residential energy management
system for residential buildings to reduce peak power
demand and reducing prosumers’ electrics bills. [46] in-
troduces an automated smart home energy manage-
ment system using L-BFGS-B (Limited-memory Broy-
den–Fletcher–Goldfarb–Shanno) algorithm with time-
of-use pricing to optimize the load schedule.

[47] introduces an RL model to meet the overall de-
mand with the current load, and the next 24 hrs load
predicted information by shifting loads. [48] formed
a fully automated energy management system by de-
composing rescheduling loads over device clusters. [49]
proposes the Consumer Automated Energy Manage-
ment System (CAES), an online learning model that
estimates the inﬂuence of future energy prices and
schedules device loads. [50] proposes a decentralized
learning-based multi-agent residential DR for eﬃcient
usage of renewable sources in the grid. Lu [51] dy-
namic pricing DR algorithm formulated as an MDP to
promote service providers proﬁt, reduce costs for con-
sumers, and attain eﬃciency in energy demand and
supply. [52] introduced RL based scheduling of con-
trollable load under a day-ahead pricing scheme.

This work encroaches into the territory of apply-
ing DRL to DR. Existing deep reinforcement learning-
based model for DR, which came out recently are
[53,54]. [53] discusses an RL and DNN based algorithm
modeled on top of real-time incentives. A complete re-
view of a collection of all control algorithms and their
assessment in DR strategies in the residential sector is
given in [54]. It discusses machine learning-based pre-
dictive approaches and rule-based approaches. Apply-
ing deep reinforcement learning models to the setting
of DR can be a lucrative ﬁeld of research. The pro-
posed model draws inspiration from DRL agents used
to play the game of Tetris. We have explored models
of DRL automated agents playing the game of Tetris
and have tried to look for areas of similarity in the
formulation of the reward function. [55] discusses the
design of a DQN model for playing Tetris and presents
the reward system of an automated Tetris agent.

3 Methods

Reinforcement learning has shown tremendous strides
in learning to take action in game environments sur-
passing humans. Bringing RL capability to DR cre-
ated the need to model DR into a game environment.
An Atari game called Tetris falls in very close with the
simulation environment needed for DR. In Tetris, the
player is asked to move blocks of diﬀerent sizes and

3

Figure 2: Demonstrates the diﬀerence in block settling in game environment (left) and proposed DR simulation (right).
Red, Green and Blue blocks aren’t supported by the grid base or another load block, thus they are allow to be broken
down and slid down to lower level in the simulation. Figure best viewed in color.

shapes in a 2D grid. Actions the player can take on
the block are rotation, moving left, right, or down. The
player is rewarded with points when a full line in the
grid is ﬁlled, and the game terminates when the block
placed touches the maximum height of the grid. We
adapted this game environment to build a simulation
to perform DR. The blocks in the game will be device
loads in the DR simulation. The player will be replaced
by an RL agent who will take action like moving load
blocks down, left, and right. Unlike solid blocks in the
game environment, the load blocks in the DR simula-
tion are ﬂexible solids, i.e., if part of the load in the
grid is not supported by the grid base or another load
block, it is allowed to slide down to the lower level as
shown in Figure 2. The agent reward is determined
by the load peak generated when a block is placed in
the simulation grid. A positive reward is given if it
doesn’t increase the current maximum load peak and a
negative reward when the current maximum load peak
increases. The simulation ends when the load placed
crosses the maximum height, which motivates the RL
agent to place more load in the simulation grid without
generating peaks.

3.1 Simulation Design

The simulation is modeled to represent loads on a
timescale of 24 hours. The simulation environment
consists of 24 columns, each column representing an
hour on the 24-hour clock. The height of each column
depicts the aggregate load on the electrical system at
that particular hour. Ideally the height of the simu-
lation grid is decided by the maximum aggregate load
from the historical load proﬁles of the consumers in a
speciﬁc grid. As a proof of concept, a maximum of
25kW of the aggregate load is set while training and
testing, this can be scaled according to the size of the
power grid. If the aggregate load at any hour exceeds
the constraints on maximum load, the simulation ter-
minates.

3.2 Delineating States and Actions

The learning algorithm stores data in the form of (state,
action, next state, reward ) tuples. The state is an im-
age of the current simulation screen. The action space
is deﬁned in terms of three actions: left, right and drop
which imply left move, right move and dropping the
block onto the simulation respectively. All actions are
single actions; i.e., for any state transition, only a sin-
gle action can be performed, there are no cases where a
combination of the individual actions can be performed
for a state transition. Taking action at any state causes
the game to transition to a new state that generates a
reward.

At any point of time if the agent decides on one of
the actions the simulation moves to a new state. A
right action shifts the current load one cell (represent-
ing timestamp) to the right and the new state is the
load block shifted one block to the right. A left action
shifts the block of load one cell to the left and the sim-
ulation transitions to a new state with the load block
shifted one cell to the left. A drop action results in the
load block being placed on the load proﬁle and a subse-
quent state transition with the load block now placed
on the load proﬁle immediately below the load block.
Actions in this sense are discrete and not continuous.
At any point of time the state of the simulation is the
load proﬁle and the load block. This is captured on
every action. The state transitions are ﬁnite as there
can be ﬁnite shapes of blocks and they can be placed in
on 24 timestamps. State transitions are deterministic
in the the sense that given a state, block, action we can
always predict the next state.

3.3 Deep-Q-Learning (DQN)

DQN [16] agent based on the idea of using a neural net-
work to approximate the below Q as shown in Equa-
tion 1, and the pipeline of this method can be found in

4

Algorithm 1.

Qπ(s, a) = max

π

Eπ[rt + γrt+1 + γ2rt+2+

.....|st = s, at = a]

(1)

where at is the action and st is the state at time t, rt is
the reward obtained by taking action at given state st,
π denote policy function deﬁnes the learning agent’s
way of behaving at a given time and γ is the discount
factor. The Q-function can be simpliﬁed as:

Q∗(s, a) = Es(cid:48)[r + γ max
a(cid:48)

Q∗(s(cid:48), a(cid:48))|s, a]

(2)

where s(cid:48) is the state generated by performing action
a in state s and a(cid:48) denoted the action taken in state
s(cid:48). The DQN model used in the proposed method is
a Double DQN [56]. Double DQNs handles the prob-
lem of the overestimation of Q-value. This consists of
two networks, namely a policy network and a target
network where all changes to the gradients are made
on the policy network, which is synced with the tar-
get network at regular intervals of episodes. Episode
is the length of the simulation at the end of which the
system ends in a terminal state. DQN network is used
to select the best action to take for the next state (the
action with the highest Q-value). The target network
is used to calculate the Q-value of taking that action
at the next state.

problem in the proposed model, as shown in Table 6
and Figure 6b. Even though the shallow network was
able to minimize cost, it creates high load peaks. With
batch normalization, the model normalizes the input
layer by adjusting and scaling the activation. Batch
normalization reduces the amount by what the hid-
den unit values of the network shift around (covariance
shift), and this is proven in [57] to speed up the learn-
ing process. RMSProp [58] is used for optimization in
the network.

For training, the network approximates the Q-values
for each action and take action with the maximum Q-
value. The target network estimate the Q-values of
the optimal Q-function Q* by using the Bellman Equa-
tion 2, since Q-function for any policy obeys the Bell-
man equation. Temporal diﬀerence error δ is computed
by taking the diﬀerence of predicted Q-value and the
optimal Q-value computed from the Bellman equation
using the target network.

δ = Q(s, a) − (r + γ max

a(cid:48)

Q∗(s(cid:48), a(cid:48)))

(3)

By minimizing the loss between the Q-value and the
optimal Q-value, the agent arrives at the optimal pol-
icy. Huber loss is used to minimize the error deﬁned
as below:

L =

1
|B|

(cid:88)

b(cid:15)B

L(δ)

(4)

where batches B of experiences (knowledge acquired in
the past) where sampled from the agent’s replay buﬀer
and L(δ) is deﬁned as

Figure 3: Network architecture used for agent. The policy
network and target network have the same architecture.
Convlayer consist of convolution block, batch normaliza-
tion layer and ReLU activation. F C stands for fully con-
nected layer.

The network consists of three convolution layers and
one fully connected layer as shown in Figure 3. All
convolution layers (CNN) use a 5 × 5 kernel and a
stride of 2 with batch normalization and ReLU (Rec-
tiﬁed Linear Unit) as the activation function. The last
hidden layer is a fully connected (FC) layer with 192
units, and the output layer consists of three units for
three actions. CNN is used over other network archi-
tectures because CNN is memory eﬃcient and best for
feature extraction. The number of parameters in FC
drastically increases with the increase in hidden layers
when compared to CNN, thus increases computation
and memory used. Designing a faster model is cru-
cial for applications like load shifting, thus memory
eﬃcient and fast neural networks like CNNs are used.
According to the design of the proposed RL state, CNN
is more apt as it is eﬃcient in feature extraction from
raw data with spatial structures like simulation screens.
We have also done an extensive analysis of how the se-
lected CNN network over other aﬀects the load shifting

L(δ) =

(cid:40) 1

2 δ2
|δ| − 1

|δ| ≤ 1
2 Otherwise

(5)

The Huber loss is robust to outliers. When the error
is small, it takes the deﬁnition of mean squared error
but acts like the mean absolute error when the error is
signiﬁcant.

3.4 Epsilon Greedy Policy and Experi-

ence Replay

Epsilon greedy policy and Experience replay are two
crucial techniques that help the learning process of the
RL agent drastically. Given the fact that the state
space is vast, the agent is enabled to explore the avail-
able state space initially without taking too many pre-
dicted actions from the network. When the agent takes
a random step at any given state its called exploration
and when it uses already accumulated experience to
make a predicted decision from the network is called
exploitation. Exploration and exploitation should not
be run exclusively. The agent explores many states
at ﬁrst by taking random actions, but with each suc-
cessive epoch, it increases the number of informed de-
cisions exploiting the known information to maximize
the reward. The decay function γ used for this pur-

5

Algorithm 1: DQN algorithm
Input: D: Empty replay buﬀer,

θ: Policy network parameters,
θ−: Copy of θ, target netparamss,
Nr: Replay buﬀer maximum size,
Nb: Training batch size,
N −: Target network replacement freq,
M : Number of episodes,
γ: Discount factor,
(cid:15)d: Decay rate of epsilon,
(cid:15)s: Starting value of epsilon,
(cid:15)e: Final value of the epsilon,
U nif (D): Uniforming sampling from D,
x: Empty frame sequence,
A: Action space, sd: Steps done,
grid: Matrix representing the state,
S0: Initial state of the game.

Set sd ←− 1
for episode ∈ {1,2,...,M} do

Set grid ←− S0
for t ∈ {0,1,2,...} do
Set state s ←− x;
Your command was ignored.Type I

¡command¿ ¡return¿ to replace it with
another command,or ¡return¿ to continue
without it.

Set

(cid:15) ←− (cid:15)e +

(cid:15)s + (cid:15)e
sd
(cid:15)d

e

Set r ←− random number between 0 and 1
if r > (cid:15) then

Set a ←− argmaxaQ(s, a; θ)

else

Set a ←− random action from A

Sample next frame X t from environment ε
given (s, a) and receive reward r and append
xt to x.
if

|x| > Nr then
Delete oldest xtmin from x

end
if

|D| ≥ Nr then
Set s(cid:48) ←− x, and add transition
tuples(s, a, r, s(cid:48)) to D, replace the oldest
tuple

end
Sample a minibatch of Nb,
tuples(s, a, r, s(cid:48)) ∼ U nif (D)
Construct target values, one for each the Nb
tuples:

yj =

(cid:40)
r
r + γmaxa(cid:48) Q(s(cid:48), a(cid:48); θ−)

if s’ is terminal
otherwise

Do a gradient descent step with loss
||yj − Q(s, a; θ)||2
Replace target parameters θ− ←(cid:45) θ every N −
steps.
Set sd ←− sd + 1
Update grid based on previous grid and the
new load.
if grid has a fully ﬁlled column then

Terminate episode

end

end

end

pose is the exponential decay function which is deﬁned
below:

γ = (cid:15)e +

(cid:15)s + (cid:15)e

sd
(cid:15)d

e

(6)

where sd is the total number of iterations till now
and (cid:15)d is a hyperparameter controlling the rate of de-
cay from (cid:15)s to (cid:15)e. One iteration translates to taking
one action. Note that episodes and iterations have dif-
ferent meanings in this context. Hyperparameters are
parameters that are manually set by the practitioner
and tuned according to a speciﬁc goal.

The Experience Replay [59] is used to avoid the agent
from forgetting previous experiences as it trains on
newer states and to reduce correlations between ex-
periences.
In Experience Replay, a buﬀer of the old
experiences is maintained and the agent is trained on
it. By sampling batches of experience from the buﬀer
at random, the correlation between the experiences can
be reduced. The reason for this is that if you were to
sample transitions from the online RL agent as they
are being experienced, there would be a strong tempo-
ral/chronological relationship between them. During
our experiments, we ﬁxed buﬀer size at 3 × 104. We
empirically found the optimal buﬀer size that works
best for our model without allotting huge amount of
reserved memory space. We have done extensive anal-
ysis on picking the right buﬀer size, which can be found
in Table 7 and Figure 6b.

3.5 Reward Systems

The proposed reward system consists of three impor-
tant aspects, maximum height hmax, variance of the
height distribution var(h) and number of complete
lines l formed from the distribution. The reward sys-
tem R is summarized as:

R = α1 ∗ var(h) + α2 ∗ l − α3 ∗ hmax

(7)

The variance of the height distribution var(h) is

given by:

var(h) =

1
1 + var(ha)

(8)

where var(ha) the variance of the height distribution

after taking the action.
The variance of

load distribution reward var(h)
would encourage the agent to shift load such that more
uniform distribution of height of the load proﬁle can
be attained. Complete lines l reward complements the
agent to increase the spread to have more complete
lines (rows) in the simulation. This reward component
enables the agent to decide to shift a load to either side
edges of the simulation. Maximum high reward com-
ponent hmax helps the agent to avoid making a peak
in the load proﬁle as this reward contributes negative
reward if the current maximum high increased. Other
hyperparameters used for the experiments are shown
in Table 2.

6

Appliances

Rated power (kWh) Preferred time Duration (hrs)

Refrigerator
TV
Indoor lighting
Oven
Stove
AC

Consumer 1
Non-shiftable

0.5
0.5
0.5
1
1
1.5

Time-shiftable

Washing machine
Dish washer
Vacuum cleaner
Grinder

1.0, 0.5
1.0
1.0
1.5

Consumer 2
Non-shiftable

Refrigerator
TV
Indoor lighting
Oven
Stove
AC

Washing machine
Dish washer
Vacuum cleaner
Grinder
Cloth dryer

Refrigerator
TV
Indoor lighting
Oven
Stove
AC

0.5
0.5
0.5
1
1
1.5

Time-shiftable

1.0, 0.5
1.0
1.0
1.5
0.5

Consumer 3
Non-shiftable

0.5
0.5
0.5
1
1
1.5

Time-shiftable

Washing machine
Dish washer
Vacuum cleaner
Cloth dryer

1.0, 0.5
1.0
1.0
0.5

Refrigerator
TV
Indoor lighting
Stove

Washing machine
Dish washer
Vacuum cleaner
Grinder
Cloth dryer

Refrigerator
TV
Indoor lighting
Oven
Stove
AC

Consumer 4
Non-shiftable

0.5
0.5
0.5
1
Time-shiftable

1.0, 0.5
1.0
1.0
1.5
0.5

Consumer 5
Non-shiftable

0.5
0.5
0.5
1
1
1.5

Time-shiftable

Washing machine
Vacuum cleaner
Grinder
Cloth dryer

1.0, 0.5
1.0
1.5
0.5

0-24
20-23
19-22
7-8
12-13
13-14

0-24
0-24
0-24
0-24

0-24
18-22
18-22
7-8
11-12
21-23

0-24
0-24
0-24
0-24
0-24

0-24
17-23
18-23
6-7
12-14
22-24

0-24
0-24
0-24
0-24

0-24
18-24
18-23
13-14

0-24
0-24
0-24
0-24
0-24

0-24
20-24
19-22
20-21
12-14
12-13

0-24
0-24
0-24
0-24

24
3
3
1
1
1

2
2
1
1

24
4
4
1
1
2

2
1
1
1
2

24
6
5
1
2
2

2
2
1
1

24
6
5
1

2
2
1
1
2

24
4
3
1
2
1

2
1
1
2

Table 1: Daily load demand of ﬁve diﬀerent consumers.
Rated power 1.0, 0.5 indicate the power rating at ﬁrst and
second hour of the device.

Hyper parameters Values

α1
α2
α3
α4
Buﬀer size
Learning rate

10.0
0.76
0.5
0.2
3 × 104
0.001

Table 2: Hyper parameters used in experiment.

Oﬀ-Peak Mid-Peak

Price (cents/kWh)
Peak
15-22hrs
15

Oﬀ-Peak
22-24hrs
0-6hrs
6
6
Table 3: Monthly energy billing scheme.

6-15hrs
9

The variance of load distribution encountered some
cases where the agent realised that placing the blocks
on the same position also lead to increase in variance
for a suﬃciently well spread out distribution. Due to
this, initially the agent would perform as expected by
placing blocks in a manner which caused the distribu-
tion to spread out, but after the certain point it would
start placing all the blocks at the same position, re-
sulting in the game terminating with poor results. To
counter this, standard deviation was utilized to prop-
erly scale the impact of distribution on the overall re-
ward vis-`a-vis the other reward parameters.

For simultaneous cost and peak minimization, an ad-
ditional reward term c is introduced to the peak min-
imization reward system. The cost is calculated with
the help of the piece-wise pricing adapted from [60] as
shown in Table 3. As the price of the load schedule
increases, the reward for the agent decreases; the term
helps the agent to shift load that results in lower billing
price, as shown in the Equation 9.

R = α1 ∗ var(h) + α2 ∗ l − α3 ∗ hmax − α4 ∗ c

(9)

4 Simulation Results and Dis-

cussion

The proposed RL model is tested in two diﬀerent case
studies with ﬁve customers load data adapted from
[18], as shown in Table 1.
In case 1, the utility en-
ergy bill is reduced with peak minimization objective.
Case 2 reduces the utility energy bill by lowering peak
and cost simultaneously. The models based on the pro-
posed peak minimization technique showcase better re-
sults than MILP.

4.1 Case 1

The objective of this case study is to minimize peak
load demand with RL and thereby reducing the energy
bill of the consumer. The model is tested with one
objective on ﬁve diﬀerent consumers, as discussed in
the above sections. DR with RL (using DQN agent)
scheduled load is compared with MILP scheduling for
all consumers in Figure 4. Figure 4 shows how the sys-
tem reacts to the aggregated load of all consumers. The
eﬀect of selecting the right reward function of the RL
model for scheduling each consumer load is shown in
Table 4 by comparing variance and standard deviation
as a reward function. The variance reward system out-

Continuous curves have been used for better visualization
instead of the discretized plot even though the load demand is
only captured at 24 timestamps.

7

(a)

(c)

(e)

(b)

(d)

(f)

Figure 4: RL-DSM results of residential loads for ﬁve diﬀerent consumers (a), (b), (c), (d) and (e) respectively with Case
1 objective of peak minimization which are compared with MILP. (f) is the RL-DSM results of aggregated residential
loads for ﬁve diﬀerent consumers to minimize daily peak load with Standard deviation(std RL) and Variance(var RL).
The ﬁgure best viewed in color.

Consumer

Consumer 1
Consumer 2
Consumer 3
Consumer 4
Consumer 5
All

Total energy bill ($/month)

Monthly savings ($)

without MILP RL with std RL with var MILP RL with std RL with var

81
92.25
87.75
88.2
82.8
432

77.85
90.5
89.55
78.75
81
418.05

76.95
82.8
87.75
80.1
82.8
410.4

74.25
81.9
79.65
80.1
78.05
391.95

3.15
1.35
-1.8
9.45
1.8
13.95

4.05
9.45
0
8.1
0
21.6

Table 4: Energy bill minimization with peak reduction (Case 1).

6.75
10.35
8.1
8.1
6.75
40.05

performed the standard deviation for all test consumer
data.

As shown in Table 4, the aggregated load reduced the
overall bill cost from $432 to $391.95, saving around
9.27%. From the results, it can be inferred that the
proposed method could reduce the monthly bill dra-
matically when compared to other traditional methods

on single peak objective minimization.

4.2 Case 2

In this case study, two diﬀerent objectives are min-
imized simultaneously with RL. Here, the model re-
duces the peak load and cost of the load schedule. This

8

(a)

(c)

(e)

(b)

(d)

(f)

Figure 5: RL-DSM results of residential loads for ﬁve diﬀerent consumers (a), (b), (c), (d) and (e) respectively with Case
2 objective of peak and cost minimization. (f) is the RL-DSM results of aggregated residential loads for ﬁve diﬀerent
consumers to minimize daily peak load and cost. The ﬁgure best viewed in color.

Consumer

Consumer 1
Consumer 2
Consumer 3
Consumer 4
Consumer 5
All

Total energy bill ($/month)
before DSM after DSM

Monthly savings ($)

81
92.25
87.75
88.2
82.8
432

73.35
81
79.65
75.15
72.45
381.6

7.65
11.25
8.1
13.05
10.35
50.4

Table 5: Energy bill minimization with peak and cost reduction (Case 2).

hybrid multi-objective function guarantee that the load
proﬁle does not have high peaks, and at the same time,
the aggregated price of the load per day is minimum,
as seen in Figure 5. It is observed that the loads from
peak hours have been moved to min peak or low peak
time to minimize high power demand. Taking load cost
into account trains the model to understand that mov-

ing most loads from peak to non-peak hours is not en-
tirely ideal for the consumer as it may not shift the load
to the time duration at lower prices. Adding the cost
factor helps the agent to move the blocks to the lower-
priced time duration, which currently has fewer loads.
However, the experiments with single cost objective,
which only considers cost and disregards peak mini-

9

(a)

(b)

Figure 6: RL-DSM results of aggregated residential loads of ﬁve diﬀerent consumer to minimize daily peak load and cost
(a) with Shallow (After with Shallow) and Deep network (After with Deep) (b) using diﬀerent memory buﬀer size like
3000 (3k), 10000 (10k), 30000 (20k) and 50000 (50k). The ﬁgure best viewed in color.

Consumer

Consumer 1
Consumer 2
Consumer 3
Consumer 4
Consumer 5
All

Before
81
92.25
87.75
88.2
82.8
432

Total energy bill ($/month)

Shallow Net Deep Net

Monthly savings ($)
Shallow Net Deep Net

Peak load (kW)
Shallow Net Deep Net

71.55
82.35
79.65
71.1
72.45
377.1

73.35
81
79.65
75.15
72.45
381.6

9.45
9.9
8.1
17.1
10.35
54.9

8.1
13.5
3.6
14.4
3.6
43.2

4
3
5
5.5
3
18.5

2
3
3
2.5
3
10.5

Table 6: Demonstration of deep CNN network’s eﬃciency compared to shallow network. Shallow Net: Shallow network
and Deep Net: Deep network.

mization entirely, shows that non-peak hours started
to get high peak loads as expected because the reward
system for the agent focused solely on cost minimiza-
tion. This multi-objective function solves this issue.
Adding peak minimization with cost minimization as-
sures that the agent is not ﬁxated on cost minimiza-
tion, causing peaks to be formed during a time when
the cost of electricity consumption is less. Thus, both
the parameters of the multi-objective function, peak
and cost minimization are mutually beneﬁcial for one
another. Figure 5 shows how the system reacts to the
aggregated load of all consumers.

As shown in Table 5, the aggregate load reduced
bill cost from $432 to $381.6, saving around 11.66%.
The current limitation of the proposed system is that
it does not support dynamic preferred time for each
device, and load demand is on the scale of 0.5kW. This
is the reason that the daily test load demand shown in
Table 1 has a 1hour/unit time scale and 0.5kW/unit
power demand scale to ﬁt the proposed RL simulator.

As traditional methods like MILP does not have a
holistic knowledge of the system, it can be only used
for a ﬁxed number of devices. If the traditional system
needs to accommodate more devices at some point in
time after deployment, the method needs to be rerun,
and scheduling factors should be computed again. The
proposed methods are more ﬂexible as it has the learn-
ing capability to handle any load given to it. Even after
the proposed RL model is deployed, there is no need
for any adjustment to add or remove devices from the
system at any time instant. The time complexity of
the proposed method is better than other traditional
methods and its linearly proportional to number of de-

10

Figure 7: RL-DSM results on loads of 14 diﬀerent devices
to demonstrate the scalability of the proposed method.

vices, unlike traditional methods that have exponential
growth in time concerning the number of devices to be
scheduled. A trained RL agent always operates with
a ﬁxed number of ﬂoating-point arithmetic (FLOPs)
and multiply-adds (MAdd) for any number of loads to
be scheduled. The number of FLOPs and MAdd only
depend on the architecture of the network. This makes
the proposed model schedule hundreds of devices time
and space-eﬃcient than traditional methods. The time
complexity of RL can be deﬁned as O(kp) and MILP
would be O(k2p) where p is the number of parameters
associated with the model and k is the number of de-
vices to be scheduled. The space complexity of RL can
be formulated as O(p), whereas for MILP would be
O(k2). The scalability of the proposed methodology
is demonstrated by scheduling 46 loads of 14 diﬀerent
devices but with power-hungry devices with RL agent
as shown in Figure 7. Deploying trained RL agents in

Consumer

Consumer 1
Consumer 2
Consumer 3
Consumer 4
Consumer 5
All

Monthly savings ($)

Peak load (kW)

3k
8.1
7.2
2.7
12.6
7.2
37.8

10k
9.45
7.2
0.45
13.5
7.2
37.8

30k
7.65
11.25
8.1
13.05
10.35
50.4

50k
8.1
8.1
2.7
9
8.1
36

3k
4
4.5
3
3.5
4
16

10k
3
4.5
4
3
4.5
16.5

30k
2
3
3
2.5
3
10.5

50k
4.5
3
3
2.5
2
14

Table 7: Demonstration of the eﬀect of diﬀerent memory buﬀer size like 3000 (3k), 10000 (10k), 30000 (20k) and 50000
(50k) in RL-DSM for peak and cost minimization.

an embedded device is as easy as deploying traditional
models.

Energy Management and the Built Environment
Project.

5 Conclusion

The exponential growth in demand for power in the
household has increased the stress on the power grid
to meet its demand. DR can help a smart grid to im-
prove its eﬃciency to meet the power need of the cus-
tomer. This paper proposes a residential DR using a
deep reinforcement learning method where both load
proﬁle deviation and utility energy bills are minimized
simultaneously. An extensive case study with a single
objective and multi-objective cases is conducted.
In
both cases, the proposed method outperformed the tra-
ditional MILP method. This paper exhibited the po-
tential of reinforcement learning for better smart grid
operations and tested all these cases on ﬁve diﬀerent
consumers and showcased the ability of the proposed
method. In the future, this work can be extended to in-
troduce variable preferred time for each device and im-
prove the RL simulation grid to accommodate devices
with lower power demand with a scale of 0.1kW. This
work can also be extended to schedule more granular
timed devices with less than 1hr to complete its task.
In this work, renewable energy sources and energy stor-
age are not considered. Designing an RL agent to man-
age renewable energy and energy storage can bring out
the full potential of AI models in energy management.

References

[1] P. Yi, X. Dong, A. Iwayemi, C. Zhou, and S. Li,
“Real-time opportunistic scheduling for residen-
tial demand response,” IEEE Transactions on
Smart Grid, vol. 4, pp. 227–234, March 2013.

[2] P. Palensky and D. Dietrich, “Demand side man-
agement: Demand response,
intelligent energy
systems, and smart loads,” IEEE Transactions on
Industrial Informatics, vol. 7, pp. 381–388, Aug
2011.

[3] C. River, “Primer on demand side management,”

Report for the World Bank, 2005.

[4] G. Strbac, “Demand side management: Beneﬁts
and challenges,” Energy Policy, vol. 36, no. 12,
pp. 4419 – 4426, 2008. Foresight Sustainable

[5] Y. Son, T. Pulkkinen, K. Moon, and C. Kim,
“Home energy management system based on
power line communication,” IEEE Transactions
on Consumer Electronics, vol. 56, pp. 1380–1386,
Aug 2010.

[6] E. P. Act, “Energy policy act of 2005,” in US

Congress, 2005.

[7] B. Severin, J. Michael, and R. Arthur, “Dynamic
pricing, advanced metering and demand response
in electricity markets,” Center for the Study of
Energy Markets, UC Berkeley, 2002.

[8] C. Kang and W. Jia, “Transition of tariﬀ struc-
ture and distribution pricing in china,” in 2011
IEEE Power and Energy Society General Meeting,
pp. 1–5, IEEE, 2011.

[9] M. H. Albadi and E. F. El-Saadany, “Demand
response in electricity markets: An overview,”
in 2007 IEEE power engineering society general
meeting, pp. 1–5, IEEE, 2007.

[10] S. Caron and G. Kesidis, “Incentive-based en-
ergy consumption scheduling algorithms for the
smart grid,” in 2010 First IEEE International
Conference on Smart Grid Communications,
pp. 391–396, IEEE, 2010.

[11] M. A. F. Ghazvini, J. Soares, N. Horta, R. Neves,
R. Castro, and Z. Vale, “A multi-objective model
for scheduling of short-term incentive-based de-
mand response programs oﬀered by electricity re-
tailers,” Applied energy, vol. 151, pp. 102–118,
2015.

[12] M. Yu and S. H. Hong, “Incentive-based de-
mand response considering hierarchical electricity
market: A stackelberg game approach,” Applied
Energy, vol. 203, pp. 267–279, 2017.

[13] H. Aalami, M. P. Moghaddam, and G. Youseﬁ,
“Demand response modeling considering inter-
ruptible/curtailable loads and capacity market
programs,” Applied Energy, vol. 87, no. 1,
pp. 243–250, 2010.

11

[14] H. Zhong, L. Xie, and Q. Xia, “Coupon incentive-
based demand response: Theory and case study,”
IEEE Transactions on Power Systems, vol. 28,
no. 2, pp. 1266–1276, 2012.

[26] E. Shirazi and S. Jadid, “Optimal residential ap-
pliance scheduling under dynamic pricing scheme
via hemdas,” Energy and Buildings, vol. 93, pp. 40
– 49, 2015.

[15] R. S. Sutton, A. G. Barto, et al., Introduction to
reinforcement learning, vol. 135. MIT press Cam-
bridge, 1998.

[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu,
J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al.,
“Human-level control through deep reinforcement
learning,” Nature, vol. 518, no. 7540, p. 529, 2015.

[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves,
I. Antonoglou, D. Wierstra, and M. Riedmiller,
“Playing atari with deep reinforcement learning,”
2013.

[18] B. Lokeshgupta and S. Sivasubramani, “Cooper-
ative game theory approach for multi-objective
home energy management with renewable en-
ergy integration,” IET Smart Grid, vol. 2, no. 1,
pp. 34–41, 2019.

[19] Z. Bradac, V. Kaczmarczyk, and P. Fiedler, “Op-
timal scheduling of domestic appliances via milp,”
Energies, vol. 8, no. 1, pp. 217–232, 2015.

[27] Y. W. Law, T. Alpcan, V. C. S. Lee, A. Lo,
S. Marusic, and M. Palaniswami, “Demand re-
sponse architectures and load management algo-
rithms for energy-eﬃcient power grids: A sur-
vey,” in 2012 Seventh International Conference on
Knowledge, Information and Creativity Support
Systems, pp. 134–141, Nov 2012.

[28] F. D’Ettorre, M. De Rosa, P. Conti, D. Testi, and
D. Finn, “Mapping the energy ﬂexibility poten-
tial of single buildings equipped with optimally-
controlled heat pump, gas boilers and thermal
storage,” Sustainable Cities and Society, 2019.

[29] Y. Guo, J. Wang, H. Chen, G. Li, J. Liu, C. Xu,
R. Huang, and Y. Huang, “Machine learning-
based thermal response time ahead energy de-
mand prediction for building heating systems,”
Applied energy, vol. 221, pp. 16–27, 2018.

[30] Y. Chen, Z. Chen, P. Xu, W. Li, H. Sha, Z. Yang,
G. Li, and C. Hu, “Quantiﬁcation of electricity
ﬂexibility in demand response: Oﬃce building
case study,” Energy, vol. 188, p. 116054, 2019.

[20] S. G. Hamed and A. Kazemi, “Multi-objective
side,”
481,

for
(ICOIN), vol.

cost-load
Information Networking
p. 485, 2016.

optimization

demand

[21] P. Palensky and D. Dietrich, “Demand side man-
agement: Demand response,
intelligent energy
systems, and smart loads,” IEEE transactions on
industrial informatics, vol. 7, no. 3, pp. 381–388,
2011.

[22] P. Chavali, P. Yang, and A. Nehorai, “A
distributed algorithm of appliance scheduling
for home energy management system,” IEEE
Transactions on Smart Grid, vol. 5, pp. 282–290,
Jan 2014.

[31] B. Alimohammadisagvand,

J. Jokisalo,

and
K. Sir´en, “Comparison of four rule-based demand
response control algorithms in an electrically and
heat pump-heated residential building,” Applied
Energy, vol. 209, pp. 167–179, 2018.

[32] J. Aghaei and M.-I. Alizadeh, “Demand response
in smart electricity grids equipped with renew-
able energy sources: A review,” Renewable and
Sustainable Energy Reviews, vol. 18, pp. 64 – 72,
2013.

[33] S. Cui, Y.-W. Wang, J. Xiao, and N. Liu,
“A two-stage robust energy sharing management
for prosumer microgrid,” IEEE Transactions on
Industrial Informatics, 2018.

[23] C. O. Adika and L. Wang, “Smart charging and
appliance scheduling approaches to demand side
management,” International Journal of Electrical
Power & Energy Systems, vol. 57, pp. 232–240,
2014.

[34] M. H. Yaghmaee, M. Moghaddassian,

and
A. Leon-Garcia, “Autonomous two-tier cloud-
based demand side management approach with
microgrid,” IEEE Transactions on Industrial
Informatics, vol. 13, no. 3, pp. 1109–1120, 2016.

[24] S. Rahim, N. Javaid, A. Ahmad, S. A. Khan,
Z. A. Khan, N. Alrajeh, and U. Qasim, “Exploit-
ing heuristic algorithms to eﬃciently utilize en-
ergy management controllers with renewable en-
ergy sources,” Energy and Buildings, vol. 129,
pp. 452 – 470, 2016.

[25] S. Salinas, M. Li, and P. Li, “Multi-objective
optimal energy consumption scheduling in smart
grids,” IEEE Transactions on Smart Grid, vol. 4,
pp. 341–348, March 2013.

[35] N. Javaid, I. Ullah, M. Akbar, Z. Iqbal, F. A.
Khan, N. Alrajeh, and M. S. Alabed, “An intelli-
gent load management system with renewable en-
ergy integration for smart homes,” IEEE Access,
vol. 5, pp. 13587–13600, 2017.

[36] M. H. K. Tushar, A. W. Zeineddine, and C. Assi,
“Demand-side management by regulating charg-
ing and discharging of the ev, ess, and utilizing re-
newable energy,” IEEE Transactions on Industrial
Informatics, vol. 14, no. 1, pp. 117–126, 2017.

12

[37] K. Ma, P. Liu, J. Yang, X. Wei, and C. Dou,
“Spectrum allocation and power optimization for
demand-side cooperative and cognitive commu-
nications in smart grid,” IEEE Transactions on
Industrial Informatics, vol. 15, no. 3, pp. 1830–
1839, 2018.

[38] D. Li, W.-Y. Chiu, H. Sun, and H. V. Poor, “Mul-
tiobjective optimization for demand side manage-
ment program in smart grid,” IEEE Transactions
on Industrial Informatics, vol. 14, no. 4, pp. 1482–
1490, 2017.

[39] S. L. Arun and M. P. Selvan, “Intelligent residen-
tial energy management system for dynamic de-
mand response in smart buildings,” IEEE Systems
Journal, vol. 12, pp. 1329–1340, June 2018.

[40] T. Li and M. Dong, “Residential energy storage
management with bidirectional energy control,”
IEEE Transactions on Smart Grid, 2018.

[41] W.-Y. Chiu, J.-T. Hsieh, and C.-M. Chen, “Pareto
optimal demand response based on energy costs
and load factor in smart grid,” IEEE Transactions
on Industrial Informatics, 2019.

[42] B. Rajasekhar, N. Pindoriya, W. Tushar, and
C. Yuen, “Collaborative energy management for
a residential community: A non-cooperative and
evolutionary approach,” IEEE Transactions on
Emerging Topics in Computational Intelligence,
vol. 3, no. 3, pp. 177–192, 2019.

[43] B. Rajasekhar and N. Pindoriya, “Decentral-
ized energy management for a group of het-
erogenous residential customers,” in 2015 IEEE
Innovative Smart Grid Technologies-Asia (ISGT
ASIA), pp. 1–6, IEEE, 2015.

[44] T. Li and M. Dong, “Real-time residential-
side joint energy storage management and load
scheduling with renewable integration,” IEEE
Transactions on Smart Grid, vol. 9, no. 1, pp. 283–
298, 2016.

[45] S. Arun and M. Selvan, “Intelligent residen-
tial energy management system for dynamic de-
mand response in smart buildings,” IEEE Systems
Journal, vol. 12, no. 2, pp. 1329–1340, 2017.

[46] M. Martinez-Pabon, T. Eveleigh, and B. Tanju,
“Optimizing residential energy management us-
ing an autonomous scheduler system,” Expert
Systems with Applications, vol. 96, pp. 373–387,
2018.

[47] I. Dusparic, C. Harris, A. Marinescu, V. Cahill,
and S. Clarke,
residential de-
“Multi-agent
mand response based on load forecasting,” in
2013 1st IEEE Conference on Technologies for
Sustainability (SusTech), pp. 90–96, Aug 2013.

[48] Z. Wen, D. O’Neill, and H. Maei, “Optimal de-
mand response using device-based reinforcement
learning,” IEEE Transactions on Smart Grid,
vol. 6, no. 5, pp. 2312–2324, 2015.

[49] D. O’Neill, M. Levorato, A. Goldsmith, and U. Mi-
tra, “Residential demand response using reinforce-
ment learning,” in 2010 First IEEE International
Conference on Smart Grid Communications,
pp. 409–414, IEEE, 2010.

[50] I. Dusparic, A. Taylor, A. Marinescu, V. Cahill,
and S. Clarke, “Maximizing renewable energy use
with decentralized residential demand response,”
in 2015 IEEE First International Smart Cities
Conference (ISC2), pp. 1–6, Oct 2015.

[51] R. Lu, S. H. Hong, and X. Zhang, “A dynamic
pricing demand response algorithm for smart
grid: Reinforcement learning approach,” Applied
Energy, vol. 220, pp. 220 – 230, 2018.

[52] F. Ruelens, B. J. Claessens,

S. Vandael,
B. De Schutter, R. Babuˇska, and R. Belmans,
“Residential demand response of thermostatically
controlled loads using batch reinforcement learn-
ing,” IEEE Transactions on Smart Grid, vol. 8,
no. 5, pp. 2149–2159, 2017.

[53] R. Lu and S. H. Hong, “Incentive-based demand
response for smart grid with reinforcement learn-
ing and deep neural network,” Applied Energy,
vol. 236, pp. 937 – 949, 2019.

[54] F. Pallonetto, M. D. Rosa, F. Milano, and
D. P. Finn, “Demand response algorithms for
smart-grid ready residential buildings using ma-
chine learning models,” Applied Energy, vol. 239,
pp. 1265 – 1282, 2019.

[55] M. Stevens and S. Pradhan, “Playing tetris with

deep reinforcement learning,”

[56] H. van Hasselt, A. Guez, and D. Silver, “Deep
reinforcement learning with double q-learning,”
2015.

[57] S.

Ioﬀe and C. Szegedy, “Batch normaliza-
tion: Accelerating deep network training by re-
ducing internal covariate shift,” arXiv preprint
arXiv:1502.03167, 2015.

[58] C. Igel and M. H¨usken, “Improving the rprop
the
learning algorithm,”
second international ICSC symposium on neural
computation (NC 2000), vol. 2000, pp. 115–121,
Citeseer, 2000.

in Proceedings of

[59] L. J. Lin, “Self-improving reactive agents based on
reinforcement learning, planning and teaching,”
Machine Learning, vol. 8, pp. 293–321, 1992.

[60] P. gas and electric company, “Residential time of

use service.” https://www.pge.com/.

13

Supplementary Material:
Intelligent Residential Energy Management System using Deep Reinforcement
Learning

(a)

(c)

(e)

(b)

(d)

(f)

Figure 8: RL-DSM results of residential loads for ﬁve diﬀerent consumers (a), (b), (c), (d) and (e) respectively with
Shallow (After with Shallow) and Deep network (After with Deep). (f) is the RL-DSM results of aggregated residential
loads for ﬁve diﬀerent consumers to minimize daily peak load and cost. The ﬁgure best viewed in color.

14

(a)

(c)

(e)

(b)

(d)

(f)

Figure 9: RL-DSM results of residential loads for ﬁve diﬀerent consumer (a), (b), (c), (d) and (e) respectively using
diﬀerent memory buﬀer size like 3000(3k), 10000(10k), 30000(20k) and 50000(50k). (f) is the RL-DSM results of ag-
gregated residential loads for ﬁve diﬀerent consumers to minimize daily peak load and cost. The ﬁgure best viewed in
color.

15

