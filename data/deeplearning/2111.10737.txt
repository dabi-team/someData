3D Visual Tracking Framework with Deep Learning
for Asteroid Exploration

Dong Zhou, Guanghui Sun, Member, IEEE, and Xiaopeng Hong, Member, IEEE

1

1
2
0
2

v
o
N
1
2

]

V
C
.
s
c
[

1
v
7
3
7
0
1
.
1
1
1
2
:
v
i
X
r
a

Abstract—3D visual

tracking is signiﬁcant

to deep space
exploration programs, which can guarantee spacecraft to ﬂexibly
approach the target. In this paper, we focus on the studied
accurate and real-time method for 3D tracking. Considering the
fact that there are almost no public dataset for this topic, A new
large-scale 3D asteroid tracking dataset is presented, including
binocular video sequences, depth maps, and point clouds of
diverse asteroids with various shapes and textures. Beneﬁtting
from the power and convenience of simulation platform, all the
2D and 3D annotations are automatically generated. Meanwhile,
we propose a deep-learning based 3D tracking framework,
named as Track3D, which involves 2D monocular tracker and a
novel light-weight amodal axis-aligned bounding-box network,
A3BoxNet. The evaluation results demonstrate that Track3D
achieves state-of-the-art 3D tracking performance in both accu-
racy and precision, comparing to a baseline algorithm. Moreover,
our framework has great generalization ability to 2D monocular
tracking performance.

Index Terms—3D visual tracking, Asteroid tracking, 3D track-
ing dataset, amodal bounding-box estimation, 2D visual tracking

(a) Itokawa

(b) Ryugu

Fig. 1: Two mission objectives of Hayabusa and Hayabusa2.
(a) Itokawa, a potato-shaped asteroid with 600-meter size, is
named after Hideo Itokawa, a Japanese rocket pioneer. (b)
Ryugu, a more primordial C-type asteroid with 900-meter size,
is discovered on May 1999 by LINEAR telescope.

I. INTRODUCTION

M ILLIONS of asteroids exist in solar system, many the

shattered remnants of planetesimals, bodies within the
young Sun’s solar nebula that never grew large enough to
become planets [1]. The vast majority of known asteroids orbit
within the main asteroid belt located between the orbits of
Mars and Jupiter. To further investigate how planets formed
and how life began, as well as improve our understanding
to asteroids that could impact Earth, some deep exploration
programs were proposed, e.g. Hayabusa [2], [3], Hayabusa2
[4], [5], and OSIRIS-Rex [6], [7]. The program objectives
involve orbiting observation, autonomous landing, geological
sampling, and so on.

Two near-Earth asteroids, Itokawa [3] and Ryugu [4], with
complex 6-DoF motion are shown in Fig. 1. It is obvious that
3D visual tracking system is important to explore these two
asteroids, which can provide object location, size, and pose.
It’s also of great signiﬁcance to spacecraft autonomous navi-
gation, asteroid sample collection, and universe origin study.
However, state-of-the-art 4-DoF trackers [8]–[10] presented
for automous driving are confused about heading angle of
asteroid, which makes inaccurate 3D bounding-box estimation.
Besides, some 6-DoF tracking methods [11]–[13] under strong

D. Zhou and G. Sun are with the Department of Control Science and
Engineering, Harbin Institute of Technology, Harbin, China, 150001. E-mail:
dongzhou@hit.edu.cn, guanghuisun@hit.edu.cn

X. Hong is with the Faculty of Electronic and Information Engineer-
ing, Xi’an Jiaotong University, Xi’an, China, 710049. E-mail: hongxi-
aopeng@mail.xjtu.edu.cn

assumptions are also impractical
to track asteroid. To be
honest, constructing an end-to-end deep network that predicts
6-DoF states of asteroid is pretty difﬁcult. We therefore
decompose 3D asteroid tracking problem into 3-DoF tracking
and pose estimation. And this paper merely focus on the 3-
DoF tracking part.

Inspired by the idea of 2D-driven 3D perception, we present
a novel deep-learning based 3D asteroid tracking framework,
Track3D. As shown in Fig. 2,
it mainly consists of 2D
monocular tracker and a light-weight amodal axis-aligned
bounding-box network, A3BoxNet, which can predict accurate
target center and size purely relied on partial object point
cloud. Extensive experiments show that Track3D reaches state-
of-the-art 3D tracking results (0.669 AO3d at 77 FPS) and has
great generalization ability to 2D monocular tracker. Moreover,
we discover that our framework with 2D-3D tracking fusion
strategy can make signiﬁcant improvement on 2D tracking
performance.

However, there are few studies on 3D asteroid tracking,
as well as relevant dataset, which have greatly hindered the
development of asteroid exploration. To this end, we construct
the ﬁrst large-scale 3D asteroid tracking dataset, by acquiring
148,500 binocular images, depth maps, and point clouds of
diverse asteroids in various shapes and textures with physics
engine. Beneﬁtting from the power and convenience of physics
engine, all
the 2D and 3D annotations are automatically
generated. Meanwhile, we also provide an evaluation toolkit,
which includes 2D monocular and 3D tracking evaluation
algorithms.

 
 
 
 
 
 
2

Fig. 2: The deep-learning based 3D tracking framework for asteroid, named as Track3D, which consists of 2D monocular
tracker and amodal axis-aligned bounding-box network, A3BoxNet. Experiment shows our framework can run at 77 FPS with
high accuracy. And it has great generalization abitlity to 2D tracking algorithm.

Our contributions in this paper are summarized as follows:
types of asteroid with various
• Considering different
shapes and textures, we construct the ﬁrst large-scale
3D asteroid tracking dataset, including 148,500 binocular
images, depth maps, and point clouds.

• The ﬁrst 3-DoF asteroid tracking framework, Track3D, is
also presented, which involves an 2D monocular tracker
and A3BoxNet network. The experimental results show
the impressive advancement and generalization of our
framework, even based on poor 2D tracking algorithm.
• We propose a novel light-weight amodal bounding-box
estimation network, A3BoxNet, which can predict ac-
curate axis-aligned bounding-box of target merely with
partial object point cloud. Randomly sampling 1024
points as network input, A3BoxNet can even achieve
0.712 AO3d and 0.345 ACE3d with up to 281.5 FPS
real-time performance.

The rest of this paper is ordered in the subsequent sec-
tions. In Section II, we review some related works from two
aspects: visual tracking in aerospace domain and 3D object
tracking. In Section III, the constructing details of 3D asteroid
tracking dataset are presented, involving simulation platform,
annotation, and evaluation metrics. We also propose a novel
deep-learning based 3D tracking framework in Section IV,
which consists of 2D monocular tracker, and a light-weight
amodal bounding-box estimation network, A3BoxNet. Section
V introduces more details about 3D tracking framework per-
formance and ablation study. Finally, we make a conclusion
in Section VI.

II. RELATED WORK

A. Visual Tracking in Aerospace

The goal of visual tracking is to precisely and robustly
perceive real-time states (e.g. location, velocity and size) of an

instereted target in complex and dynamic environment. As the
basis for pose estimation, behavior understanding, and scene
interpretation, visual object tracking has wide application in
space debris removel [14], [15], space robotic inspection [16],
spacecraft rendezvous and docking [17], [18].

At present, most of visual trackers in aerospace domain
mainly focus on feature-based method. Huang [15] proposed
a novel feature tracking algorithm. Firstly, it extracted features
by SURF detector. And then,
the pyramid-Kanade-Lucas-
Tomasi (P-KLT) algorithm was adopted to match key-points
between two adjacent frames. Finally, accurate target bounding
box is obtained with Greedy Snake method. A feature-tracking
scheme was also presented in [19] that combines traditional
feature-point detector and frame-wise matching to track a non-
cooperative and unknown satellite. Felicetti [20] put forward
an active space debris visual tracking method, in which the
chaser satellite can keep the moving object in ﬁeld of view
of optical camera by continuously pose correction. Those
feature-based trackers heavily relied on the manually designed
feature detector and cannot handle extreme cases in space (e.g.
illumination variation, scale variation, fast motion, rotation,
truncation, and background clutters).

With many challenges and benchmarks emerging, such as
OTB2015 [21], YouTube-BB [22], ILSVRC-VID [23], GOT-
10K [24], and Visual Object Tracking challenges [25], [26],
the development of generic object tracking is far beyond imag-
ination [27]. Especially, deep learning based trackers [28]–
[34] have dominated the whole tracking community in recent
years, because of its striking performance. Generic object
trackers often follow one protocol that no prior knowledges
is available. This hypothesis is naturally suitable for asteroid
visual tracking, because of high uncertainty of vision tasks in
space. In paper [35], most of state-of-the-art generic trackers
had been evaluated on space non-cooperative object visual

2D Monocular TrackerAmodalAxis-aligned Bounding-box NetworkTrack3D2D Tracking ResultsMonocular SequenceRaw Point CloudsFrustum Proposals3D Tracking Resultszhou et al.: 3D VISUAL TRACKING FRAMEWORK WITH DEEP LEARNING FOR ASTEROID EXPLORATION

3

Fig. 3: The simulated platform for constructing 3D asteroid tracking dataset. We design three types of 3D asteroid model with
6 colorful textures, of which size and luminosity are modiﬁed during data collection. The setups of visual observer spacecraft
and imaging mechanism of perspective vision camera are also shown in upper right corner. We pre-align depth map and point
cloud to reference frame (i.e. left camera coordinate system). The left and right vision cameras have the same parameters,
including far clipping plane, perspective angle, and resolution. In addition, some screenshots of 3D asteroid tracking dataset
with 2D and 3D annotations are also visualized at bottom right.

tracking (SNCOVT) dataset, which provides ﬁrm research
foundation for our work.

However, recent visual

trackers mainly cope with RGB
[36]–[38], RGB-Thermal [39]–[41], and RGB-Depth [42]
video sequences, which only provides poor target information
in 2D space and heavily restricts practical applications of vi-
sual tracking. In contrast, 3D visual tracking is more promising
and competitive. To this end, we propose a novel deep-learning
based 3D tracking framework for asteroid exploration.

B. 3D Object Tracking

Although there are plenty of related works [8]–[13], [43]–
[45], the concept of 3D object tracking remains ambiguous.
Hence that, we ﬁrst deﬁne that 3D object tracking is to obtain
3D information of any target in real-time by leveraging various
visual sensors, given initaial states at the beginning. According
to the degrees of freedom of tracking result, all the 3D trackers
can be concluded into three categories: 3-DoF, 4-DoF, and 6-
DoF Tracker. Obviously, the more degrees of freedom, the
more difﬁcult 3D tracking task is.

3-DoF tracking means that both 3D location and size of ob-
ject (i.e. 3D axis-aligned bounding-box) should be estimated.
However, most of researches only considered predicting 3D
object center. In paper [43], a color-augmented search algo-
rithm was presented to obtain the position and velocity of

vehicle. Asvadi et al. [44] utilized two parallel mean-shift
localizers in image and pcd space, and made fusion for two
localizations by Kalman ﬁlter. This algorithm can effectively
achieve low 3D center error. To the best of our knowledge, the
framework proposed in this paper is the ﬁrst real 3-DoF tracker
that can be applied for collision avoidance, 3D reconstruction,
and pose estimation.

Comparing with 3-DoF methods, 4-DoF tracker often needs
to predict an extra heading angle of target, which is original
from 3D tracking requirement in autonomous driving. Gian-
cola et al. [8] presented a novel 3D siamese tracker with
the regularization of point cloud completion. However, the
exhaustively searching for candidate shapes in this method
would consume high computational cost. Qi et al. [9] also
proposed point-wise tracking paradigm, P2B, which addressed
3D object tracking by potential center localization, 3D target
proposal and veriﬁcation. Since this method purely uses point
cloud data as input, it is vulnerable to initial object point cloud
and only achieves 0.562 AO3d at 40 FPS on KITTI tracking
dataset [46].

In paper [11], [12], 6-DoF tracking task was considered
as joint 2D segmentation and 3D pose estimation problem,
and the method looked for the pose that best segmented
the target object from the background. Crivellaro et al. [13]
present a novel 6-DoF tracking framework with monocular

Asteroid 06Texture 05Texture 06Asteroid 05Texture 03Texture 04Asteroid 04Texture 01Texture 02OBJECTIVESRight CameraLeft Camera & Depth CameraReference FrameObserver SpacecraftCamera Settings:Far clipping plane: 50mPerspective angle: 90°Resolution: 1024 x 512xyzXZwPerspective Vision Camera SIMULATED SPACE ENVIRONMENT DATASET VISUALIZATIONLeft ImageRight ImageDepth MapPoint Cloud4

TABLE I: The default object size categories and corresponding ratio of XYZ.

category

X
Y
Z

0

1
1
1

1

1/2
1
1

2

1/3
1
1

3

2/3
1
1

4

1
1/2
1

5

1
2/3
1

6

1
1
1/2

7

1
1
2/3

8

1/2
1/2
1

9

2/3
2/3
1

10

1/2
1
1/2

11

2/3
1
2/3

12

1
1/2
1/2

13

1
2/3
2/3

image, including expensive 2D detector, local pose estimation
and extended kalman ﬁlter. However, the non-textured 3D
model of target should be given in this method, which heavily
limits the scope of its application. To be honest, the asteroid
tracking is one of 6-DoF tracking tasks. We think it is very
hard to construct an end-to-end network that can directly
and precisely predict the 6-DoF object states. Therefore, we
decompose asteroid tracking problem into 3-DoF tracking and
pose estimation. It is worthwhile noting that this paper simply
focus on the 3-DoF tracking task of asteroid.

III. 3D ASTEROID TRACKING DATASET

To promote the research on 3D tracking for asteroid, we
construct a large-scale 3D asteroid tracking dataset, including
binocular video sequences, depth maps and point clouds. In
addition, 3D tracking evaluation toolkit is also provided for
performance analysis. More details about 3D asteroid tracking
dataset are introduced in this section.

A. Dataset Construction

There is no doubt that collecting real data to create large-
scale 3D asteroid tracking dataset is impractical, like KITTI
dataset [46] and Princeton RGB-D Tracking dataset [42].
Meanwhile, constructing dataset by ground simulation [35]
is very expensive and limited. Inspired by UAV123 dataset
[47], we therefore consider to collect rich 3D tracking data by
virtual physics engine, V-rep. The power and convenience of
physics engine make automatic data labelling possible, which
greatly reduces constructing cost and improves annotation
accuracy.

The critical foundation of dataset constructing based on
physics engine is 3D modeling of asteroids with diverse shapes
and textures. We create three types of 3D asteroid model (i.e.
Asteroid04, 05, and 06) with 6 different textures, which are
illustrated at the left of Fig. 3. From point of our view, one
asteroid model with diferent textures can be considered as
different ﬁne-grained categories. In addition, we introduce 9
simulated space scenes into dataset construction. All the 3D
asteroid models have been controlled by scripts to carry out
random 6-DoF motion in simulated scenes.

The detailed setup of vision sensors for data collection can
be seen at upper right corner of Fig 3. Only two perspective
vision cameras are equipmented with 0.4-meter baseline in
x-axis direction on observer spacecraft, which can not only
acquire binocular video sequences during simulation, but also
achieve aligned depth maps and point clouds from camera
buffer by the API of physics engine. The camera matrix is very
vital for our 3D tracking framework in which point clouds
should be projected to image plane, however, V-rep merely

(a) Size classes

(b) Object volumes

(c) Motion speeds

(d) Illumination

Fig. 4: The statistics of 3D asteroid tracking dataset.

provides perspective angle (αx, αy) and resolution (W, H).
To this end, we compute camera intrinsic matrix Mi following
Eq. 1 (more derived details are given in Appendix A):

Mi =






W
2 tan(αx/2)
0
0

0
H
2 tan(αy/2)
0






W
2
H
2
1

(1)

At ﬁnal, we collect 360 sequences for training and 135
sequences for testing set. Each sequence has 300 frames
binocular images, depth maps, and point clouds. Following the
protocol proposed in [24], there is no overlap on ﬁne-grained
categories between training and testing set. The screenshots of
3D asteroid tracking testing set with 2D and 3D annotations
are shown in Fig. 3.

B. Dataset Statistics

In actual, our framework predicts the object size class
and normalized size residuals, rather outputs an axis-aligned
3D bounding-box directly. Therefore, size classes should be
predeﬁned and the number of samples in each category has
better to be balanced. To this end, we make a statistic analysis
for size category of axis-aligned 3D bounding-boxes in 3D
asteroid tracking dataset and deﬁne 14 default size categories
summarized in Table I. The ﬁnal distribution of size classes
can be seen in Fig. 4a.

Meanwhile, the distribution of object volumes and motion
speeds are shown in Fig. 4b and 4c, respectively. Because
of random asteroid rotation and manual shape modiﬁcation,
the volume of samples is widely distributed (16 m3 to 1600
m3). It is worthwhile noting that object volume metioned here
is the volume of axis-aligned 3D bounding-box, instead the

012345678910111213Size class0.000.050.100.150.200.250.30RatioSize class distributiontrain settest set02505007501000125015001750object volumes0.000.050.100.150.20ratioObject volumes distributiontrain settest set0.00.20.40.60.81.01.21.4motion speed0.0000.0250.0500.0750.1000.1250.1500.1750.200ratioMotion speed distributiontrain settest set020406080100object illumination0.000.020.040.060.080.100.12ratioObject illumination distributiontrain settest setzhou et al.: 3D VISUAL TRACKING FRAMEWORK WITH DEEP LEARNING FOR ASTEROID EXPLORATION

5

Fig. 5: The detailed framework diagram of Track3D

real one. In addition, as we mentioned above that all the 3D
asteroid models are controlled by scripts to carry out random
6-DoF motion, Fig. 4c clearly demonstrates that the random
translation of object obeys normal distribution.

As paper [35] had proven that all the generic trakcers are
vulnerable to illumination variation, we also consider this
factor into our dataset. Furthermore, we have limited the
illumination of all samples at low value (see in Fig. 4d), which
conforms to the reality of space environment.

of successfully tracked frames where the overlap exceeds a
threshold. In this work, we take 0.5 as threshold to measure
the success rate of one tracker. However, SR at a speciﬁc
threshold is not representative, we further introduce success
plot presented by [21] into our evaluation tool.

Another intuitive technique index of tracker is location
precision. Therefore, we propose 3D average center error
metric (ACE3d) that measures the mean distance between all
the predicted trajectories and ground-truth trajectories:

C. Evaluation Metrics

The main metric utilized to analyse 2D tracking perfor-
mance is average overlap, AO, which is also widely applied
in visual tracking benchmarks like OTB [21], VOT [26], and
GOT-10k [24]. In this paper, we also introduce this metric
to evaluate 2D monocular tracker and 3D tracking results
in bird’s eye view (i.e. AO2d and AObev). Furthermore, We
extend average overlap measurement to evaluate 3D tracking
accuracy, named as AO3d:

ACE3d =

1
M

M
(cid:88)

(cid:16)

∆

Γs, ˜Γs

(cid:17)

(3)

s=1
where Γs = {pt|t = 1, . . . , Ns} and ˜Γs = {˜pt|t = 1, . . . , Ns}
are the predictions and ground-truths of s-th sequence, which
(cid:17)
Γs, ˜Γs
consist of a series of object center in 3D space. ∆
is formulated as:

(cid:16)

(cid:16)
Γs, ˜Γs

(cid:17)

∆

=

1
Ns

Ns(cid:88)

t=1

(cid:107)pt − ˜pt(cid:107)2

(4)

AO3d =

Ns(cid:88)

(cid:33)

Ω3d
t

;

in which, (cid:107)·(cid:107)2 denotes Euclidean distance.

(2)

;

t=1

Ω3d

t =

s=1
t ∩ A3d
t
t ∪ A3d
t
in which, M is the sequences number in 3D asteroid tracking
test set, Ns denotes the length of s-th sequence, and Ω3d
t
denotes the region overlap between axis-aligned 3D annotation
˜A3d
t at t-th frame. It is worthwhile
noting that tracking restart mechanism is introduced to 3D
tracking evaluation. That is 3D tracker is allowed to reini-
tialize, once the Intersection-over-Union (IoU) of 3D tracking
result Ω3d
t

t and 3D tracking result A3d

is zero.

Besides average overlap metric, we also adopt success
rate, SR, as our indicator, which denotes the percentage

IV. 3D TRACKING FRAMEWORK

Inspired by the 2D-drieven 3D perception, we propose a
deep-learning based 3D tracking framework shown in Fig.
5, which involves 2D monocular tracker, SiamFC [28] and
a novel light-weight amodal axis-aligned bounding-box net-
work, A3BoxNet. Although binocular images and depth maps
are also provided in our 3D asteroid tracking dataset, it is
worthwhile noting that Track3D only utilizes monocular video
sequence and corresponding point clouds, which reduces the
complexity of 3D tracking framework and more conforms to
real applications in aerospace. In addition, we will introduce
a simple but effective 2D-3D tracking fusion strategy in the
following subsections.

(cid:32)

M
(cid:88)

1
Ns

1
M
˜A3d
˜A3d

Track3DRandom SamplingTranslationAmodalBox EstimationCenter Regression NetworkA3BoxNetstage1 centernpointsn pointsExamplarKernelSiamFCNetworkEmbedding Layerssimilarity mapcross correlationMonocular SequenceRaw Point Clouds2D Tracking ResultsFrustum ProposalsA3 Bounding-box6

(a) Center regression network

Fig. 6: Two main module of A3BoxNet. (a) predicts coarse object center, (b) is to estimate object center residuals, object size
category and normalized size residuals.

(b) Amodal box estimation network

A. SiamFC

The SiamFC algorithm utilizes full convolutional

layers
as an embedding layer, denoted as embedding function ϕ.
The exemplar image z and multi-scale search images X =
{xi|i = 1, 2, ..., S} are mapped to a high-dimensional feature
space with ϕ, which can be trained by a general large-scale
dataset. And then similarity score maps are generated by cross-
correlation between exemplar kernal and the tensors of multi-
scale search images, where the 2D tracking result can be
reached after post-processing.

For this algorithm, the key is to learn discriminative em-
bedding function ϕ. In this work, we train the backbone of
SiamFC from scratch on ILSVRC-VID [23] with 50 epochs
and further ﬁne-tune it on our 3D asteroid tracking training set.
Beneﬁtting from the strong generalization ability of Siamese
network, SiamFC can achieve a decent 2D tracking perfor-
mance. Although there are multifarious deep-learning based
2D monocular trackers, like SiamRPN++ [30], Ocean [36],
STARK [37], TransT [38], which greatly outperform SiamFC
in tracking accuracy, we think weak dependence on high-
performance 2D monocular tracker can effectively guarantee
the generalization ability of our framework and improve its
running speed.

Once tracking result At is acquired from 2D monocular
tracker, it can be used for frustum proposal extraction from
raw points set Praw = (cid:8)(xi, yi, zi) ∈ R3|i = 1, 2, ..., r(cid:9), as
shown in Fig. 5. We ﬁrst crop out m points from raw point
cloud Pcrop = {(xi, yi, zi)|1 (cid:54) zi (cid:54) 45, i = 1, 2, ..., r}, and
then compute the projected point cloud in image plane Pimg =
{(ui, vi)|i = 1, 2, ..., m} with camera matrix M ∈ R3×4.
Therefore, we can achieve the frustum proposal Pfrustum =
{(xi, yi, zi)|i = 1, 2, ..., n} by the indices of points which
belong to At region in Pimg.

B. A3BoxNet

In this work, we assume that there are no other objects in the
range of LiDAR perception, that is, all of the points in frustum

Fig. 7: Two-stage object center prediction in bird’s eye view.
We ﬁrst estimate the stage-1 center of input points with center
regression network. And then center residuals are predicted by
amodal box estimation network, also named as stage-2 center.

proposal belong to tracked target, which conforms to practical
scenario in space. Therefore, we propose a novel light-weight
amodal axis-aligned bounding-box network, A3BoxNet, which
makes directly prediction on frumstum proposal and no point
segmentation is considered into. Fig. 2 clearly shows that our
A3BoxNet mainly consists of two modules: center regression
network and amodal box estimation network. The former one
is responsible for the estimation of stage-1 object center.
Another is to predict object center residuals, object size
category, and normalized size residuals. It is worthwhile noting
that both center regression network and amodal box estimation
network are only support ﬁxed number of input points. To this
end, we add random sampling at the beginning of A3BoxNet.
The architecture of center regression network and amodal
box estimation network are both illustrated in Fig. 6a and 6b.
It can be clearly seen that there is no signiﬁcant difference
between two networks which are both derived from PointNet
[48]. In A3BoxNet, we use a number of MLP for high-level

nx 3sharedMLP128nx 256sharedMLP128 sharedMLP256 128 x 13x1256 x 1FCFCFCmaxpool(256 + k) x 1class one-hot vectorCenter Regression Networkn x 3maxpoolsharedMLP128sharedMLP128sharedMLP256sharedMLP512n x 512(512 + k) x 1FCFCFC3 + default_sizex 4class one-hot vectorAmodalBox Estimation512 x 1256 x 14202442024original centroidstage1 centerstage2 centerground-truthzhou et al.: 3D VISUAL TRACKING FRAMEWORK WITH DEEP LEARNING FOR ASTEROID EXPLORATION

7

TABLE II: The evaluation results on 3D asteroid tracking dataset. First two rows are the performances of two main modules
in Track3D, and the 3rd row is a simple 3D tracking baseline as comparison. All the 2D tracking metrics of 3D tracker are
computed by using 2D-3D fusion strategy which makes great improvement on 2D tracking performance. The top-2 evaluation
results for each performance indicator are highlighted in green and blue.

name

SiamFC
A3BoxNet

3D Tracking Baseline
Track3D

AO2d ACE2d
(pixel)

AObev AO3d ACE3d
(meter)

0.513
-

0.568
0.541

29.986
-

23.767
26.741

-
0.798

0.309
0.756

-
0.721

0.165
0.669

-
0.345

0.876
0.570

FPS

101.3
281.5

103.1
77.0

amodal box estimation network also classiﬁes object size to
14 predeﬁned categories (see in Table I) as well as predicts
normalized size residuals (Nsize scores for size classiﬁcation,
Nsize × 3 for size residuals regression). At ﬁnal, we remap
the predicted size category and normalized size residual to
original scale by multiplying the largest length of enclosing
bounding-box of point clouds.

To train our A3BoxNet, we use a joint loss function Ljoint

to simultaneously optimize both two submodules:

Ljoint = Lcenter−net + Lbox−net

(6)

in which,

Lbox−net = Lcenter res + Lsize cls + Lsize res

(7)

more details about optimization function of A3BoxNet are
given in Appendix B. In addition, A3BoxNet is trained on 3D
asteroid tracking dataset with 25 epoches and 32 batch size.
All the inputs are ﬁxed number of point sets that randomly
sampled from object point cloud.

C. 2D-3D Tracking Fusion Strategy

We believe the fusion of 3D tracking and 2D monocular
tracking results can make signiﬁcant
improvement on 2D
tracking performance. To this end, we propose a simple and
effective 2D-3D tracking fusion strategy (as shown in Fig. 8).
At ﬁrst, we calculate the projection of center cross-section
plane of 3D bounding-box in image plane, Aproj
. And then,
2D monocular tracking result At is weighted with Aproj

:

t

t

ˆAt = λ1 · Aproj

t + λ2 · At

(8)

in this work, we set λ1 = 0.3 and λ2 = 0.7.

V. EXPERIMENTS

In this section, we have introduced a simple 3D tracking
baseline algorithm as comparison and implemented extensive
experiments with Track3D on 3D asteroid tracking dataset
to demonstrate the advancement and effectiveness of our
framework. We also ﬁnd out that our framework with 2D-
3D fusion strategy can not only handle 3D tracking challenge,
but also make improvement on 2D tracking performance. In
addition, valuable ablation study is also considered. All the
experiments are carried out with Intel i9-9900k@3.60GHz
CPU and Nvidia RTX 2080Ti GPU.

t

Fig. 8: The schematic diagram of 2D-3D tracking fusion
strategy. Aproj
is the projection of center cross-section plane
of A3d
in image plane and At is the result of 2D monocular
t
tracker at t-th timestep. We compute fused tracking result ˆAt
by the weighted average between Aproj

and At.

t

features extraction. And max-pooling layer is introduced to
aggregate global features with symmetric property that is crit-
ical for unsorted point sets. The global feautres concatenated
with one-hot vector of coarse-grained asteroid category can be
further used for predicting stage-1 center, stage-2 center, and
object size.

We visualize the center predction progress of A3BoxNet
in Fig. 7. The frustum proposal have been normalized by
substracting the centroid of point sets ﬁrstly, which can
improve the translation invariance of A3BoxNet and speed
up convergence rate during training. In addtion, it shows that
after two stage prediction, estimated object center is very close
to the ground-truth. The predicted center is formulateds as
follows:

Cpred = ¯C + ∆C1 + ∆C2
(5)
where ¯C is the centroid of point cloud in frustum proposal,
∆C1 is the output of center regression network, ∆C2 is the
center residuals predicted by amodal box estimation network.
Except for estimating the center residuals of object, our

8

(a) 3D tracking baseline

(b) Track3D

(c) 2D precision comparison

(d) 3D precision comparison

Fig. 9: The performance comparison between 3D tracking baseline and Track3D. (a) and (b) are respectively success plots of
3D tracking baseline and Track3D with AO2d, AObev, and AO3d metrics.

sequence 0001

sequence 0022

sequence 0031

sequence 0052

Fig. 10: The tracking results of Track3D on 3D asteroid tracking test set.

(a) sequence 0001

(b) sequence 0022

(c) sequence 0031

(d) sequence 0052

Fig. 11: 3D tracking trajectories on 4 sequences in 3D asteroid tracking test set.

A. Framework Performance

Considering that

there are a few researh results in the
scope of 3-DoF tracking at present, therefore, we replace the
A3BoxNet of our framework with minimum enclosing 3D
bounding-box algorithm (i.e. computing the length of aixs-
aligned 3D bounding-box by max {Pfrustum} − min {Pfrustum})
to realise a simple 3D tracking baseline as comparison. Both
2D and 3D tracking evaluation results of 3D tracking baseline
are summarized at 3rd row in Table II. And its success plots
are shown in Fig. 9a. We ﬁnd out that the 3D tracking baseline
with 2D-3D fusion strategy improves 2D monocular tracking

performance about 10.5%, however, it achieves quite poor
performance in 3D space.

The overall performance of deep-learning based 3D tracking
framework, also named as Track3D, is illustrated in Fig. 9b.
It reaches excellent evaluation results on 3D asteroid tracking
test set (0.669 AO3d and 0.570 ACE3d) with high real-time
performance (77.0 FPS). We visualize part of tracking results
of Track3D in Fig. 10, which demonstrates the effectiveness of
our 3D tracking method. It clearly shows that our framework
can estimate accurate 3D bounding-box even under extreme
truncation (see 150-th frame of sequence 0001 in Fig. 10).

0.00.20.40.60.81.0Overlap threshold0.00.20.40.60.81.0Success rateSuccess plotsTrack3D_Baseline_3d: [0.165]Track3D_Baseline_original_2d: [0.514]Track3D_Baseline_bev_2d: [0.309]Track3D_Baseline_2d: [0.568]0.00.20.40.60.81.0Overlap threshold0.00.20.40.60.81.0Success rateSuccess plotsTrack3D_3d: [0.669]Track3D_original_2d: [0.514]Track3D_bev_2d: [0.756]Track3D_2d: [0.541]01020304050center error threshold0.00.20.40.60.81.0precision2D Precision plotsTrack3D_Baseline: [23.767]Track3D: [26.741]0.000.250.500.751.001.251.501.752.00center error threshold0.00.20.40.60.81.0precision3D Precision plotsTrack3D: [0.570]Track3D_Baseline: [0.876]2D tracking result3D tracking resultframe 50predictionground-truth2D tracking result3D tracking resultframe 50predictionground-truth2D tracking result3D tracking resultframe 50predictionground-truth2D tracking result3D tracking resultframe 50predictionground-truth2D tracking result3D tracking resultframe 100predictionground-truth2D tracking result3D tracking resultframe 100predictionground-truth2D tracking result3D tracking resultframe 100predictionground-truth2D tracking result3D tracking resultframe 100predictionground-truth2D tracking result3D tracking resultframe 150predictionground-truth2D tracking result3D tracking resultframe 150predictionground-truth2D tracking result3D tracking resultframe 150predictionground-truth2D tracking result3D tracking resultframe 150predictionground-truth2D tracking result3D tracking resultframe 200predictionground-truth2D tracking result3D tracking resultframe 200predictionground-truth2D tracking result3D tracking resultframe 200predictionground-truth2D tracking result3D tracking resultframe 200predictionground-truth ;   P   P   P   P    P    P    P    P <     P    P    P    P   P   P   P   P =    P    P    P    P    P    P    P    P    P  '  W U D M H F W R U L H V  S O R W V 7 U D F N  ' J U R X Q G  W U X W K 7 U D F N  ' B % D V H O L Q H ;     P     P     P    P   P <    P    P   P   P   P   P   P =    P    P    P    P    P    P  '  W U D M H F W R U L H V  S O R W V 7 U D F N  ' J U R X Q G  W U X W K 7 U D F N  ' B % D V H O L Q H ;   P   P   P   P    P    P    P    P <    P    P   P   P   P   P    P    P    P =    P    P    P    P    P    P  '  W U D M H F W R U L H V  S O R W V 7 U D F N  ' J U R X Q G  W U X W K 7 U D F N  ' B % D V H O L Q H ;     P     P    P   P   P    P    P <     P     P    P   P   P    P =    P    P    P    P    P    P  '  W U D M H F W R U L H V  S O R W V 7 U D F N  ' J U R X Q G  W U X W K 7 U D F N  ' B % D V H O L Q Hzhou et al.: 3D VISUAL TRACKING FRAMEWORK WITH DEEP LEARNING FOR ASTEROID EXPLORATION

9

(a) 2D success plots

(b) 2D precision plots

Fig. 12: The 2D tracking performance of 12 classic monocular trackers.

(a) 3D success plots

(b) 3D precision plots

Fig. 13: The 3D tracking performance of Track3Ds under 12 different monocular trackers.

strategy can also make signiﬁcant improvement on 2D tracking
performance.

B. Module Performance

Extensive experiments are implemented in this subsection
that explore how two main modules of Track3D (i.e. 2D
monocular tracker and A3BoxNet) make inﬂuences on ﬁnal
3D tracking performance, which points the way to design
effective 3D tracking framework for future work.

Firstly, plenty of classic monocular trackers have been eval-
uated on left video sequences of 3D asteroid tracking dataset,
such as SiamFC [28], SiamRPN [49], ECO [50], Staple [51],
KCF [52], DAT [53], BACF [54], STRCF [55], MKCFup [56],
CSRDCF [57], CSK [58], and MOSSE [59]. The evaluation
results are illustrated in Fig. 12 which straightforward shows
the accuracy of 12 monocular trackers are distributed in a large
range from 0.375 to 0.746. SiamFC adopted in our framework
only achieves intermediate 2D tracking performance in both
accuracy and precision metrics.

And then, we evaluate the whole framework under different
monocular trackers respectively and plot 3D success and

Fig. 14: the connection between 3D tracking accuracy and 2D
tracking performance, which demonstrates great generalization
ability of our framework.

The 3D trajectory plots in Fig. 11 also intuitively show our
framework predicts precise 3D object location, which greatly
outperforms 3D tracking baseline. In addition, it can be seen
in the ﬁrst colum of Table II that Track3D with 2D-3D fusion

0.00.20.40.60.81.0Overlap threshold0.00.20.40.60.81.0Success rateSuccess plotsSiamRPN: [0.746]ECO: [0.520]SiamFC: [0.513]Staple: [0.488]KCF: [0.478]DAT: [0.466]BACF: [0.458]STRCF: [0.449]MKCFup: [0.446]CSRDCF: [0.431]CSK: [0.429]MOSSE: [0.375]01020304050center error threshold0.00.20.40.60.81.0precision2D Precision plotsSiamRPN: [10.270]ECO: [25.227]SiamFC: [29.986]DAT: [33.200]KCF: [34.052]BACF: [37.033]Staple: [37.050]STRCF: [37.561]CSRDCF: [37.808]MKCFup: [37.889]CSK: [38.715]MOSSE: [44.130]0.00.20.40.60.81.0Overlap threshold0.00.20.40.60.81.0Success rateSuccess plotsTrack3D_SiamRPN: [0.709]Track3D_SiamFC: [0.669]Track3D_DAT: [0.661]Track3D_ECO: [0.659]Track3D_KCF: [0.650]Track3D_STRCF: [0.646]Track3D_BACF: [0.644]Track3D_Staple: [0.643]Track3D_CSRDCF: [0.635]Track3D_MKCFup: [0.620]Track3D_CSK: [0.613]Track3D_MOSSE: [0.611]0.000.250.500.751.001.251.501.752.00center error threshold0.00.20.40.60.81.0precision3D Precision plotsTrack3D_SiamRPN: [0.406]Track3D_SiamFC: [0.570]Track3D_DAT: [0.581]Track3D_ECO: [0.601]Track3D_BACF: [0.644]Track3D_KCF: [0.648]Track3D_Staple: [0.653]Track3D_STRCF: [0.654]Track3D_CSRDCF: [0.710]Track3D_MKCFup: [0.736]Track3D_CSK: [0.766]Track3D_MOSSE: [0.792]0.30.40.50.60.70.8AO_2D0.00.20.40.60.81.0AO_3DTrack3D_SiamRPNTrack3D_ECOTrack3D_SiamFCTrack3D_StapleTrack3D_KCFTrack3D_DATTrack3D_BACFTrack3D_STRCFTrack3D_MKCFupTrack3D_CSRDCFTrack3D_CSKTrack3D_MOSSE10

Fig. 16: The pefromance of A3BoxNet under different num-
bers of point set. There is a contradictory between accuracy
and precision of A3BoxNet.

points) from object point clouds in 3D asteroid tracking
training set. And all
the models are trained from scratch
with 25 epoches and 32 batch size. The evaluation results
are plotted in Fig. 16, which clearly shows accuracy and
precision of A3BoxNet have contradictory with the number
of input points. Once the accuracy of model increasing, the
corresponding precision will decrease.

Besides, the inﬂuence of object category on A3BoxNet is
further studied. We remove one-hot vector of object category
in center regression network and amodal box estimation net-
work, and retrain A3BoxNet from scratch with 1024 point
sets. The performance comparison between original A3BoxNet
and A3BoxNet without category information is summarized in
Table III, which proves the object category can make a slight
improvement on the 3D accuracy of A3BoxNet.

VI. CONCLUSION

In this work, we construct the ﬁrst large-scale 3D asteroid
tracking dataset, which involves 148,500 binocular images,
depth maps, and point clouds. All the 2D and 3D annotations
are automatically generated, which greatly guarantees the qual-
ity of tracking dataset and reduce the cost of data collection.
The 3D asteroid tracking dataset will be public on website
(http://aius.hit.edu.cn/12920/list.htm). Meanwhile, we propose
a deep-learning based 3D visual tracking framework, Track3D,
which mainly consists of classic 2D monocular tracker and a
novel light-weight amodal axis-aligned bounding-box network.
The state-of-the-art 3D tracking performance and great gen-
eralization ability of our framework have been demonstrated
by sufﬁcient experiments. We also ﬁnd that Track3D with 2D-
3D tracking fusion strategy also makes improvement on 2D
tracking performance. In future work, We will further apply
our Track3D method to normoal cases like automous driving
and robot picking.

APPENDIX A
THE PERSPECTIVE CAMERA MATRIX

In section III, we have mentioned that camera matrix
is very important for Track3D to extract frustum proposal.

Fig. 15: The performance of A3BoxNet with 1024 input
points.

TABLE III: The ablation study on one-hot category vector.

name

AO3d AObev ACE3d

A3BoxNet
without category

0.721
0.715

0.798
0.792

0.345
0.316

precision curves in Fig. 13a and 13b. We found that 3D
evaluation curves become much dense comparing with 2D
curves of Fig. 11, which denotes that 2D monocular trackers
just have slight inﬂuence on 3D tracking performance of our
framework. In other words, Track3D can still work even based
on poor 2D monocular tracker. We further plot the relationship
between 2D tracking performance and framework accuracy
in Fig. 14, which intuitively demonstrates the generalization
ability of Track3D.

Meanwhile, We evaluate amodal bounding-box estimation
network, A3BoxNet, on 3D asteroid tracking dataset by
randomly sampling 1024 points from frustum proposal. Its
performance is also illustrated in Fig. 15. It can be clearly
seen that our A3BoxNet predicts high accurate axis-aligned
bounding-box purely with partial object points. Furthermore,
our network is able to run at 281.5 FPS, which totally satisﬁes
the requirement of application on edge computing device.

We also study how the number of input points affects the
performance of amodal axis-aligned bounding-box network.
We retrain the A3BoxNet by randomly sampling different
numbers of point set (e.g. 512, 1024, 2048, 3072, and 4096

0.00.20.40.60.81.0Overlap threshold0.00.20.40.60.81.0Success rateSuccess plotsA3BoxNet_overall_bev_2d: [0.798]A3BoxNet_overall_3d: [0.721]0.000.250.500.751.001.251.501.752.00center error threshold0.00.20.40.60.81.0precision3D Precision plotA3BoxNet: [0.345]5121024204830724096point nums0.7000.7050.7100.7150.7200.7250.7300.3100.3150.3200.3250.3300.3350.3400.3450.350performance on different point numsAO_3dACE_3d11

(18)

zhou et al.: 3D VISUAL TRACKING FRAMEWORK WITH DEEP LEARNING FOR ASTEROID EXPLORATION

However, physics engine V-rep only provides perspective angle
(αx, αy) and resolution (W, H). To this end, we derive the
camera matrix by perspective projection principle, which is
also illustrated in Fig. 3.

Suppose that the virtual focus of perspective camera in both
x and y axes are (fx, fy), the size of image plane is (w, h), and
an object point P = (X, Y, Z) in camera coordinate system is
projected at p = (xi, yi) in image plane. Fig. 3 clearly shows
that:

X
Z

=

xi
fx

and,

w/2
fx

= tan

(cid:17)

(cid:16) αx
2

(9)

(10)

Meanwhile, the transformation from image coordinate system
to pixel coordinate system in x axis is formulated as:

u = (xi +

) ·

w
2

W
w

W
w
W
2

= xi ·

+

(11)

Substitute Eq. 9 and 10 into Eq. 11, it can be obtained:

W

u =

X +

W
2

2 tan( αx

2 )Z
in which, the parameter fx is eliminated. Similarly, the trans-
formation in y-axis direction from camera coordinate system
to pixel coordinate system is also obtained:

(12)

v =

H
2 tan( αy

Y +

H
2

2 )Z
We further rewrite Eq. 12 and 13 in homogeneous matrix

form:







u
v
 =
1






2 tan( αx

2 )Z

W

0

0

0
H
2 tan(
0

αy
2 )Z






W
2Z
H
2Z
1/Z





X
Y
Z





(14)

(13)

where, Lcenter−net adopt huber loss function:

Lcenter−net =

(cid:26) 0.5α2,

α < 1

α − 0.5, otherwise

(cid:13)
(cid:13)
ˆC − ( ¯C + ∆C1)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)

, ˆC is 3D center label, ¯c
in which α =
is the centroid of points in frustum proposal, and ∆C1 is the
prediction of center regression network.

In addition,

Lbox−net = Lcenter res + Lsize cls + Lsize res

(19)

where Lsize cls utilizes softmax cross entropy loss function:

Lsize cls = −

Nsize(cid:88)

i=1

ˆyi · log

(cid:33)

(cid:32)

eyi
(cid:80)Nsize
j=1 eyj

(20)

in which ˆy is Nsize dimensional one-hot vector of size cate-
gory label, y is the partial outputs of amodal box estimation
network, of which dimension is also Nsize.

Furthermore, Lcenter res and Lsize res both use huber loss

function. Lcenter res is formulated as:

Lcenter−net =

(cid:26) 0.5β2,

β < 2
otherwise
(cid:13)
(cid:13)
ˆC − ( ¯C + ∆C1 + ∆C2)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)

2(β − 1),

in which, β =
, ∆C2 is 3D center
residuals predicted by amodal box estimation network. And
Lsize res is as follows:

(21)

(cid:26) 0.5γ2,

γ < 1

Lcenter−net =

γ − 0.5, otherwise
(cid:111)(cid:13)
(cid:13)
(cid:13)2

(cid:110) ˆS

(cid:13)
ˆR − r ∗ max
(cid:13)
(cid:13)

, ˆS and ˆR are the size and
where γ =
size residual label, respectively. r is normalized size residual
corresponding to the size category predicted by amodal box
estimation network.

(22)

ACKNOWLEDGMENT

To eliminate the Z variable in the transformation matrix, we
multiply both sides of Eq. 14 by Z:

This work was kindly supported by the National Key R&D

Program of China through grant 2019YFB1312001.







u
v
 =
1






W
2 tan(αx/2)
0
0

0
H
2 tan(αy/2)
0






W
2
H
2
1

Z





X
Y
Z





(15)

Because we set left camera coordinate system as reference
frame, the camera matrix of left perspective camera can be
formulated as:

M L =






W
2 tan(αx/2)
0
0

0
H
2 tan(αy/2)
0

W
2
H
2
1


0

0

0

(16)

APPENDIX B
TRANING OBJECTIVES

In this work, we utilize a joint loss function Ljoint to

optimize A3BoxNet:

Ljoint = Lcenter−net + Lbox−net

(17)

REFERENCES

[1] S. Kortenkamp, Asteroids, Comets, and Meteoroids. Capstone, 2011.
[2] A. Fujiwara, J. Kawaguchi, D. K. Yeomans, M. Abe, T. Mukai, T. Okada
et al., “The Rubble-Pile Asteroid Itokawa as Observed by Hayabusa,”
Science, vol. 312, no. 5778, pp. 1330–1334, Jun. 2006, publisher:
American Association for the Advancement of Science Section: Special
Reports.

[3] M. Yoshikawa, A. Fujiwara, and J. Kawaguchi, “Hayabusa and its adven-
ture around the tiny asteroid Itokawa,” Proceedings of the International
Astronomical Union, vol. 2, no. 14, pp. 323–324, Aug. 2006, publisher:
Cambridge University Press.

[4] S. Sugita, R. Honda, T. Morota, S. Kameda, H. Sawada, E. Tatsumi
et al., “The geomorphology, color, and thermal properties of Ryugu:
Implications for parent-body processes,” Science, vol. 364, no. 6437,
Apr. 2019, publisher: American Association for the Advancement of
Science Section: Research Article.

[5] S.-i. Watanabe, Y. Tsuda, M. Yoshikawa, S. Tanaka et al., “Hayabusa2
Mission Overview,” Space Science Reviews, vol. 208, no. 1, pp. 3–16,
Jul. 2017.

[6] D. Lauretta, S. Balram-Knutson, E. Beshore, W. Boynton, C. D.
d’Aubigny, D. DellaGiustina, H. Enos, D. Golish, C. Hergenrother,
E. Howell et al., “Osiris-rex: sample return from asteroid (101955)
bennu,” Space Science Reviews, vol. 212, no. 1, pp. 925–984, 2017.

12

[7] D. Golish, C. D. d’Aubigny, B. Rizk, D. DellaGiustina, P. Smith,
K. Becker, N. Shultz, T. Stone, M. Barker, E. Mazarico et al., “Ground
and in-ﬂight calibration of the osiris-rex camera suite,” Space science
reviews, vol. 216, no. 1, pp. 1–31, 2020.

[8] S. Giancola, J. Zarzar, and B. Ghanem, “Leveraging shape completion
for 3D siamese tracking,” in Proc. IEEE Comput. Soc. Conf. Comput.
Vis. Pattern Recognit., vol. 2019-June, 2019, pp. 1359–1368.

[9] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, “P2B: Point-to-Box
Network for 3D Object Tracking in Point Clouds,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 6329–6338.

[10] T. Yin, X. Zhou, and P. Krahenbuhl, “Center-Based 3D Object Detec-
tion and Tracking,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2021, pp. 11 784–11 793.

[11] V. A. Prisacariu and I. D. Reid, “PWP3D: Real-Time Segmentation and
Tracking of 3D Objects,” International Journal of Computer Vision,
vol. 98, no. 3, pp. 335–354, 2012-07-01.

[12] V. A. Prisacariu, A. V. Segal, and I. Reid, “Simultaneous monocular
2d segmentation, 3d pose recovery and 3d reconstruction,” in Asian
conference on computer vision. Springer, 2012, pp. 593–606.

[13] A. Crivellaro, M. Rad, Y. Verdie, K. M. Yi, P. Fua, and V. Lepetit,
“Robust 3D Object Tracking from Monocular Images Using Stable
Parts,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 40, no. 6, pp. 1465–1479, 2018.

[14] G. S. Aglietti, B. Taylor, S. Fellowes, T. Salmon, I. Retat, A. Hall,
T. Chabot, A. Pisseloup, C. Cox, A. Mafﬁcini et al., “The active space
debris removal mission removedebris. part 2: in orbit operations,” Acta
Astronautica, vol. 168, pp. 310–322, 2020.

[15] P. Huang, F. Zhang, J. Cai, D. Wang, Z. Meng, and J. Guo, “Dexterous
tethered space robot: Design, measurement, control, and experiment,”
IEEE Transactions on Aerospace and Electronic Systems, vol. 53, no. 3,
pp. 1452–1468, 2017.

[16] D. Fourie, B. E. Tweddle, S. Ulrich, and A. Saenz-Otero, “Flight results
of vision-based navigation for autonomous spacecraft
inspection of
unknown objects,” Journal of spacecraft and rockets, vol. 51, no. 6,
pp. 2016–2026, 2014.

[17] A. Petit, E. Marchand, and K. Kanani, “Vision-based space autonomous
rendezvous: A case study,” in 2011 IEEE/RSJ International Conference
on Intelligent Robots and Systems.

IEEE, 2011, pp. 619–624.

[18] S. Sharma, C. Beierle, and S. D’Amico, “Pose estimation for non-
cooperative spacecraft rendezvous using convolutional neural networks,”
in 2018 IEEE Aerospace Conference, 2018-03, pp. 1–12.

[19] R. Volpe, G. B. Palmerini, and M. Sabatini, “A passive camera based
determination of a non-cooperative and unknown satellite’s pose and
shape,” Acta Astronautica, vol. 151, pp. 805–817, 2018.

[20] L. Felicetti and M. R. Emami, “Image-based attitude maneuvers for
space debris tracking,” Aerospace science and technology, vol. 76, pp.
58–71, 2018.

[21] Y. Wu, J. Lim, and M.-H. Yang, “Object Tracking Benchmark,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 37,
no. 9, pp. 1834–1848, 2015-09.

[22] E. Real, J. Shlens, S. Mazzocchi, X. Pan, and V. Vanhoucke, “Youtube-
boundingboxes: A large high-precision human-annotated data set for
object detection in video,” in proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2017, pp. 5296–5305.
[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision, vol. 115, no. 3, pp. 211–252,
2015-12-01.

[24] L. Huang, X. Zhao, and K. Huang, “GOT-10k: A Large High-Diversity
Benchmark for Generic Object Tracking in the Wild,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, pp. 1–1, 2019.

[25] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pﬂugfelder, L. Ce-
hovin Zajc, T. Vojir, G. Hager, A. Lukezic, A. Eldesokey, and G. Fer-
nandez, “The Visual Object Tracking VOT2017 Challenge Results,” in
Proceedings of the IEEE International Conference on Computer Vision
Workshops, 2017, pp. 1949–1972.

[26] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, R. Pﬂugfelder, J.-
K. Kamarainen, L. ˇCehovin Zajc, O. Drbohlav, A. Lukezic, A. Berg
et al., “The seventh visual object tracking vot2019 challenge results,”
in Proceedings of the IEEE/CVF International Conference on Computer
Vision Workshops, 2019, pp. 0–0.

[27] X. Lan, S. Zhang, P. C. Yuen, and R. Chellappa, “Learning Common
and Feature-Speciﬁc Patterns: A Novel Multiple-Sparse-Representation-
Based Tracker,” IEEE Transactions on Image Processing, vol. 27, no. 4,
pp. 2022–2037, 2018-04.

[28] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,
“Fully-convolutional siamese networks for object tracking,” in European
conference on computer vision. Springer, 2016, pp. 850–865.

[29] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. Torr, “Fast
online object
tracking and segmentation: A unifying approach,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2019, pp. 1328–1338.

[30] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “Siamrpn++:
tracking with very deep networks,” in
Evolution of siamese visual
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2019, pp. 4282–4291.

[31] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg, “Atom: Accurate
tracking by overlap maximization,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
4660–4669.

[32] Z. Zhang and H. Peng, “Deeper and Wider Siamese Networks for Real-
Time Visual Tracking,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2019, pp. 4591–4600.
[33] Z. Chen, B. Zhong, G. Li, S. Zhang, and R. Ji, “Siamese box adaptive
network for visual tracking,” in Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, 2020, pp. 6668–6677.
[34] M. Ondraˇsoviˇc and P. Tar´abek, “Siamese Visual Object Tracking: A

Survey,” IEEE Access, vol. 9, pp. 110 149–110 172, 2021.

[35] D. Zhou, G. Sun, J. Song, and W. Yao, “2D vision-based tracking
algorithm for general space non-cooperative objects,” Acta Astronautica,
vol. 188, pp. 193–202, 2021-11-01.

[36] Z. Zhang, H. Peng, J. Fu, B. Li, and W. Hu, “Ocean: Object-aware
anchor-free tracking,” in Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI
16. Springer, 2020, pp. 771–787.

[37] B. Yan, H. Peng, J. Fu, D. Wang, and H. Lu, “Learning spatio-temporal
transformer for visual tracking,” arXiv preprint arXiv:2103.17154, 2021.
[38] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, “Transformer
Tracking,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2021, pp. 8126–8135.

[39] X. Lan, M. Ye, S. Zhang, H. Zhou, and P. C. Yuen, “Modality-
correlation-aware sparse representation for RGB-infrared object track-
ing,” Pattern Recognition Letters, vol. 130, pp. 12–20, 2020-02-01.
[40] X. Lan, M. Ye, R. Shao, B. Zhong, P. C. Yuen, and H. Zhou, “Learn-
ing Modality-Consistency Feature Templates: A Robust RGB-Infrared
Tracking System,” IEEE Transactions on Industrial Electronics, vol. 66,
no. 12, pp. 9887–9897, 2019-12.

[41] X. Lan, W. Zhang, S. Zhang, D. K. Jain, and H. Zhou, “Robust
Multi-modality Anchor Graph-based Label Prediction for RGB-Infrared
Tracking,” IEEE Transactions on Industrial Informatics, pp. 1–1, 2019.
[42] S. Song and J. Xiao, “Tracking Revisited Using RGBD Camera: Uniﬁed
Benchmark and Baselines,” in Proceedings of the IEEE International
Conference on Computer Vision, 2013, pp. 233–240.

[43] D. Held, J. Levinson, and S. Thrun, “Precision tracking with sparse 3D
and dense color 2D data,” in 2013 IEEE International Conference on
Robotics and Automation, 2013, pp. 1138–1145.

[44] A. Asvadi, P. Gir˜ao, P. Peixoto, and U. Nunes, “3D object tracking using
RGB and LIDAR data,” in 2016 IEEE 19th International Conference on
Intelligent Transportation Systems (ITSC), 2016, pp. 1255–1260.
[45] U. Kart, A. Lukezic, M. Kristan, J.-K. Kamarainen, and J. Matas,
“Object Tracking by Reconstruction With View-Speciﬁc Discriminative
Correlation Filters,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2019, pp. 1339–1348.
[46] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving? The KITTI vision benchmark suite,” in 2012 IEEE Conference
on Computer Vision and Pattern Recognition, 2012, pp. 3354–3361.

[47] M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simulator for
uav tracking,” in European conference on computer vision. Springer,
2016, pp. 445–461.

[48] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets
for 3d object detection from rgb-d data,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2018, pp. 918–
927.

[49] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High Performance Visual
Tracking With Siamese Region Proposal Network,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2018,
pp. 8971–8980.

[50] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “ECO:
Efﬁcient Convolution Operators for Tracking,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2017,
pp. 6638–6646.

zhou et al.: 3D VISUAL TRACKING FRAMEWORK WITH DEEP LEARNING FOR ASTEROID EXPLORATION

13

[51] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and P. H. Torr,
“Staple: Complementary learners for real-time tracking,” in Proc. IEEE
Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2016.

[52] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “High-Speed
Tracking with Kernelized Correlation Filters,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 37, no. 3, pp. 583–596,
2015-03-01.

[53] H. Possegger, T. Mauthner, and H. Bischof, “In defense of color-based
model-free tracking,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis.
Pattern Recognit., 2015.

[54] H. Kiani Galoogahi, A. Fagg, and S. Lucey, “Learning Background-
Aware Correlation Filters for Visual Tracking,” in Proceedings of the
IEEE International Conference on Computer Vision, 2017, pp. 1135–
1143.

[55] F. Li, C. Tian, W. Zuo, L. Zhang, and M.-H. Yang, “Learning Spatial-
Temporal Regularized Correlation Filters for Visual Tracking,” in Pro-
ceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, 2018, pp. 4904–4913.

[56] M. Tang, B. Yu, F. Zhang, and J. Wang, “High-Speed Tracking With
Multi-Kernel Correlation Filters,” in Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2018, pp. 4874–4883.
[57] A. Lukezic, T. Vojir, L. Cehovin Zajc, J. Matas, and M. Kristan,
“Discriminative Correlation Filter With Channel and Spatial Reliability,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017, pp. 6309–6318.

[58] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “Exploiting
the Circulant Structure of Tracking-by-Detection with Kernels,” in
Computer Vision – ECCV 2012, ser. Lecture Notes in Computer Science,
A. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, and C. Schmid, Eds.
Springer, 2012, pp. 702–715.

[59] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui, “Visual object
tracking using adaptive correlation ﬁlters,” in 2010 IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, 2010,
pp. 2544–2550.

Dong Zhou was born in Hunan, China, in 1996. He
received the B.S degree in automation from Harbin
Engineering University, Harbin, China, in 2018. He
is currently working toward the Ph.D. degree in
the Department of Control Science and Engineer-
ing, Harbin Institute of Technology, Harbin, China.
His research interests include space non-cooperative
object visual tracking, 3D computer vision, and deep
learning.

Guanghui Sun was born in Henan Province, China,
in 1983. He received the B.S. degree in Automation
from Harbin Institute of Technology, Harbin, China,
in 2005, and the M.S. and Ph.D. degrees in Control
Science and Engineering from Harbin Institute of
Technology, Harbin, China, in 2007 and 2010, re-
spectively. He is currently a professor with Depart-
ment of Control Science and Engineering in Harbin
Institute of Technology, Harbin, China. His research
interests include machine learning, computer vision,
and aerospace technology.

Xiaopeng Hong received the Ph.D. degree in com-
puter application and technology from the Harbin
Institute of Technology, China, in 2010. He was a
Docent with Center for Machine Vision and Signal
Analysis, University of Oulu, Finland, where he had
been a Scientist Researcher from 2011 to 2018. He
is currently a Distinguished Research Fellow with
Xi’an Jiaotong University, China. He has published
over 30 articles in mainstream journals and confer-
ences such as the IEEE T-PAMI, T-IP, CVPR, ICCV,
AAAI, and ACM UbiComp. His current research in-
terests include multi-modal learning, affective computing, intelligent medical
examination, and human-computer interaction.

