8
1
0
2

p
e
S
0
1

]

G
L
.
s
c
[

1
v
3
6
3
3
0
.
9
0
8
1
:
v
i
X
r
a

Torchbearer

Torchbearer: A Model Fitting Library for PyTorch

Ethan Harris∗
Matthew Painter∗
Jonathon Hare
Vision, Learning and Control Group
Department of Electronics and Computer Science
University of Southampton, United Kingdom

ewah1g13@ecs.soton.ac.uk
mp2u16@ecs.soton.ac.uk
jsh2@ecs.soton.ac.uk

Abstract

We introduce torchbearer, a model ﬁtting library for pytorch aimed at researchers work-
ing on deep learning or diﬀerentiable programming. The torchbearer library provides a
high level metric and callback API that can be used for a wide range of applications. We
also include a series of built in callbacks that can be used for: model persistence, learning
rate decay, logging, data visualization and more. The extensive documentation includes an
example library for deep learning and dynamic programming problems and can be found at
http://torchbearer.readthedocs.io. The code is licensed under the MIT License and
available at https://github.com/ecs-vlc/torchbearer.
Keywords: deep learning, diﬀerentiable programming, machine learning, pytorch, open
source software

1. Introduction

The meteoric rise of deep learning will leave behind a host of frameworks that support
hardware accelerated tensor processing and automatic diﬀerentiation. We speculate that
over time a more general characterization, diﬀerentiable programming, will take its place.
This involves optimizing the parameters of some diﬀerentiable program through gradient
descent in a process known as ﬁtting. One library which has seen increasing use over recent
years is pytorch (Paszke et al., 2017), which excels, in part, because of the ease with which
one can construct models that perform non-standard tensor operations. This makes pytorch
especially useful for research, where any aspect of a model deﬁnition may need to be altered
or extended. However, as pytorch is speciﬁcally focused on tensor processing and automatic
diﬀerentiation, it lacks the high level model ﬁtting API of other frameworks such as keras
(Chollet et al., 2015), a library for training deep models with tensorflow. Furthermore,
such libraries rarely support the more general case of diﬀerentiable programming.

We introduce torchbearer, a pytorch library that aids research by streamlining the
model ﬁtting process whilst preserving transparency and generality. At the core of torch-
bearer is a model ﬁtting API where every aspect of the ﬁtting process can be easily modiﬁed.
We also provide a powerful metric API which enables the gathering of rolling statistics
and aggregates. Finally, torchbearer provides a host of callbacks that perform advanced

∗Contributed equally to this work and are listed in alphabetical order.

1

 
 
 
 
 
 
E. Harris, M. Painter and J. Hare

@callbacks. on criterion
def l2 weight decay (state ):

for W in state[ MODEL ]. parameters():

state[LOSS] += W.norm (2)

@metrics . default for key (" acc")

@metrics . running mean

@metrics .std
class CategoricalAccuracy...

Listing 1: Example callback

Listing 2: Example metric deﬁnition

functions, such as using tensorboardx to log visualizations to tensorboard (included with
tensorflow (Abadi et al., 2016)) or visdom.

2. Design

The torchbearer library is written in Python (van Rossum, 1995) using pytorch, torch-
vision and tqdm and depends on numpy (Oliphant, 2006), scikit-learn (Pedregosa et al.,
2011) and tensorboardx for some features. The key abstractions in torchbearer are trials,
callbacks and metrics. There are three key principles which have motivated the design:

• Flexibility: The library supports both deep learning and diﬀerentiable programming.

• Functionality: The library includes functions that make complex behavior available.

• Transparency: The API is simple and clear.

By virtue of these design principles, torchbearer diﬀers from other, similar libraries
such as ignite or tnt. For example, neither library oﬀers a wide range of built in callbacks
for complex functions. Furthermore, both ignite and tnt use an event driven API for the
model ﬁtting which makes code less transparent and human readable.

2.1 Trial API

The Trial class deﬁnes a pytorch model ﬁtting interface built around Trial.run(...).
There are also predict(...) and evaluate(...) methods which can be used for model
inference or evaluating saved models. The Trial class also provides a state dict method
which returns a dictionary containing: the model parameters, the optimizer state and the
callback states which can be saved and later reloaded using load state dict.

2.2 Callback API

The callback API deﬁnes classes that can be used to execute functions at diﬀerent points in
the ﬁtting process. The mutable state dictionary is an integral part of torchbearer that
is given to each callback and contains intermediate variables required by the Trial. This
allows callbacks to alter the nature of the ﬁtting process dynamically. Callbacks can be
implemented as a decorated function through the decorator API, as shown in Listing 1.

2.3 Metric API

The metric API uses a tree that enables data ﬂow from one metric to a set of children.
This allows for aggregates such as a running mean or standard deviation to be computed.

2

Torchbearer

class GAN( torch.nn.Module ):

def forward ( real imgs , state ):

z = ( random sample of prior noise distribution)

state[ GEN IMGS ] = generator (z)

state[ DISC GEN ] = discriminator( state[ GEN IMGS ])

discriminator. zero grad ()

state[ DISC GEN DET ] = discriminator(state [ GEN IMGS ]. detach ())

state[ DISC REAL ] = discriminator( real imgs )

Listing 3: GAN forward pass

@callbacks. add to loss
def gan loss (state ):

fake loss = adversarial loss (state [ DISC GEN DET ], fake)

real loss = adversarial loss (state [ DISC REAL], valid)

state[ G LOSS ] = adversarial loss ( state[ DISC GEN ], valid )

state[ D LOSS ] = ( real loss + fake loss) / 2
return state[ G LOSS ] + state [ D LOSS ]

Listing 4: Loss computation

Assembling these data structures can be diﬃcult, and as such, torchbearer includes a
decorator API that simpliﬁes construction. For example, in Listing 2 producing the standard
deviation and running mean of an accuracy metric. The default for key(...) decorator
enables the metric to be referenced with a string in the Trial deﬁnition.

3. Example: Generative Adversarial Networks (GANs)

A GAN (Goodfellow et al., 2014) is a type of model that aims to learn a high quality es-
timation of an input data distribution. GANs comprise of two networks which are trained
simultaneously but with opposing goals, the ‘generator’ and the ‘discriminator’. To en-
courage the generator to produce samples that appear genuine, we use an adversarial loss.
This is minimized when the discriminator predicts that generated samples are genuine.
For the discriminator, we use a loss that maximizes the probability of correctly identifying
real and fake samples. Implementing this requires forward passes from the generator and
discriminator and separate backward passes for the loss of each network. Due to this com-
plexity, training GANs is often challenging in typical frameworks since it requires a very
ﬂexible training loop, such as that of the torchbearer Trial. In this section we will train
a standard GAN with torchbearer to demonstrate its eﬀectiveness.

To begin, the forward passes of both networks are combined in Listing 3 with interme-
diate tensors stored in state. When torchbearer is not passed a criterion, the base loss
is automatically set to zero so that callbacks can add to it. We will use the add to loss
callback shown in Listing 4 to combine the losses. The state keys are generated using

3

E. Harris, M. Painter and J. Hare

trial = Trial (GAN(), optimizer , metrics =[" loss"], callbacks =[ gan loss], pass state= True)

trial . with train generator( dataloader). run( epochs =200)

Listing 5: Training

torchbearer.state key(key) to prevent collisions. Note that the call to the underlying
model does not automatically include the state dictionary, we will set the pass state ﬂag
in Trial(...) to achieve this. Having created a data loader and optimizer using pytorch,
we train the model with just two lines in listing 5.

4. Project Management

The torchbearer library is licensed under the MIT License and the most recent release
(0.2.1) is referenced on the Python Package Index (PyPi). The code is hosted on GitHub,
see CHANGELOG.md for release information. The repository is actively monitored and
we encourage users to raise issues requesting ﬁxes or new functionality and to open pull
requests for anything they have implemented.

4.1 Continuous Integration

To support usability, the library must be as stable as possible. For this reason we use
continuous integration with Travis CI which tests all pull requests before they can be merged
with the master branch. We also use Codacy to perform automated code reviews which
ensure that new code follows the PEP8 standard. In this way we ensure that the master
copy of torchbearer is always correctly styled and passes the tests.

4.2 Example Library

An eﬀective way to improve usability is to provide examples of using the library for a
range of problems. As such, we have added an example library which includes detailed
examples showing how to use torchbearer for various deep learning and diﬀerentiable pro-
gramming models including: GANs (Goodfellow et al., 2014), Variational Auto-Encoders
(Kingma and Welling, 2013) and Support Vector Machines (Cortes and Vapnik, 1995).

5. Conclusion

To summarize, torchbearer is a library simpliﬁes the process of ﬁtting deep learning and
diﬀerentiable programming models in pytorch. This is done without reducing transparency
so that it is still useful for the purpose of research. Key features of torchbearer include
a comprehensive set of built in callbacks (such as logging, weight decay and model check-
pointing) and a powerful metric API. The torchbearer library has a strong and growing
community on GitHub and we are committed to improving it wherever possible.

4

Torchbearer

References

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean,
Matthieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, et al. Tensorﬂow: a
system for large-scale machine learning. In OSDI, volume 16, pages 265–283, 2016.

Fran¸cois Chollet et al. Keras. https://keras.io, 2015.

Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):

273–297, 1995.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing systems, pages 2672–2680, 2014.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114, 2013.

Travis E Oliphant. A guide to NumPy, volume 1. Trelgol Publishing USA, 2006.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic dif-
ferentiation in pytorch, 2017.

Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand
Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent
Dubourg, et al. Scikit-learn: Machine learning in python. Journal of machine learn-
ing research, 12(Oct):2825–2830, 2011.

Guido van Rossum. Python tutorial, Technical Report CS-R9526. Technical report, Cen-

trum voor Wiskunde en Informatica (CWI), Amsterdam, May 1995.

5

