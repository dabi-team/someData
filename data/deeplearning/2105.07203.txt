1
2
0
2

y
a
M
5
1

]

C
C
.
s
c
[

1
v
3
0
2
7
0
.
5
0
1
2
:
v
i
X
r
a

Pebbles, Graphs, and a Pinch of Combinatorics: Towards Tight
I/O Lower Bounds for Statically Analyzable Programs

Grzegorz Kwasniewski, Tal Ben-Nun, Lukas Gianinazzi, Alexandru Calotoiu, Timo Schneider,
Alexandros Nikolaos Ziogas, Maciej Besta, Torsten Hoefler
ETH Zurich, Switzerland

ABSTRACT
Determining I/O lower bounds is a crucial step in obtaining com-
munication-efficient parallel algorithms, both across the memory
hierarchy and between processors. Current approaches either study
specific algorithms individually, disallow programmatic motifs such
as recomputation, or produce asymptotic bounds that exclude im-
portant constants. We propose a novel approach for obtaining pre-
cise I/O lower bounds on a general class of programs, which we call
Simple Overlap Access Programs (SOAP). SOAP analysis covers a
wide variety of algorithms, from ubiquitous computational kernels
to full scientific computing applications. Using the red-blue pebble
game and combinatorial methods, we are able to bound the I/O of
the SOAP-induced Computational Directed Acyclic Graph (CDAG),
taking into account multiple statements, input/output reuse, and
optimal tiling. To deal with programs that are outside of our repre-
sentation (e.g., non-injective access functions), we describe methods
to approximate them with SOAP. To demonstrate our method, we
analyze 38 different applications, including kernels from the Poly-
bench benchmark suite, deep learning operators, and â€” for the first
time â€” applications in unstructured physics simulations, numeri-
cal weather prediction stencil compositions, and full deep neural
networks. We derive tight I/O bounds for several linear algebra
kernels, such as Cholesky decomposition, improving the existing
reported bounds by a factor of two. For stencil applications, we
improve the existing bounds by a factor of up to 14. We implement
our method as an open-source tool, which can derive lower bounds
directly from provided C code.

CCS CONCEPTS
â€¢ Theory of computation â†’ Communication complexity; Par-
allel computing models; Scheduling algorithms.

KEYWORDS
I/O complexity, red-blue pebble game, parallel scheduling model

1 INTRODUCTION
I/O operations, both across the memory hierarchy and between par-
allel processors, dominate time and energy costs in many scientific
applications [1â€“4]. It is thus of key importance to design algorithms
with communication-avoiding or I/O-efficient schedules [5, 6]. To in-
form, and occasionally inspire the development of such algorithms,
one must first understand the associated lower bounds on the amounts
of communicated data. Deriving these bounds has always been of
theoretical interest [7, 8]. It is particularly relevant for dense linear
algebra, as many important problems in scientific computing [9, 10]
and machine learning [11] rely on linear algebra operations such
as matrix factorization [12, 13] or tensor contractions [14].

1

Analyzing I/O bounds of linear algebra kernels dates back to
the seminal work by Hong and Kung [8], who derived the first as-
ymptotic bound for matrix-matrix multiplication (MMM) using the
red-blue pebble game abstraction. This method was subsequently
extended and used by other works to derive asymptotic [15] and
tight [16] bounds for more complex programs. Despite the expres-
siveness of pebbling, it is prohibitively hard to solve for arbitrary
programs, as it is PSPACE-complete in the general case [17].

Since analyzing programs with parametric sizes disallows the
construction of an explicit Computation Directed Acyclic Graph
(CDAG), some form of parameterization is often needed [18â€“20].
However, we argue that the widely-used approaches based on the
Loomis-Whitney or the HBL inequalities [21â€“23] (a) are often too
restrictive, requiring the programs to be expressed in the polyhe-
dral model to count the points in the projection polytopes; (b) do
not capture pebbling motifs such as recomputation [19]; or (c) are
limited to single-statement programs [7, 21â€“23, 23, 24].

In our work, we take a different approach based on a combi-
natorial method. We directly map each elementary computation
to a vertex in a parametric CDAG, which allows us not only to
operate on unstructured iteration domains, but also to precisely
count the sizes of dominator sets and model vertex recomputation.
Furthermore, to handle complex data dependencies in programs
that evaluate multiple arrays, we introduce the Symbolic Directed
Graph (SDG) abstraction, which encapsulates the data flow between
elementary computations. This allows us to cover a wider class of
programs and handle more complex data flow.

To enable precisely mapping every data access to the parametric
CDAG vertex, we introduce a class of Simple Overlap Access Pro-
grams (SOAP), and present a general method to derive precise I/O
bounds of programs in this class. Specifically, SOAPs are defined
as loop nests of statements, whose data access sets can be mod-
eled as injective functions, and their per-statement data overlap
can be expressed with constant offsets. For programs that do not
directly adhere to SOAP, with nontrivial overlaps and non-injective
access functions, we show that under a set of assumptions, we can
construct SOAP â€œprojectionsâ€ of those programs, which can be ana-
lyzed in the same way. Our method strictly contains the polyhedral
model and associated analysis methods.

To show the breadth of our approach, we demonstrate SOAP
analysis on a set of 38 applications, taking Python and C codes as in-
put to create the SDG. This automated analysis procedure generates
symbolic bounds, which match or improve upon previously-known
results. Notably, we tighten the known I/O lower bounds for nu-
merous programs, including stencils by up to a factor of 14, linear
algebra kernels (e.g., Cholesky factorization by a factor of two), and
the core convolution operation in deep learning by a factor of 8.

 
 
 
 
 
 
Technical Report, 2021,

G. Kwasniewski et al.

Figure 1: High level overview of the combinatorial SOAP analysis. An input programâ€™s schedule is modeled as the red-blue pebble game. The ğ‘‹ -Partitioning
abstraction relaxes the pebbling problem to the graph partition problem. The SOAP abstraction utilizes the static loop structure to upper-bound the size of
the optimal ğ‘‹ -partition. The Symbolic Directed Graph (SDG) models inter-statement data dependencies. Our method derives I/O lower bounds together with
accompanying tile sizes and loop fusions that can be used by a compiler to generate an I/O optimal parallel code.

Since our derivation of the bounds is constructive â€” i.e., it pro-
vides loop tilings and fusions after relaxing loop-carried depen-
dencies â€” the results can be used by a compiler to generate I/O
optimal parallel codes. This can both improve existing schedules
and possibly reveal new parallelization dimensions.
The paper makes the following contributions:

â€¢ A combinatorial method for precisely counting the number of

data accesses in parametric CDAGs.

â€¢ A class of programs â€” SOAP â€” on which I/O lower bounds can

be automatically derived.

â€¢ Symbolic dataflow analysis that extends SOAP to multiple-
statement programs, capturing input and output reuse between
statements, as well as data recomputation.

â€¢ I/O analysis of 38 scientific computing kernels, improving ex-
isting bounds [19, 20] by up to a factor of 14, and new lower
bounds for applications in deep learning, unstructured physics
simulation, and numerical weather prediction.

2 BACKGROUND
We first present several fundamental concepts used throughout the
paper. We introduce program, memory, and execution models that
are based on the work by Hong and Kung [8]. We then present a
general approach for deriving I/O lower bounds based on graph
partitioning abstractions. The birdâ€™s eye view of our method is
presented in Figure 1.

2.1 General Approach of Modeling I/O Costs
Program model: CDAG. One of the most expressive ways to
model executions of arbitrary programs is a Computation Directed
Acyclic Graph (CDAG) [8, 16, 18, 20, 25] ğº = (ğ‘‰ , ğ¸), where vertices
represent data (either inputs or results of computations) and edges
represent data dependencies. That is, for ğ‘¢, ğ‘£ âˆˆ ğ‘‰ , a directed edge
(ğ‘¢, ğ‘£) âˆˆ ğ¸ signifies that ğ‘¢ is required to compute ğ‘£. Given vertex ğ‘£,
vertices {ğ‘¢ : (ğ‘¢, ğ‘£) âˆˆ ğ¸} are referred to as parents of ğ‘£. Analogously,
{ğ‘¢ : (ğ‘£, ğ‘¢) âˆˆ ğ¸} are children of ğ‘£. Vertices with in-degree (out-
degree) zero are denoted program inputs (program outputs).
Memory model: red-blue pebble game [8]. Programs are exe-
cuted on a sequential machine equipped with a two-level memory
system, which consists of a fast memory of limited size and unlim-
ited slow memory. The contents of the fast memory are represented
by ğ‘† red pebbles. A red pebble placed on a vertex indicates that

the data associated with this vertex resides in the fast memory.
Analogously, data residing in the slow memory is represented with
blue pebbles (of unlimited number).
Execution model: graph pebbling. An execution of a program
represented by a CDAG ğº = (ğ‘‰ , ğ¸) is modeled as a sequence of four
allowed pebbling moves: 1) placing a red pebble on a vertex which
has a blue pebble (load), 2) placing a blue pebble on a vertex which
has a red pebble (store), 3) placing a red pebble on a vertex whose
parents have red pebbles (compute) 4) removing any pebble from a
vertex (discard). At the program start, all input vertices have blue
pebbles placed on them. Execution finishes when all output vertices
have blue pebbles on them. A sequence of moves leading from the
start to the end is called a graph pebbling ğ‘ƒ. The number of load
and store moves in ğ‘ƒ is called the I/O cost of ğ‘ƒ. The I/O cost ğ‘„ of
a program ğº is the minimum cost among all valid pebbling
configurations. A pebbling with cost ğ‘„ is called optimal.

2.2 I/O Lower Bounds
Assume that the optimal pebbling ğ‘ƒğ‘œğ‘ğ‘¡ is given. For any constant
ğ‘‹ > ğ‘† we can partition this sequence of moves into subsequences,
such that in each subsequence except of the last one, exactly ğ‘‹ âˆ’ ğ‘†
load/store moves are performed (the last subsequence contains at
most ğ‘‹ âˆ’ ğ‘† load/store moves). Denote the number of these subse-
quences as â„. Then observe that (ğ‘‹ âˆ’ ğ‘†)(â„ âˆ’ 1) â‰¤ ğ‘„ â‰¤ (ğ‘‹ âˆ’ ğ‘†)â„.
Graph pebbling vs graph partitioning. Since finding ğ‘ƒğ‘œğ‘ğ‘¡ is
PSPACE complete [26], we seek to derive a lower bound of ğ‘„ from
the structure of ğº. Observe that the set of vertices which are com-
puted in each subsequence defines a subgraph H âŠ† ğº. By this
construction, computing vertices in H requires ğ‘‹ âˆ’ ğ‘† load/store
operations in the optimal schedule. The number of subsequences â„
may be bounded by a particular partitioning of ğº. To do this, we
need to introduce two vertex sets defined for any subgraph of ğº.
Dominator and minimum sets [8]. Given H âŠ† ğº, a domina-
tor set Dom (H ) is a set of vertices such that every path from
an input to any vertex in H must contain at least one vertex in
Dom (H ). The minimum set Min (H ) is a set of all vertices in H
that do not have any child in H . To avoid the ambiguity of non-
uniqueness of dominator set size, we denote a minimum dominator
set Domğ‘šğ‘–ğ‘› (H ) to be a dominator set with the smallest size.
ğ‘‹ -Partitioning: bounding I/O cost. Introduced by Kwasniewski
et al. [16], ğ‘‹ -Partitioning generalizes the S-partitioning from Hong

2

Input programCDAG pebblingforiinrange(100):forjinrange(100):C[i,j]=((A[i]+A[i+1])*(B[j]+B[j+1]))foriinrange(100):forjinrange(100):forkinrange(100):E[i,j]+=C[i,k]*D[k,j]Ignoringcompute costX-partitioningSymbolic Directed GraphABDCESOAPIgnoringloop carrieddepend.Minimal I/O cost â†’ opt. scheduleFeatures:Beyond polyhedral model (non-affine accesses); recomputation; dependency struct (SDG)Improved lower bounds: Linear algebra (Cholesky, correlation, covariance); stencils (fdtd, jacobi, heat3d)New lower bounds:Neural networks (LeNet-5, BERT Encoder); climate code (vertical adv., horizontal diff.)Section 2opt. schedule â†’opt. pebblingSection 2opt. pebbling â†’ max. subsetreuse overapproxSection 4max. subset â†’ rect. subcomp.Section 6rect. subcomp.â†’ opt. subgraphreuse overapproxRecompute and reuse upper boundComp./comm. ratio upper boundPebbling schedule lower boundTiled parallel codeTight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

Figure 2: From the input code to the I/O lower bounds. First, for each statement, the access function vectors ğ“ are extracted from the input program (green and
blue fields). For each statement, the size of its dominator set is obtained using Lemma 3 (Section 4.2), and then, the I/O lower bound is obtained using inequality 9
(Section 4.5). For programs that contain multiple statements, the SDG is constructed (Section 6) and all valid subgraph statements are evaluated (Section 6.1).
Lastly, the final I/O lower bound is obtained (Section 6.2).

and Kung [8]. Given a constant ğ‘‹ , an ğ‘‹ -partition of ğº = (ğ‘‰ , ğ¸)
is a collection of ğ‘  mutually disjoint subsets Hğ‘– âŠ† ğ‘‰ (referred to
as subcomputations) P (ğ‘‹ ) = {H1, . . . , Hğ‘  } : âˆ€ğ‘–â‰ ğ‘— Hğ‘– âˆ© Hğ‘— = âˆ…âˆ§
(cid:208)ğ‘– Hğ‘– = ğ‘‰ with two additional properties:

â€¢ there are no cyclic dependencies between subcomputations:
: (âˆƒ(ğ‘¢1, ğ‘£1) âˆˆ ğ¸ s.t. ğ‘¢1 âˆˆ Hğ‘– âˆ§ ğ‘£1 âˆˆ Hğ‘— ) =â‡’

âˆ€Hğ‘– â‰  Hğ‘—
((cid:154)(ğ‘£2, ğ‘¢2) âˆˆ ğ¸ s.t. ğ‘¢2 âˆˆ Hğ‘– âˆ§ ğ‘£2 âˆˆ Hğ‘— )

â€¢ âˆ€H âˆˆ P (ğ‘‹ ), |ğ·ğ‘œğ‘šğ‘šğ‘–ğ‘› (H )| â‰¤ ğ‘‹ and |ğ‘€ğ‘–ğ‘› (Hâ„)| â‰¤ ğ‘‹ .

The authors prove that for any ğ‘‹ > ğ‘†, the optimal pebbling ğ‘ƒğ‘œğ‘ğ‘¡
has an associated ğ‘‹ -partition Pğ‘œğ‘ğ‘¡ (ğ‘‹ ) s.t. |Pğ‘œğ‘ğ‘¡ (ğ‘‹ )| = â„.
Computational intensity. In previous works it was proven that
(a) ğ‘„ is lower bounded by the number of subsequences â„ in the
optimal pebbling ğ‘ƒğ‘œğ‘ğ‘¡ [8]; (b) â„ is lower bounded by the size of
the smallest ğ‘‹ -partition |Pğ‘šğ‘–ğ‘› (ğ‘‹ )| for any value of ğ‘‹ > ğ‘† [16]; (c)
|Pğ‘šğ‘–ğ‘› (ğ‘‹ )| is bounded by the maximum size of a single subcomputa-
tion |Hğ‘‹ ,ğ‘šğ‘ğ‘¥ | in any valid ğ‘‹ -partition: |Pğ‘šğ‘–ğ‘› (ğ‘‹ )| â‰¥ |ğ‘‰ |/|Hğ‘‹ ,ğ‘šğ‘ğ‘¥ |
[16]; and (d) if |Hğ‘‹ ,ğ‘šğ‘ğ‘¥ | can be expressed as a function of ğ‘‹ , that
is, ğœ’ (ğ‘‹ ) â‰¡ |Hğ‘‹ ,ğ‘šğ‘ğ‘¥ |, then ğ‘„ is bounded by

ğ‘„ â‰¥ |ğ‘‰ |

ğ‘‹0 âˆ’ ğ‘†
ğœ’ (ğ‘‹0)

,

(1)

where ğ‘‹0 = arg minğ‘‹
The expression ğœŒ =

ğœ’ (ğ‘‹ )
ğ‘‹ âˆ’ğ‘† (Lemma 2 in Kwasniewski et al. [27]).

ğœ’ (ğ‘‹ )
ğ‘‹ âˆ’ğ‘† is called the computational intensity.

3 SIMPLE OVERLAP ACCESS PROGRAMS
In Section 2, we show how the I/O cost of a program can be
bounded by the maximum size of a subcomputation H in any valid
ğ‘‹ -partition of program CDAG. We now introduce Simple Over-
lap Access Programs (SOAP): a class of programs for which we
can derive tight analytic bounds of |H |. We leverage the SOAP
structure and design an end-to-end method for deriving I/O lower
bounds of input programs (summarized in Figure 2).
What is SOAP? Before introducing the formal definition, we start
with an illustrative example, which we use in the following sections.

Example 1. Consider the following 3-point stencil code (we use the
Python syntax in code listings):

for t in range (1 , T ):

for i in range (t ,N -t ):

A[i ,t +1]=( A [i -1 , t] + A[i ,t] + A [i +1 , t ])/3 + B[ i ]

This is what we will refer to as a single-statement SOAP. The program
consists of one statement ğ‘†ğ‘¡ : A[i,t+1]=(A[i,t] + ... which is
placed in two nested loops. All accessed data comes from static, dis-
joint, multi-dimensional arrays (A and B). Furthermore, different ac-
cesses to the same array (array A is referenced by [i,t+1], [i-1,t],
[i,t],[i+1,t]) are offset by a constant stride [0,1], [-1,0],[0,0],
[1,0]. We denote such access pattern as a simple overlap and it is
a defining property of SOAP.
Why SOAP? We use the restriction on the access pattern to pre-
cisely count the number of vertices in ğ·ğ‘œğ‘š(H ). If we allow arbi-
trary overlap of array accesses, we need to conservatively assume

3

Input program: statements St1and St2C[i,j]=(A[i]+A[i+1])*(B[j]+B[j+1])Input arrays: In(St1) = {A, B}  Output array: Out(St1) = {C} Access function vectors:statement St1n2 = 2: two 1-dim access fun. vector componentsE[i,j]+= C[i,k] * D[k,j]Input arrays In(St2) = {C, D, E}  Output array: Out(St1) = {E}  Access function vectors:statement St2n1= 1: one 2-dim access fun. vector componentArray AArray BArray DArray EProgram CDAG for N=M=2, K=3 Program SDGSubgraph statementfori in range(N): forj in range(M): St1: C[i,j] = (A[i]+A[i+1])*(B[j]+B[j+1])fori in range(N): forj in range(K): fork in range(M):St2:E[i,j]+= C[i,k] * D[k,j]ABDCEABDCEArray CSection 3Section 6Section 6.1Elements of Care recomputed, decreasing the I/O cost!Section 4Bounding by counting vertices inEquivalent of executing statements St1 and St2â€œtogetherâ€Section 6.2Technical Report, 2021,

G. Kwasniewski et al.

a maximum possible overlap of accessed vertices. This reduces the
lower bound on |ğ·ğ‘œğ‘š(H )|, which, in turn, increases the upper
bound on |H |, providing less-tight I/O lower bound for a program.
This is not a fundamental limitation of our method. However, it al-
lows a fully automatic derivation of tight I/O lower bounds for input
programs. If the restriction is violated, additional assumptions on the
access overlap are needed (Section 5).
SOAP definition. A program is a sequence of statements ğ‘†ğ‘¡1, . . . , ğ‘†ğ‘¡ğ‘˜ .
Each such statement ğ‘†ğ‘¡ is a constant time computable function ğ‘“
enclosed in a loop nest of the following form:

for ğœ“ 1 âˆˆ D1 :
. . .
for ğœ“ â„“ âˆˆ Dâ„“ (ğœ“ 1, . . . ,ğœ“ â„“ âˆ’1) :

ğ‘†ğ‘¡ : ğ´0 [ğ“0 (ğ) ] â† ğ‘“ (ğ´1 [ğ“1 (ğ) ], ğ´2 [ğ“2 (ğ) ], . . . , ğ´ğ‘š [ğ“ğ’ (ğ) ])

where:
(1) The statement ğ‘†ğ‘¡ is nested in a loop nest of depth â„“.
(2) Each loop in the ğ‘¡th level, ğ‘¡ = 1, . . . , â„“ is associated with its
iteration variable ğœ“ ğ‘¡ , which iterates over its domain Dğ‘¡ âŠ‚ N.
Domain Dğ‘¡ may depend on iteration variables from outer loops
ğœ“ 1, . . . ,ğœ“ ğ‘¡ âˆ’1 (denoted as Dğ‘¡ (ğœ“ 1, . . . ,ğœ“ ğ‘¡ âˆ’1)).

(3) All â„“ iteration variables form the iteration vector ğ = [ğœ“ 1, . . . ,ğœ“ â„“ ]
and we define the iteration domain D as the set of all values the
iteration vector iterates over during the entire execution of the
program D âŠ‚ Nâ„“ .

ğ‘— , . . . ,ğœ“
ğ‘— Ã—, . . . , Ã—D

(4) The dimension of array ğ´ğ‘— is denoted as ğ‘‘ğ‘–ğ‘š(ğ´ğ‘— ).
(5) Elements of ğ´ğ‘— are referenced by an access function vector ğ“ ğ‘—
ğ‘‘ğ‘–ğ‘š (ğ´ ğ‘— )
]
ğ‘—

which maps ğ‘‘ğ‘–ğ‘š(ğ´ğ‘— ) iteration variables ğ ğ‘— = [ğœ“ 1
to a set of ğ’ğ’‹ elements from ğ´ğ‘— , that is ğ“ ğ‘— : D1
â†’

(cid:16)Nğ‘‘ğ‘–ğ‘š (ğ´ğ‘— ) (cid:17)ğ‘› ğ‘—

ğ‘— Ã—, . . . , D

. We then write ğ“ ğ‘— = [ğ“ ğ‘—,1, . . . , ğ“ ğ‘—,ğ‘› ğ‘— ], where
ğ‘‘ğ‘–ğ‘š (ğ´ğ‘— )
. . . , ğ‘› ğ‘— . Further-
ğ‘—

â†’ Nğ‘‘ğ‘–ğ‘š (ğ´ ğ‘— ), ğ‘˜ = 1,

ğ“ ğ‘—,ğ‘˜ : D1
more, all access function components ğ“ ğ‘—,ğ‘˜ (ğ ğ‘— ) are injective.
(6) All ğ‘› ğ‘— access function vectorâ€™s components are equal up to a
constant translation vector, that is, âˆ€ğ‘˜ = 1, . . . , ğ‘› : ğ“ ğ‘—,ğ‘˜ (ğ) =
] âˆˆ Nğ‘‘ğ‘–ğ‘š (ğ´ğ‘— ) . We call
ğ“ ğ‘—,1 (ğ) + ğ’•ğ‘˜ , where ğ’•ğ‘˜ = [ğ‘¡ 1
ğ‘˜
ğ“ ğ‘— the simple overlap access.

, . . . , ğ‘¡ğ‘‘ğ‘–ğ‘š (ğ´)
ğ‘˜

(7) Arrays ğ´1, . . . ğ´ğ‘š are disjoint. If the output array ğ´0 is also used
as an input, that is, ğ´0 â‰¡ ğ´ğ‘— , ğ‘— â‰¥ 1, then ğ“0 âˆª ğ“ ğ‘— is also the
simple overlap access (c.f. Example 1).

)
3
Â§
(
n
o
i
t
i
n
i
f
e
d
P
A
O
S

ğ´0
ğ´ğ‘— , ğ‘— = 1, . . . , ğ‘š
ğ = [ğœ“ 1, . . . ,ğœ“ â„“ ]
D âŠ† D1Ã—, . . . , Ã—Dâ„“

ğ“ ğ‘— = [ğ“ ğ‘—,1, . . . , ğ“ ğ‘—,ğ‘› ğ‘— ]

ğ’• ğ‘—,ğ‘˜ = [ğ‘¡ 1

ğ‘—,ğ‘˜ , . . . , ğ‘¡

ğ‘‘ğ‘–ğ‘š (ğ´ğ‘— )
ğ‘—,ğ‘˜

]

) P (ğ‘‹ ) = {H1, . . . , Hğ‘  }

4
Â§
(
n
o
i
t
a
t
u
p
m
o
c
b
u
s

t
n
e
m
e
t
a
t
s
-
e
l
g
n
i
S

ğ‘« = ğ· 1Ã—, . . . , Ã—ğ·â„“

H âŠ† ğ‘« âŠ† ğ‘‰

A = ğ“ [H ]

^ğ‘¡ğ‘– = {ğ‘¡ğ‘–
1

, . . . , ğ‘¡ğ‘–

ğ‘› } \ {0}

Dom ( H)
ğœŒ

ğ‘„ â‰¥ |D |

(cid:205)ğ‘š
ğ‘— =1
(cid:206)â„“

ğ‘¡ =1

|A ğ‘— (ğ‘‹0 ) |âˆ’ğ‘†
|ğ·ğ‘¡ (ğ‘‹0 ) |

) ğºğ‘† = (ğ‘‰ğ‘† , ğ¸ğ‘† )

6
Â§
(

ğ¼ âŠ‚ ğ‘‰ğ‘†

G
D
S

ğºğ‘† [ğ» ], ğ» âŠ‚ ğ‘‰ğ‘† \ ğ¼

Stğ»

ğ‘—

ğ‘— , . . . ,ğœ“ ğ‘‘ğ‘–ğ‘š (ğ´ğ‘— )

] to ğ‘› ğ‘— elements in array ğ´ğ‘— .

Output array of statement St (may overlap
with input arrays).
Mutually disjoint input arrays of statement St.
Iteration vector composed of â„“ iteration variables.
Iteration domain: a set of values that iteration
vector ğ takes during the entire program execution.
Access function vector that maps ğ‘‘ğ‘–ğ‘š (ğ´ğ‘— ) variables
[ğœ“ 1
Translation vector of ğ‘˜-th access function vectorâ€™s
component ğ“ ğ‘—,ğ‘˜ , that is ğ“ ğ‘—,ğ‘˜ â‰¡ ğ“ ğ‘—,1, ğ‘˜ = 1, . . . , ğ‘› ğ‘—
An ğ‘‹ -partition of CDAG ğº = (ğ‘‰ , ğ¸) composed
of ğ‘  disjoint subcomputations.
Subcomputation domain: a Caresian product of
ranges of â„“ iteration variables during H.
Subcomputation H uniquely defined by a set
of |H | iteration vectorâ€™s values ğ âˆˆ ğ‘« taken
during H. If H = ğ‘«, we call it a rectangular
subcomputation Hğ‘Ÿğ‘’ğ‘ .
Access set: a set of vertices from array ğ´ that are
accessed by ğ“ during H.
Access offset set: set of all non-zero ğ‘–th coordinates
among ğ‘› translation vectors ğ’•ğ‘˜ , ğ‘˜ = 1, . . . , ğ‘›.
Dominator set of subcomputation H.
The computational intensity of the ğ‘‹ -partition.

A number of I/O operations of a schedule.

Symbolic Directed Graph, where every array
accessed in a program is a vertex, and edges
represent data dependencies between them.
Set of read-only arrays of the program.
SDG subgraph that represents a subcomputation
in which at least one vertex from every
array in ğ» is computed.
Subgraph SOAP statement.

ğ‘‹ -Partitioning on SOAPâ€™s CDAG. Recall that our objective is to
bound the maximum size of any subcomputation |H |. Given peb-
bling ğ‘ƒ and an associated ğ‘‹ -partition P (ğ‘‹ ), every subcomputation
H âˆˆ P (ğ‘‹ ) is therefore associated with the set of iteration vectors ğ of
the vertices computed in H . In the following section we will derive
it by counting how many non-input vertices (iteration vectors) can
H contain by bounding its dominator set size |ğ·ğ‘œğ‘š(H )| - again,
by counting vertices corresponding to each access ğ´ğ‘— [ğ“ ğ‘—,ğ‘˜ (ğ)].

ğ‘‘ğ‘–ğ‘š (ğ´ğ‘— )
ğ‘—

Table 1: Notation used in the paper.

(8) Each execution of statement ğ‘†ğ‘¡ is an evaluation of ğ‘“ for a given

value of iteration vector ğ.

4 I/O LOWER BOUNDS FOR

SINGLE-STATEMENT SOAP

Iteration variables and iteration vectors. Formally, an iteration
variable ğœ“ ğ‘¡ is an iterator: an object which takes values from its
iteration domain during the program execution. However, if it is
clear from the context, we will refer to a particular value of the
iteration variable simply as ğœ“ ğ‘¡ (or a value of iteration vector as ğ).
Vertices as iteration vectors. Since by definition of CDAG, each
computation corresponds to a different vertex, and by definition
of SOAP, every statement execution is associated with a single
iteration vector ğ, every non-input vertex in ğº is uniquely associ-
ated with an iteration vector ğ. Input vertices are referred to by
their access function vectors ğ‘¢ = ğ´ğ‘— [ğ“ ğ‘—,ğ‘˜ (ğ)]. We further define
CDAG edges as follows: for every value of iteration vector ğ, we
add an edge from all accessed elements to the vertex associated
with ğ, that is: ğ¸ = {(ğ‘¢, ğ‘£) : ğ‘¢ = ğ´ğ‘— [ğ“ ğ‘—,ğ‘˜ (ğ)], ğ‘£ = ğ, ğ âˆˆ D}.

4

We now derive the I/O bounds for programs that contain only one
SOAP statement. We start with introducing necessary definitions
that allow us to bound the size of a rectangular subcomputation.
The summary of the notation is presented in Table 1.

4.1 Definitions
Definition 1. Subcomputation domain. Denote the set of all val-
ues which iteration variable ğœ“ ğ‘¡ takes during subcomputation H
as ğ·ğ‘¡ âŠ‚ Dğ‘¡ , ğ‘¡ = 1, . . . , â„“. Then, the subcomputation domain
ğ‘« (H ) âŠ† D is a Cartesian product of ranges of all â„“ iteration vari-
ables which they take during H , that is ğ‘« (H ) = ğ·1Ã—, . . . , Ã—ğ·â„“ . We
therefore have H âŠ† ğ‘« (H ) âŠ‚ Nâ„“ . If it is clear from the context, we
will sometimes denote ğ‘« (H ) simply as ğ‘«.

Tight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

Example 2. Recall the program from Example 1. Consider subcom-
putation H in which t âˆˆ {1, 2} and i âˆˆ {1, 2}. Then, subcomputation
domain ğ‘« = {1, 2} Ã— {1, 2} = {[1, 1], [1, 2], [2, 1], [2, 2]}, but com-
putation itself can contain at most 3 elements H âŠ† {[1, 1], [1, 2], [2, 2]},
since ğ = [2, 1] âˆ‰ D does not belong to the iteration domain.

Definition 2. Access set and access subdomain. Consider input
array ğ´ and its access function vector ğ“. Given H , the access set A of
ğ´ is the set of vertices belonging to ğ´ that are accessed during H , that
is A = ğ“ [H ] = {ğ´[ğ“ (ğ)] : ğ âˆˆ H }. If function ğ“ = [ğ“1, . . . , ğ“ğ‘›]
accesses ğ‘› vertices from ğ´, we analogously define access sets for each
access function component ğ“ğ‘˜ [H ], ğ‘˜ = 1, . . . , ğ‘›. We then have A =
(cid:208)ğ‘›
ğ‘˜=1 ğ“ğ‘˜ [H ]. The access subdomain ğ‘« (A) is minimum bounding
box of the access set A.

Example 3. For program in Example 1, consider subcomputation H
evaluated on only one iteration vector H = [ğ‘– = 2, ğ‘— = 2]. We have
two accessed arrays A and B. Furthermore, we have ğ“A = [[ğ‘–, ğ‘¡ +1], [ğ‘– âˆ’
(cid:16)N2(cid:17) 4
1, ğ‘¡], [ğ‘–, ğ‘¡], [ğ‘– âˆ’ 1, ğ‘¡]]. Therefore, ğ‘‘ğ‘–ğ‘š(A) = 2, and ğ“B : N2 â†’
.
We further have ğ“B = [[ğ‘–]], ğ‘‘ğ‘–ğ‘š(B) = 1, and ğ“A : N â†’ N.
To evaluate ğ‘†ğ‘¡ for ğ = [2, 2], we need to access four elements of
A (three loads and one store), so its access set is A = ğ“A [H ] =
{[2, 3], [1, 2], [2, 2], [2, 3]}. Furthermore, we have the access subdo-
main ğ‘« (A) = {2, 3} Ã— {1, 2, 3}.

Definition 3. Access offset set. Given a simple overlap access ğ“ =
ğ‘‘ğ‘–ğ‘š (ğ´ ğ‘— )
[ğ“1, . . . , ğ“ğ‘›] consider its ğ‘› translation vectors ğ’•ğ‘˜ = [ğ‘¡ 1
]
ğ‘˜
ğ‘˜
âˆˆ Nğ‘‘ğ‘–ğ‘š (ğ´), ğ‘˜ = 1, . . . , ğ‘›. For each dimension ğ‘– = 1, . . . , ğ‘‘ğ‘–ğ‘š(ğ´ğ‘— ) we
denote ^ğ‘¡ğ‘– = {ğ‘¡ğ‘–
ğ‘› } \ {0} as the set of all unique non-zero ğ‘–th
1
coordinates among all ğ‘› translation vectors.

, . . . , ğ‘¡ğ‘–

, . . . , ğ‘¡

Definition 4. Rectangular subcomputation For a given subcom-
putation domain ğ‘«, a subcomputation H is called rectangular if
H = ğ‘« and is denoted Hğ‘Ÿğ‘’ğ‘ (ğ‘«). The size of rectangular computation
is |Hğ‘Ÿğ‘’ğ‘ (ğ‘«)| = (cid:206)â„“
ğ‘¡ =1 |ğ·ğ‘¡ |. If it is clear from the context, we will
denote Hğ‘Ÿğ‘’ğ‘ (ğ‘«) simply as Hğ‘Ÿğ‘’ğ‘ .

Observation 1. Consider a simple overlap access ğ“ = [ğ“1, . . . , ğ“ğ‘›]
of array ğ´ and a rectangular subcomputation Hğ‘Ÿğ‘’ğ‘ (ğ‘«). Then since all
ğ“ğ‘˜ are equal up to translation, the ranges of iteration variables they
access are also equal up to the same translation: âˆ€ğ‘– = 1, . . . , ğ‘‘ğ‘–ğ‘š(ğ´) :
âˆ€ğ‘— = 1, . . . , ğ‘› : ğ“ ğ‘— [ğ·ğ‘– ] = ğ“1 [ğ·ğ‘– ] + ğ‘¡ ğ‘— , which also implies that
âˆ€ğ‘– = 1, . . . , ğ‘‘ğ‘–ğ‘š(ğ´) : âˆ€ğ‘— = 1, . . . , ğ‘› : |ğ“ ğ‘— [ğ·ğ‘– ]| = |ğ“1 [ğ·ğ‘– ]|.

To bound the sizes of rectangular subcomputations, we use two

lemmas given by Kwasniewski et al. [27]:

Lemma 1. (Lemma 4 in [27]) For statement ğ‘†ğ‘¡, given ğ‘«, the size of
subcomputation H (number of vertices of ğ‘† computed during H ) is
bounded by the sizes of the iteration variablesâ€™ sets ğ·ğ‘¡ , ğ‘¡ = 1, . . . , â„“:
â„“
(cid:214)

|H | â‰¤

|ğ·ğ‘¡ |.

(2)

ğ‘¡ =1

Proof. Inequality 2 follows from a combinatorial argument:
each computation in H is uniquely defined by its iteration vector
[ğœ“ 1, . . . ,ğœ“ â„“ ]. As each iteration variable ğœ“ ğ‘¡ takes |ğ·ğ‘¡ | different val-
ues during H , we have |ğ·1| Â· Â· Â· Â· Â· |ğ·ğ‘¡ | = (cid:206)â„“
ğ‘¡ =1 |ğ·ğ‘¡ | ways how to
â–¡
uniquely choose the iteration vector in H .

5

(Lemma 5 in [27]) For the given access function ğ“ =
Lemma 2.
[ğ“1, . . . , ğ“ğ‘›] accessing array ğ´, ğ´[ğ“ (ğ)], the access set size of each
of components |ğ“ğ‘˜ [H ] | during subcomputation H is bounded by the
sizes of ğ‘‘ğ‘–ğ‘š(ğ“ğ´) iteration variablesâ€™ sets ğ·ğ‘–, ğ‘˜ = 1, . . . , ğ‘‘ğ‘–ğ‘š(ğ“ ğ‘— ):

|ğ“ğ‘˜ [H ] | â‰¤

ğ‘‘ğ‘–ğ‘š (ğ“ğ´)
(cid:214)

|ğ·ğ‘– |

(3)

ğ‘–=1

where ğ·ğ‘– âˆ‹ ğœ“ğ‘– is the iteration domain of variable ğœ“ğ‘– during H .
Proof. We use the same combinatorial argument as in Lemma 1.
Since access functions are injective, each vertex in ğ´ accessed by
]. Knowing the number
is uniquely defined by [ğœ“ 1
ğ‘˜
takes in H , we bound the number of
â–¡

ğ“ğ’Œ
of different values each ğœ“ ğ‘—
ğ‘˜
different access vectors ğ“ğ‘˜ [H ].

, . . . ,ğœ“ğ‘‘ğ‘–ğ‘š (ğ´)
ğ‘˜

4.2 Bounding SOAP Access Size
Recall that our goal is to find the maximum size of the subcompu-
tation given its dominator size. We first do the converse: given the
rectangular subcomputation Hğ‘Ÿğ‘’ğ‘ , we bound the minimum num-
ber of input vertices required to compute Hğ‘Ÿğ‘’ğ‘ . In Section 4.4 we
prove that indeed Hğ‘Ÿğ‘’ğ‘ is the subcomputation that upper-bounds
the maximum computational intensity ğœŒ. Since arrays ğ´1, . . . , ğ´ğ‘š
are disjoint, the total number of input vertices is the sum of their
access set sizes: |ğ·ğ‘œğ‘šğ‘šğ‘–ğ‘› (Hğ‘Ÿğ‘’ğ‘ ) â‰¥ (cid:205)ğ‘š
ğ‘—=1 |A ğ‘— |. We now proceed to
bound individual access set sizes |A ğ‘— |.

Consider array ğ´ with ğ‘‘ğ‘–ğ‘š(ğ´) = ğ‘‘ and its access function ğ“ (ğ) =
[ğ“1 (ğ), . . . , ğ“ğ‘› (ğ)] that access ğ‘› elements from ğ´ (to simplify
the notation, we drop the subscript ğ‘—, since we consider only one
array). Observe that during Hğ‘Ÿğ‘’ğ‘ , all combinations of iteration
variablesğœ“ 1 âˆˆ ğ·1, . . . ,ğœ“ â„“ âˆˆ ğ·â„“ are accessed, so |Hğ‘Ÿğ‘’ğ‘ | = (cid:206)â„“
ğ‘¡ =1 |ğ·ğ‘¡ |
(Lemma 1). This also implies that each of ğ‘˜ = 1, . . . , ğ‘› accesses to
ğ´ required |ğ“ğ‘˜ [Hğ‘Ÿğ‘’ğ‘ ]| = (cid:206)ğ‘‘
ğ‘¡ =1 |ğ·ğ‘¡ | vertices from ğ´ (Lemma 2 and
Observation 1). Therefore, the total number of accesses to array
ğ´ during Hğ‘Ÿğ‘’ğ‘ is |A| â‰¥ (cid:206)ğ‘‘
ğ‘¡ =1 |ğ·ğ‘¡ |. However, the sets of vertices
accessed by different ğ“ğ‘˜ may overlap, that is, there may exist two
accesses ğ“ğ‘™ and ğ“ğ‘š, for which ğ“ğ‘™ [Hğ‘Ÿğ‘’ğ‘ ]âˆ©ğ“ğ‘š [Hğ‘Ÿğ‘’ğ‘ ] â‰  âˆ…. Therefore,
we also obtain the upper bound |A| â‰¤ (cid:205)ğ‘›
ğ‘¡ =1 |ğ·ğ‘¡ |. We now
want to narrow the gap between the upper and the lower bounds.

(cid:206)ğ‘‘

ğ‘—=1

If a given input array ğ´ with ğ‘‘ğ‘–ğ‘š(ğ´) = ğ‘‘ is accessed
Lemma 3.
by a simple overlap access ğ“ (ğ) = [ğ“1 (ğ), . . . , ğ“ğ‘› (ğ)], its access set
size |A| during rectangular computation Hğ‘Ÿğ‘’ğ‘ (ğ‘«) is bounded by

|A| = |ğ“ [Hğ‘Ÿğ‘’ğ‘ (ğ‘«)]| â‰¥ 2

ğ‘‘
(cid:214)

ğ‘–=1

|ğ·ğ‘– | âˆ’

ğ‘‘
(cid:214)

ğ‘–=1

(|ğ·ğ‘– | âˆ’ |^ğ‘¡ğ‘– |),

(4)

where |^ğ‘¡ğ‘– | is the size of the access offsets set in the ğ‘–th dimension.

Proof. W.l.o.g., consider the first access function component ğ“1
and its (cid:206)ğ‘‘
ğ‘¡ =1 |ğ·ğ‘¡ | accessed vertices ğ“1 [Hğ‘Ÿğ‘’ğ‘ ]. We will lower bound
the number of accesses to ğ´ from remaining ğ“ğ‘˜, ğ‘˜ = 2, . . . , ğ‘›, which
do not overlap with ğ“1 [Hğ‘Ÿğ‘’ğ‘ ], that is | (cid:208)ğ‘›
ğ‘˜=2 ğ“ğ‘˜ [Hğ‘Ÿğ‘’ğ‘ ] \ ğ“1 [Hğ‘Ÿğ‘’ğ‘ ]|.
Since by construction of Hğ‘Ÿğ‘’ğ‘ , all ğ“ğ‘˜ [Hğ‘Ÿğ‘’ğ‘ ] are Cartesian products
of iteration variablesâ€™ ranges ğ“ğ‘˜ [ğ·1]Ã—, Â· Â· Â· , Ã—ğ“ğ‘˜ [ğ·ğ‘‘ ], there is a
bijection between ğ“ğ‘˜ [Hğ‘Ÿğ‘’ğ‘ ] and an ğ‘‘-dimensional hyperrectangle
ğ»ğ‘˜ âˆˆ Nğ‘‘ . To secure correctness of our lower bound on |A|, we need
to find the volume of the smallest union of these hyperrectangles.

Technical Report, 2021,

G. Kwasniewski et al.

4.3 Input-Output Simple Overlap
If one of the input arrays ğ´ğ‘–, ğ‘– â‰¥ 1, is also the output array ğ´0,
then their access function vectors ğ“0 and ğ“ğ‘– form together a simple
overlap access (Section 3). In such cases, some vertices accessed by
ğ“ğ‘– during Hğ‘Ÿğ‘’ğ‘ may be computed and do not need to be loaded. We
formalize it in the following corollary, which follows directly from
Lemma 3:

Corollary 1. Consider statement ğ‘†ğ‘¡ that computes array ğ´, ğ‘‘ğ‘–ğ‘š(ğ´) =
ğ‘‘ and simultaneously accesses it as an input ğ´[ğ“0 (ğ)] = ğ‘“ (ğ´[ğ“1 (ğ)]).
If ğ“0 âˆª ğ“1 is a simple overlap access, the access set size |A| during
rectangular computation Hğ‘Ÿğ‘’ğ‘ is bounded by

|A| â‰¥

ğ‘‘
(cid:214)

ğ‘–=1

|ğ·ğ‘– | âˆ’

ğ‘‘
(cid:214)

ğ‘–=1

(|ğ·ğ‘– | âˆ’ |^ğ‘¡ğ‘– |),

(6)

where ^ğ‘¡ is an access offset offset set of ğ“0 âˆª ğ“1.

ğ‘–=1 |ğ·ğ‘– | âˆ’ (cid:206)ğ‘‘

Proof. This result follows directly from Lemma 3. Since there
are at least 2 (cid:206)ğ‘‘
ğ‘–=1 (|ğ·ğ‘– | âˆ’ |^ğ‘¡ğ‘– |) vertices accessed from
ğ´ğ‘– , and at most (cid:206)ğ‘‘
ğ‘–=1 |ğ·ğ‘– | of them can be computed during Hğ‘Ÿğ‘’ğ‘
(Lemma 2) and therefore, do not have to be loaded, then at least
2 (cid:206)ğ‘‘
ğ‘–=1 (|ğ·ğ‘– | âˆ’ |^ğ‘¡ğ‘– |) âˆ’ (cid:206)ğ‘‘
ğ‘–=1 |ğ·ğ‘– | elements have to be
â–¡
accessed from the outside of Hğ‘Ÿğ‘’ğ‘ .

ğ‘–=1 |ğ·ğ‘– | âˆ’ (cid:206)ğ‘‘

4.4 Bounding Maximal Subcomputation
In Section 4.2 we lower-bounded the dominator set size of the rectan-
gular subcomputation |ğ·ğ‘œğ‘šğ‘šğ‘–ğ‘› (Hğ‘Ÿğ‘’ğ‘ )| = (cid:205)ğ‘š
ğ‘—=1 |A ğ‘— | by bounding
the sizes of simple overlap access sets sizes |A ğ‘— | (Lemma 3). Recall
that to bound the I/O lower bound we need the size ğœ’ (ğ‘‹ ) of the
maximal subcomputation Hğ‘šğ‘ğ‘¥ for given value of ğ‘‹ (Inequality 1).
We now prove that Hğ‘Ÿğ‘’ğ‘ upper-bounds the size of Hğ‘šğ‘ğ‘¥ .

(cid:205)ğ‘š

|H |
ğ‘— =1 |ğ“ ğ‘— [H ] |

Given H , denote the the ratio of the size of the subcomputa-
. By definition,

tion to the dominator set size ğ›¿ (H ) =
Hğ‘šğ‘ğ‘¥ maximizes ğ›¿ among all valid H âˆˆ P. We need to show
that for a fixed subcomputation domain ğ‘«0, among all subcompu-
tations for which ğ‘« (H ) = ğ‘«0, the rectangular subcomputation
Hğ‘Ÿğ‘’ğ‘ (ğ‘«0) upper-bounds ğ›¿. Note that an ğ‘‹ -partition derived from
the optimal pebbling schedule ğ‘ƒğ‘œğ‘ğ‘¡ may not include Hğ‘Ÿğ‘’ğ‘ . How-
ever, âˆ€ğ‘‹ : ğœ’ğ‘Ÿğ‘’ğ‘ (ğ‘‹ ) â‰¥ ğœ’ (ğ‘‹ ), that is, given ğ‘‹ , the size of Hğ‘Ÿğ‘’ğ‘ s.t.,
(cid:205)ğ‘š
ğ‘—=1 |ğ“ ğ‘— [Hğ‘Ÿğ‘’ğ‘ ]| = ğ‘‹ will always be no smaller than the size of
Hğ‘šğ‘ğ‘¥ . To show this, we first need to introduce some auxiliary
definitions.

Iteration variables, their indices, and their values. To sim-
plify the notation, throughout the paper we used the iteration vari-
ables ğœ“ğ‘– and the values they take for some iteration interchangeably.
However, now we need to make this distinction explicit. The it-
eration vector consists of â„“ iteration variables ğ = [ğœ“ 1, . . . , ğœ“ â„“ ].
Each access function ğ“ ğ‘— is defined on ğ‘‘ğ‘–ğ‘š(ğ´ğ‘— ) â‰¤ â„“ of them. Re-
call that ğ ğ‘— is the set of iteration variables accessed by ğ“ ğ‘— (Sec-
tion 3, property (5)). To keep track of the indices of particular
iteration variables, denote ğš¿ = [â„“] = {1, . . . , â„“ } âŠ‚ N, ğš¿ğ‘— âŠ† ğš¿,
and ğš¿â€²
ğ‘— = ğš¿ \ ğš¿ğ‘— as the sets of integers. If ğ‘– âˆˆ ğš¿ğ‘— , then the ğ‘–th
iteration variable ğœ“ğ‘– is accessed by the access function ğ“ ğ‘— . We fur-
ther define ğâˆ— âˆˆ Nâ„“ as a specific value of the iteration vector ğ
that uniquely defines a single non-input vertex. We analogously
define ğâˆ—
(the last one being a value of ğ‘–th iteration

ğ‘— , ğœ“ğ‘–,âˆ—, and ğœ“ğ‘–,âˆ—

ğ‘—

Intuition behind Lemma 3. Access sets ğ“ [ Hğ‘Ÿğ‘’ğ‘ (ğ‘«) ] as 3-
Figure 3:
dimensional hyperrectangles. The union | (cid:208)ğ‘›
ğ‘˜=1 ğ“ğ‘˜ [Hğ‘Ÿğ‘’ğ‘ ] | (and therefore,
the total number of accesses | A | ) is minimized when the hyperrectangles
are placed in two antipodal locations of the subcomputation domain D.

ğ‘˜=1

Note that |^ğ‘¡ğ‘– | is a lower bound on the maximum offset between
any two ğ» ğ‘— â‰  ğ»ğ‘˜ in dimension ğ‘–: the union of all hyperrectangles
(cid:208)ğ‘›
ğ»ğ‘˜ â€œstretchesâ€ at least |ğ·ğ‘– | + |^ğ‘¡ğ‘– | elements in the ğ‘–th dimension
for all ğ‘– = 1, . . . , ğ‘‘ (see Figure 3). To see this, observe that since
ğ·ğ‘– âŠ‚ ğ‘ , for each element in the access offset set ğ‘¡ğ‘–
ğ‘— âˆˆ ^ğ‘¡ğ‘– there is at
least one element in ğ·ğ‘– + ğ‘¡ğ‘–
ğ‘— that is not in ğ·ğ‘– , which implies that
ğ‘— ) \ ğ·ğ‘– | â‰¥ 1. Since ğ·ğ‘– is finite, there is a single well-defined
|(ğ·ğ‘– + ğ‘¡ğ‘–
maximum and a minimum element, which implies that (max{ğ·ğ‘– } +
ğ‘— âˆ‰ ğ·ğ‘– ). Also, because by definition of ^ğ‘¡ğ‘– we
ğ‘— âˆ‰ ğ·ğ‘– ) âˆ¨ (min{ğ·ğ‘– } +ğ‘¡ğ‘–
ğ‘¡ğ‘–
have âˆ€ğ‘¡ğ‘–
, then we also have that each ğ‘¡ğ‘–
ğ‘— â‰  ğ‘¡ğ‘–
ğ‘— accesses
ğ‘˜
at least one â€œnon-overlappingâ€ element independent of any other
ğ‘¡ğ‘–
ğ‘˜

, that is âˆ€ğ‘¡ğ‘–
The arrangement of hyperrectangles ğ»ğ‘˜, ğ‘˜ = 1, . . . , ğ‘› in a Nğ‘‘
lattice s.t., their bounding box is ğ‘« = (|ğ·1| + |^ğ‘¡ 1|) Ã— Â· Â· Â· Ã— (|ğ·ğ‘‘ | +
|^ğ‘¡ğ‘‘ |), which minimizes the size of their union | (cid:208)ğ‘˜ ğ»ğ‘˜ | satisfies two
properties:

ğ‘˜ âˆˆ ^ğ‘¡ğ‘– : max{ğ·ğ‘– } + ğ‘¡ğ‘–

ğ‘— â‰  max{ğ·ğ‘– } + ğ‘¡ğ‘–
ğ‘˜

ğ‘˜ âˆˆ ^ğ‘¡ğ‘– : ğ‘¡ğ‘–

ğ‘— , ğ‘¡ğ‘–

ğ‘— , ğ‘¡ğ‘–

.

(1) there exist two â€œextremeâ€ ğ»ğ‘ , ğ»ğ‘, such that ğ»ğ‘ = ğ»ğ‘ + ğ’—,

ğ’– = Zğ‘‘, âˆ€ğ‘–=1,...,ğ‘‘ : |ğ‘£ğ‘– | = |^ğ‘¡ğ‘– |,

(2) all the remaining ğ»ğ‘˜, ğ‘˜ â‰  ğ‘, ğ‘ perfectly overlap with the

â€œextremeâ€ hyperrectangles ğ»ğ‘˜ âŠ† ğ»ğ‘ âˆª ğ»ğ‘.

ğ‘, s.t., ğ»ğ‘–

hyperrectangles ğ»ğ‘–
ğ‘ is offset from ğ»ğ‘–
ğ»ğ‘–
(cid:208)ğ‘–, |^ğ‘¡ğ‘– |>0 (ğ»ğ‘–
equal, but there are no restrictions on ğ»ğ‘–
the union (cid:208)ğ‘–, |^ğ‘¡ğ‘– |>0 (ğ»ğ‘–

To see this, observe that for every non-zero |^ğ‘¡ğ‘– | we need two
ğ‘ â‰  ğ»ğ‘–
ğ‘ + [Â·, . . . , |^ğ‘¡ğ‘– |, . . . , Â·], that is,
ğ‘ = ğ»ğ‘–
ğ‘ by |^ğ‘¡ğ‘– | in ğ‘–th dimension. We therefore have
ğ‘ and ğ»ğ‘–
ğ‘ are pairwise non-
ğ‘, ğ» ğ‘—
ğ‘, ğ‘– â‰  ğ‘—, we have that
ğ‘ = ğ» ğ‘—
ğ‘ .
ğ»ğ‘˜ | s.t. to the claimed

ğ‘) is minimized if âˆ€ğ‘–â‰ ğ‘— ğ»ğ‘–
ğ‘˜=1

Finally, observe the volume of | (cid:208)ğ‘›

ğ‘) âŠ† (cid:208)ğ‘˜ ğ»ğ‘˜ . Since ğ»ğ‘–

ğ‘ âˆª ğ»ğ‘–

ğ‘ âˆª ğ»ğ‘–

arrangement is:

(cid:12)
(cid:12)
(cid:12)

ğ‘›
(cid:216)

ğ‘˜=1

ğ»ğ‘˜

(cid:12)
(cid:12) = |ğ»ğ‘ âˆª ğ»ğ‘ | = 2
(cid:12)

ğ‘‘
(cid:214)

ğ‘–=1

|ğ·ğ‘– | âˆ’

ğ‘‘
(cid:214)

ğ‘–=1

(|ğ·ğ‘– | âˆ’ |^ğ‘¡ğ‘– |)

(5)

It shows that for any set of ğ‘› hyperrectangles s.t. the given
constraint, the volume of their union is no smaller than the one
in Equation 5. Since the offset constraint is also a lower bound on
the number of non-overlapping accesses in each dimension, it also
forms the bound on | (cid:208)ğ‘›
â–¡

ğ‘˜=1 ğ“ğ‘˜ [Hğ‘Ÿğ‘’ğ‘ ]| = |ğ“ [Hğ‘Ÿğ‘’ğ‘ ]| = |A|.

6

Iteration vector(3 iteration variables)Access functionvector (2 components)Iteration variablesâ€™ rangesTight I/O Bounds of Statically Analyzable Programs

of the access set: ğ›¿ âˆ’1

ğ‘— =

|ğ“ ğ‘— [H ] |
|H |

=

Technical Report, 2021,

. Therefore, we can

|ğ·ğ‘– |

1
(cid:206)ğ‘–âˆˆğš¿â€²
ğ‘—

safely maximize ğ“ ğ‘— [H ] to the entire access set of the rectangular
subcomputation Hğ‘Ÿğ‘’ğ‘ without increasing ğ›¿ âˆ’1
. We conclude that
for every access function ğ“ ğ‘— and every iteration variable index ğ‘–,
evaluating all vertices ğâˆ— s.t. ğœ“ğ‘– iterates over the entire domain ğ·ğ‘–
minimizes ğ›¿ âˆ’1

.

ğ‘—

ğ‘—

â–¡

Figure 4: Intuition behind Lemma 4: extending the subcomputation in the
free dimensions w.r.t ğ“ ğ‘— does not increase |ğ“ ğ‘— [H ] |. Once the subcomputa-
tion is almost rectangular, extending ğ» in the remaining dimensions keeps
the ratio ğ›¿ âˆ’1

constant.

ğ‘—

variable of the ğ‘—th access). We also define ğœƒ (ğâˆ—
of vertices in H that have all their ğš¿ğ‘— coordinates equal to ğâˆ—
ğ‘— , H ) = |{ğâˆ— : ğâˆ— âˆˆ H âˆ§ (âˆ€ğ‘– âˆˆ ğš¿ğ‘— : ğœ“ğ‘–,âˆ— = ğœ“ğ‘–,âˆ—
is ğœƒ (ğâˆ—
)}|.
We now formalize our claim in the following lemma:

ğ‘— , H ) as the number
ğ‘— , that

ğ‘—

Lemma 4. Given the subcomputation domain ğ‘«0, Hğ‘Ÿğ‘’ğ‘ (ğ‘«0) maxi-
mizes ğ›¿ (H ) for all H s.t. ğ‘« (H ) = ğ‘«0.

âˆ€H : ğ›¿ (H ) â‰¤ ğ›¿ (Hğ‘Ÿğ‘’ğ‘ )

(7)

Proof. Instead of maximizing ğ›¿ (H ), we will minimize ğ›¿ âˆ’1 (H ) =
((cid:205)ğ‘š
ğ‘—=1 |ğ“ ğ‘— [H ]|)/|H | = (cid:205)ğ‘š
ğ‘—=1 |ğ“ ğ‘— [H ]|/|H | over all possible H . Ob-
serve that ğ›¿ âˆ’1 (H ) is linear w.r.t. the ratios of individual access
function sets sizes |ğ“ ğ‘— [H ]| and the size of subcomputation |H |.
Therefore, we can examine each access ğ“ ğ‘— [H ] separately and show
that every ğ›¿ âˆ’1
ğ‘— = |ğ“ ğ‘— [H ]|/|H | is minimized for H = Hğ‘Ÿğ‘’ğ‘ . Then,
, then ğ›¿ âˆ’1 = (cid:205)ğ‘š
if Hğ‘Ÿğ‘’ğ‘ minimizes each of ğ›¿ âˆ’1
is minimized,
ğ‘—=1
so indeed Hğ‘Ÿğ‘’ğ‘ maximizes the ratio of the subcomputation size to
the dominator set size.

ğ›¿ âˆ’1
ğ‘—

ğ‘—

ğ‘—

ğ‘—

ğ‘— , H ) for all ğâˆ—

Observe now, that for any H we have that âˆ€ğ‘— : ğ›¿ âˆ’1

is monoton-
ically decreasing w.r.t. ğœƒ (ğâˆ—
ğ‘— âˆˆ ğ“ ğ‘— [H ]. That is - pick
any input vertex ğâˆ—
ğ‘— from the set of vertices accessed by ğ“ ğ‘— [H ].
Adding compute vertices ğâˆ— to H that access ğâˆ—
ğ‘— do not increase
the access set size ğ“ ğ‘— [H ], since ğâˆ—
ğ‘— is already accessed. However,
it increases the size of H . Clearly, ğ›¿ âˆ’1
reaches its minimum if
ğ‘—
|ğ·ğ‘– |, that is, H computes all
âˆ€ğâˆ—
vertices spanned by the access set ğ“ ğ‘— [H ] and all elements in the
Cartesian product of â€œfreeâ€ (independent of the access function ğœ™ ğ‘— )
iteration domains ğ·ğ‘–, ğ‘– âˆˆ ğš¿â€²
ğ‘— .

ğ‘— âˆˆ ğ“ ğ‘— [H ] : ğœƒ (ğâˆ—

ğ‘— , H ) = (cid:206)ğ‘– âˆˆğš¿â€²

We showed that for all ğ‘—, given its initial access set ğ“ ğ‘— [H ], the
ratio ğ›¿ âˆ’1
is minimized for the â€œalmost-rectangularâ€ subcomputation,
ğ·ğ‘– .
that is, H which computes all vertices ğâˆ— âˆˆ ğ“ ğ‘— [H ] Ã— (cid:206)ğ‘– âˆˆğš¿â€²
We now need to show that also extending H over the â€œdependentâ€
ranges ğš¿ğ‘— wonâ€™t increase the ratio ğ›¿ âˆ’1. When the access set size
ğ“ ğ‘— [H ] increases by a factor ğ‘¥, H increases proportionally by ğ‘¥ too,
keeping the ratio constant (See Figure 4 for an example for â„“ = 3).
separately, indepen-
ğ‘—
dently of other ğ›¿ âˆ’1
, ğ‘– â‰  ğ‘—, assume that we have already extended
H to the â€œalmost-rectangularâ€ subcomputation, that is, all com-
ğ·ğ‘– were accessed in H . Observe now that
binations of (cid:206)ğ‘– âˆˆğš¿â€²
|ğ·ğ‘– | for any vertex ğâˆ—
ğ‘— , H ) = (cid:206)ğ‘– âˆˆğš¿â€²
ğœƒ (ğâˆ—
ğ‘— . Therefore, since |H | =
|ğ·ğ‘– |, we see that ğ›¿ âˆ’1
(cid:205)
(cid:206)ğ‘– âˆˆğš¿â€²
is constant w.r.t., the size

Since our goal is to minimize each ğ›¿ âˆ’1

ğ‘–

ğ‘—

ğ‘—

ğ‘—

ğ‘—

ğ‘—

ğâˆ—

ğ‘— âˆˆğ“ ğ‘— [H ]

ğ‘—

4.5 I/O Lower Bounds and Optimal Tiling
We now proceed to the final step of finding the I/O lower bound. Re-
call from Section 2.2, that the last missing piece is ğœ’ (ğ‘‹ ); that is, we
seek to express |Hğ‘šğ‘ğ‘¥ (ğ‘«)| = (cid:206)â„“
ğ‘¡ =1 |ğ·ğ‘¡ | as a function of ğ‘‹ . Observe
that by Lemma 4, |ğ·ğ‘œğ‘šğ‘šğ‘–ğ‘› (Hğ‘šğ‘ğ‘¥ (ğ‘«)) â‰¥ (cid:205)ğ‘š
ğ‘— | âˆ’
(cid:206)ğ‘‘ğ‘–ğ‘š (ğ´ ğ‘— )
ğ‘— |). On the other hand, by definition of
ğ‘–=1
ğ‘‹ -Partitioning, |ğ·ğ‘œğ‘šğ‘šğ‘–ğ‘› (Hğ‘šğ‘ğ‘¥ (ğ‘«))| â‰¤ ğ‘‹ . Combining these in-
equalities, we solve for all |ğ·ğ‘¡ | as functions of ğ‘‹ by formulating
it as the optimization problem (see Section 3.2 in Kwasniewski et
al. [27]):

ğ‘—=1 (2 (cid:206)ğ‘‘ğ‘–ğ‘š (ğ´ğ‘— )
ğ‘–=1

ğ‘— | âˆ’ |^ğ‘¡ğ‘–

(|ğ·ğ‘–

|ğ·ğ‘–

max

â„“
(cid:214)

ğ‘¡ =1

|ğ·ğ‘¡ |

s.t.

ğ‘š
âˆ‘ï¸

ğ‘—=1

|A ğ‘— | â‰¤ ğ‘‹

âˆ€1 â‰¥ ğ‘¡ â‰¥ â„“ : |ğ·ğ‘¡ | â‰¥ 1

(8)

Solving the above optimization problem yields ğœ’ (ğ‘‹ ) = |Hğ‘šğ‘ğ‘¥ (ğ‘«)|.

Since Lemma 4 gives a valid upper bound on computational in-
tensity for any value of ğ‘‹ , we seek to find the tightest (lowest)
ğœ’ (ğ‘‹ )
upper bound. One can obtain ğ‘‹0 = arg minğ‘‹
ğ‘‹ âˆ’ğ‘† , since ğœ’ (ğ‘‹ ) is
differentiable. Finally, combining Lemma 3, inequality 1, and the
optimization problem 8, we obtain the I/O lower bound for the
single-statement SOAP program:

ğ‘„ â‰¥ |D|

(cid:205)ğ‘š

ğ‘—=1 |A ğ‘— (ğ‘‹0)| âˆ’ ğ‘†
(cid:206)â„“
ğ‘¡ =1 |ğ·ğ‘¡ (ğ‘‹0)|

,

(9)

where |A ğ‘— (ğ‘‹0)| are the access set sizes obtained from Lemma 3 for
the optimal value of |ğ·ğ‘– | derived from the optimization problem 8.
Substituting ğ‘‹0 back to |ğ·ğ‘¡ |(ğ‘‹ ) has a direct interpretation: they
constitute optimal loop tilings for the maximal subcomputation.
Note that such tiling might be invalid due to problem relaxations:
e.g., we ignore loop-carried dependencies and we solve optimization
problem 8 over real numbers, relaxing the integer constraint on
|ğ·ğ‘¡ | set sizes. However, this result can serve as a powerful guideline in
code generation. Furthermore, if derived tiling sizes generate a valid
code, it is provably I/O optimal.

5 PROJECTING PROGRAMS ONTO SOAP
By the definition of SOAP, one input array may be accessed by
different access function vector components, only if they form the
simple overlap access â€” that is, the accesses are offset by a constant
stride. However, our analysis may go beyond this constraint if
additional assumptions are met.

7

Extend inremainingdomainsTechnical Report, 2021,

G. Kwasniewski et al.

5.1 Non-Overlapping Access Sets
Given input array ğ´ and its access function components ğ“ (ğ) =
[ğ“1 (ğ1), . . . , ğ“ğ‘› (ğğ‘›)], if all access sets are disjoint, that is: âˆ€ğ‘–â‰ ğ‘— ğ“ğ‘– [D]âˆ©
ğ“ ğ‘— [D] = âˆ…, then we represent it as ğ‘› disjoint input arrays ğ´ğ‘– ac-
cessed by single corresponding access function component ğ“ğ‘– (ğğ‘– ).

Example 4. Consider the following code fragment from LU decom-
position:

for k in range (N ):

for i in range (k +1 , N ):

for j in range (k +1 , N ):

A[i ,j] = A[i ,j] - A[i ,k] * A[k , j]

ğ‘†ğ‘¡ :
The analysis of iteration variablesâ€™ domains Dğ‘–, D ğ‘— , Dğ‘˜ shows
that for fixed value of ğ‘˜0, there are no two iteration vectors ğ1 =
[ğ‘˜0, ğ‘–1, ğ‘—1] and ğ2 = [ğ‘˜0, ğ‘–2, ğ‘—2] such that [ğ‘–1, ğ‘˜0] = [ğ‘˜0, ğ‘—2] âˆ¨[ğ‘–1, ğ‘—1] =
[ğ‘˜0, ğ‘—2] âˆ¨[ğ‘–1, ğ‘—1] = [ğ‘–1, ğ‘˜0], therefore, their access sets are disjoint.
Furthermore, for ğ‘˜0, all elements from ğ´ in range [(ğ‘˜0, ğ‘ ), (ğ‘˜0, ğ‘ )]
are updated. Therefore, all accesses of form [ğ‘–1, ğ‘˜1] = [ğ‘˜2, ğ‘—2] access
different vertices. We model this as a SOAP statement with three
disjoint arrays:

ğ‘†ğ‘¡2 : ğ´1 [ğ‘–, ğ‘—] = ğ‘“ (ğ´1 [ğ‘–, ğ‘—], ğ´2 [ğ‘–, ğ‘˜], ğ´3 [ğ‘˜, ğ‘—])

5.2 Equivalent Input-Output Accesses
If array ğ´ is updated by statement ğ‘†ğ‘¡ â€” i.e., it is both input and
output â€” then we require that the output access function ğ“0 is
different than the input access function ğ“ğ‘– . If the input program
does not meet this requirement, we can add additional â€œversion
dimensionâ€ to access functions that is offset by a constant between
input and output accesses.

Example 5. Consider again Example 4. Observe that array ğ´1 is
updated (it is both the input and the output of ğ‘†ğ‘¡2. Furthermore, both
access functions are equal: ğ“0 = ğ“1 = [ğ‘–, ğ‘—]. We can associate a
unique version (and therefore, a vertex) of each element of ğ´ with a
corresponding iteration of the ğ‘˜ loop. We add the version dimension
associated with ğ‘˜ and offset it by constant 1 between input and output:

ğ‘†ğ‘¡3 : ğ´1 [ğ‘–, ğ‘—, ğ‘˜ + 1] = ğ‘“ (ğ´1 [ğ‘–, ğ‘—, ğ‘˜], ğ´2 [ğ‘–, ğ‘˜], ğ´3 [ğ‘˜, ğ‘—])

5.3 Non-Injective Access Functions
Given input array ğ´ and its access function vector ğ“, we require that
âˆ€ğğ‘– â‰  ğ ğ‘— : ğ´[ğ“ (ğğ‘– )] â‰  ğ´[ğ“ (ğ ğ‘— )]. If this is not the case, then we
seek to bound the size of such overlap, that is, given subcomputation
domain ğ‘« (H ), how many different iteration vectors ğ ğ‘— map to
the same array element ğ´[ğ“ (ğğ‘– )]. We can solve this by analyzing
the iteration domain D and the access function vector ğ“. If one
array dimension is accessed by a function of multiple iteration
variables ğ‘”(ğœ™ 1, . . . , ğœ™ğ‘˜ ) and ğ‘” is linear w.r.t. all ğœ™ğ‘– , the number of
different values ğ‘” takes in ğ‘« (H ) is bounded by maxğ‘–=1,...,ğ‘˜ {|ğ·ğ‘– |} â‰¤
|ğ‘”[H ]| â‰¤ (cid:206)ğ‘˜

ğ‘–=1 |ğ·ğ‘– |, for ğ·ğ‘– â‰  {0}, ğ‘– = 1, . . . , ğ‘˜.

Example 6. A single layer of the direct convolution used in neural
networks may be written as seven nested loops with iteration variables
ğ‘, ğ‘, ğ‘˜, ğ‘¤, â„, ğ‘Ÿ, ğ‘  and statement (c.f. [23]):

ğ‘†ğ‘¡ : ğ‘‚ğ‘¢ğ‘¡ [ğ‘˜, â„, ğ‘¤, ğ‘]+ = ğ¼ğ‘šğ‘ğ‘”ğ‘’ [ğ‘Ÿ + ğœğ‘¤ğ‘¤, ğ‘  + ğœâ„â„, ğ‘, ğ‘] Ã— ğ¹ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ [ğ‘˜, ğ‘Ÿ, ğ‘ ]

8

Depending on the value of ğœğ‘¤ and ğœâ„, the access function of ğ¼ğ‘šğ‘ğ‘”ğ‘’,
ğ“ = [ğ‘Ÿ + ğœğ‘¤ğ‘¤, ğ‘  + ğœâ„â„, ğ‘, ğ‘] may not be injective. Yet, observe that:
(1) ğœğ‘¤ â‰¥ |ğ·ğ‘Ÿ |âˆ§ğœâ„ â‰¥ |ğ·ğ‘  | =â‡’ ğ“ is injective =â‡’ |ğ“ [Hğ‘šğ‘ğ‘¥ ]| â‰¥

|ğ·ğ‘Ÿ | Â· |ğ· ğ‘¤ | Â· |ğ·ğ‘  | Â· |ğ·â„ | Â· |ğ·ğ‘ | Â· |ğ·ğ‘ |

(2) ğœğ‘¤

=

=
max(|ğ·ğ‘Ÿ |, |ğ· ğ‘¤ |) Â· max(|ğ·ğ‘  |, |ğ·â„ |) Â· |ğ·ğ‘ | Â· |ğ·ğ‘ |,

=â‡’

1 âˆ§ ğœâ„

1

|ğ“ [Hğ‘šğ‘ğ‘¥ ]|

â‰¥

Our analysis provides a conditional computational intensity: ğœŒğ‘šğ‘–ğ‘› =
âˆš
ğ‘†/2 in case (1) and ğœŒğ‘šğ‘ğ‘¥ = ğ‘†/2 in case (2). Observe that case (2)
yields the maximum non-injective overlap (maximum number of
different iteration vectors map to the same element in ğ¼ğ‘šğ‘ğ‘”ğ‘’). For any
other values of ğœğ‘¤ and ğœâ„, we have ğœŒğ‘šğ‘–ğ‘› â‰¤ ğœŒ â‰¤ ğœŒğ‘šğ‘ğ‘¥ .

6 MULTI-STATEMENT SOAP
I/O lower bounds are not composable: the I/O cost of a program
containing multiple statements may be lower than the sum of the
I/O costs of each statement if evaluated in isolation. Data may be
reused and merging of statements may lower the I/O cost.

Note that the number of vertices in the programâ€™s CDAG ğº
depend on domain sizes ğ·ğ‘– of each iteration variable. However, our
derived upper bound of the computational intensity ğœŒ is independent
of the CDAG size, as it depends only on the access functions ğ“ ğ‘— .
This is also true for programs that contain multiple statements
- to bound ğœŒ for multi-statement SOAP, we only need to model
dependencies between the arrays and how they are accessed - e.g.,
one statement may take as an input an array that is an output of a
different statement.

We represent the data flow between the program statements
with a symbolic directed graph ğºğ‘† = (ğ‘‰ğ‘†, ğ¸ğ‘† ). For a given state-
ment ğ‘†ğ‘¡ğ‘– , denote ğ¼ğ‘›(ğ‘†ğ‘¡ğ‘– ) = {ğ´ğ‘–,1, . . . , ğ´ğ‘–,ğ‘š } a set of input arrays
of statement ğ‘†ğ‘¡ğ‘– . Analogously, denote ğ‘‚ğ‘¢ğ‘¡ (ğ‘†ğ‘¡ğ‘– ) the set containing
the output array of ğ‘†ğ‘¡ğ‘– . Analogously to program CDAG ğº that cap-
tured dependencies between particular array elements, ğºğ‘† models
dependencies between whole arrays (Figure 2).

Definition 5. Symbolic Digraph: SDG Given ğ‘˜-statement SOAP
ğ‘†ğ‘¡1, . . . , ğ‘†ğ‘¡ğ‘˜ , its symbolic digraph (SDG) ğºğ‘† = (ğ‘‰ğ‘†, ğ¸ğ‘† ) is a directed
graph where ğ‘‰ğ‘† = (cid:208)ğ‘˜
ğ‘–=1 (In(ğ‘†ğ‘¡ğ‘– )âˆªOut(ğ‘†ğ‘¡ğ‘– )) and (ğ´ğ‘¢, ğ´ğ‘£) âˆˆ ğ¸ğ‘† â‡â‡’
âˆƒğ‘†ğ‘¡ğ‘– : ğ´ğ‘¢ âˆˆ ğ¼ğ‘›(ğ‘†ğ‘¡ğ‘– ) âˆ§ ğ´ğ‘£ âˆˆ ğ‘‚ğ‘¢ğ‘¡ (ğ‘†ğ‘¡ğ‘– ).

ğºğ‘† is a directed graph, where vertices represent arrays accessed
by a program, and edges represent data dependencies between
them. Two arrays ğ´ğ‘¢ and ğ´ğ‘£ are connected if there is a statement
that accesses ğ´ğ‘¢ and computes ğ´ğ‘£. Each edge is annotated with
the corresponding access function vector of the statement that
generates it.

Example 7. Consider the example in Figure 2. We have two state-
ments ğ‘†ğ‘¡1 and ğ‘†ğ‘¡2, with In(ğ‘†ğ‘¡1) = {ğ´, ğµ}, Out(ğ‘†ğ‘¡1) = {ğ¶}, In(ğ‘†ğ‘¡2) =
{ğ¶, ğ·, ğ¸}, Out(ğ‘†ğ‘¡2) = {ğ¸}. We then construct the SDG ğºğ‘† = (ğ‘‰ğ‘†, ğ¸ğ‘† ),
with ğ‘‰ğ‘† = In(ğ‘†ğ‘¡1) âˆª Out(ğ‘†ğ‘¡1) âˆª In(ğ‘†ğ‘¡2) âˆª Out(ğ‘†ğ‘¡2) = {ğ´, ğµ, ğ¶, ğ·, ğ¸}.
Furthermore, we have edges ğ¸ğ‘† = {(ğ´, ğ¶), (ğµ, ğ¶), (ğ¶, ğ¸), (ğ·, ğ¸), (ğ¸, ğ¸)}.
The edges are annotated with the corresponding access function vectors
ğ“ğ‘†ğ‘¡ 1,1, . . . , ğ“ğ‘†ğ‘¡ 2,3.

Note: While the â€œexplicitâ€ program CDAG ğº = (ğ‘‰ , ğ¸), where
every vertex represents a single computation is indeed acyclic,
the SDG ğºğ‘† = (ğ‘‰ğ‘†, ğ¸ğ‘† ) may contain self-edges when a statement
updates the loaded array ((ğ¸, ğ¸) in the example above). In ğº, one

Tight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

vertex corresponds to one version of a single array element, while
in ğºğ‘† , one vertex encapsulates all versions of all array elements.

6.1 SDG Subgraphs
Denote ğ¼ âŠ‚ ğ‘‰ğ‘† set of input vertices of ğºğ‘† (âˆ€ğ´ âˆˆ ğ¼ : indegree(ğ´) =
0). Let ğ» âŠ‚ ğ‘‰ğ‘† \ ğ¼ be a subset of the vertices of SDG ğºğ‘† = (ğ‘‰ğ‘†, ğ¸ğ‘† ).
The SDG subgraph ğºğ‘† [ğ» ] is a subgraph of ğºğ‘† induced by the
vertex set ğ» . It corresponds to some subcomputation in which
at least one vertex from each array in ğ» was computed. We now
use the analogous strategy to the ğ‘‹ -Partitioning abstraction: since
the optimal pebbling has an associated ğ‘‹ -partition with certain
properties (the dominator set constraint), we bound the cost of
any pebbling by finding the maximum subcomputation among all
valid ğ‘‹ -partitions. We now show that every subcomputation in
the optimal ğ‘‹ -partition has a corresponding SDG subgraph ğºğ‘† [ğ» ].
Therefore, finding ğºğ‘† [ğ»ğ‘œğ‘ğ‘¡ ] that maximizes the computational
intensity among all subgraphs bounds the size of the maximal sub-
computation (which, in turn, bounds the I/O cost of any pebbling).
Recall that an optimal pebbling ğ‘ƒ has an associated ğ‘‹ -partition
P (ğ‘‹ ), where each H âˆˆ P (ğ‘‹ ) represents a sequence of operations
that are not interleaved with other subcomputations. Given ğºğ‘† ,
each H âˆˆ P (ğ‘‹ ) has an associated subgraph ğºğ‘† [ğ» ] s.t. every array
vertex ğ´ğ‘– âˆˆ ğ» represents an array from which at least one vertex
was computed in H .

Note that both the pebbling ğ‘ƒ and the partition P (ğ‘‹ ) depend on
the size of the CDAG that is determined by the sizes of the iteration
domains ğ·ğ‘– . However, the SDG does not depend on them. Thus, by
finding the subgraph that maximizes the computational intensity,
we bound ğœŒ for any combination of input parameters.

Definition 6. The subgraph SOAP statement ğ‘†ğ‘¡ğ» of subgraph
ğºğ‘† [ğ» ] is a single SOAP statement with the input ğ¼ğ‘›(ğ‘†ğ‘¡ğ» ) = {ğ´ :
ğ´ âˆ‰ ğ» âˆ§ âˆƒğµ âˆˆ ğ» : (ğ´, ğµ) âˆˆ ğ¸ğ‘† }. Additionally, for each vertex ğµ âˆˆ ğ»
that is not computed in ğ» , that is (cid:154)ğ´ âˆˆ ğ» : (ğ´, ğµ) âˆˆ ğ¸ğ‘† , self-edges
(ğµ, ğµ) âˆˆ ğ¸ are preserved (ğµ âˆˆ ğ¼ğ‘›(ğ‘†ğ‘¡ğ» )).

Intuition. T he subgraph statement ğ‘†ğ‘¡ğ» is a â€œvirtualâ€ SOAP state-
ment that encapsulates multiple statements ğ‘†ğ‘¡1, . . . , ğ‘†ğ‘¡ğ‘˜ . Given ğ» ,
its subgraph statementâ€™s inputs ğ¼ğ‘›(ğ‘†ğ‘¡ğ» ) are formed by merging
inputs (cid:208)ğ‘˜
ğ¼ğ‘›(ğ‘†ğ‘¡ğ‘– ) \ ğ‘‰ (ğ» ) from all statements that form ğ» , but
are not in ğ» . By the construction of the SDG, this is equivalent to
the definition above: take all vertices ğ´ âˆˆ ğ‘‰ğ‘  \ ğ‘‰ (ğ» ) that have a
child in ğ‘‰ (ğ» ), that is âˆƒğµ âˆˆ ğ‘‰ (ğ» ) : (ğ´, ğµ) âˆˆ ğ¸ğ‘† (see Figure 2).

ğ‘–=1

This forms the lower bound on the number of inputs for a
corresponding subcomputation H : all the vertices from arrays
ğ´ğ‘– âˆˆ ğ‘‰ (ğ» ) could potentially be computed during H and do not
need to be loaded, but at least vertices from arrays ğ¼ğ‘›(ğ‘†ğ‘¡ğ» ) have to
be accessed.

Example 8. Consider again the example from Figure 2. The set
of input nodes is ğ¼ = {ğ´, ğµ, ğ· }. There are three possible subgraph
statements: ğ»1 = {ğ¶}, with ğ¼ğ‘›(ğ‘†ğ‘¡ğ»1 ) = {ğ´, ğµ}, ğ»2 = {ğ¶} with
ğ¼ğ‘›(ğ‘†ğ‘¡ğ»2 ) = {ğ¶, ğ·, ğ¸}, and ğ»3 = {ğ¶, ğ¸} with ğ¼ğ‘›(ğ‘†ğ‘¡ğ»3 ) = {ğ´, ğµ, ğ· }.
Note that by definition, the self-edge (ğ¶, ğ¶) is preserved in ğ»2, but
not in ğ»3. Subgraphs ğ»1 and ğ»2 correspond to the input statements
ğ‘†ğ‘¡1 and ğ‘†ğ‘¡2. Subgraph ğ»3 encapsulates a subcomputation H that
computes some vertices from both arrays ğ¶ and ğ¸, merging subcom-
putations ğ‘†ğ‘¡1 and ğ‘†ğ‘¡2 and reusing outputs from ğ‘†ğ‘¡1 to compute ğ¸.

9

Then, we establish the following lemma:

Lemma 5. Given an ğ‘‹ -partition P (ğ‘‹ ) = {H1, . . . , Hğ‘  } of the ğ‘˜-
statement SOAP, with its corresponding ğºğ‘† = (ğ‘‰ğ‘†, ğ¸ğ‘† ), each subcom-
putation H has an associated intensity ğœŒ H =
|ğ·ğ‘œğ‘šğ‘šğ‘–ğ‘› ( H) |âˆ’ğ‘† that
is upper-bounded by the computational intensity of the subgraph
statement ğ‘†ğ‘¡ğ» (Lemma 4).

|H |

Proof. Recall that given the subcomputation H , its correspond-
ing SDG subgraph ğ» is constructed as follows: for each vertex
ğ‘£ âˆˆ ğ‘‰ computed during H belonging to some array ğ´ğ‘– , add the
corresponding array vertex ğ‘ ğ‘– to ğ» . Note that we allow a vertex
recomputation: if some vertex is (re)computed during the optimal
schedule of H , its array vertex will belong to ğ» .

Observe that by this construction and by definition of the sub-
graph statement, all arrays from which at least one vertex is loaded
during H are in ğ¼ğ‘›(ğ‘†ğ‘¡ğ» ). Furthermore, ğ¼ğ‘›(ğ‘†ğ‘¡ğ» ) is a subset of these
arrays: during H , there might be some loaded vertex from array
ğ´ğ‘— âˆˆ ğ» , but, by definition of ğ‘†ğ‘¡ğ» , this array will not be in ğ¼ğ‘›(ğ‘†ğ‘¡ğ» ).
Therefore, ğ‘†ğ‘¡ğ» lower bounds the input size of H .

The last step of the proof is to observe that by Lemma 4, the
computational intensity of ğ‘†ğ‘¡ğ» bounds the maximum number of
computed vertices for any H â€² âˆˆ P (ğ‘‹ ) that belong to ğ» , that is,
the union of all arrays in ğ» . But since all vertices that are com-
puted in H belong to one of these arrays, H cannot have higher
â–¡
computational intensity.

6.2 SDG I/O Lower Bounds
We now proceed to establish a method to derive the I/O lower
bounds of the multi-statement SOAP given its SDG ğºğ‘† = (ğ‘‰ğ‘†, ğ¸ğ‘† ).
For each array vertex ğ´ âˆˆ ğ‘‰ğ‘† , denote |ğ´| as the total number of
vertices in the CDAG that belong to array ğ´. Denote further S(ğ´)
the set of all subgraphs of ğºğ‘† that contain ğ´. Then we prove the
following theorem:

Theorem 1. The I/O cost ğ‘„ of a ğ‘˜-statement SOAP represented by
the SDG ğºğ‘† = (ğ‘‰ğ‘†, ğ¸ğ‘† ) is bounded by

ğ‘„ â‰¥

âˆ‘ï¸

ğ´âˆˆğ‘‰ğ‘†

|ğ´|
maxğ» âˆˆS (ğ´) ğœŒğ»

(10)

where maxğ» âˆˆS (ğ´) ğœŒğ» is the maximum computational intensity over
all subgraph statements of subgraphs ğ» that contain vertex ğ´.

Proof. This theorem is a direct consequence of Lemma 5 and
the fact that all vertices in CDAG ğº are associated with some array
vertex in SDG ğºğ‘† . Lemma 5, together with the definition of S(ğ‘),
states that maxğ» âˆˆS (ğ‘) ğœŒğ» is the upper bound on any subcompu-
tation H that contains any vertex from array ğ‘. Since there are
|ğ‘| vertices associated with ğ‘, at least
I/O operations
must be performed to compute these vertices. Since the computa-
tional intensity expresses the average cost per vertex, even if some
subcomputation in an optimal ğ‘‹ -partition spans more than one
array, this is already modeled by the set S(ğ‘). Therefore, we can
â–¡
sum the I/O costs per arrays ğ‘, yielding inequality 10.

|ğ‘ |
maxğ» âˆˆS (ğ‘) ğœŒğ»

Note that applying Theorem 1 requires iterating over all possible
subgraphs. In the worst case, this yields exponential complexity,
prohibiting scaling our method to large programs. However, many
scientific applications contain a limited number of kernels with

Technical Report, 2021,

simple dependencies. In practice we observed that our approach
scales well to programs containing up to 35 statements.

7 EVALUATION
We evaluate our lower bound analysis on a wide range of applica-
tions, ranging from fundamental computational kernels and solvers
to full workloads in hydrodynamics, numerical weather prediction,
and deep learning. The set of applications covers both the previously
analyzed kernels (the Polybench suite [28], direct convolution), and
kernels that were never analyzed before due to complicated depen-
dency structures (multiple NN layers, diffusion, advection). Not
only our tool covers broader class of programs than state-of-the-art
approaches, but also it improves bounds generated by methods
dedicated to specific narrower classes [19]. Improving I/O lower
bounds has not only theoretical implications: loose bounds may
not be applicable for generating corresponding parallel codes, as
too many overapproximations may yield an invalid schedule.

In our experiments we use DaCe [29] to extract SOAP statements
from Python and C code, and use MATLAB for symbolic analysis.
Polybench. As our first case study, we analyze Polybench [28],
a polyhedral application benchmark suite composed of 30 pro-
grams from several domains, including linear algebra kernels, linear
solvers, data mining, and computational biology. Prior best results
were obtained by IOLB [19], a tool specifically designed for analyz-
ing I/O lower bounds of affine programs. We summarize the results
in Table 2, listing the leading order term for brevity.

We find that SOAP analysis derives tight I/O lower bounds for all
Polybench kernels. Analyzing these programs as multi-statement
SOAP either reproduces existing tight bounds, or improves them by
constant factors (e.g., in Cholesky decomposition) on 14 out of 30
applications (Table 2). Of particular note is adi (Alternating Direc-
tion Implicit solver). Our algorithm detected a possible tiling in the
time dimension, yielding the lower bound (12ğ‘ 2ğ‘‡ )/
ğ‘†, compared
to ğ‘ 2ğ‘‡ reported by Olivry et al. [19]. However, due to dependency
chains incurred by alternating directions, such tiling may violate
loop-carried dependency constraints, which our algorithm relaxes.
A parallel machine could potentially take advantage of this tiling
scheme, possibly providing super-linear communication reduction.
However, this is outside of the scope of this paper.
Neural Networks. Analyzing I/O lower bounds of neural net-
works is a nascent field, and so far only single-layer convolution
was analyzed [20, 23]. We improve the previously-reported bound
reported by Zhang et al. [20] by a factor of 8.

âˆš

7.1 New Lower Bounds
Analyzing SOAP and the SDG representation enables capturing
complex data dependencies in programs with a large number of
statements. To demonstrate this, we study larger programs in three
fields, where no previous I/O bounds are known. If an application
contains both SOAP and data-dependent kernels, we find a SOAP
representation that bounds the access sizes from below.
LULESH. The Livermore Unstructured Lagrangian Explicit Shock
Hydrodynamics (LULESH) [30] application is an unstructured phy-
sics simulation. We analyze the main computational kernel, totaling
over 60% of runtime within one time-step of the simulation from

10

G. Kwasniewski et al.

Improv.
over SotA

12âˆš
ğ‘†

SOAP I/O Bound
12ğ‘ 2ğ‘‡
âˆš
ğ‘†
ğ‘€ğ‘
ğ‘€ğ‘
ğ‘ 3
âˆš
ğ‘†
3
ğ‘€ 2ğ‘
âˆš
ğ‘†
ğ‘€ 2ğ‘
âˆš
ğ‘†
3ğ»ğ‘Š
2ğ‘ 2

ğ‘ƒ ğ‘ğ‘„ ğ‘ğ‘…
âˆš
ğ‘†

3ğ‘ğ‘‹ ğ‘ğ‘Œ ğ‘‡
âˆš
ğ‘†

3ğ‘ 2
2
âˆš

2

2ğ‘ 3
âˆš
ğ‘†
2ğ‘ 2
âˆš
ğ‘†
ğ‘ 2
2ğ‘ 2
ğ‘€ğ‘ 2
âˆš
ğ‘†
6ğ‘ 3ğ‘‡
3âˆš
ğ‘†
2ğ‘ğ‘‡
ğ‘†
4ğ‘ 2ğ‘‡
âˆš
ğ‘†
4ğ‘ 3
âˆš
ğ‘†
6ğ‘ 3
âˆš
ğ‘†
2ğ‘ 3
âˆš
ğ‘†
3
2ğ‘ 3
âˆš
ğ‘†
3
ğ‘ 2
ğ‘ 3
âˆš
ğ‘†
3
4ğ‘ 2ğ‘‡
âˆš
ğ‘†
2ğ‘€ 2ğ‘
âˆš
ğ‘†
2ğ‘€ğ‘ 2
âˆš
ğ‘†
ğ‘€ğ‘ 2
âˆš
ğ‘†
ğ‘ 2
2
ğ‘€ 2ğ‘
âˆš
ğ‘†

]
9
1
[
h
c
n
e
b
y
l
o
P

Kernel

adi

atax
bicg
cholesky

correlation

covariance

deriche

doitgen

durbin

fdtd2d

floyd-warshall

gemm

gemver
gesummv
gramschmidt

heat3d

jacobi1d
jacobi2d

2mm

3mm

lu

ludcmp

mvt
nussinov

seidel2d

symm

syr2k

syrk

trisolv
trmm

s Direct conv.
k
r
o
w
t
e
N

Softmax
MLP

l
a
r
u
e
N

LeNet-5

BERT Encoder

2ğ¶ğ‘–ğ‘›ğ¶ğ‘œğ‘¢ğ‘¡ ğ»ğ‘œğ‘¢ğ‘¡ ğ‘ğ‘Šğ‘œğ‘¢ğ‘¡ğ‘Šğ‘˜ğ‘’ğ‘Ÿ ğ»ğ‘˜ğ‘’ğ‘Ÿ
âˆš
ğ‘†

4ğµğ»ğ‘€ğ‘
2ğ‘ (ğ‘“ ğ‘1 ğ‘“ ğ‘2+ğ‘“ ğ‘1ğ‘–ğ‘›ğ‘+ğ‘“ ğ‘2ğ‘œğ‘¢ğ‘¡ )
âˆš
ğ‘†

âˆš

300

2ğ¶ğ» ğ‘ğ‘Š
âˆš
ğ‘†
4 ğµ ğ» ğ‘ƒ ğ¿ (ğ¿+2 ğ» ğ‘ƒ )
âˆš
ğ‘†

1
1
2

2

2

3

1

3

6

2

1

1
1
1

âˆš

6

32
3 3âˆš
3
8
âˆš
6

3

âˆš

3

1

1

1

1

1
2

6

1

2

2

1
1

8

â€”
â€”

â€”

â€”

â€”
â€”
â€”

s LULESH
u
o
i
r
a
V

horizontal diff.
vertical adv.

22 Â· numElem
2ğ¼ ğ½ ğ¾
5ğ¼ ğ½ ğ¾

Table 2: Simplified leading-order terms of the I/O lower bounds ex-
tracted from multi-statement SOAP and previous state-of-the-art.
For the direct convolution layer, the best previously known bound
was published by Zhang et al. [20].

Tight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

the full C++ source code. As LULESH falls outside the purview of
affine programs, this result is the first reported I/O lower bound.
Numerical Weather Prediction. We select two benchmark sten-
cil applications from the COSMO Weather Model [31] â€” horizontal
diffusion and vertical advection â€” representatives of the two major
workload types in the modelâ€™s dynamical core.
Deep Neural Networks. For deep learning, we choose both in-
dividual representative operators (Convolution and Softmax) and
network-scale benchmarks. Previous approaches only study data
movement empirically [32]. To the best of our knowledge, we are
the first to obtain I/O lower bounds for full networks, including a
Multi-Layer Perceptron (MLP), the LeNet-5 CNN [33], and a BERT
Transformer encoder [34].

8 RELATED WORK
I/O analysis spans almost the entire history of general-purpose com-
puter architectures, and graph pebbling abstractions were among
the first methods to model memory requirements. Dating back to
challenges with the register allocation problem [35], pebbles were
also used to prove space-time tradeoffs [36] and maximum par-
allel speedups by investigating circuit depths [37]. Arguably the
most influential pebbling abstraction work is the red-blue pebble
game by Hong and Kung [8] that explicitly models load and store
operations in a two-level-deep memory hierarchy. This work was
extended numerous times, by: adding blocked access [38], multiple
memory hierarchies [25], or introducing additional pebbles to allow
CDAG compositions [18]. Demaine and Liu proved that finding the
optimal pebbling in a standard and no-deletion red-blue pebble
game is PSPACE-complete [17]. Papp and Wattenhofer introduced
a game variant with a non-zero computation cost and investigated
pebbling approximation algorithms [39].

Although the importance of data movement minimization is be-
yond doubt, the general solution for arbitrary algorithms is still an
open problem. Therefore, many works were dedicated to investigate
lower bounds only for single algorithms (often with accompanying
implementations), like matrix-matrix multiplication [16, 40â€“42],
LU [41] and Cholesky decompositions [43, 44]. Ballard et al. [45]
present an extensive collection of linear algebra algorithms. More-
over, a large body of work exists for minimizing communication in
irregular algorithms [46, 47], such as Betweenness Centrality [5],
min cuts [48], BFS [49], matchings [50], vertex similarity coeffi-
cients [51], or general graph computations [52, 53, 53]. Many of
them use linear algebra based formulations [54]. Recently, convolu-
tion networks gained high attention. The first asymptotic I/O lower
bound for single-layer direct convolution was proved by Demmel
et al. [23]. Chen et al. [55] propose a matching implementation, and
Zhang et al. [20] present the first non-asymptotic I/O lower bound
for Winograd convolution.

In parallel with the development of I/O minimizing implemen-
tations for particular algorithms, several works investigated I/O
lower bounds for whole classes of programs. Christ et al. [7] use a
discrete version of Loomis-Whitney inequality to derive asymptotic
lower bounds for single-statement programs nested in affine loops.
Demmel and Rusciano [22] extended this work and use discrete
HÃ¶lder-Brascamp-Lieb inequalities to find optimal tilings for such
programs. The polyhedral model [56] is widely used in practice
by many compilers [57, 58]. However, polyhedral methods have

their own limitations: 1) they cannot capture non-affine loops [59];
2) while the representation of a program is polynomial, finding
optimal transformations is still NP-hard [60]; 3) they are inappli-
cable for many neural network architectures, e.g., the Winograd
algorithm for convolution [20].

Recently, Olivry et al. [19] presented IOLB â€” a tool for automatic
derivation of non-parametric I/O lower bounds for programs that
can be modeled by the polyhedral framework. IOLB employs both
â€œgeometricâ€ projection-based bounds based on the HBL inequal-
ity [21], as well as the wavefront-based approach from Elango [61].
To the best of our knowledge, this is the only method that can
handle multiple-statement programs. However, the IOLB model
explicitly disallows recomputation that may be used to decrease the
I/O cost, e.g., in the Winograd convolution algorithm, backpropaga-
tion, or vertical advection. Furthermore, the framework is strictly
limited to affine access programs. Even then, our method is able to
improve those bounds by up to a factor of 6
6 (fdtd2d) using a
single, general method without the need to use application-specific
techniques, such as wavefront-based reasoning.

âˆš

9 CONCLUSIONS
In this work we introduce SOAP â€” a broad class of statically ana-
lyzable programs. Using the explicit assumptions on the allowed
overlap between arrays, we are able to precisely count the number
of accessed vertices on the induced parametric CDAG. This stands
in contrast with many state-of-the art approaches that are based on
bounding projection sizes, as they need to underapproximate their
union size, often resulting in a significant slack in constant factors
of their bounds. Our single method is able to reproduce or improve
existing lower bounds for many important scientific kernels from
various domains, ranging from 2Ã— increase in the lower bound
for linear algebra (cholesky, syrk), to more than 10Ã— for stencil
applications (fdtd2d, heat3d).

Our SDG abstraction precisely models data dependencies in
multiple-statement programs. It directly captures input and output
reuse, and allows data recomputation. Armed with these tools, we
are the first to establish I/O lower bounds for entire neural networks,
as well as core components of the popular Transformer architecture.
We believe that our work will be further extended to handle data-
dependent accesses (e.g., sparse matrices), as well as scale better
with input program size. The derived maximum subcomputation
sizes can guide compiler optimizations and development of new
communication-optimal algorithms through tiling, parallelization,
or loop fusion transformations.

10 ACKNOWLEDGEMENTS
This project received funding from the European Research Council
(ERC) under the European Unionâ€™s Horizon 2020 programme (grant
agreement DAPP, No. 678880). Tal Ben-Nun is supported by the
Swiss National Science Foundation (Ambizione Project #185778).
The authors wish to thank the Swiss National Supercomputing
Center (CSCS) for providing computing infrastructure and support.

REFERENCES
[1] D. Unat, A. Dubey, T. Hoefler, J. Shalf, M. Abraham, M. Bianco, B. L. Chamberlain,
R. Cledat, H. C. Edwards, H. Finkel, K. Fuerlinger, F. Hannig, E. Jeannot, A. Kamil,
J. Keasler, P. H. J. Kelly, V. Leung, H. Ltaief, N. Maruyama, C. J. Newburn, ,

11

Technical Report, 2021,

G. Kwasniewski et al.

and M. Pericas, â€œTrends in Data Locality Abstractions for HPC Systems,â€ IEEE
Transactions on Parallel and Distributed Systems (TPDS), vol. 28, no. 10, Oct. 2017.
[2] G. Kestor, R. Gioiosa, D. J. Kerbyson, and A. Hoisie, â€œQuantifying the energy cost
of data movement in scientific applications,â€ in 2013 IEEE international symposium
on workload characterization (IISWC).

IEEE, 2013, pp. 56â€“65.

[3] A. Tate, A. Kamil, A. Dubey, A. GrÃ¶ÃŸlinger, B. Chamberlain, B. Goglin, C. Edwards,
C. J. Newburn, D. Padua, D. Unat et al., â€œProgramming abstractions for data
locality.â€ PADAL Workshop 2014.

[4] D. Unat, A. Dubey, T. Hoefler, J. Shalf, M. Abraham, M. Bianco, B. L. Chamber-
lain, R. Cledat, H. C. Edwards, H. Finkel, K. Fuerlinger, F. Hannig, E. Jeannot,
A. Kamil, J. Keasler, P. H. J. Kelly, V. Leung, H. Ltaief, N. Maruyama, C. J. New-
burn, and M. PericÃ¡s, â€œTrends in data locality abstractions for hpc systems,â€ IEEE
Transactions on Parallel and Distributed Systems, pp. 3007â€“3020, 2017.

[5] E. Solomonik, M. Besta, F. Vella, and T. Hoefler, â€œScaling Betweenness Centrality
using Communication-Efficient Sparse Matrix Multiplication,â€ in SC, 2017.
[6] E. Solomonik, E. Carson, N. Knight, and J. Demmel, â€œTrade-offs between syn-
chronization, communication, and computation in parallel linear algebra compu-
tations,â€ ACM Transactions on Parallel Computing (TOPC), vol. 3, no. 1, pp. 1â€“47,
2017.

[7] M. Christ, J. Demmel, N. Knight, T. Scanlon, and K. Yelick, â€œCommunication lower
bounds and optimal algorithms for programs that reference arraysâ€“part 1,â€ arXiv
preprint arXiv:1308.0068, 2013.

[8] J. Hong and H. Kung, â€œI/O complexity: The red-blue pebble game,â€ in STOC, 1981,

pp. 326â€“333.

[9] M. Del Ben et al., â€œEnabling simulation at the fifth rung of DFT: Large scale RPA
calculations with excellent time to solution,â€ Comp. Phys. Comm., pp. 120â€“129,
2015.

[10] Q. Zheng and J. D. Lafferty, â€œConvergence analysis for rectangular matrix com-

pletion using burer-monteiro factorization and gradient descent,â€ CoRR, 2016.

[11] T. Ben-Nun and T. Hoefler, â€œDemystifying parallel and distributed deep learning:
An in-depth concurrency analysis,â€ ACM Comput. Surv., vol. 52, no. 4, 2019.

[12] C. D. Meyer, Matrix analysis and applied linear algebra.
[13] A. Krishnamoorthy and D. Menon, â€œMatrix inversion using Cholesky decom-
position,â€ in 2013 signal processing: Algorithms, architectures, arrangements, and
applications (SPA).

IEEE, 2013, pp. 70â€“72.

SIAM, 2000.

[14] E. Solomonik, D. Matthews, J. R. Hammond, J. F. Stanton, and J. Demmel, â€œA mas-
sively parallel tensor contraction framework for coupled-cluster computations,â€
Journal of Parallel and Distributed Computing, vol. 74, no. 12, pp. 3176â€“3190, 2014.
[15] V. Elango, F. Rastello, L.-N. Pouchet, J. Ramanujam, and P. Sadayappan, â€œOn
characterizing the data access complexity of programs,â€ in Proceedings of the
42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, ser. POPL â€™15. New York, NY, USA: ACM, 2015.

[16] G. Kwasniewski, M. KabiÄ‡, M. Besta, J. VandeVondele, R. SolcÃ , and T. Hoefler,
â€œRed-Blue Pebbling Revisited: Near Optimal Parallel Matrix-Matrix Multiplication,â€
in Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis (SC19), Nov. 2019.

[17] E. D. Demaine and Q. C. Liu, â€œRed-blue pebble game: Complexity of computing
the trade-off between cache size and memory transfers,â€ in Proceedings of the 30th
on Symposium on Parallelism in Algorithms and Architectures, 2018, pp. 195â€“204.
[18] V. Elango et al., â€œData access complexity: The red/blue pebble game revisited,â€

Tech. Rep., 2013.

[19] A. Olivry, J. Langou, L.-N. Pouchet, P. Sadayappan, and F. Rastello, â€œAutomated
derivation of parametric data movement lower bounds for affine programs,â€ in
Proceedings of the 41st ACM SIGPLAN Conference on Programming Language
Design and Implementation, 2020, pp. 808â€“822.

[20] X. Zhang, J. Xiao, and G. Tan, â€œI/O lower bounds for auto-tuning of convolutions

in CNNs,â€ 2020.

[21] M. Christ, J. Demmel, N. Knight, T. Scanlon, and K. Yelick, â€œCommunication lower
bounds and optimal algorithms for programs that reference arraysâ€“part 1,â€ arXiv
preprint arXiv:1308.0068, 2013.

[22] J. Demmel and A. Rusciano, â€œParallelepipeds obtaining HBL lower bounds,â€ arXiv

preprint arXiv:1611.05944, 2016.

[23] J. Demmel and G. Dinh, â€œCommunication-optimal convolutional neural nets,â€

arXiv preprint arXiv:1802.06905, 2018.

[24] G. Dinh and J. Demmel, â€œCommunication-optimal tilings for projective nested

loops with arbitrary bounds,â€ arXiv preprint arXiv:2003.00119, 2020.

[25] J. E. Savage, â€œExtending the hong-kung model to memory hierarchies,â€ in Inter-
Springer, 1995, pp. 270â€“281.
[26] Q. Liu, â€œRed-blue and standard pebble games : Complexity and applications in

national Computing and Combinatorics Conference.

the sequential and parallel models,â€ 2018.

[27] G. Kwasniewski, T. Ben-Nun, A. N. Ziogas, T. Schneider, M. Besta, and T. Hoe-
fler, â€œOn the parallel I/O optimality of linear algebra kernels: Near-optimal LU
factorization,â€ 2020.

[28] L. N. Pouchet, â€œPolyBench: The Polyhedral Benchmark suite,â€ 2016. [Online].

Available: https://sourceforge.net/projects/polybench

[29] T. Ben-Nun, J. de Fine Licht, A. N. Ziogas, T. Schneider, and T. Hoefler, â€œStateful
dataflow multigraphs: A data-centric model for performance portability on het-
erogeneous architectures,â€ in Proceedings of the International Conference for High

12

Performance Computing, Networking, Storage and Analysis, ser. SC â€™19, 2019.
[30] J. Keasler and USDOE, â€œLivermore unstructured lagrange explicit shock
hydrodynamics,â€ 9 2010. [Online]. Available: https://www.osti.gov//servlets/purl/
1231396

[31] M. Baldauf, A. Seifert, J. FÃ¶rstner, D. Majewski, and M. Raschendorfer, â€œOpera-
tional convective-scale numerical weather prediction with the COSMO model:
Description and sensitivities.â€ Monthly Weather Review, 139:3387â€“3905, 2011.
[32] A. Ivanov, N. Dryden, T. Ben-Nun, S. Li, and T. Hoefler, â€œData movement is all

you need: A case study on optimizing transformers,â€ 2020.

[33] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, â€œGradient-based learning applied
to document recognition,â€ in Proceedings of the IEEE, 1998, pp. 2278â€“2324.
[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,

and I. Polosukhin, â€œAttention is all you need,â€ 2017.

[35] R. Sethi, â€œComplete register allocation problems,â€ in STOC, 1973.
[36] W. J. Paul and R. E. Tarjan, â€œTime-space trade-offs in a pebble game,â€ Acta Infor-

matica, vol. 10, no. 2, pp. 111â€“115, Jun 1978.

[37] P. W. Dymond and M. Tompa, â€œSpeedups of deterministic machines by synchro-
nous parallel machines,â€ Journal of Computer and System Sciences, vol. 30, no. 2,
pp. 149â€“161, 1985.

[38] A. Aggarwal and S. Vitter, Jeffrey, â€œThe input/output complexity of sorting and

related problems,â€ CACM, Sep. 1988.

[39] P. A. Papp and R. Wattenhofer, â€œOn the hardness of red-blue pebble games,â€
in Proceedings of the 32nd ACM Symposium on Parallelism in Algorithms and
Architectures, 2020, pp. 419â€“429.

[40] D. Irony, S. Toledo, and A. Tiskin, â€œCommunication lower bounds for distributed-
memory matrix multiplication,â€ Journal of Parallel and Distributed Computing,
vol. 64, no. 9, pp. 1017 â€“ 1026, 2004.

[41] E. Solomonik and J. Demmel, â€œCommunication-optimal parallel 2.5D matrix
multiplication and LU factorization algorithms,â€ in Euro-Par 2011 Parallel
Processing, ser. Lecture Notes in Computer Science, E. Jeannot, R. Namyst, and
J. Roman, Eds. Springer Berlin Heidelberg, 2011, vol. 6853, pp. 90â€“109. [Online].
Available: http://dx.doi.org/10.1007/978-3-642-23397-5_10

[42] J. Demmel et al., â€œCommunication-optimal parallel recursive rectangular matrix

multiplication,â€ in IPDPS, 2013, pp. 261â€“272.

[43] G. Ballard, J. Demmel, O. Holtz, and O. Schwartz, â€œCommunication-optimal
parallel and sequential Cholesky decomposition,â€ SIAM Journal on Scientific
Computing, vol. 32, no. 6, pp. 3495â€“3523, 2010.

[44] E. Hutter and E. Solomonik, â€œCommunication-avoiding Cholesky-QR2 for rect-
angular matrices,â€ in 2019 IEEE International Parallel and Distributed Processing
Symposium (IPDPS).

IEEE, 2019, pp. 89â€“100.

[45] G. Ballard, E. Carson, J. Demmel, M. Hoemmen, N. Knight, and O. Schwartz,
â€œCommunication lower bounds and optimal algorithms for numerical linear
algebra,â€ Acta Numerica, vol. 23, p. 1, 2014.

[46] M. Besta and T. Hoefler, â€œAccelerating irregular computations with hardware
transactional memory and active messages,â€ in Proceedings of the 24th Interna-
tional Symposium on High-Performance Parallel and Distributed Computing, 2015,
pp. 161â€“172.

[47] S. Sakr, A. Bonifati, H. Voigt, A. Iosup, K. Ammar, R. Angles, W. Aref, M. Arenas,
M. Besta, P. A. Boncz et al., â€œThe future is big graphs! a community view on
graph processing systems,â€ arXiv preprint arXiv:2012.06171, 2020.

[48] L. Gianinazzi, P. Kalvoda, A. De Palma, M. Besta, and T. Hoefler, â€œCommunication-
avoiding parallel minimum cuts and connected components,â€ ACM SIGPLAN
Notices, vol. 53, no. 1, pp. 219â€“232, 2018.

[49] M. Besta, F. Marending, E. Solomonik, and T. Hoefler, â€œSlimsell: A vectorizable

graph representation for breadth-first search,â€ in IPDPS, 2017.

[50] M. Besta, M. Fischer, T. Ben-Nun, D. Stanojevic, J. D. F. Licht, and T. Hoefler,
â€œSubstream-centric maximum matchings on fpga,â€ ACM Transactions on Reconfig-
urable Technology and Systems (TRETS), vol. 13, no. 2, pp. 1â€“33, 2020.

[51] M. Besta, R. Kanakagiri, H. Mustafa, M. Karasikov, G. RÃ¤tsch, T. Hoefler, and
E. Solomonik, â€œCommunication-efficient jaccard similarity for high-performance
distributed genome comparisons,â€ in 2020 IEEE International Parallel and Dis-
tributed Processing Symposium (IPDPS).

IEEE, 2020, pp. 1122â€“1132.

[52] M. Besta, M. Podstawski, L. Groner, E. Solomonik, and T. Hoefler, â€œTo push or to
pull: On reducing communication and synchronization in graph computations,â€
in Proceedings of the 26th International Symposium on High-Performance Parallel
and Distributed Computing, 2017, pp. 93â€“104.

[53] M. Besta, Z. Vonarburg-Shmaria, Y. Schaffner, L. Schwarz, G. Kwasniewski, L. Gi-
aninazzi, J. Beranek, K. Janda, T. Holenstein, S. Leisinger et al., â€œGraphminesuite:
Enabling high-performance and programmable graph mining algorithms with
set algebra,â€ arXiv preprint arXiv:2103.03653, 2021.

[54] J. Kepner et al., â€œMathematical foundations of the GraphBLAS,â€ arXiv:1606.05790,

2016.

[55] X. Chen, Y. Han, and Y. Wang, â€œCommunication lower bound in convolution ac-
celerators,â€ in 2020 IEEE International Symposium on High Performance Computer
Architecture (HPCA).

IEEE, 2020, pp. 529â€“541.

[56] U. Bondhugula, M. Baskaran, S. Krishnamoorthy, J. Ramanujam, A. Rountev,
and P. Sadayappan, Automatic Transformations for Communication-Minimized
Berlin,
Parallelization and Locality Optimization in the Polyhedral Model.

Tight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

Heidelberg: Springer Berlin Heidelberg, 2008, pp. 132â€“146. [Online]. Available:
https://doi.org/10.1007/978-3-540-78791-4_9

[57] â€œAutomatic transformations for communication-minimized parallelization and
locality optimization in the polyhedral model,â€ in International Conference on
Compiler Construction (ETAPS CC), Apr. 2008.

[58] T. Grosser, A. Groesslinger, and C. Lengauer, â€œPollyâ€”performing polyhedral
optimizations on a low-level intermediate representation,â€ Parallel Processing
Letters, vol. 22, no. 04, p. 1250010, 2012.

[59] T. Hoefler and G. Kwasniewski, â€œAutomatic complexity analysis of explicitly
parallel programs,â€ in Proceedings of the 26th ACM symposium on Parallelism in
algorithms and architectures, 2014, pp. 226â€“235.

[60] A. Darte, â€œOn the complexity of loop fusion,â€ in PACT, 1999, pp. 149â€“157.
[61] V. Elango, F. Rastello, L.-N. Pouchet, J. Ramanujam, and P. Sadayappan, â€œOn
characterizing the data movement complexity of computational DAGs for parallel
execution,â€ in Proceedings of the 26th ACM Symposium on Parallelism in Algorithms
and Architectures, 2014, pp. 296â€“306.

13

