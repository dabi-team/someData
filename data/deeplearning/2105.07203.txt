1
2
0
2

y
a
M
5
1

]

C
C
.
s
c
[

1
v
3
0
2
7
0
.
5
0
1
2
:
v
i
X
r
a

Pebbles, Graphs, and a Pinch of Combinatorics: Towards Tight
I/O Lower Bounds for Statically Analyzable Programs

Grzegorz Kwasniewski, Tal Ben-Nun, Lukas Gianinazzi, Alexandru Calotoiu, Timo Schneider,
Alexandros Nikolaos Ziogas, Maciej Besta, Torsten Hoefler
ETH Zurich, Switzerland

ABSTRACT
Determining I/O lower bounds is a crucial step in obtaining com-
munication-efficient parallel algorithms, both across the memory
hierarchy and between processors. Current approaches either study
specific algorithms individually, disallow programmatic motifs such
as recomputation, or produce asymptotic bounds that exclude im-
portant constants. We propose a novel approach for obtaining pre-
cise I/O lower bounds on a general class of programs, which we call
Simple Overlap Access Programs (SOAP). SOAP analysis covers a
wide variety of algorithms, from ubiquitous computational kernels
to full scientific computing applications. Using the red-blue pebble
game and combinatorial methods, we are able to bound the I/O of
the SOAP-induced Computational Directed Acyclic Graph (CDAG),
taking into account multiple statements, input/output reuse, and
optimal tiling. To deal with programs that are outside of our repre-
sentation (e.g., non-injective access functions), we describe methods
to approximate them with SOAP. To demonstrate our method, we
analyze 38 different applications, including kernels from the Poly-
bench benchmark suite, deep learning operators, and — for the first
time — applications in unstructured physics simulations, numeri-
cal weather prediction stencil compositions, and full deep neural
networks. We derive tight I/O bounds for several linear algebra
kernels, such as Cholesky decomposition, improving the existing
reported bounds by a factor of two. For stencil applications, we
improve the existing bounds by a factor of up to 14. We implement
our method as an open-source tool, which can derive lower bounds
directly from provided C code.

CCS CONCEPTS
• Theory of computation → Communication complexity; Par-
allel computing models; Scheduling algorithms.

KEYWORDS
I/O complexity, red-blue pebble game, parallel scheduling model

1 INTRODUCTION
I/O operations, both across the memory hierarchy and between par-
allel processors, dominate time and energy costs in many scientific
applications [1–4]. It is thus of key importance to design algorithms
with communication-avoiding or I/O-efficient schedules [5, 6]. To in-
form, and occasionally inspire the development of such algorithms,
one must first understand the associated lower bounds on the amounts
of communicated data. Deriving these bounds has always been of
theoretical interest [7, 8]. It is particularly relevant for dense linear
algebra, as many important problems in scientific computing [9, 10]
and machine learning [11] rely on linear algebra operations such
as matrix factorization [12, 13] or tensor contractions [14].

1

Analyzing I/O bounds of linear algebra kernels dates back to
the seminal work by Hong and Kung [8], who derived the first as-
ymptotic bound for matrix-matrix multiplication (MMM) using the
red-blue pebble game abstraction. This method was subsequently
extended and used by other works to derive asymptotic [15] and
tight [16] bounds for more complex programs. Despite the expres-
siveness of pebbling, it is prohibitively hard to solve for arbitrary
programs, as it is PSPACE-complete in the general case [17].

Since analyzing programs with parametric sizes disallows the
construction of an explicit Computation Directed Acyclic Graph
(CDAG), some form of parameterization is often needed [18–20].
However, we argue that the widely-used approaches based on the
Loomis-Whitney or the HBL inequalities [21–23] (a) are often too
restrictive, requiring the programs to be expressed in the polyhe-
dral model to count the points in the projection polytopes; (b) do
not capture pebbling motifs such as recomputation [19]; or (c) are
limited to single-statement programs [7, 21–23, 23, 24].

In our work, we take a different approach based on a combi-
natorial method. We directly map each elementary computation
to a vertex in a parametric CDAG, which allows us not only to
operate on unstructured iteration domains, but also to precisely
count the sizes of dominator sets and model vertex recomputation.
Furthermore, to handle complex data dependencies in programs
that evaluate multiple arrays, we introduce the Symbolic Directed
Graph (SDG) abstraction, which encapsulates the data flow between
elementary computations. This allows us to cover a wider class of
programs and handle more complex data flow.

To enable precisely mapping every data access to the parametric
CDAG vertex, we introduce a class of Simple Overlap Access Pro-
grams (SOAP), and present a general method to derive precise I/O
bounds of programs in this class. Specifically, SOAPs are defined
as loop nests of statements, whose data access sets can be mod-
eled as injective functions, and their per-statement data overlap
can be expressed with constant offsets. For programs that do not
directly adhere to SOAP, with nontrivial overlaps and non-injective
access functions, we show that under a set of assumptions, we can
construct SOAP “projections” of those programs, which can be ana-
lyzed in the same way. Our method strictly contains the polyhedral
model and associated analysis methods.

To show the breadth of our approach, we demonstrate SOAP
analysis on a set of 38 applications, taking Python and C codes as in-
put to create the SDG. This automated analysis procedure generates
symbolic bounds, which match or improve upon previously-known
results. Notably, we tighten the known I/O lower bounds for nu-
merous programs, including stencils by up to a factor of 14, linear
algebra kernels (e.g., Cholesky factorization by a factor of two), and
the core convolution operation in deep learning by a factor of 8.

 
 
 
 
 
 
Technical Report, 2021,

G. Kwasniewski et al.

Figure 1: High level overview of the combinatorial SOAP analysis. An input program’s schedule is modeled as the red-blue pebble game. The 𝑋 -Partitioning
abstraction relaxes the pebbling problem to the graph partition problem. The SOAP abstraction utilizes the static loop structure to upper-bound the size of
the optimal 𝑋 -partition. The Symbolic Directed Graph (SDG) models inter-statement data dependencies. Our method derives I/O lower bounds together with
accompanying tile sizes and loop fusions that can be used by a compiler to generate an I/O optimal parallel code.

Since our derivation of the bounds is constructive — i.e., it pro-
vides loop tilings and fusions after relaxing loop-carried depen-
dencies — the results can be used by a compiler to generate I/O
optimal parallel codes. This can both improve existing schedules
and possibly reveal new parallelization dimensions.
The paper makes the following contributions:

• A combinatorial method for precisely counting the number of

data accesses in parametric CDAGs.

• A class of programs — SOAP — on which I/O lower bounds can

be automatically derived.

• Symbolic dataflow analysis that extends SOAP to multiple-
statement programs, capturing input and output reuse between
statements, as well as data recomputation.

• I/O analysis of 38 scientific computing kernels, improving ex-
isting bounds [19, 20] by up to a factor of 14, and new lower
bounds for applications in deep learning, unstructured physics
simulation, and numerical weather prediction.

2 BACKGROUND
We first present several fundamental concepts used throughout the
paper. We introduce program, memory, and execution models that
are based on the work by Hong and Kung [8]. We then present a
general approach for deriving I/O lower bounds based on graph
partitioning abstractions. The bird’s eye view of our method is
presented in Figure 1.

2.1 General Approach of Modeling I/O Costs
Program model: CDAG. One of the most expressive ways to
model executions of arbitrary programs is a Computation Directed
Acyclic Graph (CDAG) [8, 16, 18, 20, 25] 𝐺 = (𝑉 , 𝐸), where vertices
represent data (either inputs or results of computations) and edges
represent data dependencies. That is, for 𝑢, 𝑣 ∈ 𝑉 , a directed edge
(𝑢, 𝑣) ∈ 𝐸 signifies that 𝑢 is required to compute 𝑣. Given vertex 𝑣,
vertices {𝑢 : (𝑢, 𝑣) ∈ 𝐸} are referred to as parents of 𝑣. Analogously,
{𝑢 : (𝑣, 𝑢) ∈ 𝐸} are children of 𝑣. Vertices with in-degree (out-
degree) zero are denoted program inputs (program outputs).
Memory model: red-blue pebble game [8]. Programs are exe-
cuted on a sequential machine equipped with a two-level memory
system, which consists of a fast memory of limited size and unlim-
ited slow memory. The contents of the fast memory are represented
by 𝑆 red pebbles. A red pebble placed on a vertex indicates that

the data associated with this vertex resides in the fast memory.
Analogously, data residing in the slow memory is represented with
blue pebbles (of unlimited number).
Execution model: graph pebbling. An execution of a program
represented by a CDAG 𝐺 = (𝑉 , 𝐸) is modeled as a sequence of four
allowed pebbling moves: 1) placing a red pebble on a vertex which
has a blue pebble (load), 2) placing a blue pebble on a vertex which
has a red pebble (store), 3) placing a red pebble on a vertex whose
parents have red pebbles (compute) 4) removing any pebble from a
vertex (discard). At the program start, all input vertices have blue
pebbles placed on them. Execution finishes when all output vertices
have blue pebbles on them. A sequence of moves leading from the
start to the end is called a graph pebbling 𝑃. The number of load
and store moves in 𝑃 is called the I/O cost of 𝑃. The I/O cost 𝑄 of
a program 𝐺 is the minimum cost among all valid pebbling
configurations. A pebbling with cost 𝑄 is called optimal.

2.2 I/O Lower Bounds
Assume that the optimal pebbling 𝑃𝑜𝑝𝑡 is given. For any constant
𝑋 > 𝑆 we can partition this sequence of moves into subsequences,
such that in each subsequence except of the last one, exactly 𝑋 − 𝑆
load/store moves are performed (the last subsequence contains at
most 𝑋 − 𝑆 load/store moves). Denote the number of these subse-
quences as ℎ. Then observe that (𝑋 − 𝑆)(ℎ − 1) ≤ 𝑄 ≤ (𝑋 − 𝑆)ℎ.
Graph pebbling vs graph partitioning. Since finding 𝑃𝑜𝑝𝑡 is
PSPACE complete [26], we seek to derive a lower bound of 𝑄 from
the structure of 𝐺. Observe that the set of vertices which are com-
puted in each subsequence defines a subgraph H ⊆ 𝐺. By this
construction, computing vertices in H requires 𝑋 − 𝑆 load/store
operations in the optimal schedule. The number of subsequences ℎ
may be bounded by a particular partitioning of 𝐺. To do this, we
need to introduce two vertex sets defined for any subgraph of 𝐺.
Dominator and minimum sets [8]. Given H ⊆ 𝐺, a domina-
tor set Dom (H ) is a set of vertices such that every path from
an input to any vertex in H must contain at least one vertex in
Dom (H ). The minimum set Min (H ) is a set of all vertices in H
that do not have any child in H . To avoid the ambiguity of non-
uniqueness of dominator set size, we denote a minimum dominator
set Dom𝑚𝑖𝑛 (H ) to be a dominator set with the smallest size.
𝑋 -Partitioning: bounding I/O cost. Introduced by Kwasniewski
et al. [16], 𝑋 -Partitioning generalizes the S-partitioning from Hong

2

Input programCDAG pebblingforiinrange(100):forjinrange(100):C[i,j]=((A[i]+A[i+1])*(B[j]+B[j+1]))foriinrange(100):forjinrange(100):forkinrange(100):E[i,j]+=C[i,k]*D[k,j]Ignoringcompute costX-partitioningSymbolic Directed GraphABDCESOAPIgnoringloop carrieddepend.Minimal I/O cost → opt. scheduleFeatures:Beyond polyhedral model (non-affine accesses); recomputation; dependency struct (SDG)Improved lower bounds: Linear algebra (Cholesky, correlation, covariance); stencils (fdtd, jacobi, heat3d)New lower bounds:Neural networks (LeNet-5, BERT Encoder); climate code (vertical adv., horizontal diff.)Section 2opt. schedule →opt. pebblingSection 2opt. pebbling → max. subsetreuse overapproxSection 4max. subset → rect. subcomp.Section 6rect. subcomp.→ opt. subgraphreuse overapproxRecompute and reuse upper boundComp./comm. ratio upper boundPebbling schedule lower boundTiled parallel codeTight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

Figure 2: From the input code to the I/O lower bounds. First, for each statement, the access function vectors 𝝓 are extracted from the input program (green and
blue fields). For each statement, the size of its dominator set is obtained using Lemma 3 (Section 4.2), and then, the I/O lower bound is obtained using inequality 9
(Section 4.5). For programs that contain multiple statements, the SDG is constructed (Section 6) and all valid subgraph statements are evaluated (Section 6.1).
Lastly, the final I/O lower bound is obtained (Section 6.2).

and Kung [8]. Given a constant 𝑋 , an 𝑋 -partition of 𝐺 = (𝑉 , 𝐸)
is a collection of 𝑠 mutually disjoint subsets H𝑖 ⊆ 𝑉 (referred to
as subcomputations) P (𝑋 ) = {H1, . . . , H𝑠 } : ∀𝑖≠𝑗 H𝑖 ∩ H𝑗 = ∅∧
(cid:208)𝑖 H𝑖 = 𝑉 with two additional properties:

• there are no cyclic dependencies between subcomputations:
: (∃(𝑢1, 𝑣1) ∈ 𝐸 s.t. 𝑢1 ∈ H𝑖 ∧ 𝑣1 ∈ H𝑗 ) =⇒

∀H𝑖 ≠ H𝑗
((cid:154)(𝑣2, 𝑢2) ∈ 𝐸 s.t. 𝑢2 ∈ H𝑖 ∧ 𝑣2 ∈ H𝑗 )

• ∀H ∈ P (𝑋 ), |𝐷𝑜𝑚𝑚𝑖𝑛 (H )| ≤ 𝑋 and |𝑀𝑖𝑛 (Hℎ)| ≤ 𝑋 .

The authors prove that for any 𝑋 > 𝑆, the optimal pebbling 𝑃𝑜𝑝𝑡
has an associated 𝑋 -partition P𝑜𝑝𝑡 (𝑋 ) s.t. |P𝑜𝑝𝑡 (𝑋 )| = ℎ.
Computational intensity. In previous works it was proven that
(a) 𝑄 is lower bounded by the number of subsequences ℎ in the
optimal pebbling 𝑃𝑜𝑝𝑡 [8]; (b) ℎ is lower bounded by the size of
the smallest 𝑋 -partition |P𝑚𝑖𝑛 (𝑋 )| for any value of 𝑋 > 𝑆 [16]; (c)
|P𝑚𝑖𝑛 (𝑋 )| is bounded by the maximum size of a single subcomputa-
tion |H𝑋 ,𝑚𝑎𝑥 | in any valid 𝑋 -partition: |P𝑚𝑖𝑛 (𝑋 )| ≥ |𝑉 |/|H𝑋 ,𝑚𝑎𝑥 |
[16]; and (d) if |H𝑋 ,𝑚𝑎𝑥 | can be expressed as a function of 𝑋 , that
is, 𝜒 (𝑋 ) ≡ |H𝑋 ,𝑚𝑎𝑥 |, then 𝑄 is bounded by

𝑄 ≥ |𝑉 |

𝑋0 − 𝑆
𝜒 (𝑋0)

,

(1)

where 𝑋0 = arg min𝑋
The expression 𝜌 =

𝜒 (𝑋 )
𝑋 −𝑆 (Lemma 2 in Kwasniewski et al. [27]).

𝜒 (𝑋 )
𝑋 −𝑆 is called the computational intensity.

3 SIMPLE OVERLAP ACCESS PROGRAMS
In Section 2, we show how the I/O cost of a program can be
bounded by the maximum size of a subcomputation H in any valid
𝑋 -partition of program CDAG. We now introduce Simple Over-
lap Access Programs (SOAP): a class of programs for which we
can derive tight analytic bounds of |H |. We leverage the SOAP
structure and design an end-to-end method for deriving I/O lower
bounds of input programs (summarized in Figure 2).
What is SOAP? Before introducing the formal definition, we start
with an illustrative example, which we use in the following sections.

Example 1. Consider the following 3-point stencil code (we use the
Python syntax in code listings):

for t in range (1 , T ):

for i in range (t ,N -t ):

A[i ,t +1]=( A [i -1 , t] + A[i ,t] + A [i +1 , t ])/3 + B[ i ]

This is what we will refer to as a single-statement SOAP. The program
consists of one statement 𝑆𝑡 : A[i,t+1]=(A[i,t] + ... which is
placed in two nested loops. All accessed data comes from static, dis-
joint, multi-dimensional arrays (A and B). Furthermore, different ac-
cesses to the same array (array A is referenced by [i,t+1], [i-1,t],
[i,t],[i+1,t]) are offset by a constant stride [0,1], [-1,0],[0,0],
[1,0]. We denote such access pattern as a simple overlap and it is
a defining property of SOAP.
Why SOAP? We use the restriction on the access pattern to pre-
cisely count the number of vertices in 𝐷𝑜𝑚(H ). If we allow arbi-
trary overlap of array accesses, we need to conservatively assume

3

Input program: statements St1and St2C[i,j]=(A[i]+A[i+1])*(B[j]+B[j+1])Input arrays: In(St1) = {A, B}  Output array: Out(St1) = {C} Access function vectors:statement St1n2 = 2: two 1-dim access fun. vector componentsE[i,j]+= C[i,k] * D[k,j]Input arrays In(St2) = {C, D, E}  Output array: Out(St1) = {E}  Access function vectors:statement St2n1= 1: one 2-dim access fun. vector componentArray AArray BArray DArray EProgram CDAG for N=M=2, K=3 Program SDGSubgraph statementfori in range(N): forj in range(M): St1: C[i,j] = (A[i]+A[i+1])*(B[j]+B[j+1])fori in range(N): forj in range(K): fork in range(M):St2:E[i,j]+= C[i,k] * D[k,j]ABDCEABDCEArray CSection 3Section 6Section 6.1Elements of Care recomputed, decreasing the I/O cost!Section 4Bounding by counting vertices inEquivalent of executing statements St1 and St2“together”Section 6.2Technical Report, 2021,

G. Kwasniewski et al.

a maximum possible overlap of accessed vertices. This reduces the
lower bound on |𝐷𝑜𝑚(H )|, which, in turn, increases the upper
bound on |H |, providing less-tight I/O lower bound for a program.
This is not a fundamental limitation of our method. However, it al-
lows a fully automatic derivation of tight I/O lower bounds for input
programs. If the restriction is violated, additional assumptions on the
access overlap are needed (Section 5).
SOAP definition. A program is a sequence of statements 𝑆𝑡1, . . . , 𝑆𝑡𝑘 .
Each such statement 𝑆𝑡 is a constant time computable function 𝑓
enclosed in a loop nest of the following form:

for 𝜓 1 ∈ D1 :
. . .
for 𝜓 ℓ ∈ Dℓ (𝜓 1, . . . ,𝜓 ℓ −1) :

𝑆𝑡 : 𝐴0 [𝝓0 (𝝍) ] ← 𝑓 (𝐴1 [𝝓1 (𝝍) ], 𝐴2 [𝝓2 (𝝍) ], . . . , 𝐴𝑚 [𝝓𝒎 (𝝍) ])

where:
(1) The statement 𝑆𝑡 is nested in a loop nest of depth ℓ.
(2) Each loop in the 𝑡th level, 𝑡 = 1, . . . , ℓ is associated with its
iteration variable 𝜓 𝑡 , which iterates over its domain D𝑡 ⊂ N.
Domain D𝑡 may depend on iteration variables from outer loops
𝜓 1, . . . ,𝜓 𝑡 −1 (denoted as D𝑡 (𝜓 1, . . . ,𝜓 𝑡 −1)).

(3) All ℓ iteration variables form the iteration vector 𝝍 = [𝜓 1, . . . ,𝜓 ℓ ]
and we define the iteration domain D as the set of all values the
iteration vector iterates over during the entire execution of the
program D ⊂ Nℓ .

𝑗 , . . . ,𝜓
𝑗 ×, . . . , ×D

(4) The dimension of array 𝐴𝑗 is denoted as 𝑑𝑖𝑚(𝐴𝑗 ).
(5) Elements of 𝐴𝑗 are referenced by an access function vector 𝝓 𝑗
𝑑𝑖𝑚 (𝐴 𝑗 )
]
𝑗

which maps 𝑑𝑖𝑚(𝐴𝑗 ) iteration variables 𝝍 𝑗 = [𝜓 1
to a set of 𝒏𝒋 elements from 𝐴𝑗 , that is 𝝓 𝑗 : D1
→

(cid:16)N𝑑𝑖𝑚 (𝐴𝑗 ) (cid:17)𝑛 𝑗

𝑗 ×, . . . , D

. We then write 𝝓 𝑗 = [𝝓 𝑗,1, . . . , 𝝓 𝑗,𝑛 𝑗 ], where
𝑑𝑖𝑚 (𝐴𝑗 )
. . . , 𝑛 𝑗 . Further-
𝑗

→ N𝑑𝑖𝑚 (𝐴 𝑗 ), 𝑘 = 1,

𝝓 𝑗,𝑘 : D1
more, all access function components 𝝓 𝑗,𝑘 (𝝍 𝑗 ) are injective.
(6) All 𝑛 𝑗 access function vector’s components are equal up to a
constant translation vector, that is, ∀𝑘 = 1, . . . , 𝑛 : 𝝓 𝑗,𝑘 (𝝍) =
] ∈ N𝑑𝑖𝑚 (𝐴𝑗 ) . We call
𝝓 𝑗,1 (𝝍) + 𝒕𝑘 , where 𝒕𝑘 = [𝑡 1
𝑘
𝝓 𝑗 the simple overlap access.

, . . . , 𝑡𝑑𝑖𝑚 (𝐴)
𝑘

(7) Arrays 𝐴1, . . . 𝐴𝑚 are disjoint. If the output array 𝐴0 is also used
as an input, that is, 𝐴0 ≡ 𝐴𝑗 , 𝑗 ≥ 1, then 𝝓0 ∪ 𝝓 𝑗 is also the
simple overlap access (c.f. Example 1).

)
3
§
(
n
o
i
t
i
n
i
f
e
d
P
A
O
S

𝐴0
𝐴𝑗 , 𝑗 = 1, . . . , 𝑚
𝝍 = [𝜓 1, . . . ,𝜓 ℓ ]
D ⊆ D1×, . . . , ×Dℓ

𝝓 𝑗 = [𝝓 𝑗,1, . . . , 𝝓 𝑗,𝑛 𝑗 ]

𝒕 𝑗,𝑘 = [𝑡 1

𝑗,𝑘 , . . . , 𝑡

𝑑𝑖𝑚 (𝐴𝑗 )
𝑗,𝑘

]

) P (𝑋 ) = {H1, . . . , H𝑠 }

4
§
(
n
o
i
t
a
t
u
p
m
o
c
b
u
s

t
n
e
m
e
t
a
t
s
-
e
l
g
n
i
S

𝑫 = 𝐷 1×, . . . , ×𝐷ℓ

H ⊆ 𝑫 ⊆ 𝑉

A = 𝝓 [H ]

^𝑡𝑖 = {𝑡𝑖
1

, . . . , 𝑡𝑖

𝑛 } \ {0}

Dom ( H)
𝜌

𝑄 ≥ |D |

(cid:205)𝑚
𝑗 =1
(cid:206)ℓ

𝑡 =1

|A 𝑗 (𝑋0 ) |−𝑆
|𝐷𝑡 (𝑋0 ) |

) 𝐺𝑆 = (𝑉𝑆 , 𝐸𝑆 )

6
§
(

𝐼 ⊂ 𝑉𝑆

G
D
S

𝐺𝑆 [𝐻 ], 𝐻 ⊂ 𝑉𝑆 \ 𝐼

St𝐻

𝑗

𝑗 , . . . ,𝜓 𝑑𝑖𝑚 (𝐴𝑗 )

] to 𝑛 𝑗 elements in array 𝐴𝑗 .

Output array of statement St (may overlap
with input arrays).
Mutually disjoint input arrays of statement St.
Iteration vector composed of ℓ iteration variables.
Iteration domain: a set of values that iteration
vector 𝝍 takes during the entire program execution.
Access function vector that maps 𝑑𝑖𝑚 (𝐴𝑗 ) variables
[𝜓 1
Translation vector of 𝑘-th access function vector’s
component 𝝓 𝑗,𝑘 , that is 𝝓 𝑗,𝑘 ≡ 𝝓 𝑗,1, 𝑘 = 1, . . . , 𝑛 𝑗
An 𝑋 -partition of CDAG 𝐺 = (𝑉 , 𝐸) composed
of 𝑠 disjoint subcomputations.
Subcomputation domain: a Caresian product of
ranges of ℓ iteration variables during H.
Subcomputation H uniquely defined by a set
of |H | iteration vector’s values 𝝍 ∈ 𝑫 taken
during H. If H = 𝑫, we call it a rectangular
subcomputation H𝑟𝑒𝑐 .
Access set: a set of vertices from array 𝐴 that are
accessed by 𝝓 during H.
Access offset set: set of all non-zero 𝑖th coordinates
among 𝑛 translation vectors 𝒕𝑘 , 𝑘 = 1, . . . , 𝑛.
Dominator set of subcomputation H.
The computational intensity of the 𝑋 -partition.

A number of I/O operations of a schedule.

Symbolic Directed Graph, where every array
accessed in a program is a vertex, and edges
represent data dependencies between them.
Set of read-only arrays of the program.
SDG subgraph that represents a subcomputation
in which at least one vertex from every
array in 𝐻 is computed.
Subgraph SOAP statement.

𝑋 -Partitioning on SOAP’s CDAG. Recall that our objective is to
bound the maximum size of any subcomputation |H |. Given peb-
bling 𝑃 and an associated 𝑋 -partition P (𝑋 ), every subcomputation
H ∈ P (𝑋 ) is therefore associated with the set of iteration vectors 𝝍 of
the vertices computed in H . In the following section we will derive
it by counting how many non-input vertices (iteration vectors) can
H contain by bounding its dominator set size |𝐷𝑜𝑚(H )| - again,
by counting vertices corresponding to each access 𝐴𝑗 [𝝓 𝑗,𝑘 (𝝍)].

𝑑𝑖𝑚 (𝐴𝑗 )
𝑗

Table 1: Notation used in the paper.

(8) Each execution of statement 𝑆𝑡 is an evaluation of 𝑓 for a given

value of iteration vector 𝝍.

4 I/O LOWER BOUNDS FOR

SINGLE-STATEMENT SOAP

Iteration variables and iteration vectors. Formally, an iteration
variable 𝜓 𝑡 is an iterator: an object which takes values from its
iteration domain during the program execution. However, if it is
clear from the context, we will refer to a particular value of the
iteration variable simply as 𝜓 𝑡 (or a value of iteration vector as 𝝍).
Vertices as iteration vectors. Since by definition of CDAG, each
computation corresponds to a different vertex, and by definition
of SOAP, every statement execution is associated with a single
iteration vector 𝝍, every non-input vertex in 𝐺 is uniquely associ-
ated with an iteration vector 𝝍. Input vertices are referred to by
their access function vectors 𝑢 = 𝐴𝑗 [𝝓 𝑗,𝑘 (𝝍)]. We further define
CDAG edges as follows: for every value of iteration vector 𝝍, we
add an edge from all accessed elements to the vertex associated
with 𝝍, that is: 𝐸 = {(𝑢, 𝑣) : 𝑢 = 𝐴𝑗 [𝝓 𝑗,𝑘 (𝝍)], 𝑣 = 𝝍, 𝝍 ∈ D}.

4

We now derive the I/O bounds for programs that contain only one
SOAP statement. We start with introducing necessary definitions
that allow us to bound the size of a rectangular subcomputation.
The summary of the notation is presented in Table 1.

4.1 Definitions
Definition 1. Subcomputation domain. Denote the set of all val-
ues which iteration variable 𝜓 𝑡 takes during subcomputation H
as 𝐷𝑡 ⊂ D𝑡 , 𝑡 = 1, . . . , ℓ. Then, the subcomputation domain
𝑫 (H ) ⊆ D is a Cartesian product of ranges of all ℓ iteration vari-
ables which they take during H , that is 𝑫 (H ) = 𝐷1×, . . . , ×𝐷ℓ . We
therefore have H ⊆ 𝑫 (H ) ⊂ Nℓ . If it is clear from the context, we
will sometimes denote 𝑫 (H ) simply as 𝑫.

Tight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

Example 2. Recall the program from Example 1. Consider subcom-
putation H in which t ∈ {1, 2} and i ∈ {1, 2}. Then, subcomputation
domain 𝑫 = {1, 2} × {1, 2} = {[1, 1], [1, 2], [2, 1], [2, 2]}, but com-
putation itself can contain at most 3 elements H ⊆ {[1, 1], [1, 2], [2, 2]},
since 𝝍 = [2, 1] ∉ D does not belong to the iteration domain.

Definition 2. Access set and access subdomain. Consider input
array 𝐴 and its access function vector 𝝓. Given H , the access set A of
𝐴 is the set of vertices belonging to 𝐴 that are accessed during H , that
is A = 𝝓 [H ] = {𝐴[𝝓 (𝝍)] : 𝝍 ∈ H }. If function 𝝓 = [𝝓1, . . . , 𝝓𝑛]
accesses 𝑛 vertices from 𝐴, we analogously define access sets for each
access function component 𝝓𝑘 [H ], 𝑘 = 1, . . . , 𝑛. We then have A =
(cid:208)𝑛
𝑘=1 𝝓𝑘 [H ]. The access subdomain 𝑫 (A) is minimum bounding
box of the access set A.

Example 3. For program in Example 1, consider subcomputation H
evaluated on only one iteration vector H = [𝑖 = 2, 𝑗 = 2]. We have
two accessed arrays A and B. Furthermore, we have 𝝓A = [[𝑖, 𝑡 +1], [𝑖 −
(cid:16)N2(cid:17) 4
1, 𝑡], [𝑖, 𝑡], [𝑖 − 1, 𝑡]]. Therefore, 𝑑𝑖𝑚(A) = 2, and 𝝓B : N2 →
.
We further have 𝝓B = [[𝑖]], 𝑑𝑖𝑚(B) = 1, and 𝝓A : N → N.
To evaluate 𝑆𝑡 for 𝝍 = [2, 2], we need to access four elements of
A (three loads and one store), so its access set is A = 𝝓A [H ] =
{[2, 3], [1, 2], [2, 2], [2, 3]}. Furthermore, we have the access subdo-
main 𝑫 (A) = {2, 3} × {1, 2, 3}.

Definition 3. Access offset set. Given a simple overlap access 𝝓 =
𝑑𝑖𝑚 (𝐴 𝑗 )
[𝝓1, . . . , 𝝓𝑛] consider its 𝑛 translation vectors 𝒕𝑘 = [𝑡 1
]
𝑘
𝑘
∈ N𝑑𝑖𝑚 (𝐴), 𝑘 = 1, . . . , 𝑛. For each dimension 𝑖 = 1, . . . , 𝑑𝑖𝑚(𝐴𝑗 ) we
denote ^𝑡𝑖 = {𝑡𝑖
𝑛 } \ {0} as the set of all unique non-zero 𝑖th
1
coordinates among all 𝑛 translation vectors.

, . . . , 𝑡𝑖

, . . . , 𝑡

Definition 4. Rectangular subcomputation For a given subcom-
putation domain 𝑫, a subcomputation H is called rectangular if
H = 𝑫 and is denoted H𝑟𝑒𝑐 (𝑫). The size of rectangular computation
is |H𝑟𝑒𝑐 (𝑫)| = (cid:206)ℓ
𝑡 =1 |𝐷𝑡 |. If it is clear from the context, we will
denote H𝑟𝑒𝑐 (𝑫) simply as H𝑟𝑒𝑐 .

Observation 1. Consider a simple overlap access 𝝓 = [𝝓1, . . . , 𝝓𝑛]
of array 𝐴 and a rectangular subcomputation H𝑟𝑒𝑐 (𝑫). Then since all
𝝓𝑘 are equal up to translation, the ranges of iteration variables they
access are also equal up to the same translation: ∀𝑖 = 1, . . . , 𝑑𝑖𝑚(𝐴) :
∀𝑗 = 1, . . . , 𝑛 : 𝝓 𝑗 [𝐷𝑖 ] = 𝝓1 [𝐷𝑖 ] + 𝑡 𝑗 , which also implies that
∀𝑖 = 1, . . . , 𝑑𝑖𝑚(𝐴) : ∀𝑗 = 1, . . . , 𝑛 : |𝝓 𝑗 [𝐷𝑖 ]| = |𝝓1 [𝐷𝑖 ]|.

To bound the sizes of rectangular subcomputations, we use two

lemmas given by Kwasniewski et al. [27]:

Lemma 1. (Lemma 4 in [27]) For statement 𝑆𝑡, given 𝑫, the size of
subcomputation H (number of vertices of 𝑆 computed during H ) is
bounded by the sizes of the iteration variables’ sets 𝐷𝑡 , 𝑡 = 1, . . . , ℓ:
ℓ
(cid:214)

|H | ≤

|𝐷𝑡 |.

(2)

𝑡 =1

Proof. Inequality 2 follows from a combinatorial argument:
each computation in H is uniquely defined by its iteration vector
[𝜓 1, . . . ,𝜓 ℓ ]. As each iteration variable 𝜓 𝑡 takes |𝐷𝑡 | different val-
ues during H , we have |𝐷1| · · · · · |𝐷𝑡 | = (cid:206)ℓ
𝑡 =1 |𝐷𝑡 | ways how to
□
uniquely choose the iteration vector in H .

5

(Lemma 5 in [27]) For the given access function 𝝓 =
Lemma 2.
[𝝓1, . . . , 𝝓𝑛] accessing array 𝐴, 𝐴[𝝓 (𝝍)], the access set size of each
of components |𝝓𝑘 [H ] | during subcomputation H is bounded by the
sizes of 𝑑𝑖𝑚(𝝓𝐴) iteration variables’ sets 𝐷𝑖, 𝑘 = 1, . . . , 𝑑𝑖𝑚(𝝓 𝑗 ):

|𝝓𝑘 [H ] | ≤

𝑑𝑖𝑚 (𝝓𝐴)
(cid:214)

|𝐷𝑖 |

(3)

𝑖=1

where 𝐷𝑖 ∋ 𝜓𝑖 is the iteration domain of variable 𝜓𝑖 during H .
Proof. We use the same combinatorial argument as in Lemma 1.
Since access functions are injective, each vertex in 𝐴 accessed by
]. Knowing the number
is uniquely defined by [𝜓 1
𝑘
takes in H , we bound the number of
□

𝝓𝒌
of different values each 𝜓 𝑗
𝑘
different access vectors 𝝓𝑘 [H ].

, . . . ,𝜓𝑑𝑖𝑚 (𝐴)
𝑘

4.2 Bounding SOAP Access Size
Recall that our goal is to find the maximum size of the subcompu-
tation given its dominator size. We first do the converse: given the
rectangular subcomputation H𝑟𝑒𝑐 , we bound the minimum num-
ber of input vertices required to compute H𝑟𝑒𝑐 . In Section 4.4 we
prove that indeed H𝑟𝑒𝑐 is the subcomputation that upper-bounds
the maximum computational intensity 𝜌. Since arrays 𝐴1, . . . , 𝐴𝑚
are disjoint, the total number of input vertices is the sum of their
access set sizes: |𝐷𝑜𝑚𝑚𝑖𝑛 (H𝑟𝑒𝑐 ) ≥ (cid:205)𝑚
𝑗=1 |A 𝑗 |. We now proceed to
bound individual access set sizes |A 𝑗 |.

Consider array 𝐴 with 𝑑𝑖𝑚(𝐴) = 𝑑 and its access function 𝝓 (𝝍) =
[𝝓1 (𝝍), . . . , 𝝓𝑛 (𝝍)] that access 𝑛 elements from 𝐴 (to simplify
the notation, we drop the subscript 𝑗, since we consider only one
array). Observe that during H𝑟𝑒𝑐 , all combinations of iteration
variables𝜓 1 ∈ 𝐷1, . . . ,𝜓 ℓ ∈ 𝐷ℓ are accessed, so |H𝑟𝑒𝑐 | = (cid:206)ℓ
𝑡 =1 |𝐷𝑡 |
(Lemma 1). This also implies that each of 𝑘 = 1, . . . , 𝑛 accesses to
𝐴 required |𝝓𝑘 [H𝑟𝑒𝑐 ]| = (cid:206)𝑑
𝑡 =1 |𝐷𝑡 | vertices from 𝐴 (Lemma 2 and
Observation 1). Therefore, the total number of accesses to array
𝐴 during H𝑟𝑒𝑐 is |A| ≥ (cid:206)𝑑
𝑡 =1 |𝐷𝑡 |. However, the sets of vertices
accessed by different 𝝓𝑘 may overlap, that is, there may exist two
accesses 𝝓𝑙 and 𝝓𝑚, for which 𝝓𝑙 [H𝑟𝑒𝑐 ]∩𝝓𝑚 [H𝑟𝑒𝑐 ] ≠ ∅. Therefore,
we also obtain the upper bound |A| ≤ (cid:205)𝑛
𝑡 =1 |𝐷𝑡 |. We now
want to narrow the gap between the upper and the lower bounds.

(cid:206)𝑑

𝑗=1

If a given input array 𝐴 with 𝑑𝑖𝑚(𝐴) = 𝑑 is accessed
Lemma 3.
by a simple overlap access 𝝓 (𝝍) = [𝝓1 (𝝍), . . . , 𝝓𝑛 (𝝍)], its access set
size |A| during rectangular computation H𝑟𝑒𝑐 (𝑫) is bounded by

|A| = |𝝓 [H𝑟𝑒𝑐 (𝑫)]| ≥ 2

𝑑
(cid:214)

𝑖=1

|𝐷𝑖 | −

𝑑
(cid:214)

𝑖=1

(|𝐷𝑖 | − |^𝑡𝑖 |),

(4)

where |^𝑡𝑖 | is the size of the access offsets set in the 𝑖th dimension.

Proof. W.l.o.g., consider the first access function component 𝝓1
and its (cid:206)𝑑
𝑡 =1 |𝐷𝑡 | accessed vertices 𝝓1 [H𝑟𝑒𝑐 ]. We will lower bound
the number of accesses to 𝐴 from remaining 𝝓𝑘, 𝑘 = 2, . . . , 𝑛, which
do not overlap with 𝝓1 [H𝑟𝑒𝑐 ], that is | (cid:208)𝑛
𝑘=2 𝝓𝑘 [H𝑟𝑒𝑐 ] \ 𝝓1 [H𝑟𝑒𝑐 ]|.
Since by construction of H𝑟𝑒𝑐 , all 𝝓𝑘 [H𝑟𝑒𝑐 ] are Cartesian products
of iteration variables’ ranges 𝝓𝑘 [𝐷1]×, · · · , ×𝝓𝑘 [𝐷𝑑 ], there is a
bijection between 𝝓𝑘 [H𝑟𝑒𝑐 ] and an 𝑑-dimensional hyperrectangle
𝐻𝑘 ∈ N𝑑 . To secure correctness of our lower bound on |A|, we need
to find the volume of the smallest union of these hyperrectangles.

Technical Report, 2021,

G. Kwasniewski et al.

4.3 Input-Output Simple Overlap
If one of the input arrays 𝐴𝑖, 𝑖 ≥ 1, is also the output array 𝐴0,
then their access function vectors 𝝓0 and 𝝓𝑖 form together a simple
overlap access (Section 3). In such cases, some vertices accessed by
𝝓𝑖 during H𝑟𝑒𝑐 may be computed and do not need to be loaded. We
formalize it in the following corollary, which follows directly from
Lemma 3:

Corollary 1. Consider statement 𝑆𝑡 that computes array 𝐴, 𝑑𝑖𝑚(𝐴) =
𝑑 and simultaneously accesses it as an input 𝐴[𝝓0 (𝝍)] = 𝑓 (𝐴[𝝓1 (𝝍)]).
If 𝝓0 ∪ 𝝓1 is a simple overlap access, the access set size |A| during
rectangular computation H𝑟𝑒𝑐 is bounded by

|A| ≥

𝑑
(cid:214)

𝑖=1

|𝐷𝑖 | −

𝑑
(cid:214)

𝑖=1

(|𝐷𝑖 | − |^𝑡𝑖 |),

(6)

where ^𝑡 is an access offset offset set of 𝝓0 ∪ 𝝓1.

𝑖=1 |𝐷𝑖 | − (cid:206)𝑑

Proof. This result follows directly from Lemma 3. Since there
are at least 2 (cid:206)𝑑
𝑖=1 (|𝐷𝑖 | − |^𝑡𝑖 |) vertices accessed from
𝐴𝑖 , and at most (cid:206)𝑑
𝑖=1 |𝐷𝑖 | of them can be computed during H𝑟𝑒𝑐
(Lemma 2) and therefore, do not have to be loaded, then at least
2 (cid:206)𝑑
𝑖=1 (|𝐷𝑖 | − |^𝑡𝑖 |) − (cid:206)𝑑
𝑖=1 |𝐷𝑖 | elements have to be
□
accessed from the outside of H𝑟𝑒𝑐 .

𝑖=1 |𝐷𝑖 | − (cid:206)𝑑

4.4 Bounding Maximal Subcomputation
In Section 4.2 we lower-bounded the dominator set size of the rectan-
gular subcomputation |𝐷𝑜𝑚𝑚𝑖𝑛 (H𝑟𝑒𝑐 )| = (cid:205)𝑚
𝑗=1 |A 𝑗 | by bounding
the sizes of simple overlap access sets sizes |A 𝑗 | (Lemma 3). Recall
that to bound the I/O lower bound we need the size 𝜒 (𝑋 ) of the
maximal subcomputation H𝑚𝑎𝑥 for given value of 𝑋 (Inequality 1).
We now prove that H𝑟𝑒𝑐 upper-bounds the size of H𝑚𝑎𝑥 .

(cid:205)𝑚

|H |
𝑗 =1 |𝝓 𝑗 [H ] |

Given H , denote the the ratio of the size of the subcomputa-
. By definition,

tion to the dominator set size 𝛿 (H ) =
H𝑚𝑎𝑥 maximizes 𝛿 among all valid H ∈ P. We need to show
that for a fixed subcomputation domain 𝑫0, among all subcompu-
tations for which 𝑫 (H ) = 𝑫0, the rectangular subcomputation
H𝑟𝑒𝑐 (𝑫0) upper-bounds 𝛿. Note that an 𝑋 -partition derived from
the optimal pebbling schedule 𝑃𝑜𝑝𝑡 may not include H𝑟𝑒𝑐 . How-
ever, ∀𝑋 : 𝜒𝑟𝑒𝑐 (𝑋 ) ≥ 𝜒 (𝑋 ), that is, given 𝑋 , the size of H𝑟𝑒𝑐 s.t.,
(cid:205)𝑚
𝑗=1 |𝝓 𝑗 [H𝑟𝑒𝑐 ]| = 𝑋 will always be no smaller than the size of
H𝑚𝑎𝑥 . To show this, we first need to introduce some auxiliary
definitions.

Iteration variables, their indices, and their values. To sim-
plify the notation, throughout the paper we used the iteration vari-
ables 𝜓𝑖 and the values they take for some iteration interchangeably.
However, now we need to make this distinction explicit. The it-
eration vector consists of ℓ iteration variables 𝝍 = [𝜓 1, . . . , 𝜓 ℓ ].
Each access function 𝝓 𝑗 is defined on 𝑑𝑖𝑚(𝐴𝑗 ) ≤ ℓ of them. Re-
call that 𝝍 𝑗 is the set of iteration variables accessed by 𝝓 𝑗 (Sec-
tion 3, property (5)). To keep track of the indices of particular
iteration variables, denote 𝚿 = [ℓ] = {1, . . . , ℓ } ⊂ N, 𝚿𝑗 ⊆ 𝚿,
and 𝚿′
𝑗 = 𝚿 \ 𝚿𝑗 as the sets of integers. If 𝑖 ∈ 𝚿𝑗 , then the 𝑖th
iteration variable 𝜓𝑖 is accessed by the access function 𝝓 𝑗 . We fur-
ther define 𝝍∗ ∈ Nℓ as a specific value of the iteration vector 𝝍
that uniquely defines a single non-input vertex. We analogously
define 𝝍∗
(the last one being a value of 𝑖th iteration

𝑗 , 𝜓𝑖,∗, and 𝜓𝑖,∗

𝑗

Intuition behind Lemma 3. Access sets 𝝓 [ H𝑟𝑒𝑐 (𝑫) ] as 3-
Figure 3:
dimensional hyperrectangles. The union | (cid:208)𝑛
𝑘=1 𝝓𝑘 [H𝑟𝑒𝑐 ] | (and therefore,
the total number of accesses | A | ) is minimized when the hyperrectangles
are placed in two antipodal locations of the subcomputation domain D.

𝑘=1

Note that |^𝑡𝑖 | is a lower bound on the maximum offset between
any two 𝐻 𝑗 ≠ 𝐻𝑘 in dimension 𝑖: the union of all hyperrectangles
(cid:208)𝑛
𝐻𝑘 “stretches” at least |𝐷𝑖 | + |^𝑡𝑖 | elements in the 𝑖th dimension
for all 𝑖 = 1, . . . , 𝑑 (see Figure 3). To see this, observe that since
𝐷𝑖 ⊂ 𝑁 , for each element in the access offset set 𝑡𝑖
𝑗 ∈ ^𝑡𝑖 there is at
least one element in 𝐷𝑖 + 𝑡𝑖
𝑗 that is not in 𝐷𝑖 , which implies that
𝑗 ) \ 𝐷𝑖 | ≥ 1. Since 𝐷𝑖 is finite, there is a single well-defined
|(𝐷𝑖 + 𝑡𝑖
maximum and a minimum element, which implies that (max{𝐷𝑖 } +
𝑗 ∉ 𝐷𝑖 ). Also, because by definition of ^𝑡𝑖 we
𝑗 ∉ 𝐷𝑖 ) ∨ (min{𝐷𝑖 } +𝑡𝑖
𝑡𝑖
have ∀𝑡𝑖
, then we also have that each 𝑡𝑖
𝑗 ≠ 𝑡𝑖
𝑗 accesses
𝑘
at least one “non-overlapping” element independent of any other
𝑡𝑖
𝑘

, that is ∀𝑡𝑖
The arrangement of hyperrectangles 𝐻𝑘, 𝑘 = 1, . . . , 𝑛 in a N𝑑
lattice s.t., their bounding box is 𝑫 = (|𝐷1| + |^𝑡 1|) × · · · × (|𝐷𝑑 | +
|^𝑡𝑑 |), which minimizes the size of their union | (cid:208)𝑘 𝐻𝑘 | satisfies two
properties:

𝑘 ∈ ^𝑡𝑖 : max{𝐷𝑖 } + 𝑡𝑖

𝑗 ≠ max{𝐷𝑖 } + 𝑡𝑖
𝑘

𝑘 ∈ ^𝑡𝑖 : 𝑡𝑖

𝑗 , 𝑡𝑖

𝑗 , 𝑡𝑖

.

(1) there exist two “extreme” 𝐻𝑝 , 𝐻𝑞, such that 𝐻𝑞 = 𝐻𝑝 + 𝒗,

𝒖 = Z𝑑, ∀𝑖=1,...,𝑑 : |𝑣𝑖 | = |^𝑡𝑖 |,

(2) all the remaining 𝐻𝑘, 𝑘 ≠ 𝑝, 𝑞 perfectly overlap with the

“extreme” hyperrectangles 𝐻𝑘 ⊆ 𝐻𝑝 ∪ 𝐻𝑞.

𝑞, s.t., 𝐻𝑖

hyperrectangles 𝐻𝑖
𝑞 is offset from 𝐻𝑖
𝐻𝑖
(cid:208)𝑖, |^𝑡𝑖 |>0 (𝐻𝑖
equal, but there are no restrictions on 𝐻𝑖
the union (cid:208)𝑖, |^𝑡𝑖 |>0 (𝐻𝑖

To see this, observe that for every non-zero |^𝑡𝑖 | we need two
𝑝 ≠ 𝐻𝑖
𝑝 + [·, . . . , |^𝑡𝑖 |, . . . , ·], that is,
𝑞 = 𝐻𝑖
𝑝 by |^𝑡𝑖 | in 𝑖th dimension. We therefore have
𝑝 and 𝐻𝑖
𝑞 are pairwise non-
𝑝, 𝐻 𝑗
𝑝, 𝑖 ≠ 𝑗, we have that
𝑝 = 𝐻 𝑗
𝑝 .
𝐻𝑘 | s.t. to the claimed

𝑞) is minimized if ∀𝑖≠𝑗 𝐻𝑖
𝑘=1

Finally, observe the volume of | (cid:208)𝑛

𝑞) ⊆ (cid:208)𝑘 𝐻𝑘 . Since 𝐻𝑖

𝑝 ∪ 𝐻𝑖

𝑝 ∪ 𝐻𝑖

arrangement is:

(cid:12)
(cid:12)
(cid:12)

𝑛
(cid:216)

𝑘=1

𝐻𝑘

(cid:12)
(cid:12) = |𝐻𝑝 ∪ 𝐻𝑞 | = 2
(cid:12)

𝑑
(cid:214)

𝑖=1

|𝐷𝑖 | −

𝑑
(cid:214)

𝑖=1

(|𝐷𝑖 | − |^𝑡𝑖 |)

(5)

It shows that for any set of 𝑛 hyperrectangles s.t. the given
constraint, the volume of their union is no smaller than the one
in Equation 5. Since the offset constraint is also a lower bound on
the number of non-overlapping accesses in each dimension, it also
forms the bound on | (cid:208)𝑛
□

𝑘=1 𝝓𝑘 [H𝑟𝑒𝑐 ]| = |𝝓 [H𝑟𝑒𝑐 ]| = |A|.

6

Iteration vector(3 iteration variables)Access functionvector (2 components)Iteration variables’ rangesTight I/O Bounds of Statically Analyzable Programs

of the access set: 𝛿 −1

𝑗 =

|𝝓 𝑗 [H ] |
|H |

=

Technical Report, 2021,

. Therefore, we can

|𝐷𝑖 |

1
(cid:206)𝑖∈𝚿′
𝑗

safely maximize 𝝓 𝑗 [H ] to the entire access set of the rectangular
subcomputation H𝑟𝑒𝑐 without increasing 𝛿 −1
. We conclude that
for every access function 𝝓 𝑗 and every iteration variable index 𝑖,
evaluating all vertices 𝝍∗ s.t. 𝜓𝑖 iterates over the entire domain 𝐷𝑖
minimizes 𝛿 −1

.

𝑗

𝑗

□

Figure 4: Intuition behind Lemma 4: extending the subcomputation in the
free dimensions w.r.t 𝝓 𝑗 does not increase |𝝓 𝑗 [H ] |. Once the subcomputa-
tion is almost rectangular, extending 𝐻 in the remaining dimensions keeps
the ratio 𝛿 −1

constant.

𝑗

variable of the 𝑗th access). We also define 𝜃 (𝝍∗
of vertices in H that have all their 𝚿𝑗 coordinates equal to 𝝍∗
𝑗 , H ) = |{𝝍∗ : 𝝍∗ ∈ H ∧ (∀𝑖 ∈ 𝚿𝑗 : 𝜓𝑖,∗ = 𝜓𝑖,∗
is 𝜃 (𝝍∗
)}|.
We now formalize our claim in the following lemma:

𝑗 , H ) as the number
𝑗 , that

𝑗

Lemma 4. Given the subcomputation domain 𝑫0, H𝑟𝑒𝑐 (𝑫0) maxi-
mizes 𝛿 (H ) for all H s.t. 𝑫 (H ) = 𝑫0.

∀H : 𝛿 (H ) ≤ 𝛿 (H𝑟𝑒𝑐 )

(7)

Proof. Instead of maximizing 𝛿 (H ), we will minimize 𝛿 −1 (H ) =
((cid:205)𝑚
𝑗=1 |𝝓 𝑗 [H ]|)/|H | = (cid:205)𝑚
𝑗=1 |𝝓 𝑗 [H ]|/|H | over all possible H . Ob-
serve that 𝛿 −1 (H ) is linear w.r.t. the ratios of individual access
function sets sizes |𝝓 𝑗 [H ]| and the size of subcomputation |H |.
Therefore, we can examine each access 𝝓 𝑗 [H ] separately and show
that every 𝛿 −1
𝑗 = |𝝓 𝑗 [H ]|/|H | is minimized for H = H𝑟𝑒𝑐 . Then,
, then 𝛿 −1 = (cid:205)𝑚
if H𝑟𝑒𝑐 minimizes each of 𝛿 −1
is minimized,
𝑗=1
so indeed H𝑟𝑒𝑐 maximizes the ratio of the subcomputation size to
the dominator set size.

𝛿 −1
𝑗

𝑗

𝑗

𝑗

𝑗 , H ) for all 𝝍∗

Observe now, that for any H we have that ∀𝑗 : 𝛿 −1

is monoton-
ically decreasing w.r.t. 𝜃 (𝝍∗
𝑗 ∈ 𝝓 𝑗 [H ]. That is - pick
any input vertex 𝝍∗
𝑗 from the set of vertices accessed by 𝝓 𝑗 [H ].
Adding compute vertices 𝝍∗ to H that access 𝝍∗
𝑗 do not increase
the access set size 𝝓 𝑗 [H ], since 𝝍∗
𝑗 is already accessed. However,
it increases the size of H . Clearly, 𝛿 −1
reaches its minimum if
𝑗
|𝐷𝑖 |, that is, H computes all
∀𝝍∗
vertices spanned by the access set 𝝓 𝑗 [H ] and all elements in the
Cartesian product of “free” (independent of the access function 𝜙 𝑗 )
iteration domains 𝐷𝑖, 𝑖 ∈ 𝚿′
𝑗 .

𝑗 ∈ 𝝓 𝑗 [H ] : 𝜃 (𝝍∗

𝑗 , H ) = (cid:206)𝑖 ∈𝚿′

We showed that for all 𝑗, given its initial access set 𝝓 𝑗 [H ], the
ratio 𝛿 −1
is minimized for the “almost-rectangular” subcomputation,
𝐷𝑖 .
that is, H which computes all vertices 𝝍∗ ∈ 𝝓 𝑗 [H ] × (cid:206)𝑖 ∈𝚿′
We now need to show that also extending H over the “dependent”
ranges 𝚿𝑗 won’t increase the ratio 𝛿 −1. When the access set size
𝝓 𝑗 [H ] increases by a factor 𝑥, H increases proportionally by 𝑥 too,
keeping the ratio constant (See Figure 4 for an example for ℓ = 3).
separately, indepen-
𝑗
dently of other 𝛿 −1
, 𝑖 ≠ 𝑗, assume that we have already extended
H to the “almost-rectangular” subcomputation, that is, all com-
𝐷𝑖 were accessed in H . Observe now that
binations of (cid:206)𝑖 ∈𝚿′
|𝐷𝑖 | for any vertex 𝝍∗
𝑗 , H ) = (cid:206)𝑖 ∈𝚿′
𝜃 (𝝍∗
𝑗 . Therefore, since |H | =
|𝐷𝑖 |, we see that 𝛿 −1
(cid:205)
(cid:206)𝑖 ∈𝚿′
is constant w.r.t., the size

Since our goal is to minimize each 𝛿 −1

𝑖

𝑗

𝑗

𝑗

𝑗

𝑗

𝝍∗

𝑗 ∈𝝓 𝑗 [H ]

𝑗

4.5 I/O Lower Bounds and Optimal Tiling
We now proceed to the final step of finding the I/O lower bound. Re-
call from Section 2.2, that the last missing piece is 𝜒 (𝑋 ); that is, we
seek to express |H𝑚𝑎𝑥 (𝑫)| = (cid:206)ℓ
𝑡 =1 |𝐷𝑡 | as a function of 𝑋 . Observe
that by Lemma 4, |𝐷𝑜𝑚𝑚𝑖𝑛 (H𝑚𝑎𝑥 (𝑫)) ≥ (cid:205)𝑚
𝑗 | −
(cid:206)𝑑𝑖𝑚 (𝐴 𝑗 )
𝑗 |). On the other hand, by definition of
𝑖=1
𝑋 -Partitioning, |𝐷𝑜𝑚𝑚𝑖𝑛 (H𝑚𝑎𝑥 (𝑫))| ≤ 𝑋 . Combining these in-
equalities, we solve for all |𝐷𝑡 | as functions of 𝑋 by formulating
it as the optimization problem (see Section 3.2 in Kwasniewski et
al. [27]):

𝑗=1 (2 (cid:206)𝑑𝑖𝑚 (𝐴𝑗 )
𝑖=1

𝑗 | − |^𝑡𝑖

(|𝐷𝑖

|𝐷𝑖

max

ℓ
(cid:214)

𝑡 =1

|𝐷𝑡 |

s.t.

𝑚
∑︁

𝑗=1

|A 𝑗 | ≤ 𝑋

∀1 ≥ 𝑡 ≥ ℓ : |𝐷𝑡 | ≥ 1

(8)

Solving the above optimization problem yields 𝜒 (𝑋 ) = |H𝑚𝑎𝑥 (𝑫)|.

Since Lemma 4 gives a valid upper bound on computational in-
tensity for any value of 𝑋 , we seek to find the tightest (lowest)
𝜒 (𝑋 )
upper bound. One can obtain 𝑋0 = arg min𝑋
𝑋 −𝑆 , since 𝜒 (𝑋 ) is
differentiable. Finally, combining Lemma 3, inequality 1, and the
optimization problem 8, we obtain the I/O lower bound for the
single-statement SOAP program:

𝑄 ≥ |D|

(cid:205)𝑚

𝑗=1 |A 𝑗 (𝑋0)| − 𝑆
(cid:206)ℓ
𝑡 =1 |𝐷𝑡 (𝑋0)|

,

(9)

where |A 𝑗 (𝑋0)| are the access set sizes obtained from Lemma 3 for
the optimal value of |𝐷𝑖 | derived from the optimization problem 8.
Substituting 𝑋0 back to |𝐷𝑡 |(𝑋 ) has a direct interpretation: they
constitute optimal loop tilings for the maximal subcomputation.
Note that such tiling might be invalid due to problem relaxations:
e.g., we ignore loop-carried dependencies and we solve optimization
problem 8 over real numbers, relaxing the integer constraint on
|𝐷𝑡 | set sizes. However, this result can serve as a powerful guideline in
code generation. Furthermore, if derived tiling sizes generate a valid
code, it is provably I/O optimal.

5 PROJECTING PROGRAMS ONTO SOAP
By the definition of SOAP, one input array may be accessed by
different access function vector components, only if they form the
simple overlap access — that is, the accesses are offset by a constant
stride. However, our analysis may go beyond this constraint if
additional assumptions are met.

7

Extend inremainingdomainsTechnical Report, 2021,

G. Kwasniewski et al.

5.1 Non-Overlapping Access Sets
Given input array 𝐴 and its access function components 𝝓 (𝝍) =
[𝝓1 (𝝍1), . . . , 𝝓𝑛 (𝝍𝑛)], if all access sets are disjoint, that is: ∀𝑖≠𝑗 𝝓𝑖 [D]∩
𝝓 𝑗 [D] = ∅, then we represent it as 𝑛 disjoint input arrays 𝐴𝑖 ac-
cessed by single corresponding access function component 𝝓𝑖 (𝝍𝑖 ).

Example 4. Consider the following code fragment from LU decom-
position:

for k in range (N ):

for i in range (k +1 , N ):

for j in range (k +1 , N ):

A[i ,j] = A[i ,j] - A[i ,k] * A[k , j]

𝑆𝑡 :
The analysis of iteration variables’ domains D𝑖, D 𝑗 , D𝑘 shows
that for fixed value of 𝑘0, there are no two iteration vectors 𝝍1 =
[𝑘0, 𝑖1, 𝑗1] and 𝝍2 = [𝑘0, 𝑖2, 𝑗2] such that [𝑖1, 𝑘0] = [𝑘0, 𝑗2] ∨[𝑖1, 𝑗1] =
[𝑘0, 𝑗2] ∨[𝑖1, 𝑗1] = [𝑖1, 𝑘0], therefore, their access sets are disjoint.
Furthermore, for 𝑘0, all elements from 𝐴 in range [(𝑘0, 𝑁 ), (𝑘0, 𝑁 )]
are updated. Therefore, all accesses of form [𝑖1, 𝑘1] = [𝑘2, 𝑗2] access
different vertices. We model this as a SOAP statement with three
disjoint arrays:

𝑆𝑡2 : 𝐴1 [𝑖, 𝑗] = 𝑓 (𝐴1 [𝑖, 𝑗], 𝐴2 [𝑖, 𝑘], 𝐴3 [𝑘, 𝑗])

5.2 Equivalent Input-Output Accesses
If array 𝐴 is updated by statement 𝑆𝑡 — i.e., it is both input and
output — then we require that the output access function 𝝓0 is
different than the input access function 𝝓𝑖 . If the input program
does not meet this requirement, we can add additional “version
dimension” to access functions that is offset by a constant between
input and output accesses.

Example 5. Consider again Example 4. Observe that array 𝐴1 is
updated (it is both the input and the output of 𝑆𝑡2. Furthermore, both
access functions are equal: 𝝓0 = 𝝓1 = [𝑖, 𝑗]. We can associate a
unique version (and therefore, a vertex) of each element of 𝐴 with a
corresponding iteration of the 𝑘 loop. We add the version dimension
associated with 𝑘 and offset it by constant 1 between input and output:

𝑆𝑡3 : 𝐴1 [𝑖, 𝑗, 𝑘 + 1] = 𝑓 (𝐴1 [𝑖, 𝑗, 𝑘], 𝐴2 [𝑖, 𝑘], 𝐴3 [𝑘, 𝑗])

5.3 Non-Injective Access Functions
Given input array 𝐴 and its access function vector 𝝓, we require that
∀𝝍𝑖 ≠ 𝝍 𝑗 : 𝐴[𝝓 (𝝍𝑖 )] ≠ 𝐴[𝝓 (𝝍 𝑗 )]. If this is not the case, then we
seek to bound the size of such overlap, that is, given subcomputation
domain 𝑫 (H ), how many different iteration vectors 𝝍 𝑗 map to
the same array element 𝐴[𝝓 (𝝍𝑖 )]. We can solve this by analyzing
the iteration domain D and the access function vector 𝝓. If one
array dimension is accessed by a function of multiple iteration
variables 𝑔(𝜙 1, . . . , 𝜙𝑘 ) and 𝑔 is linear w.r.t. all 𝜙𝑖 , the number of
different values 𝑔 takes in 𝑫 (H ) is bounded by max𝑖=1,...,𝑘 {|𝐷𝑖 |} ≤
|𝑔[H ]| ≤ (cid:206)𝑘

𝑖=1 |𝐷𝑖 |, for 𝐷𝑖 ≠ {0}, 𝑖 = 1, . . . , 𝑘.

Example 6. A single layer of the direct convolution used in neural
networks may be written as seven nested loops with iteration variables
𝑏, 𝑐, 𝑘, 𝑤, ℎ, 𝑟, 𝑠 and statement (c.f. [23]):

𝑆𝑡 : 𝑂𝑢𝑡 [𝑘, ℎ, 𝑤, 𝑏]+ = 𝐼𝑚𝑎𝑔𝑒 [𝑟 + 𝜎𝑤𝑤, 𝑠 + 𝜎ℎℎ, 𝑐, 𝑏] × 𝐹𝑖𝑙𝑡𝑒𝑟 [𝑘, 𝑟, 𝑠]

8

Depending on the value of 𝜎𝑤 and 𝜎ℎ, the access function of 𝐼𝑚𝑎𝑔𝑒,
𝝓 = [𝑟 + 𝜎𝑤𝑤, 𝑠 + 𝜎ℎℎ, 𝑐, 𝑏] may not be injective. Yet, observe that:
(1) 𝜎𝑤 ≥ |𝐷𝑟 |∧𝜎ℎ ≥ |𝐷𝑠 | =⇒ 𝝓 is injective =⇒ |𝝓 [H𝑚𝑎𝑥 ]| ≥

|𝐷𝑟 | · |𝐷 𝑤 | · |𝐷𝑠 | · |𝐷ℎ | · |𝐷𝑐 | · |𝐷𝑏 |

(2) 𝜎𝑤

=

=
max(|𝐷𝑟 |, |𝐷 𝑤 |) · max(|𝐷𝑠 |, |𝐷ℎ |) · |𝐷𝑐 | · |𝐷𝑏 |,

=⇒

1 ∧ 𝜎ℎ

1

|𝝓 [H𝑚𝑎𝑥 ]|

≥

Our analysis provides a conditional computational intensity: 𝜌𝑚𝑖𝑛 =
√
𝑆/2 in case (1) and 𝜌𝑚𝑎𝑥 = 𝑆/2 in case (2). Observe that case (2)
yields the maximum non-injective overlap (maximum number of
different iteration vectors map to the same element in 𝐼𝑚𝑎𝑔𝑒). For any
other values of 𝜎𝑤 and 𝜎ℎ, we have 𝜌𝑚𝑖𝑛 ≤ 𝜌 ≤ 𝜌𝑚𝑎𝑥 .

6 MULTI-STATEMENT SOAP
I/O lower bounds are not composable: the I/O cost of a program
containing multiple statements may be lower than the sum of the
I/O costs of each statement if evaluated in isolation. Data may be
reused and merging of statements may lower the I/O cost.

Note that the number of vertices in the program’s CDAG 𝐺
depend on domain sizes 𝐷𝑖 of each iteration variable. However, our
derived upper bound of the computational intensity 𝜌 is independent
of the CDAG size, as it depends only on the access functions 𝝓 𝑗 .
This is also true for programs that contain multiple statements
- to bound 𝜌 for multi-statement SOAP, we only need to model
dependencies between the arrays and how they are accessed - e.g.,
one statement may take as an input an array that is an output of a
different statement.

We represent the data flow between the program statements
with a symbolic directed graph 𝐺𝑆 = (𝑉𝑆, 𝐸𝑆 ). For a given state-
ment 𝑆𝑡𝑖 , denote 𝐼𝑛(𝑆𝑡𝑖 ) = {𝐴𝑖,1, . . . , 𝐴𝑖,𝑚 } a set of input arrays
of statement 𝑆𝑡𝑖 . Analogously, denote 𝑂𝑢𝑡 (𝑆𝑡𝑖 ) the set containing
the output array of 𝑆𝑡𝑖 . Analogously to program CDAG 𝐺 that cap-
tured dependencies between particular array elements, 𝐺𝑆 models
dependencies between whole arrays (Figure 2).

Definition 5. Symbolic Digraph: SDG Given 𝑘-statement SOAP
𝑆𝑡1, . . . , 𝑆𝑡𝑘 , its symbolic digraph (SDG) 𝐺𝑆 = (𝑉𝑆, 𝐸𝑆 ) is a directed
graph where 𝑉𝑆 = (cid:208)𝑘
𝑖=1 (In(𝑆𝑡𝑖 )∪Out(𝑆𝑡𝑖 )) and (𝐴𝑢, 𝐴𝑣) ∈ 𝐸𝑆 ⇐⇒
∃𝑆𝑡𝑖 : 𝐴𝑢 ∈ 𝐼𝑛(𝑆𝑡𝑖 ) ∧ 𝐴𝑣 ∈ 𝑂𝑢𝑡 (𝑆𝑡𝑖 ).

𝐺𝑆 is a directed graph, where vertices represent arrays accessed
by a program, and edges represent data dependencies between
them. Two arrays 𝐴𝑢 and 𝐴𝑣 are connected if there is a statement
that accesses 𝐴𝑢 and computes 𝐴𝑣. Each edge is annotated with
the corresponding access function vector of the statement that
generates it.

Example 7. Consider the example in Figure 2. We have two state-
ments 𝑆𝑡1 and 𝑆𝑡2, with In(𝑆𝑡1) = {𝐴, 𝐵}, Out(𝑆𝑡1) = {𝐶}, In(𝑆𝑡2) =
{𝐶, 𝐷, 𝐸}, Out(𝑆𝑡2) = {𝐸}. We then construct the SDG 𝐺𝑆 = (𝑉𝑆, 𝐸𝑆 ),
with 𝑉𝑆 = In(𝑆𝑡1) ∪ Out(𝑆𝑡1) ∪ In(𝑆𝑡2) ∪ Out(𝑆𝑡2) = {𝐴, 𝐵, 𝐶, 𝐷, 𝐸}.
Furthermore, we have edges 𝐸𝑆 = {(𝐴, 𝐶), (𝐵, 𝐶), (𝐶, 𝐸), (𝐷, 𝐸), (𝐸, 𝐸)}.
The edges are annotated with the corresponding access function vectors
𝝓𝑆𝑡 1,1, . . . , 𝝓𝑆𝑡 2,3.

Note: While the “explicit” program CDAG 𝐺 = (𝑉 , 𝐸), where
every vertex represents a single computation is indeed acyclic,
the SDG 𝐺𝑆 = (𝑉𝑆, 𝐸𝑆 ) may contain self-edges when a statement
updates the loaded array ((𝐸, 𝐸) in the example above). In 𝐺, one

Tight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

vertex corresponds to one version of a single array element, while
in 𝐺𝑆 , one vertex encapsulates all versions of all array elements.

6.1 SDG Subgraphs
Denote 𝐼 ⊂ 𝑉𝑆 set of input vertices of 𝐺𝑆 (∀𝐴 ∈ 𝐼 : indegree(𝐴) =
0). Let 𝐻 ⊂ 𝑉𝑆 \ 𝐼 be a subset of the vertices of SDG 𝐺𝑆 = (𝑉𝑆, 𝐸𝑆 ).
The SDG subgraph 𝐺𝑆 [𝐻 ] is a subgraph of 𝐺𝑆 induced by the
vertex set 𝐻 . It corresponds to some subcomputation in which
at least one vertex from each array in 𝐻 was computed. We now
use the analogous strategy to the 𝑋 -Partitioning abstraction: since
the optimal pebbling has an associated 𝑋 -partition with certain
properties (the dominator set constraint), we bound the cost of
any pebbling by finding the maximum subcomputation among all
valid 𝑋 -partitions. We now show that every subcomputation in
the optimal 𝑋 -partition has a corresponding SDG subgraph 𝐺𝑆 [𝐻 ].
Therefore, finding 𝐺𝑆 [𝐻𝑜𝑝𝑡 ] that maximizes the computational
intensity among all subgraphs bounds the size of the maximal sub-
computation (which, in turn, bounds the I/O cost of any pebbling).
Recall that an optimal pebbling 𝑃 has an associated 𝑋 -partition
P (𝑋 ), where each H ∈ P (𝑋 ) represents a sequence of operations
that are not interleaved with other subcomputations. Given 𝐺𝑆 ,
each H ∈ P (𝑋 ) has an associated subgraph 𝐺𝑆 [𝐻 ] s.t. every array
vertex 𝐴𝑖 ∈ 𝐻 represents an array from which at least one vertex
was computed in H .

Note that both the pebbling 𝑃 and the partition P (𝑋 ) depend on
the size of the CDAG that is determined by the sizes of the iteration
domains 𝐷𝑖 . However, the SDG does not depend on them. Thus, by
finding the subgraph that maximizes the computational intensity,
we bound 𝜌 for any combination of input parameters.

Definition 6. The subgraph SOAP statement 𝑆𝑡𝐻 of subgraph
𝐺𝑆 [𝐻 ] is a single SOAP statement with the input 𝐼𝑛(𝑆𝑡𝐻 ) = {𝐴 :
𝐴 ∉ 𝐻 ∧ ∃𝐵 ∈ 𝐻 : (𝐴, 𝐵) ∈ 𝐸𝑆 }. Additionally, for each vertex 𝐵 ∈ 𝐻
that is not computed in 𝐻 , that is (cid:154)𝐴 ∈ 𝐻 : (𝐴, 𝐵) ∈ 𝐸𝑆 , self-edges
(𝐵, 𝐵) ∈ 𝐸 are preserved (𝐵 ∈ 𝐼𝑛(𝑆𝑡𝐻 )).

Intuition. T he subgraph statement 𝑆𝑡𝐻 is a “virtual” SOAP state-
ment that encapsulates multiple statements 𝑆𝑡1, . . . , 𝑆𝑡𝑘 . Given 𝐻 ,
its subgraph statement’s inputs 𝐼𝑛(𝑆𝑡𝐻 ) are formed by merging
inputs (cid:208)𝑘
𝐼𝑛(𝑆𝑡𝑖 ) \ 𝑉 (𝐻 ) from all statements that form 𝐻 , but
are not in 𝐻 . By the construction of the SDG, this is equivalent to
the definition above: take all vertices 𝐴 ∈ 𝑉𝑠 \ 𝑉 (𝐻 ) that have a
child in 𝑉 (𝐻 ), that is ∃𝐵 ∈ 𝑉 (𝐻 ) : (𝐴, 𝐵) ∈ 𝐸𝑆 (see Figure 2).

𝑖=1

This forms the lower bound on the number of inputs for a
corresponding subcomputation H : all the vertices from arrays
𝐴𝑖 ∈ 𝑉 (𝐻 ) could potentially be computed during H and do not
need to be loaded, but at least vertices from arrays 𝐼𝑛(𝑆𝑡𝐻 ) have to
be accessed.

Example 8. Consider again the example from Figure 2. The set
of input nodes is 𝐼 = {𝐴, 𝐵, 𝐷 }. There are three possible subgraph
statements: 𝐻1 = {𝐶}, with 𝐼𝑛(𝑆𝑡𝐻1 ) = {𝐴, 𝐵}, 𝐻2 = {𝐶} with
𝐼𝑛(𝑆𝑡𝐻2 ) = {𝐶, 𝐷, 𝐸}, and 𝐻3 = {𝐶, 𝐸} with 𝐼𝑛(𝑆𝑡𝐻3 ) = {𝐴, 𝐵, 𝐷 }.
Note that by definition, the self-edge (𝐶, 𝐶) is preserved in 𝐻2, but
not in 𝐻3. Subgraphs 𝐻1 and 𝐻2 correspond to the input statements
𝑆𝑡1 and 𝑆𝑡2. Subgraph 𝐻3 encapsulates a subcomputation H that
computes some vertices from both arrays 𝐶 and 𝐸, merging subcom-
putations 𝑆𝑡1 and 𝑆𝑡2 and reusing outputs from 𝑆𝑡1 to compute 𝐸.

9

Then, we establish the following lemma:

Lemma 5. Given an 𝑋 -partition P (𝑋 ) = {H1, . . . , H𝑠 } of the 𝑘-
statement SOAP, with its corresponding 𝐺𝑆 = (𝑉𝑆, 𝐸𝑆 ), each subcom-
putation H has an associated intensity 𝜌 H =
|𝐷𝑜𝑚𝑚𝑖𝑛 ( H) |−𝑆 that
is upper-bounded by the computational intensity of the subgraph
statement 𝑆𝑡𝐻 (Lemma 4).

|H |

Proof. Recall that given the subcomputation H , its correspond-
ing SDG subgraph 𝐻 is constructed as follows: for each vertex
𝑣 ∈ 𝑉 computed during H belonging to some array 𝐴𝑖 , add the
corresponding array vertex 𝑠𝑖 to 𝐻 . Note that we allow a vertex
recomputation: if some vertex is (re)computed during the optimal
schedule of H , its array vertex will belong to 𝐻 .

Observe that by this construction and by definition of the sub-
graph statement, all arrays from which at least one vertex is loaded
during H are in 𝐼𝑛(𝑆𝑡𝐻 ). Furthermore, 𝐼𝑛(𝑆𝑡𝐻 ) is a subset of these
arrays: during H , there might be some loaded vertex from array
𝐴𝑗 ∈ 𝐻 , but, by definition of 𝑆𝑡𝐻 , this array will not be in 𝐼𝑛(𝑆𝑡𝐻 ).
Therefore, 𝑆𝑡𝐻 lower bounds the input size of H .

The last step of the proof is to observe that by Lemma 4, the
computational intensity of 𝑆𝑡𝐻 bounds the maximum number of
computed vertices for any H ′ ∈ P (𝑋 ) that belong to 𝐻 , that is,
the union of all arrays in 𝐻 . But since all vertices that are com-
puted in H belong to one of these arrays, H cannot have higher
□
computational intensity.

6.2 SDG I/O Lower Bounds
We now proceed to establish a method to derive the I/O lower
bounds of the multi-statement SOAP given its SDG 𝐺𝑆 = (𝑉𝑆, 𝐸𝑆 ).
For each array vertex 𝐴 ∈ 𝑉𝑆 , denote |𝐴| as the total number of
vertices in the CDAG that belong to array 𝐴. Denote further S(𝐴)
the set of all subgraphs of 𝐺𝑆 that contain 𝐴. Then we prove the
following theorem:

Theorem 1. The I/O cost 𝑄 of a 𝑘-statement SOAP represented by
the SDG 𝐺𝑆 = (𝑉𝑆, 𝐸𝑆 ) is bounded by

𝑄 ≥

∑︁

𝐴∈𝑉𝑆

|𝐴|
max𝐻 ∈S (𝐴) 𝜌𝐻

(10)

where max𝐻 ∈S (𝐴) 𝜌𝐻 is the maximum computational intensity over
all subgraph statements of subgraphs 𝐻 that contain vertex 𝐴.

Proof. This theorem is a direct consequence of Lemma 5 and
the fact that all vertices in CDAG 𝐺 are associated with some array
vertex in SDG 𝐺𝑆 . Lemma 5, together with the definition of S(𝑎),
states that max𝐻 ∈S (𝑎) 𝜌𝐻 is the upper bound on any subcompu-
tation H that contains any vertex from array 𝑎. Since there are
|𝑎| vertices associated with 𝑎, at least
I/O operations
must be performed to compute these vertices. Since the computa-
tional intensity expresses the average cost per vertex, even if some
subcomputation in an optimal 𝑋 -partition spans more than one
array, this is already modeled by the set S(𝑎). Therefore, we can
□
sum the I/O costs per arrays 𝑎, yielding inequality 10.

|𝑎 |
max𝐻 ∈S (𝑎) 𝜌𝐻

Note that applying Theorem 1 requires iterating over all possible
subgraphs. In the worst case, this yields exponential complexity,
prohibiting scaling our method to large programs. However, many
scientific applications contain a limited number of kernels with

Technical Report, 2021,

simple dependencies. In practice we observed that our approach
scales well to programs containing up to 35 statements.

7 EVALUATION
We evaluate our lower bound analysis on a wide range of applica-
tions, ranging from fundamental computational kernels and solvers
to full workloads in hydrodynamics, numerical weather prediction,
and deep learning. The set of applications covers both the previously
analyzed kernels (the Polybench suite [28], direct convolution), and
kernels that were never analyzed before due to complicated depen-
dency structures (multiple NN layers, diffusion, advection). Not
only our tool covers broader class of programs than state-of-the-art
approaches, but also it improves bounds generated by methods
dedicated to specific narrower classes [19]. Improving I/O lower
bounds has not only theoretical implications: loose bounds may
not be applicable for generating corresponding parallel codes, as
too many overapproximations may yield an invalid schedule.

In our experiments we use DaCe [29] to extract SOAP statements
from Python and C code, and use MATLAB for symbolic analysis.
Polybench. As our first case study, we analyze Polybench [28],
a polyhedral application benchmark suite composed of 30 pro-
grams from several domains, including linear algebra kernels, linear
solvers, data mining, and computational biology. Prior best results
were obtained by IOLB [19], a tool specifically designed for analyz-
ing I/O lower bounds of affine programs. We summarize the results
in Table 2, listing the leading order term for brevity.

We find that SOAP analysis derives tight I/O lower bounds for all
Polybench kernels. Analyzing these programs as multi-statement
SOAP either reproduces existing tight bounds, or improves them by
constant factors (e.g., in Cholesky decomposition) on 14 out of 30
applications (Table 2). Of particular note is adi (Alternating Direc-
tion Implicit solver). Our algorithm detected a possible tiling in the
time dimension, yielding the lower bound (12𝑁 2𝑇 )/
𝑆, compared
to 𝑁 2𝑇 reported by Olivry et al. [19]. However, due to dependency
chains incurred by alternating directions, such tiling may violate
loop-carried dependency constraints, which our algorithm relaxes.
A parallel machine could potentially take advantage of this tiling
scheme, possibly providing super-linear communication reduction.
However, this is outside of the scope of this paper.
Neural Networks. Analyzing I/O lower bounds of neural net-
works is a nascent field, and so far only single-layer convolution
was analyzed [20, 23]. We improve the previously-reported bound
reported by Zhang et al. [20] by a factor of 8.

√

7.1 New Lower Bounds
Analyzing SOAP and the SDG representation enables capturing
complex data dependencies in programs with a large number of
statements. To demonstrate this, we study larger programs in three
fields, where no previous I/O bounds are known. If an application
contains both SOAP and data-dependent kernels, we find a SOAP
representation that bounds the access sizes from below.
LULESH. The Livermore Unstructured Lagrangian Explicit Shock
Hydrodynamics (LULESH) [30] application is an unstructured phy-
sics simulation. We analyze the main computational kernel, totaling
over 60% of runtime within one time-step of the simulation from

10

G. Kwasniewski et al.

Improv.
over SotA

12√
𝑆

SOAP I/O Bound
12𝑁 2𝑇
√
𝑆
𝑀𝑁
𝑀𝑁
𝑁 3
√
𝑆
3
𝑀 2𝑁
√
𝑆
𝑀 2𝑁
√
𝑆
3𝐻𝑊
2𝑁 2

𝑃 𝑁𝑄 𝑁𝑅
√
𝑆

3𝑁𝑋 𝑁𝑌 𝑇
√
𝑆

3𝑁 2
2
√

2

2𝑁 3
√
𝑆
2𝑁 2
√
𝑆
𝑁 2
2𝑁 2
𝑀𝑁 2
√
𝑆
6𝑁 3𝑇
3√
𝑆
2𝑁𝑇
𝑆
4𝑁 2𝑇
√
𝑆
4𝑁 3
√
𝑆
6𝑁 3
√
𝑆
2𝑁 3
√
𝑆
3
2𝑁 3
√
𝑆
3
𝑁 2
𝑁 3
√
𝑆
3
4𝑁 2𝑇
√
𝑆
2𝑀 2𝑁
√
𝑆
2𝑀𝑁 2
√
𝑆
𝑀𝑁 2
√
𝑆
𝑁 2
2
𝑀 2𝑁
√
𝑆

]
9
1
[
h
c
n
e
b
y
l
o
P

Kernel

adi

atax
bicg
cholesky

correlation

covariance

deriche

doitgen

durbin

fdtd2d

floyd-warshall

gemm

gemver
gesummv
gramschmidt

heat3d

jacobi1d
jacobi2d

2mm

3mm

lu

ludcmp

mvt
nussinov

seidel2d

symm

syr2k

syrk

trisolv
trmm

s Direct conv.
k
r
o
w
t
e
N

Softmax
MLP

l
a
r
u
e
N

LeNet-5

BERT Encoder

2𝐶𝑖𝑛𝐶𝑜𝑢𝑡 𝐻𝑜𝑢𝑡 𝑁𝑊𝑜𝑢𝑡𝑊𝑘𝑒𝑟 𝐻𝑘𝑒𝑟
√
𝑆

4𝐵𝐻𝑀𝑁
2𝑁 (𝑓 𝑐1 𝑓 𝑐2+𝑓 𝑐1𝑖𝑛𝑝+𝑓 𝑐2𝑜𝑢𝑡 )
√
𝑆

√

300

2𝐶𝐻 𝑁𝑊
√
𝑆
4 𝐵 𝐻 𝑃 𝐿 (𝐿+2 𝐻 𝑃 )
√
𝑆

1
1
2

2

2

3

1

3

6

2

1

1
1
1

√

6

32
3 3√
3
8
√
6

3

√

3

1

1

1

1

1
2

6

1

2

2

1
1

8

—
—

—

—

—
—
—

s LULESH
u
o
i
r
a
V

horizontal diff.
vertical adv.

22 · numElem
2𝐼 𝐽 𝐾
5𝐼 𝐽 𝐾

Table 2: Simplified leading-order terms of the I/O lower bounds ex-
tracted from multi-statement SOAP and previous state-of-the-art.
For the direct convolution layer, the best previously known bound
was published by Zhang et al. [20].

Tight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

the full C++ source code. As LULESH falls outside the purview of
affine programs, this result is the first reported I/O lower bound.
Numerical Weather Prediction. We select two benchmark sten-
cil applications from the COSMO Weather Model [31] — horizontal
diffusion and vertical advection — representatives of the two major
workload types in the model’s dynamical core.
Deep Neural Networks. For deep learning, we choose both in-
dividual representative operators (Convolution and Softmax) and
network-scale benchmarks. Previous approaches only study data
movement empirically [32]. To the best of our knowledge, we are
the first to obtain I/O lower bounds for full networks, including a
Multi-Layer Perceptron (MLP), the LeNet-5 CNN [33], and a BERT
Transformer encoder [34].

8 RELATED WORK
I/O analysis spans almost the entire history of general-purpose com-
puter architectures, and graph pebbling abstractions were among
the first methods to model memory requirements. Dating back to
challenges with the register allocation problem [35], pebbles were
also used to prove space-time tradeoffs [36] and maximum par-
allel speedups by investigating circuit depths [37]. Arguably the
most influential pebbling abstraction work is the red-blue pebble
game by Hong and Kung [8] that explicitly models load and store
operations in a two-level-deep memory hierarchy. This work was
extended numerous times, by: adding blocked access [38], multiple
memory hierarchies [25], or introducing additional pebbles to allow
CDAG compositions [18]. Demaine and Liu proved that finding the
optimal pebbling in a standard and no-deletion red-blue pebble
game is PSPACE-complete [17]. Papp and Wattenhofer introduced
a game variant with a non-zero computation cost and investigated
pebbling approximation algorithms [39].

Although the importance of data movement minimization is be-
yond doubt, the general solution for arbitrary algorithms is still an
open problem. Therefore, many works were dedicated to investigate
lower bounds only for single algorithms (often with accompanying
implementations), like matrix-matrix multiplication [16, 40–42],
LU [41] and Cholesky decompositions [43, 44]. Ballard et al. [45]
present an extensive collection of linear algebra algorithms. More-
over, a large body of work exists for minimizing communication in
irregular algorithms [46, 47], such as Betweenness Centrality [5],
min cuts [48], BFS [49], matchings [50], vertex similarity coeffi-
cients [51], or general graph computations [52, 53, 53]. Many of
them use linear algebra based formulations [54]. Recently, convolu-
tion networks gained high attention. The first asymptotic I/O lower
bound for single-layer direct convolution was proved by Demmel
et al. [23]. Chen et al. [55] propose a matching implementation, and
Zhang et al. [20] present the first non-asymptotic I/O lower bound
for Winograd convolution.

In parallel with the development of I/O minimizing implemen-
tations for particular algorithms, several works investigated I/O
lower bounds for whole classes of programs. Christ et al. [7] use a
discrete version of Loomis-Whitney inequality to derive asymptotic
lower bounds for single-statement programs nested in affine loops.
Demmel and Rusciano [22] extended this work and use discrete
Hölder-Brascamp-Lieb inequalities to find optimal tilings for such
programs. The polyhedral model [56] is widely used in practice
by many compilers [57, 58]. However, polyhedral methods have

their own limitations: 1) they cannot capture non-affine loops [59];
2) while the representation of a program is polynomial, finding
optimal transformations is still NP-hard [60]; 3) they are inappli-
cable for many neural network architectures, e.g., the Winograd
algorithm for convolution [20].

Recently, Olivry et al. [19] presented IOLB — a tool for automatic
derivation of non-parametric I/O lower bounds for programs that
can be modeled by the polyhedral framework. IOLB employs both
“geometric” projection-based bounds based on the HBL inequal-
ity [21], as well as the wavefront-based approach from Elango [61].
To the best of our knowledge, this is the only method that can
handle multiple-statement programs. However, the IOLB model
explicitly disallows recomputation that may be used to decrease the
I/O cost, e.g., in the Winograd convolution algorithm, backpropaga-
tion, or vertical advection. Furthermore, the framework is strictly
limited to affine access programs. Even then, our method is able to
improve those bounds by up to a factor of 6
6 (fdtd2d) using a
single, general method without the need to use application-specific
techniques, such as wavefront-based reasoning.

√

9 CONCLUSIONS
In this work we introduce SOAP — a broad class of statically ana-
lyzable programs. Using the explicit assumptions on the allowed
overlap between arrays, we are able to precisely count the number
of accessed vertices on the induced parametric CDAG. This stands
in contrast with many state-of-the art approaches that are based on
bounding projection sizes, as they need to underapproximate their
union size, often resulting in a significant slack in constant factors
of their bounds. Our single method is able to reproduce or improve
existing lower bounds for many important scientific kernels from
various domains, ranging from 2× increase in the lower bound
for linear algebra (cholesky, syrk), to more than 10× for stencil
applications (fdtd2d, heat3d).

Our SDG abstraction precisely models data dependencies in
multiple-statement programs. It directly captures input and output
reuse, and allows data recomputation. Armed with these tools, we
are the first to establish I/O lower bounds for entire neural networks,
as well as core components of the popular Transformer architecture.
We believe that our work will be further extended to handle data-
dependent accesses (e.g., sparse matrices), as well as scale better
with input program size. The derived maximum subcomputation
sizes can guide compiler optimizations and development of new
communication-optimal algorithms through tiling, parallelization,
or loop fusion transformations.

10 ACKNOWLEDGEMENTS
This project received funding from the European Research Council
(ERC) under the European Union’s Horizon 2020 programme (grant
agreement DAPP, No. 678880). Tal Ben-Nun is supported by the
Swiss National Science Foundation (Ambizione Project #185778).
The authors wish to thank the Swiss National Supercomputing
Center (CSCS) for providing computing infrastructure and support.

REFERENCES
[1] D. Unat, A. Dubey, T. Hoefler, J. Shalf, M. Abraham, M. Bianco, B. L. Chamberlain,
R. Cledat, H. C. Edwards, H. Finkel, K. Fuerlinger, F. Hannig, E. Jeannot, A. Kamil,
J. Keasler, P. H. J. Kelly, V. Leung, H. Ltaief, N. Maruyama, C. J. Newburn, ,

11

Technical Report, 2021,

G. Kwasniewski et al.

and M. Pericas, “Trends in Data Locality Abstractions for HPC Systems,” IEEE
Transactions on Parallel and Distributed Systems (TPDS), vol. 28, no. 10, Oct. 2017.
[2] G. Kestor, R. Gioiosa, D. J. Kerbyson, and A. Hoisie, “Quantifying the energy cost
of data movement in scientific applications,” in 2013 IEEE international symposium
on workload characterization (IISWC).

IEEE, 2013, pp. 56–65.

[3] A. Tate, A. Kamil, A. Dubey, A. Größlinger, B. Chamberlain, B. Goglin, C. Edwards,
C. J. Newburn, D. Padua, D. Unat et al., “Programming abstractions for data
locality.” PADAL Workshop 2014.

[4] D. Unat, A. Dubey, T. Hoefler, J. Shalf, M. Abraham, M. Bianco, B. L. Chamber-
lain, R. Cledat, H. C. Edwards, H. Finkel, K. Fuerlinger, F. Hannig, E. Jeannot,
A. Kamil, J. Keasler, P. H. J. Kelly, V. Leung, H. Ltaief, N. Maruyama, C. J. New-
burn, and M. Pericás, “Trends in data locality abstractions for hpc systems,” IEEE
Transactions on Parallel and Distributed Systems, pp. 3007–3020, 2017.

[5] E. Solomonik, M. Besta, F. Vella, and T. Hoefler, “Scaling Betweenness Centrality
using Communication-Efficient Sparse Matrix Multiplication,” in SC, 2017.
[6] E. Solomonik, E. Carson, N. Knight, and J. Demmel, “Trade-offs between syn-
chronization, communication, and computation in parallel linear algebra compu-
tations,” ACM Transactions on Parallel Computing (TOPC), vol. 3, no. 1, pp. 1–47,
2017.

[7] M. Christ, J. Demmel, N. Knight, T. Scanlon, and K. Yelick, “Communication lower
bounds and optimal algorithms for programs that reference arrays–part 1,” arXiv
preprint arXiv:1308.0068, 2013.

[8] J. Hong and H. Kung, “I/O complexity: The red-blue pebble game,” in STOC, 1981,

pp. 326–333.

[9] M. Del Ben et al., “Enabling simulation at the fifth rung of DFT: Large scale RPA
calculations with excellent time to solution,” Comp. Phys. Comm., pp. 120–129,
2015.

[10] Q. Zheng and J. D. Lafferty, “Convergence analysis for rectangular matrix com-

pletion using burer-monteiro factorization and gradient descent,” CoRR, 2016.

[11] T. Ben-Nun and T. Hoefler, “Demystifying parallel and distributed deep learning:
An in-depth concurrency analysis,” ACM Comput. Surv., vol. 52, no. 4, 2019.

[12] C. D. Meyer, Matrix analysis and applied linear algebra.
[13] A. Krishnamoorthy and D. Menon, “Matrix inversion using Cholesky decom-
position,” in 2013 signal processing: Algorithms, architectures, arrangements, and
applications (SPA).

IEEE, 2013, pp. 70–72.

SIAM, 2000.

[14] E. Solomonik, D. Matthews, J. R. Hammond, J. F. Stanton, and J. Demmel, “A mas-
sively parallel tensor contraction framework for coupled-cluster computations,”
Journal of Parallel and Distributed Computing, vol. 74, no. 12, pp. 3176–3190, 2014.
[15] V. Elango, F. Rastello, L.-N. Pouchet, J. Ramanujam, and P. Sadayappan, “On
characterizing the data access complexity of programs,” in Proceedings of the
42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, ser. POPL ’15. New York, NY, USA: ACM, 2015.

[16] G. Kwasniewski, M. Kabić, M. Besta, J. VandeVondele, R. Solcà, and T. Hoefler,
“Red-Blue Pebbling Revisited: Near Optimal Parallel Matrix-Matrix Multiplication,”
in Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis (SC19), Nov. 2019.

[17] E. D. Demaine and Q. C. Liu, “Red-blue pebble game: Complexity of computing
the trade-off between cache size and memory transfers,” in Proceedings of the 30th
on Symposium on Parallelism in Algorithms and Architectures, 2018, pp. 195–204.
[18] V. Elango et al., “Data access complexity: The red/blue pebble game revisited,”

Tech. Rep., 2013.

[19] A. Olivry, J. Langou, L.-N. Pouchet, P. Sadayappan, and F. Rastello, “Automated
derivation of parametric data movement lower bounds for affine programs,” in
Proceedings of the 41st ACM SIGPLAN Conference on Programming Language
Design and Implementation, 2020, pp. 808–822.

[20] X. Zhang, J. Xiao, and G. Tan, “I/O lower bounds for auto-tuning of convolutions

in CNNs,” 2020.

[21] M. Christ, J. Demmel, N. Knight, T. Scanlon, and K. Yelick, “Communication lower
bounds and optimal algorithms for programs that reference arrays–part 1,” arXiv
preprint arXiv:1308.0068, 2013.

[22] J. Demmel and A. Rusciano, “Parallelepipeds obtaining HBL lower bounds,” arXiv

preprint arXiv:1611.05944, 2016.

[23] J. Demmel and G. Dinh, “Communication-optimal convolutional neural nets,”

arXiv preprint arXiv:1802.06905, 2018.

[24] G. Dinh and J. Demmel, “Communication-optimal tilings for projective nested

loops with arbitrary bounds,” arXiv preprint arXiv:2003.00119, 2020.

[25] J. E. Savage, “Extending the hong-kung model to memory hierarchies,” in Inter-
Springer, 1995, pp. 270–281.
[26] Q. Liu, “Red-blue and standard pebble games : Complexity and applications in

national Computing and Combinatorics Conference.

the sequential and parallel models,” 2018.

[27] G. Kwasniewski, T. Ben-Nun, A. N. Ziogas, T. Schneider, M. Besta, and T. Hoe-
fler, “On the parallel I/O optimality of linear algebra kernels: Near-optimal LU
factorization,” 2020.

[28] L. N. Pouchet, “PolyBench: The Polyhedral Benchmark suite,” 2016. [Online].

Available: https://sourceforge.net/projects/polybench

[29] T. Ben-Nun, J. de Fine Licht, A. N. Ziogas, T. Schneider, and T. Hoefler, “Stateful
dataflow multigraphs: A data-centric model for performance portability on het-
erogeneous architectures,” in Proceedings of the International Conference for High

12

Performance Computing, Networking, Storage and Analysis, ser. SC ’19, 2019.
[30] J. Keasler and USDOE, “Livermore unstructured lagrange explicit shock
hydrodynamics,” 9 2010. [Online]. Available: https://www.osti.gov//servlets/purl/
1231396

[31] M. Baldauf, A. Seifert, J. Förstner, D. Majewski, and M. Raschendorfer, “Opera-
tional convective-scale numerical weather prediction with the COSMO model:
Description and sensitivities.” Monthly Weather Review, 139:3387–3905, 2011.
[32] A. Ivanov, N. Dryden, T. Ben-Nun, S. Li, and T. Hoefler, “Data movement is all

you need: A case study on optimizing transformers,” 2020.

[33] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied
to document recognition,” in Proceedings of the IEEE, 1998, pp. 2278–2324.
[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,

and I. Polosukhin, “Attention is all you need,” 2017.

[35] R. Sethi, “Complete register allocation problems,” in STOC, 1973.
[36] W. J. Paul and R. E. Tarjan, “Time-space trade-offs in a pebble game,” Acta Infor-

matica, vol. 10, no. 2, pp. 111–115, Jun 1978.

[37] P. W. Dymond and M. Tompa, “Speedups of deterministic machines by synchro-
nous parallel machines,” Journal of Computer and System Sciences, vol. 30, no. 2,
pp. 149–161, 1985.

[38] A. Aggarwal and S. Vitter, Jeffrey, “The input/output complexity of sorting and

related problems,” CACM, Sep. 1988.

[39] P. A. Papp and R. Wattenhofer, “On the hardness of red-blue pebble games,”
in Proceedings of the 32nd ACM Symposium on Parallelism in Algorithms and
Architectures, 2020, pp. 419–429.

[40] D. Irony, S. Toledo, and A. Tiskin, “Communication lower bounds for distributed-
memory matrix multiplication,” Journal of Parallel and Distributed Computing,
vol. 64, no. 9, pp. 1017 – 1026, 2004.

[41] E. Solomonik and J. Demmel, “Communication-optimal parallel 2.5D matrix
multiplication and LU factorization algorithms,” in Euro-Par 2011 Parallel
Processing, ser. Lecture Notes in Computer Science, E. Jeannot, R. Namyst, and
J. Roman, Eds. Springer Berlin Heidelberg, 2011, vol. 6853, pp. 90–109. [Online].
Available: http://dx.doi.org/10.1007/978-3-642-23397-5_10

[42] J. Demmel et al., “Communication-optimal parallel recursive rectangular matrix

multiplication,” in IPDPS, 2013, pp. 261–272.

[43] G. Ballard, J. Demmel, O. Holtz, and O. Schwartz, “Communication-optimal
parallel and sequential Cholesky decomposition,” SIAM Journal on Scientific
Computing, vol. 32, no. 6, pp. 3495–3523, 2010.

[44] E. Hutter and E. Solomonik, “Communication-avoiding Cholesky-QR2 for rect-
angular matrices,” in 2019 IEEE International Parallel and Distributed Processing
Symposium (IPDPS).

IEEE, 2019, pp. 89–100.

[45] G. Ballard, E. Carson, J. Demmel, M. Hoemmen, N. Knight, and O. Schwartz,
“Communication lower bounds and optimal algorithms for numerical linear
algebra,” Acta Numerica, vol. 23, p. 1, 2014.

[46] M. Besta and T. Hoefler, “Accelerating irregular computations with hardware
transactional memory and active messages,” in Proceedings of the 24th Interna-
tional Symposium on High-Performance Parallel and Distributed Computing, 2015,
pp. 161–172.

[47] S. Sakr, A. Bonifati, H. Voigt, A. Iosup, K. Ammar, R. Angles, W. Aref, M. Arenas,
M. Besta, P. A. Boncz et al., “The future is big graphs! a community view on
graph processing systems,” arXiv preprint arXiv:2012.06171, 2020.

[48] L. Gianinazzi, P. Kalvoda, A. De Palma, M. Besta, and T. Hoefler, “Communication-
avoiding parallel minimum cuts and connected components,” ACM SIGPLAN
Notices, vol. 53, no. 1, pp. 219–232, 2018.

[49] M. Besta, F. Marending, E. Solomonik, and T. Hoefler, “Slimsell: A vectorizable

graph representation for breadth-first search,” in IPDPS, 2017.

[50] M. Besta, M. Fischer, T. Ben-Nun, D. Stanojevic, J. D. F. Licht, and T. Hoefler,
“Substream-centric maximum matchings on fpga,” ACM Transactions on Reconfig-
urable Technology and Systems (TRETS), vol. 13, no. 2, pp. 1–33, 2020.

[51] M. Besta, R. Kanakagiri, H. Mustafa, M. Karasikov, G. Rätsch, T. Hoefler, and
E. Solomonik, “Communication-efficient jaccard similarity for high-performance
distributed genome comparisons,” in 2020 IEEE International Parallel and Dis-
tributed Processing Symposium (IPDPS).

IEEE, 2020, pp. 1122–1132.

[52] M. Besta, M. Podstawski, L. Groner, E. Solomonik, and T. Hoefler, “To push or to
pull: On reducing communication and synchronization in graph computations,”
in Proceedings of the 26th International Symposium on High-Performance Parallel
and Distributed Computing, 2017, pp. 93–104.

[53] M. Besta, Z. Vonarburg-Shmaria, Y. Schaffner, L. Schwarz, G. Kwasniewski, L. Gi-
aninazzi, J. Beranek, K. Janda, T. Holenstein, S. Leisinger et al., “Graphminesuite:
Enabling high-performance and programmable graph mining algorithms with
set algebra,” arXiv preprint arXiv:2103.03653, 2021.

[54] J. Kepner et al., “Mathematical foundations of the GraphBLAS,” arXiv:1606.05790,

2016.

[55] X. Chen, Y. Han, and Y. Wang, “Communication lower bound in convolution ac-
celerators,” in 2020 IEEE International Symposium on High Performance Computer
Architecture (HPCA).

IEEE, 2020, pp. 529–541.

[56] U. Bondhugula, M. Baskaran, S. Krishnamoorthy, J. Ramanujam, A. Rountev,
and P. Sadayappan, Automatic Transformations for Communication-Minimized
Berlin,
Parallelization and Locality Optimization in the Polyhedral Model.

Tight I/O Bounds of Statically Analyzable Programs

Technical Report, 2021,

Heidelberg: Springer Berlin Heidelberg, 2008, pp. 132–146. [Online]. Available:
https://doi.org/10.1007/978-3-540-78791-4_9

[57] “Automatic transformations for communication-minimized parallelization and
locality optimization in the polyhedral model,” in International Conference on
Compiler Construction (ETAPS CC), Apr. 2008.

[58] T. Grosser, A. Groesslinger, and C. Lengauer, “Polly—performing polyhedral
optimizations on a low-level intermediate representation,” Parallel Processing
Letters, vol. 22, no. 04, p. 1250010, 2012.

[59] T. Hoefler and G. Kwasniewski, “Automatic complexity analysis of explicitly
parallel programs,” in Proceedings of the 26th ACM symposium on Parallelism in
algorithms and architectures, 2014, pp. 226–235.

[60] A. Darte, “On the complexity of loop fusion,” in PACT, 1999, pp. 149–157.
[61] V. Elango, F. Rastello, L.-N. Pouchet, J. Ramanujam, and P. Sadayappan, “On
characterizing the data movement complexity of computational DAGs for parallel
execution,” in Proceedings of the 26th ACM Symposium on Parallelism in Algorithms
and Architectures, 2014, pp. 296–306.

13

