Neural Programming by Example

Chengxun Shu
Beihang University
Beijing 100191, China
shuchengxun@163.com

Hongyu Zhang
The University of Newcastle
Callaghan, NSW 2308, Australia
hongyu.zhang@newcastle.edu.au

7
1
0
2

r
a

M
5
1

]
I

A
.
s
c
[

1
v
0
9
9
4
0
.
3
0
7
1
:
v
i
X
r
a

Abstract

Programming by Example (PBE) targets at automatically in-
ferring a computer program for accomplishing a certain task
from sample input and output. In this paper, we propose a
deep neural networks (DNN) based PBE model called Neu-
ral Programming by Example (NPBE), which can learn from
input-output strings and induce programs that solve the string
manipulation problems. Our NPBE model has four neural
network based components: a string encoder, an input-output
analyzer, a program generator, and a symbol selector. We
demonstrate the effectiveness of NPBE by training it end-to-
end to solve some common string manipulation problems in
spreadsheet systems. The results show that our model can in-
duce string manipulation programs effectively. Our work is
one step towards teaching DNN to generate computer pro-
grams.

Introduction
Programming by Example (PBE, also called programming
by demonstration, or inductive synthesis) (Lieberman 2001;
Cypher and Halbert 1993; Gulwani 2011) gives machines
the ability to reason and generate new programs without
substantial amount of human supervision. In PBE systems,
users (often non-professional programmers) provide a ma-
chine with input-output examples of a task they would like to
perform and the machine automatically infers a program to
accomplish the task. The concept of PBE has been success-
fully used for string manipulation in spreadsheet systems
such as Microsoft Excel (Gulwani et al. 2015), computer-
aided education (Gulwani 2014) and data extracting systems
(Le and Gulwani 2014).

As an example, if a user provides the following input and

output examples:

john@example.com ⇒ john
james@company.com ⇒ james

A PBE system should understand that the user would like to
extract the user name from the email address. It will auto-
matically synthesize a program Select(Split(x, @), 0), where
x is the input string, Split is to split a string according to a
delimiter, and Select is to select a substring from an array

Copyright c(cid:13) 2017, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

of strings. Given a new email address jacob@test.com, the
program will output the string jacob.

Lau et al. (2003) applied version space algebra to search
for possible programs. More recent PBE methods (Gulwani
2011) mainly adopt search technique to ﬁnd a composition
of predeﬁned functions (such as string Split and Concatena-
tion) that satisﬁes the input-output examples. These methods
create a Directed Acyclic Graph (DAG) and search through
the sequences of functions that can generate the output string
from a given input state. These methods can generate com-
plex string manipulation programs effectively, but require
the design of complex program synthesis algorithms.

In this paper, we propose a Deep Neural Networks (DNN)
based approach to Programming by Example. We train neu-
ral networks to automatically infer programs from input-
output examples. During the past few years, research on
DNN has achieved signiﬁcant results in a variety of ﬁelds
such as computer vision (Krizhevsky, Sutskever, and Hin-
ton 2012), speech recognition (Mohamed, Dahl, and Hin-
ton 2012), natural language processing (Collobert et al.
2011), and API learning (?). Recently, researchers have
explored the feasibility of applying DNN to solve pro-
gramming and computation related problems (Neelakan-
tan, Le, and Sutskever 2015; Reed and de Freitas 2015;
Graves, Wayne, and Danihelka 2014; ?). Our work is based
on the similar idea of applying DNN to infer and execute
computer programs. Different from the existing work, we
target at the PBE problem and train a neural PBE model with
triples of input, output and program.

Our approach, called NPBE (Neural Programming by Ex-
ample), teaches DNN to compose a set of predeﬁned atomic
operations for string manipulation. Given an input-output
string pair, the NPBE model is trained to synthesize a pro-
gram - a sequence of functions and corresponding arguments
that transform the input string to the output string. The pro-
gram is generated from the atomic functions and one func-
tion may use the execution results of previous functions.
Thus the model is able to compose complex programs us-
ing only several predeﬁned operations.

We have experimentally evaluated NPBE on a large num-
ber of input-output strings for 45 string manipulation tasks
and the results are encouraging. We ﬁnd that our model can
generalize beyond the training input/output strings and the
training argument settings. Our work is one of the early at-

 
 
 
 
 
 
tempts to apply DNN to the problem of Programming by
Example.

Related Work
Reed et al. (2015) developed a framework called Neural
Programmer-Interpreter to induce and execute programs us-
ing neural networks. Their method treats programs as em-
beddings and uses neural networks to generate functions and
arguments for program execution. Their model is trained
with the supervision of execution traces and can be used
in many different scenarios. However, their model cannot
be directly applied to the problem of PBE as the input to
their model is the environment encoded with the task, while
our work is dedicated to PBE and the input to our model
are input-output examples. Neural Programmer (Neelakan-
tan, Le, and Sutskever 2015) is a neural network augmented
with a set of operations that can be called over several steps.
It is trained to output the result of program execution, while
our model is trained to output the program represented by
symbols. Neural Enquirer (Yin et al. 2015) is a fully neural,
end-to-end differentiable model capable of modeling and ex-
ecuting table query related programs. The execution of Neu-
ral Enquirer is “softly” on data table using neural networks,
while our work does not apply soft execution to input-output
strings. Boˇsnjak et al. (?) proposed a neural implementation
of an abstract machine for the language Forth. It can learn
program behaviour trained from input-output data. However,
it requires program sketches as input.

Our model is also related to the work that uses recur-
rent neural networks to solve programming and computa-
tion related problems. Graves et al. (2014) developed Neu-
ral Turing Machine which is capable of learning and exe-
cuting simple programs using an external memory. Zaremba
et al. (2015) used execution traces to train recurrent neu-
ral networks to learn simple algorithms. Ling et al. (2016)
developed a model to generate program code from natu-
ral language and structured speciﬁcation. Pointer Networks
(Vinyals, Fortunato, and Jaitly 2015) uses an attentional re-
current model to solve difﬁcult algorithmic problems.

Some machine learning methods were also proposed to
tackle PBE problems. Lau et al. (2003) applied version
space algebra to efﬁciently search for possible programs.
Given input-output pairs, genetic programming (Banzhaf et
al. 1998) can evolve useful programs from candidate popu-
lations. Liang et al. (2010) proposed a hierarchical Bayesian
approach to learn simple programs given only a few exam-
ples. Melon et al. (2013) used machine learning to speed
up the searching for possible programs by learning weights
related to textual features. Their method needs carefully de-
signed features to reduce search space, while our method re-
duces search space and avoids feature engineering through
learning representations using DNN.

The NPBE Model
Problem Statement: Let S denote the set of strings. For an
input string x ∈ S and an output string y ∈ S, our model
is to induce a function f ∈ SS, so that y = f(x). The input
to our model is the input-output pair z := (x, y) ∈ S2, and

we wish to get the correct function f, so when the user input
another string ¯x ∈ S, we can generate the desired output
¯y = f(¯x) without the need of user specifying the function
explicitly.

Figure 1: Overall architecture of NPBE.

The proposed NPBE model (Figure 1) consists of four

modules:

• A string encoder to encode input and output strings;

• An input-output analyzer which generates the transforma-
tion embedding describing the relationship between input
and output strings;

• A program generator which produces
tion/arguments embeddings over a few steps;

the

func-

• A symbol selector to decode the function/arguments em-

beddings and generate human readable symbols.

The program generator will run for a few steps, each step
it may access the outputs of previous steps, thus enables
the model to compose powerful programs using only a few
predeﬁned atomic functions. While traditional deep learning
models try to generate the output y given x and eventually ﬁt
the function f such that y = f(x), our model learns to gener-
ate the function f directly and ﬁts the higher-order function
g for which f = g(x, y) and f satisﬁes y = f(x).

Program GeneratorInput/output Analyzer••••••String EncoderString Encoderinput  string        embeddingoutput string embeddinginput characters embeddingoutput characters embeddingtransformation embeddingSymbol Predictort=1,…,Tfunctargt,5argt,4argt,3argt,2argt,1arguments embeddingfunction embeddinghistory embeddinginput stringoutput stringString Encoder
Given a string composed of a sequence of characters
{i1, i2, ..., iL}, the string encoder outputs a string embed-
ding and a list of character embeddings. First each charac-
ter ik in the sequence is mapped to the 8-dimensional raw
embeddings e(ik) via a randomly initialized and trainable
embedding matrix. To better present and attend to a char-
acter, the context of each character is fused with the char-
acter’s raw embedding to build the character embedding.
Let l(ik) denote the left context of the character ik and
r(ik) as the right context of ik. l(ik) and r(ik) are respec-
tively calculated using Equation (1) and Equation (2) with
l(0) = r(L + 1) = [0]C. In our implementation f l and f r
are the update function of LSTM (?).

l(ik) = f l(l(ik−1), e(ik))
r(ik) = f r(r(ik+1), e(ik))

(1)
(2)

The character embedding for each character is the combi-
nation of left/right contexts of character ik and e(ik) itself
as shown in Equation (3), where [·; ·] means the concatena-
tion of vectors, max is the element-wise max-pooling. We
and be are parameter matrix and vector for building the full
character embedding.

ck = tanh(We[max(l(ik), r(ik)); e(ik)] + be)

(3)

ck will be used by the attention mechanism in the program
generator.

We also need a representation which can summarize the
whole string, so we can induce the transformation from the
string embeddings of input and output strings. We use mul-
tilayer bidirectional LSTM (?) to summarize ck. The output
of forward and backward LSTM at each layer are concate-
nated and become the input of next layer’s forward and back-
ward LSTM. The topmost layer’s last hidden states of for-
ward and backward LSTM are merged to generate the string
embedding s ∈ RS through a ﬁnal fully connected layer.
The processing for input and output strings is separated but
shares the same neural network architecture and parameters,
thus producings cI,1, ...cI,L, sI and cO,1, ...cO,L, sO for in-
put and output strings, respectively.

Input/output Analyzer
The input/output analyzer converts the input and output
string embeddings to the transformation embedding, which
describes the relationship between the input and output
strings. Let t ∈ RT denote the transformation embedding.
sI ∈ RS, sO ∈ RS are the input and output string embed-
dings, respectively. The input/output analyzer can be repre-
sented as Equation (4). In our implementation, f IO is just a
2-layer fully connected neural network with activation func-
tion tanh.

t = f IO([sI ; sO])

(4)

Figure 2: Diagram of the program generator.

embeddings step by step. The function’s embedding at time
t is calculated as Equation (5), which is a fully connected
neural network taking as input the transformation embed-
ding t and the execution history of the program generator
ht−1 ∈ RH (h0 = t). Similarly, the function’s arguments
embedding at time t is calculated as Equation (6). However,
the function’s arguments are often very complex and hard
to predict accurately. So attention mechanism (similar to the
one used in neural machine translation models (?)) is ap-
plied to reﬁne the raw arguments embedding ar,t with at-
tention on input and output strings. This is summarized in
Equation (7). Finally, the function embedding ft, the reﬁned
arguments embedding at, and the previous history embed-
ding ht−1 are merged into the new history embedding ht as
shown in Equation (8). Wh and bh are parameter matrix and
vector for generating the new history, respectively.

ft = f f unc(t, ht−1)
ar,t = f args(t, ht−1)

at = f att(ar,t, cI,1, ...cI,L, cO,1, ...cO,L)
ht = tanh(Wh[ft; at; ht−1] + bh)

(5)
(6)
(7)
(8)

Program Generator
The program generator (Figure 2) is the core of NPBE. It
generates several functions and corresponding arguments’

Functions f f unc and f args can be multilayer fully connected
neural networks, and in our experiment we just use one
layer neural network. The function f att in Equation (7) is
implemented by attending to the input and output strings as
follows:

Arguments Generator••••••Function GeneratorHistory Generatorinput characters embeddingoutput characters embeddingtransformation embeddinghistory embeddingfunction embeddingarguments embeddingFunction Name Arguments
Split
Join
Select
ToLower
ToUpper
Concatenate
GetConstString
NoFunc

(String str, Char delim)
(String[] strs, Char delim)
(String[] strs, Int index)
(String str)
(String str)
(String str or Char c, ...)
(null)
(null)

Return Type Description
String[]
String
String
String
String
String
String
null

Split string using delim as a delimiter, return an array of strings
Join an array of strings using delim as a delimiter, return a string
Select an element of a string array, return a string
Turn a string str into lower case
Turn a string str into upper case
Concatenate a sequence of strings or characters into a string
Special function indicating a constant string
Special symbol indicating no more function is needed

I tanh(W1cI,i + W2ar,t)

ut
I,i = vT
I,i = softmax(ut
at
L
(cid:88)

at
I,icI,i

aI,t =

I,i)

Table 1: Atomic functions.

i ∈ (1, ..., L)

(9)

i ∈ (1, ..., L) (10)

Constant Type
Delimiter

Integer
Special Symbol

Constant
“(space)”, “(newline)”, “,”, “.”, “\”,
“@”, “:”, “;”, “ ”, “=”, “-”, “/”
0, 1, 2, 3, -1, -2, -3
x, o1, o2, o3, o4, N oArg

(11)

Table 2: Constant symbols of our model.

i=1
O,i = vT
ut
O tanh(W3cO,i + W4ar,t)
O,i = softmax(ut
at
L
(cid:88)

O,i)

aO,t =

at
O,icO,i

i ∈ (1, ..., L) (12)

i ∈ (1, ..., L) (13)

(14)

i=1

W1, W2, vI and W3, W4, vO are parameters for attending to
the input and output strings, respectively. Note that the at-
tention architecture for input and output strings are the same
but with different parameters. The ﬁnal arguments embed-
ding is generated by combining attention over input, output
and the raw arguments embedding:

at = tanh(Watt[aI,t; ar,t; aO,t] + batt)

(15)

where Watt and batt are parameters for combining.

Symbol Selector
The symbol selector uses the function embedding ft and
the arguments embedding at generated by the program
generator to select a proper function and the correspond-
ing arguments of that function. The probability distribution
αf unc,t ∈ [0, 1]P over P atomic functions is produced by
Equation (16), where Uf ∈ RP ×F is the matrix storing the
representations of the atomic functions. The arguments em-
bedding at (representing the summary of the arguments) is
decoded by a RNN (?), which is conditioned on its previ-
ous output and at, as shown in Equation (17). st,0, ...st,i−1
are hidden states of LSTM with st,0 = [0]H . In this way,
the sequence of arguments is generated by the RNN. The
probability distribution of the i-th argument at time t over
Q possible arguments, αarg,i,t ∈ [0, 1]Q, is produced using
Equation (18), where Ua ∈ RQ×A is the matrix storing the
representations of possible arguments.

αf unc,t = softmax(Uf ft)

at,i = LSTM(st,i−1, at,i−1, at)

αarg,i,t = softmax(Uaat,i)

(16)
(17)
(18)

Training
We train the NPBE model end-to-end using input-output
string pairs {Xi, Yi} as well as the programs represented as
a sequence of functions and corresponding arguments. Each
program Pi : {f 1
i } can transform Xi to Yi,
i , aT
where i means the i’th training example. For every input-
output pair we can generate a sequence of T functions and
every function has an argument list a ∈ AM , where A is
the set of possible arguments, M is the maximum number
of arguments one function can take. In our implementation,
T = 5, M = 5.

i , ..., f T

i , a1

The training is conducted by directly maximizing the log-

likelihood of the correct program P given {X , Y }:

θ∗ = argmax

(cid:88)

log P (P|X , Y ; θ)

(19)

θ

(X ,Y ,P)

where θ is the parameters of our model. Random Gaus-
sian noise (?) is injected into the transformation embedding
and the arguments embedding to improve the generalization
ability and stability of NPBE.

Experiments

Experimental Design
The NPBE model is required to induce a program consist-
ing of a sequence of functions based on only one input-
output string pair. In this section, we describe our evaluation
of the NPBE model. In our experiments, we deﬁne 7 ba-
sic string manipulation functions and 1 null function as the
atomic functions (Table 1). For simplicity, each program is
allowed to be composed by 5 functions at most. We deﬁne a
set of constant symbols (Table 2) from which our model can
choose as arguments. The constant symbols include integers
and delimiters. The integers are used by the Select function
as index to an array of strings. The negative integers are used
to access array elements from the tail. The design of integer
symbols supports the access to an array of at most 7 ele-
ments. The delimiters are used by Split and Join to split a

Function

t
Input-output Pair
Input string:
1 GetConstString
john@company.com 2 GetConstString
Output string:
Hello john, have fun!

Argument 1 Argument 2 Argument 3 Argument 4 Argument 5
—
—
—
—
o2
—
—
—
—
o2
—
—
—
—

—
—
“@”
0
o4
“/”
0
1
—
“-”
“/”
-1
“.”
0

—
—
x
o3
o1
x
o1
o1
o3
o4
x
o1
o2
o3

—
—
—
—
—
—
—
—
—
—
—
—
—
—

—
—
—
—
—
—
—
—
—
—
—
—
—
—

3
4
5
1
2
3
4
5
1
2
3
4

Split
Select
Concatenate
Split
Select
Select
ToUpper
Concatenate
Split
Select
Split
Select

Input string:
17/apr/2016
Output string:
APR-17

Input string:
/home/foo/ﬁle.cpp
Output string:
ﬁle

Table 3: Examples of input-output pairs for NPBE. The corresponding programs (from top to bottom) are: 1) Concate-
nate(“Hello ”, Select(Split(x, “@”), 0), “, have fun!”); 2) Concatenate(ToUpper(Select(Split(x, “/”), 1)), “-”, Select(Split(x,
“/”), 0)); and 3) Select(Split(Select(Split(x, “/”), -1), “.”), 0). Note that GetConstString is a placeholder, and the ﬁrst one is
evaluated to “Hello ”, the second one is evaluated to “, have fun!”.

string or join an array of strings by a delimiter. We also de-
ﬁne some special symbols. For example, the symbol x refers
to the input string, o1, o2, o3 and o4 are used to refer to the
output of the ﬁrst, second, third and fourth operation, re-
spectively. The N oArg symbol indicates that no arguments
is expected at the current position. Note that although in our
experiments, we give some constraints to the functions and
arguments in a program, our model can be easily extended
to support new functions and arguments.

To obtain training data, we ﬁrst generate programs at vari-
ous levels of complexity according to 45 predeﬁned tasks. A
task is a sequence of function in a speciﬁc order, but the
arguments of each function are not ﬁxed. The reason for
deﬁning tasks is that we want the program generated by our
model being syntactically correct and meaningful. The 45
tasks range from simple ones such as the concatenation of
input to some constant string, to more complex ones com-
prising Split, Join, Select, ToUpper and Concatenate func-
tions. For example, the task of Split, Join is to ﬁrst split the
input string by a delimiter, then join the split strings array
using another delimiter. A program derived from this task
could be Join(Split(x,“/”), “:”), which splits the input string
x according to the delimiter “/” and then joins the resulting
substrings using the delimeter “:”. The average number of
functions for accomplishing a task is 3.5.

For all the tasks, we generate a total number of around
69,000 programs for training. For each program Pi we gen-
erate a random input string Xi, which should be an valid
input to the program Pi. Next, we apply the program Pi
on Xi by actually running the Python program implement-
ing Pi and get the output string Yi. We constrain Xi and Yi
to be at most 62 characters long. After that, we use {Xi, Yi}
as the input data to our model, and Pi as the training target.
The program is generated in such a way that if there are mul-
tiple programs that can result in the same Yi given Xi, only
one speciﬁc program is chosen. Therefore, there is no am-
biguity for the model to predict the desired program. Given

the program Pi, the input string Xi is always generated
dynamically and randomly to decrease overﬁtting. Table 3
gives some concrete input-output examples for our model.

To train NPBE, we choose RMSProp (Tieleman and Hin-
ton 2012) as the optimizer and set the mini-batch size to 200.
We set the dimensionality of the transformation embedding t
and the history embedding h to 256, the function embedding
f to 16, and the arguments embedding a to 64. The actual
training process relies on an adaptive curriculum (Reed and
de Freitas 2015) in which the frequency of one speciﬁc task
being trained is proportional to its error rates over test. Every
10 epochs we estimate the prediction errors. We use softmax
with adequate temperature over error rates to sample the fre-
quency of each task that will be trained during the next 10
epochs. So tasks with the higher error rates will be sampled
more frequently than tasks with the lower error rates during
the next 10 epochs.

The evaluation of NPBE is conducted to answer the fol-

lowing research questions:

RQ1: What is the accuracy of NPBE in generating pro-
grams? In this RQ, we use randomly generated input-
output strings to evaluate the accuracy of NPBE in gener-
ating programs. For example, given a random input-output
pair: 25/11/16 and 25:11:16 (which does not appear in the
training data), we would like to test if the correct program
Join(Split(x, “/”), “:”) can be still generated. To answer this
RQ, we generate random input-output strings for each task
1000 times and apply the trained NPBE model. A program
produced by NPBE is regarded correct only if the model
predicts all the ﬁve functions (if less than ﬁve, padded with
the N oF unc symbol) and all the arguments of the functions
(also padded with the N oArg symbol) correctly (thus a total
of 5 + 5 × 5 = 30 positions). We also compare our model
with the RNN encoder-decoder models (?) implemented us-
ing LSTM and LSTM with attention mechanism, which all
have similar total number of parameters to our model.

Task
Case Change
Duplicate Input String
Case Change and Concatenate with Input String
Concatenate with Constant
Split, Select, Case Change
Split, Join, Select, Case Change, Concatenate
GetConstString, Split, Select, Case Change, Concatenate
GetConstString×2, Split, Select, Concatenate
Split, Select, Select, Concatenate
Split, Select, Select, Case Change, Concatenate
Split, Select, Split, Select
Average over All 45 Tasks

LSTM LSTM-A NPBE-Top1 NPBE-Top3 NPBE-Top5
100.0% 100.0%
9.4%
9.4%
83.1%
8.7%
0.1%
2.9%
0.1%
0.1%
0.1%
0.1%
20.4%

100.0%
99.1%
99.9%
88.4%
97.3%
97.8%
86.7%
68.1%
82.0%
94.0%
72.2%
91.0%

100.0%
99.1%
99.9%
88.4%
92.3%
92.4%
81.2%
24.7%
9.8%
35.8%
31.2%
74.1%

100.0%
99.1%
99.9%
88.4%
96.4%
96.6%
85.6%
54.4%
46.4%
73.2%
64.3%
85.8%

9.8%
9.3%
67.5%
12.8%
0.1%
5.2%
0.1%
0.1%
0.2%
0.1%
22.0%

Table 4: Results of generating programs.

Task
Split, Join
GetConstString, Split, Join, Concatenate
Split, Join, Concatenate
Split, Join, Select, Concatenate
Split, Select, Select, Concatenate
Average over 19 Tasks

Unseen
Seen
93.6% 93.9%
91.4% 91.5%
95.0% 94.9%
90.8% 89.5%
82.0% 82.1%
87.6% 87.4%

Table 5: Results with seen and unseen program arguments.

RQ2: Can NPBE generate programs with previously un-
seen arguments?
In this RQ, we test the generalization
ability of NPBE. We evaluate our model using programs
whose argument settings do not appear in the training set.
For example, if the program Join(Split(x, “/”), “:”) appears
in the training set, we would like to know if NPBE can work
for the program Join(Split(x, “@”), “-”) , which does not
appear in the training set. To answer this RQ, we design a
test set consisting of around 19,000 programs with previ-
ously unseen arguments. The experiment is conducted on
19 selected tasks that have complex argument combinations
(for simple tasks there are few arguments to choose so we
skip them).

Experimental Results
RQ1: What is the accuracy of NPBE in generating pro-
grams? Table 4 gives the evaluation results of NPBE on
predicting programs. The average Top1 accuracy achieved
by NPBE is 74.1%, which means that for 74.1% of input-
output pairs in test, NPBE successfully generates the corre-
sponding program. We found that the model prediction er-
rors most likely to occur on the integer argument of Select
because neural networks are not good at counting. So we
also let the model to give 3 or 5 predictions when it tries to
predict the integer arguments of Select. The average Top3
and Top5 accuracy results are 85.8% and 91.0%, which
means that for 85.8% and 91.0% of input-output pairs in
test, NPBE successfully returns the corresponding program
within the top 3 and top 5 results respectively. The results
show that the NPBE model can generate correct programs
for most tasks.

As an example, given the input string “17/apr/2016” and
output string “APR-17”, our model needs to induce a pro-

gram comprising Split, Select, Select, Case Change, Con-
catenate. For this task, our model gives completely correct
program in 35.8% cases. If we allow the model to give 3 or 5
predictions for the integer argument of Select, the accuracy
is increased to 73.2% or 94.0%.

The results about LSTM and LSTM with attention mech-
anism (denoted to as LSTM-A) are also shown in Table 4.
Note that the Top1, Top3 and Top5 accuracy for LSTM and
LSTM-A are almost the same, so only the Top1 accuracy is
reported. We found that the ordinary encoder-decoder model
can solve the simplest tasks but cannot tackle harder tasks.
The results show that NPBE signiﬁcantly outperforms the
ordinary encoder-decoder model.

RQ2: Can NPBE generate programs with previously
unseen arguments? We test the generalization ability of
NPBE on the programs with previously unseen argument
settings. The average Top5 accuracy results are given in Ta-
ble 5. The results shows that for seen and unseen argument
settings the accuracies achieved by our model have no big
difference. For example, the task of Split, Join ﬁrst splits an
input string by a delimiter (such as “/”) and then concate-
nates the resulting substrings by the other delimiter (such as
“:”). This task achieves 93.6% accuracy on the training set.
For different arguments (e.g., ﬁrst split the input string by
“@” then join the resulting substrings by “-”) that do not
exist in the training set, NPBE can still get 93.9% accuracy.
The results show that NPBE can be generalized to unseen
program arguments without over-ﬁtting to particular argu-
ment combinations.

Discussions and Future Work
The intention behind NPBE is to make the model learn re-
lated features from input-output strings automatically and
use the learned features to induce correct programs. The pur-
pose of this paper is not to directly compete with the exist-
ing PBE systems. Instead, we show that the use of DNN can
recognize features in string transformations and can learn
accurate programs through input-output pairs.

Currently, NPBE cannot be generalized to completely un-
seen tasks (such as Split, Join, Join, Concatenate) that never
appeared in the training set. In our future work, we will try
to build the model that really “understands” the meaning of

atomic functions to make it possible to generalize to the un-
seen tasks.

Conclusion
In this paper, we propose NPBE, a Programming by Exam-
ple (PBE) model based on DNN. NPBE can induce string
manipulation programs based on simple input-output pairs
by inferring a composition of functions and corresponding
arguments. We have shown that the novel use of DNN can
be successfully applied to develop Programming By Exam-
ple systems. Our work also explores the way of learning
higher-order functions in deep learning, and is one step to-
wards teaching DNN to generate computer programs.

References
[Banzhaf et al. 1998] Banzhaf, W.; Nordin, P.; Keller, R. E.;
and Francone, F. D. 1998. Genetic programming: an intro-
duction. Morgan Kaufmann Publishers San Francisco.
[Collobert et al. 2011] Collobert, R.; Weston, J.; Bottou, L.;
Karlen, M.; Kavukcuoglu, K.; and Kuksa, P. 2011. Natu-
ral language processing (almost) from scratch. Journal of
Machine Learning Research 12(Aug):2493–2537.
[Cypher and Halbert 1993] Cypher, A., and Halbert, D. C.
1993. Watch what I do: programming by demonstration.
MIT press.
[Graves, Wayne, and Danihelka 2014] Graves, A.; Wayne,
G.; and Danihelka, I. 2014. Neural turing machines. arXiv
preprint arXiv:1410.5401.
[Grefenstette et al. 2015] Grefenstette, E.; Hermann, K. M.;
Suleyman, M.; and Blunsom, P. 2015. Learning to transduce
with unbounded memory. In Advances in Neural Informa-
tion Processing Systems, 1828–1836.
[Gulwani et al. 2015] Gulwani, S.; Hernandez-Orallo, J.;
Kitzelmann, E.; Muggleton, S. H.; Schmid, U.; and Zorn, B.
2015. Inductive programming meets the real world. Com-
munications of the ACM 58(11):90–99.
[Gulwani 2011] Gulwani, S. 2011. Automating string pro-
In
cessing in spreadsheets using input-output examples.
ACM SIGPLAN Notices, volume 46, 317–330. ACM.
[Gulwani 2014] Gulwani, S. 2014. Example-based learning
in computer-aided stem education. Communications of the
ACM 57(8):70–80.
[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.;
Sutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances
in neural information processing systems, 1097–1105.
[Lake et al. 2016] Lake, B. M.; Ullman, T. D.; Tenenbaum,
Building ma-
J. B.; and Gershman, S. J.
2016.
arXiv preprint
chines that learn and think like people.
arXiv:1604.00289.

[Lau et al. 2003] Lau, T.; Wolfman, S. A.; Domingos, P.; and
Weld, D. S. 2003. Programming by demonstration using
version space algebra. Machine Learning 53(1-2):111–156.
[Le and Gulwani 2014] Le, V., and Gulwani, S.
2014.
Flashextract: a framework for data extraction by examples.
In ACM SIGPLAN Notices, volume 49, 542–553. ACM.
[Liang, Jordan, and Klein 2010] Liang, P.; Jordan, M. I.; and
Klein, D. 2010. Learning programs: A hierarchical bayesian
approach. In Proceedings of the 27th International Confer-
ence on Machine Learning (ICML-10), 639–646.
[Lieberman 2001] Lieberman, H. 2001. Your wish is my
command: Programming by example. Morgan Kaufmann.
[Ling et al. 2016] Ling, W.; Grefenstette, E.; Hermann,
K. M.; Kocisky, T.; Senior, A.; Wang, F.; and Blunsom, P.
2016. Latent predictor networks for code generation. arXiv
preprint arXiv:1603.06744.
[Menon et al. 2013] Menon, A. K.; Tamuz, O.; Gulwani, S.;
Lampson, B. W.; and Kalai, A. 2013. A machine learning
framework for programming by example. ICML (1) 28:187–
195.
[Mohamed, Dahl, and Hinton 2012] Mohamed, A.-r.; Dahl,
G. E.; and Hinton, G. 2012. Acoustic modeling using deep
belief networks. IEEE Transactions on Audio, Speech, and
Language Processing 20(1):14–22.
[Neelakantan, Le, and Sutskever 2015] Neelakantan, A.; Le,
Q. V.; and Sutskever, I. 2015. Neural programmer: Induc-
ing latent programs with gradient descent. arXiv preprint
arXiv:1511.04834.
[Reed and de Freitas 2015] Reed, S., and de Freitas, N.
arXiv preprint
2015. Neural programmer-interpreters.
arXiv:1511.06279.
[Sukhbaatar et al. 2015] Sukhbaatar, S.; Weston, J.; Fergus,
R.; et al. 2015. End-to-end memory networks. In Advances
in neural information processing systems, 2440–2448.
[Tieleman and Hinton 2012] Tieleman, T., and Hinton, G.
2012. Lecture 6.5-rmsprop: Divide the gradient by a run-
ning average of its recent magnitude. COURSERA: Neural
Networks for Machine Learning 4(2).
[Vinyals, Fortunato, and Jaitly 2015] Vinyals, O.; Fortunato,
M.; and Jaitly, N. 2015. Pointer networks. In Advances in
Neural Information Processing Systems, 2692–2700.
[Yin et al. 2015] Yin, P.; Lu, Z.; Li, H.; and Kao, B. 2015.
Neural enquirer: Learning to query tables with natural. arXiv
preprint arXiv:1512.00965.
[Zaremba et al. 2015] Zaremba, W.; Mikolov, T.; Joulin, A.;
and Fergus, R. 2015. Learning simple algorithms from ex-
amples. arXiv preprint arXiv:1511.07275.

