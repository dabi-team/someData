2
2
0
2

p
e
S
1
2

]
E
S
.
s
c
[

2
v
8
6
8
0
1
.
2
0
2
2
:
v
i
X
r
a

Neural Program Repair : Systems, Challenges and Solutions

Wenkang Zhong
State Key Laboratory for Novel Software Technology,
Nanjing University
Nanjing, Jiangsu, China
dg21320011@smail.nju.edu.cn

Chuanyi Li∗
State Key Laboratory for Novel Software Technology,
Nanjing University
Nanjing, Jiangsu, China
lcy@nju.edu.cn

Jidong Ge
State Key Laboratory for Novel Software Technology,
Nanjing University
Nanjing, Jiangsu, China
gjd@nju.edu.cn

Bin Luo
State Key Laboratory for Novel Software Technology,
Nanjing University
Nanjing, Jiangsu, China
luobin@nju.edu.cn

ABSTRACT
Automated Program Repair (APR) aims to automatically fix bugs
in the source code. Recently, with advances in Deep Learning (DL)
field, there has been an increase of Neural Program Repair (NPR)
studies that use neural networks to model the patch-generation
process. NPR approaches have a significant benefit in applicabil-
ity over prior APR techniques because they do not require any
specifications (e.g., a test suite) when generating patches. For this
reason, NPR has recently become a popular research topic. In this
paper, We undertake a literature review of latest NPR systems to
help interested readers understand advancements in this emerging
field. We begin by introducing background information of NPR.
Next, to make the various NPR systems more understandable, we
split them into a four-phase pipeline and discuss various design
choices for each phase. To investigate the motivations of different
design choices, We further highlight a number of challenges and
summarize corresponding solutions adopted by existing NPR sys-
tems. Finally, we suggest some intriguing directions for the future
research.

KEYWORDS
Automatic program repair, Neural networks, Software reliability

ACM Reference Format:
Wenkang Zhong, Chuanyi Li∗, Jidong Ge, and Bin Luo. . Neural Program
Repair : Systems, Challenges and Solutions . In Proceedings of
(Confer-
ence acronym ’XX). ACM, New York, NY, USA, 11 pages. https://doi.org/
XXXXXXX.XXXXXXX

1 INTRODUCTION
Since debugging is a costly but necessary activity to ensure soft-
ware quality [4], Automated Program Repair (APR) [33], which

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference acronym ’XX, ,
© Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX

Figure 1: An example of NPR. The buggy statement is high-
lighted in red dotted box and the repaired statement in
green.

aims to automatically fix bugs without human intervention, has be-
come an important research topic in both software engineering and
artificial intelligence communities. In the last decade, the majority
of popular APR approaches are test-suite-based ones[7, 17, 21, 30,
31, 38, 51, 53, 54, 59], in which test cases of the buggy program are
adopted as specifications to guide the repair process. During the
bug-fixing process, candidate patches are continuously generated
and applied to the buggy program until the patched program satis-
fies the expected behavior described by the specification. However,
the applicability of test-suite-based APR systems is limited in prac-
tice, as test suites are difficult to come by and their quality is often
inadequate for use as a repair specification [58].

Recently, researchers start paying more attention to Neural Pro-
gram Repair (NPR) approaches [3, 6, 8, 9, 13–15, 18, 19, 26, 29, 32, 46–
48, 55–57, 62] since they have shown great potential on automati-
cally generating patches without test suites. The APR task is for-
mulated as a translation from defective code to correct code in the
NPR approach. Generally, the core component of a NPR system
consists of an encoder and a decoder [2], both of which contain
several layers of neural networks that can receive sequential inputs.
Take Figure 1 as an example: when repairing, the encoder first
embeds the source buggy program into a semantic context vector,
which is called Encoding. The decoder then performs a multi-step
generation of fix scheme. It calculates the probability distribution
over a target vocabulary using the context vector and previously
generated tokens as inputs at each step. This phase is called Decod-
ing. The Encoder-Decoder architecture has been applied in many
text-processing tasks, such as Machine Translation [2] and Text
Summarization [11]. Compared with text-processing tasks, NPR
systems need to deal with programming language which has many

EncoderSrcEmbedTry         to         fix         the       bug          .DecoderTgtEmbed<S>      Essaie     de      réparer     le        bugContext vectorEssaie     de      réparer     le        bug         .EncoderSrcEmbi>mlen; DecoderTgtEmbContext vectori>= mlen;          </s><s>           i>= mlen;     
 
 
 
 
 
Conference acronym ’XX, ,

Table 1: Distribution of included NPR systems along with the publication channels

Publication Channel

Studies

IEEE International Conference on Software Engineering (ICSE)
International Conference on Machine Learning (ICML)
IEEE Transactions on Software Engineering (TSE)
International Conference on Learning Representations (ICLR)
International Conference on Automated Software Engineering (ASE)
International Symposium on Software Testing and Analysis (ISSTA)
ACM International Conference on Mining Software Repository (MSR) CodeBert-finetune [32]
Association for Computational Linguistics (ACL)
ACM Transactions on Software Engineering Methodology (TOSEM)
The Association for the Advancement of Artificial Intelligence (AAAI) DeepFix [18]
Proceedings of Machine Learning Research (PMLR)
ACM Joint European Software Engineering Conference and Sympo-
sium on the Foundations of Software Engineering (ESEC/FSE)

Tang [46]
Tufano [47], CODIT [6]

BIFI [56]
Recoder [62]

CURE [22], DLFix [26], RewardRepair [57]
TFix [3], DrRepair [55]
SequenceR [9]
Hoppity [13], Vasic [48]
PatchEdits [14]
CoCoNut [29]

differences from natural language. Thus, a complete NPR procedure
usually includes two additional modules to deal with the input
and the output: Preprocessing and Patch Ranking. The first module
prepares source codes into standard inputs that neural encoders
can receive, while the second module ranks outputs generated by
the decoder to form a candidate patch set.

NPR approaches can be applied to a wider range of scenarios
from development to production cycle than test-suite-based APR
approaches because they only require aligned bug-fix program
pairs for training. Code repositories, such as GitHub, are a good
place to look for such information. However, the complexity of the
APR task and the numerous processing steps present challenges
in designing an effective NPR system. For example, when it comes
to Context Extraction (a sub-phase of the Preprocessing module), a
larger context may contain more repair ingredients on the APR
side, but it also means a longer input that is harder to learn by
neural networks, necessitating changes to the Encoder or Decoder
Architecture. To make the best decisions among various design
options, developers must have a thorough understanding of both
the Deep Learning and the APR domains, which impedes the NPR
direction’s follow-up research. But, first and foremost, there is no
NPR-specific review to wrap up design space and discuss potential
threats. Regarding the related surveys, they either focus solely on
test-based approaches [28] or provide a broad view on a limited
scope of NPR approaches from the perspective of techniques [5].
In this work, We provide a seminal overview of recent studies to
help understand important design points on NPR systems. Different
from aforementioned related reviews, We focus on detailed method-
ology of NPR systems, including (1) their various design choices
at each step of the bug-fixing process and (2) challenges that most
NPR systems face and the existing solutions they adopt. Specifically,
we go over NPR’s design space in depth, breaking down the overall
procedure of NPR systems into a series of modules. Then, we de-
tailed analyze design choices of each NPR systems on each module,
discussing such choices’ design reasons, shortcomings and poten-
tial improvement schemes. In addition, We discuss the challenges
faced by each module and the corresponding solutions, and sum up
with some generalized conclusions using existing evidences. We
believe that this way is particularly easy to understand for readers
and can facilitate the development of new NPR approaches.

The remainder of this paper is organized as follows. Section
3 introduces the preliminary knowledge about this field, such as
task formulation, datasets and evaluation metrics. Section 4 ex-
plicate various design choices of NPR systems and analyze their
performance. Then, we identify common challenges and discuss the
impact of existing solutions to these challenges in Section 5. Fur-
thermore, we conclude and discuss some future research directions
in Section 6.

2 INCLUDED STUDIES
Our goal is to provide a review of detailed design choices of NPR sys-
tems. To include a NPR system in our review, we first search four reg-
ular databases (IEEE Xplorer1, ACM digital library2, SpringerLink3
and GoogleScholar4) with keywords "neural/translation/learning
program/bug repair/fix". Then, we use an APR-related website5
and the living review of APR [34] to supplement the search re-
sults. Since we aim to focus on their concrete design choices on
methodology in this paper, we filter out irrelevant papers and NPR
researches that do not propose a new NPR system. Finally, 17 NPR
systems are included in our study. The distribution of all selected
NPR systems along with their publication channels are presented
in Table 1. Among the included NPR systems, DeepFix [18], DrRe-
pair [55] and BIFI [56] focus on compilation errors while the other
systems [3, 6, 9, 13, 14, 22, 26, 29, 32, 46–48, 57, 62] are proposed to
fix dynamic errors of programs.

3 DATASET AND METRICS
In this section, we first give a formulation of the NPR task and then
introduce available popular datasets and evaluation metrics.

Task. The goal of the APR task is to generate patches for a
buggy program automatically. A complete repair process usually
consists of three steps: Fault Localization, Patch Generation and
Patch Validation. First, the fault statement of the buggy program
will be located by fault localization techniques [52]. Second, various
patch generation models generate a number of candidate patches.
Candidates should be validated by certain protocols in order to

1http://ieeexplore.ieee.org/Xplore/
2http://portal.acm.org
3http://www.springerlink.com/
4http://scholar.google.com/
5http://program-repair.org

Neural Program Repair : Systems, Challenges and Solutions

Conference acronym ’XX, ,

Table 2: Popular datasets for NPR approaches. "TS" denotes
whether the dataset provides corresponding test suites.

Finally, we give a summary of included studies to retrieve the SOTA
systems and point out the limitation.

Datasets

Language

Source

Size

Defects4J
QuixBugs
BFP(small)
BFP(medium)
CodRep
Restricted Test

OS Projects
Java
Java, python Competition
OS Projects
Java
OS Projects
Java
OS Projects
Java
OS Projects
JavaScript

835
80
58,350
65,454
58,069
243,054

TS

(cid:33)
(cid:33)

obtain the correct patches that are acceptable to developers. The
NPR approach mainly does the work for patch generation step. For
NPR systems, inputs should be fault-localized programs (method-
level or line-level) within a single file. Their outputs are possible
patches that address the bug.

Datasets. We list widely adopted datasets in NPR research in Ta-
ble 2. Essentially, each data instance in all of these datasets contains
an aligned bug-fix pair derived from programming competition sub-
missions or Open-Source (OS) project commits. Defects4J is built
on top of the version control system used by operating systems. It
provides both buggy and fixed program versions with correspond-
ing test suites to make bugs reproducible. QuixBugs [27] consists
of 40 programs translated to both Python and Java, each with a bug
on a single line. Owing to the test suites, these two also serve as
the most popular evaluation datasets for all APR approaches. How-
ever, their sizes are relatively too small to train a learning-based
NPR approach, with Defects4J (latest version) having 835 instances
and QuixBugs having only 80 (40 of Java and 40 of Python). As a
result, several larger datasets [3, 10, 47] are constructed to allow
for the training and evaluation of NPR systems, as shown in Ta-
ble 2. Specifically, BFP [47] provides two datasets for small methods
(less than 50 tokens) and medium methods (50 to 100 tokens) ex-
tracted from commits between March 2011 and October 2017 on
GitHub, while CodRep [10] mines bugs mined by several previous
studies [25, 35, 41, 60, 61]. Similarly, the JavaScript bug-fix dataset
Restricted Text [3] is constructed from 5.5 million GitHub com-
mits. In consideration of cost on collecting test suites, these larger
datasets only provide human-written patches for validation.

Evaluation Metrics. The NPR approach usually predicts patches
with the top confidence score to form the candidate set. If a candi-
date can pass all the given test case, it is regarded as a plausible
patch. Such patches may not be accepted by developers for some
reasons (i.e., introduce other faults) so further manual check is
performed to ensure that they are correct. In this way, perfor-
mances of NPR approaches can be measured as the number of
correct\plausible patches. And when in the absence of a corre-
sponding test suite, candidates that are exactly the same with the
human-written patches can be regarded as correct. Top-K accu-
racy is the evaluation metric in this case, where K is the size of the
candidate set for each bug.

4 NPR SYSTEMS
In this section, we review the existing NPR systems from the per-
spective of their design choices. First, we introduce the overall
procedure of NPR systems in Section 4.1. Then, design choices on
each module of the procedure are explicated in Section 4.2 to 4.5.

4.1 Overall Procedure
Generally, NPR systems follow the processing progress illustrated in
Figure 2. An end-to-end repair procedure consists of the following
phases:

(1) Preprocessing. First, buggy programs are processed into se-
quential or structural forms that are acceptable by Neural Networks
(NN) during the Preprocessing phase. This phase has four sub-phases.
In the Context Extraction sub-phase, context surrounding the buggy
fragment will be extracted to build a contextual input. Next, text-
form codes need to be divided into lists of tokens through the Code
Tokenization sub-phase. There is an option for an Code Abstraction
sub-phase to simplify the input by renaming natural elements in
the source code. Additional features such like the Abstract Syn-
tax Tree (AST) will be constructed during the Feature Construction
sub-phase.

(2) Input Representing. Next, processed inputs are fed into the
encoder, which consists of several neural layers. The embedding
layers of the encoder first represent the input as vectors, with tokens
occurring in similar contexts having a similar vector representation.
The next layers perform vector-based computation to produce a
final contextual vector that contain rich semantics of the buggy
program. In a sense, the encoder represents the knowledge of the
programming language.

(3) Output Searching. Then, the decoder module performs a
multi-step fix generation, which is called decoding. The decoder
has a structure similar to the encoder. At each step, it takes the
contextual vector produced by the encoder along with previously
generated tokens as inputs and outputs the probability distribution
over the target vocabulary. The fix can be code text or edit oper-
ations on the buggy program. This phase produced all potential
patch candidates within the search space.

(4) Patch Ranking. Finally, a rank strategy is necessary to re-
duce the patch candidates set to a reasonable size, considering the
efficiency of subsequent patch validation.

Each phase of the NPR procedure can be regarded as an inde-
pendent function module and has positional design points. The
following part will explicate various design choices of existing NPR
systems on each module.

4.2 Preprocessing
4.2.1 Context Extraction. For NPR task, the unprocessed input is
a buggy file with fault location (usually at line level). Since a no-
context line-to-line repair will gain a low performance [9], most
approaches will input the buggy line along with its surrounding
context. It is important to decide the context scope. A wider con-
text may contain more repair ingredients, but also introduces noise
that degrades the performance of repair models. The commonly
used approaches to extract context can be categorised as Text-based
or AST-based.

A text-based extraction only considers context that is location-
relevant to the buggy statement. The simplest way is to take a single
buggy line which contains the buggy statement as context [14]. An-
other case extracts the error context consisting of the buggy line

Conference acronym ’XX, ,

Figure 2: The overall procedure and corresponding design space of the NPR. The left part of the figure describes the processing
pipeline and the right part categorized design choices of each module.

and the tow neighboring lines [3]. The more common approach
to extract context is AST-based. Most studies select the nearest
MethodDeclaration-type ancestor of the buggy node as a root of con-
textual AST [13, 26, 29, 46, 47, 62]. There are also some approaches
[6, 32] trying to make the model learn to fix bugs within a more
narrow context – the least common ancestor of the buggy node
and the fix node. For a widest consideration, SequenceR [9] builds
a class-level context with a length limit of 1,000 tokens. It keeps all
the instance variables and initializers in the buggy class along with
the signature of the constructor and non-buggy methods even if
they are not called in the buggy method.

Discussion and Insight. How much context is enough for NPR sys-
tems? A too long context may lead to the performance degradation
of the model. First, the context may contain codes unrelated to the
bug. Second, as known, neural network models are poor at dealing
with long inputs. Therefore, to better use the context, researchers

should focus on including much information that can be useful to
fix the bug in a context as short as possible.

4.2.2 Code Tokenization. Neural models can only receive sequen-
tial or structural formed input, so the textual source code needs to
be divided into a list of tokens first, which is called tokenization. For
NPR task, the Tokenize Type can be Standard ways that are pop-
ular in other field or being Code-aware for maintaining semantics
of the program.

For most natural languages, a tokenization way that fits human
intuition is word-piece. However, for tasks that use a large vocab-
ulary, a word-piece tokenization will decrease the performance
since neural models are pool at dealing with unseen words that
are Out-Of-Vocabulary (OOV) [36]. Thus, modern models usually
adopt a Byte-Pair-Encoding [43] tokenization, encoding rare and
unknown words as subword units to mitigate the OOV problem.
Tracing back to NPR task, BPE-based tokenization are also used in

PreprocessingInputRepresentingOutputSearchingPatch RankingContext ExtractionCode TokenizationFeature ConstructionEncoder ArchitectureDecoder ArchitectureRank StrategyAST-basedText-basedClassFunctionLineNode AncestorIdentifierLiteralContext ScopeRenaming ScopeTokenize TypeStandardCode-awareBPELexicalCamel-awareFeature ContentAST-basedSpecificError DescriptionTree PathCFG RuleStandardStructuralSpecificStandardStructuralPretrainedPristineGraph-basedTree-basedPretrainedTree-basedOutput TypeText-basedAST-basedStandardCode-awareCamel-awareProgram AnalysisSpecificPristineLSTMTransformerTree-LSTMTree-ReaderCodeGPTCodeBERTT5GNNRule-EncoderFconv-ContextCodeEditAST NodeCFG RuleEditLSTMCodeGPTT5TransformerFConvTree-LSTMTreeGenCopy-GeneratorBeam SearchCandidates RankingCode AbstractionEncodingDecodingNeural Program Repair : Systems, Challenges and Solutions

Conference acronym ’XX, ,

some NPR systems [14, 22, 32] for the reason that programming
languages use a more irregular vocabulary.

Programming need to follow predefined lexical rules. Before
execution, they need to be decomposed into individual units by
the lexical parser. Similarly, a code-aware tokenization that most
approaches [3, 6, 9, 13, 26, 46, 47, 62] take is to fed source codes
into a language-specific lexer. Besides, some common naming rule
will be considered during tokenization to reduce the number of
uncommon tokens. For instance, CoCoNut [29] separates variable
and method names with a special "CaMel" token, using camel letters,
underscores, and numbers to split a long identifier.

Discussion and Insight. Which type of tokenization is more suit-
able for the NPR task? A lexical tokenization is a safe choice to
retain the semantics of source codes. However, as mentioned above,
NPR systems must consider the OOV problem. The typical solution
to the OOV problem is to use a BPE tokenization. However, BPE will
break the semantics of source codes. For example, an identifier may
be divided into several sub-tokens when BPE is used to tokenize
source codes. Is it possible to use a lexical tokenization, meanwhile
mitigating the OOV problem? This is a valuable question to be
explored by follow-up researchers.

4.2.3 Code Abstraction. The purpose of code abstraction is to sharp
the size of the vocabulary that NPR models use. The model generates
sequential tokens by computing probability distributions over a
predefined token vocabulary. When dealing with a large vocabulary
composed of many possible output tokens, it may become inefficient
or imprecise. Code abstraction aims to simplify the input program
by renaming natural elements like identifiers and literals, and there
are different choices among the renaming scope.

The first approach [47] adopts an ID-replace strategy. At the
beginning, the source code is divided into a stream of tokens by a
lexer. Then the strategy substitutes each identifier/literal within
the tokenized stream with a unique ordinal ID. Some frequently
appeared identifiers and literals like "add" or "replace" will be re-
tained, since they represent common concepts. This abstraction way
can significantly reduce the size of the vocabulary, but introduces
some obvious drawbacks, i.e., the abstract patch that contains an
ID not appeared in the source can’t be concretized. A more modest
option is to abstract literals only [22, 29], considering that most
uncommon words in the vocabulary are brought by strings. DLFix
[26] implemented a renaming abstraction in order to increase the
chance for model to learn fix in similar scenarios. They keep the
type of variable in the new name along with the invoked method.
At present, abstract literals and infrequent identifiers in the source
code is the best practice. But it is far from perfect because such
abstraction will decrease the applicability of models to real-world
scenarios.

Discussion and Insight. According to a recent study [37], code
abstraction is an efficient trick to improve the performance of NPR
systems. Abstracted codes reduce the size of the vocabulary, which
sharps the search space of NPR systems. However, in some cases,
such abstraction will decrease the recall rates of NPR systems. For
example, abstraction of strings and numbers can make some bugs
irreparable because they are caused by wrong strings or numbers.

Feature Construction. Programs written in high-level pro-
4.2.4
gramming languages have richer information than natural language

texts. For example, a syntactically correct program can be parsed
into an Abstract Syntax Tree (AST) that reflects structural syntactic
information. Models may get additional repair ingredients or learn
to use grammar rules from such features.

Most features are constructed from an AST. Since a tree-structure
AST is not suitable for normal neural models that require sequential
inputs, an extra process for representing AST is needed. A com-
mon solution is to use the sequential traverse results of AST. In
this way, some approaches [6, 46] try to represent the AST with a
sequence of Context-Free-Grammar (CFG) rules [24]. A CFG is a
tuple 𝐺 = (𝑁 , (cid:205), 𝑃, 𝑆), where 𝑁 is a set of non-terminals, (cid:205) is a set
of terminals, 𝑃 is a set of production rules and S is the start symbol.
The AST that belongs to the language defined by 𝐺 can be parsed
by applying the appropriate derivation rules from the start symbol
𝑆. Each CFG rule is regarded as a token of the vocabulary. For a
non-traverse representation, DLFix [26] generates four ASTs for
each bug-fix pair: a tree for buggy method, a tree for fix method,
a buggy subtree for buggy nodes and a fix subtree for fix nodes.
The authors adopt a DL-based code summarization model [50] to
represent a subtree into a single vector. Besides, authors of Recoder
[62] treat the AST as a directional graph and embed it with an
adjacent matrix. The latest novel idea of feature construction is
to get the results of program analysis tools. TFix [3] uses the bug
report produced by a static bug detector as an additional feature,
which describes the location and the type of the bug.

Discussion and Insight. The purpose of constructing extra features
is to ease the learning phase of NPR system. Features such as the
AST may contain intuitive information that are useful for the system
to fix the bug. However, the concrete role of these features in fixing
bugs is still unknown. A detailed exploration of why these features
work can provide a useful guide to designing useful features for
NPR.

4.3 Input Representing
The purpose of Input Representing is to make the model understand
the semantics of buggy programs with an encoder module, which
is also called Encoding. To this aim, the input will be mapped into
a special semantic vector space by the encoder module, which is
composed of multi-layer neural networks. In this phase, to design a
proper encoder architecture is the most important. Architectures
of encoders that existing NPR approaches use can be categorized
as Standard, Struture-aware.

Standard encoders refer to those popular in the Natural Lan-
guage Processing (NLP) field. Under this category, classic neural
networks like Long Short-Term Memory (LSTM) architecture [20]
and Transformer [49] are most commonly used [6, 9, 14, 18, 47].
Another popular choice is to reuse models that have already trained
on a large corpus of codes, which is called pre-train [12]. Given the
effectiveness of language models in the NLP domain, CURE [22]
proposes to add a GPT [39] module pre-trained on software code
to a Neural Machine Translation (NMT) architecture. TFix [3] and
RewardRepair [57] leverage T5, a Transformer-based [49] model
pre-trained on NLP tasks and fine-tune it for the task of generating
code fixes. And the CodeBERT [16] model, a bimodal pre-trained
language model for both natural and programming languages, is
also used to adapt the APR task [32].

Conference acronym ’XX, ,

Structure-aware encoders are used to capture the AST-based fea-
tures. For an original tree-structure AST, DLFix [26] encodes it with
a Tree-LSTM [45] and Hoppity [13] adopts a Gated Graph Neural
Network (GGNN) [1], treating the AST as a graph. For traverse
results of ASTs, Tang [46] designs a Cross-Attention mechanism to
make full use of token information and syntax information interac-
tively. Recoder [62] uses a special encoder called Code Reader that
combines traverse results and AST-based graph through three sub
neural layers.

Specifically, CoCoNut [29] represent the buggy line and the
context separately as a second encoder, independently of the chosen
context. They believe this way can help the model learn useful
relations from the context (such as potential donor code, variables
in scope, etc.) without adding noise to the relations learned from
the buggy line.

Discussion and Insight. A standard encoding way is to make
input suitable for the encoder. On contrast, a structure-aware way
modifies the encoder to receive structural features. The former is
quite easy to implement, while the latter requires researchers to
have some domain knowledge on both the deep learning and the
program repair field.

4.4 Output Searching
4.4.1 Output Type. The output space is defined by the vocabulary
that consists of tokens. During the Output Searching phase, the
decoder will make a probable search through the output space and
calculates probability distributions over the target vocabulary. To
model the generation of patches, the output type can be either
AST-based or Text-based.

For textual generation, the most common way is to treat fixing as
a token-to-token translation [9, 22, 29, 32, 47]. At each iteration, a
textual code token will be outputted. Such design allows the decoder
to generate the patch code directly. Specifically, Edits [14] outputs
edit operations such as insert and delete on the buggy program
instead of code tokens, stemming from human’s edit operations on
fixing error text.

For AST-based generation, the model will first build an AST as
the backstone of the potential fix. At each step of generation, the
output can be a new node to expand the AST [26], or a modification
of the buggy node [13, 62]. Another option is to generate CFG
rules of the AST [6, 46] first and then convert them to concrete
programs. The AST-based way can ensure the syntactic correctness
of generated codes to some extent.

Discussion and Insight. The AST-based way models the output
as a sequence of CFG rules, thus it can ensure the output codes’
syntax correctness. However, for the same program, the number
of CFG rules can be much bigger than individual code tokens. It
means that an AST-based generation can be less precise, since the
model needs to make more predictions to form a complete AST. A
mitigation is to design a more simple form to represent the AST.

4.4.2 Decoder Architecture. The purpose of decoding is to esti-
mate the probability distribution of potential fix schemes. This is a
multi-step process. At each step, the decoder outputs a conditional
probability distribution over vocabulary, considering previously
generated tokens and the context vector generated by the encoder.

The decoder architecture represents the activity of generating
patches. They can be categorized into Standard or Structure-aware.
Similar to standard encoders, standard decoders can also be
pristine or pretrained. Standard architectures like LSTM [20] and
Transformer [49] are also popular choices for NPR systems [6, 9,
29, 47].The only change is that some pre-training models can only
be used to initialize the encoder [32].

Some structure-aware decoders are tree-based since they model
the decoding phase as a modification of the AST rather than gen-
erating textual codes. To this aim, DLFix [26] adopts a Tree-LSTM
[45] to generate a new node to replace the buggy node. And Recoder
[62] developed a syntactic edit decoder based on TreeGen [44] to
produce edit operations on the AST of buggy program.

The APR task is different from translation in that only part of
the words need to be changed. Considering this difference, some ap-
proaches [6, 9, 14] add a copy mechanism [42] to make the decoder
task-aware. The copy mechanism allows a direct copy of tokens
from the source program during decoding.

Discussion and Insight. We observe that researchers prefer to use
standard decoders. However, such decoders are designed to model
natural languages. When applied to the program repair task, they
can even not ensure the syntactic correctness of the generated codes.
Also, we notice that DLFix [26] and Recoder [62] use a structure-
aware decoder to model the AST rather than textual codes, thus
ensuring the syntactic correctness of outputs.

4.5 Patch Ranking
The number of potential patches that the decoder produced will
be 𝑆𝑙
𝑣 where 𝑆𝑣 is the size of vocabulary and 𝑙 is the max length of
output. It is unrealistic to validate every candidate since they need
be manually checked eventually. Therefore, A rank strategy is
necessary in considering that too many candidates will significantly
decrease the inference efficiency of the model and burden the effort
on patch validation.

Beam Search is a general search strategy in the DL domain, and it
is also used as a patch ranking strategy by a lot of APR approaches
[6, 9, 26, 46, 47]. For each iteration during the generation, the beam
search algorithm checks the 𝑡 most likely tokens (𝑡 corresponds to
the beam size) and ranks them by the total likelihood score of the
next 𝑠 prediction steps (𝑠 correspond to the search depth). In the
end, the top 𝑡 most likely sequences ordered by the likelihood are
kept. To filter candidates that are of low quality, Cure [22] adopts
a code-aware beam search strategy, performing a Valid-Identifier-
Check. The strategy first uses static analysis to get valid identifiers
in the source program. Then during search, the probability of the
generated identifier which is not valid in the source will be set
to −𝑖𝑛𝑓 . DLFix [26] uses a more complex patch ranking strategy.
They first derive the possible candidates with a set of program
analysis filters and then adopt a DL-based classification to re-rank
candidates.

Discussion and Insight. Using a simple probability sampling to
limit the number of candidates is not enough, since this strategy
can even not ensure the syntactic correctness of candidates. More
code-aware filters should be developed to perform a reliable rank.
For example, researchers can design a code-related penalty for the

Neural Program Repair : Systems, Challenges and Solutions

Conference acronym ’XX, ,

Table 3: A summary of performances of advanced NPR systems and their design choices on each module. Only NPR systems
that are evaluated on datasets which have been used at least twice are included. The best results for each dataset are highlighted
in bold. Accuracy at @top-50,@top-5,@top-1 on CodRep, Restricted Text and BFP(small).

CoCoNut Codit
Node-
Method
Ancestor
\

Literal

Cure
Method

DLFix
Method

Recoder
Method

Tufano
Method

SequenceR TFix
Line
Class

Tang
Method

Literal

Literal

Identifier

Identifier
+ Literal

\

\

Identifier
+Literal

Context
Scope
Renaming
Scope

Tokenize
Type
Features

Camel-
aware
Code

Encoder
Architecture

FConv-
Context

Decoder
Architecture

Output
Type
Rank
Strategy

Defects4J

QuixBugs

BFP(small)

CodRep

Restricted
Text

Fconv

Code

Beam
Search

44/85

13/20

\

\

16.40%

s
e
c
i
o
h
C
n
g
i
s
e
D

e
c
n
a
m
r
o
f
r
e
P

Lexical

Code
+Rule
biLSTM

biLSTM
+copy

Code
+Rule
Beam
Search

30/51

\

\

\

\

Camel-
aware+BPE
Code

PT-GPT
+FConv-
Context

PT-GPT
+Fconv

Code

Code-
aware

57/104

26/35

\

\

\

Lexical

Lexical

Lexical

Lexical

BPE

lexical

AST

Code+AST Code

Code

Code

Code+Rule

Tree-
LSTM

Tree-
LSTM

Node

Code+AST
+Path
Readers

Edit-
Decoder

biLSTM

biLSTM

PT-T5

biLSTM

biLSTM
+copy

PT-T5

Token+
Grammar
Emb
Grammar-
Decoder

Node Edit Code

Code

Code

Node

DL-based
CLS

Beam
Search

Beam
Search

30/65

\

\

\

\

71/x

17/17

\

\

\

\

\

9.22%

14.050%

\

Beam
Search

18/61

\

\

30.80%

23.60%

Beam
Search

Beam
Search

\

\

\

\

46.30%

\

\

11.47%

\

\

rank strategy. If a candidate contain some primary errors such as
the compilation error, the rank of it should be lower.

4.6 The State-Of-The-Art
To summary, we provide a table view of NPR systems that achieve
remarkable performance. These systems with their design choices
on each module are summarized in Table 3. Besides, We list their
performances on five datasets for comparison. Performances on
Defects4J [23] and QuixBugs [27] are represented by the number of
correct/plausible patches. The other three datasets (BFP(small)
[47], CodRep [10], [3]) only provide human-written patches for
validation so the evaluation metric is the top-k accuracy.

Among all NPR approaches, Recoder [62] creates the most com-
plex encoder that receives the code text, AST and Tree Path as inputs
all at once. It generated 71 correct patches for Defects4J (V1.2)’s
395 bugs. According to their original report, its performance is
significantly better than that of other NPR systems as well as many
test-suite-based approaches. On the java part of QuixBugs, Cure
[22] produced 26 correct patches and 35 plausible patches among
40 bugs. It enhanced the model of CoCoNut [29] with combining
GPT [39] that pre-trained on a large code corpus. On CodRep, Se-
quenceR [9] achieves a double higher accuracy with class-level
context and additional copy mechanism compared with the base-
line in the top-50 candidates. TFix [3] that uses a pretrained T5
model [40] reported they achieve a fix rate of 46.30% within top-5
candidates on the Restricted Text dataset compared with CoCoNut
[29] and SequenceR [9]. The small version of BFP only contains

bug-fix pairs that are less than 50 tokens. On this dataset, Tang
[46] perform better with grammar-based generation than vanilla
sequence-to-sequence model [47]. It has shown a top-1 accuracy of
11.47%.

5 CHALLENGES AND SOLUTIONS
In this section, we will identify the challenges on designing an
effective NPR systems and discuss existing solutions. We begin
with an overview on typical challenges on each module of the NPR
procedure in Section 5.1. Then we provide concrete discussion on
corresponding solutions and sum up with general conclusions in
Section 5.2 to 5.4.

5.1 Overview
Although some researchers [22, 29, 62] have report that they can
achieve State-Of-The-Art (SOTA) results compared to test-suite-
base APR, there is still great room for improvement. The obsta-
cles come from differences between natural and programming lan-
guages, as well as that between the program repair and the transla-
tion task. These differences bring in challenges on different parts
of approaches. We identify the main challenges as follows:

Out-Of-Vocabulary (OOV). Programming languages use an
open vocabulary, but DNN models use a pre-defined fix one. When
dealing with tokens that are out of the predefined vocabulary,
DNN’s predictions will become imprecise. Since natural elements

Conference acronym ’XX, ,

Table 4: Challenges and existing solutions for each module of the NPR system

Module

Preprocessing

Challenge
Limit use of code-
related information

the OOV problem

Input Representing

Limit use of code-
related information

Solution
Extract context arrounding the buggy code. The context can be
neighbor lines [3], buggy method [13, 22, 26, 29, 46, 47] or a
buggy class [9].
Construct features from AST [6, 13, 26, 46, 62] , or using a Static
Analyzer [3].

Simlify Identifiers [47, 62] or Literals [22, 29, 47].
Split infrequent words into frequent sub units with BPE [3, 14,
22, 32] or Camel-aware tokenization [22, 29].
Utilizing tree-based encoders for AST-based inputs [26, 46, 62],
GNN for Graph-based inputs [13].

Output Searching

Large search space

Using Copy Mechanism [6, 9, 62]

Patch Ranking

Large search space

Perform a code-aware filter [22] or a DL-based classifier [26]

in the source code such as identifiers and literals are named subjec-
tively by programmers, the OOV problem will be more troublesome
when adopting neural models on the APR task.

Limited use of code-related information. Programs written
in high-level languages provide richer information than natural
languages. For example, an Abstract-Syntax-Tree (AST) contains
much more structural information compared with textual codes.
Such code-related features can be used to help the model to be
aware of more domain knowledge of the programming language.
However, ordinary models in NLP field can not deal with these
code-related information properly.

Large searching space. Generally, a NPR model generates 𝑉 𝑙
𝑠
candidate patches for each bug where 𝑉𝑠 is the size of the vocabulary
(usually at tens of thousands level) and 𝑙 is the length of the output.
When generating patches, the NPR model choices one token from
the vocabulary at each time. Obviously, such a huge search space
will lead to a decrease on the model performance.

A summary of these challenges and corresponding solutions is
described in Table 4. We categorized them into four aforementioned
phases of the overall NPR procedure. These phases face different
challenges and necessitate different design choices to address them.
First, in the preprocessing phase, textual codes need to be divided
into a list of tokens. All of these tokens together form a fixed vo-
cabulary that defines the input space. This step is challenging since
source codes may contain more OOV words, causing the model
to make wrong predictions. Then in the input representing phase,
additional features constructed previously pose the challenge of
designing specific neural encoders to better capture their semantics.
Next, in the output searching phase, the decoder module searches
for potential fixes during the vocabulary-defined output space. It is
challenging to select correct candidates from tens of thousands of
choices. Finally, the generated candidate patches must be ranked
and filtered in order to minimize validation costs. As a result, an
efficient patch ranking strategy is required to significantly reduce
the size of the candidate set while maintaining the correct patch hit
rate. In the following part, we will show how previous studies so-
lute these challenges and discuss the advantages and shortcomings
of various solutions.

5.2 Solutions to OOV
The first challenge is the OOV problem. When dealing with OOV
words, the NPR model will gain a poor performance [9]. For the
fact that programming languages use a more irregular vocabulary,
the OOV problem will be even more serious when using Neural
models to solve the APR task. In order to alleviate the OOV problem,
researchers have taken several measurements reflected by design
choices on the Code Abstraction and the Code Tokenization phase.
The general purpose of existing solutions is to convert uncommon
words into common ones. To this aim, some approaches choose to
rename the natural elements in the source code such as identifiers
[47, 62] and literals [22, 29, 47]. Another way is to divide word-
piece tokens into sub units with BPE [3, 14, 22, 32] or code-aware
tokenization [22, 29] that stems from commonly used naming rules
of programming languages. We look through their solutions on
these two phases and further get general conclusions as follows:

1. Abstraction of codes can reduce the size of the vocab-
ulary, thus mitigating the OOV problem. Code abstraction is
a straightforward way that works on solving the OOV problem
by renaming the natural elements in the source code to simplify
the vocabulary. As an evidence, with abstract identifiers and liter-
als, the accuracy of Tufano’s model [47] can be improved by 104%
compared to no-abstraction one on the CodRep [10] dataset. But
such abstraction can also lead to a reduction of the applicability to
real-world debugging scenarios. For example, Codit [6] mentioned
that the model trained with abstract identifiers can’t handle the
situation when the fix must introduce a new identifier not appeared
in the source program. As we concluded, the best practice on code
abstraction is to abstract all literals [29] and uncommon identifiers
[62], which can benefit a large performance improvement with a
trivial decrease of applicability.

2. BPE-based tokenization may be not suitable for APR
task. BPE [43] is a commonly used tokenization method in the
NLP field. It can mitigate the OOV problem by splitting sentences
into a series of sub-words. However, it is not suitable for the APR
task for two reasons. Firstly, dividing the program into sub words
will destroy the lexical and syntactic structure within the code text.
Approaches that adopts BPE tokenization [3, 14, 22, 32] can only

Neural Program Repair : Systems, Challenges and Solutions

Conference acronym ’XX, ,

incorporate textual features since structural features like AST need
a word-piece tokenization. Secondly, a sentence toked by BPE will
become much longer than one toked at word-level. The gain that
BPE brings by solving OOV is less than the loss on a longer input,
since DNN-based models has a weak ability to handle long inputs.
According to the results of Tufano [47] and SequenceR [9], the
accuracy of the same model on medium inputs (50 to 100 tokens)
is 30% - 60% lower than that on small inputs (less than 50 tokens).
We noticed that the existing BPE-based methods [3, 22] with good
performance largely benefit from pre-train technologies which are
too heavy and costly for APR task.

5.3 Usage of code-related information
Compared with natural languages, programming languages have
more significant features. Proper use of these additional features can
be beneficial to model’s ability on understanding and fixing buggy
programs. There are two core issues: how to construct features from
source codes and how to represent features, which is reflected by
the design choices on Preprocessing and Input Representing module.
Extra features come from AST of the program [6, 13, 26, 46, 62]
or a static program analyzer [3]. To extract AST-based features,
tree-based [26, 46, 62] or graph-based [13] are used in the Input
Representing phase.

1. The introduction of grammar rules is helpful for gener-
ating compilable patches. Patches produced by vanilla encoder-
decoder models may not be syntactically correct because they
are generated purely based on probability. These patches would
definitely be discarded because they can’t even be compiled. Ap-
proaches without grammar constrains [9, 22, 29] generate patches
with an average compilable rate less than 40%. Grammar-aware
models [6, 46] can learn to generate syntactically correct patches by
representing grammar constraints with CFG rules, thus reaching a
higher accuracy in the same-size candidate set. For one case, Codit
[6] exports an improvement with 63% on suggesting correct patches
within 5 candidates, compared with SequenceR [9]. However, such
introduction of rules is not enough. Human programmers can also
ensure correct syntax when writing code in text form. Instead of
outputting CFG rules to form an AST, future NPR systems should
ensure the syntax correctness when outputting textual codes.

2. Structural models can be more precise at a price of de-
creasing the applicability. There are two ways to represent AST:
using structural models that suitable for tree-structure inputs or
traversing AST to get sequential inputs. Most of the methods that
use the former scheme [13, 26, 62] obtained golden performance,
benefiting from structural models’ ability to represent AST. How-
ever, structural models have more restrictions on the form of input
and output, which limits the repair scope of NPR systems. For
example, DLFix [26] that adopts a Tree-LSTM [45] based encoder-
decoder model only works on one statement bugs and the fix and
bug location have to be the same. Similarly, Recoder [62] that uses
a Tree-based decoder can only repair one-hunk bugs. As s com-
parison, the sequential encoder-decoder model can deal with any
bug within the source input. We suggest that these two kinds of
models can be applied to different scenarios. For example, for small
or informal code programs, a sequential encoder-decoder can make

a quick fix. When dealing with compilable programs in projects, a
structural model can obtain a more precise prediction.

5.4 Reduction of the search space
Correct patches are sparse in the search space [28]. For NPR, large
search space will have two negative effects. During the Output
Searching phase, it will decrease the performance since correct
patches are sparse in the search space [28]. And during the Patch
Ranking phase, the large search space will result in too much patch
candidates that are time-costly to validation. Therefore, a reduc-
tion strategy is necessary to make an efficient search. We review
included studies’ design choices at these two phases and sum up
with two conclusions related to the search space.

1. A copy generator is an efficient way to reduce the search
space. The copy mechanism [42] allows the model to select tokens
from the source program as outputs during the decoding phase.
In a sense, it enables the model to learn to keep correct tokens
and only modify the buggy part. The ablation results of SequenceR
[9] have demonstrated the remarkable improvement brought by
copy mechanism. In practice, the copy mechanism can be easily
integrated into various neural models [6, 9, 14, 62].

2. The number of candidates is not the more, the better.
Since the NPR system is a kind of probability model, a larger candi-
date number surely has a higher probability to contain the correct
patch. However, the improvement on accuracy brought by more
candidates will largely decline with the increase of the beam size
according to existing studies [6, 14, 26, 47]. Such performance gain
brought by a larger candidate number can result in a significant
increase in time costs. For example, in Defects4J [23], a project
may cost several minutes to compile and run the test cases. We
observe that CURE [22] use a candidate number of 5,000. It means
that using CURE [22] to fix a bug may cost several days (at the
worst situation). Therefore, we suggest that follow-up researchers
should also pay attention to the balance between performance and
the time cost.

6 CONCLUSION AND FUTURE DIRECTIONS
This paper attempted to provide an elaborate overview on architec-
tures, challenges and corresponding solutions of latest NPR Systems.
To locate important design points, we proposed to explore the de-
sign space of each part of the NPR procedure. Furthermore, we
identified three challenges current NPR approaches encountered
and discussed the effect of existing solutions. In addition to better
solutions on the challenges mentioned in this paper, we print out
several promising future research directions on NPR:

More rules on generating. From the standpoint of a developer,
most patches generated by NPR models are of poor quality. Many
NPR systems are unable to guarantee the syntax correctness of
generated patches. The reason for this is that they are generated
purely based on probability. In reality, however, the programming
must adhere to certain syntax rules. The incorporation of these code-
related specifications into NPR models will aid in the generation of
more qualified programs.

Explicable NPR models. Since neural networks are black-box,
there is low interpretability on how NPR models produce potential

Conference acronym ’XX, ,

fixes. It may lead to users’ distrust of the generated patches, result-
ing in limited applications of NPR models on real-world bug-fix
scenarios. We suggested that besides the performance, efforts on
making the model explainable should be dedicated.

Robustness measurement. Existing NPR studies are all con-
cerned with how many correct patches are generated in the exper-
imental environment, while ignoring practical reliability metrics
such as robustness. Due to some properties of neural networks,
the NPR model may produce completely different predictions even
when given inputs that are just slightly different. Such behaviors
may result in errors that require a significant amount of manpower
to trace and review. Therefore, metrics describing the robustness
of the NPR model should be created.

REFERENCES
[1] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Con-
ference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=
BJOFETxR-

[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. In 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
http://arxiv.org/abs/1409.0473

[3] Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin T. Vechev. 2021. TFix:
Learning to Fix Coding Errors with a Text-to-Text Transformer. In Proceedings
of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139), Marina
Meila and Tong Zhang (Eds.). PMLR, 780–791. http://proceedings.mlr.press/
v139/berabi21a.html

[4] Tom Britton, Lisa Jeng, Graham Carver, and Paul Cheak. 2012. Quantify the time
and cost saved using reversible debuggers. Technical Report. Technical report,
Cambridge Judge Business School.

[5] Heling Cao, YangXia Meng, Jianshu Shi, Lei Li, Tiaoli Liao, and Chenyang Zhao.
2020. A Survey on Automatic Bug Fixing. In 2020 6th International Symposium
on System and Software Reliability (ISSSR). 122–131. https://doi.org/10.1109/
ISSSR51244.2020.00029

[6] Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray.
2020. Codit: Code editing with tree-based neural models. IEEE Transactions on
Software Engineering (2020).

[7] Liushan Chen, Yu Pei, and Carlo A. Furia. 2021. Contract-Based Program Repair
Without The Contracts: An Extended Study. IEEE Trans. Software Eng. 47, 12
(2021), 2841–2857. https://doi.org/10.1109/TSE.2020.2970009

[8] Zimin Chen, Steve Kommrusch, and Martin Monperrus. 2021. Neural Transfer
Learning for Repairing Security Vulnerabilities in C Code. CoRR abs/2104.08308
(2021). arXiv:2104.08308 https://arxiv.org/abs/2104.08308

[9] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys
Poshyvanyk, and Martin Monperrus. 2021. SequenceR: Sequence-to-Sequence
Learning for End-to-End Program Repair. IEEE Trans. Software Eng. 47, 9 (2021),
1943–1959. https://doi.org/10.1109/TSE.2019.2940179

[10] Zimin Chen and Martin Monperrus. 2018. The CodRep Machine Learning on
Source Code Competition. CoRR abs/1807.03200 (2018). arXiv:1807.03200 http:
//arxiv.org/abs/1807.03200

[11] Jianpeng Cheng and Mirella Lapata. 2016. Neural Summarization by Extracting
Sentences and Words. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany,
Volume 1: Long Papers. The Association for Computer Linguistics. https://doi.
org/10.18653/v1/p16-1046

[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill
Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-
tional Linguistics, 4171–4186. https://doi.org/10.18653/v1/n19-1423

[13] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang.
2020. Hoppity: Learning Graph Transformations to Detect and Fix Bugs in
Programs. In 8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.
net/forum?id=SJeqs6EFvB

[14] Yangruibo Ding, Baishakhi Ray, Premkumar T. Devanbu, and Vincent J. Hel-
lendoorn. 2020. Patching as Translation: the Data and the Metaphor. In 35th
IEEE/ACM International Conference on Automated Software Engineering, ASE 2020,
Melbourne, Australia, September 21-25, 2020. IEEE, 275–286. https://doi.org/10.
1145/3324884.3416587

[15] Dawn Drain, Colin B. Clement, Guillermo Serrato, and Neel Sundaresan. 2021.
DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code
Skeletons. CoRR abs/2105.09352 (2021). arXiv:2105.09352 https://arxiv.org/abs/
2105.09352

[16] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of
the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20
November 2020 (Findings of ACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and
Yang Liu (Eds.). Association for Computational Linguistics, 1536–1547. https:
//doi.org/10.18653/v1/2020.findings-emnlp.139

[17] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2012.
GenProg: A Generic Method for Automatic Software Repair. IEEE Trans. Software
Eng. 38, 1 (2012), 54–72. https://doi.org/10.1109/TSE.2011.104

[18] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish K. Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning. In Proceedings of the Thirty-
First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco,
California, USA, Satinder Singh and Shaul Markovitch (Eds.). AAAI Press, 1345–
1351. http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603

[19] Hideaki Hata, Emad Shihab, and Graham Neubig. 2018. Learning to Generate
Corrective Patches using Neural Machine Translation. CoRR abs/1812.07170
(2018). arXiv:1812.07170 http://arxiv.org/abs/1812.07170

[20] Sepp Hochreiter and Jürgen Schmidhuber. 1996. LSTM can Solve Hard Long
Time Lag Problems. In Advances in Neural Information Processing Systems 9, NIPS,
Denver, CO, USA, December 2-5, 1996, Michael Mozer, Michael I. Jordan, and
Thomas Petsche (Eds.). MIT Press, 473–479. http://papers.nips.cc/paper/1215-
lstm-can-solve-hard-long-time-lag-problems

[21] Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen.
2018. Shaping program repair space with existing patches and similar code. In
Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing
and Analysis, ISSTA 2018, Amsterdam, The Netherlands, July 16-21, 2018, Frank Tip
and Eric Bodden (Eds.). ACM, 298–309. https://doi.org/10.1145/3213846.3213871
[22] Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE: Code-Aware Neural Ma-
chine Translation for Automatic Program Repair. In 43rd IEEE/ACM International
Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021.
IEEE, 1161–1173. https://doi.org/10.1109/ICSE43902.2021.00107

[23] René Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: a database of
existing faults to enable controlled testing studies for Java programs. In Interna-
tional Symposium on Software Testing and Analysis, ISSTA ’14, San Jose, CA, USA -
July 21 - 26, 2014, Corina S. Pasareanu and Darko Marinov (Eds.). ACM, 437–440.
https://doi.org/10.1145/2610384.2628055

[24] Donald E. Knuth. 1968. Semantics of Context-Free Languages. Math. Syst. Theory

2, 2 (1968), 127–145. https://doi.org/10.1007/BF01692511

[25] Daoyuan Li, Li Li, Dongsun Kim, Tegawendé F. Bissyandé, David Lo, and Yves Le
Traon. 2019. Watch out for this commit! A study of influential software changes.
J. Softw. Evol. Process. 31, 12 (2019). https://doi.org/10.1002/smr.2181

[26] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: context-based code
transformation learning for automated program repair. In ICSE ’20: 42nd In-
ternational Conference on Software Engineering, Seoul, South Korea, 27 June
- 19 July, 2020, Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 602–614.
https://doi.org/10.1145/3377811.3380345

[27] Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. 2017.
QuixBugs: a multi-lingual program repair benchmark set based on the quixey
challenge. In Proceedings Companion of the 2017 ACM SIGPLAN International
Conference on Systems, Programming, Languages, and Applications: Software for
Humanity, SPLASH 2017, Vancouver, BC, Canada, October 23 - 27, 2017, Gail C.
Murphy (Ed.). ACM, 55–56. https://doi.org/10.1145/3135932.3135941

[28] Kui Liu, Shangwen Wang, Anil Koyuncu, Kisub Kim, Tegawendé F. Bissyandé,
Dongsun Kim, Peng Wu, Jacques Klein, Xiaoguang Mao, and Yves Le Traon. 2020.
On the efficiency of test suite based program repair: A Systematic Assessment of
16 Automated Repair Systems for Java Programs. In ICSE ’20: 42nd International
Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020,
Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 615–627. https://doi.org/10.
1145/3377811.3380338

[29] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and
Lin Tan. 2020. CoCoNuT: combining context-aware neural translation models
using ensemble for program repair. In ISSTA ’20: 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis, Virtual Event, USA, July 18-22,
2020, Sarfraz Khurshid and Corina S. Pasareanu (Eds.). ACM, 101–114. https:
//doi.org/10.1145/3395363.3397369

[30] Matias Martinez and Martin Monperrus. 2016. ASTOR: a program repair library
for Java (demo). In Proceedings of the 25th International Symposium on Software
Testing and Analysis, ISSTA 2016, Saarbrücken, Germany, July 18-20, 2016, Andreas

Neural Program Repair : Systems, Challenges and Solutions

Conference acronym ’XX, ,

Zeller and Abhik Roychoudhury (Eds.). ACM, 441–444. https://doi.org/10.1145/
2931037.2948705

[31] Matias Martinez and Martin Monperrus. 2018. Ultra-Large Repair Search Space
with Automatically Mined Templates: The Cardumen Mode of Astor. In Search-
Based Software Engineering - 10th International Symposium, SSBSE 2018, Mont-
pellier, France, September 8-9, 2018, Proceedings (Lecture Notes in Computer Sci-
ence, Vol. 11036), Thelma Elita Colanzi and Phil McMinn (Eds.). Springer, 65–86.
https://doi.org/10.1007/978-3-319-99241-9_3

[32] Ehsan Mashhadi and Hadi Hemmati. 2021. Applying CodeBERT for Automated
Program Repair of Java Simple Bugs. In 18th IEEE/ACM International Conference
on Mining Software Repositories, MSR 2021, Madrid, Spain, May 17-19, 2021. IEEE,
505–509. https://doi.org/10.1109/MSR52588.2021.00063

[33] Martin Monperrus. 2018. Automatic Software Repair: A Bibliography. ACM

Comput. Surv. 51, 1 (2018), 17:1–17:24. https://doi.org/10.1145/3105906
[34] Martin Monperrus. 2020. The living review on automated program repair. (2020).
[35] Martin Monperrus and Matias Martinez. 2012. CVS-Vintage: A Dataset of 14 CVS

Repositories of Java Software. (12 2012).

[36] Sangwhan Moon and Naoaki Okazaki. 2021. Effects and Mitigation of Out-of-
vocabulary in Universal Language Models. J. Inf. Process. 29 (2021), 490–503.
https://doi.org/10.2197/ipsjjip.29.490

[37] Marjane Namavar, Noor Nashid, and Ali Mesbah. 2021. A Controlled Experi-
ment of Different Code Representations for Learning-Based Bug Repair. CoRR
abs/2110.14081 (2021). arXiv:2110.14081 https://arxiv.org/abs/2110.14081
[38] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chan-
dra. 2013. SemFix: program repair via semantic analysis. In 35th International
Conference on Software Engineering, ICSE ’13, San Francisco, CA, USA, May 18-26,
2013, David Notkin, Betty H. C. Cheng, and Klaus Pohl (Eds.). IEEE Computer
Society, 772–781. https://doi.org/10.1109/ICSE.2013.6606623

[39] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-

proving language understanding by generative pre-training. (2018).

[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach.
Learn. Res. 21 (2020), 140:1–140:67. http://jmlr.org/papers/v21/20-074.html
[41] Ingo Scholtes, Pavlin Mavrodiev, and Frank Schweitzer. 2016. From Aristotle
to Ringelmann: a large-scale analysis of team productivity and coordination
in Open Source Software projects. Empir. Softw. Eng. 21, 2 (2016), 642–683.
https://doi.org/10.1007/s10664-015-9406-4

[42] Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point:
Summarization with Pointer-Generator Networks. In Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics, ACL 2017, Van-
couver, Canada, July 30 - August 4, Volume 1: Long Papers, Regina Barzilay
and Min-Yen Kan (Eds.). Association for Computational Linguistics, 1073–1083.
https://doi.org/10.18653/v1/P17-1099

[43] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer
Linguistics. https://doi.org/10.18653/v1/p16-1162

[44] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2020.
TreeGen: A Tree-Based Transformer Architecture for Code Generation. In The
Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-
Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020,
The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 8984–8991.
https://aaai.org/ojs/index.php/AAAI/article/view/6430

[45] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved
Semantic Representations From Tree-Structured Long Short-Term Memory Net-
works. In Proceedings of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processing, ACL 2015, July
26-31, 2015, Beijing, China, Volume 1: Long Papers. The Association for Computer
Linguistics, 1556–1566. https://doi.org/10.3115/v1/p15-1150

[46] Yu Tang, Long Zhou, Ambrosio Blanco, Shujie Liu, Furu Wei, Ming Zhou, and
Muyun Yang. 2021. Grammar-Based Patches Generation for Automated Program
Repair. In Findings of the Association for Computational Linguistics: ACL/IJCNLP
2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021),
Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for
Computational Linguistics, 1300–1305. https://doi.org/10.18653/v1/2021.findings-
acl.111

[47] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. 2019. An Empirical Study on Learning Bug-Fixing
Patches in the Wild via Neural Machine Translation. ACM Trans. Softw. Eng.
Methodol. 28, 4 (2019), 19:1–19:29. https://doi.org/10.1145/3340544

[48] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh.
2019. Neural Program Repair by Jointly Learning to Localize and Repair. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=

ByloJ20qtm

[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is
All you Need. In Advances in Neural Information Processing Systems 30: An-
nual Conference on Neural Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy
Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (Eds.). 5998–6008.
https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html

[50] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S. Yu. 2018. Improving automatic source code summarization via deep
reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Confer-
ence on Automated Software Engineering, ASE 2018, Montpellier, France, September
3-7, 2018, Marianne Huchard, Christian Kästner, and Gordon Fraser (Eds.). ACM,
397–407. https://doi.org/10.1145/3238147.3238206

[51] Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018.
Context-aware patch generation for better automated program repair. In Pro-
ceedings of the 40th International Conference on Software Engineering, ICSE 2018,
Gothenburg, Sweden, May 27 - June 03, 2018, Michel Chaudron, Ivica Crnkovic,
Marsha Chechik, and Mark Harman (Eds.). ACM, 1–11. https://doi.org/10.1145/
3180155.3180233

[52] W. Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A
Survey on Software Fault Localization. IEEE Trans. Software Eng. 42, 8 (2016),
707–740. https://doi.org/10.1109/TSE.2016.2521368

[53] Qi Xin and Steven P. Reiss. 2017. Leveraging syntax-related code for automated
program repair. In Proceedings of the 32nd IEEE/ACM International Conference on
Automated Software Engineering, ASE 2017, Urbana, IL, USA, October 30 - November
03, 2017, Grigore Rosu, Massimiliano Di Penta, and Tien N. Nguyen (Eds.). IEEE
Computer Society, 660–670. https://doi.org/10.1109/ASE.2017.8115676

[54] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and
Lu Zhang. 2017. Precise condition synthesis for program repair. In Proceedings of
the 39th International Conference on Software Engineering, ICSE 2017, Buenos Aires,
Argentina, May 20-28, 2017, Sebastián Uchitel, Alessandro Orso, and Martin P.
Robillard (Eds.). IEEE / ACM, 416–426. https://doi.org/10.1109/ICSE.2017.45
[55] Michihiro Yasunaga and Percy Liang. 2020. Graph-based, Self-Supervised
Program Repair from Diagnostic Feedback. In Proceedings of the 37th Inter-
national Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual
Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 10799–10808.
http://proceedings.mlr.press/v119/yasunaga20a.html

[56] Michihiro Yasunaga and Percy Liang. 2021. Break-It-Fix-It: Unsupervised Learn-
ing for Program Repair. In Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of Ma-
chine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR,
11941–11952. http://proceedings.mlr.press/v139/yasunaga21a.html

[57] He Ye, Matias Martinez, and Martin Monperrus. 2021. Neural Program Repair with
Execution-based Backpropagation. CoRR abs/2105.04123 (2021). arXiv:2105.04123
https://arxiv.org/abs/2105.04123

[58] Hiroaki Yoshida, Rohan Bavishi, Keisuke Hotta, Yusuke Nemoto, Mukul R. Prasad,
and Shinji Kikuchi. 2020. Phoenix: a tool for automated data-driven synthesis of
repairs for static analysis violations. In ICSE ’20: 42nd International Conference
on Software Engineering, Companion Volume, Seoul, South Korea, 27 June - 19
July, 2020, Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 53–56. https:
//doi.org/10.1145/3377812.3382150

[59] Yuan Yuan and Wolfgang Banzhaf. 2020. ARJA: Automated Repair of Java Pro-
grams via Multi-Objective Genetic Programming. IEEE Trans. Software Eng. 46,
10 (2020), 1040–1067. https://doi.org/10.1109/TSE.2018.2874648

[60] Hao Zhong and Zhendong Su. 2015. An Empirical Study on Real Bug Fixes.
In 37th IEEE/ACM International Conference on Software Engineering, ICSE 2015,
Florence, Italy, May 16-24, 2015, Volume 1, Antonia Bertolino, Gerardo Canfora,
and Sebastian G. Elbaum (Eds.). IEEE Computer Society, 913–923. https://doi.
org/10.1109/ICSE.2015.101

[61] Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where should the bugs be fixed?
More accurate information retrieval-based bug localization based on bug reports.
In 34th International Conference on Software Engineering, ICSE 2012, June 2-9, 2012,
Zurich, Switzerland, Martin Glinz, Gail C. Murphy, and Mauro Pezzè (Eds.). IEEE
Computer Society, 14–24. https://doi.org/10.1109/ICSE.2012.6227210

[62] Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei Xiong,
and Lu Zhang. 2021. A syntax-guided edit decoder for neural program repair. In
ESEC/FSE ’21: 29th ACM Joint European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering, Athens, Greece, August 23-28,
2021, Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massimiliano Di
Penta (Eds.). ACM, 341–353. https://doi.org/10.1145/3468264.3468544

