Empirical Software Engineering manuscript No.
(will be inserted by the editor)

What really changes when developers intend to
improve their source code:

A commit-level study of static metric value and static
analysis warning changes

Alexander Trautsch · Johannes Erbel ·
Steﬀen Herbold · Jens Grabowski

Received: date / Accepted: date

Abstract Many software metrics are designed to measure aspects that are
believed to be related to software quality. Static software metrics, e.g., size,
complexity and coupling are used in defect prediction research as well as soft-
ware quality models to evaluate software quality. Static analysis tools also
include boundary values for complexity and size that generate warnings for
developers. While this indicates a relationship between quality and software
metrics, the extent of it is not well understood. Moreover, recent studies found
that complexity metrics may be unreliable indicators for understandability of
the source code. To explore this relationship, we leverage the intent of develop-
ers about what constitutes a quality improvement in their own code base. We
manually classify a randomized sample of 2,533 commits from 54 Java open
source projects as quality improving depending on the intent of the developer
by inspecting the commit message. We distinguish between perfective and
corrective maintenance via predeﬁned guidelines and use this data as ground
truth for the ﬁne-tuning of a state-of-the art deep learning model for natural
language processing. The benchmark we provide with our ground truth indi-
cates that the deep learning model can be conﬁdently used for commit intent
classiﬁcation. We use the model to increase our data set to 125,482 commits.

Alexander Trautsch
Institute of Software and Systems Engineering, TU Clausthal, Germany
E-mail: alexander.trautsch@tu-clausthal.de

Johannes Erbel
Institute of Computer Science, University of Goettingen, Germany
E-mail: johannes.erbel@cs.uni-goettingen.de

Steﬀen Herbold
Institute of Software and Systems Engineering, TU Clausthal, Germany
E-mail: steﬀen.herbold@tu-clausthal.de

Jens Grabowski
Institute of Computer Science, University of Goettingen, Germany
E-mail: grabowski@cs.uni-goettingen.de

2
2
0
2

y
a
M
0
3

]
E
S
.
s
c
[

5
v
4
4
5
3
0
.
9
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Alexander Trautsch et al.

Based on the resulting data set, we investigate the diﬀerences in size and 14
static source code metrics between changes that increase quality, as indicated
by the developer, and other changes. In addition, we investigate which ﬁles
are targets of quality improvements. We ﬁnd that quality improving commits
are smaller than other commits. Perfective changes have a positive impact on
static source code metrics while corrective changes do tend to add complexity.
Furthermore, we ﬁnd that ﬁles which are the target of perfective maintenance
already have a lower median complexity than other ﬁles. Our study results
provide empirical evidence for which static source code metrics capture qual-
ity improvement from the developers point of view. This has implications for
program understanding as well as code smell detection and recommender sys-
tems.

Keywords Static code analysis · Quality evolution · Software metrics ·
Software quality

1 Introduction

Software quality is notoriously hard to measure (Kitchenham and Pﬂeeger,
1996). The main reason is that quality is subjective and that it consists
of multiple factors. This idea was formalized by Boehm and McCall in the
70s (Boehm et al., 1976; McCall et al., 1977). Both introduced a layered ap-
proach where software quality consists of multiple factors. The standard ISO
ISO/IEC (2001) and successor ISO/IEC (2011) also approach software quality
in this fashion.

All these ideas contain abstract quality factors. However, the question re-
mains what concrete measurements can we perform to evaluate the abstract
factors of which software quality consists, i.e., how do we measure software
quality. Some software quality models recommend concrete measurements,
e.g., ColumbusQM (Bakota et al., 2011) and Quamoco (Wagner et al., 2012).
Defect prediction researchers also try to build (machine learning) models to
ﬁnd a function that can map measurable metrics to the number of defects in
the source code. This can also be thought of as software quality evaluation, that
tries to map internal software quality, measured by code or process metrics,
to external software quality measured by defects (Fenton and Bieman, 2014).
The internal and external quality categories can also be mapped to perfective
and corrective maintenance categories after Swanson (1976). Perfective main-
tenance should increase internal quality while corrective maintenance should
increase external quality. Both categories should increase the overall quality
of the software. To ease the readability, we adopt the perfective and corrective
terms deﬁned by Swanson for the rest of the paper when referring to the cat-
egories. For general assumptions, we adopt the internal and external quality
terms. Internal quality represents what the developer sees, e.g., structure, size,
and complexity while external quality what the user sees, e.g., defects.

Software quality models and defect prediction models use static source
code metrics as a proxy for quality (Hosseini et al., 2017). The intuition is

What really changes when developers intend to improve their source code

3

that complex code, as measured by static source code metrics, is harder to
reason about and, therefore, is more prone to errors. However, recent research
by Peitek et al. (2021) showed that measured code complexity is perceived
very diﬀerently between developers and does not translate well to code un-
derstanding. A similar result was found by Scalabrino et al. (2021) although
their work is focused on readability measured in a static way. Both studies,
due to their nature, observe developers in a controlled experiment with code
snippets. To supplement these results, it would be interesting to measure what
developers change in their code “in the wild” to improve software quality and
if their intent matches what we can measure, e.g., if complexity is reduced in
a change that intents to improve quality.

While there are multiple publications on maintenance or change classiﬁ-
cation after Swanson (1976), e.g., Mockus and Votta (2000), Mauczka et al.
(2012), Levin and Yehudai (2017) and H¨onel et al. (2019), we are not aware
of publications that investigate diﬀerences between multiple software metrics
for corrective and perfective maintenance as well as other changes. The inclu-
sion of other changes results in computational eﬀort as we need every metric
for every ﬁle in every commit. However, we are able to provide this data via
the SmartSHARK ecosystem (Trautsch et al., 2017, 2020b). This additional
eﬀort allows us to infer if categories of changes are diﬀerent when regarding all
changes of a software project. Most recent work focuses on certain aspects in-
stead of a generic overview, e.g., how software metric values change when code
smells are removed (Bavota et al., 2015) or refactorings are applied (Bavota
et al., 2015; Alshayeb, 2009; Pantiuchina et al., 2020). However, we believe
that taking a step back from focused approaches and investigating generic
quality improvements is worthwhile. A generic overview has the advantage
of mitigating possible problems that can occur for narrow meaning keywords
of topically focused approaches while at the same time providing a cohesive
overview. Moreover, it allows for generic statements about software quality
evolution based on this information and can complement focused approaches.
In this work, we ﬁnd changes that increase the quality, while we measure
current, previous and delta of common source code metric values used in a
current version (Bakota et al., 2014) of the Columbus quality model (Bakota
et al., 2011). We use the commit message contained in each change to ﬁnd
commits where the intent of the developer is to improve software quality. This
provides us with a view of corrective and perfective maintenance commits.

Within our study, we ﬁrst classify the commit intent for a sample of 2,533
commits from 54 open source projects manually. The manual classiﬁcation
is provided by two researchers according to predeﬁned guidelines. According
to the overview of previous research in this area provided by AlOmar et al.
(2021) our study would be the largest manual classiﬁcation study of commits.
We use this data as ground truth to ﬁne-tune a state-of-the-art deep learning
model for natural language processing that was pre-trained exclusively on
software engineering data (von der Mosel et al., 2021). After we determine
the performance of the model, we classify all commits, increasing our data to
125,482 commits.

4

Alexander Trautsch et al.

We use the automatically classiﬁed data to conduct a two part study. The
ﬁrst part is a conﬁrmatory study into the expected behavior of metric values
for quality increasing changes. Expected behaviour, e.g., complexity is reduced
in quality increasing changes is derived as hypothesis from existing quality
models and the related literature.

In case our data matches the expected behavior from the literature, we
can conﬁrm the postulated theories and provide evidence in favor of using the
measurements. Otherwise, we try to establish which metrics may be unsuit-
able for quality estimation, including the potential reasons. Even further, we
determine whether metrics used in software quality models are impacted by
quality increasing maintenance, therefore providing an evaluation for software
quality measurement metrics.

The second part of our study is of exploratory nature. We investigate which
ﬁles are the target of quality improvements by the developers. We explore
whether only complex ﬁles are receiving perfective changes and which metric
values are indicative of corrective changes. This provides us with data for prac-
titioners and static analysis tool vendors for boundary values which are likely
to have a positive impact on the quality of source code from the perspective
of the developers.
Overall, our work provides the following contributions:

– A large data set of manual classiﬁcations of commit intents with improving

internal and external quality categories.

– A conﬁrmatory study of size and complexity metric value changes for qual-

ity improvements.

– An exploratory study of size and complexity metric values of ﬁles that are

the target of quality improvements.

– A ﬁne-tuned state-of-the-art deep learning model for automatic classiﬁca-

tion of commit intents.

The main ﬁndings of our study are the following:

– We conﬁrm previous work that quality increasing commits are smaller than

other changes.

– While perfective changes have a positive impact on most static source
code metric values, corrective changes have a negative impact on size and
complexity.

– The ﬁles that are the target of perfective changes are already less complex

and smaller than other ﬁles.

– The ﬁles that are the target of corrective changes are more complex and

larger than other ﬁles.

The remainder of this paper is structured as follows. In Section 2, we deﬁne
our research questions and hypotheses. In Section 3, we discuss the previous
work related to our study. Section 4 contains our case study design with de-
scriptions for subject selection as well as data sources and analysis procedure.
In Section 5, we present the results of our case study and discuss them in
Section 6. Section 7 lists our identiﬁed threats to validity and Section 8 closes
with a conclusion of our work.

What really changes when developers intend to improve their source code

5

2 Research Questions and Hypotheses

In our study, we answer two research questions.
– RQ1: Does developer intent to improve internal or external quality have a

positive impact on software metric values?
Previous work provides us with certain indications about the impact on
software metric values. This is part of our conﬁrmatory study, and we
derive two hypotheses from previous work regarding how size and software
metric values should change for diﬀerent types of quality improvement. We
formulate our assumptions as hypothesis and test these in our case study.
– H1: Intended quality improvements are smaller than other

changes.
Mockus and Votta (2000) found that corrective changes modify fewer
lines while perfective changes delete more lines. Purushothaman and
Perry (2005) also observed more deletions for perfective maintenance
and an overall smaller size of perfective and corrective maintenance.
Both studies provide measurements we base our hypothesis on. While
they are using the same closed source project we will be able to see if
our assumption holds for our multiple Java open source projects.
H¨onel et al. (2019) used size-based metrics as additional features for an
automated approach to classify maintenance types. They found that the
size-based metric values increased the classiﬁcation performance. More-
over, just-in-time quality assurance (Kamei et al., 2013) builds on the
assumption that changes and metrics derived from these changes can
predict bug introduction, meaning there should be a diﬀerence. There-
fore, we hypothesize that corrective as well as perfective maintenance
consist of smaller changes. Addition of features should be larger than
both, and therefore we assume that the categories we are interested in,
perfective and corrective, are smaller than other changes.

– H2: Intended quality improvements impact software quality

metric values in a positive way.
In this paper, we focus on metrics used in the Columbus Quality Model
(Bakota et al., 2011; Bakota et al., 2014). The metrics are speciﬁcally
chosen for a quality model so they should provide diﬀerent measure-
ments based on their maintenance category. Prior research, e.g., Ch’avez
et al. (2017) and Stroggylos and Spinellis (2007) found that refactorings,
which are part of our classiﬁcation, have a measurable impact on soft-
ware metric values. We hypothesize that an improvement consciously
applied by a developer via a perfective commit has a measurable, posi-
tive impact on software metric values. Positive means that we expect a
value change direction of the metric value, e.g., complexity is reduced.
We note our expected direction for each metric together with a descrip-
tion in Table 4.
Defect prediction research assumes a connection between software met-
rics and external software quality in the form of bugs. While most pub-
lications in defect prediction are not investigating the impact of single

6

Alexander Trautsch et al.

bug ﬁxing changes the most common datasets all contain coupling, size
and complexity metrics as independent variables, e.g., (Jureczko and
Madeyski, 2010; NASA, 2004; D’Ambros et al., 2012), see also the sys-
tematic literature review by Hosseini et al. (2017). We hypothesize that
ﬁxing bugs via corrective commits has a measurable, positive impact on
software metric values. While a bug ﬁx may add complexity, our study
compares bug ﬁx changes with all other changes including feature addi-
tions. Therefore, we do not hypothesize that bug ﬁxing decreases com-
plexity generally, but that it is decreasing complexity in comparison to
all other changes. In contrast to H1 we are not able to compare our re-
sults to concrete studies as we are not aware of a study that investigates
metric value changes of perfective and corrective changes and compares
them against all other changes. We are instead trying to validate the
assumption that quality improvements should have a positive impact
on software quality metrics as they are found to improve detection of
defects (Gyimothy et al., 2005).

Our second research question is exploratory in nature.

– RQ2: What kind of ﬁles are the target of internal or external quality im-

provements?
The ﬁrst part of our study provides us with information about metric value
changes for quality increasing commits. In this part, we are exploring which
ﬁles are the target of quality increasing commits. We are interested in how
complex, e.g., via cyclomatic complexity, a ﬁle is on average that receives
perfective maintenance. Moreover, on the external quality side we are inter-
ested in which ﬁles are receiving corrective changes. Due to the exploratory
nature of this research question, we do not derive hypotheses.

3 Related Work

We separate the discussion of the related work into publications on the classi-
ﬁcation of changes and publications on the relation between quality improve-
ments and software metrics.

Most prior work that follows a similar approach to ours is concerned with
speciﬁc types of quality improving changes, e.g., refactoring and removal of
code smells. We note that some code smell detection is based on internal
software quality metrics, which we use in our study.

We ﬁrst present previous research related to the ﬁrst phase of our study,
i.e., classiﬁcation of changes with respect to maintenance types. Mockus and
Votta (2000) study changes in a large system and identiﬁed reasons for changes.
They ﬁnd that a textual description of the change can be used to identify the
type of change with a keyword based approach which they validated with
a developer survey. The authors classiﬁed changes to Swansons maintenance
types. They ﬁnd that corrective and perfective changes are smaller and that
perfective changes delete more lines than other changes. Mauczka et al. (2012)

What really changes when developers intend to improve their source code

7

present an automatic keyword based approach for classiﬁcation into Swansons
maintenance types. They evaluate their approach and provide a keyword list
for each maintenance type together with a weight.

Fu et al. (2015) present an approach for change classiﬁcation that uses
latent drichtlet allocation. They study ﬁve open source projects and classify
changes into Swansons maintenance types together with a not sure type. The
keyword list of their study is based on Mauczka et al. (2012).

Mauczka et al. (2015) collect developer classiﬁcations for three diﬀerent
classiﬁcation schemes. Their data contains 967 commits from six open source
projects. While the developers themselves are the best source of information,
we believe that within the guidelines of our approach our classiﬁcations are
similar to those of the developers. We evaluate this assumption in Section 4.2.
Yan et al. (2016) use discriminative topic modeling also based on the key-
word list by Mauczka et al. (2012). They focus on changes with multiple cat-
egories. Levin and Yehudai (2017) improve maintenance type classiﬁcation by
utilizing source code in addition to keywords. This is an indication that met-
ric values which are computed from source code are impacted by diﬀerent
maintenance types.

H¨onel et al. (2019) use size metrics as additional features for automated
classiﬁcation of changes. In our study, we ﬁrst classify the change and then look
at how this impacts size and spread of the change. However, the diﬀerences
we found in our study support the assumption that size-based features can be
used to distinguish change categories.

More recently, Wang et al. (2021) also analyze developer intents from the
commit messages. They focus on large review eﬀort code changes instead of
quality changes or maintenance types. They also use a keyword based heuristic
for the classiﬁcation. They do not, however, include a perfective maintenance
classiﬁcation.

Ghadhab et al. (2021) also use a deep learning model to classify commits.
They use word embeddings from the deep learning model in combination with
ﬁne-grained code changes to classify into Swansons maintenance categories. In
contrast to Ghadhab et al., we do not include code changes in our automatic
classiﬁcations and focus on the commit message.

The classiﬁcation of changes for the ground truth in our study is based
on manual inspection by two researchers instead of a keyword list. We specify
guidelines for the classiﬁcation procedure which enable other researchers to
replicate our work. To accept or reject our hypotheses, we only inspect internal
and external quality improvements which would correspond to the perfective
and corrective maintenance types by Swanson. In contrast to the previous
studies, we relate our classiﬁed changes also to a set of static software metrics.
We now present research related to our second phase of our study, the rela-
tion between intended quality improvements and software metrics. Stroggylos
and Spinellis (2007) found changes where the developers intended a refactor-
ing via the commit message. The authors then measured several source code
metrics to evaluate the quality change. In contrast to the work of Stroggylos
and Spinellis (2007), we do not focus on refactoring keywords. Instead, we con-

8

Alexander Trautsch et al.

sider refactoring as a part of our classiﬁcation guidelines. Moreover, our aim
is to investigate whether the metrics most commonly used as internal quality
metrics (see also Al Dallal and Abdin (2018)) are the ones that are changing
if developers perform quality improving changes including refactoring.

Fakhoury et al. (2019) investigated the practical impact of software evolu-
tion with developer perceived readability improvements on existing readabil-
ity models. After ﬁnding target commits via commit message ﬁltering they
applied state-of-the-art readability models before and after the change and
investigated the impact of the change on the resulting readability score.

Pantiuchina et al. (2018) analyze commit messages to extract the intent of
the developer to improve certain static source code metrics related to software
quality. In contrast to their work, we are not extracting the intent to improve
certain static code metrics but instead focus on overall improvement to mea-
sure the delta of a multitude of metrics between the improving commit and its
parents. Developers may not use the terminology Pantiuchina et al. base their
keywords on, e.g., instead of writing reduce coupling or increase cohesion the
developer may simply write refactoring or simplify code.

In contrast to the previous studies, we relate developer intents to improve
the quality either by perfective maintenance or by corrective maintenance to
change size metrics and static source code metrics. In addition, we also look
at mean static source code metrics per ﬁle which are the target of quality
improvements.

4 Case Study Design

The goal of our case study is to gather empirical data about what changes when
a developer intents to improve the quality of the code base in comparison to
other changes. To achieve this, we ﬁrst sample a number of commits from
our selected study subjects. This sample is classiﬁed by two researchers into
two categories of quality improving and other changes. The classiﬁcation into
categories is only done via the commit message as it expresses the intent of
the developer on what the change should achieve.

This data is then used to train a model that can conﬁdently classify the rest
of our commit messages. The classiﬁed commits are then used to investigate
the static source code metric value changes to accept or reject our hypotheses
in the conﬁrmatory part of our study. After that, we investigate the metric
values before the change is applied in the exploratory part of our study.

4.1 Data and Study Subject Selection

The data used in our study is a SmartSHARK (Trautsch et al., 2017) database
taken from Trautsch et al. (2020a). We use all projects and commits in the
database. However, only commits that change production code and which are
not empty are considered. For each change in our data, we extract a list of

What really changes when developers intend to improve their source code

9

Table 1: Case study subjects with time frame and distribution of commits.
All considered commits (#C), sample size (#S), sample perfective commits
(#SP), sample corrective commits (#SC), all perfective commits (#AP), all
corrective commits (#AC)

Project

Timeframe

#C

#S #SP #SC

archiva
calcite
cayenne
commons-bcel
commons-beanutils
commons-codec
commons-collections
commons-compress
commons-conﬁguration
commons-dbcp
commons-digester
commons-imaging
commons-io
commons-jcs
commons-jexl
commons-lang
commons-math
commons-net
commons-rdf
commons-scxml
commons-validator
commons-vfs
eagle
falcon
ﬂume
giraph
gora
helix
httpcomponents-client
httpcomponents-core
jena
jspwiki
knox
kylin
lens
mahout
manifoldcf
mina-sshd
niﬁ
opennlp
parquet-mr
pdfbox
phoenix
ranger
roller
santuario-java
storm
streams
struts
systemml
tez
tika
wss4j
zeppelin

2005-2018
2012-2018
2007-2018
2001-2019
2001-2018
2003-2018
2001-2018
2003-2018
2003-2018
2001-2019
2001-2017
2007-2018
2002-2018
2002-2018
2002-2018
2002-2018
2003-2018
2002-2018
2014-2018
2005-2018
2002-2018
2002-2018
2015-2018
2011-2018
2011-2018
2010-2018
2010-2019
2011-2019
2005-2019
2005-2019
2002-2019
2001-2018
2012-2018
2014-2018
2013-2018
2008-2018
2010-2019
2008-2019
2014-2018
2008-2018
2012-2018
2008-2018
2014-2019
2014-2018
2005-2019
2001-2019
2011-2018
2012-2019
2006-2018
2012-2018
2013-2018
2007-2018
2004-2018
2013-2018

3,914
1,987
3,738
884
577
828
1,827
1,598
2,075
1,034
1,256
682
1,036
788
1,469
3,261
4,675
1,092
529
479
1,573
1,136
582
1,547
1,489
854
569
2,199
2,399
2,598
8,698
4,326
1,131
6,789
1,370
2,075
2,867
1,281
3,299
1,763
1,228
8,256
7,835
2,213
2,435
1,455
2,839
911
2,945
3,860
2,359
2,581
2,455
1,836

79
40
75
18
12
17
37
32
42
21
26
14
21
16
30
66
94
22
11
10
32
23
12
31
30
18
12
44
48
52
174
87
23
136
28
42
58
26
66
36
25
166
157
45
49
30
57
19
59
78
48
52
50
37

35
8
31
9
5
12
27
17
23
15
16
10
15
10
20
50
66
13
9
6
18
11
5
7
5
4
3
8
22
25
88
32
3
40
9
16
10
10
12
22
7
81
23
10
15
14
24
7
21
21
8
11
22
11

17
14
14
6
2
1
3
6
7
3
0
2
3
1
1
6
10
5
0
2
6
8
4
13
14
6
4
9
16
12
34
25
10
40
9
15
21
6
18
6
9
69
83
20
13
5
9
2
18
25
27
10
10
6

#AP

1,478
565
1,470
588
317
619
1,185
873
1,027
672
744
476
613
400
873
2,182
2,981
585
341
256
900
628
104
255
266
201
182
552
1,113
1,326
4,163
1,523
266
1,904
321
836
602
381
592
805
439
3,934
828
434
869
627
987
264
1,191
921
443
705
712
333

#AC

1,005
665
1,007
171
130
76
200
317
253
211
113
96
171
162
199
420
574
246
35
76
296
207
199
676
591
281
141
580
639
544
1,424
941
306
2,163
479
467
1,164
396
1,052
275
316
2,904
4,545
908
723
406
716
196
682
1,416
1,223
740
702
699

125,482

2,533

1,022

685

47,852

35,124

10

Alexander Trautsch et al.

changed ﬁles, the number of changed lines, the number of hunks1, and the
delta as well as the previous and current value of source code metrics from
the changed ﬁles between the parent and the current commit. To create our
ground truth sample, we randomly sample 2% of commits per project rounded
up for manual classiﬁcation.

The data consists of Java open source projects under the umbrella of the
Apache Software Foundation2. All projects use an issue tracking system and
were still active when the data was collected. Each project consist of at least
100 ﬁles and 1000 commits and is at least two years old. Table 1 shows every
project, the number of commits and the years of data we consider for sampling.
In addition, we include the number of perfective and corrective commits for
our ground truth and ﬁnal classiﬁcation.

4.2 Change Type Classiﬁcation Guidelines

As we are not relying on a keyword based approach and there is no existing
guideline for this kind of classiﬁcation, we created a guideline based on Herzig
et al. (2013). Our ground truth consists of a sample of changes which we
manually classiﬁed into perfective, corrective, and other changes. We do not
consider adaptive changes as separate a category. Instead, we include them
in the other changes. The reason is that we focus on internal and external
quality improvements and map perfective to internal quality and corrective
to external quality. Every commit message is inspected independently by two
researchers with software development experience. The inspection is using a
graphical frontend that loads the sample and displays the commit message
which can then be assigned a label by each researcher independently. If the
commit message does not provide enough information, we inspect additional
linked information in the form of bug reports or the change itself. In case of a
link between the commit message and the issue tracking system, we inspect the
bug report and determine if it is a bug according to the guidelines by Herzig
et al. (2013). We perform this step because the reporter of a bug sometimes
assigns a wrong type. We deﬁned the guidelines listed in Table 2 used by
both researchers for the classiﬁcation of changes. The deep learning model for
our ﬁnal classiﬁcation of intents only receives the commit messages. This is
a conscious trade-oﬀ. On the one hand we want the ground truth to be as
exact as possible, on the other hand we want to keep the automatic intent
classiﬁcation as simple as possible. The results of our ﬁne-tuning evaluation
(Table 3) show that the model does not need the additional data from changes
and issue reports to perform well.

Both researchers achieve a substantial inter-rater agreement (Landis and
Koch, 1977) with a Kappa score of 0.66 (Cohen, 1960). Disagreements are
discussed and assigned a label both researchers agree upon after discussion.

1 An area within a ﬁle that is changed.
2 https://www.apache.org

What really changes when developers intend to improve their source code

11

The disagreement front end shows both prior labels anonymized in random
order.

In contrast to the classiﬁcation by Mauczka et al. (2015) and Hattori and
Lanza (2008), we do not categorize release tagging, license or copyright cor-
rections as perfective. Our rationale is that these changes are not related to
the code quality, which is our main interest in this study.

In Mauczka et al. (2015) the researchers selected six projects and seven
developers with personal commitment and provided the developers with the
commit messages that they then labeled according to diﬀerent classiﬁcation
schemes. One of which is the Swanson classiﬁcation which matches our study.
Each developer labeled a sample of commit messages from their respective
project. As we are focused on Java we also use the Java projects of the Mauczka
et al. (2015) dataset to validate our guidelines.

Two authors of this paper re-classiﬁed the Java projects from Mauczka
et al. (2015): Deltaspike, Mylyn-reviews and Tapiji. The commit messages
were classiﬁed separately ﬁrst. Disagreements were then resolved together in a
separate session. In the ﬁrst session both authors achieve a substantial inter-
rater agreement (Landis and Koch, 1977) with a Kappa score of 0.62 (Cohen,
1960).

Aside from the classiﬁcation diﬀerences regarding release tagging, license
or copyright changes, we noticed further diﬀerences. Several commits contain
some variation of “minor bugﬁxes” which are classiﬁed as perfective mainte-
nance by the developers or both corrective and perfective, whereas we classify
them as corrective. Additionally, code removal or test additions were not classi-
ﬁed as perfective changes by the developers, but rather as corrective changes.
This reveals a diﬀerence of perspective between researchers and developers.
We consider pure code removal and test additions as perfective instead of cor-
rective as we think of corrective changes as improving external quality, e.g.,
by ﬁxing a customer facing bug. The data also contains clean-up and removal
messages without a hint of an underlying bug which are classiﬁed as corrective
by the developers. Based on the information available to us, we cannot decide
if these are misclassiﬁcations by the developers, the result of diﬀerences in
the classiﬁcation guidelines, or misclassiﬁcations by us due to lack of in-depth
knowledge about the projects.

The authors achieve a substantial inter-rater agreement (Landis and Koch,

1977) with the developers yielding a Kappa score of 0.63 (Cohen, 1960).

4.3 Deep Learning For Commit Intent Classiﬁcation

In order to use all available data, we use a deep learning model that classiﬁes
all data which is not manually classiﬁed into perfective, corrective or other.
Due to the size of state-of-the-art deep learning models and the computing
requirements for training them, a current best practice is to use a pre-trained
model which was trained unsupervised on a large data set. The model is then
ﬁne-tuned on labeled data for a speciﬁc task.

12

Alexander Trautsch et al.

Table 2: Classiﬁcation rules and examples, footnotes denote diﬀerent commit
messages from our data.

A change is classiﬁed as perfective if. . .

1. the commit message says code is removed or marked as deprecated.
2. code is moved to new packages.
3. generics are introduced, new Java features are used, existing code is switched to

collections, or class members are switched to ﬁnal.

4. documentation is improved or example code is updated.
5. static analysis warnings are ﬁxed even though no related bug is reported.
6. code is reformatted or the readability is otherwise improved (e.g. whitespace ﬁxes

or tabs to spaces).

7. existing code is cleaned up, simpliﬁed, or its eﬃciency improved.
8. dependencies are updated.
9. developer tooling is improved, e.g., build scripts or logging facilities.

10. the repository layout is cleaned, e.g., by removing compiled code or maintaining

.gitignore ﬁles.

11. tests are improved or added.
Examples: Eliminated unused private ﬁeld. JIRA: DBCP-2553 Because of other null
checks it was already impossible to use the ﬁeld. Thus, this is clean up. [CODEC-127]
Non-ascii characters in source ﬁles4 While the linked issue is a bug, it only aﬀects IDEs
for developers and not the compiled code. Thus, this is an improvement of developer
tooling. JEXL-240: Javadoc5 The message indicates that this commit only improved
the code comments. Therefore, it is classiﬁed as perfective.

A change is classiﬁed as corrective if. . .

1. the commit message mentions bug ﬁxes.
2. the commit message or the linked issue mentions that a wrong behaviour is ﬁxed.
3. the commit message or the linked issue mentions that a NullPointerException is

ﬁxed.

4. a bug report is linked via the commit message that is of type bug and is not just a

feature request in disguise (see Herzig et al. (2013)).

Examples: KYLIN-940 ,ﬁx NPE in monitor module ,apply patch from Xiaoyu Wang6
This ﬁxes a NullPointerException that is visible to the end user. owl syntax checker (bug
ﬁxes)7 Fixes a wrong behavior.

A change is classiﬁed as other if. . .

1. the commit message mentions feature or functionality addition.
2. the commit message mentions license information or copyrights changes.
3. the commit message mentions repository related information with unclear purpose,

e.g., merges of branches without information, tagging of releases.

4. the commit message mentions that a release is prepared.
5. an issue is linked via the commit message that requests a feature.
6. any of the 1-5 are tangled with a perfective or corrective classiﬁcation.
Examples: KYLIN-715 ﬁx license issue8 License changes or additions are not direct
improvements of source code. Support the alpha channel for PAM ﬁles. Fix the alpha
channel order when reading and writing. Add various tests.9 This change adds support
for a new feature, ﬁxes something and adds tests, it is therefore highly tangled and we
do not classify it as either or both.

What really changes when developers intend to improve their source code

13

Table 3: Change classiﬁcation model performance comparison.

Model

Acc.

F1 MCC Description

von der Mosel et al. (2021)

0.80

0.79

0.70 BERT model pre-trained on soft-
ware engineering data, ﬁne-tuned
with only commit messages

Ghadhab et al. (2021)

0.78

0.80

- BERT model

natural
changes.

language,

pre-trained

on
includes code

Gharbi et al. (2019)

-

0.46

- Multi-label active learning, only

Levin and Yehudai (2017)

0.76

H¨onel et al. (2019)

0.80

-

-

commit message

- Keywords and code changes, Ran-

-

dom Forest model
LogitBoost model,
density.

includes code

To achieve a high performance, we use seBERT (von der Mosel et al.,
2021), a model that is pre-trained on textual software engineering data in
two common Natural Language Processing (NLP) tasks. Masked Language
Model (MLM) and Next Sentence Prediction (NSP) which predict randomly
masked words in a sentence and the next sentence respectively. Combined, this
allows the model to learn a contextual understanding of the language. While
von der Mosel et al. (2021) include a similar benchmark based on our ground
truth data, it only used the perfective label, i.e., a binary classiﬁcation to
demonstrate text classiﬁcation for software engineering data. In our study, we
measure performance of the multi-class case with all three labels, perfective,
corrective and other. Within this study, we ﬁrst use our ground truth data to
evaluate the multi-class performance of the model. We perform a 10x10 cross-
validation which splits our data into 10 parts and uses 9 for ﬁne-tuning the
model and one for evaluating the performance. The ﬁne-tuning itself splits the
data into 80% training and 20% validation. The model is then ﬁne-tuned and
evaluated on the validation data for each epoch. At the end the best epoch is
chosen to classify the test data of the fold. This is repeated 10 times for every
fold which yields 100 performance measurements.

Our experiment shows suﬃcient performance comparable to other state-
of-the-art models for commit classiﬁcation. We provide the ﬁnal ﬁne-tuned
model as well as the ﬁne-tuning code as part of our replication kit for other
researchers. Performance wise our model is comparable to Ghadhab et al.
(2021) and improves performance compared other studies, e.g., Gharbi et al.
(2019); Levin and Yehudai (2017). However, we note that we ﬁne-tuned the
model with only the labels used in our study, i.e., perfective, corrective and
other. Therefore, it cannot be used or directly compared with models that
support other commit classiﬁcation labels. This would require the same data
and labels, we can only compare the given model performance metrics, which
we do in Table 3. If we look at the overview of commit classiﬁcation studies by
AlOmar et al. (2021) we can see that our model outperforms the other models
for comparable tasks where accuracy or F-measure is given. While this is

14

Alexander Trautsch et al.

Table 4: Static source code metrics and static analysis warning severities used
in this study including the expected direction of their values in quality increas-
ing commits.

Name and Description

Cyclomatic Complexity (McCabe, 1976)
The number of independent control-ﬂow paths.

Logical Lines of Code
Number of lines in a ﬁle without comments and empty lines.

Nesting Level else-if
Maximum of nesting level in a ﬁle.

Number of parameters in a method
The sum of all parameters of all methods in a ﬁle.

Clone Coverage
Ratio of code covered by duplicates.

Comment lines of code
Sum of commented lines.

Comment density
Ratio of CLOC to LLOC.

API Documentation
Number of documented public methods, +1 if class is documented.

Abbrev

McCC

LLOC

NLE

(cid:108)

↓

↓

↓

NUMPAR ↓

CC

CLOC

CD

AD

Number of Ancestors
Number of classes, interfaces, enums from which the class is inherited. NOA

Coupling between object classes
Number of used classes (inheritance, function call, type reference).

Number of Incoming Invocations
Other methods that call the current class.

Minor static analysis warnings
E.g., brace rules, naming conventions.

Major static analysis warnings
E.g., type resolution rules, unnecessary/unused code rules.

CBO

NII

Minor

Major

Critical static analysis warnings
E.g., equals for string comparison, catching null pointer exceptions.

Critical

↓

↑

↑

↑

↓

↓

↓

↓

↓

↓

evidence that our model can perform our required commit intent classiﬁcation
a throughout comparison of diﬀerent commit intent classiﬁcation approaches
is not within the scope of this study.

4.4 Metric Selection

The metric selection is based on the Columbus software quality model by
Bakota et al. (2011). The metrics are selected from the current version of the
model also in use as QualityGate (Bakota et al., 2014). The current model
consists of 14 static source code metrics related to size, complexity, documen-
tation, re-usability and fault-proneness. While the quality model provides us

What really changes when developers intend to improve their source code

15

with a selection of metrics, we do not use it directly as it requires a baseline
of projects before estimating quality of a candidate project.

Table 4 shows the metrics utilized in this study, a short description, and the
direction which we assume they change in quality improving commits. As most
of the metrics are size and complexity metrics, we expect that their values de-
crease in comparison to all other commits. The metrics we expect to increase
in quality improving commits are commented lines of code, comment density,
and API documentation, as added documentation should increase these met-
rics. The three bottom rules consist of static analysis warnings from PMD10
aggregated by severity for every ﬁle. We are of the opinion that this selection
strikes a good balance of size, complexity, documentation, clone, and coupling
based metrics.

As we are interested in static source code metrics in a commit granularity,
we sum the metrics values for all ﬁles that are changed within a commit. In
addition, we extract meta information about each change. The static source
code metrics are provided by a SmartSHARK plugin using the OpenStaticAn-
alyzer11. To answer our research question, we provide the delta of the metric
value changes as well as their current and previous value.

4.5 Analysis Procedure

For our conﬁrmatory study as part of RQ1, we compare the diﬀerence be-
tween two samples. To choose a valid statistical test of whether there is a
diﬀerence between both samples, we ﬁrst perform the Shapiro-Wilk test (Wilk
and Shapiro, 1965) to test for normality of each sample. Since we found that
the data is non-normal, we perform the Mann-Whitney U-test (Mann and
Whitney, 1947) to evaluate if the metric values of one population dominates
the other. Since we have an expectation about the direction of metric changes,
we perform a one-sided Mann-Whitney U test. The H0 hypothesis is that both
samples are the same, the alternative hypothesis is that one sample contains
lower or higher values depending on our expectation. The expected direction
of the metric value change is noted in the last column of Table 4.

As our data contains a large number of metrics, we cannot assume a sta-
tistical test with p < 0.05 is a valid rejection of a H0 hypothesis. To mitigate
the problem posed by a high number of statistical tests, we perform Bonfer-
roni correction (Abdi, 2007). We choose a signiﬁcance level of α = 0.05 with
Bonferroni correction for 192 statistical tests. They consist of four size metrics
with two groups and three statistical tests as well as 14 source code metrics
with two groups and three statistical tests (normality tests for two samples
and Mann-Whitney U for diﬀerence between samples). The second part is
repeated for RQ2. We reject the H0 hypothesis that there is no diﬀerence
between samples at p < 0.00026.

10 https://pmd.github.io/
11 https://openstaticanalyzer.github.io/

16

Alexander Trautsch et al.

To calculate the eﬀect size of the Mann-Whitney U test, we use Cliﬀ’s
d (Cliﬀ, 1993) as a non-parametric eﬀect size measure. We follow a common
interpretation of d values Griessom and Kim (2005): d < 0.10 is negligible,
0.10 ≤ d < 0.33 is small, 0.33 ≤ d < 0.474 is medium and d ≥ 0.474 is large.
We provide the eﬀect size for every diﬀerence that is statistically signiﬁcant.
We report the results visually with box plots. The box plots shows three
groups: all, perfective and corrective, this allows us to show the values for each
metric for each group and serves to highlight the diﬀerences. Additionally, we
report the diﬀerences between each group and its counterpart, e.g., perfective
and not perfective in the tables where we report the statistical diﬀerences.

A more detailed description of the procedure for each hypothesis follows.
For H1, we compare the structure of quality improving changes with every
other change. We compare the size (changed lines) and diﬀusion (number of
hunks, number of changed ﬁles) to evaluate the hypothesis. We visualize the
results with box plots and report results for statistical tests to determine if
the diﬀerence in samples is statistically signiﬁcant.

For H2, we also visualize the results via box plots. As most of the diﬀer-
ences hover around zero, we transform the data before plotting via sign(x) ·
log(abs(x + 1)). As we are interested in the diﬀerences between changes of
metric values, we also require x (cid:54)= 0 : ∀x ∈ X where X is the complete, non-
transformed data set for the visualizations. Due to the diﬀerence in changes,
we provide our data size corrected, e.g., the delta of McCC is divided by the
modiﬁed lines. Additionally, we report the percentage of data that is non-zero
to indicate how often the measurements are changing in our data. In addition
to the visualization, we provide a table with diﬀerences between the samples
and statistical test results.

As part of our exploratory study for answering RQ2, we also provide box
plots of our metric values. Instead of transformed delta values, we provide the
raw averages per ﬁle in a change before the change was applied. In addition,
we provide the median values of all of our metrics before the change was
applied. In this part, we apply a two-sided Mann-Whitney U test as we have
no expectation of the direction the metrics change into for the categories. To
complement the visualization, we also provide density plots for both categories.
They show the overlap between the perfective and corrective changes.

4.6 Replication Kit

All data and source code can be found in our replication kit (Trautsch et al.,
2021). In addition, we provide a small website for this publication that contains
all information and where the ﬁne-tuned model can be tested live12.

12 https://user.informatik.uni-goettingen.de/˜trautsch2/emse 2021/

What really changes when developers intend to improve their source code

17

5 Results

In this section, we ﬁrst present the results for evaluating our hypotheses of our
ﬁrst research question. After that, we describe the results of the exploratory
part of our study for our second research question.

5.1 Conﬁrmatory Study

We ﬁrst present the results of our conﬁrmatory study and evaluate our hy-
potheses. These results answer our ﬁrst research question: Does developer in-
tent to improve internal or external quality have a positive impact on software
metric values?

18

Alexander Trautsch et al.

Table 5: Statistical test results for perfective and corrective commits, Mann-
Whitney U test p-values (p-value) and eﬀect size (d) with category, n is neg-
ligible, s is small. Statistically signiﬁcant p-values are bolded.

Perfective

Corrective

Metric

#lines added
#lines deleted
#ﬁles modiﬁed
#hunks

p-value

<0.0001
<0.0001
0.2081
<0.0001

d

p-value

d

0.20 (s) <0.0001
0.15 (s) <0.0001
- <0.0001
0.01 (n) <0.0001

0.21 (s)
0.16 (s)
0.22 (s)
0.22 (s)

Fig. 1: Commit size distribution over all projects for all, perfective and cor-
rective commits. Fliers are omitted.

5.1.1 Results H1

Figure 1 shows the distribution of sizes between perfective, corrective, and
all commits. Table 5 shows the statistical test results for the diﬀerences. We
can see that perfective commits tend to add fewer lines but instead remove
more lines as the other commits. When we calculate a median delta between
all commits and perfective commits, we ﬁnd a diﬀerence of 28 for added lines
and -2 for deleted lines. While the eﬀect sizes are negligible to small, we can
see this diﬀerence also in Figure 1. The diﬀusion of the change over ﬁles is
also diﬀerent, however for the number of modiﬁed ﬁles the diﬀerence is not
signiﬁcant for perfective commits.

Corrective commits also tend to add less code, while they do not delete as
much, the diﬀerence in added and deleted lines is also statistically signiﬁcant.
While the eﬀect size is small, we can see the diﬀerence in Figure 1. For correc-
tive commits, we can also see a diﬀerence in the number of ﬁles changed and
the number of hunks modiﬁed. This diﬀusion of the change via the number
of ﬁles and hunks is also statistically signiﬁcant although, again, with a small
eﬀect size.

We can conclude, that perfective commits tend to remove more lines, and
are generally adding fewer lines to the repository. Corrective commits delete
fewer lines and add fewer lines than other commits. Corrective commits are
also distributed over less hunks and less ﬁles than other commits.

allperf.corr.050100150#lines addedallperf.corr.0255075100#lines deletedallperf.corr.2468#files modifiedallperf.corr.0102030#hunksWhat really changes when developers intend to improve their source code

19

Table 6: Percentage of commits where the metric value is not zero on all
commits (%nz), perfective commits (%nz p) and corrective commits (%nz c).

Metric

%NZ %NZ P %NZ C

51.03
McCC
74.69
LLOC
NLE
36.76
NUMPAR 35.93
49.41
CC
51.56
CLOC
76.07
CD
27.19
AD
10.51
NOA
30.89
CBO
27.08
NII
36.15
Minor
19.87
Major
7.23
Critical

31.01
60.93
23.92
24.44
37.81
46.52
66.48
20.63
6.96
22.52
17.78
27.02
13.23
4.20

57.70
77.99
34.28
24.98
55.14
42.51
77.35
15.82
3.62
22.22
21.09
29.77
14.77
4.95

We accept H1 that intended quality improvements are smaller than
other changes.
Perfective and corrective commits tend to addfewer lines, perfective
commits remove more lines. The eﬀect size is negligible to small in all
cases.

5.1.2 Results H2

We ﬁrst note that no metric value changes for each instance of our data. This
can be seen in Table 6, which shows the percentages for each metric value for
perfective, corrective, and all changes. We can see some diﬀerences between
changes, e.g., critical PMD warnings only change in about 7% of commits while
LLOC changes in about 75%. Some diﬀerences are also between categories, e.g.,
McCC changes in 31% of perfective changes and in 57% of corrective changes.
To evaluate H2, we present the diﬀerences in all changes visually as box

plots in Figure 2.

In addition, we provide Table 7 which shows the Mann-Whitney U test (Mann

and Whitney, 1947) p-values, and eﬀect sizes for diﬀerences between the types
of commits. We can see that most metric values are diﬀerent depending on
whether they are measured in perfective, corrective, or all other commits. In
the following, we discuss the diﬀerences for each measured metric value. A
description for each metric and the expected direction of metric value change
is shown in Table 4.

McCC: the cyclomatic complexity of perfective changes is smaller than
for other changes. Even when we do not account for the size of the change.
This is expected as some perfective commits mention simpliﬁcation of code.
For perfective commits the eﬀect size is medium. Corrective commits how-
ever have higher McCC than other commits. This can be seen in Figure 2.

20

Alexander Trautsch et al.

Fig. 2: Static source code metric value changes in all, perfective and corrective
commits divided by changed lines. Fliers are omitted.

Table 7: Statistical test results for perfective and corrective commits, Mann-
Whitney U test p-values (p-value) and eﬀect size (d) with category, n is neg-
ligible, s is small, m is medium. Statistically signiﬁcant p-values are bolded.
All values are normalized for changed lines.

Perfective

Corrective

Metric

p-val

d

p-val

d

<0.0001
McCC
<0.0001
LLOC
NLE
<0.0001
NUMPAR <0.0001
1.0000
CC
<0.0001
CLOC
1.0000
CD
<0.0001
AD
<0.0001
NOA
<0.0001
CBO
<0.0001
NII
<0.0001
Minor
<0.0001
Major
<0.0001
Critical

1.0000
0.39 (m)
1.0000
0.45 (m)
0.27 (s)
1.0000
0.25 (s) <0.0001
- <0.0001
0.16 (s) <0.0001
- <0.0001
0.02 (n) <0.0001
0.08 (n) <0.0001
0.19 (s) <0.0001
0.19 (s) <0.0001
0.19 (s) <0.0001
0.12 (s) <0.0001
0.05 (n) <0.0001

-
-
-
0.09 (n)
0.12 (s)
0.05 (n)
0.16 (s)
0.08 (n)
0.07 (n)
0.06 (n)
0.02 (n)
0.05 (n)
0.05 (n)
0.03 (n)

allperf.corr.-0.05 0.00 0.05 0.10log McCC delta + 1allperf.corr.-0.20 0.00 0.20 0.40log LLOC delta + 1allperf.corr.-0.02 0.00 0.02 0.04 0.06log NLE delta + 1allperf.corr.-0.04-0.02 0.00 0.02 0.04 0.06log NUMPAR delta + 1allperf.corr.-0.00-0.00 0.00 0.00 0.00log CC delta + 1allperf.corr.-0.10 0.00 0.10 0.20log CLOC delta + 1allperf.corr.-0.00-0.00 0.00 0.00 0.00log CD delta + 1allperf.corr.-0.00 0.00 0.00log AD delta + 1allperf.corr.-0.01 0.00 0.01 0.02log NOA delta + 1allperf.corr.-0.02 0.00 0.02 0.04 0.06log CBO delta + 1allperf.corr.-0.03 0.00 0.03 0.05 0.07log NII delta + 1allperf.corr.-0.03 0.00 0.03 0.05 0.07log Minor delta + 1allperf.corr.-0.02 0.00 0.02 0.04log Major delta + 1allperf.corr.-0.02 0.00 0.02 0.04log Critical delta + 1What really changes when developers intend to improve their source code

21

The median of corrective commits is higher than for the other commits. Our
assumption about McCC being lower in all quality improving commits is not
met in this case. While it makes sense that corrective commits add complexity,
the comparison here is one of stochastic dominance between all other commits
and only corrective commits, not if corrective commits remove or add McCC.
Thus, this means that changes in corrective commits are more complex than
those of other changes.

LLOC: the diﬀerence of LLOC is the most pronounced in our data. We ﬁnd
that even when we do not correct for size of the change the diﬀerence between
perfective and other changes in LLOC is the most pronounced. While manually
classifying the commits, we found that often code is removed because it was
marked as deprecated before or it was no longer needed due to other reasons.
The eﬀect size for perfective commits is medium. For corrective commits, we
can see the same result as for McCC. While we assumed that bug ﬁxes usually
add code, we did not expect them to dominate all other commits including
feature additions.

NLE: the nesting level if-else is smaller in perfective commits. We expect
this is due to simpliﬁcation and removal of complex code. When we look at the
box plot in Figure 2 it shows a noticeable diﬀerence. This means simpliﬁcation
is a high priority when improving code quality in perfective commits. For
corrective commits, we can see the same eﬀect as previously seen for McCC
and LLOC. The NLE is not lower but higher for corrective commits. This is
more evidence for the fact that bug ﬁxes add more complex code. There may
be a timing factor involved, e.g., if bug ﬁxes are quick ﬁxes, they would add
more complex code without a more complex refactoring which would decrease
the complexity again.

NUMPAR: the number of parameters in a method is also diﬀerent for
perfective commits. This may be a hint of the type of perfective maintenance
performed the most in perfective commits. The manual classiﬁcation showed a
lot of commit messages that claimed a simpliﬁcation of the changed code. This
metric would also be impacted by a simpliﬁcation or refactoring operations.
Corrective commits also show less additions in this metric, while it only has
a negligible eﬀect size it is still statistically signiﬁcant. Fixing bugs seems to
include some code reduction or at least less addition of parameters for methods.
CC: the clone coverage is not diﬀerent for perfective commits. We would
have expected that it is decreasing in perfective commits. However, it seems
that clone removal is not a big part of perfective maintenance in our study
subjects, which contradicts our expectation. Corrective commits contain a
lower clone coverage, however. This could either be because corrective commits
introduce fewer new clones than other commits or because they remove more.
A possible reason for clone removal may be the correction of copy and paste
related bugs.

CLOC: the comment lines of code show a diﬀerence for perfective commits
and corrective commits. While we expected the CLOC to increase in both types
of quality improving commits the eﬀect size is higher in perfective commits.

22

Alexander Trautsch et al.

It seems that bug ﬁxing operations do not add enough comment lines to show
a larger diﬀerence here for corrective commits.

CD: the comment density of perfective commits is not statistically signiﬁ-
cantly diﬀerent from other commits. We would have expected a diﬀerence here
because perfective maintenance should include additional comments on new or
previously uncommented code. We can see a diﬀerence for corrective commits
here. This shows that the density of comments is also improving in bug ﬁxing
operations probably due to clariﬁcations for parts of the code that were ﬁxed.
AD: the API documentation metric does change in perfective and correc-
tive commits compared to other commits. A reason could be that perfective
commits do add API documentation to make the diﬀerence signiﬁcant. Correc-
tive changes that introduce code in our study subjects seem to almost always
include API documentation, therefore we can see a diﬀerence here. However,
the eﬀect size is negligible in both cases.

NOA: the number of ancestors is lower in perfective commits as expected.
This metric would be aﬀected in simpliﬁcation and clean up maintenance op-
erations. For corrective commits we can also see a lower value, this hints at
some clean up operations happening during bug ﬁxing.

CBO: the coupling between objects is lower after perfective commits. This
is expected due to class removal and subsequent decoupling of classes. For cor-
rective commits we can also see a diﬀerence. While the eﬀect size is negligible,
there is some code clean up happening during bug ﬁxes, e.g., NOA and CC
are also lower in corrective than in other commits.

NII: the number of incoming invocations is lower in both perfective and
corrective commits. However, the eﬀect size is small in perfective and negligible
in corrective commits. It seems reasonable to see a diﬀerence in this metric,
because in the case of perfective commits, we have lots of source code removal.
However, there are also maintenance activities which are decoupling classes
which would also impact this metric. Corrective maintenance seems to involve
only limited decoupling operations, also seen in CBO.

Minor: The PMD warnings of minor severity are diﬀerent in both types of
changes. However, we can see that the eﬀect size is larger for perfective changes
which makes sense as those warnings can be part of perfective maintenance.

Major: The PMD warnings of major severity are also diﬀerent in both
types of changes. We can see the diﬀerence in eﬀect size again and we expect
the reason is the same as for Minor.

Critical: The PMD warnings of critical severity are diﬀerent for both types
of changes. Here, the eﬀect size is negligible for both types. However, as they
are only changed in about 7% of our commits, they are not changing often
regardless of commit type.

There are signiﬁcant diﬀerences between perfective and corrective
changes. We reject H2 that intended quality improvements have a
positive impact on quality metric values.

What really changes when developers intend to improve their source code

23

5.2 Summary RQ1

In summary, we have the following results for RQ1.

RQ1 Summary
While intended quality improvements by developers yield measurable
diﬀerences in almost all metrics we ﬁnd that not all metric values are
changing in the expected direction.

Perfective changes
Perfective commits have a positive eﬀect on metric values that measure
code complexity through the size, conditional statements, number of
parameters, and coupling. For two metrics we do not ﬁnd the expected
diﬀerence to other commits. Code clones and comment density metric
values are not statistically signiﬁcantly diﬀerent in perfective commits.

Corrective changes
Only for two metrics, we observe a non-negligible and statistically sig-
niﬁcant change that we predicted. For LLOC, McCC and NLE, we
observe the opposite of the expectation, which indicates that bug ﬁxes
add complex code.

5.3 Exploratory Study

To answer our RQ2: What kind of ﬁles are the target of internal or external
quality improvements? We conduct an exploratory study. We present the re-
sults which ﬁles are changed in which change category with respect to their
metric values. The extracted metrics are considered on a per-change basis, i.e.,
we divide the metrics by the number of changed ﬁles to get an average metric
value per ﬁle.

Figure 3 shows box plots for the metric values of ﬁles before the change
is applied. We can see that, perfective changes are not necessarily applied to
complex ﬁles. If we compare the median values in Table 8 we can see that
perfective changes are applied to smaller, simpler ﬁles than the average or
corrective change. McCC, LLOC, NLE, NUMPAR and CBO are lower for
the ﬁles which receive perfective changes, while CLOC, CD, AD are higher.
This means that less complex and well documented ﬁles are often the target
of perfective changes. If we look at corrective changes we see that they are
more complex and usually larger ﬁles. McCC, LLOC, NLE, NUMPAR, CBO,
NII as well as Minor, Major and Critical are higher than other, or perfective
changes. As we consider the metric values before the change is applied they
can be considered pre-bugﬁx. However, when we consider our results for RQ1
the corrective changes usually increase the complexity even further.

Table 9 show the results of our statistical tests. Analogous to RQ1 we com-
pare the diﬀerence between perfective and non-perfective as well as corrective

24

Alexander Trautsch et al.

Fig. 3: Static source code metrics before the change is applied. Fliers are
omitted.

Table 8: Median metric values per ﬁle before the change is applied.

Metric

All

Perfective Corrective

McCC
LLOC
NLE
NUMPAR
CC
CLOC
CD
AD
NOA
CBO
NII
Minor
Major
Critical

21.78
186.98
9.60
16.06
0.04
46.25
0.25
0.50
1.00
9.67
8.00
7.00
2.00
0.00

18.78
163.75
8.33
15.00
0.04
55.00
0.32
0.67
1.00
8.00
8.50
6.00
1.25
0.00

33.23
264.18
14.00
22.00
0.05
54.00
0.25
0.46
1.00
14.00
9.50
9.67
3.00
0.00

and non-corrective. While most metric diﬀerences are statistically signiﬁcant,
we observe only some small eﬀect sizes for the comment related metrics while
the rest is negligible.

Figure 4 shows another perspective on our data in the form of a direct
comparison of the density between perfective and corrective changes. We can

allperf.corr.050100150McCC/changedfilesallperf.corr.02505007501000LLOC/changedfilesallperf.corr.0204060NLE/changedfilesallperf.corr.020406080100NUMPAR/changedfilesallperf.corr.0.00.20.4CC/changedfilesallperf.corr.0100200300CLOC/changedfilesallperf.corr.0.00.20.40.60.81.0CD/changedfilesallperf.corr.0.00.51.01.52.0AD/changedfilesallperf.corr.0246NOA/changedfilesallperf.corr.0204060CBO/changedfilesallperf.corr.0204060NII/changedfilesallperf.corr.02040Minor/changedfilesallperf.corr.051015Major/changedfilesallperf.corr.01234Critical/changedfilesWhat really changes when developers intend to improve their source code

25

Table 9: Statistical test results for perfective and corrective commits regarding
their average metrics before the change, Mann-Whitney U test p-values (p-
value) and eﬀect size (d) with category, n is negligible, s is small, m is medium.
Statistically signiﬁcant p-values are bolded.

Perfective

Corrective

Metric

p-val

d

p-val

d

McCC
LLOC
NLE
NUMPAR
CC
CLOC
CD
AD
NOA
CBO
NII
Minor
Major
Critical

<0.0001
<0.0001
<0.0001
0.6367
<0.0001
<0.0001
<0.0001
<0.0001
0.5109
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001

0.05 (n) <0.0001
0.05 (n) <0.0001
0.04 (n) <0.0001
0.0218
-
0.0011
0.01 (n)
0.12 (s) <0.0001
0.15 (s) <0.0001
0.17 (s) <0.0001
- <0.0001
0.09 (n) <0.0001
0.05 (n) <0.0001
0.04 (n) <0.0001
0.09 (n) <0.0001
0.05 (n) <0.0001

0.08 (n)
0.05 (n)
0.07 (n)
-
-
0.06 (n)
0.15 (s)
0.15 (s)
0.02 (n)
0.07 (n)
0.04 (n)
0.02 (n)
0.04 (n)
0.03 (n)

Fig. 4: Density plot of metric values for perfective and corrective categories
before the change.

2.50.02.55.0log McCC + 10.00.20.40.6Denstiyperf.corr.2.50.02.55.07.5log LLOC + 10.00.20.40.6Denstiyperf.corr.20246log NLE + 10.00.20.40.6Denstiyperf.corr.20246log NUMPAR + 10.00.20.40.6Denstiyperf.corr.024log CC + 10246Denstiyperf.corr.2.50.02.55.07.5log CLOC + 10.00.20.40.6Denstiyperf.corr.024log CD + 101234Denstiyperf.corr.024log AD + 1012Denstiyperf.corr.20246log NOA + 10.00.51.01.5Denstiyperf.corr.20246log CBO + 10.00.20.40.6Denstiyperf.corr.20246log NII + 10.00.20.4Denstiyperf.corr.20246log Minor + 10.00.20.40.6Denstiyperf.corr.2024log Major + 10.00.51.01.5Denstiyperf.corr.024log Critical + 101234Denstiyperf.corr.26

Alexander Trautsch et al.

see that McCC, NLE, LLOC, NUMPAR, CD, CBO, NII and Minor have a
lower density for perfective than for corrective. While the diﬀerences are small
they are noticeable.

RQ2 Summary
The ﬁles that are targets of perfective changes are in median not large
and complex even before the change is applied. Corrective changes are
applied to ﬁles which are in median already complex and large. The
diﬀerences are statistically signiﬁcant for most metrics, however the
eﬀect sizes are negligible to small.

6 Discussion

Our results show that size is diﬀerent in both types of commits in H1. The
size diﬀerence between all commits and perfective as well as corrective commits
shows that both tend to be smaller than other commits. In case of perfective
commits, code is statistically signiﬁcantly more often deleted.

The diﬀerences in change size as well as the increased number of deletions
for perfective commits we found for H1 conﬁrms previous research. The studies
by Mockus and Votta (2000), Purushothaman and Perry (2005) and Alali
et al. (2008) found that perfective maintenance activities are usually smaller.
Mockus and Votta (2000) as well as Purushothaman and Perry (2005) found
that corrective maintenance is also smaller and that perfective maintenance
deletes more code. Another indication that size between maintenance types is
diﬀerent can be seen in the work by H¨onel et al. (2019), which used size based
metrics as predictors for maintenance types and showed that it improved the
performance of classiﬁcation models.

Our results for H2 show statistically signiﬁcant diﬀerences in metric mea-
surements between perfective commits and other commits. This result indi-
cates a conﬁrmation of the measurements used by quality models, as the ma-
jority of metrics change as expected when developers actively improve the
internal code quality. This empirical conﬁrmation of the connection between
quality metrics and developer intent is one of our main contributions and was,
to the best of our knowledge, not part of any prior study. However, there are
several examples of prior work that assumed this relationship.

The publications by McCabe (1976) and Chidamber and Kemerer (1994)
assume that reducing complexity and coupling metrics increases software qual-
ity which is in line with our developer intents. While all metrics are included
in a current ColumbusQM version (Bakota et al., 2014) because we used it as
a basis, the CBO, McCC, LLOC, NOA metrics are also part of the SQUALE
model (Mordal-Manet et al., 2009) AD, NLE, McCC, and PMD warnings are
also part of Quamoco (Wagner et al., 2012). It seems that developers and
the Columbus quality model agree with their view on software quality. We
ﬁnd that most of the metrics used in the quality model change when devel-
opers perceive their change as quality increasing. This is also true for most of

What really changes when developers intend to improve their source code

27

the metrics shared with the SQUALE model and with the Quamoco quality
model. However, the implementation for the metrics may diﬀer between the
models. Our work establishes that all these quality models are directly related
to intended improvements of the internal code quality by the developers.

Surprisingly, we found only few statistically signiﬁcant and non-negligible
diﬀerences for corrective commits. Not all software metric values are changing
into the expected direction for corrective commits. For example, we can see
that McCC, LLOC and NLE are increasing in corrective changes compared
to other commits. While we are not expecting them to decrease for every
corrective commit, we assumed that in comparison to all other commits they
would be decreasing. Even when considering software aging (Parnas, 2001)
we would expect the aging to impact all kinds of changes not just corrective
changes. When we look at popular data sets used in the defect prediction
domain we often ﬁnd coupling, size and complexity software metrics (Herbold
et al., 2021). For example, the popular (as per the literature review from
Hosseini et al. (2017)) data set by Jureczko and Madeyski (2010) uses such
features, but they are also common in more recent data sets, e.g., by Ferenc
et al. (2020) or Yatish et al. (2019).

That the most signiﬁcant diﬀerence is in the size of changes could explain
various recent ﬁndings from the literature, in which size was found to be a
very good indicator both for release level defect prediction (Zhou et al., 2018)
and just-in-time defect prediction (Huang et al., 2017). This could also be an
explanation for possible ceiling eﬀects (Menzies et al., 2008) when such criteria
are used, as the diﬀerence to other changes are relatively small. We believe that
these aspects should be further considered by the defect prediction community
and believe that more research is required to establish causal relationships
between features and defectiveness.

While the work by Peitek et al. (2021) indicates that cyclomatic complex-
ity may not be as indicative of code understandability as expected, we show
within our work that it often changes in quality increasing commits. It seems
that developers associate overall complexity as measured by McCC, NLE,
NUMPAR with code that needs quality improvement. However, as we can see
in the exploratory part of our study the most complex ﬁles are usually not
targeted for quality increasing changes.

Our exploratory study to answer RQ2 about ﬁles that are the target of
quality increasing commits reveals additional interesting data. We show that
perfective maintenance does not necessarily target ﬁles that are in need of it
due to high complexity in comparison to other changes. In fact, low complexity
ﬁles as measured by McCC and NLE are more often part of additional quality
increasing work by the developers. This may hint at problems regarding the
prioritization of quality improvements in the source code. Maybe errors could
have been avoided when perfective changes would have targeted more complex
ﬁles. There could also be eﬀects of diﬀerent developers or a bias for perfective
changes towards simpler code, this warrants future investigation. Corrective
changes, in contrast to perfective changes, are applied to ﬁles which are large
and complex. This was expected, however combined with the results of RQ1

28

Alexander Trautsch et al.

this means that bugs are ﬁxed in complex and large ﬁles and then the ﬁles
get, on average, even more complex and even larger.

Future work could investigate boundary values according to our data.
When we compare the median values of our measurements in Table 8 with
current boundary values from PMD13, we may think that the PMD warning
value of 80 McCC per ﬁle may be too high. A PMD warning triggered at 34
McCC per ﬁle would have warned about at least 50% of the ﬁles that were
in need of a bug ﬁx. However, lowering the boundary will also result in more
warnings for ﬁles that were not target of corrective changes.

6.1 Implications for Researchers

Our results for H1 increase the validity of previous research by conﬁrming
previous results in our study on a larger data set of diﬀerent projects. Our
conﬁrmation that quality increasing changes are smaller than other changes
shows that researchers developing a change classiﬁcation approach can beneﬁt
from including size based metrics.

Our results for H2 show that perfective changes reduce size and complex-
ity metrics in comparison to all other changes. Previous studies investigating
refactorings also found an impact on size and complexity metrics. We are
able to generalize this ﬁnding by providing results of a superset of refactoring
operations, namely perfective changes. This indicates that perfective changes
generally reduce size and complexity metrics. This also indicates that software
quality models that use the aﬀected metrics in their code quality estimations
agree with the developers on what impacts code quality.

Increasing the external quality by ﬁxing bugs, i.e., corrective changes, de-
creases the internal quality, i.e., complexity metric values. Defect prediction
models may assign a higher risk to parts of the code that contained a bug
before as there is an assumption of latent bugs still existing (Kim et al., 2007;
Rahman et al., 2011). Our data provides a ﬁne grained perspective by provid-
ing empirical data which shows that the code quality as measured by static
source code metrics is actually decreasing.

This also has implications for researchers developing and deploying defect
prediction models in practice. The fact that ﬁxing a bug increases the risk
of the ﬁle can lead to problems regarding the acceptance of the model by
practitioners as they have no way of reducing the risk (Lewis et al., 2013).
The results of our study could help to explain the reasons to developers. We
can empirically show that ﬁxing a bug is a complex operation that introduces
even more complexity than other changes, even feature additions. According
to our results, the main driver of complexity in a project are bug ﬁxes and
the only way to combat the rising complexity is perfective maintenance which
should especially target large and complex ﬁles.

In our results for RQ2 we see a diﬀerence between ﬁles before corrective
changes are applied, and before other changes are applied. This diﬀerence is one

13 https://pmd.github.io/pmd/pmd rules java design.html#cyclomaticcomplexity

What really changes when developers intend to improve their source code

29

of the sources of the predictive power of defect prediction models. However, the
diﬀerence is smaller than expected. Incorporating metrics that have a larger
diﬀerence in our data, e.g., comment density and API documentation into
defect prediction models, may increase their prediction performance.

6.2 Implications for Practitioners

Our results for H2 suggest that, for the most part, software quality models
match the expectations of the developers. If practitioners select a software
quality model which uses static source code metrics that show a diﬀerence in
our data they can expect that the model matches their intuition.

In combination with RQ2, our results indicate that bug ﬁxing is the main
driver of complexity in a software project and perfective changes are the main
reducer of complexity. This has implications for developers. If more complex
ﬁles were targeted for perfective maintenance bugs could possibly have been
prevented. As ﬁxing bugs does not decrease complexity, perfective maintenance
is the best way to reduce it and combat rising complexity of the project as
a whole. However, given the results for RQ2, we see that large and complex
ﬁles are not the main target of perfective maintenance. This is an opportunity
for improvement by shifting priorities for perfective maintenance to large and
complex ﬁles. Moreover, our results indicate that a bug ﬁx should be treated
similar to technical debt regarding its negative impact on complexity metrics.
To mitigate this, practitioners should be aware that it would be beneﬁcial to
clean up and simplify the code that is introduced as part of the bug ﬁx.

7 Threats to validity

In this section, we discuss the threats to validity we identiﬁed for our work.
We discuss four basic types of validity separately as suggested by Wohlin et al.
(2000) and include reliability due to our manual classiﬁcation approach.

7.1 Reliability

We classify changes to a software retroactively and without the developers.
This may introduce a researcher bias to the data and subsequently the results.
However, this is a necessity given the size of the data and the unrestricted
time frame for the sample and full data because it would not be feasible to ask
developers about a couple of commits from years ago. To mitigate this threat,
we perform the classiﬁcation labeling according to guidelines and every change
is independently classiﬁed by two researchers. We also compare our diﬀerences
with a sample of changes classiﬁed by the developers themselves from Mauczka
et al. (2015) and conﬁrm that we are agreeing on most changes. In addition,
we measure the inter-rater agreement between the researchers and ﬁnd that it
is substantial.

30

Alexander Trautsch et al.

7.2 Construct Validity

Our deﬁnition of quality improving may be too broad. We aggregate diﬀerent
types of quality improvement together, e.g., improving error messages, struc-
ture of the code or readability. This may inﬂuence the changes we observe
within our metric values. While these diﬀerences should be studied as well, we
believe that a broad overview of generic quality improvements independent of
their type has advantages. We avoid the risk of being focused only on struc-
tural improvements, i.e., due to use of generics or new Java features without
missing bigger changes due to simpliﬁcation of method code.

7.3 Conclusion Validity

We are reporting diﬀerences in metric value changes between perfective and
corrective changes of the software development history of our study subjects.
We ﬁnd a diﬀerence for perfective commits and only some non-negligible, sta-
tistically signiﬁcant diﬀerence for corrective commits. This could be an eﬀect
of our sample used as ground truth, however we chose to draw randomly from
a list of commits in our study subjects so that our sample should be represen-
tative.

We use a deep learning model to classify all of our commits based on the
ground truth we provide. This can introduce a bias or errors in the classiﬁ-
cation. We note however, that the non-negligible eﬀect sizes for our results
do not change. The quality metric evaluation of only the ground truth data
is included in the appendix and shows similar results. We note that for the
small eﬀect sizes we observe, a large number of observations are needed to
show a signiﬁcant diﬀerence as is demonstrated by the results in this article
when compared to the ground truth.

7.4 Internal Validity

A possible threat could be tangled commits which improve quality and at the
same time add a feature. We mitigate this in our ground truth, by manual
inspection of the commit message of every change considered. We excluded
tangled commits if it was possible to determine this by the commit message. As
no automatic untangling approach is available to us and available approaches
to label tangled commits already use the commit message to ﬁnd tangled
commits we determine that tangled commits which are not identiﬁable from
the commit message are a minor threat.

Another threat could be a lower number of feature additions in our study
subjects. Maybe feature additions happen too infrequently to inﬂuence the
results, therefore, corrective commits are seen as adding more complex code
than other commits. While we include some projects that are in development
for a long period of time, we believe this threat is mitigated by the unrestricted
time frame of our study.

What really changes when developers intend to improve their source code

31

Bots which commit code (Dey et al., 2020) could be a possible threat to
our study. We mitigate this threat by matching our author data against the
bot data set provided by Dey et al. (2020). We did not ﬁnd matches for bots
in our data. We were able to detect a Jenkins bot only when dropping the
restriction of our case study data that a commit has to change non-test code.
We also implemented the detection mechanism by Dey et al. (2020) which uses
the username and email of the author of the commit, as used by Dey et al.
to create their bot data set. This also yielded no bots in our data. Manual
inspection of the author data yielded two bot-like accounts which turned out
to be from a previous cvs2svn conversion as well as asf-sync-process which
allows user patches without an account. However, the content of changes by
the accounts we found are created by developers. We determine that the threat
of bots in our data is low.

7.5 External Validity

We focus on a convenience sample of data consisting of Java Open Source
projects under the umbrella of the Apache Software Foundation. We consider
this a minor threat to external validity. The reason is that although we are
limited to one organization, we still have a wide variety of diﬀerent types of
software in our data. We believe that this mitigates the missing variety of
project patronage.

Furthermore, we only include Java projects. However, Java is used in a
wide variety of projects and remains a popular language. Its age provides us
with a long history of data we can utilize in this study. However, we note
that this study may not generalize to all Java projects much less all software
projects in other languages.

8 Conclusion

Numerous quality measurements exist, and numerous software quality models
try to connect concrete quality metrics with abstract quality factors and sub
factors. Although it seems clear that some static source code metrics inﬂu-
ence software quality factors, the question of which and how much remains.
Instead of relying on necessarily limited developer and expert evaluations of
source code or changes we extract metrics from past changes where developers
intended to increase the quality extracted from the commit message.

Within this work, we performed a manual classiﬁcation of developer intents
on a sample of 2,533 commits from 54 Java open source projects by two re-
searchers independently and guided by classiﬁcation guidelines. We classify the
commits into three categories, perfective maintenance, corrective maintenance,
or neither. We further evaluate our classiﬁcation guidelines by re-classifying
of a developer labeled sample. We use the manually labeled data as ground
truth to evaluate and then ﬁne tune a state-of-the-art deep learning model for

32

Alexander Trautsch et al.

text classiﬁcation. The ﬁne-tuned model is then used to classify all available
commits into our categories increasing our data size to 125,482 commits. We
extract static source code metrics and static analysis warnings for all 125,482
commits which allows us to investigate the impact of changes and the distribu-
tion of metric values before the changes are applied. Based on the literature,
we hypothesize that certain metric values change in a certain direction, e.g.,
perfective changes reduce complexity. We ﬁnd that perfective commits are
more often removing code and generally add fewer lines. Regarding the metric
measurements, we ﬁnd that most metric value changes of perfective commits
are signiﬁcantly diﬀerent to other commits and have a positive, non-negligible
impact on the majority of metric values.

Surprisingly, we found that corrective changes are more complex and larger
than other changes. It seems that ﬁxing a bug increases the size, but also
the complexity measured via McCC and NLE. As we compare against all
other changes, we were expecting less addition of complexity as e.g., feature
additions. We conclude that the process of performing a bug ﬁx tends to add
more complex code than other changes.

We ﬁnd that complex ﬁles are not necessarily the primary target for quality
increasing work by developers, including refactoring. To the contrary, we ﬁnd
that perfective quality changes are applied to ﬁles that are already less complex
than ﬁles changed in other or corrective commits. Files contained in corrective
changes on the other hand are more complex and usually larger than both
perfective and other ﬁles. In combination with our ﬁrst result this shows that
corrective changes are applied to ﬁles which are already complex and get even
more complex after the change is applied.

While we explored a limited number of metrics and commits we think that
this approach can be used to evaluate more metrics connected with software
quality in a meaningful way and help practitioners and researchers with addi-
tional empirical data.

Declarations

This work was partly funded by the German Research Foundation (DFG)
through the project DEFECTS, grant 402774445.
The authors have no competing interests to declare that are relevant to the
content of this article.

Acknowledgements

We want to thank the GWDG G¨ottingen14 for providing us with computing
resources within their HPC-Cluster.

14 https://www.gwdg.de

What really changes when developers intend to improve their source code

33

References

Abdi H (2007) Bonferroni and Sidak corrections for multiple comparisons. In:
Encyclopedia of Measurement and Statistics, Sage, Thousand Oaks, CA, pp
103–107

Al Dallal J, Abdin A (2018) Empirical evaluation of the impact of object-
oriented code refactoring on quality attributes: A systematic literature
review. IEEE Transactions on Software Engineering 44(1):44–69, DOI
10.1109/TSE.2017.2658573

Alali A, Kagdi H, Maletic JI (2008) What’s a typical commit? a charac-
terization of open source software repositories. In: 2008 16th IEEE In-
ternational Conference on Program Comprehension, pp 182–191, DOI
10.1109/ICPC.2008.24

AlOmar EA, Mkaouer MW, Ouni A (2021) Toward the automatic classiﬁcation
of self-aﬃrmed refactoring. Journal of Systems and Software 171:110821,
DOI 10.1016/j.jss.2020.110821, URL http://www.sciencedirect.com/
science/article/pii/S016412122030217X

Alshayeb M (2009) Empirical

investigation of refactoring eﬀect on soft-
ware quality. Information and Software Technology 51(9):1319 – 1326,
DOI 10.1016/j.infsof.2009.04.002, URL http://www.sciencedirect.com/
science/article/pii/S095058490900038X

Bakota T, Heged˝us P, K¨ortv´elyesi P, Ferenc R, Gyim´othy T (2011) A proba-
bilistic software quality model. In: 2011 27th IEEE International Conference
on Software Maintenance (ICSM), pp 243–252, DOI 10.1109/ICSM.2011.
6080791

Bakota T, Heged˝us P, Siket I, Lad´anyi G, Ferenc R (2014) Qualitygate
sourceaudit: A tool for assessing the technical quality of software. In: 2014
Software Evolution Week - IEEE Conference on Software Maintenance,
Reengineering, and Reverse Engineering (CSMR-WCRE), pp 440–445, DOI
10.1109/CSMR-WCRE.2014.6747214

Bavota G, De Lucia A, Di Penta M, Oliveto R, Palomba F (2015) An ex-
perimental investigation on the innate relationship between quality and
refactoring. Journal of Systems and Software 107:1 – 14, DOI https://
doi.org/10.1016/j.jss.2015.05.024, URL http://www.sciencedirect.com/
science/article/pii/S0164121215001053

Boehm BW, Brown JR, Lipow M (1976) Quantitative evaluation of software
quality. In: Proceedings of the 2Nd International Conference on Software En-
gineering, IEEE Computer Society Press, Los Alamitos, CA, USA, ICSE ’76,
pp 592–605, URL http://dl.acm.org/citation.cfm?id=800253.807736
Ch’avez A, Ferreira I, Fernandes E, Cedrim D, Garcia A (2017) How does
refactoring aﬀect internal quality attributes? a multi-project study. In: Pro-
ceedings of the 31st Brazilian Symposium on Software Engineering, Asso-
ciation for Computing Machinery, New York, NY, USA, SBES’17, p 74–83,
DOI 10.1145/3131151.3131171

Chidamber SR, Kemerer CF (1994) A metrics suite for object oriented design.
IEEE Transactions on Software Engineering 20(6):476–493, DOI 10.1109/

34

32.295895

Alexander Trautsch et al.

Cliﬀ N (1993) Dominance statistics: Ordinal analyses to answer ordinal ques-

tions. Psychological Bulletin

Cohen J (1960) A coeﬃcient of agreement for nominal scales. Educational and
Psychological Measurement 20(1):37–46, DOI 10.1177/001316446002000104
D’Ambros M, Lanza M, Robbes R (2012) Evaluating defect prediction ap-
proaches: A benchmark and an extensive comparison. Empirical Softw Engg
17(4-5):531–577, DOI 10.1007/s10664-011-9173-9

Dey T, Mousavi S, Ponce E, Fry T, Vasilescu B, Filippova A, Mockus A (2020)
Detecting and characterizing bots that commit code. In: Proceedings of the
17th International Conference on Mining Software Repositories, Association
for Computing Machinery, New York, NY, USA, p 209–219, URL https:
//doi.org/10.1145/3379597.3387478

Fakhoury S, Roy D, Hassan A, Arnaoudova V (2019) Improving source code
readability: Theory and practice. In: 2019 IEEE/ACM 27th International
Conference on Program Comprehension (ICPC), pp 2–12, DOI 10.1109/
ICPC.2019.00014

Fenton N, Bieman J (2014) Software Metrics: A Rigorous and Practical Ap-
proach, Third Edition, 3rd edn. CRC Press, Inc., Boca Raton, FL, USA
Ferenc R, Gyimesi P, Gyimesi G, T´oth Z, Gyim´othy T (2020) An au-
tomatically created novel bug dataset and its validation in bug predic-
tion. Journal of Systems and Software 169:110691, DOI 10.1016/j.jss.
2020.110691, URL http://www.sciencedirect.com/science/article/
pii/S0164121220301436

Fu Y, Yan M, Zhang X, Xu L, Yang D, Kymer JD (2015) Auto-
mated classiﬁcation of software change messages by semi-supervised latent
dirichlet allocation. Information and Software Technology 57:369 – 377,
DOI 10.1016/j.infsof.2014.05.017, URL http://www.sciencedirect.com/
science/article/pii/S0950584914001347

Ghadhab L, Jenhani I, Mkaouer MW, Ben Messaoud M (2021) Augment-
ing commit classiﬁcation by using ﬁne-grained source code changes and a
pre-trained deep neural language model. Information and Software Tech-
nology 135:106566, DOI 10.1016/j.infsof.2021.106566, URL https://www.
sciencedirect.com/science/article/pii/S0950584921000495

Gharbi S, Mkaouer MW, Jenhani I, Messaoud MB (2019) On the classiﬁ-
cation of software change messages using multi-label active learning. In:
Proceedings of the 34th ACM/SIGAPP Symposium on Applied Comput-
ing, Association for Computing Machinery, New York, NY, USA, SAC ’19,
p 1760–1767, DOI 10.1145/3297280.3297452

Griessom RJ, Kim JJ (2005) Eﬀect sizes for research: A broad practical ap-

proach. Lawrence Erlbaum Associates Publishers

Gyimothy T, Ferenc R, Siket I (2005) Empirical validation of object-oriented
metrics on open source software for fault prediction. IEEE Transactions on
Software Engineering 31(10):897–910, DOI 10.1109/TSE.2005.112

Hattori LP, Lanza M (2008) On the nature of commits. In: Proceedings of
the 23rd IEEE/ACM International Conference on Automated Software En-

What really changes when developers intend to improve their source code

35

gineering, IEEE Press, Piscataway, NJ, USA, ASE’08, pp III–63–III–71,
DOI 10.1109/ASEW.2008.4686322

Herbold S, Trautsch A, Trautsch F, Ledel B (2021) Issues with szz: An empir-
ical assessment of the state of practice of defect prediction data collection,
URL http://arxiv.org/abs/1911.08938, recently Accepted at Empirical
Software Engineering

Herzig K, Just S, Zeller A (2013) It’s not a bug, it’s a feature: How misclas-
siﬁcation impacts bug prediction. In: Proceedings of the 2013 International
Conference on Software Engineering, IEEE Press, ICSE ’13, p 392–401
Hosseini S, Turhan B, Gunarathna D (2017) A systematic literature review
and meta-analysis on cross project defect prediction. IEEE Transactions on
Software Engineering PP(99):1–1, DOI 10.1109/TSE.2017.2770124

Huang Q, Xia X, Lo D (2017) Supervised vs unsupervised models: A holistic
look at eﬀort-aware just-in-time defect prediction. In: 2017 IEEE Interna-
tional Conference on Software Maintenance and Evolution (ICSME), pp
159–170, DOI 10.1109/ICSME.2017.51

H¨onel S, Ericsson M, L¨owe W, Wingkvist A (2019) Importance and aptitude of
source code density for commit classiﬁcation into maintenance activities. In:
2019 IEEE 19th International Conference on Software Quality, Reliability
and Security (QRS), pp 109–120, DOI 10.1109/QRS.2019.00027
ISO/IEC (2001) Iso/iec 9126. software engineering – product quality
ISO/IEC (2011) ISO/IEC 25010:2011, systems and software engineering —
systems and software quality requirements and evaluation (square) — sys-
tem and software quality models

Jureczko M, Madeyski L (2010) Towards identifying software project clusters
with regard to defect prediction. In: Proceedings of the 6th International
Conference on Predictive Models in Software Engineering, Association for
Computing Machinery, New York, NY, USA, PROMISE ’10, DOI 10.1145/
1868328.1868342

Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A, Ubayashi
N (2013) A large-scale empirical study of just-in-time quality assurance.
IEEE Transactions on Software Engineering 39(6):757–773, DOI 10.1109/
TSE.2012.70

Kim S, Zimmermann T, Whitehead Jr EJ, Zeller A (2007) Predicting faults
from cached history. In: 29th International Conference on Software Engi-
neering (ICSE’07), pp 489–498, DOI 10.1109/ICSE.2007.66

Kitchenham B, Pﬂeeger SL (1996) Software quality: the elusive target [special

issues section]. IEEE Software 13(1):12–21, DOI 10.1109/52.476281

Landis JR, Koch GG (1977) An application of hierarchical kappa-type statis-
tics in the assessment of majority agreement among multiple observers. Bio-
metrics 33(2):363–374, URL http://www.jstor.org/stable/2529786
Levin S, Yehudai A (2017) Boosting automatic commit classiﬁcation into main-
tenance activities by utilizing source code changes. In: Proceedings of the
13th International Conference on Predictive Models and Data Analytics in
Software Engineering, Association for Computing Machinery, New York,
NY, USA, PROMISE, p 97–106, DOI 10.1145/3127005.3127016

36

Alexander Trautsch et al.

Lewis C, Lin Z, Sadowski C, Zhu X, Ou R, Whitehead EJ (2013) Does bug
prediction support human developers? ﬁndings from a google case study.
In: 2013 35th International Conference on Software Engineering (ICSE), pp
372–381, DOI 10.1109/ICSE.2013.6606583

Mann HB, Whitney DR (1947) On a test of whether one of two random vari-
ables is stochastically larger than the other. Annals of Mathematical Statis-
tics 18(1):50–60

Mauczka A, Huber M, Schanes C, Schramm W, Bernhart M, Grechenig T
(2012) Tracing your maintenance work — a cross-project validation of an
automated classiﬁcation dictionary for commit messages. In: Proceedings of
the 15th International Conference on Fundamental Approaches to Software
Engineering, Springer-Verlag, Berlin, Heidelberg, FASE’12, p 301–315, DOI
10.1007/978-3-642-28872-2 21

Mauczka A, Brosch F, Schanes C, Grechenig T (2015) Dataset of developer-
labeled commit messages. In: Proceedings of the 12th Working Conference
on Mining Software Repositories, IEEE Press, Piscataway, NJ, USA, MSR
’15, pp 490–493, URL http://dl.acm.org/citation.cfm?id=2820518.
2820595

McCabe TJ (1976) A complexity measure. IEEE Trans Softw Eng 2(4):308–

320, DOI 10.1109/TSE.1976.233837

McCall JA, Richards PK, Walters GF (1977) Factors in software quality: con-
cept and deﬁnitions of software quality. Rome Air Development Center, Air
Force Systems Command, Griﬃss Air Force Base, New York 1(3)

Menzies T, Turhan B, Bener A, Gay G, Cukic B, Jiang Y (2008) Implications of
ceiling eﬀects in defect predictors. In: Proceedings of the 4th International
Workshop on Predictor Models in Software Engineering, Association for
Computing Machinery, New York, NY, USA, PROMISE ’08, p 47–54, DOI
10.1145/1370788.1370801

Mockus, Votta (2000) Identifying reasons for software changes using historic
databases. In: Proceedings 2000 International Conference on Software Main-
tenance, pp 120–130, DOI 10.1109/ICSM.2000.883028

Mordal-Manet K, Balmas F, Denier S, Ducasse S, Wertz H, Laval J, Bellingard
F, Vaillergues P (2009) The squale model — a practice-based industrial
quality model. In: 2009 IEEE International Conference on Software Main-
tenance, pp 531–534, DOI 10.1109/ICSM.2009.5306381

von der Mosel J, Trautsch A, Herbold S (2021) On the validity of pre-trained
transformers for natural language processing in the software engineering
domain, URL https://arxiv.org/abs/2109.04738, currently in a minor
revision to Transactions on Software Engineering

NASA (2004) Nasa IV & V facility metrics data program. URL http://mdp.

ivv.nasa.gov/repository.html

Pantiuchina J, Lanza M, Bavota G (2018) Improving code: The (mis) percep-
tion of quality metrics. In: 2018 IEEE International Conference on Software
Maintenance and Evolution (ICSME), pp 80–91, DOI 10.1109/ICSME.2018.
00017

What really changes when developers intend to improve their source code

37

Pantiuchina J, Zampetti F, Scalabrino S, Piantadosi V, Oliveto R, Bavota
G, Penta MD (2020) Why developers refactor source code: A mining-based
study. ACM Trans Softw Eng Methodol 29(4), DOI 10.1145/3408302

Parnas DL (2001) Software Aging, Addison-Wesley Longman Publishing Co.,

Inc., USA, p 551–567

Peitek N, Apel S, Parnin C, Brechmann A, Siegmund J (2021) Program
comprehension and code complexity metrics: An fmri study. In: 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE),
pp 524–536, DOI 10.1109/ICSE43902.2021.00056

Purushothaman R, Perry DE (2005) Toward understanding the rhetoric of
small source code changes. IEEE Transactions on Software Engineering
31(6):511–526, DOI 10.1109/TSE.2005.74

Rahman F, Posnett D, Hindle A, Barr E, Devanbu P (2011) Bugcache for
inspections: Hit or miss? In: Proceedings of the 19th ACM SIGSOFT Sym-
posium and the 13th European Conference on Foundations of Software
Engineering, Association for Computing Machinery, New York, NY, USA,
ESEC/FSE ’11, p 322–331, DOI 10.1145/2025113.2025157

Scalabrino S, Bavota G, Vendome C, Linares-V´asquez M, Poshyvanyk D,
Oliveto R (2021) Automatically assessing code understandability. IEEE
Transactions on Software Engineering 47(3):595–613, DOI 10.1109/TSE.
2019.2901468

Stroggylos K, Spinellis D (2007) Refactoring–does it improve software qual-
ity? In: Fifth International Workshop on Software Quality (WoSQ’07: ICSE
Workshops 2007), pp 10–10, DOI 10.1109/WOSQ.2007.11

Swanson EB (1976) The dimensions of maintenance. In: Proceedings of the 2nd
International Conference on Software Engineering, IEEE Computer Society
Press, Washington, DC, USA, ICSE ’76, p 492–497

Trautsch A, Herbold S, Grabowski J (2020a) A Longitudinal Study of Static
Analysis Warning Evolution and the Eﬀects of PMD on Software Quality
in Apache Open Source Projects. Empirical Software Engineering DOI 10.
1007/s10664-020-09880-1

Trautsch A, Trautsch F, Herbold S, Ledel B, Grabowski J (2020b) The
smartshark ecosystem for software repository mining. In: Proceedings of
the 42st International Conference on Software Engineering - Demonstra-
tions, ACM

Trautsch A, Erbel J, Herbold S, Grabowski J (2021) Replication kit. URL

https://github.com/atrautsch/emse2021_replication

Trautsch F, Herbold S, Makedonski P, Grabowski J (2017) Addressing prob-
lems with replicability and validity of repository mining studies through
a smart data platform. Empirical Software Engineering DOI 10.1007/
s10664-017-9537-x

Wagner S, Lochmann K, Heinemann L, Kl¨as M, Trendowicz A, Pl¨osch R,
Seidl A, Goeb A, Streit J (2012) The quamoco product quality modelling
and assessment approach. In: Proceedings of the 34th International Con-
ference on Software Engineering, IEEE Press, Piscataway, NJ, USA, ICSE
’12, pp 1133–1142, URL http://dl.acm.org/citation.cfm?id=2337223.

38

2337372

Alexander Trautsch et al.

Wang S, Bansal C, Nagappan N (2021) Large-scale intent analysis for
identifying large-review-eﬀort code changes. Information and Software
Technology 130:106408, URL http://www.sciencedirect.com/science/
article/pii/S0950584920300033

Wilk MB, Shapiro SS (1965) An analysis of variance test for normality (com-
plete samples)†. Biometrika 52(3-4):591–611, DOI 10.1093/biomet/52.3-4.
591

Wohlin C, Runeson P, H¨ost M, Ohlsson MC, Regnell B, Wessl´en A (2000) Ex-
perimentation in Software Engineering: An Introduction. Kluwer Academic
Publishers, Norwell, MA, USA

Yan M, Fu Y, Zhang X, Yang D, Xu L, Kymer JD (2016) Automatically
classifying software changes via discriminative topic model: Supporting
multi-category and cross-project. Journal of Systems and Software 113:296
– 308, DOI 10.1016/j.jss.2015.12.019, URL http://www.sciencedirect.
com/science/article/pii/S016412121500285X

Yatish S, Jiarpakdee J, Thongtanunam P, Tantithamthavorn C (2019) Mining
software defects: Should we consider aﬀected releases? In: 2019 IEEE/ACM
41st International Conference on Software Engineering (ICSE), pp 654–665,
DOI 10.1109/ICSE.2019.00075

Zhou Y, Yang Y, Lu H, Chen L, Li Y, Zhao Y, Qian J, Xu B (2018) How far
we have progressed in the journey? an examination of cross-project defect
prediction. ACM Trans Softw Eng Methodol 27(1), DOI 10.1145/3183339

What really changes when developers intend to improve their source code

39

Appendix A Ground truth only results

Fig. 5: Ground truth only. Commit size distribution over all projects for all,
perfective and corrective commits. Fliers are omitted.

Table 10: Ground truth only. Statistical test results for perfective and correc-
tive commits, Mann-Whitney U test p-values (p-value) and eﬀect size (d) with
category n is negligible, s is small. Statistically signiﬁcant p-values are bolded.

Perfective

Corrective

Metric

p-value

d

p-value

d

#lines added
#lines deleted
#ﬁles modiﬁed
#hunks

<0.0001
<0.0001
0.2829
0.7009

0.20 (s) <0.0001
0.13 (s) <0.0001
- <0.0001
- <0.0001

0.20 (s)
0.17 (s)
0.22 (s)
0.21 (s)

allperf.corr.050100150#lines addedallperf.corr.0255075100#lines deletedallperf.corr.2468#files modifiedallperf.corr.0102030#hunks40

Alexander Trautsch et al.

Fig. 6: Ground truth only. Static source code metrics changes in all, perfective
and corrective commits divided by changed lines. Fliers are omitted.

Table 11: Ground truth only. Statistical test results for perfective and cor-
rective commits, Mann-Whitney U test p-values (p-value) and eﬀect size (d)
with category, n is negligible, s is small, m is medium. Statistically signiﬁcant
p-values are bolded. All values are normalized for changed lines.

Perfective

Corrective

p-val
Metric
<0.0001
McCC
<0.0001
LLOC
NLE
<0.0001
NUMPAR <0.0001
1.0000
CC
<0.0001
CLOC
0.9303
CD
0.1556
AD
<0.0001
NOA
<0.0001
CBO
<0.0001
NII
<0.0001
Minor
<0.0001
Major
<0.0001
Critical

0.19 (s)

p-val
d
1.0000
0.37 (m)
0.42 (m)
1.0000
0.26 (s)
0.9577
0.24 (s) <0.0001
- <0.0001
0.1906
- <0.0001
- <0.0001
0.08 (n) <0.0001
0.0145
0.18 (s)
0.19 (s)
0.0620
0.0005
0.18 (s)
0.0002
0.10 (s)
0.1111
0.06 (n)

d
-
-
-
0.09 (n)
0.12 (s)
-
0.15 (s)
0.10 (s)
0.09 (n)
-
-
-
0.06 (n)
-

allperf.corr.-0.05 0.00 0.05 0.10log McCC delta + 1allperf.corr.-0.20 0.00 0.20log LLOC delta + 1allperf.corr.-0.02 0.00 0.02 0.04log NLE delta + 1allperf.corr.-0.04-0.02 0.00 0.02 0.04 0.06log NUMPAR delta + 1allperf.corr.-0.00-0.00 0.00 0.00 0.00log CC delta + 1allperf.corr.-0.10 0.00 0.10 0.20log CLOC delta + 1allperf.corr.-0.00-0.00 0.00 0.00 0.00log CD delta + 1allperf.corr.-0.00 0.00 0.00 0.00log AD delta + 1allperf.corr.-0.01 0.00 0.01log NOA delta + 1allperf.corr.-0.02 0.00 0.02 0.04log CBO delta + 1allperf.corr.-0.03 0.00 0.03 0.05 0.07 0.10log NII delta + 1allperf.corr.-0.02 0.00 0.02 0.04log Minor delta + 1allperf.corr.-0.02 0.00 0.02log Major delta + 1allperf.corr.-0.01 0.00 0.01 0.02 0.03 0.04log Critical delta + 1What really changes when developers intend to improve their source code

41

Fig. 7: Ground truth only. Static source code metrics before the change is
applied. Fliers are omitted.

Table 12: Median metric values before the change is applied.

Metric

All

Perfective Corrective

McCC
LLOC
NLE
NUMPAR
CC
CLOC
CD
AD
NOA
CBO
NII
Minor
Major
Critical

21.00
187.22
9.50
16.00
0.04
48.22
0.25
0.50
1.00
9.50
8.00
7.00
2.00
0.00

18.00
160.38
7.67
14.67
0.04
55.00
0.31
0.63
1.00
8.00
8.00
5.43
1.00
0.00

34.00
270.00
15.20
21.00
0.04
55.00
0.24
0.49
1.00
14.00
9.00
10.00
2.67
0.00

allperf.corr.050100150McCC/changedfilesallperf.corr.02505007501000LLOC/changedfilesallperf.corr.0204060NLE/changedfilesallperf.corr.020406080NUMPAR/changedfilesallperf.corr.0.00.10.20.30.4CC/changedfilesallperf.corr.0100200300CLOC/changedfilesallperf.corr.0.00.20.40.60.81.0CD/changedfilesallperf.corr.0.00.51.01.52.0AD/changedfilesallperf.corr.0246NOA/changedfilesallperf.corr.0204060CBO/changedfilesallperf.corr.0204060NII/changedfilesallperf.corr.02040Minor/changedfilesallperf.corr.051015Major/changedfilesallperf.corr.0123Critical/changedfiles42

Alexander Trautsch et al.

Table 13: Ground truth only. Statistical test results for perfective and cor-
rective commits regarding their average metrics before the change, Mann-
Whitney U test p-values (p-value) and eﬀect size (d) with category, n is neg-
ligible, s is small, m is medium. Statistically signiﬁcant p-values are bolded.

Perfective

Corrective

Metric

p-val

d

p-val

d

McCC
LLOC
NLE
NUMPAR
CC
CLOC
CD
AD
NOA
CBO
NII
Minor
Major
Critical

0.0003
0.0005
0.0003
0.5344
0.4142
<0.0001
<0.0001
<0.0001
0.6847
<0.0001
0.0510
0.0006
<0.0001
0.0179

0.0016
-
0.1138
-
0.0072
-
0.4704
-
0.0210
-
0.10 (n)
0.0111
0.15 (s) <0.0001
0.15 (s) <0.0001
0.2103
0.0190
0.0105
0.6288
0.0852
0.5730

-
0.11 (s)
-
-
0.12 (s)
-

-
-
-
-
-
-
0.16 (s)
0.15 (s)
-
-
-
-
-
-

What really changes when developers intend to improve their source code

43

Table 14: Detailed statistical tests results for metric changes. Accompanies
Table 7. SHA is Shapiro-Wilk, MWU is Mann-Whitney U test, the number of
samples for not perfective is 77,630, for perfective the number is 47,852. The
number of samples for not corrective is 90,258 and for corrective 35,124. For
both samples the median, Shapiro-Wilk test statistic and p-value are given
comma separated.

Metric

MWU Statistic

Median

SHA Statistic

SHA p-val

Perfective changes

McCC
LLOC
NLE
NUMPAR
CC
CLOC
CD
AD
NOA
CBO
NII
Minor
Major
Critical

McCC
LLOC
NLE
NUMPAR
CC
CLOC
CD
AD
NOA
CBO
NII
Minor
Major
Critical

2579401012.5
2691844899.5
2351847133.0
2328626543.0
1666541612.5
2158356261.5
1715608163.5
1899339427.0
1997259809.5
2208901912.0
2210463268.0
2201853734.0
2077680338.5
1952568002.5

1319862052.5
1406100592.5
1538986445.5
1736495605.5
1781604826.0
1665288104.5
1833218654.0
1719709796.5
1700427713.0
1687001103.5
1621472694.0
1664776380.0
1667877088.0
1631274846.5

0.02,0.00
0.25,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00

0.55,0.27 <0.0001,<0.0001
0.57,0.20 <0.0001,<0.0001
0.58,0.20 <0.0001,<0.0001
0.39,0.05 <0.0001,<0.0001
0.03,0.01 <0.0001,<0.0001
0.32,0.34 <0.0001,<0.0001
0.41,0.21 <0.0001,<0.0001
0.25,0.13 <0.0001,<0.0001
0.06,0.01 <0.0001,<0.0001
0.21,0.04 <0.0001,<0.0001
0.09,0.07 <0.0001,<0.0001
0.04,0.01 <0.0001,<0.0001
0.04,0.00 <0.0001,<0.0001
0.05,0.05 <0.0001,<0.0001

Corrective changes

0.00,0.00
0.07,0.18
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00
0.00,0.00

0.36,0.36 <0.0001,<0.0001
0.36,0.36 <0.0001,<0.0001
0.35,0.35 <0.0001,<0.0001
0.14,0.14 <0.0001,<0.0001
0.01,0.01 <0.0001,<0.0001
0.38,0.38 <0.0001,<0.0001
0.28,0.28 <0.0001,<0.0001
0.19,0.19 <0.0001,<0.0001
0.03,0.03 <0.0001,<0.0001
0.09,0.09 <0.0001,<0.0001
0.11,0.11 <0.0001,<0.0001
0.01,0.01 <0.0001,<0.0001
0.01,0.01 <0.0001,<0.0001
0.07,0.07 <0.0001,<0.0001

44

Alexander Trautsch et al.

Table 15: Detailed statistical tests results for metrics before the change is
applied. Accompanies Table 9. SHA is Shapiro-Wilk, MWU is Mann-Whitney
U test, the number of samples for not perfective is 77,630, for perfective the
number is 47,852. The number of samples for not corrective is 90,258 and for
corrective 35,124. For both samples the median, Shapiro-Wilk test statistic
and p-value are given comma separated.

Metric

MWU Statistic

Median

SHA Statistic

SHA p-val

Perfective changes

McCC
LLOC
NLE
NUMPAR
CC
CLOC
CD
AD
NOA
CBO
NII
Minor
Major
Critical

McCC
LLOC
NLE
NUMPAR
CC
CLOC
CD
AD
NOA
CBO
NII
Minor
Major
Critical

1946723702.0
1946637361.5
1934702498.0
1860319211.0
1881849087.0
1642584608.5
1570226793.0
1548982847.0
1861405551.0
2023896520.5
1756171669.5
1926916681.5
2025070328.5
1949852017.5

1455448657.0
1506999970.0
1477296467.5
1573653093.5
1605078855.0
1683529860.5
1832629967.5
1822953682.5
1616227506.0
1476937730.0
1655048856.5
1557520234.5
1516141644.5
1546362144.0

47.00,39.00
397.00,335.00
21.00,18.00
34.00,32.00
0.09,0.08
84.00,118.00
0.40,0.54
0.83,1.00
2.00,2.00
21.00,15.00
15.00,18.00
15.00,13.00
4.00,3.00
0.00,0.00

Corrective changes

41.00,50.00
361.00,399.00
18.00,22.00
33.00,33.00
0.09,0.08
101.00,83.00
0.51,0.35
1.00,0.75
2.00,2.00
17.00,21.00
17.00,14.00
14.00,14.00
3.00,4.00
0.00,0.00

0.27,0.21 <0.0001,<0.0001
0.26,0.21 <0.0001,<0.0001
0.28,0.24 <0.0001,<0.0001
0.25,0.20 <0.0001,<0.0001
0.07,0.10 <0.0001,<0.0001
0.18,0.24 <0.0001,<0.0001
0.08,0.14 <0.0001,<0.0001
0.11,0.14 <0.0001,<0.0001
0.13,0.09 <0.0001,<0.0001
0.24,0.16 <0.0001,<0.0001
0.25,0.21 <0.0001,<0.0001
0.15,0.13 <0.0001,<0.0001
0.23,0.17 <0.0001,<0.0001
0.19,0.12 <0.0001,<0.0001

0.23,0.23 <0.0001,<0.0001
0.23,0.23 <0.0001,<0.0001
0.25,0.25 <0.0001,<0.0001
0.22,0.22 <0.0001,<0.0001
0.09,0.09 <0.0001,<0.0001
0.22,0.22 <0.0001,<0.0001
0.12,0.12 <0.0001,<0.0001
0.13,0.13 <0.0001,<0.0001
0.10,0.10 <0.0001,<0.0001
0.19,0.19 <0.0001,<0.0001
0.22,0.22 <0.0001,<0.0001
0.14,0.14 <0.0001,<0.0001
0.19,0.19 <0.0001,<0.0001
0.15,0.15 <0.0001,<0.0001

