DeepFreight: A Model-free
Deep-reinforcement-learning-based Algorithm for
Multi-transfer Freight Delivery

Jiayu Chen, Abhishek K. Umrawal, Tian Lan, and Vaneet Aggarwal

1

1
2
0
2

r
a

M
5

]

G
L
.
s
c
[

1
v
0
5
4
3
0
.
3
0
1
2
:
v
i
X
r
a

Abstract—With the freight delivery demands and shipping
costs increasing rapidly, intelligent control of ﬂeets to enable
efﬁcient and cost-conscious solutions becomes an important
problem. In this paper, we propose DeepFreight, a model-free
deep-reinforcement-learning-based algorithm for multi-transfer
freight delivery, which includes two closely-collaborative compo-
nents: truck-dispatch and package-matching. Speciﬁcally, a deep
multi-agent reinforcement learning framework called QMIX is
leveraged to learn a dispatch policy, with which we can obtain
the multi-step joint vehicle dispatch decisions for the ﬂeet with
respect to the delivery requests. Then an efﬁcient multi-transfer
matching algorithm is executed to assign the delivery requests
to the trucks. Also, DeepFreight is integrated with a Mixed-
Integer Linear Programming optimizer for further optimization.
The evaluation results shows that the proposed system is highly
scalable and ensures a 100% delivery success while maintaining
low delivery-time and fuel consumption.

Index Terms—Multi-agent Reinforcement Learning, Freight
Delivery, Fleet Management, Intelligent Transportation System

I. INTRODUCTION

According to American Trucking Associations1, the U.S.
trucking industry shipped 11.84 billion tons of goods in 2019
and had a market of 791.7 billion dollars. With shipping costs
rising and freight volumes outpacing the supply of available
trucks, companies are thinking of radical new initiatives to
get their products into customers’ hands more efﬁciently. On
the other hand, transportation is currently the largest source
(29%) of greenhouse gas emissions in the U.S., where the
passenger cars and trucks account for more than 80% of this
section2. Thus, innovations to enable intelligent and efﬁcient
freight transportation are of central importance for the sake of
better utilization of the trucks and less fuel consumption.

We propose DeepFreight, a model-free learning framework
for the freight delivery problem. The proposed algorithm
decomposes the problem into two closely-collaborative com-
ponents: truck-dispatch and package-matching. In particular,
the dispatch policy aims to ﬁnd the dispatch decisions for the
ﬂeet according to the delivery requests. It leverages a multi-
agent reinforcement learning framework, QMIX [1], which
can train decentralised policies in a centralised end-to-end

J. Chen, A. U. Umrawal, and V. Aggarwal are with Purdue University, West
Lafayette IN 47907, USA, email: {chen3686,aumrawal,vaneet}@purdue.edu.
T. Lan is with the George Washington University, Washinton DC 20052, USA,
email:tlan@gwu.edu.

1 https://www.trucking.org/economics-and-industry-data
2 https://www.climatecentral.org/gallery/graphics/emissions-sources-2020

fashion. Then an efﬁcient matching algorithm is developed
to match the delivery requests with the dispatch decisions,
which is a greedy approach that makes use of multiple
transfers and aims at minimizing the total time use of the
ﬂeet. Further, DeepFreight is integrated with a Mixed-Integer
Linear Programming (MILP) optimizer for more efﬁcient and
reliable dispatch and assignment decisions.

To the best of our knowledge, this is the ﬁrst work that takes
a machine learning aided approach for freight scheduling with
multiple transfers. The key contributions of the paper can be
summarized as follows.
1) We propose DeepFreight, a learning-based algorithm
for multi-transfer freight delivery, which contains truck-
dispatch and package-matching. The dispatch policy is
adopted to determine the routes of the trucks, and then
matching policy is executed to assign the packages to the
trucks efﬁciently.

2) We propose to train the dispatch policy through a cen-
tralized training with decentralized execution algorithm,
QMIX [1], which considers the cooperation among the
trucks when training and makes the multi-agent dispatch
scalable for execution.

3) We propose an efﬁcient rule-based matching policy which
allows multi-transfer to minimize the usage time of the
ﬂeet.

4) We formulate a MILP model for this freight delivery
problem and integrate it with DeepFreight to achieve better
reliability and efﬁciency of the overall system.
The rest of this paper is organized as follows. Section II
introduces some related works and highlights the innovation
of this paper. Section III presents an example for the multi-
transfer freight ride-sharing system and lists the model param-
eters and optimization objectives of the system for later use.
Section IV explains the proposed approach: DeepFreight in
details, including its overall framework and main algorithm
modules. Section V introduces a hybrid approach that har-
nesses DeepFreight and MILP: DeepFreight+MILP, including
the formulation of MILP and the workﬂow of this hybrid ap-
proach. Section VI talks about evaluation and results, including
the simulation setup, implementation details of DeepFreight
and comparisons among the algorithms: DeepFreight, Deep-
Freight without Multi-transfer, MILP and DeepFreight+MILP.
Section VII concludes the paper with a discussion of future
work.

 
 
 
 
 
 
II. RELATED WORK

Truck Dispatch Problem. The freight delivery problem is
a variant of ‘Vehicle Routing Problem (VRP)’, which was
ﬁrst introduced in [2] as the ‘Truck Dispatch Problem’ and
then generalized in [3] to a linear optimization problem: how
to use a ﬂeet of trucks with capacity constraints to serve a
set of customers that are geographically dispersed around the
central depot. Over the decades, the classic VRP has been
extended to a lot of variants by introducing additional real-
life characteristics [4]. For example, VRP with Pickup and
Delivery [5], that is, goods are required to be picked up and
dropped off at certain locations and the pick-up and drop-
off must be done with the same vehicle; VPR with Time
Windows [6], that is, the pickups or deliveries for a customer
must occur in a certain time interval; Multi-Depot VRP [7],
which assumes that there are multiple depots spreading among
the customers for scheduling. The extended VRP is NP-hard,
and many heuristics and meta-heuristics algorithms have been
the meta-heuristics
adopted as the solution. Among them,
algorithms are used the most [4], such as simulated annealing
[8], genetic algorithm [9], particle swarm optimization [10],
and local search algorithm [11].

We note that these works don’t consider that goods can
be dropped off in the middle and carried further by another
truck (multi-transfer of goods), which has the potential for
better utilization of the ﬂeet and is considered in our work.
Further, the approaches they adopt are model-based, which
need execution every time new observation occurs. Also, the
execution time would increase with the scale of the problem,
so they usually have high time complexity and poor scalability.
Multi-hop Ride-Sharing. As a related area, researches
about the ride-sharing system are introduced in this part. To
handle the dynamic requests in this scenario, reinforcement
learning aided approaches for efﬁcient vehicle-dispatch and
passenger-matching have been proposed in [12], [13], [14].
However, these approaches don’t consider passengers going
over multiple hops (transfers). As shown in [15], the multi-
transfer has the potential to greatly increase the ride avail-
ability and reduce emissions of the ride-sharing system by
making better use of the vehicle capacities. Given this, a
reinforcement learning based approach for the multi-hop ride-
sharing problem has been recently studied in [16].

However, we note that there are some key differences be-
tween the ride-sharing system and freight-scheduling system.
For ride-sharing, the driving distances are smaller and it’s more
concerned with the real-time scalable solutions for the highly
dynamic requests. While for freight-scheduling, the number of
the trucks needed is much fewer due to their huge capacity
and the delivery demands can usually be acquired in advance
(e.g. one day ahead). In this case, the coordination among
trucks is possible and necessary for making better scheduling
decisions and we try to take it into consideration with our
freight delivery system through a multi-agent reinforcement
learning setting.

Multi-Agent Reinforcement Learning. Multi-agent rein-
learning (MARL) methods hold great potential
forcement
to solve a variety of real-world problems, and centralized

2

training with decentralized execution (CTDE) is an important
paradigm of it. Centralized training exploits the interaction
among the agents, while decentralized execution ensures the
system’s scalability. Speciﬁc to the multi-agent cooperative
setting (like ours), there are many related works: VDN [17],
QMIX [1], QTRAN [18] and MAVEN [19]. However, their
performance varies greatly in different benchmarks and none
of them has the absolute advantage. Considering there have
been more applications of QMIX in other domains and its
robust performance [20], [21], we choose QMIX to train the
dispatch policy in our algorithm framework.

III. SYSTEM MODEL

Fig. 1: An example for multi-transfer freight delivery

In this section, we will describe the proposed system model
for multi-transfer freight delivery. As an example, consider
the scenario shown in Figure 1, where Loads 1 and 4 need to
be shipped to a distribution center in Petersburg, VA, while
Loads 2 and 3 to the distribution center in New Castle,
DE. There are two trucks located at zone A and zone C,
with capacity R1 = 4 and R2 = 5, respectively. When
freight ride-sharing is enabled, two different possibilities are
shown in the ﬁgure to serve the requests: 1) serving all the
requests using only the truck 2 at zone C (i.e., the dashed
red line in the ﬁgure), 2) serving Loads 1 and 4 using
truck 2 (i.e., the dashed-dotted blue line) and Loads 2 and
3 using truck 1 (i.e., the dotted green line), offering different
tradeoffs between delivery time and fuel cost. Furthermore,
if multiple transfers are allowed, we may also have the two
trucks picking up their loads ﬁrst and then meeting at zone H
to fold the loads into a single truck before the ﬁnal delivery,
which provides additional elasticity for the truck and freight
scheduling problem. Through adoption of freight ride-sharing
and multi-transfer, fewer truck miles may be required, which
will lead to reduced shipping costs and less fuel consumption.

A. Model Parameters

In this section, key parameters of the system model are
there are Ns distribution centers in the
introduced. First,
freight delivery system, which can be used as the sources
and destinations of packages. Based on them, a delivery
request list, denoted by R1:Nr (Nr: the number of requests),
is collected at the beginning of each day. The i-th request ri
can be represented as (sourcei, destinationi, sizei), which
indicates the source, destination, and size of the i-th package.

Second, our target is to complete these requests using a ﬂeet
of trucks within a time limit. The number of the trucks for
dispatch is Nt and the time limit is denoted as Tmax which
can be one or two days. To fulﬁll this target, we need to
schedule the itinerary of the ﬂeet and the assignment of the
packages to the trucks.

Further,

in our system,

the itinerary of a truck is not
decided as a whole and instead consists of a sequence of
dispatch decisions, each of which shows the truck’s next
stop (distribution center) to visit. Also, the assignment of
the packages allows multiple transfers, which means that a
package can be assigned to more than one truck along the
route from its origin to its destination. Note that the total
driving time of a truck can’t exceed the time limit Tmax and
the volume of the packages it takes at the same time should
be within its maximum capacity Cmax.

B. Problem Objectives

The key objectives of the freight delivery system are:
1) Maximizing the number of served requests Nrs within

the time limit Tmax;

2) Minimizing the total fuel consumption of the ﬂeet Ftotal

during this process.

The second component Ftotal is viewed as the function of the
total driving time (denoted as Tdrive) and can be calculated
with Equation (1), where f uel f actor is a constant, repre-
senting the fuel consumption per unit time, while etak
j is the
estimated time of arrival (ETA) for the k-th dispatch decision
of truck j that can be obtained through Google Map API.

Ftotal = f uel f actor ∗ Tdrive
Ne(cid:88)

Nt(cid:88)

Tdrive =

etak
j

(1)

(2)

k=1

j=1

IV. PROPOSED APPROACH: DEEPFREIGHT

In this section, we introduce our proposed approach: Deep-
Freight Algorithm. First, we describe its overall framework in
Section IV-A, including the main algorithm modules and con-
nections between them. Then, in Section IV-B-subsec:match,
we talk about these modules further, including the detailed
procedure and the intuition to propose them.

A. Overall Framework of DeepFreight

Algorithm 1 shows the overall framework of the proposed
approach. As mentioned, the proposed approach should give
out the itinerary of the ﬂeet and the assignment of the packages
to the trucks. In this case, we split the whole system into two
parts: the dispatch policy and matching policy. The dispatch
policy determines a sequence of dispatch decisions for each
truck in the ﬂeet according to the unﬁnished delivery requests,
based on which we can get the route network of the ﬂeet.
The matching policy is then executed to assign the requests
to the trucks based on the route of each truck. We note that
the matching policy is executed based on the dispatch results,
while then the matching results provide reward feedback for

3

the training of the dispatch policy. The dispatch policy is
trained in a multi-agent reinforcement learning setting called
QMIX, which includes two key components: the agent net-
work Qsingle and mixing network Qmix (introduced in Section
IV-C).

There are some key points to note about Algorithm 1.
First, Line 3-6 is the initialization process of the simulator.
A new request list will be generated at the beginning of each
episode, however, the truck list will not be generated again
until the next operation cycle starts. Within an operation cycle,
the trucks’ locations at the beginning of a new episode are
the same as those at the end of the previous episode. This
connection in locations should be taken into consideration
when making dispatch decisions. Also, we note that
the
generation of the request list and truck list is model-based,
which is introduced in Section VI-A.

Second, Line 7-17 shows the sampling process of the agents
which is used to collect the experiences for training. In order
to decrease the search space of the agents, we add some
constraints to the truck’s routes: 1) It’s not allowed for the
truck to pass through the same stop twice, except for returning
to its origin (no sub-tours); 2) No dispatch decisions are
allowed any more after the truck returns to its origin or its
cumulative time exceeds the time limit. These constraints are
realized through action mask m1:Nt: once a stop has been
passed through by truck i, the corresponding term in mi will
be set as 1, representing the corresponding dispatch decision
(action) is not valid any more.

Third, not all the dispatch decisions can be matched with
delivery requests. For a truck, there may be some idle route
segments with no package assignment and these idle dispatch
decisions (route segments) can be eliminated on the condition
that the truck’s route continuity is not affected (show in Line
18). In this way, total fuel consumption can be further reduced.
Finally, some training tricks about deep Q-Learning are
adopted. For example, Boltzmann distribution is used to im-
prove the exploration of the environment and the experience
replay buffer is used to improve the efﬁciency of sample use
and the stability of training.

B. Modeling Dispatch problem with Decentralized Partial
Observable Markov Decision Process (Dec-POMDP)

As a cooperative multi-agent task, the dispatch problem is
modeled with Dec-POMDP, which can be described as a tuple
(S, U, P, r, Z, O, n, γ). The general deﬁnition of the elements
in the tuple are as follows:

• n : the number of agents used in the task;
• S : the environment state space;
• U : the individual action space;
• U ≡ U n: the joint action space for the n agents;
• P (s(cid:48)|s, u) : S × U × S → [0, 1], the state transition

function;

• r(s, u) : S × U → R, the reward function based on all

the agents;

• Z : the individual observation space;
• O(s, a) : S × A → Z, the observation function that
each agent in the list A = {1, 2, · · ·, n} draws individual
observation z ∈ Z according to;

4

Fig. 2: Workﬂow of QMIX Algorithm

Algorithm 1 DeepFreight Algorithm

1: Given: episode number Nepi, episode time limit Tmax,
epoch number Ne, request number Nr, truck number Nt,
operation cycle ∆, iteration number Niter, agent network
Qsingle, mixing network Qmix
2: for episode epi = 1 to Nepi do
3:
4:
5:

Generate Nr delivery requests randomly
if epi % ∆ == 1 then

Generate Nt trucks randomly

6:
7:
8:

9:

10:

11:

12:

13:

14:
15:
16:
17:
18:

19:

20:
21:

22:

end if
for epoch k = 1 to Ne do

1:Nt

1:Nt

1:Nt

1:Nt

from

, and action mask list mk

for each truck
and mk

Obtain the current environment state sk, obser-
vation list zk
Get the q-value list for each truck Qk
Qsingle
Choose the available action uk
based on the corresponding Qk
using Boltzmann distribution policy
Match the delivery requests with the dispatch
decisions using Algorithm 2 and update the un-
ﬁnished request list
Calculate the joint reward rk for this epoch using
Equation (3)
Update the cumulative time for each truck tk
if min(tk
break

1:Nt
) ≥ Tmax or no requests left then

1:Nt

1:Nt

1:Nt

,

end if

end for
Eliminate the useless dispatch decisions and calcu-
late the unﬁnished package number and average
driving time of the ﬂeet for evaluation
Update replay buffer B with experiences collected
above, including (s1:Ne , z1:Ne
, r1:Ne, m1:Ne
1:Nt
1:Nt
for iteration iter = 1 to Niter do

, u1:Ne
1:Nt

)

Sample a random batch of experiences from B
for training
Update Qsingle and Qmix by minimizing loss
function deﬁned as Equation (5)

end for

23:
24: end for

• γ ∈ [0, 1) : the discount factor.
Speciﬁcally, the environment state space S for this package

delivery dispatch problem includes three terms:

1) current epoch number k;

2) distribution of the trucks at epoch k: the number of the

trucks at each stop;

3) unﬁnished delivery requests at epoch k, which are initial-
ized at the beginning of each episode and updated with
the epochs.

As for the individual observation space Z, it is for a single
truck, so it replaces the item (2) with ”the current stop that
the truck is at” and keeps item (1) and item (3).

Moreover, the individual action space U includes:
1) next stop ∈ {1, 2, · · ·, Ns} : the index of the next stop;
2) invalid dispatch: there are some constraints for the dis-
patch decisions to be valid (e.g. within the time limit).

In this case, the dimension of the individual action space U
is Ns + 1.

The joint reward function r should reﬂect the objectives
mentioned in Section III-B, so the reward for the epoch k is
deﬁned as Equation (3), where β1:2 > 0.
rs − β2F k

rk = β1N k

total

(3)

Obviously,
it has a positive correlation with the package
number served at epoch k and a negative correlation with the
fuel consumption during this process.

Based on these elements, the dispatch policy for agent a
can be deﬁned as πa(ua|za) : Z × U → [0, 1]. The execution
of the policy is decentralized, since it conditions only on the
observation of the single agent. However, the training process
is centralized through maximizing the joint Q function for the
joint dispatch policy π, which is deﬁned as Equation (4).

Qπ(st, ut) = E[

T
(cid:88)

i=t

γi−tri] = E[

T
(cid:88)

i=t

γi−tr(si, ui)]

(4)

C. QMIX for Training the Dispatch Policy

QMIX is a value-based reinforcement learning algorithm
that can train decentralized policies in a centralized end-to-
end fashion. It is suitable to solve this multi-agent freight
delivery problem because 1) the centralized training process
takes the coordination among trucks into consideration, 2)
the decentralized execution process alleviates the complexity
to determine the joint dispatch decisions and ensures the
scalability of the dispatch policy. The workﬂow of QMIX is
shown as Figure 2, where two key components: Qsingle and
Qmix, need special attention.

The agent network Qsingle outputs the Q-value list for each
truck, which can be used for decentralized execution, or as a

part of the centralized training process. The inputs of Qsingle
include: 1) the agent (truck) id a, which is in the form of a
one-hot vector; 2) the individual observation for the current
epoch ok
a; 3) the dispatch decision (action) for the previous
epoch uk−1
. Input 1) ensures that the dispatch policy is unique
a
for each truck, which means even if different trucks have the
same observation and action history, their dispatch decisions
may still be different. Input 2) shows that the execution of the
dispatch policy is in a decentralized way, which conditions
only on local observations. Last, the dispatch decision of the
previous epoch is used as input and GRU cells are adopted
as part of the agent network structure, which means dispatch
decisions of different epochs within an episode are correlated
rather than independent. This design makes QMIX suitable
for solving multi-step decision making problems, like ours: a
sequence of dispatch decisions within an episode should be
made for each truck.

The mixing network Qmix estimates the joint Q-value Qtot
as a complex non-linear combination of Q-values for all the n
trucks. Also, HyperNetworks [22] is adopted to transform the
environment state sk into the parameters of Qmix, so the joint
Q-value also conditions on the global state information, which
has been proved necessary in its original paper [1]. Based on
Qtot, the loss function for QMIX can be deﬁned as Equation
(5), where yk
tot (Equation (6)) is the target joint q-value, which
is obtained from the target networks.

Loss(θ) =

batchsize
(cid:88)

i=1

(cid:2)(yk

tot − Qtot(τ k, uk, sk; θ))2
i

(cid:3)

tot = rk + maxuQtot(τ k+1, u, sk+1; θ−)
yk

(5)

(6)

After the centralized training, the agent network Qsingle can
be used to give dispatch decisions for each truck based on
their individual observation, action history, and agent id in a
decentralized way.

Fig. 3: Graph Representation of the Dispatch Decisions

D. Matching Algorithm

The matching algorithm, shown as Algorithm 2, is executed
to match the delivery requests with the dispatch decisions.
Considering the number of the packages to deliver is huge,
it’s nearly impossible to ﬁnd the optimal matching results
for all the packages within a reasonable time. In this case,
Algorithm 2 adopts a greedy approach to ensure the optimality
for every single package and takes the least increase in the

5

total driving time of the ﬂeet as the optimization objective. It
mainly includes two parts:

First, in Line 4-15, a weighted directed graph, like Figure
3, is created based on the dispatch decisions. Each node in the
graph represents for a stop that has occurred in the dispatch
decisions. While each edge means there is a truck that has been
dispatched between the two stops. Also, the id, departure time,
and available capacity of the truck and ETA between the two
stops are saved with the edge for later use.

Second, based on the graph, we can adopt DFS (depth-ﬁrst
search) to acquire all the available paths from the source to the
destination for each delivery request (Line 17). An available
path contains a list of edges and their truck id may be different,
which means the package may be delivered by more than one
truck (multi-transfer). In this case, there are some conditions
for a path to be available: 1) The package’s arrival time at
any node in the path should be earlier than the departure time
from that node of the truck it transfers to; 2) The package’s
size should be smaller than that truck’s capacity.

Further, among all the available paths, the one that causes
the least increase in total driving time of the ﬂeet will be
chosen (Line 19). For example, there are three available paths
(shown in different colors) from A to D in Figure 3: A → D,
A → B → D, and A → B → C → D and the edges A → B
and B → C have been matched with other requests. Then, the
time cost of the third path should only be the ETA from C to
D, since it will share the trucks denoted by the red edges with
other requests (freight ride-sharing). Similarly, the time cost of
the ﬁrst two paths are respectively the ETA from A to D and
ETA from B to D. Then, in order to minimize total driving
time of the ﬂeet, the path with the least time cost should be
chosen. Note that if there are more than one path with the
least time cost, the shortest path will be chosen. For example,
if the time cost of the three paths in Figure 3 are the same,
then the shortest one A → D will be chosen.

V. MILP FORMULATION AND COMBINATION WITH
DEEPFREIGHT

We further propose a hybrid approach that harnesses Deep-
Freight and MILP to ensure successful delivery of all pack-
ages. Speciﬁcally, experiments show that when most of the
requests have been completed and only a small number of
packages remain to be optimized, DeepFreight may have
unstable training dynamics resulting in undelivered packages
(as evidenced in Figure 8(b)). To this end, we leverage MILP
to ﬁnd the exact routing decisions for the small portion of
requests that are not efﬁciently handled by DeepFreight. This
leads to an integration of DeepFreight and MILP, denoted as
DeepFreight+MILP. The MILP’s formulation and the work-
ﬂow of DeepFreight+MILP are presented in this section.

A. MILP Formulation

Parameters.

• i, j, l ∈ L = {0, 1, . . . , m−1}: index and set of locations;
• k ∈ N = {0, 1, . . . , n − 1}: index and set of trucks;
• D: depot for each truck k, a dictionary, D(k) ∈ L;
• ti,j ∈ N: ETA from location i to location j, in seconds;

9:

10:

11:

12:

13:

18:
19:

20:

Algorithm 2 Matching Algorithm

1: Inputs: initial capacity of the trucks, dispatch decisions
viz. next stop for each truck at each valid epoch, request
list viz. (source, destination, size)1:Nr

2: Outputs: optimal matching results
3: Turn the dispatch decisions into a weighted directed graph

as follows:

4: Create as many nodes as stops in the dispatch decisions,

using the stop id as the node id

5: Set the current time for each truck as 0
6: for every truck do
7:
8:

for every valid epoch do

Create a directed edge from current stop node
to next stop node
Set the departure time from current stop as the
truck’s current time
Acquire the ETA from current stop node to
next stop node through Google Map API
Record the id, departure time, ETA, and initial
capacity of the truck as the edge’s information
Update the truck’s current stop as next stop
Update the truck’s current time as ETA +
current time

end for

14:
15: end for
16: for every delivery request do
17:

Find all the available paths from source to
destination using DFS (depth-ﬁrst search)
if success then

Choose the path that causes the least increase in
total driving time
Subtract the package size from the available ca-

pacity of

the edges it passes through

end if

21:
22: end for

• di,j ∈ N: delivery demand from location i to location j;
• C ∈ N+: truck capacity, the same for each truck k;
• Tk ∈ N+: the time limit for truck k.

Decision Variables.

• xi,j,k ∈ {0, 1}: the variable equals 1, if truck k travels

from location i to location j, and 0 otherwise;

• uk ∈ {0, 1}: the variable equals 1, if truck k is in use,

and 0 otherwise;

• ri,j,k ∈ {0, 1}: the variable equals 1, if truck k takes
the delivery demand from location i to location j, and 0
otherwise;
(Note that a truck takes the path from i to j doesn’t mean
it will take the delivery request from i to j)

• si,k ∈ N: the cumulative stop number when truck k

arrives at location i;

• vi,k ∈ N: the cumulative volume when truck k departs

from location i;

Optimization Objective:

The optimal solution should minimize the objective function

6

deﬁned as follows, where T1 and T2 respectively represent the
total time use (in hours) and the total volume of the unserved
packages, and ω1 > 0, ω2 > 0 are the weights for each term
(ω1 is set as 0.5 and ω2 is set as 0.04).

target = ω1T1 + ω2T2

m−1
(cid:88)

m−1
(cid:88)

(cid:34)

ti,j ×

T1 =

(cid:33)(cid:35)

xi,j,k

(cid:32)n−1
(cid:88)

k=0

di,j ×

n−1
(cid:89)

k=0

(cid:35)

(1 − ri,j,k)

i=0

j=0

m−1
(cid:88)

m−1
(cid:88)

(cid:34)

i=0

j=0

T2 =

Constraints:

• No self-loop:

(7)

(8)

(9)

∀i ∈ L, ∀k ∈ N, xi,i,k == ri,i,k == 0

(10)

• Within the time limit:

∀k ∈ N,

m−1
(cid:88)

m−1
(cid:88)

ti,jxi,j,k ≤ ukTk

(11)

• Depart from and return to the same depot:

i=0

j=0

∀k ∈ N,

m−1
(cid:88)

i=0

xi,D(k),k ==

m−1
(cid:88)

i=0

xD(k),i,k == uk

(12)

• Form a tour that includes the depot:

∀k ∈ N, ∀i ∈ L,

m−1
(cid:88)

xj,i,k ==

j=0
• Eliminate any subtour:

m−1
(cid:88)

j=0

xi,j,k ≤ uk

(13)

∀k ∈ N, sD(k),k == 0

∀k ∈ N, ∀i ∈ L\{D(k)}, ∀j ∈ L,
si,k ≥ 1 + sj,k − m(1 − xj,i,k)

∀k ∈ N, ∀i ∈ L, si,k ≥ 0

(14)

(15)

(16)

• For each delivery demand, one truck can be assigned at

most:

∀i ∈ L, ∀j ∈ L,

n−1
(cid:88)

k=0

ri,j,k ≤ 1

(17)

• If truck k takes the demand from location i to location j,
truck k’s trajectory(tour) should include i, j and j appears
after i:

∀i ∈ L, ∀k ∈ N, ∀j ∈ L\{D(k)},

ri,j,k ≤

(cid:32)m−1
(cid:88)

xi,l,k

(cid:33) (cid:32)m−1
(cid:88)

l=0

l=0

ri,j,k(sj,k − si,k) ≥ 0

(cid:33)

xl,j,k

,

(18)

∀i ∈ L, ∀k ∈ N, ri,D(k),k ≤

(cid:32)m−1
(cid:88)

xi,l,k

(cid:33) (cid:32)m−1
(cid:88)

l=0

l=0

(cid:33)

xl,D(k),k

(19)

• Within the capacity limit:
∀k ∈ N, ∀i ∈ L\{D(k)},

vi,k ==

m−1
(cid:88)

j=0

vj,kxj,i,k +

m−1
(cid:88)

l=0

ri,l,kdi,l −

m−1
(cid:88)

l=0

rl,i,kdl,i

(20)

7

∀k ∈ N, vD(k),k ==

m−1
(cid:88)

i=0

rD(k),i,kdD(k),i

∀i ∈ L, ∀k ∈ N, 0 ≤ vi,k ≤ ukC

(21)

(22)

Note that MILP’s time complexity increases with the num-
ber of packages and trucks and is not scalable. In this case,
we will efﬁciently combine it with DeepFreight for a scalable
solution.

B. The Workﬂow of DeepFreight+MILP

To exploit the advantages of DeepFreight and MILP, we
propose an integration of the two, that is DeepFreight gives
out dispatch and assignment decisions for most of the requests,
while MILP is adopted to ﬁnd the exact truck routing for the
remaining unmatched requests. Its workﬂow is described as
below:
1) Get the initial dispatch decisions and matching results using

Algorithm 1;

2) Deﬁne a key parameter called eﬃciency, which equals
the number of packages delivered by the truck divided by
its driving time, and calculate eﬃciency for each truck;
3) Eliminate all the dispatch decisions of the trucks whose

eﬃciency is lower than the threshold;

4) Rematch the package list with the pruned dispatch deci-

sions, and get the unmatched package list;

5) Pick the new truck list: choose two trucks from the initial
truck list for each distribution center that has unmatched
packages, based on the trucks’ priority deﬁned as Equa-
tion (23); (available time means the truck’s available
time (in seconds) for dispatch before the time limit and
eta means the ETA (in seconds) from the truck to the
distribution center, so the truck with a higher priority is
preferred.)

priority = available time − 2 ∗ eta

(23)

6) Adopt the MILP optimizer to get the routing result for the
new truck list according to the unmatched package list.
The threshold of eﬃciency should be determined by the
total number of the packages and the total driving time
of the ﬂeet that we’d like to achieve. By eliminating the
inefﬁcient dispatch decisions, better utilization of the ﬂeet can
be realized. However, note that a higher threshold means a
larger unmatched package list and the MILP optimizer may
not be able to ﬁnd the optimal routing results when the package
number is too large (e.g. shown as Figure 9(a)), so there should
be a tradeoff. After ﬁne-tuning, we set the threshold as 0.028
(≈ 40000/(20 ∗ 20 ∗ 3600)) in our experiment, that is how
much the eﬃciency should be if 20 trucks complete 40000
requests with an average driving time of 20 hours.

VI. EVALUATION AND RESULTS

In this section, we introduce the setup of the simulator for
evaluation, then the implementation details of DeepFreight,
and last compare the results among the algorithms: Deep-
Freight, DeepFreight without Multi-transfer, MILP and Deep-
Freight+MILP.

Fig. 4: Amazon Distribution Centers on Google Map

A. Simulation Setup

As shown in Figure 4, ten Amazon distribution centers in
the eastern U.S. are chosen as the origins and destinations of
delivery requests in the simulation. For each episode, Nt trucks
should complete Nr randomly generated requests within the
time limit Tmax, while trying to minimize the fuel cost. The
generation of the request and truck list is model-based, which
is introduced as follows:

As mentioned above, each delivery request can be repre-
sented as (source, destination, size). Equation (24) shows
the distribution of the requests’ sources, where popi represents
the population in the vicinity of distribution center i which is
acquired through its zip code 3. After conﬁrming the source
of a delivery request, its destination is randomly generated
according to the distribution deﬁned as Equation (25), where
norm is the normalization coefﬁcient. Further, the size of
each package is represented as an integer uniformly distributed
between 1 and 30 in our simulation.

For each truck in the ﬂeet, we need to specify its maximum
capacity and its location at the beginning of each operation
cycle. In our simulation, an operation cycle includes 7 episodes
and the time limit Tmax for each episode is 2 days. Within
an operation cycle, the trucks’ locations at the beginning of a
new episode are the same as those at the end of the previous
episode, while the distribution of the trucks’ locations at the
ﬁrst episode is the same as that of the packages’ sources
(Equation (24)), which is convenient for the ﬂeet to pick up
the packages.

P (source = i) =

popi
k=1 popk

(cid:80)10

P (destination = j|i) = norm ∗

popj√
etai,j

(24)

(25)

Moreover, we set

the truck number Nt as 20, and the
maximum capacity for each truck as 30000. Also, with the
consideration that trucks should be efﬁciently used, the total
volume of the the delivery requests should match the load
capacity of the ﬂeet, so the request number Nr is set as 40000
in our simulation.

3 https://www.cdxtech.com/tools/demographicdata/

B. Implementation Details of DeepFreight

In this section, we introduce the key parameters about the

network structure and training process of DeepFreight.

As previously introduced, the environment state deﬁned in
this task includes three items: unﬁnished delivery requests,
distribution of the trucks and the current epoch number.
The unﬁnished requests are represented as a matrix, and
the element in the i-th row and j-th column represents the
total volume of the packages that need to be delivered from
Location i to Location j, so the size of this term is 10 × 10
(location number × location number). The distribution of the
trucks means the truck number in each location, so its size
is 10 (location number). As for the current epoch number,
it will be input as a one-hot vector, so its size equals the
number of the epochs within an episode, which is set as 10
in our experiment. Compared with the environment space,
the individual observation space is for a single truck, so it
replaces the trucks’ distribution with its current location, which
will also be input as a one-hot vector whose dimension is 10
(location number). Moreover, the dimension of the individual
action apace is set as 11, where the ﬁrst 10 dimensions
represent the locations to dispatch and the last dimension is an
invalid action which means ”doing nothing”. By now, as shown
in Figure 2, we have known the dimensions of the input and
output layers of Qsingle and Qmix. As for the hidden layers,
the unit number for all of them are set as 64, and the activation
function between the hidden layers are set as ReLu.

During the training process, some tricks about deep Q-
learning are adopted. Firstly, we adopt Replay Buffer and
Target Network to improve the training stability. The size
of the replay buffer is set as 500, which means data of up
to 500 episodes can be stored for training use. Moreover, as
mentioned in Equation (6), the target network is adopted to
calculate the target joint q-value, and the weights of the target
network will be copied from Qsingle and Qmix periodically at
an interval of 50 episodes, which can cut down the instability
during training effectively. Secondly, Boltzmann Distribution
is adopted to improve the exploration in the learning process.
The key parameter for this method is temperature, and a
higher temperature leads to higher exploration. Like the
Simulated Annealing Algorithm, the initial temperature is set
as 100 and it will drop 0.1 per episode. After 1000 episodes,
the agents will begin to use the Greedy Policy instead.

C. Discussions and Results

Training Process of DeepFreight:

The training process includes 2800 episodes, that is 400
operation cycles, and at each episode, the networks will be
trained for 100 iterations. From the loss curve in Figure 5, it
can be observed that the training process starts to converge
at the 1500th episode. After that, the maximum joint Q-value
keeps ﬂuctuating around the value 21.5.

Further, the training effect can be seen from the increase
of the reward shown in Figure 6(a). The reward function for
evaluation is the same as that for training (Equation (3)), which
is related to the average driving time of the ﬂeet (0-48 hours)
and the number of packages served by them (0-40000). β1

8

Fig. 5: Changing curves of loss function (Equations (5)) and
target joint q-value (Equation (6)) for DeepFreight, which
show its learning process. We see convergence in approxi-
mately 1500 steps and in each step the network is trained for
100 iterations.

and β2 are key parameters for deﬁning the reward function.
After ﬁne-tuning, we set them as: β1 = 0.004, β2 = 0.5.
The increase of the reward is mainly due to the reduction
of the unﬁnished number of packages, which decreases from
˜2000 to ˜1200 and the lowest number shown in Figure 6(b)
is about 700. Also, it’s worth noting that the minimum of
the unﬁnished number of packages and average driving time
both occur between the 1500th and 2000th episode, so we
pick the optimal checkpoint from this interval for the further
experiments, as a comparison with other approaches. The
comparison is based on their performance within an operation
cycle, which equals two weeks as deﬁned in Section VI-A.
DeepFreight vs. DeepFreight without Multi-transfer:

In order to show the improvement brought by the ﬂexibility
of multiple transfers, we do some comparison between our
approach and a freight delivery system without multi-transfer,
of which the results are shown in Figure 6(a)-(c). For the
system without multi-transfer, each package will be delivered
to its destination by a ﬁxed truck, so there is an extra restriction
on the truck used when executing the matching policy.

It can be observed that after convergence, with the ﬂexibility
of multi-transfer,
the average driving time is shorter (˜2
hours (7.5%) reduction) and number of unﬁnished packages is
lower (˜1500 packages (60%) reduction), which means better
utilization of the ﬂeet is realized.
DeepFreight vs. MILP:

MILP is used as the baseline algorithm, which is deﬁned in
Section V.A and solved through a state-of-the-art optimization
solver: Gurobi. The evaluation of MILP shows that it has
poor scalability. The ﬂeet can carry up to 40000 packages
at the same time. Figure 7(a)-(c) show that when the number
of packages is 30000, the MILP solver can complete all the
requests at a fairly low fuel cost (with an average driving time
of 12.169 hours). However, its performance drops dramatically
with growing number of packages: not only the unﬁnished
number of packages increases, but also the ratio of the
unﬁnished requests goes up.

Further, Figure 9(a) shows the convergence curves of the
MILP solver, when the number of delivery requests is 40000.

9

(a) Reward function

(b) Number of unﬁnished packages

(c) Average driving time of the ﬂeet

Fig. 6: (a)-(c): Training curves of DeepFreight with/without Multi-transfer, in terms of reward function, number of unﬁnished
packages and the ﬂeet’s average driving time. DeepFreight with Multi-transfer can complete more package requests with a
smaller driving time, and thus has a higher reward, which shows the improvement brought by the ﬂexibility of multiple transfers.

(a) Average driving time of the ﬂeet

(b) Number of unﬁnished packages

(c) Ratio of the unﬁnished packages

Fig. 7: Plotting MILP’s performance for varying number of requests. It can be observed that not only the driving time and the
number of unﬁnished packages increase (shown as (a)(b)), the ratio of unﬁnished packages (normalized by the total package
number) also grows (shown as (c)), demonstrating the poor scalability of MILP.

It can be observed that the optimization objective (deﬁned
as Equation (7)-(9)) converges within 3000s and then the
performance doesn’t
increase any more, which means the
MILP solver cannot ﬁnd a solution for this task even if given
more time. Note that the MILP solver runs on a device with
an Intel i7-10850H processor.

Next, we compare Deepfreight and MILP. First, in terms
of time complexity, Qsingle trained with DeepFreight can be
executed in a decentralized manner. When testing, it can give
out the multi-step dispatch decisions for each truck in real
time. While, when using MILP, we have to run the optimizer
for every episode, since the problem scenario has changed.
Also, the time spent will increase with the number of the
trucks, distribution centers and packages.

Second, as for their performance, Figure 8(b) shows that
DeepFreight can complete more requests than MILP. However,
its performance is unstable, which is one of the characteristics
of reinforcement learning. Also, the average driving time taken
of DeepFreight is much higher than that of MILP.

Overall, as compared to MILP, DeepFreight is transferable
among different problem scenarios and has lower time com-
plexity, but the average driving time and unﬁnished number
of packages need to be further reduced, which motivates our
design of DeepFreight+MILP.
DeepFreight vs. DeepFreight+MILP:

Among all the evaluation metrics, serving all the pack-

ages is given top priority. As shown in Figure 8(b), Deep-
Freight+MILP completes all the delivery requests of an op-
eration cycle, eliminating the uncertainty and instability of
DeepFreight, which makes it suitable for the industrial use.

Figure 8(c) shows that DeepFreight+MILP has a further
reduction in average driving time as compared with Deep-
Freight and with comparable driving time as MILP, Deep-
Freight+MILP realizes a 100% delivery success.

Figure 9(b) shows the convergence curves of Deep-
Freight+MILP when using the optimizer, and it can be ob-
served that we can get
the routing result for the rest of
the packages within 10 minutes. That is because most of
packages have been served by the dispatch decisions made by
DeepFreight and only 4000-5000 packages are left for MILP
to serve. Also, not all the trucks are used for dispatch–as
deﬁned in Section V.B, only the trucks that are closer to the
packages’ origins and have more available driving time for
dispatch before the time limit are chosen as the new truck list.
In this case, DeepFreight+MILP can still be adopted when the
number of packages or number of truck is large, which ensures
its scalability.

Overall, DeepFreight+MILP performs best among these
algorithms, because it not only has better scalability and lower
time complexity but also can ensure a 100% delivery success
with fairly low fuel consumption.

10

(a) Reward function

(b) Number of unﬁnished packages

(c) Average driving time of the ﬂeet

Fig. 8: (a)-(c): Comparisons of different algorithms, in terms of the reward function, the number of unﬁnished packages and
the ﬂeet’s average driving time. As a comprehensive index, the reward function we use for evaluation is the same as that for
training (deﬁned in Equation (3)), which gives priority to the rate of delivery success. DeepFreight+MILP performs best among
these algorithms. It achieves 100% delivery success with low fuel consumption.

(a) Optimization objective of MILP

(b) Optimization objective of DeepFreight+MILP

Fig. 9: (a)-(b): Convergence curves of the optimization objective of MILP and DeepFreight+MILP over time, when adopting
the MILP solver. The optimization objective is deﬁned in Equation (7)-(9). It can be observed that MILP falls into a local
optimum with poor performance, while DeepFreight+MILP can obtain a signiﬁcantly improved solution within 10 minutes. It
demonstrates that DeepFreight+MILP has a better scalability and much lower time complexity.

VII. CONCLUSION

This paper proposes DeepFreight, a novel model-free ap-
proach for multi-transfer freight delivery based on deep re-
inforcement learning. The problem is sub-divided into truck-
dispatch and package-matching, where QMIX, a multi-agent
reinforcement learning algorithm, is adopted for training the
dispatch policy and Depth First Search is used for matching in
a greedy fashion. This approach is then integrated with MILP
for further optimization. The evaluation results show superior
scalability and improved performance of the combined system
as compared to the MILP formulation alone and the learning-
based solution alone.

Extending the work with different priority packages is an
important future direction. In this case, real-time decisions
for accepting priority packages within the day could also be
investigated.

REFERENCES

[1] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and
S. Whiteson, “Qmix: Monotonic value function factorisation for deep
multi-agent reinforcement learning,” arXiv preprint arXiv:1803.11485,
2018.

[2] G. B. Dantzig and J. H. Ramser, “The truck dispatching problem,”

Management science, vol. 6, no. 1, pp. 80–91, 1959.

[3] G. Clarke and J. W. Wright, “Scheduling of vehicles from a central depot
to a number of delivery points,” Operations research, vol. 12, no. 4, pp.
568–581, 1964.

[4] K. Braekers, K. Ramaekers, and I. Van Nieuwenhuyse, “The vehicle
routing problem: State of the art classiﬁcation and review,” Computers
& Industrial Engineering, vol. 99, pp. 300–313, 2016.

[5] S. N. Parragh, K. F. Doerner, and R. F. Hartl, “A survey on pickup and
delivery problems,” Journal f¨ur Betriebswirtschaft, vol. 58, no. 2, pp.
81–117, 2008.

[6] Y. Dumas, J. Desrosiers, and F. Soumis, “The pickup and delivery
problem with time windows,” European journal of operational research,
vol. 54, no. 1, pp. 7–22, 1991.

[7] J. R. Montoya-Torres, J. L. Franco, S. N. Isaza, H. F. Jim´enez, and
N. Herazo-Padilla, “A literature review on the vehicle routing problem
with multiple depots,” Computers & Industrial Engineering, vol. 79, pp.
115–129, 2015.

[8] C. Wang, D. Mu, F. Zhao, and J. W. Sutherland, “A parallel simulated
annealing method for the vehicle routing problem with simultaneous
pickup–delivery and time windows,” Computers & Industrial Engineer-
ing, vol. 83, pp. 111–122, 2015.

[9] A. S. Tasan and M. Gen, “A genetic algorithm based approach to vehicle
routing problem with simultaneous pick-up and deliveries,” Computers
& Industrial Engineering, vol. 62, no. 3, pp. 755–761, 2012.

[10] F. P. Goksal, I. Karaoglan, and F. Altiparmak, “A hybrid discrete particle
swarm optimization for vehicle routing problem with simultaneous
pickup and delivery,” Computers & Industrial Engineering, vol. 65,
no. 1, pp. 39–53, 2013.

[11] M. Avci and S. Topaloglu, “An adaptive local search algorithm for
vehicle routing problem with simultaneous and mixed pickups and
deliveries,” Computers & Industrial Engineering, vol. 83, pp. 15–29,
2015.

11

[12] T. Oda and C. Joe-Wong, “Movi: A model-free approach to dynamic
ﬂeet management,” in IEEE INFOCOM 2018-IEEE Conference on
Computer Communications.

IEEE, 2018, pp. 2708–2716.

[13] A. O. Al-Abbasi, A. Ghosh, and V. Aggarwal, “Deeppool: Distributed
model-free algorithm for ride-sharing using deep reinforcement learn-
ing,” IEEE Transactions on Intelligent Transportation Systems, vol. 20,
no. 12, pp. 4714–4727, 2019.

[14] O. de Lima, H. Shah, T.-S. Chu, and B. Fogelson, “Efﬁcient rideshar-
ing dispatch using multi-agent reinforcement learning,” arXiv preprint
arXiv:2006.10897, 2020.

[15] T. Teubner and C. M. Flath, “The economics of multi-hop ride sharing,”
Business & Information Systems Engineering, vol. 57, no. 5, pp. 311–
324, 2015.

[16] A. Singh, A. Alabbasi, and V. Aggarwal, “A distributed model-free
algorithm for multi-hop ride-sharing using deep reinforcement learning,”
arXiv preprint arXiv:1910.14002, 2019.

[17] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,
“Value-decomposition networks for cooperative multi-agent
learning
based on team reward.” in AAMAS, 2018, pp. 2085–2087.

[18] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, “Qtran:
Learning to factorize with transformation for cooperative multi-agent
reinforcement learning,” arXiv preprint arXiv:1905.05408, 2019.
[19] A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson, “Maven:
Multi-agent variational exploration,” in Advances in Neural Information
Processing Systems, 2019, pp. 7613–7624.

[20] C. Zhang, P. Odonkor, S. Zheng, H. Khorasgani, S. Serita, and C. Gupta,
“Dynamic dispatching for large-scale heterogeneous ﬂeet via multi-agent
deep reinforcement learning,” arXiv preprint arXiv:2008.10713, 2020.
[21] S. Iqbal, C. A. S. de Witt, B. Peng, W. B¨ohmer, S. Whiteson, and
F. Sha, “Ai-qmix: Attention and imagination for dynamic multi-agent
reinforcement learning,” arXiv preprint arXiv:2006.04222, 2020.
[22] D. Ha, A. M. Dai, and Q. V. Le, “Hypernetworks,” in 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
2017. [Online]. Available: https://openreview.net/forum?id=rkpACe1lx

