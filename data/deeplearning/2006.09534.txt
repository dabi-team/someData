Towards improving discriminative reconstruction via simultaneous
dense and sparse coding

Abiy Tasissa1, Emmanouil Theodosis2, Bahareh Tolooshams2, and Demba Ba2

1Department of Mathematics, Tufts University, Medford, MA
2School of Engineering and Applied Sciences, Harvard University, Cambridge, MA

Abstract

Discriminative features extracted from the sparse coding model have been shown to perform well for
classiﬁcation and reconstruction. Recent deep learning architectures have further improved reconstruc-
tion in inverse problems by considering new dense priors learned from data. We propose a novel dense
and sparse coding model that integrates both representation capability and discriminative features. The
model considers the problem of recovering a dense vector x and a sparse vector u given measurements of
the form y = Ax + Bu. Our ﬁrst analysis proposes a natural geometric condition based on the minimal
angle between spanning subspaces corresponding to the measurement matrices A and B to establish the
uniqueness of solutions to the linear system. The second analysis shows that, under mild assumptions, a
convex program recovers the dense and sparse components. We validate the eﬀectiveness of the proposed
model on simulated data and propose a dense and sparse autoencoder (DenSaE) tailored to learning the
dictionaries from the dense and sparse model. We demonstrate that a) DenSaE denoises natural images
better than architectures derived from the sparse coding model (Bu), b) in the presence of noise, training
the biases in the latter amounts to implicitly learning the Ax + Bu model, c) A and B capture low-
and high-frequency contents, respectively, and d) compared to the sparse coding model, DenSaE oﬀers a
balance between discriminative power and representation.

1
2
0
2

y
a
M
0
1

]
T
I
.
s
c
[

2
v
4
3
5
9
0
.
6
0
0
2
:
v
i
X
r
a

1

Introduction

Given a data set, learning a dictionary in which each example admits a sparse representation is tremen-
dously useful in a number of tasks (Aharon et al., 2006; Mairal et al., 2011). This problem, known as sparse
coding (Olshausen and Field, 1997) or dictionary learning (Garcia-Cardona and Wohlberg, 2018), has been
the subject of signiﬁcant investigation in recent years in the signal processing community. A growing body
of work has mapped the sparse coding problem into encoders for sparse recovery (Gregor and Lecun, 2010),
and into autoencoders purely for classiﬁcation (Rolfe and LeCun, 2013) or denoising (Simon and Elad, 2019;
Tolooshams et al., 2020) purposes.

Autoencoders are widely used for unsupervised learning. Their integration with supervised tasks and clas-
siﬁers has become popular for their regularization power and reduction of the generalization gap (Vincent
et al., 2010; Epstein et al., 2018; Epstein and Meir, 2019). Rolfe and LeCun (2013) have shown beneﬁts of
autoencoders and sparse features in discriminative tasks.

For data reconstruction, recent work has highlighted some limitations of convolutional sparse coding (CSC)
autoencoders (Simon and Elad, 2019) and its multi-layer and deep generalizations (Sulam et al., 2019; Zazo
et al., 2019). Simon and Elad (2019) argue that the sparsity levels that CSC allows can only accommodate
very sparse vectors, making it unsuitable to capture all features of signals such as natural images, and pro-
pose to compute the minimum mean-squared error solution under the CSC model, which is a dense vector
capturing a richer set of features.

To address the aforementioned limitations of classical sparse coding, we propose a dense and sparse coding
model that represents a signal as the sum of two components: one that admits a dense representation x in
a dictionary A that is useful for reconstruction, and another whose representation u is discriminative and
sparse in a second dictionary B. Based on empirical evidence, the authors in (Zazo et al., 2019) argue that
a multi-layer extension of this model can, in principle, have arbitrary depth. However, to our knowledge,
the dense and sparse coding model has not been yet fully analyzed. Our contributions are

Conditions for identiﬁability and recovery by convex optimization: We derive conditions under
which the dense and sparse representation is unique. We then propose a convex program for recovery that
minimizes

Ax

u

2
2 +
||

||

||

||1, subject to linear constraints.

Phase-transition curves: We demonstrate through simulations that the convex program can successfully
solve the dense and sparse coding problem.

Discriminative reconstruction: We propose a dense and sparse autoencoder (DenSaE) that has compet-
itive discriminative power and improves the representation capability compared to sparse networks.

The paper is organized as follows. Section 3 discusses theoretical analysis of the dense and sparse coding
problem. Phase transition, classiﬁcation, and denoising experiments appear in Section 4. We conclude in
Section 5.

1

 
 
 
 
 
 
2 Related work

We comment on the most closely related models. Given the measurements y, the problem of recovering
x and u is similar in ﬂavor to sparse recovery in the union of dictionaries (Donoho and Huo, 2001; Elad
and Bruckstein, 2002; Donoho and Elad, 2003; Soltani and Hegde, 2017; Studer et al., 2011; Studer and
Baraniuk, 2014). Most results in this literature take the form of an uncertainty principle that relates the
sum of the sparsity of x and u to the mutual coherence between A and B, and which guarantees that the
representation is unique and identiﬁable by (cid:96)1 minimization. To our knowledge, the analysis of this program
is novel and in sharp contrast to classical settings in sparse approximation, in which the objective consists
of a single sparsifying norm, rather than the combination of diﬀerent norms. Robust PCA (Candès et al.,
2011), which decomposes a matrix as the sum of low-rank and sparse matrices, uses the combination of the
(cid:96)1 and nuclear norms, giving it a ﬂavor similar to our problem.

(cid:107)

∈

0, 1

W α

(cid:107)1 to

(cid:107)1 by letting α = (cid:2)x u(cid:3)T and

Our model resembles weighted LASSO (Lian et al., 2018; Mansour and Saab, 2017). Compared to weighted
u
LASSO, we can directly map the weighted LASSO objective
(cid:107)
; however, in the weighted
choosing appropriately the entries of a diagonal matrix W , with Wij ∈ {
}
LASSO formulation, constraints can only be enforced on the sparse component u. Our work diﬀers in that
a signiﬁcant part of our analysis is the directed Euclidean norm constraint on x, which recovers a unique
solution x(cid:63)
Ker(A)⊥. Our model can also be interpreted as a special case of Morphological Component
Analysis (MCA) (Elad et al., 2005) for K = 2, s = (cid:80)K
k=1 Φkαk, with, however, some distinct diﬀerences: i)
MCA encodes diﬀerent morphological structures via the dictionaries Φk. We encode a smooth morphological
component via the whole product Ax, which is conceptually diﬀerent, and ii) we make no assumption of
sparsity on the dense component x. This leads to an optimization objective that is the combination of (cid:96)1
and (cid:96)2 norms, unlike that of MCA. Finally, a bare application of noisy sparse coding would treat e = Ax
as arbitrary noise, hence i) recovers u approximately and ii) cannot recover x. However, in our analysis,
the term Ax is not just undesired noise but represents a sought-out feature. We can recover both x and u
exactly. See Appendix C for a comparison of our model to noisy compressive sensing. We note that the
full dense and sparse coding model is y = Ax + Bu + e where e is Gaussian noise.

∈

1, ..., n

Rn and a support set S
p, AS is a submatrix of size m

Notation: Lowercase and uppercase boldface letters denote column vectors and matrices, respectively.
Given a vector x
, xS denotes the restriction of x to indices in S. For
}
∈
Rm
with column indices in S. The column space of a
a matrix A
S
×
× |
matrix A (the span of the columns of A) is designated by Col(A), its null space by Ker(A). We denote the
x
Euclidean, (cid:96)1 and (cid:96)
. The operator and inﬁnity
||1, and
x
||2,
norms of a vector, respectively as
||
||
norm of a matrix A are respectively denoted as
. The sign function, applied componentwise
A
and
||∞
||
to a vector x, is denoted by sgn(x). The indicator function is denoted by 1. The column vector ei denotes
the vector of zeros except a 1 at the i-th location. The orthogonal complement of a subspace W denoted by
W ⊥. The operator

W denotes the orthogonal projection operator onto the subspace W .

⊂ {

||∞

A

x

∞

||

||

||

|

P

3 Theoretical Analysis

The dense and sparse coding problem studies the solutions of the linear system y = Ax + Bu. Given
Rm, the goal is to provide conditions under which
matrices A
there is a unique solution (x∗, u∗), where u∗ is s-sparse, and an algorithm for recovering it.

n and a vector y

p and B

Rm

Rm

∈

∈

∈

×

×

3.1 Uniqueness results for the feasibility problem

In this subsection, we study the uniqueness of solutions to the linear system accounting for the diﬀerent
structures the measurement matrices A and B can have. For more details of all the diﬀerent cases we
consider, we refer the reader to Appendix A. The main result of this subsection is Theorem 3 which, under
a natural geometric condition based on the minimum principal angle between the column space of A and the
span of s columns in B, establishes a uniqueness result for the dense and sparse coding problem. Since the
vector u in the proposed model is sparse, we consider the classical setting of an overcomplete measurement
m. The next theorem provides a uniqueness result assuming a certain direct sum
matrix B with n
representation of the space Rm.

(cid:29)

Theorem 1 Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x∗, u∗). Let
S, with
Col(BS), the
only unique solution to the linear system, with the condition that any feasible s-sparse vector u is supported
on S and any feasible x is in Ker(A)⊥, is (x∗, u∗).

= s, denote the support of u∗. If BS has full column rank and Rm = Col(A)

S
|

⊕

|

−

Ker(A)⊥, be another solution pair. It follows that
Proof 1 Let (x, u), with u supported on S and x∗
∈
q be matrices whose
u∗S. Let U
x∗ and δ2 = uS −
Aδ1 + BS(δ2)S = 0 where δ1 = x
∈
columns are the orthonormal bases of Col(A) and Col(BS) respectively. The equation Aδ1 + BS(δ2)S = 0
Ui + (cid:80)q
can equivalently be written as (cid:80)r
Vi = 0 with Ui and Vi denoting the
BS(δ2)S , Vi(cid:105)
Aδ1 , Ui(cid:105)
i=1(cid:104)
r
i-th column of U and V respectively. More compactly, we have (cid:2)U V (cid:3)
i=1
= 0. Noting
q
i=1
{(cid:104)
that the matrix (cid:2)U V (cid:3) has full column rank, the homogeneous problem admits the trivial solution implying
, it folows
that Aδ1 = 0 and BS(δ2)S = 0. Since BS has full column rank and δ1 ∈ {
that δ1 = δ2 = 0. Therefore, (x∗, u∗) is the unique solution.

Aδ1 , Ui(cid:105)}
{(cid:104)
BS(δ2)S , Vi(cid:105)}
Ker(A)

Ker(A)⊥

r and V

i=1(cid:104)

Rm

Rm

∈

∩

(cid:20)

(cid:21)

}

×

×

2

The uniqueness result in the above theorem hinges on the representation of the space Rm as the direct sum
of the subspaces Col(A) and Col(BS). We use the deﬁnition of the minimal principal angle between two
subspaces, and its formulation in terms of singular values (Björck and Golub, 1973), to derive an explicit
geometric condition for the uniqueness analysis of the linear system in the general case.

q be matrices whose columns are the orthonormal basis of Col(A)
Deﬁnition 2 Let U
and Col(B) respectively. The minimum principal angle between the subspaces Col(A) and Col(B) is deﬁned
as follows

r and V

∈

∈

×

×

Rm

Rm

The minimum angle µ(U , V ) is also equal to the largest singular value of U T V , cos(µ(U , V )) = σ1(U T V ).

cos(µ(U , V )) =

max
Col(U ),v
∈

u

∈

Col(V )

uT v
v
||2||

u

||

||2

,

(1)

Theorem 3 Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x∗, u∗). Let
r and
S, with
= s, denote the support of u∗. Assume that BS has full column rank . Let U
S
|
|
Rm
q be matrices whose columns are the orthonormal bases of Col(A) and Col(BS) respectively. If
V
×
cos(µ(U , V )) = σ1(U T V ) < 1, the only unique solution to the linear system, with the condition that any
feasible s-sparse vector u is supported on S and any feasible x is in Ker(A)⊥, is (x∗, u∗).

Rm

∈

∈

×

Proof 2 Consider any candidate solution pair (x∗ + δ1, u∗ + δ2). We will prove uniqueness by showing
that Aδ1 + BS(δ2)S = 0 if and only if δ1 = 0 and (δ2)S = 0. Using the orthonormal basis set U and V ,
Aδ1 + BSuS can be represented as : Aδ1 + BS(δ2)S = (cid:2)U V (cid:3)
. For simplicity of notation,
let K denote the block matrix: K = (cid:2)U V (cid:3). If we can show that the columns of K are linearly independent,
it follows that Aδ1 + BS(δ2)S = 0 if and only if Aδ1 = 0 and BS(δ2)S = 0. We now consider the matrix
KT K which has the following representation

(cid:20) U T Aδ1
V T BS(δ2)S

(cid:21)

KT K =

=

(cid:20) [I]r
r
×
[V T U ]q
(cid:20)[I]r
[0]q

×

r

r

×

r
×
[0]r
[I]q

(cid:21)

q

[U T V ]r
×
[I]q
q
(cid:21)

×

+

(cid:20) [0]r
r
×
[V T U ]q

(cid:21)

q

.

[U T V ]r
×
[0]q
q

×

r

×

q

×

q

×

With the singular value decomposition of U T V being U T V = QΣRT , the last matrix in the above repre-

sentation has the following equivalent form

that

(cid:20) 0
V T U

(cid:21)

U T V
0

is similar to the matrix

(cid:21)

U T V
0

(cid:20) 0
V T U
(cid:21)
(cid:20) 0 Σ
. Hence, the nonzero eigenvalues of KT K are 1
Σ 0

(cid:21) (cid:20)Q 0
0 R

(cid:21) (cid:20) 0 Σ
Σ 0

(cid:20)Q 0
0 R

(cid:21)T

=

. It now follows

σi,

±

i

≤

1
≤
results the bound λmin
Aδ1 = 0 and BS(δ2)S = 0. Since BS is full column rank and δ1 ∈ {
δ1 = 0 and (δ2)S = 0. This concludes the proof.

min(p, q), with σi denoting the i-th largest singular value of U T V . Using the assumption σ1 < 1
(cid:0)KT K(cid:1) > 0. It follows that the columns of K are linearly independent, and hence
, it follows that
}

Ker(A)⊥

Ker(A)

∩

A restrictive assumption of the above theorem is that the support of the sought-after s-sparse solution u∗ is
known. We can remove this assumption by considering Col(A) and Col(BT ) where T is an arbitrary subset
= s. More precisely, we state the following corollary whose proof is similar to the
with
of
|
proof of Theorem 3.

1, 2, ..., n

T

{

}

|

|

|

S

= s, denote the support of u∗ and T be an arbitrary subset of

Corollary 4 Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x∗, u∗). Let
s. Assume
S, with
with
}
Rm
q be matrices whose
that any 2s columns of B are linearly independent. Let U
T ) respectively. If µ(U , V ) = σ1(U T V ) < 1,
columns are the orthonormal bases of Col(A) and Col(BS
holds for all choices of T , the only unique solution to the linear system is (x∗, u∗) with the condition that
any feasible u is s-sparse and any feasible x is in Ker(A)⊥.

{
p and V

1, 2, ..., n

Rm

| ≤

∈

∈

T

×

×

∪

|

Of interest is the identiﬁcation of simple conditions such that σ1(U T V ) < 1. The following theorem proposes
one such condition to establish uniqueness.

Theorem 5 Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x∗, u∗). Let
r and
S, with
= s, denote the support of u∗. Assume that BS has full column rank . Let U
S
|
|
Rm
q be matrices whose columns are the orthonormal bases of Col(A) and Col(BS) respectively. Let
V
×
∈
(U T V )i,j|
, the only unique solution to the linear system, with the condition that any
max
i,j
feasible s-sparse vector u is supported on S and any feasible x is in Ker(A)⊥, is (x∗, u∗).

= µ. If s < 1
√rµ

Rm

∈

×

|

Proof 3 It suﬃces to show that σ1 < 1. Noting that σ1 =
U T V
inequality

U T V

||
U T V

√r

√r

as follows: σ1 ≤

||∞

||

||2 ≤

||

||

||∞ ≤

U T V

||2, we use the following matrix norm
√rµs < 1.

The constant µ is the coherence of the matrix U T V (Donoho et al., 2005; Tropp, 2004). The above result
states that if the mutual coherence of U T V is small, we can accommodate increased sparsity of the underlying
signal component u∗. We note that, up to a scaling factor, σ1(U T V ) is the block coherence of U and V
(Eldar et al., 2010). However, unlike the condition in (Eldar et al., 2010), we don’t restrict the dictionaries
In the next subsection, we propose a convex program
A and B to have linearly independent columns.
to recover the dense and sparse vectors. Theorem 8 establishes uniqueness and complexity results for the
proposed optimization program.

3

3.2 Dense and sparse recovery via convex optimization

Given that the dense and sparse coding problem seeks a dense vector x∗ and a sparse solution u∗, with
measurements given as y = Ax∗ + Bu∗, we propose the following convex optimization program

min
x,u ||

Ax

2
2 +
||

u

||

||1 s.t. y = Ax + Bu.

(2)

In this section, we show that, under certain conditions, the above minimization problem admits a unique
solution. Our proof is a non-trivial adaptation of the existing analysis in (Kueng and Gross, 2014) for the
anistropic compressive sensing problem. This analysis is based on a single measurement matrix and can not
be directly applied to our scenario. Let a1, ..., am be a sequence of zero-mean i.i.d random vectors drawn
from some distribution F on Rp and let b1, ..., bm be a sequence of zero-mean i.i.d random vectors drawn from
some distribution G on Rn. We can eliminate the dense component in the linear constraint by projecting
PCol(A)⊥ (Bu). With this,
the vector y onto the orthogonal complement of Col(A) to obtain
i where
the matrix
PCol(A)⊥ (B)]T ei denotes the i-th measurement vector corresponding to a row of this matrix. Further
ci = [
technical discussion on the matrix C is deferred to Appendix B. We use the matrix C introduced above
and adapt the anisotropic compressive sensing theory in (Kueng and Gross, 2014) to analyze uniqueness of
the proposed program. Below, we give brief background to this theory highlighting important assumptions
and results following the notation closely therein.

PCol(A)⊥(B) is central in the analysis to follow. We deﬁne the matrix C = 1

PCol(A)⊥ (y) =

i=1 eicT

(cid:80)m

√m

Anisotropic compressive sensing: Given a sequence of zero-mean i.i.d random vectors d1, ..., dm drawn
from some distribution F on Rn, with measurements y = Du∗, the anisotropic compressive sensing problem
studies the following optimization program

min

u ||

u

s.t. y = Du,

||1

(3)

where D = 1
√m
assumptions.

(cid:80)m

i=1 eidT

i and u∗ is the sought-out sparse solution. The analysis makes three important

Completeness: The covariance matrix Σ is invertible with condition number denoted by κ.

Incoherence: The incoherence parameter is the smallest number ν such that
d , ei(cid:105)|

d , E[cc∗]−

1ei|

n |(cid:104)

n |(cid:104)

≤

ν and max
≤

max
i
1
≤
≤

≤

2

1

2

i

≤

ν

(4)

hold almost surely.

Conditioning of the covariance matrix: We start with the following deﬁnition of the s-sparse condition
number restated from (Kueng and Gross, 2014).

Deﬁnition 6 (Kueng and Gross, 2014) The largest and smallest s-sparse eigenvalue of a matrix X are
given by

λmax(s, X) : = max
||0≤

v,

||

v

s

||

v,

||

λmin(s, X) : = min
||0≤
||
||
λmax(s, X)
λmin(s, X)

v

s

.

Xv
||2
v
||2
||
Xv
||2
v
||2

.

The s-sparse condition number of X is cond(s, X) =

Given these assumptions, the main result in (Kueng and Gross, 2014) reads

Theorem 7 (Kueng and Gross, 2014) With κs = max
cond(s, Σ), cond(s, Σ−
{
vector and let ω
convex program (3) is unique and equal to u∗ with probability at least 1

1. If the number of measurements fulﬁlls m

e−

≥

≥

ω.

Cn be an s-sparse
Cκs ν ω2 s log n, then the solution u of the

1)
}

let u

∈

−

The proof of Theorem 7 is based on the dual certiﬁcate approach. The idea is to ﬁrst propose a dual
certiﬁcate vector v with suﬃcient conditions that ensure uniqueness of the minimization problem. It then
remains to construct the dual certiﬁcate satisfying the conditions. We seek a similar result for the uniqueness
of the convex program corresponding to the dense and sparse coding model. However, the standard analysis
can not be directly applied since it only considers a single measurement matrix. This requires us to analyze
the matrix C introduced earlier. The anisotropic compressive sensing analysis in (Kueng and Gross, 2014)
assumes the following conditions on the dual certiﬁcate v

vS −
||
The following condition follows from the assumptions in Theorem 7

sgn(u∗S)

vS⊥||∞ ≤
||

||2 ≤

1
4 and

1
4 .

(5)

∆S⊥ ||2,
2
||
where ∆
Ker(D). The conditions (5) and (6) will be used in the proof of our main result. The main part
of the technical analysis in (Kueng and Gross, 2014) is using the assumptions in Theorem 7 and showing
that the above conditions (5) and (6) hold with high probability.

∆S||2 ≤
||

(6)

∈

Main result: Using the the background discussed above, we assume completeness, incoherence, and condi-
tioning of the covariance matrix Σ. Our main result is stated below.

4

Theorem 8 Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x∗, u∗). Let
ω

cond(s, Σ), cond(s, Σ−
1 and deﬁne κs = max
{

1)
. Assume the two conditions
}

≥

BT
||

S A

|| ≤

32

,

A

BT
S⊥
||

1
1
x∗||∞
x∗||2
||
||
Cκs ν ω2 s log n, then the solution of the convex program (2) is

||∞ ≤

(7)

32

.

If the number of measurements fulﬁlls m
unique and equal to (x∗, u∗) with probability at least 1

≥

ω.

e−

−

Proof sketch 1 Consider a feasible solution pair (x∗ + δ1, u∗ + δ2) and let the function f (x, u) denote the
objective in the optimization program. The idea of the proof is to show that any feasible solution is not minimal
in the objective value, f (x∗ + δ1, u∗ + δ2) > f (x, u). Using duality and characterization of the subgradient
2BT Ax∗ , δ2(cid:105)
Λ of the (cid:96)1 norm, we ﬁrst show that f (x∗ + δ1, u∗ + δ2) > f (x∗, u∗) +
PCol(A)⊥(B), denoting the dual certiﬁcate. It then remains to show that the
where v
2BT Ax∗ , δ2(cid:105)
term
is positive. To show this, we further analyze this term and make use
of the assumptions of the theorem, the dual certiﬁcate conditions (5), and the deviation inequality in (6).
For a complete proof, see Appendix B.

Col(CT ), with C =
v

sgn(u∗S) + Λ
(cid:104)

sgn(u∗S) + Λ
(cid:104)

−

−

−

−

∈

v

Complexity compared to (cid:96)1 minimization: The sample complexity of solving the convex program cor-
responding to the dense and sparse coding problem is larger than that of (cid:96)1 minimization for the compressive
sensing problem. Essentially, the constants κs and ν in our analysis are expected to scale with p + n, in
contrast to the compressive sensing analysis where they scale with n.

4 Experiments

4.1 Phase transition curves

We generate phase transition curves and present how the success rate of the recovery, using the proposed
model, changes under diﬀerent scenarios. To generate the data, we ﬁx the number of columns of B to be
n = 100. Then, we vary the sampling ratio σ = m
m in the
n+p ∈
same range. The sensing matrix in our model is [A B], hence the apparent diﬀerence in the deﬁnition of
σ compared to “traditional” compressive sensing. In the case where we revert to the compressive sensing
scenario (p = 0), the ratios coincide.

[0.05, 0.95] and the sparsity ratio ρ = s

Rm

Rm

×

∈

p and B

Rp is generated as x = AT γ where γ

n whose columns have expected unit norm. The
We generate random matrices A
Rn has s randomly chosen indices, whose entries are drawn according to a standard normal
vector u
∈
Rm is a random vector. The construction
distribution, and x
ensures that x does not belong in the null space of A, and hence ignores trivial solutions with respect to
this dense component. We normalize both x and u to have unit norm, and generate the measurement vector
Rm as y = Ax + Bu. We solve the convex optimization problem in (2) to obtain the numerical solution
y
u
3.
pair ( ˆx, ˆu) using CVXPY, and register a successful recovery if both (cid:107)
(cid:107)2
(cid:107)2 ≤
For each choice of σ and ρ we average 100 independent runs to estimate the success rate.

(cid:15), with (cid:15) = 10−

x
(cid:107)2
(cid:107)2 ≤

(cid:15) and (cid:107)

∈

∈

∈

∈

−
u

−
x

×

ˆu

ˆx

(cid:107)

(cid:107)

to highlight diﬀerent ratios between p and
Figure 1 shows the phase transition curves for p
n. We observe that increasing p leads to a deterioration in performance. This is expected, as this creates a
greater overlap on the spaces spanned by A and B. We can view our model as explicitly modeling the noise
of the system. In such a case, the number of columns of A explicitly encodes the complexity of the noise
model: as p increases, so does the span of the noise space.

0.1m, 0.5m
}

∈ {

Extending the signal processing interpretation, note that we model the noise signal x as a dense vector,
which can be seen as encoding smooth areas of the signal that correspond to low-frequency components. On
the contrary, the signal u has, by construction, a sparse structure, containing high-frequency information, an
interpretation that will be further validated in the next subsection. Further numerical experiments comparing
the dense and sparse coding model to noisy compressive sensing can be found in Appendix C.

Figure 1: Phase transition curves for p = 0.1m (top) and p = 0.5m (bottom).

5

0.10.260.420.580.740.9σ=mn+p0.950.770.590.410.230.05ρ=smp=0.1m0.00.20.40.60.81.00.10.260.420.580.740.9σ=mn+p0.950.770.590.410.230.05ρ=smp=0.5m0.00.20.40.60.81.0Table 1: DenSaE’s performance on MNIST test dataset from both disjoint (D) and joint (Jβ) training.

SCNetLS

hyp SCNettied

hyp

-

-

D

J0.75

J1

A
A+B model
Acc.
Rec.
A
A+B class
A
A+B rec.
Acc.
Rec.
A
A+B class
A
A+B rec.
Acc.
Rec.
A
A+B class
A
A+B rec.

94.16
1.95
-
-
96.91
2.17
-
-

96.06
71.20
-
-

5A
25A
395B
375B
1.25 6.25

200A
200B
50

0
8

0
58

98.18 98.18 96.98
6.83 6.30 3.04
0
28
98.19 98.23 97.64
0.75 1.11 0.51
8
36

84
8

8
8

98.32
6.80
-
-
98.18
1.24
-
-

98.59 98.61 98.56 98.40
32.61 30.20 25.57
47.70
-
46
-
2

42
4

16
0

4.2 Classiﬁcation and image denoising

We formulate the dense and sparse dictionary learning problem as minimizing the objective

A,B,

{

xj

min
J
j=1,
}
{

J
(cid:88)

uj

J
j=1

}

j=1

1
2 (cid:107)

yj

−

Axj

Buj

2
2 +
(cid:107)

−

1
2λx (cid:107)

Axj

(cid:107)

2

2 + λu(cid:107)

uj

(cid:107)1,

where J is the number of images, λx controls the smoothness of Axj and λu controls the degree of sparsity.
Based on the objective, we use deep unfolding to construct a unfolding neural network (Tolooshams et al.,
2020; Gregor and Lecun, 2010), which we term the dense and sparse autoencoder (DenSaE), tailored to
learning the dictionaries from the dense and sparse model. The encoder maps yj into a dense vector xj
T
and a sparse one uj
T by unfolding T proximal gradient iterations. The decoder reconstructs the image. For
classiﬁcation, we use uT and xT as inputs to a linear classiﬁer C that maps them to the predicted class ˆq. We
learn the dictionaries A and B, as well as the classiﬁer C, by minimizing the weighted reconstruction (Rec.)
and classiﬁcation (Logistic) loss (i.e., (1
β) Rec. + β Logistic). Figure 5 shows the DenSaE architecture
(for details see Appendix D).

−

Encoder

Repeat T times

Decoder

αxAT

y

αuBT

Sb

xt

ut

xT

uT

A

B

B
A 1 + 1
λx

z C

Smax

Classiﬁer

ˆy

ˆq

Figure 2: DenSaE. The vector z is normalized stacked features with a column of 1 (i.e., z = [1;
Sb and
We examined the following questions.

Smax are the soft-thresholding and soft-max operators respectively.

[xT ;uT ]
[xT ;uT ]

(cid:107)

(cid:107)

]),

a) How do the discriminative, reconstruction, and denoising capabilities change as we vary the number of

ﬁlters in A vs. B?

b) What is the performance of DenSaE compared to sparse coding networks?
c) What data characteristics does the model capture?

As baselines, we trained two variants, CSCNettied
architecture tailored to dictionary learning for the sparse coding problem. In CSCNettied
hyper-parameter. In CSCNettied
loss. When the dictionaries are non-convolutional, we call the network SCNet.

LS , of CSCNet (Simon and Elad, 2019), an
hyp , the bias is a shared
LS , we learn a diﬀerent bias for each ﬁlter by minimizing the reconstruction

hyp and CSCNettied

4.2.1 DenSaE strikes a balance between discriminative capability and reconstruction

We study the case when DenSaE is trained on the MNIST dataset for joint reconstruction and classiﬁcation
purposes. We show a) how the explicit imposition of sparse and dense representations in DenSaE helps to
balance discriminative and representation power, and b) that DenSaE outperforms SCNet. We warm start
the training of the classiﬁer using dictionaries obtained by ﬁrst training the autoencoder, i.e., with β = 0.

Characteristics of the representations xT and uT : To evaluate the discriminative power of the repre-
sentations learned by only training the autoencoder, we ﬁrst trained the classiﬁer given the representations
(i.e., ﬁrst train A and B with β = 0, then train C with β = 1). We call this disjoint training. The ﬁrst
four rows of section D from Table 5 show, respectively, the classiﬁcation accuracy (Acc.), (cid:96)2 reconstruction

6

Noisy

Ax

Bu

Denoised

A

B

a) DenSaE 4A
60B

b) CSCNettied
LS

Original

Implicit Ax Implicit Bu Denoised

Implicit A

Implicit B

Unused

Figure 3: Visualization of a test image for τ = 50. a) DenSaE (4A, 60B), b) CSCNettied
LS .

loss (Rec.), and the relative contributions, expressed as a percentage, of the dense or sparse representations
to classiﬁcation and reconstruction for disjoint training. Each col of [A B], and of C, corresponds to either
a dense or a sparse feature. For reconstruction, we ﬁnd the indices of the 50 most important columns and
report the proportion of these that represent dense features. For each of the 10 classes (rows of C), we
ﬁnd the indices of the 5 most important columns (features) and compute the proportion of the total of 50
indices that represent dense features. The ﬁrst row of Table 5 shows the proportion of rows of [A B] that
represent dense features. Comparing this row, respectively to the third and fourth row of section D reveals
the importance of x for reconstruction, and of u for classiﬁcation. Indeed, the ﬁrst two rows of section D
show that, as the proportion of dense features increases, DenSaE gains reconstruction capability but results
in a lower classiﬁcation accuracy. Moreover, in DenSaE, the most important features in classiﬁcation are
all from B, and the contribution of A in reconstruction is greater than its percentage in the model, which
clearly demonstrates that dense and sparse coding autoencoders balance discriminative and representation
power.

The table also shows that DenSaE outperforms SCNetLS
hyp in reconstruction.
We observed that in the absence of noise, training SCNetLS
hyp results in dense features with negative biases,
hence, making its performance close to DenSaE with large number of atoms in A. We see that SCNetLS
hyp in
absence of a supervised classiﬁcation loss fails to learn discriminative features useful for classiﬁcation. On the
other hand, enforcing sparsity in SCNettied
hyp suggests that sparse representations are useful for classiﬁcation.

hyp in classiﬁcation and SCNettied

How do roles of xT and uT change as we vary β in joint training?: In joint training of the au-
toencoder and the classiﬁer, it is natural to expect that the reconstruction loss should increase compared
to disjoint training. This is indeed the case for SCNetLS
hyp; as we go from disjoint to joint training and as β
increases (Table 5, sections labeled J), the reconstruction loss increases and classiﬁcation accuracy has an
overall increase. However, for β < 1, joint training of both networks that enforce some sparsity on their
representations, SCNettied
hyp and DenSaE, improves reconstruction and classiﬁcation. Moreover, as we increase
the importance of classiﬁcation loss (i.e., increase β), the contribution of dense representations decreases in
reconstruction and increases in discrimination.

For purely discriminative training (β = 1), DenSaE outperforms both SCNetLS
hyp in classiﬁ-
cation accuracy and representation capability. We speculate that this likely results from the fact that, by
construction, the encoder from DenSaE seeks to produce two sets of representations: namely a dense one,
mostly important for reconstruction and a sparse one, useful for classiﬁcation. In some sense, the dense com-
ponent acts as a prior that promotes good reconstruction. More detailed results can be found in AppendixD.

hyp and SCNettied

Remark: As our network is non-convolutional, we do not compare it to the state-of-the-art, a convolutional
network. We do not compare our results with the network in (Rolfe and LeCun, 2013) as that work does
not report reconstruction loss and it involves a sparsity enforcing loss that change the learning behaviour.

4.2.2 Denoising

We trained DenSaE for supervised image denoising when β = 0 using BSD432 and tested it on BSD68 (Mar-
tin et al., 2001) (see Appendix D for details). We varied the ratio of number of ﬁlters in A and B as the
overall number of ﬁlters was kept constant. We evaluate the model in the presence of Gaussian noise with
standard deviation of τ =

15, 25, 50, 75

{

.
}

Ratio of number of ﬁlters in A and B: Unlike reconstruction, Table 2 shows that the smaller the
number of ﬁlters associated with A, the better DenSaE can denoise images. We hypothesize that this is a
direct consequence of our ﬁndings from Section 3 that the smaller the number of columns of A, the easier
the recovery x and u.

Dense and sparse coding vs. sparse coding: Table 3 shows that DenSaE (best network from Table 2)
denoises images better than CSCNettied
hyp , suggesting that the dense and sparse coding model represents im-
ages better than sparse coding.

Dictionary characteristics: Figure 3(a) shows the decomposition of a noisy test image (τ = 50) by Den-
SaE. The ﬁgure demonstrates that Ax captures low-frequency content while Bu captures high-frequency

7

Table 2: DenSaE’s denoising performance on test BSD68 as the ratio of ﬁlters in A and B changes.

τ 1A63B 4A60B 8A56B 16A48B 32A32B
30.18
15 30.21
30.18
27.65
25 27.70 27.70
24.43
50 24.81 24.81
23.09
23.33
75 23.31

29.89
27.26
23.68
20.09

30.14
27.56
24.44
22.09

Table 3: DenSaE vs. CSCNet on test BSD68.

hyp CSCNettied
LS

τ DenSaE CSCNettied
30.21
15
27.70
25
50 24.81
75 23.33

30.12
27.51
24.54
22.83

30.34
27.75
24.81
23.32

details (edges). This is corroborated by the smoothness of the ﬁlters associated with A, and the Gabor-like
nature of those associated with B (Mehrotra et al., 1992). We observed similar performance when we tuned
λx, and found that, as λx decreases, Ax captures a lower frequencies, and Bu a broader range.

CSCNet implicitly learns Ax + Bu model in the presence of noise: We observed that CSCNettied
LS
comprises three groups of ﬁlters: one with small bias, one with intermediate ones, and a third with large
values (see Appendix D for bias visualizations). We found that the feature maps associated with the large
bias values are all zero. Moreover, the majority of features are associated with intermediate bias values,
and are sparse, in contrast to the small number of feature maps with small bias values, which are dense.
These observations suggest that autoencoders implementing the sparse coding model (y = Bu), when learn-
ing the biases by minimizing reconstruction error, implicitly perform two functions. First, they select the
optimal number of ﬁlters. Second, they partition the ﬁlters into two groups: one that yields a dense rep-
resentation of the input, and another that yields a sparse one. In other words, the architectures trained in
this manner implicitly learn the dense and sparse coding model (y = Ax+Bu). Figure 3(b) shows the ﬁlters.

5 Conclusions

This paper proposed a novel dense and sparse coding model for a ﬂexible representation of a signal as
y = Ax + Bu. Our ﬁrst result gives a veriﬁable condition that guarantees uniqueness of the model. Our
second result uses tools from RIPless compressed sensing to show that, with suﬃciently many linear mea-
surements, a convex program with (cid:96)1 and (cid:96)2 regularizations can recover the components x and u uniquely
with high probability. Numerical experiments on synthetic data conﬁrm our observations.

We proposed a dense and sparse autoencoder, DenSaE, tailored to dictionary learning for the Ax+Bu model.
DenSaE, naturally decomposing signals into low- and high-frequency components, provides a balance between
learning dense representations that are useful for reconstruction and discriminative sparse representations.
We showed the superiority of DenSaE to sparse autoencoders for data reconstruction and its competitive
performance in classiﬁcation.

References

M. Aharon, M. Elad, and A. Bruckstein, “k-svd: An algorithm for designing overcomplete dictionaries for sparse

representation,” IEEE Transactions on signal processing, vol. 54, no. 11, pp. 4311–22, 2006.

J. Mairal, F. Bach, and J. Ponce, “Task-driven dictionary learning,” IEEE transactions on pattern analysis and

machine intelligence, vol. 34, no. 4, pp. 791–804, 2011.

B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete basis set: A strategy employed by v1?” Vision

research, vol. 37, no. 23, pp. 3311–3325, 1997.

C. Garcia-Cardona and B. Wohlberg, “Convolutional dictionary learning: A comparative review and new algorithms,”

IEEE Transactions on Computational Imaging, vol. 4, no. 3, pp. 366–81, Sep. 2018.

K. Gregor and Y. Lecun, “Learning fast approximations of sparse coding,” in Proc. International Conference on

Machine Learning (ICML), 2010, pp. 399–406.

J. T. Rolfe and Y. LeCun, “Discriminative recurrent sparse auto-encoders,” arXiv preprint arXiv:1301.3775, 2013.

D. Simon and M. Elad, “Rethinking the csc model for natural images,” in Proc. Advances in Neural Information

Processing Systems, 2019, pp. 2271–2281.

B. Tolooshams, S. Dey, and D. Ba, “Deep residual autoencoders for expectation maximization-inspired dictionary

learning,” IEEE Transactions on Neural Networks and Learning Systems, pp. 1–15, 2020.

P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, P.-A. Manzagol, and L. Bottou, “Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion.” Journal of machine learning
research, vol. 11, no. 12, 2010.

B. Epstein, R. Meir, and T. Michaeli, “Joint autoencoders: a ﬂexible meta-learning framework,” in Joint European

Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2018, pp. 494–509.

8

B. Epstein and R. Meir, “Generalization bounds for unsupervised and semi-supervised learning with autoencoders,”

arXiv preprint arXiv:1902.01449, 2019.

J. Sulam, A. Aberdam, A. Beck, and M. Elad, “On multi-layer basis pursuit, eﬃcient algorithms and convolutional

neural networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2019.

J. Zazo, B. Tolooshams, and D. Ba, “Convolutional dictionary learning in hierarchical networks,” in Proc. 2019 IEEE
IEEE,

8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP).
2019, pp. 131–135.

D. L. Donoho and X. Huo, “Uncertainty principles and ideal atomic decomposition,” IEEE transactions on information

theory, vol. 47, no. 7, pp. 2845–2862, 2001.

M. Elad and A. M. Bruckstein, “A generalized uncertainty principle and sparse representation in pairs of bases,”

IEEE Transactions on Information Theory, vol. 48, no. 9, pp. 2558–2567, 2002.

D. L. Donoho and M. Elad, “Optimally sparse representation in general (nonorthogonal) dictionaries via (cid:96)1 minimiza-

tion,” Proc. the National Academy of Sciences, vol. 100, no. 5, pp. 2197–2202, 2003.

M. Soltani and C. Hegde, “Fast algorithms for demixing sparse signals from nonlinear observations,” IEEE Transac-

tions on Signal Processing, vol. 65, no. 16, pp. 4209–4222, 2017.

C. Studer, P. Kuppinger, G. Pope, and H. Bolcskei, “Recovery of sparsely corrupted signals,” IEEE Transactions on

Information Theory, vol. 58, no. 5, pp. 3115–3130, 2011.

C. Studer and R. G. Baraniuk, “Stable restoration and separation of approximately sparse signals,” Applied and

Computational Harmonic Analysis, vol. 37, no. 1, pp. 12–35, 2014.

E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal component analysis?” Journal of the ACM (JACM),

vol. 58, no. 3, pp. 1–37, 2011.

L. Lian, A. Liu, and V. K. Lau, “Weighted lasso for sparse recovery with statistical prior support information,” IEEE

Transactions on Signal Processing, vol. 66, no. 6, pp. 1607–1618, 2018.

H. Mansour and R. Saab, “Recovery analysis for weighted l1-minimization using the null space property,” Applied

and Computational Harmonic Analysis, vol. 43, no. 1, pp. 23–38, 2017.

M. Elad, J.-L. Starck, P. Querre, and D. L. Donoho, “Simultaneous cartoon and texture image inpainting using
morphological component analysis (mca),” Applied and computational harmonic analysis, vol. 19, no. 3, pp. 340–
358, 2005.

A. Björck and G. H. Golub, “Numerical methods for computing angles between linear subspaces,” Mathematics of

computation, vol. 27, no. 123, pp. 579–594, 1973.

D. L. Donoho, M. Elad, and V. N. Temlyakov, “Stable recovery of sparse overcomplete representations in the presence

of noise,” IEEE Transactions on information theory, vol. 52, no. 1, pp. 6–18, 2005.

J. A. Tropp, “Greed is good: Algorithmic results for sparse approximation,” IEEE Transactions on Information

theory, vol. 50, no. 10, pp. 2231–2242, 2004.

Y. C. Eldar, P. Kuppinger, and H. Bolcskei, “Block-sparse signals: Uncertainty relations and eﬃcient recovery,” IEEE

Transactions on Signal Processing, vol. 58, no. 6, pp. 3042–3054, 2010.

R. Kueng and D. Gross, “Ripless compressed sensing from anisotropic measurements,” Linear Algebra and its Appli-

cations, vol. 441, pp. 110–123, 2014.

D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecological statistics,” in Proc. 8th Int’l Conf. Computer Vision,
vol. 2, July 2001, pp. 416–423.

R. Mehrotra, K. Namuduri, and N. Ranganathan, “Gabor ﬁlter-based edge detection,” Pattern Recognition, vol. 25,

no. 12, pp. 1479–94, 1992.

9

Appendix for “Towards improving discriminative reconstruction via
simultaneous dense and sparse coding”

A Uniqueness Proofs

In the dense and sparse coding problem, given matrices A ∈ Rm×p, B ∈ Rm×n, and a vector y ∈ Rm represented as
y = Ax + Bu, we study conditions under which there is a unique solution, i.e., a dense vector x∗ and an s-sparse
vector u∗, to the linear system. The ﬁrst uniqueness result assumes orthogonality of Col(A) and Col(B).

Theorem A.1 Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x∗, u∗). If A and
B have full column rank and Col(A) is orthogonal to Col(B), there is a unique solution to y = Ax + Bu.

Proof A.1 Let (x, u) be another solution pair.
= 0. Noting that the matrix
(cid:2)A B(cid:3) has full column rank, the homogeneous problem admits the trivial solution implying that x − x∗ = 0 and
u − u∗ = 0. Therefore, (x∗, u∗) is the only unique solution.

It follows that (cid:2)A B(cid:3)

(cid:21)

(cid:20)x − x∗
u − u∗

Extending Theorem A.1, we can show that as long as Col(A) and Col(B) span orthogonal subspaces, there exists
a unique solution. Indeed, AT · B = 0 and BT · A = 0, and thus we can divide our recovery problem into two
subproblems

AT y = AT Ax, BT y = BT Bu.
(8)
Note that the subproblems of (8) are well-deﬁned; AT A ∈ Rp×p has full rank, and so does BT B ∈ Rn×n. Thus the
system is invertible, leading to

x∗ = (AT A)−1AT y, u∗ = (BT B)−1BT y.

(9)

Remark: Interestingly, we can also prove the existence of a unique solution in the case where A and B do not have
full column rank, but still span orthogonal subspaces, under some extra conditions. Indeed, assume that A ∈ Rm×P
and B ∈ Rm×N , with rank(A) = p < P and rank(B) = n < N , respectively. Let A∗ ∈ Rm×p and B∗ ∈ Rm×n be
the minimum set of vectors that span the spaces of A and B, i.e. span(A∗) = span(A) and span(B∗) = span(B).
These matrices can be obtained through a slight modiﬁcation of the Gram-Schmidt process. Then (A∗)T B = 0 and
(B∗)T A = 0, and (8) becomes

(A∗)T y = (A∗)T Ax,

(B∗)T y = (B∗)T Bu,

(10)

where (A∗)T A ∈ Rp×P and (B∗)T B ∈ Rn×N . Neither of these matrices has full rank, as rank((A∗)T A) = p and
rank((B∗)T B) = n. However, note that (B∗)T y = (B∗)T Bu is the traditional sparse recovery problem, which has
a unique solution if (B∗)T B satisﬁes the RIP. Similarly, (A∗)T y = (A∗)T Ax is an underdetermined least squares
problem, which has a unique solution under the assumption that x ∈ kernel((A∗)T A)⊥.
While Theorem A.1 gives a simple condition, the condition that B is full column rank does not hold in the compressed
sensing setting. In particular, the classical setup of sparse recovery problem considers an overcomplete measurement
matrix B with n (cid:29) m. In addition, Theorem 1 restricts A to be a tall measurement matrix. The next corollary
provides a uniqueness result that relaxes these conditions.

Corollary A.2 Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x∗, u∗). Let S, the
subset of {1, 2, ..., n} with |S| = s, denote the support of u∗. Let BS ∈ Rm×s denote the restriction of B with column
indices in S. Assume that BS has full column rank and Col(A) is orthogonal to Col(BS). The only unique solution
to the linear system, with the condition that any feasible s-sparse vector u is supported on S and any feasible x is in
Ker(A)⊥, is (x∗, u∗).

Proof A.2 Let (x, u), with u supported on S and x∗ ∈ Ker(A)⊥, be another solution pair. It follows that Aδ1 +
S. Let U ∈ Rm×r and V ∈ Rm×q be matrices whose columns
BS(δ2)S = 0 where δ1 = x − x∗ and δ2 = uS − u∗
are the orthonormal bases of Col(A) and Col(BS) respectively. The equation Aδ1 + BS(δ2)S = 0 can be written as
(cid:80)r
i=1(cid:104)BS(δ2)S , Vi(cid:105)Vi = 0 with Ui and Vi denoting the i-th column of U and V respectively.
= 0. Since (cid:2)U V (cid:3) has full column rank, the homogeneous

i=1(cid:104)Aδ1 , Ui(cid:105)Ui + (cid:80)q
Compactly, we have (cid:2)U V (cid:3)

(cid:21)

(cid:20) {(cid:104)Aδ1 , Ui(cid:105)}r
i=1
{(cid:104)BS(δ2)S , Vi(cid:105)}q

i=1

problem admits the trivial solution implying that Aδ1 = 0 and BS(δ2)S = 0. Since BS has full column rank and
δ1 ∈ {Ker(A) ∩ Ker(A)⊥}, δ1 = δ2 = 0. Therefore, (x∗, u∗) is the unique solution.

B Proof of Main Result

Given measurements y = Ax∗ + Bu∗, consider the following convex optimization program for the dense and sparse
coding model.

min
x,u

||Ax||2

2 + ||u||1

s.t. y = Ax + Bu.

(11)

Our main result and proof is presented below.

Theorem B.1 Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x∗, u∗). Assume
that any feasible x is in Ker(A)⊥. Let ω ≥ 1 and deﬁne κs = max{cond(s, Σ), cond(s, Σ−1)}. Assume the two
conditions

||BT

S A|| ≤

1
32||x∗||2

,

||BT

S⊥

A||∞ ≤

1
32||x∗||∞

.

(12)

If the number of measurements fulﬁlls m ≥ Cκs ν ω2 s log n, then the solution of the convex program (11) is unique
and equal to (x∗, u∗) with probability at least 1 − e−ω.

10

Proof B.1 Consider a feasible solution pair (x∗ + δ1, u∗ + δ2). From the assumption, it follows that δ1 ∈ Ker(A)⊥.
For ease of notation, let the function f (x, u) deﬁne the objective in the optimization program. The idea of the proof
is to show that any feasible solution is not minimial in the objective value, f (x∗ + δ1, u∗ + δ2) > f (x, u), with the
inequality holding for all choices of δ1 and δ2. Before we proceed, two remarks are in order. First, using the duality
of the (cid:96)1 norm and the (cid:96)∞ norm, there exists a Λ ∈ S⊥ with ||Λ||∞ = 1 such that (cid:104)Λ , (δ2)S⊥
||1. Second,
the subgradient of the (cid:96)1 norm at u∗ is characterized as follows: ∂||u∗||1 = {sgn(u∗
||∞ ≤ 1}. It
follows that sgn(u∗) + Λ is a subgradient of the (cid:96)1 norm at u∗. Using the deﬁnition of the subgradient, the inequality
||u||1 ≥ ||u∗||1 + (cid:104)sgn(u∗

S) + Λ , u − u∗(cid:105) holds for any u. We can now lower bound f (x∗ + δ1, u∗ + δ2) as follows.

(cid:105) = ||(δ2)S⊥
S) + g | g ∈ S⊥, ||gS⊥

||A(x∗ + δ1)||2

2 + ||u∗ + δ2||1 ≥||Ax∗||2

2 + 2(cid:104)Ax∗ , Aδ1(cid:105) + ||u∗||1 + (cid:104)sgn(u∗

S) + Λ , δ2(cid:105)

=f (x∗, u∗) − 2(cid:104)Ax∗, Bδ2(cid:105) + (cid:104)sgn(u∗

S) + Λ , δ2(cid:105),

(13)

where the equality uses the feasibility condition that Aδ1 + Bδ2 = 0. We now introduce the dual certiﬁcate v. To
do so, reconsider the equation Aδ1 + Bδ2 = 0. To eliminate the component Aδ1, we project it onto the orthogonal
(B). We now assume that v ∈ Col(C T ).
complement of the range of A and obtain Cδ2 = 0 where C = PCol(A)⊥
With this, we continue with lower bounding f (x∗ + δ1, u∗ + δ2).

||A(x∗ + δ1)||2

2 + ||u∗ + δ2||1 ≥||Ax∗||2

2 − 2(cid:104)Ax∗, Bδ2(cid:105) + ||u∗||1 + (cid:104)sgn(u∗

S) + Λ − v, δ2(cid:105)

=f (x∗, u∗) + (cid:104)sgn(u∗

S) + Λ − v − 2BT Ax∗ , δ2(cid:105)

(14)

It remains to show that (cid:104)sgn(u∗
the fact that (cid:104)Λ , (δ2)S⊥

(cid:105) = ||(δ2)S⊥

||1, we obtain

S) + Λ − v − 2BT Ax∗ , δ2(cid:105) > 0. By considering projections onto S and S⊥ and using

S) + Λ − v − 2BT Ax∗ , δ2(cid:105)
S) − vS , (δ2)S(cid:105) − (cid:104)vS⊥
, (δ2)S⊥

(cid:105)
S) − vS||2||(δ2)S||2 − ||vS⊥
S A|| ||x∗||2 ||(δ2)S||2 − 2||BT
S⊥

, (δ2)S⊥

||(δ2)S||2 −

||1 + ||(δ2)S⊥

(cid:104)sgn(u∗
=(cid:104)sgn(u∗
−(cid:104)2[BT Ax∗]S⊥
≥ − || sgn(u∗
− 2||BT
1
4
5
16
10
16

||(δ2)S⊥

= −

≥ −

≥ −

||(δ2)S||2 +

1
4

||(δ2)S⊥
11
16

||(δ2)S⊥
11
16

||(δ2)S⊥

||1

||1 +

||1 =

1
16

||(δ2)S⊥

||1.

(cid:105) + ||(δ2)S⊥

||1 − (cid:104)2[BT Ax∗]S , (δ2)S(cid:105)

||1

||1 + ||(δ2)S⊥
||1

||∞||(δ2)S⊥
A||∞||x∗||∞||(δ2)S⊥
1
||(δ2)S||2 −
16

||1 −

1
16

||(δ2)S⊥

||1

(15)

(16)

(17)

Above, the inequality in (16) follows from the assumptions of the theorem and the conditions on the dual certiﬁcate

||vS − sgn(u∗

S)||2 ≤ 1

4 and ||vS⊥

||∞ ≤ 1
4 ,

(18)

and the last inequality follows from the assumption on the deviation of δ2 as ||(δ2)S||2 ≤ 2||(δ2)S⊥
(14) and the above bound with the ﬁnal result noted , we have

||2. Combining

f (x∗ + δ1, u∗ + δ2) ≥ f (x∗, u∗) +

1
16

||(δ2)S⊥

||1.

(19)

We note that f (x∗ + δ1, u∗ + δ2) = f (x, u) if and only if ||(δ2)S⊥
||2, the equality
||1 = 0 implies that ||(δ2)S||2 = 0. With this, f (x∗ + δ1, u∗ + δ2) = f (x, u) if and only if δ2 = 0. Using the
||(δ2)S⊥
relation Aδ1 + Bδ2 = 0, we have Aδ1 = 0. Since δ1 ∈ {Ker(A) ∩ Ker(A)⊥}, it folows that δ1 = 0. Therefore, the
solution (x∗, u∗) achieves the minimal value in the objective, and is a unique solution to (11).

||1 = 0. Since ||(δ2)S||2 ≤ 2||(δ2)S⊥

Two remarks are in order.

Remark 1: The existence of the dual certiﬁcate v satisfying the conditions (18) and the deviation inequality
||(δ2)S||2 ≤ 2||(δ2)S⊥
||2 follow from the anisotropic compressive sensing analysis once it is assumed that i. the covari-
ance matrix deﬁned obeys completeness, incoherence, and conditioning κs denoting its s-sparse condition number,
and ii. the sample complexity is as noted in Theorem B.1.

Remark 2: Our analysis depends on the matrix C = PCol(A)⊥
(B) satisfying certain conditions. We give three
instances of measurement models which ensure the assumed conditions on C i. A and B are random matrices with
i.i.d entries, the support set S of the underlying sparse vector is known and columns of the matrix [A BS] are linearly
independent ii. A is the identity matrix and B is a random matrix with i.i.d entries iii. A and B are random matrices
with i.i.d entries. Let T denote the indices from the set {1, 2, ..., n} such that the columns of the matrix [A BT ] are
linearly independent. We assume that the support of any feasible sparse solution is chosen from the set T .

C Noisy Compressive Sensing

Compressive sensing can be extended to the noisy case, which allows for the successful recovery of sparse signals under
the presence of noise, assuming an upper bound on the noise level. However, the performance of noisy compressive
sensing is signiﬁcantly inferior compared to the classical setting. Using our proposed model, and expanding on the
interpretation of the dense signal x as noise, we can compare dense and sparse model to noisy compressive sensing.
We devote the rest of the section to arguing that, by explicitly modeling the noise signal, we can outperform noisy
compressive sensing.

We ﬁx the number of columns in B to be n = 100, and generate the matrices A ∈ Rm×p and B ∈ Rm×n, as well as
||u∗||2
the vectors x∗ ∈ Rp and u∗ ∈ Rn, as in the main text. We deﬁne the signal-to-noise ratio as SNR = 20 log10
||x∗||2
SNR
and iterate over the range [−40dB, 40dB]. To vary the SNR, we normalize both vectors and scale u∗ by 10
20 . For
our proposed method, we solve the optimization problem introduced in the paper, whereas for noisy compressive
sensing we solve

,

ˆu = arg min

u

||u||1 ,

s.t.

||y − Bu||2 ≤ ||Ax∗||2 ,

(20)

11

(a)

(c)

(e)

(b)

(d)

(f)

Figure 4: Normalized recovery error of u as the SNR varies (lower is better).

and report the normalized error

|| ˆu−u∗||2
||u∗||2

for the two methods averaging 100 independent runs.

We present the SNR curves in Figure 4. When the sparsity ratio ρ is small, our approach is able to perfectly recover
u∗, by explicitly modeling the noise of the system, even when the measurement vector y is riddled with noise. On the
contrary, noisy compressive sensing is unable to correctly recover unless the SNR is above 25 dB. However, increasing
the sparsity level reduces the performance in both methods; the column ratio also directly aﬀects the performance of
the dense and sparse coding model. This corroborates the ﬁndings of our main text, as doing so introduces greater
overlap on the spans of A and B. In such a case, we report that increasing the number of measurements improves
performance. Note that noisy compressive sensing is unaﬀected by the relative size of A compared to the size of B.
This is expected, as in noisy compressive sensing x∗ is treated simply as bounded noise.

D Classiﬁcation and Image Denoising

D.1 DenSaE architecture

Figure 5 presents the DenSaE architecture. The encoder maps the input y into a dense xT and sparse uT repre-
sentation using two sets of ﬁlters of A and B through a recurrent network. A encodes the smooth part of the data
(low frequencies), and B encodes the details of the signal (high frequencies). The decoder reconstructs the data.
The dictionaries A and B are learned via backpropagation. We remark that b = αuλu. A larger value of b in the
proximal mapping Sb enforces higher sparsity on u, and a larger value of λx promotes smoothness on Ax. The
step sizes of the proximal algorithm, for one recurrent iteration of the encoder, are denoted as αx and αu. Having
a non-informative prior on Ax in DenSaE implies that λx → ∞. The parameters αx, αu, λu are tuned via grid search.

For comparison, Figure 6 presents the CSCNettied architecture for the sparse coding model. The encoder maps the
input y into a sparse uT representation using a set of ﬁlters B on a recurrent network, and the decoder reconstructs
the data. The dictionary B is learned by backpropagating through the network. Once again, we note that b = αuλu.
A larger the value of b in the proximal mapping S enforces higher sparsity on u. The parameter αu is the step size
of the proximal gradient algorithm (i.e., one recurrent iteration of the encoder). For CSCNettied
hyp , the parameter λu
is tuned, and for CSCNettied

LS , the bias b is learned.

12

−40−2002040SNR(dB)0.000.250.500.751.00Normalizederrorkˆu−u∗k2ku∗k2σ=0.5,ρ=0.05,p=0.1mOursCS−40−2002040SNR(dB)0.000.250.500.751.00Normalizederrorkˆu−u∗k2ku∗k2σ=0.5,ρ=0.05,p=0.5mOursCS−40−2002040SNR(dB)0.40.60.81.0Normalizederrorkˆu−u∗k2ku∗k2σ=0.5,ρ=0.5,p=0.1mOursCS−40−2002040SNR(dB)0.250.500.751.00Normalizederrorkˆu−u∗k2ku∗k2σ=0.5,ρ=0.5,p=0.5mOursCS−40−2002040SNR(dB)0.000.250.500.751.00Normalizederrorkˆu−u∗k2ku∗k2σ=0.95,ρ=0.5,p=0.1mOursCS−40−2002040SNR(dB)0.000.250.500.751.00Normalizederrorkˆu−u∗k2ku∗k2σ=0.95,ρ=0.5,p=0.5mOursCSEncoder

Repeat T times

Decoder

αxAT

αuBT

Sb

xt

ut

xT

uT

A

B

y

B
A 1 + 1
λx

z C

Smax

Classiﬁer

ˆy

ˆq

Figure 5: The DenSaE architecture. The vector z is normalized stacked features with a column of 1 (i.e.,
zT = [1,

]),

Sb is soft-thresholding, and

Smax is softmax.

[xT ,uT ]
[xT ,uT ]

(cid:107)

(cid:107)

Encoder

Repeat T times

Decoder

y

-

αuBT

Sb

B

ut

uT

B

z C

Smax

Classiﬁer

ˆy

ˆq

Figure 6: The CSCNettied architecture. The vector z is normalized features with a column of 1 (i.e.,
zT = [1,
Smax is softmax.
Table 4: Network parameters for MNIST classiﬁcation experiment.

Sb is soft-thresholding, and

[uT ]
[uT ]

]),

(cid:107)

(cid:107)

# dictionary atoms

Image size

# training examples

# validation examples

# testing examples

DenSaE CSCNettied

hyp CSCNettied
LS

400

28

28

×
50,000 MNIST

10,000 MNIST

10,000 MNIST

# trainable parameters in the autoencoder

313,600

313,600

314,000

# trainable parameters in the classiﬁer

4,010

4,010

4,010

(.)

S
Encoder layers T

αu

αx
λinit
u

ReLU

15

0.02

-

0.5

-

0.0

0.02

0.5

D.2 Classiﬁcation experiment

We warm the networks by training the autoencoder for 150 epochs using the ADAM optimizer where the weights
are initialized with the random Gaussian distribution. The learning rate is set to 10−3. We set (cid:15) of the optimizer
to be 10−15 and used batch size of 16. For disjoint classiﬁcation training, we trained for 1000 epochs, and for joint
classiﬁcation training, the network is trained for 500 epochs. All the networks use FISTA within their encoder for
faster sparse coding. Table 4 lists the parameters of the diﬀerent networks.

Table 5 demonstrates the detailed results on MNIST classiﬁcation experiment.
Figure 7 visualizes the reconstruction of MNIST test image for the disjoint training, where the autoencoder is trained
for pure reconstruction. The ﬁgure shows that SCNettied
LS has the best reconstruction among all, and the second best
is DenSaE200A,200B, having the highest number of A atoms.

Figures 8, 9, 10, and 11 visualize the reconstruction of MNIST test image for the joint training when β = 0.5, 0.75, 0.95, and 1,
respectively. Notably, when β = 1, the reconstructions from SCNettied
LS do not look like the original image. On the
other hand, DenSaE even with β = 1 (i.e., Figure 11) is able to reconstruct the image very well. In addition, the
ﬁgures show how Ax and Bu are contribution for reconstruction for DenSaE.

Figures 12 and 13 visualize the most important atoms for reconstruction and classiﬁcation for disjoint and join
training with β = 0.5 respectively.

13

Table 5: DenSaE’s performance on MNIST test dataset from both disjoint (D) and joint (Jβ) training.

SCNettied
LS

SCNettied
hyp

5A
395B

1.25

25A
375B

6.25

50A
350B

12.5

100A
300B

200A
200B

25

50

-

98.32

98.18

98.18

98.04

97.61

96.98

6.80

6.83

6.30

5.76

4.20

3.04

-

-

0

8

0

28

0

48

0

38

-

94.16

1.95

-

-

96.61

2.01

-

-

96.91

2.17

-

-

97.23

4.48

-

-

A
A+B model
Acc.

Rec.

A
A+B class
A
A+B rec.
Acc.

Rec.

A
A+B class
A
A+B rec.
Acc.

Rec.

A
A+B class
A
A+B rec.
Acc.

Rec.

A
A+B class
A
A+B rec.
Acc.

Rec.

A
A+B class
A
A+B rec.

D

J0.5

J0.75

J0.95

J1

y

97.97

97.07

97.68

0.87

0.58

0.58

-

-

0

6

6

44

98.18

98.19

98.23

1.24

0.75

1.11

-

-

8

8

8

36

98.51

98.23

98.44

1.03

1.22

1.32

-

-

18

6

70

14

96.06

71.20

98.59

47.70

98.61 98.56

32.61

30.20

-

-

-

-

ˆy

16

0

46

2

0

58

96.46

0.34

52

36

97.64

0.51

84

8

97.81

0.67

66

20

98.40

25.57

42

4
ˆy

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

(a) Original
Ax

(b) SCNettied
LS
Ax

(c) SCNettied
hyp
Ax

Bu

ˆy

Bu

ˆy

Bu

ˆy

(d) DenSaE 5A395B

(e) DenSaE 25A375B

(f) DenSaE 200A200B

Figure 7: Reconstruction of MNIST test images for disjoint classiﬁcation.

D.3 Denoising experiment

All the networks are trained for 250 epochs using the ADAM optimizer and the ﬁlters are initialized using the random
Gaussian distribution. The initial learning rate is set to 10−4 and then decayed by 0.8 every 50 epochs. We set (cid:15)
of the optimizer to be 10−3 for stability. At every iteration, a random patch of size 128 × 128 is cropped from the
training image and Gaussian noise is added to it with the corresponding noise level.

All the trained networks implement FISTA for faster sparse coding. Table 6 lists the parameters of the diﬀerent
networks. We note that compared to CSCNet, which has 63K trainable parameters, all the trained networks includ-
ing CSCNettied
LS have 20x fewer trainable parameters. We attribute the diﬀerence in performance, compared to the
results reported in the CSCNet paper, to this large diﬀerence in the number of trainable parameters and the usage
of a larger dataset.

Figure 15 shows the denoising performance of all the trained networks for τ = 50. Figures 16 and 17 visualize two
test images from BSD68 for various noise levels, along with their representations, using DenSaE4A,60B.

14

y

ˆy

ˆy

(a) Original
Ax

(b) SCNettied
LS
Ax

(c) SCNettied
hyp
Ax

Bu

ˆy

Bu

ˆy

Bu

ˆy

(d) DenSaE 5A395B

(e) DenSaE 25A375B

(f) DenSaE 200A200B

Figure 8: Reconstruction of MNIST test images for joint classiﬁcation when β = 0.5.

y

ˆy

ˆy

(a) Original
Ax

(b) SCNettied
LS
Ax

(c) SCNettied
hyp
Ax

Bu

ˆy

Bu

ˆy

Bu

ˆy

(d) DenSaE 5A395B

(e) DenSaE 25A375B

(f) DenSaE 200A200B

Figure 9: Reconstruction of MNIST test images for joint classiﬁcation when β = 0.75.

y

ˆy

ˆy

(a) Original
Ax

(b) SCNettied
LS
Ax

(c) SCNettied
hyp
Ax

Bu

ˆy

Bu

ˆy

Bu

ˆy

(d) DenSaE 5A395B

(e) DenSaE 25A375B

(f) DenSaE 200A200B

Figure 10: Reconstruction of MNIST test images for joint classiﬁcation when β = 0.95.

15

y

ˆy

ˆy

(a) Original
Ax

(b) SCNettied
LS
Ax

(c) SCNettied
hyp
Ax

Bu

ˆy

Bu

ˆy

Bu

ˆy

(d) DenSaE 5A395B

(e) DenSaE 25A375B

(f) DenSaE 200A200B

Figure 11: Reconstruction of MNIST test images for joint classiﬁcation when β = 1.

B rec.

B rec.

B class

B class

A rec.

B rec.

B class

A rec.

B rec.

A rec.

B rec.

B class

B class

(a) SCNettied
LS

(b) SCNettied
hyp

(c) DenSaE 5A395B

(d)
25A375B

DenSaE

(e)
200A200B

DenSaE

Figure 12: Most important atoms of the dictionary used for reconstruction (rec.) and classiﬁcation (class.)
for disjoint training.

B rec.

B rec.

B class

B class

A rec.

B rec.

B class

A rec.

B rec.

A class

B class

A rec.

B rec.

A class

B class

(a) SCNettied
LS

(b) SCNettied
hyp

(c) DenSaE 5A395B

(d)
25A375B

DenSaE

(e)
200A200B

DenSaE

Figure 13: Most important atoms of the dictionary used for reconstruction (rec.) and classiﬁcation (class.)
for joint training when β = 0.5.

16

Table 6: Network parameters for natural image denoising experiments.

DenSaE CSCNettied

hyp CSCNettied
LS

# ﬁlters

Filter size

Strides

Patch size

# training examples

# testing examples

64

7

7
×
5

128

128

×
432 BSD432

68 BSD68

# trainable parameters

3,136

3,136

3,200

(.)

S
Encoder layers T

αu

αx

τ = 15

τ = 25

τ = 50

τ = 75

λinit
u

Shrinkage

15

0.1

-

0.1

0.085

0.085

0.16

0.36

0.56

0.16

0.36

0.56

-

0.1

0.1

0.1

0.1

(a) τ = 15

(b) τ = 25

(c) τ = 50

(d) τ = 75

Figure 14: Histogram of biases from CSCNettied

LS for various noise levels.

17

-3-2.5-2-1.5-0.8Filterbiases(logscale)0246810Count-3-2.5-2-1.5-0.8Filterbiases(logscale)0246810Count-3-2.5-2-1.5-0.8Filterbiases(logscale)0246810Count-3-2.5-2-1.5-0.8Filterbiases(logscale)0246810CountNoisy

Noisy

Noisy

Noisy

Noisy

Noisy

Ax

Ax

Ax

Ax

Ax

Ax

Bu

Bu

Bu

Bu

Bu

Bu

Denoised

Denoised

Denoised

Denoised

Denoised

Denoised

A

B

A

B

A

B

A

B

A

B

A

Unused

B

(a) DenSaE 1A
63B

(b) DenSaE 4A
60B

(c) DenSaE 8A
56B

(d) DenSaE32A
32B

(e) CSCNettied
hyp

(f) CSCNettied
LS

Figure 15: Visualization of a test image and the learned representations of the diﬀerent networks (τ = 50).

18

Ax

Ax

Ax

Ax

Bu

Bu

Bu

Bu

Denoised

Denoised

Denoised

Denoised

Noisy

Noisy

Noisy

Noisy

Original

Original

Original

Original

(a) τ = 15

(b) τ = 25

(c) τ = 50

(d) τ = 75

Figure 16: Visualization of a test image from BDS68 for various noise levels along with the learned repre-
sentations of the DenSaE using 4A and 60B.

19

Ax

Ax

Ax

Ax

Bu

Bu

Bu

Bu

Denoised

Denoised

Denoised

Denoised

Noisy

Noisy

Noisy

Noisy

Original

Original

Original

Original

(a) τ = 15

(b) τ = 25

(c) τ = 50

(d) τ = 75

Figure 17: Visualization of a test image from BDS68 for various noise levels along with the learned repre-
sentations of the DenSaE using 4A and 60B.

20

