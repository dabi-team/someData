1
2
0
2

r
p
A
9
1

]

V
C
.
s
c
[

2
v
4
5
6
2
1
.
4
0
9
1
:
v
i
X
r
a

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

1

The Mutex Watershed and its Objective:
Efﬁcient, Parameter-Free Graph Partitioning

Steffen Wolf∗, Alberto Bailoni∗, Constantin Pape, Nasim Rahaman,
Anna Kreshuk, Ullrich Köthe, and Fred A. Hamprecht†

Abstract—Image partitioning, or segmentation without semantics, is the task of decomposing an image into distinct segments, or
equivalently to detect closed contours. Most prior work either requires seeds, one per segment; or a threshold; or formulates the task as
multicut / correlation clustering, an NP-hard problem. Here, we propose an efﬁcient algorithm for graph partitioning, the “Mutex
Watershed”. Unlike seeded watershed, the algorithm can accommodate not only attractive but also repulsive cues, allowing it to ﬁnd a
previously unspeciﬁed number of segments without the need for explicit seeds or a tunable threshold. We also prove that this simple
algorithm solves to global optimality an objective function that is intimately related to the multicut / correlation clustering integer linear
programming formulation. The algorithm is deterministic, very simple to implement, and has empirically linearithmic complexity. When
presented with short-range attractive and long-range repulsive cues from a deep neural network, the Mutex Watershed gives the best
results currently known for the competitive ISBI 2012 EM segmentation benchmark.

Index Terms—Image segmentation, partitioning algorithms, greedy algorithms, optimization, integer linear programming, machine
learning, convolutional neural networks.

(cid:70)

1 INTRODUCTION

M OST image partitioning algorithms are deﬁned over a graph

encoding purely attractive interactions. No matter whether
a segmentation or clustering is then found agglomeratively (as in
single linkage clustering / watershed) or divisively (as in spectral
clustering or iterated normalized cuts), the user either needs to
specify the desired number of segments or a termination criterion.
An even stronger form of supervision is in terms of seeds, where
one pixel of each segment needs to be designated either by a user
or automatically. Unfortunately, clustering with automated seed
selection remains a fragile and error-fraught process, because every
missed or hallucinated seed causes an under- or oversegmentation
error. Although the learning of good edge detectors boosts the
quality of classical seed selection strategies (such as ﬁnding local
minima of the boundary map, or thresholding boundary maps),
non-local effects of seed placement along with strong variability in
region sizes and shapes make it hard for any learned predictor to
place exactly one seed in every true region.

In contrast to the above class of algorithms, multicut / correla-
tion clustering partitions vertices with both attractive and repulsive
interactions encoded into the edges of a graph. Multicut has the
great advantage that a “natural” partitioning of a graph can be
found, without needing to specify a desired number of clusters, or
a termination criterion, or one seed per region. Its great drawback
is that its optimization is NP-hard.

The main insight of this paper is that when both attractive
and repulsive interactions between pixels are available, then a
generalization of the watershed algorithm can be devised that
segments an image without the need for seeds or stopping criteria
or thresholds. It examines all graph edges, attractive and repulsive,

∗ Authors contributed equally
† Corresponding author

• All authors are with HCI/IWR, Heidelberg University, Germany.

E-mail: <ﬁrstname>.<lastname>@iwr.uni-heidelberg.de
• A. Kreshuk and C. Pape are with EMBL, Heidelberg, Germany.

Fig. 1: Left: Overlay of raw data from the ISBI 2012 EM
segmentation challenge and the edges for which attractive (green)
or repulsive (red) interactions are estimated for each pixel using
a CNN. Middle: vertical / horizontal repulsive interactions at
intermediate / long range are shown in the top / bottom half. Right:
Active mutual exclusion (mutex) constraints that the proposed
algorithm invokes during the segmentation process.

sorted by their weight and adds these to an active set iff they
are not in conﬂict with previous, higher-priority, decisions. The
attractive subset of the resulting active set is a forest, with one tree
representing each segment. However, the active set can have loops
involving more than one repulsive edge. See Fig. 1 for a visual
abstract.

In summary, our principal contributions are, ﬁrst, a fast
deterministic algorithm for graph partitioning with both positive
and negative edge weights that does not need prior speciﬁcation of
the number of clusters (section 4); and second, its theoretical
characterization, including proof that it globally optimizes an
objective related to the multicut correlation clustering objective (4).
Combined with a deep net, the algorithm also happens to
deﬁne the state-of-the-art in a competitive neuron segmentation
challenge (section 5).

This is an extended version version of [1], with the second

principal contribution (section 4) being new.

 
 
 
 
 
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

2

2 RELATED WORK

In the original watershed algorithm [2], [3], seeds were automati-
cally placed at all local minima of the boundary map. Unfortunately,
this leads to severe over-segmentation. Deﬁning better seeds has
been a recurring theme of watershed research ever since. The
simplest solution is offered by the seeded watershed algorithm [4]:
It relies on an oracle (an external algorithm or a human) to provide
seeds and assigns each pixel to its nearest seed in terms of minimax
path distance.

In the absence of an oracle, many automatic methods for seed
selection have been proposed in the last decades with applications
in the ﬁelds of medicine and biology. Many of these approaches
rely on edge feature extraction and edge detection like gradient
calculation [5], [6]. Other types of methods generate seeds by ﬁrst
performing feature extraction [7], [8], whereas others ﬁrst extract
region of interests and then place seeds inside these regions by
using thresholding [9], binarization [10], k-means [11] or other
strategies [12], [13].

In applications where the number of regions is hard to estimate,
simple automatic seed selection methods, e.g. deﬁning seeds by
connected regions of low boundary probability, don’t work: The
segmentation quality is usually insufﬁcient because multiple seeds
are in the same region and/or seeds leak through the boundary.
Thus, in these cases seed selection may be biased towards over-seg-
mentation (with seeding at all minima being the extreme case). The
watershed algorithm then produces superpixels that are merged into
ﬁnal regions by more or less elaborate postprocessing. This works
better than using watersheds alone because it exploits the larger
context afforded by superpixel adjacency graphs. Many criteria
have been proposed to identify the regions to be preserved during
merging, e.g. region dynamics [14], the waterfall transform [15],
extinction values [16], region saliency [17], and (α, ω)-connected
components [18]. A merging process controlled by criteria like
these can be iterated to produce a hierarchy of segmentations
where important regions survive to the next level. Variants of such
hierarchical watersheds are reviewed and evaluated in [19].

These results highlight the close connection of watersheds to
hierarchical clustering and minimum spanning trees/forests [20],
[21], which inspired novel merging strategies and termination
criteria. For example, [22] simply terminated hierarchical merging
by ﬁxing the number of surviving regions beforehand. [23]
incorporate predeﬁned sets of generalized merge constraints into the
clustering algorithm. Graph-based segmentation according to [24]
deﬁnes a measure of quality for the current regions and stops when
the merge costs would exceed this measure. Ultrametric contour
maps [25] combine the gPb (global probability of boundary) edge
detector with an oriented watershed transform. Superpixels are
agglomerated until the ultrametric distance between the resulting
regions exceeds a learned threshold. An optimization perspective is
taken in [26], [27], which introduces h-increasing energy functions
and builds the hierarchy incrementally such that merge decisions
greedily minimize the energy. The authors prove that the optimal
cut corresponds to a different unique segmentation for every value
of a free regularization parameter.

An important line of research is given by partitioning of
graphs with both attractive and repulsive edges [28]. Solutions that
optimally balance attraction and repulsion do not require external
stopping criteria such as predeﬁned number of regions or seeds.
This generalization leads to the NP-hard problem of correlation
clustering or (synonymous) multicut (MC) partitioning. Fortunately,

modern integer linear programming solvers in combination with
incremental constraint generation can solve problem instances of
considerable size [29], and good approximations exist for even
larger problems [30], [31] Reminiscent of strict minimizers [32]
with minimal L∞-norm solution, our work solves the multicut
objective optimally when all graph weights are raised to a large
power.

Related to the proposed method, the greedy additive edge
contraction (GAEC) [33] heuristic for the multicut also sequentially
merges regions, but we handle attractive and repulsive interactions
separately and deﬁne edge strength between clusters by a maximum
instead of an additive rule. The greedy ﬁxation algorithm introduced
in [34] is closely related to the proposed method; it sorts attractive
and repulsive edges by their absolute weight, merges nodes
connected by attractive edges and introduces no-merge constraints
for repulsive edges. However, similar to GAEC, it deﬁnes edge
strength by an additive rule, which increases the algorithm’s
runtime complexity compared to the presented Mutex Watershed.
Also, it is not yet known what objective the algorithm optimizes
globally, if any.

Another beneﬁcial extension is the introduction of additional
long-range edges. The strength of such edges can often be estimated
with greater certainty than is achievable for the local edges used
by watersheds on standard 4- or 8-connected pixel graphs. Such
repulsive long-range edges have been used in [35] to represent
object diameter constraints, which is still an MC-type problem.
When long-range edges are also allowed to be attractive, the
problem turns into the more complicated lifted multicut (LMC)
[36]. Realistic problem sizes can only be solved approximately [33],
[37], but watershed superpixels followed by LMC postprocessing
achieve state-of-the-art results on important benchmarks [38]. Long-
range edges are also used in [39], as side losses for the boundary
detection convolutional neural network (CNN); but they are not
used explicitly in any downstream inference.

In general, striking progress in watershed-based segmentation
has been achieved by learning boundary maps with CNNs. This
is nicely illustrated by the evolution of neurosegmentation for
connectomics, an important ﬁeld we also address in the experi-
mental section. CNNs were introduced to this application in [40]
and became, in much reﬁned form [41], the winning entry of
the ISBI 2012 Neuro-Segmentation Challenge [42]. Boundary
maps and superpixels were further improved by progress in
CNN architectures and data augmentation methods, using U-
Nets [43], FusionNets [44] or inception modules [38]. Subsequent
postprocessing with the GALA algorithm [45], [46], conditional
random ﬁelds [47] or the lifted multicut [38] pushed the envelope
of ﬁnal segmentation quality. MaskExtend [48] applied CNNs to
both boundary map prediction and superpixel merging, while ﬂood-
ﬁlling networks [49] eliminated superpixels altogether by training
a recurrent neural network to perform region growing one region
at a time.

Most networks mentioned so far learn boundary maps on pixels,
but learning works equally well for edge-based watersheds, as was
demonstrated in [50], [51] using edge weights generated with a
CNN [52], [53]. Tayloring the learning objective to the needs
of the watershed algorithm by penalizing critical edges along
minimax paths [53] or end-to-end training of edge weights and
region growing [54] improved results yet again.

Outside of connectomics, [55] obtained superior boundary
maps from CNNs by learning not just boundary strength, but also
its gradient direction. Holistically-nested edge detection [56], [57]

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

3

couples the CNN loss at multiple resolutions using deep supervision
and is successfully used as a basis for watershed segmentation of
medical images in [58].

We adopt important ideas from this prior work (hierarchical
single-linkage clustering, attractive and repulsive interactions, long-
range edges, and CNN-based learning). The proposed efﬁcient
segmentation framework can be interpreted as a generalization of
[23], because we also allow for soft repulsive interactions (which
can be overridden by strong attractive edges), and constraints are
generated on-the-ﬂy.

3 THE MUTEX WATERSHED ALGORITHM AS AN
EXTENSION OF SEEDED WATERSHED

In this section we introduce the Mutex Watershed Algorithm, an
efﬁcient graph clustering algorithm that can ingest both attractive
and repulsive cues. We ﬁrst reformulate seeded watershed as a
graph partitioning with inﬁnitely repulsive edges and then derive the
generalized algorithm for ﬁnitely repulsive edges, which obviates
the need for seeds.

3.1 Deﬁnitions and notation

Let G = (V, E, w) be a weighted graph. The scalar attribute w :
E → R associated with each edge is a merge afﬁnity: the higher
this number, the higher the inclination of the two incident vertices
to be assigned to the same cluster. Conversely, large negative
afﬁnity indicates a greater desire of the incident vertices to be in
different clusters. In our application, each vertex corresponds to
one pixel in the image to be segmented. We call an edge e ∈ E
repulsive if we < 0 and we call it attractive if we > 0 and collect
them in E− = {e ∈ E | we < 0} and E+ = {e ∈ E | we > 0}
respectively.

In our application, each vertex corresponds to one pixel in the
image to be segmented. The Mutex Watershed algorithm, deﬁned
in subsection 3.3, maintains disjunct active sets A+ ⊆ E+, A− ⊆
E−, A+ ∩ A− = ∅ that encode merges and mutual exclusion
constraints, respectively. Clusters are deﬁned via the “connected”
predicate:

∀i, j ∈ V :

Πi→j = {paths π from i to j with π ⊆ E+}

connected(i, j; A+) ⇔ ∃ path π ∈ Πi→j with π ⊆ A+
cluster(i; A+) = {i} ∪ {j : connected(i, j; A+)}

Conversely, the active subset A− ⊆ E− of repulsive edges deﬁnes
mutual exclusion relations by using the following predicate:

mutex(i, j; A+, A−) ⇔ ∃ e = (k, l) ∈ A− with
k ∈ cluster(i; A+) and
l ∈ cluster(j; A+) and
cluster(i; A+) (cid:54)= cluster(j; A+)

Admissible active edge sets A+ and A− must be chosen such
that the resulting clustering is consistent, i.e. nodes engaged
in a mutual exclusion constraint cannot be in the same cluster:
mutex(i, j; A+, A−) ⇒ notconnected(i, j; A+). The “connected”
and “mutex” predicates can be efﬁciently evaluated using a union
ﬁnd data structure.

Fig. 2: Two equivalent representations of the seeded watershed
clustering obtained using (a) a maximum spanning tree com-
putation or (b) Algorithm 1. Both graphs share the weighted
attractive (green) edges and seeds (hatched nodes). The inﬁnitely
attractive connections to the auxiliary node (gray) in (a) are replaced
by inﬁnitely repulsive (red) edges between each pair of seeds in
(b). The two ﬁnal clusterings are deﬁned by the active sets (bold
edges) and are identical. Node colors indicate the clustering result,
but are arbitrary.

3.2 Seeded watershed from a mutex perspective

One interpretation of the proposed method is in terms of a
generalization of the edge-based watershed algorithm [20], [59],
[60] or image foresting transform [61]. This algorithm can only
ingest a graph with purely attractive interactions, E− = ∅. Without
further constraints, the algorithm would yield only the trivial
result of a single cluster comprising all vertices. To obtain more
interesting output, an oracle needs to provide seeds (e.g. one node
per cluster). These seed vertices are all connected to an auxiliary
node (see Fig. 2 (a)) by auxiliary edges with inﬁnite merge afﬁnity.
A maximum spanning tree (MST) on this augmented graph can
be found in linearithmic time; and the maximum spanning tree (or
in the case of degeneracy: at least one of the maximum spanning
trees) will include the auxiliary edges. When the auxiliary edges are
deleted from the MST, a forest results, with each tree representing
one cluster [20], [59], [61].

We now reformulate this well-known algorithm in a way that
will later emerge as a special case of the proposed Mutex Watershed:
we eliminate the auxiliary node and edges, and replace them by
a set of inﬁnitely repulsive edges, one for each pair of seeds

Seeded Watershed:
WS(cid:0)G(V, E), pos. weights w : E → R+, seeds
S ⊆ V (cid:1):

A+ ← ∅;
A− ← {(s, t) ∈ S × S | s (cid:54)= t};

(cid:46) Equivalent to introducing inﬁnitely

repulsive edges between seeds

for (i, j) = e ∈ E in descending order of we do

if not connected(i, j; A+) and not
mutex(i, j; A+, A−) then

A+ ← A+ ∪ e ;

(cid:46) merge i and j and inherit the mutex
constraints of the parent clusters

end

end
return A+ ∪ A−

Algorithm 1: Mutex version of seeded watershed algorithm.
The output clustering is deﬁned by the connected components
of the ﬁnal attractive active set A+.

-∞141512161720814189515761617201214189515761617208(a)(b)-∞∞∞∞-∞IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

4

Mutex Watershed:
MWS(cid:0)G(V, E), w : E → R, boolean connect_all(cid:1):

A+ ← ∅; A− ← ∅;
for (i, j) = e ∈ E in descending order of |we| do

if e ∈ E+ then

if not mutex(i, j; A+, A−) then

if not connected(i, j; A+) or connect_all
then

merge(i, j): A+ ← A+ ∪ e;
(cid:46) merge i and j and inherit the mutex
constraints of the parent clusters

end

end

else

if not connected(i, j; A+) then

addmutex(i, j): A− ← A− ∪ e;

(cid:46) add mutex constraint between i and j

end

end

end
return A+ ∪ A−

Algorithm 2: Mutex Watershed Algorithm. The output clus-
tering is deﬁned by the connected components of the ﬁnal
attractive active set A+. The connect_all parameter changes the
internal cluster connectedness from trees to fully connected, but
does not change the output clustering. The connected predicate
can be efﬁciently evaluated using union ﬁnd data structures.

(Fig. 2 (b)). Algorithm 1 is a variation of Kruskal’s MST algorithm
operating on the seed mutex graph just deﬁned, and gives results
identical to seeded watershed on the original graph.

This algorithm differs from Kruskal’s only by the check for
mutual exclusion in the if-statement. Obviously, the modiﬁed
algorithm has the same effect as the original algorithm, because
the ﬁnal set A+ is exactly the maximum spanning forest obtained
after removing the auxiliary edges from the original solution.

In the sequel, we generalize this construction by admitting
less-than-inﬁnitely repulsive edges. Importantly, these can be dense
and are hence much easier to estimate automatically than seeds
with their strict requirement of only-one-per-cluster.

3.3 Mutex Watersheds

We now introduce our core contribution: an algorithm that is
empirically no more expensive than a MST computation; but
that can ingest both attractive and repulsive cues and partition
a graph into a number of clusters that does not need to be speciﬁed
beforehand. Neither seeds nor hyperparameters that implicitly
determine the number of resulting clusters are required.

The Mutex Watershed, Algorithm 2, proceeds as follows. Given
a graph G = (V, E) with signed weights w : E → R, do the
following: sort all edges E, attractive or repulsive, by their absolute
weight in descending order into a priority queue. Iteratively pop all
edges from the queue and add them to the active set one by one,
provided that a set of conditions are satisﬁed. More speciﬁcally,
assuming connect_all is False, if the next edge popped from the
priority queue is attractive and its incident vertices are not yet in
the same tree, then connect the respective trees provided this is not
ruled out by a mutual exclusion constraint. If on the other hand the
edge popped is repulsive, and if its incident vertices are not yet

Fig. 3: Some iterations of the Mutex Watershed Algorithm 2 applied
to a graph with weighted attractive (green) and repulsive (red) edges.
Edges accumulated in the active set A after a given number of
iterations are shown in bold. The connect_all parameter of the
algorithm is set to False, so that only the positive edges belonging
to the maximum spanning tree of each cluster are added to the
active set. Once the algorithm terminates, the ﬁnal active set (f)
deﬁnes the ﬁnal clustering (indicated using arbitrary node colors).
Some edges are not added to the active set because they are mutex
constrained (yellow highlight) or because the associated nodes are
already connected and in the same cluster (blue highlight).

in the same tree, then add a mutual exclusion constraint between
the two trees. The output clustering is deﬁned by the connected
components of the ﬁnal attractive active set A+.

The crucial difference to Algorithm 1 is that mutex constraints
are no longer pre-deﬁned, but created dynamically whenever a
repulsive edge is found. However, new exclusion constraints can
never override earlier, high-priority merge decisions. In this case,
the repulsive edge in question is simply ignored. Similarly, an
attractive edge must never override earlier and thus higher-priority
must-not-link decisions.

The boolean value of the connect_all input parameter of the
algorithm does not inﬂuence the ﬁnal output clustering, but deﬁnes
the internal cluster connectedness: when it is set to True, the
algorithm adds all attractive intra-cluster edges to the active set
A+. When it is set to False, then a maximum spanning tree is built
for each cluster similarly to the seeded watershed. This variant of
the algorithm will be helpful in the next section 4 to highlight the
relation between the Mutex Watershed and the multicut problem.
Fig. 3 illustrates the proposed algorithm: Fig. 3a and Fig. 3b
show examples of an unconstrained merge and an added mutex
constraint, respectively; Fig. 3c and Fig. 3d show, respectively,

141895157-10-13-11-2-19-161216172088888141895157-10-13-11-2-19-161216172014141895157-10-13-11-2-19-1612161720141895157-10-13-13-11-2-19-1612161720141895157-10-13-11-2-19-161212161720141895157-10-13-11-2-19-161216172088(a)Iteration1(b)Iteration2(c)Iteration7(d)Iteration8(e)Iteration9(f)FinalactivesetIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

5

an example of an attractive edge (we = 14) and repulsive edge
(we = −13) that are not added to the active set because their
incident vertices are already “connected” and belong to the same
tree of the forest A+; ﬁnally, Fig. 3e shows an attractive edge
(we = 12) that is ruled out by a previously introduced mutual
exclusion relation.

3.4 Time Complexity Analysis

Before analyzing the time complexity of algorithm 2 we ﬁrst
review the complexity of Kruskal’s algorithm. Using a union-ﬁnd
data structure (with path compression and union by rank) the
time complexity of merge(i, j) and connected(i, j) is O(α(V )),
where α is the slowly growing inverse Ackerman function, and the
total runtime complexity is dominated by the initial sorting of the
edges O(E log E) [62].
To check for mutex constraints efﬁciently, we maintain a set of all
active mutex edges

M [Ci] = {(u, v) ∈ A−|u ∈ Ci ∨ v ∈ Ci}

for every Ci = cluster(i) using hash tables, where insertion of
new mutex edges (i.e. addmutex) and search have an average
complexity of O(1). Note that every cluster can be efﬁciently
identiﬁed by its union-ﬁnd root node. For mutex(i, j) we check
if M [Ci] ∩ M [Cj] = ∅ by searching for all elements of the
smaller hash table in the larger hash table. Therefore mutex(i, j)
has an average complexity of O(min(|M [Ci]|, |M [Cj]|). Sim-
ilarly, during merge(i, j), mutex constraints are inherited by
merging two hash tables, which also has an average complexity
O(min(|M [Ci]|, |M [Cj]|).
In conclusion, the average runtime contribution of attractive
edges O(max(|E+| · α(V ), |E+| · M )) (checking mutex con-
straints and possibly merging) and repulsive edges O(max(|E−| ·
α(V ), |E−|)) (insertion of one mutex edge) result in a total
average runtime complexity of algorithm 2:

O(max(E log E , EM )).

(1)

where M is the expected value of min(|M [Ci]|, |M [Cj]|) and
α(V ) ∈ O(log V ) ∈ O(log E) 1.
In the worst case O(M ) ∈ O(E), the Mutex Watershed Algorithm
has a runtime complexity of O(E2). Empirically, we ﬁnd that
O(EM ) ≈ O(E log E) by measuring the runtime of Mutex
Watershed for different sub-volumes of the ISBI challenge (see
Figure 4), leading to a

Empirical Mutex Watershed Complexity: O(E log E)

(2)

4 THEORETICAL CHARACTERIZATION
Towards the Multicut framework. In section 3.3, we have intro-
duced the Mutex Watershed (MWS) algorithm as a generalization
of seeded watersheds and the Kruskal algorithm in particular.
However, since we are considering graphs with negative edge
weights, the MWS is conceptually closer to the multicut problem
and related heuristics such as GAEC and GF [34]. Fortunately, due
to the structure of the MWS it can be analyzed using dynamic
programming. This section summarizes our second contribution, i.e.
the proof that the Mutex Watershed Algorithm globally optimizes
a precise objective related to the multicut.

1. In the worst case G is a fully connected graph, with |E| = |V |2, hence

log |V | = 1

2 log |E|.

1e-7

|

E

|

s
e
g
d
E

f

o
r
e
b
m
u
N

l

a

t

o
T

/

]
s
[
e
m

i
t

n
u
R

3.5

3.0

2.5

2.0

1.5

1.0

0.5

106

Total Number of Edges |E|

107

Fig. 4: Runtime T of Mutex Watershed (without sorting of edges)
measured on sub-volumes of the ISBI challenge of different
sizes (thereby varying the total number of edges E). We plot
T
|E| over |E| in a logarithmic plot, which makes T ∼ |E|log(|E|)
appear as straight line. A logarithmic function (blue line) is ﬁtted
to the measured T
|E| (blue circles) with (R2 = 0.9896). The good
ﬁt suggests that empirically T ≈ O(E log E).

4.1 Review of the Multicut problem and its objective

In the following, we will review the multicut problem not in
its standard formulation but in the Cycle Covering Formulation
introduced in [63], which is similar to the MWS formulation as it
also considers the set of attractive and repulsive edges separately.
Previously, in Sec. 3.1, we deﬁned a clustering by introducing
the concept of an active set of edges A = A+ ∪ A− ⊆ E and the
connected/mutex predicates. In particular, an active set describes a
valid clustering if it does not include both a path of only attractive
edges and a path with exactly one repulsive edge connecting any
two nodes i, j ∈ V :

connected(i, j; A+) =⇒ not mutex(i, j; A+, A−).

(3)

In other words, an active set is consistent and describes a clustering
if it does not contain any cycle with exactly one repulsive edge
(known as conﬂicted cycles).

Deﬁnition 4.1. Conﬂicted cycles – We call a cycle of G conﬂicted
w.r.t. (G, w) if it contains precisely one repulsive edge e ∈ E−,
s.t. we < 0. We denote by C−(G, w) ⊆ C(G, w) the set of all
conﬂicted cycles. Furthermore, given a set of edges A ⊆ E, we
denote by C−(A, G, w) ⊆ C−(G, w) the set of conﬂicted cycles
involving only edges in A.

From now on, in order to describe different clustering solutions
in the framework of (integer) linear programs, we associate each
active set A with the following edge indicator xA

xA := 1{e /∈ A)} ∈ {0, 1}|E|.

(4)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

6

Fig. 5: Consistent and inconsistent active sets – Two different active edge sets A1 ⊆ E (on the left) and A2 ⊆ E (on the right) on
identical toy graphs with six nodes, attractive (green) and repulsive (red) edges. The value of the edge indicator xA ∈ {0, 1}|E| deﬁned
in Eq. 4 is shown for every edge. Members of the active sets are shown as solid lines. On the left, the active set A1 is consistent, i.e.
does not include any conﬂicted cycle C−(G, w) (see Def. 4.1): Therefore, it is associated with a clustering (represented by arbitrary node
colors). On the right, the active set A2 is not consistent and includes at least one conﬂicted cycle (highlighted in yellow), thus it cannot
be associated with a node clustering.

In this way, the cycle-free property C−(A, G, w) = ∅ of an active
set can be reformulated in terms of linear inequalities:

∀C ∈ C−(G, w) :

(cid:88)

e ≥ 1 ⇐⇒ C−(A, G, w) = ∅.
xA

e∈EC

(5)
In words, the active set cannot contain conﬂicted cycles; or vice
versa, every conﬂicted cycle must contain at least one edge that
is not part of the active set. Following [63], via this property we
describe the space of all possible clustering solutions by deﬁning
the convex hull SC(G, w) of all edge indicators corresponding to
valid clusterings of (G, w):

Deﬁnition 4.2. Let SC(G, w) denote the convex hull of all
edge indicators x ∈ {0, 1}|E| satisfying the following system of
inequalities:

∀C ∈ C−(G, w) :

(cid:88)

e∈EC

xe ≥ 1.

(6)

That is, SC(G, w) contains all edge labelings for which every
conﬂicted cycle is broken at least once. We call SC(G, w) the set
covering polyhedron with respect to conﬂicted cycles, similarly to
[63].

Fig. 5 summarizes these deﬁnitions and provides an example
of consistent and inconsistent active sets with their associated
clusterings and edge indicators.

As shown in [63], the multicut optimization problem can be
formulated with constraints over conﬂicted cycles in terms of the
following integer linear program (ILP), which is NP-hard:

exists a cut B with f ∈ EB such that |wf | ≥ (cid:80)
e∈EB \{f } |we|.
These highlight an aspect of the multicut problem that can be used
to search for optimal solutions more efﬁciently. Not all weighted
graphs contain dominant edges; but if, assuming no ties, we raise all
graph weights to a large enough power a similar property emerges.

Deﬁnition 4.3. Dominant power: Let G = (V, E, w) be an edge-
weighted graph, with unique weights w : E → R. We call p ∈ N+
a dominant power if:

|we|p >

(cid:88)

|wt|p

∀e ∈ E,

(8)

t∈E, wt<we

In contrast to dominant edges [63], we do not consider edges
on a cut but rather all edges with smaller absolute weight. Note
that there exists a dominant power for any ﬁnite set of edges,
since for any e ∈ E we can divide (8) by |we|p and observe that
the normalized weights |wt|p/|we|p (and any ﬁnite sum of these
weights) converges to 0 when p tends to inﬁnity.

By considering the multicut problem in Eq. (7) and raising the
weights |we| to a dominant power p, we fundamentally change the
problem structure:

Deﬁnition 4.4. Mutex Watershed Objective: Let G = (V, E, w)
be an edge-weighted graph, with unique weights w : E → R and
p ∈ N+ a dominant power. Then the Mutex Watershed Objective is
deﬁned as the integer linear program

min
x∈SC(G,w)

|we|p xe

(cid:88)

e∈E

(9)

min
x∈SC(G,w)

(cid:88)

e∈E

|we|xe.

(7)

where SC(G, w) is the convex hull deﬁned in Def. 4.2.

The solution of the multicut problem is given by the clustering
associated to the connected components of the active set ˆA+ =
{e ∈ E+|ˆxe = 0}, where ˆx ∈ {0, 1}|E| is the solution of (7).

4.2 Mutex Watershed Objective

We now deﬁne the Mutex Watershed objective that is minimized by
the Mutex Watershed Algorithm (proof in subsection 4.3) and show
how it is closely related to the multicut problem deﬁned in Eq. (7).
Lange et al. [63] introduce the concept of dominant edges in a graph.
For example, an attractive edge f ∈ E+ is called dominant if there

In the following section, we will prove that this modiﬁed
version of the multicut objective, which we call Mutex Watershed
Objective, is indeed optimized by the Mutex Watershed Algorithm:

Theorem 4.1. Let G = (V, E, w) be an edge-weighted graph,
with unique weights w : E → R and p ∈ N+ a dominant power.
Then the edge indicator given by the Mutex Watershed Algorithm 2

xMWS := 1(cid:110)

(cid:16)

e /∈ MWS

G, w, connect_all=True

(cid:17)(cid:111)

minimizes the Mutex Watershed Objective in Eq. (9).

A2A2+001100001010011222220A1A1+10111IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

7

4.3 Proof of optimality via dynamic programming

In this section we prove Theorem 4.1, i.e. that the Mutex Watershed
Objective deﬁned in 4.4 is solved to optimality by the Mutex
Watershed Algorithm 3. Particularly, in the following Sec. 4.3.1 we
show that the edge indicator associated to the solution of the MWS
algorithm lies in SC(G, w), whereas in Sec. 4.3.2 we prove that it
solves Eq. 9 to optimality.

4.3.1 Cycle consistency

The Mutex Watershed algorithm introduced in Sec. 3 iteratively
builds an active set A = A+ ∪ A− such that nodes engaged in a
mutual exclusion constraint (encoded by edges in A−) are never
part of the same cluster. In other words, this means that the active
set built by the Mutex Watershed at every iteration does never
include a conﬂicted cycle and is always consistent. In particular,
for any attractive edge (i, j) = e+ ∈ E+ and any consistent set
A that fulﬁlls C−(A, G, w) = ∅:

not mutex(i, j, A+, A−) ⇔ C−(A ∪ {e+}, G, w) = ∅

Similarly, for any repulsive edge (s, t) = e− ∈ E−:

not connected(s, t, A+) ⇔ C−(A ∪ {e−}, G, w) = ∅

Therefore, we can rewrite Algorithm 2 in the form of Algorithm 3.
This new formulation makes it clear that

C−(cid:16)

MWS(cid:0)G, w, connect_all=True(cid:1)(cid:17)

= ∅.

(10)

Thus, thanks to Eq. 5 and deﬁnition 4.2, it follows that the MWS
edge indicator xMWS deﬁned in 4.1 lies in SC(G, w):

xMWS ∈ SC(G, w).

(11)

4.3.2 Optimality

We ﬁrst note that the Mutex Watershed Objective 4.4 and The-
orem 4.1 can easily be reformulated in terms of active sets to
minimize

arg min
A⊆E

(cid:88)

−

|we|p

e∈A

s.t. C−(A, G, w) = ∅.

(12)

We now generalize the Mutex Watershed (see Algorithm 4) and the
objective such that an initial consistent set of active edges ˜A ⊆ E
is supplied:

Conﬂicted-Cycles Mutex Watershed:
CCMWS(cid:0)G(V, E), w : E → R(cid:1):

A ← ∅;
for (i, j) = e ∈ E in descending order of |we| do

if C−(A ∪ {e}, G, w) = ∅ then

A ← A ∪ e;

end

end
return A;

Algorithm 3: Equivalent formulation of the Mutex Watershed
Algorithm 2, with input parameter connect_all=True. The set
of conﬂicted cycles C−(A, G, w) is deﬁned in Def. 4.1. The
output clustering is deﬁned by the connected components of
the ﬁnal attractive active set A+ = A ∩ E+.

Initialized Mutex Watershed:
IMWS(cid:0)G(V, E), w : E → R, initial active set ˜A(cid:1):

A ← ∅;
for e ∈ E \ ˜A in descending order of weight do

if C−(A ∪ ˜A ∪ {e}, G, w) = ∅ then

A ← A ∪ e;

end

end
return A;

Algorithm 4: Mutex Watershed algorithm starting from initial
active set ˜A. An initial set ˜A of active edges is given as
additional input and the ﬁnal active set is such that A ⊆ E \ ˜A.
Note that Algorithm 3 is a special case of this algorithm when
˜A = ∅. Differences with Algorithm 3 are highlighted in blue.

4.5. Energy

Let
Deﬁnition
G = (V, E, w) be an edge-weighted graph. Deﬁne the optimal
solution of the subproblem as

optimization

subproblem.

S(G, ˜A) := argmin
A⊆(E\ ˜A)

T (A)

with T (A) := −

s.t. C−(A ∪ ˜A, G, w) = ∅,

|we|p,

(cid:88)

e∈A

(13)

(14)

where ˜A ⊆ E is a set of initially activated edges such that
C−( ˜A, G, w) = ∅.
We note that for ˜A = ∅, the optimal solution S(G, ∅) is equivalent
to the solution minimizing the Mutex Watershed Objective and Eq.
(12).

Deﬁnition 4.6. Incomplete, consistent initial set: For an edge-
weighted graph G = (V, E, w) a set of edges ˜A ⊆ E is consistent
if

C−( ˜A, G, w) = ∅.
(15)
˜A is incomplete if it is not the ﬁnal solution and there exists a
consistent edge ˜e that can be added to ˜A without violating the
constraints.

∃ ˜e ∈ E \ ˜A s.t. C−( ˜A ∪ {˜e}, G, w) = ∅

(16)

Deﬁnition 4.7. First greedy step: Let us consider an incomplete,
consistent initial active set ˜A ⊆ E on G = (V, E, w). We deﬁne

g := argmax
e∈(E\ ˜A)

|w(e)|

s.t.

C−( ˜A ∪ {e}, G, w) = ∅.

(17)

as the feasible edge with the highest weight, which is always the
ﬁrst greedy step of Algorithm 4.

In the following two lemmas, we prove that the Mutex Watershed
problem has an optimal substructure property and a greedy
choice property [62], which are sufﬁcient to prove that the Mutex
Watershed algorithm ﬁnds the optimum of the Mutex Watershed
Objective.

Lemma 4.2. Greedy-choice property. For an incomplete, consis-
tent initial active set ˜A of the Mutex Watershed, the ﬁrst greedy
step g is always part of the optimal solution

g ∈ S(G, ˜A).

Proof. We will prove the theorem by contradiction by assuming
that the ﬁrst greedy choice is not part of the optimal solution,

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
i.e. g /∈ S(G, ˜A). Since g is by deﬁnition the feasible edge with
highest weight, it follows that:

|w(e)| < |w(g)| ∀e ∈ S(G, ˜A).

(18)

We now consider the alternative active set A(cid:48) = {g}, that is a
consistent solution, with

T (A(cid:48)) = −|wg|p

(8)
< −

(cid:88)

|wt|p = T

(cid:16)

(cid:17)
S(G, ˜A)

(19)

t∈S(G, ˜A)

which contradicts the optimality of S(G, ˜A).

Lemma 4.3. Optimal substructure property. Let us consider an
initial active set ˜A, the optimization problem deﬁned in Equation
13, and assume to have an incomplete, consistent problem (see Def.
4.6). Then it follows that:

1) After making the ﬁrst greedy choice g, we are left with a
subproblem that can be seen as a new optimization problem
of the same structure;

2) The optimal solution S(G, ˜A) is always given by the combi-
nation of the ﬁrst greedy choice and the optimal solution of
the remaining subproblem.

Proof. After making the ﬁrst greedy choice and selecting the ﬁrst
feasible edge g deﬁned in Equation 17, we are clearly left with
a new optimization problem of the same structure that has the
following optimal solution: S(G, ˜A ∪ {g}).
In order to prove the second point of the theorem, we now show
that:

S(G, ˜A) = {g} ∪ S(G, ˜A ∪ {g}).
(20)
Since algorithm 4 fulﬁlls the greedy-choice property, g ∈ S(G, ˜A)
and we can add the edge g as an additional constraint to the optimal
solution:

S(G, ˜A) = argmin
A⊆(E\ ˜A)

T (A)

s. t. C−(A ∪ ˜A, G, w) = ∅;

g ∈ A

Then it follows that:

S(G, ˜A) = {g} ∪

T (A)

argmin
A⊆ E\( ˜A∪{g})
A ∪ {g} ∪ ˜A, G, w

(cid:17)

= ∅

s. t. C−(cid:16)

(21)

(22)

which is equivalent to Equation 20.

Proof of Theorems 4.1. In Lemmas 4.2 and 4.3 we have proven
that the optimization problem deﬁned in 12 has the optimal
substructure and a greedy choice property. It follows through
induction that the ﬁnal active set MWS(cid:0)G, w, connect_all=True(cid:1)
found by the Mutex Watershed Algorithm 3 is the optimal solution
for the Mutex Watershed objective (12) [62].

4.4 Relation to the extended Power Watershed frame-
work

The Power Watershed [64] is an important framework for graph-
based image segmentation that includes several algorithms like
seeded watershed, random walker and graph cuts. Recently,
[65] extended the framework to even more general types of
hierarchical optimization algorithms thanks to the use of Γ-theory
and Γ-convergence [66], [67]. In this section, we show how the
Mutex Watershed algorithm can also be included in this extended

8

Generic hierarchical optimization:
GHO(Q0, . . . , Qt−1):

M0 = arg minx∈Rm Q0(x)
for k ∈ 1, . . . , t − 1 do

Mk = arg minx∈Mk−1

Qk(x)

end

Return: some x∗ ∈ Mt−1

Algorithm 5: Generic hierarchical optimization algorithm
introduced in [65]. The sequence of continuous functions
Qk : Rm → R is sorted according to the associated scales
λk (Eq. 23).

framework2 and how the framework suggests an optimization
problem that is solved by the Mutex Watershed.

4.4.1 Mutex Watershed as hierarchical optimization algo-
rithm

We ﬁrst start by introducing the extended Power Watershed
framework and restating the main theorem from [65]:

Theorem 4.4.
[65] Extended Power Watershed Framework.
Consider three strictly positive integers p, m, t ∈ N+ and t real
numbers

1 ≥ λ0 > λ1 > . . . λt−1 > 0

(23)

Given t continuous functions Qk : Rm → R with 0 ≤ k < t,
deﬁne the function

Qp(x) :=

(cid:88)

λp
kQk(x).

0≤k<t

(24)

Then, if any sequence (xp)p>0 of minimizers xp of Qp(x) is
bounded (i.e. there exists C > 0 such that for all p > 0, ||xp||∞ ≤
C), the sequence is convergent, up to taking a subsequence, toward
a point of Mt−1, which is the set of minimizers recursively deﬁned
in Algorithm 5.

Proof. See [65] (Theorem 3.3).

We now show that the Mutex Watershed algorithm can be seen
as a special case of the generic hierarchical Algorithm 5, for a
speciﬁc choice of scales λk and functions Qk(x) : Rm → R (see
deﬁnitions (25, 26) below) .

Scales λk: Let ˜wk be the signed edge weights w : E → R ordered
by decreasing absolute value | ˜w1| > | ˜w2| > . . . > | ˜wt−1|. If two
edges share the same weight, then the weight is called ˜wk for both
and Ek ⊆ E denotes the set of all edges with weight ˜wk. We then
deﬁne the scales λk as

λk :=

(cid:40)1
(cid:12)
(cid:12)
(cid:12)

˜wk
2 ˜w1

(cid:12)
(cid:12)
(cid:12)

if k = 0

otherwise.

(25)

The continuous functions Qk(x) : R|E| → R are deﬁned as
follows

Qk(x) :=

(cid:40)

|E| · minx(cid:48)∈ISC(G,w) ||x(cid:48) − x||
(cid:80)

e∈Ek

xe

if k = 0
otherwise,

(26)

2. The connection between the Mutex Watershed and the extended Power

Watershed framework was kindly pointed out by an anonymous reviewer.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

9

PWSMWS(Q0, . . . , Qt−1):

M0 = arg minx∈R|E| Q0(x) = ISC(G, w)
for k ∈ 1, . . . , t − 1 do

Mk = arg minx∈Mk−1

end

Return: some x∗ ∈ Mt−1

(cid:80)

e∈Ek

xe

Algorithm 6: Special case of the general hierarchical Algo-
rithm 5 obtained by substituting Def. (25) and (26). With the
additional assumption of unique signed weights w : E → R,
this algorithm is equivalent to the Mutex Watershed Algorithm
3. The sequence of functions Qk : Rm → R deﬁned in Eq. 26
is sorted according to the associated scales λk in Eq. 25.
ISC(G, w) is deﬁned in Eq. 27

where ISC(G, w) is deﬁned as:

ISC(G, w) := SC(G, w) ∩ {0, 1}|E|.

(27)

In words, Q0(x) is proportional to the distance between x and the
closest point on the set ISC(G, w), whereas Qk(x) depends only
on the indicators xe of edges in Ek, for k > 0.

Algorithm 6 is obtained by substituting the scales λk and
functions Qk(x) (respectively deﬁned in Eq. (25) and (26)) into
Algorithm 5 . The algorithm starts by setting M0 to ISC(G, w),
i.e. by restricting the space of the solutions only to integer edge
labelings x that do not include any conﬂicted cycles. Then, in the
following iterations k ∈ 1, . . . , t − 1, the algorithm solves a series
of minimization sub-problems that in the most general case are
NP-hard, even though they involve a smaller set of edges Ek ⊆
E. Nevertheless, if we assume that all weights are distinct, then
|Ek| = 1 for all k and the solution to the sub-problems amounts
to checking if the new edge can be labeled with xe = 0 without
introducing any conﬂicted cycles. This procedure is identical to
Algorithm 2: at every iteration, the Mutex Watershed tries to add
an edge to the active set A, provided that no mutual exclusion
constraints are violated.

In summary, the framework in [65] provides a new formulation
of the Mutex Watershed Algorithm that is even applicable to
graphs with tied edge weights. In practice, when edge weights are
estimated by a CNN, we do not expect tied edge weights.

4.4.2 Convergence of the sequence of minimizers

In this section, we see how Theorem 4.4 also suggests a minimiza-
tion problem that is solved by the Mutex Watershed algorithm. A
short summary is given in the ﬁnal paragraph of the section.
First, we make sure that the conditions of Theorem 4.4 are satisﬁed
when we apply it to Algorithm 6:

Lemma 4.5. Let us consider the scales λk and continuous
functions Qk(x) : R|E| → R respectively deﬁned in Eq. (25) and
(26). For any value of p ∈ N+, let xp ∈ R|E| be a minimizer of
the function Qp(x) deﬁned in Eq. (24). Then, the minimizer xp
lies in the set ISC(G, w). From this, it follows that any sequence
of minimizers (xp)p>0 is bounded and the conditions of Theorem
4.4 are satisﬁed.

Proof. See Appendix A.

Then, given any p ∈ N+ and the Def. (25, 26), we have that the
minimization of the function Qp(x) deﬁned in Eq. (24) is given
by the following problem:

arg min
x∈Rm

Qp(x) = arg min

x∈Rm

(cid:88)

λp
kQk(x)

0≤k<t

(28)

= arg min
x∈ISC(G,w)

= arg min
x∈ISC(G,w)

(cid:88)

1≤k<t
1
|2 ˜w1|p

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜wk
2 ˜w1

(cid:12)
p
(cid:12)
(cid:12)
(cid:12)

(cid:88)

e∈Ek

xe

(29)

|we|p xe

(30)

(cid:88)

e∈E

where we used Lemma 4.5 and restricted the domain of the arg min
operation to ISC(G, w), so that Q0(x) = 0 for all x ∈ ISC(G, w).
It follows from Lemma 4.5 and Theorem 4.4 that a sequence
of minimizers (xp)p>0 of the problem (30) converge, up to
taking a subsequence, to the solution x∗ returned by Algorithm
6. More speciﬁcally, we know that any minimizer xp of (30) is
in the discrete set ISC(G, w). Hence, the convergent sequence
of minimizers (xp)p>0 eventually becomes constant and there
exists a p(cid:48) ∈ N+ large enough such that xp = x∗ for all p ≥ p(cid:48).
In other words, in the case of unique weights and p ≥ p(cid:48) large
enough, the solution x∗ of the Mutex Watershed Algorithm 6 solves
the problem (30), which is just a rescaled version of the Mutex
Watershed Objective we introduced in Sec. 4.2.

To summarize, we used the extended Power Watershed frame-
work to show that the Mutex Watershed provides a solution to the
minimization problem in Eq. (30) for p large enough. In particular,
this problem suggested by the Power Watershed framework is the
same one previously derived in Sec. 4.2 by linking the Mutex
Watershed Algorithm to the multicut optimization problem.

5 EXPERIMENTS

We evaluate the Mutex Watershed on the challenging task of neuron
segmentation in electron microscopy (EM) image volumes. This
application is of key interest in connectomics, a ﬁeld of neuro-
science that strives to reconstruct neural wiring digrams spanning
complete central nervous systems. The task requires segmentation
of neurons from electron microscopy images of neural tissue – a
challenging endeavor, since segmentation has to be based only on
boundary information (cell membranes) and some of the boundaries
are not very pronounced. Besides, cells contain membrane-bound
organelles, which have to be suppressed in the segmentation. Some
of the neuron protrusions are very thin, but all of those need to be
preserved in the segmentation to arrive at the correct connectivity
graph. While a lot of progress is being made, currently only manual
tracing or proof-reading yields sufﬁcient accuracy for correct circuit
reconstruction [68].

We validate the Mutex Watershed algorithm on the most popular
neural segmentation challenge: ISBI2012 [42]. We estimate the
edge weights using a CNN as described in Section 5.1 and compare
with other entries in the leaderboard as well as with other popular
post-processing methods for the same network predictions in
Section 5.2.

5.1 Estimating edge weights with a CNN

The common ﬁrst step to EM segmentation is to predict which
pixels belong to a cell membrane using a CNN. Different post-
processing methods are then used to obtain a segmentation, see
section 2 for an overview of such methods. The CNN can either be

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

10

trained to predict boundary pixels [38], [41] or undirected afﬁnities
[39], [69] which express how likely it is for a pixel to belong to a
different cell than its neighbors in the 6-neighborhood. In this case,
the output of the network contains three channels, corresponding to
left, down and next imaging plane neighbors in 3D. The afﬁnities do
not have to be limited to immediate neighbors – in fact, [39] have
shown that introduction of long-range afﬁnities is beneﬁcial for the
ﬁnal segmentation even if they are only used to train the network.
Building on the work of [39], we train a CNN to predict short- and
long-range afﬁnities and then use those directly as weights for the
Mutex Watershed algorithm.

We estimate the afﬁnities / edge weights for the neighborhood
structure shown in Figure 6. To that end, we deﬁne local attrac-
tive and long-range repulsive edges. When attractive edges are
only short-range, the solution will consist of spatially connected
segments that cannot comprise “air bridges”. This holds true for
both (lifted) multicut and for Mutex Watershed. We use a different
pattern for in-plane and between-plane edges due to the great
anisotropy of the data set. In more detail, we pick a sparse ring
of in-plane repulsive edges and additional longer-range in-plane
edges which are necessary to split regions reliably (see Figure 6a).
We also added connections to the indirect neighbors in the lower
adjacent slice to ensure correct 3D connectivity (see Figure 6b).
In our experiments, we pick a subset of repulsive edges, by using
strides of 2 in the XY-plane in order to avoid artifacts caused by
occasional very thick membranes. Note that the stride is not applied
to local (attractive) edges, but only to long-range (repulsive) edges.
The particular pattern used was selected after inspecting the size of
typical regions. The speciﬁc pattern is the only one we have tried
and was not optimized over.

In total, C + attractive and C − repulsive edges are deﬁned for
each pixel, resulting in C + + C − output channels in the network.
We partition the set of attractive / repulsive edges into subsets H +
and H − that contain all edges at a speciﬁc offset: E+ = (cid:83)C+
c=1H +
c
for attractive edges, with H − deﬁned analogously. Each element
of the subsets H +
c corresponds to a speciﬁc channel
predicted by the network. We further assume that weights take
values in [0, 1].

c and H −

Network architecture and training
We use the 3D U-Net [43], [70] architecture, as proposed in [69].
∗
w± can be

Our training targets for attractive / repulsive edges
∗
L according to

derived from a groundtruth label image
∗
∗
Li=
1,
if
Lj
0, otherwise

e=(i,j)=

∗
w+

(cid:40)

∗
w−

e=(i,j)=

(cid:40)

∗
∗
Li=
0,
if
Lj
1, otherwise

(31)

(32)

Here, i and j are the indices of vertices / image pixels. Next,

we deﬁne the loss terms

J +

c = −

(cid:80)

e∈H +
c

(1 − w+

e )(1−
e )2 + (1−

∗
w+
e )
∗
w+
e )2)

((1 − w+

e∈H +
c

(cid:80)

J −

c = −

(cid:80)

(cid:80)

∗
w−
w−
e
e
∗
w−
e )2 + (

e∈H−
c
((w−

e )2)

e∈H−
c

(33)

(34)

for attractive edges (i.e. channels) and repulsive edges (i.e. chan-
nels).

(a) XY-plane neighborhood with local attractive edges (green) and
sparse repulsive edges (red) with approximate radius 9 and further
long-range connections with distance 27

(b) Due to the great anisotropy of the data we limit the Z-plane
edges to a distance of 1. The direct neighbors are attractive, whereas
the indirect neighbors are repulsive.

Fig. 6: Local neighborhood structure of attractive (green) and
repulsive (red) edges in the Mutex Watershed graph.

c J +

c J −

c + (cid:80)C−

Equation 33 is the Sørensen-Dice coefﬁcient [71], [72]
formulated for fuzzy set membership values. During training
we minimize the sum of attractive and repulsive loss terms
J = (cid:80)C+
c . This corresponds to summing up the
channel-wise Sørensen-Dice loss. The terms of this loss are robust
against prediction and / or target sparsity, a desirable quality for
neuron segmentation: since membranes are locally two-dimensional
and thin, they occupy very few pixels in three-dimensional the
∗
w+
volume. More precisely, if w+
e (or both) are sparse, we
e or
can expect the denominator (cid:80)
e )2 + (
e((w+
e )2) to be small,
which has the effect that the numerator is adaptively weighted
higher. In this sense, the Sørensen-Dice loss at every pixel i is
conditioned on the global image statistics, which is not the case
for a Hamming-distance based loss like Binary Cross-Entropy or
Mean Squared Error.

∗
w+

We optimize this loss using the Adam optimizer [73] and
additionally condition learning rate decay on the Adapted Rand
Score [42] computed on the training set every 100 iterations.
During training, we augment the data set by performing in-plane

(9,4)(-9,4)(9,-4)(-9,-4)(4,9)(4,-9)(-4,9)(-4,-9)(0,-9)(0,9)(9,0)(-9,0)(9,-9)(9,9)(-9,-9)(-9,9)(0,-27)(0,27)(27,0)(-27,0)IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

11

rotations by multiples of 90 degrees, ﬂips along the X- and Y-axis
as well as elastic deformations. At prediction time, we use test time
data augmentation, presenting the network with seven different
versions of the input obtained by a combination of rotations by a
multiple of 90 degrees, axis-aligned ﬂips and transpositions. The
network predictions are then inverse-transformed to correspond to
the original image, and the results averaged.

5.2 ISBI Challenge

The ISBI 2012 EM Segmentation Challenge [42] is the neuron
segmentation challenge with the largest number of competing
entries. The challenge data contains two volumes of dimensions 1.5
× 2 × 2 microns and has a resolution of 50 × 4 × 4 nm per pixel.
The groundtruth is provided as binary membrane labels, which can
easily be converted to a 2D, but not 3D segmentation. To train a
3D model, we follow the procedure described in [38].

The test volume has private groundtruth; results can be
submitted to the leaderboard. They are evaluated based on the
Adapted Rand Score (Rand-Score) and the Variation of Information
Score (VI-Score) [42].

Our method holds the top entry in the challenge’s leader
board3 at the time of submission, see Table 1a. This is especially
remarkable insofar as it is simpler than the methods holding
the other top entries. Three out of four rely on a CNN to
predict boundary locations and postprocess its output with the
complex pipeline described in [38]. This post-processing ﬁrst
generates superpixels via distance transform watersheds. Then
it computes a merge cost for local and long-range connections
between superpixels. Based on this, it deﬁnes a lifted multicut
partioning problem that is solved approximately. In contrast, our
method ﬁnds an optimal solution of its objective purely on the
pixel level.

Comparison with other segmentation methods

The weights predicted by the CNN described above can be post-
processed directly by the Mutex Watershed algorithm. To ensure
a fair comparison, we transform the same CNN predictions into
a segmentation using basic and state-of-the-art post-processing
methods. We start from simple thresholding (THRESH) and seeded
watershed. Since these cannot take long-range repulsions into
account, we generate a boundary map by taking the maximum4
values over the attractive edge channels. Based on this boundary
map, we introduce seeds at the local minima (WS) and at the
maxima of the smoothed distance transform (WSDT). For both
variants, the degree of smoothing was optimized such that each
region receives as few seeds as possible, without however causing
severe under-segmentation. The performance of these three baseline
methods in comparison to Mutex Watershed is summarized in
Table 1b. The methods were applied only in 2D, because the high
degree of anisotropy leads to inferior results when applied in 3D.
In contrast, the Mutex Watershed can be applied in 3D out of the
box and yields signiﬁcantly better 2D segmentation scores.

Qualitatively, we show patches of results in Figure 7. The
major failure case for WS (Figure 7e) and WSDT (Figure
7f) is over-segmentation caused by over-seeding a region. The
major failure case for THRESH is under-segmentation due to
week boundary evidence (see Figure 7d). In contrast, the Mutex

3. http://brainiac2.mit.edu/isbi_challenge/leaders-board-new
4. The maximum is chosen to preserve boundaries.

(a) Mutex Watershed

(b) Mutex Watershed

(c) Multicut partitioning based
segmentation (MC-FULL)

(d) Thresholding of local
boundary maps (THRESH)

(e) Watershed, seeded at local
minima of the smoothed input
map (WS)

(f) Distance Transform
Watershed (WSDT)

Fig. 7: Mutex Watershed and baseline segmentation algorithms
applied on the ISBI Challenge test data. Red arrows point out major
errors. Orange arrows point to difﬁcult, but correctly segmented
regions. All methods share the same input maps.

Watershed produces a better segmentation, only causing minor
over-segmentation (see Figure 7a, Figure 7b).

Note that, in contrast to most pixel-based postprocessing
methods, our algorithm can take long range predictions into account.
To compare with methods which share this property, we turn
to the multicut and lifted multicut-based partitioning for neuron
segmentations as introduced in [29] and [36]. As proposed in [74],
we compute costs corresponding to edge cuts from the afﬁnities
estimated by the CNN via:




,



se =

if e ∈ E+

log w+
e
1−w+
e
log 1−w−
e
w−
e
We set up two multicut problems: the ﬁrst is induced only by the
short-range edges (MC-LOCAL), the other by short- and long-
range edges together (MC-FULL). Note that the solution to the
full connectivity problem can contain “air bridges”, i.e. pixels that

, otherwise,

(35)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

12

Method

Rand-Score

VI-Score

UNet + MWS
ResNet + LMC [76]
SCN + LMC [77]
M2FCN-MFA [78]
FusionNet + LMC [44]

0.98792
0.98788
0.98680
0.98383
0.98365

0.99183
0.99072
0.99144
0.98981
0.99130

7 ACKNOWLEDGMENTS

This work was partially supported by the grants DFG HA 4364/8-1,
DFG SFB 1129 from the Deutsche Forschungsgemeinschaft and
the Baden-Württemberg Stiftung Elite PostDoc Program.

(a) Top ﬁve entries at time of submission. Our Mutex Watershed
(MWS) is state-of-the-art without relying on the complex lifted
multicut postprocessing used by most other top entries.

REFERENCES

Method

Rand-Score

VI-Score

Time [s]

MWS
MC-FULL
LMC
THRESH
WSDT
MC-LOCAL
WS

0.98792
0.98029
0.97990
0.91435
0.88336
0.70990
0.63958

0.99183
0.99044
0.99007
0.96961
0.96312
0.86874
0.89237

43.3
9415.8
966.0
0.2
4.4
1410.7
4.9

(b) Comparison to other segmentation strategies, all of which are
based on our CNN. Runtimes were measured on a single thread of a
Intel Xeon CPU E5-2650 v3 @ 2.30GHz.

TABLE 1: Results on the ISBI 2012 EM Segmentation Challenge.

are connected only by long-range edges, without a path along the
local edges connecting them. However, we found this not to be a
problem in practice. In addition, we set up a lifted multicut (LMC)
problem from the same edge costs.

Both problems are NP-hard, hence it is not feasible to solve
them exactly on large grid graphs. For our experiments, we use
the approximate Kernighan Lin [33], [75] solver. Even this allows
us to only solve individual 2D problems at a time. The results for
MC-LOCAL and MC-FULL can be found in Table 1b. The MC-
LOCAL approach scores poorly because it under-segments heavily.
This observation emphasizes the importance of incorporating the
longer-range edges. The MC-FULL and LMC approaches perform
well. Somewhat surprisingly, the Mutex Watershed yields a better
segmentation still, despite being much cheaper in inference. We
note that both MC-FULL, LMC and the Mutex Watershed are
evaluated on the same long-range afﬁnity maps (i.e. generated by
the same CNN with the same set of weights).

6 CONCLUSION AND DISCUSSION

We have presented a fast algorithm for the clustering of graphs
with both attractive and repulsive edges. The ability to consider
both gives a valid alternative to other popular graph partitioning
algorithms that rely on a stopping criterion or seeds. The proposed
method has low computational complexity in imitation of its close
relative, Kruskal’s algorithm. We have shown which objective this
algorithm optimizes exactly, and that this objective emerges as a
speciﬁc case of the multicut objective. It is possible that recent
interesting work [63] on partial optimal solutions may open an
avenue for an alternative proof.

Finally, we have found that the proposed algorithm, when
presented with informative edge costs from a good neural network,
outperforms all known methods on a competitive bioimage par-
titioning benchmark, including methods that operate on the very
same network predictions.

[1] S. Wolf, C. Pape, A. Bailoni, N. Rahaman, A. Kreshuk, U. Köthe, and
F. Hamprecht, “The mutex watershed: Efﬁcient, parameter-free image
partitioning,” Proc. ECCV’18, 2018.

[2] L. Vincent and P. Soille, “Watersheds in digital spaces: an efﬁcient
algorithm based on immersion simulations,” IEEE Trans. Pattern Analysis
Machine Intelligence, no. 6, pp. 583–598, 1991.

[3] S. Beucher and C. Lantuéjoul, “Use of watersheds in contour detection,”
in Int. Workshop on Image Processing. Rennes, France: CCETT/IRISA,
Sept. 1979.

[4] S. Beucher and F. Meyer, “The morphological approach to segmentation:
the watershed transformation,” Optical Engineering, vol. 34, pp. 433–433,
1992.

[5] R. Pohle and K. D. Toennies, “Segmentation of medical images using
adaptive region growing,” in Medical Imaging 2001: Image Processing,
vol. 4322.
International Society for Optics and Photonics, 2001, pp.
1337–1346.

[6] M. A. Alattar, N. F. Osman, and A. S. Fahmy, “Myocardial segmenta-
tion using constrained multi-seeded region growing,” in International
Conference Image Analysis and Recognition. Springer, 2010, pp. 89–98.
[7] S. Poonguzhali and G. Ravindran, “A complete automatic region growing
method for segmentation of masses on ultrasound images,” in 2006
International Conference on Biomedical and Pharmaceutical Engineering.
IEEE, 2006, pp. 88–92.
J. Wu, S. Poehlman, M. D. Noseworthy, and M. V. Kamath, “Texture
feature based automated seeded region growing in abdominal MRI seg-
mentation,” in 2008 International Conference on BioMedical Engineering
and Informatics, vol. 2.

IEEE, 2008, pp. 263–267.

[8]

[9] A. Q. Al-Faris, U. K. Ngah, N. A. M. Isa, and I. L. Shuaib, “Computer-
aided segmentation system for breast MRI tumour using modiﬁed
automatic seeded region growing (BMRI-MASRG),” Journal of digital
imaging, vol. 27, no. 1, pp. 133–144, 2014.

[10] J. Shan, H.-D. Cheng, and Y. Wang, “A novel automatic seed point selec-
tion algorithm for breast ultrasound images,” in 2008 19th International
Conference on Pattern Recognition.

IEEE, 2008, pp. 1–4.

[11] D. M. N. Mubarak, M. M. Sathik, S. Z. Beevi, and K. Revathy, “A hybrid
region growing algorithm for medical image segmentation,” International
Journal of Computer Science & Information Technology, vol. 4, no. 3,
p. 61, 2012.

[12] M. Abdelsamea, “An enhancement neighborhood connected segmentation
for 2D-Cellular Image,” International Journal of Bioscience, Biochemistry
and Bioinformatics, vol. 1, no. 4, 2011.

[13] A. Q. Al-Faris, U. K. Ngah, N. A. M. Isa, and I. L. Shuaib, “Breast MRI
tumour segmentation using modiﬁed automatic seeded region growing
based on particle swarm optimization image clustering,” in Soft Computing
in Industrial Applications. Springer, 2014, pp. 49–60.

[14] M. Grimaud, “New measure of contrast: the dynamics,” in Proc. Image Al-
gebra and Morphological Processing, ser. SPIE Conf. Series, P. D. Gader,
E. R. Dougherty, & J. C. Serra, Ed., vol. 1769, 1992, pp. 292–305.
[15] S. Beucher, “Watershed, hierarchical segmentation and waterfall algo-

rithm.” in Proc. ISMM’94, vol. 94, 1994, pp. 69–76.

[16] C. Vachier and F. Meyer, “Extinction value: a new measurement of
persistence,” in Worksh. Nonlinear Signal and Image Processing, vol. 1,
1995, pp. 254–257.

[17] L. Najman and M. Schmitt, “Geodesic saliency of watershed contours and
hierarchical segmentation,” IEEE Trans. Pattern Analysis and Machine
Intelligence, vol. 18, no. 12, pp. 1163–1173, 1996.

[18] P. Soille, “Constrained connectivity for hierarchical image decomposition
and simpliﬁcation,” IEEE Trans. Patt. Anal. Mach. Intell., vol. 30, no. 7,
pp. 1132–1145, 2008.

[19] B. Perret, J. Cousty, S. J. F. Guimaraes, and D. S. Maia, “Evaluation of
hierarchical watersheds,” IEEE Transactions on Image Processing, vol. 27,
no. 4, pp. 1676–1688, 2018.

[20] F. Meyer, “Morphological multiscale and interactive segmentation.” in

NSIP, 1999, pp. 369–377.

[21] L. Najman, “On the equivalence between hierarchical segmentations and
ultrametric watersheds,” J. of Mathematical Imaging and Vision, vol. 40,
no. 3, pp. 231–247, 2011.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

13

[22] P. Salembier and L. Garrido, “Binary partition tree as an efﬁcient
representation for image processing, segmentation, and information
retrieval,” IEEE Trans. Image Proc., vol. 9, pp. 561–576, 2000.

[23] F. Malmberg, R. Strand, and I. Nyström, “Generalized hard constraints
for graph segmentation,” in Scandinavian Conference on Image Analysis.
Springer, 2011, pp. 36–47.

[24] P. F. Felzenszwalb and D. P. Huttenlocher, “Efﬁcient Graph-Based Image
Segmentation,” Int. J. Comput. Vision, vol. 59, no. 2, pp. 167–181, 2004.
[25] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and
hierarchical image segmentation,” IEEE Trans. Patt. Anal. Mach. Intell.,
vol. 33, no. 5, pp. 898–916, 2011.

[26] B. R. Kiran and J. Serra, “Global–local optimizations by hierarchical cuts
and climbing energies,” Pattern Recognition, vol. 47, no. 1, pp. 12–24,
2014.

[27] L. Guigues, J. P. Cocquerez, and H. Le Men, “Scale-sets image analysis,”
International Journal of Computer Vision, vol. 68, no. 3, pp. 289–317,
2006.

[28] M. Keuper, S. Tang, Y. Zhongjie, B. Andres, T. Brox, and B. Schiele,
“A multi-cut formulation for joint segmentation and tracking of multiple
objects,” arXiv preprint arXiv:1607.06317, 2016.

[29] B. Andres, T. Kröger, K. L. Briggmann, W. Denk, N. Norogod, G. Knott,
U. Köthe, and F. A. Hamprecht, “Globally optimal closed-surface
segmentation for connectomics,” in Proc. ECCV’12, part 2, no. 7574,
2012, pp. 778–791.

[30] J. Yarkony, A. Ihler, and C. C. Fowlkes, “Fast planar correlation clustering
for image segmentation,” in Proc. ECCV’12, 2012, pp. 568–581.
[31] C. Pape, T. Beier, P. Li, V. Jain, D. D. Bock, and A. Kreshuk, “Solving
large multicut problems for connectomics via domain decomposition,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017, pp. 1–10.

[32] Z. Levi and D. Zorin, “Strict minimizers for geometric optimization,”
ACM Trans. Graph., vol. 33, no. 6, pp. 185:1–185:14, Nov. 2014.
[33] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres,
“Efﬁcient decomposition of image and mesh graphs by lifted multicuts,”
in Proc. ICCV’15, 2015, pp. 1751–1759.

[34] E. Levinkov, A. Kirillov, and B. Andres, “A comparative study of local
search algorithms for correlation clustering,” in German Conference on
Pattern Recognition. Springer, 2017, pp. 103–114.

[35] C. Zhang, J. Yarkony, and F. A. Hamprecht, “Cell detection and
segmentation using correlation clustering,” in Proc. MICCAI’14, 2014, pp.
9–16.

[36] A. Horˇnáková, J.-H. Lange, and B. Andres, “Analysis and optimization of
graph decompositions by lifted multicuts,” in International Conference on
Machine Learning, 2017, pp. 1539–1548.

[37] T. Beier, B. Andres, U. Köthe, and F. A. Hamprecht, “An efﬁcient
fusion move algorithm for the minimum cost lifted multicut problem,” in
European Conference on Computer Vision. Springer, 2016, pp. 715–730.
[38] T. Beier, C. Pape, N. Rahaman, and T. e. a. Prange, “Multicut brings
automated neurite segmentation closer to human performance,” Nature
Methods, vol. 14, no. 2, pp. 101–102, 2017.

[39] K. Lee, J. Zung, P. Li, V. Jain, and H. S. Seung, “Superhuman accuracy on
the SNEMI3D connectomics challenge,” arXiv preprint arXiv:1706.00120,
2017.

[40] V. Jain, J. F. Murray, F. Roth, S. Turaga, V. Zhigulin, K. L. Briggman,
M. N. Helmstaedter, W. Denk, and H. S. Seung, “Supervised learning of
image restoration with convolutional networks,” Proc. ICCV’07, pp. 1–8,
2007.

[41] D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber, “Deep
neural networks segment neuronal membranes in electron microscopy
images,” Proc. NIPS’12, 2012.

[42] I. Arganda-Carreras, S. Turaga, D. Berger et al., “Crowdsourcing the
creation of image segmentation algorithms for connectomics,” Front.
Neuroanatomy, vol. 9, p. 142, 2015.

[43] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks
for biomedical image segmentation,” Proc. MICCAI’15, pp. 234–241,
2015.

[44] T. M. Quan, D. G. Hilderbrand, and W.-K. Jeong, “FusionNet: a deep
fully residual convolutional neural network for image segmentation in
connectomics,” arXiv:1612.05360, 2016.

[45] J. Nunez-Iglesias, R. Kennedy, T. Parag, J. Shi, and D. Chklovskii,
“Machine learning of hierarchical clustering to segment 2D and 3D images,”
PLoS one, vol. 8, p. e71715, 2013.

[46] S. Knowles-Barley, V. Kaynig, T. R. Jones, A. Wilson, J. Morgan, D. Lee,
D. Berger, N. Kasthuri, J. W. Lichtman, and H. Pﬁster, “RhoanaNet
pipeline: Dense automatic neural annotation,” arXiv:1611.06973, 2016.
[47] M. G. Uzunba¸s, C. Chen, and D. Metaxas, “Optree: a learning-based
adaptive watershed algorithm for neuron segmentation,” in Int. Conf. Med-

ical Image Computing and Computer-Assisted Intervention (MICCAI’14),
2014, pp. 97–105.

[48] Y. Meirovitch, A. Matveev, H. Saribekyan, D. Budden, D. Rolnick,
G. Odor, S. K.-B. T. R. Jones, H. Pﬁster, J. W. Lichtman, and
N. Shavit, “A multi-pass approach to large-scale connectomics,” arXiv
preprint:1612.02120, 2016.

[49] M. Januszewski, J. Kornfeld, P. H. Li, A. Pope, T. Blakely, L. Lindsey,
J. Maitin-Shepard, M. Tyka, W. Denk, and V. Jain, “High-precision
automated reconstruction of neurons with ﬂood-ﬁlling networks,” Nature
methods, p. 1, 2018.

[50] A. Zlateski and H. S. Seung, “Image segmentation by size-dependent
single linkage clustering of a watershed basin graph,” arXiv:1505.00249,
2015.

[51] T. Parag, F. Tschopp, W. Grisaitis, S. C. Turaga, X. Zhang, B. Mate-
jek, L. Kamentsky, J. W. Lichtman, and H. Pﬁster, “Anisotropic EM
segmentation by 3d afﬁnity learning and agglomeration,” arXiv preprint
1707.08935, 2017.

[52] S. C. Turaga, J. F. Murray, V. Jain, F. Roth, M. Helmstaedter, K. Briggman,
W. Denk, and H. S. Seung, “Convolutional networks can learn to generate
afﬁnity graphs for image segmentation,” Neural Computation, vol. 22,
no. 2, pp. 511–538, 2010.

[53] K. Briggman, W. Denk, S. Seung, M. N. Helmstaedter, and S. C. Turaga,
“Maximin afﬁnity learning of image segmentation,” in Advances in Neural
Information Processing Systems, 2009, pp. 1865–1873.

[54] S. Wolf, L. Schott, U. Köthe, and F. Hamprecht, “Learned watershed:
End-to-end learning of seeded segmentation,” Proc. ICCV’17, 2017.
[55] M. Bai and R. Urtasun, “Deep watershed transform for instance seg-
mentation,” in 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

IEEE, 2017, pp. 2858–2866.

[56] S. Xie and Z. Tu, “Holistically-nested edge detection,” in Proc. ICCV’15,

2015, pp. 1395–1403.

[57] I. Kokkinos, “Pushing the boundaries of boundary detection using deep

learning,” arXiv:1511.07386, 2015.

[58] J. Cai, L. Lu, Z. Zhang, F. Xing, L. Yang, and Q. Yin, “Pancreas
segmentation in MRI using graph-based decision fusion on convolutional
neural networks,” in Proc. MICCAI, 2016.

[59] F. Meyer, “Topographic distance and watershed lines,” Signal processing,

vol. 38, no. 1, pp. 113–125, 1994.

[60] ——, “Minimum spanning forests for morphological segmentation,” in
Mathematical morphology and its applications to image processing, 1994,
pp. 77–84.

[61] A. X. Falcão, J. Stolﬁ, and R. de Alencar Lotufo, “The image foresting
transform: Theory, algorithms, and applications,” IEEE Trans. Patt. Anal.
Mach. Intell., vol. 26, no. 1, pp. 19–29, 2004.

[62] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to

Algorithms, Third Edition, 3rd ed. The MIT Press, 2009.

[63] J.-H. Lange, A. Karrenbauer, and B. Andres, “Partial optimality and
fast lower bounds for weighted correlation clustering,” in International
Conference on Machine Learning, 2018, pp. 2898–2907.

[64] C. Couprie, L. Grady, L. Najman, and H. Talbot, “Power watershed: A
unifying graph-based optimization framework,” IEEE Trans. Patt. Anal.
Mach. Intell., vol. 33, no. 7, 2011.

[65] L. Najman, “Extending the power watershed framework thanks to Γ-
convergence,” SIAM Journal on Imaging Sciences, vol. 10, no. 4, pp.
2275–2292, 2017.

[66] G. Dal Maso, An introduction to Γ-convergence. Springer Science &

Business Media, 2012, vol. 8.

[67] A. Braides, “A handbook of γ-convergence,” in Handbook of Differential
Elsevier, 2006,

Equations: stationary partial differential equations.
vol. 3, pp. 101–213.

[68] P. Schlegel, M. Costa, and G. S. X. E. Jefferis, “Learning from
connectomics on the ﬂy.” Current opinion in insect science, vol. 24,
pp. 96–105, 2017.

[69] J. Funke, F. D. Tschopp, W. Grisaitis, A. Sheridan, C. Singh, S. Saalfeld,
and S. C. Turaga, “Large scale image segmentation with structured loss
based deep learning for connectome reconstruction,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2018.

[70] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
“3D U-Net: learning dense volumetric segmentation from sparse anno-
tation,” in International Conference on Medical Image Computing and
Computer-Assisted Intervention. Springer, 2016, pp. 424–432.

[71] L. R. Dice, “Measures of the amount of ecologic association between

species,” Ecology, vol. 26, no. 3, pp. 297–302, 1945.

[72] T. Sørensen, “A method of establishing groups of equal amplitude in plant
sociology based on similarity of species and its application to analyses of
the vegetation on danish commons,” Biol. Skr., vol. 5, pp. 1–34, 1948.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

14

Anna Kreshuk received a diploma in maths
from Lomonosov Moscow State University and
a PhD in computer science in Heidelberg. She
is currently a group leader in EMBL Heidelberg.
Her research focuses on automating analysis of
microscopy images with machine learning.

Ullrich Köthe received a diploma in physics from
the University of Rostock and a PhD in Computer
Science from the University of Hamburg. He is
currently an associate professor for computer sci-
ence in the Interdisciplinary Center for Scientiﬁc
Computing at Heidelberg University. His research
focuses on the connection between image anal-
ysis and machine learning, and in particular on
the interpretability of machine learning results.

Fred A. Hamprecht is Professor for Image Anal-
ysis and Learning at Heidelberg University. His
primary research interests are algorithms and
how they can be used to solve biological prob-
lems.

[73] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

2015.

[74] B. Andres, T. Kroeger, K. L. Briggman, W. Denk, N. Korogod, G. Knott,
U. Koethe, and F. A. Hamprecht, “Globally optimal closed-surface
segmentation for connectomics,” in European Conference on Computer
Vision. Springer, 2012, pp. 778–791.

[75] B. W. Kernighan and S. Lin, “An efﬁcient heuristic procedure for
partitioning graphs,” The Bell system technical journal, vol. 49, no. 2, pp.
291–307, 1970.

[76] C. Xiao, J. Liu, X. Chen, H. Han, C. Shu, and Q. Xie, “Deep contex-
tual residual network for electron microscopy image segmentation in
connectomics,” in Biomedical Imaging (ISBI 2018), 2018 IEEE 15th
International Symposium on.

IEEE, 2018, pp. 378–381.

[77] M. Weiler, F. A. Hamprecht, and M. Storath, “Learning steerable ﬁlters

for rotation equivariant CNNs,” 2018, pp. 849–858.

[78] W. Shen, B. Wang, Y. Jiang, Y. Wang, and A. L. Yuille, “Multi-stage
multi-recursive-input fully convolutional networks for neuronal boundary
detection,” 2017 IEEE International Conference on Computer Vision
(ICCV), pp. 2410–2419, 2017.

Steffen Wolf is a Ph.D. candidate in the In-
terdisciplinary Center for Scientiﬁc Computing
at Heidelberg University. His research interests
include computer vision, structured prediction
and deep learning with applications in image
segmentation.

Alberto Bailoni is a Ph.D. candidate in the In-
terdisciplinary Center for Scientiﬁc Computing
at Heidelberg University. His research interests
include computer vision, image segmentation,
deep learning and clustering algorithms, with a
focus on their application to automated recon-
struction of neural circuit connectivity.

Constantin Pape is a Ph.D. candidate at Univer-
sity of Heidelberg visting the EMBL Heidelberg.
His research interests include biomedical imag-
ing with a focus on deep learning and instance
segmentation for large EM datasets.

Nasim Rahaman is a MSc student at University
of Heidelberg. His research interests are deep
learning, reinforcement learning and learning
theory.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

15

APPENDIX A
PROPERTY OF THE MINIMIZERS OF Qp(x)
Lemma 4.5. Let us consider the scales λk and continuous
functions Qk(x) : R|E| → R respectively deﬁned in Eq. (25) and
(26). For any value of p ∈ N+, let xp ∈ R|E| be a minimizer of
the function Qp(x) deﬁned in Eq. (24). Then, the minimizer xp
lies in the set ISC(G, w). From this, it follows that any sequence
of minimizers (xp)p>0 is bounded and the conditions of Theorem
4.4 are satisﬁed.

Proof. The function Qp(x) can be explicitly written as (see Eq.
24, 25 and 26):

Qp(x) =

(cid:88)

λp
kQk(x)

0≤k<t

(36)

= |E| min

x(cid:48)∈ISC(G,w)

||x − x(cid:48)|| +

(cid:88)

1≤k<t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜wk
2 ˜w1

(cid:12)
p
(cid:12)
(cid:12)
(cid:12)

(cid:88)

xe

e∈Ek

= |E| min

x(cid:48)∈ISC(G,w)

||x − x(cid:48)|| +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

we
2 ˜w1

(cid:12)
p
(cid:12)
(cid:12)
(cid:12)

(cid:88)

e∈E

xe.

We then denote these two terms by:

||x − x(cid:48)||,

Qp

A(x) := |E| min
x(cid:48)∈ISC(G,w)
(cid:12)
(cid:12)
p
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B(x) :=

we
2 ˜w1

(cid:88)

xe.

Qp

e∈E

(37)

(38)

(39)

(40)

B(x).

Intuitively, we now prove that the minimizer xp of Qp(x) lies
in ISC(G, w) by showing that the ﬁrst term Qp
A(x) is always
“dominant” as compared to Qp
First, we note that the gradient of the ﬁrst term Qp
A(x) has always
norm equal to |E| and points in the direction of the closest point
x(cid:48) ∈ ISC(G, w). Given a generic point y ∈ R|E|, the only two
cases when the gradient ∇x Qp
A(x) does not exists are: i) if y ∈
ISC(G, w); ii) if there are at least two points x(cid:48)(cid:48), x(cid:48)(cid:48)(cid:48) ∈ ISC(G, w)
such that ||y − x(cid:48)(cid:48)|| = ||y − x(cid:48)(cid:48)(cid:48)||. Clearly, Qp
A(x) presents minima
only in the ﬁrst case, when y ∈ ISC(G, w).
On the other hand, the second term Qp
and the norm of its gradient is never greater than (cid:112)|E|:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B(x) is always differentiable

||∇x Qp

B(x)|| <

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(41)

∇x

|E|

(cid:88)

xe

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:113)

=

(cid:32)

e∈E

where we used the fact that ˜wk/2 ˜w1 < 1 for every 1 ≤ k < t.
Thus, the magnitude of the gradient given by the ﬁrst term is always
larger compared to the one given by the second term. We then
conclude that the objective can always be reduced unless xp is a
point of ISC(G, w).

