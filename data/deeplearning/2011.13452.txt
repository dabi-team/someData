ShapeFlow: Dynamic Shape Interpreter for
TensorFlow

Sahil Verma
Department of Computer Science and Engineering
University of Washington
Seattle, USA
vsahil@cs.washington.edu

Zhendong Su
Department of Computer Science
ETH Zurich
Zurich, Switzerland
zhendong.su@inf.ethz.ch

0
2
0
2

v
o
N
6
2

]

G
L
.
s
c
[

1
v
2
5
4
3
1
.
1
1
0
2
:
v
i
X
r
a

Abstract
We present ShapeFlow, a dynamic abstract interpreter for
TensorFlow which quickly catches tensor shape incompat-
ibility errors, one of the most common bugs in deep learn-
ing code. ShapeFlow shares the same APIs as TensorFlow
but only captures and emits tensor shapes, its abstract do-
main. ShapeFlow constructs a custom shape computational
graph, similar to the computational graph used by Tensor-
Flow. ShapeFlow requires no code annotation or code mod-
ification by the programmer, and therefore is convenient to
use. We evaluate ShapeFlow on 52 programs collected by
prior empirical studies to show how fast and accurately it
can catch shape incompatibility errors compared to Tensor-
Flow. We use two baselines: a worst-case training dataset size
and a more realistic dataset size. ShapeFlow detects shape
incompatibility errors highly accurately â€” with no false pos-
itives and a single false negative â€” and highly efficiently â€”
with an average speed-up of 499X and 24X for the first and
second baseline, respectively. We believe ShapeFlow is a
practical tool that benefits machine learning developers. We
will open-source ShapeFlow on GitHub to make it publicly
available to both the developer and research communities.

1 Introduction
Deep learning technologies have taken off in the last decade,
and so has the code to build them. Tools like TensorFlow [8],
Pytorch [40], Caffe [27] and, Keras [15], which have been
developed in the last decade, have more than 250K stars
on their open-source repositories. Programmers use these
frameworks to code their deep learning stack, and for effi-
cient training and testing. Given the steep rise in deep learn-
ing use and deployment, bugs in such programs are also
increasing in importance and abundance. Figure 1 shows
the distribution of type of bugs that frequently occur in
deep-learning code â€” the information was obtained from
StackOverFlow programs collected in Islam et al.â€™s empirical
study [26].

Most of the operations in deep learning code are manipu-
lations involving one or more tensors, which are matrices of
any dimension (matrices are 2-dimensional tensors). Each
operation imposes a precondition on the shapes of the ten-
sors that it takes as input. For instance, an operation might

1

require that the larger first dimension of the two input ten-
sors be divisible by the smaller first dimension of the second
tensor. If this precondition is met, the operation outputs a
manipulated tensor whose dimension is a function of the
input tensors. When such a precondition is not met, an error
is encountered. The error may or may not lead to a program
crash. Given the intricate API hooks provided by the libraries
and the size of the code, the possibility of shape incompati-
bility errors is high in all stages of a deep learning pipeline,
from data loading to testing. The root causes of shape in-
compatibility errors include unexpected shapes of input data,
typically causing a mismatch with the first layer of the model
architecture or between the shapes of different architectural
layers. Unexpected input data shape errors can occur dur-
ing training or testing of the model. Even for programs that
crash, encountering shape incompatibility errors can take
a large amount of computation time, especially when the
bug is present in later stages of the pipeline. Lengthy pre-
processing due to enormous (training and testing) dataset
sizes further increases the detection time. This can lead to
a significant loss of computing and programmersâ€™ time, as
all the computations performed until the program crash is
wasted. The high prevalence of shape incompatibility errors
and the typical delay in their detection motivated this work
to develop a practical technique for quickly and accurately
detecting such errors. An ideal technique for detecting shape
incompatibility errors would:

1. be accurate and fast in catching shape incompatibility

errors;

2. be capable of supporting the highly data-dependent

deep learning code; and
3. require no code annotation.

To the best of our knowledge, ShapeFlow is the first such
technique that helps expedite the detection of shape incom-
patibility errors for deep-learning code. The only previous
work which partially attempts to address this problem is
Ariadne [16], a static analysis tool for machine learning code
written in TensorFlow. The programmer must annotate each
line of the code in which any tensor operation takes place
to indicate the expected output shape after the execution of
that operation. Annotating code is a daunting task for pro-
grammers and is a major drawback of the Ariadne approach
â€” its motivating example would require the programmer to

 
 
 
 
 
 
, ,

Sahil Verma and Zhendong Su

annotate 16 lines of the total 36 lines in the example pro-
gram. Recently Ariadne was extended by Pythia [30] which
is concurrent to our work, and is a static analysis approach
and therefore suffers from the standard limitations of static
analysis, like it may require certain annotations; whereas
ShapeFlow takes a complementary dynamic approach.

To this end, we propose a dynamic shape-abstracted ver-
sion of TensorFlow, ShapeFlow. We chose TensorFlow as
our deep-learning skeleton framework because it is the most
popular deep learning framework [2, 3, 5]. As of August
2020, 90,236 GitHub projects were using TensorFlow, which
is close to twice of those (49,145 projects) using Pytorch, the
second most popular deep-learning framework.

ShapeFlow is powered by shape-capturing APIs. Shape-
Flowâ€™s implementation has the same APIs as TensorFlow,
but have been modified to only capture and manipulate ten-
sor shape information. This design has two clear advantages:

â€¢ It accelerates the code analysis for any shape incom-
patibility error occurring later in the deep-learning
pipeline; and

â€¢ Since ShapeFlow uses Python and TensorFlow for
interpretation, it does not require any annotation by
its user.

We design ShapeFlow to be dynamic because most of the
deep-learning code is data-dependent, and therefore, code
execution starts from inferring the input data shape and
its application of operations on the data in the following
layers. If it detects any shape incompatibility errors, the
execution crashes with a message similar to the one returned
by TensorFlow; otherwise, it returns a â€œno error detectedâ€
message.

We evaluate ShapeFlow on a set of 52 programs from re-
cent empirical studies on deep-learning bugs [26, 56]. Shape-
Flow successfully detects bugs in all but one (i.e., a single
false negative) of the buggy programs which have a shape in-
compatibility error. ShapeFlow produces no false positives
on the corrected versions of all the programs. We report the
speedup ShapeFlow achieved over TensorFlow with respect
to two baselines. In the first baseline, we consider the case
when a programmer runs their code on the entire training
dataset. Datasets in machine learning, deep learning specif-
ically can be enormously large, and this can significantly
delay the catch of errors. The second baseline is more real-
istic where the programmer runs the code on a part of the
training dataset, which helps encounter any errors in the
program faster, before deploying it on the entire dataset. This
is also called sanity checking in the machine learning com-
munity. ShapeFlow achieved an average speedup of 499X
(upto 23,532X on some benchmarks) for the first baseline and
an average speedup of 24X (upto 180X on some benchmarks)
for the second baseline.

To summarize, our main contributions are:

Figure 1. The prevalence of common type of bugs in Ten-
sorFlow code according to Islam et al.â€™s empirical study [26].

1. We propose ShapeFlow, a dynamic shape abstract
version of TensorFlow for accurate and fast detection
of shape incompatibility errors in deep learning code
written in TensorFlow.

2. We implement and open-source ShapeFlow for ben-
efiting the community and for facilitating the review
and reproduction of our results. The code is available
at https://github.com/vsahil/Static-Analyser_ML.
3. We evaluate the effectiveness of ShapeFlow with an

extensive benchmark suite.

The remainder of the paper is structured as follows. Sec-
tion 2 provides a motivating example to illustrate our ap-
proach. Computational graph for shape inference is dis-
cussed in Section 3, and implementation details are given in
Section 3.3, followed by our evaluation in Section 4. Finally,
we discuss related work (Section 5) and conclude (Section 6).

2 Motivating Example
Here we illustrate shape incompatibility errors in deep-learning
code using an example. Figure 2 shows an example of a neural
network architecture to be trained (and tested) on identify-
ing hand-written digits from the MNIST dataset [31]. The
code has been adapted from the convolutional networks ex-
ample section in the open-source code of TensorFlow [4],
and modified to show only the parts of interest. Placeholders
for images and their labels are declared on lines 18-19, which
will be populated using the MNIST dataset when training or
testing. Function conv_net creates the convolutional neu-
ral network architecture. The code for training and testing
module is shortened for space considerations.

To demonstrate the difference between ShapeFlow and
Ariadne, we have added the annotations in Figure 2 (com-
ments following â€˜##â€™), which are required by Ariadne to run
and type-check tensor shapes. ShapeFlow requires no such

2

API bugsShape incompatabilityLogic bugsProcessing bugs05101520253035404550Occurence (in percentage)Occurence of different kinds of bugsShapeFlow: Dynamic Shape Interpreter for TensorFlow

, ,

code annotations or modification. Since all APIs in Shape-
Flow only hold shape information, i.e., consume shape of
a tensor as input and emit the shape of the output, the op-
erations are very lightweight when compared to their Ten-
sorFlow counterparts. In Figure 2, there is a shape incom-
patibility bug on line 27. This bug triggers after training of
the model and results in a program crash. The programmer
has interchanged the images and their labels ( batch[0]
and batch[1] respectively) which have different shapes,
causing a program crash. For 100 training epochs, Shape-
Flow takes 4.2s to reach the error, while TensorFlow takes
43s to reach it (a 10X speed-up). Ariadne was only evaluated
on verifying correct code written in TensorFlow; thus, its
exact behavior on buggy code is unclear. For Ariadne, the
programmer would also need to recompute the expected
shapes and modify the comments if the shape of the input
data has changed because Ariadne is a static analysis tool.

Figure 3 shows the original TensorFlow code for the max-
pooling operation (a common operation used in deep learn-
ing for computer vision). Figure 4 represents the shape-
abstracted version of this operation. In the abstracted version,
ShapeFlow obtains the input shape (lines 4-12), modifies
it according to the other max-pooling parameters (kernel
size, strides, padding) and returns the output shape as an
object of type SF_Operation . On the other hand, the original
TensorFlow API code, though seemingly smaller, involves a
function call that is very deeply nested and requires much
more computation. This reveals the lightweight implementa-
tion of ShapeFlow in comparison to TensorFlow. We believe
that using ShapeFlow as a pre-processing step for probing
shape incompatibility errors will help programmers gain
confidence over their code before extensive training or final
deployment of the model.

3 Approach and Realization
This section gives a detailed discussion of how ShapeFlow
abstracts shape information for different APIs and how it
handles the entire computation.

3.1 Abstract Domain in ShapeFlow

This subsection discusses how ShapeFlow abstracts a sin-
gle operation in TensorFlow code. The shapes ShapeFlowâ€™s
APIs encounters can be broadly categorized into the un-
known, partly known, or fully known categories, however,
ShapeFlow does not operate on a lattice of these three cat-
egories. ShapeFlow precisely tracks the tensor shapes if
it is completely known or partial shape in case of partly
known shapes. Figure 5 shows the abstract domain (which is
a lattice) that ShapeFlow uses, and how each of the tensor
shapes that it tracks can fall into one of the three categories.
Note that the lattice in Figure 5 is not used for joins because
ShapeFlow only performs dynamic analysis. Each opera-
tion processes its input tensors shapes to generate the output

Table 1. Transfer function for the Reshape operation in
ShapeFlow.

Reshape Operation

r
o
s
n
e
T

Known
Partly known
Unknown
âŠ¥

Desired shape
Known Partly known Unknown âŠ¥
âŠ¥
Known
âŠ¥
Known Partly known
âŠ¥
Known Partly known
âŠ¥

âŠ¥
âŠ¥
âŠ¥
âŠ¥

Known

âŠ¥

âŠ¥

Table 2. Transfer function for the Reduce Mean operation
in ShapeFlow.

Reduce Mean
Operation

Input Tensor1
Known Partly known Unknown âŠ¥
âŠ¥

1

1

1

tensor shape, and it is characterized by its specific transfer
function. For instance, Table 1 shows the transfer function
used by an API named â€œreshapeâ€, which is intuitively named
and is used to change the shape of input tensors. It takes as
input a tensor and the desired shape and returns the shape
of the output tensor (the colored cells of the table). A pro-
grammer can either fully (e.g. [3, 4, 6]) or partly specify (e.g.
[4, -1]) the desired shape. The presence of -1 in the desired
shape is treated as if the programmer only cares about the
dimensions other than the one specified by -1 (partly known
desired shape). The input tensor itself can be fully known,
partly known or be unknown. ShapeFlow tracks the re-
spective shape information. Reshape operation allows some
kind of inputs (pre-conditions) and returns the output tensor
shape if the operation is legal. For instance, an unknown
desired shape is an illegal input to reshape; therefore, the en-
tire column with â€œUnknownâ€ desired shape is âŠ¥ in Table 1. If
either the desired shape or input tensor is completely known,
the output shape is also completely known, and the precise
value of the output shape is returned as a list of integers.

Table 2 shows the transfer function for another operation
named â€œreduce_meanâ€. It takes as input a tensor and a di-
mension along which to average the values. For simplicity,
we consider the case when the programmer specifies no axis,
in which case a single integer is supposed to be returned by
TensorFlow, which is the average of the entire tensor. Since
ShapeFlow only tracks the shape of the tensors, irrespec-
tive of the input tensorâ€™s shape, it always returns one as the
output shape of an integer tensor that is 1. Note that, 1 is a
specific shape in the â€œknownâ€ category of shapes, but since
â€œreduce_meanâ€ returns a specific shape from this category,
we mention that in the transfer function (Table 2).

3

, ,

Sahil Verma and Zhendong Su

1 # Import MNIST data
2 from tensorflow . examples . tutorials . mnist . input_data import read_data_sets
3 mnist = read_data_sets (" data " , one_hot = False )
4 import tensorflow as tf
5 # Create the network architecture
6 def conv_net ( x_dict , n_classes , dropout ):
[ batch , n_classes of channel ]

## train :{ images : [ batch , y (28) * x (28) of channel ]}
## test :{ images : [ batch , y (28) * x (28) of channel ]}}

## ( x_dict : { images : [ batch , y (28) *x (28) of channel ]}) ->

7

8

9

10

11

12

13

14

15

## [ batch , y (28) , x (28) , 1 of channel ]
x = tf . reshape ( x_dict , shape =[ -1 , 28 , 28 , 1])
## [ batch , y (28) , x (28) , 1 of channel ]
conv1 = tf . layers . conv2d (x , 32 , 5, activation = tf . nn . relu )
## [ batch , y (14) , x (14) , 1 of channel ]
conv1 = tf . layers . max_pooling2d ( conv1 , 2, 2)
conv2 = tf . layers . conv2d ( conv1 , 64 , 3, activation = tf . nn . relu ) ## [ batch , y (14) , x (14) , 1 of channel ]
conv2 = tf . layers . max_pooling2d ( conv2 , 2, 2)
fc1 = tf . contrib . layers . flatten ( conv2 )
fc1 = tf . layers . dense ( fc1 , 1024)
fc1 = tf . layers . dropout ( fc1 , rate = dropout )
return tf . layers . dense ( fc1 , n_classes )

## [ batch , y (7) , x (7) , 1 of channel ]
## [ batch , y (7) * x (7) of channel ]
## [ batch , 1024 of channel ]
## [ batch , 1024 of channel ]
## [ batch , n_classes of channel ]

16
17 # Placeholders for images and their labels
18 x = tf . placeholder ( tf . float32 , shape =[ None , 784])
19 y = tf . placeholder ( tf . float32 , shape =[ None , 10])
20 # Train the Model
21 train = conv_net (x , 10 , dropout =0.25)
22 loss_op = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( logits = train , labels =y))
23 optimizer = tf . train . AdamOptimizer ( learning_rate =0.001)
24 batch = mnist . train . next_batch ( batch_size =128)
25 ...
26 # Evaluate the Model
27 print ( session . run ([ loss_op ], feed_dict ={ x: batch [1] , y: batch [0]}) )

## [ batch , 10 of channel ]

# BUG : x: batch [0] , y: batch [1]

## [ batch , y (28) * x (28) of channel ]
## [ batch , 10 of label ]

Figure 2. Code having a shape incompatibility (comments are shown for illustration, ShapeFlow does not require them).

1 def max_pool ( value , ksize , strides , padding ,

data_format =" NHWC " , name = None ):

2

3

4

5

6

7

8

9

10

with ops . name_scope ( name , " MaxPool " , [ value ]) as

name :

value = ops . convert_to_tensor ( value , name ="
input ")
return gen_nn_ops . max_pool (

value ,
ksize = ksize ,
strides = strides ,
padding = padding ,
data_format = data_format ,
name = name )

Figure 3. Original API for max pooling operation.

3.2 Computational Graph

In this subsection, we discuss how ShapeFlow handles a
set of operations that need to be executed in a particular
sequence.

The core building blocks in TensorFlow are the computa-
tional graphs. TensorFlow operates in a lazy paradigm â€” it
parses the code to first generate a graph which has opera-
tions or variables as nodes, and the input and output tensors

4

as directed edges in the graph, and the computation happens
when this graph is executed within a session. This graph
is termed as a computational graph in TensorFlow, and the
flow of data in this graph is the basic set of computations in
TensorFlow. The computational graph encodes the sequence
of operations in the Tensorflow code. ShapeFlow operates
on a computational graph which only contains shape infor-
mation for the input and output tensors for each operation,
which we call the shape computational graph. The structure
of the shape computational graph is a replica of the compu-
tational graph generated by TensorFlow for the same code
but contains no tensor data, only their shapes.

Figure 7 illustrates a computational graph for the program
in Figure 6, which computes the harmonic mean of the two
constants x and y. There are distinct steps in creating the
computational graph. We first create two constants, x and
y which have values 11 and 21, respectively. We also create
a constant z, which will be used in the calculation of the
harmonic mean. Next, we multiply x and y to compute their
product (denoted by prod), which is further multiplied with
z and the outcome is still stored in the variable prod. We also
add the two constants and store the result in the variable
sum. We then divide the two outcomes, prod and sum, to
return the harmonic mean, save in the variable harmonic,

ShapeFlow: Dynamic Shape Interpreter for TensorFlow

, ,

1 import tensorflow as tf
2 x = tf . constant (11 , name = 'x ')
3 y = tf . constant (21 , name = 'y ')
4 z = tf . constant (2 , name = 'z ')
5 prod = tf . multiply (x , y , name =" Mul1 ")
6 prod = tf . multiply (z , prod , name =" Mul2 ")
7 sum = tf . add (x , y , name = ' Add ')
8 harmonic = tf . divide ( prod , sum , name =" Div ")
9 print ( harmonic )

Figure 6. A simple tensorflow program to compute harmonic
mean of two constants, x and y.

1 def max_pool ( value , ksize , strides , padding ,

data_format =" NHWC " , name = None ):

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

# Currently implemented for " SAME " padding .
def abst_max_pool ( value , ksize , strides ):
if isinstance ( value , ops . SF_Operation ):

out_ = value
while ( not isinstance ( out_ , ops . Tensor )):

out_ = out_ . fwd_func (* out_ . input_nodes )

input_shape = out_ . shape

elif isinstance ( value , ops . Tensor ):

input_shape = value . shape

else :

raise NotImplementedError

a = ( input_shape [1] - ksize [1]) / strides [1]+1
b = ( input_shape [2] - ksize [2]) / strides [2]+1
output_shape = [ input_shape [0] , a , b ,
input_shape [3]]
return ops . Tensor ( output_shape )

return ops . SF_Operation ([ value , ksize , strides ],

ffnc = abst_max_pool , name =" max_pool ")

Figure 4. Modified API for max pooling operation.

Figure 7. Computational graph of the code to compute har-
monic mean of two constants (shown in Figure 6) with de-
piction of its data flow.

â€¢ Placeholders: Placeholders can be viewed as the empty
nodes in the graph. They represent the empty nodes
for data, for example, for training images, that can be
instantiated when the graph is to be executed within
a session. During graph construction, placeholders
require only the shape and type of data that is to be
instantiated later.

â€¢ Variables: These are the changeable parameters in a

graph.

â€¢ Constants: These hold the non-changeable graph pa-

rameters.

â€¢ Operations: These are the nodes that perform com-
putations in the graph. For example, additions and
convolutions are operations.

A session is runtime execution when the placeholders
are populated with data, operations are executed, and the
tensors are evaluated in a pre-determined order.

Since ShapeFlow has modified the implementation of all
APIs (which are the nodes in a shape computational graph)
to only keep track of shape transformation, the input to
any operation node is a shape value, and the output is the
transformed shape value (which becomes the input for the
child node). SF_Operation is a node in the shape compu-
tational graph, on which ShapeFlow operates. Each mod-
ified function (or API) must return an object of the type

Figure 5. ShapeFlowâ€™s abstract domain.

and finally print it. Note that this computational graph is
only for illustration purposes and does not contain tensors,
which Tensorflow computational graphs have.

A computational graph distinctly specifies the dependen-
cies across different operations, i.e., the data-flow operations.
Therefore, the computational graph is a directed acyclic
graph (DAG), as illustrated by Figure 7. Highly complex Ten-
sorFlow programs can have more than one computational
graph, which can be executed in isolation. When a compu-
tational graph representing the code has been constructed,
independent operations can be computed in parallel. Tensor-
Flow optimizes to distribute parallelizable operations on the
same and even across multiple machines.

A typical computational graph consists of several parts:

5

âŠ¥âŠ¤   1, ?   2, ?   3, ?   4, ?   3, 2   3, 5  3, 5, 8â€¦.                                                  â€¦. â€¦.                                      â€¦.Unknown shapePartially known shapesFully known shapesImpossible shapePrint  11  21  Add  Mul1   Div    2xxyyz  Mul2prodprodsumharmonicConstantsTensorsOperations, ,

Sahil Verma and Zhendong Su

SF_Operation, and this is added to the shape computational
graph at the necessary position, i.e., directed from its in-
put vertex (or vertices) and directed to its output vertex (or
vertices). There are some APIs, e.g., nn.relu and nn.dropout,
which cause no change to the tensor shape; they return the
input shape without any computation, one of the reasons
that make ShapeFlow significantly faster than TensorFlow.
The operations in a computational graph must be executed
in an order such that all parents of a node must be evaluated
before the child node can be evaluated. This is ensured by
topologically sorting the nodes in a computational graph.
When a session.run() function is called, ShapeFlow per-
forms a topological sorting of the shape computational graph.
The session.run() requires to be called with at least two
parameters: an operation (which usually is the last node
in a computational graph) and a dictionary which contains
concrete values (data) for the placeholders.

Another noticeable difference from TensorFlow is the ab-
sence of a backward pass. TensorFlow implements a forward
and a backward pass for each of its APIs (which feature
as vertices in the computational graph). This is necessary
for optimizations using Stochastic Gradient Descent or any
other optimization algorithms. For reaching an optimal point,
the computational graph needs to be executed many times
(depending on the problem, this can be as high as a million
times). Since ShapeFlow does not need to iterate over the
shape computational graph multiple times, it does not have
a backward pass method defined for the modified APIs.

3.3 ShapeFlow Implementation

We have built ShapeFlow inside TensorFlow [8]. We de-
cide to follow the TensorFlow skeleton because it carries the
benefits of being developed by experienced researchers/engi-
neers, and shaped by extensive usage and feedback from the
large machine learning community. By using this strategy,
we avoid not only the development of another interpreter
(ShapeFlow depends on Python for interpretation), but also
the creation of a deeply nested API chain which is shipped
by TensorFlow. The problem with a nested API is that a pro-
grammer can call the same convolution API by either calling
tensorflow.nn.conv2d or nn.conv2d, or simply conv2d,
depending on how deep TensorFlow imports are used pre-
viously in the code. Had we built our own interpreter and
analyzer, we would be required to follow the exact same nest-
ing as TensorFlow to address this problem. We have avoided
all these redundancies by constructing ShapeFlow inside
TensorFlow. There are disadvantages associated with this
choice â€” we had to add some glue code to several APIs in or-
der to support vital parts for TensorFlow, but the advantages
outweigh.

Although in principle any TensorFlow API which con-
sumers and emits a tensor can be modified for shape abstrac-
tion, in total we modified 118 APIs in TensorFlow. These
were the APIs used by the benchmark suite. Out of these 118

APIs, 60 APIs were programmer-exposed APIs (i.e., APIs that
programmer use in their code) and the rest 58 were internal
to the functioning of TensorFlow, but required to be modified
in order to build ShapeFlow. We will make ShapeFlow pub-
licly available on GitHub to benefit TensorFlow users and
follow-up research, and allow other researchers/developers
to contribute and modify the remaining APIs in order to
make ShapeFlow useful for all deep learning code written
in TensorFlow.

4 Evaluation
This section details our evaluation of ShapeFlow. In par-
ticular, we evaluate ShapeFlow to answer the following
research questions:

1. RQ1: How does ShapeFlow compare to prior work

in terms of the accuracy of analysis?

2. RQ2: Is ShapeFlow much faster than TensorFlow in

catching shape incompatibility errors?

3. RQ3: Is ShapeFlowâ€™ analysis effective and accurate?

For answering the research questions, we evaluate Shape-
Flow on the benchmarks released by Zhang et al. [56], and
also on the ones listed by Johirul et al. [26]. Zhang et al. [56]
collected 175 buggy TensorFlow programs, 88 programs from
Github, and 87 programs from StackOverFlow, respectively.
They categorized them into the following six principled root
causes:

1. Incorrect model parameter or structure
2. Unaligned Tensor or Data Flow/Data bugs (i.e., shape

incompatibility errors)

3. Confused computational model
4. API change
5. API misuse
6. Structural inefficiency

They open-sourced the collection of the buggy programs and
also added the fixed versions of the programs. We isolate
the programs with shape incompatibility errors (labeled as
Unaligned Tensor) from this collection.

Johirul et al. [26] collected 970 buggy programs written
in five deep learning libraries. Out of those, 266 programs
were in TensorFlow, 100 programs from GitHub, and 166
programs from StackOverflow. They categorized the buggy
programs based on their bug types, root causes of the bugs,
and their effects. The categorization of bugs was finer in Jo-
hirul et al.â€™s collection, and hence we combine the programs
labeled with Data Bugs and Data Flow bugs from their suite;
they had shape incompatibility errors. Although they did not
provide the code for their programs, they provided the list of
the StackOverFlow posts, and GitHub repositories examined.
For the StackOverflow posts, we used both the buggy and the
corresponding correct versions of the programs and will re-
lease them with our implementation of ShapeFlow. Johirul

6

ShapeFlow: Dynamic Shape Interpreter for TensorFlow

, ,

Table 3. Benchmark statistics.

Prior Work

Source

# programs Avg. LOC

Zhang et al. [56]
Zhang et al. [56]
Johirul et al. [26]

StackOverFlow
Github
StackOverFlow

26
6
20

42
1,304
36

et al.â€™s collection of GitHub programs had no shape incom-
patibility error bugs for the TensorFlow library. In total, we
use 20 StackOverFlow programs from their collection.

In total, we evaluate ShapeFlow on 52 programs, con-
taining a mix of buggy and correct versions of benchmarks
from the bug collection. For Zhang et al.â€™s collection, the
average lines of code in the StackOverFlow programs is 42
lines, whereas, for the GitHub programs, it is 1,304 lines.
For Johirul et al.â€™s collection, the average lines of code for
the StackOverFlow programs is 36, which is close to the
average lines of code in the StackOverFlow benchmarks col-
lected by Zhang et al. Their GitHub bug collection records
no programs having shape incompatibility errors. Table 3
summarizes these statistics.

4.1 Accuracy of the analysis

To compare ShapeFlowâ€™s analysis to the previous static anal-
ysis techniques, we calculate the number of bugs detected
by Ariadne and ShapeFlow over the set of benchmark pro-
grams. Since Ariadne is not open-sourced, we could only
compare with it on the set of common benchmarks. Ariadne
were evaluated on 14 programs which were also selected
from Zhang et al.â€™s collection of StackOverFlow programs.
Table 4 shows the analysis results for Ariadne and Shape-
Flow on these 14 benchmarks. Ariadne due to its limited
support for tensor modifying APIs is not able to report bugs
in any of the 14 benchmarks. Pythia was also evaluated on
the same set of 14 programs and it finds bugs in 11 out of
them. ShapeFlow sucessfully finds bugs in 13 out of 14 pro-
grams. The results for Ariadne and Pythia were taken from
Table 2 in [30]. Therefore, ShapeFlow outperforms Ariadne
by 93% and Pythia by 14% in terms of the ability to detect
bugs.

4.2 Experimental Setup and Timing Results
To answer RQ2, we compare the shape incompatibility error
detection time for buggy programs (or no error detected for
correct programs) with the original TensorFlow. For a fair
and direct comparison, we compiled TensorFlow and Shape-
Flow from the source using the same configuration. We also
carefully maintained the same versions for TensorFlow and
ShapeFlow (TensorFlow version in which ShapeFlow was
built into). We used TensorFlow version 1.8.0.

For a realistic comparison, we measure ShapeFlowâ€™s per-
formance with two baselines. In deep learning, practitioners

Table 4. Comparsion of the capability of catching bugs .
ShapeFlow detects bugs in more programs than both the
other tools.

Benchmarks Ariadne [16]

UT-1 buggy
UT-2 buggy
UT-3 buggy
UT-4 buggy
UT-5 buggy
UT-6 buggy
UT-7 buggy
UT-8 buggy
UT-9 buggy
UT-10 buggy
UT-11 buggy
UT-12 buggy
UT-13 buggy
UT-15 buggy

âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—

ShapeFlow
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“

Table 5. Number of data points in the original and the
dummy version of the three datasets our benchmarks used.

Dataset

Data points
in original dataset

Data points
in dummy dataset

MNIST
CelebA
Boston housing

60,000
202,599
506

2
5
4

often create a smaller dataset to test parts of their code. After
they are satisfied with results, the code is used on the full
dataset for training a model. The two baselines that we con-
sider differ in the size of the dataset, which needs to be pre-
processed and loaded before the computational graph (shape
computational graph in case of ShapeFlow) is executed. In
the first baseline, we measure the gain obtained by using
ShapeFlow over TensorFlow for the case where the pro-
grammer directly executes the code over the full dataset. For
the second baseline, we assume the programmer has created
a dummy dataset for testing the code. We manually created
these dummy datasets for the original datasets required by
all our benchmarks, which were MNIST [31], CelebA [32],
and the Boston housing data [1]. Table 5 records the number
of data points in the original and dummy versions of these
three datasets.

To measure time, we use time module [43] of Python
because it has a much higher precision that the Unix time
utility [45]. In order to amortize runtime variability, all the
programs are executed five times. We record the average and
medians (in parentheses) of the runtimes. We report timings
(in seconds) up to 6 digits after the decimal place.

7

, ,

Sahil Verma and Zhendong Su

Table 6. ShapeFlow1 and TensorFlow1 columns respectively record the time taken by ShapeFlow and TensorFlow when run
on the full dataset, and Gain1 column records the speed-up in this case. Similarly, ShapeFlow2 and TensorFlow2 columns record
the respective runtimes when run on a subset of the full dataset, and Gain2 column records the speed-up obtained in this case.

(a) Speed-up obtained by using ShapeFlow for the StackOverFlow benchmarks collected by Zhang et. al. [56].

Benchmarks

UT-1 buggy
UT-1 fix
UT-2 buggy
UT-2 fix
UT-3 buggy
UT-3 fix
UT-4 fix
UT-4 buggy
UT-5 fix
UT-5 buggy
UT-6 buggy
UT-6 fix
UT-8 buggy
UT-8 fix
UT-9 fix
UT-9 buggy
UT-10 buggy
UT-10 fix
UT-11 buggy
UT-11 fix
UT-12 buggy
UT-12 fix
UT-13 fix
UT-13 buggy
UT-15 fix
UT-15 buggy

Overall Speedup

ShapeFlow1
runtime (s)

TensorFlow1
runtime (s)

Gain1

ShapeFlow2
runtime (s)

TensorFlow2
runtime (s)

Gain2

0.004826 (0.004555)
0.011045 (0.011301)
0.016893 (0.018482)
0.016432 (0.016418)
0.003249 (0.003239)
0.062063 (0.062306)
0.052681 (0.052392)
0.016712 (0.018654)
0.013685 (0.012162)
0.010792 (0.011070)
0.016913 (0.016393)
0.016028 (0.014938)
0.004550 (0.004782)
0.014360 (0.014119)
0.018558 (0.018542)
0.019071 (0.017793)
0.476457 (0.473398)
0.722202 (0.723635)
0.082495 (0.084547)
0.053065 (0.057801)
0.021083 (0.019338)
0.012945 (0.013822)
0.017249 (0.016833)
0.014732 (0.016925)
0.015963 (0.014282)
0.016604 (0.015436)

0.505632 (0.507254)
2.235000 (2.236960)
0.020635 (0.020435)
0.028978 (0.028090)
0.005512 (0.005563)
0.056764 (0.056520)
0.551365 (0.550870
0.298007 (0.297025)
2.156230 (2.164300)
1.546890 (1.547420)
0.029426 (0.029343)
0.181201 (0.182266)
0.011710 (0.011202)
0.234234 (0.099028)
0.207446 (0.142824)
0.210779 (0.142963)
0.535116 (0.529867)
24.845400 (24.83100)
0.090225 (0.090912)
0.067534 (0.069481)
0.416095 (0.333569)
0.356951 (0.355827)
0.311994 (0.170065)
0.173761 (0.173950)
0.134329 (0.133624)
0.134649 (0.135490)

0.012453 (0.012306)
0.015355 (0.014678)
0.020888 (0.023390)
0.015112 (0.015101)
0.003109 (0.003117)
0.061828 (0.062009)
0.050347 (0.050485)
0.016412 (0.018195)
0.020843 (0.020723)
0.016171 (0.015626)
0.020511 (0.023374)
0.013293 (0.014880)
0.004267 (0.004791)
0.017531 (0.016500)
0.016070 (0.014876)
0.020065 (0.019071)
0.016264 (0.017632)
0.017035 (0.015083)
0.087516 (0.088000)
0.055528 (0.054685)
0.166605 (0.168636)
0.162853 (0.162241)
0.019410 (0.020330)
0.016721 (0.014648)
0.015457 (0.016975)
0.014803 (0.014519)

0.210542 (0.210695)
0.289705 (0.288663)
0.021937 (0.021691)
0.028099 (0.028331)
0.004920 (0.005415)
0.057082 (0.057010)
0.550860 (0.551190)
0.297931 (0.298694)
1.799380 (1.798460)
1.268940 (1.264490)
0.026882 (0.027900)
0.182537 (0.182204)
0.010029 (0.010495)
0.098577 (0.098470)
0.138113 (0.139519)
0.142899 (0.142407)
0.176798 (0.098980)
0.231835 (0.156492)
0.092765 (0.092045)
0.064511 (0.064389)
0.646697 (0.478289)
1.116350 (1.051680)
0.264122 (0.173946)
0.246741 (0.170383)
0.266368 (0.135218)
0.135418 (0.134586)

105
202
1
2
2
1
10
18
158
143
2
11
3
16
11
11
1
34
1
1
20
28
18
12
8
8

32

17
19
1
2
2
1
11
18
86
78
1
14
2
6
9
7
11
14
1
1
4
7
14
15
17
9

14

(b) Speed-up obtained by using ShapeFlow for the Github benchmarks collected by Zhang et. al. [56].

ShapeFlow1
runtime (s)

TensorFlow1
runtime (s)

Gain1

ShapeFlow2
runtime (s)

TensorFlow2
runtime (s)

Gain2

0.013490 (0.013551)
0.007925 (0.007502)
0.029057 (0.029233)
0.022813 (0.023250)
0.811806 (0.795560)
554.639000 (549.1520)

202
0.019120 (0.018631)
2.72417 (2.41282)
259
0.014209 (0.014493)
2.04975 (2.02793)
176
0.038649 (0.036645)
5.10912 (5.03372)
536.835 (535.343) 23,532 0.015904 (0.016015)
4
0.085000 (0.086116)
3.03758 (3.05285)
N/A
0.066380 (0.066954)
Timeout

0.256937 (0.255222)
0.238498 (0.236754)
2.443070 (2.443690)
0.627817 (0.631867)
2.415270 (2.406110)
2.471560 (2.465310)

13
17
63
39
28
37

33

Overall Speedup

4,835

Benchmarks

UT-1 buggy
UT-1 fix
UT-7 buggy
UT-7 fix
UT-9 buggy
UT-9 fix

We ran all the benchmarks for only one epoch. Our bench-
marks do not have program paths that are executed only
when ð‘’ð‘ð‘œð‘â„Ž > 1. Also, since our benchmarks contain sev-
eral correct programs, if we run the code for more than one
epoch, it would unfairly bias the results against TensorFlow.

Note that we run for all mini-batches of training data in
the first (and only) epoch, the reason for which is discussed
next. Figure 8 shows a program that throws an error only
from the second mini-batch onwards. Lines 2 and 3 are the
shapes of the variables input and store. After performing

8

ShapeFlow: Dynamic Shape Interpreter for TensorFlow

, ,

1 batches = 5
2 input_shape = [3 , 6]
3 initial_store_shape = [6 , 4]
4 for mini_batches in range ( batches ):

5

6

store = input * store

# ( matrix multiply ,

resulting shape = [3 , 4])

store = store .T

# ( transpose , resulting

shape = [4 , 3])

Figure 8. Example of a TensorFlow program which throws
an error only after first mini-batch.

1 import tensorflow as tf , numpy as np

2
3 x = tf . placeholder ( tf . float32 , [ None ])
4 x. set_shape ([1028178])
5 y = tf . identity ( x)
6 y . set_shape ([478 , 717 , 3]) # Bug : Can 't reset shape
7 X = np . random . normal (0 , 0.1 , 1028178)

8
9 sess = tf . Session ()
10 Y = sess . run (y , feed_dict ={ x: X })

Figure 9. Example of a TensorFlow program with small
computational graph.

the matrix multiplication (line 5), the result is stored in store
after its transposition. The error occurs on line 5 in the sec-
ond minibatch when it attempts to multiply two matrices
with incompatible shapes. It is not difficult to conceive a
program that can give error on the last minibatch. This was
the reason for choosing an experimental design that takes
all-mini-batches in the first epoch.

We adhere to the following taxonomy in Table 6a, Table 6b
and Table 7: columns labelled as ShapeFlow1 and Tensor-
Flow1 record the time taken by ShapeFlow and TensorFlow
respectively for the first baseline. Values in column Gain1
are obtained by dividing respective rows in ShapeFlow1 and
TensorFlow1 and shows the speed-up ShapeFlow achieves.
Similarly, ShapeFlow2 and TensorFlow2 columns record the
time taken by ShapeFlow and TensorFlow respectively for
the second baseline. Gain2 registers the speed-up procured
for this baseline.

Table 6a presents the timing and speed-up results for the
StackOverFlow benchmarks collected by Zhang et al. The
first three columns deal with the first baseline, and the last
three columns deal with the second baseline. The overall av-
erage speed-up for the first baseline is 32X, and for the second
baseline is 14X. The relative variance for all the benchmarks
was smaller than 0.5%.

Table 6b presents results for Github programs from the
same bug collection by Zhang et al. Noticeably, one of the
benchmarks times-out (1 hour) for TensorFlow, but Shape-
Flow was able to analyze it. In this case, the overall average

9

speed-up for the first baseline is 4,835X, and for the second
baseline is 33X. The relative variance for all the benchmarks,
but UT-9 fix was smaller than 0.5%. This shows that larger
programs (GitHub programs were much larger than Stack-
OverFlow programs on average) are expected to gain more
from using ShapeFlow. The UT-9 benchmark uses CelebA
dataset which consists of more than 200K images. Therefore,
even ShapeFlow takes significant time (9-10 minutes) to
analyse the fixed version of the program, but TensorFlow
times-out after a long wait.

This also explains the reason behind several rows in Ta-
ble 6a where the gain by using ShapeFlow is insignificant
(Gain1 and Gain2 values equalling 1 or 2). If the compu-
tational graph is very small, the user does not gain much
by using ShapeFlow. Figure 9 shows such a program, in
which only two placeholders were created. There is a bug
on line 6, where the user unknowingly sets the shape of a
placeholder, which has already been set on line 4. Shape-
Flow and TensorFlow take almost equal time to reach the
error.

Table 7 presents the timing and speed-up results for the
programs collected from Johirul et al. [26]. The column
nomenclature remains unchanged. For these programs, the
overall average speed-up for the first baseline is 23X, and
for the second baseline is 34X. The relative variance for all
the benchmarks was smaller than 0.5%. We note that these
gains are similar to the ones shown in Table 6a, and this cor-
responds well with the fact that they have similar program
length.

Overall, across the 52 benchmarks, ShapeFlowâ€™s average
speed-up for the first baseline is 499X, and for the second
baseline is 24X. Therefore, for correct programs the overhead
of running ShapeFlow is about 0.2% in the case of first
baseline and 4% in the case of second baseline. On the other
hand, for buggy programs ShapeFlow is able to catch bugs
within this highly reduced time frame, much quicker than
TensorFlow.

Our purpose to include the correct programs in our suite
of benchmarks is two-fold. First, it shows that if ShapeFlow
is used on correct programs (as the userâ€™s code may be cor-
rect and not have shape incompatibility errors), it is still
fast and offers drastically reduced runtime over the vanilla
TensorFlow. Second, by including the fixed code, we show
that ShapeFlow is not only fast, but also accurate and does
not raise false alarms on the fixed code.

4.3 Correctness of Implementation
To answer RQ3, we examined the number of false positives
(correct program being labeled as buggy) and false nega-
tives (buggy programs flagged as having no errors). From
the benchmarks that we collected, ShapeFlow generated
one false negative. We have removed the runtime perfor-
mance for that program from our experimental results. Upon
further investigation, we found that the bug being thrown

, ,

Sahil Verma and Zhendong Su

Table 7. Speed-up gained for programs taken from the StackOverFlow posts collected by Johirul et al. [26]. ShapeFlow1 and
TensorFlow1 columns respectively record the time taken by ShapeFlow and TensorFlow when run on the full dataset, and
Gain1 records the speed-up in this case. ShapeFlow2, TensorFlow2, and Gain2 record the respective runtimes and speed-up
when run on a subset of the full dataset.

Benchmarks

ShapeFlow1
runtime (s)

TensorFlow1
runtime (s)

Gain1

ShapeFlow2
runtime (s)

TensorFlow2
runtime (s)

Gain2

340892850 buggy
340892850 correct
38399609 buggy
38399609 correct
37444951 buggy
37444951 correct
39032277 buggy
39032277 correct
35488717 buggy
35488717 correct
36870792 buggy
36870792 correct
40430186 correct
40430186 buggy
34908033 buggy
34908033 correct
38114534 buggy
38114534 correct
47308181 buggy
47308181 correct

Overall Speedup

0.004327 (0.004170)
0.000899 (0.000693)
0.067773 (0.068316)
0.058631 (0.058109)
0.004292 (0.004238)
0.000840 (0.000800)
0.022443 (0.021815)
0.018372 (0.017770)
0.022591 (0.023691)
0.016953 (0.016514)
0.003583 (0.003514)
0.000185 (0.000192)
0.060506 (0.059769)
0.061721 (0.064051)
0.003358 (0.003445)
0.016301 (0.016568)
0.019703 (0.017871)
0.016325 (0.016593)
0.001928 (0.001950)
0.000593 (0.000569)

0.032657 (0.030197)
0.039737 (0.039529)
0.341563 (0.341291)
0.336807 (0.338689)
0.064222 (0.060475)
0.070589 (0.074351)
0.892349 (0.892292)
0.894322 (0.887674)
0.032989 (0.032877)
0.025879 (0.025022)
0.030363 (0.029801)
0.028432 (0.029228)
0.385061 (0.229717)
0.219258 (0.215891)
0.025395 (0.027033)
0.272701 (0.115837)
0.030618 (0.030867)
0.022600 (0.025680)
0.002032 (0.002044)
0.000617 (0.000608)

8
44
5
6
15
84
40
49
1
2
8
154
6
4
8
17
2
1
1
1

23

0.003728 (0.003471)
0.000628 (0.000581)
0.008441 (0.008277)
0.002969 (0.002957)
0.003693 (0.003587)
0.000724 (0.000606)
0.020334 (0.020277)
0.018448 (0.018779)
0.019610 (0.019100)
0.016024 (0.016271)
0.003645 (0.003618)
0.000170 (0.000163)
0.066676 (0.067285)
0.057335 (0.057502)
0.003204 (0.003216)
0.015918 (0.016526)
0.018022 (0.016896)
0.016263 (0.016469)
0.001858 (0.001802)
0.000555 (0.000574)

0.031075 (0.031325)
0.042388 (0.041677)
0.341541 (0.340354)
0.335843 (0.335065)
0.062044 (0.061669)
0.074444 (0.072752)
0.882298 (0.879868)
0.887020 (0.884134)
0.033474 (0.033891)
0.028250 (0.025550)
0.029981 (0.029758)
0.030539 (0.030009)
0.284475 (0.232514)
0.220841 (0.218171)
0.026353 (0.025084)
0.264848 (0.114233)
0.024843 (0.025435)
0.025113 (0.023905)
0.002031 (0.001887)
0.000572 (0.000532)

8
68
40
113
17
103
43
48
2
2
8
180
4
4
8
17
1
2
1
1

34

by TensorFlow was, in fact, an API misuse flag, rather than a
shape incompatibility error, but to be consistent with respect
to the classification used by Zhang et al., we consider this
program a false negative for ShapeFlow.1.

4.4 Threats to validity

We show the accuracy of the analysis of ShapeFlow and its
timing performance on an extensive set of benchmarks. To
eliminate bias, the benchmarks we selected were collected
from other studies. Nevertheless, ShapeFlow can be evalu-
ated on yet larger and challenging benchmarks, which makes
interesting future work.

5 Related Work
Bug detection in machine learning code

Although there have been attempts in improving visualiza-
tion of computational graphs in machine learning, to help in-
terpretability and debugging of models [11, 25, 50], there has
not been much focus on automated or faster deep-learning
code analysis.

1The program label is UT-7 in the bug collection of [56]

10

Ariadne [16] is a static analysis tool for machine learning
code written in TensorFlow. It uses the existing program
analysis framework WALA [44] for analyzing deep learning
code. It converts Python code (the language in which most
deep learning code is written) into WALAâ€™s Internal Repre-
sentation (IR) for analysis. The programmer must annotate
each line of the code in which any tensor operation takes
place to indicate the expected output shape after the exe-
cution of that operation. Once the programmer annotates
the code with information about the shape of the input data,
Ariadne uses dataflow analysis in a tensor tracking type sys-
tem to verify whether the expected shapes are satisfied at
each annotated line. Ariadne was only evaluated on whether
it verified correct code written in TensorFlow, so we are
unaware of how it behaves with buggy code. Given its pre-
liminary implementation (4 APIs only), programmers cannot
use Ariadne for most nontrivial TensorFlow code. We have
discussed earlier (Section 1) how ShapeFlow and Ariadne
differ. Pythia, the recent extension of Ariadne is also a static
analysis tool and therefore had to develop a whole Python
front-end (parser, IR generator) that translates the Python
source code into the IR of the WALA framework [44], a gen-
erator of relational tables for declarative program analysis in

ShapeFlow: Dynamic Shape Interpreter for TensorFlow

, ,

the Doop framework [9], and a points-to, constant-flow and
call-graph analysis for Python. Pythia is a concurrent work
to ours and we show that ShapeFlow has a more accurate
analysis compared to even Pythia.

There has been work in typesafe abstraction for tensor
using Scala [13]. Authors argue that this could form the basis
of future typesafe deep-learning frameworks that are built
on Scala. Language incompatibility would not allow this to
help the problem ShapeFlow addresses.

There is work that implements deep-learning framework
in a typesafe language like Java [6] and Haskell [7]; however,
they are in preliminary stages of development and have yet
to gain popularity.

Empirical studies on machine learning bugs

Zhang et al. [56] collected 175 TensorFlow bugs, from Stack-
Overflow and GitHub. They categorized them by their root
causes and effects, and common bug fixing patterns. They
discussed the problems faced by deep-learning programmers,
such as bug localization. Johirul et al. [26] investigated 2,716
posts from StackOverFlow, and 500 commits from GitHub
to isolate bugs in five deep-learning libraries: TensorFlow,
Pytorch, Keras, Caffe, and Theano. This work also catego-
rized the bugs based on their root causes and their impacts
along with giving insights into their frequency. They also
discuss several anti-patterns in bugs commonly found in the
deep-learning code.

Machine learning testing

Extensive work exists on testing the otherwise â€œnon-testable"
domain of machine learning programs. When conducting
ML testing, all its tightly intertwined components, includ-
ing data, learning program (SVM, neural networks, etc.) and
framework (TensorFlow, Pytorch, etc.), need to be examined
for bugs. Breck et al. [10] proposed a validation algorithm
for incoming data. The system is used on a large scale at
Google. Krishnan et al. proposed BoostClean [29], which
can be used to detect violations in domain values in training
data automatically. Pham et al. [42] proposed Cradle for test-
ing machine learning frameworks/libraries. Xiao et al. [51]
worked on security vulnerabilities in popular deep learning
frameworks like TensorFlow, Caffe, and Pytorch.

Metamorphic testing uses multiple implementations of
semantically equivalent programs and compares their out-
put on different inputs. Murphy et al. [38] proposed the
first few metamorphic relations applicable to the machine
learning domain. Since then, they have been adapted to be
applied to various problems. Xie et al. [52] proposed spe-
cific metamorphic relations for testing supervised classifiers,
which were able to reveal 39 out of the 43 injected bugs in
the Weka [24] implementations of those classifiers. Zhou
et al. [57] proposed MT4MT, which used metamorphic re-
lations to evaluate the translation consistency of machine
translation systems.

11

Other software testing inspired techniques being applied
to the machine learning domain include mutation testing [35],
combinatorial testing [33], and fuzzing [39].

There also is a focus on testing machine learning mod-
els for non-functional but desired properties, like robust-
ness, fairness, and interpretability. DeepFool [37] was pro-
posed by Moosavi-Dezfooli et al. to compute perturbations
that fool the deep neural networks to quantify their robust-
ness. There is widespread adoption of adversarial input gen-
eration [12, 28, 34, 55] to test the robustness of machine
learning systems for applications like autonomous driving.
DeepTest [46] was designed to test CNNs and RNNs. It used
a template with nine image transformations to generate real-
istic data. DeepRoad [55] used GANs to generate authentic
looking images for autonomous driving. Udacity Challenge
Dataset images were used as seed input. DeepHunter [53]
uses a metamorphic testing strategy to detect defects in neu-
ral networks. DeepStellar [18] uses adversarial techniques
for testing recurrent neural networks (RNNs). Dwarakanath
et al. [19, 20] developed metamorphic tests specifically for
testing image classifiers and deep learning forecasters. Deep-
Xplore [41] tests machine learning systems using a white-box
differential testing technique to generate test inputs. Inspired
by the criterion for line and condition coverage in traditional
software, they proposed neuron-coverage as a parallel in the
machine learning domain.

Research in fairness in machine learning focuses on dis-
covering, comprehending, and mitigating causes of bias in
models. There are multiple definitions of fairness in the liter-
ature with no firm consensus [49]. Galhotra et al. proposed
Themis [23], which uses a random strategy for generating
test cases for a biased learned model based on the definition
of individual fairness [21]. These test cases are termed as
discriminating. Aequitas [47] uses a directed testing strat-
egy for a faster and effective generation of discriminating
test cases. Doshi-Velez et al. [17] gave terminologies for the
interpretability of machine learning models. Chen et al. [14]
examined several methods for improving interpretability for
classifiers.

We refer the reader to a recent survey of the papers at the
intersection of software engineering and deep learning [22].

Fixing Machine Learning Bugs

Due to the unconfined (and sometimes undefined) nature of
the bugs in machine learning, fixing them is nontrivial. Data
resampling has emerged as one of the leading approaches
in fixing models. The generated test inputs from tools like
DeepXplore [41] and DeepTest [46] can be added to the train-
ing set to improve accuracy. DeepTest improved the modelâ€™s
accuracy by 46%. Ma et al. proposed MODE [36] as a tool for
identifying faulty neurons, neurons that are responsible for
misclassification, and for separating training data that influ-
enced such neurons. It helped improve model performance

, ,

Sahil Verma and Zhendong Su

by a significant margin. Vartak et al [48] proposed the MIST-
IQUE system, which stores and queries model intermediates
to help debug.

We refer the reader to Zhang et al.â€™s recent survey [54] for a
thorough discussion of the work in machine learning testing
and associated areas. All existing efforts surveyed above are
orthogonal to ShapeFlowâ€” none tackles the detection of
bugs in machine learning code, which is the focus of this
work.

6 Conclusion
We have designed and developed a dynamic shape abstract in-
terpreter for TensorFlow programs with the following goals:

1. The analysis should be light and fast;
2. The analysis should be accurate; and
3. The analysis should not cause programmer burden.
We have evaluated ShapeFlow on 52 benchmarks and
demonstrated its effectiveness in the detection of shape in-
compatibility errors, which should help save valuable devel-
oper time. Our current implementation modifies 118 APIs
out of the thousands in TensorFlow. We open-source Shape-
Flow to benefit both the user and research communities, and
to allow contributions from the wider open-source commu-
nity to refine ShapeFlow and support additional APIs and
functionalities. Interesting future work would include the
generation of useful error messages, localization of source
code generating shape incompatibility errors, and automatic
repair of such errors.

References
[1] 2019. Boston Housing Dataset. https://www.cs.toronto.edu/~delve/

data/boston/bostonDetail.html. Accessed: 2019-10-30.

[2] 2019. Deep Learning Framework Scores. https://towardsdatascience.
com/deep-learning-framework-power-scores-2018-23607ddf297a. Ac-
cessed: 2019-10-30.

[3] 2019. Growth of Deep Learning Framework. https://www.kdnuggets.
com/2019/05/which-deep-learning-framework-growing-fastest.
html. Accessed: 2019-10-30.

[4] 2019.

MNIST Tutorial for TensorFlow.

https://github.com/

aymericdamien/TensorFlow-Examples/blob/master/examples/3_
NeuralNetworks/convolutional_network.py. Accessed: 2019-10-30.
[5] 2019. Top 5 Deep Learning Frameworks for 2019. https://www.
springboard.com/blog/deep-learning-frameworks/. Accessed: 2019-
10-30.

[6] 2020. Deep Learning for Java. https://deeplearning4j.org. Accessed:

2020-2-28.

[7] 2020. Deep Learning in Haskell. https://github.com/HuwCampbell/

grenade. Accessed: 2020-2-28.

[8] Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,
Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry
Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016.
TensorFlow: A system for large-scale machine learning. In 12th USENIX
Symposium on Operating Systems Design and Implementation (OSDI
16). 265â€“283. https://www.usenix.org/system/files/conference/osdi16/
osdi16-abadi.pdf

[9] Martin Bravenboer and Yannis Smaragdakis. 2009. Strictly Declarative
Specification of Sophisticated Points-to Analyses. In Proceedings of
the 24th ACM SIGPLAN Conference on Object Oriented Programming
Systems Languages and Applications (OOPSLA â€™09). Association for
Computing Machinery, New York, NY, USA, 243â€“262. https://doi.org/
10.1145/1640089.1640108

[10] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Whang, and
Sudip Roy. 2019. Data Validation for Machine Learning. In Proceedings
of SysML.

[11] Shanqing Cai, Eric Breck, Eric Nielsen, Michael Salib, and D. Sculley.
2016. TensorFlow Debugger: Debugging Dataflow Graphs for Machine
Learning.

[12] Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu, and
Yew Soon Ong. 2018. Metamorphic Relation Based Adversarial At-
tacks on Differentiable Neural Computer. CoRR abs/1809.02444 (2018).
arXiv:1809.02444 http://arxiv.org/abs/1809.02444

[13] Tongfei Chen. 2017. Typesafe Abstractions for Tensor Operations.
CoRR abs/1710.06892 (2017). arXiv:1710.06892 http://arxiv.org/abs/
1710.06892

[14] Weijie Chen, Berkman Sahiner, Frank W. Samuelson, Aria Pezeshk,
and Nicholas Petrick. 2018. Calibration of medical diagnostic classifier
scores to the probability of disease. In Statistical methods in medical
research.

[15] FranÃ§ois Chollet. 2015. Keras. https://github.com/fchollet/keras.
[16] Julian Dolby, Avraham Shinnar, Allison Allain, and Jenna Reinen. 2018.
Ariadne: Analysis for Machine Learning Programs. In Proceedings of
the 2Nd ACM SIGPLAN International Workshop on Machine Learning
and Programming Languages (MAPL 2018). ACM, New York, NY, USA,
1â€“10. https://doi.org/10.1145/3211346.3211349

[17] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science

of Interpretable Machine Learning.

[18] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao.
2019. DeepStellar: Model-Based Quantitative Analysis of Stateful Deep
Learning Systems. In Proceedings of the 2019 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE 2019). Association for
Computing Machinery, New York, NY, USA, 477â€“487. https://doi.org/
10.1145/3338906.3338954

[19] Anurag Dwarakanath, Manish Ahuja, Sanjay Podder, Silja Vinu, Ar-
ijit Naskar, and Koushik MV. 2019. Metamorphic Testing of a Deep
Learning Based Forecaster. In Proceedings of the 4th International Work-
shop on Metamorphic Testing (MET â€™19). IEEE Press, 40â€“47. https:
//doi.org/10.1109/MET.2019.00014

[20] Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M.
Rao, R. P. Jagadeesh Chandra Bose, Neville Dubash, and Sanjay Podder.
2018. Identifying Implementation Bugs in Machine Learning based
Image Classifiers using Metamorphic Testing. CoRR abs/1808.05353
(2018).

[21] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and
Richard S. Zemel. 2011. Fairness Through Awareness. CoRR (2011).
http://arxiv.org/abs/1104.3913

[22] Fabio Ferreira, Luciana Lourdes Silva, and Marco Tulio Valente. 2019.
Software Engineering Meets Deep Learning: A Literature Review.
arXiv:cs.SE/1909.11436

[23] Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness
Testing: Testing Software for Discrimination. In Proceedings of the 2017
11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE
2017). ACM, New York, NY, USA, 498â€“510. https://doi.org/10.1145/
3106237.3106277

[24] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Pe-
ter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining
Software: An Update. SIGKDD Explor. Newsl. 11, 1 (Nov. 2009), 10â€“18.
https://doi.org/10.1145/1656274.1656278

[25] Fred Hohman, Minsuk Kahng, Robert Pienta, and Duen Horng Chau.
2018. Visual Analytics in Deep Learning: An Interrogative Survey for

12

ShapeFlow: Dynamic Shape Interpreter for TensorFlow

, ,

the Next Frontiers. IEEE Transactions on Visualization and Computer
Graphics 25 (2018), 2674â€“2693.

[26] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan.
2019. A Comprehensive Study on Deep Learning Bug Characteristics.
In Proceedings of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE 2019). ACM, New York, NY, USA, 510â€“520.
https://doi.org/10.1145/3338906.3338955

[27] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan
Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014.
Caffe: Convolutional Architecture for Fast Feature Embedding. In
Proceedings of the 22Nd ACM International Conference on Multimedia
(MM â€™14). ACM, New York, NY, USA, 675â€“678. https://doi.org/10.1145/
2647868.2654889

[28] Jinhan Kim, Robert Feldt, and Shin Yoo. 2018. Guiding Deep Learning
System Testing using Surprise Adequacy. CoRR abs/1808.08444 (2018).
arXiv:1808.08444 http://arxiv.org/abs/1808.08444

[29] Sanjay Krishnan, Michael J. Franklin, Ken Goldberg, and Eugene Wu.
2017. BoostClean: Automated Error Detection and Repair for Machine
Learning. CoRR abs/1711.01299 (2017). arXiv:1711.01299 http://arxiv.
org/abs/1711.01299

[30] Sifis Lagouvardos, Julian Dolby, Neville Grech, Anastasios Antoniadis,
and Yannis Smaragdakis. 2020. Static Analysis of Shape in TensorFlow
Programs. In 34th European Conference on Object-Oriented Program-
ming (ECOOP 2020) (Leibniz International Proceedings in Informatics
(LIPIcs)). Schloss Dagstuhlâ€“Leibniz-Zentrum fuer Informatik, Dagstuhl,
Germany, 30. https://doi.org/10.4230/LIPIcs.ECOOP.2020.15

[31] Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit
database. http://yann.lecun.com/exdb/mnist/. (2010). http://yann.
lecun.com/exdb/mnist/

[32] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep
Learning Face Attributes in the Wild. In Proceedings of International
Conference on Computer Vision (ICCV).

[33] L. Ma, F. Juefei-Xu, M. Xue, B. Li, L. Li, Y. Liu, and J. Zhao. 2019. DeepCT:
Tomographic Combinatorial Testing for Deep Learning Systems. In
2019 IEEE 26th International Conference on Software Analysis, Evolution
and Reengineering (SANER). 614â€“618. https://doi.org/10.1109/SANER.
2019.8668044

[34] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li,
Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong
Wang. 2018. DeepGauge: Multi-granularity Testing Criteria for Deep
Learning Systems. In Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering (ASE 2018). ACM, New
York, NY, USA, 120â€“131. https://doi.org/10.1145/3238147.3238202
[35] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bao Qin Li, Felix Juefei-
Xu, Chao Xie, Li Li, Yang P. Liu, Jianjun Zhao, and Yadong Wang. 2018.
DeepMutation: Mutation Testing of Deep Learning Systems. 2018
IEEE 29th International Symposium on Software Reliability Engineering
(ISSRE) (2018), 100â€“111.

[36] Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth
Grama. 2018. MODE: Automated Neural Network Model Debugging
via State Differential Analysis and Input Selection. In Proceedings of
the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE 2018). ACM, New York, NY, USA, 175â€“186. https://doi.org/
10.1145/3236024.3236082

[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal
Frossard. 2015. DeepFool: a simple and accurate method to fool
deep neural networks. CoRR abs/1511.04599 (2015). arXiv:1511.04599
http://arxiv.org/abs/1511.04599

[38] Chris Murphy, Gail Kaiser, and Marta Arias. 2007. An Approach to

Software Testing of Machine Learning Applications.

[39] Augustus Odena, Catherine Olsson, David Andersen, and Ian Goodfel-
low. 2019. TensorFuzz: Debugging Neural Networks with Coverage-
Guided Fuzzing. In Proceedings of the 36th International Conference on

13

Machine Learning (Proceedings of Machine Learning Research), Kama-
lika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, Long
Beach, California, USA, 4901â€“4911. http://proceedings.mlr.press/v97/
odena19a.html

[40] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward
Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga,
and Adam Lerer. 2017. Automatic differentiation in PyTorch. In NIPS-
W.

[41] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deep-
Xplore: Automated Whitebox Testing of Deep Learning Systems. In
Proceedings of the 26th Symposium on Operating Systems Principles
(SOSP â€™17). ACM, New York, NY, USA, 1â€“18. https://doi.org/10.1145/
3132747.3132785

[42] Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan. 2019.
CRADLE: Cross-backend Validation to Detect and Localize Bugs in
Deep Learning Libraries. In Proceedings of the 41st International Con-
ference on Software Engineering (ICSE â€™19). IEEE Press, Piscataway, NJ,
USA, 1027â€“1038. https://doi.org/10.1109/ICSE.2019.00107

[43] Guido Rossum. 1995. Python Reference Manual. Technical Report.

Amsterdam, The Netherlands, The Netherlands.

[44] Manu Sridharan, Satish Chandra, Julian Dolby, Stephen J. Fink, and
Eran Yahav. 2013. Alias Analysis for Object-Oriented Programs. Springer-
Verlag, Berlin, Heidelberg, 196â€“232.

[45] David Tansley. 2000. Linux and Unix Shell Programming. Addison-

Wesley Longman Publishing Co., Inc., Boston, MA, USA.

[46] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest:
Automated Testing of Deep-neural-network-driven Autonomous Cars.
In Proceedings of the 40th International Conference on Software En-
https:
gineering (ICSE â€™18). ACM, New York, NY, USA, 303â€“314.
//doi.org/10.1145/3180155.3180220

[47] Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay. 2018. Au-
tomated Directed Fairness Testing. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering (ASE 2018).
ACM, New York, NY, USA, 98â€“108. https://doi.org/10.1145/3238147.
3238165

[48] Manasi Vartak, Joana M. F. da Trindade, Samuel Madden, and Matei
Zaharia. 2018. MISTIQUE: A System to Store and Query Model Inter-
mediates for Model Diagnosis. In Proceedings of the 2018 International
Conference on Management of Data (SIGMOD â€™18). ACM, New York,
NY, USA, 1285â€“1300. https://doi.org/10.1145/3183713.3196934
[49] S. Verma and J. Rubin. 2018. Fairness Definitions Explained. In 2018
IEEE/ACM International Workshop on Software Fairness (FairWare). 1â€“7.
https://doi.org/10.23919/FAIRWARE.2018.8452913

[50] K. Wongsuphasawat, D. Smilkov, J. Wexler, J. Wilson, D. ManÃ©, D.
Fritz, D. Krishnan, F. B. ViÃ©gas, and M. Wattenberg. 2018. Visualizing
Dataflow Graphs of Deep Learning Models in TensorFlow. IEEE Trans-
actions on Visualization and Computer Graphics 24, 1 (Jan 2018), 1â€“12.
https://doi.org/10.1109/TVCG.2017.2744878

[51] Q. Xiao, K. Li, D. Zhang, and W. Xu. 2018. Security Risks in Deep
Learning Implementations. In 2018 IEEE Security and Privacy Work-
shops (SPW). 123â€“128. https://doi.org/10.1109/SPW.2018.00027
[52] Xiaoyuan Xie, Joshua W. K. Ho, Christian Murphy, Gail Kaiser, Baowen
Xu, and Tsong Yueh Chen. 2011. Testing and Validating Machine
Learning Classifiers by Metamorphic Testing. J. Syst. Softw. 84, 4
(April 2011), 544â€“558. https://doi.org/10.1016/j.jss.2010.11.920
[53] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen,
Yang Liu, Jianjun Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019.
DeepHunter: A Coverage-Guided Fuzz Testing Framework for Deep
Neural Networks. In Proceedings of the 28th ACM SIGSOFT Interna-
tional Symposium on Software Testing and Analysis (ISSTA 2019). As-
sociation for Computing Machinery, New York, NY, USA, 146â€“157.
https://doi.org/10.1145/3293882.3330579

[54] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Ma-
chine Learning Testing: Survey, Landscapes and Horizons. CoRR

, ,

Sahil Verma and Zhendong Su

abs/1906.10742 (2019). arXiv:1906.10742 http://arxiv.org/abs/1906.
10742

[55] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sar-
fraz Khurshid. 2018. DeepRoad: GAN-based Metamorphic Testing and
Input Validation Framework for Autonomous Driving Systems. In Pro-
ceedings of the 33rd ACM/IEEE International Conference on Automated
Software Engineering (ASE 2018). ACM, New York, NY, USA, 132â€“142.
https://doi.org/10.1145/3238147.3238187

[56] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and
Lu Zhang. 2018. An Empirical Study on TensorFlow Program Bugs.
In Proceedings of the 27th ACM SIGSOFT International Symposium on
Software Testing and Analysis (ISSTA 2018). ACM, New York, NY, USA,
129â€“140. https://doi.org/10.1145/3213846.3213866

[57] Zhi Quan Zhou and Liqun Sun. 2018. Metamorphic Testing for Machine
Translations: MT4MT. 2018 25th Australasian Software Engineering
Conference (ASWEC) (2018), 96â€“100.

14

