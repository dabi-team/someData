0
2
0
2

y
a
M
8
2

]

C
D
.
s
c
[

2
v
6
7
0
3
1
.
5
0
0
2
:
v
i
X
r
a

Using PHAST to port Caﬀe library:
First experiences and lessons learned

Eduardo Jos´e G´omez-Hern´andez1, Pablo Antonio Mart´ınez-S´anchez1, Biagio Peccerillo2, Sandro
Bartolini2, Jos´e Manuel Garc´ıa1, and Gregorio Bernab´e1

1 Computer Engineering Department, University of Murcia, Spain
{eduardojose.gomez,pabloantonio.martinezs,jmgarcia,gbernabe}@um.es
2 Department of Information Engineering and Mathematical Sciences, University of Siena, Italy
{peccerillo,bartolini}@dii.unisi.it

Abstract. Performance has always been a hot topic in computing. However, the viable ways
to achieve it have taken many forms in the diﬀerent moments of computing history. Today,
technological limits have pushed the adoption of increasingly parallel multi-core and many-core
architectures and even the use of highly speciﬁc hardware (aka Domain-Speciﬁc Architectures,
or DSAs) to solve very speciﬁc problems.
In this new context, one major problem is how to develop software once, and be able to run it on
multiple accelerator architectures, seamlessly. Ideally aiming at a single programming model that
can automatically target the code to diﬀerent kinds of parallel architectures, allowing speciﬁc
tuning with minimal, if any, changes to the source-code in order to seek performance portability.
A comprehensive solution to this is still lacking.
In this work, we present the use of the PHAST Library, which allows users to code once, at a high
level of abstraction and thus with high productivity, and automatically targeting diﬀerent parallel
devices by changing the compilation process. As a case study, we have worked on the porting
of the well-known deep-learning Caﬀe framework. The framework has been split into diﬀerent
parts and some of them have been ported, obtaining a working straightforward implementation
that can be run on both CPUs and GPUs.
We conclude discussing the lessons learned during the porting process, and analyzing the obtained
performance in the perspective of completing the porting and expanding it to future consequent
works.

Keywords: Machine Learning · Heterogeneous Computing · High Performance Computing ·
Performance Portability

1 Introduction

In computing, faster means better. Since the ﬁrst day of the history of computing, performance has
always been the main driving factor for innovation. CPUs evolved from a simple concept to huge
computation monsters, as their transistor number was growing with a rate described by Moore’s law.
When it seemed that this trend was going to end because of the approaching of physical limits, it
seemed that the performance growth itself was going to end too. However, it resisted and is still valid
today thanks to a paradigm shift: the adoption of parallel architectures. The idea seemed quite simple
at the same time as results were promising. The parallelism era arrived.

Nowadays, the increase of computing power comes from specialization. Instead of big powerful and
power-hungry processing units, the trend is looking at speciﬁc hardware to do speciﬁc tasks. This
speciﬁc hardware, generally called accelerator, is able to perform computations much faster, but also
using less energy. Wise linking between speciﬁc tasks and speciﬁc hardware may be diﬃcult, but leads
to otherwise unattainable results. Some researchers even see this as a new golden age for computer
architecture [5]. However, the big problem around this new era of accelerators is to adapt the software
to the speciﬁc hardware. The creation of a programming model able to take advantage of all this
diversity is extremely important. This new approach is called performance portability [9], and it is still
an open ﬁeld of investigation with no deﬁnitive solution available.

 
 
 
 
 
 
2

G. Eduardo Jos´e et al.

A growing number of approaches have been recently proposed or improved. Big companies, like
Intel (which is now launching Intel One API [6]) are also showing interest in this topic. In this paper, we
choose PHAST Library as our case study. PHAST code can be written once and targeted to diﬀerent
devices via a single macro. Its inner layers are implemented in CUDA C++ or std::threads, thus
allowing targeting both NVIDIA GPUs and multi-core CPUs.

On the application side, deep learning is of an obvious interest nowadays. Not only due to the great
number of possibilities it oﬀers, but also because of the promising future it is facing. In this work,
we present a straightforward course on how to port Caﬀe (a deep learning framework) library using
PHAST Library. These ﬁrst experiences can be utilized as knowledge base for other, more complex,
deep-learning cases (e.g. bigger and more complex networks).

The rest of the paper is organized as follows: Section 2 presents a background of state-of-the-art
in the core and related topics of this paper, in which PHAST and Caﬀe can be framed. Our work is
explained in Section 3, followed by the results obtained, in Section 4. Finally, Section 5 concludes the
paper giving some hints for future work.

2 Background

2.1 Accelerators

After reaching the power density limit in traditional processors, a slowdown in performance improve-
ments seemed to be unavoidable. However, a new paradigm, based on the increase of throughput
through the parallel execution of multiple independent workloads, prevented said slowdown: multi-
core CPUs. They quickly spread in industrial and consumer markets, growing up to the point of being
responsible for the most part of the computing power in data centers.

Meanwhile, some researchers, especially in computer vision, noticed that the GPUs, that they were
already using, were able to solve their problems faster. Therefore, they started using programming
languages meant for graphics, like OpenGL, to implement general purpose algorithms [3].

Nowadays, GPUs are used like CPUs (GPGPU), and speciﬁc languages to use them (such as
CUDA, or OpenCL) have been developed. They are able to generate graphics and compute data at
high speed exploiting their Single Instruction Multiple Threads (SIMT) execution model, based on
Single Instruction Multiple Data (SIMD), but with multi-threading.

There are many kinds of accelerators, with GPUs being the most common. Other kinds include
FPGAs and ASICs, and most of them are problem-speciﬁc. Each one of these accelerators uses its own
programming language, a Domain Speciﬁc Language (DSL). Using its own DSL for each device allows
to beneﬁt from the speciﬁc features of that device [5].

The main problem with the diﬀusion of accelerators to solve various problems is that each DSL is
quite diﬀerent from the others. Hence, making a portable software between accelerators is impossible
without having multiple source ﬁles (at least one ﬁle per target device). Having multiple versions of
the same code increases the code complexity and hampers the overall code productivity and maintain-
ability [2].

2.2 ML & HPC

Theoretical and mathematical models of the artiﬁcial intelligence techniques were developed in the
twentieth century, but the lack of computing power prevented it from progressing. Nowadays, the
performance of current hardware has become a key enabling factor for its revamping and widespread
commercial adoption in various ﬁelds. For example, medical image analysis has started to implement
deep learning for screening and localization of malignant zones. Additionally, other medical areas are
working with these kind of techniques as well, like the analysis of the genetic information inside DNA
and RNA series [1]. The common objective is not to replace physicians with deep learning techniques,
but to support them to make better diagnoses.

Machine Learning (ML) techniques could be diﬃcult to code and debug, therefore many frameworks
have been developed to ease their use. Most of them are open-source and provide software solutions

Using PHAST to port Caﬀe library: First experiences and lessons learned

3

for most of the types of neural networks. The most known ones are Caﬀe, Caﬀe2, Tensorﬂow, Theano,
PyTorch, Mxnet, and CNTK among others [4].

In ML, the training phase is very time-consuming, since it is an evaluation and optimization problem
with hundreds, thousands, even millions of parameters. Therefore, the reduction of the training phase
execution time is a desirable feature that frameworks should provide. Most of them are able to use
NVIDIA’s cuDNN, BLAS-like libraries, MKL-libraries, OpenCL, even speciﬁc libraries for custom
integrated circuits, to speed up this computation. Moreover, many of them also allow other kinds of
parallelism for multiple nodes, mainly using the MPI library for inter-process communication.

2.3 PHAST

Parallel Heterogeneous-Architecture STL-like Template (PHAST) Library [10,11,12,13] is a modern
C++ programming template library based on the classic Standar Template Library (STL)-like and
multi-dimensional containers, developed with to seek productivity and performance portability. It
currently supports multi-core CPUs and NVIDIA GPUs, allowing the users to write expressive and
concise sequential-like code that can be automatically parallelized. Its main goal is to let the program-
mers code using high-level programming approaches without preventing them from applying low-level
optimizations, if needed, keeping the main code at a higher level of abstraction.

Similar to STL containers, PHAST provides a vector container, but also adds a matrix, cube, and
grid, all of them working with a very similar interface. It also takes advantage of vector-like primitives
mapping to SSE or AVX vectors.

These containers can be modiﬁed using functors, an idea borrowed by STL. They are structs that
inherit from a base functor struct and deﬁne at least operator ‘()’ between their methods. There are
a lot of predeﬁned algorithms and functors, but this allows the creation of functionalities that are not
deﬁned by default in the library, without losing performance or portability.

In its roadmap, there is planned support for multiple devices in the same executable, lambda
syntax, Field Programmable Gate Array (FPGA) support, OpenCL backend, multi-GPU, and many
other interesting features.

After writing the sequential code in the PHAST way, it is possible to change the device target
changing a macro and the compiler. Therefore, having two diﬀerent makeﬁles makes the trick. The
most important thing is that the code has not changed, only the compilation process.

2.4 Caﬀe

Caﬀe is the ﬁrst Deep Neural Network (DNN) framework, developed by Berkeley AI Research. Nowa-
days, in the production environment, Caﬀe is replaced by Caﬀe2 and pyTorch, the Caﬀe successors, but
in other cases, TensorFlow and MXNet are the selected ones. Caﬀe continues to be used in research,
due to the fact that it is very easy to modify, extend, or use, all of this without losing ﬂexibility to run
most of the state-of-the-art DNN models.

This framework runs on CPU or GPU, just by changing a ﬂag. However, this is done by having
two implementations in two source-code ﬁles, one for CPU (.cpp) and on for GPU (.cu). Therefore,
developers are forced to maintain two diﬀerent versions of the same functionality. In this case, this is
very well done and there are not a large number of diﬀerences between the ﬁles. So, it constitutes an
interesting and challenging case for a single-source approach.

In the internal structure of Caﬀe, we have found that it is built from multiple modules that work
by themselves. It is possible to classify the blocks in two parts: we call them containers and executors.
Containers store data to be used by executors. Executors use the containers to exchange data and
process it. For example, a layer gets a set of blobs, and with its own blobs, it computes the output
ones.

A neural network has two diﬀerent phases: inference and training. In the inference phase, data is
passed through all the layers of the neural network in feed-forward mode to reach the last layer and
get a result. But, in training mode, data is passed through all layers in feed-forward, like inference,
but when it reaches the last layer of the network, that data is brought to a solver, it recalculates some
values and starts the back-propagation through each layer, but in reverse order.

4

G. Eduardo Jos´e et al.

Fig. 1: Part of the Caﬀe framework as a block diagram showing communication between blocks.

3 Porting process

In this paper, we present our port of Caﬀe to PHAST library, in order to be able to have one single
code that can be run on both CPUs and GPUs. Our goal is to port all the blocks needed to be able
to run Caﬀe for two LeNet variations of a sample network and MNIST and CIFAR-10 databases. The
blocks we have identiﬁed for the porting are the following:

– Blob: A storage block which stores two vectors (data & diff) used in most of the computations.
– InnerProduct: One of the ﬁrst layers developed in neural networks, also known as the perceptron

layer. As its name suggests, it executes an inner product.

– Convolution: The most common neural network layer nowadays. It is a sliding window that

applies a set of ﬁlters to the input.

– Pooling: Another sliding window layer that applies a simple mathematical function to reduce the

size of the input.

– ReLU: A simple layer that applies the ReLU mathematical function to each element. In Caﬀe,

the leaky-ReLU version is implemented instead of a normal ReLU.

– Accuracy: This is not a real layer (it is implicitly included), but it calculates the accuracy of the

network for a speciﬁc set of inputs.

– SoftMax: When working with classiﬁcation networks, it is expected to get probabilities as an

output. The SoftMax layer maps any set of numbers to probabilities that will add up to 1.

– SoftMax with Loss: It is the same as the SoftMax layer, but it also computes a loss that can

be used to know how the neural network is performing.

The more critical blocks are detailed later. We expect to run these neural networks through the
ported Caﬀe binary in train and test mode using PHAST. Therefore, we chose to modify only the
blob and the lowest part of the main executors (the little ”Phast” boxes in Figure 1). Furthermore,
we removed the GPU code from Caﬀe to use the CPU version as our base for this project. In this way,
we focused on the algorithm itself and not in its GPU implementation.

3.1 Convolution

Feed-Forward The Convolution block, or Convolution Layer, applies to the input a set of ﬁlters
using a sliding window over the input. There are many ways to implement this sliding window, but
the application of the ﬁlter involves the calculation of a vector inner product for each sliding window.
The most common variant of Convolution is the 2-D Convolution (Figure 2), which is a simpliﬁcation
of the N-D Convolution.

As our example network (LeNet) only uses 2-D Convolution, we only ported that speciﬁc variation.
There is not much diﬀerence between a 2-D Convolution and a N-D Convolution, because we use the
im2col + gemm implementation.

InitNetworkLayersInner ProductConvolutionPoolingSolverSGDAdamRMSProBlobSync MemoryCaffe+BindingsPhastPhastPhastPhastUsing PHAST to port Caﬀe library: First experiences and lessons learned

5

The im2col + gemm implementation is a way to map a convolution as a matrix multiplication
(General Matrix Multiplication (GeMM)), but a data manipulation is needed to accomplish it. The
im2col function maps the input matrix into columns to make the Convolution using a GeMM (Figure
3).

Fig. 2: A convolution example using a 2x2 ﬁlter with stride 1 and padding 0 over a 4x3 input matrix.

The original Caﬀe’s im2col function is a Penta-loop with dependencies in each iteration. Therefore,
we decided to adapt it to be able to exploit a bit more of parallelism. To create the PHAST version,
we merged all the loops and parameterized it with only one index. This change allowed PHAST to use
all the available threads as appropriate as each thread is now independent.

Fig. 3: Convolution as a GeMM using the im2col function with 2x2 ﬁlter, stride 1, and padding 0.

Back-Propagation In the feed-forward stage, the im2col function duplicates some values to make a
Convolution with a GeMM. In the back-propagation, we need to apply the reverse step to propagate
the gradients to the previous layers. The most important part is the usage of col2im to map the
gradients to the size of the input data.

Like in the feed-forward stage, the original implementation is also a Penta-loop, therefore we fol-
lowed the same approach as before and we merged the loops and parameterized with only one index.

abcdefghijklwyxzInputFiltera·w + d·x ++ b·y + e·zd·w + g·x ++e·y +h·zg·w + j·x ++ h·y + k·zb·w + e·x ++ c·y + f·ze·w + h·x ++ f·y + i·zh·w + k·x ++ i·y + l·zOutputadbedgehgjhkbecfehfihkilim2col·wxyzFilterOutputw·a + x·d + y· b + z·ew·d + x·g + y· e + z·hw·g + x·j + y· h+ z·kw·b + x·e + y· c + z·fw·e + x·h + y· f + z·iw·h + x·k + y· i + z·labcdefghijklInput6

G. Eduardo Jos´e et al.

3.2

InnerProduct

Feed-Forward In neural networks, there is a layer usually known as Perceptron layer (or Dense layer),
but sometimes it is also known as InnerProduct, because the output of the layer is the inner product
of the inputs with the weights.

caffe cpu gemm<Dtype>(CblasNoTrans, transpose ?

(cid:44)→ <float>∗>& bottom,

1 template <typename Dtype>
2 void InnerProductLayer<Dtype>::Forward cpu(const vector<

(cid:44)→ Blob<Dtype>∗>& bottom,

const vector<Blob<Dtype>∗>& top) {

const Dtype∗ bottom data = bottom[0]−>cpu data();
Dtype∗ top data = top[0]−>mutable cpu data();
const Dtype∗ weight = this−>blobs [0]−>cpu data();

(cid:44)→ CblasNoTrans : CblasTrans,

M , N , K , (Dtype)1.,
bottom data, weight, (Dtype)0., top data);

if (bias term ) {

caffe cpu gemm<Dtype>(CblasNoTrans, CblasNoTrans, M ,

(cid:44)→ N , 1, (Dtype)1.,
bias multiplier .cpu data(),
this−>blobs [1]−>cpu data(), (Dtype)1., top data);

3
4
5
6
7
8
9

10
11
12
13
14

15
16
17
18 }

}

Listing 1.1: Caﬀe version: InnerProduct (CPU
only)

1 template <typename T, unsigned int policy = phast::

(cid:44)→ get default policy()>

2 struct matrixPlusVectorRows : phast::functor::func vec<T, policy

(cid:44)→ > {

PHAST METHOD matrixPlusVectorRows() {}

PHAST METHOD void operator()(phast::functor::vector<T

(cid:44)→ >& row) {

for(auto r = row.begin(), i = vec.begin(); r != row.end(); ++

(cid:44)→ r, ++i)

∗r += ∗i;

}
phast::functor::vector<T> vec;

3
4
5

6

7
8
9

10 };
11
12 template <>
13 void InnerProductLayer<float>::Forward cpu(const vector<Blob

14
15

16

17

18
19
20
21
22
23

24

const vector<Blob<float>∗>& top) {
phast::matrix<float> matA = bottom[0]−>getDataAsMatrix(

(cid:44)→ M , K , false);

phast::matrix<float> matB = this−>blobs [0]−>

(cid:44)→ getDataAsMatrix(K , N , !transpose );

phast::matrix<float> matC = top[0]−>getDataAsMatrix(M ,

(cid:44)→ N , false);

phast::dot product(matA, matB, matC);

if (bias term ) {

matrixPlusVectorRows<float> matrixPlusVectorRows;
matrixPlusVectorRows.vec.link(this−>blobs [1]−>

(cid:44)→ getDataAsVector(N ));

phast::for each(matC.begin i(), matC.end i(),

(cid:44)→ matrixPlusVectorRows);

}
if(!transpose ) matB.transpose();

25
26
27 }

Listing 1.2: PHAST version: InnerProduct
(same code for CPU and GPU)

In this block (see Listing 1.1), we ﬁnd a GeMM, but going deeper it is noticeable that there is a
vector addition over the rows of the matrix. Hence, we implement that functionality in the matrix-
PlusVectorRows functor (Listing 1.2). Using a GeMM to represent other operations is that can be
found very often in Caﬀe: its creators have mapped all possible operations to matrix multiplications
to make them easier to port to GPU, but for now, we developed speciﬁc functors for these operations.
Also, it is noticeable that the code in Listing 1.1 is CPU-only code, and thus its GPU counterpart is in
another source ﬁle. Speciﬁcally, the original CPU version of Caﬀe needs other 10 lines of source-code for
caﬀe cpu gemm() function (total 28), while the original GPU version of Listing 1.1 is 23 source-code
lines plus 27 in the called functions (total 50). Conversely, the code in Listing 1.2 can be run on both
CPU and GPU and is only 27 lines of source-code.

Back-Propagation The InnerProduct back-propagation is a bit diﬀerent from the other block we
have seen. Instead of reversing the operation made in feed-forward to map to each value its own
gradient and modify its weight, we added to the weights a scaled gradient based on the original data.
Then, we did the same with the bias, and lastly, we propagated the changes to the previous layer.
Despite the trick to map all operations to matrix multiplications, this layer is very straightforward.

3.3 Pooling

Feed-Forward The Pooling layer applies a mathematical function to a set of numbers to get only
one value, such as maximum, minimum, or mean. Like the Convolution layer, it works using a sliding
window over the input data and applying the function to each set.

Using PHAST to port Caﬀe library: First experiences and lessons learned

7

The structure is very similar to the Convolution block, but this time, we did not apply the same

technique. We had only parallelized the outer loop.

Back-Propagation During the feed-forward stage, we stored the origin of each output value. There-
fore, we have to map the values from the output to the input using that list of mapped values. Its
objective is to apply all the gradients to its corresponding positions.

Like in the feed-forward stage, we did not merge all the loops, because we did not verify that it

would have continued to work properly. Therefore, we only parallelized the ﬁrst loop.

4 Evaluation

4.1 Testbench

In this work we have used the PHAST library 1.1.1 compiled with GCC 8.3.0 and CUDA 10.1. The
Caﬀe framework was obtained from the oﬃcial git repository, using the 99bd99795dcdf0b1d3086a-
8d67ab1782a8a08383 commit. The hardware platform is a high-performance workstation mounting an
Intel i9 9900K @ 3.60GHz, 8 cores hyper-threaded and 32 GiB RAM memory, and a Geforce RTX 2080
8GB GDDR6X with NVIDIA driver 430.50. This is running Ubuntu 18.04 with Linux 5.0.0-36-generic.

4.2 Results

Firstly, we trained two LeNet-based neural networks using the original Caﬀe code. The ﬁrst one, built
from 6 layers (2 Convolutions, 2 Poolings, and 2 InnerProducts) was used to classify the MNIST [8]
database. The second one was used to classify the CIFAR10 [7] database, and it is composed of 8 layers
(3 Convolutions, 3 Poolings, and 2 InnerProducts). Additionally, both networks had a SoftMax layer
with loss, an Accuracy layer, and at least 1 layer with the ReLU function.

Then, we tested the same examples with our Caﬀe version ported to PHAST. We run successfully
both neural networks in CPU and GPU using PHAST. To check the results, we used the set of inputs
that Caﬀe provides checking several parameters such as the output of the network, the accuracy, the
loss, and some intermediate matrices. We preferred to use the intermediate matrices and the outputs
to be sure that both versions (the original and ours) were obtaining the same results, despite only with
the accuracy and the loss were enough to validate the results.

Since the same results were found that with the original CPU Caﬀe’s implementation, we claim
that all blocks were working properly. Then, we corroborate that our version of the Caﬀe framework
with PHAST, which only has a single code, is able to run on CPU or GPU depending on the Makeﬁle
used.

Finally, we decided to test how accurate was our implementation with respect to the original one.
Using the Caﬀe test ﬁles, we checked all the blocks ported to PHAST that had a test (Table 2).
We noticed that all the functionality ported was working, and only tests that had unimplemented
functionality failed.

Block
Convolution
Pooling
InnerProduct
SoftMax
SoftMax Loss
Accuracy

Passed Not Passed Total %Passed

3
11
9
4
4
9

12
0
0
0
0
3

15
11
9
4
4
12

20
100
100
100
100
75

Table 1: Caﬀe tests results for the modiﬁed blocks in single precision ﬂoating point numbers.

8

G. Eduardo Jos´e et al.

4.3 Performance

By now, we have focused on the porting process and checking the functionality of our version. Addi-
tionally, some parts are still missing so our current tests are quite preliminary and based on not very
big networks. By the way, dealing with small networks implies that the associated data structures man-
aged in the layers to be distributed to, or gathered from, the parallel devices (e.g., GPU) are relatively
small and thus more subject to ﬁxed transfer overhead and less able to express the maximum rated
steady-state transfer rate. Furthermore, relatively small kernel executions (e.g., some steps elaborate
32 x 16 matrices) intrinsically do not beneﬁt a lot from being distributed onto architectures exposing
high-parallelism while, conversely, can suﬀer the overhead for managing the parallel execution itself.
So, overall, a small network can be even considered a non-trivial test case for a framework that aims
to exploit automatic parallelization.

Here, Table 2 shows some initial performance results and highlights that the original Caﬀe CPU
code, employing OpenBLAS, outperforms PHAST partially ported version by around 2.8 times, and
the GPU original version is around 4.0 times faster than the PHAST one. This is a snapshot of the
performance diﬀerences at this stage of the porting phase and, in the following, we will discuss the
factors that surely contribute to this ﬁgures as to highlight the path that we will follow to complete
the porting, along with the qualitative expected results.

Caﬀe
Caﬀe (PHAST)

CPU
71.42
198.60

GPU
7.24
21.81

Caﬀe
Caﬀe (PHAST)

CPU
399.50
1113.71

GPU
16.65
67.40

(a) MNIST

(b) CIFAR-10

Table 2: Average Forward-Backward execution time (ms)

Among the main direct reasons for this performance diﬀerence we expect a major role to be played
by the fact that not all layers of the network are ported into PHAST Library yet. However, the heaviest
layers, like the convolutional ones, have been already ported so we can expect that ﬁxing this point will
increase performance of the PHAST version but will not erode the majority of the current diﬀerence.
Furthermore, the convolutional algorithm required to be written in PHAST at the user-level because
PHAST does not currently provide a native convolutional iteration. Therefore, in the current version
of the porting, a quite eﬃcient algorithm was used but, pragmatically, we postponed to a later stage
the investigation and implementation of a highly-optimized, state-of-the-art convolutional scan on the
data structures. When all the network will be ported we will address this issue and either optimize the
algorithm/code or implement it within PHAST Library in a more intimate way with its internals in
order to potentially extract some more performance. From our pre-analyses, we expect that the intrinsic
acceleration of the convolutional phase will not be huge but potentially meaningful. And, given that
this phase accounts for a large fraction of the overall computational execution, it is reasonable to
expect that its speedup is translated into a similar overall faster execution.

From another perspective, current porting status implies that a complete porting is highly likely to
cause signiﬁcant speedups due to indirect eﬀects. In fact, having some network layers and framework
crucial parts not yet ported (e.g., in ReLu layer the activation function can be expressed by means of
PHAST algorithms) induces frequent data transfers, especially in the GPU, between PHAST parallel
execution phases and the orchestrating CPU code, as well as, the parallel execution phases performed
in the original version of Caﬀe. So, every time a layer already ported in PHAST is followed by a layer
still in the original version, or viceversa, such data transfers need to be done as well as synchronizations
between devices and/or phases need to be performed and, surely, this is going to hurt performance.
Dealing with relatively small networks as in our test cases, at a certain extent emphasises this situation
compared to bigger networks.

Using PHAST to port Caﬀe library: First experiences and lessons learned

9

We expect the ﬁx to this point to have a major eﬀect in reducing the current performance gap
between the original code and the PHAST one. The eﬀect will be indirect and induced by the port-
ing of all layers so that the inference/back-propagation activities will mainly run without artiﬁcial
interruption across the layers and unneeded data transfers. Surely this will be more evident in the
GPU-targeted code because such overhead can be expected to count more. As a rough estimate, we
can spot around 10 and 30 unnecessary transfers, for MNIST and CIFAR respectively, between the
original and PHAST ”domains” in the inference phase only. A similar number, at least, is present in
the back-propagation phase. Therefore, this activities are not negligible performance-wise.

To a closer inspection of the overhead induced by the data transfer between ported and not-yet-
ported layers, there is another factor that could even constitute the major source of the performance
diﬀerence in both CPU and GPU target code. In fact, the original Caﬀe version of the layers relies on
OpenBLAS-friendly data structures, i.e., having column-major-order memory allocation, while PHAST
is currently written assuming row-major-order in multi-dimensional containers. Therefore, every time
the code crosses the boundary between the original version and the PHAST ported one, the involved
data structures do need also a conversion from one format to the other, on top of the mere transfer.
So, not only a number of data transfers to/from the parallel PHAST execution are unnecessary in
themselves, but they require also an additional copy host-side per transfer as to transpose the memory
layout. Our expectation is that this indirect factor could be the one representing the biggest quote in
the current gap breakdown.

This situation, at the current stage of the porting, was unavoidable because it is a direct consequence
of following an incremental porting strategy, which, in turn, was required for the controllability and
eﬀectiveness of the porting work itself. In fact, this approach allowed us to port one piece at a time of
the network, focusing on debugging and performing regression tests iteratively and in a tight controlled
fashion as not to risk to have the code diverge. In all the porting phases we have always been able to
run and test Caﬀe execution comparing its output to the original version.

For sure, we expect that once we have ported the entire set of layers, some additional refactoring
steps could be performed as to make (some of) the code and (at least some of) the data structures
evolve towards a shape more naturally ﬁtting PHAST and, probably, some features that can enable a
more eﬀective parallelism exploitation.

Furthermore, some data-structures and parallel activities are quite small and could beneﬁt from
merging them into fewer, more complex, kernels to be executed in parallel. However, this beneﬁt could
even be quite small, especially for the GPU target, because once all the parallel activities are run on
the parallel architecture, the runtime scheduler is potentially capable of very advanced strategies for
making the most out of the available hardware resources.

Concluding, the current version of the PHAST partially ported Caﬀe code, despite being single-
source and allowing high coding-productivity and automatic targeting to parallel CPUs and NVIDIA
GPUs, exhibits a non-negligible performance slowdown compared to the original versions for the re-
spective architectures. However, we have described how most of the performance gap is likely to be
due to the overhead induced by the unnecessary data transfers between the already ported layers and
the original ones, both as the intrinsic not-needed transfer time and the time to adapt the memory
layout of the transferred data.

As ongoing and near-future work, we aim at completing the porting as to evaluate the exact perfor-
mance diﬀerence between the original and PHAST version of this and other neural networks as well as
to start identifying the best parallelization/performance tuning parameters in each kernel. This will be
particularly interesting considering that PHAST Library allows exploring such parallelization/tuning
space parameters without altering the source code and its correctness as to adapt the application to
diﬀerent instances of the architectures, like diﬀerent CPUs and especially GPU families and versions.

4.4 Lessons Learned

The ﬁrst lesson we learned is that current and forthcoming frameworks base their APIs on C++. Thus,
a deep understanding of the language, including its newest components (functors, templates, etc) is a
key fact that can enable eﬃciency, robustness and, potentially, portability.

10

G. Eduardo Jos´e et al.

Another important thing is that the porting process is very inﬂuenced by the software functionality
to be ported. A strong requirement is a high understanding of the original programming model, its
purpose, and the speciﬁc application domain in which it is applied. It may result in a heavy and complex
task for impedance matching between application algorithmic structure and the parallel nature of the
architecture through the PHAST Library and programming approach.

Finally, and regarding portability, we have seen very appealing the idea of a single source-code
for many devices. Virtually any application or framework can be ported. A second step (which was
out of the scope of this work) is, however, to complete and polish the porting as to improve overall
performance and verify performance portability across architectures.

In this sense, this experience has already proved very valuable from the PHAST standpoint. In
fact, some overhead that was initially emerging in PHAST Library applied in the considered applica-
tion domain, has induced speciﬁc improvements in some PHAST base mechanisms for data exchange
between the orchestrating CPU and parallel execution devices (e.g., GPU). Speciﬁcally, we were able
to release a few synchronizations in the inner layers of PHAST Library, which were originally present
and that, actually, we now proved to be unnecessary in the speciﬁc context of this work and in similar
algorithmic situations.

5 Conclusions

In this work, we have presented our developments and decisions to port the Caﬀe library to be used
with the PHAST Library. This way, we have shown that it is possible to move from two code sources
(one for CPUs and another one for GPUs) to only one single code. This single code is portable and
can run on diﬀerent devices by means of compiling options.

As deep learning and neural networks are very important today, choosing Caﬀe as a use case seems

to be a good choice.

We also have shown that PHAST is suitable to be used in real applications, it is easy to use and

remembers well-established highly-expressive techniques.

There is several work left behind for future developments, among them: Enhancing the performance:
We have detected some points of improvement in our development, as reducing the number of copies
made at each operation or maintaining some structures as long as possible. We plan to implement
them in the near future. Complete the port of Caﬀe to PHAST : Apart from reducing the performance
gap between the original Caﬀe implementation and our porting, completing this task will give us the
opportunity to release Caﬀe freely so it can be used by other researchers. Extend our work to other
devices: Increasing the portability is one of our main goals. By choosing PHAST Library, our version
will evolve as soon as new architectures are supported (such as TPUs, for example). This will also give
us valuable information for the developing of these programming models.

Acknowledgements

This work has been partially funded by the AEI (State Research Agency, Spain) and the ERDF
(European Regional Development Fund, EU) under the Contract RTI2018-098156-B-C53. We would
also like to thank our laboratory workmates, Francisco Mu˜noz-Mart´ınez and David Corbal´an Navarro,
for their fruitful discussions.

References

1. Angermueller, C., P¨arnamaa, T., Parts, L., Stegle, O.: Deep learning for computational biology. Molecular

Systems Biology 12(7), 878 (2016). https://doi.org/10.15252/msb.20156651

2. Antinyan, V., Sandberg, A.B., Staron, M.: A pragmatic view on code complexity management. Computer

52(2), 14–22 (Feb 2019). https://doi.org/10.1109/MC.2018.2888761

3. Bohn, C.A.: Kohonen feature mapping through graphics hardware. In: In Proceedings of Int. Conf. on

Compu. Intelligence and Neurosciences. pp. 64–67 (1998)

Using PHAST to port Caﬀe library: First experiences and lessons learned

11

4. Hatcher, W.G., Yu, W.: A survey of deep learning: Platforms, applications and emerging research trends.

IEEE Access 6, 24411–24432 (2018). https://doi.org/10.1109/ACCESS.2018.2830661

5. Hennessy, J.L., Patterson, D.A.: A new golden age for computer architecture. Commun. ACM 62(2), 48–60

(Jan 2019). https://doi.org/10.1145/3282307, http://doi.acm.org/10.1145/3282307

6. Intel: Intel’s ‘one api’ project delivers uniﬁed programming model across diverse architectures. https:

//newsroom.intel.com/news/intels-one-api-project-delivers-unified-programming-model

7. Krizhevsky, A., Nair, V., Hinton, G.: The cifar-10 dataset (2009), https://www.cs.toronto.edu/~kriz/

cifar.html [Last accessed 20 November 2019]

8. LeCun, Y., Cortes, C., Burges, C.J.: The mnist database, http://yann.lecun.com/exdb/mnist [Last ac-

cessed 20 November 2019]

9. Neely, J.: Doe centers of excellence performance portability meeting. Tech. Rep. LLNL-TR-700962,

Lawrence Livermore National Lab.(LLNL), Livermore, CA (United States) (2016)

10. Peccerillo, B., Bartolini, S.: PHAST library - enabling single-source and high performance code for gpus
and multi-cores. In: 2017 International Conference on High Performance Computing & Simulation, HPCS
2017, Genoa, Italy, July 17-21, 2017. pp. 715–718 (2017). https://doi.org/10.1109/HPCS.2017.109, https:
//doi.org/10.1109/HPCS.2017.109

11. Peccerillo, B., Bartolini, S.: PHAST - A portable high-level modern C++ programming li-
IEEE Trans. Parallel Distrib. Syst. 30(1), 174–189 (2019).

brary for gpus and multi-cores.
https://doi.org/10.1109/TPDS.2018.2855182, https://doi.org/10.1109/TPDS.2018.2855182

12. Peccerillo, B., Bartolini, S.: Task-dag support in single-source PHAST library: Enabling ﬂexible assign-
ment of tasks to cpus and gpus in heterogeneous architectures. In: Proceedings of the 10th International
Workshop on Programming Models and Applications for Multicores and Manycores, PMAM@PPoPP 2019,
Washington, DC, USA, February 17, 2019. pp. 91–100 (2019). https://doi.org/10.1145/3303084.3309496,
https://doi.org/10.1145/3303084.3309496

13. Peccerillo, B., Bartolini, S., Ko¸c, C¸ .K.: Parallel bitsliced AES through PHAST: a single-source high-
performance library for multi-cores and gpus. J. Cryptographic Engineering 9(2), 159–171 (2019).
https://doi.org/10.1007/s13389-017-0175-4, https://doi.org/10.1007/s13389-017-0175-4

