1

1
2
0
2

v
o
N
0
1

]

C
O
.
h
t
a
m

[

3
v
9
7
7
4
1
.
3
0
1
2
:
v
i
X
r
a

©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be
obtained for all other uses, in any current or future media, including reprinting/republishing
this material for advertising or promotional purposes, creating new collective works, for
resale or redistribution to servers or lists, or reuse of any copyrighted component of this
work in other works.

 
 
 
 
 
 
Learning to Solve the AC-OPF using
Sensitivity-Informed Deep Neural Networks

Manish K. Singh Member, IEEE, Vassilis Kekatos Senior Member, IEEE,
and Georgios B. Giannakis Fellow, IEEE

2

Abstract—To shift the computational burden from real-time
to ofﬂine in delay-critical power systems applications, recent
works entertain the idea of using a deep neural network (DNN)
to predict the solutions of the AC optimal power ﬂow (AC-
OPF) once presented load demands. As network topologies may
change, training this DNN in a sample-efﬁcient manner becomes
a necessity. To improve data efﬁciency, this work utilizes the
fact OPF data are not simple training labels, but constitute
the solutions of a parametric optimization problem. We thus
advocate training a sensitivity-informed DNN (SI-DNN) to match
not only the OPF optimizers, but also their partial derivatives
with respect to the OPF parameters (loads). It is shown that the
required Jacobian matrices do exist under mild conditions, and
can be readily computed from the related primal/dual solutions.
The proposed SI-DNN is compatible with a broad range of
OPF solvers, including a non-convex quadratically constrained
quadratic program (QCQP),
its semideﬁnite program (SDP)
relaxation, and MATPOWER; while SI-DNN can be seamlessly
integrated in other learning-to-OPF schemes. Numerical tests
on three benchmark power systems corroborate the advanced
generalization and constraint satisfaction capabilities for the OPF
solutions predicted by an SI-DNN over a conventionally trained
DNN, especially in low-data setups.

Index Terms—Sensitivity analysis; data efﬁciency; optimality

conditions; non-linear OPF solvers.

I. INTRODUCTION

Power system operation involves routinely solving various
renditions of the optimal power ﬂow (OPF) task. Physical ex-
pansion of power networks, increasing number of dispatchable
resources, and renewable generation-induced volatility call for
solving large-scale OPF problems frequently. These problems
naturally involve nonlinear alternating-current (AC) power
ﬂow equations as constraints. Such formulations, referred to
as AC-OPF, are non-convex and are often computationally
too expensive for real-time applications. Approximate formu-
lations such as the linearized or so termed DC-OPF serve as
the pragmatic resort for such setups.

A concerted effort towards handling AC-OPF efﬁciently has
yielded popular nonlinear solvers such as MATPOWER [1],
and efﬁcient conic relaxations with optimality guarantees
for frequently encountered problem instances [2], [3], [4],
[5]. Despite signiﬁcant advancements in numerical solvers,
scalability of the AC-OPF may still be a challenge, particularly
in online, combinatorial, and stochastic settings [6], [7]. To
alleviate these issues,
in
developing machine learning-based approaches (particularly
deep learning) for OPF [6], [7], [8], [9], [10], [11], [12];
see [13] for recent applications. The primary advantage of
machine learning-based approaches lies in the speed-up during

there has been growing interest

the inference phase. For instance, compared to conventional
solvers, deep learning-based approaches have offered speed-
ups by factors as high as 200 for DC-OPF, and 35 for AC-
OPF [9], [10].

To entirely bypass numerical solvers for the OPF, one
can opt for unsupervised learning approaches, such as the
ones suggested in [14], [15], [16], [17], [18]. However, the
performance of OPF solvers for ofﬂine data generation and the
availability of historical data with power utilities adequately
motivate supervised learning. Nevertheless, there are two main
challenges central to learning for OPF. First, traditional deep
learning approaches are not amenable to enforcing constraints
even for the training set. Predictions for OPF minimizers may
have limited standalone value if the related constraints are
violated. Second, power systems undergo frequent topological
and operational changes as generating units can be (de)-
committed and transmission lines or bus/line reactors can be
switched. Such changes may require retraining a DNN poten-
tially at short intervals [19]. Deep learning-based approaches
are traditionally data-intense and frequent retraining for large
systems may be prohibitive.

To cope with the ﬁrst challenge, a DNN may be engaged to
better initialize existing numerical solvers [12], or to predict
active constraints and thus result in an OPF model with much
reduced number of constraints [11], [20], [21]. Another group
of approaches targets constraint satisfaction by penalizing
constraint violations and the related Karush–Kuhn–Tucker
(KKT) conditions [8], [10], or explicitly resorting to a La-
grangian dual scheme for DNN training [7], [15], [22]. The
third alternative involves post-processing DNN predictions by
projecting them using a power ﬂow solver [8], [12]. Although
the projected point satisﬁes the power ﬂow equations, it may
still violate engineering limits.

To cater to the second challenge of frequent model changes,
sample-efﬁcient learning models that generalize well are well
motivated. One way to achieve that
is via meta-learning,
according to which training datasets generated from diverse
grid topologies are used to train a vector of meta-weights [19].
When it later comes to training a DNN to handle a par-
ticular topology, its weights are initialized to the values of
meta-weights. Alternatively, the sample efﬁciency of learning
models could be improved by prudently designing the DNN
architecture upon leveraging prior information. For example,
by seeking an input-convex DNN when the underlying input-
output mapping is convex [23], or using DNNs that unroll an
iterative optimization algorithm [24], or adopting graph-based
priors [14], [25]. The previously mentioned approaches that

design objective functions based on OPF constraints and KKT
conditions may also be seen as including prior knowledge in
training, thus enhancing learnability.

Recent works in the general area of physics-informed learn-
ing aim at incorporating prior information on the underlying
data, not occurring in conventional
training datasets [26].
Speciﬁcally, for learning solutions of differential equations, the
derivatives of DNN output with respect to input carry obvious
virtue. Suitably utilizing these derivatives yields signiﬁcant
advantages in such applications [26], [27]. While for differ-
ential equations derivatives with respect to time and/or space
dimensions emerge naturally, sensitivities in an optimization
problem have been underappreciated. To this end, our recent
work proposed a novel approach of training sensitivity in-
formed DNNs (SI-DNN), intended to learn OPF solutions [28].
Training SI-DNNs requires computing the sensitivities of OPF
minimizers with respect to the input parameters. For DC-
OPF posed as linear or quadratic programs, computing these
sensitivities is simpler, and using these to train DNNs yielded
remarkable improvements; see [28].

Contributions: The contributions of this work are on four
fronts: c1) We put forth a novel approach for training DNNs
to predict AC-OPF solutions by matching not only the OPF
minimizers, but also their sensitivities (partial derivatives col-
lected in a Jacobian matrix) with respect to the OPF problem
parameters (e.g., load demands). c2) We compute the desired
sensitivities for general nonlinear OPF formulations building
upon classical works on perturbation analysis of continuous
optimization problems [29], [30], [31]. Existing sensitivity
results assume certain constraint qualiﬁcations that may not
be satisﬁed by OPF instances. c3) We relax such assumptions
and establish that the sensitivities of primal OPF solutions
do exist under milder conditions. c4) In pursuit of globally
optimal OPF solutions for training a DNN, we also study the
SDP formulation of the AC-OPF and compute its sensitivities
by utilizing the sensitivities of the related QCQP. Such shortcut
obviates the difﬁculty of differentiating a conic program.
It is worth stressing that the proposed sensitivity-informed
methodology can be used in tandem with other learning
approaches, such as those reviewed earlier imposing physics-
inspired DNN architectures, penalizing constraint violations,
or adopting meta-learning.

Motivation: The advantage of using a DNN to predict OPF
minimizers is its computational speed. Running a DNN is
signiﬁcantly faster than running an OPF solver. Alternatively,
the DNN prediction can be used to warm-start an OPF solver
and thus expedite its convergence. This speed-up advantage
during testing (inference) is featured by any DNN. Sensitivity-
informed DNNs are advocated here as a means to expedite
training as well. This is possible because an SI-DNN can
attain the same prediction accuracy as a plain DNN using
much fewer samples. Our numerical tests demonstrate that
depending on the network an SI-DNN requires 1/4 or even
1/10 of the training samples compared to a plain DNN.
Such speed-up is signiﬁcant
in setups where there is not
sufﬁcient time for generating samples ofﬂine. Such setups
could arise when a DNN is used to predict the setpoints
of inverter-interfaced distributed energy resources and the

3

feeder is reconﬁgured frequently; or when unit commitment
decisions change regularly the set of online generators in a
transmission system; or when the DNN is part of a stochastic
unit commitment formulation and has to be retrained for the
various commitment conﬁgurations visited by a branch-and-
bound algorithm.

The rest of the paper is organized as follows: Section II
presents the key methodology for SI-DNNs. Section III poses
the AC-OPF as a parametric optimization taking the form of
a non-convex QCQP. Section IV establishes that the sought
sensivities of the AC-OPF exist under mild conditions and
explains how they can be readily computed. Section V com-
putes the sensitivities of globally optimal AC-OPF solutions
obtained via the SDP relaxation of the OPF. The numerical
tests of Section VI contrast SI-DNN to conventionally trained
DNNs using datasets generated by MATPOWER and the
SDP-based OPF solver on three benchmark power systems.
Conclusions are drawn in Section VII.

Notation: lower- (upper-) case boldface letters denote col-
umn vectors (matrices). Calligraphic symbols are reserved for
sets. Symbol (cid:62) stands for transposition, vectors 0 and 1 are
the all-zeros and all-ones vectors or matrices, and en is the
n-th canonical vector of appropriate dimensions implied by
the context. Operator D(x) returns a diagonal matrix with the
entries of vector x on its main diagonal.

II. SENSITIVITY-INFORMED TRAINING FOR DNNS
Consider the task of optimally dispatching generators and
ﬂexible loads to meet power balance constraints h(·) while
enforcing engineering limits captured by g(·). OPF boils down
to a parametric optimization problem that has to be solved
routinely for different values of the problem parameters being
the time-varying renewable generation and loads. The problem
can be abstracted as: Given a vector θ ∈ RP of problem
parameters or inputs, ﬁnd an optimal dispatch xθ ∈ RN
consisting of voltages and generator setpoints as the minimizer

xθ ∈ arg min

f (x; θ)

x
s.to h(x; θ) = 0 : λθ
g(x; θ) ≤ 0 : µθ

(Pθ)

where functions f (x; θ), h(x; θ), and g(x; θ) are continu-
to x and θ; and vectors
ously differentiable with respect
(λθ, µθ) collect the optimal dual variables corresponding to
the equality and inequality constraints, respectively.

To save on running time and computational resources,
rather than solving (Pθ), one can adopt a learning-to-optimize
approach and train a learning model such as a DNN to predict
approximate solutions of (Pθ); see e.g., [32]. Once presented
with an instance of θ, this DNN can be trained to output
a predictor ˆx(θ; w) of xθ. The DNN is parameterized by
weights w, which can be selected upon minimizing a suitable
distance metric or loss function between xθ and ˆx(θ; w) over
a training set.

Given a choice for a DNN architecture, the straightforward

approach for learning-to-optimize entails two steps:
S1) Building a labeled training dataset {θs, xθs }S

s=1 by solv-

ing S instances of (Pθ); and

4

S2) Learning w by minimizing a data ﬁtting loss over the

training dataset as

min
w

S
(cid:88)

s=1

(cid:96) (ˆx(θs; w), xθs) .

(1)

For a regression task such as the one considered here,
commonly used loss functions (cid:96) include the mean squared
error (MSE) (cid:107)ˆx(θ; w) − xθ(cid:107)2
2, or the mean absolute error
(MAE) (cid:107)ˆx(θ; w)−xθ(cid:107)1. We refer to a DNN trained by solving
(1) as a plain DNN or P-DNN for short. The conventional P-
DNN approach focuses merely on the dataset {θs, xθs }S
s=1,
and is oblivious of any additional properties the mapping
θ → xθ induced by (Pθ) bears.

The key idea here is to extend each training data pair
(θs, xθs ) as (θs, xθs , Jθs), where Jθs := [∇θxθ]θ=θs
is the
Jacobian matrix carrying the partial derivatives of the mini-
mizer xθs with respect to θ evaluated at θ = θs assuming such
sensitivities exist. To incorporate the sensitivity information
into DNN training, we propose augmenting the loss function
with an additional ﬁtting term as

yields the sensitivity-aware training task of (2) upon iden-
tifying ρ = σ2. This scalar case of P = N = 1 can be
trivially extended to a general (OPF) mapping of arbitrary
dimensions P and N upon substituting (cid:15) by a zero-mean
random vector (cid:15) ∈ RP with covariance matrix E[(cid:15)(cid:15)(cid:62)] =
σ2IP and replacing derivatives with Jacobian matrices. This
interpretation not only justiﬁes the form of (2), but also
explains geometrically how sensitivity-informed training uses
the point information {(θs, x(θs), x(cid:48)(θs))}S
s=1 to extrapolate
in a neighborhood around each training datum.

SI-DNNs for learning optimizers were ﬁrst introduced for
solving multiparametric QPs (MPQP) in the conference pre-
cursor of this work [28]. For MPQPs, the minimizer xθ is
known to be a piecewise afﬁne function of θ [33], [34]. Hence,
a DNN with rectiﬁed linear unit (ReLU) activations is well-
motivated as it can describe such mapping. If hypothetically
trained to zero training error, this SI-DNN would yield per-
fectly accurate predictions in a neighborhood of each training
datum θs. Numerical tests showed improvements of 2-3 orders
of magnitude for SI-DNN over P-DNN in inferring MPQP
solutions [28].

min
w

S
(cid:88)

s=1

(cid:107)ˆx(θs; w) − xθs(cid:107)2

2 + ρ(cid:107)ˆJ(θs; w) − Jθs (cid:107)2

F

(2)

where ρ > 0 is a scalar weight and (cid:107) · (cid:107)F denotes the matrix
Frobenius norm. The DNN trained by solving (2) aims to
match not only the target output xθ, but also the sensitivities of
xθ with respect to θ. We term this neural network a sensitivity-
informed DNN or SI-DNN.

To better understand (2) and motivate the inclusion of sen-
sitivities, let us put forth the ensuing interpretation. Consider
learning function x : R → R, which can be an OPF mapping
x(θ) with P = N = 1. Under the typical learning setup, one
aims to build a DNN ˆx(θ) and approximate x(θ) given training
samples {(θs, x(θs))}S
s=1. Different from this setup, suppose
we are given the additional information of function derivative
values at the training samples, so that the training dataset
consists of the triplets {(θs, x(θs), x(cid:48)(θs))}S
s=1. The pertinent
question is how to utilize the extra sensitivity information.

Linearizing function x around a sample θs yields

x(θs + (cid:15)) (cid:39) x(θs) + (cid:15) · x(cid:48)(θs)

for any small (cid:15). Linearizing the DNN output yields similarly

ˆx(θs + (cid:15)) (cid:39) ˆx(θs) + (cid:15) · ˆx(cid:48)(θs)

where ˆx(cid:48)(θs) is the derivative of the DNN output with respect
to its input θ evaluated at θs. Suppose now (cid:15) is a zero-mean
random variable with variance E[(cid:15)2] = σ2. Instead of training
the DNN by minimizing the loss (ˆx(θs) − x(θs))2 summed up
over all s, one can aim at ﬁtting the function around a sphere
of essential radius σ that is centered at θs by minimizing

(ˆx(θs + (cid:15)) − x(θs + (cid:15)))2(cid:105)
(cid:104)

E(cid:15)
(cid:39) (ˆx(θs) − x(θs))2 + σ2 (ˆx(cid:48)(θs) − x(cid:48)(θs))2 .

Of course, the previous loss is also summed up over all s.
Interestingly, this stochastic interpretation of function ﬁtting

This work advocates that the sample efﬁciency beneﬁt of SI-
DNN over P-DNN goes well beyond MPQPs. Before delving
into the details and for the sake of visualization, we present
some numerical tests on a toy 5-bus power system; a proper
numerical evaluation of SI-DNN is deferred to Section VI.
This PJM 5-bus system was dispatched via AC-OPF by
4](cid:62) on buses 2 and
varying the active load demands θ := [pd
4 within [1.5, 3.75] per unit (pu) and [0.4, 6] pu, respectively.
Fixing the other demands, we dispatched the generators at
buses 1 and 5, and removed other generators. Figure 1 depicts
the performance improvement of SI-DNN over P-DNN in
predicting pg

5, the optimal dispatch at bus 5.

2 pd

It is worth clarifying that this work does not train a DNN to
predict OPF solutions under different power system topologies.
On the contrary, it aims at learning the OPF mapping for a
single given topology under diverse loading conditions. The
fact that the network topology may be changing across time
justiﬁes the need to improve on data efﬁciency, so that after a
topology change, the corresponding DNN can be trained afresh
using fewer OPF examples generated using the new topology.
This new learning-to-optimize approach alters the two steps
of the P-DNN workﬂow as follows: For step S1), in addition
to the minimizer xθs , we now have to compute the Jacobian
matrix Jθs
if such sensitivities exist.
Sections IV and V explain how and when such Jacobian
matrices can be computed for a non-convex and a convexiﬁed
rendition of the AC-OPF. The punchline is that obtaining
Jθ requires minimal additional computational effort and no
intervention to the OPF solver. Once the primal/dual solutions
have been found by the OPF, computing Jθ is as simple as
solving a linear system of equations.

instances s,

for all

For step S2), we migrate from solving (1) to (2). Matrix Jθs
is a constant that has been evaluated for each s during S1).
Matrix ˆJ(θs; w) on the other hand is a function of w and is not
straightforward to compute. Fortunately, computing ˆJ(θs; w)
can be performed efﬁciently thanks to advances in automatic
differentiation [35]. Modeling (2) in existing DNN software

5

Fig. 1. The left panel depicts the optimal generation dispatch pg
4](cid:62).
Sampling the parameter space of θ’s provided 523 feasible OPF instances, of which 37 instances constituted the training set. The center and right panel show
the dispatches learned by a P-DNN and an SI-DNN. For the two DNNs, we used the same training points (red circles), architecture (two hidden layers with
16 neurons each), optimizer, and learning rates. P-DNN fails to learn the drop in pg

5(θ) for bus 5 as a function of load demands at buses 2 and 4, that is θ := [pd

2 pd

5 for larger load demands.

platforms (e.g., TensorFlow) is almost as easy as modeling (1)
modulo the coding modiﬁcations deferred to Appendix A. Our
tests of Section VI further show that the extra computational
time for solving (2) is modest. Before computing Jθ, we ﬁrst
pose AC-OPF as a parametric QCQP.

III. AC-OPF AS A PARAMETRIC QCQP
A power network with Nb buses can be represented by
an undirected connected graph G = (N , E), whose nodes
n ∈ N := {1, . . . , Nb} correspond to buses, and edges
e = (n, k) ∈ E to transmission lines, with cardinality
|E| := E. Given line impedances, one can derive the Nb × Nb
bus admittance matrix Y = G + jB. Let vn = vr
n and
pn + jqn denote respectively the complex voltage and power
injection at bus n ∈ N . Power injections are quadratically
related to bus voltages through the power ﬂow equations

n + jvi

pn =

qn =

Nb(cid:88)

k=1

Nb(cid:88)

k=1

n(vr
vr

kGnk − vi

kBnk) + vi

n(vi

kGnk + vr

kBnk)

n(vr
vi

kGnk − vi

kBnk) − vr

n(vi

kGnk + vr

kBnk).

If v ∈ R2Nb collects the real and imaginary parts of nodal
voltages as v := [(vr)(cid:62) (vi)(cid:62)](cid:62) with vr := {vr
n=1 and
vi := {vi

n=1, the power ﬂow equations can be written as

n}Nb

n}Nb

pn = v(cid:62)Mpnv
qn = v(cid:62)Mqn v

(3a)

(3b)

where Mpn and Mqn are 2Nb × 2Nb symmetric real-valued
matrices [36]. Squared voltage magnitudes can also be ex-
pressed as quadratic functions of v as

(4)

n|2 = v(cid:62)Mvn v

|vr

n + jvi
n + eNb+ne(cid:62)

where Mvn := ene(cid:62)
Nb+n. The same holds true for
the squared magnitude of line currents. If ymn is the series
admittance of line (m, n) ∈ E, the current ﬂowing on this line
is ˜imn = (˜vm − ˜vn)ymn, and thus,

|˜imn|2 = v(cid:62)Mimn v

(5)

where Mimn
|ymn|2(eNb+m − eNb+n)(eNb+m − eNb+n)(cid:62).

:= |ymn|2(em − en)(em − en)(cid:62) +

n − qd

The active power injected into bus n can be decomposed
into a dispatchable component pg
n and an inﬂexible component
pd
n − pd
n as pn = pg
n. The former captures the active power
dispatch of a generator or a ﬂexible load located at bus n.
The latter captures the inelastic load to be served at bus n.
To simplify the exposition, each bus is assumed to be hosting
at most one dispatchable resource (generator or ﬂexible load).
The reactive power injected into bus n is decomposed similarly
as qn = qg
n. Let Ng ⊆ N be the subset of buses hosting
dispatchable power injections with cardinality Ng. Bus n = 1
belongs to Ng and serves as the angle reference, so that vi
1 =
v(cid:62)eNb+1e(cid:62)
Nb+1v = 0. The remaining buses host non-ﬂexible
loads and constitute the subset N(cid:96) = N \ Ng with cardinality
Nl = Nb − Ng. For simplicity, we will henceforth term the
buses in Ng as generator buses, and the ones in N(cid:96) as load
buses. Zero-injection (junction) buses belong to N(cid:96) and satisfy
n = pd
pg
n = 0.
Given the inﬂexible loads at all buses {pd

n}n∈N , the OPF
problem aims at optimally dispatching generators and ﬂexible
loads {pg
n}n∈Ng while meeting resource and network lim-
its. The OPF can be formulated as the QCQP [3], [4]

n = qd

n = qg

n, qd

n, qg

(cid:88)

min

npg
cp

n + cq

nqg
n

n∈Ng
over v ∈ R2Nb , {pg
n, qg
n}n∈Ng
s.to v(cid:62)Mpn v = pg
n − pd
n,
v(cid:62)Mqn v = qg
n − qd
n,
v(cid:62)Mpn v = −pd
n,
v(cid:62)Mqn v = −qd
n,
≤ v(cid:62)Mpnv + pd
pg
n
≤ v(cid:62)Mqn v + qd
qg
n
vn ≤ v(cid:62)Mvn v ≤ ¯vn,
v(cid:62)eN +1e(cid:62)
N +1v = 0
v(cid:62)Mimn v ≤ ¯imn,
n, cq

n ≤ ¯pg
n,
n ≤ ¯qg
n,

(P1)

(6a)

(6b)

(6c)

(6d)

(6e)

(6f)

∀ n ∈ Ng
∀ n ∈ Ng
∀ n ∈ N(cid:96)
∀ n ∈ N(cid:96)
∀ n ∈ Ng

∀ n ∈ Ng

∀ n ∈ N (6g)

∀ (m, n) ∈ E

(6h)

(6i)

where (cp
n) are the coefﬁcients for generation cost or the
utility function for ﬂexible load at bus n. Constraints (6a)–
(6d) enforce the power ﬂow equations at load and generator
buses. Constraints (6e)–(6f) impose limits for generators and
ﬂexible loads. Constraints (6g) conﬁne squared voltage mag-

44.5505.56pg5 [p.u.]6.5377.52pd4 [p.u.]pd2 [p.u.]241644.5505.56pg5 [p.u.]6.5377.5pd4 [p.u.]2pd2 [p.u.]241644.5505.56pg5 [p.u.]6.5377.52pd4 [p.u.]pd2 [p.u.]2416nitudes within given ranges and (6h) identiﬁes the reference
bus. Finally, constraint (6i) limits squared current magnitudes
according to line ratings.

Problem (6) is a parametric QCQP as it needs to be
solved for different values of demands {pd
n, qd
n}n∈N ; costs
{cn}n∈Ng ; and generation capacities {pg
, ¯pg
n, qg
, ¯qg
n}. Voltage
n
n
limits {¯i(m,n)}
limits {vn, ¯vn} for n ∈ N and current
for (m, n) ∈ E may also be changing due to normal and
emergency ratings. To keep the exposition uncluttered, we
henceforth ﬁx all but the inelastic demands to known values.
In other words, we are interested in solving (6) over different
values of the parameter vector θ := {pd

n}n∈N ∈ R2L.

n, qd

n, qg

θ := [v(cid:62) x(cid:62)

The optimization variables of (6) consist of all nodal
voltages v and the (re)active power schedules for generators.
If vector xg collects generator schedules {pg
n}n∈Ng , then
vector x(cid:62)
g ] denotes the minimizer of (6) for the
speciﬁc parameter vector θ. Aiming for the complete xθ is
apparently an over-parameterization of the problem, adopted
only to ease the formulation in (6). What the system operator
actually needs to know in practice is only the voltage magni-
tude and active power schedule for each generator (modulo the
reference generator for which we set the voltage magnitude
and angle). In light of this and to reduce the DNN output
dimension, the DNN is trained to predict the PV setpoints for
generators. Given the predicted quantities and knowing the
values for inﬂexible loads from θ, the remaining quantities
can be readily computed using a power ﬂow solver anyway.
Given a set of (locally) optimal primal/dual solutions for (6),
we next analyze the sensitivity of the parametric AC-OPF.
Sensitivities are computed for the complete xθ, from which
the Jacobian Jθ needed in (2) can be readily obtained.

IV. SENSITIVITY ANALYSIS FOR QCQP-BASED OPF

To analyze the sensitivity of (6) with respect to θ, let us
ﬁrst express the vectors of complex power injections across
all buses as

(cid:21)

(cid:20) p
q

= Axg + Bθ

(7)

where xg stacks the generator (re)active power injections
{pg
n}n∈Ng and (A, B) are matrices assigning generators
and loads to buses. We then reformulate (6) as

n, qg

a(cid:62)
0 xg

min
v,xg
s.to v(cid:62)L(cid:96)v = a(cid:62)

v(cid:62)Mmv ≤ d(cid:62)

(cid:96) xg + b(cid:62)

(cid:96) θ,

(8b)
mθ + fm, m = 1 : M : µm (8c)

(cid:96) = 1 : L : λ(cid:96)

(8a)

where a0 collects the generation cost coefﬁcients (cf. (6)); the
ﬁrst L = 2Nb + 1 constraints in (8b) correspond to the power
ﬂow equations (6a)–(6d) and the angle reference constraint
(6h), while constraints (8c) correspond to the M = 4Ng +
2Nb + E inequality constraints of (6). Matrices (L(cid:96), Mm) are
drawn from the M matrices appearing in the quadratic forms
of (6). Vectors (a(cid:96), b(cid:96)) correspond to rows of matrices (A, B)
in (7) for (cid:96) ≤ 2Nb; and 0 for (cid:96) = 2Nb + 1. Vectors dm
are indicator (canonical) vectors and constants fm relate to
generation, voltage, and line limits.

6

g ] of (8) with respect

Aiming at computing the sensitivity of a minimizer x(cid:62)

θ =
[v(cid:62) x(cid:62)
to θ, we explored the re-
lated literature. There has indeed been signiﬁcant interest in
computing the sensitivities of OPF minimizers with respect
to load [37], [38], [39]. However,
the primary motivation
for these works was to efﬁciently compute minimizers and
look into binding constraints for a given trajectory of load
variations. Hence the related OPF was parameterized using a
scalar conveniently varied over a range of interest. Seeking
to compute the minimizer sensitivities with respect to the
vector θ in a relatively general setting, we explored beyond the
power systems literature. Fortunately, there exists a rich corpus
of work on perturbation analysis of continuous optimization
problems with applications in operation research, economics,
mechanics, and optimal control [29]. The ﬁrst approaches
applied the implicit function theorem to the related ﬁrst-
order optimality conditions [30]. Thereon, many developments
have been made towards relaxing the assumptions of initial
works, and expanding the scope to conic programs [31], [40],
[29], [41]. For several recent applications however, the early
approaches of [30] are well suited due to their simplicity; see
for example [42]. Building upon [30], we next compute the
sensitivities required for SI-DNN in Section IV-A; and relax
some of the needed assumptions in Section IV-B.

A. Perturbing Optimal Primal/Dual Solutions

Towards instantiating (Pθ) with (8) and to reduce notational
clutter, let us use symbols (x, λ, µ) to denote the optimal
primal/dual variables (xθ, λθ, µθ) of (8) for a particular θ.
Under mild technical assumptions, a local primal/dual point
for (8) satisﬁes the ﬁrst-order optimality conditions [31]. The
goal of sensitivity analysis is to ﬁnd inﬁnitesimal changes
(dx, dλ, dµ), so that the perturbed point (x+dx, λ+dλ, µ+
dµ) still satisﬁes the ﬁrst-order optimality conditions when the
input parameters change from θ to θ + dθ [30]. To this end,
we next review the optimality conditions and then differentiate
them to compute the sought sensitivities.

The Lagrangian function of (8) is deﬁned as

L(x, λ, µ; θ) := a(cid:62)

0 xg +

+

L
(cid:88)

(cid:96)=1
M
(cid:88)

λ(cid:96)

(cid:0)v(cid:62)L(cid:96)v − a(cid:62)

(cid:96) xg − b(cid:62)

(cid:96) θ(cid:1)

µm

(cid:0)v(cid:62)Mmv − d(cid:62)

mθ − fm

(cid:1) .

m=1
With x := {v, xg}, Lagrangian optimality ∇xL = 0 gives

(cid:32) L
(cid:88)

(cid:96)=1

(cid:124)

M
(cid:88)

λ(cid:96)L(cid:96) +

m=1
(cid:123)(cid:122)
:=Z

a0 =

L
(cid:88)

(cid:96)=1

λ(cid:96)a(cid:96)

(cid:33)

µmMm

v = 0.

(9a)

(cid:125)

(9b)

In addition to Lagrangian optimality, ﬁrst-order optimality
conditions include primal feasibility [cf. (8b)–(8c)], as well
as complementary slackness and dual feasibility for all m:

µm

(cid:0)v(cid:62)Mmv − d(cid:62)
(cid:123)(cid:122)
(cid:124)
:=gm

mθ − fm

= 0

(cid:1)

(cid:125)

(10a)

µm ≥ 0.

(10b)

From the aforementioned optimality conditions, let us focus
on those that take the form of equalities, namely (9a)–(9b),
(8b), and (10a). For these conditions, we will compute their
total differentials. From the ﬁrst three, we obtain

Z dv + Lλ dλ + Mµ dµ = 0
A(cid:62) dλ = 0
λ dv − A dxg − B dθ = 0
(cid:96) ; and Mµ := (cid:80)M

(cid:96)=1 L(cid:96)ve(cid:62)

2L(cid:62)

(11a)

(11b)

(11c)

where Lλ := (cid:80)L

For (10a), the total differential is

m=1 Mmve(cid:62)
m.

gm dµm + µm dgm = 0
(12)
where dgm := (∇vgm)(cid:62) dv + (∇θgm)(cid:62) dθ for all m. We
identify three cases:
c1) If µm = 0 and gm < 0, then (12) implies dµm = 0. It
follows that: i) µm + dµm = 0; ii) (µm + dµm)(gm +
dgm) = 0; and iii) gm + dgm < 0 for any small dgm.
In conclusion, condition (12) ensures that the perturbed
point satisﬁes conditions for optimality,
including the
inequalities from primal/dual feasibility.

c2) If µm > 0 and gm = 0, then (12) gives dgm = 0. It also
follows that: i) gm + dgm = 0; ii) (µm + dµm)(gm +
dgm) = 0; and iii) µm + dµm > 0 for any small dµm.
As in case c1), condition (12) ensures that the perturbed
point satisﬁes all conditions for optimality.

c3) If µm = gm = 0, then (12) is inconclusive on dgm
and dµm. In this degenerate case, for the perturbed
point to remain optimal, we need to explicitly impose:
i) dgm ≤ 0; ii) dµm ≥ 0; and iii) dgm dµm = 0.
Even though the three latter constraints can be handled
by the sensitivity analysis of [31], [40], they considerably
complicate the treatment. Moreover, such degeneracy is
seldom encountered numerically. We henceforth rely on
the so called strict complementarity assumption, which
ignores case c3) [30].

Assumption 1. Given a tuple of optimal primal/dual variables
(x, λ, µ), constraint gm(x; θ) = 0 if and only if µm > 0.

Two observations are in order. First, the analysis under
c1)-c2) reveals that although we perturbed only the equality
conditions for optimality, the obtained perturbed point satisﬁes
the inequality conditions for optimality as well. Therefore,
under Assumption 1, the point (x + dx, λ + dλ, µ + dµ)
satisfying the perturbed optimality conditions is (locally)
optimal for (8), when solved for θ + dθ. Second, despite
Assumption 1, if a degenerate instance of (8) does occur for
some θs in the training dataset, the particular pair (θs, xθs)
can still be used to train the SI-DNN, yet without the additional
sensitivity information. In other words, degenerate instances
can contribute only to the ﬁrst ﬁtting term of (2).

Applying (12) for all m, the total derivatives for (10a) can

be compactly expressed as

D(g) dµ + 2D(µ)M(cid:62)

µ dv − D(cid:62) dθ = 0,

(13)

where g := {gm}M
and matrix D := (cid:80)M

m=1 stacks the inequality constraint values,
m. Operator D(x) returns a

m=1 µmdme(cid:62)

7

0
(cid:123)(cid:122)
:=S

diagonal matrix with vector x on its main diagonal. Conditions
(11) and (13) can be collected in matrix-vector form as
















Z
0
2L(cid:62)
λ
2D(µ)M(cid:62)
µ

0
0 A(cid:62)
−A 0

Lλ Mµ
0
0

0 D(g)









dv
dxg
dλ
dµ





=





dθ





0
0
B
D(cid:62)
(cid:123)(cid:122)
:=U

(cid:124)

(cid:125)

(cid:124)

(cid:125)
(14)
To compute the sensitivities of primal and dual variables
with respect to the p-th entry θp of θ, we need to solve the
previous system of 2(Nb + Ng) + L + M linear equations for
dθ = ep. The size of the system can be reduced by dropping
the numerous inactive inequality constraints of (8) for which
µm = 0 and gm < 0, and thus, dµm = 0 as discussed
earlier under case c1). Notably, if matrix S is invertible, the
aforementioned sensitivities can all be found at once using the
respective blocks of S−1Uep. We next address two relevant
questions: q1) When is S invertible?; and q2) What are the
implications of a singular S?

B. Existence of Primal Sensitivities

To address q1) for an arbitrary (Pθ), the existing literature
identiﬁes some assumptions on (x, λ, µ; θ). We ﬁrst review
these assumptions, and then assess if they are reasonable for
the OPF task at hand. Given an optimal primal x for some
θ, let A(x) denote the subset of inequality constraints of
g(x; θ) ≤ 0 that are active or binding, that is A(x) := {m :
gm(x; θ) = 0}. A primal solution x is termed regular if the
next assumption holds.

Assumption 2. The vectors {∇xh(cid:96)}∀(cid:96) and {∇xgm}m∈A(x)
are linearly independent.

For the OPF in (8), the functions h(cid:96) and gm correspond to
the (in)equality constraints (8b)–(8c) written in the standard
form as in (Pθ). Assumption 2 is often referred to as linearly
independent constraint qualiﬁcation (LICQ). If a (locally)
optimal x satisﬁes the LICQ, the corresponding optimal dual
variables (λ, µ) are known to be unique [43]. In addition to
satisfying ﬁrst-order optimality conditions, a sufﬁcient condi-
tion for (x, λ, µ; θ) to be (locally) optimal is often provided
by the following second-order optimality condition.

Assumption 3. For a subspace orthogonal to the subspace
spanned by the gradients of active constraints

Z := (cid:8)z : z(cid:62)∇xh(cid:96) = 0 ∀ (cid:96), z(cid:62)∇xgm = 0 ∀ m ∈ A(x)(cid:9)

it holds that z(cid:62)∇2

xxLz > 0 for all z ∈ Z \ {0}.

Under the strict complementarity, regularity, and second-
order optimality conditions, matrix S is guaranteed to be
invertible; see Theorem 2.1 and Corollary 2.1 of [30].

Lemma 1 ([30]). If Assumptions 1–3 hold, matrix S−1 exists.

Lemma 1 implies that under the stated assumptions, the
optimal primal and dual variables of (Pθ) vary smoothly with
changes in parameter θ, and the associated sensitivities can
be found via (14). Prior works that compute sensitivities of

optimal primal and dual variables for scalar-parameterized
OPF instances rely on the non-singularity of S [37], [38].

While Assumptions 1–3 seem to be standard in the opti-
mization literature, our recent work on the optimal dispatch
of inverters in distribution grids demonstrated analytically
and numerically that LICQ (Assumption 2) is violated fre-
quently [28]. Instances violating LICQ can be conceived for
the AC-OPF in (8) too [44], [45]. To bring up one such
example, consider a power system where a load bus m is
connected to the rest of the system through another bus n via
a single transmission line (m, n). As bus m is a load bus,
it contributes two equality constraints (6c) and (6d). It can
be shown that if any of the three following scenarios occurs,
LICQ fails: i) the voltage limits in (6g) become binding (above
or below) for both buses m and n; ii) line (m, n) becomes
congested [cf. (6i)] and a voltage limit at bus m becomes
binding; or iii) line (m, n) becomes congested and a voltage
limit at bus n becomes binding. Further detailed examples
for AC-OPF instances violating LICQ can be found in [44],
[45]. Attempting to circumvent LICQ violation via problem
reformulations may be futile as their occurrences depend on
θ, and are thus hard to analyze. Under certain assumptions on
OPF instances and load variations, LICQ occurrences can be
shown to have zero measure [45]. If the required assumptions
are not met, resorting to Fritz-John rather than the KKT
conditions for sensitivity analysis has been proposed [44].
However, before tackling the singularity of S due to LICQ
violation, we must answer question q2).

The implications of a singular S have previously been
investigated in [31] and [40]: When LICQ is violated despite
strict complementarity, the sensitivities of some primal/dual
variables may still exist with respect to a θp. In detail, consider
the set Γ := {γ ∈ R2(Nb+Ng)+L+M : Sγ = Uep}, which
is the solution set of (14). If the n-th entry of γ remains
constant for all γ ∈ Γ, the sensitivity of the n-th entry of
[x(cid:62) λ(cid:62) µ(cid:62)](cid:62) with respect to θp does exist; see [31] and [40]
for physical interpretation and illustrative examples. While a
subset of optimal primal/dual variables may be differentiable
under LICQ violation, explicitly identifying the differentiable
quantities requires instance-based numerical evaluation in [31]
and [40]. Since for training an SI-DNN, we are interested only
in the sensitivities ∇θx, we need to ensure that all solutions
γ ∈ Γ share the same ﬁrst N entries. This is equivalent to
saying that the ﬁrst entries of n are zero for all n ∈ null(S).
The equivalence stems from the fact that if S¯γ = u for a ¯γ,
any other solution to Sγ = u takes the form γ = ¯γ + n
for some n ∈ null(S). The next claim provides sufﬁcient
conditions for the ﬁrst N entries of n to be zero.

Theorem 1. If Assumptions 1 and 3 hold, then ni = 0 for
i = 1, . . . , 2(Nb + Ng) for all n ∈ null(S).

Proof: The claim holds trivially for n = 0. The proof
for non-zero n builds on Assumption 3, and thus the terms
involved in these assumptions are computed for (8) ﬁrst.

∇2

xxL :=

(cid:21)
(cid:20)Z 0
0 0

, ∇xh(cid:96) :=

(cid:21)

(cid:20)2L(cid:96)v
−a(cid:96)

, ∇xgm :=

(cid:21)

(cid:20)2Mmv
0

.

8

Given vector n (cid:54)= 0 such that Sn = 0, partition n conformably
g λ(cid:62) µ(cid:62)](cid:62) as n := [n(cid:62)
to [v(cid:62) x(cid:62)
µ ](cid:62). By the
λ n(cid:62)
n(cid:62)
deﬁnition of S in (14), expanding Sn = 0 yields

v n(cid:62)
xg

Znv + Lλnλ + Mµnµ = 0
A(cid:62)nλ = 0
2L(cid:62)
λ nv − Anxg = 0
µ nv + D(g)nµ = 0.

2D(µ)M(cid:62)

(15a)

(15b)

(15c)

(15d)

Assumption 1 dictates that the ﬁrst and second term in (15d)
have complementary sparsity, so that D(µ)M(cid:62)
µ nv = 0
and D(g)nµ = 0. Recalling the deﬁnition Mµ
:=
(cid:80)M
m=1 Mmve(cid:62)
µ nv = 0 indeed implies
[n(cid:62)
v n(cid:62)
]∇xgm = 0 for all m ∈ A(x), and together with
xg
(15c) ensures

m, equation D(µ)M(cid:62)

(cid:21)

(cid:20) nv
nxg

∈ Z.

(16)

Pre-multiplying (15a)–(15b) by 2n(cid:62)
the two resulting equations yields

v and n(cid:62)

xg and subtracting

2n(cid:62)

v Znv + 2n(cid:62)

v Lλnλ − n(cid:62)
xg
(cid:20)2Mµ
(cid:21)
0

v n(cid:62)
x ]

A(cid:62)nλ

nµ = 0

+[n(cid:62)

(17)

where the second and third term on the left-hand side (LHS)
sum up to zero per (15c). Since D(g)nµ = 0, if gm (cid:54)= 0, then
the m-th entry nµ,m of nµ should be zero. In other words, we
µ 0](cid:62)nµ =
get that nµ,m = 0 for all m /∈ A(x), and thus, 2[M(cid:62)
(cid:80)
m∈A(x) ∇xgmnµ,m. Substituting the latter into (17) gives

2n(cid:62)

v Znv +

(cid:88)

[n(cid:62)

v n(cid:62)
xg

](cid:62)∇xgmnµ,m = 0.

m∈A(x)

The second term on the LHS equals zero due to (16). Therefore
n(cid:62)

v Znv = 0, thus implying

(cid:2)n(cid:62)

v n(cid:62)
xg

(cid:3) ∇2

xxL

(cid:21)

(cid:20) nv
nxg

= 0

which contradicts Assumption 3, unless nv = 0 and nxg = 0.

Thanks to Theorem 1, we can proceed with computing
∇θx by solving (14) even if S is singular. In other words,
Theorem 1 allows us to compute ∇θx even if the LICQ
(Assumption 2) fails. If S† is the pseudo-inverse of S, the
Jacobian matrix Jθ = ∇θx can be computed as the top
2(Nb + Ng) rows of −S†U.

The previous analysis has tacitly presumed the system
Sγ = u has at least one solution for all u ∈ range(U). The
numerical tests of Section VI demonstrate that for the AC-OPF
in (8), the system Sγ = u features a solution indeed.

As discussed earlier, we focus on training an SI-DNN
for predicting generator voltage magnitudes and active power
setpoints. Having solved (14) and found the sensitivity of x
with respect to θ, the sensitivity of active power generation
can be obtained readily using the corresponding entries of xg.
The sensitivity of voltage magnitudes can be derived from the
sensitivities of the real and imaginary components of voltages
with respect to θ. Precisely, the voltage magnitude at bus n is

given by vn = (cid:112)(vr
to θ(cid:96) can be found through the chain rule

n)2 + (vi

n)2 and its sensitivity with respect

(cid:18)

=

∂vn
∂θ(cid:96)

∂vr
n
∂θ(cid:96)
Evaluating the above completes the requirements of sensitivi-
ties for augmenting the SI-DNN training set.

∂vi
n
∂θ(cid:96)

+ vi
n

1
vn

vr
n

.

(cid:19)

From (14), the required sensitivities obviously depend on
values of optimal primal/dual variables of (8). Problem (8)
is a non-convex quadratic program and existing solvers may
converge to a local rather than a global solution. Albeit the
previous sensitivity analysis is valid even for local solutions,
the performance of the trained DNN will be apparently sub-
optimal. To train an SI-DNN to predict global OPF solutions,
we next extend the analysis to the SDP relaxation of (8).

V. SDP RELAXATION OF THE AC-OPF

In pursuit of globally optimal AC-OPF schedules, the non-

convex QCQP of (8) can be relaxed to the SDP [2]

min
xg,V(cid:23)0

a(cid:62)
0 xg

s.to Tr(L(cid:96)V) = a(cid:62)

Tr(MmV) ≤ d(cid:62)

(cid:96) xg + b(cid:62)

(cid:96) θ,
mθ + fm,

(18a)

(18b)

(18c)

∀(cid:96) : λ(cid:96)
∀m : µm.

Problem (18) is equivalent to (8) if matrix V is rank-1 at
optimality, in which case the SDP relaxation is deemed as
exact. The relaxation turns out to be exact for several power
networks and practical loading conditions; see [5] for a review
the V
of related analyses. When the relaxation is exact,
minimizer of (18) can be expressed as V = vv(cid:62).

We brieﬂy review how the solution (xg, v; λ, µ) obtained
from the SDP formulation of (18) satisﬁes the ﬁrst-order
optimality conditions for the non-convex QCQP in (8) as well.
To this end, it is not hard to derive the dual program of (18):

9

(14). This is advantageous as the QCQP features differentiable
objective and constraint functions. The obtained sensitivity
formulae can be evaluated at the AC-OPF solution provided
by any nonlinear programming solver, although such solution
may be only locally optimal. To compute a globally optimum
AC-OPF solution, we propose using (18) instead. If the
the obtained solution is globally
SDP relaxation is exact,
optimal, while the sensitivity formulae derived from QCQP
can still be used. Our suggested workﬂow avoids computing
the sensitivities of the SDP formulation for the AC-OPF:
Even though differentiating through convex cone constraints
is possible [41], it can be perplexing.

Remark 1. The aforesaid workﬂow runs the SDP-based solver
to obtain an OPF solution, but computes its sensitivities using
the convenient formulae associated with the QCQP-OPF. This
is to ensure global optimality if the SDP relaxation is exact.
An alternative way to check global optimality is to follow the
workﬂow of [46]: Obtain an OPF solution via a mature OPF
solver (e.g., QCQP or MATPOWER), and use the optimality
conditions of
the SDP-based OPF to check whether the
obtained QCQP- or MATPOWER-based solution is globally
optimal. Nevertheless, this optimality check relies on sufﬁcient
conditions. As a result, if the QCQP- or MATPOWER-based
solution does not pass the global optimality test (that is indeed
the case for the IEEE 300-bus system [46]), one may still
have to run the SDP-based OPF solver in pursuit of a better
solution or a global optimality guarantee.

VI. NUMERICAL TESTS

The novel SI-DNN approach was evaluated using the IEEE
39-bus, the IEEE 118-bus, and the Illinois 200-bus system.
Datasets were generated using either the nonlinear OPF MAT-
POWER or the globally optimal SDP-based solver.

max
λ,µ

−

L
(cid:88)

(cid:96)=1

λ(cid:96)b(cid:62)

(cid:96) θ −

M
(cid:88)

m=1

µm(d(cid:62)

mθ + fm)

(19a)

A. DNN Architecture and Training

s.to a0 =

Z :=

L
(cid:88)

(cid:96)=1
L
(cid:88)

(cid:96)=1

λ(cid:96)a(cid:96)

λ(cid:96)L(cid:96) +

M
(cid:88)

m=1

µ ≥ 0.

µmMm (cid:23) 0

(19b)

(19c)

(19d)

The optimality conditions for the SDP primal-dual pair (18)–
(19) then include:

i) Primal feasibility (18b)–(18c) implies (8b)–(8c).
ii) Dual feasibility (19d) applies to (8) as well.
iii) Complementary slackness for (18c) applies to (8c).
iv) Complementary slackness for (19c) gives Tr(VZ) = 0
or Zv = 0, which along with (19b) yield the Lagrangian
optimality conditions for the QCQP shown in (9).
Therefore (xg, v; λ, µ) is a stationary point for the QCQP in
(8). Because it further attains the optimal cost for the relaxed
problem in (18), it is in fact the globally optimal for (8).

To recapitulate, we have used the non-convex QCQP for-
mulation of the AC-OPF to derive the sensitivity formulae of

n = qd

To ease the implementation and without loss of generality,
we assumed that buses hosting generators do not host loads,
i.e., pd
n = 0 for all n ∈ Ng. As discussed at the end
of Section III, the DNN input θ consists of the 2(Nb − Ng)
(re)active power demands at load buses. The DNN output is
the setpoints for active power and voltage magnitude (pg
n, vn)
at all generators n ∈ Ng excluding pg
1 for the slack bus. We
collect these output quantities in ˇxθ, a subvector of xθ.

Both for P-DNN and SI-DNN, we chose a feed-forward
fully-connected architecture. For the number of hidden layers
being K, denote the number of neurons in layer k by uk,
with input dimension u0 = 2(Nb − Ng) and output dimension
uK+1 = 2Ng − 1. To explicitly constrain DNN outputs as
per (6e) and (6g), the output layer uses tanh as its activation
function, while all other layers use ReLU. For DNN training
and evaluation, labels ˇxθ were suitably scaled within [−1, 1].
We built all DNNs using the TensorFlow 2.0 python
platform alongside Keras libraries. Training an SI-DNN
deviates from the default routine as gradient updates are
implemented separately; see Appendix A for key differences.
For DNN training, at every weight-update step, the gradients

TABLE I
AVERAGE TEST MSE [× 10−3] FOR PREDICTING MATPOWER
SOLUTION ON IEEE 39-BUS SYSTEM, AND WEIGHTING FACTOR ρ

Training
Size
100
1000
5000
10000

MSE

P-DNN
2.80
1.00
0.54
0.19

SI-DNN
1.50
0.59
0.32
0.14

ρ
10
2
1
0.2

computed via the procedure in the appendix are passed to
the Adam optimizer. For all tests, optimizer Adam was used
with an exponential decay reducing the rate to 85% every 250
epochs. The initial learning rate will be reported later. DNNs
were compiled using Jupyter notebook on a 2.7 GHz Intel
Core i5 computer with 8 GB RAM.

B. Learning Locally Optimal OPF Solutions

We ﬁrst trained DNNs towards predicting MATPOWER
AC-OPF minimizers. We contrasted SI-DNN with P-DNN in
terms of the MSE and the related training times. With the
primary goal of improving sample efﬁciency, the numerical
tests emphasize on performance evaluation for relatively small
training datasets. Nevertheless, to gain insight on the effect of
the training dataset size, we ﬁrst present tests using larger
training datasets.

1) Tests on IEEE 39-bus system with large training
datasets: The network parameters and nominal
loads for
the IEEE 39-bus system were fetched from MATPOWER
casefile [1]. The 39-bus system hosts Ng = 10 generators.
The benchmark system has loads on two of the generator
buses. Removing these, there are 29 load buses. To build a
dataset for DNN testing and training, a set of 12,000 random
θ ∈ R2·29 was sampled. The corresponding ˇxθ’s were obtained
via MATPOWER. The dataset thus obtained was partitioned
into training, cross-validation, and testing sets of sizes 10,000;
1,000; and 1,000, respectively. If infeasibility is encountered
for some θ’s, such instances were omitted from the dataset.
To represent various demand levels, we sampled the 12,000
random θ’s by scaling the benchmark demands entry-wise by
a scalar drawn independently and uniformly within [0.8, 1.2].
For the aforementioned sampling, all 12,000 OPF instances
were feasible. Since the generator cost functions are identical
in the benchmark system, a uniform active power cost was
used for all generators. The default OPF formulation of MAT-
POWER deviates from the QCQP in (P1). These differences
introduce some nuances in building the linear system of (14)
for computing sensitivities; see Appendix B for details. Having
the aforementioned dataset {(θs, Jθs , ˇxθs )}12000
the
s=1 ,
built
architectures for SI-DNN and P-DNN were determined next.
Based on preliminary tests, identical architectures were chosen
for P-DNN and SI-DNN with K = 4 hidden layers with
uk = 256 neurons for k = 1, . . . , 4. Preliminary tests showed
negligible effect on P-DNN performance if the number of
layers is reduced to three. Nevertheless, the architecture for the
two DNNs was kept identical to ensure equal expressibility.

The performance of the two DNNs was evaluated in terms of
the MSE for training sizes (100, 1000, 5000, 10000) sampled

10

Fig. 2. Average training and testing errors for different training sizes (top);
and errors across epochs for different runs with training size 10 (bottom).

TABLE II
AVERAGE TEST MSE [× 10−3] AND TRAINING TIME [IN SEC] FOR
PREDICTING MATPOWER SOLUTION ON IEEE 39-BUS SYSTEM

Training
Size
10
50
100
250

P-DNN

SI-DNN

MSE
8.6
4.3
3.2
1.9

Time MSE
3.3
2.1
2.0
2.0

738
739
747
302

Time
746
756
776
332

from the complete training set of size 10000. The batch-
size for all tests was ﬁxed to 100. The cross-validation set
was used to determine the initial learning rate (ILR), epochs
needed, and the factor ρ in (2). The ILR for training sizes
(100, 1000, 5000, 10000) was (5, 5, 10, 50) × 10−4 and the
epochs needed were (2000, 500, 500, 250) for both DNNs. The
decrease in the training epochs needed is due to the increase
in gradient steps per epoch for larger training sizes with ﬁxed
batch size. The MSEs obtained by the two DNNs averaged
over the 1000 test instances are provided in Table I alongside
the factor ρ used for different training sizes. As anticipated,
the test errors for both DNNs decrease for larger training
sizes. However, the SI-DNN consistently outperforms the P-
DNN with the improvement being more pronounced at smaller
training sizes. Interestingly, a decreasing trend in the suitable
choice of ρ was obtained from cross-validation indicating that
as the training samples become abundant, sensitivity infor-
mation seems to be becoming less important. The remaining
numerical tests explicitly focus on small training sizes. For
simplicity, hereon we ﬁx the ILR to 5 × 10−4 and ρ = 20.

2) Tests on IEEE 39-bus system with small

datasets: A dataset {(θs, Jθs , ˇxθs )}1000

training
s=1 was created follow-

1050100250Training size10-810-610-410-2100MSEP-DNN trainingP-DNN testingSI-DNN trainingSI-DNN testingxx010002000300040005000Epochs10-810-610-410-2100MSEP-DNN trainingP-DNN testingSI-DNN trainingSI-DNN testingTABLE III
AVERAGE TEST MSE [× 10−3] AND TRAINING TIME [IN SEC] FOR
PREDICTING MATPOWER SOLUTION

.
n
i
a
r
T

e
z
i
S

25
50
100

IEEE 118-bus

Illinois 200-bus

P-DNN

SI-DNN

P-DNN

SI-DNN

MSE
1.8
1.7
1.6

Time MSE
1.1
1.1
0.9

447
458
463

Time MSE
0.19
0.15
0.09

483
527
610

Time MSE
0.04
0.04
0.06

452
456
471

Time
491
524
608

ing the methodology delineated in the previous subsection to
evaluate the two DNNs when the training sizes are varied
over a data-scarce regime. The evaluation was performed as
follows. First, for a training size of 10, we created 20 different
training sets by sampling 10 OPF instances from the dataset
without replacement. For each of these 20 times or runs, the
OPF instances not sampled for training consisted the testing
sets. We then separately trained P-DNN and SI-DNN on
these 20 sets. For the training sizes of (50, 100, 250), we had
(20, 10, 4) runs, respectively. For training sizes (10, 50, 100),
the entire training set was used for gradient computation at
each step, with the total epochs being 5000. When the training
size was 250, the batch-size was ﬁxed to 100, and total epochs
to 2000. The training and testing MSE loss for all training
sizes, and runs are shown in Fig. 2 (top). For the tests with
training size 10, the evolution of DNN errors are shown in
Fig. 2 (bottom). The average test MSE and training times for
the two DNNs are shown in Table II. From Fig. 2 (top),
the gap
we observe as anticipated,
between training and testing loss decreases for larger training
size. Further, the errors for different runs are well clustered,
indicating a numerically stable DNN implementation. From
Table II, it is fascinating to note that the test loss attained
by SI-DNN is much lower than P-DNN, especially at smaller
training sets. For instance, the P-DNN requires 100 samples to
roughly attain the average test MSE which the SI-DNN attains
with 10 samples. The lower MSE for P-DNN with training size
250 is a repercussion of not updating ρ for varying training
sizes, which was avoided for simplicity. It is worth stressing
that the improvement in sample efﬁciency comes at modest
increase in training time.

that for both DNNs,

3) Tests on other benchmarks: The DNN architecture cho-
sen for the other two power systems was similar to the
IEEE 39-bus case with the differences being in the number
of neurons per layer. Speciﬁcally, the DNNs used for the
118- and 200-bus systems had 512 neurons in hidden layers,
with the input (output) layers having 128 (107), and 302 (97)
neurons, respectively. For each of these systems, we created
a dataset with 500 feasible1 random demands generated by
scaling the nominal demands entry-wise by factors drawn
uniformly from [0.7,1.3]. The linear cost coefﬁcients from the
respective benchmark systems were retained as cp’s, while the
reactive power cost coefﬁcients were set to zero. All DNNs
were evaluated for ﬁve runs, with training sizes of 25, 50, and
100. Table III summarizes the obtained results.

1To obtain a dataset of 500 instances, the OPF was solved for 550 instances
and the ﬁrst 500 feasible instances were retained. For the 118-bus system, all
instances were feasible while for the 200-bus system, four infeasible instances
were encountered.

11

TABLE IV
AVERAGE TIME [IN SEC] TO SOLVE AC-OPF, tOPF ; COMPUTE
SENSITIVITIES, tSA ; AND OBTAIN DNN PREDICTIONS FOLLOWED BY
RUNNING AC POWER FLOW, tINFER

Test System
39-bus
118-bus
200-bus

tOPF
0.1229
0.2577
0.3032

tsa
0.0034
0.0260
0.0811

tinfer
0.0039
0.0050
0.0078

TABLE V
AVERAGE TEST MSE [× 10−3] FOR PREDICTING SDP SOLUTIONS, AND
CONSTRAINT VIOLATION STATISTICS ON THE IEEE 39-BUS SYSTEM

Train.
(c)
Size
10
9.78
3.35
50
7.38
2.06
1.96
6.87
100
(a) #violations /instance; (b) max. violation; (c) mean violation [×10−4]

P-DNN
(a)
2.61
2.45
2.59

(c) MSE
0.91
0.62
0.67

SI-DNN
(b)
0.37
0.27
0.27

MSE
6.3
3.6
2.5

(b)
0.50
0.55
0.53

(a)
2.52
2.58
2.52

Having evaluated the improvement

in MSE brought by
the sensitivity-informed learning approach, we next assessed
the additional time-complexity introduced for computing the
desired sensitivities. Speciﬁcally, while building the datasets
for the IEEE 39-bus, the IEEE 118-bus, and the Illinois 200-
bus system, we computed: i) the average time tOPF taken by
MATPOWER to solve an OPF instance; ii) the average time tsa
required for computing the Jacobian matrix Jθs using (14)2;
and iii) the average time tinfer needed to obtain a complete OPF
minimizer using SI-DNN during the inference phase. To do the
latter, we summed up the time taken for evaluating SI-DNN
predictions and the time needed to evaluate a corresponding
AC power ﬂow solution using MATPOWER. It must be noted
that evaluating tinfer is merely to assess an approximate speed-
up offered by the DNNs over conventional OPF solvers.
It does not constitute a rigorous comparison since neither
optimality nor feasibility is guaranteed for DNN predictions.
The aforementioned times are reported in Table IV. It
is
exciting to observe that while the SI-DNN approach can reduce
the training size requirement by up to a factor of 10, evaluating
sensitivities for training the SI-DNN requires substantially less
time than solving an OPF instance. Finally, the average speed-
up factor tOPF/tinfer obtained for the 39-, 118-, and 200-bus
systems was approximately 34, 63, and 52, respectively.

C. Learning Globally Optimal OPF Solutions

n = 0.1cp

The SI-DNN was evaluated towards predicting the mini-
mizer of an SDP relaxation-based OPF solver for the IEEE
39-bus system. A uniform active power cost was used for all
generators while the reactive power cost coefﬁcients were set
as cq
n. To build a dataset, a set of 1, 000 random
θ(cid:48)s was sampled as explained earlier. The corresponding ˇxθ’s
were obtained by solving (18) using the MATLAB-based
optimization toolbox YALMIP with SDP solver MOSEK [47].
For all SDP instances, the ratio of the second largest eigen-
value of matrix V to the largest eigenvalue was found to

2For improved numerical performance, matrix S was stored as a sparse

matrix and (14) was solved using MATLAB’s command lsqminnorm.

TABLE VI
AVERAGE TEST MSE [×10−4] FOR PREDICTING MATPOWER SOLUTION
AND TRAINING SET GENERATION TIME-BUDGET tGEN

Benchmark

39-bus

118-bus

200-bus

tgen
[sec]
1.2
6.2
12.3
30.7
6.45
12.89
25.77
7.58
15.16
30.32

P-DNN
MSE
0.86
0.49
0.33
0.17
17.50
17.84
17.01
1.83
1.46
0.91

SI-DNN
MSE
0.24
0.21
0.19
0.12
10.80
12.94
7.82
0.32
0.45
0.57

Improvement
%
72.1
57.1
40.6
29.4
38.3
27.5
54.1
82.5
69.2
37.4

lie in [3 · 10−7, 1 · 10−4]; numerically indicating an exact
relaxation. Thus, the eigenvector corresponding to the largest
eigenvalue was deemed as the optimal voltage v. If instances
with inexact relaxation are encountered, they can be omitted
from the dataset. As with learning MATPOWER solutions,
the sample efﬁciency of SI-DNN was found superior to P-
DNN in learning globally optimal OPF solutions; see Table V
for the average MSE attained during testing. The presented
results with local and global OPF solvers demonstrate that SI-
DNN yields a dramatic improvement in generalizability. The
feasibility statistics included in Table V are elaborated upon
in Section VI-E.

D. DNN Performance Evaluation under a Time Budget

Tables I–III and V attest the improved sample efﬁciency of
sensitivity-informed over conventional training under various
dataset sizes, benchmark networks, and OPF solvers. This
section exempliﬁes how these results translate to gains in MSE
for a ﬁxed time budget. Speciﬁcally, P-DNN and SI-DNN were
compared when allotted identical times to complete training.
This evaluation was carried out for the settings described in
Section VI-B. For all tests, the number of epochs for P-DNN
was kept ﬁxed as provided in Section VI-B, while the number
of epochs for SI-DNN was reduced, so as to match the training
time of the P-DNN. Next, a common time budget tgen was
ﬁxed for creating the training datasets. Based on Table IV, the
training sizes for P-DNN and SI-DNN can be approximately
computed as tgen/tOPF and tgen/(tOPF + tsa); implying smaller
training sets for the SI-DNN. The test MSEs obtained for
the aforementioned setup are provided in Table VI. It was
observed that with identical time budgets, an SI-DNN yields
test MSEs that are 28-83% less compared to P-DNN.

E. Assessing Feasibility of DNN Predictions

While emphasis has been on MSE,

the importance of
satisfying constraints cannot be undermined. To this end,
we tested the feasibility of SI-DNN OPF predictions using
the following metrics. For each of the DNNs, given a test
input and the associated DNN prediction, an AC power ﬂow
solution was obtained using MATPOWER. For each instance,
the inequalities in (8c) not directly enforced by the tanh
activation were evaluated. These included voltage limits on
load buses, line ﬂow limits, generator reactive power limits,

12

TABLE VII
(a) AVERAGE VIOLATIONS PER INSTANCE; (b) MAXIMUM VIOLATION;
AND (c) MEAN VIOLATION [×10−4] FOR PREDICTING MATPOWER
SOLUTION

Benchmark

39-bus

200-bus

Train.
Size
10
50
100
250
25
50
100

P-DNN
(b)
1.01
0.95
0.71
0.51
1.42
1.19
1.15

(a)
2.29
2.45
2.31
2.23
10.43
9.08
10.89

(c)
15
9.55
7.65
6.99
8.10
6.17
7.17

SI-DNN
(b)
0.33
0.24
0.26
0.28
1.37
1.14
1.24

(a)
1.91
2.14
1.96
1.87
4.99
3.69
2.74

(c)
7.66
6.30
6.26
6.89
4.27
4.64
4.71

TABLE VIII
(a) AVERAGE VIOLATIONS PER INSTANCE; (b) MAXIMUM VIOLATION;
AND (c) MEAN VIOLATION [×10−3] FOR PREDICTING MATPOWER
SOLUTION ON IEEE 118-BUS SYSTEM

Constraint
Set

Full

Reduced

Train.
Size
25
50
100
25
50
100

P-DNN
(b)
1.08
1.02
1.18
0.73
0.63
1.17

(a)
7.97
8.24
8.45
2.12
1.92
2.01

(c)
1.90
1.60
1.80
0.32
0.21
0.34

SI-DNN

(a)
9.28
10.20
10.09
1.67
1.65
1.77

(b)
1.44
0.97
0.93
0.66
0.68
0.78

(c)
3.90
3.30
2.20
0.14
0.14
0.12

and the slack bus active power limits, totalling to 126, 424,
and 647 constraints for the 39-, 118-, and 200-bus system,
respectively. For suitable scaling, the violations in ﬂows and
generation were normalized by the maximum limit. To be
speciﬁc, a normalized violation of 10−3 in generator power
injection translates to a violation of 0.1% of the maximum
power capacity of that generator. Voltage violations were
maintained in pu.

We ﬁrst evaluated the constraint violations caused by SI-
DNN and P-DNN predictions while learning globally optimal
OPF solutions obtained from the SDP-based solver. The as-
sessment was carried out on the test instances that remained
after sampling training sets of different sizes from the 1,000
random instances [cf. Section VI-C]. For different training
sizes, Table V lists: a) the average number of violations
exceeding a normalized magnitude of 10−6 per test instance;
b) the maximum constraint violation observed; and c) the
violations averaged over all constraints and test instances. In-
terestingly, while both DNNs incur similar count of violations,
SI-DNN reduces the maximum violation by half and the mean
violation to less than one third. We further investigated into
the speciﬁc constraints being violated by SI-DNN predictions.
Interestingly, there were just 5 constraints frequently violated.
Three of these were minimum reactive power generation,
and the remaining were maximum active power of the slack
generator and a line ﬂow limit.

We repeated the previous feasibility analysis for the DNNs
aimed at learning locally optimal OPF solutions from MAT-
POWER. The constraint violation statistics obtained for the
IEEE 39-bus and the Illinois 200-bus system, provided in
Table VII, consistently demonstrate the improvements yielded
by the SI-DNN approach for different training sizes. Table VIII
reports the same statistics for the IEEE 118-bus system while
predicting the MATPOWER solution. The numerical obser-
vations for the IEEE 118-bus system do not align with the

results for other benchmark networks. The statistics provided
in the top part of Table VIII exhibit much higher constraint
violations for both P-DNN and SI-DNN; note that the mean
violations are of the order 10−3 as opposed to 10−4 for other
networks. Moreover, SI-DNN performs worse than P-DNN on
several metrics. Spurred by the exceptionally high constraint
violations, we investigated the individual constraints being
violated. It was found that several generator reactive power
limits were being consistently violated by both P-DNN and
SI-DNN. It turns out that these limits were binding for all
the random OPF scenarios in the training and testing datasets.
For benchmarks that exhibit such patterns with certain dispatch
quantities being ﬁxed across scenarios, it may be prudent to
set them at the respective values and solve a reduced OPF.
To emphasize on the violation statistics for the non-trivial
constraints, we computed the feasibility metrics on a reduced
set of constraints not including those reactive power limits that
were consistently binding. The obtained results shown at the
bottom of Table VIII corroborate the superior performance of
SI-DNN over P-DNN.

VII. CONCLUSIONS

This work has built on the fresh idea of sensitivity-
informed training for learning the solutions of arbitrary AC-
OPF formulations. It comprehensively delineated the steps
for computing the involved sensitivities using the optimal
primal/dual solutions, which are readily available by AC-OPF
solvers. Such sensitivities of the primal AC-OPF solutions
have been shown to exist under mild assumptions, while
their computation is as simple as solving a system of linear
equations with multiple right-hand sides. The approach is
quite general since the OPF solutions comprising the train-
ing dataset can be obtained by off-the-shelf nonlinear OPF
solvers or modern conic relaxation-based schemes. It is also
worth stressing that sensitivity-informed training can readily
complement other existing learn-to-OPF methodologies. Ex-
tensive numerical tests on three benchmark power systems
have demonstrated that with a modest increase in training
time, SI-DNNs attain the same prediction performance as
conventionally trained DNNs by using roughly only 1/10 to 1/4
of the training data. Such improvement on sample efﬁciency
reduces the time needed for generating training datasets, and
is thus, relevant to delay-critical power systems applications.
Furthermore, SI-DNN predictions turn out to feature better
constraint satisfaction capabilities too. Sensitivity-informed
learning forms the solid foundations for several exciting and
practically relevant research directions, such as warm-starting
key optimal primal/dual variables to accelerate decentralized
OPF solvers and predicting active constraints.

A. Python Implementation for SI-DNN

APPENDIX

A typical implementation example for computing the gra-
dient of the MSE loss in P-DNN with respect to the DNN
weights (which are the trainable variables) involves
with tensorflow.GradientTape() as tape:

pred_x = model(theta)

13

loss = keras.losses.MSE(xlabel,pred_x)

model_gradients=tape.gradient(loss,

model.trainable_variables)

where model represents the DNN and GradientTape
computes the desired gradient. In transitioning to SI-DNN,
we ﬁrst need to compute the gradient of the DNN output
pred_x with respect to its input theta to deﬁne the loss.
We then compute the gradients of the two loss terms with
respect to the DNN weights. This can be implemented using
nested GradientTape as
with tensorflow.GradientTape() as tape:

with tensorflow.GradientTape() as tape2:

tape2.watch(theta)
pred_x=model(theta)
Ploss=keras.losses.MSE(xlabel,pred_x)

J_model=tape2.batch_jacobian(pred_x,

theta)

J_flat=tf.keras.backend.reshape(fgrad,

shape=(1,))
SI_loss=keras.losses.MSE(J_flat,J_label)
total_loss=P_loss+rho*SI_loss

model_gradients=tape.gradient(loss,

model.trainable_variables)

where the inner tape computes the sensitivity of DNN to
compute the overall SI-DNN loss, while the outer tape
computes the gradients for weight updates.

B. Sensitivity computation with MATPOWER

While solving the AC-OPF instances with MATPOWER,
we used the Cartesian coordinate system, and ﬂow limits
were imposed on squared currents. For computing the desired
sensitivities, we ﬁrst need to build the linear system (14),
which requires the optimal dual variables, constraint function
values, and the derivatives of the constraint functions with
respect to the optimization variables. Although MATPOWER
can deal with the AC-OPF posed with voltages in Cartesian
coordinates, it slightly differs from the QCQP in (6) as follows:
a1) MATPOWER enforces power ﬂow equations as in (6a)–
(6d). Different from (6e)–(6f) however, MATPOWER
n ≤ ¯pg
poses generator (re)active power limits as pg
n.
n
Thus, the related ∇vgm becomes zero and ∇xg gm be-
comes a signed canonical vector corresponding to bus n.
a2) MATPOWER constraints voltages, rather than squared
voltages as in (6g). While the two versions are equiv-
alent, the related derivatives ∇vg apparently differ. The
derivatives of non-squared magnitudes can be found using
the chain rule as ∇vvn = ∂vn
n and
∂v2
n
∇vv2
n = 2Mvn v from (4).

n = 1
vn

∇vv2

∇vv2

≤ pg

a3) Different from (6h), MATPOWER sets the voltage angle
reference to zero by enforcing arctan(vi
N +1) = 0.
Fortunately, simply setting the imaginary part vi
N +1 to
zero is equivalent, and the gradients of these two formu-
lations agree. Thus, despite the difference in formulation,
we use (6h) wherever needed in building (14).

N +1/vr

a4) Finally, MATPOWER poses ﬂow limits on both the
sending and receiving ends of each line, thus doubling
the number of constraints in (6i). The matrices Mimn
can be built using the from bus and to bus admittances
obtained via the MATPOWER command makeYbus().

REFERENCES

[1] R. D. Zimmerman, C. E. Murillo-Sanchez, and R. J. Thomas, “MAT-
POWER: steady-state operations, planning and analysis tools for power
systems research and education,” IEEE Trans. Power Syst., vol. 26, no. 1,
pp. 12–19, Feb. 2011.

[2] X. Bai, H. Wei, K. Fujisawa, and Y. Yang, “Semideﬁnite programming
for optimal power ﬂow problems,” Intl. Journal of Electric Power &
Energy Systems, vol. 30, no. 6, pp. 383–392, 2008.

[3] S. Bose, D. Gayme, K. Chandy, and S. Low, “Quadratically constrained
quadratic programs on acyclic graphs with application to power ﬂow,”
IEEE Trans. Control of Network Systems, vol. 2, no. 3, pp. 278–287,
Sep. 2015.

[4] R. Madani, S. Sojoudi, and J. Lavaei, “Convex relaxation for optimal
power ﬂow problem: Mesh networks,” IEEE Trans. Power Syst., vol. 30,
no. 1, pp. 199–211, Jan. 2015.

[5] S. Low, “Convex relaxation of optimal power ﬂow – Part II: Exactness,”
IEEE Trans. Control of Network Systems, vol. 1, no. 2, pp. 177–189,
Jun. 2014.

[6] A. S. Xavier, F. Qiu, and S. Ahmed, “Learning to solve large-scale
security-constrained unit commitment problems,” INFORMS Journal on
Computing, pp. 1–18, Oct. 2020, (early access).

[7] F. Fioretto, T. W. Mak, and P. V. Hentenryck, “Predicting AC optimal
power ﬂows: Combining deep learning and Lagrangian dual methods,”
in AAAI Conf. on Artiﬁcial Intelligence, New York, NY, Feb. 2020.
[8] X. Pan, T. Zhao, and M. Chen, “DeepOPF: Deep neural network for DC
optimal power ﬂow,” in Proc. IEEE Intl. Conf. on Smart Grid Commun.,
Beijing, China, Oct. 2019, pp. 1–6.

[9] T. Zhao, X. Pan, M. Chen, A. Venzke, and S. H. Low, “Deepopf+: A
deep neural network approach for DC optimal power ﬂow for ensuring
feasibility,” in Proc. IEEE Intl. Conf. on Smart Grid Commun., Tempe,
AZ, Nov. 2020, pp. 1–6.

[10] X. Pan, M. Chen, T. Zhao,

“Deepopf: A
feasibility-optimized deep neural network approach for AC optimal
power ﬂow problems,” 2020, (preprint). [Online]. Available: https:
//arxiv.org/abs/2007.01002

and S. H. Low,

[11] N. Guha, Z. Wang, M. Wytock, and A. Majumdar, “Machine learning
for AC optimal power ﬂow,” 2019, climate Change Workshop at ICML
2019. [Online]. Available: https://arxiv.org/abs/1910.08842

[12] A. Zamzam and K. Baker, “Learning optimal solutions for extremely
fast AC optimal power ﬂow,” in Proc. IEEE Intl. Conf. on Smart Grid
Commun., Tempe, AZ, Nov. 2020, pp. 1–6.

[13] Y. Zhao and B. Zhang, “Deep learning in power systems,” in Advanced
Data Analytics for Power Systems, A. Tajer, S. M. Perlaza, and H. V.
Poor, Eds. Cambridge, UK: Cambridge University Press, May 2021.
[14] D. Owerko, F. Gama, and A. Ribeiro, “Optimal power ﬂow using graph
neural networks,” in Proc. IEEE Intl. Conf. on Acoustics, Speech, and
Signal Process., Barcelona, Spain, May 2020, pp. 5930–5934.

[15] S. Gupta, V. Kekatos, and M. Jin, “Communication-limited inverter
control using deep neural networks,” in Proc. IEEE Intl. Conf. on Smart
Grid Commun., Tempe, AZ, Nov. 2020, pp. 1–6.

[16] H. Lange, B. Chen, M. Berges, and S. Kar, “Learning to solve
AC optimal power ﬂow by differentiating through holomorphic
embeddings,” 2020, (submitted). [Online]. Available: https://arxiv.org/
abs/2012.096224

[17] S. Gupta, S. Misra, D. Deka, and V. Kekatos, “DNN-based policies
for stochastic AC-OPF,” in Proc. Power Syst. Comput. Conf., Porto,
Portugal, Jun. 2021, (to appear also in the Elsevier Electric Power
Systems Research). [Online]. Available: https://www.faculty.ece.vt.edu/
kekatos/papers/PSCC2022a.pdf

[18] S. Gupta, V. Kekatos, and M. Jin, “Controlling smart

inverters
using proxies: A chance-constrained DNN-based approach,” IEEE
Trans. Smart Grid, May 2021,
[Online]. Available:
https://arxiv.org/abs/2105.00429

(submitted).

[19] Y. Chen, S. Lakshminarayana, C. Maple, and H. V. Poor, “A
meta-learning approach to the optimal power ﬂow problem under
topology reconﬁgurations,” 2020,
[Online]. Available:
https://arxiv.org/abs/2012.11524

(preprint).

[20] Y. Chen and B. Zhang, “Learning to solve network ﬂow problems
[Online]. Available: https:

via neural decoding,” 2020, preprint.
//arxiv.org/abs/2002.04091

[21] D. Deka and S. Misra, “Learning for DC-OPF: Classifying active sets
using neural nets,” in IEEE PowerTech, Milan, Italy, Jun. 2019, pp. 1–6.
[22] M. Yatin Nandwani, Abhishek Pathak and P. Singla, “A primal dual
formulation for deep learning with constraints,” in Proc. of Adv. Neural
Inf. Process. Syst., Vancouver, Canada, Dec. 2019, pp. 12 157–12 168.

14

[23] L. Zhang, Y. Chen, and B. Zhang, “A convex neural network solver for
DCOPF with generalization guarantees,” 2020, (submitted). [Online].
Available: https://arxiv.org/abs/2009.09109

[24] L. Zhang, G. Wang, and G. B. Giannakis, “Real-time power system state
estimation and forecasting via deep unrolled neural networks,” IEEE
Trans. Signal Processing, vol. 67, no. 15, pp. 4069–4077, Aug. 2019.
[25] Q. Yang, A. Sadeghi, G. Wang, G. B. Giannakis, and J. Sun,
“Robust PSSE using graph neural networks for data-driven and
topology-aware priors,” 2020, (submitted). [Online]. Available: https:
//arxiv.org/abs/2003.01667

[26] M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics-informed
neural networks: A deep learning framework for solving forward and
inverse problems involving nonlinear partial differential equations,”
Journal of Computational Physics, vol. 378, pp. 686–707, 2019.
[27] G. S. Misyris, A. Venzke, and S. Chatzivasileiadis, “Physics-informed
neural networks for power systems,” in Proc. IEEE PES General
Meeting, Montreal, Canada, Aug. 2020, pp. 1–5.

[28] M. K. Singh, S. Gupta, V. Kekatos, G. Cavraro, and A. Bern-
stein, “Learning to optimize power distribution grids using sensitivity-
informed deep neural networks,” in Proc. IEEE Intl. Conf. on Smart
Grid Commun., Tempe, AZ, Nov. 2020, pp. 1–6.

[29] J. F. Bonnans and A. Shapiro, Perturbation Analysis of Optimization
Problems. New York, NY: Springer Science & Business Media, 2000.
[30] A. V. Fiacco, “Sensitivity analysis for nonlinear programming using
penalty methods,” Mathematical Programming, vol. 10, no. 1, pp. 287–
311, Dec. 1976.

[31] A. J. Conejo, E. Castillo, R. Minguez, and R. Garcia-Bertrand, Decom-

position Techniques in Mathematical Programming. Springer, 2006.

[32] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropoulos,
“Learning to optimize: Training deep neural networks for interference
management,” IEEE Trans. Signal Processing, vol. 66, no. 20, pp. 5438–
5453, Oct. 2018.

[33] F. Borrelli, A. Bemporad, and M. Morari, “Geometric algorithm for
multiparametric linear programming,” Journal of Optimization Theory
and Applications, vol. 118, no. 3, pp. 515–540, Sep. 2003.

[34] S. Taheri, M. Jalali, V. Kekatos, and L. Tong, “Fast probabilistic hosting
capacity analysis for active distribution systems,” IEEE Trans. Smart
Grid, 2020, (early access).

[35] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind,
“Automatic differentiation in machine learning: A survey,” J. Mach.
Learn. Res., vol. 18, no. 1, pp. 5595–5637, Jan. 2017.

[36] V. Kekatos, G. Wang, H. Zhu, and G. B. Giannakis, “PSSE redux:
Convex relaxation, decentralized, robust, and dynamic approaches,” in
Advances in Power System State Estimation, M. El-Hawary, Ed. Wiley,
2021.

[37] K. Almeida, F. Galiana, and S. Soares, “A general parametric optimal
power ﬂow,” IEEE Trans. Power Syst., vol. 9, no. 1, pp. 540–547, Feb.
1994.

[38] V. Ajjarapu and N. Jain, “Optimal continuation power ﬂow,” Electric

Power Systems Research, vol. 35, no. 1, pp. 17–24, Oct. 1995.

[39] K. Almeida and R. Salgado, “Optimal power ﬂow solutions under
variable load conditions,” IEEE Trans. Power Syst., vol. 15, no. 4, pp.
1204–1211, Nov. 2000.

[40] E. Castillo, A. J. Conejo, C. Castillo, R. Minguez, and D. Ortigosa,
“Perturbation approach to sensitivity analysis in mathematical program-
ming,” Journal of Optimization Theory and Applications, vol. 128, no. 1,
pp. 49–74, Jan. 2006.

[41] A. Agrawal, S. Barratt, S. Boyd, E. Busseti, and W. M. Moursi,
“Differentiating through a cone program,” 2020, (submitted). [Online].
Available: https://arxiv.org/abs/1904.09043

[42] B. Amos and J. Z. Kolter, “OptNet: Differentiable optimization as a
layer in neural networks,” in Intl. Conf. on Machine Learning, Sydney,
NSW, Australia, 2017, p. 136–145.

[43] D. P. Bertsekas, Nonlinear Programming, 2nd ed.

Belmont, MA:

Athena Scientiﬁc, 1999.

[44] K. C. Almeida and A. Kocholik, “Solving ill-posed optimal power ﬂow
problems via Fritz-John optimality conditions,” IEEE Trans. Power Syst.,
vol. 31, no. 6, pp. 4913–4922, Nov. 2016.

[45] A. Hauswirth, S. Bolognani, G. Hug, and F. Dorﬂer, “Generic existence
of unique lagrange multipliers in AC optimal power ﬂow,” IEEE Contr.
Syst. Lett., vol. 2, no. 4, pp. 791–796, Oct. 2018.

[46] D. K. Molzahn, B. C. Lesieutre, and C. L. DeMarco, “A sufﬁcient
condition for global optimality of solutions to the optimal power ﬂow
problem,” IEEE Trans. Power Syst., vol. 29, no. 2, pp. 978–979, Mar.
2014.

[47] J. Lofberg, “Yalmip : A toolbox for modeling and optimization in

matlab,” in Proc. of the CACSD Conf., Taipei, Taiwan, 2004.

