2
2
0
2

p
e
S
2
1

]

R

I
.
s
c
[

2
v
3
4
4
3
1
.
7
0
2
2
:
v
i
X
r
a

Lecture Notes on

Neural Information Retrieval

Nicola Tonellotto
nicola.tonellotto@unipi.it

Version date: September 13, 2022

Copyright ©2022. All rights reserved.

 
 
 
 
 
 
These lecture notes focus on the recent advancements in neural information retrieval,
with particular emphasis on the systems and models exploiting transformer networks.
These networks, originally proposed by Google in 2017, have seen a large success
in many natural language processing and information retrieval tasks. While there
are many fantastic textbook on information retrieval [B ¨uttcher et al. 2010, Manning
et al. 2008] and natural language processing [Jurafsky and Martin 2009], as well as
specialised books for a more advanced audience [Cambazoglu and Baeza-Yates 2015,
Lin et al. 2021, Liu 2009, Tonellotto et al. 2018], these lecture notes target people aiming
at developing a basic understanding of the main information retrieval techniques and
approaches based on deep learning.

These notes have been prepared for a graduate course of the MSc program in Artiﬁcial
Intelligence and Data Engineering at the University of Pisa, Italy. Part of the material
is inspired by other works, and when this is done a reference is obviously provided to
the original. I would like to warmly thank all the students and colleagues who read
these notes, gave me their feedback and sent me their corrections, that allowed to ﬁx
many errors on the original manuscript.

I am aware that this document is far from perfect, and I am eager to improve it.
Feel free to contact me if you have any comments, suggestions and/or if you ﬁnd
typos/mistakes.

Version History

27/07/2022 Initial version.
12/09/2022 Fixed typos and terminology.

2

Contents

1 Text Representations for Ranking

8

1.1 BOW Encodings .
1.2 LTR Features .
.
1.3 Word Embeddings

.

.
.

.
.
.

.
.
.

.
.
.

9
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2

Interaction-focused Systems

12

2.1 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 12
2.2 Pre-trained Language Models . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3 Ranking with Encoder-only Models
. . . . . . . . . . . . . . . . . . . . . 16
2.4 Ranking with Encoder-decoder Models . . . . . . . . . . . . . . . . . . . 18
2.5 Fine-tuning Interaction-focused Systems . . . . . . . . . . . . . . . . . . . 20
2.6 Dealing with long texts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

3 Representation-focused Systems

21

Single Representations .

3.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.2 Multiple Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.3 Fine-tuning Representation-focused Systems . . . . . . . . . . . . . . . . 26

4 Retrieval Architectures and Vector Search

28

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.1 Retrieval architectures .
4.2 MIP and NN Search Problems . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.3 Locality sensitive hashing approaches . . . . . . . . . . . . . . . . . . . . 31
4.4 Vector quantisation approaches . . . . . . . . . . . . . . . . . . . . . . . . 32
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.5 Graph approaches .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
.
.
4.6 Optimisations .

.
.

.
.

.

5 Learned Sparse Retrieval

35

5.1 Document expansion learning . . . . . . . . . . . . . . . . . . . . . . . . . 36
Impact score learning .
5.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Sparse representation learning . . . . . . . . . . . . . . . . . . . . . . . . 40
5.3

6 Conclusions

41

3

This page intentionally left blank.

4

Introduction

Text Information Retrieval (IR) systems focus on retrieving text documents able to ful-
ﬁll the information needs of their users, typically expressed as textual queries. Over
the years, this inherently vague description has been formalised and characterised by
the speciﬁc nature of documents, information needs, and users. At the core of the for-
malisation lies the concept of relevance of a document with respect to a query, and how
to estimate their relevance. Over the years, many different ranking models have been
proposed to estimate the relevance of documents in response to a query. These mod-
els depend on the information provided by the queries and the documents, that are
exploited to derive “relevance signals”. Many ranking models have been developed
over the years, ranging from Boolean models to probabilistic and statistical language
models. These “bag of words” models leverage the presence or the number of occur-
rences of query terms in the documents to infer their relevance to a query, exploiting
hand-crafted functions to combine these occurrences such as BM25. With the rise of
the Web and social platforms, more sources of relevance information about documents
have been identiﬁed. Machine learning methods have been proved effective to deal
with this abundance of relevance signals, and their application to rank the documents
in order of relevance estimates w.r.t. a query has given birth to many learning-to-rank
(LTR) models. Relevance signals are input features in LTR models, and they are often
designed by hand, a time-consuming process. Motivated by their breakthroughs in
many computer vision and natural language processing tasks, neural networks repre-
sent the current state-of-the-art approach in ranking documents w.r.t. query relevance.
Neural Information Retrieval focuses on retrieving text documents able to fulﬁll the
information needs of their users exploiting deep neural networks. In neural IR, neural
networks are typically used in two different ways: to learn the ranking functions com-
bining the relevance signals to produce an ordering of documents, and to learn the
abstract representations of documents and queries to capture their relevance informa-
tion. In the following, we provide an introduction to the recent approaches in neural
IR. Since the research in the ﬁeld is rapidly evolving, we do not pretend to cover every
single aspect of neural IR, but to provide a principled introduction to the main ideas
and existing systems in the ﬁeld. When available, we provide links to relevant and
more detailed surveys.
Here is a quick overview over what the sections are about. Section 1 provides a short
depiction of the different representations for text adopted in IR, from the classical
BOW encodings to learning-to-rank features to word embeddings. Section 2 presents
the main neural architectures for computing a joint representation of query and docu-

5

ment pairs for relevance ranking. Section 3 focuses on the neural architectures specif-
ically tailored for learning abstract complex representations of query and documents
texts independently. Section 4 overviews the deployment schemes adopted in neural
IR systems, together with an overview of the most common dense retrieval indexes
supporting exact and approximate nearest neighbour search. Section 5 discusses the
current approaches in learned sparse retrieval, dealing with the learning of low di-
mensional representations of documents amenable to be stored in an inverted index
or similar data structures. Finally, Section 6 draws concluding remarks.

Notes

In the following, we will refer to the textual contents of queries and documents as
text, and we will refer to long documents and short documents, also known as pas-
sages, as documents, according to common practice in neural IR. In IR, tokenisation
algorithms, implemented by tokenisers, decompose both query and document texts
into atomic units of text called tokens. The atomic units of text can represent whole
words, e.g., username or sub-words, e.g., user## and ##name, depending on the tokeni-
sation algorithm employed to decompose them. The collection of unique tokens in a
text collection is called a vocabulary, whose elements are called terms. These terms in
turn correspond to words or sub-words in queries and documents, depending on the
tokenisation algorithm.
Moreover, for simplicity, we will describe the linear neural layer y = Ax + b, where
A is the weight matrix and b is the bias vector, using the simpliﬁed notation y = Wx,
representing exactly the same layer without explicitly referring to the bias vector.

6

List of Symbols

N
R
Rn

V1, V2, V3
V
w
t
d
q
V
D
Q
φ(q), φ
ψ(d), ψ
η(q, d)
φ1, φ2, . . .
ψ1, ψ2, . . .

s(q, d)
f (·)
k, m, n
(cid:96)

[CLS]
[SEP]
[OUT]

q1, . . . , qm
d1, . . . , dn

the set of natural numbers
the set of real numbers
the set of vectors, or embeddings, that consist of n real-valued en-
tries
latent representation spaces
a vector space
a term (word or sub-word)
a text, composed by tokens (words or sub-words)
a text document, composed by tokens (words or sub-words)
a text query, composed by tokens (words or sub-words)
a vocabulary, whose elements are terms
a document collection, whose elements are documents
a query log, whose elements are queries
an embedding of the query q
an embedding of the document d
an embedding of the query-document pair q, d
components of a query embedding or generic query embeddings
component of a document embedding or generic document em-
beddings
the relevance score of the document d w.r.t. the query q
an embedding aggregation function
positive integer values
dimension of output embeddings of transformer architectures,
e.g., BERT and T5 models; if not otherwise speciﬁed, (cid:96) = 768
a special classiﬁcation token in BERT and T5 models
a special separation token in BERT and T5 models
a special output token in T5 models
query tokens
document tokens

7

1 Text Representations for Ranking

According to the Probability Ranking Principle [Robertson 1977], under certain as-
sumptions, for a given user’s query, documents in a collection should be ranked in
order of the (decreasing) probability of relevance w.r.t.
the query, to maximise the
overall effectiveness of a retrieval system for the user. The task of ad-hoc ranking is, for
each query, to compute an ordering of the documents equivalent or most similar to
the optimal ordering based on the probability of relevance. It is common to limit the
documents to be ordered to just the top k documents in the optimal ordering.
Let D denote a collection of (text) documents, and Q denote a log of (text) queries.
Queries and documents share the same vocabulary V of terms. A ranking function,
also known as scoring function, s : Q × D → R computes a real-valued score for the
documents in the collection D w.r.t. the queries in the log Q. Given a query q and a
document d, we call the value s(q, d) relevance score of the document w.r.t. the query.
For a given query, the scores of the documents in the collection can be used to induce
an ordering of the documents, in reverse value of score. The closer this induced or-
dering is to the optimal ordering, the more effective an IR system based on the scoring
function is.
Without loss of generality, the scoring function s(q, d) can be further decomposed as:

s(q, d) = f (cid:0)φ(q), ψ(d), η(q, d)(cid:1),

(1)

where φ : Q → V1, ψ : D → V2, and η : Q × D → V3 are three representation func-
tions, mapping queries, documents, and query-document pairs into the latent repre-
sentation spaces V1, V2, and V3, respectively [Guo et al. 2020]. These functions build
abstract mathematical representations of the text sequences of documents and queries
amenable for computations. The elements of these vectors represent the features used
to describe the corresponding objects, and the aggregation function f : V1 × V2 × V3 →
R computes the relevance score of the document representation w.r.t. the query repre-
sentation.
The representation functions φ, ψ and η, and the aggregation function f can be de-
signed by hand, leveraging some axioms or heuristics, or computed through machine
learning algorithms. In the following, we will overview the representation functions
adopted in classical IR (Section 1.1), in LTR scenarios (Section 1.2) and the recently
proposed word embeddings (Section 1.3).

8

Figure 1: Representation-based decomposition of a ranking function. Adapted
from [Guo et al. 2020].

1.1 BOW Encodings

In classical IR, both representation and aggregation functions are designed manually,
incorporating some lexical statistics such as number of occurrences of terms in a doc-
ument or in the whole collection. Classical IR ranking models, e.g., vector space mod-
els [Salton et al. 1975], probabilistic models [Robertson and Zaragoza 2009] and sta-
tistical language models [Ponte and Croft 1998], are based on the bag of words (BOW)
model, where queries and documents are represented as a set of terms from the vo-
cabulary V together with the number of occurrences of the corresponding tokens in
the text. More formally, queries and documents are represented as vectors φ(q) and
ψ(d) in N|V |, called BOW encodings, where the i-th component of both representations
encodes the number of occurrences of the term wi ∈ V in the corresponding text.
The query-document representation function η is not present in these ranking func-
tions. The aggregation function f over these representations is an explicit formula
taking into account the components of the query and document representations, i.e.,
the in-query and in-document term frequencies, together with other document nor-
malisation operations. These representations are referred to as sparse representations,
since most of their components are equal to 0 because they correspond to tokens not
appearing in the query/document. Sparse representations can be trivially computed
and efﬁciently stored in specialised data structures called inverted indexes, which rep-
resent the backbone of commercial Web search engine [Cambazoglu and Baeza-Yates
2015]; see [B ¨uttcher et al. 2010, Manning et al. 2008, Tonellotto et al. 2018] for more

9

Query representation 𝜙Document representation ψQuery-document representation ηDocument dQuery qAggregation function fRelevance scores(q, d)details on inverted indexes and classical IR ranking models.

1.2 LTR Features

With the advent of the Web, new sources of relevance information about the docu-
ments have been made available. The importance of a Web page, e.g., PageRank,
additional document statistics, e.g., term frequencies in the title or anchors text, and
search engine interactions, e.g., clicks, can be exploited as relevance signals. Moreover,
collaborative and social platforms such as Wikipedia, Twitter and Facebook represent
new sources of relevance signals. These relevance signals have been exploited to build
richer query and document representations in LTR. The relevance signals extracted
from queries and/or documents are called features. There are various classes of these
features [Bendersky et al. 2011, Macdonald et al. 2012], such as:

• query-only features, i.e., components of φ(q): query features with the same value
for each document, such as query type, query length, and query performance
predictors;

• query-independent features, i.e., components of ψ(d): document features with the
same value for each query, such as importance score, URL length, and spam
score;

• query-dependent features, i.e., components of η(q, d): document features that de-
pend on the query, such as different term weighting models on different ﬁelds.

In LTR, the representation functions are hand-crafted: exploiting the relevance signals
from heterogeneous information sources, the different components of query and doc-
ument representations are computed with feature-speciﬁc algorithms. Hence, the rep-
resentations φ(q), ψ(d), and η(q, d) are elements of vector spaces over R, but whose di-
mensions depend on the number of hand-crafted query-only, query-independent, and
query-dependent features, respectively. Moreover the different components of these
vectors are heterogeneous, and do not carry any speciﬁc semantic meaning. Using
these representations, in LTR the aggregation function f is machine-learned, for ex-
ample using logistic regression [Gey 1994], gradient-boosted regression trees [Burges
2010] or neural networks [Burges et al. 2005]; see [Liu 2009] for a detailed survey.

1.3 Word Embeddings

Both BOW encodings and LTR features are widely adopted in commercial search en-
gines, but they suffer from several limitations. On the one hand, semantically-related

10

terms end up having completely different BOW encodings. Although the two terms
catalogue and directory can be considered synonyms, their BOW encodings are com-
pletely different, with the single 1 appearing in different components. Similarly, two
different documents on a same topic can end up having two unrelated BOW encod-
ings. On the other hand, LTR features create text representations by hand via feature
engineering, with heterogeneous components and no explicit concept of similarity.
In the 1950s, many linguists formulated the distributional hypothesis: words that occur
in the same contexts tend to have similar meanings [Harris 1954]. According to this
hypothesis, the meaning of words can be inferred by their usage together with other
words in existing texts. Hence, by leveraging the large text collections available, it is
possible to learn useful representations of terms, and devise new methods to use these
representations to build up more complex representations for queries and documents.
These learned representations are vectors in Rn, with n (cid:28) |V |, called distributional rep-
resentations or word embeddings. The number of dimensions n ranges approximatively
from 50 to 1000 components, instead of the vocabulary size |V |. Moreover, the com-
ponents of word embeddings are rarely 0: they are real numbers, and can also have
negative values. Hence, word embeddings are also referred to as dense representations.
Among the different techniques to compute these representations, there are algorithms
to compute global representations of the words, i.e., a single ﬁxed embedding for each
term in the vocabulary, called static word embeddings, and algorithms to compute local
representations of the terms, which depend on the other tokens used together with
a given term, i.e., its context, called contextualised word embeddings. Static word em-
beddings used in neural IR are learned from real-world text with no explicit training
labels: the text itself is used in a self-supervised fashion to compute word represen-
tations. There are different kinds of static word embeddings, for different languages,
such as word2vec [Mikolov et al. 2013], fasttext [Joulin et al. 2017] and GloVe [Pennington
et al. 2014]. Static word embeddings map terms with multiple senses into an average
or most common sense representation based on the training data used to compute the
vectors; each term in the vocabulary is associated with a single vector. Contextualised
word embeddings map tokens used in a particular context to a speciﬁc vector; each
term in the vocabulary is associated with a different vector every time it appears in
a document, depending on the surrounding tokens. The most popular contextualised
word embeddings are learned with deep neural networks such as the Bidirectional En-
coder Representations from Transformers (BERT) [Devlin et al. 2019], the Robustly Op-
timized BERT Approach (RoBERTa) [Liu et al. 2019], and the Generative Pre-Training
models (GPT) [Radford and Narasimhan 2018].
In neural IR, word embeddings are used to compute the representation functions φ,

11

ψ and η, and the aggregation function f through (deep) neural networks. Depend-
ing on the assumptions over the representation functions, the neural ranking models
can be classiﬁed in interaction-focused models and representation-focused models. In
interaction-focused models, the query-document representation function η(q, d), tak-
ing into account the interaction between the query and document contents, is explicitly
constructed and used as input to a deep neural network, or it is implicitly generated
and directly used by a deep neural network. In representation-focused models, the
query-document representation function η(q, d) is not present; query and document
representations φ(q) and ψ(d) are computed independently by deep neural networks
In the following, we discuss the main interaction-focused models for ad-hoc ranking
(Section 2) and representation-focused models for queries and documents (Section 3).

2

Interaction-focused Systems

The interaction-focused systems used in neural IR model the word and n-gram re-
lationships across a query and a document using deep neural networks. These sys-
tems receive as input both a query q and a document d, and output a query-document
representation η(q, d). Among others, two neural network architectures have been
investigated to build a representation of these relationships: convolutional neural net-
works and transformers. Convolutional neural networks represent one of the ﬁrst
approaches in building joint representations of queries and documents, as discussed
in Section 2.1. Transformers represent the major turning point in neural IR, as their
application to textual inputs gave birth to pre-trained language models, presented
in Section 2.2. In neural IR, pre-trained language models are used to compute query-
document representations, and the two main transformer models used for this task are
BERT and T5 illustrated in Sections 2.3 and 2.4, respectively. Section 2.5 describes how
pre-trained language models are ﬁne-tuned to compute effective query-document rep-
resentations, and Section 2.6 brieﬂy discusses how pre-trained language models can
deal with long documents.

2.1 Convolutional Neural Networks

A convolutional neural network is a family of neural networks designed to capture
local patterns in structured inputs, such as images and texts [LeCun and Bengio 1998].
The core component of a convolutional neural network is the convolution layer, used
in conjunction with feed forward and pooling layers. A convolutional layer can be
seen as a small linear ﬁlter, sliding over the input and looking for proximity patterns.

12

Several neural models employ convolutional neural networks over the interactions
between queries and documents to produce relevance scores. Typically, in these mod-
els, the word embeddings of the query and document tokens are aggregated into an
interaction matrix, on top of which convolutional neural networks are used to learn
hierarchical proximity patterns such as unigrams, bigrams and so on. Then, the ﬁnal
top-level proximity patterns are fed into a feed forward neural network to produce
the relevance score s(q, d) between the query q and the document d, as illustrated in
Figure 2.

Figure 2: Scheme of an interaction-focused model based on convolutional neural net-
works.

The query q and the document d are tokenised into m and n tokens, respectively, and
each token is mapped to a corresponding static word embedding. The interaction
matrix η(q, d) ∈ Rm×n is composed of the cosine similarities between a query token
embedding and a document token embedding.
One of the ﬁrst neural models leveraging the interaction matrix is the Deep Relevance
Matching Model (DRMM) [Guo et al. 2016]). In DRMM, the cosine similarities of every
query token w.r.t. the document tokens are converted into a discrete distribution using
hard bucketing, i.e., into a query token histogram. Then the histogram of each query to-
ken is provided as input to a feed forward neural network to compute the ﬁnal query
token-document relevance score. These relevance scores are then aggregated through
an IDF-based weighted sum across the different query terms. Instead of using hard
bucketing, the Kernel-based Neural Ranking Model (KNRM) [Xiong et al. 2017] pro-
poses to use Gaussian kernels to smoothly distribute the contribution of each cosine
similarity across different buckets before providing the histograms with soft bucketing
to the feed forward neural networks.
Both DRMM and KNRM exploit the interaction matrix, but they do not incorporate

13

ψ1ψnWord embeddingsd𝜙1𝜙mWord embeddingsqη(q,d)Convolutional neural networksFeed forward neural networkss(q,d)ProximityPatternsInteractionMatrixRelevanceScoreany convolutional layer. In the Convolutional KNRM model (ConvKNRM) [Dai et al.
2018], the query and document embeddings are ﬁrst independently processed through
k convolutional neural networks, to build unigam, bigram, up to k-gram embeddings.
These convolutions allow to build word embeddings taking into account multiple
close words at the same time. Then, k2 cosine similarity matrices are built, between
each combination of query and document n-gram embeddings, and these matrices
are processed with KNRM. In the Position-Aware Convolutional Recurrent Relevant
model (PACRR) [Hui et al. 2017], the interaction matrix is processed through several
convolutional and pooling layers to take into account words proximity. Convolutional
layers are used also in other similar neural models [Fan et al. 2018b, Hu et al. 2014, Hui
et al. 2018, Pang et al. 2016, 2017].

2.2 Pre-trained Language Models

Static word embeddings map words with multiple senses into an average or most
common-sense representation based on the training data used to compute the vectors.
The vector of a word does not change with the other words used in a sentence around
it. The transformer is a neural network designed to explicitly take into account the
context of arbitrary long sequences of text, thanks to a special neural layer called self-
attention, used in conjunction with feed forward and linear layers. The self-attention
layer maps input sequences to output sequences of the same length. When comput-
ing the i-th output element, the layer can access all the n input elements (bidirectional
self-attention) or only the ﬁrst i input elements (causal self-attention). A self-attention
layer allows the network to take into account the relationships among different ele-
ments in the same input. When the input elements are tokens of a given text, a self-
attention layer computes token representations that take into account their context,
i.e., the surrounding words. In doing so, the transformer computes contextualised word
embeddings, where the representation of each input token is conditioned by the whole
input text.
Transformers have been successfully applied to different natual language processing
tasks, such as machine translation, summarisation, question answering and so on. All
these tasks are special instances of a more general task, i.e., transforming an input
text sequence to some output text sequence. The sequence-to-sequence model has been
designed to address this general task. The sequence-to-sequence neural network is
composed of two parts: an encoder model, which receives an input sequence and builds
a contextualised representation of each input element, and a decoder model, which
uses these contextualised representations to generate a task-speciﬁc output sequence.

14

Both models are composed of several stacked transformers. The transformers in the
encoder employ bidirectional self-attention layers on the input sequence or the output
sequence of the previous transformer. The transformers in the decoder employ causal
self-attention on the previous decoder transformer’s output and bidirectional cross-
attention of the output of the ﬁnal encoder transformer’s output.
In neural IR, two speciﬁc instances of the sequence-to-sequence models have been
studied: encoder-only models and encoder-decoder models. Encoder-only models re-
ceive as input all the tokens of a given input sentence, and they compute an output
contextualised word embedding for each token in the input sentence. Representatives
of this family of models include BERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019],
and DistilBERT [Sanh et al. 2019]. Encoder-decoder models generate new output sen-
tences depending on the given input sentence. The encoder model receives as input all
the tokens of a given sequence and builds a contextualised representation, and the de-
coder model sequentially accesses these embeddings to generate new output tokens,
one token at a time. Representatives of this family of models include BART [Lewis
et al. 2020] and T5 [Raffel et al. 2020].
Sequence-to-sequence models can be trained as language models, by projecting with a
linear layer every output embedding to a given vocabulary and computing the tokens
probabilities with a softmax operation. The softmax operation is a function σ : Rk →
[0, 1]k that takes as input k > 1 real values z1, z2, . . . , zk and transforms each input zi as
follows:

σ(zi) =

ezi
j=1 ezj

∑k

(2)

The softmax operation normalises the input values into a probability distribution. In
the context of deep learning, the inputs of a softmax operation are usually called logits,
and they represent the raw predictions generated by a multi-class classiﬁcation model,
turned into a probability distribution over the classes by the softmax operation.
Depending on the training objective, a sequence-to-sequence model can be trained
as a masked language model (MLM), as for BERT, or a casual language model (CLM), as
for T5. MLM training focuses on learning to predict missing tokens in a sequence
given the surrounding tokens; CLM training focuses on predicting the next token in
an output sequence given the preceding tokens in the input sequence. In both cases,
it is commonplace to train these models using massive text data to obtain pre-trained
language models. In doing so, we allow the model to learn general-purpose knowledge
about a language that can be adapted afterwards to a more speciﬁc downstream task. In
this transfer learning approach, a pre-trained language model is used as initial model
to ﬁne-tune it on a domain-speciﬁc, smaller training dataset for the downstream target

15

task. In other words, ﬁne-tuning is the procedure to update the parameters of a pre-
trained language model for the domain data and target task.
As illustrated in Figure 3, pre-training typically requires a huge general-purpose train-
ing corpus, such as Wikipedia or Common Crawl web pages, expensive computa-
tion resources and long training times, spanning several days or weeks. On the other
side, ﬁne-tuning requires a small domain-speciﬁc corpus focused on the downstream
task, affordable computational resources and few hours or days of additional training.
Special cases of ﬁne-tuning are few-shot learning, where the domain-speciﬁc corpus is
composed of a very limited number of training data, and zero-shot learning, where a
pre-trained language model is used on a downstream task that it was not ﬁne-tuned
on.

Figure 3: Transfer learning of a pre-trained language model to a ﬁne-tuned language
model.

In neural IR, the interaction-focused systems that use pre-trained language models are
called cross-encoder models, as they receive as input a pair (q, d) of query and document
texts. Depending on the type of sequence-to-sequence model, different cross-encoders
are ﬁne-tuned in different ways, but, in general, they aim at computing a relevance
score s(q, d) to rank documents w.r.t. a given query. In the following, we illustrate
the most common cross-encoders leveraging both encoder-only models (Sec. 2.3) and
encoder-decoder models (Sec. 2.4).

2.3 Ranking with Encoder-only Models

The most widely adopted transformer architecture in neural IR is BERT, an encoder-
only model. Its input text is tokenised using the WordPiece sub-word tokeniser [Wu
et al. 2016]. The vocabulary V of this tokeniser is composed of 30, 522 terms, where the
uncommon/rare words, e.g., goldfish, are splitted up in sub-words, e.g., gold## and
##fish. The ﬁrst input token of BERT is always the special [CLS] token, that stands
for “classiﬁcation”. BERT accepts as input other special tokens, such as [SEP], that
denotes the end of a text provided as input or to separate two different texts provided
as a single input. BERT accepts as input at most 512 tokens, and produces an output
embedding in R(cid:96) for each input token. The most commonly adopted BERT version

16

RandomLanguageModelPre-trainedLanguageModelFine-tunedLanguageModelHuge corpusHours/days of trainingSmall corpusDays/weeks of trainingFew GPUsMany TPUsis BERT base, which stacks 12 transformer layers, and whose output representation
space has (cid:96) = 768 dimensions.
Nogueira and Cho [2019] and MacAvaney et al. [2019] illustrated how to ﬁne-tune
BERT as a cross-encoder1, in two slightly different ways. Given a query-document
pair, both texts are tokenised into token sequences q1, . . . , qm and d1, . . . , dn. Then,
the tokens are concatenated with BERT special tokens to form the following input
conﬁguration:

[CLS] q1 · · · qm [SEP] d1 · · · dn [SEP]

that will be used as BERT input. In doing so, the self-attention layers in the BERT en-
coders are able to take into account the semantic interactions among the query tokens
and the document tokens. The output embedding η[CLS] ∈ R(cid:96), corresponding to the
input [CLS] token, serves as a contextual representation of the query-document pair
as a whole.
Nogueira et al. [2019a] ﬁne-tune BERT on a binary classiﬁcation task to compute the
query-document relevance score, as illustrated in Figure 4. To produce the relevance
score s(q, d), the query and the document are processed by BERT to generate the out-
put embedding η[CLS] ∈ R(cid:96), that is multiplied by a learned set of classiﬁcation weights
W2 ∈ R2×(cid:96) to produce two real scores z0 and z1, and then through a softmax operation
to transform the scores into a probability distribution p0 and p1 over the non-relevant
and relevant classes. The probability corresponding to the relevant class, convention-
ally assigned to label 1, i.e., p1, is the ﬁnal relevance score.
MacAvaney et al. [2019] ﬁne-tune BERT by projecting the output embedding η[CLS] ∈
R(cid:96) through the learned matrix W1 ∈ R1×(cid:96) into a single real value z, that represents the
ﬁnal relevance score.

η[CLS] = BERT(q, d)

[z0, z1] = W2η[CLS] or
[p0, p1] = softmax([z0, z1])
s(q, d) = p1 or

s(q, d) = z

z = W1η[CLS]

(3)

In Section 2.5 we illustrate how a BERT-based cross-encoder is typically ﬁne-tuned for
ad-hoc ranking.

1Nogueira and Cho [2019] call it monoBERT, and MacAvaney et al. [2019] call it vanilla BERT.

17

Figure 4: BERT classiﬁcation model for ad-hoc ranking.

2.4 Ranking with Encoder-decoder Models

Instead of using an encoder-only transformer model to compute the latent representa-
tion of a query-document pair and to convert it into a relevance score, it is possible also
to use an encoder-decoder model [Raffel et al. 2020] with prompt learning, by converting
the relevance score computation task into a cloze test, i.e., a ﬁll-in-the-blank problem.
Prompting has been successfully adopted in article summarisation tasks [Radford et al.
2019] and knowledge base completion tasks [Petroni et al. 2019].
In prompt learning, the input texts are reshaped as a natural language template, and
the downstream task is reshaped as a cloze-like task. For example, in topic classiﬁ-
cation, assuming we need to classify the sentence text into two classes c0 and c1, the
input template can be:

Input: text Class: [OUT]

Among the vocabulary terms, two label terms w0 and w1 are selected to correspond to
the classes c0 and c1, respectively. The probability to assign the input text to a class
can be transferred into the probability that the input token [OUT] is assigned to the
corresponding label token:

p(c0|text) = p([OUT] = w0|Input: text Class: [OUT])
p(c1|text) = p([OUT] = w1|Input: text Class: [OUT])

18

[CLS]q1qmd1dn[SEP][SEP]TokeniserTokeniserqds(q,d)Encoder modelη[CLS]softmaxW2Nogueira et al. [2020] proposed a prompt learning approach for relevance ranking
using a T5 model2, as illustrated in Figure 5. The query and the document texts q and
d are concatenated to form the following input template:

Query: q Document: d Relevant: [OUT]

An encoder-decoder model is ﬁne-tuned with a downstream task taking as input this
input conﬁguration, and generating an output sequence whose last token is equal to
True or False, depending on whether the document d is relevant or non-relevant to
the query q.
The query-document relevance score is computed by normalising only the False and
True output probabilities, computed over the whole vocabulary, with a softmax opera-
tion.

Figure 5: T5 model for ad-hoc ranking.

To produce the relevance score s(q, d), the query and the document are processed by
T5 to generate the output embedding η[OUT] ∈ R(cid:96), that is projected by a learned set of
classiﬁcation weights WV ∈ R|V |×(cid:96) over the vocabulary V. The outputs zFalse and zTrue
corresponding to the False and True terms, respectively, are transformed into a proba-
bility distribution with a softmax operation, to yield the required predictions pFalse and
pTrue over the “non-relevant” and “relevant” classes. The prediction corresponding to

2Nogueira et al. [2020] call it monoT5.

19

Query:qdDocument:Relevant: [OUT]Encoder-decoder models(q,d)softmaxη[OUT]truefalseWVTokeniserthe relevant class, i.e., pTrue, is the ﬁnal relevance score.

η[OUT] = T5(q, d)
[. . . , zFalse, . . . , zTrue, . . .]T = WVη[OUT]

[pFalse, pTrue] = softmax([zFalse, zTrue])

s(q, d) = pTrue

(4)

In the next section we discuss how a T5-based cross-encoder is typically ﬁne-tuned for
ad-hoc ranking.

2.5 Fine-tuning Interaction-focused Systems

As discussed in Section 2.2, the pre-trained language models adopted in IR require
a ﬁne-tuning of the model on a speciﬁc downstream task. Given an input query-
document pair (q, d) a neural IR model M(θ), parametrised by θ, computes sθ(q, d) ∈
R, the score of the document d w.r.t. the query q. We are supposed to predict y ∈
{+, −} from (q, d) ∈ Q × D, where − stands for non-relevant and + for relevant.
This problem can be framed as a binary classiﬁcation problem. We perform the clas-
siﬁcation by assuming a joint distribution p over {+, −} × Q × D from which we
can sample correct pairs (+, q, d) ≡ (q, d+) and (−, q, d) ≡ (q, d−). Using sampled
correct pairs we learn a score function sθ(q, d) as an instance of a metric learning prob-
lem [Xing et al. 2002]: the score function must assign a high score to a relevant docu-
ment and a low score to a non-relevant document, as in Eq. (3) and Eq. (4). Then we
ﬁnd θ∗ that minimises the (binary) cross entropy lCE between the conditional proba-
bility p(y|q, d) and the model probability pθ(y|q, d):

θ∗ = arg min

θ

(cid:104)

E

(cid:105)

lCE(y, q, d)

(5)

where the expectation is computed over (y, q, d) ∼ p, and the cross entropy is com-
puted as:

(cid:0)y, (q, d)(cid:1) =

lCE






− log (cid:0)sθ(q, d)(cid:1)
− log (cid:0)1 − sθ(q, d)(cid:1)

if y = +

if y = −

(6)

Typically, a dataset T available for ﬁne-tuning pre-trained language models for rele-
vance scoring is composed of a list of triples (q, d+, d−), where q is a query, d+ is a
relevant document for the query, and d− is a non-relevant document for the query. In
this case, the expected cross entropy is approximated by the sum of the cross entropies

20

computed for each triple:

(cid:104)

E

lCE

(cid:0)y, (q, d)(cid:1)(cid:105)

≈

1
2|T |

∑
(q,d+,d−)∈T

(cid:16)

− log (cid:0)sθ(q, d+)(cid:1) − log (cid:0)1 − sθ(q, d−)(cid:1)(cid:17)

(7)

In doing so, we do not take into account documents in the collection that are not ex-
plicitly labeled as relevant or non-relevant. This approach is limited to take into ac-
count positive and negative triples in a pairwise independent fashion. In Section 3.3
we will discuss a different ﬁne-tuning approach commonly used for representation-
focused systems, taking into account multiple non-relevant documents per relevant
document.

2.6 Dealing with long texts

BERT and T5 models have an input size limited to 512 tokens, including the special
ones. When dealing with long documents, that cannot be fed completely into a trans-
former model, we need to split them into smaller texts in a procedure referred to as
passaging. Dai and Callan [2019a] propose to split a long document into overlapping
shorter passages, to be processed independently together with the same query by a
cross-encoder. During training, if a long document is relevant, all its passages are
relevant, and vice-versa. Then, the relevance scores for each composing passage are
aggregated back to a single score for the long document. In this scenario, the common
aggregation functions are FirstP, i.e., the document score is the score of the ﬁrst pas-
sage, MaxP, i.e., the document score is the highest score across all passages, and SumP,
i.e., the document score is the sum of the scores of its passages. Alternatively, Li et al.
[2020] generate the [CLS] output embedding for each passage to compute a query-
passage representation for each passage. Then, the different passage embeddings are
aggregated together to compute a ﬁnal relevance score for the whole document using
feed forward neural networks, convolutional neural networks or simple transformer
architectures.

3 Representation-focused Systems

The representation-focused systems build up independent query and document rep-
resentations, in such a way that document representations can be pre-computed and
stored in advance. During query processing, only the query representation is com-
puted, and the top documents are searched through the stored document representa-
tions. In doing so, representation-based systems are able to identify the relevant docu-

21

ments among all documents in a collection, rather than just among a query-dependent
sample; these systems represent a new type of retrieval approaches called dense re-
trieval systems. Thus far, two different families of representations have emerged in
In single representations systems, queries and documents are repre-
dense retrieval.
sented with a single embedding, as discussed in Section 3.1. In multiple representations
systems, queries and/or documents are represented with more than a single embed-
ding, as discussed in Section 3.2. Section 3.3 discusses how representation-focused
systems are ﬁne-tuned, exploiting noise-contrastive estimation.

3.1 Single Representations

In interaction-focused systems discussed in Section 2, the query and document texts
are concatenated together before processing with sequence-to-sequence models, yield-
ing rich interactions between the query context and the document context, as every
word in the document can attend to every word in the query, and vice-versa. At query
processing time, every document must be concatenated with the query and must be
processed with a forward pass of the whole sequence-to-sequence model. Even if
some techniques such as the pre-computation of some internal representations have
been proposed [MacAvaney et al. 2020b], interaction-focused systems cannot scale to a
large amount of documents. In fact, on a standard CPU, the processing of a query over
the whole document collection exploiting an inverted index requires a few millisec-
onds [Tonellotto et al. 2018], while computing the relevance score of a single query-
document pair with a transformer model may requires a few seconds [MacAvaney
et al. 2019].
Instead of leveraging sequence-to-sequence models to compute a semantically richer
but computationally expensive interaction representation η(q, d), representation-focused
systems employ encoder-only models to independently compute query representa-
tions φ(q) and document representations ψ(d) in the same latent vector space [Ur-
banek et al. 2019], as illustrated in Figure 6. Next, the relevance score between the
representations is computed via an aggregation function f between these representa-
tions:

φ(q) = [φ[CLS], φ1, . . . , φ|q|] = Encoder(q)
ψ(d) = [ψ[CLS], ψ1, . . . , ψ|d|] = Encoder(d)
s(q, d) = f (cid:0)φ(q), ψ(d)(cid:1)

(8)

In neural IR, the representation functions φ and ψ are computed through ﬁne-tuned
encoder-only sequence-to-sequence models such as BERT. The same neural model is

22

Figure 6: Representation-focused system.

used to compute both the query and the document representations, so the model is
also called dual encoder or bi-encoder [Bromley et al. 1993]. A bi-encoder maps queries
and documents in the same vector space R(cid:96), in such a way that the representations
can be mathematically manipulated. Usually, the output embedding corresponding to
the [CLS] token is assumed to be the representation of a given input text. Using these
single representations, the score aggregation function is the dot product:

s(q, d) = φ[CLS] · ψ[CLS]

(9)

Different single-representation systems have been proposed: DPR [Karpukhin et al.
2020], ANCE [Xiong et al. 2021], and STAR [Zhan et al. 2021b] being the most widely
adopted. The main difference among these systems is how the ﬁne-tuning of the BERT
model is carried out, as discussed in Section 3.3.

3.2 Multiple Representations

Up so far, we considered representation-focused systems in which queries and doc-
uments are represented through a single embedding in the latent vector space. This
single representation is assumed to incorporate the meaning of an entire text within
that single embedding.

23

[CLS]TokeniserqEncodermodel[CLS]TokeniserdEncodermodel𝜙[CLS] = 𝜙0ψ[CLS] = ψ0𝜙1𝜙|q|ψ|d|ψ1Score aggregations(q,d)In contrast, multiple representation systems such as poly-encoders [Humeau et al. 2019],
ME-BERT [Luan et al. 2021], ColBERT [Khattab and Zaharia 2020] and COIL [Gao et al.
2021] exploit more than a single embedding to represent a given text, which may allow
a richer semantic representation of the content.
Instead of using just the ﬁrst output embedding ψ[CLS] = ψ0 to encode a document d,
poly-encoders [Humeau et al. 2019] exploit the ﬁrst m output embeddings ψ0, ψ1, . . . , ψm−1.
A query q is still represented with the single embedding φ[CLS] = φ0, while we need to
aggregate the m output document embeddings into a single representation ψ∗ to com-
pute the ﬁnal relevance score using the dot product with the output query embedding.
To do so, poly-encoders ﬁrst compute the m similarities s0, . . . , sm−1 between the query
embedding and the ﬁrst m document embedding using the dot product. These similar-
ities are transformed into normalised weights v0, . . . , vm−1 using a softmax operation,
and the weighted output embeddings are summed up to compute the ﬁnal document
embedding ψ∗ used to compute the relevance score:

[φ0, φ1, . . .] = Encoder(q)
[ψ0, ψ1, . . .] = Encoder(d)

[s0, s1, . . . , sm−1] = [φ0 · ψ0, φ0 · ψ1, . . . , φ0 · ψm−1]
[v0, v1, . . . , vm−1] = softmax([s0, s1, . . . , sm−1])
m−1
∑
i=0

ψ∗ =

viψi

(10)

s(q, d) = φ0 · ψ∗

Similarly to poly-encoders, ME-BERT [Luan et al. 2021] exploits the ﬁrst m output
embeddings to represent a document d (including the [CLS] embedding), but uses a
different strategy to compute the relevance score s(q, d) w.r.t. a query q. ME-BERT
computes the similarity between the query embedding and the ﬁrst m document em-
bedding using the dot product, and the maximum similarity, also called maximum
inner product, represents the relevance score:

[φ0, φ1, . . .] = Encoder(q)
[ψ0, ψ1, . . .] = Encoder(d)
s(q, d) = max

i=0,...,m−1

φ0 · ψi

(11)

This relevance scoring function, called max similarity or maxsim, allows us to exploit ef-
ﬁcient implementations of maximum inner product search systems, discussed in Sec-
tion 4. On the contrary, the relevance scoring function in Eq. (10), based on a softmax

24

operation, does not permit to decompose the relevance scoring to a maximum compu-
tation over dot products.
Differently from poly-encoders and ME-BERT, ColBERT [Khattab and Zaharia 2020]
does not limit to m the number of embeddings used to represent a document.
In-
stead, it uses all the 1 + |d| output embeddings to represent a document, i.e., one
output embedding per document token, including the [CLS] special token. More-
over, also a query q is represented with multiple 1 + |q| output embeddings, i.e., one
output embedding per query token, including the [CLS] special token. As in other
representation-focused systems, query token embeddings are computed at query pro-
cessing time; queries may also be augmented with additional masked tokens to provide
“a soft, differentiable mechanism for learning to expand queries with new terms or to re-weigh
existing terms based on their importance for matching the query” [Khattab and Zaharia
2020]. In current practice, queries are augmented up to 32 query token embeddings.
Without loss of generality, query and documents embeddings can be projected in a
smaller latent vector space through a learned weight matrix W ∈ R(cid:96)(cid:48)×(cid:96), with (cid:96)(cid:48) < (cid:96).
Since there are multiple query embeddings, ColBERT exploits a modiﬁed version of
the relevance scoring function in Eq (11), where every query embedding contributes
to the ﬁnal relevance score by summing up its maximum dot product value w.r.t. every
document embeddings:

[φ0, φ1, . . .] = Encoder(q)
[ψ0, ψ1, . . .] = Encoder(d)

s(q, d) =

|q|
∑
i=0

max
j=0,...,|d|

φi · ψj

(12)

ColBERT’s late interaction scoring in Eq. (12), also called sum maxsim, performs an
all-to-all computation: each query embedding, including the masked tokens’ embed-
dings, is dot-multiplied with every document embedding, and then the maximum
computed dot products for each query embedding are summed up. In doing so, a
query term can contribute to the ﬁnal scoring by (maximally) matching a different lex-
ical token. A different approach is proposed by the COIL system [Gao et al. 2021]. In
COIL, the query and document [CLS] embeddings are linearly projected with a learned
matrix WC ∈ R(cid:96)×(cid:96). The embeddings corresponding to normal query and document
tokens are projected into a smaller vector space with dimension (cid:96)(cid:48) < (cid:96), using another
learned matrix WT ∈ R(cid:96)(cid:48)×(cid:96). Typical values for (cid:96)(cid:48) range from 8 to 32.
The query-document relevance score is the sum of two components. The ﬁrst compo-

25

nent is the dot product of the projected query and document [CLS] embeddings, and
the second component is the sum of sub-components, one per query token. Each sub
component is the maximum inner product between a query token and the document
embeddings for the same token:

[φ0, φ1, . . .] = Encoder(q)
[ψ0, ψ1, . . .] = Encoder(d)
[φ(cid:48)
[ψ(cid:48)

1, . . .] = [WCφ0, WTφ1, . . .]
1, . . .] = [WCψ0, WTψ1, . . .]
0 · ψ(cid:48)
s(q, d) = φ(cid:48)
max
tj∈d, tj=ti

0, φ(cid:48)
0, ψ(cid:48)

0 + ∑
ti∈q

(13)

i · ψ(cid:48)
φ(cid:48)

j

The COIL’s scoring function, based on lexical matching between query and document
tokens, allows us to pre-compute the projected document embeddings and, for each
term in the vocabulary, to concatenate together the embeddings in the same document
and in the whole collection, organising them in posting lists of embeddings, including
a special posting list for the [CLS] token and its document embeddings. This organ-
isation permits the efﬁcient processing of posting lists at query time with optimised
linear algebra libraries such as BLAS [Blackford et al. 2002]. Note that the projected
query embeddings are still computed at query processing time.

3.3 Fine-tuning Representation-focused Systems

The ﬁne-tuning of a bi-encoder corresponds to learning an appropriate inner-product
function suitable for the ad-hoc ranking task, i.e., for relevance scoring. As in Sec-
tion 2.5, we have a neural IR model M(θ), parametrised by θ, that computes a score
sθ(q, d) for a document d w.r.t. a query q. We now frame the learning problem as
a probability estimation problem. To this end, we turn the scoring function into a
proper conditional distribution by using a softmax operation:

pθ(d|q) =

exp (cid:0)sθ(q, d)(cid:1)
∑d(cid:48)∈D exp (cid:0)sθ(q, d(cid:48))(cid:1)

(14)

where pθ(d|q) represents the posterior probability of the document being relevant
given the query. We assume to have a joint distribution p over D × Q, and we want to
ﬁnd the parameters θ∗ that minimise the cross entropy lCE between the actual proba-

26

bility p(d|q) and the model probability pθ(d|q):

θ∗ = arg min

θ

(cid:104)

E

(cid:105)

lCE(d, q)

= arg min

θ

(cid:104)

E

− log (cid:0)pθ(d|q)(cid:1)(cid:105)

(15)

where the expectation is computed over (d, q) ∼ p. If the scoring function sθ(q, d) is
expressive enough, then, for some θ, we have p(d|q) = pθ(d|q).
The cross entropy loss in Eq. (15) is difﬁcult to optimise, since the number of doc-
uments in D is large, and then the denominator in Eq. (14), also known as partition
function, is expensive to compute. In noise contrastive estimation we choose an artiﬁcial
noise distribution g over D of negative samples and maximise the likelihood of pθ(d|q)
contrasting g(d). Given k ≥ 2 documents Dk = {d1, . . . , dk}, for each of them we
deﬁne the following conditional distribution:

ˆpθ(di|q, Dk) =

exp (cid:0)sθ(q, di)(cid:1)
∑d(cid:48)∈Dk exp (cid:0)sθ(q, d(cid:48))(cid:1)

(16)

which is signiﬁcantly cheaper to compute that Eq. (14) if k (cid:28) |D|. Now, we want
to ﬁnd the parameters θ† that minimise the noise contrastive estimation loss lNCE,
deﬁned as:

θ† = arg min

θ

(cid:104)

E

lNCE(Dk, q)

(cid:105)

= arg min

θ

(cid:104)

E

− log (cid:0) ˆpθ(d1|q, Dk)(cid:1)(cid:105)

(17)

where the expectation is computed over (d1, q) ∼ p and di ∼ g for i = 2, . . . , k.
The end goal of this ﬁne-tuning is to learn a latent vector space for query and docu-
ment representations where a query and its relevant document(s) are closer, w.r.t. the
dot product, than the query and its non-relevant documents [Karpukhin et al. 2020];
this ﬁne-tuning approach is also called contrastive learning [Huang et al. 2013].
Negative samples are drawn from the noise distribution g over D. In the following,
we list some negative sampling strategies adopted in neural IR.

• Random sampling: any random document from the corpus is considered non-
relevant, with equal probability, i.e., q(d) = 1/|D|. Any number of negative doc-
uments can be sampled. Intuitively, it is reasonable to expect that a randomly-
sampled document will obtain a relevance score deﬁnitely smaller than the rel-
evance score of a positive document, with a corresponding loss value close to 0.
Negative documents with near zero loss contribute little to the training conver-
gence to identify the parameters θ† [Johnson and Guestrin 2018, Katharopoulos
and Fleuret 2018].

27

• In-batch sampling: during training, the queries to compute the loss can be ran-
domly aggregated into batches of size b, for faster training. For each query in
a given batch, the positive passages for the other b − 1 queries are considered
as negative passages for the query [Gillick et al. 2019]. This sampling approach
suffers from the same near zero loss problem as random sampling [Xiong et al.
2021], but the sampling prodedure is faster.

• Hard negative sampling: negative documents can be generated exploiting a clas-
sical or trained retrieval system. Each query is given as input to the retrieval
system, the top documents retrieved, and the documents not corresponding to
the positive ones are treated as negatives. Note that in this case we are assuming
a conditional noise distribution p(d|q, d1), since we assume to know the relevant
document d1 for the query. In doing so, high-ranking documents are prioritised
w.r.t. low-ranking documents, that do not impact on the user experience and do
not contribute to the loss. The retrieval system used to mine the negative docu-
ments can exploit BM25 relevance model, as in DPR [Karpukhin et al. 2020], the
currently neural model under training, as in ANCE [Xiong et al. 2021], or another
ﬁne-tuned neural model, as in STAR [Zhan et al. 2021b].

4 Retrieval Architectures and Vector Search

This section illustrates how the neural IR systems discussed so far are actually de-
ployed in end-to-end systems. Section 4.1 discusses how cross-encoders and bi-encoders
are deployed in ranking architectures. Since dense retrieval systems pre-computed the
document embeddings, many actual systems focus on storing and searching through
document embeddings. Section 4.2 formally deﬁnes the search problems in vector
spaces and the embedding index, while Sections 4.3, 4.4, and 4.5 illustrate different
solutions for efﬁciently storing and searching vectors. Section 4.6 discusses some op-
timisations for embedding indexes speciﬁcally tailored for dense retrieval systems.

4.1 Retrieval architectures

Pre-trained language models successfully improve the effectiveness of IR systems in
the ad-hoc ranking task, but they are computationally very expensive. Due to these
computational costs, the interaction-focused systems are not applied directly on the
document collection, i.e., to rank all documents matching a query. They are deployed
in a pipelined architecture (Figure 7) by conducting ﬁrst a preliminary ranking stage to
retrieve a limited number of candidates, typically 1000 documents, before re-ranking

28

them with a more expensive neural re-ranking system, such as cross-encoders de-
scribed in Section 2.

Figure 7: Re-ranking pipeline architecture for interaction-focused neural IR systems.

The most important beneﬁt of bi-encoders discussed in Section 3 is the possibility to
pre-compute and cache the representations of a large corpus of documents with the
learned document representation encoder ψ(d). At query processing time, the learned
query representation encoder must compute only the query representation φ(q), then
the documents are ranked according to the inner product of their representation with
the query embedding, and the top k documents whose embeddings have the largest
inner product w.r.t. the query embedding are returned to the user (Figure 8).

Figure 8: Dense retrieval architecture for representation-focused neural IR systems.

4.2 MIP and NN Search Problems

The pre-computed document embeddings are stored in a special data structure called
index. In its simplest form, this index must store the document embeddings and pro-
vide a search algorithm that, given a query embedding, efﬁciently ﬁnds the document
embedding with the largest dot product, or, more in general, with the maximum inner
product.

29

Document CollectionCandidatesRetrieverCandidates ListResultsListQuery Learnedquery-document representationη(q,d)Neural Re-RankerDocument CollectionLearneddocumentrepresentation encoder ψ(d) Learnedquery representation encoder 𝜙(q)Document EmbeddingsIndexNeural Ranker OﬄineOnlineQueryResultsListFormally, let φ ∈ R(cid:96) denote a query embedding, and let Ψ = {ψ1, . . . , ψn} denote a set
of n document embeddings, with ψi ∈ R(cid:96) for i = 1, . . . , n. The goal of the maximum
inner product (MIP) search is to ﬁnd the document embedding ψ∗ ∈ X such that

ψ∗ = arg max
ψ∈Ψ

(cid:104)φ, ψ(cid:105)

(18)

A data structure designed to store Ψ is called embedding index. The na¨ıve embedding
index is the ﬂat index, which stores the document embeddings in Ψ explicitly and per-
forms an exhaustive search to identify ψ∗. Its complexity is O(n(cid:96)) both in space and
time, so it is particularly inefﬁcient for large n or (cid:96) values.
A common approach to improve the space and time efﬁciency of the ﬂat index is to
convert the maximum inner product search into a nearest neighbour (NN) search, whose
goal is to ﬁnd the document embedding ψ† such that

ψ† = arg min
ψ∈Ψ

(cid:107)φ − ψ(cid:107)

(19)

Many efﬁcient index data structures exist for NN search. To leverage them with em-
bedding indexes, MIP search between embeddings must be adapted to use the Eu-
clidean distance and NN search. This is possible by applying the following transfor-
mation from R(cid:96) to R(cid:96)+1 [Bachrach et al. 2014, Neyshabur and Srebro 2015]:

ˆφ =

(cid:35)

(cid:34)

φ/(cid:107)φ(cid:107)
0

(cid:34)

,

ˆψ =

ψ/M
1 − (cid:107)ψ(cid:107)2/M2

(cid:112)

(cid:35)

,

(20)

where M = maxψ∈Ψ (cid:107)ψ(cid:107). By using this transformation, the MIP search solution ψ∗
coincides with the NN search solution ˆψ†. In fact, we have:

min (cid:107) ˆφ − ˆψ(cid:107)2 = min (cid:0)(cid:107) ˆφ(cid:107)2 + (cid:107) ˆψ(cid:107)2 − 2(cid:104) ˆφ, ˆψ(cid:105)(cid:1) = min (cid:0)2 − 2(cid:104)φ, ψ/M(cid:105)(cid:1) = max(cid:104)φ, ψ(cid:105).

Hence, hereinafter we consider the MIP search for ranking with a dense retriever as
a NN search based on the Euclidean distance among the transformed embeddings ˆφ
and ˆψ in R(cid:96)+1. To simplify the notation, from now on we drop the hat symbol from
the embeddings, i.e., ˆφ → φ and ˆψ → ψ, and we consider (cid:96) + 1 as the new dimension
(cid:96), i.e., (cid:96) + 1 → (cid:96).
The index data structures for exact NN search in low dimensional spaces have been
very successful, but they are not efﬁcient with high dimensional data, as in our case,
due to the curse of dimensionality. It is natural to make a compromise between search
accuracy and search speed, and the most recent search methods have shifted to approx-

30

imate nearest neighbor (ANN) search.
The ANN search approaches commonly used in dense retrieval can be categorised
into three families: locality sensitive hashing approaches, quantisation approaches,
and graph approaches.

4.3 Locality sensitive hashing approaches

Locality sensitive hashing (LSH) [Indyk and Motwani 1998] is based on the simple idea
that, if two embeddings are close together, then after a “projection”, using an hash
function, these two embeddings will remain close together. This requires that:

• for any two embeddings ψ1 and ψ2 that are close to each other, there is a high

probability p1 that they fall into the same hash bucket;

• for any two embeddings ψ1 and ψ2 that are far apart, there is a low probability

p2 < p1 that they fall into the same hash bucket.

The actual problem to solve is to design a family of LSH functions fulﬁlling these
requirements. LSH functions have been designed for many distance metrics. For
the euclidean distance, a popular LSH function h(ψ) is the random projection [Datar
et al. 2004].A set of random projections deﬁnes a family of hash functions H that can
be used to build a data structure for ANN search. First, we sample m hash func-
tions h1(ψ), . . . , hm(ψ) independently and uniformly at random from H, and we de-
ﬁne the function family G = {g : R(cid:96) → Zm}, where g(ψ) = (cid:0)h1(ψ), . . . , hm(ψ)(cid:1),
i.e., g is the concatenation of m hash functions from H. Then, we sample r functions
g1(ψ), . . . , gr(ψ) independently and uniformly at random from G, and each function
gi is used to build a hash table Hi.
Given the set of document embeddings Ψ and selected the values of the parameters
r and m, an LSH index is composed of r hash tables, each containing m concatenated
random projections. For each ψ ∈ Ψ, ψ is inserted in the gi(ψ) bucket for each hash
table Hi, for i = 1, . . . , r. At query processing time, given a query embedding, we ﬁrst
generate a candidate set of document embeddings by taking the union of the contents
of all r buckets in the r hash tables the query is hashed to. The ﬁnal NN document
embedding is computed performing an exhaustive exact search within the candidate
set.
The main drawback of the LSH index is that it may require a large number of hash ta-
bles to cover most nearest neighbors, and it requires to store the original embeddings
to perform the exhaustive exact search. Although some optimisations have been pro-
posed [Lv et al. 2007], the space consumption may be prohibitive with very large data
sets.

31

4.4 Vector quantisation approaches

Instead of random partitioning the input space Ψ as in LSH, the input space can be
partitioned according to the data distribution. By using the k-means clustering algo-
rithm on Ψ we can compute k centroids µ1, . . . , µk, with µi ∈ R(cid:96) for i = 1, . . . , k that can
be used to partition the input space Ψ. The set M = {µ1, . . . , µk} is called a codebook.
Given a codebook M, a vector quantiser q : R(cid:96) → R(cid:96) maps a vector ψ to its closest
centroid:

(21)

(cid:107)ψ − µ(cid:107)

q(ψ) = arg min
µ∈M
Given a codebook M, an IVF (Inverted File) index built over M and Ψ stores the set
of document embeddings Ψ in k partitions or inverted lists L1, . . . , Lk, where Li = {ψ ∈
Ψ : q(ψ) = µi}. At query processing time, we specify to search for the NN document
embeddings in p > 0 partitions. If p = k, the search is exhaustive, but if p < k, the
search is carried out in the partitions whose centroid is closer to the query embedding.
In doing so the search is not guaranteed to be exact, but the search time can be sensibly
reduced. In fact, an IVF index does not improve the space consumption, since it still
needs to store all document embeddings, but it can reduce the search time depending
on the number of partitions processed for each query.
A major limitation of IVF indexes is that they can require a large number of cen-
troids [Gersho and Gray 1992]. To address this limitation, product quantisation [J´egou
et al. 2011] divides each vector ψ ∈ Ψ into m sub-vectors ψ = [ψ1|ψ2| · · · |ψm]. Each
sub-vector ψj ∈ R(cid:96)/m with j = 1, . . . , m is quantised independently using its own sub-
vector quantiser qj. Each vector quantiser qj has its own codebook Mj = {µj,1, . . . , µj,k}.
Given the codebooks M1, . . . , Mm, a product quantiser pq : R(cid:96) → R(cid:96) maps a vector ψ
into the concatenation of the centroids of its sub-vector quantisers:

pq(ψ) = [q1(ψ1)|q2(ψ2)| · · · |qm(ψm)] = [µ1,i1

|µ2,i2

| . . . |µm,im]

(22)

Note that a product quantiser can output any of the km centroid combinations in M1 ×
. . . × Mm.
A PQ (Product Quantization) index stores, for each embedding ψ ∈ Ψ, its encoding
i1, . . . , im, that requires m log k bits of storage. At query processing time, the document
embeddings are processed exhaustively. However the distance computation between
a query embedding φ and a document embedding ψ is carried out using the product
quantisation of the document embedding pq(x):

(cid:107)ψ − φ(cid:107)2 ≈ (cid:107)pq(ψ) − φ(cid:107)2 =

m
∑
j=1

(cid:107)qj(ψj) − φj(cid:107)2

(23)

32

To implement this computation, m lookup tables are computed, one per sub-vector
quantiser: the j-th table is composed of the squared distances between the j-th sub-
vector of φ, and the centroids of Mj. These tables can be used to quickly compute the
sums in Eq. (23) for each document embedding.
ANN search on a PQ index is fast, requiring only m additions, and memory efﬁcient,
but it is still exhaustive. To avoid it, an IVFPQ index exploits inverted ﬁles and product
quantisation jointly. Firstly, a coarse quantiser partitions the input dataset into inverted
lists, for a rapid access to small portions of the input data. In a given inverted list, the
difference between each input data and the list centroid, i.e., the input residual, is en-
coded with a product quantiser. In doing so, the exhaustive ANN search can be carried
out only in a limited number of the partitions computed by the coarse quantiser.

4.5 Graph approaches

The distances between vectors in a dataset can be efﬁciently stored in a graph-based
data structure called kNN graph. In a kNN graph G = (V, E), each input data ψ ∈ Ψ
is represented as a node v ∈ V, and, for its k nearest neighbours, a corresponding
edge is added in E. The computation in an exact kNN graph requires O(n2) similar-
ity computation, but many approximate variants are available [Dong et al. 2011]. To
search for an approximate nearest neighbour to an element φ using a kNN graph, a
greedy heuristic search is used. Starting from a predeﬁned entry node, the graph is vis-
ited one node at a time, keeping on ﬁnding the closest node to φ among the unvisited
neighbour nodes. The search terminates when there is no improvement in the current
NN candidate. In practice, several entry nodes are used together with a search bud-
get to avoid local optima. For a large number of nodes, the greedy heuristic search
on the kNN graph becomes inefﬁcient, due to the long paths potentially required to
connect two nodes. Instead of storing only short-range edges, i.e., edges connecting
two close nodes, the kNN graph can be enriched with randomly generated long-range
edges, i.e., edges connecting two randomly-selected nodes. This kind of kNN graph is
a navigable small world (NSW) graph [Malkov et al. 2013], for which the greedy search
heuristic is theoretically and empirically efﬁcient [Kleinberg 2000].
A hierarchical NSW (HNSW) index stores the input data into multiple NSW graphs.
The bottom layer graph contains a node for each input element, while the number of
nodes in the other graphs decreases exponentially at each layer. The search procedure
for approximate NN vectors starts with the top layer graph. At each layer, the greedy
heuristic searches for the closest node, then the next layer is searched, starting from the
node corresponding to the closest node identiﬁed in the preceding graph. At the bot-

33

tom layer, the greedy heuristic searches for the k closest nodes to be returned [Malkov
and Yashunin 2020].

4.6 Optimisations

Implementations of the embedding indexes presented in the previou sections are avail-
able in many open-source production-ready search engines such as Lucene3 and Vespa4.
In the IR research community, FAISS is the most widely adopted framework for em-
bedding indexes [Johnson et al. 2021]. Among others, FAISS includes implementations
of ﬂat, LSH, IVF, PQ, IVFPQ and HNSW indexes.
Single representation systems such as DPR [Karpukhin et al. 2020], ANCE [Xiong et al.
2021], and STAR [Zhan et al. 2021b] use ﬂat indexes. In these cases, it is unfeasible to
adopt product quantisation indexes due to their negative impact on IR-speciﬁc met-
rics, mainly caused by the separation between the document encoding and embedding
compression phases. To overcome this limitation, several recent techniques such as Po-
emm [Zhang et al. 2021], JPQ [Zhan et al. 2021a] and RepCONC [Zhan et al. 2022] aim
to train at the same time both phases. In doing so, during training, these techniques
learn together the query and document encoders together while performing product
quantisation.
Multiple representation systems such as ColBERT [Khattab and Zaharia 2020] are char-
acterised by a very large number of document embeddings. They do not use ﬂat in-
dexes, due to unacceptable efﬁciency degradation of brute-force search, and exploit
IVFPQ indexes and ANN search. With these indexes, the document embeddings are
stored in a quantised form, suitable for fast searching. However, the approximate sim-
ilarity scores between these compressed embeddings are inaccurate, and hence are not
used for computing the ﬁnal top documents. Indeed, in a ﬁrst stage, ANN search com-
putes, for each query embedding, the set of the k(cid:48) most similar document embeddings;
the retrieved document embeddings for each query embedding are mapped back to
their documents. These documents are exploited to compute the ﬁnal list of top k doc-
uments in a second stage. To this end, the set of documents computed in the ﬁrst stage
is re-ranked using the query embeddings and the documents’ multiple embeddings
to produce exact scores that determine the ﬁnal ranking, according to the relevance
function in Eq. (12) (see Figure 9.
Further optimisations can reduce the number of query embeddings to be processed
in the ﬁrst stage [Tonellotto and Macdonald 2021], or the number of documents to be
processed in the second stage [Macdonald and Tonellotto 2021].

3https://lucene.apache.org
4https://vespa.ai

34

Figure 9: Ranking pipeline architecture for multiple representation systems.

5 Learned Sparse Retrieval

Traditional IR systems are based on sparse representations, inverted indexes and lexical-
based relevance scoring functions such as BM25. In industry-scale web search, BM25
is a widely adopted baseline due to its trade-off between effectiveness and efﬁciency.
On the other side, neural IR systems are based on dense representations of queries and
documents, that have shown impressive beneﬁts in search effectiveness, but at the cost
of query processing times. In recent years, there have been some proposals to incor-
porate the effectiveness improvements of neural networks into inverted indexes, with
their efﬁcient query processing algorithms, through learned sparse retrieval approaches.
In learned sparse retrieval the transformer architectures are used in different scenarios:

• document expansion learning: sequence-to-sequence models are used to modify
the actual content of documents, boosting the statistics of the important terms
and generating new terms to be included in a document;

• impact score learning: the output embeddings of documents provided as input to
encoder-only models are further transformed with neural networks to generate a
single real value, used to estimate the average relevance contribution of the term
in the document;

• sparse representation learning: the output embeddings of documents provided as
input to encoder-only models are projected with a learned matrix on the collec-
tion vocabulary, in order to estimate the relevant terms in a document. These

35

Document CollectionLearneddocumentrepresentation encoder ψ(d) Learnedquery representation encoder 𝜙(q)Document EmbeddingsIndexNeural Re-Ranker OﬄineOnlineQueryResultsListANN SearchIVFPQIndexClustering & Quantisationrelevant terms can be part of the documents or not, hence representing another
form of document enrichment.

In Sections 5.1, 5.2, and 5.3, we describe the main existing approaches in these scenar-
ios, respectively.

5.1 Document expansion learning

Document expansion techniques address the vocabulary mismatch problem [Zhao
2012]: queries can use terms semantically similar but lexically different from those
used in the relevant documents. Traditionally, this problem has been addressed using
query expansion techniques, such as relevance feedback [Rocchio 1971] and pseudo
relevance feedback [Lavrenko and Croft 2001]. The advances in neural networks and
natural language processing have paved the way to different techniques to address the
vocabulary mismatch problem by expanding the documents by learning new terms.
Doc2Query [Nogueira et al. 2019b] and DocT5Query [Nogueira and Lin 2019] showed
for the ﬁrst time that transformer architectures can be used to expand the documents’
content to include new terms or to boost the statistics of existing termw. Both ap-
proaches focus on the same task, that is, generating new queries for which a speciﬁc
document will be relevant. Given a dataset of query and relevant document pairs,
Doc2Query ﬁne-tunes a sequence-to-sequence transformer model [Vaswani et al. 2017],
while DocT5Query ﬁne-tunes the T5 model [Raffel et al. 2020] by taking as input the rel-
evant document and generating the corresponding query. Then, the ﬁne-tuned model
is used to predict new queries using top k random sampling [Fan et al. 2018a] to enrich
the document by appending these queries before indexing, as illustrated in Figure 10.
Instead of leveraging the encoder-decoder models for sentence generation and ﬁne-
tune them on document expansion, a different approach computes the importance of
all terms in the vocabulary w.r.t. a given document and selects the most important
new terms to enrich the document, leveraging an encoder-only architecture to com-
pute the document embeddings. TILDEv2 [Zhuang and Zuccon 2021b] exploits the
BERT model to compute the [CLS] output embedding of a document, and linearly
projects it over the whole BERT vocabulary. In doing so, TILDEv2 computes a prob-
ability distribution over the vocabulary, i.e., a document language model, and then
adds to the document a certain number of new terms, corresponding to those with the
highest probabilities. As another way of expanding documents, SparTerm [Bai et al.
2020] computes a document language model for each BERT output token, including
[CLS], and sums them up to compute the term importance distribution over the vocab-
ulary for the given document. Finally, a learned gating mechanism only keeps a sparse

36

Figure 10: Example of generated queries using DocT5Query. Black terms denote
boosted important terms, red terms denote new important terms not present in the
original document.

subset of those, to compute the ﬁnal expanded document contents.

5.2 Impact score learning

Classical inverted indexes store statistical information on term occurrences in docu-
ments in posting lists, one per term in the collection. Every posting list stores a post-
ing for each document in which the corresponding term appears in, and the posting
contains a document identiﬁer and the in-document term frequency, i.e., a positive in-
teger counting the number of occurrences of the term in the document. When a new
query arrives, the posting lists of the terms in the query are processed to compute the
top scoring documents, using a classical ranking function, such as BM25, and efﬁcient
query processing algorithms [Tonellotto et al. 2018].
The goal of impact score learning is to leverage the document embeddings generated
by an encoder-only model to compute a single integer value to be stored in postings,
and to be used as a proxy of the relevance of the term in the corresponding posting, i.e.,
its term importance. The simplest way to compute term importance in a document is to
project the document embeddings of each term with a neural network into a single-
value representation, ﬁltering out negative values with ReLU functions and discarding
zeros. To save space, the real values can be further quantised into a 8-bit positive inte-
gers. A common problem in impact score learning is the vocabulary to use. Since most
encoder-only models use a sub-word tokeniser, the collection vocabulary can be con-
structed in two different ways: by using the terms produced by the encoder-speciﬁc
sub-word tokeniser, e.g., by BERT-like tokenisers, or by using the terms produced by

37

The presence of communication amid scientiﬁc minds was equally important to the success of the Manhattan Project as scientiﬁc intellect was.DocumentWhy the success of the Manhattan Project is important?what were the goals of the Manhattan Project?why was communication important to the success of the Manhattan Project? did the Manhattan Project really help scientists in the future? why was the Manhattan Project signiﬁcant?Generated QueriesDocT5Querya word tokeniser. These two alternatives have an impact on the ﬁnal inverted index:
in the former case, we have fewer terms, but longer and denser posting lists, while in
the latter case, we have more terms, with shorter posting lists and with smaller query
processing times [Mallia et al. 2022].
The current impact score learning systems are DeepCT [Dai and Callan 2019b, 2020],
DeepImpact [Mallia et al. 2021], TILDEv2 [Zhuang and Zuccon 2021a,b], and UniCOIL [Lin
and Ma 2021].
DeepCT [Dai and Callan 2019b, 2020] represents the ﬁrst example of term importance
boosting. DeepCT exploits the contextualised word representations from BERT to learn
new in-document term frequencies, to be used with classical ranking functions such
as BM25. For each term wi ∈ V in a given document, DeepCT estimates its context-
speciﬁc importance zi ∈ R, that is then scaled and rounded as frequency-like integer
value t fi that can be stored in an inverted index. Formally, for each document d ∈ D,
DeepCT projects the (cid:96)-dimensional representations ψi for each input BERT token wi
in the document, with i = 1, . . . , |d|, into a scalar term importance with the learned
matrix W ∈ R1×(cid:96):

[ψ0, ψ1, . . .] = Encoder(d)
zi = Wψi

(24)

DeepCT is trained with a per-token regression task, trying to predict the importance
of the terms. The actual term importance to predict is derived from the document
containing the term, or from a training set of query, relevant document pairs. A term
appearing in multiple relevant documents and in different queries has a higher im-
portance than a term matching fewer documents, and/or fewer queries. To handle
BERT’s sub-word tokens, DeepCT uses the importance of the ﬁrst sub-word token for
the entire word, and when a term occurs multiple times in the document, it takes the
maximum importance across the multiple occurrences.
DeepImpact [Mallia et al. 2021] proposes for the ﬁrst time to directly compute an im-
pact score for each unique term in a document, without resorting to classical ranking
functions, but simply summing up, at query processing time, the impacts of the query
terms appearing in a document to compute its relevance score. For each term wi ∈ V
in a given document d ∈ D, DeepImpact estimates its context-speciﬁc impact zi ∈ R.
DeepImpact feeds the encoder-only model with the document sub-word tokens, pro-
ducing an embedding for each input token. A non-learned gating layer Mask removes
the embeddings of the sub-word tokens that do not correspond to the ﬁrst sub-token of
the whole word. Then DeepImpact transforms the remaining (cid:96)-dimensional represen-
tations with two feed forward networks with ReLU activations. The ﬁrst network has

38

a weight matrix W1 ∈ R(cid:96)×(cid:96), and the second network has a weight matrix W2 ∈ R1×(cid:96):

[ψ0, ψ1, . . .] = Encoder(DocT5Query(d))
[x0, x1, . . .] = Mask(ψ0, ψ1, . . .)

yi = ReLU(W1xi)
zi = ReLU(W2yi)

(25)

The output real numbers zi, with i = 1, . . . , |d|, one per whole word in the input doc-
ument, are then linearly quantised into 8-bit integers that can be stored in an inverted
index. This produces a single-value score for each unique term in the document, rep-
resenting its impact. Given a query q, the score of the document d is simply the sum
of impacts for the intersection of terms in q and d. DeepImpact is trained with query,
relevant document, non-relevant document triples, and, for each triple, two scores for
the corresponding two documents are computed. The model is optimized via pair-
wise cross-entropy loss over the document scores. Moreover, DeepImpact has been the
ﬁrst sparse learned model leveraging at the same time documents expansion learn-
ing and impact score learning. In fact, DeepImpact leverages DocT5Query to enrich the
document collection before learning the term impact.
TILDEv2 [Zhuang and Zuccon 2021b] computes the terms’ impact with an approach
similar to DeepImpact. The main differences are (i) the use of a single layer feed forward
network with ReLU activations, instead of a two-layer network, to project the docu-
ment embeddings into a single positive scalar value using a learned matrix W ∈ R1×(cid:96),
(ii) the use of its own document expansion technique, as discussed in Section 5.1, (iii)
the use of an index with sub-word terms instead of whole word terms, and (iv) the
selection of the highest-valued impact score for a token if that token appears multiple
times in a document:

[ψ0, ψ1, . . .] = Encoder(TILDEv2(d))
zi = ReLU(Wψi)

(26)

The zi scores are then summed up, obtaining an accumulated query-document score.
UniCOIL [Lin and Ma 2021] exploits the COIL approach (see Sec. 3), but instead of pro-
jecting the query and document embeddings on 8-32 dimensions, it projects them to
single-dimension query weights and document weights. In UniCOIL the query and
document [CLS] embeddings are not used, and the embeddings corresponding to nor-
mal query and document tokens are projected into single scalar values v1, . . . , v|d| us-
ing a learned matrix W ∈ R1×(cid:96), with ReLU activations on the output term weights of

39

the base COIL model to force the model to generate non-negative weights.

[φ0, φ1, . . .] = Encoder(q)
[ψ0, ψ1, . . .] = Encoder(DocT5Query(d))
[v1, v2, . . .] = [Wφ1, Wφ2, . . .]
[z1, z2, . . .] = [Wψ1, Wψ2, . . .]

s(q, d) = ∑
ti∈q

max
tj∈d, tj=ti

vizj

(27)

The document weights zi are then linearly quantised into 8-bit integers, and the ﬁnal
query-document score is computed by summing up the highest valued document im-
pact scores times its query weight vi, computed at query processing time, as in Eq. (27).

5.3 Sparse representation learning

Instead of independently learning to expand the documents and then learning the
impact score of the terms in the expanded documents, sparse representation learning
aims at learning both at the same time. At its core, sparse representation learning
projects the output embeddings of an encoder-only model into the input vocabulary,
compute, for each input term in the document, a language model, i.e., a probability
distribution over the whole vocabulary. These term-based language models capture
the semantic correlations between the input term and all other terms in the collection,
and they can be used to (i) expand the input text with highly correlated terms, and (ii)
compress the input text by removing terms with low probabilities w.r.t. the other terms.
Encoder-only models such as BERT already compute term-based language models,
as part of their training as masked language models. Formally, given a document
d, together with the output embeddings ψ[CLS], ψ1, . . . , ψ|d|, an encoder-only model
also returns the masked language heads χ1, . . . , χ|d|, one for each token in the document,
where χi ∈ R|V | for i = 1, . . . , |d| is an estimation of the importance of each word in the
vocabulary implied by the i-th token in the document d. EPIC [MacAvaney et al. 2020a]
and SparTerm [Bai et al. 2020] have been the ﬁrst systems focusing on vocabulary-based
expansion and importance estimation, and inspired the SPLADE [Formal et al. 2021]
system, on which we focus.
For a given document d ∈ D, SPLADE computes its per-token masked language heads
χ1, . . . , χ|d| using BERT, ﬁlters and sums up these vocabulary-sized vectors into a sin-
gle vector γ(d) ∈ R|V | representing the whole document, and then uses this vector to

40

represent the document itself, together with the term importance scores:

[χ1, . . . , χ|d|] = Encoder(d)

γ(d) =

|d|
∑
i=1

log (cid:0)1 + ReLU(χi)(cid:1)

(28)

The logarithm and ReLU functions in Eq. (28) are computed element-wise; the loga-
rithm prevents some terms with large values from dominating, and the ReLU function
deals with the negative components of γ(d).
The document representation γ potentially contains all terms in the vocabulary, even
if the logarithm and ReLU functions in Eq. (28) can zero out some of its components.
To learn to “sparsify” the document representations, Formal et al. [2021] leverage the
FLOPS regulariser LFLOPS [Paria et al. 2020]. As part of the SPLADE loss function
used during training, the FLOPS loss is computed as the sum, across the terms in the
vocabulary, of the squared probability p2
w that a term w has a non-zero weight in a doc-
ument. Minimising the FLOPS loss coincides with minimising the non-zero weights in
a document, i.e., maximising the number of zero weights in a document. The square
operation helps in reducing high term weights more than low term weights. The prob-
ability that a term w ∈ V has a non-zero weight in a document d is proportional to the
average weight of that term γt(d) estimated through the whole collection. To make
the computation feasible, the average is computed on a batch b of documents during
training, considered as a representative sample of the whole collection:

LFLOPS = ∑
t∈V

t = ∑
p2
t∈V

(cid:18) 1
|b|

∑
d∈b

(cid:19)2

γt(d)

(29)

SPLADE does not limit expansion to documents only. Indeed, Eq. (28) can be applied
to a query q as well, to compute the corresponding vector γ(q) ∈ R|V |. However,
this query expansion must be carried out at query processing time; to reduce the la-
tency, the expanded query should be far more sparse than a document. To enforce
this different behaviour, Formal et al. [2021] adopt two distinct FLOPS regularisers for
documents and queries, both as in Eq. 29.

6 Conclusions

This overview aimed to provide the foundations of neural IR approaches for ad-hoc
ranking, focusing on the core concepts and the most commonly adopted neural archi-
tecture in current research. In particular, in Section 1 we provided a background on

41

the different ways to represent texts, such as queries and documents, to be processed
in IR systems. Sections 2 and 3 illustrated the main system architectures in neural
IR, namely interaction-focused and representation-focused systems. In Section 4 we
reviewed the multi-stage retrieval architectures exploiting neural IR systems, and we
provided a quick overview of the embedding indexes and algorithms for dense re-
trieval. Finally, in Section 5, we discussed the main state-of-the-art solutions for learn-
ing sparse representations.
Neural IR systems are currently a hot research topic, and many excellent surveys com-
plement and deepen the concepts discussed in this overview. The interested readers
can ﬁnd a fully-ﬂedged detailed presentation of pre-trained transformers for ranking,
with many experimental evaluations, in the recent book by [Lin et al. 2021], as well
as applications of dense retrieval to conversational system in the books by Gao et al.
[2022] and Zamani et al. [2022].

42

References

Y. Bachrach, Y. Finkelstein, R. Gilad-Bachrach, L. Katzir, N. Koenigstein, N. Nice, and
U. Paquet. 2014. Speeding up the Xbox Recommender System Using a Euclidean
Transformation for Inner-Product Spaces. In Proc. RecSys, p. 257–264.

Y. Bai, X. Li, G. Wang, C. Zhang, L. Shang, J. Xu, Z. Wang, F. Wang, and Q. Liu. 2020.
Sparterm: Learning term-based sparse representation for fast text retrieval. Preprint:
arXiv:2010.00768.

M. Bendersky, W. B. Croft, and Y. Diao. 2011. Quality-biased ranking of web docu-

ments. In Proc. WSDM, pp. 95–104.

L. S. Blackford, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, J. Demmel, J. Don-
garra, I. Duff, S. Hammarling, G. Henry, et al. 2002. An Updated Set of Basic Linear
Algebra Subprograms (BLAS). ACM Transanctions on Mathematical Software, 28(2):
135–151.

J. Bromley, I. Guyon, Y. LeCun, E. S¨ackinger, and R. Shah. 1993. Signature Veriﬁcation

using a ”Siamese” Time Delay Neural Network. In Proc. NIPS.

C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.

2005. Learning to Rank Using Gradient Descent. In Proc. ICML, p. 89–96.

C. J. C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An overview.

Microsoft Research Technical Report.

S. B ¨uttcher, C. Clarke, and G. V. Cormack. 2010. Information Retrieval: Implementing and

Evaluating Search Engines. The MIT Press.

B. B. Cambazoglu and R. A. Baeza-Yates. 2015. Scalability Challenges in Web Search

Engines. Morgan & Claypool Publishers.

Z. Dai and J. Callan. 2019a. Deeper Text Understanding for IR with Contextual Neural

Language Modeling. In Proc. SIGIR, p. 985–988.

Z. Dai and J. Callan. 2019b. Context-aware sentence/passage term importance esti-

mation for ﬁrst stage retrieval. Preprint: arXiv:1910.10687.

Z. Dai and J. Callan. 2020. Context-aware document term weighting for ad-hoc search.

In Proc. WWW, pp. 1897–1907.

43

Z. Dai, C. Xiong, J. Callan, and Z. Liu. 2018. Convolutional Neural Networks for

Soft-Matching N-Grams in Ad-Hoc Search. In Proc. WSDM, p. 126–134.

M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. 2004. Locality-Sensitive Hashing

Scheme Based on p-Stable Distributions. In Proc. SoCG, p. 253–262.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. 2019. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proc. NAACL, pp. 4171–
4186.

W. Dong, C. Moses, and K. Li. 2011. Efﬁcient K-Nearest Neighbor Graph Construction

for Generic Similarity Measures. In Proc. WWW, p. 577–586.

A. Fan, M. Lewis, and Y. Dauphin. 2018a. Hierarchical Neural Story Generation. In

Proc. ACL.

Y. Fan, J. Guo, Y. Lan, J. Xu, C. Zhai, and X. Cheng. 2018b. Modeling Diverse Relevance

Patterns in Ad-Hoc Retrieval. In Proc. SIGIR, p. 375–384.

T. Formal, B. Piwowarski, and S. Clinchant. 2021. SPLADE: Sparse Lexical and Expan-

sion Model for First Stage Ranking. In Proc. SIGIR, p. 2288–2292.

J. Gao, C. Xiong, P. Bennet, and N. Craswell, 2022. Neural approaches to conversa-

tional information retrieval.

L. Gao, Z. Dai, and J. Callan. 2021. COIL: Revisit Exact Lexical Match in Information
Retrieval with Contextualized Inverted List. In Proc. NAACL-HLT, pp. 3030–3042.

A. Gersho and R. M. Gray. 1992. Vector Quantization and Signal Compression. Kluwer.

F. C. Gey. 1994. Inferring Probability of Relevance Using the Method of Logistic Re-

gression. In Proc. SIGIR, p. 222–231.

D. Gillick, S. Kulkarni, L. Lansing, A. Presta, J. Baldridge, E. Ie, and D. Garcia-Olano.
2019. Learning Dense Representations for Entity Retrieval. In Proc. CoNLL, pp. 528–
537.

J. Guo, Y. Fan, Q. Ai, and W. B. Croft. 2016. A Deep Relevance Matching Model for

Ad-Hoc Retrieval. In Proc. CIKM, p. 55–64.

J. Guo, Y. Fan, L. Pang, L. Yang, Q. Ai, H. Zamani, C. Wu, W. B. Croft, and X. Cheng.
2020. A deep look into neural ranking models for information retrieval. Information
Processing & Management, 57(6): 1–20.

44

Z. S. Harris. 1954. Distributional structure. Word, 10(23): 146–162.

B. Hu, Z. Lu, H. Li, and Q. Chen. 2014. Convolutional Neural Network Architectures

for Matching Natural Language Sentences. In Proc. NIPS, p. 2042–2050.

P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. 2013. Learning Deep Struc-
tured Semantic Models for Web Search Using Clickthrough Data. In Proc. CIKM, p.
2333–2338.

K. Hui, A. Yates, K. Berberich, and G. de Melo. 2017. PACRR: A Position-Aware Neural

IR Model for Relevance Matching. In Proc. EMNLP, pp. 1049–1058.

K. Hui, A. Yates, K. Berberich, and G. de Melo. 2018. Co-PACRR: A Context-Aware

Neural IR Model for Ad-Hoc Retrieval. In Proc. WSDM, p. 279–287.

S. Humeau, K. Shuster, M.-A. Lachaux, and J. Weston. 2019. Real-time inference in
multi-sentence tasks with deep pretrained transformers. DeepAI Technical Report.

P. Indyk and R. Motwani. 1998. Approximate Nearest Neighbors: Towards Removing

the Curse of Dimensionality. In Proc. STOC, p. 604–613.

H. J´egou, M. Douze, and C. Schmid. 2011. Product Quantization for Nearest Neighbor
Search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1): 117–128.

J. Johnson, M. Douze, and H. J´egou. 2021. Billion-Scale Similarity Search with GPUs.

IEEE Transactions on Big Data, 7(03): 535–547. ISSN 2332-7790.

T. B. Johnson and C. Guestrin. 2018. Training Deep Models Faster with Robust, Ap-

proximate Importance Sampling. In Proc. NeurIPS.

A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov. 2017. Bag of Tricks for Efﬁcient

Text Classiﬁcation. In Proc. EACL, pp. 427–431.

D. Jurafsky and J. H. Martin. 2009. Speech and language processing : an introduction to
natural language processing, computational linguistics, and speech recognition. Pearson
Prentice Hall.

V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih.
In Proc.

2020. Dense Passage Retrieval for Open-Domain Question Answering.
EMNLP, pp. 6769–6781.

A. Katharopoulos and F. Fleuret. 2018. Not All Samples Are Created Equal: Deep

Learning with Importance Sampling. In Proc. ICML.

45

O. Khattab and M. Zaharia. 2020. ColBERT: Efﬁcient and Effective Passage Search via

Contextualized Late Interaction over BERT. In Proc. SIGIR, p. 39–48.

J. Kleinberg. 2000. The Small-World Phenomenon: An Algorithmic Perspective. In

Proc. STOC, pp. 163–170.

V. Lavrenko and W. B. Croft. 2001. Relevance-based Language Models. In Proc. SIGIR,

pp. 120–127.

Y. LeCun and Y. Bengio. 1998. Convolutional Networks for Images, Speech, and Time
Series. In The Handbook of Brain Theory and Neural Networks, pp. 255–258. MIT Press.

M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
and L. Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for
Natural Language Generation, Translation, and Comprehension. In Proc. ACL, pp.
7871–7880.

C. Li, A. Yates, S. MacAvaney, B. He, and Y. Sun. 2020. Parade: Passage representation

aggregation for document reranking. arXiv preprint arXiv:2008.09093.

J. Lin and X. Ma. 2021. A few brief notes on DeepImpact, COIL, and a conceptual

framework for information retrieval techniques. Preprint: arXiv:2106.14807.

J. Lin, R. Nogueira, and A. Yates. 2021. Pretrained Transformers for Text Ranking: BERT
and Beyond. Synthesis Lectures on Human Language Technologies. Morgan & Clay-
pool Publishers.

T.-Y. Liu. 2009. Learning to Rank for Information Retrieval. Foundations and Trends in

Information Retrieval, 3(3): 225–331.

Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,
and V. Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Ap-
proach. ArXiv, abs/1907.11692.

Y. Luan, J. Eisenstein, K. Toutanova, and M. Collins. 2021. Sparse, Dense, and Atten-
tional Representations for Text Retrieval. Transactions of the Association for Computa-
tional Linguistics, 9: 329–345.

Q. Lv, W. Josephson, Z. Wang, M. Charikar, and K. Li. 2007. Multi-Probe LSH: Efﬁcient

Indexing for High-Dimensional Similarity Search . In Proc. VLDB, pp. 950–961.

S. MacAvaney, A. Yates, A. Cohan, and N. Goharian. 2019. CEDR: Contextualized

Embeddings for Document Ranking. In Proc. SIGIR, p. 1101–1104.

46

S. MacAvaney, F. M. Nardini, R. Perego, N. Tonellotto, N. Goharian, and O. Frieder.
In Proc.

2020a. Expansion via Prediction of Importance with Contextualization.
SIGIR, pp. 1573–1576.

S. MacAvaney, F. M. Nardini, R. Perego, N. Tonellotto, N. Goharian, and O. Frieder.
2020b. Efﬁcient Document Re-Ranking for Transformers by Precomputing Term
Representations. In Proc. SIGIR, pp. 49–58.

C. Macdonald and N. Tonellotto. 2021. On approximate nearest neighbour selection

for multi-stage dense retrieval. In Proc. CIKM, p. 3318–3322.

C. Macdonald, R. L. T. Santos, and I. Ounis. 2012. The whens and hows of learning to

rank for web search. Information Retrieval, 16(5): 584–628.

Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. 01 2013. Approximate nearest
neighbor algorithm based on navigable small world graphs. Information Systems, 45:
61–68.

Y. A. Malkov and D. A. Yashunin. 2020. Efﬁcient and Robust Approximate Nearest
Neighbor Search Using Hierarchical Navigable Small World Graphs. IEEE Transac-
tions on Pattern Analysis & Machine Intelligence, 42(04): 824–836.

A. Mallia, O. Khattab, T. Suel, and N. Tonellotto. 2021. Learning Passage Impacts for

Inverted Indexes. In Proc. SIGIR, pp. 1723–1727.

A. Mallia, J. Mackenzie, T. Suel, and N. Tonellotto. 2022. Faster Learned Sparse Re-

trieval with Guided Traversal. In Proc. SIGIR, p. 5.

C. D. Manning, P. Raghavan, and H. Sch ¨utze. 2008. Introduction to Information Retrieval.

Cambridge University Press.

T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed Repre-

sentations of Words and Phrases and their Compositionality. In Proc. NIPS.

B. Neyshabur and N. Srebro. 2015. On Symmetric and Asymmetric LSHs for Inner

Product Search. In Proc. ICML, pp. 1926––1934.

R. Nogueira and K. Cho. 2019. Passage Re-ranking with BERT. arXiv 1901.04085.

R. Nogueira and J. Lin. 2019. From doc2query to docTTTTTquery. Online preprint.

R. Nogueira, W. Yang, K. Cho, and J. Lin. 2019a. Multi-stage document ranking with

BERT. arXiv 1910.14424.

47

R. Nogueira, W. Yang, J. Lin, and K. Cho. 2019b. Document expansion by query

prediction. Preprint: arXiv:1904.08375.

R. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. 2020. Document Ranking with a Pre-

trained Sequence-to-Sequence Model. In Proc. EMNLP, pp. 708–718.

L. Pang, Y. Lan, J. Guo, J. Xu, and X. Cheng. 2016. A Study of MatchPyramid Models

on Ad-hoc Retrieval. arXiv 1606.04648.

L. Pang, Y. Lan, J. Guo, J. Xu, J. Xu, and X. Cheng. 2017. DeepRank: A New Deep
In Proc. CIKM, pp.

Architecture for Relevance Ranking in Information Retrieval.
257–266.

B. Paria, C. Yeh, I. E. Yen, N. Xu, P. Ravikumar, and B. P ´oczos. 2020. Minimizing

FLOPs to Learn Efﬁcient Sparse Representations. In Proc. ICLR.

J. Pennington, R. Socher, and C. D. Manning. 2014. GloVe: Global Vectors for Word

Representation. In Proc. EMNLP, pp. 1532–1543.

F. Petroni, T. Rockt¨aschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and A. Miller. 2019.
Language Models as Knowledge Bases? In Proc. EMNLP-IJCNLP, pp. 2463–2473.

J. M. Ponte and W. B. Croft. 1998. A Language Modeling Approach to Information

Retrieval. In Proc. SIGIR, pp. 275–281.

A. Radford and K. Narasimhan. 2018. Improving Language Understanding by Gen-

erative Pre-Training. OpenAI Techical Report.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. 2019. Language

Models are Unsupervised Multitask Learners. OpenAI Technical report.

C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and
P. J. Liu. 2020. Exploring the Limits of Transfer Learning with a Uniﬁed Text-to-Text
Transformer. Journal of Machine Learning Research, 21(140): 1–67.

S. Robertson and H. Zaragoza. 2009. The Probabilistic Relevance Framework: BM25

and Beyond. Foundations and Trends in Information Retrieval, 3(4): 333–389.

S. E. Robertson. 1977. The Probability Ranking Principle in IR. Journal of Documentation,

33(4): 294–304.

J. Rocchio. 1971. Relevance feedback in information retrieval. The Smart Retrieval

System-experiments in Automatic Document Processing, pp. 313–323.

48

G. Salton, A. Wong, and C. S. Yang. 1975. A Vector Space Model for Automatic Index-

ing. Communications of the ACM, 18(11): 613–620.

V. Sanh, L. Debut, J. Chaumond, and T. Wolf. 2019. DistilBERT, a distilled version of
BERT: smaller, faster, cheaper and lighter. In Proc. 5th Workshop on Energy Efﬁcient
Machine Learning and Cognitive Computing @ NeurIPS 2019.

N. Tonellotto and C. Macdonald. 2021. Query Embedding Pruning for Dense Retrieval.

In Proc. CIKM, p. 3453–3457.

N. Tonellotto, C. Macdonald, and I. Ounis. 2018. Efﬁcient query processing for scalable

web search. Foundations and Trends in Information Retrieval, 12(4–5): 319–492.

J. Urbanek, A. Fan, S. Karamcheti, S. Jain, S. Humeau, E. Dinan, T. Rockt¨aschel,
D. Kiela, A. Szlam, and J. Weston. 2019. Learning to speak and act in a fantasy
text adventure game. In Proc. EMNLP-IJCNLP, pp. 673–683.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and

I. Polosukhin. 2017. Attention is All You Need. In Proc. NeurIPS, p. 6000–6010.

Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao,
2016. Google’s neural machine translation sys-
arXiv preprint

Q. Gao, K. Macherey, et al.
tem: Bridging the gap between human and machine translation.
arXiv:1609.08144.

E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. 2002. Distance Metric Learning, with

Application to Clustering with Side-Information. In Proc. NIPS, pp. 521–528.

C. Xiong, Z. Dai, J. Callan, Z. Liu, and R. Power. 2017. End-to-End Neural Ad-Hoc

Ranking with Kernel Pooling. In Proc. SIGIR, p. 55–64.

L. Xiong, C. Xiong, Y. Li, K.-F. Tang, J. Liu, P. Bennett, J. Ahmed, and A. Overwijk.
2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text
Retrieval. In Proc. ICLR.

H. Zamani, J. R. Trippas, J. Dalton, and F. Radlinski, 2022. Conversational information

seeking.

J. Zhan, J. Mao, Y. Liu, J. Guo, M. Zhang, and S. Ma. 2021a.

Query Encoder and Product Quantization to Improve Retrieval Performance.
Proc. CIKM, pp. 2487–2496.

Jointly Optimizing
In

49

J. Zhan, J. Mao, Y. Liu, J. Guo, M. Zhang, and S. Ma. 2021b. Optimizing Dense Retrieval

Model Training with Hard Negatives. In Proc. SIGIR, pp. 1503–1512.

J. Zhan, J. Mao, Y. Liu, J. Guo, M. Zhang, and S. Ma. 2022. Learning Discrete Repre-
sentations via Constrained Clustering for Effective and Efﬁcient Dense Retrieval. In
Proc. WSDM, pp. 1328–1336.

H. Zhang, H. Shen, Y. Qiu, Y. Jiang, S. Wang, S. Xu, Y. Xiao, B. Long, and W.-Y. Yang.
2021. Joint Learning of Deep Retrieval Model and Product Quantization Based Em-
bedding Index. In Proc. SIGIR, pp. 1718–1722.

L. Zhao. 2012. Modeling and solving term mismatch for full-text retrieval. PhD thesis,

Carnegie Mellon University.

S. Zhuang and G. Zuccon. 2021a. TILDE: Term Independent Likelihood MoDEl for

Passage Re-Ranking. In Proc. SIGIR, pp. 1483–1492.

S. Zhuang and G. Zuccon. 2021b. Fast passage re-ranking with contextualized exact

term matching and efﬁcient passage expansion. Preprint: arXiv:2108.08513.

50

