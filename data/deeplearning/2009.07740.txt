1
2
0
2

n
a
J

2
1

]
L
C
.
s
c
[

4
v
0
4
7
7
0
.
9
0
0
2
:
v
i
X
r
a

Automated Source Code Generation and Auto-completion Using Deep Learning:
Comparing and Discussing Current Language-Model-Related Approaches

Juan Cruz-Benito1* Sanjay Vishwakarma2†, Francisco Martin-Fernandez1, Ismael Faro1,
1IBM Quantum. IBM T.J. Watson Research Center.
Yorktown Heights, NY 10598, USA
2Electrical and Computer Engineering. Carnegie Mellon University.
Mountain View, USA
juan.cruz@ibm.com, svishwak@andrew.cmu.edu, paco@ibm.com, ismael.faro1@ibm.com

Abstract

In recent years, the use of deep learning in language mod-
els gained much attention. Some research projects claim that
they can generate text that can be interpreted as human-
writing, enabling new possibilities in many application ar-
eas. Among the different areas related to language process-
ing, one of the most notable in applying this type of model-
ing is programming languages. For years, the Machine Learn-
ing community has been researching this software engineer-
ing area, pursuing goals like applying different approaches to
auto-complete, generate, ﬁx, or evaluate code programmed by
humans. Considering the increasing popularity of the Deep-
Learning-enabled language models approach, we detected a
lack of empirical papers that compare different deep learn-
ing architectures to create and use language models based
on programming code. This paper compares different neu-
ral network architectures like AWD-LSTMs, AWD-QRNNs,
and Transformer while using transfer learning and different
tokenizations to see how they behave in building language
models using a Python dataset for code generation and ﬁll-
ing mask tasks. Considering the results, we discuss each ap-
proach’s different strengths and weaknesses and what gaps
we ﬁnd to evaluate the language models or apply them in a
real programming context.

Introduction
We are digitally surrounded by computational Language
Models (LMs) that guide us while writing to reduce the user
effort, suggest different options for words/sentences to en-
hance our style, or ﬁx our grammatical/correctness errors
accurately (Kannan et al. 2016; Bryant and Briscoe 2018;
Ghosh and Kristensson 2017). Many of the keys we press
while writing on a keyboard act as part of the inputs to com-
pose new datasets for those models that shape how we com-
municate with others. Nevertheless, does it happen in the
same way when we write code? Succinctly, yes. Accord-
ing to some recent surveys found in the literature (Alla-
manis et al. 2018; Chen, Le, and Babar 2020), the Natu-
ral Language Processing (NLP) subﬁeld related to program-
ming language includes examples of LMs used in several
tasks and contexts. For example, the authors of (Nguyen

*Contact author
†Intern at IBM Quantum at the time of writing this paper

and Nguyen 2015; Bielik, Raychev, and Vechev 2016; Cruz-
Benito et al. 2018) used different techniques such as graph-
based statistical LMs, probabilistic LMs, or Deep Learning
(DL) LMs to suggest code to programmers similarly to auto-
completer features in IDEs. LMs were used to generate auto-
mated source code based on sample code inputs or pseudo-
code and evaluating how this generated code performs (Oda
et al. 2015; Tiwang, Oladunni, and Xu 2019; Fedus, Good-
fellow, and Dai 2018). Another exciting application of NLP
into source code languages is the automatic translation be-
tween different languages. The work reported in (Nguyen,
Nguyen, and Nguyen 2013; Roziere et al. 2020) explores
different supervised and unsupervised approaches to mi-
grate code between different programming languages to im-
prove interoperability or port codebases written in obsolete
or deprecated languages (such as COBOL or Python2). An-
other example found is the use of Bayesian networks, atten-
tion mechanisms, and pointer networks (Proksch, Lerch, and
Mezini 2015; Li et al. 2017; Donahue, Lee, and Liang 2020)
to ﬁll a given code portion with missings.

There is a more general understanding of the natural lan-
guages’ different characteristics in the NLP broad ﬁeld.
Since there exist many research ﬁelds related to human lan-
guages, there is a richer background on existing language
characteristics. For example, there is much knowledge on
aspects like the minimal representation units of a word in a
speciﬁc language, the most used words of a language, or if
a word is a neologism or not. Programming languages share
some syntax similarities with spoken languages. However, it
does not have the same restrictions in the sense of common
words or neologisms (Allamanis et al. 2015; Karampatsis
and Sutton 2019), or other syntax restrictions and features
such as punctuation, format, or style. Every programming
language has indeed reserved words and symbols to denote
different actions, resources, or syntax. However, there is an
essential part of the source code that is only limited by the
programmer’s imagination, the conventions existing, or the
guides for good practices. As (Karampatsis and Sutton 2019)
claims,

[...] traditional language models limit the vocabulary to
a ﬁxed set of common words. For code, this strong as-
sumption has been shown to have a signiﬁcant negative
effect on predictive performance [...]

 
 
 
 
 
 
In that paper, Karampatsis and Sutton 2019 present how
segmenting words into subword units can improve source
code modeling. Similarly, other researchers (Ganin et al.
2016; Kim et al. 2016; Karpathy 2016) dug in representing
source code vocabulary with a similar emphasis on model-
ing words using sub-word units and envisioning their impor-
tance when using neural networks (NNs). Nevertheless, how
that word segmentation affect the accuracy or the appropri-
ateness of the code generated or auto-completed in some
modern LM using deep learning approaches? That kind of
question raises the main goal for this paper: discover what
kinds of associations between different modern neural net-
work architectures and tokenization models produce the best
results when creating LMs to generate and auto-complete
source code.

To pursue that goal, this research aims to conduct exper-
iments combining different deep neural network (DNN) ar-
chitectures with different tokenization and pre-trained mod-
els over an existing Python dataset. Using that experimenta-
tion, we want to investigate the combinations that improve
code generation and auto-completion tasks (for example,
ﬁlling the blanks) while checking the outcomes from those
tasks using metrics like accuracy and human assessment.

The rest of the paper is as follows: Section 2 presents
the different approaches followed during the research, the
DNNs used, the software methods and data employed. Sec-
tion 3 describes results achieved during the research accord-
ing to different metrics and tests, while section 4 discusses
these ﬁndings and the implications of the results as appro-
priate. Finally, Section 5 presents some conclusions.

Materials and methods
We have trained a set of deep neural networks using different
architectures, tokenization techniques, and software libraries
to develop this research work. Following, we introduce the
different materials and methods employed for that purpose.

Deep Neural Networks and tokenization models
used

Regarding the DNN architectures employed, we chose to use
the following ones: ASGD Weight-Dropped LSTM (AWD-
LSTM) (Merity, Keskar, and Socher 2018b), Quasi Recur-
rent Neural Networks (QRNNs) (Bradbury et al. 2017), and
Transformer (Vaswani et al. 2017). These DNN architec-
tures have been reportedly getting some state-of-the-art re-
sults (Merity, Keskar, and Socher 2018b; Wang, Gong, and
Liu 2019; Gong et al. 2018; Takase, Suzuki, and Nagata
2018; Yang et al. 2018; Krause et al. 2018; Rae et al. 2019;
Dai et al. 2019; Baevski and Auli 2018; Merity, Keskar, and
Socher 2018a; Howard and Ruder 2018; Eisenschlos et al.
2019; Brown et al. 2020; Devlin et al. 2019) recently in the
NLP ﬁeld in several groundbreaking digital products1,2 and
in some of the most known datasets like the Penn Tree Bank
(Mikolov et al. 2011), WikiText-2 and WikiText-103 (Mer-

1https://openai.com/blog/openai-api/
2https://blog.google/products/search/search-language-

understanding-bert/

ity et al. 2017), the One-Billion Word benchmark (Chelba
et al. 2014), or The Hutter Prize Wikipedia dataset 3.

The AWD-LSTM is a variation of the famous Long
Short-Term Memory (LSTM) architecture (Hochreiter and
Schmidhuber 1997). The LSTM is a type of Recurrent Neu-
ral Network (RNN) especially capable of processing and
predicting sequences. That ability with sequences is the rea-
son why LSTMs have been employed largely in LMs (Kim
et al. 2016). The AWD-LSTM includes several optimiza-
tions compared to the regular LSTM. Two of the most im-
portant ones are the use of Average Stochastic Gradient De-
scent (ASGD) and the WeightDropout. The ASGD is used
as the NN’s optimizer to consider the previous weights (not
only the current one) during each training iteration. The
WeightDropout introduces the dropout technique (Srivastava
et al. 2014) to avoid overﬁtting but with the characteristic of
returning zero, not with a subset of activations between lay-
ers, like in traditional dropout, but with a random subset of
weights.

The QRNN is another type of RNN that includes alternate
convolutional and pooling layers in the architecture. This de-
sign makes the QRNN able to capture better long-term se-
quences and train much faster since the convolutional layers
allow the computation of intermediate representations from
the input in parallel. They can be up to 16 times faster at train
and test time than LSTMs while having better predictive ac-
curacy than stacked LSTMs of the same hidden size. We
use a QRNN modiﬁed (AWD-QRNN) to include the same
ASGD and WeightDropout modiﬁcations to improve its sta-
bility and optimize its capabilities, as for the AWD-LSTM.
We utilize AWD-LSTM and AWD-QRNN to produce
LMs capable of solving the task of generating source code
based on input as in the literature (Merity, Keskar, and
Socher 2018a,b; Wang, Gong, and Liu 2019; Gong et al.
2018; Takase, Suzuki, and Nagata 2018; Yang et al. 2018;
Krause et al. 2018; Merity, Keskar, and Socher 2018a;
Howard and Ruder 2018; Eisenschlos et al. 2019).

The Transformer is probably the most popular current
DNN architecture in NLP due to its performance and re-
cent state-of-the-art results in many tasks. It is an encoder-
decoder architecture in which each layer uses attention
mechanisms. This use of (self-)attention mechanisms makes
Transformer able to model the relationships between all
words in a sentence regardless of their respective position.
That implies a signiﬁcant improvement over RNNs, en-
abling much more parallelization of data processing and un-
blocking the training over more massive datasets. The excel-
lent results of the Transformer architecture empowered the
NLP community to create new state-of-the-art transformer-
based models (Young et al. 2018) like those used in the cur-
rent research: GPT-2 (Radford et al. 2019), BERT (Devlin
et al. 2019), and RoBERTa (Liu et al. 2019).

We choose to use GPT-2 since it is a causal transformer
(unidirectional) that can predict the next token in a sequence.
So, it can generate source code based on input, allowing
us to compare the results with the AWD-LSTM and AWD-
QRNN experiments. Regarding BERT and RoBERTA, we

3http://prize.hutter1.net/

used them to study how a masked modeling approach can
perform auto-complete source code. In that case, we did not
use them for text generation, as in the other experiments,
since BERT and RoBERTa are not designed for text gen-
eration. However, they can generate text (more diverse but
slightly worse in quality) (Wang and Cho 2019).

Considering the tokenization techniques, for every AWD-
LSTM and AWD-QRNN, we chose the following types of
tokens: word, unigram, char, and byte-pair encoding (BPE)
(Sennrich, Haddow, and Birch 2016) -albeit some stud-
ies show that BPE is suboptimal for pre-training (Bostrom
and Durrett 2020)-. For the Transformer models, we used
the default ones from the pre-deﬁned models: Wordpiece
method (Schuster and Nakajima 2012) for BERT and BPE
over raw bytes instead of Unicode characters for GPT-2 and
RoBERTa. The different techniques were selected because
they produce different tokens’ granularity that can enrich
our experimentation: full words, sub-words of speciﬁc sizes,
character-sized tokens, or byte pairs. Also, they enable us
to compare the tokenization between the different types of
models and tasks to solve.

Experimentation details
The dataset used for the experimentation is the Python
dataset included in the “GitHub CodeSearchNet Challenge
dataset” (Husain et al. 2019). It includes 2 million (com-
ment, Python code) pairs from open-source libraries. As ob-
served during the dataset preparation for our experiments,
there are about 11 million Python code sentences. The rea-
son to choose this dataset is that it has already been used
in previous research related to NLP and source code. The
full dataset includes several languages: Go, Java, JavaScript,
PHP, Python, and Ruby. We chose to use only the Python
part of the dataset because it enables us to compare the ex-
isting literature that uses Python language more than other
programming languages. The software libraries and pack-
ages used primarily during the research were the follow-
ing: FastAI (Howard and Gugger 2020), Google Senten-
cePiece (Kudo and Richardson 2018), and HuggingFace’s
Transformers (Wolf et al. 2020). he preprocessing applied
to the dataset included removing most of the code com-
ments and autoformatting the code according to the PEP-8
Python style guide using the autopep84 package. Regarding
the AWD-LSTM networks, we have been using the FastAI-
provided base models pre-trained using the Wikitext-103
dataset (Merity et al. 2017). There are no default pre-trained
models in the FastAI’s AWD-QRNN version of those net-
works, so we trained them from scratch. As we introduced,
regarding the Transformer architectures, we have been us-
ing three standard pre-trained models as a basis: GPT-2,
BERT, and RoBERTa. In each case, the exact pre-trained
model used were gpt2, bert-base-cased, and roberta-base.
These pre-trained models are available from HuggingFace’s
model5.

As the reader could infer from the previous explana-
tions about using pre-trained versions, we followed a trans-

4https://pypi.org/project/autopep8/
5https://huggingface.co/models

fer learning approach similar to other researchers in exist-
ing literature (Howard and Ruder 2018; Ruder et al. 2019;
Chronopoulou, Baziotis, and Potamianos 2019; Eisenschlos
et al. 2019). We employed the pre-trained models on En-
glish texts to later ﬁne-tune the models for the selected tasks
using the GitHub CodeSearchNet dataset. The deep-neural-
networks-related source code was coded using the FastAI
library (versions 1.0.61 and 2 dev 0.0.21). To apply the dif-
ferent tokenization techniques into the AWD-LSTMs and
QRNNs, we replaced the default Spacy tokenizer (Honnibal
and Montani 2017) with Google Sentencepiece (Kudo and
Richardson 2018), following a similar approach to (Czapla,
Howard, and Kardas 2018). In the development of Trans-
former architectures to see how they perform ﬁlling the
blanks and generating texts, we used HuggingFace’s Trans-
former library combined with FastAI v2 (following the Fas-
tAI’s example6), as included on the code repository that
supports this paper. To train the neural networks, we have
used some techniques worth to mention (all the details are
in the code repository). To ﬁnd the most appropriate learn-
ing rate to use automatically, we used the function lr ﬁnd
provided by FastAI following the proposal of (Smith 2017).
This function trains the DNN over the dataset for a few itera-
tions while varying from very low to very high learning rates
at the beginning of each mini-batch of data to ﬁnd which
is optimal one regarding error (loss) metrics until the DNN
diverges. To pursue a faster convergence, we schedule the
learning rate as described in (Smith and Topin 2019) using
the one cycle policy (ﬁt one cycle) in FastAI. Considering
the transfer learning technique used, we trained the ﬁrst “one
cycle” on the top of the existing pre-trained model to later
unfreeze all the model layers and do a more extended train-
ing (10-30 epochs) to improve the results. Regarding other
training details, in general, we used the default parameters
from FastAI, except for a ﬁxed multiplier to control all the
dropouts (drop mult) in AWD-LSTMs and AWD-QRNNs
set to 0.3 because of some heuristics discovered during test-
ing this research. Also, we decided to train similar archi-
tectures using a ﬁxed number of epochs to make the mod-
els comparable. For the AWD-LSTM and AWD-QRNN, we
used 30 epochs for ﬁne-tuning because we found during
the experimentation that the most remarkable improvement
for every model produced occurs during that range of it-
erations. Similarly, we ﬁne-tuned the transformers for ten
epochs since we did not ﬁnd a signiﬁcant improvement after
that. For more information about the training setup and soft-
ware details, please refer to the repository that supports this
paper and the FastAI documentation.

Finally,

the hardware used to run the different soft-
ware and neural networks training was a computer running
Ubuntu Linux 18.04 LTS Bionic Beaver (64 bits). It has two
Nvidia Tesla V100 GPUs x 16 gigabytes of RAM (Nvidia
CUDA version 10.1), a CPU with 16 cores Intel(R) Xeon(R)
CPU E5-2690 v4 @ 2.60GHz, 120 gigabytes of RAM, and
120 Gigabytes for the primary disk (HDD).

All the supporting materials and software details related
to this paper are publicly available in a GitHub repository

6https://docs.fast.ai/tutorial.transformers

(Cruz-Benito and Vishwakarma 2020a). The NN models
produced are under the Zenodo record (Cruz-Benito and
Vishwakarma 2020b).

Results

This section presents the results achieved after the full train-
ing of the selected DNN architectures with the different to-
kenization models.

As outlined in the previous section, we trained AWD-
LSTM and AWD-QRNN DNN architectures using differ-
ent tokenization models - word, unigram, BPE, and char-,
and Transformer, using three different base models (GPT-2,
BERT, and RoBERTa). We trained every AWD-LSTM and
AWD-QRNN using one epoch to ﬁt the model’s head and
ﬁne-tuned for 30 epochs. Meanwhile, the Transformer net-
works were trained equally for one epoch to ﬁt the head and
ﬁne-tune the models for ten epochs.

We followed a two-way strategy to evaluate the NNs
trained: use the NN training metrics and human evaluation
of the models’ output. The metrics used are some of the most
common in the literature: accuracy for the validation set and
loss for the training and validation sets. They help the re-
searchers understand how the NN acts over time, how the
model is ﬁtted to the dataset, and the performance and error
scores while using training and validation datasets. In this
case, the accuracy is the score concerning the LM’s ability
to predict the next word of ﬁlling the missings accurately
given a sequence of words from the validation set. The loss
metrics report the error after applying the DNN to the train-
ing or validation set, respectively. Every implementation de-
tail related to the DNNs and the metrics is available in the
GitHub repository (Cruz-Benito and Vishwakarma 2020a).
Apart from those metrics, we assessed the models’ quality
by applying them in the proposed tasks -generate text and
auto-complete- and observing how they perform.

Training results

Table 1 displays the ﬁnal metrics for the different NNs at
the end of the training. Similarly, ﬁgures 1 and 2 show the
evolution of each model’s accuracy during the training. Fig-
ures 3, 4, 5, and 6 show the training loss and validation loss
evolution along the training epochs.

DNN architecture
AWD-LSTM word
AWD-LSTM unigram
AWD-LSTM BPE
AWD-LSTM char
AWD-QRNN word
AWD-QRNN unigram
AWD-QRNN BPE
AWD-QRNN char
GPT-2
BERT
RoBERTa

Epochs Accuracy Train loss Validation loss Pre-trained?

31
31
31
31
31
31
31
31
11
11
11

0.494893
0.557226
0.580373
0.779633
0.515747
0.539951
0.538290
0.736358
0.743738
0.999238
0.999468

2.308937
1.639875
1.561393
0.757956
1.972508
1.790150
1.824709
0.944526
1.407818
0.014755
0.010569

2.341698
1.826841
1.703536
0.742808
2.144126
1.894901
1.896698
0.897850
1.268246
0.004155
0.002920

Yes
Yes
Yes
Yes
No
No
No
No
Yes
Yes
Yes

Table 1: Results after full training of each NN architecture

On the one hand, according to the results displayed in Ta-
ble 1 and Figure 1, for neural networks intended for auto-
mated source code generation -AWD-LSTM, AWD-QRNN,
the overall NN-tokenization
and Transformer GPT-2-,

Figure 1: Evolution of the accuracy of neural networks de-
voted to source code generation during the training epochs

Figure 2: Evolution of the accuracy of neural networks de-
voted to ﬁlling the blanks during the training epochs

model combination performed better in the case of ac-
curacy metrics was the AWD-LSTM with char tokeniza-
tion (accuracy 0.779633). The second one was the GPT-
2 transformer model -BPE over raw bytes tokenization-
(0.743738), and the third one the AWD-QRNN with char to-
kenization (0.736358). Related to AWD-LSTM and AWD-
QRNN architectures’ combination with other tokenization
techniques, we obtained poor results on accuracy: between
0.494893 to 0.580373. On the other hand, according to the
results shown in Table 1 and Figure 2, both models (BERT
and RoBERTa) had excellent accuracy results in the trans-
former models intended for auto-completion 0.999238 and
0.999468, respectively.

About how the pre-training and transfer learning af-
fects the results, the two top results regarding the accu-
racy come up from pre-trained models in the English lan-
guage (0.779633 and 0.743738), yet the third-best result
was from a non-pre-trained network (0.736358). Compar-
ing the similar networks, the average (mean) accuracy of
the AWD-LSTM pre-trained versions is 0.603031 (standard
deviation -std- of 0.123144), while the average accuracy of
AWD-QRNN non-pre-trained versions was 0.582587 (std of

Figure 3: Evolution of the training loss of DNNs devoted to
generating source code during the training epochs

Figure 5: Evolution of the validation loss of DNNs devoted
to generating source code during the training epochs

Figure 4: Evolution of the training loss of neural networks
devoted to ﬁlling the blanks during the training epochs

Figure 6: Evolution of the validation loss of neural networks
devoted to ﬁlling the blanks during the training epochs

0.103107). The only combination NN-tokenization model
that worked worse when it was pre-trained was the one with
the word tokenization.

Regarding the observed losses, it is worth commenting
that the AWD-LSTM char, AWD-QRNN char, and the three
transformer models (GPT-2, BERT, RoBERTa) could be
trained for more epochs or with a higher learning rate. The
model may be be underﬁtting since the training loss is higher
than the validation loss (Table 1, ﬁgures 3, 4, 5, and 6).

To put in context the accuracy achieved during the ex-
perimentation, we compare the results with the existing lit-
erature. The papers (Tiwang, Oladunni, and Xu 2019; Li
et al. 2017; Raychev, Bielik, and Vechev 2016) present mod-
els trained to generate Python code considered state of the
art when published. Our three best models trained to gen-
erate source code outperform the approaches based on the
following architectures: Vanilla LSTM (accuracy: 67.3%)
(Li et al. 2017), Attention LSTM (69.8%) (Li et al. 2017),
Pointer Mixture Network (70.1%) (Li et al. 2017), or Proba-
bilistic Model (69.2%) (Raychev, Bielik, and Vechev 2016).
Our models perform worse than other approaches based on
the abstract syntax tree (AST) instead of using the textual

code: AST-LSTM AST-LSTM (accuracy 90.3%) (Tiwang,
Oladunni, and Xu 2019) or AST-MLP (90.1%) (Tiwang,
Oladunni, and Xu 2019). So, considering this, our results
are good in terms of accuracy compared to the existing peer-
reviewed literature. As a side note, we did not ﬁnd models
available publicly to compare with, neither in these nor in
other peer-reviewed papers. For the models devoted to auto-
complete code (BERT and RoBERTa), as of now, we did
not ﬁnd peer-reviewed papers trying to solve this task using
Transformer architectures.

Results of using the trained DNN models
Following, we introduce some examples of how the different
neural networks generated source code or auto-completed it.
These examples are used to assess the model’s output qual-
ity, avoiding relying only on the NN metrics. This valida-
tion using more methods than the simple metrics is com-
mon in the AI and NLP ﬁelds. Many papers employ human
evaluation or specialized metrics to grasp better the model’s
performance and capture any other particular detail or qual-
ity indicator on the related task proposed. In this case, we
show only those with the best accuracy results. To gener-

ate source code, we show output from AWD-LSTM char,
AWD-QRNN char, and GPT-2. For auto-completion, we
show results from BERT and RoBERTa. For the source code
generation, every model faced the same challenges (Cruz-
Benito and Vishwakarma 2020a). Those are the results when
any of them try to generate ﬁve different sentences with the
input from pandas import:

AWD-LSTM char

"from pandas import defaults as _rp pandas ="
"from pandas import DataFrameEncoder if self"
"from pandas import OrderedDict else: passed"
"from pandas import sequence if type(seq1) i"
"from pandas import conditional_insertion as"

AWD-QRNN char

"from pandas import webbrowser self.login fr"
"from pandas import __http_args or [ ’Pipeli"
"from pandas import iterations is not None p"
"from pandas import service params to servic"
"from pandas import difference if not isinst"

GPT-2

"from pandas import time, np\n

"

"from pandas import pandas from time.time.datetime import Date\n

"from pandas import gtk, os\n

with n"

from pandas_utils import pandas_utils\n
import pylint"

"from pandas import wcpy\n

import cpy_context as cpy_context\n

"

"from pandas import gkpy\n

"""\n
... pass\n
kwargs = cg"

To assess the generation, we do not focus on the semantics
of the imports used or whether they are part of the Pandas li-
brary or not, but on the language’s correctness. In general,
from a Python language perspective, the outputs from GPT-
2 are better. The outputs include line breaks, indentation, and
fair use of multiple inputs in one sentence (except in one of
the outputs). The AWD-LSTM and AWD-QRNN failed to
auto-generate an import sentence appropriately, or at least,
they fail regarding the usual manner used by regular users.
As a ﬁnal comment on this, the other models trained failed
on similar issues, plus they do not get enough semantic con-
text related to the Pandas library.

Similarly, concerning the source code auto-completion,
both BERT and RoBERTa tried to autoﬁll the mask token
in the sentence from pandas import [MASK]. These are the
results:

BERT

[{’sequence’: ’[CLS] from pandas import [SEP] [SEP]’,

’score’: 0.9969683289527893,
’token’: 102,
’token_str’: ’[SEP]’},

{’sequence’: ’[CLS] from pandas import [CLS] [SEP]’,

’score’: 0.0010887219104915857,
’token’: 101,
’token_str’: ’[CLS]’},

{’sequence’: ’[CLS] from pandas import. [SEP]’,

’score’: 0.0004200416151434183,
’token’: 119,
’token_str’: ’.’},

{’sequence’: ’[CLS] from pandas import ; [SEP]’,

’score’: 0.00027348980074748397,
’token’: 132,
’token_str’: ’;’},

{’sequence’: ’[CLS] from pandas import def [SEP]’,

’score’: 8.858884393703192e-05,
’token’: 19353,
’token_str’: ’def’}]

RoBERTa
[{’sequence’: ’<s>from pandas import\n</s>’,

’score’: 0.6224209666252136,
’token’: 50118,
’token_str’: ’_C’},

{’sequence’: ’<s>from pandas import.</s>’,

’score’: 0.22222988307476044,
’token’: 4,
’token_str’: ’.’},

{’sequence’: ’<s>from pandas import </s>’,

’score’: 0.038354743272066116,
’token’: 1437,
’token_str’: ’_G’},

{’sequence’: ’<s>from pandas import\n\n</s>’,

’score’: 0.028566861525177956,
’token’: 50140,
’token_str’: ’_C_C’},

{’sequence’: ’<s>from pandas import.</s>’,

’score’: 0.021909384056925774,
’token’: 479,
’token_str’: ’_G.’}]

For auto-completion, we observe that almost no auto-
completion is right. The more accurate sentences from the
Python language point are the ones in which the mask to-
ken has been replaced by white space or by a dot. Neverthe-
less, they are not correct but closer to be right compared to
the other ones. One interesting thing is that BERT assigns
a very high score to a predicted mask, which is not correct,
and shallow scores to the other possible solutions (also in-
correct). In the case of RoBERTa, it gives lower scores to
all the solutions, yet also fails on the correctness: the second
sentence predicted (score 0.222) can be closer to be right
compared to the ﬁrst one (score 0.622).

Discussion
Considering the results obtained, one could convincingly as-
sert that the tokenization model used profoundly affects the
results when generating automated source code. Although
that may be accurate, we must discuss it carefully.

Discussing the outcomes from resulting models
First, our overall results are consistent with the existing liter-
ature (Karampatsis and Sutton 2019; Ganin et al. 2016; Kim
et al. 2016; Karpathy 2016). Sub-word tokenization works
better in the case of modeling source code, as 2019 stated.
Every result obtained is consistent in that sense. Even more,
as 2016 envision, char tokenization probably should the best
option to try by default when dealing with LMs and source
code. Furthermore, according to the results achieved, mod-
els such as GPT-2 -using a tokenization model based on BPE
over raw bytes- can outperform LSTM/QRNN models like
those we tested to grasp better the internals of a program-
ming language. As showcased during the results, even if
GPT-2 was not the best model in terms of accuracy, it gave
better code outputs than the other ones selected for the com-
parison.

As future work, it would be great to check if the better
textual output in the case of GPT-2 is because of a) it is a
much bigger and better pre-trained model (163,037,184 pa-
rameters against 36,491 for AWD-LSTM and AWD-QRNN
models), b) it is related to dataset’s size or quality, c) if it is
related to both causes or, d) if it is related to other issues.

Continuing with the comments about the accuracy, one
may note that the textual outputs generated by AWD-LSTM
char, AWD-QRNN char, and GPT-2 could be polished to
be more accurate. The ﬁnal training loss is higher than the
validation loss for the three selected DNN architectures,
which can be a sign of underﬁtting. We ﬁnd the same is-
sue (train loss ¡¡ valid loss) for BERT and RoBERTa-based
models. Whether the purpose of this paper is not to produce
state-of-the-art results per se, we continued the training for
over ﬁve more epochs to verify it. The improvement ob-
tained from extending the training for the best approaches
was residual in general, so we decided to report the results
for 1+30 epochs in AWD-LSTMs and AWD-QRNNs, and
1+10 epochs for Transformer.

Examining the effect of pre-training and transfer
learning

Regarding the pre-training and transfer learning, every pre-
trained model (on English-based datasets) got better accu-
racy than its non-pre-trained counterparts except in word
tokenization models. It seems to be strongly related to the
statements we introduced at the beginning of this paper, cit-
ing (Allamanis et al. 2015; Karampatsis and Sutton 2019)
about the source code does not have the same restrictions
in the sense of common words or neologisms. In this sense,
the conclusion comes into our mind rapidly: if we consider
source code words in “word” units, they probably will not
ﬁt in the ﬁxed set of words used in a human language like
English. So, the LM’s knowledge acquired during the pre-
training is not entirely valid when we get out of that ﬁxed set
of words that compose a language. Most words in the pro-
gramming language are neologisms for the LMs pre-trained
in English, and thus, it needs to incorporate them and their
relationships into the learned knowledge. For the sub-word
units, the LM can be less sensible to the neologisms. Po-
tentially, it could be more robust the more divided a word
is since the set of bytes or chars is more straightforward
than the chunks present in richer constructions or informa-
tion units.

Going deeper into this research, concerning the pre-
training effect over LMs modeling source code, it could be
worth researching the relationship between the pre-training
in different human-spoken languages and the LM ability to
work with existing source code speciﬁc programming lan-
guages.

Reviewing the textual assessment of resulting LMs

About the tests made generating source code or ﬁlling in
the blanks using the trained LMs, we think that, in general,
the textual results obtained are not so good, yet they are in-
formative of how LMs are working and how they can be
improved. One of the things that can explain these results
is the dataset used. In this case, we used a public dataset
that other researchers can use to make results and experi-
ments comparable and replicable. In the literature, we do
not ﬁnd a standard dataset for these tasks against which
we can compare easily. Other papers (Tiwang, Oladunni,
and Xu 2019; Li et al. 2017; Raychev, Bielik, and Vechev

2016) use custom datasets, but we ﬁnd a lack in the liter-
ature of well-recognized code datasets to use. Comparing
with other recent papers in the NLP ﬁeld used as the basis
for this research (Radford et al. 2019; Devlin et al. 2019; Liu
et al. 2019; Brown et al. 2020), the dataset may be relatively
small to train a big LM to accomplish appropriately chal-
lenging tasks like generating source code or auto-completing
it. Future work may be testing these or new approaches in
bigger datasets to train big LMs focused on modeling the
Python language and checking whether the results are bet-
ter. Recent examples of LMs -such as GPT-3 (Brown et al.
2020)- claim to produce accurate textual outputs even in
contexts in which they were not trained. Part of the expla-
nation given for that ability is the use of gargantuan datasets
combined with Transformer and other attention-based archi-
tectures. So, those approaches can also be relevant to other
contexts like ours. Another line for future research can be us-
ing datasets focused on speciﬁc libraries or Python aspects
and verify if these approaches specialize positively for those
contexts the DNN models used in this paper.

Related to evaluating the code generated or ﬁlled, we ob-
served in the literature different approaches (Celikyilmaz,
Clark, and Gao 2020; Roziere et al. 2020). In the con-
text of LMs modeling source code, many papers and soft-
ware libraries devoted to translating between programming
languages typically evaluate text generation using methods
and metrics like BLEU (Papineni et al. 2002), or variants
like SacreBLEU (Post 2018). Other papers like (Tiwang,
Oladunni, and Xu 2019) rely on the accuracy to assess an
LM’s performance based on deep learning. Some models
can even solve different tasks that are part of existing bench-
marks (Weston et al. 2015) or are evaluated, checking their
perplexity (similarly to those that evaluate the model using
the accuracy). The current tendency in large models is to
evaluate them using human intervention to evaluate the out-
put’s quality (Radford et al. 2019; Brown et al. 2020). We
assessed the models using accuracy during our experiments
and evaluated the models’ textual outputs based on our prior
human knowledge. It would be interesting for the future to
plan new evaluation processes involving larger cohorts of
source code experts to evaluate the models such as (Radford
et al. 2019; Brown et al. 2020) do. One of the potential new
assessments can be usability tests conducted with program-
mers. They can compare the code they would write against
the code proposed by any of the DNNs presented here and
the result from other common code auto-completion tools
included in integrated development environments. As we
outlined in the results section, relying only on metrics like
accuracy should not be enough. As in our case, accuracy
and the other metrics can be a good indicator of the model’s
performance, yet we need to verify LMs behavior and qual-
ity using complementary methods like specialized metrics or
human evaluation. For tasks like auto-completion or source
code generation, there are no existing specialized metrics
(like BLEU in translation, for example), so one of the fu-
ture research lines is improving the evaluation of LMs for
source code. Based on some existing ideas in broad NLP,
there are many opportunities to explore in that sense. From
new test suites for language models used in source code con-

texts (Roziere et al. 2020) to behavioral testing (Ribeiro et al.
2020) or human-centric evaluation of the models (Celiky-
ilmaz, Clark, and Gao 2020) with particular emphasis on
reproducible and unbiased assessments, or combinations of
automatic testing and human-centric assessments.

Conclusions

This paper compares how different approaches to tokeniza-
tion models, deep neural network architectures, pre-trained
models, and transfer learning affect the results from lan-
guage models used to generate source code or auto-complete
software pieces. We studied different DNN architectures like
AWD-LSTM, AWD-QRNN, and Transformer to seek which
kind of them work better with different tokenization mod-
els (word, unigram, BPE, and char). Also, we compared the
pre-training effect on the results given by LMs after training
them and ﬁne-tuning them via transfer learning to work with
other languages (English language to Python programming
language). As a result of this work, we ﬁnd that in small
LMs (like our AWD-LSTM and AWD-QRNN models), the
tokenization using char-sized chunks works better than us-
ing any other tokenization models. In larger models like the
Transformer GPT-2, the accuracy was slightly worse than
the other architectures. However, GPT-2 raised better results
on the source code generation tests (even using another to-
kenization approach like BPE over raw bytes). For source
code auto-completion, we tested some transformer models
like BERT and RoBERTA. While their accuracy was above
any other models, they did not perform very well when per-
forming the tasks proposed in our tests. In general, we ﬁnd
that pre-trained models work better, even if they were not
trained initially for a programming language like Python
(our models were pre-trained using the English language).
Finally, related to evaluating tasks like automating source
code generation and source code auto-completion, we raise
concerns about the literature gaps and propose some re-
search lines to work on in the future.

Acknowledgments

We thank the IBM Quantum team and the IBM Research
ETX team for the insightful discussions about this research
and the support received during the development of this re-
search.

References

Allamanis, M.; Barr, E. T.; Bird, C.; and Sutton, C. 2015.
Suggesting accurate method and class names. In Proceed-
ings of the 2015 10th Joint Meeting on Foundations of Soft-
ware Engineering, 38–49.

Allamanis, M.; Barr, E. T.; Devanbu, P.; and Sutton, C. 2018.
A survey of machine learning for big code and naturalness.
ACM Computing Surveys (CSUR) 51(4): 1–37.

Bielik, P.; Raychev, V.; and Vechev, M. 2016. PHOG: proba-
bilistic model for code. In International Conference on Ma-
chine Learning, 2933–2942.

Bostrom, K.; and Durrett, G. 2020. Byte Pair Encoding is
In Findings
Suboptimal for Language Model Pretraining.
of the Association for Computational Linguistics: EMNLP
2020, 4617–4624. Online: Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/2020.
ﬁndings-emnlp.414.

Bradbury, J.; Merity, S.; Xiong, C.; and Socher, R. 2017.
In Proceedings of the
Quasi-recurrent neural networks.
5th International Conference on Learning Representations
(ICLR 2017). URL https://arxiv.org/pdf/1611.01576.pdf.

Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in Neural Information Processing Systems 33. URL
https://arxiv.org/pdf/2005.14165.pdf.

Bryant, C.; and Briscoe, T. 2018. Language model based
grammatical error correction without annotated training
data. In Proceedings of the Thirteenth Workshop on Inno-
vative Use of NLP for Building Educational Applications,
247–253.

Celikyilmaz, A.; Clark, E.; and Gao, J. 2020.
uation of Text Generation: A Survey.
arXiv:2006.14799 .

Eval-
arXiv preprint

Chelba, C.; Mikolov, T.; Schuster, M.; Ge, Q.; Brants, T.;
Koehn, P.; and Robinson, T. 2014. One Billion Word Bench-
mark for Measuring Progress in Statistical Language Mod-
eling. In Fifteenth Annual Conference of the International
Speech Communication Association.

Chen, H.; Le, T. H. M.; and Babar, M. A. 2020. Deep
Learning for Source Code Modeling and Generation: Mod-
els, Applications and Challenges. ACM Computing Surveys
(CSUR) .

Chronopoulou, A.; Baziotis, C.; and Potamianos, A. 2019.
An Embarrassingly Simple Approach for Transfer Learning
In Proceedings of the
from Pretrained Language Models.
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), 2089–
2095.

Cruz-Benito, J.; Faro, I.; Mart´ın-Fern´andez, F.; Ther´on, R.;
and Garc´ıa-Pe˜nalvo, F. J. 2018. A Deep-Learning-based
proposal to aid users in Quantum Computing programming.
In International Conference on Learning and Collaboration
Technologies, 421–430. Springer.

J.;

and Vishwakarma,

2020a.
Cruz-Benito,
cbjuan/tokenizers-neural-nets-2020-paper:
doi:
10.5281/zenodo.4011767. URL https://github.com/cbjuan/
tokenizers-neural-nets-2020-paper/.

S.
v1.0

Baevski, A.; and Auli, M. 2018. Adaptive Input Representa-
tions for Neural Language Modeling. In International Con-
ference on Learning Representations.

Cruz-Benito, J.; and Vishwakarma, S. 2020b. NN mod-
els produced by cbjuan/tokenizers-neural-nets-2020-paper:
v1.0 doi:10.5281/zenodo.4293857.

Czapla, P.; Howard, J.; and Kardas, M. 2018. Universal lan-
guage model ﬁne-tuning with subword tokenization for pol-
ish. arXiv preprint arXiv:1810.10222 .

Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J. G.; Le, Q.; and
Salakhutdinov, R. 2019. Transformer-XL: Attentive Lan-
guage Models beyond a Fixed-Length Context. In Proceed-
ings of the 57th Annual Meeting of the Association for Com-
putational Linguistics, 2978–2988.

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In NAACL-HLT (1).

in the Blanks.

Enabling
Donahue, C.; Lee, M.; and Liang, P. 2020.
In Proceed-
Language Models to Fill
ings of the 58th Annual Meeting of the Association for
Computational Linguistics, 2492–2501. Online: Association
for Computational Linguistics. doi:10.18653/v1/2020.acl-
main.225. URL https://www.aclweb.org/anthology/2020.
acl-main.225.

Eisenschlos, J.; Ruder, S.; Czapla, P.; Kadras, M.; Gug-
ger, S.; and Howard, J. 2019. MultiFiT: Efﬁcient Multi-
In Proceedings of
lingual Language Model Fine-tuning.
the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP),
5702–5707. Hong Kong, China: Association for Computa-
tional Linguistics. doi:10.18653/v1/D19-1572. URL https:
//www.aclweb.org/anthology/D19-1572.

Fedus, W.; Goodfellow, I.; and Dai, A. M. 2018. MaskGAN:
In Inter-
Better Text Generation via Filling in the
national Conference on Learning Representations. Online.
URL https://arxiv.org/pdf/1801.07736.pdf.

.

Ganin, Y.; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle,
H.; Laviolette, F.; Marchand, M.; and Lempitsky, V. 2016.
Domain-adversarial training of neural networks. The Jour-
nal of Machine Learning Research 17(1): 2096–2030.

Ghosh, S.; and Kristensson, P. O. 2017. Neural networks for
text correction and completion in keyboard decoding. arXiv
preprint arXiv:1709.06429 .

Gong, C.; He, D.; Tan, X.; Qin, T.; Wang, L.; and Liu, T.-Y.
2018. Frage: Frequency-agnostic word representation. Ad-
vances in neural information processing systems 31: 1334–
1345.

Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9(8): 1735–1780.

Honnibal, M.; and Montani, I. 2017.
spacy 2: Natural
language understanding with bloom embeddings, convolu-
tional neural networks and incremental parsing URL https:
//spacy.io/.

Howard, J.; and Gugger, S. 2020. Fastai: A layered API for
deep learning. Information 11(2): 108.

Howard, J.; and Ruder, S. 2018. Universal Language Model
In Proceedings of the
Fine-tuning for Text Classiﬁcation.
56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 328–339.

Husain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and
Brockschmidt, M. 2019. Codesearchnet challenge: Eval-
uating the state of semantic code search. arXiv preprint
arXiv:1909.09436 .
Kannan, A.; Kurach, K.; Ravi, S.; Kaufmann, T.; Tomkins,
A.; Miklos, B.; Corrado, G.; Lukacs, L.; Ganea, M.; Young,
P.; et al. 2016. Smart reply: Automated response suggestion
In Proceedings of the 22nd ACM SIGKDD In-
for email.
ternational Conference on Knowledge Discovery and Data
Mining, 955–964.
Karampatsis, R.-M.; and Sutton, C. 2019. Maybe deep neu-
ral networks are the best choice for modeling source code.
arXiv preprint arXiv:1903.05734 .
Karpathy, A. 2016. The unreasonable effectiveness of recur-
rent neural networks URL http://karpathy.github.io/2015/05/
21/rnn-effectiveness.
Kim, Y.; Jernite, Y.; Sontag, D.; and Rush, A. M. 2016.
Character-Aware Neural Language Models. In Proceedings
of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
AAAI’16, 2741–2749. AAAI Press.
Krause, B.; Kahembwe, E.; Murray, I.; and Renals, S. 2018.
Dynamic Evaluation of Neural Sequence Models.
In Dy,
J.; and Krause, A., eds., Proceedings of the 35th Inter-
national Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, 2766–2775.
Stockholmsm¨assan, Stockholm Sweden: PMLR. URL http:
//proceedings.mlr.press/v80/krause18a.html.
Kudo, T.; and Richardson, J. 2018. SentencePiece: A sim-
ple and language independent subword tokenizer and deto-
kenizer for Neural Text Processing. In Proceedings of the
2018 Conference on Empirical Methods in Natural Lan-
guage Processing: System Demonstrations, 66–71.
Li, J.; Wang, Y.; Lyu, M. R.; and King, I. 2017. Code com-
pletion with neural attention and pointer networks. In Pro-
ceedings of the Twenty-Seventh International Joint Confer-
ence on Artiﬁcial Intelligence (IJCAI-18), 4159–41655.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.
2019. Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Merity, S.; Keskar, N. S.; and Socher, R. 2018a. An anal-
ysis of neural language modeling at multiple scales. arXiv
preprint arXiv:1803.08240 .
Merity, S.; Keskar, N. S.; and Socher, R. 2018b. Regular-
izing and Optimizing LSTM Language Models. In Interna-
tional Conference on Learning Representations.
Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.
In Proceedings of the
Pointer sentinel mixture models.
5th International Conference on Learning Representations
(ICLR 2017). URL https://arxiv.org/pdf/1609.07843.pdf.
Mikolov, T.; Deoras, A.; Kombrink, S.; Burget, L.; and
ˇCernock`y, J. 2011. Empirical evaluation and combination
of advanced language modeling techniques. In Twelfth an-
nual conference of the international speech communication
association.

Nguyen, A. T.; and Nguyen, T. N. 2015. Graph-based sta-
tistical language model for code. In 2015 IEEE/ACM 37th
IEEE International Conference on Software Engineering,
volume 1, 858–868. IEEE.
Nguyen, A. T.; Nguyen, T. T.; and Nguyen, T. N. 2013. Lexi-
cal statistical machine translation for language migration. In
Proceedings of the 2013 9th Joint Meeting on Foundations
of Software Engineering, 651–654.
Oda, Y.; Fudaba, H.; Neubig, G.; Hata, H.; Sakti, S.; Toda,
T.; and Nakamura, S. 2015. Learning to generate pseudo-
code from source code using statistical machine translation
In 2015 30th IEEE/ACM International Conference on
(t).
Automated Software Engineering (ASE), 574–584. IEEE.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
BLEU: a method for automatic evaluation of machine trans-
In Proceedings of the 40th annual meeting of the
lation.
Association for Computational Linguistics, 311–318.
Post, M. 2018. A Call for Clarity in Reporting BLEU
Scores. In Proceedings of the Third Conference on Machine
Translation: Research Papers, 186–191. Brussels, Belgium:
Association for Computational Linguistics. doi:10.18653/
v1/W18-6319.
URL https://www.aclweb.org/anthology/
W18-6319.
Proksch, S.; Lerch, J.; and Mezini, M. 2015. Intelligent code
completion with Bayesian networks. ACM Transactions on
Software Engineering and Methodology (TOSEM) 25(1): 1–
31.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and
Sutskever, I. 2019. Language Models are Unsupervised
Multitask Learners .
Rae, J. W.; Potapenko, A.; Jayakumar, S. M.; Hillier, C.; and
Lillicrap, T. P. 2019. Compressive Transformers for Long-
Range Sequence Modelling. In International Conference on
Learning Representations.
Raychev, V.; Bielik, P.; and Vechev, M. 2016. Probabilistic
model for code with decision trees. ACM SIGPLAN Notices
51(10): 731–747.
Ribeiro, M. T.; Wu, T.; Guestrin, C.; and Singh, S. 2020.
Beyond Accuracy: Behavioral Testing of NLP Models with
CheckList. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, 4902–4912.
Online: Association for Computational Linguistics.
doi:
10.18653/v1/2020.acl-main.442. URL https://www.aclweb.
org/anthology/2020.acl-main.442.
Roziere, B.; Lachaux, M.-A.; Chanussot, L.; and Lample,
G. 2020. Unsupervised Translation of Programming Lan-
guages. Advances in Neural Information Processing Systems
33.
Ruder, S.; Peters, M. E.; Swayamdipta, S.; and Wolf, T.
2019. Transfer learning in natural language processing. In
Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics:
Tutorials, 15–18.
Schuster, M.; and Nakajima, K. 2012. Japanese and korean
In 2012 IEEE International Conference on
voice search.

Acoustics, Speech and Signal Processing (ICASSP), 5149–
5152. IEEE.
Sennrich, R.; Haddow, B.; and Birch, A. 2016. Neural Ma-
chine Translation of Rare Words with Subword Units.
In
Proceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
1715–1725.
Smith, L. N. 2017. Cyclical learning rates for training neural
networks. In 2017 IEEE Winter Conference on Applications
of Computer Vision (WACV), 464–472. IEEE.
Smith, L. N.; and Topin, N. 2019. Super-convergence: Very
fast training of neural networks using large learning rates.
In Artiﬁcial Intelligence and Machine Learning for Multi-
Domain Operations Applications, volume 11006, 1100612.
International Society for Optics and Photonics.
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and
Salakhutdinov, R. 2014. Dropout: a simple way to prevent
neural networks from overﬁtting. The journal of machine
learning research 15(1): 1929–1958.
Takase, S.; Suzuki, J.; and Nagata, M. 2018. Direct Out-
put Connection for a High-Rank Language Model. In Pro-
ceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing, 4599–4609.
Tiwang, R.; Oladunni, T.; and Xu, W. 2019. A Deep Learn-
ing Model for Source Code Generation. In 2019 Southeast-
Con, 1–7. IEEE.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. In Advances in neural information
processing systems, 5998–6008.
Wang, A.; and Cho, K. 2019. BERT has a Mouth, and It
Must Speak: BERT as a Markov Random Field Language
In Proceedings of the Workshop on Methods for
Model.
Optimizing and Evaluating Neural Language Generation
(NAACL HLT 2019), 30–36.
Wang, D.; Gong, C.; and Liu, Q. 2019. Improving Neural
In Chaud-
Language Modeling via Adversarial Training.
huri, K.; and Salakhutdinov, R., eds., Proceedings of the
36th International Conference on Machine Learning, vol-
ume 97 of Proceedings of Machine Learning Research,
6555–6565. Long Beach, California, USA: PMLR. URL
http://proceedings.mlr.press/v97/wang19f.html.
Weston, J.; Bordes, A.; Chopra, S.; Rush, A. M.; van
Merri¨enboer, B.; Joulin, A.; and Mikolov, T. 2015. Towards
ai-complete question answering: A set of prerequisite toy
tasks. arXiv preprint arXiv:1502.05698 .
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;
Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-
son, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu,
J.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest,
Q.; and Rush, A. 2020. HuggingFace’s ransformers: State-
In Proceedings
of-the-Art Natural Language Processing.
of the 2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations, 38–45.
Online: Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/2020.emnlp-demos.6.

Yang, Z.; Dai, Z.; Salakhutdinov, R.; and Cohen, W. W.
2018. Breaking the Softmax Bottleneck: A High-Rank RNN
Language Model. In International Conference on Learning
Representations.
Young, T.; Hazarika, D.; Poria, S.; and Cambria, E. 2018.
Recent trends in deep learning based natural language pro-
cessing. IEEE Computational Intelligence magazine 13(3):
55–75.

