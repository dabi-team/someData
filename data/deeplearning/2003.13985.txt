DeepLPF: Deep Local Parametric Filters for Image Enhancement

Sean Moran1, Pierre Marza∗,1,2, Steven McDonagh1, Sarah Parisot1,3, Gregory Slabaugh1
sean.j.moran@gmail.com, pierre.marza@gmail.com,
{steven.mcdonagh, sarah.parisot, gregory.slabaugh} @huawei.com,
3Mila Montr´eal
1Huawei Noah’s Ark Lab

2INSA Lyon

0
2
0
2

r
a

M
1
3

]

V
C
.
s
c
[

1
v
5
8
9
3
1
.
3
0
0
2
:
v
i
X
r
a

Abstract

Digital artists often improve the aesthetic quality of digi-
tal photographs through manual retouching. Beyond global
adjustments, professional image editing programs provide
local adjustment tools operating on speciﬁc parts of an im-
age. Options include parametric (graduated, radial ﬁlters)
and unconstrained brush tools. These highly expressive
tools enable a diverse set of local image enhancements.
However, their use can be time consuming, and requires
artistic capability. State-of-the-art automated image en-
hancement approaches typically focus on learning pixel-
level or global enhancements. The former can be noisy and
lack interpretability, while the latter can fail to capture ﬁne-
grained adjustments.
In this paper, we introduce a novel
approach to automatically enhance images using learned
spatially local ﬁlters of three different types (Elliptical Fil-
ter, Graduated Filter, Polynomial Filter). We introduce a
deep neural network, dubbed Deep Local Parametric Filters
(DeepLPF), which regresses the parameters of these spa-
tially localized ﬁlters that are then automatically applied
to enhance the image. DeepLPF provides a natural form
of model regularization and enables interpretable, intuitive
adjustments that lead to visually pleasing results. We re-
port on multiple benchmarks and show that DeepLPF pro-
duces state-of-the-art performance on two variants of the
MIT-Adobe 5k [3] dataset, often using a fraction of the pa-
rameters required for competing methods.

1. Introduction

Digital photography has progressed dramatically in re-
cent years due to sustained improvements in camera sen-
sors and image signal processing pipelines. Yet despite this
progress, captured photographs may still lack quality due
to varying factors including scene condition, poor illumina-
tion, or photographer skill. Human image retouchers often

*Work done during an internship at Huawei Noah’s Ark Lab.

Figure 1: DeepLPF for parametric local image enhance-
ment. Left: Examples of estimated ﬁlters. Right: The
produced output images. Top: Adjustment of the image red
channel with a single Elliptical Filter. Bottom: Adjustment
of the image red channel with a single Graduated Filter.

improve the aesthetic quality of digital photographs through
manual adjustments. Professional-grade software (e.g. Pho-
toshop, Lightroom) allows application of a variety of modi-
ﬁcations through both interactive and semi-automated tools.
In addition to elementary global adjustments such as
contrast enhancement and brightening, advanced editing
functionality is also available through local image adjust-
ments, such as the examples shown in Fig 1. However, man-
ual enhancement remains challenging for non-experts who
may lack appropriate skills, time, or aesthetic judgment to
improve their images effectively.

These observations motivate the development of fully

 
 
 
 
 
 
automatic photo enhancement tools that can replace non-
expert user work or provide an improved manual-editing
starting point for professional artists. Photographers often
retouch images using a combination of different local ﬁlters
that only affect limited spatial regions of the image. For ex-
ample, a photographer might want to adjust the darkness of
the sky using a graduated ﬁlter while increasing the bright-
ness of a face using an appropriately sized elliptical ﬁlter,
and retouching small ﬁne image detail using a brush tool.

Inspired by this manual workﬂow, we propose a novel
approach that learns parametric ﬁlters for local image en-
hancement. We draw inﬂuence from digital photo editing
software, but model and emulate local editing tools using a
deep neural network. Given (input, enhanced) pairs as train-
ing examples, we reproduce local, mid-level adjustments
through learned graduated and elliptical ﬁlters and a learned
brush tool. By constraining our model to learn how to uti-
lize tools that are similar to those found in a digital artists’
toolbox, we provide a natural form of model regularization
and enable interpretable, intuitive adjustments that lead to
visually pleasing results. Extensive experiments on multi-
ple public datasets show that we outperform state-of-the-
art [8, 25, 4] results with a fraction of the neural network
weight capacity.

Our Contributions can be summarised as follows:

• Local Parametric Filters: We propose a method for
automatic estimation of parametric ﬁlters for local im-
age enhancement. We instantiate our idea using Ellip-
tical, Graduated, Polynomial ﬁlters. Our formulation
provides intuitively interpretable and intrinsically reg-
ularised ﬁlters that ensure weight efﬁciency (capacity
frugality) and mitigate overﬁtting.

• Multiple Filter Fusion Block: We present a princi-
pled strategy for the fusion of multiple learned para-
metric image ﬁlters. Our novel plug-and-play neural
block is capable of fusing multiple independent param-
eter ﬁlter outputs and provides a ﬂexible layer that can
be integrated with common network backbones for im-
age quality enhancement.

• State-Of-The-Art Image Enhancement Quality:
DeepLPF provides state-of-the-art image quality en-
hancement on two challenging benchmarks.

2. Related work

Digital photo enhancement has a rich history in both the
image processing and computer vision communities. Early
automated enhancement focused primarily on image con-
trast [20, 24, 29], while recent work has employed data-
driven methods to learn image adjustments for improving
contrast, colour, brightness and saturation [11, 13, 9, 8, 10,
19]. The related image enhancement work can be broadly

divided into methods that operate globally or locally on an
image, or propose models that operate over both scales.

Global image enhancement: Bychkovsky et al. [3]
collected the popular MIT-Adobe-5K dataset1 that consists
of 5,000 photographs and their retouching by ﬁve differ-
ent artists. The authors propose a regression based ap-
proach to learn the artists’ photographic adjustments from
image pairs. To automate colour enhancement, [27] pro-
pose a learning-to-rank approach over ten popular global
colour controls. In [7] an FCN is used to learn approxima-
tions to various global image processing operators such as
photographic style, non-local dehazing and pencil drawing.
Photo post-processing is performed by a white-box frame-
work in [10] where global retouching curves are predicted
in RGB space. A Reinforcement Learning (RL) approach
enables the image adjustment process in [10] and Deep RL
is used to deﬁne an ordering for enhancement adjustments
in [19], enabling application of global image modiﬁcations
(eg. contrast, saturation).

Local image enhancement: Aubry et al. [1] propose
fast local Laplacian ﬁltering for enhancing image detail.
Hwang et al. [12] propose a method for local image en-
hancement that searches for the best local match for an im-
age in a training database of (input, enhanced) image pairs.
Semantic maps are constructed in [28] to achieve semantic-
aware enhancement and learn local adjustments. Underex-
posed photo enhancement [25] (DeepUPE) learns a scaling
luminance map using an encoder-decoder setup however no
global adjustment is performed. The learned mapping has
high complexity and crucially depends on the regularization
strategy employed. Chen et al. [7] propose a method for fast
learning of image operators using a multi-scale context ag-
gregation network. Enhancement via image-to-image trans-
lation using both cycle consistency [32] and unsupervised
learning [18] have also been proposed in recent years.

Global and local image enhancement: Chen et al. [8]
develop Deep Photo Enhancer (DPE), a deep model for en-
hancement based on two-way generative adversarial net-
works (GANs). As well as local pixel-level adjustments,
DPE introduces a global feature extraction layer to cap-
ture global scene context.
Ignatov et al. [14] design a
weakly-supervised image-to-image GAN-based network,
removing the need for pixel-wise alignment of image pairs.
Chen et al. [4] propose a low-light enhancement model
that operates directly on raw sensor data and propose a
fully-convolutional approach to learn short-exposure, long-
exposure mappings using their low-light dataset. Recent
work makes use of multitask learning [16] for real-time
image processing with various image operators. Bilateral
guided joint upsampling enables an encoder/decoder archi-
tecture for local as well as global image processing. HDR-
Net [9] learns global and local image adjustments by lever-

1https://data.csail.mit.edu/graphics/fivek/

(Elliptical, Graduated, Polynomial). Figure 2 illustrates us-
age, and resulting effects of, comparable manual Lightroom
ﬁlters. In Section 3.1 we present our global DeepLPF archi-
tecture, designed to learn and apply sets of different para-
metric ﬁlters. Section 3.2 then provides detail on the de-
sign of the three considered local parametric ﬁlters, and de-
scribes the parameter prediction block used to estimate ﬁl-
ter parameters. Finally, Sections 3.3 and 3.4 explain how
multiple ﬁlters are fused together and provide detail on our
training loss function, respectively.

3.1. DeepLPF Architecture

The architecture of DeepLPF is shown in Figure 3.
Given a low quality RGB input image I and its corre-
sponding high quality enhanced target image Y , DeepLPF
is trained to learn a transformation fθ such that ˆY =fθ(I)
is close to Y as determined by an objective function based
on image quality. Our model combines a single-stream net-
work architecture for ﬁne-grained enhancement, followed
by a two-stream architecture for higher-level,
local en-
hancement. We ﬁrst employ a standard CNN backbone
(e.g. ResNet, UNet) to estimate a H×W ×C dimensional
feature map. The ﬁrst three channels of the feature map
represent the image to be adjusted, while the remaining
C (cid:48)=C−3 channels represent additional features that feed
into three ﬁlter parameter prediction blocks. The ﬁrst sin-
gle stream path estimates the parameters of a polynomial
ﬁlter that is subsequently applied to the pixels of the back-
bone enhanced image ˆY1 input image. This block emulates
a brush tool which adjusts images at the pixel level with a
smoothness constraint imposed by the brush’s shape. The
image enhanced by the polynomial ﬁlter, ˆY2, is concate-
nated with the C (cid:48) backbone features, and serves as input
to the two stream path, which learns and applies more con-
strained, local enhancements in the form of elliptical and
graduated ﬁlters. The adjustment maps of the elliptical and
graduated ﬁlters are estimated using two parallel regression
blocks. Elliptical and graduated maps are fused using sim-
ple addition, although more involved schemes could be em-
ployed e.g. weighted combination. This fusion step results
in a scaling map ˆS that is element-wise multiplied to ˆY2 to
give image ˆY3, effectively applying the elliptical and grad-
uated adjustments to the image after polynomial enhance-
ment. The image ˆY1, enhanced by the backbone network,
is ﬁnally added through a long residual connection to ˆY3
producing the ﬁnal output image ˆY .

3.2. Local Parametric Filters

We provide detail on three examples of local parametric
ﬁlter: the Graduated Filter (3.2.2), Elliptical Filter (3.2.3)
and Polynomial Filter (3.2.4). Filters permit different types
of local image adjustment, governed by their parametric
form, and parameter values specify the exact image effect

Figure 2: Examples of local ﬁlter usage in Lightroom for
the brush tool (top row), graduated (middle row) and radial
(bottom row) ﬁlters.
Images are shown before (left) and
after ﬁlter based enhancement by a human artist (right).

aging a two stream convolutional architecture. The lo-
cal stream extracts local features that are used to predict
the coefﬁcients of local afﬁne transforms, while the global
stream extracts global features that permit an understand-
ing of scene category, average intensity etc. In [13] DSLR-
quality photos are produced for mobile devices using resid-
ual networks to improve both colour and sharpness.

In contrast with prior work, we propose to frame the en-
hancement problem by learning parametric ﬁlters, operat-
ing locally on an image. Learnable, parametrized ﬁlters
align well with the intuitive, well-understood human artis-
tic tools often employed for enhancement and this naturally
produces appealing results that possess a degree of famil-
iarity to human observers. Additionally, the use of our ﬁlter
parameterization strategy serves to both constrain model ca-
pacity and regularize the learning process, mitigating over-
ﬁtting and resulting in moderate model capacity cost.

3. Deep Local Parametric Filters (DeepLPF)

DeepLPF deﬁnes a novel approach for local image en-
hancement, introducing a deep fusion architecture capable
of combining the output from learned, spatially local para-
metric image ﬁlters that are designed to emulate the com-
bined application of analogous manual ﬁlters. In this work,
we instantiate three of the most popular local image ﬁlters

Figure 3: Architecture diagram illustrating our approach to combine the different ﬁlter types (polynomial, elliptical, gradu-
ated) in a single end-to-end trainable neural network. The architecture combines a single stream path for initial enhancement
with the polynomial ﬁlter and a two stream path for further reﬁnement with the graduated and elliptical ﬁlters.

in each case. Filter parameter values are image speciﬁc and
are predicted using supervised CNN regression (3.2.1).

Table 1: Parameters used in localized ﬁlters

3.2.1 Filter Parameter Prediction Network

Our parameter prediction block is a lightweight CNN that
accepts a feature set from a backbone network and regresses
ﬁlter parameters individually. The network block alternates
between a series of convolutional and max pooling layers
that gradually downsample the feature map resolution. Fol-
lowing these layers there is a global average pooling layer
and a fully connected layer which is responsible for predict-
ing the ﬁlter parameters. The global average pooling layer
ensures that the network is agnostic to the resolution of the
input feature set. Activation functions are Leaky ReLUs
and dropout (50%, both train and test) is applied to the fully
connected layer. Further architectural details are provided
in our supplementary material.

For the three example ﬁlters considered; the only dif-
ference between respective network regressors is the num-
ber of output nodes in the ﬁnal fully connected layer. This
corresponds to the number of parameters that deﬁne the re-
spective ﬁlter (Table 1).
In comparison to contemporary
local pixel level enhancement methods [8], the number of
parameters required is greatly reduced. Our network out-
put size can be altered to estimate the parameters of mul-
tiple instances of the same ﬁlter type. Our methodology
for predicting the parameters of an image transformation is
aligned with previous work [9, 5, 23] that shows that learn-
ing a parameterised transformation is often more effective
and simpler than directly predicting an enhanced image di-
rectly. Importantly, the regression network is agnostic to the

# Parameters Parameters

Filter
Graduated G=8
E=8
Elliptical
P =30
Cubic-10
P =60
Cubic-20

g , sB
e , sB

g , sG
sR
g , m, c, o1, o2, ginv
e , sG
sR
e , h, k, θ, a, b
{A · · · J} per colour channel
{A · · · T } per colour channel

speciﬁc implementation of backbone model allowing image
ﬁlters to be used to enhance the output of any image trans-
lation network.

3.2.2 Graduated Filter

Graduated ﬁlters are commonly used in photo editing soft-
ware to adjust images with high contrast planar regions such
as an overexposed sky. Our graduated image ﬁlter, illus-
trated in Figure 4a, is parametrised by three parallel lines.
The central line deﬁnes the ﬁlter location and orientation,
taking the form y=mx+c with slope m and intercept c, pro-
viding a linear parameterisation in standard fashion. Offsets
o1 and o2 provide two additional parameters such that each
(channel-wise) adjustment map is composed of four distinct
regions that form a heatmap s(x, y). In the the 100% area all
pixels are multiplied by scaling parameter sg. Then, in the
100-50% area, the applied scaling factor linearly decreases
from sg to sg
2 . Inside the 50-0% area, the scaling value is
further decreased linearly until reaching the 0% area where
pixels are not adjusted. Mathematically, the graduated ﬁlter
is described in Equations 1-3:

s(x, y) = se min

0, 1 −

(cid:32)

[(x − h) cos θ + (y − k) sin θ]2
a2

+

[(x − h) sin θ − (y − k) cos θ]2
b2

(cid:33)

.

(4)

An example elliptical ﬁlter parametrisation and heatmap
is illustrated in Fig. 4b. Elliptical ﬁlters are often used to
enhance, accentuate objects or speciﬁc regions of interest
within photographs e.g. human faces.

3.2.4 Polynomial Filter

Our third considered ﬁlter type constitutes a polynomial ﬁl-
ter capable of providing ﬁne-grained, regularised adjust-
ments over the entire image. The polynomial ﬁlter em-
ulates a brush tool, providing a broad class of geometric
shapes while incorporating spatial smoothness. We con-
sider order-p polynomial ﬁlters of the forms i·(x + y + γ)p
and (x + y + i + γ)p, where i is the image channel inten-
sity at pixel location (x, y), and γ is an independent scalar.
We empirically ﬁnd a cubic polynomial (p = 3) to offer
both expressive image adjustments yet only a limited set
of parameters. We explore two variants of the cubic ﬁlter,
cubic-10 and cubic-20.

Our smaller cubic ﬁlter (cubic-10) constitutes a set of 10
parameters to predict; {A, B, C, . . . , J}, deﬁning a cubic
function f that maps intensity i to a new adjusted intensity
i(cid:48):

i(cid:48)(x, y) = f (x, y, i)

= i ∗ (Ax3 + Bx2y + Cx2 + D+
+ Dxy2 + Exy + F x + Gy3 + Hy2
+ Iy + J)

(5)

Considering twice as many learnable parameters, the
cubic-20 ﬁlter explores higher order intensity terms, and
consists of a set of 20 parameters {A, B, C, . . . , T } :

i(cid:48)(x, y) = f (x, y, i)

= Ax3 + Bx2y + Cx2i + Dx2 + Exy2
+ F xyi + Gxy + Hxi2 + Ixi + Jx
+ Ky3 + Ly2i + M y2 + N yi2 + Oyi
+ P y + Qi3 + Ri2 + Si + T

(6)

Our cubic ﬁlters consider both spatial and intensity in-
formation while constraining the complexity of the learnt
mapping to a regularized form. This permits the learning of
precise, pixel level enhancement while ensuring that trans-
formations are locally smooth. We estimate an independent

(a)

(b)

Figure 4: Parametrisation and heatmap of the graduated (a)
and elliptical (b) ﬁlters. See main text for further detail.

a(x, y) = sg min

(cid:18)

(cid:26) 1
2

1 +

(cid:96)(x, y)
d2

(cid:19)

(cid:27)

, 1

b(x, y) = sg max

(cid:18)

(cid:26) 1
2

1 +

(cid:96)(x, y)
d1

(cid:19)

(cid:27)

, 0

(1)

(2)

s(x, y) =

(cid:26) ˆginva(x, y) + (1 − ˆginv)b(x, y) , l(x, y) ≥ 0
(1 − ˆginv)a(x, y) + ˆginvb(x, y) , l(x, y) < 0
(3)
where (cid:96)(x, y) = y − (mx + c) is a function of the location
of a point (x, y) relative to the central line, d1 = o1 cos α,
d2 = o2 cos α, α = tan−1(m), and ˆginv is a binary indica-
tor variable. Parameter ˆginv permits inversion with respect
to the top and bottom lines. Inversion determines the loca-
tion of the 100% scaling area relative to the central line. To
enable learnable inversion, we predict a binary indicator pa-
rameter ˆginv = 1
2 (sgn(ginv) + 1), where sgn denotes the
sign function, ginv is the real-valued predicted parameter
and ˆginv is the binarised ˆginv ∈ {0, 1} version. The gradi-
ent of this sign function is everywhere zero and undeﬁned
at zero, therefore we use the straight-through estimator [2]
on the backward pass to learn this binary variable.

3.2.3 Elliptical Filter

A further ﬁlter we utilise deﬁnes an ellipse parametrised by
the center (h, k), semi-major axis (a), semi-minor axis (b)
and rotation angle (θ). The learned scaling factor se is max-
imal at the center of the ellipse (100% point) and decreases
linearly until reaching the boundary. Pixels are not adjusted
outside the ellipse (0% area).

Mathematically, a channel-wise heatmap, deﬁned by the

elliptical ﬁlter, is denoted as:

cubic function for each colour channel yielding a total of
30 parameters for the cubic-10 and 60 parameters for the
cubic-20 ﬁlter respectively.

3.3. Fusing Multiple Filters of the Same Type

Our graduated and elliptical prediction blocks can each
output parameter values for n instances of their respective
ﬁlter type. Further exploration towards choosing n appro-
priately is detailed in Section 4.2. In the case n > 1, multi-
ple instances are combined into an adjustment map through
element-wise multiplication,

sg(x, y) =

n
(cid:89)

i=1

sgi(x, y),

se(x, y) =

n
(cid:89)

i=1

sei(x, y)

(7)

where sgi(x, y) and sei(x, y) are the adjustment map of the
ith instance of the corresponding graduated and elliptical
ﬁlters respectively. On the contrary, since a single per-
channel cubic ﬁlter allows intrinsically high ﬂexibility in
terms of expressive power, we opt not to fuse multiple cu-
bic ﬁlters together.

3.4. DeepLPF Loss Function

The DeepLPF training loss leverages the CIELab colour
space to compute the L1 loss on the Lab channels and the
MS-SSIM loss on the L channel (Equation 8). By split-
ting chrominance and luminance information into separate
loss terms, our model is able to account for separate focus
on both local (MS-SSIM) and global (L1) image enhance-
ments during training [22, 31]. Given a set of N image
pairs {(Yi, ˆYi)}N
i=1, where Yi is the reference image and ˆYi
is the predicted image, we deﬁne the DeepLPF training loss
function as:

N
(cid:88)

{ωlab||Lab( ˆYi) − Lab(Yi)||1

L =

(8)

i=1

+ ωms-ssimMS-SSIM(L( ˆYi), L(Yi))}

where Lab(·) is a function that returns the CIELab Lab
channels corresponding to the RGB channels of the in-
put image and L(·) returns the L channel of the image in
CIELab colour space. MS-SSIM is the multi-scale struc-
tural similarity [26], and ωlab, ωms-ssim are hyperparameters
weighting the relative inﬂuence of the terms in the loss func-
tion.

4. Experiments

4.1. Experimental Setup

Datasets: We evaluate DeepLPF on three challenging
benchmarks, derived from two public datasets. Firstly (i)
MIT-Adobe-5K-DPE [8]: 5000 images captured using var-
ious DSLR cameras. Each captured image is subsequently

(independently) retouched by ﬁve human artists. In order
to make our results reproducible and directly comparable to
the state-of-the-art, we consider only the (supervised) sub-
set used by DeepPhotoEnhancer (DPE) [8] and additionally
follow their dataset pre-processing procedure. The image
retouching of Artist C is used to deﬁne image enhancement
ground truth. The data subset is split into 2250, 500 training
and testing image pairs respectively. We randomly sample
500 images from the training set, providing an additional
validation set for hyperparameter optimisation. The images
are resized to have a long-edge of 500 pixels.
(ii) MIT-
Adobe-5K-UPE [25]: our second benchmark consists of
the same image content as MIT-Adobe-5K-DPE however
image pre-processing here differs and instead follows the
protocol of DeepUPE [25]. We therefore refrain from im-
age resizing and dataset samples vary in pixel resolution;
6-25M . We additionally follow the train / test split pro-
vided by [25] and our second benchmark therefore consists
of 4500 training image pairs, from which we randomly sam-
ple 500 to form a validation set. Testing images (500) are
identical to those samples selected by DeepUPE and ground
truth again consists of the Artist C manually retouched im-
ages. (iii) See-in-the-dark (SID) [4]: the dataset consists
of 5094 image pairs, captured by a Fuji camera. For each
pair the input is a short-exposure image in raw format and
the ground truth is a long-exposure RGB image. Images are
24M pixel in size and content consists of both indoor and
outdoor environments capturing diverse scenes and com-
mon objects of varying size.

Evaluation Metrics: We evaluate quantitatively using

PSNR, SSIM and the perceptual LPIPS metric [30].

Implementation details: Our experiments all employ a
U-Net backbone [21]. The base U-Net architecture, used
for MIT-Adobe-5K-DPE experimentation, is detailed in the
supplementary material. The U-Net architectures used for
the other benchmarks are similar yet have a reduced number
of convolutional ﬁlters (MIT-Adobe-5K-UPE) or include
pixel shufﬂing functionality [17] to account for RAW in-
put (SID). All experiments use the Adam Optimizer with a
learning rate of 10−4. Our architecture makes use of three
graduated (elliptical) ﬁlters per channel and we search for
loss function (Eq. 8) hyperparameters empirically resulting
in: ωlab={1, 1, 1} and ωms−ssim={10−3, 10, 10−3} for all
MIT-Adobe-5K-DPE, MIT-Adobe-5K-UPE and SID exper-
iments, respectively.

4.2. Experimental Results

Ablation study: We ﬁrstly conduct experiments to under-
stand the different contributions and credit assignment for
our method ﬁlter components, using the MIT-Adobe-5K-
DPE data. Table 2 (upper) shows ablative results for our
considered image ﬁlter components. For each conﬁgura-
tion, we report PSNR metrics and verify experimentally the

Elliptical (Green Channel)

Graduated (Blue Channel)

Cubic (Red Channel)

Figure 5: Model output images and examples of Elliptical, Graduated and Polynomial (cubic-10) image ﬁlters learnt by one
instance of DeepLPF. Lighter ﬁlter heat map colours correspond to larger image adjustment values. Cubic-10 ﬁlter is shown
without the intensity (i) term. Best viewed in colour.

Table 2: Model ﬁlter type ablation study (upper) and com-
parisons with state-of-the art methods (lower) using the
MIT-Adobe-5K-DPE benchmark. PSNR and SSIM re-
sults, reported by competing works, are replicated from [8].

Architecture
U-Net
U-Net+Elliptical
U-Net+Graduated
U-Net+Elliptical+Graduated
U-Net+Cubic-10
U-Net+Cubic-20
U-Net+Cubic-20+Elliptical+Graduated

DPED [13]
8RESBLK [32, 18]
FCN [7]
CRN [6]
DPE [8]

PSNR↑
21.57
22.56
22.64
22.88
22.69
23.44
23.93
21.76
23.42
20.66
22.38
23.80

SSIM↑ LPIPS↓
0.843
0.879
0.888
0.886
0.871
0.886
0.903
0.871
0.875
0.849
0.877
0.900

0.601
−
−
−
−
−
0.582
−
−
−
−
0.587

# Weights
1.3 M
1.5 M
1.5 M
1.6 M
1.5 M
1.5 M
1.8 M
−
−
−
−
3.3 M

importance of integrating each component and their supe-
rior combined performance.

Individual ﬁlters are shown to bring boosts in perfor-
mance c.f . the U-Net backbone alone. The polynomial (cu-
bic) ﬁlter has a greater impact than elliptical and graduated
ﬁlters. We hypothesise that this can be attributed to the
spatially large yet non-linear effects enabled by this ﬁlter
type. Combining all ﬁlter blocks incorporates the beneﬁts
of each and demonstrates strongest performance. We high-
light that only ∼ 1
4 of model parameters (i.e. around 452k)
in the full architecture are attributed to ﬁlter blocks. The
remaining capacity is dedicated to the U-Net backbone, il-
lustrating that the majority of performance gain is a result
of designing models capable of frugally emulating manual
image enhancement tools.

We further investigate the inﬂuence of graduated and el-
liptical ﬁlter quantity on model performance in Figure 6.
We ﬁnd a general upward trend in PSNR, SSIM metrics as
the number of ﬁlters per channel are increased. This upward
trend can be attributed to the increased modelling capacity
brought about by the additional ﬁlters. We select 3 ﬁlters
per channel in our experiments which provides a tradeoff
between image quality and parameter count.
Quantitative Comparison: We compare DeepLPF image

(a)

(b)

Figure 6: Experimental study evaluating the effect of
the number of graduated and elliptical ﬁlters used (MIT-
Adobe-5K-DPE dataset).

quality on MIT-Adobe-5K-DPE with contemporary meth-
ods (Table 2, lower). Our full architecture is able to out-
perform recent methods; 8RESBLK [32, 18] and the su-
pervised state-of-the-art (Deep Photo Enhancer (DPE) [8])
on each of the three considered metrics, whilst our method
parameter count approaches half that of their model capac-
ity. Our implicitly regularised model formulation of ﬁlters
allows similar quality enhancement capabilities yet is rep-
resented in a simpler form.

Similarly DeepLPF outperforms DeepUPE [25] on our
second benchmark; MIT-Adobe-5K-UPE (i.e. using their
dataset splits and pre-processing protocols) when consider-
ing PSNR and LPIPS and provides competitive SSIM per-
formance, and improves upon on all other compared works
across the PSNR, SSIM metrics (see Table 3).

Finally we examine performance on the Fuji portion of
the challenging SID dataset. Results are presented in Ta-
ble 4 where it can be observed that DeepLPF is able to im-
prove upon the U-Net method presented in [4] across all
three considered metrics. Our method again proves more
frugal, with model capacity lower by a factor of nearly four.
Qualitative Comparison: Sample visual results compar-
ing DeepLPF (trained independently using each of the
three datasets) are shown in comparison to DPE [8], Deep-
UPE [25] and SID (U-Net) [4] models in Figure 7 rows,
respectively. The park scene (ﬁrst row) can be seen to visu-
ally improve with regard to reproduction of colour faithful-

Input

DPE [8]

DeepLPF

Ground Truth

Input

DeepUPE [8]

DeepLPF

Ground Truth

Input

SID (U-Net) [4]

DeepLPF

Ground Truth

Figure 7: Qualitative comparisons between DeepLPF and various state-of-the-art methods. See text for further details.

Table 3: Quantitative comparison with state-of-the art meth-
ods on the MIT-Adobe-5K-UPE benchmark. PSNR and
SSIM reported by competing works, are replicated from [8].

Architecture
DeepLPF
U-Net [21]
HDRNet [9]
DPE [8]
White-Box [10]
Distort-and-
Recover [19]
DeepUPE [25]

SSIM↑ LPIPS↓
0.887

PSNR↑
24.48
22.24
21.96
22.15
18.57
20.97

0.850
0.866
0.850
0.701
0.841

0.103
–
–
–
–
–

# Weights
800K
1.3 M
–
3.3 M
–
–

23.04

0.893

0.158

1.0 M

Table 4: Quantitative performance comparisons for the im-
age enhancement task deﬁned by the RAW to RGB image
pairs in the SID dataset (Fuji camera) [4].

Architecture
DeepLPF

U-Net [4]

PSNR↑
26.82

SSIM↑ LPIPS↓
0.702

0.564

# Weights
2.0 M

26.61

0.680

0.586

7.8 M

ness in comparison to the DPE result. In the second row
DeepUPE has overexposed the scene, whereas DeepLPF
maintains both accurate exposure and colour content. The
ﬁnal row compares results on the challenging low-light
dataset. The SID model output suffers from a purple colour

cast whereas the DeepLPF output provides improved colour
constancy in comparison to the ground truth. Finally, Fig-
ure 5 provides a selection of parametric ﬁlters, represented
by heatmaps, learned by our model. We provide additional
qualitative results in the supplementary material.

5. Conclusion

In this paper, we have explored automated parameterisa-
tion of ﬁlters for spatially localised image enhancement. In-
spired by professional image editing tools and software, our
method estimates a sequence of image edits using gradu-
ated, elliptical and polynomial ﬁlters whose parameters can
be regressed directly from convolutional features provided
by a backbone network e.g. U-Net. Our localised ﬁlters pro-
duce interpretable image adjustments with visually pleasing
results and ﬁlters constitute plugable and reusable network
blocks capable of improving image visual quality.

In future work we can further explore automatic estima-
tion of the optimal sequence of ﬁlter application; e.g. the
Gumbel softmax trick [15] may prove useful to select oper-
ations from a potentially large bank of image editing tools.
We think that combining our presented local ﬁlters with ad-
ditional local or global ﬁlter types and segmentation masks,
reﬁning enhancements to semantically related pixels, also
provide interesting future directions towards interpretable
and frugal automated image enhancement.

References

[1] Mathieu Aubry, Sylvain Paris, Samuel W. Hasinoff, Jan
Kautz, and Fr´edo Durand. Fast local laplacian ﬁlters: Theory
and applications. ACM Trans. Graph., 33(5):167:1–167:14,
Sept. 2014. 2

[2] Yoshua Bengio. Estimating or propagating gradients through

stochastic neurons. CoRR, abs/1305.2982, 2013. 5

[3] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr´edo
Durand. Learning photographic global tonal adjustment with
a database of input/output image pairs. In CVPR, pages 97–
104. IEEE, 2011. 1, 2

[4] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun.
Learning to see in the dark. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3291–3300, 2018. 2, 6, 7, 8

[5] Jiawen Chen, Andrew Adams, Neal Wadhwa, and Samuel W.
Hasinoff. Bilateral guided upsampling. ACM Trans. Graph.,
35(6):203:1–203:8, Nov. 2016. 4

[6] Qifeng Chen and Vladlen Koltun. Photographic image syn-
In Proceedings
thesis with cascaded reﬁnement networks.
of the IEEE International Conference on Computer Vision,
pages 1511–1520, 2017. 7

[7] Qifeng Chen, Jia Xu, and Vladlen Koltun. Fast image pro-
In Proceedings
cessing with fully-convolutional networks.
of the IEEE International Conference on Computer Vision,
pages 2497–2506, 2017. 2, 7

[8] Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, and Yung-
Yu Chuang. Deep photo enhancer: Unpaired learning for
image enhancement from photographs with GANs. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6306–6314, 2018. 2, 4, 6, 7, 8
[9] Micha¨el Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W
Hasinoff, and Fr´edo Durand. Deep bilateral learning for real-
time image enhancement. ACM Transactions on Graphics
(TOG), 36(4):118, 2017. 2, 4, 8

[10] Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang,
Exposure: A white-box photo post-
ACM Transactions on Graphics

and Stephen Lin.
processing framework.
(TOG), 37(2):26, 2018. 2, 8

[11] Sung Ju Hwang, Ashish Kapoor, and Sing Bing Kang.
Context-based automatic local image enhancement. In Eu-
ropean Conference on Computer Vision, pages 569–582.
Springer, 2012. 2

[12] Sung Ju Hwang, Ashish Kapoor, and Sing Bing Kang.
Context-based automatic local image enhancement. In An-
drew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi
Sato, and Cordelia Schmid, editors, Computer Vision –
ECCV 2012, pages 569–582, Berlin, Heidelberg, 2012.
Springer Berlin Heidelberg. 2

[13] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth
Vanhoey, and Luc Van Gool. Dslr-quality photos on mobile
devices with deep convolutional networks. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 3277–3285, 2017. 2, 3, 7

[14] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth
Vanhoey, and Luc Van Gool. Wespe: weakly supervised
In Proceedings of the
photo enhancer for digital cameras.

IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops, pages 691–700, 2018. 2

[15] Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa-
rameterization with gumbel-softmax. In ICLR, 2017. 8
[16] Kyeongbo Kong, Junggi Lee, Woo-Jin Song, Minsung Kang,
Kyung Joon Kwon, and Seong Gyun Kim. Multitask bilat-
eral learning for real-time image enhancement. Journal of
the Society for Information Display, 2019. 2

[17] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P. Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
In 2017 IEEE Conference
generative adversarial network.
on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017, pages 105–114, 2017.
6

[18] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
image-to-image translation networks. In Advances in Neural
Information Processing Systems, pages 700–708, 2017. 2, 7
[19] Jongchan Park, Joon-Young Lee, Donggeun Yoo, and In
So Kweon. Distort-and-recover: Color enhancement using
In Proceedings of the IEEE
deep reinforcement learning.
Conference on Computer Vision and Pattern Recognition,
pages 5928–5936, 2018. 2, 8

[20] Stephen M Pizer, R Eugene Johnston, James P Ericksen,
Bonnie C Yankaskas, and Keith E Muller. Contrast-limited
adaptive histogram equalization: speed and effectiveness.
In Proceedings of the First Conference on Visualization in
Biomedical Computing, pages 337–345. IEEE, 1990. 2
[21] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In Med-
ical Image Computing and Computer-Assisted Intervention
(MICCAI), volume 9351 of LNCS, pages 234–241. Springer,
2015. (available on arXiv:1505.04597 [cs.CV]). 6, 8
[22] E. Schwartz, R. Giryes, and A. M. Bronstein. DeepISP: To-
wards Learning an End-to-End Image Processing Pipeline.
IEEE Transactions on Image Processing, 28(2):912–923,
2019. 6

[23] Yichang Shih, Sylvain Paris, Fr´edo Durand, and William T.
Freeman. Data-driven hallucination of different times of
day from a single outdoor photo. ACM Trans. Graph.,
32(6):200:1–200:11, Nov. 2013. 4

[24] J Alex Stark. Adaptive image contrast enhancement using
IEEE Transac-

generalizations of histogram equalization.
tions on image processing, 9(5):889–896, 2000. 2

[25] Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen,
Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhance-
In Proceedings
ment using deep illumination estimation.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6849–6857, 2019. 2, 6, 7, 8

[26] Z. Wang, E.P. Simoncelli, and A.C. Bovik. Multiscale
Structural Similarity for Image Quality Assessment. In The
Thrity-Seventh Asilomar Conference on Signals, Systems and
Computers, 2003. 6

[27] Jianzhou Yan, Stephen Lin, Sing Bing Kang, and Xiaoou
Tang. A learning-to-rank approach for image color enhance-
ment. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2987–2994, 2014. 2

[28] Zhicheng Yan, Hao Zhang, Baoyuan Wang, Sylvain Paris,
and Yizhou Yu. Automatic photo adjustment using deep
neural networks. ACM Transactions on Graphics (TOG),
35(2):11, 2016. 2

[29] Lu Yuan and Jian Sun. Automatic exposure correction of
In European Conference on Com-

consumer photographs.
puter Vision, pages 771–785. Springer, 2012. 2

[30] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR, 2018. 6

[31] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss
Functions for Neural Networks for Image Processing. IEEE
Transactions on Computational Imaging, 3(1), 2017. 6
[32] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the IEEE
International Conference on Computer Vision, pages 2223–
2232, 2017. 2, 7

