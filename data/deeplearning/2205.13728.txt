2
2
0
2

y
a
M
7
2

]
I

A
.
s
c
[

1
v
8
2
7
3
1
.
5
0
2
2
:
v
i
X
r
a

GALOIS: Boosting Deep Reinforcement Learning via
Generalizable Logic Synthesis

Yushi Cao2,âˆ—, Zhiming Li2,âˆ—, Tianpei Yang1,3,â€ , Hao Zhang1, Yan Zheng1,â€ 

Yi Li2
Jianye Hao1 Yang Liu2
1Tianjin University, Tianjin, China
2Nanyang Technological University, Singapore
3University of Alberta, Canada

Abstract

Despite achieving superior performance in human-level control problems, unlike
humans, deep reinforcement learning (DRL) lacks high-order intelligence (e.g.,
logic deduction and reuse), thus it behaves ineffectively than humans regarding
learning and generalization in complex problems. Previous works attempt to
directly synthesize a white-box logic program as the DRL policy, manifesting
logic-driven behaviors. However, most synthesis methods are built on imperative
or declarative programming, and each has a distinct limitation, respectively. The
former ignores the cause-effect logic during synthesis, resulting in low generaliz-
ability across tasks. The latter is strictly proof-based, thus failing to synthesize
programs with complex hierarchical logic. In this paper, we combine the above two
paradigms together and propose a novel Generalizable Logic Synthesis (GALOIS)
framework to synthesize hierarchical and strict cause-effect logic programs. GA-
LOIS leverages the program sketch and deï¬nes a new sketch-based hybrid program
language for guiding the synthesis. Based on that, GALOIS proposes a sketch-
based program synthesis method to automatically generate white-box programs
with generalizable and interpretable cause-effect logic. Extensive evaluations on
various decision-making tasks with complex logic demonstrate the superiority
of GALOIS over mainstream baselines regarding the asymptotic performance,
generalizability, and great knowledge reusability across different environments.

1

Introduction

Deep reinforcement learning (DRL) has achieved great breakthroughs in various domains like robotics
control [24], video game [21], ï¬nance trading [43], etc. Despite its sheer success, DRL models still
perform less effective learning and generalization abilities than humans in solving long sequential
decision-making problems, especially those requiring complex logic to solve [13, 35]. For example,
a seemingly simple task for a robot arm to put an object into a drawer is hard to solve due to the
complex intrinsic logic (e.g., open the drawer, pick the object, place the object, close the drawer) [30].
Additionally, DRL policies are also hard to interpret since the result-generating processes of the
neural network remain opaque to humans due to its black-box nature [13, 25].

To mitigate the above challenges, researchers seek the programming language, making the best
of both connectionism [27] and symbolism [38], to generate white-box programs as the policy to
execute logic-driven and explainable behaviors for task-solving. Logic contains explainable task-
solving knowledge that naturally can generalize across similar tasks. Therefore, attempts have

âˆ—Equal contribution.
â€ Corresponding authors: Yan Zheng (yanzheng@tju.edu.cn) and Tianpei Yang (tpyang@tju.edu.cn).

Preprint. Under review.

 
 
 
 
 
 
been made to introduce human-deï¬ned prior logic into the DRL models [42]. Human written
logic programs is found to be an effective way to improve the learning performance and zero-
shot generalization [35]. However, such a manner requires manually written logic programs be-
forehand for each new environment, motivating an urgent need of automatic program synthesis.
Existing program synthesis approaches can be categorized into
two major paradigms: imperative and declarative programming
[5, 32, 26], each has its distinct limitation. The imperative pro-
gramming aims to synthesize multiple sub-programs, each has
a different ability to solve the problem, and combine them se-
quentially as a whole program [39, 12, 14]. However, programs
synthesized in such a way has limited generalizability and inter-
pretability since the imperative programming only specify the
post-condition (effect) while ignores the pre-condition (cause) of
each sub-program, which is regarded as a ï¬‚awed reï¬‚ection of cau-
sation [7] that is prone to aliasing. In other words, the agent will
arbitrarily follow the synthesized program sequentially without
knowing why (i.e., cause-effect logic). For example, assume a task
in Figure 1 that requires the agent to open the box, get the key,
open the door, then reach the goal. The synthesized imperative pro-
gram would contain sub-programs: toggle_box(); get_key(); open_door(); reach_goal(),
each should be executed sequentially (the blue path). However, when applying such a program to
another similar task with minor logical differences: the key is placed outside the box, meaning the
agent does not need to open the box. The synthesized program becomes sub-optimal as the agent will
always follow the program to open the box ï¬rst. However, the optimal policy should directly head for
the key and ignores the box (denoted as the orange path).

Figure 1: A motivating example.

On the other side, declarative programming aims to synthesize programs with explicit cause-effect
logic [13, 9] in the form of ï¬rst-order logic (FOL) [23], requiring the programs to be built on the
proof system (i.e., verify the trigger condition given the facts, then decide which rule should be
activated) [4]. However, due to the trait of FOL, programs synthesized in this way lack hierarchical
logic and thus are ineffective in solving complex tasks [1].

To combine the advantages of both paradigms and synthesize program with hierarchical cause-effect
logic, we propose a novel Generalizable Logic Synthesis (GALOIS) framework3 for further boosting
the learning ability, generalizability and interpretability of DRL. First, GALOIS introduces the concept
of the program sketch [34] and deï¬nes a new hybrid sketch-based domain-speciï¬c language (DSL),
including the syntax and semantic speciï¬cations, allowing synthesizing programs with hierarchical
logic and strict cause-effect logic at the same time. Beyond that, GALOIS propose a sketch-based
program synthesis method extended from the differentiable inductive logic programming [10],
constructing a general way to synthesize hierarchical logic program given the program-sketch. In this
way, GALOIS can not only generate hierarchical programs with multiple sub-program synergistic
cooperation for task-solving but also can achieve strict cause-effect logic with high interpretability
and generalizability across tasks. Furthermore, the synthesized white-box program can be easily
extended with expert knowledge or tuned by humans to efï¬ciently adapt to different downstream
tasks. Our contributions are threefold: (1) a new sketch-based hybrid program language is proposed
for allowing hierarchical logic programs for the ï¬rst time, (2) a general and automatic way is proposed
to synthesize programs with generalizable cause-effect logic, (3) extensive evaluations on various
complex tasks demonstrate the superiority of GALOIS over mainstream DRL and program synthesis
baselines regarding the learning ability, generalizability, interpretability, and knowledge (logic)
reusability across tasks.

2 Preliminary

2.1 Markov Decision Process

The sequential decision-making problem is commonly modeled as a Markov decision process (MDP),
which is formulated as a 5-tuple (S, A, R, P, Î»), where S is the state space, A is the action space,
R : S Ã— A â†’ R is the reward function, P : S Ã— A â†’ S is the transition function, and Î» is the

3The implementation is available at: https://sites.google.com/view/galois-drl

2

Program:toggle_box();get_key();open_door();reach_goal();discount factor. The agent interacts with the environment following a policy Ï€(at|st) to collect
experiences {(st, at, rt)}T
t=0, where T is the terminal time step. The goal is to learn the optimal
policy Ï€âˆ— that maximizes the expected discounted return: Ï€âˆ— = arg maxÏ€ Eaâˆ¼Ï€[(cid:80)T

t=0 Î»trt].

2.2

Inductive Logic Programming

Logic programming is a programming paradigm that requires programs to be written in a deï¬nite
clause, which is of the form: H :âˆ’ A1, ..., An, where H is the head atom and A1, ..., An, n â‰¥ 0 is
called the body that denotes the conjunction of n atoms, :âˆ’ denotes logical entailment: H is true
if A1 âˆ§ A2... âˆ§ An is true. An atom is a function Ïˆ(Ï‰1, ..., Ï‰n), where Ïˆ is a n-ary predicate and
Ï‰i, i âˆˆ [1, n] are terms. A predicate deï¬ned based on ground atoms without deductions is called an
extensional predicate. Otherwise, it is called an intensional predicate. An atom whose terms are all
instantiated by constants is called a ground atom. The ground atoms whose propositions are known
in prior without entailment are called facts. Note that a set composed of all the concerning ground
atoms is called a Herbrand base.

Inductive Logic Programming (ILP) [17] is a logic program synthesis model which synthesizes a
logic program that satisï¬es the pre-deï¬ned speciï¬cation. In the supervised learning setting, the
speciï¬cation is to synthesize a logic program C such that âˆ€Î¶, Î» : F, C |= Î¶, F, C (cid:54)|= Î», where Î¶, Î»
denotes positive and negative samples, F is the set of background facts given in prior; and for the
reinforcement learning setting, the speciï¬cation is to synthesize C such that C = argmaxC R, where
R is the average return of each episode. Speciï¬cally, ILP is conducted based on the valuation vector
e âˆˆ {0, 1}|G|, G denotes the Herbrand base of the ground atoms. Each scalar of e represents the true
value of the corresponding ground atom. During each deduction step, e is recursively updated with
the forward chaining mechanism, such that the auxiliary atoms and target atoms would be grounded.

3 Methodology

3.1 Motivation

As aforementioned, solving real decision-making problems, e.g., robot navigation and control [39, 31,
41], commonly requires complicated logic. As humans, we use the â€œdivide-and-conquerâ€ concept to
dismantle problems into sub-problems and solve them separately. It is natural to think of generating
a hierarchical logic program to solve complex problems. This intuition, however, has hardly been
adopted in program synthesis since the strict cause-effect logic program is intrinsically non-trivial to
generate, let alone the one with hierarchical logic [32, 26].

In this work, we propose a generalized logic synthesis (GALOIS) framework for synthesizing a
white-box hierarchical logic program (as the policy) to execute logically interpretable behaviors in
complex problems. Figure 2 shows the overview of GALOIS, comprised of two key components:
1 a sketch-based DSL, and 2 a sketch-based program synthesis method. It is noteworthy that
GALOIS uses a white-box program as the policy to interact with the environment and collect data
for policy optimization. Here, a new DSL is deï¬ned for creating hierarchical logic programs; and
the sketch-based program synthesis method based on differentiable ILP is adopted for generating
effective logic for the policy synthesis. In this way, GALOIS can synthesize white-box programs
with generalizable logic more efï¬ciently and automatically.

3.2 Sketch-based Program Language

To synthesize logic programs with both hierarchical logic and explicit cause-effect logic, we design a
novel sketch-based DSL, namely Lhybrid, absorbing both the advantages of imperative and declar-
ative programming. Figure 3 shows the detail syntax and semantic speciï¬cations of Lhybrid. It is
noteworthy that Lhybrid ensures the synthesized program follow strict cause-effect logic. Beyond that,
following Lhybrid, we synthesize programs using program sketches, allowing generating hierarchical
logic programs. In the following, we describe the formal syntactic and semantic speciï¬cations ï¬rst,
and illustrate how the program sketch derives hierarchical logic programs.

Syntactic Speciï¬cation: The formal syntactic speciï¬cations of Lhybrid are deï¬ned using elements
from both the declarative and imperative language (shown in Figure 3(a)). Intuitively, the declarative

3

Figure 2: Overview of GALOIS, where the (a) sketch-based DSL deï¬nes what program can be
synthesized, and (b) sketch-based ILP synthesizes programs with logic.

Figure 3: The (a) syntactic and (b) semantic speciï¬cations of DSL Lhybrid.

language demands the synthesized program follow strict cause-effect logic, while the imperative
language enables programs with hierarchical logic. In speciï¬c, imperative language elements are
expression e and command c. Term e can be instantiated as constant n or function call f , and c can
be assignment statement x := e, sequential execution c; c or control ï¬‚ow (while loop). Declarative
language elements are function f and clause R. To expose cause-effect relations, we constrains
functions to be implemented declaratively: f ::= R | f R, where R represents logic clause in the
form of R ::= Aâ€œ:-â€A-list, where A denotes atom and A-list is the clause body.

It is noteworthy that, to implement the program sketch, we introduce a novel language element called
hole function, denoted as ??. This hole function denotes an unimplemented logic sub-program (i.e.,
code block in Figure 2) to be synthesized given the constraints speciï¬ed by the program sketch and
its corresponding semantic speciï¬cation.

Semantic Speciï¬cation: Having the syntactic
speciï¬cation, any syntactically valid program sketch
can be derived. However, without semantic guidance
(e.g., lack of task-related semantics), the synthesized
program may lack sufï¬cient hierarchical logic to efï¬-
ciently solve tasks [44]. Hence, we propose leverag-
ing the program sketch [34] and deï¬ning associated
semantic speciï¬cations to guide the synthesis to gen-
erate hierarchical logic programs. In the following,
we illustrate the details of the program sketch used
in this work and its formal semantic speciï¬cations,
based on which sketching-based inductive logic pro-
gramming is performed. Speciï¬cally, as shown in the inter-procedural control ï¬‚ow graph (ICFG)
illustration Figure 4, the program sketch contains three major basic blocks of hole functions (denoted
as Ï) to be synthesized: ??WHERE, ??HOW and ??WHAT.

Figure 4: (left) A program sketch represented
as ICFG and (right) the synthesized logic.

During each round of recursion, the program ï¬rst executes and checks whether termination condition
is satisï¬ed, if not, an assignment statement is executed: l :=??WHERE(s) by calling a hole function:
??WHERE. We deï¬ne the meaning of hole function following the formalism of standard denotational
semantics [29] in Figure 3(b). Concretely, for ??WHERE, the body of the synthesized clause AWHERE(s)

4

programsketchActionEnvHerbrandbasewhite-boxprogram(Policy)Stateğ‘ ğ‘InteractionsynthesizelogicprogramNorm & Sampleâ€¦â€¦â€¦â€¦Verifying(a) sketching-based domain specific language(b) sketch-based ILPICFGcode blockNâ€¦ExitEntrycode block1ğ‘syntactic specificationsemantic specificationEncoder Decoderğ¸ğ·DeclarativeImperative(a)syntax(b)semanticsğ‘”ğ‘¡_ğ‘˜ğ‘’ğ‘¦():âˆ’Â¬â„ğ‘ğ‘ _ğ‘˜ğ‘’ğ‘¦ğ‘‹,ğ‘–ğ‘ _ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡(ğ‘‹),â„ğ‘ğ‘ _ğ‘˜ğ‘’ğ‘¦(ğ‘Œ),ğ‘–ğ‘ _ğ‘’ğ‘›ğ‘£(ğ‘Œ)ğ‘”ğ‘¡_ğ‘‘ğ‘œğ‘œğ‘Ÿ():âˆ’â„ğ‘ğ‘ _ğ‘˜ğ‘’ğ‘¦ğ‘‹,ğ‘–ğ‘ ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ğ‘‹,Â¬ğ‘–ğ‘ _ğ‘œğ‘ğ‘’ğ‘›ğ‘Œ,ğ‘–ğ‘ _ğ‘‘ğ‘œğ‘œğ‘Ÿ(ğ‘Œ)EntryExitğ‘™=??WHERE(ğ‘ )??HOW(ğ‘™)??WHAT(ğ‘ )??WHERE??HOW??WHATâ€¦ğ‘ğ‘–ğ‘ğ‘˜():âˆ’ğ‘ğ‘¡ğ‘‹,ğ‘–ğ‘ _ğ‘˜ğ‘’ğ‘¦ğ‘‹,Â¬â„ğ‘ğ‘ _ğ‘˜ğ‘’ğ‘¦ğ‘Œ,ğ‘–ğ‘ _ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡(ğ‘Œ)ğ‘¡ğ‘œğ‘”ğ‘”ğ‘™ğ‘’():atğ‘‹,ğ‘–ğ‘ _ğ‘ğ‘œğ‘¥ğ‘‹,Â¬â„ğ‘ğ‘ _ğ‘˜ğ‘’ğ‘¦ğ‘Œ,ğ‘–ğ‘ _ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡(ğ‘Œ)â€¦âˆ€ğ‘:ğ‘¢ğ‘():âˆ’âˆ€ğ‘‹ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘‹,ğ‘Œ,ğ‘›ğ‘’ğ‘”(ğ‘Œ)âˆ€ğ‘:ğ‘™ğ‘’ğ‘“ğ‘¡():âˆ’âˆ€ğ‘Œğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘‹,ğ‘Œ,ğ‘os(ğ‘‹)â€¦(cid:74)

Â·
(cid:75)

is constructed from the Herbrand base representation of the current state s, namely objectsâ€™ states
and agentâ€™s attributes: GWHERE = {Ïˆj(obji) : i âˆˆ [1, m], j âˆˆ [1, n]} âˆª {Ïˆy(attrx) : x âˆˆ [1, u], y âˆˆ
[1, v]}. The clause body entails the head atom g, which denotes an abstract object within the
environment (i.e., a subgoal that agent shall arrive during this round of recursion, e.g., key, box,
etc.). The semantic function C
evaluates the clause and returns the relative coordinates between
the agent and the subgoal. The return value is passed to the logic sub-program ??HOW (shown
as the red dashed arrow). ??HOW deduces the direction d of next time step the agent shall move
to: pos (cid:55)â†’ C
s, where pos is the agentâ€™s next-time-step position after execution,
AHOW(@) is constructed from Herbrand base which consists of ground atoms that applies predicates
regarding numerical feature on the relative coordinates: GHOW = {Ïˆi(x), Ïˆi(y) : i = [1, n]}. ??HOW
executes recursively until the subgoal speciï¬ed by ??WHERE is achieved. Finally, ??WHAT deduces
the action a to take to interact with the object at the subgoal position: o (cid:55)â†’ C
AWHAT(s) |= a
s,
(cid:75)
(cid:74)
where o denotes the updated state of the interacted object. Note that the program sketch we used is
generalizable and can be applied to environments with different logic (see details in Section 4). For
tasks whose environments are signiï¬cantly different from the ones evaluated in this work, modifying
or redesigning the sketch is also effortless.

AHOW(@) |= d
(cid:75)

(cid:74)

3.3 Sketch-based Program Synthesis

GALOIS interacts with the environment to collect experiences to synthesize white-box program with
hierarchical and cause-effect logic following Lhybrid. As shown in Figure 2(b), in the following, we
illustrate how the program interacts with the environment, what is the structure of the program and
how it is trained.

Practically, different from the black-box model, GALOIS requires different types of input and output.
Therefore, GALOIS maintains an encoder E(Â·) and a decoder D(Â·) to interact with the environment.
E(s) maps the state s to a set of ground atoms (formatted as valuation vector eÏ) with the veriï¬cation
from Lhybrid, i.e., e = E(s, Lhybrid)). As shown in Figure 2(b), the leftmost squares with different
color represents the atoms from different hole functions (e.g., blue squares {f1,t}m
t=1 denotes the
ground atoms for ??WHERE). Based on eÏ, GALOIS outputs a predicate and a = D(p(eÏ)) decodes
it into the corresponding action in the environment, where p(Â·) denotes the deduction process.

In this way, the program is executable, and the program synthesis can be performed. Guided by
the semantics of the hole functions, GALOIS performs deduction using the weights Î¸ assigned to
each candidate clauses of the speciï¬c atom. This process is shown in Figure 2(b). The rightmost
squares represent the ï¬nal atom deduced in the corresponding hole function. GALOIS combines all
the ground atoms to perform a complex program. For example, f2,1 is performed using the conjection
operation between f1,1 and f1,2. A learnable weight is assigned to each logic operation (e.g., Î¸2,1 is
assigned to f2,1) and different weights represent different clauses (e.g., Î¸3,1 represents the clause :
gt_key():- Â¬has_key(X), is_agent(X), has_key(Y), is_env(Y) shown in Figure 4).

Now we explain in detail how a certain predicate is deduced. Given valuation vector eÏ, the deductions
of the predicates are:

p(eÏ„

Ï; Î¸) = eÏ„ âˆ’1

Ï âŠ• (

(cid:88)

softmax(Î¸Ïˆ

Ï ) (cid:12) eÏ„ âˆ’1,Ïˆ
Ï

), Ïˆ âˆˆ Î¨h(t),

Ïˆ

where eÏ„
Ï denotes valuation vector for all the ground atoms in hole function Ï and deduction step Ï„ ,
which is essentially a vector that stores the inferred truth values for all the corresponding ground
atoms. âŠ• denotes the probabilistic sum: x âŠ• y = x + y âˆ’ x Â· y. Speciï¬cally, given the normalized
weight vector Î¸Ïˆ
Ï for the predicate Ïˆ in hole function Ï, to perform a single-step deduction, we take
the Hadamard product (cid:12) of Î¸Ïˆ
Ï and the valuation vector of last forward-chaining step. We then obtain
the deductive result by taking the sum of all the intentional predicates. Finally, the valuation vector
is updated to be eÏ„
Ï by taking the probabilistic sum âŠ• of the deductive result and last step valuation
vector. Intuitively, this process is similar to the forward propagation of a neural network, while
GALOIS uses logic deduction to generate results.
Based on the experiences {(st, at, rt)}T
t=0 collected when interacting with the environment, GALOIS
can thus synthesis the optimal hierarchical logic program to get the maximum expected cumulative
return: Ï€Î¸âˆ— = arg maxÎ¸ Eaâˆ¼Ï€Î¸
, where Î¸ denotes the learnable parameters in

(cid:105)
t=0 Î³tr(st, at)

(cid:104)(cid:80)T

5

Figure 5: Visualization of various tasks in MiniGrid, each requires different logic to accomplish: (a)
the (red triangle) agent aims to pick up the (yellow) key to open the door (yellow box) and move to
the goal (in green); (b) the agent needs to open the (gray) box to get the key ï¬rst, then open the door
to reach the goal; (c) the agent has to pick up the key to open the door, and then drop the key to pick
up the (green) box; (d) the agent need to open multiple (yellow, blue, and red) doors to reach the goal.

GALOIS. We train it with the Monte-Carlo policy gradient [37]:

Î¸(cid:48) = Î¸ + Î±âˆ‡Î¸ log Ï€Î¸QÏ€Î¸ (st, at) + Î³(cid:15)âˆ‡Î¸H(Ï€Î¸).
where H(Ï€Î¸) is the entropy regularization to improve exploration [19], the Î³(cid:15) is the hyperparameter
to control the decrease rate of the entropy with time.

4 Experiments

To evaluate the effectiveness of GALOIS, we study the following research questions (RQs):
RQ1 (Performance): How effective is GALOIS regarding the performance and learning speed?
RQ2 (Generalizability): How is the generalizability of GALOIS across environments?
RQ3 (Reusability): Does GALOIS show great knowledge reusability across different environments?

4.1 Setup

Environments: We adopt the MiniGrid environments [6], which contains various tasks that require
different abilities (i.e., navigation and multistep logical reasoning) to accomplish. We consider four
representative tasks with incremental levels of logical difï¬culties as shown in Figure 5.
Baselines: Various baseline are used for comparisons, including mainstream DRL approaches, i.e.,
value-based (DQN [20]), policy-based (PPO [28]), actor-critic (SAC [11]), hierarchical (h-DQN [18])
algorithms, and the program synthesis guided methods (MPPS [40]). To avoid unfair comparison, we
use the same training settings for all methods (see Appendix B for more details).

4.2 Performance Analysis (RQ1).

To answer RQ1, we evaluate the performance of GALOIS and other baseline methods in the training
environment. The results in Figure 6 show that GALOIS outperforms all other mainstream baselines
in terms of performance in environments that require complex logic, showing that GALOIS can learn
the comprehensive task-solving logic, leading to the highest performance. Note that in the DoorKey
environment, all baseline methods can reach optimal training performance, and DQN converges the
fastest. This is because the DoorKey environment is relatively simpler, whose intrinsic logic is easy
to learn, and hierarchical models have more parameters than the DQN model, leading to a slower
convergence speed. Moreover, we observe that MPPS, hDQN, and GALOIS converge faster than the
methods without hierarchy in environments that require more complex logic (e.g., UnlockPickup,
BoxKey). We can thus conclude that introducing hierarchy contributes to more efï¬cient learning.
Besides, unlike other pure neural network baselines, GALOIS and MPPS present steadier asymptotic
performance during training with also smaller variance. This result demonstrates the effectiveness of
introducing program synthesis for steady policy learning.

Speciï¬cally, MPPS theoretically fails on MultiRoom as there exists no deterministic imperative
program description (denoted as N/A in Table 2). The reason is that the sequence of colored doors that
the agent should cross differs for each episode (e.g., ep1: red_doorâ†’yellow_doorâ†’blue_door,

6

(a) DoorKey(b) BoxKey(c) Unlockpickup(b) MultiroomFigure 6: Comparisons of GALOIS and related baselines regarding the asymptotic performance and
learning speed (all the results are averaged over 5 random seeds).

Table 1: Average return on the training environment and corresponding test environments with
different sizes, (v) denotes agent trained with valuation vectors, (tr) denotes the training environment.

Size (n)

DQN

DQN(v)

SAC

SAC(v)

PPO

PPO(v)

hDQN

hDQN(v)

MPPS

MPPS(v)

Ours(v)

DoorKey

BoxKey

UnlockPickup

Multiroom

8*8 (tr)
10*10
12*12
14*14
16*16
18*18
20*20

8*8 (tr)
10*10
12*12
14*14
16*16
18*18
20*20

6*6 (tr)
8*8
10*10
12*12
14*14
16*16
18*18

8*8 (tr)
10*10
12*12
14*14
16*16
18*18
20*20

0.473Â±0.130
0.166Â±0.072
0.050Â±0.035
0.028Â±0.022
0.006Â±0.005
0.000Â±0.000
0.000Â±0.000

0.241Â±0.166
0.072Â±0.012
0.007Â±0.012
0.000Â±0.000
0.007Â±0.012
0.000Â±0.000
0.000Â±0.000

0.236Â±0.240
0.008Â±0.017
0.000Â±0.000
0.000Â±.0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000

0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000

0.919Â±0.071
0.794Â±0.170
0.730Â±0.175
0.698Â±0.109
0.877Â±0.109
0.680Â±0.238
0.746Â±0.184

0.305Â±0.112
0.262Â±0.091
0.256Â±0.035
0.237Â±0.035
0.290Â±0.045
0.224Â±0.023
0.251Â±0.046

0.428Â±0.164
0.324Â±0.159
0.307Â±0.122
0.263Â±0.216
0.277Â±0.233
0.231Â±0.171
0.205Â±0.146

0.014Â±0.008
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000

0.966Â±0.019
0.791Â±0.133
0.527Â±0.066
0.362Â±0.044
0.161Â±0.081
0.149Â±0.071
0.099Â±0.042

0.608Â±0.046
0.610Â±0.098
0.411Â±0.084
0.235Â±0.054
0.206Â±0.062
0.131Â±0.042
0.071Â±0.035

0.222Â±0.069
0.164Â±0.059
0.080Â±0.020
0.042Â±0.012
0.021Â±0.019
0.018Â±0.022
0.003Â±0.005

0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000

0.938Â±0.052
0.818Â±0.136
0.829Â±0.184
0.787Â±0.132
0.886Â±0.065
0.734Â±0.173
0.690Â±0.185

0.711Â±0.041
0.767Â±0.064
0.830Â±0.014
0.844Â±0.040
0.846Â±0.052
0.863Â±0.005
0.844Â±0.022

0.510Â±0.145
0.457Â±0.196
0.460Â±0.252
0.488Â±0.253
0.472Â±0.318
0.496Â±0.305
0.470Â±0.317

0.007Â±0.007
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000

0.919Â±0.017
0.717Â±0.024
0.494Â±0.021
0.403Â±0.056
0.269Â±0.035
0.139Â±0.032
0.211Â±0.062

0.643Â±0.029
0.564Â±0.076
0.470Â±0.117
0.340Â±0.074
0.254Â±0.054
0.155Â±0.084
0.124Â±0.038

0.763Â±0.014
0.578Â±0.094
0.364Â±0.112
0.198Â±0.045
0.176Â±0.039
0.128Â±0.053
0.032Â±0.032

0.002Â±0.003
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.001
0.000Â±0.000
0.001Â±0.003

0.958Â±0.008
0.871Â±0.028
0.800Â±0.014
0.726Â±0.008
0.750Â±0.008
0.543Â±0.021
0.768Â±0.034

0.714Â±0.051
0.769Â±0.015
0.844Â±0.044
0.816Â±0.052
0.835Â±0.027
0.846Â±0.075
0.874Â±0.042

0.826Â±0.054
0.869Â±0.023
0.908Â±0.010
0.902Â±0.009
0.919Â±0.014
0.876Â±0.030
0.899Â±0.049

0.236Â±0.036
0.166Â±0.026
0.115Â±0.050
0.072Â±0.030
0.100Â±0.009
0.074Â±0.028
0.078Â±0.037

0.979Â±0.002
0.452Â±0.496
0.152Â±0.263
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000

0.488Â±0.273
0.359Â±0.285
0.302Â±0.227
0.231Â±0.132
0.198Â±0.124
0.099Â±0.105
0.093Â±0.011

0.496Â±0.346
0.187Â±0.225
0.097Â±0.164
0.051Â±0.102
0.024Â±0.053
0.012Â±0.026
0.000Â±0.000

0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000

0.928Â±0.114
0.834Â±0.335
0.769Â±0.232
0.734Â±0.251
0.755Â±0.235
0.799Â±0.214
0.729Â±0.256

0.541Â±0.056
0.478Â±0.028
0.604Â±0.042
0.507Â±0.084
0.607Â±0.014
0.568Â±0.014
0.463Â±0.127

0.824Â±0.233
0.820Â±0.257
0.843Â±0.208
0.822Â±0.271
0.869Â±0.192
0.800Â±0.318
0.827Â±0.273

0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000
0.000Â±0.000

0.861Â±0.046
0.894Â±0.022
0.906Â±0.029
0.904Â±0.027
0.910Â±0.047
0.911Â±0.050
0.929Â±0.028

0.864Â±0.069
0.882Â±0.065
0.881Â±0.088
0.893Â±0.090
0.861Â±0.155
0.879Â±0.120
0.905Â±0.097

0.645Â±0.104
0.747Â±0.143
0.765Â±0.115
0.802Â±0.080
0.834Â±0.075
0.841Â±0.122
0.870Â±0.062

N/A
N/A
N/A
N/A
N/A
N/A
N/A

0.949Â±0.021
0.941Â±0.033
0.950Â±0.040
0.952Â±0.040
0.944Â±0.045
0.932Â±0.033
0.963Â±0.007

0.949Â±0.003
0.946Â±0.010
0.950Â±0.000
0.952Â±0.008
0.958Â±0.001
0.963Â±0.002
0.957Â±0.013

0.813Â±0.039
0.872Â±0.025
0.935Â±0.012
0.936Â±0.002
0.962Â±0.003
0.961Â±0.010
0.963Â±0.009

N/A
N/A
N/A
N/A
N/A
N/A
N/A

0.963 Â±0.008
0.963 Â±0.007
0.963 Â±0.007
0.965 Â±0.006
0.963 Â±0.007
0.964 Â±0.005
0.966 Â±0.005

0.975 Â±0.001
0.981 Â±0.001
0.985 Â±0.000
0.987 Â±0.000
0.988 Â±0.000
0.990 Â±0.000
0.987 Â±0.009

0.901 Â±0.021
0.933 Â±0.014
0.953 Â±0.007
0.957 Â±0.011
0.969 Â±0.004
0.973 Â±0.005
0.977 Â±0.000

0.663 Â±0.018
0.622 Â±0.017
0.607 Â±0.012
0.529 Â±0.020
0.596 Â±0.015
0.529 Â±0.029
0.519 Â±0.023

ep2: blue_doorâ†’red_doorâ†’yellow_door), thus the program on solving this task is dynamically
changing, which fails the imperative program synthesizer. This further indicates the importance
of synthesizing declarative programs with cause-effect logic instead of merely ï¬nding the ordered
sequence of subprograms for solving tasks. More details are discussed in the following sections.

4.3 Generalizability Analysis (RQ2).

To answer RQ2, we evaluate modelsâ€™ performance on test environments with different sizes and task-
solving logic (i.e., semantic modiï¬cations). Concretely, as shown in Table 1, GALOIS outperforms
all the other baseline methods (highest average returns are highlighted in gray) and maintains
near-optimal performance. Furthermore, we also observe that all other baseline methods maintain
acceptable generalizability. This contradicts the conclusion in [13] that neural network-based agents
fail to generalize to environments with size changes. We hypothesize that this attributes to the use
of different types of representations. To evaluate the effectiveness of using the valuation vector
representation, we conduct experiments using the observations directly obtained from environments
(e.g., the status and locations of objects). Surprisingly, though achieving decent performance on
the training environment, all the vanilla neural network-based baselines perform poorly on test
environments of different sizes. Therefore, we conclude that by introducing logic expression as state
representation (in the form of valuation vectors), better generalizability can be obtained. However, as
illustrated by the results, the valuation vector itself is not enough to achieve supreme generalizability,
GALOIS manages to achieve even better generalizability due to explicit use of cause-effect logic
with a hierarchical structure.

We then evaluate modelsâ€™ generalizability on two test environments with minor semantic modi-
ï¬cations, namely BoxKey (sem-mod) and UnlockPickup (sem-mod), as shown in Figure 7 (left).
Speciï¬cally, for UnlockPickup (sem-mod), different from the training environment, there is no key

7

DoorKeyPerformance#Episode0.00.20.40.61.00.8010002000300040005000BoxKey#Episode0.00.20.40.61.00.8010002000300040005000#EpisodeUnlockPickup0.00.20.40.61.00.8010002000300040005000#EpisodeMultiRoom0.00.20.40.61.00.8010002000300040005000Figure 7: (left) shows the original and semantic-modiï¬ed environments of UnlockPickup and BoxKey.
The optimal traces are marked in orange; (right) shows the synthesized program.

Table 2: Average return on test environments with semantic modiï¬cations.

DQN

SAC

PPO

hDQN

MPPS

Ours

BoxKey

UnlockPickup

8*8(tr)

0.241Â±0.166

0.608Â±0.046

0.714Â±0.042

0.541Â±0.056

0.949Â±0.003

0.975 Â±0.001

sem-mod

0.040Â±0.040

0.098Â±0.005

0.126Â±0.008

0.476Â±0.091

0.119Â±0.020

0.976 Â±0.001

12*6(tr)

0.236Â±0.240

0.222Â±0.069

0.826Â±0.054

0.824Â±0.233

0.813Â±0.039

0.901 Â±0.021

sem-mod

0.007Â±0.012

0.040Â±0.005

0.098Â±0.004

0.434Â±0.390

0.000Â±0.000

0.983 Â±0.003

in the environment, and the door is already open. Thus agent should head for the target location
directly. For BoxKey (sem-mod), the key is placed outside the box. Thus, optimally, the agent should
directly head for the key and ignore the existence of the box. The results in Table 2 indicate that all
the baselines are severely compromised while GALOIS retains near-optimal generalizability. This
attributes to its explicit use of cause-effect logic.

Figure 7 (right) shows an example synthesized program of GALOIS (we include more synthesized
program examples in Appendix A). E.g. The program speciï¬es the cause of gt_box() (marked in
bold) as agent has no key and there exists no visible key in the environment. Thus when placed
under BoxKey(sem-mod), GALOIS agent would skip gt_box() and head directly for the key since
gt_box() is grounded as false by the logic clause body. The result indicates that the explicit use of
effect-effect logic is not only veriï¬able for human but allows GALOIS model perform robustly on
environments with different task-solving logic. Additionally, for MPPS, since it only learns a ï¬xed
sequence of sub-programs it fails to generalize. E.g. the synthesized program of MPSS trained on
BoxKey is: toggle_box();get_key();open_door();reach_goal(), thus when the key is placed
under BoxKey(sem-mod), the agent would still strictly follow the learned program and redundantly
toggle the box ï¬rst.

4.4 Knowledge Reusability Analysis (RQ3)

To answer RQ3, we initialize a GA-
LOIS modelâ€™s weights with the knowl-
edge base learned from other tasks
(e.g., DoorKeyâ†’BoxKey) and ï¬ne-
tune the entire model continuously.
Figure 9 shows the detailed results
of knowledge reusability among three
different environments. Apparently,
the learning efï¬ciency can be signif-
icantly increased by warm-starting
the weights of the GALOIS model
with knowledge learned from differ-
ent tasks with overlapped logic compared with the one that is learned from scratch. Furthermore, we
demonstrate the reusability of knowledge from each sub-program, respectively. The results show that
a considerable boost in learning efï¬ciency can already be obtained by reusing knowledge from each
sub-program respectively (e.g., BoxKey(DoorKey-where) agent is only warm-started with the sub-

Figure 8: The illustration of knowledge reused from DoorKey
to BoxKey. The orange path represents the reusable knowl-
edge (learned from DoorKey and directly reused in BoxKey).

8

UnlockPickUp(train)UnlockPickUp(sem-mod)BoxKey(train)BoxKey(sem-mod)whileÂ¬ goal do(1.00) gt_box() :-Â¬ has_key(X), is_agent(X), Â¬ has_key(Y), is_env(Y)(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_goal() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(1.00) toggle() :-at(X), is_box(X), Â¬ has_key(Y), is_agent(Y)(0.94) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.95) right() :-(cid:1482)Y current(X,Y), neg(X)(1.00) left() :-(cid:1482)Y current(X,Y), pos(X) (1.0) up() :-(cid:1482)X current(X,Y), neg(Y)(0.89) down() :-(cid:1482)X current(X,Y), pos(Y) whileÂ¬ subgoal doSynthesized program(a) DoorKey(b) BoxKeyFigure 9: Knowledge reusability across different environments. full denotes warm-starting policy
with the full program, {where, how, what} denotes warm-starting with only the sub-program from
the corresponding hole function (e.g., ??WHERE), none means learning from scratch.

program of ??WHERE), which is an advantage brought by GALOISâ€™s hierarchical and cause-effect
logic. Figure 8 shows an example of knowledge reusing from DoorKey to BoxKey environments
(BoxKey(DoorKey-full)). By reusing the logic learned from the DoorKey environment (the orange
path in Figure 8), agent only needs to learn the cause-effect logic of toggle_box() from scratch,
which greatly boosts the learning efï¬ciency.

5 Related Work

Neural Program Syntheis: Given a set of program speciï¬cations (e.g., I/O examples, natural
language instructions, etc.), program synthesis aims to induce an explicit program that satisï¬es
the given speciï¬cation. Recent works illustrate that neural networks are effective in boosting both
the synthesis accuracy and efï¬ciency [8, 5, 3, 10, 15]. Devlin et al. [8] propose using a recurrent
neural network to synthesize programs for string transformation. Chen et al. [5] further propose
incorporating intermediate execution results to augment the modelâ€™s input state, which signiï¬cantly
improves performance for imperative program synthesis. âˆ‚ILP [10] proposes modeling the forward
chaining mechanism with a statistical model to achieve synthesis for Datalog programs.

Program Synthesis by Sketching: Many real-world synthesis problems are intractable, posing a
great challenge for the synthesis model. Sketching [34, 44, 33] is a novel program synthesis paradigm
that proposes establishing the synergy between the human expert and the synthesizer by embedding
domain expert knowledge as general program sketches (i.e., a program with unspeciï¬ed fragments
to be synthesized), based on which the synthesis is conducted. Singh et al. [33] propose a feedback
generation system that automatically synthesizes program correction based on a general program
sketch. Nye et al. [22] propose a two-stage neural program synthesis framework that ï¬rst generates
a coarse program sketch using a neural model, then leverages symbolic search for second-stage
ï¬ne-tuning based on the generated sketch.

Program Synthesis for Reinforcement Learning: Leveraging program synthesis for the good of
reinforcement learning has been increasingly popular as it is demonstrated to improve performance
and interpretability signiï¬cantly. Jiang et al. [13] introduce using âˆ‚ILP model for agentâ€™s policy,
which improves downstream generalization by expressing policy as explicit functional programs.
Imperative programs are used as a novel implementation of hierarchical reinforcement learning in
which the agentâ€™s policy is guided by high-level programs [35, 39, 12]. In addition, program synthesis
has also been used as a post hoc interpretation method for neural policies [36, 2].

6 Conclusion

In this work, we propose a novel generalizable logic synthesis framework GALOIS that can synthesize
programs with hierarchical and cause-effect logic. A novel sketch-based DSL is introduced to allow
hierarchical logic programs. Based on that, a hybrid synthesis method is proposed to synthesize
programs with generalizable cause-effect logic. Experimental results demonstrates that GALOIS can
signiï¬cantly ourperforms DRL and previous prorgam-synthesis-based methods in terms of learning
ability, generalizablility and interpretability.

9

DoorKey  BoxKey  0400800140012002006001000UnlockPickupBoxKey  0.20.40.60.81.00.0UnlockPickupDoorKey  0.20.40.60.81.00.004008001400120020060010000400800140012002006001000Performance0.20.40.60.81.00.0#Episodes#Episodes#EpisodesRegarding limitation, as it is general for all program synthesis-based methods, the input images need
to be pre-processed into Herbrand base for the synthesis model, which is required to be done once for
each domain. Therefore, automatic Herbrand base learning would be an important future direction.
We state that our work would not produce any potential negative societal impacts.

References

[1] Chitta Baral and Michael Gelfond. Logic programming and knowledge representation. The Journal of

Logic Programming, 19:73â€“148, 1994.

[2] Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Veriï¬able reinforcement learning via policy

extraction. Advances in neural information processing systems, 31, 2018.

[3] Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging
grammar and reinforcement learning for neural program synthesis. arXiv preprint arXiv:1805.04276, 2018.

[4] Samuel R Buss. An introduction to proof theory. Handbook of proof theory, 137:1â€“78, 1998.

[5] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International

Conference on Learning Representations, 2018.

[6] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for

openai gym. https://github.com/maximecb/gym-minigrid, 2018.
[7] Francis Macdonald Cornford et al. The republic of Plato. CUP Archive, 1976.

[8] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet
Kohli. Robustï¬ll: Neural program learning under noisy i/o. In International conference on machine
learning, pages 990â€“998. PMLR, 2017.

[9] Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines.
In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019. OpenReview.net, 2019.

[10] Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of Artiï¬cial

Intelligence Research, 61:1â€“64, 2018.

[11] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International conference on machine
learning, pages 1861â€“1870. PMLR, 2018.

[12] Mohammadhosein Hasanbeig, Natasha Yogananda Jeppu, Alessandro Abate, Tom Melham, and Daniel
Kroening. Deepsynth: Automata synthesis for automatic task segmentation in deep reinforcement learning.
In Thirty-Fifth AAAI Conference on Artiï¬cial Intelligence, AAAI 2021, Thirty-Third Conference on Innova-
tive Applications of Artiï¬cial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances
in Artiï¬cial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 7647â€“7656. AAAI Press,
2021.

[13] Zhengyao Jiang and Shan Luo. Neural logic reinforcement learning. In International Conference on

Machine Learning, pages 3110â€“3119. PMLR, 2019.

[14] Kishor Jothimurugan, Rajeev Alur, and Osbert Bastani. A composable speciï¬cation language for reinforce-

ment learning tasks. Advances in Neural Information Processing Systems, 32, 2019.

[15] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets.

Advances in neural information processing systems, 28, 2015.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[17] Daphne Koller, Nir Friedman, SaÅ¡o DÅ¾eroski, Charles Sutton, Andrew McCallum, Avi Pfeffer, Pieter
Abbeel, Ming-Fai Wong, Chris Meek, Jennifer Neville, et al. Introduction to statistical relational learning.
MIT press, 2007.

[18] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforce-
ment learning: Integrating temporal abstraction and intrinsic motivation. Advances in neural information
processing systems, 29, 2016.

[19] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In
International conference on machine learning, pages 1928â€“1937. PMLR, 2016.

[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602,
2013.

10

[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. nature, 518(7540):529â€“533, 2015.

[22] Maxwell Nye, Luke Hewitt, Joshua Tenenbaum, and Armando Solar-Lezama. Learning to infer program

sketches. In International Conference on Machine Learning, pages 4861â€“4870. PMLR, 2019.

[23] Judea Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress, 19, 2000.

[24] Athanasios S Polydoros and Lazaros Nalpantidis. Survey of model-based reinforcement learning: Applica-

tions on robotics. Journal of Intelligent & Robotic Systems, 86(2):153â€“173, 2017.

[25] Erika Puiutta and Eric Veith. Explainable reinforcement learning: A survey. In International cross-domain

conference for machine learning and knowledge extraction, pages 77â€“95. Springer, 2020.

[26] Mukund Raghothaman, Jonathan Mendelson, David Zhao, Mayur Naik, and Bernhard Scholz. Provenance-

guided synthesis of datalog programs. Proc. ACM Program. Lang., 4(POPL):62â€“1, 2020.

[27] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the

brain. Psychological review, 65(6):386, 1958.

[28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[29] Dana S Scott and Christopher Strachey. Toward a mathematical semantics for computer languages,
volume 1. Oxford University Computing Laboratory, Programming Research Group Oxford, 1971.

[30] Dhruv Shah, Alexander T Toshev, Sergey Levine, and brian ichter. Value function spaces: Skill-centric
state abstractions for long-horizon reasoning. In International Conference on Learning Representations,
2022.

[31] Dhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander T Toshev, Sergey Levine, et al. Value function spaces:

Skill-centric state abstractions for long-horizon reasoning. In Deep RL Workshop NeurIPS 2021, 2021.

[32] Xujie Si, Woosuk Lee, Richard Zhang, Aws Albarghouthi, Paraschos Koutris, and Mayur Naik. Syntax-
guided synthesis of datalog programs. In Proceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages
515â€“527, 2018.

[33] Rishabh Singh, Sumit Gulwani, and Armando Solar-Lezama. Automated feedback generation for introduc-
tory programming assignments. In Proceedings of the 34th ACM SIGPLAN conference on Programming
language design and implementation, pages 15â€“26, 2013.

[34] Armando Solar-Lezama. Program synthesis by sketching. University of California, Berkeley, 2008.
[35] Shao-Hua Sun, Te-Lin Wu, and Joseph J Lim. Program guided agent. In International Conference on

Learning Representations, 2019.

[36] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Pro-
grammatically interpretable reinforcement learning. In International Conference on Machine Learning,
pages 5045â€“5054. PMLR, 2018.

[37] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3):229â€“256, 1992.

[38] Patrick Henry Winston. Artiï¬cial intelligence. Addison-Wesley Longman Publishing Co., Inc., 1992.

[39] Yichen Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, and Martin Rinard.
Program synthesis guided reinforcement learning for partially observed environments. Advances in Neural
Information Processing Systems, 34, 2021.

[40] Yichen Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, and Martin Rinard.
Program synthesis guided reinforcement learning for partially observed environments. Advances in Neural
Information Processing Systems, 34, 2021.

[41] Philipp Zech, Simon Haller, Safoura Rezapour Lakani, Barry Ridge, Emre Ugur, and Justus Piater.
Computational models of affordance in robotics: a taxonomy and systematic classiï¬cation. Adaptive
Behavior, 25(5):235â€“271, 2017.

[42] Peng Zhang, Jianye Hao, Weixun Wang, Hongyao Tang, Yi Ma, Yihai Duan, and Yan Zheng. Kogun:
accelerating deep reinforcement learning via integrating human suboptimal knowledge. arXiv preprint
arXiv:2002.07418, 2020.

[43] Zihao Zhang, Stefan Zohren, and Stephen Roberts. Deep reinforcement learning for trading. The Journal

of Financial Data Science, 2(2):25â€“40, 2020.

[44] He Zhu, Zikang Xiong, Stephen Magill, and Suresh Jagannathan. An inductive synthesis framework for
veriï¬able reinforcement learning. In Proceedings of the 40th ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 686â€“701, 2019.

11

A Synthesized Programs

(a) DoorKey

(b) BoxKey

(c) UnlockPickup

(d) MultiRoom

Figure 10: Detailed synthesized programs.

In this section, we present the detailed synthesized programs for the four evaluated environments in Figure 10.

DoorKey As shown in Figure 10a, regarding the hole function ??WHERE, agent goes to key if it veriï¬es that
the grounded fact to be true: agent has no key and the door is closed; goes to door if is has key and the door is
closed; goes to target location if it has key and the door is already open. And for hole function ??WHAT, agent
picks up key if it is adjacent to the key and it has no key; toggles the door if it is adjacent to the door and it has
key.

BoxKey As shown in Figure 10b, different form DoorKey, it has to open the box to get the key. Thus the
newly learnt sub-program for hole function ??WHERE would be: agent should go to the box if there is no key in
the environment and the agent does not have key. Accordingly, for ??WHAT, the agent would toggle (open) the
box when it arrives at the box.

UnlockPickup As shown in Figure 10c, since it has to pick up an object located at the target location to
accomplish the task, the newly learned sub-program ??WHERE would be: agent would drop the key when it has
the key and the door is already open. And for ??WHAT, whenever agent arrives at the goal position, it would
pick up the box if it does not has key and the door is already open.

MultiRoom As shown in Figure 10d, regarding ??WHERE, agent would head for a door of a speciï¬c color
if it is reachable and closed, e.g., gt_red() :- reachable(X),is_red(X), Â¬ is_open(X). Thus the learned
program is color-agnostic (i.e., the agentâ€™s policy would remain robust no matter how the colored doors sequence
changes).

B Implementation Details

In this section we present the implementation details of all the baseline methods and our approach4, along with
the information of the hardware on which these models are trained.

Environment settings. The valuation vector representations are fed to all the methods as input. Speciï¬cally,
for the MPPS model, we directly provide the oracle program to save the time for synthesizing program, which
could be considered as the upper bound performance of MPPS [40]. For the h-DQN method, we keep the

4The code is available at: https://github.com/caoysh/GALOIS

12

whileÂ¬ goal do(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_goal() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(0.94) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal dowhileÂ¬ goal do(1.00) gt_box() :-Â¬ has_key(X), is_agent(X), Â¬ has_key(Y), is_env(Y)(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_goal() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(1.00) toggle() :-at(X), is_box(X), Â¬ has_key(Y), is_agent(Y)(0.94) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal dowhileÂ¬ goal do(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_drop() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)(1.00) gt_box() :-Â¬ has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(0.95) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(0.94) drop() :-at(X), is_drop(X), has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.99) pick() :-at(X), is_box(X), Â¬ has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal dowhileÂ¬ goal do(0.99) gt_yellow() :-reachable(X), is_yellow(X), Â¬ is_open(X)(0.99) gt_blue() :-reachable(X), is_blue(X), Â¬ is_open(X)(0.98) gt_red() :-reachable(X), is_red(X), Â¬ is_open(X)(1.00) gt_goal() :-reachable(X), is_goal(X)l:=(1.00) toggle() :-at(X), is_door(X), Â¬ is_open(X)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal dohigh-level goal the same as MPPS and GALOIS, (i.e., {key, door, goal} for DoorKey; {box, key, door, goal} for
BoxKey; {box, key, door, drop} for UnlockPickup; {red door, yellow door, blue door, goal} for MultiRoom).

The reward from the MiniGrid environment is sparse (i.e., only a positive reward will be given after completing
the task), thus in order to motivate the agent to learn to solve the task using the least amount of actions, we follow
previous works [13, 39] and apply the reward shaping mechanism. For every action taken, a negative reward of
âˆ’0.01 will be given, a small positive reward of +0.20 will be given for achieving each sub-task (e.g., picking
up the key), and a reward of +1.00 will be given for completing the task.

Training settings. For all DNN-based baseline methods, we use a 2-layer multilayer perceptron (MLP) with
128 neurons in each layer. We train all the baseline methods and GALOIS with the Adam optimizer [16] with
0.001 learning rate. We use a batch size of 256. The entropy coefï¬cient Î³(cid:15) starts at 5 and decays at a factor of
10 for every 50 episodes. We conducted all experiments on a Ubuntu 16.04 server with 24 cores of Intel(R)
Xeon(R) Silver 4214 2.2GHz CPU, 251GB RAM and an NVIDIA GeForce RTX 3090 GPU.

C Knowledge Reusability

In this section, we illustrate how the programs are reused from one environment to another in details.

C.1 DoorKey to BoxKey

Figure 11: The illustration of knowledge reused from DoorKey to BoxKey. The orange path in (b)
represents the reusable knowledge from (a). The blue path in (b) denotes the new knowledge need to
learn for BoxKey.

(a) DoorKey

(b) BoxKey

Figure 12: Reused program from DoorKey to BoxKey. The subprograms marked in blue (in (b)) need
to be learned for BoxKey, while other subprograms are directly reused from (a).

Figure 11 illustrates the DoorKey and BoxKey environments and their optimal policies, respectively. Different
from DoorKey, to complete the BoxKey, the agent has to open the box to get the key ï¬rst, while the agent in
DoorKey is able to get the key directly. Afterwards, the optimal policies would be the same, i.e., pickup the
key, open the door and then go to the goal. Concretely, the BoxKey agent will only need to learn how to go to
the box and open it, and thus all the knowledge can be reusable from DoorKey. As shown in Figure 12b, the
subprograms marked in blue are the new knowledge to learn to solve the BoxKey while other sub-programs can
be directly reused from DoorKey.

13

(a) DoorKey(b) BoxKeywhileÂ¬ goal do(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_goal() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(0.94) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal dowhileÂ¬ goal do(1.00) gt_box() :-Â¬ has_key(X), is_agent(X), Â¬ has_key(Y), is_env(Y)(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_goal() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(1.00) toggle() :-at(X), is_box(X), Â¬ has_key(Y), is_agent(Y)(0.94) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal doFigure 13: The illustration of knowledge reused from DoorKey to UnlockPickup. The orange path in
(a) means knowledge learnt from scratch. The orange path in (b) represents the reusable knowledge.
The blue path in (b) means the new knowledge learnt for UnlockPickup.

(a) DoorKey

(b) UnlockPickup

Figure 14: Reused program from DoorKey to UnlockPickup. The sub-programs marked in blue (in
(b)) are needed to be learnt for UnlockPickup while other sub-programs are directly reused from (a).

C.2 DoorKey to UnlockPickup

Figure 13 shows the DoorKey and UnlockPickup environments and their respective optimal policies. After
opening the door, to solve UnlockPickup, the agent has to drop the key and then pick up the box. Speciï¬cally, the
knowledge before dropping the key is reusable and the UnlockPickup agent continues to learn the rest. Due to
the white-box nature of GALOIS, instead of reusing all the knowledge, the sub-program learnt in DoorKey ( i.e.,
gt_goal() :- has_key(X), is_agent(X), is_open(Y), is_door(Y)) can be removed when reusing.
As shown in Figure 14, the sub-programs marked in blue are the new knowledge to learn while others are directly
reused from DoorKey.

C.3 BoorKey to UnlockPickup

Figure 15: The illustration of knowledge reused from BoorKey to UnlockPickup. The orange path in
(a) means knowledge learnt from scratch. The orange path in (b) represents the reusable knowledge.
The blue path in (b) means the new knowledge learnt for UnlockPickup.

Figure 15 shows the BoorKey and UnlockPickup environments and their respective optimal policies.
In speciï¬c, the agent in BoxKey learns the clause: toggle():- at(X), is_box(X), Â¬has_key(Y),
is_agent(Y) while in UnlockPickup, the clause should be: pick():- at(X), is_box(X), Â¬has_key(Y),
is_agent(Y), as shown in Figure 16. This leads to a negative impact of reusing knowledge as in UnlockPickup
environment, the box will disappear if toggled and the task will never be completed. The experiments demon-

14

(a) DoorKey(b) UnlockPickupwhileÂ¬ goal do(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_goal() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(0.94) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal dowhileÂ¬ goal do(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_drop() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)(1.00) gt_box() :-Â¬ has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(0.95) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(0.94) drop() :-at(X), is_drop(X), has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.99) pick() :-at(X), is_box(X), Â¬ has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal do(b) UnlockPickup(b) BoxKey(a) BoorKey

(b) UnlockPickup

Figure 16: Reused program from BoorKey to UnlockPickup. The sub-programs marked in blue (in
(b)) are needed to be learnt for UnlockPickup while other sub-programs are directly reused from (a).

strate the ability of GALOIS to handle nonreusable knowledge. Additionally, thanks to its white-box nature, this
rule can also be identiï¬ed and removed manually to avoid negative effect.

15

whileÂ¬ goal do(1.00) gt_box() :-Â¬ has_key(X), is_agent(X), Â¬ has_key(Y), is_env(Y)(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_goal() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(1.00) toggle() :-at(X), is_box(X), Â¬ has_key(Y), is_agent(Y)(0.94) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal dowhileÂ¬ goal do(1.00) gt_key() :-Â¬ has_key(X), is_agent(X), has_key(Y), is_env(Y)(1.00) gt_door() :-has_key(X), is_agent(X), Â¬ is_open(Y), is_door(Y)(1.00) gt_drop() :-has_key(X), is_agent(X), is_open(Y), is_door(Y)(1.00) gt_box() :-Â¬ has_key(X), is_agent(X), is_open(Y), is_door(Y)l:=(0.95) pick() :-at(X), is_key(X), Â¬ has_key(Y), is_agent(Y)(0.94) drop() :-at(X), is_drop(X), has_key(Y), is_agent(Y)(1.00) toggle() :-at(X), is_door(X), has_key(Y), is_agent(Y)(0.99) pick() :-at(X), is_box(X), Â¬ has_key(Y), is_agent(Y)(0.95) right() :-âˆ€Y current(X,Y), neg(X)(1.00) left() :-âˆ€Y current(X,Y), pos(X) (1.0) up() :-âˆ€X current(X,Y), neg(Y)(0.89) down() :-âˆ€X current(X,Y), pos(Y) whileÂ¬ subgoal do