1

Adaptive Subcarrier, Parameter, and Power

Allocation for Partitioned Edge Learning Over

Broadband Channels

Dingzhu Wen, Ki-Jun Jeon, Mehdi Bennis, and Kaibin Huang

Abstract

In this paper, we consider partitioned edge learning (PARTEL), which implements parameter-

server training, a well known distributed learning method, in a wireless network. Thereby, PARTEL

leverages distributed computation resources at edge devices to train a large-scale artiï¬cial intelligence

(AI) model by dynamically partitioning the model into parametric blocks for separated updating at

devices. Targeting broadband channels, we consider the joint control of parameter allocation, sub-

channel allocation, and transmission power to improve the performance of PARTEL. Speciï¬cally, the

policies for joint SUbcarrier, Parameter, and POweR allocaTion (SUPPORT) are optimized under the

criterion of minimum learning latency. Two cases are considered. First, for the case of decomposable

models (e.g., logistic regression), the latency-minimization problem is a mixed-integer program and non-

convex. Due to its intractability, we develop a practical solution by integer relaxation and transforming

it into an equivalent convex problem of model size maximization under a latency constraint. Thereby,

a low-complexity algorithm is designed to compute the SUPPORT policy. Second, consider the case

of deep neural network (DNN) models which can be trained using PARTEL by introducing some

auxiliary variables. This, however, introduces constraints on model partitioning reducing the granularity

of parameter allocation. The preceding policy is extended to DNN models by applying the proposed

techniques of load rounding and proportional adjustment to rein in latency expansion caused by the

load granularity constraints.

I. INTRODUCTION

Edge machine learning is an area concerning the deployment of learning algorithms at the

network edge to gain low-latency access to data and computation resources distributed at a

D. Wen and K. Huang are with The University of Hong Kong, Hong Kong. K.-J. Jeon is with the LG Electronics, Korea.

M. Bennis is with University of Oulu, Finland. Corresponding email: huangkb@eee.hku.hk.

1
2
0
2

r
a

M
8
1

]
T
I
.
s
c
[

2
v
1
6
0
4
0
.
0
1
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

large number of edge devices [1]. In this work, we study the efï¬cient implementation of the

well-known method of parameter-server training [2] in a broadband system (e.g., 3GPP 5G)

to exploit distributed computation resources at many devices to scale up model training. To

this end, several key operations, namely computation-load allocation (via model partitioning),

sub-channel allocation, and power control, are jointly designed under the criterion of minimum

learning latency.

Two main methods for distributed learning are federated learning [3], [4] and parameter-server

training [2], [5], [6], which are designed for different scenarios and features. The key feature

of federated learning is its preservation of data privacy. Based on distributed implementation

of stochastic gradient descent (SGD), federated learning iterates the separate training of a

downloaded model at multiple devices using their local data, and the uploading and aggregation

of local models (or local stochastic gradients) to yield a more accurate global model [3], [4]. The

avoidance of direct data uploading protects their privacy. Though it is similar to federated learning

in implementing distributed SGD, the parameter-server training, which is of our interest, has one

distinction. Its purpose is to scale up learning using many resource-constrained machines in a

closed network where data privacy is not a concern [2]. To this end, the model is partitioned to

allow each device to train only a part of the model instead of the whole as in federated learning.

This overcomes the resource constraints of devices and reduces their energy consumption.

Moreover, training data are downloaded from a server to devices at the beginning of each round,

avoiding their need of persistent storage space.

A current main theme in the ï¬eld of edge learning is the design of wireless techniques

to support efï¬cient deployment of federated learning, resulting in an area called federated

edge learning (FEEL) [7]. The effort on overcoming the communication bottleneck of FEEL

has led to the design of a new class of multi-access techniques realizing over-the-air model

aggregation[8]â€“[12] and radio resource management (RRM) techniques [13]â€“[17]. Moreover,

researchers have designed energy efï¬cient RRM techniques to tackle the challenge of executing

a complex learning task at energy constrained devices in a FEEL system[18]â€“[20]. Recently,

researchers have also explored the efï¬cient implementation of parameter-server training over

wireless channels, resulting in a framework called partitioned edge learning (PARTEL) [21]. Let

parameter allocation refers to the system operation that to balance computation loads, the server

divides the model into parametric blocks of variable lengths and allocate them to devices for

separate training. To reduce the learning latency, the technique of joint parameter allocation and

3

resource allocation is proposed in [21], which jointly adapts parameter and bandwidth allocation

to devicesâ€™ channel states and computation capacities. For simplicity, the prior work assumes

narrowband channels, for which the management of uplink radio resource reduces to bandwidth

allocation. In this work, we design low-latency PARTEL techniques for a broadband system (e.g.,

3GPP 5G) with frequency selective channels. In this case, the frequency resource is managed via

sub-channel allocation, which is much more complex than bandwidth allocation. The complexity

arises from the fact that the sub-channels of each device have different gains and devices

have different channel realizations. Consequently, even if the allocated bandwidths are ï¬xed,

reshufï¬‚ing the assignments of sub-channels varies devicesâ€™ communication rates. Therefore,

jointly designing sub-channel and parameter allocation poses a new challenge that cannot be

tackled using the solution in [21].

The optimal sub-channel allocation is well known to be an integer optimization problem

that is NP hard [22]â€“[24]. For the conventional multiuser communication systems, the classic

approximate-solution approach has been established in a series of work for the purpose of

minimizing sum power under usersâ€™ rate constraints [22]â€“[24]. The essential idea is to relax the

integer program and discover the embedded convexity in the relaxed problem to design a practical

algorithm [22]. In this work, we build on the classic approach to develop a new solution for the

problem of latency minimization in a broadband PARTEL system. The distinction of the current

work is the pursuit of a communication-learning integration approach so as to minmize learning

latency in the context of PAETEL. To this end, we jointly design parameter and sub-channel

allocations. The considerations of synchronized updates by devices, which is a requirement for

parameter-server training [2], and devicesâ€™ heterogeneous computation capacities introduce more

challenges. Existing designs that aim at generic radio access cannot tackle the new challenges,

which motivate the current work.

It is also worth mentioning that we also consider a more complex model based on a deep

neural network (DNN) besides the decomposable model as in [21]. Unlike the latter, the former

is not directly decomposable and requires the modiï¬cation of learning algorithm to support

PARTEL. This introduces additional complexity to the current design.

By tackling the above challenges, we design a set of algorithms for joint SUbcarrier, Parameter,

POweR allocaTion (SUPPORT), termed joint SUPPORT. The main contributions of this work

are summarized as follows.

â€¢ Joint SUPPORT for Decomposable Models: Consider the case of a decomposable model.

4

The problem of latency minimization by joint SUPPORT is an integer program and in-

tractable. A practical solution approach is developed using two techniques. The ï¬rst is a

relaxation of binary subcarrier assignment decisions. The second is the transformation of

the relaxed problem into a convex problem of model size maximization under a latency con-

straint, which is nested in a simple search for the target model size. Considering the convex

problem, the properties of three optimal resource-management operations are analyzed and

then applied to design an efï¬cient algorithm for computing the desired SUPPORT policy.

Via analysis, it is found that the optimal number of parameters assigned to a worker for

updating avoids high power consumption due to overloading. For this reason, the optimal

number is derived to be a concave function of its speed and a monotonic decreasing function

of its computation power factor. On the other hand, the optimal subcarrier assignment and

power allocation over assigned subcarriers are found to favor high channel gains.

â€¢ Joint SUPPORT for DNN Models: Consider the case of a DNN model. The optimization

problems for joint SUPPORT in both mini-rounds are shown to have the same form as

that in the preceding case except for additional load granularity constraints. This allows

the extension of the joint SUPPORT policy for the case of decomposable models to the

current case by rounding down the obtained loads to meet the granularity constraints.

Furthermore, the remaining parameters due to rounding are allocated over devices and

subcarriers proportionally with their rounded loads, thereby reining in the latency expansion

caused by the additional constraints.

The performance gain of the above algorithms and the ï¬ndings are corroborated using experi-

ments with a real dataset.

The reminder of the paper is organized as follows. In Section II, the system model is intro-

duced. In Section III, the total learning latency minimization problem is formulated. In Section IV

and V, the joint SUPPORT designs are proposed for decomposable models and DNN models,

respectively. Section VI presents the experimental results followed by concluding remarks in

Section VII.

A. System Model

II. MODELS AND METRICS

A single cell OFDM system is considered, as illustrated in Fig. 1(a). In the cell, there
are one server equipped with a single-antenna access point (AP) and ğ¾ single-antenna edge

5

(a) PARTEL system

Figure 1. PARTEL system, operations, and latencies.

(b) PARTEL operations and their latencies

devices, serving as workers. Each worker performs one task assigned by the server. The server
is connected to workers via wireless links. The system bandwidth is divided into ğ‘ orthogonal
subcarriers. The bandwidth of each subcarrier is denoted as ğµ. The frequency-selective fading

channel is considered, where different subcarriers will experience different channel gains. We

assume that the AP has the channel state information (CSI) of all links that are useful for

subcarrier allocation. Besides, the channel gains are assumed to be static in one training iteration
but vary over different iterations. The uplink channel gain of worker ğ‘˜ on the subcarrier ğ‘› is
denoted as â„ğ‘˜,ğ‘›. We denote {ğ¶ğ‘˜,ğ‘›} as the subcarrier allocation indicators. If the ğ‘›-th subscriber
is allocated to worker ğ‘˜, then ğ¶ğ‘˜,ğ‘› = 1. Otherwise, ğ¶ğ‘˜,ğ‘› = 0.

Worker 1Worker 2Worker Kâ‹…â‹…â‹…Server  (Access Point)ModelBroadband Channels+++Cascading Local Updates for Updating Global ModelLocal ProcessorAll Subcarriers Push LatencyAll Parameters Update on Parametric Block  kPull LatencySubcarriers for Worker kComp.  LatencyOne Round LatencyServer Worker k6

B. Learning Models

1) Decomposable Models: The large-scale learning tasks with decomposable objective func-

tions (such as logistic regression) can be directly implemented using PARTEL based on the

method of block coordinate descent. According to the literature (e.g., [21]), a decomposable

objective function can be written as

L (w) = F (w) + R (w),

(1)

where w = {ğ‘¤1, ğ‘¤2, ..., ğ‘¤ ğ¿ }ğ‘‡ is the parameter vector of the learning model, ğ¿ is the size of w,
F (w) is the loss function, and R (w) is the regularized function (e.g., ğ¿1 regularization used to
increase sparsity and ğ¿2 regularization used to reduce overï¬tting). Speciï¬cally, the loss function
ğ‘š=1 |ğ‘¦ğ‘š âˆ’ ğœ‘(w; xğ‘š)|2, where ğ‘€ is the size of the dataset, {xğ‘š, ğ‘¦ğ‘š}
can be written as F (w) =
is the ğ‘š-th data sample, and ğœ‘(Â·) is a smooth inference function. The regularized function is a
block-separable function, given as R (w) = (cid:205)ğ¿
ğœ“(ğ‘¤ğ‘–), where ğ‘¤ğ‘– is the ğ‘–-th element of w and
ğ‘–=1
ğœ“(Â·) is the norm (e.g., ğ¿1 or ğ¿2 norm). During the training, the smoothness of R (Â·) decides
the method to update the learning model. If R (Â·) is smooth, gradient descent algorithm is used.

1
ğ‘€

(cid:205)ğ‘€

Otherwise, another method called proximal gradient descent, is used.

2) DNN Models: DNN models cannot be directly implemented using PARTEL, as the nested

layers therein make the gradient elements of different layers dependent. To make PARTEL

feasible and efï¬cient for DNNs, in the sequel, the method of auxiliary variables is used to

decompose the DNN models into many independent subproblems [5], [6].

First, consider a DNN model with ğº hidden layers. The model parameter matrix is denoted
as W with the size of ğ¿ parameters. For an arbitrary layer therein, say layer ğ‘”, the parameter
matrix is denoted as Wğ‘”, the number of neurons is denoted as ğ¼ğ‘”, and the ğ‘–-th neuron parametric

vector is denoted as wğ‘”,ğ‘–. Thereby, the objective function of the DNN model is given by

L (W) =

ğ‘€
âˆ‘ï¸

ğ‘š=1

|ğ‘¦ğ‘š âˆ’ F (xğ‘š; W)|2 ,

(2)

with F (x; W) = ğ‘“ğº+1 (...f2 (f1 (x; W1) ; W2) , ...; Wğº+1) ,
where the model parameter matrix can be expressed as W = [W1, W2, ..., Wğº, Wğº+1], the pa-
rameter matrix of the ğ‘”-th layer can be expressed as Wğ‘” = [wğ‘”,1, wğ‘”,2, ..., wğ‘”,ğ¼ğ‘”], and fğ‘” (cid:0)xğ‘”; Wğ‘”(cid:1)
is the set of output (activation) functions of the ğ‘”-th layer.

Auxiliary Variables: The method of auxiliary variables is used by introducing one aux-
iliary variable per neuron per data sample: ğ‘§ğ‘”,ğ‘–,ğ‘š = ğ‘“ (wğ‘”,ğ‘–; zğ‘”âˆ’1,ğ‘š), âˆ€(ğ‘”, ğ‘–, ğ‘š), where ğ‘“ (Â·)

7

is the ğ‘–-th neuron parametric vector in layer ğ‘”, ğ‘§ğ‘”,ğ‘–,ğ‘š is the
is the activation function, wğ‘”,ğ‘–
auxiliary variable introduced for the ğ‘–-th neuron in layer ğ‘” regarding data sample ğ‘š, zğ‘”âˆ’1,ğ‘š =
[ğ‘§ğ‘”âˆ’1,1,ğ‘š, ğ‘§ğ‘”âˆ’1,2,ğ‘š, ..., ğ‘§ğ‘”âˆ’1,ğ¼ğ‘”âˆ’1,ğ‘š]ğ‘‡ is the auxiliary variable vector for the layer (ğ‘” âˆ’ 1) regarding
data sample ğ‘š, and ğ¼ğ‘”âˆ’1 is the number of neurons in the (ğ‘” âˆ’ 1)-th layer. For an arbitrary
data sample, say the ğ‘š-th, the corresponding auxiliary matrix for the whole model is denoted
as Zğ‘š = [z1,ğ‘š, ..., zğ‘”,ğ‘š, ..., zğº,ğ‘š], called per-sample auxiliary matrix. Then the overall auxiliary
matrix for all samples are denoted as Z = [Z1, ..., Zğ‘š, ...Zğ‘€].

Decomposed Optimization: Following [5], [6], by using the quadratic-penalty method, the

problem in (2) is equivalent to minimizing

LQ(W; Z; ğœ‡) =

ğ‘€
âˆ‘ï¸

ğ‘š=1

(cid:12)
(cid:12)ğ‘¦ğ‘š âˆ’ ğ‘“ğº+1

(cid:0)zğº,ğ‘š; Wğº+1

(cid:1)(cid:12)
2 + ğœ‡
(cid:12)

ğº
âˆ‘ï¸

ğ‘€
âˆ‘ï¸

ğ‘”=1

ğ‘š=1

(cid:13)zğ‘”,ğ‘š âˆ’ fğ‘” (zğ‘”âˆ’1,ğ‘š; Wğ‘”)(cid:13)
(cid:13)
(cid:13)

2 ,

(3)

where z0,ğ‘š = xğ‘š and ğœ‡ â†’ +âˆ. In (3), the nested structure among layers is decoupled. Conse-
quently, the gradients of any two parameters (or auxiliary variables) are independent.

Finally, the problem in (3) can be solved using the alternating optimization over W and Z,

i.e., sequentially solving the W-stage and Z-stage, deï¬ned below, in each training iteration.

â€¢ W-stage: Fixing the values of Z, solve the problem of minW LQ(W; Z; ğœ‡), in which the

problem of each neuron is independent and can be written as
ğ‘€
âˆ‘ï¸

(cid:12)ğ‘§ğ‘”,ğ‘–,ğ‘š âˆ’ ğ‘“ (wğ‘”,ğ‘–; zğ‘”âˆ’1,ğ‘š)(cid:12)
(cid:12)
(cid:12)

2 , âˆ€(ğ‘”, ğ‘–),

min
wğ‘”,ğ‘–

ğ‘š=1

(4)

where wğ‘”,ğ‘– and ğ‘§ğ‘”,ğ‘–,ğ‘š are the parameteric vector and auxiliary variable of the ğ‘–-th neuron in
the ğ‘”-th layer, respectively, zğ‘”âˆ’1,ğ‘š is the auxiliary variable vector of the (ğ‘” âˆ’ 1)-th layer.
Note that one device is allocated a task of updating one or more neuron parametric vectors

by solving the subproblems in (4).

â€¢ Z-stage: Conditioned on the values of W, solve the problem of minZ LQ(W; Z; ğœ‡), where
the problem of optimizing each per-sample auxiliary matrix is independent of others and is

given as

(cid:12)ğ‘¦ğ‘š âˆ’ ğ‘“ğº+1(Wğº+1; zğº,ğ‘š)(cid:12)
(cid:12)
2 + ğœ‡
(cid:12)

ğº
âˆ‘ï¸

(cid:13)zğ‘”,ğ‘š âˆ’ fğ‘” (Wğ‘”; zğ‘”âˆ’1,ğ‘š)(cid:13)
(cid:13)
(cid:13)

2 , âˆ€ğ‘š,

(5)

min
Zğ‘š

ğ‘”=1
where Zğ‘š is the per-sample auxiliary matrix corresponding to data sample ğ‘š. The size of
the per-sample auxiliary matrix is (cid:205)ğº
ğ¼ğ‘” with ğ¼ğ‘” being the number of neurons in layer
ğ‘”=1
ğ‘”. Note that one device is allocated a task of updating one or more per-sample auxiliary

matrices by solving the subproblems in (5).

8

C. PARTEL Architecture

Consider the PARTEL system and operations in Fig. 1, that are elaborated as follows.
1) Decomposable Models: The model-parameter vector is partitioned into ğ¾ disjoint para-
metric blocks, as w = {w1, ..., wğ‘˜ , ..., wğ¾ }, where wğ‘˜ is allocated to worker ğ‘˜ for update, using
a downloaded global dataset from the server1. The communication overhead for the server to

broadcast the dataset is ignored, as its large power and bandwidth are used and the dataset

broadcasting requires only once before the model training. One main beneï¬t of PARTEL is low

learning latency, as each resource-constrained worker is required to calculate and transmit the

gradient or proximal gradient of only a parametric block instead of the whole parameter vector

during each iteration [21].

In the PARTEL framework, one training iteration of the decomposable models is called one

(communication) round. As shown in Fig. 1(b), there are three phases in each round, as follows.

â€¢ Push Phase: The server broadcasts the whole model-parameter vector, w, to all workers.

â€¢ Computation Phase: Each worker computes the update (e.g., gradients or proximal gradients)

of its allocated parametric block.

â€¢ Pull Phase: All workers upload the updates of their corresponding parametric blocks to the

server. The server updates the whole parameter vector.

The training process in Fig. 1(b) iterates when all parametric blocks are updated in the round,

i.e., the tasks of all workers are synchronized in each round.

2) DNN Models: As mentioned, each round of DNN models comprises two stages: W-stage

and Z-stage, described as follows.

â€¢ W-stage: The parameter matrix W is divided into ğ¾ blocks, with each being updated by

one worker. To avoid inter-communication among different workers, the following load-

granularity constraint is applied.

Granularity Constraint 1 (Neuron Allocation for W-stage).

In W-stage, each neuron

parametric vector (e.g., wğ‘”,ğ‘–) deï¬ned in (4) should be allocated to one and only one worker.

1The joint SUPPORT design of this paper can be easily extended to the case of partitioned dataset with multiple groups of

workers (each with a data subset). Each group cooperatively updates a same block. The proposed joint SUPPORT can be applied

in a hierarchical manner: applied for inter-group resource management and also applied for intra-group management.

9

â€¢ Z-stage: The auxiliary matrix Z is divided into ğ¾ blocks, with each being updated by one

worker. To avoid inter-communication among workers, another load-granularity constraint

is applied.

Granularity Constraint 2 (Per-Sample Auxiliary Matrix Allocation for Z-stage).

In Z-

stage, each per-sample auxiliary matrix (e.g., Zğ‘š) deï¬ned in (5) should be allocated to one

and only one worker.

Example 1. Since the number of neurons in a DNN model and the data samples used for

training are large, the sizes of each neuron problem and each per-sample auxiliary matrix problem

are relatively small, compared with the whole learning tasks, making the model partitioning

meaningful. As an example, our experiments involve the DNN model â€œLenet-5â€ proposed in [25]

trained on the MNIST dataset. A mini batch of 50 samples is used in each training iteration. In

â€œLenet-5â€, there are 3 convolutional layers, including 142 feature maps in total. The ï¬rst two

convolutional layers are followed by a pooling layer and the last is followed by a fully connected
layer with 84 neurons. In W-stage, the number of independent subproblems is ğ¼ = 142+84 = 226.

The size of each neuron problem is about

1
ğ¼

=

of each per-sample auxiliary matrix problem is

1
226
1
50

of the whole problem. In Z-stage, the size

of the whole problem.

Each stage (W-stage or Z-stage) comprises three phases, push, computation, and pull, which

are similar to those in the case of decomposable models. The main difference lies in the additional

Granularity Constraint 1 or 2. Each round comprises two stages and the rounds are repeated until

the DNN model converges.

D. Latency and Energy Consumption Models

Consider an arbitrary communication round and an arbitrary worker, say worker ğ‘˜. The latency

and energy consumption models of each phase are described as below.

1) Push Phase: The push latency is the time for the server to broadcast the whole model-

parameter vector to all workers. It is a constant identical for all workers. Besides, as the transmit

power and bandwidth are very large during broadcasting, the push latency can be ignored. In

this step, the energy consumption by all workers is to receive the model-parameter vector from
the server and is included in the circuit energy consumption, denoted as ğœ‰.

2) Computation Phase: The computation latency of worker ğ‘˜ depends on the size of the

allocated parametric block ğ¿ ğ‘˜ and its computation speed ğ‘“ğ‘˜ :

ğ‘‡ cmp
ğ‘˜

=

ğ¿ ğ‘˜
ğ‘“ğ‘˜

,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

(6)

where ğ‘“ğ‘˜ is measured by the number of parameters processed per second.

10

According to [26], the computation power of worker ğ‘˜ is ğ‘ƒcmp
computation power factor. Then, the computation energy of worker ğ‘˜ is

ğ‘˜

= ğ‘”ğ‘˜ ğ‘“ 3

ğ‘˜ , where ğ‘”ğ‘˜ is the

ğ¸ cmp
ğ‘˜

= ğ‘ƒcmp

ğ‘˜ Ã— ğ‘‡ cmp

ğ‘˜

= ğ‘”ğ‘˜ ğ‘“ 2

ğ‘˜ ğ¿ ğ‘˜ ,

1 â‰¤ ğ‘˜ â‰¤ ğ¾.

(7)

3) Pull Phase: The pull phase consists of two parts. One is uploading gradient blocks from

workers to the server. The other is the server updating the global model using the gradients sent

by the workers. For the latter part, there is no energy consumption at the workers. Its latency,
denoted as ğ‘‡s, is a constant and is same for all workers. In the sequel, we ignore the model
update latency, ğ‘‡s, as it is small and has no impact on the solution of latency minimization.
For uploading, worker ğ‘˜ transmits over a set of assigned subcarriers. We denote ğ‘‡ com

ğ‘˜,ğ‘› as the
uploading latency of worker ğ‘˜ on subcarrier ğ‘›. If subcarrier ğ‘› is not allocated to ğ‘˜, i.e., ğ¶ğ‘˜,ğ‘› = 0,
ğ‘‡ com
ğ‘˜,ğ‘› = 0. Otherwise,

ğ‘‡ com
ğ‘˜,ğ‘› =

ğ¿ ğ‘˜,ğ‘›ğœ
ğ‘…ğ‘˜,ğ‘›

, âˆ€ğ¶ğ‘˜,ğ‘› = 1,

(8)

where ğ¿ ğ‘˜,ğ‘› is the number of parameters uploaded by worker ğ‘˜ on subcarrier ğ‘›, ğœ is the number
of bits per gradient element, and ğ‘…ğ‘˜,ğ‘› is the channel capacity of worker ğ‘˜ on subcarrier ğ‘›.
, where ğµ is the

The channel capacity is given by
subcarrier bandwidth, ğœ2 is the power of additive white Gaussian noise, ğ‘ƒcom
ğ‘˜,ğ‘›
power, and â„ğ‘˜,ğ‘› is the channel gain of worker ğ‘˜ on subcarrier ğ‘›, respectively. It follows that

ğ‘˜,ğ‘› â„ğ‘˜,ğ‘›/ğœ2(cid:17)

ğ‘…ğ‘˜,ğ‘› = ğµ log2

is the transmit

1 + ğ‘ƒcom

, âˆ€(ğ‘˜, ğ‘›)

(cid:111)

(cid:110)

(cid:16)

ğ‘ƒcom

ğ‘˜,ğ‘› =

(cid:17)

ğœ2

(cid:16)

2ğ‘…ğ‘˜,ğ‘›/ğµ âˆ’ 1
â„ğ‘˜,ğ‘›

, âˆ€(ğ‘˜, ğ‘›).

Then, the overall uploading latency of worker ğ‘˜ is decided by the slowest subcarrier:

ğ‘‡ com
ğ‘˜

= max
ğ‘›

ğ‘‡ com
ğ‘˜,ğ‘› ,

1 â‰¤ ğ‘˜ â‰¤ ğ¾.

(9)

(10)

The uploading energy consumption of worker ğ‘˜ is modeled as follows. Let ğ¸ com

ğ‘˜,ğ‘› denote the
transmit energy consumption of worker ğ‘˜ on subcarrier ğ‘›. If subcarrier ğ‘› is not allocated, i.e.,
ğ¶ğ‘˜,ğ‘› = 0, ğ¸ com

ğ‘˜,ğ‘› = 0. Otherwise,

ğ¸ com

ğ‘˜,ğ‘› = ğ¶ğ‘˜,ğ‘›ğ‘ƒcom

ğ‘˜,ğ‘› ğ‘‡ com

ğ‘˜,ğ‘› , âˆ€(ğ‘˜, ğ‘›).

(11)

11

By substituting the transmit power density ğ‘ƒcom
ğ‘˜,ğ‘›
ğ¸ com

ğ‘˜,ğ‘› can be further derived as

in (9) and the uploading latency ğ‘‡ com
ğ‘˜,ğ‘›

in (8),

(cid:16)

ğ¶ğ‘˜,ğ‘›

(cid:17)

2ğ‘…ğ‘˜,ğ‘›/ğµ âˆ’ 1
â„ğ‘˜,ğ‘›ğ‘…ğ‘˜,ğ‘›

ğ¸ com

ğ‘˜,ğ‘› =

ğœ2ğ¿ ğ‘˜,ğ‘›ğœ

, âˆ€(ğ‘˜, ğ‘›).

(12)

The total uploading energy consumption of worker ğ‘˜ is the sum of uploading energy consumption

over all subcarriers:

(cid:110)

ğ¸ com
ğ‘˜

= (cid:205)ğ‘

ğ‘›=1

(cid:111)

. By substituting ğ¸ com

ğ‘˜,ğ‘› in (12),

ğ‘˜,ğ‘› , 1 â‰¤ ğ‘˜ â‰¤ ğ¾
ğ¸ com
(cid:16)

(cid:17)

2ğ‘…ğ‘˜,ğ‘›/ğµ âˆ’ 1
â„ğ‘˜,ğ‘›ğ‘…ğ‘˜,ğ‘›

ğ¶ğ‘˜,ğ‘›

ğœ2ğ¿ ğ‘˜,ğ‘›ğœ

ğ¸ com
ğ‘˜

=

ğ‘
âˆ‘ï¸

ğ‘›=1

,

1 â‰¤ ğ‘˜ â‰¤ ğ¾.

(13)

Next, the total latency and energy consumption of worker ğ‘˜ are deï¬ned as follows. The latency

of worker ğ‘˜ is the sum latencies of the two phases:

ğ‘‡ğ‘˜ = ğ‘‡ cmp

ğ‘˜

+ ğ‘‡ com
ğ‘˜

,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

(14)

where ğ‘‡ cmp
(10). The energy consumption of worker ğ‘˜ is given by:

is the computation latency deï¬ned in (6), ğ‘‡ com

ğ‘˜

ğ‘˜

is the uploading latency deï¬ned in

ğ¸ğ‘˜ = ğ¸ cmp

ğ‘˜

+ ğ¸ com
ğ‘˜

+ ğœ‰,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

(15)

where ğœ‰ is the constant circuit energy consumption when there is no computation and transmis-
sion, ğ¸ cmp
consumption of worker ğ‘˜, respectively.

deï¬ned in (13) are the computation and uploading energy

deï¬ned in (7) and ğ¸ com

ğ‘˜

ğ‘˜

III. PROBLEM FORMULATION

We aim at minimizing the overall learning latency of the PARTEL system, which depends on

two factors: the per-round latency and the number of rounds for model convergence. The overall

learning latency is deï¬ned as the total latency of all rounds till model convergence. In [21] for

narrowband channels, it is proved that the overall learning latency minimization is equivalent

to separately minimizing the per-round latency. The result can also apply to the current case of

broadband channels, as stated below.

Lemma 1 (Equivalent Per-Round Latency Minimization [21]). The overall learning latency

minimization is equivalent to separately minimizing the latencies for all rounds.

Lemma 1 holds because the distributed learning algorithms implemented using PARTEL are

equivalent to the corresponding centralized ones in terms of convergence rate as measured by

12

the required number of communication rounds. Speciï¬cally, for distributed learning, the values

of updates (e.g., gradients and proximal gradients) calculated in each round and the number of

rounds required for model convergence are independent of SUPPORT.

Using the result in Lemma 1, we formulate the equivalent per-round latency-minimization
problem. For an arbitrary round, we aim to minimize its latency, denoted as ğ‘‡, under the con-

straints on subcarrier assignment, latency requirement, parameter allocation, and power control,

described as follows.

1) Subcarrier Assignment Constraints: Each subcarrier can be allocated to one worker:

(C1: Subcarrier Assignment Constraint)

ğ¶ğ‘˜,ğ‘› âˆˆ {0, 1}, âˆ€(ğ‘˜, ğ‘›),
ğ¾
âˆ‘ï¸

ğ¶ğ‘˜,ğ‘› = 1,

1 â‰¤ ğ‘› â‰¤ ğ‘,

ğ‘˜=1

(16)

ï£±ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´

ï£³

where ğ¶ğ‘˜,ğ‘› = 1 represents that the subcarrier ğ‘› is allocated to worker ğ‘˜.

2) Per-Round Latency Constraints: As all parametric blocks should be updated in one round,

all workersâ€™ latencies, say {ğ‘‡ğ‘˜ }, should not exceed the overall one-round latency ğ‘‡:

ğ‘‡ğ‘˜ â‰¤ ğ‘‡,

1 â‰¤ ğ‘˜ â‰¤ ğ¾.

(17)

As mentioned, ğ‘‡ is the latency for an arbitrary round and can be different over different rounds.
By substituting ğ‘‡ğ‘˜ in (14), the constraints in (17) can be derived as

ğ‘‡ cmp
ğ‘˜

+ ğ‘‡ com
ğ‘˜

â‰¤ ğ‘‡,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

which, by substituting the uploading latency ğ‘‡ com

ğ‘˜

in (10), are equivalent to

(C2: Per-Round Latency Constraint) ğ‘‡ cmp

ğ‘˜

+ ğ‘‡ com

ğ‘˜,ğ‘› â‰¤ ğ‘‡, âˆ€ğ¶ğ‘˜,ğ‘› = 1,

(18)

(19)

where ğ‘‡ cmp
uploading latency of worker ğ‘˜ on subcarrier ğ‘›.

ğ‘˜

deï¬ned in (6) is the computation latency of worker ğ‘˜ and ğ‘‡ com

ğ‘˜,ğ‘› deï¬ned in (8) is the

3) Parameter Constraints: The parameter constraints are two tiers. On the one hand, the total

updatable number of parameters by all workers should be no smaller than the size of the model:

(C3: Inter-Worker Parameter Constraint)

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğ¿ ğ‘˜ â‰¥ ğ¿,

(20)

where ğ¿ ğ‘˜ is the size of the parametric block allocated to worker ğ‘˜ and ğ¿ is the size of the model-

parameter vector (or matrix). On the other hand, for each worker, the total uploaded number of

parameters on all subcarriers should be no smaller than its allocated parametric-block size:

(C4: Intra-Worker Parameter Constraint)

ğ‘
âˆ‘ï¸

ğ‘›=1

ğ¶ğ‘˜,ğ‘›ğ¿ ğ‘˜,ğ‘› â‰¥ ğ¿ ğ‘˜ ,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

(21)

13

where ğ¿ ğ‘˜,ğ‘› is the number of parameters uploaded by worker ğ‘˜ on subcarrier ğ‘›. In the sequel,
{ğ¿ ğ‘˜ } and {ğ¿ ğ‘˜,ğ‘›} are relaxed to be continuous for simplicity. In practice, the solved {ğ¿âˆ—
{ğ¿âˆ—
be ignored, since the values of {ğ¿ ğ‘˜ } and {ğ¿ ğ‘˜,ğ‘›} are typically large.

ğ‘˜ } and
ğ‘˜,ğ‘›} will be rounded for implementation and the loss caused by the rounding operation can

For the case of DNN models, Granularity Constraints 1 and 2 can be written mathematically

as follows.

(Cdnn: Additional Parameter Constraint for DNN Models)

ğ¿ ğ‘˜
ğ¿sub

âˆˆ N+,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

(22)

where N+ is the set of positive integers and ğ¿sub is the size of the subproblems, i.e., neurons
or per-sample auxiliary matrices. For W-stage, the size of all neurons, say ğ¿sub, are assumed
the same for simplicity, which has little impact on the solution, since the size of each neuron is

much smaller than that of the whole problem, as mentioned in Example 1. For Z-stage, the size
of each per-sample auxiliary matrix is the total number of neurons, say ğ¿sub = (cid:205)ğº
ğ¼ğ‘” with ğ¼ğ‘”
ğ‘”=1
being the number of neurons in layer ğ‘”.

4) Power Constraints: The power consumption of each worker is constrained as

(C5: Power Constraint)

ğ¸ğ‘˜
ğ‘‡ğ‘˜

â‰¤ ğ‘ƒğ‘˜ ,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

(23)

where ğ¸ğ‘˜ deï¬ned in (15), ğ‘‡ğ‘˜ deï¬ned in (14), and ğ‘ƒğ‘˜ are the energy consumption, latency, and
maximal permitted power of worker ğ‘˜, respectively.

5) Latency-Minimization Problem: Under these constraints, the per-round latency-minimization

problem by joint SUPPORT can be formulated as

min
{ğ¶ğ‘˜,ğ‘›},{ğ¿ ğ‘˜ },{ğ¿ ğ‘˜,ğ‘›},{ğ‘…ğ‘˜,ğ‘›},ğ‘‡

ğ‘‡,

(P1)

(24)

s.t. (C1) âˆ¼ (C5), & (Cdnn) for a DNN Model.

IV. JOINT SUPPORT FOR DECOMPOSABLE MODELS

In this section, joint SUPPORT is designed by developing a tractable approach for solving

Problem (P1).

A. Equivalent Latency Requirement

First, the following necessary condition for the equivalent latency requirement can be derived to

simplify Problem (P1). Note that in [21], similar equivalent latency property can be derived in the

PARTEL design for frequency non-selective channels. However, for OFDM systems considered in

14

this paper, the binary subcarrier allocation among workers and the corresponding inter-subcarrier

power and parameter allocation for each device make the problem much more complicated.

Lemma 2 (Equivalent Latency for All Workers). To achieve the optimal solution of (P1), the

following latency condition should be satisï¬ed:

ğ‘‡ cmp
ğ‘˜

+ ğ‘‡ com

ğ‘˜,ğ‘› = ğ‘‡, âˆ€ğ¶ğ‘˜,ğ‘› = 1,

(25)

where ğ‘‡ cmp
uploading latency of worker ğ‘˜ on subcarrier ğ‘›.

ğ‘˜

deï¬ned in (6) is the computation latency of worker ğ‘˜, ğ‘‡ com

ğ‘˜,ğ‘› deï¬ned in (8) is the

Proof: See Appendix A.

The result in Lemma 2 yields the following insights. First, it requires all workers the same
latency with the overall latency ğ‘‡. Second, for each worker, the uploading latency on all allocated

subcarriers should be equal.

Remark 1 (Computation Latency vs. Communication Latency). By substituting the computation
latency ğ‘‡ cmp

in (6) and the communication latency ğ‘‡ com

ğ‘˜,ğ‘› in (8) into the equivalent latency property

ğ‘˜

in (25), it can be derived as

ğ¿ ğ‘˜
ğ‘“ğ‘˜

+

ğ¿ ğ‘˜,ğ‘›ğœ
ğ‘…ğ‘˜,ğ‘›

= ğ‘‡, âˆ€ğ¶ğ‘˜,ğ‘› = 1,

(26)

with the constraints (cid:205)ğ‘
ğ‘›=1

ğ¶ğ‘˜,ğ‘›ğ¿ ğ‘˜,ğ‘› = ğ¿ ğ‘˜ . From (26), the load, say ğ¿ ğ‘˜ , has the same effect on

computation and communication latency. On the other hand, when the computation frequency
ğ‘“ğ‘˜ is small compared to the number of subcarriers and the data rates, the computation latency

dominates or vice versa.

By substituting the computation latency ğ‘‡ cmp

ğ‘˜

deï¬ned in (6) and the uploading latency ğ‘‡ com
ğ‘˜,ğ‘›

deï¬ned in (8) into the necessary condition in Lemma 2, we can derive the number of parameters
uploaded by worker ğ‘˜ on subcarrier ğ‘›, say ğ¿ ğ‘˜,ğ‘›, as

ğ¿ ğ‘˜,ğ‘› =

ğ¶ğ‘˜,ğ‘›ğ‘…ğ‘˜,ğ‘›
ğœ

(cid:18)
ğ‘‡ âˆ’

(cid:19)

ğ¿ ğ‘˜
ğ‘“ğ‘˜

, âˆ€(ğ‘˜, ğ‘›),

(27)

where ğ‘‡ is the per-round latency, ğ¶ğ‘˜,ğ‘› âˆˆ {0, 1} is the subcarrier-allocation indicator, ğ¿ ğ‘˜ is the
parametric-block size allocated to worker ğ‘˜, ğ‘…ğ‘˜,ğ‘› is the channel capacity of ğ‘˜ on subcarrier ğ‘›.

By substituting ğ¿ ğ‘˜,ğ‘› deï¬ned in (27) and the necessary condition in Lemma 2, Problem (P1) can

15

be simpliï¬ed as:

ğ‘‡,

min
{ğ¶ğ‘˜,ğ‘›},{ğ¿ ğ‘˜ },
{ğ‘…ğ‘˜,ğ‘›},ğ‘‡

(P2)

s.t. (C1), (C3),
ğ‘
âˆ‘ï¸

ğ¶ğ‘˜,ğ‘›ğ‘…ğ‘˜,ğ‘›
ğœ

ğ‘›=1

(cid:18)
ğ‘‡ âˆ’

(cid:19)

ğ¿ ğ‘˜
ğ‘“ğ‘˜

â‰¥ ğ¿ ğ‘˜ ,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

(28)

ğ¸ğ‘˜ â‰¤ ğ‘ƒğ‘˜ğ‘‡,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

where ğ¸ğ‘˜ deï¬ned in (15) is the energy consumption of worker ğ‘˜. By substituting ğ¿ ğ‘˜,ğ‘› in (27),
ğ¸ğ‘˜ can be expressed as

ğ¸ğ‘˜ = ğ‘”ğ‘˜ ğ‘“ 2

ğ‘˜ ğ¿ ğ‘˜ +

(cid:16)

ğ¶ğ‘˜,ğ‘›

(cid:17)

ğœ2

2ğ‘…ğ‘˜,ğ‘›/ğµ âˆ’ 1
â„ğ‘˜,ğ‘›

(cid:18)
ğ‘‡ âˆ’

(cid:19)

ğ¿ ğ‘˜
ğ‘“ğ‘˜

+ ğœ‰.

ğ‘
âˆ‘ï¸

ğ‘›=1

(29)

B. Equivalent Convex Problem

Problem (P2) is a mixed integer non-convex problem and is hence NP-hard [27]. In the

sequel, two steps are used to tackle it. First, following the standard approach to tackle integer

programming (see e.g., [22]), linear programming relaxation is used to relax the subcarrier-
allocation indicators in Problem (P2) to be continuous, i.e., (cid:8)ğ¶ğ‘˜,ğ‘› âˆˆ [0, 1], âˆ€(ğ‘˜, ğ‘›)(cid:9). Then,

following the method in [21], the relaxed problem can be equivalently converted to the problem

of updatable model size maximization. However, it remains non-convex and difï¬cult to tackle

due to the intra-worker parameter constraint and the power constraint. In the sequel, the problem

of updatable model size maximization is derived and solved.

Given the one-round latency ğ‘‡ for an arbitrary round, let Ë†ğ¿âˆ—(ğ‘‡) denote the maximum size
of a model that can be updated within the round. Then Ë†ğ¿âˆ—(ğ‘‡) solves the following problem of

16

model size maximization:

Ë†ğ¿âˆ—(ğ‘‡) =

max
{ğ¶ğ‘˜,ğ‘›},{ğ¿ ğ‘˜ },{ğ‘…ğ‘˜,ğ‘›}

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğ¿ ğ‘˜ ,

(P3)

s.t. 0 â‰¤ ğ¶ğ‘˜,ğ‘› â‰¤ 1, âˆ€(ğ‘˜, ğ‘›),

ğ¾
âˆ‘ï¸

ğ‘˜=1
ğ‘
âˆ‘ï¸

ğ‘›=1

ğ¶ğ‘˜,ğ‘› = 1,

1 â‰¤ ğ‘› â‰¤ ğ‘,

(30)

ğ¶ğ‘˜,ğ‘›ğ‘…ğ‘˜,ğ‘›
ğœ

(cid:18)
ğ‘‡ âˆ’

(cid:19)

ğ¿ ğ‘˜
ğ‘“ğ‘˜

â‰¥ ğ¿ ğ‘˜ ,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

ğ¸ğ‘˜ â‰¤ ğ‘ƒğ‘˜ğ‘‡,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

where ğ¶ğ‘˜,ğ‘› is the subcarrier-allocation indicator, ğ¿ ğ‘˜ is the parametric-block size allocated to
worker ğ‘˜, ğ‘…ğ‘˜,ğ‘› is the channel capacity of worker ğ‘˜ on subcarrier ğ‘›, ğ‘‡ is the one-round latency,
ğ¸ğ‘˜ deï¬ned in (29) is the energy consumption of worker ğ‘˜. Note that solving Problem (P2)

via utilizing the problem of model size maximization in Problem (P3) follows the method in

[21]. However, new challenges arise from the subcarrier allocation among workers and the inter-

subcarrier power and parameter allocation for each worker, leading to the non-convexity and a

much larger size of Problem (P3).

Lemma 3 (Relation of Maximal Model Size and Latency).
a monotonously increasing function of ğ‘‡.

Ë†ğ¿âˆ—(ğ‘‡) deï¬ned in Problem (P3) is

Proof: See Appendix B.

It follows from the result in Lemma 3 that the solution of Problem (P2) is the minimal latency,
say ğ‘‡ âˆ—, which makes the updatable model size Ë†ğ¿âˆ—(ğ‘‡ âˆ—) no less than the target size ğ¿. This suggests
a method to solve Problem (P2) by searching ğ‘‡ âˆ— using the criterion Ë†ğ¿âˆ—(ğ‘‡) â‰¥ ğ¿, which will be

elaborated in the later subsection.

To get the maximum updatable model size Ë†ğ¿âˆ—(ğ‘‡) requires solving Problem (P3). To this end,

the following variables are used to transform Problem (P3) into a convex problem.

ğœ‘ğ‘˜ =

(cid:18)
ğ‘‡ âˆ’

(cid:19) âˆ’1

,

ğ¿ ğ‘˜
ğ‘“ğ‘˜

Ëœğ‘…ğ‘˜,ğ‘› = ğ¶ğ‘˜,ğ‘›ğ‘…ğ‘˜,ğ‘›,

ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´
ï£³

(31)

By substituting the variables in (31) and ğ¸ğ‘˜ deï¬ned in (29), Problem (P3) can be written as

17

Ë†ğ¿âˆ—(ğ‘‡) = max

{ğ¶ğ‘˜,ğ‘›},{ğœ‘ğ‘˜ },
{ Ëœğ‘…ğ‘˜,ğ‘›},

ğ¾
âˆ‘ï¸

ğ‘˜=1

(cid:18)
ğ‘‡ âˆ’

ğ‘“ğ‘˜

(cid:19)

,

1
ğœ‘ğ‘˜

(P4)

s.t. 0 â‰¤ ğ¶ğ‘˜,ğ‘› â‰¤ 1, âˆ€(ğ‘˜, ğ‘›),

ğ¾
âˆ‘ï¸

ğ‘˜=1
ğ‘
âˆ‘ï¸

ğ‘›=1

ğ¶ğ‘˜,ğ‘› = 1,

1 â‰¤ ğ‘› â‰¤ ğ‘,

Ëœğ‘…ğ‘˜,ğ‘›
ğœ

â‰¥ ğ‘“ğ‘˜ (ğ‘‡ ğœ‘ğ‘˜ âˆ’ 1) ,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

ğ‘
âˆ‘ï¸

ğ¶ğ‘˜,ğ‘›ğœ2 (cid:0)2

Ëœğ‘…ğ‘˜,ğ‘›
ğµğ¶ğ‘˜,ğ‘› âˆ’ 1(cid:1)

ğ‘›=1

â„ğ‘˜,ğ‘›

+ ğ‘”ğ‘˜ ğ‘“ 3

ğ‘˜ (ğœ‘ğ‘˜ğ‘‡ âˆ’ 1) â‰¤ (ğ‘ƒğ‘˜ğ‘‡ âˆ’ ğœ‰)ğœ‘ğ‘˜ , 1 â‰¤ ğ‘˜ â‰¤ ğ¾.

Lemma 4. Problem (P4) is a convex problem.

Proof: See Appendix C.

C. Properties of Optimal Policies

Based on the results in the previous subsection, the optimal policies of Problem (P2) with

relaxed subcarrier-allocation indicators are proposed, as described in the following.

As (P4) is convex, the primal-dual method can be used to get the optimal solution:

max
{ğœ‡ğ‘›},{ğœ†ğ‘˜ },
{ğœˆğ‘˜ }

min
{ğ¶ğ‘˜,ğ‘›},{ Ëœğ‘…ğ‘˜,ğ‘›},
{ğœ‘ğ‘˜ }

LP4,

(32)

(33)

(cid:35)

Ëœğ‘…ğ‘˜,ğ‘›
ğœ

ğ‘
âˆ‘ï¸

ğ‘›=1

(cid:35)

where LP4 is the Lagrange function of Problem (P4), given as
(cid:34)

(cid:32)

(cid:33)

LP4 = âˆ’

ğœ‡ğ‘›

1 âˆ’

ğ¶ğ‘˜,ğ‘›

+

ğœ†ğ‘˜

ğ‘“ğ‘˜ (ğ‘‡ ğœ‘ğ‘˜ âˆ’ 1) âˆ’

ğ¾
âˆ‘ï¸

ğ‘˜=1
ğ¾
âˆ‘ï¸

ğ‘˜=1

+

ğ‘“ğ‘˜

ğœˆğ‘˜

(cid:18)
ğ‘‡ âˆ’

(cid:19)

+

1
ğœ‘ğ‘˜

ğ‘
âˆ‘ï¸

ğ‘›=1

(cid:18)

Ëœğ‘…ğ‘˜,ğ‘›
ğµğ¶ğ‘˜,ğ‘› âˆ’ 1

2

(cid:19)

Ã—

ğ¶ğ‘˜,ğ‘›

(cid:34) ğ‘
âˆ‘ï¸

ğ‘›=1

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğœ2
â„ğ‘˜,ğ‘›

ğ¾
âˆ‘ï¸

ğ‘˜=1

+ ğ‘”ğ‘˜ ğ‘“ 3

ğ‘˜ (ğ‘‡ ğœ‘ğ‘˜ âˆ’ 1) âˆ’ (ğ‘ƒğ‘˜ğ‘‡ âˆ’ ğœ‰)ğœ‘ğ‘˜

,

and {ğœ‡ğ‘›}, {ğœ†ğ‘˜ â‰¥ 0}, and {ğœˆğ‘˜ â‰¥ 0} are Lagrangian multipliers.

Next, the necessary conditions for achieving the optimal solution of the inner loop are used

to derive the optimal policies. The inner loop problem is given by

min
{ğ¶ğ‘˜,ğ‘›},{ Ëœğ‘…ğ‘˜,ğ‘›},{ğœ‘ğ‘˜ }

LP4,

given {ğœ‡ğ‘›}, {ğœ†ğ‘˜ }, {ğœˆğ‘˜ }.

(34)

18

(35)

(36)

(37)

The ï¬rst necessary condition is

ğœ•LP4
ğœ• Ëœğ‘…ğ‘˜,ğ‘›

= âˆ’

ğœ†ğ‘˜
ğœ

+ ğœˆğ‘˜ 2

Ëœğ‘…ğ‘˜,ğ‘›
ğµğ¶ğ‘˜,ğ‘› ln 2 Ã—

ğœ2
ğµâ„ğ‘˜,ğ‘›

= 0, âˆ€ğ¶ğ‘˜,ğ‘› â‰  0,

which gives the following optimal scheme for calculating the channel capacity:

ğ‘…âˆ—

ğ‘˜,ğ‘› =

Ëœğ‘…âˆ—

ğ‘˜,ğ‘›

ğ¶âˆ—

ğ‘˜,ğ‘›

0,

ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´
ï£³

= ğµ log2

(cid:19)

(cid:18) ğœ†ğ‘˜ ğµ
ğœˆğ‘˜ ğœ ln 2

+ ğµ log2

(cid:18) â„ğ‘˜,ğ‘›
ğœ2

(cid:19)

,

âˆ€ğ¶ğ‘˜,ğ‘› â‰  0,

otherwise.

By substituting ğ‘…âˆ—

ğ‘˜,ğ‘› in (36) into the transmission power in (9), the optimal power-allocation

scheme can be derived, as in the following lemma.

Lemma 5 (Optimal Power Allocation). The optimal power-allocation scheme is
ğœ†ğ‘˜ ğµ
ğœˆğ‘˜ ğœ ln 2

âˆ€ğ¶ğ‘˜,ğ‘› â‰  0,

ğœ2
â„ğ‘˜,ğ‘›

âˆ’

,

ğ‘ƒcom
ğ‘˜,ğ‘›

âˆ— =

0,

otherwise.

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

The water-ï¬lling like result in (37) shows that for each worker, more power should be allocated

on the subcarrier with high channel gain, say â„ğ‘˜,ğ‘›.

The second necessary condition to achieve the optimum of the inner loop problem in (34) is

ğœ•LP4
ğœ•ğœ‘ğ‘˜

= âˆ’

ğ‘“ğ‘˜
ğœ‘2
ğ‘˜

+ ğœ†ğ‘˜ ğ‘“ğ‘˜ğ‘‡ + ğœˆğ‘˜ ğ‘”ğ‘˜ ğ‘“ 3

ğ‘˜ ğ‘‡ âˆ’ ğœˆğ‘˜ (ğ‘ƒğ‘˜ğ‘‡ âˆ’ ğœ‰) = 0.

(38)

By substituting the variable transformations in (31) into (38), we can achieve the optimal inter-

worker parameter allocation scheme, as follows.

Lemma 6 (Optimal Parameter Allocation among Workers). The optimal inter-worker parameter-

allocation scheme is

(cid:20)
ğ‘‡ âˆ’

ğ¿âˆ—

ğ‘˜ =

âˆšï¸ƒ

ğœ†ğ‘˜ğ‘‡ + ğœˆğ‘˜ ğ‘”ğ‘˜ ğ‘“ 2

ğ‘˜ ğ‘‡ âˆ’ ğœˆğ‘˜ (ğ‘ƒğ‘˜ğ‘‡ âˆ’ ğœ‰)/ ğ‘“ğ‘˜

(cid:21)

ğ‘“ğ‘˜ ,

1 â‰¤ ğ‘˜ â‰¤ ğ¾.

(39)

In (39), the optimal parametric-block size allocated to worker ğ‘˜, say ğ¿âˆ—

ğ‘˜ , is a concave function
of the computation speed ğ‘“ğ‘˜ and a monotone decreasing function of the computation power factor
ğ‘”ğ‘˜ . On one hand, large ğ‘“ğ‘˜ can reduce the computation latency. On the other hand, the computation
energy increases as a square function of ğ‘“ğ‘˜ . The optimal load in (39) balances the two aspects.

Substituting the parameter-allocation scheme in (39) and the channel capacity in (36) into the

intra-worker parameter-allocation scheme {ğ¿ ğ‘˜,ğ‘›} in (27), gives the following lemma.

Lemma 7 (Optimal Parameter Allocation Among Subcarriers). The optimal intra-worker pa-

rameter allocation scheme is given by

19

âˆšï¸ƒ

ğœ†ğ‘˜ğ‘‡ + ğœˆğ‘˜ ğ‘”ğ‘˜ ğ‘“ 2

ğ‘˜ ğ‘‡ âˆ’ ğœˆğ‘˜ (ğ‘ƒğ‘˜ğ‘‡ âˆ’ ğœ‰)/ ğ‘“ğ‘˜

ğœ

0,

ğ¿âˆ—

ğ‘˜,ğ‘› =

ï£±ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´

ï£³

Ã— ğµ log2

(cid:18) ğœ†ğ‘˜ ğµâ„ğ‘˜,ğ‘›
ğœˆğ‘˜ ğœğœ2 ln 2

(cid:19)

,

if ğ¶ğ‘˜,ğ‘› â‰  0,

otherwise.

(40)

From (40), more parameters should be assigned to the channel with high gain.

The third necessary condition to achieve the optimum of the inner loop problem in (34) is

ğœ•LP4
ğœ•ğ¶ğ‘˜,ğ‘›

= âˆ’ğœ‡ğ‘› + ğ¼ğ‘˜,ğ‘› = 0, âˆ€(ğ‘˜, ğ‘›),

where ğ¼ğ‘˜,ğ‘› is the indicator function given by

ğ¼ğ‘˜,ğ‘› =

(cid:34)

(cid:16)

ğœˆğ‘˜ ğœ2
â„ğ‘˜,ğ‘›

ğ‘…âˆ—
ğ‘˜,ğ‘›/ğµ âˆ’ 1

2

(cid:17)

âˆ’

ğ‘…âˆ—

ğ‘˜,ğ‘›2

ğ‘…âˆ—
ğ‘˜,ğ‘›/ğµ

(cid:35)

ln 2

ğµ

, âˆ€(ğ‘˜, ğ‘›).

(41)

(42)

ğ¼ğ‘˜,ğ‘›. If ğ¼ğ‘˜,ğ‘› > ğœ‡ğ‘›, ğ¶ğ‘˜,ğ‘› = 0,
Note that ğ¼ğ‘˜,ğ‘› is determined when ğ‘…âˆ—
as the condition in (41) can not be satisï¬ed. If ğ¼ğ‘˜,ğ‘› = ğœ‡ğ‘› for a unique worker, say ğ‘˜, then
ğ¶ğ‘˜,ğ‘› = 1. If ğ¼ğ‘˜,ğ‘› = ğœ‡ğ‘› for multiple workers, then ğ¶ğ‘˜,ğ‘› âˆˆ (0, 1) for these workers. And in the
last case, it is easy to show that the values of the non-zero {ğ¶ğ‘˜,ğ‘›} wonâ€™t inï¬‚uence the value of

ğ‘˜,ğ‘› is known. Let ğœ‡ğ‘› = min
ğ‘˜

the Lagrange function LP4 deï¬ned in (33), as long as the subcarrier assignment constraint, say
(cid:8)(cid:205)ğ‘

1 â‰¤ ğ‘› â‰¤ ğ‘ (cid:9), are satisï¬ed.

ğ¶ğ‘˜,ğ‘› = 1,

ğ‘›=1
The optimal subcarrier allocation is summarized in the following lemma.

Lemma 8 (Optimal Subcarrier Allocation). The optimal subcarrier allocation is given as:

ğ¶âˆ—

ğ‘˜,ğ‘›

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´

ï£³

= 0,

if ğ¼ğ‘˜,ğ‘› > ğœ‡ğ‘›,

âˆˆ (0, 1),

if ğ¼ğ‘˜,ğ‘› = ğœ‡ğ‘› for multiple workers,

(43)

= 1,

if ğ¼ğ‘˜,ğ‘› = ğœ‡ğ‘› for a unique workerWğ‘˜ ,

where ğ¼ğ‘˜,ğ‘› is the indicator function deï¬ned in (42), ğœ‡ğ‘› = min

ğ¼ğ‘˜,ğ‘›, and ğ‘…âˆ—

ğ‘˜,ğ‘› is the optimal

ğ‘˜

channel capacity in (36).

In (43), a high channel gain leads to a small value of ğ¼ğ‘˜,ğ‘› and thus a high possibility to
make ğ¶ğ‘˜,ğ‘› â‰  0. That means the subcarrier with higher channel gain has larger possibility to be

allocated to the worker. Note that in the optimal scheme in Lemma 8, some subcarrier-allocation

indicators may be fractions. The standard approach is to round those to be binary (see, e.g.,

[22]), which will be elaborated in the later subsection.

20

Algorithm 1 Updatable Model Size Maximization
1: Input: channel gains {â„ğ‘˜,ğ‘›}, computation speeds { ğ‘“ğ‘˜ }, computation power factors, {ğ‘”ğ‘˜ }, and
the given one-round latency ğ‘‡.
2: Initialize {ğœ†(0)

ğ‘˜ }, and ğ‘– = 0.

ğ‘˜ }, {ğœˆ(0)

3: Loop

4:

Update the multipliers as

(cid:26)

(cid:26)

ğœ†(ğ‘–)
ğ‘˜ + ğœ‚ğœ†ğ‘˜

ğœˆ(ğ‘–)
ğ‘˜ + ğœ‚ğœˆğ‘˜

ğœ•LP4
ğœ•ğœ†ğ‘˜
ğœ•LP4
ğœ•ğœˆğ‘˜

,

,

(cid:27)

0

(cid:27)

0

, 1 â‰¤ ğ‘˜ â‰¤ ğ¾,

, 1 â‰¤ ğ‘˜ â‰¤ ğ¾,

ğ‘˜,ğ‘›} using (39), (36), and (43), respectively.

= max

ğœ†(ğ‘–+1)
ğ‘˜

ï£±ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´
ğ‘˜,ğ‘›}, and {ğ¶âˆ—

ğœˆ(ğ‘–+1)
ğ‘˜

ï£³

= max

ğ‘˜,ğ‘›} with (31).

5:

6:

Solve {ğ¿âˆ—
Get {ğœ‘âˆ—

ğ‘˜ }, {ğ‘…âˆ—
ğ‘˜ } and { Ëœğ‘…âˆ—
7: Until Convergence
8: Ë†ğ¿âˆ—(ğ‘‡) = (cid:205)ğ¾
9: Output: Ë†ğ¿âˆ—(ğ‘‡), {ğ¿âˆ—

ğ¿âˆ—
ğ‘˜ .

ğ‘˜=1

ğ‘˜ }, {ğ‘…âˆ—

ğ‘˜,ğ‘›}, and {ğ¶âˆ—

ğ‘˜,ğ‘›}.

D. Optimal Policy Computation

In this subsection, the joint SUPPORT algorithm to solve the original Problem (P1) is proposed.

First, we solve the convex Problem (P4) by the primal-dual method using the closed-form results
in Lemmas 6-8. Some notation is described as follows. {ğœ‚ğœ†ğ‘˜ } and {ğœ‚ğœˆğ‘˜ } denote the step sizes of
gradient descent. LP4 and ğœ‡, {ğœ†ğ‘˜ â‰¥ 0}, and {ğœˆğ‘˜ â‰¥ 0} are the Lagrange function and Lagrangian
multipliers deï¬ned in (33). With the notation, the application of the primal-dual method yields

Algorithm 1 for solving Problem (P4).

Remark 2 (Low Complexity of Updatable Model Size Maximization). The computation com-
plexity of Algorithm 1 is O (ğ¾ 2ğ‘) with ğ¾ being the number of workers and ğ‘ being the number

of subcarriers, as the closed-form results in Lemmas 6 - 8 makes the updating of corresponding

variables more efï¬cient. As a comparison, directly solving the non-convex Problem (P3) has a
computational complexity of at least O (ğ¾ 3ğ‘ 3) and is suboptimal.

Then, as mentioned in the preceding subsection, Problem (P2) with relaxed subcarrier-allocation
indicators can be solved by nesting a one-dimensional search over the latency ğ‘‡ and solving
the convex Problem (P4). Based on the monotonicity of Ë†ğ¿âˆ—(ğ‘‡) in Lemma 3, the search can be

21

Algorithm 2 Joint SUPPORT
1: Input: channel gains {â„ğ‘˜,ğ‘›}, computation speeds { ğ‘“ğ‘˜ }, and computation power factors, {ğ‘”ğ‘˜ }.
2: Select ğ‘‡ = ğ‘‡u that makes Ë†ğ¿âˆ—(ğ‘‡u) deï¬ned in Problem (P4) larger than ğ¿.
3: Select ğ‘‡ = ğ‘‡l that makes Ë†ğ¿âˆ—(ğ‘‡l) < ğ¿.
4: While ğ‘‡u â‰  ğ‘‡l
5:

Let ğ‘‡m = (ğ‘‡u + ğ‘‡l)/2.
Input {â„ğ‘˜,ğ‘›}, { ğ‘“ğ‘˜ }, {ğ‘”ğ‘˜ } and ğ‘‡ = ğ‘‡m into Algorithm 1 to solve (P4).
Obtain Ë†ğ¿âˆ—(ğ‘‡m), {ğ¿âˆ—
If Ë†ğ¿âˆ—(ğ‘‡m) â‰¥ ğ¿
ğ‘‡u = ğ‘‡m.

ğ‘˜,ğ‘›}, and {ğ¶âˆ—

ğ‘˜ }, {ğ‘…âˆ—

ğ‘˜,ğ‘›}.

6:

7:

8:

9:

10: Else

11:

ğ‘‡l = ğ‘‡m.

12: End if

13:End while
14:ğ‘‡ âˆ— = ğ‘‡m.
15:Output: ğ‘‡ âˆ—, {ğ¿âˆ—

ğ‘˜ }, {ğ‘…âˆ—

ğ‘˜,ğ‘›}, and {ğ¶âˆ—

ğ‘˜,ğ‘›}.

efï¬ciently implemented by bisection method. While the solution of Problem (P4) is presented in

Algorithm 1. Then the optimal policy to solve Problem (P2) with relaxed subcarrier-allocation

indicators is presented in Algorithm 2, by nesting the bisection search and Algorithm 1.

Finally, based on Algorithm 2, the joint scheme of SUPPORT without relaxation is proposed

to solve the original Problem (P1). Note that not all subcarrier-allocation indicators solved by
ğ‘˜,ğ‘› âˆˆ (0, 1) for some (ğ‘˜, ğ‘›). For these subcarriers, a practical
Algorithm 2 are integers, i.e., ğ¶âˆ—

subcarrier-allocation scheme following [22] is determined as

ğ¶âˆ—
ğ‘˜1,ğ‘› = 1,

ğ‘˜1 = arg max

ğ‘˜

ğ¿âˆ—
ğ‘˜,ğ‘›,

1 â‰¤ ğ‘› â‰¤ ğ‘,

(44)

where the subcarrier is allocated to the worker with the largest value. Then, given the subcarrier-
allocation scheme {ğ¶âˆ—

ğ‘˜,ğ‘›}, the latency-minimization problem is a special case of Problem (P1),

whose solution can also be solved by Algorithm 2.

22

V. JOINT SUPPORT FOR DNN MODELS

In this section, DNN models are considered. Since Problem (P1) is not tractable in this case

with the additional constraint (Cdnn), we propose an approximate solution method that leverages

the result for decomposable model case, described as follows.

1) For both W-stage and Z-stage, solve the joint scheme of SUPPORT using the method in

Section IV without considering Granularity Constraints 1 and 2.

2) Given the subcarrier-allocation scheme, round the parameter allocation for each worker to

satisfy Granularity Constraint 1 for W-stage and Granularity Constraint 2 for Z-stage.

The challenges lie in Step 2) and are two-fold. On one hand, how should the rounding indicator

be designed to minimize the rounding loss. On the other hand, as each workerâ€™s number of

parameters changes, the corresponding channel-capacity (or power) allocation and intra-worker

parameter allocation among the assigned subcarriers should be redesigned. To tackle these

challenges, in the sequel, we ï¬rst propose a joint scheme of SUPPORT for DNN models. Then,

the rounding scheme is designed accordingly and the resultant latency increase is analyzed.

1) Joint SUPPORT for DNN Models: Denote the solved one-round latency as ğ‘‡ âˆ—, the subcarrier-

allocation policy as {ğ¶âˆ—
worker ğ‘˜ as ğ¿âˆ—

ğ‘˜,ğ‘›}, the spectrum efï¬ciencies as {ğ‘…âˆ—

ğ‘˜,ğ‘›}, the number of parameters of

ğ‘˜ , the number of parameters uploaded by worker ğ‘˜ on subcarrier ğ‘› as ğ¿âˆ—

ğ‘˜,ğ‘›.

Consider an arbitrary worker, say worker ğ‘˜. If its number of parameters is rounded down

to satisfy (Cdnn), the reduced number of parameters is denoted as Î”ğ¿d
ğ‘˜ â‰¥ 0. If its number of
parameters is rounded up, the additional number of parameters to be uploaded is denoted as
Î”ğ¿u
ğ‘˜ â‰¥ 0. Note that if worker ğ‘˜â€™s number of parameters is rounded down, no inï¬‚uence is caused
to the one-round latency. Hence, only the case of being rounded up is considered in the sequel.

Our aim is to design rounding scheme to minimize the resulted additional one-round latency.

Next, the joint scheme of SUPPORT is designed as

(Joint SUPPORT for DNN Models)

ğ¶ğ‘˜,ğ‘› = ğ¶âˆ—

ğ‘˜,ğ‘›, ğ‘…ğ‘˜,ğ‘› = ğ‘…âˆ—

ğ‘˜,ğ‘›,

Î”ğ¿ ğ‘˜,ğ‘› = ğ¿âˆ—

ğ‘˜,ğ‘› Ã—

Î”ğ¿u
ğ‘˜
ğ¿âˆ—
ğ‘˜

,

(45)

ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´
ï£³

where Î”ğ¿ ğ‘˜,ğ‘› is the number of additional parameters allocated to subcarrier ğ‘› for uploading,
which is proportional to its currently uploaded number of parameters ğ¿âˆ—
ğ‘˜,ğ‘›. In (45), the allocation
of subcarriers {ğ¶ğ‘˜,ğ‘›} and the channel capacities {ğ‘…ğ‘˜,ğ‘›} of the assigned subcarriers remain the

same. Two concerns motivate us to design the joint SUPPORT scheme as (45). First, the assigned

23

subcarrier that can currently upload more updates of parameters can upload more additional pa-

rameters in the same additional latency. Second, the proportional additional parameter allocation

together with the unchanged allocation of subcarriers and channel capacities can yield a simple

upper bound of the additional latency for each worker, as shown in the following lemma.

Lemma 9 (Additional Latency). Consider an arbitrary worker, say worker ğ‘˜, the design in (45)

results in an upper bound of the minimum additional latency:

Î”ğ‘‡ğ‘˜ â‰¤ ğ‘‡ âˆ— Ã—

,

Î”ğ¿u
ğ‘˜
ğ¿âˆ—
ğ‘˜
ğ‘˜ , and ğ¿âˆ—

(46)

where ğ‘‡ âˆ— is the solved latency in Step 1), Î”ğ‘‡ğ‘˜ , Î”ğ¿u

ğ‘˜ are the additional latency, the number

of additional parameters after the rounding operation, and the solved number of parameters in
Step 1) of worker ğ‘˜, respectively.

The proof of Lemma 9 is straightforward and hence omitted. Two observations can be made

from Lemma 9. On one hand, as mentioned in Example 1, the size of the subproblems are far
smaller than the problems of W-stage and Z-stage, i.e., Î”ğ¿u
ğ‘˜ . Therefore, the additional
latency Î”ğ‘‡ğ‘˜ is small for all workers. On the other hand, the round-up indicator, denoted as ğ¼ğ‘˜ ,
should be the ratio ğ¼ğ‘˜ =

ğ‘˜ (cid:28) ğ¿âˆ—

.

Î”ğ¿u
ğ‘˜
ğ¿âˆ—
ğ‘˜

2) Parameter Rounding Scheme: Note that Lemma 9 only gives the additional latency for

one worker. To minimize the additional one-round latency, the rounding scheme is designed to
make the workers with least ğ¼ğ‘˜ to round up and the others to round down, described as follows.

1) Sort the round-up indicators {ğ¼ğ‘˜ } from the least to the biggest and the new permutation

is indexed by ğ‘˜ (cid:48)
2) Find the least ğ¾ (cid:48)

, i.e., {ğ¼ğ‘˜ (cid:48) } is sorted from the least to the largest.
1 following the new permutation {ğ¼ğ‘˜ (cid:48) }, which satisï¬es

ğ¾ (cid:48)
1âˆ‘ï¸

ğ‘˜ (cid:48)=1

Î”ğ¿u

ğ‘˜ (cid:48) â‰¥

ğ¾
âˆ‘ï¸

ğ‘˜ (cid:48)=ğ¾1+1

ğ‘˜ (cid:48) ,
Î”ğ¿d

(47)

where ğ¿u
and Î”ğ¿d
by rounding up ğ¾ (cid:48)

ğ‘˜ (cid:48) is the additional number of parameters of worker ğ‘˜ (cid:48)
ğ‘˜ (cid:48) is the reduced number of parameters when being rounded down. (47) means that
1 workers with least round-up indicators, the parameters of all workers

when being rounded up

can satisfy Granularity Constraints 1 and 2.

3) The additional one-round latency is Î”ğ‘‡ â‰¤ ğ‘‡ âˆ— Ã— ğ¼ğ¾ (cid:48)

1

, where ğ‘‡ âˆ— is the solved one round

latency without considering Granularity Constraints 1 and 2.

24

A. Experiment Setup

VI. EXPERIMENTAL RESULTS

The experimental settings are speciï¬ed as follows unless speciï¬ed otherwise. In the OFDM
based PARTEL system, there are ğ¾ workers and ğ‘ subcarriers. The bandwidth of each subcarrier
is ğµ = 312.5 kHz. The subcarrier channel gains {ğ»ğ‘˜,ğ‘›} are assumed to be i.i.d. Rayleigh fading
with the average path loss of 10âˆ’3. The noise power density is set as 10âˆ’9 W/Hz. The workersâ€™
computation speeds { ğ‘“ğ‘˜ } are uniformly selected from the set {0.1, 0.2, ..., 1.0} Ã— 106 parameters

processed per second in one local computation iteration. The corresponding computation power
factors {ğ‘”ğ‘˜ } are uniformly selected from the set {0.1, 0.2, ..., 1.0} Ã— 10âˆ’16. The maximum power
consumed by workers {ğ‘ƒğ‘˜ } is set as 8 W. Both decomposable models and DNN models are

trained using the PARTEL framework. The learning settings are as follows.

â€¢ Decomposable Model: A ğ¿1-regularized logistic regression task is considered, which trains a
news-ï¬ltering model using the News20 dataset collected in [28]. The model size is 1.24Ã—106.
The training and test datasets have 15936 and 3993 samples respectively. ğ¾ = 50 workers
with ğ‘ = 80 subcarriers are used to complete the task.

â€¢ DNN Model: The CNN model â€œLeNet-5â€ proposed in [25] is trained on the MNIST dataset.
In the â€œLeNet-5â€ model, there are are 60, 000 parameters in total. The method of auxiliary

variables in [6] is used to train the â€œLeNet-5â€ model at the PARTEL framework. In each
training iteration, a mini batch of 50 data samples is used. There are 469, 400 auxiliary
variables in total. ğ¾ = 30 workers with ğ‘ = 50 subcarriers are used to complete the task.

For comparison, three communication schemes are considered, described as follows.

â€¢ Joint SUPPORT: The joint schemes of SUPPORT proposed in Sections IV or V.

â€¢ Baseline: The number of parameters computed by each worker is ï¬rst allocated proportional

to their computation capacity. Then, the subcarriers are allocated, which is a special case

of the joint SUPPORT scheme.

â€¢ Greedy Scheme for FEEL: The training samples are equally distributed among workers.

Thereby, the computation latency and energy of each worker is determined. The subcarrier

allocation follows a greedy way. The subcarriers are randomly indexed and sequentially
allocated from the 1st to the ğ‘-th. The ğ‘–-th subcarrier is allocated to the worker whose

latency is currently the longest. Note that the latency minimization of one worker given the

subcarrier allocation is simple and omitted.

25

(a) Training accuracy versus latency.

(b) Test accuracy versus latency.

Figure 2. Learning performance versus (communication-plus-computation) latency.

B. Decomposable Models

The learning performance of training the logistic regression model is compared in Fig. 2. As

observed, the model trained in PARTEL with the proposed joint SUPPORT converges much

faster than the one trained in FEEL with the greedy communication scheme, in which each

worker uploads the updates of all parameters. Besides, the joint SUPPORT outperforms the

baseline in terms of model convergence with a latency reduction of 31.06% on average. Thatâ€™s

because the allocations of parameters and subcarriers are sequentially designed in the baseline.

Fig. 3 shows the impacts of number of workers and subcarriers on the per-round latency.

As observed, the per-round latencies of both schemes decrease as the number of workers or

subcarriers increases. The reasons are as follows. More workers can provide more computation

capacity and hence reduce the computation latency. Moreover, more subcarriers allocated to

workers can reduce the uploading latency.

C. DNN Models

The learning performance of training the LeNet-5 is compared in Fig. 4. In the ï¬gure, for

FEEL, the optimizer is Adam (proposed in [29]) and the corresponding learning rate is 0.002. For

PARTEL, the optimizer is SGD. The learning rates for updating weights and auxiliary variables

are 0.002 and 1, respectively. Although two stages (rounds) complete one training iteration in

PARTEL using the joint scheme, it outperforms the FEEL using the greedy scheme in terms

0100200300400500600Latency (s)0.50.550.60.650.70.750.80.850.90.951Training AccuracyPARTEL, Joint SUPPORTPARTEL, BaselineFEEL0100200300400500600Latency (s)0.750.760.770.780.790.80.810.820.830.84Testing AccuracyPARTEL, Joint SUPPORTPARTEL, BaselineFEEL26

(a) Effect of Number of Workers.

(b) Effect of Number of Subcarriers.

Figure 3. Latency performance versus (a) a varying number of workers and (b) subcarriers.

(a) Training accuracy versus latency.

(b) Test accuracy versus latency.

Figure 4. Learning performance versus (communication-plus-computation) latency.

of model convergence, as the latter has to upload the updates of all parameters in each round.

Besides, the joint SUPPORT can reduce latency by 42.11% compared to the baseline for the

similar reason in the decomposable model case.

The impacts of the number of workers and subcarriers on the latency performance of training

LeNet-5 is compared in Fig. 5. As shown in the ï¬gure, the latencies of the two schemes for

both W-stage and Z-stage decrease with the number of workers and subcarriers for the same

reasons in the case of decomposable models.

38404244464850Number of Workers0.750.80.850.90.9511.051.11.151.21.251.31.35Averaged One-Round Latency (s)Joint SUPPORTBaseline68707274767880Number of Subcarriers0.750.80.850.90.9511.051.11.151.21.251.31.35Averaged One-Round Latency (s)Joint SUPPORTBaseline050100150200250Latency (s)0.20.30.40.50.60.70.80.91Training AccuracyPARTEL, Joint SUPPORTPARTEL, BaselineFEEL050100150200250Latency (s)0.20.30.40.50.60.70.80.91Testing AccuracyPARTEL, Joint SUPPORTPARTEL, BaselineFEEL27

(a) Effect of Number of Workers.

(b) Effect of Number of Subcarriers.

Figure 5. Latency performance versus (a) a varying number of workers and (b) subcarriers.

The experimental results above show that our proposed joint scheme of SUPPORT has the

best performance regarding learning latency and veriï¬es our analysis.

VII. CONCLUDING REMARKS

In this paper, we have presented a set of algorithms for jointly controlling parameter and sub-

carrier allocation and power control, which can signiï¬cantly reduce the latency of PARTEL

deployed in a broadband system. This work opens several interesting directions. One is to

take the device scheduling into consideration for further accelerating the training process. In

the context of PARTEL, it is useful to jointly design the scheduler and parameter allocation.

Another interesting direction is to jointly control parameter allocation and local computation (e.g.,

processor speeds). In addition, the current joint design assuming OFDM can be extended to other

advanced communication techniques such as non-orthogonal multi-access, massive MIMO, and

over-the-air aggregation.

A. Proof of Lemma 2

APPENDIX

KKT conditions are used to show Lemma 2. The Lagrange function of Problem (P1) is in

(48), where {ğœ‡ğ‘›}, ğœ† â‰¥ 0, {ğœˆğ‘˜ â‰¥ 0}, {ğ›¼ğ‘˜ â‰¥ 0}, and {ğ›½ğ‘˜,ğ‘› â‰¥ 0} are multipliers.

202224262830Number of Workers00.10.20.30.40.50.60.70.80.911.11.21.31.4Averaged One-Round Latency (s)Joint SUPPORT, W-StageBaseline, W-StageJoint SUPPORT, Z-StageBaseline, Z-Stage404244464850Number of Subcarriers00.10.20.30.40.50.60.70.80.911.11.2Averaged One-Round Latency (s)Joint SUPPORT, W-StageBaseline, W-StageJoint SUPPORT, Z-StageBaseline, Z-Stageğ‘
âˆ‘ï¸

ğ‘›=1

ğ›¼ğ‘˜

L =ğ‘‡ +

+

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğ‘˜=1
(cid:19)

âˆ’ ğ‘ƒğ‘˜

+

(cid:18) ğ¸ğ‘˜
ğ‘‡ğ‘˜

(cid:32)

ğœ‡ğ‘›

1 âˆ’

ğ¾
âˆ‘ï¸

(cid:33)

(cid:32)

ğ¶ğ‘˜,ğ‘›

+ ğœ†

ğ¿ âˆ’

(cid:33)

ğ¿ ğ‘˜

+

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğ¾
âˆ‘ï¸

ğ‘˜=1

(cid:32)

ğœˆğ‘˜

ğ¿ ğ‘˜ âˆ’

ğ‘
âˆ‘ï¸

ğ‘›=1

(cid:33)

ğ¶ğ‘˜,ğ‘›ğ¿ ğ‘˜,ğ‘›

ğ¾
âˆ‘ï¸

ğ‘
âˆ‘ï¸

ğ‘˜=1

ğ‘›=1

ğ›½ğ‘˜,ğ‘›ğ¶ğ‘˜,ğ‘›

(cid:16)
ğ‘‡ cmp
ğ‘˜

+ ğ‘‡ com

ğ‘˜,ğ‘› âˆ’ ğ‘‡

(cid:17)

,

28

(48)

Then, consider an arbitrary subcarrier-allocation scheme {ğ¶ğ‘˜,ğ‘›}, KKT conditions are necessary

to solve the problem. Some related KKT conditions are given below:

ğœ•L
ğœ•ğ‘‡

= 1 âˆ’ ğ¶ğ‘˜,ğ‘› ğ›½ğ‘˜,ğ‘› = 0,

1 â‰¤ ğ‘˜ â‰¤ ğ¾,

ğ›½ğ‘˜,ğ‘›ğ¶ğ‘˜,ğ‘›

(cid:16)
ğ‘‡ cmp
ğ‘˜

+ ğ‘‡ com

ğ‘˜,ğ‘› âˆ’ ğ‘‡

(cid:17)

= 0, âˆ€(ğ‘˜, ğ‘›),

(49)

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

From the ï¬rst condition in (49), we can show that {ğ›½ğ‘˜,ğ‘› â‰  0, âˆ€ğ¶ğ‘˜,ğ‘› = 1}, which, together
with the second condition in (49), can show that {ğ‘‡ cmp
ğ‘˜,ğ‘› = ğ‘‡, âˆ€ğ¶ğ‘˜,ğ‘› = 1}. Note that the
above condition is necessary for arbitrary subcarrier-allocation schemes. Hence, it is a necessary

+ ğ‘‡ com

ğ‘˜

condition to solve (P1).

B. Proof of Lemma 3

First, we show that the equality of the third and forth constraints in Problem (P3) should be

achieved. The Lagrange function of (P3) is

ğ¾
âˆ‘ï¸

ğ‘
âˆ‘ï¸

ğœ‡ğ‘›

(cid:32) ğ¾
âˆ‘ï¸

ğ¿ ğ‘˜ +

L =

(cid:33)

âˆ‘ï¸ ğœ†ğ‘˜

+

(cid:34)

ğ¿ ğ‘˜ âˆ’

ğ¶ğ‘˜,ğ‘› âˆ’ 1

ğ‘˜=1
ğ‘›=1
ğ‘˜=1
âˆ‘ï¸ ğœˆğ‘˜ (ğ¸ğ‘˜ âˆ’ ğ‘ƒğ‘˜ğ‘‡) ,

+

ğ¶ğ‘˜,ğ‘›ğ‘…ğ‘˜,ğ‘›
ğœ

(cid:18)
ğ‘‡ âˆ’

ğ¿ ğ‘˜
ğ‘“ğ‘˜

(cid:19) (cid:35)

ğ‘
âˆ‘ï¸

ğ‘›=1

(50)

where {ğœ‡ğ‘›}, {ğœ†ğ‘˜ â‰¥ 0}, and {ğœˆğ‘˜ â‰¥ 0} are multipliers. Using KKT conditions and the similar
approaches in Appendix A, we can show that {ğœ†ğ‘˜ â‰  0, 1 â‰¤ ğ‘˜ â‰¤ ğ¾ } and {ğœˆğ‘˜ â‰  0 1 â‰¤ ğ‘˜ â‰¤ ğ¾ }

{ğ‘…âˆ—

and the equalities of the third and forth constraints in Problem (P3) should be achieved.
Then, consider ğ‘‡1 < ğ‘‡2. When ğ‘‡ = ğ‘‡1, denote the optimal solution of (P3) as {ğ¶âˆ—
ğ‘˜,ğ‘›,1}, and the maximum updatable model size as ğ¿âˆ—(ğ‘‡1).
Next, let ğ‘‡ = ğ‘‡2, {ğ¶ğ‘˜,ğ‘›,2 = ğ¶âˆ—

ğ‘˜,1 into
the third and forth conditions in Problem (P3), the equalities are not achieved. This shows that
the updatable number of parameters by each worker, denoted as {ğ¿ ğ‘˜,2}, can be larger, i.e.,
ğ‘˜,1. It follows that (cid:205)ğ¾
ğ‘˜,1 = ğ¿âˆ—(ğ‘‡1). Furthermore, the optimal solution
ğ¿âˆ—
ğ¿ ğ‘˜,2 > ğ¿âˆ—
ğ‘˜=1
for ğ‘‡ = ğ‘‡2 satisï¬es ğ¿âˆ—(ğ‘‡2) â‰¥ (cid:205)ğ¾
ğ‘˜=1

ğ¿ ğ‘˜,2 > (cid:205)ğ¾
ğ‘˜=1
ğ¿ ğ‘˜,2. Hence, we have ğ¿âˆ—(ğ‘‡2) > ğ¿âˆ—(ğ‘‡1).

ğ‘˜,ğ‘›,1}. By substituting ğ¿ ğ‘˜,1 = ğ¿âˆ—

ğ‘˜,ğ‘›,1}, and {ğ‘…ğ‘˜,ğ‘›,2 = ğ‘…âˆ—

ğ‘˜,ğ‘›,1}, {ğ¿âˆ—

ğ‘˜,1},

C. Proof of Lemma 4

First, the third constraint in Problem (P3), by dividing (ğ‘‡ âˆ’ ğ¿ ğ‘˜ / ğ‘“ğ‘˜ ) on both sides and sub-

29

stituting the variable transformations in (31), can be derived as the third constraint in Problem
Ëœğ‘…ğ‘˜,ğ‘›/ğœ â‰¥ ğ‘“ğ‘˜ (ğ‘‡ ğœ‘ğ‘˜ âˆ’ 1) , 1 â‰¤ ğ‘˜ â‰¤ ğ¾(cid:9). Obviously, the feasible region of the above
(P4): (cid:8)(cid:205)ğ‘
ğ‘›=1
constraint is a convex set. Then, by substituting ğ¸ğ‘˜ in (29), dividing

on both sides,

(cid:18)
ğ‘‡ âˆ’

(cid:19)

ğ¿ ğ‘˜
ğ‘“ğ‘˜

and substituting the variable transformations in (31), the forth constraint in Problem (P3) can be

equally derived as the forth constraint in Problem (P4):

ğ‘
âˆ‘ï¸

ğ¶ğ‘˜,ğ‘›ğœ2 (cid:0)2

Ëœğ‘…ğ‘˜,ğ‘›
ğµğ¶ğ‘˜,ğ‘› âˆ’ 1(cid:1)

ğ‘›=1

â„ğ‘˜,ğ‘›

+ ğ‘”ğ‘˜ ğ‘“ 3

ğ‘˜ (ğœ‘ğ‘˜ğ‘‡ âˆ’ 1) â‰¤ (ğ‘ƒğ‘˜ğ‘‡ âˆ’ ğœ‰)ğœ‘ğ‘˜ , 1 â‰¤ ğ‘˜ â‰¤ ğ¾.

(51)

In (51), the ï¬rst term is a convex function as ğ‘“ (ğ‘¥, ğ‘¦) = ğ‘¥ğ‘’ğ‘¦/ğ‘¥ is convex. Thereby, the feasible

region of the constraint in (51) is a convex set. Besides, the objective function and other

constraints are convex. Thus, Problem (P4) is convex.

REFERENCES

[1] J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis, S.-L. Kim, and M. Debbah, â€œCommunication-efï¬cient and distributed

learning over wireless networks: Principles and applications,â€ [online]. Available: https://arxiv.org/abs/2008.02608, 2020.

[2] M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G. Andersen, and A. Smola, â€œParameter server for distributed machine learning,â€

in Proc. of NIPS Workshop on Big Learning, (Lake Tahoe, USA), Dec. 2013.

[3] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, et al., â€œCommunication-efï¬cient learning of deep networks from

decentralized data,â€ arXiv preprint arXiv:1602.05629, 2016.

[4] J. KoneË‡cn`y, H. B. McMahan, F. X. Yu, P. RichtÂ´arik, A. T. Suresh, and D. Bacon, â€œFederated learning: Strategies for

improving communication efï¬ciency,â€ arXiv preprint arXiv:1610.05492, 2016.

[5] M. Carreira-Perpinan and W. Wang, â€œDistributed optimization of deeply nested systems,â€ in Proc. Int. Workshop on Artif.

Intell. and Statist. (AISTATS), (Reykjavik, Iceland), April 2014.

[6] A. Choromanska, B. Cowen, S. Kumaravel, R. Luss, M. Rigotti, I. Rish, P. Diachille, V. Gurev, B. Kingsbury, R. Tejwani,

et al., â€œBeyond backprop: Online alternating minimization with auxiliary variables,â€ in Proc. Int. Conf. Mach. Learn.

(ICML), pp. 1193â€“1202, 2019.

[7] G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, â€œToward an intelligent edge: Wireless communication meets

machine learning,â€ IEEE Commun. Magazine, vol. 58, pp. 19â€“25, Jan. 2020.

[8] G. Zhu, Y. Wang, and K. Huang, â€œBroadband analog aggregation for low-latency federated edge learning,â€ IEEE Trans.

Wireless Commun., vol. 19, pp. 491â€“506, Oct. 2019.

[9] M. M. Amiri and D. GÂ¨undÂ¨uz, â€œMachine learning at the wireless edge: Distributed stochastic gradient descent over-the-air,â€

IEEE Trans. on Signal Process., vol. 68, pp. 2155â€“2169, 2020.

[10] K. Yang, T. Jiang, Y. Shi, and Z. Ding, â€œFederated learning via over-the-air computation,â€ to appear in IEEE Trans.

Wireless Commun., 2020.

[11] Y. Du, S. Yang, and K. Huang, â€œHigh-dimensional stochastic gradient quantization for communication-efï¬cient edge

learning,â€ IEEE Trans. on Signal Process., vol. 68, pp. 2128â€“2142, 2020.

30

[12] E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S.-L. Kim, â€œCommunication-efï¬cient on-device machine learning:

Federated distillation and augmentation under non-iid private data,â€ [online]. Available: https://arxiv.org/abs/1811.11479,

2018.

[13] H. H. Yang, Z. Liu, T. Q. S. Quek, and H. V. Poor, â€œScheduling policies for federated learning in wireless networks,â€

IEEE Trans. Commun., vol. 68, no. 1, pp. 317â€“333, 2020.

[14] J. Ren, Y. He, D. Wen, G. Yu, K. Huang, and D. Guo, â€œScheduling in cellular federated edge learning with importance

and channel awareness,â€ to appear in IEEE Trans. Wireless Commun., 2020.

[15] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, â€œA joint learning and communications framework for federated

learning over wireless networks,â€ [online]. Available: https://arxiv.org/pdf/1909.07972.pdf, 2019.

[16] W. Shi, S. Zhou, and Z. Niu, â€œDevice scheduling with fast convergence for wireless federated learning,â€ [online]. Available:

https://arxiv.org/pdf/1911.00856.pdf, 2019.

[17] J. Ren, G. Yu, and G. Ding, â€œAccelerating DNN training in wireless federated edge learning system,â€ [online]. Available:

https://arxiv.org/pdf/1905.09712.pdf, 2019.

[18] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, â€œEnergy efï¬cient federated learning over wireless

communication networks,â€ [online]. Available: https://arxiv.org/pdf/1911.02417.pdf, 2019.

[19] Q. Zeng, Y. Du, K. Huang, and K. K. Leung, â€œEnergy-efï¬cient resource management for federated edge learning with

cpu-gpu heterogeneous computing,â€ [online]. Available: https://arxiv.org/abs/2007.07122.pdf, 2020.

[20] X. Mo and J. Xu, â€œEnergy-efï¬cient federated edge learning with joint communication and computation design,â€ [online].

Available: https://arxiv.org/abs/2003.00199.pdf, 2020.

[21] D. Wen, M. Bennis, and K. Huang, â€œJoint parameter-and-bandwidth allocation for improving the efï¬ciency of partitioned

edge learning,â€ to appear in IEEE Trans. Wireless Commun., 2020.

[22] C. Y. Wong, R. S. Cheng, K. B. Lataief, and R. D. Murch, â€œMultiuser OFDM with adaptive subcarrier, bit, and power

allocation,â€ IEEE J. Sel. Areas Commun., vol. 17, no. 10, pp. 1747â€“1758, 1999.

[23] D. W. K. Ng, E. S. Lo, and R. Schober, â€œEnergy-efï¬cient resource allocation in multi-cell OFDMA systems with limited

backhaul capacity,â€ IEEE Trans. Wireless Commun., vol. 11, no. 10, pp. 3618â€“3631, 2012.

[24] J. Jang and K. B. Lee, â€œTransmit power adaptation for multiuser OFDM systems,â€ IEEE J. Sel. Areas Commun., vol. 21,

no. 2, pp. 171â€“178, 2003.

[25] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, â€œGradient-based learning applied to document recognition,â€ Proceedings

of the IEEE, vol. 86, no. 11, pp. 2278â€“2324, 1998.

[26] C. You, K. Huang, H. Chae, and B.-H. Kim, â€œEnergy-efï¬cient resource allocation for mobile-edge computation ofï¬‚oading,â€

IEEE Trans. Wireless Commun., vol. 16, no. 3, pp. 1397â€“1411, 2016.

[27] S. Burer and A. N. Letchford, â€œNon-convex mixed-integer nonlinear programming: A survey,â€ Surveys in Operations

Research and Management Science, vol. 17, no. 2, pp. 97â€“106, 2012.

[28] K. Lang, â€œNewsweeder: Learning to ï¬lter netnews,â€ in Mach. Learn. Proc. 1995, pp. 331â€“339, Elsevier, 1995.

[29] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€ in Int. Conf. on Learn. Repr. (ICLR), (San Diego,

USA), May 2015.

