1

Adaptive Subcarrier, Parameter, and Power

Allocation for Partitioned Edge Learning Over

Broadband Channels

Dingzhu Wen, Ki-Jun Jeon, Mehdi Bennis, and Kaibin Huang

Abstract

In this paper, we consider partitioned edge learning (PARTEL), which implements parameter-

server training, a well known distributed learning method, in a wireless network. Thereby, PARTEL

leverages distributed computation resources at edge devices to train a large-scale artiﬁcial intelligence

(AI) model by dynamically partitioning the model into parametric blocks for separated updating at

devices. Targeting broadband channels, we consider the joint control of parameter allocation, sub-

channel allocation, and transmission power to improve the performance of PARTEL. Speciﬁcally, the

policies for joint SUbcarrier, Parameter, and POweR allocaTion (SUPPORT) are optimized under the

criterion of minimum learning latency. Two cases are considered. First, for the case of decomposable

models (e.g., logistic regression), the latency-minimization problem is a mixed-integer program and non-

convex. Due to its intractability, we develop a practical solution by integer relaxation and transforming

it into an equivalent convex problem of model size maximization under a latency constraint. Thereby,

a low-complexity algorithm is designed to compute the SUPPORT policy. Second, consider the case

of deep neural network (DNN) models which can be trained using PARTEL by introducing some

auxiliary variables. This, however, introduces constraints on model partitioning reducing the granularity

of parameter allocation. The preceding policy is extended to DNN models by applying the proposed

techniques of load rounding and proportional adjustment to rein in latency expansion caused by the

load granularity constraints.

I. INTRODUCTION

Edge machine learning is an area concerning the deployment of learning algorithms at the

network edge to gain low-latency access to data and computation resources distributed at a

D. Wen and K. Huang are with The University of Hong Kong, Hong Kong. K.-J. Jeon is with the LG Electronics, Korea.

M. Bennis is with University of Oulu, Finland. Corresponding email: huangkb@eee.hku.hk.

1
2
0
2

r
a

M
8
1

]
T
I
.
s
c
[

2
v
1
6
0
4
0
.
0
1
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

large number of edge devices [1]. In this work, we study the efﬁcient implementation of the

well-known method of parameter-server training [2] in a broadband system (e.g., 3GPP 5G)

to exploit distributed computation resources at many devices to scale up model training. To

this end, several key operations, namely computation-load allocation (via model partitioning),

sub-channel allocation, and power control, are jointly designed under the criterion of minimum

learning latency.

Two main methods for distributed learning are federated learning [3], [4] and parameter-server

training [2], [5], [6], which are designed for different scenarios and features. The key feature

of federated learning is its preservation of data privacy. Based on distributed implementation

of stochastic gradient descent (SGD), federated learning iterates the separate training of a

downloaded model at multiple devices using their local data, and the uploading and aggregation

of local models (or local stochastic gradients) to yield a more accurate global model [3], [4]. The

avoidance of direct data uploading protects their privacy. Though it is similar to federated learning

in implementing distributed SGD, the parameter-server training, which is of our interest, has one

distinction. Its purpose is to scale up learning using many resource-constrained machines in a

closed network where data privacy is not a concern [2]. To this end, the model is partitioned to

allow each device to train only a part of the model instead of the whole as in federated learning.

This overcomes the resource constraints of devices and reduces their energy consumption.

Moreover, training data are downloaded from a server to devices at the beginning of each round,

avoiding their need of persistent storage space.

A current main theme in the ﬁeld of edge learning is the design of wireless techniques

to support efﬁcient deployment of federated learning, resulting in an area called federated

edge learning (FEEL) [7]. The effort on overcoming the communication bottleneck of FEEL

has led to the design of a new class of multi-access techniques realizing over-the-air model

aggregation[8]–[12] and radio resource management (RRM) techniques [13]–[17]. Moreover,

researchers have designed energy efﬁcient RRM techniques to tackle the challenge of executing

a complex learning task at energy constrained devices in a FEEL system[18]–[20]. Recently,

researchers have also explored the efﬁcient implementation of parameter-server training over

wireless channels, resulting in a framework called partitioned edge learning (PARTEL) [21]. Let

parameter allocation refers to the system operation that to balance computation loads, the server

divides the model into parametric blocks of variable lengths and allocate them to devices for

separate training. To reduce the learning latency, the technique of joint parameter allocation and

3

resource allocation is proposed in [21], which jointly adapts parameter and bandwidth allocation

to devices’ channel states and computation capacities. For simplicity, the prior work assumes

narrowband channels, for which the management of uplink radio resource reduces to bandwidth

allocation. In this work, we design low-latency PARTEL techniques for a broadband system (e.g.,

3GPP 5G) with frequency selective channels. In this case, the frequency resource is managed via

sub-channel allocation, which is much more complex than bandwidth allocation. The complexity

arises from the fact that the sub-channels of each device have different gains and devices

have different channel realizations. Consequently, even if the allocated bandwidths are ﬁxed,

reshufﬂing the assignments of sub-channels varies devices’ communication rates. Therefore,

jointly designing sub-channel and parameter allocation poses a new challenge that cannot be

tackled using the solution in [21].

The optimal sub-channel allocation is well known to be an integer optimization problem

that is NP hard [22]–[24]. For the conventional multiuser communication systems, the classic

approximate-solution approach has been established in a series of work for the purpose of

minimizing sum power under users’ rate constraints [22]–[24]. The essential idea is to relax the

integer program and discover the embedded convexity in the relaxed problem to design a practical

algorithm [22]. In this work, we build on the classic approach to develop a new solution for the

problem of latency minimization in a broadband PARTEL system. The distinction of the current

work is the pursuit of a communication-learning integration approach so as to minmize learning

latency in the context of PAETEL. To this end, we jointly design parameter and sub-channel

allocations. The considerations of synchronized updates by devices, which is a requirement for

parameter-server training [2], and devices’ heterogeneous computation capacities introduce more

challenges. Existing designs that aim at generic radio access cannot tackle the new challenges,

which motivate the current work.

It is also worth mentioning that we also consider a more complex model based on a deep

neural network (DNN) besides the decomposable model as in [21]. Unlike the latter, the former

is not directly decomposable and requires the modiﬁcation of learning algorithm to support

PARTEL. This introduces additional complexity to the current design.

By tackling the above challenges, we design a set of algorithms for joint SUbcarrier, Parameter,

POweR allocaTion (SUPPORT), termed joint SUPPORT. The main contributions of this work

are summarized as follows.

• Joint SUPPORT for Decomposable Models: Consider the case of a decomposable model.

4

The problem of latency minimization by joint SUPPORT is an integer program and in-

tractable. A practical solution approach is developed using two techniques. The ﬁrst is a

relaxation of binary subcarrier assignment decisions. The second is the transformation of

the relaxed problem into a convex problem of model size maximization under a latency con-

straint, which is nested in a simple search for the target model size. Considering the convex

problem, the properties of three optimal resource-management operations are analyzed and

then applied to design an efﬁcient algorithm for computing the desired SUPPORT policy.

Via analysis, it is found that the optimal number of parameters assigned to a worker for

updating avoids high power consumption due to overloading. For this reason, the optimal

number is derived to be a concave function of its speed and a monotonic decreasing function

of its computation power factor. On the other hand, the optimal subcarrier assignment and

power allocation over assigned subcarriers are found to favor high channel gains.

• Joint SUPPORT for DNN Models: Consider the case of a DNN model. The optimization

problems for joint SUPPORT in both mini-rounds are shown to have the same form as

that in the preceding case except for additional load granularity constraints. This allows

the extension of the joint SUPPORT policy for the case of decomposable models to the

current case by rounding down the obtained loads to meet the granularity constraints.

Furthermore, the remaining parameters due to rounding are allocated over devices and

subcarriers proportionally with their rounded loads, thereby reining in the latency expansion

caused by the additional constraints.

The performance gain of the above algorithms and the ﬁndings are corroborated using experi-

ments with a real dataset.

The reminder of the paper is organized as follows. In Section II, the system model is intro-

duced. In Section III, the total learning latency minimization problem is formulated. In Section IV

and V, the joint SUPPORT designs are proposed for decomposable models and DNN models,

respectively. Section VI presents the experimental results followed by concluding remarks in

Section VII.

A. System Model

II. MODELS AND METRICS

A single cell OFDM system is considered, as illustrated in Fig. 1(a). In the cell, there
are one server equipped with a single-antenna access point (AP) and 𝐾 single-antenna edge

5

(a) PARTEL system

Figure 1. PARTEL system, operations, and latencies.

(b) PARTEL operations and their latencies

devices, serving as workers. Each worker performs one task assigned by the server. The server
is connected to workers via wireless links. The system bandwidth is divided into 𝑁 orthogonal
subcarriers. The bandwidth of each subcarrier is denoted as 𝐵. The frequency-selective fading

channel is considered, where different subcarriers will experience different channel gains. We

assume that the AP has the channel state information (CSI) of all links that are useful for

subcarrier allocation. Besides, the channel gains are assumed to be static in one training iteration
but vary over different iterations. The uplink channel gain of worker 𝑘 on the subcarrier 𝑛 is
denoted as ℎ𝑘,𝑛. We denote {𝐶𝑘,𝑛} as the subcarrier allocation indicators. If the 𝑛-th subscriber
is allocated to worker 𝑘, then 𝐶𝑘,𝑛 = 1. Otherwise, 𝐶𝑘,𝑛 = 0.

Worker 1Worker 2Worker K⋅⋅⋅Server  (Access Point)ModelBroadband Channels+++Cascading Local Updates for Updating Global ModelLocal ProcessorAll Subcarriers Push LatencyAll Parameters Update on Parametric Block  kPull LatencySubcarriers for Worker kComp.  LatencyOne Round LatencyServer Worker k6

B. Learning Models

1) Decomposable Models: The large-scale learning tasks with decomposable objective func-

tions (such as logistic regression) can be directly implemented using PARTEL based on the

method of block coordinate descent. According to the literature (e.g., [21]), a decomposable

objective function can be written as

L (w) = F (w) + R (w),

(1)

where w = {𝑤1, 𝑤2, ..., 𝑤 𝐿 }𝑇 is the parameter vector of the learning model, 𝐿 is the size of w,
F (w) is the loss function, and R (w) is the regularized function (e.g., 𝐿1 regularization used to
increase sparsity and 𝐿2 regularization used to reduce overﬁtting). Speciﬁcally, the loss function
𝑚=1 |𝑦𝑚 − 𝜑(w; x𝑚)|2, where 𝑀 is the size of the dataset, {x𝑚, 𝑦𝑚}
can be written as F (w) =
is the 𝑚-th data sample, and 𝜑(·) is a smooth inference function. The regularized function is a
block-separable function, given as R (w) = (cid:205)𝐿
𝜓(𝑤𝑖), where 𝑤𝑖 is the 𝑖-th element of w and
𝑖=1
𝜓(·) is the norm (e.g., 𝐿1 or 𝐿2 norm). During the training, the smoothness of R (·) decides
the method to update the learning model. If R (·) is smooth, gradient descent algorithm is used.

1
𝑀

(cid:205)𝑀

Otherwise, another method called proximal gradient descent, is used.

2) DNN Models: DNN models cannot be directly implemented using PARTEL, as the nested

layers therein make the gradient elements of different layers dependent. To make PARTEL

feasible and efﬁcient for DNNs, in the sequel, the method of auxiliary variables is used to

decompose the DNN models into many independent subproblems [5], [6].

First, consider a DNN model with 𝐺 hidden layers. The model parameter matrix is denoted
as W with the size of 𝐿 parameters. For an arbitrary layer therein, say layer 𝑔, the parameter
matrix is denoted as W𝑔, the number of neurons is denoted as 𝐼𝑔, and the 𝑖-th neuron parametric

vector is denoted as w𝑔,𝑖. Thereby, the objective function of the DNN model is given by

L (W) =

𝑀
∑︁

𝑚=1

|𝑦𝑚 − F (x𝑚; W)|2 ,

(2)

with F (x; W) = 𝑓𝐺+1 (...f2 (f1 (x; W1) ; W2) , ...; W𝐺+1) ,
where the model parameter matrix can be expressed as W = [W1, W2, ..., W𝐺, W𝐺+1], the pa-
rameter matrix of the 𝑔-th layer can be expressed as W𝑔 = [w𝑔,1, w𝑔,2, ..., w𝑔,𝐼𝑔], and f𝑔 (cid:0)x𝑔; W𝑔(cid:1)
is the set of output (activation) functions of the 𝑔-th layer.

Auxiliary Variables: The method of auxiliary variables is used by introducing one aux-
iliary variable per neuron per data sample: 𝑧𝑔,𝑖,𝑚 = 𝑓 (w𝑔,𝑖; z𝑔−1,𝑚), ∀(𝑔, 𝑖, 𝑚), where 𝑓 (·)

7

is the 𝑖-th neuron parametric vector in layer 𝑔, 𝑧𝑔,𝑖,𝑚 is the
is the activation function, w𝑔,𝑖
auxiliary variable introduced for the 𝑖-th neuron in layer 𝑔 regarding data sample 𝑚, z𝑔−1,𝑚 =
[𝑧𝑔−1,1,𝑚, 𝑧𝑔−1,2,𝑚, ..., 𝑧𝑔−1,𝐼𝑔−1,𝑚]𝑇 is the auxiliary variable vector for the layer (𝑔 − 1) regarding
data sample 𝑚, and 𝐼𝑔−1 is the number of neurons in the (𝑔 − 1)-th layer. For an arbitrary
data sample, say the 𝑚-th, the corresponding auxiliary matrix for the whole model is denoted
as Z𝑚 = [z1,𝑚, ..., z𝑔,𝑚, ..., z𝐺,𝑚], called per-sample auxiliary matrix. Then the overall auxiliary
matrix for all samples are denoted as Z = [Z1, ..., Z𝑚, ...Z𝑀].

Decomposed Optimization: Following [5], [6], by using the quadratic-penalty method, the

problem in (2) is equivalent to minimizing

LQ(W; Z; 𝜇) =

𝑀
∑︁

𝑚=1

(cid:12)
(cid:12)𝑦𝑚 − 𝑓𝐺+1

(cid:0)z𝐺,𝑚; W𝐺+1

(cid:1)(cid:12)
2 + 𝜇
(cid:12)

𝐺
∑︁

𝑀
∑︁

𝑔=1

𝑚=1

(cid:13)z𝑔,𝑚 − f𝑔 (z𝑔−1,𝑚; W𝑔)(cid:13)
(cid:13)
(cid:13)

2 ,

(3)

where z0,𝑚 = x𝑚 and 𝜇 → +∞. In (3), the nested structure among layers is decoupled. Conse-
quently, the gradients of any two parameters (or auxiliary variables) are independent.

Finally, the problem in (3) can be solved using the alternating optimization over W and Z,

i.e., sequentially solving the W-stage and Z-stage, deﬁned below, in each training iteration.

• W-stage: Fixing the values of Z, solve the problem of minW LQ(W; Z; 𝜇), in which the

problem of each neuron is independent and can be written as
𝑀
∑︁

(cid:12)𝑧𝑔,𝑖,𝑚 − 𝑓 (w𝑔,𝑖; z𝑔−1,𝑚)(cid:12)
(cid:12)
(cid:12)

2 , ∀(𝑔, 𝑖),

min
w𝑔,𝑖

𝑚=1

(4)

where w𝑔,𝑖 and 𝑧𝑔,𝑖,𝑚 are the parameteric vector and auxiliary variable of the 𝑖-th neuron in
the 𝑔-th layer, respectively, z𝑔−1,𝑚 is the auxiliary variable vector of the (𝑔 − 1)-th layer.
Note that one device is allocated a task of updating one or more neuron parametric vectors

by solving the subproblems in (4).

• Z-stage: Conditioned on the values of W, solve the problem of minZ LQ(W; Z; 𝜇), where
the problem of optimizing each per-sample auxiliary matrix is independent of others and is

given as

(cid:12)𝑦𝑚 − 𝑓𝐺+1(W𝐺+1; z𝐺,𝑚)(cid:12)
(cid:12)
2 + 𝜇
(cid:12)

𝐺
∑︁

(cid:13)z𝑔,𝑚 − f𝑔 (W𝑔; z𝑔−1,𝑚)(cid:13)
(cid:13)
(cid:13)

2 , ∀𝑚,

(5)

min
Z𝑚

𝑔=1
where Z𝑚 is the per-sample auxiliary matrix corresponding to data sample 𝑚. The size of
the per-sample auxiliary matrix is (cid:205)𝐺
𝐼𝑔 with 𝐼𝑔 being the number of neurons in layer
𝑔=1
𝑔. Note that one device is allocated a task of updating one or more per-sample auxiliary

matrices by solving the subproblems in (5).

8

C. PARTEL Architecture

Consider the PARTEL system and operations in Fig. 1, that are elaborated as follows.
1) Decomposable Models: The model-parameter vector is partitioned into 𝐾 disjoint para-
metric blocks, as w = {w1, ..., w𝑘 , ..., w𝐾 }, where w𝑘 is allocated to worker 𝑘 for update, using
a downloaded global dataset from the server1. The communication overhead for the server to

broadcast the dataset is ignored, as its large power and bandwidth are used and the dataset

broadcasting requires only once before the model training. One main beneﬁt of PARTEL is low

learning latency, as each resource-constrained worker is required to calculate and transmit the

gradient or proximal gradient of only a parametric block instead of the whole parameter vector

during each iteration [21].

In the PARTEL framework, one training iteration of the decomposable models is called one

(communication) round. As shown in Fig. 1(b), there are three phases in each round, as follows.

• Push Phase: The server broadcasts the whole model-parameter vector, w, to all workers.

• Computation Phase: Each worker computes the update (e.g., gradients or proximal gradients)

of its allocated parametric block.

• Pull Phase: All workers upload the updates of their corresponding parametric blocks to the

server. The server updates the whole parameter vector.

The training process in Fig. 1(b) iterates when all parametric blocks are updated in the round,

i.e., the tasks of all workers are synchronized in each round.

2) DNN Models: As mentioned, each round of DNN models comprises two stages: W-stage

and Z-stage, described as follows.

• W-stage: The parameter matrix W is divided into 𝐾 blocks, with each being updated by

one worker. To avoid inter-communication among different workers, the following load-

granularity constraint is applied.

Granularity Constraint 1 (Neuron Allocation for W-stage).

In W-stage, each neuron

parametric vector (e.g., w𝑔,𝑖) deﬁned in (4) should be allocated to one and only one worker.

1The joint SUPPORT design of this paper can be easily extended to the case of partitioned dataset with multiple groups of

workers (each with a data subset). Each group cooperatively updates a same block. The proposed joint SUPPORT can be applied

in a hierarchical manner: applied for inter-group resource management and also applied for intra-group management.

9

• Z-stage: The auxiliary matrix Z is divided into 𝐾 blocks, with each being updated by one

worker. To avoid inter-communication among workers, another load-granularity constraint

is applied.

Granularity Constraint 2 (Per-Sample Auxiliary Matrix Allocation for Z-stage).

In Z-

stage, each per-sample auxiliary matrix (e.g., Z𝑚) deﬁned in (5) should be allocated to one

and only one worker.

Example 1. Since the number of neurons in a DNN model and the data samples used for

training are large, the sizes of each neuron problem and each per-sample auxiliary matrix problem

are relatively small, compared with the whole learning tasks, making the model partitioning

meaningful. As an example, our experiments involve the DNN model “Lenet-5” proposed in [25]

trained on the MNIST dataset. A mini batch of 50 samples is used in each training iteration. In

“Lenet-5”, there are 3 convolutional layers, including 142 feature maps in total. The ﬁrst two

convolutional layers are followed by a pooling layer and the last is followed by a fully connected
layer with 84 neurons. In W-stage, the number of independent subproblems is 𝐼 = 142+84 = 226.

The size of each neuron problem is about

1
𝐼

=

of each per-sample auxiliary matrix problem is

1
226
1
50

of the whole problem. In Z-stage, the size

of the whole problem.

Each stage (W-stage or Z-stage) comprises three phases, push, computation, and pull, which

are similar to those in the case of decomposable models. The main difference lies in the additional

Granularity Constraint 1 or 2. Each round comprises two stages and the rounds are repeated until

the DNN model converges.

D. Latency and Energy Consumption Models

Consider an arbitrary communication round and an arbitrary worker, say worker 𝑘. The latency

and energy consumption models of each phase are described as below.

1) Push Phase: The push latency is the time for the server to broadcast the whole model-

parameter vector to all workers. It is a constant identical for all workers. Besides, as the transmit

power and bandwidth are very large during broadcasting, the push latency can be ignored. In

this step, the energy consumption by all workers is to receive the model-parameter vector from
the server and is included in the circuit energy consumption, denoted as 𝜉.

2) Computation Phase: The computation latency of worker 𝑘 depends on the size of the

allocated parametric block 𝐿 𝑘 and its computation speed 𝑓𝑘 :

𝑇 cmp
𝑘

=

𝐿 𝑘
𝑓𝑘

,

1 ≤ 𝑘 ≤ 𝐾,

(6)

where 𝑓𝑘 is measured by the number of parameters processed per second.

10

According to [26], the computation power of worker 𝑘 is 𝑃cmp
computation power factor. Then, the computation energy of worker 𝑘 is

𝑘

= 𝑔𝑘 𝑓 3

𝑘 , where 𝑔𝑘 is the

𝐸 cmp
𝑘

= 𝑃cmp

𝑘 × 𝑇 cmp

𝑘

= 𝑔𝑘 𝑓 2

𝑘 𝐿 𝑘 ,

1 ≤ 𝑘 ≤ 𝐾.

(7)

3) Pull Phase: The pull phase consists of two parts. One is uploading gradient blocks from

workers to the server. The other is the server updating the global model using the gradients sent

by the workers. For the latter part, there is no energy consumption at the workers. Its latency,
denoted as 𝑇s, is a constant and is same for all workers. In the sequel, we ignore the model
update latency, 𝑇s, as it is small and has no impact on the solution of latency minimization.
For uploading, worker 𝑘 transmits over a set of assigned subcarriers. We denote 𝑇 com

𝑘,𝑛 as the
uploading latency of worker 𝑘 on subcarrier 𝑛. If subcarrier 𝑛 is not allocated to 𝑘, i.e., 𝐶𝑘,𝑛 = 0,
𝑇 com
𝑘,𝑛 = 0. Otherwise,

𝑇 com
𝑘,𝑛 =

𝐿 𝑘,𝑛𝜏
𝑅𝑘,𝑛

, ∀𝐶𝑘,𝑛 = 1,

(8)

where 𝐿 𝑘,𝑛 is the number of parameters uploaded by worker 𝑘 on subcarrier 𝑛, 𝜏 is the number
of bits per gradient element, and 𝑅𝑘,𝑛 is the channel capacity of worker 𝑘 on subcarrier 𝑛.
, where 𝐵 is the

The channel capacity is given by
subcarrier bandwidth, 𝜎2 is the power of additive white Gaussian noise, 𝑃com
𝑘,𝑛
power, and ℎ𝑘,𝑛 is the channel gain of worker 𝑘 on subcarrier 𝑛, respectively. It follows that

𝑘,𝑛 ℎ𝑘,𝑛/𝜎2(cid:17)

𝑅𝑘,𝑛 = 𝐵 log2

is the transmit

1 + 𝑃com

, ∀(𝑘, 𝑛)

(cid:111)

(cid:110)

(cid:16)

𝑃com

𝑘,𝑛 =

(cid:17)

𝜎2

(cid:16)

2𝑅𝑘,𝑛/𝐵 − 1
ℎ𝑘,𝑛

, ∀(𝑘, 𝑛).

Then, the overall uploading latency of worker 𝑘 is decided by the slowest subcarrier:

𝑇 com
𝑘

= max
𝑛

𝑇 com
𝑘,𝑛 ,

1 ≤ 𝑘 ≤ 𝐾.

(9)

(10)

The uploading energy consumption of worker 𝑘 is modeled as follows. Let 𝐸 com

𝑘,𝑛 denote the
transmit energy consumption of worker 𝑘 on subcarrier 𝑛. If subcarrier 𝑛 is not allocated, i.e.,
𝐶𝑘,𝑛 = 0, 𝐸 com

𝑘,𝑛 = 0. Otherwise,

𝐸 com

𝑘,𝑛 = 𝐶𝑘,𝑛𝑃com

𝑘,𝑛 𝑇 com

𝑘,𝑛 , ∀(𝑘, 𝑛).

(11)

11

By substituting the transmit power density 𝑃com
𝑘,𝑛
𝐸 com

𝑘,𝑛 can be further derived as

in (9) and the uploading latency 𝑇 com
𝑘,𝑛

in (8),

(cid:16)

𝐶𝑘,𝑛

(cid:17)

2𝑅𝑘,𝑛/𝐵 − 1
ℎ𝑘,𝑛𝑅𝑘,𝑛

𝐸 com

𝑘,𝑛 =

𝜎2𝐿 𝑘,𝑛𝜏

, ∀(𝑘, 𝑛).

(12)

The total uploading energy consumption of worker 𝑘 is the sum of uploading energy consumption

over all subcarriers:

(cid:110)

𝐸 com
𝑘

= (cid:205)𝑁

𝑛=1

(cid:111)

. By substituting 𝐸 com

𝑘,𝑛 in (12),

𝑘,𝑛 , 1 ≤ 𝑘 ≤ 𝐾
𝐸 com
(cid:16)

(cid:17)

2𝑅𝑘,𝑛/𝐵 − 1
ℎ𝑘,𝑛𝑅𝑘,𝑛

𝐶𝑘,𝑛

𝜎2𝐿 𝑘,𝑛𝜏

𝐸 com
𝑘

=

𝑁
∑︁

𝑛=1

,

1 ≤ 𝑘 ≤ 𝐾.

(13)

Next, the total latency and energy consumption of worker 𝑘 are deﬁned as follows. The latency

of worker 𝑘 is the sum latencies of the two phases:

𝑇𝑘 = 𝑇 cmp

𝑘

+ 𝑇 com
𝑘

,

1 ≤ 𝑘 ≤ 𝐾,

(14)

where 𝑇 cmp
(10). The energy consumption of worker 𝑘 is given by:

is the computation latency deﬁned in (6), 𝑇 com

𝑘

𝑘

is the uploading latency deﬁned in

𝐸𝑘 = 𝐸 cmp

𝑘

+ 𝐸 com
𝑘

+ 𝜉,

1 ≤ 𝑘 ≤ 𝐾,

(15)

where 𝜉 is the constant circuit energy consumption when there is no computation and transmis-
sion, 𝐸 cmp
consumption of worker 𝑘, respectively.

deﬁned in (13) are the computation and uploading energy

deﬁned in (7) and 𝐸 com

𝑘

𝑘

III. PROBLEM FORMULATION

We aim at minimizing the overall learning latency of the PARTEL system, which depends on

two factors: the per-round latency and the number of rounds for model convergence. The overall

learning latency is deﬁned as the total latency of all rounds till model convergence. In [21] for

narrowband channels, it is proved that the overall learning latency minimization is equivalent

to separately minimizing the per-round latency. The result can also apply to the current case of

broadband channels, as stated below.

Lemma 1 (Equivalent Per-Round Latency Minimization [21]). The overall learning latency

minimization is equivalent to separately minimizing the latencies for all rounds.

Lemma 1 holds because the distributed learning algorithms implemented using PARTEL are

equivalent to the corresponding centralized ones in terms of convergence rate as measured by

12

the required number of communication rounds. Speciﬁcally, for distributed learning, the values

of updates (e.g., gradients and proximal gradients) calculated in each round and the number of

rounds required for model convergence are independent of SUPPORT.

Using the result in Lemma 1, we formulate the equivalent per-round latency-minimization
problem. For an arbitrary round, we aim to minimize its latency, denoted as 𝑇, under the con-

straints on subcarrier assignment, latency requirement, parameter allocation, and power control,

described as follows.

1) Subcarrier Assignment Constraints: Each subcarrier can be allocated to one worker:

(C1: Subcarrier Assignment Constraint)

𝐶𝑘,𝑛 ∈ {0, 1}, ∀(𝑘, 𝑛),
𝐾
∑︁

𝐶𝑘,𝑛 = 1,

1 ≤ 𝑛 ≤ 𝑁,

𝑘=1

(16)






where 𝐶𝑘,𝑛 = 1 represents that the subcarrier 𝑛 is allocated to worker 𝑘.

2) Per-Round Latency Constraints: As all parametric blocks should be updated in one round,

all workers’ latencies, say {𝑇𝑘 }, should not exceed the overall one-round latency 𝑇:

𝑇𝑘 ≤ 𝑇,

1 ≤ 𝑘 ≤ 𝐾.

(17)

As mentioned, 𝑇 is the latency for an arbitrary round and can be different over different rounds.
By substituting 𝑇𝑘 in (14), the constraints in (17) can be derived as

𝑇 cmp
𝑘

+ 𝑇 com
𝑘

≤ 𝑇,

1 ≤ 𝑘 ≤ 𝐾,

which, by substituting the uploading latency 𝑇 com

𝑘

in (10), are equivalent to

(C2: Per-Round Latency Constraint) 𝑇 cmp

𝑘

+ 𝑇 com

𝑘,𝑛 ≤ 𝑇, ∀𝐶𝑘,𝑛 = 1,

(18)

(19)

where 𝑇 cmp
uploading latency of worker 𝑘 on subcarrier 𝑛.

𝑘

deﬁned in (6) is the computation latency of worker 𝑘 and 𝑇 com

𝑘,𝑛 deﬁned in (8) is the

3) Parameter Constraints: The parameter constraints are two tiers. On the one hand, the total

updatable number of parameters by all workers should be no smaller than the size of the model:

(C3: Inter-Worker Parameter Constraint)

𝐾
∑︁

𝑘=1

𝐿 𝑘 ≥ 𝐿,

(20)

where 𝐿 𝑘 is the size of the parametric block allocated to worker 𝑘 and 𝐿 is the size of the model-

parameter vector (or matrix). On the other hand, for each worker, the total uploaded number of

parameters on all subcarriers should be no smaller than its allocated parametric-block size:

(C4: Intra-Worker Parameter Constraint)

𝑁
∑︁

𝑛=1

𝐶𝑘,𝑛𝐿 𝑘,𝑛 ≥ 𝐿 𝑘 ,

1 ≤ 𝑘 ≤ 𝐾,

(21)

13

where 𝐿 𝑘,𝑛 is the number of parameters uploaded by worker 𝑘 on subcarrier 𝑛. In the sequel,
{𝐿 𝑘 } and {𝐿 𝑘,𝑛} are relaxed to be continuous for simplicity. In practice, the solved {𝐿∗
{𝐿∗
be ignored, since the values of {𝐿 𝑘 } and {𝐿 𝑘,𝑛} are typically large.

𝑘 } and
𝑘,𝑛} will be rounded for implementation and the loss caused by the rounding operation can

For the case of DNN models, Granularity Constraints 1 and 2 can be written mathematically

as follows.

(Cdnn: Additional Parameter Constraint for DNN Models)

𝐿 𝑘
𝐿sub

∈ N+,

1 ≤ 𝑘 ≤ 𝐾,

(22)

where N+ is the set of positive integers and 𝐿sub is the size of the subproblems, i.e., neurons
or per-sample auxiliary matrices. For W-stage, the size of all neurons, say 𝐿sub, are assumed
the same for simplicity, which has little impact on the solution, since the size of each neuron is

much smaller than that of the whole problem, as mentioned in Example 1. For Z-stage, the size
of each per-sample auxiliary matrix is the total number of neurons, say 𝐿sub = (cid:205)𝐺
𝐼𝑔 with 𝐼𝑔
𝑔=1
being the number of neurons in layer 𝑔.

4) Power Constraints: The power consumption of each worker is constrained as

(C5: Power Constraint)

𝐸𝑘
𝑇𝑘

≤ 𝑃𝑘 ,

1 ≤ 𝑘 ≤ 𝐾,

(23)

where 𝐸𝑘 deﬁned in (15), 𝑇𝑘 deﬁned in (14), and 𝑃𝑘 are the energy consumption, latency, and
maximal permitted power of worker 𝑘, respectively.

5) Latency-Minimization Problem: Under these constraints, the per-round latency-minimization

problem by joint SUPPORT can be formulated as

min
{𝐶𝑘,𝑛},{𝐿 𝑘 },{𝐿 𝑘,𝑛},{𝑅𝑘,𝑛},𝑇

𝑇,

(P1)

(24)

s.t. (C1) ∼ (C5), & (Cdnn) for a DNN Model.

IV. JOINT SUPPORT FOR DECOMPOSABLE MODELS

In this section, joint SUPPORT is designed by developing a tractable approach for solving

Problem (P1).

A. Equivalent Latency Requirement

First, the following necessary condition for the equivalent latency requirement can be derived to

simplify Problem (P1). Note that in [21], similar equivalent latency property can be derived in the

PARTEL design for frequency non-selective channels. However, for OFDM systems considered in

14

this paper, the binary subcarrier allocation among workers and the corresponding inter-subcarrier

power and parameter allocation for each device make the problem much more complicated.

Lemma 2 (Equivalent Latency for All Workers). To achieve the optimal solution of (P1), the

following latency condition should be satisﬁed:

𝑇 cmp
𝑘

+ 𝑇 com

𝑘,𝑛 = 𝑇, ∀𝐶𝑘,𝑛 = 1,

(25)

where 𝑇 cmp
uploading latency of worker 𝑘 on subcarrier 𝑛.

𝑘

deﬁned in (6) is the computation latency of worker 𝑘, 𝑇 com

𝑘,𝑛 deﬁned in (8) is the

Proof: See Appendix A.

The result in Lemma 2 yields the following insights. First, it requires all workers the same
latency with the overall latency 𝑇. Second, for each worker, the uploading latency on all allocated

subcarriers should be equal.

Remark 1 (Computation Latency vs. Communication Latency). By substituting the computation
latency 𝑇 cmp

in (6) and the communication latency 𝑇 com

𝑘,𝑛 in (8) into the equivalent latency property

𝑘

in (25), it can be derived as

𝐿 𝑘
𝑓𝑘

+

𝐿 𝑘,𝑛𝜏
𝑅𝑘,𝑛

= 𝑇, ∀𝐶𝑘,𝑛 = 1,

(26)

with the constraints (cid:205)𝑁
𝑛=1

𝐶𝑘,𝑛𝐿 𝑘,𝑛 = 𝐿 𝑘 . From (26), the load, say 𝐿 𝑘 , has the same effect on

computation and communication latency. On the other hand, when the computation frequency
𝑓𝑘 is small compared to the number of subcarriers and the data rates, the computation latency

dominates or vice versa.

By substituting the computation latency 𝑇 cmp

𝑘

deﬁned in (6) and the uploading latency 𝑇 com
𝑘,𝑛

deﬁned in (8) into the necessary condition in Lemma 2, we can derive the number of parameters
uploaded by worker 𝑘 on subcarrier 𝑛, say 𝐿 𝑘,𝑛, as

𝐿 𝑘,𝑛 =

𝐶𝑘,𝑛𝑅𝑘,𝑛
𝜏

(cid:18)
𝑇 −

(cid:19)

𝐿 𝑘
𝑓𝑘

, ∀(𝑘, 𝑛),

(27)

where 𝑇 is the per-round latency, 𝐶𝑘,𝑛 ∈ {0, 1} is the subcarrier-allocation indicator, 𝐿 𝑘 is the
parametric-block size allocated to worker 𝑘, 𝑅𝑘,𝑛 is the channel capacity of 𝑘 on subcarrier 𝑛.

By substituting 𝐿 𝑘,𝑛 deﬁned in (27) and the necessary condition in Lemma 2, Problem (P1) can

15

be simpliﬁed as:

𝑇,

min
{𝐶𝑘,𝑛},{𝐿 𝑘 },
{𝑅𝑘,𝑛},𝑇

(P2)

s.t. (C1), (C3),
𝑁
∑︁

𝐶𝑘,𝑛𝑅𝑘,𝑛
𝜏

𝑛=1

(cid:18)
𝑇 −

(cid:19)

𝐿 𝑘
𝑓𝑘

≥ 𝐿 𝑘 ,

1 ≤ 𝑘 ≤ 𝐾,

(28)

𝐸𝑘 ≤ 𝑃𝑘𝑇,

1 ≤ 𝑘 ≤ 𝐾,

where 𝐸𝑘 deﬁned in (15) is the energy consumption of worker 𝑘. By substituting 𝐿 𝑘,𝑛 in (27),
𝐸𝑘 can be expressed as

𝐸𝑘 = 𝑔𝑘 𝑓 2

𝑘 𝐿 𝑘 +

(cid:16)

𝐶𝑘,𝑛

(cid:17)

𝜎2

2𝑅𝑘,𝑛/𝐵 − 1
ℎ𝑘,𝑛

(cid:18)
𝑇 −

(cid:19)

𝐿 𝑘
𝑓𝑘

+ 𝜉.

𝑁
∑︁

𝑛=1

(29)

B. Equivalent Convex Problem

Problem (P2) is a mixed integer non-convex problem and is hence NP-hard [27]. In the

sequel, two steps are used to tackle it. First, following the standard approach to tackle integer

programming (see e.g., [22]), linear programming relaxation is used to relax the subcarrier-
allocation indicators in Problem (P2) to be continuous, i.e., (cid:8)𝐶𝑘,𝑛 ∈ [0, 1], ∀(𝑘, 𝑛)(cid:9). Then,

following the method in [21], the relaxed problem can be equivalently converted to the problem

of updatable model size maximization. However, it remains non-convex and difﬁcult to tackle

due to the intra-worker parameter constraint and the power constraint. In the sequel, the problem

of updatable model size maximization is derived and solved.

Given the one-round latency 𝑇 for an arbitrary round, let ˆ𝐿∗(𝑇) denote the maximum size
of a model that can be updated within the round. Then ˆ𝐿∗(𝑇) solves the following problem of

16

model size maximization:

ˆ𝐿∗(𝑇) =

max
{𝐶𝑘,𝑛},{𝐿 𝑘 },{𝑅𝑘,𝑛}

𝐾
∑︁

𝑘=1

𝐿 𝑘 ,

(P3)

s.t. 0 ≤ 𝐶𝑘,𝑛 ≤ 1, ∀(𝑘, 𝑛),

𝐾
∑︁

𝑘=1
𝑁
∑︁

𝑛=1

𝐶𝑘,𝑛 = 1,

1 ≤ 𝑛 ≤ 𝑁,

(30)

𝐶𝑘,𝑛𝑅𝑘,𝑛
𝜏

(cid:18)
𝑇 −

(cid:19)

𝐿 𝑘
𝑓𝑘

≥ 𝐿 𝑘 ,

1 ≤ 𝑘 ≤ 𝐾,

𝐸𝑘 ≤ 𝑃𝑘𝑇,

1 ≤ 𝑘 ≤ 𝐾,

where 𝐶𝑘,𝑛 is the subcarrier-allocation indicator, 𝐿 𝑘 is the parametric-block size allocated to
worker 𝑘, 𝑅𝑘,𝑛 is the channel capacity of worker 𝑘 on subcarrier 𝑛, 𝑇 is the one-round latency,
𝐸𝑘 deﬁned in (29) is the energy consumption of worker 𝑘. Note that solving Problem (P2)

via utilizing the problem of model size maximization in Problem (P3) follows the method in

[21]. However, new challenges arise from the subcarrier allocation among workers and the inter-

subcarrier power and parameter allocation for each worker, leading to the non-convexity and a

much larger size of Problem (P3).

Lemma 3 (Relation of Maximal Model Size and Latency).
a monotonously increasing function of 𝑇.

ˆ𝐿∗(𝑇) deﬁned in Problem (P3) is

Proof: See Appendix B.

It follows from the result in Lemma 3 that the solution of Problem (P2) is the minimal latency,
say 𝑇 ∗, which makes the updatable model size ˆ𝐿∗(𝑇 ∗) no less than the target size 𝐿. This suggests
a method to solve Problem (P2) by searching 𝑇 ∗ using the criterion ˆ𝐿∗(𝑇) ≥ 𝐿, which will be

elaborated in the later subsection.

To get the maximum updatable model size ˆ𝐿∗(𝑇) requires solving Problem (P3). To this end,

the following variables are used to transform Problem (P3) into a convex problem.

𝜑𝑘 =

(cid:18)
𝑇 −

(cid:19) −1

,

𝐿 𝑘
𝑓𝑘

˜𝑅𝑘,𝑛 = 𝐶𝑘,𝑛𝑅𝑘,𝑛,





(31)

By substituting the variables in (31) and 𝐸𝑘 deﬁned in (29), Problem (P3) can be written as

17

ˆ𝐿∗(𝑇) = max

{𝐶𝑘,𝑛},{𝜑𝑘 },
{ ˜𝑅𝑘,𝑛},

𝐾
∑︁

𝑘=1

(cid:18)
𝑇 −

𝑓𝑘

(cid:19)

,

1
𝜑𝑘

(P4)

s.t. 0 ≤ 𝐶𝑘,𝑛 ≤ 1, ∀(𝑘, 𝑛),

𝐾
∑︁

𝑘=1
𝑁
∑︁

𝑛=1

𝐶𝑘,𝑛 = 1,

1 ≤ 𝑛 ≤ 𝑁,

˜𝑅𝑘,𝑛
𝜏

≥ 𝑓𝑘 (𝑇 𝜑𝑘 − 1) ,

1 ≤ 𝑘 ≤ 𝐾,

𝑁
∑︁

𝐶𝑘,𝑛𝜎2 (cid:0)2

˜𝑅𝑘,𝑛
𝐵𝐶𝑘,𝑛 − 1(cid:1)

𝑛=1

ℎ𝑘,𝑛

+ 𝑔𝑘 𝑓 3

𝑘 (𝜑𝑘𝑇 − 1) ≤ (𝑃𝑘𝑇 − 𝜉)𝜑𝑘 , 1 ≤ 𝑘 ≤ 𝐾.

Lemma 4. Problem (P4) is a convex problem.

Proof: See Appendix C.

C. Properties of Optimal Policies

Based on the results in the previous subsection, the optimal policies of Problem (P2) with

relaxed subcarrier-allocation indicators are proposed, as described in the following.

As (P4) is convex, the primal-dual method can be used to get the optimal solution:

max
{𝜇𝑛},{𝜆𝑘 },
{𝜈𝑘 }

min
{𝐶𝑘,𝑛},{ ˜𝑅𝑘,𝑛},
{𝜑𝑘 }

LP4,

(32)

(33)

(cid:35)

˜𝑅𝑘,𝑛
𝜏

𝑁
∑︁

𝑛=1

(cid:35)

where LP4 is the Lagrange function of Problem (P4), given as
(cid:34)

(cid:32)

(cid:33)

LP4 = −

𝜇𝑛

1 −

𝐶𝑘,𝑛

+

𝜆𝑘

𝑓𝑘 (𝑇 𝜑𝑘 − 1) −

𝐾
∑︁

𝑘=1
𝐾
∑︁

𝑘=1

+

𝑓𝑘

𝜈𝑘

(cid:18)
𝑇 −

(cid:19)

+

1
𝜑𝑘

𝑁
∑︁

𝑛=1

(cid:18)

˜𝑅𝑘,𝑛
𝐵𝐶𝑘,𝑛 − 1

2

(cid:19)

×

𝐶𝑘,𝑛

(cid:34) 𝑁
∑︁

𝑛=1

𝐾
∑︁

𝑘=1

𝜎2
ℎ𝑘,𝑛

𝐾
∑︁

𝑘=1

+ 𝑔𝑘 𝑓 3

𝑘 (𝑇 𝜑𝑘 − 1) − (𝑃𝑘𝑇 − 𝜉)𝜑𝑘

,

and {𝜇𝑛}, {𝜆𝑘 ≥ 0}, and {𝜈𝑘 ≥ 0} are Lagrangian multipliers.

Next, the necessary conditions for achieving the optimal solution of the inner loop are used

to derive the optimal policies. The inner loop problem is given by

min
{𝐶𝑘,𝑛},{ ˜𝑅𝑘,𝑛},{𝜑𝑘 }

LP4,

given {𝜇𝑛}, {𝜆𝑘 }, {𝜈𝑘 }.

(34)

18

(35)

(36)

(37)

The ﬁrst necessary condition is

𝜕LP4
𝜕 ˜𝑅𝑘,𝑛

= −

𝜆𝑘
𝜏

+ 𝜈𝑘 2

˜𝑅𝑘,𝑛
𝐵𝐶𝑘,𝑛 ln 2 ×

𝜎2
𝐵ℎ𝑘,𝑛

= 0, ∀𝐶𝑘,𝑛 ≠ 0,

which gives the following optimal scheme for calculating the channel capacity:

𝑅∗

𝑘,𝑛 =

˜𝑅∗

𝑘,𝑛

𝐶∗

𝑘,𝑛

0,





= 𝐵 log2

(cid:19)

(cid:18) 𝜆𝑘 𝐵
𝜈𝑘 𝜏 ln 2

+ 𝐵 log2

(cid:18) ℎ𝑘,𝑛
𝜎2

(cid:19)

,

∀𝐶𝑘,𝑛 ≠ 0,

otherwise.

By substituting 𝑅∗

𝑘,𝑛 in (36) into the transmission power in (9), the optimal power-allocation

scheme can be derived, as in the following lemma.

Lemma 5 (Optimal Power Allocation). The optimal power-allocation scheme is
𝜆𝑘 𝐵
𝜈𝑘 𝜏 ln 2

∀𝐶𝑘,𝑛 ≠ 0,

𝜎2
ℎ𝑘,𝑛

−

,

𝑃com
𝑘,𝑛

∗ =

0,

otherwise.





The water-ﬁlling like result in (37) shows that for each worker, more power should be allocated

on the subcarrier with high channel gain, say ℎ𝑘,𝑛.

The second necessary condition to achieve the optimum of the inner loop problem in (34) is

𝜕LP4
𝜕𝜑𝑘

= −

𝑓𝑘
𝜑2
𝑘

+ 𝜆𝑘 𝑓𝑘𝑇 + 𝜈𝑘 𝑔𝑘 𝑓 3

𝑘 𝑇 − 𝜈𝑘 (𝑃𝑘𝑇 − 𝜉) = 0.

(38)

By substituting the variable transformations in (31) into (38), we can achieve the optimal inter-

worker parameter allocation scheme, as follows.

Lemma 6 (Optimal Parameter Allocation among Workers). The optimal inter-worker parameter-

allocation scheme is

(cid:20)
𝑇 −

𝐿∗

𝑘 =

√︃

𝜆𝑘𝑇 + 𝜈𝑘 𝑔𝑘 𝑓 2

𝑘 𝑇 − 𝜈𝑘 (𝑃𝑘𝑇 − 𝜉)/ 𝑓𝑘

(cid:21)

𝑓𝑘 ,

1 ≤ 𝑘 ≤ 𝐾.

(39)

In (39), the optimal parametric-block size allocated to worker 𝑘, say 𝐿∗

𝑘 , is a concave function
of the computation speed 𝑓𝑘 and a monotone decreasing function of the computation power factor
𝑔𝑘 . On one hand, large 𝑓𝑘 can reduce the computation latency. On the other hand, the computation
energy increases as a square function of 𝑓𝑘 . The optimal load in (39) balances the two aspects.

Substituting the parameter-allocation scheme in (39) and the channel capacity in (36) into the

intra-worker parameter-allocation scheme {𝐿 𝑘,𝑛} in (27), gives the following lemma.

Lemma 7 (Optimal Parameter Allocation Among Subcarriers). The optimal intra-worker pa-

rameter allocation scheme is given by

19

√︃

𝜆𝑘𝑇 + 𝜈𝑘 𝑔𝑘 𝑓 2

𝑘 𝑇 − 𝜈𝑘 (𝑃𝑘𝑇 − 𝜉)/ 𝑓𝑘

𝜏

0,

𝐿∗

𝑘,𝑛 =






× 𝐵 log2

(cid:18) 𝜆𝑘 𝐵ℎ𝑘,𝑛
𝜈𝑘 𝜏𝜎2 ln 2

(cid:19)

,

if 𝐶𝑘,𝑛 ≠ 0,

otherwise.

(40)

From (40), more parameters should be assigned to the channel with high gain.

The third necessary condition to achieve the optimum of the inner loop problem in (34) is

𝜕LP4
𝜕𝐶𝑘,𝑛

= −𝜇𝑛 + 𝐼𝑘,𝑛 = 0, ∀(𝑘, 𝑛),

where 𝐼𝑘,𝑛 is the indicator function given by

𝐼𝑘,𝑛 =

(cid:34)

(cid:16)

𝜈𝑘 𝜎2
ℎ𝑘,𝑛

𝑅∗
𝑘,𝑛/𝐵 − 1

2

(cid:17)

−

𝑅∗

𝑘,𝑛2

𝑅∗
𝑘,𝑛/𝐵

(cid:35)

ln 2

𝐵

, ∀(𝑘, 𝑛).

(41)

(42)

𝐼𝑘,𝑛. If 𝐼𝑘,𝑛 > 𝜇𝑛, 𝐶𝑘,𝑛 = 0,
Note that 𝐼𝑘,𝑛 is determined when 𝑅∗
as the condition in (41) can not be satisﬁed. If 𝐼𝑘,𝑛 = 𝜇𝑛 for a unique worker, say 𝑘, then
𝐶𝑘,𝑛 = 1. If 𝐼𝑘,𝑛 = 𝜇𝑛 for multiple workers, then 𝐶𝑘,𝑛 ∈ (0, 1) for these workers. And in the
last case, it is easy to show that the values of the non-zero {𝐶𝑘,𝑛} won’t inﬂuence the value of

𝑘,𝑛 is known. Let 𝜇𝑛 = min
𝑘

the Lagrange function LP4 deﬁned in (33), as long as the subcarrier assignment constraint, say
(cid:8)(cid:205)𝑁

1 ≤ 𝑛 ≤ 𝑁 (cid:9), are satisﬁed.

𝐶𝑘,𝑛 = 1,

𝑛=1
The optimal subcarrier allocation is summarized in the following lemma.

Lemma 8 (Optimal Subcarrier Allocation). The optimal subcarrier allocation is given as:

𝐶∗

𝑘,𝑛






= 0,

if 𝐼𝑘,𝑛 > 𝜇𝑛,

∈ (0, 1),

if 𝐼𝑘,𝑛 = 𝜇𝑛 for multiple workers,

(43)

= 1,

if 𝐼𝑘,𝑛 = 𝜇𝑛 for a unique workerW𝑘 ,

where 𝐼𝑘,𝑛 is the indicator function deﬁned in (42), 𝜇𝑛 = min

𝐼𝑘,𝑛, and 𝑅∗

𝑘,𝑛 is the optimal

𝑘

channel capacity in (36).

In (43), a high channel gain leads to a small value of 𝐼𝑘,𝑛 and thus a high possibility to
make 𝐶𝑘,𝑛 ≠ 0. That means the subcarrier with higher channel gain has larger possibility to be

allocated to the worker. Note that in the optimal scheme in Lemma 8, some subcarrier-allocation

indicators may be fractions. The standard approach is to round those to be binary (see, e.g.,

[22]), which will be elaborated in the later subsection.

20

Algorithm 1 Updatable Model Size Maximization
1: Input: channel gains {ℎ𝑘,𝑛}, computation speeds { 𝑓𝑘 }, computation power factors, {𝑔𝑘 }, and
the given one-round latency 𝑇.
2: Initialize {𝜆(0)

𝑘 }, and 𝑖 = 0.

𝑘 }, {𝜈(0)

3: Loop

4:

Update the multipliers as

(cid:26)

(cid:26)

𝜆(𝑖)
𝑘 + 𝜂𝜆𝑘

𝜈(𝑖)
𝑘 + 𝜂𝜈𝑘

𝜕LP4
𝜕𝜆𝑘
𝜕LP4
𝜕𝜈𝑘

,

,

(cid:27)

0

(cid:27)

0

, 1 ≤ 𝑘 ≤ 𝐾,

, 1 ≤ 𝑘 ≤ 𝐾,

𝑘,𝑛} using (39), (36), and (43), respectively.

= max

𝜆(𝑖+1)
𝑘



𝑘,𝑛}, and {𝐶∗

𝜈(𝑖+1)
𝑘



= max

𝑘,𝑛} with (31).

5:

6:

Solve {𝐿∗
Get {𝜑∗

𝑘 }, {𝑅∗
𝑘 } and { ˜𝑅∗
7: Until Convergence
8: ˆ𝐿∗(𝑇) = (cid:205)𝐾
9: Output: ˆ𝐿∗(𝑇), {𝐿∗

𝐿∗
𝑘 .

𝑘=1

𝑘 }, {𝑅∗

𝑘,𝑛}, and {𝐶∗

𝑘,𝑛}.

D. Optimal Policy Computation

In this subsection, the joint SUPPORT algorithm to solve the original Problem (P1) is proposed.

First, we solve the convex Problem (P4) by the primal-dual method using the closed-form results
in Lemmas 6-8. Some notation is described as follows. {𝜂𝜆𝑘 } and {𝜂𝜈𝑘 } denote the step sizes of
gradient descent. LP4 and 𝜇, {𝜆𝑘 ≥ 0}, and {𝜈𝑘 ≥ 0} are the Lagrange function and Lagrangian
multipliers deﬁned in (33). With the notation, the application of the primal-dual method yields

Algorithm 1 for solving Problem (P4).

Remark 2 (Low Complexity of Updatable Model Size Maximization). The computation com-
plexity of Algorithm 1 is O (𝐾 2𝑁) with 𝐾 being the number of workers and 𝑁 being the number

of subcarriers, as the closed-form results in Lemmas 6 - 8 makes the updating of corresponding

variables more efﬁcient. As a comparison, directly solving the non-convex Problem (P3) has a
computational complexity of at least O (𝐾 3𝑁 3) and is suboptimal.

Then, as mentioned in the preceding subsection, Problem (P2) with relaxed subcarrier-allocation
indicators can be solved by nesting a one-dimensional search over the latency 𝑇 and solving
the convex Problem (P4). Based on the monotonicity of ˆ𝐿∗(𝑇) in Lemma 3, the search can be

21

Algorithm 2 Joint SUPPORT
1: Input: channel gains {ℎ𝑘,𝑛}, computation speeds { 𝑓𝑘 }, and computation power factors, {𝑔𝑘 }.
2: Select 𝑇 = 𝑇u that makes ˆ𝐿∗(𝑇u) deﬁned in Problem (P4) larger than 𝐿.
3: Select 𝑇 = 𝑇l that makes ˆ𝐿∗(𝑇l) < 𝐿.
4: While 𝑇u ≠ 𝑇l
5:

Let 𝑇m = (𝑇u + 𝑇l)/2.
Input {ℎ𝑘,𝑛}, { 𝑓𝑘 }, {𝑔𝑘 } and 𝑇 = 𝑇m into Algorithm 1 to solve (P4).
Obtain ˆ𝐿∗(𝑇m), {𝐿∗
If ˆ𝐿∗(𝑇m) ≥ 𝐿
𝑇u = 𝑇m.

𝑘,𝑛}, and {𝐶∗

𝑘 }, {𝑅∗

𝑘,𝑛}.

6:

7:

8:

9:

10: Else

11:

𝑇l = 𝑇m.

12: End if

13:End while
14:𝑇 ∗ = 𝑇m.
15:Output: 𝑇 ∗, {𝐿∗

𝑘 }, {𝑅∗

𝑘,𝑛}, and {𝐶∗

𝑘,𝑛}.

efﬁciently implemented by bisection method. While the solution of Problem (P4) is presented in

Algorithm 1. Then the optimal policy to solve Problem (P2) with relaxed subcarrier-allocation

indicators is presented in Algorithm 2, by nesting the bisection search and Algorithm 1.

Finally, based on Algorithm 2, the joint scheme of SUPPORT without relaxation is proposed

to solve the original Problem (P1). Note that not all subcarrier-allocation indicators solved by
𝑘,𝑛 ∈ (0, 1) for some (𝑘, 𝑛). For these subcarriers, a practical
Algorithm 2 are integers, i.e., 𝐶∗

subcarrier-allocation scheme following [22] is determined as

𝐶∗
𝑘1,𝑛 = 1,

𝑘1 = arg max

𝑘

𝐿∗
𝑘,𝑛,

1 ≤ 𝑛 ≤ 𝑁,

(44)

where the subcarrier is allocated to the worker with the largest value. Then, given the subcarrier-
allocation scheme {𝐶∗

𝑘,𝑛}, the latency-minimization problem is a special case of Problem (P1),

whose solution can also be solved by Algorithm 2.

22

V. JOINT SUPPORT FOR DNN MODELS

In this section, DNN models are considered. Since Problem (P1) is not tractable in this case

with the additional constraint (Cdnn), we propose an approximate solution method that leverages

the result for decomposable model case, described as follows.

1) For both W-stage and Z-stage, solve the joint scheme of SUPPORT using the method in

Section IV without considering Granularity Constraints 1 and 2.

2) Given the subcarrier-allocation scheme, round the parameter allocation for each worker to

satisfy Granularity Constraint 1 for W-stage and Granularity Constraint 2 for Z-stage.

The challenges lie in Step 2) and are two-fold. On one hand, how should the rounding indicator

be designed to minimize the rounding loss. On the other hand, as each worker’s number of

parameters changes, the corresponding channel-capacity (or power) allocation and intra-worker

parameter allocation among the assigned subcarriers should be redesigned. To tackle these

challenges, in the sequel, we ﬁrst propose a joint scheme of SUPPORT for DNN models. Then,

the rounding scheme is designed accordingly and the resultant latency increase is analyzed.

1) Joint SUPPORT for DNN Models: Denote the solved one-round latency as 𝑇 ∗, the subcarrier-

allocation policy as {𝐶∗
worker 𝑘 as 𝐿∗

𝑘,𝑛}, the spectrum efﬁciencies as {𝑅∗

𝑘,𝑛}, the number of parameters of

𝑘 , the number of parameters uploaded by worker 𝑘 on subcarrier 𝑛 as 𝐿∗

𝑘,𝑛.

Consider an arbitrary worker, say worker 𝑘. If its number of parameters is rounded down

to satisfy (Cdnn), the reduced number of parameters is denoted as Δ𝐿d
𝑘 ≥ 0. If its number of
parameters is rounded up, the additional number of parameters to be uploaded is denoted as
Δ𝐿u
𝑘 ≥ 0. Note that if worker 𝑘’s number of parameters is rounded down, no inﬂuence is caused
to the one-round latency. Hence, only the case of being rounded up is considered in the sequel.

Our aim is to design rounding scheme to minimize the resulted additional one-round latency.

Next, the joint scheme of SUPPORT is designed as

(Joint SUPPORT for DNN Models)

𝐶𝑘,𝑛 = 𝐶∗

𝑘,𝑛, 𝑅𝑘,𝑛 = 𝑅∗

𝑘,𝑛,

Δ𝐿 𝑘,𝑛 = 𝐿∗

𝑘,𝑛 ×

Δ𝐿u
𝑘
𝐿∗
𝑘

,

(45)





where Δ𝐿 𝑘,𝑛 is the number of additional parameters allocated to subcarrier 𝑛 for uploading,
which is proportional to its currently uploaded number of parameters 𝐿∗
𝑘,𝑛. In (45), the allocation
of subcarriers {𝐶𝑘,𝑛} and the channel capacities {𝑅𝑘,𝑛} of the assigned subcarriers remain the

same. Two concerns motivate us to design the joint SUPPORT scheme as (45). First, the assigned

23

subcarrier that can currently upload more updates of parameters can upload more additional pa-

rameters in the same additional latency. Second, the proportional additional parameter allocation

together with the unchanged allocation of subcarriers and channel capacities can yield a simple

upper bound of the additional latency for each worker, as shown in the following lemma.

Lemma 9 (Additional Latency). Consider an arbitrary worker, say worker 𝑘, the design in (45)

results in an upper bound of the minimum additional latency:

Δ𝑇𝑘 ≤ 𝑇 ∗ ×

,

Δ𝐿u
𝑘
𝐿∗
𝑘
𝑘 , and 𝐿∗

(46)

where 𝑇 ∗ is the solved latency in Step 1), Δ𝑇𝑘 , Δ𝐿u

𝑘 are the additional latency, the number

of additional parameters after the rounding operation, and the solved number of parameters in
Step 1) of worker 𝑘, respectively.

The proof of Lemma 9 is straightforward and hence omitted. Two observations can be made

from Lemma 9. On one hand, as mentioned in Example 1, the size of the subproblems are far
smaller than the problems of W-stage and Z-stage, i.e., Δ𝐿u
𝑘 . Therefore, the additional
latency Δ𝑇𝑘 is small for all workers. On the other hand, the round-up indicator, denoted as 𝐼𝑘 ,
should be the ratio 𝐼𝑘 =

𝑘 (cid:28) 𝐿∗

.

Δ𝐿u
𝑘
𝐿∗
𝑘

2) Parameter Rounding Scheme: Note that Lemma 9 only gives the additional latency for

one worker. To minimize the additional one-round latency, the rounding scheme is designed to
make the workers with least 𝐼𝑘 to round up and the others to round down, described as follows.

1) Sort the round-up indicators {𝐼𝑘 } from the least to the biggest and the new permutation

is indexed by 𝑘 (cid:48)
2) Find the least 𝐾 (cid:48)

, i.e., {𝐼𝑘 (cid:48) } is sorted from the least to the largest.
1 following the new permutation {𝐼𝑘 (cid:48) }, which satisﬁes

𝐾 (cid:48)
1∑︁

𝑘 (cid:48)=1

Δ𝐿u

𝑘 (cid:48) ≥

𝐾
∑︁

𝑘 (cid:48)=𝐾1+1

𝑘 (cid:48) ,
Δ𝐿d

(47)

where 𝐿u
and Δ𝐿d
by rounding up 𝐾 (cid:48)

𝑘 (cid:48) is the additional number of parameters of worker 𝑘 (cid:48)
𝑘 (cid:48) is the reduced number of parameters when being rounded down. (47) means that
1 workers with least round-up indicators, the parameters of all workers

when being rounded up

can satisfy Granularity Constraints 1 and 2.

3) The additional one-round latency is Δ𝑇 ≤ 𝑇 ∗ × 𝐼𝐾 (cid:48)

1

, where 𝑇 ∗ is the solved one round

latency without considering Granularity Constraints 1 and 2.

24

A. Experiment Setup

VI. EXPERIMENTAL RESULTS

The experimental settings are speciﬁed as follows unless speciﬁed otherwise. In the OFDM
based PARTEL system, there are 𝐾 workers and 𝑁 subcarriers. The bandwidth of each subcarrier
is 𝐵 = 312.5 kHz. The subcarrier channel gains {𝐻𝑘,𝑛} are assumed to be i.i.d. Rayleigh fading
with the average path loss of 10−3. The noise power density is set as 10−9 W/Hz. The workers’
computation speeds { 𝑓𝑘 } are uniformly selected from the set {0.1, 0.2, ..., 1.0} × 106 parameters

processed per second in one local computation iteration. The corresponding computation power
factors {𝑔𝑘 } are uniformly selected from the set {0.1, 0.2, ..., 1.0} × 10−16. The maximum power
consumed by workers {𝑃𝑘 } is set as 8 W. Both decomposable models and DNN models are

trained using the PARTEL framework. The learning settings are as follows.

• Decomposable Model: A 𝐿1-regularized logistic regression task is considered, which trains a
news-ﬁltering model using the News20 dataset collected in [28]. The model size is 1.24×106.
The training and test datasets have 15936 and 3993 samples respectively. 𝐾 = 50 workers
with 𝑁 = 80 subcarriers are used to complete the task.

• DNN Model: The CNN model “LeNet-5” proposed in [25] is trained on the MNIST dataset.
In the “LeNet-5” model, there are are 60, 000 parameters in total. The method of auxiliary

variables in [6] is used to train the “LeNet-5” model at the PARTEL framework. In each
training iteration, a mini batch of 50 data samples is used. There are 469, 400 auxiliary
variables in total. 𝐾 = 30 workers with 𝑁 = 50 subcarriers are used to complete the task.

For comparison, three communication schemes are considered, described as follows.

• Joint SUPPORT: The joint schemes of SUPPORT proposed in Sections IV or V.

• Baseline: The number of parameters computed by each worker is ﬁrst allocated proportional

to their computation capacity. Then, the subcarriers are allocated, which is a special case

of the joint SUPPORT scheme.

• Greedy Scheme for FEEL: The training samples are equally distributed among workers.

Thereby, the computation latency and energy of each worker is determined. The subcarrier

allocation follows a greedy way. The subcarriers are randomly indexed and sequentially
allocated from the 1st to the 𝑁-th. The 𝑖-th subcarrier is allocated to the worker whose

latency is currently the longest. Note that the latency minimization of one worker given the

subcarrier allocation is simple and omitted.

25

(a) Training accuracy versus latency.

(b) Test accuracy versus latency.

Figure 2. Learning performance versus (communication-plus-computation) latency.

B. Decomposable Models

The learning performance of training the logistic regression model is compared in Fig. 2. As

observed, the model trained in PARTEL with the proposed joint SUPPORT converges much

faster than the one trained in FEEL with the greedy communication scheme, in which each

worker uploads the updates of all parameters. Besides, the joint SUPPORT outperforms the

baseline in terms of model convergence with a latency reduction of 31.06% on average. That’s

because the allocations of parameters and subcarriers are sequentially designed in the baseline.

Fig. 3 shows the impacts of number of workers and subcarriers on the per-round latency.

As observed, the per-round latencies of both schemes decrease as the number of workers or

subcarriers increases. The reasons are as follows. More workers can provide more computation

capacity and hence reduce the computation latency. Moreover, more subcarriers allocated to

workers can reduce the uploading latency.

C. DNN Models

The learning performance of training the LeNet-5 is compared in Fig. 4. In the ﬁgure, for

FEEL, the optimizer is Adam (proposed in [29]) and the corresponding learning rate is 0.002. For

PARTEL, the optimizer is SGD. The learning rates for updating weights and auxiliary variables

are 0.002 and 1, respectively. Although two stages (rounds) complete one training iteration in

PARTEL using the joint scheme, it outperforms the FEEL using the greedy scheme in terms

0100200300400500600Latency (s)0.50.550.60.650.70.750.80.850.90.951Training AccuracyPARTEL, Joint SUPPORTPARTEL, BaselineFEEL0100200300400500600Latency (s)0.750.760.770.780.790.80.810.820.830.84Testing AccuracyPARTEL, Joint SUPPORTPARTEL, BaselineFEEL26

(a) Effect of Number of Workers.

(b) Effect of Number of Subcarriers.

Figure 3. Latency performance versus (a) a varying number of workers and (b) subcarriers.

(a) Training accuracy versus latency.

(b) Test accuracy versus latency.

Figure 4. Learning performance versus (communication-plus-computation) latency.

of model convergence, as the latter has to upload the updates of all parameters in each round.

Besides, the joint SUPPORT can reduce latency by 42.11% compared to the baseline for the

similar reason in the decomposable model case.

The impacts of the number of workers and subcarriers on the latency performance of training

LeNet-5 is compared in Fig. 5. As shown in the ﬁgure, the latencies of the two schemes for

both W-stage and Z-stage decrease with the number of workers and subcarriers for the same

reasons in the case of decomposable models.

38404244464850Number of Workers0.750.80.850.90.9511.051.11.151.21.251.31.35Averaged One-Round Latency (s)Joint SUPPORTBaseline68707274767880Number of Subcarriers0.750.80.850.90.9511.051.11.151.21.251.31.35Averaged One-Round Latency (s)Joint SUPPORTBaseline050100150200250Latency (s)0.20.30.40.50.60.70.80.91Training AccuracyPARTEL, Joint SUPPORTPARTEL, BaselineFEEL050100150200250Latency (s)0.20.30.40.50.60.70.80.91Testing AccuracyPARTEL, Joint SUPPORTPARTEL, BaselineFEEL27

(a) Effect of Number of Workers.

(b) Effect of Number of Subcarriers.

Figure 5. Latency performance versus (a) a varying number of workers and (b) subcarriers.

The experimental results above show that our proposed joint scheme of SUPPORT has the

best performance regarding learning latency and veriﬁes our analysis.

VII. CONCLUDING REMARKS

In this paper, we have presented a set of algorithms for jointly controlling parameter and sub-

carrier allocation and power control, which can signiﬁcantly reduce the latency of PARTEL

deployed in a broadband system. This work opens several interesting directions. One is to

take the device scheduling into consideration for further accelerating the training process. In

the context of PARTEL, it is useful to jointly design the scheduler and parameter allocation.

Another interesting direction is to jointly control parameter allocation and local computation (e.g.,

processor speeds). In addition, the current joint design assuming OFDM can be extended to other

advanced communication techniques such as non-orthogonal multi-access, massive MIMO, and

over-the-air aggregation.

A. Proof of Lemma 2

APPENDIX

KKT conditions are used to show Lemma 2. The Lagrange function of Problem (P1) is in

(48), where {𝜇𝑛}, 𝜆 ≥ 0, {𝜈𝑘 ≥ 0}, {𝛼𝑘 ≥ 0}, and {𝛽𝑘,𝑛 ≥ 0} are multipliers.

202224262830Number of Workers00.10.20.30.40.50.60.70.80.911.11.21.31.4Averaged One-Round Latency (s)Joint SUPPORT, W-StageBaseline, W-StageJoint SUPPORT, Z-StageBaseline, Z-Stage404244464850Number of Subcarriers00.10.20.30.40.50.60.70.80.911.11.2Averaged One-Round Latency (s)Joint SUPPORT, W-StageBaseline, W-StageJoint SUPPORT, Z-StageBaseline, Z-Stage𝑁
∑︁

𝑛=1

𝛼𝑘

L =𝑇 +

+

𝐾
∑︁

𝑘=1

𝑘=1
(cid:19)

− 𝑃𝑘

+

(cid:18) 𝐸𝑘
𝑇𝑘

(cid:32)

𝜇𝑛

1 −

𝐾
∑︁

(cid:33)

(cid:32)

𝐶𝑘,𝑛

+ 𝜆

𝐿 −

(cid:33)

𝐿 𝑘

+

𝐾
∑︁

𝑘=1

𝐾
∑︁

𝑘=1

(cid:32)

𝜈𝑘

𝐿 𝑘 −

𝑁
∑︁

𝑛=1

(cid:33)

𝐶𝑘,𝑛𝐿 𝑘,𝑛

𝐾
∑︁

𝑁
∑︁

𝑘=1

𝑛=1

𝛽𝑘,𝑛𝐶𝑘,𝑛

(cid:16)
𝑇 cmp
𝑘

+ 𝑇 com

𝑘,𝑛 − 𝑇

(cid:17)

,

28

(48)

Then, consider an arbitrary subcarrier-allocation scheme {𝐶𝑘,𝑛}, KKT conditions are necessary

to solve the problem. Some related KKT conditions are given below:

𝜕L
𝜕𝑇

= 1 − 𝐶𝑘,𝑛 𝛽𝑘,𝑛 = 0,

1 ≤ 𝑘 ≤ 𝐾,

𝛽𝑘,𝑛𝐶𝑘,𝑛

(cid:16)
𝑇 cmp
𝑘

+ 𝑇 com

𝑘,𝑛 − 𝑇

(cid:17)

= 0, ∀(𝑘, 𝑛),

(49)





From the ﬁrst condition in (49), we can show that {𝛽𝑘,𝑛 ≠ 0, ∀𝐶𝑘,𝑛 = 1}, which, together
with the second condition in (49), can show that {𝑇 cmp
𝑘,𝑛 = 𝑇, ∀𝐶𝑘,𝑛 = 1}. Note that the
above condition is necessary for arbitrary subcarrier-allocation schemes. Hence, it is a necessary

+ 𝑇 com

𝑘

condition to solve (P1).

B. Proof of Lemma 3

First, we show that the equality of the third and forth constraints in Problem (P3) should be

achieved. The Lagrange function of (P3) is

𝐾
∑︁

𝑁
∑︁

𝜇𝑛

(cid:32) 𝐾
∑︁

𝐿 𝑘 +

L =

(cid:33)

∑︁ 𝜆𝑘

+

(cid:34)

𝐿 𝑘 −

𝐶𝑘,𝑛 − 1

𝑘=1
𝑛=1
𝑘=1
∑︁ 𝜈𝑘 (𝐸𝑘 − 𝑃𝑘𝑇) ,

+

𝐶𝑘,𝑛𝑅𝑘,𝑛
𝜏

(cid:18)
𝑇 −

𝐿 𝑘
𝑓𝑘

(cid:19) (cid:35)

𝑁
∑︁

𝑛=1

(50)

where {𝜇𝑛}, {𝜆𝑘 ≥ 0}, and {𝜈𝑘 ≥ 0} are multipliers. Using KKT conditions and the similar
approaches in Appendix A, we can show that {𝜆𝑘 ≠ 0, 1 ≤ 𝑘 ≤ 𝐾 } and {𝜈𝑘 ≠ 0 1 ≤ 𝑘 ≤ 𝐾 }

{𝑅∗

and the equalities of the third and forth constraints in Problem (P3) should be achieved.
Then, consider 𝑇1 < 𝑇2. When 𝑇 = 𝑇1, denote the optimal solution of (P3) as {𝐶∗
𝑘,𝑛,1}, and the maximum updatable model size as 𝐿∗(𝑇1).
Next, let 𝑇 = 𝑇2, {𝐶𝑘,𝑛,2 = 𝐶∗

𝑘,1 into
the third and forth conditions in Problem (P3), the equalities are not achieved. This shows that
the updatable number of parameters by each worker, denoted as {𝐿 𝑘,2}, can be larger, i.e.,
𝑘,1. It follows that (cid:205)𝐾
𝑘,1 = 𝐿∗(𝑇1). Furthermore, the optimal solution
𝐿∗
𝐿 𝑘,2 > 𝐿∗
𝑘=1
for 𝑇 = 𝑇2 satisﬁes 𝐿∗(𝑇2) ≥ (cid:205)𝐾
𝑘=1

𝐿 𝑘,2 > (cid:205)𝐾
𝑘=1
𝐿 𝑘,2. Hence, we have 𝐿∗(𝑇2) > 𝐿∗(𝑇1).

𝑘,𝑛,1}. By substituting 𝐿 𝑘,1 = 𝐿∗

𝑘,𝑛,1}, and {𝑅𝑘,𝑛,2 = 𝑅∗

𝑘,𝑛,1}, {𝐿∗

𝑘,1},

C. Proof of Lemma 4

First, the third constraint in Problem (P3), by dividing (𝑇 − 𝐿 𝑘 / 𝑓𝑘 ) on both sides and sub-

29

stituting the variable transformations in (31), can be derived as the third constraint in Problem
˜𝑅𝑘,𝑛/𝜏 ≥ 𝑓𝑘 (𝑇 𝜑𝑘 − 1) , 1 ≤ 𝑘 ≤ 𝐾(cid:9). Obviously, the feasible region of the above
(P4): (cid:8)(cid:205)𝑁
𝑛=1
constraint is a convex set. Then, by substituting 𝐸𝑘 in (29), dividing

on both sides,

(cid:18)
𝑇 −

(cid:19)

𝐿 𝑘
𝑓𝑘

and substituting the variable transformations in (31), the forth constraint in Problem (P3) can be

equally derived as the forth constraint in Problem (P4):

𝑁
∑︁

𝐶𝑘,𝑛𝜎2 (cid:0)2

˜𝑅𝑘,𝑛
𝐵𝐶𝑘,𝑛 − 1(cid:1)

𝑛=1

ℎ𝑘,𝑛

+ 𝑔𝑘 𝑓 3

𝑘 (𝜑𝑘𝑇 − 1) ≤ (𝑃𝑘𝑇 − 𝜉)𝜑𝑘 , 1 ≤ 𝑘 ≤ 𝐾.

(51)

In (51), the ﬁrst term is a convex function as 𝑓 (𝑥, 𝑦) = 𝑥𝑒𝑦/𝑥 is convex. Thereby, the feasible

region of the constraint in (51) is a convex set. Besides, the objective function and other

constraints are convex. Thus, Problem (P4) is convex.

REFERENCES

[1] J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis, S.-L. Kim, and M. Debbah, “Communication-efﬁcient and distributed

learning over wireless networks: Principles and applications,” [online]. Available: https://arxiv.org/abs/2008.02608, 2020.

[2] M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G. Andersen, and A. Smola, “Parameter server for distributed machine learning,”

in Proc. of NIPS Workshop on Big Learning, (Lake Tahoe, USA), Dec. 2013.

[3] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, et al., “Communication-efﬁcient learning of deep networks from

decentralized data,” arXiv preprint arXiv:1602.05629, 2016.

[4] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, and D. Bacon, “Federated learning: Strategies for

improving communication efﬁciency,” arXiv preprint arXiv:1610.05492, 2016.

[5] M. Carreira-Perpinan and W. Wang, “Distributed optimization of deeply nested systems,” in Proc. Int. Workshop on Artif.

Intell. and Statist. (AISTATS), (Reykjavik, Iceland), April 2014.

[6] A. Choromanska, B. Cowen, S. Kumaravel, R. Luss, M. Rigotti, I. Rish, P. Diachille, V. Gurev, B. Kingsbury, R. Tejwani,

et al., “Beyond backprop: Online alternating minimization with auxiliary variables,” in Proc. Int. Conf. Mach. Learn.

(ICML), pp. 1193–1202, 2019.

[7] G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, “Toward an intelligent edge: Wireless communication meets

machine learning,” IEEE Commun. Magazine, vol. 58, pp. 19–25, Jan. 2020.

[8] G. Zhu, Y. Wang, and K. Huang, “Broadband analog aggregation for low-latency federated edge learning,” IEEE Trans.

Wireless Commun., vol. 19, pp. 491–506, Oct. 2019.

[9] M. M. Amiri and D. G¨und¨uz, “Machine learning at the wireless edge: Distributed stochastic gradient descent over-the-air,”

IEEE Trans. on Signal Process., vol. 68, pp. 2155–2169, 2020.

[10] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-the-air computation,” to appear in IEEE Trans.

Wireless Commun., 2020.

[11] Y. Du, S. Yang, and K. Huang, “High-dimensional stochastic gradient quantization for communication-efﬁcient edge

learning,” IEEE Trans. on Signal Process., vol. 68, pp. 2128–2142, 2020.

30

[12] E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Communication-efﬁcient on-device machine learning:

Federated distillation and augmentation under non-iid private data,” [online]. Available: https://arxiv.org/abs/1811.11479,

2018.

[13] H. H. Yang, Z. Liu, T. Q. S. Quek, and H. V. Poor, “Scheduling policies for federated learning in wireless networks,”

IEEE Trans. Commun., vol. 68, no. 1, pp. 317–333, 2020.

[14] J. Ren, Y. He, D. Wen, G. Yu, K. Huang, and D. Guo, “Scheduling in cellular federated edge learning with importance

and channel awareness,” to appear in IEEE Trans. Wireless Commun., 2020.

[15] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A joint learning and communications framework for federated

learning over wireless networks,” [online]. Available: https://arxiv.org/pdf/1909.07972.pdf, 2019.

[16] W. Shi, S. Zhou, and Z. Niu, “Device scheduling with fast convergence for wireless federated learning,” [online]. Available:

https://arxiv.org/pdf/1911.00856.pdf, 2019.

[17] J. Ren, G. Yu, and G. Ding, “Accelerating DNN training in wireless federated edge learning system,” [online]. Available:

https://arxiv.org/pdf/1905.09712.pdf, 2019.

[18] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy efﬁcient federated learning over wireless

communication networks,” [online]. Available: https://arxiv.org/pdf/1911.02417.pdf, 2019.

[19] Q. Zeng, Y. Du, K. Huang, and K. K. Leung, “Energy-efﬁcient resource management for federated edge learning with

cpu-gpu heterogeneous computing,” [online]. Available: https://arxiv.org/abs/2007.07122.pdf, 2020.

[20] X. Mo and J. Xu, “Energy-efﬁcient federated edge learning with joint communication and computation design,” [online].

Available: https://arxiv.org/abs/2003.00199.pdf, 2020.

[21] D. Wen, M. Bennis, and K. Huang, “Joint parameter-and-bandwidth allocation for improving the efﬁciency of partitioned

edge learning,” to appear in IEEE Trans. Wireless Commun., 2020.

[22] C. Y. Wong, R. S. Cheng, K. B. Lataief, and R. D. Murch, “Multiuser OFDM with adaptive subcarrier, bit, and power

allocation,” IEEE J. Sel. Areas Commun., vol. 17, no. 10, pp. 1747–1758, 1999.

[23] D. W. K. Ng, E. S. Lo, and R. Schober, “Energy-efﬁcient resource allocation in multi-cell OFDMA systems with limited

backhaul capacity,” IEEE Trans. Wireless Commun., vol. 11, no. 10, pp. 3618–3631, 2012.

[24] J. Jang and K. B. Lee, “Transmit power adaptation for multiuser OFDM systems,” IEEE J. Sel. Areas Commun., vol. 21,

no. 2, pp. 171–178, 2003.

[25] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proceedings

of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.

[26] C. You, K. Huang, H. Chae, and B.-H. Kim, “Energy-efﬁcient resource allocation for mobile-edge computation ofﬂoading,”

IEEE Trans. Wireless Commun., vol. 16, no. 3, pp. 1397–1411, 2016.

[27] S. Burer and A. N. Letchford, “Non-convex mixed-integer nonlinear programming: A survey,” Surveys in Operations

Research and Management Science, vol. 17, no. 2, pp. 97–106, 2012.

[28] K. Lang, “Newsweeder: Learning to ﬁlter netnews,” in Mach. Learn. Proc. 1995, pp. 331–339, Elsevier, 1995.

[29] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Int. Conf. on Learn. Repr. (ICLR), (San Diego,

USA), May 2015.

