Cavs: A Vertex-centric Programming
Interface for Dynamic Neural Networks

Hao Zhang†, Shizhen Xu†, Graham Neubig, Wei Dai,
Qirong Ho, Guangwen Yang, Eric P. Xing

Carnegie Mellon University, Tsinghua University, Petuum Inc.
† indicates equal contributions

7
1
0
2

c
e
D
1
1

]

G
L
.
s
c
[

1
v
8
4
0
4
0
.
2
1
7
1
:
v
i
X
r
a

Abstract
Recent deep learning (DL) models have moved beyond
static network architectures to dynamic ones, handling data
where the network structure changes every example, such
as sequences of variable lengths, trees, and graphs. Ex-
isting dataﬂow-based programming models for DL—both
static and dynamic declaration—either cannot readily ex-
press these dynamic models, or are inefﬁcient due to re-
peated dataﬂow graph construction and processing, and dif-
ﬁculties in batched execution. We present Cavs, a vertex-
centric programming interface and optimized system im-
plementation for dynamic DL models. Cavs represents dy-
namic network structure as a static vertex function F and
a dynamic instance-speciﬁc graph G, and performs back-
propagation by scheduling the execution of F following the
dependencies in G. Cavs bypasses expensive graph construc-
tion and preprocessing overhead, allows for the use of static
graph optimization techniques on pre-deﬁned operations in
F, and naturally exposes batched execution opportunities
over different graphs. Experiments comparing Cavs to two
state-of-the-art frameworks for dynamic NNs (TensorFlow
Fold and DyNet) demonstrate the efﬁcacy of this approach:
Cavs achieves a near one order of magnitude speedup on
training of various dynamic NN architectures, and ablations
demonstrate the contribution of our proposed batching and
memory management strategies.

Introduction

1.
Deep learning (DL), which refers to a class of neural net-
works (NNs) with deep architectures, is now a workhorse
powering state-of-the-art results on a wide spectrum of tasks
[36, 61, 63]. One reason for its widespread adoption is the
variety and quality of software toolkits, such as Caffe [30],
TensorFlow [1] and DyNet [38, 39], which ease program-
ming of DL models, and speed computation by harnessing
modern computing hardware (e.g. GPUs), software libraries
(e.g. CUDA, cuDNN [9]), and compute clusters [11, 65, 66].
One dominant paradigm in the training of DL models,
adopted by toolkits such as Caffe and TensorFlow, uses static

dataﬂow graphs [1, 37]. These graphs represent the ﬂow of
data through computational functions, and are deﬁned using
symbolic programming [1, 5], once before beginning train-
ing or testing of the model. The training of these models is
performed through auto-differentiation, in which users are
only required to assemble their model architectures by con-
necting operators using high-level language interface (e.g.
Python), after which the framework will automatically de-
rive the correct algorithm for training [4]. With proper op-
timization, the execution of these static dataﬂow graphs can
be highly efﬁcient. Speciﬁcally, by separating model decla-
ration and execution, it makes it possible for the graph to be
further processed and optimized before runtime [1]. In addi-
tion, the evaluation of multiple data samples in a dataﬂow
graph can be naturally batched to leverage the improved
computational capability of modern hardware (e.g. GPUs),
which is extremely advantageous for DL workloads [32].

While these static dataﬂow graph have major efﬁciency
advantages, their applicability highly relies on a key as-
sumption – the dataﬂow graph (i.e. NN architecture) ﬁxed
throughout the runtime. With the increasing complexity of
the problems to be addressed, DL has been extended and
applied on data with more complicated structures, such as
sequences [28, 49], trees [50] and graphs [33], over which
the NN may conditionally choose its own computation order
for speciﬁc modeling needs, i.e. the structure of its dataﬂow
graph changes over training. To better support these dynamic
models, some recent frameworks [38, 56] propose to de-
clare a dataﬂow graph per sample (a.k.a. dynamic declara-
tion). While dynamic declaration is convenient to develop-
ers as code can basically be written in the same way as it
usually is in the native programming language (e.g. Python,
C++), it exhibits a few limitations. First, programmers still
have to write code to explicitly assemble the dataﬂow graph
for each input sample, which might be nontrivial for graphs
with sophisticated structures. Second, as the graph construc-
tion needs to be performed repeatedly, its overhead grows
linearly with the number of training instances, preventing
the application of complex static graph optimization tech-

1

 
 
 
 
 
 
niques (in fact, graph construction takes longer time than
the computation in some frameworks [34], see §5.2). Fi-
nally, since each sample owns a dataﬂow graph specifying
its unique computational pattern, batching together similarly
shaped computations across instances is non-trivial. Without
batching operations, the computation is inefﬁcient due to its
lack of ability to exploit modern computational hardware,
and while some progress has been made in recent research
[34, 39], how to automatically batch the computational op-
erations from different graphs remains a difﬁcult problem.

To address these challenges, we present Cavs, a new pro-
gramming interface for dynamic NNs, and a system imple-
mentation with optimization strategies tailored to it. Cavs
leverages the recurrent and recursive nature of dynamic
NNs. Instead of declaring a dataﬂow graph per sample,
it alternatively decomposes a dynamic dataﬂow graph as
two components: one static vertex function F that is only
declared (by the user) and optimized once, and an input
graph G that is instance-speciﬁc and not used until runtime.
Thereby, the workﬂow of training a dynamic NN can be
represented as scheduling the execution of F following the
structure of the input graph G. Cavs combines the best of
symbolic construction of dataﬂow graphs for DL [1, 5] with
the vertex-centric model [19] in graph computing: it only
requires users to deﬁne F symbolically by “thinking locally
like a vertex” [55]. Cavs will perform auto-differentiation,
schedule the function execution following the dependency
reﬂected by G, and guarantee efﬁciency and correctness. It
also inherits the ﬂexibility of symbolic programming, i.e.
users are allowed to declare multiple vertex functions to ex-
press more dynamics, or connect static dataﬂow graphs with
dynamic ones to construct more complex NN architectures.
Cavs demonstrates a few advantages over other program-
ming models. It simpliﬁes user programs and avoids the
overhead of repeated dataﬂow graph construction. More-
over, this vertex-centric model naturally exposes opportuni-
ties for batched computation: we introduce a simple batching
policy in Cavs’ scheduler to parallelize the execution of F on
multiple vertices during the evaluation of a batch of samples
with different input graphs (§3.2), and a novel memory man-
agement mechanism to guarantee the memory coalescing
(§3.3). Together they yield signiﬁcant performance improve-
ments. Compared to dynamic declaration, as the dataﬂow
graph encoded by the vertex function is static throughout the
runtime, it can beneﬁt from various static graph optimiza-
tions [1, 8, 18, 20], such as lazy batching, streaming, and
kernel fusion (§3.5), which would otherwise be less effec-
tive on the scenario of dynamic declaration because of the
repeated preprocessing/optimization cost (see §6).

We implement Cavs as an additional layer pluggable to
most existing DL frameworks to enhance their support for
dynamic NNs. To evaluate its performance, we compare
Cavs to TensorFlow Fold [34] and DyNet [38, 39], two state-
of-the-art systems supporting dynamic NNs and dynamic

batching. We focus our experiments on GPU training, and
verify that both Fold and DyNet suffer from substantial over-
head caused by repeated graph preprocessing or construc-
tion, which is bypassed by Cavs (§5.2). In terms of overall
performance, on static NNs, Cavs demonstrates equivalent
or slightly better performance than Fold and DyNet, while on
several dynamic NNs with notably difﬁcult-to-batch work-
loads (e.g. Tree-LSTM [50] and Tree-FC [34]), Cavs demon-
strates near one order of magnitude speedups across various
dataset and hyper-parameter settings (§5.1). We further in-
vestigate the key contributing factors to the performance:
Cavs beneﬁts from not only a better memory management
strategy, but also graph execution optimizations which are
originally designed for static dataﬂow graphs and perhaps
less useful in dynamic declaration.

2. Background
DL is distinguished from other ML algorithms mainly by
its use of deep neural networks, a family of ML models
with many interconnected layers, each composed of vari-
ous mathematical operations (e.g. +, −, sigmoid, matmul).
Before a DL model can give predictions,
is usually
trained by stochastic gradient descent, an iterative process
in which gradients are calculated through backpropaga-
tion [44]. There is a natural connection between directed
graphs and NNs: we can map the graph nodes to the compu-
tational operations or parameters in NNs, and let the edges
indicate the direction of the data being passed between the
nodes. In this case, we can represent the process of train-
ing NNs as batches of data ﬂowing through computational
graphs, i.e. dataﬂow graphs [1, 5, 38].

it

k}K

Figure 1(a) summarizes the programming model derived
from these dataﬂow graphs, which is named as static decla-
ration and has been adopted in many DL frameworks [1, 5,
8]. Without ambiguity, we use D to denote both the dataﬂow
graph itself and the computational function implied by D.
On one hand, we note its execution is highly efﬁcient as the
computation over multiple samples is batched – at each iter-
ation p, a batched tensor of K samples {xp
k=1 is fed to D,
and the computation is executed in a single pass, allowing for
efﬁcient use of memory caches or parallelized computation.
On the other hand, this paradigm relies on a key assump-
tion: the dataﬂow graph D is static for all samples and ﬁxed
throughout the computation. Hence, D will only be declared
once with a constant graph construction/optimization over-
head; all samples share the same computational pattern spec-
iﬁed in D, so the computation of different samples can be by
nature batched by simply expanding the input with a batch
dimension. Though static declaration is effective on a wide
range of NN models, such as convolutional neural networks
(CNNs) over ﬁxed-size images, it is much more difﬁcult to
apply to graphs with dynamically changing structures, some
examples of which are shown in the next section.

2

/* (a) static declaration */
// all samples must share one graph
declare a static dataﬂow graph D.
for p = 1 → P :
read the pth data batch {xp
batched computation: D({xp

k}K
k=1.
k}K
k=1).

/* (b) dynamic declaration */
for p = 1 → P :
read the pth data batch {xp
for k = 1 → K:

declare a dataﬂow graph Dp
single-instance computation: Dp

k}K

k=1.
k for xp
k.
k(xp
k).

/* (c) our proposed vertex-centric model */
declare a symbolic vertex function F .
for p = 1 → P :
read the pth data batch {xp
k=1.
read their associated graphs {Gp
compute F over {Gp

k=1 with inputs {xp

k }K

k}K

k }K

k=1.

k}K

k=1.

Figure 1: The workﬂows of (a) static declaration, (b) dynamic declaration, (c) Cavs’ vertex-centric programming model.

Model

Frameworks

Expressiveness Batching

static declaration

dynamic declaration
(instant evaluation)
dynamic declaration
(lazy evaluation)
Fold
Vertex-centric

Caffe, Theano,
TensorFlow, MxNet

PyTorch, Chainer

DyNet

TensorFlow-Fold
Cavs

×

√

√

√
√

×

×

√

√
√

Graph Cons.
Overhead

Graph Exec.
Optimization

low

N/A

high

high
low

beneﬁcial

unavailable

not beneﬁcial

unknown
beneﬁcial

Figure 2: Left (a)-(d): A cell function shown in (a) could be applied on different structures such as a (b) chain (c) tree, or (d) graph. Right
table: the landscape of existing programming models for dynamic NNs, and their advantages and disadvantages (see §2.2 and §6).

2.1 Dynamically Structured Computational Graphs

Modern DL has been developed and applied extensively
over data with more complicated structures, e.g. data struc-
tured as sequences, trees and graphs, which are required to
tackle practical problems such as machine translation [49,
50], questionanswering [51], and semantic image segmen-
tation [33, 62]. As a concrete example of dynamic NNs, we
will use recurrent and recursive neural networks (RNNs) [16,
28, 46]. RNNs are a class of NNs generally applied on mod-
eling structured inputs or outputs, e.g. sequences or graphs.
At the core of an RNN is a cell function with trainable pa-
rameters. It will be dynamically applied at different places
of the input structure, and optionally produce an output if
needed. Figure 2(a) illustrates such a cell function: it takes
an input element x, forwards it through a few mathemati-
cal transformations, and generates some intermediate state
h and an output o. Depending on what transformations are
applied, different variants of RNNs have been derived, such
as long-short term memory units (LSTM) [28] and gated re-
current units (GRU) [10]. However, the internals of the cells
themselves are secondary; the dynamics of the net as a whole
are mainly reﬂected by the structures that the NN works on.
Sequence RNNs. When the input to the RNN are sequences
(e.g. sentences) as in Figure 2b, the cell function is applied
across all elements of the sequence. At each step t, it takes
the element xt (e.g. a word) from the input sequence, and the
state variable ht−1 maintained by the model at step t − 1. It
computes an output ot, and a new state ht that will be used
by the next step t + 1. Hence, This sequence RNN encodes
not only the data values, but also the dependencies present in
the sequence. If represented as a dataﬂow graph, the graph
exhibits a chain structure. As the input or output sequences
usually have variable length (e.g. translating an arbitrary-
length English sentence into Chinese), the dataﬂow graph
needs to be dynamically changed, i.e. the steps of the chain
must be adapted to ﬁt the length of the input or output.
Tree-structured RNNs. Further, RNNs can be enhanced to
model data with more complex structures suited for down-

stream tasks. For example, tree-structured RNNs (Tree-
RNNs, Figure 2c), have been used to classify the sentiment
of sentences [41] given an associated binary tree represent-
ing the sentence structure [45, 50]. In this case, a leaf of the
tree maps to a word of the sentence, an internal node cor-
responds to a multi-word phrase. To process this structure,
the cell function scans the tree recursively, starting from leaf
nodes until reaching the root. At the node t, it computes
the state ht = f (htl, htr , xt), where xt is the input to the
node, and htl , htr are the states of its left and right children,
respectively. As the tree structure vary from example to ex-
ample, the dataﬂow graph of a Tree-RNN is highly dynamic.
Graph-structured RNNs. Similarly, RNNs can be extended
to compute over more general graphs, such as N-ary trees or
graphs (Figure 2d), as long as their parameters are learn-
able. In fact, various NNs have been developed toward hav-
ing more dynamic workﬂows [33, 50], and proven quite ef-
fective because of their ability to encode structured infor-
mation. While we take RNNs as examples for explanation,
we note there are many other dynamic NNs in the literature
or production with their dynamics reﬂected in various per-
spectives: variably sized inputs/outputs [3, 6, 14, 16, 28, 49],
variably structured inputs/outputs [33, 45, 50], or with non-
trivial inference algorithms [21, 23, 31, 67].

2.2 Programming Dynamic Dataﬂow Graphs

As the assumption in §2 no longer holds for dynamic struc-
tures, static dataﬂow graphs in their original form cannot
be used. There are currently two remedies to this problem:
expanding the graph programming language to allow it to
explicitly include controls structure necessary to implement
these applications, or forgo the efﬁciency gains afforded by
static dataﬂow graphs and instead use a dynamic declaration
framework that reconstructs the graph for every training ex-
ample. We explain below and summarize them in Figure 2.
Static declaration. Static unrolling [1] is a standard way to
express sequence RNNs with ﬁxed steps. To handle variable-
length data, it declares an RNN that has number of steps
equal with the length of the longest sequence in the dataset.

3

SequenceTreeCellFunctionGraphVUoWhx(a)(b)(c)(d)It then appends zeros at the end of other sequences to have
equal length, and feeds them in batches to the dataﬂow graph
for computation. Static unrolling enables batched computa-
tion of multiple sequences, but obviously results in substan-
tial unnecessary computation.1 Dynamic unrolling imple-
ments basic control ﬂow functionality within static graphs,
allowing for the declaration of graph operators similar to
while loops. At each iteration of the training, the cell func-
tion of the RNN will be executed a conditional number of
times determined at runtime by the length of the longest sen-
tence in the batch. It then pads the sequences in the batch
and perform batched computation, it waste computational
resources. Notably, both of these methods essentially cannot
support more complex structures than sequences.
Dynamic declaration. Dynamic declaration is able to ex-
press dynamically varying dataﬂow graphs, by creating a
unique dataﬂow graph for each sample according to its asso-
ciated structure. It however requires users to explicitly write
(more) code to build a dataﬂow graph for each input graph,
which is nontrivial for graphs with sophisticated structures.
As the dataﬂow graphs are always changing, it can hardly
beneﬁt from well-established dataﬂow graph optimization
techniques (§3.5) – we will have to perform graph process-
ing/optimization for each dataﬂow only for a single sam-
ple, but incorporating the optimization itself has an over-
head. More importantly, as we are unable to naturally batch
the computation of different sample, single-instance training
would be very inefﬁcient in the absence of batched compu-
tation. At the backend, since a dataﬂow graph needs to be
constructed per sample, the overhead is linearly increasing
with the number of samples, and sometimes yields down-
graded performance [34] (§5.2), even for frameworks with
optimized graph construction implementations [38].

Tensorﬂow Fold [34] and DyNet [39] go one step further
and perform dynamic batching for dynamic dataﬂow graphs.
Fold turns dynamic dataﬂow graphs into a static control ﬂow
graph to enable batched execution, but introduces a compli-
cated functional programming-like languages and a large
graph preprocessing overhead. DyNet proposes an auto-
batching strategy that searches for batching opportunities
by proﬁling every ﬁne-grained operator, while this step it-
self has non-negligible overhead, and loses the opportunities
of graph-level optimizations. There are also some “imper-
ative” frameworks, such as PyTorch [17] and Chainer [57]
that allow users to construct dynamic NNs but performs in-
stant evaluation of each user expression. As model construc-
tion and execution are coupled, they are usually difﬁcult to
perform dynamic batching or graph optimization. Overall,
they are still far from efﬁcient when handling dynamic NNs.
We next describe our proposed vertex-centric programming
model to overcome the aforementioned limitations.

1 It is also possible to split sentences into several buckets of different
lengths, which alleviates this problem somewhat but adds some degree of
code complexity and is not a fundamental solution.

4

3. Cavs Design and Optimization
Our motivation comes from several key principles ML devel-
opers usually comply with to ensure the feasibility and learn-
ability of the model during their design of dynamic NNs. We
note most dynamic NNs are designed to exhibit a recursive
structure (e.g. sequence RNN, Tree-RNN), or a combination
of static and recursive structures (e.g. LRCN [2, 13], atten-
tion [60]), or even a combination of different recursive struc-
tures (e.g. encoder-decoder RNNs [49]). Within one such
structure, a function is dynamically applied over instance-
speciﬁc graphs, and every vertex of the graph usually inter-
acts in a same way with it neighboring vertices following the
function. The computational function itself, however, is usu-
ally static and parameterized by ﬁxed learnable parameters.
This observation motivates us to design a new program-
ming model, called Cavs, that combines the best of dataﬂow
graphs with the vertex-centric model in graph computing. As
a comparison, we present the workﬂow of Cavs in Figure 1c.
For clarity, we will use the following terminology and nota-
tion in the rest of the paper: we call the instance-speciﬁc
structure associated with the input sample as an input graph,
and notate it as G, and a node in that graph as a vertex, to
be distinguished from a dataﬂow graph D and the nodes
(which are usually operators or variables) therein. Figure 3
illustrates the concept of this vertex-centric programming
model. To describe an aforementioned dynamic structure,
different from dynamic declaration, which requires users to
manually declare dataﬂow graphs for each sample accord-
ing to its associated graph, Cavs instead directly takes it as
an input argument. To be aware of what computation shall be
performed, Cavs requires users to implement a simple ver-
tex function F by “thinking like a vertex”, informing the
framework how one vertex in a dynamic NN will interact
with its connected vertices (if these is any). In F, users can
utilize conventional DL operators to assemble a symbolic
construct that will be evaluated dynamically following the
structure of G, while Cavs will ensure the correctness and
efﬁciency. Therefore, a vertex function F, together with an
input graph G, implicitly encodes a recurrent dataﬂow graph,
which maps to a subgraph of the implicit full dataﬂow graph
of the model that may needs to be explicitly declared in tra-
ditional programming models. For convenience of notations,
we will call any part of the structure that cannot be encoded
by F and G as external to (F, G), and vice versa. Cavs al-
lows users to connect any external static dataﬂow graph to
a dynamic structure encoded by (F, G) to express various
model architectures (e.g. connecting a CNN to an RNN), or
declare multiple vertex functions for different structures, and
connect them appropriately to express more complex models
(e.g. an encoder-decoder LSTM network).

While it is still necessary to create an I/O function to read
input graphs for each sample, this must be done in any mod-
els, and only once before training commences, which means
that it can be shared across epochs or even training runs.

1 def F ():
2

for k in range(N):

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

S = gather(k) # gather states of child vertices
ck, hk = split(S, 2) # get hidden states c and h

x = pull() # pull the first external input x

# specify the computation
h = (cid:80)N −1
i = sigmoid(W(i)× x + U(i)× h + b(i))
for k in range(N):

k=0 hk

fk = sigmoid(W(f )× x + U(f )× hk + b(f ))

o = sigmoid(W(o)× x + U(o)× h + b(o))
u = tanh(W(u)× x + U(u)× h + b(u))
c = i ⊗ u + (cid:80)N −1
h = o ⊗ tanh(c)

k=0 fk ⊗ ck

scatter(concat([c, h], 1)) # scatter c, h to parent vertices
push(h)

# push to external connectors

Figure 4: An N-ary child-sum TreeLSTM [50] in Cavs.
graph computing, a vertex of a graph always interacts with
other vertices of this graph, while in DL, the vertex of a dy-
namic NN usually takes input from not only the internal of
the structure expressed by F and G (internal data path in
Figure 3), but also from the external of it, e.g. a step in an
RNN can take inputs from a CNN feature extractor or some
external I/O (external data path in Figure 3). In this case,
gather and scatter are insufﬁcient to express such seman-
tics. Cavs therefore provides another two APIs:

• pull(): pull grabs inputs from the external of the cur-
rent dynamic structure, e.g. another NN, or some I/O.
• push(op): push is thus the reverse of pull that sets the
output of the current vertex as op. If this vertex is pulled
by others, the content of op will be returned.

With appropriate indexing, push and pull connect a vertex
inside a dynamic structure expressed by (F, G) to other con-
nectors external to (F, G), such as another dynamic struc-
ture, or another static dataﬂow graph. With these four APIs,
we present in Figure 4 an example user program how the N -
ary child-sum Tree-LSTM [50] can be simply expressed by
using them and other mathematical operators.
Expressiveness. With these four APIs, Cavs can be seen as
a middle ground between static and dynamic declaration: In
the best case, the model can be easily represented by a single
vertex function plus input graphs. While in the worse case
scenario, that every sample has a unique input graph while
every vertex in the graph has a unique way to interact with its
neighboring vertices, Cavs reduces to dynamic declaration
that one has to deﬁne a vertex function for each vertex of
input graphs. However, dynamic NNs in this scenario are
very rare and usually not preferred because of the difﬁculty
of design, programming and learning.

3.2 Scheduling
Once users deﬁne the vertex function F and launch the
execution, the Cavs scheduler arranges the evaluation of F
over the input graphs. Given F, Cavs’s scheduler follows
designed policies to efﬁciently perform backpropagation for
all samples {xi}N
Backpropagation. Cavs performs backpropagation [27] as
follows. For a sample xi with its input graph Gi, the sched-

i=1 and their associated graphs {Gi}N

i=1.

Figure 3: Cavs represents a dynamic structure as a dynamic input
graph G (left) and a static vertex function F (right).

Cavs no longer requires users to construct the full dataﬂow
graphs for each sample by themselves. As repeated graph
construction is bypassed, the overhead will also be avoided.
With this vertex-centric model, Cavs transforms the problem
of evaluating multiple dataﬂow graphs with different struc-
tures [34, 39] into a simpler form – scheduling the execution
of the vertex functions following the input graphs. For the
later problem, we can easily batch the execution of F over
multiple vertices at runtime (§3.2), leveraging the batching
computational capability of modern hardware. Moreover, as
the vertex function itself maps to a static symbolic dataﬂow
graph, it is open and can beneﬁt from various graph opti-
mization techniques originally developed for static declara-
tion, such as kernel fusion, streaming, and our proposed lazy
batching, which might not be effective in the scenario of dy-
namic declaration. We next describe Cavs’ APIs.

3.1 Programming Interface

Besides the generic math operators used to declare the com-
putation, Cavs exposes four symbolic APIs for users to spec-
ify how the messages shall be passed between vertices in
their vertex functions: gather, scatter, pull, push.

• gather(child idx): gather accepts an index of the
child vertices, gets the child content from gather/scatter
buffer and returns a list of symbols that represent the
output of these vertices.

• scatter(op): scatter is a reverse API of gather, and
has a symbol op as its input argument. Scatter will set the
output of current vertex to gather/scatter buffer.

gather and scatter resemble the GAS model in graph
computing [19] – both are vertex-centric APIs that help users
express the overall computational patterns by thinking lo-
cally like a vertex: gather receives messages from depen-
dent vertices, while scatter updates information to parent
vertices. But note several key differences: (1) gather and
scatter here are fully symbolic – gather allows back-
propagation through it; (2) In graph computing, all nodes
interact with their connected nodes in the same way follow-
ing a user-speciﬁed apply function, while in dynamic NNs,
a vertex usually interacts differently with its different child
vertices, speciﬁed by the symbolic programs (between the
call of gather and scatter) in the vertex function; (3) In

5

gatherscatterpushpullInternal Data PathExternal Data PathInputOutputChildsumLSTMh0c0h1c1xcf1f0ChainTreeu|iOLSTMht-1ct-1xtctfu|iOGRUht-1ct-1xtctfuhtOhthfOn Both Data Pathuler starts the forward pass from the input vertices of Gi, and
proceeds following the direction indicated by the edges in
Gi: at each sub-step, the scheduler ﬁgures out the next acti-
vated vertex in Gi, and evaluates F at this vertex following
the symbolic programs in F. It then marks this vertex as
evaluated, and proceeds with the next activated vertex until
reaching a terminal vertex (e.g. the loss function). A ver-
tex of G is activated if and only if all its dependent vertices
have been evaluated. The backward pass is continued right
after the forward. The scheduler ﬁrst resets the status of all
vertices as not evaluated, then scans the graph in a reverse
direction, starting from the ending point of the forward pass.
It similarly ﬁgures out the next activated vertex, but applies
another function ∂F, which is the backward function of F
and automatically derived by Cavs via auto-differentiation
(§3.4), until all vertices have been evaluated in backward.
To train a NN to convergence, the above process has to be

Algorithm 1 Backpropagation with the batching policy.

k=1, {Gk}K
set task ID t ← 0, task stack S ← ∅.

k=1, F )

k=1 are evaluated do

k=1 as a set Vt.

ﬁgure out all activated vertices in {Gk}K
push Vt into S.
evaluate F on Vt: GraphExecute(Vt, F ) (see §3.5).
set the status of all vertices in Vt as evaluated.
set t ← t + 1.
end while
return S.

1: function FORWARD({xk}K
2:
3: while NOT all vertices in {Gk}K
4:
5:
6:
7:
8:
9:
10:
11: end function
12: function BACKWARD(S, {Gk}K
13:
14: while S is not empty do
15:
16:
17:
18:
19: end function

set t as the size of S.

pop the top element of S as Vt.
Evaluate ∂F on Vt: GraphExecute(Vt, ∂F ) (§3.5).
set t ← t − 1.
end while

k=1, ∂F )

iterated by the scheduler over all samples {xi}N
i=1 and their
associated graphs {Gi}N
i=1, for many epochs. Instead a se-
quential execution, Cavs designs a batching policy to per-
form batched computation, considering the fact that evalu-
ating a set of same arithmetic operations together is signiﬁ-
cantly faster than the sequential evaluation of each of them.
Batching policy. Given a data batch {xk}K
k=1 ⊆ {xi}N
i=1
and associated graphs {Gk}K
k=1, this policy groups multiple
vertices and then performs batched evaluation of F in or-
der to reduce kernel launches and exploit parallelism. Algo-
rithm 1 details this policy. Speciﬁcally, the scheduler divides
the forward evaluation of (a batch of) K graphs into multiple
steps. At each step t, it analyzes {Gk}K
k=1 at runtime and de-
termines a set Vt contains all activated vertices in {Gk}K
k=1.
It then evaluates F over these vertices by creating a batching
execution task, with the task ID set to t2. The task is passed
to a graph execution engine, which will further optimize the

2 Without ambiguity, we use Vt to denote both the set of vertices to be
batched together, as well as the batching task itself.

6

execution and conduct the actual computation (§3.5). Mean-
while, the scheduler records the information of this task by
pushing Vt into a stack S. At each step of the backward, the
scheduler pops out an element Vt from S, creates a corre-
sponded backward batching task – the execution engine will
evaluate the derivative function ∂F over vertices in Vt, until
all vertices of {Gk}K

k=1 are evaluated.

We note the batching policy plays a similar role as the
dynamic batching in Fold [34] and DyNet [38] in the sce-
nario of dynamic declaration. However, Cavs determines
how to batch fully dynamically during runtime using sim-
ple breadth-ﬁrst search with negligible cost (instead of an-
alyzing graphs before every iteration of the execution). We
next describe an improved management management strat-
egy based on this batching policy.

3.3 Memory Management

In static declaration [1, 38], a symbol in the user program
usually corresponds to a tensor object, with its shape in-
ferred from the program and batch size speciﬁed in advance.
The framework usually preallocates a continuous storage on
memory for each tensor and ﬁxes it throughout runtime.
However, in Cavs, as each batching task is determined at
runtime and not visible before execution, its memory man-
agement exhibits more complexities. For the batched com-
putation to be efﬁcient, Cavs must guarantee the inputs and
intermediate states during the evaluation of F over a group
of runtime-determined vertices coalescing in memory. If we
adopt the aforementioned strategy, for each operation in F,
Cavs has to index each slice of its input tensor (which may be
scattered in different places) and rearrange them as a contin-
uous memory block, which might cause nontrivial overhead.
Cavs proposes a novel data structure dynamic tensor to
address this challenge (Figure 6). A dynamic tensor is a
wrapper of a multi-dimensional array [1, 59] that contains
four main members: shape, bs, a pointer p to a chunk of
memory, and offset. shape is an array of integers rep-
resenting the speciﬁc shape of the tensor excluding the
batch dimension. It can be inferred from the user pro-
gram and set before execution. The batch size bs is im-
plemented as a placeholder in Cavs, with its value dynam-
ically set by the scheduler at runtime at the beginning of
a batching task. For each dynamic tensor, Cavs will pre-
allocate a chunk of continuous memory, and point p to its
starting address. This memory block is often very large
and not ﬁxed-sized – it can be further extended if needed.
To access a dynamic tensor, the execution engine moves
p forward with the value speciﬁed in offset, and read-
s/writes number of elements equal to bs · (cid:81)
i shape[i].
Therefore, bs together with offset provide a view of
the tensor, and the state of the tensor will vary based on
their values. Given a vertex function F, Cavs creates dy-
namic tensors {αn}N
n=1 for each non-parameter symbol
sn(n = 1, . . . , N ) in F, and also {∇αn}N
n=1 as their gra-
dients, while it creates static tensors for model parameters.

Figure 5: A color or a dashed lined box corresponds to a batching task. The rectangles are memory blocks. The numbers are vertex IDs.
Memory blocks in one row belong to a dynamic tensor (e.g. α0 ) and are physically continuous, though we separate them in different boxes.

(cid:81)

Figure 6: Dynamic tensor.

struct DynamicTensor {
vector<int> shape;
int bs;
int offset;
void* p; };

Figure 5 illustrates how the mem-
ory is assigned during the forward
pass by simply manipulating dy-
namic tensors. In particular, in a
training iteration, for a batching
task Vt, the scheduler sets bs of all {αn}N
n=1 to Mt = |Vt|
(the number of vertices in Vt). The execution engine then
performs batched evaluation of each expression in F one
by one. For an expression sl = op(sr)3, the engine ﬁrst
reads αr (the dynamic tensor of the RHS symbol sr) by
offsetting αr.p by αr.offset then reading a block of
Mt
i αr.shape[i] elements, and presents it as a tensor with
batch size Mt and other dimensions speciﬁed in αr.shape.
It then applies batched computational kernels of the oper-
ator op over this continuous block, and writes the results
to αl (the dynamic tensor of the LHS symbol sl) on the
continuous block in between [αl.p + αl.offset, αl.p +
αl.offset + Mt
i αl.shape[i]]. Upon the completion of
Vt, the scheduler increases the offset of all {αn}N
n=1
by Mt
i αn.shape[i], respectively. It then starts the next
batching task Vt+1 until F has been evaluated at all vertices
of {Gk}K
k=1. Hence, intermediate results generated in each
batching task at forward pass are stored continuously in the
dynamic tensors, and their offsets are recorded.

(cid:81)

(cid:81)

The scheduler then starts the backward pass. It initial-
izes ∇αn.offset for each n as αn.offset. Since the back-
ward execution follows an exactly reverse order of the for-
ward pass (Algorithm 1), the intermediate results generated
during forward can be easily accessed by decreasing the
offset of {αn}N
n=1. Speciﬁcally, for each batching task
Vt popped out from S, the execution engine sets bs of all
dynamic tensors to Mt, and for each αn and ∇αn, de-
(cid:81)
creases their offset by Mt
i αn.shape[i]. For an expres-
sion ∇sr = grad op(∇sl, sl, sr) in ∂F that corresponds to
sl = op(sr) in F (see §3.4), the engine reads the current
states of ∇sl, sl, sr (which are continuous on memory) and
performs batched execution of grad op. Different from the
forward pass, the gradient result will be added to the current
state of ∇sr instead of overwriting.

3 Without losing generality the user-deﬁned expressions can be arbitrary,
e.g. with more than one argument or return values.

7

At the entrance of F, the vertices {vm}Mt

m=1 in Vt need
to interact with its dependent vertices in previous Vt−1 to
gather their outputs as inputs (L3 of Figure 4), or pull
inputs from the external (L5 of Figure 4). Cavs maintains
memory buffers to enable this (Figure 5). The memory
buffers are key-value stores where the key is the vertex ID
m and the value is a tensor slice corresponding to the re-
sults of the scattered symbol at vertex vm (batch size
is 1). Cavs provides a query function IndexBuffer(op,
key) that returns the value in op’s corresponded buffer
given a key. During the execution, a gather expression
sl = gather(child idx) will trigger memory movements:
for each vm ∈ Vt, the scheduler ﬁgures out the vertex ID of
its child vertex in the input graph with index child idx; it
then indexes the content in gatherBuffer with the vertex
ID as key, and copies and writes it into αl as a slice. Simi-
larly, at the exit of F, a scatter expression scatter(sr)
will split the current state of αr into a few slices, and move
them to the gatherBuffer for its parent nodes to gather.
The push/pull work similarly with gather/scatter, but
over the pushBuffer and pullBuffer, respectively, to
communicate messages with the external.

Algorithm 2 summarizes the memory management pro-
cess during forward pass. With this strategy, Cavs guarantees
memory continuity for any batched computation in F. Com-
pared to dynamic batching in DyNet, Cavs performs mem-
ory movement only at the entrance and exit of F, instead
of for each expression (operator), it therefore signiﬁcantly
reduces overhead by memory operations (§5.3).

3.4 Auto-differentiation

Cavs by nature supports auto-differentiation. Given a vertex
function F it derives ∂F following the auto-differentiation
rules: for each math expression such as sl = op(sr) in
F, Cavs generates a corresponded backward expression
in ∂F as ∇sr = grad op(∇sl, sl, sr). For the four pro-
posed operators, with the memory management strategy de-
scribed above, we note scatter is the backward operator
of gather in the sense that if gather collects inputs from
gatherBuffer previously written by scatter at the for-
ward pass, a scatter needs to be performed to write the
gradients to the gatherBuffer for its dependent vertices

def   ():    = pull()     = gather(0)    = gather(1)    = matmul(  , u)      +matmul(  , v)      +matmul(  , w) scatter(0,  ) push(  )00132456111213710891256789111031241398765210111031241369110212580731113121110987654321013121110987654321001256789012567893101101256789310114120125678931011412pullpushgatherscatterbatching taskgather/scatter bufferpush bufferpull buffer0123012330123Algorithm 2 Memory management at forward pass.

n=1, F )

t=1, {αn}N

i αl.shape[i], q ← αl.p + αl.offset.

1: function FORWARD({Vt}T
for t = 1 → T do
2:
for n = 1 → N do αn.bs ← Mt end for
3:
for each expression like sl = op(sr) in F do
4:
if op ∈ {gather, pull} then
5:
C ← (cid:81)
6:
for vm ∈ Vt(m = 1 → Mt) do
7:
src ← IndexBuffer(op, m), dest ← q + (m − 1)C.
8:
memcpy(dest, src, C).
9:
end for
10:
else if op ∈ {scatter, push} then
11:
C ← (cid:81)
12:
for vm ∈ Vt(m = 1 → Mt) do
13:
dest ← IndexBuffer(op, m), src ← q + (m − 1)C.
14:
memcpy(dest, src, C).
15:
end for
16:
else
17:
perform batched computation: αl = op kernel(αr).
18:
end if
19:
end for
20:
for n = 1 → N do αn.offset+ = Mt
21:
end for
22:
23: end function

i αr.shape[i], q ← αr.p + αr.offset.

(cid:81)

i αn.shape[i] end for

Figure 7: The dataﬂow graph encoded by F of Tree-LSTM.

to gather at the backward pass. Hence, for an expression
like sl = gather(child idx) in F, Cavs will generate a
backward expression scatter(∇sl) in ∂F. Similarly, the
gradient operator of scatter is gather. The same auto-
differentiation rule applies for push and pull as well.

3.5 Graph Execution Engine

Beneﬁting from the vertex-centric representation, the vertex
function F essentially deﬁnes a (small) static dataﬂow graph
that is open to various graph execution optimization tech-
niques (which might not be the case in dynamic declaration).
We next discuss three proposed optimizations on Cavs’ exe-
cution engine for improved performance.
Lazy batching and streaming4. In addition to batched exe-
cution of F, the lazy batching and streaming explore poten-
tial parallelism for a certain group of ﬁner-grained operators
in F or ∂F called lazy and eager operators.
Deﬁnition 1. An operator in F (∂F) is a lazy operator if at
the forward (backward) pass, for ∀v ∈ G, ∀G ∈ {Gk}K
k=1,

4 Streaming is a borrowed terminology from CUDA programming which
means executing different commands concurrently or out of order with re-
spect to each other on different GPU streams. As Cavs’ optimization strate-
gies are agnostic to the low-level hardware, we use streaming interchange-
ably with multi-threading if the underlying computing hardware is CPU.

8

the evaluation of F (∂F) at any parent (dependent) vertex
of v does not rely on the evaluation of F at v. It is an eager
operator if the evaluation at v does not rely on the evaluation
of F (∂F) at any dependent (parent) vertices of v.

In Cavs, ﬁguring out eager and lazy operators in F and ∂F
is straightforward given the following proposition:
Proposition 2. Denote DF (D∂F ) as the dataﬂow graph en-
coded by F (∂F), and g, s ∈ DF (D∂F ) as corresponded
nodes of the gather and scatter operator, respectively. A
node that has g as its dependent and is not on any path from
g to s is a lazy operator. A node that has s as its ancestor and
is not on any path from g to s is an eager operator.

Figure 7 illustrates a dataﬂow graph of the vertex function of
Tree-LSTM, with eager and lazy operators colored. A prop-
erty of them is that their evaluation is not fully subject to
the dependency reﬂected by the input graph G. For instance,
the pull operator in Figure 7 is eager and can be executed
in prior – even before F has been evaluated at the vertices
gather tries to interact with; the push operator is lazy, so
we can defer its execution without impacting the evaluation
of F at parent vertices. Similarly, in ∂F, the gradient deriva-
tion for model parameters are mostly lazy – their execution
can be deferred as long as the gradients of hidden states are
derived and propagated in time. Cavs leverages this property
and proposes the lazy batching strategy. It defers the execu-
tion of all lazy operators in F and ∂F until all batching tasks
{Vt}T
t=1 has ﬁnished. It then performs a batched execution of
these lazy operators over all vertices of {Gk}K
k=1. These op-
erators includes, but is not limited to, the push operator that
are doing memory copy, and the math operators for comput-
ing gradients of the model parameters. Lazy batching helps
exploit more parallelism for the execution of lazy operators
and signiﬁcantly reduces kernel launches. Empirically lazy
batching brings 20% overall improvement (see §5.3).

On the other hand, we are unable to “eagerly” batch ea-
ger operators, as their execution over some vertices relies on
knowing the detailed memory location of all intermediate re-
sults in advance, a condition which is not satisﬁed in Cavs
where memory is dynamically assigned. To leverage the ex-
hibited parallelization opportunity between eager operators
and the operators on the path from gather to scatter (Fig-
ure 7), Cavs proposes a streaming strategy that pipelines the
execution of these two groups of operators. It allocates two
streams, and puts the eager operators on one stream, and the
rest (excluding lazy operators) on the other. Hence, indepen-
dent operators in two streams run in parallel, while for those
operators that depend on an eager operator, this dependency
is respected by synchronization barriers (see Figure 7). It is
also possible to parallelize independent paths from g to s on
the graph, but we ﬁnd it does not yield further improvement.
Automatic kernel fusion. Since Cavs abstracts out a static
dataﬂow graph encoded by F that will be dynamically eval-
uated elsewhere, we can replace the original F with an op-

embeddinglookuppullgatherslicepushscattermatmulslicesigtanlinear transformationFused KernelsliceconcatmatmulsigsigaddaddaddaddmulmulmulmuladdSYNCparametereager operatorlazy operatorlazy batchingstream1stream2timized one that runs more efﬁciently, as long as it accepts
the same input and produces the same output.

Particularly, given F, before execution, Cavs will run a
fusion detector [26] to scan its corresponded dataﬂow graph
and report all fuse-able subgraphs therein, i.e. all nodes in
a fuse-able subgraph can be fused as a single operator that
behaves equivalently but takes less execution time (e.g. with
fewer kernel launches and I/O, or faster computation). Cur-
rently, we only detect groups of directly linked elementwise
operators, such as +, −, ×, ÷, tanh, sigmoid, as shown in
Figure 7, and we use a simple union-ﬁnd algorithm to de-
tect the largest possible fuse-able subgraphs. Given a fuse-
able subgraph, Cavs adopts de facto automatic code genera-
tion techniques [12, 40, 42, 43] to generate lower-level ker-
nel codes as an implementation for it. Replacing the original
fuse-able subgraphs with fused operators during execution is
beneﬁcial in many aspects: (1) it reduces the number of ker-
nel launches; (2) on some devices such as GPUs, kernel fu-
sion transform device memory access into faster device reg-
isters access. We empirically report another 20% improve-
ment with automatic kernel fusion (§5.3).

Implementation

4.
We implement Cavs as a pluggable C++ library that can be
integrated with existing DL frameworks to provide or en-
hance their support for dynamic NNs. We next brieﬂy dis-
cuss implementation details. For clarity, we assume the host
framework is composed of three major layers (which is the
case for most popular frameworks [1, 5, 38]): (1) a frontend
that provides device-agnostic symbolic programming inter-
face; (2) an intermediate layer that implements the core ex-
ecution logic; (3) a backend with device-speciﬁc kernels for
all provided operators.
Frontend. Cavs provides a base class GraphSupport in
addition to conventional operators and the four proposed
APIs (§3.1). Users are required to instantiate it by providing
a symbolic vertex function F – therefore an instantiation
of GraphSupport represents a single dynamic structure.
To construct more complex structures (e.g. encoder-decoder
LSTM [49], LRCN [13])), users employ push and pull to
connect this dynamic structure to external structures.
Intermediate Layer. At the intermediate layer, Cavs will
create a unique scope [1], and generates a small dataﬂow
graph for each instantiation of GraphSupport, connecting
them appropriately with other parts of the model according
to user programs. Cavs implements its core runtime logic at
this layer, i.e. the scheduler, the memory management, and
the graph execution engine, etc. During execution, the exe-
cution engine ﬁrst analyzes the received dataﬂow graphs and
incorporates described optimization in §3.5. The scheduler
then instructs the system to read training samples and their
associates graphs (e.g. adjacency matrices). It starts training
by submitting batching tasks to the execution engine and as-
signing memory accordingly.

Backend. Following common practice [1, 18, 38], Cavs puts
device-speciﬁc kernel implementations for each supported
operator at this layer. Each operator implementation is a
function that takes as inputs static tensors and produces static
tensors as outputs – therefore the higher-layer logic, i.e. how
the computation is scheduled or how the memory is assigned
are invincible to this layer. Cavs will reuse the native op-
erator implementations from the host framework, while it
provides optimized implementations for the four proposed
primitives (gather, scatter, pull, push). Speciﬁcally,
gather and pull index different slices of a tensor and puts
them together continuously on memory; scatter and push
by contrast splits a tensor along its batch dimension, and
copy different slices to different places. Cavs implements a
customized memcpy kernel for there four operators so that
copying multiple slices from (or to) different places is per-
formed within one kernel, to further reduce kernel launches.

5. Evaluation
In this section, we evaluate Cavs on training different NNs
across multiple datasets, obtaining the following major ﬁnd-
ings: (1) Cavs has little overhead: when training static NNs
that can be by nature batched, Cavs demonstrates equal per-
formance with other DL systems. On several NNs with no-
tably difﬁcult-to-batch structures, Cavs outperforms all ex-
isting frameworks by a large margin; (2) We conﬁrm the
graph construction overhead is substantial in both Fold [34]
and dynamic declaration [38], while Cavs bypasses it by
loading input graphs through I/O; (3) We verify the effec-
tiveness of our proposed design and optimization via abla-
tion studies, and discuss Cavs’ advantages over other state-
of-the-art DL systems for dynamic dataﬂow graphs.
Environment. We perform all experiments in this paper
on a single machine with an NVIDIA Titan X (GM200)
GPU, a 16-core (32 threads) CPU, and CUDA toolkit 8.0
and cuDNN v6 installed. As modern DL models are mostly
trained using GPUs, we focus our evaluation on GPUs, but
note Cavs’ design and implementation do not reply on a spe-
ciﬁc type of device. We borrow the implementations of most
mathematical operators from TensorFlow v1.2, while we im-
plement the four proposed operators and other system mod-
ules by ourselves. We mainly compare Cavs to TensorFlow
v1.2 [1] with XLA [20] and its variant Fold [34], as well
as DyNet v2.0 [38] with autobatching [39], as they have re-
ported better performance than other frameworks [17, 57] on
dynamic NNs. We focus on metrics for system performance,
e.g. the average time to scan one epoch of data. Cavs pro-
duces exactly the same numerical results with other frame-
works, hence the same per-epoch convergence5.
Models and dataset. We experiment on the following mod-
els with increasing difﬁculty to batch: (a) Fixed-LSTM lan-
guage model (LM): a static sequence LSTM with ﬁxed steps

5 The code of Cavs will be released along with a the next major release of
the DyNet project: http://dynet.io/.

9

Figure 8: Comparing ﬁve systems in terms of the averaged time to ﬁnish one epoch of training (lower is better) on four models: Fixed-LSTM,
Var-LSTM, Tree-FC and Tree-LSTM. In (a)-(d) we ﬁx the hidden size h and vary the batch size bs, while in (e)-(h) we ﬁx bs and vary h.

for language modeling [48, 49, 64]. We train it using the
PTB dataset [54] that contains over 10K different words. We
set the number of steps as 64, i.e. at each iteration of train-
ing, the model takes a 64-word sentence from the training
corpus, and predicts the next word of each word therein. Ob-
viously, the computation can be by nature batched easily,
as each sentence has exactly the same size. (b) Var-LSTM
LM: that accepts variable-length inputs. At each iteration
the model takes a batch of natural sentences with different
length from PTB, and predicts the next words; (c) Tree-FC:
the benchmarking model used in [34] with a single fully-
connected layer as its cell function. Following the same
setting in [34], we train it over synthetic samples gener-
ated by their code [53] – each sample is associated with
a complete binary tree with 256 leaves (therefore 511 ver-
tices per graph); (d) Tree-LSTM: a family of dynamic NNs
widely adopted for text analysis [33, 58]. We implement
the binary child-sum Tree-LSTM model in [50], and train
it as a sentiment classiﬁer using Stanford sentiment treebank
(SST) dataset [46], which contains 8544 training sentences
in which the longest sentence has 54 words. Each sentence
is associated with a human annotated grammar tree.

5.1 Overall Performance

We ﬁrst verify the viability of our design on the easiest-
to-batch case: Fixed-LSTM language model. We compare
Cavs to the following three strong baselines: (1) CuDNN [9]:
a CuDNN-based ﬁxed-step sequence LSTM, which is highly
optimized by NVIDIA using handcrafted kernels and stands
as the best performed implementation on NVIDIA GPUs;
(2) TF: the ofﬁcial implementation of Fixed-LSTM LM in
TensorFlow repository [52] based on static declaration; (3)
DyNet: we implement a 64-step LSTM in DyNet based on
dynamic declaration – we declare a dataﬂow graph per sam-
ple, and train with the autobatching [39] enabled; (4) Cavs
with batching policy, and all input samples have a same input
graph – a 64-node chain. We train the model to converge, and
report the average time per epoch in Figure 8(a)(e), where

in (a) we ﬁx the hidden size h of the LSTM unit as 512
and vary the batch size bs, and in (e) we ﬁx bs = 64 and
vary h. Empirically, CuDNN performs best in all cases, but
note it is highly inﬂexible. Cavs performs slightly better than
TF in various settings, verifying that our system has little
overhead dealing with fully static graphs, though it is spe-
cialized for dynamic ones. We also conclude from Figure 8
that batching is essential for GPU-based DL: bs = 128 is
nearly one order of magnitude faster than bs = 1 regardless
of used frameworks. For Cavs, the batching policy is 1.7x,
3.8x, 7.0x, 12x, 15x, 25x, 36x faster than the serial policy at
bs = 2, 4, 8, 16, 32, 64, 128, respectively.

Next, we experiment with Var-LSTM, the most com-
monly used RNN for variable-length sequences. We com-
pare the following three implementations (CuDNN-based
LSTM cannot handle variable-length inputs): (1) TF: an
ofﬁcial TensorFlow implementation based on the dynamic
unroll approach described in §2.2; (2) DyNet: an ofﬁcial
implementation from DyNet benchmark repository based
on dynamic declaration [15]; (3) Cavs: where each input
sentence is associated with a chain graph that has number
of vertices equal to the number of words. We vary h and
bs, and report the results in Figure 8(b)(f), respectively. Al-
though all three systems perform batched computation in
different ways, Cavs is constantly 2-3 times faster than TF,
and outperforms DyNet by a large margin. Compared to TF,
Cavs saves computational resources. TF dynamically unrolls
the LSTM unit according to the longest sentence in the cur-
rent batch, but it cannot prevent unnecessary computation
for those sentences that are shorter than the longest one.

We then turn to Tree-FC, a dynamic model for bench-
marking. Since vanilla TensorFlow is unable to batch its
computation, we compare Cavs to (1) DyNet and (2) Fold, a
specialized library built upon TensorFlow for dynamic NNs,
with a depth-based dynamic batching strategy. To enable the
batching, it however needs to preprocess the input graphs,
translate them into intermediate representations and pass
them to lower-level TensorFlow control ﬂow engine for ex-

10

11632641282560.030.060.090.12Time (x1e3 s)0.130.270.550.75(a) Fixed-LSTM (h = 512)CuDNNCavsTFDyNet11632641282560.050.100.150.200.391.370.82(b) Var-LSTM (h = 512)CavsTFDyNet11632641282560.020.040.060.080.3(c) Tree-FC (h = 512)CavsFoldDyNet11632641282560.020.040.060.080.55(d) Tree-LSTM (h = 512)CavsFoldDyNet64128256512102420480.030.060.090.12Time (x1e3 s)0.16(e) Fixed-LSTM (b = 64)CuDNNCavsTFDyNet64128256512102420480.050.100.150.200.29(f) Var-LSTM (b = 64)CavsTFDyNet64128256512102420480.020.040.060.080.12(g) Tree-FC (b = 64)CavsFoldDyNet64128256512102420480.020.040.060.08(h) Tree-LSTM (b = 64)CavsFoldDyNetecution. We report the results in Figure 8(c)(g) with varying
bs and h, respectively. For all systems, we allocate a single
CPU thread for graph preprocessing or construction. Cavs
shows at least an order of magnitude speedups than Fold
and DyNet at (h ≤ 512). Because the size of the synthetic
trees is large, one major advantage of Cavs over them is
the alleviation of substantial graph preprocessing/construc-
tion overhead. With a single CPU thread, Fold takes even
more time on graph preprocessing than computation (§5.3).
Finally, we compare three frameworks on Tree-LSTM in
Figure 8(d)(h): Cavs is 8-10x faster than Fold, and con-
sistently outperforms DyNet. One difference in this exper-
iment is that we allocate as many CPU threads as possible
(32 on our machine) to accelerate graph preprocessing for
Fold, otherwise it will take much longer time. Further, we
note DyNet performs much better here than on Tree-FC, as
the size of the input graphs in SST (maximally 52 leaves)
is much smaller than the synthetic ones (256 leaves each) in
Tree-FC experiments. We observe DyNet needs more time
on graph construction for large input graphs, and DyNet’s
dynamic batching is less effective on larger input graphs, as
it has to perform frequent memory checks to support its dy-
namic batching, which we will discuss in §5.3.

5.2 Graph Construction and Computation

In this section, we investigate the graph construction over-
head in Fold and DyNet. To batch computation of different
graphs, Fold analyzes the input graphs to recognize batch-
able dynamic operations, then translates them into interme-
diate instructions, with which, TensorFlow generates appro-
priate control ﬂow graphs for evaluation – we will treat the
overhead caused in both steps as Fold’s graph construction
overhead. DyNet, as a typical dynamic declaration frame-
work, has to construct as many dataﬂow graphs as the num-
ber of samples. Though DyNet has optimized its graph con-
struction to make it lightweight, the overhead still grows
with the training set and the size of input graphs. By contrast,
Cavs takes constant time to construct a small dataﬂow graph
encoded by F, then reads input graphs through I/O. To quan-
tify the overhead, we separate the graph construction from
computation, and visualize in Figure 9(a) how it changes
with the average number of leaves (graph size) of input
graphs on training Tree-FC, with ﬁxed bs = 64, h = 512.
We compare (1) Cavs (2) Fold-1 which is Fold with one
graph processing thread and (3) DyNet. We plot for one
epoch, both the (averaged) absolute time for graph construc-
tion and it percentage of the overall time. Clearly we ﬁnd
that all three systems take increasingly more time when the
size of the input graphs grows, but Cavs, which loads graphs
through I/O, causes the least overhead at all settings. In terms
of the relative time, Fold unfortunately wastes 50% at 32
leaves, and 80% when the tree has 1024 leaves, while DyNet
and Cavs take only 10% and 20%, respectively.

We also wonder how the overhead is related with batch
size when there is ﬁxed computational workload. We report

Figure 9: The averaged graph construction overhead per epoch
when training (a) Tree-FC with different size of input graphs (b)
Tree-LSTM with different batch size. The curves show absolute
time in second (left y-axis), and the bar graphs show its percentage
of the overall time (right y-axis).

in Figure 9(b) the same metrics when training Tree-LSTM
with varying bs. We add another baseline Fold-32 with 32
threads for Fold’s graph preprocessing. As Fold-1 takes
much longer time than others, we report its time at bs =
1, 16, 32, 64, 128, 256 here (instead of showing in Figure 9):
1.1, 7.14, 31.35, 40.1, 46.13, 48.77. Except bs = 1, all
three systems (except Fold-1) take almost constant time
for graph construction in one epoch, regardless of bs, while
Fold-32 and DyNet take similar time, but Cavs takes 20x
less. Nevertheless, at the percentage scale, increasing bs
makes this overhead more prominent, because larger batch
size yields improved computational efﬁciency, therefore less
time to ﬁnish one epoch. This, from one perspective, reﬂects
that the graph construction is a main obstacle that grows with
the number of training samples and prevents the efﬁcient
training of dynamic NNs in existing frameworks, while Cavs
successfully overcomes this barrier through its design.

Apart from the graph construction we report in Table 1
the computation-only time – Cavs demonstrates maximally
5.4x/9.7x and 7.2x/2.4x speedups over Fold/DyNet on
Tree-FC and Tree-LSTM, respectively. Besides less sys-
tem overhead, the advantages stem from two main sources:
an optimized graph execution engine, and a better-suited
memory management strategy, which we investigate next.

#
leaves
32
64
128
256
512
1024

Comp. time (s)

Speedup

0.58 / 3.1 / 4.1
1.1 / 3.9 / 8.0
2.0 / 6.2 / 16.0
3.9 / 10.6 / 33.7
8.0 / 18.5 / 70.6
15.8 / 32.4 / 153

5.4x / 7.1x
3.7x / 7.5x
3.0x / 7.9x
2.7x / 8.7x
2.3x / 8.9x
2.1x / 9.7x

bs

1
16
32
64
128
256

Comp. time (s)

Speedup

76.2 / 550 / 61.6
9.80 / 69 / 12
6.15 / 43 / 9.9
4.1 / 29 / 7.4
2.9 / 20.5 / 5.9
2.3 / 15.8 / 5.4

7.2x / 0.8x
7.0x / 1.2x
7.0x / 1.6x
7.2x / 1.8x
7.1x / 2.0x
7.0x / 2.4x

Table 1: The computation time in second (Cavs/Fold/DyNet) and
the speedup (Cavs vs Fold/DyNet) for training one epoch on
Tree-FC with varying size of the input trees (left part), and on
Tree-LSTM with varying batch size (right part).

5.3 Ablation Studies

Graph Execution Engine. To reveal how much each op-
timization in §3.5 contributes to the ﬁnal performance, we
disable lazy batching, fusion and streaming in Cavs and
set this conﬁguration as a baseline (speedup = 1). We then

11

2040608020406080Percentage(%)PercentageCavsFold-1Fold-32DyNet32641282565121024Num of Leaves481216Time (s)(a) Tree-FC (bs = 64, h = 512)1163264128256Batch Size (bs)0.61.21.82.4(b) Tree-LSTM (h = 512)TimeCavsFold-32DyNetMemory operations
(s) (Cavs / DyNet)
Train

Inference

Computation (s)
(Cavs / DyNet)

Train

Inference

1.14 / 1.33
0.67 / 0.87
0.39 / 0.6
0.25 / 0.44
0.17 / 0.44

0.6 / 1.33
0.35 / 0.87
0.21 / 0.6
0.13 / 0.44
0.09 / 0.44

9.8 / 12
6.1 / 9.8
4.0 / 7.4
2.9 / 5.9
2.3 / 5.4

2.9 / 8.53
1.9 / 5.35
1.3 / 3.48
0.97 / 2.52
0.77 / 2.58

bs

16
32
64
128
256

Table 2: The breakdowns of the average time per epoch on memory-
related operations and computation. We compare Cavs to DyNet on
Tree-LSTM on training and inference, with varying bs.

the memory-related overhead during both training and infer-
ence, and separate it from computation. We compare them
on TreeLSTM, and report the breakdown time per epoch in
Table 2 under different bs. We observe the improvement is
signiﬁcant (2x - 3x) at larger bs, especially during inference
where DyNet has its continuity checks concentrated.
Others. Despite system advantages, we also try to inves-
tigate whether Cavs, as an interface, simpliﬁes user pro-
grams (though we do not claim as a contribution). We com-
pare Cavs to Fold and DyNet in terms of the lines of code
(LoC) needed to create a few notable dynamic NNs, in-
cluding Var-LSTM, Tree-LSTM, and multi-layer sequence
LSTM, with Python as the host language. If only for model
declaration, Fold in general has 3.5x more LoC than Cavs,
while DyNet has slightly more LoC than Cavs because of
the function to repeatedly declare graphs.

6. Related Work
In addition to §2, we discuss some other related works.
Graph execution optimization. Optimizing the execution
of DL dataﬂow graphs comes in mainly two ways: better op-
erator implementations or optimizing the execution of (sub-
)graphs. As Cavs is implemented as a plug-in to enhance
existing frameworks, it beneﬁts from any improved imple-
mentations of speciﬁc operators (e.g. cuDNN) [9, 22, 24,
29]. In addition, Cavs has optimized implementations for
its proposed four primitives (gather/scatter/pull/push).
At the graph level, a variety of well-developed techniques
from other areas, such as kernel fusion, common subexpres-
sion elimination, and constant folding, have been adapted
and applied on speeding the computation of DL dataﬂow
graphs [1, 8, 18, 20]. They are usually incorporated after the
graph declaration, but before the execution, so that the ac-
tual computation is conducted on an optimized graph other
than the original one. However, these graph optimizations
are less beneﬁcial in dynamic declaration, in which the graph
changes with the sample, and needs to be re-processed and
re-optimized every iteration, and may cause substantial over-
head. On the contrary, Cavs separates the static vertex func-
tion from the dynamic-varying input graph, so it beneﬁts
from most of the aforementioned optimizations, as we have
shown in §5.3. We draw insights from these strategies and
reﬂect them in Cavs’ execution engine. We further propose
lazy batching and streaming to exploit more parallelism ex-
posed by our programming model.

Figure 10: Improvement of each optimization strategy on execution
engine over a baseline conﬁguration (speedup = 1).

turn on one optimization at a time and record how much
speedup it brings. We train Fixed-LSTM and Tree-LSTM,
and report the averaged speedups one computation-only time
in one epoch over the baseline conﬁguration in Figure 10,
with bs = 64 but varying h. Lazy batching and fusion con-
sistently deliver nontrivial improvement – lazy batching is
more beneﬁcial with a larger h while fusion is more effective
at smaller h, which are expected: lazy batching mainly paral-
lelizes matrix-wise operations (e.g. matmul) commonly with
O(h2) our higher complexity, while fusion mostly works on
elementwise operations with a linear complexity with h [25].
Streaming, compared to the other strategies, is less ef-
fective on Tree-LSTM than on Fixed-LSTM, as we have
found the depth of the input trees in SST exhibit high vari-
ance, i.e. some trees are much deeper than others. In this
case, many batching tasks only have one vertex to be eval-
uated. The computation is highly fragmented and the efﬁ-
ciency is bounded by kernel launching latency. Lazy batch-
ing and fusion still help as they both reduce kernel launches
(§3.5). Streaming, which tries to pipeline multiple kernels,
can hardly yield obvious improvement.
Memory Management. Cavs’ performance advantage also
credits to its better memory management that reduces mem-
ory movements while guarantees memory continuity.

Quantitatively, it is difﬁcult to compare Cavs to Fold, as
Fold relies on TensorFlow where memory management is
highly coupled with other system aspects. Qualitatively, we
ﬁnd Cavs requires less memory movement (e.g. memcpy)
during dynamic batching. Built upon the tf while opera-
tor, whenever Fold performs depth-based batching at d, it has
to move all the contents of nodes in the dataﬂow graphs at
depth d − 1 to a desired location, as the control ﬂow does not
support cross-depth memory indexing. This results in redun-
dant memcpy, especially when the graphs are highly skewed.
By contrast, Cavs only copies contents that are necessary to
the batching task. DyNet has a specialized memory manage-
ment strategy for dynamic NNs. Compared to Cavs, it how-
ever suffers substantial overhead caused by repeated checks
of the memory continuity – whenever DyNet wants to batch
operators with same signatures, it checks whether their in-
puts are continuous on memory [39]. The checking overhead
increases with bs and is more prominent on GPUs. Thanks
to the simplicity of both systems, we are able to proﬁle

12

6412825651210242048Hidden Size (h)1.01.11.21.31.41.5Speedup (x)(a) Fixed-LSTM (bs = 64)Lazy-batchingFusionStreamming6412825651210242048Hidden Size (h)1.01.11.21.31.41.5(b) Tree-LSTM (bs = 64)Lazy-batchingFusionStreammingVertex-centric models. The vertex-centric programming
model has been extensively developed in the area of graph
computing [7, 19, 35, 47]. Cavs draws insights from the GAS
model [19], but faces totally different challenges in sys-
tem and interface design, such as expressiveness, scheduling
for batched execution of different graphs, guaranteeing the
memory continuity, etc., as we have discussed in §3.1.

7. Conclusion
We present Cavs as a vertex-centric programming inter-
face as well as an efﬁcient system for dynamic deep learn-
ing. Cavs represents a dynamic NN structure as static ver-
tex functions and dynamic input graphs. It provides four
novel APIs to allow users to easily program these types
of NNs. With designed scheduling policy, memory man-
agement strategy, and graph execution optimizations, Cavs
avoids substantial graph construction overhead suffered by
dynamic declaration, and reports new state-of-the-art system
performance for various notable dynamic NN architectures.

References
[1] ABADI, M., BARHAM, P., CHEN, J., CHEN, Z., DAVIS, A.,
DEAN, J., DEVIN, M., GHEMAWAT, S., IRVING, G., ISARD,
M., ET AL. Tensorﬂow: A system for large-scale machine
learning. arXiv preprint arXiv:1605.08695 (2016).

[2] ANDREAS, J., ROHRBACH, M., DARRELL, T., AND KLEIN,
D. Neural module networks. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (2016),
pp. 39–48.

[3] BAHDANAU, D., CHO, K., AND BENGIO, Y. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473 (2014).

[4] BARTHOLOMEW-BIGGS, M., BROWN, S., CHRISTIANSON,
B., AND DIXON, L. Automatic differentiation of algorithms.
Journal of Computational and Applied Mathematics 124, 1
(2000), 171–190.

[5] BERGSTRA, J., BASTIEN, F., BREULEUX, O., LAMBLIN, P.,
PASCANU, R., DELALLEAU, O., DESJARDINS, G., WARDE-
FARLEY, D., GOODFELLOW, I. J., BERGERON, A., AND
BENGIO, Y. Theano: Deep Learning on GPUs with Python.
In NIPSW (2011).

[6] BUCKMAN,

J., BALLESTEROS, M., AND DYER, C.
Transition-based dependency parsing with heuristic back-
tracking. In Proceedings of the 2016 Conference on Empir-
ical Methods in Natural Language Processing; 2016 Nov 1-5;
Austin, Texas, USA. Stroudsburg (USA): Association for Com-
putational Linguistics (ACL); 2016. p. 2313-18. (2016), ACL
(Association for Computational Linguistics).

[7] CHEN, R., SHI, J., CHEN, Y., AND CHEN, H. Powerlyra:
Differentiated graph computation and partitioning on skewed
graphs. In Proceedings of the Tenth European Conference on
Computer Systems (2015), ACM, p. 1.

[8] CHEN, T., LI, M., LI, Y., LIN, M., WANG, N., WANG, M.,
XIAO, T., XU, B., ZHANG, C., AND ZHANG, Z. Mxnet: A
ﬂexible and efﬁcient machine learning library for heteroge-

neous distributed systems. arXiv preprint arXiv:1512.01274
(2015).

[9] CHETLUR, S., WOOLLEY, C., VANDERMERSCH, P., CO-
HEN, J., TRAN, J., CATANZARO, B., AND SHELHAMER, E.
cudnn: Efﬁcient primitives for deep learning. arXiv preprint
arXiv:1410.0759 (2014).

[10] CHUNG, J., GULCEHRE, C., CHO, K., AND BENGIO, Y.
Empirical evaluation of gated recurrent neural networks on
sequence modeling. arXiv preprint arXiv:1412.3555 (2014).

[11] CUI, H., ZHANG, H., GANGER, G. R., GIBBONS, P. B.,
AND XING, E. P. Geeps: Scalable deep learning on dis-
tributed gpus with a gpu-specialized parameter server. In Pro-
ceedings of the Eleventh European Conference on Computer
Systems (2016), ACM, p. 4.

[12] DAVE, C., BAE, H., MIN, S.-J., LEE, S., EIGENMANN,
R., AND MIDKIFF, S. Cetus: A source-to-source compiler
infrastructure for multicores. Computer 42, 12 (2009).

[13] DONAHUE, J., ANNE HENDRICKS, L., GUADARRAMA, S.,
ROHRBACH, M., VENUGOPALAN, S., SAENKO, K., AND
DARRELL, T. Long-term recurrent convolutional networks
for visual recognition and description. In Proceedings of the
IEEE conference on computer vision and pattern recognition
(2015), pp. 2625–2634.

[14] DYER, C., BALLESTEROS, M., LING, W., MATTHEWS,
A., AND SMITH, N. A. Transition-based dependency pars-
arXiv preprint
ing with stack long short-term memory.
arXiv:1505.08075 (2015).

[15] DYNET VARIABLE LENGTH LSTM. https://github.

com/neulab/dynet-benchmark.

[16] ELMAN, J. L. Finding structure in time. Cognitive science

14, 2 (1990), 179–211.

[17] FACEBOOK. http://pytorch.org/.

[18] FACEBOOK OPEN SOURCE. Caffe2 is a lightweight, modular,
and scalable deep learning framework. https://github.
com/caffe2/caffe2, 2017.

[19] GONZALEZ, J. E., LOW, Y., GU, H., BICKSON, D., AND
GUESTRIN, C. Powergraph: Distributed graph-parallel com-
putation on natural graphs.

[20] GOOGLE

TENSORFLOW XLA.

https://www.

tensorflow.org/performance/xla/.

[21] GORMLEY, M. R., DREDZE, M., AND EISNER,

J.
Approximation-aware dependency parsing by belief propaga-
tion. arXiv preprint arXiv:1508.02375 (2015).

[22] GRAVE, ´E., JOULIN, A., CISS ´E, M., GRANGIER, D., AND
J ´EGOU, H. Efﬁcient softmax approximation for gpus. arXiv
preprint arXiv:1609.04309 (2016).

[23] GRAVES, A., FERN ´ANDEZ, S., GOMEZ, F., AND SCHMID-
HUBER, J. Connectionist temporal classiﬁcation: labelling
unsegmented sequence data with recurrent neural networks,
2006.

[24] GUENNEBAUD, G.,

JACOB, B., ET AL.

Eigen v3.

http://eigen.tuxfamily.org, 2010.

[25] GUSTAFSON, J. L. Reevaluating amdahl’s law. Communica-

tions of the ACM 31, 5 (1988), 532–533.

13

[26] GYSI, T., OSUNA, C., FUHRER, O., BIANCO, M., AND
SCHULTHESS, T. C. Stella: A domain-speciﬁc tool for struc-
tured grid methods in weather and climate models. In High
Performance Computing, Networking, Storage and Analysis,
2015 SC-International Conference for (2015), IEEE, pp. 1–
12.

[27] HINTON, G. E., AND SALAKHUTDINOV, R. R. Reducing
the dimensionality of data with neural networks. science 313,
5786 (2006), 504–507.

[28] HOCHREITER, S., AND SCHMIDHUBER, J. Long short-term
memory. Neural computation 9, 8 (1997), 1735–1780.

[29] INTEL OPEN SOURCE TECHNOLOGY CENTER. Intel(r) math
kernel library for deep neural networks (intel(r) mkl-dnn).
https://github.com/01org/mkl-dnn, 2017.

[30] JIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV, S.,
LONG, J., GIRSHICK, R., GUADARRAMA, S., AND DAR-
RELL, T. Caffe: Convolutional architecture for fast feature
embedding. arXiv preprint arXiv:1408.5093 (2014).

[31] KONG, L., DYER, C., AND SMITH, N. A. Segmental re-
current neural networks. arXiv preprint arXiv:1511.06018
(2015).

[32] KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. Im-
ageNet Classiﬁcation with Deep Convolutional Neural Net-
works. In NIPS (2012).

[33] LIANG, X., SHEN, X., FENG, J., LIN, L., AND YAN, S. Se-
mantic object parsing with graph lstm. In European Confer-
ence on Computer Vision (2016), Springer, pp. 125–143.

[34] LOOKS, M., HERRESHOFF, M., HUTCHINS, D., AND
NORVIG, P. Deep learning with dynamic computation graphs.
arXiv preprint arXiv:1702.02181 (2017).

[35] MALEWICZ, G., AUSTERN, M. H., BIK, A. J., DEHNERT,
J. C., HORN, I., LEISER, N., AND CZAJKOWSKI, G. Pregel:
a system for large-scale graph processing. In Proceedings of
the 2010 ACM SIGMOD International Conference on Man-
agement of data (2010), ACM, pp. 135–146.

[36] MIKOLOV, T., CHEN, K., CORRADO, G., AND DEAN, J.
Efﬁcient estimation of word representations in vector space.
arXiv preprint arXiv:1301.3781 (2013).

[37] MURRAY, D. G., MCSHERRY, F., ISAACS, R., ISARD, M.,
BARHAM, P., AND ABADI, M. Naiad: a timely dataﬂow sys-
tem. In Proceedings of the Twenty-Fourth ACM Symposium
on Operating Systems Principles (2013), ACM, pp. 439–455.

[38] NEUBIG, G., DYER, C., GOLDBERG, Y., MATTHEWS,
A., AMMAR, W., ANASTASOPOULOS, A., BALLESTEROS,
M., CHIANG, D., CLOTHIAUX, D., COHN, T., ET AL.
Dynet: The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 (2017).

[39] NEUBIG, G., GOLDBERG, Y., AND DYER, C. On-the-ﬂy
operation batching in dynamic computation graphs. arXiv
preprint arXiv:1705.07860 (2017).

[40] NVIDIA.

http://docs.nvidia.com/cuda/nvrtc/

index.html.

[41] PANG, B., LEE, L., AND VAITHYANATHAN, S. Thumbs up?:
sentiment classiﬁcation using machine learning techniques. In
Proceedings of the ACL-02 conference on Empirical methods

in natural language processing-Volume 10 (2002), Associa-
tion for Computational Linguistics, pp. 79–86.

[42] QUINLAN, D. Rose: Compiler support for object-oriented
frameworks. Parallel Processing Letters 10, 02n03, 215–226.

[43] RAGAN-KELLEY, J., BARNES, C., ADAMS, A., PARIS, S.,
DURAND, F., AND AMARASINGHE, S. Halide: a language
and compiler for optimizing parallelism, locality, and recom-
putation in image processing pipelines. ACM SIGPLAN No-
tices 48, 6 (2013), 519–530.

[44] RUMELHART, D. E., HINTON, G. E., WILLIAMS, R. J.,
ET AL. Learning representations by back-propagating errors.
Cognitive modeling 5, 3 (1988), 1.

[45] SOCHER, R., LIN, C. C., MANNING, C., AND NG, A. Y.
Parsing natural scenes and natural language with recursive
In Proceedings of the 28th international
neural networks.
conference on machine learning (ICML-11) (2011), pp. 129–
136.

[46] SOCHER, R., PERELYGIN, A., WU, J., CHUANG, J., MAN-
NING, C. D., NG, A., AND POTTS, C. Recursive deep mod-
els for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 conference on empirical methods
in natural language processing (2013), pp. 1631–1642.

[47] SUNDARAM, N., SATISH, N., PATWARY, M. M. A., DUL-
LOOR, S. R., ANDERSON, M. J., VADLAMUDI, S. G., DAS,
D., AND DUBEY, P. Graphmat: High performance graph ana-
lytics made productive. Proceedings of the VLDB Endowment
8, 11 (2015), 1214–1225.

[48] SUNDERMEYER, M., SCHL ¨UTER, R., AND NEY, H. Lstm
neural networks for language modeling. In Thirteenth Annual
Conference of the International Speech Communication Asso-
ciation (2012).

[49] SUTSKEVER, I., VINYALS, O., AND LE, Q. V. Sequence
In Advances
to sequence learning with neural networks.
in neural information processing systems (2014), pp. 3104–
3112.

[50] TAI, K. S., SOCHER, R., AND MANNING, C. D. Improved
semantic representations from tree-structured long short-term
memory networks. arXiv preprint arXiv:1503.00075 (2015).

[51] TAN, M., SANTOS, C. D., XIANG, B., AND ZHOU, B. Lstm-
based deep learning models for non-factoid answer selection.
arXiv preprint arXiv:1511.04108 (2015).

[52] TENSORFLOW FIXED-SIZED LSTM LANGUAGE MODEL.
https://github.com/tensorflow/models/blob/
master/tutorials/rnn/ptb/ptb_word_lm.py.

[53] TENSORFLOW FOLD BENCHMARK CODE.

https:

//github.com/tensorflow/fold/tree/master/
tensorflow_fold/loom/benchmarks.

[54] THE PENN TREE BANK (PTB) DATASET. http://www.
fit.vutbr.cz/~imikolov/rnnlm/simple-examples.
tgz.

[55] TIAN, Y., BALMIN, A., CORSTEN, S. A., TATIKONDA, S.,
AND MCPHERSON, J. From think like a vertex to think like
a graph. Proceedings of the VLDB Endowment 7, 3 (2013),
193–204.

14

[56] TOKUI, S., OONO, K., HIDO, S., AND CLAYTON, J.
Chainer: a next-generation open source framework for deep
In Proceedings of workshop on machine learn-
learning.
ing systems (LearningSys) in the twenty-ninth annual confer-
ence on neural information processing systems (NIPS) (2015),
vol. 5.

[57] TOKUI, S., OONO, K., HIDO, S., AND CLAYTON, J.
Chainer: a next-generation open source framework for deep
In Proceedings of Workshop on Machine Learn-
learning.
ing Systems (LearningSys) in The Twenty-ninth Annual Con-
ference on Neural Information Processing Systems (NIPS)
(2015).

[58] VINYALS, O., KAISER, Ł., KOO, T., PETROV, S.,
SUTSKEVER, I., AND HINTON, G. Grammar as a foreign
language. In Advances in Neural Information Processing Sys-
tems (2015), pp. 2773–2781.

[59] WALT, S. V. D., COLBERT, S. C., AND VAROQUAUX, G. The
numpy array: a structure for efﬁcient numerical computation.
Computing in Science & Engineering 13, 2 (2011), 22–30.

[60] XU, K., BA, J., KIROS, R., CHO, K., COURVILLE, A.,
SALAKHUDINOV, R., ZEMEL, R., AND BENGIO, Y. Show,
attend and tell: Neural image caption generation with visual
attention. In International Conference on Machine Learning
(2015), pp. 2048–2057.

[61] YAN, Z., ZHANG, H., JAGADEESH, V., DECOSTE, D., DI,
W., AND PIRAMUTHU, R. Hd-cnn: Hierarchical deep convo-
lutional neural network for image classiﬁcation. ICCV (2015).

[62] YAN, Z., ZHANG, H., JIA, Y., BREUEL, T., AND YU, Y.
Combining the best of convolutional layers and recurrent lay-
arXiv
ers: A hybrid network for semantic segmentation.
preprint arXiv:1603.04871 (2016).

[63] YAN, Z., ZHANG, H., WANG, B., PARIS, S., AND YU,
Y. Automatic photo adjustment using deep neural networks.
ACM Transactions on Graphics (TOG) 35, 2 (2016), 11.

[64] ZAREMBA, W., SUTSKEVER, I., AND VINYALS, O. Re-
arXiv preprint

current neural network regularization.
arXiv:1409.2329 (2014).

[65] ZHANG, H., HU, Z., WEI, J., XIE, P., KIM, G., HO, Q.,
AND XING, E. Poseidon: A system architecture for efﬁcient
gpu-based deep learning on multiple machines. arXiv preprint
arXiv:1512.06216 (2015).

[66] ZHANG, H., ZHENG, Z., XU, S., DAI, W., HO, Q., LIANG,
X., HU, Z., WEI, J., XIE, P., AND XING, E. P. Poseidon:
An efﬁcient communication architecture for distributed deep
learning on GPU clusters. In 2017 USENIX Annual Techni-
cal Conference (USENIX ATC 17) (Santa Clara, CA, 2017),
USENIX Association, pp. 181–193.

[67] ZHENG, S., JAYASUMANA, S., ROMERA-PAREDES, B., VI-
NEET, V., SU, Z., DU, D., HUANG, C., AND TORR, P. H.
Conditional random ﬁelds as recurrent neural networks.
In
Proceedings of the IEEE International Conference on Com-
puter Vision (2015), pp. 1529–1537.

15

