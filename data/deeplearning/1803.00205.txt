8
1
0
2

t
c
O
7

]

C
O
.
h
t
a
m

[

3
v
5
0
2
0
0
.
3
0
8
1
:
v
i
X
r
a

COMPOSITE DIFFERENCE-MAX PROGRAMS FOR MODERN
STATISTICAL ESTIMATION PROBLEMS

YING CUI ∗, JONG-SHI PANG ∗, AND BODHISATTVA SEN †

Abstract. Many modern statistical estimation problems are deﬁned by three major components:
a statistical model that postulates the dependence of an output variable on the input features; a loss
function measuring the error between the observed output and the model predicted output; and a
regularizer that controls the overﬁtting and/or variable selection in the model. We study the sam-
pling version of this generic statistical estimation problem where the model parameters are estimated
by empirical risk minimization, which involves the minimization of the empirical average of the loss
function at the data points weighted by the model regularizer.
In our setup we allow all three
component functions discussed above to be of the diﬀerence-of-convex (dc) type and illustrate them
with a host of commonly used examples, including those in continuous piecewise aﬃne regression
and in deep learning (where the activation functions are piecewise aﬃne). We describe a nonmono-
tone majorization-minimization (MM) algorithm for solving the uniﬁed nonconvex, nondiﬀerentiable
optimization problem which is formulated as a specially structured composite dc program of the
pointwise max type, and present convergence results to a directional stationary solution. An eﬃ-
cient semismooth Newton method is proposed to solve the dual of the MM subproblems. Numerical
results are presented to demonstrate the eﬀectiveness of the proposed algorithm and the superiority
of continuous piecewise aﬃne regression over the standard linear model.

Key words. Continuous piecewise aﬃne regression, nonconvex optimization, nondiﬀerentiable

objective, ReLu activation function, semismooth Newton method

AMS subject classiﬁcations. 62J02, 90C26, 49J52

1. Introduction. Many modern statistical estimation problems are deﬁned by
three major components: a statistical model that postulates the dependence of an
output quantity on the input features; a loss function measuring the error between
the observed output and model predicted output; and a regularizer that controls the
overﬁtting and/or variable selection in the model. The overall estimation problem
is to determine certain unknown parameters in the statistical model.
In practical
computation, samples on the underlying covariates (inputs) and output are available
and a data-based empirical objective combined with a regularizer is formulated as a
minimization problem that constitutes the workhorse of the estimation process. This
paper addresses the computational solution of the latter optimization problem which
is challenged by its nonconvexity and nondiﬀerentiability. These features immedi-
ately raise the question about the stationarity, let alone minimizing, properties of the
computed solution by an iterative algorithm. Our goal is to compute a directional
stationary solution [47], which is the sharpest kind among all stationary solutions.

Traditionally, a statistical model is usually described by a linear combination of
the input features, resulting in a linear regression or classiﬁcation model. Nevertheless,
in recent years, research into the use of piecewise aﬃne, convex [19, 20, 39] and
nonconvex [24] estimation functions has been on the rise. There are other piecewise
aﬃne/quadratic functions that arise from diﬀerent applications, such as those using
piecewise aﬃne functions to approximate the signum function and multi-layer neural
networks in deep learning [61, Chapter 4]. These nontraditional, nondiﬀerentiable

∗Department of Industrial and Systems Engineering, University of Southern California, Los An-
geles, CA 90089 (yingcui@usc.edu; jongship@usc.edu). The work of these two authors was based on
research partially supported by the U.S. National Science Foundation grant IIS–1632971.

†Department of Statistics, Columbia University, New York, NY 10027 (bodhi@stat.columbia.edu).
The work of this author was based on research partially supported by the U.S. National Science
Foundation grant DMS–1712822.

1

 
 
 
 
 
 
estimation functions have provided an important impetus for our work.

Some classical convex loss measures are based on the absolute-value or squared
errors between the model predicted outputs and the observed outputs, leading to a (cid:96)1
or (cid:96)2 loss function for regression. Other loss functions include a logarithmic loss for
logistic regression, and an exponential loss for boosting. In the area of support vector
machines, a hinge loss is commonly used for binary classiﬁcation. In recent years, a
truncated hinge loss function has been proposed to reduce the eﬀect of outliers [64].
A distinguished property of the latter loss function is that it is nonconvex and nondif-
ferentiable. All these loss measures are composed with the statistical model involving
the parameters to be estimated, leading to an overall composite loss function to be
minimized, which is nonconvex and nondiﬀerentiable, due either to a piecewise sta-
tistical model or a loss function that lacks convexity and/or diﬀerentiability. In the
area of support vector machines and other applications, a norm function of the model
variables is often used as the model regularizer to avoid overﬁtting. In recent years,
sparsity constraints to avoid model overﬁtting [26] is an important consideration in
high-dimensional statistical learning. Starting with the pioneering work of Fan and
Li [22], various sparsity functions have been proposed as surrogates of the discontin-
uous counting step function; it was shown in [1, 34] that all these surrogate sparsity
functions can be expressed as the diﬀerence of two convex functions of a particular
type.

In this paper, we introduce a composite diﬀerence-of-convex-piecewise program of
the pointwise max type as a uniﬁcation of a host of statistical models, loss functions,
and model regularizers. A highlight of this formulation is the emphasis on the separate
roles of each component function that is key to the development of a nonmonotone
majorization-minimization (MM) algorithm for solving the overall nonconvex, nondif-
ferentiable optimization problem. While global optima of nonconvex problems cannot
be computed, stationary solutions of various kinds are computable by iterative algo-
rithms with guaranteed convergence. What is essential is that focus should be placed
on computing sharp stationary solutions which distinguish themselves as being the
ones that must satisfy all other relaxed deﬁnitions of stationarity. With guaranteed
subsequential convergence, the developed MM algorithm aims to compute such a sta-
tionary point that is based on the elementary notion of directional derivatives of the
objective function. By using the Kurdyka-(cid:32)Lojaziewicz (KL) theory of semi-analytic
functions [2, 3, 9], we also show the sequential convergence of the iterates produced by
the algorithm. In order to handle the nondiﬀerentiable pointwise max terms, auxiliary
variables are employed to express these pointwise maximum functions as constraints.
With the regularization of the added variables in deﬁning the subproblems, the re-
sulting MM algorithm no longer guarantees the monotone property of the original
objective; this leads to our terminology of “nonmonotone MM algorithm”. Due to
the modiﬁcations which are introduced to facilitate the fast and eﬃcient solution
of the subproblems in the iterative steps, a separate proof of convergence is needed
for the algorithm. To ensure the rapid convergence and high-accuracy required of
the solution of the subproblems, we employ the semismooth Newton method (SN)
[50, 46] for semismoothly diﬀerentiable (SC1) functions applied to the dual of such
subproblems. We present numerical results to demonstrate the eﬀectiveness of the
overall algorithm and the superiority of a nontraditional continuous piecewise aﬃne
regression model over the traditional linear model using least-squares regression. In
addition to this speciﬁc problem, the nonmonotone MM algorithm combined with the
SN method (abbreviated as MM+SN) developed in this paper has wide applications
to many related problems that include the multi-layer neural network problems with

piecewise activation functions in deep learning. Due to page limit, we will report the
details of the other applications in separate papers.

The major contributions of this paper are fourfold: (a) we identify a uniﬁed com-
posite diﬀerence-max program for a host of modern statistical estimation problems
and illustrate the formulation with a variety of special instances; (b) we develop a
majorization-minimization based algorithm for computing, for the ﬁrst time, a direc-
tional stationary solution of such a nonconvex, nonsmooth program; (c) we present
a semismooth Newton method for solving the MM subproblems and illustrate its ef-
fectiveness when applied to a piecewise aﬃne regression problem; and (d) we report
computational results on the latter application that demonstrate the superiority of
this nontraditional statistical estimation approach over traditional linear regression.

2. A Uniﬁed Composite Diﬀerence-Convex-Piecewise Program. In the
deﬁnition of a statistical estimation problem, we place particular emphasis and care
on the mathematical properties of its deﬁning functions; identifying these properties
is needed to eﬀectively deal with the joint feature of nonconvexity and nondiﬀeren-
tiability and to facilitate the design of the MM+SN algorithm and its analysis for
solving the resulting optimization problem. Speciﬁcally, the optimization problem
consists of the following four major components, each playing a separate role in the
overall statistical estimation process. As such, distinguishing them oﬀers ﬂexibility to
accommodate a variety of model speciﬁcations.

• A parametric statistical model: y = ψ(x; θ)+ε, where y is the response (output)
variable (taken to be a scalar) given the input x ∈ Rd, ε is the unobserved random
error assumed to have (conditional) mean zero, and θ ∈ Rm is the model parameter to
be estimated, for some positive integers d (number of input features) and m (number
of parameters). In general, these two dimensions, d and m, may be diﬀerent (see e.g.
(3)); however, they may be the same as in the case of linear regression.

• An output-dependent loss function ϕ(y, •), whose composition with the sta-
tistical model ψ(x; θ) leads to the composite function (θ; y, x) (cid:55)→ ϕ(y, ψ(x; θ)) that
provides a measure of the deviation between the model predicted output ψ(x; θ) and
the observed response y; we stress the importance of this composition as properties of
ϕ and ψ may be very diﬀerent and need to be understood separately for best results.
• A regularizing function P (θ) that is used either for the purpose of strongly
convexifying the objective function or in high-dimensional problems to control the
selection of the model parameter θ; the latter control is particularly important to
avoid overﬁtting when the number of features d is relatively large.

• An admissible set Θ ⊆ Rm that further restricts the parameter space in the
presence of domain knowledge on the parameter θ; often this set Θ is the whole space
Rm, leading to an unconstrained selection of the parameter.

Together, the tuple (ϕ, ψ, Θ) deﬁnes the following constrained minimization prob-

lem that is central to many statistical estimation problems:

(1)

minimize
θ ∈ Θ

IE [ ϕ(y, ψ(x; θ)) ] ,

where the expectation E is taken over the joint distribution of the input x and the
output y. The objective expresses the average loss of accuracy in the model output
versus the realized output taking into account the uncertainties in the random input-
output pair (x, y). This perspective of statistical estimation is in line with stochastic
programming and is a departure from classical statistical estimation where there is
always a postulate of a “ground truth” of the estimator. In this paper, we investi-
gate the numerical solution of problem (1) via the application of the sample average

approximation (SAA) scheme. Speciﬁcally, taking N independent and identically dis-
s=1 ⊆ Rd+1 and adding the regularizer P (θ) weighted by
tributed samples {(xs, ys)}N
the scalar γN ≥ 0, we consider the empirical optimization problem:

(2)

minimize
θ ∈ Θ

1
N

N
(cid:88)

s=1

[ ϕ(ys, ψ(xs; θ)) ] + γN P (θ).

While this formulation (2) is rather classical, the detailed treatment of the distin-
guished properties of the functions (ϕ, ψ, P ) constitutes the novelty and intellectual
merits of the present paper. In what follows, we present some details of these func-
tions and describe how they arise. An important point of the discussion is to motivate
a focused formulation of problem (2) that allows a uniﬁed treatment of these statis-
tical estimation problems as a composite diﬀ-max program by a common algorithmic
procedure with guaranteed convergence properties; see formulation (6) and its setting
(assumptions C1-C3) as well as developments in the subsequent sections.

Statistical models. We are interested in piecewise smooth models, including:

• A continuous piecewise aﬃne parametric model: this is a (generally nonconvex)
piecewise aﬃne function expressed as the diﬀerence of two convex piecewise aﬃne
functions [53] each with its own parameters: for two positive integers ka and kb,

(3)

ψ(x; θ) (cid:44) max
1≤i≤ka

(cid:8) ( ai )T x + αi

(cid:9) − max
1≤i≤kb

(cid:8) ( bi )T x + βi

(cid:9) ,

(cid:111)

(cid:110)(cid:0)ai, αi

(cid:1)ka
i=1 , (cid:0)bi, βi

with parameter θ (cid:44)
∈ R(ka+kb)(d+1). The convex case
kb = 1 was studied in [19, 20, 39]; study of the nonconvex case (where kb > 1)
can be found in the unpublished manuscript [24] and also in [4] with a max-min
representation of the piecewise aﬃne function. Model (3) includes the 1-layer neural
network by the rectiﬁed linear unit (ReLu) activation function [43, 23] that has the
simple form ψ(x; θ) = max(aT x + α, 0) where the parameter θ = (a, α) ∈ Rd+1.

(cid:1)kb
i=1

• A multi-layer neural network with the ReLu activation function: for simplicity,

we present a 2-layer model in deep learning [61],

(4)

ψ(x; θ) (cid:44) max (cid:0) bT max (Ax + a, 0) + β, 0(cid:1) ,

where the parameter θ consists of the vectors b and a in Rk, the matrix A ∈ Rk×d,
and scalar β ∈ R. The two occurrences of the max ReLu functions indicate the action
of 2 hidden layers, where the max of Ax + a and 0 is taken along each coordinate.
Omitting the proof, we can show that this function can be written in the following
diﬀerence-of-max form:

(5)

ψ(x; θ) = max
1≤j≤k1

ψ1,j(x; θ) − max
1≤j≤k2

ψ2,j(x; θ),

for some positive integers k1 and k2 and convex functions ψi,j(x; •) that are all once
but not twice continuously diﬀerentiable piecewise linear-quadratic. (A continuous
function ψ is piecewise linear-quadratic (PLQ) [52, Deﬁnition 10.20] on a domain if
the domain can be represented as the union of ﬁnitely many polyhedral sets on each
of which ψ is a quadratic function; see [52, 54, 55] for properties of piecewise functions
of this type.) The above diﬀerence-of-convex pointwise maximum representation (in
short, diﬀerence-max representation) of ψ(x; θ) has several advantages over the origi-
nal deﬁnition (4). Extending the piecewise aﬃne model (3) to a piecewise quadratic

model, the formulation (5) motivates a uniﬁed form of the function ψ in these two
models. More importantly, one can take advantage of the piecewise linear-quadratic
components for the computation of directional stationary solutions (to be deﬁned
later). Furthermore, while it is clear from (4) that ψ(x; •) is a piecewise quadratic
function, deﬁnition (4) does not immediately reveal the linear-quadratic feature of
this function.

We make two important remarks. One, it is possible to extend the above 2-layer
treatment in two major directions: (i) to multi-layers and (ii) to general piecewise
aﬃne activation functions of which the ReLu function is a special case. Due to their
signiﬁcance, the full treatment of these deep-learning problems is presented in a sep-
arate paper. Two, in the algorithmic development, we will take each ψi,j(x; θ) in (5)
as a once continuously diﬀerentiable convex function.

Loss functions. These include both diﬀerentiable and piecewise aﬃne (thus nondif-
ferentiable) functions. Of particular interest are the following convex loss functions:
the classical quadratic function for least-squares regression; the Huber loss function
in robust estimation; the quantile function [28] that includes the absolute deviation
loss function; the one-parameter exponential family via log-likelihood maximization
[7, 11]. We are also interested in the nonconvex yet piecewise aﬃne truncated hinge
loss function for binary and multicategory classiﬁcation [64] with the form

ϕ(y, t) (cid:44) max(1 − (t − y), 0) − max(δ − (t − y), 0)

for some parameter δ ≤ 0.

With the proof omitted, we can show that a composition of this function with the
diﬀ-max function (5) is also a function of the same kind. Therefore, the composite
function ϕ(y, ψ(x; θ)) is itself a diﬀerence-max function, thus amenable to treatment
by the methodology developed in the later sections.

Regularizers. Traditionally, strongly convex regularizers are very prominent;
in
sparse linear regression, convex and diﬀerence-of-convex regularizers are becoming
popular as they are employed as surrogate sparsity functions. Most of the latter
regularizers are not diﬀerentiable. Frequently used convex regularizers include the
(cid:96)2-norm used in support vector machines [14], the weighted (cid:96)1 in sparsity represen-
tation [26] and the total variation norm in image processing [56]. Examples of dc
surrogate sparsity functions include the smoothly clipped absolute deviation (SCAD)
function [22], the minimax concave penalty function [63], the truncated transformed
(cid:96)1 [60, 18], the truncated logarithmic penalty [12, 18]. It can be shown that all of the
above mentioned dc regularizers can be written in the uniﬁed form

P (θ) =

m
(cid:88)

i=1

ci | θi | −

m
(cid:88)

i=1

pi(θi)

with each pi being a univariate diﬀerentiable convex function, see e.g., [1].

Putting together the above families of functions ϕ(y, •), ψ(x; •) and P (•), we

arrive at the following detailed formulation of problem (2)

(6)

minimize
θ ∈ Θ

fN (θ) (cid:44) 1
N

N
(cid:88)

s=1

ϕs ◦ ψs(θ) + γN [ P1(θ) − P2(θ) ]

with the composite diﬀ-max structure summarized below:
C1: each ϕs is a univariate convex function;

C2: each ψs is a diﬀerence-of-convex function of the pointwise max type (5); speciﬁ-
cally, for some nonnegative integers ks;1 and ks;2,

(7)

ψs(θ) (cid:44) max

1≤i≤ks;1

ψs;1,i(θ) − max

1≤i≤ks;2

ψs;2,i(θ)

with each ψs;1,i(θ) and ψs;2,i(θ) being convex and continuously diﬀerentiable;
C3: both P1 and P2 are convex with P2 being additionally the pointwise maximum
(cid:3)
of ﬁnitely many convex diﬀerentiable functions.

z∈Z

In this setting, the overall objective function fN is not convex. Since the compo-
sition of a convex function with a dc function is of the dc type, by [25, Theorem II,
page 708], it follows that fN is a dc function; thus the diﬀerence-of-convex algorithm
(DCA) in dc programming [33, 32] is in principle applicable to compute a critical point
[ g(z) − h(z) ]
of fN on the feasible set Θ. In general, for a dc program: minimize
where Z is a closed convex set in Rn and g and h are convex functions, a vector ¯z ∈ Z
is a critical point if ∂h(¯z) ∩ [ ∂g(¯z) + N (¯z; Z) ] (cid:54)= ∅, where the notation ∂ϕ(¯z) denotes
the subdiﬀerential of a convex function ϕ at a given vector ¯z and N (¯z; Z) denotes the
normal cone of the set Z at ¯z ∈ Z. Nevertheless, this criticality concept has several
major drawbacks. One, it depends on the dc representation of the objective function
fN . Two, although fN is known to be dc, a dc decomposition is not readily available
when each of the composite functions ϕs ◦ ψs is derived from the statistical functions
given above. Third, as will be shown in the next section, criticality is a very weak
property and can have no bearings at all with a desirable minimizing property. For
these reasons, our research goal is to seek an alternative deﬁnition of stationarity that
is the “sharpest” of its kind and which is applicable to problem (5) without demanding
a dc decomposition of the composite functions ϕs ◦ ψs. One shall see from the subse-
quent discussion that the particular (diﬀerence-of) pointwise maximum structures of
ψs and P2 are critical to achieve this goal.

3. A Detour: Subdiﬀerentials and Stationarity. As a ﬁrst step in studying
the composite nonconvex, nondiﬀerentiable problem (6), we take a detour to present
some results from variational analysis; see [52] for details. These materials will prepare
for the introduction of two key notions of stationary solutions that provide the objects
of convergence of the iterative algorithms for solving problem (6), to be presented in
the following sections. Let Φ : Ω → Rm be a locally Lipschitz continuous vector-valued
function deﬁned on an open set Ω ⊆ Rn. It follows that Φ is F(r´echet)-diﬀerentiable
almost everywhere on Ω (c.f. [52, Theorem 9.60]). Denote by DΦ ⊆ Ω the set of points
where Φ is F-diﬀerentiable and by JΦ(x) ∈ Rm×n the Jacobian of Φ at x ∈ DΦ. Let
¯x ∈ Ω be given. The B(ouligand)-subdiﬀerential of Φ at ¯x is denoted by

(cid:26)

∂BΦ(¯x) (cid:44)

V ∈ Rm×n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∃ {xk} ⊆ DΦ with lim
k→∞

xk = ¯x and lim
k→∞

JΦ(xk) = V

(cid:27)

.

The Clarke subdiﬀerential (also called the Clarke generalized Jacobian) of Φ at ¯x is
deﬁned as ∂CΦ(¯x) (cid:44) conv ( ∂B Φ(¯x) ), where “conv” stands for the convex hull of a
given set. For a real-valued function φ : Ω → R, the Clarke subdiﬀerential of φ at ¯x
can also be characterized by

(cid:40)

∂Cφ(¯x) =

v ∈ Rn

(cid:12)
(cid:12)
(cid:12)
(cid:12)

lim sup
x→¯x, t↓0

φ(x + tw) − φ(x) − t vT w
t

≥ 0,

∀ w ∈ Rn

.

(cid:41)

The regular subdiﬀerential of φ at ¯x is deﬁned as

(cid:98)∂φ(¯x) (cid:44)

(cid:26)

v ∈ Rn

(cid:12)
(cid:12)
(cid:12)
(cid:12)

lim inf
¯x(cid:54)=x→¯x

φ(x) − φ(¯x) − vT (x − ¯x)
(cid:107) x − ¯x (cid:107)

(cid:27)

≥ 0

.

The limiting subdiﬀerential of φ at ¯x is deﬁned as

∂φ(¯x) (cid:44)

(cid:26)

v ∈ Rn

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∃ {xk} → ¯x and {vk} → v such that vk ∈ (cid:98)∂ φ(xk) for all k

(cid:27)

.

When φ is convex, its Clarke subdiﬀerential, regular subdiﬀerential and limiting sub-
diﬀerential coincide with the set of all subgradients of φ. In general, the relationship
between the above four deﬁnitions is demonstrated in Figure 1. The detailed expla-
nations are given in the proposition below.

Proposition 1. Let Ω be an open set in Rn. For any locally Lipschitz continuous

function φ : Ω → R and any ¯x ∈ Ω, it holds that:
(i)

⊆ ∂φ(¯x) ⊆ ∂Cφ(¯x);

∂Bφ(¯x) ∪ (cid:98)∂φ(¯x)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(ii) conv
Moreover, counter-examples exist for the omitted inclusions.

⊆ conv ( ∂Bφ(¯x) ) = conv ( ∂φ(¯x) ) = ∂Cφ(¯x).

(cid:98)∂φ(¯x)

Proof. It is known from [52, Theorem 8.6] that (cid:98)∂φ(¯x) ⊆ ∂φ(¯x) and from [42,
Theorem 3.57] that conv( ∂φ(¯x) ) = ∂C φ(¯x). Thus, it remains to show that ∂Bφ(¯x) ⊆
∂φ(¯x). Let ¯v ∈ ∂Bφ(¯x). Then there exists {xk} ⊆ Ω converging to ¯x such that φ is
F-diﬀerentiable at xk and ∇φ(xk) → ¯v. Since (cid:98)∂φ(xk) = {∇φ(xk)} (c.f. [52, Exercise
8.8]), we have ¯v ∈ ∂φ(¯x) by the deﬁnition of ∂φ.

We provide two examples to show the possible invalidity of the omitted inclusions.
For φ(x) = |x|, we have ∂φ(0) = (cid:98)∂φ(0) = [−1, 1] (cid:42) ∂Bφ(0) = {−1, 1}. For
φ(x) = −|x|, we have ∂Cφ(0) = [−1, 1] (cid:42) ∂Bφ(0) = ∂φ(0) = {−1, 1} (cid:42) (cid:98)∂φ(0) = ∅
and conv ( ∂Bφ(0) ) = [−1, 1] (cid:42) conv ( (cid:98)∂φ(0) ) = ∅.

(cid:98)∂φ(x)

∂Bφ(x)

∂φ(x)

∂C φ(x)

locmin

d-stat
(for dd fncs.)

l-stat

C-stat
critical
(for dc fncs.)

Fig. 1: Relationship between the subdiﬀerentials

Fig. 2: Relationship between the stationary points

Let X be a closed convex set in Ω. It is known that a necessary condition for
¯x ∈ X to be a local minimum of φ is 0 ∈ (cid:98)∂ (φ(¯x) + δX (¯x)) [52, Theorem 10.1],
where δX (x) (cid:44)

is the indicator function of X at x.

if x ∈ X

If φ is

(cid:26) 0

+∞ otherwise

locally Lipschitz continuous near ¯x and directionally diﬀerentiable (dd) at ¯x, the
latter condition is equivalent to the d(irectional)-stationarity of ¯x, i.e.,

φ (cid:48)(¯x; v) (cid:44) lim
δ↓0

φ(¯x + δv) − φ(¯x)
δ

≥ 0,

∀ v ∈ X − ¯x.

Using the limiting subdiﬀerential and Clarke subdiﬀerential, respectively, we say that
a point ¯x ∈ X is a l(imiting)-stationary point of φ on X if 0 ∈ ∂ (φ(¯x) + δX (¯x)), and

a C(larke)-stationary point if 0 ∈ ∂Cφ(¯x) + N (¯x; X), which implies that

φ◦(¯x; v) (cid:44) lim sup
x→¯x, δ↓0

φ(x + δv) − φ(x)
δ

≥ 0,

∀ v ∈ X − ¯x.

Based on Proposition 1 and [52, Exercise 10.10], l-stationarity implies C-stationarity;
and if φ is locally Lipschitz near ¯x and dd at this point, d-stationarity implies l-
stationarity. The following examples show that the reverse implications do not hold
without additional assumptions.

Example 1. Consider the univariate function φ(x) = max( −|x|, x−1 ) for x ∈ R.
Since ∂C φ(0) = [−1, 1] and ∂φ(0) = {−1, 1}, it holds that x = 0 is a C-stationary
point of φ, but fails to be a l-stationary point. The unique l-stationary point of φ on
R is x = 1
2 . For the univariate function φ(x) = max ( −x − 1, min(−x, 0) ), since
∂φ(0) = {−1, 0} and (cid:98)∂φ(0) = ∅, it holds that x = 0 is a l-stationary point of φ, but
(cid:3)
not a d-stationary point. The unique d-stationary point of φ on R is x = −1.
If φ = φ1 − φ2 is a diﬀerence of two convex functions φ1 and φ2, it holds that
∂Cφ(x) ⊆ ∂Cφ1(x) − ∂Cφ2(x) for any x ∈ Ω (cf. [13, Corollary 2]). A point ¯x ∈ Ω is
said to be a critical point of φ on X if

∂Cφ2(¯x) ∩ [ ∂Cφ1(¯x) + N (¯x; X) ] = ∂φ2(¯x) ∩ [ ∂φ1(¯x) + N (¯x; X) ] (cid:54)= ∅.

Diﬀerent from the above mentioned concepts of stationary points, a critical point
depends on the dc decomposition. Since there are inﬁnite many dc decompositions
of a given dc function, it is likely that a critical point provides no information on the
local minima of that function. This can be seen from the following example.

Example 2. Consider the univariate function φ(x) = max{x, −x − 4} on R, for
which x = −2 is the unique C-stationary point (and the global minimizer). For the
dc decomposition φ = φ1 − φ2 with φ1(x) = max{2x, 0, −2x − 4} and φ2(x) = |x|, we
have ∂φ1(0) ∩ ∂φ2(0) = [0, 1], implying that x = 0 is a critical point of φ on R. (cid:3)
The relationship between the various kinds of stationary points and a critical
point (for dc problems) is summarized in Figure 2. As a caution to the reader, we
note that 0 ∈ ∂Bφ(¯x) is not a necessary condition for ¯x being a local minima of a
locally Lipschitz continuous function φ; this can be seen from φ(x) = |x| at x = 0.

We close this section by mentioning three properties of d-stationarity that further
highlight the fundamental importance of this stationarity concept. The ﬁrst property
asserts that a d-stationary point must be “locally ε-ﬁrst-order minimizing”; the sec-
ond and third property are applicable to the least-squares piecewise aﬃne regression
problem. We recall that a function ψ is B(ouligand)-diﬀerentiable at a point ¯x [21,
Deﬁnition 3.1.2] if it is both locally Lipschitz continuous near ¯x and directionally
diﬀerentiable at ¯x. Proof of the proposition below is omitted as it is not diﬃcult; for
related results, see [15, 16].

Proposition 2. Let X be a closed convex set contained in the open set Ω ⊆ Rn.

The following three statements hold:
(i) Let ψ : Ω → R be B-diﬀerentiable at a d-stationary point ¯x ∈ X. It holds that for
every ε > 0, there exists an open neighborhood N of ¯x such that ψ(x) ≥ ψ(¯x)−ε (cid:107)x−¯x (cid:107)
for all x ∈ X ∩ N .
(ii) Let ψ : Ω → Rm be piecewise aﬃne and Φ : Rm → R be convex. It holds that every
d-stationary point of the composite function Φ ◦ ψ on the set X is a local minimizer.
(iii) If X is polyhedral and ψ is piecewise linear-quadratic on X, then the set of values
of ψ on the set of d-stationary points of ψ on X is ﬁnite.

4. The Nonmonotone Majorization-Minimization Algorithm. We aim
to compute a d-stationary solution of problem (6). The approach we propose is to
apply the majorization-minimization (MM) algorithm with suitable modiﬁcations.
Originally described in [44, Section 9.3(d)], the idea of the basic MM algorithm is to
solve a sequence of convex minimization subproblems by creating surrogate functions
that majorize the original objective function. It uniﬁes various optimization methods,
including the projected/proximal gradient method [6] and the dc algorithm [33, 32].
It is a generalization of the expectation-maximization (EM) algorithm, as known in
the statistics community, for ﬁnding maximum likelihood estimators of parameters
in statistical models [17, 58]. See [27, 31] for comprehensive discussions of the MM
algorithm (without modiﬁcations) and [37, 38, 9] for recent developments.

Given a locally Lipschitz continuous function φ : Ω → R deﬁned on an open
set Ω ⊆ Rn containing the closed convex set X and a point y ∈ X, the continuous
function (cid:98)φ (•, y) is said to be a majorizing function of φ if: (i) (cid:98)φ (•, y) is convex on X;
(ii) φ(x) ≤ (cid:98)φ (x, y) for all x ∈ X; and (iii) (cid:98)φ (y) = (cid:98)φ (y, y). The key to a successful
application of the MM algorithm to the optimization problem (6) hinges on a readily
available convex majorization of the objective function fN at a given iterate θ ν. In
turn, it suﬃces to derive such a majorizing function for each summand ϕs ◦ ψs; for the
dc regularizer P (θ) = P1(θ)−P2(θ) a convex majorizing function is easily obtained as:
(cid:98)P (θ, θ ν) = P1(θ) − [ P2(θ ν) + (a2;ν)T (θ − θ ν) ], where a2;ν ∈ ∂P2(θν). The following
fact combined with the diﬀerence-max structure (7) of each function ψs easily yields
a convex majorant of each composite function ϕs ◦ ψs.

Lemma 3. A univariate convex function f can be written as the sum of a convex

non-decreasing function f ↑ and a convex non-increasing function f ↓.

Proof. Note that if there exists t0 ∈ R such that f (cid:48)(t0; ±1) ≥ 0, then f achieves
its minimum value at t0. By the convexity of f , one can easily check that f is
non-increasing on (−∞, t0 ] and non-decreasing on [ t0, ∞), leading to a choice of
f ↑(t) (cid:44)

(cid:26) f (t) − f (t0)

and f ↓(t) (cid:44)

To see the

(cid:26) f (t0)
f (t)

if t ≥ t0
if t < t0.

if t ≥ t0
if t < t0

0

convexity of f ↑, it suﬃces to observe that for any t1 and t2 in R satisfying t1 < t0 < t2
and for any λ ∈ (0, 1),

f ↑((1 − λ)t1 + λt2) ≤ f ↑((1 − λ)t0 + λt2) = f ((1 − λ)t0 + λt2) − f (t0)

≤ (1 − λ)f (t0) + λf (t2) − f (t0) = (1 − λ) f ↑(t1) + λ f ↑(t2).

By a similar argument, we can show the convexity of f ↓.

If such t0 does not exist, then for all t ∈ R, either f (cid:48)(t; 1) ≥ 0 ≥ f (cid:48)(t; −1), or
f (cid:48)(t; 1) ≤ 0 ≤ f (cid:48)(t; −1). The former situation implies that f is non-decreasing on R;
hence we may take f ↑ = f and f ↓ = 0 in this case; while the latter situation implies
that f is non-increasing on R; hence we may choose f ↓ = f and f ↑ = 0.

Since a convex non-decreasing (non-increasing) function composed with a convex
(concave) function is convex, the following corollary is easy to obtain. The proof is
omitted for brevity.

Corollary 4. Let ϕ be a univariate convex function with the decomposition ϕ =
ϕ ↑ + ϕ ↓ as in Lemma 3. For any θ 0 ∈ Θ, if (cid:98)ψ(•, θ 0) is a concave minorizing function
and q

ψ(•, θ 0) a convex majoring function of ψ on Θ, i.e.,

(cid:98)ψ(θ, θ 0) ≤ ψ(θ) ≤ q

ψ(θ, θ 0),

∀ θ ∈ Θ,

then ϕ ↑ ◦ q

ψ(•, θ 0) + ϕ ↓ ◦ (cid:98)ψ(•, θ 0) is a convex majorant of the composite ϕ ◦ ψ on Θ.

To simplify the notation, we present the MM algorithm and its convergence prop-

erty for solving the following single-summand formulation of (6):

(8)

minimize
θ∈Θ

Ψ(θ) (cid:44) ϕ ◦ ψ(θ),

where ϕ is a univariate convex function and ψ(θ) = g(θ) − h(θ), where

g(θ) (cid:44) max
1≤i≤k1

ψ1,i(θ)

and

h(θ) (cid:44) max
1≤i≤k2

ψ2,i(θ),

with each ψ1,i and ψ2,i being a diﬀerentiable convex function. The treatment is

N
(cid:88)

ϕs ◦ ψs(θ) + γN P (θ) with each pair (ϕs, ψs) as above and

clearly extendable to

1
N

s=1

P satisfying assumption C3. We omit the details of the extended treatment but will
employ it in the computational experiments; see Section 6.

Denote the index sets of maximizing functions in g and h at θ ∈ Θ as,

A1(θ) (cid:44) argmax
1≤i≤k1

{ ψ1,i(θ) }

and A2(θ) (cid:44) argmax
1≤i≤k2

{ ψ2,i(θ) } .

For any (θ, ¯θ) ∈ Θ × Θ and any (i1, i2) ∈ A1(¯θ) × A2(¯θ), we have
(cid:2) g(¯θ) + ∇ψ1,i1(¯θ)T (cid:0) θ − ¯θ (cid:1) (cid:3)
(cid:125)
(cid:123)(cid:122)
(cid:124)
linearization of ψ1,i1 at ¯θ

−h(θ) ≤ ψ(θ) ≤ g(θ) − (cid:2) h(¯θ) + ∇ψ2,i2(¯θ)T (cid:0) θ − ¯θ (cid:1) (cid:3)
(cid:125)
(cid:123)(cid:122)
(cid:124)
linearization of ψ2,i2 at ¯θ

,

which, by Corollary 4, leads to the following convex majorant of Ψ in (8):
MΨ(i1,i2)(θ, ¯θ) (cid:44) ϕ ↑ (cid:0) g(θ) − (cid:2)h(¯θ) + ∇ψ2,i2 (¯θ)T (cid:0)θ − ¯θ(cid:1)(cid:3) (cid:1) +
ϕ ↓ (cid:0) (cid:2)g(¯θ) + ∇ψ1,i1(¯θ)T (cid:0)θ − ¯θ(cid:1)(cid:3) − h(θ) (cid:1) .

The above function is nonsmooth because of the “max” in the functions g and h.
Notice that we essentially leave ϕ unchanged instead of approximating it.
In this
way, we may presumably obtain a tighter approximation of the original composite
objective function ϕ ◦ ψ.

4.1. From pointwise max to constraints. Before describing the MM-based
algorithm that can be shown to converge to a d-stationary solution of problem (8),
we ﬁrst present the following lemma that characterizes a d-stationary solution of this
problem as a solution of ﬁnitely many convex programs.

Lemma 5. The point θ ∈ Θ is a d-stationary point of (8) if and only if for all
MΨ(i1,i2)(θ, ¯θ).

(i1, i2) ∈ A1(¯θ) × A2(¯θ), ¯θ ∈ argmin

θ∈Θ

Proof. “Only if.” If θ ∈ Θ is a d-stationary point of (6), then for all θ ∈ Θ,

0 ≤ Ψ (cid:48)(θ; θ − θ) = ϕ (cid:48)(ψ(θ); ψ (cid:48)(θ; θ − θ))

= (cid:0)ϕ ↑(cid:1)(cid:48) (cid:0)ψ(θ); ψ (cid:48)(θ; θ − θ)(cid:1) + (cid:0)ϕ ↓(cid:1)(cid:48) (cid:0)ψ(θ); ψ (cid:48)(θ; θ − θ)(cid:1) .

We have ψ(θ) = g(θ) − h(θ) and

ψ (cid:48)(θ; θ − θ) = max
i∈A1(θ)
(cid:124)

∇ψ1,i(θ)T (cid:0) θ − θ (cid:1)

∇ψ2,i(θ)T (cid:0) θ − θ (cid:1)

.

(cid:123)(cid:122)
= g (cid:48)(θ; θ − θ)

(cid:123)(cid:122)
= h (cid:48)(θ; θ − θ)

(cid:125)

− max
i∈A2(θ)
(cid:124)

(cid:125)

Since ϕ ↑ is a univariate non-decreasing convex function, it follows that the directional
derivative (cid:0)ϕ ↑(cid:1)(cid:48)
(t; •) is a non-decreasing function of the direction for every t ∈ R.
Hence, for every i2 ∈ A2(θ),
(cid:0)ϕ ↑(cid:1)(cid:48)

(ψ(θ); ψ (cid:48)(θ; θ − θ)) = (cid:0)ϕ ↑(cid:1)(cid:48) (cid:0)g(θ) − h(θ); g (cid:48)(θ; θ − θ) − h (cid:48)(θ; θ − θ)(cid:1)

≤ (cid:0)ϕ ↑(cid:1)(cid:48) (cid:0)g(θ) − h(θ); g (cid:48)(θ; θ − θ) − ∇ψ2,i2(θ)T (cid:0) θ − θ (cid:1)(cid:1) .

Similarly, we also have, for every i1 ∈ A1(θ),

(cid:0)ϕ ↓(cid:1)(cid:48)

(ψ(θ); ψ (cid:48)(θ; θ − θ)) ≤ (cid:0)ϕ ↓(cid:1)(cid:48) (cid:0)g(θ) − h(θ); ∇ψ1,i1 (θ)T (cid:0) θ − θ (cid:1) − h (cid:48)(θ; θ − θ)(cid:1) ,

because (cid:0)ϕ ↓(cid:1)(cid:48)
(t; •) is a non-increasing function for every t ∈ R. It therefore follows
that MΨ(i1,i2)(•, ¯θ) (cid:48)(¯θ; θ − ¯θ) ≥ 0 for all θ ∈ Θ. Since MΨ(i1,i2)(•, ¯θ) is a convex
program, it follows that ¯θ is a minimizer of this function over Θ.

“If.” Conversely, suppose that for every (i1, i2) ∈ A1(¯θ) × A2(¯θ), it holds that ¯θ ∈
∇ψ1,i(θ)T (cid:0) θ − θ (cid:1)

MΨ(i1,i2)(θ, ¯θ). Let θ ∈ Θ be arbitrary. Pick i1 ∈ argmax
i∈A1(θ)

argmin
θ∈Θ

and i2 ∈ argmax
i∈A2(θ)

∇ψ2,i(θ)T (cid:0) θ − θ (cid:1). We then have

Ψ (cid:48)(θ; θ − θ) = MΨ(i1,i2)(•, ¯θ) (cid:48)(¯θ; θ − ¯θ) ≥ 0.

Since θ ∈ Θ is arbitrary, it follows that ¯θ is a d-stationary point of Ψ on Θ.

Lemma 5 indicates that there is a “for all index pairs” condition in the requirement
of d-stationarity. Though MΨ(i1,i2)(•, θ ν) is a convex majorant of Ψ for any pair
(i1, i2) ∈ A1(θ ν) × A2(θ ν), by arbitrarily picking a single pair of indices (i1, i2) at
each iteration of the MM algorithm, one may not obtain an algorithm that converges
to a d-stationary point. This non-convergence has been observed in [47] when the dc
algorithm (a special MM algorithm) is applied to solve dc programs (a special case
of problem (8) with ϕ being the identity function); see speciﬁcally Example 4 in this
reference.
In order for the MM algorithm to converge to a d-stationary point, we
employ the ε-technique to expand the argmax index sets of the functions g and h.
Speciﬁcally, given ε > 0 and θ ∈ Θ, let






A1;ε(θ) (cid:44) ε-argmax
1≤i≤k1
A2;ε(θ) (cid:44) ε-argmax
1≤i≤k2

g(θ) = { 1 ≤ i ≤ k1 | ψ1,i(θ) ≥ g(θ) − ε } ,

h(θ) = { 1 ≤ i ≤ k2 | ψ2,i(θ) ≥ h(θ) − ε } .

To facilitate the solution of the MM subproblem, we further introduce auxiliary
variables r and s in R to write the max functions in g and h as constraints, thus
smoothing out the arguments in the functions ϕ ↑ and ϕ ↓, and then regularize the
added variables in the objective function. Speciﬁcally, for any ¯θ ∈ Θ and any (i1, i2) ∈
A1(¯θ) × A2(¯θ), deﬁne the convex set

Z(i1,i2)(¯θ) (cid:44)

(cid:40) z (cid:44) (θ, r, s)
∈ Θ × R × R

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ψ1,j(θ) − h(¯θ) − ∇ψ2,i2 (¯θ)T (cid:0) θ − ¯θ (cid:1) ≤ r, 1 ≤ j ≤ k1
g(¯θ) + ∇ψ1,i1 (¯θ)T (cid:0) θ − ¯θ (cid:1) − ψ2,j(θ) ≥ s, 1 ≤ j ≤ k2

(cid:41)

,

which must contain the triple (¯θ, r, s) for any pair of scalars (r, s) satisfying r ≥ ψ(¯θ) ≥
s. Since, ϕ ↑ is non-decreasing and ϕ ↓ is non-increasing, we have

(9)

ϕ ↑(r) + ϕ ↓(s) ≥ MΨ(i1,i2)(θ, ¯θ) ≥ Ψ(θ),

for all (θ, r, s) ∈ Z(i1,i2)(¯θ).

Moreover, equalities hold throughout (9) if r = s = ψ(θ).

With the above preparations, the MM-based algorithm is given below.

The Nonmonotone Majorization-Minimization algorithm for solving (8).

Initialization. Given are positive scalars c and ε and an initial point θ 0 ∈ Θ. Let
z 0 (cid:44) (θ 0, r0, s0) where r0 (cid:44) ψ(θ 0) (cid:44) s0. Set ν = 0.
Step 1. For every pair (iν

2) ∈ A1;ε(θ ν) × A2;ε(θ ν), compute

1, iν

(10)

z ν+ 1

2 ;iν

1 ,iν

2 (cid:44)

argmin
1 ,iν

2 )(θ ν )

z∈Z(iν

(cid:110)

(cid:98)Ψc(z, z ν) (cid:44) ϕ ↑(r) + ϕ ↓(s) +

(cid:107) z − z ν (cid:107)2 (cid:111)

.

c
2

Step 2. Set z ν+1 (cid:44) z ν+ 1

2 ;(cid:98)i ν

1 ,(cid:98)i ν

2 , where ((cid:98)i ν

1 , (cid:98)i ν

2 ) is a minimizing index in

(cid:110)

argmin

(cid:98)Ψc(zν+ 1

2 ;i ν

1 ,i ν

2 , z ν) (cid:12)

(cid:12) (i ν

1 , i ν

2 ) ∈ A1;ε(θ ν) × A2;ε(θ ν)

(cid:111)

.

Step 3. If z ν+1 satisﬁes a prescribed stopping rule, terminate; otherwise, return to
(cid:3)
Step 1 with ν replaced by ν + 1.

Because of the regularization of the variables r and s, subproblem (10) is a slight

modiﬁcation of the MM subproblem which would be:

(11)

minimize
1 ,iν

2 )(θ ν )

z∈Z(iν

(cid:101)Ψc(z, θ ν) (cid:44) ϕ ↑(r) + ϕ ↓(s) +

c
2

(cid:107) θ − θ ν (cid:107)2.

The modiﬁcation is needed when we discuss the solution of (10) that is based on a
smoothness property of its Lagrangian dual. Unlike (11), a minimizing triple (θ, r, s)
of (10) does not necessarily satisfy r = g(θ) − (cid:2) h(θ ν) + ∇ψ2,iν
2 (θ ν)T ( θ − θ ν ) (cid:3) or
s = (cid:2) g(θ ν) + ∇ψ1,iν
1 (θ ν)T ( θ − θ ν ) (cid:3) − h(θ) because of the regularization of these
2 )(θ ν) changes with the iterate θ ν. Due
variables. In addition, the feasible set Z(iν
1 ,iν
to all these anomalies, care is needed in the proof of convergence of the algorithm.
In particular, the sequence {Ψ(θ ν)} of objective values of the original function Ψ to
be minimized is not shown to be decreasing; instead it is the substituted sequence
(cid:8)ϕ ↑(rν+1) + ϕ ↓(sν+1)(cid:9) that is decreasing. The term “nonmonotone” is employed
to highlight this non-standard feature of the algorithm. We ﬁrst give a necessary
and suﬃcient condition for a vector ¯θ to be an optimal solution of the problem:
minimize
θ∈Θ
Lemma 6. A vector ¯θ ∈ argmin

MΨ(i1,i2)(θ, ¯θ), where (i1, i2) ∈ A1(¯θ) × A2(¯θ), if
and only if there exist a scalar c > 0 and a pair of scalars (¯r, ¯s) such that the triple
¯z (cid:44) (¯θ, ¯r, ¯s) ∈ argmin

MΨ(i1,i2)(θ, ¯θ), this being a key requirement in the d-stationarity of ¯θ.

θ∈Θ

(cid:98)Ψc(z, ¯z).

z∈Z(i1 ,i2)(¯θ)

Proof. “Only if.” Suppose ¯θ ∈ argmin

MΨ(i1,i2)(θ, ¯θ). Let c > 0 be arbitrary. It

then follows that ¯θ ∈ argmin

(cid:104)

θ∈Θ

θ∈Θ
MΨ(i1,i2)(θ, ¯θ) +

(cid:107) θ − ¯θ (cid:107)2 (cid:105)

c
2

. Deﬁne ¯r (cid:44) ψ(¯θ) (cid:44) ¯s.

Then ¯z (cid:44) (¯θ, ¯r, ¯s) ∈ Z(i1,i2)(¯θ). Let z (cid:44) (θ, r, s) ∈ Z(i1,i2)(¯θ) be arbitrary. we have

(cid:98)Ψc(z, ¯z) = ϕ ↑(r) + ϕ ↓(s) +

≥ MΨ(i1,i2)(θ, ¯θ) +
≥ MΨ(i1,i2)(¯θ, ¯θ)

(cid:107) z − ¯z (cid:107)2

c
2
c
2
by assumption on ¯θ

(cid:107) θ − ¯θ (cid:107)2

by (9)

= ϕ ↑(¯r) + ϕ ↓(¯s) = (cid:98)Ψc(¯z, ¯z)

since ¯r = ψ(¯θ) = ¯s.

“If.” Conversely, suppose ¯z (cid:44) (¯θ, ¯r, ¯s) ∈ argmin

follows that ¯z (cid:44) (¯θ, ¯r, ¯s) ∈ argmin

z∈Z(i1 ,i2)(¯θ)

(cid:98)Ψc(z, ¯z) for some scalar c > 0. It

z∈Z(i1 ,i2)(¯θ)
(cid:2) ϕ ↑(r) + ϕ ↓(s) (cid:3), which implies in particular

that ¯r ≥ ψ(¯θ) ≥ ¯s. Hence (¯θ, ψ(¯θ), ψ(¯θ)) ∈ argmin

(cid:2) ϕ ↑(r) + ϕ ↓(s) (cid:3) and ϕ ↑(¯r) +

ϕ ↓(¯s) = Ψ(¯θ) = MΨ(i1,i2)(¯θ, ¯θ). Let θ ∈ Θ be arbitrary. Deﬁne

z∈Z(i1,i2)(¯θ)

r (cid:44) g(θ)−(cid:2) h(¯θ) + ∇ψ2,i2(¯θ)T (cid:0) θ − ¯θ (cid:1) (cid:3) ,

s (cid:44) (cid:2) g(¯θ) + ∇ψ1,i1(¯θ)T (cid:0) θ − ¯θ (cid:1) (cid:3)−h(θ).

Then (θ, r, s) ∈ Z(i1,i2)(¯θ) and ϕ ↑(r) + ϕ ↓(s) = MΨ(i1,i2)(θ, ¯θ). It therefore follows
that MΨ(i1,i2)(θ, ¯θ) ≥ MΨ(i1,i2)(¯θ, ¯θ), establishing that ¯θ ∈ argmin

MΨ(i1,i2)(θ, ¯θ).

θ∈Θ

Now we present the main result of this section on the subsequential convergence
of {θ ν} to a d-stationary point of (8), which generalizes the result in [47, Proposition
6] for the dc algorithm to solve dc programs.

Proposition 7. Suppose that Ψ is bounded below on the closed convex set Θ. Let
{z ν = (θ ν, rν, sν)} be a sequence generated by the MM algorithm. The following four
statements hold.
(a) For any (i ν

2 ) ∈ A1;ε(θ ν) × A2;ε(θ ν) and z ∈ Z(i ν

2 )(θ ν), it holds that

1 , i ν

1 ,i ν

ϕ ↑(rν+1) + ϕ ↓(sν+1) +

c
2

(b) lim
ν→∞

(cid:107) z ν+1 − z ν (cid:107) = 0.

(cid:107) z ν+1 − z ν (cid:107)2 ≤ ϕ ↑(r) + ϕ ↓(s) +

c
2

(cid:107) z − z ν (cid:107)2.

(c) For any accumulation point (θ ∞, r∞, s∞) of {z ν}, if it exists, θ ∞ is a d-stationary
point of (8).
(d) If the set {θ ∈ Θ | Ψ(θ) ≤ Ψ(θ 0)} is bounded, then the sequence {θ ν} is bounded.
If in addition, the set {t | ϕ(t) ≤ Ψ(θ 0)} is bounded, then the sequence {(rν, sν)} is
also bounded.

Proof. (a) For any (i ν

1 , i ν

2 ) ∈ A1;ε(θ ν) × A2;ε(θ ν) and z ∈ Z(i ν

2 )(θ ν), we derive

1 ,i ν

ϕ ↑(r) + ϕ ↓(s) +
(cid:17)

rν+ 1

2 ;i ν

1 ,i ν
2

≥ ϕ ↑ (cid:16)

(cid:107) z − z ν (cid:107)2

c
2
+ ϕ ↓ (cid:16)

≥ ϕ ↑(rν+1) + ϕ ↓(sν+1) +

(cid:17)

+

2 ;i ν

1 ,i ν
2

c
2
(cid:107) z ν+1 − z ν (cid:107)2,

sν+ 1
c
2

(cid:107) z ν+ 1

2 ;i ν

1 ,i ν

2 − z ν (cid:107)2

1 , i ν
where the two inequalities are due to the deﬁnitions of z ν+ 1
(b) By the strong convexity of (cid:98)Ψc(•, z ν), it follows that z ν+ 1
2 is the unique
minimizer of (10). We claim that z ν ∈ Z(i1,i2)(θ ν) for all ν and all (i1, i2) ∈
{1, . . . , k1} × {1, . . . , k2}. Indeed, this is clearly true for ν = 0 by the choice of (r0, s0).

2 and z ν+1.
2 ;iν

2 ; i ν

1 ,iν

For any ν ≥ 0, since z ν+1 ∈ Z(iν
have

2 )(θ ν) for some (iν

1 ,iν

1, iν

2) ∈ A1;ε(θ ν) × A2;ε(θ ν), we

rν+1 ≥ g(θ ν+1) − (cid:2) h(θ ν) + ∇ψ2,iν

2 (θ ν)T (cid:0) θ ν+1 − θ ν (cid:1) (cid:3)
2 (θ ν+1) ≥ g(θ ν+1) − h(θ ν+1) = ψ(θ ν+1).

≥ g(θ ν+1) − ψ2,iν

Similarly, we can deduce sν+1 ≤ ψ(θ ν+1). Consequently, the claim holds. It then
follows from (a) that

(12)

ϕ ↑(rν+1) + ϕ ↓(sν+1) +

c
2

(cid:107) z ν+1 − z ν (cid:107)2 ≤ ϕ ↑(rν) + ϕ ↓(sν),

∀ ν.

Thus the sequence (cid:8)ϕ ↑(rν) + ϕ ↓(sν)(cid:9) is non-increasing. Since ϕ ↑(rν) + ϕ ↓(sν) ≥
Ψ(θ ν) and Ψ is bounded below on Θ, it follows that the sequence (cid:8)ϕ ↑(rν) + ϕ ↓(sν)(cid:9)
converges. Hence by (12), the sequence (cid:8)(cid:107)zν+1 − zν(cid:107)(cid:9) converges to zero.
(c) Let z ∞ (cid:44) (θ ∞, r∞, s∞) be the limit of a convergent subsequence {z ν}ν∈κ. We
must have θ ∞ ∈ Θ. To prove that θ ∞ is a d-stationary point, it suﬃces to show, by
(cid:98)Ψc(z, z ∞) for all (i1, i2) ∈
Lemma 5 and Lemma 6, that z ∞ belongs to

argmin
z∈Z(i1 ,i2)(θ ∞)

A1(θ∞)×A2(θ∞). Consider any (i1, i2) ∈ A1(θ ∞)×A2(θ ∞) and any z ∈ Z(i1,i2)(θ ∞).
Deﬁne

(cid:98)rν (cid:44) r + (cid:12)
(cid:12)
(cid:98)sν (cid:44) s + (cid:12)
(cid:12)

(cid:2) h(θ ν ) + ∇ψ2,i2 (θ ν )T ( θ − θ ν ) (cid:3) − (cid:2) h(θ ∞) + ∇ψ2,i2 (θ ∞)T ( θ − θ ∞ ) (cid:3)(cid:12)
(cid:12)
(cid:2) g(θ ν ) + ∇ψ1,i1 (θ ν )T ( θ − θ ν ) (cid:3) − (cid:2) g(θ ∞) + ∇ψ1,i1 (θ ∞)T ( θ − θ ∞ ) (cid:3)(cid:12)
(cid:12) .

We then have (cid:98)z ν (cid:44) (θ, (cid:98)rν, (cid:98)sν) ∈ Z(i1,i2)(θ ν) and
( (cid:98)rν, (cid:98)sν ) = (r, s). One
can easily show that for all ν ∈ κ suﬃciently large, A1(θ ∞) × A2(θ ∞) ⊆ A1;ε(θ ν) ×
A2;ε(θ ν), which in turn implies (i1, i2) ∈ A1;ε(θ ν)×A2;ε(θ ν) for all such ν. It therefore
follows from (a) that

lim
ν(∈κ)→∞

ϕ ↑(rν+1) + ϕ ↓(sν+1) +

c
2

(cid:107) z ν+1 − z ν (cid:107)2 ≤ ϕ ↑((cid:98)rν) + ϕ ↓((cid:98)sν) +

c
2

(cid:107) (cid:98)z ν − z ν (cid:107)2.

Passing to the limit ν(∈ κ) → ∞, we obtain z ∞ ∈

argmin
z∈Z(i1,i2)(θ ∞)

(cid:98)Ψc(z, z ∞). This

completes the proof of statement (c).
(d) Since Ψ(θ ν) ≤ ϕ ↑(rν) + ϕ ↓(sν) ≤ ϕ ↑(r0) + ϕ ↓(s0) = Ψ(θ 0) for all ν, it follows
that the sequence {θ ν} is bounded by assumption. Since rν ≥ sν, it follows that

ϕ ↑(rν) + ϕ ↓(sν) ≥ max ( ϕ(sν), ϕ(rν) ) ,

the boundedness of {(rν, sν)} follows similarly.

One may further establish the convergence of the full sequence {z ν} under an
isolatedness assumption of an accumulation point as in the general theory of sequential
convergence [21, Proposition 8.3.10] or by invoking the Kurdyka-(cid:32)Lojaziewicz (KL)
theory of semi-analytic functions as in [2, 3, 9, 10], Recall that a proper lower semi-
continuous function f : Rn → (−∞, +∞] is said to have the KL property at ¯x ∈
dom (∂ f ) (cid:44) { x ∈ dom f | ∂ f (x) (cid:54)= ∅ } if there exist α > 0, a neighborhood N of ¯x
and a continuous concave function φ : [ 0 , α ) → R+ such that
• φ(0) = 0 and φ is continuously diﬀerentiable on (0, α) with φ (cid:48) > 0; and
• φ(cid:48)(f (x) − f (¯x)) dist (0 , ∂f (x)) ≥ 1 for any x ∈ N with f (¯x) < f (x) < f (¯x) + α,
(cid:107)x − y(cid:107) is the distance from a point x ∈ Rn to a
where dist (x, C) (cid:44) minimize

y∈C

nonempty closed set C ⊆ Rn. If the function φ in the above deﬁnition can be chosen
as φ(s) = γ s1−β for some scalars γ > 0 and β ∈ [0, 1), we say that f has the KL
property at ¯x with an exponent β. It is known that a proper closed semi-algebraic
function is a KL function [8].

To proceed, we introduce the following function of z (cid:44) (θ, r, s):

(cid:40)

Ψ(z) (cid:44)

ϕ ↑(r) + ϕ ↓(s),
+∞

if z ∈ Z (cid:44) {(θ, r, s) ∈ Θ × R × R | s ≤ ψ(θ) ≤ r},
otherwise

and also deﬁne the set Z ∞ (cid:44) Θ ∞ × R ∞ × S ∞ of all accumulation points of the
sequence {z ν} produced by the nonmonotone MM algorithm. This set is nonempty
under condition (d) of Proposition 7; moreover, every one of its elements is a d-
stationary solution of problem (8).

Proposition 8. Suppose that Ψ is bounded below on the closed convex set Θ.
Then the whole sequence {z ν} converges to an unique element of the set Z ∞ under
either one of the following two conditions (a) or (b);
(a) Z ∞ contains an isolated element;
(b) {z ν} is bounded; the index sets A1;2ε(θ) and A2;2ε(θ) are singletons for all θ ∈
Θ ∞; ∇ψ1,i1 and ∇ψ2,i2 are locally Lipschitz continuous near all θ ∈ Θ ∞ for all
(i1, i2) ∈ A1;2ε(θ) × A2;2ε(θ), respectively; the functions ϕ ↑ and ϕ ↓ are diﬀerentiable
with Lipschitz continuous gradients near all r ∈ R ∞ and s ∈ S ∞, respectively; more-
over, the function Ψ satisﬁes the KL property at all z ∞ ∈ Z ∞.
If {zk} converges to z ∞ ∈ Z ∞ under condition (b) and the function Ψ has the KL
exponent β ∈ [0, 1) at z ∞, it holds that
• if β = 0, the sequence {z ν} converges in a ﬁnite number of steps;
• if β ∈ (0 , 1
r ∈ [0, 1) such that (cid:107)z ν − z ∞(cid:107) ≤ η r ν for all ν suﬃciently large;
• if β ∈ ( 1
η > 0 such that (cid:107)z ν − z ∞(cid:107) ≤ η ν− 1−β

2 , 1), the sequence {z ν} converges R-sublinearly, speciﬁcally, there exists

2 ], the sequence {z ν} converges R-linearly, i.e., there exist η > 0 and

2β−1 for all ν suﬃciently large.

Proof. It follows from Proposition 7(b) that lim
ν→∞

(cid:107) z ν+1 − z ν (cid:107) = 0. Under as-
sumption (a) here, the convergence of the whole sequence {z ν} to the isolated element
of Z ∞ follows from [21, Proposition 8.3.10]. To prove the sequential convergence of
{z ν} under the conditions in assumption (b), it suﬃces to show the following three
properties of {z ν} and then apply the convergence results in [3, Theorem 2.9] to the
function Ψ:
(i) Ψ(z ν+1) ≤ Ψ(z ν) −

(cid:107) z ν+1 − z ν (cid:107)2 for all ν ≥ 1 ;

c
2

(ii) there exists a subsequence {z ν}ν∈κ of {z ν} such that

lim
ν(∈κ)→∞

z ν = z ∞ and

Ψ(z ν) = Ψ(z ∞) ;

lim
ν(∈κ)→∞
(iii) there exists a scalar η > 0 such that for all ν suﬃciently large, a ν+1 ∈ ∂ Ψ(z ν+1)
exists satisfying (cid:107) a ν+1 (cid:107) ≤ η (cid:107) z ν+1 − z ν (cid:107).

Properties (i) and (ii) readily follow from Proposition 7. To show (iii), we ﬁrst
note that Z ∞ is a nonempty, compact, and connected set [21, Proposition 8.3.9].
By assumption (b), we further derive the existence of ¯ij ∈ {1, . . . , kj} such that
Aj;2ε(θ) = {¯ij} for all θ ∈ Θ ∞, j = 1, 2. Since the scalar sequence {dist (θ ν, Θ ∞)} is
dist (θ ν, Θ ∞) = 0. Hence,
bounded with a unique accumulation point 0, we have lim
ν→∞

for some constant κ > 0 it holds that for all ν suﬃciently large, some θ ν;∞ ∈ Θ ∞ exists
for which (cid:107) θ ν − θ ν;∞ (cid:107) ≤
for any i = 1, . . . , k1. Then for any i ∈ A1;ε(θ ν),

and | ψ1,i(θ ν) − ψ1,i(θ ν;∞) | ≤ κ (cid:107) θ ν − θ ν;∞ (cid:107) ≤

ε
2κ

1
2

ε

ψ1,i(θ ν;∞) ≥ ψ1,i(θ ν) −

≥ ψ1,¯i1(θ ν) −

3
2
= max
1≤i≤k1

1
ε ≥ max
2
1≤i≤k1
ε ≥ ψ1,¯i1(θ ν;∞) − 2 ε
ψ1,i(θ ν;∞) − 2 ε,

ψ1,i(θ ν) −

3
2

ε

since i ∈ A1;ε(θ ν)

since A1;2ε(θ ν;∞) = {¯i1} = A1(θ ν;∞),

i.e., i ∈ A1;2ε(θ ν;∞) = {¯i1}. Therefore, for all ν suﬃciently large, A1;ε(θ ν) = {¯i1};
and similarly, A2;ε(θ ν) = {¯i2}. By assumption (b) and [52, Exercise 10.10], we further
derive,

(13)






∇ψ(θ ν+1) = ∇ψ1,¯i1 (θ ν+1) − ∇ψ2,¯i2 (θ ν+1)

∂ Ψ(z ν+1) =





0
∇ϕ ↑(rν+1)
∇ϕ ↓(sν+1)


 + N (z ν+1; Z)

for all ν suﬃciently large.

Note that Z may not be a convex set; we write N (z; Z) as the normal cone of Z at
z ∈ Z based on the deﬁnition in [52, Deﬁnition 6.3]. One can easily check that





α1 ∈ N (ψ(θ ν+1) − rν+1; R−), α2 ∈ N (−ψ(θ ν+1) + sν+1; R−),





0 ∈

(α1 − α2)∇ψ(θ ν+1) + N (θ ν+1; Θ)
−α1
α2









=⇒ α1 = α2 = 0,

where we denote R− (cid:44) {t ∈ R | t ≤ 0}. It then follows from [52, Theorem 6.14] that
(14)

N (z ν+1; Z) =










(α1 − α2)∇ψ(θ ν+1) + N (θ ν+1; Θ)
−α1
α2

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

α1 ≥ 0 , α2 ≥ 0 ,

α1 = 0 if ψ(θ ν+1) < rν+1

α2 = 0 if ψ(θ ν+1) > sν+1






.

For all ν suﬃciently large, since A1;ε(θ ν) × A2;ε(θ ν) = {(¯i1,¯i2)}, we deduce z ν+1 =
z ν+ 1
(cid:98)Ψc(z, z ν). To proceed, we denote

2 ;¯i1,¯i2 = argmin

z∈Z(¯i1,¯i2)(θ ν )

I1(z ν+1) (cid:44) (cid:8)i : ψ1,i(θ ν+1) − ψ2,¯i2 (θ ν) − ∇ψ2,¯i2 (θ ν)T (θ ν+1 − θ ν) = rν+1
I2(z ν+1) (cid:44) (cid:8)i : ψ1,¯i1 (θ ν) + ∇ψ1,¯i1(θ ν)T (θ ν+1 − θ ν) − ψ2,i(θ ν+1) = sν+1

(cid:9) ,
(cid:9) .

It then follows from z ν+1 ∈ Z(¯i1,¯i2)(θ ν) that one of the following four cases must
hold for any ν suﬃciently large: (1) I1(z ν+1) = I2(z ν+1) = ∅; (2) I1(z ν+1) = {¯i1}
and I2(z ν+1) = ∅; (3) I1(z ν+1) = ∅ and I2(z ν+1) = {¯i2}; (4) I1(z ν+1) = {¯i1} and
I2(z ν+1) = {¯i2}. For those z ν+1 satisfying case (1), we derive

ψ(θ ν+1) = ψ1,¯i1(θ ν+1) − ψ2,¯i2(θ ν+1)

≤ ψ1,¯i1(θ ν+1) − ψ2,¯i2(θ ν) − ∇ψ2,¯i2(θ ν)T (θ ν+1 − θ ν) < rν+1.

Similarly one has ψ(θ ν+1) > sν+1. Then N (z ν+1; Z) =
the inequality constraints in z ∈ Z(¯i1,¯i2)(θ ν) are inactive at z = z ν+1, by applying
the optimality condition of the problem:

(cid:98)Ψc(z, z ν), we deduce

. Since all

(cid:18) N (θ ν+1; Θ)
0R2

(cid:19)

min
z∈Z(¯i1,¯i2)(θ ν )

0 ∈ N (θ ν+1; Θ) + c (θ ν+1 − θ ν),

and

∇ϕ↑(rν+1) + c (rν+1 − rν) = 0, ∇ϕ↓(sν+1) + c (sν+1 − sν) = 0,

which, together with (13), yields − c (z ν+1 − z ν) ∈ ∂ Ψ(z ν+1). Hence, property (iii)
holds with the constant η1 = c. For those z ν+1 satisfying case (2), there exists a
multiplier λν+1 ∈ R corresponding to the active constraint ψ1,¯i1 (θ ν+1) − ψ2,¯i2 (θ ν) −
∇ψ2,¯i2(θ ν)T (cid:0) θ ν+1 − θ ν (cid:1) = rν+1 such that






0 ∈

λν+1

(cid:2) ∇ψ1,¯i1(θ ν+1) − ∇ψ2,¯i2(θ ν) (cid:3) + N (θ ν+1; Θ)

∇ϕ↑(rν+1) − λν+1
∇ϕ↓(sν+1)


 + c (z ν+1 − z ν).


Since ϕ↑ is non-decreasing, we have ∇ϕ↑(rν+1) = λν+1 − c (rν+1 − rν) ≥ 0, which,
together with (13) and (14), yields





−b ν+1
c (rν+1 − rν)
0


 − c (z ν+1 − z ν) ∈ ∂ Ψ(z ν+1), where

b ν+1 (cid:44) c (rν+1−rν ) (cid:2)∇ψ1,¯i1 (θ ν+1) − ∇ψ2,¯i2 (θ ν )(cid:3)+∇ϕ↑(rν+1) (cid:2)∇ψ2,¯i2 (θ ν+1) − ∇ψ2,¯i2 (θ ν )(cid:3) .
In addition, {∇ψ1,¯i1 (θ ν+1) − ∇ψ2,¯i2(θ ν)} and {∇ϕ ↑(rν+1)} are bounded for all ν
suﬃciently large under the assumptions in (b). Hence, property (iii) holds for some
constant η2 > 0. Similarly, those points z ν+1 satisfying cases (3) and (4) can be
proved to have property (iii) for some positive constants η3 and η4, respectively. Con-
sequently, property (iii) holds for all ν suﬃciently large with η = max{η1, . . . , η4}.
The desired global convergence of {z ν} is thus established based on the above argu-
ments. Finally, with the proven properties (i)-(iii), the stated convergence rate can
be established by similar arguments to [2, Theorem 2]; see also [9, Proposition 4].

The sequential convergence of the standard MM algorithm (without regularizing
the slack variables) has been studied in [10] under a condition that is in the same spirit
as property (iii) in the above proof. In order to verify this property, we essentially
assume that ϕ ↑, ϕ ↓, ψ1, and ψ2 are diﬀerentiable with locally Lipschitz continuous
gradients near Z ∞. This type of assumptions have also been made in [32, 57, 36] to
prove the sequential convergence of the dc algorithm (with extrapolation) for solving
f (x) − g(x), where the function g is assumed to be dif-
the dc program: minimize
ferentiable with a locally Lipschitz gradient near all accumulation points. Unlike the
dc algorithm that only employs a majorization of the function g, we majorize both
ψ1 and ψ2 and do not rely on a dc decomposition of the overall objective function
Ψ. Admittedly, this diﬀerentiability assumption is rather restrictive for a nondiﬀer-
entiable problem. The diﬃculty in trying to relax this assumption is the evaluation
of the subdiﬀerential ∂Ψ(z ν+1) of the extended-value function Ψ that involves the
It is our interest to
nonconvex set Z deﬁned by the nondiﬀerentiable function ψ.
pursue a thorough investigation of this issue that links the theory of error bounds to

x

the sequential convergence of iterative methods for specially structured nondiﬀeren-
tiable optimization problems, without relaying the hard-to-evaluate subdiﬀerential as
needed here. Regrettably, such a study is beyond the scope of this paper.

4.2. Two variants of the MM algorithm. Though the MM algorithm dis-
cussed above converges to a d-stationary point, it potentially requires high computa-
tional cost per iteration because multiple subproblems need to be solved. To reduce
the computational cost, we consider two variants of the algorithm. The ﬁrst one
requires solving only one subproblem associated with an arbitrary pair of indices
(i1, i2) ∈ A1(θ ν) × A2(θ ν) in each iteration.

MM-1: a simpliﬁed version of the MM algorithm for solving (8).

Initialization. Given are a scalar c > 0, an initial point θ 0 ∈ Θ and an arbitrary
2) ∈ A1(¯θ) × A2(¯θ). Let r0 (cid:44) ψ(θ 0) (cid:44) s0; thus z 0 (cid:44) (θ 0, r0, s0) ∈
pair in indices (i0
1, i0
2)(¯θ). Set ν = 0.
Z(i0
Step 1. Let z ν+1 (cid:44)

(cid:98)Ψc(z, z ν) for some (iν

2) ∈ A1(θ ν) × A2(θ ν).

1, iν

1,i0

argmin
1 ,iν

2 )(θ ν )

z∈Z(iν

Step 2. If z ν+1 satisﬁes a prescribed stopping rule, terminate; otherwise, return to
(cid:3)
Step 1 with ν replaced by ν + 1.

Unfortunately, the above MM algorithm is not guaranteed to converge to a d-
stationary solution of (8). The reason is easy to understand; namely, there is a “for
all index pairs” condition in the requirement of d-stationarity, whereas each iteration
of the MM-1 algorithm solves only one subprogram corresponding to a single index
pair. Relaxing the “for all” requirement in d-stationarity, we say that a point ¯θ ∈ Θ
is a weak M-stationary point of (8) if there exists (¯i1,¯i2) ∈ A1(¯θ) × A2(¯θ) such
MΨ(¯i1,¯i2)(θ, ¯θ). The preﬁx M is used to highlight that the concept
that ¯θ ∈ argmin
employs the majorization of the function Ψ at ¯θ by the family of convex functions
(cid:8)MΨ(¯i1,¯i2)(•, ¯θ)(cid:9). In the case where ϕ is the identity function, a weak M-stationary
point of the function Ψ on Θ coincides with a “weak d-stationary point” as deﬁned
in [47, Subsection 3.3], thus a weak M-stationary point must be a critical point (in
the sense of dc programming) of a un-composed diﬀerence-max program.

θ∈Θ

The convergence of the MM-1 algorithm is stated in the proposition below, whose
proof can be derived similarly to the proof of Proposition 7. We omit the details here.
Proposition 9. Suppose that Ψ is bounded below on the closed convex set Θ. For
any accumulation point (θ ∞, r∞, s∞) of the sequence {z ν = (θ ν, rν, sν)} generated by
(cid:3)
the MM-1 algorithm, if it exists, θ ∞ is a weak M-stationary point of (8).
It is worth mentioning that unlike a d-stationary point of (8) which is independent
of the representation of Ψ by its components ϕ and ψ, the concept of a weak M-
stationary point not only depends on the decomposition of ϕ = ϕ ↑ + ϕ ↓, but also
depends on the “max-max” representation of ψ. To see the dependence on the former,
consider univariate functions:

ϕ(t) = 2t
(cid:124)(cid:123)(cid:122)(cid:125)
ϕ ↑
1 (t)

+ −t
(cid:124)(cid:123)(cid:122)(cid:125)
ϕ ↓
1 (t)

= t

(cid:124)(cid:123)(cid:122)(cid:125)
ϕ ↑
2 (t)

,

+ 0

(cid:124)(cid:123)(cid:122)(cid:125)
ϕ ↓
2 (t)

ψ(t) = max (2t, 1.5t)
(cid:125)

(cid:124)

(cid:123)(cid:122)
g(t)

− max (t , 0.5t)
(cid:123)(cid:122)
(cid:125)
h(t)

(cid:124)

.

One can easily check that t = 0 is a M-stationary point of ϕ ◦ ψ under the decom-
position ϕ = ϕ ↑
1 (with (i1, i2) = (1, 1) ∈ A1(0) × A2(0)), but fails to be that

1 + ϕ ↓

2 + ϕ ↓

under the decomposition ϕ = ϕ ↑
2 . To see the dependence on the latter, we may
consider the example ϕ(t) = t and ψ(t) = max (2t, 0) − max (t, −t) = t − 0 for a
scalar t, and verify t = 0 is a weak M-stationary point of ϕ ◦ ψ under the former
representation, but fails to be weakly M-stationary under the latter representation.
In addition, a weak M-stationary point of (8) may not be its C-stationary point as
indicated by ϕ(t) = t and ψ(t) = max (2t, 0) − max (t, −t) at t = 0, and vice versa as
shown by ϕ(t) = t and ψ(t) = t − max (0, 2t) at t = 0.

The second variant of the MM algorithm is a probabilistic version that requires
solving only one single convex subprogram corresponding to a randomly selected pair
(i1, i2) ∈ A1;ε(θ ν) × A2;ε(θ ν), each with positive probability of being selected.

MM-2: A randomized version of the MM algorithm for solving (8).

Initialization. Given are a scalar c > 0, an initial point θ 0 ∈ Θ and an arbitrary
2) ∈ A1(¯θ) × A2(¯θ). Let r0 (cid:44) ψ(θ 0) (cid:44) s0; thus z 0 (cid:44) (θ 0, r0, s0) ∈
pair in indices (i0
Z(i0
Step 1. Choose (iν
previous iterations so that

2) ∈ A1;ε(θ ν) × A2;ε(θ ν) randomly and independently from the

1, i0
2)(¯θ). Set ν = 0.
1, iν

1,i0

pν,i1,i2 (cid:44) Prob {(i1, i2) is chosen | given z ν} ≥ pmin > 0.

Let z ν+ 1

2 ;iν

1 ,iν

2 (cid:44)

(cid:98)Ψc(z, z ν).

z∈Z(iν

argmin
2 )(θ ν )
1 ,iν
(cid:40) z ν+ 1

Step 2. Set z ν+1 (cid:44)

2 ;iν

1 ,iν

2

if (cid:98)Ψc(z ν+ 1

2 ; iν

1 , iν

2 , z ν) < ϕ ↑(rν) + ϕ ↓(sν),

otherwise.
Step 3. If z ν+1 satisﬁes a prescribed stopping rule, terminate; otherwise, return to
(cid:3)
Step 1 with µ replaced by µ + 1.

z ν

The following proposition extends the convergence result of [47, Proposition 7] for
dc programming. We omit the proof again since it can be derived with little diﬃculty
by combining the arguments of Proposition 7 and [47, Proposition 7].

Proposition 10. Suppose that Ψ is bounded below on the closed convex set Θ.
Let {z ν = (θ ν, rν, sν)} be a sequence generated by the MM-2 algorithm. For any
accumulation point (θ ∞, r∞, s∞) of this sequence, if it exists, θ ∞ is a d-stationary
(cid:3)
point of (8) with probability one.

5. A Semismooth Newton Method for the Subproblems. The convex
subproblems in Step 1 (see (10)) are the workhorse of the MM algorithms. In the
present section, we discuss a semismooth Newton (SN) method for solving them.
There are two important motivations for us to adopt this Newton-type method instead
of various ﬁrst-order methods that are quite popular in recent years. One, although
the computational cost for each step of the SN method may be more than that of ﬁrst-
order methods, this cost is compensated by much fewer iterations, especially if the
SN method is warm started by the solution of the preceding subproblem. The overall
computational time of the SN method is thus potentially less than that of ﬁrst-order
methods. The recent papers [49, 65, 41, 59, 35] provide computational evidence to
support the practical advantages of the SN method. Two, the MM algorithm itself
is a ﬁrst-order method, whose objective value can easily be stagnant after several
iterations; see e.g., [31, Chapter 1]. First-order methods may perform fairly well if

it is directly employed to solve stand-alone problems to low or moderate accuracy.
But unlike the SN method, which is able to compute very accurate solutions of the
subproblems reasonably fast, ﬁrst-order methods may not provide suﬃciently accurate
solutions when embedded in an iterative process such as the MM algorithm as the
methods of choice for solving the subproblems.

Recall that a locally Lipschitz continuous function Φ : Ω → Rm deﬁned on the
open set Ω is said to be semismooth at ¯x ∈ Ω if Φ is directionally diﬀerentiable at ¯x and
for any V ∈ ∂CΦ(x) with x suﬃciently near ¯x, Φ(x) − Φ(¯x) − V (x − ¯x) = o((cid:107) x − ¯x (cid:107));
Φ is said to be strongly semismooth at ¯x if o((cid:107) x − ¯x (cid:107)) is strengthened to O((cid:107) x − ¯x (cid:107)2)
[40, 50]. The function Φ is said to be a (strongly) semismooth function on Ω if it is
(strongly) semismooth everywhere in Ω. The class of semismooth functions is broad,
including piecewise semismooth functions, and thus piecewise smooth functions [21,
Proposition 7.4.6]. It is also known that every piecewise aﬃne map from Rn into Rm
is strongly semismooth [21, Proposition 7.4.7]. A real-valued function φ : Ω → R is
said to semismoothly diﬀerentiable (SC1) at ¯x ∈ Ω [21, Subsections 7.4.1 and 8.3.3] if
it is diﬀerentiable near ¯x and its gradient is a semismooth function at ¯x.

With pioneering work by Kojima and Shindo [29], Kummer [30], Pang [45], Qi
and Sun [50], among others, the SN method is a generalization of Newton’s method
for solving the semismooth equation Φ(x) = 0, which has also been extended to a
convex constrained SC1 minimization problem in [46]; this is the method that we
propose to apply to solve the dual problem of (10).

Letting 1 denote the vector of all ones of appropriate dimensions, the subproblem
1, iν

(10) can be written as: for the given pair (iν

2):

(15)

minimize
θ∈Θ, r,s∈R
subject to b1;iν

ϕ ↑(r) + ϕ ↓(s) +

(cid:2) (cid:107) θ − θ ν (cid:107)2 + ( r − rν )2 + ( s − sν )2 (cid:3)

c
2

2 (θ; θ ν) − 1 r ≤ 0, and b2;iν

1 (θ; θ ν) + 1 s ≤ 0,

with b1;iν
tiable components given by:

2 (•; θ ν) and b2;iν

1 (•; θ ν) denoting two vector functions with convex diﬀeren-

1;iν
2
i
2;iν
1
i

b

b

(θ; θ ν) (cid:44) ψ1,i(θ) − (cid:2) h(θ ν) + ∇ψ2,iν
(θ; θ ν) (cid:44) g(θ ν) + ∇ψ1,iν

2 (θ ν)T ( θ − θ ν ) (cid:3) , ∀ i = 1, · · · , k1,
∀ i = 1, · · · , k2.

1 (θ ν)T ( θ − θ ν ) − ψ2,i(θ),

The Lagrangian dual program is

maximize
λ≥0, µ≥0

ζ(iν

2 )(λ, µ; z ν), where

1 ,iν

ζ(iν

1 ,iν

2 )(λ, µ; z ν ) (cid:44) minimize
θ∈Θ, r,s∈R






ϕ ↑(r) + ϕ ↓(s) +

c
2

(cid:107) z − z ν (cid:107)2

+λT ( b1;iν

2 (θ; θ ν ) − 1 r ) + µT ( b2;iν

1 (θ; θ ν ) + 1 s )

and






(cid:110)

= minimize

θ∈Θ

(cid:124)

+ minimize
r∈R

(cid:124)

λT b1;iν

2 (θ; θ ν ) + µT b2;iν

1 (θ; θ ν ) +

(cid:123)(cid:122)
sub-prob ν
θ
(r − rν )2(cid:111)
c
2
(cid:125)

(cid:110)

ϕ ↑(r) − (1T λ)r +

(cid:123)(cid:122)
sub-probν
r

c
2

(cid:107) θ − θ ν (cid:107)2 (cid:111)
(cid:125)

+ minimize
s∈R

(cid:124)

(cid:110)

ϕ ↓(s) + (1T µ)s +

(cid:123)(cid:122)
sub-probν
s

c
2

(s − sν )2(cid:111)
(cid:125)

.

By Danskin’s Theorem [21, Theorem 10.2.1], the concave dual objective function
ζ(iν

2 )(•, •; z ν) is diﬀerentiable with the gradient given by

1 ,iν

∇λζ(iν

2 )(λ, µ; z ν) = b1;iν

1 ,iν

2 (¯θ ν; θ ν) − 1 ¯rν, ∇µζ(iν

2 )(λ, µ; z ν) = b2;iν

1 ,iν

1 (¯θ ν; θ ν) + 1 ¯sν,

where ¯θ ν, ¯rν and ¯sν are, respectively, the unique minimizers of sub-prob ν
θ , sub-probν
r ,
and sub-probν
s corresponding to the pair (λ, µ). Here, we see the beneﬁt of regularizing
the scalar variables r and s that ensures the uniqueness of the minimizers ¯rν and ¯sν.
We can use the concept of proximal mappings to characterize these three minimizers.
For a proper closed convex function φ deﬁned on Rn and a scalar c > 0, the proximal
mapping P c

φ is deﬁned by

φ (u) (cid:44) argmin
P c
v∈Rn

(cid:110)

φ(v) +

c
2

(cid:107) v − u (cid:107)2(cid:111)

,

u ∈ Rn,

which is globally Lipschitz continuous [5, Proposition 12.27]. One property of this
mapping is that a function φ is convex piecewise linear-quadratic if and only if P c
φ
is piecewise linear [52, Proposition 12.30]. (A function φ is called piecewise linear-
quadratic if dom φ can be represented as the union of ﬁnitely many polyhedral sets,
relative to each of which φ(x) is given by a quadratic function; see, e.g., [52, Def-
inition 10.20].) Given that b1;iν
2 (•; θ ν) and b2;iν
1 (•; θ ν) are smooth mappings, the
2 ) is a SC1 function for many interesting instances, in particular if Θ
function ζ(iν
is a polyhedral set and the functions λT b1;iν
1 (•; θ ν), ϕ ↑ and ϕ ↓ are
all convex piecewise linear-quadratic, or the proximal mappings of these functions are
piecewise smooth.

2 (•; θ ν) + µT b2;iν

1 ,iν

The following is the globally convergent and locally superlinearly convergent SN
method to minimize a convex SC1 function φ on a closed convex set X; see [46] for
details. In the case of the dual program in question, the set X is the nonnegative
orthant in the (λ, µ)-space. Specializations of the method are possible and will be
illustrated with a least-squares piecewise aﬃne regression problem.

A Semismooth Newton method for minimize

x∈X

φ(x) with a SC 1 function φ.

Initialization. Given an initial point x0 ∈ X and two scalars ρ, σ ∈ (0, 1). Set
k = 0.

Step 1. (Find the search direction.) Given xk ∈ X, pick V k ∈ ∂C∇φ(xk) and a
scalar εk > 0. Solve the following strictly convex quadratic program in the variable d
to obtain dk:

minimize
d∈X−xk

∇φ(xk)T d +

1
2

d T (V k + εkI) d

Step 2. (Line search by the Armijo rule.) Set xk+1 (cid:44) xk + ρmk dk, where mk is the
smallest nonnegative integer m for which

φ(xk + ρmdk) ≤ φ(xk) + σ ρm ∇φ(xk)T dk.

Step 3. If xk+1 satisﬁes a prescribed stopping rule, terminate; otherwise, return to
(cid:3)
Step 1 with k replaced by k + 1.

The major computational cost of the above semismooth Newton method is to ﬁnd
the search direction in Step 1. To accomplish this task, one may apply any quadratic
programming solver. For the special case when ψ is a diﬀerence of convex piecewise
aﬃne function as in (3), we may instead ﬁnd the search direction by solving a system
of linear equations, leading to a very eﬀective, provably convergent, overall algorithm
for solving problem (8) that includes the problem of piecewise aﬃne regression.

5.1. A special case where ψ is diﬀerence-convex-piecewise-aﬃne. Let
ψ be a diﬀerence of convex piecewise aﬃne functions as in (3). Then the functions

2 (•; θ ν) and b2;iν
1 and iν

b1;iν
1 (•; θ ν) in (15) are aﬃne, for which we may write (dropping the
2 and the dependence on θ ν) as b1(θ) (cid:44) B1θ−β1 and b2(θ) (cid:44) B2θ−β2
indices iν
for some matrices B1 ∈ Rk1×m and B2 ∈ Rk2×m and vectors β1 ∈ Rk1 and β2 ∈ Rk2 .
Instead of (10), we may consider the following alternative subproblems in the MM
algorithm with two additional auxiliary variables (cid:98)r ∈ Rk1 and (cid:98)s ∈ Rk2:

minimize
θ∈Θ, r, s, (cid:98)r, (cid:98)s

ϕ ↑(r) + ϕ ↓(s)

+

c
2

(cid:2) (cid:107) θ − θ ν (cid:107)2 + ( r − rν )2 + ( s − sν )2 + (cid:107) (cid:98)r − (cid:98)r ν (cid:107)2 + (cid:107) (cid:98)s − (cid:98)s ν (cid:107)2 (cid:3)

subject to B1 θ − 1r + (cid:98)r = β1, B2 θ + 1s + (cid:98)s = β2,

(cid:98)r ≥ 0,

(cid:98)s ≥ 0.

The (subsequential) convergence of the MM algorithms discussed in Section 4 can
be easily extended to the above formulation with the additional regularization term
c
2

(cid:2) (cid:107) (cid:98)r − (cid:98)r ν (cid:107)2 + (cid:107) (cid:98)s − (cid:98)s ν (cid:107)2 (cid:3). The Lagrangian dual program is

maximize
λ∈Rk1 , µ∈Rk2

ξ(λ, µ; (cid:98)z ν), where

(cid:98)z ν (cid:44) ( θ ν, rν, sν, (cid:98)r ν, (cid:98)s ν )

and

ξ(λ, µ; (cid:98)z ν ) (cid:44) minimize
θ∈Θ, r,s∈R,
(cid:98)r ≥ 0, (cid:98)s ≥ 0





= −λT β1 − µT β2 + minimize

θ∈Θ

(cid:2) (cid:107) θ − θ ν (cid:107)2 + ( r − rν )2

c
ϕ ↑(r) + ϕ ↓(s) +
2
+( s − sν )2 + (cid:107) (cid:98)r − (cid:98)r ν (cid:107)2 + (cid:107) (cid:98)s − (cid:98)s ν (cid:107)2 (cid:3)
+λT ( B1 θ − 1r + (cid:98)r − β1 ) + µT ( B2 θ + 1s + (cid:98)s − β2 )
c
2

λT B1 θ + µT B2 θ +

(cid:107) θ − θ ν (cid:107)2 (cid:111)

(cid:110)





s∈R
(cid:110)

µT

ϕ ↓(s) + (1T µ) s +
(cid:107) (cid:98)s − (cid:98)s ν (cid:107)2(cid:111)

.

c
2

(cid:98)s +

+ minimize

r∈R

(cid:110)

ϕ ↑(r) − (1T λ) r +

(r − rν )2(cid:111)

c
2

+ minimize

(cid:110)

(s − sν )2(cid:111)

c
2

+ minimize

(cid:110)

λT

(cid:98)r +

(cid:107) (cid:98)r − (cid:98)r ν (cid:107)2(cid:111)

c
2

+ minimize

(cid:98)r ≥ 0

(cid:98)s ≥ 0
As in the previous discussions, the concave function ξ(•, •; (cid:98)z ν) is continuously diﬀer-
entiable. The proximal mappings associated with (cid:98)r and (cid:98)s are essentially the projec-
tions onto the nonnegative orthant, which must be semismooth [21, Theorem 4.5.2 &
Proposition 7.4.6]. If Θ is a polyhedral set and the proximal mappings of ϕ ↑ and ϕ ↓
are also semismooth, the dual program is an unconstrained SC1 problem. Step 1 in
the SN method is thus to solve an unconstrained strictly convex quadratic program,
or equivalently, to solve a linear equation. Furthermore, the minimization in θ within
the dual function ξ(λ, µ; (cid:98)z ν) is the projection onto the set Θ, which is easy if Θ is
a simple set; e.g., containing upper and lower bounds only. In this case, the bulk of
the computational eﬀort per iteration of the combined MM and SN methods essen-
tially consists of solving systems of linear equations in the (λ, µ)-space within the fast
convergent SN method.

6. Numerical Experiments. In order to demonstrate the eﬀectiveness of the
proposed nonsmooth MM+SN method for solving some non-standard statistical es-
timation problems, we conduct various numerical experiments on an unconstrained
least-squares continuous piecewise aﬃne regression problem by taking the error func-
tion ϕ(y, t) = 1
2 (t − y)2 and the piecewise aﬃne model ψ as in (3). Besides providing
evidence of the promise of the MM+SN method itself (see Table 1 and Figures 12 and
13 for summaries of its performance statistics), our experiments also aim to establish
the superiority of the piecewise aﬃne statistical model over the standard linear model.

Speciﬁcally, the optimization problem we consider in this section is the following:

minimize
θ





fN (θ) (cid:44) 1
N

N
(cid:88)

(cid:20)

s=1

ys − max
1≤i≤k1

(cid:110)

( ai )T xs + αi

(cid:111)

− max
1≤i≤k2

(cid:110)

( bi )T xs + βi

(cid:111) (cid:21)2

+ γN

m
(cid:88)

i=1

[ | θi | − pi(θi) ]





(cid:111)

(cid:110)(cid:0)ai, αi

(cid:1)k2
i=1

i=1 , (cid:0)bi, βi
(cid:1)k1

∈ R(k1+k2)(d+1) and each pi is a diﬀerentiable
where θ =
convex function. For this optimization problem which must have a global minimizer,
Proposition 2 yields some interesting properties of its d-stationary points, provided
that each univariate pi(θi) is piecewise linear-quadratic. All our computations are
done in Matlab on Mac OS X with 1.7 GHz Intel Core i7 and 8 GB RAM. We employ
the randomized version of the MM algorithm to solve the problem with ε = 10−4
for the “ε-argmax” expansion. The iterations of the MM algorithm are terminated
≤ 10−4; the iterations of the inner SN algorithm are termi-
if
nated if (cid:107)∇ξ(λk, µk; (cid:98)z ν)(cid:107) ≤ max (cid:0) 10−6, 10−2 | fN (θ ν+1) − fN (θ ν) | (cid:1), where ξ is the
objective of the ν-th dual subproblem.

| fN (θ ν+1) − fN (θ ν) |
max (1, | fN (θ ν) | )

6.1. Synthetic data. In this set of runs we consider two simulation examples.

In both examples the penalty parameter γN is taken to be 0.
Example 1. We consider a 2-dimensional convex piecewise linear model

y = max {x1 + x2 , x1 − x2 , −2x1 + x2 , −2x1 − x2 } + ε,

whose 3-D plot is given in Figure 3. Although this is a convex model, the resulting
least-squares problem is still nonconvex. Random samples of size N = 50, 100, 200, 500
are generated uniformly from [−1, 1] × [−1, 1], respectively, and ε is drawn from a
uniform distribution in [ −0.5, 0.5 ]. The objective values of the computed solutions
for k1 = 4 and k2 = 0 over 500 runs (with each run corresponding to one initial point)
by the MM+SN algorithm are shown in Figure 5. Two observations are made from
this ﬁgure. One, the d-stationary values (which are also locally minimum values)
In a recent paper [15], we provide
are clustered into a few distinguished groups.
theoretical justiﬁcation of this observation by showing that there are only ﬁnitely
many d-stationary values for several classes of piecewise programs, of which the least-
squares piece aﬃne regression problem is a special instance. Two, with the increase
of the sample size, more and more initial points lead to the solutions that appear
to yield the globally minimum value. The total numbers of initial points that lead
to the smallest objective values with respect to diﬀerent sample sizes are listed in
Figure 4. In Figure 6, we plot the solutions given by the smallest objective values
of diﬀerent sample sizes in Figure 5. These solutions almost recover the original
convex regression model, thus (empirically) validating the MM+SN methodology for
a nonconvex minimization problem in the recovery of a convex statistical model.
Example 2. Next we consider a 2-dimensional nonconvex piecewise aﬃne model

y = max {x1 − 2x2 , −2x1 + x2 + 1} − max {3x1 − 2x2 , 2x1 + 5x2} + ε,

whose 3D-plot is given in Figure 7. The projection of this ﬁgure onto y = 0 is demon-
strated in Figure 8. As in Example 1, random samples of size N = 50, 100, 200, 500
are generated uniformly from [−1, 1] × [−1, 1], and ε is drawn from a uniform distri-
bution in [−0.5, 0.5]. The objective values of the computed solutions over 500 runs

Fig. 3: The 3-D plot of Example 1.

Fig. 4: The number of initial points that lead to the
smallest objective values.

Fig. 5: The objective values computed by the MM+SN algorithm of Example 1.

are shown in Figure 10. Similar observations to Example 1 can be made with regards
to the concentration of the (computed) stationary values. We also plot the solutions
corresponding to smallest objective values for diﬀerent sample sizes in Figure 11.

Fig. 7: The 3-D plot of Exam-
ple 2.

Fig. 8: The projection of Ex-
ample 2 onto y = 0.

Fig. 9: The number of initial
points that lead to the samllest
objective values.

Fig. 10: The objective values computed by the MM+SN algorithm of Example 2.

−1−0.500.51−1−0.500.5100.511.522.53x1x2yN=50N=100N=200N=500010020030040050024829835138601002003004005000.060.080.10.120.140.160.180.2sample size = 5001002003004005000.080.090.10.110.120.130.140.150.16sample size = 10001002003004005000.080.090.10.110.120.130.140.150.16sample size = 20001002003004005000.080.090.10.110.120.130.14sample size = 500−1−0.500.51−1−0.500.51y = −2x1y = −4x1 −4x2 +1y = −4x1 −6x2 +1     y = −x1 −7x2N=50N=100N=200N=500010020030040050020233236037101002003004005000.050.10.150.20.250.30.350.40.45sample size = 50010020030040050000.10.20.30.40.50.60.7sample size = 100010020030040050000.10.20.30.40.50.60.70.80.9sample size = 200010020030040050000.20.40.60.81sample size = 500Fig. 6: The solutions of Example 1 with the smallest objective values.

Fig. 11: The solutions of Example 2 with the smallest objective values.

6.2. Real data. We also conduct the experimental evaluation of the proposed
algorithm on the real datasets obtained from the UCI Repository of machine learning
databases https://archive.ics.uci.edu/ml/index.php. We ﬁrst list the problem charac-
teristics and report the performance of the MM+SN method averaged over the pairs
(k1, k2) employed in the experiments, i.e., satisfying 1 ≤ k1, k2 ≤ 4 in Table 1, where
the two columns under “iterations” present, respectively, the number of iterations of
the MM algorithm and the total numbers of iterations of the SN algorithm. For the
ﬁrst four problems where the sample sizes N are much larger than the dimensions
d, no sparse regularization term is added to the model. For the other two problems
where d is relatively large, we take the SCAD function as the sparsity inducing penalty
function. The sparse penalty parameter is estimated by 5-fold cross-validation. One
can see the eﬃciency of the proposed MM+SN method, where it usually takes no more
than 10 SN iterations for each MM subproblem. As an illustration, Figures 12 and 13
exhibit the performance of the MM algorithm (objective values vs iteration number)
and the SN algorithm (log10 (cid:107)∇ξ(λk, µk; (cid:98)z ν)(cid:107) vs iteration number) for solving the
“auto MPG” problem with (k1, k2) = (2, 2).

problem name

banknote authentication
concrete compressive strength
auto MPG
airfoil self-noise
Libras Movement
Communities and Crime

N

1372
1030
392
1503
360
2215

d

4
8
7
5
91
147

iterations
MM SN
72
197
185
156
187
251

9
23
25
18
22
31

time

3.5 s
10.8 s
3.9 s
12.3 s
14.7 s
43.7 s

Table 1: The performance of the MM+SN algorithm

Fig. 12: The performance of the MM algorithm.

Fig. 13: The performance of the SN algorithm.

To demonstrate the advantage of using the piecewise aﬃne model over the classical
linear regression model, we compare the solution of the least-squares piecewise aﬃne

regression θ PA with the ordinary least-squares solution θ LS ∈ argmin

θ

1
2N

(xs, 1)T θ (cid:107)2 in the following way. Let the full dataset of each instance obtained from
the UCI Repository be T , which is further divided into a training set T f
training and
a test set T f
test for f = 1, . . . , 5 by 5-fold cross-validation. For each fold, we ﬁnd
the estimator θ PA,f based on the training set T f
training by choosing the solution with
the smallest objective value over 20 runs of the MM+SN algorithm (with each run
corresponding to one initial point). Such a θ PA,f is likely to be an (un-proven) global
minimizer of the nonconvex least-squares piecewise aﬃne regression program. We
compute the prediction errors of the two estimators respectively by

N
(cid:88)

(cid:107) ys −

s=1

EPA (cid:44)

5
(cid:88)

f=1

1
|T f
test|

(cid:88)

(cid:16)

ys − ψ(xs; θPA,f )

(cid:17)2

, ELS (cid:44)

s∈T f

test

5
(cid:88)

f=1

1
|T f
test|

(cid:88)

ys − (xs, 1)T θ LS,f (cid:17)2
(cid:16)

.

s∈T f

test

We take 100 simulations and report the average ratio EPA/ELS for diﬀerent choices of
(k1, k2) in Table 2. It can be seen that all piecewise aﬃne results are better than the
linear regression results (k1 = k2 = 1); moreover, with proper choices of (k1, k2), the
prediction error given by the piecewise aﬃne regression model is signiﬁcantly smaller
than that given by the linear regression model as highlighted by the best entry in
each table.

As a ﬁnal remark, we have observed in our simulation studies that even when
the true model is linear, the continuous piecewise aﬃne regression ﬁts (for diﬀerent
choices of k1 and k2) are very close to that of linear regression, i.e., the ratios of the
prediction errors (in 5-fold cross validation as given in Table 2) are all very close to 1.
This provides further evidence that the continuous piecewise aﬃne regression model
could be a worthwhile and useful extension of linear regression.

In conclusion, we have demonstrated the practical worthiness of the LS piece-
wise aﬃne regression model formulated as a nonconvex nondiﬀerentiable program and
solved by the nonmonotone MM+SN method. Further applications of this method to
other statistical estimation problems will be reported separately.

REFERENCES

[1] M. Ahn, J.S. Pang, and J. Xin, Diﬀerence-of-convex learning: directional stationarity, opti-

mality, and sparsity, SIAM J. Optim., 27 (2017), pp. 1637–1665.

[2] H. Attouch and J. Bolte, On the convergence of the proximal algorithm for nonsmooth

functions involving analytic features, Math. Program., 116 (2009), pp. 5–16.

02468101214160123456789iteration numberobjective value1234567810−510−410−310−210−1100101102103104105iteration numberlog10!∥∇ξ(λk,µk;zν∥"banknote authentication

concrete compressive strength

k2

k2

k2

1
2
3
4

1
2
3
4

1
2
3
4

k1

k1

k1

1

1.00
0.84
0.81
0.85

2

0.74
0.73
0.72
0.71

auto MPG

1

1.00
0.97
0.83
0.85

2

0.77
0.74
0.78
0.84

Libras Movement

1

1.00
0.83
0.78
0.90

2

0.93
0.86
0.72
0.73

3

0.68
0.65
0.74
0.69

3

0.72
0.79
0.76
0.73

3

0.82
0.77
0.72
0.78

4

0.67
0.63
0.74
0.74

4

0.77
0.73
0.76
0.73

4

0.80
0.77
0.76
0.80

k2

k2

k2

1
2
3
4

1
2
3
4

1
2
3
4

k1

k1

1

1.00
0.85
0.82
0.77

2

0.74
0.46
0.65
0.54

airfoil self-noise

1

1.00
0.76
0.68
0.62

2

0.83
0.72
0.68
0.62

3

0.47
0.39
0.62
0.57

3

0.77
0.72
0.67
0.62

Communities and Crime
k1

2

3

1

1.00
0.862
0.82
0.87

0.88
0.82
0.83
0.84

0.85
0.84
0.83
0.86

4

0.38
0.39
0.61
0.60

4

0.74
0.59
0.57
0.425

4

0.83
0.84
0.83
0.87

Table 2: Ratio of the prediction error for diﬀerent choices of (k1, k2).

[3] H. Attouch, J. Bolte, and B.F. Svaiter, Convergence of descent methods for semi-algebraic
and tame problems: proximal algorithms, forward-backward splitting, and regularized
Gauss-Seidel methods, Math. Program., 137 (2013), pp. 91–129.

[4] A.M. Bagirov, C. Clausen, and M. Kohler, An algorithm for the estimation of a regression
function by continuous piecewise linear functions, Comput. Optim. Appl., 45 (2010), pp.
159–179.

[5] H.H. Bauschke and P.L. Combettes, Convex Analysis and Monotone Operator Theory in

Hilbert Spaces, Vol. 408., Springer, New York, 2011.

[6] D.P. Bertsekas, Nonlinear Programming, Third Edition, Athena Scientiﬁc, Belmont, Mas-

sachusetts, 2016.

[7] P.J. Bickel and K. Doksum, Mathematical Statistics, Basic Ideas and Selected Topics,

Volume I, Prentice Hall, New Jersey, 2006.

[8] J. Bolte, A. Daniilidis A, and A. Lewis, The (cid:32)Lojasiewicz inequality for nonsmooth sub-
analytic functions with applications to subgradient dynamical systems, SIAM J. Optim.,
17 (2006), pp. 1205–1223.

[9] J. Bolte and E. Pauwels, Majorization-minimization procedures and convergence of SQP
methods for semi-algebraic and tame programs, Math. Oper. Res., 41 (2016), pp. 442–465.
[10] J. Bolte, S. Sabach, and M. Teboulle, Proximal alternating linearized minimization for
nonconvex and nonsmooth problems, Math. Program., 146 (2014), pp. 459–494.
[11] L.D. Brown, Fundamentals of Statistical Exponential Families, IMS Lecture Notes and Mono-

graphs Series, California, 1986.

[12] E.J. Cand`es, M. Wakin, and S. Boyd, Enhancing sparsity by reweighted (cid:96)1 minimization,

J. Fourier Anal. Appl., 14 (2008), pp. 877–905.

[13] F.H. Clarke, Optimization and Nonsmooth Analysis, SIAM, 1990.
[14] C. Cortes and V. Vapnik, Support-vector networks, Mach. Learn., 20 (1995), pp. 273-297.
[15] Y. Cui and J.S. Pang, On the ﬁnite number of directional stationary values of piecewise

programs, arXiv:1803.00190, 2018.

[16] Y. Cui, T.H. Chang, M. Hong, and J.S. Pang, A study of piecewise linear-quadratic pro-

grams, arXiv:1709.05758, 2018.

[17] A.P. Dempster, N.M. Laird, and D.B. Rubin, Maximum likelihood from incomplete data

via the EM algorithm, J. Roy. Stat. Soc. B: Met., 39 (1977), pp. 1–38.

[18] H. Dong, M. Ahn, and J.S. Pang, Structural properties of aﬃne sparsity constraints, Math.

Program., in print, 2018.

[19] D. Dunson and L.A. Hannah, Bayesian non-parametric multivariate convex regression,

arXiv:1109.0322, 2011.

[20] D. Dunson and L.A. Hannah, Multivariate convex regression with adaptive partitioning, J.

Mach. Learn. Res., 14 (2013), pp. 3261–3294.

[21] F. Facchinei and J.S. Pang, Finite-dimensional Variational Inequalities and Complementar-

ity Problems, Springer, New York, 2003.

[22] J. Fan and R. Li, Variable selection via nonconcave penalized likelihood and its oracle prop-

erties, J. Am. Stat. Assoc., 96 (2001), pp. 1348–1360.

[23] X. Glorot, A. Bordes, and Y. Bengio, Deep sparse rectiﬁer neural networks, In Proceedings
of the 14th International Conference on Artiﬁcial Intelligence and Statistics, 2011, pp.
315–323.

[24] G. Hahn, M. Banergjee, and B. Sen, Parameter estimation and inference in a continuous
piecewise linear regression model, Technical report, Columbia University, 2016.
[25] P. Hartman, On functions representable as a diﬀerence of convex functions, Pac. J. Optim.,

9 (1959), pp. 707–713.

[26] T. Hastie, R. Tibshirani, and M. Weinwright, Statistical Learning with Sparsity: The

Lasso and Generalizations, CRC Press, 2015.

[27] D.R. Hunter and K. Lange, A tutorial on MM algorithms, Am. Stat., 58 (2004), pp. 30–37.
[28] R. Koenker and Jr G. Bassett, Regression quantiles, Econometrica, 46 (1978), pp. 33–50.
[29] M. Kojima and S. Shindo, Extensions of Newton and quasi-Newton methods to systems of

PC1 Equations, J. Oper. Res. Soc. Jpn., 29 (1986), pp. 352–374.

[30] B. Kummer, Newton’s method for nondiﬀerentiable functions, In Advances in Mathematical

Optimization, Akademie-Verlag, Berlin, 1988, pp. 114–125.

[31] K. Lange, MM Optimization Algorithms, SIAM, 2016.
[32] H.A. Le Thi, V.N. and D.T. Pham, Convergence analysis of diﬀerence-of-convex algorithm

with subanalytic data, J. Optim. Theory Appl., 179 (2018), pp. 103–126.

[33] H.A. Le Thi and D.T. Pham, The DC programming and DCA revised with DC models of

real world nonconvex optimization problems, Ann. Oper. Res., 133 (2005), pp. 25–46.

[34] H.A. Le Thi, D.T. Pham, and X.T. Vo, DC approximation approaches for sparse optimiza-

tion, Eur. J. Oper. Res., 244 (2015), pp. 26–46.

[35] X. Li, D.F. Sun, and K.C. Toh, A highly eﬃcient semismooth Newton augmented Lagrangian
method for solving Lasso problems, SIAM J. Optim., 28 (2018), pp. 433–458.
[36] Z. Lu, Z. Zhou, and Z. Sun, Enhanced proximal DC algorithms with extrapolation for a
class of structured nonsmooth DC minimization, Math. Program., in print, 2018.
[37] J. Mairal, Optimization with ﬁrst-order surrogate functions, In Proceedings of the 30th

International Conference on Machine Learning, 2013, pp. 783–791.

[38] J. Mairal, Incremental majorization-minimization optimization with application to large-

scale machine learning, SIAM J. Optim., 25 (2015), pp. 829–855.

[39] R. Mazumder, A. Choudhury, G. Iyengar, and B. Sen, A computational framework for

multivariate convex regression and its variants, J. Am. Stat. Assoc., in print, 2018.

[40] R. Mifflin, Semismooth and semiconvex functions in constrained optimization, SIAM J.

Control Optim., 15 (1977), pp. 959–972.

[41] A. Milzarek and M. Ulbrich, A semismooth Newton method with multidimensional ﬁlter

globalization for (cid:96)1 optimization, SIAM J. Optim., 24 (2014), pp. 298–333.

[42] B.S. Mordukhovich, Variational Analysis and Generalized Diﬀerentiation I, Springer, 2006.
[43] V. Nair and G.E. Hinton, Rectiﬁed linear units improve restricted Boltzmann machines, In
Proceedings of the 27th International Conference on Machine Learning, 2010, pp. 807–
814.

[44] J. M. Ortega and W. C. Rheinboldt, Iterative solution of nonlinear equations in several

variables, Vol. 30, SIAM, 1970.

[45] J.S. Pang, Newton’s method for B-diﬀerentiable equations, Math. Oper. Res., 15 (1990), pp.

311–341.

[46] J.S. Pang and L. Qi, A globally convergent Newton method for convex SC1 minimization

problems, J. Optim. Theory Appl., 85 (1995), pp. 633–648.

[47] J.S. Pang, M. Razaviyayn, and A. Alvarado, Computing B-stationary points of nonsmooth

DC programs, Math. Oper. Res., 42 (2016), pp. 95–118.

[48] D.T. Pham and H.A. Le Thi, Convex analysis approach to DC programming: Theory, algo-
rithm and applications, Acta Mathematica Vietnamica, 22 (1997), pp. 289–355.
[49] H. Qi and D.F. Sun, A quadratically convergent Newton method for computing the nearest

correlation matrix, SIAM J. Matrix Anal. A., 28 (2006), pp. 360–385.

[50] L. Qi and J. Sun, A nonsmooth version of Newtons method, Math. Program., 58 (1993), pp.

353–367.

[51] R.T. Rockafellar, Convex Analysis, Princeton University Press, Princeton, 1970.

[52] R.T. Rockafellar and R.J.-B. Wets, Variational Analysis, Springer, New York, 1998.
[53] S. Scholtes, Introduction to Piecewise Diﬀerentiable Equations, Springer Briefs in Optimiza-

tion, 2002.

[54] J. Sun, On Monotropic Piecewise Quadratic Programming, Ph.D. dissertation, Department

of Applied Mathematics, University of Washington, 1986.

[55] J. Sun, On the structure of convex piecewise quadratic functions, J. Optim. Theory Appl., 72

(1992), pp. 499–510.

[56] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight, Sparsity and smoothness

via the fused lasso, J. Roy. Stat. Soc. B: Met., 67 (2005), pp. 91–108.

[57] B. Wen, X. Chen, and T.K. Pong, A proximal diﬀerence-of-convex algorithm with extrap-

olation, Comput. Optim. Appl., 2 (2018), pp. 297–324.

[58] C.J. Wu, On the convergence properties of the EM algorithm, Ann. Stat., 11 (1983), pp.

95–103.

[59] L.Q. Yang, D.F. Sun, and K.C. Toh, SDPNAL+: a majorized semismooth Newton-CG aug-
mented Lagrangian method for semideﬁnite programming with nonnegative constraints,
Math. Program. Comput., 7 (2015), pp. 331–366.

[60] P. Yin, Y. Lou, Q. He and J. Xin, Minimization of (cid:96)1 − (cid:96)2 for compressed sensing, SIAM J.

Sci. Comput., 37 (2015), pp. 536–563.

[61] D. Yu and L. Deng, Automatic Speech Recognition: A Deep Learning Approach, Signals

and Communications Technology, Springer, 2015.

[62] M. Yuan and Y. Lin, Model selection and estimation in regression with grouped variables,

J. Roy. Stat. Soc. B: Met., 68 (2006), pp. 49–67.

[63] C. Zhang, Nearly unbiased variable selection under minimax concave penalty, Ann. Stat., 38

(2010), pp. 894–942.

[64] C. Zhang, M. Pham, S. Fu, and Y. Liu, Robust multicategory support vector machines
using diﬀerence convex algorithm, Math. Program., 169 (2018), pp. 277–305.
[65] X. Zhao, D.F. Sun, and K.C. Toh, A Newton-CG augmented Lagrangian method for

semideﬁnite programming, SIAM J. Optim., 20 (2010), pp. 1737–1765.

