Neural language models for network conﬁguration:
Opportunities and reality check

Zied Ben Houidia, Dario Rossia

aHuawei Technologies France SASU, 20 quai du point du jour, Boulogne-Billancourt, France

Abstract

Boosted by deep learning, natural language processing (NLP) techniques have recently
seen spectacular progress, mainly fueled by breakthroughs both in representation learning
with word embeddings (e.g. word2vec) as well as novel architectures (e.g. transformers).
This success quickly invited researchers to explore the use of NLP techniques to other
ﬁeld, such as computer programming languages, with the promise to automate tasks in
software programming (bug detection, code synthesis, code repair, cross language trans-
lation etc.). By extension, NLP has potential for application to network conﬁguration
languages as well, for instance considering tasks such as network conﬁguration veriﬁca-
tion, synthesis, and cross-vendor translation. In this paper, we survey recent advances
in deep learning applied to programming languages, for the purpose of code veriﬁcation,
synthesis and translation:
in particularly, we review their training requirements and
expected performance, and qualitatively assess whether similar techniques can beneﬁt
corresponding use-cases in networking.

Keywords: Natural language processing, Code veriﬁcation, synthesis, translation,
Network conﬁguration veriﬁcation, synthesis translation

1. Introduction

Network operators often rely on a heterogeneous set of equipments from diﬀerent ven-
dors, each of which uses diﬀerent proprietary conﬁguration languages. Such heterogeneity
poses a number of challenges, that have the potential to turn the dream of intent-based,
fully autonomous and self-driving networks into a waking nightmare. While the reliance
on multiple vendors is a very logical choice from a business perspective, it makes network
management a quite complex task. Eﬃciently managing real networks needs knowledge
about the speciﬁcs of each of these multiple vendors, a skill that is rare in practice. This
has pervasive consequences and aﬀects many tasks from provisioning (i.e. generating
new conﬁgurations) to veriﬁcation, to monitoring, debugging and troubleshooting.

To counter this problem, the network community attempted to create additional
abstractions to unify network control, of which Batﬁsh[1] from Intentionet is one popular
example. Batﬁsh uses expert rules to transform each proprietary conﬁguration into a
uniﬁed language model, that can be used to verify the correctness of a conﬁguration or
visualize its eﬀects before it is actually put in place. Similarly, many other approaches (see
Sec. 2.2) leverage formal methods to automate various aspects of network conﬁguration,

Preprint submitted to Computer Communications VSI: SI Marco Ajmone

July 26, 2022

2
2
0
2

l
u
J

5
2

]
I

N
.
s
c
[

3
v
8
9
3
1
0
.
5
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
often transforming a network-wide conﬁguration into a large logical formula, then for
example adding speciﬁcation constraints on top, to verify that the speciﬁcations match
the conﬁguration. Although promising, such approaches still need humans in the loop,
either to build or update the uniﬁed language model, or to manually design the numerous
veriﬁcation tests required for the new uniﬁed language. Devil’s advocating, one can say
that approaches like Batﬁsh solve the problem of “too many proprietary languages” by
adding another proprietary language [2].

At the same time, advances in deep learning based Natural Language Processing
(NLP) techniques have opened new opportunities, allowing to perform tasks that seemed
impossible a decade ago. For example, recent progress in unsupervised neural machine
translation makes it possible today to translate between diﬀerent languages, using mostly
mono-lingual1 corpora [3], which are usually abundant. Text generation has also reached
impressive performance [4], as widely popularized by OpenAI GPT-3 [5]. This giant step
in representation learning from natural language corpora naturally raises the question of
whether these breakthroughs can proﬁt other artiﬁcial languages, such as those used for
computer programming or network conﬁguration.

Whereas neural NLP techniques have so far enjoyed a limited adoption in the network
community, the last few years have witnessed an emerging trend in the application of NLP
technologies to programming languages, with growing success on tasks such as code (i)
veriﬁcation, (ii) synthesis and even (iii) translation. We observe that the above three axes
are very much in line with important questions regarding the beneﬁts of NLP for network
management, speciﬁcally: (i) whether properly modeled network conﬁguration languages
could ease the detection of misconﬁgurations or underspeciﬁcations, as it is possible to
detect grammatical mistakes in natural language; (ii) whether recent advances in NLP
could allow to make advances in router conﬁguration synthesis, in a way that is similar
to natural text and code generation; (iii) whether it is feasible to automatically translate
between routing conﬁguration languages of diﬀerent vendors – which, if successful, could
be a signiﬁcant step towards reducing the interoperability gap.

In this paper we argue that, to some extent, progress on NLP for programming
languages can be informative so as to estimate the potential of NLP for solving similar
tasks on network conﬁguration languages. Yet, the state of progress in the programming
language domain is not fully clear: generally speaking, it is diﬃcult to discriminate
hype from small-step methodological contributions, so to select which approach among
the numerous proposals could the most promising for network conﬁguration. Second, it
is also non-trivial to assess at which cost in terms of data and computing power such
prowesses were made possible, and thus at which upfront cost it would be possible to
apply NLP advances to network conﬁguration use-cases. Finally, it is even harder to
estimate if, and within which time-frame, such pioneering approaches can transition to
useful products extensively used in real systems.

The goal of this paper is therefore to systematically analyze the progress of recent
NLP technologies for artiﬁcial programming languages, and assess their potential for
application to the network conﬁguration languages for management purposes.
In the
remainder of the paper, we ﬁrst overview state of the art in the network management
use-cases of conﬁguration veriﬁcation, synthesis and cross-vendor translation (Sec. 2).

1Otherwise stated, neural language models can learn to translate from reading independently books in
each language, as opposed to requiring a curated parallel corpora of books translated in many languages
2

We then review the recent literature on NLP-empowered programming languages for
the use-cases of code veriﬁcation, synthesis and cross-language translation (Sec. 3.2). We
next make an explicit parallel between these two ﬁelds: considering both a broad view, as
well as using an illustrative networking example, we assess limits of current methods, data
and model training requirements, need for pre-processing, task complexity and expected
performance (Sec. 4). Based on this analysis, we gather conclusive remarks and sketch a
high-level research roadmap (Sec. 5).

2. NLP for “Network Language” Processing

We ﬁrst identify opportunities where NLP could help network operations.

In this
paper, we focus on three relevant use-cases of network conﬁguration, introduced next,
and overview how they are dealt with in the networking literature. For the sake of
illustration, throughout the paper we will also systematically make reference to the toy
case of an ISP network administrator that has to generate, verify or translate the BGP
conﬁgurations of one or all its routers.

2.1. NLP Potential for Network Conﬁguration

Conﬁguration Veriﬁcation. A critical task receiving growing attention in the network-
ing community is conﬁguration veriﬁcation, with the accessory goals of detecting and
ﬁxing anomalies in the conﬁguration. The classic examples are reachability (e.g., how
can we verify that a network conﬁguration is free from e.g. black holes, loops, or other
conﬂicting rules) and compliance (e.g., verifying that the conﬁguration satisﬁes a given
policy). As we shall overview later, existing approaches rely on formal methods heavily
involving human experts. The question (and opportunity) here is whether data-driven
NLP methods can be of any help. For example, an open question is whether language
models (trained in an unsupervised manner on mostly correct network-wide conﬁgura-
tions) coupled with classic anomaly detection techniques can spot misconﬁgurations or
inconsistencies. A related question is whether the further addition of a supervised layer
(ﬁne-tuned on certainly correct conﬁgurations) can assist in repairing the errors. If this
task could be successfully implemented by NLP then, for the BGP toy case, an operator
would only have to submit the conﬁguration ﬁles of its network and inspect an output
report about syntax and reachability errors. If the operator further provided speciﬁca-
tions about routing policies, the checker would verify if routes are not redistributed in
ways that violate these policies.

Conﬁguration Synthesis. One recurrent task in networking is the conﬁguration of vari-
ous devices, either to satisfy customer requests (i.e. provisioning) or to later optimize
various resources when adapting to network conditions. The input would be customer
requests or SLAs expressed either in some arbitrary speciﬁcation language (possibly com-
plemented by natural language description in the longer term) and the output is a set
of conﬁgurations that satisfy this goal. The paramount question in this case is to what
extent can NLP tools help automating such generative process – which is inline with the
long-term wish of intent-based networking.

With reference to the BGP toy case, if this task is successfully implemented by NLP,
the network operator would only have to master a high-level speciﬁcation language,

3

Table 1: Simple taxonomy and non-exhaustive list of example work related to network management and
conﬁguration
Use-case

Network application

Methods

Veriﬁcation

Model checking [11]
SMT [12, 13, 14]
Graph-based [15]
Custom [16, 1, 17, 18]

Synthesis

SMT [19, 14]
Stratiﬁed Datalog [20, 21]
Custom model [22]

Explanation Custom [23, 24]

BGP-only[12]
ACL [14]
Subset of protocols [17, 15, 11]
General [1, 13, 18]

BGP-only [19]
ACL[14]
BGP/OSPF/Static [20, 21]
General[22]

OSPF/BGP [23]
Forwarding state [23]

describing reachability requirements depending on negotiated policies, and the NLP agent
would automatically generate the corresponding conﬁgurations.

Conﬁguration Translation. Finally, as earlier mentioned, another signiﬁcant problem
faced by large operators is due to their control of large heterogeneous ﬂeets of network
devices from multiple vendors, each using own proprietary languages. Besides the day-
to-day management hurdle, migrating an equipment from one vendor to the other today
can be cumbersome, not only in terms of translation but also in terms of network conﬁg-
uration understanding and cleaning. Therefore, the potential for NLP is manifest, as it
would be desirable to leverage neural language techniques for learning to automatically
translate between conﬁguration languages of various vendors. If successful, this would
not only ease the advent of the self driving network vision (one language to control them
all) but also the management of legacy networks.

With reference to the BGP toy case, a network operator who needed to replace
a router with another from a diﬀerent vendor, would only have to submit the legacy
conﬁguration lines to obtain the new one in the target language.

2.2. Current State of the Art in Network Conﬁguration

At the same time, existing attempts to automate network conﬁguration veriﬁcation
and synthesis are mostly rule-based, often relying on formal methods, and have not yet
exploited NLP techniques to the best of our knowledge.

Outside the above network conﬁguration use cases, we point out that recent NLP
techniques are used in networking, such as word embeddings to learn representations[6,
7, 8] and transformers applied to graphs[9, 10].
In this section, focusing on network
conﬁguration, and without aiming at presenting an exhaustive survey of related literature,
we compactly present in Tab.1 a simple taxonomy of the state of the art, along with
representative samples for each category.

Conﬁguration Veriﬁcation. As summarized in Table 1, veriﬁcation methodologies cur-
rently used for network conﬁguration are not leveraging NLP yet and can be mapped

4

into several families, namely: (i) graph models or (ii) custom graph-based abstractions
and exploration, (iii) explicit state model checking on top of a formal language and (iv)
satisﬁability modulo theory.
In more details, Netdb [16] was among the ﬁrst seminal
attempts to automate the parsing, modelling, and the correctness veriﬁcation of network
wide conﬁguration ﬁles, which received more attention lately. Batﬁsh [1] is one popular
example which builds a data plane model from router conﬁgurations and uses it among
others for conﬁguration veriﬁcation. ARC[15] builds a graph-based abstraction of the
control plane from network conﬁguration ﬁles and uses it to analyze the control plane
under arbitrary failures, without generating the data plane. Bagpipe [12] uses an SMT
solver to verify that BGP conﬁgurations satisfy the policies expressed by the operator.
Minesweeper[13] also transforms network conﬁguration ﬁles into a logical formula that
captures the ﬁnal state or behavior of the data plane, that is then combined with in-
tended properties or desired speciﬁcations to see if both can match. Plankton[11] uses
explicit-state model checking (together with symbolic partioning) to considerably speed
up network policies veriﬁcation. NetDice [17] analyzes the probabilities that certain
properties hold in the network without fully enumerating all possible link failures. Net-
Dice input is, however, already curated network conﬁgurations expressing BGP, OSPF,
ECMP, and static route properties and, methodology-wise, the paper heavily relies on
domain knowledge. Finally, instead of relying on too general search strategies used by
SMT solvers during veriﬁcation, Tiramisu [18] builds its own network model and search
strategies that are more eﬃcient for network models: for example, it leverages routing
algebra to verify paths in the graph in one shot (instead of emulating protocols as done
with explicit-state model checking).

With reference to the BGP toy case, the network operator currently has to master one
of the declarative languages to express its BGP routing policies or intents, and use any of
the existing tools above to evaluate whether the conﬁgurations match these policies. A
challenge there is to actually derive the properties that must be satisﬁed. For example,
Bagpipe [12] infers such desired policies from real AS conﬁgurations or from prior re-
search work (e.g., the famous Gao-Rexford conditions [25]). Unlike such exact methods,
applying unsupervised anomaly detection methods on learned NLP representations could
be promising to detect and locate anomalies in conﬁgurations and policies without incur-
ring the burden of having to formally express them. Alternatively, if the speciﬁcations or
intents of the operator are available, then NLP methods could broaden the current set of
exact methods, e.g, by learning to generalize over a larger set of speciﬁcation languages.

Conﬁguration Synthesis. Automatic generation of network conﬁguration is another ac-
tive area where, e.g., Jinjing[14] helps Alibaba’s operators to transform their declarative
conﬁguration intents into ACL rule conﬁguration updates. Alibaba group has also de-
veloped NetCraft [22], a tool that builds a uniﬁed network model and uses it to safely
manage the life cycle of network conﬁguration. Prior to that, Propane/AT [19] builds
new abstractions to synthesise BGP conﬁgurations with correctness proofs from high-
level speciﬁcations and requirements. SyNet [21] augments stratiﬁed datalog to perform
synthesis for protocols that can be expressed in this language, leading the authors to
support OSPF, Static routes and a simpliﬁed version of BGP. Netcomplete[20] later
complemented the work with full support of BGP and orders of magnitude faster com-
putation. Finally, thinking about synthesis in a broader sense, it is worth mentioning
that similar formal methods have been used to programmatically (i) generate protocol
5

code (but not conﬁgurations) from RFC requirements[26], and (ii) speciﬁcations from
lower-level axiomatic requirements [27].

With our BGP example in mind, similarly to the veriﬁcation use case, the operator has
only to master a high-level speciﬁcation language, more concise and thus easier to express
than actual conﬁguration language. While the conﬁguration is currently generated with
algorithms (such as SMT, datalog or other custom models) that do not leverage NLP,
in this paper we consider the question of to what extent NLP advances could help the
network expert in his operational tasks.

Conﬁguration Explanation. Whereas translation of conﬁguration across languages has
not been heavily investigated so far, a related task consists in summarizing and explain-
In this context, a ﬁrst body of literature
ing conﬁgurations to the human operators.
empirically studied the evolution of network conﬁgurations in the wild [28, 29, 30]. For
example, using custom parsers, authors perform a longitudinal evaluation over a time
period of two [30] to ﬁve [29] years, conﬁrming that network conﬁguration grow more
complex over time. A second body of literature attempted to automate the extraction
of human-readable insights from network conﬁguration, or indirectly from its forwarding
state. For the latter, Net2text [23] aims at generating highly interpretable text explain-
ing network forwarding behavior. Conﬁg2spec [24] transforms router conﬁgurations into
formal speciﬁcations, using Batﬁsh[1] to parse router conﬁgurations – which helps oper-
ators in formalizing policies, but could be also a ﬁrst step towards building datasets for
more systematic usage of NLP techniques. In the BGP toy case and beyond, generative
abilities of NLP techniques popularized in the latest years by, e.g., GPT-3, are promising
to further provide comments and explanations in a native human language.

3. NLP for “Computer Language” Processing

Whereas spectacular progress [5] in NLP has primarily beneﬁted “natural” language
applications, a research trend emerged lately for application of NLP to “artiﬁcial” pro-
gramming languages. In particular, recent NLP applications fall within the areas of code
veriﬁcation, code synthesis and code translation – which mirror the use cases above that
we envision for networking, and that we overview in this section.

3.1. Code Veriﬁcation

A considerable recent eﬀort has explored the use of neural networks and NLP for bug
detection and code veriﬁcation. Habib et al.[31] studied the opportunity of formulating
bug-ﬁnding as a classiﬁcation problem trained on examples of buggy and bug-free code
snippets. Richter and Wehrheim [32] stress that existing neural bug detection techniques
work only for unrealistically generated bugs, and proposed novel ways to build realistic
training datasets. This joins the conclusions of [31], in that despite having good quanti-
tative performance, the models struggled to understand obvious program properties.

As often the case in machine learning, learning good latent representations is key
to good performance in later tasks [33]:
in the software context, code2vec [34] is a
notable example. Code2vec ﬁrst transforms the code into an Abstract Syntax tree (AST)
then builds embeddings leveraging the multiple AST paths between program entities.
Code2vec learns the representation of each path together with the representation of an

6

aggregation of paths. As a use case, Code2vec allows to successfully predict the name
of a method from the vector representation of its body. Also, similarly to word2vec,
code2vec learns method name vectors that interestingly capture semantic similarities
between code snippets. Encouraged by this success, Briem et al. [35] conﬁrmed the
potential of code2vec representation in other use cases than function naming, as e.g., to
ﬁnd simple bugs such oﬀ-by-one bugs in Java. To further enhance code representation
learning for bug-detection, Li et al. [36] complement the local context extracted from
the generated AST (used by code2vec) with a global context coming from Program
Dependence Graph (PDG) and Data Flow Graph (DFG): the latter allows to take into
account also the “far-apart” dependencies between the various methods used in the code.
For the local context part, they apply word2vec [37] on the sequence of nodes in AST
paths and use node2vec [38] for the global context given by PDG and DFG graphs.

Overall, we see that signiﬁcant progress in code veriﬁcation is achieved by leveraging
the existing domain expert knowledge to increase the power of the learned NLP represen-
tations – which can be expected to hold for other artiﬁcial languages..

3.2. Code Synthesis

The goal of code synthesis is, starting from a text description in natural language or
sometimes simply a function name, to generate the right code snippet. Machine learning
on the other hand promises to get rid of these manual eﬀorts: for example, huge amounts
of commented code are available online, so that one can learn to transform the comments
describing a function into the actual code of the function.

One notorious example of such attempts is represented by the 2020 NLC2CMD [39]
NeurIPS competition on learning how to transform natural language descriptions into
CLI bash syntax: since the task takes natural language as input, an appealing choice is
to start from pre-trained large NLP models (such as GPT-3) and only reﬁne the training
for the task at hand, an approach followed by several teams of 2020 NLC2CMD and by
subsequent work [40, 41, 42].

PLBART[40] is a bidirectional and autoregressive transformer for program and lan-
guage understanding and generation, that achieves state of the art performance in code
summarization, synthesis and translation. As usual practice with transformers for NLP,
PLBART is pre-trained on huge amounts of unlabeled data (see later Sec.4.3 for details),
of both programming (GitHub) and natural (StackOverﬂow) languages. Overall, the au-
thors stressed the importance of task-independent pre-training (to achieve a good level
of program and language understanding) before specializing in various tasks. To illus-
trate its high representational power, authors show that thanks to pre-training, PLBART
learns deep aspects of code such as syntax, naming conventions etc.

Open AI Codex[41] is a related eﬀort, whose production version empowers the GitHub
Copilot project assisting developers with code autocompletion from docstring-like de-
scriptions and function names. Codex is obtained by ﬁne-tuning GPT-3 models over
python sources from GitHub, and exhibit signiﬁcant improvements over the GPT-3 per-
formance trained on natural language only:
if given the chance to produce many code
attempts, it can solve all the problems in the Human-eval dataset. At the same time,
limits remain since Codex (i) is not sample-eﬃcient, i.e. humans learn to code from sig-
niﬁcantly fewer examples, (ii) can invoke undeﬁned functions and variables and struggles
with long/high-level docstring and (iii) has trouble binding variables to objects. Along
the same direction, Austin et al.[42] performed a critical and thorough evaluation of the
7

abilities of large NLP models to synthesize code, shedding light on how, e.g., performance
varies according to model size, how chat interaction with a human in natural language
can reduce the error, etc.

Finally, we point out that relevant work targeting the synthesis of network-related
software (but not network-related conﬁgurations) recently started to appear, with e.g.,
NLP techniques applied to text of IETF Request For Comments (RFC) normative docu-
ments for the sake of either auto-discovering and ﬁxing ambiguities [43] or automatically
generating protocol implementations [26].

Overall, we see that the success of code synthesis is certainly tributary to learning
good representations from raw code. Equally importantly however, it is the availability of
docstrings and code comments that allowed to ﬁne tune large language models to perform
synthesis. Hence, the value of commenting code extends beyond the realm of human
understanding – whenever available, natural language comments accompanying code can
complement the formal representation of artiﬁcial language.

3.3. Code Translation

Driven by the success of deep learning in NLP translation from one human language
to another, recent work tackled the equivalent problem of code translation from one
programming language to another.

Chen et al. [44] is among the ﬁrst attempts at using recurrent neural networks (RNN)
for programming language code translation. They ﬁrst observe that, starting from a
certain length, RNNs struggle to generate syntactically correct programs. Inspired by
traditional approaches where the human translator builds a rule-based correspondence
between the grammars of the two programming languages, they propose to exploit the
modularity of the translation task, transforming it into a translation between a source
(parsed) tree and a target (parsed) tree.

Inspired by unsupervised translation for natural languages [3], more recent work [45,
46, 47] leverages transformer architectures for code translation. Similarly to unsupervised
translation in natural language, the goal is to ﬁnd a latent representation that is common
between the two languages, such that sentences or code snippets with the same meaning
have the same representation in the latent space. La Chaux et al. [45] proposed the ﬁrst
of such seq2seq architectures, leveraging attention and encoder/decoder blocks. The ﬁrst
step is to pre-train a cross-lingual language model, which can be done in a fully unsu-
pervised manner from an independent corpus, especially when few anchor points exist
between the two languages (e.g. for, while and other keywords for source code): after
training, the encoder can turn any input sequence into a common latent representation
vector. The second step is to learn a decoder that can generate a sentence in the target
domain: simplifying, this task is implemented using a denoising autoencoder, trained to
generate a sentence from a noisy version of it. The above architecture is shown [45] to
outperform all available commercial rule-based software, on a custom evaluation dataset
built by leveraging the GeeksforGeeks online platform2.

In a subsequent work, Roziere et al.[46] acknowledge one major limitation of their
previous natural language methods: namely the use of back-translation which implies
training on possibly noisy inputs and learning to generate noisy ones. While a small

2https://practice.geeksforgeeks.org/

8

noise in natural language may be imperceptible, this can lead to an incorrect program
in more structured source code. To counter this, the authors simply propose to leverage
automated unit-testing to ﬁlter incorrectly generated code.

Overall, we see that NLP is able to extract powerful representations from raw code
that, by leveraging multiple independent corpora each representing a diﬀerent program-
ming language, are also able to partly address automated translation among languages.

3.4. Other software-related tasks

In addition, other software related tasks are currently being investigated in the lit-
erature, that we brieﬂy overview next as they potentially open other opportunities for
network conﬁguration purposes. While we disregard them in this paper due to lack of
space, they are at least worth mentioning.

Automated code documentation. CodeTrans [48] is an example of a pre-trained model to
perform a variety of such tasks such as (i) code documentation Generation, (ii) source
code summarization (iii) code comment generation, (iv) commit message generation,
(v) API sequence recommendation. One question is whether similar methods could be
helpful for network conﬁguration explanation (for example, to document thousand-lines
long conﬁguration ﬁles of complex routing policies). Thinking back about our network
administrator, automatically generating BGP policy summarizations or explanations can
speed up the reasoning, easing the updates of existing policies as well as the transfer of
duties between various administrators.

Automated code completion/repair. Code completion [49] and repair [50] could also ﬁnd
applications in networking: network conﬁguration completion is tackled e.g., in [20], by
however using formal methods and not machine learning. In automated code repair, the
task is to transform a program with syntax errors into a correct one – a use-case that sits
in between code veriﬁcation (as code is altered) and synthesis (as the code generation
is more limited). Code repair is tackled for example in Break-It-Fix-It (BIFI) [50] by
a sophisticated yet more realistic training approach. Instead of creating pairs of (bad,
good) programs for training using heuristics, which leads to non representative training
data and models that overﬁt to unrealistic synthetic errors, BIFI introduces a breaker
network that is trained to generate realistic errors, in addition to a critic that checks the
Fixer’s output and augment training data with good outputs. The usefulness of such
bug detection/repair capabilities clearly extends to network conﬁguration as well.

4. Transfer from programming to conﬁguration languages: a reality check

In this section, we summarize the literature we exposed previously from a diﬀerent
angle. In particular, we aim at summarizing the lessons learned from NLP applications
to computer language tasks (in terms of challenges, model training requirements, data
availability, data pre-processing, solution quality, etc.), projecting the expected impact
of applying these NLP techniques to network conﬁguration tasks.

9

Figure 1: Learning from programming languages: a generic pipeline

4.1. Limits of current approaches

Lessons from computer language ﬁeld. Despite the tremendous advances of NLP for
programming language tasks, limits still exist. In recent work, Peng et al.[51] argue for
the need to take more into account the speciﬁcities of programming languages, which are
obviously diﬀerent from natural language. As we mentioned earlier, prior work used to
counter this lack of semantic understanding by adding expert features such as data ﬂow
graph [36]. For instance, programming language theory can formally deﬁne the semantics
of a program, seeing it as an entity that modiﬁes its environment (e.g. memory) and
I/O): this helps in [51] to learn intermediate
performing elementary operations (e.g.
code representations that align well with operations deﬁned in formal semantics and
additionally leverage information on environment changes. Similarly, Chakraborty et
al. [52] investigate the potential of deep learning methods in uncovering software bugs and
vulnerabilities. Their careful analysis sheds light on poor performance that the authors
attribute to a set of issues related to (i) unrealistic or bad-quality training data (ii)
simplistic models (e.g. simple tokenization instead of taking into account code structure).
Interestingly, their analysis reveals that whenever deep learning models perform well, it
may be due to the “wrong reasons” (e.g.
leveraging speciﬁc user-deﬁned variable or
function names to take their decisions, instead of more fundamental bug causes). Last,
as argued by Hellendoorn and Sawant [53], a major limit of current approaches remains
the exorbitant cost, both monetary and data-wise, to train the most performing models
– as we further develop in the next subsections.

Impact on network conﬁguration. Limits of NLP for computer programs are still far
from being well understood. But whatever these limits are, the same problems can be
expected to rise in network-use cases: for example, the diﬃculty of taking into account
programming languages speciﬁcity, is likely to also apply to network conﬁguration lan-
guages, which also have their own structured semantics. Unsurprisingly, the lack of large
volumes of good quality applies also to network use-cases, and so does the cost of train-
ing large models, for which the upfront cost to bootstrap NLP techniques for network
conﬁguration can be expected to be sizeable.

10

Input Parsing(Syntax Tree,Data flow graph…)Domain knowledgeUnsupervisedlearningMethodSupervisedlearningRepresentation Learning(embedding, seq2seq,  transformer..)Per task refinementO(106)  MethodsCorpusO(104)  labelsUsecasesVerification, TranslationSynthesis4.2. Model and training complexity

Lessons from computer language ﬁeld. We here report examples of model size and train-
ing time for computer language (whereas training cost for few models is extrapolated
in [53, 54]). Austin et al.’s models [42] ranged between 244 million and 137 billion non-
embedding parameters. CodeTrans[48] starts with small models of 60 million parameters
(17 days of training) but the large models have nearly 800 millions (nearly 90 days). Small
models are trained on one single NVIDIA GPU Quadro RTX 8000, while larger models
relied also on a number of Google TPUs (8× TPUv2 and 8× TPUv3). PLBART [40]
employs 140M parameters and (trained in about 2 weeks on 8 Nvidia GeForce RTX 2080
GPU). Codex [41] models containing up to 12B parameters (costing few hundreds of
petaﬂop/s-days). To give an idea, GPT-3 [4], one of the current largest natural language
models has 175 billion parameters (and would need 355 years of training on one Tesla
V100 GPU). Thus, we see that most programming-language models have a high com-
plexity, comparable to natural language ones: this is the case because such models take
both natural language and code as input to training (e.g. text to code) and hence, they
need both capabilities.

Impact on network conﬁguration. Assuming the right data for training is available (which
we assess next), the impact on network conﬁgurations depends mainly on the use case and
whether it needs only conﬁguration language or also natural language as input. Training
both on natural language and conﬁguration languages would lead to models at least as
complex as natural languages ones, so that only a handful of actors could aﬀord to train
them. Therefore, for network conﬁguration synthesis from high-level language, it is more
reasonable to start from pre-trained models on natural language (as opposite to train
from scratch) and ﬁne tune them with hybrid natural and conﬁguration language data.
Conversely, natural language is not needed for both network conﬁguration veriﬁcation and
translation, where training from scratch may be more feasible (given limited vocabulary
size and its complexity).

4.3. Data availability and corpus sizes

Lessons from computer language ﬁeld. Complementarily, we summarize the amounts of
data exploited to train NLP models for code-related tasks: in almost all existing litera-
ture [55], the (i) unsupervised pre-training phase uses huge amounts of data, while the
(ii) supervised reﬁnement needed reasonable amounts of labels. Examining the volumes
for the (i) unsupervised pre-training phase, for instance Code2vec [34] used a dataset of
14 M methods for unsupervised training. Li et al. [36] similarly leveraged a dataset of
almost 5 M methods. Austin et al.[42] trained on almost 3 B documents (web, dialog and
wikipedia), which were tokenized into 3 T byte-pair-encoding (BPE) tokens with a vocab-
ulary of 32 K tokens. Not all those documents however pertained to code: data including
both code and text constituted “only” around 14 M documents (roughly 18 B BPE to-
kens). Similarly, PLBART [40] used 36 B (Python), 28 B (Java) and 7 B (StackOverﬂow
natural language) tokens for pre-training extracted from 224 GB (Python), 352 GB (Java)
and 79 GB (StackOverﬂow HTML) code respectively. Although ﬁne-tuned from GPT-
3 family models that have already learned good NLP representations, Codex [41] was
further trained on 179 GB of unique python ﬁles scraped from GitHub.

11

Examining the volumes for the (ii) supervised reﬁnement phase, CodeTrans[48] needed
100 K labeled samples for its various tasks (as opposed to nearly 8 M samples, all pro-
gramming languages included, for the unsupervised pre-training). For comparison, the
NL2bash natural langage to bash dataset [56] used for the text to bash NeurIPS NLC2CMD
Competition in 2020[39] contained 10 K pairs.

Impact on network conﬁguration. Overall, it is the online accessibility of a plethora of
code-related data (source code, online questions and answers etc.) that fueled break-
through and successful applications of NLP to programming languages [55]. This is
clearly less the case for network conﬁguration languages, where publicly available data is
surely not abundant. The three use cases we consider need raw network conﬁgurations
for the unsupervised representation learning step: although corpus size requirements
are much less stringent than those of natural language, they remain considerable. Ad-
ditionally, unlike programming languages data which is abundant in various software
repositories and community forums, real network conﬁguration data is extremely scarce.
For instance, reconsidering datasets used in the network conﬁguration literature (see
Sec. 2.2), the largest dataset is Bagpipe [12] which was tested on 3 autonomous systems
totalling just 240K lines of BGP router conﬁgurations.

As a consequence, only few big ISPs and vendors could aﬀord in theory to create such
datasets. To oﬀset this problem, pooling of datasets (e.g., from university IT) seems a
way forward – though this can open security risks. Another option is to synthetically
generate network conﬁgurations for various topologies – with the risk of oversimpliﬁca-
tion. Alternatively, some narrow use-cases may beneﬁt from automatically collecting, at
the same time, pairs of conﬁgurations and corresponding forwarding tables and states,
which would allow training models to generate one from the other. Generally speaking,
the lack of large volumes of good quality and real data is expected to be a more stringent
limitation than the model complexity and training cost in our opinion.

4.4. Input pre-processing
Lessons from computer language ﬁeld. Similarly to tokenization for natural language,
programming languages, and network conﬁguration languages alike, need some pre-
processing beforehand. Many of the tools described earlier employ pre-processors to
extract other intermediate representations such as Abstract Syntax Trees (AST) from
source code before feeding them to Neural Networks. tree-sitter to extract AST from
code. Further work has studied the importance of using additional abstractions [57, 36]
or learning more sophisticated representations[58]. Allamanis et al. [57] proposed to ﬁrst
use graph structures to represent code (e.g. take into account long range dependencies
between variables) then use gated graph neural networks to learn from code for toy ex-
ample applications such as Varnaming (ﬁnding the best variable name given its usage)
and Varmisuse (predicting which variable to use at which place). In addition to AST, Li
et al. [36] use also program dependence graph and data ﬂow graph. Conversely, Chirkova
et al.[58] study whether transformers architecture are good to process AST instead of
raw code investigating several design choices – testifying that this is still an open area
for future research.

Impact on network conﬁguration. Clearly, appropriate parsers must be used for network
conﬁguration languages as well. Batﬁsh [1] is one popular example to extract a net-
work model from conﬁgurations, which could be leveraged to build the necessary input
12

to train NLP network conﬁguration language models. As Batﬁsh limitedly support a
number of cases/languages, it could be complemented by custom parsers developed as
side-contributions in other work (for example, in their study of network conﬁguration
complexity, Benson et al.[28] leverage the syntax contained in the documentation to
manually create a grammar from router conﬁgurations). Generally speaking, we make
two observations. First, as stressed by Caldwell et al.[59] the main challenge is tied to
the parser maintenance cost, because of frequent changes to conﬁguration languages and
features. Second, network conﬁguration language are syntactically poor, as they contain
a lot of “identiﬁers” that only have local semantic (e.g., constant values, weights, IP
addresses): while pre-processing may help letting the structure of the (relatively simple)
syntax emerge, it may be more diﬃcult to let the tacit structure of (signiﬁcantly more
complex) arbitrarily selected identiﬁers emerge.

4.5. Expected performance
Lessons from computer language ﬁeld. Despite progress is rapidly made, with models able
to solve problems without even training on code-only datasets [42] and as others empower
production-level tools [41], it is diﬃcult to understand, already on the ﬂourishing ﬁeld
of NLP for programming languages, up to which level nowadays models are mature for
real deployment.
Indeed, with the exception of GitHub Copilot [41], most of the use
cases are so far limited to toy examples with limited complexity (e.g., method or variable
naming). Code veriﬁcation and bug detection has been done mainly for simple tasks
such as oﬀ-by-one bugs [35].

To get a concrete sense of the complexity of the tasks that are solved by today
models, the reader can refer to the largest available benchmarks against which the models
are evaluated. For example, one of the largest evaluations of NLP for code related
tasks [42] runs its models on two benchmarks that were made public: the Mostly Basic
Programming Problems (MBPP ) dataset3 and the MathQA4. Despite their relatively
large number (974 and 23914 respectively), the problems remain fairly simple, barely
matching the skills of entry-level programmers. Thus, existing models perform well but
on rather easy tasks, and it thus unclear to project expected performance on more useful
and realistic tasks.

Furthermore, quality of the resulting code is also of not straightforward evaluation
as stressed by Agarwal et al.[60] in the case of code translation. Similarly, Codebleu[61]
proposes a metric to evaluate code synthesis tasks, that aims to be as close as possible to
human ratings for the three tasks of text to code synthesis, translation and reﬁnement.
Thus, more work as [60, 61] seems needed to programmatically evaluate generated soft-
ware, especially when output of tasks will span thousands of lines per task, as opposed
to few lines today.

Impact on network conﬁguration. In reason of the above limits, speculating how much
of this research would beneﬁt realistic network conﬁguration use cases is not an easy
task. Despite great progress on programming language, the performance is still limited
for both synthesis and translation, especially taking into account how elementary the
tasks are.

3https://github.com/google-research/google-research/tree/master/mbpp
4https://math-qa.github.io/

13

Table 2: Summary of data requirements for the diﬀerent network conﬁguration use-cases

Use-case

Data and labels

Availability

Diﬃculty/comment

Synthesis

Spec-to-conf
Text-to-conf

Possible [24]
Diﬃcult [23]

Almost ready for trial
Very hard at present stage

Veriﬁcation

Binary (“bogus” or not)
Segmented (“bogus” parts

Possible

Easy for synthetic errors [50, 32],
harder for real errors. Unsupervised
veriﬁcation does not need labels

Translation Mono-language corpora

Diﬃcult

Need for multiple datasets

From a practical, cultural and historical perspective, networks strives to achieve “four
nines” to “ﬁve nines” reliability [62]. Under this perspective, even an almost perfect
“four nines” ML model (resp. ﬁve nines), i.e., a model that is correct 99.99% (resp.
99.999%) of the times, would still generate 10 (resp. 1) conﬁguration errors per 100k
conﬁguration lines. Otherwise stated, an almost perfect network conﬁguration obtained
via ML synthesis or translation, remains an incorrect conﬁguration: no rational operator
would accept to use it as-is. Additionally, ﬁnding ML-generated conﬁguration bugs can
be more diﬃcult for humans than ﬁnding their own bugs.

Of course, proper veriﬁcation and correction tools could be developed in parallel.
Besides, as rightly argued by Weisz et al.[63], “perfection is may be not required” as
humans and AI could “partner” for code translation and other tasks. This was empirically
shown as well by Austin et al. [42] who demonstrated that chatting with the model in
natural language could help in practice improving the performance on the task. The
ML model could be thus an assistant that eases the work of the engineer in generating
the ﬁrst translation or code, whereas the latter could build on it to generate the ﬁnal
conﬁguration. For this to happen, proper tools are required to inspect, explain and debug
the work of ML models.

5. Conclusion and recommendations

In this paper, we overview recent progress on NLP application to computer languages,
and project its potential impact to the area of automated network conﬁguration. In light
of our analysis, we make the following conclusive observations:

• Generally speaking, the bootstrap cost in terms of amounts of data, as well as
computational power for unsupervised learning, could be aﬀordable but only for
top largest vendors and ISPs. Especially, we assess that data cost primes over the
model training cost: unlike the computer language ﬁeld, where large corpora are
available over the Internet, network conﬁguration examples are scarce. That is to
say, academic community may have a high entry barrier unless a community-wide
eﬀort is made a priori to make a systematic, organized and organic collection.

• Additionally, unlike computer programming language that are semantically rich,
conﬁguration languages are likely to contain a large amount of identiﬁers with local
signiﬁcance (e.g., addresses, interfaces, tunnels, etc.) for which the importance of
structured network topological metadata is likely to be of key importance (similarly
to AST, PDG, DFG graph structures for programming language).

14

We additionally report in Table 2 the requirements for each use case, that we summarize
as follows:

• For what concerns conﬁguration veriﬁcation, we estimate that anomaly detection on
top of learned conﬁguration language models has realistic potential for application.
Indeed, provided that few false positive are generated, even if the error recall rate
is not perfect, automated detection of conﬁguration errors is helpful and may have
a readily practical impact.

• The expected performance for code/conﬁguration synthesis is still elementary given
the use cases tried so far are rather simple. As such conﬁguration synthesis is for
the time being still a “moonshot”, especially that the text/specs from which to
generate the conﬁguration is a large challenge. A second, non lesser, challenge
is reliability: one cannot push to production networks a conﬁguration that might
contain some bugs (a 99.9% correct model still generates 100 bugs for 100K lines
conﬁguration) so that extremely reliable veriﬁcation is a mandatory pre-condition
for signiﬁcant development in this area.

• The same applies but to a lesser extent for conﬁguration translation: current per-
formance of software/code translation between programming languages does not
need labels and is very promising, but the fact that it might contain translation
errors, might make it hard to use in practice today. We expect this use case to be
further developed, conditioned to the success of the previous two.

More broadly, we point out that, beyond the network conﬁguration use cases discussed
in this paper, several networking problems [6, 7, 8, 9, 10] are already beneﬁting from NLP
techniques. As such, a growing research trend is expected to emerge in the community,
increasing the availability, spread and knowledge of NLP tools for networking in general.
Overall, we believe that given the amount of work that has been done on programming
languages, and given the reach of NLP to other networking use-cases already, it is only a
matter of time before these techniques are actively researched, and successfully applied,
onto network conﬁguration languages.

At the same time, we want to express a word of caution — while network languages
will largely beneﬁt from progress on NLP for programming languages, the two ﬁelds are
largely diﬀerent. For the computer programming languages, some argue that “Auto-
mated AI tools will work hand in hand with software developers. The focus of software
engineering will move from writing code from scratch to reviewing code written and
tested by AI.” (Thomas Zimmermann) [64]. For the network conﬁguration case, as long
as NLP tools are not perfect, manual checking of large volumes of automatically gen-
erated conﬁguration to spot rare yet critical errors generated by NLP tools seems less
appealing. Having this diﬀerence clear in mind, can help devising a successful research
agenda.

References

[1] A. Fogel, S. Fung, L. Pedrosa, M. Walraed-Sullivan, R. Govindan, R. Mahajan, T. Millstein, A
general approach to network conﬁguration analysis, in: 12th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 15), 2015, pp. 469–483.

15

[2] https://xkcd.com/927/.
[3] G. Lample, A. Conneau, L. Denoyer, M. Ranzato, Unsupervised machine translation using mono-

lingual corpora only, arXiv preprint arXiv:1711.00043 (2017).

[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al., Language models are few-shot learners, Advances in neural information
processing systems (NeurIPS) 33 (2020) 1877–1901.

[5] https://openai.com/blog/gpt-3-apps/.
[6] L. Gioacchini, L. Vassio, M. Mellia, I. Drago, Z. Ben Houidi, D. Rossi, Darkvec: Automatic analysis

of darknet traﬃc with word embeddings, in: ACM CoNEXT, 2021.

[7] M. Ring, A. Dallmann, D. Landes, A. Hotho, Ip2vec: Learning similarities between IP addresses, in:

IEEE International Conference on Data Mining Workshops (ICDMW), IEEE, 2017, pp. 657–666.

[8] D. Cohen, Y. Mirsky, M. Kamp, T. Martin, Y. Elovici, R. Puzis, A. Shabtai, Dante: A framework
for mining and monitoring darknet traﬃc, in: European Symposium on Research in Computer
Security, Springer, 2020, pp. 88–109.

[9] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, Y. Bengio, Graph attention networks,

in: International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=rJXMpikCZ

[10] W. Kool, H. van Hoof, M. Welling, Attention, learn to solve routing problems!, arXiv:1803.08475

(2018).

[11] S. Prabhu, K. Y. Chou, A. Kheradmand, B. Godfrey, M. Caesar, Plankton: Scalable network
conﬁguration veriﬁcation through model checking, in: 17th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 20), 2020, pp. 953–967.

[12] K. Weitz, D. Woos, E. Torlak, M. D. Ernst, A. Krishnamurthy, Z. Tatlock, Scalable veriﬁcation of
border gateway protocol conﬁgurations with an smt solver, in: Proceedings of the 2016 acm sigplan
international conference on object-oriented programming, systems, languages, and applications,
2016, pp. 765–780.

[13] R. Beckett, A. Gupta, R. Mahajan, D. Walker, A general approach to network conﬁguration veriﬁca-
tion, in: Proceedings of the Conference of the ACM Special Interest Group on Data Communication,
2017, pp. 155–168.

[14] B. Tian, X. Zhang, E. Zhai, H. H. Liu, Q. Ye, C. Wang, X. Wu, Z. Ji, Y. Sang, M. Zhang, et al.,
Safely and automatically updating in-network acl conﬁgurations with intent language, in: ACM
SIGCOMM, 2019, pp. 214–226.

[15] A. Gember-Jacobson, R. Viswanathan, A. Akella, R. Mahajan, Fast control plane analysis using
an abstract representation, in: Proceedings of the 2016 ACM SIGCOMM Conference, 2016, pp.
300–313.

[16] A. Feldmann, Netdb: Ip network conﬁguration debugger/database”, in: AT&T Software Sympo-

sium, Citeseer, 1999.

[17] S. Steﬀen, T. Gehr, P. Tsankov, L. Vanbever, M. Vechev, Probabilistic veriﬁcation of network
conﬁgurations, in: Proceedings of the Annual conference of the ACM Special Interest Group on
Data Communication on the applications, technologies, architectures, and protocols for computer
communication, 2020, pp. 750–764.

[18] A. Abhashkumar, A. Gember-Jacobson, A. Akella, Tiramisu: Fast multilayer network veriﬁcation,
in: 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20), 2020,
pp. 201–219.

[19] R. Beckett, R. Mahajan, T. Millstein, J. Padhye, D. Walker, Network conﬁguration synthesis with
abstract topologies, in: Proceedings of the 38th ACM SIGPLAN Conference on Programming
Language Design and Implementation, 2017, pp. 437–451.

[20] A. El-Hassany, P. Tsankov, L. Vanbever, M. Vechev, {NetComplete}: Practical {Network-Wide}
conﬁguration synthesis with autocompletion, in: 15th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 18), 2018, pp. 579–594.

[21] A. El-Hassany, P. Tsankov, L. Vanbever, M. Vechev, Network-wide conﬁguration synthesis, in:

International Conference on Computer Aided Veriﬁcation, Springer, 2017, pp. 261–281.

[22] H. H. Liu, X. Wu, W. Zhou, W. Chen, T. Wang, H. Xu, L. Zhou, Q. Ma, M. Zhang, Automatic
life cycle management of network conﬁgurations, in: Proceedings of the Afternoon Workshop on
Self-Driving Networks, 2018, pp. 29–35.

[23] R. Birkner, D. Drachsler-Cohen, L. Vanbever, M. Vechev, {Net2Text}:{Query-Guided} summariza-
tion of network forwarding behaviors, in: 15th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 18), 2018, pp. 609–623.

[24] R. Birkner, D. Drachsler-Cohen, L. Vanbever, M. Vechev, {Conﬁg2Spec}: Mining network speciﬁ-
16

cations from network conﬁgurations, in: 17th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 20), 2020, pp. 969–984.

[25] L. Gao, J. Rexford, Stable internet routing without global coordination, IEEE/ACM Transactions

on networking 9 (6) (2001) 681–692.

[26] J. Yen, T. L´evai, Q. Ye, X. Ren, R. Govindan, B. Raghavan, Semi-automated protocol disambigua-
tion and code generation, in: Proceedings of the 2021 ACM SIGCOMM 2021 Conference, 2021, pp.
272–286.

[27] Z. B. Houidi, A knowledge-based systems approach to reason about networking, in: Proceedings of

the 15th ACM Workshop on Hot Topics in Networks, 2016, pp. 22–28.

[28] T. Benson, A. Akella, D. A. Maltz, Unraveling the complexity of network management., in: NSDI,

2009, pp. 335–348.

[29] H. Kim, T. Benson, A. Akella, N. Feamster, The evolution of network conﬁguration: A tale of
two campuses, in: Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement
conference, 2011, pp. 499–514.

[30] S. Lee, T. Wong, H. S. Kim, To automate or not to automate: on the complexity of network
conﬁguration, in: 2008 IEEE International Conference on Communications, IEEE, 2008, pp. 5726–
5731.

[31] A. Habib, M. Pradel, Neural bug ﬁnding: A study of opportunities and challenges, arXiv preprint

arXiv:1906.00307 (2019).

[32] C. Richter, H. Wehrheim, Deepmutants: Training neural bug detectors with contextual mutations,

arXiv preprint arXiv:2107.06657 (2021).

[33] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, P. Isola, Rethinking few-shot image classiﬁcation:
a good embedding is all you need?, in: Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16, Springer, 2020, pp. 266–282.

[34] U. Alon, M. Zilberstein, O. Levy, E. Yahav, code2vec: Learning distributed representations of code,

Proceedings of the ACM on Programming Languages 3 (POPL) (2019) 1–29.

[35] J. A. Briem, J. Smit, H. Sellik, P. Rapoport, Using distributed representation of code for bug

detection, arXiv preprint arXiv:1911.12863 (2019).

[36] Y. Li, S. Wang, T. N. Nguyen, S. Van Nguyen, Improving bug detection via context-based code rep-
resentation learning and attention-based neural networks, Proceedings of the ACM on Programming
Languages 3 (OOPSLA) (2019) 1–30.

[37] T. Mikolov, K. Chen, G. Corrado, J. Dean, Eﬃcient estimation of word representations in vector

space, arXiv preprint arXiv:1301.3781 (2013).

[38] A. Grover, J. Leskovec, node2vec: Scalable feature learning for networks, in: Proceedings of the
22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp.
855–864.

[39] M. Agarwal, T. Chakraborti, Q. Fu, D. Gros, X. V. Lin, J. Maene, K. Talamadupula, Z. Teng,
J. White, Neurips 2020 nlc2cmd competition: Translating natural language to bash commands,
arXiv preprint arXiv:2103.02523 (2021).

[40] W. U. Ahmad, S. Chakraborty, B. Ray, K.-W. Chang, Uniﬁed pre-training for program understand-

ing and generation, arXiv preprint arXiv:2103.06333 (2021).

[41] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, et al., Evaluating large language models trained on code, arXiv preprint
arXiv:2107.03374 (2021).

[42] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,
Q. Le, et al., Program synthesis with large language models, arXiv preprint arXiv:2108.07732 (2021).
[43] J. Yen, R. Govindan, B. Raghavan, Tools for disambiguating rfcs, in: Proceedings of the Applied

Networking Research Workshop, 2021, pp. 85–91.

[44] X. Chen, C. Liu, D. Song, Tree-to-tree neural networks for program translation, arXiv preprint

arXiv:1802.03691 (2018).

[45] B. Roziere, M.-A. Lachaux, L. Chanussot, G. Lample, Unsupervised translation of programming

languages, Advances in Neural Information Processing Systems 33 (2020).

[46] B. Roziere, J. M. Zhang, F. Charton, M. Harman, G. Synnaeve, G. Lample, Leveraging automated

unit tests for unsupervised code translation, arXiv preprint arXiv:2110.06773 (2021).

[47] B. Roziere, M.-A. Lachaux, M. Szafraniec, G. Lample, Dobf: A deobfuscation pre-training objective

for programming languages, arXiv preprint arXiv:2102.07492 (2021).

[48] A. Elnaggar, W. Ding, L. Jones, T. Gibbs, T. Feher, C. Angerer, S. Severini, F. Matthes, B. Rost,
Codetrans: Towards cracking the language of silicon’s code through self-supervised deep learning
and high performance computing, arXiv preprint arXiv:2104.02443 (2021).

17

[49] M. Pradel, S. Chandra, Neural software analysis, arXiv preprint arXiv:2011.07986 (2020).
[50] M. Yasunaga, P. Liang, Break-it-ﬁx-it: Unsupervised learning for program repair, arXiv preprint

arXiv:2106.06600 (2021).

[51] D. Peng, S. Zheng, Y. Li, G. Ke, D. He, T.-Y. Liu, How could neural networks understand pro-

grams?, arXiv preprint arXiv:2105.04297 (2021).

[52] S. Chakraborty, R. Krishna, Y. Ding, B. Ray, Deep learning based vulnerability detection: Are we

there yet, IEEE Transactions on Software Engineering (2021).

[53] V. J. Hellendoorn, A. A. Sawant, The growing cost of deep learning for source code, Communications

of the ACM 65 (1) (2021) 31–33.

[54] C. Li, Lambda labs, https://lambdalabs.com/blog/demystifying-gpt-3/, [Online; accessed 05-

April-2022] (June 2020).

[55] A. A. Sawant, P. Devanbu, Naturally!: How breakthroughs in natural language processing can

dramatically help developers, IEEE Software 38 (5) (2021) 118–123.

[56] X. V. Lin, C. Wang, L. Zettlemoyer, M. D. Ernst, Nl2bash: A corpus and semantic parser for
natural language interface to the linux operating system, arXiv preprint arXiv:1802.08979 (2018).
[57] M. Allamanis, M. Brockschmidt, M. Khademi, Learning to represent programs with graphs, arXiv

preprint arXiv:1711.00740 (2017).

[58] N. Chirkova, S. Troshin, Empirical study of transformers for source code, in: Proceedings of the
29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2021, pp. 703–715.

[59] D. Caldwell, S. Lee, Y. Mandelbaum, Adaptive parsing of router conﬁguration languages, in: 2008

IEEE Internet Network Management Workshop (INM), IEEE, 2008, pp. 1–6.

[60] M. Agarwal, K. Talamadupula, S. Houde, F. Martinez, M. Muller, J. Richards, S. Ross, J. D. Weisz,
Quality estimation & interpretability for code translation, arXiv preprint arXiv:2012.07581 (2020).
[61] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan, M. Zhou, A. Blanco, S. Ma,
Codebleu: a method for automatic evaluation of code synthesis, arXiv preprint arXiv:2009.10297
(2020).

[62] B. Treynor, M. Dahlin, V. Rau, B. Beyer, The calculus of service availability, Commun. ACM 60 (9)

(2017) 42–47. doi:10.1145/3080202.

[63] J. D. Weisz, M. Muller, S. Houde, J. Richards, S. I. Ross, F. Martinez, M. Agarwal, K. Tala-
madupula, Perfection not required? human-ai partnerships in code translation, in: 26th Interna-
tional Conference on Intelligent User Interfaces, 2021, pp. 402–412.

[64] https://www.acm.org/articles/people-of-acm/2022/thomas-zimmermann.

18

