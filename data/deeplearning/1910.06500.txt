0
2
0
2

l
u
J

4
1

]
E
N
.
s
c
[

2
v
0
0
5
6
0
.
0
1
9
1
:
v
i
X
r
a

DeepVS: An Eﬃcient and Generic Approach for Source
Code Modeling Usage

Yasir Hussaina,∗, Zhiqiu Huanga,b,c,∗, Yu Zhoua, Senzhang Wanga

aCollege of Computer Science and Technology, Nanjing University of Aeronautics and
Astronautics (NUAA), Nanjing 211106, China
bKey Laboratory of Safety-Critical Software, NUAA, Ministry of Industry and Information
Technology, Nanjing 211106, China
cCollaborative Innovation Center of Novel Software Technology and Industrialization,
Nanjing 210093, China

Abstract

The source code suggestions provided by current IDEs are mostly dependent
on static type learning. These suggestions often end up proposing irrelevant
suggestions for a peculiar context. Recently, deep learning-based approaches
have shown great potential in the modeling of source code for various software
engineering tasks. However, these techniques lack adequate generalization and
resistance to acclimate the use of such models in a real-world software develop-
ment environment. This letter presents DeepVS, an end-to-end deep neural code
completion tool that learns from existing codebases by exploiting the bidirec-
tional Gated Recurrent Unit (BiGRU) neural net. The proposed tool is capable
of providing source code suggestions instantly in an IDE by using pre-trained
BiGRU neural net. The evaluation of this work is two-fold, quantitative and
qualitative. Through extensive evaluation on ten real-world open-source soft-
ware systems, the proposed method shows signiﬁcant performance enhancement
and its practicality. Moreover, the results also suggest that DeepVS tool is ca-
pable of suggesting zero-day (unseen) code tokens by learning coding patterns
from real-world software systems.

1. Introduction

Source code suggestion and completion are vital features of an integrated
development environment (IDE). Software developers excessively rely on such
features while developing software. Most of the modern IDEs source code com-
pletion tools [3] provide the next possible source code by leveraging the informa-
tion already exists in the code editor of the IDE. Due to the limited historical

∗Corresponding author
Email addresses: yaxirhuxxain@nuaa.edu.cn (Yasir Hussain), zqhuang@nuaa.edu.cn
(Zhiqiu Huang), zhouyu@nuaa.edu.cn (Yu Zhou), szwang@nuaa.edu.cn (Senzhang Wang)

Preprint submitted to Arxiv

 
 
 
 
 
 
data available in the code editor, the IDE may not be able to provide the correct
suggestions or possibly torment the developer with irrelevant suggestions.

Recently, deep learning techniques have been widely applied for various
source code modeling tasks such as source code completion [8, 10], error ﬁx-
ing [5, 9], method naming [1], etc. These approaches [8, 10] have shown how
they can signiﬁcantly improve such tools by learning from historical codebases
(e.g. GitHub). However, there are various limitations and concerns:

• Most of these techniques lack to capture long-term context dependencies

of source code resulting in subordinate performance.

• Most techniques are usually formed from recognized natural events and
therefore, are incapable of suggesting zero-day (unseen) code tokens.

• Most methods require a huge amount of computation power as well as

time to train such models.

• Current techniques lack to illustrate, how they can assist software devel-

opers in a real-world software development environment.

• The existing works present no enlightenment, how these models can be

integrated into an IDE.

To defy the above apprehensions, we propose a neural code completion tool
named DeepVS. The proposed tool leverages the bi-directional neural gating
mechanism to learn coding patterns from existing software systems (codebase)
resulting in a signiﬁcant performance boost. This letter makes the subsequent
contributions:

• An end-to-end neural code completion tool trained on ten real-world open-

source software systems with over 13 million code tokens.

• The projected method is capable of providing suggestions in real-time
directly in an IDE. Further, DeepVS tool is capable of providing zero-day
code tokens. The DeepVS tool is publicly available at https://github.
com/yaxirhuxxain/DeepVS/.

• The quantitative evaluation with ten real-world open-source software sys-
tems and qualitative analysis of DeepVS tool veriﬁes that the projected
method is accurate and subordinate the existing approaches.

2. Approach

The projected method begins by preprocessing the codebase. First, we per-
form standardization to obtain the optimal feature set. To learn real-world
coding patterns, we tokenize each source code ﬁle in the codebase into multi-
ple sequences of ﬁxed context. Next, for the purpose of neural language model
training, we vectorize these extracted contexts. Finally, we train and test the
BiGRU classiﬁer for the code completion task.

2

3. Problem Statement

Given a source code ﬁle, the predictions for the next source code token
could be deﬁned as: Y = f (τ ) where Y represents the set of next source code
suggestions, f represents the classiﬁcation function that predicts the next pos-
sible source code tokens and τ represents the context which is an input to the
classiﬁcation function. We use the source code of real-world software systems
extracted from GitHub1 to build our codebase for the purpose of neural model
training and testing. The collected codebase (CB) can be formalized as: CB =<
SS1, SS2, ..., SSi > and SSi =< cf1, cf2, ..., cfj >, where SS1, SS2, ..., SSi rep-
resents the i software systems involved in CB and cf1, cf2, ..., cfj represents the
j code ﬁles involved in ith software systems.

4. Preprocessing and Feature Extraction

Preprocessing is a vital process in building neural language models. Follow-
ing grassroots, we perform standardization, context extraction, and vectoriza-
tion.

5. Standardization

To standardize CB, we remove all blank lines, inline and block-level com-
ments from each software system. Further, we replace all literal values to their
generic data types to build the optimum feature set.

6. Context Extraction

To learn code patterns from historical software systems, each source code
ﬁle in the codebase is parted into a sequence of space-separated tokens. These
sequences are then subdivided into multiple sequences of ﬁxed context (τ ).

7. Vectorization

To train the neural language model, we need to build a global vocabulary
system. For this intend, we replace all singleton code tokens with a special
token (”UNK”). Next, we build the vocabulary V =< v1, v2, ..., vk > where
v1, v2, ..., vk represents k unique source code token corresponds to an entry in the
vocabulary. Then, each code token is replaced with its corresponding vocabulary
index (positive integer) to convert context vectors into a form that is suitable for
neural language model training. The vectorized source code can be expressed
as: SSv
i =< τ1, τ2, ..., τk > where τ1, τ2, ..., τk represents the k number of context
vectors found in ith software systems.

1https://www.github.com/ , accessed on 1/12/2019.

3

8. Bi-Directional Context Learner

We adopt GRU [4] which is a superior type of recurrent neural network
(RNN) for the purpose of code completion classiﬁcation. Conventional RNN
suﬀers from fading gradient issue, limiting it from learning long-term context
dependencies. Similar to LSTM, the GRU uses gating approach that exposes
full hidden context on each time step i, which advances it to learn long-term
context dependencies by an excessive edge. It is composed of two gates, the rest
gate ri and the update gate zi. It can be expressed as

hi = (1 − zi)hi−1 + zi¯hi

Where h and ¯h is prior context and fresh context respectively.

zi = φ(Wzτi + Uzhi−1)

¯hi = tanh(W τi + ri ⊗ U hi−1)

ri = φ(Wrτi + Urhi−1)

(1)

(2)

(3)

(4)

The ¯h is modulated by the reset gates ri. Here ⊗ is element-wise multiplication
and φ is the softmax activation function which can be expressed as

S(Y ) =

eY
j eY j

(cid:80)

(5)

We utilize GRU in a bi-directional fashion to eﬀectively learn the source
code context bidirectionally. Further, we expect the neural learner to assign
high probability to the correct source code suggestions by having low cross-
entropy that can be expressed as

Hp(C) ≈ −

1
N

N
(cid:88)

i=1

log2 P (τ i|τ i−1

i−n+1)

(6)

9. Tool Implementation

The trained model is saved, in order to use the model in our proposed tool.
The main ingredient of this work is to serve the pre-trained BiGRU locally or
over a cloud and use a plugin interface for real-time classiﬁcation. Fig. 1 shows
the overall workﬂow of the proposed approach where the oﬄine part shows the
steps involved in the pre-training BiGRU classiﬁer and the online part illustrates
the tool implementation details.

4

Figure 1: Overall workﬂow of DeepVS.

10. Neural Server

A cloud-based neural server is built to serve the pre-trained BiGRU neural
language model as a centralized attendant to determine the likelihood scores for
the next source code suggestions. In this work, the cloud platform [2] is selected
because of its centralized nature and scalability. The neural cloud server takes
source code context (τ ) as input and provides most likely next source code
suggestions (Y ) by consulting the pre-trained BiGRU classiﬁer.

11. Deep Suggestion Engine

As a proof of concept and demonstration purposes, we implement the plugin
interface in Visual Studio Code IDE. The DeepVS tool can be triggered by using
shortcut key CTRL+SPACE. By triggering DeepVS, it takes the source code
prior to the current cursor position as context (τ ) and requests Neural Server
for classiﬁcation Y = f (τ ).

12. Evaluation

The evaluation of this work is two-fold, quantitative and qualitative.
In
quantitative evaluation, we assess the classiﬁcation outcomes to test the perfor-
mance of the proposed method and compare it with state-of-the-art methods.
Metrics used to assess the quantitative eﬃciency of this work are top-k ac-
curacy and mean reciprocal rank (MRR), which are mostly applied. For the
qualitative evaluation of the work, we conduct an experimental study involving

5

Visual Studio Code IDECode BaseStandardizationContext ExtractionVectorizationBi-Directional Context LearnerTraining SetTesting SetBackpropagation Update WeightsTrained ModelSource Code SuggestionsRequest Suggestions       ...   .    ...      . ... ..     ..  Neural ServerIDE s Plugin InterfaceNeural Language Model Training Phase (Offline)Tool Implementation (Online)Questions

Table 1: Questionnaire used for evaluation.

Q1: How easy it is to set up and use the DeepVS tool?
Q2: Does DeepVS tool provides suggestions in real-world coding environment?
Q3: Does DeepVS tool performs better than IDE’s default suggestion engine?
Q4: I will recommend DeepVS to others.

seven university students having software development experience. The experi-
ment was performed with the help of a questionnaire. All the participants were
demonstrated with the basic usage of the tool. The questionnaire given to the
participants is shown in Table 1.

13. Dataset

For the training and testing of the proposed method, we utilize the codebase
anticipated in [6, 7]. The codebase consists of ten open-source software systems
(ant, cassandra, db40, jgit, poi, batik, antlr, itext, jts, maven) containing over
13M code tokens with a large vocabulary of size 145,457 (singletons removed).
Table 2 shows the statistics of model training and testing sets in detail.

Table 2: Code database statistics
Line of Code (LoC) Total Code Tokens Vocab Size

Training
Testing

Total

1,684,158
187,129

1,871,287

12,086,347
1,342,927

13,429,274

145,457

14. Experimental Setup and Results

: The neural language model and cloud platform are build using Python 3.6.
The training of neural language models is done by using tensorﬂow4 v1.14 on
Intel(R) Xeon(R) Silver 4110 CPU 2.10GHz with 32 cores and 128GB of ram
running Ubuntu 18.04.2 LTS operating system, equipped with NVIDIA RTX
2080 GPU. The neural language model training is oﬄine, thus has no impact
on classiﬁcation time. The pre-trained BiGRU model is used for classiﬁcation
purpose in neural server. The Neural Serve consists of a single core Intel CPU
with 1GB Ram running Ubuntu-18 x64-bit non-GUI edition. The average clas-
siﬁcation time of the proposed approach is less than 20 milliseconds which is
fairly adequate and acceptable.

15. Quantitative Results

In order to realistically evaluate the eﬀectiveness of the proposed method,
we compare the performance of our proposed method with other state-of-the-art

6

[6] and White et al.

[10] work as baselines
methods. We choose Hindle el al.
because they are related to our target task. The comprehensive results are given
in Table 3. It can be witnessed that the proposed method proves noticeably
better performance as compared to other baselines. We can observe that the
proposed approach achieves 22.47% higher accuracy as compared to the best
baseline(RNN) by ranking the correct suggestions on the ﬁrst index(Top-1).
Fig. 2 and Fig. 3 represents the accuracy and loss of training and validation
accordingly.

Figure 2: Training and validation accuracy comparison.

Figure 3: Training and validation loss comparison.

16. Qualitative Results

As reported in Fig. 4, participants found the DeepVS tool is easy to set
up and use. In Q2, all participants have agreed that DeepVS tool is capable
of providing suggestions in a real-world development environment. In Q3, out
of seven participants three voted strongly agree, three of them voted agree and
one voted neutral. In Q4, most of the participants have agreed that they would

7

Table 3: Accuracy and MRR scores comparison.

Model

Accuracy

MRR

Top-1

Top-3

Top-5

Top-10

N-gram [6]
RNN [10]
BiGRU

61.78% 0.535
59.87%
48.47%
76.21% 0.596
72.18%
51.30%
73.77% 84.39% 86.97% 89.34% 0.792

57.68%
67.67%

recommend DeepVS to their peers. The illustrative examples of source code
suggestion tasks are provided in Section 17.

Figure 4: Questionnaire result.

17. DeepVS Tool Demo

To show the eﬀectiveness of DeepVS tool for real-world usage, consider the
example presented in Fig. 5 where a software developer is writing a complex pro-
gram to ﬁnd the transpose of a matrix. Hereby triggering (line 9 ) our DeepVS
tool it suggests the most probable next source code tokens i, j and k where
the Visual Studio Code’s default suggestion engine fails to provide any relevant
suggestions instead, it torments the developer with irrelevant suggestions. Later
on, at line 21 the software developer is writing a print statement. Here the most
probable next source code token is column based on the given context. Hereby
triggering our DeepVS tool it suggests the correct next source code token col-
umn at its ﬁrst index whereas the IDEs default source code suggestion tool
ranks the correct suggestion on its third index. From the given examples, we
can observe that the DeepVS tool is capable of suggesting zero-day code tokens
and have a better understanding of context while providing suggestions.

8

Figure 5: Source code suggestion example with and without the DeepVs tool along with their
probabilities.

Figure 6: Another Source code suggestion example along with their probabilities.

9

18. Proposed Work’s Major Beneﬁts

The qualitative and quantitative evaluation veriﬁes the feasibility and prac-
ticality of this work. We summarize the beneﬁts of our proposed framework and
the DeepVS tool as follows:

• To the best of our knowledge, the proposed approach is the ﬁrst one that
enables the usage of machine/deep learning-based source code models di-
rectly in an IDE.

• We present an end-to-end deep neural code completion tool named DeepVS

which veriﬁes the practicality of the proposed approach.

• The DeepVS tool is trained on ten real-world open-source software systems
with over 13M code tokens, thus capable of suggesting zero-day (unseen)
code tokens by leveraging large scale historical codebase.

• Further, DeepVS tool has better understanding of context while providing
suggestions. The proposed tool ranks the suggestions eﬀectively without
tormenting the developer with unnecessary suggestion.

19. Conclusion

In this letter, we presented DeepVS, an end-to-end deep neural code comple-
tion tool that leverages from the pre-trained BiGRU classiﬁer to provide code
suggestions directly in an IDE in real-time. The proposed tool is trained and
tested on real-world software systems with over 13M code tokens. Further, we
have demonstrated the eﬀectiveness and practicality of the proposed tool in a
real-world use case(IDE). Moreover, the approach illustrated in this work is gen-
eral and can help enable the usage of neural language models for other source
code modeling tasks directly in an IDE.

Acknowledgments

This work was supported by the National Key R&D (grant no. 2018YFB1003902),

Natural Science Foundation of Jiangsu Province (No. BK20170809), National
Natural Science Foundation of China (No. 61972197) and Qing Lan Project.

References

[1] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.

code2vec:
Learning distributed representations of code. Proceedings of the ACM on
Programming Languages, 3(POPL):40, 2019.

[2] Michael Armbrust, Armando Fox, Rean Griﬃth, Anthony D Joseph, Randy
Katz, Andy Konwinski, Gunho Lee, David Patterson, Ariel Rabkin, Ion
Stoica, et al. A view of cloud computing. Communications of the ACM, 53
(4):50–58, 2010.

10

[3] Marcel Bruch, Martin Monperrus, and Mira Mezini. Learning from exam-
ples to improve code completion systems. In Proceedings of the 7th joint
meeting of the European software engineering conference and the ACM
SIGSOFT symposium on the foundations of software engineering, pages
213–222, 2009.

[4] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bah-
danau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning
phrase representations using rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078, 2014.

[5] Rahul Gupta, Aditya Kanade, and Shirish Shevade. Deep reinforce-
arXiv preprint

ment learning for programming language correction.
arXiv:1801.10467, 2018.

[6] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar
Devanbu. On the naturalness of software. In 2012 34th International Con-
ference on Software Engineering (ICSE), pages 837–847. IEEE, 2012.

[7] Anh Tuan Nguyen, Trong Duc Nguyen, Hung Dang Phan, and Tien N
Nguyen. A deep neural network language model with contexts for source
code. In 2018 IEEE 25th International Conference on Software Analysis,
Evolution and Reengineering (SANER), pages 323–334. IEEE, 2018.

[8] Veselin Raychev, Martin Vechev, and Eran Yahav. Code completion with
In Acm Sigplan Notices, volume 49, pages

statistical language models.
419–428. ACM, 2014.

[9] Eddie Antonio Santos, Joshua Charles Campbell, Dhvani Patel, Abram
Hindle, and Jos´e Nelson Amaral. Syntax and sensibility: Using language
models to detect and correct syntax errors. In 2018 IEEE 25th International
Conference on Software Analysis, Evolution and Reengineering (SANER),
pages 311–322. IEEE, 2018.

[10] Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys
Poshyvanyk. Toward deep learning software repositories. In Proceedings
of the 12th Working Conference on Mining Software Repositories, pages
334–345. IEEE Press, 2015.

11

