8
1
0
2

p
e
S
5
2

]

G
L
.
s
c
[

1
v
1
0
5
9
0
.
9
0
8
1
:
v
i
X
r
a

European Workshop on Reinforcement Learning 14 (2018)

October 2018, Lille, France.

Anderson Acceleration for Reinforcement Learning

Matthieu Geist
Universit´e de Lorraine, CNRS, LIEC, F-57000 Metz, France
(Now at Google Brain)

matthieu.geist@univ-lorraine.fr

Bruno Scherrer
Universit´e de Lorraine, CNRS, Inria, IECL, F-54000 Nancy, France

bruno.scherrer@inria.fr

Abstract

Anderson (1965) acceleration is an old and simple method for accelerating the computation
of a ﬁxed point. However, as far as we know and quite surprisingly, it has never been
applied to dynamic programming or reinforcement learning.
In this paper, we explain
brieﬂy what Anderson acceleration is and how it can be applied to value iteration, this
being supported by preliminary experiments showing a signiﬁcant speed up of convergence,
that we critically discuss. We also discuss how this idea could be applied more generally
to (deep) reinforcement learning.

Keywords: Reinforcement learning; accelerated ﬁxed point.

1. Introduction

Reinforcement learning (RL) (Sutton and Barto, 1998) is intrinsically linked to ﬁxed-point
computation: the optimal value function is the ﬁxed point of the (nonlinear) Bellman
optimality operator, and the value function of a given policy is the ﬁxed point of the
related (linear) Bellman evaluation operator. Most of the time, these ﬁxed points are
computed recursively, by applying repeatedly the operator of interest. Notable exceptions
are the evaluation step of policy iteration and the least-squares temporal diﬀerences (LSTD)
algorithm1 (Bradtke and Barto, 1996).

Anderson (1965) acceleration (also known as Anderson mixing, Pulay mixing, direct
inversion on the iterative subspace or DIIS, among others2) is a method that allows speeding
up the computation of such ﬁxed points. The classic ﬁxed-point iteration applies repeatdly
the operator to the last estimate. Anderson acceleration considers the m previous estimates.
Then, it searches for the point that has minimal residual within the subspace spanned by
these estimates, and applies the operator to it. This approach has been successfully applied
to ﬁelds such as electronic structure computation or computational chemistry, but it has
never been applied to dynamic programming or reinforcement learning, as far as we know.
For more about Anderson acceleration, refer to Walker and Ni (2011), for example.

1. In the realm of deep RL, as far as we know, all ﬁxed points are computed iteratively, there is no LSTD.
2. Anderson acceleration and variations have been rediscovered a number of times in various communities in
the last 50 years. Walker and Ni (2011) provide a brief overview of these methods, and a close approach
has been recently proposed in the machine learning community (Scieur et al., 2016).

c(cid:13)2018 Matthieu Geist and Bruno Scherrer.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.

 
 
 
 
 
 
Geist and Scherrer

2. Anderson Acceleration for Value Iteration

In this section, we brieﬂy review Markov decision processes, value iteration, and show how
Anderson acceleration can be used to speed up convergence.

2.1 Value Iteration

Let ∆X be the set of probability distributions over a ﬁnite set X and Y X the set of ap-
plications from X to the set Y . By convention, all vectors are column vectors. A Markov
Decision Process (MDP) is a tuple {S, A, P, R, γ}, where S is the ﬁnite state space, A is
the ﬁnite action space, P ∈ (∆S)S×A is the Markovian transition kernel (P (s(cid:48)|s, a) denotes
the probability of transiting to s(cid:48) when action a is applied in state s), R ∈ RS×A is the
bounded reward function (R(s, a) represents the local beneﬁt of doing action a in state s)
and γ ∈ (0, 1) is the discount factor.

A stochastic policy π ∈ (∆A)S associates a distribution over actions to each state (de-
terministic policies being a special case of this). The policy-induced reward and transition
kernels, Rπ ∈ RS and Pπ ∈ (∆S)S, are deﬁned as

Rπ(s) = Eπ(.|s)[R(s, A)] and Pπ(s(cid:48)|s) = Eπ(.|s)[P (s(cid:48)|s, A)].

The quality of a policy is quantiﬁed by the associated value function vπ ∈ RS:

vπ(s) = E[

(cid:88)

t≥0

γtRπ(St)|S0 = s, St+1 ∼ Pπ(.|St)].

The value vπ is the unique ﬁxed point of the Bellman operator Tπ, deﬁned as Tπv =
Rπ + γPπv for any v ∈ RS.

Let deﬁne the second Bellman operator T as, for any v ∈ RS, T v = maxπ∈(∆A)S Tπv.

This operator is a γ-contraction (in supremum norm), so the iteration

vk+1 = T vk

converges to its unique ﬁxed-point v∗ = maxπ vπ, for any v0 ∈ RS. This is the value iteration
algorithm.

2.2 Accelerated Value Iteration

Anderson acceleration is a method that aims at accelerating the computation of the ﬁxed
point of any operator. Here, we describe it considering the Bellman operator T , which
provides an accelerated value iteration algorithm.

Assume that estimates have been computed up to iteration k, and that in addition to
vk the m previous estimates vk−1, . . . , vk−m are known. The coeﬃcient vector αk+1 ∈ Rm+1
is deﬁned as follows:

αk+1 = argmin
α∈Rm+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m
(cid:88)

i=0

αi(T vk−m+i − vk−m+i)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

s.t.

m
(cid:88)

i=0

αi = 1.

Notice that we don’t impose a positivity condition on the coeﬃcients. We will consider
practically the (cid:96)2-norm for this problem, but it could be a diﬀerent norm (for example (cid:96)1 or

2

Anderson Acceleration for Reinforcement Learning

(cid:96)∞, in which case the optimization problem is a linear program). Then, the new estimate
is given by:

m
(cid:88)

vk+1 =

αk+1
i

T vk−m+i.

i=0

The resulting Anderson accelerated value iteration is summarized in Alg. 1. Notice that the
solution to the optimization problem can be obtained analytically for the (cid:96)2-norm, using
the Karush-Kuhn-Tucker conditions. With the notations of Alg. 1 and writting 1 ∈ Rmk+1
the vector with all components equal to one, it is

αk+1 =

(∆(cid:62)
1(cid:62)(∆(cid:62)

k ∆k)−11
k ∆k)−11

.

(1)

This can be regularized to avoid ill-conditioning.

Algorithm 1: Anderson Accelerated Value Iteration

given: v0 and m ≥ 1
Compute v1 = T v0;
for k = 1, 2, 3 . . . do

Set mk = min(m, k);
Set ∆k = [δk−mk , . . . , δk] ∈ RS×(m+1) with δi = T vi − vi ∈ RS;
Solve minα∈Rm+1 (cid:107)∆kα(cid:107) s.t. (cid:80)mk
Set vk+1 = (cid:80)mk

i=0 αk = 1;

i=0 αiT vk−mk+i;

The rationale of this acceleration scheme is better understood with an aﬃne operator.
We consider here the Bellman evaluation operator Tπ. Given the current and the m previous
estimates, deﬁne

m
(cid:88)

˜vα
k+1 =

αivk−m+i with

αi = 1.

m
(cid:88)

Thanks to this constraint, for an aﬃne operator (here Tπ), we have that

i=0

i=0

Tπ ˜vα

k+1 =

m
(cid:88)

i=0

αiTπvk−m+i.

Then, one searches for a vector α (satisfying the constraint) that minimizes the residual

(cid:107)Tπ ˜vα

k+1 − ˜vα

k+1(cid:107) = (cid:107)

m
(cid:88)

i=0

αi(Tπvk−m+i − vk−m+i)(cid:107).

Eventually, the new estimate is obtained by applying the operator to the vector ˜vα
minimal residual.

k+1 of

The same approach can be applied (heuristically) to non-aﬃne operators. The conver-
gence of this scheme has been studied (e.g., Toth and Kelley (2015)) and it can be linked
to quasi-Newton methods (Fang and Saad, 2009).

3

Geist and Scherrer

2.3 Preliminary Experimental Results

We consider Garnet problems (Archibald et al., 1995; Bhatnagar et al., 2009). They are a
class of randomly built MDPs meant to be totally abstract while remaining representative
of the problems that might be encountered in practice. Here, a Garnet G(|S|, |A|, b) is
speciﬁed by the number of states, the number of actions and the branching factor. For each
(s, a) couple, b diﬀerent next states are chosen randomly and the associated probabilities
are set by randomly partitioning the unit interval. The reward is null, except for 10% of
states where it is set to a random value, uniform in (1, 2).

We generate 100 random MDPs G(100, 4, 3) and set γ to 0.99. For each MDP, we apply
value iteration (denoted as m = 0 in the graphics) and Anderson accelerated value iteration
for m ranging from 1 to 9. The inital value function v0 is always the null vector. We
run all algorithms for 250 iterations, and measure the normalised error for algorithm alg
at iteration k, (cid:107)v∗−valg
k (cid:107)1
(cid:107)v∗(cid:107)1
MDP.

, where v∗ stands for the optimal value function of the considered

a. Normalized error
.

b. Normalized error
(mean, log-scale).

c. Normalized error
(std, log-scale).

Figure 1: Results on the Garnet problems.

Fig. 1 shows the results. Fig. 1.a shows how the normalized error evolves with the
number of iterations (recall that m = 0 stands for classic value iteration). Shaded areas
correspond to standard deviations and lines to means (due to randomness of the MDPs,
the algorithms being deterministic given the ﬁxed initial value function). Fig. 1.b and 1.c
show respectively the mean and the standard deviation of these errors, in a logarithmic
scale. One can observe that Anderson acceleration consistently oﬀers a signiﬁcant speed-up
compared to value iteration, and that rather small values of m (m ≈ 5) seem to be enough.

2.4 Nuancing the Acceleration

We must highlight that the optimal policy is the object of interest, the value function being
only a proxy to it. Regarding the value function, its level is not that important, but its rel-
ative diﬀerences are. This is addressed by the relative value iteration algorithm (Puterman,
1994, Ch. 6.6). For a given state s0, it iterates as vk+ 1
(s0)1. It
usually converges much faster than value iteration (towards v∗ − v∗(s0)1), but the greedy
policies resp. to each iterate’s estimated values are the same for both algorithms. This
scheme can also be easily accelerated with Anderson’s approach.

= T vk, vk+1 = vk+ 1

− vk+ 1

2

2

2

4

050100150200250# of iterations0.20.00.20.40.60.81.0normalized errorm = 0m = 1m = 2m = 3m = 4m = 5m = 6m = 7m = 8m = 9050100150200250# of iterations106105104103102101100normalized error (mean)m = 0m = 1m = 2m = 3m = 4m = 5m = 6m = 7m = 8m = 9050100150200250# of iterations106105104103102101normalized error (std)m = 0m = 1m = 2m = 3m = 4m = 5m = 6m = 7m = 8m = 9Anderson Acceleration for Reinforcement Learning

a. Error on greedy policies
(accelerated VI).

b. Normalized error
(accelerated relative VI).

c. Error on greedy policies
(accelerated relative VI).

Figure 2: Additional results.

We provide additional results on Fig. 2 (for the same MDPs as previously). Fig. 2.a
being greedy

shows the error of the greedy policy, that is (cid:107)v∗ − vπalg
k
respectively to valg
, for the ﬁrst 10 iterations (same data as for Fig. 1). This is what we’re
really interested in. One can observe that value iteration provides more quickly better
solutions than Anderson acceleration. This is due to the fact that if the level of the value
function converges slowly, its relative diﬀerences converge more quickly.

(cid:107)1/(cid:107)v∗(cid:107)1, with πalg

k

k

So, we compare relative value iteration and its accelerated counterpart in Fig. 2.b (nor-
malized error of the estimate, not of the greedy policy), to be compared to Fig. 1.b. There
is still an acceleration with Anderson, at least at the beginning, but the speed-up is much
less than in Fig. 1. We compare the error on greedy policies for the same setting in Fig. 2.c,
and all approaches perform equally well.

3. Anderson Acceleration for Reinforcement Learning

So, the advantage of Anderson acceleration applied to exact value iteration on simple Garnet
problems is not that clear. Yet, it could still be interesting for policy evaluation or in the
approximate setting. We discuss brieﬂy its possible applications to (deep) RL.

3.1 Approximate Dynamic Programming

Anderson acceleration could be applied to approximate dynamic programming and related
methods. For example, the well-known DQN algorithm (Mnih et al., 2015) is nothing else
than a (very smart) approximate value iteration approach. A state-action value function
Q is estimated (rather than a value function), and this function is represented as a neural
network. A target network Qk is maintained, and the Q-function is estimated by solving
the least-squares problem (for the memory buﬀer {(si, ai, ri, s(cid:48)

i)1≤i≤n})

1
n

n
(cid:88)

i=1

(yi − Qθ(si, ai))2 with yi = ri + γ max
a∈A

Qk(s(cid:48)

i, a).

Anderson acceleration can be applied directly as follows. Assume that the m + 1 previous
target networks Qk, . . . , Qk−m are maintained. Deﬁne for k − m ≤ j ≤ k

δj = [r1 + γ max

a

Qj(s(cid:48)

1, a) − Qj(s1, a1), . . . , rn + γ max

Qj(s(cid:48)

n, a) − Qj(sn, an)](cid:62) ∈ Rn

a

5

0246810# of iterations104103102101100normalized error (mean)m = 0m = 1m = 2m = 3m = 4m = 5m = 6m = 7m = 8m = 9050100150200250# of iterations10121010108106104102100normalized error (mean)m = 0m = 1m = 2m = 3m = 4m = 5m = 6m = 7m = 8m = 90246810# of iterations104103102101100normalized error (mean)m = 0m = 1m = 2m = 3m = 4m = 5m = 6m = 7m = 8m = 9Geist and Scherrer

and ∆k = [δk−m, . . . , δk] ∈ Rn×(m+1). Solve αk+1 as in Eq. (1) and deﬁne for all 1 ≤ i ≤ n

yi =

n
(cid:88)

j=0

αj(ri + γ max
a∈A

Qk−m+j(s(cid:48)

i, a)).

So, Anderson acceleration would modify the targets in the regression problem, the necessary
coeﬃcients being obtained with a cheap least-squares (given m is small enough, as suggested
by our preliminary experiments). Notice that the estimate αk+1 is biased, as being the
solution to a residual problem with sampled transitions. However, if a problem, this could
probably be handled with instrumental variables, giving an LSTD-like algorithm (Bradtke
and Barto, 1996). Variations of this general scheme could also be envisionned, for example
by computing the α vector on a subset of the memory replay or even on the current mini-
batch, or by considering variations of Anderson acceleration such as the one of Henderson
and Varadhan (2018).

This acceleration scheme could be more generally applied to approximate modiﬁed policy
iteration, or AMPI (Scherrer et al., 2015), that generalizes both approximate policy and
value iterations. Modiﬁed policy iteration is similar to policy iteration, except that instead
of computing the ﬁxed point in the evaluation step, the Bellman evaluation operator is
applied p times (p = 1 gives value iteration, p = ∞ policy iteration), the improvement
step (computing the greedy policy) being the same (up to possible approximation). In the
approximate setting, the evaluation step is usually performed by performing the regression
of p-step returns, but it could be done by applying repeatedly the evaluation operator, this
being combined with Anderson acceleration (much like DQN, but with Tπ instead of T ).

3.2 Policy Optimization

Another popular approach in reinforcement learning is policy optimization, or direct policy
search (Deisenroth et al., 2013), that maximizes J(w) = ES∼µ[vπw (S)] (or a proxy), for a
user-deﬁned state distribution µ, over a class of parameterized policies. This is classically
done by performing a gradient ascent:

wk+1 = wk + η∇wJ(w)|w=wk .

(2)

This gradient is given by ∇wJ(w) = ES∼dµ,πw ,A∼π[Qπw (S, A)∇w ln πw(A|S)]. Thus, it
depends on the state-action value function of the current policy. This gradient can be
estimated with rollouts, but it is quite common to estimate the Q-function itself. Related
approaches are known as actor-critic methods (the actor being the policy, and the critic
the Q-function). It is quite common to estimate the critic using a SARSA-like approach,
especially in deep RL. In other words, the critic is estimated by applying repeatedly the
Bellman evaluation operator. Therefore, Anderson acceleration could be applied, in the
same spirit as what we described for DQN.

Yet, Anderson acceleration could also be used to speed up the convergence of the policy.
Consider the gradient ascent in Eq. (2). This can be seen as a ﬁxed-point iteration to solve
w = w+η∇wJ(w). Anderson acceleration could thus be used to speed it up. Seeing gradient
descent as a ﬁxed point is not new (Jung, 2017), nor is applying Anderson acceleration to
speed it up (Scieur et al., 2016; Xie et al., 2018). Yet, it has never been applied to policy
optimization, as far as we know.

6

Anderson Acceleration for Reinforcement Learning

References

Donald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the

ACM (JACM), 12(4):547–560, 1965.

TW Archibald, KIM McKinnon, and LC Thomas. On the generation of Markov decision

processes. Journal of the Operational Research Society, pages 354–361, 1995.

Shalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural

actor-critic algorithms. Automatica, 45(11):2471–2482, 2009.

Steven J. Bradtke and Andrew G. Barto. Linear Least-Squares algorithms for temporal

diﬀerence learning. Machine Learning, 22(1-3):33–57, 1996.

Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for

robotics. Foundations and Trends R(cid:13) in Robotics, 2(1–2):1–142, 2013.

Haw-ren Fang and Yousef Saad. Two classes of multisecant methods for nonlinear acceler-

ation. Numerical Linear Algebra with Applications, 16(3):197–221, 2009.

Nicholas C Henderson and Ravi Varadhan. Damped anderson acceleration with restarts
and monotonicity control for accelerating em and em-like algorithms. arXiv preprint
arXiv:1803.06673, 2018.

Alexander Jung. A ﬁxed-point of view on gradient methods for big data. Frontiers in

Applied Mathematics and Statistics, 3:18, 2017.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.

Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Program-

ming. Wiley-Interscience, 1994.

Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and Matthieu
Geist. Approximate modiﬁed policy iteration and its application to the game of tetris.
Journal of Machine Learning Research, 16:1629–1676, 2015.

Damien Scieur, Alexandre d’Aspremont, and Francis Bach. Regularized nonlinear acceler-
ation. In Advances In Neural Information Processing Systems, pages 712–720, 2016.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT

press Cambridge, 1998.

Alex Toth and CT Kelley. Convergence analysis for anderson acceleration. SIAM Journal

on Numerical Analysis, 53(2):805–819, 2015.

Homer F Walker and Peng Ni. Anderson acceleration for ﬁxed-point iterations. SIAM

Journal on Numerical Analysis, 49(4):1715–1735, 2011.

7

Geist and Scherrer

Guangzeng Xie, Yitan Wang, Shuchang Zhou, and Zhihua Zhang. Interpolatron: Interpola-
tion or extrapolation schemes to accelerate optimization for deep neural networks. arXiv
preprint arXiv:1805.06753, 2018.

8

