On Generation of Adversarial Examples
using Convex Programming

Emilio Rafael Balda, Arash Behboodi, Rudolf Mathar
{emilio.balda, arash.behboodi, mathar}@ti.rwth-aachen.de
Institute for Theoretical Information Technology, RWTH Aachen University, Aachen, 52074

Abstract—It has been observed that deep learning architectures tend to
make erroneous decisions with high reliability for particularly designed
adversarial instances. In this work, we show that the perturbation analysis
of these architectures provides a framework for generating adversarial
instances by convex programming which, for classiﬁcation tasks, is able
to recover variants of existing non-adaptive adversarial methods. The
proposed framework can be used for the design of adversarial noise under
various desirable constraints and different types of networks. Moreover,
this framework is capable of explaining various existing adversarial
methods and can be used to derive new algorithms as well. We make use
of these results to obtain novel algorithms. The experiments show the
competitive performance of the obtained solutions, in terms of fooling
ratio, when benchmarked with well-known adversarial methods.1

I. INTRODUCTION

Deep Neural Networks (DNNs) enjoy excellent performances in
speech analysis [1] and visual
tasks [2], [3], [4], [5]. Despite
their success, they have been shown to suffer from instability in
their classiﬁcation under adversarial perturbations [6]. Adversarial
perturbations are intentionally worst-case designed noise that aims at
changing the output of a DNN to an incorrect one. Interestingly, the
example of adversarial perturbations on the ImageNet dataset show
that adversarial examples are almost indistinguishable to the human
eye from the original images. Related to the concept of adversarial
perturbations is the notion of rubbish class or fooling images [7],
[8] where the examples are clearly perceived by the human eye as
not belonging to any categories in the training set but nevertheless
classiﬁed with high conﬁdence as one of the categories by DNNs.
Moreover, as in [8], [9], [10], the adversarial method has access to the
input of a neural network based system. In this context, the attacker
would attempt to apply perturbations to system input that are not
perceived by the system’s administrator, such that the performance
of the system is severely degraded.

This is of signiﬁcant importance in safety critical systems such
as autonomous driving architectures and surveillance applications.
These discoveries gave rise to extensive research on understanding
the instability of DNNs (for instance refer to [11], [12], [13] and
references therein). Although DNNs might achieve robustness to
random noise [14], it has been shown that there is a clear distinction
between the robustness of a classiﬁer to random noise and its robust-
ness to adversarial perturbations. In [6], the adversarial perturbation
was obtained to maximize the prediction error at the output and it
was approximated using box-constrained L-BFGS. The Fast Gradient
Sign Method (FGSM) in [8] was based on ﬁnding the scaled sign
of the gradient of the cost function. Note that the FGSM aims at
minimizing (cid:96)∞-norm of the perturbation while the former algorithm
minimizes (cid:96)2-norm of the perturbation under box constraint on the
perturbed example. In practice, the perturbed input values are limited
inside a certain dynamic range, such as values between 0 and 1 for

8
1
0
2

c
e
D
3

]

G
L
.
s
c
[

4
v
7
0
6
3
0
.
3
0
8
1
:
v
i
X
r
a

1For

the
plementations
hppts://github.com/ebalda/adversarialconvex.

reproducible
paper
this

sake
used

of
in

research,
have

the
been made

tensorﬂow im-
at
available

the case of images. The algorithm DeepFool [9] utilizes an iterative
linearization of the DNN to generate perturbations that are minimal
in (cid:96)p-norm for p > 1. Although improving on the FGSM, the
algorithm is an iterative method requiring calculation of the gradient
function at each step and its convergence rate, which depends on a
previously selected step size parameter, is not guaranteed. In [15]
the authors propose an iterative version of the FGSM, called Basic
Iterative Method (BIM). This method was later extended in [16],
where randomness was introduced in the computation of adversarial
perturbations. This attack is called the Projected Gradient Descent
(PGD) method. An interesting feature of these algorithms is that some
of the perturbations generalize over other datasets and DNNs [10],
[8]. These perturbations are called universal adversarial perturbations.
This is partly explained by the fact that certain underlying properties
of the perturbation, such as direction in case of image perturbation,
matters the most and therefore generalized through different datasets.
There are various theories regarding the nature of adversarial
examples. The authors in [8] propose the linearity hypothesis where
the existence of adversarial images is attributed to the approximate
linearity of classiﬁers, although this hypothesis has been challenged in
[17]. There are other theories focusing mostly on decision boundaries
of classiﬁers and their analytic properties [14], [18].

In this paper, the adversarial examples are generated using an
approximation of the target classiﬁer with an afﬁne function. In
Section II, we ﬁrst introduce the concept of a ﬁrst-order perturbation
analysis, and its application to neural network classiﬁers. Then,
in Section III,
the ﬁrst-order perturbation analysis is utilized to
formulate the general formula for generation of adversarial attacks as
a convex optimization problem. In particular it is shown that the worst
perturbation incurred by an imperceptible adversarial perturbation
can be found using a convex optimization problem and the closed
form solutions are provided for the classiﬁcation problem. In Section
IV, we show the applicability of our framework for various learning
tasks such as regression, image segmentation, and detection. Later,
in Section V, we benchmark the obtained methods, for the context
of image classiﬁcation, against the FGSM and DeepFool algorithms.
In addition, we show that these algorithms can be formulated within
our framework. Furthermore, it is shown that our proposed algorithm
manages to outperform existing methods using empirical simulations
on the MNIST and CIFAR-10 datasets.

II. PERTURBATION ANALYSIS OF GENERAL CLASSIFIERS

The perturbation analysis, also called sensitivity analysis, is used in
signal processing for analytically quantifying the error at the output
of a system that occurs as consequence of a known perturbation
at the system’s input. Adversarial images can also be considered
as a slightly perturbed version of original images that manage to
change the output of the classiﬁer. Indeed, the adversarial methods
in [9], [8] are implicitly based on approximating the effect of an
input perturbation on a relevant function which is either the classiﬁer

 
 
 
 
 
 
function or the cost function used for training. The perturbation
analysis of classiﬁers provide a unifying view of previous methods.
For a classiﬁer given as a function f (.) of inputs x, if the input
vector is perturbed by a sufﬁciently small perturbation ∆x(0) given
by ˆx(0) = x(0) + ∆x(0), the ﬁrst-order perturbation incurred at the
output is given by the ﬁrst-order Taylor series of f (x(0) + ∆x(0)) as

f (x(0) + ∆x(0)) ≈ f (x(0)) + Jf (x(0))∆x(0)

(1)

where Jf (x) is the Jacobian of the function f (x). Therefore,
the error at the output of the classiﬁer can be approximated as
f (x(0) + ∆x(0)) − f (x(0)) ≈ Jf (x(0))∆x(0). For neural networks,
the perturbation analysis has been previously studied as in [19].

layer sizes (m1, . . . , mL),

Consider an L-layered neural network with the input vector
x(0) ∈ Rm0 and the corresponding output vector x(L) ∈ RmL
the weights of the hidden layer l
,
denoted by the matrix W(l) ∈ Rml×ml−1 ,
the bias vector by
b(l) ∈ Rml , and differentiable point-wise activation functions by
φ(l) with the derivative φ(l)(cid:48)
at the layer l ∈ [L]2. Let the function
f : Rm0 → RmL be the DNN’s function that maps the input vector
x(0) to the output vector x(L). The following proposition provides a
ﬁrst-order perturbation analysis of DNNs.

Theorem 1: For a given x(0), the ﬁrst-order perturbation at the
output of a DNN, ∆f , with continuously differentiable activation
functions caused by a small input perturbation ∆x(0) is given by:

∆f = Z(L)∆x(0),

where Z(L) is the Jacobian of the DNN function Jf (x(0)) given by:

Z(L) = D(L) · W(L) · D(l−1) · W(l−1) · · · D(1) · W(1) ,

with D(l) (cid:44) Diag{φ(l)(cid:48)

(W(l) · x(l−1) + b(l))}.
The proof is based on approximating each layer with a linear func-
tion. The output perturbation follows from consecutive application of
linear approximations. In the L-layered neural network, the l-th layer
output x(l) is given as
x(l) = φ(l) (cid:16)

W(l)x(l−1) + b(l)(cid:17)

∀l ∈ [L] .

In the context of perturbation analysis, it is assumed that all the
system parameters (i.e., x(0), φ(l), W(l), and b(l)) are known for
l ∈ [L].

Suppose that there is a perturbation ∆x(l−1) at the l-th layer output
and the perturbed version of the l-th layer output be given by ˆx(l) =
x(l) + ∆x(l). From the above relations we have

ˆx(l) = φ(l) (cid:16)
= φ(l) (cid:16)

W(l) ˆx(l−1) + b(l)(cid:17)
W(l)x(l−1) + W(l)∆x(l−1) + b(l)(cid:17)
W(l)x(l−1) + b(l)(cid:17)

(cid:16)

W(l)∆x(l−1).

≈ x(l) + Jφ(l)

But since l-th layer activation function is applied in a point-wise
fashion, its Jacobian is given by a diagonal matrix that is

The above theorem can be applied to general classiﬁers as well
as other learning functions such as regression. Note that
if the
activation functions are not differentiable at some points, one can
instead use sub-derivatives instead. One can recourse to higher order
perturbation analysis where the perturbation is quadratic or higher
order function of ∆x(0). This might be necessary if the perturbation
affects the output mainly through its higher orders, for example when
the perturbation belongs to the null space of the Jacobian matrix.

III. GENERATING MALICIOUS EXAMPLES VIA CONVEX
PROGRAMMING

As mentioned before, the adversarial examples can be considered
as perturbed versions of training examples and hence the analysis
above ﬁts our scenario, where the adversarial perturbation ∆x(0)
should be imperceptible to the target system. In [8], the proposed
method is based on ﬁnding a perturbation with bounded (cid:96)∞-norm
that maximizes the error function used for the training which utilizes
the ﬁrst-order perturbation analysis of the error function. On the
other hand in [9], the authors directly minimize the norm of the
perturbation that changes the classiﬁer’s output. Their analysis is
based on linearized approximation of the underlying classiﬁer which
is indeed its ﬁrst order-perturbation analysis. While DeepFool might
generate perturbations that are perceptible, the FGSM might not
change the classiﬁer output. In this work, we proposed another
method based on the ﬁrst-order perturbation analysis that targets
the classiﬁer’s output directly and simultaneously guarantees that the
perturbation is imperceptible.

For classiﬁcation tasks, let k : Rm0 → {1, 2, . . . , mL} be the
classiﬁer function that maps the input x ∈ Rm0 to its estimated
label k (x) ∈ {1, 2, . . . , mL}. The function k, deﬁned in this way, is
not differentiable anymore. However. In the context of classiﬁcation,
there is a proxy function f (x) given by a vector (f1(x), . . . , fm(x))
which has unit (cid:96)1-norm and with each of mL scalar functions fl(x)
interpreted as the probability of class belonging. The classiﬁer k is
given then as

k(x) = argmax
l∈[mL]

{fl (x)} .

(2)

The input perturbation aims at changing the output of the classiﬁer.
Suppose that the input vector is perturbed by a small perturbation
η ∈ Rm0 . Then, the classiﬁer k is said to be fooled by the adversarial
sample ˆx = x + η if k(x) (cid:54)= k(ˆx), that is:

L(x + η) = min
l(cid:54)=k(x)

{fk(x)(x + η) − fl(x + η)} < 0 .

(3)

However what is particularly disturbing in adversarial images is that
the image looks almost unchanged to the naked eye. Therefore the
input perturbation should not change the output the ground truth
classiﬁer, also called oracle classiﬁer in [12], which is here the naked
eye. As in [12], the proxy functions of the oracle classiﬁer are denoted
by gl and we should have:

D(l) (cid:44) Diag{φ(l)(cid:48)

(W(l) · x(l−1) + b(l))}.

Lo(x + η) = min
l(cid:54)=k(x)

{gk(x)(x + η) − gl(x + η)} > 0 .

Therefore the perturbation of l-th layer is given by:

Therefore the problem of adversarial design can be formulated as:

∆x(l) = D(l)W(l)∆x(l−1).

Find: η

By consecutive application of this result, ∆x(l) can be approximated
as ∆x(l) ≈ Z(l) · ∆x(0), where

Z(l) = D(l) · W(l) · D(l−1) · W(l−1) · · · D(1) · W(1) .

2In this work [L] (cid:44) {1, . . . , L}.

s.t. L(x + η) < 0, Lo(x + η) > 0.

(4)

There are two problems with the above formulation. First, the oracle
function is not known in general and second the function L can be
non-convex. One solution is to linearize L through the perturbation
analysis performed on each individual function and replacing the

constraint on the oracle function with a simpler one like (cid:96)p-norm
of the perturbation.

The ﬁrst order perturbation analysis of L yields:

L(x + η) ≈ L(x) + ηT∇xL(x),

where ∇xL(x) is the gradient of L(x). The condition that corre-
sponds to the oracle function can be approximated by (cid:107)η(cid:107)p ≤ ε for
sufﬁciently small ε ∈ R+. This means that the noise is sufﬁciently
small
the observer does not notice it.
These Gradient and norm relaxations yield to the following alternative
optimization problem:

in (cid:96)p-norm sense so that

Find: η

s.t. L(x) + ηT∇xL(x) < 0,

(cid:107)η(cid:107)p ≤ ε.

(GN)

The above problem was also derived in [20] and is a convex
optimization problem that can be efﬁciently solved. As we will see
later, this formulation of the problem can be relaxed into some well
known existing adversarial methods. However it is interesting to
observe that this problem is not always feasible as stated in the
following proposition.

Proposition 1: The optimization problem (GN) is not feasible if

for q = p
p−1

ε(cid:107)∇xL(x)(cid:107)q < L(x).

(5)

Proof. The proof follows a simple duality argument and is an elemen-
tary optimization theory result. We repeat the proof for completeness.
Note that the dual norm of (cid:96)p is deﬁned by:

(cid:107)x(cid:107)∗

p = sup{aT x : (cid:107)a(cid:107)p ≤ 1}.

p = (cid:107)x(cid:107)q for q = p

Furthermore (cid:107)x(cid:107)∗
p−1 . Since the (cid:96)p-norm of η
is bounded by ε, the value of ηT∇xL(x) is always bigger than
−ε(cid:107)∇xL(x)(cid:107)∗

p. However if the condition 5 holds, then we have:

L(x) + ηT∇xL(x) ≥ L(x) − ε(cid:107)∇xL(x)(cid:107)∗

p > 0.

Therefore, the problem is not feasible. (cid:4)

Proposition 1 shows that given a vector x, the adversarial pertur-
. In other

bation should have at least (cid:96)p-norm equal to
L(x)
is too small, then it is easier to fool
words if the ratio
(cid:107)∇xL(x)(cid:107)q
the network. In that sense, Proposition 1 provides an insight into the
stability of classiﬁers. In [9], the authors suggest that the robustness
of the classiﬁers can be measured as:
(cid:88)

L(x)
(cid:107)∇xL(x)(cid:107)q

ˆρ1(f ) =

1
|D|

x∈D

(cid:107)ˆr(x)(cid:107)p
(cid:107)x(cid:107)p

,

where D denotes the test set and ˆr(x) is the minimum perturbation
required to change the classiﬁer’s output. The above theorem suggests
that one can also use the following as the measure of robustness:

ˆρ2(f ) =

1
|D|

(cid:88)

x∈D

L(x)
(cid:107)∇xL(x)(cid:107)q

.

The lower ˆρ2(f ), the easier it gets to fool the classiﬁer and therefore
it becomes less robust to adversarial examples. One can also look
at other statistics related to
in order to evaluate the
robustness of classiﬁers.

L(x)
(cid:107)∇xL(x)(cid:107)q

Since Proposition 1 shows that the optimization problem (GN)
might not be feasible, alternatively we propose to solve the following
optimization problem, called the Gradient-base Norm-constrained
method:

(cid:110)

min
η

L(x) + ηT∇xL(x)

(cid:111)

s.t. (cid:107)η(cid:107)p ≤ ε ,

(GNII)

which ﬁnds the best perturbation under a given constraint. The
constraint aims at guaranteeing that the adversarial images is still
imperceptible by an ordinary observer. Note that (GNII) is funda-
mentally different from [9], [20], where the norm of the noise does
not appear as a constraint. Using a similar duality argument, the
problem (GNII) has a closed form solution given below.

Proposition 2: If ∇xL(x) =

solution to the problem (GNII) is given by

(cid:16) ∂L(x)
∂x1

, . . . , ∂L(x)
∂xm0

(cid:17)

, the closed form

η = −ε

(cid:32)

sign(

×

1
(cid:107)∇xL(x)(cid:107)q−1
(cid:12)
∂L(x)
∂L(x)
(cid:12)
(cid:12)
∂x1
∂x1
(cid:12)

)

q

q−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

, . . . , sign(

∂L(x)
∂xm0

)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L(x)
∂xm0

q−1(cid:33)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(6)

for q = p
is given by the following:

p−1 . Particularly for p = ∞, we have q = 1 and the solution

η = −ε sign(∇xL(x)) .

(7)

Proof. Based on the duality deﬁnition, we know that

sup
(cid:107)η(cid:107)p≤1

ηT∇xL(x) = (cid:107)∇xL(x)(cid:107)∗
p,

which in turn implies that the objective function is lower bounded
by L(x) − ε(cid:107)∇xL(x)(cid:107)∗
p. It is easy to verify that the minimum is
attained by the expression 6. (cid:4)

The advantage of (GNII), apart from being convex and enjoying
computationally efﬁcient solutions, is that one can incorporate other
convex constraints into it for different scenarios. In the next sections,
we examine this method for fooling neural networks.
Remark 1: There are various hypothesis about

the nature of
adversarial
images (see [11]). A popular hypothesis is so-called
linearity hypothesis according to which the neural networks are
intentionally designed to operate in linear regimes and that makes
them susceptible to adversarial examples. The above formulation
of the problem basically presupposes that the behavior of DNN
classiﬁers around particular image can be approximated by a linear
classiﬁer. In this sense, the current formulation is compatible with
the linearity hypothesis.

Note that the introduced method in (GNII) can also be used for
other target functions or learning problems. One can use the cost
function used for training as in [8] in which case the solution of
(GNII) with p = ∞ recovers the adversarial perturbations obtained
via the FGSM. Again the algorithm can be also used to generate
adversarial examples for regression problems. The feasibility problem
of (GN) can be also simpliﬁed to

min
η

(cid:107)η(cid:107)p

s.t. L(x) + ηT∇xL(x) ≤ 0 ,

(8)

which recovers the result in [9] although without the iterative pro-
cedure. However the iterative procedure can be easily adapted to
the current formulation by repeating the optimization problem until
the classiﬁer output changes. In any case, the formulation in (GN)
provides a general framework for generating adversarial examples
using a computationally efﬁcient way.

IV. FROM CLASSIFICATION TO REGRESSION AND OTHER
PROBLEMS

In this paper we have modeled the generation of adversarial attacks
using a convex optimization problem. While we have focused on
the task of classiﬁcation, the convex formulation from (GNII) is not
restricted to that speciﬁc task. Furthermore, in this section we discuss

the applicability of this framework for tasks beside classiﬁcation. As
an example, we apply this framework in the particular context of
regression.

In context of regression problems, we assume that the aim of
the adversarial perturbation algorithm is to maximize the (cid:96)2-norm
of the output perturbation, that is to maximize L(x + η) = (cid:107)f (x) −
f (x + η)(cid:107)2 subject to (cid:107)η(cid:107)p ≤ ε. In this case ﬁnding the adversarial
perturbation is indeed solving

argmax
η

(cid:8)(cid:107)Jf (x) · η(cid:107)2

2

(cid:9) s.t. (cid:107)η(cid:107)p ≤ ε .

(9)

In this problem, the objective function is quadratic with a positive
semi-deﬁnite kernel and hence convex. The constraint is also convex.
Maximizing convex functions is in general very difﬁcult, however
the problem can be solved efﬁciently in some cases. For general p,
the maximum value is related to the operator norm of Jf (x). The
operator norm of a matrix A ∈ Cm×n between (cid:96)p and (cid:96)q is deﬁned
as [21]

(cid:107)A(cid:107)p→q (cid:44) sup

(cid:107)Ax(cid:107)q.

(cid:107)x(cid:107)p≤1

Using this notion, we can see that ﬁrst (cid:107) η

ε (cid:107)p ≤ 1 and therefore

(cid:107)Jf (x) · η(cid:107)2 = ε(cid:107)Jf (x) ·

η
ε

(cid:107)2 ≤ ε(cid:107)Jf (x)(cid:107)p→2.

Therefore the problem of ﬁnding a solution to (9) amounts to ﬁnding
the operator norm (cid:107)Jf (x)(cid:107)p→2. First observe that the maximum
value is achieved on the border namely for (cid:107)η(cid:107)p = ε. In the case
of p = 2, this problem has a closed-form solution. If vmax is the
unit (cid:96)2-norm eigenvector corresponding to the maximum eigenvalue
of Jf (x)TJf (x), then η = εvmax solves the optimization problem.
Note that, the maximum eigenvalue of Jf (x)TJf (x) corresponds to
the square of the spectral norm (cid:107)Jf (x)(cid:107)2→2.

Another interesting case is when p = 1, that is when the (cid:96)1-
norm of the perturbation is bounded by ε. Note that penalizing high
(cid:96)1-norm values is a technique used to promote sparsity. When the
solution of a problem should satisfy a sparsity constraint, the direct
introduction of such constraint into the optimization leads to NP-
hardness of the problem. Instead the constraint is relaxed by adding
(cid:96)1-norm regularization. The adversarial perturbation designed in this
way tends to have only a few non-zero entries. This corresponds
to scenarios like single pixel attacks where only a few pixels are
supposed to change. For this choice, we have

(cid:107)A(cid:107)1→2 = max
k∈[n]

(cid:107)ak(cid:107)2,

Original Adversarial

Original Adversarial

nine

zero

airplane

ship

eight

three

truck

car

two

three
MNIST dataset

cat
dog
CIFAR-10 dataset

Fig. 1: Examples of correctly classiﬁed images that are missclassﬁed
when adversarial noise is added using Algorithm 1.

that the semi-deﬁnite programming scales badly with input dimension
in terms of computational complexity and therefore might not be
suitable for fast generation of adversarial examples when the input
dimension is very high.

Apart from regression, another example where the above method
might be useful to generate adversarial images is the image seg-
mentation problem where there is a class assigned to every pixel of
an image. This problem was considered in [24] where the objective
of an attacker is to draw certain geometric ﬁgures on the output
segmentation. In this setup,
the noise is designed such that an
is missclassiﬁed as certain target class t. This constitutes
input
a variation in the type of loss considered in (3). Instead of just
changing the output classiﬁer, we aim at changing the output of the
classiﬁer into a designated class. In this case, one can instead use
Lt(x + η) = fk(x)(x + η) − ft(x + η), where we have a ﬁxed
class t as target. The above analysis applies directly to this problem
as well.

Finally, in the context of anomaly detection and monitoring, the
goal of an attacker is to maximize the false positives and/or false
negatives. This naturally leads to algorithms of the same nature as
Algorithm 2 (introduced later on Section V), where a single score
function (e.g. the probability of being detected) is the subject of
minimization or maximization.

where ak’s are the columns of A. Therefore, if the columns of the
Jacobian matrix are given by Jf (x) = [J1 . . . Jm0 ], then

V. EXPERIMENTS

(cid:107)Jf (x) · η(cid:107)2 ≤ ε max
k∈[m0]

(cid:107)Jk(cid:107)2,

and the maximum attained for

η∗ = εek∗

for

k∗ = arg max
k∈[m0]

(cid:107)Jk(cid:107)2,

where the vector ei is the i-th canonical vector. This constitutes a
single pixel attack.

Finally, the case where the adversarial perturbation is bounded in
(cid:96)∞-norm is of particular interest. This bound guarantees that the noise
entries have bounded values. The problem of ﬁnding an adversarial
noise corresponds to obtaining the vector for which the operator norm
(cid:107)Jf (x)(cid:107)∞→2 is attained. Unfortunately this problem turns out to be
NP-hard [22]. However it is possible to approximately ﬁnd this norm
using semi-deﬁnite programming as proposed in [23]. The problem is

In this section, the Gradient-based Norm-constrained method is
used to fool the classiﬁer trained on the task of classiﬁcation for the
MNIST [25] and CIFAR-10 [26] datasets. As discussed in Section III,
for this context of image classiﬁcation the appropriate loss function
L(x) to be used in (GNII) is given by (3). For this problem,
(cid:107)η(cid:107)∞ ≤ ε is a common constraint that models the undetectability,
for sufﬁciently small ε, of adversarial noise by an observer. However
solving (GNII) involves ﬁnding the function L(x) which is deﬁned
as the minimum of mL − 1 functions with mL being the number of
different classes. In large problems, this may signiﬁcantly increase
the computations required to fool one image. Therefore, we include
a simpliﬁed version of this algorithm in our simulations. The non-
iterative methods might not guarantee the fooling of the underlying
network but on the other hand, the iterative methods might suffer
from convergence problems.

• Random: For benchmarking purpose, we also consider random
noise with independent Bernoulli distributed entries with P(X =
ε) = P(X = −ε) = 1
2 .

The above methods are tested on the following deep neural network

architectures:

• MNIST : A fully connected network with two hidden layers
of size 150 and 100 respectively, as well as the LeNet-5
architecture [27].

• CIFAR-10 : The Network In Network (NIN) architecture [28],

and a 40 layer DenseNet [29].

As a performance measure, we use the fooling ratio deﬁned in [9]
as the percentage of correctly classiﬁed images that are missclassiﬁed
when adversarial perturbations are applied. Of course, the fooling
ratio depends on the constraint on the norm of adversarial examples.
Therefore, in Figure 2 we observe the fooling ratio for different
values of ε on the aforementioned neural networks. As expected,
the increased computational complexity of iterative methods such as
DeepFool and Algorithm 1-n translates into increased performance
with respect to non-iterative methods. Nevertheless, as shown in
Figures 2(a) and (c), the performance gap between iterative and non-
iterative algorithms is not always signiﬁcant. For the case of iterative
algorithms, the proposed Algorithm 1-n outperforms DeepFool. The
same holds true for Algorithm 1 with respect to other non-iterative
methods such as the FGSM, while Algorithm 2 obtains competitive
performance with respect to the FGSM.

Finally, we measure the robustness of different networks using
ˆρ1(f ) and ˆρ2(f ), with p = ∞. We also include the minimum ε,
such that DeepFool obtains a fooling ratio greater than 99%, as a
performance measure as well. These results are summarized in Table
I, where we obtain coherent results between the 3 measures.

FCNN (MNIST)
LeNet-5 (MNIST)
NIN (CIFAR-10)
DenseNet (CIFAR-10)

Test
ˆρ1(f )
[9]
error
1.7%
0.036
0.9%
0.077
13.8% 0.012
5.2%
0.006

ˆρ2(f )
(ours)
0.034
0.061
0.004
0.002

fooled
>99%
ε =0.076
ε =0.164
ε =0.018
ε =0.010

TABLE I: Robustness measures for different classiﬁers.

VI. CONCLUSION

In this paper, we have shown that the perturbation analysis of
different models leads to methods for generating adversarial examples
via convex programming. For classiﬁcation we have formulated
already existing methods as special cases of the proposed framework.
Moreover, novel methods for designing adversarial noise under vari-
ous desirable constraints have been derived. Finally the applicability
of this framework has been tested for classiﬁcation through empirical
the well-
simulations of the fooling ratio, benchmarked against
known FGSM, PGD and DeepFool methods. We have also discussed
how the current framework can be extended to variety of different
problems. As future works, it is still worth exploring the reason
behind the existence of adversarial examples, and the design of
effective defenses.

(a) FCNN

(b) LeNet-5

(c) NIN

(d) DenseNet

Fig. 2: (a) and (b): Fooling ratio of the adversarial samples for
different values of ε on the MNIST test dataset. (c) and (d): Fooling
ratio of the adversarial samples for different values of ε on the
CIFAR-10 test datasets.

To benchmark the proposed adversarial algorithms, we consider

following methods tested on the aforementioned datasets:

• Algorithm 1: This algorithm solves (GNII) with L(x) given by
(3). Note that, for evaluating L at a given x one must search
over all l (cid:54)= k(x). This can be computationally expensive when
the number of possible classes (i.e., the number of possible
values for l) is large. The (cid:96)∞-norm is chosen for the constraint.
Moreover, an example of adversarial images obtained using this
algorithm is shown in Figure 1.

• Algorithm 1-n: Iterative version of Algorithm 1 with n itera-
tions. The adversarial noise is the sum of n noise vectors with
(cid:96)∞-norm of ε/n, computed through n successive approxima-
tions.

• Algorithm 2: This algorithm approximates (3) with L(x) ≈
fk(x)(x), thus reducing the computation of L(x) when the
number of classes is large. Note that we cannot use L(x+η) < 0
to guarantee that we have fooled the network. Nevertheless, the
lower the value of L(x+η) the most likely it is that the network
has been fooled. The same reasoning is valid for the FGSM.
• FGSM: This well-known method was proposed by [8] where
L is replaced by the negative training loss. Usually the cross-
entropy loss is used for this purpose. With the newly replaced
function, (GNII) is solved for p = ∞.

• DeepFool: This method was designed by [9] and makes use
of iterative approximations. Every iteration of DeepFool can be
written within our framework by replacing L by

L(x + η) = fk(x)(x + η) − fˆl(x + η) ,

where

ˆl = argmin
l(cid:54)=k(x)

(cid:26) |fk(x)(x) − fl(x)|

(cid:107)∇fk(x)(x) − ∇fl(x)(cid:107)q

(cid:27)

.

The adversarial perturbations are computed using p = ∞, thus
q = 1, with a maximum of 50 iterations. These parameters were
taken from [9].

• PGD: This method is an iterative version of the FGSM where
the initial point is randomly chosen from an ε vicinity of x [16].

0.000.020.040.060.080.10ε020406080100FoolingRatio(in%)DeepFoolAlg1-10Alg1-5Alg1Alg2FGSMPGDrandom0.0000.0250.0500.0750.1000.1250.1500.1750.200ε020406080100FoolingRatio(in%)0.0000.0050.0100.0150.0200.0250.030ε020406080100FoolingRatio(in%)0.0020.0040.0060.0080.010ε020406080100FoolingRatio(in%)[21] Simon Foucart and Holger Rauhut, A Mathematical Introduction to
Compressive Sensing, Applied and Numerical Harmonic Analysis.
Springer New York, New York, NY, 2013.

[22] Ji Rohn,

“Computing the norm (cid:107)A(cid:107)∞,1 is NP-hard,” Linear and

Multilinear Algebra, vol. 47, no. 3, pp. 195–204, May 2000.

[23] David Hartman and Milan Hladk,

“Tight Bounds on the Radius of
in Scientiﬁc Computing, Computer Arithmetic, and
Nonsingularity,”
Validated Numerics. Sept. 2015, Lecture Notes in Computer Science,
pp. 109–115, Springer, Cham.

[24] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and
Alan Yuille, “Adversarial examples for semantic segmentation and object
detection,” in International Conference on Computer Vision. IEEE, 2017.
“Mnist handwritten
digit database,” AT&T Labs [Online]. Available: http://yann. lecun.
com/exdb/mnist, vol. 2, 2010.

[25] Yann LeCun, Corinna Cortes, and CJ Burges,

[26] Alex Krizhevsky and Geoffrey Hinton, “Learning multiple layers of

features from tiny images,” 2009.

[27] Yann LeCun, Patrick Haffner, L´eon Bottou, and Yoshua Bengio, “Object
in Shape, contour and

recognition with gradient-based learning,”
grouping in computer vision, pp. 319–345. Springer, 1999.

[28] Min Lin, Qiang Chen, and Shuicheng Yan, “Network in network,” arXiv

preprint arXiv:1312.4400, 2013.

[29] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der
Maaten, “Densely connected convolutional networks,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2017, vol. 1, p. 3.

REFERENCES

[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury,
“Deep Neural Networks for Acoustic Modeling in Speech Recognition:
The Shared Views of Four Research Groups,” IEEE Signal Processing
Magazine, vol. 29, no. 6, pp. 82–97, Nov. 2012.

[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “ImageNet
Classiﬁcation with Deep Convolutional Neural Networks,” in Advances
in Neural Information Processing Systems 25, F. Pereira, C. J. C.
Burges, L. Bottou, and K. Q. Weinberger, Eds., pp. 1097–1105. Curran
Associates, Inc., 2012.

[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for
Image Recognition,” in 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016, pp. 770–778.

[4] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich, “Going deeper with convolutions,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2015,
pp. 1–9.

[5] S. Ren, K. He, R. Girshick, and J. Sun,

“Faster R-CNN: Towards
Real-Time Object Detection with Region Proposal Networks,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no.
6, pp. 1137–1149, June 2017.

[6] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus, “Intriguing properties
of neural networks,” International Conference on Learning Representa-
tions, 2014, arXiv: 1312.6199.

[7] Anh Nguyen, Jason Yosinski, and Jeff Clune, “Deep neural networks are
easily fooled: High conﬁdence predictions for unrecognizable images,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 427–436.

[8] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy, “Explaining
and Harnessing Adversarial Examples,” in International Conference on
Learning Representations, Dec. 2014.

[9] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard,
“Deepfool: a simple and accurate method to fool deep neural networks,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 2574–2582.

[10] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and
Pascal Frossard, “Universal adversarial perturbations,” arXiv preprint
arXiv:1610.08401, 2016.

[11] Naveed Akhtar and Ajmal Mian, “Threat of Adversarial Attacks on
Deep Learning in Computer Vision: A Survey,” arXiv:1801.00553 [cs],
Jan. 2018, arXiv: 1801.00553.

[12] Beilun Wang, Ji Gao, and Yanjun Qi, “A Theoretical Framework for
in
Robustness of (Deep) Classiﬁers against Adversarial Examples,”
International Conference on Learning Representations, 2017,
arXiv:
1612.00334.

[13] Alhussein Fawzi, Omar Fawzi, and Pascal Frossard, “Fundamental limits
on adversarial robustness,” Proceedings of ICML, Workshop on Deep
Learning, 2015.

[14] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard,
in
“Robustness of classiﬁers: from adversarial
Advances in Neural Information Processing Systems 29, D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, Eds., pp. 1632–
1640. Curran Associates, Inc., 2016.

to random noise,”

[15] Alexey Kurakin, Ian Goodfellow, and Samy Bengio,

“Adversarial
examples in the physical world,” arXiv preprint arXiv:1607.02533, 2016.
[16] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
Tsipras, and Adrian Vladu, “Towards deep learning models resistant
to adversarial attacks,” arXiv preprint arXiv:1706.06083, 2017.
[17] Thomas Tanay and Lewis Grifﬁn, “A Boundary Tilting Persepective on
the Phenomenon of Adversarial Examples,” arXiv:1608.07690 [cs, stat],
Aug. 2016, arXiv: 1608.07690.

[18] A. Fawzi, S. M. Moosavi-Dezfooli, and P. Frossard, “The Robustness of
Deep Networks: A Geometrical Perspective,” IEEE Signal Processing
Magazine, vol. 34, no. 6, pp. 50–62, Nov. 2017.

[19] Li Fu and Tinghuai Chen,

“Sensitivity analysis for input vector in
multilayer feedforward neural networks,” in Neural Networks, 1993.,
IEEE International Conference on. IEEE, 1993, pp. 215–218.

[20] Matthias Hein and Maksym Andriushchenko, “Formal guarantees on
the robustness of a classiﬁer against adversarial manipulation,” in NIPS,
2017.

