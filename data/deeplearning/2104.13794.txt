1
2
0
2

r
p
A
8
2

]

G
L
.
s
c
[

1
v
4
9
7
3
1
.
4
0
1
2
:
v
i
X
r
a

Finding High-Value Training Data Subset through
Differentiable Convex Programming

Soumi Das, Arshdeep Singh, Saptarshi Chatterjee1, Suparna Bhattacharya2, and
Sourangshu Bhattacharya1

1 Indian Institute of Technology, Kharagpur
2 Hewlett Packard Enterprise, Bangalore

Abstract. Finding valuable training data points for deep neural networks has
been a core research challenge with many applications. In recent years, various
techniques for calculating the ”value” of individual training datapoints have been
proposed for explaining trained models. However, the value of a training data-
point also depends on other selected training datapoints - a notion which is not
explicitly captured by existing methods. In this paper, we study the problem of
selecting high-value subsets of training data. The key idea is to design a learnable
framework for online subset selection, which can be learned using mini-batches
of training data, thus making our method scalable. This results in a parameterised
convex subset selection problem that is amenable to a differentiable convex pro-
gramming paradigm, thus allowing us to learn the parameters of the selection
model in an end-to-end training. Using this framework, we design an online al-
ternating minimization based algorithm for jointly learning the parameters of the
selection model and ML model. Extensive evaluation on a synthetic dataset, and
three standard datasets, show that our algorithm ﬁnds consistently higher value
subsets of training data, compared to the recent state of the art methods, some-
times ∼ 20% higher value than existing methods. The subsets are also useful in
ﬁnding mislabelled training data. Our algorithm takes running time comparable
to the existing valuation functions.

Keywords: Data valuation, Subset selection, Convex optimisation, Explainabil-
ity

1

Introduction

Estimation of ”value” of a training datapoint from a Machine Learning model point of
view, broadly called data valuation [10,19,16] , has become an important problem with
many applications. One of the early applications included explaining and debugging
training of deep neural models, where the inﬂuence of the training data points on the test
loss is estimated [13,16]. Another application involves estimating the expected marginal
value of a training datapoint w.r.t. general value functions e.g. accuracy on a test set
[10], which can be used in buying and selling of data in markets. [19] lists a number
of other applications, including detecting domain shift between training and test data,
suggesting adaptive data collection strategy, etc.

Most existing formulations for data valuation assume a supervised machine learning
model, y = f (x, θ) with parameters θ. For all the above mentioned applications, the

 
 
 
 
 
 
2

Authors Suppressed Due to Excessive Length

key technical question is to determine the improvement in the value function (usually
test set loss) given that a new set of datapoints are added to the training set. The inﬂu-
ence function based methods [13,16] aim to estimate the inﬂuence of each individual
datapoint, on the test loss of a trained model. While these techniques are computation-
ally efﬁcient, they ignore an important fact: the value of a training datapoint depends
on the other datapoints in the training dataset. For a given training data point x, an-
other datapoint y may add to the value of x, possibly because it is very different from
x. Alternately, y may reduce the value of x if it is very similar to x.

Shapley value based methods [10] also estimate the value of individual datapoints
using the expected marginal gain in the value function while adding the particular data-
point. Hence, this method considers the effect of other datapoints, but only in an aggre-
gate sense. Also, this method is computationally expensive. DVRL [19] is the closest to
our approach. It learns a parameterised selection function which maximizes the reward
of selecting training datapoints using Reinforcement Learning. The reward function is
the marginal improvement in value function after selection of a training datapoint over
the previous running average of value function. Unfortunately, this reward leads to se-
lection of many training datapoints which are similar to each other, if their inclusion
contributes to making the loss better than past average. To the best of our knowledge,
none of the existing data valuation techniques explicitly consider similarities between
training points, when selecting high value subsets.

In this paper, we consider a set value function for assigning value to a subset of
training datapoints. A proxy for the value function is learned though an embedding of
datapoints, such that distance between the embedded datapoints represent the inverse
value of replacing one of the datapoints with another. In this framework, we propose the
problem of high-value data subset selection, which computes the optimal subset of a
given size maximizing the value of the subset, or minimizing the total distance between
selected points and other training datapoints in the embedding space.

In order to efﬁciently compute the high value data subsets, we address two chal-
lenges. Firstly, the embedding function of datapoints needs to be learned by optimizing
the value of the selected subset, which itself is the output of an optimization problem.
Hence, our formulation should be able to propagate the gradient of value-function back
through the optimization problem. Secondly, for practicality, our formulation should be
able to select the datapoints in an online manner from minibatches of data. We address
both these challenges by designing a novel learned online subset selection formulation
for selecting subsets of training datapoints from minibatches. Our formulation is in-
spired by the online subset selection technique based on facility location objective [6],
but uses a learned distance function using a parameterised embedding of the training
datapoints. In order to learn the embedding parameters, our formulation is also com-
patible with the differentiable convex programming paradigm [1], which allows us to
propagate the gradients of the value function (loss on test set) back to the embedding
layer. We propose an online alternating minimization based algorithm to jointly learn
the embedding function for selection of optimal training data subset, and learn the op-
timal model parameters for optimizing the loss on the selected subset. Our algorithm
scales linearly in training data size for a ﬁxed number of epochs.

High-Value Training Data Selection

3

We benchmark our algorithm against state of the art techniques. e.g. DVRL [19],
Data Shapley [10], etc. using extensive experiments on multiple standard real world
datasets, as well as a synthetic dataset. We show that selection of high value subset
of training data points using the proposed method consistently leads to better test set
accuracy, for similar sized selected training dataset, with upto 20% difference in test
accuracy. Also, removal of high value subset of training datapoints leads to a much
more signiﬁcant drop in test accuracy, compared to baselines. We also show that the
proposed method can detect higher fraction of incorrectly labelled data, for all sizes of
selected subsets. Moreover, correcting the detected mislabelled points leads to higher
increase in test set accuracy compared to baseline. We observe that DVRL [19], which
is the closest competitor, selects datapoints in a biased manner, possibly because there,
it selects many similar high-performing datapoints at each stage.

To summarise, our key contributions are:

1. We highlight the problem of high-value data subset selection, which selects a high-

value data subset, rather than assigning valuation to individual datapoints.

2. We formulate a novel learned online data subset selection technique which can be

trained jointly with the ML model.

3. We propose an online alternating minimization based algorithm which trains the
joint model using mini-batches of training data, and scales linearly with training
data.

4. Results with extensive experimentation show that our method consistently outper-
forms recent baselines, sometimes with an accuracy improvement of ∼ 20% for the
same size subsets.

2 Related Work

The exponential rise in quantity of data and depth of deep learning models have given
rise to the question of scalability. Several subset selection methods have been designed
using submodular [2] and convex approaches [7], to combat the problem. However, all
these techniques only rely on some measure of similarity (pairwise, pointwise) [6][5]
and do not really account for the explanation behind its choice in view of the end-task.
Besides scalability, the other question which arises is explainability. One set of
methods surrounding explainability is prototype based approaches which ﬁnds the im-
portant examples aimed towards the optimisation of a given value function, or in other
words, improvement of a given performance metric (e.g test set accuracy). Pioneering
studies on inﬂuence functions [4] have been used to estimate the change in parameter
θ , given an instance xi is absent and is used to obtain an estimate of θ−i − θ. This
aids in approximating the importance of the instance xi towards the performance of the
learning model. [13] have built upon the framework of inﬂuence functions and obtained
an approximation of strongly convex loss functions. Following the works of [13], there
have been several other works which try to measure the inﬂuence of training points on
overall loss by tracking the training procedure [12] while some try to measure the inﬂu-
ence of training points on a single test point [16] leveraging the training checkpoints. A
recent work by [18] develops an algorithm for rapid retraining of models using subset
of data by reusing information cached during training phase.

4

Authors Suppressed Due to Excessive Length

Shapley value, an idea originated from game theory has been a driving factor in
the ﬁeld of economics. It was used as a feature ranking score for explaining black-box
models [14][15]. However, [10] were the ﬁrst to use shapley value to quantify data
points where they introduce several properties of data valuation and use Monte Carlo
based methods for approximation. There has also been a recent study in quantifying the
importance of neurons of a black box model [11] based on shapley values. A follow-
up work by [9] aims to speed up the computation of data shapley values. However,
all the above methods do not have a distinctive selection mechanism for providing a
representative set of points from incoming mini-batches which will be beneﬁcial for
explaining test set predictions.

Our work is in close proximity to the prototype based approaches where we intend
to select the important training data points. While [9] aims to work with training set
distribution, our work is essentially inclined towards selection from training datasets,
hence leading to choose [10] as one of our baselines. Among the methods around inﬂu-
ence functions, we use the work of [13] , [16] as two of our baselines. A recent work
by [19] attempts to adaptively learn data values using reinforcement signals, along with
the predictor model. This work is the closest in terms of the problem objective we are
intending to solve and hence we use it as one of our baselines. However, they intend
to optimize the selection using reinforcement trick which requires a greater number of
runs, while we use a learnable convex framework for the purpose of selection. We try
to select the points using a learning selection framework which will be then used to
optimize the subset through a differentiable value function. To the best of our knowl-
edge, this is the ﬁrst of its kind in using a differentiable convex programming based
learning framework to optimize the selection mechanism in order to improve on a given
performance metric.

3 High-Value Data Subset Selection

In this section, we formally deﬁne the problem of high-value data subset selection and
propose a learned approximate algorithm for the same.

3.1 Motivation and Problem Formulation

Data valuation for deep learning is an important tool for generating explanations of deep
learning models in terms of training data. Let D = {(xi, yi)|i = 1, . . . , n} be the train-
ing dataset with features xi ∈ X , and labels yi ∈ Y, and Dt = {(xt
i )|i = 1, . . . , m}
be a test dataset, which is used for valuation of the training data in D. Also, let s ∈ S
denote a subset of of the training data D (S is the power set of D). Let f (θ) de-
note a parameterized family of learnable functions (typically implemented with neu-
ral networks), parameterized by a set of parameters θ. Given an average loss function
L(f (θ), s), deﬁned as L(f (θ), s) = 1
(xi,yi)∈s L(yi, f (xi; θ)), we deﬁne the opti-
|s|
mal parameters θ∗(s) for a training data subset s as:

i, yt

(cid:80)

θ∗(s) = arg min

θ

L(f (θ), s)

(1)

High-Value Training Data Selection

5

Given an optimal parameter θ∗(s) and the test dataset Dt, we deﬁne the value function
v(s), which provides a valuation of the subset s as:

v(s) = v(θ∗(s), Dt) = L(f (θ∗(s)), Dt)

The high-value subset selection problem can be deﬁned as:

max
s∈S

v(s) sub. to |s| ≤ γn

(2)

(3)

where γ is the fraction of retained datapoints in the selected subset.

This problem is NP-Hard for a general value function v(s). Next, we describe our

broad framework to approximate the above problem with a two step approach:

1. Learn an embedding function h(x, φ) of the data point x, such that the distance
between datapoints xi and xj, dij = d(xi, xj) = D(h(xi, φ), h(xj, φ)) represents
their inverse-ability to replace each other for a given learning task. Here, D is a
ﬁxed distance function.

2. Select the set of most important datapoints s∗ by minimizing the sum of distance
between each datapoint and its nearest selected neighbor, and update the parameters
φ of the embedding function such that v(s∗) is maximized.

The objective function for the selection problem in second step can be written as:

s∗ = min
s∈S

(cid:88)

(x,y)∈D

min
(x(cid:48),y(cid:48))∈s

d(x, x(cid:48))

(4)

This objective function corresponds to the facility location problem [6], which can be
relaxed to the following convex linear programming problem:

min
zij ∈[0.1]

sub. to

n
(cid:88)

zijd(xi, xj)

i,j=1
n
(cid:88)

zij = 1, ∀i = {1, . . . , n}

j=1

(5)

Here zij = 1 indicate that the datapoint j is the nearest selected point or “represen-
tative” for data point i. Hence, the objective function sums the total distance of each
point from its representative point, and the constraint enforces that every datapoint i
should have exactly one representative point. A point j is selected if it is representative
to at least one point, i.e. (cid:80)n
i=1 zij ≥ 1. Let uj be the indicator variable for selection of
datapoint j. For robustness to numerical errors, we calculate uj as:

uj =

1
ξ

max{

n
(cid:88)

i=1

zij, ξ}

(6)

where 0 ≤ ξ ≤ 1 is a constant.

There are two main challenges in utilizing this formulation for the problem of opti-
mizing the value function. Firstly, the optimal parameters θ∗(s) depend on the variables

6

Authors Suppressed Due to Excessive Length

zij through variables uj, which are output of an optimization problem. Hence, opti-
mizing v(s∗) w.r.t parameters (φ) of the embedding function requires differentiating
through the optimization problem. Secondly, for most practical applications calculating
the subset of training data, s, by optimizing the above optimization problem is too ex-
pensive for an iterative optimization algorithm which optimizes φ. Moreover, optimiz-
ing the parameters (θ) of the neural network model f (θ), typically involves an online
stochastic optimization algorithm (e.g. SGD) which operates with mini-batches of train-
ing data points. In the next section, we describe the proposed technique for joint online
training of both the learning model parameters θ and embedding model parameters φ.

3.2

Joint Online Training of Subset Selection and Learning Model

Our problem of selection of high-value subsets from training data can be described as
a joint optimization of value function for selection of best subset, and optimization of
loss function on the selected subset computing the best model. Given the training and
test datasets D and Dt respectively, our overall value function parameterised by the
embedding model parameters φ for selection was deﬁned as (equation 2):

v(s(φ)) = v(θ∗(s(φ)), Dt) =

1
|Dt|

(cid:88)

(x,y)∈Dt

L(y, f (θ∗(s(φ)), x))

(7)

where θ∗(s(φ)) = arg minθ L(f (θ), s(φ)) (from equation 1). Representing s(φ) in
terms of the selection variables uj (Eqn 6), we can write the model loss on selected
training set using embedding model parameter φ as:

L(θ; φ, D) =

1
γ|D|

(cid:88)

(xj ,yj )∈D

uj(φ)L(yj, f (xj; θ))

(8)

We note that the above objective functions L(θ) and v(s(φ)) are coupled in the vari-
ables φ and θ. On one hand, the loss on the selected training set L(θ; φ, D) depends
on the current embedding model parameters φ. On the other hand, intuitively, the value
v(s(φ)) of a selected set of datapoints s(φ) should also depend on the current model pa-
rameter value θ(cid:48), which maybe be updated using loss from the selected set of datapoints
s(φ). Hence, we deﬁne the cumulative value function V(φ; θ(cid:48), D, Dt) as:

V(φ; θ(cid:48), D, Dt) = v(ˆθ, Dt), where ˆθ = θ(cid:48) − α∇θL(θ; φ, D)

(9)

Here, ˆθ is the one step updated model parameter using the selected examples from
dataset D using parameter φ, and α is the stepsize for the update. We combine the two
objectives into a joint objective function, J(θ, φ; D, Dt). The combined optimization
problem becomes:

θ∗, φ∗ = arg min
θ,φ

J(θ, φ; D, Dt) = arg min
θ,φ

(V(φ; θ(cid:48), D, Dt) + L(θ; φ, D))

(10)

As discussed above, since the training dataset D is normally much bigger in size, we
design our algorithm to optimize the above objective function in an online manner.

High-Value Training Data Selection

7

Hence the training dataset D is split into k equal-sized minibatches D = {D1, . . . , Dk}.
The above joint objective function can be decomposed as:

J(θ, φ; D, Dt) =

1
k

k
(cid:88)

i=1

{L(θ; φ, Di) + V(φ; θ(cid:48), Di, Dt)}

(11)

We solve the above problem using an online alternating minimization method similar
to the one described in [3]. Our algorithm updates θ and φ as:

Initialize θ1 and φ1 randomly
for t = 1, . . . , T :

1 :

2 :

θt+1 = θt − α

φt+1 = φt − β

1
∇θL(θ; φt, Di(t))
k
1
k

∇φV(φ; θt, Di(t), Dt)

Here i(t) is the index of the minibatch chosen at update number t. Step 1 is the stan-
dard SGD update minimizing the loss over a subset of the minibatch Di(t) chosen
using embedding parameters φt. Hence, it can be implemented using standard deep
learning platforms. Implementation of step 2 poses two challenges: (1) computation of
∇φV(φ; θt, Di(t), Dt) requires differentiation through an optimization problem, since
uj(φ) is the result of an optimization. (2) the datapoints in Di(t) must also be compared
with datapoints in a previous minibatch for selecting the best representative points.
Hence we need an online version of the optimization problem deﬁned in equation 5.
We describe solutions to these challenges in the next section.

3.3 Trainable Convex Online Subset Selection Layer

In order to implement the algorithm outlined in the previous section, we have to com-
pute ∇φV(φ; θt, Di, Dt), which can be done by combining equations 7,8 and 9 as:

∇φV(φ; θt, Di, Dt) = −

1
|Dt|

(cid:88)

∇ˆθl(yj, f (ˆθ, xj))·

(xj ,yj )∈Dt

(12)

1
γ|Di|

(cid:88)

(xj ,yj )∈Di

(∇φuj(φ)∇θL(yj, f (xj, θ))

The key challenge here is computation of ∇φuj(φ), since uj(φ) is the output of an
optimization problem. Also, the optimization problem in equation 5 selects from all
datapoints in D, whereas efﬁcient loss minimization demands that we only select from
the current minibatch Di(t). We use the online subset selection formulation, proposed
in [6]. Let O(t) denote the set of old datapoints at time t which have already been
selected, and let Di(t) denote the new set from which points will be selected. We use
two sets of indicator variables: zo
ij = 1 indicating that old datapoint j is a representative
of new datapoint i, and zn
ij = 1 indicating that new datapoint j is a representative of

8

Authors Suppressed Due to Excessive Length

new datapoint i. Under this deﬁnition the optimization problem can written as:

min
ij ∈[0,1]

ij ,zn
zo

(cid:88)

zo
ijd(xi, xj) +

(cid:88)

zn
ijd(xi, xj)

(13)

xi∈Di(t),xj ∈O(t)

xi∈Di(t),xj ∈Di(t)

sub. to

(cid:88)

zo
ij +

(cid:88)

zn
ij = 1, ∀i = {1, . . . , n}

xj ∈Di(t)

uj ≤ γ|Di(t)|

xj ∈O(t)
(cid:88)

xj ∈Di(t)

ξ max{(cid:80)

xi∈Di(t)

where uj = 1
zn
ij, ξ}. The ﬁrst constraint enforces that all new points
must have a representative, and the second constraint enforces that the total number
of representatives from the new set is less than γ|Di(t)|. The objective function mini-
mizes the total distance of between all points and their representatives. As previously,
d(xi, xj) = D(h(xi, φ), h(xj, φ)), where h(x, φ) is the embedding of the point x using
parameters φ.

In order to compute ∇φuj(φ) we use the differentiable convex programming paradigm

[1]. The DCP framework allows us to embed solutions to optimization problems as lay-
ers in neural networks, and also calculate gradients of output of the layers w.r.t. parame-
ters in the optimization problem, so long as the problem formulation satisﬁes all the dis-
ciplined parameterized programming (DPP) properties. The above convex optimization
problem generally satisﬁes all the properties of disciplined parameterized programming
(DPP) [1], with parameter φ, except for the objective function. Speciﬁcally, the con-
straints are all convex and are not functions of parameters. The objective function is also
linear in variables zo
ij, but should be an afﬁne function of the parameters φ in order to
satisfy the DPP constraints. Hence, we use the afﬁne distance function D(a, b) = |a−b|.
Hence, the DCP framework allows us to calculate ∇φuj(φ) = ∇zn
ij(φ). Algo-
rithm 1 summarizes the proposed framework for jointly learning a deep learning model
(parameterized by θ) and a selection function for selecting a fraction γ of the training
examples, parameterized by φ.

uj∇φzn

ij

Computational complexity: Running time of the proposed algorithm scales lin-
early with the training dataset size. The algorithm also works at a time with a minibatch
of training data, and hence is memory efﬁcient. These two properties make the cur-
rent algorithm work on very large training datasets, and complex deep learning models,
provided there is enough computational power for inference of the model. A signiﬁcant
overhead of the current algorithm comes from solving an optimization problem for each
parameter update. However, this cost can be controlled by keeping the minibatch sizes
|Di| small, typically |Di| ≤ 100, which results in a linear optimization in 10000 vari-
ables. Another practical trick is to use a ﬁxed number of datapoints for the old set O(t),
where the datapoints can be chosen to be most similar to points in the minibatch Di
using a locality sensitive hashing based nearest neighbor search algorithm. Experimen-
tal results show that our algorithm takes a comparable amount of time as our closest
competitor DVRL [19], while being much more accurate.

High-Value Training Data Selection

9

Algorithm 1 HOST-CP: High-value Online Subset selection of Training samples
through differentiable Convex Programming

1: Input:
2:

Training dataset D with minibatches D1, . . . , Dk, Test dataset Dt, fraction of selected

instances γ

3: Output:
4: Model Parameter θ, Embedding model for selection parameter φ
5: Algorithm:
6: Initialize old set O(0) → Φ (Null set)
7: Initialize parameters θ0 and φ0 randomly.
8: for each timestep t = 1, . . . , T do
9:
10:

Let Di(t) be the current minibatch.
for each datapoint xi ∈ Di(t) and xj ∈ O(t), calculate embeddings h(xi, φ) and

h(xj, φ)

11:
12:
13:
14:
15:
16:
17:
18:

ij and zo

ij by solving optimization problem in Equation 13

for each pair (i, j) , xi ∈ O and xj ∈ Di(t) calculate do(xi, xj)
for each pair (i, j) , xi ∈ Di(t) and xj ∈ Di(t) calculate dn(xi, xj)
Calculate zn
Calculate uj = 1
Include the selected points in the old set O(t + 1) for which uj = 1
Calculate L(θt; φt, Di(t)) on the selected set by forward propagation.
Calculate θt+1 = θt − α 1
Calculate ˆθ = θt − α∇θL(θ; φt, Di(t)) followed by V(φ; θ(cid:48), D, Dt) using Equation 9,

k ∇θL(θ; φt, Di(t)) by backpropagation.

zn
ij, ξ} for all xj ∈ Di(t)

ξ max{(cid:80)

xi∈Di(t)

where ∇φuj(φ) is calculated using DCP as described in Section 3.3
Update the embedding model parameter as: φt+1 = φt − β 1

k ∇φV(φ; θt, Di(t), Dt)

19:
20: end for

10

Authors Suppressed Due to Excessive Length

4 Experiment

In this section, we provide our experimental setup by describing the datasets and the
different data valuation baselines. In Section 4.2, we show the effectiveness of our pro-
posed method - High-value Online Subset selection of Training samples through difer-
entiable Convex Programming(HOST-CP) over the baselines in terms of performance
metric with addition and removal of high value data points. Next in Section 4.3, we
show the use case of diagnosing mislabelled examples which, when ﬁxed can improve
the performance. Following that in Section 4.4, we provide an analysis of the quality of
subsets obtained by one of the baselines, DVRL [19] and the proposed method. We also
show a ranking evaluation of selected points and their correspondence with ground truth
relevance in Section 4.5. Lastly, Section 4.6 describes the performance of the proposed
method in terms of its computational complexities.

4.1 Experimental Setup

Dataset: We have considered datasets of different modalities for our experiments. We
use four types of datasets: CIFAR10 - a multi-class dataset for image classiﬁcation,
protein dataset 3 for multi-class protein classiﬁcation, 20newsgroups 4 for text classiﬁ-
cation and a synthetic dataset for binary classiﬁcation. Following [10], we generate 10
synthetic datasets, using third-order polynomial function.

For CIFAR10, we obtain the image features from a pre-trained ResNet-18 model
and train a 2 layer neural network on the learned features. In case of synthetic data
and protein data, we pass the datapoints through a 3 layer neural network, whereas in
case of 20newsgroups, we pass the TF-IDF features of the documents through a 3 layer
network for classiﬁcation.
Baselines: We consider four state-of-the-art techniques as our baselines. Two of the
baselines - Inﬂuence Function - IF [13] and TracIn-CP [16] use the inﬂuence function
method for data valuation, one uses data shapley (DS) value [10] for selecting high
value data points while the fourth one, DVRL [19] uses reinforcement technique while
sampling, to rank the data points.

4.2 Valuable training data

We compare our method with state-of-the-art methods in terms of ﬁnding the most
valuable training data points. We show the effectiveness of the proposed method by
addition and removal of an incremental fraction of training instances to and from the
dataset. We examine the test set performance for each fraction of selected or removed
training points. The baseline methods are used to assign values to each data point, fol-
lowing which the top k% fraction of data are selected/removed from the whole dataset
to train the classiﬁcation network models. Our proposed method has the ﬂexibility to
optimally select the signiﬁcant k% of data from the entire given dataset. We use the

3 https://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/
4 https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html

High-Value Training Data Selection

11

Fig. 1: Addition (top) and removal (bottom) of valuable training datapoints: We
compare HOST-CP with the aforementioned baselines (IF[13], DS[10], TracIN-CP[16],
DVRL[19]) in terms of performance metric (accuracy) on test set. For each method, we
select (for addition) or remove (for removal) top k% important training points and assess
their importance. The proposed method, HOST-CP performs consistently better(highest
for addition and lowest for removal) across all fractions and all datasets.

subsets of varying k or (100 − k) fractions for training the networks and report the test
set accuracies.

In Figure 1, we report the test set accuracies after addition and removal of high-
value training data points, that get selected using the respective methods. We can ob-
serve in Figure 1 (top) that our method surpasses the baselines for all fractions and
it approximately performs equivalent to the whole set (WS) within a fraction of 50 -
65%. We also note a similar trend in removal of valuable data points. We can observe
in Figure 1 (bottom) that our method shows a signiﬁcant drop in accuracy with removal
of high-value data points. All the baselines show varying performances on different
datasets. However, the proposed method consistently outperforms across all of them.
This clearly suggests that HOST-CP is efﬁcient enough to provide the optimal subset of
training data points necessary for explaining the predictions on a given test set.

4.3 Fixing mislabelled data

Increase in data has led to rise in need of crowd sourcing for obtaining labels. Besides
the chances of obtained labels being prone to errors [8], several data poisoning attacks
are also being developed for mislabelling the data [17]. In this experiment, we impute
10% training data with incorrect labels, using which we compare our method with that
of the baselines. The baseline methods consider that the mislabelled data shall lie to-
wards the low ranked data points. We adopt the same idea and use the data points not
selected by our method (reverse-selection) for analysis.

102030405060Fraction(%) of added data405060708090Test Set accuracyCIFAR10 dataset::2 layer networkIFDSDVRLTracIn-CPHOST-CPWS1020304050607080Fraction(%) of added data556065707580Test Set accuracySynthetic dataset::3 layer network1020304050607080Fraction(%) of added data4550556065707580Test Set accuracyProtein dataset::3 layer network1020304050607080Fraction(%) of added data20304050607080Test Set accuracy20Newsgroups dataset::3 layer network0102030405060Fraction(%) of removed data868890929496Test Set accuracyCIFAR10 dataset::2 layer network01020304050607080Fraction(%) of removed data556065707580Test Set accuracySynthetic dataset::3 layer network01020304050607080Fraction(%) of removed data5560657075Test Set accuracyProtein dataset::3 layer network01020304050607080Fraction(%) of removed data556065707580Test Set accuracy20Newsgroups dataset::3 layer networkIFDSDVRLTracIn-CPHOST-CP12

Authors Suppressed Due to Excessive Length

In this experiment, we inspect the bottom k% fraction (for baselines) and unselected
k% fraction of the data (for the proposed method) for presence of wrong labels. We run
this experiment on CIFAR10 and synthetic dataset and report the fraction of incorrect
labels ﬁxed with varying k. While imputing 10% of training data, we ﬂip the labels of
synthetic dataset since it consists of binary labels, while in case of CIFAR10, we change
the original label to one out of 9 classes. We can observe in Table 1 that the proposed
method is able to ﬁnd comparable mislabelled fraction of incorrect labels in comparison
to the baselines. We also run another experiment where, we ﬁx the incorrect labels in
the considered k% fraction of data and train the entire data. Figure 2 shows a rise in
test set accuracy with a signiﬁcant margin using the proposed method. This denotes the
effectiveness of HOST-CP, in diagnosing mislabelled examples which turns out to be
an important use-case in data valuation techniques.

Table 1: Diagnosing mislabelled data: We inspect the training points starting from
the least signiﬁcant data and report the fraction of labels ﬁxed from the inspected data.
Overall, the proposed method, HOST-CP detects a higher rate of mislabelled data.

Fraction of
data checked
(%)

20
35
50
65
80

Fraction of incorrect labels ﬁxed

CIFAR10

IF DS DVRL

TracIn
-CP

0.21 0.19 0.195 0.20
0.35 0.34 0.336 0.35
0.50
0.50 0.50 0.49
0.63
0.61 0.61 0.65
0.79
0.74 0.78 0.80

HOST
-CP
0.23
0.36
0.51
0.67
0.81

Synthetic Data
TracIn
-CP

IF DS DVRL

0.22 0.22 0.19
0.34 0.37 0.33
0.43 0.54 0.46
0.56 0.67 0.58
0.66 0.81 0.79

0.23
0.35
0.51
0.68
0.81

HOST
-CP
0.23
0.39
0.55
0.68
0.83

Fig. 2: Accuracy after ﬁxing mislabelled data: For every k% fraction of inspected
data, we ﬁx the mislabelled instances and re-train. HOST-CP shows improved perfor-
mance with each ﬁxing of labels.

20304050607080Fraction (%) of data checked888990919293949596Test Set accuracyCIFAR10 dataset::2 layer network20304050607080Fraction (%) of data checked767778798081Test Set accuracySynthetic dataset::3 layer networkIFDSDVRLTracIn-CPHOST-CPWSHigh-Value Training Data Selection

13

4.4 Qualitative analogy of data valuation

As we had mentioned earlier, methodologically, DVRL [19] is the closest method to
our approach. Hence, in order to qualitatively explain the difference in performance
between that of DVRL and the proposed method, we try to analyse the quality of sub-
sets obtained by both of them on CIFAR10 dataset. Figure 3 shows the fraction of
selected images across classes by both the methods. We compare the top 5% selected
subset by DVRL and the proposed method in terms of class distribution. Unlike the pro-
posed method which compares the past selected data points with the incoming points
to compute the new subset, DVRL keeps track of past average test losses and values
the instances based on the marginal difference with the current loss. This leads to a
certain bias in case of DVRL, towards some particular class of images or similar high-
performing datapoints which are more contributory towards reduction in test set losses.
Following this, we observe that HOST-CP selects a far more diverse set of samples,
thus justifying its better performance.

Fig. 3: Quality of subset: We observe that unlike DVRL, HOST-CP selects diverse
images across the classes leading to better performance.

4.5 Ranking function for value points

Unlike the baseline methods which provide a score to each data point, the proposed
method has no such analogous scoring function. It is designed to return the appropriate
explainable subset necessary for optimum performance on a given test set. We design
this experiment to assess the signiﬁcance of the selected data points in terms of ranking
function values. We use NDCG score for reporting the ranking value.

We use the synthetic dataset and create a series of ground-truth values by ﬂipping
n% {=5,10,15} of the data. We assign ground-truth to the ﬂipped and unﬂipped points
with 0 and 1 respectively. We examine the bottom(or low-value) k% of the data for
computing the ranking values since the ﬂipped points are more inclined to lie towards
the bottom. As we have observed in Section 4.3, we use the reverse-selection procedure
to obtain the bottom k% of data for the proposed method, while for the baselines, the
low-valued k% points occupy the bottom rung.

airplaneautomobbirdcatdeerdogfroghorseshiptruckClasses in CIFAR100.00.10.20.30.40.5Fraction of selected instancesHOST-CPDVRL14

Authors Suppressed Due to Excessive Length

Fig. 4: Ranking of data points: We select k% fraction of data for each method and pro-
vide scores for each datum. While IF and DS subsets inherently come along with scores,
the proposed method, HOST-CP returns a subset which is scored using IF and DS. Fol-
lowing the acquisition of scores for each data point, we use NDCG@k to calculate the
ranking values. The proposed method is found to have a higher NDCG score.

In order to compute NDCG scores, we adopt the two baselines [13],[10] to pro-
vide scores to the subset of points obtained using the proposed method. In case of the
baselines, we use the k-lowest scoring points for computing the rank values.

We ﬂip 5%, 10% and 15% of the data and compute NDCG@k for k ∈ {5,10,15,25,40}.

Here k refers to the bottom k% subset from the methods. In Figure 4, we compare the
respective ranking values(NDCG) of subsets obtained using inﬂuence function (IF) or
data shapley (DS) with the subset obtained using the proposed method followed by us-
age of the corresponding scoring method (IF or DS) on this acquired subset. We can
observe that NDCG values obtained by the proposed method’s subset followed by scor-
ing using IF or DS always stays ahead than using the subsets obtained by IF or DS,
starting from an early fraction. This shows that the subset obtained using the proposed
method, HOST-CP is efﬁcient enough in keeping the proportions of relevant(unﬂipped)
and non-relevant(ﬂipped) points in their respective positions leading to a higher-NDCG
score.

4.6 Computational complexity

We analysed the computational complexity of the proposed method on a 64-bit machine
with one Quadro-P5000 GPU. For this experiment, we generated synthetic datasets
with varying number of training datapoints (500,2000,4000,6000,10000). Keeping the
network architecture ﬁxed at a 3 layer network, we varied the size of the training data, on
which the subset is meant to be computed. We also varied the size of the test datapoints
for which explanations are sought to be found from the training datapoints. We report
the time taken to perform the joint subset-training over an epoch, using the proposed

NDCG@5NDCG@10NDCG@15NDCG@25NDCG@400.9400.9450.9500.9550.9600.9650.970NDCG@k using IF scoringFlip 5% of dataIFHOST-CP using IFNDCG@5NDCG@10NDCG@15NDCG@25NDCG@400.880.900.920.940.960.98NDCG@k using IF scoringFlip 10% of dataNDCG@5NDCG@10NDCG@15NDCG@25NDCG@400.8000.8250.8500.8750.9000.9250.9500.975NDCG@k using IF scoringFlip 15% of dataNDCG@5NDCG@10NDCG@15NDCG@25NDCG@400.930.940.950.960.970.980.99NDCG@k using DS scoringFlip 5% of dataDSHOST-CP using DSNDCG@5NDCG@10NDCG@15NDCG@25NDCG@400.900.920.940.960.98NDCG@k using DS scoringFlip 10% of dataNDCG@5NDCG@10NDCG@15NDCG@25NDCG@400.860.880.900.920.940.960.98NDCG@k using DS scoringFlip 15% of dataHigh-Value Training Data Selection

15

Fig. 5: Scaling with data: We show that the change in time consumed by HOST-CP
with variation in training data points and test data points using a ﬁxed network is linear
with increasing data.

Table 2: Time comparison: We compare the times consumed by DVRL and HOST-CP.
We observe that HOST-CP has a lower running time compared to DVRL.

Accuracy
(5%)

Accuracy
(20%)

DVRL

HOST
-CP
62.46 57.8
63.34 57.2
65.14 57.4
65.71 57.6
65.72 57.4

DVRL

HOST-
-CP
71.32 66.4
73.82 66.8
74.89 66.5
66.6
75.0
66.4
75.0

DVRL

Epochs (HOST-CP)/
Iterations (DVRL)
HOST-
-CP
1
2
3
4
5

25
50
100
150
200

Time
(mins)

HOST-
-CP
1.1
2.01
3.05
4.08
5.12

DVRL

3.40
5.31
6.50
10.57
14.26

method, which aims at ﬁnding the best set of explanations from the training set for a
given set of test data. Figure 5 shows a linear pattern in time (in minutes) consumed by
the proposed method across the varying size of datapoints.

We have earlier observed that the proposed method is consistently performing better
than all the baselines. Since DVRL is the closest to our approach, we also record the
time taken by DVRL and the proposed method on the synthetic dataset. We increase the
number of iterations for DVRL to trace if the accuracy obtained by the proposed method
is achievable by DVRL under a certain subset cardinality (5%, 20%). We can observe
that DVRL saturates at a certain accuracy value even after increasing the number of
iterations to a much higher value. Thus, our method, HOST-CP besides attaining a
higher accuracy, also takes considerably lower running time than that of DVRL.

5 Conclusion

In this paper, we propose a technique for ﬁnding high-value subsets of training data-
points essential for explaining the test set instances. We design a learning convex frame-
work for subset selection, which is then used to optimise the subset with respect to a
differentiable value function. The value function is essentially the performance metric

200040006000800010000Training datapoints0510152025Time consumedSynthetic dataset::3 layer networkTest set size 100Test set size 500Test set size 100016

Authors Suppressed Due to Excessive Length

which helps to evaluate the trained models. We compare our method against the state-
of-the-art baselines , inﬂuence functions [13], data shapley [10], TracIn-CP [16] and
DVRL [19] across different datasets in a range of applications like addition or removal
of data, detecting mislabelled examples. We also analyse the quality of obtained subsets
from DVRL and the proposed method. Lastly, we show that the proposed method scales
linearly with the dataset size and takes a lower running time than DVRL which is the
closest method to our approach methodologically. Thus, we are able to show that our
method, HOST-CP outperforms the baselines consistently in all the applications used
for the experiments, thus proving its efﬁciency in terms of providing high-value subsets.

References

1. Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., Kolter, Z.: Differentiable convex

optimization layers. arXiv preprint arXiv:1910.12430 (2019)

2. Buchbinder, N., Feldman, M., Naor, J., Schwartz, R.: Submodular maximization with car-
dinality constraints. In: Proceedings of the twenty-ﬁfth annual ACM-SIAM symposium on
Discrete algorithms. pp. 1433–1452. SIAM (2014)

3. Choromanska, A., Cowen, B., Kumaravel, S., Luss, R., Rigotti, M., Rish, I., Diachille, P.,
Gurev, V., Kingsbury, B., Tejwani, R., et al.: Beyond backprop: Online alternating mini-
mization with auxiliary variables. In: International Conference on Machine Learning. pp.
1193–1202. PMLR (2019)

4. Cook, R.D., Weisberg, S.: Residuals and inﬂuence in regression. New York: Chapman and

Hall (1982)

5. Das, S., Mandal, S., Bhoyar, A., Bharde, M., Ganguly, N., Bhattacharya, S., Bhattacharya, S.:
Multi-criteria online frame-subset selection for autonomous vehicle videos. Pattern Recog-
nition Letters (2020)

6. Elhamifar, E., Kaluza, M.C.D.P.: Online summarization via submodular and convex opti-

mization. In: CVPR. pp. 1818–1826 (2017)

7. Elhamifar, E., Sapiro, G., Sastry, S.S.: Dissimilarity-based sparse subset selection. IEEE

transactions on pattern analysis and machine intelligence 38(11), 2182–2197 (2015)

8. Fr´enay, B., Verleysen, M.: Classiﬁcation in the presence of label noise: a survey. IEEE trans-

actions on neural networks and learning systems 25(5), 845–869 (2013)

9. Ghorbani, A., Kim, M., Zou, J.: A distributional framework for data valuation. In: Interna-

tional Conference on Machine Learning. pp. 3535–3544. PMLR (2020)

10. Ghorbani, A., Zou, J.: Data shapley: Equitable valuation of data for machine learning. In:

International Conference on Machine Learning. pp. 2242–2251. PMLR (2019)

11. Ghorbani, A., Zou, J.: Neuron shapley: Discovering the responsible neurons. arXiv preprint

arXiv:2002.09815 (2020)

12. Hara, S., Nitanda, A., Maehara, T.: Data cleansing for models trained with sgd. arXiv preprint

arXiv:1906.08473 (2019)

13. Koh, P.W., Liang, P.: Understanding black-box predictions via inﬂuence functions. In: Inter-

national Conference on Machine Learning. pp. 1885–1894. PMLR (2017)

14. Lundberg, S., Lee, S.I.: A uniﬁed approach to interpreting model predictions. arXiv preprint

arXiv:1705.07874 (2017)

15. Lundberg, S.M., Erion, G.G., Lee, S.I.: Consistent individualized feature attribution for tree

ensembles. arXiv preprint arXiv:1802.03888 (2018)

16. Pruthi, G., Liu, F., Kale, S., Sundararajan, M.: Estimating training data inﬂuence by tracing

gradient descent. Advances in Neural Information Processing Systems 33 (2020)

High-Value Training Data Selection

17

17. Steinhardt, J., Koh, P.W., Liang, P.: Certiﬁed defenses for data poisoning attacks. arXiv

preprint arXiv:1706.03691 (2017)

18. Wu, Y., Dobriban, E., Davidson, S.: Deltagrad: Rapid retraining of machine learning models.

In: International Conference on Machine Learning. pp. 10355–10366. PMLR (2020)

19. Yoon, J., Arik, S., Pﬁster, T.: Data valuation using reinforcement learning. In: International

Conference on Machine Learning. pp. 10842–10851. PMLR (2020)

