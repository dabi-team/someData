Automatic variational inference with cascading ﬂows

Luca Ambrogioni 1 Gianluigi Silvestri 2 1 Marcel van Gerven 1

Abstract

The automation of probabilistic reasoning is one
of the primary aims of machine learning. Re-
cently, the conﬂuence of variational inference and
deep learning has led to powerful and ﬂexible au-
tomatic inference methods that can be trained by
stochastic gradient descent. In particular, normal-
izing ﬂows are highly parameterized deep models
that can ﬁt arbitrarily complex posterior densities.
However, normalizing ﬂows struggle in highly
structured probabilistic programs as they need
to relearn the forward-pass of the program. Au-
tomatic structured variational inference (ASVI)
remedies this problem by constructing variational
programs that embed the forward-pass. Here, we
combine the ﬂexibility of normalizing ﬂows and
the prior-embedding property of ASVI in a new
family of variational programs, which we named
cascading ﬂows. A cascading ﬂows program in-
terposes a newly designed highway ﬂow archi-
tecture in between the conditional distributions
of the prior program such as to steer it toward
the observed data. These programs can be con-
structed automatically from an input probabilis-
tic program and can also be amortized automat-
ically. We evaluate the performance of the new
variational programs in a series of structured in-
ference problems. We ﬁnd that cascading ﬂows
have much higher performance than both normal-
izing ﬂows and ASVI in a large set of structured
inference problems.

1
2
0
2

b
e
F
9

]
L
M

.
t
a
t
s
[

1
v
1
0
8
4
0
.
2
0
1
2
:
v
i
X
r
a

1. Introduction

The aim of probabilistic programming is to provide a fully
automated software system for statistical inference on ar-
bitrary user-speciﬁed probabilistic models (also referred to
as probabilistic programs) (Milch et al., 2007; Sato, 1997;
Kersting and De Raedt, 2007; Pfeffer, 2001; Park et al.,

*Equal contribution 1Donders Centre for Cognition, Radboud
University, Netherlands 2One Planet, Netherlands. Correspon-
dence to: Luca Ambrogioni <l.ambrogioni@donders.ru.nl>.

Preprint.

2005; Goodman et al., 2012; Wingate et al., 2011; Patil
et al., 2010; Dillon et al., 2017; Bingham et al., 2019; Tran
et al., 2017; 2016). When the probabilistic programs are
built from differentiable components, stochastic variational
inference (VI) by automatic differentiation offers an effec-
tive and computationally efﬁcient solution (Bishop et al.,
2002; Kucukelbir et al., 2017; Tran et al., 2017). However,
the performance of stochastic VI depends strongly on the
choice of a variational program (also known as variational
family or variational guide) (Bishop and Winn, 2003).

Until recently, automatic variational program construction
was limited to simple mean ﬁeld and multivariate normal
coupling approaches (Kucukelbir et al., 2017). These pro-
grams are highly constrained and exploit only minimally
the structure of the original probabilistic program. For ex-
ample, the multivariate normal approach only depends on
the number of variables and their support. More recently,
normalizing ﬂows revolutionized the ﬁeld of variational
inference by offering a highly ﬂexible parametric model
for complex multivariate distributions. Normalizing ﬂows
can be easily used to construct automatic variational pro-
grams by mapping all the variables to a spherical normal
latent space through a learnable diffeomorphism (Rezende
and Mohamed, 2015; Papamakarios et al., 2019). While
highly ﬂexible, the normalizing ﬂow approach still uses
only minimal information concerning the input probabilistic
program. This leads to poor performance in highly struc-
tured probabilistic programs with complex computational
ﬂows (Ambrogioni et al., 2021). For example, consider an
inference problem deﬁned by a highly parameterized natural
language model (Brown et al., 2020). A typical problem
of this kind is controlled language generation, where the
goal is to steer the model to general text conditioned on its
past output and future targets (e.g. the presence of some
speciﬁc words). The controlled network can be formulated
in probabilistic programming terms as a posterior distribu-
tion that is parameterized by the variational program. In a
problem like this, most of the complexity of the posterior is
encoded in the structure of the prior probabilistic program
(the language model). The model just needs to be steered in
the right direction. However, normalizing ﬂow completely
disregards this structure and has to relearn all the structure
embedded in the prior.

This problem was discussed in a recent work that introduced

 
 
 
 
 
 
Automatic structured variational inference with cascading ﬂows

a form of automatic structured variational inference (ASVI)
with an automated variational program that incorporates the
prior probabilistic program as a special case. ASVI has been
shown to have very good performance in time series analysis
and deep learning problems in spite of being very parsimo-
niously parameterized compared to normalizing ﬂows (Am-
brogioni et al., 2021). However, the convex-update families
used in ASVI have two main limitations: I) they cannot
model statistical dependencies arising from colliders in the
graphical model (the "explaining away" effect) and II) they
constrain the conditional posterior distributions to be in the
same family as the prior. In this paper we integrate ASVI
and normalizing ﬂows to obtain a new form of automatic VI,
which we refer to as cascading ﬂows (CF). The approach
incorporates the forward-pass of the probabilistic program
while being capable of modeling collider dependencies and
arbitrarily complex conditional distributions. To this end,
we make use of a novel highway ﬂow architecture. CF
respects the following design principles:

• Automation: It can be constructed using a fully auto-
matic algorithm that takes the prior probabilistic pro-
gram as input.

• Locality: It is constructed by locally transforming each
conditional distribution in the conditional prior so that
it inherits the graphical structure of the input program
(with potentially some extra coupling). This ensures
scalability and modularity.

• Prior information ﬂow: It embeds the forward-pass of

the probabilistic program.

2. Related work

Automatic algorithms for VI date back to VIBES (Bishop
et al., 2002; Bishop and Winn, 2003). These older works
are based on self-consistency equations and variational mes-
sage passing (Winn and Bishop, 2005). The use of gradient-
based automatic VI for probabilistic programming was pi-
oneered in (Wingate and Weber, 2013) using a mean ﬁeld
approach. Automatic differentiation VI (ADVI) perfected
this approach by exploiting the automatic-differentiation
tools developed in the deep learning community (Kucukel-
bir et al., 2017). ADVI uses ﬁxed invertible transformations
to map each variable into an unbounded domain where the
density can be modeled with either univariate of multivari-
ate normal distributions. Structured stochastic VI (Hoffman
and Blei, 2015) exploits the local conjugate update rules
of locally conjugate models. This has the major beneﬁt of
preserving the local structure of the probabilistic program
but it severely limits the class of possible input programs
since local conjugacy is a stringent condition. Automatic
structured VI (ASVI) lifts this constraint as it applies a
trainable convex-update rule to non-conjugate links (Am-

brogioni et al., 2021). Several other papers introduced new
techniques for constructing structured variational programs.
Copula VI introduces the use of vine copulas to model the
statistical coupling between the latent variables in the pro-
gram (Tran et al., 2015). Similarly, hierarchical VI uses
auxiliary latent variables to achieve the same goal (Ran-
ganath et al., 2016). Neither of those approaches are truly
automatic however, as they do not prescribe a unique way
to construct the coupling functions.

Normalizing ﬂows are very expressive and highly param-
eterized models that use deep learning components to ap-
proximate complex multivariate densities (Rezende and Mo-
hamed, 2015). Normalizing ﬂows can approximate arbi-
trarily complex densities and can therefore be turned into
powerful automatic VI methods (Rezende and Mohamed,
2015; Kingma et al., 2016). However, conventional nor-
malizing ﬂows do not exploit the input probabilistic pro-
gram. Structured conditional continuous normalizing ﬂows
are a newly introduced class of ﬂows constrained to have
the same conditional independence structure of the true
posterior (Weilbach et al., 2020). However, these ﬂow ar-
chitectures only inherit the graphical structure of the input
program, without incorporating the forward pass (i.e. the
form of the conditional distributions and link functions).

Modern deep probabilistic programming frameworks pro-
vide automatic implementation of many of variational pro-
grams. For example, PyMC (Patil et al., 2010) provides an
implementation of ADVI, Pyro (Bingham et al., 2019) im-
plements the automatic construction of mean ﬁeld, multivari-
ate normal and normalizing ﬂow programs (i.e. variational
guides) and TensorFlow probability also offers an automatic
implementation of ASVI (Dillon et al., 2017; Ambrogioni
et al., 2021).

Our new highway ﬂow architecture is a key component
of our new approach and it is inspired by highway net-
works (Srivastava et al., 2015). To the best of our knowl-
edge, highway ﬂows are the ﬁrst form of residual-like ﬂows
with a tractable Jacobian determinant that can be expressed
in closed form. Existing residual ﬂows express the log de-
terminant of the Jacobian as an inﬁnite series that needs
to be estimated using a “Russian roulette” estimator (Chen
et al., 2019). We achieve tractability by applying a highway
operator to each single layer of the network.

Our use of auxiliary variables is similar to the approach
adopted in (Dupont et al., 2019) and (Weilbach et al., 2020).
The bound we use for training the augmented model was
initially derived in (Ranganath et al., 2016). To the best of
our knowledge we are the ﬁrst to exploit the statistics of the
auxiliary variables to implement inference amortization.

Structured variational programs have the most clear bene-
ﬁts in timeseries analysis problems (Eddy, 1996; Foti et al.,

Automatic structured variational inference with cascading ﬂows

2014; Johnson and Willsky, 2014; Karl et al., 2016; Fortu-
nato et al., 2017). Our approach, together with ASVI, differs
from the conventional timeseries approach by exploiting the
structure of both temporal and non-temporal variables in the
model.

3. Preliminaries

In this section we introduce the notation used in the rest of
the paper and discuss in detail the methods that form the
basis for our work.

3.1. Differentiable probabilistic programs

We denote the values of (potentially multivariate) random
variables using lowercase notation and arrays of random
variables using boldface notation. We consider probabilistic
programs constructed as a chain of conditional probabilities
over an array of random variables x = (x1, . . . , xN ). We
denote the array of values of the parents of the j-th variable
as πj ⊆ {xi}i(cid:54)=j, which is a sub-array of parent variables
such that the resulting graphical model is a directed acyclic
graph. Using this notation, we can express the joint density
of a probabilistic program as follows:

p (x) =

N
(cid:89)

j=1

ρj (xj | θj(πj)) ,

(1)

where θj(πj) is a link function that maps the values of the
parents of the j-th variable to the parameters of its density
ρj (· | ·). For example, if the density is Gaussian the link
function outputs the value of its mean and scale given the
values of the parents. Note that θj is a constant when the
array of parents is empty.

3.2. Convex-update variational programs

An automatic structured variational inference method pro-
vides an algorithm that takes a probabilistic program as in-
put and outputs a variational program q(x). Convex-update
variational programs (Ambrogioni et al., 2021) are deﬁned
by the following transformation:

p(x) (cid:55)→
CU

q(x) =

N
(cid:89)

j

U αj
λj

ρj (xj | θj(πj)) ,

(2)

with convex-update operator

U αj
λj

ρj (xj | θj(πj)) = ρj(xj |λj (cid:12) θj(πj)

(3)

+ (1 − λj) (cid:12) αj) ,

where λj and αj are (potentially vector-valued) learnable
parameters and (cid:12) is the element-wise product. This reduces
to the prior probabilistic program for λj → 0 and to a mean
ﬁeld variational program for λj → 1. Convex-update ASVI

has the advantage of preserving the forward-pass of the
probabilistic program, which is often a major source of sta-
tistical coupling in the posterior distribution. The variation
program can be trained to ﬁt the posterior by minimizing
the evidence lower bound (ELBO) by stochastic gradient
descent (SGD) on the differentiable parameters λj and αj.

3.3. Variational inference with normalizing ﬂows

Normalizing ﬂows express the density of arbitrary multi-
variate distributions as the push forward of a known base
distribution through a differentiable invertible function (i.e.
a diffeomorphism). The base distribution is usually a spheri-
cal normal distribution, hence the "normalizing" in the name.
Consider a d-variate probability density p0(z) and a diffeo-
morphism Ψ : Rd → Rd. We can derive the density of the
new random variable x = Ψ(z) using the change of variable
formula

pX (x) = | det J(Ψ−1(x))|p0(Ψ−1(x)) ,

(4)

where Ψ−1 is the inverse transformation and J(z) is the
Jacobi matrix of Ψ. We can write this more compactly
using the push-forward operator TΨ associated to Ψ that
maps the density p0(z) to the density pX (x) given in Eq. 4:
pX (x) = TΨ [p0(·)] (x).

Now consider a ﬂexible family of functions Ψw(x) parame-
terized by the "weights" w. We can use Eq. 13 to approx-
imate arbitrarily complicated target densities by training
w by SGD. Given a probabilistic program p(x, y) with y
observed, we can approximate the posterior p(x | y) by
minimizing the evidence lower bound:

ELBO(w) = Ew

(cid:20)

log

p(x, y)
| det Jw(Ψ−1(x))|p0(Ψ−1(x))

(cid:21)

(5)
where the expectation is taken with respect to the trans-
formed distribution TΨw [p0(·)] (x). Since Ψ is differen-
tiable, we can obtain the reparameterization gradient for the
ELBO as follows:

∇wELBO(w) = E0

(cid:20)
∇w log

p(Ψw(z), y)
| det Jw(z)|p0(z)

(cid:21)

(6)

where the expectation is now taken with respect to the ﬁxed
distribution p0(z). In the following we refer to this approach
as global ﬂow (GF) since the transformation is applied syn-
chronously to all variables in the model.

4. Variational inference with cascading ﬂows

Now that we have covered the relevant background, we
will introduce our new approach that combines ASVI with
normalizing ﬂows.

Automatic structured variational inference with cascading ﬂows

4.1. Cascading ﬂows variational programs

The convex-update parameterization poses strong con-
straints on the form of the conditional posteriors. On the
other hand, normalizing ﬂow programs do not embed the
potentially very complex structure of the input program. In
this paper we combine the two approaches by replacing
the convex-update operator in Eq. 2 with the push-forward
operator used in normalizing ﬂows. More precisely, con-
1 , . . . , ΨwN
sider a family of diffeomorphisms Ψw = (Ψw1
N )
depending on the array of parameters w = (w1, . . . , wN ).
Using these transformations, we can deﬁne the cascading
ﬂows variational program associated with a program deﬁned
by Eq. 1:

p(x) (cid:55)→
CF

qw (x) =

N
(cid:89)

j

T w
j

[ρj (· | θj(πj))] (xj) ,

(7)

where we denoted the push-forward operator induced by the
diffeomorphism Ψwj
to simplify
j
the notation. A cascading ﬂows variational program can be
trained by SGD using gradient estimators of the ELBO with
the reparameterization given in Eq. 6.

(as deﬁned by Eq. 4) as T w

j

The name cascading ﬂows comes from the fact that, since
the transformation is performed locally for each conditional
distribution, its effects cascade down the variational model
through the (transformed) conditional dependencies of the
prior program. However, in order to preserve the forward-
pass of the probabilistic program it is crucial to use a param-
eterized diffeomorphism that can be initialized around the
identity function and whose deviation from the identity can
be easily controlled. Speciﬁcally, we opt for transformations
of the following form:

Ψwj

j (x) = γx + (1 − γ)fj(x; wj) ,

(8)

where γ ∈ (0, 1). The resulting architecture of a cascading
ﬂows variational program is visualized in Fig 1. From the
diagram it is clear that a cascading ﬂow is constructed by
inserting transformation layers within the architecture of the
probabilistic program. Transformations of the form given
in Eq. 8 with tractable Jacobians are not currently present
in the normalizing ﬂow literature. Therefore in the next
subsection we will introduce a new ﬂow architecture of this
form, which we named highway ﬂow.

4.2. Highway ﬂow architecture

Here we introduce highway ﬂow blocks gated by a learn-
able scalar gate parameter. As also visualized in Fig. 2, a
highway ﬂow block is comprised by three separate highway
layers:

Figure 1. Diagram of a cascading ﬂows architecture. A) Detail of
architecture of an input probabilistic program. B) Detail of the
associated cascading ﬂows architecture.

Figure 2. Diagram of a highway ﬂow block.

1. Upper triangular highway layer:

lU (z; U, λ) = λz + (1 − λ) (U z + bU )

log det JU =

(cid:88)

k

log (λ + (1 − λ)Ukk)

2. Lower triangular layer:

lL(z; L, λ) = λz + (1 − λ) (Lz + bL)

log det JL =

(cid:88)

k

log (λ + (1 − λ)Lkk)

(9)

(10)

(11)

(12)

3. Highway activation functions:

f (z; λ) = λz + (1 − λ)g(z)
(cid:18)

log det

df (xk)
dx

(cid:88)

=

k

log

λ + (1 − λ)

(13)
(cid:19)

dg(xk)
dx

(14)

where U is an upper-triangular matrix with positive-valued
diagonal, L is a lower-triangular matrix with ones in the
diagonal and g(x) is a differentiable non-decreasing acti-
vation function. A highway ﬂow layer is a composition of
these three types of layers:

lh(·) = f ◦ lL ◦ lU (·) .

(15)

Layer jLayer j + 1Layer j + 2Layer jLayer j + 1Layer j + 2CF Layer jCF Layer j + 1A)B)Upper TriLower TriActivationAutomatic structured variational inference with cascading ﬂows

A highway network is a sequence of M highway layers with
a common gate parameter and different weights and biases.
Note that a highway ﬂow can be expressed in the form given
in Eq. 8 by deﬁning γ as follows:

γ = λ3M ,

(16)

which is clearly equal to one (zero) when λ is equal to one
(zero).

The highway ﬂow architecture can be easily adapted to the
augmented variable space. Since there is no need to gate the
ﬂow of the auxiliary variables, the scalar λ in Eq. 9, Eq. 11
and Eq. 13 should be replaced by the vector l whose entries
corresponding to the original variables are equal to λ while
the entries corresponding to the auxiliary variables are equal
to zero.

4.4. Modeling collider dependencies with auxiliary

4.3. Auxiliary variables and inﬁnite mixtures of ﬂows

coupling

The expressiveness of a normalizing ﬂow is limited by the
invertible nature of the transformation. This is particularly
problematic for low-dimensional variables since the expres-
sivity of many ﬂow architectures depends on the dimen-
sionality of the input (Papamakarios et al., 2019). This
limitation can be a major shortcoming in cascading ﬂows
as they are particularly useful in highly structured problems
characterized by a large number of coupled low-dimensional
variables. Fortunately, we can remedy this limitation by in-
troducing a set of auxiliary variables. For each variable
xj, we create a D-dimensional variable (cid:15)j following a base
distribution pj((cid:15)j). We can now use an augmented diffeo-
morphism ˆΨwj
j (xj, (cid:15)j) and deﬁne the joint posterior over
both xj and (cid:15)j:

q (xj, (cid:15)j | πj) = ˆT w
j

[ρj (· | θj(πj)) pj(·)] (xj, (cid:15)j) ,

(17)

where the push-forward operator now transforms the (inde-
pendent) joint density of xj and (cid:15)j. The conditional varia-
tional posterior is then obtained by marginalizing out (cid:15)j:

(cid:90)

q (xj | πj) =

q (xj, (cid:15)j | πj) d(cid:15)j .

(18)

This inﬁnite mixture of ﬂows is much more capable of mod-
eling complex and potentially multi-modal distributions.

The use of latent mixture models for VI was originally
introduced in (Ranganath et al., 2016). Given a set of obser-
vations y, the ELBO of any mixture variational probabilistic
program can be lower bounded using Jensen’s inequality:

(cid:20)

log

Ex

p(x, y)
(cid:82) q (x, (cid:15)) d(cid:15)

(cid:21)

≥ Ex,(cid:15)

(cid:20)

log

p(x, y)r((cid:15))
q (x, (cid:15))

(cid:124)

(cid:123)(cid:122)
Augmented ELBO

,

(19)

(cid:21)

(cid:125)

where q((cid:15)) = (cid:82) q (x, (cid:15)) dx is the marginal variational pos-
terior and r((cid:15)) is an arbitrarily chosen distribution over the
auxiliary variables. This result justiﬁes the use of the aug-
mented ELBO for training the variational posterior. The
gap between the mixture ELBO (Eq. 19) and the augmented
ELBO can be reduced by parameterizing r((cid:15)) and training
its parameters by minimizing the augmented ELBO such as
to match q((cid:15)).

Figure 3. Backward auxiliary coupling. A) Example graphical
model. B) Cascading ﬂows model coupled with reversed local
linear Gaussian model.

So far, we outlined a model that has the same conditional in-
dependence structure of the prior and is therefore incapable
of modeling dependencies arising from colliders. Fortu-
nately, the local auxiliary variables that we introduced in
order to increase the ﬂexibility of the local ﬂows can assume
a second role as "input ports" that can induce non-local cou-
plings. In fact, by coupling their auxiliary variables we can
induce statistical dependencies between any subset of vari-
ables in the model. Importantly, this coupling can take a very
simple form since additional complexity can be modeled by
the highway ﬂows. We will now introduce a local auxiliary
model with a non-trivial conditional independence structure
inspired by the forward-backward algorithm. The graphical
structure of the posterior probabilistic program is a sub-
graph of the moralization of the prior graph (Bishop, 2006).
Therefore, in order to be able to capture all the dependencies
in the posterior, it sufﬁces to couple the auxiliary variables
by reversing the arrows of the original prior model. In fact,
all the parents of every node get coupled by the reversed
arrows. As an example, consider the following probabilistic
model (Fig. 3A): ρ1(x1 | x2, x3)ρ2(x2)ρ3(x3 | x4)ρ4(x4) .
We couple the auxiliary variables by reversing the arrows, as
formalized in the graphical model p1((cid:15)1)p2((cid:15)2, | (cid:15)1)p3((cid:15)3, |
(cid:15)1)p4((cid:15)4 | (cid:15)3) . The graphical structure of the resulting aug-
mented model is shown in Fig. 3B. The auxiliary variables
can be coupled in a very simple manner since the diffeo-

A)B)Automatic structured variational inference with cascading ﬂows

Table 1. Predictive and latent log-likelihood (forward KL) of variational timeseries models. Error are SEM estimated over 10 repetitions.

CF

ASVI

MF

GF

MVN

CF (non-res)

BR-r

BR-c

LZ-r

LZ-c

PD-r

PD-c

RNN-r

RNN-c

Pred
Latent

−2.27 ± 0.26 −2.23 ± 0.21 −3.79 ± 0.82
−1.48 ± 0.19 −1.45 ± 0.14 −4.02 ± 0.63

Pred
Latent −1.53 ± 0.21

1.61 ± 0.18

1.45 ± 0.14
−1.55 ± 0.19

1.04 ± 0.03
−5.78 ± 0.89

−2.81 ± 0.56
−2.41 ± 0.52

2.00 ± 0.29
−2.06 ± 0.53

−2.88 ± 0.53
−2.02 ± 0.48

1.02 ± 0.03
−2.82 ± 0.77

−3.33 ± 0.65
−3.63 ± 0.74

1.31 ± 0.18
−5.07 ± 0.85

−2.89 ± 0.17
Pred
Latent −2.39 ± 0.45

−4.48 ± 0.60
−4.38 ± 0.67

−8.26 ± 0.28
−10.28 ± 0.18

−8.03 ± 0.37
−9.44 ± 0.20

−8.24 ± 0.29
−8.25 ± 0.27
−9.45 ± 0.22 −10.00 ± 0.18

5.10 ± 0.52
Pred
Latent −4.19 ± 0.66

−3.19 ± 0.22
Pred
Latent −2.32 ± 0.19

1.97 ± 0.07
Pred
Latent −2.77 ± 0.18

0.92 ± 0.03
−7.47 ± 0.30

−3.25 ± 0.11
−3.14 ± 0.12

1.65 ± 0.06
−3.09 ± 0.15

0.90 ± 0.003
−9.89 ± 0.19

−4.42 ± 0.22
−9.12 ± 0.29

0.86 ± 0.003
−8.40 ± 0.43

0.86 ± 0.15
−8.71 ± 0.32

−3.84 ± 0.28
−4.16 ± 0.33

01.07 ± 0.02
−6.20 ± 0.40

0.89 ± 0.001
−8.58 ± 0.34

−4.30 ± 0.22
−7.72 ± 0.30

1.09 ± 0.02
−7.45 ± 0.42

0.88 ± 0.04
−9.59 ± 0.29

−4.29 ± 0.25
−8.27 ± 0.36

0.96 ± 0.01
−8.41 ± 0.43

−1.68 ± 0.05
Pred
Latent −1.34 ± 0.33

−2.30 ± 0.18
−1.95 ± 0.35

−5.20 ± 0.94 −1.60 ± 0.09 −4.47 ± 0.92
−6.39 ± 1.27
−10.30 ± 0.20

−1.97 ± 0.21
−6.61 ± 0.50 −10.47 ± 0.22

Pred
Latent

5.77 ± 1.40
−2.30 ± 0.61 −2.05 ± 0.32 −10.22 ± 0.29 −10.75 ± 0.15 −10.22 ± 0.29 −11.22 ± 0.04

1.05 ± 0.06

0.81 ± 0.03

2.81 ± 0.36

0.86 ± 0.02

1.39 ± 0.04

morphisms can add arbitrary complexity to the ﬁnal joint
density. Here we use a simple linear normal model:

(cid:15)k | υk =

K
(cid:88)

j=1

aj (cid:12) υj + a0 (cid:12) ξk ,

(20)

where aj ≥ 0 and (cid:80)
n an = 1. In this expression, ξk is a
standard Gaussian vector and υk = (υ1, ..., υK) is an array
containing all the parents auxiliary variables (the auxiliaries
of the children of xk in the original graph).

4.5. Inference amortization

We will now discuss a strategy to implement automatic infer-
ence amortization in cascading ﬂows models. The problem
of inference amortization is closely connected to the prob-
lem of modeling collider dependencies as the latter arise
from the backward messages originating from observed
nodes (Bishop, 2006). It is therefore not a coincidence that
we can implement both collider dependencies and amorti-
zation by shifting the statistics of the local auxiliary vari-
ables. This results in a very parsimonious amortization
scheme where data streams are fed locally to the various
nodes and propagate backward through the linear network
of auxiliary Gaussian variables. We denote the set of aux-
iliary variables associated to the children of xk in the in-
put probabilistic program as υk. Furthermore, we denote
the observed value of xk as yk and the set of observed
values of the j-th child of xk as yk = (yk
K). The
amortized auxiliary model can then be obtained as a mod-
iﬁcation of Eq. 20: p0((cid:15)k | υk) = N (cid:0)mk, σ2I(cid:1) with
(cid:15)k | υk = B(k)[yk] + (cid:80)K
j=1 aj (cid:12) υj + a0 (cid:12) ξk , where
B(k) are learnable afﬁne transformations (i.e. linear layers

1 , ..., yk

Figure 4. Visualization of the backward amortization algorithm.

in deep learning lingo).

5. Experiments

Our experiments are divided into three sections. In the ﬁrst
section we focus on highly structured timeseries problems
exhibiting linear and non-linear dynamics. Bayesian prob-
lems of this nature have relevance for several scientiﬁc ﬁelds
such as physics, chemistry, biology, neuroscience and pop-
ulation dynamics. In the second section we compare the
capacity of several variational programs to model depen-

Auxiliary graphForward pass connectionAuxiliary connectionAmortization inputLatent nodeObservable nodeAuxiliary nodeAutomatic structured variational inference with cascading ﬂows

dencies arising from colliders in the graphical structure. In
fact, the inability of modeling collider dependencies was the
main limitation of the original convex-update ASVI fam-
ily (Ambrogioni et al., 2021). Finally, in the third section we
test the performance of the automatic amortization scheme.
All the code used in the experiments is included in the
supplementary material and documented in Supplementary
A. Architectures: In all experiments, the CF architectures
were comprised of three highway ﬂow blocks with softplus
activation functions in each block except for the last which
had linear activations. CF programs use an independent
network of this form for each latent variable in the program.
Each variable was supplemented with 10 auxiliary variables,
the width of each network was therefore equal to the dimen-
sionality of the variable plus 10. Weights and biases were
initialized from centered normal distributions with scale
0.01. The λ variable was deﬁned independently for each
network as the logistic sigmoid of a leanable parameter l,
which was initialized as 4 in order to keep the variational
program close to the input program. For each variable, the
posterior auxiliary distributions r((cid:15)) (see Eq. 19) were spher-
ical normals parameterized by mean and standard deviation
(i.e. Gaussian mean ﬁeld).

Figure 5. Example of qualitative results in the Lorentz system (ﬁrst
coordinate). The gray shaded area is observed through noisy mea-
surements. The dashed lines are the ground-truth timeseries while
the thin colored lines are samples from the trained variational
program.

5.1. Timeseries analysis

We consider discretized SDEs, deﬁned by conditional densi-
ties of the form:

ρt(xt+1 | xt) = N (cid:0)xt+1; µ(xt, t)dt, σ2(xt, t)dt(cid:1) , (21)

where µ(xt, t) is the drift function and σ(xt, t) is the volatil-
ity function. Speciﬁcally, we used Brownian motions (BR:
µ(x) = x, σ2(x) = 1), Lorentz dynamical systems (LZ:
µ(x1, x2, x3) = (10(x2 − x1), x1(28 − x3) − x2, x1x2 −
8/3x3), σ2 = 2), Lotka-Volterra population dynamics
models (PD: µ(x1, x2) = (0.2x1 − 0.02x1x2, 0.1x1x2 −
0.1x2), σ2 = 3 ) and recurrent neural dynamics with ran-
domized weights (RNN, see Supplementary B). The initial
distributions were spherical Gaussians (see Supplementary
B for the parameters). For each dynamical model we gen-
erated noisy observations using Gaussian emission models
(denoted with an "r" in the tables):

p(yt | xt) = N ([xt]1, σ2

lk)

(22)

or Bernoulli logistic models ("c" in the tables):

p(yt | xt) = Bernoulli(g(k[xt]1)) ,

(23)

where [xt]1 denotes the ﬁrst entry of the vector xt, σlk
is the standard deviation of the Gaussian emission model,
g(·) is the logistic sigmoid function and k is a gain fac-
tor. All the numeric values of the parameters in different
experiments are given in Supplementary B. For each task
we evaluate the performance of the cascading ﬂows pro-
grams (CF) against a suite of baseline variational posteriors,
including convex-update ASVI (Ambrogioni et al., 2021),
mean ﬁeld (MF) and multivariate normal (MN) (Kucukelbir
et al., 2017). We also compare our new cascading ﬂows
family with the more conventional global normalizing ﬂow
(GF) approach where a single diffeomorphism transforms
all the latent variables in the probabilistic program into
a spherical normal distribution (Rezende and Mohamed,
2015). For the sake of comparison, we adopt an archi-
tecture identical to our highway ﬂow model (including 10
auxiliary variables) except for the absence of the highway
gates. Note however that the global architecture has much
greater width as it is applied to all variables in the pro-
gram. Finally, we compare the performance of CF with a
modiﬁed version of CF that does not use highway gates.
Ground-truth multivariate timeseries ˜x = (˜x1, . . . , ˜xT )
were sampled from the generative model together with sim-
ulated ﬁrst-half observations y1:T /2 = (y1, . . . , yT /2) and
second-half observations yT /2:T = (yT /2+1, . . . , yT ). All
the variational models were trained conditioned only on
the ﬁrst half observations. Performance was assessed us-
ing two metrics. The ﬁrst metric is the average marginal
log-probability of the ground-truth given the variational pos-
j log q([˜xt]j | y1:T /2) , where J is the
terior:
dimensionality of the ground-truth vector ˜xt. The marginal
posterior density q([˜xt]j | y1:T /2) was estimated from 5000
samples drawn from the variational program using Gaus-
sian kernel density estimation with bandwidth 0.9ˆσN −1/5 ,
where ˆσ is the empirical standard deviation of the samples.
Our second metric is log p(yT /2:T | y1:T /2): the predic-
tive log-probability of the ground-truth observations in the

(cid:80)T
t

(cid:80)J

1
T J

CFASVI00.80.400.80.400.80.400.80.4RegressionClassificationAutomatic structured variational inference with cascading ﬂows

Table 2. Multivariate latent log-likelihood (forward KL) of variational binary tree models. Error are SEM estimated over 15 repetitions.
MF
0.6 ± 0.01
1.68 ± 0.02
0.24 ± 0.01

ASVI
0.49 ± 0.05
3.56 ± 0.26

CF
1.24 ± 0.01
5.42 ± 0.35
1.20 ± 0.01 −7.86 ± 3.47
3.29 ± 3.17

GF
0.97 ± 0.01
6.56 ± 0.04
0.90 ± 0.01
10.32 ± 0.05

MN
1.00 ± 0.01
6.52 ± 0.01
1.15 ± 0.01
5.37 ± 2.61

Linear-2
Linear-4
Tanh-2
Tanh-4

−55.44 ± 4.53 −43.24 ± 8.87

second half of the timeseries given the observations in the
ﬁrst half, estimated using kernel density estimation from
5000 predicted observation samples drawn from the exact
likelihood conditioned on the samples from the variational
program. Each experiment was repeated 10 times. In each
repetition, all the variational programs were re-trained for
8000 iterations (enough to ensure convergence in all meth-
ods) given new sampled ground-truth timeseries and ob-
servations. The reported results are the mean and SEM of
the two metrics over these 10 repetitions. Results: The
results are given in Table 1. The cascading ﬂows family
(CF) outperforms all other approaches by a large margin in
all non-linear tasks. Convex-update ASVI consistently has
the second highest performance, easily outperforming the
highly parameterized global normalizing ﬂow (GF). This
result is consistent with (Ambrogioni et al., 2021), where
ASVI was shown to outperform inverse autoregressive ﬂow
architectures (Kingma et al., 2016). Finally, the CF model
without highway gates (CF non-res) has very low perfor-
mance, proving the crucial importance of our new highway
ﬂow architecture. A qualitative comparison (LZ-r and LZ-c)
between CF and ASVI is shown in Fig. 5.

5.2. Binary tree experiments

j2

j1

and πd−1

j1 , πd−1

In our second experiment we test the capability of the vari-
ational programs to model statistical dependencies arising
from colliders. We tested this in a Gaussian binary tree
model where the mean of a scalar variable xd
j in the d-th
layer is a function of two variables in the d − 1-th layer:
j ∼ N (cid:0)link(πd−1
j2 ), σ2(cid:1) , where πd−1
xd
are
the two parents of xd
j . All the 0-th layer variables have
zero mean. The inference problem consists of observing
the only variable in the last layer and computing the pos-
terior of all the other variables. We considered a setting
with linear coupling link(x, y) = x − y and tanh coupling
link(x, y) = tanh (x) − tanh (y) with 2 and 4 layers. Per-
formance was estimated using the posterior log-probability
of the ground-truth sample estimated using a multivariate
Gaussian approximation that matches the empirical mean
and covariance matrix of 5000 samples drawn from the
trained variational program. We compared performance
with ASVI, mean-ﬁeld (MF), global ﬂow (GF) and multi-
variate normal (MN). Note that neither ASVI nor MF can
model collider dependencies. Full details are given in Sup-
plementary C. The reported results are the mean and SEM

Table 3. Latent log-likelihood (forward KL) of amortized time-
series models. Error are SEM estimated over 50 repetitions.

CF

GF
PD −2.8 ± 0.2 −8.0 ± 0.2
−5.1 ± 0.2
LZ −2.7 ± 0.2 −10.0 ± 0.2 −9.1 ± 0.2

MF

of the two metrics over 15 repetitions. Results: The results
are given in Table 2. The CF model always outperforms
ASVI and MF, proving that it can capture collider dependen-
cies. CF has the highest performances in shallow models
but lower performance than GF and MN in deeper models.

5.3. Inference amortization

Finally, we evaluate the performance of the amortization
scheme in non-linear timeseries inference problems. We
used the population dynamics (PD) and the Lorentz (LZ)
systems. The CF program was amortized using the approach
described in Sec. 4.5. The MF and normalizing ﬂow base-
lines were amortized using inference networks (see Supple-
mentary D). All the details were identical to the timeseries
experiments except for the fact that all time points were
observed. Performance was again quantiﬁed as the average
marginal log-probability of the ground-truth kernel density
estimated from the samples from the amortized variational
programs. Each amortized program was trained once and
tested on 50 new sampled timeseries/observations pairs. The
reported results are the mean and SEM of these 50 repeti-
tions. Results: The results are reported in Table 3. Like in
the non-amortized case, CF greatly outperforms both MF
and GF in both timeseries inference problems.

6. Discussion

Our CF approach solves the two main limitations of ASVI
as it can model collider dependencies and can be amortized
automatically. We show that cascading ﬂows programs
have extremely high performance in structured inference
problems. However, their ability to capture deep collider de-
pendencies is lower than normalizing ﬂows. In our opinion,
designing principled ways to model collider dependencies
is a promising research direction.

Automatic structured variational inference with cascading ﬂows

References

B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and
A. Kolobov. Blog: Probabilistic models with unknown
objects. Statistical Relational Learning, page 373, 2007.

T. Sato. Prism: A symbolic-statistical modeling language.
In Proceeding of the Intl. Joint Conference on Artiﬁcial
Intelligence, 1997.

K. Kersting and L. De Raedt. Bayesian logic programming:
Theory and tool. Statistical Relational Learning, page
291, 2007.

A. Pfeffer. Ibal: A probabilistic rational programming lan-
guage. In Proceeding of the Intl. Joint Conference on
Artiﬁcial Intelligence, 2001.

S. Park, F. Pfenning, and S. Thrun. A probabilistic language
based upon sampling functions. ACM SIGPLAN Notices,
40(1):171–182, 2005.

N. Goodman, V. Mansinghka, D. M. Roy, K. Bonawitz, and
J. B. Tenenbaum. Church: A language for generative
models. arXiv preprint arXiv:1206.3255, 2012.

D. Wingate, A. Stuhlmüller, and N. Goodman. Lightweight
implementations of probabilistic programming languages
via transformational compilation. In Proceedings of the
International Conference on Artiﬁcial Intelligence and
Statistics, 2011.

A. Patil, D. Huard, and C. J. Fonnesbeck. Pymc: Bayesian
stochastic modelling in python. Journal of Statistical
Software, 35(4):1, 2010.

J. V Dillon, I. Langmore, D. Tran, E. Brevdo, S. Vasude-
van, D. Moore, B. Patton, A. Alemi, M. Hoffman, and
R. A Saurous. Tensorﬂow distributions. arXiv preprint
arXiv:1711.10604, 2017.

E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer,
N. Pradhan, T. Karaletsos, R. Singh, P. Szerlip, P. Hors-
fall, and N. D. Goodman. Pyro: Deep universal proba-
bilistic programming. The Journal of Machine Learning
Research, 20(1):973–978, 2019.

D. Tran, M. D. Hoffman, R. A. Saurous, E. Brevdo, K. Mur-
phy, and D. M. Blei. Deep probabilistic programming.
arXiv preprint arXiv:1701.03757, 2017.

D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang,
and D. M. Blei. Edward: A library for probabilis-
tic modeling, inference, and criticism. arXiv preprint
arXiv:1610.09787, 2016.

C. Bishop, D. Spiegelhalter, and J. Winn. Vibes: A varia-
tional inference engine for bayesian networks. Neural
Information Processing Systems, 15:793–800, 2002.

A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M.
Blei. Automatic differentiation variational inference. The
Journal of Machine Learning Research, 18(1):430–474,
2017.

C. Bishop and J. Winn. Structured variational distributions
in vibes. In International Conference on Artiﬁcial Intelli-
gence and Statistics, 2003.

D. J. Rezende and S. Mohamed. Variational inference with
normalizing ﬂows. In International Conference on Ma-
chine Learning, 2015.

G. Papamakarios, E. Nalisnick, Danilo J. Rezende, S. Mo-
hamed, and B. Lakshminarayanan. Normalizing ﬂows
for probabilistic modeling and inference. arXiv preprint
arXiv:1912.02762, 2019.

L. Ambrogioni, K. Lin, E. Fetig, S. Vikram, M. Hinne,
D. Moore, and M. van Gerven. Automatic structured
In International Conference on
variational inference.
Artiﬁcial Intelligence and Statistics, 2021.

T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners.
Neural Information Processing Systems, 2020.

J. Winn and C. M. Bishop. Variational message passing.
Journal of Machine Learning Research, 6(Apr):661–694,
2005.

D. Wingate and T. Weber. Automated variational in-
ference in probabilistic programming. arXiv preprint
arXiv:1301.1299, 2013.

M. D. Hoffman and D. M. Blei. Structured stochastic varia-
tional inference. In Proceedings of the Artiﬁcial Intelli-
gence and Statistics, pages 361–369, 2015.

D. Tran, D. Blei, and E. M. Airoldi. Copula variational
inference. In Neural Information Processing Systems,
2015.

R Ranganath, D Tran, and D Blei. Hierarchical variational
models. In International Conference on Machine Learn-
ing, pages 324–333, 2016.

D. P Kingma, T. Salimans, R. Jozefowicz, X. Chen,
I. Sutskever, and M. Welling. Improved variational infer-
ence with inverse autoregressive ﬂow. In Neural Informa-
tion Processing Systems, 2016.

C Weilbach, B Beronov, F Wood, and W Harvey. Structured
conditional continuous normalizing ﬂows for efﬁcient
amortized inference in graphical models. In International
Conference on Artiﬁcial Intelligence and Statistics, pages
4441–4451. PMLR, 2020.

Automatic structured variational inference with cascading ﬂows

R K Srivastava, K Greff, and J Schmidhuber. Highway

networks. arXiv preprint arXiv:1505.00387, 2015.

R. T.Q. Chen, J. Behrmann, D. K. Duvenaud, and J. Jacob-
sen. Residual ﬂows for invertible generative modeling.
In Neural Information Processing Systems, pages 9916–
9926, 2019.

E Dupont, A Doucet, and Y W Teh. Augmented neural
ODEs. In Neural Information Processing Systems, pages
3140–3150, 2019.

S. R. Eddy. Hidden Markov models. Current Opinion in

Structural Biology, 6(3):361–365, 1996.

N. Foti, J. Xu, D. Laird, and E. Fox. Stochastic variational
inference for hidden Markov models. In Neural Informa-
tion Processing Systems, 2014.

M. Johnson and A. Willsky. Stochastic variational infer-
ence for Bayesian time series models. In International
Conference on Machine Learning, 2014.

M. Karl, M. Soelch, Ju. Bayer, and P. Van der Smagt.
Deep variational Bayes ﬁlters: Unsupervised learning
of state space models from raw data. arXiv preprint
arXiv:1605.06432, 2016.

M. Fortunato, C. Blundell, and O. Vinyals. Bayesian recur-
rent neural networks. arXiv preprint arXiv:1704.02798,
2017.

C. M. Bishop. Pattern recognition and machine learning.

Springer, 2006.

Automatic structured variational inference with cascading ﬂows

A. Experiments

The experiments where implemented in Python 3.8 using the packages PyTorch, Numpy and Matplotlib. The implementation
code is contained in the "modules" folder. The probabilistic program objects are deﬁned in the ﬁle "models.py" and the deep
architectures are in "networks.py". The experiments can be run from the ﬁles "timeseries experiment.py", "collider linear
experiments.py", "collider tanh experiments.py" and "amortized experiments.py" in the "experiments" folder.

B. Models

In the experiments, we use the following dynamical models.

B.1. Brownian motion (BR)

Dimensionality:
Drift:
Diffusion:
Initial density:
Time horizon:
Time step:
Regression noise:
Classiﬁcation gain:

J = 1
µ(x) = x
σ2(x) = 1
p(x0) = N (0, 1)
T = 40
dt = 1
σlk = 1
k = 2

B.2. Lorentz system (LZ)

Dimensionality:
Drift:
Diffusion:
Initial density:
Time horizon:
Time step:
Regression noise:
Classiﬁcation gain:

J = 3
µ(x1, x2, x3) = (10(x2 − x1), x1(28 − x3) − x2, x1x2 − 8/3x3)
σ2(x) = 22
p(x0) = N (3, 202)
T = 40
dt = 1
σlk = 3
k = 2

B.3. Population dynamics (PD)

Dimensionality:
Drift:
Diffusion:
Initial density:
Time horizon:
Time step:
Regression noise:
Classiﬁcation gain:

J = 2
µ(x1, x2, x3) = (ReLu(0.2x1 − 0.02x1x2), ReLu(0.1x1x2 − 0.1x2))
σ2(x) = 2
p(x0) = N (0, 1)
T = 100
dt = 0.02
σlk = 3
k = 2

Note: The ReLu activation was added to avoid instabilities arising from negative values.

B.4. Recurrent neural network (RNN)

Dimensionality:
Drift:
Diffusion:
Initial density:
Time horizon:
Time step:
Regression noise:
Classiﬁcation gain:

J = 3
µ(x) = tanh(W3 tanh(W2 tanh(W1x + b1) + b2))
σ2(x) = 0.12
p(x0) = N (0, 1)
T = 40
dt = 0.02
σlk = 1
k = 2

Automatic structured variational inference with cascading ﬂows

Note: W1 is a 2 × 5 matrix, W2 is a 5 × 5 matrix, W3 is a 5 × 2 matrix, b1 is a 2D vector and b2 is a 5D vec-
tor. All the entries of these quantities are sampled from normal distributions with mean 0 and standard deviation (SD) 1.
These parameters we re-sampled for every repetition of the experiment.

C. Collider dependencies

We tested performance under collider dependencies using a Gaussian binary tree model where the mean of a scalar variable
xd
j in the d-th layer is a function of two variables in the d − 1-th layer:

j ∼ N (cid:0)link(πd−1
xd

j1 , πd−1

j2 ), σ2(cid:1)

j1

and πd−1

where πd−1
SD σ was 0.15 and the initial SD was 0.2. In non-linear experiment the link was link(πd−1
The SD σ was 0.05 and the initial SD was 0.1.

j . In the linear experiment the link was link(πd−1
j1 , πd−1

are the two parents of xd

j2

j1 , πd−1
j2 ) = πd−1
j2 ) = tanh πd−1

j1 − πd−1
j2 The
j1 −tanh πd−1
j2 .

D. Amortized experiments

The amortized experiments had the same details and parameters as the corresponding timeseries experiment. We use the
following inference network for the MF and GF baselines (PyTorch code):

In the case of GF, the network provided means and scales of the pre-transformation normal variables.

