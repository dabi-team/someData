0
2
0
2

p
e
S
1
2

]
S
D
.
s
c
[

1
v
7
1
2
0
1
.
9
0
0
2
:
v
i
X
r
a

A Faster Interior Point Method for Semideﬁnite Programming

Haotian Jiang∗

Tarun Kathuria†

Yin Tat Lee‡

Swati Padmanabhan§

Zhao Song¶

Abstract

Semideﬁnite programs (SDPs) are a fundamental class of optimization problems with im-
portant recent applications in approximation algorithms, quantum complexity, robust learning,
algorithmic rounding, and adversarial deep learning. This paper presents a faster interior point
method to solve generic SDPs with variable size n

n and m constraints in time

O(√n(mn

2

ω

+ m

) log(1/ǫ)),

×
ω
+ n

where ω is the exponent of matrix multiplication and ǫ is the relative accuracy. In the predom-
n, our runtime outperforms that of the previous fastest SDP solver, which is
inant case of m
based on the cutting plane method [JLSW20].

≥

e

Our algorithm’s runtime can be naturally interpreted as follows:

O(√n log(1/ǫ)) is the num-
ber of iterations needed for our interior point method, mn2 is the input size, and mω + nω is the
time to invert the Hessian and slack matrix in each iteration. These constitute natural barriers
to further improving the runtime of interior point methods for solving generic SDPs.

e

∗jhtdavid@uw.edu. University of Washington.
†tarunkathuria@berkeley.edu. University of California, Berkeley.
‡yintat@uw.edu. University of Washington.
§pswati@uw.edu. University of Washington
¶zhaos@ias.edu. Princeton University and Institute for Advanced Study.

 
 
 
 
 
 
Contents

1 Introduction

1.1 Our results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Technique overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.1
Interior point method for solving SDPs . . . . . . . . . . . . . . . . . . . . . .
1.2.2 Our techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.3 Bottlenecks of our interior point method . . . . . . . . . . . . . . . . . . . . .
1.2.4 LP techniques unlikely to improve SDP runtime . . . . . . . . . . . . . . . . .
1.3 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Preliminaries

2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Useful facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Matrix Multiplication

3.1 Exponent of matrix multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Technical results for matrix multiplication . . . . . . . . . . . . . . . . . . . . . . . .
3.3 General upper bound on
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Speciﬁc upper bound on

Tmat(m, n2, m)

Tmat(n, mn, n) and
Tmat(m, n2, m)

4 Main Theorem

5 Approximate Central Path via Approximate Hessian

5.1 Main result for approximate central path . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Approximate slack update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Closeness of slack implies closeness of Hessian . . . . . . . . . . . . . . . . . . . . . .
5.4 Approximate Hessian maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5
Invariance of Newton step size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Approximate optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Low-rank Update

7 Runtime Analysis

8 Comparison with Cutting Plane Method

9 Initialization

A Matrix Multiplication: A Tensor Approach

A.1 Exponent of matrix multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Matrix multiplication tensor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.3 Implication of matrix multiplication technique . . . . . . . . . . . . . . . . . . . . . .
Tmat(m, n2, m) . . . . . . . . . . . . . . . . . .
A.4 General bound on

Tmat(n, mn, n) and

1

2
3
4
4
6
8
9
10

11
11
11

12
12
12
14
15

16

16
16
18
19
20
20
20

21

25

29

29

36
36
36
37
39

1 Introduction

Semideﬁnite programs (SDPs) constitute a class of convex optimization problems that optimize a
linear objective over the intersection of the cone of positive semideﬁnite matrices with an aﬃne space.
SDPs generalize linear programs and have a plethora of applications in operations research, control
theory, and theoretical computer science [VB96]. Applications in theoretical computer science in-
clude improved approximation algorithms for fundamental problems (e.g., Max-Cut [GW95], color-
ing 3-colorable graphs [KMS94], and sparsest cut [ARV09]), quantum complexity theory [JJUW11],
robust learning and estimation [CG18, CDG19, CDGW19], and algorithmic discrepancy and round-
n and m constraints:
ing [BDG16, BG17, Ban19]. We formally deﬁne SDPs with variable size n

×

Deﬁnition 1.1 (Semideﬁnite programming). Given symmetric1 matrices C, A1,
and bi ∈

[m], the goal is to solve the convex optimization problem

R for all i

∈

, Am ∈

· · ·

Rn

n

×

where

A, B
h

i

:=

subject to X

max

C, X
h

i
(cid:23)
i,j Ai,jBi,j is the trace product.

0,

Ai, X
h

i

= bi ∀

i

∈

[m]

(1)

P

Cutting plane and interior point methods Two prominent methods for solving SDPs, with
runtimes depending logarithmically on the accuracy parameter ǫ, are the cutting plane method and
the interior point method.

The cutting plane method maintains a convex set containing the optimal solution.

In each
iteration, the algorithm queries a separation oracle, which returns a hyperplane that divides the
convex set into two subsets. The convex set is then updated to contain the subset with the optimal
solution. This process is repeated until the volume of the maintained set becomes small enough and
a near-optimal solution can be found. Since Khachiyan proved [Kha80] that the ellipsoid method
solves linear programs in polynomial time, cutting plane methods have played a crucial role in both
discrete and continuous optimization [GLS81, GV02].

In contrast, interior point methods add a barrier function to the objective and, by adjusting the
weight of this barrier function, solve a diﬀerent optimization problem in each iteration. The solutions
to these successive problems form a well-deﬁned central path. Since Karmarkar proved [Kar84] that
interior point methods can solve linear programs in polynomial time, these methods have become
an active research area. Their number of iterations is usually the square root of the number of
dimensions, as opposed to the linear dependence on dimensions in cutting plane methods.

Since cutting plane methods use less structural information than interior point methods, they
are slower at solving almost all problems where interior point methods are known to apply. However,
SDPs remain one of the most fundamental optimization problems where the state of the art is, in
fact, the opposite: the current fastest cutting plane methods2 of [LSW15, JLSW20] solve a general
SDP in time m(mn2 +m2 +nω), while the fastest SDP solvers based on interior point methods in the
work of [NN92] and [Ans00] achieve runtimes of √n(m2n2 +mnω +mω) and (mn)1/4(m4n2 +m3nω),
[n, n2] (see Table 1.2). This
respectively, which are slower in the most common regime of m
apparent paradox raises the following natural question:

∈

How fast can SDPs be solved using interior point methods?
1We can assume that C, A1, · · · , Am are symmetric, since given any M ∈ {C, A1, · · · , Am}, we have Pi,j Mij Xij =

Pi,j Mij Xji = Pi,j (M ⊤)ijXij , and therefore we can replace M with (M + M ⊤)/2.

2[JLSW20] improves upon the runtime of [LSW15] in terms of the dependence on log(n/ǫ), while the polynomial

factors are the same in both runtimes.

2

1.1 Our results

We present a faster interior point method for solving SDPs. Our main result is the following theorem,
the formal version of which is given in Theorem 4.1.

Theorem 1.2 (Main result, informal). There is an interior point method that solves a general SDP
n and m constraints in time3 O∗(√n(mn2 + mω + nω)).
with variable size n

×

Our runtime can be roughly interpreted as follows:

• √n is the iteration complexity of the interior point method with the log barrier function.

• mn2 is the input size.

• mω is the cost of inverting the Hessian of the log barrier.

• nω is the cost of inverting the slack matrix.

Thus, the terms in the runtime of our algorithm arise as a natural barrier to further speeding up
SDP solvers. See Section 1.2.2, 1.2.3, and 1.2.4 for more detail.

Table 1.1 compares our result with previous SDP solvers. The ﬁrst takeaway of this table and
Theorem 1.2 is that our interior point method always runs faster than that in [NN92] and is faster
√n,
than that in [NN94] and [Ans00] when m
our interior point method is faster than the current fastest cutting plane method [LSW15, JLSW20].
n2 is satisﬁed in most SDP applications known to us, such as classical
We note that n
combinatorial optimization problems over graphs, experiment design problems in statistics and
machine learning, and sum-of-squares problems. An explicit comparison to previous algorithms in
the cases of m = n and m = n2 is shown in Table 1.2.

n1/13. A second consequence is that whenever m

m

≥

≤

≤

≥

Year References
[Sho77, YN76, Kha80] CPM
1979
CPM
[KTE88, NN89]
1988
CPM
[Vai89a]
1989
IPM
[NN92]
1992
IPM
[NN94, Ans00]
1994
CPM
[KM03]
2003
CPM
[LSW15]
2015
CPM
2020
[JLSW20]
IPM
2020 Our result

Method #Iters Cost per iter
mn2 + m2 + nω
mn2 + m3.5 + nω
mn2 + mω + nω
m2n2 + mnω + mω

m2
m
m
√n
(mn)1/4 m4n2 + m3nω
m
m
m
√n

mn2 + mω + nω
mn2 + m2 + nω
mn2 + m2 + nω
mn2 + mω + nω

Table 1.1: Summary of key SDP algorithms. CPM stands for cutting plane method, and IPM, interior point method.
n is the size of the variable matrix, and m ≤ n2 is the number of constraints. Runtimes hide no(1), mo(1) and
poly log(1/ǫ) factors, where ǫ is the accuracy parameter.
[Ans00] simpliﬁes the proofs in [NN94, Section 5.5]. Nei-
ther [Ans00] nor [NN94] explicitly analyzed their runtimes, and their runtimes shown here are our best estimates.

Even in the more general case where the SDP might not be dense, where nnz(A) is the input
size (i.e., the total number of non-zeroes in all matrices Ai for i
[m] and C), our interior point
method runs faster than the current fastest cutting plane methods[LSW15, JLSW20], which run in
time O∗(m(nnz(A) + m2 + nω)).

∈

3We use O

∗

parameter.

to hide no(1) and logO(1)(n/ǫ) factors and eO to hide logO(1)(n/ǫ) factors, where ǫ is the accuracy

3

Year References

Method

[Sho77, YN76, Kha80] CPM
1979
CPM
[KTE88, NN89]
1988
CPM
[Vai89a]
1989
IPM
[NN92]
1992
IPM
[NN94, Ans00]
1994
CPM
[KM03]
2003
CPM
[LSW15]
2015
CPM
2020
[JLSW20]
IPM
2020 Our result

Runtime
m = n m = n2
n5
n4.5
n4
n4.5
n6.5
n4
n4
n4
n3.5

n8
n9
n6.746
n6.5
n10.75
n6.746
n6
n6
n5.246

Table 1.2: Total runtimes for the algorithms in Table 1.1 for SDPs when m = n and m = n2, where n is the size of
matrices, and m is the number of constraints. The runtimes shown in the table hide no(1), mo(1) and poly log(1/ǫ)
factors, where ǫ is the accuracy parameter and assume ω to equal its currently best known upper bound of 2.373.

Theorem 1.3 (Comparison with Cutting Plane Method). When m
method that solves an SDP with n
the current best cutting plane method [LSW15, JLSW20], over all regimes of nnz(A).

n, there is an interior point
n matrices, m constraints, and nnz(A) input size, faster than

≥

×

1.2 Technique overview

1.2.1 Interior point method for solving SDPs

n2 in the primal
By removing redundant constraints, we can, without loss of generality, assume m
formulation of the SDP (1). Thereafter, instead of solving the primal SDP, which has variable size
n

n, we solve its dual formulation, which has dimension m

n2:

≤

×

min b⊤y subject to S =

m

Xi=1

≤

yiAi −

C, and S

0.

(cid:23)

Interior point methods solve (2) by minimizing the penalized objective function:

min
Rm
y
∈

fη(y), where fη(y) := η

b⊤y + φ(y),

·

R is a barrier function that approaches inﬁnity as y
where η > 0 is a parameter and φ : Rm
→
approaches the boundary of the feasible set
y
. These methods ﬁrst obtain
{
}
an approximate minimizer of fη for some small η > 0, which they then use as an initial point to
minimize f(1+c)η, for some constant c > 0, via the Newton method. This process repeats until the
parameter η in (3) becomes suﬃciently large, at which point the minimizer of fη is provably close to
the optimal solution of (2). The iterates y generated by this method follow a central path. Diﬀerent
choices of the barrier function φ lead to diﬀerent run times in solving (3), as we next describe.

i=1 yiAi (cid:23)

Rm :

P

C

∈

m

The log barrier Nesterov and Nemirovski [NN92] use the log barrier function,

φ(y) = g(y) :=

log det

−

m

Xi=1

yiAi −

C

,

!

(4)

in (3) and, in O(√n log(n/ǫ)) iterations, obtain a feasible dual solution y that satisﬁes b⊤y
b⊤y∗ + ǫ, where y∗ ∈

≤
Rm is the optimal solution for (2). Within each iteration, the costliest step

4

(2)

(3)

 
is to compute the inverse of the Hessian of the log barrier function for the Newton step. For each
(j, k)

[m], the (j, k)-th entry of H is given by

[m]

∈

×

Hj,k = tr[S−

1AjS−

1Ak].

(5)

[m], which takes time O∗(mnω),
1/2AjS−
The analysis of [NN92] ﬁrst computes S−
and then calculates the m2 trace products tr[S−
[m], each of
[m]
which takes O(n2) time. Inverting the Hessian costs O∗(mω), which results in a total runtime of
O∗(√n(m2n2 + mnω + mω)).

1/2 for all j
1AjS−

1Ak] for all (j, k)

×

∈

∈

Rn : Ax

The volumetric barrier Vaidya [Vai89a] introduced the volumetric barrier for a polyhedral set
Rm. Nesterov and Nemirovski [NN94] studied the
x
{
Rm :
of the
following extension of the volumetric barrier to the convex subset
polyhedral cone:

, where A
c
}

i=1 yiAi (cid:23)

n and c

y
{

Rm

≥

C

∈

∈

∈

∈

m

}

×

P

V (y) =

1
2

log det(

∇

2g(y)),

where g(y) is the log barrier function deﬁned in (4). They proved that choosing φ(y) = √nV (y)
O(√mn1/4) iterations, which is smaller than the
in (3) makes the interior point method converge in
O(√n) iteration complexity of [NN92] when m
√n. They also studied the combined volumetric-
e
logarithmic barrier
e

≤

and showed that taking φ(y) =
of
p
readers to the much simpler proofs in [Ans00] for these results.

·
Vρ(y) for ρ = (m
n, this iteration complexity is lower than

O((mn)1/4). when m

1)/(n

n/m

−

−

≤

·

Vρ(y) = V (y) + ρ

g(y)

1) yields an iteration complexity
O(√n) of [NN92]. We refer

e
However, the volumetric barrier (and thus the combined volumetric-logarithmic barrier) leads to
complicated expressions for the gradient and Hessian that make each iteration costly. For instance,
the Hessian of the volumetric barrier is

e

∇
where Q(y), R(y), and T (y) are m

2V (y) = 2Q(y) + R(y)

2T (y),

−

m matrices such that for each (j, k)

[m]

×

∈

[m],

×
S−

1AjS−

1AkS−

1

1

S−

⊗
(cid:1)
1AkS−
S−
b

(cid:1)i
1

,

,

(cid:0)(cid:0)

S−

1AjS−

1

⊤

⊗

A

A

h

h

1

H −

1

H −

1

H −

⊤

A

A

A

Q(y)j,k = tr

R(y)j,k = tr

T (y)j,k = tr

A
m is the n2

h

(cid:0)(cid:0)

S−

1AjS−

1

(cid:1)

⊤

1

(cid:0)
S−

b
⊗

1

H −

(cid:1)(cid:1)i
⊤
A

A

S−

1AkS−

1

1

S−

⊗

Rn2

Here,
A ∈
of length n2, and

×

m matrix whose ith column is obtained by ﬂattening Ai into a vector

(cid:1)

b

(cid:1)

(cid:0)(cid:0)

(cid:1)

b

(cid:1)i

(cid:0)(cid:0)

×

is the symmetric Kronecker product

(6)

.

⊗

b

A

B :=

⊗

1
2

(A

⊗

B + B

A),

⊗

b

⊗

is the Kronecker product (see Section 2.1for formal deﬁnition). Due to the complicated
where
formulas in (6), eﬃcient computation of Newton step in each iteration of the interior point method
is diﬃcult; in fact, each iteration runs slower than the Nesterov-Nemirovski interior point method
by a factor of m2. Since most applications of SDPs known to us have the number of constraints m
be at least linear in n, the total runtime of interior point methods based on the volumetric barrier
and the combined volumetric-logarithmic barrier is inevitably slow.

5

1.2.2 Our techniques

Given the ineﬃciency of implementing the volumetric and volumetric-logarithmic barriers discussed
above, this paper uses the log barrier in (4). We now describe some of our key techniques that
improve the runtime of the Nesterov-Nemirovski interior point method [NN92].

Hessian computation using fast rectangular matrix multiplication As noted in Sec-
tion 1.2.1, the runtime bottleneck in [NN92] is computing the inverse of the Hessian of the log
In [NN92], each of these m2 entries is
barrier function, where the Hessian is described in (5).
computed separately, resulting in a runtime of O(m2n2) per iteration.

Instead contrast, we show below how to group these computations using rectangular matrix

multiplication. The expression from (5) can be re-written as

Hj,k = tr[S−

1/2AjS−

1/2

S−

1/2AkS−

1/2].

(7)

·
Rn

1/2AjS−

1/2

×

Rn

We ﬁrst compute the key quantity S−
n into a tall matrix of size mn
Aj ∈
tall matrix. This matrix product can be computed in time
matrix multiplication. We then ﬂatten each S−
all m vectors to form a matrix
It follows that the Hessian can be computed as

1/2AjS−

of size m

[m] by stacking all matrices
Rn
n with this
Tmat(n, mn, n)4 using fast rectangular
1/2 into a row vector of length n2 and stack
Bj = vec(S−
1/2).

1/2AjS−

n, and then compute the product of S−

n2, i.e., the j-th row of

1/2

×

×

is

∈

∈

∈

B

B

×

×

n for all j

H =

⊤,

BB

(8)

Tmat(m, n2, m) by applying fast rectangular matrix multiplication. By leveraging
which takes time
recent developments in this area [GU18], this approach already improves upon the runtime in [NN92].
Thus far, we have reduced the per iteration cost of O∗(m2n2 + mnω) for Hessian computation

down to

Tmat(n, mn, n) +

Tmat(m, n2, m).

Low rank update on the slack matrix The fast rectangular matrix multiplication approach
noted above, however, is still not very eﬃcient, because the Hessian must be computed from scratch
in each iteration of the interior point method. If there are T iterations in total, it then takes time

T

Tmat(n, mn, n) +
(

·

Tmat(m, n2, m)).

×

∈

To further improve the runtime, we need to eﬃciently update the Hessian for the current iteration
from the Hessian computed in the previous one. Generally, this is not possible, as the slack matrix
Rn
n in (7) might change arbitrarily in the Nesterov-Nemirovski interior point method.
S
To overcome this problem, we propose a new interior point method that maintains an approxi-
n

n, which is a spectral approximation of the true slack matrix S

mate slack matrix
×
S admits a low-rank update in each iteration. Where needed, we will now use the subscript
such that
t to denote a matrix in the t-th iteration. Our algorithm updates only the directions in which
St
deviates too much from St+1; the changes to St for the remaining directions are not propagated in
e
St even when St suﬀers from a
St. This process of selective update ensures a low-rank change in

Rn

Rn

∈

∈

S

e

e

×

4See Section 3 for the deﬁnition.

e

e

6

full-rank update; it also guarantees the proximity of the algorithm’s iterates to the central path.
Speciﬁcally, for each iteration t

[T ], we deﬁne the diﬀerence matrix

∈

Zt = S−
t

1/2

1/2

StS−
t

I

−

∈

Rn

n,

×

which intuitively captures how far the approximate slack matrix
We maintain the invariant
Ztkop ≤
k
iteration when St gets updated to St+1, our construction of
out some of the largest eigenvalues of
matrix.

St is from the true slack matrix St.
c for some suﬃciently small constant c > 0. In the (t + 1)-th
St+1 involves a novel approach of zeroing
to bound the rank of the update on the approximate slack

Zt|
|

e

e

e

We prove that with this approach, the updates on

n over all T =
satisfy the following rank inequality (see Theorem 6.1for the formal statement).

∈

S

×

Rn

O(√n) iterations

Theorem 1.4 (Rank inequality, informal version). Let
of approximate slack matrices generated in our interior point method. For each t
St. Then, the sequence r1, r2,
by rt = rank(

St) the rank of the update on

ST ∈
e

S2,

· · ·

e

e

×

,

n denote the sequence
1], denote

[T
, rT satisﬁes

−

Rn

e
S1,

e

∈
· · ·

St+1 −
e

e

T

t=1
X

e
O(T ).
√rt =

e

The key component to proving Theorem 1.4 is the potential function Φ : Rn

n

×

R

0

≥

→

Φ(Z) :=

λ(Z)
|
√ℓ

|[ℓ]

,

n

Xℓ=1

λ(Z)
|

|[ℓ] is the ℓ-th in the list of eigenvalues of Z

n sorted in decreasing order of their
where
absolute values. We show an upper bound on the increase in this potential when S is updated, a
lower bound on its decrease when
S is updated, and combine the two with non-negativity of the
potential to obtain Theorem 1.4.

∈

×

Rn

Speciﬁcally, ﬁrst we prove that whenever S is updated in an iteration, the potential function
O(1) (see Lemma 6.2). The proof of this statement crucially uses the structural
increases by at most
property of interior point method that slack matrices in consecutive steps are suﬃciently close to
[T ], we show in Theorem 5.1 that the consecutive slack
each other. Formally, for any iteration t
matrices St and St+1 satisfy

∈

e

e

1/2

1/2

St+1S−
t

S−
t
k

I

kF = O(1)

−

(9)

and combine this bound with the Hoﬀman-Wielandt theorem [HJ12], which relates the ℓ2 distance
between the spectrum of two matrices with the Frobenius norm of their diﬀerence (see Fact 2.2).
S gets updated, we prove that our method of zeroing out the rt largest eigenvalues of
Next, when
O(√rt) (see
Zt|
|
Lemma 6.3).

St, results in a potential decrease of at least

, thereby incurring a rank-rt update to

e

e

e

Maintaining rectangular matrix multiplication for Hessian computation. Given the low-
H,
rank update on
deﬁned as

S described above, we show how to eﬃciently update the approximate Hessian

e

Hj,k = tr[

S−

1Aj

S−

1Ak]

e

e
7

e

e
(10)

[m]

S being a spectral approximation of
for each entry (j, k)
the true slack matrix S implies that the approximate Hessian
H is also a spectral approximation of
the true Hessian H (see Lemma 5.3). This approximate Hessian therefore suﬃces for our algorithm
to approximately follow the central path.

[m]. The approximate slack matrix

×

∈

e

e

To eﬃciently update the approximate Hessian

S
1 via the Woodbury matrix identity (see Fact 2.4). The change in
e

H in (10), we notice that a rank-r update on

e

implies a rank-r update on
1 can be expressed as
S−

S−

e

∆(

S−

1) = V+V ⊤+ −

V

,

V ⊤
−

−

×

e

Rn

− ∈

r. Plugging (11) into (10), we can express ∆

e
Hj,k as the sum of multiple terms,
where V+, V
Rn
r is either
among the costliest of which are those of the form tr[
1AjV V ⊤Ak] for all (j, k)
Tmat(r, n, mn) by ﬁrst
S−
. We compute tr[
V+ or V
×
[m] by horizontally concatenating all Ak’s into a wide matrix of size
computing V ⊤Ak for all k
∈
e
[m], which can be done in time
n
×
1/2AjV
Tmat(n, n, mr), which equals
S−
nr with j-th row
into a vector of length nr and stacking all these vectors to form a matrix
e

Tmat(n, mr, n) (see Lemma 3.3). Finally, by ﬂattening each

1AjV V ⊤Ak], where V
e
[m] in time
[m]

mn. We then compute the product of

1/2 with AjV for all j

B ∈

Rm

S−

S−

∈
e

∈

∈

×

−

×

(11)

the task of computing tr[
Tmat(m, nr, m).
costs

S−

In this way, we reduce the runtime of T

e

Hessian using fast rectangular matrix multiplication down to

[m]

×

∈
Tmat(n, mn, n) +
(

·

e
[m] reduces to computing

B⊤, which
Tmat(m, n2, m)) for computing the
e
e

B

e
Bj = vec(
1AjV V ⊤Ak] for all (j, k)
e

S−

e

1/2AjV ),

T

t=1
X

Tmat(rt, n, mn) +
(

Tmat(n, mrt, n) +

Tmat(m, nrt, m)) ,

(12)

St. Applying Theorem 1.4 with several properties of fast
where rt is the rank of the update on
rectangular matrix multiplication that we prove in Section 3 , we upper bound the runtime in (12)
by

e

O∗(√n(mn2 + mω + nω)),

which implies Theorem 1.2. In Section 1.2.3 and 1.2.4, we discuss bottlenecks to further improving
our runtime.

1.2.3 Bottlenecks of our interior point method

In most cases, the costliest term in our runtime is the per iteration cost of mn2, which corresponds
to reading the entire input in each iteration. Our subsequent discussions therefore focus on the
steps in our algorithm that require at least mn2 time per iteration.

Slack matrix computation. When y is updated in each iteration of our interior point method,
we need to compute the true slack matrix S as

S =

[m]
Xi
∈

yiAi −

C.

S remains a spectral
Computing S is needed to update the approximate slack matrix
approximation to S. As S might suﬀer from full-rank changes, it naturally requires mn2 time to
compute in each iteration. This is the ﬁrst appearance of the mn2 cost per iteration.

S so that

e

e

8

Gradient computation. Recall from (3) that our interior point method follows the central path
deﬁned via the penalized objective function

fη(y) where

fη(y) := ηb⊤y + φ(y),

min
Rm
y
∈

for a parameter η > 0 and φ(y) =
gradient of the penalized objective is computed as

−

log det S. In each iteration, to perform the Newton step, the

gη(y)j = η

bj −
[m]. Even if we are given S−

·

tr[S−

1Aj]

(13)

1, it still requires mn2 time to compute (13)

[m]. This is the second appearance of the per iteration cost of mn2.

for each coordinate j
for all j

∈

∈

Approximate Hessian computation. Recall from Section 1.2.2 that updating the approximate
slack matrix S by rank r means the time needed to update the approximate Hessian is dominated
by computing the term

∆j,k = tr[

S−

1/2AjV

V ⊤Ak

S−

1/2],

·

r is a tall, skinny matrix that comes from the spectral decomposition of ∆

Rn
where V
Computing ∆j,k for all (j, k)
time mn2. This is the third bottleneck that leads to the mn2 term in the cost per iteration.

[m] requires reading at least Aj for all j

1.
[m], which takes

[m]

S−

×

∈

∈

∈

e

e

e

×

1.2.4 LP techniques unlikely to improve SDP runtime

The preceeding discussion of bottlenecks suggests that reading the entire input in each iteration,
which takes mn2 time per iteration, stands as a natural barrier to further improving the runtime of
SDP solvers based on interior point methods.

In the context of linear programming (LP), several recent results [CLS19, BLSS20] yield faster
interior point methods that bypass reading the entire input in every iteration. Two techniques crucial
to these results are: (1) showing that the Hessian (projection matrix) admits low-rank updates, and
(2) speeding computation of the Hessian via sampling.

We now describe these techniques in the context of SDP and argue that they are unlikely to

improve our runtime.

Showing that the Hessian admits low-rank updates. We saw in Section 1.2.2 that construct-
ing an approximate slack matrix
S that admits low-rank updates in each iterations leveraged the
fact that the true slack matrix S changes “slowly” throughout our interior point method as described
in (9). One natural question that follows is whether a similar upper bound can be obtained for the
Hessian. If such a result could be proved, then one could maintain an approximate Hessian that
admitted low-rank updates, which would speed up the approximate Hessian computation. Indeed,
in the context of LP, such a bound for the Hessian can be proved (e.g.,

[BLSS20, Lemma 47]).

e

Unfortunately, it is impossible to prove such a statement for the Hessian in the context of SDP.

To show this, it is convenient to express the Hessian using the Kronecker product (Section 2.1)as

H =

⊤

A

·

1

(S−

S−

1)

,

· A

⊗

where
m matrix whose ith column is obtained by ﬂattening Ai into a vector of
length n2. By proper scaling, we can assume without loss of generality that the current slack matrix

A ∈

×

×

m is the n2

Rn2

9

is S = I, and the slack matrix in the next iteration is Snew = I + ∆S, which satisﬁes
for some tiny constant c > 0. Consider the simple example where
that m = n2 so that
approximately computed as

kF = c
= I (we are assuming here
is a square matrix), which implies that the change in the Hessian can be

∆S
k

A

A

H −

1/2∆HH −

1/2

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

2

F ≈

tr

((I

∆S)

(I

⊗

−

∆S)

I

−

⊗

−

I)2

i

tr

≈

h
(I
⊗
tr[I 2]
(cid:2)
·
≥
= 2n

∆S

2

k

∆S + ∆S
⊗
(∆S)2

tr

I)2

(cid:3)

·
2
(cid:2)
F ≫
k

1.

(cid:3)

This large change indicates that we are unlikely to obtain an approximation to the Hessian that
admits low-rank updates, which is a key diﬀerence between LP and SDP.

Sampling for faster Hessian computation. Recall from (8) that the Hessian can be computed
as

H =

⊤,

B · B

n2

Rm

B ∈

[m]. We might attempt
where the jth row of
∈
[n2]
to approximately compute H faster by sampling a subset of columns of
and compute the product for only the sampled columns. This could reduce the dimension of the
matrix multiplication and speed up the Hessian computation. Indeed, sampling techniques have
been successfully used to obtain faster LP solvers [CLS19, BLSS20].

indexed by L

Bj = vec(S−

1/2) for all j

1/2AjS−

⊆

is

B

×

For SDP, however, sampling is unlikely to speed up the Hessian computation. In general, we
to spectrally approximate H or the computed
1/2 that
[m], which requires reading all Aj’s and thus still takes O(mn2)

must sample at least m columns (i.e.
| ≥
matrix will not be full rank. However, this requires computing the entries of S−
correspond to L
time.

[n2] for all j

1/2AjS−

m) of

L
|

⊆

∈

B

1.3 Related work

Linear Programming. Linear Programming is a class of fundamental problems in convex opti-
mization. There is a long list of work focused on fast algorithms for linear programming [Dan47,
Kha80, Kar84, Vai87, Vai89b, LS14, LS15, Sid15, Lee16, CLS19, Bra20, BLSS20].

Cutting Plane Method. Cutting plane method is a class of optimization methods that itera-
tively reﬁne a convex set that contains the optimal solution by querying a separation oracle. Since
its introduction in the 1950s, there has been a long line of work on obtaining fast cutting plane
methods [Sho77, YN76, Kha80, KTE88, NN89, Vai89a, AV95, BV02, LSW15, JLSW20].

First-Order SDP Algorithms. As the focus of this paper, cutting plane methods and interior
point methods solve SDPs in time that depends logarithmically on 1/ǫ, where ǫ is the accuracy
parameter. A third class of algorithms, the ﬁrst-order methods, solve SDPs at runtimes that depend
polynomially on 1/ǫ. While having worse dependence on 1/ǫ compared to IPM and CPM, these
ﬁrst-order algorithms usually have better dependence on the dimension. There is a long list of work
on ﬁrst-order methods for general SDP or special classes of SDP (e.g. Max-Cut SDP [AK07, GH16,
AZL17, CDST19, LP20, YTF+19], positive SDPs [JY11, PT12, ALO16, JLL+20].)

10

2 Preliminaries

2.1 Notation

n

×
0

· · ·

1, 2,
{
×

. We use Sn
, d
}

For any integer d, we use [d] to denote the set
n matrices, Sn
for the set of n
symmetric n
×
≥
the set of n
n positive deﬁnite matrices. For two matrices A, B
×
means that B
(e.g. A
diag(v)i,i = vi. For A, B
deﬁned as
Kronecker product of A and B, denoted as A
(i, j) block is Ai,jB, for all (i, j)

n to denote the set of
for
B
n
0 . When clear from the context, we use 0 to denote the all-zeroes matrix
×
n matrix with
∈
n, we deﬁne the inner product to be the trace product of A and B,
l, the
×
∈
nl block matrix whose

n positive semideﬁnite matrices, and Sn
×
>0
Sn
n, the notation A

Rn, we use diag(v) to denote the diagonal n

[n] Ai,jBi,j. For two matrices A
∈

0). For a vector v
Sn

∈
B, is deﬁned as the mk

×
∈
:= tr[A⊤B] =

n and B

A, B
h

Sn
≥

[m]

[n].

Rm

Rk

×

×

⊗

(cid:23)

−

(cid:22)

A

∈

∈

i,j

×

×

×

i

n

Throughout this paper, unless otherwise speciﬁed, m denotes the number of constraints for the
n. The number of non-zero entries in all

primal SDP (1), and the variable matrix X is of size n
the Ai and C of (1) is denoted by nnz(A).

×

P
∈

×

2.2 Useful facts

×

×

∈

p

∈
Rn

kop of A
A
k

tr[A⊤A]. The operator (or spectral) norm

Linear algebra. Some matrix norms we frequently use in this paper are the Frobenius and op-
Rn
n is deﬁned to be
erator norms, deﬁned as follows. The Frobenius norm of a matrix A
n is deﬁned to be the
kF :=
A
k
largest singular value of A. In the case of symmetric matrices (which is what we encounter in this
paper), this can be shown to equal the largest absolute eigenvalue of the matrix. A property of trace
nk , the
we frequently use is the following: given matrices A1 ∈
trace of their product is invariant under cyclic permutation tr[A1A2 . . . Ak] = tr[A2A3 . . . AkA1] =
Rn
n is called normal if A commutes with its transpose,
2Ak
×
· · ·
Rn
n
i.e. AA⊤ = A⊤A. We note that all symmetric n
×
∈
1BS. In
are said to be similar if there exists a nonsingular matrix S
particular, if matrices A and B are similar, then they have the same set of eigenvalues. We use the
following simple fact involving Loewner ordering: given two invertible matrices A and B satisfying
1
1. We further need the following facts.
α B
A−
Fact 2.1 (Generalized Lieb-Thirring Inequality [Eld13, ALO16, JLL+20]). Given a symmetric ma-
trix B, a positive semi-deﬁnite matrix A and α

n matrices are normal. Two matrices A, B
n such that A = S−

αB for some α > 0, we have 1

n2, . . . , Ak ∈

= tr[AkA1 . . . Ak

1]. A matrix A

n1, A2 ∈

[0, 1], we have

Rnk−1×

α B−

αB−

Rn1

Rm

Rn

(cid:22)

(cid:22)

×

(cid:22)

(cid:22)

A

∈

∈

−

−

×

×

×

1

1

tr[AαBA1
−

∈
αB]

tr[AB2].

≤

n such that A and A + E
Fact 2.2 (Hoﬀman-Wielandt Theorem, [HW53, HJ12]). Let A, E
λn be the
λ2, . . . ,
are both normal matrices. Let λ1, λ2, . . . , λn be the eigenvalues of A, and let
eigenvalues of A + E in any order. There is a permutation σ of the integers 1, . . . , n such that

λ1,

∈

×

Rn

2

λi|

E

≤ k

2
F = tr[E∗E].
k

i

[n] |
∈

λσ(i) −
b

n such that A is
Fact 2.3 (Corollary of the Hoﬀman-Wielandt Theorem, [HJ12]). Let A, E
P
Hermitian and A + E is normal. Let λ1, . . . , λn be the eigenvalues of A arranged in increasing order
λn).
λ1 ≤
Then,

λn be the eigenvalues of A + E, ordered so that Re(
E
b

λ1, . . . ,
2
λi|
b

2
F .
k

[n] |
∈

Re(

≤ k

λ1)

≤
i

. . .

. . .

≤

≤

∈

×

Rn

P

Fact 2.4 (Woodbury matrix identity, [Woo49, Woo50]). Given matrices A
C

n, such that A, C, and A + U CV are invertible, we have

k, and V

Rk

Rk

×

×

b
∈

Rn

n, U

×

∈

b
Rn
×

k,

λn. Let
λi −
b

∈

∈

b

b

b

(A + U CV )−

1 = A−

1

−

A−

1U (C −

1 + V A−

1U )−

1V A−

1.

11

3 Matrix Multiplication

The main goal of this section is to derive upper bounds on the time to perform the following two
rectangular matrix multiplication tasks (Lemma 3.9, 3.10, and 3.11):

• Multiplying a matrix of dimensions m

• Multiplying a matrix of dimensions n

n2 with one of dimensions n2
×
mn with one of dimensions mn

×

×

m,

n.

×

Besides being crucial to the runtime analysis of our interior point method in Section 7, these results
(as well as several intermediate results) might be of independent interest.

3.1 Exponent of matrix multiplication

We need the following deﬁnitions to describe the cost of certain fundamental matrix operations we
use.

Deﬁnition 3.1. Deﬁne
of matrices of dimensions n

Tmat(n, r, m) to be the number of operations needed to compute the product

r and r

m.

×

×

Deﬁnition 3.2. We deﬁne the function ω(k) to be the minimum value such that
nω(k)+o(1). We overload notation and use ω to denote the cost of multiplying two n
Thus, we have ω(1) = ω.

×

Tmat(n, nk, n) =
n matrices.

The following is a basic property of

Tmat that we frequently use.

Lemma 3.3 ([BCS97, Blä13]). For any three positive integers n, m, r, we have

Tmat(n, m, r)) = O(
We refer to Table 3 in [GU18] for the latest upper bounds on ω(k) for diﬀerent values of k. In

Tmat(n, r, m) = O(

Tmat(m, n, r)).

particular, we need the following upper bounds in our paper.

Lemma 3.4 ([GU18]). We have:

• ω = ω(1)

≤

• ω(1.5)

• ω(1.75)

• ω(2)

≤

2.372927,

≤
2.79654,

3.02159,

≤
3.251640.

3.2 Technical results for matrix multiplication

In this section, we derive some technical results on
Tmat and ω that we extensively use for our
runtime analysis. Some of these results can be derived using tensors, and we demonstrate this in
Appendix A. We hope that the use of tensors can yield better runtimes for this problem in future.

Lemma 3.5 (Sub-linearity). For any p

q

1, we have

≥

p

q + ω(q).

≥
ω(p)

≤

−
Proof. We assume that np and nq are integers for notational simplicity. Consider multiplying an
q rectangular blocks
n matrix. One can cut the n
n
n matrix into np
n, and compute the
of size n
×
multiplication of the corresponding blocks. This approach takes time np
q+ω(q)+o(1), from which
the desired inequality immediately follows.

np matrix with an np
nq and the np

q rectangular blocks of size nq

np matrix into np

×
×

×

×

×

−

−

−

12

Key to our analysis is the following lemma, which establishes the convexity of ω(k).

Lemma 3.6 (Convexity). The fast rectangular matrix multiplication time exponent ω(k) as deﬁned
in Deﬁnition 3.2 is convex in k.

·

·

∈

−

α)

p + (1

Proof. Let k = α
q for α
and nk are all integers. Consider a rectangular matrix of dimensions n
tile this rectangular matrix with matrices of dimensions nα
matrix with another similarly tiled matrix of dimensions nk
a multiplication of a matrix of dimensions n/nα
where each “element” of these two matrices is itself a matrix of dimensions nα
recursion in tow, we obtain the following upper bound.

(0, 1). For notational simplicity, we assume that np, nq
nk. Since αp
k, we can
nαp. Then, the product of this tiled
n can be obtained by viewing it as
n1/α,
×
nαp. With this

nk/nαp with one of dimensions nk/nαp

×
×

×

×

×

≤

Tmat(n, nk, n)

≤Tmat(nα, nαp, nα)
Tmat(nα, nαp, nα)
=
n(1
nα
−
·

ω(p)+o(1)

· Tmat(n/nα, nk/nαp, n/nα)
α)q, n(1
α), n(1
· Tmat(n(1
−
−
−
ω(q)+o(1).

α)
·

α))

≤

·

The ﬁnal step above follows from denoting m = nα and observing that multiplying matrices of
p costs, by Deﬁnition 3.2, mω(p)+o(1), which is exactly nα(ω(p)+o(1)). Applying
dimensions nα
Deﬁnition 3.2 and comparing exponents, this implies that

nα
·

×

ω(k)

α

·

≤

ω(p) + (1

α)

·

−

ω(q),

which proves the convexity of the function ω(k).

Claim 3.7. ω(1.68568)

2.96370.

≤

Proof. We can upper bound ω(1.68568) in the following sense

ω(1.68568) = ω(0.25728

1.5 + (1

·

·

ω(1.5) + (1

−
2.79654 + (1

−

0.25728)

·
0.25728)

·
0.25728)

−

1.75)

ω(1.75)

3.02159

·

0.25728

0.25728

·
2.96370,

≤

≤

≤

where the ﬁrst step follows from convexity of ω (Lemma 3.6), the third step follows from ω(1.5)
2.79654 and ω(1.75)

3.02159 (Lemma 3.4).

≤

≤

Lemma 3.8. Let
we have

Tmat be deﬁned as in Deﬁnition 3.1. Then for any positive integers h, ℓ, and k,

Tmat(h, ℓk, h)

O(

Tmat(hk, ℓ, hk)).

≤

Proof. Given any matrices A, B⊤ ∈
product AB is
We cut A and B⊤ into k sub-matrices each of size h
(B⊤1 ,
blockwise, we can write

Tmat(h, ℓk, h). We now show how to compute this product in time O(
· · ·
ℓ for all i

Rh,ℓk, by Deﬁnition 3.1, the cost of computing the matrix
Tmat(hk, ℓ, hk)).
, Ak) and B⊤ =
×
[k]. By performing matrix multiplication

, B⊤k ), where each Ai, B⊤i ∈

ℓ, i.e. A = (A1,

Rh

· · ·

∈

×

k

AB =

AiBi.

Xi=1

13

Next, we stack the k matrices A1,
· · ·
stack the k matrices B1,
Deﬁnition 3.1, we can compute A′B′ ∈
note that we can derive AB from A′B′ as follows: for each j
of size h

Rhk,ℓ. Similarly, we
, Ak vertically to form a matrix A′ ∈
Rℓ,hk. By
, Bk horizontally to form a matrix B′ = (B1,
· · ·
Rhk,hk in time
Tmat(hk, ℓ, hk). To complete the proof, we
[k], the jth diagonal block of A′B′
h blocks of A′B′ gives AB.

∈
h is exactly AjBj, and summing up the k diagonal h

, Bk)

· · ·

∈

×

3.3 General upper bound on

Tmat(n, mn, n) and

Lemma 3.9. Let
If m

n, then we have

Tmat be deﬁned as in Deﬁnition 3.1.

≥

×
Tmat(m, n2, m)

If m

≤

n, then we have

Tmat(n, mn, n)

≤

O(

Tmat(m, n2, m)).

Tmat(m, n2, m)

O(

Tmat(n, mn, n)).

≤

Proof. We only prove the case of m
≥
immediate consequence of Lemma 3.8 by taking h = n, ℓ = n2, and k =
positive integer because m

n, as the other case where m < n is similar. This is an
, where k is a
⌋

m/n
⌊

n.

≥

In the next lemma, we derive upper bounds on the term
Tmat(n, mn, n) when m < n, which is crucial to our runtime analysis.
Lemma 3.10. Let
Property I. We have

Tmat be deﬁned as in Deﬁnition 3.1 and ω be deﬁned as in Deﬁnition 3.2.

Tmat(m, n2, m) when m

n and

≥

Property II. We have

Proof. Property I.

Tmat(n, mn, n)

≤

O(mnω+o(1)).

Tmat(m, n2, m)

≤

O

√n

mn2 + mω

.

(cid:0)

(cid:0)

(cid:1)(cid:1)

Recall from Deﬁnition 3.1 that

mn
×
with one of size mn
n each.
The product in question then can be obtained by multiplying these sub-matrices. Since there are m
of them, and each product of an n
n submatrix costs, by deﬁnition,
n submatrix with another n
nω+o(1), we get

Tmat(n, mn, n) is the cost of multiplying a matrix of size n
×

n. We can cut each of the matrices into m sub-matrices of size n

×
O(mnω+o(1)), as claimed.

×

×

Property II.
Let m = na, where a

Tmat(n, mn, n)
(0,

≤

∞
n2 with one of size n2

∈

of size m

×

×

). By deﬁnition,

Tmat(m, n2, m) is the cost of multiplying a matrix
m. Expressing n2 as m2/a then gives, by Deﬁnition 3.2, that

Property II is then an immediate consequence of the following inequality, which we prove next:

Tmat(m, n2, m) = mω(2/a)+o(1) = na

·

ω(2/a)+o(1)

ω(2/a) < max(1 + 2.5/a, ω(1) + 0.5/a)

a

∀

∈

(0,

).

∞

Deﬁne b = 2/a

(0,

∈

∞

). Then the desired inequality in (14) can be expressed in terms of b as

ω(b) < max(1 + 5b/4, ω(1) + b/4)

b

∀

∈

(0,

).

∞

14

(14)

(15)

1. By the convexity of ω(

Notice that the RHS of (15) is a maximum of two linear functions of b and these intersect at
) as proved in Lemma 3.6, it suﬃces to verify (15) at the
b∗ = ω(1)
−
and b = b∗. In the case where b = δ for any δ < 1, (15) follows immediately
endpoints b
. By Lemma 3.4 we
from the observation that ω(δ) < ω(1). We next argue about the case b
2 + ω(2). Combining these two facts
have ω(2)
implies that for any b > 2, we have

3.252. Using Lemma 3.5, we have ω(b)

→ ∞

→ ∞

0, b

→

−

≤

≤

b

·

ω(b)

b

−

≤

2 + ω(2)

≤

1 + 5b/4,

which again satisﬁes (15). The ﬁnal case is b = b∗ = ω(1)

ω(ω(1)

−

1) < 5ω(1)/4

1, for which (15) is equivalent to

1/4.

(16)

−

−

By Lemma 3.4, we have that ω(1)
that

2

−

∈

[0, 0.372927]. Then to prove (16), it is suﬃcient to show

ω(t + 1) < 5t/4 + 9/4

t

∀

∈

[0, 0.372927].

By the convexity of ω(
3.4, and recalling that ω(1) = t + 2 for t

·

) as proved in Lemma 3.6, the upper bound of ω(2)

[0, 0.372927], we have for k

∈

∈

≤
[1, 2],

(17)

3.251640 in Lemma

ω(k)

≤

ω(1) + (k

1)

·

−

(3.251640

−

(t + 2)) = t + 2 + (k

1)

·

−

(1.251640

t).

−

In particular, using this inequality for k = t + 1, we have

ω(t + 1)

5t/4

9/4

−

−

≤
=

(t + 2) + t

(1.251640

·

t2 + 1.00164t

t)

−

−

5t/4

−

9/4

−

1/4,

−

which is negative on the entire interval [0, 0.372927]. This establishes (17) and ﬁnishes the proof.

3.4 Speciﬁc upper bound on

Tmat(m, n2, m)

Lemma 3.11. For any two positive integers n and m, we have

Tmat(m, n2, m) = o
(cid:0)
Proof. Let m = na where a
Tmat(m, n2, m) = mω(2/a)+o(1) = naω(2/a)+o(1).
). Recall that
(0,
We consider the following two cases according to the range of a.
ω(1.68568) < 3,

). In this case, we have ω(2/a)

m3 + mn2.37

ω(2/1.18647)

Case 1: a

[1.18647,

∞

∈

(cid:1)

.

where the last inequality follows from Claim 3.7. This implies that

∈

∞

≤

≤

Tmat(m, n2, m) = o(n3a) = o(m3).
[1.68567,

(0, 1.18647]. In this case, we have 2/a

∈

∞

Case 2: a

∈

By Claim 3.7, we have

y(t) = 1 + 2.37

t
2

.

·

ω(1.68567) < 2.997

y(1.68567).

≤

15

(18)

). Consider the linear function

(19)

(20)

By Lemma 3.4, we have

ω(2) < 3.37 = y(2).

An application of Lemma 3.5 then gives, for any t

2, the inequality

≥

ω(t)

t

−

≤

2 + ω(2) < t

2 + y(2)

y(t),

≤

−

(21)

(22)

where the last inequality is by deﬁnition of y(t) from (19). Therefore, combining the convexity of
ω(
), as proved in Lemma 3.6, with (20), (21), and (22), we conclude that for any t
),
∈
the function ω is bounded from above by the aﬃne function y, expressed as follows.

[1.68567,

∞

·

This implies that

ω(t) < y(t) = 1 + 2.37

t
2

.

·

Tmat(m, n2, m) = na

·

ω(2/a)+o(1) = o(na+2.37) = o(mn3.27).

(23)

Combining the results from (18) and (23) ﬁnishes the proof of the lemma.

4 Main Theorem

In this section, we give the formal statement of our main result.

Theorem 4.1 (Main result, formal). Consider a semideﬁnite program with variable size n
m constraints (assume there are no redundant constraints):

×

n and

max

C, X
h

i

subject to X

0,

Ai, X
h

i

(cid:23)

= bi for all i

[m].

∈

(24)

Assume that any feasible solution X
0 < δ
a positive semideﬁnite matrix X

≤

R. Then for any error parameter
0.01, there is an interior point method that outputs in time O∗(√n(mn2+mω +nω) log(n/δ))

satisﬁes

kop ≤

X
k

∈

×
0

such that

n

Sn
≥

n

Rn
≥

×
0

∈

C, X
h

i ≥ h

C, X ∗

δ

C

i −

· k

kop ·

R and

X

Ai,
h

i −

bi

≤

where ω is the exponent of matrix multiplication, X ∗ is any optimal solution to the semideﬁnite
program in (24), and

(cid:12)
(cid:12)
(cid:12)
Aik1 is the Schatten 1-norm of matrix Ai.
The proof of Theorem 4.1 is given in the subsequent sections.

[m] (cid:12)
Xi
∈
(cid:12)
(cid:12)

b

k

4nδ

(R

·

[m]
Xi
∈

Aik1 +
k

b
k

k1),

5 Approximate Central Path via Approximate Hessian

5.1 Main result for approximate central path

Our main result of this section is the following.

Theorem 5.1 (Approximate central path). Consider a semideﬁnite program as in Deﬁnition 1.1
R.
with no redundant constraints. Assume that any feasible solution X
0.1,
Then for any error parameter 0 < δ

kop ≤
0.01 and Newton step size ǫN satisfying √δ < ǫN ≤

satisﬁes

Sn
≥

X
k

≤

∈

×
0

n

16

Algorithm 1 outputs, in T = 40
ǫN
that satisﬁes

√n log(n/δ) iterations, a positive semideﬁnite matrix X

n

Rn
≥

×
0

∈

C, X
h

C, X ∗

i ≥ h

δ

C

· k

kop ·

i −

R and

X

i −

bi

Ai,
h
[m] (cid:12)
Xi
∈
(cid:12)
(cid:12)

b

(cid:12)
(cid:12)
(cid:12)

4nδ

(R

·

≤

[m]
Xi
∈

Aik1 +
k

b

k1),

k

(25)

where X ∗ is any optimal solution to the semideﬁnite program in Deﬁnition 1.1, and
Aik1 is the
Schatten 1-norm of matrix Ai. Further, in each iteration of Algorithm 1, the following invariant
holds for αH = 1.03:

k

1/2SnewS−

1/2

S−
k

I

kF ≤

αH ·

−

ǫN .

(26)

Proof. At the start of Algorithm 1, Lemma 9.1 is called to modify the semideﬁnite program to
obtain an initial dual solution y for the modiﬁed SDP that is close to the dual central path at
ǫ2
η = 1/(n + 2). This ensures that the invariant gη(y)⊤H(y)−
N holds at the start of the
algorithm. Therefore, by Lemma 5.4 and Lemma 5.5, this invariant continues to hold throughout the
run of the algorithm. Therefore, after T = 40
√n log
iterations, the step size η in Algorithm 1
ǫN
grows to η = (1 + ǫN

2n/δ2. It then follows from Lemma 5.6 that

1gη(y)

≤

n
δ

20√n )T /(n + 2)

≥

(cid:0)

(cid:1)

b⊤y

≤

b⊤y∗ +

n
η ·

(1 + 2ǫN )

≤

b⊤y∗ + δ2.

Thus when the algorithm stops, the dual solution y has duality gap at most δ2 for the modiﬁed SDP.
Lemma 9.1 then shows how to obtain an approximate solution to the original SDP that satisﬁes the
guarantees in (25).

To prove (26), deﬁne ∆S = Snew −

S

∈

δy,i to denote the i-th coordinate of vector δy. We rewrite

Rn

×

y
∈
1/2SnewS−

Rm. For each i
2
F as
k

1/2

−

I

[n], we use

∈

n and δy = ynew −

S−
k
1/2(∆S)S−

1/2)2

1/2SnewS−

1/2

S−
k

I

2
F = tr
k

−

= tr

(S−

h



1

S−

i
S−

1



δy,iAi

!

m

Xi=1

m

Xj=1

δy,jAj





m

=

δy,iδy,jtr[S−

1AiS−


1Aj]

Xi,j=1

= (δy)⊤H(y)δy

= gη(y)⊤

H(y)−

1H(y)

H(y)−

1gη(y),

(27)

m
i=1(δy)iAi. It then follows from Lemma 5.4 and the invariant

e

e

where we used the fact that ∆S =
1gη(y)
gη(y)⊤H(y)−

ǫ2
N that

≤

P

gη(y)⊤

H(y)−

1H(y)

H(y)−

1gη(y)

α2

H ·

ǫ2
N ,

≤

(28)

where αH = 1.03. Combining Equation (27) with Inequality (28) completes the proof of the
e
theorem.

e

17

 
2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

Table 5.1: Summary of parameters in approxiate central path.

Notation Choice Appearance Meaning
αH
ǫN
ǫS

Lemma 5.4
Lemma 5.5
Algorithm 2

1.03
0.1
0.01

Spectral approximation factor α−
(cid:22)
Upper bound on the Newton step size (g⊤η H −
S
Spectral approximation error (1

H

H

(cid:22)

S

ǫS)

1
H ·

αH ·
H
1gη)1/2

−

·

e
(cid:22)

(1 + ǫS)

S

·

(cid:22)

Algorithm 1
1: procedure Main(n, m, δ, ǫN , C, A, b)

⊲ C

Sn

n,

×

Ai}
{

m
i=1 ∈

∈

e
n, vector b

Sn

×

Rm, error

∈

parameter 0 < δ < 0.1, Newton step size parameter 0 < ǫN < 0.1

Modify the SDP and obtain an initial dual solution y according to Lemma 9.1
η
T

C.

i

n
δ

←
←

1/(n + 2)
40
√n log
ǫN
S
S
[m] yiAi −
(cid:1)
(cid:0)
←
←
∈
for iter = 1
T do
→
P
1 + ǫN
e
η
ηnew ←
20√n
, m do
for j = 1,
(cid:17)
(cid:16)
· · ·
gηnew (y)j ←
ηnew ·
, m do

end for
for j = 1,

· · ·
for k = 1,
Hj,k(y)

, m do
1
S−
tr[

· · ·
←

tr[S−

1

bj −

Aj]

·

⊲ Gradient computation

⊲ Hessian computation

Aj ·

·

1

S−

Ak]

·

e

e
1gηnew (y)

end for
end for
e
H(y)−
δy ← −
y + δy
ynew ←
e
Snew ←
ApproxSlackUpdate(Snew,
Snew ←
P
S
ynew, S
y
←
end for
e
Return an approximate solution to the original SDP according to Lemma 9.1

[m](ynew)iAi −

Snew,

Snew

S)

←

←

C

e

e

e

∈

i

⊲ Update on y
⊲ Approximate Newton step

⊲ Approximate slack computation
⊲ Update variables

22:
23: end procedure

5.2 Approximate slack update

Sn
Lemma 5.2. Given positive deﬁnite matrices Snew,
>0 and any parameter 0 < ǫS < 0.01,
×
there is an algorithm (procedure ApproxSlackUpdate in Algorithm 2) that takes O(nω+o(1)) time
to output a positive deﬁnite matrix

e
such that

Sn

∈

S

n

n

×
>0

Snew ∈
S−
e
new
k

1/2

SnewS−

1/2
new −

I

kop ≤

ǫS.

(29)

Proof. The runtime of O(nω+o(1)) is by the spectral decomposition Z = U
·
in the algorithm. To prove (29), we notice that λnew are the eigenvalues of S−
new
by the algorithm description (lines 6 - 13), the upper bound (λnew)i ≤

Λ

e

·

SnewS−
new
ǫS holds for each i

U ⊤, the costliest step
1/2
1/2
I and

−
[n].

∈

e

18

Algorithm 2 Approximate Slack Update
1: procedure ApproxSlackUpdate(Snew ,
2:

ǫS ←
S−
Zmid ←
new
Compute spectral decomposition Zmid = U

0.01

S−

new

1/2

1/2

−

S

e

I

·

·

S)

⊲ Snew,

S

Sn
n
0 are positive deﬁnite matrices
×
≥
⊲ Spectral approximation constant

∈

e

⊲ Λ = diag(λ1,

e

· · ·

[n] be a sorting permutation such that

Λ

U ⊤
, λn) are the eigenvalues of Zmid, and U
λπ(i+1)|

λπ(i)| ≥ |
|

·

·

Rn

×

n is orthogonal

∈

3:
4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

Let π : [n]
→
ǫS then
if
λπ(1)| ≤
|
S
Snew ←
else
e
1
r
←
while
r

e
λπ(2r)|
|
r + 1
←
end while

> ǫS or

λπ(2r)|
|

> (1

λπ(r)|
1/ log n)
|

−

do

15:

14:

(λnew)π(i) ← (
S + S1/2
Snew ←
new
end if
return
e
e
Snew
17:
18: end procedure

16:

0
λπ(i)
U

·

·

, 2r;

· · ·

if i = 1, 2,
otherwise.
diag(λnew −

λ)

U ⊤ ·

·

S1/2
new

e

5.3 Closeness of slack implies closeness of Hessian

e

1, if

Lemma 5.3. Given symmetric matrices A1,
Sn
H
>0 , deﬁne matrices
×

m and H

Rm

×

n

· · ·
Rm
×

, Am ∈
m as

Sn

×

n, and positive deﬁnite matrices

S, S

∈

∈

Hj,k = tr[
e

S−

1Aj

S−

∈
1Ak]

and

Hj,k = tr[S−

1AjS−

1Ak].

Then both

e

H and H are positive semideﬁnite. For any accuracy parameter αS ≥
S
e

α−

S,

S

e

e

αS ·

(cid:22)

1
S ·

(cid:22)

then we have that

Proof. For any vector v

m

m

∈

v⊤Hv =

vivjHi,j =

Xi=1
Similarly, we have

Xj=1

α−

2
S ·

H

(cid:22)
Rn, we deﬁne A(v) =

e
H

e

α2

S ·

H.

(cid:22)
m
i=1 viAi. We can rewrite v⊤Hv as follows.

vivjtr[S−

P
1AiS−

1Aj] = tr[S−

1/2A(v)S−

1A(v)S−

1/2].

(30)

m

m

Xi=1

Xj=1

v⊤

Hv = tr[

S−

1/2A(v)

S−

1A(v)

S−

1/2].

(31)

As the RHS of (30) and (31) are non-negative, both
e
S

e
e
1 (see Section 2.2), which gives the following inequalities
S−

H and H are positive semideﬁnite. Since

S, we have S−

e

1

αS ·

(cid:22)

e

tr[S−

(cid:22)

αS ·
1A(v)S−
1/2A(v)S−
e

1/2]

e
tr[S−

tr[

S−

αS ·
α2
S ·

≤

≤

1/2A(v)
1/2A(v)

S−

S−
e

1A(v)S−
1A(v)

S−

1/2]
1/2],

(32)

e

e

e

19

tr[S−

1/2A(v)S−

1
S ·
1A(v)S−

(cid:22)
1/2]

where the ﬁrst inequality follows from viewing tr[S−
ui = A(v)S−
property of trace. Similarly, using α−

1ui for
1/2ei and the second inequality follows similarly, after using the cyclic permutation

1/2A(v)S−

S, we have

1A(v)S−

1/2] as

n
i=1 u⊤i S−

P

S

α−

2
S ·

e
≥

tr[

S−

1/2A(v)

S−

1A(v)

S−

1/2].

(33)

Combining (32) and (33) with (30) and (31) along with the fact that v can be any arbitrary n-
dimensional vector ﬁnishes the proof of the lemma.

e

e

e

5.4 Approximate Hessian maintenance

Lemma 5.4. In each iteration of Algorithm 1, for αH = 1.03, the approximate Hessian
satisﬁes that

H(y)

α−

1
H H(y)

H(y)

αH ·

(cid:22)

(cid:22)

H(y).

Proof. By Lemma 5.2, given as input two positive deﬁnite matrices Snew and
a matrix

Snew such that

e

e

S, Algorithm 2 outputs

e

1/2
new −
where ǫS = 0.01 as in Algorithm 2. By deﬁnition of operator norm, this implies that in each iteration
of Algorithm 1, we have, for αS = 1.011,

SnewS−

kop ≤

S−
k

ǫS,

new

1/2

e

I

e

α−

1
S ·

S

S

S.

αS ·

(cid:22)

(cid:22)

The statement of this lemma then follows from Lemma 5.3.

e

5.5 Invariance of Newton step size

The following lemma is standard in the theory of interior point methods (e.g. see [Ren01]).

Lemma 5.5 (Invariance of Newton step [Ren01]). Given any parameters 1
1gη(y)
0 < ǫN ≤
and parameter η > 0, and positive deﬁnite matrix

ǫ2
N holds for some feasible dual solution y
H

1/10, suppose that gη(y)⊤H(y)−

αH ≤

satisﬁes

Sn

≤

≤

n

1.03 and
Rm

∈

Then ηnew = η(1 + ǫN

20√n ) and ynew = y

H −

−

α−

1
H H(y)

×
>0

∈
αH H(y)

H

e
(cid:22)
(cid:22)
1gηnew (y) satisfy

e

gηnew (ynew)⊤H(ynew)−

e

1gηnew (ynew)

ǫ2
N .

≤

5.6 Approximate optimality

The following lemma is also standard in interior point method.
Lemma 5.6 (Approximate optimality [Ren01]). Suppose 0 < ǫN ≤
y
∈

Rm, and parameter η

1 satisfy the following bound on Newton step size:

≥

1/10, dual feasible solution

Let y∗ be an optimal solution to the dual formulation (2). Then we have

gη(y)⊤H(y)−

1gη(y)

ǫ2
N .

≤

b⊤y

≤

b⊤y∗ +

n
η ·

(1 + 2ǫN ).

20

6 Low-rank Update

Crucial to being able to eﬃciently approximate the Hessian in each iteration is the condition that
the rank of the update be not too large. We formalize this idea in the following theorem, essential
to the runtime analysis in Section 7.
Theorem 6.1 (Rank inequality). Let r0 = n and ri be the rank of the update to the approximate
S when calling Algorithm 2 in iteration i of Algorithm 1. Then, over T iterations of
slack matrix
Algorithm 1, the ranks ri satisfy the inequality

e

T

The rest of this section is devoted to proving Theorem 6.1. To this end, we deﬁne the “error”

√ri ≤

O(T log1.5 n).

Xi=0

matrix Z

∈

Rn

n as follows

×

and the potential function Φ : Rn

n

×

Z = S−
R

→

e
n

1/2

1/2

SS−

I

−

(34)

Φ(Z) =

λ(Z)
|

λ(Z)
|
√i
where
|[i] denotes the i’th entry in the list of absolute eigenvalues of Z sorted in descend-
ing order. The following lemma bounds, from above, the change in the potential described by
Equation (35), when S is updated to Snew.
Lemma 6.2 (Potential change when S changes). Suppose matrices S, Snew and
inequalities

S satisfy the

Xi=1

(35)

|[i]

,

S−
k

1/2SnewS−
1/2

1/2

−
1/2
SS−

I

kF ≤

and

0.02

S−
k
I and Zmid = (Snew)−

1/2

Deﬁne matrices Z = S−

1/2

1/2

SS−

S(Snew)−

e

I

−
1/2

−

e

0.01.

kop ≤

(36)

I. Then we have

Proof. Our goal is to prove

e

−
Φ(Zmid)

Φ(Z)

−

log n.
e

≤

p

n

Xi=1

(λ(Z)[i] −

λ(Zmid)[i])2

10−

3.

≤

(37)

We ﬁrst show that the lemma statement is implied by (37). We rearrange the order of the eigenvalues
of Zmid and Z so that λ(Zmid)i and λ(Z)i are the ith largest eigenvalues of Zmid and Z, respectively.
3. Let τ be the
For each i
descending order of the magnitudes of eigenvalues of Zmid, i.e.
.
λ(Zmid)τ (n)|
The potential change Φ(Zmid)

2
10−
∆
2 ≤
k
k
λ(Zmid)τ (1)| ≥ · · · ≥ |
|
Φ(Z) can be upper bounded as

[n], denote ∆i = λ(Zmid)i−

λ(Z)i. Then (37) is equivalent to

∈

−

Φ(Zmid) =

≤

≤

≤

1
√i |

λ(Zmid)τ (i)|

n

Xi=1
n

Xi=1 (cid:18)

Φ(Z) +

Φ(Z) +

1
√i |

λ(Z)τ (i)|

+

1
√i |

∆τ (i)|
(cid:19)

1/2

n

1/2

2

∆i|
|

!

Xi=1

n

1
i !
Xi=1
log n,

p

21

 
 
where the third line follows from

Xi
and Cauchy-Schwarz inequality. This proves the lemma.

Xi

1
√i |

λ(Z)τ (i)| ≤

1
√i |

λ(Z)

|[i]

The remaining part of this proof is therefore devoted to proving (37). Deﬁne W = S−

1/2

new S1/2.

Then, we can express Zmid in terms of Z and W in the following way.

Zmid = (Snew)−

1/2

= (Snew)−

S(Snew)−
1/2

1/2S1/2S−
e

1/2

−
SS−

I
1/2S1/2(Snew)−

1/2

I

−

= W ZW ⊤ + W W ⊤

I.
e

−

Let λ(M )[i] denote the i’th (ordered) eigenvalue of a matrix M . We then have

n

Xi=1

(λ(Zmid)[i] −

λ(W ZW ⊤)[i])2

Zmid −

≤ k

W ZW ⊤

2
F
k

=

W ⊤W
k

I

2
F ,
k

−

(38)

(39)

where the ﬁrst inequality is by Fact 2.3 (which is applicable here because Zmid and W ZW ⊤ are
1/2SnewS−
1/2
both normal matrices) and the second step is by (38). Denote the eigenvalues of S−
4. It follows
by
νi}
10−
{
that

n
i=1. Then the ﬁrst assumption in (36) implies that

[n](νi −

1)2

×

≤

4

∈

i

W ⊤W
k

I

2
F =
k

S1/2S−
k

newS1/2
1

I

2
F =
k

−

−

1)2

5

×

≤

10−

4,

(40)

P
(1/νi −

[n]
Xi
∈

where the last inequality is because the ﬁrst assumption from (36) implies νi ≥
Plugging (40) into the right hand side of (39), we have

0.98 for all i

[n].

∈

n

Xi=1

(λ(Zmid)[i] −

λ(W ZW ⊤)[i])2

5

×

≤

10−

4.

(41)

n unitary
Let W = U ΣV ⊤ be the singular value decomposition of W , with U and V being n
matrices. Because of the invariance of the Frobenius norm under unitary transformation, (40) is
then equivalent to

×

n

Σ2
k

I

kF =

−

(σ2

i −

Xi=1

1)2

5

×

≤

10−

4.

(42)

Since U and V are unitary, the matrix W ZW ⊤ = U ΣV ⊤ZV ΣU ⊤ is similar to ΣV ⊤ZV Σ, and the
matrix Z ′ = V ⊤ZV is similar to Z. Therefore,

n

(λ(W ZW ⊤)[i] −

Xi=1

n

λ(Z)[i])2 =

(λ(ΣZ ′Σ)[i] −
2
F ,
Z ′
k

−

Xi=1
ΣZ ′Σ

≤ k

λ(Z ′)[i])2

(43)

22

where the last inequality is by Fact 2.3. We rewrite the Frobenius norm as

ΣZ ′Σ
k

−

Z ′

kF =
≤ k

(Σ
k
(Σ

−

−

I)Z ′(Σ
I)Z ′(Σ

−

−

I)Z ′ + Z ′(Σ
I) + (Σ
−
−
kF .
I)Z ′
(Σ
kF + 2
k

I)

−

I)

kF

(44)

The ﬁrst term can be bounded as:

(Σ
k

−

I)Z ′(Σ

−

I)2Z ′(Σ

I)]

−

2
I)
F = tr[(Σ
k
tr[(Σ
0.012
n

≤

≤

−

I)Z ′(Σ
I)4

−

·
tr[(Σ

−
(Z ′)2]
I)4]

−

1)4

·
(σi −
8,
10−

=

≤

Xi=1
5
×

(45)

Z ′kop =
k
n
i=1(σ2
i −

0.01,
Z
kop ≤
k
1)2. Similarly,

The ﬁrst inequality above uses Fact 2.1, the second used the observation that
and the last inequality follows from (42) and the fact that
we can bound the second term as

i=1(σi −

1)4

≤

n

P

P

(Σ
k

−

I)Z ′

2
F = tr[(Σ
k
tr[(Σ
0.012

≤

≤

·

−

−

I)(Z ′)2(Σ
I)2(Z ′)2]
tr[(Σ

I)2]

−

I)]

−

10−

7.

≤

It follows from (43), (44) and (46) that

n

(λ(W ZW ⊤)[i] −

Xi=1

λ(Z)[i])2

10−

6.

≤

(46)

(47)

Combining (41) and (47), we get that
This completes the proof of the lemma.

n

i=1(λ(Z)[i] −

λ(Zmid)[i])2

≤

10−

3 which establishes (37).

P
S changes). Given positive deﬁnite matrices Snew,
Snew and r be generated during the run of Algorithm 2 when the inputs are Snew and
1/2

Lemma 6.3 (Potential change when
let
the matrices Zmid = (Snew)−
e
we have

I and Znew = (Snew)−

Snew(Snew)−

e
S(Snew)−

1/2

1/2

1/2

−

Sn
>0,
S
∈
S. Deﬁne
e
I. Then
−
e

e

Φ(Zmid)

−

Φ(Znew)

4

10−
log n

≥

√r.

e

Proof. The setup of the lemma considers the eigenvalues of Z when
notational convenience, we deﬁne y =
1/2
Zmid = S−

S changes. For the sake of
, the vector of absolute values of eigenvalues of
λ(Zmid)
|
|

I. Recall from Table 5.1 that ǫS = 0.01. We consider two cases below.

SS−

new

new

1/2

e

−

Case 1. There does not exist an i
(1

e

≤

1/10 log n)y[i]. In this case, we have r = n/2. We consider two sub-cases.

n/2 that satisﬁes the two conditions y[2i] < ǫS and y[2i] <

−
• Case (a). For all i

In this case, we change all n coordinates of
y, and the change in each coordinate contributes to a potential decrease of at least ǫS/√n.
Therefore, we have Φ(Zmid)

[n], we have y[i] ≥

Φ(Znew)

ǫS√n

ǫS.

∈

10−4
log n √r.

−

≥

≥

23

• Case (b). There exists a minimum index i

≤
n/2. In this case, for all j in the above range, we have that y[2j] ≥

n/2 such that y[2j] < ǫS holds for all j in the range
1/10 log n)y[j].

i
In particular, picking j = i, 2i,

gives

(1

−

≤

≤

j

· · ·
y[i] ·

y[n] ≥

1/(10 log n))⌈

log n

(1

−

ǫS/10.

⌉

≥

Recalling that our notation y[i] denotes the i’th absolute eigenvalue in decreasing order, we
use the above inequality and repeat the argument from the previous sub-case to conclude that
Φ(Zmid)

Φ(Znew)

ǫS/10

√r.

√n

−

≥

·

10−4
log n ·

≥

Case 2. There exists an index i for which both the conditions y[2i] < ǫS and y[2i] < (1
are satisﬁed. By deﬁnition, r
ǫS and for all j′ ≥
all j′ < j, we have y[j′] ≥
1(b), we can prove y[r] ≥
ǫS/10. Moreover, y[2r] < (1
ynew the vector of magnitudes of the eigenvalues of Znew. Since ynew
have ynew
y[i]. Further, y[2r] < (1

1/10 log n)y[i]
n/2 is the smallest such index. Consider the index j such that for
j, we have y[j] < ǫS. By the same argument as in Case
1/10 log n)y[r] by deﬁnition of r. Denote by
[2r], we
[r], we have

[i]
1/10 log n)y[r] implies that for each i

is set to 0 for each i

−

−

≤

∈

[i] = y[i+2r] ≤

−

∈

y[i] −

ynew
[i] ≥

1
10 log n ·

y[r] ≥

2ǫS
10−
log n

=

4

10−
log n

,

where ǫS = 0.01 by Table 5.1. Therefore, we can bound, from below, the decrease in potential
function as

Φ(Zmid)

−

Φ(Znew)

ynew
[i]

y[i] −
√i

4

10−
log n

≥

√r.

r

≥

Xi=1

This ﬁnishes the proof of the lemma.

Proof of Theorem 6.1. Recall the deﬁnition of the potential function in (35) for an error matrix
Z

Sn

n:

×

∈

Φ(Z) =

λ(Z)
|
√i

|[i]

.

n

Xi=1

Let S(i) and
ﬁne Z (i) = (S(i))−
we have that

e

S(i) be the true and approximate slack matrices in the ith iteration of Algorithm 1. De-
I. By Lemma 6.2,

I and Z (i)

S(i)(S(i+1))−

S(i)(S(i))−

mid = (S(i+1))−

1/2

1/2

1/2

1/2

−

−

e

Φ(Z (i)

mid)

−

Φ(Z (i))

≤

e
log n.

p
From Lemma 6.3, we have the following potential decrease:

Φ(Z (i)

mid)

−

Φ(Z (i+1))

4

10−
log n

≥

√ri.

These together imply that

Φ(Z (i+1))

Φ(Z (i))

−

≤

log n

4

10−
log n

−

√ri.

(48)

We note that Φ(Z (0)) = 0 as we initialized
S = S in the beginning of the algorithm, and that the
potential function Φ(Z) is always non-negative. The theorem then follows by summing up (48) over
all T iterations.

e

p

24

7 Runtime Analysis

Our main result of this section is the following bound on the runtime of Algorithm 1.

Theorem 7.1 (Runtime bound). The total runtime of Algorithm 1 for solving an SDP with vari-
√n
, where ω is the matrix
able size n
multiplication exponent as deﬁned in Deﬁnition 3.2.

n and m constraints is at most O∗

mn2 + max(m, n)ω

×

(cid:0)

(cid:0)

(cid:1)(cid:1)

To prove Theorem 7.1, we ﬁrst upper bound the runtime in terms of fast rectangular matrix

multiplication times. The iteration complexity of Algorithm 1 is T =

O(√n).

Lemma 7.2 (Total cost). The total runtime of Algorithm 1 over T iterations is upper bounded as

e

TTotal ≤

O∗

min

n

·

nnz(A), mn2.5

+ √n max(m, n)ω +

(cid:0)

(cid:1)

T

Xi=0

Tmat(n, mri, n) +
(

Tmat(m, nri, m))

,

!
(49)

where nnz(A) is the total number of non-zero entries in all the constraint matrices, ri, as deﬁned
in Theorem 6.1, is the rank of the update to the approximation slack matrix
S in iteration i, and ω
and

Tmat are deﬁned in Deﬁnitions 3.2 and 3.1, respectively.

Remark 7.3. A more careful analysis can improve the ﬁrst term in the RHS of (49) to √n
·
1
nnz(A)1
ω(1)) . For the purpose of this paper, however, we will only need the
simpler bound given in Lemma 7.2.

(mn2)γ for γ =

2(3

−

−

γ

·

e

Proof. The total runtime of Algorithm 1 consists of two parts:
• Part 1. The time to compute the approximate Hessian

H(y) (which we abbreviate as

Line 11 - 15.

• Part 2. The total cost of operations other than computing the approximate Hessian.

e

H) in

e

Part 1.
We analyze the cost of computing the approximate Hessian
Part 1a. Initialization.
We start with computing

H.

H in the ﬁrst iteration of the algorithm. Each entry of

e

H involves the

computation

e

Hj,k = tr

S−
(

1/2Aj

S−

1/2)(

S−

1/2Ak

S−

e

.

1/2)
i

e
It ﬁrst costs O∗(nω) to invert
1/2Aj
Hessian,

S−

S−

e
S. Then the cost of computing the key module of the approximate

e

e

e

[m], is obtained by stacking the matrices Aj together:

h

1/2 for all j
e
e
S−1/2Aj

∈

T

e

e

e
S−1/2 for all j

[m] ≤
∈

O(

Tmat(n, mn, n)).

(50)

1/2Aj
Vectorizing the matrices
these rows vertically to form a matrix B of dimensions m
therefore have,

S−

S−

e

e

×

1/2 into row vectors of length n2, for each j

n2, one observes that

∈

[m], and stacking
H = BB⊤. We

Tcomputing e

H from B ≤

O(

Tmat(m, n2, m)).

e

(51)

25

 
Combining (50), (51), and the initial cost of inverting
for the ﬁrst iteration:

S gives the following cost for computing

H

T

≤

O∗(

part 1a

Tmat(m, n2, m) +

e
Tmat(n, mn, n) + nω).
Part 1b. Accumulating low-rank changes over all the iterations
Once the approximate Hessian in the ﬁrst iteration has been computed, every next iteration has
S (see
Snew has rank ri, Fact 2.4 implies that we can
1
e
.
V ⊤
V
new =
S−
−
−
1/2], where
1/2AjV V ⊤Ak
e

the approximate Hessian computed using a rank ri update to the approximate slack matrix
Line 15 of Algorithm 2). If the update from
compute, in time O(nω+o(1)), the n
The cost of updating
V

satisfying
H is then dominated by the computation of tr[

S to
ri matrices V+ and V
e

1 + V+V ⊤+ −
S−

ri is either V+ or V

. We note that

e
(52)

Rn

×

e

×

−

∈

−

e

nnz(A), mn2rω
i

O∗

min

[m] ≤
∈

TAjV for all j

ri ·
(cid:16)
where nnz(A) is the total number of non-zero entries in all the constraint matrices, and the second
term in the minimum is obtained by stacking the matrices Aj together and splitting it and V into
[m] essentially
matrices of dimensions ri ×
involves computing the matrix product of an n
mri matrix, which, by
Deﬁnition 3.1, costs

ri. Further, pre-multiplying

1/2 with AjV for all j

n matrix and an n

Tmat(n, mri, n). This, together with (53), gives

(53)

S−

(cid:17)(cid:17)

×

×

∈

(cid:16)

e

−

,

e
S−1/2Aj V for all j

T

[m] ≤

∈

O∗

Tmat(n, mri, n) + min
(cid:16)

ri ·
(cid:16)

nnz(A), mn2rω
i

−

2+o(1)

.

(54)

The ﬁnal step is to vectorize all the matrices
to get an m
costs, by deﬁnition,
update to the approximate Hessian:

1/2AjV , for each j
[m], and stack these vertically
nri matrix B, which gives the update to Hessian to be computed as BB⊤. This
Tmat(m, nri, m). Combining this with (54) gives the following run time for one

S−

×

∈

e

e

(cid:17)(cid:17)

S−
S−
e
e
2+o(1)

rank ri Hessian update

T

O∗

≤

Tmat(n, mri, n) + min

ri ·

nnz(A), mn2rω
i

−

2

+

Tmat(m, nri, m) + nω

(cid:0)

(cid:0)

Using this bound over all T =

O(√n) iterations, and applying

.
(55)
(cid:1)

(cid:1)
i=0 √ri ≤

T

O(√n) from Theo-

rem 6.1, gives

e

part 1b

T

≤

O∗

min(n

·

nnz(A), mn2.5) + √n

nω +

·

T

Xi=1

P

e

Tmat(n, mri, n) +
(

Tmat(m, nri, m))
!

. (56)

Combining Part 1a and 1b.
Combining (52) and (56), we have

part 1

T

≤ T

part 1a +

part 1b

T

O∗

min(n

·

≤

nnz(A), mn2.5) + √n

T

nω +

·

Tmat(n, mri, n) +
(
Xi=0

Tmat(m, nri, m))
!

, (57)

where we incorporated the bound from (52) into the i = 0 case.

Part 2.
Observe that there are four operations performed in Algorithm 1 other than computing

H:

26

e

 
 
• Part 2a. computing the gradient gη(y)
• Part 2b. inverting the approximate Hessian
• Part 2c. updating the dual variables ynew and S(ynew)
• Part 2d. computing the new approximate slack matrix

H

e

S(ynew)

Part 2a. The i’th coordinate of the gradient is expressed as gη(y)i = ηbi −

1Ai]. The cost
per iteration of computing this quantity equals O(nnz(A) + nω+o(1)), where the second term comes
from inverting the matrix S.

tr[S−

e

Part 2b. The cost of inverting the approximate Hessian
Part 2c. The cost of updating the dual variable ynew = y

is O(m2) per iteration. The cost of computing the new slack matrix Snew =
O(nnz(A)) per iteration.

P
Part 2d. The per iteration cost of updating the approximate slack matrix

−

H is O(mω+o(1)) per iteration.
H −
e
e

1gηnew (y), given
i

1 and gηnew (y),
C is

H −
[m](ynew)iAi −
∈
e
Snew is O(nω+o(1))

by Lemma 5.2.

Combining Part 2a, 2b, 2c and 2d.
e
O(√n) iterations
The total cost of operations other than computing the Hessian over the T =

is therefore bounded by

part 2

T

≤ T

part 2b +
part 2a +
T
O∗(√n(nnz(A) + max(m, n)ω)).

part 2c +

T

T

part 2d

≤

e

(58)

Combining Part 1 and Part 2.
Combining (57) and (58) and using r0 = n ﬁnishes the proof of the lemma.

total

T

≤ T

part 1 +

part 2

T

O∗

min

n

·

≤

nnz(A), mn2.5

+ √n max(m, n)ω +

(cid:0)

(cid:1)

T

Xi=0

Tmat(n, mri, n) +
(

Tmat(m, nri, m))

.

!

Lemma 7.4. Let
sequence that satisﬁes

Tmat be as deﬁned in Deﬁnition 3.1. Let T =

O(√n) and

r1,
{

· · ·

, rT }

be a

e

T

Xi=1

√ri ≤

O(T log1.5 n)

Tmat(m, nri, m)

≤

O∗(√n max(mω, nω) +

Tmat(m, n2, m)),

Property I. We have

T

Xi=1
Property II. We have

T

Xi=1

Tmat(n, mri, n)

≤

O∗(√n max(mω, nω) +

Tmat(n, mn, n)).

27

 
Proof. We give only the proof of Property I, as the proof of Property II is similar. Let m = na. For
each i

[0, 1]. Then

[T ], let ri = nbi, where bi ∈

∈

(59)

For each number k

Tmat(m, nri, m) =
, log n
0, 1,

∈ {

· · ·

Ik =

Then our assumption on the sequence

r1,
{
O(T log1.5 n). This implies that for each k
0, 1,
{
taking the summation of Eq. (59) over all i

Tmat(na, n1+bi, na) = naω((1+bi)/a)+o(1).
, deﬁne the set of iterations
}
i
{

ri ≤

2k+1

: 2k

.
}

[T ]

∈

≤
, rT }
can be expressed as
, we have
, log n
· · ·
}
[T ], we have

Ik| ≤
|

· · ·

∈

log n
k=0 |

Ik| ·

2k/2

≤
O(T log1.5 n/2k/2). Next,

P

T

Xi=1

Tmat(m, nri, m) =

=

≤

≤

≤

ω((1+bi)/a)

na
·

T

Xi=1
log n

ω((1+bi)/a)

na
·

Ik
Xk=0 Xi
∈
O(log n)

max
k

·

max
Ik
i
∈

O(1)

O(1)
e

·

·

max
k

2k

max
[0,1]
bi∈

max
nbi
≤
n1/2
−

≤

T log1.5 n
2k/2
√n
2k/2 ·

2k+1

ω((1+bi)/a)

na
·

·

ω((1+bi)/a)

na
·

bi/2+a
·

ω((1+bi )/a),

where the fourth step follows from T =
function g,

e
O(√n). To bound the exponent on n above, we deﬁne the

e
g(bi) = 1/2

bi/2 + a

·

−

ω((1 + bi)/a).

(60)

This function is convex in bi due to the convexity of the function ω (Lemma 3.6). Therefore, over
[0, 1], the maximum of g is attained at one of the end points. We simply evaluate
the interval bi ∈
this function at the end points.

Case 1. Consider the case bi = 0. In this case, we have g(0) = 1/2 + aω(1/a). We consider the

following two subcases. Case 1a. If a

1, then we have

≥

g(0) = 1/2 + a

ω(1/a)

·

≤

1/2 + aω(1) = 1/2 + aω

Case 1b. If a

(0, 1), then we deﬁne k = 1/a > 1. It follows from Lemma 3.5 and ω > 1, that

∈
g(0) = 1/2 + a

ω(1/a) = 1/2 + ω(k)/k

1/2 + (k

−

≤

1 + ω)/k

≤

1/2 + ω.

·

Combining both Case 1a and Case 1b, we have that

ng(0)

≤

max(n1/2+aω, n1/2+ω)

√n

·

≤

max(mω, nω).

Case 2 Consider the other case of bi = 1. In this case, g(1) = 1/2
−
We now ﬁnish the proof by combining Case 1 and Case 2 as follows.

1/2 + aω(2/a) = aω(2/a).

max
[0,1]
bi∈

n1/2
−

bi+a
·

ω((1+bi)/a)

≤

√n max(mω, nω) + na
·

ω(2/a).

28

Proof of Theorem 7.1. In light of Lemma 7.4, the upper bound on runtime given in Lemma 7.2 can
be written as

Tmat(m, n2, m)
(cid:1)

. (61)

TTotal ≤

O∗

min

n

·

nnz(A), mn2.5

+ √n max(m, n)ω +

Tmat(n, mn, n) +

Combining this with 3.10, we have the following upper bound on the total runtime of Algo-

(cid:0)

(cid:8)

(cid:9)

rithm 1:

TTotal ≤
≤

O∗

min

O∗

√n
(cid:0)

nnz(A), mn2.5
n
mn2 + max(m, n)ω
(cid:8)
(cid:9)

·

+ √n max(m, n)ω + √n

mn2 + mω

.

(cid:0)

(cid:1)(cid:1)

This ﬁnishes the proof of the theorem.

(cid:0)

(cid:0)

(cid:1)(cid:1)

8 Comparison with Cutting Plane Method

In this section, we prove Theorem 1.3, restated below.

Theorem 1.3 (Comparison with Cutting Plane Method). When m
method that solves an SDP with n
the current best cutting plane method [LSW15, JLSW20], over all regimes of nnz(A).

n, there is an interior point
n matrices, m constraints, and nnz(A) input size, faster than

≥

×

Remark 8.1. In the dense case with nnz(A) = Θ(mn2), Algorithm 1 is faster than the cutting
plane method whenever m

√n.

≥

Proof of Theorem 1.3. Recall that the current best runtime of the cutting plane method for solving
nnz(A) + mn2.372927 + m3) [LSW15, JLSW20], where 2.372927 is the
an SDP (1) is
current best upper bound on the exponent of matrix multiplication ω. By Lemma 7.2 and 7.4, we
have the following upper bound on the total runtime of Algorithm 1:

CP = O∗(m

T

·

TTotal ≤

O∗

min

n

·

nnz(A), mn2.5

+ √n max(m, n)ω +

Tmat(n, mn, n) +

(cid:0)

(cid:8)

(cid:9)

n by assumption, Lemma 3.9 and 3.9 further simplify the runtime to

Tmat(m, n2, m)
(cid:1)

Since m

≥

O∗

min

n

·

nnz(A), mn2.5

TTotal ≤
n

nnz(A), mn2.5
nnz(A)
Note that min
n. Furthermore, Lemma 3.11 states that
m
each term on the RHS of (62) is upper bounded by

m

≥

(cid:8)

(cid:9)

·

·

(cid:8)
≤

(cid:0)

+ √nmω +

Tmat(m, n2, m)
(cid:1)
CP) and that √nmω = o(m3)
≤
Tmat(m, n2, m) = o(m3 + mn2.37)

(cid:9)
O(
T

T
CP, we make the stated conclusion.

≤
o(

o(

≤

(62)

CP) since
T
CP). Since

T

9 Initialization

Lemma 9.1 (Initialization). Consider a semideﬁnite program as in Deﬁnition 1.1 of dimension
n

n with m constraints, and assume that it has the following properties.

×
1. Bounded diameter: for any X

2. Lipschitz objective:

C

k

kop ≤

(cid:23)
L.

0 with

Ai, X
h

i

= bi for all i

∈

[m], we have

X
k

kop ≤

R.

29

For any 0 < δ

≤

1, the following modiﬁed semideﬁnite program

max
X

0 h

C, X

i

(cid:23)
s.t.

Ai, X
h

i

= bi,

i

∀

∈

[m + 1],

where

Ai =

0n
0
0

Ai
0⊤n
0⊤n





0n
0
tr[Ai]





bi
R −

,

[m],

i

∀

∈

Am+1 =

In
0⊤n
0⊤n



0n 0n
0
1
0
0

satisﬁes the following statements.



, b =

1
R b
n + 1

(cid:21)

(cid:20)

, C =





1. The following are feasible primal and dual solutions:

C
·
0⊤n
0⊤n

δ
L 0n
0
0





,

0n
0

1


−

X = In+2 , y =

0m
1

(cid:20)

(cid:21)

, S =

C
In −
0⊤n

0⊤n

·



δ
L 0n 0
0
1

1
0


.

2. For any feasible primal and dual solutions (X, y, S) with duality gap at most δ2, the matrix
n block submatrix of X, is an approximate

X [n]

[n], where X [n]

X = R
solution to the original semideﬁnite program in the following sense:
b

[n] is the top-left n

×

×

×

·

X

C,
h

C, X ∗

LR

δ,

·

i −

i ≥ h
0,

(cid:23)

X
b
bi
b
(cid:12)
(cid:12)
(cid:12)

X

i −

Ai,
h
[m] (cid:12)
Xi
∈
(cid:12)
(cid:12)

b

4nδ

(R

·

≤

[m]
Xi
∈

Aik1 +

k

b

k1),

k

where X ∗ is any optimal solution to the original SDP and
of a matrix A.

k1 denotes the Schatten 1-norm
A

k

Proof. For the ﬁrst result, straightforward calculations show that
[m + 1],
S = C. Now we prove the second result. Denote OPT and OPT the
and that
optimal values of the original and modiﬁed SDP respectively. Our ﬁrst goal is to establish a lower
bound for OPT in terms of OPT. For any optimal solution X
n of the original SDP, consider
the following matrix X

[m+1] yiAi −

= bi for all i

Ai, X
h

R(n+2)

(n+2)

P

Sn

∈

∈

×

i

∈

i

×

∈

X =



1
R X
0⊤n
0⊤n

0n

n + 1

−
0

1
R tr[X]

0n
0
0

.




Notice that X is a feasible primal solution to the modiﬁed SDP, and that



OPT

C, X

≥ h

=

i

δ
LR · h

C, X

δ
LR ·

=

i

OPT,

30

where the ﬁrst step follows because the modiﬁed SDP is a maximization problem, and the ﬁnal step
is because X is an optimal solution to the original SDP.

Given a feasible primal solution X
[n] 0n 0n
0
τ
θ
0

X [n]
×
0⊤n
0⊤n



could assume X =





R(n+2)

×

∈

(n+2) of the modiﬁed SDP with duality gap δ2, we

without loss of generality, where τ, θ

0. This is because if

≥

the entries of X other than the diagonal and the top-left n
n block are not 0, then we could zero
these entries out and the matrix remains feasible and positive semideﬁnite. We thus immediately
have

0. Notice that

X

×



(cid:23)

b

δ
L · h

C, X [n]

[n]i −

×

θ =

C, X
h

i ≥

OPT

δ2

−

≥

δ
LR ·

OPT

δ2.

−

(63)

Therefore, we can lower bound the objective value for X [n]

[n] in the original SDP as

×

· h
where the last inequality follows from (63). By matrix Hölder inequality, we have

−

×

i

·

X

= R

C, X [n]

C,
h

[n]i ≥

OPT

LR

δ,

b

δ
L · h

C, X [n]

[n]i ≤

×

≤

C

δ
L · k
δ
L · k
(n + 1)δ,

C

kop ·

tr

X [n]

[n]

×

(cid:2)

Am+1, X

(cid:3)

i

kop · h

where in the last step follows from

C

k

kop ≤

≤
L and bm+1 = n + 1. We can thus upper bound θ as

θ

δ
L · h

C, X [n]

≤

[n]i
×
where the ﬁrst step follows from (63), the second step follows from OPT
where
feasiblity of X for the modiﬁed SDP, we have

k·k1 is the Schatten 1-norm, and the last step follows from δ

−

≤

≥ − k
1
≤

≤

+ δ2

OPT

(2n + 1)δ + δ2

δ
LR ·

4nδ,

≤

(64)

C
nLR
X ∗k1 ≥ −
n. Notice that by the

kop ·k

Ai, X [n]
h

[n]i

×

+ (

1
R ·

bi −

tr[Ai])θ =

1
R ·

bi.

This implies that

X

Ai,
h

i −

bi

=

(bi −
|

R

·

tr[Ai])θ

4nδ

(R

Aik1 +

k

),
bi|
|

·

| ≤

where the ﬁnal step follows from the upper bound of θ in (64). Summing the above inequality up
over all i

[m] ﬁnishes the proof of the lemma.

b

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

∈

Acknowledgment

We thank Aaron Sidford for many helpful discussions and Deeksha Adil, Sally Dong, Sandy Kaplan,
and Kevin Tian for useful feedback on the writing. We gratefully acknowledge funding from CCF-
1749609, CCF-1740551, DMS-1839116, Microsoft Research Faculty Fellowship, and Sloan Research
Fellowship. Zhao Song is partially supported by Ma Huateng Foundation, Schmidt Foundation,
Simons Foundation, NSF, DARPA/SRC, Google and Amazon.

31

References

[AK07]

[ALO16]

Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semideﬁnite
programs. In Proceedings of the 39th Annual ACM Symposium on Theory of Computing
(STOC), 2007.

Zeyuan Allen Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a
width-independent, parallel, simpler, and faster positive SDP solver. In Proceedings of
the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms(SODA),
2016.

[Ans00]

Kurt M Anstreicher. The volumetric barrier for semideﬁnite programming. Mathematics
of Operations Research, 2000.

[ARV09]

Sanjeev Arora, Satish Rao, and Umesh Vazirani. Expander ﬂows, geometric embeddings
and graph partitioning. Journal of the ACM (JACM), 2009.

[AV95]

[AZL17]

[Ban19]

[BCS97]

[BDG16]

[BG17]

David S Atkinson and Pravin M Vaidya. A cutting plane algorithm for convex pro-
gramming that uses analytic centers. Mathematical Programming, 69(1-3):1–43, 1995.

Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: faster online learning
of eigenvectors and faster mmwu. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, 2017.

Nikhil Bansal. On a generalization of iterated and randomized rounding. In Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory of Computing (STOC), 2019.

Peter Bürgisser, Michael Clausen, and Mohammad A Shokrollahi. Algebraic complexity
theory, volume 315. Springer Science & Business Media, 1997.

Nikhil Bansal, Daniel Dadush, and Shashwat Garg. An algorithm for komlós conjecture
matching banaszczyk. In 57th Annual IEEE Symposium on Foundations of Computer
Science (FOCS), 2016.

Nikhil Bansal and Shashwat Garg. Algorithmic discrepancy beyond partial coloring.
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing
(STOC), 2017.

[Blä13]

Markus Bläser. Fast matrix multiplication. Theory of Computing, pages 1–60, 2013.

[BLSS20]

Jan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense
linear programs in nearly linear time. In 52nd Annual ACM SIGACT Symposium on
Theory of Computing (STOC), 2020.

[Bra20]

[BV02]

Jan van den Brand. A deterministic linear program solver in current matrix multipli-
cation time. In ACM-SIAM Symposium on Discrete Algorithms (SODA), 2020.

Dimitris Bertsimas and Santosh Vempala. Solving convex programs by random walks.
In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing
(STOC), pages 109–115. ACM, 2002.

[CDG19]

Yu Cheng, Ilias Diakonikolas, and Rong Ge. High-dimensional robust mean estimation
in nearly-linear time. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA). SIAM, 2019.

32

[CDGW19] Yu Cheng, Ilias Diakonikolas, Rong Ge, and David Woodruﬀ. Faster algorithms for high-
dimensional robust covariance estimation. In Conference on Learning Theory (COLT),
2019.

[CDST19] Yair Carmon, John C. Duchi, Aaron Sidford, and Kevin Tian. A rank-1 sketch for
matrix multiplicative weights. In Conference on Learning Theory, COLT 2019, 25-28
June 2019, Phoenix, AZ, USA, pages 589–623, 2019.

[CG18]

[CLS19]

Yu Cheng and Rong Ge. Non-convex matrix completion against a semi-random adver-
sary. In Conference On Learning Theory (COLT), 2018.

Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current
In Proceedings of the 51st Annual ACM Symposium on
matrix multiplication time.
Theory of Computing (STOC), 2019.

[Dan47]

George B Dantzig. Maximization of a linear function of variables subject to linear
inequalities. Activity analysis of production and allocation, 13:339–347, 1947.

[Eld13]

[GH16]

[GLS81]

[GU18]

[GV02]

[GW95]

[HJ12]

[HW53]

Ronen Eldan. Thin shell implies spectral gap up to polylog via a stochastic localization
scheme. Geometric and Functional Analysis, 2013.

Dan Garber and Elad Hazan. Sublinear time algorithms for approximate semideﬁnite
programming. Mathematical Programming, 158(1-2):329–361, 2016.

Martin Grötschel, László Lovász, and Alexander Schrijver. The ellipsoid method and
its consequences in combinatorial optimization. Combinatorica, 1981.

François Le Gall and Florent Urrutia. Improved rectangular matrix multiplication using
powers of the coppersmith-winograd tensor. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms, SODA ’18, 2018.

Jean-Louis Goﬃn and Jean-Philippe Vial. Convex nondiﬀerentiable optimization: A
survey focused on the analytic center cutting plane method. Optimization methods and
software, 2002.

Michel X Goemans and David P Williamson. Improved approximation algorithms for
maximum cut and satisﬁability problems using semideﬁnite programming. Journal of
the ACM (JACM), 1995.

Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press,
New York, NY, USA, 2nd edition, 2012.

A. J. Hoﬀman and H. W. Wielandt. The variation of the spectrum of a normal matrix.
Duke Math. J., 20(1):37–39, 03 1953.

[JJUW11] Rahul Jain, Zhengfeng Ji, Sarvagya Upadhyay, and John Watrous. QIP = PSPACE.

Journal of the ACM (JACM), 2011.

[JLL+20] Arun Jambulapati, Yin Tat Lee, Jerry Li, Swati Padmanabhan, and Kevin Tian. Posi-
tive semideﬁnite programming: mixed, parallel, and width-independent. In Proccedings
of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020,
Chicago, IL, USA, June 22-26, 2020. ACM, 2020.

33

[JLSW20] Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting
plane method for convex optimization, convex-concave games and its applications. In
STOC, 2020.

[JY11]

[Kar84]

[Kha80]

[KM03]

[KMS94]

Rahul Jain and Penghui Yao. A parallel approximation algorithm for positive semideﬁ-
nite programming. In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foun-
dations of Computer Science (FOCS), 2011.

Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In
Proceedings of the sixteenth annual ACM symposium on Theory of computing (STOC),
1984.

Leonid G Khachiyan. Polynomial algorithms in linear programming. USSR Computa-
tional Mathematics and Mathematical Physics, 20(1):53–72, 1980.

Kartik Krishnan and John E Mitchell. Properties of a cutting plane method for semidef-
inite programming. submitted for publication, 2003.

David Karger, Rajeev Motwani, and Madhu Sudan. Approximate graph coloring by
semideﬁnite programming. In Proceedings 35th Annual Symposium on Foundations of
Computer Science (FOCS). IEEE, 1994.

[KTE88]

Leonid G Khachiyan, Sergei Pavlovich Tarasov, and I. I. Erlikh. The method of inscribed
ellipsoids. In Soviet Math. Dokl, volume 37, pages 226–230, 1988.

[Lee16]

[LP20]

[LS14]

[LS15]

[LSW15]

[NN89]

Yin Tat Lee. Faster algorithms for convex and combinatorial optimization. PhD thesis,
Massachusetts Institute of Technology, 2016.

Yin Tat Lee and Swati Padmanabhan. An $\widetilde\mathcalo(m/\varepsilonˆ3.5)$-
cost algorithm for semideﬁnite programs with diagonal constraints. In Jacob D. Aber-
nethy and Shivani Agarwal, editors, Conference on Learning Theory, COLT 2020, 9-12
July 2020, Virtual Event [Graz, Austria], Proceedings of Machine Learning Research.
PMLR, 2020.

Yin Tat Lee and Aaron Sidford. Path ﬁnding methods for linear programming: Solving
linear programs in O(√rank) iterations and faster algorithms for maximum ﬂow. In
55th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2014.

Yin Tat Lee and Aaron Sidford. Eﬃcient inverse maintenance and faster algorithms for
linear programming. In 56th Annual IEEE Symposium on Foundations of Computer
Science (FOCS), 2015.

Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method
and its implications for combinatorial and convex optimization. In 56th Annual IEEE
Symposium on Foundations of Computer Science (FOCS), 2015.

Yurii Nesterov and Arkadi Nemirovski. Self-concordant functions and polynomial time
methods in convex programming. preprint, central economic & mathematical institute,
ussr acad. Sci. Moscow, USSR, 1989.

[NN92]

Yurii Nesterov and Arkadi Nemirovski. Conic formulation of a convex programming
problem and duality. Optimization Methods and Software, 1(2):95–115, 1992.

34

[NN94]

[PT12]

[Ren01]

[Sho77]

[Sid15]

[Str91]

[Vai87]

[Vai89a]

[Vai89b]

Yurii Nesterov and Arkadi Nemirovski. Interior-point polynomial algorithms in convex
programming, volume 13. Siam, 1994.

Richard Peng and Kanat Tangwongsan. Faster and simpler width-independent parallel
algorithms for positive semideﬁnite programming. In Proceedings of the twenty-fourth
annual ACM symposium on Parallelism in algorithms and architectures (SPAA), pages
101–108, 2012.

James Renegar. A Mathematical View of Interior-point Methods in Convex Optimiza-
tion. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2001.

Naum Z Shor. Cut-oﬀ method with space extension in convex programming problems.
Cybernetics and systems analysis, 13(1):94–96, 1977.

Aaron Daniel Sidford. Iterative methods, combinatorial optimization, and linear pro-
gramming beyond the universal barrier. PhD thesis, Massachusetts Institute of Tech-
nology, 2015.

Volker Strassen. Degeneration and complexity of bilinear maps: some asymptotic spec-
tra. J. reine angew. Math, 413:127–180, 1991.

Pravin M Vaidya. An algorithm for linear programming which requires o(((m + n)n2 +
(m + n)1.5n)l) arithmetic operations. In 28th Annual IEEE Symposium on Foundations
of Computer Science (FOCS), 1987.

Pravin M Vaidya. A new algorithm for minimizing convex functions over convex sets.
In 30th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages
338–343, 1989.

Pravin M Vaidya. Speeding-up linear programming using fast matrix multiplication. In
30th Annual Symposium on Foundations of Computer Science (FOCS), pages 332–337.
IEEE, 1989.

[VB96]

Lieven Vandenberghe and Stephen P. Boyd. Semideﬁnite programming. SIAM Review,
1996.

[Woo49]

Max A Woodbury. The stability of out-input matrices. Chicago, IL, 9, 1949.

[Woo50]

Max A Woodbury. Inverting modiﬁed matrices. 1950.

[YN76]

David B Yudin and Arkadi S Nemirovski. Evaluation of the information complexity of
mathematical programming problems. Ekonomika i Matematicheskie Metody, 12:128–
142, 1976.

[YTF+19] Alp Yurtsever, Joel A. Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher.

Scalable semideﬁnite programming, 2019.

35

A Matrix Multiplication: A Tensor Approach

The main goal of this section is to rederive, using tensors, some of the technical results from
Section 3. In particular, we use tensors to derive upper bounds on the time to perform the following
two rectangular matrix multiplication tasks (Lemma A.12 and A.13):

• Multiplying a matrix of dimensions m

• Multiplying a matrix of dimensions n

n2 with one of dimensions n2
×
mn with one of dimensions mn

m,

n.

×

×

×

Our hope is that these techniques will eventually be useful in further improving the results of this
paper.

A.1 Exponent of matrix multiplication

We recall two deﬁnitions to describe the cost of certain fundamental matrix operations, along with
their properties.

Deﬁnition A.1. Deﬁne
of matrices of dimensions n

Tmat(n, r, m) to be the number of operations needed to compute the product

r and r

m.

×

×

Tmat(n, nk, n) =
Deﬁnition A.2. We deﬁne the function ω(k) to be the minimum value such that
nω(k)+o(1). We overload notation and use ω to denote the exponent of matrix multiplication (in
n matrices is nω), and let α denote the dual exponent
other words, the cost of multiplying two n
of matrix multiplication. Thus, we have ω(1) = ω and ω(α) = 2.

×

Lemma A.3 ([GU18]). We have :

• ω = ω(1)

≤

• ω(1.5)

• ω(1.75)

• ω(2)

≤

2.372927,

≤
2.79654,

3.02159,

≤
3.251640.

Lemma A.4 ([BCS97, Blä13]). For any three positive integers n, m, r, we have

Tmat(n, r, m) = O(
A.2 Matrix multiplication tensor

Tmat(n, m, r)) = O(

Tmat(m, n, r)).

The rank of a tensor T , denoted as R(T ), is the minimum number of simple tensors that sum up
to T . For any two tensors S = (Si,j,k)i,j,k and T = (Ta,b,c)a,b,c, we write S
T if there exist three
matrices A, B and C (of appropriate sizes) such that Si,j,k =
a,b,c Ai,aBj,bCk,cTa,b,c for all i, j, k.
For any i, j, k, denote ei,j,k the tensor with 1 in the (i, j, k)-th entry, and 0 elsewhere.

≤

P

Deﬁnition A.5 (Matrix-multiplication tensor). For any three positive integers a, b, c, we deﬁne

a, b, c
i
h

:=

[c]
[b] Xk
[a] Xj
Xi
∈
∈
∈

ei(b

−

1)+j,j(c

−

1)+k,k(a

−

1)+i

to be the matrix-multiplication tensor corresponding to multiplying a matrix of size a
of size b

c.

×

b with one

×

36

It’s not hard to show that for any ni and mi where i = 1, 2, 3, we have

n1, n2, n3i ⊗ h
h

m1, m2, m3i

=

.
n1m1, n2m2, n3m3i
h

=

n
Let
h
i
we have

P

i

[n] ei,i,i be the identity tensor. For any three tensors S, T1 and T2, if T1 ≤

∈

T2, then

T1 ≤
Lemma A.6 (Monotonicity of tensor rank, [Str91]). Tensor rank is monotone under the relation
, i.e. if T1 ≤

T2, then we have

T2.

⊗

⊗

≤

S

S

R(T1)

≤

R(T2).

Lemma A.7 (Sub-multiplicity of tensor rank, [Str91]). For any tensors T1 and T2, we have

Lemma A.8. The tensor rank of a matrix multiplication tensor is equal to the cost of multiplying
the two correponding sized matrices up to some constant factor, i.e.,

R(T1 ⊗

T2)

≤

R(T1)

·

R(T2).

Tmat(a, b, c)).
A.3 Implication of matrix multiplication technique

) = Θ(
a, b, c
R(
i
h

Lemma A.9 (Sub-linearity). For any p

q

≥

≥

1, we have

ω(p)

p

−

≤

q + ω(q).

Proof. We have

Applying tensor rank on both sides

n, np, n
h

i

=

n, nq, n
h

1, np

i ⊗ h

−

q, 1
.
i

n, np, n
R(
h

n, nq, n
) = R(
h
i
n, nq, n
R(
h

≤

1, np
−
1, np
R(
h

q, 1
)
i
q, 1
),
i

−

i ⊗ h
)
i

·

where the last line follows from Lemma A.7. Applying Lemma A.8, we have

Tmat(n, np, n)

≤

O(1)

· Tmat(n, nq, n)

·

np

−

q

Using the deﬁnition of ω(p), we have

nω(p)+o(1)

O(1)

·

≤

nω(q)+o(1)

np

q.

−

·

Comparing the exponent on both sides completes the proof.

The next lemma establishes the convexity of ω(k) as a function of k.

Lemma A.10 (Convexity of ω(k)). The fast rectangular matrix multiplication time exponent ω(k)
as deﬁned in Deﬁnition A.2 is convex in k.

37

Proof. Let k = α

·

p + (1

α)

q for α

(0, 1). We have

·

−
n, nk, n
h

=

i

∈
nα, nα
·
h

p, nα

n1
−

i ⊗ h

α, n(1
−

α)p, n1
−

α

.
i

Applying the tensor rank on both sides,

n, nk, n
R(
h

nα, nα
) = R(
·
h
i
nα, nα
R(
·
h

≤

p, nα
p, nα

α

n1
α, n(1
−
−
α, n(1
n1
R(
−
−
h

α)p, n1
)
−
i
α
α)p, n1
−

i ⊗ h
)
i

·

),
i

where the last line follows from Lemma A.7. By Lemma A.8, we have

Tmat(n, nk, n)
), we have
By deﬁnition of ω(
·

O(1)

· Tmat(nα, nαp, nα)

· Tmat(n1

−

≤

α, n(1
−

α)p, n1
−

α)

nω(k)+o(1)

O(1)

·

≤

ω(p)

nα
·

·

n(1
−

α)ω(1

−

p).

By comparing the exponent, we know that

ω(k)

α

·

≤

ω(p) + (1

α)

·

−

ω(1

−

p).

Lemma A.11. Let
k, we have

Tmat be deﬁned as in Deﬁnition A.1. Then for any positive integers a, b, c and

Tmat(a, bk, c)

O(

Tmat(ak, b, ck)).

≤

Proof. Notice that

Therefore, we have

1, k, 1
h

i ≤ h

k, 1, k

.
i

a, bk, c
i
h

=

a, b, c
h
a, b, c

≤ h
=

ak, b, ck
h

i ⊗ h

i ⊗ h
.
i

1, k, 1
i
k, 1, k

i

It then follows from Lemma A.6 that

Finally, using Lemma A.8 gives

)
a, bk, c
R(
i
h

ak, b, ck
R(
h

).
i

≤

Thus we complete the proof.

Tmat(a, bk, c)

O(

Tmat(ak, b, ck)).

≤

38

A.4 General bound on

Tmat(n, mn, n) and

Tmat(m, n2, m)

Lemma A.12. Let
If m

n, then we have

Tmat be deﬁned as in Deﬁnition A.1.

≥

If m

≤

n, then we have

Tmat(n, mn, n)

≤

O(

Tmat(m, n2, m)).

Tmat(m, n2, m)

O(

Tmat(n, mn, n)).

≤

Proof. We only prove the case of m
≥
immediate consequence of Lemma A.11 by taking a = c = n, b = n2, and k =
positive integer because m

n, as the other case where m < n is similar. This is an
, where k is a
⌋

m/n
⌊

n.

≥

In the next lemma, we derive upper bounds on the term
Tmat(n, mn, n) when m < n, which is crucial to our runtime analysis.
Lemma A.13. Let
Property I. We have

Tmat be deﬁned as in Deﬁnition A.1 and ω be deﬁned as in Deﬁnition A.2.

Tmat(m, n2, m) when m

n and

≥

Property II. We have

Proof. Property I.

Since

Tmat(n, mn, n)

≤

O(mnω+o(1)).

Tmat(m, n2, m)

≤

O

√n

mn2 + mω

.

(cid:0)

(cid:0)

(cid:1)(cid:1)

Applying the tensor rank on both sides, we have

n, mn, n
h

i

=

n, n, n
h

.
1, m, 1
i

i ⊗ h

n, mn, n
R(
h

n, n, n
) = R(
h
i
n, n, n
R(
h

≤

Thus, we complete the proof.

Property II.
Let m = na, where a

(0,

∈

∞

). We have

)
1, m, 1
i

)
1, m, 1
R(
i
h

i ⊗ h
)
i

·

It implies that

m, n2, m
h

i

=

na, (na)2/a, na
h

i

Tmat(m, n2, m) = na

·

ω(2/a)+o(1)

The Property II is then an immediate consequence of the following inequality, which we prove

next:

ω(2/a) < max(1 + 2.5/a, ω(1) + 0.5/a)

a

∀

∈

(0,

).

∞

39

Deﬁne b = 2/a

(0,

∈

∞

). Then the above desired inequality can be expressed in terms of b as

ω(b) < max(1 + 5b/4, ω(1) + b/4)

b

∀

∈

(0,

).

∞

(65)

1. By the convexity of ω(

Notice that the RHS of (15) is a maximum of two linear functions of b and these intersect at
) as proved in Lemma A.10, it suﬃces to verify (15) at the
b∗ = ω(1)
−
and b = b∗. In the case where b = δ for any δ < 1, (15) follows immediately
endpoints b
3.252.
from the observation that ω(δ) < ω(1). For the case b
→ ∞
It then follows from Lemma A.9 that for any b > 2, we have

, by Lemma A.3 we have ω(2)

→ ∞

0, b

→

≤

·

The ﬁnal case is where b = b∗ = ω(1)

1, for which (15) is equivalent to

−

ω(b)

b

−

≤

2 + ω(2)

≤

1 + 5b/4.

ω(ω(1)

−

1) < 5ω(1)/4

1/4.

−

(66)

By Lemma A.3, we have that ω(1)
that

2

−

∈

[0, 0.372927]. Then to prove (66), it is suﬃcient to show

ω(t + 1) < 5t/4 + 9/4

t

∀

∈

[0, 0.372927].

By the convexity of ω(
Lemma A.3, we have for k

·

[1, 2],

∈

) as proved in Lemma A.10 and the upper bound of ω(2)

(67)

3.251640 in

≤

ω(k)

≤

ω(1) + (k

1)

·

−

(3.251640

−

(t + 2)) = t + 2 + (k

1)

·

−

(1.251640

t).

−

In particular, using this inequality for k = t + 1, we have

ω(t + 1)

5t/4

9/4

−

−

≤
=

(t + 2) + t

(1.251640

·

t2 + 1.00164t

t)

−

−

5t/4

−

9/4

−

1/4,

−

which is negative on the entire interval [0, 0.372927]. This establishes (67) and ﬁnishes the proof of
the lemma.

40

