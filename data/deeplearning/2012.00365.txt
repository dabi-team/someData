Python Workﬂows on HPC Systems

Dominik Strassel1,

, Philipp Reusch1,

and Janis Keuper2,1,

1CC-HPC, Fraunhofer ITWM, Kaiserslautern, Germany
2Institute for Machine Learning and Analytics, Offenburg University,Germany

0
2
0
2

c
e
D
1

]

G
L
.
s
c
[

1
v
5
6
3
0
0
.
2
1
0
2
:
v
i
X
r
a

Abstract—The recent successes and wide spread application of
compute intensive machine learning and data analytics methods
have been boosting the usage of the Python programming lan-
guage on HPC systems. While Python provides many advantages
for the users, it has not been designed with a focus on multi-
user environments or parallel programming - making it quite
challenging to maintain stable and secure Python workﬂows on a
HPC system. In this paper, we analyze the key problems induced
by the usage of Python on HPC clusters and sketch appropriate
workarounds for efﬁciently maintaining multi-user Python soft-
ware environments, securing and restricting resources of Python
jobs and containing Python processes, while focusing on Deep
Learning applications running on GPU clusters.

Index Terms—high performance computing, python, deep

learning, machine learning, data analytics

I. INTRODUCTION
In recent years, Python has become the most popular
programming language [1] and there are many reasons why
Python has evolved from a simple scripting language into the
dominating implementation environment: it is easy to learn,
platform independent, supports the full range of programming
paradigms (from functional to full object oriented), it allows
easy integration of external software modules (allowing perfor-
mance critical parts to be written in other languages) and for
most, it comes with an exhaustive ecosystem of available open
source software libraries. But Python also has a “dark” side.
Originally, it was not designed with a focus on multi-user,
shared resource systems or to be used for massive parallel
processing. While users and developers have come up with
many ways to work around these initial design choices over
the last years, some core properties of Python (and some of
the workarounds) are still prone to cause massive problems in
the maintenance and operation of HPC systems. In result, we,
as many others in the HPC community, have been experiencing
a wide range of small and severe issues related to Python
workﬂows on various HPC systems. Typical examples are:

• Python jobs tend to spin off a vast amount of child
processes, some are not terminating after a job ends. A

Electronic address: dominik.strassel@itwm.fraunhofer.de
Submitted and accepted at the PyHPC Workshop at SuperComputing 2020
and will be published at IEEE TCHPC Proceedings. The copyright is ﬁnally
transferred after publication.

©202X IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

phenomenon that regularly causes following GPU-jobs
to crash when these processes are still occupying GPU
memory.

• Python jobs appear to be “escaping” the resource con-
trol mechanisms of batch systems on a regular basis,
allocating more memory and CPU cores than scheduled
- hence, affecting other users on the system.

• The maintenance and management of the diverse and
fast evolving Python software environment
is quite
challenging, especially when the needs of a diverse user
group are contradicting.

This paper presents the intermediate results of our ongoing
investigation of causes and possible solutions to these prob-
lems in the context of machine learning applications on GPU-
clusters. In section II we address the necessity of restrictive
job termination in the context of Python based algorithms on
multi-user HPC systems. Followed by section III, where we
discuss in detail how to control and restrict ressources aquired
by those Python processes. In section IV we show some
possibilities to create and maintain Python environments.

II. SAFELY TERMINATING PYTHON PROCESSES

Correlating with the increasing use of Python workﬂows,
we1 observed a growing number of incidents, where different
batch systems on several HPC-Clusters were not able to
sufﬁciently terminate jobs running python scripts. While these
problems were quite hard to track and to isolate, they all shared
the common pattern, that a child process triggered by a python
script did not terminate with the associated job. Especially on
GPU-Clusters, where machine learning applications require
a lot GPU memory, these renegade processes cause severe
problems to following GPU jobs of other users.

In order to analyze the details of this problem (in sec-
tion II-C), we ﬁrst need to review the intrinsics of child
processes and their termination in Linux (section II-A) and
the dynamics of child process generation in Python (sec-
tion II-B).

A. Terminating Child Processes in Linux

The spawning of child processes is (along with multi-
threading) the standard parallelization technique provided by
the Linux kernel. Hence, one should assume, that the usage of
child processes is a safe and deterministic approach. However,

1And many forum posts suggest that this is a wide spread problem

 
 
 
 
 
 
has not PPID = 1). This indicates that the parent process is
stalled on a task and therefore cannot read the exit state of
the child process. On the other side, we have what is called
an “orphaned process”. This kind of process is very similar
to a zombie process. But in contrast to a zombie process,
an orphaned process is a process that is still running and
whose parent has died. By default, an orphaned process will
be adopted by the init process which results in a parent-pid
of PPID = 1, but is still owned by the user that has started
the parent process. Following this deﬁnition, every “daemon
process” is strictly speaking an orphaned process, where the
termination of the parent process is done on purpose and not
by error or a SIGKILL.

If we stop Algorithm 1 with either SIGTERM or SIGKILL
the child processes become orphaned processes and are not
killed. As these processes continue running, they continue
consuming resources. In this simple example this is not prob-
lematic, but if we consider more resource intensive scenarios,
e.g. a child that allocates GPU memory, we will run into
massive problems. Then, the GPU memory – one of the most
limited resources on an HPC cluster – will not be freed at
the end of a job because the child is not terminated. At this
point one would argue that we have a batch system on every
HPC cluster and every user program is started as a batch
job. Which is true. The problem is that orphaned processes
have PPID = 1 and are therefore no longer part of the batch
job. This results in the fact that the batch system cannot stop
such orphaned processes directly. Hence, Algorithm 1 is a
shockingly trivial example of a scenario where child process
are actually able to “escape” the batch system if no further
counter measures are in place.

B. Generation of child processes in Python

The problem of terminating child processes in batch system
environments is not speciﬁc to the execution Python scripts
on GPUs - in fact Algorithm 1 is a BASH script running
on CPU. However, there are several reasons why these prob-
lems correlate with Python machine learning applications
on GPU-Clusters: First, GPU memory is (compared to main
memory) a precious resource - orphans blocking a few GB
of memory will more likely cause problems to following
users. Second, users running machine learning applications
are often trying to “work around” the batch system, by start-
ing interactive environments like Jupyter2 [2]. Within these
environments, users then run scripts interactively and tend to
terminate them via CTRL-C, producing orphans “on the ﬂy”.
Last, there is the Python speciﬁc, extensive usage of child
processes, which makes it simply more likely to end up with
orphans compared to languages like C/C++, were most of the
intra-node parallelization is implemented via multi-threading.
The usage of multi-threading is the standard in-node par-
allelization mechanism used on HPC systems and compiled
high-level programming language – like Fortran, C and

2Of cause this can’t be done without the necessary network conﬁguration,
but we see more and more HPC sites allowing their users to use Jupyter [2]
within the batch system

Figure 1.

(color online) Example of child process spawning.

there are some pitfalls, which are often caused by the way
these processes are handled in the event that their parents
terminate.

To illustrate the termination mechanics, we use a BASH
script (Algorithm 1) that generates two child processes that
do nothing but sleep for 100 and 150 seconds, respectively.

#!/bin/bash
set -m
trap ’kill %%’ EXIT
sleep 100 &
sleep 150 &
wait
exit 0

Listing 1. Simple BASH script generating two child processes.

In order to stop this script we have different possibilities,
but in the end it always involves sending a signal to the process
telling it to end. In the following we focus on only two signals
– SIGTERM and SIGKILL. SIGTERM is the default signal
sent when invoking kill, e.g. used by top or htop. It tells the
process to shut-down gracefully. This is the signal that should
be used if we want that the process to shut down cleanly.
The problem is, that technically SIGTERM can be ignored by
a process, as it is only informed to shut-down and close all
open ﬁles, databases and so on. But this termination signal is
handled inside the process. In most situations it is considered
as “bad practice” not to terminate after getting a SIGTERM
signal. In contrast, SIGKILL cannot be ignored by a process
as the termination is handled outside of the process. SIGKILL
is useful when an application does not respond any more
or will not terminate with SIGTERM. In general SIGKILL
should be able to stop every process, but there are exceptions.
The most prominent example for such an exception are the
so-called “zombie processes”.

A zombie process is a process that has ﬁnished normally
and is waiting for its parent to read its exit state. The crucial
point is, that the child process has ﬁnished its execution, or
in other words it is “dead”. But the entry in the process table
still exists. The most likely situation for a zombie process
to appear is when the parent job has died (e.g. segfault or
the process got a SIGKILL). Sometimes it is also possible
that the parent process is still active (and the parent process

parent processchildprocesschildprocesschildprocesschildprocesschildprocesschildprocessC++. Due to basic design choices within Python, multi-
threading does not work the same way as in these languages.
The standard Python implementations3 have a global lock on
the interpreter [3]. This lock “...must be held by the current
thread before it can safely access Python objects.” [4] This
remains true, even if we use libraries like threading. Even
though we can create multiple threads with this library, the
execution is still limited via the global interpreter lock (GIL).
Such a behavior is totally different to the way most program-
mers would understand the concept of “threading”. In order
to get around the limitation of the GIL, Python developers
started neglecting parallelization by multi-threading and turned
to multi process solutions instead. In this approach, jobs spawn
several child processes (see Fig. 1), e.g. with Pool [5], in
order to achieve true parallel execution of Python code.
Note that this does not violate the GIL as “...[t]he multi-
processing package offers both local and remote concurrency,
effectively side-stepping the Global Interpreter Lock by using
subprocesses instead of threads...” [5]. In combination with
the tendency to use many third party libraries, which are
spawning their own children, this leads to the notable effect,
that many python applications end up with dozens or more
child processes.

C. Jailing Processes

In order to overcome the previously described behavior,
one has to somehow “jail” everything that runs inside the
batch jobs. On Unix-like systems one can make use of the
seccomp [6] kernel feature, process groups [7] (e.g.
setpgid, setpgrp) or PID namespaces [8]. This could
be done by adjusting the batch system either via integrating
the usage of seccomp and PID namespaces in the batch
system or the less preferable variant – forcing users to im-
plement those features in their programs. Besides the ability
to suppress escaping a batch job with the help of seccomp,
one has the opportunity to limit the folders that are visible or
accessible within the batch job. Another quickly implemented
possibility is to use already existing tools like firejail [9]
or newpid [10]. No matter which solution is going to be
the preferred one, it will be unavoidable to implement such a
feature in batch systems. Not only because of the fact that we
want to avoid orphaned processes, but also with regard to the
coming integration of cloud solutions in HPC clusters in the
near future.

III. CONTROLLING THE RESOURCES OF PYTHON
PROCESSES
The second problem we empirically observed on various
HPC clusters, is the lack of resource control. Using batch-
systems, one would expect that (Python) processes would
not be able to exceed predeﬁned memory and CPU resources.
However, in the next sections we discuss a number of direct
and indirect effects caused by Python workﬂows, that can
bypass resource limits at the expense of other users on the
system.

3e.g. CPython

Command
ulimit -m 1
ulimit -m
1048576
ulimit -m
5242880
ulimit -m
20971520
ulimit -m
41943040

Limit Size

1 KB
1 GB

5 GB

20 GB

40 GB

5GB allocated
yes
yes

yes

yes

yes

Table I
TRYING TO ALLOCATE 5GB OF MAIN MEMORY WITH VARIOUS MEMORY
RESTRICTIONS USING U L I M I T -M.

A. Memory Limits of Batch-Jobs

The ﬁrst observation is only indirectly caused by Python
and is more likely the result of the high dynamics in the
“Python-GPU-Machine-Learning” environment. The appli-
cation driven need to run on the latest GPU drivers (also see
section IV), forcing HPC system maintainers to use much
newer kernel versions than the typically very conservative
system management would allow. If not all system compo-
nents, especially the batch-system, are up to date with newer
kernels, we have seen heavy side effects. Most notably the total
breakdown of memory limits in batch-systems. The problem
is, that the usage of “ulimit -m” by older versions of batch-
systems, which is supposed to limit the max memory size, has
no effects since Linux kernel version 2.4! [11]. We performed
a little test with a simple program to illustrate the effects, see
Table I.

In today’s Unix-like systems with activated control
groups [12], this is not a problem as long as the control groups
are activated in the batch system as well. But there are some
restrictions to this statement. On some Debian derivatives the
memory cgroups are excluded and have to be added explicitly
to the kernel command line. [13] The same yields true for
the conﬁguration of batch systems, where the default setting
in many cases is still without control groups. It is even the
case that some batch systems reimplement the usability of
“ulimit -m” in such a way, that they check (e.g.) every
30 seconds the used memory of a batch job. [14] This makes
it, with some limitations, possible to kill programs that use
more memory than the batch job has allocated. But relying on
those kinds of reimplementation is dangerous, as these checks
rely on a given time interval, in the example 30 seconds. Note
that if the process hierarchy in a batch job is too deep this
reimplementation does not work either. Combining both makes
it possible to kill an entire compute node within less than a
few seconds with a simple “tail /dev/zero”!

Besides control groups, there is another ulimit that can be
used to limit the memory usage of program – “ulimit -d”.
With this one can limit “...[t]he maximum size of the process’s
data segment (initialized data, uninitialized data, and heap).
This limit affects calls to brk(2) and sbrk(2), which fail with

Command
ulimit -d 1
ulimit -d
1048576
ulimit -d
5242880
ulimit -d
20971520
ulimit -d
41943040

Limit Size

1 KB
1 GB

5 GB

20 GB

40 GB

5GB allocated
no
no

no

yes

yes

Table II
TRYING TO ALLOCATE 5GB OF MAIN MEMORY WITH VARIOUS MEMORY
RESTRICTIONS USING U L I M I T -D.

the error ENOMEM upon encountering the soft limit of this
resource...” [11] To check this we tested this in the same way
like before, see Table II.

And indeed this limit still works. Nevertheless using control
groups has much more beneﬁts and has the possibility to act
on a different level but “ulimit -d” can still be a handy
command to know.

B. Pitfalls of Python Memory Management

Once we are able to fully contain the memory usage of
Python jobs, users start
to experience yet another, very
Python speciﬁc problem:
the odds of Python memory
takes all
management. Being an interpreted language that
direct memory interaction away from the programmer, the
Python memory management makes it hard to estimate the
actual memory demands of an algorithm. On top of that,
memory consumption often appears to be non deterministic,
in the sense that small changes in the code can cause large
changes in memory usage, which leaves users with unexpected
out of memory job terminations.

Python has been designed to abstract various levels of
control from the users, including memory management. This
enables users with less knowledge in software development
to write custom applications, but bears a challenge, if more
control is required. While it is theoretically possible to predict
the memory usage under certain conditions, it requires deep
knowledge of the Python memory management and the used
interpreter.

To abstract

from the user,
the memory management
Python creates a private heap for Python objects [15]
and the Python data model deﬁnes, that “[a]ll data [...] is
represented by objects or by relations between objects” [16].
This results in wrapping every value in an object, before
storing it in the memory. The overhead of the wrapper can
be shown by running the Python code from Algorithm 2.

# Python 3.6.4, manjaro 18.0.4 x64 kernel

4.19.66
>>> import sys
>>> print("integer:", sys.getsizeof

(2147483647))

integer: 32

Listing 2. Print the object size of an integer literal

call

The

“sys.getsizeof()”

in Algorithm 2
“[r]eturn[s] the size of an object
in bytes” [17]. The
call argument is an integer literal representing the number
2147483647 and the reported object size is 32 bytes. In
the number 2147483647 is
an comparable C application,
the maximum positive value of the “signed integer”
type, which takes up four bytes. In comparison, the Python
object requires eight times the space of the C equivalent.
This overhead is introduced by the metadata required for
each object. The exact amount of overhead depends on the
stored value – e.g. a small integer like 127 requires 28 bytes.
In result, the memory complexity depends on the concrete
values of the data. Especially in research with huge amounts
of heterogeneous data, this can be hard to predict.

Even after determining the maximum size of an object
in bytes, the creation of an object is not directly related to
memory allocations. Python internally handles the allocation
of memory depending on the size of an object. In CPython
by default objects larger than 512 bytes are directly allocated
on the application heap, while smaller objects will be stored in
pre-allocated arenas “[...] with a ﬁxed size of 256 KB” [15].
This aggravates the prediction of allocations, as it depends on
the size of the object and the current utilization of the arenas.
Similar behavior applies to the deallocation of memory.
Non-aggregate objects are freed, when the reference counter
for the object is zero. Aggregate data types like dicts, lists or
tuples, are watched by the garbage collection (gc) module.
The gc stores objects in multiple generations, depending on
the age of an object. A generation is a collection of objects
with the same age. The age of an object
is the number
of gc rounds without being freed. Each generation has a
counter for the number of allocations minus the number of
deallocations since the last gc round. Based on this counter
a per-generation threshold can be deﬁned. When the counter
exceeds the threshold, a new gc round is initiated. In a gc
round, all unused objects of the generation with exceeded
threshold are freed and the remaining objects are moved to the
next-older generation. [18] The default values in CPython are
three generations with thresholds g0 = 700, g1 = 10, g2 = 10
with g0 as youngest generation. Even knowing the parameters,
it is really hard to predict the exact time of deallocations,
because they highly depend on the object handling of the
application.

The overhead introduced by the object pattern also impedes
the compute performance. In research with huge amounts of
data, it is common to use third-party libraries like numpy
or scipy to overcome these performance issues. A major
reason for the better performance, is an improved memory
management – e.g. for arrays in numpy. By storing the data
in a custom structure, the overhead of the object pattern can be
reduced. The custom structure further allows to provide opti-
mized algorithms, which can achieve remarkable performance
gains. The downside of the use of third-party libraries is the

introduction of unknown memory operations, which requires
additional knowledge of the memory management of the used
library.

Apart from the mentioned arguments, the ﬁnal memory
management is done by the Python interpreter executing the
script in a speciﬁc hardware/software environment. Even with
the prediction for a speciﬁc setup, changing a single part may
result in a highly different memory pattern. On HPC-clusters,
this leads to a situation in which the cluster requires hard
limits, but the users do not have the tools to write Python
applications, which guarantee to respect these limits. As a ﬁx
it might be considered to provide a test queue with short wall
time and high priority. This enables users to trial and error their
Python programs to achieve speciﬁc utilization goals, before
submitting them in the intended queue. Tools like “ulimit”
or “cgroups” can be used to provide the required memory
limitation to avoid out-of-memory crashes of compute nodes.

C. Limiting CPU Usage

Traditional HPC workﬂows use mostly multi-threading for
in-node parallelization. The threading APIs of compiled high-
level programming languages like C/C++ or Fortran, the
libraries written in these languages and HPC batch systems
support this standard approach by honoring given thread limits.
These limits are usually set by the batch-script at the start of
a job, setting the limits via a few environment variables like
OMP_NUM_THREADS, which sets the number of threads to
be used by openMP, or MKL_NUM_THREADS, which sets the
same for calls to the Intel Math Kernel Library, just
to name a few examples. Unfortunately, Python jobs will
often not respond to these mechanism: while pure Python
scripts, that are using child process for parallelisation (see
section II-B), will simply ignore thread limits and occupy
as many cores as they can get by spawning process after
process, some Python scripts which are interfacing external
libraries, e.g. such as NumPy [21], might follow the limits to
some extent. In the following, we will analyze the threading
behavior of three very popular libraries – Tensorflow [19],
PyTorch [20] and NumPy – which we observed to be
commonly used in Python jobs on HPC-systems.

the appearance of threads we have
In order to control
like, MKL_THREADING_LAYER,
environment
variables
MKL_NUM_THREADS,
NUMEXPR_NUM_THREADS,
or
OMP_NUM_THREADS. Normally we can use those variables
in a BASH script – Algorithm 3 – and we are able to limit
the number of threads.

#!/bin/bash
export MKL_THREADING_LAYER=SEQUENTIAL
export MKL_NUM_THREADS=α
export NUMEXPR_NUM_THREADS=β
export OMP_NUM_THREADS=γ
COMMAND-TO-EXECUTE
exit 0

Listing 3. Limiting the number of threads

Whereas α, β, γ are simple numbers larger than zero.
We did in total 576 (3 × 192) simulations with α, β, γ =

Framework Topology Dataset

Comment

–

–

NumPy

mandelbrot
calculation
with x ∈ [−2.4, 1.2],
y
[−1.2, 1.2],
resolution 6400 × 4800,
256 iterations
Tensorﬂow ResNet50 ImageNet classiﬁcation

∈

PyTorch

ResNet50 Voc2012

task
(18000 training steps)
segmentation
task
(1500 training steps)

Table III
SIMULATION SETUP TO TEST ALGORITHM 3.

(color online) Number of

Figure 2.
function of
OMP_NUM_THREADS for the TensorFlow (top), PyTorch (middle) and NumPy
(bottom) simulations as deﬁned in Table III. The remaining environment
variables as deﬁned in Algorithm 3 are encoded via colour or symbol type
respectively.

threads

as

a

{1, 2, 8, 16} and 20 CPU cores. The node was used exclusively
for the simulations. For the tests we used three different
algorithms summarized in Table III.

The simulation using NumPy acts as reference simulation
that does not
involve any GPU speciﬁc libraries. For the
simulations using PyTorch and TensorFlow we adjusted
the number training steps in such a way that the simulations
without any limitation have a runtime of roughly 30 minutes,
logging various information during the simulations. In the
following we focus on the number of threads (Fig. 2), CPU
usage, GPU usage and runtime (Fig. 3).
purpose

like
variables
OMP_NUM_THREADS is to control
the number of threads
(NoT) a program uses, therefore this is the ﬁrst variable to
have a closer look at, Fig. 2.
Without any restrictions,

the NoT for TensorFlow is
NoT = 80, for PyTorch NoT = 9 and for NumPy NoT = 1.

environment

The main

of

For all combinations of the environment variables deﬁned in
Algorithm 3, we do not see any change in NoT for NumPy
and PyTorch. Whereas for TensorFlow the NoT varies
between 57 and 72. Regarding NumPy and PyTorch, both
do not use libraries that are sensitive to any of the used
environment variables, meaning the appearance of the threads
is not related to a class or function deﬁned in libc. In the
case of TensorFlow, at least some of the threads can be
controlled but this number is marginal compared to the NoT
that cannot be controlled.

This result raises the question if the environment variables
deﬁned in Algorithm 3 have any effect on the performance at
all? As shown in Fig. 3, there is no difference within errors
in CPU usage or GPU usage and runtime.

There are situations in which we can use environment vari-
ables to control the number of threads even within Python
scripts, as long as the appropriate libraries are used. For
the most prominent DL frameworks – TensorFlow and
PyTorch – this is not the case. This means one has to ﬁnd
other ways to control Python scripts or scripting languages
in general. In the next section we have a closer look at one
possibility which is taskset.

As it is the case for control groups we can use taskset
to control
to pin processes to speciﬁc cores. In contrast
groups, every user can use taskset. We used the same
PyTorch and TensorFlow setup as deﬁned in Table III
and omit the NumPy simulations. In detail we pinned the
simulations to 1, 4, 8 and 20 CPU cores without any further
limitations and in addition did the same again with setting
OMP_NUM_THREADS=1. We do this extra restriction in order
to detect or exclude the possibility of interactions between the
pinning via taskset and environment variables.

it

First of all,

interesting to see that

Before we go into details regarding the explicit pinning
of the processes, we have a look at the CPU that the main
process of the simulations uses as a function of time without
any restrictions, Fig. 4.
is

for both
TensorFlow and PyTorch the used CPU cores of the main
process changes over time. In the case of TensorFlow,
the used CPU core stays sometimes for less that 5 seconds
the same. For PyTorch in contrast there are longer periods
with the same CPU core. If we use taskset ,and pin the
simulation to speciﬁc cores, the main process can only switch
been cores it is pinned to.

A similar behavior can be found for the NoT, Fig. 5.

Regarding the details we have to be a bit restrictive.

In the case of PyTorch (blue symbols in Fig. 5) the NoT
is not effected by the number of available cores as it was in
Fig. 2. For TensorFlow we see that the NoT is effected by
the number of available cores. Nevertheless the lowest NoT
– during the training – that can be reached is approximately
19. In addition we see no difference between no restrictions
and OMP_NUM_THREADS=1 for 1, 4 and 8 cores. Whereas
for 20 cores there is a difference in the two settings. In order
to understand it is necessary to have a detailed look at the
source code implementation of TensorFlow. Note that this

Figure 3.
(color online) CPU utilization (top), GPU utilization (middle),
runtime (bottom) as a function of OMP_NUM_THREADS for the TensorFlow
(orange), PyTorch (blue) and NumPy (green) simulations as deﬁned in
Table III.

is not the perspective of this paper and we would leave this
open for future ongoing research.

From the view of a user it is perhaps even more interesting
to know if limiting the number of cores has a signiﬁcant effect
on the performance or not. Even though most DL users focus
on the GPU usage – Fig. 6 (middle) – the CPU usage is equally

Figure 4. (color online) CPU ID of the main process as a function of runtime
of the TensorFlow (orange) and PyTorch (blue) simulations as deﬁned in
Table III without any restrictions. (Note that the group like splitting is because
of the fact that 4 CPU cores are blocked for the operating system of the
compute node.)

Figure 5.
(color online) NoT as a function the available CPU cores of
the TensorFlow (orange) and PyTorch (blue) simulations as deﬁned in
Table III.

(color online) CPU usage (top), GPU usage (middle) and runtime
Figure 6.
(bottom) as a function of the available CPU cores of the TensorFlow
(orange) and PyTorch (blue) simulations as deﬁned in Table III.

important, Fig. 6 (top).

For PyTorch it makes no difference if we have 1 or
20 cores available the CPU usage stays at 100%. Note that
we talk about a straight forward implementation with default
settings and without explicit parallelization. Like before, the
TensorFlow implementation is more sensitive. For one
available CPU core, the CPU usage stays at 100% which
proves that taskset works as supposed. With increasing
number of cores the CPU usages increases as well but
seems to saturate at roughly 300%. With GPUs present the
CPU does not focus on heavy calculations but has the main
focus on preprocessing and similar tasks. This raises the
question if the GPU is effected by the CPU usage and in
is the case
deed it

is, Fig. 6 (middle). To be precise,

it

for TensorFlow, for PyTorch we see no effect at all.
Regarding TensorFlow, we ﬁnd a nearly linear correlation
between GPU and CPU usage. If we have only one CPU
core the GPU usage drops to 63% in average. With increasing
number of cores the usage grows and reaches a saturation at
approximately 90%.

As both CPU and GPU usage show partially strong differ-
ences depending on the number of available CPU cores we
ﬁnd that the runtime is affected as well, see Fig. 6 (bottom).
Regarding the interpretation of the runtime, we have to be a
bit more careful. In the case of PyTorch there is no effect as
we see no effect in CPU or GPU usage as well. The runtime
for the TensorFlow simulations resembles the results of the
CPU and GPU usage. As the entire preprocessing is done on

installation for every user has about 5GB mostly redundant
data. We say “redundant” as there is a huge amount of libraries
and binary ﬁles that many users need or is installed by ana-
conda on default. Despite the drawbacks there are advantages
as well, e.g. multiple python environments (Python 2 and
Python 3 simultaneously) and faster updates than one has
in most Linux repositories (which is most of the time needed
by ML/DL users).

B. Maintaining Python Software Stacks

In order to make it possible for users to get the advantages
and have a better control over what is installed, we can make
use of different approaches. For the redundancy part there is
the possibility to install Anaconda in such a way that one gets
multi-user support. In case of security concerns, one can create
a local Anaconda repository – or in “Anaconda language”
creating custom channel. With such a local repository system
administrators are able to control the packages that are allowed
on the system. Note that this follows the idea of a local clone
of a Linux repository – a well-known “trick” on HPC systems.
Besides the already mentioned difﬁculties with Python
libraries,
there are more restrictions, e.g.
different program versions for different users, more diverse
security issues or regimentation in connection with the Gen-
eral Data Protection Regulation [24]. One way to realize
such requirements is the usage of virtualization tools, like
virtual machines or software containers4. Depending on the
particular use case and the cluster environment settings there
are solutions that ﬁt better. For example if we already have
an existing batch system, e.g. Slurm [28] or LSF [29], the
preferred container solution has to make it possible to start
containers as normal users.

is likely that

it

C. Challenges with GPU Drivers

The perhaps most important aspect regarding DL is the
native and easy GPU implementation and usage. The tricky
part with the implementation is twofold. The GPU has to
be available inside the container. This visibility goes hand
in hand with the respective driver of the installed GPU. At
the moment we have the situation for container solutions that
the GPU driver has to be installed on the host system as
it can otherwise not be available inside the container. This
meant for a long time that inside the container was only
one speciﬁc CUDA and therefore TensorFlow or PyTorch
version possible. As a consequence one would have to boot
the host system with another GPU driver to have another
CUDA version inside the Container. This is not feasible on
a productive cluster, as it would have a massive impact on
both the workload and other users. Starting with CUDA version
10, it is now possible to update both CUDA and the GPU
driver without touching the respective kernel modules. [30]
Therefore the remaining “limiting part” is the GPU driver.

4Note that the goal of this paper is not to distinguish between container
software like Singularity [25] or Docker [26] or giving an overview of
available container solutions. Regarding this we would like to recommend
Ref. [27].

(color online) Main Memory as a function of the available CPU
Figure 7.
cores of the TensorFlow (orange) and PyTorch (blue) simulations as
deﬁned in Table III.

the CPU the less cores are available the harder it is too feed
the GPU in a proper manor. This results in longer runtimes for
less cores. Besides this it is not the case that the simulation
time would continue to decrease with even more cores.

Although we ﬁnd effects in the case of CPU or GPU
usage, the used main memory is, for both PyTorch and
TensorFlow, nearly not effected at all, see Fig. 7.

IV. MANAGEMENT OF PYTHON ENVIRONMENTS

In order to provide a useful and manageable Python envi-
ronment on HPC systems one has to rethink the common way
software and libraries are installed. Essentially there are two
main problems – extremely fast changing software releases
and dependency issues. The latter problem has rather diverse
reasons. First of all we want to make the clear statement that
Python 2 and Python 3 are two independent languages.
Because of this it is not possible to execute scripts that contain
Python 2 code with a Python 3 interpreter and vice versa.
For ML, DL and DS it is standard that different methods or
models have totally different dependencies which makes it
impossible installing them at the same time on all compute
nodes of a cluster. As a consequence we have to ﬁnd another
approach.

A. Installing Python Software Stacks

As most of the ML/DL community uses Python as main
the usage of Anaconda Distribu-
programming language,
tion [22] and/or pip [23] to install needed packages and
libraries is widespread. These make it possible for users to
install essentially everything in their respective home directo-
ries. On the one side, this may be an advantage for the users
as they can easily install things they need without explicitly
asking for example system administrators. On the other side,
this has two major drawbacks. First,
it makes it hard to
control what users install, which can end up with serious
security issues. Depending on the provider of the respective
cluster – university, research institute or industry – the legal
consequences can be tremendous. Second, a local Anaconda

[22] https://www.anaconda.com
[23] https://pip.pypa.io/en/stable
[24] Regulation (EU) 2016/679 of the European Parliament and of the
Council of 27 April 2016 on the protection of natural persons with
regard to the processing of personal data and on the free movement of
such data, and repealing Directive 95/46/EC

[25] https://www.sylabs.io/singularity
[26] www.docker.com
[27] CNCF Cloud Native Interactive Landscape, https://landscape.cncf.io
[28] https://slurm.schedmd.com
[29] https://www.ibm.com/support/knowledgecenter/en/SSETD4/product_

welcome_platform_lsf.html

[30] https://docs.nvidia.com/deploy/cuda-compatibility

But this simpliﬁes the situation a lot as we can now simply
bind mount the libraries provided by the GPU driver in the
container and install the different CUDA, TensorFlow and
PyTorch versions directly in the container. This gives the
user full control over which framework and version he wants
to use during his training. Depending on the cluster, the used
Linux distribution and the update policies on the cluster it
might take some time until this feature will be available to
all users. But the needed tools and software are available and
it opens the door to the HPC world for the entire machine
and deep learning community without making HPC clusters
“useless” for other researchers.

V. CONCLUSION

The rise of compute intensive machine learning and data
analytics applications has a big impact on the design and
maintenance of HPC-Systems: the increased usage of GPUs on
the hardware side, and the application of Python workﬂows
on the software side. In combination with new user groups,
which have little HPC background, these factors can have
a massive impact on the operation of a HPC-cluster. The
problems that we have discussed throughout this paper mostly
originate directly or indirectly from the use of Python
environments. As we have shown, most of these problems are
solvable, but need some attention until HPC batch-systems and
other parts of the HPC software stack have evolved to handle
them automatically.

The most promising solution towards a user friendly, secure
and efﬁcient adaption of Python workﬂows on HPC systems
appears the usage of containers. This aspect has not been part
of the presented investigations, but we are planing to extend
our work in this direction.

REFERENCES

[1] https://stackoverﬂow.blog/2017/09/06/incredible-growth-python
[2] Kluyver, Thomas, et al. "Jupyter Notebooks-a publishing format for

reproducible computational workﬂows.", ELPUB, (2016)

[3] https://wiki.python.org/moin/GlobalInterpreterLock
[4] https://docs.python.org/3/c-api/init.html#

thread-state-and-the-global-interpreter-lock

[5] https://docs.python.org/dev/library/multiprocessing.html
[6] http://man7.org/linux/man-pages/man2/seccomp.2.html
[7] http://man7.org/linux/man-pages/man2/setpgid.2.html
[8] http://man7.org/linux/man-pages/man7/pid_namespaces.7.html
[9] https://ﬁrejail.wordpress.com
[10] https://github.com/df7cb/newpid
[11] https://linux.die.net/man/2/setrlimit
[12] https://www.kernel.org/doc/Documentation/cgroup-v2.txt
[13] https://slurm.schedmd.com/cgroups.html
[14] https://slurm.schedmd.com/slurm.conf.html
[15] https://docs.Python.org/3.6/c-api/memory.html
[16] https://docs.Python.org/3.6/reference/datamodel.html
[17] https://docs.Python.org/3.6/library/sys.html
[18] https://docs.Python.org/3.6/library/gc.html
[19] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,
S. Moore, D.G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,
M. Wicke, Y. Yu and X. Zheng, “TensorFlow: A system for large-scale
machine learning”, OSDI 16, 265–283, (2016)

[20] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z.
Lin, A. Desmaison, L. Antiga, A. Lerer, “Automatic differentiation in
PyTorch”, NIPS, (2017)

[21] https://numpy.org

