8
1
0
2
c
e
D
5

]
E
S
.
s
c
[

1
v
7
5
2
2
0
.
2
1
8
1
:
v
i
X
r
a

On Testing Machine Learning Programs

Houssem Ben Braieka, Foutse Khomha

aSWAT Lab., Polytechnique Montr´eal, Canada

Abstract

Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many safety-critical systems, thanks
to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems
based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google
Home. As the ﬁeld of ML continues to grow, we are likely to witness transformative advances in a wide range of areas,
from ﬁnance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily
live, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting
concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help
ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML
programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report
existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to
the testing of ML programs and make recommendations of future research directions for the scientiﬁc community. We
hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to
improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed
research directions to advance the state of the art of testing for ML programs.

Keywords: Machine Learning, Data Cleaning, Feature Engineering Testing, Model Testing, Implementation Testing

1. Introduction

Machine learning (ML) is increasingly deployed in large-
scale and critical systems thanks to recent breakthroughs
in deep learning and reinforcement learning. We are now
using software applications powered by ML in critical as-
pects of our daily lives; from ﬁnance, energy, to health
and transportation. However, detecting and correcting
faults in ML programs is still very challenging as evi-
denced by the recent Uber’s car incident that resulted in
the death of a pedestrian1. The main reason behind the
diﬃculty to test ML programs is the shift in the devel-
opment paradigm induced by ML and AI. Traditionally,
software systems are constructed deductively, by writing
down the rules that govern the behavior of the system as
program code. However, with ML, these rules are inferred
from training data (i.e.,, they are generated inductively).
This paradigm shift in application development makes it
diﬃcult to reason about the behavior of software systems
with ML components, resulting in systems that are intrin-
sically challenging to test and verify, given that they do not
have (complete) speciﬁcations or even source code corre-
sponding to some of their critical behaviors. In fact some
ML programs rely on proprietary third-party libraries like

Email addresses: houssem.ben-braiek@polymtl.ca (Houssem

Ben Braiek), foutse.khomh@polymtl.ca (Foutse Khomh)

1https://www.theguardian.com/technology/2018/may/08/

Intel Math Kernel Library for many critical operations.
A defect in a ML program may come from its training
data, program code, execution environment, or third-party
frameworks. Compared with traditional software, the di-
mension and potential testing space of a ML programs
is much more larger as shown in Figure 1. Current ex-
isting software development techniques must be revisited
and adapted to this new reality.

In this paper, we survey existing testing practices that
have been proposed for ML programs, explaining the con-
text in which they can be applied and their expected out-
come. We also, identify gaps in the literature related to
the testing of ML programs and suggest future research
directions for the scientiﬁc community. This paper makes
the following contributions:

• We present and explain challenges related to the test-
ing of ML programs that use diﬀerentiable models.

• We provide a comprehensive review of current soft-

ware testing practices for ML programs.

• We identify gaps in the literature related to the test-
ing of ML programs and provide future research di-
rections for the scientiﬁc community.

To the best of our knowledge, this is the most compre-
hensive study of testing practices for ML programs. The
rest of the paper is organized as follows. Section 2

ubers-self-driving-car-saw-the-pedestrian-but-didnt-swerve-report

Preprint submitted to the Journal of Systems and Software

December 7, 2018

 
 
 
 
 
 
Figure 1: Testing Space of ML programs

provides background knowledge on the ML programs. Sec-
tion 3 presents research trends in ML application testing.
Finally, Section 4 concludes the paper.

2. Background on Machine Learning Model

In this section, we provide background information about

Machine Learning (ML) programs and explain challenges
that should be addressed when testing ML programs.
The ﬁrst step when constructing a ML component is to
collect data from which concepts and hidden patterns can
be learned using some algorithms. Most machine learning
algorithms require huge volume of data to be able to con-
verge and make meaningful inferences, which makes data
collection a challenging step. Once data is collected, often
it has to be pre-processed before it can be used for learning.
Failing to complete this pre-processing properly is likely
to result in noisy data which can signiﬁcantly aﬀect the
quality of trained models. After this pre-processing step,
important features are identiﬁed from the data. These
features also often need to be processed before they can
be used in a learning algorithm. Inadequate feature en-
gineering (i.e., failure to process the features adequately)
is also likely to result in poor models. Once, the data is
cleaned, and features are extracted properly, a learning al-
gorithm is used to infer relations capturing hidden patterns
in the data. During this learning process, the parameters
of the algorithm are tuned to ﬁt the input data through
an iterative process, during which the performance of the
model is assessed continuously. A poor choice of parameter
or an ineﬀective model testing mechanism will also result
in a poor model. After training and testing steps, the
model is deployed in a production environment which can
be diﬀerent from the training and testing environments.
In production, the model often has to interact with the
other components of the application. There are two main
source of faults in ML programs : the data and the model.
For each of these two dimensions (i.e., data and model),
there can be errors both at the conceptual and implemen-
tation levels, which makes fault location very challenging.
Approaches that rely on tweaking variables and watching

signals from execution are generally ineﬀective because of
the exponential increase of the number of potential faults
locations. In fact, if there are n potential faults related to
data, and m potential faults related to the model, we have
n × m total possible faults locations when data is feed in
the model. This number grows further when the model
is implemented as code, since more types of faults can be
introduced in the program at that stage as illustrated on
Figure 1. Systematic and eﬃcient testing practices are
necessary to help ML engineers detect and correct faults
in ML programs.
In the following we present potential
errors that can occur in data and models, both at design
and implementation levels.

2.1. Data Engineering: Challenges and Issues

Data is an essential artifact when building ML models.
It often comes from a variety of sources e.g., mainframe
databases, sensors, IOT devices, and software systems,
and is presented in diﬀerent formats (e.g., various media
types). It can be structured (such as database records) or
unstructured (such as raw texts) and is delivered to ML
models either in batch (e.g., discrete chunks from main-
frame databases and ﬁle systems) and/or real-time (e.g.,
continuous ﬂow from IOT devices or Stream REST API).
ML engineers generally have to leverage complementary
automated tools that support batch and/or real-time data
ingestion strategies, to collect data needed for training ML
models.

Conceptual issues. Once data is gathered, cleaning data
tasks are required to ensure that the data is consistent, free
from redundancy and given a reliable starting point for
statistical learning. Common cleaning tasks include: (1)
removing invalid or undeﬁned values (i.e., Not-a-Number,
Not-Available), duplicate rows, and outliers that seems to
be too diﬀerent from the mean value); and (2) unifying the
variables’ representations to avoid multiple data formats
and mixed numerical scales. This can be done by data
transformations such as normalization, min-max scaling,
and data format conversion. This pre-processing step al-
lows to ensure a high quality of raw data, which is very

2

important because decisions made on the basis of noisy
data could be wrong.
In fact, recent sophisticated ML
models endowed by a high learning capacity are highly
sensitive to noisy data [1]. This brittleness makes model
training unreliable in the presence of noisy data, which
often results in poor prediction performances [2].

Once data is cleaned, pertinent features that describe
the structures inherent in the data entities are selected
from a large set of initial variables in order to eliminate re-
dundant or irrelevant ones. This selection is based on sta-
tistical techniques such as correlation measures and vari-
ance analysis. Afterwards, these features are encoded to
particular data structures to allow feeding them to a cho-
sen ML model that can handle the features and recog-
nize their hidden related patterns in an eﬀective way. The
identiﬁed patterns represent the core logic of the model.
Changes in data (i.e., the input signals) are likely to have a
direct impact on these patterns and hence on the behavior
of the model and its corresponding predictions. Because
of this strong dependence on data, ML models are consid-
ered to be data-sensitive or data-dependent algorithms. A
poor selection of features can impact a ML system nega-
tively. Sculley et al. [3] report that unnecessary dependen-
cies to features that contribute with little or no value to
the model quality can generate vulnerabilities and noises
in a ML system. Examples of such features are : Ep-
silon Feature, which are features that have no signiﬁcant
contribution to the performance of the model, Legacy Fea-
ture, which are features that lost their added information
value on model accuracy improvement when other more
rich features are included in the model, or Bundled Fea-
tures, which are groups of features that are integrated to
a ML system simultaneously without a proper testing of
the contribution of each individual feature.

Implementation issues. To process data as described
above, ML engineers implement data pipelines containing
components for data transformations, validation, enrich-
ment, summarization, and–or any other necessary treat-
ment. Each pipeline component is separated from the
others, and takes in a deﬁned input, and returns a de-
ﬁned output that will be served as input data to the next
component in the pipeline. Data pipelines are very use-
ful in the training phase as they help process huge vol-
ume of data. They also transform raw data into sets of
features ready-to-be consumed by the models. Like any
other software component, data pipelines are not immu-
nized to faults. There can be errors in the code written to
implement a data pipeline. Sculley et al.[4] identiﬁed two
common problems aﬀecting data pipelines:

• Pipelines jungles which are overly convoluted and
unstructured data preparation pipelines. This ap-
pears when data is transformed from multiple sources
at various points through scraping, table joins and
other methods without a clear, holistic view of what
is going on. Such implementation is prone to faults

since developers lacking a good understanding of the
code are likely to make mistakes. Also, debugging
errors in such code is challenging.

• Dead experimental code paths which happens when
code is written for rapid prototyping to gain quick
turnaround times by performing additional experi-
ments simply by tweaks and experimental code paths
within the main production code. The remnants of
alternative methods that have not been pruned from
the code base could be executed in certain real world
situations and create unexpected results.

For systems that rely on mini-batch stochastic optimiz-
ers to estimate the model’s parameters, like deep learning
models, another data-related component is required in ad-
dition to data pipelines, i.e., the Data Loader. This com-
ponent is responsible of the generation of the batches that
are used to assess the quality of samples, during the train-
ing phase. It is also prone to errors.

2.2. Model Engineering: Challenges and Issues

Once ML engineers have collected and processed the
data, they proceed to ﬁnding the appropriate statistical
learning model that could ﬁt the available data in order
to build its own logic and solve the given problem. A
wide range of statistical models can be acquired and–or
extended to suit diﬀerent classiﬁcation and regression pur-
poses. There are simple models that make initial assump-
tions about hidden relationships in the data. For exam-
ple, the linear regression assumes that the output can be
speciﬁed as a linear combination of features and the SVM
classiﬁer assumes that there is an hyperplane with maxi-
mum margin that can optimally separate the two classes of
the output. Besides, there are more complex models such
as neural network models, which usually do not make as-
sumptions about the relationship between incoming pairs
of data. In fact, Neural Network is structured in terms of
interconnected layers of computation units, much like the
neurons and their connectivity in the brain. Each neuron
includes an activation function (i.e., a non-linear transfor-
mation) to a weighted sum of input values with bias. The
predicted output is calculated by computing the outputs of
each neuron through the network layers in a feed-forward
manner. At the end, Neural Network’s mapping function
can be seen as a composite function encoding a sequence of
linear transformations and their following non-linear ones.
The strength of this complex model lies in the universal ap-
proximation theorem. A feed-forward network with a sin-
gle hidden layer containing a ﬁnite number of neurons can
approximate any continuous function, under mild assump-
tions on the activation function. However, this does not
indicate how much neurons are required and the number
can evaluate exponentially with respect to the complexity
of formulating the relationships between inputs and out-
puts. Deep neural networks, which are the backbone be-
hind deep learning, use a cascade of multiple hidden layers
to reduce the number of neurons required in each layer.

3

Regardless of the model, ML programs discover hid-
den patterns in training data and build a mathematical
model that makes predictions or identiﬁcations on future
unseen data, using these patterns. The learning aspect
resides in the model ﬁtting which is an iterative process
during which little adjustments are made repeatedly, with
the aim of reﬁning the model until it predicts mostly the
right outputs. Generally, supervised machine learning al-
gorithms are based on a diﬀerentiable model that trains
itself on input data through optimization routines using
gradient learning, to create a better model or probably
the best ﬁtted one (i.e., this could happen with a convex
objective function or advanced update steps). The princi-
pal components of a diﬀerentiable model are:

• Parameters, on which the model depends to make
its internal calculation and to provide its prediction
outputs. Therefore, these signals or factors are inner
variables of the model that the ML program gradu-
ally adjusts on its own through successive training it-
erations to build its logic and form its decision about
future data. For example, weights and biases used
by simple linear regression models or by Neural Net-
work’s neurons are parameters.

• Loss Function, at its core, represents a mathemat-
ical function given a real value that provide an es-
timation on how the model is performing in terms
of learning goals. It assesses the sum or the average
of the distance measure between predicted outputs
and actual outcomes. As indicated by its name, it
reﬂects the cost or the penalty of making bad pre-
dictions. If the model’s predictions are perfect, the
loss is zero; otherwise, the loss is greater.

• Regularization, assembles techniques, that penal-
ize the model’s complexity to prevent overﬁtting.
An example of regularization technique is the use
of L2 norm in linear regression models, which keeps
smaller overall weight values, relying on a prior be-
lief that weights should be small and normally dis-
tributed around zero. Another example of regular-
ization technique is the dropout in neural networks,
which allows removing a random selection of a per-
centage of neurons from training during an iteration.
This is to prevent models with high learning capac-
ity from memorizing the peculiarities of the train-
ing data, which would result in complex models that
overﬁt the training data. To guarantee a model’s
generalization ability, two parallel objectives should
be reached: (1) build the best-ﬁtted model i.e., low-
est loss and (2) keep the model as simple as possible
i.e., strong regularization.

• Optimizer, adjusts iteratively the internal param-
eters of the model in a way that reduces the objec-
tive function, i.e., includes generally the loss func-
tion plus a regularisation term. The most used op-
timizers are based on gradient descent algorithms,

which minimize gradually the objective function by
computing the gradients of loss with respect to the
model’s parameters and updates their values in di-
rection opposite to the gradients until ﬁnding a local
or the global minimum. The objective function has
to be globally continuous and diﬀerentiable. It is de-
sirable that this function be also strictly convex and
has exactly one local minimum point, which is also
the global minimum point. A great deal of research
in ML has focused on formulating various problems
as convex optimization problems and solving them
eﬃciently. Deep neural networks are never convex
functions but they are very successful because many
variations of gradient descent have a high proba-
bility of ﬁnding reasonably good solutions anyway,
even though these solutions are not guaranteed to
be global minimums.

• Hyperparameters, represent the model’s parame-
ters that are constant during the training phase and
which can be ﬁxed before running the ﬁtting pro-
cess. By learning a ML model, the ML engineers
identify a speciﬁc point in the model space with
some desirable behavior. In fact, choosing in prior
the model’s hyperparameters, such as the number of
layers and neurons in each layer for neural network,
the learning rate for gradient descent optimizer or
the regularization rate, allows to identify a subset of
the model space to search. It lasts to use the opti-
mizer with aim of ﬁnding the best ﬁtted model from
the selected subset through parameters adjustments.
Similarly, hyperparameters tuning is highly recom-
mended to ﬁnd the composition of hyper-parameters
that helps the optimization process ﬁnding the best-
ﬁtted model. The most used search methods are (1)
Grid search, which explores all possible combinations
from a discrete set of values for each hyperparame-
ter; (2) Random search which samples random values
from a statistical distribution for each hyperparam-
eter, and (3) Bayesian optimization which chooses
iteratively the optimal values according to the pos-
terior expectation of the hyperparameter. This ex-
pectation is computed based on previously evaluated
values, until converging to an optimum.

To train a diﬀerentiable model one needs three diﬀer-
training dataset, validation dataset, and
ent datasets :
testing dataset. After readying these data sets, ML en-
gineers set initial hyperparameters values and select loss
functions, regularization terms, and gradient-based opti-
mizers following best practices or guidelines from other
works that addressed a similar problem. Training a ML
model using an optimizer consists in gradually minimiz-
ing the loss measure plus the regularization term with re-
spect to the training dataset. Once the model’s parame-
ters are estimated, hyperparameters are tuned by evaluat-
ing the model performance on the validation dataset, and
selecting the next hyperparameters values according to a

4

search-based approach that aims to optimize the perfor-
mance of the model. This process is repeated using the
newly selected hyperparameters until a best-ﬁtted model
is obtained. This best-ﬁtted model is therefore tested us-
ing the testing dataset (which should be diﬀerent from
training and validation datasets).

Conceptual issues. One key assumption behind the train-
ing process of supervised ML models is that the training
dataset, the validation dataset, and the testing dataset,
which are sampled from manually labeled data, are rep-
resentative samples of the underlying problem. Following
the concept of Empirical Risk Minimization (ERM), the
optimizer allows ﬁnding the ﬁtted model that minimizes
the empirical risk; which is the loss computed over the
training data assuming that it is a representative sample
of the target distribution. The empirical risk can correctly
approximates the true risk only if the training data dis-
tribution is a good approximation of the true data dis-
tribution (which is often out of reach in real-world sce-
narios). The size of the training dataset has an impact
on the approximation goodness of the true risk, i.e., the
larger is a training data, the better this approximation will
be. However, manual labeling of data is very expensive,
time-consuming and error-prone. Training data sets that
deviate from the reality induce erroneous models.

Implementation issues. ML algorithms are often pro-
posed in pseudo-code formats that include jointly scien-
tiﬁc formula and algorithmic rules and concepts. When
it comes to implementing ML algorithms, ML engineers
sometimes have diﬃculties understanding these formulas,
rules, or concepts. Moreover, because there is no ‘test or-
acle” to verify the correctness of the estimated parameters
(i.e., the computation results) of a ML model, it is diﬃ-
cult to detect faults in the learning code. Also, ML algo-
rithms often require sophisticated numerical computations
that can be diﬃcult to implement on recent hardware ar-
chitectures that oﬀer high-throughput computing power.
Weyuker [5] identiﬁed three distinct sources of errors in
scientiﬁc software programs such as ML programs.

1. The mathematical model used to describe the

problem
A ML program is a software implementation of a
statistical learning algorithms that requires substan-
tial expertise in mathematics (e.g., linear algebra,
statistics, multivariate analysis, measure theory, dif-
ferential geometry, topology) to understand its in-
ternal functions and to apprehend what each com-
ponent is supposed to do and how we can verify that
they do it correctly. Non-convex objectives can cause
unfavorable optimization landscape and inadequate
search strategies. Model mis-speciﬁcations or a poor
choice of mathematical functions can lead to unde-
sired behaviors. For example, many ML algorithms
require mathematical optimizations that involve ex-
tensive algebraic derivations to put mathematical ex-

5

pressions in closed-form. This is often done through
informal algebra by hand, to derive the objective or
loss function and obtain the gradient. Generally, we
aim to adjust model parameters following the direc-
tion that minimizes the loss function, which is calcu-
lated based on a comparison between the algorithmic
outputs and the actual answers. Like any other infor-
mal tasks, this task is subject to human errors. The
detection of these errors can be very challenging, in
particular when randomness is involved. Stochastic
implementation errors can persist indeﬁnitely with-
out detection, since some errors may be masked by
the distributions of random variables and may re-
quire writing customized statistical tests for their
detection.
To avoid overﬁtting, ML engineers often add a reg-
ularization loss (e.g., norm L2 penalty on weights).
This regularization term which has a simple gradi-
ent expression can overwhelm the overall loss; result-
ing in a gradient that is primarily coming from it.
Which can make the detection of errors in the real
gradient very challenging. Also, non-deterministic
regularization techniques, such as dropout in neural
network can cause high-variance in gradient values
and further complicate the detection of errors, espe-
cially when techniques like numerical estimation are
used.

2. The program written to implement the com-

putation
The program written to implement a mathemati-
cal operation can diﬀer signiﬁcantly from the in-
tended mathematical semantic when complex opti-
mization mechanisms are used. Nowadays, most ML
programs leverage rich data structures (e.g., data
frames) and high performance computing power to
process massive data with huge dimensionality. The
optimization mechanisms of these ML programs is
often solved or approximated by linear methods that
consists of regular linear algebra operations involving
for example multiplication and addition operations
on vectors and matrices. This choice is guided by the
fact that high performance computers can leverage
parallelization mechanisms to accelerate the execu-
tion of programs written using linear algebra. How-
ever, to leverage this parallelism on Graphics Pro-
cessing Unit (GPU) platforms for example, one has
to move to higher levels of abstraction. The most
common abstractions used in this case are tensors,
which are multidimensional arrays with some related
operations. Most ML algorithms nowadays are for-
mulated in terms of matrix-vector, matrix-matrix
operations, and tensor-based operations (to extract a
maximum of performance from the hardware). Also,
ML models are more and more sophisticated, with
multiple layers containing huge number of param-
eters each. For such models, the gradient which

represents partial derivatives that specify how the
loss function is altered through individual changes in
each parameter is computed by grouping the partial
derivatives together in multidimensional data struc-
tures such as tensors, to allow for more straight for-
ward optimized and parallelized calculations. This
large gap between the mechanics of the high perfor-
mance implementation and the mathematical model
of one ML algorithm makes the translation from sci-
entiﬁc pseudo-code to highly-optimized program dif-
ﬁcult and error-prone. It is important to test that
the code representations of these algorithms reﬂect
the algorithms accurately.

3. Features of the environment such as round-oﬀ

error
The computation of continuous functions such as
gradient on discrete computational environments like
a digital computer incur some approximation errors
when one needs to represent inﬁnitely many real
numbers with a ﬁnite number of bit patterns. These
numerical errors of real numbers’ discrete represen-
tations can be either overﬂow or underﬂow. An over-
ﬂow occurs when numbers with large magnitude are
approximated as +∞ or −∞, which become not-
a-number values if they are used for many arith-
metic operations. An underﬂow occurs when num-
bers near zero are rounded to zero. This can cause
the numerical instability of functions such as divi-
sion (i.e., division by a zero returns not-a-number
value) or logarithm (i.e., logarithm of zero returns
−∞ that could be transform into not-a-number by
further arithmetic).
Hence, it is not suﬃcient to validate a scientiﬁc com-
puting algorithm theoretically since rounding errors
can lead to the failure of its implementation. Round-
ing errors on diﬀerent execution platforms can cause
instabilities in the execution of ML models if their ro-
bustness to such errors is not handled properly. Test-
ing for these rounding errors can help select adequate
mathematical formulations that minimize their ef-
fect.

When implementing ML programs, developers often
rely on third-party libraries for many critical operations.
These libraries provide optimized implementations of highly
intensive computation functions, allowing ML developers
to leverage distributed infrastructures such as high-performance
computing (HPC) clusters, parallelized kernels executing
on high-powered graphics processing units (GPUs), or to
merge these technologies to breed new clusters of multiple
GPUs. However, a misuse of these libraries can result in
faults that are hard to detect. For example, a copy-paste
of a routine that creates a neural network layer, without
changing the corresponding parameters (i.e., weights and
biases variables) would result in a network where the same
parameters are shared between two diﬀerent layers, which
is problematic. Moreover, this error can remain in the

6

code unnoticed for a long time. Which is why it is utterly
important to test that conﬁguration choices do not cause
faults or instabilities in ML programs.
In the following
we explain the three main categories of libraries that exist
today and discuss of their potential misuse.

• High-level ML libraries: A high-level ML library
emphasizes the ease of use through features such as
state-of-the-art supervised learning algorithms, un-
supervised learning algorithms, and evaluation meth-
ods (e.g., confusion matrix and cross-validation). It
serves as an interface and provides a high level of
abstraction, where ML algorithms can be easily con-
ﬁgured without hard-coding. Using such library, ML
developers can focus on converting a business objec-
tive into a ML-solvable problem. However, ML de-
velopers still need to test the quality of data and en-
sure that it is conform to the requirements of these
built-in functions, such as input data formats and
types. Moreover, a poor conﬁguration of these pro-
vided algorithms could result in unstable or miscon-
ceived models. For example, choosing a sigmoid as
activation functions in a deep neural network can
cause the saturation of neurons and consequently,
slowing down the learning process. Therefore, after
ﬁnishing the conﬁguration, developers need to set
up monitoring routines to watch internal variables
and metrics during the execution of the provided al-
gorithms, in order to check for possible numerical
instabilities, or suspicious outputs.

• Medium-level ML libraries: Medium-level libraries
provide machine learning or deep learning routines
as ready-for-use features, such as numerical opti-
mization techniques, mathematical function and au-
tomatic diﬀerentiation capabilities, allowing ML de-
velopers to not only conﬁgure the pre-deﬁned ML
algorithms, but also to use the provided routines to
deﬁne the ﬂow of execution of the algorithms. This
ﬂexibility allows for easy extensions of the ML mod-
els using more optimized implementations. However,
this ability to design the algorithm and its computa-
tion ﬂow through programming variables and native
loops increases the risk of faults and poor coding
practices, as it is the case for any traditional pro-
gram.

• Low-level ML libraries: Contrary to high level
and medium level libraries, this family of libraries
do not provide any pre-deﬁned ML feature, instead,
they provide low level computations primitives that
are optimized for diﬀerent platforms. They oﬀer
powerful N -dimensional arrays, which are commonly
used in numerical operations and linear algebra rou-
tines for a high level of eﬃciency. They also help with
data processing operations such as slicing and index-
ing. ML developers can use these libraries to build a
new ML algorithm from scratch or highly-optimized

implementations of particular algorithms for speciﬁc
contexts or hardwares. However, this total control
on the implementation is not without cost. Eﬀec-
tive quality assurance operations such as testing and
code reviews are required to ensure bug free imple-
mentations. ML developers need strong backgrounds
in mathematics and programming to be able to work
eﬃciently with these low level libraries.

The amount of code that is written when implement-
ing a ML program depends on the type of ML library
used. The more a developer uses high-level features from
libraries, the more he has to write glue code to integrate
incompatible components, which are putted together into
a single implementation. Sculley et al. [4] observed that
the amount of this glue can account for up to 95% of the
code of certain ML programs. This code should be tested
thoroughly to avoid faults.

3. Research Trends in ML Application Testing

In this section, we analyze research works that pro-
posed testing techniques for ML programs. We organize
the testing techniques in two categories based on the in-
tention behind the techniques, i.e., techniques that aim
to detect conceptual and implementation errors in data,
and techniques that focus on ensuring correct conception
and implementation of ML models. In each of these cat-
egories, we divide the techniques in sub-groups based on
the concepts used in the techniques. Next, we discuss the
fundamental concepts behind each proposed techniques,
explaining the types of errors that can be identiﬁed using
them while also outlining their limitations.

3.1. Approaches that aim to detect conceptual and imple-

mentation errors in data

The approaches proposed in the literature to test the
quality of data addresses both conceptual and implemen-
tation issues, therefore, we discuss these two aspects to-
gether in this section. The most common technique used
to test the quality of data is the analysis-driven data clean-
ing that consists of applying analytical queries (e.g., ag-
gregates, advanced statistical analytic, etc.) in order to
detect errors and perform adequate transformations.
In this approach, aggregations such as sum or count and
central tendencies such as mean, median or mode are used
to verify if each feature’s distribution matches expecta-
tion. For example, one can check that features take on
their usual set or range of values, and the frequencies of
these values in the data.
After this initial veriﬁcation, advanced statistical analy-
ses such as hypothesis testing and correlation analysis are
applied to verify correlations between pair of features and
to assess the contribution of each feature in the prediction
or explanation of the target variable. The beneﬁt of each
feature can also be estimated by computing the proportion
of its explained variance with respect to the target output

or by assessing the resulting accuracy of the model when
removing it in prior to the ﬁtting process. Besides, when
assessing the contribution of each feature to the model, it is
recommended to take into account the added inference la-
tency and RAM usage, more upstream data dependencies,
and additional expected instability incurred by relying on
that feature. It is important to consider whether this cost
is worth paying when traded oﬀ against the provided im-
provement in model quality.

Recent research work by Krishnan et al. [6] remarked
that these aggregated queries can sometimes diminish the
beneﬁts of data cleaning. They observed that cleaning
small samples of data often suﬃces to estimate results with
high accuracy. They also observed that the power of sta-
tistical anomaly detection techniques rapidly deteriorates
in the high-dimensional feature-spaces.
This comes from the fact that the aforementioned analysis-
driven data cleaning operations require data queries in or-
der to calculate the aggregate values and correlation mea-
sures. Indeed, performing many data queries and clean-
ing operation on the entire dataset could be impractical
with huge amount of training datasets that likely contain
dirty records. Moreover, ML developers often face diﬃ-
culties to establish the data cleaning process. To address
these issues, Krishnan et al. proposed ActiveClean [1], an
interactive data-cleaning framework for ML models that
allows ML developers to improve the performance of their
model progressively as they clean the data. The frame-
work has an embedding process that ﬁrstly samples a sub-
set of likely dirty records from training data using a set of
optimizations, which includes importance weighting and
dirty data detection. Secondly, the ML developer is inter-
actively invited to transform or remove each data instance
from the selected batches of probably dirty data. Finally,
the framework updates the model’s parameters and contin-
ues the training using partially cleaned data. This process
is repeated until no potential dirty instances could be de-
tected. With ActiveClean, developers are still responsible
for deﬁning data cleaning operations. The framework only
decides where to apply these operations. Recently, Krish-
nan et al. [7] proposed a full-automated framework, Boost-
Clean, to establish a pipeline of data transformations that
allow cleaning eﬃciently the data in order to ﬁt well the
model. BoostClean ﬁnds automatically the best combina-
tion of dirty data detection and repair operations by lever-
aging the available clean test labels in order to improve
model accuracy. It selects this ensemble from extensible
libraries : (1) pre-populated general detection functions,
allow identifying numerical outliers, invalid and missing
values, checking whether a variable values match the col-
umn type signature, and detecting eﬀectively text errors
in string-valued and categorical attributes using word em-
bedding; (2) a pre-populated set of simple repair functions
that can be applied to records identiﬁed by a detector’s
predicate, such as impute a cell value with one central
tendency (i.e., mean, median and mode value), or discard
a dirty record from the dataset. Thus, the boosting tech-

7

nique, which combines a set of weak learners and estimates
their corresponding weights to spawn a single prediction
model, is applied to solve the problem of detecting the
optimal sequence of repairs that could best improve the
ML model by formulating it as an ensembling problem.
It consists of generating a new model trained on input
data with new additional cleaned features and selecting
the best collection of models that collectively estimate the
predict label. Krishnan et al. evaluated their proposed
framework on 8 ML datasets from Kaggle and the UCI
repository which contained real data errors. They showed
that BoostClean can increase the absolute prediction accu-
racy by 8-9% over the best non-ensemble alternatives, in-
cluding statistical anomaly detection and constraint-based
techniques.
By automating the selection of cleaning operations, Boost-
Clean signiﬁcantly simpliﬁes the data cleaning process,
however, this framework is resource consuming as it re-
quires the use of multiple models and boosting techniques.
Moreover, the evaluation of the embedded cleaning process
results on new datasets is challenging because the creation
of the cleaning data pipeline is driven by pure statistical
analysis.
Hynes et al.[8] inspired by code linters, which are well-
known software quality tools, introduced data linter to
help ML developers track and ﬁx issues in relation to data
cleaning, data transformation and feature extraction. The
data linter helps reduce the human burden by automati-
cally generating issues explanations and building more so-
phisticated human-interactive loop processing. First, it in-
spects errors in training datasets such as scale diﬀerences
in numerical features, missing or illegal values (e.g., NaN),
malformed values of special string types (e.g., dates), and
other problematic issues or ineﬃciencies discussed in Sec-
tion 2.1. The inspection relies on data’s summary statis-
tics, individual items inspection, and column names given
to the features. Second, given the detected errors and
non-optimal data representations, it produces a warning,
a recommendation for how to transform the feature into
a correct or optimal feature, and a concrete instance of
the lint taken directly from the data. Data linter guides
its users in their cleaning data and features engineering
process through providing actionable instructions of how
individual features can be transformed to increase the like-
lihood that the model can learn from them. The main
strength of this tool resides in the semi-automated data
engineering process and the fact that it can be applied to
all statistical learning models and several diﬀerent data
types. The proposed data linter has the ability to infer
semantic meaning/intent of a feature as a complement
to statistical analysis with the aim of providing speciﬁc
and comprehensible feature engineering recommendations
to ML developers.
As mentioned in Section 2.1, a Data loader is often re-
quired for systems that rely on mini-batch stochastic opti-
mizers to estimate the model’s parameters. To test the re-
liability of this component, the following best practices are

often used: (1) Shuﬄing of the dataset before starting the
batches generation. This action is recommended to pre-
vent the occurrence of one single label batch (i.e., sample
of data labeled by the same class) which would negatively
aﬀect the eﬃciency of mini-batch stochastic optimizers in
ﬁnding the optimal solution.
In fact, a straightforward
extraction of batches in sequence from data ordered by
label or following a particular semantic order, can cause
the occurrence of one single label batch. (2) Checking the
predictor/predict inputs matching. A random set of few
inputs should be checked to verify if they are correctly
connected to their labels following the shuﬄe of data; (3)
reduce class imbalance. This step is important to keep
the class proportions relatively conform to the totality of
training data.

3.2. Approaches that aim to detect conceptual and imple-

mentation errors in ML models

As discussed in Section 2.2 and illustrated on Figure 1,
errors in ML models can be due to conceptual mistakes
when creating the model or implementation errors when
writing the code corresponding to the model. In the fol-
lowing, we discuss testing approaches that focus on these
two aspects, separately.

3.2.1. Approaches that aim to detect conceptual er-

rors in ML models

Approaches in this category assume that the models
are implemented into programs without errors and focus
on providing mechanisms to detect potential errors in the
calibration of the models. These approaches can be di-
vided in two groups: black-box and white-box approaches [9].
Black-box approaches are testing approaches that do not
need access to the internal implementation details of the
model under test. These approaches focus on ensuring
that the model under test predicts the target value with
a high accuracy, without caring about its internal learned
parameters. White-box testing approaches on the other
hand take into account the internal implementation logic
of the model. The goal of these approaches is to cover a
maximum of speciﬁc spots (e.g., neurons) in models. In
the following, we elaborate more on approaches from these
two groups.

A: Black-box testing approaches for ML models.
The common denominator to black-box testing approaches
is the generation of adversarial data set that is used to
test the ML models. These approaches leverage statisti-
cal analysis techniques to devise a multidimensional ran-
dom process that can generate data with the same statis-
tical characteristics as the input data of the model. More
speciﬁcally, they construct generative models that can ﬁt a
probability distribution that best describes the input data.
These generative models allows to sample the probabil-
ity distribution of input data and generate as many data
points as needed for testing the ML models. Using the
generative models, the input data set is slightly perturbed

8

to generate novel data that retains many of the original
data properties. The advantage of this approach is that
the synthetic data that is used to test the model is inde-
pendent from the ML model, but statistically close to its
input data. Adversarial machine learning is an emerging
technique that aims to assess the robustness the machine
learning models based on the generation of adversarial ex-
amples. The latter are pairs of test inputs that cause a
disagreement on their classiﬁcation labels when close to
each other (in terms of a given distance metric).
In fact, ML models are designed to identify latent concepts
from the training in order to learn how to predict the tar-
get value in relation to the unseen test data. However, the
fact that both of training and testing data set are assumed
to be generated from the same distribution makes the ML
system vulnerable with respect to malicious adaptive ad-
versaries that manipulate the input data and violate some
of their prior hypotheses. Hence, it is important to test
the robustness of ML models to such variations in input
data. Several mechanisms exist for the creation of adver-
sarial examples, such as : making small modiﬁcations to
the input pixels[10], applying spatial transformations[11],
or simple guess-and-check to ﬁnd misclassiﬁed [12].

Recent results [13] [14] involving adversarial evasion
attacks against deep neural network models have demon-
strated the eﬀectiveness of adversarial examples in reveal-
ing weaknesses in models. Multiple DNN-based image
classiﬁers that achieved state-of-the-art performance lev-
els on randomly selected dataset where found to perform
poorly on synthetic images generated by adding humanly
imperceptible perturbations.

One major limitation of these black-box testing tech-
niques is the representativeness of the generated adversar-
ial examples. In fact, many adversarial models that gener-
ate synthetic images often apply only tiny, undetectable,
and imperceptible perturbations, since any visible change
would require manual inspection to ensure the correctness
of the model’s decision. This can result in strange aber-
rations or simpliﬁed representations in synthetic datasets,
which in turn can have a hidden knock-on eﬀects on the
performance of a ML model when unleashed in a real-
world setting. These black-box testing techniques that
rely only on adversarial data (ignoring the internal imple-
mentation details of the models under test) often fail to
uncover diﬀerent erroneous behaviors of the model, even
after performing a large number of tests. This is because
the generated adversarial data often fail to cover the pos-
sible behaviors of the model adequately. An outcome that
is not surprising given that the adversarial data are gener-
ated without considering information about the structure
of the models. To help improve over these limitations, ML
researchers have developed the white-box techniques de-
scribed below, which use internal structure speciﬁcities to
guide the generation of more relevant test cases.

B: White-box testing approaches for ML models.
Pei et al. proposed DeepXplore [15], the ﬁrst white-box

approach for systematically testing deep learning mod-
els. DeepXplore is capable of automatically identifying
erroneous behaviors in deep learning models without the
need of manual labelling. The technique makes use of a
new metric named neuron coverage, which estimates the
amount of neural network’s logic explored by a set of in-
puts. This neuron coverage metric computes the rate of
activated neurons in the neural network. It was inspired
by the code coverage metrics used for traditional software
systems. The approach circumvent the lack of a reference
oracle, by using diﬀerential testing. Diﬀerential testing is
a pseudo-oracle testing approach that has been success-
fully applied to traditional software that do not have a
reference test oracle [16]. It is based on the intuition that
any divergence between programs’ behaviors, solving the
same problem, on the same input data is a probably due
to an error. Therefore, the test process consists of writ-
ing at ﬁrst multiple independent programs to fulﬁll the
same speciﬁcation. Then, the same inputs are provided
to these similar programs, which are considered as cross-
referencing oracles. Diﬀerences in their executions are in-
spected to identify potential errors. In a same way, Deep-
Xplore leverages a group of similar deep neural networks
that solve the same problem. Perturbations are introduced
in inputs data to create many realistic visible diﬀerences
(e.g., diﬀerent lighting, occlusion, etc.) and automatically
detect erroneous behaviors of deep neural networks un-
der these circumstances. Applying diﬀerential testing to
deep learning with the aim of ﬁnding a large number of
diﬀerence-inducing inputs while maximizing neuron cov-
erage can be formulated as a joint optimization problem.
DeepXplore performs gradient ascent to solve eﬃciently
this optimization problem using the gradient of the deep
Its objective
neural network with respect to the input.
is to generate test data that provokes a diﬀerent behav-
ior from the group of similar deep neural networks under
test in order to ensure a high neuronal coverage. We no-
ticed that domain-speciﬁc constraints are added to gener-
ate data that is valid and realistic. In the end of the testing
process, the generated data are kept for future training, to
have more robustness in the model.

Ma et al.[17] generalized the concept of neuron coverage
by proposing DeepGauge, a set of multi-granularity testing
criteria for Deep Learning systems. DeepGauge measures
the testing quality of test data (whether it being genuine
or synthetic) in terms of its capacity to trigger both ma-
jor function regions as well as the corner-case regions of
DNNs(Deep Neural Networks). It separates DNNs testing
coverage in two diﬀerent levels.
At the neuron-level, the ﬁrst criterion is k-multisection
neuron coverage, where the range of values observed dur-
ing training sessions for each neuron are divided into k sec-
tions to assess the relative frequency of returning a value
belonging to each section. In addition, the authors insist
on the need for test inputs that are enough diﬀerent from
training data distribution to cover rare neurons’ outputs.
They introduced the concept of neuron boundary cover-

9

age to measure how well the test datasets can push acti-
vation values to go above and below a pre-deﬁned bound
(i.e., covering the upper boundary and the lower bound-
ary values). Their design intentions are complementary
in the sense that the k-multisection neuron
to Pei et al.
coverage could potentially help to cover the main function-
alities provided by DNN. However, the neuron boundary
coverage could relatively approximate corner-cases DNN’s
behaviors.
At layer-level, the authors leveraged recent ﬁndings that
empirically showed the potential usefulness of discovered
patterns within the hyperactive neurons, which render rel-
atively larger outputs. On the one hand, each layer allows
DNN to characterize and identify particular features from
input data and its main function is in large part supported
by its top active neurons. Therefore, regarding the eﬀec-
tiveness in discovering issues, test cases should go beyond
these identiﬁed hyperactive neurons in each layer. On
the other hand, DNN provide the predicted output based
on pattern recognized from a sequence features, including
simple and complex ones. These features are computed
by passing the summary information through hidden lay-
ers. Thereby, the combinations of top hyperactive neurons
from diﬀerent layers characterize the behaviors of DNN
and the functional scenarios covered. Intuitively, test data
sets should trigger other patterns of activated neurons in
order to discover corner-cases behaviors.
In their empirical evaluation, Ma et al. showed that Deep-
Gauge scales well to practical sized DNN models (e.g.,
VGG-19, ResNet-50) and that it could capture erroneous
behavior introduced by four state-of-the-art adversarial
data generation algorithms (i.e., Fast Gradient Sign Method
(FGSM) [10], Basic Iterative Method (BIM) [18], Jacobian-
based Saliency Map Attack (JSMA)[19] and Carlini/Wag-
ner attack (CW) [13]). Therefore, a higher coverage of
their criteria potentially plays a substantial role, in im-
proving the detection of errors in the DNNs. These posi-
tive results show the possibility to leverage this multi-level
coverage criteria to create automated white-box testing
frameworks for neural networks.

Building on the pioneer work of Pei et al., Tian et al.
proposed DeepTest [20], a tool for automated testing of
DNN-driven autonomous cars.
In DeepTest, Tian et al.
expanded the notion of neuron coverage proposed by Pei
et al. for CNNs (Convolutional Neural Networks), to other
types of neural networks, including RNNs (Recurrent Neu-
ral Networks). Moreover, instead of randomly injecting
perturbations in input image data, DeepTest focuses on
generating realistic synthetic images by applying realistic
image transformations like changing brightness, contrast,
translation, scaling, horizontal shearing, rotation, blur-
ring, fog eﬀect, and rain eﬀect, etc. They also mimic dif-
ferent real-world phenomena like camera lens distortions,
object movements, diﬀerent weather conditions, etc. They
argue that generating inputs that maximize neuron cover-
age cannot test the robustness of trained DNN unless the
inputs are likely to appear in the real-world. They provide

a neuron-coverage-guided greedy search technique for eﬃ-
ciently ﬁnding sophisticated synthetic tests which capture
diﬀerent realistic image transformations that can increase
neuron coverage in a self-driving car DNNs. To compen-
sate for the lack of a reference oracle, DeepXplore used dif-
ferential testing. However, DeepTest leverages metamor-
phic relations (MRs) to create a test oracle that allows it
to identify erroneous behaviors without requiring multiple
DNNs or manual labeling. Metamorphic testing [21] is an-
other pseudo-oracle software testing technique that allows
identifying erroneous behaviors by detecting violations of
domain-speciﬁc metamorphic relations (MR). These MRs
are deﬁned across outputs from multiple executions of the
test program with diﬀerent inputs. The application of such
test consists of performing some input changes in a cer-
tain way that allows testers to predict the output based
on identiﬁed MRs, so any signiﬁcant diﬀerences in output
would break the relation, which would indicate the exis-
tence of errors in the program. DeepRoad [22] continued
the same line of work as DeepTest, designing a systematic
mechanism for the automatic generation of test cases for
DNNs used in autonomous driving cars. Data sets captur-
ing complex real-world driving situations is generated and
Metamorphic Testing is applied to map each data point
into the predicted continuous output. However, Deep-
Road diﬀerentiates from DeepTest in the approach used
to generate new test images. DeepRoad relies on a Gen-
erative Adversarial Network (GAN)-based method to pro-
vide realistic snowy and rainy scenes, which can hardly
be distinguished from original scenes and cannot be gen-
erated by DeepTest using simple aﬃne transformations.
Zhang et al. argue that DeepTest synthetic image trans-
formations, such as adding blurring/fog/rain eﬀect ﬁlters,
cannot simulate complex weather conditions. They claim
that DeepTest’s produced road scenes may be unrealis-
tic, because simply adding a group of lines over the origi-
nal images cannot reﬂect the rain condition or mixing the
original scene with the scrambled “smoke” eﬀect does not
simulate the fog. To solve this lack of realism in generated
data, DeepRoad leveraged a recent unsupervised DNN-
based method (i.e., UNIT) which is based on GANs and
VAEs, to perform image-to-image transformations. UNIT
can project images from two diﬀerent domains (e.g., a dry
driving scene and a snowy driving scene) into a shared
latent space, allowing the generative model to derive the
artiﬁcial image (e.g., the snowy driving scene) from the
original image (e.g., the dry driving scene). Evaluation
results show that the generative model used by DeepRoad
successfully generates realistic scenes, allowing for the de-
tection of thousands of behavioral inconsistencies in well-
known autonomous driving systems.

Despite the relative success of DeepXplore, DeepTest,
and DeepRoad, in increasing the test coverage of neural
networks, Ma et al. [23] remarked that the runtime state
space is very large when each neuron output is considered
as a state, which can lead to a combinatorial explosion.
To help address this issue, they proposed DeepCT, which

10

is a new testing method that adapts combinatorial test-
ing (CT) techniques to deep learning models, in order to
reduce the testing space. CT [24] has been successfully
applied to test traditional software requiring many conﬁg-
urable parameters. It helps to sample test input param-
eters from a huge original space that are likely related to
undetected errors in a program. For example, the t-way
combinatorial test set covers all the interactions involving
t input parameters, in a way that expose eﬃciently the
faults under the assumption of a proper input parameters’
modeling. In DeepCT, K-way CT is adapted to allow for
selecting eﬀectively samples of neuron interactions inside
diﬀerent layers with the aim of decreasing the number of
test cases. Given the initial test data sets. DeepCT gen-
erates some DNN-related K-way coverage criteria using
constraint-based solvers (i.e., by linear programming us-
ing the CPLEX solver [25]). Next, a new test data is gen-
erated by perturbing the original data within a preﬁxed
value range, while ensuring that previously generated CT
coverage criteria are satisﬁed on each layer. Ma et al. [23]
conducted an empirical study, comparing the 2-way CT
cases with random testing in terms of the number of ad-
versarial examples detected. They observed that random
testing was ineﬀective even when a large number of tests
were generated. In comparison, DeepCT performed well
even when only the ﬁrst several layers of the DNN were
analyzed, which shows some usefulness for their proposed
CT coverage criteria in the context of adversarial exam-
ples detection and local-robustness testing.
However, even though solvers like CPLEX represents the
state-of-the-practice, their scalability remains an issue. Hence,
the eﬀectiveness of the proposed DeepCT approach on
real-world problems using large and complex neural net-
works remains to be seen.

Sun et al. [26] examined the eﬀectiveness of the neu-
ron coverage metric introduced by DeepXplore and report
that a 100% neuron coverage can be easily achieved by a
few test data points while missing multiples incorrect be-
haviors of the model. To illustrate this fact, they showed
how 25 randomly selected images from the MNIST test set
yield a close to 100% neuron coverage for an MNIST clas-
siﬁer. Thereby, they argue that testing DNNs should take
into account the semantic relationships between neurons in
adjacent layers in the sense that deeper layers use previous
neurons’ information represented by computed features
and summarize them in more complex features. To pro-
pose a solution to this problem, they adapted the concept
of Modiﬁed Condition/Decision Coverage (MC/DC)[27]
developed by NASA. The concepts of “decision” and “con-
dition” in the context of DNN-based systems correspond to
testing the eﬀects of ﬁrst extracted less complex features,
which can be seen as potential factors, on more complex
features which are intermediate decisions. Consequently,
they specify each neuron in a given layer as a decision and
its conditions are its connected input neurons from the
previous layer.
They propose a testing approach that consists of a set of

11

four criteria inspired by MC/DC and a test cases genera-
tor based on linear programming (LP). As an illustration
of proposed criteria, we detail their notion of SS cover-
age, which is very close to the spirit of MC/DC. Since the
neurons’ computed outputs are numeric continuous val-
ues, the SS coverage could not catch all the interactions
between neurons in successive layers. To address this lim-
itation, three additional coverage criteria have been added
to allow detecting diﬀerent ways of how the changes of the
conditions can aﬀect the models’ decision.

Sun et al. [28] also applied concolic testing [29] to DNNs.
Concolic testing combines concrete executions and sym-
bolic analysis to explore the execution paths of a program
that are hard to cover by blind test cases generation tech-
niques such as random testing. The proposed adaptation
of concolic testing to DNNs leverages state-of-the-art cov-
erage criteria and search approaches. The authors ﬁrst for-
mulate an objective function that contains a set of existing
DNN-related coverage requirements using Quantiﬁed Lin-
ear Arithmetic over Rationals (QLAR). Then, their pro-
posed method incrementally ﬁnds inputs data that satisfy
each test coverage requirement in the objective. Iterating
over the set of requirements, the concolic testing algorithm
ﬁnds the existing test input that is the closest data point
to satisfy the current requirement following an evaluation
based on concrete execution. Relying on this found in-
stance, it applies symbolic execution to generate a new
data point that satisﬁes the requirement and adds it to
the test suite. The process ﬁnishes by providing a test
suite that helps reaching a satisfactory level of coverage.
To assess the eﬀectiveness of their proposed approach, they
evaluated the number of adversarial examples that could
be detected by the produced test suites.

3.2.2. Approaches that aim to detect errors in ML code

implementations

Given the stochastic nature of most ML algorithms and
the absence of oracles, most existing testing techniques
are inadequate for ML code implementations. As a con-
sequence, the ML community have resorted to numerical
testing, property-based testing, metamorphic testing, mu-
tation testing, coverage-guided fuzzing testing, and proof-
based testing techniques to detect issues in ML code imple-
mentations. In the following, we present the most promi-
nent techniques.

Numerical-based testing: Finite-diﬀerence techniques.
Most machine learning algorithms are formulated as op-
timization problems that can be solved using gradient-
based optimizers, such as gradient descent or L-BFGS
(i.e., Limited-memory Broyden–Fletcher–Goldfarb–Shanno
algorithm). The correctness of the objective function gra-
dient that are computed with respect to the model pa-
rameters, is crucial.
In practice, developers often check
the accuracy of gradients using a ﬁnite diﬀerence tech-
nique that simply consists in performing the comparison
between the analytic gradient and the numerical gradient.

However, because of the increasing complexity of models’
architectures, this technique is prone to errors. To help
improve this situation, Karpathy [30] have proposed a set
of heuristics to help detect faulty gradients.

a)Use of the centered formula. Instead of relying on
the traditional gradient formula, Karpathy recommends
using the centered formula from Equation 1 which is more
precise. The taylor expansion of the numerator indicates
that the centered formula has an error in the order of
O (cid:0)h2(cid:1), while the standard formula has an error of O (h).

df (x)
dx

=

f (x + h) − f (x − h)
2h

(1)

b) Use of relative error for the comparison. As
mentioned above, developers perform gradient checking by
computing the diﬀerence between the numerical gradient
f (cid:48)
n and the analytic gradient f (cid:48)
a. This diﬀerence can be
seen as an absolute error and the aim of the gradient check-
ing test is to ensure that it remains below a pre-deﬁned
ﬁxed threshold. With deep neural networks, it can be
hard to ﬁx a common threshold in advance for the abso-
lute error. Karpathy recommends ﬁxing a threshold for
relative errors. So, for a deep neural network’s loss that is
a composition of ten functions, a relative error 2 of 1 exp−2
might be acceptable because the errors build up through
backpropagation. Conversely, an error of 1 exp−2 for a
single diﬀerentiable function likely indicates an incorrect
gradient.

|f (cid:48)
max(|f (cid:48)

a − f (cid:48)
n|
a|, |f (cid:48)

n|)

(2)

c) Use of double precision ﬂoating point. Karpathy
recommends to avoid using single precision ﬂoating point
to perform gradient checks, because it often causes high
relative errors even with a correct gradient implementa-
tion.

d) Stick around active range of ﬂoating point. To
train complex statistical models, one needs large amounts
of data. So, it is common to opt for mini-batch stochastic
gradient descent and to normalize the loss function over
the batch. However, if the back-propagated gradient is
very small, additional divisions by data inputs count will
yield extremely smaller vales, which in turn can lead to
numerical issues. As a solution to this issue, Karpathy
recommends computing the diﬀerence between values with
minimal magnitude, otherwise one should scale the loss
function up by a constant to bring the loss to a denser
ﬂoats range, ideally on the order of 1.0 (where the ﬂoat
exponent is 0).

e) Use only few random data inputs. The use of
fewer data inputs reduces the likelihood to cross kinks
when performing the ﬁnite-diﬀerence approximation. These
kinks refer to non-diﬀerentiable parts of an objective func-
tion and can cause inaccuracies in the numerical gradi-
ents. For example, let’s consider an activation function

12

ReLU that has an analytic gradient at the zero point, i.e.,
it is exactly zero. The numerical gradient can compute
a non-zero gradient because it might cross over the kink
and introduce a non-zero contribution. Karpathy strongly
recommends to perform the gradient checking for a small
sample of the data and infer the correctness of gradient
for an entire batch, because it makes this sanity-check fast
and more eﬃcient in practice.

f ) Check only few dimensions. Recent statistical mod-
els are more and more complex and may contain thousands
parameter with millions of dimensions. The gradients are
also multi-dimensional. To mitigate errors, Karpathy rec-
ommends checking a random sample of the dimensions of
the gradient for each separate model’s parameter, while
assuming the correctness of the remaining ones.

g) Turn oﬀ dropout/regularization penalty. Devel-
opers should be aware of the risk that the regularization
loss overwhelms the data loss and masks the fact that there
exists a large error between the analytic gradient of data
loss function and its numerical one. Such circumstance
can result in a failure to detect an incorrect implementa-
tion of the loss function or its corresponding gradient us-
ing the ﬁnite-diﬀerence approximation technique. Karpa-
thy recommends turning oﬀ regularization and checking
the data loss alone, and then the regularization term, in-
dependently. Moreover, recent regularization techniques
applied to deep neural networks such as dropout, induce a
non-deterministic eﬀect when performing gradient check.
Thereby, to avoid errors when estimating the numerical
gradient, it is recommended to turn them oﬀ. An alter-
native consists in forcing a particular random seed when
evaluating both gradients.

Property-based testing. Property-based testing is a tech-
nique that consists in inferring the properties of a compu-
tation using the theory and formulating invariants that
should be satisﬁed by the code. Using the formulated in-
variants, test cases are generated and executed repeatedly
throughout the computation to detect potential errors in
the code. Using property-based testing, one can ensure
that probability laws hold throughout the execution of a
model. For example, one can test that all the computed
probability values are non-negatives. For a discrete prob-
ability distribution, as in the case of a classiﬁer, one can
verify that the probabilities of all events add up to one.
Also, marginalization can be applied to test probabilistic
classiﬁers.
Roger et al. [31] applied property-based testing to detect
errors in implementations of MCMC (Markov chain Monte
Carlo) samplers. They derived by hand the update rules
for individual variables using the theory, i.e., they sample
a random variable from its conditional distribution. Then,
they wrote a single iterative routine including those rules.
To test the correctness of the produced samples, they ver-
iﬁed that the conditional distribution was consistent with
the joint distribution at each update iteration.

Karpathy [30] recommend the veriﬁcation of the fol-

lowing properties.

Initial random loss : when training a neural network
classiﬁer, turn oﬀ the regularization by setting its
corresponding strength hyperparameter to zero and
(cid:17)
verify that initial softmax loss is equal − log
with Nc: number of label classes. One expect a dif-
fuse probability of 1
Nc

for each class.

(cid:16) 1
Nc

Overﬁtting a tiny dataset : Keeping the regularization
term turned oﬀ, extract a sample portion of data,
(one or two examples inputs from each class) in or-
der to ensure that the training algorithm can achieve
eﬃciently zero loss. Breck et al. also recommend
watching the internal state of the model on small
amounts of data with the aim of detecting issues like
numerical instability that can induce invalid numeric
values like NaNs or inﬁnities.

Regularization role : increase the regularization strength

and check if the data loss is also increasing.

Another testing technique that shares the same philos-

ophy as property-based testing is metamorphic testing.

Metamorphic testing. Murphy et al. [32] introduced
metamorphic testing to ML in 2008. They deﬁned several
Metamorphic Relationships (MRs) that can be classiﬁed
into six categories (i.e., additive, multiplicative, permu-
tative, invertive, inclusive, and exclusive). They applied
these MRs to test three ML applications: Marti-Rank,
SVM-Light (support vector machine with a linear kernel),
and PAYL [33]. Xie et al. [34] expanded the work of Mur-
phy et al. by introducing new MRs for testing K-Nearest
Neighbors classiﬁer and Naive Bayesian classiﬁer.
Recent research works [35] have investigated the appli-
cation of metamorphic testing to more complex machine
learning algorithms such as SVM with non-linear kernel
and deep residual neural networks (ResNET). The tech-
nique was able to successfully ﬁnd mutants in open-source
machine learning applications.

Mutation testing. Ma et al.[36] proposed DeepMutation
that adapts mutation testing [37] to DNN-based systems
with the aim of evaluating the test data quality in terms
of its capacity to detect faults in the programs. Mu-
tation testing consists in injecting artiﬁcial faults (i.e.,
mutants) in a program under test and generating test
cases to detect them. To build DeepMutation, Ma et
al. deﬁned a set of source-level mutation operators to
mutate the source of a ML program by injecting faults.
These operators allow injecting faults in the training data
(using data mutation operators) and the model training
source code (using program mutation operators). After
the faults are injected, the ML program under test is ex-
ecuted, using the mutated training data or code, to pro-
duce the resulting mutated DNNs. The data mutation

13

operators are intended to introduce potential data-related
faults that could occur during data engineering (i.e., dur-
ing data collection, data cleaning, and–or data transfor-
mation). Program mutation operators’ mimic implemen-
tation faults that could potentially exist in the model im-
plementation code. These mutation operators are sematic-
based and specialized for DNNs’ code. Training models
from scratch following source-level mutations is very time-
consuming since advanced deep learning systems require
often tens of hours and even days to learn the model’s
parameters. Moreover, manually designing speciﬁc muta-
tion operators using information about faults occurring in
real-world DNNs systems is challenging, since it is diﬃ-
cult to imagine and simulate all possible faults that oc-
cur in real-world DNNs. To circumvent the cost of mul-
tiple re-execution and ﬁll in the gap between real-world
erroneous models and mutated models, the authors de-
ﬁne model-level mutation operators to complement source-
level mutation operators. These operators directly change
the structure and the parameters of neural network models
to scale the number of resulted mutated models for testing
ML programs in an eﬀective way, and for covering more
ﬁne-grained model-level problems that might be missed
by only mutating training data and–or programs. Once
the mutated models are produced, the mutation testing
framework assesses the eﬀectiveness of test data and spec-
ify its weaknesses based on evaluation metrics related to
the killed mutated models count. ML engineers can lever-
age this technique to improve data generation and increase
the identiﬁcation of corner-cases DNN behaviors.

Coverage-Guided Fuzzing. Odena and Goodfellow [38]
developed a coverage-guided fuzzing framework specialized
for testing neural networks. Coverage-guided fuzzing has
been used in traditional software testing to ﬁnd critical er-
rors. For ML code, the fuzzing process consists of handling
an input corpus that evolves through the execution of tests
by applying random mutation operations on its contained
data and keeping only interesting instances that allow trig-
gering new program behavior. Iteratively, the framework
samples an input instance from testing data corpus and
mutates it in a way that preserves the correctness of its
associated label. Then, it checks whether the correspond-
ing state vector is meaningfully diﬀerent from the previous
ones using a fast approximate nearest neighbor algorithm
based on a pre-speciﬁed distance. Each input data that is
relatively far from the existing nearest neighbor, is added
to the test cases set. The framework, entitled TensorFuzz,
is implemented to test TensorFlow-based neural network
models automatically. The eﬀectiveness of the proposed
testing approach was assessed using three known issues
in neural networks’ implementations. Results show that
TensorFuzz surpasses random search in : (1) ﬁnding NaNs
values in neural network models trained using numerical
instable cross-entropy loss, (2) generating divergences in
decision between the original model that is encoded with
32-bit ﬂoating point real and its quantized version that

is encoded with only 16-bit, and (3) surfacing undesirable
behavior in character level language RNN models.

Proof-based testing. Selsam et al.[39] proposed to for-
mally specify the computations of ML programs and con-
struct formal proofs of written theorems that deﬁne what
it means for the programs to be correct and error-free. Us-
ing the formal mathematical speciﬁcation of a ML program
and a machine-checkable proof of correctness representing
a formal certiﬁcate that can be veriﬁed by a stand-alone
machine without human intervention, ML developers are
able to ﬁnd errors in the code.
Using their proposed approach, Selsam et al. analyzed a
ML program designed for optimizing over stochastic com-
putation graphs, using the interactive proof assistant Lean,
and reported it to be bug-free. However, although this
approach allows to detect errors in the mathematical for-
mulation of the computations, it cannot help detect ex-
ecution errors due to numerical instabilities, such as the
replacement of real numbers by ﬂoating-point with limited
precision.

4. Conclusion and future research

The inductive nature of ML programs makes it diﬃ-
cult to reason about their behavior. Recently, researchers
have started to develop new testing techniques to help ML
developers detect debug and test ML programs.
In this
paper, we describe the generic process of a ML program
creation, from data preparation to the deployment of the
model in production, and explain the main sources of faults
in a ML program. Next, we review testing techniques pro-
posed in the literature to help detect these faults both at
the model and implementation levels; explaining the con-
text in which they can be applied as well as their expected
outcome. We also identify gaps in the literature related
to the testing of ML programs and suggest future research
directions for the scientiﬁc community. By consolidating
the progress made in testing ML programs in a single doc-
ument, we hope to equip ML and software engineering
communities with a reference document on which future
research eﬀorts can be built. Practitioners can also use
this paper to learn about existing testing techniques for
ML programs, which in turn can help improve the quality
of their ML programs.

References

[1] S. Krishnan, J. Wang, E. Wu, M. J. Franklin, K. Goldberg,
Activeclean: Interactive data cleaning while learning convex loss
models, arXiv preprint arXiv:1601.03797.

[2] Z. Qi, H. Wang, J. Li, H. Gao, Impacts of dirty data: and
experimental evaluation, arXiv preprint arXiv:1803.06071.
[3] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,
D. Ebner, V. Chaudhary, M. Young, J.-F. Crespo, D. Den-
nison, Hidden technical debt in machine learning systems, in:
Advances in neural information processing systems, 2015, pp.
2503–2511.

[4] D. Sculley, T. Phillips, D. Ebner, V. Chaudhary, M. Young, Ma-
chine learning: The high-interest credit card of technical debt.
[5] E. J. Weyuker, On testing non-testable programs, The Com-

puter Journal 25 (4) (1982) 465–470.

[6] S. Krishnan, J. Wang, M. J. Franklin, K. Goldberg, T. Kraska,
T. Milo, E. Wu, Sampleclean: Fast and reliable analytics on
dirty data., IEEE Data Eng. Bull. 38 (3) (2015) 59–75.

[7] S. Krishnan, M. J. Franklin, K. Goldberg, E. Wu, Boostclean:
Automated error detection and repair for machine learning,
arXiv preprint arXiv:1711.01299.

[8] N. Hynes, D. Sculley, M. Terry, The data linter: Lightweight,

automated sanity checking for ml data sets.

[9] S. Nidhra, J. Dondeti, Black box and white box testing
techniques-a literature review, International Journal of Embed-
ded Systems and Applications (IJESA) 2 (2) (2012) 29–50.
[10] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and
arXiv preprint

adversarial

examples

(2014),

harnessing
arXiv:1412.6572.

[11] L. Engstrom, D. Tsipras, L. Schmidt, A. Madry, A rotation and
a translation suﬃce: Fooling cnns with simple transformations,
arXiv preprint arXiv:1712.02779.

[12] J. Gilmer, R. P. Adams, I. Goodfellow, D. Andersen, G. E.
Dahl, Motivating the rules of the game for adversarial example
research, arXiv preprint arXiv:1807.06732.

[13] S. Gu, L. Rigazio, Towards deep neural network architectures
robust to adversarial examples, arXiv preprint arXiv:1412.5068.
[14] S.-M. Moosavi-Dezfooli, A. Fawzi, P. Frossard, Deepfool: a sim-
ple and accurate method to fool deep neural networks, in: Pro-
ceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, 2016, pp. 2574–2582.

[15] K. Pei, Y. Cao, J. Yang, S. Jana, Deepxplore: Automated white-
box testing of deep learning systems, in: Proceedings of the 26th
Symposium on Operating Systems Principles, ACM, 2017, pp.
1–18.

[16] W. M. McKeeman, Diﬀerential testing for software, Digital

Technical Journal 10 (1) (1998) 100–107.

[17] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen,
T. Su, L. Li, Y. Liu, et al., Deepgauge: multi-granularity testing
criteria for deep learning systems, in: Proceedings of the 33rd
ACM/IEEE International Conference on Automated Software
Engineering, ACM, 2018, pp. 120–131.

[18] A. Kurakin, I. Goodfellow, S. Bengio, Adversarial examples in

the physical world, arXiv preprint arXiv:1607.02533.

[19] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Ce-
lik, A. Swami, The limitations of deep learning in adversarial
settings, in: Security and Privacy (EuroS&P), 2016 IEEE Eu-
ropean Symposium on, IEEE, 2016, pp. 372–387.

[20] Y. Tian, K. Pei, S. Jana, B. Ray, Deeptest: Automated testing
of deep-neural-network-driven autonomous cars, in: Proceed-
ings of the 40th International Conference on Software Engineer-
ing, ACM, 2018, pp. 303–314.

[21] T. Y. Chen, S. C. Cheung, S. M. Yiu, Metamorphic testing: a
new approach for generating next test cases, Tech. rep., Techni-
cal Report HKUST-CS98-01, Department of Computer Science,
Hong Kong University of Science and Technology, Hong Kong
(1998).

[22] M. Zhang, Y. Zhang, L. Zhang, C. Liu, S. Khurshid, Deep-
road: Gan-based metamorphic autonomous driving system test-
ing, arXiv preprint arXiv:1802.02295.

[23] L. Ma, F. Zhang, M. Xue, B. Li, Y. Liu, J. Zhao, Y. Wang,
Combinatorial testing for deep learning systems, arXiv preprint
arXiv:1806.07723.

[24] C. Nie, H. Leung, A survey of combinatorial testing, ACM Com-

puting Surveys (CSUR) 43 (2) (2011) 11.

[25] IBM, Ibm cplex mathematical programming solver for linear

programming.
URL https://www.ibm.com/analytics/cplex-optimizer
[26] Y. Sun, X. Huang, D. Kroening, Testing deep neural networks,

arXiv preprint arXiv:1803.04792.

[27] K. J. Hayhurst, D. S. Veerhusen, J. J. Chilenski, L. K. Rierson,

A practical tutorial on modiﬁed condition/decision coverage.

14

[28] Y. Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska,
D. Kroening, Concolic testing for deep neural networks, arXiv
preprint arXiv:1805.00089.

[29] K. Sen, D. Marinov, G. Agha, Cute: a concolic unit testing
engine for c, in: ACM SIGSOFT Software Engineering Notes,
Vol. 30, ACM, 2005, pp. 263–272.

[30] A. Karpathy, Cs231n: Convolutional neural networks for visual

recognition (2018).
URL http://cs231n.github.io/neural-networks-3/

[31] R. B. Grosse, D. K. Duvenaud, Testing mcmc code, arXiv

preprint arXiv:1412.5218.

[32] C. Murphy, G. E. Kaiser, L. Hu, Properties of machine learning

applications for use in metamorphic testing.

[33] C. Murphy, G. E. Kaiser, M. Arias, An approach to software
testing of machine learning applications., in: SEKE, Vol. 167,
2007.

[34] X. Xie, J. W. Ho, C. Murphy, G. Kaiser, B. Xu, T. Y. Chen,
Testing and validating machine learning classiﬁers by metamor-
phic testing, Journal of Systems and Software 84 (4) (2011)

544–558.

[35] A. Dwarakanath, M. Ahuja, S. Sikand, R. M. Rao, R. Bose,
N. Dubash, S. Podder, Identifying implementation bugs in ma-
chine learning based image classiﬁers using metamorphic test-
ing, in: Proceedings of the 27th ACM SIGSOFT International
Symposium on Software Testing and Analysis, ACM, 2018, pp.
118–128.

[36] L. Ma, F. Zhang, J. Sun, M. Xue, B. Li, F. Juefei-Xu, C. Xie,
L. Li, Y. Liu, J. Zhao, et al., Deepmutation: Mutation testing
of deep learning systems, arXiv preprint arXiv:1805.05206.
[37] Y. Jia, M. Harman, An analysis and survey of the development
of mutation testing, IEEE transactions on software engineering
37 (5) (2011) 649–678.

[38] A. Odena,

I. Goodfellow, Tensorfuzz: Debugging neu-
ral networks with coverage-guided fuzzing, arXiv preprint
arXiv:1807.10875.

[39] D. Selsam, P. Liang, D. L. Dill, Developing bug-free ma-
chine learning systems with formal mathematics, arXiv preprint
arXiv:1706.08605.

15

