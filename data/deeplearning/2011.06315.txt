Biomedical Named Entity Recognition at Scale

Veysel Kocaman
John Snow Labs Inc.
16192 Coastal Highway
Lewes, DE , USA 19958
veysel@johnsnowlabs.com

David Talby
John Snow Labs Inc.
16192 Coastal Highway
Lewes, DE , USA 19958
david@johnsnowlabs.com

0
2
0
2

v
o
N
2
1

]
L
C
.
s
c
[

1
v
5
1
3
6
0
.
1
1
0
2
:
v
i
X
r
a

Abstract—Named entity recognition (NER) is a widely appli-
cable natural language processing task and building block of
question answering, topic modeling, information retrieval, etc.
In the medical domain, NER plays a crucial role by extracting
meaningful chunks from clinical notes and reports, which are
then fed to downstream tasks like assertion status detection, entity
resolution, relation extraction, and de-identiﬁcation. Reimple-
menting a Bi-LSTM-CNN-Char deep learning architecture on top
of Apache Spark, we present a single trainable NER model that
obtains new state-of-the-art results on seven public biomedical
benchmarks without using heavy contextual embeddings like
BERT. This includes improving BC4CHEMD to 93.72% (4.1%
gain), Species800 to 80.91% (4.6% gain), and JNLPBA to 81.29%
(5.2% gain). In addition, this model is freely available within
a production-grade code base as part of the open-source Spark
NLP library; can scale up for training and inference in any Spark
cluster; has GPU support and libraries for popular programming
languages such as Python, R, Scala and Java; and can be extended
to support other human languages with no code changes.

I. INTRODUCTION

Electronic health records (EHRs) are the primary source of
information for clinicians tracking the care of their patients.
Information fed into these systems may be found in structured
ﬁelds for which values are inputted electronically (e.g. labo-
ratory test orders or results) Liede et al. [2015] but most of
the time information in these records is unstructured making it
largely inaccessible for statistical analysis Murdoch and Detsky
[2013]. These records include information such as the reason
for administering drugs, previous disorders of the patient or
the outcome of past treatments, and they are the largest source
of empirical data in biomedical research, allowing for major
scientiﬁc ﬁndings in highly relevant disorders such as cancer
and Alzheimer’s disease Perera et al. [2014]. Unlocking this
information can bring a signiﬁcant advancement to biomedical
research.

The widespread adoption of EHRs and the growing wealth
of digitized information sources about patients are opening
new doors to uncover previously unidentiﬁed associations and
accelerating knowledge discovery via state-of-the-art Machine
Learning (ML) algorithms and new statistical methods. Due to
innate obstacles in extracting information from unstructured
text data and the high level of preciseness dictated in healthcare
domain, manual abstraction has been prevalent in the industry.
As the manual abstraction is highly expensive, time consuming
and error prone process, there has been a growing trend in
natural language processing (NLP) applications in clinical and

biomedical domain to automate the abstraction process as well
as making the EHR data available through high-performant
and fail-safe pipelines.

As the key ingredient of any NLP system, named entity
recognition (NER) is regarded as the ﬁrst building block of
question answering, topic modelling, information retrieval,
etc Yadav and Bethard [2019]. In the medical domain, NER
plays the most crucial role by giving out the ﬁrst meaningful
chunks of a clinical note, and then feeding them as an input
to the subsequent downstream tasks such as clinical assertion
status Uzuner et al. [2011], clinical entity resolvers Tzitzivacos
[2007] and de-identiﬁcation of the sensitive data Uzuner et al.
[2007]. However, segmentation of clinical and drug entities is
considered to be a difﬁcult task in biomedical NER systems
because of complex orthographic structures of named entities
Liu et al. [2015].

ML methods formulate the clinical NER task as a sequence
labeling problem that aims to ﬁnd the best label sequence
(e.g., BIO format labels) for a given input sequence (individual
words from clinical text) Wu et al. [2017]. Many top-ranked
NER systems applied the Conditional Random Fields (CRFs)
model Lafferty et al. [2001], which is the most popular
solution among conventional ML algorithms. A typical state-
of-the-art clinical NER system usually utilizes features from
different linguistic levels, including orthographic information
(e.g., capitalization of letters, preﬁx and sufﬁx), syntactic
information (e.g. POS tags), word n-grams, word embeddings,
and semantic information (e.g., the UMLS concept unique
identiﬁer) Wu et al. [2017]. These features are usually utilized
in LSTM Hochreiter and Schmidhuber [1997] based neural
network frameworks Huang et al. [2015], Chiu and Nichols
[2016], Ma and Hovy [2016] and gained popularity among
researchers due to their effectiveness of modeling the sequential
patterns.

In the last few months, pretraining large neural language
models and rich contextual embeddings, such as BERT Devlin
et al. [2018] and ELMO Peters et al. [2018], have also
led to impressive gains on NER systems and many clinical
variants of BERT models such as BioBert LEE et al. [2019],
ClinicalBert Alsentzer et al. [2019], BlueBert Peng et al.
[2019], SciBert Beltagy et al. [2019] and PubmedBert Gu et al.
[2020] have been crafted to address biomedical and clinical
NER tasks with state-of-the-art results. However, since these
methods require signiﬁcant computational resources during both

 
 
 
 
 
 
pretraining and getting prediction, using them in production
is impractical under the restricted computational resources
compared to classical pretrained embeddings (e.g. Glove). A
recent study Arora et al. [2020] empirically shows that classical
pretrained embeddings can match contextual embeddings on
industry-scale data, and often perform within 5 to 10% accuracy
(absolute) on benchmark tasks.

Despite the growing interest and all these ground breaking
advances in NER systems, easy to use production ready models
and tools are scarce and it is one of the major obstacles for
clinical NLP researchers to implement the latest algorithms
into their workﬂow and start using immediately. On the other
hand, NLP tool kits specialized for processing biomedical
and clinical text, such as MetaMap Aronson and Lang [2010]
and cTAKES Savova et al. [2010] typically do not make use
of new research innovations such as word representations or
neural networks discussed above, hence producing less accurate
results Zhang et al. [2020], Neumann et al. [2019]. In the
last year, two new libraries, Stanza Zhang et al. [2020] and
SciSpacy Neumann et al. [2019] took the stage to ﬁnd a solution
to the issues discussed above and released Python-based, de
facto language of data science, production grade libraries.
Both libraries offer out of the box clinical and biomedical
pretrained NER models utilizing state-of-the-art deep learning
frameworks mentioned above. However, none of these libraries
or tools can scale up in clusters in terms of distributed data
processing principles and do not support in-memory distributed
data processing solutions such as Spark.

In this study, we show through extensive experiments that
our NER module in Spark NLP library, one of the most
widely used NLP libraries in industry, exceeds the biomedical
NER benchmarks reported by Stanza in 7 out of 8 benchmark
datasets and in every dataset reported by SciSpacy. Using the
modiﬁed version of the well known BiLSTM-CNN-Char NER
architecture Chiu and Nichols [2016] into Spark environment,
Spark NLP’s NER module can also be extended to other spoken
languages with zero code changes and can scale up in Spark
clusters.

The speciﬁc novel contributions of this paper are the

following:

• Delivering the ﬁrst production-grade scalable NER model

implementation.

• Delivering a state-of-the-art NER model that exceeds
the biomedical NER benchmarks reported by Stanza and
SciSpaCy.

• Comparing the effectiveness of domain speciﬁc clinical
word embeddings with general purpose GloVe embeddings
inside the same NER architecture.

• Explaining the NER model implementation in Spark NLP
which is the only NLP library that can scale up in Spark
clusters while supporting popular programming languages
(Python, R, Scala and Java).

The remainder of the paper is organized as follows: Section II
introduces Spark NLP and explains the NER model framework
implemented in Spark NLP. Section III elaborates the imple-
mentation details, datasets and settings for our experiments

and presents results for Spark NLP, Stanza and SciSpacy on
the same benchmark datasets. Section IV concludes this paper
by pointing out key points and future directions.

II. NER MODEL IMPLEMENTATION IN SPARK NLP

The deep neural network architecture for NER model in
Spark NLP is BiLSTM-CNN-Char framework, a slightly mod-
iﬁed version of the architecture proposed by Chiu et.al. Chiu
and Nichols [2016]. It is a neural network architecture that
automatically detects word and character-level features using a
hybrid bidirectional LSTM and CNN architecture, eliminating
the need for most feature engineering steps.

In the original framework, the CNN extracts a ﬁxed length
feature vector from character-level features. For each word,
these vectors are concatenated and fed to the BLSTM network
and then to the output layers. They employed a stacked
bi-directional recurrent neural network with long short-term
memory units to transform word features into named entity
tag scores. The extracted features of each word are fed into a
forward LSTM network and a backward LSTM network. The
output of each network at each time step is decoded by a linear
layer and a log-softmax layer into log-probabilities for each tag
category. These two vectors are then simply added together to
produce the ﬁnal output Chiu and Nichols [2016]. The detailed
architecture of the proposed framework in the original paper is
illustrated at Figure 1. In sum, 50-dimensional pretrained word
embeddings is used for word features, 25-dimension character
embeddings is used for char features, and capitalization features
(allCaps, upperInitial, lowercase, mixedCaps, noinfo) are used
for case features. They also made use of lexicons as a form of
external knowledge as proposed in Ratinov and Roth [2009].

Fig. 1: Overview of the original BiLSTM-CNN-Char architec-
ture Chiu and Nichols [2016].

In Spark NLP, we modiﬁed this framework as follows:

• Habibi et al. [2017] compared the performance of LSTM-
CRF approach on 33 data sets covering ﬁve different
entity classes with that of best-of-class NER tools and
an entity-agnostic CRF implementation. On average, F1-
score of LSTM-CRF is 5% above that of the baselines,
using WikiPubMed-PMC word embeddings.
Using a similar neural network architecture, we trained
our own biomedical word embeddings with skip-gram
model on PubMed abstracts and case studies, as described
in Mikolov et al. [2013], for learning distributed represen-
tations of words using contextual information. The trained
word embeddings has 200-dimensions and a vocabulary
size of 2.2 million. In order to compare the effectiveness of
this embeddings, we also used 300-dimension pretrained
GloVe embeddings with 6 billion tokens, trained on
Wikipedia and Gigaword-5 dataset Pennington et al.
[2014]. Both embeddings are ported into Spark through
an annotator concept speciﬁcally designed for Spark NLP.
The average word coverage of our implementation of
domain speciﬁc word embeddings (we call it Spark-
Biomedical embeddings in this study) is 99.5% and
the coverage of Glove6B embeddings is 96.1% on the
biomedical datasets used in this study (see Table I).
• Even though better results were reported by Ghaddar
and Langlais [2018] through robust lexical features, after
experimenting with different parameters and components,
we decided to remove lexical features in order to reduce
the complexity and relied on pretrained biomedical em-
beddings, casing features and char features through CNN.
As sentences are represented through 2 nested sequences
(words & chars), a CNN is applied in a way that each
character is embedded in a character embedding matrix,
of dimension 25. Then, a 1D Convolution layer processes
the sequence of embedded char vectors, followed by a
MaxPooling operation. This way, each word gets a vector
representation. We used 25 ﬁlters and kernel size of 3. It
is worth to mention that char features are proved to be
highly useful in NER models and had provided a level of
immunity to typos and spelling errors.

• We built a modiﬁed version of the framework Chiu and
Nichols [2016] in Tensorﬂow (TF) and used LSTM-
BlockFusedCell. This is an extremely efﬁcient LSTM
implementation based on Zaremba et al. [2014], that
uses a single TF operation for the entire LSTM. Our
experiments show that it is both faster and more memory-
efﬁcient than LSTMBlockCell. Then we implemented this
framework in Scala using TensorFlow API. This setup is
ported into Spark and let the driver node run the entire
training using all the available cores on the driver node.
We also added CuDA version of each TF component to
be able to train our models on GPU when available.
Due to architectural design choices by Tensorﬂow imple-
mentation in JVM at the time of writing this paper, distributing
the model
training over the worker nodes in the cluster
was not viable and effective, and putting the burden of

entire training process on the driver node mandated some
limitations in terms of training speed and computational
resources. Nevertheless, being able to get predictions on scale
from voluminous data with state-of-the-art accuracy would
overwhelm the aforementioned disadvantage.

III. IMPLEMENTATION DETAILS AND EXPERIMENTAL
RESULTS

In this section, we describe the datasets, evaluation metrics,

and provide an overview of experimental setup.

A. Datasets

In this study, we trained individual NER models on 8 publicly
available biomedical NER datasets provided by Wang et al.
[2019]: AnatEM Pyysalo and Ananiadou [2014], BC5CDR
Li et al. [2016], BC4CHEMD Krallinger et al. [2015],
BioNLP13CG Pyysalo et al. [2015], JNLPBA Kim et al.
[2004], Linnaeus Gerner et al. [2010], NCBI-Disease Do˘gan
et al. [2014] and S800 Paﬁlis et al. [2013]. These models
cover a wide variety of entity types in domains ranging from
anatomical analysis to genetics and cellular biology. For the
sake of brevity, we didn’t include details about the nature of
the data sets and readers can refer to cited papers for more
information. We trained several other clinical and biomedical
NER models in Spark NLP, but we just report metrics on these
8 biomedical data sets as Stanza and SciSpacy also reported
their benchmarks on these data sets that are freely available
without any restrictions.

B. Overview of Experimental Setup

Biomedical NER datasets provided by Wang et al. [2019]
are already in BIO and BIOES schemes for encoding entity
annotations as token tags. IOB (or BIO) stands for Begin,
Inside and Outside. Words tagged with O are outside of named
entities and the I-XXX tag is used for words inside a named
entity of type XXX. Whenever two entities of type XXX are
immediately next to each other, the ﬁrst word of the second
entity will be tagged B-XXX to highlight that it starts another
entity. On the other hand, BIOES (also known as BIOLU) is
a little bit sophisticated annotation method that distinguishes
between the end of a named entity and single entities. BIOES
stands for Begin, Inside, Outside, End, Single. In this scheme,
for example, a word describing a gene entity is tagged with
“B-Gene” if it is at the beginning of the entity, “I-Gene” if
it is in the middle of the entity, and “E-Gene” if it is at the
end of the entity. Single-word gene entities are tagged with
“S-Gene”. All other words not describing entities of interest
are tagged as ‘O’.

BIOES scheme was also used in the original implementation
of our NER architecture and considerable performance improve-
ments over BIO are reported Chiu and Nichols [2016]. Ratinov
and Roth [2009] also showed that the minimal BIO scheme
was more difﬁcult to learn than the BIOES scheme, which
explicitly marks boundary tokens. However, we experienced
various performance issues when we used BIOES schema
(converging very fast in the early epochs but then fail to

TABLE I: Word embeddings coverage ratios on biomedical datasets. Our domain speciﬁc embeddings have near-perfect word
coverages. The average word coverage of our implementation of domain speciﬁc word embeddings (we call it Spark-Biomedical
Embeddings in this study) is 99.5% and the average word coverage of Glove6B embeddings is 96.1% on the biomedical datasets
used in this study)

Dataset

NBCI-Disease
BC5CDR
BC4CHEMD
Linnaeus
Species800
JNLPBA
AnatEM
BioNLP-CG

Spark-Biomedical Embeddings

Spark-Glove6B Embeddings

Training set

99.700
99.171
99.571
99.162
99.350
99.530
99.580
99.859

Test set

99.695
99.106
99.551
99.181
99.345
99.496
99.623
99.814

Training set

96.703
96.059
96.409
96.801
95.909
92.566
96.992
97.750

Test set

96.710
95.795
96.434
96.867
96.258
92.690
96.945
96.663

generalize further and stuck at local minima), and then decided
to use BIO scheme.

In terms of hyperparameter tuning, we run experiments
by tuning the hyperparamaters with the following parameter
ranges through Random Search Bergstra and Bengio [2012]
and found out that the following parameters would produce
the best results (ﬁgures within the parenthesis represent the
parameter ranges tested):

• LSTM state size: 200 (200, 250)
• Dropout rate: 0.5 (0.3, 0.7)
• Batch size: 8 (4, 256)
• Learning rate: 0.001 (0.01, 0.0003)
• Epoch: 10-15 (10, 100)
• Optimizer: Adam
• Learning rate decay coefﬁcient (po) (real learning rate =
lr / (1 + po * epoch) Smith [2018] : 0.005 (0.001, 0.01))

C. Experiment Results

We run our experiments on Colab1 server provided by Google
(2vCPU @ 2.2GHz, 13GB RAM) and used Apache Spark in
local mode (no cluster). We present our results at Table II and
Figure 2. As the only NLP library that scales up for training and
inference in any Spark cluster, Spark NLP NER architecture
obtains new state-of-the-art results on seven public biomedical
benchmarks without using heavy contextual embeddings like
BERT. This includes improving BC4CHEMD to 93.72% (4.1%
gain), Species800 to 80.91% (4.6% gain), and JNLPBA to
81.29% (5.2% gain). Given that Stanza already claims that
its NER performance is on par with or superior to the strong
performance achieved by BioBERT, our proposed NER model
can get better results despite using considerably more compact
model. Moreover, this model is available within a production-
grade code base as part of the open-source Spark NLP library
and a new NER model can be trained with a single line of
code as presented in Appendix A.

As you can see on the leaderboard given at Table III, our
NER model with pretrained biomedical embeddings produces
better results than Stanza in 7 out of 8 biomedical datasets and

1https://colab.research.google.com/

exceeds SciSpacy in all the benchmarks. It is also surprising
to see that our NER model with GloVe6B embeddings, despite
being a general purpose embeddings, can also exceed Stanza’s
(also using domain speciﬁc embeddings, CharLM - character-
level language model Akbik et al. [2018]) benchmarks in half
of the benchmarks and again exceeds SciSpacy in all the
benchmarks.

IV. CONCLUSION

Despite the growing interest and ground breaking advances
in NLP research and NER systems, easy to use production
ready models and tools are scarce in Biomedical domain and
it is one of the major obstacles for clinical NLP researchers to
implement the latest algorithms into their workﬂow and start
using immediately.

In this study, we show through extensive experiments that
NER module in Spark NLP library, one of the most widely
used NLP libraries in industry, exceeds the biomedical NER
benchmarks reported by Stanza in 7 out of 8 benchmark datasets
and in every dataset reported by SciSpacy without using heavy
contextual embeddings like BERT. Using the modiﬁed version
of the well known BiLSTM-CNN-Char NER architecture Chiu
and Nichols [2016] into Spark environment, we also presented
that even with a general purpose GloVe embeddings (GloVe6B)
and with no lexical features, we were able to achieve state-of-
the-art results in biomedical domain and produces better results
than Stanza in 4 out of 8 benchmark datasets. Given that Stanza
also uses domain speciﬁc clinical embeddings, exceeding its
benchmarks with general purpose embeddings is also another
important observation.

Spark NLP’s NER module can also be extended to other
spoken languages with zero code changes and can scale up
in Spark clusters. In addition, this model is available within a
production-grade code base as part of the Spark NLP library;
can scale up for training and inference in any Spark cluster; has
GPU support and libraries for popular programming languages
such as Python, R, Scala and Java; and is already extended to
support other human languages with no code changes.

TABLE II: NER performance across different datasets in the biomedical domain. All scores reported are micro-averaged test F1
excluding O’s. Stanza results are from the paper reported in Zhang et al. [2020], SciSpaCy results are from the scispacy-medium
models reported in Neumann et al. [2019]. The ofﬁcial training and validation sets are merged and used for training and then
the models are evaluated on the original test sets. For reproducibility purposes, we use the preprocessed versions of these
datasets provided by Wang et al. [2019] and also used by Stanza. Spark-x preﬁx in the table indicates our implementation.
Bold scores represent the best scores in the respective row.

Dataset

Entities

Spark - Biomedical

Spark - GloVe 6B

Stanza

SciSpacy

NBCI-Disease
BC5CDR
BC4CHEMD
Linnaeus
Species800
JNLPBA
AnatEM
BioNLP13-CG

Disease
Chemical, Disease
Chemical
Species
Species
5 types in cellular
Anatomy
16 types in Cancer Genetics

89.13
89.73
93.72
86.26
80.91
81.29
89.13
85.58

87.19
88.32
92.32
85.51
79.22
79.78
87.74
84.3

87.49
88.08
89.65
88.27
76.35
76.09
88.18
84.34

81.65
83.92
84.55
81.74
74.06
73.21
84.14
77.6

Fig. 2: NER performance across different biomedical benchmark datasets. Our implementation of NER model with domain
speciﬁc embeddings exceeds Stanza in 7 out of 8 datasets and exceeds SciSpacy in all the benchmarks. The same implementation
with general purpose GloVe embeddings is also better than SciSpacy in every dataset and exceeds Stanza in 4 out of 8 datasets.

REFERENCES

Alexander Liede, Rohini K Hernandez, Maayan Roth, Geoffrey
Calkins, Katherine Larrabee, and Leo Nicacio. Validation
of international classiﬁcation of diseases coding for bone
metastases in electronic health records using technology-
enabled abstraction. Clinical epidemiology, 7:441, 2015.
Travis B Murdoch and Allan S Detsky. The inevitable
application of big data to health care. Jama, 309(13):1351–
1352, 2013.

Gayan Perera, Mizanur Khondoker, Matthew Broadbent,
Gerome Breen, and Robert Stewart. Factors associated

with response to acetylcholinesterase inhibition in dementia:
a cohort study from a secondary mental health care case
register in london. PloS one, 9(11):e109484, 2014.

Vikas Yadav and Steven Bethard. A survey on recent advances
in named entity recognition from deep learning models.
arXiv preprint arXiv:1910.11470, 2019.

Özlem Uzuner, Brett R South, Shuying Shen, and Scott L
DuVall. 2010 i2b2/va challenge on concepts, assertions, and
relations in clinical text. Journal of the American Medical
Informatics Association, 18(5):552–556, 2011.

D Tzitzivacos.

International classiﬁcation of diseases 10th
edition (icd-10):: main article. CME: Your SA Journal of

TABLE III: Biomedical NER benchmarks leaderboard. Spark-x preﬁx indicates our implementation.

Dataset

Model

Score Model

Score Model

Best

2nd Best

3rd Best

NBCI-Disease
BC5CDR
BC4CHEMD
Linnaeus
Species800
AnatEM
BioNLP-CG

Spark-Biomedical
Spark-Biomedical
Spark-Biomedical
Stanza
Spark-Biomedical
Spark-Biomedical
Spark-Biomedical

89.13
89.73
93.72
88.27
81.29
89.13
85.58

Stanza
Spark-GloVe6B
Spark-GloVe6B
Spark-Biomedical
Spark-GloVe6B
Stanza
Stanza

87.49
88.32
92.32
86.26
79.78
88.18
84.34

Spark-GloVe6B
Stanza
Stanza
Spark-GloVe6B
Stanza
Spark-GloVe6B
Spark-GloVe6B

Score

87.19
88.08
89.65
85.51
76.09
87.74
84.3

CPD, 25(1):8–10, 2007.

Özlem Uzuner, Yuan Luo, and Peter Szolovits. Evaluating the
state-of-the-art in automatic de-identiﬁcation. Journal of the
American Medical Informatics Association, 14(5):550–563,
2007.

Shengyu Liu, Buzhou Tang, Qingcai Chen, and Xiaolong Wang.
Effects of semantic features on machine learning-based drug
name recognition systems: word embeddings vs. manually
constructed dictionaries. Information, 6(4):848–865, 2015.
Yonghui Wu, Min Jiang, Jun Xu, Degui Zhi, and Hua Xu.
Clinical named entity recognition using deep learning models.
In AMIA Annual Symposium Proceedings, volume 2017, page
1812. American Medical Informatics Association, 2017.
John Lafferty, Andrew McCallum, and Fernando CN Pereira.
Conditional random ﬁelds: Probabilistic models for segment-
ing and labeling sequence data. 2001.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term

memory. Neural computation, 9(8):1735–1780, 1997.

Zhiheng Huang, Wei Xu, and Kai Yu.
lstm-crf models for sequence tagging.
arXiv:1508.01991, 2015.

Bidirectional
arXiv preprint

Jason PC Chiu and Eric Nichols. Named entity recognition
with bidirectional lstm-cnns. Transactions of the Association
for Computational Linguistics, 4:357–370, 2016.

Xuezhe Ma and Eduard Hovy. End-to-end sequence la-
arXiv preprint

lstm-cnns-crf.

beling via bi-directional
arXiv:1603.01354, 2016.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
arXiv preprint
formers for
arXiv:1810.04805, 2018.

language understanding.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
Christopher Clark, Kenton Lee, and Luke Zettlemoyer.
Deep contextualized word representations. arXiv preprint
arXiv:1802.05365, 2018.

J LEE, W YOON, S KIM, D KIM, and S KIM. So ch &
kang j.(2019). biobert: a pretrained biomedical language
representation model for biomedical text mining. arXiv
preprint arXiv:1901.08746, 2019.

Publicly available clinical bert embeddings. arXiv preprint
arXiv:1904.03323, 2019.

Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning
in biomedical natural language processing: An evaluation of
bert and elmo on ten benchmarking datasets. arXiv preprint
arXiv:1906.05474, 2019.

Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pre-
trained language model for scientiﬁc text. arXiv preprint
arXiv:1903.10676, 2019.

Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto
Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao,
and Hoifung Poon. Domain-speciﬁc language model pre-
training for biomedical natural language processing. arXiv
preprint arXiv:2007.15779, 2020.

Simran Arora, Avner May, Jian Zhang, and Christopher Ré.
Contextual embeddings: When are they worth it? arXiv
preprint arXiv:2005.09117, 2020.

Alan R Aronson and François-Michel Lang. An overview of
metamap: historical perspective and recent advances. Journal
of the American Medical Informatics Association, 17(3):229–
236, 2010.

Guergana K Savova, James J Masanz, Philip V Ogren, Ji-
aping Zheng, Sunghwan Sohn, Karin C Kipper-Schuler,
and Christopher G Chute. Mayo clinical text analysis
and knowledge extraction system (ctakes): architecture,
Journal of the
component evaluation and applications.
American Medical Informatics Association, 17(5):507–513,
2010.

Yuhao Zhang, Yuhui Zhang, Peng Qi, Christopher D Manning,
and Curtis P Langlotz. Biomedical and clinical english model
packages in the stanza python nlp library. arXiv preprint
arXiv:2007.14640, 2020.

Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar.
Scispacy: Fast and robust models for biomedical natural
language processing. arXiv preprint arXiv:1902.07669, 2019.
Lev Ratinov and Dan Roth. Design challenges and miscon-
ceptions in named entity recognition. In Proceedings of the
Thirteenth Conference on Computational Natural Language
Learning (CoNLL-2009), pages 147–155, 2009.

Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung
Weng, Di Jin, Tristan Naumann, and Matthew McDermott.

Maryam Habibi, Leon Weber, Mariana Neves, David Luis
Wiegandt, and Ulf Leser. Deep learning with word em-

James Bergstra and Yoshua Bengio. Random search for hyper-
parameter optimization. The Journal of Machine Learning
Research, 13(1):281–305, 2012.

Leslie N Smith. A disciplined approach to neural network hyper-
parameters: Part 1–learning rate, batch size, momentum, and
weight decay. arXiv preprint arXiv:1803.09820, 2018.
Alan Akbik, Duncan Blythe, and Roland Vollgraf. Contextual
string embeddings for sequence labeling. In Proceedings
of the 27th International Conference on Computational
Linguistics, pages 1638–1649, 2018.

APPENDIX

from pyspark.ml import Pipeline
import sparknlp
from sparknlp.training import CoNLL
from sparknlp.annotator import *

spark = sparknlp.start()

training_data = CoNLL().readDataset(spark, ’

BC5CDR_train.conll’)

word_embedder = WordEmbeddings.pretrained(’

wikiner_6B_300’, ’xx’) \

.setInputCols(["sentence",’token’])\
.setOutputCol("embeddings")

nerTagger = NerDLApproach()\

.setInputCols(["sentence", "token", "embeddings"])

\

.setLabelColumn("label")\
.setOutputCol("ner")\
.setMaxEpochs(10)\
.setDropout(0.5)\
.setLr(0.001)\
.setPo(0.005)\
.setBatchSize(8)\
.setValidationSplit(0.2)\

pipeline = Pipeline(

stages = [
word_embedder,
nerTagger

])

ner_model = pipeline.fit(training_data)

beddings improves biomedical named entity recognition.
Bioinformatics, 33(14):i37–i48, 2017.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
Efﬁcient estimation of word representations in vector space.
arXiv preprint arXiv:1301.3781, 2013.

Jeffrey Pennington, Richard Socher, and Christopher D Man-
In
ning. Glove: Global vectors for word representation.
Proceedings of the 2014 conference on empirical methods
in natural language processing (EMNLP), pages 1532–1543,
2014.

Abbas Ghaddar and Philippe Langlais. Robust lexical features
for improved neural network named-entity recognition. arXiv
preprint arXiv:1806.03489, 2018.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Re-
arXiv preprint

current neural network regularization.
arXiv:1409.2329, 2014.

Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka
Zitnik, Jingbo Shang, Curtis Langlotz, and Jiawei Han. Cross-
type biomedical named entity recognition with deep multi-
task learning. Bioinformatics, 35(10):1745–1752, 2019.
Sampo Pyysalo and Sophia Ananiadou. Anatomical entity
mention recognition at literature scale. Bioinformatics, 30
(6):868–875, 2014.

Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-
Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J
Mattingly, Thomas C Wiegers, and Zhiyong Lu. Biocreative
v cdr task corpus: a resource for chemical disease relation
extraction. Database, 2016, 2016.

Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel
Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan
Lu, Donghong Ji, Daniel M Lowe, et al. The chemdner
corpus of chemicals and drugs and its annotation principles.
Journal of cheminformatics, 7(1):1–17, 2015.

Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Andrew Rowley,
Hong-Woo Chun, Sung-Jae Jung, Sung-Pil Choi, Jun’ichi
Tsujii, and Sophia Ananiadou. Overview of the cancer
genetics and pathway curation tasks of bionlp shared task
2013. BMC bioinformatics, 16(S10):S2, 2015.

Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka
Tateisi, and Nigel Collier.
Introduction to the bio-entity
recognition task at jnlpba. In Proceedings of the interna-
tional joint workshop on natural language processing in
biomedicine and its applications, pages 70–75. Citeseer,
2004.

Martin Gerner, Goran Nenadic, and Casey M Bergman. Lin-
naeus: a species name identiﬁcation system for biomedical
literature. BMC bioinformatics, 11(1):85, 2010.

Rezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong Lu. Ncbi
disease corpus: a resource for disease name recognition and
concept normalization. Journal of biomedical informatics,
47:1–10, 2014.

Evangelos Paﬁlis, Sune P Frankild, Lucia Fanini, Sarah Faulwet-
ter, Christina Pavloudi, Aikaterini Vasileiadou, Christos
Arvanitidis, and Lars Juhl Jensen. The species and organisms
resources for fast and accurate identiﬁcation of taxonomic
names in text. PloS one, 8(6):e65390, 2013.

