2
2
0
2

g
u
A
4

]

V
C
.
s
c
[

2
v
6
7
8
0
1
.
0
1
1
2
:
v
i
X
r
a

Evolving Transferable Neural Pruning Functions

Yuchen Liu
Princeton University
yl16@princeton.edu

S.Y. Kung
Princeton University
kung@princeton.edu

David Wentzlaï¬€
Princeton University
wentzlaf@princeton.edu

ABSTRACT
Structural design of neural networks is crucial for the success of
deep learning. While most prior works in evolutionary learning
aim at directly searching the structure of a network, few attempts
have been made on another promising track, channel pruning, which
recently has made major headway in designing eï¬ƒcient deep learn-
ing models. In fact, prior pruning methods adopt human-made prun-
ing functions to score a channelâ€™s importance for channel pruning,
which requires domain knowledge and could be sub-optimal. To
this end, we pioneer the use of genetic programming (GP) to dis-
cover strong pruning metrics automatically. Speciï¬cally, we craft
a novel design space to express high-quality and transferable prun-
ing functions, which ensures an end-to-end evolution process where
no manual modiï¬cation is needed on the evolved functions for
their transferability after evolution. Unlike prior methods, our ap-
proach can provide both compact pruned networks for eï¬ƒcient
inference and novel closed-form pruning metrics which are math-
ematically explainable and thus generalizable to diï¬€erent pruning
tasks. While the evolution is conducted on small datasets, our func-
tions shows promising results when applied to more challenging
datasets, diï¬€erent from those used in the evolution process. For
example, on ILSVRC-2012, an evolved function achieves state-of-
the-art pruning results.

KEYWORDS
Convolutional Neural Network, Channel Pruning, Eï¬ƒcient Deep
Learning, Genetic Programming

ACM Reference Format:
Yuchen Liu, S.Y. Kung, and David Wentzlaï¬€. 2022. Evolving Transferable
Neural Pruning Functions. In Genetic and Evolutionary Computation Con-
ference (GECCO â€™22), July 9â€“13, 2022, Boston, MA, USA. ACM, Boston, MA,
USA, 12 pages. https://doi.org/10.1145/3512290.3528694

1 INTRODUCTION
Convolutional neural networks (CNNs) have demonstrated supe-
rior performance on various computer vision tasks [12, 13, 23, 64].
However, CNNs require huge storage space, high computational
budget, and large memory utilization, which could far exceed the
resource limit of edge devices like mobile phones and embedded
gadgets. As a result, many methods have been proposed to reduce
their cost, such as weight quantization [7, 10, 25], tensor factoriza-
tion [34, 39], weight pruning [26, 81], and channel pruning [31, 48].

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9237-2/22/07.
https://doi.org/10.1145/3512290.3528694

Among them all, channel pruning is the preferred approach to
learn dense compact models, which has been receiving increased
focus from the research community.

Channel pruning is usually achieved in three steps: (1) score
channelsâ€™ importance with a hand-crafted pruning function; (2)
remove redundant channels based on the scores; (3) retrain the
network. The performance of channel pruning largely depends on
the pruning function used in step (1). Current scoring metrics are
mostly handcrafted to extract crucial statistics from channelsâ€™ fea-
ture maps [32, 80] or kernel parameters [31, 42] in a labelless [29,
51] or label-aware [38, 82] manner. However, the design space of
pruning functions is so large that hand-crafted metrics are usu-
ally sub-optimal, and enumerating all functions with human labor
under the space is impossible. While prior evolutionary learning
works aim to automate the design for the structure of the network
directly [67, 72], no attempts, to the best of our knowledge, have
been made to evolve the pruning metrics. These raise the question:
can we leverage evolutionary strategies to automatically develop
strong pruning functions to advance channel pruning?

To this end, we take the ï¬rst step to adopt genetic program-
ming (GP) to learn transferable pruning functions, as shown in
Fig. 1. In particular, a population of functions is evolved by apply-
ing them to pruning tasks of small image classiï¬cation datasets,
and the evolved functions can later be transferred to larger and
more challenging datasets. Our closed-form, explainable, learned
functions are transferable and generalizable: (1) They are applica-
ble to pruning tasks of diï¬€erent image datasets and networks, and
can also be used for other machine learning tasks, e.g., feature se-
lection; (2) they demonstrate competitive pruning performance on
datasets and networks that are diï¬€erent from those used in the evo-
lution process. Such transferability and generalizability provides a
unique advantage to our method, where prior meta-pruning meth-
ods like MetaPruning [52] and LFPC [28] are learned and evolved
on the same tasks with no transferability and perform inferior to
our approach.

Speciï¬cally, we encode pruning functions using expression trees
where we carefully design our search space to allow transferabil-
ity of the evolved functions. For example, we propose a uni-tree
search space for label-aware pruning metrics, which makes them
applicable to diï¬€erent datasets. Such a design space ensures an end-
to-end evolution process, where the learned functions are transfer-
able to other datasets without any manual modiï¬cation after evo-
lution. Moreover, under our encoding space, we are able to build
a group of competitive hand-crafted pruning functions, which we
name as SOAP (state-of-the-art population), and we ï¬nd the use
of SOAP considerably improves the evolution performance. The
populations of the functions are evolved with two diï¬€erent prun-
ing tasks, LeNet on MNIST and VGGNet on CIFAR-10. We observe
that evolving on two tasks produces better functions than only
evolving on one of them, and more surprisingly, our scheme can

 
 
 
 
 
 
GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

Liu et al.

Figure 1: Illustration of our approach. Compared to conventional methods which mainly use handcrafted pruning functions,
we aim to learn the pruning functions automatically via an evolution strategy, genetic programming. The evolved functions
are transferable and generalizable, further enhancing the pruning performance.

even produce more eï¬€ective pruning functions than direct evolu-
tion on a large dataset, e.g., ILSVRC-2012, under the same compu-
tational budget. We analyze the merits of an evolved function both
mathematically and visually and transfer it to three larger datasets,
CIFAR-100, SVHN, and ILSVRC-2012, where it exceeds the state-of-
the-art pruning results on all of them.

Our main contributions are summarized as follows:

â€¢ We propose a novel paradigm where we leverage genetic
programming to learn transferable channel pruning func-
tions which advance pruning eï¬ƒcacy.

â€¢ We develop a novel design space to allow an end-to-end co-
evolution process for searching transferable pruning func-
tions. Such a space also enables us to express SOAP, which
helps improve the eï¬€ectiveness of the evolution.

â€¢ We provide an analysis on our closed-form evolved func-
tions, which could further streamline the design of pruning
metrics. The evolved functions also show generalizability to
other machine learning tasks, e.g., feature selection.

â€¢ When transferred to datasets unseen by the evolution, our
evolved functions achieve state-of-the-art pruning results.
For example, with 26.9% and 53.4% FLOPs1 reduction from
MobileNet-V2, we achieve top-1 accuracies of 71.90% and
69.16% on ILSVRC-2012, outperforming the state of the art.

2 RELATED WORK
Hand-Crafted Channel Pruning. Channel pruning [31, 38, 80,
82] is generally realized by using a handcrafted pruning function
to score channelsâ€™ saliency and remove redundant ones. Based on
the scoring procedure, it can be categorized into labelless pruning
and label-aware pruning.

Labelless channel pruning typically adopts the norm-based prop-
erty of the channelâ€™s feature maps or associated ï¬lters as prun-
ing criterion [29, 31, 32, 42, 44, 51, 53, 54, 78]. For example, Liu et
al. [51] and Ye et al. [78] use the absolute value of scaling factors
in the batch-norm, while â„“1-norm and â„“2-norm of channelsâ€™ asso-
ciated ï¬lters are computed in [29, 42, 44] as channelsâ€™ importance.
On the other hand, researchers have designed metrics to evaluate
class discrepancy of channelsâ€™ feature maps for label-aware prun-
ing [38, 49, 82]. Zhuang et al. [82] insert discriminant losses in
the network and remove channels that are the least correlated to

1Number of ï¬‚oating points operations for an image inference.

the losses after iterative optimization. Kung et al. [38] and Liu et
al. [49, 50] adopt closed-form discriminant functions to accelerate
the scoring process.

While these works use handcrafted scoring metrics, we learn

transferable and generalizable pruning functions automatically.
Meta-Learning. Our work falls into the category of meta-learning,
where prior works have attempted to optimize machine learning
components, including hyper-parameters [5, 17, 68], optimizers [4,
8, 76], and neural network structures [46, 47, 62, 63, 74, 77, 83, 84].
Prior works on neural architecture search (NAS) have leveraged
reinforcement learning (RL) to discover high-performing network
structures [2, 6, 73, 74, 83, 84]. Recently, NAS has also been adopted
to ï¬nd eï¬ƒcient network structures [73, 74]. Another line of works
adopts evolution strategies (ES) to explore the space of network
structures [11, 16, 47, 56, 60, 62, 63, 67, 69, 72, 75, 77], which demon-
strates competitive performance to RL methods. This notion is pio-
neered by neuro-evolution [18, 70, 71] which evolves the topology
of small neural networks. In the era of deep learning, Suganuma
et al. [72] leverage Cartesian genetic programming to ï¬nd compet-
itive network structures. Real et al. [62] evolve networks that im-
prove over the ones found by RL-based NAS [84]. Dai et al. [11] ap-
ply ES to design eï¬ƒcient and deployable networks for mobile plat-
forms. Templier et al. [75] propose a geometric encoding scheme
for more eï¬ƒcient parameter search.

Compared to prior works, we employ evolutionary learning from
a new angle for eï¬ƒcient network design, where we learn transfer-
able pruning functions that produce state-of-the-art pruning re-
sults. Our work is orthogonal to prior works, for example, our
evolved functions can be potentially applied on evolutionary NAS-
learned networks to further enhance their eï¬ƒciency.
Meta-Pruning. Prior works [9, 28, 30, 33, 52] have also adopted a
similar notion of learning to prune a CNN. We note that an evolu-
tion strategy is used in LeGR [9] and MetaPruning [52] to search
for a pair of pruning parameters and network encoding vectors,
respectively. However, our approach is drastically diï¬€erent from
them in terms of search space and search candidates, where we
search for eï¬€ective combinations of operands and operators to build
transferable and powerful pruning functions. He et al. propose LFPC [28]
to learn network pruning criteria (functions) across layers by train-
ing a diï¬€erentiable criteria sampler. However, rather than learning

Evolving Transferable Neural Pruning Functions

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

Figure 2: Illustration of our approach to evolve channel pruning functions. A population of functions is applied to conduct
pruning tasks on two datasets, MNIST and CIFAR-10. Each function receives a ï¬tness value by combining its pruned networksâ€™
accuracies. The population will then go through a natural selection process to improve the functionsâ€™ eï¬€ectiveness.

new pruning functions, their goal is to search within a pool of ex-
isting pruning criteria and ï¬nd candidates that are good for a cer-
tain layerâ€™s pruning. On the contrary, our evolution recombines
the operands and operators and produces novel pruning metrics,
which are generally good for all layers.

We also notice that MetaPruning [52], LFPC [28], and other
methods [9, 30, 33] are all learned on one task (dataset and net-
work) and applied only on the same task with no transferability. In
contrast, we only need one evolution learning process, which out-
puts evolved functions that are transferable across multiple tasks
and demonstrate competitive performance on all of them.

3 METHODOLOGY
In Fig. 2, we present our evolution framework, which leverages
genetic programming [36] to learn high-quality channel pruning
functions. We ï¬rst describe the design space to encode channel
scoring functions. Next, we discuss the pruning tasks to evaluate
the functionsâ€™ eï¬€ectiveness. Lastly, genetic operators are deï¬ned
to traverse the function space for competitive solutions.

3.1 Function Design Space
Expression Tree. In channel pruning, a pruning function ğœ‰ : C â†¦âˆ’â†’
R scores the channels to determine their importance/redundancy,
where C denotes feature maps, ï¬lters, and their statistics associ-
ated with the channels. This scoring process can be viewed as a se-
ries of operations with operators (addition, matrix multiplication,
etc.) and operands (feature maps, ï¬lters, etc.). We thus adopt an
expression tree encoding scheme to represent a pruning function
where inner nodes are operators, and leaves are operands.

As shown in Tab. 1 and 2, our function design space includes
two types of operands (6 operands in total) and four types of op-
erators (23 operators in total), via which a vast number of pruning
functions can be expressed. The statistics operators can compute

the statistics of an operand in two dimensions, namely, global di-
mension (subscript with â€˜gâ€™) and sample dimension (subscript with
â€˜sâ€™). The global dimension operators ï¬‚atten operands into a 1D se-
quence and extract corresponding statistics, while the sample di-
mension operators compute statistics on the axis of samples. For
example, sumğ‘” (W) returns the summation of all entries of a kernel
tensor, while meanğ‘  (F ) returns Â¯ğ‘“ âˆˆ Rğ» Ã—ğ‘Š , which is the sample
average of all feature maps. We also include specialized operators
which allow us to build complicated but competitive metrics like
maximum mean discrepancy (MMD) [24] and ï¬lterâ€™s geometric me-
dian [31].
Function Encoding. The channel scoring functions can be cate-
gorized into two types: labelless metrics and label-aware metrics.
For labelless functions like ï¬lterâ€™s â„“1-norm, we adopt a direct en-
coding scheme as sumğ‘” (abs(Wğ¼ )) with the expression tree shown
in Fig. 3.

For label-aware metrics such as the one in [38] and MMD [24],
which measure class discrepancy of the feature maps, we observe
a common computation graph among them, as shown in Fig. 3:
(1) partition the feature maps in a labelwise manner; (2) apply the
same operations on each label partition and all feature maps; (3)
average/sum the scores of all partitions to obtain a single scalar.
These metrics can be naively encoded as ğ¶-branch trees (ğ¶: number
of class labels in the dataset). However, directly using the naive
encoding scheme will result in data-dependent non-transferable
metrics because: (1) ğ¶ varies from dataset to dataset (e.g., metrics
for CIFAR-10 is not transferable to CIFAR-100); (2) mutating the
subtrees diï¬€erently could make the metric overï¬t to a speciï¬c label
numbering scheme (e.g., for a metric with diï¬€erent subtrees on
class-1 and class-2, renumbering the labels would mean the metric
would compute something diï¬€erent, which is undesirable).

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

Liu et al.

Filter-based operands

Map-based operands

whole layerâ€™s ï¬lter W âˆˆ Rğ‘ğ‘œğ‘¢ğ‘¡ Ã—ğ‘ğ‘–ğ‘› Ã—â„Ã—ğ‘¤, channelâ€™s incoming ï¬lter Wğ¼ âˆˆ Rğ‘ğ‘–ğ‘› Ã—â„Ã—ğ‘¤, channelâ€™s
batch-normed parameter B âˆˆ R4
Feature maps collection F = {(ğ‘“ğ‘–, ğ‘¦ğ‘–)|ğ‘“ğ‘– âˆˆ Rğ» Ã—ğ‘Š , ğ‘¦ğ‘– âˆˆ [1 : ğ¶], ğ‘– âˆˆ [1 : ğ‘ ]}, two partitions of
feature maps collections F + = {ğ‘“ğ‘– |ğ‘¦ğ‘– = ğ‘˜, ğ‘˜ âˆˆ [1 : ğ¶]} and F âˆ’ = {ğ‘“ğ‘– |ğ‘¦ğ‘– â‰  ğ‘˜, ğ‘˜ âˆˆ [1 : ğ¶]}

Table 1: Operand Space

Elementwise operators

Matrix operators

Statistics operators
Specialized operators

addition, subtraction, multiplication, division, absolute value, square, square root, adding ridge
factor
matrix trace, matrix multiplication, matrix inversion, inner product, outer product, matrix/vector
transpose
summation, product, mean, standard deviation, variance, counting measure
rbf kernel matrix getter, geometric median getter, tensor slicer

Table 2: Operator Space

Figure 3: Illustration of the pruning function encoding. Left: For labelless scoring metrics like ï¬lterâ€™s â„“1-norm, we adopt a
direct tree encoding scheme. Right: For label-aware scoring metrics, we encode the ğ¶-subtree computation graph by a uni-tree
(ğ¶: number of class labels). The uni-tree encodes the common operations (op) on each label partition (F +, F âˆ’) and all feature
maps (F ). This scheme allows transferable function evolution.

To combat the above issues, we express a label-aware function
by a uni-tree which encodes the common operations that are ap-
plied to each label partition, as explained in Fig. 3. Instead of di-
rectly encoding the operands from a speciï¬c label partition, like
F 1+ (feature maps with labels equal to 1) and F 1âˆ’ (feature maps
with labels not equal to 1), we use a symbolic representation of F +
and F âˆ’ to generically encode the partition concept. In the actual
scoring process, the uni-tree is compiled back to a ğ¶-branch com-
putation graph, with F + and F âˆ’ converted to the speciï¬c map
partitions. Such uni-tree encoding allows us to evolve label-aware
metrics independent of ğ¶ and label numbering schemes, which en-
sures their transferability to datasets unseen by the evolution pro-
cess.
SOAP. Using the above described function encoding, we can im-
plement a broad range of competitive pruning functions: ï¬lterâ€™s
â„“1-norm [42], ï¬lterâ€™s â„“2-norm [29], batch normâ€™s scaling factor [51],
ï¬lterâ€™s geometric median [31], Discriminant Information [38], Max-
imum Mean Discrepancy [24], Absolute SNR [22], Studentâ€™s T-Test [41],
Fisher Discriminant Ratio [61], and Symmetric Divergence [55].
For the last four metrics, we adopt the scheme in [49] for channel
scoring. We name this group of functions the state-of-the-art pop-
ulation (SOAP), which helps our evolution in many aspects. For
instance, in Sec. 6, we ï¬nd that initializing the population with

SOAP evolves better pruning functions than random initialization.
Detailed implementation of SOAP is included in Supplementary.

3.2 Function Eï¬€ectiveness Evaluation
The encoded functions are applied to empirical pruning tasks to
evaluate their eï¬€ectiveness. To avoid overï¬tting on certain data
patterns and increase the generality of the evolved functions, we
evolve the population of functions on two diï¬€erent pruning tasks,
LeNet-5 [40] on MNIST [40] and VGG-16 [66] on CIFAR-10 [37]. In
both pruning tasks, we adopt a one-shot pruning scheme and re-
port the retrained accuracies on validation sets. For each pruning
task, we keep the pruning settings (layersâ€™ pruning ratios, target
pruning layers, etc.) and the retraining hyper-parameters (learning
rate, optimizer, weight decay factor, etc.) the same for all evalua-
tions throughout the evolution process. This guarantees a fair ef-
fectiveness comparison over diï¬€erent functions in all generations
and ensures we are evolving better functions rather than better
hyper-parameters. In this way, we can meta-learn powerful func-
tions that perform well on both MNIST and CIFAR-10 and are gen-
eralizable to other datasets. Not surprisingly, evolving with both
tasks produce stronger pruning functions than evolving on only
one of them, shown in Sec. 3.3. Moreover, in Sec. 6, we ï¬nd our
strategy enjoys better cost-eï¬€ectiveness compared to direct evolu-
tion on a large dataset, e.g., ILSVRC-2012.

Evolving Transferable Neural Pruning Functions

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

e
u
l
a
V
s
s
e
n
t
i
F
t
n
i
o
J

96.8

96.6

96.4

96.2

96.0

95.8

Max Fitness
25 Percentile Fitness

0

5

10
15
Generation #

20

25

Figure 4: Preliminary evolution tests on the choice
of ï¬tness combination scheme. The best evolved
function from each scheme is applied to conduct
a pruning test on CIFAR-100 with ResNet-38, and
their accuracies are plotted.

Figure 5: Progress of the evolution experiment. Each dot represents an
individual function evaluation. The red curve shows functions with the
best ï¬tness over generations, while the green curve shows the individ-
uals at the 25 percentile ï¬tness. The eï¬€ectiveness of the best function
and the populationâ€™s overall quality are both monotonically increasing.

3.3 Function Fitness
After evaluation, each encoded function receives two accuracies,
AccMNIST and AccCIFAR, from the pruning tasks. We investigate
two accuracy combination schemes, weighted arithmetic mean (Eqn. 1)
and weighted geometric mean (Eqn. 2), to obtain the joint ï¬tness
of a function. A free parameter ğ›¼ âˆˆ [0, 1] is introduced to control
the weights of diï¬€erent tasks.

(1)
(2)

Fitness = ğ›¼ Ã— AccMNIST + (1 âˆ’ ğ›¼) Ã— AccCIFAR
Fitness = (AccMNIST)ğ›¼ Ã— (AccCIFAR)1âˆ’ğ›¼
Ablation Study. To decide the ï¬tness combination scheme for
the main experiments, we conduct 10 small preliminary evolution
tests using a grid of ğ›¼ âˆˆ {0, 0.3, 0.5, 0.7, 1} with both combination
schemes. Note that when ğ›¼ âˆˆ {0, 1}, the process degenerates to sin-
gle dataset evolution. We empirically evaluate the generalizability
of the best evolved functions from each test by applying them to
prune a ResNet-38 on CIFAR-100. Note CIFAR-100 is not used in
the evolution process, and thus the performance on it speaks well
for evolved functionsâ€™ generalizability. In Fig. 4, we ï¬nd that solely
evolving on MNIST (ğ›¼ = 1) would be the least eï¬€ective option
for CIFAR-100 transfer pruning. In addition, we ï¬nd that functions
evolved on two datasets (ğ›¼ âˆˆ [0.3, 0.5, 0.7]) generally perform bet-
ter than the ones that just evolve on a single dataset (ğ›¼ âˆˆ [0, 1]).
We observe that setting ğ›¼ = 0.5 with weighted geometric mean
leads to the best result, which we adopt in the main experiments.

3.4 Genetic Operations
Selection. After evaluation, the population will undergo a selec-
tion process, where we adopt tournament selection [21] to choose
a subset of competitive functions.
Diversity Maintenance. This subset of functions is then used to
reproduce individuals for the next generation. However, we ob-
serve shrinkage of the populationâ€™s genetic diversity when all chil-
dren are reproduced from parents, as the selected parents only rep-
resent a small pool of genomes. Such diversity shrinkage would re-
sult in premature convergence of the evolution process. To combat
this issue, we reserve a slot in the next generation and produce in-
dividuals in the slots by randomly cloning functions from SOAP or

building random trees. We ï¬nd this adjustment empirically useful
to help the evolution proceed longer.
Mutation and Crossover. We conduct mutation and crossover on
the reproduced population to traverse the function design space
for new expressions. We adopt the conventional scheme of ran-
dom tree mutation and one point crossover [3]. After mutation and
crossover, the population will go through the next evolution itera-
tion.
Function Validity. The function expressions generated from mu-
tation and crossover can be invalid (non-invertible matrix, dimen-
sion inconsistency, etc.) due to the random selections of operators,
operands, and nodes in the expression trees. To combat this issue
and enlarge our valid function space, some operators are deliber-
ately modiï¬ed from their standard deï¬nition. For instance, when-
ever we need to invert a positive semi-deï¬nite scatter matrix ğ‘†, we
automatically add a ridge factor ğœŒğ¼ , and invert the matrix ğ‘† +ğœŒğ¼ . For
dimension inconsistency in elementwise operations, we have two
options to pad the operand with a smaller dimension: (1) with 0 for
+ and âˆ’, and 1 for Ã—, and Ã·, (2) with its own value if it is a scalar.
Moreover, we conduct a validity test on the mutated/crossovered
functions every time after the mutation/crossover process. The in-
valid expressions are discarded, and the mutation/crossover oper-
ations are repeated until we recover the population size with all
valid functions. These methods ensure we generate valid function
expressions under our vast design space during the evolution pro-
cess.

4 EVOLUTION ON MNIST AND CIFAR-10
Experiment Settings. We conduct the experiment with a popula-
tion size of 40 individuals over 25 generations. The population is
initialized with 20 individuals chosen by randomly cloning func-
tions from SOAP and 20 random expression trees. The size of the
selection tournament is 4 and we select 10 functions in each gen-
eration. 24 individuals are reproduced from the selected functions,
while 6 individuals are from SOAP or randomly built. The mutation
and crossover probability are both set to be 0.75. We prune 92.4%
of FLOPs from a LeNet-5 (baseline acc: 99.26%) and 63.0% of FLOPs
from a VGG-16 (baseline acc: 93.7%), respectively. Such aggressive

 
 
GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

Liu et al.

Network Method

ResNet
164

SLIM [51]
Ours

Test Acc (%)
98.22 â†’ 98.15
98.22 â†’ 98.26

Acc â†“ (%)
0.07
-0.04

FLOPs

Pruned (%)

Parameters

Pruned (%)

172M
92M

31.1
63.2

1.46M
0.64M

14.5
63.0

Table 3: SVHN Transfer Pruning Results

Network

Method

VGG19

ResNet
56

ResNet
110

ResNet
164

SLIM [51]
G-SD [49]
Ours

SFP [29]
FPGM [31]
LFPC [28]
LeGR [9]
Ours

LCCL [14]
SFP [29]
FPGM [31]
TAS [15]
Ours

LCCL [14]
SLIM [51]
DI [38]
Ours

FLOPs

Pruned (%)

37.1
59.5
61.0

256M
161M
155M

2.96
2.61
0.58
1.37
0.35

39.3
52.6
51.6
51.4
56.2

76M
59M
61M
61M
55M

Acc â†“ (%)
-0.22
-0.27
-0.62

Test Acc (%)
73.26 â†’ 73.48
73.40 â†’ 73.67
73.40 â†’ 74.02
71.33 â†’ 68.37
71.40 â†’ 68.79
71.33 â†’ 70.83
72.41 â†’ 71.04
72.05 â†’ 71.70
72.79 â†’ 70.78
74.14 â†’ 71.28
74.14 â†’ 72.55
75.06 â†’ 73.16
74.40 â†’ 73.85
75.67 â†’ 75.26
76.63 â†’ 76.09
76.63 â†’ 76.11
77.15 â†’ 77.77
Table 4: CIFAR-100 Transfer Pruning Results

173M
121M
121M
120M
111M

195M
124M
105M
92M

2.01
2.86
1.59
1.90
0.55

31.3
52.3
52.3
52.6
56.2

0.41
0.54
0.52
-0.62

21.3
50.6
58.0
63.2

Parameters

Pruned (%)

5.0M
3.2M
2.9M

-
-
-
-
0.38M

1.75M
-
-
-
0.77M

1.73M
1.21M
0.95M
0.66M

75.1
84.0
85.5

-
-
-
-
54.9

0.0
-
-
-
55.8

0.0
29.7
45.1
61.8

ğœ‰âˆ— (C) =

varğ‘” (F âˆ’)
varğ‘” (F +)

+

varğ‘” (F +)
varğ‘” (F âˆ’)

+

||stdğ‘” ( Â¯ğ‘“ ) Ã— varğ‘” (F âˆ’) Ã— Â¯ğ‘“ + (varğ‘” (F +) âˆ’ meanğ‘” (F âˆ’))1||2
2
varğ‘” (F +) + varğ‘” (F âˆ’)

(3)

pruning schemes help us better identify functionsâ€™ eï¬€ectiveness.
We use the weighted geometric mean in Eqn. 2 to combine two val-
idation accuracies with ğ›¼ = 0.5. Our codes are implemented with
DEAP [19] and TensorFlow [1] for the genetic operations and the
neural network pruning. The experiments are carried out on a clus-
ter with SLURM job scheduler [79] for workload parallelization.
Experiment Result. Our evolution progress is shown in Fig. 5,
where the red curve denotes the functions with the maximum ï¬t-
ness while the green curve plots the ones with the top 25 percentile
ï¬tness. Both curves increase monotonically over generations, indi-
cating that the quality of both the best function and the entire pop-
ulation improves over time. This demonstrates the eï¬€ectiveness of
our scheme. Speciï¬cally, the best pruned LeNet-5/VGG-16 in the
ï¬rst generation have accuracies of 99.15%/93.55% while the best
accuracies in the last generation are 99.25%/94.0%. As the ï¬rst gen-
eration is initialized with SOAP functions, such results suggest that
the algorithm derives metrics that outperform handcrafted func-
tions in SOAP. The whole evolution takes 98 GPU-days on P100,
which is a reasonable amount of computation for modern evolu-
tion learning. While this is a pioneering work2, we envision that
future work could further reduce the evolution computation.
Evolved Function. We present the winning function in Eqn. 3,
where Â¯ğ‘“ = meanğ‘  (F ) denotes sample average of the feature maps
and 1 is a vector with all entries set to be 1. The ï¬rst two terms
of the function award a high score to channels with class-diverged

2Compared to initial works on NAS, which take 2000 GPU-days [84] and 3000 GPU-
days [63], we are 20/30x faster.

feature maps whose varğ‘” (F +) or varğ‘” (F âˆ’) is signiï¬cantly smaller
than the other. Channels with these feature maps contain rich class
information as it generates distinguishable responses to diï¬€erent
classes. The third termâ€™s denominator computes the sum of the fea-
ture maps variances while its numerator draws statistics from the
average feature maps and the distance between F + and F âˆ’, which
resembles the concept of signal-to-noise ratio. Two points worth
mentioning for this function: (1) it identiï¬es important statistical
concepts from human-designed metrics, where it learns from Sym-
metric Divergence [55] to measure the divergence of class feature
maps. (2) it contains unique math concepts that are empirically
good for channel importance measurement, which is shown in the
novel statistics combination of the feature maps in the third termâ€™s
numerator. Our visual result in Sec. 6 shows ğœ‰âˆ— can be further ap-
plied to feature selection, which represents another machine learn-
ing task.

5 TRANSFER PRUNING
Benchmarks. To show the generalizability of our evolved pruning
function, we apply ğœ‰âˆ— in Eqn. 3 to more challenging datasets that
are not used in the evolution process: CIFAR-100 [37], SVHN [58],
and ILSVRC-2012 [12]. We compare our method with metrics from
SOAP, e.g., L1 [42], FPGM [31], G-SD [49], and DI [38], where ğœ‰âˆ—
outperforms all these handcrafted metrics. We also include other
â€œlearn to prune" methods like Meta [52] and LFPC [28] and other
state-of-the-art methods like DSA [59] and CC [43] for comparison.
The results are summarized in Tab. 3, 4, and 5, where the accuracies

Evolving Transferable Neural Pruning Functions

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

Network

Method

VGG16

ResNet
18

MobileNet
V2

L1 [42]
CP [32]
G-SD [49]
Ours 59%-pruned
RNP [45]
SLIM [51]
FBS [20]
Ours 67%-pruned

Ours 17%-pruned
SLIM [51]
LCCL [14]
Ours 37%-pruned
SFP [29]
DCP [82]
FPGM [31]
DSA [59]
Ours 41%-pruned

Uniform [65]
AMC [30]
CC [43]
Meta [52]
LeGR [9]
Ours 27%-pruned
DCP [82]
Meta [52]
Ours 53%-pruned

Top-1
Acc. (%)

-
-
71.30 â†’ 71.88
71.30 â†’ 72.37
-
-
-
71.30 â†’ 71.64
70.05 â†’ 70.08
68.98 â†’ 67.21
69.98 â†’ 66.33
70.05 â†’ 69.09
70.28 â†’ 67.10
69.64 â†’ 67.35
70.28 â†’ 68.41
69.72 â†’ 68.61
70.05 â†’ 68.85
71.80 â†’ 69.80
71.80 â†’ 70.80
71.88 â†’ 70.91
72.70 â†’ 71.20
71.80 â†’ 71.40
72.18 â†’ 71.90
70.11 â†’ 64.22
72.70 â†’ 68.20
72.18 â†’ 69.16

Top-1
â†“ (%)
-
-
-0.58
-1.07
-
-
-
-0.34

-0.03
1.77
3.65
0.96
3.18
2.29
1.87
1.11
1.20

2.00
1.00
0.89
1.50
0.40
0.28
5.89
4.50
3.02

Top-5
Acc. (%)
89.90 â†’ 89.10
89.90 â†’ 89.90
90.10 â†’ 90.66
90.10 â†’ 91.05
89.90 â†’ 86.67
89.90 â†’ 88.53
89.90 â†’ 89.86
90.10 â†’ 90.60
89.40 â†’ 89.24
88.68 â†’ 87.39
89.24 â†’ 86.94
89.40 â†’ 88.59
89.63 â†’ 87.78
88.98 â†’ 87.60
89.63 â†’ 88.48
89.07 â†’ 88.35
89.40 â†’ 88.45
-
-
-
-
-
90.49 â†’ 90.38
-
-
90.49 â†’ 88.66

Top-5
â†“ (%)
0.80
0.00
-0.56
-0.95
3.23
1.37
0.04
-0.50

0.16
1.29
2.30
0.81
1.85
1.38
1.15
0.72
0.95

-
-
-
-
-
0.11
3.77
-
1.83

FLOPs (B)
Pruned (%)

7.74 (50.0)
7.74 (50.0)
6.62 (57.2)
6.34 (59.0)
5.16 (66.7)
5.16 (66.7)
5.16 (66.7)
5.12 (66.9)

1.50 (16.8)
1.31 (28.0)
1.18 (34.6)
1.14 (36.7)
1.06 (41.8)
0.98 (46.0)
1.06 (41.8)
1.09 (40.0)
1.07 (41.0)

0.22 (26.9)
0.22 (26.9)
0.22 (28.3)
0.22 (27.9)
0.22 (26.9)
0.22 (26.9)
0.17 (44.7)
0.14 (53.4)
0.14 (53.4)

Params (M)
Pruned (%)

-
-
133.6 (3.4)
133.5 (3.5)
138.3 (0.0)
-
138.3 (0.0)
131.6 (4.8)

11.2 (3.9)
-
11.7 (0.0)
9.3 (20.1)
-
6.2 (47.0)
-
-
8.8 (24.5)

-
-
-
-
-
2.8 (20.4)
2.6 (25.9)
-
2.1 (39.3)

Table 5: ILSVRC-2012 Transfer Pruning Results. We report our pruned models at diï¬€erent FLOPs levels to ensure a fair com-
parison with diï¬€erent prior arts. We add a suï¬ƒx specifying FLOPs pruning percentage for each of our pruned model.

are shown as â€œbaseline acc. â†’ pruned acc." and the numbers for
all other methods are copied from their papers. On ILSVRC-2012,
we report our pruned models at diï¬€erent FLOPs reduction levels
and add a suï¬ƒx specifying their FLOPs pruning ratios (e.g., Ours
60%-pruned). This is because diï¬€erent prior arts report their com-
pressed models at diï¬€erent rates, and we want to make a fair com-
parison to all of them. We ï¬nd that our evolved function achieves
state-of-the-art results on all datasets.
Settings. We adopt a one-shot pruning scheme with a uniform
pruning ratio across layers for our transfer pruning and use the
SGD optimizer with Nesterov Momentum [57] for retraining. The
weight decay factor and the momentum are set to be 0.0001 and 0.9,
respectively. On SVHN/CIFAR-100, we use a batch size of 32/128
to ï¬ne-tune the network with 20/200 epochs. The learning rate is
initialized at 0.05 and multiplied by 0.14 at 40% and 80% of the total
number of epochs. On ILSVRC-2012, we use a batch size of 128 to
ï¬ne-tune VGG-16/ResNet-18/MobileNet-V2 for 30/100/100 epochs.
For VGG-16/ResNet-18, the learning rate is started at 0.0006 and
multiplied by 0.4 at 40% and 80% of the total number of epochs. We
use a cosine decay learning rate schedule for MobileNet-V2 [65]
with an initial rate of 0.03.
SVHN. We ï¬rst evaluate ğœ‰âˆ— on SVHN with ResNet-164. Ours out-
performs SLIM [51] by 0.1% in accuracy with signiï¬cant hardware
resource savings: 32.1% more FLOPs saving and 48.5% more param-
eters saving, which well demonstrates the eï¬€ectiveness of ğœ‰âˆ—.

CIFAR-100. On VGG-19, our pruned model achieves an accuracy
gain of 0.35% with respect to G-SD [49]. Compared to LFPC [28]
and LeGR [9], our pruned ResNet-56 achieves an accuracy gain
of 0.87% and 0.66%, respectively, while having 5% less FLOPs. On
ResNet-110, our method outperforms FPGM [31] and TAS [15] by
1.30% and 0.69% in terms of accuracy with 4% less FLOPs. In com-
parison with LCCL [14], SLIM [51], and DI [38], our pruned ResNet-
164 achieves an accuracy of 77.77% with 63.2% FLOPs reduction
which advances all prior methods.
ILSVRC-2012. On VGG-16, our approach improves over the base-
line by nearly 1.1% in top-1 accuracy with 2.4Ã— acceleration. Our
3.3Ã—-accelerated model advances the state of the art by achiev-
ing top-1/top-5 accuracies of 71.64%/90.60%. On ResNet-18, our ap-
proach reduces 16.8% of the FLOPs without top-1 accuracy loss.
Compared to LCCL [14], our method achieves a 2.72% top-1 accu-
racy gain with a higher FLOPs reduction ratio. We demonstrate
top-1 accuracy gains of 1.75% and 1.50% with respect to SFP [29]
and DCP [82] with over 40% FLOPs reduction. We ï¬nally show
our performance on a much more compact network, MobileNet-
V2, which is speciï¬cally designed for mobile deployment. When
26.9% of FLOPs are pruned, our approach outperforms AMC [30],
Meta [52], and LeGR [9] with a top-1 accuracy of 71.90%. At a
higher pruning ratio, our method advances DCP [82] and Meta [52]
by top-1 accuracies of 4.94% and 0.96%, with 53.4% FLOPs reduc-
tion.

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

Liu et al.

e
u
l
a
V
s
s
e
n
t
i
F
t
n
i
o
J

96.8

96.6

96.4

96.2

96.0

95.8

95.6

Max Fitness
25 Percentile Fitness

0

5

10
15
Generation #

20

25

Figure 6: Comparing random initial population evolution
(dashed line) with the evolution in Sec. 4 (solid line). Thanks
to the expressiveness of our function space, the evolution
with randomly-initialized functions also achieve good prun-
ing ï¬tness. However, we observe that it converges very early
around the 8th generation and stalls at the plateau for a long
period. Moreover, its ï¬nal ï¬tness has a clear performance
gap with respect to the one in Sec. 4.

6 ABLATION STUDY
Random Initial Population. In Fig. 6, we conduct a control ex-
periment which initializes all individuals as random expression
trees to study the eï¬€ectiveness of initializing our population with
SOAP. We also turn oï¬€ the SOAP function insertion in the repro-
duction process for the control experiment. All other parameters
(number of generations, size of population, ğ›¼, etc.) are kept to be
the same as in Sec. 4 for a fair comparison. We ï¬nd that evolv-
ing with random population also achieves a good pruning ï¬tness,
which indicates that our design space is of powerful expressiveness.
However, we observe early convergence and a ï¬nal performance
gap in the control experiment compared to the main experiment in
Sec. 4, demonstrating the advantage of using SOAP for evolution.
Evolution on ILSVRC-2012. In contrast to our evolution strategy
with a joint ï¬tness function on MNIST and CIFAR-10, we conduct
an evolution on only ILSVRC-2012 as a control experiment. We re-
strict the total computation budget to be the same as Sec. 4, i.e. 98
GPU-days, and evolve on ResNet-18 with a population size of 40
over 25 generations. Due to the constrained budget, each pruned
net is only retrained for 4 epochs. We include detailed evolution
settings and results in Supplementary. Two major drawbacks are
found with this evolution strategy: (1) Imprecise evaluation. Due
to the lack of training epochs, the functionâ€™s actual eï¬€ectiveness
is not precisely revealed. We take two functions with ï¬tness 63.24
and 63.46 from the last generation, and use them again to prune
ResNet-18 but fully retrain for 100 epochs. We ï¬nd that the one
with lower ï¬tness in evolution achieves an accuracy of 68.27% in
the full training, while the higher one only has an accuracy of
68.02%. Such result indicates that the evaluation in this evolution
procedure could be inaccurate, while our strategy ensures a full
retraining for precise eï¬€ectiveness assessment. (2) Inferior per-
formance. The best evolved function with this method, ğœ‰ğ¼ğ‘šğ‘ğ‘”ğ‘’ğ‘ ğ‘’ğ‘¡
(in Supplementary), performs inferior to ğœ‰âˆ— shown in Eqn. 3 when
transferred to a diï¬€erent dataset. In particular, when applied to
pruning 56% FLOPs from ResNet-110 on CIFAR-100, ğœ‰ğ¼ğ‘šğ‘ğ‘”ğ‘’ğ‘ ğ‘’ğ‘¡ only
achieves an accuracy of 72.51% while ğœ‰âˆ— reaches 73.85%. These two

Figure 7: Feature selection by DI [38] (middle) and ğœ‰âˆ— (right)
for MNIST, where ğœ‰âˆ— tends to preserve features with higher
means and more robust pattern in reference of the average
feature values map (left).

issues suggest that evolving on two small datasets would have bet-
ter cost-eï¬€ectiveness than using a single large scale dataset like
ILSVRC-2012.
Feature Selection. We further apply ğœ‰âˆ— to another machine learn-
ing task, feature selection, to visually understand our evolved func-
tion. In particular, we compare ğœ‰âˆ— (right) vs. DI [38] (middle) on
MNIST feature selection in Fig. 7. The red pixels indicate the im-
portant features evaluated by the metrics, while the blue ones are
redundant. Taking the average feature values map (left) for ref-
erence, we ï¬nd that our evolved function tends to select features
with higher means, where the MNIST pattern is more robust.

7 CONCLUSION
In this work, we propose a novel paradigm integrating evolution-
ary learning with channel pruning, which ï¬rst learns novel chan-
nel pruning functions from small datasets, and then transfers them
to larger and more challenging datasets. We develop an end-to-end
genetic programming framework to automatically search for trans-
ferable pruning functions over our novel function design space
without any manual modiï¬cation after evolution. We present and
analyze a closed-form evolved function which can oï¬€er strong prun-
ing performance and further streamline the design of our pruning
strategy. The learned pruning function exhibits remarkable gener-
alizability to datasets diï¬€erent from those in the evolution process.
Speciï¬cally, on SVHN, CIFAR-100, and ILSVRC-2012, we achieve
state-of-the-art pruning results.

8 ACKNOWLEDGEMENT
We thank a former colleague from Princeton Parallel Group, Yanqi
Zhou, for her help with parallel implementation of the evolution.
This material is based upon work supported by the National Sci-
ence Foundation under Grant No. CCF-1822949. Any opinions, ï¬nd-
ings, and conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reï¬‚ect the views
of the National Science Foundation.

REFERENCES
[1] MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeï¬€rey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoï¬€rey Irving, Michael Isard, et al.
2016. Tensorï¬‚ow: A system for large-scale machine learning. In 12th {USENIX}
Symposium on Operating Systems Design and Implementation ({OSDI} 16). 265â€“
283.

[2] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2016. Design-
ing neural network architectures using reinforcement learning. arXiv preprint
arXiv:1611.02167 (2016).

 
 
Evolving Transferable Neural Pruning Functions

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

[3] Wolfgang Banzhaf, Peter Nordin, Robert E Keller, and Frank D Francone. 1998.

Genetic programming. Springer.

[4] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le. 2017. Neural opti-
mizer search with reinforcement learning. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70. JMLR. org, 459â€“468.

[5] James Bergstra, Daniel Yamins, and David Daniel Cox. 2013. Making a science
of model search: Hyperparameter optimization in hundreds of dimensions for
vision architectures. (2013).

[6] Han Cai, Ligeng Zhu, and Song Han. 2018. Proxylessnas: Direct neural architec-
ture search on target task and hardware. arXiv preprint arXiv:1812.00332 (2018).
[7] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.
2015. Compressing neural networks with the hashing trick. In International Con-
ference on Machine Learning. 2285â€“2294.

[8] Yutian Chen, Matthew W Hoï¬€man, Sergio GÃ³mez Colmenarejo, Misha Denil,
Timothy P Lillicrap, Matt Botvinick, and Nando De Freitas. 2017. Learning to
learn without gradient descent by gradient descent. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70. JMLR. org, 748â€“756.
[9] Ting-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana Marculescu. 2020. Towards
Eï¬ƒcient Model Compression via Learned Global Ranking. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1518â€“1528.

[10] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. 2016. Binarized neural networks: Training deep neural networks with
weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830
(2016).

[11] Xiaoliang Dai, Peizhao Zhang, Bichen Wu, Hongxu Yin, Fei Sun, Yanghan Wang,
Marat Dukhan, Yunqing Hu, Yiming Wu, Yangqing Jia, et al. 2019. Chamnet:
Towards eï¬ƒcient network design through platform-aware model adaptation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
11398â€“11407.

[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-
agenet: A large-scale hierarchical image database. In 2009 IEEE conference on
computer vision and pattern recognition. Ieee, 248â€“255.

[13] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. 2015.

Image
super-resolution using deep convolutional networks. IEEE transactions on pat-
tern analysis and machine intelligence 38, 2 (2015), 295â€“307.

[14] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. 2017. More is less: A
more complicated network with less inference complexity. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. 5840â€“5848.
[15] Xuanyi Dong and Yi Yang. 2019. Network pruning via transformable architec-
ture search. In Advances in Neural Information Processing Systems. 759â€“770.
[16] Chrisantha Fernando, Dylan Banarse, Malcolm Reynolds, Frederic Besse, David
Pfau, Max Jaderberg, Marc Lanctot, and Daan Wierstra. 2016. Convolution by
evolution: Diï¬€erentiable pattern producing networks. In Proceedings of the Ge-
netic and Evolutionary Computation Conference 2016. ACM, 109â€“116.

[17] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg,
Manuel Blum, and Frank Hutter. 2015. Eï¬ƒcient and robust automated machine
learning. In Advances in neural information processing systems. 2962â€“2970.
[18] Dario Floreano, Peter DÃ¼rr, and Claudio Mattiussi. 2008. Neuroevolution: from

architectures to learning. Evolutionary Intelligence 1, 1 (2008), 47â€“62.

[19] FÃ©lix-Antoine Fortin, FranÃ§ois-Michel De Rainville, Marc-AndrÃ© Gardner, Marc
Parizeau, and Christian GagnÃ©. 2012. DEAP: Evolutionary Algorithms Made
Easy. Journal of Machine Learning Research 13 (jul 2012), 2171â€“2175.

[20] Xitong Gao, Yiren Zhao, Åukasz Dudziak, Robert Mullins, and Cheng-zhong
Xu. 2018. Dynamic channel pruning: Feature boosting and suppression. arXiv
preprint arXiv:1810.05331 (2018).

[21] David E Goldberg and Kalyanmoy Deb. 1991. A comparative analysis of selection
schemes used in genetic algorithms. In Foundations of genetic algorithms. Vol. 1.
Elsevier, 69â€“93.

[22] Todd R Golub, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle
Gaasenbeek, Jill P Mesirov, Hilary Coller, Mignon L Loh, James R Downing,
Mark A Caligiuri, et al. 1999. Molecular classiï¬cation of cancer: class discovery
and class prediction by gene expression monitoring. science 286, 5439 (1999),
531â€“537.

[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. Advances in neural information processing systems 27 (2014).

[24] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard SchÃ¶lkopf, and
Alexander Smola. 2012. A kernel two-sample test. Journal of Machine Learning
Research 13, Mar (2012), 723â€“773.

[25] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compress-
ing deep neural networks with pruning, trained quantization and huï¬€man cod-
ing. arXiv preprint arXiv:1510.00149 (2015).

[26] Song Han, Jeï¬€ Pool, John Tran, and William Dally. 2015. Learning both weights
and connections for eï¬ƒcient neural network. In Advances in neural information
processing systems.

[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.

[28] Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, and Yi Yang.
2020. Learning Filter Pruning Criteria for Deep Convolutional Neural Networks
Acceleration. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 2009â€“2018.

[29] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. 2018. Soft ï¬lter
pruning for accelerating deep convolutional neural networks. arXiv preprint
arXiv:1808.06866 (2018).

[30] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. 2018. Amc:
Automl for model compression and acceleration on mobile devices. In Proceed-
ings of the European Conference on Computer Vision (ECCV). 784â€“800.

[31] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. 2019. Filter pruning
via geometric median for deep convolutional neural networks acceleration. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
4340â€“4349.

[32] Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel pruning for accelerating
very deep neural networks. In Proceedings of the IEEE International Conference
on Computer Vision. 1389â€“1397.

[33] Qiangui Huang, Kevin Zhou, Suya You, and Ulrich Neumann. 2018. Learning to
prune ï¬lters in convolutional neural networks. In 2018 IEEE Winter Conference
on Applications of Computer Vision (WACV). IEEE, 709â€“718.

[34] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. 2014. Speeding up
arXiv preprint

convolutional neural networks with low rank expansions.
arXiv:1405.3866 (2014).

[35] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980 (2014).

[36] John R Koza and John R Koza. 1992. Genetic programming: on the programming

of computers by means of natural selection. Vol. 1. MIT press.

[37] Alex Krizhevsky and Geoï¬€rey Hinton. 2009. Learning multiple layers of features

from tiny images. Technical Report. Citeseer.

[38] S.Y. Kung, Zejiang Hou, and Yuchen Liu. 2019. Methodical Design and Trim-
ming of Deep Learning Networks: Enhancing External BP Learning with Inter-
nal Omnipresent-supervision Training Paradigm. In ICASSP 2019-2019 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,
8058â€“8062.

[39] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor
Lempitsky. 2014. Speeding-up convolutional neural networks using ï¬ne-tuned
cp-decomposition. arXiv preprint arXiv:1412.6553 (2014).

[40] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haï¬€ner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998).
[41] Erich L Lehmann and Joseph P Romano. 2006. Testing statistical hypotheses.

Springer Science & Business Media.

[42] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016.
Pruning ï¬lters for eï¬ƒcient convnets. arXiv preprint arXiv:1608.08710 (2016).
[43] Yuchao Li, Shaohui Lin, Jianzhuang Liu, Qixiang Ye, Mengdi Wang, Fei Chao, Fan
Yang, Jincheng Ma, Qi Tian, and Rongrong Ji. 2021. Towards Compact CNNs
via Collaborative Compression. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 6438â€“6447.

[44] Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David Doermann,
Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2019. Exploiting kernel spar-
sity and entropy for interpretable CNN compression. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 2800â€“2809.

[45] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. 2017. Runtime neural pruning. In

Advances in Neural Information Processing Systems. 2181â€“2191.

[46] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,
Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. 2018. Progressive
neural architecture search. In Proceedings of the European Conference on Com-
puter Vision (ECCV). 19â€“34.

[47] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Ko-
ray Kavukcuoglu. 2017. Hierarchical representations for eï¬ƒcient architecture
search. arXiv preprint arXiv:1711.00436 (2017).

[48] Yuchen Liu, Zhixin Shu, Yijun Li, Zhe Lin, Federico Perazzi, and Sun-Yuan Kung.
2021. Content-aware gan compression. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 12156â€“12166.

[49] Yuchen Liu, David Wentzlaï¬€, and SY Kung. 2020.

Rethinking Class-
Discrimination Based CNN Channel Pruning. arXiv preprint arXiv:2004.14492
(2020).

[50] Yuchen Liu, David Wentzlaï¬€, and SY Kung. 2021. Class-Discriminative CNN

Compression. arXiv preprint arXiv:2110.10864 (2021).

[51] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Chang-
shui Zhang. 2017. Learning eï¬ƒcient convolutional networks through network
slimming. In Proceedings of the IEEE International Conference on Computer Vision.
2736â€“2744.

[52] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting
Cheng, and Jian Sun. 2019. Metapruning: Meta learning for automatic neural
network channel pruning. In Proceedings of the IEEE International Conference on
Computer Vision. 3296â€“3305.

[53] Christos Louizos, Max Welling, and Diederik P Kingma. 2017. Learning Sparse
Neural Networks through ğ¿_0 Regularization. arXiv preprint arXiv:1712.01312

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

Liu et al.

arXiv preprint arXiv:1802.00124 (2018).

[79] Andy B Yoo, Morris A Jette, and Mark Grondona. 2003. Slurm: Simple linux
utility for resource management. In Workshop on Job Scheduling Strategies for
Parallel Processing. Springer, 44â€“60.

[80] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han,
Mingfei Gao, Ching-Yung Lin, and Larry S Davis. 2018. Nisp: Pruning networks
using neuron importance score propagation. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition. 9194â€“9203.

[81] Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad,
and Yanzhi Wang. 2018. A systematic dnn weight pruning framework using
alternating direction method of multipliers. In Proceedings of the European Con-
ference on Computer Vision (ECCV). 184â€“199.

[82] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao
Wu, Junzhou Huang, and Jinhui Zhu. 2018. Discrimination-aware channel prun-
ing for deep neural networks. In Advances in Neural Information Processing Sys-
tems. 875â€“886.

[83] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement

learning. arXiv preprint arXiv:1611.01578 (2016).

[84] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. 2018. Learning
transferable architectures for scalable image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recognition. 8697â€“8710.

(2017).

[54] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. 2017. Thinet: A ï¬lter level pruning
method for deep neural network compression. In Proceedings of the IEEE inter-
national conference on computer vision. 5058â€“5066.

[55] Man-Wai Mak and Sun-Yuan Kung. 2006. A solution to the curse of dimensional-
ity problem in pairwise scoring techniques. In International Conference on Neural
Information Processing. Springer, 314â€“323.

[56] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink,
Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duï¬€y,
et al. 2019. Evolving deep neural networks. In Artiï¬cial Intelligence in the Age
of Neural Networks and Brain Computing. Elsevier, 293â€“312.

[57] Yurii E Nesterov. 1983. A method for solving the convex programming problem
with convergence rate O (1/kË† 2). In Dokl. akad. nauk Sssr, Vol. 269. 543â€“547.
[58] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-
drew Y Ng. 2011. Reading digits in natural images with unsupervised feature
learning. (2011).

[59] Xuefei Ning, Tianchen Zhao, Wenshuo Li, Peng Lei, Yu Wang, and Huazhong
Yang. 2020. Dsa: More eï¬ƒcient budgeted pruning via diï¬€erentiable sparsity allo-
cation. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK,
August 23â€“28, 2020, Proceedings, Part III 16. Springer, 592â€“607.

[60] Damien Oâ€™Neill, Bing Xue, and Mengjie Zhang. 2020. Neural architecture search
for sparse DenseNets with dynamic compression. In Proceedings of the 2020 Ge-
netic and Evolutionary Computation Conference. 386â€“394.

[61] Paul Pavlidis, Jason Weston, Jinsong Cai, and William Noble Grundy. 2001. Gene
functional classiï¬cation from heterogeneous data. In Proceedings of the ï¬fth an-
nual international conference on Computational biology. 249â€“255.

[62] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2019. Regular-
ized evolution for image classiï¬er architecture search. In Proceedings of the aaai
conference on artiï¬cial intelligence, Vol. 33. 4780â€“4789.

[63] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Sue-
matsu, Jie Tan, Quoc V Le, and Alexey Kurakin. 2017. Large-scale evolution of
image classiï¬ers. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70. JMLR. org, 2902â€“2911.

[64] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. In Advances
in neural information processing systems. 91â€“99.

[65] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-
Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
4510â€“4520.

[66] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional net-

works for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[67] Nilotpal Sinha and Kuan-Wen Chen. 2021. Evolving neural architecture using
one shot model. In Proceedings of the Genetic and Evolutionary Computation Con-
ference. 910â€“918.

[68] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish,
Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. 2015.
Scalable bayesian optimization using deep neural networks. In International con-
ference on machine learning. 2171â€“2180.

[69] Kenneth O Stanley, Jeï¬€ Clune, Joel Lehman, and Risto Miikkulainen. 2019. De-
signing neural networks through neuroevolution. Nature Machine Intelligence 1,
1 (2019), 24â€“35.

[70] Kenneth O Stanley, David B Dâ€™Ambrosio, and Jason Gauci. 2009. A hypercube-
based encoding for evolving large-scale neural networks. Artiï¬cial life 15, 2
(2009), 185â€“212.

[71] Kenneth O Stanley and Risto Miikkulainen. 2002. Evolving neural networks
through augmenting topologies. Evolutionary computation 10, 2 (2002), 99â€“127.
[72] Masanori Suganuma, Shinichi Shirakawa, and Tomoharu Nagao. 2017. A ge-
netic programming approach to designing convolutional neural network archi-
tectures. In Proceedings of the Genetic and Evolutionary Computation Conference.
ACM, 497â€“504.

[73] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, An-
drew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architec-
ture search for mobile. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. 2820â€“2828.

[74] Mingxing Tan and Quoc V Le. 2019. Eï¬ƒcientnet: Rethinking model scaling for

convolutional neural networks. arXiv preprint arXiv:1905.11946 (2019).

[75] Paul Templier, Emmanuel Rachelson, and Dennis G Wilson. 2021. A geometric
encoding for neural network evolution. In Proceedings of the Genetic and Evolu-
tionary Computation Conference. 919â€“927.

[76] Olga Wichrowska, Niru Maheswaranathan, Matthew W Hoï¬€man, Sergio Gomez
Colmenarejo, Misha Denil, Nando de Freitas, and Jascha Sohl-Dickstein. 2017.
Learned optimizers that scale and generalize. In Proceedings of the 34th Interna-
tional Conference on Machine Learning-Volume 70. JMLR. org, 3751â€“3760.
[77] Lingxi Xie and Alan Yuille. 2017. Genetic cnn. In Proceedings of the IEEE Inter-

national Conference on Computer Vision. 1379â€“1388.

[78] Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. 2018. Rethinking the smaller-
norm-less-informative assumption in channel pruning of convolution layers.

Evolving Transferable Neural Pruning Functions

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

addition
subtraction
multiplication
division
absolute value
square
square root
adding ridge factor
matrix trace
matrix multiplication
matrix inversion
inner product
outer product
matrix/vector transpose
summation
product
mean
standard deviation
variance
counting measure
rbf kernel matrix getter
geometric median getter
tensor slicer

add(+)
sub(âˆ’)
mul(Ã—)
div(Ã·)
abs
sq
sqrt
ridge
tr
matmul
inv
dot
outprod
tran
sum{ğ‘ ,ğ‘” }
prod{ğ‘ ,ğ‘” }
mean{ğ‘ ,ğ‘” }
std{ğ‘ ,ğ‘” }
var{ğ‘ ,ğ‘” }
count{ğ‘ ,ğ‘” }
rbf
geo
slice

Elementwise
operators

Matrix
operators

Statistics
operators

Specialized
operators

Table 6: Detailed Operator Space

We organize our supplementary material as follows. In Sec. 9,
we present a more detailed table for the operator space and our im-
plementation of the state-of-the-art population (SOAP). In Sec. 10,
we include more experimental details of our evolution and pruning
study. We discuss more detailed settings and results of evolution
on ILSVRC-2012 in Sec. 11. Lastly, we present extra evolved func-
tions in Sec. 12.

9 SOAP IMPLEMENTATION
9.1 Operator Space
In Tab. 6, we present the detailed operator space with operators
and their abbreviations.

9.2 SOAP Functions
With the abbreviations of operators in Tab. 6 and the symbols of
operands presented in Tab. 1 of the main paper, we can thus give
the precise expressions of the functions in SOAP:

â€¢ Filterâ€™s â„“1-norm: sumğ‘” (abs(Wğ¼ ))
â€¢ Filterâ€™s â„“2-norm: sqrt(sumğ‘” (sq(Wğ¼ )))
â€¢ Batch normalizationâ€™s scaling factor: abs(slice(B))
â€¢ Filterâ€™s geometric median: sqrt(sumğ‘” (sq(Wğ¼ âˆ’ geo(W))))
â€¢ Discriminant Information:

countğ‘  (F +) Ã— matmul(tran(meanğ‘  (F +) âˆ’ meanğ‘  (F )),
inv(ridge(matmul(tran(F âˆ’meanğ‘  (F )), F âˆ’meanğ‘  (F )))),
meanğ‘  (F +) âˆ’ meanğ‘  (F ))
â€¢ Maximum Mean Discrepancy:

div(sumğ‘” (rbf (F +, F +)), sq(countğ‘  (F +)))
+ div(sumğ‘” (rbf (F âˆ’, F âˆ’)), sq(countğ‘  (F âˆ’)))
âˆ’div(sumğ‘” (rbf (F +, F âˆ’)), mul(countğ‘  (F +)), countğ‘  (F âˆ’)))âˆ’

div(sumğ‘” (rbf (F +, F âˆ’)), mul(countğ‘  (F +)), countğ‘  (F âˆ’)))

â€¢ Generalized Absolute SNR:

div(abs(meanğ‘” (F +) âˆ’ meanğ‘” (F âˆ’)), stdğ‘” (F +) + stdğ‘” (F âˆ’))

â€¢ Generalized Studentâ€™s T-Test:

div(abs(meanğ‘” (F +) âˆ’ meanğ‘” (F âˆ’)),
sqrt(div(varğ‘” (F +), countğ‘  (F +)) +
div(varğ‘” (F âˆ’), countğ‘  (F âˆ’))))

â€¢ Generalized Fisher Discriminat Ratio:

div(sq(meanğ‘” (F +) âˆ’ meanğ‘” (F âˆ’)), varğ‘” (F +) + varğ‘” (F âˆ’))

â€¢ Generalized Symmetric Divergence:

div(varğ‘” (F +), varğ‘” (F âˆ’)) + div(varğ‘” (F âˆ’), varğ‘” (F +))
+div(sq(meanğ‘” (F +)âˆ’meanğ‘” (F âˆ’)), varğ‘” (F +)+varğ‘” (F âˆ’))

10 EXPERIMENTAL DETAILS
10.1 Study on Fitness Combination Scheme
Preliminary Evolution. We conduct 10 preliminary experiments,
where the variables are: ğ›¼ âˆˆ {0, 0.3, 0.5, 0.7, 1} and combination
scheme âˆˆ {weighted geometric mean, weighted arithmetic mean}.
For each experiment, we have a population of 15 functions which
are evolved for 10 generations. The population is initialized with
10 individuals randomly cloned from SOAP and 5 random expres-
sion trees. The tournament size is 3, and the number of the selected
functions is 5. The next generation is reproduced only from the se-
lected functions. Other settings are the same as the main evolution
experiment.
CIFAR-100 Pruning. We apply the best evolved functions from
each preliminary evolution test to prune a ResNet-38 [27] on CIFAR-
100 [37]. The baseline ResNet-38 adopts the bottleneck block struc-
ture with an accuracy of 72.3%. We use each evolved function to
prune 40% of channels in all layers uniformly, resulting in a 54.7%/52.4%
FLOPs/parameter reduction. The network is then ï¬ne-tuned by
the SGD optimizer with 200 epochs. We use the Nesterov Momen-
tum [57] with a momentum of 0.9. The mini-batch size is set to
be 128, and the weight decay is set to be 1e-3. The training data is
transformed with a standard data augmentation scheme [27]. The
learning rate is initialized at 0.1 and divided by 10 at epoch 80 and
160.

10.2 Main Evolution Experiment
MNIST Pruning. On MNIST [40] pruning task, we prune a LeNet-
5 [40] with a baseline accuracy of 99.26% from shape of 20-50-800-
500 to 5-12-160-40. Such pruning process reduces 92.4% of FLOPs
and 98.0% of parameters. The pruned network is ï¬ne-tuned for 300
epochs with a batch size of 200 and a weight decay of 7e-5. We use
the Adam optimizer [35] with a constant learning rate of 5e-4.
CIFAR-10 Pruning. For CIFAR-10 [37] pruning, we adopt the
VGG-16 structure from [42] with a baseline accuracy of 93.7%. We
uniformly prune 40% of the channels from all layers resulting in
63.0% FLOPs reduction and 63.7% parameters reduction. The ï¬ne-
tuning process takes 200 epochs with a batch size of 128. We set
the weight decay to be 1e-3 and the dropout ratio to be 0.3. We use
the SGD optimizer with Nesterov momentum [57], where the mo-
mentum is set to be 0.9. We augment the training samples with a
standard data augmentation scheme [27]. The initial learning rate

GECCO â€™22, July 9â€“13, 2022, Boston, MA, USA

Liu et al.

e
u
l
a
V
s
s
e
n
t
i

F

64.0

63.5

63.0

62.5

62.0

61.5

61.0

60.5

60.0

Max Fitness
25 Percentile Fitness

0

5

10
Generation #

15

20

25

Figure 8: Function evolution on ImageNet.

|| Â¯ğ‘“ âˆ’ varğ‘” (F âˆ’)1||2
2
varğ‘” (F +) + varğ‘” (F âˆ’)
ğœ‰2 (C) = varğ‘” (F +)
ğœ‰3 (C) = varğ‘” (Wğ¼ )

12 EXTRA EVOLVED FUNCTIONS
We present additional evolved functions from our co-evolution strat-
egy:

ğœ‰1 (C) =

+ varğ‘” (F +)

(5)

(6)
(7)
Eqn. 5 presents a metric with the concept of SNR for classiï¬ca-
tion, while having a novel way of statistics combination. Moreover,
our evolution experiments ï¬nd that measuring the variance across
all elements in F + (Eqn. 6) and Wğ¼ (Eqn. 7) would help us identify
important channels empirically. These two functions are simple
and eï¬€ective yet remain undiscovered from the literature.

ğœ‰ğ¼ğ‘šğ‘ğ‘”ğ‘’ğ‘ ğ‘’ğ‘¡ (C) = (

varğ‘” (meanğ‘  (F +))
stdğ‘” (tr(F +)) Ã— meanğ‘” (F âˆ’)

)4 Ã· varğ‘” (sqrt(F ))

(4)

is set to be 0.006 and multiplied by 0.28 at 40% and 80% of the total
number of epochs.

10.3 Transfer Pruning
We implement the pruning experiments in TensorFlow [1] and
carry them out with NVIDIA Tesla P100 GPUs. CIFAR-100 con-
tains 50,000/10,000 training/test samples in 100 classes. SVHN is a
10-class dataset where we use 604,388 training images for network
training with a test set of 26,032 images. ILSVRC-2012 contains 1.28
million training images and 50 thousand validation images in 1000
classes. We adopt the standard data augmentation scheme [27] for
CIFAR-100 and ILSVRC-2012.

10.4 Channel Scoring
As many of our pruning functions require activation maps of the
channels to determine channelsâ€™ importance, we need to feed-forward
the input images for channel scoring. Speciï¬cally, for pruning ex-
periments on MNIST, CIFAR-10, and CIFAR-100, we use all their
training images to compute the channel scores. On SVHN and ILSVRC-
2012, we randomly sample 20 thousand and 10 thousand training
images for channel scoring, respectively.

11 EVOLUTION ON ILSVRC-2012
Evolution. We use ResNet-18 as the target network for pruning
function evolution on ILSVRC-2012. Since only one task is evalu-
ated, we directly use the retrained accuracy of the pruned network
as the functionâ€™s ï¬tness. Other evolution settings for population,
selection, mutation, and crossover are kept the same as Sec. 4 of
the main paper.
Evaluation. We uniformly prune 30% of channels in each layer
from a pretrained ResNet-18, resulting in a FLOPs reduction of
36.4%. Due to the constrained computational budget, we only ï¬ne-
tune it for 4 epochs using the SGD optimizer with Nesterov mo-
mentum [57]. We use a batch size of 128 and initialize our learning
rate at 0.001. The learning rate is multiplied by 0.4 at epoch 1 and
2.
Result. We show the evolution progress in Fig. 8. Due to the lack
of training budget, the pruned net is clearly not well retrained as
they only achieve around 63.5% accuracy, much lower than the
performance of methods shown in Tab. 5 of the main paper at
the similar pruning level. Such inadequate training results in a
imprecise function ï¬tness evaluation evidenced in Sec. 6 of the
main paper. Moreover, the best evolved function from this strategy,
ğœ‰ğ¼ğ‘šğ‘ğ‘”ğ‘’ğ‘ ğ‘’ğ‘¡ (Eqn. 4), performs inferior to the co-evolved function
ğœ‰âˆ— when transferred for CIFAR-100 pruning. These results demon-
strate the advantage of our small dataset co-evolution strategy in
cost-eï¬€ectiveness.

 
