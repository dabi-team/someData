A Survey on Artiﬁcial Intelligence for Source Code: A Dialogue Systems
Perspective

Erfan Al-Hossami and Samira Shaikh
Department of Computer Science
University of North Carolina at Charlotte
Charlotte, NC 28223
{ealhossa, sshaikh2}@uncc.edu

Abstract

In this survey paper, we overview ma-
jor deep learning methods used in Natu-
ral Language Processing (NLP) and source
code over the last 35 years. Next, we
present a survey of the applications of Arti-
ﬁcial Intelligence (AI) for source code, also
known as Code Intelligence (CI) and Pro-
gramming Language Processing (PLP). We
survey over 287 publications and present
a software-engineering centered taxonomy
for CI placing each of the works into one
category describing how it best assists the
Then, we
software development cycle.
overview the ﬁeld of conversational assis-
tants and their applications in software en-
gineering and education. Lastly, we high-
light research opportunities at the intersec-
tion of AI for code and conversational as-
sistants and provide future directions for re-
searching conversational assistants with CI
capabilities.

1

Introduction & Motivation

Conversational Assistants, also known as task-
oriented dialogue systems, are very widely used
and accessible such as Siri and Alexa. These
assistants have been increasingly used to assist
human users in a variety of tasks such as re-
serving hotels, booking ﬂights, or forecasting the
weather.
In recent years, we have also seen ad-
vancements in the ﬁeld of Artiﬁcial Intelligence
(AI) applied to source code also known as Code
Intelligence (CI) and Programming Language Pro-
cessing (PLP). Github Copilot1 powered by GPT-
Codex (Chen et al., 2021a) is currently in beta
as an Integrated Development Environment plugin
that is able to assist software developers as a pair-
programmer by suggesting code snippets or writ-
ing source code on its own. Given these advance-
ments, perhaps task-oriented bots can be equipped

1https://copilot.github.com/

with more capabilities to assist humans in cog-
nitively demanding tasks, including programming
by professionals or novices. By building mod-
els and tools that can generate both language and
code, we could potentially better understand the
cognitive basis of programming which can have
key impacts on computer science education prac-
tices (Fedorenko et al., 2019). This survey is struc-
tured as follows: Section §2 explores general deep
learning techniques that have been used to model
language and source code over the last 35 years.
Section §3 surveys the ﬁeld of CI with a systemic
review (a) on datasets containing natural language
and executable code (§3.8.2), and (b) of all meth-
ods used to generate source code from natural lan-
guage on the CoNaLa dataset, a popular python
code generation benchmark (§3.8.4). Section §4
overviews the ﬁeld of conversational artiﬁcial in-
telligence and its applications in software engi-
neering and education. Finally, Section §5 high-
lights research opportunities at the intersection of
CI and conversational assistants to provide future
directions for research in this new area.

2 Deep Learning Methods

This section overviews major developments over
the last 35 years in deep learning neural archi-
tectures that are used to generate and understand
both natural language and source code. These
deep learning methods can be used to generate
natural language (e.g. by chatbots (Zhang et al.,
2020)).
In the context of code generation, deep
learning methods have been used to generate code
snippets (e.g.
(Liguori et al., 2021b,a; Frem-
pong et al., 2021; Yin and Neubig, 2018)). The
need for neural architectures arose as deep learn-
ing was tasked to solve speciﬁc problems such
as machine translation, question-answering, and
code generation. Neural architectures describe the
general structure of the neural network(s), includ-
ing how many layers it has and how units in these

2
2
0
2

b
e
F
0
1

]
L
C
.
s
c
[

1
v
7
4
8
4
0
.
2
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
layers are connected to each other (Sarkar et al.,
2019). Neural models can solve problems speciﬁc
to the target domain through abstract modeling.
For instance, in natural language, often, word or-
der matters to the semantics of a sentence (Payne,
1992). Furthermore, word order differences be-
tween languages can be problematic when trans-
lating from one language to another (Jurafsky and
Martin, 2000). How can neural architectures con-
sider the order of the input sequence? We explore
relevant neural architectures further in the follow-
ing subsections.

2.1 Multilayered Perceptrons

Here we introduce the multi-layered perceptron
(MLP) as well as common terms associated with
neural networks, including activation functions,
loss functions, and gradient descent, and back-
propagation.

A multi-layered perceptron (MLP), commonly
known as an artiﬁcial neural network, is a vari-
ant of the Perceptron model (Rosenblatt, 1961),
composed of an input layer, multiple hidden lay-
ers fully connected with each other, and an output
layer (c.f. Fig. 1). MLPs excel at learning the in-
put representations and mapping them to the out-
put variable. The input given to neural networks
is numerical vectors. Often, categorical inputs are
one-hot encoded.

Figure 1: The Multilayer Perceptron architecture
is composed of an input layer, an output layer and
multiple hidden layers. Figure from (Yuen et al.,
2013).

Each layer is made up of several neurons. A
neuron is a unit that takes in weighted inputs, a
bias (an input of 1.0 with a computed weight),
and computes an output using an activation func-

tion. Weights can be initialized to a constant num-
ber, a random number, or through more nuanced
means such as the initialization proposed by He et
al. (2015). Each neuron computes the dot product
of its input vector Z = (z1, z2, ...zn) and weight
vector W = (w1, w2, wn) through x = Z · W .
Once the dot product is computed it is passed to
an activation function (see table 1) before being
passed on as the output of the neuron. The pro-
cess of neurons passing values forward in the net-
work using their activation functions until an out-
put is produced is called forward passing. Lastly,
another activation function called softmax is com-
monly used in the output layer. Given a vector
z from the last hidden layer, the softmax function
produces a vector σ(z) with length K. The num-
bers in σ(z) are values between 0 and 1. Further-
more, the sum of all elements in σ(z) equal to 1,
(cid:80)K
j=1 σ(z) = 1. This property of softmax is often
useful in classiﬁcation tasks. The softmax equa-
tion is illustrated below:

σ(zj) =

ezj
k ezk

(cid:80)

(1)

where j = 1, 2, ..., K.
Once the MLP network produces an output
(prediction), it is compared with the target val-
ues in the dataset (ground truth). This compar-
ison yields a measurement of how well the out-
put matches the expected values. There are sev-
eral methods, called loss functions, to measure
how well the predicted output minimizes the error.
Here are some examples of loss functions:

• Binary cross-entropy: It is also called the
log-loss applied for a two class classiﬁcation
problem.

• Categorical cross-entropy: log-loss applied

for a classiﬁcation task of N classes

• Mean Squared Error: Mean of the squared
sum of error, often used in regression tasks.

• Mean Absolute Error: Average magnitude
of the errors given a set of predictions.
It
gives higher weights to larger errors since it
squares the errors.

In the MLP, the computed error is propagated
back through each of the layers. This is done
so that the weights for each neuron can be up-
dated empirically. This process is called back-
propagation (Rumelhart et al., 1986). The weights

Activation Function

Binary Threshold

Deﬁnition
(cid:26) 1 , if x > 0
0 , otherwise

f (x) =

Sigmoid
Tanh

ReLu

f (x) = 1
1+e−x
f (x) = ex−e−x
ex+e−x
x , if x > 0
0 , otherwise OR max{0, x}

(cid:26)

f (x) =

Table 1: Example activation functions used in neural networks

a training set sample and a loss function. Other
algorithms are commonly used to further opti-
mize SGD such as Adaptive Moment Estimation
(Adam) (Kingma and Ba, 2014a). Adam further
optimizes SGD by computing adaptive learning
rates for each parameter. MLPs excel at mapping
inputs to outputs, however, these inputs are as-
sumed to be independent of one another. What if
the inputs are dependent? What if their order mat-
ters such as in word sequences forming sentences?
Next, we introduce another neural network archi-
tecture designed speciﬁcally to address this prob-
lem, the Recurrent Neural Networks.

2.2 Recurrent Neural Networks

Recurrent Neural Networks (RNNs) are special-
ized network architectures for processing a se-
quence of input values that are dependent upon
one another. RNNs are often applied in predict-
ing the next word/token in a sequence of words,
or in translating from one language to another.
RNNs follow an architecture distinct from that
of MLPs. RNNs are equipped with a feedback
loop. This feedback loop enables the RNN to
share weights v, u, w across different time steps,
meanwhile, MLPs use a different set of parame-
ters and weights each at each hidden layer. This is
illustrated in Figure 2 where Wh1 and Wh2 are dif-
ferent. We can imagine that the RNN is perform-
ing the same computations in each time step, but
with different inputs and updated weights. This
setup greatly reduces the number of parameters
required to learn a task. Figure 3 showcases the
RNN architecture with a single hidden unit. When
unfolding the RNN, we are left with multiple feed-
forward MLP networks, a network at each time
step t. The total number of time steps is equiva-
lent to the length of the input sequence.

While MLPs compute the hidden layer values
from the input values and weights exclusively, the
hidden layers in RNNs are computed from the in-

Figure 2: The MLP network has different and in-
dependent weights at each layer. Wh1 and Wh2
are different. Source: Medium post authored by
Mady.

can be updated after each training example, in a
process called online learning. This process can
result in big changes in the network from example
to example and is largely unstable. Alternatively,
computed errors can be saved for a set of training
examples and the MLP network weights are up-
dated after training on the set. This is called batch
learning. The learning rate α controls the size of
a change made to a weight, so the weights do not
change dramatically and learning can be more sta-
ble.

MLP networks are trained using a training
algorithm such as stochastic gradient descent
(SGD) (Bottou, 2010).

SGD is a simpliﬁed version of gradient descent
that computes the gradient descent for only a small
sample of training data to estimate the gradient
descent given a loss function for the entire train-
ing dataset. These optimization algorithms are re-
sponsible for updating weights in a model given

Figure 3: The Recurrent Neural Network architecture is composed of an input layer x, an output layer
o, a hidden layer h, and a feedback loop v for each time step t. The weights v, u, w are not different for
each layer. The weights are instead shared across all the time steps. Source: Wikipedia.

put value xt at time step t, weights, and the previ-
ous hidden layer value. This is illustrated below:

ht = σ(u ∗ xt + v ∗ ht−1)
ot = sof tmax(w ∗ ht)

(2)

where v is a weight vector for the different time
steps, σ as an activation function as described in
Table 1, xt is an input value at time step t, u is a
weight vector for the hidden layer, w is a weight
vector for the output layer, and ot is the output
value at time t. Softmax is described in Equation 1
and it is used on the product of the hidden state ht
and the weight vector w. The error is computed us-
ing a loss function E (loss functions are described
in section 2.1) for output ot and target sequence
st. This yields an error Et. As for the total er-
ror across the different timestamps it is computed
through summing all of Et for an input length of
n (cid:80)n
t=0 Et. The weights are updated using back-
propagation (Rumelhart et al., 1986) as described
in section 2.1, with a small tweak, the current time
step is dependent on the previous time step, so the
back-propagation traverses back to the ﬁrst time
step. u,v, and w are updated using SGD (Bottou,
2010). There is a big drawback for RNNs how-
ever, as we go back to adjust the weights during
back-propagation, the signal gets close to zero this
is called vanishing gradient, or it grows exponen-
tially, this is called exploding gradient. This is an
issue in particular when we expect RNNs to ’re-
member’ and keep track of long-term dependen-
cies. For that researchers have developed a spe-
cial type of RNNs, the Long-Short Term Memory
(LSTM).

2.3 Long-Short Term Memory

Recurrent Neural Networks (RNNs) lose historical
context and dependencies over longer periods. To
address the problem of memory in RNNs Hochre-
iter et al. (1997) proposed a novel RNN architec-
ture called the Long-Short Term Memory (LSTM)
It excels at maintaining long-term de-
in 1997.
pendencies and preventing problems such as the
exploding and vanishing gradient issues. LSTMs
are a variant, or a type, of RNNs. Their similarity
is denoted in Figure 4.

LSTMs contain gates which enable the LSTM
to add or remove information to a cell state. Each
gate is like a one-layered neural network with each
with its weight vector and biases. These networks
then learn what information is relevant and what
is not throughout training. LSTMs consist of three
gates: The input, output, and forget gates. As
shown in Figure 5, an LSTM cell is composed of 3
different gates. At a given time step, the input gate
allows or denies incoming signals from the input
layer. The output gate controls what gets passed
on to other cells or layers as the cell’s ﬁnal output.
The forget gate administers the self-recurrent con-
nection to forget or remember previous cell states
as deemed ﬁt.

The LSTM cell architecture is displayed in Fig-
ure 6. Let C indicate a cell state, h the hidden
state for each time step t. Let I, F , and O, rep-
resent the input, forget, and output gates respec-
tively. All gates are regulated by a sigmoid σ func-
tion, which as mentioned earlier outputs a number
between 0 and 1, controlling how much of the out-
puts of each gate should get passed along to the
cell state. The processes of LSTM computations

Figure 4: The Long-Short Term Memory (LSTM) network is similar to the Recurrent Neural Network
(RNN) except the hidden state is replaced with an LSTM cell. Similarly to Figure 3 n represents a
timestep, x(n) represents the input at time n, U represents a weight vector for the LSTM cell, W repre-
sents the weight vector in the feedback loop, C represents the LSTM cell state, V represents the weight
vector for the output layer, and o(n) represents the output at time n. Source: CodeEmporium on Youtube.

corresponding to each number in the previous
hidden state and current input vectors, indi-
cating how much should be remembered (if
the value is greater than 0) and forgotten (if
the value is 0). The process is described in
mathematical notation in equation 3.

2. Deciding what to write to memory: This
decision is handled by the input layer. The
input layer utilizes information from the pre-
vious hidden state ht−1 , the current input
xt, its own weight vector WI , and its own
bias bI . ht−1 and xt are concatenated and
then are multiplied in a dot product with WI
and summed with bI . The result of that is
then squashed through the sigmoid function
σ. The result is It a vector of numbers be-
tween 0 and 1, corresponding to each num-
ber in the previous hidden state and current
input vectors, indicating where should new
information should be written to the current
cell state (memory). Next, another layer ˜Ct,
decides what new information are good can-
didates to be written to the current cell state
using tanh as its activation function. The pro-
cess is described in mathematical notation in
equation 4.

3. Updating the memory: The current cell

Figure 5: The LSTM is composed of 3 different
gates: 1) an input gate controlling what is updated
in the cell state, 2) a forget gate controlling what
is omitted from the previous cell states (memory),
and 3) an output gate controls what is passed on as
the ﬁnal output of the cell.

can be illustrated in four steps:

1. Deciding what to forget and what to re-
member: This decision is made by the for-
get gate at each time step. The forget gate
utilizes information from the previous hid-
den state ht−1 , the current input xt, its own
weight vector WF , and its own bias bF . ht−1
and xt are concatenated and they are multi-
plied in a dot product with WF and summed
with bF . The result of that is then squashed
through the sigmoid function σ. The result
is Ft a vector of numbers between 0 and 1,

Figure 6: The Long-Short Term Memory (LSTM) cell architecture. At teach time step t, an LSTM cell
takes in the previous cell state and the previous hidden state as inputs. It contains multiple sigmoid and
tanh functions to compute on those inputs for each of the 3 gates (input, forget, and output). The LSTM
cell outputs, a cell state Ct, and a hidden state ht that is passed onto other layers to produce an output ot,
and to LSTM cell’s future self at time step t + 1. Source: Colah’s blog.

state is decided by utilizing the forget gate
vector output Ft, erasing things that are de-
termined to be forgettable from the previous
cell state Ct−1, and the new information from
step 2 is added after scaling by the input gate
vector It. The process is described in mathe-
matical notation in equation 5.

4. Output: The ﬁnal output for the LSTM cell
is decided by the current input xt, and the
current cell state Ct. The output gate vector
Ot decides which parts of the cell state are
going to be outputted and is computed using
the previous hidden state ht−1 and the cur-
rent input xt. Then, the current cell state Ct
is passed through a tanh function and mul-
tiplied by the output gate vector giving us ht
which is passed on as the output of the LSTM
to other cells and a copy to itself. The pro-
cess is described in mathematical notation in
equation 6.

It = σ(WI
˜Ct = tanh(WC

˙[ht−1, xt] + bI )

˙[ht−1, xt] + bC)

Ct = Ft ∗ Ct−1 + It ∗ ˜Ct

Ot = σ(WO

˙[ht−1, xt] + bO)

ht = Ot ∗ tanh(Ct)

(4)

(5)

(6)

LSTM hidden states (as seen in ﬁgure 6) can
be connected to a softmax layer for text classi-
ﬁcation problems or it can be connected to an-
other LSTM cell composing several LSTM lay-
ers. These LSTM layers would output a sequence
of hidden state vectors, a vector for each input
in a timestamp. This hierarchy of LSTMs lay-
ers enables a more complex representation of se-
quential data and it is often referred to as stacked
LSTMs. In the next section, we discuss an archi-
tecture that often utilizes stacked LSTMs to gen-
erate language.

2.4 Sequence-to-Sequence

Ft = σ(WF

˙[ht−1, xt] + bF )

In this
sequence-to-sequence (Seq2Seq) architecture.

subsection, we describe the neural
It

(3)

Figure 7: A Sequence to Sequence architecture is composed of an encoder and decoder LSTM networks.
Source: Jalammar’s Blog.

is used to generate a sequence of tokens in a tar-
get language given a sequence of tokens from a
source language. Seq2Seq learning maximizes
the likelihood (or potentially other evaluation met-
rics (Shen et al., 2016)) of the target sequence
(conversational response) given an input source
sequence (user utterances).

Typically, Encoder-Decoder architectures using
RNNs and applied to solve Seq2Seq tasks were
ﬁrst proposed by (Cho et al., 2014) and (Sutskever
In practice, gated RNNs such as
et al., 2014).
LSTMs (Hochreiter and Schmidhuber, 1997) tend
to perform better over vanilla RNNs (Gu et al.,
2016a).

Notably, Bahadanau et al. (2014) propose an
encoder-decoder architecture with attention, using
a bi-directional LSTM as the encoder to transform
an embedded source sequence E = |e1, ..., eTS |
into a vector c of hidden states with equal length.
This vector is known as the context vector or the
thought vector.

Each hidden state ht corresponds to an embed-
ded token et Each hidden state is computed by
concatenating the hidden states of the forward and
backward orders as follows:

ht = [f (et, ht−1); f (et, ht+1)];

(7)

c = φ({h1, ..., hTS })

where {ht} are hidden LSTM states, f is a stan-
dard LSTM update function, and φ summarizes
the hidden states. The decoder LSTM (recurrent
neural network (RNN) architecture) (Luong et al.,
2015a) generates the target token a using the con-
ditional probability deﬁned in Eq. 8 which takes in
the context vector c and generates target token a at
time t using the following conditional probability:

st = f (at−1, st−1, c)

p(at|a<t, E) = g(at−1, st, c))

(8)

where st is the decoder LSTM state at time t, at
is the conversational response token at t using
function g. a<t denotes previous predicted tokens
{a1, ..., at−1}.

The context vector by itself is of a ﬁxed length
and hence it struggles with long sequences or sen-
tences. To address this, the Seq2Seq model often
comes with an attention mechanism. A noteable
attention mechanism is the Bahdanau-style atten-
tion mechanism described in (Bahdanau et al.,
2014), in which it considers a dynamic context ct
in the decoding process. This attender attends to
the entire input space (i.e. soft attention) by repre-
senting ct as the weighted sum of the source hid-
den states as follows:

ct = (cid:80)TS

i=1 αt,i hi

αt,i = sof tmax(score(st−1, hi))

(9)

score(st, hi) = v(cid:62)

z tanh(Wz[st; hi])

where αt,i represents how well a target token at
and source token ei align, TS represents the last
hidden state in c, st is the decoder LSTM state at
time t, and lastly, vz and Wz are weight matrices to
be learned by the alignment model. The score αt,i
is parametrized by a feed-forward multi-layer per-
ceptron neural network. Since the encoder uses a
bi-directional LSTM each hidden state hi is aware
of the context on both ends.

In this section, we looked at the Seq2Seq model
that uses an encoder LSTM network that processes
input into a context vector and a decoder LSTM

Figure 8: The sequence to sequence attention mechanism computing attention weights for each of the
hidden states from the encoder network forming the context vector ct at time t that is used by the decoder
LSTMs. Figure adapted from (Luong et al., 2015b). Adapted by: SyncedReview on Medium.

network that produces the output one token at a
time. We noted how attention mechanisms weigh
tokens based on how important they are at the cur-
rent time step. In the next section, we discuss a
novel architecture, that still manages to perform
well on very long sequences and is not as vulnera-
ble to over-ﬁtting as LSTMs.

2.5 Transformers & Transfer Learning

In this section, we discuss the Transformer
architecture proposed by Vaswani et al. (2017).
In the previous section, we’ve touched on the
attention mechanisms and how they supplemented
RNNs in Seq2Seq models to model
longer
sequences. The Transformer architecture takes
attention a step further to improve state of the
In the infamous paper Attention is All you
art.
Need, Vaswani et al. (2017) propose a new
encoder-decoder architecture with a novel atten-
tion mechanism. The novel attention mechanism
called the multi-head attention not only enables
models to model longer sequences of text more
effectively. The Transformer model also opens
the door for transfer learning in natural language
processing and generation.

Transfer learning is a situation where what
has been learned in one setting is exploited to

improve generalization in another setting. Pan
et al. (2010) deﬁne transfer learning as follows,
consider a domain D consisting of a feature
space χ and a marginal probability P (X) and
X = {x1, ..., xn}. Here, xi is a speciﬁc vector
and X ∈ χ . Consider task T , consisting of a
label space γ and an objective function P (γ|X).
Transfer learning is the process aimed at
im-
proving target task TT in the target domain DT
using knowledge from the source task TS in
the domain DS.
Inductive learning is inferring
mapping from a set of training data samples. The
trained model then works with inductive bias,
a set of assumptions related to the training data
used to generalize on unseen data. Deep learning
techniques
algorithms use inductive transfer
that utilize inductive biases of the source task
to assist in the target task. To apply transfer
learning, several methods are used: 1) Feature
extraction, using pre-trained networks without
its ﬁnal
layer as a feature extractor for other
tasks. 2) Fine-tuning, where selected layers are
re-trained from a pre-trained model, while other
layers are frozen (weights are ﬁxed). 3) Pretrained
models, are models that are trained and perform
well on a source task TS, and the model weights
and parameters are saved and then re-trained
to perform a target task TT . There are several
types of transfer learning: 1) Domain adaptation,

Figure 9: Overview of the Transformer architecture proposed in (Vaswani et al., 2017). It is composed
of embeddings, positional encoders, stacked encoders (left), and stacked decoders (right),

noted when there is a data shift between the
source and target domains, usually the marginal
probabilities P (XS) (cid:54)= P (XT ). An example of
domain adaptation can be training a pre-trained
sentiment classiﬁer for movie reviews to classify
product review sentiments. 2) Domain confusion
aims to make the domain representations as
similar as possible by applying pre-processing
steps on the representations. The basic idea is
to add another objective to the source model to
encourage similarity between the domains by
confusing the domain itself (Ganin et al., 2016).
3) Multitask learning, where the model receives
information about multiple tasks simultaneously.
4) One and few-shot learning, where the model
infers the required output based on one or a few
training examples respectively (Fei-Fei et al.,
2006). 5) Zero-shot learning, where the model
relies on no examples to learn a task.

The Transformer architecture is composed
of 6 stacked encoders and 6 stacked decoders
along with embeddings for both the inputs
and outputs and positional encoders. Figure 9
overviews the proposed architecture.

We will ﬁrst start by describing the encoders
and decoders. The encoder receives inputs and
passes them through the multi-headed attention.
Multi-headed attention in each encoder is a type
of self-attention. Self-attention helps the encoder
map relationships between the current input to-
ken and other words in the input sequence as it
encodes a speciﬁc word or a token (Cheng et al.,
2016). Then the output of the attention is fed into
a feedforward (MLP) neural network. The output
of each encoder is fed into another encoder until it
reaches the last encoder, the sixth encoder. Each
decoder receives the output of the sixth encoder
and the target language input. It has both a feed-
forward network and multi-headed attention but it
also has another attention mechanism, the masked
multi-headed attention. The masked multi-headed
attention mechanism is also a self-attention mech-
anism, which takes in the output sequence as its
input. It is modiﬁed to prevent attending to sub-
sequent tokens/words so that predictions for the
current timestep are only dependent on previously
known output tokens. All the produced matrices
are added with either the previous layer or the em-
bedding and normalized according to (Ba et al.,

2016) in between as indicated in Figure 9.

The multi-headed attention has an overall algo-
rithmic complexity of O(N 2) and is made up of
8 heads h = 8 running in parallel. Each token in
a sequence has an embedding vector. An embed-
ding is a numerical form of a word that preserves
its meaning by using dimensionality reduction and
maintaining context. The paper uses 512 dimen-
sions for the embedding size (dmodel = 512).
Each attention head takes in 3 input vectors for
each input token: the query vector Q, the key vec-
tor K, and the value vector V . Each of these vec-
tors has a dimension of 64. These vectors are com-
puted by multiplying the embedded input token
with 3 different weight matrices: W Q, W K, and
W V respectively. The output of the attention head
is computed as a weighted sum of the values vec-
tor. dk is the dimension of the key vectors which
is 64.

QKT
√
dk

)V

Attention(Q, K, V ) = sof tmax(

√

(10)
To compute the attention for each head we start
by computing the dot product of the query and the
transposition of the key vector. The output is then
dk which is 8. Next, the softmax func-
scaled by
tion is applied so that all the values are between 0
and 1. Lastly, we compute the dot product of the
softmax output with the values vector. The process
is called the Scaled Dot-Product Attention and is
described in Equation 10.

To compute the multi-head attention we take
the output from each attention head, concatenate
them, and multiply them with a bigger weight ma-
trix W O as described in Equation 11.

M ultiHead(Q, K, V ) = Concat(head1, ..., headh)W O

(11)
The weight matrices W Q, W K, W V , and W O
are all learned and updated throughout the train-
ing process which greatly assists in the process of
transfer learning. Each of the 8 attention heads can
focus on a different relationship between the input
words (e.g., one for pronoun-noun, one for gender,
etc.)

Another component is positional encoding ap-
plied to both the input and output sequences. The
positional encoding has a dimension of 512 which
is the same as the embeddings. Positional encod-
ings provide a vector to be added to the input em-
bedding. This provides ﬁxed positioning of to-

kens during training which allows further extrap-
olation of longer sequences. The paper uses sinu-
soidal version of positional embeddings described
in Equation 12, where pos is the position and i is
the dimension. Each dimension of a positional en-
coding corresponds to a sinusoid which goes from
2π to 10000.

P E(pos,2i) = sin(pos/100002i/512)
P E(pos,2i+1) = cos(pos/100002i/512)

(12)

There are many applications of the Transformer
model and it is not restricted to the ﬁeld of NLP.
We note in particular BERT (Devlin et al., 2018)
which advanced state of the art for language mod-
eling and GPT-3 (Brown et al., 2020) which ad-
vanced language generation. In the next section,
we will discuss the GPT models.

2.6 GPT Models

In the section, we describe a speciﬁc transformer
architecture,
the Generative Pretrained Trans-
former family of models.

2.6.1 GPT-1

Prior to GPT-1 most NLP models required an-
notated corpora for natural language understand-
ing. Annotated data for speciﬁc natural
lan-
guage understanding (NLU) tasks are scarce while
large unlabeled corpora are abundant. GPT-1,
or Generative Pre-trained Transformer 1, intro-
duces the concept of generative pre-training on
large unlabeled text and then ﬁne-tunes for spe-
ciﬁc NLU tasks (Radford et al., a). GPT-1 demon-
strates signiﬁcant gains in several NLU tasks and
marks a signiﬁcant step towards the trend of
task-agnostic NLP models. Next, we describe
the semi-supervised learning process (pre-training
then ﬁne-tuning) more in detail.

The unsupervised pre-training is a language
model is tasked with maximizing the following
objective for a corpus of tokens U = u1, ..., un:

L1(U ) =

(cid:88)

i

logP (ui|ui−k, ..., ui−1; Θ)

(13)

where k is the size of the context window, P
is a conditional probability modeled by a neural
network with parameters Θ. The parameters are
trained using stochastic gradient descent (SGD)
described earlier in 2.1.

Figure 10: The GPT-1 architecture proposed in (Radford et al., a). It is composed of 12 stacked de-
coders. Each decoder containing self-attention, followed by a position-wise feed-forward network with
normalization in between.

The ﬁne-tuning training (supervised training)
takes in a labeled dataset C with a sequence of
tokens x1, ..., xm, and a label y for the token se-
quence and aims to maximize the objective given
the pre-training objective L1, and a weight λ set to
0.5 in the paper:

L2(C) = (cid:80)

(x,y) logP (y|x1, ..., xm)

L3(C) = L2(C) + λL1(C)

(14)

Using the language modeling (pre-training) objec-
tive helps with improving the supervised model’s
generalization and accelerating convergence (Rad-
ford et al., a).
The GPT Architecture
is composed of 12
stacked transformer decoder layers (see the de-
coder component in Figure 9), with multi-headed
masked self-attention. The decoder layer had
the embedding size is 768
12 attention heads,
(dmodel = 768), an Adam optimizer (Kingma and
Ba, 2014b) was used with a learning rate of α =
2.5 × 10−4, and a drop out rate of 0.1. As shown
in Figure 10, the model applies its multi-headed
masked self-attention on the input tokens. Follow-
ing attention, are position-wise feed-forward lay-
ers generate a distribution over the target tokens.
The dimension of the position-wise feed-forward
layer is 3072. During pre-training, the output dis-
tribution P (u) for a token u is computed through

the following:

h0 = U We + Wp
hi = decoder(hi−1)∀i ∈ [1, n]
P (u) = softmax(hnW T
e )

(15)

Here, U = uk, ..., u1 is the context vector of to-
kens starting from k the size of the context win-
dow, n is the number of layers (12 layers), and
We is the token embedding matrix, and Wp is the
position embedding matrix. We observe that the
ﬁrst decoder layer h0 takes in the embedding ma-
trix and position embedding matrix. Subsequent
decoder layers take in the previous layer, and soft-
max is applied at the last transformer layer and
maximizes for Equation 13. When ﬁne-tuning
GPT-1, input tokens x1, ..., xm which come along
with label y are passed through the pre-trained
model to obtain the last decoder activation hm
. It
l
is then passed to the linear layer (shown in Fig-
ure 9) with parameters Wy to predict y and maxi-
mizes for Equation 14 :

P (y|x1, ..., xm) = softmax(hm

l Wy)

(16)

The model was trained for 100 epochs with mini-
batches of 64 and a sequence length (context win-
dow) of 512 and used GELU (Gaussian Error Lin-
ear Unit) (Hendrycks and Gimpel, 2016) as its ac-
tivation function. Byte Pair Encoding (BPE) (Sen-
nrich et al., 2016) with a vocabulary size of 40,000
was used. GPT-1 had 117 million parameters in to-
tal. The Bookcorpus dataset (Zhu et al., 2015) was

used for the pre-training stage. GELU is approx-
imated by the following using tanh described in
section 2.1:

GELU (x) = 0.5x(1 + tanh[
(cid:112)2/π(x + 0.044715x3)])

(17)

2.6.2 GPT-2

The Generative Pre-trained Transformer 2 (GPT-
2) serves as the ﬁrst attempt at creating a pre-
trained language model that can perform down-
stream tasks without any ﬁne-tuning or architec-
tural change. This is also known as the zero-
shot setting. To achieve this goal, GPT-2 updates
the objective function from the pre-training objec-
tive P (output|input) to include a particular task
P (output|input, task) (Radford et al., b). This is
called task conditioning. With task conditioning
GPT-2 can produce different outputs for the same
input given a different task. The task is passed
as a natural language description (sequence of to-
kens describing the task). GPT-2 is trained using a
dataset called WebText2 which is around 40 GB in
size (Radford et al., b). GPT-2 experiments high-
lighted that the perplexity (intrinsic evaluation) of
language models on the same dataset decreases the
bigger the language model in terms of parameters.
The GPT-2 Architecture follows the architecture
of GPT (Radford et al., a) with a few notable dif-
ferences. GPT-2 is made up of 48 stacked trans-
former decoder layers. The normalization layers
were moved to the input of the self-attention and
feed-forward sub-blocks. There is also an addi-
tional normalization layer was added after the ﬁ-
nal self-attention block. Weight initialization was
also modiﬁed by a factor of 1/
N where N is
the number of residual layers. The word embed-
ding dimension was increased to 1600 (dmodel =
1600). Byte Pair Encoding (BPE) (Sennrich et al.,
2016) with an increased vocabulary size of 50,257
was used. The model was trained with a larger
batch size of 512 and a larger context window of
1024. This resulted in a total of 1.5B parameters
for GPT-2 which is almost 10 times bigger than
GPT-1.

√

2.6.3 GPT-3

Large transformers such as BERT (Devlin et al.,
2018), and GPT-2 (Radford et al., b) are task ag-

nostic in architecture, however, they require ﬁne-
tuning on speciﬁc tasks using thousands of ex-
amples.
In a new transformer-based architec-
ture, researchers at OpenAI explore greatly scal-
ing up transformer-based language models to ex-
plore its task-agnostic performance using few-
shot and zero-shot learning. GPT-3, or Genera-
tive Pre-trained Transformer 3, is a massive auto
regressive transformer-based model that under-
went unsupervised pre-training on large corpora
(over 300 billion tokens of text or 45 TB). The
data sources include 3 years of ﬁltered Common-
Crawl (Wenzek et al., 2020), Wikipedia, and Web-
Text. GPT-3’s pre-training objective is to predict
the next word given a sequence of words. GPT-3 is
massive, containing about 175 billion parameters,
making it one of the world’s largest pre-trained
transformer models for language generation in the
world. GPT-3 is used without gradient updates
nor ﬁne-tuning but rather with few-short demon-
strative interactions with the model. Researchers
have tested the model on various NLP tasks in-
cluding question answering, translation, and cloze
tasks competing with state-of-the-art models with-
out ﬁne-tuning. GPT-3 generates language one to-
ken at a time, just like previously mentioned mod-
els, given a sequence of tokens as input in many
different domains. Upon its release to the pub-
lic enthusiasts have used GPT-3 for many differ-
ent applications. These applications include main-
stream tasks such as translating into different lan-
guages, writing google ads and has been particu-
larly used to generate code from natural language
such as SQL, Bash, and Python. For a thorough
list of GPT-3 applications please refer to this3.
The GPT-3 Architecture is the same as that
of a GPT-2 model (Radford et al., b).
Simi-
lar to the GPT-2 (Radford et al., b), the GPT-3
model is composed of stacked transfomer Decoder
layers (see the decoder component in Figure 9)
and it includes the modiﬁed initialization, pre-
normalization, and reversible tokenization. GPT-3
is different from GPT-2, in that it uses alternating
dense and banded sparse attention patterns in the
transformer layers proposed in the Sparse Trans-
former paper (Child et al., 2019). Open AI re-
searchers have experimented with different GPT-
3 conﬁgurations, sizes, and parameters. An ex-
ample experiment is shown in Figure 11. The

2https://github.com/jcpeterson/

3https://github.com/elyase/

openwebtext

awesome-gpt3

Figure 11: Figure from (Brown et al., 2020). Shows GPT-3 with different number of parameters making
more use of context (K) on a simple task requiring the model to remove random symbols from a word.

ﬁnal GPT-3 model uses 96 transformer decoder
layers, 96 attention heads, the dimension of the
model (dmodel = 12888), dimension of each at-
tention head (dhead = 128), batch size is 3.2M,
and α = 0.6 × 10−4.

3 Code Intelligence (CI)

With the availability of big code from public
sources like GitHub and StackOverﬂow, an op-
portunity to use data-driven tools and algorithms
comes to light. This opens the door for machine
learning, and NLP techniques, in particular, to
be used for various applications on source code.
Such applications tend to improve the software de-
velopment process and are gaining attention from
both AI and software engineering academics. Ap-
plications of machine learning and NLP tech-
niques on source code includes applying statisti-
cal models (Oda et al., 2015a), neural networks
such as LSTMs (Rabinovich et al., 2017), and
more recently pre-trained transformers including
CodeBERT (Feng et al., 2020), CodeGPT (Svy-
atkovskiy et al., 2020), Codex (Chen et al., 2021a)
which are respectively variants of BERT (Devlin
et al., 2018), GPT-2 (Radford et al., b), and GPT-
3 (Brown et al., 2020) pretrained on source code .
Recently, researchers at Microsoft have published
an international benchmark CodeXGLUE (Lu
et al., 2021a) for Code Intelligence, similar to
existing NLP benchmarks such as GLUE (Wang
et al., 2018) and SQuaD (Rajpurkar et al., 2018,

2016). Another benchmark worth mentioning
is GLUECode (Karmakar et al., 2020). GLUE-
Code evaluates machine learning models on lo-
cal and global understanding evaluation of source
code. Different from CodeXGLUE, GLUECode
contains two tasks for “local reasoning" in which
models are tasked with showing understanding of
codes on a local level. The ﬁrst task is operator
prediction, in which a model is tasked with pre-
dicting a masked operator including boolean com-
parison operators and arithmetic operators. The
second task is NPath prediction, where models are
tasked with counting the number of distinct paths
control ﬂow can take in a method. We categorize
works in NLP on source code fall into a set of ar-
eas. The areas include Program Synthesis, Pro-
gram Analysis, Program Repair, Clone Detection,
Defect Detection, Cloze Testing, Code Transla-
tion, Code Reﬁnement, Code Completion, Code
Search, Code Documentation Generation, Doc-
umentation Translation, Semantic Parsing, and
Code Generation. This section will overview each
of these areas and highlight notable works. We
focus on Code Generation and semantic parsing
more in particular for this survey.

3.1 Program Synthesis

Artiﬁcial
intelligence research has long aimed
their pro-
at having computers
synthesize
grams (Manna and Waldinger, 1971; Waldinger
Program Synthesis involves
and Lee, 1969).

language descriptions it

constructing full or partial programs that complete
a speciﬁed task (Allamanis et al., 2018; Gulwani
et al., 2017). Program speciﬁcations are taken by
the program synthesizer model as input and an
executable program is synthesized by the model
as the output. Program speciﬁcations are tradi-
tionally expressed with formal logic statements.
Other program speciﬁcations include providing
the model with example input/output pairs and
language descriptions
more recently natural
of what the program needs to accomplish.
In
program synthesis, if the program speciﬁcations
is ac-
include natural
companied by another speciﬁcation type such as
input/output pairs (Austin et al., 2021; Shi et al.,
If the program speciﬁ-
2020; Ye et al., 2020b).
cation is in natural language only then it would
be considered a semantic parsing task which
is described in §3.7. Program synthesis works
usually synthesis programming using restricted
domain-speciﬁc
(Gulwani,
2011) or
languages that have more features
but are still designed with program synthesis
in mind (Odena and Sutton, 2020; Ellis et al.,
2021). More recently, program synthesizers have
started to synthesize programs in general-purpose
languages such as Python (Shi et al., 2020; Austin
et al., 2021).

languages

(DSLs)

Program synthesis approaches include apply-
ing both symbolic (Manna and Waldinger, 1975,
1980) and neuro-symbolic (Parisotto et al., 2016)
techniques (Balog et al., 2016; Odena and Sut-
ton, 2020; Devlin et al., 2017). Symbolic ap-
proaches are also known as rule-based program
synthesis (Manna and Waldinger, 1980) focus on
deriving a program from a well-deﬁned speciﬁ-
cation using a set of formal grammar rules. A
drawback of the symbolic approach is it often re-
lies on using formal logic to describe the pro-
gram speciﬁcation, however writing the formal
logic to describe the program is often more dif-
ﬁcult than writing the program itself. To remedy
this issue, modern program synthesis models often
learn from input/output examples. A recent no-
table work utilizing input/output examples in var-
ious domains is DreamCoder (Ellis et al., 2021).
DreamCoder learns to synthesize programs in a
domain-speciﬁc language using input and output
examples to solve inductive programming tasks
such as list processing and text editing, creative
tasks such as drawing pictures and building scenes

and to solve classical physics problems. Dream-
In
Coder uses a wake-sleep learning algorithm.
its, sleep DreamCoder goes through to learn con-
cepts from combinations of DSL primitive oper-
ations improving its library of operations in the
process. When DreamCoder is awake it uses op-
erations from its library to solve problems given
input and output examples. There are also no-
table works that synthesize programs in Python,
a general-purpose programming language, which
has the potential to power tools to further enable
programmers. Austin et al. (2021) explore the lim-
its of using a large pretrained transformer-based
language model to synthesize basic python pro-
grams to solve entry-level programmer tasks and
math problems given input/output examples and
a natural language description. The authors note
that large language models enable program syn-
thesis models to consider various types of pro-
gram speciﬁcations such as natural language. Fur-
thermore, the authors demonstrate that such lan-
guage models can reduce the error rate of synthe-
sized programs by up to 50% when engaging in
dialogue with a human and incorporating human
feedback given on the synthesized code. Shi et
al. (2020) propose TF-Coder, a program synthe-
sizer for TensorFlow, a well-known deep learning
library by Google. TF-Coder uses input/output
examples and natural language description as pro-
gram speciﬁcations and generates Python code in
the TensorFlow library to solve real-world ten-
sor manipulation problems achieving superhuman
performance.

Program synthesis models are generally evalu-
ated based on how many compilable (executable)
programs are synthesized. They are also evalu-
ated based on how many programs are logically
correct, i.e. match the speciﬁcation constraints
or solve the programming problem by passing
pre-programmed unit tests (Austin et al., 2021).
Other metrics also include how long it takes for
the model to synthesize the program, and dura-
tion speedup compared to human programmers.
Recent work by Schuster et al. (2021) proposes
a dataset, programming puzzles (P3), to evalu-
ate program synthesis models. Program synthesis
models are tasked with predicting an input x to a
short Python program f with makes f output True.

Although program synthesis generally assumes
that the program compiles with some speciﬁca-
tion, some notable works utilize program synthe-

sizers to generate random but functioning pro-
grams for benchmarks and compiler fuzzing.
Cummins et al. (2017a) synthesize a large num-
ber of programs used as benchmarks in OpenCL
code. Fuzzing is a popular method that creates
structured data samples that are used as inputs to
test complex software such as compilers and web
applications. Patra and Pradel (2016) synthesize
programs using TreeFuzz in JavaScript using a cor-
pus of example structured data (in this case exam-
ple program outputs). These programs generate
random structured data for fuzz-testing JavaScript
interpreters. Program Synthesis has been applied
to build usable products in ﬁelds including data
science (Drosos et al., 2020), to assist data scien-
tists in wrangling data using Python, and general
software engineering to generate code edit sugges-
tions based on repetitive changes (Miltner et al.,
2019).

3.2 Program Analysis

Computers can execute programs but they do not
necessarily understand what the programs do or
mean. When analyzing program source code,
we can computationally estimate a program’s be-
havior, functionality, complexity, meaning, etc.
Program analysis focuses on extracting semantic
properties from programs. In a sense, it is similar
to natural language understanding (NLU) in natu-
ral language processing where both ﬁelds focus on
comprehending a snippet of. Probabilistic models
of source code compose a notable family of mod-
els to analyze programs and extracting properties.
Sometimes codebases contain uninformative vari-
able names such as e and which makes it difﬁ-
cult for humans to comprehend code. This can be
remedied by refactoring codebases to include de-
scriptive comments and variable names. Raychev
et al. (2015) propose a system JSNice that mod-
els the statistical patterns of source code through
an AST-based dependency network to predict vari-
able names and types in JavaScript programs. A
notable issue in program analysis has been scal-
able but imprecise models. Models that tended
to scale on large datasets tended to have higher
numbers of false positives. To remedy this, Oh
et al. (2015) proposes using Bayesian Optimiza-
tion to optimize models for static program analy-
sis and report achieving high precision while low-
ering false positives. Mangal et al. (2015) take
a more user-centered approach and utilize online

learning to tune their model. The user would pro-
vide feedback to the program analysis system as
to whether the user likes or dislike the program
analysis output. Mapping comments onto code to-
kens can be a useful feature in helping program-
mers debug and understand code.
In an attempt
to model this, Panthaplackel et al. (2020) work
on associating entities mentioned in Javadoc com-
ments with source code tokens using a feedfor-
ward neural network trained as a classiﬁer trained
on noisy data from Github. Managing large soft-
ware repositories can be gruesome. Source code
classiﬁcation and tagging are crucial in helping to
organize, search, and reuse codebases. In an at-
tempt to automate source code tagging and clas-
siﬁcation, Mou et al. (2016a) propose a novel ar-
chitecture of Tree-based convolutional neural net-
works (TBCNNs). TBCNNs combine structured
information from Abstract-Syntax Trees (ASTs),
which are tree-based meaning representations of
code snippets, with Convolutional Neural Net-
works (CNNs). CNN’s are a deep learning model
derived from MLPs. Their work is then applied to
classify code bases by functionality with over 104
unique functionalities. TBCNNs were reported to
outperform other classiﬁcation machine learning
models including SVM and RNNs for this task.
Program analysis has been also useful in software
reverse engineering, in particular, recovering vari-
able names when decompiling C code (Lacomis
et al., 2019).

3.3 Clone Detection

Code Clone Detection is another area of research
that focuses on detecting portions of code that
mean or do the same thing. Program analysis
(see §3.2) focuses on analyzing semantic proper-
ties of programs. Clone detection is closely re-
lated to program analysis and can be considered
a sub-ﬁeld of program analysis that is particularly
focused on estimating semantic similarity between
two codes. Early work on code clone detection by
Chilowicz and Duris (2009) focused on indexing
and retrieving abstract syntax trees. Clone detec-
tion has been approached as a machine learning
problem of binary classiﬁcation of two codebases
(e.g. labeling a code pair with 1 to indicate that the
pair is a clone and labeling with 0 to indicate oth-
erwise) (Svajlenko et al., 2014; Wei and Li, 2017;
Wang et al., 2020b, 2021a). Code clone detection
has also been approached as an information re-

trieval problem, where the model is tasked with re-
trieving semantically similar codes to a given code
input (Ganguly et al., 2018; Mou et al., 2016b; Lu
et al., 2021a). Code clone detection is included
as a task on the CodeXGLUE benchmark (Lu
et al., 2021a). The task evaluates models using the
F1-score for the binary classiﬁcation subtask and
Mean Average Precision (MAP) (Beitzel et al.,
2009; Lu et al., 2021a) for the code retrieval task.
Mean Average Precision (MAP) is the arithmetic
mean of the average precision values (AP ) for a
system over a set of n documents. MAP is de-
scribed in equation 18.

M AP =

1
n

(cid:88)

n

APn

(18)

Notable datasets for clone detection include the
POJ-104 dataset (Mou et al., 2016b) for clone
code retrieval. The POJ-104 dataset is collected
from a programming open judge (OJ) system
where students submit their source code for 104
problems. The dataset contains about 500 stu-
dent submissions in the C/C++ programming lan-
guage for each problem. The task is to retrieve
other codes that solve the same problem as the
input example. Another notable dataset is the
BigCloneBench dataset (Svajlenko et al., 2014)
which focuses on the binary classiﬁcation of clone
codes. After ﬁltering the dataset contains about
1, 731, 860 Java code pairs.

Currently, to the best of our knowledge, Code-
BERT (Feng et al., 2020) outperforms prior mod-
els on both the POJ-104 and the BigCloneBench
datasets. Prior works in clone detection have
used AST-based RNNs with pretrained word em-
beddings like word2vec (Büch and Andrzejak,
2019). Other notable approaches for clone de-
tection include Clone Detection with Learning to
Hash (CDLH) (Wei and Li, 2017) which learns
hash functions and representations of code snip-
pets via an AST-based LSTM. Another approach
is Flow-augmented abstract syntax tree (FA-AST),
which augments the original AST representa-
tion of source code with data ﬂow edges and
uses graph neural networks (Wang et al., 2020b).
MulCode (Wang et al., 2021a) integrates a pre-
trained transformer model BERT and an AST-
based LSTM to encode a code sequence and struc-
ture and is evaluated on three classiﬁcation tasks
including clone detection.

importance since the rise of machine learning on
“big code". Lopes et al. (2017) discovered that
most of the code on Github is near-duplicate. A
study followed by Allamanis (2019) explores the
effects of code duplication on machine learning
models for source code. Allamanis observes big
inﬂation in machine learning metrics when test-
ing on duplicated code corpora. Code clone de-
tection is crucial in creating higher quality code
datasets for machine learning and can also have
other uses such as detecting code plagiarism in
academic contexts (Ganguly et al., 2018).

3.4 Code Search

Searching for code snippets online is a common
activity for programmers and software engineers.
Semantic code search is the task of retrieving se-
mantically equivalent code to a natural language
query (Husain et al., 2019). Husain et al. (2019)
release a corpus speciﬁc for the task of code search
called CodeSearchNet4. The corpus is collected
from public,
licensed, and non-forked GitHub
repositories. Functions and methods are parsing
using TreeSitter and corresponding documentation
is parsing using regular expressions. The train-
ing set of the CodeSearchNet corpus includes only
code functions that have an associated documenta-
tion string with them. Other inclusion and exclu-
sion criterion include: the documentation is short-
ened to only the ﬁrst paragraph, samples that in-
clude 3 tokens or less of documentation or 3 lines
or less in its function body are removed. Ad-
ditionally, methods with the string test in its
function name, constructors and standard exten-
sion methods such as __str__ are removed, and
lastly duplicate functions are removed from the
dataset using clone detection methods described
in (Lopes et al., 2017; Allamanis, 2019)5. It con-
tains around 6 million functions in six program-
ming languages: Java, Go, JavaScript, Python,
Ruby, and PHP along with corresponding 2 mil-
lion natural language queries in English. While
the dataset is relatively big, it suffers from being
quite noisy. Scraped function documentation is far
from a query, hence the authors reported difﬁculty
in identifying how accurately a documentation
sample describes its corresponding code function.
The code search task is evaluated using Mean Re-

4https://github.com/github/

CodeSearchNet

5https://github.com/Microsoft/

Code clone detection has become of increasing

near-duplicate-code-detector

Model

Description

Best NDCG Baseline

GitHub

Neural Bag of Words (BoW)

Encodes the codebases and English queries by en-
coding them into a standard token embedding with
masking. The model optimizes to minimize the co-
sine distance between the code and the query.

0.384

0.340

(Wu, 2020)

Self-Attention

One Dimensional Convolu-
tional Network (CNN)

Bidirectional RNN

masked

Transformer
atten-
tion (Vaswani et al., 2017) is used to compute
the token embeddings.

multi-headed

0.240

0.240

CNN is used over query and code embeddings.

0.165

0.165

Employing gated recurrent network (GRU) to sum-
marize the query.

0.046

0.046

-

-

-

Table 2: Overview of top scoring CodeSearchNet models sorted by the best score for the particular model
across all leaderboard submissions. The score is the average Normalized Discounted Cumulative Gain
(NDCG) score across all six programming languages.

ciprocal Rank (MRR) and Normalized Discounted
Cumulative Gain (NCDG) a commonly used met-
ric in information retrieval described in (Vechto-
mova, 2009). Top scoring attempts of each unique
method are listed in Table 2 along with the GitHub
reference, and the baseline score for that approach.
Submissions without publications or with private
code bases were excluded. A notable attempt not
mentioned in the table by Arumugam (Arumugam,
2020) who uses code2vec (Alon et al., 2019), a
code embedding of abstract syntax tree paths for
semantic code search. He uses code2vec to em-
bed the code functions and combined it with an-
other neural model such as Neural Bag of Words
(NBoW) which are table-look up embeddings or
embeddings with self-attention for the query and
scores 0.148 average NDCG using NBoW for the
query and code2vec for code.

There are additional works that

tackled the
problem of code search that do not use the Code-
SearchNet dataset exclusively. Reid et al. (2020)
propose NLP2TestableCode an Eclipse IDE plu-
gin that retrieves code snippets from the Stack-
Overﬂow database using natural language queries
and integrates the retrieved code into a Java code-
base. Li et al. (2019) propose a dataset composed
of natural language queries and code snippets for
evaluating code search models. The dataset was
constructed using 287 Stackoverﬂow question and
answer pairs as well as from method code snip-
pets from the most popular Android repositories
on Github that correctly answer the Stackover-
ﬂow question. The Github corpora contains about
24,549 repositories and 4,716,814 methods.

3.5 Code Documentation Generation &

Summarization

The goal of code documentation generation is to
generate a comment in natural language that ac-
curately describes a given code snippet. This re-
search area intersects heavily in terms of datasets
and models with Code Generation (§3.8) since
both areas heavily utilize corpora of source code
and natural language and utilize machine trans-
lation approaches. Notable work by Oda et
al. (2015b) propose two novel dataset of Python
and natural language comments called the Django
and Euler datasets. The authors then statistical
machine translation to generate comments in nat-
ural language given a Python source code snip-
pet. The system was evaluated using BLEU (Pa-
pineni et al., 2002) and human evaluation of
the generated comments. Fudaba et al. (2015)
build a tool called Pseudogen that also gener-
ates pseudocode in both Japanese and English
from Python code using statistical machine trans-
lation. Iyer et al. (2016a) propose a dataset and
an LSTM-encoder decoder model to produce sen-
tences in natural language that describe C# code
snippets. Gros et al. (2020) examine the task of
generating comments in 4 code-natural language
datasets using a sample information-retrieval (IR)
model using the BM25 scoring function (Robert-
son and Walker, 1994; Amati, 2009), LSTM-
based sequence-to-sequence, and a transformer-
based sequence-to-sequence model. The authors
ﬁnd that the simple IR model provides competi-
tive performance on these datasets. Notable work

by Gao et al. (2020b) propose generating Stack
Overﬂow questions given source code which is
similar to generating comments. Code Documen-
tation Generation is also included as a task in
the CodeXGLUE (Feng et al., 2020) which uti-
lizes the CodeSearchNet corpus (Husain et al.,
2019) for training and testing machine learning
models. More recent approaches to code docu-
mentation utilize pretrained transformers. Cur-
rently, CoTexT (Phan et al., 2021) outperforms
CodeBERT (Feng et al., 2020), PLBART (Ah-
mad et al., 2021), and ProphetNET-X (Qi et al.,
2021) in this task. The CodeSearchNet corpus
is further described in §3.4 and the models are
described later on in the code generation subsec-
tion §3.8. Code summarization focuses on gen-
erating method names in natural language given a
method’s body of code snippets.A notable work on
representing codes as vectors is code2vec (Alon
et al., 2019) which focuses on embedding ab-
stract syntax tree paths into a ﬁxed-length vector.
Code2seq (Alon et al., 2018) is an LSTM-based
sequence to sequence model that encodes the raw
source code tokens along with the AST paths in
its encoder and attends to the relevant paths while
decoding. Code2seq outperforms code2vec on
code summarization and outperformed prior state
of the art in code documentation generation Co-
deNN (Iyer et al., 2016a) by 2% BLEU.

3.6 Documentation Translation

Code Documentation Translation focuses on
translating web-pages containing code-related
documentation such as library docs from a source
language into a target language. CodeXGLUE
proposes this task and curates a dataset by crawl-
ing the Microsoft Documentation website6. and it
includes software and code description documents
in various languages (Lu et al., 2021a). The task
focuses on translation between English and other
languages such as Danish, Latvian, Norwegian,
and Chinese. The multi-lingual zero-shot trans-
former model proposed by Johnson et al. (2017)
and a variant of the model with the encoder ini-
tialized with XLM-R (Conneau et al., 2020) pre-
trained weights were used as models for this task.
The pretrained transformer was reported to outper-
form the baseline on all translation tasks (Lu et al.,
2021a) using the token overlap metric BLEU (Pa-
pineni et al., 2002). The pretrained transformer

6https://docs.microsoft.com/en-us/

scored 66.16 while the transformer baseline score
52.67 BLEU.

3.7 Semantic Parsing

When interfacing with machines via natural lan-
guage, a key component is the machine’s un-
derstanding of the natural language intent. The
language process-
intersection between natural
ing,
information retrieval, and interaction with
humans is called natural language understanding
(NLU). Semantic parsing converts natural lan-
guage utterances to executable meaning repre-
sentations, such as lambda calculus or database
queries, that can be executed on a knowledge-
base (Kamath and Das, 2019). Semantic parsing
can also be known as auto-formalization, which
is deﬁned as the task of turning informal descrip-
tions to formal and automatically veriﬁable for-
mats (Szegedy, 2020). Semantic parsing has be-
come increasingly prominent in applications fo-
cused on interfacing via natural language with
computer systems (Yin, 2020). Semantic Pars-
ing enables machines to identify roles and objects
in a sentence (Fernández-González and Gómez-
Rodríguez, 2020), converting natural language to
database queries (Zhong et al., 2017) that can
then be plugged into conversational assistants like
Alexa, interfacing with robots using natural lan-
guage commands (Matuszek et al., 2012; Artzi
and Zettlemoyer, 2013), generating mathemati-
cal statements (Wang et al., 2020a; Sun et al.,
2019a), and interacting with time series medical
data (Chen Jr and Bunescu, 2019).

Semantic parsing works typically generate or
map a natural language utterance into an unam-
biguous meaning representation (also referred to
as logical forms or semantic representations). The
representations need to be executable in an envi-
ronment or a speciﬁc context. The representations
typically follow a speciﬁed grammar (also referred
to as a logical formalism). Grammar is used to
identify valid meaning representations. Grammar
can interact with a model which produces a dis-
tribution over meaning representations, to ensure
that all meaning representations in the distribution
are valid.

Meaning Representations of natural language
utterances for semantic parsing tend to often fall
into one of three categories: ﬁrst order logic
(FOL), graphs, and code snippets. First order logic
often expresses unambiguous logical expressions.

(a) Sample task of parsing natural language interactions into a domain speciﬁc logical form (Chen Jr and Bunescu,
2019).

(b) Abstract Meaning Representation of two sentences
that are semantically equivalent but have different gram-
mar structures (Banarescu et al., 2013).

(c) Sample task of parsing natural language into an SQL
query (Zhong et al., 2017).

Figure 12: Example semantic parsing tasks with various meaning representations.

It consists of variables and functions that only
take objects as arguments. First order logic can
be augmented with lambda calculus to increase
its expressiveness (Carpenter, 1997). An illus-
trative example from Singh et al. (2020), “a man
eating" can be represented as ∃A(∃B(man(A) ∧
eat(B) ∧ agent(B, A))) in ﬁrst order logic. First
order logic has been used in various seman-
tic parsing works to query databases (Zettle-
moyer and Collins, 2005), knowledge bases (Be-
rant et al., 2013), Wikipedia tables (Pasupat and
Liang, 2015), parsing conversations and imple-
menting conversational agents (Artzi and Zettle-
moyer, 2011), and mapping linguistic instruc-
tions onto robotic actions (Artzi and Zettlemoyer,
2013). Graph-based meaning representations of-
ten denote a labeled graph where nodes repre-
sents entities or events and edges represent se-
mantic relationships between the nodes. Graphs
tend to be easy to understand compared to other
meaning representations like code snippets and
ﬁrst order logic. Graph-based meaning represen-
tations also have the advantage in that they also
provide abstraction and can pivot away from any
general syntax. Notable examples of graph-based

meaning representations include Semantic Depen-
dency Parsing (Oepen et al., 2015; Fernández-
González and Gómez-Rodríguez, 2020), Abstract
Meaning Representation (AMR) (Banarescu et al.,
2013), and Universal Conceptual Cognitive Anno-
tation (Abend and Rappoport, 2013). Code snip-
pets in general purpose programming languages
provide a good medium for representing mean-
ing since they are domain speciﬁc and easily ex-
ecutable. More recently there have been vari-
ous works that focused on converting natural lan-
guage into equivalent executable code snippet rep-
resentations in languages such as SQL (Zhong
et al., 2017), Python (Yin and Neubig, 2017a), and
Bash (Lin et al., 2018). These works will be dis-
cussed later on in §3.8. Figure 12 showcases ex-
ample semantic parsing tasks with an AMR, a log-
ical form, and SQL, a database speciﬁc program-
ming language.

Grammar of meaning representations deﬁnes
rules to determine whether a representation is
valid or not. Grammar also plays a role in deter-
mining how a semantic parser expresses its mean-
ing representations and how computationally com-
plex building the meaning representation is. A no-

table example of strict grammar for meaning rep-
resentations is Combinatory Categorical Gram-
mar (CCG)7(Steedman, 2000; Artzi et al., 2014).
It is efﬁciently parsable, but still expressive. CCG
has a lot of rules and hence it makes it easier to
parse since it lessens the amount of possible mean-
ing representations for a given input (Zettlemoyer
and Collins, 2007). Kwiatkowski et al. (2011)
consider learning a probabilistic model of CCG
grammar for semantic parsing. Recent works on
code generation from natural language leverage
the programming language grammar itself by us-
ing an abstract syntax tree associated with the pro-
gramming language.

Context of semantic parsing tasks form the
bases of mapping natural language to its mean-
ing representation. Often these contexts or en-
vironments are executable. Example semantic
parsing contexts include knowledge bases (Be-
rant et al., 2013), Wikipedia tables (Pasupat
and Liang, 2015), geography queries (Zelle and
Mooney, 1996), SQL databases (Zhong et al.,
2017), spreadsheets (Gulwani et al., 2012), and
programming languages (Yin and Neubig, 2018,
2017b).

Models for semantic parsing relied on rule-
(Johnson, 1984; Woods,
based techniques (e.g.
1973; Hendrix et al., 1978)), symbolic artiﬁcial in-
telligence (e.g. (Zelle and Mooney, 1996, 1993)),
statistical-based techniques (e.g.
(Zettlemoyer
and Collins, 2005; Tang and Mooney, 2001)),
RNN-based sequence-to-sequence (e.g. (Chen Jr
and Bunescu, 2019; Jia and Liang, 2016; Singh
et al., 2020; Yin and Neubig, 2017a; Rabinovich
et al., 2017; Ling et al., 2016; Sun et al., 2019a)),
and transformer-based architectures (e.g. (Kacu-
paj et al., 2021; Ferraro and Suominen, 2020;
Shen et al., 2019; Sun et al., 2019b; Gemmell
et al., 2020; Svyatkovskiy et al., 2020; Kusupati
and Ailavarapu)).
In the late 1970’s, Hendrix
et al. (1978) pioneer the task of interfacing with
databases using natural language. The rule-based
system proposed called LADDER which takes in a
restricted set of natural language questions. LAD-
DER accepts questions if they match a preset tem-
plate, extract ﬁeld and ﬁle names, and then pro-
duces a query or a sequence of queries in a lan-
guage Data Language by inserting the relevant ﬁle
In 1993,
and ﬁeld names into preset templates.
Zelle et al. (1993) introduce a ﬁrst-order induc-

7https://yoavartzi.com/tutorial/

tion algorithm that utilizes symbolic artiﬁcial in-
telligence to learn word classes an semantic re-
lationships between the words to support pars-
ing of sentences and conduct semantic analyses.
The algorithm takes in a set of training samples
containing sentences paired with ’case represen-
tations’ (e.g. parts of speech tags). The algo-
rithm utilizes shift-reduce parsing where in each
time step either a new word from the sentence is
added (shifted) onto a stack or the top two ele-
ments on the stack are popped from the stack then
merged to form a new element and pushed back
into the stack. After the training is complete the
shift-reduce parser is introduced to search control
heuristics to help the parser become more special-
ized. In 2005, Zettlemoyer and Collins (2005) pro-
pose a statistical probabilistic model that learns
given a training set to induce grammar for map-
ping natural language sentences to lambda calcu-
lus. The probabilistic model learns given a sen-
tence in lambda calculus L and a parse tree T . The
parse tree T is deﬁned as the sequence of transfor-
mations needed to derive L from the given natural
language sentence S within the constrains of the
grammar CCG. The conditional distribution would
be P (L, T |S). In 2020, Singh et al. (2020) pro-
pose using an LSTM-based sequence-to-sequence
with Bahdanau attention (Bahdanau et al., 2014)
to convert natural language utterances into ﬁrst or-
der logic. The authors report on improving upon
the sequence-to-sequence by introducing a mech-
anism that aligns variables across predicted predi-
cates in ﬁrst order logic. This mechanism utilizes a
classiﬁer at each decoding step of a variable token
to predict whether the variable is aligned with any
previously decoded tokens or whether it’s a new
variable. Following the classiﬁer is self-attention-
like mechanism where the decoder hidden states
are used to estimate alignment between the vari-
able and other previous decoded tokens. Chen and
Bunescu (2019) design an LSTM-based encoder
decoder model to parse natural language or GUI
interactions into a database-executable lambda-
calculus logical form. The database contains time
series data from sensors monitoring type I dia-
betes patients along with patient-reported informa-
tion about discrete life events (e.g. stress, sleep,
meals, etc. The data is temporal and user interac-
tions are sequential, where the current interaction
may dependent on a prior interaction with the sys-
tem. This is illustrated in the Figure 12a example.

Hence, it is crucial for the model to understand
coreferential time expressions such as “then" and
temporal relations between entities such as “af-
ter". To enhance the model with such understand-
ing capabilities, the encoder decoder architecture
works with a copying mechanism to model con-
text dependency and utilizes 3 attention mecha-
nisms: the ﬁrst attention mechanism attends over
the previous input, the second attends over the
previous logical form, and the third attends over
the current input. The proposed approach scores
66.9 in exact match accuracy signiﬁcantly out-
performs the baseline of a standard LSTM-based
seq2seq model (Bahdanau et al., 2017; Cho et al.,
2014) which scored 22.2 accuracy on a real-world
dataset. Kacupaj et al. (2021) employ a trans-
former architecture to generate logical forms in in
a question-answering conversational assistant ca-
pable of answering complex questions by utilizing
a pre-ﬁlled knowledge graph. Ferraro et al. (2020)
compare the transformer architecture with other
statistical and neural semantic parsing systems on
the ATIS (Dahl et al., 1994) and Geo (Cai and
Yates, 2013) semantic parsing datasets and ﬁnd
that the transformer architecture outperforms prior
strong models in certain settings and achieve com-
petitive results across all experimental settings.

Semantic parsing is a unique area with the
aim of improving natural language understanding
(NLU). It is distinguished from natural language
generation and machine translation. Both natu-
ral language generation and some semantic pars-
ing works use similar deep learning models and
techniques such as an LSTM based sequence-to-
sequence architecture in the works of Yin and
Neubig (Yin and Neubig, 2017a). The objec-
tive of semantic parsing involves the prediction or
generation of inherently structured representations
(not only utterances). These representations need
to be executable against a context or an environ-
ment. Semantic parsing is related to Code Gen-
eration from Natural Language, since both pre-
dict structured representations of natural language,
in this case these structure representations would
be code snippets which are executable in an en-
vironment and can also be represented as abstract
syntax trees. However, semantic parsing also in-
cludes other executable representations that are
not limited to code such as ﬁrst order logic and
graphs. For other survey papers on semantic pars-
ing c.f. (Kamath and Das, 2019; Lee et al., 2021).

3.8 Code Generation

This section overviews generating a snippet of
code given a natural language description.

3.8.1 Overview

Code generation is the task of generating an exe-
cutable snippet given a description in natural lan-
guage. The executable snippet can be in a par-
ticular programming language or domain speciﬁc
language (DSL) that can be trans-compiled into
a programming language. To illustrate consider
the following example generating a Python snip-
pet from an English intent: check if all elements
in list ‘mylist’ are the same → len(set(mylist))
== 1. Code generation from natural language
focuses on linking natural language with source
code which is something it has in common with
other aforementioned code intelligence tasks code
search and semantic parsing. Code generation is
distinct from code search. Code search focuses
on ranking code snippets and codebases given a
natural language query, whilst code generation at-
tempts to generate a meaning representation, an
executable code snippet that aligns with a natu-
ral language description. With that being said,
code generation is closely related to the area of
It can be considered a sub-
semantic parsing.
ﬁeld of semantic parsing that specializes in gen-
erating executable code snippets in programming
languages an excludes any other meaning repre-
sentation of natural language. Code generation
has been inspired by Natural Language Generation
methods including statistical machine translation,
RNN-based sequence-to-sequence, and generative
transformer models. Linking codebases to natural
language intents or other forms of unstructured in-
put compose face a series of difﬁcult challenges.
Firstly, the compositional and abstract nature of
code can make pairing text with code difﬁcult.
Secondly, natural language is ambiguous in nature
while source code is not ambiguous. Code gener-
ation can enable users to interface with machines
using natural language. For instance, Kuhlmann et
al. (2004) enable interfacing with robots using nat-
ural language. Furthermore, code generation en-
ables users to build code more effectively and efﬁ-
ciently and enhance the overall software engineer-
ing process. Code software engineering (Miltner
et al., 2019; Xu et al., 2021), robotics (Kuhlmann
et al., 2004), and cyber-security (You et al., 2017;
Liguori et al., 2021a; Frempong et al., 2021;

(a) An assembly code generation task. The task is to gen-
erate the assembly code that is then compiled into shell-
code (small pieces of code used as a payload to exploit
software vulnerabilities) using the natural language de-
scriptions on the right. The dataset contains multi-line
snippets mapping onto one intent. Lines 4-5, 6-7-8, 9-10,
11-12 are multi-line snippets (Liguori et al., 2021a).

(b) A Python code generation task where models are
tasked with generating the blue code cell given the nat-
ural language utterance Create and train the model along
with the previous natural language and code cells in the
Jupyter notebook (Agashe et al., 2019).

Figure 13: Example code generation parsing tasks with various programming languages.

Liguori et al., 2021b, 2022). Figure 13 showcases
two examples of code generation tasks.

Recent works in code generation have been used
to assist data scientists to perform data visualiza-
tion and ﬁle manipulation (Xu et al., 2021), gen-
erating bash commands (Lin et al., 2018), gen-
erate exploits (Liguori et al., 2021b,a; Frempong
et al., 2021; Liguori et al., 2022), solve interview-
level programming questions (Hendrycks et al.,
2021), manipulate data (Zhong et al., 2018),
lan-
and generate code snippets from natural
guage descriptions in many programming lan-
guages. These programming languages include
but are not limited to Python (Xu et al., 2020a;
Ling et al., 2016; Liguori et al., 2021b), Java (Ling
et al., 2016), SQL (Zhong et al., 2017), Excel
macro commands (Gulwani and Marron, 2014),
Assembly (Liguori et al., 2021a,b, 2022), and
JavaScript (Frempong et al., 2021).

3.8.2 Datasets

This section introduces a comprehensive collec-
tion of supervised datasets containing natural lan-
guage (NL) mapped onto programming language
snippets (PL). The snippets can be in general pur-
pose programming languages (e.g. Python, Java)
or in a Domain Speciﬁc Language (DSL). In ta-
ble 3, we curate a systematic list (to the best of our

knowledge) of English NL-PL datasets.
Survey Method. The survey process started by
searching through publications on GoogleScholar8
with keywords such as “Code generation", “Pro-
gram synthesis from natural language", “natural
language to code", and “Conversational Seman-
tic Parsing". We collected highly cited papers and
carefully reviewed all of the works that cite them
for potentially proposed datasets. For each pa-
per we reviewed, we carefully checked the refer-
ences for any papers proposing datasets as well.
We also searched through PapersWithCode9 and
Huggingface Datasets 10 for datasets with the tags
“Code Generation", “Code Search", and “Code".
Datasets that map other forms of input like images
of Graphical User Interfaces (Beltramelli, 2018),
input-output examples (without natural language
descriptions) (Drosos et al., 2020), questions in
natural language not descriptions of non-query
code (e.g. the neural code search dataset (Li et al.,
2019)), and compiled code (Lacomis et al., 2019)
to source code are excluded from the table. We’ve
also excluded large mined datasets that are not
processed into NL-PL pairs like the Code-Clippy
dataset (Cooper et al., 2021), CodeNet (Puri et al.,

8https://scholar.google.com/
9https://paperswithcode.com/datasets
10https://huggingface.co/datasets

2021), and The Pile (Gao et al., 2020a). Multi-
lingual datasets were not systematically surveyed.
Automatic Exploit Generation. Automatic ex-
ploit generation is deﬁned as an offensive secu-
rity technique in which software exploits are au-
tomatically generated to explore and test critical
vulnerabilities before malicious attackers discover
such vulnerabilities (Avgerinos et al., 2011). With
the goal of offensive security in mind, Liguori et
al. (Liguori et al., 2021a) share a dataset, Shell-
code_IA32, for shellcode generation from natu-
ral language descriptions. Shellcodes are com-
piled from assembly programs for the 32-bit ver-
sion of the x86 Intel Architecture and contain a
payload that is used in exploiting software vul-
nerabilities. The Shellcode_IA32 dataset is later
on extended in the EVIL-Decoder (Liguori et al.,
2021b) dataset. Shellcodes are often encoded us-
ing Python software to evade detection by an-
tivirus programs and then decoded using an as-
sembly program to the victim machine. In an at-
tempt to automate the whole pipeline Liguori et
al. (2021b) propose the EVIL dataset composed
of two different languages. EVIL-Decoder dataset
contains full assembly programs along with their
NL descriptions that decode a shellcode on a host
machine. The EVIL-Encoder dataset extends the
general purpose Python Django dataset (Oda et al.,
2015a) with 1,114 original exploit-oriented snip-
pets. Frempong et al. (2021) curate a synthesized
dataset of JavaScript snippets used in cross-site
scripting (XSS) which are a common web-based
attack that target websites.
Bash. In an effort to make bash terminal (a Linux
operating system command line interface) interac-
tions more accessible, Lin et al. (2018) curate a
dataset of bash commands along with their cor-
responding natural language descriptions. This
work helps lay down a foundational dataset and
a baseline model in building natural language in-
terfaces to terminals.
General Purpose Programming. There has been
extensive work in curating datasets for general
purpose programming languages such as Java,
JavaScript, and Python, since general purpose pro-
gramming languages are abundant in open source
code -bases (Husain et al., 2019). While some
datasets were expert curated (Oda et al., 2015a;
Chen et al., 2021a), and crowd sourced (Long
et al., 2016; Zavershynskyi et al., 2018; Kulal
et al., 2019; Austin et al., 2021; Huang et al.,

2021), extensive datasets with NL-PL pairs were
mined from open source code sources such as
Github (Allamanis et al., 2016; Barone and Sen-
nrich, 2017; Iyer et al., 2018; Alon et al., 2018;
Hu et al., 2018; Husain et al., 2019; Agashe
et al., 2019; Clement et al., 2020; Hu et al., 2020;
Bahrami et al., 2021), coding competition web-
sites (Caballero et al., 2016; Li et al., 2022),
Sourcerer (Lopes et al., 2010; LeClair et al., 2019),
IFTTT (Quirk et al., 2015) (a website that al-
lows users to create simple programs using trig-
gers and actions), and StackOverﬂow (Iyer et al.,
2016b; Yin et al., 2018; Yao et al., 2018; Orlanski
and Gittens, 2021). CoDesc (Hasan et al., 2021b)
combines multiple mined Java datasets into one
bigger dataset after removing data-related noise.
Hendrycks et al. (2021) propose the Automated
Programming Progress Standard, a dataset cu-
rated from speciﬁally mining websites where pro-
grammers share programming problems with each
other such as CodeForces, Codewars, AtCoder,
and Kattis. APPS contains Python programming
problems in various difﬁculties (Introductory, In-
terview, and Competition levels), a natural lan-
guage description, and unit tests for each prob-
lem. Some datasets contain synthetically gener-
ated NL descriptions such as the Hearthstone and
Magic The Gathering trading card game datasets,
were the NL-descriptions were automatically pop-
ulated ﬁelds describing the card (Ling et al.,
2016). Akinbou et al. (2021) propose using back-
translation on Python code collected from Aizu
Online Judge (AOJ)11 using a trans-compiler. The
trans-compiler outputs natural language descrip-
tions, in Japanese, from a Python code input. The
Python snippet is converted into an abstract syn-
tax tree and then using pre-deﬁned transformation
rules, the abstract syntax tree is transformed into a
natural language description. Hasan et al. (2021a)
propose text2app, a dataset containing descrip-
tions of Android app speciﬁcations along with a
Simpliﬁed App Representation (SAR) codebase.
SAR is a domain speciﬁc language that is then
compiled into a Java Android app. Text2App used
data BERT-masking for data augmentation to cu-
rate NL descriptions of apps from a small amount
of crowd-sourced app descriptions.
Turducken-style Code Generation. Turducken-
style code snippets include one language embed-
ded in another. Often-times a general purpose

11https://judge.u-aizu.ac.jp/onlinejudge/

programming language like Python being em-
bedded with another server-related programming
language like SQL. In an effort to benchmark
code generation systems on Turducken-style code,
Liang et al. (2021) propose the Lyra dataset which
contains natural language intents in both English
and Chinese languages mapped onto Python code
with embedded SQL snippets.
Conversational Semantic Parsing.
Semantic
parsing is a task in which an utterance is con-
verted into an executable form (§3.7). Semantic
Parsing works often focus on isolated utterances.
Conversational Semantic Parsing takes that a step
further, where the context is a conversation. A cru-
cial part of a dialogue system is being able to track
the user’s goal through out the conversation, this
task is called Dialogue State Tracking (DST). In
conversational semantic parsing, researchers have
formulated the user’s goal into an SQL query (Yu
et al., 2019a), a tree-like representation using a do-
main speciﬁc language (Gupta et al., 2018; Cheng
et al., 2020; Aghajanyan et al., 2020), and a pro-
gram in a domain speciﬁc language that extends a
dataﬂow graph (Andreas et al., 2020). These rep-
resentations are executable by a dialogue system
to interface with a data source. People tend to ex-
plore databases by asking multiple related ques-
tions (Iyyer et al., 2017), which require systems
to be able to process conversational data requests,
clarify ambiguous questions, and process user ut-
terances that can not be mapped onto queries (Yu
et al., 2019a). To address this, Yu et al. (2019a)
release the Conversational Text-to-SQL (CoSQL)
corpus, to build database querying dialogue sys-
tems. Human utterances in dialogue sometimes
contain nested requests. An example from (Gupta
et al., 2018) illustrates this well: “Get me driv-
ing directions to the Eagles game" it is com-
posed of two requests GET_DIRECTIONS and
GET_EVENT. Linear representations do not allow
for such compositionality. To tackle this phenom-
ena, both the TreeDST (Cheng et al., 2020) and
the TOP datasets (Gupta et al., 2018), use a com-
positional (tree-like) forms. SB-TOP builds on the
TOP dataset to include a decoupled representation
that can represent co-reference and context carry
over (Aghajanyan et al., 2020).
Database Querying. Creating natural language
interfaces to interacting with data sources such as
databases has been a long standing research (Price,
1990; Dahl et al., 1994). To this end, various

hand-curated datasets containing NL-descriptions
and data-related questions mapping onto a domain
speciﬁc language (such as a logical form) trans-
compilable into a query in various domains such as
Airline Travel in English (Price, 1990; Dahl et al.,
1994) and other various languages (Upadhyay
et al., 2018; Xu et al., 2020b), Job posts (Tang
and Mooney, 2001), open-domain question an-
swering (Cai and Yates, 2013; Berant et al., 2013;
Wang et al., 2015; Yih et al., 2016). We also ob-
serve an array of that focus on generating SQL
queries from natural language. Some of these
datasets are synthetic (Zhong et al., 2017), mined
from StackOverﬂow (Yao et al., 2018; Hazoom
et al., 2021) and Github (Yao et al., 2018), and
human-curated (Tang and Mooney, 2000; Popescu
et al., 2003; Giordani and Moschitti, 2012; Li and
Jagadish, 2014; Iyer et al., 2017; Yu et al., 2018;
Yaghmazadeh et al., 2017; Finegan-Dollak et al.,
2018; Yu et al., 2019b).
Map Question-Answering. Maps (e.g. Google
Maps) contain data on entities restaurants, land-
marks, geographic landmarks like rivers and
mountains. This data is often stored in a database.
A speciﬁc-type of text-to-query task deals with
querying map databases.
In an effort to main-
stream interfacing through natural language with
geographic data in map-related databases, Zelle
and Mooney (1996) formulated the dataset Geo-
Query contain questions about the United States
geography in natural language and correspond-
ing them to Prolog programs to answer them.
GeoQuery was later adapted to SQL (Iyer et al.,
2017; Finegan-Dollak et al., 2018). Another no-
table dataset that contains more complex natural
language questions and entities such as restau-
rants, museums, and hotels is NLMAPS (Haas
and Riezler, 2016; Lawrence and Riezler, 2016).
NLMAPS contain manually curated human ques-
tions that can be run against a worldwide map data
from OpenStreetMap. Natural language intents
were later on expanded using synthetic methods
in NLMAPS-V2 (Lawrence and Riezler, 2018).
Data Manipulation. Datases were curated to with
the aim of automatically generated codes that ma-
nipulate data. Gulwani et al (2014) curated a
dataset that maps English utterances onto Excel
macro-commands. Other datasets focus on gener-
ating a regular expression (Regex) (Kushman and
Barzilay, 2013; Locascio et al., 2016; Zhong et al.,
2018) given a natural language description.

Dataset Name

Programming
Language
(PL)

#NL-PL pairs #NL tokens #PL tokens

#Avg tokens
per NL intent

#Avg tokens per
code snippet

Data
Collection
(NL)

Public Reference

Shellcode_IA32
EVIL-Encoder
EVIL-Decoder
HIJAX

Assembly
Python
Assembly
JavaScript

3,202.00
15,540
3,715
100,000

1,490.00
10,605
1,924
-

1,238.00
9,511
1,657
-

NL2Bash

Bash

9,305

7,790

6,234

Bash

9.24
14.90
9.53
-

11.70

Automatic Exploit Generation

General Purpose Programming

IFTTT
Code-Docstring Corpus Python

DSL

C#2NL
Django
CoNaLa
CoNaLa Mined
CoNaLa-Ext

C#
Python
Python
Python
Python

Java
Java
Python
Python (Jupyter)
SAR (DSL)*
Java
Java

Python
Python, Java, Go,
JavaScript,
Ruby, and PHP
Java

Magic The Gathering
CONCODE
HearthStone
JuICe
Text2App
DeepCom
FunCom

PyMT5

CodeSearchNet

Java-small

Java-med
Java-large

86,960
150,370

66,015
18,805
2,879*
593,837
596,711

13,297
104,000
665
1,521,774
50,000
588,108
2,100,000

7,700,000

2,326,976

665,115

Java
Java

3,004,536
15,344,512

593,123
5,789,741

1,881,451
12,601,929

24,857
-
-
-
-

91,156
-
-
-
-

-

-

-
862,269
-
-
-

-
1,006,402
-
794,711
-

-

-

-

-
-

-

-

-

-
-

6.82
38.50

12.00
14.30
-
11.41
11.41

21.00
13.73
7.00
39.78
-
8.86
-

-

-

3.00

3.00
3.00

4.40
11.90
4.75
-

7.70

21.64
83.81

38.00
-
-
28.70
28.70

1,080.00
26.30
352.00
38.75
-
99.94
-

-

-

60.00

63.00
65.00

×

×

×

EC
EC
EC
Synth

EC

Mined
Mined

Mined
EC
EC
Mined
Mined

Synth
Mined
Synth
Mined
Synth
Mined
Mined

Mined

Mined

Mined

Mined
Mined

(Liguori et al., 2021a)
(Liguori et al., 2021b)
(Liguori et al., 2021b)
(Frempong et al., 2021)

(Lin et al., 2018)

and Gittens,

(Quirk et al., 2015)
(Barone and Sennrich,
2017)
(Iyer et al., 2016b)
(Oda et al., 2015a)
(Yin et al., 2018)
(Yin et al., 2018)
(Orlanski
2021)
(Ling et al., 2016)
(Iyer et al., 2018)
(Ling et al., 2016)
(Agashe et al., 2019)
(Hasan et al., 2021a)
(Hu et al., 2018, 2020)
(Lopes
et
LeClair et al., 2019)
(Clement et al., 2020)

2010;

al.,

(Husain et al., 2019)

(Allamanis et al., 2016;
Alon et al., 2018)
(Alon et al., 2018)
(Alon et al., 2018)

Dataset Name

StaQC-Python
Euler
AOJ
NAPS

APPS
CodeContests
Description2Code
SCONE-Scene
SCONE-Alchemy
SCONE-Tangrams
CoDesc
HumanEval
MBPPS
Math-QA

Programming
Language
(PL)

Python
Python
Python
DSL

Python
Python, C++, Java
Python, C++
DSL
DSL
DSL
Java
Python
Python
Python

PyTorrent
SPoC
CoSQA

Python
C++
Python

Lyra-English
Lyra-Chinese

Python + SQL
Python + SQL

CoSQL
TOP
SB-TOP
TreeDST
SMCalFlow

ATIS

Multi-ATIS
Multi-ATIS++

SQL
DSL
DSL
DSL
DSL

DSL

DSL
DSL

#NL-PL pairs #NL tokens #PL tokens

#Avg tokens
per NL intent

#Avg tokens per
code snippet

147,546
589
89,862
2,716

10,000
13,610
7,764
4,402
4,560
4,989
4,200,000
164
974
23,914

13,825,647
18,356
20,604

2,000
2,000

11,039
44,000
60,000
167,507
155,923

17,635
-
-
-

-
-
-
-
-
-
813,078
-
-
-

-
-
6,784

137,123
-
-
-

-
-
-
-
-
-
1,128,909
-
-
-

-
-
28,254

9.00
-
-
-

-
-
-
56.20
39.90
27.20
21.04
-
-
-

-
-
6.60

Turducken-style Code Generation

-
-

-
-

57.71
70.46

Conversational Semantic Parsing

9,585
-
-
-
17,397

-
-
-
-
338

11.21
8.93
-
7.59
8
Database Querying

86.00
-
-
-

-
-
-
-
-
-
77.97
-
-
-

-
-
71.51

44.24
44.24

-
-
-
-
40

5,410

936

176

11.10

28.10

3,846
44,943

-
-

-
-

-
-

-
-

Data
Collection
(NL)

Mined
EC
Synth-BT
CC

Mined+revision
Mined
Mined
CC
CC
CC
Mined
EC
CC
CC

Mined
CC
CC

EC
EC

CC
CC
CC
CC
CC

CC

CC
CC

Public Reference

×

al.,

et

(Yao et al., 2018)
(Oda et al., 2015a)
(Akinobu et al., 2021)
(Zavershynskyi
2018)
(Hendrycks et al., 2021)
(Li et al., 2022)
(Caballero et al., 2016)
(Long et al., 2016)
(Long et al., 2016)
(Long et al., 2016)
(Hasan et al., 2021b)
(Chen et al., 2021a)
(Austin et al., 2021)
(Amini
et
Austin et al., 2021)
(Bahrami et al., 2021)
(Kulal et al., 2019)
(Huang et al., 2021)

al.,

2019;

(Liang et al., 2021)
(Liang et al., 2021)

(Yu et al., 2019a)
(Gupta et al., 2018)
(Aghajanyan et al., 2020)
(Cheng et al., 2020)
(Andreas et al., 2020)

(Price, 1990; Dahl et al.,
1994)
(Upadhyay et al., 2018)
(Xu et al., 2020b)

#Avg tokens
per NL intent

#Avg tokens per
code snippet

Data
Collection
(NL)

Public Reference

Dataset Name

Freebase917

Jobs640
WebQSP
WikiSQL
StaQC-SQL
SQL2NL
SParC
SPIDER
Restaurants

Scholar
Yelp

IMDB

Advising

Academic
Overnight
SEDE

NLMAPS-V1

Programming
Language
(PL)

DSL

DSL
DSL
SQL
SQL
SQL
SQL
SQL
SQL

SQL
SQL

SQL

SQL

SQL
DSL
SQL

DSL

#NL-PL pairs #NL tokens #PL tokens

917

-

-

640
4,737
80,654
119,519
32,337
12,726
10,181
378

817
128

131

4,570

196
12,602
12,023

391
-
-
9,920
10,086
3,794
-
-

58
-
-
21,413
1,287
3,794
-
-

-
-

-

-

-
-
-

-
-

-

-

-
-
-

2,380

1,014

Map Question-Answering

-

-

NLMAPS-V2

DSL

202,088

8,710

GeoQuery

Prolog, SQL

880

284

60

-

9.80
-
-
9.00
9.00
8.10
13.00
-

-
-

-

-

-
-
-

-

22.90
-
-
60.00
46.00
-
21
-

-
-

-

-

-
-
-

10.90

16.00

7.06

7.60

-

19.10

Data Manipulation

EC

EC
EC
Synth
Mined
Mined
EC
EC
EC

EC
EC

EC

EC

EC
CC
Mined

EC

Synth

EC

(Cai and Yates, 2013; Be-
rant et al., 2013)
(Tang and Mooney, 2001)
(Yih et al., 2016)
(Zhong et al., 2017)
(Yao et al., 2018)
(Iyer et al., 2016b)
(Yu et al., 2019b)
(Yu et al., 2018)
(Tang and Mooney, 2000;
Popescu et
al., 2003;
Giordani and Moschitti,
2012)
(Iyer et al., 2017)
(Yaghmazadeh
2017)
(Yaghmazadeh
2017)
(Finegan-Dollak et al.,
2018)
(Li and Jagadish, 2014)
(Wang et al., 2015)
(Hazoom et al., 2021)

al.,

al.,

et

et

(Haas and Riezler, 2016;
Lawrence and Riezler,
2016)
(Lawrence and Riezler,
2018)
(Zelle
and Mooney,
1996; Iyer et al., 2017;
Finegan-Dollak
al.,
2018)

et

Dataset Name

Programming
Language
(PL)

NLyze-Data

DSL (Excel)

RegexLib
NL2RX
NL2RX-KB13

Regex
Regex
Regex

#NL-PL pairs #NL tokens #PL tokens

#Avg tokens
per NL intent

#Avg tokens per
code snippet

3,570

3,619
10,000
824

-

13,491
560
715

-

179
45
85

-

36.40
10.60
7.10

-

58.80
26.00
19.00

Data
Collection
(NL)

CC

Mined
Synth
CC

Public Reference

×

×

and Marron,

(Gulwani
2014)
(Zhong et al., 2018)
(Locascio et al., 2016)
(Kushman and Barzilay,
2013)

Table 3: Survey of datasets containing natural-language-programming language (NL-PL) pairs that are usable for the tasks of code generation and semantic
parsing. We also include domain speciﬁc languages (DSL) if they can be trans-compiled into a programming language. EC stands for expert curated, CC,
stands for crowdsourced, Synth stands for synthesized, Synth-BT sands for synthesized with back-translation. The authors will add footnote to table for datasets
reporting on total number of tokens vs total number of unique tokens.

3.8.3 Evaluation

Code Generation systems are generally evaluated
using exact match accuracy. Exact match accu-
racy refers to the ratio of model-generates snip-
pets that exactly match the ground-truth snippets.
Another common metric used for evaluating code
generation systems is token level BLEU (Papineni
It is commonly used to evaluate
et al., 2002).
machine translation systems. BLEU is used to
evaluate code generation systems since many prior
works in code generation formulated the prob-
lem as a machine translation problem of translat-
ing English to code snippets (e.g. (Liguori et al.,
2021a)). Both exact match and averaged token
level BLEU scores have been extensively used in
evaluating code generation models (Liguori et al.,
2021a,b; Oda et al., 2015b; Ling et al., 2016; Gem-
mell et al., 2020). It is becoming increasingly im-
portant to note the drawback of using BLEU to
evaluate code generation systems. Recent studies
have shown low correlation between BLEU score
and program correctness (Austin et al., 2021; Chen
et al., 2021a; Hendrycks et al., 2021), i.e. where
the generated program logically matches the nat-
ural language description and passes all the unit
tests. BLEU score increases when there is signiﬁ-
cant token overlap between the generated code and
the ground-truth. The low correlation can be at-
tributed to the abundance of variable and method
identiﬁers in programming languages, which if the
model predicts the identiﬁer tokens correctly, it
would result in a higher BLEU score. However,
even though there may be signiﬁcation token over-
lap the generated code may not compile (be syn-
tactically incorrect), token differences between the
ground truth and generated code may cause the
generate code to follow a distinct logic. Further-
more, there may be more than one correct code
snippet that accomplishes the task described in
natural language. Both token-level BLEU and ex-
act match accuracy penalize models for generat-
ing working code snippets that are different from
the ground truth, even if they are correct. To rem-
edy this, recent works in code generation such as
APPS (Hendrycks et al., 2021), HumanEval (Chen
et al., 2021a), and EVIL (Liguori et al., 2021b)
focus on evaluating code generation systems by
formulating metrics focused on evaluating the se-
mantics of the generated snippets. Liguori et
al. (2021b) deﬁne two new metrics of syntactic
and semantic correctness. Syntactic correctness

is the ratio of snippets that are compilable. This
is achieved by compiling the generated snippets
with a compiler and recording the ratio of gen-
erated snippets that do compile. For semantic
correctness, domain experts were asked to evalu-
ate whether generated code snippets semantically
express the natural language description. The
APPS dataset evaluates generated snippets based
on the percentage of unit tests the generated snip-
pets pass, this metric is called the Test Case Aver-
age (Hendrycks et al., 2021). Additionally, APPS
evaluate generated codes on their ability to pass
all unit tests for a particular problem, this met-
ric is called Strict Accuracy. Similarly in the Hu-
manEval dataset, the authors introduce a pass@k
metric, in which k code snippets are sampled from
the model. pass@k would be the ratio of prob-
lems in which any of the k samples pass the prob-
lem’s corresponding unit tests. Another notable
metric proposed for evaluating code generation
systems is CodeBLEU (Ren et al., 2020). Code-
BLEU12, combines the n-gram match inspired
from the BLEU score, along with weighed N-gram
match, abstract syntax tree match, and semantic
data-ﬂow match into one metric. The drawback
of CodeBLEU is it needs to have language spe-
ciﬁc AST-parsers implemented which may not be
available to low-resource programming languages
like Haskell and Assembly yet. Current Code-
BLEU supports 6 programming languages: Java,
JavaScript, C#, PHP, GO, Python, and Ruby.

3.8.4 Methods
Here we introduce different deep learning archi-
tectures used for the task of code generation. We
categorize the approaches into LSTM-based se-
quence to sequence, LSTM-based sequence to se-
quence with abstract syntax trees, transformers,
pretrained transformers, and transformers with ab-
stract syntax trees. We survey works in each of
these approaches and describe them in detail. We
focus on the CoNaLa dataset (Yin et al., 2018)
as a case study in Figure 14 and overview all the
works that have benchmarked their systems on this
dataset.

LSTM-based Seq2Seq. Ling et al. (2016)
LSTM based sequence-to-sequence model called
the Latent Predictor Network (LPN) has been
proposed for general-purpose programming lan-
guages (Python and Java) it includes a copying

12https://github.com/microsoft/CodeXGLUE/tree/main/Code-

Code/code-to-code-trans/evaluator/CodeBLEU

Figure 14: Comprehensive review of all code generation models evaluated on the CoNaLa dataset (Yin
et al., 2018) across time. Models are scored using BLEU-4 (Papineni et al., 2002) and use one of the
following deep learning architectures: LSTM-based sequence to sequence, LSTM-based sequence to
sequence models that encode Abstract Syntax Trees (ASTs), Transformer-based models, Transformer
models loaded with pretrained weights, and Transformer models that encode Abstract Syntax Trees. We
note that the LSTM-Seq2Seq with Bahdanau attention (Bahdanau et al., 2014) and CodeBERT (Feng
et al., 2020) experiments were conducted by the authors of this paper. The method of variable standard-
ization is described in (Liguori et al., 2021b, 2022).

mechanism similar to (Gu et al., 2016b) to copy
over identiﬁer names. A weakness in that ap-
proach is that it does not consider code syntax
in its processing and output. Dong et al. (2018)
developed Coarse-to-Fine Decoding, a method-
ology for semantic parsing and code generation
simultaneously. Coarse-to-Fine Decoding essen-
tially uses two neural networks. The ﬁrst neural
network is used to parse the input into a rough
sketch. The rough sketch preserves the original
meaning of the input while removing lower-level
details. This sketch and the original input is fed
into the second neural network which then pro-
duces the output. This method is better than sim-
ply parsing the input through a single neural net-
work. The results show that this method increases
performance when compared to using one neu-
ral network to parse the input to code. Addition-
ally, the Coarse-to-Fine Decoding can be used for
a variety of parsing tasks making it a fairly ﬂex-
ible method. Lin et al. (2018) use an LSTM-
based Seq2Seq and CopyNet (Gu et al., 2016b) to
generate linux bash commands from natural lan-
guage descriptions. Zhong et al. (2017) develop an

LSTM-based sequence to sequence model adapted
from Dong and Lapta (2016) with a pointer net-
work (Vinyals et al., 2015) containing augmented
inputs speciﬁc to the SQL language. Chen et
al. (2021b) propose PlotCoder, an LSTM-based
Seq2Seq with a copying mechanism trained on a
portion on the JuICe dataset (Agashe et al., 2019)
trained to generate plot code snippets in Python
Liguori et al. (2021a) use a standard LSTM-based
sequence to sequence model (Bahdanau et al.,
2014) to generate assembly snippets from natural
language intents that are then compiled into shell-
codes used in exploiting vulnerabilities. Frem-
pong et al. (2021) generate JavaScript XSS ex-
ploits from natural language intents. Weaknesses
of LSTM-based models include generating syn-
tactically incorrect code, however given enough
training samples or on syntactically simple lan-
guages like Assembly, LSTM-based models tend
to generate syntactically correct code with good
accuracy (Liguori et al., 2021b).

LSTM-based Seq2Seq with Abstract Syntax
Trees. To remedy generated syntactically incor-
rect codes, Yin and Neubig (2017a) propose a

novel LSTM-based architecture that generates an
Abstract Syntax Tree (AST) given a natural lan-
guage description and a grammar model. Their
system includes a set of production rules for
python’s abstract grammar, a probabilistic gram-
mar model, and ﬁnally a Recurrent Neural Net-
work with the dimensions of 256 by 50 with a
beam size of 15 for the decoder. The results
in the model mentioned above scored 16.2 exact
match accuracy and a 75.8 BLEU-4 on the Hearth-
stone dataset (Ling et al., 2016), scoring 11.7 ac-
curacy above the state-of-the-art at the time the
Latent Predictor Network model. The proposed
system’s weaknesses include mismatching param-
eter names when deﬁning a function, omitting or
adding default values of parameters when deﬁn-
ing a function, failing to copy a variable name into
the correct position, and lastly the generated code
partially implemented the required functionality.
Rabinovich et al. (2017) leveraged the Abstract
Syntax Description Language (ASDL) framework
for semantic parsing and a Long Short-term Mem-
ory network (LSTM) for code generation. These
trees include two types of modules–composite
types (function and class deﬁnitions, return state-
ments, etc) and primitive types (integers, identi-
ﬁers). Composite modules use LSTM to deter-
mine the appropriate constructor module which
details how the node should expand. These con-
structor modules use LSTM to determine what
constructor ﬁeld modules are needed. TranX (Yin
and Neubig, 2018) uses Abstract Syntax Trees
as an intermediary representation (interlingua) to
generate a programming language snippet from a
natural language statement. TranX achieved state
of the art on the CoNaLa dataset (Yin et al., 2018)
at the time scoring 24.3 BLEU-4 (Yin and Neu-
big, 2019). Yin and Neubig (2019) then incorpo-
rate a hypothesis reranker to rerank the outputs of
TranX and see a 5.7% increase in BLEU on the
CoNaLa dataset setting state of the art at 30.11
(2020a) then explore incor-
BLEU-4. Xu et al.
porating external knowledge from API documen-
tation and mined NL-PL pairs from StackOver-
ﬂow to the TranX model (Yin and Neubig, 2018)
with the hypothesis reranking proposed in (Yin
and Neubig, 2019). This approach improves upon
the reranker model by 2.2% scoring 32.26 BLEU.
We note that LSTM methods with ASTs signif-
icantly outperform the standard LSTM sequence
to sequence scoring around 14.72 BLEU on the

CoNaLa dataset. Another notable work, although
not used in code generation to our knowledge, is
Code2Seq (Alon et al., 2018). Code2seq encodes
the source code tokens along with the AST paths
simultaneously and has seen good success in the
area of Code Documentation Generation & Sum-
marization (c.f.§3.5).

Incorporating structural knowledge from ASTs
into a neural deep learning model ensures correct
syntax of generated code all the time, it also im-
proves model generalizability to NL intents (Yin
and Neubig, 2017a). The drawback is incorporat-
ing ASTs will take time since grammar rules nor-
mally need to be speciﬁed.

Transformer Models. Bonthu et al. (2021)
use a standard transformer architecture (Vaswani
et al., 2017) to generate Python source code from
natural language intents and report BLEU score
of 32.4 and Rouge-L of 82.1 on a custom cu-
rated dataset. Liang et al. (2021) set a baseline
for a turducken-style code generation task where
SQL is embedded with Python. The baseline
used is a transformer model and it achieves 48-
49 BLEU and 18%-21% executable codes when
generating codes from the English and Chinese
natural language intents respectively. Akinbou
et al. (2021) use a standard transformer archi-
tecture and achieve 8.97 BLEU on the CoNaLa
dataset with a 42% syntax error rate. Gem-
mell et al. (2020) utilize a transformer architec-
ture to translate natural language intents to ex-
ecutable snippet representations in Python. The
authors further improve upon the baseline trans-
former model by introducing pseudo-relevance
feedback during the decoding process. During de-
coding process top k documents relevant to the
input are retrieved and a set of common tokens
from the documents are emphasized. The authors
additionally use copying mechanisms inspired by
prior work in pointer networks (See et al., 2017)
to assist with tokens common between intents
and snippets like method names and identiﬁers.
This method achieves 22.3 BLEU on the CoNaLa
dataset. We observe that it is outperformed by
a standard LSTM seq2seq which scores around
24.34 BLEU when using variable standardization
using an intent parser as proposed in (Liguori
et al., 2021b, 2022). A higher BLEU score using
the standardization method is likely due to higher
likelihood of generalization and more token over-
lap when standardizing tokens referring to variable

names and values.

Pretrained Transformer Models.

Mas-
tropaolo et al. (2021) propose pretraining and ﬁne-
tuning a T5 model (Raffel et al., 2019a) on a cor-
pora of English text and source code and then
ﬁne-tune on several software engineering tasks in-
cluding to generate assert statements (test meth-
ods) in Java. CodeBERT (Feng et al., 2020) is
a pretrained transformer language model based
on RoBERTa (Liu et al., 2019) (a variant of the
BERT model (Devlin et al., 2018)) pretrained on
the CodeSearchNet corpus (Husain et al., 2019).
CodeBERT has been used to generate exploit code
snippets from natural language intents (Liguori
et al., 2021b). Orlanski and Gittens (2021) ﬁne-
tune the pretrained transformer BART (Lewis
et al., 2020) on the CoNaLa dataset and achieve
a performance of 26.24 BLEU which is compa-
rable to that of CodeBERT scoring around 22.51
BLEU. Furthermore, BART when ﬁnetuned with
both the CoNaLa annotated and mined datasets
it achieved a BLEU score of 30.55. The authors
also explore adding the question body along with
the natural language intent as input to BART and
ﬁnd an increase in performance. BART achieves
34.35 when ﬁnetuned with question bodies and
35.32 when ﬁnetuned with questioned bodies on
the both the annotated CoNaLa dataset and the
mined CoNaLa datasets. Norouzi et al. (2021)
ﬁnd that a BERT-based encoder (Devlin et al.,
2018) and decoder model with a copying mech-
anism (Gu et al., 2016b) can achieve better per-
formance by utilizing mine-able monolingual code
data. The encoder is frozen on the monolingual
code data and the decoder gets an additional ob-
jective during training of autoencoding the mono-
lingual source code that corresponds to the in-
tent that is fed into the encoder. This process is
named Target Auto Encoding (TAE). Using the ap-
proach the decoder gets to also learn the source
code meaning representation along with the en-
coder representation on the natural language in-
tent. This approach achieves 32.57 BLEU on the
CoNaLa dataset compared to the same set up with-
out TAE which scores 30.98 BLEU. Scholak et
al. (2021) propose PICARD a simple and effective
decoder-constraint algorithm that works with pre-
trained encoder-decoder models. Using PICARD
with a T5-3B model (Raffel et al., 2019b) achieves
state of the art on two SQL generation tasks from
NL Spider (Yu et al., 2018) and CoSQL (Yu et al.,

2019a). GraphCodeBERT (Guo et al., 2020) is a
pretrained model for programming language that
is pretrained using dataﬂow which encompasses
the semantic structure of the code. The pretrain-
ing objectives used include masked language mod-
eling, code structure edges, and representation
alignment between source code and code struc-
ture. Other pretrained transformers used on source
code include CodeT5 (Wang et al., 2021b). Code-
Trans (Elnaggar et al., 2021), PyMT5 (Clement
et al., 2020), CuBERT (Kanade et al., 2020),
PLBART (Ahmad et al., 2021), ProphetNet-X (Qi
et al., 2021), CoTexT (Phan et al., 2021), T5-
Code (Mastropaolo et al., 2021), GraphCode-
BERT (Guo et al., 2020), and AlphaCode (Li et al.,
2022). Pretrained GPT-style Models for source
code generation include CodeGPT (Svyatkovskiy
et al., 2020), and GPT-Codex (Chen et al., 2021a).

Transformer Models with Abstract Syntax
Trees. Dahal et al. (2021) explore using tree rep-
resentations of natural language utterances in code
generation. Tree representations of natural lan-
guage utterances are derived using constituency
trees, which aim to describe the syntactic (gram-
matical) structure of an uttered sentence by divid-
ing the sentence into sub-phrases. The authors run
a series of experiments generating the AST repre-
sentation of code using text-to-AST as a baseline,
linearized tree-to-tree which uses the constituency
tree of NL as input encoded using a standard trans-
former encoder, and structured tree-to-tree model.
For the structured tree-to-tree model the authors
utilize a structure aware Tree Transformer archi-
tecture (Nguyen et al., 2020) with a pointer net-
work (Vinyals et al., 2015) to copy tree leaves
from the input to the output AST. The authors
report improvements when using structure aware
encodings of NL in their structured tree-to-tree
model and achieve 30.30 BLEU on the CoNaLa
dataset. Sun et al. (2019b) propose TreeGen a neu-
ral architecture based on the Transformer model
that incorporates AST grammar rules into the net-
work.
Incorporating ASTs into Transformers is
observed to alleviate long-range dependencies be-
tween source code tokens (Sun et al., 2019b; Dahal
et al., 2021) and better models input with multiple
variables (Dahal et al., 2021). Multi-task Learn-
ing for Code Generation. Multi-task learning in
machine learning is training the model to perform
multiple tasks simultaneously. Wei et al. (2020a)
propose using multi-task learning of code genera-

Area

Description

Evaluation

References

Program Synthesis

The task of synthesizing complete or partial programs
given a speciﬁcation, often structured input.

% compilable programs,
% logically correct pro-
grams, synthesis duration,
problems solved (Schuster
et al., 2021) & other appli-
cation speciﬁc metrics

(Shi et al., 2020; Patra
and Pradel, 2016; Gul-
wani et al., 2017; Cum-
mins et al., 2017a; Ellis
et al., 2021; Austin et al.,
2021)

Program Analysis

Focuses on extracting semantic properties from pro-
grams and often classifying codebases using extracted
properties.

Accuracy, Precision, Re-
call, & F1

(Raychev et al., 2015;
Mangal et al., 2015; Oh
et al., 2015; Panthaplackel
et al., 2020; Mou et al.,
2016a)

Program Repair

Focuses on building models to repair codebases given a
compiler error message and a corresponding codebase.

full repair, partial repair,
mes-
resolved
al.,
sages
2019).

error
(Hajipour

et

et

al.,

(Gupta
2017;
Hajipour et al., 2019; Ya-
sunaga and Liang, 2020;
and Troshin,
Chirkova
2020; Lutellier
al.,
et
2020; Lu et al., 2021b;
Jiang et al., 2021)

Clone Detection∗

Focuses on measuring the semantic similarity between
two codes. There are variations of this task binary clas-
siﬁcation and retrieving code of semantic equivalence.

Precision, Recall,
F1,
MAP (Beitzel et al., 2009;
Lu et al., 2021a)

(Svajlenko et al., 2014;
Wang et al., 2020b; Büch
and Andrzejak, 2019; Wei
and Li, 2017; Chilowicz
et al., 2009)

Defect Detection∗

Cloze Test∗

Code Translation∗

Focuses on detecting insecure code that can be ex-
ploited in attacks such as DoS. The task is a binary clas-
siﬁcation problem.

Accuracy, F1, Precision,
Recall

(Zhou et al., 2019; Li et al.,
2018)

Focuses on predicting the correct token in a code func-
tion given a natural language description and a code
function with a masked token.

Accuracy

Focuses on using machine learning to translate source
code from a particular language (e.g. Python) to an-
other language (e.g. JavaScript). Also known as Code
Transpiling.

Exactness, BLEU (Pap-
ineni et al., 2002), &
CodeBLEU (Ren et al.,
2020)

(Husain et al., 2019; Lu
et al., 2021a; Feng et al.,
2020)

(Lachaux et
Feng et al., 2020)

al., 2020;

Code Reﬁnement∗

Focuses on automatically rewriting a codebase which is
either buggy or convoluted.

Exactness, BLEU (Pap-
ineni et al., 2002), &
CodeBLEU (Ren et al.,
2020)

(Hata et al., 2018; Tufano
et al., 2019)

Code Completion∗

Focuses on generating a code snippet that best com-
pletes a given program.

ROUGE (Lin, 2004), ex-
actness, edit similarity

(Svyatkovskiy et al., 2019,
2020; Chirkova, 2020)

Search

Code
Retrieval∗
tion Retrieval)

and
(Informa-

Retrieve a code snippet given a natural language query. Mean Reciprocal Rank
(MRR) & NDCG (Vechto-
mova, 2009; Husain et al.,
2019)

(Feng et al., 2020; Hu-
sain et al., 2019; Gu et al.,
2018)

Code
Generation∗

Documentation

Generating natural language descriptions of code snip-
pets. Includes generating a function name for a function
in code. Also known as Code summarization.

BLEU (Papineni et al.,
2002) and human evalua-
tion

(Oda et al., 2015a; Feng
et al., 2020)

Focuses on translating online code doc pages from one
human language to another.

BLEU (Papineni et al.,
2002)

(Lu et al., 2021a)

Documentation
Translation∗

Code Generation∗

Generating code snippets given natural language in-
tents.

et

al.,

Exactness, BLEU (Pa-
2002),
pineni
correct-
semantic
ness
al.,
et
pass@k (Chen
2021b),
2021a), Code-
al.,
et
BLEU (Ren et al., 2020)

(Liguori

(Yin and Neubig, 2018;
Xu et al., 2020a; Lin et al.,
2018)

(Chen Jr and Bunescu,
2019; Carpenter, 1997;
Berant and Liang, 2014)

Semantic Parsing

Converting natural
cutable logical forms.

language to unambiguous exe-

Exactness

∗ In CodeXGLUE.

Table 4: Overview of the areas in Code Intelligence (CI) , typical evaluation methods, and notable works
in each area.

tion and summarization and report improvements
in code summarization and retrieval benchmarks.
Wei et al. (2019) report improvements when dual
training a model to generate code and to sum-
marize code simultaneously. Parvez et al. (2021)
propose REDCODER, a code-search augmented
model that retrieves code relevant to the inputted
natural language description from an open source
database to supplement the code generator model.

3.9 Other Code Intelligence Areas

Other areas of Code Intelligence (CI) include pro-
gram repair, which focuses on building systems
that automatically ﬁx bugs in codebases using a
compiler error message and a codebase as in-
put. Notable program repair works include Deep-
Fix (Gupta et al., 2017) and DrRepair (Yasunaga
and Liang, 2020). Defect detection is another area
of CI focused on detecting insecure code primarily
through binary classiﬁcation of codebases e.g. (Li
et al., 2018; Zhou et al., 2019)). Cloze Test is
coding multiple choice problem where a model is
tasked with predicting a masked code token. Code
translation focuses on translating a codebase from
one language (e.g. Java) to another programming
language (e.g Python). There are various works
in code translation (Lachaux et al., 2020; Feng
et al., 2020). Code reﬁnement focuses on rewrit-
ing a codebase with the aim of simpliﬁcation of re-
moving bugs e.g. (Hata et al., 2018; Tufano et al.,
2019). Lastly, code completion focuses on gener-
ating a code snippet that best completes a given
codebase e.g. (Svyatkovskiy et al., 2019, 2020;
Chirkova, 2020). See Table 4 for an overview of
all the CI ﬁelds, metrics used in each ﬁeld, and
example works.

3.10 Summary

Application areas of NLP on source code such
as program synthesis and program repair focus
on automating redundant processes of software
development. Program repair tends to focus on
repairing programs given a compiler error mes-
sage and is evaluated by percent of compilable
programs, completely repaired (no logical errors)
and partially repaired programs (some logical er-
rors). Program synthesis tends to focus on gen-
erating full or partial programs often given struc-
tured input such as program examples, program
description and output (Shi et al., 2020). Synthe-
sized programs are often evaluated using percent-
ages (%) of semantic and compilable programs.

However, not all programming synthesis evalu-
ate their programs, some works such as Cum-
mins et al. (2017b), just focus on their applica-
tion domain (program run-time speed-up) and dis-
card generated programs that do not work. On
the other hand, Program analysis is analogous to
understanding language as opposed to generating
it.
It tends to focus on extracting features from
developed code and using classiﬁcation methods.
CodeXGLUE lists several tasks of code intelli-
gence and understanding. Those tasks are further
listed along with all other known applications of
NLP on source code in Table 4. The table also
lists a description of the task, how the task is eval-
uated, and works that have participated in the task
domain.

For an extensive survey on machine learning for
big code c.f. Allamanis et al. (2018), code genera-
tion and semantic parsing (Lee et al., 2021), and a
short survey speciﬁcally on code generation from
natural language (Shin and Nam, 2021).

Code generation has promise in being inte-
grated into conversational interfaces to enhance
(Artzi and Zettle-
language understanding (e.g.
moyer, 2011)).
If code generation is integrated
with conversational assistants we can see a rise of
programming assistants which could be very help-
ful for programmers. Code generation may also
be helpful for novice programmers and can have
applications in ﬁelds such as computer science ed-
ucation. In the next section we will overview Con-
versational Artiﬁcial Intelligence (A.I.) and appli-
cation of conversational artiﬁcial intelligence in
education and software engineering.

4 Conversational Artiﬁcial Intelligence

In this section, we will focus on overviewing deep-
learning-based automated conversational agents,
also known as, dialogue systems. Conversational
agents are popular and are widely accessible, from
virtual sales agents to personal assistants like
Google, Alex, or Siri, and have been applied in
big domains such as general healthcare (Montene-
gro et al., 2019) and mental healthcare (Weizen-
baum et al., 1966; D’Alfonso et al., 2017). There
are generally two types of deep learning dialogue
systems: Open-domain dialogue systems (also
known as chit-chat chatbots) and task-oriented di-
alogue systems (also known as close-domain dia-
logue systems) (Santhanam and Shaikh, 2019; Ni
et al., 2021). Task-oriented systems tend to be

Figure 15: Example output from a Natural Language Understanding (NLU) module by (Ni et al., 2021).
The utterance “Recommend a movie at Golden Village tonight" is within the “movie" domain and
the user’s goal is to “ﬁnd_movie" at the destination “Golden Village" with the time “tonight". Here
desti and time are slots and are accompanied by their corresponding slot-values “Golden Village"
and “tonight" respectively.

oriented towards a speciﬁc goal or a task such as
booking a hotel, reserving a restaurant table, etc.
Task-Oriented dialogue systems such as Simple-
TOD (Hosseini-Asl et al., 2020) are often entirely
data-driven and are proﬁcient in a certain set of do-
mains given enough training examples within that
domain. Task-oriented systems are often made up
of 4 modules: Natural Language Understanding
(NLU), Dialogue State Tracking (DST), Dialogue
Policy Learning, and Natural Language Genera-
tion (NLG). NLU focuses on classifying what the
user’s goal is within a domain and parses out task-
relevant words from the user utterance into slots
c.f. Figure 15. DST focuses on calibrating the dia-
logue state based on the current user utterance and
the conversational history, it includes the user’s
goal and slot-value pairs. Dialogue Policy Learn-
ing is a module that decides the next action the
system should take based on the dialogue state.
NLG focuses on generating natural language from
the selected dialogue action. Open-domain dia-
logue systems such as MILABOT (Serban et al.,
2017) focus on chatting with a user without any
domain restrictions and can cover a wide variety
of topics (Ram et al., 2018). For a survey on dia-
logue systems c.f. (Santhanam and Shaikh, 2019;
Ni et al., 2021). Next, we will overview works that
apply task-oriented dialogue systems in software
engineering and computer science education.

4.1 Conversational Assistants for Software

Engineering

This section focuses on overviewing some of the
uses of conversational systems in the ﬁeld of soft-
ware engineering. We observe conversational as-
sistants assisting programming in performing gen-

eral programming tasks and assisting with speciﬁc
software engineering workﬂows (e.g. Github ver-
sion control actions).
General Programming Assistants. Chaurisa and
Mooney (2017) propose a system that can engage
a human user in dialogue for IFTTT code gen-
eration (Quirk et al., 2015). However, the di-
alogue system mainly engages with the user to
clarify their intent until the correct code is pro-
duced. Austin et al. (2021) run controlled exper-
iments with a simple dialogue system that gen-
erates Python code. The system mainly focuses
on collaborating with a human to solve a particu-
lar programming task primarily through asking for
clariﬁcations and modifying the generated code as
illustrated in Figure 16. These dialogue systems
are simple and are mainly focused on solving one
particular program, they neither include modules
for dialogue state tracking nor code understand-
ing. The dialogue systems also do not seek to col-
laborate with the human as much as get the hu-
man to help the system solve a particular program-
ming task. The CoSQL dataset (Yu et al., 2019a)
does include some Task-Oriented dialogue com-
ponents such as Dialogue State Tracking and user-
system actions for dialogue policy learning, how-
ever, interfacing with a database via conversation
does not assist with engineering new software. It is
also worthy to mention IDE plugins and tools that
utilize CI functionalities such as code search and
code generation to assist software engineers and
data scientists by generating or retrieving code us-
ing natural language speciﬁcations written by the
user (Xu et al., 2021; Heyman et al., 2021). Xu et
al. (2021) conduct a user study and ﬁnd largely
positive qualitative results however, the plugin’s

relationship with increased productivity, program
quality, and correctness were inconclusive.
Conversational Assistants for Software Engi-
neering Workﬂows. Bradly et al. (2018) build
Devy, a conversational assistant with an NLU
module that recognizes high-level Github-related
intents and can execute a set of pre-deﬁned low-
level Github tasks. Paikari et al. (2019) develop a
task-oriented bot to inform software engineers of
code conﬂicts using Github and to assist develop-
ers in locking/unlocking a source ﬁle for edits.

4.2 Conversational Assistants for Computer

Science Education

Hobert (2019) proposes a coding tutoring system
that is able to respond to open-ended knowledge
questions, assess submitted source code, and guide
students in natural language through a chat inter-
face. The coding tutor’s interface has a coding
area and a chat area where the user is able to con-
verse with the automated coding tutor. The task-
oriented dialogue tutor contains intent and entity
recognition modules and a natural language gen-
It also has a set of predeﬁned
eration module.
“learning paths" based on a Finite State Machine
stored in a database. Responses to student ques-
tions and guidance given by the system are all de-
rived from the predeﬁned ﬁnite state machine spe-
ciﬁc to a programming problem. To evaluate the
coding tutor, a questionnaire was handed out to
students asking them to rate the system on useful-
ness, ease of use, practice, and whether students
would use the system again.

5 Future Directions

This section explores possible future directions of
research at the intersection between conversational
systems and CI. We focus on the two areas of com-
puter science education and software engineering.

5.1 Programming-Oriented Dialogue

Systems

Task-oriented dialogues currently focus on assist-
ing the human user in completing API-centered
redundant tasks such as booking a hotel. Cur-
rent advances in Code Intelligence research such
as the availability of big data containing pairs in
both code and natural language, code search, pro-
gram repair, code generation, and recently pro-
gram execution tracing (Nye et al., 2021), en-
able us to create more intelligence tools to enable

professional and novice programmers. There is
room to create dialogue systems that can assist hu-
mans in speciﬁc programming tasks such as code-
refactoring, generating methods from NL descrip-
tions, retrieving code examples, bug ﬁxing, and
explaining portions of the code in natural language
through conversation. Some of these tasks are
currently automated through various IDE plugins
e.g. (Xu et al., 2021) however there is a back-and-
forth process between a system and a programmer
that can be captured well in dialogue. To support
such capabilities, task-oriented dialogue systems
can be adapted. The natural language understand-
ing module in task-oriented dialogue systems can
be enhanced with a code-understanding module,
and likewise, with the natural language generation
component, it can be coupled with a code genera-
tion module. To the best of our knowledge, there
exist no programming-oriented dialogue datasets
which are crucial to driving this research area for-
ward.

5.2 Computer Science Education

Self-Regulated Learning (SRL). SRL is a frame-
work for understanding how students control their
behavior during learning and it includes: 1) Cog-
nitive processes related to content knowledge, rea-
soning, and problem-solving; 2) Metacognitive
processes where the learner plans their learning
endeavors, and in identifying gaps in knowledge
or seeking help; 3) Affective processes that in-
clude a student’s goals and emotional states. (Pin-
trich, 2000). There is potential in adapting di-
alogue systems to deliver personalized learning
interventions through a dialogue system. Exam-
ple cognitive interventions that can be delivered
through a conversational assistant with CI include:
Generating code-related hints to students in the
form of Socratic questions, also known as guided-
inquiry i.e., smaller subquestions that can guide
to a ﬁnal answer. Cobbe et al. (2021) utilize
GPT-3 (Brown et al., 2020) and ﬁnetune on 800
examples to generate mathematical Socratic sub-
questions by conditioning each subquestion on a
ground-truth mathematical step in the solution.13.
Other interventions can include generating exam-
ple code as hints, code repair assistance, and rec-
ommending course content. Metacognitive in-
terventions by code-aware dialogue systems can

13See an example here: https://github.com/openai/grade-

school-math

Figure 16: Example of human-AI conversational programming from (Austin et al., 2021). Human utter-
ances are shown in purple and AI utterances are denoted in blue.

include identifying knowledge gaps from coding
patterns and mistakes and suggesting that the stu-
dent asks the human Teaching Assistant (TA) for
help.

IDE-based Learning Analytics.

IDE-based
learning analytics utilizes Integrated Development
Environments (IDEs) to collect data about learn-
ers’ programming patterns and deliver learning in-
terventions (Hundhausen et al., 2017). Program-
ming data collected from IDEs can include: (1)
editing data (e.g., code snapshots), (2) compila-
tion data (e.g., compilation errors), (3) execution
data (e.g., run-time exceptions), and (4) debugging
data (e.g., breakpoints, steps, and inspecting vari-
ables). Hundhausen et al. (2017) proposed a four-
phase process model for IDE-based data analytics
consisting of: (1) data collection, (2) data anal-
ysis, (3) intervention design, and (4) intervention
delivery. There is extensive work in data collection
and data analysis in IDE-based learning analyt-
ics (Watson et al., 2013; Diana et al., 2018; Ahadi
et al., 2016; Carter et al., 2015) however to the best
of our knowledge, there is little work on deliver-
ing automated interventions. Interventions are pri-
marily focused on ﬁxing syntax errors (Bhatia and

Singh, 2016), enhanced error messages (Becker
et al., 2016), and generating hints to program-
mers (Chow et al., 2017; Rivers and Koedinger,
2017). Carter et al (2015) proposes the Pro-
gramming State Model (PSM), which “categorizes
students’ programming within a two-dimensional
space that captures both a student’s current activ-
ity (e.g., editing, debugging) and the correctness
of the student’s most recently compiled program-
ming solution". A dialogue system with PLP ca-
pabilities can automate the process of intervention
design and delivery when integrated with IDE-
analytics. Furthermore, it can work with an IDE-
based learning analytics code understanding mod-
ule. A programming-oriented dialogue system in
which the user intents are derived from learning
analytics-inspired IDE-activity similar to the pro-
posed Programming State Model. The dialogue
policy learner would decide what intervention to
perform given a set of pre-deﬁned educational in-
terventions. Dorodchi et al. (2020) propose a cus-
tom IDE prototype that can be integrated with a
dialogue system to deliver personalized learning
interventions.

5.3 Human Computer Interaction

Another area of research can focus on designing
programmer-centered IDE interfaces around con-
versational programming, where both a human
and a conversational agent with CI can collabo-
rate on programming. A glimpse of conversa-
tional programming can be seen in Figure 16 from
Austin et al. (2021).

6 Conclusion

In this paper, we overviewed the ﬁeld of code in-
telligence (CI), which focuses on applying artiﬁ-
cial intelligence (AI) to source code. We identi-
ﬁed 14 key areas of research in CI summarized
in Table 4. Through analyzing existing works
in code generation using the CoNaLa dataset we
observe a trend of Transformer-based models be-
ing more heavily used in recent years and push-
ing state-of-the-art boundaries on this task see Fig-
ure 14. We summarize our ﬁndings from system-
atically reviewing works that propose and curate
datasets containing natural language and source
code ( Table 3). Conversational AI is generally
divided into two areas of research: Open-domain
dialogue systems which can chat about a wide va-
riety of topics, and task-oriented dialogue systems
which focus on assisting the user with complet-
ing a speciﬁc goal or a task. We identify existing
works in task-oriented dialogue systems with CI
capabilities, mainly falling into assisting profes-
sional programmers with the software engineering
process or assisting novice programmers in edu-
cational contexts. We identify future directions
for research at the intersection of dialogue sys-
tems and code intelligence. The ﬁrst major direc-
tion is Programming-Oriented Dialogue Systems
(PODS), which are task-oriented dialogue systems
with CI capabilities.

We identify a dire need for dialogue datasets for
this area. Dialogue datasets can then be utilized
to create PODS to assist professional developers
by having a dialogue system encompass many ex-
isting code intelligence plugins. We also identify
a research opportunity for POD systems to assist
computer science learners by facilitating the self-
regulated learning process. Enhancing POD sys-
tems with learning analytics would enable PODS
to learn more about the student user and intervene
more appropriately. Finally, we identify research
opportunities in human computer interaction to
create custom interfaces where a PODS and a user

can collaborate on solving programming problems
effectively.

References

Omri Abend and Ari Rappoport. 2013. Univer-
sal conceptual cognitive annotation (ucca).
In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 228–238.

Rajas Agashe, Srinivasan Iyer, and Luke Zettle-
moyer. 2019. Juice: A large scale distantly su-
pervised dataset for open domain context-based
In Proceedings of the 2019
code generation.
Conference on Empirical Methods in Natural
Language Processing and the 9th International
Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 5439–5449.

Armen Aghajanyan, Jean Maillard, Akshat Shri-
vastava, Keith Diedrick, Michael Haeger, Hao-
ran Li, Yashar Mehdad, Veselin Stoyanov,
Anuj Kumar, Mike Lewis, and Sonal Gupta.
2020. Conversational semantic parsing.
In
Proceedings of the 2020 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), pages 5026–5035, Online. Associa-
tion for Computational Linguistics.

A. Ahadi, V. Behbood, A Vihavainen, J. Prior, and
R. Lister. 2016. Students’ Syntactic Mistakes in
Writing Seven Different Types of SQL Queries
and its Application to Predicting Students’ Suc-
cess. In Proceedings of the 47th ACM Technical
Symposium on Computing Science Education,
SIGCSE ’16, pages 401–406.

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray,
and Kai-Wei Chang. 2021. Uniﬁed pre-training
for program understanding and generation. In
the
Proceedings of
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, pages 2655–2668, Online. Asso-
ciation for Computational Linguistics.

the 2021 Conference of

Yuka Akinobu, Momoka Obara, Teruno Kajiura,
Shiho Takano, Miyu Tamura, Mayu Tomioka,
Is neural ma-
and Kimio Kuramitsu. 2021.
chine translation approach accurate enough for
In Proceedings of the 1st
coding assistance?

ACM SIGPLAN International Workshop on Be-
yond Code: No Code, BCNC 2021, page 23–28,
New York, NY, USA. Association for Comput-
ing Machinery.

Miltiadis Allamanis. 2019. The adverse effects
of code duplication in machine learning mod-
els of code. In Proceedings of the 2019 ACM
SIGPLAN International Symposium on New
Ideas, New Paradigms, and Reﬂections on Pro-
gramming and Software, Onward! 2019, page
143–153, New York, NY, USA. Association for
Computing Machinery.

Miltiadis Allamanis, Earl T. Barr, Premkumar De-
vanbu, and Charles Sutton. 2018. A Survey of
Machine Learning for Big Code and Natural-
ness. ACM Comput. Surv., 51(4):81:1–81:37.

Miltiadis Allamanis, Hao Peng, and Charles Sut-
ton. 2016. A convolutional attention network
for extreme summarization of source code. In
International conference on machine learning,
pages 2091–2100. PMLR.

Uri Alon, Shaked Brody, Omer Levy, and Eran
Yahav. 2018. code2seq: Generating sequences
from structured representations of code. In In-
ternational Conference on Learning Represen-
tations.

Uri Alon, Meital Zilberstein, Omer Levy, and Eran
Yahav. 2019. code2vec: Learning distributed
representations of code. Proceedings of the
ACM on Programming Languages, 3(POPL):1–
29.

Giambattista Amati. 2009. BM25. Springer US,

Boston, MA.

Aida Amini, Saadia Gabriel, Shanchuan Lin,
Rik Koncel-Kedziorski, Yejin Choi, and Han-
naneh Hajishirzi. 2019. MathQA: Towards in-
terpretable math word problem solving with
In Proceedings
operation-based formalisms.
of the 2019 Conference of the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers), pages
2357–2367, Minneapolis, Minnesota. Associa-
tion for Computational Linguistics.

Jacob Andreas, John Bufe, David Burkett, Charles
Chen, Josh Clausman, Jean Crawford, Kate

Crim, Jordan DeLoach, Leah Dorner, Jason Eis-
ner, Hao Fang, Alan Guo, David Hall, Kristin
Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk,
Smriti Jha, Dan Klein, Jayant Krishnamurthy,
Theo Lanman, Percy Liang, Christopher H. Lin,
Ilya Lintsbakh, Andy McGovern, Aleksandr
Nisnevich, Adam Pauls, Dmitrij Petters, Brent
Read, Dan Roth, Subhro Roy, Jesse Rusak,
Beth Short, Div Slomin, Ben Snyder, Stephon
Striplin, Yu Su, Zachary Tellman, Sam Thom-
son, Andrei Vorobev, Izabela Witoszko, Jason
Wolfe, Abby Wray, Yuchen Zhang, and Alexan-
der Zotov. 2020. Task-Oriented Dialogue as
Dataﬂow Synthesis. Transactions of the Asso-
ciation for Computational Linguistics, 8:556–
571.

Yoav Artzi, Nicholas Fitzgerald, and Luke Zettle-
moyer. 2014. Semantic parsing with Combi-
In Proceedings
natory Categorial Grammars.
of the 2014 Conference on Empirical Methods
in Natural Language Processing: Tutorial Ab-
stracts, Doha, Qatar. Association for Computa-
tional Linguistics.

Yoav Artzi and Luke Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations.
In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 421–432.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly
supervised learning of semantic parsers for
mapping instructions to actions. Transactions
of the Association for Computational Linguis-
tics, 1:49–62.

Lakshmanan Arumugam. 2020. Semantic code
search using code2vec: A bag-of-paths model.
Master’s thesis, University of Waterloo.

Jacob Austin, Augustus Odena, Maxwell Nye,
Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry,
Program synthesis
Quoc Le, et al. 2021.
arXiv preprint
with large language models.
arXiv:2108.07732.

Thanassis Avgerinos, Sang Kil Cha, Brent
Lim Tze Hao, and David Brumley. 2011. AEG:
Automatic Exploit Generation.
Publisher:
Carnegie Mellon University.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E
arXiv

Hinton. 2016. Layer normalization.
preprint arXiv:1607.06450.

on Engineering Interactive Computing Systems,
EICS ’18, New York, NY, USA. Association for
Computing Machinery.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau,
Aaron C. Courville, and Yoshua Bengio. 2017.
An actor-critic algorithm for sequence predic-
tion. ArXiv, abs/1607.07086.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. arXiv
preprint arXiv:1409.0473.

Mehdi Bahrami, NC Shrikanth, Shade Ruangwan,
Lei Liu, Yuji Mizobuchi, Masahiro Fukuyori,
Wei-Peng Chen, Kazuki Munakata, and Tim
Menzies. 2021. Pytorrent: A python library
corpus for large-scale language models. arXiv
preprint arXiv:2110.01710.

Matej Balog, Alexander L Gaunt, Marc
Brockschmidt, Sebastian Nowozin, and Daniel
Tarlow. 2016. Deepcoder: Learning to write
programs. arXiv preprint arXiv:1611.01989.

Laura Banarescu, Claire Bonial, Shu Cai,
Madalina Georgescu, Kira Grifﬁtt, Ulf Herm-
jakob, Kevin Knight, Philipp Koehn, Martha
Palmer, and Nathan Schneider. 2013. Ab-
stract meaning representation for sembanking.
In Proceedings of the 7th linguistic annotation
workshop and interoperability with discourse,
pages 178–186.

Antonio Valerio Miceli Barone and Rico Sennrich.
2017. A parallel corpus of python functions and
documentation strings for automated code doc-
umentation and code generation. arXiv preprint
arXiv:1707.02275.

B. A. Becker, G. Glanville, R. Iwashima, C. Mc-
Donnell, K. Goslin, and C. Mooney. 2016. Ef-
fective compiler error message enhancement for
novice programming students. Computer Sci-
ence Education, 26(2-3):148–175.

Steven M. Beitzel, Eric C. Jensen, and Ophir
Frieder. 2009. MAP. Springer US, Boston,
MA.

Tony Beltramelli. 2018. Pix2code: Generating
code from a graphical user interface screenshot.
In Proceedings of the ACM SIGCHI Symposium

Jonathan Berant, Andrew Chou, Roy Frostig,
and Percy Liang. 2013. Semantic parsing on
In Pro-
freebase from question-answer pairs.
ceedings of the 2013 conference on empirical
methods in natural language processing, pages
1533–1544.

Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 1415–1425.

S. Bhatia and R. Singh. 2016. Automated correc-
tion for syntax errors in programming assign-
ments using recurrent neural networks. arXiv
preprint arXiv:1603.06129.

Sridevi Bonthu, S. Rama Sree, and M. H. M. Kr-
ishna Prasad. 2021. Text2pycode: Machine
translation of natural language intent to python
source code. In Machine Learning and Knowl-
edge Extraction, pages 51–60, Cham. Springer
International Publishing.

Léon Bottou. 2010. Large-scale machine learn-
In Pro-
ing with stochastic gradient descent.
ceedings of COMPSTAT’2010, pages 177–186.
Springer.

Nick C. Bradley, Thomas Fritz, and Reid Holmes.
2018. Context-aware conversational developer
In Proceedings of the 40th Inter-
assistants.
national Conference on Software Engineering,
ICSE ’18, page 993–1003, New York, NY,
USA. Association for Computing Machinery.

Tom B Brown, Benjamin Mann, Nick Ry-
der, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell, et al. 2020. Lan-
arXiv
guage models are few-shot learners.
preprint arXiv:2005.14165.

Lutz Büch and Artur Andrzejak. 2019. Learning-
based recursive aggregation of abstract syn-
In 2019
tax trees for code clone detection.
IEEE 26th International Conference on Soft-
ware Analysis, Evolution and Reengineering
(SANER), pages 95–104.

Ethan Caballero, . OpenAI, and Ilya Sutskever.

2016. Description2Code Dataset.

Qingqing Cai and Alexander Yates. 2013. Seman-
tic parsing Freebase: Towards open-domain se-
mantic parsing. In Second Joint Conference on
Lexical and Computational Semantics (*SEM),
Volume 1: Proceedings of the Main Conference
and the Shared Task: Semantic Textual Similar-
ity, pages 328–338, Atlanta, Georgia, USA. As-
sociation for Computational Linguistics.

Bob Carpenter. 1997.

Type-logical semantics.

MIT press.

A. S. Carter, C. D. Hundhausen, and O. Ades-
ope. 2015. The normalized programming state
model: Predicting student performance in com-
puting courses based on programming behavior.
In Proceedings of the eleventh annual Interna-
tional Conference on International Computing
Education Research, pages 141–150.

Shobhit Chaurasia and Raymond J Mooney. 2017.
In Proceedings
Dialog for language to code.
of the Eighth International Joint Conference on
Natural Language Processing (Volume 2: Short
Papers), pages 175–180.

Mark Chen, Jerry Tworek, Heewoo Jun, Qim-
ing Yuan, Henrique Ponde, Jared Kaplan, Harri
Edwards, Yura Burda, Nicholas Joseph, Greg
Brockman, et al. 2021a. Evaluating large lan-
guage models trained on code. arXiv preprint
arXiv:2107.03374.

Xinyun Chen, Linyuan Gong, Alvin Cheung, and
Dawn Song. 2021b. Plotcoder: Hierarchical
decoding for synthesizing visualization code in
In Proceedings of the
programmatic context.
59th Annual Meeting of the Association for
Computational Linguistics and the 11th Inter-
national Joint Conference on Natural Language
Processing (Volume 1: Long Papers).

Charles Chen Jr and Razvan Bunescu. 2019. Con-
text dependent semantic parsing over tempo-
In Proceedings of the
rally structured data.
2019 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers), pages 3576–
3585.

Jianpeng Cheng, Devang Agrawal, Héctor
Joris
Martínez Alonso, Shruti Bhargava,
Driesen, Federico Flego, Dain Kaplan, Dimitri
Kartsaklis, Lin Li, Dhivya Piraviperumal,
Jason D. Williams, Hong Yu, Diarmuid
Ó Séaghdha, and Anders Johannsen. 2020.
Conversational semantic parsing for dialog
In Proceedings of the 2020
state tracking.
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 8107–
8117, Online. Association for Computational
Linguistics.

Jianpeng Cheng, Li Dong, and Mirella La-
pata. 2016.
Long short-term memory-
networks for machine reading. arXiv preprint
arXiv:1601.06733.

Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. 2019. Generating long sequences
arXiv preprint
with sparse transformers.
arXiv:1904.10509.

Michel Chilowicz, Etienne Duris, and Gilles
Roussel. 2009. Syntax tree ﬁngerprinting for
source code similarity detection. In 2009 IEEE
17th international conference on program com-
prehension, pages 243–247. IEEE.

Nadezhda Chirkova. 2020. Neural code comple-
tion with anonymized variable names. arXiv
preprint arXiv:2010.12693.

Nadezhda Chirkova and Sergey Troshin. 2020.
Empirical study of transformers for source
code. arXiv preprint arXiv:2010.07987.

Kyunghyun Cho, Bart van Merriënboer, Caglar
Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014.
Learning phrase representations using rnn
encoder–decoder for statistical machine transla-
tion. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1724–1734.

S. Chow, K. Yacef, I. Koprinska, and J. Curran.
2017. Automated data-driven hints for com-
In Adjunct Pub-
puter programming students.
lication of the 25th Conference on User Model-
ing, Adaptation and Personalization, pages 5–
10.

Colin Clement, Dawn Drain, Jonathan Timcheck,
Alexey Svyatkovskiy, and Neel Sundaresan.

2020. Pymt5: Multi-mode translation of natural
language and python code with transformers. In
Proceedings of the 2020 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), pages 9052–9065.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Jacob Hilton, Reiichiro Nakano, Christo-
pher Hesse, and John Schulman. 2021. Training
veriﬁers to solve math word problems. arXiv
preprint arXiv:2110.14168.

Alexis Conneau, Kartikay Khandelwal, Naman
Goyal, Vishrav Chaudhary, Guillaume Wen-
zek, Francisco Guzmán, Edouard Grave, Myle
Ott, Luke Zettlemoyer, and Veselin Stoyanov.
2020. Unsupervised cross-lingual representa-
In Proceedings of the
tion learning at scale.
58th Annual Meeting of the Association for
Computational Linguistics, pages 8440–8451,
Online. Association for Computational Linguis-
tics.

Nathan Cooper, Artashes Arutiunian, Santiago
Hincapié-Potes, Ben Trevett, Arun Raja, Erfan
Al-Hossami, and Mrinal Mathur. 2021. Code
Clippy Data: A large dataset of code data from
Github for research into code language models.

Chris Cummins, Pavlos Petoumenos, Zheng
Wang, and Hugh Leather. 2017a. Synthesizing
In 2017
benchmarks for predictive modeling.
IEEE/ACM International Symposium on Code
Generation and Optimization (CGO), pages
86–99. IEEE.

Chris Cummins, Pavlos Petoumenos, Zheng
Wang, and Hugh Leather. 2017b. Synthesizing
In 2017
benchmarks for predictive modeling.
IEEE/ACM International Symposium on Code
Generation and Optimization (CGO), pages
86–99, Austin, TX, USA. IEEE.

Samip Dahal, Adyasha Maharana, and Mohit
Bansal. 2021. Analysis of tree-structured ar-
chitectures for code generation. In Findings of
the Association for Computational Linguistics:
ACL-IJCNLP 2021, pages 4382–4391, Online.
Association for Computational Linguistics.

Simon D’Alfonso, Olga Santesteban-Echarri, Si-
mon Rice, Greg Wadley, Reeva Lederman,
Christopher Miles, John Gleeson, and Mario
Alvarez-Jimenez. 2017. Artiﬁcial intelligence-
assisted online social therapy for youth mental
health. Frontiers in psychology, 8:796.

Jacob Devlin, Ming-Wei Chang, Kenton Lee,
Bert: Pre-
transformers
arXiv preprint

and Kristina Toutanova. 2018.
training of deep bidirectional
for language understanding.
arXiv:1810.04805.

Jacob Devlin, Jonathan Uesato, Surya Bhupati-
raju, Rishabh Singh, Abdel-rahman Mohamed,
and Pushmeet Kohli. 2017. Robustﬁll: Neural
In Interna-
program learning under noisy i/o.
tional conference on machine learning, pages
990–998. PMLR.

N. Diana, M. Eagle, and J. Stamper. 2018. Mea-
suring Transfer of Data-Driven Code Features
In Proceedings of
Across Tasks in Alice.
SPLICE 2018 workshop Computing Science
Education Infrastructure, page 5.

Li Dong and Mirella Lapata. 2016. Language to
logical form with neural attention. In Proceed-
ings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1:
Long Papers), pages 33–43, Berlin, Germany.
Association for Computational Linguistics.

Li Dong and Mirella Lapata. 2018. Coarse-to-ﬁne
decoding for neural semantic parsing. arXiv
preprint arXiv:1805.04793.

Mohsen Dorodchi, Mohammadali Fallahian, Er-
fan Al-Hossami, Aileen Benedict, and Alexan-
dria Benedict. 2020. Custom" Caring IDE" for
Online Offering of CS1.

Ian Drosos, Titus Barik, Philip J. Guo, Rob De-
Line, and Sumit Gulwani. 2020. Wrex: A Uni-
ﬁed Programming-by-Example Interaction for
Synthesizing Readable Code for Data Scien-
tists. In ACM CHI Conference on Human Fac-
tors in Computing Systems.

D. Dahl, M. Bates, M. Brown, W. Fisher,
K. Hunicke-Smith, D. Pallett, Christine Pao,
Alexander I. Rudnicky, and E. Shriberg. 1994.
Expanding the scope of the atis task: The atis-3
corpus. In HLT.

Kevin Ellis, Catherine Wong, Maxwell Nye,
Mathias Sablé-Meyer, Lucas Morales, Luke
Hewitt, Luc Cary, Armando Solar-Lezama, and
Joshua B. Tenenbaum. 2021. Dreamcoder:
Bootstrapping inductive program synthesis with

wake-sleep library learning. In Proceedings of
the 42nd ACM SIGPLAN International Confer-
ence on Programming Language Design and
Implementation, PLDI 2021, page 835–850,
New York, NY, USA. Association for Comput-
ing Machinery.

Yaw Frempong, Yates Snyder, Erfan Al-Hossami,
Meera Sridhar, and Samira Shaikh. 2021. Hi-
jax: Human intent javascript xss generator.
In Proceedings of the 18th International Con-
ference on Security and Cryptography - SE-
CRYPT,, pages 798–805. INSTICC, SciTePress.

Ahmed Elnaggar, Wei Ding, Llion Jones, Tom
Gibbs, Tamas Feher, Christoph Angerer, Silvia
Severini, Florian Matthes, and Burkhard Rost.
2021. Codetrans: Towards cracking the lan-
guage of silicone’s code through self-supervised
deep learning and high performance computing.

Evelina Fedorenko, Anna Ivanova, Riva Dhamala,
and Marina Umaschi Bers. 2019. The lan-
guage of programming: a cognitive perspective.
Trends in cognitive sciences, 23(7):525–528.

Li Fei-Fei, Rob Fergus, and Pietro Perona. 2006.
IEEE
One-shot learning of object categories.
transactions on pattern analysis and machine
intelligence, 28(4):594–611.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan
Duan, Xiaocheng Feng, Ming Gong, Lin-
jun Shou, Bing Qin, Ting Liu, Daxin Jiang,
and Ming Zhou. 2020. CodeBERT: A Pre-
Trained Model for Programming and Natural
arXiv:2002.08155 [cs]. ArXiv:
Languages.
2002.08155.

Daniel Fernández-González and Carlos Gómez-
Rodríguez. 2020.
Transition-based seman-
tic dependency parsing with pointer networks.
arXiv preprint arXiv:2005.13344.

Gabriela Ferraro and Hanna Suominen. 2020.
In Proceed-
Transformer semantic parsing.
ings of
the The 18th Annual Workshop of
the Australasian Language Technology Associ-
ation, pages 121–126.

Catherine Finegan-Dollak, Jonathan K. Kummer-
feld, Li Zhang, Karthik Ramanathan, Sesh
Sadasivam, Rui Zhang, and Dragomir Radev.
Improving text-to-SQL evaluation
2018.
In Proceedings of the 56th An-
methodology.
nual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 351–360, Melbourne, Australia. Associa-
tion for Computational Linguistics.

Hiroyuki Fudaba, Yusuke Oda, Koichi Akabe,
Graham Neubig, Hideaki Hata, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2015.
Pseudogen: A tool to automatically generate
In 2015 30th
pseudo-code from source code.
IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE), pages 824–
829. IEEE.

Debasis Ganguly, Gareth JF Jones, Aarón
Ramírez-De-La-Cruz, Gabriela Ramírez-De-
La-Rosa, and Esaú Villatoro-Tello. 2018. Re-
trieving and classifying instances of source
Information Retrieval Jour-
code plagiarism.
nal, 21(1):1–23.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François
Laviolette, Mario Marchand, and Victor Lem-
pitsky. 2016. Domain-adversarial training of
The Journal of Machine
neural networks.
Learning Research, 17(1):2096–2030.

Leo Gao, Stella Biderman, Sid Black, Laurence
Golding, Travis Hoppe, Charles Foster, Ja-
son Phang, Horace He, Anish Thite, Noa
Nabeshima, Shawn Presser, and Connor Leahy.
2020a. The pile: An 800gb dataset of diverse
text for language modeling.

Zhipeng Gao, Xin Xia, John Grundy, David Lo,
and Yuan-Fang Li. 2020b. Generating question
titles for stack overﬂow from mined code snip-
pets. ACM Trans. Softw. Eng. Methodol., 29(4).

Carlos Gemmell, Federico Rossetto, and Jeffrey
Dalton. 2020. Relevance Transformer: Gen-
erating Concise Code Snippets with Relevance
arXiv:2007.02609 [cs]. ArXiv:
Feedback.
2007.02609.

Alessandra Giordani and Alessandro Moschitti.
2012. Automatic generation and reranking of
In Pro-
sql-derived answers to nl questions.
ceedings of the Second International Confer-
ence on Trustworthy Eternal Systems via Evolv-
ing Software, Data and Knowledge, pages 59–
76.

David Gros, Hariharan Sezhiyan, Prem Devanbu,
and Zhou Yu. 2020. Code to comment “trans-
lation”: Data, metrics, baselining & evalua-
In 2020 35th IEEE/ACM International
tion.
Conference on Automated Software Engineer-
ing (ASE), pages 746–757. IEEE.

Jiatao Gu, Zhengdong Lu, Hang Li, and Vic-
Incorporating Copying
tor O. K. Li. 2016a.
Mechanism in Sequence-to-Sequence Learning.
arXiv:1603.06393 [cs]. ArXiv: 1603.06393.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016b. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings
of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), pages 1631–1640.

Xiaodong Gu, Hongyu Zhang, and Sunghun Kim.
2018. Deep code search. In Proceedings of the
40th International Conference on Software En-
gineering, ICSE ’18, page 933–944, New York,
NY, USA. Association for Computing Machin-
ery.

Sumit Gulwani. 2011. Automating string process-
ing in spreadsheets using input-output exam-
ples. ACM Sigplan Notices, 46(1):317–330.

Sumit Gulwani, William R Harris, and Rishabh
Singh. 2012. Spreadsheet data manipulation us-
ing examples. Communications of the ACM,
55(8):97–105.

Sumit Gulwani and Mark Marron. 2014. NLyze:
Interactive Programming by Natural Language
for SpreadSheet Data Analysis and Manipula-
tion. In SIGMOD’14, June 22-27, 2014, Snow-
bird, UT, USA. Edition: SIGMOD’14, June
22–27, 2014, Snowbird, UT, USA.

Sumit Gulwani, Oleksandr Polozov, Rishabh
Singh, et al. 2017. Program synthesis. Founda-
tions and Trends® in Programming Languages,
4(1-2):1–119.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie Liu, L. Zhou, N. Duan,
Jian Yin, Daxin Jiang, and M. Zhou. 2020.
Graphcodebert: Pre-training code representa-
tions with data ﬂow. ArXiv, abs/2009.08366.

Rahul Gupta, Soham Pal, Aditya Kanade, and
Shirish Shevade. 2017. Deepﬁx: Fixing com-
mon c language errors by deep learning. In Pro-
ceedings of the aaai conference on artiﬁcial in-
telligence, volume 31.

Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj
Kumar, and Mike Lewis. 2018. Semantic pars-
ing for task oriented dialog using hierarchical
In Proceedings of the 2018
representations.
Conference on Empirical Methods in Natural
Language Processing, pages 2787–2792, Brus-
sels, Belgium. Association for Computational
Linguistics.

Carolin Haas and Stefan Riezler. 2016. A cor-
pus and semantic parser for multilingual natural
language querying of OpenStreetMap. In Pro-
ceedings of the 2016 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 740–750, San Diego, Califor-
nia. Association for Computational Linguistics.

Hossein Hajipour, Apratim Bhattacharya, and
Mario Fritz. 2019. Sampleﬁx: Learning to cor-
rect programs by sampling diverse ﬁxes. arXiv
preprint arXiv:1906.10502.

Masum Hasan, Kazi Sajeed Mehrab, Wasi Ud-
din Ahmad,
and Rifat Shahriyar. 2021a.
Text2app: A framework for creating android
arXiv preprint
apps from text descriptions.
arXiv:2104.08301.

Masum Hasan, Tanveer Muttaqueen, Abdullah Al
Ishtiaq, Kazi Sajeed Mehrab, Md Haque,
Mahim Anjum, Tahmid Hasan, Wasi Uddin
Ahmad, Anindya Iqbal, and Rifat Shahriyar.
2021b. Codesc: A large code-description par-
allel dataset. arXiv preprint arXiv:2105.14220.

Hideaki Hata, Emad Shihab, and Graham Neu-
big. 2018.
Learning to generate corrective
patches using neural machine translation. arXiv
preprint arXiv:1812.07170.

Moshe Hazoom, Vibhor Malik, and Ben Bogin.
2021. Text-to-sql in the wild: A naturally-
occurring dataset based on stack exchange data.
arXiv preprint arXiv:2106.05006.

Kaiming He, Xiangyu Zhang, Shaoqing Ren,
and Jian Sun. 2015. Delving deep into rec-
tiﬁers: Surpassing human-level performance

on imagenet classiﬁcation.
arXiv:1502.01852.

arXiv preprint

Gary G Hendrix, Earl D Sacerdoti, Daniel Saga-
lowicz, and Jonathan Slocum. 1978. Devel-
oping a natural language interface to complex
data. ACM Transactions on Database Systems
(TODS), 3(2):105–147.

Dan Hendrycks, Steven Basart, Saurav Kada-
vath, Mantas Mazeika, Akul Arora, Ethan Guo,
Collin Burns, Samir Puranik, Horace He, Dawn
Song, and Jacob Steinhardt. 2021. Measuring
coding challenge competence with apps.

Dan Hendrycks and Kevin Gimpel. 2016. Gaus-
sian error linear units (gelus). arXiv preprint
arXiv:1606.08415.

Geert Heyman, Rafael Huysegems, Pascal Justen,
and Tom Van Cutsem. 2021. Natural language-
In Proceedings of the
guided programming.
2021 ACM SIGPLAN International Symposium
on New Ideas, New Paradigms, and Reﬂections
on Programming and Software, pages 39–55.

Sebastian Hobert. 2019. Say Hello to ‘Coding Tu-
tor’! Design and Evaluation of a Chatbot-based
Learning System Supporting Students to Learn
to Program. ICIS 2019 Proceedings.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780. Publisher: MIT Press.

Ehsan Hosseini-Asl, Bryan McCann, Chien-
and Richard
A simple language model
arXiv preprint

Sheng Wu, Semih Yavuz,
Socher. 2020.
for task-oriented dialogue.
arXiv:2005.00796.

Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin.
2018. Deep code comment generation. In 2018
IEEE/ACM 26th International Conference on
Program Comprehension (ICPC), pages 200–
20010. IEEE.

Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin.
2020. Deep code comment generation with hy-
brid lexical and syntactical information. Empir-
ical Software Engineering, 25(3):2179–2217.

Junjie Huang, Duyu Tang, Linjun Shou, Ming
Gong, Ke Xu, Daxin Jiang, Ming Zhou, and
Nan Duan. 2021. CoSQA: 20,000+ web queries

for code search and question answering. In Pro-
ceedings of the 59th Annual Meeting of the As-
sociation for Computational Linguistics and the
11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Pa-
pers), pages 5690–5700, Online. Association
for Computational Linguistics.

C. D. Hundhausen, D. M. Olivares, and A. S.
Carter. 2017.
IDE-Based Learning Ana-
lytics for Computing Education: A Process
Model, Critical Review, and Research Agenda.
ACM Transactions on Computing Education,
17(3):1–26.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit,
Miltiadis Allamanis, and Marc Brockschmidt.
2019.
CodeSearchNet Challenge: Evalu-
ating the State of Semantic Code Search.
arXiv:1909.09436 [cs, stat].

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
Jayant Krishnamurthy, and Luke Zettlemoyer.
2017. Learning a neural semantic parser from
user feedback. In Proceedings of the 55th An-
nual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 963–973.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
and Luke Zettlemoyer. 2016a. Summarizing
source code using a neural attention model. In
Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2073–2083.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
and Luke Zettlemoyer. 2016b. Summarizing
Source Code using a Neural Attention Model.
In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 2073–
2083, Berlin, Germany. Association for Com-
putational Linguistics.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
and Luke Zettlemoyer. 2018. Mapping lan-
guage to code in programmatic context. In Pro-
ceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing,
pages 1643–1652.

Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang.
2017. Search-based neural structured learning

for sequential question answering. In Proceed-
ings of the 55th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1:
Long Papers), pages 1821–1831.

Robin Jia and Percy Liang. 2016. Data Recom-
bination for Neural Semantic Parsing. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Vol-
ume 1: Long Papers), pages 12–22, Berlin, Ger-
many. Association for Computational Linguis-
tics.

Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021.
CURE: Code-Aware Neural Machine Trans-
lation for Automatic Program Repair.
In
2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE), pages 1161–
1173. ISSN: 1558-1225.

Melvin Johnson, Mike Schuster, Quoc V Le,
Maxim Krikun, Yonghui Wu, Zhifeng Chen,
Nikhil Thorat, Fernanda Viégas, Martin Wat-
tenberg, Greg Corrado, et al. 2017. Google’s
multilingual neural machine translation system:
Enabling zero-shot translation. Transactions of
the Association for Computational Linguistics,
5:339–351.

Tim Johnson. 1984. The commercial application
of expert systems technology. The Knowledge
Engineering Review, 1(1):15–25.

Dan Jurafsky and James H. Martin. 2000. Speech
and language processing - an introduction to
natural language processing, computational lin-
In Prentice
guistics, and speech recognition.
Hall series in artiﬁcial intelligence.

Endri Kacupaj,

Joan Plepi, Kuldeep Singh,
Harsh Thakkar, Jens Lehmann, and Maria
Maleshkova. 2021. Conversational question
answering over knowledge graphs with trans-
In Pro-
former and graph attention networks.
ceedings of the 16th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics: Main Volume, pages 850–
862, Online. Association for Computational
Linguistics.

Aishwarya Kamath and Rajarshi Das. 2019. A
Survey on Semantic Parsing. arXiv:1812.00978
[cs].

Aditya Kanade, Petros Maniatis, Gogul Balakrish-
nan, and Kensen Shi. 2020. Learning and eval-
uating contextual embedding of source code.
In International Conference on Machine Learn-
ing, pages 5110–5121. PMLR.

Anjan Karmakar, Julian Aron Prenner, Miltiadis
Allamanis, and Romain Robbes. 2020. Glue-
code: A benchmark for source code machine
learning models.

Diederik P Kingma and Jimmy Ba. 2014a. Adam:
A method for stochastic optimization. arXiv
preprint arXiv:1412.6980.

Diederik P Kingma and Jimmy Ba. 2014b. Adam:
A method for stochastic optimization. arXiv
preprint arXiv:1412.6980.

Gregory Kuhlmann, Peter Stone, Raymond
Mooney, and Jude Shavlik. 2004. Guiding a
reinforcement learner with natural language ad-
In The
vice: Initial results in robocup soccer.
AAAI-2004 workshop on supervisory control of
learning and adaptive systems. San Jose, CA.

Sumith Kulal, Panupong Pasupat, Kartik Chan-
dra, Mina Lee, Oded Padon, Alex Aiken, and
Percy S Liang. 2019. Spoc: Search-based pseu-
docode to code. Advances in Neural Informa-
tion Processing Systems, 32:11906–11917.

Nate Kushman and Regina Barzilay. 2013. Us-
ing semantic uniﬁcation to generate regular ex-
In Proceed-
pressions from natural language.
ings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, pages 826–836, Atlanta, Georgia. Associ-
ation for Computational Linguistics.

Uday Kusupati and Venkata Ravi Teja Ailavarapu.
Natural Language to Code Using Transformers.
page 7.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon
Goldwater, and Mark Steedman. 2011. Lexi-
cal generalization in ccg grammar induction for
In Proceedings of the 2011
semantic parsing.
Conference on Empirical Methods in Natural
Language Processing, pages 1512–1523.

Marie-Anne Lachaux, Baptiste Roziere, Lowik
Chanussot, and Guillaume Lample. 2020. Un-
supervised translation of programming lan-
guages. arXiv preprint arXiv:2006.03511.

J. Lacomis, P. Yin, E. Schwartz, M. Allamanis,
C. Le Goues, G. Neubig, and B. Vasilescu.
2019. Dire: A neural approach to decompiled
identiﬁer naming. In 2019 34th IEEE/ACM In-
ternational Conference on Automated Software
Engineering (ASE), pages 628–639.

Carolin Lawrence and S. Riezler. 2016. Nlmaps:
A natural language interface to query open-
streetmap. In COLING.

Carolin Lawrence and Stefan Riezler. 2018. Im-
proving a neural semantic parser by counterfac-
tual learning from human bandit feedback.

Alexander LeClair, Siyuan Jiang, and Collin
McMillan. 2019. A neural model for gener-
ating natural language summaries of program
In 2019 IEEE/ACM 41st Inter-
subroutines.
national Conference on Software Engineering
(ICSE), pages 795–806. IEEE.

Celine Lee, Justin Gottschlich, and Dan Roth.
2021. Toward code generation: A survey and
lessons from semantic parsing. arXiv preprint
arXiv:2105.03317.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence
pre-training for natural
language generation,
In Proceed-
translation, and comprehension.
ings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, pages
7871–7880, Online. Association for Computa-
tional Linguistics.

Fei Li and H. V. Jagadish. 2014. Constructing an
interactive natural language interface for rela-
tional databases. volume 8, pages 73–84.

Hongyu Li, Seohyun Kim, and Satish Chandra.
2019. Neural code search evaluation dataset.
arXiv preprint arXiv:1908.09804.

Yujia Li, David Choi, Junyoung Chung, Nate
Kushman, Julian Schrittwieser, Rémi Leblond,
Tom Eccles, James Keeling, Felix Gimeno,
Agustin Dal Lago, Thomas Hubert, Pe-
ter Choy, Cyprien de Masson d’Autume,
Po-Sen
Igor Babuschkin, Xinyun Chen,
Huang, Johannes Welbl, Sven Gowal, Alexey
Cherepanov, James Molloy, Daniel Mankowitz,

Esme Sutherland Robson, Pushmeet Kohli,
Nando de Freitas, Koray Kavukcuoglu, and
Oriol Vinyals. 2022. Competition-level code
generation with alphacode.

Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou,
Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi
Zhong. 2018. Vuldeepecker: A deep learning-
based system for vulnerability detection. arXiv
preprint arXiv:1801.01681.

Qingyuan Liang, Zeyu Sun, Qihao Zhu, Wen-
jie Zhang, Lian Yu, Yingfei Xiong, and
Lu Zhang. 2021.
Lyra: A benchmark for
turducken-style code generation. arXiv preprint
arXiv:2108.12144.

Pietro Liguori, Erfan Al-Hossami, Domenico
Cotroneo, Roberto Natella, Bojan Cukic, and
Samira Shaikh. 2021a.
Shellcode_IA32: A
dataset for automatic shellcode generation. In
Proceedings of
the 1st Workshop on Nat-
ural Language Processing for Programming
(NLP4Prog 2021), pages 58–64, Online. Asso-
ciation for Computational Linguistics.

Pietro Liguori, Erfan Al-Hossami, Domenico
Cotroneo, Roberto Natella, Bojan Cukic, and
Samira Shaikh. 2022. Can we generate shell-
codes via natural language? an empirical study.
arXiv preprint arXiv:2202.03755.

Pietro Liguori, Erfan Al-Hossami, Vittorio
Orbinato, Roberto Natella, Samira Shaikh,
Domenico Cotroneo, and Bojan Cukic. 2021b.
Evil: Exploiting software via natural language.
arXiv preprint arXiv:2109.00279.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summa-
rization branches out, pages 74–81.

Xi Victoria Lin, Chenglong Wang, Luke Zettle-
moyer, and Michael D Ernst. 2018. Nl2bash: A
corpus and semantic parser for natural language
interface to the linux operating system. arXiv
preprint arXiv:1802.08979.

Wang Ling, Edward Grefenstette, Karl Moritz
Hermann, Tomás Kociský, Andrew W. Senior,
Fumin Wang, and Phil Blunsom. 2016. Latent
predictor networks for code generation. CoRR,
abs/1603.06744.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin
Stoyanov. 2019. Roberta: A robustly opti-
mized bert pretraining approach. arXiv preprint
arXiv:1907.11692.

Nicholas Locascio, Karthik Narasimhan, Eduardo
DeLeon, Nate Kushman, and Regina Barzilay.
2016. Neural generation of regular expressions
from natural language with minimal domain
In Proceedings of the 2016 Con-
knowledge.
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1918–1923, Austin,
Texas. Association for Computational Linguis-
tics.

Reginald Long, Panupong Pasupat, and Percy
Liang. 2016. Simpler context-dependent logi-
In Proceed-
cal forms via model projections.
ings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1:
Long Papers), pages 1456–1465.

C. Lopes, S. Bajracharya, J. Ossher, and P. Baldi.

2010. UCI source code data sets.

Cristina V. Lopes, Petr Maj, Pedro Martins, Vaib-
hav Saini, Di Yang, Jakub Zitny, Hitesh Saj-
nani, and Jan Vitek. 2017. Déjàvu: A map
of code duplicates on github. Proc. ACM Pro-
gram. Lang., 1(OOPSLA).

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang,
Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu
Tang, Ge Li, Lidong Zhou, Linjun Shou, Long
Zhou, Michele Tufano, Ming Gong, Ming
Zhou, Nan Duan, Neel Sundaresan, Shao Kun
Deng, Shengyu Fu, and Shujie Liu. 2021a.
Codexglue: A machine learning benchmark
dataset for code understanding and generation.

to attention-based neural machine translation.
arXiv preprint arXiv:1508.04025.

Thibaud Lutellier, Hung Viet Pham, Lawrence
Pang, Yitong Li, Moshi Wei, and Lin Tan.
2020.
CoCoNuT: combining context-aware
neural translation models using ensemble for
In Proceedings of the 29th
program repair.
ACM SIGSOFT International Symposium on
Software Testing and Analysis, ISSTA 2020,
pages 101–114, New York, NY, USA. Associ-
ation for Computing Machinery.

Ravi Mangal, Xin Zhang, Aditya V Nori, and
Mayur Naik. 2015. A user-guided approach to
program analysis. In Proceedings of the 2015
10th Joint Meeting on Foundations of Software
Engineering, pages 462–473.

Zohar Manna and Richard Waldinger. 1975.
Knowledge and reasoning in program synthesis.
Artiﬁcial intelligence, 6(2):175–208.

Zohar Manna and Richard Waldinger. 1980. A de-
ductive approach to program synthesis. ACM
Transactions on Programming Languages and
Systems (TOPLAS), 2(1):90–121.

Zohar Manna and Richard J Waldinger. 1971. To-
ward automatic program synthesis. Communi-
cations of the ACM, 14(3):151–165.

Antonio Mastropaolo, Simone Scalabrino, Nathan
Cooper, David Nader Palacio, Denys Poshy-
vanyk, Rocco Oliveto, and Gabriele Bavota.
2021. Studying the usage of text-to-text trans-
fer transformer to support code-related tasks. In
2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE), pages 336–
347. IEEE.

Yunlong Lu, Na Meng, and Wenxin Li. 2021b.
Fapr: Fast and accurate program repair for
arXiv
introductory programming courses.
preprint arXiv:2107.06550.

Cynthia Matuszek, E. Herbst, Luke Zettlemoyer,
and D. Fox. 2012. Learning to parse natural lan-
guage commands to a robot control system. In
ISER.

Minh-Thang Luong, Hieu Pham, and Christo-
pher D Manning. 2015a. Effective approaches
to attention-based neural machine translation.
arXiv preprint arXiv:1508.04025.

Minh-Thang Luong, Hieu Pham, and Christo-
pher D Manning. 2015b. Effective approaches

Anders Miltner, Sumit Gulwani, Vu Le, Alan
Leung, Arjun Radhakrishna, Gustavo Soares,
Ashish Tiwari, and Abhishek Udupa. 2019. On
the ﬂy synthesis of edit suggestions. In Object-
Oriented Programming, Systems, Languages &
Applications (OOPSLA). ACM.

Joao Luis Zeni Montenegro, Cristiano André da
Costa, and Rodrigo da Rosa Righi. 2019. Sur-
vey of conversational agents in health. Expert
Systems with Applications, 129:56–67.

Lili Mou, Ge Li, Lu Zhang, Tao Wang, and
Zhi Jin. 2016a. Convolutional Neural Net-
works over Tree Structures for Programming
Language Processing. In Thirtieth AAAI Con-
ference on Artiﬁcial Intelligence.

Lili Mou, Ge Li, Lu Zhang, Tao Wang, and
Convolutional neural net-
Zhi Jin. 2016b.
works over tree structures for programming lan-
guage processing. In Proceedings of the Thirti-
eth AAAI Conference on Artiﬁcial Intelligence,
pages 1287–1293.

Xuan-Phi Nguyen, Shaﬁq Joty, Steven Hoi, and
Richard Socher. 2020. Tree-structured atten-
tion with hierarchical accumulation. In Interna-
tional Conference on Learning Representations.

Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao
Xue, Vinay Adiga, and Erik Cambria. 2021.
Recent advances in deep learning based dia-
arXiv
logue systems: A systematic survey.
preprint arXiv:2105.04387.

Sajad Norouzi, Keyi Tang, and Yanshuai Cao.
2021. Code generation from natural language
with less prior knowledge and more monolin-
In Proceedings of the 59th Annual
gual data.
Meeting of the Association for Computational
Linguistics and the 11th International Joint
Conference on Natural Language Processing
(Volume 2: Short Papers), pages 776–785, On-
line. Association for Computational Linguis-
tics.

Maxwell Nye, Anders Johan Andreassen, Guy
Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz,
Maarten Bosma, David Luan, et al. 2021. Show
your work: Scratchpads for intermediate com-
putation with language models. arXiv preprint
arXiv:2112.00114.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015a. Learning to Gener-
ate Pseudo-Code from Source Code Using Sta-
tistical Machine Translation (T). In 2015 30th

IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE), pages 574–
584.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015b. Learning to gen-
erate pseudo-code from source code using sta-
In 2015 30th
tistical machine translation (t).
IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE), pages 574–
584. IEEE.

Augustus Odena and Charles Sutton. 2020. Learn-
ing to represent programs with property signa-
tures. arXiv preprint arXiv:2002.09030.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinková, Dan Flickinger,
Jan Hajic, and Zdenka Uresova. 2015. Semeval
2015 task 18: Broad-coverage semantic depen-
In Proceedings of the 9th In-
dency parsing.
ternational Workshop on Semantic Evaluation
(SemEval 2015), pages 915–926.

Hakjoo Oh, Hongseok Yang, and Kwangkeun Yi.
2015. Learning a strategy for adapting a pro-
gram analysis via bayesian optimisation. ACM
SIGPLAN Notices, 50(10):572–588.

Gabriel Orlanski and Alex Gittens. 2021. Read-
ing stackoverﬂow encourages cheating: Adding
question text improves extractive code genera-
tion. arXiv preprint arXiv:2106.04447.

Elahe Paikari, JaeEun Choi, SeonKyu Kim, Sooy-
oung Baek, MyeongSoo Kim, SeungEon Lee,
ChaeYeon Han, YoungJae Kim, KaHye Ahn,
Chan Cheong, et al. 2019. A chatbot for conﬂict
In 2019 IEEE/ACM
detection and resolution.
1st International Workshop on Bots in Software
Engineering (BotSE), pages 29–33. IEEE.

Sinno Jialin Pan and Qiang Yang. 2010. A
IEEE Trans-
Survey on Transfer Learning.
actions on Knowledge and Data Engineering,
22(10):1345–1359.

Sheena Panthaplackel, Milos Gligoric, Ray-
mond J. Mooney, and Junyi Jessy Li. 2020. As-
sociating natural language comment and source
code entities. In AAAI, pages 8592–8599.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for au-
In
tomatic evaluation of machine translation.

Proceedings of the 40th annual meeting on as-
sociation for computational linguistics, pages
311–318. Association for Computational Lin-
guistics.

Singh,

Emilio Parisotto, Abdel-rahman Mohamed,
Rishabh
Lihong Li, Dengyong
Zhou, and Pushmeet Kohli. 2016. Neuro-
arXiv preprint
symbolic program synthesis.
arXiv:1611.01855.

Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat
Chakraborty, Baishakhi Ray, and Kai-Wei
Chang. 2021. Retrieval augmented code gen-
arXiv preprint
eration and summarization.
arXiv:2108.11601.

Panupong Pasupat and Percy Liang. 2015. Com-
positional semantic parsing on semi-structured
tables. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Lin-
guistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume
1: Long Papers), pages 1470–1480.

Jibesh Patra and Michael Pradel. 2016. Learning
to fuzz: Application-independent fuzz testing
with probabilistic, generative models of input
data.

Doris L Payne. 1992. Pragmatics of word order
ﬂexibility, volume 22. John Benjamins Publish-
ing.

Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen,
James Anibal, Alec Peltekian, and Yanfang
Cotext: Multi-task learning
Ye. 2021.
arXiv preprint
with code-text
transformer.
arXiv:2105.08645.

Paul R. Pintrich. 2000. Chapter 14 - the role
of goal orientation in self-regulated learning.
In Monique Boekaerts, Paul R. Pintrich, and
Moshe Zeidner, editors, Handbook of Self-
Regulation, pages 451–502. Academic Press,
San Diego.

Ana-Maria Popescu, Oren Etzioni, and Henry
Kautz. 2003. Towards a theory of natural lan-
In Proceedings
guage interfaces to databases.
of the 8th International Conference on Intelli-
gent User Interfaces, pages 149–157.

P. J. Price. 1990. Evaluation of spoken language
In Speech and

the ATIS domain.

systems:

Natural Language: Proceedings of a Workshop
Held at Hidden Valley, Pennsylvania, June 24-
27,1990.

Ruchir Puri, David S Kung, Geert Janssen, Wei
Zhang, Giacomo Domeniconi, Vladmir Zolo-
tov, Julian Dolby, Jie Chen, Mihir Choudhury,
Lindsey Decker, et al. 2021. Project codenet:
A large-scale ai for code dataset for learning
arXiv preprint
a diversity of coding tasks.
arXiv:2105.12655.

Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu,
Bolun Yao, Bartuer Zhou, Biao Cheng, Daxin
Jiang, Jiusheng Chen, Ruofei Zhang, et al.
2021. Prophetnet-x: Large-scale pre-training
models for english, chinese, multi-lingual, di-
arXiv preprint
alog, and code generation.
arXiv:2104.08006.

Chris Quirk, Raymond Mooney, and Michel Gal-
ley. 2015. Language to Code: Learning Seman-
tic Parsers for If-This-Then-That Recipes.
In
Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and
the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Pa-
pers), pages 878–888, Beijing, China. Associa-
tion for Computational Linguistics.

Maxim Rabinovich, Mitchell Stern, and Dan
Abstract Syntax Networks
Klein. 2017.
for Code Generation and Semantic Pars-
arXiv:1704.07535 [cs, stat]. ArXiv:
ing.
1704.07535.

Alec Radford, Karthik Narasimhan, Tim Sali-
mans, and Ilya Sutskever. a.
Improving lan-
guage understanding by generative pre-training.

Alec Radford, Jeffrey Wu, Rewon Child, David
Luan, Dario Amodei, and Ilya Sutskever. b.
Language models are unsupervised multitask
learners.

Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu.
2019a. Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer. arXiv
preprint arXiv:1910.10683.

Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael

Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
2019b. Exploring the Limits of Transfer Learn-
ing with a Uniﬁed Text-to-Text Transformer.
arXiv:1910.10683 [cs, stat].

Frank Rosenblatt. 1961. Principles of neurody-
namics. perceptrons and the theory of brain
mechanisms. Technical report, Cornell Aero-
nautical Lab Inc Buffalo NY.

Pranav Rajpurkar, Robin Jia, and Percy Liang.
2018. Know what you don’t know: Unan-
In Proceedings
swerable questions for squad.
of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short
Papers), pages 784–789.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-
rev, and Percy Liang. 2016. Squad: 100,000+
questions for machine comprehension of text.
In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 2383–2392.

Ashwin Ram, Rohit Prasad, Chandra Khatri,
Anu Venkatesh, Raefer Gabriel, Qing Liu,
Jeff Nunn, Behnam Hedayatnia, Ming Cheng,
Ashish Nagar, and others. 2018. Conversational
ai: The science behind the alexa prize. arXiv
preprint arXiv:1801.03604.

Veselin Raychev, Martin Vechev, and Andreas
Krause. 2015. Predicting program properties
ACM SIGPLAN Notices,
from" big code".
50(1):111–124.

Brittany Reid, Christoph Treude, and Markus
Wagner. 2020. Optimising the ﬁt of stack
overﬂow code snippets into existing code.
In
Proceedings of the 2020 Genetic and Evolu-
tionary Computation Conference Companion,
pages 1945–1953.

Shuo Ren, Daya Guo, Shuai Lu, Long Zhou,
Shujie Liu, Duyu Tang, Ming Zhou, Ambro-
sio Blanco, and Shuai Ma. 2020. Codebleu: a
method for automatic evaluation of code syn-
thesis. arXiv preprint arXiv:2009.10297.

K. Rivers and K. R. Koedinger. 2017. Data-driven
hint generation in vast solution spaces: a self-
improving python programming tutor. Interna-
tional Journal of Artiﬁcial Intelligence in Edu-
cation, 27(1):37–64.

David E Rumelhart, Geoffrey E Hinton, and
Ronald J Williams. 1986. Learning represen-
nature,
tations by back-propagating errors.
323(6088):533–536. Publisher: Nature Pub-
lishing Group.

Sashank Santhanam and Samira Shaikh. 2019.
A survey of natural language generation tech-
niques with a focus on dialogue systems-past,
present and future directions. arXiv preprint
arXiv:1906.00500.

Dipanjan Sarkar, Raghav Bali, and Tamoghna
Ghosh. 2019. Hands-on transfer learning with
Python: implement advanced deep learning and
neural network models using TensorFlow and
Keras. OCLC: 1148204369.

Torsten Scholak, Nathan Schucher, and Dzmitry
Bahdanau. 2021. Picard - parsing incrementally
for constrained auto-regressive decoding from
In Proceedings of the 2021
language models.
Conference on Empirical Methods in Natural
Language Processing. Association for Compu-
tational Linguistics.

Tal Schuster, Ashwin Kalyan, Oleksandr Polozov,
and Adam Tauman Kalai. 2021. Programming
puzzles. arXiv preprint arXiv:2106.05784.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
In Proceed-
with pointer-generator networks.
ings of the 55th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1:
Long Papers), pages 1073–1083.

Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016. Neural machine translation of rare
In Proceedings of
words with subword units.
the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 1715–1725, Berlin, Germany. As-
sociation for Computational Linguistics.

Stephen E Robertson and Steve Walker. 1994.
Some simple effective approximations to the
2-poisson model for probabilistic weighted re-
trieval. In SIGIR’94, pages 232–241. Springer.

Iulian V Serban, Chinnadhurai Sankar, Math-
ieu Germain, Saizheng Zhang, Zhouhan Lin,
Sandeep Subramanian, Taesup Kim, Michael
Pieper, Sarath Chandar, Nan Rosemary Ke, and

others. 2017. A deep reinforcement learning
chatbot. arXiv preprint arXiv:1709.02349.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He,
Hua Wu, Maosong Sun, and Yang Liu. 2016.
Minimum Risk Training for Neural Machine
In Proceedings of the 54th An-
Translation.
nual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1683–1692.

Tao Shen, Xiubo Geng, Tao Qin, Daya Guo, Duyu
Tang, Nan Duan, Guodong Long, and Daxin
Jiang. 2019. Multi-task learning for conver-
sational question answering over a large-scale
In Proceedings of the 2019
knowledge base.
Conference on Empirical Methods in Natural
Language Processing and the 9th International
Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 2442–2451,
Hong Kong, China. Association for Computa-
tional Linguistics.

Kensen Shi, David Bieber,

and Rishabh
Tf-coder: Program synthesis
arXiv preprint

Singh. 2020.
for
arXiv:2003.09040.

tensor manipulations.

Jiho Shin and Jaechang Nam. 2021. A survey
of automatic code generation from natural lan-
guage. Journal of Information Processing Sys-
tems, 17(3):537–555.

Hrituraj Singh, Milan Aggrawal, and Balaji Krish-
namurthy. 2020. Exploring neural models for
parsing natural language into ﬁrst-order logic.
arXiv preprint arXiv:2002.06544.

Mark Steedman. 2000. The syntactic process, vol-

ume 24. MIT press Cambridge, MA.

Ruiyong Sun, Yijia Zhao, Qi Zhang, Keyu Ding,
Shijin Wang, and Cui Wei. 2019a. A neural se-
mantic parser for math problems incorporating
multi-sentence information. ACM Trans. Asian
Low-Resour. Lang. Inf. Process., 18(4).

Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun,
Lili Mou, and Lu Zhang. 2019b. TreeGen: A
Tree-Based Transformer Architecture for Code
Generation. arXiv:1911.09983 [cs]. ArXiv:
1911.09983.

Ilya Sutskever, Oriol Vinyals, and Quoc V
Sequence to Sequence Learning

Le. 2014.

with Neural Networks.
In Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and
K. Q. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 27, pages
3104–3112. Curran Associates, Inc.

Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo,
Chanchal K Roy, and Mohammad Mamun Mia.
2014. Towards a big data curated benchmark of
inter-project code clones. In 2014 IEEE Inter-
national Conference on Software Maintenance
and Evolution, pages 476–480. IEEE.

Alexey Svyatkovskiy, Shao Kun Deng, Shengyu
Fu, and Neel Sundaresan. 2020.
Intellicode
compose: Code generation using transformer.
In Proceedings of the 28th ACM Joint Meeting
on European Software Engineering Conference
and Symposium on the Foundations of Software
Engineering, pages 1433–1443.

Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu,
and Neel Sundaresan. 2019. Pythia: Ai-assisted
code completion system. In Proceedings of the
25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pages
2727–2735.

Christian Szegedy. 2020. A promising path to-
wards autoformalization and general artiﬁcial
In Intelligent Computer Mathe-
intelligence.
matics, pages 3–20, Cham. Springer Interna-
tional Publishing.

Lappoon R. Tang and Raymond J. Mooney. 2000.
Automated construction of database interfaces:
Intergrating statistical and relational learning
In 2000 Joint SIGDAT
for semantic parsing.
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 133–141.

Lappoon R Tang and Raymond J Mooney. 2001.
Using multiple clause constructors in inductive
logic programming for semantic parsing. In Eu-
ropean Conference on Machine Learning, pages
466–477. Springer.

Michele Tufano, Cody Watson, Gabriele Bavota,
Massimiliano Di Penta, Martin White, and
Denys Poshyvanyk. 2019. An empirical study
on learning bug-ﬁxing patches in the wild via
ACM Transac-
neural machine translation.
tions on Software Engineering and Methodol-
ogy (TOSEM), 28(4):1–29.

Shyam Upadhyay, Manaal Faruqui, Gokhan Tür,
Hakkani-Tür Dilek, and Larry Heck. 2018. (al-
most) zero-shot cross-lingual spoken language
In 2018 IEEE International
understanding.
Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 6034–6038. IEEE.

Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
\Lukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need. In Advances in neural
information processing systems, pages 5998–
6008.

Olga Vechtomova. 2009.

Introduction to infor-
mation retrieval christopher d. manning, prab-
hakar raghavan, and hinrich schütze (stanford
university, yahoo!
research, and university
of stuttgart) cambridge: Cambridge university
press, 2008, xxi+ 482 pp; hardbound, isbn 978-
0-521-86571-5, $60.00.

Oriol Vinyals, Meire Fortunato, and Navdeep
In Advances
Jaitly. 2015. Pointer networks.
in Neural Information Processing Systems, vol-
ume 28. Curran Associates, Inc.

Richard J Waldinger and Richard CT Lee. 1969.
Prow: A step toward automatic program writ-
In Proceedings of the 1st international
ing.
joint conference on Artiﬁcial intelligence, pages
241–252.

Alex Wang, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman.
2018. Glue: A multi-task benchmark and anal-
ysis platform for natural language understand-
ing. In Proceedings of the 2018 EMNLP Work-
shop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP, pages 353–355.

Deze Wang, Yue Yu, Shanshan Li, Wei Dong,
Ji Wang, and Liao Qing. 2021a. Mulcode: A
multi-task learning approach for source code
In 2021 IEEE International
understanding.
Conference on Software Analysis, Evolution
and Reengineering (SANER), pages 48–59.
IEEE.

Qingxiang Wang, Chad Brown, Cezary Kaliszyk,
and Josef Urban. 2020a. Exploration of neu-
ral machine translation in autoformalization of
In Proceedings of the
mathematics in mizar.
9th ACM SIGPLAN International Conference

on Certiﬁed Programs and Proofs, CPP 2020,
page 85–98, New York, NY, USA. Association
for Computing Machinery.

Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and
Zhi Jin. 2020b. Detecting code clones with
graph neural network and ﬂow-augmented ab-
stract syntax tree. In 2020 IEEE 27th Interna-
tional Conference on Software Analysis, Evolu-
tion and Reengineering (SANER), pages 261–
271. IEEE.

Yue Wang, Weishi Wang, Shaﬁq Joty, and
Steven CH Hoi. 2021b. Codet5:
Identiﬁer-
aware uniﬁed pre-trained encoder-decoder
models for code understanding and generation.
arXiv preprint arXiv:2109.00859.

Yushi Wang, Jonathan Berant, and Percy Liang.
2015. Building a semantic parser overnight. In
Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and
the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Pa-
pers), pages 1332–1342.

C. Watson, F. W.B. Li, and J. L. Godwin. 2013.
Predicting Performance in an Introductory Pro-
gramming Course by Logging and Analyzing
Student Programming Behavior. In 2013 IEEE
13th International Conference on Advanced
Learning Technologies, pages 319–323. ISSN:
2161-377X.

Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin.
2019. Code generation as a dual task of code
summarization. In Proceedings of the 33rd In-
ternational Conference on Neural Information
Processing Systems, pages 6563–6573.

Huihui Wei and Ming Li. 2017. Supervised deep
features for software functional clone detection
by exploiting lexical and syntactical informa-
In IJCAI, pages 3034–
tion in source code.
3040.

Joseph Weizenbaum et al. 1966. Eliza—a com-
puter program for the study of natural lan-
guage communication between man and ma-
chine. Communications of the ACM, 9(1):36–
45.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis
Francisco

Conneau, Vishrav Chaudhary,

Guzmán, Armand Joulin, and Édouard Grave.
2020. Ccnet: Extracting high quality mono-
lingual datasets from web crawl data.
In
Proceedings of The 12th Language Resources
and Evaluation Conference, pages 4003–4012.

William A Woods. 1973. Progress in natural lan-
guage understanding: an application to lunar
geology. In Proceedings of the June 4-8, 1973,
national computer conference and exposition,
pages 441–450.

Chen Wu. 2020.

Semantic code search.
https://github.com/overwindows/
SemanticCodeSearch.

Frank F. Xu, Zhengbao Jiang, Pengcheng Yin,
Bogdan Vasilescu, and Graham Neubig. 2020a.
Incorporating external knowledge through pre-
training for natural language to code generation.
ArXiv, abs/2004.09015.

Frank F. Xu, Bogdan Vasilescu, and Graham
Neubig. 2021.
In-IDE Code Generation from
Natural Language: Promise and Challenges.
arXiv:2101.11149 [cs]. ArXiv: 2101.11149.

Weijia Xu, Batool Haider, and Saab Mansour.
2020b. End-to-end slot alignment and recog-
nition for cross-lingual nlu. In Proceedings of
the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages
5052–5063.

Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig,
and Thomas Dillig. 2017. Sqlizer: Query syn-
thesis from natural language. In International
Conference on Object-Oriented Programming,
Systems, Languages, and Applications, ACM,
pages 63:1–63:26.

Ziyu Yao, Daniel S Weld, Wei-Peng Chen, and
Huan Sun. 2018.
Staqc: A systematically
mined question-code dataset from stack over-
In Proceedings of the 2018 World Wide
ﬂow.
Web Conference, pages 1693–1703.

Michihiro Yasunaga and Percy Liang. 2020.
Graph-based, self-supervised program repair
In International
from diagnostic feedback.
Conference on Machine Learning (ICML).

Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xi-
aoyin Wang, and Shikun Zhang. 2020a. Lever-
aging code generation to improve code retrieval

In Pro-
and summarization via dual learning.
ceedings of The Web Conference 2020, WWW
’20, page 2309–2319, New York, NY, USA. As-
sociation for Computing Machinery.

Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Dur-
rett. 2020b. Optimal neural program synthesis
from multimodal speciﬁcations. arXiv preprint
arXiv:2010.01678.

Wen-tau Yih, Matthew Richardson, Chris Meek,
Ming-Wei Chang, and Jina Suh. 2016. The
value of semantic parse labeling for knowledge
In Proceedings of
base question answering.
the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short
Papers), pages 201–206, Berlin, Germany. As-
sociation for Computational Linguistics.

Pengcheng Yin. 2020. Towards Generalized Neu-

ral Semantic Parsing. page 121.

Pengcheng Yin, Bowen Deng, Edgar Chen, Bog-
dan Vasilescu, and Graham Neubig. 2018.
Learning to mine aligned code and natural lan-
In Interna-
guage pairs from stack overﬂow.
tional Conference on Mining Software Reposi-
tories, MSR, pages 476–486. ACM.

Pengcheng Yin and Graham Neubig. 2017a. A
syntactic neural model for general-purpose code
generation. arXiv preprint arXiv:1704.01696.

Pengcheng Yin and Graham Neubig. 2017b. A
Syntactic Neural Model for General-Purpose
arXiv:1704.01696 [cs].
Code Generation.
ArXiv: 1704.01696.

Pengcheng Yin and Graham Neubig. 2018.
TRANX: A Transition-based Neural Abstract
Syntax Parser for Semantic Parsing and Code
Generation. In EMNLP.

Pengcheng Yin and Graham Neubig. 2019.
Reranking for neural semantic parsing. In Pro-
ceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, pages
4553–4559.

Wei You, Peiyuan Zong, Kai Chen, XiaoFeng
Wang, Xiaojing Liao, Pan Bian, and Bin Liang.
2017. Semfuzz: Semantics-based automatic
generation of proof-of-concept exploits. In Pro-
ceedings of the 2017 ACM SIGSAC Confer-
ence on Computer and Communications Secu-
rity, pages 2139–2154.

Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric
Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan,
Tianze Shi, Zihan Li, et al. 2019a. Cosql:
A conversational text-to-sql challenge towards
language interfaces to
cross-domain natural
In Proceedings of the 2019 Con-
databases.
ference on Empirical Methods in Natural Lan-
guage Processing and the 9th International
Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 1962–1979.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Ya-
sunaga, Dongxu Wang, Zifan Li, James Ma,
Irene Li, Qingning Yao, Shanelle Roman, Zilin
Zhang, and Dragomir Radev. 2018. Spider: A
large-scale human-labeled dataset for complex
and cross-domain semantic parsing and text-to-
In Proceedings of the 2018 Con-
SQL task.
ference on Empirical Methods in Natural Lan-
guage Processing, pages 3911–3921, Brussels,
Belgium. Association for Computational Lin-
guistics.

Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern
Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene
Li, Bo Pang, Tao Chen, et al. 2019b. Sparc:
Cross-domain semantic parsing in context.
In
Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics,
pages 4511–4523.

Luke Zettlemoyer and M. Collins. 2005. Learn-
ing to map sentences to logical form: Struc-
tured classiﬁcation with probabilistic categorial
grammars. In UAI.

Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for pars-
ing to logical form. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 678–687.

Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun
Chen, Chris Brockett, Xiang Gao, Jianfeng
Gao, Jingjing Liu, and Bill Dolan. 2020. Di-
alogpt: Large-scale generative pre-training for
In ACL,
conversational response generation.
system demonstration.

Victor Zhong, Caiming Xiong, and Richard
Socher. 2017.
Seq2SQL: Generating Struc-
tured Queries from Natural Language using Re-
inforcement Learning. arXiv:1709.00103 [cs].
ArXiv: 1709.00103.

Zexuan Zhong, Jiaqi Guo, Wei Yang, Tao Xie,
Jian-Guang Lou, Ting Liu, and Dongmei
Zhang. 2018. Generating regular expressions
from natural language speciﬁcations: Are we
there yet? In AAAI Workshops.

J.K.K. Yuen, E.W.M. Lee, Stanley Lo, and RKK
Yuen. 2013. An intelligence-based optimiza-
tion model of passenger ﬂow in a transporta-
IEEE TRANSACTIONS ON IN-
tion station.
TELLIGENT TRANSPORTATION SYSTEMS,
14:1290–1300.

Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaon-
ing Du, and Yang Liu. 2019. Devign: Effective
vulnerability identiﬁcation by learning compre-
hensive program semantics via graph neural
In Advances in Neural Information
networks.
Processing Systems, pages 10197–10207.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. 2015. Aligning books
and movies: Towards story-like visual explana-
tions by watching movies and reading books. In
Proceedings of the IEEE international confer-
ence on computer vision, pages 19–27.

Maksym Zavershynskyi, Alex Skidanov, and
Naps: Natural
arXiv preprint

Illia Polosukhin. 2018.
program synthesis dataset.
arXiv:1807.03168.

John M Zelle and Raymond J Mooney. 1993.
Learning semantic grammars with constructive
In AAAI, pages
inductive logic programming.
817–822. Citeseer.

John M Zelle and Raymond J Mooney. 1996.
Learning to parse database queries using induc-
tive logic programming. In Proceedings of the
national conference on artiﬁcial intelligence,
pages 1050–1055.

