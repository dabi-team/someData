Differentiable Inductive Logic Programming for Structured Examples

Hikaru Shindo1, Masaaki Nishino2, Akihiro Yamamoto1
1 Kyoto University, Japan
2 NTT Communication Science Laboratories, Japan
shindo@iip.ist.i.kyoto-u.ac.jp, masaaki.nishino.uh@hco.ntt.co.jp, akihiro@i.kyoto-u.ac.jp

1
2
0
2

r
a

M
2

]
I

A
.
s
c
[

1
v
9
1
7
1
0
.
3
0
1
2
:
v
i
X
r
a

Abstract

The differentiable implementation of logic yields a seam-
less combination of symbolic reasoning and deep neural net-
works. Recent research, which has developed a differentiable
framework to learn logic programs from examples, can even
acquire reasonable solutions from noisy datasets. However,
this framework severely limits expressions for solutions, e.g.,
no function symbols are allowed, and the shapes of clauses
are ﬁxed. As a result, the framework cannot deal with struc-
tured examples. Therefore we propose a new framework to
learn logic programs from noisy and structured examples,
including the following contributions. First, we propose an
adaptive clause search method by looking through structured
space, which is deﬁned by the generality of the clauses, to
yield an efﬁcient search space for differentiable solvers. Sec-
ond, we propose for ground atoms an enumeration algorithm,
which determines a necessary and sufﬁcient set of ground
atoms to perform differentiable inference functions. Finally,
we propose a new method to compose logic programs softly,
enabling the system to deal with complex programs consist-
ing of several clauses. Our experiments show that our new
framework can learn logic programs from noisy and struc-
tured examples, such as sequences or trees. Our framework
can be scaled to deal with complex programs that consist of
several clauses with function symbols.

Introduction
Integrating symbolic reasoning and numerical computation
is increasingly becoming a vital factor in artiﬁcial intelli-
gence and its applications (De Raedt et al. 2020). Due to the
success of deep neural networks (DNNs), one of the main in-
tegrated techniques is to combine DNNs with logical reason-
ing, which is called neuro-symbolic computation (d’Avila
Garcez et al. 2019). The main goal is to establish a uni-
ﬁed framework that can make ﬂexible approximations using
DNNs and perform tractable and multi-hop reasoning using
ﬁrst-order logic.

Although many approaches have been developed for the
(Rockt¨aschel and Riedel
integration of logic and DNNs
2017; Yang, Yang, and Cohen 2017; ˇSourek et al. 2018;
Manhaeve et al. 2018; Si et al. 2019; Cohen, Yang, and
Mazaitis 2020; Riegel et al. 2020; Marra et al. 2020), most

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

existing approaches involve the learning of continuous pa-
rameters, not discrete structures. Structure learning (Kok
and Domingos 2005), in which logical expressions are ob-
tained explicitly, presents a challenge to neuro-symbolic ap-
proaches (De Raedt et al. 2020).

Evans and Grefenstette proposed (Evans and Grefenstette
2018) Differentiable Inductive Logic Programming (∂ILP),
which is a framework for learning logic programs from
given examples in a differentiable manner. Inductive Logic
Programming (ILP) (Muggleton 1991) is a sound formal-
ization for ﬁnding theories from given examples using ﬁrst-
order logic as its language (Nienhuys-Cheng et al. 1997).
The ∂ILP framework formulates ILP problems as numer-
ical optimization problems that can be solved by gradient
descent. Its differentiability establishes a seamless combina-
tion of ILP and neural networks to deal with subsymbolic
and noisy data.

However, previous work has put severe limitations on ex-
pressions for solutions. For instance, no function symbols
are allowed, the arity of predicates must be less than 2, the
number of atoms in the clause body must not exceed 2,
and every program must be comprised of pairs of rules for
each predicate. Thus it is unsuitable for complex structured
data, such as sequences or trees, or complex programs that
are comprised of several clauses for a predicate. One main
characteristic of ﬁrst-order logic is the expressibility and
learnability for structured data with function symbols (Lloyd
2003; Dantsin et al. 2001; Fredouille et al. 2007). We face
three main challenges to deal with complex programs and
structured data: (i) the number of clauses to be considered
increases, (ii) an inﬁnite number of ground atoms can be
generated with function symbols, and (iii) the memory and
computation costs increase quadratically with respect to the
size of the search space. We address these issues by propos-
ing a new differentiable approach to learning logic programs
by combining adaptive symbolic search methods and contin-
uous optimization methods and make the following contri-
butions for each problem:
Clause Search with Reﬁnement We propose an efﬁcient
clause search method for a differentiable ILP solver. We
generate clauses by beam searching and leveraging the gen-
erality of clauses and the given examples. We start from
general (strong) clauses and incrementally specify (weaken)
the clauses. We only take clauses that contribute to accu-

 
 
 
 
 
 
rate classiﬁcation results into the search space. Our approach
yields an efﬁcient search space that includes only promising
clauses for the differentiable ILP solver.
Adaptive Fact Enumeration We present a fact enumeration
algorithm to implement the differentiable inference func-
tion. The number of ground atoms deﬁnes the size of the
tensors used in the differentiable step, thus the set of re-
quired ground atoms must be determined. We enumerate the
ground atoms using the given examples and the generated
clauses by backward-chaining. Our approach yields a small
set of ground atoms, and this small set is a key factor to
achieve differentiable learning from structured objects.
Soft Program Composition We propose a practical algo-
rithm to learn complex logic programs in a differentiable
manner. In past studies, the weights were assigned to each
pair of clauses because some information is lost if weights
are assigned to each clause, and thus the number of parame-
ters increased quadratically. In our approach, we compose a
differentiable inference function by assigning multiple dis-
tinct weights to each clause and introducing a function to
compute logical or softly. Our approach efﬁciently estimates
logic programs in terms of memory and computation costs.

Notation We use bold lowercase letters v, w, . . . for vectors
and the functions that return vectors. We use bold capital
letters X, . . . for tensors. We use calibrate letters C, A, . . .
for sets and ordered sets.

Related Work
A pioneering study of inductive inference was done in
the early 70s (Plotkin 1971). The Model Inference System
(MIS) (Shapiro 1983) has been implemented as an efﬁcient
search algorithm for logic programs using the generality
of expressions. Inductive Logic Programming (Muggleton
1991) has emerged at the intersection of machine learn-
ing and logic programming. The Elementary Formal System
(EFS) (Arikawa, Shinohara, and Yamamoto 1992) is a well-
established system for strings based on ﬁrst-order logic.

Dealing with uncertainty in ILP has been a major obsta-
cle. Probabilistic Inductive Logic Programming (De Raedt
and Kersting 2004) combines probability theory with ILP. It
is also known as Statistical Relational Learning (De Raedt
et al. 2016). Another approach to cope with uncertainty is
to combine neural methods with differentiable implementa-
tions of logic (Rockt¨aschel and Riedel 2017; Yang, Yang,
and Cohen 2017; Evans and Grefenstette 2018; ˇSourek et al.
2018; Manhaeve et al. 2018; Si et al. 2019; Riegel et al.
2020; Cohen, Yang, and Mazaitis 2020; Marra et al. 2020).
Both of these approaches blazed a trail for the integration
of logic, probability, and neural methods. However, almost
all of these approaches are domain-speciﬁc (De Raedt et al.
2020), i.e., the expressions are severely limited. A critical
gap exists between these past approaches and logic-based
systems for structured data, such as MIS and EFS. Our work
ﬁlls the gap by incorporating symbolic methods with differ-
entiable approaches.

A propositional approach for ILP is one established ap-
proach, which was developed to integrate ILP and SAT
solvers or Binary Decision Diagrams (Chikara et al. 2015;

Shindo, Nishino, and Yamamoto 2018). The ∂ILP system
performed differentiable learning by incorporating contin-
uous relaxation into these approaches. We also follow this
approach.

Beam searching with clause reﬁnement was developed for
structure learning for probabilistic logic programs (Bellodi
and Riguzzi 2015; Nguembang Fadja and Riguzzi 2019). We
use this approach because it requires fewer declarative bi-
ases than approaches based only on templates.

Inductive Logic Programming Concepts
Basic Concepts Language L is a tuple (P, F, A, V),
where P is a set of predicates, F is a set of function sym-
bols, A is a set of constants, and V is a set of variables. We
denote n-ary predicate p by p/n and n-ary function sym-
bol f by f /n. A term is a constant, a variable, or an ex-
pression f (t1, . . . , tn) where f is a n-ary function symbol
and t1, . . . , tn are terms. A function symbol yields a struc-
tured expression. An atom is a formula p(t1, . . . , tn), where
p is an n-ary predicate symbol and t1, . . . , tn are terms.
A ground atom or simply a fact is an atom with no vari-
ables. A literal is an atom or its negation. A positive lit-
eral is just an atom. A negative literal is the negation of
an atom. A clause is a ﬁnite disjunction (∨) of literals. A
deﬁnite clause is a clause with exactly one positive literal.
If A, B1, . . . , Bn are atoms, then A ∨ ¬B1 ∨ . . . ∨ ¬Bn is
a deﬁnite clause. We write deﬁnite clauses in the form of
A ← B1 ∧ . . . ∧ Bn. Atom A is called the head, and set
of negative atoms {B1, . . . , Bn} is called the body. We de-
note special constant true as (cid:62) and false as ⊥. We denote
a set of variables in clause C as V (C). DVn(C) is a set
of all n-combinations of distinct variables in clause C, i.e.,
DVn(C) = {(x1, . . . , xn)|xi ∈ V (C) ∧ xi (cid:54)= xj(i (cid:54)= j)}.
Substitution θ = {x1 = t1, ..., xn = tn} is an assignment
of term ti to variable xi. An application of substitution θ to
atom A is written as Aθ. A uniﬁer for the set of expressions
{A1, . . . , An} is a substitution θ such that A1θ = A2θ =
. . . = Anθ, written as θ = σ({A1, . . . , An}), where σ is a
uniﬁcation function. A uniﬁcation function returns the (most
general) uniﬁer for the expressions if they are uniﬁable. De-
cision function ¯σ({A1, . . . , An}) returns a Boolean value
whether or not A1, . . . , An are uniﬁable.

Inductive Logic Programming
ILP problem Q is tuple (E +, E −, B, L), where E + is a set
of positive examples, E − is a set of negative examples, B
is background knowledge, and L is a language. We assume
that the examples and the background knowledge are ground
atoms. The solution to an ILP problem is a set of deﬁnite
clauses H ⊆ L that satisﬁes the following conditions:
• ∀A ∈ E + H ∪ B |= A.
• ∀A ∈ E − H ∪ B (cid:54)|= A.
Typically the search algorithm starts from general clauses.
If the current clauses are too general (strong), i.e., they en-
tail too many negative examples, then the solver incremen-
tally speciﬁes (weakens) them. This weakening operation is
called a reﬁnement, which is one of the essential tools for
ILP.

Reﬁnement Operator The reﬁnement operator deﬁnes be-
tween clauses the complexity that varies from general to
speciﬁc. The reﬁnement operator takes a clause and returns
weakened clauses. Generally, there are four types of reﬁne-
ment operators: (i) application of function symbols, (ii) sub-
stitution of constants, (iii) replacement of variables, and (iv)
addition of atoms. For clause C = A ← B1, . . . , Bm, each
reﬁnement operation for language L = (P, F, A, V) is as
follows:
• For z ∈ V (C), f ∈ F, and x1, . . . , xn ∈ V\V (C), let
L (C), where x1, . . . , xn are

C{z = f (x1, . . . , xn)} ∈ ρfun
pairwise different.

• For z ∈ V (C) and a ∈ A, let C{z = a} ∈ ρsub
• For z, y ∈ V (C) (z (cid:54)= y), let C{z = y} ∈ ρrep
• For n-ary predicate p ∈ P and (x1, . . . , xn) ∈ DVn(C),

L (C).
L (C).

let A ← B1, . . . , Bm, p(x1, . . . , xn) ∈ ρadd
L (C).
The reﬁnement operator for language L is deﬁned:

ρL(C) = ρfunc

L (C) ∪ ρsubs

L (C) ∪ ρrep

L (C) ∪ ρadd

L (C).

(1)

Example 1 Let L = (P, F, A, V), where P = {p/2, q/2},
F = {f /1}, A = {a, b} and V = {x, y, z}. Let E + =
{p(a, a), p(b, b)}, E − = {p(a, b), p(b, a)}, B = {}. One of
the solutions is H = {p(x, x)}.
Example 2 Let L be the language speciﬁed in Exam-
ple 1. The following is the result of the reﬁnement:
ρL(p(x, y)) = {p(a, y), p(x, a), p(b, y), p(x, b) p(x, x),
p(f (z), y), p(x, f (z)), p(x, y) ← q(x, y) }.

Differentiable Inductive Logic Programming
In the ∂ILP framework (Evans and Grefenstette 2018), an
ILP problem is formulated as an optimization problem that
has the following general form:

min
W

L(Q, C, W),

(2)

where Q is an ILP problem, C is a set of clauses speciﬁed
by templates, W is a set of weights for clauses, and L is a
loss function that returns a penalty when training constraints
are violated. We brieﬂy summarize the steps of the process
as follows:
Step 1 Set of ground atoms G is speciﬁed by given language
L ∈ Q.
Step 2 Tensor X is built from given set of clauses C and
ﬁxed set of ground atoms G. It holds the relationships be-
tween clauses C and ground atoms G. Its dimension is pro-
portional to |C| and |G|.
Step 3 Given background knowledge B ∈ Q is compiled
into vector v0 ∈ R|G|. Each dimension corresponds to each
ground atom Gj ∈ G, and v0[j] represents the valuation of
Gj.
Step 4 A computational graph is constructed from X and
W. The weights deﬁne probability distributions over clauses
C. A probabilistic forward-chaining inference is performed
by the forwarding algorithm on the computational graph
with input v0.
Step 5 The loss is minimized with respect to weights W by

Figure 1: Overview of our model

gradient descent techniques. After minimization, a human-
readable program is extracted by discretizing the weights.

Method
Although we begin by following the ∂ILP approach, we in-
troduce several new algorithms to deal with structured ex-
amples and complex programs with function symbols. An
overview of our approach is illustrated in Fig. 1. First, we
generate clauses by beam searching with reﬁnement to spec-
ify an efﬁcient search space. Second, we enumerate ground
atoms by backward-chaining using the set of generated
clauses. This enumeration results in efﬁcient inference com-
putation because the number of ground atoms determines the
dimensions of tensors for the differentiable steps. Third, we
propose a new approach to softly compose complex logic
programs. We assign several weights for each clause to de-
ﬁne several probability distributions over the clauses and
efﬁciently estimate logic programs that consist of several
clauses.

Clause Search with Reﬁnement
We incrementally generate candidates of clauses by reﬁne-
ment and beam searching. Promising clauses for an ILP
problem are those that entail many positive examples but
few negative examples. Algorithm 1 is our generation algo-
rithm. The inputs are initial clauses C0, ILP problem Q, the
size of the beam in search Nbeam , and the number of steps
of beam searching Tbeam . We start from the initial clauses
and iteratively weaken the top-Nbeam clauses based on how
many positive examples can be entailed by clause combining
with background knowledge. The following is the evaluation
function for clause R:

eval (R, Q) = |{E | E ∈ E + ∧ B ∪ {R} |= E}|,

(3)

where E + is a set of positive examples.

The key difference from ∂ILP is that we leverage the
given examples to specify the search space for the differen-
tiable solver. In ∂ILP, since the clauses are generated only by
templates many meaningless clauses tend to be generated.
Example 3 Let E + = {p(a, a), p(b, b), p(b, c), p(c, b)},
B = {q(b, c), q(c, b)}, C0 = {p(x, y)}, Tbeam = 2, and
Nbeam = 2. Fig. 2 illustrates an example of beam search-
ing for this problem. In the 2nd layer, we show examples

ILP problemfact enumeratorrefinementclause generatortensor encoderconvertinfercross-entropylossweightsLegendnon-differentiablefunctiondifferentiablefunctioninputparameters∗p(x, y)

Algorithm 2 Enumeration of ground atoms

p(a, y)

∗p(x, x)

p(f (x), y) ∗p(x, y) ← q(x, y)

...

...

...

...

...

...

Figure 2: Beam searching for clauses

Algorithm 1 Clause generation by beam searching

Input: C0, Q, Nbeam , Tbeam

1: Cto open ← C0
2: C ← ∅
3: t = 0
4: while t < Tbeam do
Cbeam ← ∅
5:
for Ci ∈ Cto open do
6:
C = C ∪ {Ci}
7:
for R ∈ ρL(Ci) do
8:
9:
10:

score = eval (R, Q) //Evaluate each clause
Cbeam = insert(Cbeam , R, score) //Insert reﬁned
clause in order of scores possibly discarding it
Cto open = Cbeam //top-Nbeam clauses are reﬁned in the next
loop
t = t + 1

11:

12:
13: return C

of generated clauses by reﬁning the initial clause. Each new
clause is evaluated and selected to be reﬁned. In this case,
clause p(x, x) and p(x, y) ← q(x, y) is reﬁned in the next
step because it entails more positive examples with back-
ground knowledge B than other clauses. Reﬁned clauses are
added to set C. By contrast, since clause p(f (x), y) does
not entail any positive examples with background knowl-
edge B, it is discarded. Finally, we get set of clauses C =
{p(x, y), p(x, x), p(x, y) ← q(x, y)}.

Adaptive Fact Enumeration
We enumerate ground atoms using the given clauses and ex-
amples. Algorithm 2 is our enumeration algorithm. The in-
puts are ILP problem Q = (E +, E −, B, L), set of clauses
C, and time-step parameter T that determines the number of
forward-chaining steps in the differentiable inference. We
start from the given examples, the background knowledge,
and special symbols that represent true and false respec-
tively. We unify the head of each clause and each ground
atom. If they are uniﬁable, then we compute the ground
atoms on the body by applying the uniﬁer. Here we assume
that the body has fewer variables than the head.

The key difference from ∂ILP is that we utilize the given
ILP problem to specify the set of ground atoms. In ∂ILP,
the solver considers all the visible ground atoms, which is
known as the Herbrand Base. However, since an inﬁnite
number of ground atoms are yielded by function symbols,
it is unsuitable for the case with function symbols.
Example 4 Let E + = {e(s6(0))}, E − = {e(s(0))},
B = {e(0)}, C = {e(s2(x)) ← e(x)}, and T = 2.

Input: Q, C, T
1: G ← {⊥, (cid:62)} ∪ E + ∪ E − ∪ B
2: for i = 0 to T − 1 do
3:
4:
5:
6:
7:

S ← ∅
for A ← B1, . . . , Bn in C do

for G ∈ G do

if ¯σ(A, G) then
θ ← σ(A, G)
S ← S ∪ {B1θ, . . . , Bnθ}

G ← G ∪ S

8:
9: return G

First G is initialized as G = {⊥, (cid:62), e(s6(0)), e(s(0)), e(0)}.
Atom e(s6(0)) and clause head e(s2(x)) are uniﬁable
with θ = {x = s4(0)}. Then body e(x)θ = e(s4(0)),
and this ground atom is added to G. In the next step,
atom e(s4(0)) and clause head e(s2(x)) are uniﬁable with
θ = {x = s2(0)}. Hence body e(x)θ = e(s2(0))
is added to G. Finally, the enumeration algorithm returns
G = {⊥, (cid:62), e(0), e(s(0)), e(s2(0)), e(s4(0)), e(s6(0))}.
Note that ground atoms e(s3(0)) and e(s5(0)) are not re-
quired in this case.

Soft Program Composition
Tensor Encoding We build a tensor that holds the rela-
tionships between clauses C and ground atoms G. We as-
sume that C and G are an ordered set, i.e., where every ele-
ment has its own index. Let b be the maximum body length
in C. Index tensor X ∈ N|C|×|G|×b contains the indexes
of the ground atoms to compute forward inferences. Intu-
itively, X[i, j] ∈ Nb contains a set of the indexes of the sub-
goals to entail the j-th fact using the i-th clause. For clause
Ci = A ← B1, . . . , Bn ∈ C and set of ground atoms G, we
compute tensor X:

X[i, j, k] =






IG(Bkθ) if ¯σ({A, Gj}) ∧ k ≤ n
IG((cid:62)) if ¯σ({A, Gj}) ∧ k > n
IG(⊥) if ¬¯σ({A, Gj})

,

(4)

where 0 ≤ j ≤ |G| − 1, 0 ≤ k ≤ b − 1, θ = σ({A, Gj}),
and IG(x) returns the index of x in G. If clause head A and
ground atom Gj are uniﬁable, then we put the index of sub-
goal Bkθ into the tensor (line 1 in Eq. 4). If the clause has
fewer body atoms than the longest clause in C, we ﬁll the gap
with the index of (cid:62) (line 2 in Eq. 4). If clause head A and
ground atom Gj are not uniﬁable, then we place the index
of ⊥ (line 3 in Eq. 4).
Example 5 Let C0 = e(x), C1 = e(s2(x)) ← e(x) and G =
{⊥, (cid:62), e(0), e(s(0)), e(s2(0)), e(s4(0))}. Then the follow-
ing table shows tensor X:

j
G
X[0, j]
X[1, j]

0
⊥
[0]
[0]

2

1
(cid:62) e(0)
[1]
[1]
[0]
[1]

3
e(s(0))
[1]
[0]

4
e(s2(0))
[1]
[2]

5
e(s4(0))
[1]
[4]

For example, X[1, 4] = [2] because clause C1 entails
e(s2(0)) with substitution θ = {x = 0}. Then subgoal

e(x)θ = e(0), which has index 2. Clause C0 does not have a
body atom, and so the body is ﬁlled by (cid:62), which has index
1.

Valuation Valuation vector vt ∈ R|G| maps each ground
atom into a continuous value at each time step t. The back-
ground knowledge is compiled into v0:

v0[j] = fconvert (B)[j] =

(cid:26)1 (Gj ∈ B ∨ Gj = (cid:62))

0 (otherwise)

. (5)

The differentiable inference function is performed based on
valuation vectors. To compute the T -step forward-chaining
inference, we compute the sequence of valuation vectors
v0, . . . , vT in the differentiable inference process.

Clause Weights We assign weights to softly compose
the logic programs as follows: (i) We ﬁx the target pro-
grams’ size as m, i.e., where we try to ﬁnd a logic pro-
gram with m clauses. (ii) We introduce |C|-dim weights
W = {w1, . . . , wm}. (iii) We take the softmax of each
weight vector wl ∈ W and softly choose m clauses to com-
pose the logic program. As a probabilistic interpretation, we
deﬁne a probability distribution p(xl
i is a proba-
bilistic variable representing clause Ci is the l-th component
of the target program.

i), where xl

In ∂ILP, the weights are assigned to each pair of clauses
by assuming all programs are composed of pairs of clauses
for each predicate. In our method, we assign several weights
to each clause and softly choose each clause. Our approach
enables the solver to deal with complex programs that con-
sist of several clauses with identical predicates.

Differentiable Inference We compose a differentiable
function, called an infer function, that performs forward-
chaining inference. The inference result is obtained:

vT = finfer (X, v0, W, T ),

(6)

where finfer is the infer function, X is the index tensor, v0
is the initial valuation vector, W is the set of weight vectors,
and T is the time step.

The infer function is computed as follows. First, each
clause Ci ∈ C is compiled into a function ci : R|G| → R|G|:

ci(vt)[j] =

(cid:89)

k

gather(vt, X[i])[j, k],

(7)

where function gather : R|G| × N|G|×b → R|G|×b is:

gather(a, B)[j, k] = a[B[j, k]].

(8)

The gather function replaces the indexes of the ground
atoms by the current valuation values. To take logical and
across the subgoals in the body, we take the product across
dimension 1.

Next we take the weighted sum of the clause function us-

ing wl ∈ W:

hl(vt) =

(cid:88)

i

softmax(wl)[i] · ci(vt),

(9)

where softmax(x)[i] =
that
softmax(wl)[i] is interpreted as a probability that Ci ∈ C
is the l-th component of the target program.

i(cid:48) exp(x[i(cid:48)]) . Note

(cid:80)

exp(x[i])

Then we compute the forward-chaining inference using

clauses C and weights W:

r(vt) = softorγ (h1(vt), . . . , hm(vt)) ,
(10)
where softorγ is a smooth logical or function on the valua-
tion vectors:

softorγ(x1, . . . , xm)[j] = γ log

(cid:88)

exl[j]/γ,

(11)

l

where γ > 0 is a smooth parameter. Taking logical or softly
for the valuation vectors corresponds to the fact that a logic
program is generally represented as a conjunction of clauses.
Finally, we perform T -step inference by iteratively amal-

gamating the results:

vt+1 = softorγ (vt, r(vt)) .

(12)

Infer function finfer (X, v0, W, T ) returns vT .

Learn Target Program
Let Q = (E +, E −, B, L). We generate pairs of atoms and
labels as:

Y = {(E, 1) | E ∈ E +} ∪ {(E, 0) | E ∈ E −}.

(13)

Each pair (E, y) represents whether atom E is positive or
negative. We compute the conditional probability of label y
of atom E:

p(y | E, Q, C, W, T ) = finfer (X, v0, W, T )[IG(E)], (14)
where C = fbeam search (C0, Q, Nbeam , Tbeam ), G =
fenumerate (C, Q, T ), v0 = fconvert (B), X is the index ten-
sor, and IG(x) returns the index of x in G. Here fbeam search
is the clause generation function following Algorithm 1,
fenumerate is the fact enumeration function following Al-
gorithm 2, W is the set of weights, and T is the time step for
the infer function.

We solve ILP problem Q by minimizing cross-entropy

loss, deﬁned as:
loss = −E(E,y)∼Y [y log p(y | E, Q, C, W, T )+

(1 − y) log(1 − p(y | E, Q, C, W, T ))].
(15)

Experiments
In this section, we experimentally support the following
claims: (1) Our enumeration algorithm yields a reasonable
number of ground atoms. (2) Our clause generation algo-
rithm improves the performance of differentiable program
searching. (3) Our soft program composition is efﬁcient in
terms of memory and computation costs. (4) Our framework
learns logic programs successfully from noisy and structured
examples, which are outside the scope of both ∂ILP and
standard ILP approaches.
We performed our experiments1 on several standard ILP
tasks with structured examples, partially adopted from

1The source code of all experiments will be available at

https://github.com/hkrsnd/dilp-st

Shapiro and Caferra (Shapiro 1983; Caferra 2013). Through
all the tasks, sets of variables were consistently ﬁxed, i.e.,
V = {x, y, z, v, w}.
Member The task is to learn the membership function for
lists. The language is given as P = {mem/2}, F = {f /2},
A = {a, b, c, ∗}. The initial clause is C0 = {mem(x, y)}.
The problem is brieﬂy described:

E + = {mem(a, [a, c]), mem(a, [b, a]), . . .},
E − = {mem(c, [b, a]), mem(c, [a]), . . .},

B = {mem(a, [a]), mem(b, [b]), mem(c, [c])}.

Plus The task is to learn the plus operation for natural num-
bers. The language is given as P = {plus/3}, F = {s/1},
A = {0}. The initial clause is C0 = {plus(x, y, z)}. The
problem is brieﬂy described:
E + = {plus(s(0), 0, s(0)), plus(s5(0), s3(0), s8(0)), . . .},
E − = {plus(s(0), s2(0), 0), plus(0, s2(0), s4(0)), . . .},

B = {plus(0, 0, 0)}.

Append The task is to learn the append function for lists.
The language is given as P = {app/3}, F = {f /2}, A =
{a, b, c, ∗}. The initial clause is C0 = {app(x, y, z)}. The
problem is brieﬂy described:
E + = {app([c], [], [c]), app([a, a, b], [b, c], [a, a, b, b, c]), . . .},
E − = {app([], [a, a], [a, a, b]), app([b], [], [c]), . . .},

B = {app([], [], [])}.

Delete The task is to learn the delete operation for lists.
The language is given as P = {del/3}, F = {f /2},
A = {a, b, c, ∗}. The initial clause is C0 = {del (x, y, z)}.
The problem is brieﬂy described:

E + = {del (b, [a, c, b], [a, c]), del (a, [b, a, a], [b, a]), . . .},
E − = {del (c, [c, a, a], [a, b]), del (b, [b], [a]), . . .},
B = {del (a, [a], []), del (b, [b], []), del (c, [c], [])}.

Subtree The task is to learn the subsumption relation for
binary trees. The language is given as P = {sub/2},
F = {f /2}, A = {a, b, c}. The initial clause is C0 =
{sub(x, y)}. The problem is brieﬂy described:

E + = {sub(f (b, b), f (f (f (b, b), f (a, c)), f (a, c))), . . .},
E − = {sub(f (a, a), f (f (c, a), f (a, c))), . . .},

B = {sub(a, a), sub(b, b), sub(c, c)}.

In each task, we randomly generate 50 examples for each
class. Note that the list objects are represented in a readable
form, e.g., term f (a, f (b, ∗)) is represented as [a, b].

Experimental Methods and Results
Hyperparameters To generate clauses, we used several
biases for them: (i) the maximum number of bodies, denoted
by Nbody , and (ii) the maximum number of the nests of func-
tion symbols, denoted by Nnest . In all experiments, we set
Nbody = 1 and Nnest = 1. We set beam size Nbeam , and
beam step Tbeam is (Nbeam , Tbeam ) = (3, 3) for the Mem-
ber task, (Nbeam , Tbeam ) = (15, 3) for the Subtree task, and
(Nbeam , Tbeam ) = (10, 5) for the other tasks.

Member
228

Plus
1857

Append Delete
2513

2899

Subtree
2172

Table 1: Number of enumerated ground atoms

Figure 3: AUC for number of generated clauses

We set target program size m as m = 2 for the Member
and Delete tasks, m = 3 for the Plus and Append tasks, and
m = 4 for the Subtree task. We set T for the differentiable
inference as T = 8 for the Plus task and T = 4 for the other
tasks. We set γ = 10−5 for the softor function.

We trained our model with the RMSProp optimizer with
a learning rate of 0.01 for 3000 epochs. We sampled mini-
batches during the optimization, and each mini-batch con-
tained 5% of the training examples chosen randomly for
each iteration. The weights were initialized randomly in
each trial. We divided the data into 70% training and 30%
test. All experiments were performed on a desktop computer
using its GPU2.
Experiment 1 To support claim 1, we show the number of
enumerated ground atoms for training data in each dataset
in Table 1. Our enumeration algorithm yielded a reasonable
number of ground atoms in each dataset. The ∂ILP approach
is infeasible in our setting because, although it considers all
the ground atoms generated in the language, an inﬁnite num-
ber of them can be generated with function symbols.
Experiment 2 To support claim 2, we compared two clause
generation algorithms: (i) generation by beam searching and
reﬁnement and (ii) naive generation without beam search-
ing. In setting (ii), we generated clauses without evaluation
by examples. Like ∂ILP, it did not use the given examples
during clause generation. We set a number of clauses, de-
noted by Nclause . The generation stopped when the number
of generated clauses exceeded Nclause . We performed clas-
siﬁcation with different Nclause . We changed the value from
10 to 40 by increments of 10 and ran the experiments 5-times
with random-weight initialization.

Figure 3 shows the AUC for the Append and Delete tasks.
In each task, our approach achieved AUC scores of 1.0 with
fewer clauses. These results show that our clause generation
algorithm improved the differentiable solver, i.e., yielded an
efﬁcient search space.
Experiment 3 To support claim 3, we compared 2 differ-
ent approaches for the infer function: (i) multiple weights

2CPU: Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60 GHz, GPU:

GeForce 1080Ti 11 GB, RAM: 64 GB

Parameters

Runtime [s]

Proposed
24
120
150
150
80

Pair
144
1600
2500
2500
400

Proposed
0.015
0.03
0.09
0.06
0.039

Pair
0.12
6.91
5.18
5.4
1.11

Member
Plus
Append
Delete
Subtree

Table 2: Number of parameters and mean runtime in learn-
ing steps

and softor approach (proposed here) and (ii) 2-d weights
for pairs of clauses. Setting (ii) is a ∂ILP approach, which
deﬁnes a probability distribution over the pairs of clauses
C, i.e., we assigned weights in the form of a 2-d matrix
W ∈ R|C|×|C|. We compared the number of parameters and
mean runtimes for each step of the gradient descent.

Table 2 shows our results. In each dataset, the proposed
approach had fewer parameters. Moreover, the mean run-
time of the gradient descent was much shorter than with the
pairing approach. These results show that our approach was
efﬁcient in terms of memory and computation costs.
Experiment 4 To support claim 4, we evaluated our ap-
proach by changing the proportion of the mislabeled training
data. First, we generated training examples. Then we ﬂipped
the label of examples to make noise according to the pro-
portion. We changed the proportion of mislabeled data from
0.0 to 0.5 by increments of 0.05. We ran the experiments
5-times with random-weight initialization.

Figure 4 shows the mean-squared test error for the propor-
tion of mislabeled training data in the Member and Subtree
tasks. In each task, the test error increased gradually as the
noise proportion increased. Moreover, our method achieved
test error less than 0.05 with 10% mislabeled training data
in both tasks. This shows that our approach was robust to
noise, i.e., it found a functional theory even if there were
mislabeled data. Note that standard ILP approaches fail to
ﬁnd a theory when there are mislabeled data.

We show an example of the obtained programs in Table 3.
The clauses for lists are represented in a readable form, e.g.,
term f (x, y) is represented as [x|y]. In the Plus task, the last
clause represents the plus operation considering commuta-
tivity for natural numbers. Also the last clause in the Append
task can be interpreted clearly. If v is obtained by append-
ing y to z, then the result of appending y with head x to z
is obtained just by v with head x. Our framework learned
structured knowledge from structured examples beyond re-
lational logic.

Conclusion
We proposed a new differentiable inductive logic program-
ming framework that deals with complex logic programs
with function symbols that yield readable outputs for struc-
tured data. To establish our framework, we proposed three
main contributions. First, we proposed a clause generation
algorithm that uses beam searching with reﬁnement. Second,
we proposed an enumeration algorithm for ground atoms.
Third, we proposed a soft program composition approach

Figure 4: Mean-squared test error as proportion of misla-
beled training data

Problem

Member

Plus

Append

Delete

Subtree

Learned logic program
mem(x, [y|z]) ← mem(x, z)
mem(x, [x|y])
plus(0, x, x)
plus(x, s(y), s(z)) ← plus(x, y, z)
plus(s(x), y, s(z)) ← plus(y, x, z)
app([], x, x)
app(x, [], x)
app([x|y], z, [x|v]) ← app(y, z, v)
del (x, [x|y], y)
del (x, [y|z], [y|v]) ← del (x, z, v)
sub(f (x, y), f (x, y))
sub(x, f (y, z)) ← sub(x, z)
sub(x, f (y, z)) ← sub(x, y)
sub(x, f (y, x))

Table 3: Learned logic programs in standard ILP tasks

using multiple weights and the softor function.

In our experiments, we showed: (i) our enumeration al-
gorithm yields a reasonable number of ground atoms, (ii)
our clause generation algorithm improves the performance
of differentiable program searching, (iii) our soft program
composition is efﬁcient in memory and computation costs,
and (iv) our framework learns logic programs successfully
from noisy and structured examples, which are outside the
scope of both ∂ILP and standard ILP approaches.

One major limitation of our framework is its scalabil-
ity for large-scale programs. A high-quality search space
is necessary to deal with more expressive programs, such
as sorting. Further research could tackle this problem by
incorporating such declarative bias (Claire et al. 1996;
De Raedt 2012) as mode declarations (Muggleton 1991)
and metarules (Cropper, Tamaddoni-Nezhad, and Muggle-
ton 2015) to manage the search space.

To the best of our knowledge, this is the ﬁrst work that
incorporates symbolic methods, such as reﬁnement, with a
differentiable ILP approach. We believe that our work will
trigger future work to combine the best of both the symbolic
and subsymbolic worlds.

Acknowledgments This work was partly supported by
JSPS KAKENHI Grant Number 17K19973.

References
Arikawa, S.; Shinohara, T.; and Yamamoto, A. 1992. Learn-
In Theoretical Computer
ing elementary formal systems.
Science, volume 95, 97–113.

Bellodi, E.; and Riguzzi, F. 2015. Structure learning of prob-
abilistic logic programs by searching the clause space. The-
ory Pract. Log. Program. 15(2): 169–212.

Caferra, R. 2013. Logic for Computer Science and Artiﬁcial
Intelligence. Wiley.

Chikara, N.; Koshimura, M.; Fujita, H.; and Hasegawa, R.
2015. Inductive logic programming using a MaxSAT solver.
In 25th International Conference on Inductive Logic Pro-
gramming (ILP 2015).

Claire, N.; C´eline, R.; Hilde, A.; Francesco, B.; and Birgit,
T. 1996. Declarative bias in ILP. Advances in inductive logic
programming 32: 82––103.

Cohen, W. W.; Yang, F.; and Mazaitis, K. 2020. Tensor-
Log: A Probabilistic Database Implemented Using Deep-
Learning Infrastructure. J. Artif. Intell. Res. (JAIR) 67: 285–
325.

Cropper, A.; Tamaddoni-Nezhad, A.; and Muggleton, S. H.
2015. Meta-Interpretive Learning of Data Transformation
In 25th International Conference on Inductive
Programs.
Logic Programming (ILP 2015), volume 9575, 46–59.

Dantsin, E.; Eiter, T.; Gottlob, G.; and Voronkov, A. 2001.
Complexity and Expressive Power of Logic Programming.
ACM Comput. Surv. 33(3): 374–425.

d’Avila Garcez, A. S.; Gori, M.; Lamb, L. C.; Seraﬁni, L.;
Spranger, M.; and Tran, S. N. 2019. Neural-symbolic Com-
puting: An Effective Methodology for Principled Integration
of Machine Learning and Reasoning. FLAP 6(4): 611–632.

De Raedt, L. 2012. Declarative Modeling for Machine
In 24th International Confer-
Learning and Data Mining.
ence on Algorithmic Learning Theory (ALT 2013), 12–12.

De Raedt, L.; Dumancic, S.; Manhaeve, R.; and Marra, G.
2020. From Statistical Relational to Neuro-Symbolic Artiﬁ-
cial Intelligence. In 29th International Joint Conference on
Artiﬁcial Intelligence (IJCAI 2020), 4943–4950.

De Raedt, L.; and Kersting, K. 2004. Probabilistic Inductive
In 15th International Conference on
Logic Programming.
Algorithmic Learning Theory (ALT 2004), 19–36.

De Raedt, L.; Kersting, K.; Natarajan, S.; and Poole, D.
2016. Statistical Relational Artiﬁcial Intelligence: Logic,
Probability, and Computation. Synthesis Lectures on Artiﬁ-
cial Intelligence and Machine Learning. Morgan & Claypool
Publishers.

Evans, R.; and Grefenstette, E. 2018. Learning Explanatory
Rules from Noisy Data. J. Artif. Intell. Res. (JAIR) 61: 1–64.

Fredouille, D. C.; Bryant, C. H.; Jayawickreme, C. K.; Jupe,
S.; and Topp, S. 2007. An ILP Reﬁnement Operator for Bi-
ological Grammar Learning. In 16th International Confer-
ence on Inductive Logic Programming (ILP 2006), 214–228.

Kok, S.; and Domingos, P. 2005. Learning the Structure of
Markov Logic Networks. In 22th International Conference
on Machine Learning (ICML 2005), 441–448.
Lloyd, J. W. 2003. Logic for Learning. Springer-Verlag
Berlin Heidelberg.

Inductive logic programming. New

Manhaeve, R.; Dumancic, S.; Kimmig, A.; Demeester, T.;
and De Raedt, L. 2018. DeepProbLog: Neural Probabilis-
tic Logic Programming. In Advances in Neural Information
Processing Systems 31 (NeurIPS 2018), 3749–3759.
Marra, G.; Diligenti, M.; Giannini, F.; Gori, M.; and Mag-
gini, M. 2020. Relational Neural Machines. In 24th Euro-
pean Conference on Artiﬁcial Intelligence (ECAI 2020).
Muggleton, S. 1991.
Generation Computing 8(4): 295–318.
Nguembang Fadja, A.; and Riguzzi, F. 2019. Lifted discrim-
inative learning of probabilistic logic programs. Machine
Learning 108(7): 1111–1135.
Nienhuys-Cheng, S.-H.; Wolf, R. d.; Siekmann, J.; and Car-
bonell, J. G. 1997. Foundations of Inductive Logic Program-
ming. Springer-Verlag.
Plotkin, G. 1971. A further note on inductive generaliza-
tion. In Machine Intelligence, volume 6. Edinburgh Univer-
sity Press.

Riegel, R.; Gray, A. G.; Luus, F. P. S.; Khan, N.; Makondo,
N.; Akhalwaya, I. Y.; Qian, H.; Fagin, R.; Barahona, F.;
Sharma, U.; Ikbal, S.; Karanam, H.; Neelam, S.; Likhyani,
A.; and Srivastava, S. K. 2020. Logical Neural Networks.
CoRR abs/2006.13155.
Rockt¨aschel, T.; and Riedel, S. 2017. End-to-end Differen-
tiable Proving. In Advances in Neural Information Process-
ing Systems 30 (NeurIPS 2017), 3788–3800.
Shapiro, E. Y. 1983. Algorithmic Program DeBugging. MIT
Press.

Shindo, H.; Nishino, M.; and Yamamoto, A. 2018. Using Bi-
nary Decision Diagrams to Enumerate Inductive Logic Pro-
gramming Solutions. In 28th International Conference on
Inductive Logic Programming (ILP 2018), 52–67.
Si, X.; Raghothaman, M.; Heo, K.; and Naik, M. 2019. Syn-
thesizing Datalog Programs using Numerical Relaxation.
In 28th International Joint Conference on Artiﬁcial Intelli-
gence, (IJCAI 2019), 6117–6124.
ˇSourek, G.; Aschenbrenner, V.; ˇZelezn´y, F.; Schockaert, S.;
and Kuˇzelka, O. 2018. Lifted Relational Neural Networks:
Efﬁcient Learning of Latent Relational Structures. J. Artif.
Int. Res. (JAIR) 62(1): 69–100.
Yang, F.; Yang, Z.; and Cohen, W. W. 2017. Differentiable
Learning of Logical Rules for Knowledge Base Reasoning.
In Advances in Neural Information Processing Systems 30
(NeurIPS 2017), 2316–2325.

