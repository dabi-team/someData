Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN

Shengxian Wan∗, Yanyan Lan†, Jun Xu†, Jiafeng Guo†, Liang Pang∗, and Xueqi Cheng†
CAS Key Lab of Network Data Science and Technology
Institute of Computing Technology, Chinese Academy of Sciences, China
∗{wanshengxian, pangliang}@software.ict.ac.cn, †{lanyanyan, junxu, guojiafeng, cxq}@ict.ac.cn

6
1
0
2

r
p
A
5
1

]
L
C
.
s
c
[

1
v
8
7
3
4
0
.
4
0
6
1
:
v
i
X
r
a

Abstract

Semantic matching, which aims to determine the
matching degree between two texts, is a fundamen-
tal problem for many NLP applications. Recently,
deep learning approach has been applied to this
problem and signiﬁcant improvements have been
achieved.
In this paper, we propose to view the
generation of the global interaction between two
texts as a recursive process: i.e. the interaction of
two texts at each position is a composition of the
interactions between their preﬁxes as well as the
word level interaction at the current position. Based
on this idea, we propose a novel deep architec-
ture, namely Match-SRNN, to model the recursive
matching structure. Firstly, a tensor is constructed
to capture the word level interactions. Then a spa-
tial RNN is applied to integrate the local interac-
tions recursively, with importance determined by
four types of gates. Finally, the matching score
is calculated based on the global interaction. We
show that, after degenerated to the exact match-
ing scenario, Match-SRNN can approximate the
dynamic programming process of longest common
subsequence. Thus, there exists a clear interpre-
tation for Match-SRNN. Our experiments on two
semantic matching tasks showed the effectiveness
of Match-SRNN, and its ability of visualizing the
learned matching structure.

1 Introduction

Semantic matching is a critical task for many applications in
natural language processing, including information retrieval,
question answering and paraphrase identiﬁcation [Li and Xu,
2013]. The target of semantic matching is to determine a
matching score for two given texts. Taking the task of ques-
tion answering as an example, given a pair of question and an-
swer, a matching function is created to determine the match-
ing degree between these two texts. Traditional methods such
as BM25 and feature based learning models usually rely on
exact matching patterns to determine the degree, and thus suf-
fer from the vocabulary mismatching problem [Li and Xu,
2013].

Recently, deep learning approach has been applied to this
area and well tackled the vocabulary mismatching problem.
Some existing work focus on representing each text as one
or several dense vectors, and then calculate the matching
score based on the similarity between these vectors. Exam-
ples include RAE [Socher et al., 2011], DSSM [Huang et
al., 2013], CDSSM [Shen et al., 2014], ARC-I [Hu et al.,
2014], CNTN [Qiu and Huang, 2015], LSTM-RNN [Palangi
et al., 2015], MultiGranCNN [Yin and Sch¨utze, 2015a;
Yin and Sch¨utze, 2015b] and MV-LSTM [Wan et al., 2016].
However, it is usually difﬁcult for these methods to model the
complicated interaction relationship between two texts [Lu
and Li, 2013] because the representations are calculated inde-
pendently. To address the problem, some other deep methods
have been proposed to directly learn the interaction relation-
ship between the two texts, including DeepMatch [Lu and Li,
2013], ARC-II [Hu et al., 2014], and MatchPyramid [Pang
et al., 2016] etc. All these models conducts the matching
through a hierarchical matching structure: the global inter-
action between two texts is a composition of different levels
of the local interactions, such as word level and phrase level
interactions.

In all of these methods, the mechanism on the generation
of the complicated interaction relationship between two texts
is not clear, and thus lack of interpretability.
In this pa-
per, we propose to tackle the problem in a recursive man-
ner. Speciﬁcally, we view the generation of the global in-
teractions as a recursive process. Given two texts S1 =
{w1, w2, · · · , wm} and S2 = {v1, v2, · · · , vn}, the interac-
tion at each position (i, j) (i.e. interaction between S1[1:i]
and S2[1:j]) is a composition of the interactions between
three interactions, S1[1:i−1]∼S2[1:j],
their preﬁxes (i.e.
S1[1:i]∼S2[1:j−1], S1[1:i−1]∼S2[1:j−1]), and the word
level interaction at this position (i.e. the interaction between
wi and vj), where S[1:c] stands for the preﬁx consisting of
the previous c words of text S. Compared with previous hier-
archical matching structure, the recursive matching structure
can not only capture the interactions between nearby words,
but also take the long distant interactions into account.

Based on the above idea, we propose a novel deep architec-
ture, namely Match-SRNN, to model the recursive matching
structure. Firstly, a similarity tensor is constructed to capture
the word level interactions between two texts, where each el-
ement (cid:126)sij stands for a similarity vector between two words

 
 
 
 
 
 
from different texts. Then a spatial (2D) recurrent neural net-
work (spatial RNN) with gated recurrent units is applied to
the tensor. Speciﬁcally, the representation at each position
(cid:126)hij can be viewed as the interactions between the two pre-
ﬁxes, i.e. S1[1:i] and S2[1:j]. It is determined by four factors:
(cid:126)hi−1,j, (cid:126)hi,j−1, (cid:126)hi−1,j−1 and the input word level interaction
(cid:126)sij, depending on the corresponding gates, zt, zl, zd, and zi,
respectively. Finally, the matching score is produced by a
linear scoring function on the representation of the global in-
teraction (cid:126)hmn, obtained by the aforementioned spatial RNN.
We show that Match-SRNN can well approximate the dy-
namic programming process of longest common subsequence
(LCS) problem [Wikipedia, -]. Furthermore, our simulation
experiments show that a clear matching path can be obtained
by backtracking the maximum gates at each position, similar
to that in LCS. Thus, there is a clear interpretation on how the
global interaction is generated in Match-SRNN.

We conducted experiments on question answering and pa-
per citation tasks to evaluate the effectiveness of our model.
The experimental results showed that Match-SRNN can sig-
niﬁcantly outperform existing deep models. Moreover, to vi-
sualize the learned matching structure, we showed the match-
ing path of two texts sampled from the real data.

The contributions of this paper can be summarized as:
• The idea of modeling the mechanism of semantic match-
ing recursively, i.e. the recursive matching structure.
• The proposal of a new deep architecture, namely Match-
SRNN, to model the recursive matching structure. Ex-
perimental results showed that Match-SRNN can signif-
icantly improve the performances of semantic matching,
compared with existing deep models.

• The reveal of the relationship between Match-SRNN
i.e. Match-SRNN can reproduce the

and the LCS,
matching path of LCS in an exact matching scenario.

2 Related Work
Existing deep learning methods for semantic matching can be
categorized into two groups.

One paradigm focuses on representing each text

to a
dense vector, and then compute the matching score based
on the similarity between these two vectors. For example,
DSSM [Huang et al., 2013] uses a multi-layer fully connected
neural network to encode a query (or a document) as a vec-
tor. CDSSM [Shen et al., 2014] and ARC-I [Hu et al., 2014]
utilize convolutional neural network (CNN), while LSTM-
RNN [Palangi et al., 2015] adopts recurrent neural network
with long short term memory (LSTM) units to better rep-
resent a sentence. Different from above work, CNTN [Qiu
and Huang, 2015] uses a neural tensor network to model
the interaction between two sentences instead of using the
cosine function. With this way, it can capture more com-
plex matching relations. Some methods even try to match
two sentences with multiple representations, such as words,
phrases, and sentences level representations. Examples in-
clude RAE [Socher et al., 2011], BiCNN [Yin and Sch¨utze,
2015a], MultiGranCNN [Yin and Sch¨utze, 2015b], and MV-
LSTM [Wan et al., 2016].
In general, the idea behind the

approach is consistent with users’ experience that the match-
ing degree between two sentences can be determined once the
meanings of them being well captured. However, it is usually
difﬁcult for these methods to model the complicated inter-
action relationship between two texts, especially when they
have already been represented as a compact vector [Lu and
Li, 2013; Bahdanau et al., 2014].

The other paradigm turns to directly model the interaction
relationship of two texts. Speciﬁcally, the interaction is rep-
resented as a dense vector, and then the matching score can
be produced by integrating such interaction. Most existing
work of this paradigm create a hierarchical matching struc-
ture, i.e. the global interaction between two texts is generated
by compositing the local interactions hierarchically. For ex-
ample, DeepMatch [Lu and Li, 2013] models the generation
of the global interaction between two texts as integrating local
interactions based on hierarchies of the topics. MatchPyra-
mid [Pang et al., 2016] uses a CNN to model the generation
of the global interaction as an abstraction of the word level
and phrase level interactions. Deﬁning the matching struc-
ture hierarchically has limitations, since hierarchical match-
ing structure usually relies on a ﬁxed window size for com-
position, the long distant dependency between the local inter-
actions cannot be well captured in this kind of models.

3 The Recursive Matching Structure
In all existing methods, the mechanism of semantic matching
is complicated and hard to interpret. In mathematics and com-
puter science, when facing a complicated object, a common
method of simpliﬁcation is to divide a problem into subprob-
lems of the same type, and try to solve the problems recur-
sively. This is the well-known thinking of recursion. In this
paper, we propose to tackle the semantic matching problem
recursively. The recursive rule is deﬁned as follows.

two
Deﬁnition 1 (Recursive Matching Structure) Given
texts S1={w1, · · · , wm} and S2={v1, · · · , vn},
the in-
teraction between preﬁxes S1[1:i]={w1, · · · , wi} and
S2[1:j]={v1, · · · , vj} (denoted as (cid:126)hij) is composited by the
interactions between the sub-preﬁxes as well as the word
level interaction of the current position, as shown by the
following equation:

(cid:126)hij = f ((cid:126)hi−1,j, (cid:126)hi,j−1, (cid:126)hi−1,j−1, (cid:126)s(wi, vj)),

(1)

where (cid:126)s(wi, vj) stands for the interaction between words wi
and vj.

Figure 1 illustrates an example of the recursive match-
ing structure for sentences S1={The cat sat on the mat}
and S2={The dog played balls on the ﬂoor}.
Consider-
ing the interaction between S1[1:3]={The cat sat} and
S2[1:4]={The dog played balls} (i.e. (cid:126)h34),
the recursive
matching structure deﬁned above indicates that it is the com-
position of the interactions between their preﬁxes (i.e. (cid:126)h24,
(cid:126)h33, and (cid:126)h23) and the word level interaction between ‘sat’
and ‘balls’, where (cid:126)h24 stands for the interaction between
S1[1:2]={The cat} and S2[1:4]={The dog played balls}, (cid:126)h33
denotes the interaction between S1[1:3]={The cat sat} and

Figure 1: Illustration of the recursive matching structure.

interaction

interaction,

the most
between

important
S1[1:3]={The cat sat}

S2[1:3]={The dog played}, and (cid:126)h23 denotes the interaction
between S1[1:2]={The cat} and S2[1:3]={The dog played}.
i.e.
We can see that
the
and
S2[1:3]={The dog played}, has been utilized for represent-
ing (cid:126)h34, which consists well with the human understanding.
Therefore, it is expected that this recursive matching struc-
ture can well capture the complicated interaction relationship
between two texts because all of the interactions between
preﬁxes have been taken into consideration. Compared with
the hierarchical one, the recursive matching structure is able
to capture long-distant dependency among interactions.

4 Match-SRNN
In this section, we introduce a new deep architecture, namely
Match-SRNN, to model the recursive matching structure. As
shown in Figure 2, Match-SRNN consists of three compo-
nents: (1) a neural tensor network to capture the word level
interactions; (2) a spatial RNN applied on the word interac-
tion tensor to obtain the global interaction; (3) a linear scoring
function to obtain the ﬁnal matching score.

4.1 Neural Tensor Network
In Match-SRNN, a neural tensor network is ﬁrst utilized to
capture the basic interactions between two texts, i.e. word
level interactions. Speciﬁcally, each word is ﬁrst represented
as a distributed vector. Given any two words wi and vj, and
their vectors u(wi) and u(vj), the interaction between them
can be represented as a vector:

(cid:126)sij = F (u(wi)T T [1:c]u(vj) + W

(cid:21)

(cid:20)u(wi)
u(vj)

+ (cid:126)b),

where T i, i ∈ [1, ..., c] is one slice of the tensor parameters,
W and (cid:126)b are parameters of the linear part. F is a non-linear
function, and we use rectiﬁer F (z) = max(0, z) in this paper.
The interaction can also be represented as a similarity
score, such as cosine. We adopt neural tensor network here
because it can capture more complicated interactions [Socher
et al., 2013a; Socher et al., 2013b; Qiu and Huang, 2015].

4.2 Spatial RNN
The second step of Match-SRNN is to apply a spatial RNN
to the word level interaction tensor. Spatial RNN, also re-
ferred to as two dimensional RNN (2D-RNN), is a special
case of multi-dimensional RNN [Graves et al., 2007; Graves
and Schmidhuber, 2009; Theis and Bethge, 2015]. Accord-
ing to spatial RNN, given the representations of interac-
tions between preﬁxes S1[1:i−1]∼S2[1:j], S1[1:i]∼S2[1:j−1]

Figure 2: The architecture of Match-SRNN.

and S1[1:i−1]∼S2[1:j−1], denoted as (cid:126)hi−1,j, (cid:126)hi,j−1, and
(cid:126)hi−1,j−1,
the interaction between preﬁxes
S1[1:i] and S2[1:j] can be represented as follows:

respectively,

(cid:126)hij = f ((cid:126)hi−1,j, (cid:126)hi,j−1, (cid:126)hi−1,j−1, (cid:126)sij).

(2)

Therefore we can see that spatial RNN can naturally model
the recursive matching structure deﬁned in Equation (1).

For function f , we have different choices. The basic RNN
usually uses a non-linear full connection layer as f . This type
of function is easy for computing while often suffers from
the gradient vanishing and exploding problem [Pascanu et al.,
2013]. Therefore, many variants of RNN has been proposed,
such as Long Short Term Memory (LSTM) [Hochreiter and
Schmidhuber, 1997], Gated Recurrent Units (GRU) [Cho et
al., 2014] and Grid LSTM [Kalchbrenner et al., 2015]. Here,
we adopt GRU since it is easy to implement and has close
relationship with LCS as discussed in the following sections.
GRU is proposed to utilize several gates to tackle the afore-
mentioned problems of basic RNN, and has shown excellent
performance for tasks such as machine translation [Cho et
al., 2014]. In this paper, we extend traditional GRU for se-
quences (1D-GRU) to spatial GRU. Figure 3 describes clearly
about the extensions.

For 1D-GRU , given a sentence S=(x1, x2, · · · , xT ),
where (cid:126)xt stands for the embedding of the t-th words, the rep-
resentation of position t, i.e. (cid:126)ht, can be computed as follows:

(cid:126)z =σ(W (z)(cid:126)xt + U (z)(cid:126)ht−1), (cid:126)r=σ(W (r)(cid:126)xt + U (r)(cid:126)ht−1),
t =φ(W (cid:126)xt+U ((cid:126)r (cid:12) (cid:126)ht−1)), (cid:126)ht=((cid:126)1 − (cid:126)z) (cid:12) (cid:126)ht−1+(cid:126)z (cid:12) (cid:126)h(cid:48)
(cid:126)h(cid:48)
t,

is

the

where (cid:126)ht−1
representation of position t−1,
W (z), U (z), W (r), U (r), W and U are the parameters, (cid:126)z
is the updating gate which tries to control whether to propa-
gate the old information to the new states or to write the new
generated information to the states, and (cid:126)r is the reset gate
which tries to reset the information stored in the cells when
generating new candidate hidden states.

When extending to spatial GRU, context

information
will come from three directions for a given position (i, j),
i.e. (i−1, j), (i, j−1) and (i−1, j−1), therefore, we will have
four updating gates (cid:126)z, denoted as (cid:126)zl, (cid:126)zt, (cid:126)zd and (cid:126)zi, and three
reset gates (cid:126)r, denoted as (cid:126)rl, (cid:126)rt, (cid:126)rd. The function f is com-

Thedogplayedballsonthefloor.Thecatsatonthemat.Thecatsatonthemat.Thedogplayedballsonthefloor.S1[1:2]S1[1:3]S2[1:3]S2[1:4]WordInteractionTensorSpatialRNNLinearLayerS1S2where M (S1, S+
matching scores.

2 ) and M (S1, S−

2 ) are the corresponding

Figure 3: Illustration of Gated Recurrent Units. The left one
is 1D-GRU, where different hs are denoted as one node. The
right one is the spatial GRU used in this paper.

puted as follows.

ij]T ,

l , (cid:126)rT

i,j−1, (cid:126)hT

i−1,j, (cid:126)hT

i−1,j−1, (cid:126)sT

(cid:126)qT = [(cid:126)hT
(cid:126)rl = σ(W (rl)(cid:126)q + (cid:126)b(rl)), (cid:126)rt = σ(W (rt)(cid:126)q + (cid:126)b(rt)),
(cid:126)rd = σ(W (rd)(cid:126)q + (cid:126)b(rd)), (cid:126)rT = [(cid:126)rT
i = W (zi)(cid:126)q + (cid:126)b(zi), (cid:126)z(cid:48)
(cid:126)z(cid:48)
t = W (zt)(cid:126)q + (cid:126)b(zt), (cid:126)z(cid:48)
(cid:126)z(cid:48)
[(cid:126)zi, (cid:126)zl, (cid:126)zt, (cid:126)zd] = SoftmaxByRow([(cid:126)z(cid:48)
(cid:126)h(cid:48)
ij=φ(W (cid:126)sij + U ((cid:126)r (cid:12) [(cid:126)hT
(cid:126)hij=(cid:126)zl (cid:12) (cid:126)hi,j−1+(cid:126)zt (cid:12) (cid:126)hi−1,j+(cid:126)zd (cid:12) (cid:126)hi−1,j−1+(cid:126)zi (cid:12) (cid:126)h(cid:48)

t , (cid:126)rT
l = W (zl)(cid:126)q + (cid:126)b(zl),
d = W (zd)(cid:126)q + (cid:126)b(zd),
l, (cid:126)z(cid:48)
d]),
i−1,j−1]T ) + (cid:126)b),
ij,

i, (cid:126)z(cid:48)
i−1,j, (cid:126)hT

i,j−1, (cid:126)hT

d ]T ,

t, (cid:126)z(cid:48)

(3)

(4)

where U , W ’s, and b’s are parameters, and SoftmaxByRow
is a function to conduct softmax on each dimension across the
four gates, that is:

[(cid:126)zp]j =

e[ (cid:126)z(cid:48)

i]j + e[ (cid:126)z(cid:48)

p]j

e[ (cid:126)z(cid:48)
l]j + e[ (cid:126)z(cid:48)

t]j + e[ (cid:126)z(cid:48)

d]j

, p = i, l, t, d.

4.3 Linear Scoring Function
Since spatial RNN is a recursive model scanning the input
from left top to right bottom, we can obtain the last repre-
sentation as (cid:126)hmn at the right bottom corner. (cid:126)hmn reﬂects the
global interaction between the two texts. The ﬁnal matching
score can be obtained with a linear function:

M (S1, S2) = W (s)(cid:126)hmn + (cid:126)b(s),

(5)

where W (s) and (cid:126)b(s) denote the parameters.

4.4 Optimization
For different tasks, we need to utilize different loss functions
to train our model. Taking regression as an example, we can
use square loss for optimization:

L(S1, S2, y) = (y − M (S1, S2))2,
where y ∈ R is the real-valued ground-truth label to indicate
the matching degree between S1 and S2.

(6)

For ranking problem, we can utilize pairwise ranking loss
2 , S−
2 ),
2 ) is higher than

such as hinge loss for training. Given a triple (S1, S+
where the matching degree of (S1, S+
(S1, S−
L(S1, S+

2 ), the loss function is deﬁned as:

2 ) = max(0, 1 − M (S1, S+

2 ) + M (S1, S−

2 , S−

2 ))

All parameters of the model, including the parameters of
word embedding, neural tensor network, spatial RNN are
jointly trained by BackPropagation and Stochastic Gradient
Descent. Speciﬁcally, we use AdaGrad [Duchi et al., 2011]
on all parameters in the training process.

5 Discussion
In this section, we show the relationship between Match-
SRNN and the well known longest common subsequence
(LCS) problem.

5.1 Theoretical Analysis
The goal of LCS problem is to ﬁnd the longest subsequence
common to all sequences in a set of sequences (often just two
sequences). In many applications such as DNA detection, the
lengths of LCS are used to deﬁne the matching degree be-
tween two sequences.

Formally, given two sequences, e.g. S1={x1, · · · , xm}
let c[i, j] represents the length of
and S2={y1, · · · , yn},
LCS between S1[1:i] and S2[1:j]. The length of LCS be-
tween S1 and S2 can be obtained by the following recur-
sive progress, with each step c[i, j] determined by four fac-
tors, i.e. c[i−1, j−1], c[i−1, j], c[i, j−1], and the matching
between xi and yj.
c[i, j]= max(c[i, j−1], c[i−1, j], c[i−1, j−1] + I{xi=yj }),
(7)
where I{xi=yj } is an indicator function, it is equal to 1 if xi =
yj, and 0 otherwise. c[i, j]=0 if i=0 or j=0.

Match-SRNN has strong connection to LCS. To show
this, we ﬁrst degenerate the Match-SRNN to model an ex-
act matching problem, by replacing the neural tensor network
with a simple indicator function which returns 1 if the two
words are identical and 0 otherwise, i.e. sij=I{xi=yj }. The
dimension of spatial GRU cells is also set to 1. The reset gates
of spatial GRU are disabled since the length of LCS is accu-
mulated depending on all the past histories. Thus, Equation
(4) can be degenerated as

hij = zl · hi,j−1 + zt · hi−1,j + zd · hi−1,j−1 + zi · h(cid:48)

ij,
where zl · hi,j−1, zt · hi−1,j, and zd · hi−1,j−1 + zi · h(cid:48)
ij cor-
respond to the terms c[i, j−1], c[i−1, j], and c[i−1, j−1] +
I{xi=yj } in Equation (7), respectively.
Please note that
zl, zt, zd and zi are calculated by SoftmaxByRow, and thus
can approximate the max operation in Equation (7). By ap-
propriately setting zi and zd and other parameters of Match-
SRNN, zd · hi−1,j−1 + zi · h(cid:48)
ij can approximate the simple
addition operation hi−1,j−1+sij, where hi−1,j−1 and sij cor-
respond to the c[i−1, j−1] and I{xi=yj }, respectively. There-
fore, the computation of hij in Eq. (4) can well approximate
c[i, j] in Eq. (7).

5.2 Simulation Results
We conducted a simulation experiment to verify the analysis
result shown above. The dataset was constructed by many
random sampled sequence pairs, with each sequence com-
posed of characters sampled from the vocabulary {A B C D E

Match-SRNN.

QA dataset is collected from Yahoo! Answers, a commu-
nity question answering system where some users propose
questions to the system and other users will submit their an-
swers, as in [Wan et al., 2016]. The whole dataset contains
142,627 (question, answer) pairs, where each question is ac-
companied by its best answer. We select the pairs in which
questions and their best answers both have a length between
5 and 50. After that the dataset contains 60,564 (questions,
answer) pairs which form the positive pairs. For each ques-
tion, we ﬁrst use its best answer as a query to retrieval the top
1,000 results from the whole answer set, with Lucene search
engine. Then we randomly select 4 answers from them to
construct the negative pairs.

PC task is to match two papers with citation relationship.
The dataset is constructed as in [Pang et al., 2016]. The
paper abstract information and citation network are collected
from a commercial academic website. The negative pairs are
randomly sampled from the whole dataset. Finally, we have
280K positive and 560K negative instances.

6.1 Effectiveness of Match-SRNN
We compared Match-SRNN with several existing deep learn-
ing methods,
including ARC-I, ARC-II, CNTN, LSTM-
RNN, MultiGranCNN, MV-LSTM and MatchPyramid. We
also compared with BM25 [Robertson et al., 1995], which is
a popular and strong baseline for semantic matching in infor-
mation retrieval. For Match-SRNN, we also implemented the
bidirectional version for comparison, which also scans from
right bottom to left top on the word interaction tensor, denoted
as Bi-Match-SRNN.

In our experiments, we set the parameters and the base-
lines as follows. Word embeddings used in our model and in
some baseline deep models are all initialized by SkipGram
of Word2Vec [Mikolov et al., 2013]. Following the previ-
ous practice, word embeddings are trained on the whole ques-
tion answering data set, and the dimension is set to 50. The
batch size of SGD is set to 128. All other trainable parame-
ters are initialized randomly by uniform distribution with the
same scale, which is selected according to the performance
on validation set. The initial learning rates of AdaGrad are
also selected by validation. The dimension of neural tensor
network and spatial RNN is set to 10, because it won the best
validation results among the settings of d = 1, 2, 5, 10, and
20. The other parameters for the baseline methods are set by
taking the values from the original papers.

The QA task is formulated as a ranking problem. There-
fore, we use the hinge loss for optimization, as shown in
Section 4.4, and the results are evaluated by typical ranking
measures, such as Precision at 1 (denoted as P@1) and Mean
Reciprocal Rank (MRR).

P @1 =

1
N

N
(cid:88)

i=1

δ(r(S+(i)
2

) = 1), M RR =

1
N

N
(cid:88)

i=1

1
r(S+(i)
2

)

,

where N is the number of testing ranking lists, S+(i)
is the
positive sentence in the i − th ranking list, r(·) denotes the
rank of a sentence in the ranking list, and δ is the indicator
function. The PC task is formulated as a binary classiﬁcation

2

Figure 4: The simulation result of LCS by Match-SRNN.
Figure (a) shows the matching degree and path discovered
in LCS. Figure (b) shows the simulation results of Match-
SRNN. Figure (c) shows the backtracing process of ﬁnding
the gray path in Match-SRNN.

F G H I J}. Firstly, the dynamic programming algorithm of
LCS was conducted on each sequence pair, and the normal-
ized length of LCS is set to be the matching degree of each se-
quence pair. For simulation, we split the data into the training
(10000 pairs) and testing set (1000 pairs), and trained Match-
SRNN with regression loss. The simulation results on two
sequences S1 = {A, B, C, D, E} and S2 = {F, A, C, G, D}
are shown in Figure 4.

Figure 4 (a) shows the results of LCS, where the scores at
each position (i, j) stands for c[i, j], and the gray path indi-
cates the process of ﬁnding the LCS between two sequences,
which is obtained by backtracing the dynamic programming
process. Figure 4 (b) gives the results of Match-SRNN, where
the score at each position (i, j) stands for the representation
(cid:126)hij (please note that for simpliﬁcation the dimension of (cid:126)hij
is set to 1). We can see that the scores produced by Match-
SRNN is identical to that obtained by LCS, which reveals the
relationship between Match-SRNN and LCS.

The gray path in Figure 4 (b) shows the main path of
how local interactions are composited to the global inter-
action, which is generated by backtracing the gates. Fig-
ure 4 (c) shows the path generation process, where the
three values at each positions stands for the three gates,
e.g. zl=0.9, zt=0.1, zd=0 at position (5, 5). Considering the
last position (5, 5), the matching signals are passed over from
the direction with the largest value of gates, i.e. zl, therefore,
we move to the position (5, 4). At position (5, 4), the largest
value of gates is zd=0.7, therefore, we should move to posi-
tion (3, 3). We can see that the path induced by Match-SRNN
is identical to that of by dynamic programming. This analy-
sis gives a clear explanation on the mechanism of how the
semantic matching problem be addressed by Match-SRNN.

6 Experiments

We conducted experiments on the tasks of question answering
(QA) and paper citation (PC) to evaluate the effectiveness of

0.00.00.00.00.01.01.01.01.00.91.01.02.12.12.01.01.02.12.02.01.01.02.03.13.1ABCDE0000011111112221122211233FACGDABCDE0.80.00.0𝑧"𝑧#𝑧$(a)(c)(b)FACGDBCDEAAFCDG0.00.10.80.80.00.00.00.90.10.70.10.10.00.10.9Model
Random Guess
BM25
ARC-I
CNTN
LSTM-RNN
MultiGranCNN
MV-LSTM
ARC-II
MatchPyramid-Tensor
Match-SRNN
Bi-Match-SRNN

QA
P@1 MRR
0.457
0.200
0.726
0.579
0.756
0.581
0.781
0.626
0.822
0.690
0.840
0.725
0.869
0.766
0.765
0.591
0.867
0.764
0.879
0.785
0.882
0.790

PC
Acc
0.500
0.832
0.845
0.862
0.878
0.885
0.890
0.865
0.894
0.898
0.901

Table 1: Experimental results of QA and PC tasks.

task. Therefore the matching score is used by a softmax layer
and cross entropy loss is used for training. We use classiﬁca-
tion accuracy (Acc) as the evaluation measure.

The experimental results are listed in Table 1. We have the

following experimental ﬁndings:

(1) By modeling the recursive matching structure, Match-
SRNN can signiﬁcantly improve the performances, compared
with all of the baselines. Taking QA task as an example, com-
pared with BM25, the improvement is about 36.4% in terms
of P@1. Compared with MV-LSTM, the best one among
deep learning methods focusing on learning sentence repre-
sentations, the improvement is about 3.1%. Compared with
the deep models using hierarchical composition structures
(i.e. ARC-II and MatchPyramid), the improvements are at
least 3.4%. For PC task, Match-SRNN also achieves the best
results, though the improvements are smaller as compared to
those on QA task. This is because the task is much easier,
and even simple model such as BM 25 can produce a good
result. From the above analysis, we can see that the recursive
matching structure can help to improve the results of semantic
matching.

(2) Both of the two matching paradigms (representing text
into dense vectors and modeling the interaction relation-
ship) have their own advantages, and the results are com-
parable, e.g. the previous best results of the two paradigms
on QA dataset are 0.766/0.869 (MV-LSTM) and 0.764/0.867
(MatchPyramid).

6.2 Visualization
To show how Math-SRNN works and give an insight on its
mechanism on real dataset, we conducted a case study to vi-
sualize the interactions generated by Match-SRNN.

The example sentences are selected from the testing set of

QA dataset.

Question: “How to get rid of memory stick error of my

sony cyber shot?”

Answer: “You might want to try to format the memory

stick but what is the error message you are receiving.”

We can see that in this example, the matching of a bigram
(memory, stick) and a keyword (error) is important for cal-
culating the matching score. In this experiment, we used a
simpliﬁed version Match-SRNN to give a better interpreta-

Figure 5: A representative interaction learned by Match-
SRNN, where the brightness is dependent on the interaction
value at each position, and the path in red denotes the infor-
mation diffussion process generated by backtracing the max-
imum gates.

tion. Speciﬁcally, we set the values of different dimensions in
the gates to be identical, which is convenient for the backtrac-
ing process. Since the hidden dimension is set to 10, as used
in the above Match-SRNN, we can obtain 10 values for each
(cid:126)hij. We choose to visualize the feature map of the dimension
with the largest weight in producing the ﬁnal matching score.
Similar visualization can be obtained for other dimensions,
and we omit them due to space limitation.

The visualization results are shown in Figure 5, where the
brightness of each position stands for the interaction strength.
We can see that the recursive matching structure can be
shown clearly. When there is a strong word level interac-
tion happened in some position (e.g., the exact word match of
(memory, memory)), the interaction between the two texts
are strengthened and thus the bottom-right side of the position
becomes brighter. The interactions are further strengthened
with more strong word level interactions, i.e., the bottom-
right side of the matching positions of (stick, stick) and
(error, error) become even brighter. Backtracing the gates,
we obtain the matching path which crosses all the points with
strong word interactions, as shown by red curve in Figure 5. It
gives a clear interpretation on how Match-SRNN conducted
the semantic matching on this real example.

7 Conclusions
In this paper, we propose a recursive thinking to tackle
the complicated semantic matching problem. Speciﬁcally,
a novel deep learning architecture, namely Match-SRNN is
proposed to model the recursive matching structure. Match-
SRNN consists of three parts: a neural tensor network to ob-
tain the word level interactions, a spatial RNN to generate
the global interactions recursively, and a linear scoring func-
tion to output the matching degree. Our analysis reveals an
interesting connection of Match-SRNN to LCS. Finally, our
experiments on semantic matching tasks showed that Match-
SRNN can signiﬁcantly outperform existing deep learning
methods. Furthermore, we visualized the recursive matching
structure discovered by Match-SRNN on a real example.

Acknowledgments
This work was funded by 973 Program of China under Grants
No. 2014CB340401 and 2012CB316303, 863 Program of
China under Grants No. 2014AA015204, the National Nat-
ural Science Foundation of China (NSFC) under Grants No.
61232010,61472401, 61433014, 61425016, 61203298, and
61572473, Key Research Program of the Chinese Academy
of Sciences under Grant No. KGZD-EW-T03-2, and Youth
Innovation Promotion Association CAS under Grants No.
20144310 and 2016102.

References
[Bahdanau et al., 2014] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio.
Neural machine transla-
tion by jointly learning to align and translate. CoRR,
abs/1409.0473, 2014.

[Cho et al., 2014] Kyunghyun Cho, Bart van Merrienboer,
C¸ aglar G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using
RNN encoder-decoder for statistical machine translation.
CoRR, abs/1406.1078, 2014.

[Duchi et al., 2011] John Duchi, Elad Hazan, and Yoram
Singer. Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 12:2121–2159, 2011.
[Graves and Schmidhuber, 2009] Alex Graves and J¨urgen
Schmidhuber. Ofﬂine handwriting recognition with mul-
tidimensional recurrent neural networks. In NIPS, pages
545–552, 2009.

[Graves et al., 2007] Alex Graves, Santiago Fern´andez, and
J¨urgen Schmidhuber. Multi-Dimensional Recurrent Neu-
ral Networks. CoRR, abs/0705.2, 2007.

[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and
J¨urgen Schmidhuber. Long Short-Term Memory. Neural
computation, 9(8):1735–1780, 1997.

[Hu et al., 2014] Baotian Hu, Zhengdong Lu, Hang Li, and
Qingcai Chen. Convolutional neural network architectures
for matching natural language sentences. In NIPS, pages
2042–2050, 2014.

[Huang et al., 2013] Po-Sen Huang, Xiaodong He, Jianfeng
Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep
structured semantic models for web search using click-
through data. In CIKM, pages 2333–2338, 2013.

[Kalchbrenner et al., 2015] Nal Kalchbrenner,

Ivo Dani-
helka, and Alex Graves. Grid Long Short-Term Memory.
CoRR, abs/1507.01526, 2015.

[Li and Xu, 2013] Hang Li and Jun Xu. Semantic Match-
ing in Search. Foundations and Trends in Information Re-
trieval, 7(5):343–469, 2013.

[Lu and Li, 2013] Zhengdong Lu and Hang Li. A Deep Ar-
chitecture for Matching Short Texts. In NIPS, pages 1367–
1375, 2013.

[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai
Chen, Greg S Corrado, and Jeff Dean. Distributed rep-
resentations of words and phrases and their composition-
ality. In NIPS, pages 3111–3119, 2013.

[Palangi et al., 2015] Hamid Palangi, Li Deng, Yelong Shen,
Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song,
and Rabab K. Ward. Deep sentence embedding using
the long short term memory network: Analysis and ap-
plication to information retrieval. CoRR, abs/1502.06922,
2015.

[Pang et al., 2016] Liang Pang, Yanyan Lan, Jiafeng Guo,
Jun Xu, Shengxian Wan, and Xueqi Cheng. Text Matching
as Image Recogonition. In AAAI, 2016.

[Pascanu et al., 2013] Razvan Pascanu, Tomas Mikolov, and
Yoshua Bengio. On the difﬁculty of training recurrent neu-
ral networks. In ICML, pages 1310–1318, 2013.

[Qiu and Huang, 2015] Xipeng Qiu and Xuanjing Huang.
Convolutional Neural Tensor Network Architecture for
Community-Based Question Answering. In IJCAI, pages
1305–1311, 2015.

[Robertson et al., 1995] Stephen E Robertson, Steve Walker,
Susan Jones, Micheline M Hancock-Beaulieu, Mike Gat-
ford, and Others. Okapi at TREC-3. NIST SPECIAL PUB-
LICATION SP, 1995.

[Shen et al., 2014] Yelong Shen, Xiaodong He, Jianfeng
Gao, Li Deng, and Gregoire Mesnil. A Latent Semantic
Model with Convolutional-Pooling Structure for Informa-
tion Retrieval. In CIKM, pages 101–110, 2014.

[Socher et al., 2011] Richard Socher, Eric H Huang, Jeffrey
Pennington, Andrew Y Ng, and Christopher D Manning.
Dynamic Pooling and Unfolding Recursive Autoencoders
for Paraphrase Detection. In NIPS, pages 801–809, 2011.
[Socher et al., 2013a] Richard Socher, Danqi Chen, Christo-
pher D Manning, and Andrew Ng. Reasoning with neural
tensor networks for knowledge base completion. In NIPS,
pages 926–934, 2013.

[Socher et al., 2013b] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. Recursive deep
models for semantic compositionality over a sentiment
treebank. In EMNLP, pages 1631–1642, 2013.

[Theis and Bethge, 2015] Lucas Theis and Matthias Bethge.
Generative image modeling using spatial LSTMs. In NIPS,
pages 1918–1926, 2015.

[Wan et al., 2016] Shengxian Wan, Yanyan Lan, Jiafeng
Guo, Jun Xu, Liang Pang, and Xueqi Cheng. A Deep Ar-
chitecture for Semantic Matching with Multiple Positional
Sentence Representations. In AAAI, 2016.

[Wikipedia, -] Wikipedia. https://en.wikipedia.org/wiki/

Longest common subsequence problem.
[Yin and Sch¨utze, 2015a] Wenpeng Yin

and Hinrich
Sch¨utze. Convolutional Neural Network for Paraphrase
Identiﬁcation. In NAACL, pages 901–911, 2015.

[Yin and Sch¨utze, 2015b] Wenpeng Yin

and Hinrich
Sch¨utze. MultiGranCNN: An Architecture for Gen-
eral Matching of Text Chunks on Multiple Levels of
Granularity. In ACL, pages 63–73, 2015.

