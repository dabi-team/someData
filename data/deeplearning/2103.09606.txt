1
2
0
2

r
a

M
7
1

]
L
C
.
s
c
[

1
v
6
0
6
9
0
.
3
0
1
2
:
v
i
X
r
a

Code Word Detection in Fraud Investigations using a
Deep-Learning Approach

Y. van der Zee, J.C. Scholtes, M. Westerhoud and J. Rossi

March 2021

Abstract

In modern litigation, fraud investigators often face an overwhelming number of documents that
must be reviewed throughout a matter. In the majority of legal cases, fraud investigators do not
know beforehand, exactly what they are looking for, nor where to ﬁnd it. In addition, fraudsters may
use deception to hide their behaviour and intentions by using code words. Eﬀectively, this means
fraud investigators are looking for a needle in the haystack without knowing what the needle looks
like.

As part of a larger research program, we use a framework to expedite the investigation process
applying text-mining and machine learning techniques. We structure this framework using three
well-known methods in fraud investigations: (i) the fraud triangle (ii) the golden (”W”) investigation
questions, and (iii) the analysis of competing hypotheses. With this framework, it is possible to
automatically organize investigative data, so it is easier for investigators to ﬁnd answers to typical
investigative questions.

In this research, we focus on one of the components of this framework: the identiﬁcation of
the usage of code words by fraudsters. Here for, a novel (annotated) synthetic data set is created
containing such code words, hidden in normal email communication. Subsequently, a range of machine
learning techniques are employed to detect such code words. We show that the state-of-the-art BERT
model signiﬁcantly outperforms other methods on this task. With this result, we demonstrate that
deep neural language models can reliably (F1 score of 0.9) be applied in fraud investigations for the
detection of code words.

1 Background

In fraud investigations, investigators have to deal with ever increasing mass of unstructured data. These
data can be a valuable source of information and sometimes even direct evidence in relation to the
matter that is investigated. More often than not, these investigations are supported by eDiscovery tools.
The development of AI-techniques to enhance these tools is primarily aimed at isolated topics, such as
sentiment and emotion analysis, assisted review (searching using machine learning), or Named Entity
Recognition (NER). Although these techniques as such are very promising, there is no direct link to the
way an investigator approaches a fraud investigation.

In this paper we present a model that grants AI-techniques a logical role in a fraud investigation. For
this, let us ﬁrst have a look how a typical investigator approaches an investigation. This can be done
by examining three building blocks that provide a basis where we can ‘plug in’ an AI-technique and use
the outcome as a diagnostic variable in the investigated case. These building blocks are: (i) the Fraud
Triangle (Cressey, 1951) (Kassem & Higson, 2012), (ii) the six (6) ‘Golden’ investigation questions and
(iii) the Theory of the Analysis of Competing Hypotheses (Heuer, 1999). With these building blocks we
can deconstruct a (partial) investigation question into a number of tasks that can each be executed by a
speciﬁc search, text-mining or machine-learning algorithm. Let us ﬁrst explain these three components
individually, and then explain how we combine them.

1.1 Fraud Triangle

A widely used method to model organizational fraud risk is the fraud triangle, visualized in ﬁgure 1.
Just as ﬁre requires fuel, oxygen and a spark, in the case of a fraud there are also three ingredients
are essential: The perpetrator must have a motive to commit fraud, the situation must provide an
opportunity, and the fraudster must ﬁnd a way for herself to rationalize her dishonesty. Motives can
vary from perverse ﬁnancial incentives to personal problems, such as ﬁnancial need or addiction. All these

1

 
 
 
 
 
 
Figure 1: The Fraud Triangle with relevant investigation questions

can be referred to as pressure. The opportunity is often related to the control environment of the victim
organization: weak controls and tone at the top. And ﬁnally the rationalization relates to the perceived
relation between the fraudster and his environment. This relation provides the internal justiﬁcation of a
fraud: ‘I was mistreated’, ‘everybody does it’, etc.

1.2 Six (6) Golden Investigation Questions

Usually the fraud triangle is used as a risk tool. But, we can also use the model as part of our investigation
framework. To do this, we propose a relationship between the three (3) edges of the fraud triangle and
the six golden questions that lie at the basis of almost every fraud investigation: who, why, what,
how, when and where. Answering these question will almost automatically lead to the construction of
a possible fraud scenario and ﬁll the elements of an evidence matrix. If one is in need to know what
the motives of a fraudster are, one needs to know who did it and why. If one needs to know about
possible fraud opportunities, questions about the what and how need to be answered. And ﬁnally, for
the rationalization component of the fraud triangle, situational variables are important, in particular:
where and when.

Motive

Opportunity

Rationalisation

Who did it?
Why did she do it?
What happened?
How did he do it
Where did it happen?
When did it happen?

Table 1: Combining the Fraud Triangle (left side of table) with the Golden Investigation Questions (right
side of table)

Answers to (variations of) these questions produce evidence items that can populate elements of the

evidence matrix.

1.3 Analysis of Competing Hypotheses (ACH)

For each type of crime, a so-called evidence matrix can be constructed holding key items to be proved.
For instance, in case of a murder one needs a victim, a murder weapon, a motive, a crime scene, intent,
etc. These items relate to the above mentioned Golden Investigation Questions. Instead of using a simple
numeration of such items, we can use a more advanced model of an evidence matrix as developed in the

2

1970’s by Richard Heuer (Heuer, 1999). This methodology was named: Analysis of Competing Hypotheses
(ACH). It is based on the evaluation of various competing hypotheses, given a set of information items
(i.e. evidence). This involves the following step-by-step approach:

Step-by-Step Outline of Analysis of Competing Hypotheses:

• Identify the possible hypotheses to be considered. Use a group of analysts with diﬀerent perspectives

to brainstorm the possibilities.

• Make a list of signiﬁcant evidence and arguments for and against each hypothesis.

• Prepare a matrix with hypotheses across the top and evidence down the side. Analyze the ”diag-
nosticity” of the evidence and arguments–that is, identify which items are most helpful in judging
the relative likelihood of the hypotheses.

• Reﬁne the matrix. Reconsider the hypotheses and delete evidence and arguments that have no

diagnostic value.

• Draw tentative conclusions about the relative likelihood of each hypothesis. Proceed by trying to

disprove the hypotheses rather than prove them.

• Analyze how sensitive your conclusion is to a few critical items of evidence. Consider the conse-
quences for your analysis if that evidence were wrong, misleading, or subject to a diﬀerent inter-
pretation.

• Report conclusions. Discuss the relative likelihood of all the hypotheses, not just the most likely

one.

• Identify milestones for future observation that may indicate events are taking a diﬀerent course

than expected.

Figure 2: Visualization of the ACH Matrix. I Inconsistent, II Strongly inconsistent, C Consistent, CC
Strongly consistent, NA Not applicable

The ‘weighted inconsistency score’ provides a measure for the plausibility of a speciﬁc hypothesis,
given a set of evidence items in terms of credibility and relevance. Lower values of the scores, correspond
with a lower plausibility of the hypothesis. The numerical values are determined based on a simple lookup
table. These initial values do not represent probabilities, but they can be normalized towards a [0-1] range,
giving a normalized conﬁdence score. Combining conﬁdence scores can be done by multiplication. There
are obvious issues with this approach, as the use multiplication in the calculations presumes complete
independence of the underlying hypothesis, which is oﬀ course not always the case.
In addition, the
values are manually assigned, which leads to bias risks. But for now, this is what is used.1

1See: http://www.pherson.org/PDFFiles/ACHTechnicalDescription.pdf for a technical discussion on the use of ’weighted

inconsistency scores’

3

Now we have a conceptual model we can systematically inject the results of a various set of AI-methods
into. For example in a case of the investigation of a possible purchasing scheme. Typical for this scheme
is the incidence of collusion between perpetrators.

Several of the evidence components listed above, can be ﬁlled automatically with possible candidates
using basic text mining techniques. For example: Named Entity Extraction in combination with basic
linguistic contextual analysis can provide candidates for the Who, Where, and When questions. Sentiment
and emotion mining can identify the textual sections containing providing valuable insights into Why
something is done and Who is driving the actions. On-going research focuses on the creation a further
mapping between fraud types being investigated, related questions to populate the scenario’s and AI-
methods that can be applied to generate evidence items in answering these questions. But initial results
are promising as is described in (Scholtes, 2020) and (Scholtes & Herik, 2021).

Often these accomplices use code words in their communications. Finding these, is actually the
beginning of a fraud investigation and an essential ﬁrst step to truly understand the meaning of extracted
information in the context of a crime investigation. For example, if the suspects communicate about
”players” instead of ”employees that can be bribed” and about ”how much coﬀee did you give him”
referring to monetary compensation. It is essential to understand such code words before one is truly
able to ﬁll the ACH-matrix with possible candidates.

Therefore, we are interested in the use of text-mining and machine learning techniques for the auto-

matic and domain-independent detection of code words.

This element of the scenario can be constructed as follows when added to the ACH-matrix:

AI-method:
code word detection

Question:
Who is likely to be involved in the scheme?
Result:
Use of code words between employee A and employee B and supplier C

Entering this result as an evidence item in an ACH-matrix would look like this:

Credibility Relevance

H1:
A is acting alone

H2:
A and B are colluding

Use of code words
between A, B and C

High

Medium

I

CC

An additional goal of this research is to avoid the need for speciﬁc domain or world knowledge
when detecting code words: we wish to recognize any type of code words in any kind of context. We
do understand that there will be a language dependency (is communication for instance in English or in
German?), but by using general language models, we wish to avoid the need for speciﬁc domain-dependent
language models.

2 Introduction

The ability to model the context of a text is vital in fraud investigations, especially for code-word detec-
tion. The ability to properly model this context has greatly advanced in recent years due to the successful
advances using deep-learning algorithms for highly context sensitive Natural Language Processing (NLP)
tasks such as machine translation, human-machine dialogues or co-reference and pronoun resolution. For
this reason, we are interested to investigate the application of these techniques towards the detection of
code word detection by bench marking their performance against existing baseline models.

The above mentioned progress mainly originates from the development of the so-called Transformer
architecture. Transformer models are based on large pre-trained networks that already embed signiﬁcant
linguistic knowledge and which can be ﬁne-tuned on speciﬁc tasks requiring a relatively small amount of
additional training. Let us explain how this is done:

• A fundamental beneﬁt of the transformer architecture is the ability to perform Transfer Learning.
Traditionally, deep learning models require a large amount of task-speciﬁc training data in order to
achieve a desirable performance (billions of data points required to ﬁne tune hundreds-of-millions
neural interconnections). However, for most tasks, we do not have the amount of labeled training
data required to train these networks. By pre-training with large sets of natural text, the model
learns a signiﬁcant amount of task-invariant information on how language is constructed. With all

4

this information already contained in these models, we can focus our training process on learning
the patterns that are speciﬁc for the task at hand. We will still require more data points than
required in most statistical models (typically 50-100k based on our experience in earlier NLP Deep-
Learning projects), but not as much as the billions required, should we start the training of the
deep-learning models from scratch.

• Transformers are able to model a wide scope of linguistic context, both depending on previous
words, but also on (expected) future words. They are, so to say, more context sensitive than
models that can only use past-context into consideration. In addition, this context is included in
the embedding vectors, which allows for a richer representation and more complex linguistic tasks.

By the end of 2018, researchers at Google AI Language released a new model named the Bidirectional
Encoder Representations from Transformers (BERT) (Devlin et al., 2018). In the subsequent months,
BERT models achieved impressive benchmark performance on most downstream NLP tasks. Currently,
BERT is considered to be the state-of-the-art language representation model. For this reason, we aim to
apply BERT to the task of code word detection.

3 Related Work

Published research on code-word detection in criminal investigations is sparse. Jabbari et al. (2008)
proposes a distributional model of word usage and word meaning, derived purely from a corpus of the
text. This model is applied to determine whether certain words are used out of context. To be more
speciﬁc, the authors generate four diﬀerent test sets, choosing one speciﬁc word for each test set and
selecting 500 sentences containing the chosen word. These sentences are considered to represent the
normal meaning of the word. Next, the authors substitute a second, unrelated word for the chosen word
in 500 other sentences to artiﬁcially create examples of word obscurities. The authors show that a certain
understanding of the context is captured by the distributions, and that this understanding is valuable in
detecting obfuscated words. No overall quantitative results are reported.

In the work of Fong et al. (2008), the authors follow a similar approach by randomly sampling sentences
from the Enron data set and replacing the ﬁrst noun in each sentence with a diﬀerent noun. On a test set
of a few hundred samples, the authors achieve a detection ratio of 83% and 90% on two other synthetic
data sets.

Two more recent works (Magu et al., 2017; Magu & Luo, 2018), focus on the detection of euphemistic
hate speech on social media platforms. Euphemistic hate speech is distinct from other forms of implicit
hate speech because in reality they are often direct poisonous attacks as opposed to veiled or context-
dependent attacks. They are implicit because they use clever word substitutions in the language to avoid
detection. The authors use word embeddings and network analysis to identify these word euphemisms.
Here too, no overall quantitative results are reported.

To our knowledge, no work has been done on detecting code words using large pre-trained deep-

learning models such as BERT.

4 Method

We follow a similar methodology as Fong et al. (2008). However, we make the necessary adjustments
to generate signiﬁcantly more samples that we can use as training data compared to their work. As
explained earlier, we anticipate to need for about 50k training data points. As a starting point, we
randomly sample the content of emails from the ENRON data set. Here, we limit our attention to the
substitution of nouns since they are the more likely candidates to be substituted by code words. This
then results in a sentence which contains one word that is out of context. Consider the example: it is
not diﬃcult to imagine a scenario in which colleagues use a nickname or code word for their oﬃce, such
as ”the rock”. This will result in the following substitutions in their email communication: I will be out
of the oﬃce on Friday. Here, we take the noun oﬃce and replace it with a diﬀerent noun, for example,
rock. The result is I will be out of the rock on Friday, a sentence in which the word rock is an outlier
concerning the rest of the words. These are precisely the kind of examples our models are supposed to
identify.

We use the methodology described above to generate a data set that can be used to train and evaluate
diﬀerent machine learning techniques. A second synthetic data set is used to investigate the extent to
which the trained models can be used in practice, with the aim of testing the approaches in a more

5

realistic scenario. In this scenario, three types of drugs are mentioned, namely cocaine, marijuana and
heroin. Individuals communicating have the goal to hide the fact that they are talking about an illegal
drug. Therefore, they use code words to describe the drugs. We believe this scenario is more realistic
than the ﬁrst synthetic data set because in this scenario individuals speciﬁcally avoid certain terminology
in their communication. To create realistic code words, we use a list of Drug Slang words provided by
the U.S. Drug Enforcement Administration (DEA) 2. As an example, imagine the following text message;
I’m about to buy some cocaine for our party tonight; see you there. We replace the instance of the word
cocaine with snow, one of the code words for cocaine, resulting in the following sentence; I’m about to
buy some snow for our party tonight; see you there. Ideally, our trained models will identify this sentence
as an example of code word usage, since the word snow does not belong in this context.

4.1 Synthetic Data sets

4.1.1 ENRON Code word data set

The ENRON data set3 was made public as a result of the prosecution of ENRON personnel. It contains
about half a million emails to and from ENRON employees over three and a half years. The authors of
the emails never expected them to be made public, so it is an excellent example of informal writing. The
data set contains emails from a large number of authors, from many backgrounds. As such, it is a good
surrogate for the type of messages that could be intercepted in a fraud investigation.

We randomly sample body messages of emails from the data set and evaluate each sentence individ-
ually. We only consider sentences containing between ﬁve and twenty words and apply part-of-speech
tagging to verify if the sentence contains a noun. If the sentence is of the desired length and contains a
noun, it will be added to our pool of candidate sentences. We keep sampling until the candidate pool
consists of 60,000 sentences. For half of the candidate sentences, we replace the ﬁrst noun with a noun
taken from the BNC noun list4. To ensure that the models do not over-ﬁt on this limited list of nouns,
we split the BNC noun list into train, validation, and test subsets and sample a substitute word from the
appropriate sub-list. We complete the train, validation, and test set with the other half of the candidate
sentences, in which no words have been replaced. These sentences will serve as negative samples. The
training and validation sets consist of 48,000 and 6,000 samples, respectively, with equal samples from
each class. The test set consists of 6,000 samples, but here we introduce a class of imbalance to make the
setting more like a typical eDiscovery data set. The set contains 400 positive samples, which means that
only 5% of the samples contain a code word.

4.1.2 Reddit Drugs Data set

To create the Reddit Drugs Data set, we extract user-submitted Reddit comments from the subreddit
/r/worldnews. The PushShift API5 is utilized to gather comments before April 2020. As with the
selection of candidate sentences for the ENRON Code word data set, we select candidate sentences of
between ﬁve and twenty words. To make sure the sentence originates from the English language, we use
the Spacy language detector6 to detect the language of the comment. The ﬁrst 600 sentences that meet
these criteria are added to our data set and will act as our negative samples.

The same process follows, only this time, we use PushShift’s search function to search for comments
that mention three pre-selected drugs. The chosen drugs are Cocaine, Marijuana, and Heroin. The same
pre-selection is applied to each sentence of each comment, and if the sentence contains one of the drugs,
the sentence is added to the candidate list. Each sentence is then manipulated by replacing each mention
of all three drugs with a code word. The code words chosen for each drug are shown in Table 2. Each
modiﬁed sentence is added to the data set as a positive example. The result is a balanced data set of
1200 samples with 600 samples of each class.

2https://ndews.umd.edu/sites/ndews.umd.edu/ﬁles/dea-drug-slang-code-words-may2017.pdf
3https://www.cs.cmu.edu/ ./enron/
4The British National Corpus (BNC) was originally created by Oxford University press in the 1980s - early 1990s, and
it contains 100 million words of text texts from a wide range of genres (e.g. spoken, ﬁction, magazines, newspapers, and
academic). https://www.word frequency.info/compare bnc.asp

5https://github.com/pushshift/api
6https://spacy.io/universe/project/spacy-langdetect

6

Meaning Code word in text
Cocaine
Line
Marijuana Bush
Pure
Heroin

Table 2: The selected code words for the Reddit Drugs data set.

4.2 Models

In this research, we will benchmark four commonly used NLP models on the code word detection task.
The four selected methods are Bag of Words (BoW), TF-IDF, BiLSTM initialized with GloVe embeddings
and BERT. With these selected models, we transition from shallow lexical approaches to dense contextual
text representations. BERT was mainly chosen over other contextual methods like ELMo (Peters et al.,
2018) as it is considered state-of-the-art on most downstream NLP tasks. In addition, we prefer BERT
over ELMo because the use of sub words allows the former to better deal with out-of-vocabulary words.
We benchmark each of the models’ performance to determine the importance of contextual representations
on the task of code word detection.

Both the BoW and TF-IDF method are used in combination with a Logistic Regression classiﬁer. Both
methods consider unigrams, bigrams and trigrams and ignore terms that have a frequency lower than
three. The deep learning models are implemented using the PyTorch framework (Paszke et al., 2019).
The BiLSTM model uses an embeddings layer that is initialized with pre-trained GloVe embeddings. The
model is trained on binary cross-entropy loss until convergence using the Adam optimizer (Kingma &
Ba, 2014). The BERT implementation uses the bert-base-uncased conﬁguration from the hugging face
transformers package7. This speciﬁc model was preferred over other models due to its size. All the
encoder layers are updated during training. The ﬁne-tuning of BERT is done using the standard settings
for ﬁne-tuning, namely training for ten epochs and updating the model parameters using the Adam
optimizer with a learning rate of 2e − 5 and (cid:15) = 1e − 8.

4.3 Evaluation Metrics

For this task, the metrics used are the standard Accuracy, Precision, Recall, and F1 scores. In addition,
we assess the results on the unbalanced data set using macro-averages. Macro-averaging computes the
metrics independently for each class and then take the average, thus treating all classes equally. We select
macro-averaging over micro-average, where the contributions of all classes are aggregated to compute the
average metric, as we are primarily interested in the performance of the models on the code word class.
Lastly, with this interest in mind, we evaluate the models on the class-speciﬁc precision and recall of the
code word class (C1 Precision & C1 Recall). Our primary interest is to achieve high recall on this class,
as we want to recover all samples that may contain a code word. Beyond that, it is also essential to
achieve reasonable precision on the positive class, because otherwise the user will have to navigate a sea
sheet positively.

5 Results

Table 3 shows the performance of the selected models on the validation set. As discussed, this set is
generated using the same methodology as the training set, only using a diﬀerent set of nouns to replace
with. We observe that both lexical approaches perform signiﬁcantly better than a naive random classiﬁer.
This is a somewhat surprising result as one would expect that these models do not model the essential
context needed for this task. It seems that for a number of examples, the lexical approaches provide
suﬃcient information to diﬀerentiate between the use of code words for some of the samples.

We see the ﬁrst signiﬁcant performance increase with the Bi-LSTM model. This performance increase
can be attributed to the use of pre-trained word embeddings. As discussed, word embeddings are learned
by predicting words given the surrounding context. This task is related to our task in a big way, as we
want to ﬁnd words that do not belong in a given context. Since word embeddings capture information
about which words are being used together, they help us ﬁnd better examples of sentences using one
word from the context.

7https://github.com/huggingface/transformers

7

Model
Random 0.50
0.63
BoW
TF-IDF
0.63
Bi-LSTM 0.80
0.90
BERT

Accuracy Precision Recall F1-Score C1 Precision C1 Recall
0.50
0.62
0.62
0.80
0.90

0.50
0.67
0.52
0.83
0.95

0.50
0.50
0.58
0.76
0.84

0.50
0.63
0.62
0.80
0.90

0.50
0.64
0.63
0.80
0.90

Table 3: Results on the ENRON Codeword Validation Set.

We observe a signiﬁcant performance increase from the pre-trained BERT model. As discussed in the
background section, BERT has demonstrated that it performs better on most downstream NLP tasks
than the previous state-of-the-art models, and for our task, this is also the case. This signiﬁcant increase
can be attributed to two aspects of the model. First, deep neural language models are larger than the
other models and can learn more parameters, which means more stored information. Secondly, because
of how BERT is trained, the model can capture the context of the sentence very well. Because this
context information is vital to our task, the BERT model is best suited to this task, as demonstrated by
its performance on the validation set.

Model
Random 0.50
BoW
0.54
0.53
TF-IDF
Bi-LSTM 0.60
0.79
BERT

Macro Precision Macro Recall C1 Precision C1 Recall C1 F1-Score
0.05
0.12
0.09
0.22
0.59

0.09
0.19
0.15
0.34
0.69

0.50
0.64
0.62
0.82
0.90

0.50
0.49
0.50
0.79
0.84

Table 4: Results on the ENRON Codeword Test Set.

We observe similar performances on the imbalanced test set, as shown in table 4. The models follow
a similar trajectory in terms of macro precision and recall performance. However, both count-based
approaches achieve a precision of around 0.1 on the code word class and a random performance in
terms of recall on samples from that class. This performance is to be expected since we work with an
unbalanced dataset. However, within eDiscovery, we virtually always work with unbalanced datasets,
which makes it extremely important that a method also works for unbalanced data. We prefer a high
recall on the code word class since no samples must be missed during a review. Of course, it is also
essential that reasonable precision is achieved. Otherwise, the user will mainly see false positives. We
observe a signiﬁcant performance increase from the BERT model on this unbalanced dataset, especially
in terms of recall on the code word class. From these results, we can conclude that the BERT model
signiﬁcantly outperforms the other models on all selected metrics.

6 Conclusions

To conclude, we will cover the main ﬁndings of our study. We have observed how BERT is able to
capture richer contextual representations than the other bench-marked models. This results in higher
performance on the code word detection task. But more importantly, this new approach creates the
possibility to detect patterns that were previously not automatically detected (highest recall). Detecting
code words is one example of this, but as discussed earlier, there are many other examples of these
fraudulent patterns within fraud investigations. The method is domain independent, so any type of code
words will be recognized. In fact, the BERT-based method will pick up any kind of unexpected language
use.

There is the more philosophical questions on how well the BERT model really understands the code
words. On the one hand, we cannot under-estimate BERT’s ability to memorize due to the enormous
amount of connections that allow it to memorize word patterns. Clever Hans (in German, der Kluge
Hans) was a horse that was supposed to be able to do lots of diﬃcult mathematical sums and solve
complicated problems. Later, it was discovered that the horse was giving the right answers by watching
the reactions of the people who were watching him. By observing reactions of the human, the horse
“answered” correctly. But, in reality the horse only responded with memorized behavior. Is BERT just
a clever Hans by “memorizing” word patterns without really understanding this? On the other hand, we

8

can ask ourselves how much of our human language skills can be attributed to memorization (Niven &
Kao, 2019) (Heinzerling, 2019).

In conclusion: from a pragmatic point of view we can observe that by teaching a valid language model
for a certain language, BERT can successfully learn to recognize deviating language patterns given this
In the context of this research, those are code words used by fraudsters. But it is
language model.
fair to say, that other patterns are also picked up, for instance non-English language, misspellings, and
scanning-related Optical Character Recognition (OCR) errors. So, the model we developed appears to
be more a general model to pick up unexpected language patterns, instead of a model that explicitly
understands and detects code words. But for the application of code word detection, it can be deployed
successfully.

7 Discussion

While these initial results are promising for the application of the detection of code words, much remains
to be done to successfully use machine-learning techniques in fraud investigations. One of the biggest
research challenges in the AI research community is the challenge of explainable AI. Due to the sheer
size of these neural networks, it is extremely diﬃcult to ﬁgure out why the model makes a certain
classiﬁcation and what exactly this classiﬁcation is based on. Therefore, in this research we choose to use
machine learning techniques to support fraud investigation rather than replace the human investigator.
As a result, the beneﬁt to the fraud investigation is twofold. By setting up the framework, the fraud
investigator brings structure to the search space, and with the employment of machine learning techniques,
this search space is reduced and made insightful. At the same time the framework oﬀers cohesion to the
collection, classiﬁcation and weighting of evidence that is collected via AI-methods.

Further research will be done to identify more relevant evidence items which have a discriminative
relation to a fraud scenario and which can be obtained by an appropriate AI-method. In our proposed
model these evidence items all are formatted as an answer to one or more variants of the six golden
investigation questions. With an adequate amount of these ’triples’ (scenario-evidence-AI-method) we
expect that many investigations can beneﬁt signiﬁcantly in terms of eﬃciency and quality. Another topic
for further development is the automation of applying weights in terms of relevance and credibility to the
output of the AI-method and subsequently inserting consistency values into the ACH-matrix.

But, by being able to detect code words, a great ﬁrst step is made into the proper interpretation of

potential relevant patterns that can be used to ﬁll other components of the ACH-matrix.

8 Acknowledgements

The authors are grateful for the extensive support obtained for this research from ZyLAB Technologies
BV and Ebben Partners BV, both based in the Netherlands.

References

Cressey, D. Why do trusted persons commit fraud? a social-psychological study of defalcators. Journal

of Accountancy, 92:576, 1951.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional trans-

formers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Fong, S., Roussinov, D., and Skillicorn, D. B. Detecting word substitutions in text. In IEEE Transactions

on Knowledge and Data Engineering, volume 20, pp. 1067–1076. IEEE, 2008.

Heinzerling, B. Nlp’s clever hans moment has arrived. The Gradient, 2019.

Heuer, R. J. Psychology of intelligence analysis. Center for the Study of Intelligence, 1999.

Jabbari, S., Allison, B., and Guthrie, L. Using a probabilistic model of context to detect word obfuscation.
In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2008,
pp. 2216–2220, 2008.

Kassem, R. and Higson, A. The new fraud triangle model. Journal of emerging trends in economics and

management sciences, 3(3):191–195, 2012.

9

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,

2014.

Magu, R. and Luo, J. Determining code words in euphemistic hate speech using word embedding networks.

In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pp. 93–100, 2018.

Magu, R., Joshi, K., and Luo, J. Detecting the hate code on social media.

arXiv preprint

arXiv:1703.05443, 2017.

Niven, T. and Kao, H. Probing neural network comprehension of natural language arguments. CoRR,

abs/1907.07355, 2019. URL http://arxiv.org/abs/1907.07355.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,
Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances
in neural information processing systems, pp. 8026–8037, 2019.

Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep

contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.

Scholtes, J. C. Text-mining and ediscovery for big-data audits. European Court Auditors Journal. No 1,

pp. 133–140, 2020.

Scholtes, J. C. and Herik, H. v. d. Big data analytics for ediscovery. In Legal Big Data, Roland Vogl,

Eds. Edgar Publishing, 2021.

10

