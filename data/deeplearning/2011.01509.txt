2
2
0
2

n
u
J

6

]

R
C
.
s
c
[

6
v
9
0
5
1
0
.
1
1
0
2
:
v
i
X
r
a

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

1

MalFox: Camouﬂaged Adversarial Malware
Example Generation Based on Conv-GANs
Against Black-Box Detectors

Fangtian Zhong , Xiuzhen Cheng, Fellow, IEEE, Dongxiao Yu, Bei Gong, Shuaiwen Song,
Jiguo Yu, Senior Member, IEEE

Abstract—Deep learning is a thriving ﬁeld currently stuffed with many practical applications and active research topics. It allows
computers to learn from experience and to understand the world in terms of a hierarchy of concepts, with each being deﬁned through
its relations to simpler concepts. Relying on the strong capabilities of deep learning, we propose a convolutional generative adversarial
network-based (Conv-GAN) framework titled MalFox, targeting adversarial malware example generation against third-party black-box
malware detectors. Motivated by the rival game between malware authors and malware detectors, MalFox adopts a confrontational
approach to produce perturbation paths, with each formed by up to three methods (namely Obfusmal, Stealmal, and Hollowmal) to
generate adversarial malware examples. To demonstrate the effectiveness of MalFox, we collect a large dataset consisting of both
malware and benignware programs, and investigate the performance of MalFox in terms of accuracy, detection rate, and evasive rate of
the generated adversarial malware examples. Our evaluation indicates that the accuracy can be as high as 99.0% which signiﬁcantly
outperforms the other 12 well-known learning models. Furthermore, the detection rate is dramatically decreased by 56.8% on average,
and the average evasive rate is noticeably improved by up to 56.2%.

Index Terms—Adversarial Malware Examples, Deep Learning, Generative Adversarial Network, Malware

(cid:70)

1 INTRODUCTION

D EEP learning has been broadly investigated and

showed great merits in various applications [1], [2].
Nevertheless, with its rapid development, security issues
have raised serious alarms in recent years. Various attacks
on AI algorithms and models have emerged. At the train-
ing process, poisoning attacks can undermine the original
probability distribution of the training data by injecting
malicious examples to lower the predictive precision of the
model. At the test or inference process, evasion attacks can
confuse a target system by feeding carefully designed input
examples without changing the model [3]. Szegedy et al.
formally proposed the concept of adversarial examples [4],

•

Fangtian Zhong and Xiuzhen Cheng are with the Department of Com-
puter Science, The George Washington University, Washington DC,
USA.
E-mail: squareky zhong@gwu.edu, cheng@gwu.edu

• Dongxiao Yu is with the School of Computer Science and Technology,

Shandong University, Qingdao, Chian.
E-mail: dxyu@sdu.edu.cn

• Bei Gong is with Faculty of Information Science, Beijing University of

Technology, Beijing, China.
E-mail: gongbei@bjut.edu.cn

•

•

Shuaiwen Song is with the School of Computer Science, University of
Sydney, Sydney, Australia.
E-mail: shuaiwen.song@sydney.edu.au

Jiguo Yu (Corresponding Author) is with the School of Computer Science
and Technology, Qilu University of Technology (Shandong Academy of
Sciences) and Shandong Computer Science Center (National Supercom-
puter Center in Jinan), Jinan, China.
E-mail: jiguoyu@sina.com

which can be constructed by applying small but worst-
case perturbations. These examples can make the model
outputting incorrect answers with high conﬁdence. More
seriously, even though different machine learning systems
have different architectures and training datasets, the same
set of adversarial examples demonstrate great transferable
abilities to attack related models [5]. As a result, a variety
of adversarial examples used in different areas are thrived
[5]–[9], which are summarized as follows.

In the study of natural language processing, Zhang et
al. presented an adversarial example generator by refer-
ring to the Metropolis-Hastings attack algorithm to de-
velop ﬂuent adversarial examples for natural languages,
and the examples successfully mislead the bi-LSTM and
BiDAF models on IMDB and SNLI datasets [5]. In text
classiﬁcation, Ebrahimi et al. proposed an efﬁcient method to
produce white-box adversarial examples, greatly decreasing
the accuracy of a character-level neural classiﬁer [6]. In
image processing, Qiu et al. proposed SemanticAdv to create
semantically realistic adversarial examples via attribute-
conditioned image editing, which achieves high targeted
attack success rates under both white-box and black-box
settings [7]. In automatic speech recognition, Alzantot et al.
presented a demonstration of adversarial attacks against the
speech classiﬁcation model with 87% success rate by adding
small background noises, without knowing the underlying
model parameter and architecture [8]. In autonomous driv-
ing, Kong et al. presented PhysGAN to generate realistic and
physical-world-resilient adversarial examples for attacking
common autonomous driving scenarios [9].

Nevertheless, there have been no successful attacks
against the collection of practical antivirus products and

 
 
 
 
 
 
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

2

online scan engines, e.g. VirusTotal [10]. In this paper, we
aim at generating adversarial examples that can bypass
the detection by VirusTotal. One of the major challenges
herein is to edit malware ﬁles. Additionally, for a practical
application, any generated adversarial example shall be
functionally indistinguishable from its original ﬁle.

To tackle these challenges, we propose MalFox, an intel-
ligent adversarial malware example generation framework
based on convolutional generative adversarial networks
(Conv-GANs). MalFox consists of 5 major components: PE
Parser, Generator, PE Editor, Detector, and Discriminator. PE
Parser extracts features from malware and benignware and
transforms them into vectors serving as the input to Gen-
erator and Discriminator. Generator produces perturbation
paths based on the malware features and Gaussian noises to
make Discriminator misclassify malware as benign, while
Discriminator is trained to distinguish benignware and
malware precisely. PE Editor is used to generate adversar-
ial malware examples by following the perturbation paths
and makes the change functionally indistinguishable from
the original malware. Detector is used to label adversarial
malware examples such that the labels can be exploited by
Discriminator to provide feedback to Generator. We adopt
VirusTotal as the MalFox Detector, as it provides a collection
of practical antivirus products and online scan engines that
are well-received by security professionals and public users.
Our multi-fold contributions can be summarized as follows.
First, in this paper, we propose an intelligent Conv-
GAN-based framework, namely MalFox, which can gen-
erate functionally indistinguishable adversarial malware
examples by successfully editing malware ﬁles following
customized perturbation paths. Our framework is ﬂexible
in that it can accept any number and type of perturbation
methods, thus enlarging the search space for more effective
perturbation paths that can improve the abilities of malware
to evade detection. In our implementation, we develop three
novel framework perturbation methods whose combina-
tions help produce adversarial malware examples that can
mislead third-party black-box malware detectors with high
probability under realistic environments.

Second, distinctive from other GAN-based adversar-
ial malware example generation methods that attack self-
developed scan engines based on machine learning models,
MalFox intends to attack a collection of antivirus products
and online scan engines provided by third parties, which
renders it a more practical and powerful attack framework.
We make use of VirusTotal, which is adopted by realworld
applications. On the other hand, existing GAN-based meth-
ods cannot generate practical adversarial malware examples
since they just simply inject benignware’s function names
as features into malware feature vectors without actually
implementing them in the corresponding malware ﬁles,
which implies that they cannot attack antivirus products
and online scan engines in practice. To our best knowledge,
MalFox is the ﬁrst framework that can launch adversarial
attacks targeting practical malware detectors from third
parties, under black-box settings.

Third, we successfully conduct attacks on 82 malware
detectors and search engines in VirusTotal without know-
ing their underlying implementation details. The results
demonstrate that MalFox possesses outstanding perfor-

mance in its ability of evading from detection. More specif-
ically, MalFox can produce adversarial malware examples
that can successfully confuse Discriminator with an ac-
curacy of 99.0%, and greatly lower the detection rate up
to 56.8% in VirusTotal. Besides, the average evasive rate
reaches 56.2%, which indicates that the generated adversar-
ial malware examples may largely escape from detection as
a regular user typically installs only one or a few antivirus
products (much less than 82).

The rest of the paper is organized as follows. Section 2
provides an overview on popular malware evasion tech-
niques. Section 3 presents the design of MalFox. Section 4
details the implementations of MalFox components. Sec-
tion 5 evaluates the performance of MalFox, and Section 6
concludes the paper with a future research discussion.

2 BACKGROUND AND RELATED WORK

Traditional antivirus products and online scan engines were
built by running virtual machines (VMs) over one of the
available virtualization platforms such as VMware, Virtual-
Box, KVM, and Xen, which leaves huge gaps for malware
to manipulate. Evasion is an action in which malicious
payloads exploit these gaps to avoid detection. As malware
detectors become more and more sophisticated and keep on
evolving to defeat emerging evasion techniques, malware
authors have to change their tactics continuously in order
to remain one step ahead. In this section, we provide an
overview on existing popular malware evasion methods,
classifying them as either artiﬁcial intelligence (AI) based
or non-AI based, as AI provides powerful tools for smart
evasion solutions in recent years.

2.1 Non-AI Based Evasion Techniques

The most prevalent non-AI based evasion techniques in-
clude delaying execution, ﬁngerprinting, obfuscation, and multi-
stage evasion, which are summarized as follows.

Delaying Execution. Due to the fact that it is compu-
tationally costly to run a complete detection environment,
detectors usually only accurately visualize the behaviors of
malware within a short period of time. This vulnerability
can be exploited by malware to delay the normal execution
of its malicious process. Evasion techniques based on de-
laying execution include manipulating the delay application
programing interfaces (NtDelayExecution, CreateWaitTable-
Timer, SetTimer, etc.), sleep patching, and time bombs. Many
malware programs exploit these techniques to successfully
bypass anti-virus products. For example, in 2011, the Khe-
lios botnet, which is capable of sending roughly 4 billion
spam messages a day, called the NtDelayExecution() API
with a 10-minute extended sleep period to evade detec-
tion [11]. Black POS malware, one of the most pervasive
types of point-of-sale malware observed in-the-wild in 2013,
exhibited time bomb evasion by executing during certain
periods while remaining dormant at the rest of the time
and successfully disguised an installed service of a known
anti-virus software (KrebsOnSecurity). Consequently, cus-
tomers’ debit and credit card information was exposed to
the public [12]. In 2012, the Trojan UpClicker employed the
SetWindowsHookEX() API function to hide its malicious

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

3

activity. By sending OEH as the parameter to the function,
the malicious code was activated only when the left mouse
button was clicked and released. Because most ﬁle-based
detectors do not mimic human interactions, this malware
remained dormant during analysis and successfully evaded
detection [11].

Fingerprinting. Fingerprinting is a technique to de-
tect the signs that attest to the presence of an analysis
environment or debuggers of malware detectors. Major
ﬁngerprinting techniques employed by malware include
analyzing Process Environment Block (PEB), searching for
breakpoints, and probing for system artifacts. PEB is a
data structure that exists per-process in Windows, and the
ﬁelds in PEB contain information that can be retrieved by
malware to detect the presence of a debugger [13]. The
most obvious one is a ﬁeld named BeingDebugged. Anti-
debugging tactics relying on PEB constitute the critical part
of evasion techniques observed in malware [14]. Besides, to
analyze malware, detectors often set breakpoints during the
execution of malware and save the breakpoint address in
CPU DR registers. The behavior of detectors can be spotted
by the malware through a self-scan or integrity check. For
example, the malware CIH uses GetThreadContext to check
the CPU register and erases the breakpoint information to
avoid analysis [15]. Moreover, from installation to conﬁg-
uration and execution, detectors often leave traces behind
in different levels of the OS, e.g. in the ﬁle system, registry,
process name, etc. Hence, malware can simply look for these
traces and wait for opportunities to activate codes.

Obfuscation. Obfuscation concerns the practice of de-
liberately degrading the quality of information in some
ways, to protect the privacy of the individual to whom
that information refers. This technique converts a program
into a form functionally equal to the original one but may
make the program hard to be understood. At the early stage,
obfuscation was used to protect the intellectual property of
software developers by hiding the codes from the public.
However, it has been widely used by malware authors to
elude detection in recent years. Sharif et al. presented a
malware obfuscation technique that automatically conceals
the trigger-based behavior of malware from input-oblivious
malware analyzers [16]. Their technique transforms speciﬁc
branch conditions that rely on inputs by incorporating one-
way hash functions such that it is hard to identify the values
of the variables for which the conditions are satisﬁed. The
conditional code is identiﬁed and encrypted with a key
that is derived from the value satisfying the condition [16].
Schrittwieser et al. introduced a novel approach for code
obfuscation called covert computation and raised the bar for
semantic-aware code analysis [17]. Because current malware
detection approaches ignore the fundamental knowledge
of the underlying hardware, and the instructions cannot
completely express the state of a microprocessor, this vulner-
ability was exploited to hide the implementation of certain
speciﬁc functionality of malware [17].

Multi-stage Evasion. Detectors via data signature scan-
ning and behavior signature scanning assume that a holistic
malware is present in the analysis environment and is
examined to trigger an alert. However, malware with multi-
stage delivery can bypass such an analysis, as it includes a
set of sequential stages, wherein each stage does not trigger

the alert. For example, Ispoglou et al. proposed malWash,
a dynamic diversiﬁcation engine that hides the behaviors
of malware by distributing the malware’s execution across
many processes. Target malware is decomposed into small
components that are then executed in the context of other
processes. A master as a coordinator connects these compo-
nents and transfers the execution ﬂow among the different
processes [18]. Pavithran et al. proposed D-TIME, a new dis-
tributed threadless independent malware execution frame-
work to evade runtime detection. They chopped malware
into small chunks of instructions and executed one chunk at
a time in the context of an infected thread. The coordination
between chunk executions is organized by shared memory
with asynchronous operations [19].

The evasion techniques mentioned above play their
roles via different tricks. Speciﬁcally, delaying execution,
ﬁngerprinting, and obfuscation mainly evade detection by
advanced techniques outperforming detectors, while multi-
stage evasion depends on other processes under the same
runtime environment.

2.2 AI-Based Evasion Techniques

Artiﬁcial Intelligence (AI) essentially comprises algorithms
capable of processing and learning from vast amounts of
data and then making decisions autonomously. With the
advancement of AI, detectors using AI techniques are com-
mon. Hence, malware evasion powered by AI appears in
our vision recently. Such techniques can be classiﬁed as
biology-inspired and deep neural network-based (DNN-based).

Biology-Inspired Approaches. In [20], Castro et al. used
genetic programming that makes malware samples evolve
to attack four static learning classiﬁers (namely Gradient
Boosted Decision Tree, Sophos, ESET, and Kaspersky) until
they are no longer able to precisely detect. Each malware is
sent to a manipulation box to inject byte-level perturbations,
and then different malware mutations are produced. Those
malware mutations are sent to classiﬁers to receive scores
according to their evasive capability. The two with the
highest scores are considered as the most valid evasion and
continue to generate next generation which would inherit
their evasive abilities. The children repeates the above pro-
cess and ends up with the strongest malware mutation.
In [21], Zelinka et al. put forward a scheme implemented
by swarm intelligence-based algorithms that can smoothly
evade machine learning-based detectors. This scheme em-
ulates the behavior of the biological swarm systems that
don’t require a central communication point. The commu-
nications between members in the swarm system can be
realized by having a collective memory via which every
individual can share knowledge as well as ”learn from it”.
Therefore detectors that exploit patterns extracted from a
central communication point can not detect the abnormality.
DNN-Based Approaches. Fang et al. proposed a frame-
work named DQEAF, which uses reinforcement learning to
evade machine learning-based detections [22]. This frame-
work is composed of an attacking model and an AI agent.
The attacking model is a series of functionality-preserving
actions to alter malware samples. The agent inspects mal-
ware samples and chooses a sequence of actions to de-
liberately modify the samples for evasion. In the end, the

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

4

framework can determine an optimal sequence of actions to
evade detection. In [23], Grosse et al. performed adversarial
crafting attacks on neural networks for malware classiﬁca-
tion. They generated adversarial examples by adding entries
to the AndroidManifest.xml ﬁle in malware. This scheme
ensures that the addition of perturbations does not affect
the utility of the malware.

Papernot et al. introduced the ﬁrst demonstration that
black-box attacks against detectors implemented by DNN
classiﬁers are practical for real-world adversaries with no
knowledge about the classiﬁers [24]. Their attack strategy
is to train a local substitute DNN with a synthetic dataset:
the inputs are synthetic and generated by the adversary
while the outputs are labels assigned by the target DNN
and observed by the adversary. Speciﬁcally, some features
extracted from the malware are replaced to generate an ad-
versarial example serving as the synthetic input to mislead
the local substitute DNN classiﬁers. Because the local and
target models have similar decision boundaries, the attacks
to the target DNN classiﬁers are successful if the malware
evades the detection of the local DNN classiﬁers. Hu et al.
proposed a generative adversarial network (GAN) based
algorithm named MalGAN to generate adversarial malware
examples, which can bypass black-box machine learning-
based detectors. MalGAN consists of a substitute detector, a
generator, and a black-box malware detector. The substitute
detector is trained to ﬁt the black-box detector, while the
generator is used to minimize the probability of malware
being detected by the substitute detector via adding new
features to the malware [25].

The AI-based malware evasion techniques mentioned
above build models based on a varity of views. Genetic
programming and swarm-based intelligence algorithms em-
ulate the behaviors of animals, while other approaches are
on the basis of deep neural networks. In this paper, we
present MalFox, an intelligent adversarial malware example
generation framework based on Conv-GANs. Compared to
the above-mentioned techniques, MalFox is able to gener-
ate practical functionality-preserving adversarial malware
examples by making use of the power of classical and AI
methods, and conduct practical attacks against a collection
of antivirus products and online scan engines provided
by third parties even without knowing the underlying im-
plementation details. As a comparison, MalFox attacks the
realworld malware detectors from third parties that exploit
AI-based as well as other unknown techniques, while the
models provided in [22]–[25] heavily rely on the powerful
recognition capability of self-developed machine learning-
based detectors. Moreover, [23]–[25] actually do not gener-
ate adversarial malware examples that can be adopted in
practice.

3 MALFOX DESIGN

The objective of MalFox is to transform malware programs
into foxy ones that are expected to be examined as benign
by third party detectors such as VirusTotal (see Fig. 1). Fig. 2
demonstrates the architecture of MalFox, which consists of
5 components: PE Parser, Generator, PE Editor, Detector, and
Discriminator.

Fig. 1: The MalFox Framework

Fig. 2: Architecture of MalFox

PE Parser is developed to parse malware and benign-
ware programs, and retrieve the dynamic link libraries
(DLLs) and system functions that are called by each pro-
gram under runtime environment. The names of DLLs and
system functions are used as features by which MalFox can
specify particular malware or benignware. When the names
of all DLLs and system functions are extracted from the
available malware and benignware under study, a feature
set containing unique features is deﬁned. We represent all
features in the set as a vector; then PE Parser can denote
each malware and benignware by a binary feature vector
with each element corresponding to the presence (‘1’) or
absence (‘0’) of a certain feature.

Generator and Discriminator are components of the gen-
erative adversarial network. They are used to help generate
an adversarial malware example for the input malware in
MalFox. Speciﬁcally, for each malware program, Generator
produces a perturbation path that is used by PE Editor
to construct an adversarial malware example, which is ex-
pected to confuse Discriminator and ﬁnally escape detection
by malware detectors. Discriminator, on the other hand,
attempts to precisely distinguish malware and benignware.
When Discriminator correctly identiﬁes an adversarial mal-
ware example during the training process, Generator has to
repeat its process to generate a new perturbation path for
the malware; the process stops when Discriminator fails to
identify the adversarial malware example, at which time the
PE Editor outputs the desired adversarial malware example.
Therefore one can see that the stronger the Discriminator,
the more powerful the Generator (and the PE Editor).

PE Editor manages the procedure of generating ad-
versarial malware examples by following the perturbation
paths produced by Generator. The implementation of PE
Editor is the most challenging one in MalFox. We propose
three novel framework methods, namely Obfusmal, Stealmal,
and Hollowmal in this paper, with each framework method
supporting various ﬂexible implementations and each per-

MalwareBenignwarePE ParserGeneratorDiscriminatorGaussianNoisesPE EditorDetectorTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

5

turbation path consisting of zero or more of the instantiated
methods. Obfusmal encrypts the code segment of a malware
ﬁle and attaches a DLL named Shell.dll at its end. Shell.dll
is responsible for decrypting the code segment and recov-
ering the normal execution of malware. Stealmal encrypts
the entire malware and attaches it to the end of a program
named Shell.exe. The duty of Shell.exe is to decrypt the
malware, create a suspended process, obtain the process
space, copy the malware into the space, change the context
of the process to the entry point of the malware, and resume
the process. Hollowmal encrypts the malware and attaches
the encrypted malware into the end of a benignware pro-
gram. Then, a DLL named Hollow.dll, which embraces
similar functionality as Shell.exe, is added into the end of
the modiﬁed benignware. All these three methods would
not affect the normal execution of the malware. Note that
we say Obfusmal, Stealmal and Hollowmal are framework
methods because the implementations of the correspond-
ing Shell.dll, Shell.exe and Hollow.dll are not speciﬁcally
deﬁned, which implies that the space of the instantiated per-
turbation methods is very large, signiﬁcantly enhancing the
evasiveness of the produced adversarial malware examples.
Particularly, Hollow.dll employs a benignware program,
which can be any benign program, further enlarging the
search space of the perturbation methods.

In our study, we employ VirusTotal as the MalFox
Detector. VirusTotal is a webiste launched in June 2004,
and deveoped by the Spanish security company Hispasec
Sistemas, a subsidiary of Google Inc., which aggregates
many antivirus products and online scan engines to check
for malware. The virtualization solution used by VirusTotal
is the Cuckoo sandbox. Users can upload ﬁles up to 550
MB to the website and receive the detective outcome from
the antivirus products and online scan engines. Currently,
VirusTotal handles approxiamtely one million submissions
each day. The results of each submission are then shared
with the entire community of antivirus vendors who lend
their tools to the VirusTotal service, which in return allows
vendors to beneﬁt by adding into their products the mal-
ware signatures of new variants that their tools have missed
but a preponderance of other tools have ﬂagged as malicious
[26]. Popular tools such as McAfee, F-Secure, Tencent, 360,
and Microsoft in VirusTotal, have been widely adopted on
laptop and mobile devices. In this study, we use VirusTotal
to ensure the reliability of our datasets, provide labels for
Discriminator, and validate the performance of MalFox.

The procedure for constructing MalFox is illustrated by
Algorithm 1. Each benignware and malware program in the
training dataset is ﬁrst sent to PE Parser to get a binary
malware feature vector (Line 1). A while loop follows to
train MalFox iteratively (Lines 2-13) until Generator and
Discriminator are stable (their model parameters do not
change much from iteration to iteration). At each iteration,
a minibatch of malware feature vectors are combined with
Gaussian noises (one noise sample for each malware to get
a noised malware program) as inputs to Generator (Line
3). Generator produces a perturbation path for each noised
malware (Line 4), which is employed by PE Editor to edit
the malware for producing the corresponding adversarial
malware example (Line 5). Then the adversarial malware
examples and a minibatch of benighware programs are used

Algorithm 1 MalFox Training Procedure

1: Convert each malware and benignware program in the
training dataset into a binary feature vector by PE
Parser;

2: while not converging do
3:

Sample a minibatch of malware feature vectors and
three-dimensional Gaussian noises, combine each
malware feature vector with a noise sample, and input
the results to Generator;

4: Generator generates perturbation paths and inputs

5:

them to PE Editor;
PE Editor produces adversarial malware examples
following the perturbation paths;
Sample a minibatch of benignware feature vectors;
6:
7: Update Discriminator’s parameters with the adver-
sarial malware examples and benignware programs
by descending along the gradient of LD (Eq.(10));
Sample three-dimensional Gaussian noises, combine
each with a malware feature vector in the minibatch,
and input the results to Generator;

8:

9: Generator generates perturbation paths and inputs

10:

them to PE Editor;
PE Editor produces adversarial malware examples
following the perturbation paths;

11: Detector labels the adversarial malware examples;
12: Update Generator’s parameters with the newly gen-
erated adversarial malware examples by descending
along the gradient of LG (see Eq.(8))

13: end while

to train Discriminator (Line 6-7). Next, a new set of three-
dimensional Gaussian noises is produced and combined
with the malware feature vectors selected in line 3 as inputs
to Generator (Line 8), which constructs a new perturbation
path for each noised malware program (Line 9). PE Edi-
tor follows the perturbation paths to generate adversarial
malware examples (Line 10). Detector labels the adversarial
malware examples (Line 11), and the labeled adversarial
malware examples are used to trained Generator (Line 12).
Producing an adversarial malware example by MalFox
for a given malware program is simple. The malware pro-
gram is ﬁrst transformed into a binary feature vector by PE
parser, then combined with a random 3-dimensional noise
to get the input to Generator. Generator outputs a pertur-
bation path for the malware and the PE Editor modiﬁes
the original malware according to the perturbation path to
produce an adversarial malware example.

4 MALFOX IMPLEMENTATION
In this section, we detail our implementations of the four
components of MalFox, namely PE Parser, Generator, PE
Editor, and Discriminator.

4.1 PE Parser

As mentioned in Section 3, the objective of PE Parser
is to obtain features from benignware or malware pro-
grams and transform them into binary feature vectors.
Since each malware or benignware was developed to sat-
isfy certain functionality by employing a speciﬁc set of

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

6

DLLs and calling a number of system functions, the names
of these DLLs and system functions can be used as fea-
tures to differentiate different benignware or malware.
For example, to realize the special memory leakage check
module, Avant Browser makes use of kernel32.dll, AD-
VAPI32.dll, COMCTL32.dll, CompareFileTime, GetShort-
PathNameA, GetFullPathNameA, MoveFileA, and SetCur-
rentDirectoryA, just to name a few.

Fig. 3: Structure of PE

follows the portable executable format

We collected a dataset with benignware and malware
that
(PE) used
in Windows Operating Systems. To retrieve the names
of the DLLs and system functions from the dataset, we
need to take a close look at the structure of the PE as
shown in Fig. 3. One can see that the PE has 6 compo-
nents: DOS M Z Header, DOS Stub, P E F ile Header,
Image Optional Header, Section T ables, and Sections,
with each occupying certain number of bytes and including
information related to different aspects of the PE. More
speciﬁcally, DOS MZ Header has 64 bytes and contains 19
ﬁelds, including the signature ﬁeld that tells whether the
program follows the PE and the address ﬁeld that directs
to PE File Header; DOS Stub has 224 bytes and intends to
print linkers’ information, which implies that the runtime
environment is MS DOS; PE File Header has 64 bytes and
speciﬁes the machine type, the number of sections, and the
size of Image Optional Header; Image Optional Header
contains 224 bytes for 32-bit programs or 240 bytes for 64-
bit programs and includes additional information such as
ImageBase, SectionAlignment, FileAlignment, SizeOfImage,
SizeOfHeaders, and DataDirectories, which are required by
the linker and loader in Windows; Section Tables is an
array possessing the IMAGE SECTION HEADER structure
with each entry containing information about one section
in the Sections component; Sections contains the contents
of the ﬁle, e.g. code, data, resources, and other executable
information.

To obtain the features of the benignware and malware
programs, PE Parser needs to parse DataDirectories, which
indicates where to ﬁnd important components of the exe-
cutable information in a ﬁle. DataDirectories has 16 tables,
and each table references a data directory with an 8-byte
entry, among which the Import (Import) and Import Ad-
dress Table (IAT) are two of the most important ones for
PE Parser. Import points to a squence of structures with
each storing information that corresponds to a DLL and

the system functions supplied in the DLL when loaded into
memory, and is stored in one of the sections. The entry to
Import is a relative virtual address (RVA) in the memory.
In this study, we should convert all RVAs to ﬁle offset ad-
dresses (FOAs) in the malware or benignware ﬁles because
they are not executed. The conversion can be caculated by

F OA = RV A − SectionRV A + SectionF OA·

(1)

where SectionRVA is the start address of the section in the
memory where Import resides, and SectionFOA is the start
address of the section in the malware or benignware ﬁle.
Then, one can locate Import, in which Name is a RVA
pointing to the name of a DLL, and OriginalFirstThunk
(RVA) points to an import name table (INT) where the
function names’ addresses are stored. Each address points
to a structure called _IMAGE_IMPORT_BY_NAME at which
the function name stays. For the sake of rationality, we ﬁlter
the function name that is imported as an ordinal when the
value of Ordinal is greater than 0x80, 000, 000. We then
acquire the DLL name and all system function names in the
DLL by NAME and INT. IAT has the same functionality as
OriginalFirstThunk, and FirstThunk is a pointer to
IAT, which can also be searched from DataDirectories.
Moreover, each Import or IAT is followed by another
Import or IAT. Therefore in order to extract all DLL
names and the system function names called by them, PE
Parser should repeat the procedure iteratively until no more
Import or IAT is available.

After parsing all malware and benignware, the set of
features, i.e., the names of all DLLs and system functions,
are collected. Then the feature vector of each program is
represented by a binary vector with an entry value ’1’ indi-
cating that the program possesses the correponding feature
(calling the DLL or system function) and ’0’ otherwise.

Fig. 4: Network Structure of the Generator

4.2 Generator

Generator is used to produce a perturbation path that im-
proves malware’s evasive capability while maximizing the
probability of Discriminator making a mistake. It employs
a transposed CNN with 4 types of layers: fully connected
feed-forward layers, upsampling layers, dropout layers, and
convolutional layers (see Fig. 4). As the transposed CNN
can utilize its property of composing a speciﬁc set of mal-
ware features to uniquely represent the malware, Generator

ImportIATVirtual Address (RVA)SizeVirtual Address (RVA)SizeImport desc 0Import desc 10...Address 1Address 1Address 10000 0000......DOS MZ HeaderDOS StubPE SignaturePE File HeaderSections.idata.rsrc.data.text.src.relocSection TablesData DirectoriesImage_Optional_HeaderGaussian noisesConv. layerDropout layerFully-conn.feed-forward layerUpsampling layerConcatBatchNormalizationBatchNorm.sigmoidReshapeLeakyReLU(α) is (u, u)FlattenBinary MalwareFeaturesLeakyReLU(α)sigmoidPerturbation Pathsmzkernel_reg. is re, neuron no. is ne1dr. rate is rneuron no. is ne2dr. rate is rr(conv1a, conv1b, conv1c)filter no. is cf1, conv. window size is (conv1, conv1)upsampling size is (u, u)dr. rateis rfilter no. is cf2, conv. window size is (conv2, conv2)dr. rateis rupsampling size is (u, u)filter no. is cf3, conv. window size is (conv3, conv3)neuron no. is ne3oo'FGLeakyReLU(α)rupsampling size is (u, u)THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

7

makes use of this property to generate the appropriate
perturbation path for each malware to help the malware
escape from detection. In the following we describe the 13-
layer Generator structure employed by our study for the
dataset presented in Section 5.

The input to Generator is a malware feature vector
concatenated with three random values in the range of
[0, 1), followed by a BatchNormalization process. We let
each random number correspond to one framework per-
turbation method, in the order of Obfusmal, Stealmal, and
Hollowmal. Each random number is drawn from the Gaus-
sian distribution N (0, 1). Since each framework method has
multiple implementations, the value of the corresponding
random number speciﬁes the exact instantiated method to
be used. For example, if we instantiate 100 Obfusmal meth-
ods, then (0, 0.01] , (0.01, 0.02], . . . , (0.99, 1.0) correspond
to the ﬁrst, second, . . . , the 100th Obfusmal method.

Next the normalized vector is fed to a fully connected
feed-forward layer with ne1 neurons, a sigmoid activation
function and a l2 regularizer re, where ne1 is the number
of antivirus products and online scan engines VirusTotal
has. The output of this layer is sent to the ﬁrst dropout
layer with a drop rate r, which is used to ensure that the
output is not over-ﬁtting. Following the dropout layer is
the second fully connected feed-forward layer with ne2
neurons and a LeakyReLU activation function. Note that all
the LeakyReLU activation functions used in Generator have
the same slope coefﬁcient α and intend to add nonlinearity
to Generator. The output of this second fully connected
feed-forward layer is the input to the second dropout layer
with the same drop rate as the ﬁrst one. After the second
dropout layer, its output is reshaped into a 3-D vector with
a shape of (conv1a, conv1b, conv1c) before sent to the ﬁrst
convolutional layer. The ﬁrst convolutional layer contains
cf 1 ﬁlters and a 2D convolution window with a size of
(conv1, conv1). Then, the ﬁrst upsampling layer with a size
of (u, u) follows the ﬁrst convolutional layer. Its output is
sent to the third dropout layer with a LeakyReLU activation
function, which discards each element with a probability
r. The output of the third dropout layer passes through
the second convolutional layer with cf 2 ﬁlters and a 2D
convolution window shaped to (conv2, conv2). The output
obtained from the second convolutional layer is sent to the
fourth dropout layer with a drop rate r. After the dropout
layer, the output is upsampled by the second upsampling
layer. The third convolutional layer with cf 3 ﬁlters and a 2D
convolution window shaped to (conv3, conv3) follows the
second upsampling layer before the last upsampling layer
with a LeakyReLU activation function. Finally, their output
is ﬂattened, and sent to the last fully connected feed-forward
layer with ne3 neurons and a sigmoid activation function.
The sigmoid activation function restricts the values in the
generated perturbation path within the range of (0, 1). We
denote the perturbation path to be o. Since the generated
perturbation path is supposed to be binary, a smooth func-
tion FG is deﬁned to implement binarization tranformation.
The deﬁnition of FG is shown in Eq. (2), where each element
in o is compared with a threshold 0.5.

(2)
At last, we obtain a binary vector o(cid:48) with a shape (z, ).

F G(o(cid:48)) = o > 0.5.

Note that o(cid:48) suggests the customized perturbation path
that PE Editor should follow to generate the adversarial
malware example for the input malware. For example, if
the customized perturbation path o(cid:48) has a value (0, 0, 1),
PE Editor adopts Hollowmal only (actually the speciﬁc
Hollowmal method determined by the random number);
and if the customized perturbation path o(cid:48) has a value
(1, 0, 1), it adopts Obfusmal followed by Hollowmal (i.e.,
the speciﬁc Obfusmal method followed by the Hollowmal
method determined by the random numbers).

For better elaboration, we denote the data output by the
i-th layer to be X C
i , where i = 1, 2, 3, . . . , 13, the weight
vector between the i-th layer and the i+1-th layer to be
W C
i . Then
one can represent the relationship between the data from
two neighboring layers in our model as

i,i+1 and the bias vector of the i-th layer to be BC

X C

i+1 = {X C

i where x>0

i otherwise.

0.01X C

(3)

where the relationship is LeakyReLU and x is the element in
X C
i . When the relationship between two neighboring layers
is sigmoid, it is calculated by

X C

i+1 =

i

eX C
i + 1)

(eX C

.

(4)

The calulations within fully connected feed-forword layers
are performed according to
i+1 = W c

i + BC
i .

i,i+1X C

X C

(5)

Suppose at the i-th convolutional layer and upsampling
layer, we denote the number of ﬁlters to be nf , the size
of the 2D convolution window to be (f, f ), and the shape
of the upsampling map to be (u, u). Then within convolu-
tional layers and upsampling layers, Eq. (6) and Eq. (7) are
performed.

X C
i = Conv2D(nf, (f, f ), X C
i ).
i = U pSampling2D((u, u), X C

i ).

X C

(6)

(7)

To train Generator, the loss function is deﬁned as shown
in Eq. (8), where Sm is a set of malware feature vectors,
z is a three-dimensional vector with each element drawn
from a normal distribution N (0, 1), G is Generator, and D
is Discriminator. Note that Generator aims at lowering the
probability of malware to be detected by Discriminator. To
achieve this goal, LG should be minimized with respect to
the weights and parameters of Generator. Thus the gradient
information of Generator is updated with respect to the loss
value in Discriminator via backpropagation when Discrimi-
nator doesn’t detect the adversarial malware example.

LG=Esm∈Sm,z∼N (0,1) log D(G(sm, z)).

(8)

4.3 PE Editor

PE Editor is in charge of increasing the evasive ability of
malware by following the perturbation path output from
Generator to produce an adversarial malware example. Re-
call that the output o(cid:48) of Generator has 3 elements, specify-
ing whether each of Obfusmal, Stealmal, and Hollowmal is
adopted. The idea of Obfusmal is motivated by application
shielding software such as ASPack [27] and PECompact

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

8

[28], which are used to compress and protect programs. We
employ encryption instead of compression to futher hide
the malware, which can obliviously avoid the detection
by statistics-based or data comparison methods [29]. The
principles of Stealmal and Hollowmal come from malicious
applications NETWIRE Trojan [30], Monero miner [31], and
BitPaymer Ransomware [32], but we bring encryption to
the processes to further conceal the malware. Note that we
are not able to ﬁnd any description on the implementation
procedures of the programs mentioned above, and thus
largely rely on the public documents to infer their functions.
In this aspect, we are the ﬁrst to reveal the methods. In
the following we detail the implementations of these three
framework perburbation methods and explain how they are
applied. In this paper, we use malware.exe to refer to the
malware program that will be converted to a foxy one by
MalFox.

4.3.1 Obfusmal
The implementation of Obfusmal consists of 4 steps: (i) read
malware.exe, obtain the address and size of its code section,
and encrypt the code section; (ii) develop Shell.dll with
the functionality that can store the crucial information (code
section address, code section size, and decryption key),
decrypt the code section, ﬁx up relocation information, and
jump to the OEP (an OEP is the start address for program
execution) of malware.exe to execute the code; (iii) load
Shell.dll into memory to save the address and size of the
code section, decryption key, and the OEP of malware.exe
in an extern export global variable, and change the OEP of
malware.exe to the OEP of Shell.dll; (iv) add a section with
the length up to Shell.dll in malware.exe, save Shell.dll in
the newly added section of malware.exe, create a buffer
with the length up to malware.exe plus Shell.dll, copy
malware.exe to the buffer and write the buffer into disk.
The buffer in the disk contains the foxy malware. The
address and size of the code section can be retrieved in
the section tables and each section table corresponds to a
section in Sections in Fig. 3. The section tables follow Im-
age Optional Header and each table has the same structure.
To encrypt the code section, we look for PointerToRaw-
Data and SizeOfRawData at the code section table, as
therein PointerToRawData stores the ﬁle offset address and
SizeOfRawData stores the size of the code section. After
specifying the address and size of the code section, one can
employ any secure encryption algorithm such as DES, AES,
3DES, etc. For convinience and simplicity, we utilize the
exclusive OR algorithm with the key 0x15 for encryption.
Since the decryption process is taken effect in memory, the
address of the encrypted section saved in Shell.dll should
be PhysicalAddress.

When it comes to develop Shell.dll, an extern export
global variable should be deﬁned to save PhysicalAddress,
SizeOfRawData, OEP and the key, such that Shell.dll can
decrypt the code section by using the extern export global
variable. Next we need to ﬁx up the relocation informa-
tion since malware.exe might be loaded into an unpref-
ered address that is distinct from the ImageBase ﬁeld in
Image Optional Header of malware.exe, and thus some
addresses of the static varibles should be modiﬁed. To
ﬁx up relocation information, VirtualProtect, VirtualAlloc

and GetModuleHandleA supplied by Windows have to be
called. However, these functions are no longer provided by
our method as the start address of program execution is
changed to the OEP of Shell.dll and the execution process
of malware.exe loaded into memory through Windows
is interrupted. Hence, we have to obtain these functions
by resorting to kernal32.dll, which is a Windows system
kernel DLL that supplies these functions. The base address
of kernal32.dll can be acquired by the assembly code shown
in Fig. 5. After kernal32.dll is found, these functions can be
retrieved by going through AddressOfName, AddressOfOr-
dinals and AddressOfFunctions ﬁelds in Export Tables. Up
to now, Shell.dll can use GetModuleHandleA to acquire the
start address of malware.exe when loaded into memory.
Next, Shell.dll searches for all variable addresses by Reloc
Tables that need to be ﬁxed up in malware.exe. Virtual-
Protect and VirtualAlloc are used to change the values in
variable addresses. Finally, Shell.dll switches the execution
power to malware.exe.

Fig. 5: Assembly Code

After the development of Shell.dll, we read the values
of the PhysicalAddress, SizeOfRawData, and OEP ﬁelds
from malware.exe, and load Shell.dll into memory to
save these values and the encryption key in the extern
export global variable. Next, malware.exe needs to add
a section to store Shell.dll. The value of the NumberOf-
Sections ﬁeld in PE File Header of malware.exe should
be increased by 1. Correspondingly the four ﬁelds in the
last Section Table of malware.exe, namely VirtualSize,
VirtualAddress, SizeOfRawData, and PointerToRawData,
should be set to the value of SizeOfImage in the Im-
age Optional Header of Shell.dll and that of malware.exe,
the size of Shell.dll and that of malware.exe, respec-
tively. Besides, the execution power should be changed to
Shell.dll by revising the OEP of malware.exe to that of
Shell.dll. In order to change the OEP of malware.exe,
PE Editor has to retrieve the OEP and SizeOfImage ﬁelds
in Image Optional Header of malware.exe and Shell.dll.
Finally the OEP of malware.exe is changed to the OEP of
Shell.dll plus the SizeOfImage of malware.exe.

Last, PE Parser creates a buffer to save the modiﬁed
malware.exe, writes the buffer into disk, and outputs the
foxy malware. If the value of the perturbation path o(cid:48) is (1,
0, 0), Obfusmal is applied to malware.exe to generate an
adversarial malware example. The normal execution ﬂow
of the adversarial malware example is illustrated in Fig.6

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

9

(a). In Phase I, the OEP of the adversarial malware example
is directed to the start address of Shell.dll. In Phase II,
Shell.dll decrypts the code section, ﬁxes up the relocation
information, and jumps to the OEP of malware.exe. In
Phase III, the decrypted code section is executed.

Fig. 6: Process of Program Execution

4.3.2 Stealmal

To implement Stealmal, we need to carry out 3 steps:
(i) encrypt the entire malware.exe; (ii) develop a pro-
gram Shell.exe and add a section in Shell.exe to save
malware.exe, where Shell.exe is responsible for decrypting
the encrypted malware.exe, creating a suspended process,
obtaining the process space, copying malware.exe into the
space, ﬁxing up the relocation information, and changing
the context of the process to the OEP of malware.exe;
(iii) create a buffer with the length up to Shell.exe plus
malware.exe and write the buffer into disk. The last step
generates the foxy malware.

We use the encryption algorithm mentioned above to
encrypt malware.exe. The encrypted malware.exe is then
attached to the end of Shell.exe as a section. Thus, for
Shell.exe, the value of NumberOfSections ﬁeld in File
Header needs to be increased by 1, and PointerToRaw-
Data and SizeOfRawData ﬁelds in the last Section Table
should be ﬁlled with the size of Shell.exe and that of
malware.exe, respectively. Besides, VirtualAddress and Vir-
tualSize are respectively assigned values equal to SizeOfIm-
age in Image Optional Header of Shell.exe and the size of
malware.exe accordingly.

To decipher malware.exe, Shell.exe has to search its
own last section to locate the encrypted malware.exe
with the help of
the NumberOfSections ﬁeld in File
Header and the PointerToRawData and SizeOfRawData
ﬁelds in Section Tables. After successfully decrypting
malware.exe, Shell.exe creates a suspended process by

CreateProcess with the sixth parameter valued CRE-
ATE SUSPENDED, which aims to provide runtime envi-
ronment for malware.exe. To vacate the process space for
malware.exe, NtUnmapViewOfSection has to be called.
However, NtUnmapViewOfSection is a kernel-level appli-
cation programming interface stored in ntdll.dll, thus can’t
be directly called at the User Layer. To overcome this barrier,
we take a tactful strategy by loading ntdll.dll to obtain
the function address of NtUnmapViewOfSection, and then
deﬁning a function pointer with the same parameters as
NtUnmapViewOfSection and referencing it to the function
address. Speciﬁcally, the parameters are the handle and
loading address of the suspended process. The handle is
provided by CreateProcess, while the loading address has
to be retrieved from Process Environment Block (PEB). PEB
is a user-mode data structure that can be employed by
applications to get information such as the list of loaded
modules, process startup arguements, heap address, image
base address of imported DLLs, etc., [33]. One can acquire
the PEB by calling NtQueryInformationProcess, which is
also stored in ntdll.dll and can be obtained in the same way
as NtUnmapViewOfSection. After the suspended process is
uninstalled, malware.exe is copied into the space. Besides,
the loading address may be different from the preferred
address for malware.exe. The relocation information is
supposed to be ﬁxed up in the same way as that mentioned
in Obfusmal. Next, the context of the suspended process
should be changed to the OEP of malware.exe. Shell.exe
resumes the process, and the power to execute is switched
to malware.exe.

The last step operates similarly as Obfusmal, which
ﬁnally generates the foxy malware. If the value of the
perturbation path o(cid:48)
is (0, 1, 0), Stealmal is applied to
malware.exe to generate an adversarial malware example,
whose execution ﬂow is illustrated in Fig. 6 (b). In Phase I,
the OEP of the adversasrial malware example is directed
to the start address of Shell.exe. In Phase II, Shell.exe
decrypts malware.exe, creates a suspended process, vacates
the process space, copies malware.exe to the space, ﬁxes
up the relocation information, and changes the context
of the process to the OEP of malware.exe. In Phase III,
malware.exe is executed.

4.3.3 Hollowmal

Hollowmal is slightly different from Stealmal. It consists
of 3 steps: (i) select benignware, add a section in the
benignware with the length up to malware.exe, encrypt the
entire malware.exe, and attach the encrypted malware.exe
to the end of the benignware; (ii) develop a DLL named
Hollow.dll embracing similar functionality as Shell.exe,
and add a section in the benginware to store Hollow.dll
following malware.exe; (iii) create a buffer with the length
up to the total size of the benignware, malware.exe, and
Hollow.dll, and write the buffer into disk.

In our implementation, we choose a very small benign-
ware that only prints out ”Hello World”, and VirusTotal ex-
amine it as benign. To add a section in the benignware so as
to store the encrypted malware.exe, the values in the Num-
berOfSections, PointerToRawData, SizeOfRawData, Virtual-
Address, and VirtualSize ﬁelds of the last section have to

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

10

be modiﬁed. Then, we encrypt malware.exe employing the
same encryption algorithm as mentioned earlier.

To develop Hollow.dll, an extern export global vari-
able is deﬁned to save the encryption key, encryption size,
and the address of the benignware to which malware.exe
is attached. The functions of Shell.exe that decrypt
malware.exe, creat a suspended process, obtain the process
space, copy malware.exe into the space, ﬁx up the relo-
cation information, and change the context of the process
to the OEP of malware.exe can be moved to Hollow.dll,
but the section that needs to be decrypted is the second to
last section in the benignware. Furthermore, the benignware
adds another section attaching Hollow.dll to its end. The
values of the NumberOfSections, PointerToRawData, Size-
OfRawData, VirtualAddress, and VirtualSize ﬁelds in the
benignware are changed correspondingly.

The last step behaves similarly as that of Obfusmal.
Finally it generates the foxy malware. If the value of the
perturbation path o(cid:48) is (0, 0, 1), Hollowmal is applied to
malware.exe to generate an adversarial malware example,
whose execution ﬂow is illustrated in Fig. 6 (c). In Phase
I, the OEP of the adversarial malware example is directed
to the start address of Hollow.dll. In Phase II, Hollow.dll
decrypts malware.exe, creates a suspended process, vacates
the process space, copies malware.exe to the space, ﬁxes
up the relocation information, and changes the context
of the process to the OEP of malware.exe. In Phase III,
malware.exe is executed.

4.3.4 Combination

When the output of Generator is a path that involves more
than one perturbation method, PE Editor needs to combine
and execute all of them when generating the adversarial
malware example for malware.exe. For example, when
the value o(cid:48) = (1, 1, 0), PE Parser processes malware.exe
by employing Obfusmal to generate an intermediate ver-
sion of the adversarial malware example IntermediateAdv,
which should be the input to generate the ﬁnal version
of the adversarial malware example F inalAdv by Stealmal.
The execution ﬂow of F inalAdv is illustrated in Fig. 6
(d). In Phase I, the OEP of F inalAdv is directedd to the
start address of Shell.exe. In Phase II, Shell.exe decrypts
IntermediateAdv, creates a suspended process, vacates the
process space, copies IntermediateAdv to the space, ﬁxes
up the relocation information, and changes the context of
the process to the OEP of IntermediateAdv. In Phase III, the
OEP of IntermediateAdv is directed to the start address of
Shell.dll. In Phase IV, Shell.dll decrypts the code section,
ﬁxes up the relocation information, and jumps to the OEP
of malware.exe. In Phase V, the decrypted code section is
executed.

4.4 Discriminator

Discriminator estimates the probability that a malware pro-
gram is recognized as benign, and provides gradient infor-
mation to train Generator. Discriminator employs a CNN
algorithm with similar types of layers as Generator except
that the upsampling layers are substituted by max-pooling
layers. The structure of Discriminator employed by our
study for the dataset presented in Section 5 is shown in Fig.

Fig. 7: Network Structure of Discriminator

7, which contains 15 layers. CNN is adopted here because
it has demonstrated promising feature extraction capability
in many ﬁelds such as snapshot compressive imaging [34],
which could precisely distinguish benignware and malware
for our purpose.

Discriminator is constructed with a training dataset that
contains both benignware and malware. The input is a 1-D
vector obtained by concatinating the binary feature vectors
of all training programs. This vector is ﬁrst normalized via
BatchNormalization, then resized into a 3-D vector with a
shape of (n, n step, n input), where n is the number of
samples being fed into the network, and the other two
parameters are used to meet the shape requirements of
the ﬁrst convolutional layer. The ﬁrst convolutional layer
contains cf 1 zero padded ﬁlters and a convolution window
with a shape of (conv1, conv1). The activation function used
here is LeakyReLU with a slope coefﬁcient sc. The following
LeakyReLU activation functions use the same setting. One
can get the output with one more dimension in depth, i.e.,
X C
1 has the shape of (n, n step, n input, nf 1). Following
the ﬁrst convolutional layer is the ﬁrst max-pooling layer
with a stride st and a pooling size (ps, ps). After the
ﬁrst max-pooling layer, we have two sets of convolutional
layers, max-pooling layers and dropout layers. The two
max-pooling layers have the same settings as the ﬁrst max-
pooling layer, while the second and the third convolutional
layers are different in the number of ﬁlters, i.e., cf 2, cf 3,
and the third convolutional layer consists of a LeakyReLU
activation function. Besides, all dropout layers used in Dis-
criminator have the same drop rate r. The output of the
second dropout layer is an input to the fourth convolutional
layer with cf 4 ﬁlters, a convolution window shaped to
(conv1, conv1), and a LeakyReLU activation, before it is sent
to the third dropout layer. The output of the third dropout
layer is ﬂattened, and then sent to the ﬁrst fully connected
feed-forward layer with f c1 neurons and a l2 regularizer
r2e. Following the ﬁrst fully connected feed-forward layer
is the fourth dropout layer and the second fully connected
feed-forward layer consisting of f c2 neurons, a l2 regular-
izer re, and a LeakyReLU activation function where f c2 is
the number of antivirus products and online scan engines
VirusTotal has. The last dropout layer contains a sigmoid
activation function following the second fully connected
feed-forward layer. The output of the last dropout layer

LeakyReLU(sc)ConcatBatchNorm.ReshapeBinary MalwareFeaturesPerturbation Paths o' from Generatormfilter no. is nf1, conv. window size is (cs, cs)pooling size is (ps, ps), strd. is stfilter no. is nf2, conv. window size is (cs, cs)pooling size is (ps, ps), strd. is str(n, n_step, n_input)dr. rateis r1filter no. is nf3, conv. window size is (cs, cs)pooling size is (ps, ps), strd. is stdr. rateis r1filter no. is nf4, conv. window size is (cs, cs)dr. rateis r1dr. rateis r1LeakyReLU(sc)zLeakyReLU(sc)rLeakyReLU(sc)neuron no. is fc1, kernel_reg. is r2erneuron no. is fc2, kernel_reg. is r2edr. rateis r1rneuron no. is fc3sigmoidsoftmaxBenignConv. layerDropout layerFully-conn.feed-forward layerMax pooling layerMaliciousFlattenTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

11

is fed into the last fully connected feed-forward layer con-
sisting of a softmax activation function (see Eq. (9)), which
ﬁnally outputs the features with a shape ( f c3, ), where f c3
corresponds to the classiﬁcation of the programs.

X C

ij(cid:48) =

(cid:16)

(cid:17)

X C
exp
ij
Σk exp (cid:0)X C
ik

(cid:1) .

(9)

The loss function of Discriminator is deﬁned in Eq. (10),
where Sb is a set of benignware feature vectors, and Sm is a
set of malware feature vectors.

LD= −Esb∈Sb log(1 − D(sb)) − Esm∈Sm log(D(sm)). (10)

To precisely distinguish benignware and malware in the
dataset, LD should be minimized with regard to the weights
and parameters of Discriminator. Currently, one can see
that minimizing LG would lower the predicted malicious
probability of malware, and force Discriminator to recognize
malware as benign. In our design, Discriminator adjusts
itself to have the same detective capability as VirusTotal
to differentiate benignware and malware. As a result, the
continuous training of Generator to generate adversarial
malware examples needs to ﬁrst confuse Discriminator and
then further fool VirusTotal.

When employing Discriminator to test an adversarial
malware example, the input is the concatination of the
malware feature vector and the corresponding perturbation
path for that malware, and the output is either benign
or malicious indicating the produced adversarial malware
example is incorrectly or correctly detected.

5 EVALUATION
The metrics to measure the performance of MalFox are
accuracy, detection rate and evasive rate via Discriminator or
VirusTotal. Accuracy is the ratio of incorrectly predicted ad-
versarial malware examples (a) over all adversarial malware
examples (A) by Discriminator (see Eq. (11)). Detection rate
is the ratio of entities (n) that detect the malware or adver-
sarial malware example over all entities (N ) in VirusTotal
(see Eq. (12)). Evasive rate is computed by Eq. (13), where
Norig is the number of entities that detect the malware, and
Nadv is the number of entities that detect the corresponding
adversarial malware example. The higher the evasive rate,
the higher the probability of the adversarial example being
recognized as benign.

accuracy =

a
A

.

detection rate =

(11)

(12)

(13)

.

.

n
N
Norig − Nadv
Norig

evasive rate =

5.1 Experimental Setup

Datasets. In our experiment, we collected a malicious
dataset of 13425 malware from VirusShare [35] and a be-
nign dataset of 1719 benignware from Only Freeware [36],
SnapFiles [37], and downloadcrew [38]. As shown in Table
1, the test dataset of the Generator includes 1000 malware
that are randomly chosen from the malicious dataset while

TABLE 1: The Training and Test Datasets for the Generator

–
Training Dataset
Test Dataset

Malicious Dataset
12,425
1,000

Benign Dataset
1,719
0

TABLE 2: Options of the Perturbation Path

perturbation path
(0, 0, 0)
(1, 0, 0)
(0, 1, 0)
(0, 0, 1)
(1, 1, 0)
(1, 0, 1)
(0, 1, 1)
(1, 1, 1)

the training dataset includes all the benignware as well as
the leftover 12425 malware.

Both malware and benignware need to be screened to
ensure the relability of our dataset. For this purpose we
employ VirusTotal to test all of them, and regard a program
as malware if it is detected to be milicous by one of the
82 entities (antivirus software and online scan engies)1 in
VirusTotal while it is benign otherwise. Additionally PE
Parser makes use of 8 options (see Table 2) of the pertur-
bation path o(cid:48) according to the order of Obfusmal, Stealmal,
and Hollowmal discribed in Section 4.1. For example, when
o(cid:48) is (1,0,1), malware.exe is ﬁrst processed by Obfusmal and
then by Hollowmal while Stealmal is ignored, and when o(cid:48)
is (1,1,1), malware.exe is processed by all three perturbation
schemes in the order of Obfusmal, Stealmal, and Hollow-
mal. Note that in this experimental study, we instantiate
each framework method with only one implementation for
demonstration purpose only.

Codes are available upon request – we are happy to share
the codes with researchers who are interested in relevant
research. Written in C++, PE Parser consists of 541 lines of
code, and PE Editor consists of 2565 lines of code; written in
Python, Generator and Discriminator contains 850 lines of
code in total.

We conduct 30 independent simulation trials for statisti-
cal conﬁdence. The training process follows Algorithm 1.
The accuracies of MalFox during the training and test-
ing processes are evaluated via Discriminator. For the test
dataset, the detection rate and evasive rate of each malware
and its corresponding adversarial malware example are
computed by collecting the detection results after uploading
it to VirusTotal. At last, the averaged detection rate and
evasive rate are calculated.

Model Parameters. In order to identify a proper struc-
ture to realize our goals, we attempt the number of layers
for both Generator and Discriminator from 1 to 20. It turns
out that a structure with 13 layers for Generator and one
with 15 layers for Discriminator are the most effective ones
in terms of accuracy. Speciﬁcally, Generator includes 3 fully
connected feed-forward layers, 4 dropout layers, 3 upsam-
pling layers, and 3 convolutional layers; and Discriminator
includes 4 convolutional layers, 3 max-pooling layers, 5
dropout layers, and 3 fully connected feed-forward layers.

1. VirusTotal may contain more than 82 entities but the results of our

submissions indicate that 82 entities examined our programs.

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

12

Model parameter details for Generator and Discriminator
are presented in Table 3 and Table 4, respectively.

TABLE 3: Model Parameters for Generator

Parameters
m

|z|

ne1

re
r
ne2
α
conv1a
conv1b
conv1c
cf 1
conv1
u
cf 2
conv2
cf 3
conv3
ne3

Implications
num of features
num of perturbation
methods
num of entities in
VirusTotal
penalty
drop rate
num of neurons
slope coefﬁcient
shape size
shape size
shape size
num of ﬁlters
shape
repeatitions
num of ﬁlters
shape
n um of ﬁlters
shape
dimension

Values
16156

3

82

0.01
0.5
1248
0.1
8
13
12
32
5
2
64
2
256
2
3

TABLE 4: Model Parameters for Discriminator

Parameters
n
n step
n input
cf 1
conv1
sc

st

ps
cf 2
r
cf 3
cf 4
f c1
re
f c2
f c3

Implications
num of samples
shape
shape
num of ﬁlters
convolution window
slope coefﬁcient
num of pooling
window moves
pooling size
num of ﬁlters
drop rate
num of ﬁlters
num of ﬁlters
num of neurons
penalty
num of entities
num of classes

Values
32
143
113
512
2
0.1

1

2
256
0.5
64
32
1024
0.01
2
2

5.2 Evaluation Results

5.2.1 Accuracy

We ﬁrst evaluate the accuracy of MalFox with the collected
dataset that contains in total 15144 programs. When taking
Discriminator as the attack target, we compare the accuracy
of MalFox against 6 machine learning models (Random For-
est (RF), Logistic Regression (LR), Decision Tree (DT), Sup-
port Vector Machine (SVM), Multi-layer Perceptron (MLP),
and a voting based ensemble of these classiﬁers (VOTE)
[25]), and 6 deep learning models (LSTM, BiLSTM, LSTM-
Average, BiLSTM-Average, LSTM-Attention, and BiLSTM-
Attention [39]) that also employ generative adversarial
networks to produce adversarial malware examples. The
results indicate that our MalFox framework has an average
accuracy of 99.13% for the training dataset and 99.01%
for the test dataset, which outperform the contrastive ma-
chine learning models whose highest accuracy for training
is 97.89% and for testing is 95.64%, and the contrastive
deep learning models whose highest accuracy for training

Fig. 8: Evaluation Results on Accuracy

is 93.87% and for testing is 93.53%. Detailed comparison
results are shown in Fig. 8.

5.2.2 Detection Rate And Evasive Rate

In this subsection, we evaluate the performance of MalFox
via detection rate and evasive rate. Low detection rate and
high evasive rate indicate the effectiveness of adversarial
malware examples to avoid detection by VirusTotal. Since
each user normally installs one or two malware detectors on
its devices, our attacks are considered to be effective if the
adversarial malware example causes lower detection rate
than the malware, and has positive evasive rate. Therefore
it is reasonable for us to use the detection rate and evasive
rate to evaluate the performance of MalFox.

TABLE 5: Comparison Results

Evaluation Metrics
Detection Rate (Malware)
Detection Rate (Foxy Malware)
Evasive Rate (Foxy Malware)

Average Max Min
26.8
18.3
9.1

68.8
29.7
56.2

85.4
43.9
74.6

Anti-virus products and scan engines in VirusTotal were
developed by different companies or institutes, and they
examine malware based on different techniques. Therefore
for single malware, it might be detected by some entities,
but escape detection from others. The detection rate cannot
reach 100% for the malware. When comparing malware
and their adversarial malware examples in Table 5, one
can see that the average detection rate for malware has

THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

13

dropped dramatically by about 56.8% after being processed
by MalFox. Correspondingly, the average evasive rate for
adversarial malware examples has been signiﬁcantly im-
proved (about 56.2%). Speciﬁcally, as suggested in Fig. 9,
the detection rate for malware ranges from 26.8% to 85.4%.
However, the detection rate of their adversarial malware
examples are far lower than the original malware, which is
between 18.3% and 43.9%. Speaking of single malware, the
detection rate for its corresponding adversarial malware ex-
ample has a signiﬁcant decline in most cases, and the largest
drop even reaches 61.0%. Moreover, the highest evasive rate
is 74.6%, which means that 74.6% of the detection entities for
the malware report the corresponding adversarial malware
example as benign, though they previously label the original
malware malicious.

[3] H. Qiu, T. Dong, T. Zhang, J. Lu, G. Memmi, and M. Qiu, “Adver-
sarial attacks against network intrusion detection in iot systems,”
IEEE Internet of Things Journal, pp. 1–1, 2020.

[4] C. Szegedy, W. Zaremba, L. Sutskever, J. Bruna, D. Erhan, I. Good-
fellow, and R. Fergus, “Intriguing properties of neural networks,”
https://arxiv.org/abs/1312.6199, 2014.

[5] H. Zhang, H. Zhou, N. Miao, and L. Li, “Generating ﬂuent
adversarial examples for natural languages,” https://arxiv.org/
abs/2007.06174, 2020.
J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, “Hotﬂip: White-box
adversarial examples for text classiﬁcation,” https://arxiv.org/
abs/1712.06751, 2018.

[6]

[7] H. Qiu, C. Xiao, L. Yang, X. Yan, H. Lee, and B. Li, “Semanticadv:
Generating adversarial examples via attribute-conditional image
editing,” https://arxiv.org/abs/1906.07927, 2020.

[8] M. Alzantot, B. Balaji, and M. Srivastava, “Did you hear that?: Ad-
versarial examples against automatic speech recognition,” https:
//arxiv.org/abs/1801.00554, 2018.

[9] Z. Kong, J. Guo, A. Li, and C. Liu, “Physgan: Generating physical-
world-resilient adversarial examples for autonomous driving,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR).

IEEE, 2020, pp. 14 254–14 263.

Singh

and
Bypassing

[10] “Virustotal,” https://www.virustotal.com/gui/, 2020.
[11] A.
ter:
www.ﬁreeye.com/blog/threat-research/2013/08/
hot-knives-through-butter-bypassing-ﬁle-based-sandboxes.html,
2013.

but-
https://

sandboxes,”

ﬁle-based

through

knives

“Hot

Bu,

Z.

[12] D.

Bisson,

“The

four most

by

used

niques
com/state-of-security/security-data-protection/
the-four-most-common-evasive-techniques-used-by-malware/,
2015.

malware,”

common

tech-
evasive
https://www.tripwire.

Fig. 9: Evasive Rate and Detection Rate

6 CONCLUSION AND FUTURE RESEARCH

In this paper, we propose a novel Conv-GAN-based adver-
sarial malware example generation framework titled Mal-
Fox, which can transform a malware program into a foxy
one with a signiﬁcantly higher chance of evading detec-
tion. MalFox is ﬂexible to accept any number and type of
perturbation methods, thus enlarging the options for more
effective perturbation paths to enhance malware ability of
escaping detection. On the other hand, MalFox successfully
attacks practical malware detectors without knowing their
underlying implementation details, which renders it a more
practical and powerful attack framework.

In our future research, we will consider more perturba-
tion methods as well as other powerful evasion tricks such
as junk instructions and anti-debugging [40], which can be
added into adversarial malware examples to improve their
evasive abilities. We also intend to conduct fuzzing tests [41]
with adversarial malware examples generated by different
perturbation paths to expose weaknesses of commercial
anti-virus products or scan engines, which would contribute
to enhance their reliability.

REFERENCES

Yener,

“Avleak:

[13] J. Blackthorne, A. Bulazel, A. Fasano, P. Biernat,

and
emulators
B.
10th USENIX Workshop
through
on Offensive Technologies (WOOT 16). USENIX Association,
2016. [Online]. Available: https://www.usenix.org/conference/
woot16/workshop-program/presentation/blackthorne

Fingerprinting

black-box

antivirus

testing,”

in

[14] F. J. S. Rodrigues, “Dissection of modern malicious software,”

https://ubibliorum.ubi.pt/handle/10400.6/6169, 2014.

[15] P. Ducklin, “20 years ago today! what we can learn from the
https://nakedsecurity.sophos.com/2018/04/26/

cih
20-years-ago-today-what-we-can-learn-from-the-cih-virus/,
2018.

virus...”

[16] M. Sharif, A. Lanzi, J. Gifﬁn, and W. Lee, “Impeding malware
analysis using conditional code obfuscation,” in The Network and
Distributed System Security Symposium (NDSS).
Internet Society,
2018, pp. 1–13.

[17] S. Schrittwieser, S. Katzenbeisser, P. Kieseberg, M. Huber, M. Leith-
ner, and M. Mulazzani, “Covert computation: hiding code in code
for obfuscation purposes,” in Proceedings of the 8th ACM SIGSAC
symposium on Information, computer and communications security.
ACM, 2013, pp. 529–534.

[18] K. K. Ispoglou and M. Payer, “malwash: Washing malware
in 10th USENIX Workshop
to evade dynamic
on Offensive Technologies
(WOOT16). USENIX Association,
2016. [Online]. Available: https://www.usenix.org/conference/
woot16/workshop-program/presentation/ispoglou

analysis,”

execution

independent malware

[19] J. Pavithran, M. Patnaik, and C. Rebeiro, “D-time: Distributed
threadless
runtime
obfuscation,” in 13th USENIX Workshop on Offensive Technologies
(WOOT19). USENIX Association, 2019. [Online]. Available: https:
//www.usenix.org/conference/woot19/presentation/pavithran
[20] R. L. Castro, C. Schmitt, and G. Dreo, “Aimed: Evolving malware
with genetic programming to evade detection,” in 2019 18th IEEE
International Conference On Trust, Security And Privacy In Computing
And Communications/13th IEEE International Conference On Big Data
Science And Engineering (TrustCom/BigDataSE), 2019, pp. 240–247.

for

[1]

J. Zhang and C. Li, “Adversarial examples: Opportuniteis and
challenges,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 31, pp. 2578–2593, 2020.

[2] Y. Li, Y. Song, L. Jia, S. Gao, Q. Li, and M. Qiu, “Intelligent fault
diagnosis by fusing domain adversarial training and maximum
mean discrepancy via ensemble learning,” IEEE Transactions on
Industrial Informatics, vol. 17, no. 4, pp. 2833–2841, 2021.

[21] I. Zelinka, S. Das, and R. Senkerik, “Swarm virus- next-generation
virus and antivirus paradigm?” https://www.sciencedirect.com/
science/article/abs/pii/S221065021730768X, 2018.

[22] Z. Fang, j. Wang, B. Li, S. Wu, Y. Zhou, and H. Huang, “Evading
anti-malware engines with deep reinforcement learning,” IEEE
ACCESS, vol. 7, no. 18615933, pp. 48 867–48 879, 2019.

[23] K. Grosse, P. Nicolas, P. Manoharan, M. Backes, and P. McDaniel,

0 20 40 60 80 100 02004006008001000Rate (%)Detection Rate of the Original MalwareDetection Rate of the Adversarial Malware ExampleEvasive RateTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON COMPUTERS IN AN UPCOMING ISSUE FOR PUBLICATION

14

“Adversarial perturbations against deep neural networks for mal-
ware classiﬁcation,” https://arxiv.org/abs/1606.04435, 2016.
[24] N. Papernot, P. McDaniel, and I. Goodfellow, “Practical black-box
attacks against machine learning,” https://arxiv.org/pdf/1602.
02697.pdf, 2017.

[25] W. Hu and Y. Tan, “Generating adversarial malware examples
for black-box attacks based on gan,” https://arxiv.org/abs/1702.
05983, 2017.

[26] “Posts

tagged: virustotal,” https://krebsonsecurity.com/tag/

virustotal/, 2020.

[27] “Aspack software,” http://www.aspack.com/, 2020.
[28] “Bitsum technologies compression, win32 pe tools, power utilities
empowering your software to be smaller, faster, and more efﬁ-
cient,” https://bitsum.com/pecompact.htm, 2020.

campaign’s

[29] Y. Xiao, Y. Jia, C. Liu, X. Cheng, J. Yu, and W. Lv, “Edge computing
security: State of the art and challenges,” Proceedings of the IEEE,
vol. 107, no. 8, pp. 1608–1631, 2019.
[30] S. Maniath and P. K. Kadam,
of

phishing
https://www.ﬁreeye.com/blog/threat-research/2019/03/
dissecting{-}netwire{-}phishing{-}campaign{-}usage{-}of{-}
process{-}hollowing.html, 2019.
[31] “New cryptojacking malware

campaign evades detection
using process hollowing,” https://cyware.com/news/new{-}
cryptojacking{-}malware{-}campaign{-}evades{-}detection{-}
using{-}process{-}hollowing{-}50ad13a9, 2019.

a netwire
hollowing,”

“Dissecting
process

usage

[32] “Bitpaymer

ransomware

framework against
morphisec.com/bitpaymer{-}ransomware{-}with{-}new{-}
custom{-}packer{-}framework, 2019.

targets

across

leveraging new custom packer
the u.s.” https://blog.

[33] “Peb-process-environment-block,”

https://www.aldeid.com/

wiki/PEB-Process-Environment-Block, 2020.

[34] X. Han, B. Wu, Z. Shou, X.-Y. Liu, Y. Zhang, and L. Kong,
“Tensor ﬁsta-net for real-time snapshot compressive imaging,” in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2020, pp.
10 933–10 940.

[35] “Virusshare.com-because sharing is caring,”

https://virusshare.com/, 2020.

[36] “Only freeware,”

http://www.onlyfreewares.com/, 2020.

[37] “Latest new releases and updates,”

https://www.snapﬁles.com/new/list-whatsnew.html, 2020.

[38] “The ﬁnest hand-selected downloads,”
https://downloadcrew.com/, 2020.

[39] W. Hu and Y. Tan, “Generating adversarial malware examples
for black-box attacks based on gan,” https://arxiv.org/abs/1702.
05983, 2017.

[40] D. Shen, S. L. Song, A. Li, and X. Liu, “Cudaadvisor:
Llvm-based runtime proﬁling for modern gpus,” in Proceedings
the 2018 International Symposium on Code Generation and
of
Optimization, CGO 2018, V¨osendorf
/ Vienna, Austria, February
24-28, 2018, J. Knoop, M. Schordan, T. Johnson, and M. F. P.
O’Boyle, Eds. ACM, 2018, pp. 214–227. [Online]. Available:
https://doi.org/10.1145/3168831

[41] X. Zhu, S. Wen, A. Jolfaei, M. Sayad Haghighi, S. Camtepe, and
Y. Xiang, “Vulnerability detection in siot applications: A fuzzing
method on their binaries,” IEEE Transactions on Network Science and
Engineering, pp. 1–10, 2020.

