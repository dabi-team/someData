VELTAIR: Towards High-Performance Multi-tenant Deep
Learning Services via Adaptive Compilation and Scheduling

Zihan Liu, Jingwen Leng*, Zhihui Zhang, Quan Chen, Chao Li, Minyi Guo*
Emerging Parallel Computing Center, Shanghai Jiao Tong University
Shanghai Qi Zhi Institution
{altair.liu, leng-jw, zhihui-zhang}@sjtu.edu.cn, {chen-quan, lichao, guo-my}@cs.sjtu.edu.cn

2
2
0
2

n
a
J

7
1

]

C
D
.
s
c
[

1
v
2
1
2
6
0
.
1
0
2
2
:
v
i
X
r
a

Abstract
Deep learning (DL) models have achieved great success in
many application domains. As such, many industrial compa-
nies such as Google and Facebook have acknowledged the
importance of multi-tenant DL services. Although the multi-
tenant service has been studied in conventional workloads, it
is not been deeply studied on deep learning service, especially
on general-purpose hardware.

In this work, we systematically analyze the opportunities
and challenges of providing multi-tenant deep learning ser-
vices on the general-purpose CPU architecture from the as-
pects of scheduling granularity and code generation. We
propose an adaptive granularity scheduling scheme to both
guarantee resource usage efﬁciency and reduce the scheduling
conﬂict rate. We also propose an adaptive compilation strategy,
by which we can dynamically and intelligently pick a program
with proper exclusive and shared resource usage to reduce
overall interference-induced performance loss. Compared to
the existing works, our design can serve more requests under
the same QoS target in various scenarios (e.g., +71%, +62%,
+45% for light, medium, and heavy workloads, respectively),
and reduce the averaged query latency by 50%.

Keywords: Multi-tenant, Deep Learning Service, Compil-

ing, Scheduling

1. Introduction

Deep learning (DL) models have achieved great success in
the various domains including vision [1, 2, 3, 4, 5, 6], natural
language processing [7, 8], and even graph learning [9, 10].
To meet the need of rising computation power of DL models,
computer architects have proposed various hardware designs
including general-purpose hardware [11] and domain-speciﬁc
architectures [12, 13, 14, 15, 16, 17, 18, 19, 20, 21] for acceler-
ating deep learning models for their superior energy efﬁciency.
Different from the computation-heavy training process, it
is difﬁcult for the inference of a single deep learning model
to fully use the hardware, which typically runs with a small
batch size [22]. As such, sharing multiple DL models on
a single hardware, i.e., multi-tenant deep learning serving,
has become increasingly important [23, 24]. Compared to

∗Jingwen Leng and Minyi Guo are the corresponding authors of VELTAIR

the single-tenant serving, multi-tenancy brings several chal-
lenges, including resource management and allocation, shared
resource competition [25, 26], tasks scheduling [27, 28], etc.
For conventional multi-tenant workloads, researchers have
proposed various solutions based on resource partition [29],
hardware isolation [30], and so on. Similarly, researchers
have proposed various architectural support for multi-tenant
DL serving [22, 24, 23] that leverages temporal and spatial
multitasking.

However, the multi-tenant DL serving has its unique chal-
lenges, which are overlooked by previous multi-tenant DL
serving works. We ﬁrst ﬁnd that owing to the complex
inner-structure of the DL models [31], the scheduling gran-
ularity has a profound impact on the multi-model serving
throughput. Meanwhile, we demonstrate that the performance
of DL models is very sensitive to code generation strate-
gies [32, 33, 34, 35]. In speciﬁc, those current DL compilers
mainly focus on optimizing the performance of a single model
or even a single layer by various code transformations under
the assumption of singe-tenancy. Our experimental results
show that the performance of generated code degrades rapidly
under multi-tenant scenarios due to the shared resource con-
tention.

In this work, we propose VELTAIR, a software solution
that provides the high-throughput and low-interference multi-
tenant deep learning serving. We systematically analyze the
resource allocation conﬂict and inter-layer interference on
the CPU platform, which closely represents the industrial
practice [36]. Our analysis indicates that the ﬁxed scheduling
granularity adopted by previous works [23, 24] is sub-optimal
when the system load changes. Meanwhile, we perform a
naive extension to the TVM’s auto-scheduler [32], which lets
us identify the best-performing code version under different
interference levels. We show that the performance of the
best code version under a speciﬁc interference level degrades
quickly under a different interference level. These insights
call for both adaptive scheduling and adaptive compilation for
achieving the high-performance multi-tenant DL serving.

For the adaptive scheduling, we ﬁnd that the sub-optimal
performance of the ﬁxed model-wise scheduling scheme is
caused by the inefﬁcient CPU resource utilization, while the
ﬁxed layer-wise scheduling scheme is caused by the frequent
resource conﬂict. To reduce the resource conﬂict with CPU
resource usage efﬁciency guaranteed under different situa-

1

 
 
 
 
 
 
tions, we propose a layer-block granularity scheduling strat-
egy, which is ﬁner than the model-wise scheduling but coarser
than layer-wise scheduling. By setting a dynamic threshold,
we can achieve both low conﬂict possibility and high CPU
resource usage efﬁciency.

For the adaptive compilation, we analyze the relationship be-
tween the interference-prone code version and the interference-
tolerant code version for a set of deep learning layers. We ﬁnd
that those different versions essentially lie in the Pareto fron-
tier of trade-off space between parallelism and locality. Given
this insight, we propose a single pass compiling strategy based
on the existing auto-scheduler. The extended auto-scheduler
is able to compile multiple versions of implementations that
are suitable for different system interference pressure levels.
To evaluate our design, we choose various workloads from
the industry-level MLPerf [37] benchmark ranging from light
to heavy workload and compare with the existing multi-tenant
DL serving solution, Planaria [23]. Compared to the existing
work, our design serves more requests under the same QoS
target in various scenarios (e.g., +71%, +62%, +45% for light,
medium, and heavy workloads, respectively), and reduce the
averaged query latency by 50%.

To summarize, we make the following contributions in this

work.
• We analyze and identify the performance-critical optimiza-
tion knobs for multi-tenant DL services, including the adap-
tive scheduling and the adaptive compilation (Sec. 3).

• We propose a static multi-version compiler that extends the
existing TVM’s compilation framework and can identify
different optimal code versions under different interference
levels. The key novelty in our compiler is a multi-version
search algorithm in a single pass (Sec. 4.1).

• We propose a runtime scheduler design that dynamically
forms a layer-block as the scheduling unit. The scheduler
uses a dynamic threshold-based layer-block formation algo-
rithm to balance the resource usage efﬁciency and schedul-
ing conﬂict rate (Sec. 4.2).

• We evaluate and compare the proposed ideas in VELTAIR,
where the combined adaptive compilation and scheduler can
improve the system by 45% - 71% in different workload
mixes. We also show that the query execution latency in
our design is within 10% gap of the isolated execution case,
meaning VELTAIR is close to the performance upper bound
on the studied hardware platform (Sec. 5).

2. Motivation and Challenges for Multi-Tenant

DNNs

In this section, we explain why the CPU architecture is suit-
able for providing the multi-tenant DL services. In speciﬁc,
we show that the existing high-performance CPU is more than
enough to serve multiple deep learning inference tasks un-
der their QoS target. We then show that the quality of code
generation is the key to fully unleashing the potential of the

Figure 1:
(a) All models in MLPerf vision can meet the QoS
target by using a few cores. (b) Performance slowdown when
simply co-locating multiple tasks together.

underlying hardware.

2.1. DNN Execution Characterization on CPU

With the tremendous improvement of hardware architecture
design and manufacturing process, the performance of single
computing hardware is increasing rapidly, making training
and inference of deep neural networks easier and faster. Some
companies even use CPUs as their deep learning back-end.
These hardware mostly use multi-degree parallelism to in-
crease overall throughput. However, when providing deep
learning inference services with small batch sizes, these hard-
ware will suffer from severe under-utilization since the deep
learning inference service is not intense enough to ﬁll the
hardware resource.

As illustrated in Fig. 1a, a high-performance CPU (AMD
Threadripper 3990X [38]) is more than enough to provide
deep learning inference tasks. When serving vision tasks in
MLPerf [37], the CPU platform can reach around 300 Query
per Second by simply using all CPU cores for a task. So,
to fully utilize the hardware and increase the energy efﬁ-
ciency, increasing deep learning service providers begin to
introduce the task-level parallelism by sharing one computing
hardware among multiple customers/requests which is called
multi-tenant deep learning service, by either temporal mul-
tiplexing (e.g. PREMA [24], AI-MT [22]) or spatial multi-
plexing (e.g. NVIDIA Multi-Process Service [39], NVIDIA
Ampere Multi-Instance GPU [40]). By leveraging task-level
parallelism, multiple customers/requests can fully occupy the
throughput of the hardware, thus increasing the overall efﬁ-
ciency.

On the other hand, some deep learning tasks also consist of
multiple sub-tasks. For example, auto-piloting on a smart ve-
hicle consists of multiple direction object sensing and tracking
tasks, SLAM tasks, decision-making tasks, etc.; personal voice
assistant service on a home device consists of voice recogni-
tion, voice synthesis, etc. Those sub-tasks can but also should
be launched in a parallel way for real-time interaction, and
thus may share the resources on a single computing hardware.
Currently, few deep learning systems are designed to face the
multi-tenant serving scenarios. So in this work, we propose
to explore the optimization opportunities at both compiling

2

(a)Inference Latency:ms07.51522.530Core Number Used8163264ResNet-50GoogLeNetEfficientNetMobileNet[QoS] Heavy[QoS] Light(b)Perf. Downgrade0.811.21.41.61.82Co-locating Tasks Number1234ResNet-50GoogLeNetBertAverageTable 1: Optimization strategies in VELTAIR and prior works.

Multiplexing

Granularity

Compilation

Work

Temporal

Spatial

Static (Model)

Static (Layer)

Static (Model)

Static (Model/Layer)

Static

Static

PREMA [24]

AI-MT [22]

Planaria [23]

Parties [29]

Static (Model/Layer)

Adaptive

Protean [44]

Adaptive (Layer Block)

Adaptive

VELTAIR (ours)

MKL-DNN [41] and MLAS [42] on CPU, cuDNN [43] on
GPU.

The other advantage of using the DNN compiler is that the
generated code is user-visible while vendor-supplied libraries
are usually closed-sourced. Given those reasons, we choose
the TVM compiler to generate the codes for running DNN
models in this work. We also conduct a performance com-
parison experiment between the Intel MKL-DNN [41] and
TVM. As Fig. 2 shows, the TVM generally outperforms the
vendor-supplied library.

For compiling DNN layers or models on the CPU, we
mainly consider the nested loop transformation and some CPU-
speciﬁc annotation or pragma including parallelization and
unrolling. The compiling procedure is actually a trade-off
between the parallelism and locality of the program, which we
will discuss later in the paper.

3. Optimization Space Analysis

In this section, we ﬁrst identify the optimization space that
is critical for achieving high-performance multi-tenant DL
services. In speciﬁc, we study the two optimization knobs,
namely the scheduling granularity and compilation strategy.

We characterize the impact of those two knobs on the per-
formance measured by QoS satisfaction rate [45, 46, 29, 28]
representing how many requests are ﬁnished within the QoS
target of multi-tenant deep learning services on the CPU.

Our main ﬁnding is two-fold. First, a ﬁxed scheduling
granularity, such as the entire model [24] or the sub-layer
block [22, 23], leads to the sub-optimal performance, owing to
the diversity of DNN models and their distinctive inner charac-
teristics. Second, the performance of the existing compilation
strategies, which aim to maximize the code performance under
the solo-run case, degrades signiﬁcantly when multiple DNN
models run together and interfere with each other. Such two
ﬁndings motivate the design of the adaptive scheduling and
adaptive compilation in VELTAIR.

3.1. Optimization Space Deﬁnition

We ﬁrst explain and deﬁne the optimization space of multi-
tenant DL services. Conventional workloads such as Silo [47]
and Moses [48] choose the entire query as the scheduling
unit because the query has no internal structures. In contrast,
DNNs are layer-based, for which the scheduling unit can range
from one layer to the entire model. Meanwhile, DNNs are

Figure 2: Performance comparison between vendor-supplied
MKL-DNN library and TVM compiler.

and runtime for co-locating and scheduling multiple tasks on
a single hardware. Speciﬁcally, we focus on the problem of
co-locating multiple latency-critical DL tasks on a multi-core
architecture hardware, and the objective is to serve as many
DL tasks as possible (i.e., maximize the metric of query per
second) under the task latency constraints (i.e., ensuring that
it ﬁnishes within a time limit). However, our design can be
easily extended to support the co-location of DL tasks and
best-effort tasks.

To co-locate multiple deep learning tasks on a single com-
puting back-end, one naive approach is to simply dump all
the candidate tasks onto the hardware and ﬁll the empty slot
once a task is complete. However, the most important chal-
lenge is how to manage the limited hardware resources like
physical cores for the CPU architecture, streaming multipro-
cessors (SMs) for the GPU architecture, or even sub-arrays of
a systolic architecture.

In addition to exclusive resources, various shared resources
on the computing back-end are critical to the performance
of a task, including cache bandwidth, cache capacity, mem-
ory bandwidth, etc. Naively scheduling all candidate tasks
to the hardware would result in severe interference and per-
formance loss due to the competition of these resources. We
conduct a simple experiment that co-locates multiple ResNet-
50, GoogLeNet, and SSD inference tasks on a single CPU. As
illustrated in Fig. 1b, the task suffers from up to 1.8× latency
under heavy workload pressure. As such, the inference can
severely impact the QoS, but is considered by current DL serv-
ing systems. In contrast, our work considers both compilation
and scheduling strategies to handle the interference.

2.2. DNN Compilation on CPU

Although vendor-provided libraries can offer optimized DNN
computation with convenient APIs, an increasing number of
researchers and developers have begun to use automatic high-
performance code generators for even higher performance.
Among deep learning compilers, TVM gains great success
for its convenience, high quality of generated code, and cross-
platform capability. Moreover, things are getting more con-
venient once the TVM Auto-Scheduler (i.e., Ansor [34]) is
introduced. Now, researchers can simply deﬁne the compu-
tation logic they want, run the auto-scheduling procedure,
and TVM would return the code with similar or even better
performance compared to vendor-provided libraries, such as

3

Latency:ms 0481216ModelResNet-50GoogLeNetMobileNetEfficientNetTVMMKL-DNNFigure 3: Performance comparison of different scheduling
granularities under different query arrival rate.
(a) QoS sat-
isfaction rate. (b) Average query latency.

also computation-intensive, and their performances are sen-
sitive to the code quality as shown in Sec. 2.2. In this work,
we consider these two knobs jointly, i.e., scheduling granular-
ity and compilation strategy, for achieving high-performance
multi-tenant deep learning services.

Scheduling granularity refers to the size of the entity
for allocating resources and scheduling on the hardware.
For example,
in the conventional online services, prior
works typically choose the entire query as the scheduling
unit [29, 44, 27, 30]. However, we have more choices on
the scheduling granularity in deep learning services because
DNN models have a complex inner organization consisting
of layers or operators, such as conv (i.e., convolution), relu
and pooling. As such, we can either choose an entire model
(i.e., coarse-grained) or a single layer (i.e., ﬁne-grained) as the
scheduling unit. To achieve higher resource usage efﬁciency
and reduce the resource usage conﬂict, we consider a new
scheduling granularity of multiple layers as a unit, which we
call layer block.

Compilation strategy refers to the code generation options
for a DNN model or a DNN layer. For example, we have de-
scribed the different code generation options (i.e., nested loop
transformation) in Sec. 2.2. As we will show later, the optimal
code generation option for minimizing the execution latency
depends on the interference levels caused by co-running DNN
models. To the best of our knowledge, we are the ﬁrst to
consider the compilation as an optimization knob in the multi-
tenant DNN serving scenario.

Tbl. 1 compares the choices of those two optimization knobs
in prior state-of-the-art solutions against VELTAIR. Specif-
ically, our work is adaptive in both the scheduling granu-
larity and compilation strategy. Previous work AI-MT [22]
and Planaria [23] decompose a layer into multiple smaller
parts, or sub-layers, for more ﬂexible scheduling. However,
the improvement is limited as we will show that the layer-
wise scheduling unit is already inferior to our adaptive block
scheduling in Sec. 3.2. In other words, sub-layer scheduling is
overly ﬁne-grained. Other work Protean [44] and Parties [29]
mainly target conventional interactive services, so both of
them use static scheduling. The static scheduling can either
choose the layer or the entire model in the case of multi-tenant

4

Figure 4:
(a) Speedup trend of increasing core number for
4 different conv layers selected from the ResNet-50 model.
(b) Core number allocation comparison for different schedul-
ing granularities under the same QoS target. The layer-wise
scheduling approach (red shadowed area) represents the min-
imum required core number for meeting the QoS target.

DL services. Note that Protean [44] also uses an adaptive com-
pilation strategy, only targeting non-DL workloads. As such,
it can not be applied to compile DNN models because they
have a different set of compiler optimization options as we
will show later. In the following parts of this section, we will
justify the choice of our optimization knobs through detailed
experimental results.

3.2. Scheduling Granularity Analysis

We ﬁrst compare the performance of multi-tenant DL services
under different scheduling granularities, including layer-wise,
model-wise, and layer-block scheduling. We then explain
why these static schedulings fail to fully utilize the hardware
resources, which leads to the need for an adaptive scheduling
granularity.

Experimental Setup. For the model-wise scheduling, we
implement a simple First Come First Serve (FCFS) strategy
used in prior work [49, 28]. In other words, the tasks will be
served immediately if there are available resources while wait-
ing otherwise. For the layer-wise scheduling, we implement
an algorithm similar to Planaria [23] that allocates the resource
to every layer and allow tile-wise preemption if the requested
resources exceed the available number. For the layer block
scheduling, we simply set the layer block-size to 6 and 11 re-
spectively to study the impact of the block-size. We compare
the performance of those scheduling schemes under different
query arrival rates (i.e., query per second, QPS). We report the
QoS satisfaction ratio in Fig. 3a and averaged model execution
latency in Fig. 3b as evaluation metrics. For the fair compari-
son of different scheduling strategies, we run a total number
of 30, 000 ResNet-50 models with identical uniform arriving
times to eliminate the instability caused by the randomness.

Results. As shown in Fig. 3a, the performance of both
model-wise and layer-wise scheduling degrade much faster
than the block-wise scheduling. Meanwhile, the best block-
size for optimal performance varies with the query arrival rate.
For example, the block-size of 6 layers performs best at 150
QPS, while the block-size of 11 is better at 200 QPS. We have
the same observations in Fig. 3b for the averaged query execu-

(a)Satisfaction Rate: %0%25%50%75%100%Query Per Second50100150200250300ModelLayerBlock(6)Block(11)(b)Latency: ms05101520253035Query Per Second50150300ModelLayerBlock(6)Block(11)8162432404856CPU cores012345678Speed-up ratio(a)Size:56×56, C:(64,64), Ker=1×1Size:224×224, C:(3,64), Ker=7×7Size:7×7, C:(512,1024), Ker=1×1Size:56×56, C:(64,64), Ker=3×3010Time0102030405060CPU requirement(b)ModelBlock(6)Block(11)LayerFigure 5: (a) Scheduling conﬂict rate comparison for different
scheduling granularities under different query arrival rates. (b)
Measured per-layer conﬂict scheduling overhead.

Figure 6: (a) Performance of four versions of the same layer
under different interference levels. (b) The best performance
(dotted line) is achieved by combining all four versions.

tion latency. These results conﬁrm the criticality of scheduling
granularity for multi-tenant DL services.

Model-Wise Inefﬁciency. We ﬁnd that the distinctive com-
putation resource requirement across DNN layers is the root
cause for why the model-wise scheduling is sub-optimal.
Fig. 4a plots the speedup for different ResNet layers under
different CPU core numbers, which shows that different layers
have different scalability trends when allocated core number
increases. However, the model-wise scheduling evenly assigns
a ﬁxed number of cores to all layers in the model, which results
in the CPU core resource wastage because many layers only
require a small number of cores. Fig. 4b compares the core
number allocation between the model-wise scheduling and
layer-wise scheduling. Intuitively, the layer-wise scheduling
scheme represents the minimum core allocation for satisfying
the model’s QoS target. We ﬁnd that the model-wise scheme
allocation (black line) is far from the optimal core allocation
(red shadowed area). As a result, the QoS satisfaction ratio
drops dramatically once the query arrival rate exceeds 50 QPS
in Fig. 3a.

Layer-Wise Inefﬁciency. We ﬁnd that
the layer-wise
scheduling is sub-optimal owing to the frequent scheduling
conﬂict when the query arrival rate is high. For example, there
are layers in Fig. 4b that require large core numbers (e.g., more
than 48 out of 64 cores). The scheduling conﬂict occurs when
a layer requests more cores than currently available cores.
Fig. 5a compares the conﬂict rate among different scheduling
granularities, where the layer-wise scheduling is highest (e.g.,
23.8% conﬂict rate with 300 QPS).

For a layer that experiences scheduling conﬂict, we im-
plement a technique to increase the resource utilization. In
speciﬁc, we ﬁrst let the layer use all the available cores and in-
crease its core usage once more cores become available. How-
ever, using more cores needs to spawn more threads, whose
overhead is non-negligible and worsens the model’s overall
latency. To illustrate this point, we quantify this overhead for
each layer in ResNet-50 by measuring a layer’s latency with
and without scheduling conﬂict. Fig. 5b shows the results,
with the mean of 220 µs and median of 100 µs.

The scheduling conﬂict overhead measured above explains
the overall latency of the layer-wise scheduling for ResNet-50

at the 300 QPS. The execution latency without scheduling
conﬂict is 18.54 ms. But with the conﬂict rate of 23.8%,
the total conﬂict overhead is estimated to be 23.8% × 55 ×
220 µs = 2.86 ms, with the 55 layers (53 conv and 2 GEMM)
in ResNet-50. As such, the estimated overall latency is 2.86 +
18.54 = 21.4ms, which matches the measured latency for the
layer-wise scheduling at 300 QPS in Fig. 3b.

Summary. The above results show that the optimal schedul-
ing scheme should strike a good balance between the averaged
resource usage and the scheduling conﬂict. The model-wise
scheduling generates the smooth resource usage pattern and
hence low conﬂict, but uses unnecessarily more resources to
meet the QoS target. In contrast, layer-wise scheduling uses
the minimum CPU resources, but the per-layer characteristics
lead to a substantial scheduling conﬂict overhead. The layer
block scheduling combines the advantages of both model-wise
and layer-wise scheduling. Furthermore, the optimal perfor-
mance cannot be achieved by simply setting a ﬁxed layer block
size as demonstrated in Fig. 3. In other words, the optimal
block organization depends on the model characteristics and
query arrival rate. As such, we propose to use adaptive layer
block-size, and will explain it with greater details in later
sections.

3.3. Compilation Strategy Analysis

We now perform a set of experiments to study the impact
of compilation strategies on multi-tenant deep learning ser-
vices. The key insight in our experiments is that the optimal

Figure 7: (a) Performance loss of retaining different numbers
of versions compared against retaining all ten versions under
different interference levels. (b) Distribution of code version
count to maintain various performance loss.

5

(a)Conflict Rate-10%0%10%20%30%Query Per Second50100150200250300ModelLayerBlock(6)Block(11)(b)Overhead: us0.01110010000Layer (Operator) IDOverheadMedian(Overhead)Mean(Overhead)(a)Performance0500100015002000Implementation IDImpl. 1Impl. 2Impl. 3Impl. 4IsolatedLow InterferenceMed InterferenceHigh Interference(b)Performance040080012001600Interference Pressure: %0%20%40%60%80%100%Impl. 1Impl. 4Impl. 3Impl. 2Best0%25%50%75%100%Interference level: %0%10%20%30%40%50%60%70%Performance Loss: %10% Perf.drop region(a)Version Num=1Version Num=2Version Num=3Version Num=4Version Num=510%20%30%40%50%60%70%Performance loss: %0.00.20.40.60.81.0Operators amount(b)Version Num=1Version Num=2Version Num=3Version Num=4Version Num=5Figure 8: Overview of VELTAIR, which comprises of the ofﬂine static compiler and the online runtime scheduler for adaptive
compiling and scheduling. The static compiler and runtime scheduler leverage the single-pass multi-version search and dynamic
threshold-based layer-block formation algorithm, respectively. By monitoring the system load and interference pressures, the
scheduler adaptively selects the optimal code version and scheduling granularities.

compilation strategy changes under different interference lev-
els. As such, the adaptive compilation is needed to achieve
high-performance multi-tenant DL services. Furthermore, we
propose to use multi-version static compilation to avoid the
overhead of just-in-time (JIT) compilation.

Extending TVM Auto-Scheduler. Recall that in Sec. 2.2,
the current TVM compilation strategy uses an auto-
scheduler [34] to search for the implementation that achieves
the best or the lowest latency. This compilation strategy does
not consider the existence of interference when multiple DNN
models run together, which can lead to a signiﬁcant perfor-
mance slowdown as shown in Fig. 1b.

To mitigate the impact of interference, we propose a naive
extension for the TVM’s existing auto-scheduler [34]. To
identify the best code implementation for the target layer at
a given interference level, we launch a background layer that
produces the desired level of interference and run the TVM’s
auto-scheduler with long enough iterations (e.g., 1024 itera-
tions). As such, the returned schedule can be regarded as the
optimal version under this interference level. In this experi-
ment, we use a frequently occurred ResNet conv layer with
the feature map size of 14 × 14, kernel size of 3 × 3, input
and output channel size of 256, and study the performance of
different compilation strategies under different interference
levels.

Results. Fig. 6 compares the performance of four different
implementations under different interference levels. In spe-
ciﬁc, the four implementations correspond to the optimal ones
searched with zero, low, medium, and high interference lev-
els, respectively. As Fig. 6a shows, the impl.-1, which is
also the default choice of TVM auto-scheduler, achieves the
best performance when no interference exists. However, its
performance also degrades rapidly, which can be up to 7×
at the high inference level. In contrast, the impl.-4 has the
lowest performance when no inference exists but achieves
the highest performance under the high interference. These
results show that the optimal code implementations vary ac-

cording to the interference levels, and our simple extension
to the TVM auto-scheduler can effectively ﬁnd these optimal
implementations.

Since a model may experience all ranges of interferences in
the multi-tenant DL services at one run, a static code version
cannot achieve the best performance. Fig. 6b further quantiﬁes
the performance trend of the above four versions against differ-
ent interference levels, where each version outperforms others
only within a narrow interference interval. As such, we have to
combine all the four versions across all the interference levels
to achieve the best performance, which is the dotted grey line
in Fig. 6.

General Cases. We further proﬁle the rest of the ResNet-50
layers under different interference levels to fully understand
the impact of the compilation strategy. Speciﬁcally, we choose
ten interference levels and identify the best-performing version
at each level, which leads to a total number of ten implementa-
tion versions for each layer. Fig. 7a compares the performance
loss of using a various number of versions against using all
the ten versions. If we use only one implementation, the per-
formance loss increases as the interference level increases.
In contrast, using ﬁve versions out of the ten versions can
maintain the performance loss within 10%.

Multi-Version Static Compilation. One naive way to ex-
ploit the above insights for multi-tenant DL services is to
perform a Just-in-Time (JIT) compilation according to the
interference level. However, the JIT compilation overhead can
offset the beneﬁt of adaptive compilation. Instead, we propose
to use the static multi-version compilation to achieve the same
beneﬁt of the adaptive JIT compilation. Fig. 7b plots the ratio
of code version count to maintain various performance losses
compared to the case of using all the ten versions. Although
the above results have shown that it requires ﬁve code versions
to stay within 10% performance loss, the majority (i.e., over
80%) of layers only require three code versions.

6

CPUTask TemplateExec.EngineRuntime MonitorLayer-blocks queueAlg. 2: DynamicLayer BlockDominantimplementationsAlg.1: Single PassMulti-VersionCodeSchedulerCandidateimplementationsProfilingOffline CompilerAlg. 3: VELTAIRRuntime Scheduler      Sec 4.1: Find multiple implementationsto handle different interference levels.       Sec 4.3: VELTAIR runtime scheduler,aware of system interference and conflict.      Sec 4.2: Generate layer blocks withdynamic size for low conflict scheduling.Figure 9: (a) The heavy-inference-optimal version generally prefers a large parallelism and a small blocking size (low locality),
while the light-inference-optimal version prefers the opposite. (b-d) Steps to ﬁnd optimal code version under different interfer-
ence levels. We use an exemplary conv layer of Hin = Win = 7, Cin = 832, Cout = 384, HK = WK = 1, Hout = Wout = 7.

4. Detailed Design of VELTAIR

In this work, we propose VELTAIR, a software solution for
high-performance multi-tenant deep learning model serving.
Based on the previous insights, VELTAIR performs adaptive
compiling and scheduling. Fig. 8 shows its overview that has
two main components, i.e., the ofﬂine static compiler and the
online runtime scheduler.

Instead of performing dynamic compilation whose overhead
can account for the model serving latency, we propose to use a
static multi-version compiler that extends the existing TVM’s
compilation framework and can identify different optimal code
versioned under different interference levels. The key novelty
in our compiler is a single-pass multi-version search algorithm
(described in Sec. 4.1).

The VELTAIR runtime scheduler dynamically forms the
layer block as the scheduling unit, which balances the core
usage efﬁciency and scheduling conﬂict rate. The key in the
scheduler is a dynamic threshold-based layer block formation
algorithm that we describe in Sec. 4.2. The runtime sched-
uler exploits a performance counter-based interference proxy.
By monitoring the system load and interference pressures, it
adaptively selects the optimal code version and scheduling
granularities, which are detailed in Sec. 4.3.

4.1. Single-pass Static Multi-version Compiler

In Sec. 3.3, we have described the beneﬁts of adaptive com-
pilation for handling the interference in the multi-tenant DL
services. We have proposed a naive extension for the TVM
auto-scheduler to search for the best code version at a given
interference level. The extended auto-scheduler launches an
additional background layer that can generate the desired in-
terference level during the search process. This approach is
effective for identifying the best code version at different in-
terference levels but is time-consuming as it requires multiple
passes of the TVM auto-scheduler. A single pass for a layer
is typically 20 minutes on our high-end CPU, which means
searching for ﬁve versions would take close to two hours.

To facilitate the multi-version compilation process, we pro-
pose a single-pass search algorithm that adds almost no over-

head to the original TVM auto-scheduler. Our key insight is
that we can use the well-known computer architecture tradeoff
between parallelism and locality to explain why certain ver-
sions are extremely sensitive to interference while others are
much less sensitive. Built upon this insight, we can explore
the parallelism-locality tradeoff space in a single search pass,
from which we then pick the desired versions.

Parallelism-Locality Tradeoff. We ﬁrst use experimental
results to illustrate that the ﬁnding of different optimal versions
under different interference levels is essentially a tradeoff be-
tween program parallelism and locality. In this experiment,
we use the straightforward extension described in Sec. 3.3 to
search for the two optimal code versions, one under the light
interference level and the other one under the heavy interfer-
ence level. We then record the corresponding complication
ﬂags for these two versions. Based on the recorded ﬂags,
we compute a parallelism metric by simply multiplying the
loop unrolling factor and parallelization factor. We compute a
locality metric by directly using the tiling/blocking size.

In Fig. 9a, we compare the above two metrics of the two
code versions that achieve the best performance under the
light inference and heavy inference, respectively. We observe
that the heavy-inference-optimal version generally prefers
high parallelism and a small blocking size, while the light-
inference-optimal version prefers the opposite. We then derive
the following insight: generated codes with a higher locality
(a larger blocking size) perform better under the light interfer-
ence (interference-vulnerable), while generated codes with a
higher parallelism perform better under the heavy interference
(interference-tolerant).

The above insight reﬂects the well-understood parallelism-
locality tradeoff. To exploit the large locality, a layer needs
to use more on-chip memory like the LLC in CPU, which are
shared resources among multiple CPU cores. However, the
performance of the layer quickly degrades when there is con-
tention on the shared resources (L3 cache and the correspond-
ing bandwidth according to our observation). To mitigate the
impact of contention, the layer can limit its locality and use
more parallelism to remedy its performance loss.

7

Blocking SizeParallelism(a)Light InterferenceHeavy InterferenceBlocking SizeParallelismStep 1(b)All implementationsBlocking SizeParallelismStep 2(c)All implementationsUnqualified implementationsBlocking SizeParallelismStep 3(d)All implementationsUnqualified implementationsPareto FrontierSingle-Pass Compilation. We now use the examples in
Fig. 9b-d to walk through our single-pass compilation algo-
rithm, which has three steps. The details of the algorithm are
provided in Agl. 1.

The ﬁrst step (Line 2 - 4 in Agl. 1) directly leverages
the TVM’s auto-scheduler to collect candidate implementa-
tions. In this step, we enable the operator fusion optimiza-
tion in the auto-scheduler, which includes common fusion
patterns like convolution followed by ReLU (conv-relu)
and convolution followed by batch normalization and ReLU
(conv-batchnorm-relu). Instead of searching for the best-
performing implementation, we record as many samples as
possible and calculate their parallelism and locality metrics as
Fig. 9b shows. In the second step (Line 5), we then ﬁlter out
samples whose performance can not satisfy this layer’s QoS
target as Fig. 9c shows. We set the layer’s performance as the
minimal ﬂoating-point operation per second (i.e., FLOPS) that
the corresponding model needs to achieve to meet the model’s
latency target.

In the third step (Line 6 to 7 and Line 14 to 29), we select
the dominant implementations via ExtractDominant function,
where there are no other implementations with both smaller
blocking size and parallelism than each chosen one. In other
words, these dominant implementations form the Pareto fron-
tier (red markers in Fig. 9d), which is an optimal solution
to the multi-objective optimization problem. In the last step
(Line 8 to 12), we uniformly choose ﬁve versions from the
Pareto frontier (circled ones in Fig. 9d). Since not all layers
require ﬁve versions to maintain the close performance to the
optimal, we test the performance of the selected ﬁve versions
under different interference levels and remove the ones whose
performance is within 90% of the full ﬁve versions. This

Algorithm 1 Static multi-version compilation in a single
pass.

Input: layers[N], qos
Output: candidate_impls[N][V ], dominant_impls[N][]

3:

4:

5:
6:
7:

8:
9:
10:
11:

1: function FINDINGIMPL(layers, qos)
2:

for _l in layers[N] do
_l.qos ← qos ×

_l.op_count
∑x∈layers[1:N](x.op_count)

impls[] ← Ansor(_l, 1024)
impls[] ← [x.time ≤ _l.qos f or x in impls]
d_impl[] ← ExtractDominant(impls)
d_impl[].sort(key = x.block_size)
for _i in d_impl, step ← d_impl.length

do

V

c_impl.push_back(_i)

end for
candidate_impls.push_back(c_impl)
dominant_impls.push_back(d_impl)

end for
return candidate_impls, dominant_impls

12:
13:
14:
15: end function

Figure 10:
(a) Forming the layer-blocks by a threshold and
minimize the layer-blocks’ CPU usage. (b) Average and maxi-
mal CPU usage of various scheduling granularity.

optimization leads to the reduced storage overhead of code
multi-versioning.

4.2. Dynamic Threshold Based Layer-Block Formation

As previously explained in Sec. 3.2, the layer-block-based
scheduling outperforms the layer-wise and model-wise
scheduling through balancing the minimal average core usage
and scheduling conﬂict rate. However, a ﬁxed-sized layer-
block is not efﬁcient because the optimal block size varies with
the system load and the interference from other co-executed
models. As such, we propose a dynamic-sized layer-block
approach to achieve the high core efﬁciency and low conﬂict
rate according to the system load and interference level.

To reduce the scheduling conﬂict rate via the layer-block
scheduling, we ﬁrst identify the layers that are most likely
to trigger conﬂicts. To identify these conﬂict-prone layers,
we calculate the required CPU core number for each layer to
complete within its QoS target, from which we can compute
the model’s averaged core number. We compare the layer-
wise core number against the model-wise average value, and
identify the layers with a much higher CPU core number
requirement than the averaged value as conﬂict-prone layers.
For each conﬂict-prone layer, we form a layer-block that can
reduce its core usage by increasing the core usage for other
layers in the block while still satisfying the QoS target.

We walk through the ResNet example in Fig. 10a to illus-
trate the intuition of our method. We ﬁrst form the layer-wise
(red shadowed area) and model-wise (black horizontal line)
scheduling plan in Fig. 10a. We denote the average core count
in the mode-wise scheduling as Avg_C. We then use a runtime-
decided threshold thres that ranges from zero to maximal core
count. We iterate over all layers and identify the conﬂict-prone
ones whose core requirement exceeds Avg_C + thres, i.e., the
blue line in Fig. 10a. We refer to each conﬂict-prone layer
to the splitting pivot, which is essentially the beginning layer
for a block. As a result, there are four blocks marked by
arrows. For each formed block, we calculate its QoS target
by summing up all its layers. We then recalculate the core
requirement (yellow line) of each block to satisfy its QoS
target.

Agl. 2 formally describes the above dynamic threshold-
based layer-block formation algorithm. where the threshold

8

010Time0102030405060CPU requirementthres(a)ModelLayerModelLayerLBs(6)LBs(11)LBs(Dyn)Scheduling granularity0102030405060CPU cores usage(b)Average CPU UsageMax CPU Usageis determined at runtime according to the system load and
co-executed models’ characteristics. Sec. 4.3 will provide
the details of how we adjust the threshold. With Agl. 2, we
can generate proper layer-blocks using no more than Avg_C +
thres CPU cores under different system loads. The basic idea
is that when the system load is low, we use a high threshold
since the conﬂict possibility is low, which means each layer
can use as many cores as possible for maximizing the CPU
resource usage efﬁciency. When the system load is high, we
use a low threshold which means each layer should have core
counts close to the average value for reducing the scheduling
conﬂict rate.

The essence of Agl. 2 is to reduce the high core usage of
error-prone layers and remedy its latency loss by increasing
the core usage of other layers. This can increase average core
usage compared to the most efﬁcient layer-wise scheduling.
However, according to our evaluation, the gap between optimal
CPU resource usage is smaller than 10%, which is acceptable.
Fig. 10b compares the average core usage and maximum core
usage of different scheduling granularities when co-locating
two ResNet-50 models. Our algorithm is effective at reducing
the gap between optimal CPU cores usage (i.e., achieving high
resource efﬁciency) and maximum core usage (i.e., reducing
the scheduling conﬂict rate).

Algorithm 2 Dynamic threshold based layer-block forma-
tion algorithm.

Input: layers[N], impls, thres
Output: Layer_Block

1: function FINDING1STPIVOT(layers, impls, thres)
2:
3:
4:
5:

splitting_pivot ← 0
Avg_C ← Core@ Model Granularity(layers)
for _l in layers[1 : N] do

if Core(impls[_l]) ≥ thres + Avg_C then

end if

splitting_pivot ← _l
break

end for
return splitting_pivot

6:
7:
8:
9:
10:
11: end function
12: function FORMINGLBS(layers)
Layer_Block ← [], begin ← 0
13:
while layers.length() (cid:54)= 0 do
14:
15:
16:
17:

18:
19:
20:
21: end function

end while
return Layer_Blocks

sp ← Finding1stPivot(layers, impls,thres)
Layer_Block.push_back(layers[begin : sp])
layers ← layers[sp + 1 :]
begin ← sp + 1

Figure 11: (a) Dominant components of performance counters
according to PCA. (b) Accuracy validation of the interference
pressure proxy using L3 miss rate and access counters.

4.3. VELTAIR Runtime Scheduler

In this subsection, we describe the details of the runtime sched-
uler in VELTAIR. Our scheduler monitors the current CPU
inference level and dynamically chooses the code version
derived from Sec. 4.1 and scheduling granularity using the
algorithm described in Sec. 4.2. In speciﬁc, we explain how
we derive the system interference pressure level and how we
determine the dynamic threshold for Agl. 2.

Interference Proxy. We ﬁrst build the proxy for monitoring
the system interference pressure level by using hardware per-
formance counters. According to previous studies [44, 50, 51],
performance counters have a strong relation with interference
pressure level. We deﬁne the interference pressure level of
the system as the average performance slowdown ratio of lay-
ers running on the system. To ﬁgure out what performance
counters decide the interference level, we conduct a princi-
pal component analysis (PCA) [52] on collected performance
counters, including L3 cache miss Rate, L3 access, instruction
per cycle (IPC), ﬂoat-point operations, etc. It turns out that
L3 cache-related counters account for over 99% of the data
variance, as shown in Fig. 11a. As such, we choose the L3
miss rate and L3 access to construct a simple linear interfer-
ence model. As shown in Fig. 11b, the predicted interference
level matches the measured interference level well. Using this
simple linear model, we can derive the interference pressure
level with low cost at runtime.

Dynamic Scheduling Threshold. The threshold used by
the layer-block formation in Sec. 4.2 indicates the additional
core counts that each layer block can use beyond the model’s
averaged requirement. When a model runs exclusively, it
can use as many cores as it desires. However, when multiple
models run concurrently, each model should try to reduce its
core usage to avoid scheduling conﬂicts. As such, we use a
simple heuristic that determines the threshold by subtracting
the total core number by the sum of all models’ average core
count and distributing the remaining cores according to each
model’s average core count. For example, three models A, B, C
use 12, 12, 24 CPU cores on average respectively. On average,
64−(12+12+24) = 16 cores are idle, and we assign 4, 4, 8 as

9

L3 Miss RateL3 AccessIPCFP OPPerformance counters10−210−1100PCA component ratioAccont forless than 1%(a)Predicted interference levelMeasured interference level(b)Light InterferenceMedium InterferenceHeavy InterferenceSevere Interferencethreshold to A, B, C respectively. In our study, we observe that
a model with a high average core usage typically has a high
peak core usage. Thus, dividing the idle cores by the model’s
average core usage can better ﬁt each model’s computation
demand.

Putting All Together. We now describe the runtime sched-
uler in VELTAIR that exploits the aforementioned algorithms.
Agl. 3 illustrates the pseudo-code of the scheduler, where the
task dispatcher simply sends tasks to a worker if there are
enough idle cores. The code implementation search is done
ofﬂine at the compiling stage as Agl. 1 details. At runtime,
VELTAIR collects the performance counters once a layer block
is ﬁnished, and the scheduler will form the next layer block
from the remaining layers according to the current system
load with different implementations according to the current
interference pressure level. As mentioned before, codef im-
plementations with different interference tolerance levels have
signiﬁcant differences in parallelism and locality, which means
the same layer in a model will have different CPU require-
ments under different interference pressure levels leading to
different layer blocks. Note that when selecting the interfer-
ence tolerance level of the next layer-block, we will ignore
the ongoing but soon-to-ﬁnish layer-blocks, since they will
have little inﬂuence on system interference from now on. To
determine the soon-to-ﬁnish layer-block, we examine whether
its remaining execution latency is within a threshold (e.g.,
10%) according to our ofﬂine proﬁle-based latency model and
interference proxy model.

5. Evaluation

We now demonstrate the effectiveness of the adaptive compil-
ing and scheduling in VELTAIR. We ﬁrst describe our evalua-

Algorithm 3 The details of VELTAIR scheduler.

Dispatch tasks following Poisson distribution

1: function VELTAIRTASKDISPATCHER
2:
3: end function
4: function VELTAIRWORKER
5:
6:
7:
8:

Wait for last task to ﬁnish

if worker is busy then

while true do

9:
10:
11:
12:
13:
14:

end if
t ← f etch_task(), begin ← 0
while t. f inished (cid:54)= True do
i ← system inter f erence
thres ← #CTotal−∑tactive(#CModel Granularity(t))
pivot ← Finding1stPivot(t, implsi,thres)
t[begin : pivot].Execute()
t ← t[pivot + 1 :]
begin ← pivot + 1

15:
16:
17:
end while
18:
19: end function

end while

tion setup, baseline, and metrics. We compare the performance
of VELTAIR against other work and show that the combina-
tion of our adaptive compiling and scheduling is essential for
achieving the hight-performance in multi-tenant DL services.

5.1. Experimental Setup

Multi-Tenant Deep Learning Models. To simulate the re-
alistic situation of deep learning services, we use deep learning
models from MLPerf (Server) [37] as listed in Tbl. 2. The eval-
uated models include image classiﬁcation, object detection,
and neural machine translation (NMT) tasks. We categorize
the workload of the models from light, medium to heavy,
and set the QoS target for them according to the guidance of
MLPerf.

Workload Generation. We also follow the MLPerf guid-
ance to generate random queries with Poisson distribution,
where the λ parameter of the distribution stands for the QPS
(query per second) of the workload. We evaluate our design un-
der Light, Medium, Heavy, and Mix workload. For the mixed
workload, the frequency of every task is set to be inversely
proportional to QoS requirements [53].

Hardware and Software. For all experiments, we use a
machine equipped with a high-end server-level CPU Ryzen
Threadripper 3990X [38] and 256 GB DDR4 RAM at
3200 MHz. The CPU has 64 physical cores and 256MB L3
cache capacity, and works at 2.9 GHz with AVX-2 enabled. To
obtain stable experimental results, we turn off certain features
such as simultaneous multi-threading (SMT) and dynamic
voltage and frequency scaling (DVFS). We believe that turn-
ing off these features do not change our insights owing to the
following reasons. The SMT mainly enhances the sharing
of L1 cache, while we identify LLC as the main contentious
resource. However, SMT leads to a signiﬁcant latency ﬂuc-
tuation because of the possibility that two logical threads of
different tasks are assigned to the same physical core. The
DVFS also leads to latency ﬂuctuation that increases the con-
ﬂict rate. We implement the static multi-version compiler
by extending the TVM v0.8 [32]. For the runtime scheduler
implementation, we use MPICH 3.3.2 [54], which serves the
multi-tenant DNN models via multi-processing.

Table 2: Evaluated multi-tenant DL models.

Category

Workload

Name

QoS (ms)

Image

Classiﬁcation

Object

Detection

NMT

Medium

Medium

Light

Light

Heavy

Light

Heavy

ResNet-50 [1]

GoogLeNet [4]

EfﬁcientNet [3]

MobileNet-V2 [2]

SSD [6]

Tiny-YOLOV2 [5]

Bert-Large [7]

15

15

10

10

100

10

130

10

Figure 12: Query per second (QPS) with 95% tasks QoS satisﬁed for various workloads and scheduling strategies.

Evaluation Metrics. We use QPS with 95% tasks QoS sat-
isﬁed, average latency, and CPU usage efﬁciency as our evalu-
ation metrics.
• QPS with 95% Tasks QoS Satisﬁed: this metric represents
how many requests the system can serve per second with
almost all the query requests (95%) ﬁnish within the QoS
target.

• Average Latency: This metric measures the average execu-

tion latency of all the queries.

• CPU Usage Efﬁciency: This metric measures the average
CPU usage of the tasks by dividing the total execution time
by the sum of multiplying of the core usage and execution
time of each layer.

Baseline Choice. Since we co-locate multiple DNN models
and let them spatially share the hardware, we choose Pla-
naria [23] as the baseline in our evaluation. It should be noted
that Planaria is based on the hardware-software co-design,
while we port the software scheduling part to the CPU plat-
form. To justify why we only consider the spatial multitask-
ing scenario, we also implement another baseline scheduling
method PREMA [24], which is a temporal multitasking algo-
rithm and lets tasks with high priority preempt.

Evaluation Plan. We study the effectiveness of different
components by evaluating the following conﬁgurations of
VELTAIR.
• VELTAIR-AS: with only adaptive scheduling.
• VELTAIR-AC: with only adaptive compilation.
• VELTAIR-FULL: with both adaptive scheduling and adap-

tive compilation enabled.

5.2. Query per Second (QPS) Improvement

Fig. 12 demonstrates the QPS improvement of VELTAIR
against the baseline Planaria [23] for studied models in differ-
ent levels of workloads. VELTAIR-FULL achieves an average
of 71%, 62%, 44% improvement in the light, medium, heavy
workloads respectively, and an average of 68% improvement
in the mix workloads.

We also observe that the adaptive compilation (VELTAIR-
AC) achieves better improvements than the adaptive schedul-
ing (VELTAIR-AS). However, these two techniques are syn-
ergistic, and both are critical components for fulﬁlling the
performance improvement of the full version of our design
(VELTAIR-FULL). Without the adaptive scheduling, VELTAIR-
AC only achieves 50% QPS improvement in contrast to the
68% improvement of VELTAIR-FULL in the mix workloads.

Figure 13: Average query execution latency comparison be-
tween solo-run (isolated) and various VELTAIR conﬁgurations
(VELTAIR-AS, VELTAIR-AC, VELTAIR-FULL).

The reason is that without adaptive scheduling, many layers
will choose implementations with lower locality but higher
parallelism to handle interference, leading to increased CPU
requirement and thus increased conﬂict possibility. In contrast,
the dynamic layer block formation in adaptive scheduling can
mitigate these conﬂicts.

In Fig. 12, we also observe that the temporal multitasking-
based multi-DNN serving scheme (PREMA [24]) generally
performs worse than the spatial multitasking-based multi-
DNN serving. This observation justiﬁes the choice of spatial
multitasking of our work.

5.3. Query Execution Latency Result

We compare the average query latency of various VELTAIR
conﬁgurations against the solo-run case in Fig. 13. For each
model, the latency is measured at the QPS where 95% of
queries can meet their QoS target (i.e., same to the QPS metric
in Fig. 12). Since the solo-run latency is the shortest latency
each model can achieve on the studied CPU platform, this
comparison lets us identify how much additional room there
is for further optimization in VELTAIR.

Fig. 13 shows that the inference latency of VELTAIR-AS
is 1.6× of the isolated solo-run execution, which means the
adaptive scheduling cannot reduce the execution latency. On
the other hand, the latency of VELTAIR-AC is 1.17× of the
isolated execution, conﬁrming its ability for reducing latency
under interference. With both adaptive scheduling and compi-
lation, the average latency is only 1.1× of the isolated execu-
tion, which means that VELTAIR-FULL is close enough to the
optimal serving result on the studied platform.

5.4. Result of CPU Efﬁciency

the layer-block-based scheduling leads to
In VELTAIR,
smoother CPU core usage with reduced conﬂict rate but po-
tentially uses more cores. We quantify the gap of average
core usage between VELTAIR scheduling and the layer-wise

11

Normalized QPS0.000.501.001.502.00WorkloadEfficientNetMobileNetTiny-YOLOv2LightResNet-50GoogLeNetMedianSSDBertHeavyMixPlanariaPREMAVeltair-ASVeltair-ACVeltair-FULLEvaluation-(b)Normalized Latency00.511.52Deep Learning ModelEfficientNetMobileNetTiny-YOLOv2ResNet-50GoogLeNetSSDBertAvg.IsolatedVeltair-ASVeltair-ACVeltair-FULLa sampling-reconstruction-prediction-based strategy with re-
conﬁgurable architecture. Bubble-series [25, 26] proposed an
online contention measurement and control system to relax
the performance loss caused by contention and is free from the
online compiler, execution checkpoint, code variants rerouting,
etc. While previous works mainly focus on resource partition,
isolation, and management, Protean [44] and other works [56]
extend the optimization space by introducing runtime code
transformation for lower L3 cache pollution.

6.2. Multi-Tenant Deep Learning Service

Different from conventional workload, deep learning ser-
vices are computation-intensive with complex inner struc-
tures and should be speciﬁcally treated when co-locating them.
DART [57] proposes a pipeline-based method to co-locate
multiple DNN workloads on multiple heterogeneous compu-
tation nodes. While in this work, we mainly consider co-
locating multiple DNN tasks on one homogenous hardware.
PREMA [24] and AI-MT [22] propose temporal multiplexing
architectures with preemption-based strategy and computation-
memory overlapping-based strategy, respectively. In contrast,
Planaria [23] proposes a spatially decomposable systolic ar-
chitecture to co-locate tasks with proper computation and
memory resources. This work aims at relaxing the problem
from compiling and scheduling aspect with less constraint
on the back-end hardware as long as it is programmable. In
addition to hardware-software co-design, DyNet [58] mainly
handles the problem of scheduling RNNs. LazyBatch [31]
proposes a batch-based approach to handle multiple DNN re-
quests. Ebird [59] also proposes a batch-based approach to
enable concurrent execution of DNNs with high data transfer-
compute overlapping. Abacus [60] proposes an operator over-
lapping strategy based on precise latency prediction. Besides
the multi-DNN serving scenario, emerging microservice-based
workloads also have complex inner structures similar to DNN
models [61], to which our design may also be applied.

6.3. Deep Learning Compiler

For the better ﬂexibility and performance of DNN model
execution, recent researchers propose various DL com-
pilers including TVM [32], TensorComprehensions [35],
Tiramisu [62], TensorFlow-XLA [63]. These DL compilers
are often integrated with front-end optimizers like TASO [64]
or Grappler [65]. Meanwhile, they also introduce domain-
speciﬁc language to make it convenient for users to deﬁne
their own computation. For the code generation optimization
with a huge search space, researchers apply both machine-
learning-based methods including AutoTVM [66], Ansor [34],
FlexTensor [33] and heuristic based methods including DLFu-
sion [67] and Paleozoic [68]. These compilers target general-
purpose hardware or DL accelerators, and generally outper-
form vendors-provided libraries. However, these works mainly
focus on optimizing the performance of the stand-alone execu-
tion of DNN operators. In contrast, we explore compilation

Figure 14: (a) Gap between optimal core usage of layer block-
based scheduling strategy under different system load, com-
paring to the ﬁne-grained layer-wise scheduling. (b) Improve-
ment under different version number. (c) The ratio of number
of used versions for all the layers in seven DNN models.

scheduling. Recall that the ﬁne-grained layer-wise scheduling
indicates the minimal core usage. Fig. 14 shows that even
under 75% system load, the core usage gap of VELTAIR is less
than 10% compared to the minimal core usage of the layer-
wise scheduling. In contrast, the model-wise has a much larger
gap of 47%. These results conﬁrm that our layer-block-based
scheduling strikes a balance between reducing the scheduling
conﬂict rate and maintaining the high resource usage.

5.5. Sensitivity and Overhead Analysis

Sensitivity.
In VELTAIR, we empirically set the maximal
version number V to 5. We now study the performance im-
provement under different V in Fig. 14b, which shows the
improvement saturates after four versions. Fig. 14c plots the
version count distribution for different layers, which shows
that only 3% layers require ﬁve versions. These results justify
the choice of using ﬁve versions.

Scheduling Overhead. The
scheduling overhead of
VELTAIR mainly consists of two parts. The ﬁrst part is the
runtime layer block formation procedure, which scans the
layers only once and has the complexity of O(N). The second
part comes from the linear-model-based interference proxy.
Owing to the low complexity of the scheduling algorithm and
the proxy model, we ﬁnd that their overall overhead is less
than 0.1 ms for serving each DNN model.

6. Related Works

We compare and contrast VELTAIR with previous works in the
following three aspects, including the task co-location, multi-
tenant deep learning services, and deep learning compiler.

6.1. Task Co-location in Datacenter

The rapid improvement of hardware computation power makes
it possible to share the hardware among multiple tasks for
higher throughput. Many works have studied how to co-locate
a latency-critical (LC) task with multiple best-effort tasks
[55, 27, 46]. Parties [29] proposes a resource-partitioning tech-
nique to co-locate multiple LC services. For more intelligent
resource partition and management, CuttleSys [30] proposes

12

(a)Gap V.S. optimal0%20%40%60%80%System Load25%75%ModelLightMediumHeavy(c)3%11%39%32%14%1 Ver2 Ver3 Ver4 Ver5 Veroptimization for co-locating multiple deep learning tasks, for
which we show the interference-aware compilation is critical.

7. Conclusion

In this work, we proposed VELTAIR, a compiler-scheduler
system for high performance multi-tenant deep learning ser-
vice. By leveraging multi-version compiling and layer-block
scheduling, we achieve 1.7× system maximal QPS and reduce
50% of the computation latency with little overhead. We ﬁrst
evaluate the proper scheduling granularity in deep learning
tasks, and we propose a layer-block scheduling strategy with
dynamically adjustable size to reduce the resource conﬂict.
Then we study the compilation options and propose a single-
pass multi-version compilation to handle the performance
loss of interference caused by shared resource competition
in multiple neural networks co-locating. We demonstrate the
advantages of VELTAIR in the aspects of improvement in QPS,
QoS satisfaction rate, computation latency, and resource usage
efﬁciency using the standard MLPerf Server test suite.

Acknowledgement

This work was supported by the National Key R&D Program
of China under Grant 2021ZD0110104, the National Natu-
ral Science Foundation of China (NSFC) grant (U21B2017,
62072297, 61832006). We thank the anonymous reviewers
and our shepherd Prof. Xipeng Shen for their constructive
feedback for improving the work. We also thank Zhanda
Zhu, Zihan Liu, Yijia Diao, and Vega Jiang for the beneﬁcial
discussion and continuous support.

References

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,
2016, pp. 770–778, IEEE Computer Society, 2016.

[2] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen,
“MobileNetV2: Inverted residuals and linear bottlenecks,” in 2018
IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 4510–4520,
IEEE Computer Society, 2018.

[3] M. Tan and Q. V. Le, “EfﬁcientNet: Rethinking model scaling for con-
volutional neural networks,” in Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, vol. 97 of Proceedings of Machine Learning
Research, pp. 6105–6114, PMLR, 2019.

[4] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 1–9,
IEEE Computer Society, 2015.

[5] J. Redmon and A. Farhadi, “YOLO9000: better, faster, stronger,” in
2017 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 6517–6525,
IEEE Computer Society, 2017.

[6] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and
A. C. Berg, “SSD: Single shot multibox detector,” in Computer Vision -
ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands,
October 11-14, 2016, Proceedings, Part I, vol. 9905 of Lecture Notes
in Computer Science, pp. 21–37, Springer, 2016.

[7] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training
of deep bidirectional transformers for language understanding,” in
Proceedings of the 2019 Conference of the North American Chapter

of the Association for Computational Linguistics: Human Language
Technologies, 2019.

[8] Y. Guan, J. Leng, C. Li, Q. Chen, and M. Guo, “How far does BERT
look at: Distance-based clustering and analysis of bert’s attention,” in
Proceedings of the 28th International Conference on Computational
Linguistics (COLING), pp. 3853–3860, International Committee on
Computational Linguistics, 2020.

[9] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and
M. Sun, “Graph neural networks: A review of methods and applica-
tions,” 2021.

[10] Z. Zhang, J. Leng, L. Ma, Y. Miao, C. Li, and M. Guo, “Architectural
implications of graph neural networks,” IEEE Computer Architecture
Letter, 2020.

[11] NVIDIA, “Nvidia a100 tensor core gpu.” https://www.nvidia.

com/en-us/data-center/a100/, Aug 2021.

[12] Y. Chen, J. S. Emer, and V. Sze, “Eyeriss: A spatial architecture for
energy-efﬁcient dataﬂow for convolutional neural networks,” in 43rd
ACM/IEEE Annual International Symposium on Computer Architecture,
ISCA 2016, Seoul, South Korea, June 18-22, 2016, pp. 367–379, IEEE
Computer Society, 2016.

[13] T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O. Temam,
“Diannao: a small-footprint high-throughput accelerator for ubiquitous
machine-learning,” in Architectural Support for Programming Lan-
guages and Operating Systems, ASPLOS 2014, Salt Lake City, UT,
USA, March 1-5, 2014, pp. 269–284, ACM, 2014.

[14] S. Zhang, Z. Du, L. Zhang, H. Lan, S. Liu, L. Li, Q. Guo, T. Chen,
and Y. Chen, “Cambricon-x: An accelerator for sparse neural net-
works,” in 49th Annual IEEE/ACM International Symposium on Mi-
croarchitecture, MICRO 2016, Taipei, Taiwan, October 15-19, 2016,
pp. 20:1–20:12, IEEE Computer Society, 2016.

[15] C. Guo, Y. Zhou, J. Leng, Y. Zhu, Z. Du, Q. Chen, C. Li, B. Yao, and
M. Guo, “Balancing efﬁciency and ﬂexibility for DNN acceleration via
temporal gpu-systolic array integration,” in 57th ACM/IEEE Design
Automation Conference, DAC 2020, San Francisco, CA, USA, July
20-24, 2020, pp. 1–6, IEEE, 2020.

[16] N. P. Jouppi, C. Young, N. Patil, D. A. Patterson, G. Agrawal, R. Ba-
jwa, S. Bates, et al., “In-datacenter performance analysis of a tensor
processing unit,” in Proceedings of the 44th Annual International Sym-
posium on Computer Architecture, ISCA 2017, Toronto, ON, Canada,
June 24-28, 2017, pp. 1–12, ACM, 2017.

[17] Y. Zhou, M. Yang, C. Guo, J. Leng, Y. Liang, Q. Chen, M. Guo, and
Y. Zhu, “Characterizing and demystifying the implicit convolution
algorithm on commercial matrix-multiplication accelerators,” in 2021
IEEE International Symposium on Workload Characterization (IISWC),
2021.

[18] Y. Wang, C. Zhang, Z. Xie, C. Guo, Y. Liu, and J. Leng, “Dual-
side sparse tensor core,” in 48th ACM/IEEE Annual International
Symposium on Computer Architecture, ISCA 2021, Valencia, Spain,
June 14-18, 2021, pp. 1083–1095, IEEE, 2021.

[19] Y. Gan, Y. Qiu, J. Leng, M. Guo, and Y. Zhu, “Ptolemy: Architecture
support for robust deep learning,” in 53rd Annual IEEE/ACM Interna-
tional Symposium on Microarchitecture (MICRO), pp. 241–255, IEEE,
2020.

[20] C. Guo, B. Y. Hsueh, J. Leng, Y. Qiu, Y. Guan, Z. Wang, X. Jia,
X. Li, M. Guo, and Y. Zhu, “Accelerating sparse DNN models without
hardware-support via tile-wise sparsity,” in Proceedings of the Inter-
national Conference for High Performance Computing, Networking,
Storage and Analysis (SC), IEEE/ACM, 2020.

[21] Y. Gan, Y. Qiu, L. Chen, J. Leng, and Y. Zhu, “Low-latency proactive
continuous vision,” in Proceedings of the ACM International Confer-
ence on Parallel Architectures and Compilation Techniques (PACT),
2020.

[22] E. Baek, D. Kwon, and J. Kim, “A multi-neural network acceleration
architecture,” in 47th ACM/IEEE Annual International Symposium on
Computer Architecture, ISCA 2020, Valencia, Spain, May 30 - June 3,
2020, pp. 940–953, IEEE, 2020.

[23] S. Ghodrati, B. H. Ahn, J. K. Kim, S. Kinzer, B. R. Yatham, N. Alla,
H. Sharma, M. Alian, E. Ebrahimi, N. S. Kim, C. Young, and
H. Esmaeilzadeh, “Planaria: Dynamic architecture ﬁssion for spa-
tial multi-tenant acceleration of deep neural networks,” in 53rd Annual
IEEE/ACM International Symposium on Microarchitecture, MICRO
2020, Athens, Greece, October 17-21, 2020, pp. 681–697, IEEE, 2020.
[24] Y. Choi and M. Rhu, “PREMA: A predictive multi-task scheduling
algorithm for preemptible neural processing units,” in IEEE Interna-
tional Symposium on High Performance Computer Architecture, HPCA
2020, San Diego, CA, USA, February 22-26, 2020, pp. 220–233, IEEE,
2020.

13

[25] J. Mars, L. Tang, R. Hundt, K. Skadron, and M. L. Soffa, “Bubble-
up: increasing utilization in modern warehouse scale computers via
sensible co-locations,” in IEEE/ACM International Symposium on
Microarchitecture (MICRO), 2011.

[26] H. Yang, A. D. Breslow, J. Mars, and L. Tang, “Bubble-ﬂux: precise
online qos management for increased utilization in warehouse scale
computers,” in The 40th Annual International Symposium on Computer
Architecture (ISCA), 2013.

[27] D. Lo, L. Cheng, R. Govindaraju, P. Ranganathan, and C. Kozyrakis,
“Heracles: improving resource efﬁciency at scale,” in Proceedings of
the 42nd Annual International Symposium on Computer Architecture
(ISCA), 2015.

[28] H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Philipose, A. Kr-
ishnamurthy, and R. Sundaram, “Nexus: a GPU cluster engine for
accelerating DNN-based video analysis,” in Proceedings of the 27th
ACM Symposium on Operating Systems Principles (SOSP), ACM,
2019.

[29] S. Chen, C. Delimitrou, and J. F. Martínez, “PARTIES: QoS-aware re-
source partitioning for multiple interactive services,” in Proceedings of
the Twenty-Fourth International Conference on Architectural Support
for Programming Languages and Operating Systems (ASPLOS), 2019.
[30] N. Kulkarni, G. Gonzalez-Pumariega, A. Khurana, C. A. Shoemaker,
C. Delimitrou, and D. H. Albonesi, “Cuttlesys: Data-driven resource
management for interactive services on reconﬁgurable multicores,” in
53rd Annual IEEE/ACM International Symposium on Microarchitec-
ture (MICRO), 2020.

[31] Y. Choi, Y. Kim, and M. Rhu, “Lazy batching: An sla-aware batching
system for cloud machine learning inference,” in IEEE International
Symposium on High-Performance Computer Architecture, HPCA 2021,
Seoul, South Korea, February 27 - March 3, 2021, pp. 493–506, IEEE,
2021.

[32] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Q. Yan, H. Shen, M. Cowan,
L. Wang, Y. Hu, L. Ceze, C. Guestrin, and A. Krishnamurthy, “TVM:
an automated end-to-end optimizing compiler for deep learning,” in
13th USENIX Symposium on Operating Systems Design and Implemen-
tation, OSDI 2018, Carlsbad, CA, USA, October 8-10, 2018, pp. 578–
594, USENIX Association, 2018.

[33] S. Zheng, Y. Liang, S. Wang, R. Chen, and K. Sheng, “Flextensor: An
automatic schedule exploration and optimization framework for tensor
computation on heterogeneous system,” in Architectural Support for
Programming Languages and Operating Systems, Lausanne (ASPLOS),
2020.

[34] L. Zheng, C. Jia, M. Sun, Z. Wu, C. H. Yu, A. Haj-Ali, Y. Wang, J. Yang,
D. Zhuo, K. Sen, J. E. Gonzalez, and I. Stoica, “Ansor: Generating
high-performance tensor programs for deep learning,” in 14th USENIX
Symposium on Operating Systems Design and Implementation (OSDI),
2020.

[35] N. Vasilache, O. Zinenko, T. Theodoridis, P. Goyal, Z. DeVito, W. S.
Moses, S. Verdoolaege, A. Adams, and A. Cohen, “Tensor compre-
hensions: Framework-agnostic high-performance machine learning
abstractions,” CoRR, vol. abs/1802.04730, 2018.

[36] K. M. Hazelwood, S. Bird, D. M. Brooks, S. Chintala, U. Diril,
D. Dzhulgakov, M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee,
J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang, “Applied
machine learning at facebook: A datacenter infrastructure perspective,”
in IEEE International Symposium on High Performance Computer
Architecture, HPCA 2018, Vienna, Austria, February 24-28, 2018,
pp. 620–629, IEEE Computer Society, 2018.

[37] V. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C. Wu,
et al., “MLPerf Inference Benchmark,” in 47th ACM/IEEE Annual
International Symposium on Computer Architecture, (ISCA), pp. 446–
459, IEEE, 2020.

[38] AMD, “Ryzen Threadripper 3990X Processor.” https://www.
amd.com/en/products/cpu/amd-ryzen-threadripper-3990x,
2020.

[39] NVIDIA, Multi-Process Service. NVIDIA.
[40] NVIDIA, “Nvidia multi-instance gpu.” https://www.nvidia.com/
en-us/technologies/multi-instance-gpu/, Aug 2021.
[41] Intel, “Math kernel library for deep neural networks.” https://

github.com/rsdubtso/mkl-dnn, 2019.

[42] Microsoft, “Optimize and accelerate machine learning inferencing and

training.” https://onnxruntime.ai/, Aug 2021.

[43] NVIDIA, “Nvidia cudnn.” https://developer.nvidia.com/

cudnn, Aug 2021.

[44] M. A. Laurenzano, Y. Zhang, L. Tang, and J. Mars, “Protean code:
Achieving near-free online code transformations for warehouse scale
computers,” in 47th Annual IEEE/ACM International Symposium on
Microarchitecture, (MICRO), pp. 558–570, IEEE Computer Society,
2014.

14

[45] W. Wang, Y. Chang, W. Lo, and Y. Lee, “Adaptive scheduling for
parallel tasks with qos satisfaction for hybrid cloud environments,” J.
Supercomput., vol. 66, no. 2, pp. 783–811, 2013.

[46] Q. Chen, H. Yang, M. Guo, R. S. Kannan, J. Mars, and L. Tang,
“Prophet: Precise qos prediction on non-preemptive accelerators to
improve utilization in warehouse-scale computers,” in Proceedings of
the Twenty-Second International Conference on Architectural Support
for Programming Languages and Operating Systems, ASPLOS 2017,
Xi’an, China, April 8-12, 2017, pp. 17–32, ACM, 2017.

[47] S. Tu, W. Zheng, E. Kohler, B. Liskov, and S. Madden, “Speedy
transactions in multicore in-memory databases,” in ACM SIGOPS 24th
Symposium on Operating Systems Principles, SOSP ’13, Farmington,
PA, USA, November 3-6, 2013 (M. Kaminsky and M. Dahlin, eds.),
pp. 18–32, ACM, 2013.

[48] H. Kasture and D. Sánchez, “Tailbench: a benchmark suite and eval-
uation methodology for latency-critical applications,” in 2016 IEEE
International Symposium on Workload Characterization, IISWC 2016,
Providence, RI, USA, September 25-27, 2016, pp. 3–12, IEEE Com-
puter Society, 2016.

[49] A. Gujarati, R. Karimi, S. Alzayat, W. Hao, A. Kaufmann, Y. Vig-
fusson, and J. Mace, “Serving dnns like clockwork: Performance
predictability from the bottom up,” in 14th USENIX Symposium on
Operating Systems Design and Implementation, OSDI 2020, Virtual
Event, November 4-6, 2020, pp. 443–462, USENIX Association, 2020.
[50] W. Zhao, Q. Chen, H. Lin, J. Zhang, J. Leng, C. Li, W. Zheng, L. Li,
and M. Guo, “Themis: Predicting and reining in application-level
slowdown on spatial multitasking gpus,” in 2019 IEEE International
Parallel and Distributed Processing Symposium, (IPDPS), pp. 653–
663, IEEE, 2019.

[51] X. Zhao, M. Jahre, and L. Eeckhout, “HSM: A hybrid slowdown model
for multitasking gpus,” in Architectural Support for Programming
Languages and Operating Systems (ASPLOS), pp. 1371–1385, ACM,
2020.

[52] K. P. F.R.S., “On lines and planes of closest ﬁt to systems of points in
space,” The London, Edinburgh, and Dublin Philosophical Magazine
and Journal of Science, vol. 2, no. 11, pp. 559–572, 1901.

[53] P. Minet, E. Renault, I. Khouﬁ, and S. Boumerdassi, “Analyzing traces
from a google data center,” in 14th International Wireless Communi-
cations & Mobile Computing Conference, (IWCMC), pp. 1167–1172,
IEEE, 2018.

[54] M. P. I. Forum, “MPI: A message - passing interface standard,” 1994.
[55] Q. Chen, H. Yang, J. Mars, and L. Tang, “Baymax: Qos awareness
and increased utilization for non-preemptive accelerators in warehouse
scale computers,” in Proceedings of the Twenty-First International
Conference on Architectural Support for Programming Languages and
Operating Systems, ASPLOS 2016, Atlanta, GA, USA, April 2-6, 2016,
pp. 681–696, ACM, 2016.

[56] L. Tang, J. Mars, and M. L. Soffa, “Compiling for niceness: mitigating
contention for qos in warehouse scale computers,” in 10th Annual
IEEE/ACM International Symposium on Code Generation and Opti-
mization, CGO 2012, San Jose, CA, USA, March 31 - April 04, 2012,
pp. 1–12, ACM, 2012.

[57] Y. Xiang and H. Kim, “Pipelined data-parallel CPU/GPU scheduling
for multi-dnn real-time inference,” in IEEE Real-Time Systems Sym-
posium, RTSS 2019, Hong Kong, SAR, China, December 3-6, 2019,
pp. 392–405, IEEE, 2019.

[58] G. Neubig, C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anasta-
sopoulos, M. Ballesteros, D. Chiang, D. Clothiaux, T. Cohn, K. Duh,
M. Faruqui, C. Gan, D. Garrette, Y. Ji, L. Kong, A. Kuncoro, G. Ku-
mar, C. Malaviya, P. Michel, Y. Oda, M. Richardson, N. Saphra,
S. Swayamdipta, and P. Yin, “DyNet: The dynamic neural network
toolkit,” arXiv preprint arXiv:1701.03980, 2017.

[59] W. Cui, M. Wei, Q. Chen, X. Tang, J. Leng, L. Li, and M. Guo, “Ebird:
Elastic batch for improving responsiveness and throughput of deep
learning services,” in 37th IEEE International Conference on Computer
Design, ICCD 2019, Abu Dhabi, United Arab Emirates, November
17-20, 2019, pp. 497–505, IEEE, 2019.

[60] W. Cui, H. Zhao, Q. Chen, N. Zheng, J. Leng, J. Zhao, Z. Song, T. Ma,
Y. Yang, C. Li, and M. Guo, “Enable simultaneous DNN services
based on deterministic operator overlap and precise latency prediction,”
in The International Conference for High Performance Computing,
Networking, Storage and Analysis (SC), 2021.

[61] Y. Gan, Y. Zhang, D. Cheng, A. Shetty, P. Rathi, N. Katarki, et al., “An
open-source benchmark suite for microservices and their hardware-
software implications for cloud & edge systems,” in Proceedings of
the Twenty-Fourth International Conference on Architectural Support
for Programming Languages and Operating Systems, ASPLOS 2019,
Providence, RI, USA, April 13-17, 2019, pp. 3–18.

[62] R. Baghdadi, J. Ray, M. B. Romdhane, E. D. Sozzo, A. Akkas,
Y. Zhang, P. Suriana, S. Kamil, and S. P. Amarasinghe, “Tiramisu:
A polyhedral compiler for expressing fast and portable code,” in
IEEE/ACM International Symposium on Code Generation and Op-
timization (CGO), 2019.

[63] Google, “XLA: Optimizing compiler for TensorFlow.” https://www.

tensorflow.org/xla, 2020.

[64] Z. Jia, O. Padon, J. J. Thomas, T. Warszawski, M. Zaharia, and
A. Aiken, “TASO: optimizing deep learning computation with au-
tomatic generation of graph substitutions,” in Proceedings of the 27th
ACM Symposium on Operating Systems Principles (SOSP), pp. 47–62,
ACM, 2019.

[65] Google, “TensorFlow graph optimization with Grappler.” https://
www.tensorflow.org/guide/graph_optimization, 2021.
[66] T. Chen, L. Zheng, E. Q. Yan, Z. Jiang, T. Moreau, L. Ceze, C. Guestrin,
and A. Krishnamurthy, “Learning to optimize tensor programs,” in
Advances in Neural Information Processing Systems 31, pp. 3393–
3404, 2018.

[67] Z. Liu, J. Leng, Q. Chen, C. Li, W. Zheng, L. Li, and M. Guo, “DL-
Fusion: An Auto-Tuning Compiler for Layer Fusion on Deep Neural
Network Accelerator,” in IEEE International Conference on Parallel &
Distributed Processing with Applications (ISPA), pp. 118–127, IEEE,
2020.

[68] Z. Liu, J. Leng, G. Lu, C. Wang, Q. Chen, and M. Guo, “Survey and
design of paleozoic: a high-performance compiler tool chain for deep
learning inference accelerator,” CCF Trans. High Perform. Comput.,
vol. 2, no. 4, pp. 332–347, 2020.

15

