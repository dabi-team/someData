2
2
0
2

b
e
F
7
1

]

G
L
.
s
c
[

1
v
4
8
3
8
0
.
2
0
2
2
:
v
i
X
r
a

Limitations of Neural Collapse
for Understanding Generalization in Deep Learning

Like Hui
lhui@ucsd.edu

Mikhail Belkin
mbelkin@ucsd.edu

Preetum Nakkiran
preetum@ucsd.edu

Halıcıo˘glu Data Science Institute
University of California San Diego

Abstract

The recent work of Papyan, Han, and Donoho (2020) presented an intriguing “Neural Collapse” phenomenon,
showing a structural property of interpolating classifiers in the late stage of training. This opened a rich area of
exploration studying this phenomenon. Our motivation is to study the upper limits of this research program: How
far will understanding Neural Collapse take us in understanding deep learning? First, we investigate its role in
generalization. We refine the Neural Collapse conjecture into two separate conjectures: collapse on the train set
(an optimization property) and collapse on the test distribution (a generalization property). We find that while
Neural Collapse often occurs on the train set, it does not occur on the test set. We thus conclude that Neural
Collapse is primarily an optimization phenomenon, with as-yet-unclear connections to generalization. Second, we
investigate the role of Neural Collapse in representation learning. We show simple, realistic experiments where
more collapse leads to worse last-layer features, as measured by transfer-performance on a downstream task. This
suggests that Neural Collapse is not always desirable for representation learning, as previously claimed. Finally, we
give preliminary evidence of a “cascading collapse” phenomenon, wherein some form of Neural Collapse occurs not
only for the last layer, but in earlier layers as well. We hope our work encourages the community to continue the
rich line of Neural Collapse research, while also considering its inherent limitations.

1 Introduction

In science, and in deep learning, novel empirical observations often catalyze deeper scientific understanding (Kuhn,
1962). When faced with a new or surprising experiment, we can then try to understand the phenomenon more
precisely: How universal is the behavior? In what settings does it hold? Can we describe it quantitatively? What does
it teach us more generally? This overall roadmap for understanding —from observations to quantitative conjectures &
laws— has a long history of success in the natural sciences, and has also enjoyed recent successes in deep learning.

The recent “Neural Collapse” work of Papyan, Han, and Donoho
(2020) initiated another instance of such a research program in under-
standing deep learning. Their work presented a new experimental
observation, along with a partial characterization. At a high level,
Neural Collapse conjectures several structural properties of deep
neural networks when trained past the point of 0 classification error
on the train set. Their weakest conjecture— and thus the one most
likely to be true— is “variability collapse (NC1).” Variability collapse
proposes, informally, that when a deep network is trained on a k-
way classification task, the last-layer representations converge to
k discrete points. This is apriori surprising, since this internal struc-
ture is in no way required to achieve low train loss and high test
performance: there exist networks with identical decision bound-
aries which do not satisfy collapse. However, our standard training

1

Figure 1: Failure of Test Collapse. Neural
Collapse for ResNet18 on CIFAR-10. Collapse
occurs on the train set, but not on test.

020K40K60K80KSGDiterations100101FeatureVarianceNeuralCollapseonCIFAR-10TestTrain 
 
 
 
 
 
methods (Stochastic Gradient Descent and variants) on standard architectures and datasets empirically seem to satisfy
some form of collapse, as demonstrated in Papyan et al. (2020). This work has since inspired many follow-up works
investigating this phenomenon, both theoretically and empirically.

A motivating factor in this research program is the belief that Neural Collapse is not an isolated phenomenon, but
rather is deeply connected to other important and unsolved aspects of deep learning— in particular generalization.
The problem of generalization, informally, is the study of why a model trained on a finite set of samples has good per-
formance on out-of-sample inputs. Although this is not apriori related to Neural Collapse, the original work proposes
that collapse “confers important benefits, including better generalization performance, better robustness, and better
interpretability.” And it is stated as a hypothesis that “the benefits of the interpolatory regime of overparametrized
networks are directly related to Neural Collapse” (Papyan et al., 2020). This postulated connection between Neural
Collapse and generalization is implicit in many of the follow-up works as well, and motivates studying collapse as a
phenomenon.

However, the nature of the connection between Neural Collapse and generalization remains muddled. Some works
argue they are closely related (Poggio & Liao, 2020a; Banburski et al., 2021), while others cast some doubt (Elad et al.,
2020; Zhu et al., 2021). There are at least two reasons for this confusion in the literature: First, it is often not clear
whether Neural Collapse refers to a phenomenon on the train set, or on the test set. The behaviors most relevant to
generalization occur on the test set, and yet most experiments and theorems consider only the train set. Second, the
Neural Collapse conjectures do not precisely specify the role of the sample size, and thus it is not always clear how to
connect to generalization— where sample size is fundamental. This ambiguity is especially problematic because some
natural ways to extend the Neural Collapse conjecture to the test set turn out to be impossible to satisfy, as we will
describe.

Our Contributions. We study the powers and limitations of the Neural Collapse (NC) research program in
understanding deep learning. That is, we study what NC can explain about deep learning, and what it cannot. To do
so, we clarify several ambiguities surrounding the Neural Collapse conjectures, and carefully investigate its relation
to both generalization and optimization. Specifically:

1. We propose more precise versions of the Neural Collapse conjectures (“variability collapse”), stating different

versions for the train set and the test set, with both “strong” and “weak” forms. (Section 2)

2. We discuss the theoretical feasibility of these different conjectures. As we will see, strong test collapse is
extremely unlikely, while weak test collapse is in principle possible but does not occur in practice. (Section 2.1)
3. We empirically confirm the finding of Papyan et al. (2020), that train-collapse occurs in many realistic settings.

However, we find that test-collapse does not occur. (Section 3)

4. We show several settings where increasing train-collapse is anti-correlated with test performance, in both
on-distribution and transfer-learning settings. This demonstrates that train-time neural collapse is not always
desirable— and indeed, can be counterproductive— for many kinds of generalization. (Section 4).

5. As an addendum, we present a preliminary proposal of how Neural Collapse could be extended to describe
collapse of not only the final layer, but of earlier layers as well—a phenomena we term “cascading collapse.” We
leave fully investigating this as an interesting area for future work. (Section 5).

We thus conclude that Neural Collapse is primarily an optimization phenomenon, and not a generalization one.

1.1 Related Works

The Neural Collapse phenomenon was originally presented in Papyan et al. (2020), and led to a series of follow-up
works investigating and extending it. Many of the subsequent works develop simplified models in which Neural
Collapse on the train set can be theoretically proven and understood. For example, Fang et al. (2021) develops a
“layer-peeled” model of training, and explores neural collapse in class-imbalanced settings. Mixon et al. (2020) proposes
an alternate simplification, an “unconstrained features” model, in which train collapse also occurs. Wojtowytsch et al.
(2020) and Zhu et al. (2021) also investigate the train collapse under unconstrained features model. Several works
Poggio & Liao (2020a,b); Rangamani et al. (2021); Han et al. (2021) examine the Neural collapse with the square loss
under different settings. Specifically, Poggio & Liao (2020a,b) give theory which predicts the properties of neural

2

collapse for homogeneous, weight-normalized networks. Rangamani et al. (2021) proves that quasi-interpolating
solutions obtained by gradient descent in the presence of weight decay have Neural collapse properties. Han et al.
(2021) proposes a generic decomposition of the MSE loss which, under certain assumptions, results in a simplified
dynamical description (the “central path”) which exhibits neural collapse on the train set. Lu & Steinerberger (2020)
extend theoretical analysis of neural collapse to the cross-entropy loss (while previous works mainly considered
MSE loss). They prove neural collapse on the train set in the “unconstrained features” setting. Ergen & Pilanci (2021)
reformulate the last-layers of networks to convex formulations and give an explanation of Neural Collapse properties.
There are also other works (Ji et al., 2021a,b) investigating Neural Collapse under different settings.

However, all the above papers present results for Neural Collapse on the train set. Han et al. (2021) gives a preliminary
experiment (see Figure 12 in Han et al. (2021)) on the test set collapse. One of the few papers focusing on collapse at
test time is Galanti et al. (2021). Their work considers feature variance for test samples as well. However, the results
of Galanti et al. (2021) do not satisfy our definition of test-collapse, as we define it. Specifically, to be considered a
“collapse”, we require the collapse to occur for finite sample size n, but in the limit of SGD steps t
. We elaborate
on this important point in Section 2. In particular, Galanti et al. (2021) do not demonstrate test collapse in the relevant
regime according to our definitions.

→ ∞

1.2 Notation

Y

be the target distribution over

be the label space. We consider multi-class classification problems, where

be the input space, and
N. Let
(
X × Y

Let
= [k]
X
. Training procedures1 are functions which map a
for some k
N and to a model f . In this work, we will always consider
train set S
Stochastic-Gradient-Descent (SGD)-based training procedures, where t is the number of SGD steps. For a fixed train
set S of size n, let f t
f t
S,
where Train denotes the training procedure. For a given model f t
S, let the last-hidden-layer feature map be denoted

S denote the model output by the training procedure after t iterations. So Train : (S, t)

D
)n and an iteration count t

X × Y

∈
∈

(cid:55)→

X

∈

This is the feature-map induced by the trained model, as a map from inputs into Rd.

ht
S :

X →

Rd.

2 Defining Neural Collapse

We first define two kinds of Neural Collapse: on the train set, and on the test set. Our definitions naturally extend the
definitions in Papyan et al. (2020), but are more precise since we explicitly include the train/test distinction, and the
dependency on training iterations t and train samples n. This is essential to describe the relevant asymptotic limits in
the “collapse”.

Throughout this work, we focus only on the first, and weakest, conjecture from Papyan et al. (2020): “NC1 (Variability
Collapse).” The subsequent conjectures (NC2-4) are particularly meaningful only if NC1 is true. When we refer to
“neural collapse” in this work, we specifically are referring to “variability collapse.” We first define collapse on the
train set, which follows closely the definition in Papyan et al. (2020).

Definition 1 (Train-Collapse). For a particular train set S, we say a training procedure T exhibits Train-Collapse on S
if there exists some distinct µ1, µ2, . . . , µk

Rd such that

∈
(xi, yi)

∀

S :

∈

lim
t→∞

ht
S(xi) = µyi

That is, the trained network converges to representations such that all train points of class k get embedded to a single
point µk (called the “class means” in Papyan et al. (2020)). The conjecture below then states conditions under which
Train-Collapse occurs. This conjecture is meant to capture the original NC1 conjecture of Papyan et al. (2020), which
was demonstrated empirically across many settings.

1We can consider randomized training procedures by allowing an additional random string as input. We omit this randomness throughout, for

notational clarity.

3

Conjecture 1 (Train-Collapse Conjecture, informal). For all train sets S containing at least two distinct labels, and
all training procedures T corresponding to SGD on “natural” sufficiently-deep and sufficiently-large neural network
architectures: T exhibits Train-Collapse on S.

Crucially, we state Conjecture 1 for train sets of all sizes. This dependency on train set size is implicit, but omitted
from Papyan et al. (2020)— it will become especially important when we discuss test-collapse. This behavior is called
a “collapse” because regardless of the train set size, any big-enough network will converge to this discrete limiting
structure. We replicated this finding in most of our experiments. However, for completeness we acknowledge that
this conjecture does not hold fully universally, and there are subtleties in practice2. Nevertheless, we believe the NC1
conjecture captures the right qualitative behavior in many realistic settings.

We also acknowledge that Conjecture 1, while more precise than the conjectures in Papyan et al. (2020), is still not fully
formal. For example, it only applies to “natural” architectures and not all architectures, and does not quantify what
“sufficiently large” means. This restriction to “natural” architectures is a known obstacle to formalism in deep learning
theory (e.g. Nakkiran (2021)) and is necessary to avoid pathologies such as Abbe & Sandon (2020). Nevertheless, our
definitions take a step towards greater formalism, and this precision will be useful in understanding connections to
generalization. Refining our definitions and conjectures further is an area for future work.

The notion of train-collapse described above (and in Papyan et al. (2020)) is an optimization notion: it involves only
behavior of a model on its train set, and not behavior at test time. Thus, it is a priori unclear whether this notion is
related to generalization aspects of models. To explore this, we first extend the definition of Neural Collapse to the
test set, and then investigate whether this test-collapse occurs in practice. There are two natural ways to extend the
notion of collapse to the test distribution: a “weak” way and a “strong” way. Weak-collapse requires only that test
points embed as one of k discrete points µ1, µ2, . . . µk, without requiring that all points of class i map to µi.

Definition 2 (Weak Test-Collapse). A training procedure T exhibits Weak Test-Collapse on distribution
all sample sizes n
µ1, µ2, . . . , µk

N, the following holds with probability 1 over sampling S

if for
n: there exists some distinct

Rd such that

∼ D

D

∈

∈

with prob 1 over (x, y)

:

∼ D

lim
t→∞

ht
S(x)

µi

∈ {

}i∈[k]

Strong-collapse, on the other hand, requires that test points x map to their “correct” embedding point µi, where i is
the Bayes-optimal class for x.

Definition 3 (Strong Test-Collapse). A training procedure T exhibits Strong Test-Collapse on distribution
all sample sizes n
µ1, µ2, . . . , µk

N, the following holds with probability 1 over sampling S

if for
n: there exists some distinct

Rd such that

∼ D

D

∈

∈

with prob 1 over (x, y)

:

∼ D

lim
t→∞

ht
S(x) = µy∗(x)

where y∗(x) := arg maxy pD(y

x) is the Bayes-optimal classification under distribution
|

.

D

There are several important differences between the notions of test-collapse and train-collapse. First, for test-collapse
we require that the train set S is not arbitrary, but sampled from some distribution
. And we check for limiting
behavior with respect to new samples from
, as opposed to train samples from S. However, both train and test
collapse require the collapse to occur for all finite sample sizes n, letting only time t
. This is the meaningful
asymptotic, since taking limit of samples n
would obscure almost all aspects of learning, which is most
interesting at finite-sample sizes.

→ ∞

→ ∞

D

D

2.1 Remarks on Feasibility

With the above definitions, we can see that strong test-collapse is too strong a property to apply in realistic settings.
We discuss this infeasibility here, and then corroborate this with experiments in the following section.

2For example, we found in some settings training variability does not collapse to negligible value, such as CIFAR-10 and STL-10 dataset
with VGG architectures (see Figure 2) . In some preliminary experiments we also found that adding stochasticity (such as dropout noise) often
accelerated collapse, which is consistent with the theoretical model in Papyan et al. (2020).

4

Infeasibility of Strong Test-Collapse.
First, note that both train-collapse and test-collapse definitions require
N. This property is easy to satisfy for train-collapse, but is an extremely
that collapse occurs for all train set sizes n
strong property for test-collapse. In particular, the “strong” form of test collapse (Definition 3) is too strong to hold in
practice: it implies that a Bayes-optimal classifier can be extracted from the trained model features, even if the model
is trained on only e.g. n = 10 samples. This is because, according to Definition 3, the representation must map test
inputs to their “correct” cluster, and thus the correct label can be extracted from the cluster identity.

∈

However, the “weak” form of NC1-test (Definition 2) still has hope of holding, since it does not imply learning a
Bayes-optimal classifier. Nevertheless, note that even the “weak” form is a fairly strong condition for neural networks:
it implies that trained networks (on any size train set) learn feature-maps h such that the push-forward h∗(
) is
a discrete measure. Mapping the continuous measure
to a discrete measure is a strong property, and one that is
unlikely to hold for standard neural networks.

D

D

Feasibility of Weak-Collapse. While weak-collapse is unlikely to hold for neural networks trained with SGD, the
definition itself is non-vacuous: there exist learning methods which are “reasonable” (asymptotically consistent) and
exhibit weak test-collapse. To see this, consider the following modified training procedure: first, train a neural network
. Then, construct another network f (cid:48) such that the last-layer representation of f (cid:48)
as usual to get a network f :
Rk satisfies h(cid:48)(x) := (cid:126)ef (x)
is a one-hot encoding of the classification decision of f . That is, the representation h(cid:48)(x)
where
are standard basis vectors. This can be constructed by, for example, adding post-processing layers to
f . Now, the training procedure which outputs f (cid:48) will satisfy weak test-collapse of its representations, since its
representations are always one of the k standard basis vectors by construction.

X → Y

(cid:126)ei
{

∈

}

Desirability of Neural Collapse for Generalization. Armed with these definitions, we can now consider whether
train or test collapse are necessary or sufficient for on-distribution generalization. First, neither train nor test collapse
are strictly necessary for good generalization: As discussed, it is possible to construct models with identically good
generalization performance, but which satisfy neither train nor test collapse. There are even natural, non-contrived
examples of this: models trained for less than one epoch (the “Ideal World” in the terminology of Nakkiran et al.
(2020)) will not exhibit train collapse, because they are not trained to fit their train set. And yet, as demonstrated
in Nakkiran et al. (2020), they can match the performance of interpolating models. This “one epoch” regime is also
relevant in practice, where models are trained on massive data sources such as internet scrapes, often for less than
one epoch (Brown et al., 2020; Raffel et al., 2020; Komatsuzaki, 2019).

Further, neither train collapse (Definition 1) nor weak test-collapse (Definition 2) are sufficient for generalization. It
is possible to construct models which satisfy train collapse perfectly, but which are random functions at test time.
Likewise, it is possible to construct models which satisfy weak test-collapse, but have random classification decisions.

Strong test-collapse (Definition 3) is sufficient for good test performance, since it implies that test inputs map to the
“correct” cluster in representation-space. However, as we discussed, strong test-collapse is infeasible, and impossible
in practice.

3 Experiments: Train and Test Collapse

Here we complement our theoretical discussion by measuring both train and test collapse in realistic settings, following
the experiments of Papyan et al. (2020). We find that train-collapse occurs in many settings, while test-collapse (both
strong and weak) does not. We also show the dependency on the train set size: larger train sets lead to stronger test
collapse, but weaker train collapse. This further highlights the importance of distinguishing between the two forms of
collapse, since they can be anti-correlated in some settings.

3.1 Measuring Collapse

It is not possible to measure collapse strictly according to Definitions 1 to 3, since they involve a t
limit. Instead,
we follow exactly the experimental procedure of Papyan et al. (2020), and measure approximations which capture the

→ ∞

5

“degree of collapse.” We restate their procedure here for convenience. Measuring collapse require finding the vectors
µ1, µ2, . . . µk

Rd, which embeddings collapse to. The choice of these vectors depends on the setting, as below.

∈

Train Collapse. For the train set, µi is defined as the train class-means:

ˆµi := E

(x,y)∈S

[hT

S (x)

|

y = i]

where T is the maximum train time in the experiment. Define the global mean as ˆµ := (cid:80)
of train collapse” is measured as:

i ˆµi/

|Y|

. Then, the “degree

TrainVariance(t) :=

E(x,y)∈S[
Ei[

ht
S(x)
||
ˆµ
ˆµi
||

−

−
2]
||

ˆµy

2]
||

Smaller values of this quantity indicate more “collapse.” The numerator here is the “within-class variance” and it is
normalized by the “between-class variance”, in the terminology of Papyan et al. (2020). This definition follows the
experimental measurements in Papyan et al. (2020).

Strong Test Collapse. For test collapse, µi is defined as the test class-means:

The global mean is µ := (cid:80)

i µi/

|Y|

µi := E

(x,y)∼D

[hT

S (x)

|

y = i]

. Then, the “degree of strong test collapse” is measured as:

StrongTestVariance(t) :=

E(x,y)∼D[
Ei[

ht
S(x)
||
µ
µi −
||

−
2]
||

2]
µy||

Weak Test Collapse. For weak test-collapse (Definition 2), we do not require that representations collapse to their
class means, but simply to some µi. Thus, we define
as the result of k-means clustering on the following set of
vectors:

}

{ (cid:101)µi
hT
S (x)
x∈TestSet
}
{

The global mean is (cid:101)µ := (cid:80)

i (cid:101)µi/

|Y|

. And the “degree of weak test collapse” is measured as:

WeakTestVariance(t) :=

ht
E(x,y)∼D[arg mini∈[k] ||
S(x)
Ei[
2]
− (cid:101)µ
||

||(cid:101)µi

− (cid:101)µi

2]
||

3.2 Experimental Setup

We briefly describe the setup for the train and test collapse experiments, including the datasets, modern architectures
and training mechanisms.

Datasets. We consider image classification tasks with MNIST LeCun et al. (1998), FashionMNIST Xiao et al. (2017),
CIFAR-10 Krizhevsky et al. (2009), SVHN Netzer et al. (2011) and STL-10 datasets Coates et al. (2011). SVHN was
sub-sampled to N = 4600 samples per class as training set and N = 1500 samples per class for test set. Other
datasets are following the standard setup. No data argumentation was done and we pre-process the images pixel-wise
by subtracting the mean and dividing by the standard deviation.

Models. We train standard Resnet18 and DenseNet201 for MNIST, FashionMNIST, CIFAR10 and SVHN. Resnet50 and
DenseNet201 were trained for STL10. For all datasets we also train VGG11 with batch normalization. All models were
trained from scratch with open source code from torchvision models.

We use stochastic gradient descent (SGD) with momentum 0.9 and minimize the cross-entropy loss. All tasks were
trained on a single GPU with batch size 128 and 80000 SGD iterations. Initial learning rate is 0.1 for Resnet18 and
Resnet50 and 0.01 for DenseNet201 and VGG architectures. We decay the learning rate with cosine annealing scheme.

3.3 Results

6

Figure 2: Failure of Test Collapse. Training and test variance vs. SGD iterations, for various dataset and architecture
combinations. We train all models to 0 training error and continue training to achieve close to 0 training loss. All test
sets (black line) do not collapse to negligible variance, and have much less collapse than the train sets (purple line).

In this section we show that the test collapse does not occur with
experiments on a wide range of datasets and model architecture
combinations. We show that train collapse and test collapse can be
anti-correlated in some settings.

Failure of Test Collapse.
In Figure 3, we train a single model
(ResNet-18 on CIFAR-10) and measure TrainVariance, WeakTestVari-
ance, and StrongTestVariance as a function of train time t. That is,
we measure the degree of train and test collapse over increasing
time. We see that train collapse appears to occur, while test collapse
does not. In particular, there is a “generalization gap” in the Train
vs. Test Variances: the TrainVariance appears to converge to 0 as
, while TestVariance (both weak and strong) do not. For
t
the remainder of the experimental results, we plot only “strong”
test collapse, since we generally observe that both strong and weak
collapse have similar behavior.

→ ∞

In Figure 2, we train different models on various datasets and mea-
sure TrainVariance and StrongTestVariance as a function of train
time t. We train all models to get 0 training error and continue
training to achieve close to 0 training loss3. We see Strong Test

3We use “close to 0” to mean when the loss is below 10−5.

7

Figure 3: Neural Collapse on CIFAR-10. Col-
lapse occurs on the train set, but not on the
test set (neither Strong nor Weak). Weak test
collapse has smaller variance than Strong test
collapse.

6×10−17×10−18×10−1MNISTResNet1006×10−12×100VGG5×10−16×10−17×10−18×10−19×10−1DenseNetStrongTestTrain10−1100FashionMNIST10−11001012×1003×1004×10010−1100101CIFAR-101012×1003×1004×1006×1001001012×1003×1004×100SVHN1002×1003×1002×1003×1004×1006×100020K40K60K80KSGDiterations100101STL-10020K40K60K80KSGDiterations1012×1003×1004×1006×100020K40K60K80KSGDiterations100101020K40K60K80KSGDiterations100101FeatureVarianceNeuralCollapseonCIFAR-10StrongTestWeakTestTrainFigure 4: Train vs. Test Anti-Correlation. We vary the size of the train set (N ), and observe that train and test
collapse are anti-correlated. Top: ResNet18 trained on subsets of CIFAR-10. Bottom: VGG11 trained on subsets of
FashionMNIST.

Collapse does not occur on all settings, and has a large gap with Train Collapse. Again, the results show that Neural
Collapse is mainly an optimization phenomenon and not a generalization one: test set does not collapse to negligible
value in any setting. As an aside, we observe that even TrainVariance is not always close to 0, such as on CIFAR-10,
SVHN and STL-10 datasets.

Train vs. Test Anti-Correlation.
In Figure 4 we train a ResNet18 on CIFAR-10, and vary the size of the train set
from N = 12500, 25000, 30000, 40000 to N = 50000. We also report results on training a VGG11 network with
batch normalization on different subsets of FashionMNIST. We train all models past the point of 0 training loss and
note that in experiments of Figure 4 we stop training when the training loss decreases to 10−6. Figure 4 plots the
train collapse compared to the test collapse at the end of training, for different train set sizes. We find that as the train
set size increases, the test variation decreases (i.e. more test collapse), while the train variation increases (less train
collapse). This illustrates that test and train collapse are not always correlated, and thus it is important to distinguish
between the two: “better” optimization behavior accompanies worse generalization behavior.

One limitation of this experiment is that we evaluate collapse at finite train time, and not at t =
∞
we expect the train variation to be identically 0 for all data sizes (by the definition of collapse), but the test variation
to decay with larger data sizes. This situation is analogous to measuring train/test error itself for overparameterized
models: for large enough models, train error will always be 0, but test error will decay with the data size. This
experiment thus highlights the importance of measuring both train & test quantities, and the subtlety involved in
measuring collapse at finite time.

. Indeed, at t =

∞

We also acknowledge that in this experiment, increasing the size of the train set is correlated with both better test
collapse, and better generalization. However, we caution that this should not be seen as evidence that test collapse is
mechanistically related to generalization. First, because the test variance does not truly “collapse”, it just reduces, as
already discussed. And second, because this reduction in test variance is in some sense necessary for any model with
improved test error— since high test variance would produce noisy classification decisions. Thus, the correlation of
test variance and generalization in this experiment should not be surprising.

4 Collapsed Features Transfer Worse

In the previous section, we showed that train-time collapse can be anti-correlated with generalization performance,
when measuring generalization on-distribution. Now we investigate generalization on downstream tasks, to under-
stand the role of Neural Collapse in transfer-learning and representation learning.

8

0.060.080.100.120.140.160.180.201.01.21.41.61.82.0StrongTestVarianceCIFAR-10(varyingtrainsize)withResNet18N=12500N=25000N=30000N=40000N=500000.600.650.700.750.80TrainVariance2.22.42.62.83.03.23.43.63.8StrongTestVarianceFashionMNIST(varyingtrainsize)withVGGN=10000N=20000N=30000N=40000N=500004.1 Test Collapse implies Bad Representations

We first observe that, using our definition of test collapse, a model which has fully test-collapsed will have repre-
sentations that are bad for most downstream tasks. To see this, consider the following example. Suppose we have a
with ten types of images (as in CIFAR-10), but we group them into two superclasses, such as “animals”
distribution
and “objects.” We then train a classifier on this binary problem (e.g. CIFAR-10 images with these binary labels). Let
the feature map of the fully-trained model (that is, the limiting model as t
) be denoted h. If this model exhibits
even weak test collapse, then there exist vectors

→ ∞
such that the representations satisfy:

D

µ1, µ2

}

{
[h(x)

Pr
x∼D

µ1, µ2

∈ {

] = 1.
}

(1)

That is, the representations will by definition “collapse”: every input x
will map to exactly one of two points µ1, µ2.
This property is clearly undesirable for representation learning. For example, suppose we use these representations
for learning on a related task: the original 10-way classification problem. It is clear that no classifier using the fixed
representations from h can achieve more than 20% test accuracy on the original 10-way task: each group of 5 classes
will collapse to a single point after passing through h (by Equation (1)), and will become impossible to disambiguate
among these 5 classes. This shows that test collapse is undesirable for even an extremely simple transfer learning task
(where we transfer to the same distribution, with finer label structure). In the following sections, we will demonstrate
almost this exact example through experiments.

∼ D

Figure 5: Collapsed Features Transfer Worse. We save different checkpoints during pre-training, and use them to
initialize the downstream models. The x-axis shows the TrainVariance of those checkpoints on the pre-training train
set, and y-axis shows the test accuracy after fine-tuning on downstream tasks. We find that stronger train collapse (i.e.
lower variance) is correlated with lower downstream test accuracy. Left: MNIST with a 3 hidden layer fully-connected
network. Right: CIFAR-10 with a standard Resnet18.

4.2 Experimental Setup

The transfer learning setup follows the standard pre-training and fine-tuning scheme. We train a 3 hidden layer
fully-connected networks with 1024 units per layer on MNIST, and a standard Resnet18 on CIFAR-10. For pre-training,
we use a subset of the train set and perform 2-class classification (via super-classing). For fine-tuning, we use the
weights pre-trained as initialization of the weights other than the last classification layer, and do standard (10-class for
MNIST, and 8-class for CIFAR-10) classification with a held-out subset. We do not report results with linear probing,
as it gives much worse transfer-performance than fine-tuning scheme. See more details below.

Super-class pre-training. For MNIST, we set all odd numbers as one class and all even numbers as the other class.
We train the model with the first N = 1000 training samples as train set and the first N = 200 test samples as test
set.

9

0.140.160.180.200.22TrainVariance(pre-training)94.294.494.694.895.095.295.4Testaccuracy(downstreamﬁne-tuning)MNISTpre-trainingiterations=90kpre-trainingiterations=80kpre-trainingiterations=70kpre-trainingiterations=40kpre-trainingiterations=20k0.200.220.240.260.280.30TrainVariance(pre-training)89.089.289.489.689.890.0CIFAR-10pre-trainingiterations=90kpre-trainingiterations=70kpre-trainingiterations=60kpre-trainingiterations=40kpre-trainingiterations=20kFor CIFAR-10, we combine samples of ‘airplane, automobile, ship, truck’ as one (objects) class and ‘bird, cat, frog,
horse’ as the other (animals) class. The two classes are balanced and have 40000 training samples, and 8000 test
samples. We use a subset with N = 20000 training samples (to keep each class balanced, we randomly choose 2500
samples from each original class) and N = 4000 (500 samples from each original class) test samples for pre-training.

The learning rate for MNIST with fully-connected networks is 0.001 while for CIFAR-10 with ResNet18 is 0.1. We
decay learning rate with cosine annealing scheme. The models were trained minimizing the cross-entropy loss using
SGD with momentum 0.9 for 100000 SGD iterations.

Fine-tuning. We initialize the weights (other than the last classification layer) of the downstream task with the
pre-trained weights and fine-tune the whole network. For MNIST, we do the standard 10-class classification, while
we sample another 500 samples from training set for training and 100 samples from the test set for inference. For
CIFAR-10 we implement a 8-class classification (‘airplane, automobile, ship, truck, bird, cat, frog, horse’) with another
10000 training samples as train set and another 2000 test samples as test set.

The optimization methodology is the same as in pre-training, other than the learning rate. We search over 0.0005 to
0.25 in fine-tuning for both MNIST and CIFAR-10 and report the best test accuracy of all swept learning rates.

4.3 Results

Here we show transfer learning results on MNIST and CIFAR-10. As illustrated in Figure 5, we see that for both
MNIST and CIFAR-10, the checkpoints with more Train Collapse gives worse transfer-performance on downstream
tasks. That is, in these settings more Train Collapse actually leads to learning worse features. This demonstrates that
neural collapse does not always lead to good representation learning— and in some settings, collapse actually harms
representation quality.

5 Cascading Collapse

Previous sections demonstrated that Neural Collapse is primarily an optimization phenomenon. In this section, as
an addendum, we present preliminary investigations which extend the Neural Collapse conjectures by discussing
optimization dynamics of even earlier layers (beyond the last layer).

Existing work on Neural Collapse has focused on collapse of the
last-layer features. However, for sufficiently deep networks, the
mechanisms driving last-layer collapse could potentially extend to
preceding layers as well. Specifically, we know that at some late
stage in training, the last layer exhibits Train Collapse and remains
essentially fixed for the remainder of training. After this stage,
we may expect that this fixed last-layer drives a “collapse” in the
preceeding layer features, and then in the layer before that, and
so on. Here we give preliminary experimental evidence for such
a “cascading collapse,” as an optimization phenomenon. We do not
formally define cascading collapse nor establish it conclusively— this
section is meant only to suggest a potentially interesting avenue of
future work.

5.1 Experiments

Figure 6: Cascading collapse.

Setup. We train a fully-connected network with 3 hidden layer (1024 units per layer) on a subset of MNIST dataset.
The subset contains 1000 training samples (100 per class) and 200 test samples (20 per class).

We minimize the cross-entropy loss using SGD with momentum 0.9 for 80000 iterations. The initial learning rate is
0.001 and we decay the learning rate with cosine annealing scheme.

10

020K40K60K80KSGDiterations2×1003×1004×1006×100FeatureVarianceCascadingCollapse(3-layerMLPonMNIST)ﬁrstlayerpenultimatelayerlastlayerResults. We report one preliminary result in Figure 6. We measure TrainVariance as a function of train time t. The
“last layer” refers to the last hidden layer before the classification head. As can be seen in Figure 6, the last layer
features appear to collapse first, and has the greatest degree of collapse. The penultimate layer collapses qualitatively
later, and to a lesser extent than the last layer. And the first layer collapses the least, during the time scales observed.
We thus call this qualitative pattern of collapse among layers cascading collapse.

We acknowledge that this is a preliminary experiment, and there are many details to be determined. For example, it
is unclear whether earlier layers will collapse to 0 variance in the t
limit, or whether they will “saturate” at
some finite variance, as they appear to do in the finite-time experiments. Further, the relative speeds and sequential
ordering of the collapse, across layers, is unclear from this experiment. However, we believe that better understanding
the implications of Neural Collapse for internal layers is a fruitful research area, and we hope this section inspires
future work on cascading collapse.

→ ∞

6 Conclusion

We point out that Neural Collapse is primarily an optimization phenomenon, not a generalization one, by investigating
the train collapse and test collapse on various dataset and architecture combinations. We propose more precise
definitions— “strong” and “weak” Neural Collapse for both the train set and the test set— and discuss their theoretical
feasibility. We show that while train collapse reliably occurs in many settings, test collapse does not. We also show that
train collapse can be anti-correlated with test performance in both on-distribution and transfer learning settings. Our
theoretical formulations and empirical observations suggest that while neural collapse continues to be an intriguing
phenomenon and a promising optimization research program, its relevance to generalization may be limited.

Acknowledgements

LH thanks X. Y. Han for sharing experimental details. We thank Tatsunori Hashimoto for useful feedback on an early
draft.

We are grateful for support of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations
of Deep Learning4 through awards DMS-2031883 and #814639. We also acknowledge NSF support through IIS-1815697
and the TILOS institute (NSF CCF-2112665). We thank Nvidia for the donation of GPUs. This work used the Extreme
Science and Engineering Discovery Environment (XSEDE, Towns et al. (2014)), which is supported by National
Science Foundation grant number ACI-1548562 and allocation TG-CIS210104.

Author Contributions

LH conceived the project, designed and performed all experiments, and contributed to the writing. MB advised
the project, and contributed to the editing. PN managed the project, and contributed to the framing, writing, and
theoretical formulation.

4https://deepfoundations.ai/

11

References

Abbe, E. and Sandon, C. On the universality of deep learning.

In Larochelle, H., Ranzato, M., Hadsell, R., Bal-
can, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 20061–20072.
Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
e7e8f8e5982b3298c8addedf6811d500-Paper.pdf.

Banburski, A., De La Torre, F., Pant, N., Shastri, I., and Poggio, T. Distribution of classification margins: Are all data

equal? arXiv preprint arXiv:2107.10199, 2021.

Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,

A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Coates, A., Ng, A., and Lee, H. An analysis of single-layer networks in unsupervised feature learning. In Proceedings
of the fourteenth international conference on artificial intelligence and statistics, pp. 215–223. JMLR Workshop and
Conference Proceedings, 2011.

Elad, M., Simon, D., and Aberdam, A. Another step toward demystifying deep neural networks. Proceedings of the

National Academy of Sciences, 117(44):27070–27072, 2020.

Ergen, T. and Pilanci, M. Revealing the structure of deep neural networks via convex duality.

In International

Conference on Machine Learning, pp. 3004–3014. PMLR, 2021.

Fang, C., He, H., Long, Q., and Su, W. J. Exploring deep neural networks via layer-peeled model: Minority collapse in

imbalanced training. Proceedings of the National Academy of Sciences, 118(43), 2021.

Galanti, T., Gy¨orgy, A., and Hutter, M. On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121,

2021.

Han, X. Y., Papyan, V., and Donoho, D. L. Neural collapse under mse loss: Proximity to and dynamics on the central

path. ArXiv, abs/2106.02073, 2021.

Ji, W., Lu, Y., Zhang, Y., Deng, Z., and Su, W. J. How gradient descent separates data with neural collapse: A

layer-peeled perspective. 2021a.

Ji, W., Lu, Y., Zhang, Y., Deng, Z., and Su, W. J. An unconstrained layer-peeled perspective on neural collapse. arXiv

preprint arXiv:2110.02796, 2021b.

Komatsuzaki, A. One epoch is all you need. arXiv preprint arXiv:1906.06669, 2019.

Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.

Kuhn, T. The structure of scientific revolutions. University of Chicago Press, 1962.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings

of the IEEE, 86(11):2278–2324, 1998.

Lu, J. and Steinerberger, S. Neural collapse with cross-entropy loss. ArXiv, abs/2012.08465, 2020.

Mixon, D. G., Parshall, H., and Pi, J. Neural collapse with unconstrained features. arXiv preprint arXiv:2011.11619,

2020.

Nakkiran, P. Towards an Empirical Theory of Deep Learning. Doctoral dissertation, Harvard University Gradu-
ate School of Arts and Sciences, 2021. URL https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:
37370110.

Nakkiran, P., Neyshabur, B., and Sedghi, H. The deep bootstrap framework: Good online learners are good offline

generalizers. In International Conference on Learning Representations, 2020.

Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised

feature learning. 2011.

12

Papyan, V., Han, X. Y., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learning
ISSN 0027-8424. doi:

training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020.
10.1073/pnas.2015509117. URL https://www.pnas.org/content/117/40/24652.

Poggio, T. and Liao, Q. Explicit regularization and implicit bias in deep network classifiers trained with the square

loss. arXiv preprint arXiv:2101.00072, 2020a.

Poggio, T. and Liao, Q. Implicit dynamic regularization in deep networks. Technical report, Center for Brains, Minds

and Machines (CBMM), 2020b.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of
transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.

Rangamani, A., Xu, M., Banburski, A., Liao, Q., and Poggio, T. Dynamics and neural collapse in deep classifiers trained

with the square loss. 2021.

Towns, J., Cockerill, T., Dahan, M., Foster, I., Gaither, K., Grimshaw, A., Hazlewood, V., Lathrop, S., Lifka, D., Peterson,
G. D., Roskies, R., Scott, J., and Wilkins-Diehr, N. Xsede: Accelerating scientific discovery. Computing in Science &
Engineering, 16(05):62–74, sep 2014. ISSN 1558-366X. doi: 10.1109/MCSE.2014.80.

Wojtowytsch, S. et al. On the emergence of tetrahedral symmetry in the final and penultimate layers of neural

network classifiers. arXiv preprint arXiv:2012.05420, 2020.

Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning

algorithms. arXiv preprint arXiv:1708.07747, 2017.

Zhu, Z., Ding, T., Zhou, J., Li, X., You, C., Sulam, J., and Qu, Q. A geometric analysis of neural collapse with

unconstrained features. arXiv preprint arXiv:2105.02375, 2021.

13

