Evolving Character-level Convolutional Neural
Networks for Text Classiﬁcation

Trevor Londt ID , Member, IEEE, Xiaoying Gao ID , Member, IEEE, Bing Xue ID , Member, IEEE
and Peter Andreae ID , Member, IEEE

1

0
2
0
2
c
e
D
3

]
L
C
.
s
c
[

1
v
3
2
2
2
0
.
2
1
0
2
:
v
i
X
r
a

neural

convolutional

Abstract—Character-level

networks
(char-CNN) require no knowledge of the semantic or syntactic
structure of the language they classify. This property simpliﬁes its
implementation but reduces its classiﬁcation accuracy. Increasing
the depth of char-CNN architectures does not result in break-
through accuracy improvements. Research has not established
which char-CNN architectures are optimal for text classiﬁcation
tasks. Manually designing and training char-CNNs is an iterative
and time-consuming process that requires expert domain knowl-
edge. Evolutionary deep learning (EDL) techniques, including
surrogate-based versions, have demonstrated success in auto-
matically searching for performant CNN architectures for image
analysis tasks. Researchers have not applied EDL techniques to
search the architecture space of char-CNNs for text classiﬁcation
tasks. This article demonstrates the ﬁrst work in evolving char-
CNN architectures using a novel EDL algorithm based on genetic
programming, an indirect encoding and surrogate models, to
search for performant char-CNN architectures automatically. The
algorithm is evaluated on eight text classiﬁcation datasets and
benchmarked against ﬁve manually designed CNN architectures
and one long short-term memory (LSTM) architecture. Experi-
ment results indicate that the algorithm can evolve architectures
that outperform the LSTM in terms of classiﬁcation accuracy
and ﬁve of the manually designed CNN architectures in terms of
classiﬁcation accuracy and parameter count.

Index Terms—Character-level convolutional neural network,
evolutionary deep learning, genetic programming, text classiﬁca-
tion.

I. INTRODUCTION

DEEP learning [1] is a modern machine learning tech-
nique based on artiﬁcial neural networks. The ﬁeld of nat-
ural language processing (NLP) has signiﬁcantly beneﬁted
from the use of deep learning techniques in recent years
[2][3][4][5][6][7]. There are three prevalent deep learning
architectures concerned with NLP tasks: long-short term mem-
ory (LSTM) [8] , transformer networks [9] and convolutional
neural networks (CNNs) [10]. LSTMs exhibit relatively slow
inference speeds and are less performant than transformers
and CNNs with regards to text classiﬁcation accuracy [4].
Transformers are a recent innovation and have shown signif-
icant successes in many NLP tasks [3][6][7]. Their massive
complexity with trainable parameters in the order of hundreds
of millions presents critical experiment reproducibility chal-
lenges to researchers. State-of-the-art transformers are difﬁcult
to reproduce in lab conditions as they have a high training cost
in monetary terms. There are only a limited number of pre-
trained transformer models available for different languages.
CNNs have demonstrated excellent success in text classiﬁ-
cation tasks [4][5][11][12][13]. There are two paradigms avail-

able when using CNNs for text classiﬁcation tasks, namely:
world-level (word-CNN) [14] and character-level CNNs [4].
Word-level approaches are dependant on a word-model to
represent the text. The reliance on a pre-trained word-model
poses the potential problem of not having one available for
a particular language. Training new word models is computa-
tionally time-consuming and costly. There is also the technical
challenges of dealing with misspellings and words that may
not exist in the word-model. The other paradigm is char-
CNNs. No pre-trained language or word models are required.
They also do not require a costly pre-processing step of the
text data. In general, char-CNNs are not as accurate as word-
level CNNs or transformers. Adding depth has not given the
beneﬁt of improved classiﬁcation accuracy, as seen in image
classiﬁcation tasks. There is an open question in the research
literature of what is the optimal architecture for char-CNNs.
Little research has been performed to address these limitations.
Deep learning is an iterative process requiring the tuning of
many hyper-parameters and repeated experiments to test the
efﬁcacy of any potential architecture. It is a time consuming,
costly and a tedious process that requires expert skills and
domain knowledge. The task of ﬁnding optimal char-CNNs is
an NP-hard problem.

Evolutionary computation (EC) [15] is a collection of search
algorithms inspired by the principals of biological evolution, in
particular the concept of survival of the ﬁttest. EC methods use
a population of individuals (candidate solutions) to conduct a
simultaneous search during a limited time frame to improve
the optimisation of a speciﬁed objective function via the ex-
change of information between individuals in the population.
The exchange of information is one of the key motivating
factors of selecting EC methods for evolving char-CNNs in
this work. There is the potential that this information exchange
may reveal the essential characteristics of what makes a non-
performant char-CNN into a performant one. EC methods are
concerned with locating near-optimal solutions to NP-hard
problems.

Evolutionary deep learning (EDL) is the technique of using
EC methods to search for candidate CNN architectures com-
bined with the backpropagation algorithm to train any potential
candidate network architecture. EDL has demonstrated success
when searching for performant CNN architectures on image
classiﬁcation tasks [16][17][18]. EDL has not been used to
search for performant char-CNN architectures.

Motivated by the success of applying EDL techniques in
the image classiﬁcation domain, we propose a novel surrogate-
based EDL algorithm appropriate for searching the landscape

 
 
 
 
 
 
of char-CNN architectures for the text classiﬁcation domain.
The proposed algorithm is based on genetic programming (GP)
and an indirect encoding that is capable of representing novel
char-CNN architectures. The algorithm employs the use of
surrogate models to signiﬁcantly reduce the training time of
the candidate char-CNNs during the evolutionary process.

In summary, the contributions of the proposed algorithm

and work are:

1) A fully automated approach to constructing, training and
evaluating char-CNNs of variable length and complexity.
2) A surrogate model approach that signiﬁcantly reduces
the computational time required to evolve performant
char-CNNs.

3) An expressive indirect encoding that ensures that all
evolved candidate networks in the population are struc-
turally valid and trainable networks, thereby preventing
wasted computational power and time.

4) Evidence that branching (width) in the early stages of
a char-CNNs architecture may aid in improving classi-
ﬁcation performance.

5) Evidence that the genealogy of an evolved char-CNN
can provide insights into the architectural properties that
aid in improving char-CNN performance.

II. LITERATURE REVIEW

A. Background

1) Character-level Convolutional Neural Networks: Kim
et al. [14] were the ﬁrst
to use a CNN with pre-trained
word embeddings, word2vec [19], to perform sentence-level
text classiﬁcation. Their simple CNN architecture with one
convolutional layer and a single max-pooling layer outper-
formed state-of-the-art
traditional methods on four of the
seven datasets tested. Filter widths of 3, 4 and 5 each having
100 channels were implemented. Their choice of activation
function for non-linearity was the ReLU [20] function. It
should be noted that their model is shallow. Notably, their
baseline model with randomly initialised word embeddings
performed poorly relative to all other models. This ﬁnding
highlighted the importance of word2vec in their performance
gains. Another interesting ﬁnding was that the use of dropout
as a regularisation technique provided a 2-4% performance
increase across all datasets. Although their model achieved
good performance, it should be noted that all the datasets used
were relatively small in size. A limitation is that their work
was only conducted across English datasets and has not been
proven to work with other languages.

Zhang et al. [4] were the ﬁrst to conduct research on the
use of char-CNNs for text classiﬁcation. Their model had a
modular design using back-propagation [21] for gradient opti-
misation via a stochastic gradient descent [22] algorithm. The
main component of their design was a temporal convolutional
module that computed a one-dimensional convolution. Max-
pooling was implemented to allow their network to be deeper
than six layers. ReLU [20] was used for non-linearity. The
classiﬁer section of the network was two fully connected lay-
ers. The text was encoded by converting each character in the
sequence of text as a one-hot vector. The vector was created

2

according to a lookup table consisting of a predetermined
alphabet of length m. A maximum sentence length of 1014
characters was speciﬁed. A sequence of characters of length
j would have 1014 one-hot vectors, each of length m. Any
characters beyond the maximum length were ignored. Their
experiment was conducted over eight datasets. The datasets
were constructed by the authors from large publicly available
datasets and were chosen to represent different
tasks and
volume sizes. The datasets have now become the standard for
testing char-CNNs. The major ﬁnding of their paper was that
char-CNNs are an effective approach for text classiﬁcation. It
was shown that their model performed better on larger datasets
than smaller datasets. According to their ﬁndings, traditional
approaches performed well until the datasets approached the
scale of millions of instances. Another interesting insight was
that the choice of the alphabet made a signiﬁcant difference
in the accuracy performance. Zhang et al. [4] demonstrated
the utility of char-CNNs. However,
their model was not
particularly deep when compared to CNNs used for image
classiﬁcation tasks.

Conneau et al. [5] demonstrated the beneﬁts of adding
depth to a char-CNN with their very deep convolutional
neural network (VDCNN) model. Their model was built in a
modular format where they used the concept of a convolutional
block stacked multiple times in sequence one after the other.
Each convolutional block consisted of a convolutional layer
followed by a temporal batch normalisation [23] layer and then
a ReLu activation function. This sequence is repeated twice in
each block. Implementing shortcut links, inspired by ResNet
skip links [24], their model was able to be extended to a depth
of 29 layers. Their model outperformed all current state-of-
the-art models on the eight datasets introduced by Zhang et
al. [4]. VDCNN demonstrated the advantage of adding depth
to a char-CNN to increase performance. The caveat to their
ﬁndings was that depth only increased performance up to a
certain depth after which adding additional layers degraded
the model’s performance. Their deepest model reached 49
layers and had a reduced relative accuracy of approximately
3% compared to the 29 layer model over the yelp dataset. The
larger layer model was not tested over the other datasets.

Le et al. [25] conducted a study into the role of depth
for both char-CNNs and word-CNNs for text classiﬁcation.
Motivated by the success of the state-of-the-art DenseNet
[26] model used for image classiﬁcation tasks, Le et al.[25]
implemented both a world-level and char-level DenseNet
model. Their word-level DenseNet model used Word2vec for
the word embeddings. The character-level DenseNet model
used the same alphabet as in [4] and [5]. Both models were
tested on only ﬁve of the original datasets in [4]. Both models
performed comparatively similar to each other with the word-
level DenseNet model being marginally better. Both models
only slightly under-performed the shallower model in [27].
The main ﬁnding of their research is that adding depth to
CNNs for text classiﬁcation is still not a well-understood
technique. Although there has been an increase in performance
with depth, the increase has not been substantial. A second
ﬁnding is that the state-of-the-art DenseNet model did not
provide the same breakthrough improvements as seen in image

classiﬁcation tasks. The authors conclude that if a char-CNN
is to be used then the model must be deep. However, it is not
yet known what architectures can further improve char-CNN
performance to the level of word-CNNs performance.

B. Related work

The search for network architectures is currently an in-
teresting and challenging research task. However, evolving
char-CNNs for text classiﬁcation is a nascent research topic
and there is no research work directly related to evolv-
ing char-CNNs. However it
is worth noting the work of
Liang et al. [28]. Their work presented an evolutionary-
based framework named LEAF that simultaneously evolved
network architectures and optimised hyperparameters. Their
algorithm consisted of three components: an algorithm layer,
a system layer and a problem-domain layer. The algorithm
layer was responsible for evolving network topologies and
hyperparameters. The system layer distributed the training of
the networks across multiple cloud computing services. The
algorithm and system layer cooperated to support the problem-
domain layer, and the problem-domain layer performed hyper-
parameter tuning and architecture search. The algorithm layer
was based on a cooperative co-evolution algorithm named
CoDeepNEAT [29]. A population of network architectures of
minimal complexity and size were initially generated. The net-
work architectures were all encoded as graphs. CoDeepNEAT
was based on the NEAT [30] algorithm, where a mutation
operation adds new nodes or connections to the network. The
alignment of parent chromosomes facilitated the crossover
operation according to historical markings placed on the genes
of the chromosomes during the evolutionary process. This ap-
proach allowed segments of the network to be crossed over and
remain a valid network structure. CoDeepNEAT differs from
NEAT in that instead of nodes representing neurons; layers
are represented instead. Layers can be components such as
convolutional layers, LSTM layers and fully connected layers.
The nodes also encapsulated the associated hyperparameters
such as kernel size and activation function type. Notably,
the algorithm used an indirect encoding. Their algorithm was
benchmarked on an image dataset, chest x-rays [31], and
on the Wikipedia comment toxicity dataset. Although this
algorithm evolved networks for text classiﬁcation tasks, the
networks were based on the LSTM paradigm and not a char-
CNN approach. Further, their work was not applied on datasets
commonly used to test char-CNNs.

III. PROPOSED ALGORITHM

A. Network Architecture Encoding

Many evolutionary inspired network architecture search
algorithms employ a direct encoding where the layers of the
network are stated explicitly. This direct encoding approach
often results in networks with questionable architecture ar-
rangements, for example placing fully connected layers before
convolutional layers, or worse, networks that are not fully
formed or trainable. Further, direct encodings are susceptible
to evolutionary operators being destructive on the network
architecture. For example it is easy for a crossover operation to

3

destroy the topology of a valid network architecture, resulting
in wasted compute power. Special care needs to be taken when
designing evolutionary operators for direct encodings.

Indirect encodings specify indirectly how a network should
be constructed through the use of program symbols, grammars
or production rules. Networks can therefore be constructed in
an incremental manner, ensuring that the structural integrity
of a network is always maintained. Further, since evolutionary
operations such as crossover are conducted on the data struc-
ture containing the program symbols to be executed, and not
the network itself, the result will still generate a structurally
valid neural network.

An appropriate architecture encoding scheme is required to
study the role of both depth and width (branching) in char-
CNNs. The scheme must be complex enough to capture the
properties of depth and width but also simple enough so as not
to introduce additional variables of complexity. An encoding
scheme representing a subset of cellular encoding [32] oper-
ations is proposed. Cellular encoding draws inspiration from
observation of cell divisions as seen in the biological domain.
The encoding was originally designed to encode multi-layered
perceptron (MLP) networks where the nodes in a MLP were
represented as a biological cell to be operated on.

Fig. 1: Network cell.

The chosen operations are the sequential (SEQ) and parallel
(PAR) division operations. These two operations are a suitable
choice as their application on a network’s cell can construct
network architectures of varying depth and width. A cell is
deﬁned as a convolutional block as used in [5] and presented
in Figure 1. The SEQ and PAR operations are therefore applied
to cells. Only one operation is applied to any given cell.

An ancestor network is deﬁned as a cell coupled with
an embedded input layer and an output layer. The output
layer of the ancestor network consists of a temporal adaptive
average pooling layer [33] followed by a fully connected layer.
The cross-entropy loss is propagated backwards through the
ancestor network.

In essence, the deﬁned indirect encoding scheme represents
a program consisting of a sequence of operations to be
performed on the cells of an ancestor network, making GP
an appropriate and natural choice to evolve cellular encoded
programs.

B. Cellular Operations

1) SEQ operation: This operation produces a new cell
(child cell) from the cell on which it operates (mother cell).
The child cell is connected sequentially to the mother cell.
The output of the mother cell is reassigned to the input of the

Temporal convolutionTemporal convolutionTemporal convolutionBatch normalisationTemporal convolutionTemporal convolutionReLUTemporal convolutionBatch normalisationReLUchild cell. The output of the child cell is, in turn, assigned to
the original output of the mother cell.

the depth of a network and a PAR operation contributes to the
width of a network.

4

2) PAR operation: This operation also produces a child cell
from the mother cell. However, the child cell is assigned a
new kernel size and connected in parallel to the mother cell.
The kernel size is selected from a list of possible values. The
chosen list includes kernel sizes of 3, 5 or 7. These values
are optimum with regards to char-CNNs [5]. The selection
is based on a shift-right method. If the mother cell has a
value of 3, then the child kernel size is assigned a value
of 5. A mother cell with a kernel size of 7 will result in a
child cell with a kernel size of 3. This method is deterministic
and required in order to ensure that the same phenotype can
be constructed consistently from a given genotype. The input
and output destination of the child cell are assigned the same
input and output destination cells as the mother cell. This
implies that if the mother and child cell are connected to
another cell and not the output layer, then a concatenation
operation is to be performed in the destination cell’s input.
The concatenation operation is simply the stacking of each
incoming cell’s channels on top of each other. For example, if
two cells, each having 64 output channels, connect to the same
destination cell, then the destination cell will have 128 input
channels. In order to make the concatenation of input channels
possible, due to the varying temporal lengths resulting from
different kernel sizes, padding with a zero value is used to
extend the temporal dimension to equal size lengths. A legend
is provided in ﬁgure 2 to aid in the description of genotypes
and phenotypes for the remainder of this work.

Fig. 2: Colour legend describing genotypes and phenotypes.

(a) Ancestor.

(b) SEQ operation.

(c) PAR operation.

Fig. 3: Smallest possible phenotypes.

When discussing the genotype, each cellular operation is
represented by a colour-coded circle, as indicated in ﬁgure 2.
The cells in a network (phenotype) are represented by coloured
circles where the colour is related to the assigned kernel size.
A double circle represents a cell with more than the default
64 input channels, indicating that a concatenation operation
has occurred previously along the path of the input layer to
the current cell. An example of the smallest phenotypes that
can be constructed from the ancestor phenotype is displayed
in ﬁgure 3. Each phenotype is displayed alongside its relevant
genotype. It can be seen that a SEQ operation contributes to

C. Surrogate Models

To aid in reducing the computational time for evaluating
char-CNNs, this work makes use of half precision (16-bit)
training. This work was conducted over four retail RTX 2070
cards. These RTX cards contain tensor cores1 that are signiﬁ-
cantly faster than CUDA cores. Tensor cores are only activated
under certain conditions, one of which is using half precision
training. Nvidia states that tensor cores reduce the training
time of deep neural networks by a factor between 2 and 4
depending on the task. There is a slight trade-off in reduced
accuracy. We refer to models trained using half precision as
surrogate models. The loss in accuracy performance is not
relevant when using evolutionary deep learning techniques
as we are only interested in evolving surrogate phenotypes
and then using full resolution (32-bit) training for the ﬁttest
phenotype. An added beneﬁt of using lower resolution training
is that the GPU’s memory is effectively doubled. However, this
poses the problem of producing surrogate phenotypes that ﬁll
the entire available GPU memory and implies that the full
resolution version of the phenotype will be too large to ﬁt
in the GPU’s available memory. To overcome this potential
problem, the high resolution phenotype is always trained over
two GPU’s. Nvidia’s Apex AMP library2 was used to enable
half precision training.

D. Algorithm Overview

The proposed algorithm evolves genotypes, represented as
GP trees containing program symbols, by using evolutionary
crossover and mutation operators. These program symbols rep-
resent actions that are to be performed when constructing the
network architecture. The evolved genotypes are decoded, by
executing the program symbols, to construct phenotypes which
represent trainable network architectures. The phenotypes are
trained using the backpropogation algorithm, and their ﬁnal
validation accuracy is used to evaluate the ﬁtness of the
phenotype relative to all other phenotypes in the population.
The use of surrogate models enables the phenotypes to be
trained signiﬁcantly faster. At the end of the evolutionary
process, the ﬁttest surrogate phenotype is automatically located
and trained as a non-surrogate phenotype. The trained non-
surrogate phenotype is then evaluated on the test set. The
algorithm terminates by presenting the genealogy of the ﬁttest
phenotype for analysis.

The framework of the proposed algorithms is presented in
Algorithm 1. The algorithm is assigned a randomly generated
seed. The population is then initialised as detailed in section
III-D2. During the evolutionary process, each genotype in the
population is ﬁrst decoded into its half resolution (16-bit) phe-
notype, which represents a trainable CNN. The phenotype is
uploaded to the GPU. If the phenotype is too large to ﬁt in the
memory the GPU, the phenotype is destroyed and the genotype

1https://www.nvidia.com/en-us/data-center/tensorcore/
2https://github.com/NVIDIA/apex

aaaaaaaaSEQPARk=3k=5k=7ch.>64InputOutputaENDGenotype.Phenotype.abcINPUT33OUTPUTINPUT35OUTPUT3

4

5

6

7

8

9

10

11

12

13

14

Algorithm 1: Proposed algorithm.
1 begin;
2

seed ← Assign next seed from list.
population ← genotypes with depth range [1,3].
while not maximum generations do
foreach genotype ∈ population do

GPU ← phenotype ← decode(genotype);
while phenotype not accepted by GP U do
genotype ← genotype depth halved;
GPU ← phenotype ← decode(genotype);

end while
evaluate(genotype, reduced train. set, val. set);

end foreach
elite ← ﬁttest from population;
selected ← tournament(population);
of f spring population ← crossover(selected);
population ← mutate(of f spring population);
limit(population ∪ elite);

15
16 end while
17 f ittest ← population;
18 GPU ← 32-bit phenotype ← decode(f ittest);
19 evaluate(f ittest, f ull train. set, test set);
20 end;

is reduced in depth by a factor of two. The modiﬁed genotype
is then decoded to a new phenotype and again transferred
to the GPU. This process is repeat until the a phenotype is
accepted. The phenotype on the GPU is then trained using
the reduced training set and evaluated on the validation set.
After each genotype and its corresponding phenotype has been
evaluated, the elite population is constructed from the ﬁttest
10% of the population. These elite individuals are copied over
to the next generation without any modiﬁcation to them. A
tournament selection function is then applied to the entire
population. The selected individuals are used to produced
new offspring via the single point crossover operation. A
uniform mutation operation is then applied to the offspring.
A new population is then constructed consisting of the newly
generated offspring and the elite population. This process is
repeated till the maximum number of generations has been
achieved. The ﬁttest individual from the ﬁnal population is
selected and decoded as a full resolution (32-bit). This full
resolution phenotype is then trained using the full training set
and evaluated on the test set.

1) Evolutionary operators: The proposed method uses sin-
gle point crossover operations. Two selected genotypes are
crossed over at a randomly selected position, resulting in
two offspring genotypes. The mutation operation selects a
random position in the genotype and connects a randomly
generated sub-tree. This produces a small change in the
genotype program which translates to a physical alteration in
the construction of the phenotype. The crossover and mutation
operations described were selected for their simplicity as
future research will investigate novel crossover and mutation
methods.

5

2) Population Initialisation:

IV. EXPERIMENTAL DESIGN

A. Peer Competitors

There is no research using EDL to evolve char-CNNs in
the literature, therefore to test the efﬁcacy of the SurDG-EC
algorithm, a comparison is conducted against an algorithm
using the same encoding but with no evolutionary operators
(SurDG-Random). The ﬁttest evolved phenotype located by
both the SurDG-EC algorithm and SurDG-Random are re-
trained as higher resolution phenotypes and compared against
three hand-crafted state-of-the-art char-CNNs: Zhang et al’s
[4] small and large models (Small Char-CNN, Large Char-
CNN) and Conneau et al’s [5] model (VDCNN-29). All these
models are pure character-level models, meaning that there is
no data augmentation or pre-processing of the input text data.
A comparison is also made against three word-level CNNs
using the popular Word2vec model. The peer competitor
results are reported from [4] and [5].

B. Benchmark Datasets

Zhang et al.’s [4] work on the ﬁrst char-CNN was tested
on eight datasets as listed in Table I. The datasets are con-
sidered to be the standard for evaluating text classiﬁcation
performance of char-CNNs.

Dataset
AG’s News
Sogou News
DBPedia
Yelp Review Polarity
Yelp Review Full
Yahoo! Answers
Amazon Review Full
Amazon Review Polarity

Classes
4
5
14
2
5
10
5
2

Train
112,852
397,058
497,777
524,414
603,571
1,342,465
2,465,753
3,240,000

Validation
7,148
52,942
62,223
35,586
46,429
57,535
534,247
360,000

Test
7,600
60,000
70,000
38,000
50,000
60,000
650,000
400,000

TABLE I: Datasets by training, validation and test splits.

The AG’s News dataset is regarded as a challenging dataset
to classify because it contains a small number of instances.
This dataset was chosen for this work as there is still potential
in improving the classiﬁcation accuracy over it. The remainder
of the datasets were not used in the evolutionary process;
however, they were used to evaluate the ability of the ﬁttest
evolved phenotype to generalise over the other unseen datasets.
It is noted that neither of the original eight datasets had a
validation set. Therefore the original training sets were each
split into a reduced training set and a validation set. The split
ratio was kept the same as between each original training set
and test set. The original test sets remained unaltered.

An analysis of the instances in each dataset is listed in Table
II. Zhang et al.’s [4] original char-CNN used a temporal length
of 1014 characters. Most of the instances in the AG’s News
dataset are closer to 256 characters. Setting the temporal length
to 1014 would imply unnecessary padding and convolutional
operations, resulting in wasted compute power and time.
Therefore this work used a maximum sentence length of
256 characters. This aided in improving model training times
without the loss of signiﬁcant discriminative information from
each instance with the regards to the AG’s News dataset. It is

noted that the other seven datasets have a mean length greater
than 256, implying that important sentence data may have
been truncated when the evolved architecture was evaluated
on them.

Dataset
AG’s News
Sogou News
DBPedia
Yelp Review Polarity
Yelp Review Full
Yahoo! Answers
Amazon Review Full
Amazon Review Polarity

Mean Minimum Maximum
1,012
100
185,674
40
13,574
12
8,787
10
5,849
10
8,191
12
1,884
96
1,981
71

236±66
2,793±3,338
301 ±139
725 ±669
732±664
520±577
439±240
430±237

TABLE II: Sentence lengths.

C. Parameter Settings

The parameters for the components of the experiment are
listed in Table III. Thirty runs were conducted for each
algorithm, where each run was assigned a single unique
seed. Every surrogate phenotype was trained for ten epochs.
This approach ensured that evolutionary pressure to converge
quickly to high accuracy was applied to all evolved pheno-
types. Limiting the epochs to 10 also aided in reducing the run
time of the experiment. The batch size, initial learning rate and
learning schedule were assigned values as in [4] and [5]. These
values are considered best practice. An AMP proﬁle of O2
was used to generate the low-resolution surrogate models. The
selected optimiser was a stochastic gradient descent (SGD)
function. SGD was used in both [4] and [5]. The initial settings
of the cellular cells were the same as the convolutional blocked
used in [5].

Each run consisted of 30 generations over a population
size of 30. There is much research on determining what is
the best ratio of generation size to population size for evolu-
tionary algorithms, however not with regards to the domain
of evolutionary deep learning, where computation times are
signiﬁcantly longer than most other domains. A pragmatic
approach was taken by setting the number of generations and
population size to be equal to each other with the assumption
that any effect of increasing one over the other is neutralised.
The elitism, crossover and mutation rate settings were based
on common values found in the literature [34]. The mutation
growth depth was set to a maximum of size two, ensuring that
a mutation event did not signiﬁcantly change the structure of
the genotype to prevent the possibility of destroying the quality
of the phenotype. The maximum tree depth was set to 17,
as recommended in [34]. The GPU rejection re-initialisation
was set to a maximum of depth 4. A low value was chosen
to increase the odds of the GPU accepting the model on any
further submission attempts. The ﬁtness function was the same
as in [4] and [5]. The overall objective of the evolutionary
process was to maximise the validation accuracy. Momentum

D. Statistical Tests

Thirty random seeds were generated before any algorithms
or searches were conducted. Each seed was used to conducted
one run of the SurDG-EC algorithm and one run of SurDG-
Random separately, in different application processes. It is

Parameter
Run count
Random seeds
Deep Learning:
Epochs
Batch Size
Initial Learning Rate
Momentum
Learning Schedule
Weight Initialisation
Surrogates:
AMP Proﬁle
AMP Optimiser
Training Data Usage
Character-level Model:
Alphabet
Max Sentence Length
Cellular Cell - Initial
In and Out Channels
Activation Function
Kernel size
Stride
Padding
Evolutionary:
Number of Generations
Population Size
Elitism
Crossover Probability
Crossover Type
Mutation Probability
Mutation Distribution
Mutation Growth Type
Mutation Tree Growth Size
Tournament Selection Size
Primitives
Terminals
Max Tree Depth
Initial Tree Depth
Initial Tree Growth
GPU Rejection Tree Re-initialisation
Fitness Function

6

Value
30
Unique per Run

10
128 [4],[5]
0.01 [5]
0.9 [5]
Halve every 3 epochs [4]
Kaiming[35][4],[5]

SGD [4]
0.25

Same as in [4] and [5]
256

64 [5]
ReLu [5]
3 [5]
2 [5]
1 [5]

30
30
0.1
0.5
Single Point
0.1
Uniform
Grow
[1,2]
3
{SEQ, PAR} [32]
{END} [32]
17 [34]
[1,3]
Half and Half
[1,Half previous]
max(validation accuracy) [4] [5]

TABLE III: Parameter settings.

noted that both runs were conducted on the same hardware.
Both the SurDG-EC algorithm and SurDG-Random were
conducted on the exact same reduced training set. This implies
a paired coupling of observations. As the distribution of
the samples is not know and the observations are paired, a
Wilcoxon signed-rank test was used.

The ﬁttest phenotype from each run of SurDG-Random was
compared against the ﬁttest phenotype from each SurDG-EC
run. This translates to the 30 ﬁttest phenotypes located by
SurDG-Random compared to the 30 ﬁttest phenotypes located
by the SurDG-EC algorithm. A signiﬁcance level of 0.05 was
applied. The null hypothesis was that the distribution of the
observations of both methods came from the same population.

V. RESULTS AND ANALYSIS

A. Overall results

The aggregated validation accuracies of the surrogate phe-
notypes generated by both SurDG-Random and the SurDG-EC
algorithm are presented in Figure 4. The distribution of the
validation accuracies sampled by SurDG-Random, presented
in light blue, represents a normal distribution. This result
indicates that sufﬁcient samples were extracted to represent
is noted that SurDG-
the overall population landscape. It
Random located a negligible number of surrogate phenotypes

7

Fig. 4: AG’s News: Distributions of validation accuracies including competitor test accuracies.

with validation accuracies of less than 60 percent. These were
omitted to improve visual clarity in the ﬁgure. Zhang et al.’s
[4] original char-CNN model has a test accuracy close to the
mean of the surrogate phenotypes validation accuracy located
by SurDG-Random. This result suggests, with caution, that
the search space may contain surrogate phenotypes that are
on average, similarly performant as Zhang et al.’s [4] original
char-CNN model.

The distribution presented in orange represents the vali-
dation accuracies sampled by the SurDG-EC algorithm. The
distribution consists of the ﬁnal population of each run of the
SurDG-EC algorithm, representing 900 surrogate phenotypes.
It is noted that the distribution loosely represents half a normal
distribution. This observation is expected as the lower bound
is clipped due to only the ﬁttest surrogate models surviving
up to the end of an evolutionary run. It is easily observable
that the mean of this distribution is shifted from the mean
the
of SurDG-Random’s distribution. It can be seen that
right-hand tail of the SurDG-EC distribution extends further
than the right-hand tail of SurDG-Random’s distribution. The
SurDG-EC algorithm has located higher accuracy models
compared to those found by SurDG-Random. Application
of the Wilcoxon signed-rank test resulted in a rejected null
hypothesis implying that the distributions were signiﬁcantly
different. The ﬁttest surrogate phenotypes located by the
SurDG-Random and SurDG-EC algorithm achieved validation
accuracies of 87.57% and 89.03% respectively. The genotype
that generated the ﬁttest SurDG-EC surrogate phenotype was
used to construct a higher resolution phenotype. This higher
resolution phenotype was then trained on 100% of the reduced
training set and evaluated on the same test set used in [4] and
[5]. It is noted that both Zhang et al.’s and Conneau et al.’s
models were trained on the original training set of 120,000
instances whereas the SurDG-EC algorithm could only be
trained on the reduced training set of 112,852 instances in
order not to introduce training bias. This decision potentially
limited the accuracy during the training of the SurDG-EC
algorithm and gave an unfair advantage to Zhang et al.’s and
Conneau et al.’s models. Regardless, the ﬁnal test accuracy
achieved by the trained full resolution phenotype was 90.72%
as indicated by the red dashed line in ﬁgure 4. The full
resolution phenotype outperformed Zhang et al.’s model by
6.37% and under-performed Conneau et al.’s model by only
0.61%.

Measure
Validation Accuracy (Best)
Train Time (Seconds)
Parameter Count
SEQ operations
PAR operations
Crossover operations (30 runs)
Mutation operations (30 runs)

SurDG-Random
83.26±3.8 (87.57)
141.60±132.42
1,034k±501k
3,716
3,595
-
-

SurDG-EC
86.42±0.42(89.03)
134.11±90.59
441k±77k
3,046
634
5,732
2,358

TABLE IV: AG’s News: SurDG-Random vs SurDG-EC.

B. Comparison of SurDG-random and SurDG-EC

The average validation accuracies achieved by both the
SurDG-Random and SurDG-EC algorithm are listed in Table
IV. Average accuracies of 83.26% for SurDG-Random and
86.42% for the SurDG-EC algorithm were attained. The aver-
age training time of surrogate phenotypes from both methods
are roughly similar at 141 and 134 seconds. SurDG-Random
has a higher standard deviation, indicating a wider spread of
training times from the mean train time when compared to
the SurDG-EC algorithm. This ﬁnding is not surprising as
SurDG-Random is likely to have covered a wider search area,
indicating a broader range of trainable parameter sizes. The
SurDG-EC algorithm found better solutions, in general, in less
time than SurDG-Random.

The ratio between SEQ and PAR operations executed during
SurDG-Random was approximately 50:50 at 3,716 SEQ and
3,595 PAR executions. This ratio is expected as each operation
has a 50% chance of being selected when constructing the
genotype. Interestingly, the SurDG-EC algorithm has a higher
number of SEQ operations to PAR operations executed. This
observation indicates that SEQ operations played a more
prominent role in achieving higher validation accuracies dur-
ing the evolutionary process. In general, more SEQ operations
hint at deeper networks, agreeing with the ﬁndings in [25], that
deeper character-level CNNs are more accurate than shallow
ones.

The number of crossover and mutation operations exe-
cuted were 5,732 and 2,358, respectively, for the SurDG-
EC algorithm as listed in Table IV. Running the SurDG-
EC algorithm 30 times with a population of size 30 over
30 generations gives a potential of 27,000 model evalua-
tions that could be performed. With an elite population of
10%, the number reduces to approximately 24,390 as any
model in the elite population is only evaluated once. The
crossover operator is applied pairwise to each individual in
the population and its neighbour. This technique limits the

405060708090100Accuracy020406080100120140Number of ModelsZhang Small (32-bit)[db=100%][Test=84.35%]VDCNN29 (32-bit)[db=100%][Test=91.33%]SurDG-EC Best (32-bit)[db=100%][Test=90.72%]SurDG-EC Best (16-bit)[db=25%][Val=89.03%]SurDG-Random Surrogates (16-bit)[db=25%][Val.]SurDG-EC Surrogates (16-bit)[db=25%][Val.]maximum number of crossover operations to 12,195, assuming
a crossover rate of 100%. However, the crossover probability
is set at 50%, limiting the maximum number of crossover
operations to 6,097, which is close to the reported value of
5,732 operations. The mutation rate of 10% resulted in 2,358
mutation operations being executed. This value translates to
10% of the possible 24,390 potential evaluation operations.
The reported crossover and mutation values are consistent with
their settings. This observation highlights one aspect of the
veracity of the SurDG-EC algorithm, namely that the correct
percentage of evolutionary operations have been performed.

(a) SurDG-Random.

(b) SurDG-EC.

Fig. 5: Density of number of SEQ vs PAR operations.

The distribution of the number of SEQ and PAR operations
that constitutes each phenotype is presented in Figure 5 with
sub-ﬁgure 5a representing SurDG-Random and sub-ﬁgure 5b
the SurDG-EC algorithm. It can be seen that all the surrogate
phenotypes located by SurDG-Random, cluster around the
blue diagonal centre line. This behaviour is due to each cellular
operation (SEQ and PAR) having a 50:50 chance of being
selected when constructing the genotype. It is doubtful that
a phenotype with 20 SEQ operations and 1 PAR operation
would be located without the aid of an external force such as
evolutionary pressure. It can be observed that the number of
surrogate models located by SurDG-Random becomes sparse
with the increase of SEQ and PAR operations. This is due to
the models getting larger and not being able to be loaded into
the GPU’s memory. An analysis of the diagonal heat cluster
located near the centre of the image conﬁrms the existence
of models that were initially rejected, modiﬁed and then re-
uploaded to the GPU. This also explains the high concentration
of phenotypes with SEQ and PAR operations between 0 and
10 operations as, again, any GPU rejected model is restricted
to smaller tree depths and reloaded to the GPU.

The ﬁttest phenotype located by SurDG-Random, is high-
lighted with a lime green square and located in the ﬁrst
quadrant in Figure 5a. It is interesting that this model has
a large number of SEQ and PAR operations and thereby
possibly a large number of parameters. However, the number
of parameters is not only related to the number of SEQ and
PAR operations but also the order in which those operations
are executed. For example, a network constructed of 10 SEQ
operation executed and then 1 PAR operation executed, will
have less trainable parameters than a network constructed
from 1 PAR operation executed and then 10 SEQ operations
executed. This is due to the concatenation of channels from the
PAR operation which will increase the number of channels,

8

and the subsequent SEQ operations will propagate those
increased channel numbers down the network stack, increasing
the number of trainable parameters. It is noted that this located
surrogate phenotype has roughly 18 million parameters. The
SurDG-EC algorithm located a high concentration of phe-
notypes consisting of SEQ operations numbering between 0
and 10, and PAR operations numbering between 0 and 5.
This ﬁnding indicates that SEQ operations played a dominant
role during the evolutionary process. The ﬁttest model
is
highlighted in lime green and located in the third quadrant
near the blue centre line of Figure 5b. It is of interest that both
the ﬁttest phenotypes found by SurDG-Random and SurDG-
EC algorithm are located around the centre line. 32 SEQ and
33 PAR operations were executed to produce the phenotype
found by SurDG-Random. 14 SEQ and 11 PAR operations
were executed to produce the phenotype found by the SurDG-
EC algorithm. This implies that each phenotype had an almost
equal ratio of PAR and SEQ operations applied to it. This
may be an indication that both PAR and SEQ operations are
important, alluding to the conclusion that width and depth may
potentially be an important combination for char-CNNs.

1) SurDG-Random: The ﬁttest genotype and corresponding
phenotype found by SurDG-Random is presented in Figure 6.
The phenotype has an almost diagonally-mirrored symmetry
to it.

Fig. 6: Fittest genotype/phenotype found by SurDG-Random.

There are a few interesting properties to note about the
genotype and phenotype. Firstly the genotype has little visual
representation to the phenotype, implying that it is difﬁcult to
predict the effect that a change in the genotype may have on
the phenotype. This could potentially be a limitation of the
chosen encoding scheme. If a small change in the genotype
results in a signiﬁcant structural change in the phenotype, the
ﬁtness landscape may not be smooth enough for evolutionary
computation techniques to perform any better than a random
search. There is another interesting property to note about this
phenotype. There are a few critical nodes present almost as if
the phenotype consists of a collection of smaller phenotypes
connected by these critical nodes. It is noted that the bottom
part of the network has a wide segment, followed by a narrow
and deep segment of the network. This same property is
present in the ﬁttest phenotype located by the SurDG-EC
algorithm, that will be discussed further on.

05101520253035SEQ05101520253035PAR0.00.20.40.60.81.005101520253035SEQ05101520253035PAR0.00.20.40.60.81.0INPUT33577735357735773737737OUTPUT7335777777337733737373333557333557735775777C. Analysis of SurDG-EC

The combined performance of the evolved surrogate models
over each generation for 30 runs is presented in Figure 7. It
can be observed that most surrogate phenotypes have attained a
validation accuracy above 80% even before the ﬁrst generation.
This indicates that the reduced cellular encoding scheme using
the chosen convolutional block design is performant. However,
it is also observable that there are still a few phenotypes
with low validation accuracy after the evolutionary process has
begun. The ﬁttest performing surrogate model was evolved

9

in the early stages. The width of the child phenotype is
effectively a summation of the parent’s width. There were no
mutation operations over the entire genealogy of the ﬁttest
phenotype. The lack of a contributing mutation operator raises
the question of how important mutation is in the evolutionary
process for this particular encoding and is left for future
research.

Fig. 9: Crossover operation that produced ﬁttest phenotype.

D. Results of Full Precision Model

The training and validation history of the full resolution
version of the ﬁttest evolved phenotype is presented in Figure
10.

Fig. 7: AG’s News: SurDG-EC performance over generations.

during the 27th generation, as can be seen in Figure 7. The
corresponding genotype and phenotype are presented in Figure
8. It is of interest to note that the model is both wide and deep
- similar to the ﬁttest phenotype found by SurDG-Random. It
would appear that the ﬁttest performing phenotype has built a
rich feature representation in the wide part of the network
and successfully extracted hierarchical relationships in the
deep part of the network. In Figure 8, the GP tree structure
(genotype) shows that two SEQ operations were executed ﬁrst,
creating an initial network of six convolutional layers.

Fig. 8: Fittest evolved genotype and phenotype.

It may be that in order for a potentially wide network to
survive the ﬁrst few generations, its genealogy may need to
start with models that are deep ﬁrst and then spread out wide
in later generations.

1) Genealogy Analysis: Analysis of the ﬁttest phenotype’s
genealogy shows that a crossover operation generated its
genotype. This crossover operation is presented in ﬁgure
9. The components involved in the crossover operation are
highlighted in blue and red. Note that both parents are deep
networks, and both are wide at the early stages of the topology.
The resulting phenotype is as deep as the parents but wider

Fig. 10: AG’s News: Training of high resolution phenotype.

The network converged before the seventh epoch, reﬂecting
the successful application of evolutionary pressure applied by
the SurDG-EC algorithm on its population to converge before
the tenth epoch. The validation accuracy declined after the
second epoch until the learning rate was halved at the third
epoch, after which the validation accuracy began improving
again. The validation accuracy continued improving between
epoch ﬁve and six but plateaued after the learning rate was
halved again. This adverse effect may indicate that the learning
rate was too low to escape a local optima, thereby stalling any
future accuracy improvements. The importance of dynamically
adjusting the learning rate during training is left for future
research.

E. Results on AG’s News test dataset

The test performance of the full resolution phenotype and
peer competitors is listed in Table V. The SurDG-EC algorithm

Init.123456789101112131415161718192021222324252627282930Generation number.505560657075808590Validation accuracy. (%)INPUT35573577773555735573573333OUTPUTINPUT3573577735735333OUTPUTINPUT357355573573333OUTPUTINPUT35573577773555735573573333OUTPUT0500010000150002000025000Iteration020406080100Validation Accuracy (%)Learning RateTraining Accuracy (Smoothed)Validation Accuracy012345678910111213141516171819202122232425262728293031EpochMaximum Validation Accuracy: 90.851%0.0000.0020.0040.0060.0080.010Learning Rateevolved a surrogate phenotype that when trained as a full
resolution phenotype, outperformed six of the competitors,
including all word-level models. The evolved phenotype com-
pared favourably with the current state-of-the-art VDCNN-29-
Kmax model. The ﬁttest evolved phenotype contained roughly
half the number of parameters found in the VDCNN-29-
Kmax model. It should be noted that the VDCNN-29-Kmax
parameter count includes the trainable parameters of its fully
connected layers. Interestingly, SurDG-Random located a phe-
notype that outperformed four expert-designed peer competitor
models with a comparable number of parameters.

Model or Algorithm
word-LSTM (w2v)
Large word-CNN (w2v)
Small word-CNN (w2v)
Large Char-CNN [4]
Small Char-CNN [4]
VDCNN-29-Kmax [5]
SurDG-Random (32-bit phenotype)
SurDG-EC (32-bit phenotype)

Test Accuracy (%)
86.06
90.08
88.65
87.18
84.35
91.27
89.11
90.72

Params. (Millions)
-
-
-
˜15
˜11
˜17
˜18
˜9

TABLE V: AG’s News: Testing accuracy.

1) Architecture generalisation ability: To test how well
the ﬁttest surrogate phenotypes generalised across other text
classiﬁcation domains, they were retrained as full resolution
phenotypes and trained and evaluated across each of the
remaining unseen datasets.

(a) SurDG-Random

(b) Zhang Small

(c) Zhang Large

(d) VDCNN-29-Kmax

Fig. 11: SurDG-EC’s relative performance.

The relative accuracies are presented in Figure 11. It can
be seen that the SurDG-EC evolved phenotype outperformed
the SurDG-Random located phenotype on all the datasets. It is
noted that the SurDG-EC algorithm performed better by only
the slightest of margins on the Yahoo Answers dataset.

10

The SurDG-EC phenotype outperformed the remaining
competitors on the Sogou dataset and all peer competi-
tors on the DP Pedia dataset. Comparable results were at-
tained on the Yahoo Answers dataset, slightly less so against
VDCNN-29-Kmax. The SurDG-EC phenotype signiﬁcantly
under-performed the remaining competitors on the Yelp and
Amazon datasets. It should be remembered that the SurDG-EC
phenotype was evolved using only 25% of the smallest dataset.
It is impressive that the evolved phenotype could compete
favourably on at least three of the unseen datasets. DB Pedia
and AG’s News are both datasets containing curated articles,
and this may explain the ability of the phenotype to generalise
so well across the DB Pedia dataset. Sogou news contains
romanised Chinese text (pinyin) which is different from the
text contained in AG’s News. The SurDG-EC phenotype
was still able to generalise enough to be performant on the
Sogou dataset. This implies that the SurDG-EC algorithm
has generalised reasonably well over some unseen datasets.
The Yahoo Answers dataset is large, and this appears not to
have hindered the performance of SurDG-EC phenotypes. The
SurDG-EC phenotype has not generalised well over the Yelp
dataset.

F. Further analysis

Further analysis was conducted across all distinct surrogate
phenotypes evaluated during both SurDG-Random and the
evolutionary process. Visualisations of the analysis conducted
are presented in Figure 12. Each visualisation represents
14,848 distinct surrogate phenotypes. Four metrics are pro-
posed to aid in determining what properties of a networks
architecture contribute to improved or reduced classiﬁcation
accuracy. The proposed metrics are:

1) Cell-to-depth ratio is a measure between the number of
cells in a phenotype divided by the maximum path length
of cells from the input layer to the output layer. A cell-
to-depth ratio of 1 implies that all cells are located on
the same path. This means the phenotype would have a
width of 1. A value approaching zero would imply most
cells are on their own path, thus implying the network is
wide with a depth of 1. A value between 0 and 1 would
imply a network with some combination of width and
depth.

2) Path density is a complexity measure of how many
paths an input vector would traverse before arriving
at the output. The more paths, the more complex the
phenotype is.

3) Trainable Parameters count is a complexity measure
that is simply the number of trainable parameters in
the phenotype. A higher value implies a more complex
phenotype.

4) Depth is a measure that reﬂects the longest path of
sequential cells in a phenotype. The larger the value,
the deeper the network.

0.0%2.5%5.0%7.5%10.0%12.5%15.0%17.5%20.0%-60.0%-40.0%-20.0%0.0%20.0%40.0%-80.0%-60.0%-40.0%-20.0%0.0%20.0%-120.0%-100.0%-80.0%-60.0%-40.0%-20.0%0.0%AG's NewsYelp FullYelp Pol.Amaz. FullAmaz. Pol.SogouDB PediaYahoo Ans.11

evidence that as the depth of phenotypes increase, so does
the validation accuracy. After a depth of approximately ten
cells, the validation accuracy degrades. It should be mentioned
that ten cells represent twenty convolutional layers as deﬁned
in [5]. The VDCNN-29-Kmax model is 29 layers deep and
consists of the same convolutional blocks that constitute a
cell in a SurDG-EC evolved phenotype. VDCNN-29-Kmax,
however, has skip links in its architecture which enables the
model to be deeper than the phenotypes evolved by SurDG.
The conclusion drawn is that depth does improve the accuracy
of char-CNN up to a certain depth as evidenced in [5] and this
work.

G. Phenotype Analysis

(a) Cell-to-depth ratio vs accuracy

(b) Path density vs accuracy

(c) Accuracy vs parameter count

(d) Depth vs accuracy

Fig. 12: Surrogate phenotypes analysis.

Figure 12a presents the cell-to-depth ratio of each surrogate
phenotype against their corresponding validation accuracies.
The most accurate surrogate phenotypes are located in the
centre of the ﬁgure, implying that phenotypes with a similar
measure of width and depth are the most performant. Although
the highest accuracies were attained in this region, there are
also many phenotypes with lower accuracies located here too.
It would appear that although depth may be important, the
situation is more complicated than just merely having a wide
network. Deep and narrow networks also attained high accu-
racies, with no accuracies observed below 80%. These narrow
and deep networks seem to be more performant in general,
but unable to achieve as high accuracy as simultaneously wide
and deep networks. This lack of high accuracy may also be
due to the deep networks getting too deep and not having
skip links to improve the training performance. Implementing
skip links is left for future research. Figure 12b displays the
path density measure of each surrogate phenotype against its
corresponding validation accuracy. It is observable that most
of the accurate phenotypes have a lower path density measure.
This observation implies that phenotypes with too much com-
plex branching and paths may be less performant in general.
Most performant phenotypes have less than 25 distinct paths
between the input and output. The parameter count measure
is presented in Figure 12c. It would appear that an increase
in the trainable parameter count results in an increase in
validation accuracy only up to a certain number of parameters.
Beyond this amount, the validation accuracy begins to degrade.
This observation is consistent with the ﬁndings in [5]. The
depth measure is presented in Figure 12d. There is clear

Fig. 13: Cellular cell’s activations for inference on a single
sentence.

The feature maps of a cellular cell contained in the ﬁttest full
resolution phenotype is presented in Figure 13. A single sen-
tence was sent through the phenotype to capture the activations
during inference. The right-hand side of the ﬁgure contains
a view of the signal produced in channel 60. The channel
was selected arbitrarily. There has been little to no research
in visualising what takes place within the activations in char-
CNNs during inference. It can be observed that as the signal is
convolved over, prominent peaks start appearing - representing
neurons being excited at that temporal position. There are
three prominent spikes after the ﬁnal ReLU application. It
is interesting to note that other channels display many more
neurons activating. The value of a channel that has many
excited neurons is questionable. Considering the workings of
biological evolution, it would make sense that nature would
prefer a more efﬁcient encoding of knowledge using a spare
representation, meaning less energy consumed. This would
imply that a sparse reaction to a stimulus would be preferred.
This raises the interesting question of which of the above
channels could be pruned and is left for future research.

VI. CONCLUSION
This work proposed an evolutionary deep learning ap-
proach to discover performant char-CNN architectures. This
goal was achieved through the implementation of a genetic
programming-based algorithm (SurDG) coupled with a re-
duced cellular encoding scheme and the backpropogation al-
gorithm. The SurDG-EC algorithm located, on average, higher

0.20.40.60.81.0Nodes-to-Depth Ratio30405060708090Validation Accuracy (%)Shallow-Wide              Deep-Narrow0255075100125150175Number of Paths in Phenotype30405060708090Validation Accuracy (%)105106107Number of parameters. (Log)2030405060708090Validation accuracy. (%)0510152025Depth30405060708090Validation Accuracy (%)0100200Temporal Length050Channelconv10100200Temporal Length050Channelrelu10100200Temporal Length050Channelconv20100200Temporal Length050Channelrelu2050100150200250Temporal Length05Strengthconv1050100150200250Temporal Length05Strengthrelu1050100150200250Temporal Length05Strengthconv2050100150200250Temporal Length05Strengthrelu212

[18] B. Wang, Y. Sun, B. Xue, and M. Zhang, “Evolving deep convolutional
neural networks by variable-length particle swarm optimization for im-
age classiﬁcation,” in 2018 IEEE Congress on Evolutionary Computation
(CEC), pp. 1–8, 2018.

[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Distributed Represen-

tations of Words and Phrases and their Compositionality,” tech. rep.

[20] K. Hara, D. Saito, and H. Shouno, “Analysis of function of rectiﬁed
linear unit used in deep learning,” in 2015 International Joint Conference
on Neural Networks (IJCNN), pp. 1–8, 2015.

[21] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning Repre-
sentations by Back-Propagating Errors, p. 696–699. Cambridge, MA,
USA: MIT Press, 1988.

[22] C. De Sa, M. Feldman, C. R´e, and K. Olukotun, “Understanding and
optimizing asynchronous low-precision stochastic gradient descent,” in
2017 ACM/IEEE 44th Annual International Symposium on Computer
Architecture (ISCA), pp. 561–574, 2017.

[23] S. Ioffe and C. Szegedy, “Batch Normalization: Batch normalization

original paper,” tech. rep., 2015.

[24] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for

Image Recognition,” CoRR, vol. abs/1512.0, 2015.

[25] H. T. Le, C. Cerisara, and A. Denis, “Do Convolutional Networks Need

to Be Deep for Text Classiﬁcation?,” tech. rep.

[26] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely

Connected Convolutional Networks,” aug 2016.

[27] R. Johnson and T. Zhang, “Deep Pyramid Convolutional Neural Net-

works for Text Categorization,” pp. 562–570.

[28] J. Liang, E. Meyerson, B. Hodjat, D. Fink, K. Mutch, and R. Miikku-

lainen, “Evolutionary Neural AutoML for Deep Learning,” feb 2019.

[29] R. Miikkulainen, J. Liang, E. Meyerson, A. Rawal, D. Fink, O. Francon,
B. Raju, H. Shahrzad, A. Navruzyan, N. Duffy, and B. Hodjat, “Evolving
deep neural networks,” 2017.

[30] K. O. Stanley and R. Miikkulainen, “Evolving neural networks through
augmenting topologies,” Evolutionary Computation, vol. 10, no. 2,
pp. 99–127, 2002.

[31] P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Ding,
A. Bagul, C. Langlotz, K. Shpanskaya, M. P. Lungren, and A. Y. Ng,
“Chexnet: Radiologist-level pneumonia detection on chest x-rays with
deep learning,” 2017.

[32] F. Gruau, F. Gruau, L. C. B.-l. I, O. A. D. De Doctorat, M. J. Demongeot,
E. M. M. Cosnard, M. J. Mazoyer, M. P. Peretto, and M. D. Whitley,
“Neural Network Synthesis Using Cellular Encoding And The Genetic
Algorithm.,” 1994.

[33] M. Lin, Q. Chen, and S. Yan, “Network In Network,” tech. rep.
[34] J. R. Koza, Genetic Programming: On the Programming of Computers
by Means of Natural Selection. Cambridge, MA, USA: MIT Press, 1992.
[35] H. Kaiming, Z. Xiangyu, R. Shaoqing, and S. Jian, “Delving Deep
into Rectiﬁers: Surpassing Human-Level Performance on ImageNet
Classiﬁcation Kaiming,” Biochemical and Biophysical Research Com-
munications, vol. 498, no. 1, pp. 254–261, 2018.

accuracy models than those located by SurDG-Random. The
ﬁttest evolved phenotype defeated one of the state-of-the-
art char-CNN models[4] and achieved comparable results to
the state-of-the-art VDCNN-29[5] architecture. The evolved
model also generalised favourably across most unseen datasets.
There is clear evidence that width may potentially add to the
efﬁcacy of char-CNNs.This does not mean that width will
always result in increased accuracy, as also observed in the
results. There are many other factors to consider. It is not
known how much of the efﬁcacy of the evolved phenotypes
are due to increased width or some other unknown variable or
combination of variables. There are, however, clear indications
that the importance of width should be further researched. The
SurDG-EC algorithm also revealed two interesting properties
of char-CNNs. Building a rich tapestry of feature represen-
tations at the early stages of the network potentially aids in
improving the accuracy of the networks as they grow deeper -
in turn constructing a hierarchy of relations from this rich
feature tapestry. The evolutionary crossover operation also
revealed that combing the widths of two phenotypes produced
a wider phenotype with greater validation accuracy. This is
a further clue that there may be value in making char-CNNs
with increased width.

REFERENCES

[1] Y. Lecun, Y. Bengio, and G. Hinton, “Deep learning,” may 2015.
[2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and
P. Kuksa, “Natural Language Processing (almost) from Scratch,” mar
2011.

[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training

of deep bidirectional transformers for language understanding,” 2018.

[4] X. Zhang and Y. LeCun, “Text Understanding from Scratch,” 2015.
[5] A. Conneau, H. Schwenk, Y. Le Cun, and L. Loic Barrault, “Very Deep
Convolutional Networks for Text Classiﬁcation,” tech. rep., 2017.
[6] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le,
“XLNet: Generalized Autoregressive Pretraining for Language Under-
standing,” 2019.

[7] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep Contextualized Word Representations,” in
Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics, pp. 2227–2237, jun
2018.

[8] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

Comput., vol. 9, p. 1735–1780, Nov. 1997.

[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
in Neural Information Processing Systems 30 (I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
eds.), pp. 5998–6008, Curran Associates, Inc., 2017.

[10] Y. Lecun, L. Bottou, Y. Bengio, and P. Ha, “Gradient-Based Learning
Applied to Document Recognition 1 Introduction,” Most, pp. 1–75, 1998.
[11] X. Zhang, Y. Lecun, and D. Sontag, “PAC-Learning for Energy-based

Models,” tech. rep., 2013.

[12] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, “Character-Aware Neural

Language Models,” aug 2015.

[13] R. Johnson and T. Zhang, “Convolutional Neural Networks for Text
Categorization: Shallow Word-level vs. Deep Character-level,” aug 2016.
[14] Y. Kim, “Convolutional Neural Networks for Sentence Classiﬁcation,”

tech. rep.

[15] B. Xue, M. Zhang, W. N. Browne, and X. Yao, “A survey on evolutionary
computation approaches to feature selection,” IEEE Transactions on
Evolutionary Computation, vol. 20, no. 4, pp. 606–626, 2016.

[16] Y. Sun, B. Xue, and M. Zhang, “Evolving deep convolutional neural

networks for image classiﬁcation,” CoRR, vol. abs/1710.10741, 2017.

[17] Y. Sun, B. Xue, M. Zhang, and G. G. Yen, “A particle swarm
optimization-based ﬂexible convolutional autoencoder for image classi-
ﬁcation,” IEEE Transactions on Neural Networks and Learning Systems,
vol. 30, no. 8, pp. 2295–2309, 2019.

