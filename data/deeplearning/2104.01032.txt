Plot2API: Recommending Graphic API from Plot
via Semantic Parsing Guided Neural Network

Zeyu Wang1,2, Sheng Huang1,2âˆ—, Zhongxin Liu3, Meng Yan1,2âˆ—â€ , Xin Xia4, Bei Wang2, Dan Yang2
1Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University),
Ministry of Education, China
2School of Big Data and Software Engineering, Chongqing University, Chongqing, China
3College of Computer Science and Technology, Zhejiang University, Hangzhou, China
4Faculty of Information Technology, Monash University, Australia
Email:{zeyuwang, huangsheng, mengy, bwang2013, dyang}@cqu.edu.cn, liu zx@zju.edu.cn, xin.xia@monash.edu

1
2
0
2

r
p
A
2

]
E
S
.
s
c
[

1
v
2
3
0
1
0
.
4
0
1
2
:
v
i
X
r
a

API

Abstractâ€”Plot-based

Graphic
is an unstudied but meaningful

recommendation
(Plot2API)
issue, which
has several important applications in the context of software
engineering and data visualization, such as the plotting guidance
of the beginner, graphic API correlation analysis, and code
conversion for plotting. Plot2API is a very challenging task,
since each plot is often associated with multiple APIs and the
appearances of the graphics drawn by the same API can be
extremely varied due to the different settings of the parameters.
Additionally, the samples of different APIs also suffer from
extremely imbalanced.

Considering the lack of technologies in Plot2API, we present
a novel deep multi-task learning approach named Semantic
Parsing Guided Neural Network (SPGNN) which translates
the Plot2API issue as a multi-label
image classiï¬cation and
an image semantic parsing tasks for the solution. In SPGNN,
the recently advanced Convolutional Neural Network (CNN)
named Efï¬cientNet is employed as the backbone network for
API recommendation. Meanwhile, a semantic parsing module is
complemented to exploit the semantic relevant visual information
in feature learning and eliminate the appearance-relevant visual
information which may confuse the visual-information-based
API recommendation. Moreover, the recent data augmentation
technique named random erasing is also applied for alleviating
the imbalance of API categories.

We collect plots with the graphic APIs used to drawn them
from Stack Overï¬‚ow, and release three new Plot2API datasets
corresponding to the graphic APIs of R and Python programming
languages for evaluating the effectiveness of Plot2API techniques.
Extensive experimental results not only demonstrate the superi-
ority of our method over the recent deep learning baselines but
also show the practicability of our method in the recommendation
of graphic APIs.

Index Termsâ€”API Recommendation, Data Visualization, Im-

age Recognition

I. INTRODUCTION

Figures and plots are the indispensable tools for data visu-
alization which provide people with intuitive understanding
of data and interaction with data. In software engineering,
almost all the programming languages support such functions
and possess a series of relevant APIs as one of core libraries
or packages. It is very common for the software developer
particularly the beginner to search API on the web based on

âˆ—Corresponding authors.
â€ also with Pengcheng Laboratory, Shenzhen, China.

Fig. 1. The help post about Plot2API in Stack Overï¬‚ow from link:
https://stackoverï¬‚ow.com/questions/12786334/how-to-plot-in-r-like-this
a case ï¬gure for guiding the plot. Figure 1 shows a help post
where a developer asks how to draw a ï¬gure like the one posts
in Stack Overï¬‚ow. Whatâ€™s more, people might want to know
the APIs starting from a plot, such as imitating visualization
styles. In agile development, developers often sufï¬ciently
utilize the materials of previous projects for speeding up the
development, thereby they expect to convert the ï¬gures plotted
in one language into APIs of the other directly to reduce the
time cost. In these scenarios, a tool that can automatically
recommend graphic APIs based on a plot can provide guidance
for developers and improve their productivity. Therefore, how
to identify API based on Plot (Plot2API) is a meaningful task
in software engineering and data visualization.

Plot2API can be deemed as a plot-based API recommen-
dation task, since the set of APIs regards to a programming
language is ï¬xed and a plot is often drawn by multiple APIs.
API recommendation is not a new issue now in software
engineering and many researchers have worked in this direc-
tion [1], [2], [3], [4], [5], [6]. However, these existing works

 
 
 
 
 
 
erating more training data for each category. We release three
Plot2API datasets which are collected from Stack Overï¬‚ow
and are carefully preprocessed for evaluating our work. The
experimental results show that SPGNN consistently performs
better than Efï¬cientNet with a considerable improvement and
defeats all deep learning baselines on all datasets.

The main contributions of our work are summarized as

follows:

â€¢ A novel software engineering task named Plot2API is
introduced, which attempts to recommend the graphic APIs
based on the plots. Plot2API has many potential and mean-
ingful applications in software engineering.

â€¢ A novel deep learning method named Semantic Parsing
Guided Neural Network (SPGNN) for tackling the Plot2API
task is proposed. SPGNN translates this task into the multi-
label image classiï¬cation and the semantic parsing tasks for
the solution. The semantic parsing is expected to facilitate
Efï¬cientNet to extract deep features that are more robust to
appearance variation and thereby supports the plot-based API
recommendation.

â€¢ Three novel Plot2API datasets, namely Python-Plot13, R-

Plot32 and R-Plot14, are released for evaluation.

â€¢ An empirical comparison of classical CNN models on
Plot2API is conducted and extensive experimental results on
the released datasets demonstrate the superiority of our method
over the recent deep learning baselines and its signiï¬cant
improvement over Efï¬cientNet.

II. APPROACH

In this section, we ï¬rst introduce the Plot2API issue and
then elaborate on our proposed method named Semantic
Parsing Guided Neural Network (SPGNN).

A. Overview

Problem Formulation: In this paper, we formulate a new
problem in software engineering named Plot2API which stud-
ies how to recommend the graphic APIs from plots or ï¬gures.
According to the facts that each plot may be drawn by multiple
APIs and the set of graphic APIs regarding to a programming
language is ï¬xed, Plot2API can be deemed as a multi-label im-
age classiï¬cation task. Let X = {xi|i = 1, 2, . . . , n} âˆˆ RnÃ—d
be the collection of ï¬gures and Y = {yi|i = 1, 2, . . . , n} âˆˆ
RnÃ—c be the corresponding labels where xi is the i-th plot and
its label yi is a binary vector. n, d, and c are the number of
samples, the dimension, and the number of APIs respectively.
The Plot2API technique aims at learning a mapping function
F (Â·) to map the plots to the labels, i.e.,

F (Â·)
â†’ Y,

X

(1)
where yi = F (xi). In multi-label image classiï¬cation, such
mapping function is often further divided into two steps,
F (Â·) := PÏ‰(EÏ†(Â·)) where E(Â·) and P (Â·) are the feature
learning and API recommendation respectively. Ï† and Ï‰ are
their learnable parameters.

We consider Plot2API as a multi-label image classiï¬cation
issue for the solution. The recently advanced CNN model
is adopted as the backbone of the framework. However,

Fig. 2. Examples of data graphics with APIs.
are quite different to the Plot2API since they accomplished
the API recommendation tasks based on the source code or
textual descriptions. It is not convenient to ï¬rst convert a plot
into textural descriptions or code and then accomplish the task
in text to text manner, since the translation of the plot to
the code or the textual description leads to the unnecessary
time cost and the misinterpretation risk which may target the
question to the wrong answer. Instead, the plot-based API
recommendation provides an image to text solution which is
more intuitional, convenient, and efï¬cient. Nevertheless, to the
best of our knowledge, the Plot2API issue remains unstudied.
Although the Plot2API issue can be deemed as a common
multi-label image classiï¬cation, it is very challenging due
to the extremely varied appearances of the plots drawn by
the same API and the unnoticed visualization functions of
some subsidiary APIs. Moreover, the APIs also suffer from a
serious imbalance which is also a fatal limitation for Plot2API.
Figure 2 gives some of such examples in the R programming
language.

In the recent decade, the Convolutional Neural Networks
(CNN) have achieved a signiï¬cant advance in supervised
learning particularly in image classiï¬cation [7], [8], [9], [10],
[11]. They are proï¬cient in learning the discriminative features
for images. Here, we leverage a recently advanced CNN model
named Efï¬cientNet [7] as the backbone network to develop
a novel end-to-end trainable deep learning approach named
Semantic Parsing Guided Neural Network (SPGNN) for ï¬lling
the aforementioned missing technology. SPGNN introduces
an extra semantic parsing module to the Efï¬cientNet which
considers the Plot2API issue as a multi-task learning problem
for the solution. Besides the conventional Efï¬cientNet-based
plot classiï¬cation ï¬‚ow path, SPGNN extra employs a semantic
translation network to translate the visual features of a plot
learned from Efï¬cientNet into the semantic representations
of APIs and then uses a relation network to compare these
estimated semantics with their ground truth for accomplishing
the task from the perspective of semantic parsing. By fully
exploiting the semantics of APIs, the semantic parsing module
facilitates the Efï¬cientNet to better learn the semantic relevant
to the appearance
visual features which are more robust
variation caused by the different parameter settings of the same
API. In order to alleviate the sample distribution imbalance of
APIs, the random erasing trick is applied to the plots for gen-

polar()boxplot() line() path()segment() bar() step()polar()C. Semantic Translation

fi = EÏ†(xi),
(2)
where the visual feature fi is the pooling result of the last
convolutional layerâ€™s output.

empirical study in Section III also indicates that it is the
best performed CNN model for Plot2API. We use the feature
extraction network as the mapping function of our feature
learning module, which can be denoted as follows,

Fig. 3. The overview of our method. The input data graphics xi are sent to the feature learning network to extract the visual features fi, and then generating
the predicted API labels Ë†yi in API recommendation network and generating the semantic information Ë†vi in semantic translation network. The real semantic
information is produced by word2vec. After concatenating the semantic vectors, these features are sent to the semantic metric network to evaluate the relational
reasoning. The relation is stronger, the output of relation network ri is more approximate to 1. And then, the Ë†yi and ri are used to recommend APIs in API
recommendation network.
Plot2API is quite different from the original object-based
image classiï¬cation where the samples from the same cate-
gory often share similar visual appearances. The appearances
of the ï¬gures drawn by the same API often suffer from
the extreme variation since different parameter settings can
seriously perplex the plot-based API recommendation, as
shown in Figure 2. To overcome this challenge, we intend
to utilize the semantics of APIs to guide the feature learn-
ing and preserve the semantic relevant visual
information
which reï¬‚ects the semantic nature of appearances. Instead
of considering the issue as a single-task learning problem,
we present a novel deep learning method named Semantic
Parsing Guided Neural Network (SPGNN) and regard this
issue as a multi-task learning problem for the solution. The
merit of this fashion is that relevant tasks can beneï¬t from the
solution of each other due to the information complementary.
SPGNN contains two relevant tasks namely plot-based API
recommendation and plot-based semantic parsing. The plot-
based API recommendation is the main task while the plot-
based semantic parsing is extra introduced for extracting the
semantics of APIs from plots. More speciï¬cally, SPGNN
consists of feature learning, API recommendation, semantic
translation and semantic metric modules. The feature learning
and API recommendation modules compose the ï¬‚ow path of
plot-based API recommendation while the feature learning,
semantic translation and semantic metric modules compose the
ï¬‚ow path of plot-based semantic parsing, as shown in Figure 3.
The following subsections give the details of these modules.

The feature learning is the key to the success of the
supervised learning model. The single-task learning schema
is easy to fall into the overï¬tting due to the single view of
optimization. Here we integrate the semantic parsing module
with the aforementioned CNN-based API recommendation
ï¬‚ow path and convert such a single task learning issue into a
two-task learning issue. The semantic parsing module utilizes
a semantic translation network, which consists of one fully
connected layer followed by a ReLU layer, to translate the
visual feature learned by CNNs into the semantic representa-
tion of APIs. Here, we employ the Wikipedia dataset retrained
word2vec [12], [13] to attain the ground truth semantic rep-
resentation of each API, Vi = [vi1, Â· Â· Â· , vit, Â· Â· Â· , vic] where
vit is a 400-dimensional word embedding corresponding to
the t-th API and regarding to the i-th sample. The semantic
translation can be denoted as follows,
Ë†Vi = TÏˆ(fi),
(3)
where T (Â·) is the mapping function of the semantic translation
network with parameters Ïˆ, and Ë†Vi = [Ë†vi1, Â· Â· Â· , Ë†vic] is the
translated semantics of all APIs corresponding to sample xi.

B. Feature Learning

In the recent decade, CNN is deemed as the most inï¬‚uential
machine learning technique for visual feature learning. Here,
we also adopt CNN as the feature learning module. Here,
we choose a very recent CNN model named Efï¬cientNet-
B3 [7] as the feature learning network by considering the
trade off between the performance and the efï¬ciency. The

D. Semantic Metric

By applying the idea of learning to compare [14], we
establish a relation network for judging if the translated
semantics are identical to the ground truth,

si = RÏ‘(Vi, Ë†Vi),

(4)

EfficientNetâ… . Feature Learningâ…¡. Semantic Translationbarviolinboxplotcontourlineâ€¦Wordsğ¶Word2Vecâ€¦...GroundTruthWord Embeddingğ‘“ğ‘–ğ‘£ğ‘–1ğ‘£ğ‘–ğ‘ğ‘£ğ‘–2ğ‘£ğ‘–ğ‘âˆ’1ğ‘£ğ‘–3â…¢. Semantic Metricâ…£. API Recommendationğ¿ğ‘ ğ‘’ğ‘šğ¿ğ‘£ğ‘–ğ‘ à·œğ‘£ğ‘–à·œğ‘¦ğ‘–â€¦ğ‘£ğ‘–1ğ‘£ğ‘–ğ‘ğ‘£ğ‘–2ğ‘£ğ‘–ğ‘âˆ’1ğ‘£ğ‘–3...ğ‘Ÿğ‘–where si is a c-dimensional semantic relation vector whose j-
th element sj
i encodes the semantic relation score between Vi
and Ë†Vi. R(Â·) is the mapping function of the relation network
with parameters Ï‘ which consists of two fully connected
layers followed by ReLU layers. For supervising the semantic
translation network to extract the true semantics of APIs, the
semantic relation scores should be higher if the corresponding
APIs exist in the given ï¬gure, and vice versa. We employ
the sigmoid function Ïƒ(Â·) to normalized the semantic relation
scores, ri = Ïƒ(si), and consider the normalized ones as the oc-
currence probabilities of APIs in semantics. Then, the above-
mentioned target can be reached by measuring the distribution
difference between the normalized semantic relation scores
and the labels based on the cross-entropy again,

Lsem = âˆ’

N
(cid:88)

c
(cid:88)

i=1

j=1

i log(rj
yj

i ) + (1 âˆ’ yj

i ) log(1 âˆ’ rj

i ).

(5)

By optimizing this loss, the normalized semantic relation score
is expected to be 1 or 0 when the ï¬gure is not drawn by the
corresponding API. Finally, if we rank APIs according to the
relation scores, we can obtain a list of API semantics of a plot
and then accomplish the plot-based semantic parsing task.

E. API Recommendation

A one-layer fully connected neural network is leveraged
to map the extracted feature fi into a c-dimensional binary
label vector. The API recommendation module is denoted as
follows,

Ë†yi = PÏ‰(fi),

(6)

where P (Â·) is the mapping of the neural network with param-
eters Ï‰, and Ë†yi is a c-dimensional predicted label vector whose
elements are essentially the estimated occurrence probabilities
of the corresponding APIs. In the API recommendation task,
we expect to keep the predicted labels be consistent with the
ground truth, therefore we adopt the cross-entropy function
for measuring such label consistency and denote the label
recommendation loss as follows,

Lvis = âˆ’

N
(cid:88)

c
(cid:88)

i=1

j=1

yj
i log(Ë†yj

i ) + (1 âˆ’ yj

i ) log(1 âˆ’ Ë†yj

i ),

(7)

i and Ë†yj

where yj
i âˆˆ [0, 1] are the label and the predicted
occurrence probability of the j-th API for the i-th sample,
and N is the number of samples. Conventionally, for each
sample, the graphic APIs are sorted according to Ë†yi and then
output as the recommendation.

We formulate the model of SPGNN which tackles both the
plot-based API recommendation and the plot-based semantic
parsing tasks via integrating their losses in Equation 5 and 7,

Ë†F â† arg min
Ï†,Ïˆ,Ï‘,Ï‰

L := Lvis + Î± Ã— Lsem,

(8)

where Ë†F is the trained model and Î± is a manually tunable
positive hyper-parameter for reconciling the losses. After ad-
justing Î±, we can obtain the trained model.

F. Data Augmentation and API Recommendation

However, there is a problem that we cannot overlook, that
the Plot2API data are extremely imbalanced due to the usage
frequency of different APIs. Such imbalance can easily corrupt
the supervised learning model. Data augmentation is one of
the commonest means for alleviating such problem and also
a practical way for avoiding the overï¬tting. Here, we adopt
the random horizontal ï¬‚ips and the very recently proposed
data augmentation approach named random erasing [15] for
enriching the training data of each API. Please note that the
semantic parsing module is deemed as a booster, and after
the SPGNN model is trained, we only preserve the plot-based
API recommendation ï¬‚ow path for API recommendations.
Speciï¬cally, in testing phase, a plot or ï¬gure xt is input into
the feature learning module and then the extracted feature
is fed into the API recommendation module for getting its
estimated API occurrence probabilities,
Ë†yt = PÏ‰(EÏ†(xt)).

(9)
the recommended graphic APIs which are corre-
Finally,
sponding to the top k-highest occurrence probabilities are
recommended to this plot.

III. EXPERIMENTAL SETUP

In this section, we ï¬rst present the three datasets newly
released by us. Then, we introduce the evaluation metrics, the
implementation details and baselines.

A. Datasets

To construct datasets for this problem, we ï¬rst downloaded
the Stack Overï¬‚ow Data Dump of March 2018. Next, we
extracted the Python-related and R-related threads from the
data dump according to the tags of each thread. Each thread
contains a question post and zero or more answer posts.
We choose Python and R because they are two popular
programming languages and are frequently used for plotting.
We further processed the extracted threads, and only kept
their posts which are answer posts and contain both image
URLs and code. Then, we crawled the images in each answer
post from thousands of websites. Whatâ€™s more, the crawled
images and extracted code in each post are associated with
each other and manually veriï¬ed by us. The image-code pairs
of which the image and code are not matched, the image is not
a visualization plot and the code is not Python or R code were
removed by us. Finally, we classiï¬ed the dataset relying on the
APIs used in code. To avoid missing and incorrect labels, the
labels of each image are manually checked and adjusted by
us too.

1) Python-Plot13 Dataset1: We present a novel Python-
based Plot2API dataset named Python-Plot13 dataset. It con-
sists of 6350 python-related plot instances in total and involves
13 APIs, namely bar, barh, boxplot, broken barh, errorbar,
hist, pie, plot, polar, scatter, stackplot, stem and step. We
utilize 5080 samples for training and the rest of 1270 for
testing. The data distribution of the Python-Plot13 dataset is
shown in Figure 4. From the ï¬gure, it is not hard to ï¬nd that
the data are extremely imbalanced. For example, the API plot()

and linerange(), which are also removed in R-Plot14. As
a supplement dataset, R-Plot14 remove 502 samples(about
5.51%) from the original 9114 samples, and contains 8612
graphics where 6890 for training and 1722 for testing. The R-
Plot14 dataset involves 14 graphic APIs, namely bar, boxplot,
contour, density, hex, histogram, line, map, point, polygon,
raster, ribbon, smooth, and violin, which have the same images
with R-Plot32.

4) Data Split Protocol: We randomly select around 80%
of the data to produce the training set while the rest is used
as the testing set. In the data split, we ensure that the testing
set at least contains one instance for each API.

B. Evaluation Metrics

We employ Average Precision (AP) as the performance
metric for evaluating the recommendation performance for
each API. And the AP is essentially the area under the
Precision-Recall (P-R) curve which is a popular metric for
evaluating the binary classiï¬cation performances. The mean
Average Precision (mAP), known as the mean of APs over all
classes, is adopted as a comprehensive metric for evaluating
the API-recommendation performance of different methods.
The mAP is also known as the commonest metric for multi-
label image classiï¬cation.

C. Implementation Details

We here choose the Efï¬cientNet-B3 [7] as our backbone
for the trade off between performance and efï¬ciency. Like
other deep learning baselines, our backbone network is also
pre-trained on ImageNet [16]. The feature learning module
is built from successive MBConv [17], [18] and convolution
layers. After these layers, there is a global average pooling
layer. Before being fed into the network, the data graphics
will be resized to 300 Ã— 300. And the dimension of the
learned visual feature is 1536. Please refer to the original
paper [7] for the detailed architecture of Efï¬cientNet-B3. We
adopt word2vec [13] trained on the Wikipedia dataset [12]
to generate the 400-dimensional semantic representations of
APIs (word embeddings) for all datasets. Note, the word2vec
is retrained, since there is a word (â€œhistogramâ€) not included
in the Wikipedia dataset. With regard to the case that an API
contains multiple words, we average the embeddings of the
words as the APIâ€™s semantic representation. The semantic
translation network and API recommendation network all
consist of just one fully connected layer while the relation
network is a neural network with two fully connected layers,
whose hidden layer is 256.

We train the proposed model using an Adam optimizer [19]
with the batch size of 32 and momentum of 0.99. ReLU is used
as the activation function in all the fully connected layers. The
network is trained for 100 epochs in total. We implement the
network based on PyTorch.

1The datasets and the source code are publicly available at

https://github.com/cqu-isse/Plot2API.

Fig. 4. The data distribution of the Python-Plot13 dataset.

TABLE I
THE DATA DISTRIBUTION OF THE R-PLOT32 DATASET.

API
bar
map
rug
hex
step
sf
path
point

#
2111
90
20
20
38
16
232
3665

API
bin2d
jitter
smooth
curve
line
spoke
violin
raster

#
12
105
395
8
2312
4
46
73

API
density
boxplot
segment
dotplot
freqpoly
crossbar
polygon
ribbon

#
205
638
385
40
10
14
303
223

API
density 2d
quantile
contour
errorbar
errorbarh
linerange
pointrange
histogram

#
4
4
21
335
39
49
49
387

possesses more than 4000 instances while broken barh() only
has 10 instances. Clearly, such imbalance makes the Plot2API
very challenging.

2) R-Plot32 Dataset1: The R programming language is
regarded as an inï¬‚uential statistical computing language that
owns fruitful graphic APIs. Hence, we also propose a new R-
based Plot2API dataset named R-Plot32 dataset. The R-Plot32
dataset contains 9114 images where 7292 for training and 1822
for testing. The R-Plot32 involves 32 graphic APIs, namely
bar, bin2d, boxplot, contour, crossbar, curve, density, den-
sity 2d, dotplot, errorbar, errorbarh, freqpoly, hex, histogram,
jitter, line, linerange, map, path, point, pointrange, polygon,
quantile, raster, ribbon, rug, segment, sf, smooth, spoke, step
and violin. The number of samples for each API is tabulated
in Table I. Similar to the Python-Plot13 dataset, this dataset
also suffers from the extreme imbalance of data. Moreover, it
is larger and possesses more categories which makes it more
challenging than the Python-Plot13 dataset.

3) R-Plot14 Dataset1: As we can see in Table I, some API
functions are used by few images. For example, density 2d(),
spoke() and quantile() classes only have four images in R-
Plot32. These functions are used to draw 2D density, direc-
tional data points and percentile ratio of total respectively.
Besides them, there are some APIs which are rarely used, such
as rug(), step(), sf(), curve(), dotplot(), freqpoly(), bin2d(),
crossbar() and so on. Hence, we removed these classes from
R-Plot32 dataset and construct a reduced version of R-Plot32
named R-Plot14. In addition, there are some APIs belonging
to the same super class, such as bar(), errorbar(), errorbarh()
and segment(), point(), jitter() and pointrange(), line(), path()

41581140112640218010592794231291710plotscatterbarhistboxplotbarherrorbarpiestepstempolarstackplotbroken_barh050010001500200025003000350040004500TABLE II
THE PERFORMANCE COMPARISON ON ALL DATASETS.

Datasets

mAP

VGG-16
VGG-16 + DA
ResNet-50
ResNet-50 + DA
Inception-v1
Inception-v1 + DA
Efï¬cientNet-B3
Efï¬cientNet-B3 + DA
SPGNN
SPGNN + DA

D. Baselines

Python-Plot13 R-Plot32 R-Plot14

67.46
64.10
56.33
55.95
52.92
54.93
68.51
69.33
71.16
75.95

38.39
40.84
29.64
29.81
26.06
32.59
44.61
44.46
45.63
47.76

66.08
67.96
55.81
56.53
51.84
53.41
70.75
71.29
71.84
75.13

VGG-16 [9], ResNet-50 [20],

Inception-V1 [8], and
Efï¬cientNet-B3 [7] are deemed as representative deep learning
approaches for image classiï¬cation and are regarded as the
baseline methods. The main contribution of VGGNet is the
increased depth with very small convolution ï¬lters [9]. ResNet
utilized a residual network, which is easy to optimize, to im-
prove the accuracy from considerably increased depth [20]. To
improve the utilization of the computing resources, Inception
was proposed as a sparse structure by readily available dense
building blocks to improve neural networks for computer
vision [8]. Efï¬cientNet balanced network depth, width, and
resolution to lead a better performance than other CNNs [7].

IV. EXPERIMENTAL RESULTS

In this section, we conduct experiments to evaluate our
proposed model on three datasets. Then, we carry out ablation
studies to evaluate the effectiveness of the proposed module
in SPGNN. The goal of experimental results shown in this
section is to answer the following questions:

â€¢ RQ1: How effective is SPGNN for API recommendation?
â€¢ RQ2: How well do our SPGNN model perform after
combining the semantic parsing module and the random
erasing-based data augmentation?

â€¢ RQ3: How well do our SPGNN model perform when
training and testing across different programming languages?

A. RQ1: How effective is SPGNN for API recommendation?

We compare SPGNN with four well-known image classi-
ï¬cation approaches, including VGG-16 [9], ResNet-50 [20],
Inception-V1 [8], and Efï¬cientNet-B3 [7] on our datasets.
Table II tabulates the mAP of different methods on different
datasets. Tables III, IV and V report the AP of each API on
Python-Plot13, R-Plot32 and R-Plot14 datasets respectively.
Clearly, Efï¬cientNet-B3 signiï¬cantly outperforms VGG-16,
ResNet-50 and Inception-V1 on all datasets. Therefore, we
choose the Efï¬cientNet-B3 as our backbone. From obser-
vations,
is not hard to ï¬nd that our proposed model
consistently performs much better than state-of-the-art CNN
approaches and achieves considerable mAP improvement over
Efï¬cientNet-B3 which is our baseline on all datasets. Here, we
will present the detail experimental analysis individually.

it

dataset:

1) Results

on Python-Plot13

SPGNN and
SPGNN+DA respectively achieve 71.76% and 75.95% mAP
and perform the best in comparison with all baselines. The
performance gains of SPGNN+DA over VGG-16, ResNet-50,
Inception-V1 and Efï¬cientNet-B3 in mAP are 8.49%, 19.62%,
23.03% and 7.44% respectively. After introducing the same
data augmentation to these four baselines, our method still
demonstrates the signiï¬cant advantages over these methods
and the performances gains are 11.85%, 20.00%, 21.02% and
6.62% respectively. Moreover, it also worthwhile to point out
DA is not always work for all CNNs. For examples, VGG-16
and ResNet-50 with DA are performs much worse than their
original versions on Python-Plot14 dataset.

According to Table III, our model also gets the ï¬rst on
the API recommendations of barh(), broken barh(), errorbar(),
pie(), plot(), stackplot() and step() APIs among all 13 APIs.
Particularly, our model gets 100% AP in broken barh() predic-
tion where such numbers of VGG-16, ResNet-50, Inception-
V1 and Efï¬cientNet-B3 are only 50.29%, 0.58%, 4.61% and
1.72% respectively. In bar(), boxplot(), scatter() and stem(),
the performance of our model is very close to the ï¬rst one.
More than half of APIs get over 80% AP via our model.
This implies that SPGNN possesses the good potential for
Python graphic API recommendation in reality. Moreover,
the experimental results demonstrate that the random erasing-
based data augmentation improves SPGNN by the mAP of
4.79% and makes SPGNN become more balanced cross all
APIs. We attribute these to the fact that the random erasing-
based data augmentation enriches the appearances of plots and
mitigates the overï¬tting of the proposed model.

All the methods do not perform well on the API recom-
mendation of barh(), errorbar(), polar(), stem() and step().
The reason behind this phenomenon we believe is that the
appearances of the plots drawn by barh() and errorbar() are
extremely similar, since barh() and errorbar() are both variants
of bar(), while the ï¬gures drawn by polar() have the similar
appearance with pie(), which both contain the circle element.
The graphics plotted by step() share the similar feature with
bar() and the plots drawn by stem() have the visual features
of point() and line(). Whatâ€™s more, the number of step() and
stem() is only 42 and 31 in the dataset, which limits the
learning power of CNNs to a certain extent. Even so, by
incorporating the semantic information of APIs, SPGNN still
signiï¬cantly improves the performance of the recommendation
of these APIs.

2) Results on R-Plot32 dataset: The R-Plot32 dataset is a
more challenging dataset with more samples and more APIs.
Our method still performs the best. The gains of SPGNN over
VGG-16, ResNet-50, Inception-V1 and Efï¬cientNet-B3 in
mAP are 7.24%, 15.99%, 19.57% and 1.02% respectively and
such numbers of SPGNN+DA are 9.37%, 18.12%, 21.70% and
3.15%. The improvements of SPGNN+DA over baselines+DA
are 6.92%, 17.95%, 15.17% and 3.30%. Moreover, our method
also achieves the ï¬rst rank 18 times among 32 APIs.

According to the results, many similar phenomena on the
Python-Plot13 dataset are also observed on the R-Plot32

TABLE III
THE PERFORMANCE COMPARISON ON THE PYTHON-PLOT13 DATASET (THE AP FOR EACH CATEGORY WHILE THE MAP FOR ALL, THE BOLD NUMBER
INDICATES THE BEST PERFORMANCE AND DA = RANDOM ERASING-BASED DATA AUGMENTATION ).

Methods
VGG-16
VGG-16 + DA
ResNet-50
ResNet-50 + DA
Inception-V1
Inception-V1 + DA
Efï¬cientNet-B3
Efï¬cientNet-B3 + DA
SPGNN
SPGNN + DA

mAP
67.46
64.10
56.33
55.95
52.92
54.93
68.51
69.33
71.16
75.95

bar
85.02
85.72
79.04
80.29
82.91
83.64
87.67
88.53
86.57
86.15

barh
45.33
46.57
44.33
47.59
42.77
42.66
53.36
49.02
54.68
56.76

boxplot
95.11
96.10
84.56
89.49
90.41
88.49
97.82
97.91
95.85
96.72

broken barh
50.29
6.87
0.58
1.30
4.61
1.41
1.72
75.00
4.32
100.00

errorbar
55.29
56.44
24.13
24.72
17.59
17.46
58.66
55.88
71.98
55.71

hist
71.62
71.45
55.63
54.17
56.17
62.87
78.47
65.52
73.50
77.50

pie
91.16
91.69
98.66
99.36
100.00
96.10
100.00
100.00
100.00
93.41

plot
92.97
93.61
92.66
92.89
90.47
91.67
93.53
92.87
94.00
94.08

polar
66.11
73.65
70.41
70.33
55.68
67.42
68.00
82.23
79.29
62.93

scatter
80.39
77.66
74.76
76.01
74.79
75.01
77.82
79.13
79.12
80.37

stackplot
55.80
66.79
38.36
40.32
50.11
47.98
74.36
47.39
75.76
80.95

stem
66.90
25.33
34.56
35.22
6.56
21.06
66.75
33.63
66.85
66.81

step
20.98
41.50
34.66
15.63
15.91
18.34
32.48
34.19
43.15
35.97

TABLE IV
THE PERFORMANCE COMPARISON ON THE R-PLOT32 DATASET (THE AP FOR EACH CATEGORY WHILE THE MAP FOR ALL, THE BOLD NUMBER
INDICATES THE BEST PERFORMANCE AND DA = RANDOM ERASING-BASED DATA AUGMENTATION).

Methods
VGG-16
VGG-16 + DA
ResNet-50
ResNet-50 + DA
Inception-V1
Inception-V1 + DA
Efï¬cientNet-B3
Efï¬cientNet-B3 + DA
SPGNN
SPGNN + DA
Methods
VGG-16
VGG-16 + DA
ResNet-50
ResNet-50 + DA
Inception-V1
Inception-V1 + DA
Efï¬cientNet-B3
Efï¬cientNet-B3 + DA
SPGNN
SPGNN +DA

mAP
38.39
40.84
29.64
29.81
26.06
32.59
44.61
44.46
45.63
47.76
map
34.55
37.60
48.33
37.82
31.77
44.70
49.80
48.90
40.61
60.50

bar
92.09
93.96
92.31
92.05
90.80
92.96
94.82
95.38
92.55
95.96
path
12.96
20.70
18.81
19.02
9.05
8.66
29.90
26.66
22.18
23.40

bin2d
7.22
8.22
0.80
1.71
1.24
4.26
29.08
31.94
2.00
4.01
point
94.68
93.68
87.94
88.28
89.83
89.92
94.54
95.46
94.84
95.13

boxplot
92.14
91.43
83.25
83.37
86.61
89.36
92.55
92.86
94.10
91.74
pointrange
31.05
61.31
19.59
22.37
32.07
43.96
41.42
48.76
79.04
64.05

contour
31.48
12.92
14.94
12.84
0.79
1.91
6.39
12.40
10.67
8.83
polygon
44.09
43.70
43.14
44.47
42.32
43.29
47.68
56.90
42.72
58.65

crossbar
0.69
3.42
5.79
4.01
0.42
17.04
1.85
0.36
2.63
1.32
quantile
0.35
0.20
0.32
0.36
0.25
0.22
0.08
0.34
0.17
0.20

curve
33.78
0.46
5.12
4.46
0.84
0.33
13.61
17.23
34.19
34.49
raster
41.97
45.63
47.27
42.76
8.52
38.93
60.25
50.93
49.02
48.88

density
79.05
82.17
69.49
71.83
56.00
69.66
87.66
92.00
85.69
88.69
ribbon
40.16
42.03
27.41
28.13
41.08
39.22
64.86
59.81
63.89
65.57

density 2d
100.00
0.28
0.44
0.69
2.63
100.00
100.00
3.33
100.00
100.00
rug
18.09
19.93
9.09
8.28
0.50
3.69
31.41
29.15
27.03
44.15

dotplot
17.89
30.59
11.57
12.75
1.58
1.73
50.07
32.44
51.66
47.61
segment
15.88
20.43
9.18
9.16
11.46
15.44
26.77
26.81
26.92
23.96

errorbar
65.73
75.26
39.05
40.66
35.48
51.69
71.56
71.14
77.56
75.36
sf
10.07
10.16
13.66
8.40
3.83
0.53
2.86
1.17
0.80
7.06

errorbarh
46.44
56.52
41.09
53.58
15.79
26.44
49.74
54.44
47.02
41.49
smooth
42.14
49.06
39.53
39.42
37.11
37.28
50.70
50.76
52.29
50.97

freqpoly
0.72
0.32
0.79
0.66
0.41
0.16
0.72
5.15
2.74
0.31
spoke
0.31
0.39
0.31
0.28
0.11
0.21
6.82
70.00
0.46
0.94

hex
42.20
43.49
41.16
26.10
67.33
34.40
47.89
35.37
64.65
68.55
step
34.42
35.66
28.92
22.15
20.61
37.85
26.90
43.18
31.40
38.11

histogram
50.88
63.33
41.35
44.13
46.21
50.89
69.99
67.10
63.32
69.57
violin
26.97
31.24
11.28
11.08
7.98
7.41
40.36
51.47
57.21
60.08

jitter
10.24
15.25
13.63
12.72
6.01
7.58
19.68
34.03
13.26
26.71
linerange
26.45
16.94
3.29
2.51
5.00
1.85
30.02
30.17
41.71
45.73

line
83.80
87.31
79.73
79.91
80.38
81.21
87.54
87.02
87.95
86.43
-
-
-
-
-
-
-
-
-
-
-

TABLE V
THE PERFORMANCE COMPARISON ON THE R-PLOT14 DATASET(THE AP FOR EACH CATEGORY WHILE THE MAP FOR ALL, THE BOLD NUMBER
INDICATES THE BEST PERFORMANCE AND DA = RANDOM ERASING-BASED DATA AUGMENTATION).

Methods
VGG-16
VGG-16 + DA
ResNet-50
ResNet-50 + DA
Inception-V1
Inception-V1 + DA
Efï¬cientNet-B3
Efï¬cientNet-B3 + DA
SPGNN
SPGNN + DA

mAP
66.08
67.96
55.81
56.53
51.84
53.41
70.75
71.29
71.84
75.13

bar
95.09
93.93
91.57
91.49
93.54
93.28
96.23
93.81
95.83
95.04

boxplot
93.10
94.42
88.32
89.33
86.20
90.14
97.28
97.78
97.03
96.61

contour
33.26
23.82
20.97
22.93
20.01
32.89
23.29
25.37
29.36
39.87

density
81.24
78.75
71.17
71.35
57.59
54.12
84.52
83.30
87.61
84.20

hex
59.19
53.28
38.48
41.76
32.64
18.29
76.86
39.49
63.58
75.09

histogram
59.10
70.69
51.85
50.79
57.72
56.70
74.93
81.64
79.39
80.55

line
88.98
88.03
83.77
83.48
79.96
83.89
90.17
91.85
90.27
90.42

map
43.24
45.15
47.41
46.66
30.34
32.11
54.83
53.99
52.91
51.37

point
95.02
94.61
90.66
90.54
92.24
93.20
95.56
95.98
95.52
95.81

polygon
52.35
48.39
45.23
45.29
30.88
39.08
56.00
55.64
59.66
54.96

raster
60.62
69.73
34.48
37.50
25.96
28.60
62.84
70.79
63.08
75.79

ribbon
36.28
43.02
25.57
26.74
29.25
35.01
42.31
51.81
46.68
55.05

smooth
52.16
58.57
50.59
50.71
45.09
51.24
60.35
61.88
67.56
69.41

violin
75.46
89.08
41.32
42.79
44.29
39.19
75.37
94.76
77.24
87.72

dataset. Here, we will not give the same conclusions intro-
duced in the previous section and only focus on analyzing
the phenomena speciï¬c to the R-Plot32 dataset. The most
obvious phenomenon is that almost all methods fail on the
recommendation of some APIs, such as bin2d(), contour(),
crossbar(), freqpoly(), qunatitle(), sf() and spoke(). We believe
the reason behind this is the small training size of these APIs
limits the learning power of CNN. For example, sf(), spoke()
and qunatitle() only have 4 samples in total. Additionally,
the ï¬gures or shape appearances drawn by the subsidiary
APIs, such as contour() and spoke(), often highly relate to the
appearances of the main objects in the plot or only cover a
tiny fraction of the ï¬gure which is hard to be visually noticed.
It is also difï¬cult to distinguish the ï¬gures drawn by APIs like
bin2d() and crossbar(), since some other APIs can draw very
similar ï¬gures. For example, the ï¬gure drawn by bin2d() can
be easily identiï¬ed as a rectangle, and there are many graphic
APIs in R, such as bar() and line(), can draw the rectangle-like
shapes.

Although our method performs fairly well on some fre-
quently used APIs, such as line(), point() and bar(), there
exists a large gap between the Python-Plot13 and the R-Plot32
datasets in terms of the overall performance measured by mAP.
The main reason of such low mAP we believe is the lack of
sufï¬cient training data for some APIs. Hence, we have also
conducted several experiments on a reduced version of the
R-Plot32 dataset, namely R-Plot14, for validating the effects
of our methods in the case that each API contains enough
training data.

3) Results on R-Plot14 dataset: The results on R-Plot14
are shown in Table V. We ï¬nd in surprise that all methodsâ€™
performance is signiï¬cantly boosted on the R-Plot14 dataset,
which removed the similar APIs and the classes with few
graphics. Since the classes have obvious distinguishing fea-
tures, our model demonstrates a better performance (+27.37%)
on R-Plot14 compared with R-Plot32, which is very similar
to that of the Python-Plot13 dataset. This phenomenon reï¬‚ects
the application possibility of our method on R programming

can beneï¬t the recommendation of API which owns limited
samples.

4) Some Successful Plot2API Examples of SPGNN: Fig-
ure 5 shows two cases that our method obtains a better API
recommendation over Efï¬cientNet on Python and R plots.
In Figure 5(a), SPGNN gets the right python API label as
the ï¬rst recommendation with the conï¬dence of 0.97 while
Efï¬cientNet fails. Figure 5(b) indicates that SPGNN ï¬nds all
three correct R graphic APIs while Efï¬cientNet misses the
line().

Result 1: The SPGNN outperforms the state-of-the-
art baselines VGG-16, ResNet-50,
Inception-v1 and
Efï¬cientNet-B3 substantially on the respect of API recom-
mendation. The results reï¬‚ect that our model is effective
and can be used to assist developers in plotting.

B. RQ2: How well do our SPGNN model perform after com-
bining the semantic parsing module and the random erasing-
based data augmentation?

The Efï¬cientNet-B3 can be deemed as the plain version
of SPGNN without the semantic parsing. From the observa-
tions in Table III, IV and V, SPGNN are consistently better
than Efï¬cientNet-B3 on all three datasets. More speciï¬cally,
the mAP improvements of SPGNN over Efï¬cientNet-B3 are
2.65%, 1.02% and 1.09% on Python-Plot13, R-Plot32 and
R-Plot14 datasets respectively. Moreover, these observations
also demonstrate the considerable improvement of the off-
the-shelf data augmentation trick on SPGNN. As we can see
from Table III, IV and V, the performance of SPGNN+DA are
4.79%, 2.13% and 3.29% higher than SPGNN.

SPGNN only involves one manually tunable parameter Î±,
which is used to reconcile the optimization of the involved two
tasks. A greater Î± means to pay more attention on the solution
of the semantic parsing task. Figure 6 shows the impacts of
different Î± on the performance of SPGNN. According to the
results, the best Î± is 1, 10 and 1 on Python-Plot13, R-Plot32
and R-Plot14 datasets respectively, which means the visual
features and semantic features have the similar weight in our
model.

Result 2: The semantic parsing and data augmentation
modules are two important parts of our model. After
composing these two tricks, the performance conï¬rms the
effectiveness of these modules for the API recommenda-
tion.

C. RQ3: How well do our SPGNN model perform when
training and testing across different programming languages?
TABLE VI
THE CROSS-LANGUAGE API RECOMMENDATION PERFORMANCES OF
SPGNN IN MAP.

Datasets

APIs

bar

boxplot

plot/line

Python-Plot13
R-Plot32
R-Plot13

87.07
93.65
90.40
In order to evaluate the effectiveness of our method in
ten

dealing with the cross-language API recommendation,

84.85
82.20
89.74

96.70
93.04
97.54

(a) Python graphic example

(b) R graphic example

Fig. 5. The Python and R graphic API recommendation examples. The top-3
APIs recommended by Efï¬cientNet and our method via giving the Python or
R based plots. The green ones are the correct APIs while the red ones are the
wrong API.
language in the future.

As we can see from Table V, the performance of most APIs
is boosted compared with the R-Plot32 dataset. SPGNN+DA
gets 75.13% in mAP, which is 9.05%, 19.32%, 23.29% and
4.38% higher than VGG-16, ResNet-50, Inception-V1, and
Efï¬cientNet-B3, and also gets a better performance than
baselines+DA about 7.17%, 18.60%, 21.72% and 3.84%.
Speciï¬cally, among 14 APIs, our approach achieves the best
API recommendation performance on contour(), density(),
polygon(), raster(), ribbon(), and smooth(). As for the other
APIs, Efï¬cientNet-B3 achieves the best performance, but our
model has the little gap with it.

The performance of contour() is not good among all the
methods on the API recommendation because there are only
24 samples in total, but our model still performs best via all
the methods. After getting more training data, we believe that
the performance will be better. It is also worthwhile to point
out that data augmentation trick signiï¬cantly improves the
recommendation performance of contour(). This implies that
the random erasing-based data augmentation indeed alleviate
the imbalance of sample across the categories, particulary

Fig. 6. The inï¬‚uence of hyper-parameter Î± to the performance of SPGNN
(in mAP).

broken_barh() in Pythonbroken_barh9.67 Ã—ğŸğŸâˆ’ğŸbar3.01 Ã—ğŸğŸâˆ’ğŸbarh2.01 Ã—ğŸğŸâˆ’ğŸ‘bar9.99 Ã—ğŸğŸâˆ’ğŸerrorbar1.62Ã—ğŸğŸâˆ’ğŸ•step1.15 Ã—ğŸğŸâˆ’ğŸ–EfficientNetSPGNNsmooth() violin() line() in Rsmooth9.98 Ã—ğŸğŸâˆ’ğŸviolin2.80 Ã—ğŸğŸâˆ’ğŸ–line1.67 Ã—ğŸğŸâˆ’ğŸğŸsmooth9.99 Ã—ğŸğŸâˆ’ğŸviolin2.10 Ã—ğŸğŸâˆ’ğŸğŸpolygon1.11 Ã—ğŸğŸâˆ’ğŸğŸEfficientNetSPGNN0.010.11102050404550556065707580mAP(%)a Python-Plot13 R-Plot32 R-Plot14developers independently pick up the shared APIs in Python-
Plot13, R-Plot32 and R-Plot14, namely bar(), boxplot() and
plot(), which is called line() in R programming language. In
these experiments, we employ the data of one programming
language for training our model while the data of the other
programming language is used for testing. Table VI records
such experimental results. Taking the ï¬rst row of results as an
example, we train our model on Python-Plot13 dataset, and
test the model using the ï¬gures plotted by R programming
language. In such case,
the recommendation accurcies of
bar(), boxplot() and plot() are 87.07%, 84.85% and 96.70%
respectively. With regard to the experiments related to the last
two rows of results, the data of R-Plot32 and R-Plot14 are
used for trained respectively, while the samples of Python-
Plot13 related to the involved APIs are used for testing. The
observations on the last two rows of Table VI show that our
method still obtains the similarly good results. Moreover, the
recommendation performance of these three APIs via using
our model trained in a cross language way is very similar
to the one observed in Tables III, IV and V, which are
the results produced by our model trained in normal way.
These phenomena all imply that SPGNN essentially learns
the structural geometric characteristics of plots across different
languages and it is possible to conduct the cross-language API
conversion based on the plots.

Result 3: Our model shows the effectiveness of cross-
language API recommendation. No matter what language
is used for plotting, it can recommend the APIs of Python
and R programming languages successfully, as long as
similar features shared among the graphics.

V. DISCUSSION

In this section, we ï¬rst present the real-world user scenarios.

Then, threats to validity will be introduced.

A. Usage Scenarios

To validate the effectiveness of our method, we visualize
several practical applications for demonstrating the utility of
our model in reality. Figures 7 and 8 show plot-based API
recommendation and plot-based cross-language API conver-
sion user scenarios respectively.

Consider the sample of the â€œBreakdown of building typesâ€
shown in the ï¬rst case in Figure 7(a), we suppose that Peter, a
developer with little experience, needs to do a similar project
to show the newly breakdown of building types. Therefore, the
task of Peter is to plot a similar ï¬gure to demonstrate the data.
If Peter knows which API can draw the ï¬gure, he can report
the presentation successfully. To solve the drawing problem,
he can use Plot2API model for API recommendation. In this
step, the only thing he needs to do is to input the graphic
in Figure 7(a) (such graphics may be just downloaded from
web or acquired from other documents) to our tool, and then
the tool will recommend the relevant APIs. There is another
circumstance that Peter does not have a similar ï¬gure. So he
has to draw a ï¬gure manually by himself. Then, he can do

(a) The Plot2API example of web ï¬gure

(b) The Plot2API example of hand-drawn ï¬gure

Fig. 7. Several Plot2API Examples. The top-3 APIs recommended by the
different models via giving the plot. The green ones are the correct APIs
while the red ones are the wrong API.

Fig. 8. Two examples of cross-language API conversion. The green ones are
the correct APIs while the red ones are the wrong API.

the same workï¬‚ow with our tool to acquire the recommended
APIs just based on this hand-drawn ï¬gure as the case shown
in Figure 7(b).

In agile development, some junior developers may not
have broad knowledge of different programming languages
and there are many software projects have similar modules
or functions that can be referenced. In such a manner, the
developers expect to use the output plots in some old projects
developed with the familiar languages as the cues to obtain
the APIs in other language which can draw the similar ï¬gures
directly to accelerate the development process. Our method
can support such a plot-based cross-language API recommen-
dation scheme. Figure 8 shows two successful examples in this
scheme. The ï¬rst case is a R-Plot32 trained SPGNN gives the
reasonable API recommendation for a plot drawn by Python
language while the second one is a Python-Plot13 trained
SPGNN recommends the correct APIs for a plot drawn by
R language.

bar1.00 Ã—ğŸğŸ+ğŸribbon1.23 Ã—ğŸğŸâˆ’ğŸğŸ‘contour1.24 Ã—ğŸğŸâˆ’ğŸğŸ‘pie3.84 Ã—ğŸğŸâˆ’ğŸpolar6.08Ã—ğŸğŸâˆ’ğŸbar6.26 Ã—ğŸğŸâˆ’ğŸ‘Python-based SPGNNR-basedSPGNNpoint2.52 Ã—ğŸğŸâˆ’ğŸline7.48 Ã—ğŸğŸâˆ’ğŸpointrange1.14 Ã—ğŸğŸâˆ’ğŸ–plot2.52 Ã—ğŸğŸâˆ’ğŸscatter7.48Ã—ğŸğŸâˆ’ğŸbarh6.27 Ã—ğŸğŸâˆ’ğŸ“Python-based SPGNNR-basedSPGNNDrawn by step() in PythonDrawn by bar() line() in Rplot9.98 Ã—ğŸğŸâˆ’ğŸbar1.81 Ã—ğŸğŸâˆ’ğŸ‘barh2.16 Ã—ğŸğŸâˆ’ğŸ–step9.95Ã—ğŸğŸâˆ’ğŸpolygon3.98 Ã—ğŸğŸâˆ’ğŸ‘line6.83 Ã—ğŸğŸâˆ’ğŸ“The Recommended APIs of  SPGNN Trained on R-Plot32The Recommended APIs of SPGNNTrained on Python-Plot13B. Threats To Validity

Since our tool is limited to Python and R programming
languages, our techniques may not generalize for other pro-
gramming languages. However, if the features of ï¬gures drawn
from other programming languages are similar to R or Python,
our tool may still work at
these languages. With regard
to the application to the other programming languages, we
believe that our method can still success if the training data
is sufï¬cient.

The other issue is that the performance of API recommen-
dation in some APIs of Python and R is not very well. This is
due to the insufï¬cient training data and the extremely similar
characteristics of different APIs in visual appearance. The
increased training samples of these APIs can address this issue,
since the abundant data can facilitate SPGNN to learn more
visual knowledge to better distinguish the APIs particularly
the similar APIs with each other.

We only pick up some same named APIs between Python
and R programming languages for validating the cross-
language API recommendation due to the lack of ground
truth of automatic evaluation. The manual veriï¬cation will be
conducted to make a more comprehensive veriï¬cation in the
future.

VI. RELATED WORK
API Recommendation: There are a lot of impressive works
in API recommendation [21], [22], [23], [24], [25], [26]. The
most common way for API recommendation is to rank APIs
via using the similarity between the natural language query and
the API description, and then recommend the APIs according
to the ranks. For example, Rahman et al. [27] offered a
recommendation of the relevant API list by using keyword-API
mapping from the crowdsourced knowledge of Stack Over-
ï¬‚ow. Huang et al. [1] proposed BIKER to tackle the lexical
gap and knowledge gap, so that BIKER could automatically
recommend relevant APIs for a programming task described in
natural language. Besides the natural language query, source
code is also an important cue for API recommendation, several
researchers work in this direction. McMillan et al. [2] proposed
Portfolio to ï¬nd highly relevant APIs and projects from a
large archive of C/C++ source code. Chan et al. [28] improved
the Portfolio by employing further sophisticated graph-mining
and textual similarity techniques. A graph-based statistical
language model named GraLan was proposed to develop an
API suggestion engine via computing the probability of usage
graphs which were learned from a corpus of source code to
compute the probability of usage [29].

In conclusion, the existing API recommendation works are
quite different from us. They used the natural language query
or source code as cues for API recommendation task in
these works, which is essentially a text to text pure Natural
Language Processing (NLP) task while Plot2API is an image
to text cross-model machine learning task.

Visual Semantic Embedding: Semantics are widely used
in many neural network models for boosting the visual recog-
nition or classiï¬cation [30], [31], [32], since the visual recog-

nition models are often limited by the increasing difï¬culty of
obtaining sufï¬cient training data in the form of labeled images
as the number of object categories grows [33]. For example,
Wang et al. [34] utilized recurrent neural networks(RNNs) to
address the label dependencies in an image. By combining
CNNs, the proposed CNN-RNN model learned both the se-
mantic redundancy and the co-occurrence dependency in an
end-to-end way. To improve multi-label image classiï¬cation,
Zhu et al. [35] proposed a uniï¬ed deep neural network to
capture both semantic and spatial relations of these multiple
labels based on weighted attention maps. A generic structured
model proposed in [36] employed a stacked label prediction
neural network, capturing both inter-level and intra-level label
semantics to improve image classiï¬cation performance.

Multi-task Learning: Multi-task learning is a popular
machine technique. It aims at developing an integrated model,
which can tackle multiple relevant tasks simultaneously, to
exploit the complementary information among tasks for fur-
ther beneï¬ting the solution of each task [37]. The multi-
task learning works often enjoy a better generalization ability
than the single-task learning method, and have already been
successfully applied to many domains such as computer vision
[38], [39], [40], medical image analysis [41], [42], [43], and
natural language processing [44], [45], [46], and so on. For
example, Sanh et al [47] proposed a hierarchically supervised
multi-task learning model focused on a set of semantic tasks,
such as entity recognition and entity mention detection. Liu et
al. [48] presented a multi-task framework to guide the genera-
tion of TIR-speciï¬c discriminative features for distinguishing
the TIR objects belonging to different classes and ï¬ne-grained
correlation features for TIR tracking. Lu et al. [30] studied
the correlation between vision-and-language tasks for large-
scale, multi-modal, multi-task learning, which shown signif-
icant gains over independent task training. Inspired by these
successes, our method intends to introduce the extra semantic
parsing task to boost the performance of API recommendation.

VII. CONCLUSIONS AND FUTURE WORK

In this paper, we cast a novel and meaningful software
engineering task named Plot2API. To address such an issue,
a deep multi-task learning method named Semantic Parsing
Guided Neural Network (SPGNN) is presented. SPGNN in-
troduces the plot-based semantic parsing to the Efï¬cientNet for
pairing the semantic parsing of plots with the plot-based API-
recommendation. Then the semantics of APIs can be exploited
via the semantic parsing module for boosting the plot-based
API recommendation. Three new Plot2API datasets named
Python-Plot13, R-Plot32 and R-Plot14 are released for evalua-
tion. The experimental results demonstrate the superiority over
other deep learning baselines for Plot2API with a signiï¬cant
advantage and validate the effectiveness of our method in some
application contexts of software engineering.

ACKNOWLEDGE

This work was in part supported by the National Natural
Science Foundations of China (NO. 61772093 and 62002034),

the Fundamental Research Funds for the Central Universities
(NO. 2019CDCGRJ314, 2019CDYGYB014 and 2020CDC-
GRJ072).

REFERENCES

[1] Q. Huang, X. Xia, Z. Xing, D. Lo, and X. Wang, â€œApi method
recommendation without worrying about the task-api knowledge gap,â€ in
2018 33rd IEEE/ACM International Conference on Automated Software
Engineering (ASE).
IEEE, 2018, pp. 293â€“304.

[2] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and C. Fu,
â€œPortfolio: ï¬nding relevant functions and their usage,â€ in Proceedings of
the 33rd International Conference on Software Engineering, 2011, pp.
111â€“120.

[3] B. A. Campbell and C. Treude, â€œNlp2code: Code snippet content assist
via natural language tasks,â€ in 2017 IEEE International Conference on
Software Maintenance and Evolution (ICSME).
IEEE, 2017, pp. 628â€“
632.

[4] M. Allamanis, D. Tarlow, A. Gordon, and Y. Wei, â€œBimodal modelling
of source code and natural language,â€ in International conference on
machine learning, 2015, pp. 2123â€“2132.

[5] T. Gvero and V. Kuncak, â€œInteractive synthesis using free-form queries,â€
in 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, vol. 2.
IEEE, 2015, pp. 689â€“692.

[6] A. Nguyen, P. Rigby, T. Nguyen, D. Palani, M. Karanï¬l, and T. Nguyen,
â€œStatistical translation of english texts to api code templates,â€ in 2018
IEEE International Conference on Software Maintenance and Evolution
(ICSME).

IEEE, 2018, pp. 194â€“205.

[7] M. Tan and Q. Le, â€œEfï¬cientnet: Rethinking model scaling for con-
volutional neural networks,â€ in International Conference on Machine
Learning, 2019, pp. 6105â€“6114.

[8] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, â€œGoing deeper with convolutions,â€
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2015, pp. 1â€“9.

[10] Z.-M. Chen, X.-S. Wei, P. Wang, and Y. Guo, â€œMulti-label

[9] K. Simonyan and A. Zisserman, â€œVery deep convolutional networks for
large-scale image recognition,â€ arXiv preprint arXiv:1409.1556, 2014.
image
recognition with graph convolutional networks,â€ in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2019,
pp. 5177â€“5186.

[11] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, â€œConvolutional neural
networks over tree structures for programming language processing,â€ in
Proceedings of the Thirtieth AAAI Conference on Artiï¬cial Intelligence,
2016, pp. 1287â€“1293.

[12] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanckriet,
R. Levy, and N. Vasconcelos, â€œA new approach to cross-modal multime-
dia retrieval,â€ in Proceedings of the 18th ACM international conference
on Multimedia, 2010, pp. 251â€“260.

[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
â€œDistributed representations of words and phrases and their composi-
tionality,â€ in Advances in neural information processing systems, 2013,
pp. 3111â€“3119.

[14] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales,
â€œLearning to compare: Relation network for few-shot
learning,â€ in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018, pp. 1199â€“1208.

[15] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, â€œRandom erasing

data augmentation.â€ in AAAI, 2020, pp. 13 001â€“13 008.

[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, â€œImagenet:
A large-scale hierarchical image database,â€ in 2009 IEEE conference on
computer vision and pattern recognition.

Ieee, 2009, pp. 248â€“255.

[17] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,
and Q. V. Le, â€œMnasnet: Platform-aware neural architecture search for
mobile,â€ in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2019, pp. 2820â€“2828.

[18] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
â€œMobilenetv2: Inverted residuals and linear bottlenecks,â€ in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2018, pp. 4510â€“4520.

[19] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€

arXiv preprint arXiv:1412.6980, 2014.

[20] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image
recognition,â€ in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770â€“778.

[21] F. Thung, S. Wang, D. Lo, and J. Lawall, â€œAutomatic recommendation
of api methods from feature requests,â€ in Proceedings of International
Conference on Automated Software Engineering, 2013, pp. 290â€“300.

[22] M. Raghothaman, Y. Wei, and Y. Hamadi, â€œSwim: Synthesizing what
i mean-code search and idiomatic snippet synthesis,â€ in Proceedings of
International Conference on Software Engineering, 2016, pp. 357â€“367.
[23] X. Ye, H. Shen, X. Ma, R. Bunescu, and C. Liu, â€œFrom word embeddings
to document similarities for improved information retrieval in software
engineering,â€ in Proceedings of International Conference on Software
Engineering, 2016, pp. 404â€“415.

[24] X. Gu, H. Zhang, D. Zhang, and S. Kim, â€œDeep api learning,â€ in
Proceedings of ACM SIGSOFT International Symposium on Foundations
of Software Engineering, 2016, pp. 631â€“642.

[25] C. Xu, B. Min, X. Sun, J. Hu, B. Li, and Y. Duan, â€œMulapi: A tool for api
method and usage location recommendation,â€ in Proceedings of Inter-
national Conference on Software Engineering: Companion Proceedings,
2019, pp. 119â€“122.

[26] L. Cai, H. Wang, Q. Huang, X. Xia, Z. Xing, and D. Lo, â€œBiker: a
tool for bi-information source based api method recommendation,â€ in
Proceedings of ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering,
2019, pp. 1075â€“1079.

[27] M. M. Rahman, C. K. Roy, and D. Lo, â€œRack: Automatic api recom-
mendation using crowdsourced knowledge,â€ in 2016 IEEE 23rd Interna-
tional Conference on Software Analysis, Evolution, and Reengineering
(SANER), vol. 1.

IEEE, 2016, pp. 349â€“359.
[28] W.-K. Chan, H. Cheng, and D. Lo, â€œSearching connected api subgraph
via text phrases,â€ in Proceedings of the ACM SIGSOFT 20th Interna-
tional Symposium on the Foundations of Software Engineering, 2012,
pp. 1â€“11.

[29] A. T. Nguyen and T. N. Nguyen, â€œGraph-based statistical language
model for code,â€ in 2015 IEEE/ACM 37th IEEE International Confer-
ence on Software Engineering, vol. 1.

IEEE, 2015, pp. 858â€“868.

[30] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee, â€œ12-in-1: Multi-
task vision and language representation learning,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 10 437â€“10 446.

[31] Z.-M. Chen, X.-S. Wei, P. Wang, and Y. Guo, â€œMulti-label

image
recognition with graph convolutional networks,â€ in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2019,
pp. 5177â€“5186.

[32] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image
recognition,â€ in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770â€“778.

[33] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato,
and T. Mikolov, â€œDevise: A deep visual-semantic embedding model,â€
in Advances in neural information processing systems, 2013, pp. 2121â€“
2129.

[34] J. Wang, Y. Yang, J. Mao, Z. Huang, C. Huang, and W. Xu, â€œCnn-rnn: A
uniï¬ed framework for multi-label image classiï¬cation,â€ in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2016, pp. 2285â€“2294.

[35] F. Zhu, H. Li, W. Ouyang, N. Yu, and X. Wang, â€œLearning spatial
regularization with image-level supervisions for multi-label image clas-
siï¬cation,â€ in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2017, pp. 5513â€“5522.

[36] H. Hu, G.-T. Zhou, Z. Deng, Z. Liao, and G. Mori, â€œLearning structured
inference neural networks with label relations,â€ in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2016,
pp. 2960â€“2968.

[37] R. Caruana, â€œMultitask learning,â€ Machine learning, vol. 28, no. 1, pp.

41â€“75, 1997.

[38] F. J. Bragman, R. Tanno, S. Ourselin, D. C. Alexander, and J. Cardoso,
â€œStochastic ï¬lter groups for multi-task cnns: Learning specialist and
generalist convolution kernels,â€ in Proceedings of the IEEE International
Conference on Computer Vision, 2019, pp. 1385â€“1394.

[39] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert, â€œCross-stitch net-
works for multi-task learning,â€ in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2016, pp. 3994â€“4003.

[40] G. Strezoski, N. v. Noord, and M. Worring, â€œMany task learning with
task routing,â€ in Proceedings of the IEEE International Conference on
Computer Vision, 2019, pp. 1375â€“1384.

[41] B. Wu, Z. Zhou, J. Wang, and Y. Wang, â€œJoint learning for pulmonary
nodule segmentation, attributes and malignancy prediction,â€ in 2018
IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018).
IEEE, 2018, pp. 1109â€“1113.

[42] S. Hussein, K. Cao, Q. Song, and U. Bagci, â€œRisk stratiï¬cation of
lung nodules using 3d cnn-based multi-task learning,â€ in International
conference on information processing in medical imaging.
Springer,
2017, pp. 249â€“260.

[43] N. Khosravan and U. Bagci, â€œSemi-supervised multi-task learning for
lung cancer diagnosis,â€ in 2018 40th Annual International Conference of
the IEEE Engineering in Medicine and Biology Society (EMBC).
IEEE,
2018, pp. 710â€“713.

[44] R. Collobert and J. Weston, â€œA uniï¬ed architecture for natural language
processing: Deep neural networks with multitask learning,â€ in Proceed-
ings of the 25th international conference on Machine learning, 2008,

pp. 160â€“167.

[45] X. Liu, P. He, W. Chen, and J. Gao, â€œMulti-task deep neural networks
for natural language understanding,â€ in Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, 2019, pp.
4487â€“4496.

[46] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu, â€œExploring the limits of
trans-
transformer,â€ arXiv preprint
fer learning with a uniï¬ed text-to-text
arXiv:1910.10683, 2019.

[47] V. Sanh, T. Wolf, and S. Ruder, â€œA hierarchical multi-task approach for
learning embeddings from semantic tasks,â€ in Proceedings of the AAAI
Conference on Artiï¬cial Intelligence, vol. 33, 2019, pp. 6949â€“6956.
[48] Q. Liu, X. Li, Z. He, N. Fan, D. Y. 0002, W. Liu, and Y. Liang, â€œMulti-
task driven feature models for thermal infrared tracking.â€ in AAAI, 2020,
pp. 11 604â€“11 611.

