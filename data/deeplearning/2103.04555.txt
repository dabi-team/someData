1
2
0
2

l
u
J

8

]

G
L
.
s
c
[

3
v
5
5
5
4
0
.
3
0
1
2
:
v
i
X
r
a

Real-world Ride-hailing Vehicle Repositioning using
Deep Reinforcement Learning ∗

Yan Jiao
DiDi Labs
Mountain View, CA 94043
yanjiao@didiglobal.com

Xiaocheng Tang
DiDi Labs
Mountain View, CA 94043
xiaochengtang@didiglobal.com

Zhiwei (Tony) Qin†
DiDi Labs
Mountain View, CA 94043
qinzhiwei@didiglobal.com

Shuaiji Li
DiDi Labs
Mountain View, CA 94043
shuaijili@didiglobal.com

Fan Zhang
Didi Chuxing
Beijing, China
feynmanzhangfan@didiglobal.com

Hongtu Zhu
Didi Chuxing
Beijing, China
zhuhongtu@didiglobal.com

Jieping Ye
University of Michigan
Ann Arbor, MI
jieping@gmail.com

Abstract

We present a new practical framework based on deep reinforcement learning and
decision-time planning for real-world vehicle repositioning on ride-hailing (a type
of mobility-on-demand, MoD) platforms. Our approach learns the spatiotemporal
state-value function using a batch training algorithm with deep value networks.
The optimal repositioning action is generated on-demand through value-based
policy search, which combines planning and bootstrapping with the value networks.
For the large-ﬂeet problems, we develop several algorithmic features that we
incorporate into our framework and that we demonstrate to induce coordination
among the algorithmically-guided vehicles. We benchmark our algorithm with
baselines in a ride-hailing simulation environment to demonstrate its superiority
in improving income efﬁciency measured by income-per-hour. We have also
designed and run a real-world experiment program with regular drivers on a major
ride-hailing platform. We have observed signiﬁcantly positive results on key
metrics comparing our method with experienced drivers who performed idle-time
repositioning based on their own expertise.

Keywords: ridesharing, vehicle repositioning, deep reinforcement learning

∗This paper is a signiﬁcantly extended version of [12].
†corresponding author

Preprint. Under review.

 
 
 
 
 
 
1

Introduction

The emergence of ride-hailing platforms, led by companies such as DiDi, Uber, and Lyft, has
revolutionized the way that personal mobility needs are met. As urban populations continue to grow
in world’s largest markets [20, 33], the current modes of transportation are increasingly insufﬁcient to
cope with the growing and changing demand. The digital platforms offer possibilities of much more
efﬁcient on-demand mobility by leveraging more global information and real-time supply-demand
data. Auto industry experts expect that ride-hailing apps would eventually make individual car
ownership optional, leading towards subscription-based services and shared ownership [6]. However,
operational efﬁciency is a tough challenge for ride-hailing platforms, e.g., passenger waiting time can
still be long [29], while drivers often chase surge based on their own experience which may not be
always effective [8], and ride-hailing vehicles can be vacant 41% of the time in a large city [4].

Vehicle repositioning and order matching are two major levers to improve the system efﬁciency of
ride-hailing platforms. Vehicle repositioning [27, 15, 22] has direct inﬂuence on driver-side metrics
and is an important lever to reduce driver idle time and increase the overall efﬁciency of a ride-hailing
system, by proactively deploying idle vehicles to a speciﬁc location in anticipation of future demand at
the destination or beyond. It is also an integral operational component of an autonomous ride-hailing
system, where vehicles are fully managed by the dispatching algorithm. On the other hand, order
dispatching [38, 37, 32, 23, 39] matches idle drivers (vehicles) to open trip orders, which is a related
but different problem. Order dispatching directly impacts passenger-side metrics, e.g. order response
rate and fulﬁllment rate. Together, these two levers collectively aim to align supply and demand
better in both spatial and temporal spaces.

In this paper, we tackle the problem of vehicle repositioning on a ride-hailing platform, motivated
by the development of an intelligent driver assistant program for helping the drivers on the platform
to achieve better income. There are two scenarios for vehicle repositioning: small and large ﬂeets.
Both have their speciﬁc use cases. In the small-ﬂeet problem, our objective is to learn an optimal
policy that maximizes an individual driver’s cumulative income rate, measured by income-per-hour
(IPH, see (2)). This scenario can target, for example, those who are new to a ride-hailing platform
to help them quickly ramp up by providing learning-based idle-time cruising strategies. This has
signiﬁcant positive impact to driver satisfaction and retention. Such program can also be used as
bonus to incentivize high quality service that improves passenger ridership experience. Of course,
our proposed repositioning algorithm is equally applicable to the management of a small autonomous
vehicle ﬂeet within a large-scale ride-hailing system. In the large-ﬂeet case, on the other hand, we are
interested in optimizing the IPH at the group level (see (3)). This applies to the more general use case
of managing the ride-hailing vehicles within an entire region (city). Care has to be taken to make sure
that decisions on individual vehicles do not cause macro-level problems, for example, repositioning
too many idle vehicles to a single high-demand spot.

We have implemented our proposed algorithm as the decision engine for an AI driver assistant
program on a major ride-hailing platform and performed real-world ﬁeld tests for both small and
large ﬂeets that demonstrate its superiority over human expert strategies.

1.1 Related Works

Many of the works on vehicle repositioning or taxi dispatching are under the setting of autonomous
MoD, where a ﬂeet of autonomous vehicles [24] are deployed in an MoD system and fully managed
by a controller.

Model predictive control (MPC) and receding horizon control (RHC) [11, 19, 41, 18] are a popular
class of methods that involve repeatedly solving a mathematical program using predictions of
future costs and system dynamics over a moving time horizon to choose the next control action. A
related method for ﬂeet management in the trucking industry is [28] where approximate dynamic
programming (ADP) is used. Previous works are all at grid-level granularity (with the solution only
specifying the number of vehicles needed to transfer between cells) and using discrete time buckets
with a short planning horizon (small number of buckets) due to computational complexity. For
example, [5] has a planning horizon of 30 minutes with 5-minute time steps. With the objective of
ﬁnding the shortest route to the immediate next trip, a multi-arm bandits (MAB) method with Monte
Carlo tree search [7] has also been proposed. Our method differentiates in that it has a much longer

2

optimization horizon (whole day) by using state-value functions and can easily meet the run-time
requirement of our driver assistant application.

A number of recent works use RL and deep RL to learn vehicle repositioning policies, such as
dynamic programming [27] and Monte Carlo learning [35]. Through training within a grid-based
simulation environment where vehicle within a grid are indistinguishable and matching happens only
within a grid, DQN [22, 10], multi-agent RL [15], and hierarchical RL [13] have been proposed for
the repositioning problem, and in some cases, jointly with order dispatching.

For order matching/dispatching on ride-hailing platforms, a related but different problem from the
vehicle repositioning problem, extensive literature has been proposed on, e.g., dynamic matching
[40, 39], multi-stage stochastic optimization [16], neural ADP for carpooling [26], network ﬂow
optimization with rolling horizon [3], and (deep) reinforcement learning [38, 37, 32, 14].

Vehicle repositioning and ﬂeet management are also related to classical vehicle routing problems,
where machine learning methods with deep neural networks have been used as new approaches to the
TSP and the VRP, under an encoding-decoding framework [21, 2, 36].

1.2 Algorithm Design Considerations

In designing the solution framework for the vehicle repositioning problem, there are a few factors
for consideration. First of all, the algorithm has to be practical for implementation on real-world
production system and for real-time execution. That means we would not be able to make signiﬁcant
simpliﬁcation assumptions that many some prior works have done. In particular, grid-based algorithms
with low decision granularity (i.e., only determining the cell-to-cell reposition quantities) and those
require training or planning in a simulation environment (e.g., [10, 7]) are hard to deploy in our
setting. Second, our objective is to maximize an agent’s daily income rate, which is long-term
reward optimization on the scale of trips. MPC with long horizon is expensive to solve. A coarse
discretization for the time would render the solution hard to implement and execute in our driver
assistant program where review intervals are short (e.g., 100s). On the other hand, RL, which focuses
on long-term values, is well-suited for such objectives. We show later in simulation experiments
the advantage in IPH of our method over an MAB-based method [7] without considering temporal
dependency of the decisions. Third, data from regular ride-hailing system is usually incomplete
regarding idle-time repositioning due to online-ofﬂine behavior. It is thus hard to learn a state-action
value function for repositioning directly from data using such method as Monte Carlo learning [35].

Considering the factors discussed above, we have developed a solution framework that combines
ofﬂine batch RL and decision-time planning for guiding vehicle repositioning. We model the
problem within a semi-Markov decision process (semi-MDP) framework [31], which optimizes a
long-term cumulative reward (daily income rate) and models the impact of temporally extended
actions (repositioning movements) on the long-term objective through state transitions along a policy.
We learn the state value function using tailored spatiotemporal deep value networks trained within a
batch RL framework with dual policy evaluation. We then use the state-value function and learned
knowledge about the environment dynamics to develop a value-based policy search algorithm for
real-time vehicle repositioning, which is a type of decision-time planning algorithm [30] and can
be easily plugged into a generalized policy iteration framework [30] for continuous improvement.
For managing a large ﬂeet, we simplify the search and focus more on the value function, taking into
account the need for coordination among the vehicles.

2 The Repositioning Problem

We start from the environment dynamics. The driver, when idle, can be assigned to an open trip order
by the ride-hailing platform per the matching policy. Order dispatching/matching takes place in a
batch fashion typically with a time window of a few seconds [32, 38, 40]. The driver goes to the origin
of the trip (i.e. where the passenger is located) and transports the passenger to the destination. Trip fee
is collected upon the completion of the trip. After dropping off the passenger, the driver becomes idle.
If the idle time exceeds a threshold of L minutes (typically ﬁve to ten minutes), the driver performs
repositioning by cruising to a speciﬁc destination, incurring a non-negative cost. If the driver is to
stay around the current location, he/she stays for L minutes before another repositioning could be
triggered. During the course of any repositioning, the driver is still eligible for order assignment.

3

Figure 1: A sample map (obtained from Google Maps) overlaid with a hexagon grid system. The
cell with a red circle represents the current cell of the vehicle, and those with blue circles are its
neighboring cells.

For modeling purpose, we assume that the driver can always complete a repositioning task before
the platform would match the driver to an order, but this has little impact to the generality of the
formulation, and the policy is not restricted by this assumption in any way. As we will explain below,
a basic reposition moves the vehicle to a neighboring zone deﬁned by a hexagonal grid system. As
long as the zone structure is sufﬁciently granular, it is reasonable to assume that the reposition has
been completed if the driver is dispatched in the destination zone. If the driver is dispatched while
still in the origin zone, we simply treat it as if the reposition never takes place.

The objective of a repositioning algorithm is to maximize income efﬁciency (or used interchangeably,
income rate), measured by income per (online) hour, IPH. This can be measured at an individual
driver’s level or at an aggregated level over a group of drivers. It is clear that vehicle repositioning
is a sequential decision problem in which the current action affects the future income of the driver
because of the spatiotemporal dependency.

2.1 Semi-MDP Formulation

Mathematically, we model the trajectory of a driver by a semi-MDP with the agent being the driver
as follows.

State The driver’s state s contains basic spatiotemporal information of location l and time t, and
it can include additional supply-demand contextual features f . Hence, we have s = (l, t, f ). 3
Although we use GPS coordinates for l, the basic geographical zones are deﬁned by a hexagonal grid
system, as illustrated in Figure 1. The hexagonal grid system is commonly used in mapping systems
because it has a desirable property that the Euclidean distance between the center points of every
pair of neighboring grid cells is the same, and hexagonal grids have the optimal perimeter/area ratio,
which leads to a good approximation of circles [9].

Option The eligible actions for the agent include both vehicle repositioning and order fulﬁllment
(as a result of order dispatching). These actions are temporally extended, so they are options in
the context of a semi-MDP and are denoted by o. A basic repositioning option is to go towards a
destination in one of the six neighboring hexagonal cells of the driver plus the current cell itself. See
the cell with a red circle (current cell) and those with blue circles (neighboring cells) in Figure 1.
We use a single option od to represent all the dispatching (order fulﬁllment) options, since in the
context of this paper, the agent does not need to make dispatching decisions. The time duration of a
repositioning or order fulﬁllment option is τo.

3In Section 3 and the associated experiments, only the basic spatiotemporal features (l, t) are used. The

contextual features f are elaborated in Section 4.

4

Reward The price of the trip corresponding to an option is po > 0. The cost of a repositioning
option is co ≥ 0. Hence, the immediate reward of a transition is r = −co for repositioning and
r = po for order fulﬁllment. The corresponding estimated version of τo, po, and co are ˆτo, ˆpo, and ˆco,
respectively.

Transition The transition of the agent given a state and a repositioning option is deterministic,
while the transition probability given a dispatching option P (s(cid:48)|s, od) is the probability of a trip
going to s(cid:48) given it being assigned to the agent at s.

An episode of this semi-MDP runs till the end of a day, that is, a state with its time component at
midnight is terminal. We denote the repositioning and the order dispatching policies separately by
πr and πd. At any decision point, either a dispatch or a reposition is performed to a vehicle. The
vehicle is reviewed for order dispatching at highly granular time intervals (e.g., every two seconds).
If it is matched, it starts to serve the order. Otherwise, it waits for the next review for dispatching or
repositioning. Repositioning is reviewed at much coarser time intervals, e.g., every 100 seconds. If it
is idle and not being reviewed for dispatching, πr determines a reposition for the vehicle. The overall
policy π can therefore be deﬁned as

π(s) =

(cid:26)πr(s)

if s is a reposition review point,

πd(s) otherwise (i.e., s is a dispatch review point).

(1)

In this paper, we are interested in only the reposition policy πr since dispatching is handled by an
exogenous policy πd0. 4 Hence, we would not explicitly learn πd0, though we learn a state-value
function associated with the current policy π0 from the trip data as in Section 3.1.
The state-option value function is denoted by Qπr (s, o), with the understanding that it is also
associated with πd0. ˆQ denotes the approximation of the Q-function. In the following sections,
we develop a planning method to compute ˆQ(s, o) for a particular s so that the repositioning agent
would be able to select the best movement at each decision point. Our objective is to maximize the
cumulative income rate (IPH), which is the ratio of the total price of the trips completed during an
episode and the total online hours logged by a driver (individual level) or a group of drivers (group
level). The individual-level IPH for a driver x is deﬁned as

p(x) :=

c(x)
h(x)

,

(2)

where c(·) is the total income of the driver over the course of an episode, and h(·) is the total online
hours of the driver. Similarly, the group-level IPH for a driver group X is deﬁned as

p(X) :=

.

(3)

(cid:80)
(cid:80)

x∈X c(x)
x∈X h(x)

3 State Value-based Decision-time Planning

With the semi-MDP formulation in place, we ﬁrst describe a repositioning strategy designed from the
perspective of individual drivers who account for a small percentage of the driver population in a city
and whose spatial activity ranges hardly interact. This approach falls in the family of decision-time
planning, utilizing learned state values.

3.1 Learning State Values

We assume the following environment model per the dynamics and learn the state values associated
with the semi-MDP above. At state s, the probability of the driver being dispatched is p(s)
d . The
probability of being idle within the time interval of L minutes is p(s)
d , upon which
repositioning is triggered next, and the driver can either stay around or move to another location.
If order dispatch takes place, the driver is relocated to the destination of the trip. The estimated
time interval for transitioning from the current state s0 to the target state si is t0i := ∆t(s0, si).
Transitions for both vehicle repositioning and order fulﬁllment are deterministic, given the current
state and option. This is illustrated in Figure 2.

id = 1 − p(s)

4In fact, per our deﬁnition of the order dispatching option, πd is degenerate - there is only one option

available.

5

Figure 2: Environment dynamics model. Any order dispatch happening during ‘reposition’ or ‘stay’
is assumed to take place at the next node. The nodes without marks represent the other eligible
reposition destinations. The numbered nodes are repositions that the policy chooses.

We decompose the state value function into four components by conditioning on whether the driver is
being dispatched or not, e.g.,

V (s) = p(s)

d V (s|dispatch) + p(s)
where V (s|dispatch) and V (s|idle) are the corresponding long-term value function conditioned on
whether the associated driver is being dispatched or not at state s. In particular, we are interested in
the conditional value function V (s|dispatch). The reason will be clear in Section 3.2.

id V (s|idle),

(4)

In our semi-MDP formulation, dispatching is represented by the single option od. So, V (s|dispatch)
is the conditional value function V (s|o = od), and the dispatch probability p(s)
is π(o = od|s). To
d
keep the notation concise, we will write V (s|o = od) and π(o = od|s) as V (s|od) and π(od|s) re-
spectively. Learning of (4) can be done by evaluating the behavior policy on the observed transactions
collected from the ride-hailing platform. In particular, we apply variance reduction techniques to the
joint learning of both V (s) and the conditional value function V (s|od). Here, we directly learn V (s)
from data instead of computing it using (4) in order to minimize the errors due to estimation and
composition. The dispatch probabilities are estimated separately after applying negative sampling to
drivers’ trajectories. Next we describe the approach in details.

3.1.1 Dual Policy Evaluation

Notice that V (s|od) is equivalent to the state-option value function Q(s, od). Evaluation of V (s|od)
can thus be done using standard TD algorithms like SARSA [30]. In this work we propose Dual
Policy Evaluation (DPE) that prevents stochasticity in the policy from increasing variance. It does so
by jointly learning V (s|od) and V (s) while basing the update of V (s|od) not on the next-state value
function V (s(cid:48)|od), but on the marginal expectation V (s(cid:48)). Formally, consider the k-step transition
from s0 to sk by applying the option od. We can write down the k-step Bellman updates at the i-th
iteration as follows,

V (i)(s0|od) ←

Rd(γk − 1)
k(γ − 1)

+ γkV (i)(sk),

where the value function V (sk) in the RHS is updated using the standard value iteration,

V (i)(s0) ←

R(γk − 1)
k(γ − 1)

+ γkV (i−1)(sk).

(5)

(6)

Update (5) is applied only when the transition involves a dispatch option. Rd is the immediate reward
from a dispatch option, i.e., the trip price. Update (6) can be applied for both dispatch and reposition
transitions, and R is the reward from the option, which is either the cost of idle movement (≤ 0) or
the trip fee (> 0). The time discounting technique proposed in [32] is applied to the reward as in (5).
The γ is the discount factor between 0 and 1, and k ≥ 1 is the time step index.

The update equation (5) is, in essence, similar to expected SARSA [34]. The main difference is that
expected SARSA uses empirical samples to approximate the expected value while DPE does so by
directly learning a separate function approximator. To see that, note that V (s) can be considered
as the marginal expectation of the conditional value function V (s|o), i.e., V (sk) = Eo(V (sk|o)).
Hence the RHS of the updates in (5) is directly using the expectation, i.e., Rd(γk−1)
k(γ−1) +γkEo(V (sk|o))

6

while expected SARSA uses empirical transition samples Sk starting from the state sk to approximate
the corresponding expectation, i.e., Rd(γk−1)
V (sk|o). The overhead of learning
two policies is minimal in this case since both V (s|od) and V (s) are required for the value-based
policy search as described in Section 3.2.

k(γ−1) + 1

|Sk| γk (cid:80)

o∈Sk

Similar to [32], we use a neural network to represent the value function, but the training is different
since we now maintain and update both the conditional value network V (s|o) and the marginalized
one V (s). We employ the same state representation and training techniques as introduced by [32].
Besides, for the conditional network we engage a separate embedding matrix to encode the option and
use the multiplicative form to force interactions between the state features and the option embedding.
Both V (s|o) and V (s) share the same state representation but have a separate branch for the output.
An algorithm statement of DPE can be found in Appendix A.

3.1.2 Dispatch Probabilities

We estimate the dispatching probability p(s)
d ≡ π(od|s) by maximizing its log-likelihood on the
transition data. To generate the training data we collect drivers’ historical trajectories including the
descriptions of completed trips as well as the online and ofﬂine states. We use the states when the
driver receives the trip request as the positive examples indicating the option o being od. For the
negative examples we are unable to enumerate all possibilities considering the limited observed
trajectories and the system complexity. To that end we perform negative samplings. The negative
examples we use for training are drivers’ starting states of idle transaction in-between orders as
well as the states when they become active or inactive. The training is done using one-month driver
trajectories. Experiments on hold-out data sets show that the learned estimator achieves AUC of
0.867 ± 0.010 across multiple days and cities. Detailed results are presented in Appendix B.

3.2 Value-based Policy Search (VPS)

With the deep value network learned within the training framework in Section 3.1, the optimal action
is selected using decision-time planning when repositioning is triggered. Speciﬁcally, we use our
environment model to carry out planning of potentially multiple steps from the particular state s0,
the state of the agent when repositioning is triggered. This allows us to evaluate at run-time the
state-option pairs associated with s0 and all the available repositioning options at that state and select
the best move for the next step. The next time repositioning is triggered, the same planning process is
repeated.
We want to estimate Q∗(s0, o) associated with the optimal policy π∗
r and the given πd0, so that
o∗ = arg maxo ˆQ∗(s0, o) gives the approximate optimal repositioning at decision-time with respect
to a given dispatch policy. The one-step expansion per the environment model in Section 3.1 writes
Q∗(s0, o) = r(0,1) + (V ∗)(t01)(s1),
(7)
where r(0,1) ≤ 0 is the repositioning cost from s0 to s1, and s1 is the state after repositioning o, with
location l1 and time t0 + t01. V ∗ is the state-value function associated with π∗
r (and the given πd0).
To make the time component of the input explicit, (V ∗)(t01) is the same value function with time
component t01 ahead of the decision-time t0. All value functions with a future time component are
assumed to be properly discounted without clogging the notation in Section 3.2. The discount factor
is γ(t−t0), where t is discretized time component for the input state, and t0 is the time for current
decision point. The duration that incurs cost r is ∆t, and the starting time for the cost is t. The cost
used in this paper is also properly discounted: r ← γ(t−t0)r(γ∆t−1)

.

∆t(γ−1)

In practice, we use the state-value function V learned in Section 3.1 to replace V ∗ for computing
ˆQ∗(s0, o), writing V (t01)(s1) concisely as V (t01)

. Then,

1

ˆQ∗(s0, o) = r(0,1) + V (t01)
(8)
That is, that the one-step expansion renders a greedy policy by selecting the repositioning movement
leading to the next-state with the highest value given by the state value networks, V . We also notice
that ˆQ∗(s0, o) is in fact Qπ0(s0, o) in this case because V is computed by policy evaluation on histor-
ical data generated by π0. Hence, ﬁnding the optimal option to execute by o∗ = arg maxo ˆQ∗(s0, o)
is essentially one-step policy improvement in generalized policy iterations.

1

.

7

Figure 3: Planning + bootstrapping perspective of the proposed algorithm. At the leaf nodes,
bootstrapping is used through the given state-value function V . The subtree within the red loop
illustrates the back-up of values.

We can use our environment model to expand ˆQ∗ further:

ˆQ∗(s0, o) = r(0,1) + p(1)
d

ˆV ∗(s1|dispatch) + p(1)
id

ˆV ∗(s1|idle).

(9)

p(1)
is the dispatch probability at s1, and we use the conditional value networks discussed in Section
d
3.1, denoted by ˜V (t01)
, for ˆV ∗(s1|dispatch). When the driver is idle, the immediate next option
has to be a reposition, so we have ˆV ∗(s1|idle) = maxj ˆQ∗(s1, oj), where oj is the second-step
reposition option. ˆQ∗(s0, o) can be recursively expanded, eventually written in terms of the given
estimated state-value function.

1

A two-step expansion is that

ˆQ∗(s0, o) = r(0,1) + p(1)
d

˜V (t01)
1

+ p(1)

id max
j

= r(0,1) + p(1)
d

˜V (t01)
1

+ p(1)

id max

For a three-step expansion, we further write

ˆQ∗(s1, oj) = r(1,j) + p(j)
d

˜V (t0j )
j

+ p(j)

id max

ˆQ∗(s1, oj)
(cid:26)

max
j(cid:54)=1

(cid:26)

max
k(cid:54)=j

r(1,j) + V (t0j )

j

, V (t01+L)

1

(cid:27)

.

(10)

r(j,k) + V (t0k)

k

, V (t0j +L)

j

(cid:27)

.

(11)

In the above equations, t0j := t01 + t1j, t0k := t01 + t1j + tjk, are the total ETA of two-step and
three-step repositions respectively. Figure 3 summarizes the process of our value-based policy search.

We can interpret the above decision-time planning through path value approximation. For reposition-
ing, not only the destination cell is important, the route is important as well, since the driver may be
matched to orders along the way, generating income. Our approach can be viewed as computing the
long-term expected values of n-step look-ahead repositioning paths from the current spatiotemporal
state, selecting the optimal one, and then executing its ﬁrst step. An n-step look-ahead path also
corresponds to a path from the root to a leaf node with a depth of 2n in Figure 3. We show in Section
3.2.1 that selecting the max-value path is equivalent to o∗ = arg maxo ˆQ∗(s0, o) in terms of the
action executed. The new data generated by the current learned policy is collected over time and can
be used to update the state-value function, which in turn updates the policy through decision-time
planning.

3.2.1 Practical Implementation

Our implementation of the policy search algorithm (VPS) is based on the path-value perspective
instead of directly implementing (8) to (11). The reason is that the latter does not allow batch

8

inference of the state value networks, which is crucial for fast computation. We ﬁrst show the
equivalence of the two approaches taking the three-step expansion as an example.

By convention, for the case of staying at the same location (j = k), r(j,k) = 0 and t0k = t0j + L.
(cid:111)
. Through
Then, we have max
further transformation, we get ˆQ∗(s, o)

r(j,k) + V (t0k)

r(j,k) + V (t0k)

, V (t0j+1)

= maxk

maxk(cid:54)=j

(cid:110)

(cid:111)

(cid:111)

(cid:110)

(cid:110)

k

k

j

= r(0,1) + p(1)
d

˜V (s1) + p(1)

id (max

j

= r(0,1) + p(1)
d

˜V (s1) + p(1)

id max
j

ˆQ∗(s1, oj))
(cid:26)

r(1,j) + p(j)
d

˜V (sj) + p(j)

id max
k

(cid:110)

r(j,k) + V (t0k)

k

(cid:111)(cid:27)

= r(0,1) + p(1)
d

˜V (s1) + p(1)

id max
j,k

(cid:110)

r(1,j) + p(j)
d

˜V (sj) + p(j)

id (r(j,k) + V (t0k)

k

(cid:111)
)

(cid:110)

r(0,1) + p(1)
d

= max
j,k

˜V (s1) + p(1)

id (r(1,j) + p(j)

d

˜V (sj) + p(j)

id (r(j,k) + V (t0k)

k

(cid:111)
.

))

(12)

Equation (12) shows the correctness of our implementation, supported by the following proposition:

The state-option value that we use for decision-time planning (Eq (11)) is the maximum expected
value of the three-step paths with o being the ﬁrst step.

We have developed a highly efﬁcient implementation of VPS that has met the strict production
requirement on latency for our real-world experiment. Since the vehicle repositioning requests can be
processed independently, the proposed algorithm is horizontally scalable.

Our algorithm based on (12) is split into two phases. First, we generate all the paths of a certain
length originating at the current grid cell using breadth-ﬁrst-search. Second, we calculate the value
of each path and select the ﬁrst step of the path which has the maximum value as our repositioning
action. Keeping two separate phases has the beneﬁt of allowing batch inference of the state value
networks model in the second phase. We will now describe the details of each phase.

Path generation Each city is divided into hexagon cells, each covering an equal size area. In each
grid cell, we use the pick-up points (from an in-house map service) as the candidate repositioning
destinations. Some of those cells might be “inaccessible"(e.g. lakes, rivers, mountains) or might be
missing well-known pick-up points (e.g. low population density areas). These cells are excluded
from our search.

The main parameter for the path generation is the maximum path length that our algorithm should
consider. We call this parameter the expansion depth (also from the perspective of (10) and (11)). Our
algorithm then uses a breadth-ﬁrst search (BFS) to generate paths of a maximal length no longer than
the expansion depth. Because of the presence of invalid cells, it may happen that no generated path
reaches the search depth. In the infrequent case where no cell is valid within the search depth distance
to the origin cell, we expand our breadth-ﬁrst search to the city limit to ﬁnd the closest valid cells to
the current cell and use those as repositioning candidates. We also note that with bootstrapping the
state values, the path values account for an horizon far beyond the expansion depth.

Step selection Once a set of paths has been generated, depending on the length reached, our
algorithm applies a different formula to each path in order to calculate its value, e.g., (12) being
applied to the paths of length three, and (8) being applied to unit-length paths. Finally, the algorithm
returns as reposition action the ﬁrst step of the path which has the maximum value.

Long search Occasionally, it can take the above search algorithm many repositioning steps to
guide a driver to an area of higher value because e.g., the driver is stuck in a large rural area with low
demand, leading to low repositioning efﬁciency. To tackle this issue, we have developed a special
mechanism called long-search, triggered by idling for more than a time threshold (e.g. 100 minutes).
Long-search repositions drivers to a globally relevant area in order to help them escape from ‘local
minima’, by choosing among destinations with globally top state values (with coarser time window)
discounted by their respective travel times. Speciﬁcally, for each hex cell, we use the most popular
pick-up location (provided by an in-house map service) to represent it and consider the location?s
state value given by the spatiotemporal value network developed in Section 3.1. Then, we retrieve
the locations corresponding to the top 200 state values averaged over 20-minute intervals. The

9

values are pre-computed and stored in a look-up table. During long search, the destination with top
time-discounted state value for that hour with respect to the current location of the vehicle is selected
for reposition. Time discounting is done by considering the estimated travel time τ (in minutes) from
the current location of the vehicle to the candidate destination and using a multiplicative discount
factor λτ /10, where λ is the discount parameter in Section 3.2.

Computational complexity & approximation error The complexity of BFS on the grid cell
graph (each unit being a hexagon cell) for solving (11) is O(n2) in the expansion depth of n. As
in Section 5.1, we have found that a small n works the best in practice, which limits the impact of
this quadratic behavior. Larger depth values would result in accumulation of errors introduced by
model assumption and estimation, which outweighs the beneﬁts of the models. The primary source
of approximation error in planning is the estimation of dispatch probabilities. Per (10) and (11), the
multiplicative factors for the value terms V ’s and ˜V ’s are products of the dispatch probabilities at
each expansion level above them. The estimation error is hence compounding in nature, and the
predictive capability of the product of dispatch probabilities as a binary classiﬁer quickly deteriorates
as n increases, considering the AUC of the dispatch probability model.

4 Learning Action-values for a Large Fleet

There are more challenges for managing the repositioning of a large ﬂeet. More global coordination
is required so that repositioning does not create additional supply-demand imbalance. As a simple
example, under independent individual strategy, all the vehicles with the same spatiotemporal
state may be repositioned to the same high-value destination. This ‘over-reaction’ phenomenon is
undesirable because it may cause the seemingly high-value destination to become over-supplied in
the future while leaving the other areas under-supplied. Hence, it is necessary to make modiﬁcation
to the learning algorithm so that coordination among a group of vehicles can be better induced into
the repositioning policy.

Similar to the small-ﬂeet case, we learn a single shared action-value network to generate policies
for each managed vehicle. There are multiple beneﬁts associated with this decision. First of all,
the number of agents in the vehicle repositioning problem changes over time, since drivers may
get online and ofﬂine at different times, and existing vehicles may exit and new vehicles enter the
system, making it hard to manage agent-speciﬁc networks especially for new agents without any
data. A single value network is agnostic to the number of agents in the environment, thus naturally
handling this issue well. In contrast, learning agent-speciﬁc value functions, as in many multi-agent
reinforcement learning (MARL) methods, is challenging to ﬁt in this setting. The second advantage
is the ability to crowd-source training data. Abundance and diversity in experience samples are
especially important in ofﬂine RL. Training a separate network for each agent may not be the most
effective way to utilize the data because the data speciﬁc for each agent is limited and may cover only
a small part of the state-action space. Lastly, from a practical perspective, a single value network
requires a simpler training pipeline, making it deployment-friendly for a high usage service like
vehicle repositioning.

We choose to learn the action-value function Q(s, o) directly in this case. The motivations mostly
come from the ease of incorporating real-time contextual features and algorithmic elements of MARL,
which we will elaborate in the subsections below. Incorporating contextual features in VPS is harder,
because that requires prediction of those features for future times (see (8),(10), and (11)), introducing
more sources of error. Another crucial reason for this algorithmic choice is speed. Our method is
designed to be production-ready, which means that the online planning component has to meet strict
latency requirements. An action-value network allows for much faster decision-time planning than
VPS because no tree search is required and batch inference is possible. For a concrete comparison,
the QPS (queries-per-second) of a single action-value network is about six times higher than that of
VPS, which is a difference of go or no-go in the case of managing a large ﬂeet.

Speciﬁcally, we use deep SARSA to learn the state-action value function using a similar training
framework as CVNet [32]. The structure of the Q-network consists of three blocks of layers: the
embedding layer, the attention layer, and the output layer. The architecture is illustrated in Figure 4.
We use the same state representation for spatial information through hierarchical sparse coding and
cerebellar embedding as in CVNet. The destination of the action selected by the policy is determined

10

among a set of predeﬁned pick-up points, and the policy is also supplemented by Long Search as
discussed in Section 3.2.1.

It is easy to see that vanilla deep SARSA in this case is indeed just VPS with an expansion depth of 1.
Expanding Q(s, o) and assuming a stochastic policy π, we have

Qπ(s, o) = Eo(cid:48)∼π(·|s(cid:48)) [r(s, o) + γQπ(s(cid:48), o(cid:48))]

= r + γV π(s(cid:48)),

(13)

(14)

where s(cid:48) is the next state after s with the option o. Relating to (8), the connection is now established.
Albeit its simpler form, deep SARSA allows for a more natural incorporation of several highly
desirable algorithmic elements as described below.

4.1 Supply-demand Context

To enable the policy of the vehicles to better induce coordination and to be more adaptive to the
dynamic nature of the environment, we augment the state space with additional supply-demand
(SD) contextual features in the neighborhood of the agent. Typical SD features are the numbers of
open orders and idle drivers to be matched. These help characterize the state of the vehicle and its
surrounding environment more accurately, allowing for better state representation and responsiveness
to changes in the environment. For this purpose, we include the SD features of six neighboring
hexagon grid cells as well as that of the current cell. These quantities are local and are speciﬁc to
each hex cell. The SD features of the vehicle?s state includes SD quantities of seven hex cells, six
neighboring cells and the current cell. For each cell, the vector representation of the SD features
include real-time numerical values such as number of idle drivers, number of total requests, and
number of orders that have not been assigned yet within the geo-boundary of the cell. Those
quantities are aggregate values computed over a 10-min sliding window. This structure design is
intuitive because the action space of the agent covers destinations in the same set of seven grid cells.
Speciﬁcally, we adopt the general form of global attention mechanism proposed in [17]. As illustrated
in Figure 4, the attention layer assigns scores to each pair of SD features (current cell and one of its
neighbor cell)

αi = softmax(sd(cid:62)

0 Wαsdi)

(15)
where i ∈ Z, i = [1..6], and Wα is a trainable weight matrix in the attention layer. The scores are then
used to re-weight the neighboring SD vectors, obtaining a dense and robust context representation
vector. The motivation of constructing such an attention module in our learning network is to cast
more weights into nearby grids possessing better supply-demand ratio than the current cell. The
mechanism utilizes real-time contextual features to model the relationships between spatial regions
according to the change of the dynamic. For nearby cells sharing similar spatial and static features,
more attention would be given to the action destination with abundant ride requests.

4.2 Stochastic Policy

With the deterministic policy in Section 3, vehicles in the same state will be repositioned to exactly
the same destination. When the algorithmically controlled ﬂeet is small compared to the general
vehicle pool (and assuming that the size of the environment is inline with that of the general pool),
the vehicles in the ﬂeet can be essentially treated independently because the probability of multiple
vehicles being in the same state is small. As the size of the ﬂeet increases, it happens more often
that the vehicles would come across each other, and the effect of the ‘over-reaction’ phenomenon
becomes more severe. To mitigate this undesirable effect, we randomize the reposition action output
by adding a softmax layer to obtain the Boltzmann distribution

σ(q)k =

exp(qk)
j exp(qj)

(cid:80)

, ∀k ∈ K,

(16)

where q represents the vector of reposition action-values output, and K is the set of eligible desti-
nations. While this is a common approach in reinforcement learning to convert action-values into
action probabilities for a stochastic policy, we argue that it is particularly appealing in the case of
vehicle repositioning: First, the sign of the action values would not be a concern. As we will see
in Section 4.3, sometimes the action values are penalized to account for the current SD condition,
making negative resulting values possible. Second, we dispatch the vehicles following the action

11

Figure 4: The Q-network architecture for SARSA.

distribution σ(q), so when there are multiple idle vehicles in the same area at a given time, we
basically dispatch them in proportion to the exponentiated values, which aligns with intuition. It is
easy to see that this transformation can be applied to VPS in general to derive a stochastic policy. In
Section 5.3 of the simulation experiments, we demonstrate the importance of stochastic policy in
achieving better performance when managing a large ﬂeet.

4.3 Decision-time SD Regularization

The semi-MDP formulation in Section 2.1 is from a single vehicle’s perspective, and the action value
does not take into account the supply-demand gap at the destination explicitly (but through part of the
state features). To explicitly regularize the action values, and hence the action distribution, so that the
system generates SD-aware repositioning actions, we penalize the action values by their respective
destination SD gaps in a linear form,

q(cid:48)
k := qk + αgk, ∀k ∈ K,

(17)

where q(cid:48)
k is the penalized version of the Q-value qk in (16), and gk is the SD gap in the destination
cell k. Considering the construction of a stochastic policy through the Boltzmann distribution in (16),
the SD-gap penalty is in fact multiplicative on the action distribution. We have empirically observed
that it helps to truncate the penalty by setting a threshold on the SD gap, i.e.,

q(cid:48)
k := qk + αgk1(gk>β), ∀k ∈ K,

(18)

where β is the threshold parameter for SD gaps and is usually city-speciﬁc.

To better understand what we are doing here, it helps to relate to an assignment formulation which
assigns idle vehicles from every grid cell at the decision time to destination cells that are under-
supplied while maximizing some utility function, e.g., the action-value function. SD regularization
with Boltzmann action distribution can be thought as soft-constrained version of the above decision-
time planning. A major advantage over the assignment formulation is that it is generally less sensitive
to perturbation in the input SD data, which is dynamic and prone to prediction errors.

5 Simulation Experiments

We describe our simulation experiments in this section and discuss empirical results and observations.
We have developed a realistic simulation environment for evaluating vehicle repositioning policies

12

based on prior works on order dispatching for ride-hailing platforms [32, 38]. This environment
captures all the essential elements of a large-scale ride-hailing system. We emphasize that our
proposed algorithms do not require the simulation environment for training. For realistic modeling of
the real-world MoD system, we have augmented the multi-driver simulator in [32] with individual
vehicle repositioning capability. In this environment, one can specify a given number of vehicles
to follow a particular repositioning policy πr. We set this number to ten for the experiments in
Sections 5.1 and 5.2, and it is set to 1600 for Section 5.3 and 3200 for Sections 5.4 and 5.5. A
ﬁxed order dispatching policy πd assigns trip orders to the drivers in batch windows, and minimizes
the within-batch total pick-up distance. In addition, order dispatching can interrupt a repositioning
action. The same evaluation environment framework has been deployed for hosting KDD Cup 2020
RL Track [25]. More details on the simulation environment and the data sets used can be found in
Appendix C.

The experiments are divided into two types, individual-oriented and group-oriented, for testing the
performance of VPS and deep SARSA using individual- and group-level metrics respectively. As
deﬁned in Section 2.1, the individual-level IPH for a driver x and the group-level IPH for a driver
group X are (2) and (3) respectively. For the purpose of the experiments below, both the income and
online hours are accumulated over the course of the speciﬁc tests. For all the experiments, We report
group-level IPH. In the small-ﬂeet cases, where all the managed vehicles’ online hours are the same
in the simulations, the group-level IPH is equivalent to the mean of the individual-level IPHs.

5.1 Expansion Depth of VPS

We benchmark implementations of VPS with expansion depth from 1 (Greedy) to 5 through simulation
with data from a different city than the three cities in Section 5.2. Each method was tested using 10
random seeds, and we report both mean and standard deviation of the income rates (IPH) computed
over three different days. To understand the effect of the expansion depth in VPS, we see that it
controls the extent to which we leverage the estimated environment model information to compute
the approximate action-values (see Figure 3). The hope is that by exploiting some model information
of the MDP, we would be able to learn better action-values than a completely data-driven model-free
method. On the other hand, the errors in model estimation, i.e., dispatch probability model, are
compounded quickly as the expansion depth increases. So a deeper expansion will likely hurt the
quality of the learned action-values. Empirically, this is indeed observed. From 5a, we see that all the
expansion depths greater than 1 yield higher average IPH than an expansion depth of 1, demonstrating
that the policy search with model information does help improve the policy. In particular, the depths
of 2 and 3 lead to more than 10% higher average IPH than the greedy method, and they also perform
signiﬁcantly better than the depths of 4 and 5, where the error in model estimation has apparently
kicked in. Figure 5b shows the rapid growth in CPU time per simulation with respect to the expansion
depth. The computation corresponding to a depth of more than 3 is unacceptably expensive. The
mean IPH’s for the depth of 2 and 3 are basically the same, but the standard deviation for the depth of
2 is noticeably smaller than that of 3, and the policy search with a depth of 2 is also faster. According
to Figure 5, we have chosen an expansion depth of 2 for both the simulation and the real-world
experiments based on average performance, robustness, and computational efﬁciency. It should be
stressed that the expansion depth here is a different concept from the optimization horizon of the
MDP. Regardless the value of the expansion depth, the state values that we use for bootstrapping
always have the same horizon, i.e., inﬁnite horizon.

5.2 Benchmarking VPS

We benchmarked our algorithm in the simulation environment generated by ride-hailing trip data
from three mid-sized cities, which we denote by A, B, and C. Table 3 shows the basic background
information of the cities. Ten drivers are selected for each experiment out of hundreds of drivers in
the environment. We chose three different days, and for each city and each day, we use ﬁve random
seeds, which give different initial states for the experiment. We compare four algorithms: Random:
This policy randomly selects a grid cell from the six neighbouring cells and the current location as
the repositioning destination. Greedy: This is the implementation of (8), which is one-step policy
improvement of the generalized policy iterations. MAB: This is a multi-arm bandits algorithm adapted
from [7], with 7 arms corresponding to the 7 possible actions and an arm’s value being the average
return for repositioning from the current location to the destination speciﬁed by the arm. The policy
is generated by UCB1 [1] with one day of pretraining. VPS: This is the value-based policy search

13

(a) IPH v.s. expansion depth. All the results have been
standardized w.r.t. that of greedy, i.e., depth=1.

(b) CPU time per simulation (standardized w.r.t.
depth=1) v.s. expansion depth

Figure 5: VPS expansion depth selection (10 drivers).

algorithm described in Section 3.2. An expansion depth of two is chosen by benchmarking different
expansion depths as in Figure 5.

We computed the average income rate across three days for each random seed. We then compare for
the three methods the mean and standard deviation of the ﬁve random seeds results. We can see from
Figure 6 that both VPS and Greedy methods consistently outperform Random and MAB across the
three cities. Moreover, VPS yields signiﬁcantly higher income rates than Greedy, with 12%, 6%, and
15% improvement in the mean for each of the three cities respectively. The comparison between VPS,
Greedy and MAB shows the advantage of a longer horizon in the setting of a long-term objective.

5.3 Stochastic Policy

To demonstrate the necessity of a stochastic policy for managing a large ﬂeet, we create a new version
of VPS with stochastic policy, VPS-stoch, by applying the softmax function to the approximate action
values ˆQ∗(s0, o) as in Section 4.2. Both VPS and VPS-stoch have the same expansion depth and
other parameters. The evaluation environment is generated using data from city K, which is one of the
cities for the large-ﬂeet real-world deployment. We set the number of algorithmically-guided vehicles
in the environment to 1600. Figure 7 compares the performance in IPH of VPS and VPS-stoch. It is
clear that VPS-stoch consistently outperforms VPS with a deterministic policy by about 10% across
different days in the large-ﬂeet setting. We expect the performance comparison between deep SARSA
with greedy policy and the one in Section 4 to be similar.

14

Figure 6: Simulation experiment results comparing income rates of Random, MAB, Greedy (Baseline)
and VPS policy using the multi-driver simulation environment. Results are normalized with respect
to the mean Random.

Figure 7: IPH comparison between VPS with stochastic and deterministic policies for City K. Results
are standardized with respect to the numbers for the deterministic policy.

5.4 SD Contextual State Features

We carry out ablation study to justify the use of SD contextual state features in deep SARSA. To
make sure that the contextual features in the training data aligns with the simulation environment,
we run a deep SARSA agent trained on historical data in the simulation environment to collect six
weeks of training data. We then train two new deep SARSA agents with and without the contextual
features and evaluate them on a set of different dates. In Figure 9, we present the comparison results
for the time interval of 3-5pm, which we have found most signiﬁcant within the experiment period
(11am - 7pm). We surmise that such an observation is because the SD conditions during lunch hours
and evening rush hours are relatively stable, whereas that during the off-peak hours is more volatile,
making the SD contextual features more useful there.

5.5 SD Regularization

To demonstrate the effect of SD regularization on the action values, we present ablation study results
comparing deep SARSA with and without SD regularization at decision time. As one could see from
ﬁgure 9, the IPH of SARSA with SD regularization is signiﬁcantly higer than SARSA without SD

15

Figure 8: IPH comparison between the versions of SARSA with and without SD features. Results are
standardized with respect to the numbers for SARSA without SD features. The results are for the
time interval 3-5pm over three different days.

Figure 9: IPH comparison between the versions of SARSA with and without SD regularization.
Results are standardized with respect to the numbers for SARSA without SD regularization. The
results are for the ﬁrst hour of the experiment period with all the drivers online (no new drivers getting
online or existing drivers getting ofﬂine) in the environment.

regularization across three different days, verifying that it is important to consider the locations of
other idle vehicles for repositioning a large ﬂeet.

We also present in Figure 10 the results from using different combinations of the SD regularization
parameters, i.e., the SD-gap threshold β and the penalty parameter α. Figure 10 plots the IPH values
resulted from three random seeds for each combination of the parameters (gap and alpha) on the
x-axis. The data points with the same color correspond to the same parameter combination. This plot
shows that the SD regularization parameters have signiﬁcant impact on the IPH performance, with
the empirically best combination at β = 17 and α = 0.2. Within the respective appropriate range,
the general observation is that larger penalty and threshold are preferred possibly because smaller
thresholds tend to introduce noise from the dynamic changes in the SD context and smaller penalties
diminish the effect of regularization.

5.6 Benchmarking with Baseline

In this section, we compare SARSA (with all the algorithmic elements presented in Section 4) to the
MAB algorithm discussed in Section 5.2. VPS (with or without stochastic policies) has a much higher
run-time latency than these two methods as we explained in Section 4. Since it does not meet the

16

Figure 10: IPH comparison between different parameters for SD regularization (SD gap threshold
and penalty parameter α in Section 4.3). The data points are the IPH values resulted from three
random seeds for each combination of the parameters on the x-axis. The data points with the same
color correspond to the same parameter combination.

Figure 11: IPH comparison between SARSA with SD regularization and MAB. Results are standard-
ized with respect to the average driver IPH for the MAB.

17

Figure 12: Individual driver IPH distribution comparison between SARSA with SD regularization
and MAB. Results are standardized with respect to the average individual driver IPH using MAB.

production latency requirement in the large-ﬂeet scenario, we have excluded it from the comparison.
Figure 11 shows that the average IPH of SARSA is over 5% higher than that of MAB across the
different days. In Figure 12, we look at the IPH distributions of the two algorithms. The group with
SARSA shows a more Gaussian distribution, whereas the distribution for the MAB group is skewed
towards the lower side, partly explaining the difference in the means.

6 Real-world Deployment

We have developed an AI Driver Assistant application powered by our proposed repositioning
algorithms. We designed and ran a ﬁeld experiment program on DiDi to evaluate the performance in
the real-world environment. The goal is to compare VPS with human expert repositioning strategies,
which is signiﬁcantly more challenging than the simulation baselines. Human drivers have obtained
the domain knowledge through experience and the real-time demand information provided by the
app.

Unlike operating on an autonomous ride-hailing platform, testing a vehicle repositioning algorithm
with human drivers in our case requires additional consideration of the way the repositioning rec-
ommendations are delivered and the drivers’ willingness to accept and follow the recommendations,
because the drivers within the experiment program were on a voluntary basis in terms of executing
the repositioning tasks, due to various practical constraints.

Since our experiment’s goal is to benchmark algorithms on long-term cumulative metrics (daily
income rate) and the supply-demand context could vary signiﬁcantly within a day, it would be ideal if
all the drivers in the program are online for the same period time which is also sufﬁciently long, and
the drivers always follow the repositioning recommendations, for a fair comparison. We designed an
incentive scheme to encourage the drivers to participate as closely to the ideal situation as possible.
Speciﬁcally, we required that they are online for at least ﬁve hours out of the eight hours from 11am
to 7pm (the experiment interval) during weekdays and that they skip or fail to complete no more
than three tasks each day. The drivers were rewarded for each repositioning task that they ﬁnished,
and they received additional reward for each day that they met the daily requirements. Income rates
are computed using the data from the drivers who have met the daily requirements. The form and
level of the incentives certainly would inﬂuence the drivers’ willingness to follow the repositioning
tasks, and the associated elasticity apparently depends on the speciﬁc driver and task. We leave the
investigation on the optimal incentive design in this context for future works.

18

Figure 13: Illustrative screenshots of the AI Driver Assistant program. The screenshot on the left
shows that the driver receives a repositioning task. The middle one shows the GPS navigation. When
the driver completes the task, the app displays a conﬁrmation (right).

6.1 AI Driver Assistant

Repositioning recommendations are delivered through pop-up message cards within the mobile AI
driver assistant app. Once repositioning is triggered, a message card appears at the target driver’s
app. The message card contains instructions for the repositioning task, including the destination and
the target time that the driver is required to be there. After the driver acknowledges the task, GPS
navigation is launched to provide turn-by-turn route guidance to the driver. The system automatically
determines if the driver has reached the prescribed destination within the required time frame. Figure
13 shows a few illustrative screen shots of the app. This app is integrated into the driver-side of the
rideshare application and can be activated/deactivated on an individual basis.

6.2 Small Fleet

We recruited 61 experienced drivers across the same three cities as in the simulation experiments
to participate in our experiment, which lasted for two weeks. The drivers were randomly divided
into two groups of similar sizes. The algo group received repositioning recommendations from our
proposed algorithm. The human group did not receive any recommendation, and they were free to
determine their own cruising plan while idle. Order dispatching was handled by the platform and was
exogenous to both groups.

First, we look at the income rates across the three cities. We have found that the advantage of
algorithmic repositioning appears most signiﬁcant for the group of regular drivers without preferential
order matching.5 Figure 14 shows daily income rate comparison between the algo group and the
human group by city. Cities B and C both show signiﬁcant improvement in income rate, with a
difference of 6.1% and 13.1% in the mean respectively. There is also a relative improvement of more
than 3% in the median for both cities. The improvement in city A is smaller than the other two cities
because its demand is more abundant with respect to the supply (see Table 3 in Appendix D), leading
to less advantage for a repositioning algorithm - the drivers are less likely to idle and hence there is
less room for improvement. Over the entire group of drivers in the program, the relative improvement
of the algo group over the human group is up to 2.6% across the three cities. We surmise that regular
drivers without preferential order matching were more likely to be in need of idle-time cruising
guidance as they have a higher chance of becoming idle during the day, thus offering larger room for
improvement through algorithmic repositioning.

Since our participation requirements to the drivers were voluntary and incentive-based, all the drivers
might not have accepted or completed all the repositioning tasks that our algorithm issued. We
analyzed the algo group data with more than ﬁve hours of daily online time and compared daily
income rates of the drivers who followed repositioning instructions closely to those of whom failed.
We have found that those drivers following the algorithmic repositioning faithfully enjoy an average
income rate 9% (4% for median resp.) higher than those not, corroborating the effectiveness of our
algorithm.

5Preferential order matching is typically applied to drivers with high service scores as incentives.

19

Figure 14: Daily income rate comparison for regular drivers without preferential order matching in
the real-world experiment, normalized w.r.t. the means of the human group.

6.3 Large Fleet

To evaluate the performance of our algorithm on larger driver groups, we recruited about 1200 drivers
(300 each for cities K and F, and 600 for city M) from three cities on the DiDi platform to participate
in our pilot programs. We have shown in Section 5.3 that coordination can no longer be ignored for a
driver group of this size. Hence, we used the learning method for action values developed in Section
4.

Different from the experiment for small driver groups, the drivers recruited in this case were to
form the experiment group only. We selected the control pool directly from the general driver
population. The control pool is constructed carefully so that the distributions of several key features,
e.g., service scores, daily average online duration, are identical to those of the experiment group. The
differences in IPH and utilization between the experiment group and control pool are also statistically
insigniﬁcant. 6

The potential network effect between the experiment and control groups is mitigated in two ways.
First, despite a much larger algorithmically managed ﬂeet, the experiment group still accounts for a
minor portion of the total vehicle population (about 1% for cities K and F, and about 11% for city M).
Hence, the interaction of the two groups of vehicles is likely limited. Second, the evaluation method
that we adopt (described in the next section) considers the performance of a large number of random
group samples from the control pool, which are diverse in their degree of potential network effect
with the experiment group depending on their spatiotemporal states. Our comparison is done against
the distribution of performances over these samples.

6.3.1 Evaluation Methodology

Unlike the experiment for small driver groups, where we compute individual driver performance and
compare the distributions of the two groups, here the goal of the experiment is to compare group
level performance. It should be noted that it is incorrect to determine the statistical signiﬁcance of the
results by computing the within-group performance variability because it is affected by the different
individuals’ characteristic proﬁles, whereas in comparing group-level performance, we care about
the variability due to different samples of groups. In our case, we perform the signiﬁcance test by
bootstrapping. We treat the experiment (‘algo’) group as one sample X, which gives the group income
rate p(X) as in (3). Then, we sample with replacement N control groups of size |X| from a control
pool whose size is several times of |X|. We denote each control group sample Y (n), n = 1, · · · , N .
Then, we compute the 95% conﬁdence interval of the control group performance CY based on the set
{p(Y (n))}N
n=1 and conclude that the performance difference is signiﬁcant if p(X) falls outside CY .
We tested different values of N and have selected N = 5000 when the conﬁdence interval converges.

6.3.2 Results

We present the aggregated performance of the experiment group drivers by comparing it to that of a
large number of sampled control groups from the general driver population as detailed in Section

6For the experiment in city M, the control group was randomly selected from the drivers who showed interest

to participate and were simply kept as an observation group during the experiment.

20

6.3.1. We report results on group-level IPH (3) as well as utilization, which is deﬁned as

u(X) :=

(cid:80)
(cid:80)

x∈X s(x)
x∈X h(x)

,

(19)

where s(x) is the total time in service for a driver x over the course of the experiment. Utilization
is the complement of idle time ratio. It is another important measure to quantify the efﬁciency of a
ride-hailing platform.

The improvement in aggregated IPH and utilization for both cities is quite impressive as shown in
Figure 15, with 3.1% and 6.8% increase in IPH for city K and F respectively, and 3.5 and 3.8 percent
points increase in utilization respectively. These results are all statistically signiﬁcant through the
bootstrap sampling tests discussed in Section 6.3.1.

We further analyzed the performance comparison by dividing the drivers into service score buckets.
Drivers typically receive different levels of preferential matching according to their service scores,
so it is an important factor contributing to the income variations within the experiment and control
groups. Figure 16 shows the bar plots of IPH by service score buckets, which are arranged in
increasing order of the score from the left to the right. We observe that except for two high score
buckets, the experiment group consistently outperformed the control groups across all the buckets.

Although our primary metrics in the large-ﬂeet experiments are at group level, we also compare
individual-level IPH’s in terms of their distributions for the two groups. Figure 17 shows the individual
IPH distributions in the form of histograms over discretized and equally spaced IPH buckets, arranged
in an increasing order. The results are apparently consistent with those of the group-level metrics.
For each city, we see that there are generally more algo group drivers in the higher IPH buckets and
fewer algo group drivers in the lower buckets than their counterparts from the control group. The
difference in the IPH distribution is larger for cities F and M, which also echos with the group-level
results in Figure 15. Another observation is that both groups have their IPH values distributed over
multiple buckets, exhibiting in-group variations and thus empirically justifying the evaluation method
in Section 6.3.1.

6.4 Long Search

From the experiment data, we have observed that more than 80% of the time, the experiment group
drivers do not need more than two consecutive repositioning actions before getting matched to an
order. This is echoed by further observation from analysis of the driver trajectories that long-search
was rarely triggered during the experiment, implying that the drivers rarely went a long time without
being dispatched to an order. This demonstrates that our general search algorithm could handle most
of the cases successfully.

However, we do have cases when long-search has been particularly useful, as shown in Figure 18.
The driver was brought by a long-distance order to the outside of the city in Figure 18a. In this case,
the driver self-repositioned once after ﬁnishing the long order without getting matched to another
order.

After two regular search steps (labels 3 and 4), long-search (label 5) was triggered and the search
algorithm managed to ﬁnd the right direction to reposition the driver right back to the city center.

In Figure 18b, the driver was at a rural area far from a town at the bottom left corner of the map. Two
long-search repositions were triggered and guided the driver successfully to the town center, where
consecutive orders were matched to the driver. (See inset at the left.) In both cases above, long-search
prevented the drivers from being stuck in large low-value areas with targeted destinations of higher
earning prospect, thus helping to improve their overall income efﬁciency.

A participants survey was conducted after the experiment program and the feedback showed that the
overwhelming majority of the drivers rated the program positively and would like to participate again.

7 Discussion

In this paper, we have considered two paradigms of vehicle repositioning, small and large ﬂeets,
which we have observed distinct enough to demand different algorithm designs. We focus on
individual performance for small-ﬂeet problems, where the vehicles under algorithmic guidance are

21

Figure 15: Income efﬁciency and utilization comparison for the large-ﬂeet real-world experiment.
All ﬁgures are standardized with respect to those of the control group. The 95% conﬁdence intervals
for the control group are also plotted.

essentially independent agents and their actions do not have signiﬁcant impact on the supply-demand
context. For large-ﬂeet problems, we care about group level performance, and hence, coordination
among the agents’ actions becomes important. In this case, the SD information not only allows for a
better characterization of the state, more responsive to the changes in environment, but also induces
awareness of the neighboring agents in the policy at planning stage.

Our approach does not explicitly depend on the type of MoD, e.g., single occupancy, carpooling. Our
vehicle repositioning agent is equally applicable to an MoD platform with the presence of carpooling
and other modes of ridesharing, as long as appropriate transition experience data is available. In
fact, in our real-world deployment, the drivers are free to accept carpooling orders in addition to the
regular ride-hailing trips. To speciﬁcally optimize for the carpool mode, more state features (e.g.,
number of passengers on board, the destination for each passenger) can be added to the Q-value
network as done in [?].

We have assumed that the reposition tasks are always executed by the drivers. In practice, this happens
only up to an acceptance probability pa. There are a number of reasons for which the driver would
reject the task, e.g., perceived travel distance, the need for a break. It is reasonable to think that pa
would depend on both the driver’s state and the reposition task, as well as some environment factors,
e.g., weather, trafﬁc. One possible simple way to account for pa in our algorithm is to multiply the
estimate of the probabilities to the action-value vector q in (16) so that the learned values are properly
discounted by the corresponding chances of task rejection. Nevertheless, developing a prediction
model that accurately learns pa remains a challenging task, and it requires more in-depth treatment
for handling the resulting uncertainty.

Our current approach comprises an ofﬂine learning stage and an online planning stage. The value
networks are ﬁxed at run-time. For a highly dynamic environment like ride-hailing, it is natural to
hope that online on-policy updates would make the value function more adaptive, thus leading to
better performance. We leave the investigation in this direction to future works.

22

Figure 16: Income efﬁciency comparison by service score buckets. The value intervals of the buckets
are mutually exclusive and cover the entire range of service scores in each city.

Acknowledgments

The authors would like to thank Jing He, Jing Chen, Yuanfang Wang, Shaohui Su, Chunyang Liu,
Zhe Xu, Ben Wang, Bin Liu, Chen Sang, Chengyi Shi, the DiDi regional operations teams, and other
sister engineering teams for their support and efforts in product, platform, and operations for the
experiment program of this research.

References

[1] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine learning, 47(2-3):235–256, 2002.

[2] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. Neural combinatorial optimization

with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

[3] D. Bertsimas, P. Jaillet, and S. Martin. Online vehicle routing: The edge of optimization in

large-scale applications. Operations Research, 67(1):143–162, 2019.
Stuck

The Ride-Hail Utopia That Got

[4] E. Brown.

2020
the-ride-hail-utopia-that-got-stuck-in-traffic-11581742802.

(accessed December

2020).

4,

- WSJ,
https://www.wsj.com/articles/

in Trafﬁc

[5] S.-F. CHENG, S. S. JHA, and R. RAJENDRAM. Taxis strike back: A ﬁeld trial of the
driver guidance system. In AAMAS’18: Proceedings of the 17th International Conference on

23

Figure 17: Individual-level IPH distribution comparison by IPH buckets. The value intervals of the
buckets are mutually exclusive and cover the entire range of IPH in each city.

(a)

(b)

Figure 18: Examples of using long-search to reposition a driver from large low demand area to a
higher value region. Green arrow: order dispatching. Purple arrow: self-reposition. Red arrow:
algorithmic reposition.

24

Autonomous Agents and Multiagent Systems, Stockholm, July 10, volume 15, pages 577–584,
2018.

[6] T. Choi. Uber and Lyft to turn the wheels on car ownership: industry experts, 2020 (ac-
cessed December 4, 2020). https://www.reuters.com/article/us-autos-ownership/
uber-and-lyft-to-turn-the-wheels-on-car-ownership-industry-experts-idUSKCN1SS33A.

[7] N. Garg and S. Ranu. Route recommendations for idle taxi drivers: Find me the shortest route to
a customer! In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pages 1425–1434, 2018.

[8] M. Goncharova.

Ride-Hailing Drivers Are Slaves

2017 (ac-
https://www.nytimes.com/2017/01/12/nyregion/

to the Surge,

cessed December 4, 2020).
uber-lyft-juno-ride-hailing.html.

[9] T. C. Hales. The honeycomb conjecture. Discrete & Computational Geometry, 25(1):1–22,

2001.

[10] J. Holler, R. Vuorio, Z. Qin, X. Tang, Y. Jiao, T. Jin, S. Singh, C. Wang, and J. Ye. Deep
reinforcement learning for multi-driver vehicle dispatching and repositioning problem. In
J. Wang, K. Shim, and X. Wu, editors, 2019 IEEE International Conference on Data Mining
(ICDM), pages 1090–1095. Institute of Electrical and Electronics Engineers, Washington, DC,
2019.

[11] R. Iglesias, F. Rossi, K. Wang, D. Hallac, J. Leskovec, and M. Pavone. Data-driven model
predictive control of autonomous mobility-on-demand systems. In 2018 IEEE International
Conference on Robotics and Automation (ICRA), pages 1–7. IEEE, 2018.

[12] Y. Jiao, X. Tang, Z. T. Qin, S. Li, F. Zhang, H. Zhu, and J. Ye. A deep value-based policy search
approach for real-world vehicle repositioning on mobility-on-demand platforms. In NeurIPS
2020 Deep Reinforcement Learning Workshop, 2020.

[13] J. Jin, M. Zhou, W. Zhang, M. Li, Z. Guo, Z. Qin, Y. Jiao, X. Tang, C. Wang, J. Wang, et al.
Coride: Joint order dispatching and ﬂeet management for multi-scale ride-hailing platforms.
In Proceedings of the 28th ACM International Conference on Information and Knowledge
Management, pages 1983–1992, 2019.

[14] M. Li, Z. Qin, Y. Jiao, Y. Yang, Z. Gong, J. Wang, C. Wang, G. Wu, and J. Ye. Efﬁcient
ridesharing order dispatching with mean ﬁeld multi-agent reinforcement learning. In To appear
in Proceedings of the 2019 World Wide Web Conference on World Wide Web. International
World Wide Web Conferences Steering Committee, 2019.

[15] K. Lin, R. Zhao, Z. Xu, and J. Zhou. Efﬁcient large-scale ﬂeet management via multi-agent deep
reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pages 1774–1783, 2018.

[16] M. Lowalekar, P. Varakantham, and P. Jaillet. Online spatio-temporal matching in stochastic

and dynamic domains. Artiﬁcial Intelligence, 261:71–112, 2018.

[17] M.-T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural

machine translation, 2015.

[18] F. Miao, S. Han, S. Lin, J. A. Stankovic, D. Zhang, S. Munir, H. Huang, T. He, and G. J. Pappas.
Taxi dispatch with real-time sensing data in metropolitan areas: A receding horizon control
approach. IEEE Transactions on Automation Science and Engineering, 13(2):463–478, 2016.

[19] J. Miller and J. P. How. Predictive positioning and quality of service ridesharing for campus mo-
bility on demand systems. In 2017 IEEE International Conference on Robotics and Automation
(ICRA), pages 1402–1408. IEEE, 2017.

[20] . National Bureau of Statistics of China. National economic performance maintained within
an appropriate range in 2018 with main development goals achieved. National Bureau of
Statistics of China, 2019 Accessed May 16, 2020. http://www.stats.gov.cn/english/
PressRelease/201901/t20190121_1645832.html.

[21] M. Nazari, A. Oroojlooy, L. Snyder, and M. Takác. Reinforcement learning for solving the
vehicle routing problem. In Advances in Neural Information Processing Systems, pages 9839–
9849, 2018.

25

[22] T. Oda and C. Joe-Wong. Movi: A model-free approach to dynamic ﬂeet management. In IEEE
INFOCOM 2018-IEEE Conference on Computer Communications, pages 2708–2716. IEEE,
2018.

[23] E. Özkan and A. R. Ward. Dynamic matching for real-time ride sharing. Stochastic Systems,

10(1):29–70, 2020.

[24] S. D. Pendleton, H. Andersen, X. Du, X. Shen, M. Meghjani, Y. H. Eng, D. Rus, and M. H. Ang.
Perception, planning, control, and coordination for autonomous vehicles. Machines, 5(1):6,
2017.

[25] Z. T. Qin, X. Tang, L. Zhang, F. Zhang, C. Zhang, J. Zhang, and Y. Ma. KDD Cup 2020
Reinforcement Learning Track, 2020 (accessed December 4, 2020). https://www.kdd.org/
kdd2020/kdd-cup.

[26] S. Shah, M. Lowalekar, and P. Varakantham. Neural approximate dynamic programming for
on-demand ride-pooling. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 34, pages 507–515, 2020.

[27] Z. Shou, X. Di, J. Ye, H. Zhu, H. Zhang, and R. Hampshire. Optimal passenger-seeking policies
on e-hailing platforms using markov decision process and imitation learning. Transportation
Research Part C: Emerging Technologies, 111 (February):91–113, February 2020.

[28] H. P. Simao, J. Day, A. P. George, T. Gifford, J. Nienow, and W. B. Powell. An approxi-
mate dynamic programming algorithm for large-scale ﬂeet management: A case application.
Transportation Science, 43(2):178–197, 2009.

[29] M. Smith.

Here’s how long you have to wait

2019 (accessed December 4, 2020).
how-long-you-have-to-wait-for-an-uber-or-lyft-in-d-c/.

in DC,
https://wtop.com/dc-transit/2019/12/

for an Uber or Lyft

[30] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
[31] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal
abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–211, 1999.

[32] X. Tang, Z. Qin, F. Zhang, Z. Wang, Z. Xu, Y. Ma, H. Zhu, and J. Ye. A deep value-network
based approach for multi-driver order dispatching. In Proceedings of the 25th ACM SIGKDD
international conference on knowledge discovery & data mining, pages 1780–1790, 2019.
[33] . University of Michigan Center for Sustainable Studies. Us cities factsheet. Center for
Sustainable System, University of Michigan, 2019 Accessed April 21, 2020. http://css.
umich.edu/sites/default/files/US%20Cities_CSS09-06_e2019.pdf.

[34] H. van Seijen, H. van Hasselt, S. Whiteson, and M. Wiering. A theoretical and empirical
analysis of expected sarsa. In 2009 IEEE Symposium on Adaptive Dynamic Programming and
Reinforcement Learning, pages 177–184, March 2009.

[35] T. Verma, P. Varakantham, S. Kraus, and H. C. Lau. Augmenting decisions of taxi drivers
In Twenty-Seventh International

through reinforcement learning for improving revenues.
Conference on Automated Planning and Scheduling, 2017.

[36] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In Advances in Neural Information

Processing Systems, pages 2692–2700, 2015.

[37] Z. Wang, Z. Qin, X. Tang, J. Ye, and H. Zhu. Deep reinforcement learning with knowledge
transfer for online rides order dispatching. In International Conference on Data Mining. IEEE,
2018.

[38] Z. Xu, Z. Li, Q. Guan, D. Zhang, Q. Li, J. Nan, C. Liu, W. Bian, and J. Ye. Large-scale
order dispatch in on-demand ride-hailing platforms: A learning and planning approach. In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pages 905–913. ACM, 2018.

[39] C. Yan, H. Zhu, N. Korolko, and D. Woodard. Dynamic pricing and matching in ride-hailing

platforms. Naval Research Logistics, 2019.

[40] L. Zhang, T. Hu, Y. Min, G. Wu, J. Zhang, P. Feng, P. Gong, and J. Ye. A taxi order dispatch
model based on combinatorial optimization. In Proceedings of the 23rd ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data Mining, pages 2151–2159. ACM,
2017.

26

[41] R. Zhang, F. Rossi, and M. Pavone. Model predictive control of autonomous mobility-on-
demand systems. In 2016 IEEE International Conference on Robotics and Automation (ICRA),
pages 1382–1389. IEEE, 2016.

A The DPE Algorithm

For the purpose of implementation, we introduce a new binary option b in place of o under the same
semi-MDP framework, with 0 indicating idle and 1 for dispatched. V (s|dispatch) and V (s|idle)
can then be represented as V (s|b = 1) and V (s|b = 0), respectively. The required conditional value
function V (s|od) is given by V (s|b = 1). The main procedure of DPE is presented in Algorithm
1. We use the same discount factor γ = 0.92 in DPE (for computing the state values) and in VPS
(for computing the path values). For the cerebellar embedding we use 3 quantization functions and a
memory size A of 20000. The embedding dimension m is chosen to be 50. Following the cerebellar
embedding layer are fully connected layers having [32, 128, 32] hidden units with ReLU activation.
To evaluate the policy we apply Adam optimizer with a constant step size 3e−4 and the Lipschitz
regularization parameter λ as 1e−4.

Algorithm 1 Dual Policy Evaluation (DPE) with Cerebellar Value Network (CVNet)

1: Given: historical driver trajectories {(si,0, oi,0, ri,1, si,1, oi,1, ri,2, ..., ri,Ti , si,Ti)}i∈H collected

by executing a (unknown) policy π in the environment.

2: Given: n cerebellar quantization functions {q1, ..., qn}, regularization parameter, max iterations,
embedding memory size, embedding dimension, memory mapping function, discount factor
λ, N, A, m, g(·), γ.

3: Compute training data from the driver trajectories as a set of (state, reward, next state) tuples,

e.g., {(si,t, Ri,t, si,t+ki,t)}i∈H,t=0,...,Ti where ki,t is the duration of the trip.

4: Initialize the embedding weights θM .
5: Initialize the state value network V (s) with random weights θ1.
6: Initialize the conditional state value network V (s|b) with weights θ2.
7: for κ = 1, 2, · · · , N do
8:
9:

Sample a random mini-batch {(si,t, Ri,t, si,t+ki,t)} from the training data.
Embed the states si,t and si,t+ki,t using the quantization functions {q1, ..., qn} and the memory
mapping function g(·), e.g., s ← c(s)T θM /n where the activation vector c(s) is initialized to
0 and iteratively adding 1 to the g(qi(s))-th entry of c(s) for i = 1, ..., n.
Set the binary option indicating idle movement or dispatch b ← 1Ri,t≥0
Update the state value network V (s) with inputs si,t and targets Ri,t(γki,t −1)
γki,tV (si,t+ki,t).
Update the conditional state value network V (s|b) with inputs [si,t; b] and targets
Ri,t(γki,t −1)
ki,t(γ−1) + γki,tV (si,t+ki,t).

ki,t(γ−1) +

10:

11:

12:

13: end for
14: return V

B Dispatch Probability Model

We describe the experiment conﬁguration for the results in Table 1. For each city, we train a
LightGBM decision tree, and use it to predict the probability of a driver receiving trip request given
his or her current state. State features include driver_ids, driver’s location, time, and day of the week.
We hash and encode each driver_id into a 5 dimensional dense vector, and treat each element of the
vector as an input feature. Hyper-parameters are tuned via Bayesian optimization, where we ﬁt a
Gaussian process on target hyper-parameters, and use it as a surrogate model to optimize test AUC.
We report the best hyper-parameter conﬁguration in Table 2.

27

City Recall
0.7782
0.7568
0.7745

A
B
C

Precision
0.7596
0.7592
0.7834

F1
0.7835
0.7618
0.7812

Accuracy
0.8014
0.7761
0.7977

AUC
0.876
0.853
0.8729

Table 1: Evaluation results of the dispatch probability models.

num_leaves
6500
lambda_l2
0.01846772
Table 2: Hyper-parameter conﬁguration of the dispatch probability models.

boosting_type
gbdt
min_data_in_leaf
26

max_bin
8500
learning_rate
0.0834044

C Simulation Environment

We describe in detail the dynamics of the ride-hailing simulation environment and the associated data
sets.

C.1 Dynamics

The data generation of the simulator is based on replaying the historical trip request data of the
particular dates. For each given date, the simulation environment is initialized by the drivers’ states
and the orders information per historical data at the start time of the simulation. The set of vehicles
managed by the reposition policy are sampled from the initial group of drivers. The subsequent states
of the vehicles are completely determined by the matching and repositioning operations, as well
as the idle cruising and online-ofﬂine behavior models, which are trained on real data prior to the
simulation date. The reposition-managed vehicles are always online and are independent from the
behavior models. Speciﬁcally, the simulator batches idle drivers and open requests over matching
windows of a constant length (typically a few seconds) and performs order-driver matching at the
end of each window using the KM (Hungarian) algorithm on a bipartite graph of the drivers and
passengers. The edge weights of the graph are determined by the order dispatching policy. With the
minimum-distance policy, for example, the edge weight is the travel distance between the passenger
and the driver at the request time, i.e., the pick-up distance. All the matched vehicles will then go to
the request location and transport the passenger to the destination with the timestamp determined
by the corresponding predicted travel time. With a probability determined by an order cancellation
model, an order is cancelled based on the pick-up distance. Upon order completion, regular drivers
move according to the idle cruising model to a neighboring hexagon grid cell, if there is no immediate
new request matched to the drivers. For reposition-managed vehicles, they are batched in a similar
way to matching (but with a longer interval, e.g., 100s) and the reposition destinations are determined
by the reposition policy. Prior to the next round of matching, current drivers go ofﬂine and potential
new drivers appear online according to the online-ofﬂine behavior model.
We have released an open simulation platform7 powered by environments with the same dynamics
described above. The simulation engine has also supported the KDD Cup RL Track competition in
2020.

C.2 Data

The anonymized data sets used to power the simulator consists of four parts: vehicle trajectories,
trip requests, idle cruising, and order cancellation. The vehicle trajectory data contains instances of
timestamps and the corresponding vehicle location in coordinates for each combination of driver ID
and order ID. The trip request data speciﬁes for each order the start and end times, the pick-up and
drop-off locations, and the reward units (in the sense of trip fee). The idle cruising model is trained
on real data and speciﬁes the transition probability for each pair of origin and destination grid cell for
each hour of the day. Each OD pair is a potential idle cruising trip. The order cancellation model
outputs the cancellation probability for a given value of pick-up distance. It captures the nonlinear

7https://outreach.didichuxing.com/Simulation/

28

City
A
B
C

Supply vs Demand
scarce
abundant
abundant

Population Size (millions)
4.51
7.31
4.65

Table 3: The setup of the three pilot cities for the small-ﬂeet experiments.

cancellation behavior of a passenger in response to the pick-up distance once an order is matched. A
sample of these data have been released through the DiDi GAIA Initiative.8

D Real-world Experiment: Small Fleet

Table 3 shows the basic background information of the three cities in the small-ﬂeet experiments in
Section 6.2. The ﬁrst column shows the relative amount of supply (available vehicles) with respect to
the demand (passenger requests). The comparison is over the spatiotemporal dimensions instead of a
simple comparison of the total numbers. For example, ‘scarce’ means that during the experiment
period, there are more zones (deﬁned by hex cells) and time intervals where it is under-supplied than
those over-supplied.

8https://outreach. didichuxing.com/research/opendata/en/

29

