9
1
0
2

c
e
D
1
2

]
x
e
-
p
e
h
[

3
v
3
6
6
5
0
.
3
0
9
1
:
v
i
X
r
a

Scalable Deep Convolutional Neural Networks for Sparse, Locally Dense Liquid Argon
Time Projection Chamber Data

Laura Domin´e1, 2 and Kazuhiro Terao2
(on behalf of the DeepLearnPhysics Collaboration)∗
1Stanford University, Stanford, CA, 94305, USA
2SLAC National Accelerator Laboratory, Menlo Park, CA, 94025, USA

Deep convolutional neural networks (CNNs) show strong promise for analyzing scientiﬁc data in
many domains including particle imaging detectors such as a liquid argon time projection cham-
ber (LArTPC). Yet the high sparsity of LArTPC data challenges traditional CNNs which were
designed for dense data such as photographs. A naive application of CNNs on LArTPC data results
in ineﬃcient computations and a poor scalability to large LArTPC detectors such as the Short
Baseline Neutrino Program and Deep Underground Neutrino Experiment. Recently Submanifold
Sparse Convolutional Networks (SSCNs) have been proposed to address this class of challenges. We
report their performance on a 3D semantic segmentation task on simulated LArTPC samples. In
comparison with standard CNNs, we observe that the computation memory and wall-time cost for
inference are reduced by a factor of 364 and 33, respectively, without loss of accuracy. The same
factors for 2D samples are found to be 93 and 3.1 respectively. Using SSCN and public 3D LArTPC
samples, we present the ﬁrst machine learning-based approach to the reconstruction of Michel elec-
trons, a standard candle for energy calibration in LArTPC due to their very well understood energy
spectrum. We ﬁnd a Michel electrons identiﬁcation eﬃciency of 93.9% and a 96.7% purity. Re-
constructed Michel electron clusters yield 95.4% in average pixel clustering eﬃciency and 95.5% in
purity. The results are compelling in showing the strong promise of scalable data reconstruction
technique using deep neural networks for large scale LArTPC detectors.

I.

INTRODUCTION

Deep convolutional neural networks (CNNs) have be-
come the standard machine learning (ML) technique in
the ﬁelds of computer vision, natural language process-
ing, and other scientiﬁc research domains [1]. Applica-
tions of CNNs are actively developed for neutrino oscilla-
tion experiments [2–4], including those that employ liq-
uid argon time projection chambers (LArTPC). LArT-
PCs are a type of particle imaging detector which can
make 2D or 3D images of charged particles’ trajectories
with a breathtaking resolution (∼mm/pixel) over many
meters of detection volume. Current and future neutrino
oscillation experiments using LArTPCs include Micro-
BooNE [5], Short Baseline Near Detector (SBND) [6],
ICARUS [7] and the Deep Underground Neutrino Ex-
periment (DUNE) [8]. The active volumes of these ex-
periments are respectively about 90 tons, 112 tons, 600
tons and 40,000 tons of liquid argon.

Particle trajectories in LArTPC data, many of which
have the shape of 1D lines, are recorded in 2D or 3D
matrix format with an approximate pixel resolution of
3 mm to 5 mm. Each image has millions to billions of
pixels for large LArTPC detectors (e.g. MicroBooNE
produces 80 mega-pixels images). Those trajectories are
produced by ionization electrons, and are thin (a few
pixels in width) and continuous. In each recorded data,
depending on the experimental environment, there may
be a few to dozens of particle trajectories. Therefore

LArTPC images are generally sparse, yet locally dense
(i.e. no gap in between pixels that form a trajectory).
This characteristic of LArTPC data poses two serious
challenges for the application of CNNs. First, the matrix
algebra associated with CNNs is computationally ineﬃ-
cient for LArTPC data which is mostly ﬁlled with zeros.
Second, in photographs for which CNNs are originally
developed, all pixels carry information. The strength of
CNNs to automatically extract signal features may be af-
fected when applied on mostly zero-ﬁlled LArTPC data.
Recently a Submanifold Sparse Convolutional Network
(SSCN) [9, 10] has been proposed to address these con-
cerns with data represented by sparse matrix or point
clouds. In this paper we demonstrate that SSCN holds
strong promise for analyzing LArTPC image data with
respect to both accuracy and computational eﬃciency,
thus for being scalable to future large detectors includ-
ing DUNE. Our contributions include:

• Demonstration of the better performance and scal-
ability of sparse techniques in data reconstruction
tasks that may be part of a general reconstruction
chain in LArTPC data.

• Study of typical mistakes made by the algorithms

and mitigation methods.

• First ML-based approach for the reconstruction of
Michel electrons using a publicly available simula-
tion sample, and quantiﬁcation of the purity and
eﬃciency of this approach.

∗ contact@deeplearnphysics.org

All studies in this paper are reproducible using a

 
 
 
 
 
 
Singularity software container1 [11], our implemen-
tation of semantic segmentation algorithms2, and pub-
lic data samples3 [12] provided and maintained by the
DeepLearnPhysics collaboration.

Section II gives an overview of the public data set used
in this paper. Section III details the design of a neu-
ral network, U-ResNet, chosen for studying the impact
of SSCN. Section IV describes our experiment including
the performance metrics and training setup. Section V
presents the results including the performance compari-
son between a SSCN (sparse) and standard (dense) im-
plementations of U-ResNet. We discuss causes of poor
performance of U-ResNet and propose mitigation meth-
ods in Section VI. Lastly, in Section VII, we present our
approach and the results of reconstructing Michel elec-
trons in the public simulation sample using the sparse
U-ResNet.

II. DATA SET

A. Particle Images

In this paper we use 2D and 3D LArTPC simula-
tion samples made publicly available by the DeepLearn-
Physics collaboration [12]. These are images of particles
traversing a cubic volume of liquid argon, whose size can
be 192px, 512px or 768px. The spatial resolution of each
pixel is 3 mm. The dataset contains 100,000 images for
each size. We split each sample into 80 % and 20 % frac-
tions as train and test sets respectively. There are two
sources of particles in this dataset:

• Single, isolated particle: an electron, muon, anti-
muon or proton. 1 to 10 such particles are gener-
ated in a larger volume and a cropped 3D volume
is recorded.

• Multi-particle vertex: multiples particles produced
at the same 3D point, including electrons, gamma-
rays, muons, anti-muon, charged pions, and pro-
tons.

Particle interactions with the liquid argon medium
are simulated using Geant4 [13] and LArSoft [14]. The
particle energy depositions are recorded in each pixel.
The drift simulation, which would include for example
the electron lifetime, recombination, diﬀusion and space
charge eﬀects, is not included in this dataset. However,
energy depositions are smeared by a few pixels to mimic a
diﬀusion eﬀect while total deposited energy is conserved.

1 https://www.singularity-hub.org/containers/6596
2 https://github.com/Temigo/uresnet_pytorch
3 https://dx.doi.org/10.17605/OSF.IO/VRUZP

2

HIP MIP Shower Delta rays Michel e−

Fraction 17 % 34 % 47 %

1 %

1 %

TABLE I. On average only 0.01 % of pixels in an event are
non-zero. This table shows to which class these non-zero pix-
els belong.

B. Labels

Among the tasks available for a benchmark in this pub-
lic dataset, we choose the semantic segmentation. The
task is to predict a class of particle at pixel-level. The
labels in the dataset for supervised learning include ﬁve
possible classes for each pixel:

• Protons, referred to as heavily ionizing particle
(HIP), which typically display short, highly ion-
ized tracks

• Minimum ionizing particles (MIP) such as muons

or pions, with usually longer tracks

• Electromagnetic showers induced by electrons,
positrons and photons, with kinetic energy above
critical value (about 33MeV in argon)

• Delta ray electrons from hard scattering of other

charged particles

• Michel electrons from the decay of muons

The statistics of each class in the dataset are shown
in Table I. Figure 1 shows an example of a simulated
image from this dataset and the corresponding pixel-wise
labels. More details about the dataset can be found in
the reference [12].

III. NETWORK ARCHITECTURES

A. Dense U-ResNet: baseline

We use a network architecture which we call U-ResNet.
It is a hybrid between two popular architectures: U-
Net [15] and ResNet [16].

U-Net is an auto-encoder network architecture (Fig-
ure 2) which has been successful for medical image seg-
mentation. It is made of two parts: the ﬁrst half down-
samples the spatial size (using strided convolutions in our
case) the input image with several convolution blocks.
This part learns the image features on diﬀerent scales in
a hierarchical manner, yielding a tensor with a low spa-
tial resolution but a large number of channels. These
channels contain a lot of compressed feature informa-
tion: hence it is called the encoder part of the U-Net.
The number of downsizing operations is referred to as
depth in this paper, and aﬀects the receptive ﬁeld area
of the network. The second half of the network applies

3

FIG. 1. Simulated LArTPC event data (left) and labels (right). The data shows energy deposits from charged particle
trajectories. The color corresponds to an energy scale. In the label image, each pixel is assigned one of ﬁve colors: heavily
ionizing particles (HIP) in blue, minimum ionizing particles (MIP) in cyan, electromagnetic showers in green, delta rays in
yellow and Michel electrons in orange.

FIG. 2. U-ResNet architecture for semantic segmentation. In this example we say that the U-ResNet has a depth of 3 since
we perform 3 downsamplings. Turquoise boxes represent convolutions with stride 2 and increasing the number of ﬁlters. Dark
blue boxes are transpose convolutions with stride 2 and decreasing the number of ﬁlters. Purple boxes are convolutions with
stride 1 that decrease the number of ﬁlters. The spatial size of feature maps is constant across the horizontal dimension.

to this tensor several up-sampling (we use transpose con-
volutions) and convolution operations. It is called a de-
coding path. Concatenation between the feature map of
the previous layer in the decoding path and the feature
map of the same spatial size in the encoding path helps
the decoder to restore the original image resolution. The
input image is single channel but the output has as many
channels as there are classes.

deeper. In our implementation the number of ﬁlters at
each layer increases with depth in a power law for our
dense U-ResNet and linearly for our sparse U-ResNet.

U-Net is a generic CNN architecture. In our case each
block of convolutions/up- or down- sampling is made of
two convolution layers, followed by a batch normalization
and a rectiﬁed linear unit (ReLU) function. According
to the ResNet architecture we also add residual skip con-
nections which allow the network to learn faster and be

The strong performance of this network for 2-class se-
mantic segmentation (between particle track and electro-
magnetic shower) at pixel-level on real detector data was
already demonstrated [3] by MicroBooNE experiment,
which makes it a network of choice to benchmark a sparse
technique on LArTPC simulation data.

4

B. Submanifold Sparse Convolutional Networks

A. Evaluation Metrics

The key element of submanifold sparse convolutional
networks [9] is a so-called submanifold sparse convolution
operation. It was designed for cases where the eﬀective
dimension of the data is lower than the ambient space,
for example a 2D surface or a 1D curve in a 3D space.
For such cases the standard dense convolutions are not
suitable for several reasons:

• Traditional convolutions involve dense matrix mul-
tiplication operations, which are computationally
ineﬃcient for sparse data.

• The submanifold dilation problem, as described in
reference [9]: a single non-zero site in the image
yields 3d non-zero sites in the next feature map
after a dense convolution, where d is the spatial
dimension (in our case d = 2 or d = 3). After 2
convolutions there will be 5d non-zero sites, and so
on. This inescapable growth “dilates” the originally
sparse image which becomes denser.

The idea of SSCNs is to keep the same level of sparsity
throughout the network computations, especially convo-
It has been shown to require signiﬁcantly less
lutions.
computations while outperforming the dense CNNs on
two 3D semantic segmentation challenges in the ﬁeld of
computer vision [10].

Reference [9] deﬁnes two new operations. First, sparse
convolutions SC(n, m, f, s) with n input features, m out-
put features, f ﬁlters and a stride s are deﬁned. They
address the ﬁrst issue mentioned above and work in the
same way as standard convolutions except they assume
that the input from non-active pixels, which are zero
or close to zero, is zero. The output feature map will
have a size (l − f + s)/s where l is the size of the in-
put. Secondly they deﬁne a submanifold sparse convolu-
tion SSC(n, m, f ) with similar notations as a modiﬁed
SC(n, m, f, s = 1): the input is padded with (f − 1)/2
zeros on each side to ensure that the output image will
have the exact same size. An output pixel will be nonzero
if and only if the central pixel of the receptive ﬁeld is
nonzero in the input feature map. SSC operation tack-
les the second issue by constraining the output sparsity.
In order to build complete CNNs based on these two op-
erations, the authors also deﬁne a set of other custom
operations such as activation functions and batch nor-
malization layers by restricting the corresponding stan-
dard operations to the set of nonzero pixels.

IV. EXPERIMENTS

We perform two sets of experiments. First, we compare
the performance between dense and sparse U-ResNet us-
ing several evaluation metrics for 2D and 3D samples.
Second, we study the variation of the performance of
sparse U-ResNet with key architecture hyper-parameters
and diﬀerent image sizes.

The network is trained by minimizing a loss which is
a softmax cross-entropy loss averaged over all the pixels
of an image. We deﬁne diﬀerent metrics of interest:

• Non-zero accuracy:

fraction of non-zero pixels

whose label is correctly predicted.

• Class-wise non-zero accuracy:

for each event and
for each class, fraction of non-zero pixels in that
class that are correctly predicted.

• Resources usage during the training and testing

time:

– GPU memory occupied

– Computation wall time

B.

Implementation and Training Details

All networks were implemented using the PyTorch [17]
(version 1.0) deep learning framework. SSCN relies on
the library SparseConvNet 4. We use LArCV2 5
to interface with the LArTPC data ﬁles. To train the
networks we used ADAM optimizer [18] with the de-
fault learning rate of 0.001. We trained the networks for
30k iterations in 3D and 40k iterations in 2D. We used
NVIDIA V100 GPUs with 32GB memory. On 3D im-
ages of size 192px approximately 10 and 212 hours were
required for convergence of the sparse and dense networks
respectively.

V. RESULTS

Notation: we write, for example, [2D, 512px, 5-16]
to represent “2D images of size 512px, and U-ResNet of
depth 5 with 16 ﬁlters”.

A. Sparse vs dense U-ResNet

We start by comparing the performance of dense versus
sparse U-ResNet using the non-zero accuracy metric as
well as the computational resources usages at train and
inference (or test) time.

As shown in Table II, for a ﬁxed 3D image size of 192px
and identical training parameters (notably batch size),
the ﬁnal non-zero accuracy mean value over the whole
dataset for the sparse U-ResNet is slightly higher than
the dense counterpart by 2%. However using the exact
same architecture doesn’t do justice to the real feat of

4 https://github.com/facebookresearch/SparseConvNet
5 https://github.com/DeepLearnPhysics/larcv2

Sparse

5

Dense
4

Batch size
192px 192px 192px 512px 768px
Image size
Nonzero accuracy mean 92% 94% 98% 99% 99%
0.096 0.088 0.049 0.014 0.0037
Nonzero accuracy std

64

4

Nvidia V100 GPU

Memory (test) [GB]
Memory (train) [GB]
Wall-time (test) [s]
Wall-time (train) [s]

16
26x4
3.3
25

0.044
0.21
0.10
0.21

0.19
1.3
0.66
1.2

Intel Xeon Silver 4110 CPU

Memory (test) [GB]
Memory (train) [GB]
Duration (test) [s]
Duration (train) [s]

-
-
-
-

0.57
0.59
0.25
1.1

0.81
1.9
1.7
6.1

0.67
5.1
2.4
5.0

1.9
3.9
8.0
24

1.3
9.3
4.4
8.8

3.0
4.0
16
47

TABLE II. Sparse and dense U-ResNet scalability with the
3D image spatial size. The dense U-ResNet could not ﬁt 3D
images of size 512px nor 768px on a single GPU. Both sparse
and dense networks here have a depth 5 and number of ﬁlters
16.

SSCN: the GPU memory usage and computation dura-
tion are drastically cut down using sparse convolutions,
which allows us to train the sparse U-ResNet with a dra-
matically larger batch size and a larger 3D image size.
Harnessing both of these advantages allows one to beat
the baseline dense 3D U-ResNet by a large margin in
non-zero accuracy.

Figures 3 and 4 show the variation of memory and com-
putation wall-time for sparse U-ResNet in [2D, 512px, 5-
16]. The latter grows linearly but slowly as a function of
batch size, which makes larger batch sizes practical not
only for training but also for the inference. In particular,
the sparse U-ResNet can easily process a whole Micro-
BooNE event data with a conventional GPU (memory
of 4 to 11GB). The resource usage scales well with the
batch size to handle ICARUS detector, which is about
6 times larger than MicroBooNE. At the batch size 88,
which is the maximum possible for a single NVIDIA V-
100 GPU with the dense version, the reduction factors
for memory and computation wall-time with the sparse
U-ResNet are 93 and 3.1 respectively. Further, because
the computational cost scales with nonzero pixel count
instead of the total pixel count in the bounded volume,
sparse U-ResNet will be an ideal solution for DUNE far
detector which will be sparser in the absence of cosmic
rays. These beneﬁts apply to a training phase of an algo-
rithm. Figure 5 shows how using sparse U-ResNet speeds
up the training by several orders of magnitude. This is
crucial for reconstruction algorithm R&D work which of-
ten requires a short turn-around time for development.

Finally looking at the evolution of the softmax scores
for diﬀerent classes across iterations indicates that the
sparse U-ResNet may be learning more uniformly over
pixels than its dense equivalent. Figure 6 shows how the
standard deviation of the mean softmax value in the im-
age evolves with the training iterations. The dense net-
work results in a much higher variance. Their variances

FIG. 3. GPU memory usage as a function of batch size at
inference time [2D, 512px, 5-16].

FIG. 4. Computation wall-time as a function of batch size at
inference time [2D, 512px, 5-16].

end up converging after about 1000 iterations. This ob-
servation is illustrated in Figure 7, in which a MIP tra-
jectory crosses an EM shower. Figures 8 and 9 compare
how the softmax scores for track and shower particles
change over training iterations between sparse and dense
U-ResNet. The diﬀerence appears most strikingly at the
iteration 40, where the dense network is extremely con-
ﬁdent in some pixels (yellow ones) and still very unsure
about others (in dark blue), while the sparse one is in-
creasing its conﬁdence level much more uniformly across
all pixels.

B. Sparse U-ResNet Performance Variation

We study the inﬂuence of the two main parameters
of the network architecture on performance and resource
usage: depth (number of layers) and the number of ﬁl-
ters in the ﬁrst layer. Table III, Figure 10 and 11 show
the results of non-zero accuracy and computational re-
source usage respectively. The ﬁlter counts have a larger

02004006008001000Batch size0246Forward time (s)SparseDense6

FIG. 7. Top: energy depositions in the image, the pixel color
labels, each color
corresponds to an energy scale. Bottom:
corresponds to a diﬀerent class. Electromagnetic shower pix-
els are colored in purple and MIP pixels are in green.

8

16

Filters
Depth 6 98.94% 99.16% 99.23%
Depth 5 98.86% 99.07% 99.06%
Depth 4 98.74% 99.00% 99.07%

32

TABLE III. Comparison of the non-zero accuracy at inference
time on the test set of 3D 512px images for sparse U-ResNet,
for diﬀerent depths and initial number of ﬁlters.

recorded volume boundaries. We looked at the distribu-
tion of the fraction of mis-classiﬁed pixels as a function
of their distance to the boundaries, which is deﬁned as
follows in d dimensions:

FIG. 5. Nonzero accuracy as a function of wall-time during
the training [3D, 192px, 5-16]. The sparse U-ResNet uses a
batch size of 64, dense U-ResNet uses a batch size of 4.

FIG. 6. Standard deviation of the mean softmax value of
pixels predicted as shower pixels in an image, as a function of
the training iteration step. The sparse U-ResNet appears to
learn in a more uniform manner across the pixels [2D, 512px,
5-16].

eﬀect on achieving a higher accuracy while it also causes
a linear increase in memory usage. The increase in the
computation wall-time is only ≈ 10% between 8 and 32
ﬁlter counts.

Table IV shows the result of comparing network class-
wise non-zero accuracies for varying 3D image sizes at
the train and test times. For a given image size at train
time, using a larger image size at test time systematically
improves the performance. This table also shows that the
class-wise accuracy of delta rays and Michel electrons is
lower than other classes across all image sizes.

C. Mistakes Analysis

One may consider that a poor performance may be
partially due to particle trajectories being cut out at the

d({xi}i=1,...,d) = min
i=1...d

min xi

(1)

050100150200Time elapsed (h)0.50.60.70.80.91.0Nonzero AccuracySparse (batch size 64)Dense (batch size 4)101102103104Iteration0.050.100.15Standard deviationSparseDense7

FIG. 8. Dense U-ResNet evolution of softmax value for EM shower (top) and MIP (bottom) across training iterations.

FIG. 9. Sparse U-ResNet evolution of softmax value for EM shower (top) and MIP (bottom) across training iterations.

192px

768px

Test image
Train image 192px 512px 768px 192px 512px 768px
96.0% 95.6% 93.7% 98.8% 99.0% 98.9%
HIP
96.2% 96.6% 95.4% 99.4% 99.7% 99.6%
MIP
97.6% 96.9% 96.6% 99.5% 99.6% 99.7%
EM shower
74.3% 76.7% 75.1% 85.9% 89.6% 90.1%
Delta rays
Michel e−
36.5% 42.6% 43.9% 62.6% 70.0% 70.4%
98.0% 98.1% 97.7% 98.9% 99.2% 99.3%
Overall

TABLE IV. Class-wise non-zero accuracy. Comparing the
performance of sparse U-ResNet with diﬀerent 3D image sizes
at train and test time. The batch size, the depth, and the
initial number of ﬁlters are 64, 5, and 16 respectively.

where d runs from 1 to 3 for 3D data. In other terms,
the distance of the pixel to the image boundaries is the

distance from the pixel to the closest face of the cubic
image boundaries.

Figure 12 shows that, in general, pixels are more likely
mis-classiﬁed near image boundaries as expected. We can
see that, however, this is not clearly visible for Michel
electrons and delta rays. Therefore this is not an ex-
planation for the poor performance on these two classes.
We investigated possible explanations beyond originally
planned experiments, and report our ﬁndings in the fol-
lowing sections.

Figure 13 shows some 3D images of size 512px, which
are examples of typical mistakes made by the sparse U-
ResNet. This includes: Michel electrons mistaken for an
electromagnetic shower and vice-versa, HIP mistaken for
a MIP and the track-like beginning of a short EM shower
mistaken for a MIP.

8

FIG. 10. Memory usage of sparse U-ResNet with depth 6, 3D
512px images and a varying number of initial ﬁlters.

FIG. 12. Fraction of misclassiﬁed pixels as a function of the
pixel distance to the image boundaries [3D, 192px, 5-16].

Relabeled Relabeled+Weights

Relabeled

Regular

Train data
Test data Regular
98.0% 98.1% 98.1%
HIP
99.4% 99.2% 99.4%
MIP
99.4% 97.9% 99.2%
Shower
Delta rays
85.7% 94.8% 96.0%
Michel e− 56.6% 94.4% 94.7%
99.2% 99.2% 99.6%
Overall

99.3%
98.1%
99.2%
97.2%
95.7%
99.1%

FIG. 11. Computation wall-time of sparse U-ResNet with
depth 6, 3D 512px images and a varying number of initial
ﬁlters.

VI. MICHEL ELECTRONS AND DELTA RAYS

We propose two hypothetical explanations for low pre-
diction accuracies on Michel and delta ray pixels by U-
ResNet. The ﬁrst is statistical imbalance in the fraction
of pixels of each class: delta rays and Michel electrons
represent each about 1 % of the total pixels. The sec-
ond is an ambiguous deﬁnition of these two classes: both
Michel electrons and delta rays can emit gamma rays
(e.g. Bremsstrahlung radiation) which appear to be in-
distinguishable from EM shower class. During training,
we employed a softmax loss for classifying pixels under
the assumption of exclusive class deﬁnitions, which may
not hold for these classes.

We implemented two changes in order to test our hy-
pothesis. The ﬁrst is a modiﬁcation to the pixel labels
used in our supervised training. For Michel electrons
and delta rays, pixels are re-labeled as EM shower except
for those that belong to a primary ionization trajectory,
which carries distinctive features. Secondly we experi-

TABLE V. A comparison of class-wise nonzero accuracies be-
tween 3 ﬂavors of sparse U-ResNet: regular, trained with re-
labeled dataset, and trained with both the relabeled dataset
and the weighting scheme [3D, 512px, 5-32]. We also com-
pare the performance of the regular sparse U-ResNet on a
test relabeled dataset.

mented a pixel-wise loss weighting factor to accommo-
date statistical imbalance across ﬁve classes. This allows
U-ResNet to focus more on pixels with low statistics, in-
spired by attention mechanisms.

We train a sparse U-ResNet using the re-labeled
dataset and optionally the pixel-wise loss weighting
scheme. The results are presented in Table V. First,
regardless of whether the U-ResNet was trained on the
regular or relabeled dataset, the non-zero accuracy on
Michel electrons increased by more than 40%. This im-
plies that the algorithm did learn the distinctive features
of Michel electrons and delta rays without relabeling.
Secondly, we see a slight improvement for delta rays and
EM shower pixels by training with the re-labeled dataset.
Finally, pixel-wise loss weighting further improved the ac-
curacy of both Michel and delta ray classes as expected.

VII. MICHEL ELECTRON RECONSTRUCTION

Finally we present a study on reconstructing Michel
electron energy spectrum using the public simulation
sample. Michel electron is one of well understood physics
signals, and thus useful for detector calibrations. This

81632Number of filters0246810Memory (Gb)2.635.029.950.400.731.49traintest81632Number of filters012345Time (s)4.614.615.192.522.562.78traintest01020304050Distance to boundary0.000.010.020.030.04Fraction of misclassified voxelsTrue labelHIPMIPShowerDeltaMichel9

FIG. 13. Typical mistakes of sparse U-ResNet [3D, 512px, 5-16]. Images are selected among the worst 0.05% with respect to the
non-zero accuracy metric. Mistakes are circled in red. Left column: data. Middle column: labels. Right column: predictions of
the network. First row: an electromagnetic shower is mistaken for a Michel electron. Second row: a Michel electron is mistaken
for an electromagnetic shower. Third row: a part of a HIP is mistaken for a MIP. Fourth row: a short shower is mistaken for
a (MIP) track.

analysis has been done by LArTPC experiments with
real data including MicroBooNE and ICARUS [19, 20].
Our contribution is to show the ﬁrst ML-based approach
with quantiﬁcation of both eﬃciency and purity of recon-
structed signal.

A. Reconstruction Method

Our goal is to quantify the eﬃciency and purity of
clustering Michel electron energy depositions by only us-
ing the primary ionization component of its trajectory.

We use the 3D 512px images from the re-labeled sam-
ple presented in the previous section. After running
the U-ResNet for semantic segmentation we isolate pix-
els that belong to each of the ﬁve classes. We run a
common density-based spatial clustering algorithm DB-
SCAN [21] to identify diﬀerent predicted Michel electron
clusters and MIP clusters, with parameters (cid:15) = 2.8 and
minP ts = 5 and 10 respectively. We then select the can-
didate Michel electron clusters that are attached to the
edge of a predicted MIP cluster. Here “attached” is de-
ﬁned as less than 1px distance between the nearest pixels
of a Michel electron and MIP clusters. The “edginess”
of a given pixel is evaluated by masking surrounding pix-
els within the radius of 15px, and making sure that the
DBSCAN algorithm only ﬁnds one cluster when run over
the remaining MIP cluster pixels.

B. Performance Metrics

i

After identifying candidate Michel electron clusters, we
match each of them to a true Michel cluster by maximiz-
ing the overlap pixel count between true and predicted
Michel cluster. We can then deﬁne several performance
metrics. Let us deﬁne notations: N pred
is the total num-
ber of pixels in the predicted Michel electron cluster i,
N true
the total number of pixels in the matched true
i
Michel electron cluster i, and Ni the number of pixels
which belong to the intersection of both candidate and
matched Michel electron clusters. Then we deﬁne clus-
tering eﬃciency and purity as Ni/N true
re-
spectively. Similarly if N true is the total number of true
Michel electron clusters in the sample, Npred is the total
number of candidate Michel electron clusters, and N true
pred
is the number of matched candidate Michel electron clus-
ters over the whole sample, then we deﬁne ID eﬃciency
and purity as N true
pred/Npred respectively.

pred/N true and N true

and Ni/N pred

i

i

C. Results

Figure 14 shows the pixel count of matched true Michel
electron clusters against the pixel count of reconstructed
Michel electron clusters. As expected, most of the clus-
ters lie on the diagonal. The majority of strayed clusters
are present below the diagonal and are under-clustered.
Table VI shows the evaluation metrics with and with-
out an analysis quality cut, which requires reconstructed
Michel electron clusters to contain 10 or more pixels. We
ﬁnd that 89.8 % of the reconstructed Michel electrons
have both cluster eﬃciency and purity above 95%. Mi-
croBooNE collaboration has published Michel electron
reconstruction study with 2% ID eﬃciency and 80-90%
ID purity where the focus of the analysis was to maxi-
mize the purity of the sample for accurate energy recon-
struction [19]. The outcome of this study with the public
simulation sample cannot be directly compared with oth-
ers using real detector data because the public simulation

10

FIG. 14. A comparison of pixel counts between the true and
candidate Michel electron clusters.

Cut
Sample size
ID purity
ID eﬃciency

10
None
6961
6998
96.7 % 97.3 %
93.9 % 93.4 %
Cluster eﬃciency 95.4 % 96.0 %
95.5 % 96.0 %

Cluster purity

TABLE VI. ID purity and eﬃciency as well as cluster purity
and eﬃciencies of reconstructed Michel electrons. The sample
size is the number of true positives. The cluster eﬃciency
and purity are averaged over all reconstructed Michel electron
clusters.

sample lacks complicated detector eﬀects. However, the
results are compelling to show the promise of ML-based
reconstruction approach.

Finally, using the matched candidate Michel electrons,
we compare the reconstructed and true energy distribu-
tion in the primary ionization component. Figure 15
shows a reasonable agreement. In order to reconstruct
the total true Michel electron energy, the reconstruction
step needs to account for EM shower pixels resulting
from Bremsstrahlung radiation as described in the Mi-
croBooNE publication [19]. This is out of the scope of
this paper.

VIII. CONCLUSIONS

In this paper, we demonstrated the strong performance
of SSCN against our baseline dense CNN for LArTPC
data reconstruction, speciﬁcally for the task of semantic
segmentation to identify ﬁve particle classes at a pixel-
level. We employed U-ResNet, an architecture pioneered
by MicroBooNE collaboration, and showed that the im-
plementation using SSCN makes a drastic improvement
in the computational resource usage. For U-ResNet un-
der the same condition of batch size 4 with 192px 3D
images, SSCN reduces the computational cost in mem-
ory and wall-time at inference by a factor of 354 and 33

02004006008001000Pixels in matched true Michel cluster02004006008001000Pixels in candidate Michel cluster10010110211

We presented the ﬁrst demonstration of reconstructing
Michel electron clusters, deﬁned as the primary ioniza-
tion component of a trajectory, using a primarily ML-
based method. Our result using the public simulation
sample shows a naive approach with DBSCAN on U-
ResNet output can yield 93.9% Michel electron identi-
ﬁcation eﬃciency with 96.7% true positive rate. Pixel
clustering eﬃciency for reconstructed Michel electrons is
found to be 95.4% with the purity of 95.5%. In partic-
ular, 89.8% of reconstructed Michel electrons are found
to carry both the eﬃciency and purity of clusters above
95%.

SSCN is a solution to address scalable CNN applica-
tions for LArTPC data, which is generically sparse but
locally dense. Furthermore, SSCN is a generic alterna-
tive to dense CNN, and can be applied to tasks beyond
semantic segmentation including image classiﬁcation, ob-
ject detection and more. We strongly recommend SSCN
for any CNN applications that exist for LArTPC ex-
periments including MicroBooNE, ICARUS, SBND, and
DUNE.

IX. ACKNOWLEDGEMENT

This work is supported by the U.S. Department of En-
ergy, Oﬃce of Science, Oﬃce of High Energy Physics,
and Early Career Research Program under Contract DE-
AC02-76SF00515.

FIG. 15. Energy spectrum of Michel electrons. The primary
ionization energy is the energy of Michel after relabeling. The
candidate Michel energy is the sum of pixel values predicted
as Michel electron by the U-ResNet [3D, 512px, 5-32].

respectively as shown in Table II. For 2D samples, using
batch size 88, those reduction factors are 93 and 3.1 re-
spectively. While a naive application of standard CNN
for 3D data (e.g. the DUNE near detector) comes with
prohibitive and extremely ineﬃcient computational re-
source usage, we demonstrated that SSCN can mitigate
such costs and generalize U-ResNet for 3D data samples
without loss in the algorithm performance.

[1] Y. LeCun, Y. Bengio, and G. Hinton, Nature 521, 436

[13] S. Agostinelli et al. (GEANT4), Nucl. Instrum. Meth.

(2015).

[2] R. Acciarri et al. (MicroBooNE), Journal of instrumen-

tation 12, P03011 (2017).

[3] C. Adams

et
arXiv:1808.07269 [physics.ins-det].

al.

(MicroBooNE),

(2018),

[4] A. Radovic, M. Williams, D. Rousseau, M. Kagan,
D. Bonacorsi, A. Himmel, A. Aurisano, K. Terao, and
T. Wongjirad, Nature 560, 41 (2018).

[5] R. Acciarri et al. (MicroBooNE), Journal of Instrumen-

tation 12, P02017 (2017).

[6] M. Antonello et al. (MicroBooNE, LAr1-ND, ICARUS-
WA104), (2015), arXiv:1503.01520 [physics.ins-det].
[7] S. Amerio et al. (ICARUS), Nuclear Instruments and
Methods in Physics Research Section A: Accelerators,
Spectrometers, Detectors and Associated Equipment
527, 329 (2004).

A506, 250 (2003).

[14] E. L. Snider and G. Petrillo, Proceedings, 22nd Interna-
tional Conference on Computing in High Energy and Nu-
clear Physics (CHEP2016): San Francisco, CA, October
14-16, 2016, J. Phys. Conf. Ser. 898, 042057 (2017).

[15] O. Ronneberger, P. Fischer,

in In-
ternational Conference on Medical
image computing
and computer-assisted intervention, Vol. 9351 (Springer,
2015) pp. 234–241.

and T. Brox,

[16] K. He, X. Zhang, S. Ren, and J. Sun, in Proceedings
of the IEEE conference on computer vision and pattern
recognition (2016) pp. 770–778.

[17] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, L. Desmaison, Alban Antiga, and
A. Lerer, in NIPS-W (2017).

[18] D. P. Kingma and J. Ba, CoRR abs/1412.6980 (2014),

[8] R. Acciarri et al. (DUNE),

(2016), arXiv:1601.02984

arXiv:1412.6980.

[physics.ins-det].

[19] R. Acciarri et al. (MicroBooNE), Journal of Instrumen-

[9] B. Graham and L. van der Maaten, CoRR (2017),

tation 12, P09014 (2017).

arXiv:1706.01307.

[20] S. Amoruso et al. (ICARUS), Eur. Phys. J. C33, 233

[10] B. Graham, M. Engelcke,

and L. van der Maaten,
Proceedings of the IEEE Computer Vision and Pattern
Recognition CVPR, Salt Lake City, UT, USA , 18 (2018).

[11] K. G. Sochat VV, Prybol CJ, PLoS ONE 12 (2017).
[12] C. Adams, K. Terao, and T. Wongjirad (DeepLearn-

Physics), in preparation (2019).

(2004), arXiv:hep-ex/0311040 [hep-ex].
[21] M. Ester, H.-P. Kriegel, J. Sander,

in
Proceedings of the Second International Conference on
Knowledge Discovery and Data Mining, KDD’96 (AAAI
Press, 1996) pp. 226–231.

and X. Xu,

0100200300400500600700800Number of pixels0200400600800Michel electrons clustersReconstructedTrue cluster