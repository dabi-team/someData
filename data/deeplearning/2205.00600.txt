HatCUP: Hybrid Analysis and Attention based Just-In-Time
Comment Updating

Hongquan Zhu
State Key Lab for Novel Software
Technology, Nanjing University
Nanjing, China
hqzhu@smail.nju.edu.cn

Xincheng He
State Key Lab for Novel Software
Technology, Nanjing University
Nanjing, China
xinchenghe2016@gmail.com

Lei Xu∗
State Key Lab for Novel Software
Technology, Nanjing University
Nanjing, China
xlei@nju.edu.cn

2
2
0
2

y
a
M
2

]
E
S
.
s
c
[

1
v
0
0
6
0
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
When changing code, developers sometimes neglect updating the
related comments, bringing inconsistent or outdated comments.
These comments increase the cost of program understanding and
greatly reduce software maintainability. Researchers have put for-
ward some solutions, such as CUP and HEBCUP, which update
comments efficiently for simple code changes (i.e. modifying of a
single token), but not good enough for complex ones. In this paper,
we propose an approach named HatCUP (Hybrid Analysis and
Attention based Comment UPdater), to provide a new mechanism
for comment updating task. HatCUP pays attention to hybrid anal-
ysis and information. First, HatCUP considers the code structure
change information and introduces a structure-guided attention
mechanism combined with code change graph analysis and opti-
mistic data flow dependency analysis. With a generally popular
RNN-based encoder-decoder architecture, HatCUP takes the action
of the code edits, the syntax, semantics and structure code changes,
and old comments as inputs and generates a structural represen-
tation of the changes in the current code snippet. Furthermore,
instead of directly generating new comments, HatCUP proposes
a new edit or non-edit mechanism to mimic human editing behav-
ior, by generating a sequence of edit actions and constructing a
modified RNN model to integrate newly developed components.
Evaluation on a popular dataset demonstrates that HatCUP outper-
forms the state-of-the-art deep learning-based approaches (CUP)
by 53.8% for accuracy, 31.3% for recall and 14.3% for METEOR of
the original metrics. Compared with the heuristic-based approach
(HEBCUP), HatCUP also shows better overall performance.

CCS CONCEPTS
• Software and its engineering → Software maintenance tools;
Maintaining software; Software evolution.

∗Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICPC ’22, May 16–17, 2022, Virtual Event, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9298-3/22/05. . . $15.00
https://doi.org/10.1145/3524610.3527901

KEYWORDS
comment updating, code-comment co-evolution, hybrid analysis,
data flow analysis, deep learning

ACM Reference Format:
Hongquan Zhu, Xincheng He, and Lei Xu. 2022. HatCUP: Hybrid Analysis
and Attention based Just-In-Time Comment Updating. In 30th International
Conference on Program Comprehension (ICPC ’22), May 16–17, 2022, Virtual
Event, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/
3524610.3527901

1 INTRODUCTION
As the complexity of software projects and the frequency of soft-
ware product iterations continue to increase, program comprehen-
sion is becoming more important throughout the software develop-
ment process. As recently shown by Xia et al. [47], 58% of develop-
ers’ time was spent in comprehending code. In addition to the code
itself, code comments are considered as the most important form
of documentation for program comprehension [5]. Source code
is constantly evolving, with developers regularly refactoring and
integrating new functionality; however, code comments are often
ignored when the code goes through changes [32, 37, 45], leading
to the inconsistency between code and comments that not only
brings about confusion in software development and maintenance
[15] but can also result in bugs [37].

Comment generation aims to summarize code snippets with
code representations [1, 15, 23, 42, 51] by generating an entirely
new comment related to the current version of the code. However,
they cannot retain some content intended to be highlighted in the
existing comment. Recently, some approaches have been proposed
to focus on automatic comment updating. For example, Liu et al.
[26, 28] propose a just-in-time technique, called CUP, to cope with
the problem of the widespread presence of inconsistent comments.
The core idea of CUP is to leverage a neural sequence-to-sequence
model to learn comment update patterns from old comments and
changed code tokens.

Although CUP has good performance on comment updating, it
has some limitations. Lin et al. [24] find that since major correct
comments generated are related to modifying a single token, CUP
tends to fail when its actual application scope is limited due to fre-
quent updates. Hence, Lin et al. propose a heuristic-based comment
updater HEBCUP that has the edge over CUP by focusing on the
changed code patterns. However, HEBCUP also lacks efficiency
faced with processing complex updating. When there are many
changed code tokens or the code changes are not directly related
to the old comment, CUP and HEBCUP may fail to make correct
updates.

 
 
 
 
 
 
ICPC ’22, May 16–17, 2022, Virtual Event, USA

Zhu, He and Xu

This paper proposes a new approach called HatCUP to address
comment updating in complex scenarios. Firstly, HatCUP consid-
ers more about the code structure change information. Instead of
only focusing on code text changes, HatCUP pays attention to hy-
brid analysis and information with a structure-guided attention
mechanism. It proposes a constraint-based optimistic data flow
dependency analysis. Specifically, the derivation of such data-flow
is not through conservative standard data-flow analysis, but rather
similar to how humans derive data-flow, and more effective to
the comment updating scenario. Combined with constraint-based
optimistic data flow dependency analysis and code change graph
analysis, HatCUP can obtain the changed variable nodes and their
associated dependencies on each other. Then, HatCUP constructs
a change-guided and dependency-guided attention mechanism to
help the model focus on changed syntax nodes and long-term de-
pendencies among variables. Hence, HatCUP can collect complete
information even when the code changes are not directly related
to the old comment.

Secondly, with the core idea of imitating human editing behav-
iors, HatCUP considers more about edit actions(e.g., inserting, delet-
ing and updating) on original comments instead of directly gen-
erating new comments. HatCUP modifies an RNN-based encoder-
decoder model that is shown to be effective for many Software
Engineering (SE) tasks [15, 27, 40] to integrate our newly devel-
oped component. Based on the model, HatCUP proposes a new edit
or non-edit mechanism to emphasize the possibility that a certain
token in the old comment will be edited to match the new code pat-
terns. Specifically, the edit or non-edit mechanism leverages three
different encoders to encode code changes, syntax changes and
old comments. Then, HatCUP determines how the source code
changes associated with the current decoding step changing the
relevant parts of the old comment with three scenarios: (1) it de-
cides whether a new edit action should be executed by generating
an action-start keyword; (2) it preserves the current edit action
by generating a common token; and (3) it suspends the current
action until generating an action-end keyword. Finally, the decoder
produces a series of edit actions, and HatCUP generates an updated
comment based on the old comment and the corresponding edit
actions.

To evaluate our approach, we use the same dataset in previous
work[24, 28], which contains code-comment co-change samples
extracted from 1 496 popular engineered Java projects hosted on
GitHub.

In summary, the contributions of this paper include:
• Hybrid analysis and attention: We consider the code struc-
ture change information and introduce a structure-guided
attention mechanism combined with code change graph anal-
ysis and optimistic data flow dependency analysis. Based on
the multiple information about code changes(e.g., the action
of the code edits, the syntax, semantics and structure code
changes, and old comments), a structural representation of
the changes in the current code snippet can be generated.
• A new mechanism for comment updating: We propose a
new mechanism, called edit or non-edit mechanism. Instead of
directly generating a new comment sentence from scratch, the

edit or non-edit mechanism generates a sequence of edit ac-
tions and constructs a modified RNN model to integrate newly
developed component to mimic human editing behavior.
• Better performance: HatCUP is shown to outperform the
two state-of-the-art techniques and can reduce developers’
efforts in updating comments. The results show that HatCUP
outperforms CUP by 53.8% for accuracy, 31.3% for recall and
14.3% for METEOR of the original metrics. Compared with
HEBCUP, HatCUP also shows better overall performance.
• Open Source:We open source the replication package of our
work, including the dataset, the source code of HatCUP, our
trained model and test results. All data in the study are publicly
available at GitHub1.

The rest of this paper is organized as follows: the motivating
example is presented in Section 2. The technical details of HatCUP
are described in Section 3. The evaluation for our approach is shown
in Section 4. Section 5 discusses the situations where our approach
may fail and the threats to validity. Related work and conclusions
are in Section 6 and Section 7.

2 MOTIVATION
Some comment updating approaches based on neural model learn-
ing and heuristic rules are not sufficiently effective beyond simple
updates [24]. We take CUP [26, 28] and HEBCUP [24] as examples
to demonstrate these limitations.

Figure 1 shows an example of stale comments we found in a
real-world GitHub repository, Jitsi. In an earlier version, a project
developer added a judgment condition (lines 2-3) in the method
parse() to check if the variable text is null. However, the developer
forgot to update the comment associated with this method, leading
to a case of inconsistent comment. Fortunately, a developer found
this problem and updated the comment later.

To realize the goal of updating automatically, CUP leverages a
neural sequence-to-sequence model to learn comment update pat-
terns from old comments and changed code tokens, and generates
a new comment "returns the null text". However, since the return
value of the target API parse() has another value assignment related
to the variable builder in line 6 and the variable text in lines 1-4,
which is not included in the changed token sequence, the informa-
tion of structure and data flow dependency in code is ignored. CUP
only gives comments about null and lacks the description "processed
message" related to the original return value in line 6.

As a heuristic-based approach, HEBCUP applies updates of the
code (sub)tokens to the corresponding comment (sub)tokens by
matching the (sub)token in the old comment with the (sub)token in
the old code one by one. For instance, if a method name is changed
from "getX" to "getY", its comment may be updated from “return
X” to “return Y”. Its obvious disadvantage is that if the tokens of
the code change do not match any tokens of the old comment, no
updates can be made. Therefore, as shown in Figure 1, HEBCUP
could not add the new return value "null" to the comment.

We propose HatCUP to provide a new mechanism for comment
updating. In addition to the token sequences of code change and old
comment, HatCUP considers more about the code structure change

1https://github.com/HATCUP0/hatcup

HatCUP: Hybrid Analysis and Attention based Just-In-Time Comment Updating

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Figure 1: Motivating Example

information and introduces a structure-guided attention mecha-
nism combined with code change graph analysis and optimistic
data flow dependency analysis. Hence, the addition of structure
about return value null and builder.toString(), and the related data
flow dependency about text and builder can be caught. Instead of
generating new comments, we propose a new edit and non-edit
mechanism to capture the edit action and construct a modified
RNN model to integrate newly developed components, with the
core idea of mimicking human editing behaviors. Finally, the new
comment about the insert action "or null" is updated successfully.

3 APPROACH
Our approach, HatCUP, consists of three phases: edit representa-
tion, model training, and comment updating. Specifically, for each
code-comment co-change sample extracted from source code repos-
itories, we first represent them as edit sequences. Then, our model
is trained using the preprocessed data. Finally, given code edits,
syntax changes and associated the old comment, the trained model
can automatically update the old comment to a new comment. In
this section, we elaborate on the steps of our approach.

3.1 Representing Edits
In this phase, we convert code changes and comments into se-
quences. Different from CUP [28], we adopt a new representation
of code and comment changes. We will describe it in detail in 3.1.2.

3.1.1 Data Pre-Processing. In the preprocessing, each code snippet
is split into tokens, and each identifier is tokenized based on camel
casing and snake casing. For comments, HTML tags and comment
symbols(e.g., "/*" and "//") are all filtered out. Then, each string will
be tokenized by space. After that, compound words, which are the
tokens constructed by concatenating multiple vocabulary words
according to camel or snake conventions, are split into multiple
tokens to reduce OOV (Out Of Vocabulary) words.

3.1.2 Text Change Representation of Code and Comment. After
tokenization, the old and new code snippets are converted to two

token sequences separately. We use difflib2 to extract code edits. The
code token sequence pair consists of a series of edit actions, which
means editing an old code snippet to the new one. We construct
each edit action as <Action> [tokens] </Action>, which has proven
to be highly effective in other tasks in preliminary experiments
[33]. We define four types of editing actions in our work: INSERT,
DEL, UPDATE and KEEP. Notably, UPDATE action must incorporate
content both before and after the update, thus explicitly indicating
which tokens in the old comment are to be replaced with the new
tokens, so it has a slightly different structure:

<UPDATEFROM> [old tokens]
<UPDATETO> [new tokens]
</UPDATE>

Especially, the comment edit representation is slightly different
from the code representation. During inference, we only need to
know the position and information of the changes made to the old
comment. Therefore, we do not consider KEEP type when building
the sequence of edit actions, since we can copy tokens that are
retained between the old and new comments, instead of generating
them anew. For DEL and UPDATE, we can remove or replace ex-
actly the corresponding content from the old comment. For INSERT
action, we design a method to determine which position of the old
comment should be edited. Considering the example in Figure 1, the
raw sequence <INSERT> or null ... </INSERT> does not contain infor-
mation about where the new string should be inserted. Therefore,
we select the minimum number of tokens before the insert position
as a tag, so the place of insertion can be uniquely identified. Conse-
quently, we will generate the edit action: <INSERTTAG> message
<INSERT> message or null if text message was null </INSERT>. Similar
to the process of UPDATE, this sequence indicates that “message”
should be replaced with “message or null if text message was null,”
effectively inserting “or null if text message was null” into the old
comment.

2https://docs.python.org/3/library/difflib.html

Motivating Example: jitsi/jitsi(stars:3.5k)Commit Id: c868e951public static String parse(String text) {2+if (text == null)3+return null;4StringBuilder builder = new StringBuilder(text);5…6return builder.toString();7}Old Comment: returns the processed messageNew Comment:  returns the processed message or null if text message was nullCUP: returns the null textHEBCUP: returns the processed messageHatCUP: returns the processed message or nullnulltextmismatchmismatchmismatchmismatchgenerategenerategenerategenerateCUPHEBCUPHatCUPnon-editnon-editnon-editnon-editinsertinsertDo no updatesGenerate new comment from scratch, focus on changed tokensEdit or non-edit mechanism, structure analysisreturnstheprocessedmessageornulliftextmessagewasnullGround TruthreturnsthereturnstheprocessedmessageornullreturnstheprocessedmessageHeuristic-based approach“if”, “(”, “text”, “=”, “=”, “null”, “)”, “return”, “null”“returns”, “the”, “processed”, “message”old comment tokenschanged code tokensreturnnullBuilder.toString()code change & structure-guided attention&…ICPC ’22, May 16–17, 2022, Virtual Event, USA

Zhu, He and Xu

Figure 2: The structure-guided attention

Syntax Change Representation. AST (Abstract Syntax Tree)
3.1.3
provides crucial structure information for code understanding [10].
Given a pair of old source code 𝐶1 and new source code 𝐶2, we can
obtain syntax change information with GumTreeDiff [7]. Variable
nodes in the syntax tree will be used as the input of the syntax
change encoder. However, this is not enough to help the model
obtain structural information. We introduce a new attention mech-
anism for the comment updating scenario, the structure-guided
attention shown in Figure 2.

In Figure 2, a code change graph is created by analyzing the syn-
tax change information provided by GumTreeDiff. Each node in this
graph is a triple tuple 𝑛𝑖 = <𝑂𝑝𝑒𝑟𝑎𝑡𝑖𝑜𝑛,𝑇𝑦𝑝𝑒, 𝑉 𝑎𝑙𝑢𝑒>. 𝑂𝑝𝑒𝑟𝑎𝑡𝑖𝑜𝑛
means the edit operation of a node from 𝐶1 to 𝐶2, namely: keep,
insert, del, update; 𝑇𝑦𝑝𝑒 is the syntax type of a node, such as "Sim-
pleName", "IfStatement", "Assignment" and so on; 𝑉 𝑎𝑙𝑢𝑒 is the value
of a node (if the node has).

To efficiently generate comment fragments for modified parts
of the code, we propose change-guided attention. It focuses on the
variable nodes in the code change graph involved in the change and
their associated nodes. More formally, we introduce the following
change-guided attention matrix:

𝑀𝑖 𝑗 =

(cid:40)

0
-inf

if (cid:10)𝑛𝑖, 𝑛 𝑗 (cid:11) ∈ 𝐴 and 𝑛𝑖 /𝑛 𝑗 ∈ 𝐶𝑁
otherwise

(1)

𝐴 is the set of node pairs (cid:10)𝑛𝑖, 𝑛 𝑗 (cid:11), in which 𝑛𝑖 and 𝑛 𝑗 are the same
node (i.e., 𝑖 = 𝑗) or nodes of the same value, or there is an assignment
relationship between them. 𝐶𝑁 means the changed nodes set. After
this attention matrix is passed to the softmax logical regression, all
the parts set to -inf will be ignored in the subsequent calculations.

Consequently, the change-guided attention is designed to block out
information other than changes.

We also constructed the data flow dependency graph. On the
one hand, the data flow contains semantic code information, which
is crucial for code understanding. On the other hand, the data flow
supports the model in considering long-term dependencies induced
by using the same variables in distant locations. For example, there
are six variables with the same name (i.e. 𝑚𝑖𝑛4, 𝑚𝑖𝑛8, 𝑚𝑖𝑛10, 𝑚𝑖𝑛12,
𝑚𝑖𝑛14 and 𝑚𝑖𝑛16) but different semantics in the data flow depen-
dency graph in Figure 2. The graph demonstrates dependencies
between variables and supports 𝑚𝑖𝑛16 in paying more attention to
𝑚𝑖𝑛8, 𝑚𝑖𝑛10 and 𝑚𝑖𝑛14 instead of 𝑚𝑖𝑛4.

We determine the data flow dependencies based on each variable
object’s Output-flow and Input-flow. Inspired by PyART [14], we
define a set of constraint-based optimistic data flow dependency
extraction rules according to the target of our task. Such data flow
is neither sound nor complete, and it just appears to be concise and
largely precise, thus facilitating our incorporation of this informa-
tion in the model. As shown in Table 1, we have summarized 11
output or input-flow patterns, including: MethodParameter, Postfix-
Expression, Assignment(left-hand), PrefixExpression, InfixExpression,
PostfixExpression, ContainerAccess, MethodInvocation, ReturnState-
ment and Assignment(right-hand) rules. We filtered out some pat-
terns defined as 𝑃𝑟𝑒𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛, because they are not common and
therefore cannot provide enough data to support the learning of
the model (and hence should be killed). We construct data flow
dependencies from 𝑂𝑈𝑇 (𝑀) collection to 𝐼 𝑁 (𝑀) collection. For
each variable 𝑣 in 𝐼 𝑁 (𝑀), there must be a flow from the nearest
variable 𝑣 in 𝑂𝑈𝑇 (𝑀) before it. Taking the method in Figure 2

Source CodeCode Change GraphDataflow Dependency Graphpublic int compare(int a, int b) {int max=0;if(a>b){max=a;}else{max=b;}return max;}public int compare(int a, int b, int c) {int min=0;if(a>b){min=b;}else{min=a;}if(min>c){min=c;}return min;}a    b    c  min   0    a    b  min  b    …  abcmin0minminabbamincminminc000001000000000101000000000000000000000100000000000000000000000000000000000100000abcmin0abminb    …Dependency-guided attentionBefore change:After change:DependencyChange-guided attentionStructure-guided attentiona    b    c  min   0    a    b  min  b    …  -inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf0-inf0-inf-inf0-inf-inf-inf-inf-inf-inf-inf-inf-inf00-inf-inf-inf0-inf-inf-inf0-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf0-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf0-inf-inf0-inf0-inf-inf-inf-inf0abcmin0abminb    …+-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf0-inf0-inf-inf0-inf-inf-inf-inf-inf-inf-inf-inf-inf00-inf-inf-inf0-inf-inf-inf0-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf0-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf-inf0-inf-inf0-inf0-inf-inf-inf-inf00000010000000001010000000000000000000001000000000000000000000000000000000001000000167234581091311121415HatCUP: Hybrid Analysis and Attention based Just-In-Time Comment Updating

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Table 1: Constrained data flow extraction rules

Pattern name

Output-flow

Input-flow

Sub-pattern name
𝑀𝑒𝑡ℎ𝑜𝑑𝑃𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟
𝑃𝑜𝑠𝑡 𝑓 𝑖𝑥𝐸𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛
𝐴𝑠𝑠𝑖𝑔𝑛𝑚𝑒𝑛𝑡(left-hand)
𝑃𝑟𝑒 𝑓 𝑖𝑥𝐸𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛
𝐼𝑛𝑓 𝑖𝑥𝐸𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛
𝑃𝑜𝑠𝑡 𝑓 𝑖𝑥𝐸𝑥𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛
𝐶𝑜𝑛𝑡𝑎𝑖𝑛𝑒𝑟𝐴𝑐𝑐𝑒𝑠𝑠
𝑀𝑒𝑡ℎ𝑜𝑑𝐼𝑛𝑣𝑜𝑐𝑎𝑡𝑖𝑜𝑛
𝑅𝑒𝑡𝑢𝑟𝑛𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡
𝐴𝑠𝑠𝑖𝑔𝑛𝑚𝑒𝑛𝑡(right-hand)
-

Flow derivation
{∀𝑒𝑖 ∈ 𝐸, ∀𝑢 ∈ 𝑒𝑖 } ⊆ 𝑂𝑈𝑇 (𝑀)
{∀𝑒𝑖 ∈ 𝐸, ∀𝑢 ∈ 𝑒𝑖 } ⊆ 𝑂𝑈𝑇 (𝑀)
{∀𝑣 ∈ 𝑂 (𝑙)} ⊆ 𝑂𝑈𝑇 (𝑀)
{∀𝑒𝑖 ∈ 𝐸, ∀𝑢 ∈ 𝑒𝑖 } ⊆ 𝐼 𝑁 (𝑀)
{∀𝑒𝑖 ∈ 𝐸, ∀𝑢 ∈ 𝑒𝑖 } ⊆ 𝐼 𝑁 (𝑀)
{∀𝑒𝑖 ∈ 𝐸, ∀𝑢 ∈ 𝑒𝑖 } ⊆ 𝐼 𝑁 (𝑀)
{∀𝑣 ∈ 𝐶, ∀𝑢 ∈ 𝑒} ⊆ 𝐼 𝑁 (𝑀)
{∀𝑣 ∈ 𝐸, ∀𝑒𝑖 ∈ 𝐸, ∀𝑢 ∈ 𝑒𝑖 } ⊆ 𝐼 𝑁 (𝑀)
{∀𝑒𝑖 ∈ 𝐸, ∀𝑢 ∈ 𝑒𝑖 } ⊆ 𝐼 𝑁 (𝑀)
{∀𝑒 ∈ 𝑂 (𝑟 ), ∀𝑢 ∈ 𝑒} ⊆ 𝐼 𝑁 (𝑀)
{∀𝑒𝑖 ∈ 𝑘𝑖𝑙𝑙 (𝐸), ∀𝑢 ∈ 𝑒𝑖 } ⊈ 𝐼 𝑁 (𝑀) ∪ 𝑂𝑈𝑇 (𝑀)

Example
𝑓 (𝑒1, ..., 𝑒𝑛)
𝑒++
𝒗 = 𝑒
!𝑒
𝑒1 < 𝑒2
𝑒++
𝑣 [𝑒]
𝑣.𝑓 (𝑒1, ..., 𝑒𝑛)
𝑟𝑒𝑡𝑢𝑟𝑛 𝑒1, ..., 𝑒𝑛
𝑣 = 𝒆
-

Preservation
1 𝑒 ∈ 𝐸 represents an expression; {𝑢, 𝑣 } ⊆ 𝑒 represents a variable object; 𝑂 (𝑙) and 𝑂 (𝑟 ) represent the left-hand and right-hand operand; 𝑀

is a method; 𝑂𝑈𝑇 (𝑀) contains all the variable objects which has an output-flow.

as an example, 𝑎5 (in 𝑎 > 𝑏, InfixExpression) belongs to 𝐼 𝑁 (𝑀),
the nearest 𝑎 ∈ 𝑂𝑈𝑇 (𝑀) before 𝑎5 is 𝑎0 (in 𝑐𝑜𝑚𝑝𝑎𝑟𝑒 (𝑖𝑛𝑡 𝑎, ...),
𝑀𝑒𝑡ℎ𝑜𝑑𝑃𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟 ). Consequently, there is a data flow from 𝑎0 to
𝑎5. In addition, there must be a flow from the right-hand operand
to the left-hand operand in an 𝐴𝑠𝑠𝑖𝑔𝑛𝑚𝑒𝑛𝑡 expression.

To represent the dependency relationship, we take a direct edge
𝜀 = ⟨𝑛𝑖, 𝑛 𝑗 ⟩ from 𝑛𝑖 to 𝑛 𝑗 , which means that the value of the 𝑗-th
node comes from the 𝑖-th node. The following dependency-guided
attention matrix represents the dependency relationship:

𝑀𝑖 𝑗 =

(cid:40)

1
0

if (cid:10)𝑛𝑖, 𝑛 𝑗 (cid:11) ∈ 𝐸
otherwise

(2)

𝐸 is the set of directed edges, {𝜀1, 𝜀2, ..., 𝜀𝑛 }. Similar to the change-
guided attention matrix, the dependency-guided attention matrix
will also be passed to the softmax logical regression; after calcula-
tion, the parts set to 1 will have a higher attention score than the
parts set to 0, which means that the model will pay more attention
to the dependency relationship among the variables.

Finally, we compute the weighted sum of change-guided at-
tention and relation-guided attention matrices to obtain the final
structure-guided attention matrix. We filter the variable nodes as-
sociated with the change for the task of comment updating and try
to preserve the dependencies among the nodes. In summary, we
do not serialize the traversal of AST nodes as previous work and
input the flattened sequence, which results in corrupted structural
information.

3.2 Overview of Our Model
Figure 3 is the architecture of our model. Our model leverages three
Bi-GRU (Bi-Directional Gated Recurrent Unit) encoders, i.e., Code
Edits Encoder, Syntax Change Encoder and Old Comment Encoder,
and then generates a sequence of edit actions for the old comment
through a GRU decoder. An encoder-side co-attention mechanism
is leveraged to learn the relationships among code text change, code
structure change and old comment. After the generation of the edit
sequence, a parser will apply the edit actions to the old comment
to obtain the updated comment.

3.3 Encoders
After extracting as features the sequences of sub-tokens and nodes
for the codes, we need to convert those sequences into vector rep-
resentations for the models in the later steps. In detail, HatCUP
leverages three different encoders, to encode code edit sequences,
syntax change sequences and old comments, respectively. We use
multiple GRUs for the different structures and types of informa-
tion. Multiple GRUs also help reduce the cross-influence between
different contexts.

3.3.1 Token Embedding Layer. This layer is designed to map the
various tokens, i.e., code tokens, comment tokens, edit action to-
kens and variable node tokens, into embeddings. We first created a
vocabulary separately for code, variable nodes and comments. We
choose to train the embedding layers from scratch instead of using
a pretrained model, since the pretrained model may not contain
some tokens, which will result in low effectiveness. Especially, for
syntax change, we put the values of variable nodes as tokens into
the encoder; as for old comments, we input only the split subtokens.

Feature Fusing Layer. After the initial embedding of the to-
3.3.2
kens, we select extra features for each token, which has proven to be
effective in learning associations between source code entities and
comments [30]. These features are represented as one-hot vectors
and are concatenated to code edit token embeddings or comment
token embeddings.

For code edit sequences, which contain edit keywords, Java key-
words, operators, variable names, etc., we need to make the model
distinguish these tokens. If a token is not an edit keyword, we
have indicator features for whether it is part of an INSERT, DEL,
UPDATEFROM, UPDATETO, or KEEP span. These features are par-
ticularly helpful for longer spans, as the edit keyword only appears
at the beginning or end of the span. In addition, the tokens in return
statements usually appear in comments, and we introduce these
features to guide the model in identifying relevant tokens in the
code edits sequence and the old comment sequence.

For syntax change, we also have indicator features such as that in
code edits tokens, telling the model which operation a node belongs
to, i.e., keep, insert, del, update. Additionally, we take Type into
consideration. According to a large-scale empirical study conducted

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Zhu, He and Xu

Figure 3: Architecture of HatCUP

by Wen et al. [45], change types Variable Declaration and Selection
are among those more likely to trigger comment updates, at the
method and class level. These changes could severely impact the
application logic (selection) or the data manipulated in the code
Variable Declaration.

Regarding old comments, we include whether a token matches
an AST node that is insert, del, update in AST-diff. This treatment
helps align parts of old comments with AST changes, helping the
model determine where the edits should be made.

3.3.3 Modeling Layer. This layer produces the hidden states of
each token based on its contextual vector. For each context, we
encode it with a GRU. In our approach, the three GRUs share a
similar structure.

As shown in Figure 3, the input for each GRU is the sequence 𝑉
of 𝑛 vectors representing a context. Each vector represents a sub-
token combining other features. For example, one GRU is used to
obtain the hidden states of code edits contextual vectors. For each
time step 𝑡, we input one vector 𝑉𝑡 in these 𝑛 vectors, and the GRU
returns one hidden state vector ℎ𝑡 as the output for this time step.
By collecting all outputs for each time step, we have a sequence
of hidden state vectors 𝐻 = [ℎ1, ℎ2, ..., ℎ𝑛], which is the output of
the GRU. The whole procedure can be expressed by Formula 3. The
other two GRUs share the same process.

ℎ𝑡 = 𝑓 (𝑣𝑡 , ℎ𝑡 −1)

(3)

3.4 Attention Layer
In the modeling layer, we obtain the hidden states 𝐻 of the three
GRUs separately. However, code edit, syntax change and old com-
ment are represented independently. We cannot directly use them
as input to the decoder to generate the result sequence. It is neces-
sary to link and fuse their information to capture the relationships

between different contexts. Therefore, we design three attention
layers.

For each encoder, an attention layer is obtained by weighting
and summing the outputs of all its timings. This attention layer
contains information about the weight of each timing output, which
is equivalent to identifying which text is important for the current
token in the decoder. For instance, Code Edit Attention is used to
identify the parts of the code relevant to the target edit sequence to
be generated; Old Comment Attention is used to identify the notes
needing to be edited in old comments. Syntax Change Encoder will
be slightly different, and it uses the structure-guided attention we
defined in Section 3.1.3.

For convenience, we take Code Edit Attention as an example. The
attention layer takes as input the code edits contextual vectors,
i.e., 𝐻 = [ℎ1, ℎ2, ..., ℎ𝑛], and outputs an attention-aware contextual
vector 𝐶 = [𝑐1, 𝑐2, ..., 𝑐𝑛] for each edit token in Code Edits Attention.
𝑐𝑡 in 𝐶 is calculated as the weighted sum of the encoder’s hidden
states:

𝑛
∑︁

𝑐𝑡 =

𝛼𝑡𝑖ℎ𝑖

𝑖=1
𝑒𝑟 (ℎ′
(cid:205)𝑛
𝑖′≠𝑖 𝑒𝑟 (ℎ′

𝑡 −1

,ℎ𝑖 )

,ℎ𝑖′)

𝑡 −1

𝛼𝑡𝑖 =

(4)

(5)

𝑡 −1

where ℎ′
is the previous hidden state in the decoder, and 𝑟 is the
function used to represent the strength for attention, approximated
by a multi-layer neural network.

3.5 Decoder
By combining all the contextual vector outputs 𝐶 of all attention
layers, we obtain a joint context; thus, the corresponding content
from three input sequences is merged. We use a GRU as the decoder
to generate a series of edit actions. At every decoding step, the

Code Edits SequenceOld Comment SequenceSyntax Change SequenceToken Embedding LayerModelingLayerAttentionLayerh1𝑒2𝑒3𝑒1𝑒1𝑒2𝑒3h2h3h1h2h3h1h2h3……Code Edits  AttentionOld Comment AttentionSyntax Change Attention…………GRUGRUGRUDecoderh'1h'2h'3…𝑓1𝑓2𝑓3𝑓1𝑓2𝑓3𝑒1′𝑒2′𝑒3′𝑒2𝑒3𝑒1𝑓1𝑓2𝑓3𝑒1′𝑒2′𝑒3′𝑒1′𝑒2′𝑒3′Feature Fusing Layer++Rebuild Comment<KEEP> public </KEEP>…int     List     Table …Returns   the    number…returns the numberof editable columns.number…returns the     listof editable columns.updateOld Comment:New Comment:Structure-guidedattentionEdit Sequence<UPDATEFROM>h’4list<UPDATETO>HatCUP: Hybrid Analysis and Attention based Just-In-Time Comment Updating

ICPC ’22, May 16–17, 2022, Virtual Event, USA

𝑡 −1

previous hidden state ℎ′
is used as the input for the attention
layer and the output of the attention layer will be used as the
input of the GRU at time step 𝑡. Therefore, the resulting vector
contains information related to the current decoder state together
with knowledge aggregated from relevant parts of code edits, AST-
diffs and old comments.

Different from the previous work, we do not generate a full new
comment. Instead, we generate a series of edit actions to show how
to update the old comment. Specifically, the decoder must determine
how the source code changes associated with the current decoding
step will change the relevant parts of the old comment. At each
step, the decoder decides whether a new edit action should be
executed by generating an action-start keyword from INSERT, DEL
or UPDATE and continues the current edit action by generating a
comment token; it will not stop the current action until an action-
end keyword is generated. Since DEL will include tokens in the old
comment, and INSERT tends to include tokens in the code, we add
a pointer network to the decoder [41] to accommodate copying
tokens from code and comment. The decoder generates a series of
edit actions. Consequently, we can generate an updated comment
by parsing the old comment and the corresponding edit actions.

3.6 Parsing Edit Sequences
Since the decoder gives us a series of edit actions, we should align it
with the old comment and apply it to obtain the updated comment.
We denote the old comment as 𝑆𝑜𝑙𝑑 , the predicted edit actions as
𝑆𝑒𝑑𝑖𝑡 and the corresponding parsed output as 𝑆𝑛𝑒𝑤. This procedure
involves simultaneously following pointers, from left to right, on
𝑆𝑜𝑙𝑑 and 𝑆𝑒𝑑𝑖𝑡 , which we refer to as 𝑃𝑜𝑙𝑑 and 𝑃𝑒𝑑𝑖𝑡 respectively. As
𝑃𝑜𝑙𝑑 moves forward, the current token is copied into 𝑆𝑛𝑒𝑤 at each
point, until the pointer reaches an edit location. Then 𝑃𝑒𝑑𝑖𝑡 applies
the edit action of the current position, and the span tokens corre-
sponding to the action are copied into 𝑆𝑛𝑒𝑤 if applicable. Finally,
𝑃𝑒𝑑𝑖𝑡 moves to the next action; so do cases involving deletions and
replacements; 𝑃𝑜𝑙𝑑 is also advanced to the appropriate position.
This process will repeat until the two pointers reach the end of
their respective sequences.

4 EVALUATION
4.1 Dataset
We use the same dataset in HEBCUP [24] and CUP [28]. The au-
thors of the two works built a dataset from 1,496 Java projects
hosted on GitHub and design rules to automatically filter out some
types of syntactic optimizations (i.e., the old and new comments
are of the same meaning) which may introduce bias. The cleaned
dataset finally contains 80,591, 8,827, and 9,204 method-comment
co-change samples for training, validation, and test sets, discarding
6,183 instances in total.

4.2 Research Questions
To evaluate HatCUP, we propose the following research questions.
RQ1: How effective is HatCUP compared with the two state-of-

the-art approaches, CUP and HEBCUP?

RQ2: How do the key components of HatCUP affect the result?
RQ3: How effective is HatCUP when dealing with complex sce-

narios?

4.3 Experiment Setup
We conducted our experiment on Ubuntu 18.04.6 with Intel(R)
Xeon(R) Gold 5118 CPU @ 2.30GHz. We utilized 1 NVIDIA Tesla
V100 GPU to train and evaluate our model. The model was im-
plemented in Python 3 with PyTorch V1.10.0. For our approach,
64-dimensional word embeddings are used for code edits tokens,
AST-diff tokens and comment tokens. The hidden states of the Bi-
GRUs (encoder) and the GRU (decoder) in our model are 64 and 128
dimensions respectively. All GRUs have two layers.

In our model, Code Edit Encoder, Syntax Change Encoder, Old
Comment Encoder and the decoder are jointly trained to minimize
the cross-entropy. During the training phase, we optimized the
parameters of our model using Adam [20] with a batch size of 32.
We set the learning rate of Adam to 0.001. A dropout [36] of 0.6 is
used for dense layers before computing the final probability. The
model with the best (smallest) validation perplexity is used for
evaluation. A beam search of width 5 is used to generate the target
sequence when testing.

4.4 Evaluation Metrics
We use Accuracy, Recall@5, METEOR [3], SARI [49], GLUE [29]
and two metrics proposed by the authors of CUP [28] for this task,
namely Average Edit Distance (AED) and Relative Edit Distance
(RED), to evaluate our approach and the baselines.
Our evaluation metrics are defined as follows:

• Accuracy: Accuracy represents the proportion of the test
samples where correct comments are generated at Top-1
among the total number of cases examined. Here, correct
comments refer to those that are identical to the ground-
truth (i.e., written by developers).

• Recall@5: Similar to Accuracy, Recall@5 is the proportion
of the test samples where correct comments are generated at
Top-5.

• AED: AED measures the average word-level edit distance
required to change the predicted results from CUP into the
ground-truth. This value indicates the distance between the
generated comments and the ground truth: the smaller, the
better. The AED metric is defined as follows:

𝐴𝐸𝐷 =

1
𝑁

𝑁
∑︁

𝑘=1

edit_distance (cid:16)

^𝒚 (𝑘), 𝒚 (𝑘) (cid:17)

(6)

Where 𝑁 is the number of test samples, 𝑒𝑑𝑖𝑡_𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 is
the word-level Levenshtein distance and ^𝒚 (𝑘) refers to the
comment generated for the 𝑘𝑡ℎ sample.

• RED: RED is similar to AED, but measures the average of
relative edit distances. The RED metric is defined as follows:
edit_distance (cid:16)

1
𝑁

𝑁
∑︁

𝑘=1

^𝒚 (𝑘), 𝒚 (𝑘) (cid:17)
edit_distance (cid:0)𝒙 (𝑘), 𝒚 (𝑘) (cid:1)

𝑅𝐸𝐷 =

(7)

Where 𝒙 (𝑘) is the old comment for the 𝑘𝑡ℎ sample. If an
approach’s RED is less than 1, and developers can expect to
spend less effort updating comments by using this approach.
• METEOR: METEOR (Metric for Evaluation of Translation
with Explicit ORdering) is a metric for the evaluation of
machine-translation output. The metric was designed to fix

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Zhu, He and Xu

Table 2: Comparisons of our approach with each baseline

Approach Accuracy Recall@5 AED RED
0.960
CUP
0.896
HEBCUP
0.861
HatCUP

26.8%
27.6%
35.2%

15.8%
25.6%
24.3%

3.62
3.52
3.44

Table 3: METEOR, SARI and GLUE scores

Approach METEOR SARI GLUE
CUP
HEBCUP
HatCUP

50.30
54.14
56.67

51.22
53.96
58.52

38.62
41.29
45.63

* The scores are presented as percentage values be-
tween 0 and 100.

some of the problems found in the more popular BLEU metric
and produce a good correlation with human judgment at the
sentence or segment level.

• SARI: SARI is a metric used initially for evaluating auto-
matic text simplification systems. The metric compares the
predicted simplified sentences against the reference and
the source sentences. It explicitly measures the goodness
of words added, deleted and kept by the system.

• GLEU: GLEU metric is a variant of BLEU proposed for eval-
uating grammatical error corrections using n-gram overlap
with a set of reference sentences, as opposed to precision or
recall of specific annotated errors.

4.5 Result Analysis
4.5.1 RQ1: The Effectiveness Evaluation. To evaluate the effective-
ness of our proposed model, HatCUP, we evaluate it and the baseline
methods on the testing set in terms of various metrics. The evalua-
tion results for the dataset are shown in Table 2 and Table 3. From
the tables, we can observe the following:

• HatCUP is slightly below HEBCUP by 5% in terms of the
accuracy metric, and there may be several reasons. First, the
ground-truth is rather subjective as a modified comment by
real developers, while the use of the accuracy metric in gen-
eration tasks is very demanding. It is not easy to guarantee
that the modifications inferred by the model are consistent
with the developer’s, unless they are relatively simple modi-
fications (i.e., modifying a single token). Second, HEBCUP
is a heuristic-based approach specifically designed for this
scenario, which pays attention to the changed code and per-
forms token-level comment updates. While for CUP, which
is also a deep learning method, HatCUP outperforms it in
terms of accuracy by more than 50%.

• For the Recall@5 metric, HatCUP achieves the best results.
We attribute this to the following reasons: First, it introduces
the syntax change encoder and structure-guided attention
mechanism compared to previous work, which utilizes code
structure change hints. In addition, our newly developed edit
or non-edit mechanism can imitate the behavior of human
developers to a large extent, which does not generate a full
new comment but generate a series of edit actions to show

how to update the old comment. So the comment will be
updated as wishes.

• HatCUP also outperforms the state-of-the-arts in terms of
AED and RED. The AED metric drops from 3.52 to 3.44,
which means for each comment, the developer can edit fewer
words on average with HatCUP compared to the other tools.
The lower RED metric also indicates that our approach can
reduce the edits developers need to perform for just-in-time
comment updating.

• Our model is better at editing comments, as shown by the
results on METEOR, SARI, and GLEU in Table 3. The three
metrics are flexible in word order and are often used to
evaluate comment generation methods in prior studies.
In general, considerable improvements are achieved by HatCUP
over CUP in terms of all metrics. Compared to HEBCUP, HatCUP
performs much better on Recall@5 and outperforms it in AED and
RED by substantial margins. This highlights that our approach
can update comments more effectively and accurately than the
baselines.

4.5.2 RQ2: The Effects of Key Components. The key of our com-
ment updating task is to effectively capture the relationship and
references between code changes and comments. Previous deep
learning-based work, i.e., CUP, has considered code changes and
old comments. To better capture the potential code change infor-
mation, we introduced a syntax change encoder together with
a structure-guided attention mechanism. Additionally, we try to
model the edit actions rather than generate comment sequences
from scratch, defined as edit or non-edit mechanism. Therefore, we
want to determine if the two key components would improve the
task of comment updating. To this end, we compare HatCUP with
its two variants: 1) HatCUP-syntax, which does not use the Syntax
Change encoder and the structure-guided attention mechanism, and
2) HatCUP-edit, which removes the edit or non-edit mechanism
from HatCUP, generating the new comment directly instead of edit
actions. The results are shown in Table 4. It can be seen that:

• HatCUP performs better than two variants in terms of all met-
rics. For accuracy, the improvements achieved by HatCUP
range from 2.0% to 8.5%; for Recall@5, HatCUP improves by
at least 4.2%, which means HatCUP can generate more cor-
rect comments than the variants. For AED and RED, HatCUP
still achieves the lowest result. HatCUP minimizes the num-
ber of editing operations required for developers to update
the old comments.

• HatCUP-edit achieves the worst performance. We manually
inspected the test results to determine why the performance
declined so much. Based on our inspection, we find that
HatCUP-edit model tends to generate the same comments
as the old comments. Since the old comment and the new
comment are closely related, training a model to directly
generate a new comment risks having it learn to just copy
the old one.

• The introduction of the Syntax Change Encoder and the
structure-guided attention mechanism improves the effec-
tiveness of the model to a certain extent. From another point
of view, even though HatCUP-syntax does not obtain the top
results as HatCUP, it still achieves considerable performance

HatCUP: Hybrid Analysis and Attention based Just-In-Time Comment Updating

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Table 4: Impact of the key components

Approach
HatCUP-syntax 22.3%
15.8%
HatCUP-edit
24.3%
HatCUP

Accuracy Recall@5 AED RED METEOR SARI GLUE

31.0%
27.7%
35.2%

3.49
3.59
3.44

0.890
0.933
0.861

56.23
53.18
58.52

44.19
41.27
45.63

54.86
50.40
56.67

manually define templates for comments whose updated contents
do not appear within the code change content. Therefore, it is
reasonable to assume that the instances HEBCUP cannot handle
correctly are more complex. We isolated instances from the test
dataset that HEBCUP could not handle, yielding a total of 6 844
samples. We evaluated HatCUP and the baselines on these samples.
The results are shown in Table 5. HatCUP performs much better
on those complex instances when compared to the baselines.

To better understand these performance differences, we manu-
ally inspect the test results of two examples, which have multiple
code changes, including <INSERT>, <DEL>, and <UPDATE>. Ex-
isting approaches cannot capture any change based only on text
analysis, while with multiple considerations, HatCUP can deal with
these changes successfully. Considering sample 1 in Figure 4, the
code snippet changed in the type of method and the return value,
and the developer only deleted the span "number of " in the com-
ment. It is noticeable that there is no overlap between the changed
tokens in the code snippet and those in the old comment. That is
why the two state-of-the-art techniques cannot generate the correct
comment (do not update the old comment). However, our model
recognized that the token number is not suitable for the new code
due to the update of the method type. Then, it updates the token to
list according to the new type List<...>, which is more suitable for
the changed new code. For sample 2, the developer modified one
method call, resulting in a change to the assignment of the variable
option. The variable option is an instance of the class GetWorkerOp-
tions. The developer’s intention was to test GetWorkerOptions. We
could not have given the correct suggestion without using AST to
establish associations among the individual variable nodes.

In conclusion, different from the two baselines, which use text
analysis to capture change information, HatCUP takes structure
code changes into account, which covers the shortage of baselines.
In addition, updating comments through an edit mechanism rather
than writing new comments from scratch also makes sense.

5 DISCUSSION
In this section, we discuss the situations where HatCUP may fail,
and the threats to the validity of this work.

5.1 Where Does Our Approach Fail
Although our method has proven superior to existing methods,
there are still scenarios where HatCUP does not perform perfectly.
A common bad situation is that the code changes are too massive
for HatCUP to handle perfectly. For example, the developer may
rewrite the entire method, including the method name. In this case,
the modified code can no longer be considered a variant of the
original method. There is no connection between the old comment
and the new code snippet. It is difficult for the model to update the
old comment correctly by applying some edit actions.

Figure 4: Case study

Table 5: Effectiveness in complex scenarios

Approach Accuracy Recall@5 AED RED
0.978
CUP
0.992
HEBCUP
0.963
HatCUP

8.8%
1.1%
20.5%

3.2%
0.0%
7.8%

4.36
4.41
4.27

(e.g., with recall@5 over 30%), which further confirms the
strength of our approach. Moreover, since HatCUP-syntax
does not rely on the AST information, it can be used as a light
variant of our approach, which can help developers update
comments just-in-time with incomplete code snippets.
In summary, HatCUP significantly outperforms the two variants.
These results demonstrate that the structure information and the
edit or non-edit mechanism are helpful and valuable for this task.

4.5.3 RQ3: The Effectiveness in Complex Scenarios. To cope with
comment updating in complex scenarios, three encoders are used
and various attention mechanisms are introduced. At the same
time, we wanted to check whether the edit or non-edit mechanism
could handle more complex updates. As illustrated by the authors,
HEBCUP, can only work on code-indicative updates. It is difficult to

1-public int getEditableColumns() {2-        return m_editableColumns.size();3+public List<TableProperty> getEditableColumns() {4+        return m_editableColumns;5}1@Test2public void defaults() throws IOException {3-        GetWorkerOptions options = GetWorkerOptions.defaults();4+5        assertEquals(null, options.getBlockWorkerInfos());6-        assertEquals(0, options.getBlockId());7-        assertEquals(0, options.getBlockSize());8+        assertEquals(0, options.getBlockInfo().getBlockId());9+        assertEquals(0, options.getBlockInfo().getLength());…HEBCUP: Tests for defaults {@link Createoptions.getBlockInfo}.HatCUP: Tests for defaults {@link GetWorkerOptions}.        GetWorkerOptions options = GetWorkerOptions.defaults().                                                     setBlockInfo(new BlockInfo());Sample 2: Alluxio/alluxio (stars:5.4k)Old Comment: Tests for defaults {@link CreateOptions}.New Comment: Tests for defaults {@link GetWorkerOptions}.CUP: Tests for defaults {@link CreateBlockInfo}.Sample 1: alkacon/opencms-core (stars:467)Old Comment: Returns the number of editable columns.HEBCUP: Returns the number of editable columns.New Comment: Returns the editable columns.HatCUP: Returns the list of editable columns.CUP: Returns the number of editable columns.ICPC ’22, May 16–17, 2022, Virtual Event, USA

Zhu, He and Xu

Another situation, which we believe cannot simply be called a
failure, is the optimization of language expression. For instance,
the motivating example in Figure 1, whose comment was updated
with a conditional clause "... if text message was null", is a typical
example of this situation. HatCUP may correctly update some of the
comment phrases but not always all, which leads to inconsistency.

5.2 Threats to Validity
5.2.1 External Validity. A threat to external validity is related to
GumTreeDiff we used to obtain the difference between ASTs. There
is no guarantee that the syntax information extracted by GumTreeD-
iff is exactly correct. However, we manually checked 100 samples
in the dataset and found only one incorrect mapping. Therefore,
we believe the threat is limited.

Internal Validity. A threat to internal validity is related to
5.2.2
the dataset we used. For comparison with CUP and HEBCUP, we
directly used the dataset provided by them. The dataset is built only
from Java projects and only contains updates of method comments,
which may not be representative of all programming languages and
comment types. Another threat is that the performance of HatCUP
in solving complex cases is still not perfect. It needs more precise
program analysis and efficient NLP algorithms.

6 RELATED WORK
In this section, we discuss related work concerning code-comment
inconsistency detection, comment updating and comment gener-
ation. All these works focus on maintaining comments, but have
different emphases.

6.1 Code-Comment Inconsistency Detection
A large amount of work has been conducted by researchers to de-
tect inconsistent comments. Most prior works targeted comments
related to specific code properties [37–39]. For instance, Tan et al.
[37–39] proposed several approaches to detect the consistency be-
tween code and comment concerning specific code properties, such
as lock mechanisms [37, 38], function calls [37] and interrupts [39].
They use static program analysis to check whether the source code
conforms to specific rules. Some work has focused on specific types
of comments [9, 18, 34]. For example, Huang et al. [18] used the
text mining-based methods to predict whether a comment contains
self-admitted technical debt (SATD) (e.g., TODO, FIXME, HACK).
Sridhara [34] proposed a technique to identify obsolete TODO com-
ments based on information retrieval, linguistics and semantics.
Gao et al. [9] proposed a deep learning-based approach TDCleaner,
which outperforms Sridhara’s by a large margin. Several studies fo-
cused on general comments. Ratol et al. [32] designed a rule-based
approach named Fraco to detect fragile comments during identifier
renaming. Panthaplackel et al. [31] developed a deep learning-based
approach for just-in-time code-comment inconsistency detection
by learning to relate comments and code changes.

6.2 Comment Updating
Following the work of code-comment inconsistency detection, some
approaches have been proposed to focus on automatic comment up-
dating. Liu et al. [28] are the first to propose a just-in-time comment

updating technique, called CUP. The core idea of CUP is to leverage
a neural sequence-to-sequence model to learn comment update
patterns from old comments and changed code tokens; then, it can
update the comment in time after the developer modifies the code.
Lin et al. [24] performed an in-depth analysis on the effectiveness
of CUP. They found that most of the successful updating conducted
by CUP was related to a single token change. Therefore, for the case
of single token modification in the code, they proposed HEBCUP, a
heuristic-based approach, which achieves better performance on
CUP. However, HEBCUP is not sufficiently effective beyond simple
updates due to the limitation of heuristic rules.

6.3 Comment Generation
Source code comment generation has been studied by many re-
searchers previously. In earlier studies, scholars tend to use template-
based approaches [6, 11, 12]. However, a well-designed template
requires expert domain knowledge, which is not easy work. Con-
sequently, IR-based approaches [35, 46] have been proposed. To
generate comments for Java methods, Sridhara et al. [35] use sum-
mary information in source code and manually define templates.
ColCom [46] proposed an approach that generates comments by
reusing and tailoring comments of similar code snippets from open
source projects. However, the retrieved comments may not correctly
describe the semantics and behavior of code snippets, leading to the
mismatches between code and comments. Recently, Neural Machine
Translation (NMT) based models have been exploited to generate
summaries for code snippets. CodeNN [19] is an early attempt that
uses only code token sequences, followed by various approaches
that utilize AST [2, 15, 16, 21, 22, 25], API knowledge [17], type
information [4], global context [13, 25], reinforcement learning
[42, 43], multitask and dual learning [44, 48, 50], and pretrained
language models [8].

7 CONCLUSION
We propose a new approach, HatCUP, for just-in-time comment
updating. To the best of our knowledge, this is the first work that
considers the code structure change information. Combined with
code change graph analysis and data flow dependency analysis,
we introduce a syntax change encoder together with a structure-
guided attention mechanism to more fully utilize code structure
change hints. Additionally, the edit or non-edit mechanism, which
is aimed at generating a sequence of edit actions to mimic human
editing behavior, has proven better suited to the comment updat-
ing task than traditional approaches. Our results demonstrate that
HatCUP outperforms the two state-of-the-art techniques and can
substantially reduce developers’ efforts in updating comments.

ACKNOWLEDGMENTS
We thank the anonymous reviewers for their constructive com-
ments. This research was supported, in part by NSFC 61832009,
Cooperation Fund of Huawei-Nanjing University Next Genera-
tion Programming Innovation Lab (No. YBN2019105178SW27, No.
YBN2019105178SW32). Any opinions, findings, and conclusions
in this paper are those of the authors only and do not necessarily
reflect the views of our sponsors.

HatCUP: Hybrid Analysis and Attention based Just-In-Time Comment Updating

ICPC ’22, May 16–17, 2022, Virtual Event, USA

REFERENCES
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020.
A Transformer-based Approach for Source Code Summarization. In Proceed-
ings of the 58th Annual Meeting of the Association for Computational Linguis-
tics. Association for Computational Linguistics, Online, 4998–5007.
https:
//doi.org/10.18653/v1/2020.acl-main.449

[2] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Gen-
erating sequences from structured representations of code. arXiv preprint
arXiv:1808.01400 (2018).

[3] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MT evaluation with improved correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization. 65–72.

[4] Ruichu Cai, Zhihao Liang, Boyan Xu, Zijian Li, Yuexing Hao, and Yao Chen. 2020.
TAG: Type Auxiliary Guiding for Code Comment Generation. In Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics. 291–301.
[5] Sergio Cozzetti B. de Souza, Nicolas Anquetil, and Káthia M. de Oliveira. 2005.
A Study of the Documentation Essential to Software Maintenance. In Proceed-
ings of the 23rd Annual International Conference on Design of Communication:
Documenting & Designing for Pervasive Information (Coventry, United Kingdom)
(SIGDOC ’05). Association for Computing Machinery, New York, NY, USA, 68–75.
https://doi.org/10.1145/1085313.1085331

[6] Brian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, and Jeffrey C Carver. 2013.
Evaluating source code summarization techniques: Replication and expansion.
In 2013 21st International Conference on Program Comprehension (ICPC). IEEE,
13–22.

[7] Jean-Rémy Falleri, Floréal Morandat, Xavier Blanc, Matias Martinez, and Martin
Monperrus. 2014. Fine-Grained and Accurate Source Code Differencing. In
Proceedings of the 29th ACM/IEEE International Conference on Automated Software
Engineering (Vasteras, Sweden) (ASE ’14). Association for Computing Machinery,
New York, NY, USA, 313–324. https://doi.org/10.1145/2642937.2642982

[8] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing: Findings.
1536–1547.

[9] Zhipeng Gao, Xin Xia, David Lo, John Grundy, and Thomas Zimmermann. 2021.
Automating the removal of obsolete TODO comments. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 218–229.

[10] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCode{BERT}: Pre-training Code Representations
with Data Flow. In International Conference on Learning Representations. https:
//openreview.net/forum?id=jLoC4ez43PZ

[11] Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program com-
prehension with source code summarization. In 2010 acm/ieee 32nd international
conference on software engineering, Vol. 2. IEEE, 223–226.

[12] Sonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. 2010. On the
use of automated text summarization techniques for summarizing source code.
In 2010 17th Working Conference on Reverse Engineering. IEEE, 35–44.

[13] Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Im-
proved automatic summarization of subroutines via attention to file context. In
Proceedings of the 17th International Conference on Mining Software Repositories.
300–310.

[14] Xincheng He, Lei Xu, Xiangyu Zhang, Rui Hao, Yang Feng, and Baowen Xu.
2021. PyART: Python API Recommendation in Real-Time. In 2021 IEEE/ACM
43rd International Conference on Software Engineering (ICSE). IEEE, 1634–1645.
https://doi.org/10.1109/ICSE43902.2021.00145

[15] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment gener-
ation. In 2018 IEEE/ACM 26th International Conference on Program Comprehension
(ICPC). IEEE, 200–20010.

[16] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment
generation with hybrid lexical and syntactical information. Empirical Software
Engineering 25, 3 (2020), 2179–2217.

[17] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
source code with transferred API knowledge. In Proceedings of the 27th Interna-
tional Joint Conference on Artificial Intelligence. 2269–2275.

[18] Qiao Huang, Emad Shihab, Xin Xia, David Lo, and Shanping Li. 2018. Identifying
self-admitted technical debt in open source projects using text mining. Empirical
Software Engineering 23, 1 (2018), 418–451.

[19] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing source code using a neural attention model. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). 2073–2083.

[20] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980 (2014).

[21] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Im-
proved code summarization via a graph neural network. In Proceedings of the
28th International Conference on Program Comprehension. 184–195.

[22] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model
for generating natural language summaries of program subroutines. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE,
795–806.

[23] Yuding Liang and Kenny Zhu. 2018. Automatic generation of text descriptive
comments for code blocks. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 32.

[24] Bo Lin, Shangwen Wang, Kui Liu, Xiaoguang Mao, and Tegawendé F Bissyandé.
2021. Automated Comment Update: How Far are We?. In 2021 IEEE/ACM 29th
International Conference on Program Comprehension (ICPC). IEEE, 36–46. https:
//doi.org/10.1109/ICPC52881.2021.00013

[25] Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen, Hui Li, and Rongxin
Wu. 2021. Improving code summarization with block-wise abstract syntax tree
splitting. In 2021 IEEE/ACM 29th International Conference on Program Compre-
hension (ICPC). IEEE, 184–195. https://doi.org/10.1109/ICPC52881.2021.00026

[26] Zhongxin Liu, Xin Xia, David Lo, Meng Yan, and Shanping Li. 2021. Just-In-
Time Obsolete Comment Detection and Update. IEEE Transactions on Software
Engineering (2021), 1–1. https://doi.org/10.1109/TSE.2021.3138909

[27] Zhongxin Liu, Xin Xia, Christoph Treude, David Lo, and Shanping Li. 2019. Auto-
matic generation of pull request descriptions. In 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 176–188.

[28] Zhongxin Liu, Xin Xia, Meng Yan, and Shanping Li. 2020. Automating Just-
in-Time Comment Updating. In Proceedings of the 35th IEEE/ACM International
Conference on Automated Software Engineering. Association for Computing Ma-
chinery, New York, NY, USA, 585–597. https://doi.org/10.1145/3324884.3416581
[29] Courtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault. 2015. Ground
truth for grammatical error correction metrics. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing (Volume 2: Short Papers). 588–
593.

[30] Sheena Panthaplackel, Milos Gligoric, Raymond J. Mooney, and Junyi Jessy
Li. 2020. Associating Natural Language Comment and Source Code Entities.
Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020),
8592–8599. https://doi.org/10.1609/aaai.v34i05.6382

[31] Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric, and Raymond J Mooney.
2021. Deep Just-In-Time Inconsistency Detection Between Comments and Source
Code. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 427–
435.

[32] Inderjot Kaur Ratol and Martin P. Robillard. 2017. Detecting fragile comments. In
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
(ASE). 112–122. https://doi.org/10.1109/ASE.2017.8115624

[33] Richard Shin, Illia Polosukhin, and Dawn Song. 2018. Towards specification-
directed program repair. In International Conference on Learning Representations
Workshop. https://openreview.net/forum?id=B1iZRFkwz

[34] Giriprasad Sridhara. 2016. Automatically detecting the up-to-date status of ToDo
comments in Java programs. In Proceedings of the 9th India Software Engineering
Conference. 16–25.

[35] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K. Vijay-
Shanker. 2010. Towards Automatically Generating Summary Comments for Java
Methods. In Proceedings of the IEEE/ACM International Conference on Automated
Software Engineering (Antwerp, Belgium) (ASE ’10). Association for Computing
Machinery, New York, NY, USA, 43–52. https://doi.org/10.1145/1858996.1859006
[36] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.
[37] Lin Tan, Ding Yuan, Gopal Krishna, and Yuanyuan Zhou. 2007. /*icomment:
Bugs or Bad Comments?*/. In Proceedings of Twenty-First ACM SIGOPS Sym-
posium on Operating Systems Principles (Stevenson, Washington, USA) (SOSP
’07). Association for Computing Machinery, New York, NY, USA, 145–158.
https://doi.org/10.1145/1294261.1294276

[38] Lin Tan, Ding Yuan, and Yuanyuan Zhou. 2007. Hotcomments: how to make

program comments more useful?. In HotOS, Vol. 7. 49–54.

[39] Lin Tan, Yuanyuan Zhou, and Yoann Padioleau. 2011. aComment: mining anno-
tations from comments and code to detect interrupt related concurrency bugs. In
2011 33rd International Conference on Software Engineering (ICSE). IEEE, 11–20.
[40] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and
Denys Poshyvanyk. 2019. On learning meaningful code changes via neural
machine translation. In 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE). IEEE, 25–36.

[41] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer Networks.
Advances in Neural Information Processing Systems 28 (2015), 2692–2700.
[42] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S. Yu. 2018. Improving Automatic Source Code Summarization via Deep

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Zhu, He and Xu

Reinforcement Learning. Association for Computing Machinery, New York, NY,
USA, 397–407. https://doi.org/10.1145/3238147.3238206

[43] Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao, Jian Wu, Philip
Yu, and Guandong Xu. 2020. Reinforcement-learning-guided source code sum-
marization via hierarchical attention. IEEE Transactions on software Engineering
(2020).

[44] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as a Dual
Task of Code Summarization. Advances in Neural Information Processing Systems
32 (2019), 6563–6573.

[45] Fengcai Wen, Csaba Nagy, Gabriele Bavota, and Michele Lanza. 2019. A Large-
Scale Empirical Study on Code-Comment Inconsistencies. In 2019 IEEE/ACM
27th International Conference on Program Comprehension (ICPC). IEEE, 53–64.
https://doi.org/10.1109/ICPC.2019.00019

[46] Edmund Wong, Taiyue Liu, and Lin Tan. 2015. Clocom: Mining existing source
code for automatic comment generation. In 2015 IEEE 22nd International Confer-
ence on Software Analysis, Evolution, and Reengineering (SANER). IEEE, 380–389.

[47] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, and Shan-
ping Li. 2017. Measuring program comprehension: A large-scale field study with
professionals. IEEE Transactions on Software Engineering 44, 10 (2017), 951–976.
[48] Rui Xie, Wei Ye, Jinan Sun, and Shikun Zhang. 2021. Exploiting Method Names to
Improve Code Summarization: A Deliberation Multi-Task Learning Approach. In
2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC).
IEEE.

[49] Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-
Burch. 2016. Optimizing statistical machine translation for text simplification.
Transactions of the Association for Computational Linguistics 4 (2016), 401–415.
[50] Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, and Shikun Zhang.
2020. Leveraging Code Generation to Improve Code Retrieval and Summarization
via Dual Learning. Association for Computing Machinery, New York, NY, USA,
2309–2319. https://doi.org/10.1145/3366423.3380295

[51] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In 2020 IEEE/ACM 42nd Inter-
national Conference on Software Engineering (ICSE). IEEE, 1385–1397.

