2
2
0
2

r
p
A
8
2

]
E
N
.
s
c
[

1
v
1
5
7
9
0
.
5
0
2
2
:
v
i
X
r
a

Taylor Genetic Programming for Symbolic Regression

Baihe He
Beijing Key Laboratory of Petroleum
Data Mining, China University of
Petroleum
Beijing, China
hebaihe@hotmail.com

Qiang Luâˆ—
Beijing Key Laboratory of Petroleum
Data Mining, China University of
Petroleum
Beijing, China
luqiang@cup.edu.cn

Qingyun Yang
Beijing Key Laboratory of Petroleum
Data Mining, China University of
Petroleum
Beijing, China
yangqingyun.cup@hotmail.com

Jake Luo
Department of Health Informatics
and Administration, University of
Wisconsin Milwaukee
Milwaukee, United States
jakeluo@uwm.edu

Zhiguang Wang
Beijing Key Laboratory of Petroleum
Data Mining, China University of
Petroleum
Beijing, China
cwangzg@cup.edu.cn

ABSTRACT
Genetic programming (GP) is a commonly used approach to solve
symbolic regression (SR) problems. Compared with the machine
learning or deep learning methods that depend on the pre-defined
model and the training dataset for solving SR problems, GP is more
focused on finding the solution in a search space. Although GP has
good performance on large-scale benchmarks, it randomly trans-
forms individuals to search results without taking advantage of
the characteristics of the dataset. So, the search process of GP is
usually slow, and the final results could be unstable. To guide GP
by these characteristics, we propose a new method for SR, called
Taylor genetic programming (TaylorGP)1. TaylorGP leverages a
Taylor polynomial to approximate the symbolic equation that fits
the dataset. It also utilizes the Taylor polynomial to extract the
features of the symbolic equation: low order polynomial discrimi-
nation, variable separability, boundary, monotonic, and parity. GP
is enhanced by these Taylor polynomial techniques. Experiments
are conducted on three kinds of benchmarks: classical SR, machine
learning, and physics. The experimental results show that TaylorGP
not only has higher accuracy than the nine baseline methods, but
also is faster in finding stable results.

CCS CONCEPTS
â€¢ Computing methodologies â†’ Genetic programming.

KEYWORDS
Taylor polynomials, genetic programming, symbolic regression

1 INTRODUCTION
Symbolic regression (SR) refers to finding a symbolic equation ğ‘“ğœƒ
fitted to a given dataset (ğ‘‹, ğ‘Œ ) from the mathematical expression
space, i.e., ğ‘“ğœƒ (ğ‘‹ ) = ğ‘Œ . The mathematical expression space is huge,
even for rather simple symbolic equations. For example, if a sym-
bolic equation is represented by a binary tree with a maximum
depth of 4, with 20 variables (ğ‘¥1, ğ‘¥2, ..., ğ‘¥20) and 18 basic functions
(such as +, âˆ’ and sqrt), the size of the space is 8.2 Ã— 10162 [25].

âˆ—Corresponding author.
1Code and appendix at https://kgae-cup.github.io/TaylorGP/

Therefore, finding a good symbolic equation for the data from the
huge possible search space is a challenging task.

Research communities of evolutionary computation (EC) and
machine learning (ML) have been trying to solve the SR problems
from their perspectives. The EC methods, especially genetic pro-
gramming (GP) methods [14, 26, 36, 44], are designed to search the
mathematical expression space by evolving the encoding of each
individual in a population. The main advantage of the GP approach
is that the algorithm components are generalizable and adaptive, in-
cluding processes such as selection, crossover, mutation, and fitness
evaluation. Using these components, the GP algorithm randomly
searches for a model ğ‘“ğœƒ that fits the dataset in the mathematical
expression space, unlike machine learning methods (e.g., neural
networks) that try to find and optimize a set of parameters ğœƒ under
known models ğ‘“ . However, the GP search process usually does
not consider the features of the given dataset, which could be an
opportunity for improvement. The ML (including neural networks)
methods [2, 22, 28, 33, 42, 47] find parameters ğœƒ in pre-defined mod-
els ğ‘“ so that the models fit the dataset, i.e., ğ‘“ğœƒ (ğ‘‹ ) = ğ‘Œ . Machine
learning methods heavily utilize the features of a dataset to guide
the search and optimization for parameters. Therefore, the parame-
ter search process is effective. However, when solving SR problems,
though the machine learning approaches could quickly recover
some correct symbolic equations ğ‘“ğœƒ s , the results are commonly
biased by the pre-defined regression model ğ‘“ s as well as the train-
ing dataset. For example, in a recent evaluation [5] of algorithms
for SR problems using large-scale benchmarks, the results show
that the top three approaches are still GP-based approaches, and
GP approaches still have a substantial advantage over machine
learning-based approaches.

In this paper, we propose a new GP approach called the Taylor
Genetic Programming (TaylorGP). To overcome the common GPâ€™s
drawback â€“ a search process without considering the data features,
we embed Taylor features into GP and leverage the features to
guide GP to search for solutions. TaylorGP, as shown in Figure
1, first obtains a ğ‘˜-order Taylor polynomial at a point from the
given dataset. According to the Taylorâ€™s theorem, the ğ‘˜-order Tay-
lor polynomial approximates a smooth function that fits the given
dataset near the point. Moreover, the ğ‘˜-order Taylor polynomial

 
 
 
 
 
 
Baihe He, Qiang Lu, Qingyun Yang, Jake Luo, and Zhiguang Wang

SVM, and XGBoost, on the three types of benchmarks: classical
SR [34], AIFeynman [47], and Penn machine learning benchmarks
[41].

The remainder of this paper is organized as follows. In Section 2,
we detail related work. Section 3 presents the Taylor features. Then,
we propose the Taylor genetic programming in Section 4. Section 5
and 6 report the experimental results and discussion. Finally, we
conclude the paper in Section 6.

2 RELATED WORK
2.1 Machine learning for symbolic regression
The regression analysis in machine learning, such as linear regres-
sion [50], SVM [10], XGBoost [8], and neural network(NN) [30],
can be viewed as a special case of SR. Different from SR that needs
to find both the model and its parameter values, machine learning
aims to find the values (ğœƒ ) of parameters in a pre-defined model
ğ‘“ , so that ğ‘“ğœƒ (ğ‘‹ ) = ğ‘Œ . Many ML methods, such as deep neural
networks, usually applies the gradient descent method to obtain
the target parameter set ğœƒ . Neural networks [2, 23, 28, 42, 43] are
trained to learn from a dataset (ğ‘‹, ğ‘Œ ) to generate a mathematical
equation ğ‘“ (ğ‘‹ ) = ğ‘Œ according to the features of the training dataset
{(ğ‘‹, ğ‘Œ ), F }. For example, GrammarVAE (GVAE) [28] trains a vari-
ational autoencoder [23] to directly encode from and decode to
the parse trees using a context-free grammar. Recently, DSR [42]
employed a recurrent neural network trained by a reinforcement
learning algorithm (RL) for SR. The algorithm uses a risk-seeking
policy gradient to emit a distribution over tractable mathemati-
cal expressions. According to the distribution, DSR samples the
mathematical expressions with constant placeholders and obtains
these constants with the nonlinear optimization algorithm â€“ BFGS
[15]. Inspired by the recent successes of pre-trained models on large
datasets, such as BERT [13] and GPT [4], NeSymReS [2] pre-trains a
model called Set Transformer on hundreds of millions of equations
to generate a distribution of mathematical expressions according
to the given dataset (ğ‘‹, ğ‘Œ ). In addition, NeSymReS samples mathe-
matical expressions with constant placeholders by the beam search
on the distributions and uses BFGS to optimize these constants.

Besides the above neural networks for SR, EQL [32, 43] designs a
shallow fully-connected neural network where standard activation
functions (e.g., ğ‘¡ğ‘ğ‘›â„, ReLU) are replaced with basic functions (e.g.,
"+", "Ã—, "ğ‘ ğ‘–ğ‘›). Once the neural network is trained, it can represent a
symbolic equation fitted to the given dataset. AIFeynman [46, 47]
employs neural networks to map the given dataset into simplifying
properties (e.g., symmetries, separability, and compositionality).
The method then uses a brute-force recursive algorithm guided
by these simplifying properties and a suite of physics-inspired
techniques to search possible symbolic expressions.

The above machine learning methods, especially deep learning
methods, have an excellent ability to discover mathematical equa-
tions on some specific benchmarks. However, experiments on the
large-scale benchmarks [5] indicate that the mathematical equa-
tions found by ML or DL are less accurate than those found by
GP-based methods. For example, four of the top five methods and
six of the top ten methods are GP-based methods, and the other
top methods are ensemble tree-based methods, such as XGBoost
and LightGBM [19]. Surprisingly, the top methods do not include

Figure 1: Taylor genetic programming.

can show the local features of the smooth function (called Taylor
features). Taylor features include three key components: variable
separability, low order polynomial discrimination, and function fea-
ture. Using the variable separability, TaylorGP can decompose the
multivariate Taylor polynomial ğ‘‡ into multiple univariate Taylor
polynomials ğ‘‡ğ‘–ğ‘ . For example, the two-variable Taylor polynomial
"1.1ğ‘¥ + 0.2ğ‘¥ 2 âˆ’ 3.7 + 1.5ğ‘¦ âˆ’ 0.25ğ‘¦3 + 0.0125ğ‘¦5" can be decomposed
into two univariate Taylor polynomials "1.1ğ‘¥ + 0.2ğ‘¥ 2 âˆ’ 3.7" and
"1.5ğ‘¦ âˆ’ 0.25ğ‘¦3 + 0.0125ğ‘¦5" according the variable separability. Tay-
lorGP then applies the polynomial discrimination to determine
whether each ğ‘‡ğ‘– is a polynomial. If ğ‘‡ğ‘– is not a polynomial, TaylorGP
runs the function feature evolution method to find symbolic equa-
tions to fit ğ‘‡ğ‘– . The function feature evolution method creates a set
of rules based on the function features to recombine ğœ† individuals.
The method also employs the individual initialization method to
randomly generate ğ›½ individuals to prevent premature convergence.
TaylorGP finally assembles mathematical expressions of all ğ‘‡ğ‘– s to
generate the final symbolic equation.

As the Taylor features are directly induced from the coefficients
in the ğ‘˜-order Taylor polynomial, obtaining the features are simple
and quick. For example, the coefficient of each two-variable prod-
uct term "ğ‘¥ğ‘šğ‘¦ğ‘›" in "1.1ğ‘¥ + 0.2ğ‘¥ 2 âˆ’ 3.7 + 1.5ğ‘¦ âˆ’ 0.25ğ‘¦3 + 0.0125ğ‘¦5"
is zero, meaning the Taylor polynomial is variable separable. So,
embedding these features into GP does not increase the GPâ€™s com-
puting time complexity. Moreover, the search process guided by
the Taylor features enables TaylorGP to find a correct symbolic
equation quicker than without using the features.

The main contributions of this paper are the following:
(1) We propose a simple yet powerful Taylor genetic program-
ming (TaylorGP) method for symbolic regression. TaylorGP com-
bines the general characteristics of GPâ€™s solution search strate-
gies (e.g., mutation, crossover) with the ML (NN)â€™s feature-directed
search.

(2) We design a new Taylor feature extraction method. Using a
Taylor polynomial obtained from a dataset, the method can map the
dataset into features that can represent the properties of a target
symbolic equation. Moreover, we create a function feature evolution
method to transform individuals according to these features.

(3) We demonstrate that TaylorGP significantly outperforms
state-of-the-art approaches, such as FFX[33], GSGP [38], BSR [18],

Foreachğ‘‡!ğ‘¥!DataObtainingak-orderTaylorpolynomialğ‘‡ğ‘¥,ğ‘¦=&!"#$1ğ‘˜!ğ‘¥âˆ’ğ‘¥%â‹…ğœ•ğœ•ğ‘¥+ğ‘¦âˆ’ğ‘¦%â‹…ğœ•ğœ•ğ‘¦$ğ‘“(ğ‘¥%,ğ‘¦%)â‰ˆ1.1ğ‘¥+0.2ğ‘¥"âˆ’3.7+1.5ğ‘¦âˆ’0.25ğ‘¦#+0.0125ğ‘¦$Decomposingthek-orderTaylorpolynomial(1.1ğ‘¥+0.2ğ‘¥"âˆ’3.7)+(1.5ğ‘¦âˆ’0.25ğ‘¦#+0.0125ğ‘¦$)â€¢+separabilityğ‘‡ğ‘¥%,â€¦,ğ‘¥&=ğ‘‡%ğ‘¥%+â‹¯+ğ‘‡&(ğ‘¥&)â€¢Ã—separabilityğ‘‡ğ‘¥%,â€¦,ğ‘¥&=ğ‘‡%ğ‘¥%Ã—â‹¯Ã—ğ‘‡&(ğ‘¥&)is_polynomialFunctionFeatureEvolutionforğ‘‡!ğ‘¥!â€¢odd/evenğ‘‡!âˆ’ğ‘¥!=âˆ’ğ‘‡!ğ‘¥!ğ‘‡!âˆ’ğ‘¥!=ğ‘‡!ğ‘¥!â€¢boundaryExtractingfunctionfeaturefromğ‘‡!ğ‘¥!â€¢initializesğ‘›individualsinthefunctionboundaryâ€¢recombinesğ›¼individualsaccordingtothesefeaturesâ€¢randominitializesğ›½individualsâ€¢selectğ‘›individualsasthenextpopulation1.5ğ‘¦âˆ’0.25ğ‘¦#+0.0125ğ‘¦$NoAssemblingmathematicalexpressions1.1ğ‘¥+0.2ğ‘¥"âˆ’3.7Yes1.5sinğ‘¦,1.5yâˆ’0.25ğ‘¦",â€¦1.1ğ‘¥+0.2ğ‘¥"+1.5sin(ğ‘¦)âˆ’3.71.1ğ‘¥+0.2ğ‘¥"+1.5sin(ğ‘¦)âˆ’3.71.1ğ‘¥+0.2ğ‘¥"+1.5yâˆ’0.25y"âˆ’3.7â€¦ğ‘‡!ğ‘¥!âˆˆ[ğ‘,ğ‘]orâ€¢monotonicğ‘‡!ğ‘¥!â€™â‰¥ğ‘‡!ğ‘¥!(orğ‘‡!ğ‘¥!â€™â‰¤ğ‘‡!ğ‘¥!(Taylor Genetic Programming for Symbolic Regression

the two neural network methods, DSR and AIFeynman. Further-
more, the neural network methods seem to be more dependent on
the training dataset. Neural networks could not discover a correct
mathematical equation if their structures (layer by layer) could not
extract valid features from the dataset.

2.2 Genetic programming for symbolic

regression

GP [26] is still a commonly used approach to deal with SR. GP
uses evolutionary operators â€“ crossover, mutation, and selection,
to change the individual encoding and generate better offspring for
searching a solution in the mathematical expression space. Vari-
ous GPs employ different individual encodings to represent math-
ematical equations, such as tree-encoded GPs [7, 26, 35, 38, 45],
graph-encoded GPs [37, 44], and linearly encoded GPs [3, 14, 31].
For the mathematical expression space, the presence of real con-
stants accounts for a significant portion of the size of the space. For
example, the size of the aforementioned problem in the Introduc-
tion section is 8.2 Ã— 10162. In comparison, without real constants,
its size is 1.054 Ã— 1019 [25]. So, some GP methods [24, 27, 51] with
a constant optimizer are proposed to search the space. These meth-
ods represent the skeleton of a mathematical expression by using
constant placeholders. And a constant optimizer is used to find
the values in these constant placeholders. AEG-GP [24, 25] uses
the abstract expression grammar to represent the skeleton of a
mathematical expression and utilizes PSO [21] to find constant
values. Unlike AEG-GP, PGE [51] is a deterministic SR algorithm
that offers reliable and reproducible results. While PGE maintains a
tree-based representation and Pareto non-dominated sorting from
GP, it replaces the genetic operators and random numbers with
grammar rules. The method also uses nonlinear regression to fit
the constants of a mathematical equation. The approaches to GP-
based feature engineering, such as GP-based feature construction
[27], MRGP [1], FEW [29], M3GP [40], and FEAT [6], utilize EC to
search for possible representations and couple with an ML model
to handle the parameters of the representations. Different from
GP-based feature engineering approaches, FFX [33] is a determin-
istic SR algorithm. It enumerates a massive set of basic features
(basis functionsâ€“ğµğ‘– (ğ‘¥)) by a production rule. It then find coefficient
values(ğ‘) in "ğ‘¦ = ğ‘0 +(cid:205)ğ‘
ğ‘ğ‘– Ã—ğµğ‘– (ğ‘¥)" by using pathwise regularized
ğ‘–=1
learning.

The other research line uses the hybrid of a neural network and a
GP, called DL-GP. DL-GPs [11, 39, 52, 53] leverage a neural network
to obtain features from the given dataset and apply these features to
guide GP. For example, Xing et al. [52] design an encoder-decoder
neural network based on super-resolution ResNet to predicate the
importance of each mathematical operator from the given data; and
utilize the importance of each mathematical operator to guide GP.
Cranmer et al. [11] train a graph neural network to represent sparse
latent features of the given dataset, and employ GP to generate
symbolic expressions fitted to these latent features. The DL-GPs
still depend on the time-consuming training work and the training
dataset.

Like AIFeynman, TaylorGP also needs to extract the symbolic
equationâ€™s properties (e.g., separability and low-order polynomial)
from the given dataset. However, the difference is that AIFeynman

employs a neural network to obtain these properties while TaylorGP
achieves the goal using the coefficients in the Taylor series on the
given dataset. Therefore, TaylorGP does not need to train a model
and does not depend on the training data.

3 TAYLOR FEATURES ANALYSIS
3.1 Obtaining a Taylor polynomial
Taylorâ€™s theorem [17] states that if a function ğ‘“ has ğ‘› +1 continuous
derivatives on an open interval containing ğ‘, for each ğ‘¥ in the
interval,

ğ‘“ (ğ‘¥) =

(cid:34) ğ‘›
âˆ‘ï¸

ğ‘˜=0

ğ‘“ (ğ‘˜) (ğ‘)
ğ‘˜!

(cid:35)

(ğ‘¥ âˆ’ ğ‘)ğ‘˜

+ ğ‘…ğ‘›+1 (ğ‘¥).

(1)

So, the ğ‘˜-order Taylor polynomial ((cid:205)ğ‘›
mates to ğ‘“ around ğ‘.

ğ‘˜=0

ğ‘“ (ğ‘˜ ) (ğ‘)
ğ‘˜!

(ğ‘¥ âˆ’ ğ‘)ğ‘˜ ) approxi-

Given a dataset (ğ‘‹, ğ‘Œ ), for any point (ğ‘¥0, ğ‘¦0) âˆˆ (ğ‘‹, ğ‘Œ ), the ğ‘˜-
order Taylor polynomial around the point can be obtained by the
following three steps. First, select ğ‘˜ points ({(ğ‘¥1, ğ‘¦1), ..., (ğ‘¥ğ‘˜, ğ‘¦ğ‘˜ )})
around (ğ‘¥0, ğ‘¦0) from the dataset. Next, according to the selected ğ‘˜
points, gather ğ‘˜ ğ‘˜-order Taylor polynomials by Equation 2.

ğ‘“ (ğ‘˜) (ğ‘¥0) â‰ˆ ğ‘“ (ğ‘¥1) âˆ’ ğ‘“ (ğ‘¥0)
ğ‘“ (ğ‘˜) (ğ‘¥0) â‰ˆ ğ‘“ (ğ‘¥2) âˆ’ ğ‘“ (ğ‘¥0)

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´

(ğ‘¥1 âˆ’ ğ‘¥0) ğ‘“ â€² (ğ‘¥0) + . . . + (ğ‘¥1âˆ’ğ‘¥0)ğ‘˜
(ğ‘¥2 âˆ’ ğ‘¥0) ğ‘“ â€² (ğ‘¥0) + . . . + (ğ‘¥2âˆ’ğ‘¥0)ğ‘˜
ğ‘˜!
. . .
(ğ‘¥ğ‘˜ âˆ’ ğ‘¥0) ğ‘“ â€² (ğ‘¥0) + . . . + (ğ‘¥ğ‘˜ âˆ’ğ‘¥0)ğ‘˜

ğ‘˜!

ğ‘˜!

ğ‘“ (ğ‘˜) (ğ‘¥0) â‰ˆ ğ‘“ (ğ‘¥ğ‘˜ ) âˆ’ ğ‘“ (ğ‘¥0)

ï£³
(2)
, where ğ‘“ (ğ‘¥ğ‘– ) = ğ‘¦ğ‘– . The final step is to obtain the ğ‘˜ derivatives (ğ¹ )
by Equation 3

(ğ‘), ğ‘“ â€²â€²
, where ğ¹ = [ğ‘“ â€²
ğ‘¦0]ğ‘‡ . For each ğ‘ğ‘– ğ‘— âˆˆ ğ´, ğ‘ğ‘– ğ‘— = (ğ‘¥ğ‘– âˆ’ğ‘¥0) ğ‘—
Taylor polynomial.

ğ‘—!

ğ¹ â‰ˆ ğ·ğ´âˆ’1

(3)
(ğ‘), ..., ğ‘“ (ğ‘˜) (ğ‘)]ğ‘‡ , ğ· = [ğ‘¦1âˆ’ğ‘¦0, ğ‘¦2âˆ’ğ‘¦0, ..., ğ‘¦ğ‘˜ âˆ’
. ğ¹ can generate the ğ‘˜-order

Scaling to a high dimensional dataset. Mathematically, the
3.1.1
higher the order of ğ‘˜, the more accurate the Taylor polynomial is.
However, in practice, ğ‘˜ can not be too high in a high dimensional
dataset ğ· because of the two following limitations. According to
the Taylorâ€™s theorem for multivariate functions [16], the ğ‘›-variable
ğ‘˜-order Taylor polynomial has ğ¶ğ‘›
ğ‘›+ğ‘˜ âˆ’ 1 points need
to be sampled around the point to obtain the Taylor polynomial.
However, if ğ‘˜ is too high, ğ¶ğ‘›
ğ‘›+ğ‘˜ âˆ’1 will be greater than all points in ğ·.
This is impossible. Therefore, ğ‘˜ must be limited so that ğ¶ğ‘›
ğ‘›+ğ‘˜ âˆ’ 1 <
|ğ· |. The other limitation is that, if ğ‘˜ is too high, the inverse of the
matrix ğ´ in Equation 3 would be a big challenge to compute since
ğ´ is an ultra-large-scale ((ğ¶ğ‘›
ğ‘›+ğ‘˜ âˆ’ 1)) matrix. So, we
usually set ğ‘˜ = 1 ğ‘œğ‘Ÿ 2 in a high-dimensional dataset because of the
two limitations.

ğ‘›+ğ‘˜ terms. So, ğ¶ğ‘›

ğ‘›+ğ‘˜ âˆ’ 1) Ã— (ğ¶ğ‘›

3.2 Extracting Taylor features
Since the above ğ‘˜-order Taylor polynomial is generated from a
given dataset (ğ‘‹, ğ‘Œ ), it can approximate a function ğ‘“ that fits the
dataset (i.e., ğ‘“ (ğ‘‹ ) = ğ‘Œ ) around a point (ğ‘¥0, ğ‘¦0). It can also represent
some ğ‘“ â€™s local features, called Taylor features. This paper discusses
the Taylor features, low order polynomial, variable separability,
function boundary, monotony and parity.

3.2.1 Low order polynomial discrimination. For SR, a key problem
is to discriminate whether there is (or only) a low-order polynomial
that can represent the given dataset. If it exists, a linear regres-
sion algorithm can be used to get it and the algorithm could solve
SR quickly. The ğ‘˜-order Taylor polynomial can easily solve the
discrimination problem owing to its coefficients. If a function is
a ğ‘˜-order polynomial, its Taylor expansion at a point is also a ğ‘˜-
order polynomial. For example, for "1.1ğ‘¥ + 0.2ğ‘¥ 2 âˆ’ 3.7", its Taylor
expansion at ğ‘¥ = 1 is also â€™1.1ğ‘¥ + 0.2ğ‘¥ 2 âˆ’ 3.7â€™. While, if a function
is not a ğ‘˜-order polynomial, its Taylor expansion at a point is an
infinite-order polynomial. For 1.5ğ‘ ğ‘–ğ‘›(ğ‘¥), its Taylor expansion is an
infinite order polynomial. So, for the ğ‘˜-order Taylor polynomial
obtained from a dataset, in each term whose degree is greater than
ğ‘– (ğ‘– < ğ‘˜), if the coefficient is zero, the function that ğ‘˜-order Taylor
polynomial approximates is a low ğ‘–-order polynomial.

3.2.2 Variable separability. For a multivariate function ğ‘“ (ğ‘¥1, ..., ğ‘¥ğ‘›),
if there is an operator "â—¦" that lets ğ‘“ (ğ‘¥1, ..., ğ‘¥ğ‘›) = ğ‘“1 (ğ‘¥ğ‘–, ..., ğ‘¥ğ‘˜ )
â—¦ğ‘“2 (ğ‘¥ğ‘š, ..., ğ‘¥ğ‘ ) where the two variable sets, {ğ‘¥ğ‘–, ..., ğ‘¥ğ‘˜ } and {ğ‘¥ğ‘š,...,ğ‘¥ğ‘
}, both belong to {ğ‘¥1, ..., ğ‘¥ğ‘› }, and {ğ‘¥ğ‘–, ..., ğ‘¥ğ‘˜ } âˆ© {ğ‘¥ğ‘š, ..., ğ‘¥ğ‘ } = ğœ™, it
is called "â—¦" separability. The separability property can decompose
a complex multivariate function into multiple simple functions.

If "â—¦" is addition or multiplication, it is called addition sep-
arability or multiplication separability, respectively. The ğ‘›-
variable ğ‘˜-order Taylor polynomial can represent the two sepa-
rability properties, respectively. For the Taylor polynomial, if the
coefficient in each multi-variable term is zero, the function that the
Taylor polynomial approximates is addition separability. As shown
in Figure 1, the Taylor expansion of "1.1ğ‘¥ + 0.2ğ‘¥ 2 + 1.5ğ‘ ğ‘–ğ‘›(ğ‘¦) âˆ’ 3.7"
is "1.1ğ‘¥ + 0.2ğ‘¥ 2 âˆ’ 3.7 + 1.5ğ‘¦ âˆ’ 0.25ğ‘¦3 + 0.0125ğ‘¦5", where the coeffi-
cient in each multi-variable term is zero, i.e., ğ‘ in each ğ‘ğ‘¥ğ‘–ğ‘¦ ğ‘— is 0.
So, according to the addition separability, the Taylor polynomial
is decomposed into multiple polynomials, such as "1.1ğ‘¥ + 0.2ğ‘¥ 2 +
1.5ğ‘ ğ‘–ğ‘›(ğ‘¦) âˆ’ 3.7 â‰ˆ (1.1ğ‘¥ + 0.2ğ‘¥ 2 âˆ’ 3.7) + (1.5ğ‘¦ âˆ’ 0.25ğ‘¦3 + 0.0125ğ‘¦5)".
The above method about addition separability also can be used
to discriminate multiplication separability. Because, if a function
is multiplication separability, i.e., ğ‘“ (ğ‘¥1, ..., ğ‘¥ğ‘›) = ğ‘“1 (ğ‘¥ğ‘–, ..., ğ‘¥ğ‘˜ ) Ã—
ğ‘“2 (ğ‘¥ğ‘š, ..., ğ‘¥ğ‘ ), then log ğ‘“ (ğ‘¥1, ..., ğ‘¥ğ‘›) = log ğ‘“1 (ğ‘¥ğ‘–, ..., ğ‘¥ğ‘˜ ) + log ğ‘“2 (ğ‘¥ğ‘š,
..., ğ‘¥ğ‘ ). So, if the ğ‘›-variable ğ‘˜-order Taylor polynomial obtained
after computing the log of the dataset is addition separability, then
the function is the multiplication separability.

3.2.3 Boundary. The ğ‘˜-order Taylor polynomial can be used to
evaluate the boundary of the function ğ‘“ that it approximates to
at the interval [ğ‘¥ğ‘, ğ‘¥ğ‘ ]. Since it is a polynomial, its boundary is
computed by interval arithmetic [12]. For example, the boundary
of the Taylor polynomial "1.1ğ‘¥ + 0.2ğ‘¥ 2 âˆ’ 3.7", where ğ‘¥ âˆˆ [âˆ’1, 1], is
"[âˆ’1.1, 1.1] + 0.2 Ã— [0, 1] âˆ’ 3.7 = [âˆ’4.8, âˆ’2.4]".

3.2.4 Monotonic. For all points (ğ‘¥ğ‘–, ğ‘¦ğ‘– ) in a dataset (X,Y), if ğ‘¦ğ‘– â‰¥ ğ‘¦ ğ‘—
and ğ‘¥ğ‘– â‰¥ ğ‘¥ ğ‘— , the function that the dataset represents is a monotonic-
increasing function. Otherwise, if ğ‘¦ğ‘– â‰¥ ğ‘¦ ğ‘— and ğ‘¥ğ‘– â‰¤ ğ‘¥ ğ‘— , the function
is a monotonic-decreasing function.

3.2.5 Parity. If the ğ‘˜-order Taylor polynomial ğ‘‡ (ğ‘¥) is an odd or
even function, i.e., ğ‘‡ (âˆ’ğ‘¥) = âˆ’ğ‘‡ (ğ‘¥) or ğ‘‡ (âˆ’ğ‘¥) = ğ‘‡ (ğ‘¥), then the
function that it approximates is also an odd or even function. While
the method is simple, testing all points is time-consuming.

Baihe He, Qiang Lu, Qingyun Yang, Jake Luo, and Zhiguang Wang

Another method is to count odd-order terms and even-order
terms in the ğ‘˜-order Taylor polynomial except for the 0-order term
(constant term). If the ğ‘˜-order Taylor polynomial only contains odd
(or even) order terms, it is an odd (or even) function. For example,
given that ğ‘“ (ğ‘¥) = 1.5ğ‘ ğ‘–ğ‘›(ğ‘¥) âˆ’ 3.7 and its Taylor polynomial is
1.5ğ‘¥ âˆ’ 0.25ğ‘¥ 3 + 0.0125ğ‘¥ 5 + ... + 4.217ğ‘’ âˆ’ 15ğ‘¥ 17 âˆ’ 3.7 at the point ğ‘¥ = 0,
after removing the 0-order term "âˆ’3.7", the Taylor polynomial only
contain odd-order terms, such as "0.25ğ‘¥ 3"and "0.0125ğ‘¥ 5". So, it is
an odd function.

4 TAYLOR GENETIC PROGRAMMING
TaylorGP, as shown in Figure 1, includes the following six steps: 1)
obtaining a Taylor polynomial ğ‘‡ from a given dataset, 2) decompos-
ing the Taylor polynomial into multiple simple Taylor polynomials
({ğ‘‡1,ğ‘‡2, ...,ğ‘‡ğ‘› }), 3) discriminating the low order polynomial, 4) ex-
tracting function features, 5) running the function feature evolution
method, and 6) assembling mathematical expressions. How to exe-
cute the steps: 1), 2), 3), and 4) has been introduced in Section 3. Step
6) is simple, which only composes the mathematical expressions
found by each simple Taylor polynomials into various complete
mathematical equations and evaluates them. So, the following con-
tent details step 5).

The function feature evolution method (FFEM), as shown in Algo-
rithm 1, evolves individuals based on the function feature ğ¹ that
includes boundary, monotonic, and parity. FFEM mainly contains
two evolvable operators, individual initialization (initIndividual-
ByFeatures) and individual recombination (recombineByFeatures).
The individual initialization operator randomly generates individu-
als that satisfy the function feature. The individual recombination
operator transforms individuals to ensure that the generated in-
dividuals satisfy the function feature. In each generation, FFEM
leverages individual recombination to produce offspring with the
probability ğ›¼; utilizes the individual initialization to produce off-
spring with the probability ğ›½; saves individuals as other offspring
with the probability (1 âˆ’ ğ›¼ âˆ’ ğ›½).

4.1 Individual initialization
The probability of randomly generating an individual that satisfies
the function feature ğ¹ is very small. And the process for obtaining
ğ‘ number of these candidate individuals is very time-consuming.
To speed up the process, the individual initialization operator first
segments the mathematical expression space into many sub-spaces.
It then evaluates the function features of these sub-spaces. Finally,
it randomly selects the sub-spaces that satisfy ğ¹ and randomly
generates individuals in these sub-spaces until they satisfy ğ¹ .

Segmenting mathematical expression space . A tree can be
4.1.1
used to represent a mathematical expression. A tree with depth â„
also shows a sub-space that contains all mathematical expressions
expanded from the tree. Moreover, all trees with depth â„ represent a
segment of the mathematical expression space. For example, given a
basic function set {+, ğ‘ ğ‘–ğ‘›} and a variable set {ğ‘¥, ğ‘}, the mathematical
expression space is divided into the sub-spaces encoded by trees
with depth 3, such as "+ +ğ‘ ğ‘–ğ‘›ğ‘ğ‘¥ğ‘¥", "+ + +ğ‘¥ğ‘ğ‘", "+ğ‘ ğ‘–ğ‘›ğ‘ ğ‘–ğ‘›ğ‘¥ğ‘¥", and "ğ‘ ğ‘–ğ‘› +
ğ‘¥ğ‘". The sub-space "+ + ğ‘ ğ‘–ğ‘›ğ‘ğ‘¥ğ‘¥" is a mathematical expression "ğ‘ +
ğ‘¥ + ğ‘ ğ‘–ğ‘›(ğ‘¥)", which contains all mathematical expressions expanded
from "+ + ğ‘ ğ‘–ğ‘›ğ‘ğ‘¥ğ‘¥", such as "+ + ğ‘ ğ‘–ğ‘› + ğ‘ğ‘¥ğ‘¥ğ‘¥"="(ğ‘ + ğ‘¥) + ğ‘ ğ‘–ğ‘›(ğ‘¥) + ğ‘¥".

Taylor Genetic Programming for Symbolic Regression

Algorithm 1 function feature evolution method
Input: (ğ‘‹ğ‘–, ğ‘Œğ‘– ), ğ›¼, ğ›½, threshold, maxGen,ğ¹
Output: best

1: ğ‘ƒ â† initIndividualByFeatures(ğ¹ ,popsize=ğ‘ )
2: best â† selectBestIndividual(ğ‘ƒ)
3: while best.fitness â‰¤ threshold and ğ‘” < maxGen do
4:

for all ğ‘– = 1 to ğ‘ do

5:

6:

7:

8:

9:

10:

11:

12:

13:

ğ‘1, ğ‘2 â† randomSelectTwoIndividuals(ğ‘ƒ)
if rand() < ğ›¼ then

child â† recombineByFeatures(ğ‘1, ğ‘2,ğ¹ )

else if rand() < ğ›¼ + ğ›½ then

child â† initIndividualByFeatures(ğ¹ ,popSize=1)

else

child â† ğ‘1

end if
nextP [ğ‘–] â† child

14:

15:

16:

end for
ğ‘ƒ â† nextP
best â† min(best, selectBestIndividual(ğ‘ƒ,(ğ‘‹ğ‘–, ğ‘Œğ‘– )))
ğ‘” + +
17:
18: end while
19: return best

4.1.2 Evaluating sub-space. Interval arithmetic [12] can be used to
compute the boundary of a sub-space. In the tree that represents the
sub-space, if there is a path from a leaf node to the root node con-
sisting of unbound functions (ğ‘¥, +, âˆ’, Ã—, /, ğ‘ğ‘¥ , ln, ...), the boundary
of the sub-space is [âˆ’âˆ, âˆ]. Otherwise, it is computed by interval
arithmetic. For example, for the sub-space "+ + ğ‘ ğ‘–ğ‘›ğ‘ğ‘¥ğ‘¥", there is
a path "ğ‘¥ + +", so its boundary is [âˆ’âˆ, âˆ]. There is no such path
for the sub-space "+ğ‘ ğ‘–ğ‘›ğ‘ ğ‘–ğ‘›ğ‘¥ğ‘¥", so its boundary is [-2,2] obtained by
interval arithmetic.

A sub-spaceâ€™s non-monotonic or monotonic increasing/decrease
is determined by its derivative ğ‘‘. If ğ‘‘ â‰¥ 0 (or ğ‘‘ â‰¤ 0) in all variable
values, it is a monotone increasing (decreasing) function. Otherwise,
it is a non-monotone function. The sub-space is an odd/even func-
tion according to "ğ‘“ (âˆ’ğ‘¥) = âˆ’ğ‘“ (ğ‘¥)" or "ğ‘“ (âˆ’ğ‘¥) = ğ‘“ (ğ‘¥)" for all values
in ğ‘¥. For the sub-space "+ + ğ‘ ğ‘–ğ‘›ğ‘ğ‘¥ğ‘¥=ğ‘ + ğ‘¥ + ğ‘ ğ‘–ğ‘›(ğ‘¥)", its derivative
is "1 + ğ‘ğ‘œğ‘  (ğ‘¥)", meaning that it is a monotone-increasing function.
Owing to "ğ‘" in "ğ‘ +ğ‘¥ +ğ‘ ğ‘–ğ‘›(ğ‘¥)", it is a non-odd and non-even function.

4.1.3 Generating individual. The method "generating individual"
obtains the segmented sub-spaces whose boundaries contain the
given boundary. It then randomly selects a sub-space from these
sub-spaces. If the sub-space does not satisfy the given monotony
and parity requirements, the method randomly generates a new
individual from the sub-space until it satisfies the given function
features. Otherwise, the method randomly generates an individual
based on the following rules, as listed in Table 1. For example,
given the function features {[âˆ’10, 10], ğ‘œğ‘‘ğ‘‘ }, for the selected sub-
space "+ + ğ‘ ğ‘–ğ‘›ğ‘ğ‘¥ğ‘¥=ğ‘ + ğ‘¥ + ğ‘ ğ‘–ğ‘›(ğ‘¥)", it is a non-odd and non-even
function. So, the method randomly generates individuals until an
individual is an odd function whose boundary contains [âˆ’10, 10].
For the selected sub-space "+ + ğ‘ ğ‘–ğ‘›ğ‘¥ğ‘¥ğ‘¥=2ğ‘¥ + ğ‘ ğ‘–ğ‘›(ğ‘¥)", it is an odd
function. So, the method randomly constructs an odd function (e.g.,

ğ‘ ğ‘–ğ‘›(ğ‘¥)). It then combines the odd function and the selected sub-
space according to an operator randomly selected (e.g. "+") from
{+, âˆ’, Ã—, /, ğ‘“ (ğ‘”(ğ‘¥)}. With these steps the method finally generates
an individual "2ğ‘¥ + 2ğ‘ ğ‘–ğ‘›(ğ‘¥)" whose boundary contains [âˆ’10, 10].

Table 1: Function Combination Rules

op

ğ‘“ (ğ‘¥)

ğ‘”(ğ‘¥)

results

+
+
âˆ’
âˆ’
Ã—, /
Ã—, /
Ã—, /
ğ‘“ (ğ‘”(ğ‘¥))
ğ‘“ (ğ‘”(ğ‘¥))
ğ‘“ (ğ‘”(ğ‘¥))
ğ‘“ (ğ‘”(ğ‘¥))
+
Ã—

odd
odd
even
even
odd
odd
even
even
odd
odd
even
even
even
odd
even
even
odd
odd
odd
even
even
odd
â†— â†—
â†— â†—
ğ‘“ (ğ‘”(ğ‘¥)) â†— â†—

odd
even
odd
even
even
even
odd
even
odd
even
even
â†—
â†—
â†—

"â†—" represents a monotone-increasing function. The monotone-decreasing func-
tion has similar properties to the monotone-increasing function.

4.2 Individual recombination
According to the rules in Table 1, the individual recombination op-
erator recombines two individuals from the population to construct
an individual that satisfies the given function feature. Meanwhile,
if the recombined individual exceeds the limit length, the operator
prunes it to avoid the individual bloating. For example, given a
function feature â€“ odd function, the operator recombines the two
individuals "2ğ‘¥ + 2ğ‘ ğ‘–ğ‘›(ğ‘¥)" and ğ‘¥ + ğ‘¥ 3 with "+". The recombined
individual "2ğ‘¥ + 2ğ‘ ğ‘–ğ‘›(ğ‘¥) + ğ‘¥ + ğ‘¥ 3" exceeds the limited length 12. It
then prunes "ğ‘ ğ‘–ğ‘›(ğ‘¥)" in the individual and replaces "ğ‘ ğ‘–ğ‘›(ğ‘¥)" with
"ğ‘¥" according to "ğ‘“ (ğ‘”(ğ‘¥))". The method in the end generates the
individual "5ğ‘¥ + ğ‘¥ 3" whose length is 9.

5 EXPERIMENT
5.1 Datasets
We evaluate the performance of TaylorGP on three kinds of bench-
marks: classical Symbolic Regression Benchmarks (SRB) [34], Penn
Machine Learning Benchmarks (PMLB) [41], and Feynman Sym-
bolic Regression Benchmarks (FSRB) [47]. SRB consists of twenty-
three SR problems derived from the five canonical symbolic re-
gression benchmarks, Nguyen [48], Korns [24], Koza [9], Keijzer
[20], and Vladislavleva [49]. PMLB includes seven regression tasks
and three classification tasks. FSRB contains forty-eight Feynman
equations in [47]. The distribution of the total 81 benchmark sizes
by samples and features is shown in Figure 2. The details of these
benchmarks are listed in the appendix.

5.2 Algorithm Parameter Settings
We compare TaylorGP with two kinds of baseline algorithms 2: four
symbolic regression methods and five machine learning methods.

2The nine baseline algorithms are implemented in SRBench [5]

Baihe He, Qiang Lu, Qingyun Yang, Jake Luo, and Zhiguang Wang

6 RESULTS AND DISCUSSION
6.1 Performance Metrics
TaylorGP and nine baseline algorithms run 30 times on each bench-
mark. Their fitness results are listed in the appendix. In addition,
the following ğ‘…2 test [5] is introduced to evaluate the performance
of these algorithms on these benchmarks.

R2 = 1 âˆ’

(cid:205)ğ‘›
ğ‘–=1 ( Ë†ğ‘¦ğ‘– âˆ’ ğ‘¦ğ‘– )2
(cid:205)ğ‘›
ğ‘–=1 ( Â¯ğ‘¦ âˆ’ ğ‘¦ğ‘– )2

,

(4)

, where ğ‘¦ğ‘– is the value in the dataset, Â¯ğ‘¦ is mean and Ë†ğ‘¦ğ‘– is the output
value of the best solution.

Figure 3 illustrates the normalized ğ‘…2 scores of the ten algorithms
running 30 times on all benchmarks. Since the normalized ğ‘…2 closer
to 1 indicates better results, overall TaylorGP can find more accurate
results than other algorithms. Moreover, TaylorGPâ€™s results are
more stable. The normalized ğ‘…2 scores of the ten algorithms on each
benchmark (in the appendix) show that TaylorGP can outperform
the nine baseline algorithms on most benchmarks.

Figure 3: Normalized ğ‘…2 scores of the ten algorithms.

Table 3 shows that TaylorGP still outperforms the nine base-
line algorithms on the pairwise statistical comparisons with the
Wilcoxon signed-rank test. Except for TaylorGP, it is not easy to
find one algorithm outperform all other algorithms consistently on
the benchmarks.

6.2 Discussion
Why does TaylorGP outperform the nine baseline algorithms on
most benchmarks? The main reason is that the Taylor features can
guide TaylorGP to search the problem space more effectively than
the baselines. Compared with the other five ML methods, TaylorGP
does not need to construct a predefined model to find a model that
fits the given dataset. Therefore, on large-scale benchmarks, it can
find better results. Compared with other GPs that need to search
the whole mathematical expression space, TaylorGPâ€™s search space
is smaller. So, it can find the correct results faster.

6.2.1 Convergence Analysis. We compare TaylorGP with the other
three SR methods, GPLearn, GSGP, and BSR. Two benchmarks are
used in the following evaluation. One benchmark is the "ğ‘¥ğ‘¥1
0 " from

Figure 2: Properties of Benchmarks.

Table 2: Algorithm parameters

Parameter

Function Set
Max Generations
Population Size
Crossover Rate
Mutation Rate
Copy Rate
Stopping Threshold

Function Set
max generations
Population Size
Crossover Rate
Mutation Rate
Copy Rate
Stopping Threshold

Function Set
Max Generations
Population Size
Crossover Rate
Mutation Rate
Stopping Threshold

Function Set
MM
k
Stopping Threshold

None

Normalize

Kernal
Gamma
Regularization

Value
+,âˆ’,Ã—,Ã·,sin,cos,ğ‘™ğ‘›(|ğ‘›|),ğ‘’ğ‘¥ğ‘,ğ‘ ğ‘ğ‘Ÿğ‘¡
10000
1000
0.7
0.2
0.1
1e-5
+,âˆ’,Ã—,Ã·,sin,cos,ğ‘™ğ‘›(|ğ‘›|),ğ‘’ğ‘¥ğ‘,ğ‘ ğ‘ğ‘Ÿğ‘¡
10000
1000
0.7
0.2
0.1
1e-5

+,âˆ’,Ã—,Ã·
10000
1000
0.7
0.2
1e-5
+,âˆ’,Ã—,Ã·,sin,cos,ğ‘™ğ‘›(|ğ‘›|),ğ‘’ğ‘¥ğ‘,ğ‘ ğ‘ğ‘Ÿğ‘¡
10000
2
1e-5

FALSE

â€™linearâ€™, â€™polyâ€™, â€™rbfâ€™, â€™sigmoidâ€™
0.01,0.1,1,10
0.001,0.1,1

Number of Estimators
Max Features

10, 100, 1000
â€™sqrtâ€™,â€™log2â€™,None

Name

TaylorGP

GPLearn

GSGP

BSR

FFX

LR

KR

RF

SVM

XGBoost

Kernal

â€˜linearâ€™, â€˜polyâ€™, â€˜rbfâ€™, â€˜sigmoidâ€™, â€˜precomputedâ€™

Learning Rate
Gamma

0.0001,0.01, 0.05, 0.1, 0.2
0,0.1,0.2,0.3,0.4

The symbolic regression methods include GPlearn3, FFX [33], geo-
metric semantic genetic programming (GSGP)[38] and bayesian
symbolic regression (BSR) [18]. The machine learning methods
include linear regression (LR), kernel ridge regression (KR), ran-
dom forest regression (RF), support vector machines (SVM), and
XGBoost [8]. The detailed parameters of each algorithm are tuned
according to Table 2.

3https://github.com/trevorstephens/gplearn

2010812038040048810004000No. of Samples0510152025No. of FeaturesPMLBSRBFSRBTaylorGPGPLearnFFXLRKRRFSVMXGBoostGSGPBSR-0.4-0.20.00.20.40.6Score(Normalized R2)TaylorGPXGBoostGSGPBSR0.6900.6910.6920.693Taylor Genetic Programming for Symbolic Regression

Table 3: Wilcoxon signed-rank test of normalized ğ‘…2 scores for pairwise statistical comparisons.

TaylorGP GPLearn

FFX

LR

KR

RF

SVM

XGBoost

GSGP

GPLearn
FFX
LR
KR
RF
SVM
XGBoost
GSGP
BSR

5.73e-13
8.87e-11
5.86e-15
2.73e-15
4.31e-14
2.84e-15
6.72e-05
1.08e-04
3.09e-03

6.55e-03
8.70e-01
1.00e+00
1.00e+00
5.34e-01
1.00e+00
1.00e+00
1.00e+00

9.82e-01
1.00e+00
1.00e+00
1.00e+00
1.00e+00
1.00e+00
1.00e+00

1 bold number means that ğ‘ < 0.05.

1.00e+00
1.00e+00
7.74e-01
1.00e+00
1.00e+00
1.00e+00

8.04e-01
2.28e-05
1.00e+00
1.00e+00
1.00e+00

1.36e-05
1.00e+00
1.00e+00
1.00e+00

1.00e+00
1.00e+00
1.00e+00

2.43e-03
3.62e-02

9.84e-02

SRB. The other is the "ğ‘ˆ = 1
ğ‘˜ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”ğ‘¥ 2" from FSRB. We illustrate
2
how the Taylor features help TaylorGP quickly find the correct
results through the two benchmarks. Figures 4 and 5 show the
processes of the four methods running on the two benchmarks,
respectively.

Figure 5: Convergence Comparison for "ğ‘ˆ = 1
2
where ğ‘˜ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” and ğ‘¥ are variables.

ğ‘˜ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”ğ‘¥ 2",

Figure 4: Convergence Comparison for "ğ‘¥ğ‘¥1
0 ".

For the benchmark "ğ‘¥ğ‘¥1

ğ‘¥1 + 163.688ğ‘¥ 2

0 " where ğ‘¥0 and ğ‘¥1 both are in [2, 4], com-
pared with GPLearn, GSGP and BSR, TaylorGP can find the optimal
result "ğ‘’ğ‘¥ğ‘ (ğ‘™ğ‘œğ‘”(ğ‘¥0)ğ‘¥1) = ğ‘¥ğ‘¥1
0 " at the 1000th generation. While the
other three algorithms still can not find the optimal results until
they run for 10,000 generations. TaylorGP first generates the Taylor
+ 15.171ğ‘¥ 2
0 + 2.932ğ‘¥ 3
polynomial "(2.596ğ‘’ âˆ’ 3)ğ‘¥ 4
ğ‘¥ 2
ğ‘¥1 âˆ’ 7.828ğ‘¥ 3
1 âˆ’
0
0
0
1 + 710.31ğ‘¥0ğ‘¥1 âˆ’
âˆ’ 167.424ğ‘¥0ğ‘¥ 2
0 + 11.398ğ‘¥0ğ‘¥ 3
100.026ğ‘¥ 2
0
1
932.144ğ‘¥0 + 1.636ğ‘¥ 4
1 âˆ’ 1393.536ğ‘¥1 + 1590.457"
+ 416.67ğ‘¥ 2
1 âˆ’ 47.859ğ‘¥ 3
1
at the point(2,2). According to the Taylor polynomial, the function
boundary is [4.233, 230.513], and the function is monotonically
increasing. The two Taylor features can reduce the space used to
initialize individuals. As "log(ğ‘¥0)", "ğ‘¥0 Ã— ğ‘¥1", and "ğ‘’ğ‘¥ğ‘ (ğ‘¥0)" all are
monotone increasing functions at the range [2,4], they are very
likely to be selected as initialized. The individual recombination that
recursively merges the three functions using the operator ğ‘“ (ğ‘”(ğ‘¥))
in Table 1 may generate "ğ‘’ğ‘¥ğ‘ (ğ‘™ğ‘œğ‘”(ğ‘¥0)ğ‘¥1)". So, TaylorGP, compared
with the other three algorithms, can initialize better individuals
and get the optimal result earlier, as shown in Figure 4.

For the Feynman benchmark "ğ‘ˆ = 1
2

ğ‘˜ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”ğ‘¥ 2", TaylorGP finds
the optimal results at the 0th generation, because TaylorGP can
directly obtain 0.2ğ‘˜ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”ğ‘¥ 2 owing to the low polynomial discrimi-
nation. TaylorGP achieves the Taylor polynomial "(1.454ğ‘’ âˆ’ 6)ğ‘¥ 3
âˆ’
0
1 + (5.809ğ‘’ âˆ’ 6)ğ‘¥0ğ‘¥1 +
(1.181ğ‘’ âˆ’ 6)ğ‘¥ 2
0
(9.249ğ‘’ âˆ’ 6)ğ‘¥0 âˆ’ (2.53ğ‘’ âˆ’ 6)ğ‘¥ 3
1 âˆ’ (6.481ğ‘’ âˆ’ 5)ğ‘¥1 +
1
(5.579ğ‘’ âˆ’ 5)" from the benchmark. After omitting insignificant co-
efficients that are less than ğ‘’ âˆ’ 4, the Taylor polynomial is "0.5ğ‘¥0ğ‘¥ 2
1 ".
As the RMSE of "0.5ğ‘¥0ğ‘¥ 2
1 " is less than the stopping threshold ğ‘’ âˆ’ 5,
it is the final result that TaylorGP finds.

0 + (0.5)ğ‘¥0ğ‘¥ 2
+ (2.041ğ‘’ âˆ’ 5)ğ‘¥ 2

ğ‘¥1 âˆ’ (7.784ğ‘’ âˆ’ 6)ğ‘¥ 2

Besides the two figures, the figures of the convergence compari-
son that the four algorithms run on the other benchmarks are listed
in the appendix.

Fitness Analysis. Figure 6 illustrates that TaylorGP, when
6.2.2
compared with the nine baseline algorithms, can obtain more ac-
curate and stable results on the two benchmarks, SRB and FSRB.
However, on the benchmark PMLB, the two algorithms, FFX and
XGBoost, outperform TaylorGP. This is due to PMLB has more fea-
tures (variables) than the other two benchmark sets, Figure 6 shows
that TaylorGP has the best performance (normalized ğ‘…2 score) on
the-low dimensional datasets. In contrast, its performance degrades
as the datasetâ€™s dimension increases. For a high dimension dataset,
TaylorGP can only obtain a low order Taylor polynomial according

Baihe He, Qiang Lu, Qingyun Yang, Jake Luo, and Zhiguang Wang

(b) FSRB
Figure 6: Normalized ğ‘…2 comparisons of the ten SR methods on classical Symbolic Regression Benchmarks (SRB), Feynman
Symbolic Regression Benchmarks (FSRB), and Penn Machine Learning Benchmarks (PMLB), respectively.

(c) PMLB

(a) SRB

to the analysis in Section 3.1.1. However, the low order Taylor poly-
nomial may not approximate the real function that fits the given
high-dimensional dataset. The Taylor features extracted from the
Taylor polynomial may be incorrect or incomplete; therefore the
features cannot help TaylorGP find a correct result.

6.2.3 The accuracy of extracting Taylor features. As the real func-
tion that fits the dataset in PMLB is unknown, Table 4 lists the
accuracy of extracting each Taylor feature on SRB and FSRB (total
71 benchmarks). TaylorGP can correctly identify the two Taylor
features, monotone and boundary, on all benchmarks, meaning that
the two Taylor features always help Taylor reduce the search space.
However, TaylorGP recognizes the variable separability and
even/odd function with low accuracies (12.5% and 36.7%). For iden-
tifying the variable separability and the odd/even function, Tay-
lorGP requires that the Taylor polynomial can not contain some
order terms, i.e., the coefficients in these order terms must be zero.
However, as the Taylor polynomial approximates the real function
around a point, some inconsistencies exist between the polynomial
and the real function. The coefficients on these terms are slight er-
rors. These slight error coefficients affect the recognition of the vari-
able separability and the odd/even function. For example, for ğ‘ ğ‘–ğ‘›(ğ‘¥),
its Taylor polynomial at the point (0,0) is " 1
ğ‘¥ 7+...".
1!
However, according to Equation 3, TaylorGP sets a 4-order Taylor
polynomial and obtains the polynomial "0.015+ 1
ğ‘¥ 3"
1!
from the given dataset. The polynomial is not an odd function due
to the two coefficients, "0.015" and "0.003". To prevent this from
happening, we set a threshold for these coefficients and omit the
terms whose coefficients are less than the threshold. However, it
is not easy to get a suitable threshold for the Taylor polynomial
because of the diversity of datasets.

ğ‘¥ +0.003ğ‘¥ 2 âˆ’ 1
3!

ğ‘¥ 5âˆ’ 1
7!

ğ‘¥ 3+ 1
5!

ğ‘¥âˆ’ 1
3!

Table 4: The accuracy of extracting Taylor features on 71
benchmarks.

Taylor Features

Accuracy Correct No Ground Truth No

LowOrderPoly
Separability
Boundary
Odd/even function
Monotone

73.9%
12.5%
100.0%
36.7%
100.0 %

17
3
71
18
10

23
24
71
49
10

Although the two Taylor features (variable separability, odd/even
function) have a low recognition accuracy, they still can help Tay-
lorGP to find the correct symbolic equation, such as running Tay-
lorGP on the two above benchmarks, "ğ‘¥ğ‘¥1
ğ‘˜ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”ğ‘¥ 2".
So, TaylorGP can utilize the Taylor features to reduce its search
space and speed up its search.

0 " and "ğ‘ˆ = 1
2

7 CONCLUSION
This paper proposes a new method called TaylorGP to search the
mathematical expression space using Taylor features. As most of
the Taylor features are obtained by the coefficients in a Taylor
polynomial, the modeling process can be computationally efficient
and straightforward to implement. TaylorGP leverages the two
operators based on Taylor features, individual initialization, and
individual recombination, to evolve the population. Experiments
show that TaylorGP can quickly find the correct result with the
help of the two evolution operators.

However, TaylorGP will degrade when the dataset dimension in-
creases because of the local approximation of the Taylor polynomial.
In a high-dimensional dataset, a low order Taylor polynomial ob-
tained from the dataset only represents the datasetâ€™s local features,
not global features. So, our future work will involve investigating
how to utilize many low-order Taylor polynomials to represent
global features in high-dimensional datasets.

ACKNOWLEDGMENTS
This work is supported by China National Key Research Project
(No.2019YFC0312003)

REFERENCES
[1] Ignacio Arnaldo, Krzysztof Krawiec, and Una-May Oâ€™Reilly. 2014. Multiple
regression genetic programming. In Proceedings of the 2014 Annual Conference on
Genetic and Evolutionary Computation. 879â€“886.

[2] Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambat-
tista Parascandolo. 2021. Neural Symbolic Regression that scales. In Proceedings of
the 38th International Conference on Machine Learning, Vol. 139. PMLR, 936â€“945.
[3] Markus F Brameier and Wolfgang Banzhaf. 2007. Linear genetic programming.

Springer Science & Business Media.

[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 (2020).

TaylorGPGPLearnFFXLRKRRFSVMXGBoostGSGPBSR0.00.10.20.30.40.50.60.7Score(Normalized R2)TaylorGPXGBoostGSGPBSR0.69200.69250.6930TaylorGPGPLearnFFXLRKRRFSVMXGBoostGSGPBSR-0.4-0.20.00.20.40.6Score(Normalized R2)TaylorGPXGBoostGSGPBSR0.692250.692500.692750.69300TaylorGPGPLearnFFXLRKRRFSVMXGBoostGSGPBSR0.10.20.30.40.50.60.7Score(Normalized R2)Taylor Genetic Programming for Symbolic Regression

[5] William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabricio Olivetti de
Franca, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H. Moore. 2021.
Contemporary Symbolic Regression Methods and their Relative Performance.
In Advances in Neural Information Processing Systems Datasets and Benchmarks
Track.

[6] William La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, and Jason Moore.
2019. Learning concise representations for regression by evolving networks of
trees. In International Conference on Learning Representations.

[7] Q. Chen, B. Xue, and M. Zhang. 2019.

Improving Generalization of Genetic
Programming for Symbolic Regression With Angle-Driven Geometric Semantic
Operators.
IEEE Transactions on Evolutionary Computation 23, 3 (June 2019),
488â€“502.

[8] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
In Proceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785â€“794.

[9] Steffen Christensen and Franz Oppacher. 2002. An Analysis of Kozaâ€™s Computa-
tional Effort Statistic for Genetic Programming. In Genetic Programming, James A.
Foster, Evelyne Lutton, Julian Miller, Conor Ryan, and Andrea Tettamanzi (Eds.).
Springer Berlin Heidelberg, Berlin, Heidelberg, 182â€“191.

[10] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine

learning 20, 3 (1995), 273â€“297.

[11] Miles Cranmer, Alvaro Sanchez Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer,
David Spergel, and Shirley Ho. 2020. Discovering Symbolic Models from Deep
Learning with Inductive Biases. In Advances in Neural Information Processing
Systems, Vol. 33. Curran Associates, Inc., 17429â€“17442.

[12] Hend Dawood. 2011. Theories of interval arithmetic: mathematical foundations

and applications. LAP Lambert Academic Publishing.

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
NAACL. Association for Computational Linguistics, 4171â€“4186.

[14] Candida Ferreira. 2001. Gene Expression Programming: a New Adaptive Algo-

rithm for Solving Problems. Complex Systems 13, 2 (2001), 87â€“129.

[15] Roger Fletcher. 2013. Practical methods of optimization. John Wiley & Sons.
[16] Mark H Holmes. 2009. Introduction to the foundations of applied mathematics.

(2009).

[17] Harold Jeffreys, Bertha Jeffreys, and Bertha Swirles. 1999. Methods of mathemati-

cal physics. Cambridge university press.

[32] Georg Martius and Christoph H Lampert. 2016. Extrapolation and learning

equations. arXiv preprint arXiv:1610.02995 (2016).

[33] Trent McConaghy. 2011. FFX: Fast, scalable, deterministic symbolic regression
technology. In Genetic Programming Theory and Practice IX. Springer, 235â€“260.
[34] James McDermott, David R. White, Sean Luke, Luca Manzoni, Mauro Castelli,
Leonardo Vanneschi, Wojciech Jaskowski, Krzysztof Krawiec, Robin Harper,
Kenneth De Jong, and Una-May Oâ€™Reilly. 2012. Genetic Programming Needs
Better Benchmarks (GECCO â€™12). Association for Computing Machinery, New
York, NY, USA, 791â€“798.

[35] Robert I. McKay, Nguyen Xuan Hoai, Peter Alexander Whigham, Yin Shan, and
Michael Oâ€™Neill. 2010. Grammar-based Genetic Programming: a survey. Genetic
Programming and Evolvable Machines 11, 3 (Sept. 2010), 365â€“396.

[36] Julian Francis Miller. 2019. Cartesian genetic programming: its status and future.

Genetic Programming and Evolvable Machines (Aug. 2019), 1â€“40.

[37] Julian Francis Miller and Simon L. Harding. 2008. Cartesian Genetic Program-
ming. In Proceedings of the 10th Annual Conference Companion on Genetic and
Evolutionary Computation (GECCO â€™08). ACM, New York, NY, USA, 2701â€“2726.
[38] Alberto Moraglio, Krzysztof Krawiec, and Colin G. Johnson. 2012. Geometric
Semantic Genetic Programming. In Parallel Problem Solving from Nature - PPSN
XII. Vol. 7491. Springer Berlin Heidelberg, Berlin, Heidelberg, 21â€“31.

[39] T Nathan Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P Santiago,
Daniel M Faissol, and Brenden K Petersen. 2021. Symbolic Regression via Neural-
Guided Genetic Programming Population Seeding. In Advances in Neural Infor-
mation Processing Systems.

[40] Luis MuÃ±oz, Leonardo Trujillo, Sara Silva, Mauro Castelli, and Leonardo Van-
neschi. 2019. Evolving multidimensional transformations for symbolic regression
with M3GP. Memetic Computing 11, 2 (2019), 111â€“126.

[41] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz,
and Jason H. Moore. 2017. PMLB: a large benchmark suite for machine learning
evaluation and comparison. BioData Mining 10, 36 (11 Dec 2017), 1â€“13.

[42] Brenden K Petersen, Mikel Landajuela Larma, Terrell N Mundhenk, Claudio Prata
Santiago, Soo Kyung Kim, and Joanne Taery Kim. 2021. Deep symbolic regression:
Recovering mathematical expressions from data via risk-seeking policy gradients.
In International Conference on Learning Representations.

[43] Subham Sahoo, Christoph Lampert, and Georg Martius. 2018. Learning equations
for extrapolation and control. In International Conference on Machine Learning.
PMLR, 4442â€“4450.

[18] Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, and Jian Guo. 2019. Bayesian symbolic

[44] Michael Schmidt and Hod Lipson. 2009. Distilling Free-Form Natural Laws from

regression. arXiv preprint arXiv:1910.08892 (2019).

Experimental Data. Science 324, 5923 (2009), 81â€“85.

[19] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting
decision tree. Advances in neural information processing systems 30 (2017), 3146â€“
3154.

[20] Maarten Keijzer. 2003. Improving symbolic regression with interval arithmetic
and linear scaling. In European Conference on Genetic Programming. Springer,
70â€“82.

[21] James Kennedy and Russell Eberhart. 1995. Particle swarm optimization. In
Proceedings of ICNNâ€™95-international conference on neural networks, Vol. 4. IEEE,
1942â€“1948.

[22] Samuel Kim, Peter Y Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir
ÄŒeperiÄ‡, and Marin SoljaÄiÄ‡. 2020. Integration of neural network-based symbolic
regression in deep learning for scientific discovery. IEEE Transactions on Neural
Networks and Learning Systems (2020).

[23] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.

arXiv preprint arXiv:1312.6114 (2013).

[24] Michael F Korns. 2011. Accuracy in symbolic regression. In Genetic Programming

Theory and Practice IX. Springer, 129â€“151.

[25] Michael F Korns. 2013. A baseline symbolic regression algorithm. In Genetic

Programming Theory and Practice X. Springer, 117â€“137.

[26] Koza and John R. 1994. Genetic Programming as a Means for Programming
Computers by Natural Selection. Statistics and Computing 4, 2 (June 1994), 87â€“
112.

[27] Krzysztof Krawiec. 2002. Genetic programming-based construction of features
for machine learning and knowledge discovery tasks. Genetic Programming and
Evolvable Machines 3, 4 (2002), 329â€“343.

[28] Matt J Kusner, Brooks Paige, and JosÃ© Miguel HernÃ¡ndez-Lobato. 2017. Grammar
variational autoencoder. In International Conference on Machine Learning. PMLR,
1945â€“1954.

[29] William La Cava and Jason Moore. 2017. A General Feature Engineering Wrapper
for Machine Learning Using ğœ–-Lexicase Survival. In European Conference on
Genetic Programming. Springer, 80â€“95.

[30] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature

521, 7553 (2015), 436â€“444.

[31] Qiang Lu, Shuo Zhou, Fan Tao, Jake Luo, and Zhiguang Wang. 2021. Enhancing
gene expression programming based on space partition and jump for symbolic
regression. Information Sciences 547 (2021), 553â€“567.

[45] Leonardo Trujillo, Luis MuÃ±oz, Edgar GalvÃ¡n-LÃ³pez, and Sara Silva. 2016. neat
genetic programming: Controlling bloat naturally. Information Sciences 333 (2016),
21â€“43.

[46] Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu,
and Max Tegmark. 2020. AI Feynman 2.0: Pareto-optimal symbolic regression
exploiting graph modularity. arXiv preprint arXiv:2006.10782 (2020).

[47] Silviu-Marian Udrescu and Max Tegmark. 2020. AI Feynman: A physics-inspired
method for symbolic regression. Science Advances 6, 16 (2020), eaay2631.
[48] Nguyen Quang Uy, Nguyen Xuan Hoai, Michael Oâ€™Neill, Robert I McKay, and
Edgar GalvÃ¡n-LÃ³pez. 2011. Semantically-based crossover in genetic program-
ming: application to real-valued symbolic regression. Genetic Programming and
Evolvable Machines 12, 2 (2011), 91â€“119.

[49] Ekaterina J Vladislavleva, Guido F Smits, and Dick Den Hertog. 2008. Order
of nonlinearity as a complexity measure for models generated by symbolic
regression via pareto genetic programming. IEEE Transactions on Evolutionary
Computation 13, 2 (2008), 333â€“349.

[50] Sanford Weisberg. 2005. Applied linear regression. Vol. 528. John Wiley & Sons.
[51] Tony Worm and Kenneth Chiu. 2013. Prioritized grammar enumeration: symbolic
regression by dynamic programming. In Proceedings of the 15th annual conference
on Genetic and evolutionary computation. 1021â€“1028.

[52] Hengrui Xing, Ansaf Salleb-Aouissi, and Nakul Verma. 2021. Automated Sym-
bolic Law Discovery: A Computer Vision Approach. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 35. 660â€“668.

[53] Jinghui Zhong, Yusen Lin, Chengyu Lu, and Zhixing Huang. 2018. A deep learn-
ing assisted gene expression programming framework for symbolic regression
problems. In International Conference on Neural Information Processing. Springer,
530â€“541.

