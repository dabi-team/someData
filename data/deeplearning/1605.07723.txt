7
1
0
2

n
a
J

8

]
L
M

.
t
a
t
s
[

3
v
3
2
7
7
0
.
5
0
6
1
:
v
i
X
r
a

Data Programming:
Creating Large Training Sets, Quickly

Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, Christopher Ré
Stanford University
{ajratner,cdesa,senwu,dselsam,chrismre}@stanford.edu

Thursday 26th May, 2016

Abstract

Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers
of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and
expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of
training sets called data programming in which users express weak supervision strategies or domain heuristics as
labeling functions, which are programs that label subsets of the data, but that are noisy and may conﬂict. We show
that by explicitly representing this training set labeling process as a generative model, we can “denoise” the generated
training set, and establish theoretically that we can recover the parameters of these generative models in a handful
of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate
our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the
2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and
also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over
a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we
observed that data programming may be an easier way for non-experts to create machine learning models when training
data is limited or unavailable.

1 Introduction

Many of the major machine learning breakthroughs of the last decade have been catalyzed by the release of a new
labeled training dataset.1 Supervised learning approaches that use such datasets have increasingly become key building
blocks of applications throughout science and industry. This trend has also been fueled by the recent empirical success
of automated feature generation approaches, notably deep learning methods such as long short term memory (LSTM)
networks [14], which ameliorate the burden of feature engineering given large enough labeled training sets. For many
real-world applications, however, large hand-labeled training sets do not exist, and are prohibitively expensive to create
due to requirements that labelers be experts in the application domain. Furthermore, applications’ needs often change,
necessitating new or modiﬁed training sets.

To help reduce the cost of training set creation, we propose data programming, a paradigm for the programmatic
creation and modeling of training datasets. Data programming provides a simple, unifying framework for weak
supervision, in which training labels are noisy and may be from multiple, potentially overlapping sources. In data
programming, users encode this weak supervision in the form of labeling functions, which are user-deﬁned programs
that each provide a label for some subset of the data, and collectively generate a large but potentially overlapping set of
training labels. Many diﬀerent weak supervision approaches can be expressed as labeling functions, such as strategies
which utilize existing knowledge bases (as in distant supervision [22]), model many individual annotator’s labels (as
in crowdsourcing), or leverage a combination of domain-speciﬁc patterns and dictionaries. Because of this, labeling
functions may have widely varying error rates and may conﬂict on certain data points. To address this, we model the
labeling functions as a generative process, which lets us automatically denoise the resulting training set by learning the
accuracies of the labeling functions along with their correlation structure. In turn, we use this model of the training
set to optimize a stochastic version of the loss function of the discriminative model that we desire to train. We show

1 http://www.spacemachine.net/views/2016/3/datasets-over-algorithms

1

 
 
 
 
 
 
that, given certain conditions on the labeling functions, our method achieves the same asymptotic scaling as supervised
learning methods, but that our scaling depends on the amount of unlabeled data, and uses only a ﬁxed number of
labeling functions.

Data programming is in part motivated by the challenges that users faced when applying prior programmatic
supervision approaches, and is intended to be a new software engineering paradigm for the creation and management of
training sets. For example, consider the scenario when two labeling functions of diﬀering quality and scope overlap and
possibly conﬂict on certain training examples; in prior approaches the user would have to decide which one to use, or
how to somehow integrate the signal from both. In data programming, we accomplish this automatically by learning a
model of the training set that includes both labeling functions. Additionally, users are often aware of, or able to induce,
dependencies between their labeling functions. In data programming, users can provide a dependency graph to indicate,
for example, that two labeling functions are similar, or that one “ﬁxes” or “reinforces” another. We describe cases
in which we can learn the strength of these dependencies, and for which our generalization is again asymptotically
identical to the supervised case.

One further motivation for our method is driven by the observation that users often struggle with selecting features
for their models, which is a traditional development bottleneck given ﬁxed-size training sets. However, initial feedback
from users suggests that writing labeling functions in the framework of data programming may be easier [12]. While
the impact of a feature on end performance is dependent on the training set and on statistical characteristics of the
model, a labeling function has a simple and intuitive optimality criterion: that it labels data correctly. Motivated by this,
we explore whether we can ﬂip the traditional machine learning development process on its head, having users instead
focus on generating training sets large enough to support automatically-generated features.

Summary of Contributions and Outline Our ﬁrst contribution is the data programming framework, in which users
can implicitly describe a rich generative model for a training set in a more ﬂexible and general way than in previous
approaches. In Section 3, we ﬁrst explore a simple model in which labeling functions are conditionally independent.
We show here that under certain conditions, the sample complexity is nearly the same as in the labeled case. In
Section 4, we extend our results to more sophisticated data programming models, generalizing related results in
crowdsourcing [17]. In Section 5, we validate our approach experimentally on large real-world text relation extraction
tasks in genomics, pharmacogenomics and news domains, where we show an average 2.34 point F1 score improvement
over a baseline distant supervision approach—including what would have been a new competition-winning score for the
2014 TAC-KBP Slot Filling competition. Using LSTM-generated features, we additionally would have placed second
in this competition, achieving a 5.98 point F1 score gain over a state-of-the-art LSTM baseline [32]. Additionally, we
describe promising feedback from a usability study with a group of bioinformatics users.

2 Related Work

Our work builds on many previous approaches in machine learning. Distant supervision is one approach for program-
matically creating training sets. The canonical example is relation extraction from text, wherein a knowledge base of
known relations is heuristically mapped to an input corpus [8, 22]. Basic extensions group examples by surrounding
textual patterns, and cast the problem as a multiple instance learning one [15, 25]. Other extensions model the accuracy
of these surrounding textual patterns using a discriminative feature-based model [26], or generative models such as
hierarchical topic models [1, 27, 31]. Like our approach, these latter methods model a generative process of training
set creation, however in a proscribed way that is not based on user input as in our approach. There is also a wealth of
examples where additional heuristic patterns used to label training data are collected from unlabeled data [7] or directly
from users [21, 29], in a similar manner to our approach, but without any framework to deal with the fact that said labels
are explicitly noisy.

Crowdsourcing is widely used for various machine learning tasks [13, 18]. Of particular relevance to our problem
setting is the theoretical question of how to model the accuracy of various experts without ground truth available,
classically raised in the context of crowdsourcing [10]. More recent results provide formal guarantees even in the
absence of labeled data using various approaches [4, 9, 16, 17, 24, 33]. Our model can capture the basic model of the
crowdsourcing setting, and can be considered equivalent in the independent case (Sec. 3). However, in addition to
generalizing beyond getting inputs solely from human annotators, we also model user-supplied dependencies between
the “labelers” in our model, which is not natural within the context of crowdsourcing. Additionally, while crowdsourcing

2

d e f

lambda_1 ( x ) :
r e t u r n 1 i f

d e f

lambda_2 ( x ) :
r e t u r n -1 i f

d e f

lambda_3 ( x ) :
r e t u r n 1 i f

( x . gene , x . pheno )

i n KNOWN_RELATIONS_1 e l s e 0

r e . match ( r ’ . ∗ n o t (cid:23) c a u s e . ∗ ’ , x . t e x t _ b e t w e e n )

e l s e 0

r e . match ( r ’ . ∗ a s s o c i a t e d . ∗ ’ , x . t e x t _ b e t w e e n )

and ( x . gene , x . pheno )

i n KNOWN_RELATIONS_2 e l s e 0

(a) An example set of three labeling functions written by a user.

Y

λ2

λ3

λ1

(b) The generative model of a training
set deﬁned by the user input (unary fac-
tors omitted).

Figure 1: An example of extracting mentions of gene-disease relations from the scientiﬁc literature.

results focus on the regime of a large number of labelers each labeling a small subset of the data, we consider a small
set of labeling functions each labeling a large portion of the dataset.

Co-training is a classic procedure for eﬀectively utilizing both a small amount of labeled data and a large amount
of unlabeled data by selecting two conditionally independent views of the data [5]. In addition to not needing a set
of labeled data, and allowing for more than two views (labeling functions in our case), our approach allows explicit
modeling of dependencies between views, for example allowing observed issues with dependencies between views to
be explicitly modeled [19].

Boosting is a well known procedure for combining the output of many “weak” classiﬁers to create a strong classiﬁer
in a supervised setting [28]. Recently, boosting-like methods have been proposed which leverage unlabeled data in
addition to labeled data, which is also used to set constraints on the accuracies of the individual classiﬁers being
ensembled [3]. This is similar in spirit to our approach, except that labeled data is not explicitly necessary in ours, and
richer dependency structures between our “heuristic” classiﬁers (labeling functions) are supported.

The general case of learning with noisy labels is treated both in classical [20] and more recent contexts [23]. It has
also been studied speciﬁcally in the context of label-noise robust logistic regression [6]. We consider the more general
scenario where multiple noisy labeling functions can conﬂict and have dependencies.

3 The Data Programming Paradigm

In many applications, we would like to use machine learning, but we face the following challenges: (i) hand-labeled
training data is not available, and is prohibitively expensive to obtain in suﬃcient quantities as it requires expensive
domain expert labelers; (ii) related external knowledge bases are either unavailable or insuﬃciently speciﬁc, precluding
a traditional distant supervision or co-training approach; (iii) application speciﬁcations are in ﬂux, changing the model
we ultimately wish to learn.

In such a setting, we would like a simple, scalable and adaptable approach for supervising a model applicable to our
problem. More speciﬁcally, we would ideally like our approach to achieve (cid:15) expected loss with high probability, given
O(1) inputs of some sort from a domain-expert user, rather than the traditional ˜O((cid:15)−2) hand-labeled training examples
required by most supervised methods (where ˜O notation hides logarithmic factors). To this end, we propose data
programming, a paradigm for the programmatic creation of training sets, which enables domain-experts to more rapidly
train machine learning systems and has the potential for this type of scaling of expected loss. In data programming,
rather than manually labeling each example, users instead describe the processes by which these points could be labeled
by providing a set of heuristic rules called labeling functions.

In the remainder of this paper, we focus on a binary classiﬁcation task in which we have a distribution π over object
and class pairs (x, y) ∈ X × {−1, 1}, and we are concerned with minimizing the logistic loss under a linear model given
some features,

l(w) = E(x,y)∼π

(cid:105)
(cid:104)
log(1 + exp(−wT f (x)y))

,

where without loss of generality, we assume that (cid:107) f (x)(cid:107) ≤ 1. Then, a labeling function λi : X (cid:55)→ {−1, 0, 1} is a
user-deﬁned function that encodes some domain heuristic, which provides a (non-zero) label for some subset of the
objects. As part of a data programming speciﬁcation, a user provides some m labeling functions, which we denote in
vectorized form as λ : X (cid:55)→ {−1, 0, 1}m.

Example 3.1. To gain intuition about labeling functions, we describe a simple text relation extraction example. In
Figure 1, we consider the task of classifying co-occurring gene and disease mentions as either expressing a causal

3

relation or not. For example, given the sentence “Gene A causes disease B”, the object x = (A, B) has true class y = 1.
To construct a training set, the user writes three labeling functions (Figure 1a). In λ1, an external structured knowledge
base is used to label a few objects with relatively high accuracy, and is equivalent to a traditional distant supervision
rule (see Sec. 2). λ2 uses a purely heuristic approach to label a much larger number of examples with lower accuracy.
Finally, λ3 is a “hybrid” labeling function, which leverages a knowledge base and a heuristic.

A labeling function need not have perfect accuracy or recall; rather, it represents a pattern that the user wishes to
impart to their model and that is easier to encode as a labeling function than as a set of hand-labeled examples. As
illustrated in Ex. 3.1, labeling functions can be based on external knowledge bases, libraries or ontologies, can express
heuristic patterns, or some hybrid of these types; we see evidence for the existence of such diversity in our experiments
(Section 5). The use of labeling functions is also strictly more general than manual annotations, as a manual annotation
can always be directly encoded by a labeling function. Importantly, labeling functions can overlap, conﬂict, and even
have dependencies which users can provide as part of the data programming speciﬁcation (see Section 4); our approach
provides a simple framework for these inputs.

Independent Labeling Functions We ﬁrst describe a model in which the labeling functions label independently,
given the true label class. Under this model, each labeling function λi has some probability βi of labeling an object
and then some probability αi of labeling the object correctly; for simplicity we also assume here that each class has
probability 0.5. This model has distribution

µα,β(Λ, Y) = 1
2

m(cid:89)

i=1

(cid:0)βiαi1{Λi=Y} + βi(1 − αi)1{Λi=−Y} + (1 − βi)1{Λi=0}

(cid:1) ,

(1)

where Λ ∈ {−1, 0, 1}m contains the labels output by the labeling functions, and Y ∈ {−1, 1} is the predicted class. If we
allow the parameters α ∈ Rm and β ∈ Rm to vary, (1) speciﬁes a family of generative models. In order to expose the
scaling of the expected loss as the size of the unlabeled dataset changes, we will assume here that 0.3 ≤ βi ≤ 0.5 and
0.8 ≤ αi ≤ 0.9. We note that while these arbitrary constraints can be changed, they are roughly consistent with our
applied experience, where users tend to write high-accuracy and high-coverage labeling functions.

Our ﬁrst goal will be to learn which parameters (α, β) are most consistent with our observations—our unlabeled
training set—using maximum likelihood estimation. To do this for a particular training set S ⊂ X, we will solve the
problem

( ˆα, ˆβ) = arg max
α,β

(cid:88)

x∈S

log P(Λ,Y)∼µα,β (Λ = λ(x)) = arg max
α,β

(cid:88)

x∈S

log

(cid:88)

µα,β(λ(x), y(cid:48))

y(cid:48)∈{−1,1}









(2)

In other words, we are maximizing the probability that the observed labels produced on our training examples occur
under the generative model in (1). In our experiments, we use stochastic gradient descent to solve this problem; since
this is a standard technique, we defer its analysis to the appendix.

Noise-Aware Empirical Loss Given that our parameter learning phase has successfully found some ˆα and ˆβ that
accurately describe the training set, we can now proceed to estimate the parameter w which minimizes the expected risk
of a linear model over our feature mapping f , given ˆα, ˆβ. To do so, we deﬁne the noise-aware empirical risk L ˆα, ˆβ with
regularization parameter ρ, and compute the noise-aware empirical risk minimizer

ˆw = arg min
w

L ˆα, ˆβ(w; S ) = arg min

w

1
|S |

(cid:88)

x∈S

E(Λ,Y)∼µ ˆα, ˆβ

(cid:16)

(cid:104)

log

1 + e−wT f (x)Y (cid:17)(cid:12)(cid:12)(cid:12)(cid:12)

Λ = λ(x)

(cid:105) + ρ (cid:107)w(cid:107)2

(3)

This is a logistic regression problem, so it can be solved using stochastic gradient descent as well.

We can in fact prove that stochastic gradient descent running on (2) and (3) is guaranteed to produce accurate
estimates, under conditions which we describe now. First, the problem distribution π needs to be accurately modeled by
some distribution µ in the family that we are trying to learn. That is, for some α∗ and β∗,

∀Λ ∈ {−1, 0, 1}m, Y ∈ {−1, 1}, P(x,y)∼π∗ (λ(x) = Λ, y = Y) = µα∗,β∗(Λ, Y).

(4)

Second, given an example (x, y) ∼ π∗, the class label y must be independent of the features f (x) given the labels λ(x).
That is,

(x, y) ∼ π∗ ⇒ y ⊥ f (x) | λ(x).

(5)

4

Y

s

λ1

λ2

lambda_1 ( x ) = f ( x . word )
lambda_2 ( x ) = f ( x . lemma )

S i m i l a r ( lambda_1 ,

lambda_2 )

Y

λ1

r

λ3

f

λ2

lambda_1 ( x ) = f ( ’ . ∗ c a u s e . ∗ ’ )
lambda_2 ( x ) = f ( ’ . ∗ n o t (cid:23) c a u s e . ∗ ’ )
lambda_3 ( x ) = f ( ’ . ∗ c a u s e . ∗ ’ )

F i x e s ( lambda_1 ,
R e i n f o r c e s ( lambda_1 ,

lambda_2 )

lambda_3 )

Y

e

λ1

λ2

lambda_1 ( x ) = x i n DISEASES_A
lambda_2 ( x ) = x i n DISEASES_B

E x c l u d e s ( lambda_1 ,

lambda_2 )

Figure 2: Examples of labeling function dependency predicates.

This assumption encodes the idea that the labeling functions, while they may be arbitrarily dependent on the features,
provide suﬃcient information to accurately identify the class. Third, we assume that the algorithm used to solve (3) has
bounded generalization risk such that for some parameter χ,

(cid:20)

ES

(cid:104)

E ˆw

(cid:105)

L ˆα, ˆβ( ˆw; S )

− min
w

ES

(cid:104)

L ˆα, ˆβ(w; S )

(cid:105)(cid:21)

≤ χ.

(6)

Under these conditions, we make the following statement about the accuracy of our estimates, which is a simpliﬁed
version of a theorem that is detailed in the appendix.

Theorem 1. Suppose that we run data programming, solving the problems in (2) and (3) using stochastic gradient
descent to produce ( ˆα, ˆβ) and ˆw. Suppose further that our setup satisﬁes the conditions (4), (5), and (6), and suppose
that m ≥ 2000. Then for any (cid:15) > 0, if the number of labeling functions m and the size of the input dataset S are large
enough that

|S | ≥

356
(cid:15)2 log

(cid:19)

(cid:18) m
3(cid:15)

then our expected parameter error and generalization risk can be bounded by

(cid:107) ˆα − α∗(cid:107)2(cid:105)
(cid:104)

E

≤ m(cid:15)2

(cid:20)(cid:13)(cid:13)(cid:13) ˆβ − β∗(cid:13)(cid:13)(cid:13)

2(cid:21)

E

≤ m(cid:15)2

(cid:20)

E

l( ˆw) − min

w

(cid:21)
l(w)

≤ χ + (cid:15)
27ρ

.

We select m ≥ 2000 to simplify the statement of the theorem and give the reader a feel for how (cid:15) scales with respect
to |S |. The full theorem with scaling in each parameter (and for arbitrary m) is presented in the appendix. This result
establishes that to achieve both expected loss and parameter estimate error (cid:15), it suﬃces to have only m = O(1) labeling
functions and |S | = ˜O((cid:15)−2) training examples, which is the same asymptotic scaling exhibited by methods that use
labeled data. This means that data programming achieves the same learning rate as methods that use labeled data, while
requiring asymptotically less work from its users, who need to specify O(1) labeling functions rather than manually
label ˜O((cid:15)−2) examples. In contrast, in the crowdsourcing setting [17], the number of workers m tends to inﬁnity while
here it is constant while the dataset grows. These results provide some explanation of why our experimental results
suggest that a small number of rules with a large unlabeled training set can be eﬀective at even complex natural language
processing tasks.

4 Handling Dependencies

In our experience with data programming, we have found that users often write labeling functions that have clear
dependencies among them. As more labeling functions are added as the system is developed, an implicit dependency
structure arises naturally amongst the labeling functions: modeling these dependencies can in some cases improve
accuracy. We describe a method by which the user can specify this dependency knowledge as a dependency graph, and
show how the system can use it to produce better parameter estimates.

Label Function Dependency Graph To support the injection of dependency information into the model, we augment
the data programming speciﬁcation with a label function dependency graph, G ⊂ D × {1, . . . , m} × {1, . . . , m}, which

5

is a directed graph over the labeling functions, each of the edges of which is associated with a dependency type from
a class of dependencies D appropriate to the domain. From our experience with practitioners, we identiﬁed four
commonly-occurring types of dependencies as illustrative examples: similar, ﬁxing, reinforcing, and exclusive (see
Figure 2).

For example, suppose that we have two functions λ1 and λ2, and λ2 typically labels only when (i) λ1 also labels,
(ii) λ1 and λ2 disagree in their labeling, and (iii) λ2 is actually correct. We call this a ﬁxing dependency, since λ2 ﬁxes
mistakes made by λ1. If λ1 and λ2 were to typically agree rather than disagree, this would be a reinforcing dependency,
since λ2 reinforces a subset of the labels of λ1.

Modeling Dependencies The presence of dependency information means that we can no longer model our labels using
the simple Bayesian network in (1). Instead, we model our distribution as a factor graph. This standard technique lets us
describe the family of generative distributions in terms of a known factor function h : {−1, 0, 1}m × {−1, 1} (cid:55)→ {−1, 0, 1}M
(in which each entry hi represents a factor), and an unknown parameter θ ∈ RM as

µθ(Λ, Y) = Z−1

θ exp(θT h(Λ, Y)),

where Zθ is the partition function which ensures that µ is a distribution. Next, we will describe how we deﬁne h using
information from the dependency graph.

To construct h, we will start with some base factors, which we inherit from (1), and then augment them with

additional factors representing dependencies. For all i ∈ {1, . . . , m}, we let

h0(Λ, Y) = Y,

hi(Λ, Y) = ΛiY,

hm+i(Λ, Y) = Λi,

h2m+i(Λ, Y) = Λ2

i Y,

h3m+i(Λ, Y) = Λ2
i .

These factors alone are suﬃcient to describe any distribution for which the labels are mutually independent, given the
class: this includes the independent family in (1).

We now proceed by adding additional factors to h, which model the dependencies encoded in G. For each
dependency edge (d, i, j), we add one or more factors to h as follows. For a near-duplicate dependency on (i, j), we
add a single factor hι(Λ, Y) = 1{Λi = Λ j}, which increases our prior probability that the labels will agree. For a ﬁxing
dependency, we add two factors, hι(Λ, Y) = −1{Λi = 0 ∧ Λ j (cid:44) 0} and hι+1(Λ, Y) = 1{Λi = −Y ∧ Λ j = Y}, which encode
the idea that λ j labels only when λi does, and that λ j ﬁxes errors made by λi. The factors for a reinforcing dependency
are the same, except that hι+1(Λ, Y) = 1{Λi = Y ∧ Λ j = Y}. Finally, for an exclusive dependency, we have a single factor
hι(Λ, Y) = −1{Λi (cid:44) 0 ∧ Λ j (cid:44) 0}.

Learning with Dependencies We can again solve a maximum likelihood problem like (2) to learn the parameter ˆθ.
Using the results, we can continue on to ﬁnd the noise-aware empirical loss minimizer by solving the problem in (3).
In order to solve these problems in the dependent case, we typically invoke stochastic gradient descent, using Gibbs
sampling to sample from the distributions used in the gradient update. Under conditions similar to those in Section 3,
we can again provide a bound on the accuracy of these results. We deﬁne these conditions now. First, there must be
some set Θ ⊂ RM that we know our parameter lies in. This is analogous to the assumptions on αi and βi we made in
Section 3, and we can state the following analogue of (4):

∃θ∗ ∈ Θ s.t. ∀(Λ, Y) ∈ {−1, 0, 1}m × {−1, 1}, P(x,y)∼π∗ (λ(x) = Λ, y = Y) = µθ∗(Λ, Y).

(7)

Second, for any θ ∈ Θ, it must be possible to accurately learn θ from full (i.e. labeled) samples of µθ. More speciﬁcally,
there exists an unbiased estimator ˆθ(T ) that is a function of some dataset T of independent samples from µθ such that,
for some c > 0 and for all θ ∈ Θ,

Cov
Third, for any two feasible models θ1 and θ2 ∈ Θ,

(cid:17)
(cid:16)
ˆθ(T )

(cid:22) (2c |T |)−1I.

E(Λ1,Y1)∼µθ1

(cid:104)
Var(Λ2,Y2)∼µθ2

(Y2|Λ1 = Λ2)

(cid:105)

≤ cM−1.

(8)

(9)

That is, we’ll usually be reasonably sure in our guess for the value of Y, even if we guess using distribution µθ2 while the
the labeling functions were actually sampled from (the possibly totally diﬀerent) µθ1 . We can now prove the following
result about the accuracy of our estimates.

6

Features

Hand-tuned

LSTM

Method
ITR
DP
ITR
DP

KBP (News)
Rec.
26.72
29.21
28.81
27.88

Prec.
51.15
50.52
37.68
47.47

F1
35.10
37.02
32.66
35.78

Genomics
Rec.
41.67
43.43
50.76
48.48

Prec.
83.76
83.90
69.07
75.48

F1
55.65
57.24
58.52
58.99

Pharmacogenomics
F1
Rec.
57.23
49.32
60.83
54.80
37.23
43.84
42.17
47.95

Prec.
68.16
68.36
32.35
37.63

Table 1: Precision/Recall/F1 scores using data programming (DP), as compared to distant supervision ITR approach,
with both hand-tuned and LSTM-generated features.

Theorem 2. Suppose that we run stochastic gradient descent to produce ˆθ and ˆw, and that our setup satisﬁes the
conditions (5)-(9). Then for any (cid:15) > 0, if the input dataset S is large enough that

|S | ≥

2
c2(cid:15)2 log

(cid:32)

2 (cid:107)θ0 − θ∗(cid:107)2
(cid:15)

(cid:33)

,

then our expected parameter error and generalization risk can be bounded by

(cid:20)(cid:13)(cid:13)(cid:13)ˆθ − θ∗(cid:13)(cid:13)(cid:13)

2(cid:21)

E

≤ M(cid:15)2

(cid:20)

E

l( ˆw) − min

w

(cid:21)

l(w)

≤ χ + c(cid:15)
2ρ

.

As in the independent case, this shows that we need only |S | = ˜O((cid:15)−2) unlabeled training examples to achieve
error O((cid:15)), which is the same asymptotic scaling as supervised learning methods. This suggests that while we pay
a computational penalty for richer dependency structures, we are no less statistically eﬃcient. In the appendix, we
provide more details, including an explicit description of the algorithm and the step size used to achieve this result.

5 Experiments

We seek to experimentally validate three claims about our approach. Our ﬁrst claim is that data programming can
be an eﬀective paradigm for building high quality machine learning systems, which we test across three real-world
relation extraction applications. Our second claim is that data programming can be used successfully in conjunction
with automatic feature generation methods, such as LSTM models. Finally, our third claim is that data programming is
an intuitive and productive framework for domain-expert users, and we report on our initial user studies.

Relation Mention Extraction Tasks
In the relation mention extraction task, our objects are relation mention can-
didates x = (e1, e2), which are pairs of entity mentions e1, e2 in unstructured text, and our goal is to learn a model
that classiﬁes each candidate as either a true textual assertion of the relation R(e1, e2) or not. We examine a news
application from the 2014 TAC-KBP Slot Filling challenge2, where we extract relations between real-world entities
from articles [2]; a clinical genomics application, where we extract causal relations between genetic mutations and
phenotypes from the scientiﬁc literature3; and a pharmacogenomics application where we extract interactions between
genes, also from the scientiﬁc literature [21]; further details are included in the Appendix.

For each application, we or our collaborators originally built a system where a training set was programmatically
generated by ordering the labeling functions as a sequence of if-then-return statements, and for each candidate, taking
the ﬁrst label emitted by this script as the training label. We refer to this as the if-then-return (ITR) approach, and note
that it often required signiﬁcant domain expert development time to tune (weeks or more). For this set of experiments,
we then used the same labeling function sets within the framework of data programming. For all experiments, we
evaluated on a blind hand-labeled evaluation set. In Table 1, we see that we achieve consistent improvements: on average
by 2.34 points in F1 score, including what would have been a winning score on the 2014 TAC-KBP challenge [30].

We observed these performance gains across applications with very diﬀerent labeling function sets. We describe the
labeling function summary statistics—coverage is the percentage of objects that had at least one label, overlap is the
percentage of objects with more than one label, and conﬂict is the percentage of objects with conﬂicting labels—and see
in Table 2 that even in scenarios where m is small, and conﬂict and overlap is relatively less common, we still realize

2http://www.nist.gov/tac/2014/KBP/
3https://github.com/HazyResearch/dd-genomics

7

performance gains. Additionally, on a disease mention extraction task (see Usability Study), which was written from
scratch within the data programming paradigm, allowing developers to supply dependencies of the basic types outlined
in Sec. 4 led to a 2.3 point F1 score boost.

Application

# of LFs Coverage

KBP (News)
Genomics
Pharmacogenomics
Diseases

40
146
7
12

29.39
53.61
7.70
53.32

|S λ(cid:44)0|

2.03M
256K
129K
418K

Overlap Conﬂict

1.38
26.71
0.35
31.81

0.15
2.05
0.32
0.98

F1 Score Improvement
HT
1.92
1.59
3.60
N/A

LSTM
3.12
0.47
4.94
N/A

Table 2: Labeling function (LF) summary statistics, sizes of generated training sets S λ(cid:44)0 (only counting non-zero labels), and
relative F1 score improvement over baseline IRT methods for hand-tuned (HT) and LSTM-generated (LSTM) feature sets.

Automatically-generated Features We additionally compare both hand-tuned and automatically-generated features,
where the latter are learned via an LSTM recurrent neural network (RNN) [14]. Conventional wisdom states that deep
learning methods such as RNNs are prone to overﬁtting to the biases of the imperfect rules used for programmatic
supervision. In our experiments, however, we ﬁnd that using data programming to denoise the labels can mitigate this
issue, and we report a 9.79 point boost to precision and a 3.12 point F1 score improvement on the benchmark 2014
TAC-KBP (News) task, over the baseline if-then-return approach. Additionally for comparison, our approach is a 5.98
point F1 score improvement over a state-of-the-art LSTM approach [32].

Usability Study One of our hopes is that a user without expertise in ML will be more productive iterating on labeling
functions than on features. To test this, we arranged a hackathon involving a handful of bioinformatics researchers,
using our open-source information extraction framework Snorkel4 (formerly DDLite). Their goal was to build a
disease tagging system which is a common and important challenge in the bioinformatics domain [11]. The hackathon
participants did not have access to a labeled training set nor did they perform any feature engineering. The entire eﬀort
was restricted to iterative labeling function development and the setup of candidates to be classiﬁed. In under eight
hours, they had created a training set that led to a model which scored within 10 points of F1 of the supervised baseline;
the gap was mainly due to recall issue in the candidate extraction phase. This suggests data programming may be a
promising way to build high quality extractors, quickly.

6 Conclusion and Future Work

We introduced data programming, a new approach to generating large labeled training sets. We demonstrated that our
approach can be used with automatic feature generation techniques to achieve high quality results. We also provided
anecdotal evidence that our methods may be easier for domain experts to use. We hope to explore the limits of our
approach on other machine learning tasks that have been held back by the lack of high-quality supervised datasets,
including those in other domains such imaging and structured prediction.

Acknowledgements Thanks to Theodoros Rekatsinas, Manas Joglekar, Henry Ehrenberg, Jason Fries, Percy Liang,
the DeepDive and DDLite users and many others for their helpful conversations. The authors acknowledge the support
of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSFCCF-1111943; DOE 108845; NSF CCF-1337375; DARPA
FA8750-13-2-0039; NSF IIS-1353606;ONR N000141210041 and N000141310129; NIH U54EB020405; DARPA’s
SIMPLEX program; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American
Family Insurance; Google; and Toshiba. The views and conclusions expressed in this material are those of the authors
and should not be interpreted as necessarily representing the oﬃcial policies or endorsements, either expressed or
implied, of DARPA, AFRL, NSF, ONR, NIH, or the U.S. Government.

4snorkel.stanford.edu

8

References

[1] E. Alfonseca, K. Filippova, J.-Y. Delort, and G. Garrido. Pattern learning for relation extraction with a hierarchical

topic model. In Proceedings of the ACL.

[2] G. Angeli, S. Gupta, M. Jose, C. D. Manning, C. Ré, J. Tibshirani, J. Y. Wu, S. Wu, and C. Zhang. Stanford’s

2014 slot ﬁlling systems. TAC KBP, 695, 2014.

[3] A. Balsubramani and Y. Freund. Scalable semi-supervised aggregation of classiﬁers. In Advances in Neural

Information Processing Systems, pages 1351–1359, 2015.

[4] D. Berend and A. Kontorovich. Consistency of weighted majority votes. In NIPS 2014.

[5] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh

annual conference on Computational learning theory, pages 92–100. ACM, 1998.

[6] J. Bootkrajang and A. Kabán. Label-noise robust logistic regression and its applications. In Machine Learning

and Knowledge Discovery in Databases, pages 143–158. Springer, 2012.

[7] R. Bunescu and R. Mooney. Learning to extract relations from the web using minimal supervision. In Annual

meeting-association for Computational Linguistics, volume 45, page 576, 2007.

[8] M. Craven, J. Kumlien, et al. Constructing biological knowledge bases by extracting information from text sources.

In ISMB, volume 1999, pages 77–86, 1999.

[9] N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi. Aggregating crowdsourced binary ratings. In Proceedings of

the 22Nd International Conference on World Wide Web, WWW ’13, pages 285–294, 2013.

[10] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the em algorithm.

Applied statistics, pages 20–28, 1979.

[11] R. I. Do˘gan and Z. Lu. An improved corpus of disease mentions in pubmed citations. In Proceedings of the 2012

workshop on biomedical natural language processing.

[12] H. R. Ehrenberg, J. Shin, A. J. Ratner, J. A. Fries, and C. Ré. Data programming with ddlite: putting humans in a

diﬀerent part of the loop. In HILDA@ SIGMOD, page 13, 2016.

[13] H. Gao, G. Barbier, R. Goolsby, and D. Zeng. Harnessing the crowdsourcing power of social media for disaster

relief. Technical report, DTIC Document, 2011.

[14] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

[15] R. Hoﬀmann, C. Zhang, X. Ling, L. Zettlemoyer, and D. S. Weld. Knowledge-based weak supervision for

information extraction of overlapping relations. In Proceedings of the ACL.

[16] M. Joglekar, H. Garcia-Molina, and A. Parameswaran. Comprehensive and reliable crowd assessment algorithms.

In Data Engineering (ICDE), 2015 IEEE 31st International Conference on.

[17] D. R. Karger, S. Oh, and D. Shah. Iterative learning for reliable crowdsourcing systems. In Advances in neural

information processing systems, pages 1953–1961, 2011.

[18] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al.
Visual genome: Connecting language and vision using crowdsourced dense image annotations. arXiv preprint
arXiv:1602.07332, 2016.

[19] M.-A. Krogel and T. Scheﬀer. Multi-relational learning, text mining, and semi-supervised learning for functional

genomics. Machine Learning, 57(1-2):61–81, 2004.

[20] G. Lugosi. Learning with an unreliable teacher. Pattern Recognition, 25(1):79 – 87, 1992.

[21] E. K. Mallory, C. Zhang, C. Ré, and R. B. Altman. Large-scale extraction of gene interactions from full-text

literature using deepdive. Bioinformatics, 2015.

9

[22] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without labeled data. In

Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL, 2009.

[23] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari. Learning with noisy labels. In Advances in Neural

Information Processing Systems 26.

[24] F. Parisi, F. Strino, B. Nadler, and Y. Kluger. Ranking and combining multiple predictors without labeled data.

Proceedings of the National Academy of Sciences, 111(4):1253–1258, 2014.

[25] S. Riedel, L. Yao, and A. McCallum. Modeling relations and their mentions without labeled text. In Machine

Learning and Knowledge Discovery in Databases, pages 148–163. Springer, 2010.

[26] B. Roth and D. Klakow. Feature-based models for improving the quality of noisy training data for relation

extraction. In Proceedings of the 22nd ACM Conference on Knowledge management.

[27] B. Roth and D. Klakow. Combining generative and discriminative model scores for distant supervision. In

EMNLP, pages 24–29, 2013.

[28] R. E. Schapire and Y. Freund. Boosting: Foundations and algorithms. MIT press, 2012.

[29] J. Shin, S. Wu, F. Wang, C. De Sa, C. Zhang, and C. Ré. Incremental knowledge base construction using deepdive.

Proceedings of the VLDB Endowment, 8(11):1310–1321, 2015.

[30] M. Surdeanu and H. Ji. Overview of the english slot ﬁlling track at the tac2014 knowledge base population

evaluation. In Proc. Text Analysis Conference (TAC2014), 2014.

[31] S. Takamatsu, I. Sato, and H. Nakagawa. Reducing wrong labels in distant supervision for relation extraction. In

Proceedings of the ACL.

[32] P. Verga, D. Belanger, E. Strubell, B. Roth, and A. McCallum. Multilingual relation extraction using compositional

universal schema. arXiv preprint arXiv:1511.06396, 2015.

[33] Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan. Spectral methods meet em: A provably optimal algorithm for

crowdsourcing. In Advances in Neural Information Processing Systems 27, pages 1260–1268. 2014.

10

A General Theoretical Results

In this section, we will state the full form of the theoretical results we alluded to in the body of the paper. First, we
restate, in long form, our setup and assumptions.

We assume that, for some function h : {−1, 0, 1}m × {−1, 1} (cid:55)→ {−1, 0, 1}M of suﬃcient statistics, we are concerned

with learning distributions, over the set Ω = {−1, 0, 1}m × {−1, 1}, of the form

πθ(Λ, Y) = 1
Zθ

exp(θT h(Λ, Y)),

(10)

where θ ∈ RM is a parameter, and Zθ is the partition function that makes this a distribution. We assume that we are
given, i.e. can derive from the data programming speciﬁcation, some set Θ of feasible parameters. This set must have
the following two properties.

First, for any θ ∈ Θ, learning the parameter θ from (full) samples from πθ is possible, at least in some sense. More
speciﬁcally, there exists an unbiased estimator ˆθ that is a function of some number D samples from πθ (and is unbiased
for all θ ∈ Θ) such that, for all θ ∈ Θ and for some c > 0,

(cid:17)

(cid:16)

ˆθ

(cid:22)

Cov

I
2cD

.

Second, for any θ1, θ2 ∈ Θ,

E(λ2,y2)∼πθ2

(cid:104)
Var(λ1,y1)∼πθ1

(y1|λ1 = λ2)

(cid:105)

≤

c
M

.

(11)

(12)

That is, we’ll always be reasonably certain in our guess for the value of y, even if we are totally wrong about the true
parameter θ.

On the other hand, we are also concerned with a distribution π∗ which ranges over the set X × {−1, 1}, and represents
the distribution of training and test examples we are using to learn. These objects are associated with a labeling function
λ : X (cid:55)→ {−1, 0, 1}m and a feature function f : X (cid:55)→ Rn. We make three assumptions about this distribution. First, we
assume that, given (x, y) ∼ π∗, the class label y is independent of the features f (x) given the labels λ(x). That is,

(x, y) ∼ π∗ ⇒ y ⊥ f (x) | λ(x).

(13)

Second, we assume that we can describe the relationship between λ(x) and y in terms of our family in (10) above. That
is, for some parameter θ∗ ∈ Θ,

P(x,y)∼π∗ (λ(x) = Λ, y = Y) = πθ∗(Λ, Y).

Third, we assume that the features themselves are bounded; for all x ∈ X,

(cid:107) f (x)(cid:107) ≤ 1.

(14)

(15)

Our goal is twofold. First, we want to recover some estimate ˆθ of the true parameter θ∗. Second, we want to produce

a parameter ˆw that minimizes the regularized logistic loss

l(w) = E(x,y)∼π∗

(cid:104)

log(1 + exp(−wT f (x)y))

(cid:105) + ρ (cid:107)w(cid:107)2 .

We actually accomplish this by minimizing a noise-aware loss function, given our recovered parameter ˆθ,

lˆθ(w) = E( ¯x,¯y)∼π∗

(cid:104)
E(Λ,Y)∼πˆθ

(cid:104)

log(1 + exp(−wT f ( ¯x)Y))

(cid:12)(cid:12)(cid:12)Λ = λ( ¯x)

(cid:105)(cid:105) + ρ (cid:107)w(cid:107)2 .

In fact we can’t even minimize this; rather, we will be minimizing the empirical noise-aware loss function, which is
only this in expectation. Since the analysis of logistic regression is not itself interesting, we assume that we are able to
run some algorithm that produces an estimate ˆw which satisﬁes, for some χ > 0,

(cid:20)

E

lˆθ( ˆw) − min
w

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

lˆθ(w)

(cid:21)

ˆθ

≤ χ.

(16)

The algorithm chosen can be anything, but in practice, we use stochastic gradient descent.

We learn ˆθ and ˆw by running the following algorithm.
Under these assumptions, we are able to prove the following theorem about the behavior of Algorithm 1.

11

Algorithm 1 Data Programming
Require: Step size η, dataset S ⊂ X, and initial parameter θ0 ∈ Θ.

θ → θ0
for all x ∈ S do

Independently sample (Λ, Y) from πθ, and ( ¯Λ, ¯Y) from πθ conditionally given Λ = λ(x).
θ ← θ + η(h(Λ, Y) − h( ¯Λ, ¯Y)).
θ = PΘ(θ)

(cid:46) Here, PΘ denotes orthogonal projection onto Θ.

end for
Compute ˆw using the algorithm described in (15)
return (θ, ˆw).

Theorem A.1. Suppose that we run Algorithm 1 on a data programming speciﬁcation that satisﬁes conditions (11),
(12), (13), (14), (15), and (16). Suppose further that, for some parameter (cid:15) > 0, we use step size

and our dataset is of a size that satisﬁes

η = c(cid:15)2
4

|S | = 2

c2(cid:15)2 log

(cid:32)

2 (cid:107)θ0 − θ∗(cid:107)2
(cid:15)

(cid:33)

.

Then, we can bound the expected parameter error with

and the expected risk with

(cid:20)(cid:13)(cid:13)(cid:13)ˆθ − θ∗(cid:13)(cid:13)(cid:13)

2(cid:21)

E

≤ (cid:15)2M

(cid:20)

E

l( ˆw) − min

w

(cid:21)

l(w)

≤ χ + c(cid:15)
2ρ

.

This theorem’s conclusions and assumptions can readily be seen to be identical to those of Theorem 2 in the main
body of the paper, except that they apply to the slightly more general case of arbitrary h, rather than h of the explicit
form described in the body. Therefore, in order to prove Theorem 2, it suﬃces to prove Theorem A.1, which we will do
in Section C.

B Theoretical Results for Independent Model

For the independent model, we can obtain a more speciﬁc version of Theorem A.1. In the independent model, the
variables are, as before, Λ ∈ {−1, 0, 1}m and Y ∈ {−1, 1}. The suﬃcient statistics are ΛiY and Λ2
i .
To produce results that make intuitive sense, we also deﬁne the alternate parameterization

Pπ (Λi|Y) =






1+γi
2

Λi = Y
βi
(1 − βi) Λ = 0
βi

Λi = −Y

1−γi
2

.

In comparison to the parameters used in the body of the paper, we have

αi = 1 + γi
2
Now, we are concerned with models that are feasible. For a model to be feasible (i.e. for θ ∈ Θ), we require that it

.

satisfy, for some constants γmin > 0, γmax > 0, and βmin,

γmin ≤ γi ≤ γmax

βmin ≤ βi ≤

1
2

.

For 0 ≤ β ≤ 1 and −1 ≤ γ ≤ 1.

For this model, we can prove the following corollary to Theorem A.1

12

Corollary B.1. Suppose that we run Algorithm 1 on an independent data programming speciﬁcation that satisﬁes
conditions (13), (14), (15), and (16). Furthermore, assume that the number of labeling functions we use satisﬁes

m ≥

9.34 artanh(γmax)
(γβ)minγ2

min

log

(cid:32)

(cid:33)

.

24m
βmin

Suppose further that, for some parameter (cid:15) > 0, we use step size

and our dataset is of a size that satisﬁes

η = βmin(cid:15)2
16

|S | = 32
β2
min(cid:15)2

(cid:32)

log

2 (cid:107)θ0 − θ∗(cid:107)2
(cid:15)

(cid:33)

.

Then, we can bound the expected parameter error with

and the expected risk with

(cid:20)(cid:13)(cid:13)(cid:13)ˆθ − θ∗(cid:13)(cid:13)(cid:13)

2(cid:21)

E

≤ (cid:15)2M

(cid:20)

E

l( ˆw) − min

w

(cid:21)

l(w)

≤ χ + βmin(cid:15)
8ρ

.

We can see that if, as stated in the body of the paper, βi ≥ 0.3 and 0.8 ≤ αi ≤ 0.9 (which is equivalent to

0.6 ≤ γi ≤ 0.8), then

2000 ≥ 1896.13 = 9.34 artanh(0.8)

0.3 · 0.63

(cid:32)

log

24 · 2000
0.3

(cid:33)

.

This means that, as stated in the paper, m = 2000 is suﬃcient for this corollary to hold with

|S | =

32
0.32 · (cid:15)2 log

(cid:32)

2m(artanh(0.8) − artanh(0.6))2
(cid:15)

(cid:33)

= 356

(cid:15)2 log

(cid:19)

.

(cid:18) m
3(cid:15)

Thus, proving Corollary B.1 is suﬃcient to prove Theorem 1 from the body of the paper. We will prove Corollary B.1
in Section E

C Proof of Theorem A.1

First, we state some lemmas that will be useful in the proof to come.

Lemma D.1. Given a family of maximum-entropy distributions

πθ(x) = 1
Zθ

exp(θT h(x)),

for some function of suﬃcient statistics h : Ω (cid:55)→ RM, if we let J : RM (cid:55)→ R be the maximum log-likelihood objective for
some event A ⊆ Ω,

J(θ) = log Px∼πθ (x ∈ A) ,

then its gradient is

and its Hessian is

∇J(θ) = Ex∼πθ [h(x)|x ∈ A] − Ex∼πθ [h(x)]

∇2J(θ) = Covx∼πθ (h(x)|x ∈ A) − Covx∼πθ (h(x)) .

13

Lemma D.2. Suppose that we are looking at a distribution from a data programming label model. That is, our
maximum-entropy distribution can now be written in terms of two variables, the labeling function values λ ∈ {−1, 0, 1}
and the class y ∈ {−1, 1}, as

πθ(λ, y) = 1
Zθ

exp(θT h(λ, y)),

where we assume without loss of generality that for some M, h(λ, y) ∈ RM and (cid:107)h(λ, y)(cid:107)∞ ≤ 1. If we let J : RM (cid:55)→ R
be the maximum expected log-likelihood objective, under another distribution π∗, for the event associated with the
observed labeling function values λ,

J(θ) = E(λ∗,y∗)∼π∗

(cid:104)
log P(λ,y)∼πθ (λ = λ∗)

(cid:105)

,

then its Hessian can be bounded with

∇2J(θ) (cid:22) MIE(λ∗,y∗)∼π∗

(cid:104)
Var(λ,y)∼πθ (y|λ = λ∗)

(cid:105)

− I(θ),

where I(θ) is the Fisher information.

Lemma D.3. Suppose that we are looking at a data programming distribution, as described in the text of Lemma D.2.
Suppose further that we are concerned with some feasible set of parameters Θ ⊂ RM, such that the any model with
parameters in this space satisﬁes the following two conditions.

First, for any θ ∈ Θ, learning the parameter θ from (full) samples from πθ is possible, at least in some sense. More
speciﬁcally, there exists an unbiased estimator ˆθ that is a function of some number D samples from πθ (and is unbiased
for all θ ∈ Θ) such that, for all θ ∈ Θ and for some c > 0,

Second, for any θ, θ∗ ∈ Θ,

(cid:17)

(cid:16)

ˆθ

(cid:22)

Cov

I
2cD

.

E(λ∗,y∗)∼π∗

(cid:104)

(cid:105)
Var(λ,y)∼πθ (y|λ = λ∗)

≤

c
M

.

That is, we’ll always be reasonably certain in our guess for the value of y, even if we are totally wrong about the true
parameter θ∗.

Under these conditions, the function J is strongly concave on Θ with parameter of strong convexity c.

Lemma D.4. Suppose that we are looking at a data programming maximum likelihood estimation problem, as described
in the text of Lemma D.2. Suppose further that the objective function J is strongly concave with parameter c > 0.

If we run stochastic gradient descent on objective J, using unbiased samples from a true distribution πθ∗, where

θ∗ ∈ Θ, then if we use step size

and run (using a fresh sample at each iteration) for T steps, where

η = c(cid:15)2
4

T = 2

c2(cid:15)2 log

(cid:33)

(cid:32)

2 (cid:107)θ0 − θ∗(cid:107)2
(cid:15)

then we can bound the expected parameter estimation error with

(cid:20)(cid:13)(cid:13)(cid:13)ˆθ − θ∗(cid:13)(cid:13)(cid:13)

2(cid:21)

E

≤ (cid:15)2M.

Lemma D.5. Assume in our model that, without loss of generality, (cid:107) f (x)(cid:107) ≤ 1 for all x, and that in our true model π∗,
the class y is independent of the features f (x) given the labels λ(x).

Suppose that we now want to solve the expected loss minimization problem wherein we minimize the objective

l(w) = E(x,y)∼π∗

(cid:104)
log(1 + exp(−wT f (x)y))

(cid:105) + ρ (cid:107)w(cid:107)2 .

We actually accomplish this by minimizing our noise-aware loss function, given our chosen parameter ˆθ,

lˆθ(w) = E( ¯x,¯y)∼π∗

(cid:104)
E(Λ,Y)∼πˆθ

(cid:104)

log(1 + exp(−wT f ( ¯x)Y))

(cid:12)(cid:12)(cid:12)Λ = λ( ¯x)

(cid:105)(cid:105) + ρ (cid:107)w(cid:107)2 .

14

In fact we can’t even minimize this; rather, we will be minimizing the empirical noise-aware loss function, which is only
this in expectation. Suppose that doing so produces an estimate ˆw which satisﬁes, for some χ > 0,
(cid:20)

(cid:21)

E

lˆθ( ˆw) − min
w

lˆθ(w)

≤ χ.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ˆθ

(Here, the expectation is taken with respect to only the random variable ˆw.) Then, we can bound the expected risk with

(cid:20)

E

l( ˆw) − min

w

(cid:21)

l(w)

≤ χ + c(cid:15)
2ρ

.

Now, we restate and prove our main theorem.

Theorem A.1. Suppose that we run Algorithm 1 on a data programming speciﬁcation that satisﬁes conditions (11),
(12), (13), (14), (15), and (16). Suppose further that, for some parameter (cid:15) > 0, we use step size

and our dataset is of a size that satisﬁes

η = c(cid:15)2
4

|S | = 2

c2(cid:15)2 log

(cid:32)

2 (cid:107)θ0 − θ∗(cid:107)2
(cid:15)

(cid:33)

.

Then, we can bound the expected parameter error with

and the expected risk with

(cid:20)(cid:13)(cid:13)(cid:13)ˆθ − θ∗(cid:13)(cid:13)(cid:13)

2(cid:21)

E

≤ (cid:15)2M

(cid:20)

E

l( ˆw) − min

w

(cid:21)

l(w)

≤ χ + c(cid:15)
2ρ

.

Proof. The bounds on the expected parameter estimation error follow directly from Lemma D.4, and the remainder of
(cid:3)
the theorem follows directly from Lemma D.5.

D Proofs of Lemmas

Lemma D.1. Given a family of maximum-entropy distributions

πθ(x) = 1
Zθ

exp(θT h(x)),

for some function of suﬃcient statistics h : Ω (cid:55)→ RM, if we let J : RM (cid:55)→ R be the maximum log-likelihood objective for
some event A ⊆ Ω,

J(θ) = log Px∼πθ (x ∈ A) ,

then its gradient is

and its Hessian is

Proof. For the gradient,

∇J(θ) = Ex∼πθ [h(x)|x ∈ A] − Ex∼πθ [h(x)]

∇2J(θ) = Covx∼πθ (h(x)|x ∈ A) − Covx∼πθ (h(x)) .

∇J(θ) = ∇ log Pπθ (A)

= ∇ log

= ∇ log

(cid:33)

(cid:32) (cid:80)
(cid:80)


(cid:88)



x∈A

x∈A exp(θT h(x))
x∈Ω exp(θT h(x))


exp(θT h(x))

− ∇ log


(cid:88)



x∈Ω



exp(θT h(x))

(cid:80)

=

x∈A h(x) exp(θT h(x))
(cid:80)
x∈A exp(θT h(x))

−

x∈Ω h(x) exp(θT h(x))
(cid:80)
x∈Ω exp(θT h(x))

(cid:80)

= Ex∼πθ [h(x)|x ∈ A] − Ex∼πθ [h(x)] .

15

And for the Hessian,

∇2 J(θ) = ∇

(cid:80)

x∈A h(x) exp(θT h(x))
(cid:80)
x∈A exp(θT h(x))

(cid:80)

− ∇

(cid:80)

=

x∈A h(x)h(x)T exp(θT h(x))
x∈A exp(θT h(x))

(cid:80)

−

x∈Ω h(x) exp(θT h(x))
(cid:80)
x∈Ω exp(θT h(x))
(cid:16)(cid:80)
x∈A h(x) exp(θT h(x))

(cid:17) (cid:16)(cid:80)

x∈A h(x) exp(θT h(x))

(cid:17)T

(cid:80)





−

x∈Ω h(x)h(x)T exp(θT h(x))
x∈Ω exp(θT h(x))

(cid:80)

−

(cid:16)(cid:80)

(cid:0)(cid:80)

x∈A exp(θT h(x))(cid:1)2
(cid:17) (cid:16)(cid:80)

x∈Ω h(x) exp(θT h(x))

x∈Ω h(x) exp(θT h(x))

(cid:0)(cid:80)

x∈Ω exp(θT h(x))(cid:1)2

(cid:105)
(cid:104)
h(x)h(x)T (cid:12)(cid:12)(cid:12)x ∈ A
= Ex∼πθ
(cid:16)
h(x)h(x)T (cid:105)
(cid:104)
−
Ex∼πθ
= Covx∼πθ (h(x)|x ∈ A) − Covx∼πθ (h(x)) .

− Ex∼πθ [h(x)|x ∈ A] Ex∼πθ [h(x)|x ∈ A]T
− Ex∼πθ [h(x)] Ex∼πθ [h(x)]T (cid:17)

(cid:17)T





(cid:3)

Lemma D.2. Suppose that we are looking at a distribution from a data programming label model. That is, our
maximum-entropy distribution can now be written in terms of two variables, the labeling function values λ ∈ {−1, 0, 1}
and the class y ∈ {−1, 1}, as

πθ(λ, y) = 1
Zθ

exp(θT h(λ, y)),

where we assume without loss of generality that for some M, h(λ, y) ∈ RM and (cid:107)h(λ, y)(cid:107)∞ ≤ 1. If we let J : RM (cid:55)→ R
be the maximum expected log-likelihood objective, under another distribution π∗, for the event associated with the
observed labeling function values λ,

J(θ) = E(λ∗,y∗)∼π∗

(cid:104)
log P(λ,y)∼πθ (λ = λ∗)

(cid:105)

,

then its Hessian can be bounded with

∇2J(θ) (cid:22) MIE(λ∗,y∗)∼π∗

(cid:104)
Var(λ,y)∼πθ (y|λ = λ∗)

(cid:105)

− I(θ),

where I(θ) is the Fisher information.

Proof. From the result of Lemma D.1, we have that

∇2J(θ) = E(λ∗,y∗)∼π∗

(cid:104)
Cov(λ,y)∼πθ (h(λ, y)|λ = λ∗)

(cid:105)

− Cov(λ,y)∼πθ (h(λ, y)) .

(17)

We start byu deﬁning h0(λ) and h1(λ) such that
1 − y
2

h(λ, y) = h(λ, 1)

1 + y
2

+ h(λ, −1)

= h(λ, 1) + h(λ, −1)
2

+ y

h(λ, 1) − h(λ, −1)
2

= h0(λ) + yh1(λ).

This allows us to reduce (17) to

∇2J(θ) = E(λ∗,y∗)∼π∗

(cid:105)
(cid:104)
h1(λ∗)h1(λ∗)T Var(λ,y)∼πθ (y|λ = λ∗)

− Cov(λ,y)∼πθ (h(λ, y)) .

On the other hand, the Fisher information of this model at θ is

exp(θT h(x))
z∈Ω exp(θT h(z))

(cid:33)(cid:33)2


I(θ) = E

= E

= E

= E



(cid:80)

∇θ log


∇θ log

(cid:104)(cid:0)∇θ log πθ(x)(cid:1)2(cid:105)

(cid:32)
(cid:32)


(cid:32)

h(x) −
(h(x) − E [h(z)])2(cid:105)
(cid:104)



(cid:80)



(cid:16)

(cid:17)
exp(θT h(x))

= E
= Cov (h(x)) .

z∈Ω h(z) exp(θT h(z))
(cid:80)
z∈Ω exp(θT h(z))

(cid:33)2


− ∇θ log


(cid:88)



z∈Ω


exp(θT h(z))





2




16

Therefore, we can write the second derivative of J as

∇2J(θ) = E(λ∗,y∗)∼π∗

(cid:105)
(cid:104)
h1(λ∗)h1(λ∗)T Var(λ,y)∼πθ (y|λ = λ∗)

− I(θ).

If we apply the fact that

then we can reduce this to

This is the desired result.

h1(λ∗)h1(λ∗)T (cid:22) I (cid:107)h1(λ∗)(cid:107)2 (cid:22) MI (cid:107)h1(λ∗)(cid:107)2

∞ (cid:22) MI,

∇2J(θ) (cid:22) MIE(λ∗,y∗)∼π∗

(cid:104)
Var(λ,y)∼πθ (y|λ = λ∗)

(cid:105)

− I(θ).

(cid:3)

Lemma D.3. Suppose that we are looking at a data programming distribution, as described in the text of Lemma D.2.
Suppose further that we are concerned with some feasible set of parameters Θ ⊂ RM, such that the any model with
parameters in this space satisﬁes the following two conditions.

First, for any θ ∈ Θ, learning the parameter θ from (full) samples from πθ is possible, at least in some sense. More
speciﬁcally, there exists an unbiased estimator ˆθ that is a function of some number D samples from πθ (and is unbiased
for all θ ∈ Θ) such that, for all θ ∈ Θ and for some c > 0,

Second, for any θ, θ∗ ∈ Θ,

(cid:17)

(cid:16)

ˆθ

(cid:22)

Cov

I
2cD

.

E(λ∗,y∗)∼π∗

(cid:104)

(cid:105)
Var(λ,y)∼πθ (y|λ = λ∗)

≤

c
M

.

That is, we’ll always be reasonably certain in our guess for the value of y, even if we are totally wrong about the true
parameter θ∗.

Under these conditions, the function J is strongly concave on Θ with parameter of strong convexity c.

Proof. From the Cramér-Rao bound, we know in general that the variance of any unbiased estimator is bounded by the
reciprocal of the Fisher information

(cid:16)

(cid:17)

ˆθ

Cov

(cid:23) (I(θ))−1 .

Since for the estimator described in the lemma statement, we have D independent samples from the distribution, it
follows that the Fisher information of this experiment is D times the Fisher information of a single sample. Combining
this with the bound in the lemma statement on the covariance, we get

It follows that

(cid:23) Cov

(cid:17)

(cid:16)
ˆθ

(cid:23) (DI(θ))−1 .

I
2cD

I(θ) (cid:23) 2cI.

On the other hand, also from the lemma statement, we can conclude that

MIE(λ∗,y∗)∼π∗

(cid:104)
Var(λ,y)∼πθ (y|λ = λ∗)

(cid:105)

(cid:22) cI.

Therefore, for all θ ∈ Θ,

(cid:104)
Var(λ,y)∼πθ (y|λ = λ∗)
This implies that J is strongly concave over Θ, with constant c, as desired.

∇2J(θ) (cid:22) MIE(λ∗,y∗)∼π∗

(cid:105)

− I(θ) (cid:22) −cI.

(cid:3)

Lemma D.4. Suppose that we are looking at a data programming maximum likelihood estimation problem, as described
in the text of Lemma D.2. Suppose further that the objective function J is strongly concave with parameter c > 0.

If we run stochastic gradient descent on objective J, using unbiased samples from a true distribution πθ∗, where

θ∗ ∈ Θ, then if we use step size

η = c(cid:15)2
4

17

and run (using a fresh sample at each iteration) for T steps, where

T = 2

c2(cid:15)2 log

(cid:33)

(cid:32)

2 (cid:107)θ0 − θ∗(cid:107)2
(cid:15)

then we can bound the expected parameter estimation error with

(cid:20)(cid:13)(cid:13)(cid:13)ˆθ − θ∗(cid:13)(cid:13)(cid:13)

2(cid:21)

E

≤ (cid:15)2M.

Proof. First, we note that, in the proof to follow, we can ignore the projection onto the feasible set Θ, since this
projection always takes us closer to the optimum θ∗.

If we track the expected distance to the optimum θ∗, then at the next timestep,

(cid:107)θt+1 − θ∗(cid:107)2 = (cid:107)θt − θ∗(cid:107)2 + 2γ(θt − θ∗)∇ ˜J(θt) + γ2 (cid:13)(cid:13)(cid:13)∇ ˜Jt(θt)

2

(cid:13)(cid:13)(cid:13)

.

Since we can write our stochastic samples in the form

for some samples λt, yt, ¯λt, and ¯yt, we can conclude that

∇ ˜Jt(θt) = h(λt, yt) − h(¯λt, ¯yt),

(cid:13)(cid:13)(cid:13)∇ ˜Jt(θt)

(cid:13)(cid:13)(cid:13)

2 ≤ M

(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)∇ ˜Jt(θt)

2
∞

≤ 4M.

Therefore, taking the expected value conditioned on the ﬁltration,

(cid:104)

(cid:107)θt+1 − θ∗(cid:107)2(cid:12)(cid:12)(cid:12)Ft

E

(cid:105) = (cid:107)θt − θ∗(cid:107)2 + 2γ(θt − θ∗)∇J(θt) + 4γ2M.

Since J is strongly concave,

and so,

(θt − θ∗)∇J(θt) ≤ −c (cid:107)θt − θ∗(cid:107)2 ;

(cid:104)
(cid:107)θt+1 − θ∗(cid:107)2(cid:12)(cid:12)(cid:12)Ft

(cid:105)

E

≤ (1 − 2γc) (cid:107)θt − θ∗(cid:107)2 + 4γ2M.

If we take the full expectation and subtract the ﬁxed point from both sides,

(cid:107)θt+1 − θ∗(cid:107)2(cid:105)
(cid:104)

−

E

2γM
c

≤ (1 − 2γc)E

(cid:107)θt − θ∗(cid:107)2(cid:105) + 4γ2M −
(cid:104)

2γM
c

= (1 − 2γc)

(cid:32)
E

(cid:107)θt − θ∗(cid:107)2(cid:105)
(cid:104)

−

(cid:33)

.

2γM
c

Therefore,

and so

In order to ensure that

it therefore suﬃces to pick

and

(cid:107)θt − θ∗(cid:107)2(cid:105)
(cid:104)

−

E

2γM
c

≤ (1 − 2γc)t

(cid:32)

(cid:107)θ0 − θ∗(cid:107)2 −

(cid:33)

,

2γM
c

(cid:104)

(cid:107)θt − θ∗(cid:107)2(cid:105)

E

≤ exp(−2γct) (cid:107)θ0 − θ∗(cid:107)2 + 2γM
c

.

(cid:107)θt − θ∗(cid:107)2(cid:105)
(cid:104)

E

≤ (cid:15)2,

γ = c(cid:15)2
4M

t = 2M

c2(cid:15)2 log

(cid:32)

2 (cid:107)θ0 − θ∗(cid:107)2
(cid:15)

(cid:33)

.

Substituting (cid:15)2 → (cid:15)2M produces the desired result.

(cid:3)

18

Lemma D.5. Assume in our model that, without loss of generality, (cid:107) f (x)(cid:107) ≤ 1 for all x, and that in our true model π∗,
the class y is independent of the features f (x) given the labels λ(x).

Suppose that we now want to solve the expected loss minimization problem wherein we minimize the objective

l(w) = E(x,y)∼π∗

(cid:104)
log(1 + exp(−wT f (x)y))

(cid:105) + ρ (cid:107)w(cid:107)2 .

We actually accomplish this by minimizing our noise-aware loss function, given our chosen parameter ˆθ,

lˆθ(w) = E( ¯x,¯y)∼π∗

(cid:104)
E(Λ,Y)∼πˆθ

(cid:104)

log(1 + exp(−wT f ( ¯x)Y))

(cid:12)(cid:12)(cid:12)Λ = λ( ¯x)

(cid:105)(cid:105) + ρ (cid:107)w(cid:107)2 .

In fact we can’t even minimize this; rather, we will be minimizing the empirical noise-aware loss function, which is only
this in expectation. Suppose that doing so produces an estimate ˆw which satisﬁes, for some χ > 0,

(cid:20)

E

lˆθ( ˆw) − min
w

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

lˆθ(w)

(cid:21)

ˆθ

≤ χ.

(Here, the expectation is taken with respect to only the random variable ˆw.) Then, we can bound the expected risk with

(cid:20)

E

l( ˆw) − min

w

(cid:21)

l(w)

≤ χ + c(cid:15)
2ρ

.

Proof. (To simplify the symbols in this proof, we freely use θ when we mean ˆθ.)

The loss function we want to minimize is, in expectation,

l(w) = E(x,y)∼π∗

(cid:104)
log(1 + exp(−wT f (x)y))

(cid:105) + ρ (cid:107)w(cid:107)2 .

By the law of total expectation,

l(w) = E( ¯x,¯y)∼π∗

(cid:104)
E(x,y)∼π∗

(cid:104)
log(1 + exp(−wT f ( ¯x)y))

(cid:12)(cid:12)(cid:12)x = ¯x

(cid:105)(cid:105) + ρ (cid:107)w(cid:107)2 ,

and by our conditional independence assumption,

l(w) = E( ¯x,¯y)∼π∗

(cid:104)

E(x,y)∼π∗

(cid:104)
log(1 + exp(−wT f ( ¯x)y))

(cid:12)(cid:12)(cid:12)λ(x) = λ( ¯x)

(cid:105)(cid:105) + ρ (cid:107)w(cid:107)2 .

Since we know from our assumptions that, for the optimum parameter θ∗,

P(x,y)∼π∗ (λ(x) = Λ, y = Y) = P(λ,y)∼πθ∗ (λ = Λ, y = Y) ,

we can rewrite this as

l(w) = E( ¯x,¯y)∼π∗

(cid:104)
E(Λ,Y)∼πθ∗

(cid:104)
log(1 + exp(−wT f ( ¯x)Y))

(cid:12)(cid:12)(cid:12)Λ = λ( ¯x)

(cid:105)(cid:105) + ρ (cid:107)w(cid:107)2 .

On the other hand, if we are minimizing the model we got from the previous step, we will be actually minimizing

lθ(w) = E( ¯x,¯y)∼π∗

(cid:104)

E(Λ,Y)∼πθ

(cid:104)
log(1 + exp(−wT f ( ¯x)Y))

(cid:12)(cid:12)(cid:12)Λ = λ( ¯x)

(cid:105)(cid:105) + ρ (cid:107)w(cid:107)2 .

We can reduce this further by noticing that

E(Λ,Y)∼πθ

(cid:104)
log(1 + exp(−wT f ( ¯x)Y))

(cid:12)(cid:12)(cid:12)Λ = λ( ¯x)

(cid:105)

(cid:34)

log(1 + exp(−wT f ( ¯x)))

= E(Λ,Y)∼πθ
= log(1 + exp(−wT f ( ¯x))) + log(1 + exp(wT f ( ¯x)))
2

1 + Y
2

+ log(1 + exp(wT f ( ¯x)))

1 − Y
2

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Λ = λ( ¯x)

+ log(1 + exp(−wT f ( ¯x))) − log(1 + exp(wT f ( ¯x)))
2

= log(1 + exp(−wT f ( ¯x))) + log(1 + exp(wT f ( ¯x)))
2

−

wT f ( ¯x)
2

E(Λ,Y)∼πθ [Y|Λ = λ( ¯x)] .

E(Λ,Y)∼πθ [Y|Λ = λ( ¯x)]

19

It follows that the diﬀerence between the loss functions will be

|l(w) − lθ(w)| =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E( ¯x,¯y)∼π∗

(cid:34)

wT f ( ¯x)
2

(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:0)E(Λ,Y)∼πθ [Y|Λ = λ( ¯x)] − E(Λ,Y)∼πθ∗ [Y|Λ = λ( ¯x)](cid:1)

.

Now, we can compute that

∇θE(Λ,Y)∼πθ [Y|Λ = λ] = ∇θ

= ∇θ

exp(θT h(λ, 1)) − exp(θT h(λ, −1))
exp(θT h(λ, 1)) + exp(θT h(λ, −1))
exp(θT h1(λ)) − exp(−θT h1(λ))
exp(θT h1(λ)) + exp(θT h1(λ))

= ∇θ tanh(θT h1(λ))
(cid:16)
= h1(λ)
= h1(λ)Var(Λ,Y)∼πθ (Y|Λ = λ) .

(cid:17)
1 − tanh2(θT h1(λ))

It follows by the mean value theorem that for some ψ, a linear combination of θ and θ∗,

|l(w) − lθ(w)| =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E( ¯x,¯y)∼π∗

(cid:34)

wT f ( ¯x)
2

(θ − θ∗)T h1(λ)Var(Λ,Y)∼πψ (Y|Λ = λ)

(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

.

Since Θ is convex, clearly ψ ∈ Θ. From our assumption on the bound of the variance, we can conclude that

E( ¯x,¯y)∼π∗

(cid:105)
(cid:104)
Var(Λ,Y)∼πψ (Y|Λ = λ)

≤

c
M

.

By the Cauchy-Schwarz inequality,

|l(w) − lθ(w)| ≤

(cid:12)(cid:12)(cid:12)(cid:12)E( ¯x,¯y)∼π∗

1
2

(cid:104)
(cid:107)w(cid:107) (cid:107) f ( ¯x)(cid:107) (cid:107)θ − θ∗(cid:107) (cid:107)h1(λ)(cid:107) Var(Λ,Y)∼πψ (Y|Λ = λ)

(cid:105)(cid:12)(cid:12)(cid:12)(cid:12) .

Since (by assumption) (cid:107) f (x)(cid:107) ≤ 1 and (cid:107)h1(λ)(cid:107) ≤

√

M,

|l(w) − lθ(w)| ≤

(cid:107)w(cid:107) (cid:107)θ − θ∗(cid:107)

2

√

√

M

M

(cid:12)(cid:12)(cid:12)(cid:12)E( ¯x,¯y)∼π∗

(cid:104)
Var(Λ,Y)∼πψ (Y|Λ = λ)

(cid:105)(cid:12)(cid:12)(cid:12)(cid:12)

·

c
M

(cid:107)w(cid:107) (cid:107)θ − θ∗(cid:107)

≤
= c (cid:107)w(cid:107) (cid:107)θ − θ∗(cid:107)
√
2

M

2

.

Now, for any w that could conceivably be a solution, it must be the case that

(cid:107)w(cid:107) ≤

1
2ρ

,

since otherwise the regularization term would be too large Therefore, for any possible solution w,

|l(w) − lθ(w)| ≤

c (cid:107)θ − θ∗(cid:107)
√
M
4ρ

.

Now, we apply the assumption that we are able to solve the empirical problem, producing an estimate ˆw that satisﬁes

where w∗

θ is the true solution to

E (cid:2)lθ( ˆw) − lθ(w∗

θ)(cid:3) ≤ χ,

w∗
θ

= arg min
w

lθ(w).

20

Therefore,

E (cid:2)l( ˆw) − l(w∗)(cid:3) = E (cid:2)lθ( ˆw) − lθ(w∗

θ) + lθ(w∗

θ) − lθ( ˆw) + l( ˆw) − l(w∗)(cid:3)

≤ χ + E (cid:2)lθ(w∗) − lθ( ˆw) + l( ˆw) − l(w∗)(cid:3)
≤ χ + E (cid:2)|lθ(w∗) − l(w∗)| + |lθ( ˆw) − l( ˆw)|(cid:3)


E (cid:2)(cid:107)θ − θ∗(cid:107)(cid:3)

c (cid:107)θ − θ∗(cid:107)
√
M
2ρ

≤ χ + E

= χ +

c
√




2ρ

M

≤ χ +

c
√

M

2ρ

(cid:113)

E

(cid:107)θ − θ∗(cid:107)2(cid:105)
(cid:104)
.

We can now bound this using the result of Lemma D.4, which results in
√

E (cid:2)l( ˆw) − l(w∗)(cid:3) ≤ χ +

c
√

M

M(cid:15)2

2ρ
= χ + c(cid:15)
2ρ

.

This is the desired result.

(cid:3)

E Proofs of Results for the Independent Model

To restate, in the independent model, the variables are, as before, Λ ∈ {−1, 0, 1}m and Y ∈ {−1, 1}. The suﬃcient
statistics are ΛiY and Λ2

i . That is, for expanded parameter θ = (ψ, φ),
πθ(Λ, Y) = 1
Z

exp(ψT ΛY + φT Λ2).

This can be combined with the simple assumption that P (Y) = 1
prove the following simple result about the moments of the suﬃcient statistics.

2 to complete a whole distribution. Using this, we can

Lemma E.1. The expected values and covariances of the suﬃcient statistics are, for all i (cid:44) j,

i γ2
i

E

(cid:104)Λ2

E [ΛiY] = βiγi
(cid:105) = βi
i
Var (ΛiY) = βi − β2
(cid:17) = βi − β2
(cid:17) = 0
(cid:17) = 0
(cid:17) = 0.

(cid:16)Λ2
Var
(cid:16)ΛiY, Λ jY
(cid:16)Λ2
i , Λ2
(cid:16)ΛiY, Λ2

j

i

i

j

Cov

Cov

Cov

We also prove the following basic lemma that relates ψi to γi.

Lemma E.2. It holds that

γi = tanh(ψi).

We also make the following claim about feasible models.

Lemma E.3. For any feasible model, it will be the case that, for any other feasible parameter vector ˆψ,

(cid:18)

P

ˆψT ΛY ≤

m
2

γmin(γβ)min

(cid:19)

≤ exp


−

m(γβ)minγ2
9.34 artanh(γmax)

min


 .

21

We can also prove the following simple result about the conditional covariances

Lemma E.4. The covariances of the suﬃcient statistics, conditioned on Λ, are for all i (cid:44) j,

Cov

(cid:16)ΛiY, Λ jY
(cid:16)Λ2
i , Λ2

j

(cid:12)(cid:12)(cid:12)Λ(cid:17) = ΛiΛ j sech2(ψT Λ)
(cid:12)(cid:12)(cid:12)Λ(cid:17) = 0.

Cov

We can combine these two results to bound the expected variance of these conditional statistics.

Lemma E.5. If θ and θ∗ are two feasible models, then for any u,

Eθ∗ [Varθ (Y|Λ)] ≤ 3 exp


−

mβ2

minγ3
8 artanh(γmax)

min


 .

We can now proceed to restate and prove the main corollary of Theorem A.1 that applies in the independent case.

Corollary B.1. Suppose that we run Algorithm 1 on an independent data programming speciﬁcation that satisﬁes
conditions (13), (14), (15), and (16). Furthermore, assume that the number of labeling functions we use satisﬁes

m ≥

9.34 artanh(γmax)
(γβ)minγ2

min

log

(cid:32)

(cid:33)

.

24m
βmin

Suppose further that, for some parameter (cid:15) > 0, we use step size
η = βmin(cid:15)2
16

and our dataset is of a size that satisﬁes

Then, we can bound the expected parameter error with

|S | = 32
β2
min(cid:15)2

(cid:32)

log

2 (cid:107)θ0 − θ∗(cid:107)2
(cid:15)

(cid:33)

.

and the expected risk with

(cid:20)(cid:13)(cid:13)(cid:13)ˆθ − θ∗(cid:13)(cid:13)(cid:13)

2(cid:21)

E

≤ (cid:15)2M

(cid:20)

E

l( ˆw) − min

w

(cid:21)

l(w)

≤ χ + βmin(cid:15)
8ρ

.

Proof. In order to apply Theorem A.1, we have to verify all its conditions hold in the independent case.

First, we notice that (11) is used only to bound the covariance of the suﬃcient statistics. From Lemma E.1, we

know that these can be bounded by βi − β2

i γ2

i ≥ βmin

2 . It follows that we can choose

c = βmin
4

,

and we can consider (11) satisﬁed, for the purposes of applying the theorem.
Second, to verify (12), we can use Lemma E.5. For this to work, we need

3 exp


−

m(γβ)minγ2
9.34 artanh(γmax)

min


 ≤

c
M

= βmin
8m

.

This happens whenever the number of labeling functions satisﬁes

m ≥

9.34 artanh(γmax)
(γβ)minγ2

min

log

(cid:32)

(cid:33)

.

24m
βmin

The remaining assumptions, (13), (14), (15), and (16), are satisﬁed directly by the assumptions of this corollary. So,
(cid:3)

we can apply Theorem A.1, which produces the desired result.

22

F Proofs of Independent Model Lemmas

Lemma E.1. The expected values and covariances of the suﬃcient statistics are, for all i (cid:44) j,

i γ2
i

E

(cid:104)Λ2

E [ΛiY] = βiγi
(cid:105) = βi
i
Var (ΛiY) = βi − β2
(cid:17) = βi − β2
(cid:17) = 0
(cid:17) = 0
(cid:17) = 0.

(cid:16)Λ2
Var
(cid:16)ΛiY, Λ jY
(cid:16)Λ2
i , Λ2
(cid:16)ΛiY, Λ2

i

j

i

j

Cov

Cov

Cov

Proof. We prove each of the statements in turn. For the ﬁrst statement,

For the second statement,

E [ΛiY] = P (Λi = Y) − P (Λi = −Y)

1 + γi
2

= βi
= βiγi.

− βi

1 − γi
2

(cid:104)Λ2

i

E

(cid:105) = P (Λ = Y) + P (Λ = −Y)

1 + γi
2

= βi
= βi.

+ βi

1 − γi
2

For the remaining statements, we derive the second moments; converting these to an expression of the covariance is
trivial. For the third statement,
(cid:104)Λ2

(cid:104)Λ2

(ΛiY)2(cid:105) = E
(cid:104)

E

i Y 2(cid:105) = E

(cid:105) = βi.

i

For the fourth statement,

(cid:104)
(Λ2

i )2(cid:105) = E

(cid:104)Λ4

i

(cid:105) = E

(cid:104)Λ2

i

(cid:105) = βi.

E

For subsequent statements, we ﬁrst derive that

and

Now, for the ﬁfth statement,

E [ΛiY|Y] = βi

1 + γi
2

− βi

1 − γi
2

= βiγi

(cid:104)Λ2

i

(cid:12)(cid:12)(cid:12)Y

(cid:105) = βi

E

1 + γi
2

+ βi

1 − γi
2

= βi.

(cid:104)
(ΛiY)(Λ jY)

(cid:105) = E

(cid:104)
E [ΛiY|Y] E

E

(cid:104)Λ jY

(cid:12)(cid:12)(cid:12)Y

(cid:105)(cid:105) = βiγiβ jγ j.

For the sixth statement,

Finally, for the seventh statement,

(cid:104)

E

(Λ2

i )(Λ2
j )

(cid:105) = E

(cid:104)
E

(cid:105)

(cid:104)Λ2

i

(cid:12)(cid:12)(cid:12)Y

E

(cid:104)Λ2

i

(cid:12)(cid:12)(cid:12)Y

(cid:105)(cid:105) = βiβ j.

(cid:104)

(cid:105) = E
(ΛiY)(Λ2
j )

E

(cid:104)
E [ΛiY|Y] E

(cid:104)Λ2

i

(cid:12)(cid:12)(cid:12)Y

(cid:105)(cid:105) = βiγiβ j.

This completes the proof.

Lemma E.2. It holds that

γi = tanh(ψi).

23

(cid:3)

Proof. From the deﬁnitions,

and

Therefore,

βi = exp(ψi + φi) + exp(−ψi + φi)
exp(ψi + φi) + exp(−ψi + φi) + 1

βiγi = exp(ψi + φi) − exp(−ψi + φi)
exp(ψi + φi) + exp(−ψi + φi) + 1

.

γi = exp(ψi + φi) − exp(−ψi + φi)
exp(ψi + φi) + exp(−ψi + φi)

= tanh(ψi),

which is the desired result.

(cid:3)

Lemma E.3. For any feasible model, it will be the case that, for any other feasible parameter vector ˆψ,

(cid:18)

P

ˆψT ΛY ≤

m
2

γmin(γβ)min

(cid:19)

≤ exp


−

m(γβ)minγ2
9.34 artanh(γmax)

min


 .

Proof. We start by noticing that

ˆψT ΛY =

m(cid:88)

ˆψiΛiY.

i=1
Since in this model, all the ΛiY are independent of each other, we can bound this sum using a concentration bound.
First, we note that

(cid:12)(cid:12)(cid:12) ˆψiΛiY

(cid:12)(cid:12)(cid:12) ≤ ˆψi.

Second, we note that

and

but

(cid:104)
ˆψiΛiY

(cid:105) = ˆψiβiγi

E

Var

(cid:16)

ˆψiΛiY

(cid:17) = ˆψ2

i

(cid:16)

βi − β2

i γ2
i

(cid:17)

(cid:12)(cid:12)(cid:12) ˆψiΛiY

(cid:12)(cid:12)(cid:12) ≤ ˆψi ≤ artanh(γmax) (cid:44) ˆψmax

because, for feasible models, by deﬁnition

Therefore, applying Bernstein’s inequality gives us, for any t,

γmin ≤ artanh(γmin) ≤ ˆψi ≤ artanh(γmax).

P


m(cid:88)


i=1

ˆψiΛiY −

m(cid:88)

i=1

ˆψiβiγi ≤ −t





≤ exp


−

6 (cid:80)m
i=1

3t2

i γiβiγi + 2 ˆψmaxt
ˆψ2


 .

It follows that, if we let

t = 1
2

m(cid:88)

i=1

ˆψiβiγi,

24

then we get

P


m(cid:88)


i=1

ˆψiΛiY −

m(cid:88)

i=1

ˆψiβiγi ≤ −t





≤ exp

≤ exp





−

(cid:32)
−

(cid:32)

≤ exp

−






−


−

≤ exp

≤ exp


−



−

≤ exp

≤ exp

(cid:16) 1
2

3

(cid:80)m
i=1

ˆψiβiγi
(cid:16) 1
2

(cid:17)2

(cid:80)m
i=1

ˆψiβiγi





(cid:17)

(cid:33)

ˆψiβiγi

6 (cid:80)m
i γiβiγi + 2 ˆψmax
ˆψ2
i=1
3 (cid:80)m
i=1
24γmax ˆψmax + 4 ˆψmax
3m(1 − γmax)
28 ˆψmax

(cid:33)





(cid:17)

ˆψiβiγi
(cid:17)

ˆψiβiγi
(cid:16)(cid:80)m
i=1

ˆψiβi

(cid:16)(cid:80)m
i=1

(cid:17)2

ˆψiβiγi

(cid:16)(cid:80)m
3
i=1
i βi + 4 ˆψmax
ˆψ2
(cid:16)(cid:80)m
(cid:17) (cid:16)(cid:80)m
i=1
i=1
ˆψiβi + 4 ˆψmax
(cid:17)

ˆψiβiγi


ˆψiβi

24 (cid:80)m
i=1

3γmin

24 ˆψmax

3γmin

(cid:80)m
i=1
(cid:16)(cid:80)m
i=1
28 ˆψmax
min(γβ)min
9.34 ˆψmax

mγ2


 .

This is the desired expression.

Lemma E.4. The covariances of the suﬃcient statistics, conditioned on Λ, are for all i (cid:44) j,

Cov

(cid:16)ΛiY, Λ jY
(cid:16)Λ2
i , Λ2

j

(cid:12)(cid:12)(cid:12)Λ(cid:17) = ΛiΛ j sech2(ψT Λ)
(cid:12)(cid:12)(cid:12)Λ(cid:17) = 0.

Cov

Proof. The second result is obvious, so it suﬃces to prove only the ﬁrst result. Clearly,
1 − E [Y|Λ]2(cid:17)
(cid:16)

(cid:12)(cid:12)(cid:12)Λ(cid:17) = ΛiΛ jVar (Y|Λ) = ΛiΛ j

(cid:16)ΛiY, Λ jY

Cov

.

Plugging into the distribution formula lets us conclude that

E [Y|Λ] = exp(ψT Λ + φT Λ2) − exp(−ψT Λ + φT Λ2)
exp(ψT Λ + φT Λ2) + exp(−ψT Λ + φT Λ2)

= tanh2(ψT Λ),

and so

Cov

(cid:16)ΛiY, Λ jY

(cid:12)(cid:12)(cid:12)Λ(cid:17) = ΛiΛ j

(cid:16)

1 − tanh2(ψT Λ)

(cid:17) = ΛiΛ j sech2(ψT Λ),

which is the desired result.

Lemma E.5. If θ and θ∗ are two feasible models, then for any u,

Eθ∗ [Varθ (Y|Λ)] ≤ 3 exp


−

mβ2

minγ3
8 artanh(γmax)

min


 .

Proof. First, we note that, by the result of Lemma E.4,

Therefore,

Varθ (Y|Λ) = sech2(ψT Λ).

Eθ∗ [Varθ (Y|Λ)] = Eθ∗

(cid:104)
sech2(ψT Λ)

(cid:105)

.

25





(cid:17)

(cid:3)

(cid:3)

Applying Lemma E.3, we can bound this with

(cid:104)
Varθ

(cid:16)

Eθ∗

uT ΛY

(cid:12)(cid:12)(cid:12)Λ(cid:17)(cid:105)

≤

≤

(cid:18) m
2

(cid:18)
−


sech2

2 exp

−

≤ 3 exp

(γβ)minγ2

min

(cid:19)

+ exp

+ exp

(cid:19)

min

(γβ)minγ2

m
2
m(γβ)minγ2
9.34 artanh(γmax)

min


 .

min


m(γβ)minγ2
−
9.34 artanh(γmax)

m(γβ)minγ2
−
9.34 artanh(γmax)










min

This is the desired expression.

(cid:3)

G Additional Experimental Details

G.1 Relation Extraction Experiments

G.1.1 Systems

The original distantly-supervised experiments which we compare against as baselines–which we refer to as using
the if-then-return (ITR) approach of distant or programmatic supervision–were implemented using DeepDive, an
open-source system for building extraction systems.5 For our primary experiments, we adapted these programs to the
framework and approach described in this paper, directly utilizing distant supervision rules as labeling functions.

In the disease tagging user experiments, we used an early version of our new lightweight extraction framework based
around data programming, formerly called DDLite [12], now Snorkel.6 Snorkel is based around a Jupyter-notebook
based interface, allowing users to iteratively develop labeling functions in Python for basic extraction tasks involving
simple models. Details of the basic discriminative models used can be found in the Snorkel repository; in particular,
Snorkel uses a simple logistic regression model with generic features deﬁned in part over dependency paths7, and a
basic LSTM model implemented using the Theano library.8 Snorkel is currently under continued development, and all
versions are open-source.

G.1.2 Applications

We consider three primary applications which involve the extraction of binary relation mentions of some speciﬁc type
from unstructured text input data. At a high level, all three system pipelines consist of an initial candidate extraction
phase which leverages some upstream model or suite of models to extract mentions of involved entities, and then
considers each pair of such mentions that occurs within the same local neighborhood in a document as a candidate
relation mention to be potentially extracted. In each case, the discriminative model that we are aiming to train–and that
we evaluate in this paper–is a binary classiﬁer over these candidate relation mentions, which will decide which ones to
output as ﬁnal true extractions. In all tasks, we preprocessed raw input text with Stanford CoreNLP9, and then either
used CoreNLP’s NER module or our own entity-extraction models to extract entity mentions. Further details of the
basic information extraction pipeline utilized can be seen in the tutorials of the systems used, and in the referenced
papers below.

In the 2014 TAC-KBP Slot Filling task, which we also refer to as the News application, we train a set of extraction
models for a variety of relation types from news articles [30]. In reported results in this paper, we average over scores
from each relation type. We utilized CoreNLP’s NER module for candidate extraction, and utilized CoreNLP outputs
in developing the distant supervision rules / labeling functions for these tasks. We also considered a slightly simpler
discriminative model than the one submitted in the 2014 competition, as reported in [2]: namely, we did not include any
joint factors in our model in this paper.

In the Genomics application, our goal with our collaborators at Stanford Medicine was to extract mentions of genes
that if mutated may cause certain phenotypes (symptoms) linked to Mendelian diseases, for use in a clinical diagnostic

5http://deepdive.stanford.edu
6http://snorkel.stanford.edu
7https://github.com/HazyResearch/treedlib
8http://deeplearning.net/software/theano/
9stanfordnlp.github.io/CoreNLP/

26

(a) m = 20

(b) m = 100

(c) Adding dependencies.

Figure 3: Comparisons of data programming to two oracle methods on synthetic data.

setting. The code for this project is online, although it remains partially under development and thus some material
from our collaborators is private.10

In the Pharmacogenomics application, our goal was to extract interactions between genes for use in downstream

pharmacogenomics research analyses; full results and system details are reported in [21].

In the Disease Tagging application, which we had our collaborators work on during a set of short hackathons as a
user study, the goal was to tag mentions of human diseases in PubMed abstracts. We report results of this hackathon
in [12], as well as in our Snorkel tutorial online.

G.1.3 Labeling Functions

In general, we saw two broad types of labeling functions in both prior applications (when they were referred to as
“distant supervision rules”) and in our most recent user studies. The ﬁrst type of labeling function leverages some weak
supervision signal, such as an external knowledgebase (as in traditional distant supervision), very similar to the example
illustrated in Fig. 1(a). All of the applications studied in this paper used some such labeling function or set of labeling
functions.

The second type of labeling function uses simple heuristic patterns as positive or negative signals. For our text
extraction examples, these heuristic patterns primarily consisted of regular expressions, also similar to the example
pseudocode in Fig. 1(a). Further speciﬁc details of both types of labeling functions, as well as others used, can be seen
in the linked code repositories and referenced papers.

G.2 Synthetic Experiments

In Fig. 3(a-b), we ran synthetic experiments with labeling functions having constant coverage β = 0.1, and accuracy
drawn from α ∼ Uniform(µα − 0.25, µα + 0.25) where µα = 0.75 in the above plots. In both cases we used 1000
normally-drawn features having mean correlation with the true label class of 0.5.

In this case we compare data programming (DP-Pipelined) against two baselines. First, we compare against an
if-then-return setup where the ordering is optimal (ITR-Oracle). Second, we compare against simple majority vote
(MV).

In Fig. 3(c), we show an experiment where we add dependent labeling functions to a set of mind = 50 independent
labeling functions, and either provided this dependency structure (LDM-Aware) or did not (Independent). In this
case, the independent labeling functions had the same conﬁgurations as in (a-b), and the dependent labeling functions
corresponded to “ﬁxes” or “reinforces”-type dependent labeling functions.

10https://github.com/HazyResearch/dd-genomics

27

�������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������