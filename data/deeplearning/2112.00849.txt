1
2
0
2
c
e
D
0
2

]

V
C
.
s
c
[

2
v
9
4
8
0
0
.
2
1
1
2
:
v
i
X
r
a

Interpretable Deep Learning-Based Forensic Iris Segmentation and Recognition

Andrey Kuehlkamp

Aidan Boyd

Adam Czajka

Kevin Bowyer

Patrick Flynn

University of Notre Dame, Notre Dame, IN, USA
{akuehlka, aboyd3, aczajka, kwb, flynn}@nd.edu

Dennis Chute

Eric Benjamin

Duchess County Medical Examiner’s Ofﬁce, Poughkeepsie, NY, USA
{dchute, ebenjamin}duchessny.gov

Abstract

Iris recognition of living individuals is a mature biomet-
ric modality that has been adopted globally from govern-
mental ID programs, border crossing, voter registration and
de-duplication, to unlocking mobile phones. On the other
hand, the possibility of recognizing deceased subjects with
their iris patterns has emerged recently. In this paper, we
present an end-to-end deep learning-based method for post-
mortem iris segmentation and recognition with a special vi-
sualization technique intended to support forensic human
examiners in their efforts. The proposed postmortem iris
segmentation approach outperforms the state of the art and
– in addition to iris annulus, as in case of classical iris
segmentation methods – detects abnormal regions caused
by eye decomposition processes, such as furrows or irregu-
lar specular highlights present on the drying and wrinkling
cornea. The method was trained and validated with data ac-
quired from 171 cadavers, kept in mortuary conditions, and
tested on subject-disjoint data acquired from 259 deceased
subjects. To our knowledge, this is the largest corpus of
data used in postmortem iris recognition research to date.
The source code of the proposed method are offered with the
paper. The test data will be available through the National
Archive of Criminal Justice Data (NACJD) archives.

1. Introduction

Iris recognition has been a solid biometric identiﬁcation
means for almost three decades [16], observing successful
governmental and consumer-level deployments. These in-
clude national ID programs such as AADHAAR in India
[21] (currently the largest biometric identiﬁcation program
worldwide), voter de-duplication in Ghana, Somaliland and
Tanzania [4], border control [17], biometric ATMs or un-
locking mobile devices [1]. With the FBI’s Next Generation

Identiﬁcation system [19] gaining momentum, and with re-
cent quest for contactless biometric recognition approaches
due to the COVID-19 pandemic, iris recognition is more
often brought to the table as a strong candidate for reliable,
fast and hygienic human recognition method.

One of the strengths of iris as a biometric identiﬁer is its
high temporal stability compared to other biometric modes
[22]. Although the assessment of how biological changes
impact the systems’ decisions (not only similarity scores)
remains still one of the research challenges awaiting an au-
thoritative answer [8, 23, 32, 14, 39], observations from op-
erational deployments suggest that iris recognition may be
a good candidate for a “cradle-to-grave” biometrics across
all human identiﬁcation approaches. However, one of the
recent discoveries made by several research groups was
that iris recognition is possible to be done also after death
[7, 40]. This is gruesome and fascinating at the same time.
Gruesome, since the possibility of using a deceased per-
son’s eyeball for authentication calls for new presentation
attack detection mechanisms [41]. Fascinating, as it brings
another, potentially very fast and reliable element of the
forensic toolkit. This paper proposes the ﬁrst, known to us,
fully deep learning-based and open-sourced methodology
of processing and matching iris images captured from ca-
davers in near-infrared light (following the ISO-compliant
acquisition protocol [26]) with the primary goal of provid-
ing interpretable visualizations, crucial in deploying iris as
an element of forensic examination.

The method proposed in this paper was designed on the
largest, known to us, dataset of iris images captured from
430 deceased subjects. Data from 171 subjects was used for
training and validation, and sequestered data from another
259 cadavers was used for testing. Our approach (Fig. 1)
consists of three elements, which are at the same time the
main contributions of this work:

(a) multi-class segmentation detecting iris annulus, con-

1

 
 
 
 
 
 
Iris annulus / specular highlights
/ postmortem wrinkles

Mask R-CNN
semantic
segmentation

Cropped iris

TLPIM
network

CAM

Feature
extraction

Human-interpretable
visualization

Iris image features
(network embeddings)

Figure 1. Overview of the proposed approach: postmortem iris images are ﬁrst processed by ﬁne-tuned Mask R-CNN, to produce a cropped
image of the iris and individual masks for the iris and decomposition artifacts. The cropped image is then fed into our TLPIM network,
which produces two outputs: a representation of the input image for distance matching, and a Class Activation Map visualization of the
network activations. The latter, combined with the segmentation masks, produces a human-interpretable visualization of the matching
outcome.

stellations of specular highlights and the cornea/tissue
wrinkles, which appear due to the postmortem decom-
position processes;

(b) ResNet-50-based iris feature extractor, ﬁne-tuned with
the goal of providing appropriate network embeddings
for postmortem iris images;

(c) a Class Activation Map-based visualization explaining
the decisions to human forensic examiners, who – at
this point – are still the only entities to make authori-
tative judgments about the matching of biometric sam-
ples when acting in the court of law.

Additionally, as a fourth contribution, we offer source
codes of the method available with this paper. The test
dataset (images from 259 cadavers) will be submitted to
the National Archive of Criminal Justice Data (NACJD)
archives [2] and can be requested for research purposes di-
rectly at the NACJD.

2. Related works

2.1. Forensic Iris Recognition

While iris recognition for living individuals has been
around for three decades, the recognition of deceased in-
dividuals using their iris patterns has long been believed
to be impossible. The old assumption of iris recognition
inventor that “soon after death, the pupil dilates consider-
ably, and the cornea becomes cloudy” [15] propagated to
future research papers and left us with conclusions such as

“the iris ... decays only a few minutes after death” [38].
The ﬁrst evidence that perimortem (acquired right before
death) and postmortem iris matching is possible has been
however demonstrated by Sansola [33], who also observed
correct matching results for at least 70% of cases when only
postmortem irises were compared (depending on time af-
ter death). Trokielewicz et al. [40] proposed the ﬁrst pub-
licly available dataset of near infrared and visible-light post-
mortem iris images, and estimated that if bodies are kept in
temperatures below 42◦F (6◦C), the correct matching can
be obtained after 27 hours with state-of-the-art iris match-
ers. Their extended study [42] and research by Sauerwein
et al. [34] suggest that correct matches may be expected
even from three to ﬁve weeks after demise in mortuary
and winter-time outdoor conditions, respectively. However,
Bolme et al. [7] reported much shorter time horizons for
successful post-portem iris matching when bodies are kept
outdoor during summer, due to severe and rapid decomposi-
tion processes happening in the eye. These studies suggest
that (a) postmortem iris recognition is possible, and (b) its
performance highly depends on ambient conditions. Boyd
et al. [11] summarized multiple facets of postmortem iris
recognition in a comprehensive survey.

The methods speciﬁcally designed for cadaver eyes are
sparse and yet emerging. Trokielewicz et al. [44] proposed
the ﬁrst deep learning-based segmentation algorithm based
on SegNet model for cadaver eyes, combined with classi-
cal (Gabor wavelet-based) and newly designed (data-driven
[43]) feature extractors. Our work is different from these
past efforts in that (a) the segmentation model, based on

2

Mask R-CNN, instead of detecting only iris annulus de-
tects also other areas affected by decomposition processes,
such as tissue wrinkles and irregular corneal specular high-
lights, (b) leverages a convolutional neural network (CNN)
to extract postmortem iris features, (c) provides human-
interpretable visualizations of the network’s output poten-
tially helpful for forensic examiners, and (d) was designed
on the largest known to us research dataset of postmortem
iris images acquired from 430 deceased subjects.

2.2. Deep Learning Based Iris Recognition

With iris recognition being a popular means of biomet-
ric veriﬁcation for quite some time, it comes to no surprise
that several attempts to improve it are based on deep learn-
ing methods. Among the more relevant earliest works in
this area can be mentioned DeepIris [28] and DeepIrisNet
[20]. These approaches proposed their own architectures
to solve the iris recognition problem, but were limited by
network design, given the lack of large-scale datasets for
training a CNN from scratch. An alternative approach was
chosen by Minaee et al. [29], which instead proposed the
use of off-the-shelf CNNs to extract iris image features. A
more complete evaluation of known deep network architec-
tures applied to iris recognition is presented in Nguyen et
al. [30]. They show that many CNNs trained for general
object classiﬁcation can be well adapted to work with iris
images. A similar study later showed that ﬁne-tuning the
pre-trained general purpose CNNs can lead to better results
than training from scratch [9].

The use of CNNs to extract embeddings into a high-
dimensional Euclidean space to measure similarity between
images has been around for a few years, but one of the most
relevant works in biometrics is Schroff et al. [35], where the
authors proposed the use of triplets for training a network
to learn face representations. The main idea is to measure
the distance between embeddings from an anchor sample, a
positive sample which belongs to the same class as the an-
chor, and a negative sample coming from a different class.
A loss function was created to minimize the anchor-positive
distance at the same that it maximizes the anchor-negative
distance, effectively separating subjects faces in the embed-
ding space.

Although several works have already used triplet net-
works for iris recognition [47, 5, 45, 46], this work was
speciﬁcally inspired by Boyd et al. [10], who compared
the efﬁcacy of the original Gabor-based iris features with
data-driven features for recognition, using a triplet model
similar to Schroff et al. Their conclusion showed very sim-
ilar performance between hand-crafted and data-driven fea-
tures, also conﬁrming the applicability of a triplet model to
iris recognition.

3. Datasets

3.1. Segmentation dataset

Training a model for semantic segmentation requires an-
notations of the areas to be demarcated. We used a dataset
of images collected from several larger datasets by Trok-
ielewicz et al. [43], used to train SegNet to perform iris seg-
mentation. This dataset contains samples from the follow-
ing public iris segmentation benchmarks: ND-0405, CASIA,
BATH, BioSec, UBIRIS, and Warsaw-Biobase-Postmortem-
Iris v2.0. The authors provide two types of ground truth
annotation along with the images: coarse, where iris an-
nulus and major occlusions are approximated by polygons,
and ﬁne, where smaller occlusion details like eyelashes and
specular highlights were also annotated. This dataset con-
tains a total of 7,193 images.

We want to leverage the ability of Mask R-CNN, the
segmentation model used in this work, to detect and seg-
ment multiple types of objects in a single pass. This allows
to create additional masks that can then be merged into a
more accurate boundary of the usable iris texture, but also
provides richer information to the forensic examiner about
the detected non-usable iris regions. Thus, we manually
annotated 179 images with wrinkles and 252 images with
specular highlights for a subset of images from the Warsaw-
Biobase-Postmortem-Iris v2.0 dataset.

3.2. Recognition datasets

The Warsaw BioBase Postmortem Iris v2.0 (Warsaw
v2.0) [42] is one of the ﬁrst datasets of postmortem iris
images made publicly available. It comprises 1,200 near-
infrared (NIR) postmortem images of 37 deceased subjects
collected in controlled settings. The time between death
and image acquisition (postmortem interval – PMI) ranges
between 5 and 800 hours.

Two other datasets used in this work were collected in
an operational setting in a medical examiner’s ofﬁce. The
ﬁrst (DCMEO 01) contains 621 NIR images from 134 de-
ceased subjects (254 distinct identities) collected in 2019.
The images were acquired in up to 9 sessions, where the
latest session occurred at most 284 hours after the time of
death. The second (DCMEO 02) contains 5,770 NIR im-
ages from 259 subjects collected in the year of 2020. In
this dataset, the longest time elapsed after death was 1,674
hours ( 69 days), captured in 53 sessions.

The DCMEO 02 was kept solely for subject-disjoint
testing purposes. A union of Warsaw v2.0 and DCMEO
01, which we call Combined dataset, was used in training
and validation of the recognition models. These training
datasets were collected by two different teams in different
mortuary premises, and that makes the training set more di-
verse.

3

(a)

(b)

(c)

(d)

Figure 2. The proposed segmentation of postmortem degraded irises: (a) eyeball presenting wrinkles and irregular specular highlights; (b)
ground-truth annotations for iris (blue), wrinkles (green) and highlights (yellow); (c) iris annulus, wrinkles and highlights automatically
predicted with Mask R-CNN; (d) overlapping predicted masks, yielding a usable iris area (blue).

4. Multi-object Postmortem Iris Segmentation

One of the most impactful factors on postmortem iris
recognition accuracy is segmentation [42], and the chal-
lenge of such images lies in the artifacts that are created by
the decomposition process. Dehydration of the eye tissues
causes them to shrink and crease. This introduces detrimen-
tal features to accurate segmentation such as wrinkles and
additional highlights (Fig. 2a). These are not adequately
handled by traditional iris recognition methods, leading to
false positive or false negative errors. Our intention is to
consider such damaged regions as occlusions and eliminate
them from the usable iris area, informing forensic exam-
iners about the potential locations of intact portions of the
texture (Fig 2b-e).

Mask R-CNN [24] is a network designed for object in-
stance segmentation that is able to not only perform object
instance detection, but has also a branch for object mask
prediction, and can be adapted to detect a variety of ob-
jects. Mask R-CNN was originally trained on the COCO
dataset [27] for general object detection on RGB images.
We ﬁrst ﬁne-tuned Mask R-CNN to detect the iris texture in
NIR images, using images and labels from a combination
of live and postmortem iris images, and then ﬁne-tuned the
model to perform instance detection and segmentation us-
ing data with newly annotated wrinkles and highlights (as
described in Sec. 3).
In the ﬁrst step of Mask R-CNN
ﬁne-tuning the parameters used for training were: ResNet-
50 backbone with Feature Pyramid Network (FPN), 3x
schedule backbone, 8 images per batch, base learning rate
0.00025, 5000 as maximum iterations, 128 proposals per
image (ROI_HEADS.BATCH_SIZE_PER_IMAGE) and
one class (iris). These same settings were employed in fur-
ther training Mask R-CNN to detect and segment highlights
and wrinkles, except for the number of classes increased to
3 (iris, highlights and wrinkles). In both cases, Detectron2
[18] pre-conﬁgured data augmentation settings were used.

5. Postmortem Iris Recognition

To create a model capable to perform iris recognition,
we chose an implementation of VGGFace2 [12] on the
Keras framework, that uses the ResNet-50 backbone [3].

This model was ﬁrst ﬁne-tuned to live iris images, and
then trained to generate high dimensional embeddings us-
ing triplet loss. Further details of these procedures will be
described in this section. We call our model Triplet Loss
Postmortem Iris Model (TLPIM).

Anchor 
input

Positive 
input

Negative 
input

l

e
d
o
M
s
i
r
I

m
e
t
r
o
m
t
s
o
P

s
s
o
L

l

t
e
p
i
r
T

l

e
d
o
M
n
o
i
t
c
i
d
e
r
P

Anchor Positive Negative 

Embedding 
Representation

(b)

(c)

0
5
-
t
e
N
s
e
R

CNN Block 12

CNN Block 13

CNN Block 14

CNN Block 15

CNN Block 16

Output

(a)

Figure 3. TLPIM Architecture: (a) using feature extraction layers
from ResNet-50 VGGFace2 ﬁne-tuned for iris images; (b) compo-
sition of a triplet loss model to obtain iris features (network em-
beddings); (c) the ﬁnal model for iris encoding extraction.

5.1. Tuning VGGFace2 to iris images

We used CASIA-Iris-Thousand dataset [31] to ﬁne-tune
VGGFace2 to iris images, by training it to perform classiﬁ-
cation on its 2,000 classes. Images segmented using Mask
R-CNN and cropped to a size of 256×256 pixels around the
detected iris were fed into the network, which was trained
for 8 epochs with a learning rate of 0.01 and stochastic gra-
dient descent as the optimizer. After this tuning, features
were extracted and classiﬁed using an SVM with the RBF
kernel, yielding a classiﬁcation accuracy of 96.28%, sug-
gesting a satisfactory adaptation to NIR iris images.

4

 
 
 
 
 
5.2. Adapting VGGFace2 to a triplet loss based

model

Previous work evaluated the use of ResNet architecture
to extract features for iris recognition, and has determined
the peak recognition accuracy happens with features ex-
tracted at the end of the 11th convolutional block [30]. Fol-
lowing these ﬁndings, we extracted features at this layer,
and appended a 512 × 1 × 1 convolution layer with Relu
activation for dimensionality reduction (Fig. 3a-b). Using
this as a backbone, we build our triplet model network by
adding a dense 128-neuron output layer with sigmoid acti-
vation to ensure the output is contained between 0 and 1.
Figure 3 illustrates this architecture.

As proposed by Schroff et al. [35], the triplet loss is ex-

pressed by

T

Loss =

[kf (xa

i ) − f (xp

i )k2

2 − kf (xa

i ) − f (xn

i )k2 + α]+,

X
i=1

i , xn

i , xp

where T is the number of training triplets, xa

(1)
i is
an image triplet (anchor, positive and negative), f is the
function that extracts embedding representation of images,
and α is an enforced margin. One problem with this loss
function is that for any difference between positive and neg-
ative samples that is smaller than 0, the ﬁnal result is forced
to be 0, eliminating the ability of the network to learn from
negative distances. We thus adopted a modiﬁcation pro-
posed by Arsenault [6] that forces the distance to be posi-
tive (through the use of a sigmoid output layer), and adding
a logarithmic penalty to the error:

Loss =

T

X
i=1

[− ln(−

(f (xa

i ) − f (xp
β

i ))2

+ 1 + ǫ)

− ln(

β − (f (xa

i ))2

i ) − f (xn
β

(2)

+ 1 + ǫ)],

where β is a scaling factor (which is equal to the dimen-
sionality of the embedding space) and ǫ is a small value to
prevent ln(0). This triplet loss function allows the network
to learn even when the difference between samples is nega-
tive and converge faster.

5.3. Training the Triplet based model

The main idea of a triplet loss model is to map the im-
age into a high dimensional feature space, where ideally
the embeddings representing the same identity will be lo-
cated closer to each other than to those belonging to dif-
ferent identities. Using such space allows us to use a dis-
tance metric and assess whether the samples originate from
the same person. We used the Combined dataset for train-
ing and evaluating TLPIM. A random disjoint 80%/20%

5

split was created for training and testing, respectively. Hard
triplet mining [35, 25] was used in the formation of triplets
for training.

We trained TLPIM with an initial

learning rate of
0.00001 and Adam optimizer, in batches of 24 triplets for
a minimum of 20,000 iterations, interrupting the training if
validation loss did not improve for 20 iterations. To perform
recognition, we ﬁrst extract the image embeddings with
TLPIM, and then calculate the Euclidean distance between
their 128-dimension coordinates. Using the distribution of
distances for genuine and impostor pairs, it is possible to
deﬁne a decision threshold to produce match or non-match
decisions.

6. Visual Explanations

Despite its acceptance as a reliable means of identiﬁca-
tion, automated iris recognition methods still offer limited
and non-standard methods of visualization to let human ex-
aminers interpret the model output. This is mainly due to
pending research debate what are the best iris features that
could be named and understood by human examiners, hav-
ing at the same time adequate discriminatory strength. The
goal of this work is to offer visualizations that can provide
human experts hints regarding the image properties that led
to an automatic matching decision. In addition to the en-
hanced individual segmentation masks generated by Mask
R-CNN, we also applied Class Activation Maps (CAM)
[48] for that purpose.

While more modern derivations of the original CAM
may seem to offer attractive results, they are not appropri-
ate for our use case. Grad-CAM [36] attempts to merge
the coarse localization provided by CAM to the ﬁne-grained
feature activations yielded through guided backpropagation
[37]. The main problem with the Grad-CAM approach is
that it is targeted at classiﬁcation problems, and requires a
target label to calculate the gradients for backpropagation.
A more recent work [13] tries to address this by making
adaptations to problems in the embedding space, however
their solution is more appropriate for closed-set settings,
since a target embedding is calculated by averaging mul-
tiple embeddings of the same class. Since our application
cannot rely on multiple enrollments of the same identity, we
decided to use the traditional CAM approach to provide the
activation maps produced by the input.

7. Results

7.1. Postmortem iris segmentation

After training Mask R-CNN to perform iris segmentation
using the segmentation dataset, we evaluate both models on
the Combined dataset. Using Intersection Over Union (IoU)
to measure how well the predicted segmentation masks ﬁt
to the ground truth annotation, we compare our method to

1.00

0.95

0.90

U
o

I

0.85

0.80

0.75

0.70

Coarse Annotations

Segmentation

Mask R-CNN
SegNet

24

48

72

96
PMI

120

168

336

Figure 4. Segmentation accuracy by postmortem interval.

Average IoU (%)

Annotations Mask R-CNN
88.38 ± 6.54
Coarse
86.82 ± 7.07
Fine

SegNet[43]
85.97 ± 7.26
83.31 ± 7.62

Table 1. Segmentation accuracy in comparison to SegNet.

state of the art SegNet-based [43]. Since the annotations
used to train SegNet provide two versions, Coarse and Fine,
we trained and evaluated our model using both of them. Ta-
ble 1 shows our model achieves better accuracy and smaller
variation than SegNet-based using both types of annotation.
Given that the training and testing data are distinct, these
results also indicate generalization capabilities.

An evaluation that takes into consideration the PMI was
also conducted. Fig. 4 illustrates how the decay process
constrains the accuracy of iris segmentation. Although, as
the PMI increases the segmentation accuracy goes down
and variance increases, the proposed Mask R-CNN-based
segmenter consistently outperforms the previous one based
on SegNet. Following these results, we adopted the coarse
segmentation-based model for the remainder of this work.
We ﬁne-tuned our Mask R-CNN model to perform si-
multaneous detection of specular highlights and wrinkles
(Section 4). Finally, we combined the segmentation masks
with the network class activation maps to produce a com-
prehensive visual explanation of the matching process (pre-
sented in detail in Sec. 7.3).

7.2. Recognition

After training on one partition of the Combined dataset,
we evaluated TLPIM on the remaining, subject-disjoint test
partition of the same dataset, as well as on the newly col-
lected DCMEO 02. As a general performance metric, we
calculated the Area Under the ROC (AUROC) for these
datasets, on which TLPIM achieved 0.84 and 0.89 respec-
tively.

We also evaluated the proposed solution as a function of
the PMI range of the reference and probe samples (Fig. 5 (a)

6

and (b)). Here, we could speciﬁcally measure the extent to
which postmortem decay affects recognition. AUROC was
as high as 0.95 and 0.96 on Combined set and DCMEO 02
respectively, if we only consider pairs where PMI of both
samples is under 24 hours. If we keep the same restriction
on the reference samples (<=24h) and allow probe samples
up until longer periods, the result is that accuracy drops pro-
portionally to the increase in the PMI period. On the Com-
bined set, the AUROC is 0.88, 0.80 and 0.78 as we allow
probe samples to be up to 72, 120 and 336 hours, respec-
tively. Similarly, on the larger DCMEO 02, the AUROC
declines to 0.81 for probe samples with the PMI above 672
hours.

A comparison of our approach with the only existing
postmortem end-to-end iris recognition method is presented
in Fig 5(c). The AUROC-wise performance of our method
is slightly inferior to Trokielewicz et al. [43]. The impor-
tant distinction to be made here is that the proposed method
offers interpretability through visualization, apparently at
some small cost of accuracy.

7.3. Visualization

Interpretability and visualization are not frequent com-
ponents in iris recognition systems. However, especially
when machine learning methods are involved, there are sce-
narios where a human examiner would be still required to
inspect and interpret the machine’s results. With this goal,
we created a visualization that allows examiners to under-
stand multiple outputs of our method, as well as validate its
output, in order to ground their expert decision.

Figure 6 shows visualizations for sample matching pairs.
Each output produced by our model is represented as a layer
in these images, which can be switched on and off indepen-
dently, e.g. to make the output less cluttered or focus on a
particular artifact. The background is the contrast-enhanced
input image, on top of which the contours of segmented ob-
jects are drawn – while blue denotes the iris boundaries, yel-
low and green represent highlight and wrinkle occlusions,
respectively. Lastly, an overlay with the CAM heatmap
hints to the examiner the regions with higher importance
in the extraction of the iris image features.

It is important to note that there are no (known to us)
iris segmentation benchmarks that would allow for indepen-
dent evaluation of segmentation accuracy of postmortem-
speciﬁc iris artefacts, so quantitative evaluation of the re-
sulting visualization methodology could not be performed.
By inspecting the matching pair visualization, the exam-
iner can quickly assess whether the similarity score between
the images is appropriate or it was inﬂuenced by mistakes
at one or more stages of iris image processing. For instance,
Fig 6 (b) and 6 (c) are cases where the examiner may decide
the similarity scores should not be trusted. Despite both
images have yielded good segmentation results and some

TLPIM on
Combined (Test)

TLPIM on
DCMEO 02

Comparison on
DCMEO 02

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

PMI<24h = 0.95
PMI<72h = 0.88
PMI<120h = 0.80
PMI<336h = 0.78

1.0

0.8

0.6

0.4

0.2

0.0

PMI<24h = 0.96
PMI<120h = 0.82
PMI<672h = 0.81
PMI<1176h = 0.81
PMI<1656h = 0.81

TLPIM AUC = 0.89
Trokielewicz et al. AUC = 0.95

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

(a)

(b)

(c)

Figure 5. Matching results: (a) TLPIM performance by PMI on the Combined dataset; (b) TLPIM performance by PMI on the DCMEO 02
dataset; (c) performance comparison between our approach and Trokielewicz et al. [43].

minor degradation, the heatmaps reveal that for both pairs,
signiﬁcant areas outside the iris annulus played a major role
in calculating the network embedding. In turn, pairs shown
in (a) and (d) seem to be matched by a network using areas
inside the iris annulus, what may suggest a stronger set of
features being used in matching.

7.4. Interpretation of visual outputs

While recognition accuracy is the most desirable quality
in a biometric matching system, it is not the only goal of
this work. Many recognition methods, especially the deep-
learning based ones, yield very high accuracy rates, but of-
fer essentially nothing that explains its output. Although
the proposed method does not surpass the current state of
the art in postmortem iris recognition, it delivers a substan-
tial contribution to the interpretability of its decisions. By
analyzing the visual output of TLPIM, forensic examiners
can get insights regarding the quality of the algorithm quan-
titative and categorical output. By examining the results de-
picted in Figure 6 we offer examples of how visual output
interpretation can offer valuable insight on the quality of the
decisions.

Figure 6(a) – This impostor pair resulted in low simi-
larity score (0.2558), and consequently a non-match by the
algorithm (denoted in the ﬁgure by the red box). Segmenta-
tion of the iris are adequate as well as the specular highlight
and wrinkled occlusion areas in both images. A signiﬁcant
network activation occurs within the usable iris area, in re-
gions approximately equivalent in both images. The strong
activation located in the upper right area of the ﬁrst image
can be considered a demerit: it is outside the bounds of the
iris annulus, and corresponds to the eyelash region and is
therefore weak in identifying features.

Figure 6(b) – A genuine pair, with good iris segmen-
tation despite the substantial eyelid occlusion, and correct
segmentation of the highlight constellations overlapping

mostly the pupil regions. In the second image of the pair,
there is a questionable identiﬁcation of a wrinkled region at
the bottom, but mostly outside the iris boundary. TLPIM
declared this pair a match, as indicated by the green box
around it. However, the strong similarity score for this pair
(0.9903) can be disputed if one considers the potent activa-
tions in the eyelash regions of both images: they suggest the
score relies more in the evident similarities between these
areas, and not the irises.

Figure 6(c) – This impostor pair was correctly classiﬁed
as a non-match by TLPIM with a similarity score of 0.2563.
Segmentation of the iris and highlights presents no obvious
inaccuracies, and no wrinkled occlusion area was detected.
Little could be challenged in this case, except for the fact
that again the strongest network activations are in regions
outside the iris boundaries.

Figure 6(d) – This is perhaps the most indisputable de-
cision out of these examples, this pair was deemed a match
with the highest possible similarity score (1.0). In both im-
ages, it is possible to note very good segmentation, where
only minor occlusions occur to the iris area. At the same
time, one can observe the highest network activation areas
are inside the iris boundaries, in regions that partially over-
lap between both images. In addition to the strong similar-
ity score, visual analysis corroborates this pair as a highly
probable match.

8. Conclusions

In this work, we have presented an interpretable end-to-
end deep learning-based iris recognition system with a pri-
mary goal of providing interpretations to forensic and med-
ical examiners. Typical scenarios for these professionals
consist in comparing live iris images with those collected
after death, containing eye decay-sourced artifacts. The
existing iris recognition tools have shown to perform sub-

7

0101_L_4_4.bmp

9079_L_2_1.bmp

0137_R_5_3.bmp

0137_R_5_4.bmp

(a) Similarity: 0.2558

(b) Similarity: 0.9903

0111_R_1_4.bmp

0150_L_1_3.bmp

9052_R_2_1.bmp

9052_R_3_1.bmp

(c) Similarity: 0.2563

(d) Similarity: 1.0000

Figure 6. Visual inspection of matching pairs (genuine pairs marked in green and impostor pairs marked in red) is intended to provide an
aid to experts making a ﬁnal judgment on the analyzed pairs. Composite visualization aggregates three layers of information: 1) enhanced
input image, 2) iris and postmortem artifacts boundaries, and 3) network activation heatmap.

optimally in postmortem scenarios, and did not offer visual
explanations speciﬁc to postmortem iris recognition. This
paper makes a ﬁrst step in this direction.

Using the visual explanations produced by the proposed
TLPIM method, human experts can verify the results of iris
matching scores, and in some sense assess the trustworthi-
ness of the matcher. This is especially important when de-
cay artifacts overlap with the iris annulus and may impact
the comparison score. Furthermore, it is possible to easily
identify whether such regions played a relevant part in the
extraction of embeddings by the network, owing to the class
activation mapping embedded into this tool.

Objectively, we developed the ﬁrst (to the best of our
knowledge) fully deep learning-based processing pipeline
completely based on open source frameworks that is able to
perform postmortem iris segmentation and recognition and
provide human-interpretable insights into its decision pro-
cess. We make the source code of the entire method avail-
able along with this paper1. Additionally, the curated test
dataset collected during this work will be submitted to the
NACJD archives, and can be requested to facilitate further
research in postmortem iris identiﬁcation.

9. Acknowledgements

This project was supported by Award No. 2018-DU-
BX-0215, awarded by the National Institute of Justice, Of-
ﬁce of Justice Programs, U.S. Department of Justice. The
opinions, ﬁndings, and conclusions or recommendations ex-
pressed in this presentation are those of the authors and do
not necessarily reﬂect those of the Department of Justice.

References

app

the ﬁrst
via

bank with
scanning,

[1] BBVA,
bile
https://www.bbva.com/en/bbva-first-bank-access-
mobile-app-iris-scanning-thanks-samsung.
Accessed: 2021-08-12.

to
its mo-
to Samsung.

access
thanks

iris

[2] National Archive of Criminal
source

data

for

Justice Data archives
justice.
and

– The
crime
https://www.icpsr.umich.edu/web/pages/NACJD/index.html.
Accessed: 2021-08-3.

on

VGGFace

us-
[3] Oxford
ing
v2+.
https://github.com/rcmalli/keras-vggface.
Accessed: 2021-08-18.

Implementation
Framework

Functional

Keras

1Available at https://github.com/akuehlka/xai4b_tlpim.git

[4] Ghana, Tanzania and Somaliland introduce biometric voter
veriﬁcation. Biometric Technology Today, 2015(10):3–12,

8

2015.

[5] Sohaib Ahmad and Benjamin Fuller. Thirdeye: Triplet based
iris recognition without normalization. In 2019 IEEE 10th
International Conference on Biometrics Theory, Applica-
tions and Systems (BTAS), pages 1–9, 2019.

[6] Marc-Olivier Arsenault.

Lossless Triplet

loss:

[18] Facebook

AI

Research.

Detectron2.

https://github.com/facebookresearch/detectron2.
Accessed: 2021-08-18.

[19] FBI. Next Generation Identiﬁcation (NGI), May 2016.

https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi,
Last accessed on 2021-08-07.

efﬁcient

more
https://coffeeanddata.ca/lossless-triplet-loss,
Last accessed on 2021-08-03.

Siamese,

function

loss

for

A
2018.

[7] David S. Bolme, Ryan A. Tokola, Chris B. Boehnen,
Tiffany B. Saul, Kelly A. Sauerwein, and Dawnie Wolfe
Steadman.
Impact of environmental factors on biometric
matching during human decomposition. In IEEE Int. Conf.
on Biometrics: Theory Applications and Systems (BTAS),
pages 1–8, Niagara Falls, NY, USA, Sept 2016. IEEE.
[8] K. W. Bowyer and E. Ortiz. Critical examination of the IREX

VI results. IET Biometrics, 4(4):192–199, 2015.

[9] Aidan Boyd, Adam Czajka, and Kevin Bowyer. Deep
Learning-Based Feature Extraction in Iris Recognition: Use
Existing Models, Fine-tune or Train From Scratch? In 2019
IEEE 10th International Conference on Biometrics Theory,
Applications and Systems (BTAS), pages 1–9, 2019.

[10] Aidan Boyd, Adam Czajka, and Kevin Bowyer. Are gabor
In 2020 IEEE Inter-
kernels optimal for iris recognition?
national Joint Conference on Biometrics (IJCB), pages 1–9,
2020.

[11] Aidan Boyd, Shivangi Yadav, Thomas Swearingen, An-
drey Kuehlkamp, Mateusz Trokielewicz, Eric Benjamin, Pi-
otr Maciejewicz, Dennis Chute, Arun Ross, Patrick Flynn,
Kevin Bowyer, and Adam Czajka. Post-mortem iris recogni-
tion—a survey and assessment of the state of the art. IEEE
Access, 8:136570–136593, 2020.

[12] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and An-
drew Zisserman. Vggface2: A dataset for recognising faces
across pose and age. In 2018 13th IEEE International Con-
ference on Automatic Face Gesture Recognition (FG 2018),
pages 67–74, 2018.

[13] L. Chen, Jianhui Chen, Hossein Hajimirsadeghi, and Greg
Mori. Adapting Grad-CAM for Embedding Networks. 2020
IEEE Winter Conference on Applications of Computer Vision
(WACV), pages 2783–2792, 2020.

[14] Adam Czajka. Inﬂuence of iris template aging on recognition
reliability. In Mireya Fern´andez-Chimeno, Pedro L. Fernan-
des, Sergio Alvarez, Deborah Stacey, Jordi Sol´e-Casals, Ana
Fred, and Hugo Gamboa, editors, Biomedical Engineering
Systems and Technologies, pages 284–299, Berlin, Heidel-
berg, 2014. Springer Berlin Heidelberg.

[15] John Daugman.

BBC News:

The eyes have it.

[16] John Daugman. High conﬁdence visual recognition of per-
sons by a test of statistical independence. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 15:1148–
1161, 1993.
[17] John Daugman.

Iris recognition at airports and border-
crossings. In Encyclopedia of Biometrics, pages 998–1004.
Springer, New York, 2015.

[20] A. Gangwar and Akanksha Joshi. DeepIrisNet: Deep iris
representation with applications in iris recognition and cross-
sensor iris recognition. 2016 IEEE International Conference
on Image Processing (ICIP), pages 2301–2305, 2016.
[21] Government of India. Unique Identiﬁcation Authority of In-
dia. https://uidai.gov.in/, Last accessed on 2021-
08-07.

[22] Patrick Grother, James Matey, Elham Tabassi, George
Quinn, and Michael Chumakov. IREX VI - Temporal Stabil-
ity of Iris Recognition Accuracy. National Institute of Stan-
dards and Technology, 2013-07-11 2013.
[23] P. Grother, J. R. Matey, and G. W. Quinn.

IREX VI:
mixed-effects longitudinal models for iris ageing: response
to Bowyer and Ortiz. IET Biometrics, 4(4):200–205, 2015.

[24] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
In Proceedings of the IEEE Inter-
shick. Mask R-CNN.
national Conference on Computer Vision, pages 2961–2969,
2017.

[25] Alexander Hermans, Lucas Beyer, and Bastian Leibe.

In
defense of
loss for person re-identiﬁcation,
2017. https://arxiv.org/pdf/1703.07737, Last
accessed on 2021-08-18.

the triplet

[26] ISO/IEC. INTERNATIONAL STANDARD ISO/IEC 29794-
6 Information technology — Biometric sample quality —
Part 6: Iris image data, 2014.

[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
European Conference on Computer Vision, pages 740–755.
Springer, 2014.

[28] N. Liu, Man-Lian Zhang, Haiqing Li, Zhenan Sun, and T.
Tan. DeepIris: Learning pairwise ﬁlter bank for heteroge-
neous iris veriﬁcation. Pattern Recognition Letters, 82:154–
161, 2016.

[29] Shervin Minaee, AmirAli Abdolrashidi, and Yao Wang. An
experimental study of deep convolutional features for iris
recognition. 2016 IEEE Signal Processing in Medicine and
Biology Symposium (SPMB), pages 1–6, 2016.

[30] Kien Nguyen, Clinton Fookes, Arun Ross, and Sridha Srid-
Iris recognition with off-the-shelf cnn features: A
IEEE Access, 6:18848–18855,

haran.
deep learning perspective.
2018.

[32] E. Ortiz, K. W. Bowyer, and P. J. Flynn. A linear regression
analysis of the effects of age related pupil dilation change in
iris biometrics. In 2013 IEEE Sixth International Conference
on Biometrics: Theory, Applications and Systems (BTAS),
pages 1–6, September 2013.

9

http://news.bbc.co.uk/2/hi/science/nature/1477655.stm.
[31] Chinese
Accessed: 2021-08-14.
Database
SIA
http://www.cbsr.ia.ac.cn/china/Iris%20Databases%20CH.asp.
Accessed: 2021-07-21.

Academy
Iris

of
Image

CA-
V4.0.

Sciences.

[47] Zijing Zhao and Ajay Kumar. Towards more accurate iris
recognition using deeply learned spatially corresponding
features. In Proceedings of the IEEE International Confer-
ence on Computer Vision (ICCV), Oct 2017.

[48] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2016.

[33] Alora Sansola. Postmortem iris recognition and its applica-
tion in human identiﬁcation. Master’s thesis, Boston Univer-
sity, Boston, MA, USA, 2015.

[34] Kelly Sauerwein, Tiffany B. Saul, Dawnie Wolfe Steadman,
and Chris B. Boehnen. The effect of decomposition on the
efﬁcacy of biometrics for positive identiﬁcation. Journal of
Forensic Sciences, 62(6):1599–1602, 2017.

[35] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniﬁed embedding for face recognition and clus-
tering. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2015.

[36] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek
Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-
tra. Grad-cam: Visual explanations from deep networks
via gradient-based localization. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), Oct
2017.

[37] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
Brox, and Martin Riedmiller. Striving for simplicity: The
In 2015 International Conference
all convolutional net.
on Learning Representations (ICLR), 2015. arXiv preprint
arXiv:1412.6806.

[38] Adam Szczepa´nski, Krzysztof Misztal, and Khalid Saeed.
Pupil and iris detection algorithm for near-infrared cap-
ture devices.
In Khalid Saeed and V´aclav Sn´aˇsel, editors,
Computer Information Systems and Industrial Management,
pages 141–150, Berlin, Heidelberg, 2014. Springer Berlin
Heidelberg.

[39] M. Trokielewicz. Linear regression analysis of template ag-
ing in iris biometrics. In 3rd International Workshop on Bio-
metrics and Forensics (IWBF 2015), pages 1–6, March 2015.
[40] Mateusz Trokielewicz, Adam Czajka, and Piotr Maciejew-
icz. Post-mortem human iris recognition. In 2016 Interna-
tional Conference on Biometrics (ICB), pages 1–6, 2016.
[41] Mateusz Trokielewicz, Adam Czajka, and Piotr Maciejew-
icz. Presentation attack detection for cadaver iris. In 2018
IEEE 9th International Conference on Biometrics Theory,
Applications and Systems (BTAS), pages 1–10, 2018.
[42] M. Trokielewicz, A. Czajka, and P. Maciejewicz. Iris recog-
nition after death. IEEE Transactions on Information Foren-
sics and Security, 14(6):1501–1514, June 2019.

[43] Mateusz Trokielewicz, Adam Czajka, and Piotr Maciejew-
icz. Post-mortem iris recognition resistant to biological eye
In Proceedings of the IEEE/CVF Winter
decay processes.
Conference on Applications of Computer Vision (WACV),
pages 2307–2315, 2020.

[44] Mateusz Trokielewicz, Adam Czajka, and Piotr Maciejew-
Post-mortem iris recognition with deep-learning-
Image and Vision Computing,

icz.
based image segmentation.
94:103866, 2020.

[45] Xiao Wang, Hui Zhang, Jing Liu, Lihu Xiao, Zhaofeng He,
Liang Liu, and Pengrui Duan. Iris Image Super Resolution
Based on GANs with Adversarial Triplets. In Chinese Con-
ference on Biometric Recognition (CCBR), 2019.

[46] Kai Yang, Zihao Xu, and Jingjing Fei. DualSANet: Dual
Spatial Attention Network for Iris Recognition. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV), pages 889–897, January 2021.

10

