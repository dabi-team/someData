Differentiable Drawing and Sketching

Daniela Mihai*
The University of Southampton
Southampton, UK
adm1g15@ecs.soton.ac.uk

Jonathon Hare*
The University of Southampton
Southampton, UK
jsh2@ecs.soton.ac.uk

1
2
0
2

l
u
J

9
1

]

V
C
.
s
c
[

2
v
4
9
1
6
1
.
3
0
1
2
:
v
i
X
r
a

Parametric
Primitives

point(-0.5,-0.5)
point(0.5,-0.5)
line(0.5,0,0,0)
bezier(-0.5,0,-0.5,-0.5,0.5,-0.5,0.5.0)

Relaxed
Rasterisation

Distance
Transforms

Differentiable
Composition

Parameter
Decoder
Net

Encoder
Net

Rasteriser

r
o
t
c
e
V

g
n
i
d
o
c
n
E

e
v
i
t
i

m

i
r
P

s
r
e
t
e
m
a
r
a
P

(a) Overview of the proposed approach to
drawing. The ﬁnal image is differentiable
with respect to the primitive parameters.

(b) Seurat’s ‘Une baignade `a Asni`eres’ re-
duced to straight lines instead of points by
gradient descent through the rasteriser.

(c) Encoder-Decoder-Rasteriser model for
learning to map images to primitives. Or-
ange blocks have learnable parameters.

Figure 1: With a differentiable rasteriser (a), it is possible to optimise primitives (b), and build end-to-end learnable models (c).

Abstract
We present a bottom-up differentiable relaxation of the
process of drawing points, lines and curves into a pixel raster.
Our approach arises from the observation that rasterising
a pixel in an image given parameters of a primitive can be
reformulated in terms of the primitive’s distance transform,
and then relaxed to allow the primitive’s parameters to be
learned. This relaxation allows end-to-end differentiable pro-
grams and deep networks to be learned and optimised, and
provides several building blocks that allow control over how
a compositional drawing process is modelled. We emphasise
the bottom-up nature of our proposed approach, which al-
lows for drawing operations to be composed in ways that can
mimic the physical reality of drawing rather than being tied
to, for example, approaches in modern computer graphics.
With the proposed approach we demonstrate how sketches
can be generated by directly optimising against photographs
and how auto-encoders can be built to transform rasterised
handwritten digits into vectors without supervision. Extens-
ive experimental results highlight the power of this approach
under different modelling assumptions for drawing tasks.

1. Introduction and Motivation

This paper proposes a differentiable relaxation of the ras-
terisation process, which we ultimately demonstrate allows

*Authors contributed equally.

us to build end-to-end learnable machines that can perform
both image generation and inference tasks. More concretely,
we demonstrate that we can build machines that turn digital
raster images into parametric representations of continuous
paths, and back into rasterised images again. Our approach
is not constrained by any particular modelling assumptions
about how images should be composed beyond the functions
used being differentiable. This allows us to closely model
the physical act of drawing with a pen on paper for example.
We believe that the proposed approach will ultimately have
many applications in future approaches to computer vision
tasks related to topics including sketch retrieval, recognition,
and generation, as well as topics related to understanding
and analysing handwriting, and even to more general topics
around understanding visual communication.

When humans use drawing, sketching and writing to com-
municate they rarely do so by ﬁlling in pixels on a grid. Most
methods (with some notable exceptions) of producing phys-
ically realised forms of drawing and writing by hand involve
manipulating an instrument (a pen, paintbrush, pastel, etc)
to mark a surface (paper, for example). In the digital world,
this process is often approximated with vector graphics, in
which paths are ‘stroked’ and then most often rasterised onto
a pixel grid to produce digital images that can be displayed
on a monitor or reproduced in hard copy.

To date, modelling the act of drawing with techniques
such as deep neural networks has been relatively limited

1

 
 
 
 
 
 
because the process of rasterisation using traditional ap-
proaches is not differentiable. The vast majority of recent
work on image generation has operated on the principle of
trying to optimise outputs broadly at the pixel level utilising
tools such as transpose convolutions which operate on ras-
ter representations. There are of course exceptions to this
statement, where researchers have attempted to more closely
consider the underlying process that humans use to draw
and write [e.g. 14], or to circumvent the non-differentiability
of rasterisation [e.g. 39] using learning. These techniques,
as well as a contemporaneous approach to relaxing modern
vector graphics [17] (taking a complementary, but different
approach to ours) which was published during the production
of this manuscript, are described and discussed in section 2.
Our contributions are as follows: 1. We present a bot-
tom-up differentiable approach (see ﬁg. 1a) to generating
pixel rasters from parameterised vector primitives by refor-
mulating and relaxing the rasterisation problem. This is
coupled with a set of formulations that allow different ap-
proaches to composition. A full exposition of our approach
is in section 3. 2. We demonstrate that primitives can be
optimised by minimising a loss against an existing raster
image (cf. ﬁg. 1b), and show how different losses inform
the result. Details are in section 4. 3. We create a range
of parameterisations of primitives in end-to-end learnable
autoencoder architectures (see ﬁg. 1c) for handwritten char-
acters and objectively compare performance. See section 5
for details. We provide a PyTorch implementation of our
approach, which allows others to experiment further. Code
is available at https://bit.ly/2PHtt5v.

2. Related Work

Drawing, and in particular sketching, has been a means
of conveying concepts, objects and stories since ancient
times. There is a long history of sketch research in com-
puter vision and human-computer interaction dating back
to the 1960s [29]. Sketch applications have become in-
creased in recent years due to the rapid development of deep-
learning techniques that can successfully tackle tasks such
as sketch recognition [37], generation [39, 9, 25], sketch-
based retrieval [4, 24, 6], semantic segmentation [33, 36],
grouping[16], parsing[26] and abstraction[21]. Xu et al.
[35] offer a recent and detailed survey of free-hand sketch
research and applications, focusing on contemporary deep-
learning techniques. Our long-term goal for the work presen-
ted in this paper is to be able to train models to learn how
to produce the parameters of drawing primitives based on
visual inputs with only limited supervision. Internally within
our models, we want to bridge the gap between input and
output rasters, and internal vector representations.

There is a body of recent literature describing models
that operate purely on vector stroke data (that is, the process
of actually drawing the vectors into an image is not part of

the learning machinery). This includes recurrent generative
models for sketch data [e.g. 9], generative models utilising
GANs for sketch generation in vector format [e.g. 1], and
reinforcement learning [e.g. 40, 34]. Another line of work
within sketch generation uses Bayesian Program Learning,
rather than deep networks, to represent the act of drawing as
a probabilistic generative model [14].

With respect to models that turn raster images into vec-
tors, there is considerable classical literature looking at the
problem of ‘stroke-based rendering’ where the objective is to
turn raster images into a sequence of strokes [e.g. 31, 10, 30]
for artistic or visual communication purposes. A good over-
view of these can be found in the tutorial by Hertzmann [11],
which breaks these approaches into Voronoi (broadly based
on Lloyd’s algorithm), or ‘trial and error’ approaches which
try to minimise a loss based on heuristic tests. The approach
presented in section 4 is clearly of relevance to this ﬁeld of re-
search, but a differentiable rasteriser allows for a potentially
more principled or ﬂexible approach to modelling the draw-
ing process or the loss that is optimised. A number of models
have been proposed that incorporate drawing into learning
machinery. Until very recently, the process of rasterisation
and rendering was thought to be non-differentiable, so two
approaches were used to circumvent this problem: ﬁrstly
there were models that use reinforcement learning to learn
drawing actions through a traditional (non-differentiable)
renderer [e.g. 7, 20], and, secondly there were approaches
that ‘learn’ renderers (typically formulated as networks of
transposed convolutions, or convolutions and upsampling
operations) that can take vector inputs and produce raster out-
puts [39, 42, 22]. Of the latter, the work by Zheng et al. [39]
is most similar to ours in its intent to work with sketches,
and to utilise encoder models to produce accurate stroke
parameters from raster images; our models in section 5 are
however fully end-to-end learnable, unlike Zheng et al.’s
model in which the renderer network is trained separately.
Models with learned rasterisers are also inherently inﬂexible
in the sense that they have to be trained for every type of
stroke parameterisation they can work with.

Recent approaches to differentiable 3D rendering have
garnered attention in the computer vision community [e.g.
18, 12], and indeed it is the work of Liu et al. [18] that ori-
ginally helped inform the approach we detail in section 3.
Most recently, during the development of our approach, Li
et al. [17] presented a differentiable relaxation that takes
advantage of how anti-aliasing is performed in modern com-
puter graphics systems using multi-sampling, by providing
differentiable relaxations. We consider this to be a top-down
approach to the problem because it does not change the
underlying rendering model. Conversely, we consider our
approach to be bottom-up because we explicitly allow the
rendering model to be ﬂexibly deﬁned in a way that is appro-
priate to the task.

2

(a) Nearest-neighbour rasterisation:
the closest pixel is shaded by the by
ﬂooring the point ordinate. There
is no useful gradient information.

(b) Anti-aliased rasterisation: the
closest two pixels are shaded pro-
portionally by to the distance from
the point to those two pixels.

(c) Rasterisation using eq. (3) with
σ2 = 1. Every pixel in the image
will have a (small) gradient with
respect to the point ordinate.

Figure 2: Different point rasterisation functions illustrated in one-dimension.

3. Differentiable relaxations of rasterisation

In this section we discuss the problem of drawing, or
rasterising points, lines and curves deﬁned in a continuous
world space W into an image space I. Our objective is to
present a formalisation that allows us to ultimately deﬁne
rasterisation functions that are differentiable with respect to
their world space parameters (e.g. the (co)ordinate of a point,
or (co)ordinates of the beginning and end of a line segment).

3.1. 1D Rasterisation

We ﬁrst consider the problem of rasterising a one-
dimensional point p ∈ W where W = R. Concretely, the
process of rasterisation of the point p can be deﬁned by a
function, f (n; p), that computes a value (typically [0, 1]) for
every pixel in the image space I, whose position is given
by n ∈ I. Such a function represents a scalar ﬁeld over
the space of possible values of n. Commonly we consider
values of n to be non-negative integers from the lattice or
grid, Z1

0+, deﬁning a pixel in the image.

Simple closest-pixel rasterisation functions.
If we as-
sume that the 0th pixel covers the domain [0, 1) in the world
space of a point p, and that the 1st pixel covers [1, 2), etc.
Nearest-neighbour rasterisation then maps the real-valued
point, p, to an image by rounding down:

f (n; p) =

(cid:40)

1
0

if (cid:98)p(cid:99) = n
otherwise .

(1)

This process is illustrated in ﬁg. 2a. An alternative rasterisa-
tion scheme, illustrated in ﬁg. 2b is to interpolate over the
two closest pixels. Assuming that a pixel has maximal value
when the point being rasterised lies at its midpoint, then:

f (n; p) =






1.5 − p + (cid:98)p − 0.5(cid:99)
0.5 + p − (cid:100)p − 0.5(cid:101)
0

if (cid:98)p − 0.5(cid:99) = n
if (cid:100)p − 0.5(cid:101) = n
otherwise .

(2)

These functions (extended to 2D) are actually implicitly used
in many computer graphics systems, but rarely in the form
we have written them. Most graphics subroutines approach
the rasterisation problem from the perspective of directly de-
termining which pixels in n should have a colour associated
with them given p as this is more efﬁcient if the objective is
just to draw the primitive p.

Differentiable Relaxations.
Ideally, we would like to be
able to deﬁne a rasterisation function that is differentiable
with respect to p. This would allow p to be optimised with
respect to some objective. The rasterisation function given
by eq. (1) is piecewise differentiable with respect to p, but
the gradient is zero almost everywhere which is not useful.
Although eq. (2) has some gradient in the two pixels nearest
to p, overall it has the same key problem: the gradient is zero
almost everywhere.

We would like to deﬁne a rasterisation function that has
gradient for all (or at least a large proportion of) possible
values of n. This function should be continuous and differ-
entiable almost everywhere. The anti-aliased rasterisation
approach gives some hint as to how this could be achieved:
the function could compute a value for every n based on
the distance between n and p. Distance metrics have an
inﬁnite upper bound, whereas we want our pixel values to
be ﬁnitely bounded in [0, 1], so inversion and application of
a non-linearity are necessary. The properties of the chosen
function should give values close to 1 when n and p are
close, and values near 0 when they are far apart.

An obvious choice of non-linearity would be to exponen-
tiate the negative squared distances, and use a scaling factor
σ2 to control the fuzziness of the rasterisation and the size
of the point or width of the line stroke (see ﬁg. 2c):

f (n; p) = exp(−d2(n, p − 0.5)/σ2) .

(3)

It can be shown that there is a direct linear relationship (see
proof in appendix A) between the size of a point or thickness
of a line, t, and the value of σ: σ ≈ 0.54925t ∀ t > 0.

3.2. Relaxed Rasterisation in N-dimensions

All of the 1D rasterisation functions previously deﬁned
can be trivially extended to rasterise a point in two or more
dimensions. For example, if the point p was considered to
be a vector in the world space W = R2 and correspondingly
n was a vector in the image space1, I = Z2
0+, and the ﬂoor
and ceiling operators are applied element-wise then all three
1D rasterisation functions hold in two (or more) dimensions.

1Note that it is most common to use positive integers to index pixels
in the image space, but this isn’t a requirement; the image space could be
unbounded or real for example.

3

Line Segments. A line segment can be deﬁned by its start
coordinate s = [sx, sy] and end coordinate e = [ex, ey]. The
normal approaches to rasterising lines in computer graph-
ics [e.g. 2, 32] are highly optimised and work by considering
just the pixels that intersect the line or are within a few pixels
of it. These algorithms typically iterate over the line, setting
the underlying pixels values accordingly. To develop a gen-
eral set of (potentially differentiable) rasterisation functions
we need to consider a formalisation of rasterisation as we did
in the 1D case where we consider a function that deﬁnes a
scalar ﬁeld over the set of all pixel positions, n, in the image
given a particular line segment: f (n; s, e).

To rasterise a line segment one needs to consider how
close a pixel is to the segment. We can efﬁciently compute
the squared Euclidean distance of an arbitrary pixel n to the
closest point on the line segment as follows:

m = e − s ,

t = ((n − s) · m)/(m · m) ,

d2
seg(n, s, e) =






||n − s||2
2
||n − (s + tm)||2
2
||n − e||2
2

if t ≤ 0
if 0 < t < 1
if t ≥ 1 .

(4)

Concretely, d2
seg(n, s, e) is the squared Euclidean Distance
Transform of the line segment. It deﬁnes a scalar ﬁeld in
which the value is equal to the squared distance to the closest
point on the line segment. This function is piecewise smooth
and differentiable with respect to the line segment parameters
everywhere for a given n.

In the case of nearest-neighbour rasterisation one would
ask if the line passes through the pixel in question and only
ﬁll it if that were the case:

f (n; s, e) =

(cid:40)
1
0

seg(n, s, e) ≤ δ2

if d2
otherwise .

(5)

Assuming a 1-1 mapping between the domains of the co-
ordinate system of the image space and world space, then
δ2 = 0.5 would give a rasterisation that mimics the 1-pixel
wide line that would be drawn by Bresenham’s algorithm [2].
If we replace the calculation of distance to a point in eq. (3)
with the minimum distance to the line segment we get a line
segment rasteriser that is differentiable with respect to the
parameters of the line segment s and e:

f (n; s, e) = exp(− d2

seg(n, s, e)/σ2) .

(6)

Curves.
It is common in computer graphics to utilise para-
metric curves C(t, θ) where θ deﬁnes the parameters and
0 ≤ t ≤ 1. Typically C(t, θ) is polynomial (usually quad-
ratic or cubic in t). The parameters θ are commonly speciﬁed
in B´ezier (e.g. B´ezier Curves) or Hermite form (e.g. Catmull-
Rom splines) as described in appendix B. To rasterise a

4

curve (irrespective of the parameterisation) in a way that is
differentiable with respect to the parameters we can follow
the same general approach that was taken for line segments:
compute the minimum Squared Euclidean distance between
each coordinate n ∈ I and the curve:

d2
cur(n, θ) = min

t
s.t.

||C(t, θ) − n||2
2

0 ≤ t ≤ 1 .

(7)

As in the case of line segments, this distance transform can
then be combined with a rasterisation function that works in
terms of a distance:

f (n; θ) = exp(− d2

cur(n, θ)/σ2) .

(8)

The only additional challenge over the rasterisation of line
segments is that the computation of the distance map requires
solving a constrained minimisation problem and doesn’t have
a closed form solution. A number of different approaches
are possible (see appendix C), however, in practice, we have
had success with both a fast polyline approximation2 and a
recursive approach which are both easily vectorised as tensor
operations that can be performed efﬁciently on a GPU.

3.3. Composing Multiple Primitives

To rasterise multiple lines3 we can consider combining
the rasterisations of different line segments into a single
image. We denote images produced by rasterising dif-
ferent line segments {s1, e1}, {s2, e2}, . . . , {si, ei} into
matrices I (1), I (2), . . . , I (n) deﬁned over the same image
space I. In the simplest case, where we have binary rasterisa-
tions, we might consider that the logical-or of corresponding
pixels would produce the desired effect of selecting any
pixels that were shaded in the individual rasterisations as
being shaded in the ﬁnal output:

c(I (1), I (2), . . . , I (n)) = I (1) ∨ I (2) ∨ · · · ∨ I (n) .

(9)

We can relax this composition to be differentiable and also
allow the pixel values to be non binary (but restricted to
[0, 1]) as follows:

csoftor(I (1), I (2), . . . , I (n)) = 1 −

n
(cid:89)

(1 − I (i)) .

(10)

i=1

Effectively if a pixel is ‘on’ in any of the individual images
then this will select it as being ‘on’ in the output. This
approach treats all the input images as a set; the output
will be the same irrespective of the order they appear in.

2Note that there is a potential for a small change in curve’s parameters
to cause a large difference in the polyline approximation, although we have
not seen this become an issue in practice.

3We’re considering composing multiple line segments, but everything
here also applies to multiple points and curves, as well as combinations of
line segments, points and curves, or indeed any other raster.

n1n2set<0t>0n1n2n1n2n1n2n1n2n1n2C(t=0)C(t=1)n1n2n1n2Undoubtedly many other possible differentiable composition
functions exist with alternative properties; we propose and
discuss a number of these alternatives in appendix D. The
experiments that follow use the above soft-or function, with
the notable exception of the image in ﬁg. 1b which was
generated using the over composition (see appendix D) as
this is more appropriate for colour images.

3.4. Extended drawing

Clearly, at this point, we now have all the components
required to construct a basic drawing system. There are
however a number of aspects that haven’t been considered,
including, for example how to draw in colour. As we focus
the remainder of the paper on utilising the approach we have
already described, discussion of additional extensions related
to drawing and rasterisation can be found in appendix E.

3.5. Advantages and Limitations

The rasterisation process described in this section in prin-
ciple allows gradients to ﬂow from every pixel in the image
to the parameters of a rendered primitive (note however that
in practice this is not the case because of ﬁnite numeric pre-
cision). This is in contrast to the work of Li et al. [17] where
the gradients are limited by the size of the ﬁlter. The ad-
vantage of our method is that optimisation should be easier
with more gradient, however of course this does itself also
have disadvantages. Computationally, our approach can be
entirely implemented as batched tensor operations (this in-
cludes computation of distance transforms for all primitives),
so all computation can be performed on the GPU making
use of all available processing resources, and unlike Li et al.
[17]’s approach, does not involve the CPU for rendering.
The disadvantage is that memory usage could be very high,
particularly for batches of large images with many primit-
ives (in our original envisaged use case of exploring simple
sketching and writing this is not a problem however). One
interesting idea to explore in the future would be to utilise
sparse tensors to reduce storage requirements by not storing
pixels contributing to no value or gradient. Another potential
criticism of our approach is that the generated images will
be very slightly blurry as a result of the relaxation; again, for
our envisaged use case this is not a problem, and it is always
possible to use the relaxation for learning/optimisation, and
then switch to a regular render for generation at inference
time. Finally, we draw attention to the fact that our approach
is not restricted to 2D, and can be e.g. directly applied to 3D
data for voxel rasterisation.

4. Direct Optimisation of Primitive Parameters

With the machinery deﬁned in section 3 it is now possible
to deﬁne a complete system that takes the parameters describ-
ing primitives and rasterises those primitives into an image.
If a loss function is introduced in the image space, between

the complete rasterised image and a ﬁxed target image, it
becomes possible to compute gradients with respect to the
parameters of the primitives that created the rasterised image.
Minimising this loss will adapt the underlying primitives to
“shapes” that best ﬁt the target image.

A commonly used ‘reconstruction’ loss function for im-
ages is the mean squared error between the target and the
generated image. We can thus formalise the optimisation
problem as,

min
θ

(cid:107)R(θ) − T (cid:107)2
2 ,

(11)

for a target image, T and rasterisation function R deﬁned
over the same image space I. The rasterisation function itself
is deﬁned as a composition c(. . . ) (see section 3.3) over k
primitives, themselves rasterised by primitive rasterisation
functions, f (i) (e.g. eqs. (3), (6) and (8), etc.):

R(θ) = c(f (1)(θ(1)), f (2)(θ(2)), . . . , f (k)(θ(k)))
where θ = [θ(1)|θ(2)| . . . |θ(k)] .

(12)

If the rasterisation function R is differentiable with respect
to θ, then the minimisation problem in eq. (11) can be solved
using gradient descent. Note that the problem is in general
non-convex, with potentially many local optima4; see ap-
pendix G for more discussion. Additionally, the magnitude
of gradients can become vanishingly small, which is par-
ticularly problematic with ﬁxed-precision arithmetic; this
problem can however be overcome as we demonstrate in the
following sections.

4.1. Loss Functions

MSE loss is not the only possible choice; in fact, MSE
has one signiﬁcant disadvantage in that if we are drawing in
black and white, but optimising a grey-level image, then the
loss landscape would be very ﬂat. This is illustrated in table 1
where it can be seen that both conﬁgurations of the generated
image in the ﬁrst input to the loss function produce exactly
the same MSE value. Human vision does not suffer from the
same problem; we can see that the two input images to the
loss functions are clearly different. In addition, if we look

4The optimisation landscape has considerable permutation symmetry.
For example: the start and end of a line segment could be swapped with
no change to the resultant image; if the composition is non-sequential the
order of the rendered primitives could be permuted; etc.

L =
(cid:16)

L

(cid:16)

L

(cid:17)

(cid:17)

,

,

MSE SSMSE BlurMSE

0.25

0.25

0.42

0.25

0.13

0.02

Table 1: Loss functions incorporating scale can overcome
limitations of MSE and induce gradients.

5

from far enough away the striped example on the second
row, the image and the target would begin to look the same
to us. To build this phenomenon into the loss function we
can incorporate a notion of spatial scale. We utilise two
such approaches: BlurMSE, a single-scale version of MSE
in which the input, and optionally the target are blurred by a
Gaussian ﬁlter of a predetermined standard deviation; and
the SSMSE, a scale-space variant in which a scale-space
(or optionally a scale pyramid) is built for both the input
and target, and the loss is accumulated over all levels. Our
implementation of the scale-space follows Lowe [19], and
constructs a space with octaves deﬁned by a doubling of
the standard deviation, and a ﬁxed number of intervals per
octave. As can be seen from table 1, the losses incorporating
scale produce smaller values for the striped input image on
the second row, compared to the half-half image on the ﬁrst
row, thus indicating usable gradient information.

A Gaussian scale-space alone is not necessarily a good
measure of how a human perceives an image as it fundament-
ally only helps capture areas of light and dark, and ensures
they are shaded accordingly in the generated image. Many
perceptually motivated distance metrics have been proposed
in the past, such as the well-known SSIM [41] and its vari-
ants. More recently, it has been shown that features from
deep convolutional networks can correlate well with human
perceptual judgements of image similarity, and this has mo-
tivated the development of CNN-based perceptual losses like
LPIPS [38]. Because a loss based on deep features would
inherently be differentiable, we can utilise it as an objective
when optimising primitive parameters that deﬁne an image.

4.2. Examples: image based optimisation

To demonstrate the effectiveness of our approach for op-
timising primitives against a real image we provide a number
of examples. All of the generated images in ﬁgs. 4 and 5
utilise the 200 × 266 pixels input image in ﬁg. 3a as the
target image to optimise against. Points and pixels are optim-
ised to have a 1-pixel diameter/thickness in the image space.
The domain of the world-space is constrained to [-1,1] on
the y-axis and scaled proportionally on the x-axis. All gen-
erated examples were optimised using Adam [13] with a
learning rate of 0.01 for 500 iterations. Figure 4 shows the
results from optimising 1000 points and 1000 lines using
blurred MSE loss and demonstrates the overall effect that
can be achieved. Figure 5 shows the effect of optimising 500
line segments from the same starting point using a range of
different losses.

It is instructive to compare how the automatically gen-
erated sketches compare to an image drawn by a human.
Figure 3b is a hand-drawn pen and ink sketch of the same
scene as used in the generation of ﬁgs. 4 and 5. It is clear
that all of the sketches broadly capture the overall structure
of the scene and areas of light and dark. However, there

are signiﬁcant differences in the way this is captured. The
losses based on MSE (including scale-space and blurred)
all display the same trait of capturing the local intensity,
although this is much more pronounced in the scale-space
and blur variants, which also capture more detail. Changing
the number of intervals per octave in the scale-space losses
has very little overall effect (subtle changes around the ‘bal-
loon’). The perceptual loss using AlexNet captures highly
local structure, but overall the resultant image is perhaps the
least perceptually similar (or interpretable) of all the images.
The perceptual loss using VGG captures a lot of the structure
of the image; it is interesting how much of the broad shape
information is captured, and how areas of light and dark are
also represented. In addition, we can observe that the over-
all brightness on the right-hand side is lighter than the left,
mimicking the human-drawn sketch, even though the raw
grey-level values in the input image are similar on both sides.
The differences between the two perceptual losses reﬂect
the observation that the VGG variant is closer to traditional
notions of perceptual difference when used for optimisa-
tion [38]. Related to the observation that the VGG model
seems to capture shape information rather well, we wonder
if direct optimisation in the way we have performed it might
lead to a new way to probe the (lack of) shape bias in dif-
ferent neural architectures [8]. This could ultimately help
us move closer to networks that robustly recognise objects
from both sketches and photographs.

5. Autotracing autoencoders

We next look at models that learn to perform autotra-
cing of handwritten characters with only self-supervision.
The structure of our autotracing model, shown in ﬁg. 1c,
is similar to that of a standard autoencoder, with two main
components: an image encoder that creates a latent encod-
ing, and a parameter decoder that decodes a latent vector
to ‘stroke data’. This stroke data is then rasterised into the
output image. Both the encoder and parameter decoder have
learnable parameters, but the rasterisation is entirely ﬁxed.
We next demonstrate a series of decoders which allow
for different approaches to drawing. For example, we con-
sider stroke parametrisation functions such as independent
straight lines/curves, connected lines/curves through a series
of consecutive points, and sets of points with learned con-
nections between them. These models lay the groundwork
for future exploration of learned, differentiable models of
sketching that are more similar to how humans write/draw
that e.g. address the challenges set out by Lake et al. [14].

Encoders. For experiments on MNIST [15], we present
results using a simple multi-layer perceptron encoder net-
work. For more complex characters of Omniglot [14], a
convolutional network is preferred. When comparing against

6

(a) Photo

(b) Pen & Ink Sketch

(a) 1000 Points

(b) 1000 lines

Figure 3: Pictures of the author, by the other author.

Figure 4: Optimising against ﬁg. 3a using BlurMSE (σ = 1.0).

(a) Initialisation

(b) MSE

(c) BlurMSE (σ = 1.0)

(d) BlurMSE (σ = 3.0)

(e) BlurMSE (σ = 5.0)

(f) SSMSE, 1i/o

(g) SSMSE, 2i/o

(h) SSMSE, 4i/o

(i) LPIPS(AlexNet)

(j) LPIPS(VGG)

Figure 5: Images created by optimising parameters using gradient descent with different losses. Parameters optimised to ﬁt the
photo shown in ﬁg. 3a starting from the random lines in ﬁg. 5a. All images using SSMSE use 5 octaves, and ‘i/o’ abbreviates
the number of intervals per octave. Note that regular MSE is just BlurMSE with σ = 0.

StrokeNet [39] (table 2b), we replicate their VGG-like En-
coder. Full model details are provided in appendix H.1.

Decoders. Our decoder networks allow for different para-
metrisations of ‘stroke data’ that is then used by the rasteriser
described in section 3. The decoder transforms a vector en-
coding of the input image to lists of stroke primitives which
aim to reproduce the input image when rasterised. In the
simplest case, the latent vector can be decoded to a ﬁxed num-
ber of line segments (LineDecoder), each deﬁned by their
start and end points. Next, we provide PolyLineDecoder,
for which a stroke is represented as a sequence of consecut-

ive points. Instead of line segments, we can choose to use
curves parameterised as Catmull-Rom splines (CRSDecoder)
or B´ezier curves (B´ezierDecoder). In both cases, we can
control the number of joined curve segments by specifying
how many points (CRS) or segments (B´ezier) are used. To
allow more ﬂexibility in modelling, we have also explored
decoders which incorporate sub-networks to learn to produce
a set of 2d points, and the upper-triangular portion of a soft
connection matrix between points (optionally including the
diagonal). The network producing the connection matrix
uses a sigmoid to ensure values are between 0 and 1. To

7

Decoder

Line
PolyLine
PolyConnect
CRS
B´ezier
B´ezierConnect

#P

10
16
16
16
20
16

#S

1
15
-
14
1
-

#L MSE

Acc.

Model

5
1
-
1
5
-

0.0195
0.0225
0.0118
0.0208
0.0136
0.0116

94.06%
93.27%
96.47%
94.63%
96.34%
96.43%

StrokeNet [39]
StrokeNet [39]
Ours, CRS
Ours, B´ezier
Ours, B´ezier
Ours, B´ezier

Steps

3 (SN)
1
1
3 (GRU)
1
1

#P

16
16
16
4
7
43

#S

14
14
14
1
2
14

#L

Acc.

1
1
1
1
2
1

95.25%
97.75%
97.12%
96.97%
98.28%
97.94%

(a) MNIST Test Dataset (baseline unencoded acc. 98.60%).

(b) Scaled MNIST Dataset (baseline unencoded acc. 98.58%).

Table 2: Reconstruction performance of parameterisations, measured by MSE and classiﬁcation accuracy with a classiﬁer
trained on unencoded training sets of the respective datasets. #* indicates the number of (L)ines, (S)egments, and (P)oints. All
Scaled MNIST models use the same ‘StrokeNet Agent’ architecture [39] to map images to primitive parameters.

utilise the connection matrix, all possible combinations of
lines are rasterised and are multiplied by the appropriate
connection weight before composition (PolyConnect). In
the case of B´ezier curves (B´ezierConnect) each point in the
connection matrix corresponds to both an end point and its
corresponding control point, and when drawing curves, the
end point is drawn using the mirror of its control point allow-
ing for smooth multiple-segment curves to be created. Zheng
et al. [39] proposed a recurrent model using a visual work-
ing memory; the network is presented at each timestep with
the features of the target image, together with the current
canvas, which is then encoded, concatenated with the input,
and transformed to the parameters of a new stroke which is
rendered and overlaid on the canvas. We experimented with
this approach but found it hard to train and computationally
expensive, so we also investigated a simple GRU [3] based
RNN which is fed a target image’s encoding as its initial
hidden state, along with a projection of a zeroed input. The
GRU output is projected to a set of B´ezier curve parameters
for rendering, and also re-projected for input at the next time
step. Full details are given in appendix H.2.

Table 2a shows the effect of different stroke parametrisa-
tions on MNIST (reconstructions shown in appendix I). As
an objective measure, we compute the classiﬁcation accur-
acy of rasterised sketches from the test set using a classiﬁer
(baseline accuracy of 98.6%); reconstructions that capture
the character should have higher accuracy. Connect mod-
els, which generate strokes based on a learned connection
matrix for the given number of points, perform best due to
the ﬂexibility of deciding which points should be joined in
a line/curve segment. Following Zheng et al. [39] we per-
form a similar experiment on their scaled MNIST dataset
(see appendix K), and also show results using the pretrained
StrokeNet models that are publicly available. The accuracies
of all models are high indicating good reconstructions, but
we note that MNIST doesn’t require complex decoders.

Figure 6 illustrates reconstruction of Omniglot [14]. Note
that the test set contains alphabets completely disjoint from

training/validation. Some small details of the characters
are missing, and it is clear that the models do not always
choose to draw stokes in the way a human would, but the
performance is generally good (see appendix J). Similar
analysis on additional datasets is shown in appendix L.

6. Future Directions

We have presented a derivation of a bottom-up differen-
tiable approach to rasterising vector primitives into images,
that allows gradients to ﬂow through every pixel in the im-
age to the underlying primitive’s parameters. Our approach
allows us to construct end-to-end models of vision that learn
primitive parameters directly from raster images. Further,
we have demonstrated how effective sketch generation can
be achieved with different losses, and how parameterisations
can change what a model learns.

Our approach is only a building block towards future ap-
plications and research. Our own motivation for designing
this approach is to use it to explore writing and visual com-
munication, although there are undoubtedly many potential
use-cases. For us, questions to be answered next involve
looking at how we might build models that can learn to pro-
duce the appropriate number of strokes (and choose between
different types of primitive). As part of this, it is clear that re-
construction performance alone should not be the key driver
of gradient; the ability to communicate information is more
important. Both attention and weak supervision to better

(a) Val

(b) Val rec.

(c) Test

(d) Test rec.

Figure 6: 28 pixel Omniglot validation and test data samples
and B´ezier model (3 segment, 5 line) reconstructions.

8

mimic humans are also key to this endeavour.

References

[1] S Balasubramanian, Vineeth N Balasubramanian, et al.
Teaching gans to sketch in vector format. arXiv preprint
arXiv:1904.03620, 2019. 2

[2] J. E. Bresenham. Algorithm for computer control of a digital

plotter. IBM Systems Journal, 4(1):25–30, 1965. 4

[3] Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using RNN
encoder–decoder for statistical machine translation. In Pro-
ceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1724–1734,
Doha, Qatar, Oct. 2014. Association for Computational Lin-
guistics. 8

[4] Jungwoo Choi, Heeryon Cho, Jinjoo Song, and Sang Min
Yoon. Sketchhelper: Real-time stroke guidance for free-
IEEE Transactions on Multimedia,
hand sketch retrieval.
21(8):2083–2092, 2019. 2

[5] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex
Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for
classical japanese literature. arXiv preprint arXiv:1812.01718,
2018. 35, 36

[6] Antonia Creswell and Anil Anthony Bharath. Adversarial
In European Conference on

training for sketch retrieval.
Computer Vision, pages 798–809. Springer, 2016. 2

[7] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali
Eslami, and Oriol Vinyals. Synthesizing programs for im-
ages using reinforced adversarial learning.
In Jennifer G.
Dy and Andreas Krause, editors, Proceedings of the 35th
International Conference on Machine Learning, ICML 2018,
Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018,
volume 80 of Proceedings of Machine Learning Research,
pages 1652–1661. PMLR, 2018. 2

[8] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Mat-
thias Bethge, Felix A. Wichmann, and Wieland Brendel.
Imagenet-trained CNNs are biased towards texture; increasing
shape bias improves accuracy and robustness. In International
Conference on Learning Representations, 2019. 6

[9] David Ha and Douglas Eck. A neural representation of sketch
drawings. In International Conference on Learning Repres-
entations, 2018. 2

[10] Aaron Hertzmann. Painterly rendering with curved brush
In Proceedings of the 25th An-
strokes of multiple sizes.
nual Conference on Computer Graphics and Interactive Tech-
niques, SIGGRAPH ’98, page 453–460, New York, NY, USA,
1998. Association for Computing Machinery. 2

[11] A. Hertzmann. A survey of stroke-based rendering. IEEE
Computer Graphics and Applications, 23(4):70–81, 2003. 2
[12] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada.
Neural 3d mesh renderer. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018. 2
[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In Yoshua Bengio and Yann LeCun,
editors, 3rd International Conference on Learning Repres-
entations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, 2015. 6

9

[14] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Ten-
enbaum. Human-level concept learning through probabilistic
program induction. Science, 350(6266):1332–1338, 2015. 2,
6, 8

[15] Yann LeCun, Corinna Cortes, and Christopher JC Burges. The
mnist database of handwritten digits, 1998. URL http://yann.
lecun. com/exdb/mnist, 10:34, 1998. 6

[16] Ke Li, Kaiyue Pang, Jifei Song, Yi-Zhe Song, Tao Xiang,
Timothy M. Hospedales, and Honggang Zhang. Universal
sketch perceptual grouping. In Proceedings of the European
Conference on Computer Vision (ECCV), September 2018. 2
[17] Tzu-Mao Li, Michal Luk´aˇc, Micha¨el Gharbi, and Jonathan
Ragan-Kelley. Differentiable vector graphics rasterization for
editing and learning. ACM Trans. Graph., 39(6), Nov. 2020.
2, 5, 13

[18] S. Liu, W. Chen, T. Li, and H. Li. Soft rasterizer: A differ-
In 2019
entiable renderer for image-based 3d reasoning.
IEEE/CVF International Conference on Computer Vision
(ICCV), pages 7707–7716, 2019. 2

[19] David G. Lowe. Distinctive image features from scale-
invariant keypoints. Int. J. Comput. Vision, 60(2):91–110,
Nov. 2004. 6

[20] John F. J. Mellor, Eunbyung Park, Yaroslav Ganin, Igor
Babuschkin, Tejas Kulkarni, Dan Rosenbaum, Andy Bal-
lard, Theophane Weber, Oriol Vinyals, and S. M. Ali Eslami.
Unsupervised doodling and painting with improved SPIRAL.
CoRR, abs/1910.01007, 2019. 2

[21] Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao
Xiang, and Timothy M. Hospedales. Learning deep sketch
abstraction. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018.
2

[22] Reiichiro Nakano. Neural painters: A learned differenti-
able constraint for generating brushstroke paintings. CoRR,
abs/1904.08410, 2019. 2

[23] Thomas Porter and Tom Duff. Compositing digital images.
In Proceedings of the 11th Annual Conference on Computer
Graphics and Interactive Techniques, SIGGRAPH ’84, page
253–259, New York, NY, USA, 1984. Association for Com-
puting Machinery. 14, 15

[24] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James
Hays. The sketchy database: learning to retrieve badly drawn
bunnies. ACM Transactions on Graphics (TOG), 35(4):1–12,
2016. 2

[25] Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and
James Hays. Scribbler: Controlling deep image synthesis with
sketch and color. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 5400–5409,
2017. 2

[26] Ravi Kiran Sarvadevabhatla, Isht Dwivedi, Abhijat Biswas,
and Sahil Manocha. Sketchparse: Towards rich descriptions
for poorly drawn sketches using multi-task hierarchical deep
In Proceedings of the 25th ACM international
networks.
conference on Multimedia, pages 10–18, 2017. 2

[27] Thomas W Sederberg and Geng-Zhe Chang. Isolator poly-
nomials. In Algebraic Geometry and Its Applications, pages
507–512. Springer, 1994. 13

[28] Erik Sintorn and Ulf Assarsson. Hair self shadowing and
transparency depth ordering using occupancy maps. In Pro-

ceedings of the 2009 Symposium on Interactive 3D Graphics
and Games, I3D ’09, page 67–74, New York, NY, USA, 2009.
Association for Computing Machinery. 14

[29] Ivan E Sutherland. Sketchpad a man-machine graphical com-

munication system. Simulation, 2(5):R–3, 1964. 2

[30] Georges Winkenbach and David H. Salesin. Computer-
In Proceedings of the
generated pen-and-ink illustration.
21st Annual Conference on Computer Graphics and Interact-
ive Techniques, SIGGRAPH ’94, page 91–100, New York,
NY, USA, 1994. Association for Computing Machinery. 2

[31] Georges Winkenbach and David H. Salesin. Rendering para-
metric surfaces in pen and ink. In Proceedings of the 23rd
Annual Conference on Computer Graphics and Interactive
Techniques, SIGGRAPH ’96, page 469–476, New York, NY,
USA, 1996. Association for Computing Machinery. 2
[32] Xiaolin Wu. An efﬁcient antialiasing technique. SIGGRAPH

Comput. Graph., 25(4):143–152, July 1991. 4

[33] X. Wu, Y. Qi, J. Liu, and J. Yang. Sketchsegnet: A rnn
model for labeling sketch strokes. In 2018 IEEE 28th Interna-
tional Workshop on Machine Learning for Signal Processing
(MLSP), pages 1–6, 2018. 2

[34] Ning Xie, Hirotaka Hachiya, and Masashi Sugiyama. Artist
agent: A reinforcement learning approach to automatic stroke
generation in oriental ink painting. IEICE TRANSACTIONS
on Information and Systems, 96(5):1134–1144, 2013. 2
[35] Peng Xu, Timothy M Hospedales, Qiyue Yin, Yi-Zhe Song,
Tao Xiang, and Liang Wang. Deep learning for free-hand
sketch: A survey and a toolbox. arXiv e-prints, pages arXiv–
2001, 2020. 2

[36] Lumin Yang, Jiajie Zhuang, Hongbo Fu, Kun Zhou, and

Sketchgcn: Semantic sketch segmenta-
Youyi Zheng.
tion with graph convolutional networks. arXiv preprint
arXiv:2003.00678, 2020. 2

[37] Qian Yu, Yongxin Yang, Feng Liu, Yi-Zhe Song, Tao Xiang,
and Timothy M Hospedales. Sketch-a-net: A deep neural
network that beats humans. International journal of computer
vision, 122(3):411–425, 2017. 2

[38] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018. 6

[39] Ningyuan Zheng, Yifan Jiang, and Dingjiang Huang.
Strokenet: A neural painting environment. In International
Conference on Learning Representations, 2019. 2, 7, 8, 32,
33, 34

[40] Tao Zhou, Chen Fang, Zhaowen Wang, Jimei Yang, Byung-
moon Kim, Zhili Chen, Jonathan Brandt, and Demetri Terzo-
poulos. Learning to doodle with stroke demonstrations and
deep q-networks. In British Machine Vision Conference 2018,
BMVC 2018, Newcastle, UK, September 3-6, 2018, page 13.
BMVA Press, 2018. 2

[41] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-
celli. Image quality assessment: from error visibility to struc-
IEEE Transactions on Image Processing,
tural similarity.
13(4):600–612, 2004. 6

[42] Zhengxia Zou, Tianyang Shi, Shuang Qiu, Yi Yuan, and
Zhenwei Shi. Stylized neural painting. CoRR, 2020. 2

10

Appendices

Figure VII: Is this what Georges might choose to paint if he decided to paint with straight lines rather than points? This image
was created by taking a photo of the original ‘Une baignade `a Asni`eres’ from Wikipedia (https://en.wikipedia.org/
wiki/Bathers_at_Asnieres) and optimising 500 uniformly coloured lines to ﬁt the image.

A. Relating σ to line thickness.

Consider the 1D rasterisation of a point of size t given by
the scaled unit box function Π(x/t) and the relaxed raster-
isation given by exp(−d2(x)/σ2) as illustrated in ﬁg. VIII.
We want to ﬁnd a relationship between the value of t and

1

0.5

σ2 when trying to minimise the squared difference of the
functions across the entire domain x,

min
σ2

s.t.

(cid:90) ∞

(e−x2/σ2

− Π(x/t))2 dx

−∞
t > 0
σ2 > 0 .

(A.1)

0
−7−6−5−4−3−2−1 0 1 2 3 4 5 6 7
x

The integral term can be expanded and evaluated as follows
(assuming the constraints t > 0 and σ2 > 0):

exp(−x2/σ2); σ2 = 1
exp(−x2/σ2); σ2 = 4
Π(x/t); t = 2

(cid:90) ∞

−∞

(e−x2/σ2

− Π(x/t))2 dx

(cid:90) ∞

=

e−2x2/σ2

− 2Π(x/t)e−x2/σ2

+ Π(x/t)2 dx

Figure VIII: Illustration of a target point of thickness t = 2
pixels and approximations with the exp rasterisation function
with different σ2 values.

−∞
(cid:114) π
2

= σ

√

π erf

− 2σ

(cid:19)

(cid:18) t
2σ

+ t .

(A.2)

11

the control point P1 which is the point the tangents to the
curve at P0 and P2 intersect. The curve would not normally
pass through P1. The curve can be thought of as leaving P0
in the direction of P1 and gradually bending to arrive at P2
from the direction of P1. The quadratic B´ezier is deﬁned as:

Cbez2 (t, θ) =(1 − t)2P0 + 2(1 − t)tP1 + t2P2

(B.1)

where

(cid:33)

θ = [P0|P1|P2] .

Now, differentiating and setting to zero gives
√

d (cid:0)σ(cid:112) π

2 − 2σ

π erf (cid:0) t

(cid:1) + t(cid:1)

2σ

0 =

(cid:114) π
2
(cid:114) π
2
(cid:114) π
2
(cid:114) π
2

=

=

=

=

√

π

− 2

√

π

− 2

(cid:1)(cid:1)

dσ

2σ

d (cid:0)σ erf (cid:0) t
dσ
(cid:18) t
2σ

erf

(cid:32)

(cid:19)

(cid:1)(cid:1)

(cid:33)

+ σ

d (cid:0)erf (cid:0) t
dσ

2σ

√

− 2

π erf

√

− 2

π erf

(cid:19)

(cid:19)

(cid:18) t
2σ
(cid:18) t
2σ

√

− 2

(cid:32)

πσ

−

te−t2/(4σ2)
√

σ2

π

+

2te−t2/(4σ2)
σ

.

(A.3)

Noting the common factors of t/σ in eq. (A.3) we can write
the right hand side as an expression in terms of c = t/σ:

(cid:114) π
2

√

− 2

π erf

(cid:17)

(cid:16) c
2

+ 2ce−c2/4 .

(A.4)

As shown in ﬁg. IX this expression is monotonically decreas-
ing and has a single root, which can be estimated numerically
as c ≈ 1.820657.

where

4
2
0
−2

−15 −10

−5

5

10

15

0
c

Figure IX: Plot of eq. (A.4).

where

This implies the relationship between t and σ is linear: σ ≈
0.54925t ∀ t > 0. This can be easily veriﬁed by substituting
σ = 0.54925t in eq. (A.3).

B. Curve Parameterisations

Curves are often represented mathematically by paramet-
ric functions C(t) that give the coordinates of the curve for
values of t, commonly in the closed interval [0, 1], with t = 0
representing the start point and t = 1 representing the end
point of the curve. Curves are often parameterised as the
coefﬁcients of polynomial basis functions, either in Hermite
(e.g. the curve is a linear combination of Hermite bases) or
B´ezier form (the curve is represented as a linear combination
of Bernstein bases). The following parametric curve formu-
lations are commonly used in computer graphics (and can
all be used in our differentiable rasterisation approach):

Quadratic B´ezier Curves
are parameterised by three
points: the start of the curve P0, the end of the curve P2 and

Cubic B´ezier Curves
are deﬁned by four points: P0 is the
start of the curve; P1 is the ﬁrst control point and indicates
the direction the curve leaves P0 from; P2 is the second
control point and indicates the direction that the curve arrives
at the ﬁnal end point P3 from. The curve would not normally
pass through either control point. The cubic B´ezier is deﬁned
as:

Cbez3(t, θ) =(1 − t)3P0 + 3(1 − t)2tP1

+ 3(1 − t)t2P2 + t3P3

(B.2)

θ = [P0|P1|P2|P3] .

Catmull-Rom Splines parameterise a curve by 4 points
which the curve passes through smoothly. The curve is only
drawn between the middle pair of points:

Ccrs(t, θ) =

t2 − t
t2 − t1

B1 +

t − t1
t2 − t1

B2

(B.3)

B1 =

B2 =

A1 =

A2 =

A3 =

t2 − t
t2 − t0
t3 − t
t3 − t1
t1 − t
t1 − t0
t2 − t
t2 − t1
t3 − t
t3 − t2

A1 +

A2 +

P0 +

P1 +

P2 +

t − t0
t2 − t0
t − t1
t3 − t1
t − t0
t1 − t0
t − t1
t2 − t1
t − t2
t3 − t2

A2

A3

P1

P2

P3

t0 = 0

ti+1 = ||Pi+1 − Pi||α

2 + ti

and

θ = [P0|P1|P2|P3] .

The centripetal Catmull-Rom spline sets α to 0.5, which
has the advantage that cusps or self-intersections cannot be
formed in the curve.

12

Algorithm 1: Polyline approximation for the closest point on a curve. This approximation breaks the curve into
segments uniform-∆t line segments, and might be sub-optimal in areas of high curvature (if such areas were to exist,
then an adaptive variant of this algorithm could instead be used).

Function MinDistanceToCurvePolyline(θ, C, n, segments)

Data:

θ: curve parameters.
C: function deﬁning coordinates of curve at a distance 0 ≤ t ≤ 1 along it.
n: coordinate to compute distance from.
segments: number of line segments to use in the approximation.
Result: the square of the minimum distance between n and the curve.

mindist ← ∞
for ( i = 1; i ≤ segments ; i = i + 1 ) {

t0 ← (i − 1) / segments
t1 ← (i) / segments
dist ← d2
if dist < mindist then
mindist ← dist

seg(n, C(t0, θ), C(t1, θ)) // See eq. (4)

return mindist

C. Computing the Squared Euclidean Distance

Transform for a curve

In general, it is not possible to write a closed form expres-
sion for the (squared) distance of an arbitrary point, n to the
closest point on a curve, C(t),

d2
cur(n) = min

||C(t) − n||2
2

t
s.t. 0 ≤ t ≤ 1 .

(C.1)

Potential approaches to computing this would for example
be through a polyline approximation (see algorithm 1), a
recursive brute force search (see algorithm 2) or a method
based on ﬁnding the roots of the polynomial given by the
derivative

d
dt

||C(t) − n||2
2 .

(C.2)

In the latter case, the root-ﬁnding itself could be achieved
in several ways; for example, by computing the real eigenval-
ues of the companion matrix formed from eq. (C.2) that lie
between 0 and 1 and selecting the one that gives minimum
distance, or by locating two values of t that give oppos-
ing signs of eq. (C.2) and applying the bisection method.
Another potential alternative is the method proposed by Li
et al. [17] which uses bisection with the Newton-Raphson
method, with the initial guess computed using isolator poly-
nomials [27].

The challenge of all the latter approaches is efﬁcient vec-
torised batch implementation, whereby computation of dis-
tance transforms (the computation of the minimum distance
to a curve for all points in the image space) is performed for
a batch of curves in parallel making efﬁcient use of many-
core hardware. An approach based on root ﬁnding using

13

the real eigenvalues of the companion matrix, for example,
should ultimately prove to be more accurate than a poly-
line approximation, and potentially better and faster than
the brute-force search, however at the time of writing there
are not any hardware optimised batch generalised Eigen-
decomposition (GEVD) implementations available; a batch
GEVD implementation (for small matrices) is necessary as
the decomposition would have to be computed for every
pixel n ∈ I.

Currently, we have proof-of-concept implementations
using the former polyline approximation and brute force ap-
proaches, and these are both vectorised to run on many-core
(particularly GPU) hardware. The polyline approximation
is in general faster (obviously both the polyline and brute
force approaches allow the degree of precision to be adjusted,
and that changes the computational complexity), but it does
have a potential disadvantage that the approximation can
introduce degeneracies whereby a small change in a curve’s
parameters cause a topological change in the polyline ap-
proximation. In practice, however, we have not found this to
be a problem in all our experiments with handwritten char-
acters, which all use a 10-segment polyline approximation
for each curve segment that is drawn.

D. Composition functions

The soft-or composition operator (eq. (10)) deﬁned in
section 3.3 provides a good model of drawing with an in-
strument like a black ink pen, where overlapping strokes
are not visible. Such a function is invariant to the order of
the strokes. We might however consider alternative drawing
functions that enable different effects and models of draw-

Algorithm 2: Recursive brute-force search for the closest point on a curve. This is approximate in the sense that if
slices is too small the wrong minima might be located, and that iters controls the precision of the solution that is
found.

Function MinDistanceToCurveBruteForce(θ, C, n, tmin, tmax, iters, slices, mindist=∞)

Data:

θ: curve parameters.
C: function deﬁning coordinates of curve at a distance 0 ≤ t ≤ 1 along it.
n: coordinate to compute distance from.
tmin: starting value of t for the search.
tmax: ending value of t for the search.
iters: number of iterations to perform.
slices: number of intervals between tmin and tmax to compute the distance at.
mindist: current minimum distance estimate.

Result: the square of the minimum distance between n and the curve.

if iters ≤ 0 then

return mindist

∆t ← (tmax − tmin) / slices
t ← tmin
tbest ← tmin

repeat

dist ← ||C(t, θ) − n||2
2
if dist < mindist then
mindist ← dist
tbest ← t

t ← t + ∆t
until t ≥ tmax;
return MinDistanceToCurveBruteForce(θ, C, n, tbest − ∆t, tbest + ∆t, iters−1, slices, mindist)

ing and blending, to be achieved. Here we discuss a few
potential options, including the over operator used for our
colour drawing examples. Note the focus here is on drawing
opaque colours; compositions for colour with transparency
are discussed in appendix E.3.

D.1. The over composition operator

The ﬁrst potential alternative approach to the soft-or
would be to deﬁne a composition that respects the order-
ing of the images and ‘paints’ each stroke over the top of
the other (whilst not allowing background 0 pixels to cover
already ﬁlled pixels) from the background to the foreground.
Taking inspiration from Porter and Duff [23]’s methods for
alpha composition of computer graphics we could deﬁne a
composition of image A painted over image B as:

cover(A, B) = A + B(1 − A) .

(D.1)

This function could then be applied recursively over a se-
quence of depth-ordered rasterisations to compose in the
desired way:

This type of approach does however have a signiﬁcant prob-
lem in terms of implementation: because it is recursive and
sequential, it is not easily vectorised and introduces a sig-
niﬁcant processing bottleneck which makes it intractable
to use with large numbers of images. This problem can be
circumvented by rewriting5 eq. (D.2) as follows,

cover(I (1), . . . , I (n)) =

n
(cid:88)

i=1

i−1
(cid:89)

I (i) (cid:12)

(1 − I (j)) . (D.3)

j=1

In this form, we can see that in essence the computation
required consists of the calculation of the cumulative product
of a difference, a multiplication, and a summation; all of
which can be efﬁciently vectorised. For numerical stability,
the cumulative product can be computed as the exponentiated
sum of the log differences,

cover(. . . ) =

n
(cid:88)

i=1

I (i) (cid:12) exp





i−1
(cid:88)



log(1 − I (j))

 .

j=1

(D.4)

cover(I4, cover(I3, cover(I (2), I (1)))) .

(D.2)

5This was ﬁrst noted by Sintorn and Assarsson [28] for Porter and Duff’s

over operator with an alpha channel (see also appendix E.3).

14

The inner summation can easily be implemented using the
cumsum operator built into most tensor processing libraries;
note, however, that standard implementations will likely in-
clude cumulative sum up to and including the i-th image, so
this must then be subtracted to give the required value. Addi-
tional care must also be taken to avoid taking the logarithm
of zero; in practice adding a small epsilon value sufﬁces.

D.2. The max composition operator

Another possible alternative composition would be to

take the per-pixel maximum over the set of images:

cmaxi,j (I (1), I (2), . . . , I (n))

stroke (either independently or together) by appropriately
parameterising the model.

Real drawings sometimes exhibit a variation in stroke
width along the length of a stroke; often this is a result of
variations in pressure on the drawing instrument. It is pos-
sible to incorporate such variation into our drawing model by
noting that our functions for both line segments and curves
have a parameter 0 ≤ t ≤ 1 along their length that can be
used as an input to a function that produces different values
of σ along the length of the line (or equivalently we can
modify the distance map). Such a function could be para-
meterised by e.g. a simple neural network, and thus learned
during the training or optimisation of a model.

= max(I (1)

i,j , I (2)

i,j , . . . , I (n)

i,j ) .

(D.5)

E.2. Colour

Clearly this does not have usable gradients because of the
max, however, a suitable differentiable relaxation exists with
the smoothmax function,

smoothmax(x) = softmax(x/τ )(cid:62)x ,

(D.6)

where x is a vector of values to ﬁnd the maximum of, and τ
is a temperature parameter. As τ → 0, smoothmax(x) →
max(x). Equation (D.6) can be applied pixel-wise over a
vector formed from the stacking of [I (1)
i,j ] to
form a composition function:

i,j , . . . , I (n)

i,j , I (2)

csmoothmaxi,j(I (1), I (2), . . . , I (n))

= smoothmax([I (1)

i,j , I (2)

i,j , . . . , I (n)

i,j ](cid:62)) .

(D.7)

E. Extended Drawing

The main body of this paper focused on the act of draw-
ing strokes in a differentiable manner and did not explore
extensions to the model that would allow for more nuanced
drawing — for example in colour, and with different types
of stroke. We demonstrate here how the framework we have
already described can be extended to allow for more control
over the drawings that are produced.

E.1. Stroke width

Appendix A demonstrates that there is a direct relation-
ship between stroke thickness and the σ parameter used by
the rasterisation function. In all the experimental results
shown, we used the same ﬁxed σ for all strokes, although it
should be immediately evident that this isn’t a requirement,
and that different strokes could have different σ values, and
thus different thicknesses.

Going further, the σ value doesn’t have to be a hyper-
parameter of the model; without changing anything within
the rasterisation approach it is evident that one can compute
gradients with respect to σ for every stroke that is drawn. As
such, it is entirely possible to learn the line thickness of each

15

Different shades of grey for individual strokes can be
achieved by scalar multiplication of each stroke’s raster with
a grey-value before composition (note that soft-or would no
longer necessarily be appropriate, so a different composition
would likely be used). The grey-value could be learned or
be a hyperparameter.

To rasterise full-colour strokes, the simplest approach
is to replicate the image for a rasterised stroke three times
in the channel dimension, and then multiply by a tuple of
values corresponding to the desired red, green, and blue
values. Again, the parameters can be learned, as is illus-
trated in ﬁg. VII, which uses the over composition operation
(eq. (D.4)).

If we want to rasterise lines along which the colour
changes, we can follow the same methodology for chan-
ging stroke width and learn functions that emit colour as a
function of the relative position, t, along the stroke.

E.3. Incorporating Transparency

The differentiable rasterisation approach for colour de-
scribed above can also be extended to deal with transparency.
If we assume a pre-multiplied alpha colour model, where
the the red, green and blue values of a pixel represent emis-
sion, and the alpha value represents occlusion, then we can
directly use Porter and Duff [23]’s compositing arithmetic.
For example, the over operator with alpha,

co = ca + cb(1 − αa)
αo = αa + αb(1 − αa) ,

(E.1)

allows for models that can learn appropriate colour and trans-
parency for each stroke drawn.

F. Additional Optimisation Results

The extended drawing operators described in appendix E
can be utilised directly in the optimisation approach de-
scribed and illustrated in section 4.

Optimising Photographs.
In ﬁgs. XI to XXIV we present
additional results on a wide variety of images showing the
result of performing image optimisation with both the MSE
loss and LPIPS(VGG) loss, together with different draw-
ing conﬁgurations. There is not much additional to note
from these examples than was already covered in section 4,
however we reiterate that the LPIPS(VGG) loss is strikingly
good at giving results that capture strong perceptual features.

Optimising Cartoons. Optimising cartoons is not well
suited for the ‘direct optimisation’ setup described in sec-
tion 4. Whilst the rasteriser can be used for this task, there
are considerable inductive biases that would be useful to
incorporate to make optimisation easier. For example, incor-
porating strong priors for initialisation and using a more sens-
ible loss (probably one based on chamfer distance) would
considerably reduce the speed of convergence. It might also
be beneﬁcial to work iteratively adding one curve at a time
(with appropriate modiﬁcations to the loss to allow it to re-
main local). However, as demonstrated in ﬁg. X, optimising
randomly initialised curves (single segment Catmull-Rom
splines) and their widths with the MSE loss works well for a
cartoon raster to vector conversion task. The errors in these
conversions are limited to missing small features — for ex-
ample in the ears. The only optimisation ‘tricks’ required
were to run for enough iterations (the ﬁg. X images were al-
lowed 10000 iterations, although they had converged before
the end) and either using more lines than required, or using
a smaller number and randomly re-initialising any that had
widths learned to be zero (which become invisible) during
the ﬁrst half of the iteration limit. All of these details are
only to overcome the fact that poorly initialised curves (ones
far from any black pixel in the raster along their length) will
naturally have strong gradients forcing them to be removed
by reducing their stroke width to zero.

G. Potential Challenges in Optimisation

As mentioned in section 4 the loss landscape when using
a differentiable rasterister is challenging because of factors
such including considerable permutation symmetry from
being able to draw strokes in either direction, as well as many
local optima. In the case of direct optimisation, the resultant
images can be very sensitive to initialisation; we discuss
this further in the following subsection. For our autoencoder
experiments we found no problems with training, however
in the future we want to explore how different priors (e.g.
preference for long strokes, preference to draw from left to
right), could affect this.

(a) Bart; target raster

(b) Bart; generated vector

(c) Homer; target raster

(d) Homer; generated vector

Figure X: Results of converting cartoon rasters to vectors
using direct optimisation of Catmull-Rom Spline curve seg-
ments with randomly initialised curves and learned stroke
width. 10000 iterations were performed, and for the ﬁrst
5000 iterations any curve with near-zero stroke width was
randomly reinitialised. Zoom-in to better view the differ-
ences and the details of the generated vector versions.

the resultant image. This is in itself not something to worry
about as it just results in a artistically different result, but the
broad perceptual appearance of tone is still preserved. Differ-
ent losses can overcome this sensitivity to an extent however.
As can be seen in ﬁg. XXVb, using the LPIPS(VGG) loss for
example always captures perceptually important directions
in the resultant image, although the initialisation can still
affect the rendition in more localised areas.

H. Autotracing Model Architectures

This section details the model architectures used for the
autotracing autoencoders described in section 5 of the main
paper.

H.1. Encoders

Effect of initialisation. Simple losses like MSE are very
sensitive to initialisation when used to optimise against a
photo. As can be seen in ﬁg. XXVa, initialisations can have
a dramatic effect on the orientation of ‘shaded’ sections of

A series of encoder networks were used for different
In each encoder network architecture, one can
datasets.
control the dimensionality of the latent vector encoding
(latent_size).

16

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XI: Examples of image optimisation (i).

17

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XII: Examples of image optimisation (ii).

18

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XIII: Examples of image optimisation (iii).

19

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XIV: Examples of image optimisation (iv).

20

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XV: Examples of image optimisation (v).

21

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XVI: Examples of image optimisation (vi).

22

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XVII: Examples of image optimisation (vii).

23

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XVIII: Examples of image optimisation (viii).

24

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XIX: Examples of image optimisation (ix).

25

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XX: Examples of image optimisation (x).

26

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XXI: Examples of image optimisation (xi).

27

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XXII: Examples of image optimisation (xii).

28

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XXIII: Examples of image optimisation (xiii).

29

Fixed Width

Learned Width

Black & White

Uniform Colour

Black & White

Uniform Colour

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

s
t
n
i
o
P
0
0
5

s
e
n
i
L
0
0
5

s
e
v
r
u
C
0
0
5

E
S
M

)

G
G
V
(
S
P
I
P
L

Figure XXIV: Examples of image optimisation (xiv).

30

(a) MSE Loss

Figure XXV: Exploring the effect of different random initialisations. Four different random seeds were used for the initial
lines, and the resultant images were created using MSE and LPIPS(VGG) loss. Images directly above/below each other
correspond to the same random seed/initialisation.

(b) LPIPS(VGG) Loss

31

MLP Encoder The encoder network used for MNIST ex-
periments is a simple multi-layer perceptron with default
hidden = 64 and latent size = 64:

Linear(28*28, hidden)
ReLU()
Linear(hidden, latent_size)
ReLU()

CNN Encoder For resized Omniglot experiments (28×28
pixel images), a convolutional encoder network with batch
normalization was used:

Conv2d(in_channels=1, out_channels=64,

kernel_size=3, padding=1, stride=1)

BatchNorm2d(num_features=64)
ReLU()
Conv2d(in_channels=64, out_channels=64,
kernel_size=3, padding=1, stride=1)

BatchNorm2d(num_features=64)
ReLU()
Conv2d(in_channels=64, out_channels=64,
kernel_size=3, padding=1, stride=1)

BatchNorm2d(num_features=64)
ReLU()
Conv2d(in_channels=64, out_channels=64,
kernel_size=3, padding=1, stride=1)

BatchNorm2d(num_features=64)
ReLU()
AdaptiveAvgPool2d(output_size=8)
Flatten()
Linear(4096, latent_size)

StrokeNet Agent Encoder Finally, when comparing
against StrokeNet [39] on scaled MNIST (256 × 256), we
present results replicating Zheng et al.’s AgentCNN en-
coder6.

Conv2d(in_channels=1, out_channels=16,

kernel_size=3, padding=1)

BatchNorm2d(num_features=16)
LeakyReLU(negative_slope=0.2)
Conv2d(in_channels=16, out_channels=16,

kernel_size=3, padding=1)

BatchNorm2d(num_features=16)
LeakyReLU(negative_slope=0.2)
AvgPool2d(kernel_size=2)

Conv2d(in_channels=16, out_channels=32,

kernel_size=3, padding=1)

BatchNorm2d(num_features=32)
LeakyReLU(negative_slope=0.2)

6Code
strokenet

available

at https://github.com/vexilligera/

Conv2d(in_channels=32, out_channels=32,

kernel_size=3, padding=1)

BatchNorm2d(num_features=32)
LeakyReLU(negative_slope=0.2)
AvgPool2d(kernel_size=2)

Conv2d(in_channels=32, out_channels=64,

kernel_size=3, padding=1)

BatchNorm2d(num_features=64)
LeakyReLU(negative_slope=0.2)
Conv2d(in_channels=64, out_channels=64,

kernel_size=3, padding=1)

BatchNorm2d(num_features=64)
LeakyReLU(negative_slope=0.2)
AvgPool2d(kernel_size=2)

Conv2d(in_channels=64, out_channels=128,

kernel_size=3, padding=1)
BatchNorm2d(num_features=128)
LeakyReLU(negative_slope=0.2)
Conv2d(in_channels=128, out_channels=128,

kernel_size=3, padding=1)
BatchNorm2d(num_features=128)
LeakyReLU(negative_slope=0.2)
AvgPool2d(kernel_size=2)

Conv2d(in_channels=128, out_channels=256,

kernel_size=3, padding=1)
BatchNorm2d(num_features=256)
LeakyReLU(negative_slope=0.2)
Conv2d(in_channels=256, out_channels=256,

kernel_size=3, padding=1)
BatchNorm2d(num_features=256)
LeakyReLU(negative_slope=0.2)
AvgPool2d(kernel_size=2)

Flatten()

H.2. Decoders

All our decoder networks, which provide different stroke
parameterisations, have a common structure consisting of
two linear layers followed by ReLU non-linear activation.
For 28 × 28 pixel MNIST and Omniglot experiments, we
used hidden1 = 64 and hidden2 = 256.

Linear(latent_size, hidden1)
ReLU()
Linear(hidden1, hidden2)
ReLU()

This common structure is followed by a sub-network to
produce stroke parameters; this is usually a single linear
layer followed by a tanh function. Speciﬁc details for the
chosen type of primitive parameterisation is as follows:

32

Line. Line decoder outputs the start and end coordinates
of nlines segments. The default number of lines used for
MNIST is 5.

Linear(hidden2, nlines * 4)
Tanh()

PolyLine. This allows us to decode the stroke data to a se-
quence of consecutive points (each deﬁned as p = [px, py]).
Default value of npoints is 16, but it can be varied.

Linear(hidden2, npoints * 2)
Tanh()

PolyConnect. Similar to PolyLine, but instead of decod-
ing to a sequence of consecutive points, it outputs a set of
points joined together by a learned connection matrix. The
network computing npoints 2d coordinates is the same
as in PolyLine and the sub-network computing the upper
triangular part of the connection matrix is:

Linear(hidden2, nlines)
Sigmoid()

where nlines is computed as

int((npoints ** 2 + npoints) / 2)

if we allow single points to be drawn (i.e. compute the diag-
onal of the connection matrix), and as follows, otherwise:

int(npoints * (npoints - 1) / 2)

All possible combinations of lines formed between the set
of npoints are rasterised and shaded by the appropriate
connection weight before composition.

CRS. Decoder that parametrises stroke data as Catmull-
Rom splines with default of nlines = 1 and npoints =
16 control points:

Linear(hidden2, nlines * npoints * 2),
Tanh()

B´ezier. We can also choose to parametrise strokes as
B´ezier curved lines (default nlines = 5) and can specify
the number of segments (default is 1).

Linear(hidden2, 2 * npoints * nlines)
Tanh()

where the number of control points is computed based on
the number of segments:

npoints = (4 + (segments - 1) * 3)

Note that this allows for a connected path which isn’t
necessarily smooth as each segment has independent control
points. It is possible to formulate a version which is smooth
and has fewer (4 + (segments − 1) × 2) control points.

B´ezierConnect. Following the same pattern as PolyCon-
nect, this decodes stroke data to a set of control points for
B´ezier curves. The connection matrix is learned using a
network as described in the paragraph PolyConnect and each
point corresponds to both a curve’s end point and corres-
ponding control point to allow for smooth curve segments.

H.3. Recurrent Decoders

We implemented Zheng et al. [39]’s recurrent model that
at each time step uses two separate CNN networks, one to
encode the target image and one for the previous frame. The
vector encodings are concatenated, decoded to ‘stroke data’
and a new stroke is rendered. The new stroke is then overlaid
on the previous frame. However, this approach proved is
computationally expensive and difﬁcult to train well.

Instead, we used a GRU-based RNN which initially starts
with a projection of a zeroed input and the target image’s
vector encoding as its initial hidden state. The RNN decoder
architecture is as follows:

Linear(output_size, latent_size)
ReLU()
GRU(latent_size, latent_size)
Linear(latent_size, output_size)

where output size can be modiﬁed depending on the
chosen type of primitive parametrisation (e.g. a B´ezier curve
has 4 control points, hence output size = 8). The GRU
model is both trained and evaluated for a predeﬁned number
of time steps (3 in all our experiments), corresponding to the
number of independent strokes produced.

I. MNIST Reconstructions

Figures XXVI and XXVII illustrate the effect of differ-
ent stroke parameterisations of the MNIST dataset. Varying
the number of (L)ines, (S)egments, and (P)oints and intro-
ducing a learned connection matrix between them leads to
distinct approaches to drawing. As depicted in ﬁgs. XXVIg
and XXVIIg, Connect models produce the closest recon-
structions. Likewise, parameterisation using simple B´ezier
curves (ﬁg. XXVIIc) leads to convincing results.

J. Omniglot (28 × 28 pixels) Comparison

Table J.1 shows the effect of different parameterisations
on Omniglot dataset. All the models demonstrate reasonable
generalisation to the test dataset (as measured by MSE) even
though the test alphabets are completely disjoint from the
training/validation ones. Reconstructions of models with
different parametrisations are shown in ﬁg. XXVIII. B´ezier
curves work particularly well, although we note that they
do appear to struggle with forming dots (for example in the
Braille alphabet which can found in the training/validation
sets).

33

(a) Test Samples

(b) Lines(L=5)

(c) PolyLine(P=8)

(d) PolyLine(P=16)

(e) PolyConnect(P=5)

(f) PolyConnect(P=8)

(g) PolyConnect(P=16)

(h) PolyConnect(P=32)

Figure XXVI: MNIST test set samples and reconstructions using different parameterisations of ‘stroke data’: Lines, PolyLine
(i.e. a series of consecutive (P)oints) and PolyConnect (a set of 2d (P)oints joined by a learned connection matrix).

Decoder

Line
PolyConnect
B´ezier
B´ezierConnect
RNNB´ezier
B´ezier*

St

1
1
1
1
10
1

#P

20
16
20
16
16
50

#S

#L

Val

Test

1
-
1
-
1
3

10
-
5
-
1
5

0.0189
0.0127
0.0158
0.0117
0.0152
0.0091

0.0223
0.0151
0.0194
0.0144
0.0181
0.0118

Table J.1: Omniglot validation and test MSE for models
constructed with different parameterisations and architecture
(i.e. recurrent vs single-(St)ep). B´ezier* corresponds to the
model whose reconstructions were shown in ﬁg. 6 and has
hidden1 = 512 and hidden2 = 1024.

K. StrokeNet ScaledMNIST Comparison

The StrokeNet paper [39] describes an evaluation of the
model on scaled-up MNIST characters by comparing per-
formance against a CNN-based classiﬁer trained on the
scaled images, and then evaluated on the reconstructions.
The paper implies that the MNIST characters were just re-
sampled to 256x256, however from analysis of the source
code it can be determined that the scaling procedure was to:
resize the 28x28 characters to 120x120 using bilinear inter-
polation, pad the 120x120 images to 256x256, and change
the contrast by multiplying pixels by 0.6. Although the ori-
ginal rationale for these choices is unclear, we follow exactly
the same procedure for our experiments.

The structure of the classiﬁer model in the paper is not
described beyond it being convolutional with 5-layers, and
no code for this aspect of the experiments was provided. We
thus chose to implement our own classiﬁer as follows:

34

(a) CRS(L=1, P=8)

(b) CRS(L=1, P=16)

(c) B´ezier(L=5, S=1)

(d) B´ezier(L=2, S=2)

(e) B´ezierConnect(P=5)

(f) B´ezierConnect(P=8)

(g) B´ezierConnect(P=16)

(h) B´ezierConnect(P=32)

Figure XXVII: MNIST test set reconstructions (of samples in ﬁg. XXVIa) with curves parametrised as Catmull-Rom splines
(CRS) and B´ezier curves (B´ezier and B´ezierConnect). In both CRS and B´ezier Decoders, we can vary the number of (L)ines,
(P)oints and, respectively, (S)egments. B´ezierConnect allows control over the (P)oints joined by the learned connection matrix.

Conv2d(in_channels=1, out_channels=30,

kernel_size=5, padding=0, stride=1)

ReLU()
Conv2d(in_channels=30, out_channels=15,
kernel_size=5, padding=0, stride=1)

ReLU()
Linear(6000, 128)
ReLU()
Linear(128, 50)
ReLU()
Linear(50, 10)

the results presented in the original paper on the raw scaled
MNIST test dataset (originally reported accuracy is 90.82%,
whereas the above network achieves 98.58%). To compute
the performance of the StrokeNet paper with our classiﬁca-
tion network we take the pretrained model weights provided
by the StrokeNet authors and use them to generate recon-
structions of the scaled MNIST test set, which are then fed
to the classiﬁer network to make predictions from. Again we
found considerably higher performance than was originally
reported, as detailed in the main paper.

We did not use any form of regularisation or dropout during
training. The network was trained for 10 epochs using the
Adam optimiser with a learning rate of 0.001 and PyTorch’s
CrossEntropyLoss which incorporates the Softmax ac-
tivation. This network performs considerably better than

L. Additional Autoencoders Results

We provide results of autotracing autoencoders tested on
additional datasets: Japanese handwritten characters [5] and
human quick drawings.

35

(a) Test Samples

(b) Lines(L=10)

(c) PolyConn(P=16)

(d) B´ezier(L=5,S=1)

(e) B´ezierConn(P=16) (f) RNNB´ezier(St=10)

Figure XXVIII: Omniglot test set samples and reconstructions using different parameterisations of ‘stroke data’.

L.1. KMNIST (28 × 28 pixels)

L.2. QuickDraw (128 × 128 pixels)

Decoder

Line
PolyLine
PolyConnect
CRS
B´ezier
B´ezierConnect
RNNB´ezier

St

1
1
1
1
1
1
10

#P

#S

#L

Test

Acc. %

20
16
16
16
217
16
16

1
15
-
14
10
-
1

10
1
-
1
7
-
1

0.0431
0.0654
0.0282
0.0635
0.061
0.0249
0.0496

87.2
75.06
89.09
76.2
82.07
90.15
80.19

Table L.1: KMNIST test MSE and classiﬁcation accuracy
(with a classiﬁer trained on the un-encoded training set) for
models constructed with different parameterisations.

Table L.1 shows a comparison between different paramet-
risations performed on KMNIST [5], the Japanese Hiragana
dataset. We provide test MSE and the classiﬁcation ac-
curacy of the drawn sketches. Samples of test reconstruc-
tions using different decoders are shown in ﬁg. XXIX. The
B´ezierConnect model reaches the highest accuracy and cre-
ates the closest reconstructions as shown in ﬁg. XXIXg.

7https://github.com/googlecreativelab/

quickdraw-dataset

Next, we presents results of the autotracing experiment
run on the Yoga class of QuickDraw7, a 50 million human
drawing dataset across 345 image categories. We used 70000
doodles of yoga poses and split them so that the test, val-
idation and train subsets were disjoint. Table L.2 shows
validation and test MSE for different parametrisations. Fig-
ure XXX illustrates reconstructions of test samples for the
different models. As seen before, learning the connections
between points leads to the best results and produces the
most similar reconstructions (ﬁgs. XXXd and XXXg).

Decoder

Line
PolyLine
PolyConnect
CRS
B´ezier
B´ezierConnect
RNNB´ezier

St

1
1
1
1
1
1
10

#P

#S

#L

Val

20
16
16
16
50
16
16

1
15
-
14
3
-
1

10
1
-
1
5
-
1

0.086
0.101
0.063
0.100
0.0766
0.049
0.0844

Test

0.080
0.092
0.062
0.091
0.070
0.048
0.076

Table L.2: QuickDraw validation and test MSE for models
constructed with different parameterisations.

36

(a) Test Samples

(b) Lines(L=10)

(c) PolyLine(P=16)

(d) PolyConnect(P=16)

(e) CRS(L=1, P=16)

(f) B´ezier(L=7, S=10)

(g) B´ezierConnect(P=16)

(h) RNNB´ezier(St=10)

Figure XXIX: KMNIST test set samples and reconstructions using different parameterisations of ‘stroke data’.

37

(a) Test Samples

(b) Line(L=10)

(c) PolyLine(P=16)

(d) PolyConnect(P=16)

(e) CRS(L=1, P=16)

(f) B´ezier(L=5, S=3)

(g) B´ezierConnect(P=16)

(h) RNNB´ezier(St=10)

Figure XXX: QuickDraw test set samples and reconstructions using different parameterisations of ‘stroke data’. Learning the
connections between points leads to the most similar reconstructions (ﬁgs. XXXd and XXXg).

38

