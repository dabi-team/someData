2
2
0
2

g
u
A
5
2

]

G
L
.
s
c
[

1
v
7
8
1
2
1
.
8
0
2
2
:
v
i
X
r
a

JAXFit: Trust Region Method for Nonlinear Least-Squares Curve Fitting on the GPU

Lucas R. Hofer,∗ Milan Krstaji´c, and Robert P. Smith

Clarendon Laboratory, University of Oxford, Parks Road, Oxford OX1 3PU, United Kingdom
(Dated: August 26, 2022)
Abstract
We implement a trust region method on the GPU for nonlinear least squares curve ﬁtting problems using a new deep
learning Python library called JAX. Our open source package, JAXFit, works for both unconstrained and constrained curve
ﬁtting problems and allows the ﬁt functions to be deﬁned in Python alone—without any specialized knowledge of either the
GPU or CUDA programming. Since JAXFit runs on the GPU, it is much faster than CPU based libraries and even other
GPU based libraries, despite being very easy to use. Additionally, due to JAX’s deep learning foundations, the Jacobian in
JAXFit’s trust region algorithm is calculated with automatic diﬀerentiation, rather than than using derivative approximations
or requiring the user to deﬁne the ﬁt function’s partial derivatives.

I.

INTRODUCTION

Nonlinear least squares regression (NLSQ), colloqui-
ally known as curve ﬁtting, is an essential tool in many
ﬁelds and is used to determine the set of parameters
which cause a model function to best match observational
data. If the model function represents the data well, the
extracted parameters often allow useful information to
be recovered from the observed data. For example, im-
ages of laser beams are often ﬁt with 2D Gaussians to
measure the beams’ size [1, 2] and in biomedicine NLSQ
regression is used to determine the location of cells in
microscopy images [3].

Unlike linear least squares regression, NLSQ regres-
sion cannot be solved analytically and numerical methods
must be used instead. Algorithms for performing NLSQ
regression have existed for centuries with Gauss im-
proving on Newton’s method to give the Gauss-Newton
method [4]. Levenberg [5]—and later Marquardt [6]—
added a damping term to the Gauss-Newton method to
form the Levenberg-Marquardt method. By reformulat-
ing the Levenberg-Marquardt algorithm as a trust region
problem [7], Mor´e [8] developed a robust NLSQ algorithm
which is still used today. Although the aforementioned
methods address NLSQ regression problems without pa-
rameter constraints, Coleman and Li [9] developed a con-
strained NLSQ regression method, which they further
expanded for large data [10].

The advent of modern computing has greatly enhanced
the utility of these NLSQ regression algorithms and
there are several well established software packages such
as MATLAB’s curvefit toolbox [11], MINPACK [12]
and SciPy optimize [13] which have have been robustly
tested. These packages rely on the central processing unit
(CPU) for all of the computation and when regressing a
relatively small number of data points (e.g. < 104) these
algorithms run quickly; however, regressing a large num-
ber of data points, which is quite common when ﬁtting
image data, can drastically increase the ﬁt time. The

∗ lucas.hofer@physics.ox.ac.uk

advent of graphical processing units (GPUs)—which can
parallelize operations on their many cores—means these
NLSQ regression algorithms can be signiﬁcantly sped up
as most of the computation time is spent on parallelizable
operations such as matrix multiplication.

In the past, programming the GPU required special-
ized knowledge, usually of CUDA, which is likely why
only a few frameworks implement NLSQ regression al-
gorithms on the GPU [14, 15]. Even Gpuﬁt [16], which
allows its Levenberg-Marquardt algorithm to be called
from MATLAB or Python, requires the user to deﬁne
both custom ﬁt functions as well as the function’s par-
tial derivatives with CUDA before rebuilding the entire
project with a C++ compiler.

In this paper we introduce our open source package
JAXFit [17], which implements both unconstrained and
constrained NLSQ regression algorithms on the GPU and
solely in Python by using a relatively new deep learning
package called JAX [18]. JAX executes Python deﬁned
operations on the GPU and additionally has in-built au-
tomatic diﬀerentiation of functions [19], which removes
the need to explicitly deﬁne the ﬁt function’s partial
derivatives. JAXFit is easy to use—working as a drop in
replacement for SciPy’s curve ﬁt function—and achieves
signiﬁcant speed-ups due to the GPU.

The rest of the paper is laid out as follows:

in Sec-
tion II we detail the trust region method algorithm. For
readers interested in the implementation alone, we rec-
ommend skipping to Section III in which the speed-up of
the trust region method using JAX is described. Finally,
in Section IV we evaluate the ﬁtting speed of JAXFit in
comparison with both SciPy and Gpuﬁt.

II. TRUST REGION METHOD ALGORITHM

We use SciPy’s trust region method algorithm—which
is an amalgamation of both Mor´e’s [7, 8] and others’
[9, 20] work—as the basis for our own trust region method
(TRM) implementation. SciPy is the default library for
curve ﬁtting in Python and since, to our knowledge, the
SciPy TRM algorithm has been only partially explained

1

 
 
 
 
 
 
in a blog post [21] by the author, we describe the uncon-
strained TRM algorithm below. JAXFit also implements
SciPy’s adapted version of the Coleman-Li [9, 10] method
for constrained NLSQ regression, but we neglect to detail
it.

We ﬁrst deﬁne a function h(y; x) which relies on
an independent variable y and n ﬁxed parameters
{x1, x2, ..., xn} which comprises a parameter vector x. If
we have v number of observations {yi, zi} which we want
to model with h we ﬁrst deﬁne a residual function ri

ri(x) = h(yi, x) − zi.

(1)

Now, to ﬁnd the parameters x which best approximate
the observed data, the least squares of the residual func-
tions are minimized as follows

min
x

f (x) =

1
2

v
(cid:88)

i

r2
i (x) =

1
2

rT (x)r(x).

(2)

where r(x) is a vector containing the v residual functions.
For linear functions Eq. 2 can be analytically solved; how-
ever, for nonlinear functions numerical methods must be
used and these generally involve iterative steps in which
the parameter vector x is successively updated with a
parameter update vector w.

The problem then is to ﬁnd the parameter update vec-
tor w which minimizes f (x + w). To do this, a quadratic
model of f (x + w) is formed using Taylor’s theorem [20],

f (x + w) ≈ f (x) + ∇f (x)T w +

1
2

wT H(x)w,

(3)

where the expansion has been truncated after the
quadratic term. The gradient ∇f (x) is given by

∇f (x) =

v
(cid:88)

i=1

∇ri(x)ri(x) = J(x)T r(x)

(4)

where the Jacobian J(x) is

J(x) =










∂r1
∂x1
∂r2
∂x1

...

∂r1
∂x2
∂r2
∂x2

...

∂rv
∂x1

∂rv
∂x2

. . .

. . .
. . .
. . .










.

∂r1
∂xn
∂r2
∂xn

...

∂rv
∂xn

The Hessian, H(x), in Eq. 3 is given by

H(x) =

v
(cid:88)

i=1

∇ri(x)∇ri(x)T +

v
(cid:88)

i=1

ri(x)∇2ri(x);

(5)

however, since either the Laplacians or residuals in the
second term of Eq. 5 are often quite small, it is common
in NLSQ algorithms to ignore it and approximate the
Hessian with only the ﬁrst term [20]. Additionally, the
ﬁrst term can be rewritten using the Jacobian such that
the approximate Hessian is H(x) ≈ J T (x)J(x). Finally

2

FIG. 1. Visualization of the trust region method algorithm.
The parameter vector x is iteratively updated to ﬁnd the min-
imum of the function f (x) (solid contour lines, see Eq. 2). (a)
A single update step k in which an approximate quadratic
model ˆm(pk) (dashed contour lines) of the function is mini-
mized within a trust region of radius ∆ (dotted line) to deter-
mine the parameter update vector pk (see Algorithm 1). (b)
Multiple update steps—with their respective trust regions—
showing the convergence of the algorithm from the seed pa-
rameter vector x0 to the ﬁnal parameter vector xf at the
minimum of f (x). Although these trust region sizes are all
the same, they will often diﬀer in practice.

Eq. 3 is rewritten such that x is implicit in the various
terms

m(w) = f + gT w +

1
2

wT Bw,

(6)

where m(w) ≈ f (x + w), f = f (x) is the function,
g = ∇f (x) is the gradient and B = J T (x)J(x) is the
approximate Hessian.

A parameter update w is now needed such that the
quadratic model in Eq. 6 is minimized. Since, the model
is only good as a local approximation, a trust region ra-
dius ∆ is deﬁned within the parameter space such that

min
w
s.t.

m(w) = f + gT w +

1
2

wT Bw

(7)

(cid:107)Dw(cid:107) ≤ ∆

where D is a—usually diagonal—scaling matrix used to
make the trust region an ellipsoid rather than a sphere
as the diﬀerent parameters {x1, x2, ..., xn} comprising x
can have vastly diﬀerent scalings. However, according to
Mor´e [7], this is equivalent to solving the problem in a
scaled parameter space where the trust region is a sphere
(see Fig. 1a) and the scaled update vector is p = Dw.
In this case Eq. 7 is equivalent to

min
p
s.t.

ˆm(p) = f + ˆgT p +

pT ˆBp

1
2

(8)

(cid:107)p(cid:107) ≤ ∆.

where ˆg = ˆJ T r, ˆB = ˆJ T (x) ˆJ(x) and ˆJ(x) = J(x)D−1
with theˆsymbol denoting the scaled space.

xkxk+1(a)Function f(x)Model m(pk)Trust Region Update Vector pkx0xf(b)Algorithm 1: Overall trust region method
algorithm. The Levenberg-Marquardt (LM)
parameter and update sub-problems are detailed
in Algorithm 2 and Algorithm 3 respectively.
xk ← x0;
∆k ← ∆0;
while not accuracy condition do

pt ← − ˆB−1ˆg;
if (cid:107)pt(cid:107) ≤ ∆k then

pk ← pt;

else

αk ← LM parameter, Algorithm 2;
(cid:17)−1

pk ←

(cid:16) ˆB + αkI

ˆg;

end
xk+1, ∆k+1 ← Update, Algorithm 3 (∆k, xk, pk);

end

Algorithm 2: Solving for the
Levenberg-Marquardt parameter α when the
update vector p lies on the trust region radius.
αq ← α0;
uq ← (cid:107)ˆg(cid:107)
∆ ;
lq ← − φ(α0)
φ(cid:48)(α0) ;
while |φ(αq)| ≤ σ∆ do
(cid:18) φ(αq)+∆

(cid:19)

αt ← αq −

∆

(cid:19) (cid:18) φ(αq)
φ(cid:48)(αq)

;

if lq ≤ αt ≤ uq then
αq+1 ← αt;

else

αq+1 ← max

(cid:110)

0.001uq, (lquq)

1
2

(cid:111)

;

end

(cid:26)

lq+1 ← max

lq, αq −

(cid:27)

;

φ(αq)
φ(cid:48)(αq)

if φ (αq) < 0 then
uq+1 ← αq;

else

uq+1 ← uq;

end

end

Algorithm 3: Determining the updated
parameter vector xk+1 and trust region radius
∆k+1.
wk ← D−1pk;
if γ(xk, wk) > 0.75 then
xk+1 ← xk + wk;
∆k+1 ← 2∆k;

Mor´e and Sorensen [7] showed that the parameter up-
date p which is a solution to Eq. 8 is also the solution to
the following

(cid:16) ˆB + αI

(cid:17)

p = −ˆg

α (∆ − (cid:107)p(cid:107)) = 0
(cid:16) ˆB + αI

(cid:17)

is positive semideﬁnite

(9)

(10)

where I is the identity matrix and the Levenberg-
Marquardt parameter is a scalar α ≥ 0. This is the
problem we solve.

A basic outline of the TRM algorithm (see Algo-
rithm 1) can now be given. After being seeded with an
initial parameter vector x0 and initial trust radius ∆0,
the parameter vector is iteratively updated for each step
k using Eq. 9-10 until a given accuracy is reached (see
Fig. 1b). A trial update step with α = 0 is ﬁrst at-
tempted for which pt = − ˆB−1ˆg and if the trial update
vector is within the trust region ((cid:107)pt(cid:107) ≤ ∆) then it is ac-
cepted as the parameter update vector. However, if it lies
outside the trust region then the trial parameter update
vector must be discarded as in actuality α > 0. Further-
more, in this case, the Levenberg-Marquardt parameter
α must then be explicitly solved for as a sub-problem.

To solve for α, we follow Mor´e’s [8] numerical method
in which an initial guess α0 = 0 is iteratively updated
Since, according to
with q steps (see Algorithm 2).
Eq. 10, the parameter update vector lies on the trust
region radius ((cid:107)p(cid:107) − ∆ = 0) when α > 0, we iteratively
update αq until p lies within a distance σ∆ of the trust
radius—where σ is the numerical accuracy. Formally this
is |φ(α)| ≤ σ∆ where φ(αq) = (cid:107)p(cid:107) − ∆. Solving Eq. 9
for p allows φ(αq) to be rewritten as

φ(αq) =

(cid:13)
(cid:16) ˆB + αqI
(cid:13)
(cid:13)
(cid:13)

(cid:17)−1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

ˆg

− ∆.

(11)

Using Eq. 11 and φ(cid:48) (αq), the derivative of φ (αq) with
respect to αq, the update to the Levenberg-Marquardt
parameter is given by

αq+1 = αq −

(cid:18) φ (αq) + ∆
∆

(cid:19) (cid:18) φ (αq)
φ(cid:48) (αq)

(cid:19)

(12)

subect to the condition that lq ≤ αq+1 ≤ uq, where lq and
uq are the lower and upper bounds [8] respectively (see
Algorithm 2). If the update lies outside these bounds, it
is instead given by

When given a reasonable initial value for α0 this method
for determining the Levenberg-Marquardt parameter
generally converges to a solution within a few iterations.
This solution, in conjunction with Eq. 9, is then used to
calculate the scaled update vector p and thereafter the
unscaled parameter update vector w = D−1p.

αq+1 = max

(cid:110)

0.001uq, (lquq)

(cid:111)

1
2

.

(13)

else if 0.25 ≤ γ(xk, wk) ≤ 0.75 then

xk+1 ← xk + wk ;
∆k+1 ← ∆k;

else

xk+1 ← xk;
∆k+1 ← 0.25 (cid:107)pk(cid:107);

end

3

Once the parameter update vector has been found, it is
important to determine how much it improves the accu-
racy and whether w should be accepted as an update to
the parameter vector x. To do this, the following equa-
tion is used [20]

γ(x, w) =

f (x) − f (x + w)
m(0) − m(w)

(14)

which gives the ratio of the actual reduction over the pre-
dicted reduction. If γ(x, w) is larger than a set threshold
value, then the update w is accepted and the trust re-
gion radius is either kept the same or increased (see Algo-
rithm 3). However, if γ(x, w) is lower than the threshold
value then the parameter vector is kept the same and the
size of the trust region radius is decreased.

IMPLEMENTING THE TRUST REGION AL-

III.
GORITHM WITH JAX

Our goal when designing JAXFit was to have a fast,
yet easy to use NLSQ regression algorithm. We there-
fore decided to use Python as it is a popular data analy-
sis language featuring several libraries which implement
array operations on the GPU. Python’s popularity as a
data analysis language is due in large part to open source
packages such as NumPy [22], SciPy [13] and Pandas [23].
NumPy provides eﬃcient routines for array operations
on the CPU, including an extensive set of linear alge-
bra functions, whereas SciPy complements NumPy with
a large set of scientiﬁc functions and algorithms.

Although SciPy features NLSQ ﬁtting, its original—
and currently default—implementation simply wraps
MINPACK’s [12] Fortran Levenberg-Marquardt algo-
rithm for unconstrained NLSQ problems. Nikolay May-
orov [21] signiﬁcantly improved SciPy’s curve ﬁtting code
with a more modern approach by writing a trust region
algorithm for both unconstrained (based on More [7, 8],
see Section II) and constrained NLSQ problems [9] solely
in Python.

Unlike NumPy for CPU array operations, a similar uni-
fying Python library for array operations on the GPU
has not been established. We decided to use a relative
newcomer, JAX, for GPU array operations—which since
its release by Google in 2018 has seen rapid adoption by
both the machine learning and scientiﬁc computing com-
munities. JAX describes itself as “NumPy on the GPU”
and it implements almost all of NumPy’s functionality
and a signiﬁcant portion of SciPy’s on the GPU.

In addition to running on the GPU, JAX also speeds
up operations through use of Just in Time (JIT) com-
piliation in which JIT wrapped Python functions are
compiled to optimized machine code at run time. When
coupled with a GPU this massively speeds-up execution
times as JAX converts the JIT wrapped functions to
highly optimized Accelerated Linear Algebra (XLA) code
[18] which runs eﬃciently on the GPU hardware. Thus,
JAX allows code to be written in a high-level language

4

(Python), but with the performance of a hardware spe-
ciﬁc language (XLA).

Although for large array operations JAX is signiﬁ-
cantly faster than NumPy, it is often slower for small
array operations due to the overhead of transferring data
to the GPU. Therefore, we only move large array oper-
ations (array sizes ≥ v, where v is the number of data
points being ﬁt) to JAX and the GPU, while leaving the
small array operations to NumPy and the CPU. The op-
erations we run with JAX include calculating the func-
tion f (x), the gradient ∇f (x), the quadratic model m(w)
and the loss function.

We also to rely on JAX to calculate the Jacobian J(x)
and to perform singular value decomposition (SVD, see
Appendix B) of the Jacobian matrix—which is a robust
method for solving the linear least squares problem in
Eq. 9 [24]. When calculating the Jacobian, we use JAX’s
in-built automatic diﬀerentiation [25] rather than either
requiring the user to provide partial derivatives (as in
Gpuﬁt) or using approximations to the partial derivatives
(the SciPy default).

Even though JAX is both fast and easy to use, its
dynamism comes at a price. When creating optimized
machine code, JAX traces out a computational graph of
the function using abstract tracer arrays for the function
inputs—which can be computationally expensive. How-
ever, the computational overhead pays oﬀ when the code
is repeatedly called.

The functional style of programming used in SciPy’s
TRM code caused signiﬁcant retracing to occur and to
avoid this we incorporated the TRM algorithm into a
Python object. The object’s JAX functions are traced
only when ﬁrst called and successive calls to the ob-
ject’s curve ﬁt function run without tracing. Addition-
ally, since changing the size of a JIT function’s input
array also causes retracing to occur, we developed an ar-
ray zero-padding method which ﬁxes the size of the JIT
functions’ input arrays (see Appendix A).

IV. FIT PERFORMANCE COMPARISON

We compared the ﬁt speed of JAXFit against both
SciPy and Gpuﬁt by creating a simulated dataset. The
dataset is comprised of 2D rotated, elliptical Gaussians
(see Figure 2) which feature seven ﬁtting parameters [26].
We chose ﬁfteen data lengths (number of residuals v)
evenly distributed between v = 104 and v = 8 × 106 on
a logarithmic scale and simulated ﬁfty-one 2D Gaussian
images for each data length. The seven ﬁtting parameters
for each 2D Gaussian image were drawn from a uniform,
random distribution. Gaussian distributed noise was also
added to each image with a mean of zero and a ﬁxed
standard deviation for all the images in the dataset.

JAXFit and SciPy use the same TRM algorithm which
enables us to compare the speed of the large array
operations—which run on the GPU and CPU for JAXFit
and SciPy respectively. The dataset’s simulated Gaus-

FIG. 2. Dataset sample images. (a) Image with the smallest
data length, v = 104. (b) Image with the largest data length,
v = 8×106. Although the simulated Gaussian noise in (b) has
the same standard deviation as the noise in (a), it is visually
smoothed out due to the large number of data points.

sian images were ﬁt for each data length in turn using
both JAXFit and SciPy. We then averaged the ﬁt speed
for each of the ﬁfteen data lengths (see Fig. 3) to show
the data length; however, for
the operation speed vs.
each data length the ﬁrst ﬁt (tracing) was not included
in the time average as tracing can generally be avoided
(see Appendix A).

As expected, the JAXFit operations—–including the
function evaluations f (x), calculation of Jacobians J(x),
SVD of the Jacobian matrix, calculation of the gradient
∇f (x) and evaluation of the model function m(w)—were
signiﬁcantly faster than SciPy’s. Although the JAXFit
operations generally scaled linearly with the data length,
a few of the operations including SVD and the gradient
calculation showed a ﬂat region for small data lengths.
This is likely due to not all the GPU resources being
utilized. However, since the CPU resources are gener-
ally fully utilized, almost all the SciPy operations scale
linearly—irrespective of the data length.

In addition to the computation time, we measured the
time required to move the ﬁt data from the CPU to the
GPU (see Fig. 3a), as well as the time to transfer data
back from the GPU to CPU (see Fig. 3b). As expected,
the CPU to GPU time scales linearly with the size of the
data being transferred, whereas the GPU to CPU trans-
fer time is roughly constant—since the size of the arrays
transferred back are solely dependent on the number of
ﬁtting parameters n.

We also perform an overall ﬁt speed comparison of
JAXFit, SciPy and Gpuﬁt for unconstrained NLSQ re-
gression (see Fig. 4). Using each method we measured
the overall ﬁt time for all the simulated Gaussian images
in the dataset. We again ignored the ﬁrst image for each
data length and averaged the remaining ﬁfty ﬁt times.
As expected, JAXFit and Gpuﬁt are much faster than
SciPy due to the GPU vs. CPU hardware advantage.
When comparing the GPU methods, Gpuﬁt
is
slightly faster than JAXFit if the data length is small

5

FIG. 3. Comparison of SciPy vs. JAXFit (JAX) for individ-
ual operations. (a) Speed of the more computationally expen-
sive operations—including function evaluation f (x), Jacobian
calculation J(x) and singular value decomposition (SVD) of
the Jacobian—as a function of the data length. Additionally,
the time it takes to transfer ﬁt data from the CPU to the
GPU is shown.
(b) Other operations, including the gradi-
ent calculation ∇f (x) and evaluation of the quadratic model
m(w) as a function of the data length. The summative CPU
to GPU array transfer time for one iteration step within the
trust region method algorithm is also shown.

(v < 3 × 104), as its Levenberg-Marquardt algorithm
runs entirely in CUDA on the GPU. However, as the data
length increases, this advantage quickly disappears and
JAXFit signiﬁcantly outperforms Gpuﬁt when the data
length is large—almost an order of magnitude faster for
v > 106. Since the number of iterations it takes the al-
gorithms in JAXFit and Gpuﬁt to converge is roughly
equal when using the same ﬁt data, JAXFit’s better per-
formance is likely due to the highly optimized XLA li-
brary underpinning JAX, which uses the GPU hardware
resources more eﬃciently.

Finally, we note that these experiments were all run in
a Google Colab notebook featuring a single 2 GHz Intel
Xeon CPU core and a NVIDIA Tesla V100-SXM2-16GB
GPU—which is already several years old and is roughly

02550751000255075100(a)010002000010002000(b)00.511.5Arbitrary Units103102101100Time (s)(a)SciPy f(x)SciPy J(x)SciPy SVDCPU to GPUJAX f(x)JAX J(x)JAX SVD104105106107Data Length104103102101Time (s)(b)SciPy f(x)SciPy m(w)GPU to CPUJAX f(x)JAX m(w)JAXFit incorporates features not previously described
such as handling ﬁt data uncertainty (see Appendix C),
scaling of the Jacobian and implementation of various
loss functions—see the Github repository for all the avail-
able features [17].

Appendix A: Avoiding Retracing of Dynamic Array

Sizes

JAX needs to retrace a JIT function whenever the size
of the input array changes. Although this isn’t a problem
for neural networks—which it was primarily constructed
for—it can be an issue for curve ﬁtting. For example
in our laboratory, we ﬁt images of cold atom clouds [27]
with 2D Gaussians or 2D parabolas and since the size
of the clouds can be very diﬀerent, we often need to ﬁt
diﬀerent sized data arrays.

To circumvent the JAX retracing process for these dif-
ferent sized arrays, JAXFit allows the user to deﬁne a
ﬁxed input data size s. When the ﬁt data passed to
JAXFit is smaller than this ﬁxed size then the ﬁt data
is padded with dummy data to account for the diﬀer-
ence between the actual data length v and the ﬁxed data
length s.

This does require the user to pick a sensible value for
s. If s (cid:29) v, then signiﬁcant time is wasted processing
the dummy data. On the other hand, if the actual data
size is larger than the ﬁxed input data size (v > s) then
the JAXFit functions must be retraced. Thus, when all
the data being analyzed is roughly the same size, the
optimal ﬁxed size is s = vmax where vmax is the largest
data size being ﬁt. However, if the ﬁt data sizes diﬀer
signiﬁcantly then it can be more eﬃcient to instantiate
multiple curve ﬁt objects—each with a diﬀerent ﬁxed s
to handle the range of data sizes.

Lastly, simply padding the input ﬁt data with dummy
data would lead to incorrect ﬁts. Thus, we’ve added
masking operations (see Listing 1) throughout the TRM
algorithm so that the dummy data doesn’t aﬀect the ﬁt,
but merely operates as a placeholder to keep JAX from
retracing the functions.

import jax.numpy as jnp
import jax.jit as jit

@jit
def masked_jac(coords, args, data_mask)

J = jac_func(coords, args)
J_masked = jnp.where(data_mask, J, 0)
return J_masked

Listing1: Example of masking dummy data within a
JIT wrapped Python function.

FIG. 4. Overall ﬁt speed of JAXFit, SciPy and Gpuﬁt as a
function of the data length. JAXFit (solid-circle) and Gpuﬁt
(dotted-triangle) are much faster than SciPy (dash-square) as
they utilize the GPU rather than CPU. Although Gpuﬁt is
slightly faster for a small data lengths (v < 3 × 104), JAXFit
is signiﬁcantly faster for large data lengths.

equivalent to NVIDIA’s latest series of gaming GPUs
(RTX3080/RTX3090). Although the speed of JAXFit
will depend primarily on the GPU speciﬁcations, even
when using a lower spec’d GPU, JAXFit is much faster
than CPU based NLSQ regression methods.

V. CONCLUSION

We have implemented a trust region method for both
unconstrained and constrained nonlinear least squares
(NLSQ) curve ﬁtting on the GPU using a deep learning
Python library, JAX. Our open source package, JAXFit
[17], allows the user to dynamically deﬁne ﬁt functions in
Python and, unlike other GPU NLSQ libraries, requires
no specialized knowledge of CUDA or the GPU. Further-
more, the partial derivatives which comprise the Jaco-
bian in the trust region algorithm are calculated with
automatic diﬀerentiation—rather than either requiring
the user to deﬁne the partial derivatives or using approx-
imations.

JAXFit signiﬁcantly outperforms SciPy’s CPU based
NSLQ curve ﬁtting function due to its use of the GPU.
Furthermore, JAXFit is easier to use and generally faster
than another GPU ﬁtting library (Gpuﬁt) since it makes
more eﬃcient use of the GPU resources. However, JAX-
Fit’s simplicity comes with the computational overhead
of JAX’s function tracing. Although the initial trace is
unavoidable, JAXFit is generally able to avoid retrac-
ing even when the size of the ﬁt data changes (see Ap-
pendix A) and thus remains very fast.

For ease of use, JAXFit is designed as a drop-in re-
placement for SciPy’s curve ﬁt function. Additionally,

6

104105106107Data Length102101100101Fit Time (s)JAXFitSciPyGpufitAppendix B: Solving Linear Least Squares Equa-

tion with Singular Value Decomposition

where the residual vector is explicitly deﬁned using Eq. 1
and where W is the weighting matrix given by

Singular value decomposition [28] factors any matrix
M which has size v ×n into the product of three matrices

M = U ΣV T

(B1)

where U is a v × v orthonormal matrix, Σ is a v × n
diagonal matrix and V is a n × n orthonormal matrix.
Since both U and V are orthonormal U T U = U U T = I
(with the same expression for V ) where I is the identity
matrix.

The special properties of SVD allow the least squares
problem (see Eq. 9) in the trust region method algorithm
to be solved. We start by rewriting Eq. 9 explicitly in
terms of the scaled Jacobian J as

(cid:0)J T J + αI(cid:1) p = −J T r,

(B2)

where the scaled space’s hat notationˆhas been dropped.
Now using SVD, the Jacobian is J = U ΣV T and its
transpose is J T = V ΣT U T . Multiplying these two to-
gether and using the orthonormality of the matrix U gives
J T J = V ΣT ΣV T . Plugging all this into Eq. B2

W =










1
σ2
11
0
...
0

0
1
σ2
22

...
0

. . .

. . .
. . .
. . .










.

0

0
...

1
σ2

vv

(C2)

However, this diagonal weighting matrix is a special case
and the more general form is W = C −1 [29] where C is
the symmetric covariance matrix given by

C =










12 . . . σ2
σ2
11 σ2
1v
22 . . . σ2
21 σ2
σ2
2v
...
...
...
. . .
v2 . . . σ2
v1 σ2
σ2
vv










.

(C3)

By using Eq. C1 instead of Eq. 2 and working through the
math in Section II the gradient (Eq. 4) and approximated
Hessian are modiﬁed to be

g = ∇f (x) = J(x)T W r(x)

(C4)

(cid:0)V ΣT ΣV T + αI(cid:1) p = V ΣT U T r

(B3)

and

and multiplying from the left on both sides of this equa-
tion by V T yields

(cid:0)ΣT Σ + αI(cid:1) V T p = ΣT U T r.

(B4)

Finally, solving Eq. B4 for p gives

p = V (cid:0)ΣT Σ + αI(cid:1)−1

ΣT U T r

(B5)

which is signiﬁcantly simpler to work with as ΣT Σ + αI
is diagonal and therefore easily invertible.

The SVD of the scaled Jacobian is done once per pa-
rameter update step k (see Algorithm 1), as is calculat-
ing both ΣT Σ and ΣT U T r. Even though Eq. B5 may be
called multiple times within a single update step k when
solving for the Levenberg-Marquardt parameter (see Al-
gorithm 2), the matrices involved in this repeated calcu-
lation are all of size n × n and the operation is therefore
computationally inexpensive.

Appendix C: Dealing with Uncertainty

Sometimes the data points {yi, zi} which are being ﬁt
with the function h(y; x) have diﬀerent weightings/errors
σi which need to be taken into consideration. These er-
rors modiﬁy Eq. 2 to the following form [29]

f (x) =

1
2

v
(cid:88)

i

(cid:20) h(yi, x) − zi
σi

(cid:21)2

=

1
2

rT (x)W r(x)

(C1)

H(x) ≈ B = J T (x)W J(x).

(C5)

respectively. This causes Eq. 9 to have the following form
(cid:2)J T W J + αI(cid:3) p = −J T W r,

(C6)

where, again, the scaled space’s hat notation ˆ has been
removed for readability. The weighting matrix in Eq. C6
seemingly complicates the algorithmic implementation in
Section II; however, we show below that Eq. C6 can be
rewritten in the same form as Eq. 9 thereby keeping the
algorithm the same.

Since by deﬁnition the covariance matrix C is real,
Hermitian and positive-deﬁnite it can be factorized via
Cholesky decomposition as C = LLT where L is a real,
lower triangular matrix. The weighting matrix can then
be deﬁned in terms of this factorization W = C −1 =
(cid:0)LT (cid:1)−1
L−1 and substituted into Eq. C6

L−1 = (cid:0)L−1(cid:1)T

(cid:104)
J T (cid:0)L−1(cid:1)T

L−1J + αI

(cid:105)

p = −J T (cid:0)L−1(cid:1)T

L−1r. (C7)

By using matrix transpose and multiplication rules
Eq. C7 is rewritten as

(cid:104) ˜J T ˜J + αI

(cid:105)

p = − ˜J T ˜r.

(C8)

where ˜J = L−1J and ˜r = L−1r—which successfully gives
us the same form as Eq. 9.

Thus, when the user gives a covariance matrix, the
weighted residual vector ˜r and weighted Jacobian ˜J are
determined before following the algorithm in Section II.

7

To do this, the Cholesky decomposition is ﬁrst performed
to ﬁnd L. Since calculating the inverse of L can be com-
putationally expensive the following two equations are
solved to ﬁnd ˜J and ˜r

L ˜J = J
L˜r = r.

(C9)
(C10)

However, it is rare to use the full covariance matrix
and often only the diagonal elements are known—which
results in the weighting matrix given by Eq. C2. In this
case the inverted form of L is easily found to be

factorized via Cholesky decomposition to yield L. Dur-
ing the iterative part of the algorithm (see Algorithm 1),
the weighted residual vector ˜r must be determined for
each parameter update step k using either Eq. C10 and
(cid:0)S−1(cid:1)T
or Eq. C13 and L—depending on whether S or
C has been given.

The weighted Jacobian ˜J also needs to be calculated
for each parameter update step k. By default JAXFit cal-
culates the Jacobian with automatic diﬀerentiation and
˜J is therefore calculated by simply passing the weighted
residual function ˜r to JAX’s autodiff function. How-
ever, JAXFit also allows users to explicitly deﬁne the Ja-
cobian matrix—in terms of h(y; x)—in which case ˜J must
be solved for explicitly using either Eq. C9 or Eq. C12.

L−1 =










1
σ11
0
...
0

0
1
σ22

...
0

. . .

. . .
. . .
. . .










.

0

0
...

1
σvv

which means ˜J and ˜r can be calculated without having
to solve Eqs. C9-C10. Additionally, this allows vector
rather than matrix multiplication to be used

(C11)

FUNDING

This work was supported by EPSRC Grant Nos.
EP/P009565/1 and EP/TO19913/1, the John Fell Ox-
ford University Press (OUP) Research Fund and the
Royal Society.

˜J = L−1J = (cid:0)S−1(cid:1)T
˜r = L−1r = (cid:0)S−1(cid:1)T

J

r

(C12)

(C13)

ACKNOWLEDGEMENTS

L.R. thanks Oliver Karnbach and Mark Ijspeert for

helpful discussions.

which is a less computationally expensive operation. S
is a vector containing the diagonal elements of L








.

DISCLOSURES

(C14)

The authors declare no conﬂicts of interest. After ﬁn-
ishing this paper, we became aware of a recently released
Levenberg-Marquardt algorithm using JAX [30].

S =








σ11
σ22
...
σvv

Similar to SciPy’s curve ﬁt, JAXFit allows the user
to give either S or C for the ﬁt data error when calling
the curve ﬁt function. At the beginning of the algorithm
(cid:0)S−1(cid:1)T
is calculated if given S, whereas if C is given it is

DATA AVAILABILITY

We make code available in [17].

[1] A. E. Siegman, How to (maybe) measure laser beam qual-
ity, in DPSS (Diode Pumped Solid State) Lasers: Appli-
cations and Issues (Optica Publishing Group, 1998) p.
MQ1.

[2] L. R. Hofer, M. Krstaji´c, and R. P. Smith, Measuring
laser beams with a neural network, Applied Optics 61,
1924 (2022).

[3] M. Bates, B. Huang, and X. Zhuang, Super-resolution
microscopy by nanoscale localization of photo-switchable
ﬂuorescent probes, Current Opinion in Chemical Biology
12, 505 (2008).

[4] C. F. Gauss, Theoria motus corporum coelestium in sec-
tionibus conicis solem ambientium, Vol. 7 (FA Perthes,
1877).

[5] K. Levenberg, A method for the solution of certain non-
linear problems in least squares, Quarterly of Applied
Mathematics 2, 164 (1944).

[6] D. W. Marquardt, An algorithm for least-squares estima-
tion of nonlinear parameters, Journal of the Society for
Industrial and Applied Mathematics 11, 431 (1963).
[7] J. J. Mor´e and D. C. Sorensen, Computing a trust region
step, SIAM Journal on Scientiﬁc and Statistical Comput-
ing 4, 553 (1983).

[8] J. J. Mor´e, The Levenberg-Marquardt algorithm: imple-
mentation and theory, in Numerical analysis (Springer,
1978) pp. 105–116.

[9] T. F. Coleman and Y. Li, An interior trust region ap-
proach for nonlinear minimization subject to bounds,

8

SIAM Journal on Optimization 6, 418 (1996).

[10] M. A. Branch, T. F. Coleman, and Y. Li, A subspace,
interior, and conjugate gradient method for large-scale
bound-constrained minimization problems, SIAM Jour-
nal on Scientiﬁc Computing 21, 1 (1999).

[21] N. Mayorov, Trust region reﬂective algorithm (2015).
[22] C. R. Harris, K. J. Millman, S. J. Van Der Walt, R. Gom-
mers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor,
S. Berg, N. J. Smith, et al., Array programming with
NumPy, Nature 585, 357 (2020).

[11] MATLAB, version 7.10.0 (R2010a) (The MathWorks

[23] W. McKinney, Pandas: a foundational Python library for

Inc., Natick, Massachusetts, 2010).

data analysis and statistics (2011).

[12] J. J. Mor´e, B. S. Garbow, and K. E. Hillstrom, User guide
for MINPACK-1 , Tech. Rep. (CM-P00068642, 1980).
[13] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haber-
land, T. Reddy, D. Cournapeau, E. Burovski, P. Peter-
son, W. Weckesser, J. Bright, et al., SciPy 1.0:
funda-
mental algorithms for scientiﬁc computing in Python,
Nature Methods 17, 261 (2020).

[14] A. L. Gaunt, Degenerate Bose gases:

tuning interac-
tions & geometry, Ph.D. thesis, University of Cambridge
(2015).

[15] X. Zhu and D. Zhang, Eﬃcient parallel Levenberg-
Marquardt model ﬁtting towards real-time automated
parametric imaging microscopy, PloS One 8, e76665
(2013).

[16] A. Przybylski, B. Thiel, J. Keller-Findeisen, B. Stock,
and M. Bates, Gpuﬁt: An open-source toolkit for GPU-
accelerated curve ﬁtting, Scientiﬁc Reports 7, 1 (2017).
[17] L. R. Hofer, M. Krstaji´c, and R. P. Smith, JAXFit,

GitHub (2022).

[18] R. Frostig, M. J. Johnson, and C. Leary, Compiling ma-
chine learning programs via high-level tracing, Systems
for Machine Learning , 23 (2018).

[19] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson,
C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-
derPlas, S. Wanderman-Milne, and Q. Zhang, JAX: com-
posable transformations of Python+NumPy programs
(2018).

[20] J. Nocedal and S. J. Wright, Numerical optimization

(Springer, 1999).

[24] Even though SciPy uses a LSMR algorithm [31] for solv-
ing the least squares problem when the Jacobian matrix
is both large and sparse, JAX does not currently support
most SciPy sparse matrix operations and we therefore
implement only SVD.

[25] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M.
Siskind, Automatic diﬀerentiation in machine learning:
a survey, Journal of Machine Learning Research 18, 1
(2018).

[26] L. R. Hofer, R. V. Dragone, and A. D. MacGregor, Scale
factor correction for Gaussian beam truncation in second
moment beam radius measurements, Optical Engineering
56, 043110 (2017).

[27] L. R. Hofer, M. Krstaji´c, P. Juh´asz, A. L. Marchant,
and R. P. Smith, Atom cloud detection and segmentation
using a deep neural network, Machine Learning: Science
and Technology 2, 045008 (2021).

[28] C. D. Martin and M. A. Porter, The extraordinary SVD,
The American Mathematical Monthly 119, 838 (2012).
[29] H. P. Gavin, The Levenberg-Marquardt algorithm for
nonlinear least squares curve-ﬁtting problems, Depart-
ment of Civil and Environmental Engineering, Duke Uni-
versity 19 (2019).

[30] M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer,
F. Llinares-L´opez, F. Pedregosa, and J.-P. Vert, Eﬃ-
cient and modular implicit diﬀerentiation, arXiv preprint
arXiv:2105.15183 (2021).

[31] D. C.-L. Fong and M. Saunders, LSMR: An iterative al-
gorithm for sparse least-squares problems, SIAM Journal
on Scientiﬁc Computing 33, 2950 (2011).

9

