2
2
0
2

n
u
J

6

]

C
D
.
s
c
[

3
v
8
0
2
9
0
.
1
1
0
2
:
v
i
X
r
a

Whale: Efﬁcient Giant Model Training over Heterogeneous GPUs

Xianyan Jia1, Le Jiang1, Ang Wang1, Wencong Xiao1, Ziji Shi12, Jie Zhang1,
Xinyuan Li1, Langshi Chen1, Yong Li1, Zhen Zheng1, Xiaoyong Liu1, Wei Lin1

1Alibaba Group 2National University of Singapore

Abstract

The scaling up of deep neural networks has been demon-
strated to be effective in improving model quality, but also
encompasses several training challenges in terms of train-
ing efﬁciency, programmability, and resource adaptability.
We present Whale, a general and efﬁcient distributed train-
ing framework for giant models. To support various parallel
strategies and their hybrids, Whale generalizes the program-
ming interface by deﬁning two new primitives in the form
of model annotations, allowing for incorporating user hints.
The Whale runtime utilizes those annotations and performs
graph optimizations to transform a local deep learning DAG
graph for distributed multi-GPU execution. Whale further
introduces a novel hardware-aware parallel strategy, which
improves the performance of model training on heterogeneous
GPUs in a balanced manner. Deployed in a production cluster
with 512 GPUs, Whale successfully trains an industry-scale
multimodal model with over ten trillion model parameters,
named M6, demonstrating great scalability and efﬁciency.

1 Introduction

The training of large-scale deep learning (DL) models has
been extensively adopted in various ﬁelds, including computer
vision [15, 30], natural language understanding [8, 35, 43, 44],
machine translation [17, 26], and others. The scale of the
model parameters increases from millions to trillions, which
signiﬁcantly improves the model quality [8, 24]; but at the
cost of considerable efforts to efﬁciently distribute the model
across GPUs. The commonly used data parallelism (DP) strat-
egy is a poor ﬁt, since it requires the model replicas in GPUs
perform gradient synchronization proportional to the model
parameter size for every mini-batch, thus easily becoming a
bottleneck for giant models. Moreover, training trillions of
model parameters requires terabytes of GPU memory at the
minimum, which is far beyond the capacity of a single GPU.
To address the aforementioned challenges, a series of new
parallel strategies in training DL models have been pro-

posed, including model parallelism (MP) [25], pipeline par-
allelism [20, 32], etc. For example, differing from the DP
approach where each GPU maintains a model replica, MP
partitions model parameters into multiple GPUs, avoiding gra-
dient synchronization but instead letting tensors ﬂow across
GPUs.

Despite such advancements, new parallel strategies also
introduce additional challenges. First, different components
of a model might require different parallel strategies. Consider
a large-scale image classiﬁcation task with 100K classes,
where the model is composed of ResNet50 [19] for feature
extraction and Fully-Connected (FC) layer for classiﬁcation.
The parameter size of ResNet50 is 90 MB, and the parameter
size of FC is 782 MB. If DP is applied to the whole model, the
gradient synchronization of FC will become the bottleneck.
One better solution is to apply DP to ResNet50 and apply
MP to FC (Section 2.3). As a result, the synchronization
overhead can be reduced by 89.7%, thereby achieving better
performance [25].

Additionally, using those advanced parallel strategies in-
creases user efforts signiﬁcantly. To apply DP in distributed
model training, model developers only need to program the
model for one GPU and annotate a few lines, and DL frame-
works can replicate the execution plan among multiple GPUs
automatically [27]. However, adopting advanced parallelism
strategies might make different GPUs process different parti-
tions of the model execution plan, which is difﬁcult to achieve
automatically and efﬁciently [23,46]. Therefore, signiﬁcant ef-
forts are required for users to manually place computation op-
erators, coordinate pipeline among mini-batches, implement
equivalent distributed operators, and control computation-
communication overlapping, etc. [26, 38, 41, 43]. Such an
approach exposes low-level system abstractions and requires
users to understand system implementation details when pro-
gramming the models, which greatly increases the amount of
user effort.

Further, the training of giant models requires huge com-
puting resources. In industry, the scheduling of hundreds of
homogeneous high-end GPUs usually requires a long queuing

1

 
 
 
 
 
 
time. Meanwhile, heterogeneous GPUs can be obtained much
easier (e.g., a mixture of P100 [2] and V100 [3]) [21, 47]. But
training with heterogeneous GPUs efﬁciently is even more
difﬁcult, since both the computing units and the memory ca-
pacity of GPUs need to be considered when building the
model. In addition, due to the dynamic scheduling of GPUs,
users are unaware of the hardware speciﬁcation when building
their models, which brings a gap between model development
and the hardware environment.

We propose Whale, a deep learning framework designed for
training giant models. Unlike the aforementioned approaches
in which the efﬁcient model partitions are searched automat-
ically or low-level system abstractions and implementation
details are exposed to users, we argue that deep learning frame-
works should offer high-level abstractions properly to support
complicated parallel strategies by utilizing user hints, espe-
cially when considering the usage of heterogeneous GPU re-
sources. Guided by this principle, Whale strikes a balance by
extending two necessary primitives on top of TensorFlow [7].
Through annotating a local DL model with those primitives,
Whale supports all existing parallel strategies and their com-
binations, which is achieved by automatically rewriting the
deep learning execution graph. This design choice decouples
the parallel strategies from model code, and lowers them into
dataﬂow graphs, which not only reduces user efforts but also
enables graph optimizations and resources-aware optimiza-
tions for efﬁciency and scalability. In this way, Whale eases
users from the complicated execution details of giant model
training, such as scheduling parallel executions on multiple de-
vices, and balancing computation workload among heteroge-
neous GPUs. Moreover, Whale introduces a hardware-aware
load balancing algorithm when generating a distributed execu-
tion plan, which bridges the gap between model development
and the heterogeneous runtime environment.

We summarize the key contributions of Whale as follows:

1. For carefully balancing user efforts and distributed graph
optimization requirements, Whale introduces two new
high-level primitives to express all existing parallel
strategies as well as their hybrids.

2. By utilizing the annotations for graph optimization,
Whale can transform local models into distributed mod-
els, and train them on multiple GPUs efﬁciently and
automatically.

3. Whale proposes a hardware-aware load balancing al-
gorithm, which is seamlessly integrated with parallel
strategies to accelerate training on heterogeneous GPUs.

4. Whale demonstrates its capabilities by setting a new
milestone in training the largest multi-modality pre-
trained model M6 [28] with ten trillion model parame-
ters, which requires only four lines of code change to
scale the model and run on 512 NVIDIA V100M32
GPUs (Section 5.3.2).

Whale has been deployed as a production system for large-
scale deep learning training at Alibaba. Using heterogeneous
GPUs, further speedup of Bert-Large [13], Resnet50 [19],
and GNMT [48] from 1.2x to 1.4x can be achieved owing
to the hardware-aware load balancing algorithm in Whale.
Whale also demonstrates its capabilities in the training of
industry-scale models. With only four-line changes to a local
model, Whale can train a Multi-Modality to Multi-Modality
Multitask Mega-transformer model with 10 billion parameters
(M6-10B) on 256 NVIDIA V100 GPUs (32GB), achieving
91% throughput in scalability. What’s more, Whale scales
to ten trillion parameters in model training of M6-10T using
tensor model parallelism on 512 V100 GPUs (32GB), setting
a new milestone in large-scale deep learning model training.

2 Background and Motivation

In this section, we ﬁrst recap the background of distributed
DL model training, especially the parallel strategies for large
model training. We then present the importance and the chal-
lenges of utilizing heterogeneous GPU resources. Finally, we
discuss the gaps and opportunities among existing approaches
to motivate the design of a new training framework.

2.1 Parallel Strategies

Deep learning training often consists of millions of iterations,
referred to as mini-batches. A typical mini-batch includes
several phases to process data for model updating. Firstly, the
training data is fed into the model layer-by-layer to calculate
a set of scores, known as a forward pass. Secondly, a training
loss is calculated between the produced scores and desired
scores, which is then utilized to compute gradients for model
parameters, referred to as a backward pass. Finally, the gra-
dients scaled by a learning rate are used to update the model
parameters and optimizer states.

Data parallelism. Scaling to multiple GPUs, data paral-
lelism is a commonly adopted strategy where each worker
holds a full model replica to process different training data
independently. During the backward pass of every mini-batch,
the gradients are averaged through worker synchronization.
Therefore, the amount of communication is proportional to
the model parameter size.

Pipeline Parallelism. As shown in Figure 1, a DL model
is partitioned into two modules, i.e., M0 and M1 (which are
also named pipeline stages), which are placed on 2 GPUs
respectively. The training data of a mini-batch is split into
two smaller micro-batches. In particular, GPU0 starts with
the forward of the 1st micro-batch on M0, and then it switches
to process the forward of the 2nd micro-batch while sending

2

Figure 1: Pipeline parallelism
of 2 micro-batches on 2 GPUs.

Figure 2: Tensor model paral-
lelism for matmul on 2 GPUs.

Figure 3: Hybrid parallelism
for image classiﬁcation.

Figure 4: Data parallelism on
heterogeneous GPUs

the output of the 1st micro-batch to GPU1. After GPU1 ﬁn-
ishes processing forward and backward of the 1st micro-batch
on M1, GPU0 continues to calculate the backward pass for
M0 after receiving the backward output of M1 from GPU1.
Therefore, micro-batches are pipelined among GPUs, which
requires the runtime system to balance the load and overlap
computation and communication carefully [16, 20, 32, 54].
The model parallelism [11,12] can be treated as a special case
of pipeline parallelism with only one micro-batch.

Tensor Model Parallelism. With the growing model size,
to process DL operators beyond the memory capacity of the
GPU, or to avoid signiﬁcant communication overhead across
model replicas, an operator (or several operators) might be
split over multiple GPUs. The tensor model parallelism strat-
egy partitions the input/output tensors and requires an equiv-
alent distributed implementation for the corresponding op-
erator. For example, Figure 2 illustrates the tensor model
parallelism strategy for a matmul operator (i.e., matrix multi-
plication) using 2 GPUs. A matmul operator can be replaced
by two matmul operators, wherein each operator is responsi-
ble for half of the original computation. An extra all-gather
operation is required to merge the distributed results.

In selecting a proper parallel strategy for model training,
both model properties and resources need to be considered.
For example, transformer [44] is an important model in natu-
ral language understanding, which can be trained efﬁciently
using pipeline parallelism on a few GPUs (e.g., 8 V100 GPUs
with NVLINK [4]). However, pipeline parallelism does not
scale well with more GPUs (e.g., 64 V100 GPUs). Given more
GPUs, each training worker is allocated with fewer operators,
of which the GPU computation is not sufﬁcient enough to
overlap with the inter-worker communication cost, resulting
in poor performance. Therefore, a better solution is to apply
hybrid parallelism, where model partitions can be applied
with different parallel strategies in combination, and parallel
strategies can be nested. Particularly, for the training of a
transformer model on 64 GPUs, the model parameters can
be partitioned into 8 GPUs using a pipeline strategy, and ap-
ply model replica synchronization among 8 pipelined groups
using nested data parallelism. Moreover, different parallel
strategies can also apply to different model partitions for a hy-

brid. As an example, a large-scale image classiﬁcation model
(i.e., 100K categories) consists of the image feature extraction
partition and the classiﬁcation partition. The image feature ex-
traction partition requires a signiﬁcant amount of computation
on fewer model parameters. Conversely, the classiﬁcation par-
tition includes low-computation fully-connected and softmax
layers, which are often 10x larger in model size compared
to that of image feature extraction. Therefore, adopting a ho-
mogeneous parallel strategy will hinder the performance of
either partitions. Figure 3 illustrates a better hybrid parallelism
approach, in which data parallelism is applied for features
extraction partition, tensor model parallelism is adopted for
classiﬁcation partition, and the two are connected.

2.2 Heterogeneity in GPU Clusters

Training a giant model is considerably resource-intensive [17,
33]. Moreover, distributed model training often requires re-
sources to arrive at the same time (i.e., gang schedule [21,50]).
In industry, the shared cluster for giant model training is usu-
ally mixed with various types of GPUs (e.g., V100, P100, and
T4) for both model training and inference [47]. Training gi-
ant models over heterogeneous GPUs lowers the difﬁculty of
collecting all required GPUs (e.g., hundreds or thousands of
GPUs) simultaneously, therefore speeding up the model explo-
ration and experiments. However, deep learning frameworks
encounter challenges in efﬁciently utilizing heterogeneous
resources. Different types of GPUs are different in terms
of GPU memory capacity (e.g., 16GB for P100 and 32GB
for V100) and GPU computing capability, which natively in-
troduces an imbalance in computational graph partition and
deep learning operator allocation. Figure 4 illustrates train-
ing a model using data parallelism on two heterogeneous
GPUs, i.e., V100 and T4. The V100 training worker com-
pletes forward and backward faster when training samples are
allocated evenly, thereby leaving idle GPU cycles before gra-
dient synchronization at the end of every mini-batch. Through
the awareness of hardware when dynamically generating an
execution plan, Whale allocates more training samples (i.e.,
batch-size=4) for V100 and the rest of 2 samples for T4 to
eliminate the idle waiting time. Combined with advanced
parallel strategies and the hybrids over heterogeneous GPUs,
different GPU memory capacities and capabilities need to

3

timeline M1GPU1M0GPU0F0F1B0B1F0F1B0B1AllGatherLocalSplitGPU0GPU1GPU1InputResNet50FCSoftmaxInput0ResNet50 replica0Input1ResNet50 replica1FC shard0FC shard1Softmax shard0Softmax shard1GPU0GPU1GPU0(a) Local Model(b) Distributed Model[bs, 100K]FeatureClassificationV100T4SyncSyncIdle GPU cycleV100T4SyncSynctimeline(a) Naïve DP with identical batch size(b) Hardware-aware DP with load balancebe further considered when partitioning the model for efﬁ-
cient overlapping, which is a complex process (Section 3.3).
Model developers can hardly consider all resources issues
when programming, and we argue that developers should not
have to. A better approach for a general deep learning frame-
work would be automatically generating the execution plan
for heterogeneous resources adaptively.

2.3 Gaps and Opportunities

Recent approaches [20, 26, 38, 41, 43] have been proposed for
giant model training, however, with limitations as a general
DL framework. Firstly, they only support a small number of
parallel strategies, which lack a uniﬁed abstraction to support
all of the parallel strategies and the hybrids thereof. Secondly,
signiﬁcant efforts are required in code modiﬁcations to utilize
the advanced parallel strategies, compared with local model
training and DP approach. Mesh-tensorﬂow [41] requires the
re-implementation of DL operators in a distributed manner.
Megatron [43], GPipe [20], DeepSpeed [38], and GShard [26]
require user code refactoring using the exposed low-level
system primitives or a deep understanding for the implemen-
tation of parallel strategies. Thirdly, automatically parallel
strategy searching is time-consuming for giant models. Al-
though Tofu [46] and SOAP [23] accomplish model parti-
tioning and replication automatically through computational
graph analysis, the search-based graph optimization approach
has high computational complexity, which is further positively
associated with the number of model operators (e.g., hundreds
of thousands of operators for GPT3 [8]) and allocated GPUs
(e.g., hundreds or thousands), making such an approach im-
practical when applying to giant model training. Finally, due
to the heterogeneity in both GPU computing capability and
memory, parallel strategies should be used adaptively and
dynamically.

There are signiﬁcant gaps in supporting giant model train-
ing using existing DL frameworks. Exposing low-level inter-
faces dramatically increases user burden and limits system
optimization opportunities. Users need to understand the im-
plementation details of distributed operators and handle the
overlapping of computation with communication, which is
hard for model developers. Using a low-level approach tightly
couples model code to a speciﬁc parallel strategy, which re-
quires code rewriting completely when switching between
parallel strategies (i.e., from pipeline parallelism to tensor
model parallelism). More constraints are introduced to model
algorithm innovations, because the efforts of implementing a
new module correctly in hybrid strategies are not trivial, let
alone consider the performance factors such as load balancing
and overlapping. From the system aspect, seeking a better
parallel strategy or a combination using existing ones also
requires rewriting user code, demanding a deep understanding
of the DL model.

To address the aforementioned challenges, Whale explores

a new approach that supports various parallel strategies while
minimizing user code modiﬁcations. By introducing new uni-
ﬁed primitives, users can focus on implementing the model
algorithm itself, while switching among various parallel strate-
gies by simply changing the annotations. Whale runtime uti-
lizes the user annotations as hints to select parallel strategies
at best effort with automatic graph optimization under a lim-
ited search scope. Whale further considers heterogeneous
hardware capabilities using a balanced algorithm, making
resource heterogeneity transparent to users.

3 Design

In this section, we ﬁrst introduce key abstractions and parallel
primitives which can express ﬂexible parallelism strategies
with easy programming API (Section 3.1). Then, we describe
our parallel planner that transforms a local model with parallel
primitives into a distributed model, through partitioning Task-
Graphs, inserting bridge layers to connect hybrid strategies,
and placing TaskGraphs on distributed devices (Section 3.2).
In the end, we propose a hardware-aware load balance al-
gorithm to speed up the training with heterogeneous GPU
clusters (Section 3.3).

3.1 Abstraction

3.1.1

Internal Key Concepts

Deep learning frameworks such as TensorFlow [7] provide
low-level APIs for distributed computing, but is short of ab-
stractions to represent advanced parallel strategies such as
pipeline. The lack of proper abstractions makes it challeng-
ing in the understanding and implementation of complicated
strategies in a uniﬁed way. Additionally, placing model oper-
ations to physical devices properly is challenging for compli-
cated hybrid parallel strategies, especially in heterogeneous
GPU clusters. Whale introduces two internal key concepts,
i.e., TaskGraph and VirtualDevice. TaskGraph is used to mod-
ularize operations for applying a parallel strategy. VirtualDe-
vice hides the complexity of mapping operations to physical
devices. The two concepts are abstractions of internal system
design and are not exposed to users.

TaskGraph(TG) is a subset of the model for parallel trans-
formation and execution. One model can have one or more
non-overlapping TaskGraphs. We can apply parallel strate-
gies to each TaskGraph. By modularizing model operations
into TaskGraphs, Whale can apply different strategies to dif-
ferent model parts, as well as scheduling the execution of
TaskGraphs in a pipeline. A TaskGraph can be further repli-
cated or partitioned. For example, in data parallelism, the
whole model is a TaskGraph, which can be replicated to mul-
tiple devices. In pipeline parallelism, one pipeline stage is
a TaskGraph. In tensor model parallelism, we can shard the
TaskGraph into multiple submodules for parallelism.

4

import wh ale as wh
wh .init ( wh .Config ({

" num_micro_batch ": 8}) )

with wh .replicate (1) :

model_stage1 ()

with wh .replicate (1) :

model_stage2 ()

import wh ale as wh
wh .init ()
with wh .replicate ( total_gpu ):
features = ResNet50 ( inputs )

with wh .split ( total_gpu ):
logits = FC ( features )
predictions = Softmax ( logits )

Example 1: Pipeline with 2
TaskGraphs

Example 2: Hybrid of replicate
and split

VirtualDevice (VD) is the logical representation of com-
puting resources, with one VirtualDevice having one or more
physical devices. VirtualDevice hides the complexity of de-
vice topology, computing capacity as well as device placement
from users. One VirtualDevice is assigned to one TaskGraph.
Different VirtualDevices are allowed to have different or the
same physical devices. For example, VD0 contains physical
devices GPU0 and GPU1, VD1 contains physical devices
GPU2 and GPU3 (different from VD0), and VD2 contains
physical devices GPU0 and GPU1 (the same as VD0).

3.1.2 Parallel Primitives

The parallel primitive is a Python context manager, where
operations deﬁned under it are modularized as one TaskGraph.
Each parallel primitive has to be conﬁgured with a parameter
device_count, which is used to generate a VirtualDevice by
mapping the device_count number of physical devices. Whale
allows users to suggest parallel strategies with two uniﬁed
primitives, i.e., replicate and split. The two primitives can
express all existing parallel strategies, as well as a hybrid of
them [20, 25, 26, 32, 43].

replicate(device_count) annotates a TaskGraph to be repli-
cated. device_count is the number of devices used to compute
the TaskGraph replicas. If device_count is not set, Whale al-
locates a TaskGraph replica per device. If a TaskGraph is
annotated with replicate(2), it is replicated to 2 devices, with
each TaskGraph replica consuming half of the mini-batch.
Thus the mini-batch size for one model replica is kept un-
changed.

split(device_count) annotates a TaskGraph to apply intra-
tensor sharding. The device_count denotes the number of
partitions to be sharded. Each sharded partition is placed on
one device. For example, split(2) shards the TaskGraph into
2 partitions and placed on 2 devices respectively.

The parallel primitives can be used in combination to ap-
ply different parallel strategies to different partitions of the
model. Additionally, Whale also provides JSON Conﬁg API
to enable system optimizations. The conﬁg auto_parallel is
used to enable automatic TaskGraph partitioning given a pro-
vided partition number num_task_graph, which further eases
the programming for users and is necessary for hardware-
aware optimization when resource allocation is dynamic (Sec-
tion 3.3). In Whale, pipeline parallelism is viewed as an ef-
ﬁcient inter-TaskGraph execution strategy. Whale uses the

5

Figure 5: Whale Overview

conﬁg num_micro_batch to enable efﬁcient pipeline paral-
lelism among TaskGraphs when the value is greater than 1.
In this way, Whale decouples the generation of TaskGraph
from the choice of pipeline parallelism strategies [16, 20, 32].
The system can easily extend to incorporate more pipeline
strategies (e.g., swap the execution order of B0 and F1 for
M1 in Figure 1).

Besides the combination of parallel strategies or pipeline
parallelism, Whale further supports nested data parallelism
to the whole parallelized model. Nested data parallelism is
enabled automatically when the number of available devices
is times of total devices requested by TaskGraphs.

Example 1 shows an example of pipeline parallelism with
two TaskGraphs, with each TaskGraph being conﬁgured with
1 device. The pipeline parallelism is enabled by conﬁguring
the pipeline.num_micro_batch to 8. The total device number
of the two TaskGraphs is summed to 2. If the available device
number is 8, which is 4 times of total device number, Whale
will apply a nested 4-degree data parallelism beyond the
pipeline. In contrast, when using two available devices, it is a
pure pipeline. Example 2 shows a hybrid strategy that repli-
cates ResNet50 feature part while splitting the classi f ication
model part for the example in Figure 3.

wh .init ( wh .Config ({ " num_task_graph ":2 ,

" num_micro_batch ":4 ," auto_parallel ": True }) )

model_def ()

Example 3: Auto pipeline

Example 3 shows an automatic pipeline example with two
TaskGraphs. When auto_parallel is enabled, Whale will par-
tition the model into TaskGraphs automatically according
to the computing resource capacity and the model structure.
(Section 3.3)

3.2 Parallel Planner

The parallel planner is responsible for producing an efﬁcient
parallel execution plan, which is the core of Whale runtime.
Figure 5 shows an overview of the parallel planner. The work-
ﬂow can be described as follows: (a) The parallel planner
takes a local model with optional user annotations, computing

Bridgelegend:pipeline excutionsync gradientsdevice mappingVD1VD2GPU0GPU1GPU2GPU3BridgeGPU4GPU5GPU6GPU7GPU0GPU1GPU2GPU3GPU6GPU7GPU4TG1GPU5TG2M2M1TG2TG1with replicate(2):with split(2):Local Model(a) Parallel primitive annotation(b) Virtual device generation(c) Parallel plan Generationresources, and optional conﬁgs as inputs. The model hyperpa-
rameters (e.g., batch size and learning rate), and computing
resources (e.g., #GPU and #worker) are decided by the users
manually. While the parallel primitive annotations and con-
ﬁgs (e.g., num_task_graph and num_micro_batch) could be
either be manual or decided by Whale automatically; (b) the
VirtualDevices are generated given computing resources and
optional annotations automatically (Section 3.2.1); and (c)
the model is partitioned into TaskGraphs, and the TaskGraph
is further partitioned internally if split is annotated. Since we
allow applying different strategies to different TaskGraphs,
there may exist an input/output mismatch among TaskGraphs.
In such case, the planner will insert the corresponding bridge
layer automatically between two TaskGraphs (Section 3.2.3).

3.2.1 Virtual Device Generation

i di, Whale will apply a nested DP of K
∑N
i di

VirtualDevices are generated given the number of devices
required by each TaskGraph. Given K requested physical
devices GPU0, GPU1, ..., GPUK and a model with N Task-
Graphs, with corresponding device number d1, d2, ...dN. For
the ith TaskGraph, Whale will generate a VirtualDevice with
di number of physical devices. The physical devices are taken
sequentially for each VirtualDevice. As mentioned in Sec-
tion 3.1.2, when the available device number K is divisible
by the total number of devices requested by all TaskGraphs
∑N
-degree to the
whole model. In such case, we also replicate the correspond-
ing VirtualDevice for TaskGraph replica. By default, devices
are not shared among TaskGraphs. Sharing can be enabled
to improve training performance in certain model sharding
cases by setting cluster conﬁguration1. Whale prefers to place
one model replica (with one or more TaskGraphs) within a
node, and replicates the model replicas across nodes. Ad-
vanced behaviors such as placing TaskGraph replicas within
a node to utilize NVLINK for AllReduce communication can
be achieved by setting the aforementioned conﬁguration. For
example, as shown in Figure 5, there are two TaskGraphs, and
each TaskGraph requests 2 GPUs. Two VirtualDevices VD1
and VD2 are generated for two TaskGraphs. VD1 contains
GPU0 and GPU1, and VD2 contains GPU2 and GPU3. As
the number of available GPUs is 8, which is divisible by the to-
tal GPU number of TaskGraphs 4, a replica of VirtualDevices
can be generated but with different physical devices.

Figure 6: Sharding pattern example for MatMul. One
ShardingUnit can map to multiple sharding patterns.

parameter num_task_graph and hardware information. The
details of the hardware-aware model partitioning is described
in Section 3.3.

If a TaskGraph is annotated with split(k), Whale will au-
tomatically partition it by matching and replacing sharding
patterns with a distributed implementation. Before describ-
ing the sharding pattern, we introduce two terminologies for
tensor model parallelism: 1) ShardingUnit is a basic unit for
sharding, and can be an operation or a layer with multiple
operations; and 2) ShardingInfo is the tensor sharding infor-
mation, and is represented as a list [s0, s1, ..., sn] given a tensor
with n dimensions, where si represents whether to split the
ith dimension, 1 means true and 0 means false. For example,
given a tensor with shape [6, 4], the ShardingInfo [0, 1] indi-
cates splitting in the second tensor dimension, whereas [1, 1]
indicates splitting in both dimensions. A sharding pattern(SP)
is a mapping from a ShardingUnit and input ShardingInfo
to its distributed implementations. For example, Figure 6
shows two sharding patterns SP1 and SP2 with different input
ShardingInfo for ShardingUnit MatMul.

To partition the TaskGraph, Whale ﬁrst groups the oper-
ations in the split TaskGraph into multiple ShardingUnits
by hooking TensorFlow ops API2. The TaskGraph sharding
process starts by matching ShardingUnits to the predeﬁned
sharding patterns in a topology order. A pattern is matched by
a ShardingUnit and input ShardingInfos. If multiple patterns
are matched, the pattern with a smaller communication cost is
selected. Whale replaces the matched pattern of the original
ShardingUnit with its distributed implementation.

3.2.3 Bridge Layer

3.2.2 TaskGraph Partitioning

Whale ﬁrst partitions a model into TaskGraphs, either by us-
ing explicit annotations or automatic system partitioning. If a
user annotation is given, operations deﬁned within certain par-
allel primitive annotation compose a TaskGraph. Otherwise,
the system generates TaskGraphs based on the given conﬁg

When applying different parallel strategies to different Task-
Graphs, the input/output tensor number and shape may change
due to different parallelism degrees or different parallel strate-
gies, thereby resulting in a mismatch of input/output tensor
shapes among TaskGraphs. To address the mismatch, Whale
proposes a bridge layer to gather the distributed tensors and
feed them to the next TaskGraph.

1https://easyparallellibrary.readthedocs.io/en/latest/

2TensorFlow Ops: https://github.com/tensorflow/tensorflow/

api/config.html#clusterconfiguration

tree/r1.15/tensorflow/python/ops

6

ShardingUnit: MatMulAllReduceInput ShardingInfo {[0, 0], [0, 1]}Input ShardingInfo {[0, 1], [1, 0]}SP1SP2Figure 7: Bridge patterns.

Whale designs two bridge patterns for replicate and split
respectively, as shown in Figure 7. For replicate, the Task-
Graph is replicated to N devices, with different input batches.
The bridge layer gathers the outputs from different batches
for concatenation in batch dimension batch_dim. For split,
the outputs of TaskGraph are partitioned in split dimension
split_dim. The bridge layer gathers TaskGraph outputs for
concatenation in split_dim. By using the bridge layer, each
TaskGraph can obtain a complete input tensor. If the gather
dimension of the bridge layer is the same as the successor
TaskGraph input partition dimension, Whale will optimize
by fusing the aforementioned two operations to reduce the
communication overhead. As an example, if the outputs of
the TaskGraph are gathered in the ﬁrst dimension, and the
inputs of the successor TaskGraph are partitioned in the same
dimension, then Whale will remove the above gather and
partition operations.

3.3 Hardware-aware Load Balance

In this section, we describe how we utilize the hardware infor-
mation to balance the workloads among TaskGraphs, which
achieves high performance even in heterogeneous GPU clus-
ters. The Whale parallel planner obtains the hardware infor-
mation from the cluster scheduler when the training job is
launched, and is responsible for both intra-TaskGraph and
inter-TaskGraph load balancing.

3.3.1 Intra-TaskGraph Load Balance

When the allocated devices are homogeneous, by default
Whale distributes the workloads within a TaskGraph to multi-
ple devices evenly. However, when allocated with heteroge-
neous GPUs with different computing capacities (e.g., V100
and P100), the aforementioned identical distribution effectu-
ates suboptimal performance. Such performance can be at-
tributed to a synchronization barrier at the end of TaskGraph
execution, which leads to long idle GPU time for the faster
GPU, as shown in Figure 4(a). To improve the overall utiliza-
tion of heterogeneous GPUs, we need to balance the comput-
ing according to the device’s computing capacity. The intra-
TaskGraph load balance attempts to minimize the idle time
within a TaskGraph, which is achieved by balancing the work-
loads proportional to device computing capacity while being
subject to memory constraints. For a TaskGraph annotated
with replicate, Whale balances the workload by adjusting the
batch size for each TaskGraph replica. The local batch size on
heterogeneous devices might differ due to the load balancing
strategy (Whale keeps the global batch size unchanged). If

7

batch-sensitive operators such as BatchNorm exist, the local
batch differences might have statistical effects. Yet, no users
suffer convergence issues when using heterogeneous training
in Whale, which is probably due to the robustness of DL. Be-
sides, techniques like SyncBatchNormaliazaion3 might help.
For a TaskGraph annotated with split, Whale balances the
FLOP of a partitioned operation through uneven sharding in
splitting dimension among multiple devices.

We proﬁle the TaskGraph T G on single-precision ﬂoating-
point operations(FLOP) as T G f lop and peak memory con-
sumption as T Gmem. Given N GPUs, we collect the infor-
mation for device i including the single-precision FLOP per
second as DFi and memory capacity as DMi. Assuming the
partitioned load ratio on the device i is Li, we need to ﬁnd
a solution that minimizes the overall GPU waste, which is
formulated in Formula 1. We try to minimize the ratio of the
computational load of the actual model for each device Li
and the ratio of the computing capacity of the device over the
total cluster computing capacity DFi/ ∑N
i=0 DFi, the maximum
workload being bounded by the device memory capacity DMi.

min

(cid:13)
N
(cid:13)
(cid:13)
∑
(cid:13)
(cid:13)
i

Li −

DFi
i=0 DFi

∑N

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

s.t.

N
∑
i=0

Li = 1; Li ∗ T Gmem <= DMi, (i = 1, 2, ..., N)

(1)

The load ratio Li in each device is initialized in propor-
tional to the device’s computing capacity, which ideally re-
sults in a most balanced partition. However, when the memory
constraint is not satisﬁed, we need to adjust the load alloca-
tion to avoid out-of-memory (OOM) errors, while still trying
to achieve good performance. Whale proposes a memory-
constraint balancing algorithm to balance the workloads
among devices. The main idea of the algorithm is to shift the
workload from the memory-overload device to a memory-free
device with the lowest computation load. The details of the
algorithm are illustrated in Algorithm 1. It takes a TaskGraph
T G and VirtualDevice with N physical devices as inputs. The
algorithm ﬁrst initializes (line 3-10) the proﬁling results in-
cluding 1) load_ratios as the workload ratios of devices; 2)
mem_utils as the memory utilization of devices; 3) f lop_utils
as the FLOP utilization of devices; 4) oom_devices records
out of memory devices whose value in mem_utils is greater
than 1; and 5) f ree_devices records devices that have free
memory space. The algorithm then iteratively shifts the load
from a memory-overload device to a memory-available de-
vice (line 11-18). It ﬁrst ﬁnds a peak_device with maxi-
mum memory utilization from oom_devices, then it ﬁnds a
valley_device with available memory space and the lowest
FLOP utilization. The shi f t_load function attempts to shift

3https://www.tensorflow.org/api_docs/python/tf/keras/

layers/experimental/SyncBatchNormalization

(a) replicate bridgeGather (3, batch_dim)Gather (3, split_dim)(b) split bridgeAlgorithm 1: Memory-Constraint Load Balancing

Input: TaskGraph T G,VirtualDevice(N)

1 load_ratios = /0; mem_utils = /0 ; f lop_utils = /0
2 oom_devices = /0 ; f ree_devices = /0
3 foreach i ∈ 0...N do

4

5

6

7

8

9

10

∑N

i=0 DFi

load_ratios[i] = DFi
mem_utils[i] = load_ratios[i]∗T Gmem
f lop_utils[i] = load_ratios[i]∗T G f lop
if mem_utils[i] > 1 then

DMi

DFi

oom_devices.append(i)

else

f ree_devices.append(i)

11 while oom_devices (cid:54)= /0 & f ree_devices (cid:54)= /0 do
12

peak_device = argmax(oom_devices, key = mem_utils)
valley_device = argmin( f ree_devices, key =

( f lop_utils, mem_utils))

if shi f t_load(peak_device, valley_device) == success

then

update_pro f ile(mem_utils, f lop_utils)
oom_devices.pop(peak_device)

else

f ree_devices.pop(valley_device)

13

14

15

16

17

18

Figure 8: Pipeline TaskGraphs on heterogeneous GPUs

the workload from a peak_device to a valley_device. For data
parallelism, the batch size in the peak_device is decreased
by b, and the batch size in the valley_device is increased by
b. b is the maximum number that the valley_device will not
go OOM after getting the load from the peak_device. The
proﬁling information for each device is updated after a suc-
cessful workload shift is found. The aforementioned process
iterates until the oom_devices are empty or the f ree_devices
are empty.

3.3.2 Inter-TaskGraph Load Balance

When multiple TaskGraphs are executed in a pipeline, we
need to balance the inter-TaskGraph workloads on hetero-
geneous GPUs. As we introduced in Section 2.1, pipeline
parallelism achieves efﬁcient execution by interleaving for-
ward/backward execution among multiple micro-batches. For
a model with N TaskGraphs, the ith TaskGraph needs to cache
N − i forward activations [32]. Notably, ith TaskGraph has to
cache one more micro-batch forward activation than the pre-

8

vious TaskGraph. Since activation memory is proportional to
batch size and often takes a large proportion of the peak mem-
ory, e.g., the activation memory VGG16 model with batch size
256 takes up around 74% of the peak memory [18], resulting
in uneven memory consumption among different TaskGraphs.
The different memory requirements of TaskGraphs motivate
us to place earlier TaskGraphs on devices with higher mem-
ory capacity. This can be achieved by sorting and reordering
the devices in the corresponding VirtualDevice by memory
capacity, from higher to lower. Figure 8 shows the memory
breakdown of the pipeline example (Figure 1) with two Task-
Graphs over heterogeneous GPUs V100 (32GB) and P100
(16GB), we prefer putting TaskGraph0 to V100, which has
a higher memory conﬁg. The TaskGraph placement heuris-
tic is efﬁcient for common Transformer-based models (i.e.,
BertLarge and T5 in Figure 18). There might be cases where
later stages contain large layers (i.e., large sparse embedding),
which can be addressed in Algorithm 1 on handling OOM er-
rors. After reordering the virtual device according to memory
requirement, we partition the model operations to TaskGraphs
in a topological sort and apply Algorithm 1 to balance the
computing FLOP among operations, subject to the memory
bound of the memory capacity of each device.

4

Implementation

Whale is implemented as a standalone library without modiﬁ-
cation of the deep learning framework, which is compatible
with TensorFlow1.12 and TensorFlow1.15 [7]. The source
code of Whale includes 13179 lines of Python code and 1037
lines of C++ code. We have open-sourced4 the Whale frame-
work to help giant model training accessible to more users.

Whale enriches the local model with augmented informa-
tion such as phase information, parallelism annotation, etc.,
which is crucial to parallelism implementation. To assist
the analysis of the user model without modifying the user
code, Whale inspects and overwrites TensorFlow build-in
functions to capture augmented information. For example,
operations are marked as backward when t f .gradients or
compute_gradients functions are called.

The parallel strategy is implemented by rewriting the com-
putation graph. We implement a general graph editor module
for ease of graph rewriting, which includes functions such as
subgraph clone, node replacement, dependency control, and
so on. To implement data parallelism, Whale ﬁrst clones all
operations and tensors deﬁned in a local TaskGraph and re-
places the device for model replicas. Then it inserts NCCL [6]
AllReduce [40] operation to synchronize gradients for each
TaskGraph replica. To implement tensor model parallelism,
Whale shards the TaskGraph by matching a series of prede-
ﬁned patterns, replacing them with corresponding distributed
implementation, and inserting communication operations as

4https://github.com/alibaba/EasyParallelLibrary

Other MemoryConsumptionMB FWD ActivationOther Memory ConsumptionMB FWD ActivationMemoryTaskGraph0V100 32GBTaskGraph1 P100 16GBMB FWD ActivationFigure 9: Whale DP vs TF DP
on ResNet.

Figure 10: Whale DP vs TF DP
on BertLarge.

Figure 11: Whale Pipeline vs
GPipe.

Figure 12: Hybrid pipeline par-
allelism on BertLarge.

needed. To implement pipeline parallelism, Whale builds a
pipeline strategy module that supports state-of-the-art strate-
gies [16, 20, 32]. By default, Whale adopts a backward-ﬁrst
strategy which is similar to PipeDream [32]. The pipeline
strategy is implemented by ﬁrst partitioning the minibatch
into micro-batches. The interleaving of forward-backward
micro-batch execution is achieved by inserting control de-
pendency operations among entrance and exit operations of
different TaskGraphs.

To assist hardware-aware optimizations, Whale implements
proﬁling tools that proﬁle the model FLOPS and peak mem-
ory consumption. The parallel planner gets the hardware in-
formation from our internal GPU cluster, which is used to
generate an efﬁcient parallel plan by balancing the computing
workloads over heterogeneous GPUs.

Besides, Whale is highly optimized in both computing ef-
ﬁciency and memory utilization by integrating with a series
of optimization technologies such as ZERO [36], recomputa-
tion [10], CPU ofﬂoad [39], automatic mixed precision [31],
communication optimization [40], XLA [7], etc.

5 Experiment

In this section, we ﬁrst demonstrate the efﬁciency of the par-
allelism strategy by evaluating micro-benchmarks. We then
evaluate the training with heterogeneous GPUs to show the
advantages of the hardware-aware load balance algorithm.
We end by showing the effectiveness and efﬁciency of Whale
by two industry-scale multimodal model training cases. All
the experiments are conducted on a shared cloud GPU cluster.
Every cluster node is equipped with a 96-core Intel Xeon Plat-
inum 8163 (Skylake) @2.50GHz with 736GB RAM, running
CentOS 7.7. Each node consists of 2/4/8 GPUs, with NVIDIA
32-GB V100 GPUs [3] or NVIDIA 16-GB P100 GPUs [2],
powered by NVIDIA driver 418.87, CUDA 10.0, and cuDNN
7. Nodes are connected by 50Gb/s ethernet. All the models
are implemented based on TensorFlow 1.12.

9

5.1 Micro-benchmark

In this section, we evaluate Whale with a series of micro-
benchmarks. We ﬁrst demonstrate that Whale is efﬁcient in
single parallel strategy by comparing with TensorFlow Esti-
mator [14] DP and GPipe [20] pipeline. We then show the
advantages of Whale hybrid strategies over single parallel
strategy. Next, we measure the overhead of the bridge layer
for hybrid strategies. Finally, we evaluate the effect of shard-
ing patterns in automatic TaskGraph partitioning.

5.1.1 Performance of Single Parallel Strategy

We evaluate Whale DP by comparing it with TensorFlow
Estimator DP, using the BertLarge [13] and ResNet50 [19] on
different number of V100 GPUs. Figure 9 and Figure 10 show
the training throughput speedup on ResNet50 and BertLarge
respectively. The throughput speedup is calculated by dividing
the training throughput on N devices by the throughput on
one device. Whale DP consistently obtained better speedup
and higher GPU utilization than TensorFlow Estimator DP.
Such ﬁndings could be attributed to Whale’s communication
optimization technologies such as hierarchical and grouped
AllReduce, which is similar to Horovod [40].

We then evaluate the efﬁciency of Whale pipeline paral-
lelism by comparing with GPipe [20]. The pipeline scheduling
strategy in Whale is similar to PipeDream [32]. The exper-
iments are conducted using the BertLarge model with 4/8
pipeline stages on the different numbers of V100 GPUs. As
shown in Figure 11, the training throughput speedup of Whale
outperforms GPipe in both 4 stages and 8 stages by 1.45X
and 1.14X respectively. We attribute the performance gain to
the use of the alternating forward-backward scheduling pol-
icy [32], which improves GPU utilization. We also ﬁnd that
the pipeline performance is sensitive to the num_task_graph,
thus exposing it as a conﬁgurable parameter can help achieve
a better performance when models and computing resources
change.

5.1.2 Performance of Hybrid Strategy

We evaluate hybrid strategies by comparing them with the
single parallel strategy. We also compare the performances of

 0 5 10 15 20 25 30 35 40181632 0 0.2 0.4 0.6 0.8 1 1.2 1.4SpeedupGPU UtilizationNumber of GPU requestTF speedupWhale speedupTF GPU UtilWhale GPU Util 0 5 10 15 20 25 30 35 40181632 0 0.2 0.4 0.6 0.8 1 1.2 1.4SpeedupGPU UtilizationNumber of GPU requestTF speedupWhale speedupTF GPU UtilWhale GPU Util 0 1 2 3 4 5 6 7 848 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1SpeedupGPU UtilizationNumber of TaskGraphsGpipeWhaleGpipe GPU UtilWhale GPU Util 0 5 10 15 20 25 3081632 0 0.2 0.4 0.6 0.8 1SpeedupGPU UtilizationNumber of GPU request#TG=2 Speedup#TG=4 Speedup#TG=8 SpeedupFigure 13: DP vs Hybrid on
ResNet50 w/ 100K classes.

Figure 14: Hybrid strategy on
ResNet50 w/ 1M classes.

Figure 15: Effect of Sharding
Pattern.

Figure 16: Overhead of Bridge
Layer.

hybrid strategies on different numbers of devices. We select
two typical types of hybrid strategies: 1) Nested pipeline with
DP; and 2) Combination of DP and tensor model parallelism.
We ﬁrst apply a nested pipeline with DP to the BertLarge
model on V100 GPUs. The model is partitioned into 2/4/8
number of TaskGraphs, and we measure the training perfor-
mance of each model on 8/16/32 GPUs. Figure 12 shows that
pipelines with 2 TaskGraphs and 4 TaskGraphs get similar
training speedups and GPU utilization. However, we observe a
performance drop on 8 TaskGraphs and lower GPU utilization
compared to 2/4 TaskGraphs. This is because 8 TaskGraphs
lead to relatively fewer model operations in each TaskGraph,
and the GPU computation is not enough to overlap the inter-
TaskGraph communication, resulting in poor performance.

Next, we evaluate the combination hybrid strategy on a
large-scale image classiﬁcation model, as we have discussed
in Section 2.1 and illustrated in Figure 3. We perform experi-
ments on classiﬁcation numbers 100K and 1M on different
numbers of V100 GPUs. To reduce the communication over-
head of hybrid parallelism, we collocate the ResNet50 repli-
cas with FC partitions. We compare the hybrid results of 100K
classes with DP, as shown in Figure 13, hybrid parallelism
outperforms data parallelism by 1.13X, 1.66X, and 2.43X
training throughput speedup with 8, 16, and 32 GPUs respec-
tively, with the line plot corresponding to GPU utilization.
When the number of workers increases, hybrid parallelism
maintains a near-linear speedup, while the DP strategy fails
drastically beyond 16 workers. This is because the heavy FC
layer (the parameter size of ResNet50 backbone is 90 MB,
while the parameter size of FC layer is 782MB) incurs a huge
gradient synchronization overhead. For the task of 1M classes,
DP fails due to OOM. With hybrid parallelism, Whale allows
for the training of image classiﬁcation task with one million
classes. Figure 14 shows the performance of hybrid paral-
lelism over 8/16/32 GPUs. The training throughputs from
8 GPUs to 32 GPUs achieve 95% scaling efﬁciency, which
highlights the need for using a hybrid strategy.

5.1.3 Overhead of Bridge Layer

To demonstrate the efﬁciency of the hybrid strategy, We mea-
sure the overhead of the bridge layer by proﬁling the bridge
layer time with 100K classes on 8/16/32 GPUs. We then com-
pare the overhead of gradient AllReduce time in DP with the

Figure 17: Hardware-Aware
Data Parallelism.

Figure 18: Hardware-Aware
Pipeline Parallelism.

bridge overhead to understand the performance gain from
hybrids. As shown in Figure 16, the overhead of the bridge
layer takes around 6% in overall training time in 8 GPUs and
10% in 32 GPUs. The overhead of the hybrid is reduced by 6X
on 32 GPUs compared to gradient synchronization overhead
of pure DP.

5.1.4 Effect of Sharding Pattern

As Whale automatically chooses a sharding pattern with min-
imum communication cost (Section 3.2.2), to demonstrate
the effect of exploring the sharding patterns, we force the
framework to use a speciﬁc pattern in this experiment. We
evaluate two types of sharding patterns as illustrated in Fig-
ure 6 on large scale image task with 100K classes. SP1 shards
the second input tensor in the second tensor dimension, and
SP2 shards the two input tensors and aggregates the results
with AllReduce. The comparison results of the two sharding
patterns are shown in Figure 15, where SP1 outperforms SP2
by 1.6X to 3.75X as the number of requested GPUs increases
from 8 to 32, as SP1 has a lower communication cost than SP2.
The exploration of sharding patterns allows for the possibility
of system optimization in distributed model implementation.

5.2 Performance of Load Balance

We show the beneﬁts of the hardware-aware load balancing
algorithm by evaluating data parallelism and pipeline paral-
lelism.

For data parallelism, we evaluate three typical models, in-
cluding ResNet50, BertLarge, and GNMT [48]. The experi-
ments are conducted on heterogeneous GPUs that consist of
8 32GB V100 GPUs and 8 16GB P100 GPUs. We set the
same batch size for all model replicas as the baseline. We

10

 0 1000 2000 3000 4000 5000 6000 7000 8000 9000181632 0 0.2 0.4 0.6 0.8 1 1.2 1.4Throughput(samples/s)GPU UtilizationNumber of GPU requestDPDP+SplitDP GPU UtilDP+Split GPU Util 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 700081632 0 0.2 0.4 0.6 0.8 1 1.2 1.4Throughput(samples/s)GPU UtilizationNumber of GPU requestDP+SplitDP+Split GPU Util 1000 2000 3000 4000 5000 6000 700081632 0 0.2 0.4 0.6 0.8 1 1.2 1.4Throughput (samples/s)GPU UtilizationNumber of GPU requestSP2SP1SP2 GPU UtilSP1 GPU Util 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.881632Communication time ratioNumber of GPU requestDP comm ratioHybrid comm ratio 0.6 0.8 1 1.2 1.4 1.6 1.8 2ResNet-50GNMTBertLarge 0.4 0.6 0.8 1 1.2 1.4SpeedupGPU UtilizationHardware-Aware SpeedupBase P100 GPU UtilHardware-Aware P100 GPU UtilBase V100 GPU UtilHardware-Aware V100 GPU Util 0 0.5 1 1.5 2BertLargeT5 0 0.2 0.4 0.6 0.8 1 1.2 1.4SpeedupGPU UtilizationHardware-Aware SpeedupBase P100 GPU UtilHardware-Aware P100 GPU UtilBase V100 GPU UtilHardware-Aware V100 GPU Utilimport wh ale as wh
wh .init ( wh .Config ({

" num_micro_batch ": 35 ,
" num_task_graph ": 8}) )

# Define M6 model .
m6_model_def ()

Example 4: M6-10B model
with pipeline

Figure 19: M6-10B
with Pipeline and DP.

then apply the hardware-aware algorithm to each model and
get the speedup compared with the baseline performance, as
shown in Figure 17. Whale outperforms the baseline in all
three models by a factor from 1.3X to 1.4X. We also measure
GPU utilization and report the average metric for each GPU
type. The hardware-aware policy signiﬁcantly improves the
GPU utilization of V100 by 1.39X to 1.96X for the three
models, which improves the overall training performance.

For pipeline parallelism, we evaluate two models, including
BertLarge and T5-Large [52]. The training is performed on
heterogeneous GPUs that consist of 4 32GB V100 GPUs and
4 16GB P100 GPUs. Both BertLarge and T5-Large are parti-
tioned into 4 stages. We further apply a nested DP to pipeline.
We set the evenly partitioned model as the baseline. We con-
ducted training with the hardware-aware policy and got about
20% speedup on both models, as shown in Figure 18. The
GPU utilization of hardware-aware load balancing strategy
improved the GPU utilization of V100 by around 40%, which
shows the efﬁciency of the hardware-aware load balancing
algorithm.

5.3

Industry-Scale Giant Model Training

5.3.1 Training M6-10B Model

The M6-10B [28] model is a Chinese multimodal model with
10 billion parameters. The model consists of 24 encoder layers
and 24 decoder layers. We use Adafactor [42] as the training
optimizer. We parallelize the training of M6-10B model with
a hybrid parallel strategy, by nesting pipeline parallelism and
data parallelism. Whale can easily scale a local M6 model
to a distributed one by only adding a few lines on top of the
model deﬁnition as shown in Example 4. We set the number
of pipeline stages to 8 and the number of micro-batches to
35. We enable recomputation [10] to save activation mem-
ory during training. The training performance is evaluated
on 32-GB V100 GPUs. Each node contains 8 GPUs. When
scaling the computing nodes from 8 to 32, Whale achieved
91% scalability, as shown in Figure 19.

5.3.2 Training M6-MoE Model to Trillions

We scale the model parameters to 10 trillion (10T) by switch-
ing to hybrids of DP and tensor model parallelism with only
a small number of lines of code change. The computation
cost of training dense models is proportional to the model
parameters. If we scale the dense 10B model to the dense

10T model linearly without considering overhead, we need at
least 256,000 NVIDIA V100 GPUs. Instead of scaling the M6
model with dense structure, we adopt M6-MoE [53] model
with sparse expert solution [17, 26]. The sample code of the
MoE structure is implemented with Whale by adding four
lines, as shown in Example 5. Line 3 sets the default parallel
primitive as replicate, i.e., data parallelism is applied for the
operations if not explicitly annotated. Line 5 partitions the
computation deﬁned under split scope across devices.

1
2
3
4
5
6

import wh ale as wh
wh .init ()
wh .set_default_strategy ( wh .replicate ( total_gpus ))
combined_weights , dispatch_inputs = gating_dispatch ()
with wh .split ( total_gpus ):

outputs = MoE ( combined_weights , dispatch_inputs )

Example 5: Distributed MoE model

We evaluate M6-MoE model with 100 billion, 1 trillion and
10 trillion parameters respectively, the detailed conﬁgurations
can be found in [29, 53]. We enable built-in technologies of
Whale to optimize the training process, such as recomputa-
tion [10], AMP (auto mixed precision) [1], XLA [5], CPU
ofﬂoading [39], etc. We can train the M6-MoE-100B model
with 100 million samples on 128 V100 in 1.5 days. We ad-
vance the model scale to 1 trillion parameters on solely 480
NVIDIA V100 GPUs, in comparison with the recent SOTA
on 2048 TPU cores [17]. We further scale the model to 10
trillion parameters by adopting optimized tensor ofﬂoading
strategies [29] with 512 NVIDIA V100 GPUs. Whale can
scale models from 100 billion to 10 trillion without code
changes, which makes giant model training accessible to most
users.

6 Related Work

Giant model training. TensorFlow [7] and PyTorch [34]
provide well-supported data parallelism and vanilla model par-
allelism by explicitly assigning operations to speciﬁc devices.
However, they are not efﬁcient enough for giant model train-
ing. Megatron [43], GPipe [20], and Dapple [16] have pro-
posed new parallel training strategies to scale the training of
large-scale models. DeepSpeed [38] lacks general support for
tensor model parallelism, besides, model layers are required
to rewrite in sequential for pipeline parallelism. GShard [26]
supports operator splitting by introducing model weight an-
notations and tensor dimension speciﬁcations. The high per-
formance of those works is achieved by exposing low-level
system abstractions to users (e.g., device placement, equiva-
lent distributed implementation for operators), or enforcing
model or tensor partition manually, which results in signiﬁ-
cant user efforts. As a parallel work to Whale, GSPMD [51]
extends GShard by annotating tensor dimensions mapping for
both automatic and manual operator partitioning. As a gen-
eral giant model training framework, Whale adopts a uniﬁed
abstraction to express different parallel strategies and their hy-

11

 0 50 100 150 200 250 30081664128256Throughput(samples/s)Number of GPU requestbrid nests and combinations, utilizing high-level annotations
and pattern matching for operator splitting. Whale further
scales to M6-10T through automatically distributed graph
optimizations with the awareness of heterogeneous resources.
Zero [36, 37, 39] optimizes memory usage by removing
redundant GPU memory, ofﬂoading computation to the CPU
host, and utilizing non-volatile memory respectively. Recom-
putation [10] trades computation for memory by recomput-
ing tensors from checkpoints. Such memory optimization
approaches are orthogonal to Whale, which can be further
combined for giant model training efﬁciently.
Graph optimization. Deep learning is powered by
dataﬂow graphs with optimizations to rewrite the graph for
better performance, such as TensorFlow XLA [7], TVM [9],
Ansor [55], AStitish [56], etc. TASO [22] and PET [45] adopt
a graph substitution approach to optimize the computation
graph automatically. Those works mainly focus on the per-
formance of a single GPU, while Whale utilizes the graph
optimization approach for achieving efﬁcient performance in
distributed training. Tofu [46] and SOAP [23] also use graph
partition to produce distributed execution plans, but with a
high search cost. Whale utilizes the introduced annotations
to shrink the search space, thus making graph optimization
practical for giant model training at a trillion scale. Besides,
Whale extends the graph optimization approach to compli-
cated parallel strategies in a uniﬁed abstraction, capable of
pipeline parallelism, tensor model parallelism, and hybrid
parallelism.
Resource heterogeneity. Philly [21] reports the trace study
in multi-tenant GPU clusters of Microsoft and shows the ef-
fect of gang scheduling on job queuing. MLaaS [47] studies a
two-month trace of a heterogeneous GPU cluster in Alibaba
PAI. Gandiva [49] shows jobs are different in sensitivity to
allocated resources. Whale is capable of adapting to resource
heterogeneity, which can reduce the queuing delay of giant
model training with hundreds of GPUs. The design of Whale
advocates the approach of decoupling model programming
and distributed execution. It dynamically generates an efﬁ-
cient execution plan by considering the properties of both
model and heterogeneous resources.

7 Conclusion

Whale demonstrates the possibility of achieving efﬁciency,
programmability, and adaptability in a scalable deep learning
framework for training trillion-parameter models. Whale sup-
ports various parallel strategies using a uniﬁed abstraction,
hides distributed execution details through new primitive an-
notations, and adapts to heterogeneous GPUs with automatic
graph optimizations. Going forward, we hope that Whale can
become a large-scale deep learning training foundation to
further engage model algorithm innovations and system opti-
mizations in parallel, making giant model training technology
to be adopted easily and efﬁciently at scale.

Acknowledgements

We would like to thank our anonymous shepherd and review-
ers for their valuable comments and suggestions. We would
also like to thank the M6 team and all users of Whale for their
help and suggestions.

References

[1] Automatic mixed

precision

learn-
for
https://developer.nvidia.com/

deep

ing.
automatic-mixed-precision.

[2] Nvidia tesla p100.

https://www.nvidia.com/

en-us/data-center/tesla-p100/.

[3] Nvidia v100 tensor core gpu. https://www.nvidia.

com/en-us/data-center/v100/.

[4] NVLink.

https://www.nvidia.com/en-us/

data-center/nvlink/.

[5] Xla: Optimizing compiler for machine learning. https:

//www.tensorflow.org/xla.

[6] Nccl. https://developer.nvidia.com/nccl, 2019.

[7] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Geoffrey Irving, Michael Isard, et al.
Tensorﬂow: A system for large-scale machine learning.
In 12th USENIX symposium on operating systems de-
sign and implementation (OSDI 16), pages 265–283,
2016.

[8] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al. Language models are few-shot learners. arXiv
preprint arXiv:2005.14165, 2020.

[9] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin
Zheng, Eddie Yan, Haichen Shen, Meghan Cowan,
Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin,
and Arvind Krishnamurthy. TVM: An automated end-
to-end optimizing compiler for deep learning. In 13th
USENIX Symposium on Operating Systems Design and
Implementation (OSDI 18), pages 578–594, Carlsbad,
CA, October 2018. USENIX Association.

[10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos
Guestrin. Training deep nets with sublinear memory
cost. arXiv preprint arXiv:1604.06174, 2016.

[11] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and
Karthik Kalyanaraman. Project adam: Building an efﬁ-
cient and scalable deep learning training system. In 11th

12

USENIX Symposium on Operating Systems Design and
Implementation (OSDI 14), pages 571–582, Broomﬁeld,
CO, October 2014. USENIX Association.

[12] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai
Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao,
Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker,
Ke Yang, and Andrew Y. Ng. Large scale distributed
deep networks. In NIPS, 2012.

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv
preprint arXiv:1810.04805, 2018.

[14] Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene
Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton,
Alex Alemi, Matt Hoffman, and Rif A Saurous. Ten-
sorﬂow distributions. arXiv preprint arXiv:1711.10604,
2017.

[15] Alexey Dosovitskiy, Lucas

Beyer, Alexander
Zhai,
Kolesnikov, Dirk Weissenborn, Xiaohua
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image
recognition at scale. arXiv preprint arXiv:2010.11929,
2020.

[16] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu
Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun
Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei
Lin. Dapple: A pipelined data parallel approach for
training large models, 2020.

[17] William Fedus, Barret Zoph, and Noam Shazeer. Switch
transformers: Scaling to trillion parameter models with
simple and efﬁcient sparsity, 2021.

[18] Yanjie Gao, Yu Liu, Hongyu Zhang, Zhengxian Li,
Yonghao Zhu, Haoxiang Lin, and Mao Yang. Estimat-
ing gpu memory consumption of deep learning models.
In Proceedings of the 28th ACM Joint Meeting on Eu-
ropean Software Engineering Conference and Sympo-
sium on the Foundations of Software Engineering, pages
1342–1352, 2020.

[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016.

[21] Myeongjae Jeon, Shivaram Venkataraman, Amar Phan-
ishayee, Junjie Qian, Wencong Xiao, and Fan Yang.
Analysis of large-scale multi-tenant GPU clusters for
In 2019 USENIX Annual
DNN training workloads.
Technical Conference (USENIX ATC 19), pages 947–
960, 2019.

[22] Zhihao Jia, Oded Padon, James Thomas, Todd Warsza-
wski, Matei Zaharia, and Alex Aiken. Taso: optimizing
deep learning computation with automatic generation
of graph substitutions. In Proceedings of the 27th ACM
Symposium on Operating Systems Principles, pages 47–
62, 2019.

[23] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond
data and model parallelism for deep neural networks.
In Ameet Talwalkar, Virginia Smith, and Matei Zaharia,
editors, Proceedings of Machine Learning and Systems
2019, MLSys 2019, Stanford, CA, USA, March 31 - April
2, 2019. mlsys.org, 2019.

[24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scal-
ing laws for neural language models. arXiv preprint
arXiv:2001.08361, 2020.

[25] Alex Krizhevsky. One weird trick for paralleliz-
arXiv preprint

ing convolutional neural networks.
arXiv:1404.5997, 2014.

[26] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, De-
hao Chen, Orhan Firat, Yanping Huang, Maxim Krikun,
Noam Shazeer, and Zhifeng Chen. Gshard: Scaling gi-
ant models with conditional computation and automatic
sharding. arXiv preprint arXiv:2006.16668, 2020.

[27] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar,
Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith,
Brian Vaughan, Pritam Damania, and Soumith Chintala.
Pytorch distributed: Experiences on accelerating data
parallel training. Proc. VLDB Endow., 13(12):3005–
3018, 2020.

[28] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming
Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang,
Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang
Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou,
Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie
Tang, and Hongxia Yang. M6: A chinese multimodal
pretrainer, 2021.

[20] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan
Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee,
Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe:
Efﬁcient training of giant neural networks using pipeline
parallelism. arXiv preprint arXiv:1811.06965, 2018.

[29] Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang,
Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin,
et al. M6-10t: A sharing-delinking paradigm for efﬁ-
cient multi-trillion parameter pretraining. arXiv preprint
arXiv:2110.03888, 2021.

13

[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,
Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using
shifted windows. arXiv preprint arXiv:2103.14030,
2021.

[39] Jie Ren, Samyam Rajbhandari, Reza Yazdani Am-
inabadi, Olatunji Ruwase, Shuangyan Yang, Minjia
Zhang, Dong Li, and Yuxiong He. Zero-ofﬂoad: De-
mocratizing billion-scale model training. arXiv preprint
arXiv:2101.06840, 2021.

[31] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gre-
gory Diamos, Erich Elsen, David Garcia, Boris Ginsburg,
Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh,
arXiv preprint
et al. Mixed precision training.
arXiv:1710.03740, 2017.

[32] Deepak Narayanan, Aaron Harlap, Amar Phanishayee,
Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger,
Phillip B Gibbons, and Matei Zaharia. Pipedream: gen-
eralized pipeline parallelism for dnn training. In Pro-
ceedings of the 27th ACM Symposium on Operating
Systems Principles, pages 1–15, 2019.

[33] Deepak Narayanan, Mohammad Shoeybi, Jared Casper,
Patrick LeGresley, Mostofa Patwary, Vijay Anand Kor-
thikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
Bernauer, Bryan Catanzaro, et al. Efﬁcient large-scale
language model training on gpu clusters. arXiv preprint
arXiv:2104.04473, 2021.

[34] Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
Pytorch: An imperative style, high-performance deep
learning library. arXiv preprint arXiv:1912.01703, 2019.

[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, and Peter J Liu. Exploring the limits of transfer
learning with a uniﬁed text-to-text transformer. arXiv
preprint arXiv:1910.10683, 2019.

[36] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and
Yuxiong He. Zero: Memory optimizations toward train-
ing trillion parameter models. In SC20: International
Conference for High Performance Computing, Network-
ing, Storage and Analysis, pages 1–16. IEEE, 2020.

[37] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley,
Shaden Smith, and Yuxiong He. Zero-inﬁnity: Breaking
the gpu memory wall for extreme scale deep learning.
arXiv preprint arXiv:2104.07857, 2021.

[38] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. Deepspeed: System optimizations enable
training deep learning models with over 100 billion pa-
rameters. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining, pages 3505–3506, 2020.

[40] Alexander Sergeev and Mike Del Balso. Horovod: fast
and easy distributed deep learning in tensorﬂow. arXiv
preprint arXiv:1802.05799, 2018.

[41] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin
Tran, Ashish Vaswani, Penporn Koanantakool, Peter
Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff
Young, et al. Mesh-tensorﬂow: Deep learning for super-
computers. In Advances in Neural Information Process-
ing Systems, pages 10414–10423, 2018.

[42] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive
learning rates with sublinear memory cost. In Interna-
tional Conference on Machine Learning, pages 4596–
4604. PMLR, 2018.

[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
Megatron-lm: Training multi-billion parameter lan-
guage models using model parallelism. arXiv preprint
arXiv:1909.08053, 2019.

[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. arXiv
preprint arXiv:1706.03762, 2017.

[45] Haojie Wang, Jidong Zhai, Mingyu Gao, Zixuan Ma,
Shizhi Tang, Liyan Zheng, Yuanzhi Li, Kaiyuan Rong,
Yuanyong Chen, and Zhihao Jia. PET: Optimizing ten-
sor programs with partially equivalent transformations
In 15th USENIX Sympo-
and automated corrections.
sium on Operating Systems Design and Implementation
(OSDI 21), pages 37–54, 2021.

[46] Minjie Wang, Chien-Chin Huang, and Jinyang Li. Sup-
porting very large models using automatic dataﬂow
In George Candea, Robbert van
graph partitioning.
Renesse, and Christof Fetzer, editors, Proceedings of
the Fourteenth EuroSys Conference 2019, Dresden, Ger-
many, March 25-28, 2019, pages 26:1–26:17. ACM,
2019.

[47] Qizhen Weng, Wencong Xiao, Yinghao Yu, Wei Wang,
Cheng Wang, Jian He, Yong Li, Liping Zhang, Wei Lin,
and Yu Ding. MLaaS in the wild: Workload analysis and
scheduling in large-scale heterogeneous gpu clusters. In
19th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 22). USENIX Association,
2022.

14

Wei Lin. Astitch: Enabling a new multi-dimensional
optimization space for memory-intensive ml training
In Pro-
and inference on modern simt architectures.
ceedings of the 27th ACM International Conferenceon
Architectural Support for Programming Languages and
Operating Systems. ACM, 2022.

[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
Google’s neural machine translation system: Bridging
the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

[49] Wencong Xiao, Romil Bhardwaj, Ramachandran Ram-
jee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han,
Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang,
Fan Yang, and Lidong Zhou. Gandiva: Introspective
cluster scheduling for deep learning. In 13th USENIX
Symposium on Operating Systems Design and Imple-
mentation, OSDI 2018, Carlsbad, CA, USA, October 8-
10, 2018, pages 595–610. USENIX Association, 2018.

[50] Wencong Xiao, Shiru Ren, Yong Li, Yang Zhang,
Pengyang Hou, Zhi Li, Yihui Feng, Wei Lin, and
Yangqing Jia. Antman: Dynamic scaling on GPU clus-
ters for deep learning. In 14th USENIX Symposium on
Operating Systems Design and Implementation (OSDI
20), pages 533–548. USENIX Association, November
2020.

[51] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake
Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun,
Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al.
Gspmd: General and scalable parallelization for ml
computation graphs. arXiv preprint arXiv:2105.04663,
2021.

[52] Linting Xue, Noah Constant, Adam Roberts, Mihir
Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,
and Colin Raffel. mt5: A massively multilingual
arXiv preprint
pre-trained text-to-text transformer.
arXiv:2010.11934, 2020.

[53] An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang,
Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong
Li, et al. Exploring sparse expert models and beyond.
arXiv preprint arXiv:2105.15082, 2021.

[54] Bowen Yang, Jian Zhang, Jonathan Li, Christopher Ré,
Christopher Aberger, and Christopher De Sa. Pipemare:
Asynchronous pipeline parallel dnn training. Proceed-
ings of Machine Learning and Systems, 3, 2021.

[55] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu,
Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang,
Danyang Zhuo, Koushik Sen, et al. Ansor: Generating
high-performance tensor programs for deep learning. In
14th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 20), pages 863–879, 2020.

[56] Zhen Zheng, Xuanda Yang, Pengzhan Zhao, Guoping
Long, Kai Zhu, Feiwen Zhu, Wenyi Zhao, Xiaoyong
Liu, Jun Yang, Jidong Zhai, Shuaiwen Leon Song, and

15

