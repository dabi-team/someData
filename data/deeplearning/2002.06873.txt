Springer Nature 2021 LATEX template

πVAE: a stochastic process prior for Bayesian deep learning
with MCMC

Swapnil Mishra1,2*†, Seth Flaxman3†, Tresnia Berah4, Harrison Zhu4, Mikko
Pakkanen4 and Samir Bhatt1,2†

1*MRC Centre for Global Infectious Disease Analysis, Jameel Institute for Disease and
Emergency Analytics, School of Public Health, Imperial College London, London, UK.
2Section of Epidemiology, Department of Public Health, University of Copenhagen,
Copenhagen, Denmark.
3Department of Computer Science, University of Oxford, Oxford, UK.
4Department of Mathematics, Imperial College London, London, UK.

*Corresponding author(s). E-mail(s): s.mishra@imperial.ac.uk;
†These authors contributed equally to this work.

Abstract

Stochastic processes provide a mathematically elegant way to model complex data. In theory, they
provide ﬂexible priors over function classes that can encode a wide range of interesting assumptions.
However, in practice eﬃcient inference by optimisation or marginalisation is diﬃcult, a problem
further exacerbated with big data and high dimensional input spaces. We propose a novel varia-
tional autoencoder (VAE) called the prior encoding variational autoencoder (πVAE). πVAE is a
new continuous stochastic process. We use πVAE to learn low dimensional embeddings of function
classes by combining a trainable feature mapping with generative model using a VAE. We show
that our framework can accurately learn expressive function classes such as Gaussian processes,
but also properties of functions such as their integrals. For popular tasks, such as spatial interpo-
lation, πVAE achieves state-of-the-art performance both in terms of accuracy and computational
eﬃciency. Perhaps most usefully, we demonstrate an elegant and scalable means of performing fully
Bayesian inference for stochastic processes within probabilistic programming languages such as Stan.

Keywords: Bayesian inference, MCMC, VAE, Spatio-temporal

2
2
0
2

p
e
S
3
1

]

G
L
.
s
c
[

6
v
3
7
8
6
0
.
2
0
0
2
:
v
i
X
r
a

1 Introduction

A central task in machine learning is to specify
a function or set of functions that best gener-
alises to new data. Stochastic processes (Pavliotis,
2014; Ross, 1996) provide a mathematically ele-
gant way to deﬁne a class of functions, where each
element from a stochastic process is a (usually
inﬁnite) collection of random variables. Popular

examples of stochastic processes in computational
statistics and machine learning are Gaussian pro-
cesses (Rasmussen & Williams, 2006), Dirichlet
processes (Antoniak, 1974),
log-Gaussian Cox
processes (Møller, Syversveen, & Waagepetersen,
1998), Hawkes processes (Hawkes, 1971), Mondrian
processes (Roy & Teh, 2009) and Gauss-Markov
processes (Lindgren, Rue, & Lindström, 2011).
Many of these processes are intimately connected

1

 
 
 
 
 
 
2

πVAE

Springer Nature 2021 LATEX template

with popular techniques in deep learning, for
example, both the inﬁnite width limit of a sin-
gle layer neural network and the evolution of
a deep neural network by gradient descent are
Gaussian processes (Jacot, Gabriel, & Hongler,
2018; R. Neal, 1996). However, while stochastic
processes have many favourable properties, they
are often cumbersome to work with in practice.
For example, inference and prediction using a
Gaussian process requires matrix inversions that
scale cubicly with data size, log-Gaussian Cox
processes require the evaluation of an intractable
integral and Markov processes are often highly
correlated. Bayesian inference can be even more
challenging due to complex high dimensional pos-
terior topologies. Gold standard evaluation of
posterior expectations is done by Markov Chain
Monte Carlo (MCMC) sampling, but high auto-
correlation, narrow typical sets (Betancourt, Byrne,
Livingstone, & Girolami, 2017) and poor scalabil-
ity have prevented use in big data and complex
model settings. A plethora of approximation algo-
rithms exist (Blundell, Cornebise, Kavukcuoglu,
& Wierstra, 2015; Lakshminarayanan, Pritzel, &
Blundell, 2017; Minka, 2001; Ritter, Botev, &
Barber, 2018; Welling & Teh, 2011), but few actu-
ally yield accurate posterior estimates (Hoﬀman,
Blei, Wang, & Paisley, 2013; Huggins, Kasprzak,
Campbell, & Broderick, 2019; J. Yao, Pan, Ghosh,
& Doshi-Velez, 2019; Y. Yao, Vehtari, Simp-
son, & Gelman, 2018). In this paper, rather
than relying on approximate Bayesian inference
to solve complex models, we extend variational
autoencoders (VAE) (Kingma & Welling, 2014;
Rezende, Mohamed, & Wierstra, 2014) to develop
portable models that can work with state-of-the-art
Bayesian MCMC software such as Stan (Car-
penter et al., 2017). Inference on the resulting
models is tractable and yields accurate posterior
expectations and uncertainty.

An autoencoder (Hinton & Salakhutdinov,
2006) is a model comprised of two component net-
works. The encoder e : X → Z encodes inputs
from space X into a latent space Z of lower dimen-
sion than X . The decoder d : Z → X decodes
latent codes in Z to reconstruct the input. The
parameters of e and d are learned through the
minimisation of a reconstruction loss on a training
dataset. A VAE extends the autoencoder into a
generative model (Kingma & Welling, 2014). In a
VAE, the latent space Z is given a distribution,

such as standard normal, and a variational approxi-
mation to the posterior is estimated. In a variety of
applications, VAEs do a superb job reconstructing
training datasets and enable the generation of new
data: samples from the latent space are decoded to
generate synthetic data (Kingma & Welling, 2019).
In this paper we propose a novel use of VAEs: we
learn low-dimensional representations of samples
from a given function class (e.g. sample paths from
a Gaussian process prior). We then use the result-
ing low dimensional representation and the decoder
to perform Bayesian inference.

One key beneﬁt of this approach is that we
decouple the prior from inference to encode arbi-
trarily complex prior function classes, without
needing to calculate any data likelihoods. A second
key beneﬁt is that when inference is performed,
our sampler operates in a low dimensional, uncor-
related latent space which greatly aids eﬃciency
and computation, as demonstrated in the spatial
statistics setting in PriorVAE (Semenova et al.,
2022). One limitation of this approach (and of
PriorVAE) is that we are restricted to encoding
ﬁnite-dimensional priors, because VAEs are not
stochastic processes. To overcome this limitation,
we take as inspiration the Karhunen-Loève decom-
position of a stochastic process as a random linear
combination of basis functions and introduce a
new VAE called the prior encoding VAE (πVAE).
πVAE is a valid stochastic process by construction,
it is capable of learning a set of basis functions,
and it incorporates a VAE, enabling simulation
and highly eﬀective fully Bayesian inference.

We employ a two step approach: ﬁrst, we encode
the prior using our novel architecture; second we
use the learnt basis and decoder network—a new
stochastic process in its own right—as a prior,
combining it with a likelihood in a fully Bayesian
modeling framework, and use MCMC to ﬁt our
model and infer the posterior. We believe our frame-
work’s novel decoupling into two stages is critically
important for many complex scenarios, because
we do not need to compromise in terms of either
the expressiveness of deep learning or accurately
characterizing the posterior using fully Bayesian
inference.

We thus avoid some of the drawbacks of other
Bayesian deep learning approaches which rely
solely on variational inference, and the drawbacks

Springer Nature 2021 LATEX template

πVAE

3

of standard MCMC methods for stochastic pro-
cesses which are ineﬃcient and suﬀer from poor
convergence.

Taken together, our work is an important
advance in the ﬁeld of Bayesian deep learning,
providing a practical framework combining the
expressive capability of deep neural networks to
encode stochastic processes with the eﬀectiveness
of fully Bayesian and highly eﬃcient gradient-
based MCMC inference to ﬁt to data while fully
characterizing uncertainty.

Once a πVAE is trained and deﬁned, the com-
plexity of the decoder scales linearly in the size of
the largest hidden layer. Additionally, because the
latent variables are penalised via the KL term from
deviating from a standard normal distribution, the
latent space is approximately uncorrelated, leading
to high eﬀective sample sizes in MCMC sampling.
The main contributions of this paper are:

• We apply the generative framework of VAEs to
perform full Bayesian inference. We ﬁrst encode
priors in training and then, given new data,
perform inference on the latent representation
while keeping the trained decoder ﬁxed.

• We propose a new generative model, πVAE,
that generalizes VAEs to be able to learn priors
over both functions and properties of functions.
We show that πVAE is a valid (and novel)
stochastic process by construction.

• We show the performance of πVAE on a range
of simulated and real data, and show that
πVAE achieves state-of-the-art performance in
a spatial interpolation task.

The rest of this paper is structured as follows.
Section 2 details the proposed framework and the
generative model along with toy ﬁtting examples.
The experiments on large real world datasets are
outlined in Section 3. We discuss our ﬁndings and
conclude in Section 4.

2 Methods

2.1 Variational Autoencoders

(VAEs)

A standard VAE has three components:

1. an encoder network e(x, γ) which encodes inputs

x ∈ X using learnable parameters γ,

2. random variables z for the latent subspace,

3. a decoder network d(z, ψ) which decodes latent
embeddings z using learnable parameters ψ.

In the simplest case we are given inputs x ∈ Rd =
X such as a ﬂattened image or discrete time series.
The encoder e(x, γ) and decoder d(z, ψ) are fully
connected neural networks (though they could
include convolution or recurrent layer). The output
of the encoder network are vectors of mean and
standard deviation parameters zµ and zsd. These
vectors can thus be used to deﬁne the random
variable Z for the latent space:

[zµ, zsd](cid:62) = e(x, γ)

Z ∼ N (zµ, z2
sd

I)

(1)

(2)

For random variable Z, the decoder network
reconstructs the input by producing ˆx:

ˆx = d(Z, ψ)

(3)

To train a VAE, a variational approximation is

used to estimate the posterior distribution

p(Z | x, γ, ψ) ∝ p(x | Z, γ, ψ) × p(Z)

The variational approximation greatly simpliﬁes
inference by turning a marginalisation problem into
an optimisation problem. Following (Kingma &
Ba, 2014), the optimal parameters for the encoder
and decoder are found by maximising the evidence
lower bound:

(cid:20)

EZ

arg max
γ,ψ

log p (x | Z, γ, ψ)

− KL (Z (cid:107) N (0, I))

(cid:21)

(4)

The ﬁrst term in Eq. (4) is the likelihood quan-
tifying how well ˆx matches x. In practice we can
simply adopt the mean squared error loss directly,
referred to as the reconstruction loss, without tak-
ing a probabilistic perspective. The second term is
a Kullback-Leibler divergence to ensure that Z is
as similar as possible to the prior distribution, a
standard normal. Again, this second term can be
speciﬁed directly without the evidence lower bound
derivation: we view the KL-divergence as a regular-
ization penalty to ensure that the latent parameters
are approximately uncorrelated by penalizing how
far they deviate from N (0, I).

4

πVAE

Springer Nature 2021 LATEX template

Once training is complete, we ﬁx ψ, and use
the decoder as a generative model. To simplify
subsequent notation we refer to a fully trained
decoder as d(z). Generating a new sample is simple:
ﬁrst draw a random variable Z ∼ N (0, I) and
then apply the decoder, which is a deterministic
transformation to obtain d(Z). We see immediately
that d(Z) is itself a random variable. In the next
section, we will use this generative model as a
prior in a Bayesian framework by linking it to a
likelihood to obtain a posterior.

2.2 VAEs for Bayesian inference

VAEs have been typically used in the literature to
create or learn a generative model of observed data
(Kingma & Welling, 2014), such as images. (Semen-
ova et al., 2022) introduced a novel application of
VAEs in a Bayesian inference setting, using a two
stage approach that is closely related to ours. In
brief, in the ﬁrst stage, a VAE is trained to encode
and decode a large dataset of vectors consisting of
samples drawn from a speciﬁed prior p(θ) over ran-
dom vectors. In the second stage, the original prior
is replaced with the approximate prior: θ := d(Z)
where Z ∼ N (0, I).

To see how this works in a Bayesian infer-
ence setting, consider a likelihood p(y | θ) linking
the parameter θ to data y. Bayes’ rule gives the
unnormalized posterior:

p(θ | y) ∝ p(y | θ) × p(θ)

(5)

The trained decoder serves as a drop-in replace-
ment for the original prior class in a Bayesian
setting:

p(Z | y, d) ∝ p(y | d(Z)) × p(Z) .

(6)

The implementation within a probabilistic pro-
gramming language is very straightforward: a
standard normal prior and deterministic function
(the decoder) are all that is needed.

It is useful to contrast the inference task from
Eq. (6) to a Bayesian neural network (BNN)
(R. Neal, 1996) or Gaussian process in primal form
(Rahimi & Recht, 2008). In a BNN with parame-
ters ω and hyperparameters λ, the unnormalised
posterior would be

p(ω, λ | y) ∝ p(y | ω, λ) × p(ω | λ) × p(λ) .

(7)

The key diﬀerence between Eq. (7) and Eq. (6) is
the term p(ω | λ). The dimension of ω is typically
huge, sometimes in the millions, and is conditional
on λ, whereas in Eq. (6) the latent dimension
of Z is typically small (< 50), uncorrelated and
unconditioned. Full batch MCMC training is typ-
ically prohibitive for BNNs due to large datasets
and the high-dimensionality of ω, but approximate
Bayesian inference algorithms tend to poorly cap-
ture the complex posterior (J. Yao et al., 2019;
Y. Yao et al., 2018). Additionally, ω tends to be
highly correlated, making eﬃcient MCMC nearly
impossible. Finally, as the dimension and depth
increases, the posterior distribution suﬀers from
complex multimodality, and concentration to a
narrow typical set (Betancourt et al., 2017). By
contrast, oﬀ-the-shelf MCMC methods are very
eﬀective for equation (6) because the prior space
they need to explore is as simple as it could be:
a standard normal distribution, while the com-
plexity of the model lives within the deterministic
(and diﬀerentiable) decoder. In a challenging spa-
tial statistics setting, (Semenova et al., 2022) used
this approach and achieved MCMC eﬀective sam-
ple sizes exceeding actual sample sizes, due to the
incredible eﬃciency of the MCMC sampler.

An example of using VAEs to perform infer-
ence is shown in Figure 1 where we train a VAE
with latent dimensionality 10 on samples drawn
from a zero mean Gaussian process with RBF
kernel (K(δ) = e−δ2/82
) observed on the grid
0, 0.01, 0.02, . . . , 1.0. In Figure 1 we closely recover
the true function and correctly estimate the data
noise parameter. Our MCMC samples showed vir-
tually no autocorrelation, and all diagnostic checks
were excellent (see Appendix). Solving the equiva-
lent problem using a Gaussian process prior would
not only be considerably more expensive (O(n3))
but correlations in the parameter space would com-
plicate MCMC sampling and necessitate very long
chains to achieve even modest eﬀective sample
sizes.

This example demonstrates the promise that
VAEs hold to improve Bayesian inference by
encoding function classes in a two stage process.
While this simple example proved useful in some
settings (Semenova et al., 2022), inference and
prediction is not possible at new input locations,
because a VAE is not a stochastic process. As
described above, a VAE provides a novel prior over
random vectors. Below, we take the next step by

Springer Nature 2021 LATEX template

πVAE

5

(a)

(b)

Fig. 1: Learning functions with VAE: (a) Prior samples from a VAE trained on Gaussian process samples
(b) we ﬁt our VAE model to data drawn from a GP (blue) plus noise (black points). The posterior mean
of our model is in red with the 95% epistemic credible intervals shown in purple.

introducing πVAE, a new stochastic process capa-
ble of approximating useful and widely used priors
over function classes, such as Gaussian processes.

2.3 Encoding stochastic processes

with πVAE

To create a model with the ability to perform
inference on a wide range of problems we have to
ensure that it is a valid stochastic process. Pre-
vious attempts in deep learning in this direction
have been inspired by the Kolmogorov Extension
Theorem and have focused on extending from a
ﬁnite-dimensional distribution to a stochastic pro-
cess. Speciﬁcally, (Garnelo et al., 2018) introduced
an aggregation step (typically an average) to cre-
ate an order invariate global distribution. However,
as noted by (Kim et al., 2019), this can lead to
underﬁtting.

by

We take a diﬀerent approach with πVAE,
inspired
the Karhunen-Loève Expan-
sion (Karhunen, 1947; Loeve, 1948). Recall that a
centered stochastic process f (s) can be written as
an inﬁnite sum:

f (s) =

∞
(cid:88)

j=1

βjφj(s)

(8)

for pairwise uncorrelated random variables βj
and continuous real-valued functions forming an
orthonormal basis φj(s). The random βj’s provide
a linear combination of a ﬁxed set of basis func-
tions, φj. This perspective has a long history in
neural networks, cf. radial basis function networks.
What if we consider a trainable, deep learn-
ing parameterization of Eq. (8) as inspiration? We
need to learn deterministic basis functions while

allowing the βj’s to be random. Let Φ(s) be a fea-
ture mapping with weights w, i.e. a feed-forward
neural network architecture over the input space,
representing the basis functions. Let β be a vector
of weights on the basis functions, so f (s) = β(cid:62)Φ(s).
We use a VAE architecture to encode and decode
β, meaning we maintain the random variable per-
spective and at the same time learn a ﬂexible
low-dimensional non-linear generative model.

How can we specify and train this model? As
with the VAE in the previous section, πVAE is
trained on draws from a prior. Our goal is to encode
a stochastic process prior Π, so we consider i =
1, . . . , N function realizations denoted fi(s). Each
fi(s) is an inﬁnite dimensional object, a function
deﬁned for all s, so we further assume that we
are given a ﬁnite set of Ki observation locations.
We set Ki = K for simplicity of implementation
i.e. the number of evaluations for each function is
constant across all draws i. We denote the observed
values as yk
i ). The training dataset thus
consists of N sets of K observation locations and
function values:

i := fi(sk

{(s1

i , y1

i ) . . . , (sK

i , yK

i )}N
i=1

Note that the set of K observation locations varies
across the N realizations.

We now return to the architecture of πVAE
(Fig. 2). The feature mapping Φ(s) is shared across
all i = 1, . . . , N function draws, so it consists of a
feedforward neural network and is parameterized
by a set of global parameters w which must be
learned. However, a particular random realization
fi(s) is represented by a random vector βi, for
which we use a VAE architecture. We note the
following non-standard setup: βi is a learnable

0.00.20.40.60.81.0210120.00.20.40.60.81.01012TruePosterior meanObservations95% Credible Interval6

πVAE

Springer Nature 2021 LATEX template

parameter of our model, but it is also the input to
the encoder of the VAE. The decoder attempts to
reconstruct βi with an output ˆβi. We denote the
encoder and decoder as:

[zµ, zsd](cid:62) = e(β, γ)

Z ∼ N (zµ, z2
sd
ˆβ = d(Z, ψ)

I)

(9)

(10)

(11)

We are now ready to express the loss, which
combines the two parts of the network, summing
across all observations. Rather than deriving an
evidence lower bound, we proceed directly to spec-
ify a loss function, in three parts. In the ﬁrst, we
use MSE to check the ﬁt of the βi’s and Φ to the
data:

Loss 1 :

1
N K

(cid:88)

i,k

(yk

i − β(cid:62)

i Φ(sk

i ))2

In the second, we use MSE to check the ﬁt of the
reconstructed ˆβi’s and Φ to the data:

Loss 2 :

1
N K

(cid:88)

i,k

(yk

i − ˆβ(cid:62)

i Φ(sk

i ))2

existing stochastic processes: we can obtain a Gaus-
sian process with kernel k(·, ·) using a single-layer
linear VAE for β (meaning the βs are simply stan-
dard normals) and setting Φ(s) = L(cid:62)s for L the
Cholesky decomposition of the Gram matrix K
where Kij = k(si, sj).

In contrast to a standard VAE encoder that
takes as input the data to be encoded, πVAE
ﬁrst transforms input data (locations) to a higher
dimensional feature space via Φ, and then con-
nects this feature space to outputs, y, through a
linear mapping, β. The πVAE decoder takes out-
puts from the encoder, and attempts to recreate
β from a lower dimensional probabilistic embed-
ding. This re-creation, ˆβ, is then used as a linear
mapping with the same Φ to get a reconstruction
of the outputs y. It is crucial to note that a sin-
gle global β vector is not learnt. Instead, for each
function i = 1, . . . , N a βi is learnt.

In terms of number of parameters, we need to
learn w, γ, ψ, β1, . . . , βN . While this may seem like
a huge computational task, K is typically quite
small (< 200) and so learning can be relatively
quick (dominated by matrix multiplication of hid-
den layers). Algorithm 1 in the Appendix presents
the step-by-step process of training πVAE.

We also require the standard variational loss:

2.3.1 Simulation and Inference with

KL (Z (cid:107) N (0, I))

Note that we do not consider reconstruction loss
(cid:107)βi − ˆβi(cid:107)2 because in practice this did not improve
training.

To provide more intuition: the feature map Φ(s)
transforms each observed location to a ﬁxed fea-
ture space that is shared for all locations across
all functions. Φ(s) could be an explicit feature
representation for an RKHS (e.g. an RBF net-
work or a random Fourier feature basis (Rahimi
& Recht, 2008)), a neural network of arbitrary
construction or, as we use in the examples in
this paper, a combination of both. Following this
transformation, a linear basis β (which we obtain
from a non-linear decoder network) is used to pre-
dict function evaluations at an arbitrary location.
The intuition behind these two transformations
is to learn the association between locations and
observations while allowing for randomness—Φ
provides the correlation structure over space and
β the randomness. Explicit choices can lead to

πVAE

Given a trained embedding Φ(·) and trained
decoder d(z), we can use πVAE as a generative
model to simulate sample paths f as follows. A
single function f is obtained by ﬁrst drawing
Z ∼ N (0, I) and deﬁning f (s) := d(Z)(cid:62)Φ(s). For
a ﬁxed Z, f (s) is a deterministic function—a sam-
ple path from πVAE deﬁned for all s. Varying Z
produces diﬀerent sample paths. Computationally,
f can be eﬃciently evaluated at any arbitrary loca-
tion s using matrix algebra: f (s) = d(Z)(cid:62)Φ(s).
We remark that the stochastic process perspec-
tive is readily apparent: for a random variable Z,
d(Z)(cid:62)Φ(s) is a random variable deﬁned on the
same probability space for all s.

Algorithm 3 in the Appendix presents the step-

by-step process for simulation with πVAE.

πVAE can be used for inference on new data
pairs (sj, yj), where the unnormalised posterior
distribution is

p(Z | d, yj, sj, Φ) ∝ p(yj | d, sj, Z, Φ)p(Z)

(12)

Springer Nature 2021 LATEX template

πVAE

7

Fig. 2: Schematic description of end-to-end trainig procedure for πVAE including the reconstruction loss.
Dashed arrows contribute to the loss, blue circles are reconstructions, and grey boxes are functions.

with likelihood p(yj | d, sj, Z, Φ) and prior p(Z).
MCMC can be used to eﬃciently obtain sam-
ples from the posterior distribution over Z using
Equation (12). An implementation in probabilistic
programming languages such as Stan (Carpenter
et al., 2017) is very straightforward.

The posterior predictive distribution of yj at a

location sj is given by:

p(yj | d, sj, Φ) =
(cid:90)

p(yj | d, sj, Φ, Z)p(Z | d, yj, sj, Φ)dZ

(13)

While equations Eqs. (12)-(13) are written for
a single location sj, we can extend them to any
arbitrary collection of locations without loss of
generality, a necessary condition for πVAE to be a
valid stochastic process. Further, the distinguishing
diﬀerence between Eq. (6) and Eqs. (12)-(13) is
conditioning on input locations and Φ. It is Φ that
ensures πVAE is a valid stochastic process. We
formally prove this below.

Algorithm 2 in the Appendix presents the step-

by-step process for inference with πVAE.

2.3.2 πVAE is a stochastic process

Claim. πVAE is a stochastic process.
Recall that, mathematically, a stochastic pro-
cess is deﬁned as a collection {f (s) : s ∈ S}, where
f (s) for each location s ∈ S is a random vari-
able on a common probability space (Ω, F, P ), see,
e.g., Pavliotis (2014, Deﬁnition 1.1). This tech-
nical requirement is necessary to ensure that for
any locations s1, . . . , sn ∈ S, the random variables
f (s1), . . . , f (sn) have a well-deﬁned joint distri-
bution. Subsequently, it also ensures consistency.
Namely, writing fi := f (si) and integrating fn out,
we get

p(f1, . . . , fn−1) =

(cid:90)

fn

p(f1, . . . , fn)dfn.

Proof. For πVAE, we have f (·) := d(Z)Φ(·),
where Z is a multivariate Gaussian random vari-
able, hence deﬁned on some probability space
(Ω, F, P ). Since d and Φ are deterministic (measur-
able) functions, it follows that f (si) := d(Z)Φ(si)
for any i = 1, . . . , n, is a random variable on
(Ω, F, P ), whereby {f (s) : s ∈ S} is a stochastic
process. (cid:4)

We remark here that πVAE is a new stochastic
process. If πVAE is trained on samples from a zero

Training πVAE{fi}Ni=1(ski)Kk=1Encoder̂βiβie(⋅,γ)μzσzd(⋅,ψ)DecoderEmbeddingπVAE Loss = Loss 1  + Loss 2  + Regularisation LossLoss 1=[yki−βTiΦ(ski)]2:MSE from approximating         by ykiβTiΦ(ski)Loss 2=yki−̂βTiΦ(ski)]2:MSE from approximating         by ̂βTiΦ(ski)ykiRegLoss=DKL(N(μz,σz)||N(0,𝕀))Reconstructed yki≈̂βTiΦ(ski)yki≈βTiΦ(ski)K locationsFunction  evaluationsyki=fi(ski)N functions]Φ()ski⋅Training data8

πVAE

Springer Nature 2021 LATEX template

mean Gaussian process with a squared exponen-
tial covariance function, and similarly choose Φ to
have the same covariance function, and d is linear,
then πVAE will be a Gaussian process. But for a
non-positive deﬁnite Φ and / or non-linear d, even
if πVAE is trained on samples from a Gaussian pro-
cess, it will not truly be a Gaussian process, but
some other stochastic process which approximates
a Gaussian process. We do not know the theoret-
ical conditions under which πVAE will perform
better or worse than existing classes of stochastic
processes; its general construction means that the-
oretical results will be challenging to prove in full
generality. We demonstrate below that in practice,
πVAE performs very well.

2.4 Examples

We ﬁrst demonstrate the utility of our proposed
πVAE model by ﬁtting the simulated 1-D regres-
sion problem introduced in (Hernández-Lobato &
Adams, 2015). The training points for the dataset
are created by uniform sampling of 20 inputs, x,
between (−4, 4). The corresponding output is set
as y ∼ N (x3, 9). We ﬁt two diﬀerent variants of
πVAE, representing two diﬀerent prior classes of
functions. The ﬁrst prior produces cubic monotonic
functions and the second prior is a GP with an
RBF kernel and a two layer neural network. We
generated 104 diﬀerent function draws from both
priors to train the respective πVAE. One important
consideration in πVAE is to chose a suﬃciently
expressive Φ, we used a RBF layer (see Appendix
D) with trainable centres coupled with two layer
neural network with 20 hidden units each. We com-
pare our results against 20,000 Hamiltonian Monte
Carlo (HMC) samples (R.M. Neal, 1993) imple-
mented using Stan (Carpenter et al., 2017). Details
of the implementation for all the models can be
found in the Appendix.

Figure 3(a) presents results for πVAE with a
cubic prior, Figure 3(b) with an RBF prior, and
Figure 3(c) for standard Gaussian processes ﬁt-
ting using an RBF kernel. The mean absolute
error (MAE) for all three methods are presented
in Table 1. Both, the mean estimates and the
uncertainty from πVAE variants, are closer, and
more constrained than the ones using Gaussian pro-
cesses with HMC. Importantly, πVAE with cubic
prior not only produces better point estimates
but is able to capture better uncertainty bounds.

Method

Test MAE

πVAE (cubic functions)
πVAE (Gaussian process with RBF kernel)
Gaussian process with RBF kernel

10.47
33.15
67.37

Table 1: Test results of ﬁtting to a cubic function
with noise y ∼ N (x3, 9).

We note that πVAE does not exactly replicate an
RBF Gaussian process, but does retain the main
qualitative features inherent to GPs - such as the
concentration of the posterior where there is data.
Despite πVAE ostensibly learning an RBF function
class, diﬀerences are to be expected from the VAE
low dimensional embedding. This simple example
demonstrates that πVAE can be used to incorpo-
rate domain knowledge about the functions being
modelled.

In many scenarios, learning just the mapping
of inputs to outputs is not suﬃcient as other func-
tional properties are required to perform useful
(interesting) analysis. For example, using point pro-
cesses requires knowing the underlying intensity
function, however, to perform inference we need to
calculate the integral of that intensity function too.
Calculating this integral, even in known analytical
form, is very expensive. Hence, in order to circum-
vent the issue, we use πVAE to learn both function
values and its integral for the observed events.
Figure 4 shows πVAE prediction for both the inten-
sity and integral of a simulated 1-D log-Gaussian
Cox Process (LGCP).

In order to train πVAE to learn from the func-
tion space of 1-D LGCP functions, we ﬁrst create a
training set by drawing 10,000 diﬀerent samples of
the intensity function using an RBF kernel for 1-D
LGCP. For each of the drawn intensity function,
we choose an appropriate time horizon to sample
80 observed events (locations) from the intensity
function. πVAE is trained on the sampled 80 loca-
tions with their corresponding intensity and the
integral. πVAE therefore outputs both the instan-
taneous intensity and the integral of the intensity.
The implementation details can be seen in the
Appendix. For testing, we ﬁrst draw a new intensity
function (1-D LGCP) using the same mechanism
used in training and sample 100 events (locations).
As seen in Figure 4 our estimated intensity is very
close to true intensity and even the estimated inte-
gral is close to the true integral. This example

Springer Nature 2021 LATEX template

πVAE

9

(a)

(b)

(c)

Fig. 3: Fitting to a cubic function with noise y ∼ N (x3, 9). (a) πVAE trained on a class of cubic functions,
(b) πVAE trained on samples from a Gaussian process with RBF kernel and (c) is a Gaussian process
with RBF kernel. All methods use Hamiltonian Markov Chain Monte Carlo for posterior inference.

(a)

(b)

Fig. 4: Inferring the intensity of a log-Gaussian Cox Process. (a) compares the posterior distribution of
the intensity estimated by πVAE to the true intensity function on train and test data. (b) compares the
posterior mean of the cumulative integral over time estimated by πVAE to the true cumulative integral
on train and test data.

shows that the πVAE approach can be used to
learn not only function evaluations but properties
of functions.

3 Results

Here we show applications of πVAE on three real
world datasets. In our ﬁrst example we use πVAE
to predict the deviation in land surface tempera-
ture in East Africa (Ton et al., 2018). We have the
deviation in land surface temperatures for ∼89,000
locations across East Africa. Our training data
consisted of 6,000 uniformly sampled locations.
Temperature was predicted using only the spatial
locations as inputs. Figure 5 and Table 2 shows
the results of the ground truth (a), our πVAE (b),
a full rank Gaussian process with Matérn kernel
(Gardner, Pleiss, Bindel, Weinberger, & Wilson,

2018) (c), and low rank Gauss Markov random
ﬁeld (GMRF) (a widely used approach in the ﬁeld
of geostatistics) with 1, 046 ( 1
6 th of the training
size) basis functions (Lindgren et al., 2011; Rue,
Martino, & Chopin, 2009) (d). We train our πVAE
model on 107 functions draws from 2-D GP with
small lengthscales between 10−5 to 2. Φ was set to
be a Matérn layer ( see Appendix D) with 1,000
centres followed by a two layer neural network of
100 hidden units in each layer. The latent dimen-
sion of πVAE was set to 20. As seen in Figure 5,
πVAE is able to capture small scale features and
produces a far better reconstruction than the both
full and low rank GP and despite having a much
smaller latent dimension of 20 vs 6,000 (full) vs
1,046 (low). The testing error for πVAE is substan-
tially better than the full rank GP which leads to
the question, why does πVAE perform so much

64202462001000100200300TruePostreior meanObservations95% Credible Interval64202462001000100200642024620010001002000102030405060Time Points050100150200250300IntensityTrue IntensityEstimated IntensityObserved LocationsTrain DataTest Data0102030405060Time Points0200040006000800010000Integral IntensityIntegral True IntensityIntegral Estimated IntensityTrain DataTest Data10

πVAE

Springer Nature 2021 LATEX template

(a)

(b)

(c)

(d)

Fig. 5: Deviation in land surface temperature for East Africa trained on 6000 random uniformly chosen
locations (Ton et al., 2018). Plots: (a) the data, (b) our πVAE approach (testing MSE: 0.38), (c) a full
rank GP with Matérn 3
2 kernel (testing MSE: 2.47), and (d) a low rank SPDE approximation with 1046
basis functions (Lindgren et al., 2011) and a Matérn 3
2 kernel (testing MSE: 4.36). πVAE not only has
substantially lower test error, it captures ﬁne scale features much better than Gaussian processes or neural
processes.

Method

Test MSE

Method

RMSE

NLL

Full rank GP
πVAE
low rank GMRF (basis = 1046)

2.47
0.38
4.36

Table 2: Test results for πVAE, a full rank GP,
and low rank GMRF on land surface temperature
for East Africa trained on 6000 random uniformly
chosen locations (Ton et al., 2018).

Full rank GP
πVAE
SGPR (m = 512)
SVGP (m = 1024)

0.099
0.112
0.273
0.268

-0.258
0.006
0.087
0.236

Table 3: Test results for πVAE, a full rank GP
and approximate algorithms SGPR and SVGP on
Kin40K.

better than a GP, despite being trained on samples
from a GP? One possible reason is that the extra
hidden layers in Φ create a much richer structure
that could capture elements of non-stationarity
(Ton et al., 2018). Alternatively, the ability to use
state-of-the-art MCMC and estimate a reliable
posterior expectation might create resilience to
overﬁtting. The training/testing error for πVAE
is 0.07/0.38, while the full rank GP is 0.002/2.47.
Therefore the training error is 37 times smaller in
the GP, but the testing error is only 6 times smaller
in πVAE suggesting that, despite marginalisation,
the GP is still overﬁtting.
3

the
Kin40K (Schwaighofer & Tresp, 2003) dataset
to state-of-the-art full and approximate GPs,
with results taken from (Wang et al., 2019). The
objective was to predict the distance of a robotic
arm from the target given the position of all 8
links present on the robotic arm. In total we have
40,000 samples which are divided randomly into

compares

πVAE

Table

on

2
3 training samples and 1
3 test samples. We train
πVAE on N = 107 functions drawn from an 8-D
GP, observed at K = 200 locations, where each
of the 8 dimensions had values drawn uniformly
from the range (−2, 2) and lengthscale varied
between 10−3 and 10. Once πVAE was trained on
the prior function we use it to infer the posterior
distribution for the training examples in Kin40K.
Table 3 shows results for RMSE and negative
log-likelihood (NLL) of πVAE against various GP
methods on test samples. The full rank GP results
reported in (Wang et al., 2019) are better than
those from πVAE, but we are competitive, and far
better than the approximate GP methods. We
also note that the exact GP is estimated via max-
imising the log marginal likelihood in closed form,
while πVAE performs full Bayesian inference; all
posterior checks yielded excellent convergence
measured via ˆR and eﬀective samples sizes. Cal-
ibration was checked using posterior predictive
intervals. For visual diagnostics see the Appendix.

Springer Nature 2021 LATEX template

πVAE

11

10% pixels

20% pixels

30% pixels

(a) Observation

(b) Observation

(c) Observation

(d) Sample 1

(e) Sample 1

(f) Sample 1

(g) Sample 2

(h) Sample 2

(i) Sample 2

(j) Sample 3

(k) Sample 3

(l) Sample 3

Fig. 6: MNIST reconstruction after observing only 10, 20 or 30% of pixels from original data.

Finally, we apply πVAE to the task of recon-
structing MNIST digits using a subset of pixels
from each image. Similar to the earlier tempera-
ture prediction task, image completion can also be
seen as a regression task in 2-D. The regression
task is to predict the intensity of pixels given the
pixel locations. We ﬁrst train neural processes on
full MNIST digits from the training split of the
dataset, whereas πVAE is trained on N = 106 func-
tions drawn from a 2-D GP. The latent dimension

of πVAE is set to be 40. As with previous examples,
the decoder and encoder networks are made up of
two layer neural networks. The hidden units for the
encoder are 256 and 128 for the ﬁrst and second
layer respectively, and the reverse for decoder.

Once we have trained πVAE we now use images
from the test set for prediction. Images in the
testing set are sampled in such a way that only 10,
20 or 30% of pixel values are observed. Inference
is performed with πVAE to predict the intensity

12

πVAE

Springer Nature 2021 LATEX template

at all other pixel locations using Eq. (13). As seen
from Figure 6, the performance of πVAE increases
with increase in pixel locations available during
prediction but still even with 10% pixels our model
is able to learn a decent approximation of the image.
The uncertainty in prediction can be seen from
the diﬀerent samples produced by the model for
the same data. As the number of given locations
increases, the variance between samples decreases
with quality of the image also increasing. Note that
results from neural processes, as seen in Figure C4,
look better than from πVAE. Neural processes
performed better in the MNIST case because they
were speciﬁcally trained on full MNIST digits from
the training dataset, whereas piVAE was trained
on the more general prior class of 2D GPs.

4 Discussion and Conclusion

In this paper we have proposed a novel VAE for-
mulation of a stochastic process, with the ability
to learn function classes and properties of func-
tions. Our πVAEs typically have a small (5-50) ,
uncorrelated latent dimension of parameters, so
Bayesian inference with MCMC is straightforward
and highly eﬀective at successfully exploring the
posterior distribution. This accurate estimation
of uncertainty is essential in many areas such as
medical decision-making.

πVAE combines the power of deep learning
to create high capacity function classes, while
ensuring tractable inference using fully Bayesian
MCMC approaches. Our 1-D example in Figure
3 demonstrates that an exciting use of πVAE is
to incorporate domain knowledge about the prob-
lem. Monotonicity or complicated dynamics can be
encoded directly into the prior (Caterini, Doucet,
& Sejdinovic, 2018) on which πVAE is trained.
Our log-Gaussian Cox Process example shows that
not only functions can be modelled, but also prop-
erties of functions such as integrals. Perhaps the
most surprising result is the performance of πVAE
on spatial interpolation. Despite being trained on
samples from a Gaussian process, πVAE substan-
tially outperforms a full rank GP. We conjecture
this is due to the more complex structure of the
feature representation Φ and due to a resilience to
overﬁtting.

There are costs to using πVAE, especially the
large upfront cost in training. For complex priors,

training could take days or weeks and will invari-
ably require the heuristics and parameter searches
inherent in applied deep learning to achieve a good
performance. However, once trained, a πVAE net-
work is applicable on a wide range of problems,
with the Bayesian inference MCMC step taking
seconds or minutes.

Future work should investigate the performance
of πVAE on higher dimensional settings (input
spaces > 10). Other stochastic processes, such as
Dirichlet processes, should also be considered.

Declarations

Funding

SB acknowledge the The Novo Nordisk Young
Investigator Award (NNF20OC0059309) which also
supports SM. SB also acknowledges the Danish
National Research Foundation Chair grant, The
Schmidt Polymath Award and The NIHR Health
Protection Research Unit (HPRU) in Modelling
and Health Economics. SM and SB acknowledge
funding from the MRC Centre for Global Infec-
tious Disease Analysis (reference MR/R015600/1)
and Community Jameel. SF acknowledges the
EPSRC (EP/V002910/2) and the Imperial College
COVID-19 Research Fund.

Conﬂict of interest/Competing
interests

NA

Ethics approval

NA

Consent to participate

NA

Consent for publication

NA

Availability of data and materials

All data used in the paper is available at https://
github.com/MLGlobalHealth/pi-vae.

Springer Nature 2021 LATEX template

πVAE

13

Code availability

is

Code
MLGlobalHealth/pi-vae.

available

at

https://github.com/

References

Antoniak, C.E.

(1974). Mixtures of dirich-
let processes with applications to bayesian
nonparametric problems. The annals of
statistics, 1152–1174.

Betancourt, M., Byrne, S., Livingstone, S., Giro-
lami, M. (2017). The geometric foundations
of Hamiltonian Monte Carlo. Bernoulli .

10.3150/16-BEJ810

Blundell, C., Cornebise, J., Kavukcuoglu, K., Wier-
stra, D. (2015). Weight uncertainty in neural
networks. 32nd international conference on
machine learning, icml 2015.

Garnelo, M., Rosenbaum, D., Maddison, C.J.,
Ramalho, T., Saxton, D., Shanahan, M., . . .
Eslami, S.M.
(2018). Conditional neural
processes. 35th international conference on
machine learning, icml 2018.

Hawkes, A.G.

(1971).

Spectra of some self-
exciting and mutually exciting point pro-
cesses. Biometrika, 58 (1), 83–90.

10.1093/biomet/58.1.83

Hernández-Lobato, J.M., & Adams, R.

(2015).
Probabilistic backpropagation for scalable
learning of bayesian neural networks. Inter-
national conference on machine learning (pp.
1861–1869).

Hinton, G.E., & Salakhutdinov, R.R.

(2006).
Reducing the dimensionality of data with
neural networks. science, 313 (5786), 504–
507.

Broomhead, D.S., & Lowe, D.

(1988, March).
Radial Basis Functions, Multi-Variable
Functional
and Adap-
Interpolation
tive Networks. DTIC . Retrieved from
https://apps.dtic.mil/sti/citations/ADA196234

Hoﬀman, M.D., Blei, D.M., Wang, C., Paisley,
J. (2013). Stochastic variational inference.
The Journal of Machine Learning Research,
14 (1), 1303–1347.

Carpenter, B., Gelman, A., Hoﬀman, M.D., Lee, D.,
Goodrich, B., Betancourt, M., . . . Riddell, A.
(2017). Stan : A Probabilistic Programming
Language. Journal of Statistical Software,
76 (1), 1–32.

10.18637/jss.v076.i01

Caterini, A.L., Doucet, A., Sejdinovic, D.
(2018). Hamiltonian variational auto-encoder.
Advances in neural information processing
systems.

Gardner, J.R., Pleiss, G., Bindel, D., Weinberger,
K.Q., Wilson, A.G.
(2018). Gpytorch:
Blackbox matrix-matrix Gaussian process
inference with GPU acceleration. Advances
in neural information processing systems.

Huggins, J.H., Kasprzak, M., Campbell, T., Brod-
erick, T. (2019). Practical posterior error
bounds from variational objectives. arXiv
preprint arXiv:1910.04102 .

Jacot, A., Gabriel, F., Hongler, C. (2018). Neural
tangent kernel: Convergence and generaliza-
tion in neural networks. Advances in neural
information processing systems.

Karhunen, K.

(1947). On linear methods in
probability theory. Annales academiae scien-
tiarum fennicae, ser. al (Vol. 37, pp. 3–79).

Kim, H., Mnih, A., Schwarz, J., Garnelo, M.,
Eslami, S.M.A., Rosenbaum, D., . . . Teh,
Y.W. (2019). Attentive Neural Processes.
CoRR, abs/1901.0 .

14

πVAE

Springer Nature 2021 LATEX template

Kingma, D.P., & Ba, J.

(2014, 12). Adam: A
Method for Stochastic Optimization. arXiv
preprint arXiv:1412.6980 .

https://doi.org/10.1145/2983323.2983812
10.1145/2983323.2983812

Kingma, D.P., & Welling, M.

(2014). Auto-
Encoding Variational Bayes (VAE, reparam-
eterization trick). ICLR 2014 .

Møller, J., Syversveen, A.R., Waagepetersen, R.P.
(1998). Log gaussian cox processes. Scandi-
navian Journal of Statistics, 25 (3), 451-482.
https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-
9469.00115

10.1111/1467-9469.00115

Kingma, D.P., & Welling, M. (2019). An intro-
duction to variational autoencoders. Foun-
dations and Trends® in Machine Learning,
12 (4), 307–392.

Neal, R.

(1996). Bayesian Learning for Neu-
LECTURE NOTES IN
ral Networks.
STATISTICS -NEW YORK- SPRINGER
VERLAG-.

Lakshminarayanan, B., Pritzel, A., Blundell, C.
(2017).
Simple and scalable predictive
uncertainty estimation using deep ensembles.
Advances in neural information processing
systems.

Lindgren, F., Rue, H., Lindström, J. (2011). An
explicit link between Gaussian ﬁelds and
Gaussian Markov random ﬁelds: the stochas-
tic partial diﬀerential equation approach.
Journal of
the Royal Statistical Society:
Series B (Statistical Methodology), 73 (4),
423–498.

10.1111/j.1467-9868.2011.00777.x

Loeve, M. (1948). Functions aleatoires du second
ordre. Processus stochastique et mouvement
Brownien, 366–420.

Minka, T.P. (2001). Expectation propagation for
approximate bayesian inference. Proceedings
of the seventeenth conference on uncertainty
in artiﬁcial intelligence (p. 362–369). San
Francisco, CA, USA: Morgan Kaufmann
Publishers Inc.

Mishra, S., Rizoiu, M.-A., Xie, L. (2016). Fea-
ture driven and point process approaches
Proceed-
for popularity prediction.
the 25th acm international on
ings of
knowl-
conference
edge management (p. 1069–1078). New
York, NY, USA: Association for Com-
Retrieved from
puting Machinery.

on information and

Neal, R.M. (1993). Probabilistic inference using
markov chain monte carlo methods. Depart-
ment of Computer Science, University of
Toronto Toronto, Ontario, Canada.

Park, J., & Sandberg, I.W. (1991, June). Universal
Approximation Using Radial-Basis-Function
Networks. Neural Comput., 3 (2), 246–257.

10.1162/neco.1991.3.2.246

Paszke, A., Gross, S., Massa, F., Lerer, A.,
Bradbury, J., Chanan, G.,
others
(2019). Pytorch: An imperative style, high-
performance deep learning library. Advances
information processing systems
in neural
(pp. 8024–8035).

. . .

Pavliotis, G.A. (2014). Stochastic processes and
applications: diﬀusion processes, the fokker-
planck and langevin equations (Vol. 60).
Springer.

Rahimi, A., & Recht, B. (2008). Random features
for large-scale kernel machines. Advances in
neural information processing systems (pp.
1177–1184).

Rasmussen, C.E., & Williams, C.K.I.

Gaussian processes
ing.
http://www.worldcat.org/oclc/61285753

for machine

MIT Press.

(2006).
learn-
Retrieved from

Rezende, D.J., Mohamed, S., Wierstra, D. (2014).
Stochastic backpropagation and approximate

systems (pp. 14622–14632).

Welling, M., & Teh, Y.W.

(2011). Bayesian
learning via stochastic gradient langevin
dynamics. Proceedings of the 28th interna-
tional conference on machine learning, icml
2011.

Yao, J., Pan, W., Ghosh, S., Doshi-Velez, F.
(2019). Quality of uncertainty quantiﬁcation
for bayesian neural network inference. arXiv
preprint arXiv:1906.09686 .

Yao, Y., Vehtari, A., Simpson, D., Gelman, A.
(2018). Yes, but did it work?: Evaluat-
ing variational inference. 35th international
conference on machine learning, icml 2018.

Springer Nature 2021 LATEX template

πVAE

15

inference in deep generative models. Interna-
tional conference on machine learning (pp.
1278–1286).

Ritter, H., Botev, A., Barber, D. (2018). A scalable
laplace approximation for neural networks.
6th international conference on learning rep-
resentations,
iclr 2018 - conference track
proceedings.

Ross, S.M. (1996). Stochastic processes (Vol. 2).

John Wiley & Sons.

Roy, D.M., & Teh, Y.W. (2009). The Mondrian
process. Advances in neural information pro-
cessing systems 21 - proceedings of the 2008
conference.

Rue, H., Martino, S., Chopin, N.

(2009, 4).
Approximate Bayesian inference for latent
Gaussian models by using integrated nested
Journal of the
Laplace approximations.
Royal Statistical Society. Series B: Statisti-
cal Methodology, 71 (2), 319–392.

10.1111/j.1467-9868.2008.00700.x

Schwaighofer, A., & Tresp, V. (2003). Transduc-
tive and inductive methods for approximate
gaussian process regression. Advances in
neural information processing systems (pp.
977–984).

Semenova, E., Xu, Y., Howes, A., Rashid, T.,
Bhatt, S., Mishra, S., Flaxman, S. (2022).
Priorvae: encoding spatial priors with vari-
ational autoencoders for small-area estima-
tion. Journal of the Royal Society Interface,
19 (191), 20220094.

Ton, J.-F., Flaxman, S., Sejdinovic, D., Bhatt, S.
(2018). Spatial mapping with Gaussian pro-
cesses and nonstationary Fourier features.
Spatial Statistics.

10.1016/j.spasta.2018.02.002

Wang, K., Pleiss, G., Gardner, J., Tyree, S., Wein-
berger, K.Q., Wilson, A.G. (2019). Exact
gaussian processes on a million data points.
Advances in neural information processing

Springer Nature 2021 LATEX template

16

πVAE

Appendix A MCMC diagnostics

Fig. A1: MCMC diagnostics for VAE inference presented in Figure 1: (a) and (b) shows the values for ˆR

Nef f
N

and
the draws from the posterior predictive distribution.

for all parameters inferred with Stan. (c) shows the true distribution of observations along with

Figure A1 presents the MCMC diagnostics for the 1-D GP function learning example shown in Figure 1.
Both ˆR and eﬀective sample size for all the inferred parameters (latent dimension of the VAE and noise in
the observation) are well behaved with ˆR ≤ 1.01 (Figure A1(a)) and eﬀective sample size greater than 1
(Figure A1(b)). Furthermore, even the draws from the posterior predictive distribution very well capture
the true distribution in observations as shown in Figure A1(c).

Springer Nature 2021 LATEX template

πVAE

17

Fig. A2: MCMC diagnostics for πVAE inference presented in Table 3: (a) and (b) shows the values for ˆR

Nef f
N

and
the draws from the posterior predictive distribution.

for all parameters inferred with Stan. (c) shows the true distribution of observations along with

Figure A2 presents the MCMC diagnostics for the kin40K dataset with πVAE as shown in Table 3.
Both ˆR and eﬀective sample size for all the inferred parameters (latent dimension of the VAE and noise in
the observation) are well behaved with ˆR ≤ 1.01 (Figure A2(a)) and eﬀective sample size greater than 0.5
(Figure A2(b)). Furthermore, the draws from the posterior predictive distribution are shown against the
true distribution in observations as shown in Figure A2(c).

Figure A3 presents the MCMC calibration plots for the posterior. Both the marginal predictive check

and leave one out predictive intervals plots demonstrate that our posterior is well calibrated.

Springer Nature 2021 LATEX template

18

πVAE

Fig. A3: MCMC calibration for πVAE inference presented in Table 3: (a) shows the marginal predictive
check using a leave one out probability integral transform and (b) shows the leave one out predictive
intervals compared to observations.

0.000.250.500.751.000.000.250.500.751.00UniformLOO−PIT−3−2−10120255075100Data point (index)yrepySpringer Nature 2021 LATEX template

πVAE

19

Appendix B Algorithm

πVAE proceeds in two stages. In the ﬁrst stage (Algorithm 1) we train πVAE, using a very large set of
draws from a pre-speciﬁed prior class. In the second stage (Algorithm 2) we use the trained πVAE from
Algorithm 1 as a prior, combine this with data using a likelihood, and perform inference. MCMC for
Bayesian inference or optimization with a loss function are alternative approaches to learn the best Z
to explain the data. While these two algorithms are all that is needed to apply πVAE, for completeness
Algorithm 3 shows how one can use a trained πVAE, which encodes a stochastic process, to sample
realisations from this stochastic process.

Algorithm 1 Prior Training for πVAE (stage 1)

1: Simulate draws from N functions evaluated at K points to create input data consisting of location,

function value pairs: {(s1

i , y1

i ), . . . , (sK

i , yK

i )}N

i=1

2: repeat
3:

for each function i = 1, . . . , N do

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

for each location k = 1, . . . , K do
transform locations: Φ(sk
i )
inner product with a linear basis:
i Φ(sk
i )

i,1 ← βT
ˆyk
end for
append loss1: loss1 ← M SE(ˆyi,1, yi)
encode βi with VAE:

I)

[zµ, zsd](cid:62) = e(βi, γ)
reparameterize for Z: Z ∼ N (zµ, z2
sd
decode with VAE, ˆβi : ˆβi = d(Z, ψ)
for each location k = 1, . . . , K do
transform locations: Φ(sk
i )
inner product with decoded ˆβi:
i Φ(sk
i )

i,2 = ˆβ(cid:62)
ˆyk
end for
append loss2: loss2 ← M SE(ˆyi,2, yi)
minimize loss1 + loss2 + KL (Z(cid:107)N (0, I))

to get Φ, βi, γ, ψ

end for

18:
19: until termination criterion satisﬁed (epochs)

Springer Nature 2021 LATEX template

20

πVAE

Algorithm 2 Inference from πVAE (stage 2)

Require: Trained decoder d (ψ ﬁxed) and Φ (learnt from Algorithm 1)
1: Input: J observations consist of location, function value pairs: {(sj, yj)}J
2: Goal: infer latent function with parameters Z.
3: Sample Z: Z ∼ N (0, I)
4: decode with VAE to get β := d(Z, ψ)
5: for each location j do
6:

transform locations: Φ(sj)
inner product with decoder β: ˆyj ← βT Φ(sj)

j=1.

7:
8: end for
9: Perform Bayesian inference with MCMC for Z to obtain a set of draws from the posterior distribution:

p(Z | d, y1, s1, . . . , yJ , sJ , Φ) ∝ p(y1, . . . , yJ | d, s1, . . . , sJ , Z, Φ)p(Z)

= p(y1, . . . , yJ | ˆy1, . . . , ˆyJ , Z)p(Z)

10: (alternative to step 9:) minimize an expected loss (e.g. gradient decent with mean squared error):

arg minZ

(cid:80)J

j=1 (cid:107)ˆyj - yj(cid:107)2.

Algorithm 3 Sampling from πVAE

Require: Trained decoder d (ψ ﬁxed) and Φ (learnt from Algorithm 1)

1: Input: Locations s1, . . . , sJ where we want to evaluate the sampled function.
2: Sample Z: Z ∼ N (0, I)
3: decode with VAE to get : β := d(Z, ψ)
4: for each location j do
5:

transform locations: Φ(sj)
inner product with β: ˆyj ← βT Φ(sj)

6:
7: end for

Appendix C MNIST Example

Figure C4 below is the MNIST example referenced in the main text for neural processes.

Appendix D Implementation Details

All models were implemented with PyTorch (Paszke et al., 2019) in Python. For Bayesian inference
Stan (Carpenter et al., 2017) was used. For training while using a ﬁxed grid, when not mentioned in main
text, in each dimension was on the range -1 to 1. Our experiments ran on a workstation with two NVIDIA
GeForce RTX 2080 Ti cards.

RBF and Matérn layers: RBF and Matérn layers are implemented as a variant of the original RBF
networks as described in (Broomhead & Lowe, 1988; Park & Sandberg, 1991). In our setting we deﬁne a
set of C trainable centers, which act as ﬁxed points. Now for each input location we calculate a RBF or
Matérn Kernel for all the ﬁxed points. These calculated kernels are weighted for each ﬁxed center and
then summed over to create a scalar output for each location. We can describe the layer as follows:-

Φ(s) =

c
(cid:88)

i=1

αiρ ((cid:107)s − ci(cid:107))

Springer Nature 2021 LATEX template

πVAE

21

10% pixels

20% pixels

30% pixels

(a) Observation

(b) Observation

(c) Observation

(d) Sample 1

(e) Sample 1

(f) Sample 1

(g) Sample 2

(h) Sample 2

(i) Sample 2

(j) Sample 3

(k) Sample 3

(l) Sample 3

Fig. C4: MNIST reconstruction using neural processes after observing only 10, 20 or 30% of pixels from
original data.

where s is an input location (point), αi is the weight for each center ci, and ρ ((cid:107)x − ci(cid:107)) is RBF or

Matérn kernel for RBF and Matérn layer respectively.

LGCP simulation example

We study the function space of 1-D LGCP realisations. We deﬁne a nonnegative intensity function at any
time t as λ(t) : T → R+. The number of events in an interval [t1, t2] within some time period, y[t1,t2], is
distributed Poisson via the following Bayesian hierarchical model:

Z(t) ∼ GP(0, k)

Springer Nature 2021 LATEX template

22

πVAE

λ(t) = γ · exp(Z(t))
(cid:18)(cid:90) t2

y[t1,t2] ∼ Poisson

t1

(cid:19)

λ(t)dt

(D1)

where γ is a constant event rate, set to 5 in our experiments.

To train πVAE on functions from an LGCP, we draw 10,000 samples from Eq (D1), assuming k is an
RBF kernel/covariance function with form e−σx2
, with inverse lengthscale σ chosen randomly from the
set {8, 16, 32, 64}. We then choose an observation window suﬃciently large to ensure that 80 events are
observed. This approach is meant to simulate the situation in which we observe a point process until a
certain number of events have occurrence, at which point we conduct inference (Mishra, Rizoiu, & Xie,
2016).

Given the set of 80 × 10, 000 events, we train πVAE with their corresponding intensity and integral
of the intensity over the corresponding observation window. The integral is calculated numerically. We
concatenate the integral of the intensity at the end with the intensity itself (value of the function evaluated
the speciﬁc location). Note, in this setup we have β setup as a 2 − D vector, ﬁrst value corresponding to
the intensity and second to the integral of the intensity. The task for πVAE is to simultaneously learn both
the instantaneous intensity and the integral of the intensity. At testing, we expand the number of events
(and hence the time horizon) to 100, and compare the intensity and integral of πVAE compared to the true
LGCP. As seen in Figure 4, in this extrapolation, our estimated intensity is very close to the true intensity
and even the estimated integral is close to the true (numerically calculated) integral. This example shows
that the πVAE approach can be used to learn not only function evaluations but properties of functions.

