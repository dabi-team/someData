2
2
0
2

t
c
O
3

]
S
D
.
h
t
a
m

[

2
v
3
9
0
3
1
.
6
0
2
2
:
v
i
X
r
a

Learning Deep Input-Output Stable Dynamics

Ryosuke Kojima∗
Graduate School of Medicine
Kyoto University
Kyoto, 606-8501
kojima.ryosuke.8e@kyoto-u.ac.jp

Yuji Okamoto∗
Graduate School of Medicine
Kyoto University
Kyoto, 606-8501
okamoto.yuji.2c@kyoto-u.ac.jp

Abstract

Learning stable dynamics from observed time-series data is an essential problem
in robotics, physical modeling, and systems biology. Many of these dynamics are
represented as an inputs-output system to communicate with the external environ-
ment. In this study, we focus on input-output stable systems, exhibiting robustness
against unexpected stimuli and noise. We propose a method to learn nonlinear
systems guaranteeing the input-output stability. Our proposed method utilizes the
differentiable projection onto the space satisfying the Hamilton-Jacobi inequality
to realize the input-output stability. The problem of ﬁnding this projection can
be formulated as a quadratic constraint quadratic programming problem, and we
derive the particular solution analytically. Also, we apply our method to a toy
bistable model and the task of training a benchmark generated from a glucose-
insulin simulator. The results show that the nonlinear system with neural networks
by our method achieves the input-output stability, unlike naive neural networks.
Our code is available at https://github.com/clinfo/DeepIOStability.

1

Introduction

Learning dynamics from time-series data has many applications such as industrial robot systems
[1], physical systems[2], and biological systems [3, 4]. Many of these real-world systems equipped
with inputs and outputs to connect for each other, which are called input-output systems [5]. For
example, biological systems sustain life by obtaining energy from the external environment through
their inputs. Such real-world systems have various properties such as stability, controllability, and
observability, which provide clues to analyze the complex systems.

Our purpose is to learn a complex system with “desired properties” from a dataset consisting of
pairs of input and output signals. To represent the target system, this paper considers the following
nonlinear dynamics:

˙x = f (x) + G(x)u,
y = h(x).

x(0) = x0

(1)

where the inner state x, the input u, and the output y belong to a signal space that maps time interval
[0, ∞) to the Euclidean space. We denote the dimension of x, u, and y as n, m, and l, respectively.

Recently, with the development of deep learning, many methods to learn systems from time-series
data using neural networks have been proposed [6–8]. By representing the maps (f, G, h) in Eq. (1)
as neural networks, complex systems can be modeled and trained from a given dataset. However,
guaranteeing that a trained system has the desired properties is challenging.

A naively trained system ﬁts the input signals contained in the training dataset, but does not always
ﬁt for new input signals. For example, Figure 1 shows our preliminary experiments where we

∗Equal contribution.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
(A)

(B)

Figure 1: (A) The model prediction of neural networks (B) the reaction of trained model. These
results are min-max normalized.

naively learned neural networks (f, G, h) in Eq. (1) from input and output signals. The trained neural
networks provided small predictive errors for an input signal in the training dataset (Figure 1 (A) ).
Contrastingly, the unbounded output signals were computed by the trained system with new step
input signals (Figure 1 (B) ). The reason can be expected that the magnitude (integral value) of this
input signal was larger than that of the input signals in the training dataset.

The internal stability is known as one of the attractive properties that should be often satisﬁed in
real-world dynamical systems. The conventional methods to train internal stable systems consisting of
neural networks adopt the Lyapunov-based approaches [9–11]. These methods focus on the internal
system, ˙x = f (x) in the input-output system (1). Thus, how to learn the entire system (1) with the
desired property related to the inﬂuence of the input signal is still challenging.

We propose a novel method to learn a dynamical system consisting of neural networks considering
the input-output stability. The notion of the input-output stability is often used together with the
Hamilton-Jacobi inequality for controller design of the target system in the ﬁeld of control theory.
The Hamilton-Jacobi inequality is one of the sufﬁcient conditions for the input-output stability. The
feature of this condition is that the variable for an input signal u does not appear in the expression,
i.e., we do not need to evaluate the condition for unknown inputs [12]. To the best of our knowledge,
this is the ﬁrst work that establishes a learning method for the dynamical systems consisting of neural
networks using the Hamilton-Jacobi inequality.

The contributions of this paper are as follows:

• This paper derives differentiable projections to the space satisfying the Hamilton-Jacobi

inequality.

• This paper presents the learning method for the input-output system proven to always satisfy

the Hamilton-Jacobi inequality.

• This paper also provides a loss function derived from the Hamilton-Jacobi inequality. By
combining this loss function with the projection described above, efﬁcient learning can be
expected.

• This paper presents experiments using two types of benchmarks to evaluate our method.

2 Background

This section describes the L2 stability, a standard deﬁnition of the input-output stability, and the
Hamilton-Jacobi inequality.

First, we deﬁne the L2 stability of the nonlinear dynamical system (1). If the norm ratio of the output
signal to the input signal is bounded, the system is L2 stable and this norm ratio is called the L2 gain.
The L2 norm on the input and output signal space is used for the deﬁnition of the L2 stability. To
deal with the L2 stability on the nonlinear system (1), we assume that the origin x ≡ 0 of the internal
system ˙x = f (x) is an asymptotically stable equilibrium point and h(0) = 0. Since the translation of

2

0.00.51.0y:outputtrainingdatapredict02505007501000t:time0.00.51.0u:input0.00.51.0y:output0.00.51.01.5t:time0.00.51.0u:inputany asymptotically stable equilibrium points is possible, this assumption can be satisﬁed without loss
of generality.

Deﬁnition 1. The L2 norm is deﬁned as (cid:107)x(cid:107)L2 :=
function β on a domain D ⊂ Rn such that

(cid:113)(cid:82) ∞

0 (cid:107)x(t)(cid:107)2dt. If there exists γ ≥ 0 and a

(cid:107)y(cid:107)L2 ≤ γ(cid:107)u(cid:107)L2 + β(x0),
(2)
then the system (1) is L2 stable, where the function β(·) is non-negative and β(0) = 0. Furthermore,
the minimum γ satisfying (2) is called the L2 gain of the nonlinear system (1).
Next, we describe a sufﬁcient condition of the L2 stability using the Lyapunov function V : D → R,
where the function V is positive deﬁnite, i.e., V (x) ≥ 0 and V (0) = 0. The Hamilton-Jacobi
inequality is an input-independent sufﬁcient condition.
Proposition 1 ([5, Theorem 5.5]). Let f be locally Lipschitz and let G and h be continuous. If there
exist a constant γ > 0 and a continuously differentiable positive deﬁnite function V : D ⊂ Rn → R
such that

∇V T(x)f (x) +

1
2γ2 (cid:107)GT(x)∇V (x)(cid:107)2 +

1
2

(cid:107)h(x)(cid:107)2 ≤ 0,

∀x ∈ D \ {0},

(3)

then the system (1) is L2 stable and the L2 gain is less than or equal to γ. This condition is called
the Hamilton-Jacobi inequality.

The above proposition can be generalized to allow the more complicated situations such as limit-cycle
and bistable cases, where the domain D contains multiple asymptotically stable equilibrium points.
The equilibrium point assumed in this proposition can be replaced with positive invariant sets by
extending the L2 norm [13]. Furthermore, by mixing multiple Lyapunov functions, this proposition
can be generalized around multiple isolated equilibrium points.

3 Method

The goal of this paper is to learn the L2 stable system represented by using neural networks (f, G, h).
The Hamiltonian-Jacobi inequality, which implies the L2 stability, is expressed by (f, G, h). We
present a method to project (f, G, h) onto the space where the Hamilton-Jacobi inequality holds.

3.1 Modiﬁcation of nonlinear systems

Supposing fn : Rn → Rn, Gn : Rn → Rn×m, and hn : Rn → Rl, a triplet map (fn, Gn, hn)
denote as nominal dynamics. Introducing a distance on the triplet maps, the nearest triplet satisfying
the Hamilton-Jacobi inequality from the nominal dynamics (fn, Gn, hn) is called modiﬁed dynamics
(fm, Gm, hm). The purpose of this section is to describe the modiﬁed dynamics (fm, Gm, hm)
associated with the nominal dynamics (fn, Gn, hn) by analytically deriving a projection onto the
space satisfying the Hamilton-Jacobi inequality.

The problem of ﬁnding the modiﬁed dynamics (fm, Gm, hm) is written as a quadratic constraint
quadratic programming (QCQP) problem for the nominal dynamics (fn, Gn, hn). Since there is
generally no analytical solution for QCQP problems, we aim to ﬁnd the particular solution by
adjusting the distance on the triplets.

To prepare for the following theorem, we deﬁne the ramp and the clamp functions as

R(x) (cid:44)

(cid:26)0, x ≤ 0
x, x > 0

, C(x; a, b) (cid:44)






a, x ≤ a
x, a < x ≤ b
b, x > b

,

and deﬁne the Hamilton-Jacobi function as

HJ(f, G, h) (cid:44) ∇V Tf +

1
2γ2 (cid:107)GT∇V (cid:107)2 +

1
2

(cid:107)h(cid:107)2,

(4)

where V is a given positive deﬁnite function. A way to design V is to determine the desired positive
invariant set and design the increasing function around this set. For example, if the target system has
a unique stable point, x = 0 can be regarded as the positive invariant set and the increasing function
V can be designed as V (x) = 1

2 x2.

3

Theorem 1. Consider that the following optimal problem:

(1 − k)
(cid:107)∇V (cid:107)

minimize
fm,Gm,hm
subject to HJ(fm, Gm, hm) ≤ 0,
where k ∈ [0, 1]. The solution of (5) is given by

(cid:107)fm − fn(cid:107) +

k
2γ2 (cid:107)Gm − Gn(cid:107)2 +

k

(cid:107)∇V (cid:107)2 (cid:107)hm − hn(cid:107)2

(5a)

(5b)

fm = fn −

1

(cid:107)∇V (cid:107)2 R (cid:0)Vf + k2VGh

(cid:1) ∇V,

(cid:32)

(cid:114)

Gm = Gn −

1 −

(cid:16)

C

− Vf
VGh

(cid:17)

; k2, 1

(cid:33)

PV Gn,

(cid:114)

hm =

C

(cid:16)

− Vf
VGh

(cid:17)

; k2, 1

hn,

where

Vf (cid:44) ∇V Tfn,

Proof: See Appendix A.

VGh (cid:44) 1

2γ2 (cid:107)GT

n ∇V (cid:107)2 +

1
2

(cid:107)hn(cid:107)2, PV (cid:44) ∇V ∇V T
(cid:107)∇V (cid:107)2 .

The objective function of (5a) is a new distance between the nominal dynamics (fn, Gn, hn) and the
modiﬁed dynamics (fm, Gm, hm). This new distance allows the derivation of analytical solutions by
combining three distances of f , G, and h.

Focusing on the objective function (5a), the constant k represents the ratio of the distance scale
of f to G and h. When k = 0, the result of this problem (5) are consistent with the projection of
the conventional method that guarantee internal stability [9]. As the constant k converges 1, the
modiﬁcation method of Theorem 1 approaches Gm and hm to Gn and hn, respectively. In this
case, the objective function (5a) becomes the distance between fm to fn. Therefore, the following
corollary is satisﬁed.
Corollary 1. The solution of

minimize
fm

(cid:107)fm − fn(cid:107)

subject to HJ(fm, Gn, hn) ≤ 0,

(6a)

(6b)

is given by

fm = fn −

1

(cid:107)∇V (cid:107)2 R (HJ(fn, Gn, hn)) ∇V.

Proof: This solution is easily derived from Theorem 1.

Corollary 1 derives a solution of a linear programming problem rather than QCQP problems. Re-
placing the Hamilton-Jacobi function HJ(fm, Gn, hn) with the time derivative of a positive deﬁnite
function ∇V Tf , this corollary matches the result of the conventional study [9].

When the map hm is ﬁxed as hn, a similar solution as Theorem 1 is derived. Although Corollary 1
is proved by changing k, the modiﬁed dynamics with the ﬁxed hn are not derived. We reprove this
modiﬁed dynamics in a similar way to Theorem 1.
Corollary 2. Consider the following problem:

minimize
fm,Gm

(1 − k)
(cid:107)∇V (cid:107)
subject to HJ(fm, Gm, hn) ≤ 0,

(cid:107)fm − fn(cid:107) +

k
2γ2 (cid:107)Gm − Gn(cid:107)2

(7a)

(7b)

where k ∈ [0, 1]. The solution of (7) is given by

fm = fn −

1

(cid:107)β(cid:107)2 R (cid:0)Vf h + k2VG

(cid:1) ∇V, Gm = Gn −

(cid:32)

(cid:114)

1 −

C

(cid:33)

(cid:16)

− Vf h
VG

(cid:17)

; k2, 1

PV Gn,

(8)

4

(A)

(B)

(C)

Figure 2: Sketches of our method : (A) minimizing the prediction error (the ﬁrst term of the loss
function (9)) in the blue region, (B) moving the nominal dynamics to the blue region (the second
term), and (C) reducing the blue region while keeping the same level of the prediction error (the last
term).

where

Vf h (cid:44) ∇V Tfn +

1
2

(cid:107)hn(cid:107)2,

Proof: See Appendix A.

3.2 Loss function

VG (cid:44) 1

2γ2 (cid:107)GT

n ∇V (cid:107)2.

We represent (fn, Gn, hn) as neural networks and denote (fm, Gm, hm) as the L2 stable dynamics
modiﬁed by Theorem 1, Corollary 1, or 2. Note that the modiﬁcation depends on the candidate of
L2 gain γ. Figure 2 (A) shows the sketch of this modiﬁcation, where the blue region satisﬁes the
Hamilton-Jacobi inequality.

Since the nonlinear system of the modiﬁed dynamics (fm, Gm, hm) is represented as ordinary
differential equations (ODEs) consisting of the differentiable functions fn,Gn, and hn, the techniques
of training neural ODEs can be applied [14]. Once a loss function is designed, the parameters of
neural networks in the modiﬁed system can be learned from given data.

Loss = E(x0,u,ˆy)∈D[||y − ˆy||2
L2

] + λLHJ + αγ2,

(9)

where λ and α are non-negative coefﬁcients.

The ﬁrst term shows the prediction error of the output signal y ( Figure 2 (A)). A dataset D consists of
tuples (x0, u, y) where the initial value x0, the input signal u, and the output signal y. The predicted
output ˆy is calculated from x0, u, and the modiﬁed dynamics (fm, Gm, hm).

The second term aims to improve the nominal dynamics (fn, Gn, hn) closer to the modiﬁed dynamics
(fm, Gm, hm) and is deﬁned as

LHJ = Ex[R(HJ(fn, Gn, hn)(x) + ε)],
where ε is a positive constant ( Figure 2 (B)). Since this term is a form of the hinge loss, ε represents
the magnitude of the penalties for the Hamilton-Jacobi inequality. To evaluate this inequality for any
x ∈ D, we introduce a distribution of x over the domain D. In our experiments, this distribution is
decided as a Gaussian distribution N (µ, σ2) where the mean µ is placed at the asymptotically stable
point and the variance σ2 is an experimental parameter. Without this second term of the loss function,
there are degrees of freedom in the nominal dynamics, i.e., multiple nominal dynamics give the same
loss by the projection, which negatively affects the parameter learning.

The modifying parameter γ can be manually designed for the application or automatically trained
from data by introducing the last term. This training explores smaller γ while keeping the same level
of the prediction error (Figure 2 (C)).

5

(f,G,h)(f,G,h)ModificationE[||yy||2]Contour line of E[||yy||2](f,G,h)(f,G,h) LHJHJ(f,G,h)<0(f,G,h)(f,G,h) 2Reduced region4 Related work

Estimating parameters of a given system is traditionally studied as system identiﬁcation. In the
ﬁeld of system identiﬁcation, much research on the identiﬁcation of linear systems have been done,
where the maps (f, G, h) in Eq. (1) assumes to be linear [15]. Linear state-space models include
identiﬁcation methods by impulse response like the Eigensystem Realization Algorithm (ERA)[16],
by the state-space representation like Multivariable Output Error State sPace (MOESP)[17] and
ORThogonal decomposition method (ORT)[18]. System identiﬁcation methods for non-state-space
models, unlike our target system, contain Dynamic Mode Decomposition with control (DMDc)[19]
and its nonlinear version, Sparse Identiﬁcation of Nonlinear DYnamics with control (SINDYc)[20].
Also, the nonlinear models such as the nonlinear ARX model [21] and the Hammerstein-Wiener
model [22] have been developed and often trained by error minimization. For example, the system
identiﬁcation method using a piece-wise ARX model allows more complicated functions by relaxing
the assumption of linearity [21].

These traditional methods often concern gray-box systems, where the maps (f, G, h) in Eq. (1) are
partially known [23]. This paper deals with a case of black-box systems, where f , G, and h in the
system (1) are represented by neural networks. Our method can be used regardless of whether all f ,
G, and h functions are parameterized using neural networks. So, our method can be easily applied
to application-dependent gray-box systems when the functions f , G, and h are differentiable with
respect to the parameters.

Because the input-output system (1) can be regarded as a differential equation, our study is closely
related to the method of combining neural networks and ODEs [7, 14]. These techniques have been
improved in recent years, including discretization errors and computational complexity. Although we
used an Euler method for simplicity, we can expect that learning efﬁciency would be further improved
by using these techniques.

The internal stability is a fundamental property in ODEs, and learning a system with this property
using neural networks plays an important role in the identiﬁcation of real-world systems [24]. In
particular, the ﬁrst method to guarantee the internal stability of the trained system has been proposed
in [9]. Furthermore, another method [25] extends this method to apply positive invariant sets, e.g.
limit cycles and line attractors. Encouraged by these methods based on the Lyapunov function, our
method further generalizes these methods using the Hamilton-Jacobi inequality to guarantee the
input-output stability.

In this paper, the Lyapunov function V is considered to be given, but this function can be learned from
data. Since a pair of dynamics (1) and V has redundant degrees of freedom, additional assumptions
are required to determine V uniquely. In [9], it is realized by limiting the dynamics to the internal
system and restricting V to a convex function [26].

Lyapunov functions are also used to design controllers, where the whole system should satisfy the
stability condition. A method for learning such a controller using neural nets has been proposed [27].
This method deals with optimization problems over the space that satisﬁes the stability condition,
which is similar to our method. Whereas we solve the QCQP problem to derive the projection onto
the space, this method uses a Satisﬁability Modulo Theories (SMT) solver to satisfy this condition.
The method has also been extended to apply unknown systems [28].

Although this paper deals with deterministic systems, neural networks for stochastic dynamics with
the variational inference is well studied [6, 29–31]. Guaranteeing the internal stability of trained
stochastic dynamics is important for noise ﬁltering and robust controller design [10].

5 Experiments

We conduct two experiments to evaluate our proposed method. The ﬁrst experiment uses a benchmark
dataset generated from a nonlinear model with multiple asymptotically equilibrium points. In the next
experiment, we applied our method to a biological system using a simulator of the glucose-insulin
system.

6

5.1 Experimental setting

Before describing the results of the experiments, this section explains the evaluation metrics and our
experimental setting.

For evaluation metrics, we deﬁne the root mean square error (RMSE) and the average L2 gain
(GainIO) for the given input and output signals as follows:
(cid:118)
(cid:117)
(cid:117)
(cid:116)

RMSE (cid:44)

N
(cid:88)

N
(cid:88)

,

(cid:107)yi − ˆyi(cid:107)2
L2

, GainIO (cid:44) 1
N

1
N

(cid:107)ˆyi(cid:107)L2
(cid:107)ui(cid:107)L2

i=1

i=1

where N is the number of signals in the dataset. ui(·) and yi(·) are the input and output signal at the
i-th index, respectively. The prediction signal ˆyi(·) is computed from ui(·), the trained dynamics,
and the initial state. Note that the integral contained in the L2 norm is approximated by a ﬁnite
summation. The RMSE is a metric of the prediction errors related to the output signal, and the
GainIO is a metric of the property of the L2 stability. Whether the target system satisﬁes or does not
satisfy the L2 stability, the GainIO with a given ﬁnite-size dataset can be calculated. The GainIO
error is deﬁned by the absolute error between the GainIO of the test dataset and that of the prediction.

In our experiments, 90% of the dataset is used for training and the remaining 10% is used for testing.
We retry ﬁve times for all experiments and show the mean and standard deviations of the metrics.

For simplicity in our experiments, the sampling step ∆t for the output y is set as constant and the
Euler method is used to solve ODEs. x0 is put at an asymptotically stable point for each benchmark
and is known. In this experiment, to prevent the state from diverging during learning of dynamics,
the clipping operation is used so that the absolute values of the states are less than ten.

For comparative methods, we use vanilla neural networks, ARX, ORT [18], MOESP [17], and
piece-wise ARX[21]. In the method of vanilla neural networks, the maps (f, G, h) in the nonlinear
system (1) is represented by using three neural networks, i.e., this method is consistent with a method
used in Figure 1. To determine the hyperparameters of comparative methods except for neural
networks, the grid search is used. Note that these comparative methods only consider the prediction
errors.

For training each method with neural networks, an NVIDIA Tesla T4 GPU was used. Our experiments
are totally run on 20 GPUs over about three days.

5.2 Bistable model benchmark

The ﬁrst experiment is carried out using a bistable model, which is known as a bounded system with
multiple asymptotically equilibrium points. This bistable model is deﬁned as
˙x = x(1 − x2) + u,

x(0) = −1,

y = x.

The internal system of this model has two asymptotically stable equilibrium points x ≡ 1, −1.

We generate 1000 input and output signals for this experiment. To construct this dataset, we prepare
input signals using positive and negative pulse wave signals whose pulse width is changed at random.
The input and output signals on the period [0, 10] are sampled with an interval ∆t = 0.1. In this
benchmark, we set the number of dimensions of the internal system as one and use a ﬁxed function
V (x) = min((x − 1)2, (x + 1)2), a mixture of the two positive deﬁnite functions.

In the result of our experiments, we name the proposed methods modiﬁed by Theorem 1, Corollary 1,
and 2 as DIOS-fgh, DIOS-f, and DIOS-fg, respectively. In these methods, the parameters of our loss
function are set as λ = 0 and α = 0.01. Also, DIOS-fgh+ uses λ = 0.01 and α = 0.01 under the
same conditions as DIOS-fgh. For this example (1000 input signal), it took about 1 hour using 1
GPU to learn one model training.

The results of the RMSE and the GainIO error in this experiment are shown in Figure 3. Figures 3 (A)
and (B) demonstrate that a piece-wise ARX model (PWARX) gives very low RMSE but its GainIO
error is high. Our proposed methods achieve a small GainIO error while keeping the RMSE. Note
that linear models such as MOESP and ORT only approach one point although the bistable model has
two asymptotically stable equilibrium points. Figures 3 (C) and (D) display the effect of dataset sizes.
When the dataset size was varied to 100, 1000, and 3000, the result of the larger dataset provided

7

Figure 3: Results of the bistable model benchmark. The upper part shows (A) the RMSE and (B) the
GainIO error of the vanilla neural networks (gray), our proposed methods (red), and the conventional
methods (blue). The lower part shows (C) the RMSE and (D) the GainIO error for the different sizes
of the datasets.

Figure 4: The sketch x-f (x) displaying the internal dynamics trained from the bistable datasets with
the different sizes. The sign of the bistable model is shown at the bottom of each ﬁgure.

the smaller RMSE for all methods. The vanilla neural networks do not consider the L2 gain, so the
GainIO error was large in the case of the low RMSE.

Figure 4 shows the relationship between x and f (x) in the trained system in (1) to compare the
vanilla neural networks, DIOS-fgh and DIOS-fgh+. DIOS-fgh and DIOS-fgh+ successfully ﬁnd two
stable points, i.e., the trained function f (x) had roots of two stable points x = ±1 and an unstable
point x = 0. Especially, these stable points were robustly estimated in the trained system using
our presented loss function (DIOS-fgh+) even when the dataset size was small. The vanilla neural
networks failed to obtain the two stable points in all cases.

8

RMSERMSEGainIO errorGainIO error(A)(B)(C)(D)Data size    100Data size   1000Data size   3000Data size    100Data size   1000Data size   3000VanillaDIOS-fDIOS-fgDIOS-fghDIOS-fgh+ARXMOESPORTPWARXVanillaDIOS-fDIOS-fgDIOS-fghDIOS-fgh+ARXMOESPORTPWARXVanillaDIOS-fghDIOS-fgh+VanillaDIOS-fghDIOS-fgh+VanillaDIOS-fghDIOS-fgh+VanillaDIOS-fghDIOS-fgh+VanillaDIOS-fghDIOS-fgh+VanillaDIOS-fghDIOS-fgh+++--Truexf(x)++--Truexf(x)++--Truexf(x)Data size: 100Data size: 1000Data size: 3000DIOS-fgh+DIOS-fghVanillaFigure 5: The input and output signals of the glucose-insulin simulator and the predicted output.

Figure 6: The step reaction of the trained systems. The color of each line indicates the magnitude of
the step input.

5.3 Glucose-insulin benchmark

This section addresses an example of the identiﬁcation of biological systems. We consider the task of
learning the glucose-insulin system using a simulator [32] to construct responses for various inputs
and evaluate the robustness of the proposed method for unexpected inputs. This simulator outputs the
concentrations of plasma glucose y1 and insulin y2 for the appearance of plasma glucose per minute
u. To determine the realistic input u, we adopt another model, an oral glucose absorption model [33].

Using this simulator, 1000 input and output signals are synthesized for this experiment. The input
and output signals are sampled with a sampling interval ∆t = 1 and 1000 steps for each sequence.
In this benchmark, we set the number of dimensions of the internal system as six, ﬁx a positive
deﬁnite function V (x) = x2, and use our loss function with λ = 0.001 and α = 0.001. Training one
model for this examples (1000 input signal) tooks about 7.5hours using 1GPU. The hyperparameters
including the number of layers in the neural networks, the learning rate, optimizer, and the weighted
decay are determined using the tree-structured Parzen estimator (TPE) implemented in Optuna [34].

The RMSE of vanilla and our proposed method are 0.0103 and 0.0050, respectively. So, from
the perspective of RMSE, these methods achieved almost the same performance. Figure 5 shows
input and output signals in the test dataset and the predicted output by vanilla neural networks and
our method (DIOS-fgh+). The L2 stability of the system using the vanilla neural networks is not
guaranteed. Since the proposed method guarantees the L2 stability, the output signals of DIOS-fgh+
are bounded even if the input signals are unexpectedly large.

To demonstrate this, we conducted an additional experiment using the trained system. Figure 6 shows
the transition of output behavior caused by the magnitude of the input signal changing from 2 to 10.
Note that the maximum magnitude in the training dataset is one. In this experiment, ∆t was changed
to 0.01 and the clipping operation was removed to deal with the large values of the state.

9

050010000.20.40.6y1:GlucoseVanilla05001000t:Time0.000.050.100.15y2:InsulinOurmethod(DIOS-fgh+)050010000.00.20.40.6u:InputTrue(A) Vanilla(B) Our method (DIOS-fgh+)(B) TrueThis result shows the output of the vanilla neural networks quickly diverged with an unexpectedly
large input. Contrastingly, the output behavior of our proposed method is always bounded. Therefore,
we actually conﬁrmed that our proposed method satisﬁes the L2 stability.

6 Conclusion

This paper proposed a learning method for nonlinear dynamical system guaranteeing the L2 stability.
By theoretically deriving the projection of a triplet (f, G, h) to the space satisfying the Hamilton-
Jacobi inequality, our proposed method realized the L2 stability of trained systems. Also, we
introduced a loss function to empirically achieve a smaller L2 gain while reducing prediction errors.
We conducted two experiments to learn dynamical systems consisting of neural networks. The ﬁrst
experiment used a nonlinear model with multiple asymptotically equilibrium points. The result of
this experiment showed that our proposed method can robustly estimate a system with multiple stable
points. In the next experiment, we applied our method to a biological system using a simulator of the
glucose-insulin system. It was conﬁrmed that the proposed method can successfully learn a proper
system that works well under unexpectedly large inputs due to the L2 stability. There is a limitation
that our method cannot apply the system without the L2 stability. Future work will expand the L2
stability-based method to a more generalized learning method by dissipativity and apply our approach
of this study to stochastic systems.

Acknowledgements

This research was supported by JST Moonshot R&D Grant Number JPMJMS2021 and JPMJMS2024.
This work was also supported by JSPS KAKENHI Grant No.21H04905. This paper was also based on
a part of results obtained from a project commissioned by the New Energy and Industrial Technology
Development Organization (NEDO).

References

[1] Jan Swevers, Walter Verdonck, and Joris De Schutter. Dynamic model identiﬁcation for industrial robots.

IEEE control systems magazine, 27(5):58–71, 2007.

[2] Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from data by
sparse identiﬁcation of nonlinear dynamical systems. Proceedings of the national academy of sciences,
2016.

[3] Timothy S. Gardner, Diego Di Bernardo, David Lorenz, and James J. Collins. Inferring genetic networks

and identifying compound mode of action via expression proﬁling. Science, 301(5629):102–105, 2003.

[4] Geoffrey Roeder, Paul K Grant, Andrew Phillips, Neil Dalchau, and Edwards Meeds. Efﬁcient amor-
tised bayesian inference for hierarchical and nonlinear dynamical systems. Proceeding of International
Conference on Machine Learning (ICML), 2019.

[5] Hassan K. Khalil. Nonlinear systems. Prentice-Hall, 3rd edition, 2002.

[6] Rahul G. Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state space

models. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, page 2101–2109, 2017.

[7] Ricky T.Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential

equations. In Advances in Neural Information Processing Systems (NeurIPS), volume 31, 2018.

[8] Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning hamiltonian
dynamics with control. In Proceeding of International Conference on Learning Representations, 2019.

[9] Gaurav Manek and J. Zico Kolter. Learning stable deep dynamics models.

In Advances in Neural

Information Processing Systems (NeurIPS), volume 32, 2019.

[10] Nathan Lawrence, Philip Loewen, Michael Forbes, Johan Backstrom, and Bhushan Gopaluni. Almost
surely stable deep dynamics. In Advances in Neural Information Processing Systems (NeurIPS), volume 33,
2020.

[11] Andreas Schlaginhaufen, Philippe Wenk, Andreas Krause, and Florian Dörﬂer. Learning stable deep
dynamics models for partially observed or delayed dynamical systems. In Advances in Neural Information
Processing Systems (NeurIPS), volume 34, 2021.

10

[12] Arjan van der Schaft. L2-Gain and Passivity Techniques in Nonlinear Control. Springer Publishing

Company, Incorporated, 3rd edition, 2016.

[13] W.M. Haddad and V.S. Chellaboina. Nonlinear Dynamical Systems and Control: A Lyapunov-Based

Approach. Princeton University Press, 2008.

[14] Xinshi Chen. Review: Ordinary differential equations for deep learning. CoRR, abs/1911.00502, 2019.

[15] Tohru Katayama. Subspace Method for System Identiﬁcation. Springer, 2005.

[16] Jer-Nan Juang and Richard S Pappa. An eigensystem realization algorithm for modal parameter identiﬁca-

tion and model reduction. Journal of guidance, control, and dynamics, 8(5):620–627, 1985.

[17] Michel Verhaegen and Patrick Dewilde. Subspace model identiﬁcation part 1. the output-error state-space
model identiﬁcation class of algorithms. International journal of control, 56(5):1187–1210, 1992.

[18] Tohru Katayama, Hideyuki Tanaka, and Takeya Enomoto. A simple subspace identiﬁcation method of
closed-loop systems using orthogonal decomposition. IFAC Proceedings Volumes, 38(1):512–517, 2005.

[19] Joshua L Proctor, Steven L Brunton, and J Nathan Kutz. Dynamic mode decomposition with control. SIAM

Journal on Applied Dynamical Systems, 15(1):142–161, 2016.

[20] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Sparse identiﬁcation of nonlinear dynamics with

control (SINDYc). IFAC-PapersOnLine, 49(18):710–715, 2016.

[21] Andrea Garulli, Simone Paoletti, and Antonio Vicino. A survey on switched and piecewise afﬁne system

identiﬁcation. In IFAC Proceedings Volumes, volume 45, pages 344–355. IFAC, 2012.

[22] Adrian Wills, Thomas B. Schon, Lennart Ljung, and Brett Ninness. Identiﬁcation of hammerstein-wiener

models. Automatica, 49(1):70–81, 2013.

[23] Lennart Ljung. System identiﬁcation. In Signal analysis and prediction, pages 163–173. Springer, 1998.

[24] Martin Braun and Martin Golubitsky. Differential equations and their applications. Springer, 1983.

[25] Naoya Takeishi and Yoshinobu Kawahara. Learning dynamics models with stable invariant sets. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages 9782–9790, 2021.

[26] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference on

Machine Learning, pages 146–155, 2017.

[27] Ya-Chien Chang, Nima Roohi, and Sicun Gao. Neural lyapunov control. In Advances in Neural Information

Processing Systems (NeurIPS), volume 32, 2019.

[28] Vrushabh Zinage and Efstathios Bakolas. Neural koopman lyapunov control. arXiv:2201.05098, 2022.

[29] Daniel Gedon, Niklas Wahlström, Thomas B. Schön, and Lennart Ljung. Deep state space models for
nonlinear system identiﬁcation. In Proceedings of the 19th IFAC Symposium on System Identiﬁcation
(SYSID), 2021.

[30] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C. Courville, and Yoshua Bengio. A
recurrent latent variable model for sequential data. In Advances in Neural Information Processing Systems
(NeurIPS), volume 28, 2015.

[31] Justin Bayer and Christian Osendorfer. Learning stochastic recurrent networks. ArXiv, abs/1411.7610,

2014.

[32] A. De Gaetano and O. Arino. A statistical approach to the determination of stability for dynamical systems

modelling physiological processes. Mathematical and Computer Modelling, 31(4):41–51, 2000.

[33] Chiara Dalla Man, Robert A. Rizza, and Claudio Cobelli. Meal simulation model of the glucose-insulin

system. IEEE Transactions on Biomedical Engineering, 54(10):1740–1749, 2007.

[34] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A
next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, 2019.

11

A Proof of Theorem 1 and Corollary 2

To prove Theorem 1 and Corollary 2, we use the following particular solution of QCQP problems.
Lemma 1. Suppose that A ∈ Rn×n is a positive deﬁnite matrix, a vector b ∈ Rn, a scalar c, and
positive constants kx, ky. The solution of this problem

minimize kxxTAx + ky|y|
subject to

y ≥ xTAx − 2bTx + c.

is given by

where

(cid:18)

1 −

x∗ =

(cid:113)

C(1 − ˜c; ˜k2

x, 1)

(cid:19)

A−1b,

y∗ = R

(cid:16)

c − (1 − ˜k2

(cid:17)
x)bTA−1b

,

˜c =

c
bTA−1b

,

˜kx =

kx
kx + ky

.

Proof: See Appendix B.

To keep the notation short in the following proofs, we deﬁne

β (cid:44) ∇V.

Proof of Theorem 2: For the Hamilton-Jacobi function HJ(fm, Gm, hm) of the modiﬁed dynamics
(fm, Gm, hm), the map fm and each row of the map Gm don’t depend on the orthogonal vector of
β. Hence, fm and Gm are written as

where p is scalar and q ∈ Rm. hm depends only on the norm in HJ(fm, Gm, hm), i. e.,

fm = fn − βp, Gm = Gn − βqT,

where r is scalar.

The optimal function (5a) is rewritten as the p, q, and r, i.e.,

hm = (1 − r)hn,

k1
(cid:107)β(cid:107)

(cid:107)fm − fn(cid:107) +

k2
2γ2 (cid:107)Gm − Gn(cid:107)2 +

k2
2(cid:107)β(cid:107)2 (cid:107)hm − hn(cid:107)

(cid:107)β(cid:107)2
2γ2 (cid:107)q(cid:107)2 + k2
Also, the condition of constraint (5b) is rewritten as

= k1|p| + k2

(cid:107)hn(cid:107)
2(cid:107)β(cid:107)2 r2.

HJβ(fm, Gm, hm)

= βTfm +

1
2

(cid:107)hm(cid:107)2

mβ(cid:107)2 +

1
2γ2 (cid:107)GT
1
1
2γ2 (cid:107)(Gn − βqT)Tβ(cid:107)2 +
2
1
1
(cid:107)ˆh(cid:107)2r2 −
γ2 (cid:107)β(cid:107)2βTGnq − (cid:107)hn(cid:107)2r + HJβ(fn, Gn, hn)
2

1
2γ2 (cid:107)β(cid:107)4(cid:107)q(cid:107)2 +

(cid:13)
(cid:13)(1 − r)hn(cid:107)2
(cid:13)

= βT(fn − βp) +

= −(cid:107)β(cid:107)2p +

≤ 0.

If x (cid:44) [qT, r]T, y (cid:44) p, and

A (cid:44)

(cid:34) (cid:107)β(cid:107)2

2γ2 Im
0

(cid:35)

0
(cid:107)hn(cid:107)2
2(cid:107)β(cid:107)2

,

b (cid:44)

(cid:35)

(cid:34) 1
2γ2 GT
n β
(cid:107)hn(cid:107)2
2(cid:107)β(cid:107)2

(cid:16)

c (cid:44) 1
(cid:107)β(cid:107)2

HJβ(fn, Gn, hn)

(cid:17)

,

12

then this optimal problem (5) of this theorem becomes a problem used in Lemma 1. The optimal
point of p, q and r is given by

p∗ =

q∗ =

1
(cid:107)β(cid:107)2 R
(cid:32)
1
(cid:107)β(cid:107)2

(cid:16)

V ˆf + ˆk2VGn,hn

(cid:17)

,

(cid:114)

C

(cid:16)

− Vfn

VGn ,hn

; ˜k2, 1

(cid:33)

(cid:17)

GT

n β,

1 −

r∗ = 1 −

(cid:114)

(cid:16)

C

− Vfn

VGn ,hn

(cid:17)
; ˜k2, 1

.

Therefore, Theorem 1 is derived.

Proof of Theorem 2:

Optimal maps fm and Gm are also written as

Hence, the objective function (7a) is rewritten as p and q function, i.e.,

fm = fn − βp, Gm = Gn − βqT.

k1
(cid:107)β(cid:107)

(cid:107)fm − fn(cid:107) +

k2
2γ2 (cid:107)Gm − Gn(cid:107)2 = k1|p| + k2

(cid:107)β(cid:107)2
2γ2 (cid:107)q(cid:107)2.

Also the condition of constraint (7b) is written

HJβ(fm, Gm, hn)

= βTfm +

1
2

(cid:107)hm(cid:107)2

mβ(cid:107)2 +

1
2γ2 (cid:107)GT
1
2γ2 (cid:107)(Gn − βqT)Tβ(cid:107)2 +
1
γ2 (cid:107)β(cid:107)2βTGnq + HJβ(fn, Gn, hn)

1
2γ2 (cid:107)β(cid:107)4(cid:107)q(cid:107)2 −

(cid:107)hn(cid:107)2

1
2

= βT(fn − βp) +

= −(cid:107)β(cid:107)2p +

≤ 0.

If x (cid:44) q, y (cid:44) p, and

A (cid:44) (cid:107)β(cid:107)2

2γ2 Im,

b (cid:44) 1

2γ2 GT

mβ,

(cid:16)

c (cid:44) 1
(cid:107)β(cid:107)2

HJβ(fn, Gn, hn)

(cid:17)

,

then this optimal problem (7) becomes the problem used in Lemma 1. The optimal points of p and q
are given by

p∗ =

q∗ =

1
(cid:107)β(cid:107)2 R
(cid:32)
1
(cid:107)β(cid:107)2

(cid:16)

Vfn,hn + ˆk2

2(VGn)

(cid:17)

,

(cid:114)

(cid:16)

C

− Vfn ,hn
VGn

(cid:33)

(cid:17)
; ˜k2, 1

GT

n β,

1 −

Therefore, the solution of problem (7) becomes Eqs. (8).

B Proof of the particular solution of QCQP

This section presents the proof of the following QCQP problem.

minimize kxxTAx + ky|y|
subject to

y ≥ xTAx − 2bTx + c,

(10a)

(10b)

where A is a positive deﬁnite matrix. We classify this problem according to the parameter A, b and c.

First, the solution of this optimal problem is switched depending on the positive or negative value of
c.

13

Lemma 2. If c ≤ 0 then the solution of Eq. (10) is (x, y) = (0, 0). If c > 0, the solution x∗ of
Eq. (10) equals the solution of the following problem:

Furthermore, the solution y∗ is given by

minimize kxxTAx + ky|xTAx − 2bTx + c|.

y∗ = kxx∗TAx∗ − 2bTx∗ + c.

(11)

(12)

Proof: The objective function (10a) is strictly convex and the minimum point is (x, y) = (0, 0). If
c ≤ 0, the origin (x, y) = (0, 0) satisﬁes the condition of constraint (10b). Therefore, the solution of
Eq. (10) is (x, y) = (0, 0).

If c > 0, (x, y) = (0, 0) does not satisﬁes the constraint condition (10b), and the optimal solution
belong the boundary of the region satisfying (10b). Therefore, the solution point (x∗, y∗) satisﬁes
Eq. (12).

Also, the solution of the new optimal problem (11) is switched by the ratio between c and bTA−1b.
Lemma 3. If ˜c > 1 − ˜k2

x then the solution x∗ of the optimal problem (11) is given by

where

x∗ = (1 − ˜kx)A−1b,

˜c =

c
bTA−1b

,

˜kx =

kx
kx + ky

.

If ˜c ≤ 1 − ˜k2

x, the solution x∗ equals the solution of the following QCQP problem, such that

minimize xTAx
subject to xTAx − 2bTx + c = 0.

(13)

(14a)

(14b)

Proof: The optimal problem (11) is split on the sign of x∗TAx∗ − 2bTx∗ + c.
Case x∗TAx∗ − 2bTx∗ + c < 0:

The optimal problem of this case is written as

minimize

(kx − ky)xTAx + 2kybTx − kyc.

If kx − ky ≤ 0, there is no optimal point. Otherwise kx − ky > 0, the optimal point is written as

x∗ = −

ky
kx − ky

A−1b.

The optimal point does not satisﬁes the condition x∗TAx∗ − 2bTx∗ + c < 0, because

x∗TAx∗ − 2bTx∗ + c

k2
y

=

(kx − ky)2 bTA−1b +
where A−1 is a positive deﬁnite matrix and c > 0.
Case x∗TAx∗ − 2bTx∗ + c > 0:

2ky
kx − ky

bTA−1b + c > 0,

The problem (11) is written as

Hence, this optimal point is written as

minimize

(kx + ky)xTAx − 2kybTx + kyc.

x∗ =

ky
kx + ky

A−1b

= (1 − ˜kx)A−1b,

14

√

A is a square root of the positive deﬁnite matrix A. The condition x∗TAx∗ − 2bTx∗ + c > 0

where
is rewritten by the previous optimal solution, such that

x∗TAx∗ − 2bTx∗ + c
= (1 − ˜kx)2bTA−1b − 2(1 − ˜kx)bTA−1b + c
bTA−1b + c > 0

(cid:16)

(cid:17)

= −

1 − ˜k2
x
⇔ ˜c > 1 − ˜k2
x.

Case xTAx∗ − 2bTx∗ + c = 0: The problem (11) is written as the QCQP problem (14).

The solution of the simple QCQP problem (14) is easily derived using the method of Lagrange
multiplier.
Lemma 4. The solution of the simple QCQP problem (14) is given by

(cid:16)

√

(cid:17)

x∗ =

1 −

1 − ˜c

A−1b.

Proof: Supposing a Lagrange multiplier λ > 0, the Lagrange function is written as

L(x, λ) = xTAx + λ(xTAx − 2bTx + c).

The KKT condition is given by

∂L(x∗, λ)
∂x
∂L(x∗, λ)
∂λ

= 2(1 + λ)Ax∗ − 2λb = 0.

= x∗TAx∗ − 2bTx∗ + c = 0,

x∗ =

λ
1 + λ

A−1b,

(15)

(16)

x∗TAx∗ − 2bTx∗ + c = 0
2λ
1 + λ

(1 + λ)2 bTA−1b −

λ2

bTA−1b + c = 0

Eq. (15) is written as

and the Eq. (16) is given by

⇔

⇔ −

λ2 + 2λ
(1 + λ)2 bTA−1b + c = 0
⇔ − (λ2 + 2λ)bTA−1b + (1 + λ)2c = 0
⇔(c − bTA−1b)λ2 + 2(c − bTA−1b)λ + c = 0

⇔λ2 + 2λ +

⇔λ = −1 ±

⇔λ = −1 ±

= 0

˜c
˜c − 1

˜c
(˜c − 1)
(cid:114)

1 −
(cid:114) 1

.

1 − ˜c

As λ > 0 and ˜c > 0, the Lagrange multiplier is written as
(cid:114) 1

λ = −1 +

.

1 − ˜c

Therefore, the optimal point x∗ is given by

x∗ =

=

λ
1 + λ

−1 +

A−1b

(cid:113) 1
1−˜c

A−1b

(cid:113) 1
1−˜c
√
1 − ˜c)A−1b.

= (1 −

15

Finally, we summarize three Lemmas 2-4 and solve the QCQP problem (10).
Lemma 1. The solution of the problem (10) is given by

(cid:32)

x∗ =

1 −

(cid:114)

(cid:16)

C

1 − ˜c; ˜k2

x, 1

(cid:17)

(cid:33)

A−1b,

y∗ = R

(cid:16)
c − (1 − ˜k2

x)bTA−1b

(cid:17)

.

Proof: Lemmas 2-4 split the solution x∗ as three cases:

(17a)

(17b)

x∗ :=

:=

1 − ˜c(cid:1) A−1b




(1 − ˜kx)A−1b
√
(cid:0)1 −

0
(cid:18)



(cid:113)

(cid:19)

˜k2
A−1b
x
1 − ˜c(cid:1) A−1b
1(cid:1) A−1b

√
√

1 −
(cid:0)1 −
(cid:0)1 −
(cid:114)


(cid:32)

=

1 −

(cid:16)

C

1 − ˜c; ˜k2

x, 1

(cid:17)

1 − ˜k2
x < ˜c
0 < ˜c ≤ 1 − ˜k2
x
c ≤ 0

1 − ˜c < ˜k2
x
˜k2
x ≤ 1 − ˜c < 1
1 ≤ 1 − ˜c
(cid:33)

A−1b.

Furthermore, the solution of y∗ is written as,

y∗ :=






x < ˜c

x∗TAx∗ − 2bTx∗ + c 1 − ˜k2
x∗TAx∗ − 2bTx∗ + c 0 < ˜c ≤ 1 − ˜k2
x
0
c − (1 − ˜k2
0
0

c ≤ 0
x)bTA−1b 1 − ˜k2

x < ˜c
0 < ˜c ≤ 1 − ˜k2
x
˜c ≤ 0






=

Therefore, the solution become Eq. (17).

= R(c − (1 − ˜k2

x)bTA−1b).

C Overall schematic of the learning process

Algorithm 1 shows the overall schematic of the learning process. The ﬁrst line deﬁnes the modiﬁed
dynamics (fm, Gm, hm) from the nominal dynamics (fn, Gn, hn), deﬁned by the neural network,
where φ is a set of parameters of the nominal dynamics. The 2-7 line represents a training loop,
where the gradient-based optimization methods can be used by using the forward and backward
calculation. Note that an ODE solver is used for forward calculation, and Algorithm 2 shows the
forward calculation when the Euler method is used. For simplicity, mini-batch computation omitted
in this schematic.

D Neural network architecture and hyper parameters

This section details how to determine the neural network architecture. The architecture and hyper
parameters of the neural networks were basically determined by Bayesian optimizing using the
validation dataset.

Table 1 shows the search space of Bayesian optimization. The ﬁrst three parameters: learning rate,
weight decay, and batch size are parameters for training the neural networks. Also, an optimizer is
selected from AdamW, Adam, and RMSProp. The structure of neural network is determined from

16

Algorithm 1 Training process
Input: x0: initial state, u: input signal,y: output signal, (fn, Gn, hn): nominal dynamics, V : a

designed function

ˆy ← ODE with (fm, Gm, hm) from x0, u (Algorithm 2)
forward computation of Loss function (9) from y

1: deﬁne modiﬁed functions (fm, Gm, hm) from (fn, Gn, hn) and V
2: for 1 to #iterations do
3:
4:
5: ∇φLoss ← backward computation with Loss
6:
7: end for

φ ← Optimizer(φ, ∇φLoss)

Algorithm 2 Forward computation for dynamics Eq. (1)
Input: x0: initial state, u: input signal, (fm, Gm, hm): dynamics
Output: ˆy: output signal
1: for t ← 0 to T do
2:
3:
4: end for
5: return ˆy

xt+1 ← xt + ∆t(fm(xt) + Gm(xt)ut)
ˆyt ← hm(xt)

the number of intermediate layers and dimensions for each layer. One layer in our setting consists of
a fully connected layer with a ReLU activation. The last three rows represent parameters related to
our proposed methods. (cid:15) is a parameter of the loss function LHJ. Initial scale parameter is multiplied
with the output of fn to prevent the value of fn(x) from becoming large in the initial stages of
learning. When fn(x), which determine the behavior of the internal system, outputs a large value, it
diverges due to time evolution, and the learning of the entire system may not progress. Therefore, it
is empirically preferable to start with a small value for fn(x) at the initial stage of learning. When
the ﬂag of ‘stop gradient for projection’ is false, backward computation related to the second term of
modiﬁcation of fm and Gm is disabled. Note that modiﬁcation related to fm and Gm consists of
two terms (see Theorem 1). Setting this parameter to false resulted in better performance in our all
experiments.

We ran 300 trials using the Bayesian optimization for the bistable model benchmark and the glucose
insulin benchmark with the above settings, and the hyper parameters obtained are shown in Table 2,
where the number of dimensions for each hidden layer is shown in tuple from the order closest to the
input layer.

Hyperparameters of comparative methods were determined by grid search using the validation dataset.
ARX and PWARX have an order parameter n of the autoregressive model, and this parameter is
searched in the range of 1 − 5. The number of iterations was set to 10000 so that the optimization of
PWARX converges sufﬁciently. MOESP and ORT have an internal dimension n (1 ≤ n ≤ 20) and
the number of subsequences used for estimation k (2n < k ≤ 20).

17

Table 1: The search space of Bayesian optimization

parameter name
learning rate
weight decay
batch size
optimizer
#layer for fn
#layer for Gn
#layer for hn
#dim. for a hidden layer of fn
#dim. for a hidden layer of Gn
#dim. for a hidden layer of hn
(cid:15)
Initial scale parameter for fn
Stop gradient for projection

range
10−5 – 10−3
10−10 – 10−6
10 – 100
{ AdamW, Adam, RMSProp}
0 – 3
0 – 3
0 – 3
8 − 32
8 − 32
8 − 32
0 – 1.0
10−5 – 0.1
true, false

type
log scale
log scale
integer
categorical
integer
integer
integer
integer
integer
integer
log scale
log scale
boolean

Table 2: Selected parameters for each benchmark

parameter name
learning rate
weight decay
batch size
optimizer
#layer for fn
#layer for Gn
#layer for hn
#dim. for a hidden layer of fn
#dim. for a hidden layer of Gn
#dim. for a hidden layer of hn
(cid:15)
Initial scale parameter for fn
Stop gradient for projection

bistable
3.01 × 10−4
4.76 × 10−9
100
RMSProp
3
1
3
(17,10,22)
(34)
(10,62,58)
0.63
9.64 × 10−2
false

glucose insulin
3.28 × 10−4
2.28 × 10−9
100
RMSProp
1
2
2
(8)
(27,29)
(35,18)
0.75
8.94 × 10−2
false

E Glucose-Insulin system

Glucose concentration in the blood is modeled as a time-delay system regulated by insulin concentra-
tion (See Fig. 7 (A)) [32]. Suppose that G I, and X are the glucose, insulin, and accumulated glucose
plasma concentration ([mg/100ml],[µUI/ml],and [min mg/100ml], respectively) and u is the amount
of ingested glucose per minute [min−1 mg/100ml]. The dynamics of each concentration is given by

˙G(t) = −k1G(t) − k2G(t)I(t) + g0 + u(t),
k4
τ

X(t),

˙I(t) = −k3I(t) +
˙X(t) = G(t) − G(t − τ ),
y(t) = [G(t), I(t)]T,

where k1 is a spontaneous glucose disappearance rate, k2 is an insulin-dependent glucose disappear-
ance rate, g0 is a constant increase in plasma glucose concentration, k3 is an insulin disappearance
rate, k4 is an insulin release rate per the average glucose concentration within the last τ minute.

18

Table 3: Model parameters.

Parameter
k1
k2
k3
k4
g0
τ

Value
3.35 × 10−2
5.22 × 10−5
1.055
0.293
3.13
6

Unit
1
min
1
min(µUI/ml)
1
min
(µUI/ml)
min(mg/100ml)
(mg/100ml)
min
min

(A)

(B)

Figure 7: (A) Overview and (B) input and output behavior of the glucose insulin system.

This system has a unique asymptotically stable equilibrium point (G, I, X) ≡ (G∗, I ∗, X ∗) on the
nonlinear plain such that

G∗ =

I ∗ =

X ∗ =

−k1k3 + (cid:112)(k1k3)2 + 4k2k3k4g0
2k2k4
−k1k3 + (cid:112)(k1k3)2 + 4k2k3k4g0
2k2k3
−k1k3 + (cid:112)(k1k3)2 + 4k2k3k4g0
2k2k4

,

,

τ.

Here we set the initial state of this model as G(0) = G∗, I(0) = I ∗, X(0) = X ∗ and set the model
parameters as shown in the following Table 3. Furthermore, we adopt the output of the previous
oral glucose absorption system [33] as u (See Fig. 7 (B)), and the glucose absorption amount u is
normalized based on human blood volume per body weight (0.80[100 ml/kg]).

19

