1
2
0
2

r
a

M
6
2

]
L
P
.
s
c
[

3
v
8
4
3
0
0
.
4
0
0
2
:
v
i
X
r
a

Probabilistic Type Inference by Optimising
Logical and Natural Constraints(cid:63)

Irene Vlassi Pandi1, Earl T. Barr2, Andrew D. Gordon1,3, and Charles
Sutton1,4,5

1 University of Edinburgh, Edinburgh, UK
irene.vp@ed.ac.uk
2 University College London, London, UK
e.barr@ucl.ac.uk
3 Microsoft Research Cambridge, Cambridge, UK
adg@microsoft.ac.uk
4 The Alan Turing Institute, London, UK
5 Google AI, Mountain View, USA
charlessutton@google.com

Abstract. We present a new approach to the type inference problem for
dynamic languages. Our goal is to combine logical constraints, that is,
deterministic information from a type system, with natural constraints,
that is, uncertain statistical information about types learnt from sources
like identiﬁer names. To this end, we introduce a framework for prob-
abilistic type inference that combines logic and learning: logical con-
straints on the types are extracted from the program, and deep learning
is applied to predict types from surface-level code properties that are
statistically associated. The foremost insight of our method is to con-
strain the predictions from the learning procedure to respect the logical
constraints, which we achieve by relaxing the logical inference problem of
type prediction into a continuous optimisation problem. We build a tool
called OptTyper to predict missing types for TypeScript ﬁles. OptTyper
combines a continuous interpretation of logical constraints derived by
classical static analysis of TypeScript code, with natural constraints ob-
tained from a deep learning model, which learns naming conventions for
types from a large codebase. By evaluating OptTyper , we show that the
combination of logical and natural constraints yields a large improvement
in performance over either kind of information individually and achieves
a 4% improvement over the state-of-the-art.

Keywords: Type Inference, Optional Typing, Continuous Relaxation,
Numerical Optimisation, Deep Learning, TypeScript

1

Introduction

Statically-typed programming languages aim to enforce correctness and safety
properties on programs by guaranteeing constraints on program behaviour. A

(cid:63) This work was supported by Microsoft Research through its PhD Scholarship Pro-

gramme.

 
 
 
 
 
 
2

I. Vlassi Pandi et al.

large scale user-study suggests that programmers beneﬁt from type safety [23];
use of types has also been shown to prevent ﬁeld bugs [14]. However, type safety
comes at a cost: these languages often require explicit type annotations, which
imposes the burden of declaring and maintaining these annotations on the pro-
grammer. Strongly statically-typed, usually functional languages, like Haskell or
ML, oﬀer type inference procedures that reduce the cost of explicitly writing
types but come with a steep learning curve [51].

Dynamically typed languages, which either lack or do not require type anno-
tations, are relatively more popular [34]. Initially designed for quick scripting or
rapid prototyping, these languages have begun reaching the limits of what can be
achieved without the help of type annotations, as witnessed by the heavy indus-
trial investment in and proliferation of static type systems for these languages
(TypeScript [35] and Flow [12] are just two examples). Retroﬁt for dynamic lan-
guages, these type systems include gradual [46] and optional type systems [10].
Like classical type systems, these type systems require annotations to provide
beneﬁts. Hence, reducing the annotation type tax for dynamic languages remains
an open research topic.

1.1 Probabilistic Type Inference

Probabilistic type inference has recently been proposed as an attempt to re-
duce the burden of writing and maintaining type annotations [55,42,24]. Just
as the availability of large data sets has transformed artiﬁcial intelligence, the
increased volume of publicly available source code, through code repositories like
GitHub [16] or GitLab [17], enables a new class of applications that leverage sta-
tistical patterns in large codebases [1]. Speciﬁcally, for type inference, machine
learning allows us to develop less strict type inference systems that learn to
predict types from uncertain information, such as comments, names, and lexical
context, even when traditional type inference procedures fail to infer a useful
type.

The classic literature on conventional type systems takes great care to demon-
strate that type inference only suggests sound types [36,39]. Probabilistic type
inference is not in conﬂict with classical type inference but complements it. There
are settings, like TypeScript, where correct type inference is too imprecise. In
these settings, probabilistic type inference helps the human in the loop to move
a partially typed codebase—one lacking so many type annotations that classical
type inference can make little progress—to a suﬃciently annotated state that
classical type inference can take over and ﬁnish the job.

Two examples of probabilistic type inference tools are JSNice [42], which
uses probabilistic graphical models to statistically infer types of identiﬁers in
programs written in JavaScript, and DeepTyper [24], which targets TypeScript
via deep learning techniques. However, none explicitly models the underlying
type inference rules, and, thus, their predictions ignore useful type constraints.
Most recently, Wei et al. [55] introduced LambdaNet to exploit both type con-
straints and name usage information by using graph neural networks [1]. Lamb-
daNet, however, does not constrain the output of the network to satisfy these

Probabilistic Type Inference by Optimising Logical and Natural Constraints

3

constraints; this must be learnt automatically from data, and there is no guar-
antee that the resulting model will respect the type constraints at test time.
Indeed, in practice, we observe that LambdaNet produces annotations that do
not respect the learnt logical relationships (see Fig. 5).

Recognising this problem TypeWriter [41] combines a deep learning model
and a type checker for probabilistic type prediction for Python. TypeWriter
proposes enumerating the top-ranked predictions from a neural type predictor,
and subsequently ﬁltering those that do not type-check. A key diﬀerence is that
OptTyper incorporates both sorts of constraint into a single prediction step,
whereas TypeWriter validates the probabilistically predicted types in a second,
potentially expensive, step using a type checker.

1.2 Our Contribution

In our view, probabilistic type inference should be considered as a constrained
problem, as it makes no sense to suggest types that violate type constraints.
To respect this principle, we propose OptTyper (from “optimising for optional
types”), a principled framework for probabilistic type inference that couples
hard, logical type constraints with soft constraints drawn from structural, nat-
ural patterns into a single optimisation problem. While, in theory, there is the
option of ﬁltering out the incorrect predictions, our framework goes beyond that;
our composite optimisation serves as a communication channel between the two
diﬀerent sources of information.

Current type inference systems rely on one of two sources of information

(I) Logical Constraints on type annotations that follow from the type system.
These are the constraints used by standard deterministic approaches for
static type inference.

(II) Natural Constraints are statistical constraints on type annotations that can
be inferred from relationships between types and surface-level properties such
as names and lexical context. These constraints can be learned by applying
machine learning to large codebases.

Our goal is to improve the accuracy of probabilistic type inference by combining
both kinds of constraints into a single analysis, unifying logic and learning into
a single framework. We start with a formula that deﬁnes the logical constraints
on the types of a set of identiﬁers in the program, and a machine learning model,
such as a deep neural network, that probabilistically predicts the type of each
identiﬁer.

The key idea behind our methods is a continuous relaxation of the logical
constraints [22]. This means that we relax the logical formula into a continuous
function by relaxing type environments to probability matrices and deﬁning a
continuous semantic interpretation of logical expressions. The relaxation has a
special property, namely, that when this continuous function is maximised with
respect to the relaxed type environment, we obtain a discrete type environment

4

I. Vlassi Pandi et al.

Logical Constraints

Static
Analysis

Constraints
Generator

Untyped Code

Optimiser

Typed Code

Pre-trained
ML Model

Natural Constraints

Fig. 1: Overview of general framework that combines logical and natural con-
straints in a single optimisation problem.

that satisﬁes the original constraints. The beneﬁt of this relaxation is that log-
ical constraints can now be combined with the probabilistic predictions of type
assignments that are produced by machine learning methods.

More speciﬁcally, this allows us to deﬁne a continuous function over the
continuous version of the type environment that sums the logical and natural
constraints. And once we have a continuous function, we can optimise it: we
set up an optimisation problem that returns the most natural type assignment
for a program, while at the same time respecting type constraints produced by
traditional type inference. Our main contributions follow:

• We introduce a general, principled framework that uses soft logic to combine
logical and natural constraints for type inference, based on transforming a
type inference procedure into a single numerical optimisation problem.

• We instantiate this framework in OptTyper , a probabilistic type inference

tool for TypeScript.

• We evaluate OptTyper and ﬁnd that combining logical and natural con-
straints has better performance than either alone. Further, OptTyper out-
performs state-of-the-art systems, LambdaNet, DeepTyper and JSNice.
• We show how OptTyper achieves its high performance by combining Logical

and Natural constraints as an optimisation problem at test time.

2 General Framework for Probabilistic Type Inference

This section introduces our general framework, shown in Fig. 1 which we instan-
tiate in the next section by building a tool for predicting types in TypeScript.

Probabilistic Type Inference by Optimising Logical and Natural Constraints

5

Inject Code with
Type Parameters

Untyped Code

function addNum<START, END, ADDNUM> (
start: START, end: END): ADDNUM {

var addN = start + end;
return addN;

}

(a) A minimal function with unknown type signature.

Continuous Space

Logical Constraints

Variable

Type

l1: number l2: string lT : any

x1: START
x2: END
xV : ADDNUM

p1,1
p2,1
pV,1

p1,2
p2,2
pV,2

p1,T
p2,T
pV,T

E = [(START is number) and (END is number)]
or
[(START is string) and (END is string)]

[[E]]P = (p1,1 ∗ p2,1) + (p1,2 ∗ p2,2) −
[(p1,1 ∗ p2,1) ∗ (p1,2 ∗ p2,2)]

(b) Probability matrix P = [p1, . . . , pV ]T, T = V = 3.

(c) Generated logical constraints for type parameters

and their corresponding relaxation.

Natural Constraints

min
P

(cid:80)

v ||pv − µv||2

2 − λ[[E]]P

Logical Constraints Relaxation

number string any

number string any

number string any

0.84
START
END
0.60
ADDNUM 0.82

0.11 0.05
0.08 0.32
0.14 0.04

0.90
START
END
0.86
ADDNUM 0.83

0.07 0.03
0.05 0.09
0.14 0.03

0.05
START
END
0.06
ADDNUM 0.01

0.93 0.02
0.93 0.01
0.01 0.98

(d) Prob. matrix M = [µ1, . . . , µV ]T.

(f) Combined probability matrix.

(e) Prob. matrix P after optimisation.

Typed Code

function addNum(start: number,

end: number): number

(g) Resulting typed annotated code.

Fig. 2: An overview of the three type inference procedures via a minimal example.

Fig. 2 illustrates our general framework through a simpliﬁed running example
of predicting types.

Our input is a minimal function with no type annotations on its parameters
or result. Our goal is to exploit both logical and natural constraints to suggest
more speciﬁc types. To begin, in Box (a), we propose fresh type annotations
START and END (uppercasing the identiﬁer) for each parameter and ADDNUM for the
return type. We insert these annotations into the function’s deﬁnition. Our logical
constraints on these types represent knowledge obtained by a symbolic analysis
of the code in the function’s body. In our example, the use of a binary operation
implies that the two parameter types are equal. Box (c) shows a minimal set of

6

I. Vlassi Pandi et al.

logical constraints that state that addNum’s two operands have the same type.
In general, the logical constraints can be much more complex than our simple
example. If we only have logical constraints, we cannot tell whether string or
number is a better solution, and so may fall back to the type any. The crux
of our approach is to take into account natural constraints; that is, statistical
properties learnt from a source code corpus that seek to capture human intention.
In particular, we use a machine learning model to capture naming conventions
over types. We represent the solution space for our logical or natural constraints
or their combination as a V ×T matrix P of the form in Box (b): each row vector
is a discrete probability distribution over our universe of T = 3 concrete types
(number, string, and any) for one of our V = 3 identiﬁers. Box (d) shows the
natural constraints M induced by the identiﬁer names for the parameters and
the function name itself. Intuitively, Box (d) shows that a programmer is more
likely to name a variable start or end if she intends to use it as a number than
as a string. We can relax the boolean constraint to a numerical function on
probabilities as shown in Box (c). When we numerically optimise the resulting
expression, we obtain the matrix in Box (e); it predicts that both variables
are strings with high probability. Although the objective function is symmetric
between string and number, the solution in (e) is asymmetric because it depends
on the initialisation of the optimiser. Finally, Box (f) shows an optimisation
objective that combines both sources of information: E consists of the logical
constraints and each probability vector µv (the row of M for v) is the natural
constraint for variable v. Box (f) also shows the solution matrix and Box (g)
shows the induced type annotations, now all predicted to be number.

2.1 An Outline of Probabilistic Type Inference

We consider a dynamic language of untyped programs that is equipped with an
existing deterministic type system, that requires type annotations on identiﬁers.
Given a program U plus a typing environment Γ let Γ (cid:96) U mean that the
program U is well-typed according to the (deterministic) type system, given
types for identiﬁers provided by Γ . Formally, a typing environment Γ is a ﬁnite
function with domain {xv | v ∈ 1 . . . V }, where xv is an identiﬁer, and range
{lτ | τ ∈ 1 . . . T }, where each lτ is a literal type. Given an untyped program U ,
let probabilistic type inference consist of these steps:

1. We choose a ﬁnite universe of T distinct literal types {lτ | τ ∈ 1 . . . T }.
2. We compute a set {xv | v ∈ 1 . . . V } of a number V of distinct identiﬁers in

U that need to be assigned types.

3. We extract a set of constraints from U .
4. By optimising these constraints, we construct a matrix P with V rows and
T columns, such that each row is a probability vector (a discrete distribution
over the T literal types).

5. For each identiﬁer xv, we set type tv to the literal type lτ which we compute
from the vth probability vector (the one for identiﬁer xv). In this work, we
pick the column τ that has the maximum probability in xv’s probability
vector.

Probabilistic Type Inference by Optimising Logical and Natural Constraints

7

6. The outcome is the environment Γ = {xv : tv | v ∈ 1 . . . V }.

We say that probabilistic type inference is successful if Γ (cid:96) U , that is, the
untyped program U is well-typed according to the deterministic type system.
Since several steps may involve approximation, the prediction Γ may only be
partially correct. Still, given a known ˆΓ such that ˆΓ (cid:96) U we can measure how
well Γ has predicted the identiﬁers and types of ˆΓ . A key idea is that there are
two sorts of constraints in step (3): logical constraints and natural constraints.
A logical constraint is a formula E that describes necessary conditions for U
to be well-typed. In principle, E can be any formula such that if Γ (cid:96) U, then
Γ satisﬁes E. Thus, the logical constraints do not need to uniquely determine
Γ . For this reason, a natural constraint encodes less-certain information about
Γ , for example, based on comments or names. Just as we can conceptualise
the logical constraints as a function to the set of boolean values {0, 1}, we can
conceptualise the natural constraints as functions that map Γ to the set of
probabilities [0, 1], which can be interpreted as a prediction of the probability
that Γ would be successful. To combine these two constraints, we relax the
boolean operations to continuous operators on [0, 1]. Since we can conceptualise
E as a function that maps Γ to a boolean value {0, 1}, we relax this function to
map to [0, 1], using a continuous interpretation of the semantics of E. Similarly,
we relax Γ to a V ×T matrix of probabilities. Having done this, we formalise type
inference as a problem in numerical optimisation, with the goal to ﬁnd a relaxed
type assignment that satisﬁes as much as possible both sorts of constraints. The
result of this optimisation procedure is the matrix P of probabilities described
in step (4).

2.2 Logical Constraints in Continuous Space

Logical constraints are extracted from our untyped input program U using stan-
dard program analysis techniques. Here, we rely on a Constraint Generator for
this purpose. Section 3.2 describes its realisation. The generator takes into ac-
count a set of rules that the type system enforces and produces a boolean type
constraint for them.

In this work, we consider the following logical constraints.

Deﬁnition 1 (Grammar of Logical Constraints). A logical constraint is
an expression E of the following form:

E ::= xv is lτ
| not E

| E and E

| E or E.

(1)

Let E be the set of all logical constraints.

Recall that a typing environment Γ is a ﬁnite function with domain {xv | v ∈
1 . . . V }, and range {lτ | τ ∈ 1 . . . T }. The standard logical satisfaction relation

8

I. Vlassi Pandi et al.

Γ |= E, is deﬁned by induction on the structure of E, as follows. The typing
environment Γ plays the role of a model for the formula E.

Γ |= xv is lτ ⇔ Γ (xv) = lτ
Γ |= not E
⇔ not Γ |= E
Γ |= E1 and E2 ⇔ Γ |= E1 and Γ |= E2
Γ |= E1 or E2 ⇔ Γ |= E1 or Γ |= E2.

(2)

Continuous Relaxation We explain how to specify a continuous relaxation of the
discrete logical semantics. Intuitively, the logical semantics deﬁnes a truth func-
tion that maps typing environments to {0, 1}; a continuous relaxation extends
the range of the truth function to [0, 1]. To see this, start with two auxiliary
deﬁnitions:

– We deﬁne Π V ×T to be the set of all probability matrices of size V × T , that
(cid:3)T

is, matrices of the form P = (cid:2)p1 . . . pV
is a vector that deﬁnes a probability distribution over concrete types.

, where each pv = (cid:2)pv,1 . . . pv,T

(cid:3)T

– We convert an environment Γ into a V × T binary matrix B(Γ ) by setting
bv,τ = 1 if (xv, lτ ) ∈ Γ, and 0 otherwise. Each binary matrix is also a
probability matrix: B(Γ ) ∈ Π V ×T .

Given a formula E, we deﬁne a truth function fE : {0, 1}V ×T → {0, 1} that
maps binary matrices to {0, 1}, namely, for all Γ, we deﬁne fE(B(Γ )) = 1 if
and only if Γ |= E. A relaxed semantics is a continuous function that always
agrees with the logical semantics, that is, a relaxed semantics is a function ˜fE :
Π V ×T → [0, 1] such that for all formulas E and environments Γ , ˜fE(B(Γ )) =
fE(B(Γ )). Essentially, a relaxed semantics extends the domain and range of fE
to be continuous instead of discrete.

Our continuous semantics (or relaxed semantics) [[E]]P is a function Π V ×T ×

E → [0, 1], deﬁned by induction on the structure of E, as follows.

[[xv is lτ ]]P = pv,τ

[[not E]]P = 1 − [[E]]P
[[E1 and E2]]P = [[E1]]P · [[E2]]P

[[E1 or E2]]P = [[E1]]P + [[E2]]P − [[E1]]P · [[E2]]P .

(3)

(In the actual implementation, we use logits instead of probabilities for numerical
stability, see Appendix A.) Our semantics is based on standard many-valued
interpretations of propositional logic formulas as described for instance by Hajek
et al. [22]; however, our atomic propositions xv is lτ and their interpretation via
the matrix P are original. To interpret conjunction, we use what is known as
the product t-norm, where the conjunction of two constraints is interpreted as
the numeric product of their interpretations: [[E1 and E2]]P = [[E1]]P · [[E2]]P .
We make this choice because the numeric product is smooth and ﬁts with our
optimisation-based approach. The product t-norm has already been used for

Probabilistic Type Inference by Optimising Logical and Natural Constraints

9

obtaining relaxed logical semantics in machine learning, for example in [44].
Other choices are possible as we discuss in ??.

Our continuous semantics respects the duality between conjunction and dis-

junction:

Lemma 1 (Duality). For all E1, E2, and P :

1. [[not (E1 and E2)]]P = [[(not E1) or (not E2)]]P
2. [[not (E1 or E2)]]P = [[(not E1) and (not E2)]]P

The following asserts essentially that the relaxed semantics is a continuous

function that always agrees with the logical semantics.

Theorem 1 (Relaxation). For all E and Γ , [[E]]B(Γ ) = 1 ⇔ Γ |= E.

The proof is by induction on the structure of E; the details are in Appendix B.
Our general formulation of a relaxed semantics was in terms of the functions
f and ˜f , where we deﬁned f by fE(B(Γ )) = 1 if and only if Γ |= E. We sought
a relaxed semantics ˜f which we can now deﬁne by: ˜fE(P ) = [[E]]P . Our desired
equation ˜fE(B(Γ )) = fE(B(Γ )), for all formulas E and environments Γ , follows
as a corollary of Theorem 1.

Recall that in our setting, we know E but do not know Γ . To address that,
we observe that because the continuous semantics ˜fE(P ) = [[E]]P is a function of
P , we can optimise numerically the function ˜fE(P ) with respect to P ∈ Π V ×T .
If the optimisation is successful in ﬁnding an optimal value P ∗ such that P ∗ ≈
B(Γ ) for some Γ and that ˜fE(P ∗) = [[E]]P ∗ = 1, then the theorem tells us that
we have a typing environment Γ that models E.

2.3 Natural Constraints via Machine Learning

A complementary source of information about types arises from statistical de-
pendencies in the source code of the program. For example, names of variables
provide information about their types [59], natural language in method-level
comments provide information about function types [31], and lexically nearby
tokens provide information about a variable’s type [55,24]. This information is
indirect, and extremely diﬃcult to formalise, but we can still hope to exploit it
by applying machine learning to large corpora of source code.

Recently, the software engineering community has adopted the term natural-
ness of source code to refer to the concept that programs have statistical regu-
larities because they are written by humans to be understood by humans [25].
Following the idea that the naturalness in source code may be in part respon-
sible for the eﬀectiveness of this information, we refer generically to indirect,
statistical constraints about types as natural constraints. Because natural con-
straints are uncertain, they are naturally formalised as probabilities. A natural
constraint is a mapping from a type variable to a vector of probabilities over
possible types.

10

I. Vlassi Pandi et al.

Deﬁnition 2 (Natural Constraints). For each identiﬁer xv in a program
U , a natural constraint is a probability vector µv = [µv,1, . . . , µv,T ]T. We aggre-
gate the probability vectors of the learning model in a matrix deﬁned as M =
(cid:2)µ1 . . . µV

(cid:3)T

.

In principle, natural constraints can be deﬁned based on any property of
U , including names and comments. In this paper, we consider a simple but
practically eﬀective example of natural constraint, namely, a deep network that
predicts the type of a variable from the characters in its name. We consider each
variable identiﬁer xv to be a character sequence (cv1 . . . cvN ), where each cvi is a
character. (This instantiation of the natural constraint is deﬁned only on types
for identiﬁers that occur in the source code, such as a function identiﬁer or a
parameter identiﬁer.) This is a classiﬁcation problem, where the input is xv, and
the output classes are the set of T concrete types. Ideally, the classiﬁer would
learn that identiﬁer names that are lexically similar tend to have similar types,
and speciﬁcally which subsequences of the character names, like lst, are highly
predictive of the type, and which subsequences are less predictive. One simple
way to do so is to use a recurrent neural network (RNN).

For our purposes, an RNN is simply a function (hi−1, zi) (cid:55)→ hi that maps a
state vector hi−1 ∈ RH and an arbitrary input zi to an updated state vector hi ∈
RH . (The dimension H is one of the hyperparameters of the model, which can
be tuned to obtain the best performance.) The RNN has continuous parameters
that are learned to ﬁt a given data set, but we elide these parameters to lighten
the notation, because they are trained in a standard way. We use a particular
variant of an RNN called a long-short term memory network (LSTM) [26], which
has proven to be particularly eﬀective both for natural language and for source
code [47,33,56,28]. We write the LSTM as LSTM(hi−1, zi).

With this background, we can describe the speciﬁc natural constraint that
we use. Given the name xv = (cv1 . . . cvN ), we input each character cvi to the
LSTM, obtaining a ﬁnal state vector hN , which is then passed as input to a
small neural network that outputs the natural constraint µv. That is, we deﬁne

hi = LSTM(hi−1, cvi)
µv = F (hN ),

i ∈ 1, . . . , N

(4a)

(4b)

where F : RH → RT is a simple neural network. In our instantiation of this
natural constraint, we choose F to be a feedforward neural network with no
additional hidden layers, as deﬁned in (10). We provide more details regarding
the particular structure of our neural network in Section 3.3.

This network structure is, by now, a fairly standard architectural motif in
deep learning. More sophisticated networks could certainly be employed, but are
left to future work.

Probabilistic Type Inference by Optimising Logical and Natural Constraints

11

2.4 Combining Logical and Natural Constraints to Form an

Optimisation Problem

Logical constraints pose challenges to the probabilistic world of machine learn-
ing. Neural networks cannot handle hard constraints explicitly and thus it is not
straightforward how to incorporate the logical rules that they must follow. Our
way around that problem is to relax the logical constraints to numerical space
and combine them with the natural constraints through a continuous optimisa-
tion problem.

Intuitively, we design the optimisation problem to be over probability ma-
trices P ∈ Π V ×T ; we wish to ﬁnd P that is as close as possible to the natural
constraints M subject to the logical constraints being satisﬁed. A simple way
to quantify the distance is via the Euclidean norm || · ||2 of a vector, which is a
convex function and thus well suited with our optimisation approach. Hence, we
obtain the constrained optimisation problem

min
P ∈RV ×T

s.t.

(cid:88)

||pv − µv||2
2

v
pvτ ∈ [0, 1],
T
(cid:88)

pvτ = 1,

τ =1
[[E]]P = 1.

∀v, τ

∀v

(5)

We use Mean Squared Error (MSE) here to quantify the performance of
our ﬁtting. We could have used the Cross Entropy (CE), another common loss
function. The MSE is a proper scoring rule [18], meaning that smaller values
correspond to better matching of our optimisation variables with the logical
constraints. We do not claim any particular advantage of MSE versus CE.

Instead of solving optimisation problem (5), we proceed to make some re-
marks that exploit its structure. First, we reparameterise the problem to remove
the probability constraints, by using the softmax function

σ(x) =

(cid:20) exp{x1}
(cid:80)

i exp{xi}

,

exp{x2}
i exp{xi}

(cid:80)

(cid:21)T

, · · ·

,

(6)

which maps real-valued vectors to probability vectors. Our transformed problem
takes the form

min
Y ∈RV ×T

(cid:88)

v

||σ(yv)T − µv||2
2

(7)

s.t.

[[E]][σ(y1),...,σ(yV )]T − 1 = 0.

It is easy to see that for Y ∗ that minimises (7), then P ∗ = [σ(y1), . . . , σ(yV )]T
minimises (5). We remove the last constraint by introducing a multiplier λ ∈ R,
yielding the ﬁnal form of our optimisation problem

min
Y,λ

(cid:88)

v

||σ(yv)T − µv||2

2 − λ(cid:0)[[E]][σ(y1),...,σ(yV )]T − 1(cid:1).

(8)

12

I. Vlassi Pandi et al.

This corresponds to the Lagrange multiplier method [7]. According to the ﬁrst-
order necessary conditions, at a saddle point of the objective function of (8) the
following conditions should be satisﬁed

∇Y L(Y, λ) = 0
∇λL(Y, λ) = 0,

(9a)

(9b)

where L(Y, λ) equals the objective of (8). Equation (9b) guarantees that at
our optimisation’s problem solution, the equality constraint in (7) is satisﬁed.
In general, the necessary conditions are concerned with a saddle point. In our
experience, we have not faced any issue converging to optimal solutions when
starting with large initial values for λ. Nevertheless, a more systematic study is
required for removing such possibilities.

Finally, we note that by adding more terms to the combined objective func-
tion, we can extend the sources of information we are getting as inputs to other
channels, such as dynamic analysis.

3 OptTyper : Predicting Types for TypeScript

To evaluate our approach in a real-world scenario, we implement an end-to-end
application, called OptTyper , which aims to suggest missing types for TypeScript
ﬁles. The goal of OptTyper ’s implementation is to serve as a proof of concept for
our general framework. Thus, we acknowledge that our mechanisms to generate
logical and natural constraints are both pragmatic under-approximations, and
could in further work be replaced by more sophisticated mechanisms.

Regarding natural constraints, as every learning model outputs a probability
vector over types, we can extend our method to include natural constraints
generated by LambdaNet [55], DeepTyper [24] or indeed any other deep learning
approach that oﬀers the same kind of output.

Regarding logical constraints, unfortunately the TypeScript typechecker does
not produce explicit logical formulas to constrain unknown types. Therefore, to
build our prototype we generate logical constraints by relying on a mode of
operation where the compiler infer some types from usage on TypeScript code.
By doing that we obtain less information than we would if a full typechecker were
to emit logical constraints, but nonetheless show state-of-the-art performance.

Even with the limitations of these two implementation choices for extracting
logical and natural constraints, we show that OptTyper outperforms JSNice,
DeepTyper, and LambdaNet.

3.1 Background: TypeScript’s Type System

TypeScript [35] is a typed superset of JavaScript designed for developing large-
scale, stable applications. TypeScript’s compiler (tsc) typechecks TypeScript
programs then emits plain JavaScript, to leverage the fact that JavaScript is the
only cross-platform language that runs in any browser, any host, and any OS.

Probabilistic Type Inference by Optimising Logical and Natural Constraints

13

Structural type systems consider record types (classes), whose ﬁelds or mem-
bers have the same names and types, to be equal. TypeScript supports a struc-
tural type system because it permits TypeScript to handle many JavaScript
idioms that depend on dynamic typing. One of the main goals of TypeScript’s
designers is to support idiomatic JavaScript to provide a smooth transition from
JavaScript to TypeScript. Therefore, TypeScript’s type system is deliberately
unsound [8]. It is an optional type system, whose annotations can be omitted
and have no eﬀect on runtime. As, TypeScript erases them when transpiling to
JavaScript [8]. TypeScript’s type system defaults to assigning its any type to
unannotated parameters, and methods or properties signatures.

3.2 Logical Constraints for TypeScript

We cannot rely on the TypeScript compiler [35] tsc directly to generate the
logical constraints of Section 2.2, because tsc does not construct logical formulas
explicitly during typechecking. Still, we can rely on a mode of operation where
the compiler infers types on TypeScript code with no types annotations. To
ensure that no types are present to the input ﬁles the ﬁrst step of our process it
to parse the gold ones and remove all type annotations, then we continue with
generating the constraints.

Speciﬁcally, to generate logical constraints on argument types, we build on
a command-line tool, named TypeStat [19], that calls tsc to predict type hints
for function arguments, usually to provide codeﬁxes within a development en-
vironment. When the predicted type for an argument identiﬁer xv is a literal
lτ within our universe we emit the constraint xv is lτ . When the predicted
type for xv is a union type (l1 | · · · | ln) of literals, we emit the disjunction
((xv is l1) or . . . or (xv is ln)).

Inferring return types is easier for the compiler than inferring argument types.
We can harvest return types as is type constraints by calling tsc directly. Since
inference of the return types is more straightforward than argument types, we
see in Table 1 that the Logical phase of our method works better for return types
than parameters.

Overall, for each function or group of functions in a ﬁle, we return a con-
junction of the logical constraints generated for parameter and return types,
as described above. We note that our logical constraints include propositional
logic, and therefore are able to express a wide range of interesting type con-
straints [37,40].

3.3 Natural Constraints for TypeScript

To learn naming conventions over types we use a Char-Level LSTM which pre-
dicts for any identiﬁer a probability vector over all the available types. Our model
is trained on (id, type) pairs with the goal to learn naming conventions for iden-
tiﬁers, treated as sequences of characters. The main intuition behind this choice
is that developers commonly use multiple abbreviations for the same word and

14

I. Vlassi Pandi et al.

TypeScript

(id, type)
pairs

Char-Level
LSTM

Naming
Conventions

Github repos
˜300

# of pairs
˜45,000

64 epochs
Val. acc.: 0.81

Fig. 3: Pipeline of learning naming conventions with a Char-Level LSTM, repre-
sented by a probability vector for each identiﬁer.

this family of abbreviations shares a type. A Char-Level model is well-suited to
predict the type for any identiﬁer in an abbreviation families.

Data Set for the LSTM Following the work of [55] and [24], to train our model we
use as dataset the 300 most starred Typescript projects from Github, containing
between 500 to 10,000 lines of code. Our dataset was randomly split by project
into 70% training data, 10% validation data and 20% test data. Fig. 3 shows
a summary of the pipeline used to train our model, for speciﬁc implementation
details of the LSTM refer to Appendix C.

Prediction Space We deﬁne our type vocabulary to consist of top-100 most
common default library types in our training set. As Wei et al. [55] report, this
prediction space covers 98% of the non-any annotations for the training set.
Handling a larger set of types is straightforward, but we decided instead to work
with the same set of library types used by [24] and [55], to be consistent in our
comparisons. Conforming to the practice of prior work in this space [55,24,59,42],
we consider all diﬀerent polymorphic type arguments to be any; for example
a Promise<boolean> type corresponds to Promise<any>. Accordingly higher-
order functions correspond to the type Function.

Implementation Details Regarding the implementation details of the LSTM net-
work, for the F in (4b), we use a feedforward neural network

F (h) = log (cid:0)σ (cid:0)hAT + b(cid:1)(cid:1) ,

(10)

where the log function is applied componentwise, and A and b are learnable
weights and bias. The softmax function (6) corresponds to the last layer of our
neural network and essentially maps the values of the previous layer to [0, 1],
while the sum of all values is 1 as expected for a probability vector as already
explained. We work in log space to help numerical stability since computing (6)
directly can be problematic. As a result, F outputs values in [−∞, 0].

We train the model by supplying sets of variable identiﬁers together with their
known types, and minimising a loss function. Our loss function is the negative

Probabilistic Type Inference by Optimising Logical and Natural Constraints

15

log likelihood function—conveniently combined with our log output—deﬁned as

L(y) = −

(cid:88)

i

log(yi).

(11)

Essentially, we select, during training, the element that corresponds to the cor-
rect label from the output F and sum all the values of the correct labels for the
entire training set.

3.4 Combining Logical and Natural Constraints

In our framework both solving the relaxed logical constraints alone, and combin-
ing them with the natural constraints correspond to an optimisation problem as
described in (3) and (8) respectively. To ﬁnd the minimum of the generated func-
tion we use RMSprop [50]; an alternative to stochastic gradient descent [43],
with an adaptive learning rate. Both the code for the deep learning and the
combined optimisation part is written in PyTorch [38].

4 Evaluation of OptTyper

This section opens with Section 4, which deﬁnes the prediction and query spaces,
explains the experimental setup and establishes the performance measure we use
throughout the section. OptTyper is built to combine and exploit both logical
and natural constraints, so Section 4.1 quantiﬁes their separate contributions
and demonstrates their synergy. It then compares and analyses the performance
of OptTyper with that of LambdaNet and DeepTyper. It closes by comparing
OptTyper with JSNice, the pioneering work in probabilistic type inference.

Type Prediction Accuracy For a statically typed language, let a declaration slot
be a point in a program text where the grammar permits annotating an identiﬁer
with its type. The set of nodes that we predict types for includes variables (VAR),
parameters (PAR), properties declarations and signatures (PROP ), return types
for function declarations (FUN ) and method declarations (METH ).

Predicting types is a multi-class prediction task: at each annotation slot,
we ask the predictor to propose a type from our type vocabulary, T = {lτ |
τ ∈ 1 . . . T }. We note that T does not include the gradual any type or Out-Of-
Vocabulary OOV token. Concretely, Section 3.3 deﬁnes T . Even if we regard
some types as OOV for our evaluation, we do allow all return types inferred by
the Logical part of our method to be present at the ﬁnal output as they may be
useful for the programmer.

?? shows the query space for types in an optional type setting. TypeScript
itself deﬁnes built-in types. Library types are those types deﬁned by the libraries
a project imports and project types, sometimes called user types in the literature,
are those types a project deﬁnes itself. In general, developer annotations are
those slots that a developer is likely to annotate; in training data, we under-
approximate these slots by the slots that a developer did annotate. The compiler

16

I. Vlassi Pandi et al.

1

2

3

4

5

6

7

8

9

10

11

import { NotificationService, SimpleNotificationsModule }

from ’angular2-notifications’;

class AppComponent {
constructor(

public service: Object

) {}
open() {

this.service.create(’bla’, ’blu’);

}

}

Fig. 4: A snippet from flauc angular2-notifications project that fails to pre-
dict a correct type. On ln. 6 we predict that service is an Object as we don’t
have any Logical constraint we base our decision based on the LSTM’s predic-
tion but the compiler complains that Property ‘create’ does not exist
on type ‘Object’.The correct type is NotificationService, which has been
imported.

inferable slots are those slots for which an optional compiler can infer a type given
the developer annotations. Developers annotate some slots to document and
clarify the code, to aid navigation and completion, and so that the compiler can
infer types for other slots. Developer annotated slots are special for two reasons:
they provide a natural source of labelled training data and, since developers went
to the eﬀort of annotating them, relieving developers of the burden of doing so
is clearly useful.

Let TPi be the number of times that a probabilistic type predictor correctly
labelled a slot with the ith type in T ; let FPi be the number of times that the
type predictor incorrectly labelled a slot with the ith type. Our ground truth
for determining whether a prediction is correct is the set of developer annotated
slots and the set of types that the compiler can infer; in our test set, we call this
set the gold ﬁle. This is a working assumption in the sense that some ﬁles may
contain errors [57].

We report performance using Top-1 Accuracy [32], deﬁned as

(cid:80)T

i TPi
i (TPi + FPi )

(cid:80)T

,

(12)

where T = |T |. We perform our evaluation in all declaration and signatures
annotation slots available in a TypeScript ﬁle.

Test Set We evaluate OptTyper on the same 20% test set kept for the evaluation
of our Char-Level model. Our data set consists of 2074 ﬁles from 60 GitHub
packages with ∼10000 declaration annotation slots in total.

Probabilistic Type Inference by Optimising Logical and Natural Constraints

17

Table 1: Ablation analysis of OptTyper , the cells report Top-1 Accuracy(%);
FUN and METH refers to return types of functions and methods respectively,
PAR represents parameters, PROP represents properties, and VAR variables.
Test set of 2074 ﬁles from 60 GitHub packages on ∼10000 annotation slots [55].

Tool

FUN METH PAR PROP VAR ALL

Logical
0.69
Natural 0.42
OptTyper 0.69

0.71
0.52
0.71

0.06 0.08 0.42 0.23
0.82 0.40 0.37 0.69
0.85 0.43 0.56 0.76

Type Checking Results Our approach does not guarantee that the resulting code
always type check. Our only guarantee is that we respect the logical constraints
that we generate from the code. There are cases where these constraints under-
determine the problem and the natural constraints help to mitigate that, but
not necessarily successfully. Furthermore, optional typing checks for local type
consistency, not the whole program guarantee of traditional type checking and
that may lead to unspeciﬁed types from imports. For a speciﬁc example, where
this occurs and leads to a wrong prediction see Fig. 4. Nevertheless, bridging
two diﬀerent sources of information, logical and natural, is not a trivial process
and our aim in this paper is to show a principled way to tackle this problem by
solving a corresponding numerical optimisation problem.

4.1 Ablation Analysis: Leveraging Both Logical and Natural

Constraints

OptTyper has two phases — Logical and Natural — and combines them. To
evaluate the eﬀect of each stage, Table 1 reports the Top-1 Accuracy of each
stage. Table 1’s columns deﬁne disjoint sets of slots. Regarding the Logical ap-
proach, the results are signiﬁcantly better for function and method return types
than parameter types. This happens because the OptTyper harvests a richer
set of constraints for return types. For parameter types, the Logical stage (Sec-
tion 3.2) most of the times fails to infer the corresponding type from its usage
in code. An example where no hard constraint is available and the Char-Level
fails to ﬁnd a correct type is shown below in Fig. 4.

For the Natural phase, the results swap, the prediction accuracy for the pa-
rameters’ is almost double than the one for the return types. Our assumption
is that this is a result of largely using the same parameters ids over diﬀerent
projects, for example path, than using the same function or methods ids. Nev-
ertheless, the results from our Char-Level model indicate that just the naming
of a variable carries a lot of information about its actual type; Overall, Table 1
show that the Logical and Natural phases complements each other and thus their
combination in OptTyper greatly improves our type inference capabilities.

We note here that the concrete implementations of both phases of OptTyper
serve as a proof of concept to our theoretical approach described in Section 2.

18

I. Vlassi Pandi et al.

Table 2: Top-1 Accuracy(%) for DeepTyper, LambdaNet and OptTyper . (Same
test set as Table 1.)

Tool

Acc(%)

DeepTyper
LambdaNet
OptTyper

0.61
0.72
0.76

Therefore, there is space of further improving the overall result by improving
each of the two components with more sophisticated techniques.

4.2 Comparison with DeepTyper and LambdaNet

DeepTyper There are two main diﬀerences between DeepTyper and OptTyper
that we need to address. Firstly, DeepTyper considers a much larger predic-
tion space of T = 1100 types, including many project types. Thus, to measure
the accuracy, we restrict the prediction space to 100 library types (a subset of
DeepTyper’s vocabulary, exactly those LambdaNet’s authors [55] chose when
comparing DeepTyper to LambdaNet. We note that our Logical phase is able to
infer types that are out of this small prediction space, but for a fair evaluation
we choose to treat them as OOV.

Secondly, DeepTyper predicts a type, sometimes diﬀerent, for each occur-
rence of an identiﬁer, while we predict types only for declaration slots. To ad-
dress this, we compare OptTyper with a DeepTyper variant proposed by Wei et
al. [55]. This variant makes a single prediction for each identiﬁer, by averaging
over the RRN internal states for a particular identiﬁer before the actual predic-
tion. The DeepTyper results we report are for this variant, retrained over the
vocabulary of 100 library types speciﬁed above, using Top-1 Accuracy.

Table 2 summarises the results of our comparisons. We conjecture that Opt-
Typer outperforms DeepTyper mainly because OptTyper ’s logical constraints
deﬁne a wider, lexically independent, prediction context than DeepTyper. Per-
haps taking into account only information in the vicinity as DeepTyper does
can be problematic; for instance, function deﬁnitions may be placed relatively
far away from their calls and hence the context, that DeepTyper learns, is not
very informative.

LambdaNet The comparison with LambdaNet is straightforward because they
provide a pretrained model trained on the same dataset and for the same set of
types as ours. Table 2 shows that our accuracy is on par with LambdaNet’s, and
indeed somewhat better. We think that a reason for the small diﬀerence between
LambdaNet’s perfomance in all declaration slots and the reported performance
in [55] is due to the fact that LambdaNet fails to learn types that are inferable for

Probabilistic Type Inference by Optimising Logical and Natural Constraints

19

1

2

3

4

5

6

7

8

9

function f1(

x: boolean,
z: Window,
y: Event
): number {

x = true;
z = window;
y =

Event.prototype;

return 1;

10

}

function P1{f1}(
P2{x}: L [ty]{Boolean},
P3{z}: (L [ty]{String} (cid:54)=
L [ty]{Window}),
P4{y}: (L [ty]{Number} (cid:54)=
L [ty]{Event})

): (P5: (L [ty]{Number}) {
P2{x} ← L {Boolean};
P3{z} ← L {window};
P4{y} ← L {Event}.prototype;
return P5 = L {Number}

}

(a) Gold TypeScript ﬁle.

(b) LambdaNet output.

Fig. 5: Minimal example showing 2 cases where LambdaNet gives incorrect pre-
dictions.

the compiler but yet not apparent in the training data. We have found examples
where this occurs. Fig. 5 shows two examples. The parameters on lines 3 and
4 actually have type Window and Event, as you can see in Section 4.2, which
contains the developer-annotated ground truth. Section 4.2, the ﬁgure on the
right, shows that LambdaNet mispredicts their types as String and Number.
We conjecture that the misprediction is because of data sparsity. LambdaNet
correctly predicts the type of the ﬁrst parameter because uses of boolean are
relatively common in the training data, while uses of Window and Event are not,
so the assignments on lines 7 and 8 provide too little signal for LambdaNet to pick
up. OptTyper , in contrast, correctly predicts all three parameter types. OptTyper
succeeds here because the assignments on lines 7 and 8 generate hard logical
constraints that OptTyper incorporates, at test time, into its optimisation search
for a satisfying type environment. These examples may explain the diﬀerence
between LambdaNet’s and OptTyper ’s prediction accuracy. This diﬀerence in
performance between the two approaches will crop up whenever the training
data lacks suﬃcient number of examples of a particular logical relation.

Finally, we note that we see LambdaNet and indeed any learning approach
is complementary to our work. In theory, it is straightforward to treat them as
an instantiation of the Natural phase, as a probability distribution over types
described in our theoretical framework (Section 2).

4.3 Comparison with JSNice

Only portions of the JSNice [42] system have been made open source. The por-
tion of the implementation that the authors have made public is not suﬃcient

20

I. Vlassi Pandi et al.

Table 3: Top-1 Accuracy for JSNice and OptTyper . (Same test set as Table 1.)

Tool Acc

JSNice 0.45
OptTyper 0.74

to retrain the JSNice models. Instead, we follow the approach of DeepTyper [24]
and LambdaNet [55] and manually compare JSNice and OptTyper over a smaller
dataset. As JSNice targets JavaScript, it cannot predict types for classes or inter-
faces, so we report accuracy on top-level functions return types and parameters
randomly sampled from our test set. The prediction space for this comparison
is restricted to JavaScript primitive types.

As Table 3 shows, OptTyper outperforms JSNice. We conjecture that this
is because JSNice exploits the relation paths between types up to a shallow
depth, and thus it may not capture some typing relevant element dependencies.
In contrast, our logical constraints can leverage information of a more expan-
sive context. Additionally, OptTyper ’s grouping together of names that share a
type despite minor variations in their names could be proven useful for learning
techniques.

5 Related Work

OptTyper is a new form of probabilistic type inference that optimises over both
logical and natural constraints. Related work spans classical, deterministic type
inference, soft logic for the relaxation of the constraints, and earlier machine
learning approaches.

5.1 Classical Type Inference

Rich type inference mitigates the cost of explicitly annotating types. This feature
is an inherent trait of strongly, statically-typed, functional languages (like Haskell
or ML).

Dynamic languages have also started to pay more attention to typings. Sev-
eral JavaScript extensions, like Closure Compiler [20], Flow [12] and TypeScript
(See Section 3.1) are all focusing on enabling sorts of static type checking for
JavaScript. However, these extensions often fail to scale to realistic programs
that make use of dynamic evaluation and complex libraries, for example, jQuery,
which cannot be analysed precisely [27]. There are similar extensions for other
popular scripting languages, like [49], an optional static type checker for Python,
or RuboCop [6], which serves as a static analyzer for Ruby by enforcing many
of the guidelines outlined in the community Ruby Style Guide [5].

The quest for more modular and extensible static analysis techniques has
resulted in the development of richer type systems. Reﬁnement types, that is,

Probabilistic Type Inference by Optimising Logical and Natural Constraints

21

subsets of types that satisfy a logical predicate (like Boolean expression), con-
strain the set of values described by the type and hence allow the use of modern
logic solvers (such as SAT and SMT engines) to extend the scope of invariants
that can be statically veriﬁed. An implementation of this concept comes with
Logically Qualiﬁed Data Types, abbreviated to Liquid Types. DSOLVE is an
early application of liquid type inference in OCAML [45]. A type-checking al-
gorithm, which relies on an SMT solver to compute subtyping eﬃciently for a
core, ﬁrst-order functional language enhanced with reﬁnement types [9], provides
a diﬀerent approach. LiquidHaskell [52] is a static veriﬁer of Haskell based on
Liquid Types via SMT and predicate abstraction. DependentJS [11] incorpo-
rates dependent types into JavaScript. We note also, that HM(X) is a family
of constraint-based type systems [37,40], that ﬁts our formulation of the logical
constraints.

A line of research closely related to our work concerns the speciﬁc prob-
lem of predicting a TypeScript declaration ﬁle for an underlying JavaScript li-
brary. Writing and maintaining a declaration ﬁle is a non-trivial process. Both
TSCHECK [13] and TSTEST [30] demonstrates the diﬃcult of the task, by
detecting numerous errors in the declaration ﬁles of most of the libraries they
have checked. [29] created the TSINFER and TSEVOLVE tools to that to assist
the programmer for creating and maintaining TypeScript declaration ﬁles from
JavaScript ﬁles. These tools are based on a combination of a static and dynamic
analysis that uses a recorded snapshot of a concretely initialised library to gener-
ate TypeScript declaration ﬁles from JavaScript libraries. Dynamic analysis for
TypeScript could serve as a diﬀerent source of information in our type inference
framework. We could capture results from dynamic analysis by adding a new
term to our objective (8).

5.2 Probabilistic Type Inference

Although the interdisciplinary ﬁeld between machine learning and programming
languages is still young, complete reviews of this area are already available. A
detailed description of the area can be found in [53]. While [21] is a position
paper, which examines this research area by categorising the challenges involved
in three main, overlapping pillars: intention, invention, and adaptation. An ex-
tensively survey work that probabilistically models source code via a learning
component and complex representations of the underlying code is given in [1].

A sub-ﬁeld of this emerging area applies machine learning in probabilistic
graphical models to infer semantic properties of programs, such as types. The
ﬁrst example of this class of approach was JSNice [42], which uses probabilistic
graphical models to infer types (and deobfuscate names) for JavaScript ﬁles.
They use conditional random ﬁelds (CRF) [48], to encode variable relations
between types, but not type constraints. OptTyper diﬀers from JSNice in incor-
porating logical type constraints and reformulating probabilistic type inference
as an optimisation problem. LambdaNet [59] use a diﬀerent graphical model
to statistically infer types for Python. Their method trains a classiﬁer for each

22

I. Vlassi Pandi et al.

project that predicts the type of each variable from its name. The classiﬁer’s pre-
dictions for each variable are combined with semantic constraints using a kind
of graphical model called a factor graph [60], which is closely related to CRFs.
To build their factor graph, they leverage type hints derived from data ﬂow,
attribute accesses, subtyping, and naming conventions. Compared to our work,
[59] requires heuristically chosen weights in the factors that integrate naming
and semantic constraints; these heuristics may need to be tuned separately for
each new kind of semantic constraint that is added to the model. In contrast, our
method relies on an optimisation that integrates semantic constraints and nam-
ing constraints in a principled way. An important advantage of our approach is
that we can introduce type constraints from any standard type inference engine
by automatically relaxing them.

Most recent works have used deep learning approaches to tackle the prob-
lem of probabilistic type inference. Hellendoorn et al. were the ﬁrst to do so,
by building DeepTyper, a tool that infers types for partially typed TypeScript
code [24]. DeepTyper uses a bidirectional neural network that leverages local
lexical information to make type predictions for every identiﬁer but otherwise
ignores type constraints. A strength of DeepTyper is that it can handle a very
large type vocabulary that spans both library and user-deﬁned types.

NL2Type, a tool by [31], also takes a deep learning approach to the task, using
JSDoc comments as an additional type hint. Our approach diﬀers from Deep-
Typer and NL2Type in incorporating logical type constraints. Our reformulation
of probabilistic type inference as an optimisation problem that incorporates ex-
plicitly logical constraints separates us from these two approaches. Williams et
al. present a diﬀerent application of combining logical constraints and machine
learning for inferring units in spreadsheets [58].

LambdaNet [55] was the ﬁrst to apply a graph neural network (GNN) model
[15,3] to the probabilistic type inference task. LambdaNet targets TypeScript
and uses static analysis to build its GNN from the training data. This GNN com-
bines logical and contextual (which subsumes OptTyper ’s natural) constraints.
They are the ﬁrst to be able to predict unseen user-deﬁned types by using a
pointer-network-like architecture [54,4] to predict over an open vocabulary. Typ-
ilus [2] is a GNN that employs one-shot learning to predict an open vocabulary
of types, including rare and unseen user-deﬁned types, for Python. Interestingly,
both of these GNN models use an iterative computation called message-passing
to compute predictions, which is closely related to the sum-product algorithm
used for inference in [59], which also relies on message passing. The main diﬀer-
ence between our method and these two GNN-based approaches is that we do
not learn the logical constraints but rather extracts and enforces them at test
time. Incorporating previously known, hard constraints into a learning model
is an eﬀective way to improve its overall performance, but it does not imply
that the result will respect them. What our approach does instead, is to oﬀer a
principled way to explicitly impose the logical constraints while constructively,
and only at the places where is needed, absorbing information from the natural
channel. In that sense, our work is complementary to each prior work above, as

Probabilistic Type Inference by Optimising Logical and Natural Constraints

23

the described tools output a probability vector over types, which our approach
can take as its input to the Natural part of our combined optimisation equation
(8).

TypeWriter [41] applies probabilistic type checking to infer argument and
result types for Python programs. To solve the problem that inferred types may
not correctly typecheck, TypeWriter invokes a standard gradual type checker to
validate diﬀerent combinations of types suggested by probabilistic inference, so to
suggest types that will correctly typecheck. Adding a second phase to OptTyper ,
in the spirit of TypeWriter, would guarantee that the results of OptTyper would
typecheck.

6 Conclusion

This paper addresses the lack of rich type inference process for dynamically
typed languages. To tackle this, we combine Logical constraints, deterministic
information from a type system, with Natural constraints, uncertain informa-
tion about types, learnt by machine learning techniques, while focusing on the
satisfaction of the typing rules dictated by the language.

A core aim of our method is to guide the predictions from the learning pro-
cedure to respect the logical constraints. We achieve this by relaxing the logical
type inference problem into a continuous space. This allows us to constructively
combine the natural and logical part in a single optimisation problem with guar-
anteed constraint satisfaction. Our framework is extensible: it can incorporate
information from arbitrary models into its natural part and type constraints
generated by traditional deterministic type inference systems.

We evaluate our framework by implementing OptTyper , a tool that predicts
types for TypeScript. Our experiments show that OptTyper achieves an accu-
racy of 76% for Top-1 prediction on the standard test set [55] for probabilistic
type inference on TypeScript, an improvement of 4% on previous work. An ab-
lation study shows the improved performance arises from our new principled
combination of both Logical and Natural constraints at test time.

References

1. Allamanis, M., Barr, E.T., Devanbu, P., Sutton, C.: A survey of machine learning
for big code and naturalness. ACM Computing Surveys (CSUR) 51(4), 81:1–81:37
(Jul 2018). https://doi.org/10.1145/3212695

2. Allamanis, M., Barr, E.T., Ducousso, S., Gao, Z.: Typilus: Neural type hints. ArXiv

abs/2004.10657 (2020)

3. Allamanis, M., Brockschmidt, M., Khademi, M.: Learning to represent programs

with graphs (2017)

4. Allamanis, M., Peng, H., Sutton, C.: A Convolutional Attention Network for Ex-
treme Summarization of Source Code. In: International Conference in Machine
Learning (ICML) (2016)

5. Bastov, B.: Ruby style guide (2018), https://github.com/bbatsov/ruby-style-

guide/

24

I. Vlassi Pandi et al.

6. Bastov, B.: Rubycop (2018), https://github.com/bbatsov/rubocop/
7. Bertsekas, D.: Constrained Optimization and Lagrange Multiplier Methods.
Athena Scientiﬁc (1982). https://doi.org/10.1016/B978-0-12-093480-5.50001-5
8. Bierman, G., Abadi, M., Torgersen, M.: Understanding TypeScript. In: Jones, R.
(ed.) Lecture Notes in Computer Science. pp. 257–281. Springer, Berlin, Heidelberg
(2014). https://doi.org/10.1007/978-3-662-44202-9 11

9. Bierman, G.M., Gordon, A.D., Hritcu, C., Langworthy, D.E.: Semantic sub-
typing with an SMT solver. J. Funct. Program. 22(1), 31–105 (2012).
https://doi.org/10.1017/S0956796812000032

10. Bracha, G.: Pluggable type systems. In: OOPSLA workshop on revival of dynamic
languages. vol. 4. Association for Computing Machinery, New York, NY, USA (01
2004)

11. Chugh, R., Herman, D., Jhala, R.: Dependent types for javascript. In: Proceedings
of the ACM International Conference on Object Oriented Programming Systems
Languages and Applications. pp. 587–606. OOPSLA ’12, ACM, New York, NY,
USA (2012). https://doi.org/10.1145/2384616.2384659

12. Facebook: Flow (2019), https://ﬂow.org, [Online; accessed 24-February-2018]
13. Feldthaus,
script
1–16
http://doi.acm.org/10.1145/2714064.2660215

type-
49(10),
https://doi.org/10.1145/2714064.2660215,

of
SIGPLAN Not.

A.:
javascript

Checking
libraries.

A.,
interfaces

Møller,
for

correctness

2014).

(Oct

14. Gao, Z., Bird, C., Barr, E.T.B.: To type or not to type: Quantifying de-
International Conference on Software En-
IEEE, Piscataway, NJ, USA (2017).

In:
tectable bugs in javascript.
gineering. pp. 758–769.
ICSE ’17,
https://doi.org/10.1109/ICSE.2017.75

15. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message
passing for quantum chemistry. In: International Conference on Machine Learning
(ICML) (Apr 2017)

16. GitHub: GitHub (2020), https://github.com
17. GitLab: GitLab (2020), https://gitlab.com
18. Gneiting, T., Raftery, A.E.: Strictly proper scoring rules, prediction, and estima-
tion. Journal of the American Statistical Association 102(477), 359–378 (2007).
https://doi.org/10.1198/016214506000001437

19. Goldberg, J.K.: TypStat (2020), https://github.com/JoshuaKGoldberg/TypeStat
20. Google: Google Closure Compiler (2019), [Online; accessed 26-February-2018]
21. Gottschlich, J., Solar-Lezama, A., Tatbul, N., Carbin, M., Rinard, M., Barzi-
lay, R., Amarasinghe, S., Tenenbaum, J.B., Mattson, T.: The three pillars
of machine programming. In: Proceedings of the 2nd ACM SIGPLAN In-
ternational Workshop on Machine Learning and Programming Languages. p.
69–80. Association for Computing Machinery, New York, NY, USA (2018).
https://doi.org/10.1145/3211346.3211355

22. H´ajek, P.: Metamathematics of fuzzy logic. Trends in logic ; v. 4, Kluwer, Dordrecht

; London (1998). https://doi.org/10.1007/978-94-011-5300-3

23. Hanenberg, S., Kleinschmager, S., Robbes, R., Tanter, ´E., Steﬁk, A.: An empir-
ical study on the impact of static typing on software maintainability. Empirical
Software Engineering 19(5), 1335–1382 (2014). https://doi.org/10.1007/s10664-
013-9289-1

24. Hellendoorn, V.J., Bird, C., Barr, E.T., Allamanis, M.: Deep learning
type inference.
In: ACM Joint Meeting on European Software Engineer-
ing Conference and Symposium on the Foundations of Software Engineer-

Probabilistic Type Inference by Optimising Logical and Natural Constraints

25

ing. pp. 152–162. ESEC/FSE 2018, ACM, New York, NY, USA (2018).
https://doi.org/10.1145/3236024.3236051

25. Hindle, A., Barr, E.T., Su, Z., Gabel, M., Devanbu, P.: On the nat-
on Software Engi-
IEEE, Piscataway, NJ, USA (Jun 2012).

uralness
neering (ICSE). pp. 837–847.
https://doi.org/10.1109/ICSE.2012.6227135

International Conference

software.

In:

of

26. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation

9(8), 1735–1780 (1997). https://doi.org/10.1162/neco.1997.9.8.1735

27. Jensen, S.H., Møller, A., Thiemann, P.: Type analysis for JavaScript. In: Palsberg,
J., Su, Z. (eds.) Static Analysis. pp. 238–255. Springer, Berlin, Heidelberg (Aug
2009). https://doi.org/10.1007/978-3-642-03237-0 17

28. Khanh Dam, H., Tran, T., Pham, T.: A deep language model for software code

(Aug 2016), https://arxiv.org/abs/1608.02715

29. Kristensen, E.K., Møller, A.: Inference and evolution of TypeScript decla-
ration ﬁles. In: Huisman, M., Rubin, J. (eds.) Fundamental Approaches to
Software Engineering. pp. 99–115. Springer, Berlin, Heidelberg (Apr 2017).
https://doi.org/10.1007/978-3-662-54494-5 6

30. Kristensen, E.K., Møller, A.: Type

testing.
ACM on Programming Languages 1(OOPSLA), 90:1–90:25 (Oct 2017).
https://doi.org/10.1145/3133914

typescript

scripts

test

for

31. Malik, R.S., Patra, J., Pradel, M.: Nl2type:
In:

language information.

types from natural
Software Engineering. pp. 304–315.
https://doi.org/10.1109/ICSE.2019.00045

Inferring javascript
function
International Conference on
IEEE, Piscataway, NJ, USA (2019).

32. Manning, C.D., Raghavan, P., Sch¨utze, H.: Introduction to Information Retrieval.

Cambridge University Press, USA (2008)

33. Melis, G., Dyer, C., Blunsom, P.: On the state of the art of evaluation in neural
language models. In: International Conference on Learning Representations (2018),
https://openreview.net/forum?id=ByJHuTgA-

34. Meyerovich, L.A., Rabkin, A.S.: Socio-plt: Principles for programming language
adoption. In: International Symposium on New ideas, New Paradigms, and Re-
ﬂections on Programming and Software. pp. 39–54. ACM, New York, NY, USA
(2012)

35. Microsoft: TypeScript (2020), https://github.com/microsoft/TypeScript
36. Milner, R.: A theory of type polymorphism in programming. J. Comput. Syst. Sci.

17(3), 348–375 (1978)

37. Odersky, M., Sulzmann, M., Wehr, M.: Type inference with constrained
1999). https://doi.org/10.1002/(SICI)1096-

35–55

types. TAPOS 5,
9942(199901/03)5:1¡35::AID-TAPO4¿3.0.CO;2-4

(01

38. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch (2017),
https://pdfs.semanticscholar.org/b36a/5bb1707bb9c70025294b3a310138aae8327a.
pdf

39. Pierce, B.: Types and Programming Languages. MIT Press, London (2002)
40. Pottier, F., R´emy, D.: The essence of ml type inference. In: Pierce, B.C. (ed.)
Advanced Topics in Types and Programming Languages, pp. 389–489. MIT Press
(2005)

41. Pradel, M., Gousios, G., Liu, J., Chandra, S.: Typewriter: Neural type predic-
tion with search-based validation. ArXiv abs/1912.03768 (2019), to appear at
ESEC/FSE 2020

26

I. Vlassi Pandi et al.

42. Raychev, V., Vechev, M., Krause, A.: Predicting program properties from
of Pro-
”big
gramming Languages. pp. 111–124. ACM, New York, NY, USA (2015).
https://doi.org/10.1145/2676726.2677009

In: SIGPLAN-SIGACT Symposium on Principles

code”.

43. Robbins, H., Monro, S.: A stochastic approximation method. The Annals of Math-

ematical Statistics 22(3), 400–407 (1951), http://www.jstor.org/stable/2236626

44. Rockt¨aschel, T., Singh, S., Riedel, S.: Injecting logical background knowledge into
embeddings for relation extraction. In: Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies.
pp. 1119–1129. Association for Computational Linguistics, Denver, Colorado (May
2015). https://doi.org/10.3115/v1/N15-1118

45. Rondon, P.M., Kawaguci, M., Jhala, R.: Liquid types. In: Proceedings of the
29th ACM SIGPLAN Conference on Programming Language Design and Im-
plementation. pp. 159–169. PLDI ’08, ACM, New York, NY, USA (2008).
https://doi.org/10.1145/1375581.1375602

46. Siek, J.G., Taha, W.: Gradual typing for functional languages. scheme and func-
tional programming workshop (2006), http://www.schemeworkshop.org/2006/
13-siek.pdf

47. Sundermeyer, M., Schl¨uter, R., Ney, H.: LSTM neural networks for language mod-
eling. In: INTERSPEECH 2012, 13th Annual Conference of the International
Speech Communication Association, Portland, Oregon, USA, September 9-13,
2012. pp. 194–197. ISCA, Portland, Oregon, USA (2012), http://www.isca-speech.
org/archive/interspeech 2012/i12 0194.html

48. Sutton, C., McCallum, A., et al.: An introduction to conditional random ﬁelds.

Foundations and Trends® in Machine Learning 4(4), 267–373 (2012)

49. The-Mypy-Project: Mypy lang (2014), http://mypy-lang.org/, http://mypy-

lang.org/

50. Tieleman, T., Hinton, G.: RMSprop: Divide the gradient by a running average
of its recent magnitude (2014), http://www.cs.toronto.edu/∼tijmen/csc321/slides/
lecture slides lec6.pdf

51. Tirronen, V., Uusi-Makela, S., Isomottoen, V.: Understanding beginners’ mis-
e11 (2015).

takes with haskell. Journal of Functional Programming 25,
https://doi.org/10.1017/S0956796815000179

52. Vazou, N., Seidel, E.L., Jhala, R., Vytiniotis, D., Peyton-Jones, S.: Reﬁnement
types for haskell. In: Proceedings of the 19th ACM SIGPLAN International Con-
ference on Functional Programming. pp. 269–282. ICFP ’14, ACM, New York, NY,
USA (2014). https://doi.org/10.1145/2628136.2628161
53. Vechev, M., Yahav, E.: Programming with

“big
in Programming Languages 3(4),

code”.
231–284

Founda-
(2016).

and Trends

tions
https://doi.org/10.1561/2500000028

54. Vinyals, O., Fortunato, M., Jaitly, N.: Pointer networks. In: Advances in Neural
Information Processing Systems 28, pp. 2692–2700. Curran Associates, Inc. (2015),
http://papers.nips.cc/paper/5866-pointer-networks.pdf

55. Wei, J., Goyal, M., Durrett, G., Dillig, I.: LambdaNet: Probabilistic type inference
using graph neural networks. In: 8th International Conference on Learning Repre-
sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net
(2020), https://openreview.net/forum?id=Hkx6hANtwH

56. White, M., Vendome, C., Linares-Vasquez, M., Poshyvanyk, D.: Toward
deep learning software repositories. In: Working Conference on Mining Soft-
ware Repositories. pp. 334–345.
IEEE, Piscataway, NJ, USA (May 2015).
https://doi.org/10.1109/MSR.2015.38

Probabilistic Type Inference by Optimising Logical and Natural Constraints

27

57. Williams, J., Morris, J.G., Wadler, P., Zalewski, J.: Mixed Messages: Measuring
Conformance and Non-Interference in TypeScript (Artifact). Dagstuhl Artifacts
Series 3(2), 8:1–8:2 (2017). https://doi.org/10.4230/DARTS.3.2.8

58. Williams, J., Negreanu, C., Gordon, A., Sarkar, A.: Understanding and inferring
units in spreadsheets. In: IEEE Symposium on Visual Languages and Human-
Centric Computing (VL/HCC). IEEE (July 2020), https://www.microsoft.com/
en-us/research/publication/understanding-and-inferring-units-in-spreadsheets/
59. Xu, Z., Zhang, X., Chen, L., Pei, K., Xu, B.: Python probabilistic type inference
with natural language support. In: ACM SIGSOFT International Symposium on
Foundations of Software Engineering. pp. 607–618. ACM, New York, NY, USA
(2016). https://doi.org/10.1145/2950290.2950343

60. Yedidia, J.S., Freeman, W.T., Weiss, Y.: Understanding Belief Propagation and
Its Generalizations, p. 239–269. Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA (2003)

A Appendix: Continuous Relaxation in the Logit Space

In Section 2.2, we present the continuous interpretation based on probabilities.
As already mentioned, in the actual implementation we use logit instead for
numerical stability. The logit of a probability is the logarithm of the odds ratio.
It is deﬁned as the inverse of the softmax function; that is, an element of a
probability vector p ∈ [0, 1] corresponds to

π = log

p
1 − p

.

It allows us to map probability values from [0, 1] to [−∞, ∞].

Given the matrix L, which corresponds to the logit of the matrix P in Sec-
tion 2.2, we have that log([[E]]P ) = [[E]]L. we interpret an expression E as a
number [[E]]P ∈ R as follows:

[[xv is lτ ]]L = πv,τ

[[not E]]L = log(1 − exp([[E]]L)

[[E1 and E2]]L = [[E1]]L + [[E2]]L
[[E1 or E2]]L = log(cid:0)exp([[E1]]L) + exp([[E2]]L) − exp([[E1]]L + [[E2]]L)(cid:1).

The sigmoid function is deﬁned as

sigmoid(a) =

exp{a}
1 + exp{a}

,

while the LogSumExp function is deﬁned as

LogSumExp(x) = log

(cid:33)

exp{xi}

.

(cid:32)

(cid:88)

i

28

I. Vlassi Pandi et al.

B Appendix: Formal Proofs

B.1 Proofs for Logical Constraints

Lemma 2. For all E and Γ , [[E]]B(Γ ) ∈ {0, 1}.

Proof. By structural induction on the continuous semantics.

Lemma 3. For all E, E1, E2, and Γ :

1. [[E]]B(Γ ) = 0 ⇔ not([[E]]B(Γ ) = 1)
2. [[E1]]B(Γ ) = 1 and [[E2]]B(Γ ) = 1 ⇔ [[E1]]B(Γ ) · [[E2]]B(Γ ) = 1
3. [[E1]]B(Γ ) = 1 or [[E2]]B(Γ ) = 1 ⇔ [[E1]]B(Γ )+[[E2]]B(Γ )−[[E1]]B(Γ )·[[E2]]B(Γ ) =

1

Proof. These follow by cases analyses based on Lemma 2.

Lemma 4. For all E and Γ , either Γ |= E or Γ |= not E.

Proof. By structural induction on the satisfaction relation.

Restatement of Theorem 1. For all E and Γ : [[E]]B(Γ ) = 1 ⇔ Γ |= E.

Proof. We prove the property by structural induction; that is, we prove that
φ(N ) holds for all N , where φ(N ) is as follows.

φ(N ) (cid:44) ∀E, ∀Γ : size(E) = N ⇒ (Γ |= E ⇔ [[E]]B(Γ ) = 1).

We proceed by course-of-values induction on N . Consider any E, Γ and N =

size(E). We proceed by a case analysis at E.

Base Case For N = 1, the base case is E = (xv is lτ ). For any Γ we are to

show

Γ |= xv is lτ ⇔ [[xv is lτ ]]B(Γ ) = 1.

By deﬁnition, [[xv is lτ ]]B(Γ ) = pv,τ where pv,τ is the probability that variable
xv has type lτ according to the matrix B(Γ ). By deﬁnition of B(Γ ) and
because B results to a binary matrix, [[xv is lτ ]]B(Γ ) = 1 means that the
element pv,τ is equal to 1, that is Γ |= xv is lτ . Also, Γ |= xv is lτ , implies
that Γ (xv) = lτ . By deﬁnition, that means [[xv is lτ ]]B(Γ ) = 1.

Case E = not E(cid:48). We are to show Γ |= not E(cid:48) ⇔ [[not E(cid:48)]]B(Γ ) = 1. We have

that,

[[not E(cid:48)]]B(Γ ) = 1 ⇔
1 − [[E(cid:48)]]B(Γ ) = 1 ⇔
[[E(cid:48)]]B(Γ ) = 0 ⇔
not ([[E(cid:48)]]B(Γ ) = 1) ⇔
not Γ |= E(cid:48) ⇔
Γ |= not E(cid:48)

(Deﬁnition)

(Lemma 2)
(Induction Hypothesis)

(Deﬁnition).

Probabilistic Type Inference by Optimising Logical and Natural Constraints

29

Case E = (E1 and E2). For size(E1) < N and size(E2) < N , we are to show
that Γ |= (E1 and E2) ⇔ [[(E1 and E2)]]B(Γ ) = 1. Our induction hypothesis
is that φ(M ) holds for all M < N . We have that

Γ |= (E1 and E2) ⇔
Γ |= E1 and Γ |= E2 ⇔
[[E1]]B(Γ ) = 1 and [[E2]]B(Γ ) = 1 ⇔
[[E1]]B(Γ ) · [[E2]]B(Γ ) = 1 ⇔
[[(E1 and E2)]]B(Γ ) = 1

(Deﬁnition)

(Induction Hypothesis)

(Lemma 2)

(Deﬁnition)

which completes the proof for this case.

Case E = (E1 or E2). For size(E1) < N and size(E2) < N , we are to show
Γ |= (E1 and E2) ⇔ [[(E1 and E2)]]B(Γ ) = 1. Our induction hypothesis is
that φ(M ) holds for all M < N . We have that

Γ |= (E1 or E2) ⇔
Γ |= E1 or Γ |= E2 ⇔
[[E1]]B(Γ ) = 1 or [[E2]]B(Γ ) = 1 ⇔
([[E1]]B(Γ ) − 1) · (1 − [[E2]]B(Γ )) = 0 ⇔

(Deﬁnition)

(Induction Hypothesis)

[[E1]]B(Γ ) − [[E1]]B(Γ ) · [[E2]]B(Γ ) + [[E2]]B(Γ ) − 1 = 0 ⇔ (Case Analysis & Lemma 2)
[[E1 or E2]]B(Γ ) = 1.

which completes the proof for this case.

C Appendix: Neural Model

In this appendix we present the implementation details of the deep neural used
in Section 3.3.

LSTMClassifier(
(embedding): Embedding(90, 128)
(lstm): LSTM(128, 64)
(hidden2out): Linear(in_features=64,

out_features=100, bias=True)

(softmax): LogSoftmax()
(optimization fun): ADAM)

Listing 1.1: Our Character Level LSTM model.

