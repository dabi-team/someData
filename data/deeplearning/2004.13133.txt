Deep Reinforcement Learning Based Spectrum
Allocation in Integrated Access and Backhaul
Networks

Wanlu Lei, Student Member, IEEE, Yu Ye, Student Member, IEEE, Ming Xiao, Senior Member, IEEE,

1

0
2
0
2

r
p
A
7
2

]
T
I
.
s
c
[

1
v
3
3
1
3
1
.
4
0
0
2
:
v
i
X
r
a

Abstract—We develop a framework based on deep reinforce-
ment learning (DRL) to solve the spectrum allocation problem in
the emerging integrated access and backhaul (IAB) architecture
with large scale deployment and dynamic environment. The avail-
able spectrum is divided into several orthogonal sub-channels,
and the donor base station (DBS) and all IAB nodes have the
same spectrum resource for allocation, where a DBS utilizes those
sub-channels for access links of associated user equipment (UE)
as well as for backhaul links of associated IAB nodes, and an
IAB node can utilize all for its associated UEs. This is one of
key features in which 5G differs from traditional settings where
the backhaul networks were designed independently from the
access networks. With the goal of maximizing the sum log-rate
of all UE groups, we formulate the spectrum allocation problem
into a mix-integer and non-linear programming. However,
it
is intractable to ﬁnd an optimal solution especially when the
IAB network is large and time-varying. To tackle this problem,
we propose to use the latest DRL method by integrating an
actor-critic spectrum allocation (ACSA) scheme and deep neural
network (DNN)
to achieve real-time spectrum allocation in
different scenarios. The proposed methods are evaluated through
numerical simulations and show promising results compared with
some baseline allocation policies.

Index Terms—integrated access and backhaul, spectrum allo-

cation, deep reinforcement learning

I. INTRODUCTION

The global trafﬁc data rate is estimated to continuously
increase with an annual rate of 30% between 2018 to 2024
due to the exponential
increase in the number of mobile
broadband subscribers such as smartphones and tablets [1].
An effective solution to handle these relentless demands is to
increase the network capacity through network densiﬁcation
and frequency reuse. However, the acquisition cost of site
and ﬁber deployments is one of the biggest challenges in
implementing densiﬁcation at a large scale in the networks.
According to [2], the ﬁber optic link costs about 85% of
total
installation and trenching. To handle this challenge,
the investigation of self-backhauling, where same spectrum
resources can be shared by both the access and the backhaul
transmissions, has attracted much attention due to its ﬂexibility
and cost efﬁciency compared with ﬁber-based wired solutions.
Recently, the emergence of integrated access and backhaul
(IAB), which is also refereed to as self-backhauling networks
in the literature, has gained much attention, and it becomes

Wanlu Lei, Yu Ye and Ming Xiao are with the School of Electrical
Engineering and Computer Science, Royal Institute of Technology (KTH),
Stockholm, Sweden (email: wllei@kth.se, yu9@kth.se, mingx@kth.se).

Wanlu Lei is with Ericsson AB, Stockholm, Sweden.

one of the key scenarios in 5G and future millimeter wave
(mmWave) networks. Actually, 3GPP Release 15 also involved
a new study item (SI) to analyze the feasibility of IAB
deployment for New Radio (NR). Unlike traditional wireless
backhauling, IAB nodes can use the same radio resource for
access and backhaul links. Besides, the dense deployment of
IAB nodes together with the aggressive frequency reuse brings
about a direct consequence of challenges in providing reliable
backhaul transmission and mitigating severe co-tier and cross-
tier interference.

The emerging architecture of IAB has motivated lots of
recent research activities from both academia and industries.
References [3], [4] analyze a two-tier heterogeneous cellular
networks (HetNeT) with IAB where a donor base station
(DBS) provides mmWave backhaul to the IAB nodes, and
concluded that due to the the bottleneck of backhaul links
between DBS and IAB nodes, it is difﬁcult to improve user
rates in an IAB settings. In [5],
it evaluates the end-to-
end performance of IAB architecture through system-level
full-stack simulation and shows that IAB can be a viable
solution to effectively relay trafﬁc in very congested networks.
The investigation in ﬁnding optimal routing and scheduling
strategies has also been studied in [6]. As many prior works
consider IAB in mmWave scenarios due to the advantage of
large bandwidth offered by mmWave, none has considered the
impact of limited backhaul capacity together with spectrum
allocation problem in the sub-6 Ghz setup of IAB, to the best
of our knowledge.

IAB settings in sub-6 Ghz frequency actually plays a
signiﬁcant role in many practical scenarios in 5G and beyond
networks. Such as in public safety applications, where a net-
work extension is required by adding more IAB nodes to reach
higher coverage, or where a temporary IAB network needs
to be deployed collaboratively with existing DBS in certain
emergency situations. The spectrum allocation schemes in
these scenarios are too divergent from the conventional setup
and make it difﬁcult to apply any off-the-shelf methods. Some
tailored framework and algorithms for spectrum allocation
need to be investigated to adapt to the system dynamics and
fulﬁl real time requirements.

The spectrum allocation problem has been extensively stud-
ied in [7]–[10] and is usually solved by formulating as an op-
timization problem. Nevertheless, most of these methods need
accurate or complete information about the network, such as
channel state information (CSI) or are achieved with very ex-
pensive computational complexity. Besides, network dynamics

 
 
 
 
 
 
are seldom addressed and many solutions to the optimization
problem are solved in only a snapshot of the network or valid
in a speciﬁc network architecture. These model-dependent
schemes are indeed inappropriate for complex and highly time-
varying scenarios. In an established network environment,
base station (BS) employs static spectrum allocation strategy,
such as full-reuse or ﬁxed orthogonal allocation methods to
ease the system computation and implementation complexities.
However, ultra dense IAB environment makes full-spectrum
reuse or other static schemes less efﬁcient due to the severe
co-tier interference and cross-tier interference introduced by
neighboring BSs, as shown in Fig. 1. The rate of user equip-
ment (UE) associated with IAB is determined by the minimum
rate of backhaul link and access link, which makes the ﬁnal
rate sensitive to the spectrum allocation strategies. When more
IAB nodes are deployed and more spectrum resource becomes
available in the IAB network, the solution space for spectrum
allocation increases exponentially. To address this issue, we
learning (RL)
latest ﬁndings in reinforcement
will exploit
and deep neural network (DNN) to develop a scalable and
model-free framework to solve this problem. The framework
is expected to have capability to effectively adapt with IAB
network topology changes, large-scale sizes and different real
time system requirements. We ﬁrstly consider a centralized
approach in this work, and leave the distributed approach to
future work.

Many exciting news in Artiﬁcial Intelligence(AI) have just
happened in recent years [11]. Learning-based method in
playing Atari 2600 video games outperforms all previous
approaches and surpasses human expert, Alpha Go [12] beats
the best human players in the game of Go. Very soon the
extended algorithm AlphaGo Zero beats AlphaGo by 100 − 0
without supervised learning on human knowledge. The magic
behind these happenings is the well-known RL algorithm. The
development of Q-learning is a big breakout in the early days
of RL [13]. The classical Q-learning is demonstrated to per-
form well in small-size models but becomes less efﬁcient when
the network is scaling up. RL combined with the state-of-the-
art technique DNN addresses the problem and its capability of
handling large state spaces to provide a good approximation
of Q-value inspires remarkable upsurge of research works in
wireless communication problems. The deep Q-learning based
power allocation problem has been considered in [14] and [15].
In [16], it exploits DRL and proposes an intelligent modulation
and coding scheme (MCS) selection algorithm for the primary
transmission. A similar approach has also been proposed in [8]
for dynamic spectrum access in wireless network. However,
directly applying deep Q-network (DQN) to the spectrum
allocation problem is not feasible, because the action space can
be very large with increasing of the network size and available
spectrum, and it consumes much longer time for convergence.
Recent work from DeepMind has introduced an actor-critic
method that embraces DNN to guide decision making [17]
for continuous control. This model can be applied to large
discrete action space [18] to tasks having up to one million
actions.

Based on our observations, it can be concluded that deep
reinforcement learning (DRL) is a promising technique for

2

wireless communication systems in the future. On one hand,
the large amount of data from intelligent radio can be used
for training and predicting purpose, and hence improving
system performance by better decision making (spectrum
mapping and allocation for UE). On the other hand, it is
able to handle highly dynamic time-variant system with dif-
ferent network setups and UE demands. Moreover, another
appealing advantage in RL is the ﬂexibility in reward function
design. With proper designed reward at each step, the system
performance can be improved according to different desired
objective. The interaction between the agent (can be a center
located controller or distributed UE) and the environment are
beneﬁcial in the learning process such that the agent can learn
to select appropriate strategies to gain more rewards.

In what follows, we will study the spectrum allocation prob-
lem in the IAB architecture. To the best of our knowledge, we
may be the ﬁrst to investigate practical DRL-based spectrum
allocation framework in an IAB setup. Our main contributions
are as follows:

• We formulate the spectrum resource allocation problem
in an IAB setup into a non-convex mix integer program-
ming, the objective of which tries to maximize a generic
network utility function.

• We develop a novel model-free framework based on dou-
ble deep Q-network (DDQN) and actor-critic techniques
for dynamically allocating spectrum resource with system
requirements. The proposed architecture is simple for
implementation and does not require CSI related infor-
mation. The training process can be performed off-line
at a centralized controller, and the updating is required
only when signiﬁcant changes occur in IAB setup.

• We show that with proposed learning framework, the
improvement of the existing policy can be achieved with
guarantee, which yields a better sum log-rate perfor-
mance. We evaluate the learning speed of two algorithms
and show that actor-critic based learning applying policy
gradient for Q-value maximization is more effective in
convergence speed than value-based DDQN when action-
space is large.

• We show that the actor-critic based spectrum allocation
scheme outperforms in system sum log-rate compared
with the static allocation strategy when the network size
increases.

The remaining of this paper is organized as follows. We
ﬁrst formulate the spectrum allocation problem in IAB setup
in Section II. To solve the optimization problem, we propose
two DRL-based algorithms in Section III. Simulation results
are presented in Section IV to demonstrate the effectiveness
of the proposed approaches. We conclude the paper in Section
V.

II. SYSTEM MODEL

Consider a downlink (DL) transmission two-tier IAB net-
work, where DBS b0 is located at the center of the network
with IAB nodes deployed uniformly within the coverage area.
We denote the set of IAB nodes as B− = {bl | l = 1, 2, ..., L}.
Each IAB is equipped with two antennas: the receiving antenna

3

Without loss of generality, we assume that several consec-
utive sub-carriers are grouped into one to form a spectrum
sub-channel, and the channel fading is the same within one
sub-channel and independent across different sub-channels. At
a coherence time period, we denote the downlink channel gain
from transmitter i to receiver j at the m-th sub-channel as

gm
i, j

= αi, j hm
i, j,
(1)
where i ∈ B, j ∈ F1 ∪ U−. For example, if i = b0 and j ∈ B−,
gm
i, j represents the backhaul link channel gain between DBS
and IAB node. And hm
i, j is the frequency dependent small-scale
fading which undergoes Rayleigh fading, i.e., hm
i, j ∼ exp(1).
αi, j represents the large-scale fading coefﬁcient, and it is a
function of distance between i and j including the pathloss
and shadowing effect.

to j at

During the transmission session between i

the
m-th sub-channel, the interference to the target receiver is
introduced by other BSs downlink transmissions on the same
the
sub-channel. As DBS the transmitter in the ﬁrst-tier,
received signal-to-interference-and-noise ratio (SINR) of UE
u0 and IAB node bl over the m-th sub-channel is expressed
as
Pb0 gm
gm
i,u0

xm
u0
+σ2
u

Γm
b0,u0

(cid:213)

(2)

=

,

b0,u0
zm
i
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

i ∈B−
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
(cid:125)
(cid:124)
co-tier interference

and

Γm
b0,bl

=

(cid:213)

Pbl gm
zm
i

xm
ul
b0,bl
+ Pbl ξbl zm
bl
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)
(cid:123)(cid:122)
(cid:124)
self-interference

gm
i,bl
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

i ∈B−\bl
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124)
(cid:125)
(cid:123)(cid:122)
co-tier interference

,

+σ2
bl

(3)

where ξbl is the self-interference cancellation ability of IAB
node bl, and σ2
are noise power for the UE and IAB
node, respectively. The received SINR for the second-tier UE
groups is

u and σ2
bl

Γm
bl,ul

=

(cid:213)

Pbl gm
zm
i

bl,ul
+ gm

zm
l

(cid:213)

b0,ul
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

j ∈ F1
(cid:125)
(cid:123)(cid:122)
(cid:124)
cross-tier interference

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

gm
i,ul
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

i ∈B−\bl
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124)
(cid:125)
(cid:123)(cid:122)
co-tier interference

.

(4)

xm
j

+σ2
u

The instantaneous rate of u0 is determined by the spectrum

allocation matrix X and Z that is expressed as

Cu0(X, Z) =

M
(cid:213)

m=1

log(1 + Γm

b0,u0

(X, Z)).

(5)

Since the IAB node receives signal from DBS and transmits to
its associated UE group, the instantaneous rate of the second-
tier UE ul is decided by the minimum value of the backhaul
rate and the access rate, which is given by

Cul (X, Z) = min(Cb0,bl (X, Z), Cbl,ul (X, Z)),
where Cb0,bl represents the backhaul link rate from DBS to
IAB node bl, and Cbl,ul is the access link rate for correspond-
ing associated UE group of the IAB node bl.

(6)

Fig. 1.

Integrated Access and Backhaul With shared Spectrum Resource.

at the mobile termination (MT) side for the wireless backhaul
with DBS, and the transmitting antenna at digital unit (DU)
side for access to serve its associated UE groups. IAB nodes
are assumed to be full-duplex (FD) capable of certain self-
cancellation ability. FD is an important topic in the IAB setup.
Since our focus in the work is a general spectrum allocation
framework, the research for the self-cancellation scheme is
not in the scope of this work. Let B = {b0} ∪ B− denote the
set of all base stations (BS) including the DBS and all IAB
nodes, where |B| = 1 + L. The total bandwidth in which each
BS can operate is divided into M orthogonal sub-channels,
denoted by M = {1, 2, ..., M }. We assume that each BS has a
representative UE group associated to it where all UEs in this
group are collocated. For notation simplicity, we use similar
denotation for UEs as in BSs. The UE group associated with
DBS is denoted as u0, while the set of UE groups associated
with IAB nodes is denoted as U− = {ul |l = 1, 2, ..., L}, where
ul denotes the UE group that is associated with IAB node bl.
The complete UE group set is denoted as U = {u0} ∪ U−. As
often described in IAB networks, the IAB nodes are treated as
UEs to its associated parent, i.e., the DBS in our setup. Thus,
the ﬁrst-tier receivers set is denoted as F1 = {u0} ∪ B−, while
the second-tier receivers set is U−.

i , ..., z M

i ]T , where zm

Note that each BS has M spectrum resources for allocation,
and we assume that access and backhaul links share the same
pool of resource through M orthogonal sub-channels. The
spectrum resource of the IAB node is dedicated assigned to
its associated UE group. We denote the spectrum-allocation
vector at the i-th IAB as zi = [z1
i ∈ {0, 1},
m ∈ M, i ∈ B−. When the m-th sub-channel is used by the
= 0. Differently, the
= 1 otherwise zm
i-th IAB node, zm
i
i
spectrum resource at DBS can be utilized either by its asso-
ciated UE group for access transmission or the corresponding
associated IAB node for backhaul transmission. We denote
the spectrum-allocation vector at DBS for its f -th receiver
as x f = [x1
f , ..., x M
f ∈ {1, 0} and f ∈ F1. For
= 1, the DBS utilizes the m-th sub-channel for
example, if xm
u0
access link of its associated UE u0, and for backhaul link to the
= 1, i ∈ B−. Thus, for notation simplicity,
i-th IAB node if xm
i
the allocation mappings is denoted as X = [x1, ..., x1+L]T and
Z = [z1, ..., zL]T for the ﬁrst-tier receivers and the second-
tier receivers, respectively. We assume that DBS utilizes all
its spectrum resource in the required coverage area.

f ]T , where xm

First tier linkSelf interferenceInterferenceFD-IAB node0bX1z3z2z0u1u2u3uMBSUEaccess linkbackhaulSecond tier link1b2b3bBased on the description above, we are interested in max-
imizing a generic network utility function f (x) as expressed
below

(P1) : max
X,Z

(cid:213)

j ∈U

f (Cj(X, Z)),

s.t. Cj ≥ Ωj, ∀ j ∈ U;
xm
f

= 1, m ∈ M;

(cid:213)

f ∈ F1
(cid:213)

f ∈ F1
(cid:213)

(cid:213)

xm
f ≤ M;

m∈M
i ≤ M, ∀i ∈ B−;
zm

(7a)

(7b)

(7c)

(7d)

(7e)

m∈M
f , zm
xm

i ∈ {0, 1}, ∀ f ∈ F1, ∀i ∈ B−, ∀m ∈ M,

(7f)

where (7b) is Quality of Service (QoS) requirement of each
UE group in the system, (7c) and (7d) are constraints for
allocation vector of DBS, such that each sub-channel can
be only allocated to either access or backhaul transmission.
(7e) and (7f) are constraints for the IAB node allocation
vector that only M sub-channels are available to employ. The
spectrum allocation problem (P1) is non-convex mix integer
programming, which has been shown to be NP-hard [19].
The solution of (P1) becomes intractable especially when the
network size and available sub-channels increase.

The function f (x) can be designed to obtain a certain
network utility. Since we assume that each BS always has
a UE group associated to it. Since each UE group requires to
reach a certain rate level for transmission purpose at all time
step, we consider the objective which aims at maximizing the
global system rate. This is known as proportional fairness [20],
and it yields f (x) = log(x).

To this end, the spectrum resource allocation problem of
the IAB network is to design a spectrum allocation scheme for
backhaul and access link transmission using the same resource
pool. Spectrum resources expressed through binary variables
(∀ f ∈ F1, ∀i ∈ B−, ∀m ∈ M) are designed to
f and zm
xm
i
maximize the network utility function as described in (P1).

III. REINFORCEMENT LEARNING IN RESOURCE
ALLOCATION

A. Background of Reinforcement Learning

The essential idea behind RL is to learn good policy through
trail-and-error interaction with a dynamic environment [21],
[22]. RL dates back to the early days of work in psychol-
ogy, neuroscience and computer science. It receives massive
attention recently in the machine learning communities. It has
been widely used in decision making problems, mainly for the
reason that it does not require prior knowledge of the system
model and can be effectively adapted to stochastic transitions
in the system. The RL algorithm is initially designed to ﬁnd the
optimal policy with a fully observable Markovian environment.
RL methods can also be applied in various settings such as
non-Markovian environment and and is shown to have good
performance.

4

Let S denote a set of possible states in the IAB network en-
vironment, and A denote a discrete set of actions. The problem
can be modeled as an Markov Decision Process (MDP). The
transition probability distribution is the set P : S×A×S → R,
and the reward function is r : S × A → R. Assuming discrete
time steps, at each coherence time t, the central controller
of the IAB network observes a state st ∈ S, and takes an
action at ∈ A according to a certain policy π : S → A. The
policy π(a|s) is the probability of taking action a conditioned
on the current state s. Therefore, the policy function must
satisfy (cid:205)
a ∈A π(a|s) = 1. The controller, as the agent, receives
an immediate reward r and the environment evolves to next
state s(cid:48) with probability P(s(cid:48)|s, a). Here let Rt be the future
cumulative discounted reward at time t expressed as

Rt =

∞
(cid:213)

τ=0

γτrt+τ+1,

(8)

where γ ∈ [0, 1] is a discounted factor for future reward. The
agent is only concerned with an immediate reward if γ is
set to 0, which is considered as myopic. As γ approaches 1,
the agent takes future reward into account more strongly. The
objective of RL agent is to ﬁnd a policy π that maximizes
the expected accumulative discounted reward given the state-
action pair (s, a) at time t. This is also deﬁned as Q-function
in the classical Q-learning algorithm

Qπ(s, a) = Eπ [Rt |st = s, at = a] .

(9)

A commonly used off-line algorithm to ﬁnd the optimal
stochastic policy function π∗(a|s) takes greedy search, which
is expressed as

π∗(a|s) =

(cid:40) 1, if a = arg max
a ∈A

Qπ(s, a);

0, o.w.

(10)

The policy function is written in π(s) for deterministic tran-
sitions. Further analysis on policy improvement is discussed
in Sec. III-D. It has been proved in [23] that Q-learning can
effectively converge to the optimal policy with some iterative
algorithms to achieve the optimal Q-value as

Q∗(s, a) = r(s, a) + γ

(cid:213)

s ∈S

P(s(cid:48)|s, a) max
a(cid:48)

Q∗(s(cid:48), a(cid:48)).

(11)

The classical Q-learning constructs a lookup table for all
state-action pairs and updates the table at each step to approach
optimal Q-value. The updating of Q-value for each state-action
pair usually applies the (cid:15)-greedy policy. With probability (cid:15)
the agent picks a random action, and with probability 1 − (cid:15)
the agent takes the action that gives the maximum Q-value
at the current state. This is also referred as exploration and
exploitation in RL. Normally, at the beginning of the iteration,
(cid:15) is set to 1 to enable the agent to have more chances to explore
the environment with different new actions, and then it decays
with the iteration episodes. After obtaining new experience
through interacting with the environment by taking action a

at state s, the agent transits to next state s(cid:48) and updates the
corresponding new Q-value

Algorithm 1: Double DQN training algorithm for Spec-
trum Allocation

5

Q(s, a) ←Q(s, a)+

β (cid:0)r(s, a) + γmax
a(cid:48)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

Q(s(cid:48), a(cid:48)) − Q(s, a)(cid:1)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:123)(cid:122)
temporal difference error

(cid:125)

(cid:124)

,

(12)

where β is the learning rate. And r(s, a) + γmaxa(cid:48) Q(s(cid:48), a(cid:48))
is the learned value for each step, the former of which is
the immediate reward obtained from the environment after
taking the action a, and the latter is the estimated discounted
maximum Q-value after transiting to the state s(cid:48) by taking
action a(cid:48). The updating rule is called Temporal Difference
(TD) method since its changes are based on the estimated
error between two different times.

The above value-based Q-learning method performs well
when dealing with small state and action space. However,
it becomes computationally infeasible when the state space
and action space are large, such as in the spectrum allocation
problem at hand. This is due to that the storage of the lookup
table in (12) for all state-action pairs is intolerable for large-
scale problems. Besides, many states are rarely visited as the
state space increases, which consequently leads to much longer
time to converge to the optimal Q-value. To solve these two
problems, deep Q-learning is proposed to improve the classical
Q-learning method by applying the DNN to approximate Q-
function in lieu of the lookup table, which is called DQN.

B. DQN and Spectrum Allocation

In DQN, the Q-function in (9) is approximated by a DNN

with a weight parameter θ as

˜Q(s, a; θ) ≈ Q(s, a).

(13)

The essence of DQN is to determine the weights θ for Q-
function. Similar to Q-learning, at each step, the agent collects
experience through interacting with the IAB network and
stores this experience as the form (si, ai, ri, s(cid:48)
i) in a data set
D. Then DQN updates its θ to minimize the loss by sampling
a minibatch of size N from data set D as

θ = arg min

θ

L(θ) = arg min

θ

N
(cid:213)

i

(cid:0)yi − ˜Q(si, ai; θ)(cid:1) 2 ,

(14)

where

yi = ri + γ max

a(cid:48)
i

˜Q(s(cid:48)

i, a(cid:48)

i; θ).

(15)

As DQN aims to greatly improve and stabilize the training
procedure of Q-learning, it applies two innovative mecha-
nisms: experience replay and periodically update target. By
applying experience replay mechanism, the DQN randomly
samples previous transitions in the data set D to alleviate
the problem of correlated data and non-stationary distributions
[24]. However, the operator maxa(cid:48) Q(s(cid:48), a(cid:48); θ) in (15) uses the
same values as that in evaluating an action, this may lead to
overoptimistic value estimates. Therefore, as the "quasi-static
target network" method implies in [25], DDQN can solve this
issue by introducing two DQNs: the target DQN with weights
θ− and the train DQN with weights θ in order to remove

1: Start environment simulator for IAB network;
2: Initialize Memory data set D;
3: Initialize the target and the train DQN θ− = θ;
4: for episode = 1, 2, ... do
5:
6:

Receive the initial observed state s1;
for t = 1 : T do

7:

8:

9:
10:

11:
12:

13:
14:
15:

Select an action using (cid:15)-greedy method from the
train network ˜Q(s, a; θ);
Obtains a reward according to Eq. (17), revolves to
new state st+1;
Store transition pair (st, at, rt, st+1) in data set D;
if training is TRUE then

for i = 1 : N do

Sample a random mini-batch of transitions
(si, ai, ri, si+1) from data set D;
Set {yi } according to (15);

end for
Update train network according to (14);
Update target network weight θ− = θ;

16:
17:
18:
19: end for

end if
end for

upward bias. Parameters θ− are basically clones from the train
network but only updated periodically according to the settings
in hyper-parameter. The difference of loss functions between
(14) and DDQN is that the train Q-network is used to select
actions while the target Q-network is used to evaluate actions.
The target Q-value is calculated as

yDDQN = r(s, a) + γ ˜Q (cid:0)s(cid:48), arg max
a(cid:48) ∈A
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

˜Q(s(cid:48), a(cid:48); θ)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:124)

(cid:123)(cid:122)
select action a(cid:48)

(cid:125)

; θ−(cid:1).

(16)

Finally, the stochastic gradient descent, which minimizes the
loss in (14) by applying the target value in (16) is used for
training over the mini-batch data set D at a central controller.

At each coherence time, the agent builds its state using
collected information from its associated UE and IAB nodes.
To better control the transmission overhead, we restrict the
state information to the quality of Service (QoS) status of all
UEs. The center unit ﬁrst computes the rate level for each UE
according to (5) and (6) at t-th step, and then obtains the state
denoted as st = {st,u0, st,u1, ..., st,u1+L }, where st, j ∈ {0, 1}.
And st, j = 1 indicates that the demand of the UE group
( j ∈ U) has been satisﬁed with Ct, j ≥ Ωt, j at time t, and
st, j = 0 otherwise. In the simulation, we observe that the agent
can learn accurately by using these QoS information without
extra CSI feedback from IAB nodes or UEs. With the goal
of optimizing the spectrum allocation scheme to maximize
the sum log-rate, we deﬁne the action as the corresponding
allocation matrix for DBS and IAB nodes as at = [Xt, Zt ]T .
The action decision for channel allocation at IAB nodes is to
evaluate the contribution of speciﬁc channels to the system
objective. For example, if the condition of channel m is weak

at node f ∈ B−, and contributes more to interfering neighbors
than to its associated UE signal strength, then this channel is
= 0. The
very likely to be set to off at next state, i.e., xm
f
reward function is designed to optimize the network objective
in (P1) for proportional fairness allocation. For all 0 ≤ t ≤ T,
we deﬁne the reward as

rt = (cid:213)

f (cid:0)Ct, j(Xt, Zt )(cid:1) .

j ∈U

(17)

The DQN is trained at the central controller in an off-line
manner. The training procedure is stated in Algorithm 1. Both
target network and train network are initialized in the same
manner. We use a memory replay buffer for storing transition
samples and then fetch a mini-batch of transition samples from
D to train the deep Q-learning networks. Though there are
many investigations regarding the sampling method to improve
the efﬁciency and accuracy of the training process [26], [27],
we apply a random sampling method to take into account all
possible experiences while interacting with the IAB network
environment. Through step 14 in Algorithm 1, the weights θ−
for the target network get periodically updated.

Although value-based DQN learning has been demonstrated
very powerful in many application areas, there are several
limitations which have been addressed in [17], [28]. For
instance, as action space becomes large, the evaluation over
|A| for action selection becomes intractable. As the spectrum
allocation problem stated in (P1), the action space of the DBS
is (1 + L)M , and each IAB has 2M actions. It is obvious that
the total action space increases exponentially with the number
of IAB nodes and available spectrum. The required training
episodes also increase signiﬁcantly. Hence to improve learning
efﬁciency, we propose a actor-critic learning method that can
be more effectively adapting to the size of IAB networks.

C. Actor-Critic Spectrum Allocation Method

In this section we describe an Actor-Critic Spectrum Alloca-
tion (ACSA)-based framework for solving spectrum allocation
problem (P1). Unlike the value-based DQN, the ACSA model
is premised on having two interacting modulus. As shown in
Fig. 2, it has two aptly named components: an actor and a
critic. The role of the actor is to take current environment state
and output an action upon certain policy, which is essentially
as normal DQN does. The critic is responsible for evaluating

Fig. 2. Actor-Critic Spectrum allocation architecture

6

Algorithm 2: Actor-Critic Spectrum Allocation Algo-
rithm(ACSA)

Observed the initial state s1;
for t = 1 : T do

1: Start environment simulator for IAB network;
2: Initialize Memory data set D;
3: Initialize target critic and actor network, θ− = θ, ω− = ω;
4: for episode = 1, 2, ... do
5:
6:
7:
8:

Select action at from actor network;
Execute action at to the spectrum allocation matrix
X and Z and obtains reward rt ;
Store transition pair (st, at, rt, st+1) in data set D;
if training is TRUE then

9:
10:
11:
12:
13:

14:
15:
16:
17:

18:

19:
20:

for i = 1 : N do

Sample mini-batch from data set D;
Set yi = ri + γ ˜Q(s(cid:48)
i, ˜π(s(cid:48)
Compute TD error δi = yi − ˜Q(si, ai; θ)

i; ω−); θ−);

end for
Update θ for critic network according to (14);
Compute sampled policy gradient

∇ω J ≈

1
N

(cid:213)

i

∇a ˜Q(s, a; θ)|s=si,a= ˜π(si ;ω)

× ∇ω ˜π(s; ω)|s=si ;

Update ω for actor network by

ω =ω + β∇ω J;

end if
Update the weights of the target networks

θ− =τθ + (1 − τ)θ−;
ω− =τω + (1 − τ)ω−;

end for

21:
22: end for

how good the action is for the state through returning a score
from DQN by taking the action and current state as inputs.
The critic is updated using TD errors similar as Q-network.
The parameters of actor are updated by the returning score
to improve its performance towards actions that bring higher
score in the future. In most actor-critic models, the policy
function is modeled as a probability distribution over actions
A given the current state and thus it is stochastic. This model
has been widely used given support by the policy gradient
theorem [28], [29]. However, the appealing simple form for
deterministic policy gradient has been demonstrated to be
more efﬁcient than the usual stochastic policy gradient in [30].
Therefore, we model the policy of the spectrum allocation
problem as a deterministic decision. Following DDQN, the
Q-function in (9) is approximated via DQN with a weight
vector θ, while the actor network uses a similar approach
which approximates the policy function with the parameter
ω as ˜π(s; ω). The goal of the actor network is to determine
parameter ω in policy function ˜π(s; ω), which may result

ActionIAB EnvironmentCritic (Q-value): updateActor (Action): updateTD-errorStateReward(;)ttasts,(,;)|ttassaaQsatrin the largest increase of Q-value. According to [30], the
actor network can be updated by applying the chain rule to
the expected cumulative reward J with respect to the actor
parameters ω,

∇ω J ≈ E (cid:2)∇ω ˜Q(s, a; θ)|s=st,a= ˜π(st,ω)

(cid:3)

= E (cid:2)∇a ˜Q(s, a; θ)|s=st,a= ˜π(st ;ω)∇ω ˜π(s; ω)|s=st

(cid:3) .

(18)

We formally present

the proposed actor-critic spectrum
allocation method in Algorithm 2. Recall that DDQN stabilizes
the learning of Q-function by experience replay and the frozen
target network, i.e., the parameters of evaluating Q-network
are copied to the target Q-network periodically. We similarly
employ target networks for critic and actor with parameters
θ− and ω− respectively. The target networks are clones of
the train actor and critic networks, which are initialized in
the same way as the train networks but updated with rate τ
(step 18). In iteration t, the agent applies (cid:15)-greedy exploration
method in the actor network to obtain an action. Then the
state-action pair is sent into critic train network for updating
θ. After receiving the gradient with respect to action output
by the critic, the train network for actor updates ω.

D. Improvement of baseline policy

In this subsection, we will investigate the improvement of
the policy determined by ACSA, which may consequently
achieve a better sum log-rate compared with baseline spectrum
allocation strategy. Consider an IAB network that adopts
a baseline policy ¯π(s) which generates the state sequence
{st, st+1, ...sT } from time t. We deﬁne the expected return
under this policy ¯π starting from st = s as a value function

V(s, ¯π) = E [Rt |st = s] .

(19)

To show whether the policy π†(s; ω) obtained by ACSA is
better than ¯π, we consider a special case where an action
a† = π†(s; ω) (cid:44) ¯π(s) is selected at state s. And then the agent
follows the baseline policy. We denote Qπ†(s, π†(s; ω)) as the
value obtained with this behavior. If ACSA is a better strategy,
it is natural to obtain Qπ†(s, π†(s; ω)) ≥ V(s, ¯π).

Proposition 1. If the policy π†(s; ω) of ACSA is no worse than
an existing policy ¯π, it must obtain V(s, ¯π) ≤ V(s, π†).

Proof. According to (19) we derive

V(s, ¯π) = E

(cid:34) ∞
(cid:213)

τ=0

(cid:34)

(cid:35)

γτr(s, ¯π(s))|st = s

= E

r(s, ¯π(s) +

γτr(st+1, ¯π(st+1)|st = s

(cid:35)

∞
(cid:213)

τ=0

= E [r(s, ¯π(s))|st = s] + γE [Rt+1|st+1 = s(cid:48)]
= E [r(s, ¯π(s))|st = s] + γV(s(cid:48), ¯π).

(20)

7

Letting a† = π†(s; ω) be the action taken at state st , we have

V(s, ¯π) ≤Q ¯π (cid:16)

s, π†(s; ω)

(cid:17)

=E (cid:2)Rt |st = s, at = π†(s; ω)(cid:3)
=E ¯π
≤E ¯π
=E ¯π[r(s, π†(s; ω)) + γE[r(st+1, π†)

(cid:2)r(s, π†(s; ω))) + γV(st+1, ¯π(st+1))|st = s(cid:3)
(cid:2)r(s, π†(s; ω)) + γQ ¯π(st+1, π†(st+1))|st = s(cid:3)

+ V(st+2, ¯π)|st+1, at+1 = π†(s; ω)]|st = s]

=E ¯π[r(s, π†(s; ω)) + γr(st+1, π†)

+ γ2V(st+2, ¯π)|st = s]

≤E ¯π[r(s, π†(s; ω)) + γr(st+1, π†) + γ2r(st+2, π†)

+ γ3V(st+3, ¯π)|st = s]
· · ·

≤E ¯π[r(s, π†(s; ω)) + γr(st+1, π†) + γ2r(st+2, π†)

+ γ3r(st+3, π†) + · · · |st = s]

=V(s, π†),

which completes the proof.

(21)

(cid:3)

The proof can be extended to all states and possible actions
for comparing policy of ACSA with the baseline policy.
Since V(s, π†) represents the expected sum log-rate of IAB
networks, a better rate performance can be guaranteed by
ACSA compared with the baseline policy ¯π if Proposition 1
is satisﬁed.

IV. SIMULATION RESULTS

In this section, we present simulation results to demonstrate
the performance of proposed methods for proportional fairness
spectrum allocation in different IAB scenarios.

A. Simulation Setup

We consider an IAB network consisting of one DBS at
the center, and L IAB nodes which follow the Poisson point
process (PPP) deployed at the radius of 250-meter from DBS.
UE groups are located at the radius of 150-meter from its
associated BS, the initial locations of which follow PPP. We
adopt random walk model for simulating mobility of UEs
in the IAB networks. The moving speed of UE uj at time
t follows uniform distribution, i.e., νj ∼ U(0, 2) m/s, while
the moving angel follows ψj ∼ U(0, 2π). The distance-based
path loss is characterized by the line-of-sight (LoS) model
for urban environments at 2.4 Ghz, and is compliant with the
LTE standard [31]. The simulator initializes an IAB network
setup according to the parameters shown in Table. I. We
consider the proportional fairness schedule which aims to
maximize (cid:205)
j ∈U log(Cj). The required rate is unique among
all UEs, where Ωj = 5, ∀ j ∈ U.

We next describe the hyper-parameters used in our DDQN
and ACSA methods. To better compare two algorithms, we
apply the same network structure in DDQN and in critic
of ACSA, which composes of three-hidden layers of fully-
connected neural networks with 500, 1000, 500 neurons. This

Parameter
Carrier Frequency
Bandwidth W
DBS Pathloss
IAB Pathloss
Subchannel
Transmit power at DBS
Transmit power at IAB
Self-interference
Spectral Noise

Value
2.4 Ghz
20 Mhz
34 + 40 log(d)
37 + 30 log(d)
Rayleigh Fading
43 dBm
33 dBm
−70dB
−174dBm/Hz + 10 log(W) + 10dB

TABLE I
SIMULATOR SETUP

structure is also applied to the actor network in ACSA. We
found that employing different number of neurons at each
hidden layer renders similar rate performance for the IAB
network with small size and few sub-channels. However, the
stability signiﬁcantly degrades as the action space increases
when few neurons are adopted. In the ACSA, the third-hidden
layer is set as a customize activation function to fulﬁll the
constraint condition in problem (P1). The softmax function
is applied on each column of allocation matrix X for DBS
since the constraint of the sum probability for allocating one
typical channel equals to one. Correspondingly, this is to select
the best "candidate" for each channel to get a higher score in
critic networks. The sigmoid function is applied on each row
of Z for the IAB node to select the best "combination" of
sub-channels for allocation. The discount rate is γ = 0.9 and
the exploration rate is (cid:15) = 0.9 with a decay rate of 0.9995.
The learning rate is set to be the same for both algorithms as
β = 10−4. The soft update rate in ACSA model is τ = 0.01.
The input of the proposed models is the observed state
from the environment where st . The output of DDQN is a
single action for spectrum allocation. This action decision is
converted to the mapping matrix [X, Z]T by a post-process.
The output of the ACSA is the processed action, which can
be directly applied to the allocation vectors. Thus, the output
action a is with size (2L + 1) × M, where the ﬁrst (L + 1) × M
is the spectrum allocation X for DBS, and the next L × M is
the allocation Z for all IAB nodes.

B. Learning to Improve Fariness Schedule

We present a typical result with L = 4 IAB nodes, which are
deployed around DBS. The available sub-channels for resource
allocation at each BS are M = 10. As illustrated in Fig. 3, we
compare the sum log-rate for the proposed DDQN and ACSA
algorithms with the full-spectrum reuse strategy. The results
show that both proposed methods can effectively learn from
interacting with the time-varying environment and achieve
better log sum-rate performance than the full-spectrum reuse
strategy. It shows that reusing full-spectrum can not guarantee
the optimal log sum-rate for IAB networks due to the severe
interference introduced by turning on all sub-channels at IAB
nodes and MBS. This interference not only affects co-tier
IAB nodes during backhaul transmission, but also affects the
UE at DBS. In addition, the convergence speed of ACSA
exceeds that of DDQN. This shows that the ACSA is more
suitable for solving the problem (P1) with IAB networks. For

8

the proposed methods, it can be seen that their performance
keeps low in the beginning of learning phase. This is because
the agent is exploring the environment with trial-and-error
experiments. After around 120 episodes for the ACSA and
200 episodes for the DDQN, the performance of both methods
intend to increase and become relatively stable. However,
due to the mobility of UEs, the sum log-rate of all methods
ﬂuctuates. Unlike the static full-reuse strategy,
the DRL-
based methods can be trained to dynamically allocate limited
spectrum resource according to the current link conditions in
the IAB network, which hence achieve a better sum log-rate
performance [32].

Fig. 4 illustrates one instance of the average rate of individ-
ual UE using ACSA method. The log sum-rate is relatively low
in the beginning since only two UEs achieve rate requirements.
This is possible since at some speciﬁc time periods,
the
UEs that locate close to neighbouring BSs have worse link
condition than other UEs. Then the agent learns to adapt the
allocation strategy to improve the log-sum rate. It can be seen
that the rates of UE1, UE2 and UE3 increase rapidly after
100 steps. After around 300 steps, all UEs have achieved
their system requirement. However, with increasing the rates
the rate of UE2 is slightly decreased.
of UE1 and UE3,
This is due to that the system has reached an equilibrium
allocation solution, where no UE can improve their individual
rate without decreasing the rate of at least one other UE.

To better understand the learning process of DRL methods,
we show how allocation decisions are made in consecutive 4
time steps in one test case, which is presented in Fig. 5. It
can be seen that the 6-th sub-channel is allocated for UE0
associated with MBS at the ﬁrst 3 time steps. However, at
the last time-step, the 3-rd sub-channel is allocated to UE0
instead of sub-channel 6. For all the 4 steps, UE0 experiences
cross-tier interference from other two transmitting IAB nodes.
As numerical result shown at this test case, as UE0 moves
dynamically in the environment, the channel gain between
UE0 and interfering IAB nodes gi,u0, i ∈ B− varies. From

Fig. 3. Average sum log-rate for L = 4, M = 10

0100200300400500Episode0.02.55.07.510.012.515.017.520.0Average Sum Log-rateACSADDQNFull-spectrum reuse9

Fig. 4. Average rate of each UE applying ACSA

Fig. 5. Spectrum allocation in 4 snapshots

step 3 to step 4, the interference introduces by IAB node1
and node3 decreases while by which IAB node2 and node4
increases. To maximize the expected cumulative sum log-rate
in the system, the optimal resource mapping at step4 is to
allocate sub-channel 3 to UE0 and allocate sub-channel 6 to
IAB node1 in order to guarantee the access link rate for UE0
as well as backhaul link rate for IAB node1.

j ∈U f ﬁxed(Cj))/(cid:205)

Besides, we evaluate the performance gain of ACSA to
a ﬁxed policy by simulating the average sum log-rates over
500 episodes with different number of IAB nodes. The per-
formance gain is deﬁned as Gain = ((cid:205)
j ∈U f ACSA(Cj) −
(cid:205)
j ∈U f ﬁxed(Cj). We choose 4 representative
scenarios with L = 2, 4, 8, 10. The results in Fig. 6 show that
as we increase the number of IAB nodes, the proposed DRL
method can outperform the ﬁxed allocation strategy. Since
with increasing IAB nodes, the performance degradation of
the static strategy dominates that of the proposed method,

Fig. 6. Average performance gain using ACSA with different number of IAB
nodes

the performance gain increases with L. As more spectrum
the proposed method shows
is available in the network,
to perform better in interference mitigation than the static
allocation method.

V. CONCLUSIONS

We investigate the spectrum allocation in IAB networks
by characterizing the optimization problem to maximize the
sum log-rate while guaranteeing UEs system requirements.
Speciﬁcally, we propose two DRL-based algorithms to solve
the optimization problem at hand. We show that both algo-
rithms can learn to adapt allocation strategy for backhaul and
access transmission with dynamic network settings. More-
over, the proposed ACSA is more efﬁcient in convergence
speed compared with DDQN, and it achieves considerable
performance gain in system sum log-rate comparing to static
strategy especially when the size of IAB networks gets larger.
Simulation results are given to show promising potentials
and better sum log-rate performance of proposed methods
compared with the traditional full-spectrum reuse strategy.

REFERENCES

[1] O. M. Teyeb, A. Muhammad, G. Mildh, E. Dahlman, F. Barac,
and B. Makki, “Integrated access backhauled networks,” CoRR, vol.
abs/1906.09298, 2019. [Online]. Available: http://arxiv.org/abs/1906.
09298

[2] H. A. Willebrand and B. S. Ghuman, “Fiber optics without ﬁber,” IEEE

spectrum, vol. 38, no. 8, pp. 40–45, 2001.

[3] C. Saha, M. Afshang, and H. S. Dhillon, “Bandwidth partitioning and
downlink analysis in millimeter wave integrated access and backhaul for
5g,” IEEE Transactions on Wireless Communications, vol. 17, no. 12,
pp. 8195–8210, 2018.

[4] ——, “Integrated mmwave access and backhaul in 5g: Bandwidth parti-
tioning and downlink analysis,” in 2018 IEEE International Conference
on Communications (ICC).

IEEE, 2018, pp. 1–6.

[5] M. Polese, M. Giordani, A. Roy, S. Goyal, D. Castor, and M. Zorzi,
“End-to-end simulation of integrated access and backhaul at mmwaves,”
in 2018 IEEE 23rd International Workshop on Computer Aided Mod-
eling and Design of Communication Links and Networks (CAMAD).
IEEE, 2018, pp. 1–7.

0100200300400500Steps0510152025UE_Rate(Mbps)ue0ue1ue2ue3ue4Sub-channela) Step1b) Step2c) Step3d) Step41234567891012345678910Sub-channel1234567891012345678910Sub-channelSub-channelIAB2_BH-IAB3_BH-IAB4_BH-UE1-UE2-UE3-UE0-IAB1_BH-UE4-IAB2_BH-IAB4_BH-IAB4_BH-UE1-UE2-UE3-UE0-IAB1_BH-UE4-IAB2_BH-IAB3_BH-IAB4_BH-UE1-UE2-UE3-UE0-IAB1_BH-UE4-IAB2_BH-IAB3_BH-IAB4_BH-UE1-UE2-UE3-UE0-IAB1_BH-UE4-10

[30] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,

“Deterministic policy gradient algorithms,” 2014.

[31] E. U. T. R. Access, “Radio frequency (rf) system scenarios,” Release,

vol. 8, p. 56, 2011.

[32] J. Munk, J. Kober, and R. Babuška, “Learning state representation for
deep actor-critic control,” in 2016 IEEE 55th Conference on Decision
and Control (CDC).
IEEE, 2016, pp. 4667–4673.

[6] M. Polese, M. Giordani, A. Roy, D. Castor, and M. Zorzi, “Dis-
tributed path selection strategies for integrated access and backhaul at
mmwaves,” in 2018 IEEE Global Communications Conference (GLOBE-
COM).

IEEE, 2018, pp. 1–7.

[7] V. Chandrasekhar and J. G. Andrews, “Spectrum allocation in tiered
cellular networks,” IEEE Transactions on Communications, vol. 57,
no. 10, pp. 3059–3068, 2009.

[8] B. Zhuang, D. Guo, E. Wei, and M. L. Honig, “Large-scale spectrum
allocation for cellular networks via sparse optimization,” IEEE Trans-
actions on Signal Processing, vol. 66, no. 20, pp. 5470–5483, 2018.
[9] Z. Zhou, X. Chen, Y. Zhang, and S. Mumtaz, “Blockchain-empowered
secure spectrum sharing for 5g heterogeneous networks,” IEEE Network,
vol. 34, no. 1, pp. 24–31, 2020.

[10] Q. Yang, T. Jiang, N. C. Beaulieu, J. Wang, C. Jiang, S. Mumtaz, and
Z. Zhou, “Heterogeneous semi-blind interference alignment in ﬁnite-snr
networks with fairness consideration,” IEEE Transactions on Wireless
Communications, 2020.

[11] L. Zhang, Y.-C. Liang, and D. Niyato, “6g visions: Mobile ultra-
broadband, super internet-of-things, and artiﬁcial intelligence,” China
Communications, vol. 16, no. 8, pp. 1–14, 2019.

[12] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural networks
and tree search,” nature, vol. 529, no. 7587, p. 484, 2016.

[13] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no.

3-4, pp. 279–292, 1992.

[14] E. Ghadimi, F. D. Calabrese, G. Peters, and P. Soldati, “A reinforcement
learning approach to power control and rate adaptation in cellular
networks,” in 2017 IEEE International Conference on Communications
(ICC).

IEEE, 2017, pp. 1–7.

[15] Y. Zhang, C. Kang, T. Ma, Y. Teng, and D. Guo, “Power allocation in
multi-cell networks using deep reinforcement learning,” in 2018 IEEE
88th Vehicular Technology Conference (VTC-Fall).
IEEE, 2018, pp.
1–6.

[16] L. Zhang, J. Tan, Y.-C. Liang, G. Feng, and D. Niyato, “Deep re-
inforcement learning-based modulation and coding scheme selection
in cognitive heterogeneous networks,” IEEE Transactions on Wireless
Communications, vol. 18, no. 6, pp. 3281–3294, 2019.

[17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[18] G. Dulac-Arnold, R. Evans, H. van Hasselt, P. Sunehag, T. Lillicrap,
J. Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin, “Deep re-
inforcement learning in large discrete action spaces,” arXiv preprint
arXiv:1512.07679, 2015.

[19] Z.-Q. Luo and S. Zhang, “Dynamic spectrum management: Complexity
and duality,” IEEE journal of selected topics in signal processing, vol. 2,
no. 1, pp. 57–73, 2008.

[20] R. Srikant and L. Ying, Communication networks: an optimization,
Cambridge University

control, and stochastic networks perspective.
Press, 2013.

[21] R. S. Sutton, A. G. Barto et al., Introduction to reinforcement learning.

MIT press Cambridge, 1998, vol. 2, no. 4.

[22] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement
learning: A survey,” Journal of artiﬁcial intelligence research, vol. 4,
pp. 237–285, 1996.

[23] J. N. Tsitsiklis, “Asynchronous

stochastic approximation and q-

learning,” Machine learning, vol. 16, no. 3, pp. 185–202, 1994.

[24] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves,

I. Antonoglou,
D. Wierstra, and M. A. Riedmiller, “Playing atari with deep
learning,” CoRR, vol. abs/1312.5602, 2013. [Online].
reinforcement
Available: http://arxiv.org/abs/1312.5602

[25] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” CoRR, vol. abs/1509.06461, 2015. [Online].
Available: http://arxiv.org/abs/1509.06461

[26] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience

replay,” arXiv preprint arXiv:1511.05952, 2015.

[27] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and
N. De Freitas, “Dueling network architectures for deep reinforcement
learning,” arXiv preprint arXiv:1511.06581, 2015.

[28] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy
gradient methods for reinforcement learning with function approxima-
tion,” in Advances in neural information processing systems, 2000, pp.
1057–1063.

[29] T. Degris, P. M. Pilarski, and R. S. Sutton, “Model-free reinforcement
learning with continuous action in practice,” in 2012 American Control
Conference (ACC).
IEEE, 2012, pp. 2177–2182.

