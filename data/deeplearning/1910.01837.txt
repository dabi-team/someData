9
1
0
2

t
c
O
4

]

G
L
.
s
c
[

1
v
7
3
8
1
0
.
0
1
9
1
:
v
i
X
r
a

Enriching Visual with Verbal Explanations for
Relational Concepts – Combining LIME with
Aleph

Johannes Rabold, Hannah Deininger, Michael Siebers, and Ute Schmid

Cognitive Systems, University of Bamberg, Germany

Abstract. With the increasing number of deep learning applications,
there is a growing demand for explanations. Visual explanations provide
information about which parts of an image are relevant for a classiﬁer’s
decision. However, highlighting of image parts (e.g., an eye) cannot cap-
ture the relevance of a speciﬁc feature value for a class (e.g., that the
eye is wide open). Furthermore, highlighting cannot convey whether the
classiﬁcation depends on the mere presence of parts or on a speciﬁc spa-
tial relation between them. Consequently, we present an approach that
is capable of explaining a classiﬁer’s decision in terms of logic rules ob-
tained by the Inductive Logic Programming system Aleph. The examples
and the background knowledge needed for Aleph are based on the ex-
planation generation method LIME. We demonstrate our approach with
images of a blocksworld domain. First, we show that our approach is ca-
pable of identifying a single relation as important explanatory construct.
Afterwards, we present the more complex relational concept of towers.
Finally, we show how the generated relational rules can be explicitly
related with the input image, resulting in richer explanations.

Keywords: XAI · Deep Learning · Inductive Logic Programming.

1

Introduction

Explainable Artiﬁcial Intelligence (XAI) mostly refers to visual highlighting of
information which is relevant for the classiﬁcation decision of a given instance
[9,19]. In general, the mode of an explanation can be visual, but also verbal or
example-based [13]. Visual explanations have been introduced to make black-
box classiﬁers such as (deep) neural networks more transparent [9,19,18]. In
the context of white-box machine learning approaches, such as decision trees
or Inductive Logic Programming (ILP) [16], it is argued that these models are
already transparent and interpretable by humans [16]. In the context of ILP it
has been shown that a local verbal explanation can easily be generated from
symbolic rules with a template-based approach [20].

For image classiﬁcation tasks, it is rather obvious that visual explanations
are helpful for technical as well as for domain experts: Information about what
pixels or patches of pixels most strongly contribute to a class decision can help
to detect model errors which might have been caused by non-representative

 
 
 
 
 
 
2

J. Rabold et al.

sampling. Highlighting can also support domain experts to assess the validity of
a learned model [18]. In general, an explanation can be characterized as useful, if
it meets the principles of cooperative conversations [13]. These pragmatic aspects
of communication are described in the Gricean maximes [8] which encompass the
following four categories: (1) quality – explanations should be based on truth
or empirical evidence; (2) quantity – be as informative as required; (3) relation
– explanations should communicate only relevant information; (4) manner –
avoidance of obscurity and ambiguity. We argue that visual explanations can in
general not avoid obscurity and ambiguity since they cannot or only partially
capture the following kinds of information:

– Feature values: Visual highlighting can explain that a speciﬁc aspect of an entity
is informative for a speciﬁc class – e.g., that an emotion is expressed near the eye.
However, the relevant information is whether the eye is wide open or the lid is
tightened [21].

– Negation: While approaches like LRP [19] allow to visualize which pixels have
a negative contribution to the classiﬁcation, it is not generally possible to inform
that the absence of a feature or object is relevant. E.g., it might be relevant to
explain that a person is not classiﬁed as a terrorist because he or she does not hold
a weapon (but a ﬂower).

– Relations: If two parts of an image are highlighted, it is not possible to discrim-
inate whether the conjunction (e.g., there is a green block and a blue block) or a
more speciﬁc relation (e.g., the green block is on the blue block) is relevant.

ILP approaches [14] can capture all three kinds of information because the
models are expressed as ﬁrst-order Horn clauses. Relational concepts such as
grandparent(X,Y) [15] or mutagenicity of chemical structures [23] can be in-
duced. Furthermore, classes involving relations, such as the Michalski Train Do-
main [15], can be learned. Here, the decision whether a train is east- or west-
bound depends on relational information of arbitrary complexity, e.g., that a
waggon with six wheels needs to be followed by a waggon with an open top.

Recently, there have been proposed several deep learning approaches to tackle
relational concepts, such as the diﬀerentiable neural computer [7], RelNNs [10],
or RelNet [2]. In contrast to ILP, these approaches depend on very large sets of
training examples and the resulting models are black-box. A helpful explanation
interface should be able to take into account visual/image-based domains as well
as abstract/graph-based domains. The model agnostic approach of LIME [18]
provides linear explanations based on sets of super-pixels or words. This is not
suﬃcient when more expressive relational explanations are necessary. Current
focus of our work is to provide relational explanations for black-box, end-to-end
classiﬁers for image-based domains. We believe that for image-based domains, a
combination of visual and verbal explanations is most informative with respect to
the Gricean maximes. Psychological experiments also give evidence that humans
strongly proﬁt from a combination of visual and verbal explanations [12].

In a previous study [17], we could show that relational symbolic explana-
tions (Prolog rules) can be generated by combining the ILP approach Aleph
[22] with LIME [18]. However, simple visual concepts have been pre-deﬁned and
used as input to Aleph and not extracted automatically. In the following, we

Enriching Visual with Verbal Explanations for Relational Concepts

3

(a) A house, because three windows left
of each other.

(b) A tower, because three windows on
top of each other.

Fig. 1. Combining visual and symbolic explanations for house in contrast to tower.
Photo of house by Pixasquare, photo of lighthouse by Joshua Hibbert, both on Unsplash

present an extension of [17] covering end-to-end image classiﬁcation with a con-
volutional neural network (CNN) [11], partitioning images into sub-structures,
as well as automatic extraction of visual attributes and spatial relations. Local
symbolic explanations are learned with Aleph, providing logical descriptions of
original and perturbed images. Finally, local symbolic explanations are related
to visual highlighting of informative parts of the image to provide a combined
visual-symbolic explanation. The symbolic explanation can be transformed in a
verbal one with a template-based approach as demonstrated in [20]. An illus-
trative example is given in Figure 1. Here the concept of house is explained by
the fact that three windows are next to each other. This information is given by
identifying three relevant parts of the image, naming them (A, B, C), labeling
them as windows (which might be done by another automatic image classiﬁca-
tion or by the user) and stating the spatial relation between the objects using
the left of relation. This example also demonstrates an important aspect of
symbolic explanations: Which attributes and relations are useful to explain why
some object belongs to some class depends on the contrasting class [5].

In the next section, we introduce the core concepts for our approach. Then
we present a signiﬁcantly extended version of the LIME-Aleph algorithm [17].
We demonstrate the approach on images of a blocksworld domain. In a ﬁrst
experiment we show that LIME-Aleph is capable of identifying a single relation
(left of(block1, block2)) as relevant for the learned concept. In a second ex-
periment, we demonstrate that more complex relational concepts such as tower
can be explained. Finally we show how the fusion of visual and symbolic expla-
nations might be realized.

4

J. Rabold et al.

2 Explaining Relational Concepts with LIME-Aleph

2.1 Core Concepts

The ILP System Aleph. To ﬁnd symbolic explanations for relational concepts
we use Aleph [22]. Aleph infers a logic theory T given a set of positive (E+)
and negative (E−) examples. An example is represented by the target predicate
(e.g. stack(e1). or not stack(e2).) together with additional predicates (e.g.
contains(b1, e1).) as background knowledge (BK). Predicates in BK are used
to build the preconditions for the target rules. Aleph is based on speciﬁc-to-
general reﬁnement search. It ﬁnds rules covering as many positive examples
as possible, avoiding covering negative ones. Search is guided by modes which
impose a language bias. The general algorithm is [22]:

1. As long as positive exist, select one. Otherwise halt.
2. Construct the most-speciﬁc clause that entails the selected example and is within

the language constraints

3. Find a more-general clause which is a subset of the current literals in the clause.
4. Remove covered by the current clause.
5. Repeat from step 1.

An example of a rule from T in Prolog is stack(Stack) :- contains(Block1,

Stack), contains(Block2, Stack), on(Block2, Block1).
denoting that a stack is deﬁned by one block on top of another.

LIME’s Identiﬁcation of Informative Super-Pixels. LIME (Local Interpre-
table Model-Agnostic Explanations) is an approach to explain the decision result
of any learned model [18]. Explanations state the parts of an instance that are
positively or negatively correlated to the class. It works by creating a simpler,
local surrogate model around the instance to be explained. In case of an image,
the explanation is a set of connected pixel patches called super-pixels.

Let x be an image and x(cid:48) be the binary vector that states whether super-
pixels x(cid:48)
i ∈ x(cid:48) are switched on or oﬀ (see below). LIME ﬁnds a sparse linear model
g(x(cid:48)) that locally approximates the unknown decision function f (x) represented
by a black-box classiﬁer. It eﬀectively ﬁnds the coeﬃcients w for the super-
pixel representations being variables in a simpliﬁed linear model. This is done
by generating a pool of perturbed examples z(cid:48) by taking the original super-pixel
representation x(cid:48) and randomly selecting elements in a uniformly distributed
fashion. That way, images z are obtained with some super-pixels still original
and some altered according to a transform function h eﬀectively removing the
information they contained (Switching them oﬀ). Each sample z(cid:48) (The binary
vector indicating if super-pixels are switched oﬀ in this sample) is stored in a
sample pool Z along with the classiﬁer result f (z) and a distance measure πx(z)
that expresses the distance of the perturbed example z to the original image
x. For images this can be the Mean Squared Error. The distance is needed for
the linear model to be locally faithful to the original function f (x) and thus has
to be minimized. The “un-faithfulness” of the model g to the black-box model
f with respect to the distance measure πx(z) is expressed with the following
formula [18]:

Enriching Visual with Verbal Explanations for Relational Concepts

5

(a) All positions in
the image, where a
block has to be lo-
cated in order to be
left, right, top, or
bottom of block a.

for

the

(b)
rela-
tions on and under,
with
illustrated
on(c,d),
on(b,a),
resp.
under(a,b)
and under(d,c).

Fig. 2. Diagrams to show the diﬀerent concepts of the relations used.

L(f, g, πx) = (cid:80)

z,z(cid:48)∈Z πx(z)(f (z) − g(z(cid:48)))2.

The goal is to ﬁnd the coeﬃcients w for g that minimize this un-faithfulness L.
The coeﬃcients ultimately translate back to weights for the super-pixels. LIME
uses K-Lasso to ﬁnd the weights [3].

The original LIME uses the algorithm Quick Shift [25] to ﬁnd super-pixel. It
imposes an irregular pixel mask over the input image that segments it in terms
of pixel similarity. The segmentation is performed in a 5D space consisting of the
image space and the color space. Quick Shift is only one of several segmentation
algorithms that are available for LIME. They all share the attribute of impos-
ing an irregular mask over an image. In many domains, this irregularity is not
wanted. For the domain used in this paper it is preferable to use a segmentation
algorithm that divides an image into a regular grid with square cells.

2.2 Extraction of Image Parts and Their Relations

Based on image segmentation into a grid of super-pixels i with domain-speciﬁc
cell size, a set of attributes Ai for cells and spatial relations between cells can be
automatically extracted. Attributes Ai are taken from a pool of attributes A. An
example for an attribute in A is the mean color of i in the RGB color space. To
ﬁnd a human-comprehensible name, the nearest color according to the Euclidean
distance in a pool of commonly known color names is assigned. Other extractable
attributes are the size or the general location in the image. The coordinates of
the center point of i are stored for spatial reasoning. Extracted attributes are
converted into predicates for BK. The attribute that a given super-pixel SP is
blue is represented as has color(SP, blue).

Spatial relations can be deﬁned between pairs of super-pixels. To restrict the
number of pairs, we need a pre-selection S of super-pixels that might be relevant
for the concept. LIME’s w describe the magnitude of relevance for either the

6

J. Rabold et al.

S ← LIM E(f, x, k)
(cid:46) Selection of k most important super-pixels.
A ← extract attribute values(S, A) (cid:46) Find all attribute values Ai for all i ∈ S.
(cid:46) Find all relations r : S × S between all i ∈ S.

Algorithm 1 Explanation Generation with LIME-Aleph.
1: Require: Instance x ∈ X
2: Require: Classiﬁer f , Selection size k, Threshold θ
3: Require: Attribute pool A, Relation pool R
4:
5:
6: R ← extract relations(S, R)
7:
8:
9:
10:
11:
12:
13:

E+ ← {(cid:104)A, R(cid:105)}
E− ← {}
for each r(i, j) ∈ R do

z ← ﬂip in image(x, i, j)
(cid:46) Flip the super-pixels in the image space.
r(cid:48) ← r(j, i)
(cid:46) Obtain new predicate for the BK by ﬂipping parameters.
R(cid:48) ← R \ {r} ∪ {r(cid:48)}
(cid:46) All relations in the BK; also the altered one.
R(cid:48) ← calculate side ef f ects(R(cid:48), r(cid:48)) (cid:46) Re-calculate relations that are aﬀected

by the ﬂipped relation.

c(cid:48) ← f (z)
if c(cid:48) ≥ θ do

(cid:46) Obtain new estimator for the perturbed image.
(cid:46) If estimator reaches threshold, add new positive example.

14:
15:
16:
17:
18:
19:
20:
21: return T

else

E+ ← E+ ∪ {(cid:104)A, R(cid:48)(cid:105)}

E− ← E− ∪ {(cid:104)A, R(cid:48)(cid:105)}

end for
T ← Aleph(E+, E−)

(cid:46) Else, add negative example.

(cid:46) Obtain theory T with Aleph.

true classiﬁcation (positive weight) or the counter-class (negative weight). By
introducing a user-deﬁned constant k, we restrict how many super-pixels the
selection S should contain, taking the k super-pixels with the highest values in
w. Spatial relations r : S × S are drawn from a pre-deﬁned pool R. For this
work, we use the relations left of, right of, top of, bottom of as well as
relations that represent an immediate adjacency in the regular grid mentioned
earlier, namely on and under. Relations are deﬁned with respect to the center
coordinates of the super-pixels in S. Figure 2 sketches the underlying semantics
of these relations. It is possible to include additional relations as long as they are
automatically extractable and their inverses are deﬁned in the image space. In
domains with super-pixels that diﬀer in size, a larger relation between super-
pixels could be deﬁned. Also, a not equal relation can be considered.

2.3 Learning Rules for Relational Concepts via Aleph

To generate symbolic relational explanations for visual domains, we combine
LIME’s super-pixel weighting with Aleph’s theory generation. The input into
LIME-Aleph is an image x and a model f returning class probability estima-
tions for x. Currently our approach is only applicable for explaining one class in
contrast to all other classes, eﬀectively re-framing the original classiﬁcation as a
concept learning problem. The output of LIME-Aleph is a theory T of logic rules
describing the relations between the super-pixels that lead to the class decision.

Enriching Visual with Verbal Explanations for Relational Concepts

7

LIME’s explanation relies on that linear surrogate model which contains the
set of super-pixels with the highest positive weights for the true class. When
dealing with the question which relations contribute most to the classiﬁcation,
identifying the most informative super-pixels has to be replaced by identifying
the most informative pairs of super-pixels. Instead of turning super-pixels on
and oﬀ, LIME-Aleph inverts extracted relations between super-pixels and ob-
serves the eﬀects on the classiﬁcation. Algorithm 1 shows our approach. Given
the selection S of super-pixels together with the extracted attributes, our ap-
proach ﬁrst ﬁnds all relations R ⊆ R that hold between them. For every relation
r(i, j) ∈ R, a new perturbed example z from the image space is created by ﬂip-
ping the super-pixels i and j in the image space. To generate a new example
for Aleph, the resulting perturbed image is ﬁrst put through the classiﬁer f . If
the estimator f (z) exceeds a threshold θ for the class we want to explain, a new
positive example is declared. Otherwise, the example is declared negative. All
relations holding for the perturbed image are written in the BK characterizing
this example. The initial positive example for Aleph is always generated for the
unaltered constellation.

3 Experiments and Results

We investigate the applicability of LIME-Aleph in a blocksworld domain consist-
ing of diﬀerently colored squares that can be placed in a regular-grid world. For
a ﬁrst investigation, we decided to focus on artiﬁcially generated images rather
that real world domains.

3.1 An Artiﬁcial Dataset for Relational Concepts

We implemented a generator to create a huge variety of positive and negative
example images for diﬀerent blocksworld concepts. All generated images are
of size 32 × 32 pixels consisting of a single-colored red background, containing
constellations of colored squares of dimension 4×4 pixels. The squares are single-
colored (excluding red) with color-channel values either being set to 0.0 or 0.8.
The squares are placed into the image according to an 8×8 uniform grid. Positive
examples are generated by ﬁrst randomly placing a reference square. Then, the
other squares are placed randomly following the relation conventions shown in
Figure 2. For the experiments we restricted A = {color} with attribute values
in {cyan, green, blue} and R = {left of, right of, top of, bottom of,
on, under}.

3.2 Training a Black-Box Model

To obtain a black-box model for image classiﬁcation, we used a small convolu-
tional neural network [11] which we trained from scratch with commonly known
best-practice hyper-parameters. The network consists of two convolution layers
with kernel size 2 × 2 and ReLU activations. Each layer learns 16 ﬁlters to be

8

J. Rabold et al.

able to robustly recognize the colored squares. After ﬂattening the output, the
convolution layers are followed by 2 fully connected layers each with a ReLU ac-
tivation. The ﬁrst layer consists of 256 neurons, the second one of 128 neurons.
A small amount of dropout is applied past each layer to cope with potential
overﬁtting [24]. The network does not contain a pooling layer. That way, fewer
location information is lost in aggregation during the learning process which we
believe is crucial for preserving spatial relationships (see [6] p. 331). For the ex-
periments, we generated perfectly balanced datasets with 7.000 training-, 2000
validation- and 1000 test-images for both the concept and the counter-examples.
We trained the networks for a maximum of 10 epochs with early stopping if the
validation loss did not decrease after 5 epochs.

3.3 Experiment 1: Single Relation Concept

The concept for the ﬁrst experiment can be described by the single relation that
a green square is left of a blue square in an image x. Figure 3 shows two posi-
tive examples (a, b) and one negative example (c). After the full 10 epochs, the
accuracy on the validation set reached 93.47%. For image 3.a the classiﬁer gave
the estimator for the concept to be 89.83%. For image 3.b the estimator was
94.18%. The estimator output for belonging to the concept for 3.c was 0.28%
showing that the network is able to discriminate the positive and negative exam-
ples. To generate explanations for these three images, each image is separately
fed into LIME. The number of kept super-pixels k is set to 3. We choose this
value for k because we were aware that there are 2 squares in the image that
are distinguishable from the background. One additional super-pixel was taken
to generate a richer pool for selection S containing also some background. In
general, for many domains it is not that easy to estimate good values for k.
So in most of the cases it is preferable to over-estimate the value to not lose
information for the explanation.

Finally, a symbolic explanation is generated with LIME-Aleph. We describe
the procedure for the positive example 3.a. First, Algorithm 1 extracts the col-
ors of the selected super-pixels and the relations between them. Then, all the
relations get ﬂipped one after the other to produce the example set and BK for
Aleph. The original example 3.a is used as the seed for a set of perturbed versions
of the image. Threshold θ indicates whether the perturbed example is classiﬁed

(a)

(b)

(c)

Fig. 3. Positive (a, b) and negative (c) for the ﬁrst experiment.

Enriching Visual with Verbal Explanations for Relational Concepts

9

(a)

(b)

(c)

Fig. 4. Positive (a) and negative (b, c) for the tower experiment.

positive or negative. Based on the ﬁnal validation accuracy of the trained f and
from the estimator for the original image 3.a, it was set to θ = 0.8. For example
3.a 3 positive and 4 negative examples were created. From these 7 examples,
Aleph induced a theory T consisting of a single rule with accuracy of 100%:

concept(A) :- contains(B,A), has color(B,green), contains(C,A),
has color(C,blue), left of(B,C).

The learned rule accurately resembles the construction regulation of the wanted
concept; a green square has to be left of a blue square in an example A. Also,
this explanation matches the input image.

For image 3.b we used the same hyper-parameters (k = 3, θ = 0.8). Again,
Aleph came up with an accuracy of 100% and a rule structurally diﬀerent, but
conveying the same concept as the ﬁrst rule:

concept(A) :- contains(B,A), has color(B, blue), contains(C,A),
has color(C, green), left of(C,B).

3.4 Experiment 2: Tower Concept

In the second experiment, we investigated a speciﬁc concept of towers. Positive
examples consist of three diﬀerently colored blocks with a given restriction on
their stacking order. An example belongs to the concept tower, if a blue square
is present as a foundation. Directly on the foundation (one grid cell above) there
has to be a square of either cyan or green color. Directly on that square has
to be the remaining square (green or cyan). Figure 4 gives a positive and two
negative examples.

We again trained the CNN for 10 epochs. The ﬁnal validation accuracy was
98.70%. The original estimator for example 4.a gave f (a) = 94.88%. We ﬁrst set
k = 3 being the smallest selection of which we know can contain a tower. Again
setting θ = 0.8, LIME-Aleph came up with 5 positive and 6 negative examples
(accuracy 81.82%) and the following rule:

concept(A) :- contains(B,A), has color(B,cyan), contains(C,A),
on(B,C).

10

J. Rabold et al.

Fig. 5. An example for the combination of visual and verbal explanations. Here it is
explained, why and where this particular image shows evidence for belonging to the
concept tower.

This rule expresses the fact, that the cyan square can not be the foundation.
When setting the selection size k = 4, we let an additional background super-
pixel be part of S. The resulting rule is:

concept(A) :- contains(B,A), has color(B,cyan), contains(C,A),
has color(C,blue), top of(B,C).

This rule captures the fact, that a cyan block has to be above a blue one. The
generated explanations are only partial representations of the intended concept.
The symbolic explanations capture relevant aspects, but are too general.

4 Bringing Together Visual and Symbolic Explanations

The generated rules give explanations in symbolic form which can be re-written
into verbal statements. We postulate that helpful explanations for images should
relate highlighting of relevant parts of the image with explicit symbolic infor-
mation of attributes and relations. In this section we give an example on how
this fusion might look like. Let us take the tower example from Section 3.4. In
Figure 5, the output of standard LIME is given with the 3 most important super-
pixels matching the expected region in the image. Additionally, the relation from
the instantiated rule from the experiment for k = 3 is given. Since cyan is the
only square that is mentioned in the rule, we take it as a reference. The rela-
tion on links the cyan square to another unknown square below. This relation
is shown explicitly in the image by connecting the two squares and writing the
instantiated relation.

Enriching Visual with Verbal Explanations for Relational Concepts

11

5 Conclusion and Further Work

We proposed an approach to extract symbolic rules from images which can be
used to explain classiﬁer decisions in a more expressive way than visual high-
lighting alone. For a simple artiﬁcial domain we gave a proof of concept for
our approach LIME-Aleph. The work presented here signiﬁcantly extends [17]
by providing a method of automated extraction of visual attributes and spa-
tial relations from images. As a next step we want to also let the explanative
power be evaluated by humans. Also we plan to cover real world image datasets
like explaining diﬀerences between towers and houses as shown in Figure 1. The
challenge here is to come up with arbitrarily placeable segmentations that are
easily interchangeable. While our algorithm relies on a regular grid, in an image
“in the wild”, the semantic borders of sub-objects can be irregular in shape and
not easily be ﬂipped in order to test for diﬀerent relations. One idea to cope
with this problems is to use relevance information from inner layers in a CNN
(e.g., with LRP, [19]) to ﬁrst pinpoint small important regions and sub-objects,
then super-imposing a standardized selection shape (square, circle, etc.) over the
pixel values to ﬁnd interchangeable super-pixels for ﬁlling selection S.

In general, it might be useful to consider a variety of explanation formats to
accommodate speciﬁc personal preferences and situational contexts. For exam-
ple, visual highlighting is a quick way to communicate what is important while
verbal explanations convey more details. Likewise, examples which prototypically
represent a class and near-miss counter-examples could be used to make system
decisions more transparent [1]. Explanations might also not be a one-way street.
In many domains, it is an illusion that the labeling of the training is really a
ground truth. For example, in medical diagnosis, there are many cases where not
even experts agree. Therefore, for many practical applications, learning should be
interactive [4]. To constrain model adaption, the user could mark-up that parts
of an explanation which are irrelevant or wrong. Such a cooperative approach
might improve the joint performance of the human-machine-partnership.

References

1. Adadi, A., Berrada, M.: Peeking inside the black-box: A survey on explainable

artiﬁcial intelligence (XAI). IEEE Access 6, 52138–52160 (2018)

2. Bansal, T., Neelakantan, A., McCallum, A.: Relnet: End-to-end modeling of enti-
ties & relations. In: NIPS Workshop on Automated Knowledge Base Construction
(AKBC) (2017)

3. Efron, B., Hastie, T., Johnstone, I., Tibshirani, R.: Least angle regression. The

Annals of Statistics 32(2), 407–499 (2004)

4. Fails, J.A., Olsen Jr, D.R.: Interactive machine learning. In: Proceedings of the 8th
International Conference on Intelligent User Interfaces. pp. 39–45. ACM (2003)
5. Gentner, D., Markman, A.B.: Structural alignment in comparison: No diﬀerence

without similarity. Psychological Science 5(3), 152–158 (1994)

6. Goodfellow, I., Bengio, Y., Courville, A.: Deep learning. MIT press (2016)

12

J. Rabold et al.

7. Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-
Barwi´nska, A., Colmenarejo, S.G., Grefenstette, E., Ramalho, T., Agapiou, J.,
et al.: Hybrid computing using a neural network with dynamic external memory.
Nature 538, 471–476 (2016)

8. Grice, H.P.: Logic and conversation. In: Cole, P., Morgan, J. (eds.) Syntax &

Semantics, vol. 3, pp. 41–58. Academic Press (1975)

9. Gunning, D.: Explainable artiﬁcial intelligence (XAI) (2017), https://www.darpa.

mil/attachments/XAIProgramUpdate.pdf

10. Kazemi, S.M., Poole, D.: RelNN: A deep neural model for relational learning. In:
McIlraith, S.A., Weinberger, K.Q. (eds.) Proceedings of the Thirty-Second AAAI
Conference on Artiﬁcial Intelligence, (AAAI-18, New Orleans, Louisiana, USA,
Feb. 2-7). pp. 6367–6375. AAAI Press (2018)

11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: 26th Annual Conference on Neural Information
Processing Systems (NIPS, Lake Tahoe, NV, December 3-6). pp. 1106–1114 (2012)
12. Mayer, R.E., Sims, V.K.: For whom is a picture worth a thousand words? Ex-
tensions of a dual-coding theory of multimedia learning. Journal of Educational
Psychology 86(3), 389–401 (1994)

13. Miller, T.: Explanation in artiﬁcial intelligence: Insights from the social sciences.

Artiﬁcial Intelligence 267, 1–38 (2019)

14. Muggleton, S., De Raedt, L.: Inductive logic programming: Theory and methods.
Journal of Logic Programming, Special Issue on 10 Years of Logic Programming
19-20, 629–679 (1994)

15. Muggleton, S.H., Lin, D., Tamaddoni-Nezhad, A.: Meta-interpretive learning
of higher-order dyadic datalog: Predicate invention revisited. Machine Learning
100(1), 49–73 (2015)

16. Muggleton, S.H., Schmid, U., Zeller, C., Tamaddoni-Nezhad, A., Besold, T.: Ultra-
strong machine learning: comprehensibility of programs learned with ILP. Machine
Learning 107(7), 1119–1140 (2018)

17. Rabold, J., Siebers, M., Schmid, U.: Explaining black-box classiﬁers with ILP -
empowering LIME with aleph to approximate non-linear decisions with relational
rules. In: Riguzzi, F., Bellodi, E., Zese, R. (eds.) 28th International Conference,
(ILP, Ferrara, Italy, September 2-4). LNCS, vol. 11105, pp. 105–117. Springer
(2018)

18. Ribeiro, M.T., Singh, S., Guestrin, C.: Why should I trust you?: Explaining the
predictions of any classiﬁer. In: Proceedings of the 22nd ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining. pp. 1135–1144. ACM
(2016)

19. Samek, W., Wiegand, T., M¨uller, K.R.: Explainable artiﬁcial intelligence: Under-
standing, visualizing and interpreting deep learning models. ITU Journal: ICT
Discoveries - Special Issue 1 - The Impact of Artiﬁcial Intelligence (AI) on Com-
munication Networks and Services 1(1), 39–48 (2018)

20. Siebers, M., Schmid, U.: Please delete that! why should I? – Explaining learned

irrelevance classiﬁcations of digital objects. KI 33(1), 35–44 (2019)

21. Siebers, M., Schmid, U., Seuß, D., Kunz, M., Lautenbacher, S.: Characterizing
facial expressions by grammars of action unit sequences–a ﬁrst investigation using
abl. Information Sciences 329, 866–875 (2016)

22. Srinivasan, A.: The Aleph Manual. http://www.cs.ox.ac.uk/

activities/machinelearning/Aleph/ (2004)

Enriching Visual with Verbal Explanations for Relational Concepts

13

23. Srinivasan, A., Muggleton, S.H., Sternberg, M.J., King, R.D.: Theories for muta-
genicity: A study in ﬁrst-order and feature-based induction. Artiﬁcial Intelligence
85(1-2), 277–299 (1996)

24. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overﬁtting. The Journal
of Machine Learning Research 15(1), 1929–1958 (2014)

25. Vedaldi, A., Soatto, S.: Quick shift and kernel methods for mode seeking. In: Eu-

ropean Conference on Computer Vision. pp. 705–718. Springer (2008)

