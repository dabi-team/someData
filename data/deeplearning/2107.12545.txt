Double Deep Q-learning Based Real-Time
Optimization Strategy for Microgrids

Hang Shuai, Member,

IEEE, Xiaomeng Ai, Member, IEEE, Jiakun Fang, Senior Member, IEEE, Wei Yao, Senior

Member, IEEE, Jinyu Wen, Member, IEEE

i

1
2
0
2

l
u
J

7
2

]

Y
S
.
s
s
e
e
[

1
v
5
4
5
2
1
.
7
0
1
2
:
v
i
X
r
a

Abstract—The uncertainties from distributed energy resources
(DERs) bring signiﬁcant challenges to the real-time operation
of microgrids. In addition, due to the nonlinear constraints
in the AC power ﬂow equation and the nonlinearity of the
battery storage model, etc., the optimization of the microgrid is
a mixed-integer nonlinear programming (MINLP) problem. It is
challenging to solve this kind of stochastic nonlinear optimization
problem. To address the challenge, this paper proposes a deep
reinforcement learning (DRL) based optimization strategy for the
real-time operation of the microgrid. Speciﬁcally, we construct
the detailed operation model for the microgrid and formulate
the real-time optimization problem as a Markov Decision Process
(MDP). Then, a double deep Q network (DDQN) based archi-
tecture is designed to solve the MINLP problem. The proposed
approach can learn a near-optimal strategy only from the histor-
ical data. The effectiveness of the proposed algorithm is validated
by the simulations on a 10-bus microgrid system and a modiﬁed
IEEE 69-bus microgrid system. The numerical simulation results
demonstrate that the proposed approach outperforms several
existing methods.

Index Terms—Real-time optimization, microgrid, deep rein-

force learning (DRL), deep Q network, nonlinearity.

Acronyms
DE
DGs
DQN
DRL
MINLP
MT
PV , W T
SOC

Parameters
ag, bg, cg
Cr
cbat

Gi, j, Bi, j

∆t
θ , ˆθ

NOMENCLATURE

Diesel engine generator.
Distributed generators.
Deep Q network.
Deep reinforcement learning.
Mixed-integer nonlinear programming.
Micro-gas turbine generator.
Photovoltaic and wind turbine.
State of charge.

Fuel cost coefﬁcients of controllable DG g.
Rated capacity of battery, kW h.
Per KWh degradation price for battery,
$/kW h.
Real part and imaginary part of the nodal
admittance matrix, respectively.
Time step.
Weights of evaluate neural network and tar-
get neural network.

Variables
a
ap, as
L
P, Q

p
r
s
V
δ
η
ω

Functions
f (·)
P.(·, ·)
Q(·, ·)
R.(·, ·)

Superscripts
c, d
max,min
on,o f f
∗

Indices
bat
g
grid
i, j
l
n
pv, wt
t
π

Action or decision variable.
Primary actions and secondary actions.
Discharging or charging loss of the battery.
Active (kW ) and reactive power (kVar) gen-
eration of power sources, such as control-
lable DGs, utility grid, battery, PV panels,
and WT.
Electricity price, $/kW h.
Single time period reward, $.
State variable.
Voltage amplitude of bus in the microgrid.
Phase angle of bus in the microgrid.
Discharge or charge efﬁciency of the battery.
Uncertainty variables of the system.

Transition function in MDP.
State transition probability.
Action-value function.
Immediate reward.

Charge and discharge mode of battery.
Maximum and minimum value.
ON and OFF status of controllable DGs.
Optimal value.

Battery.
Controllable DGs, such as MT and DE.
Utility grid.
Bus index.
Power cable.
Iteration index.
PV panel and wind generator.
Time index.
Operational policy mapping from the state s
to the action a.

I. INTRODUCTION

This work was supported by (Corresponding author : Xiaomeng Ai.)
H. Shuai, X. Ai, J. Fang, W. Yao, and J. Y. Wen are with School
of Electrical and Electronic Engineering, Huazhong University of Science
and Technology, Wuhan, 430074, China, and the Department of Electrical,
Computer and Biomedical Engineering, University of Rhode Island,Kingston,
RI, 02881,USA. (email: xiaomengai1986@foxmail.com)

M ICROGRID is a group of interconnected loads and

distributed generators (DGs) that acts as a single con-
trollable entity with respect to the grid [1]. Recent researches
show that microgrids can increase the efﬁciency of the power

 
 
 
 
 
 
supply, improve reliability of the power system, and enable
the integration of distributed energy resources (DERs) [2],
[3]. However, the random and intermittent characteristics of
DERs bring signiﬁcant challenges to the optimal operation of
microgrids.

To deal with the uncertainty in microgrids,

there have
been rich prior works [4]–[7] which focused on the energy
management of microgrids, and researchers have proposed
plenty of optimization approaches [5]–[7], such as robust
optimization [6] and stochastic programming [7]. Speciﬁ-
cally, Robust optimization uses prior knowledge to model
uncertainty parameters in a predeﬁned uncertainty set [8].
Stochastic programming deals with uncertainty by sampling
a set of scenarios that are typically generated by Monte Carlo
technique. Then, the stochastic problem is formulated as a
deterministic problem. In generally, theses approaches formu-
late the optimization of microgrid as an off-line optimization
problem, which makes it hard to adopt these methods to real-
time scheduling.

In recent years, real-time optimization of microgrids has
attracted much attention [9]–[14]. Many real-time optimization
methods, such as dynamic programming (DP) [9], approximate
dynamic programming (ADP) [10], model predictive control
(MPC) [11], [12], alternating direction method of multipliers
(ADMM) [13], and Lyapunov optimization [14], [15], have
been applied in microgrids. Although DP is a good optimiza-
tion method, it is time-consuming when applied in complex
environment. Moreover, traditional DP is difﬁcult to adapt
to any changing probabilities or un-modeled uncertainties
in the environment [14]. MPC is a widely used real-time
optimization approach. But, it needs the near future forecasting
information of the system. And the linear or mixed-integer
linear optimization model is usually adopted in MPC approach,
since nonlinear problems are intrinsically more difﬁcult to
solve [16]. In comparison with MPC, the algorithms proposed
in [13], [14] do not require any forecasting information, which
decreases the inﬂuence of the forecast error on the decision
making process. In [14], Lyapunov optimization relaxes the
time-coupled constraints and dynamically solves the single
time-period problem only according to the current system
state. However, the quadratic power ﬂow constraints are non-
linear equations and they are hard to be dealt with using
the approach proposed in [14]. Thus, the original nonlinear
optimization problem is relaxed to a convex optimization
problem. Last but not least, the historical data are not fully
utilized in the above real-time optimization methods.

Generally speaking, the above real-time optimization meth-
ods applied in microgrids at least have one of the following
drawbacks.

1) The method requires certain forecasting information. The
real-time optimization performance may deteriorate due
to the forecasting error.

2) The method needs to assume a stationary stochastic
process with known distribution information for the re-
newable energy and/or demand side [14], [17]. It
is
difﬁcult for the method to adapt to changing stochastic
processes.

3) The historical data of the system are not fully utilized.

ii

4) Because it is tough to solve multi-time period nonlinear
programming problems [16], the method usually simplify
or relax the optimization model to reduce the computa-
tional cost.

In order to overcome the drawbacks of the traditional op-
timization methods, learning-based optimization approaches
have been proposed to solve linear/nonlinear optimization
problems [18]–[23]. Reinforcement learning (RL) is a widely
used learning-based decision-making method which has been
applied in many aspects like the communication resource
allocation [18] and nonlinear optimization in smart grid [19]–
[21]. But traditional RL algorithms generally require manually
designed features. With the breakthrough of the deep learning
(DL), the deep neural network has became a powerful feature
extractor and has achieved great success in speech and image
recognition [24], [25], and natural language processing, etc. To
combine the advantages of the traditional RL and DL method,
researchers proposed the deep reinforcement learning (DRL)
approachs, such as deep Q network (DQN) [26], deep deter-
ministic policy gradient (DDPG) [27], asynchronous advantage
actor-critic (A3C) [28], soft actor critic (SAC) [29], AlphaZero
[30], and the effectiveness were validated in complex real-
time strategy games. Realizing the advantages of the DRL
approach, some researchers made the effort to use the DRL to
solve the optimal operation/control problems in power industry
in the past few years [31]–[35], including the electrical vehicle
(EV) charging/discharging scheduling [32], building energy
management [33], [34], and power system emergency control
[35], etc.

Regarding microgrid optimization problems, the DRL based
optimization approaches are also developed in some literature
recently [36]–[38]. Francois-Lavet et al. proposed a deep Q-
learning based approach for the battery and hydrogen tank
storage system scheduling in a microgrid [36]. In [37], a
double deep Q-learning based algorithm was proposed to
optimize the operation of the battery system in a micro-
grid. A deep Q-learning based approach was applied in the
energy management of a microgrid in [38]. Although these
methods achieved promising results, they generally formulated
the problem by either linearizing the component model or
neglecting some constraints in a microgrid. To obtain applica-
ble real-time operation decisions, formulating a high ﬁdelity
microgrid model is critical [39]. Constraints like AC power
ﬂow equations and detailed device models in a microgrid need
to be considered in the real-time optimization process.

Unlike the above approaches that generally ignore the
operational constraints in the microgrid, in this paper, we
consider all the necessary operational constraints (for example,
the detailed battery operation constraints, the AC power ﬂow
equations, the ON/OFF constraints of micro-generators). To
the best of our knowledge, this is the ﬁrst time that these
constraints have been considered in DRL based microgrid real-
time optimization approach.

In this paper, we propose a DRL based real-time opti-
mization strategy for the optimal operation of the microgrid.
More Speciﬁcally, a double deep Q-learning based real-time
optimization algorithm is proposed. The battery storage system
the necessary physical constraints such as
model and all

iii

power ﬂow equations are carefully considered. The complex
constraints in the microgrid cannot be directly dealt with by
the neural network, so a constraint processing mechanism
is proposed in this work. We use a deep neural network
to approximate the action-value function, and the proposed
DRL based optimization algorithm can learn to determine an
optimal operation strategy from the historical data. Numerical
simulations demonstrate the effectiveness of the proposed
algorithm.

The contributions of this paper are threefold.

1) A double deep Q network based real-time optimization
(DDQN-RTO) algorithm for the microgrid is proposed.
The proposed algorithm takes necessary nonlinear con-
straints such as the AC power ﬂow equations into con-
sideration.

2) To bridge the gap between deep reinforcement learning
theory and the formulated MINLP problem, a novel
procedure is proposed to deal with the constraints in the
microgrid model.

3) The simulation results demonstrate that

the proposed
DDQN-RTO algorithm can directly learn an effective
real-time operation strategy by utilizing the historical data
without depending on forecasting information.

The remainder of this paper is organized as follows. The
mathematical model of a microgrid is formulated in Section
II. Then a deep reinforcement learning based real-time op-
timization strategy for microgrid is proposed in Section III.
In Section IV, numerical simulations are presented to demon-
strate the effectiveness of the proposed approach. Conclusions
are summarized in section V.

B. Learning Problem Formulation

We formulate the real-time optimization of the microgrid as
a Markov Decision Processes (MDP) with discrete time step
t = {1, 2, . . . , T }. The objective is to ﬁnd the optimal schedules
to minimize the total costs.

The MDP formulation has ﬁve elements,

including the
system state S, the set of actions A, the system state transition
probability P.(·, ·),
the immediate reward R.(·, ·), and the
discount factor γ. The details of each element are presented
as follow.

1) State: The system state at time t is shown in Eq. (1).

st = (sg,t−1, Pg,t−1, Ppv,t , Pwt,t , Dt , Qt , pt , SOCt )

(1)

where sg,t−1 and Pg,t−1 represent the unit commitment and
the generation dispatch of controllable DGs at previous time
step; Unit commitment state, sg,t−1, is a binary variable which
represents the ON/OFF status of the controllable generator g at
time t − 1. Ppv,t and Pwt,t denote the generation dispatch of PV
panel and WT at current time; Dt and Qt represent the active
and reactive power demand of the system; pt is the electricity
price when exchanging electricity with the main grid; SOCt is
the state of charge (SOC) of the battery.

In this work, we also consider the various constraints of
the microgrid, including output limits of controllable DGs,
ramping rates of controllable DGs, minimum ON/OFF time
limits of controllable DGs, power ﬂow constraints, voltage
constraints, power cable capacity constraints, and battery oper-
ational constraints. The controllable DGs in this work include
the DE and MT.
Output limits of controllable DGs:

II. PROBLEM FORMULATION

(cid:40)

g ≤ Pg,t ≤ Pmax
Pmin
g ≤ Qg,t ≤ Qmax
Qmin

g

g

A. Optimal Operation of the Microgrid

Ramping rates of controllable DGs:

In this paper, the optimal operation model of the microgrid
is investigated. A typical microgrid is shown in Fig. 1 which
contains the diesel engine generator (DE), micro-gas turbine
generator (MT), wind turbines, rooftop PV panels, and a
battery storage. In this work, the microgrid can exchange
power with the utility grid.





∀g, ∀t

(2)

(sg,t − sg,t−1)

Pg,t − Pg,t−1 ≤ sg,t Rup,g + Pmin

Pg,t−1 − Pg,t ≤ sg,t−1Rdn,g + Pmin

(sg,t−1 − sg,t )

∀g, ∀t

g
+Pmax
g

(1 − sg,t )

g
(1 − sg,t−1)

+Pmax
g

(3)
where Rup,g and Rdn,g are the ramping up and ramping down
rates of controllable DG g, respectively.
Minimum ON/OFF time limits of controllable DGs:

(cid:40) (sg,t−1 − sg,t )(Son
(sg,t − sg,t−1)(So f f

g,t−1 − T on
g,t−1 − T o f f

g ) ≥ 0
) ≥ 0

g

∀g, ∀t

(4)

g,t−1 and So f f

where Son
the DG until time t − 1, respectively; T on
g
minimum ON and OFF duration of the DG, respectively.
Power ﬂow constraints:

g,t−1 are the ON and OFF time duration of
are the

and T o f f

g

Vi,t

Vj,t (Gi jcosδi j,t + Bi jsinδi j,t ) =

Ii,sPs,t − Di,t ∀t

(5)

Nbus
∑
j=1
Nbus
∑
j=1

Ns
∑
s=1
Ns
∑
s=1

Fig. 1. The components of a microgrid.

Vi,t

Vj,t (Gi jsinδi j,t − Bi jcosδi j,t ) =

Ii,sQs,t − Qi,t ∀t

PV PanelLoadWind TurbineBatteryUtility GridMicro-gas turbine generatorDiesel generator   Energy Management  System (EMS)where s denotes the different power sources in the microgrid,
including WT, PV, MT, DE, battery, and utility grid. Nbus is the
total bus number of the system. Ns is the total bus number that
is connected to power sources. Ii,s is the element of the bus -
generator correlation matrix. Di,t and Qi,t are the active (kW )
and reactive (kVar) load of the bus i at time t, respectively.
Gi, j and Bi, j are the real part and imaginary part of the nodal
admittance matrix, respectively.
Voltage amplitude and phase angle constraints:

(cid:40)

V min
i ≤ Vi,t ≤ V max
i ≤ δi,t ≤ δ max
δ min
Power cable capacity constraints:

i

i

Pl,t ≤ Pmax

l

∀l, ∀t

∀i, ∀t

(6)

(7)

where Pl,t and Pmax
maximum power transmission of cable l, respectively.
Battery constraints:

represent the power transmission and the

l

Battery SOC constraints are expressed as Eq. (8).

SOCmin ≤ SOCt ≤ SOCmax

(8)

In this work, the battery discharging/charging constraints
bat,t and charging

are also considered. The discharging loss Ld
loss Lc
bat,t are shown in Eq. (9) and (10) [40].
103 (cid:16)

(cid:17)

Ld
bat,t =

Rin + Kb
SOCt
V 2
r
103Cr.Kb (1 − SOCt )
SOCt .V 2
r

P2
bat,t

+

Pbat,t

(cid:17)

(9)

103 (cid:16)

−

(10)

1.1−SOCt

P2
bat,t

Lc
bat,t =

Rin + Kb
V 2
r
103Cr.Kb (1 − SOCt )
SOCt .V 2
r
where Rin is the internal resistance of the battery; Vr and Cr
are the rated voltage and rated capacity of the battery respec-
tively; Kb is the polarisation constant of battery; where Pbat,t
is the battery discharging/charging power, which is positive
during discharging and negative during charging. Therefore,
the discharging and charging efﬁciencies can be expressed as
Eq. (11) and (12).

Pbat,t

η d
t =

Pbat,t
Pbat,t + Ld

bat,t

η c
t = 1 +

Lc
bat,t
Pbat,t

(11)

(12)

According to the laboratory and ﬁeld tests [41], the battery
discharging/charging power limits are not only related to the
maximum discharging power Pd,max and minimum charging
power Pc,min but also determined by the SOC. The discharg-
ing/charging power limits are expressed as Eq. (13) and (14).

Pbat,t ≤ min

(cid:110)

bat,t (SOCt , η d,min
Pd,max

t

Pbat,t ≥ max

(cid:110)

bat,t (SOCt , η c,min
Pc,min

t

), Pd,max(cid:111)

), Pc,min(cid:111)

(13)

(14)

where Pd,max
(16).

bat,t (.) and Pc,min

bat,t (.) can be calculated as Eq. (15) and

iv

bat,t (SOCt , η d,min
Pd,max

t

) =

V 2
r SOCt (

− 1) − 103CrKb(1 − SOCt )

1
η d,min
t
103(Rin · SOCt + Kb)

(15)
r (1 − η c,min
)

t

)

(16)

bat,t (SOCt , η c,min
Pc,min

t

) =

103CrKb(1 − SOCt ) − SOCtV 2
103 · SOCt (Rin + Kb

1.1−SOCt

2) Action: The action is deﬁned as follows:

at = (sg,t , Pg,t , Qg,t , Pgrid,t , Qgrid,t , Pbat,t , Qbat,t ,Vi,t , δi,t )

(17)

where sg,t represents the unit commitment of controllable DGs;
Pg,t and Qg,t are the active and reactive power generation of
controllable DGs; Pgrid,t and Qgrid,t are the active and reactive
power exchange between the microgrid and the external grid;
Pbat,t is the battery charging/discharging power; Qbat,t is the
reactive power generation of the battery; Vi,t and δi,t are the
magnitude and the phase angle of the ith bus, respectively. The
subscript t represents the current time period.

3) State transition: The state transition from the current

state st to the next state st+1 is deﬁned as

st+1 = f (st , at , ωt )

(18)

where the state transition is determined by the action at and the
uncertainty ωt . Speciﬁcally, the state transition for the battery
SOC is determined by Eq. (19).




bat,t

SOCt =

SOCt−1 −

Pbat,t +Ld
Cr
−Pbat,t −Lc
Cr
where ∆t is the time step. In this work, we set ∆t equals to 1
hour.

SOCt−1 +

Pbat,t < 0

Pbat,t > 0

(19)

∆t,

∆t,



bat,t

For the state transition of the unit commitment of the
controllable DGs, it is directly controlled by the action at .
For Ppv,t , Pwt,t , Dt , Qt , pt , their state transitions are subject to
randomness because the generation dispatch of WT and PV
panel, the active and reactive load, and the electricity price are
all unknown in advance. For the transition of the generation
dispatch of DG, it is determined by the optimization process.
4) Reward: The reward at each time step is calculated as

Eq. (20).

rt = −sg,t (ag(Pg,t ∆t)2 + bgPg,t ∆t + cg)
−Csup,g − pt Pgrid,t ∆t −Cbat,t

(20)

This reward consists of four parts: the ﬁrst part sg,t (agP2
g,t +
bgPg,t +cg)∆t shows the fuel cost of controllable DGs, and it is
a quadratic function of its generation dispatch; the second part
Csup,g represents the startup cost of controllable DGs; the third
part pt Pgrid,t ∆t denotes the electricity cost when purchasing
electricity from the main grid; the fourth part Cbat,t
is the
battery degradation cost that is calculated according to [40]
and is shown in Eq. (21).

Cbat,t =

(cid:40) cbat (Pbat,t + Ld

bat,t )∆t,

cbat Lc

bat,t ∆t,

Pbat,t > 0
Pbat,t < 0

(21)

5) Action-value function: The quality of taking an action
a at a state s is measured by the expected total sum of future
rewards as Eq. (22).

Qπ(s, a) = Eπ

(cid:34) K
∑
k=t

γ k−t · rk

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

st = s, at = a

(22)

where Qπ(s, a) denotes the action-value function; π represents
the operational policy mapping from the state s to the action
a; γ is the discount factor that
is applied to balance the
importance between the immediate reward and future rewards;
K is the optimization horizon.

The objective of the microgrid optimal operation is to
the action-value

achieve an optimal policy π∗ such that
function can be maximized as Eq. (23).

Q∗(s, a) = max
π
In the above equation, Q∗(s, a) represents the optimal action-
value function.

Qπ(s, a)

(23)

III. PROPOSED APPROACH

This work focuses on the real-time optimization of mi-
crogrids. Due to the nonlinear AC power ﬂow constraints
(5), the nonlinear battery model (8) - (16), and the binary
variables in actions (17), etc., the optimization model is a
nonlinear optimization problem. In addition, the renewable
energy introduces uncertainty into the optimization problem.
These make it challenging for the traditional model-based
approaches to get the optimal operation policy. A RL-based
approach is proposed in this paper to solve this problem. The
proposed approach can directly learn an optimal policy from
the historical data.

Speciﬁcally, the RL approach updates the action-value func-

tion based on the Bellman equation as:

Qn+1(s, a) = E

(cid:110)

rt + γ max
at+1

Qn(st+1, at+1)|st = s, at = a

(cid:111)

(24)

where n is the iteration index. Theoretically, Q(s, a) will
converge to the optimal function when the number of iteration
n approaches inﬁnite value [42]. Then, the optimal operation
policy can be determined by:

a∗ = arg max

a

Q∗(s, a)

(25)

A deep neural network is used to approximate the action-
value function. With the combination of deep neural network
and the RL, a deep reinforcement learning based approach
is proposed in this paper to solve the real-time optimization
problem for microgrid.

A. Architecture of the Proposed DRL based Microgrid Opti-
mization

The architecture of the proposed DRL based microgrid op-
timization approach is shown in Fig. 2. The DQN based agent
determines the optimal actions of the microgrid according to
the state information of the system. Then, the actions are
executed in the environment, i.e., the microgrid system, and the
next system state and the reward of the action are obtained. In

v

this procedure, all the necessary constraints shown in Section
II are considered. The reward signal is fed into the agent to
guide the agent to make a better decision in the next iteration.
The details of the architecture are unfolded in the following
parts.

Neural network (NN) can uniformly approximate any con-
tinuous functions. The state information of the microgrid, s, is
fed to the input layer of the DQN, then by computing forward
through the layers of the NN we can obtain the output value
Q(s, a).

vin
t = g(W1 ∗ st + b1)
t = g(W2 ∗ vin
vh1
t + b2)
...

(26)

t = g(Wm ∗ vhm−1
vhm
Q(s, a) = g(Wm+1 ∗ vhm

+ bm)
t + bm+1)

t

where v is the value of the hidden unit, g(·) is the activation
function, W is the matrix of weights, b is the vector of biases,
and Q(s, a) is the output of the DQN which denotes the action-
value for all feasible actions when the microgrid under state
s.

The number of the output neurons of the DQN is determined
by the action space. From Eq. (17), the decisions Pg,t , Qg,t ,
Pgrid,t , Qgrid,t Vi,t , and δi,t are all continuous variables and
the dimensions of the variables increase quickly with the
number of the generators and buses in the microgrid. If all
these actions are included in the output layer of the DQN,
the NN will become very large. Besides, the actions made by
the DRL algorithm must fulﬁll the constraints Eq.(2) - Eq.(16).
These constraints are pretty hard to deal with, which will bring
signiﬁcant challenges to the design of the DQN and the solving
of Eq.(23).

To decrease the dimension of the output neurons of the
DQN, we divide the original actions into the primary actions
and the secondary actions. The primary actions include the
ON/OFF actions of the DGs and the charge/discharge power
of the battery, as given by:

(cid:9)

t = (cid:8)sg,t , Pbat,t
ap
The secondary actions include the generation dispatch of
DGs,
the power exchange between the microgrid and the
external grid, the reactive power generation of the battery bus,
and the magnitude/phase angle of the buses:

(27)

(cid:9)

(28)

t = (cid:8)Pg,t , Qg,t , Pgrid,t , Qgrid,t , Qbat,t ,Vi,t , δi,t
as
The primary actions are determined by solving Eq.(23). While
the secondary actions are obtained by solving the optimal
power ﬂow problem by considering the constraints Eq.(2)
- Eq.(16) using the interior point method (IPM) after the
primary actions have been determined. The details of the
decision making process are shown in Fig. 2. According to
the state information of the microgrid, the primary actions are
determined by the DQN agent using the following equation:
t = arg max Q(s, ap
ap
t )
Then, the DQN based real-time optimization algorithm needs
to ensure that the primary actions fulﬁll the minimum ON/OFF

(29)

vi

[43]. The target neural network is introduced in Algorithm 1
to reduce overestimations. The target Q network has the same
architecture with the evaluate Q network.

The ﬁrst step is to randomly initialize the parameters of the
neural networks. Then, in the outer loop the DQN is updated
for N epoches. Each epoch includes T training steps. In every
time step, the agent selects a action based on ε-greedy policy:

ap
t =

(cid:40) arg maxap

t ∈A Q(st , ap
t ; θ ),
select a random action,

randn(1)> ε
randn(1)< ε

(30)

where ε is gradually decreasing with the training step, as
shown below

ε = max(ε − ∆ε, ε min)

(31)

where ε min is the minimum value. According to the selected
primary action,
the reward and the next system state can
be calculated by the procedure shown in Fig. 2. Then the
transition (st , at , rt , st+1) is stored in the experience buffer D.
In the next, a minibatch of the history transitions are selected
and used to train the DQN agent. By minimizing the loss
function L(·), the parameters of the evaluate Q network are
updated as:

θt+1 = θt − β ∇θt L(θt )

(32)

where ∇θt is the gradient of the loss function, β is the learning
rate of the algorithm. The loss function is the error between
the target value and the evaluate value which can be deﬁned
as:

L(θ ) =

(cid:20)(cid:16)

E

#F
∑
j=1

y j − Q

(cid:16)
s j, ap

j ; θ

(cid:17)(cid:17)2(cid:21)

In Eq. (33), the target value is calculated as:

y j = r j + γ arg max
ap

ˆQ(s j+1, ap; ˆθ )

Finally, the parameters of the target Q network are updated. To
further stable the learning process, we adopted the soft update
strategy to update the target network:

ˆθ = (1 − τ) ˆθ + τθ
(35)
where ˆθ and θ are the parameters of the target Q network
and evaluate Q network, respectively; τ is the updating rate.
It is worth to note that the target Q network is also updated
in every step.

C. DDQN based Real-time Optimization for the Microgrid

After the DQN agent has been well trained by Algorithm 1,
it will be directly utilized in the real-time optimization process
of the microgrid. The details of the real-time optimization
process are shown in Algorithm 2. The parameters of the
DDQN won’t be updated during the process. For the current
time t, the DQN agent obtains the system state ﬁrstly. Then
the Q values are calculated for all feasible primary actions
under the system state st . The optimal primary actions will be
selected and the secondary actions are obtained by solving
the OPF problem. By executing the real-time operational
decisions, the system steps into the next time period. In the
next time period, the decisions can be obtained by repeating
the above procedure.

(33)

(34)

Fig. 2. The proposed architecture of the proposed algorithm.

time limits shown in Eq. (4). When the constraints are not
fulﬁlled, the reward is set to be a small value M2. After
receiving a small reward, the agent learns that the action
should obey Eq. (4). Thus, in the following training iterations,
the agent will
try to make sure the DG’s ON/OFF time
duration larger than the limitation to get a bigger reward.
Otherwise, using the charge/discharge power given by the
primary actions, the SOC of the battery at the next time
period can be calculated by Eq. (19). If the SOC of the
battery exceeds the upper or lower boundary, we set the SOC
equals to SOCmax or SOCmin and recompute the output power
of the battery. Then, the active/reactive power output of all
controllable and uncontrollable DGs, and the voltage of all
buses are determined by solving the optimal power ﬂow (OPF)
subproblem. In this work, the OPF is calculated by IPM. If
the power ﬂow converged, we could obtain the optimal power
output of DGs and the external grid, so the reward can be
calculated by Eq. (20). Otherwise, the reward is set to be a
small number M1.

Fig. 2 shows that

the proposed architecture greatly de-
creased the action space. More importantly, a large part of
the operational constraints of the microgrid is handled by
IPM. These make it possible to use DQN agent to solve this
problem.

B. Training Process of the DDQN-RTO Algorithm

The DQN agent needs to be trained off-line ﬁrst. The train-
ing process of the proposed algorithm is shown in Algorithm 1.
To overcome unstable learning process, experience replay [26]
technique is adopted in Algorithm 1. Experience replay mech-
anism works by storing historical transitions (st , at , rt , st+1) in
the replay memory and randomly sampling a mini-batch data
from the stored transitions to train the NN. Randomizing the
samples breaks the temporal correlations between the system
state transitions and therefore reduces the variance of the
learning process. Besides, using the same neural network both
to select and to evaluate an action will lead to overestimations

DQN agentState variables: sReward: rPrimary actions: (sg,t ,Pbat,t)Compute the SOCt+1 after taking action Pbat,tObtain the secondary actions using IPMCompute rtPower flow converged？Set rt=M1YNFulfill the  constraints (4) ?YSOCmin  SOCt+1 SOCmaxYNSet SOCt+1=SOCmin or SOCmax，and re-compute Pbat,tNSet rt=M2rtst+1Environment(Microgrid system)LSTMLSTMLSTMPV powerWind powerLoad powerCurrent state informationAlgorithm 1 Training of Double Deep Q Network with
experience replay
1: Initialization: set the replay memory D; initialize action-
value function Q and target action-value function ˆQ with
random weights θ and ˆθ , respectively.

TABLE I
THE FUEL COST COEFFICIENTS OF MT AND DE

Controllable DGs
MT
DE

a ($/kW h)
0.00051
0.00104

b ($/kW h)
0.0397
0.0304

c ($)
0.4
1.3

vii

turbine and the diesel generator is 30 kW and 10 kW, re-
spectively. The minimum start-up and shut-down time of all
controllable DGs are 1 hour. The start-up cost of the micro-
gas turbine generator and diesel generator are $ 2 and $
3, respectively. The fuel cost coefﬁcients of the controllable
DGs are shown in Tab. I. The minimum and maximum
energy can be stored in the battery are 18kWh and 60kWh,
respectively. The maximum charge/discharge power is 12 kW.
The operation cost coefﬁcient of the battery is 0.059 $/kWh.
Other necessary parameters of the battery can be found in
[19]. The power exchange limitation between the microgrid
and the external grid is 50kW. The reactance and resistance
parameters of all cables are X = 0.1Ω/km, R = 0.64Ω/km.
We implemented the proposed DDQN-RTO method in Python
and the PYPOWER 5.1.4 package was adopted as the optimal
power ﬂow solver. All case studies are conducted on a 64-bit
windows based computer with 16 GB RAM and Intel Core i7
processor clocking at 3.41 GHz.

2: for Epoch=1:N do
3:
4:
5:

Generate a training scenario and initialize state s1.
for Time step t=1:T do

t based on ε-greedy policy.

(cid:110)(cid:16)

(cid:1) in D.

in emulator. Then, observe

Select primary action ap
Execute action ap
t
reward rt and process to the new state st+1 using
the method shown in Fig. 2.
Store transition (cid:0)st , ap
t , rt , st+1
Sample random minibatch of transitions
F =
j , r j, s j+1
y j ←− r j + γ argmaxap ˆQ (cid:0)s j+1, ap; ˆθ (cid:1).
Update parameters θ by minimizing loss
(cid:17)(cid:17)2(cid:21)
(cid:20)(cid:16)
function ∑#F
.
j=1
Update the parameters of the target Q network
using Eq. (35)

(cid:16)
s j, ap

from D.

s j, ap

y j − Q

(cid:17)(cid:111)#F

j ; θ

j=1

E

6:

7:
8:

9:
10:

11:

end for

12:
13: end for

Algorithm 2 Real-time optimization of the microgrid using
the well-trained DQN agent

1: Load the well-trained DQN’s parmeters θ .
2: for Time step t = 1 : T do
3:
4:
5:

Obtain current state information of the microgrid.
DQN calculates the action-value Q(st , ap
primary
Obtain
t ; θ ).
arg maxap
Calculate the secondary actions by solving OPF prob-
lem using IPM.
Output the optimal actions.

the
t ∈A Q(st , ap

t ; θ ).
by

ap
t =

actions

6:

7:
8: end for

IV. EXPERIMENTAL RESULTS

The performance of the proposed DDQN-RTO algorithm
was tested through simulations on a 10-bus microgrid system
and a modiﬁed 69-bus microgrid system. To validate the
effectiveness of the proposed algorithm, we ﬁrstly designed
a deterministic case and compared the DDQN-RTO algorithm
with look-up table based approximate dynamic programming
(ADP) algorithm [19], myopic policy [44] and particle swarm
optimization (PSO) algorithm, etc. Then, the real-time opti-
mization performance of the proposed algorithm was tested in
the stochastic case study. Finally, in order to demonstrate the
algorithm can learn an optimal decision policy from historical
data, we explored the optimization performance of the DDQN-
RTO algorithm using the historical data from actual power
grid.

The test microgrid system [19] is shown in Fig. 3. The
microgrid includes a micro-gas turbine generator, a diesel
generator, wind turbines, PV panels, and a battery storage
system. The maximum/minimum power output of micro-gas

Fig. 3. The test microgrid system.

A. Case Study I: Deterministic Optimization

The forecasted power generation of wind turbines and PV
panels are shown in Fig. 4. We use Algorithm 1 to train
the DQN agent. Since there is not any uncertainty exits
in the deterministic case, the training scenarios are all the
same in each epoch. Part of the system states which include
(sg,t−1, Ppv,t , Pwt,t , Dt , pt , SOCt ) are directly input to the DDQN,
thus the input layer has 7 neurons. The number of layers and
the number of neurons in each layer was carefully designed to
obtain a good optimization performance. Finally, the neuron
network has four layers of hidden neurons, each layer having
50, 100, 100, 50 neurons, respectively. All the neurons located
in input and hidden layers are with ReLU as the activation
function. We discretize the charge/discharge power of the
battery into 9 levels (-12 kW, -9 kW, -6 kW, -3 kW, 0 kW,

Utility gridTransformer10/0.4kVACDCPCC50m50m100m80m30m100m100m200m150mMicro-gas turbine generatorDiesel generatorWind turbineBatteryBusFeederLoadPV panel21345678910TABLE II
THE OPERATION COST OF THE MICROGRID

Algorithm

Operation Cost ($)

Optimality Gap

DDQN-RTO
ADP
PSO
Myopic
DP

90.25
90.26
95.68
102.68
89.49

0.85%
0.86%
6.92%
14.74%
0%

viii

3 kW, 6 kW, 9 kW, 12 kW). The output layer of the DDQN
has 36 neurons and each neuron represents the Q-value of a
feasible action.

The learning rate is set to β = 0.01, the discount factor to
γ = 0.99, the reply memory size to D = 10000, the batch size
to B = 32, ∆ε = 5 ∗ 10−5, and ε0 = 1. The model is trained
for 1500 episodes, where an episode is composed by 24 time
steps. The parameters of the evaluate and target Q network are
updated in every step. The Adam optimizer is adopted in the
training process to minimize the loss function.

Fig. 5. The optimized power output of the power sources using the DDQN-
RTO algorithm.

TABLE III
THE FORECASTING ERROR DISTRIBUTION

Forecasting error distribution

Day-ahead

Intra-day

Wind power
PV power
Load
Electricity price

N (0, 0.12) N (0, 0.052)
N (0, 0.12) N (0, 0.052)
N (0, 0.052) N (0, 0.022)
N (0, 0.052) N (0, 0.032)

pt+1, pt+2, · · · , pt+H ), and (Dt+1, Dt+2, · · · , Dt+H ), are stacked
with the state vector shown in Eq. (1) and set as the input of
the DDQN at time t. In this case, the H is set to be 4. So the
input layer of the DDQN consists of 23 neurons. The structure
of the hidden layer of the network is the same as the NN in
the case study I. The proposed DQN agent is ﬁrst trained to
learn the optimal operation strategy under uncertainties. Then,
the well-trained algorithm is used in the real-time optimization
process and compared with several other methods.
1) T he O f f − Line Training Process o f

the DDQN −

RT O Algorithm

The training procedure is shown in Algorithm 1. To train the
agent, we generated 1500 set of training scenarios according to
the day-ahead forecast curves and the day-ahead forecast error
distribution information of wind and PV power generation,
load power, and electricity price. The scenarios are generated
using the Monte Carlo simulation method. Then, the intra-
day forecasting errors will be added to the above generated
scenarios to take intra-day uncertainty into consideration. In
this work, we assume the day-ahead and intra-day forecasting
errors of renewable energy, electricity price, and load obey
the Normal Distribution as shown in Tab. III [45]–[48]. The
proposed algorithm is trained in 1500 epochs. In each epoch,
we randomly select a training scenario from the generated
scenarios in step 3 of Algorithm 1, and the cumulative rewards
of the scenario are calculated. Besides, we select 50 test sce-
narios to calculate the expected operation cost of the microgrid
using the updated DQN agent every 5 training epochs. The
cumulative rewards over 1500 training epochs are shown in
Fig. 6. The expected operation cost of the microgrid over
the training epochs is shown in Fig. 7. From the results, the
DDQN-RTO algorithm converged after 1500 training epochs.

2) T he Real −Time Optimization Per f ormance o f the DDQN −

RT O Algorithm

After the DQN agent was well-trained day-ahead, we can
use it to obtain the real-time operational decisions according

Fig. 4. The forecast information of the microgrid.

The simulation results are shown in Fig. 5. From the ﬁgure,
it can be found that the load electricity is mainly provided
by the MT and DE. The microgrid needs to buy electricity
from the external grid in the evening during which the demand
is high, but renewable energy is relatively low. The battery
stores energy during the midnight hours and discharges in the
evening peak hours. To validate the optimization performance
of the proposed DDQN-RTO algorithm, we compared the
method with PSO, ADP [19], and the DP algorithm. The total
operation cost of the microgrid obtained by different methods
is shown in Tab. II. In this work, the optimization result of
the DP method is used as ground truth. From the results, the
DDQN-RTO algorithm performs better than myopic policy,
PSO algorithm and ADP algorithm. Thus, the effectiveness of
the proposed DDQN-RTO algorithm is demonstrated in the
deterministic case.

B. Case Study II: Real-Time Optimization Considering Un-
certainty

Since the load and wind/solar power exit forecasting errors,
real-time optimization is needed to deal with the uncertainty.
To facilitate the DQN agent to learn the uncertainties brought
by wind/solar power, electricity price, and electricity load, a
sequence of the most recent forecast information including
(Ppv,t+1, Ppv,t+2, · · · , Ppv,t+H ), (Pwt,t+1, Pwt,t+2, · · · , Pwt,t+H ), (

00.20.4Electricity price ($)Time (h)0510152025050100Power (kW)Electricity price curvePV powerWind powerLoad-40-30-20-100102030Time (h)Output Power (kW)05101520250.30.40.50.60.70.80.91SOC PgridPMTPDEPbatSOCix

Fig. 6. The convergence process of the cumulative rewards.

Fig. 7. The expected operation cost of the microgrid during the training
process.

to the actual state of the system. To validate the real-time opti-
mization performance of the proposed algorithm, we randomly
selected 200 test scenarios. Similar to the training scenarios,
the Monte Carlo simulation is adopted to generate the test
scenarios. For every scenario, the online optimization process
is shown in Algorithm 2. We also compared the performance
of the DDQN-RTO algorithm with ADP, MPC+PSO algo-
rithm, and myopic policy. It is worth to note that the real-time
optimization model is a MINLP problem. Thus the traditional
MPC method is hard to be directly used to solve the problem.
So the MPC embedded with PSO method which indicated by
MPC+PSO is adopted to solve the nonlinear problem. For a
single test scenario, we use the result optimized by DP as
the baseline. The simulation results are shown in Fig. 8. The
average optimality gap of the DDQN-RTO is 1.23%. While the
average optimality gap of the ADP, MPC+PSO algorithm, and
myopic policy are 1.80%, 7.22%, and 13.75%, respectively.
We can ﬁnd that the average optimization performance of the
DDQN-RTO algorithm outperforms the other three algorithms.
Compared with the value table based function approximation
method adopted by the ADP algorithm, the learning ability of
DQN agent is more powerful. Thus the proposed algorithm
can obtain better real-time optimization performance in the
stochastic environment. MPC+PSO policy gets the operational
decisions on a shorter optimization window which makes
the optimization result is not global optimality. Although the
MPC+PSO method also utilized intra-day updated forecasting
information, the proposed learning-based algorithm performs
better.

Fig. 8. The real-time optimization gaps of the DDQN-RTO, ADP, MPC+PSO,
and myopic methods.

C. Case Study III: Real-Time Optimization by Learning from
Historical Data

The real-time optimization process shown in case study II
needs to train the DQN agent according to the day-ahead
forecasting information and the forest error distributions about
the DERs, load, and electricity price. Then, system operators
use the well-trained DQN agent to optimize the operational
decisions of the microgrid in real-time. Thus, the DQN agent
needs to be trained every day. Unfortunately, this training
procedure is very time-consuming. For example,
the total
training process shown in Fig. 6 takes about 10 hours. Besides,
the forecasting error distribution information is also needed to
generate training scenarios. However, ﬁnding out the accurate
forecasting error distributions of renewable energy and load
is still not a easy task. Considering the load demand and the
power generation of PV are periodic, the DQN agent can be
trained by the historical data and be utilized in the real-time
optimization in the future. In this section, we will train the
DQN agent by using the historical renewable generation data
[49] and load power data [50] from the open power system
data platform and further investigate the effectiveness of the
proposed algorithm in the forecasting free circumstance.

The data used in this case is shown in Fig. 9, and the
electricity price is shown in Fig. 4. We used 100 days of
historical data to train the agent and 10 days of data to test
the real-time optimization performance of the DDQN-RTO. To
learn features from historical data, the LSTM technique is used
to extract features. The inputs of the LSTM include the past 24
hours load, and power generation of WT and PV. The outputs
of the LSTM are fed to the input layer of the DDQN. In this
work, 3 LSTM networks are adopted to extract features from
the historical wind/solar power, and load curves. The number
of the output of each LSTM is 64. The output of LSTM and the
current system state (sg,t−1, Ppv,t , Pwt,t , Dt , pt , SOCt ) are input
to the DDQN. The DQN has two hidden layers, and each
layer has 256 neurons. The output layer of the DDQN also
has 36 neurons. The hyperparameters of the DDQN are the
same as the above case study. The training process is similar
with the procedures given in Algorithm 1, and we randomly
select a day and use the corresponding data to train the agent in
each epoch. Every 10 training epochs, we use the selected test
scenarios to test the performance of the proposed algorithm.

050010001500-180-160-140-120-100-80-60Training epochCumulative rewards ($)025050075010001250150090100110120130140150160Training epochThe expectation of the operation cost of the microgrid ($)DDQN-RTOADPMPC+PSOMyopicOptimization methods051015Real-Time optimization gap (%)Fig. 10 shows the convergence process of the DDQN-RTO
algorithm. It can be seen from the result that the objective
function gradually decreased with the training process. Finally,
the operational cost of the microgrid converged around $
1524.49. The result illustrates that the proposed DDQN-RTO
algorithm can successfully learn a policy to minimize the op-
erational cost of the microgrid. Then, we use the well-trained
DQN agent to test the real-time optimization performance
of the DDQN-RTO algorithm under the test scenarios. The
generation dispatch of all DGs over 7 consecutive days are
shown in Fig. 11. The fuel consumption costs of MT and
DE are lower than electricity price. We can observe that the
proposed DDQN-RTO algorithm has learned to provide the
load by DGs ﬁrst and to purchase electricity from the utility
grid in the peak load hours and sell surplus power to the grid
in the midnight.

To further validate the effectiveness of the proposed algo-
rithm, we compared the real-time optimization performance of
the DDQN-RTO algorithm with ADP algorithm and myopic
policy. For each test scenario, the optimal operation cost can
be calculated by the DP algorithm. Thus, the optimization
result of DP is used as ground truth. The operation costs
optimized by different methods are shown in Table IV. The
optimality gaps of the three algorithms are also shown in
Fig. 12. The average optimality gap of the DDQN-RTO, ADP
algorithm, and myopic policy are 2.98%, 4.41 %, and 4.94%,
respectively. From the results, it can be found that the DDQN-
RTO algorithm performs better than the ADP algorithm and
the myopic policy.

Fig. 9. The training and test data.

D. Case Study IV: Real-Time Optimization Performance of
the DDQN-RTO Algorithm on the Modiﬁed IEEE 69-bus
Microgrid System

To further demonstrate the proposed algorithm can learn
to operate the system from historical data, we tested the

x

Fig. 10. The convergence process of the DDQN-RTO algorithm.

performance of the DDQN-RTO algorithm on a relatively
larger microgrid system, as shown in Fig. 13. The test system
is modiﬁed based on the standard IEEE 69-bus distribution
system [51]. In the test system, there includes a battery storage
system and 8 micro-generators which including distributed
PV panels, wind turbines, gas turbine generators, and diesel
generators. The locations of the micro-generators have been
shown in Fig. 13. The maximum power output of the diesel
generator and micro-gas turbine generator are 450 kW, and the
minimum value are 150 kW. The capacity of the wind farm
on bus 37 and PV panels on bus 21 are 525 kW and 465 kW,
respectively. The total capacity of the wind power and the solar
power in the system are 1050 kW and 930 kW, respectively.
The maximum charge/discharge power of the battery in Fig.
13 is 360 kW, and the capacity is 1800 kWh. The power
transmission limitation between the microgrid and the main
grid is 1500 kW. The maximum load power is 3000 kW. The
grid network parameters are modiﬁed accordingly in this work
based on the parameters provided by [51].

We discretize the charge/discharge power of the battery
into 9 levels (-360kW, -270kW, -180kW, -90kW, 0kW, 90kW,
180kW, 270kW, 360kW). The commitment number of the
controllable micro-generators is 16. So, the size of the decision
space is 144. Then, the output layer of the DDQN used in
this simulation includes 144 neurons. The neural network
architecture and the hyperparameter of the LSTM and DDQN
are the same with Case study III. The training data and test
date used in this case are similar with the data used in Case
study III, which are generated by scaling up the curves in Fig.
9 according to the capacity of the renewable energy and the
maximum load power of the system.

After 17 hours of training, the algorithm converged to a
stable objective value. The convergence curve of the proposed
algorithm is shown in Fig. 14. Using the agent trained off-
line, we randomly selected 10 days which were not used in the
training process to test the real-time optimization performance
of the algorithm. The results are shown in Table V. It can
be found that
the DDQN-RTO algorithm reaches a much
smaller average optimality gap compared with the other two
algorithms. Besides, it takes 1.30 s to generate one schedule
for the DDQN-RTO algorithm. The time consumptions to
generate one schedule for the ADP and the myopic policy
are 13.46 s and 12.20 s, respectively. One can ﬁnd that
the ADP and the myopic policy take a longer optimization
time than the proposed algorithm. This is because these two
methods need to traverse all
the feasible primary actions
to select the optimal one, this process is time-consuming.
However, the proposed algorithm gets the optimal primary

05001000150020002500406080100120  LoadLoad power (kW)Time slot (h)05001000150020002500010203040Time slot (h)Wind power (kW)  050010001500200025000102030Time slot (h)PV power (kW)  WindPVTraining dataTest data02004006008001000120014001600180020001400160018002000Training epochTotal operation cost of the microgrid ($)TABLE IV
THE OPERATION COST OF THE MICROGRID UNDER THE TEST SCENARIOS ($)

Test Scenario

#1

#2

#3

#4

#5

#6

#7

#8

#9

#10

DDQN-RTO
ADP
Myopic
DP

184.19
185.95
185.49
180.20

195.98
197.27
196.81
191.20

191.83
193.12
192.65
187.04

110.43
112.36
115.25
106.21

154.02
155.88
155.42
149.81

128.06
129.99
131.84
123.82

122.33
124.26
127.15
118.47

165.78
170.44
167.08
161.47

140.36
142.12
145.02
136.07

131.50
133.42
132.96
127.28

xi

Fig. 11. The power output of the DGs and the utility grid.

space and the complex constraints in the microgrid model,
we constructed the primary actions and the secondary ac-
tions. Then, a DDQN based DRL algorithm was proposed to
solve the stochastic nonlinear optimization problem. Finally,
the effectiveness of the proposed approach was validated by
numerical simulations on a 10-bus microgrid test system and
a modiﬁed IEEE 69-bus microgrid test system. From the
simulation results, the proposed algorithm outperformed the
traditional ADP, MPC, and myopic methods in both deter-
ministic and stochastic case studies. Especially, we found that
the DDQN-RTO algorithm can learn from the historical data
to make near-optimal decisions in the real-time optimization
process.

The DDQN based optimization approach needs to discretize
the continuous actions in the system which is not convenient
to the application of the algorithm to much complex systems.
To overcome this deﬁciency, the authors will combine the
proposed method with other state-of-the-art reinforcement
learning approach such as Trust Region Policy Optimization
(TRPO) in the future work.

REFERENCES

[1] D. T. Ton and M. A. Smith, “The us department of energy’s microgrid
initiative,” The Electricity Journal, vol. 25, no. 8, pp. 84–94, 2012.
[2] A. Izadian, N. Girrens, and P. Khayyer, “Renewable energy policies: A
brief review of the latest us and eu policies,” IEEE Industrial Electronics
Magazine, vol. 7, no. 3, pp. 21–34, 2013.

[3] Z. Wang, B. Chen, J. Wang, M. M. Begovic, and C. Chen, “Coordinated
energy management of networked microgrids in distribution systems,”
IEEE Transactions on Smart Grid, vol. 6, no. 1, pp. 45–53, 2015.
[4] S. Grillo, M. Marinelli, S. Massucco, and F. Silvestro, “Optimal manage-
ment strategy of a battery-based storage system to improve renewable
energy integration in distribution networks,” IEEE Transactions on
Smart Grid, vol. 3, no. 2, pp. 950–958, June 2012.

[5] B. Huang, L. Liu, H. Zhang, Y. Li, and Q. Sun, “Distributed optimal
economic dispatch for microgrids considering communication delays,”
IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 49,
no. 8, pp. 1634–1642, Aug 2019.

Fig. 12. The optimality gap of the DDQN-RTO and the ADP algorithm.

TABLE V
THE ON-LINE OPTIMIZATION RESULTS OF THE DDQN-RTO, ADP, AND
MYOPIC POLICY ON THE MODIFIED IEEE-69 BUS MICROGRID SYSTEM

Algorithm

DDQN-RTO

ADP

Myopic

Average operation cost ($)
Average optimality gap (%)
Maximum optimality gap (%)
Minimum optimality gap (%)
Standard deviation of
the optimization gaps (10−2)

3196.3
4.33
7.86
3.19

1.715

3218.5
5.06
8.11
3.27

3248.3
6.03
10.26
4.25

1.675

2.092

actions by Eq.(29) which corresponds to a neural network
forward calculation procedure, then we solve an optimal power
ﬂow sub-problem to get
the
results validated the effectiveness of the proposed method on
a relative larger test system.

the secondary actions. Thus,

V. CONCLUSION AND FUTURE WORK

In this paper, we proposed a DDQN based real-time op-
timization strategy for the optimal operation of microgrids.
The power ﬂow constraints and the detailed battery model
were considered in this paper. To deal with the huge action

020406080100120140160Time slot (h)-50050100Power (kW)GridMTDEBattery123456789100246810Test caseOn-line optimization error (%)  MyopicADPDDQN-RTOxii

Fig. 13. The single-line diagram of the IEEE 69-bus microgrid system.

[16] D. P. Bertsekas, “Nonlinear programming,” Journal of the Operational

Research Society, vol. 48, no. 3, pp. 334–334, 1997.

[17] K. Rahbar, J. Xu, and R. Zhang, “Real-time energy storage manage-
ment for renewable integration in microgrid: An off-line optimization
approach,” IEEE Transactions on Smart Grid, vol. 6, no. 1, pp. 124–134,
Jan 2015.

[18] N. Jiang, Y. Deng, A. Nallanathan, and J. A. Chambers, “Reinforcement
learning for real-time optimization in nb-iot networks,” IEEE Journal
on Selected Areas in Communications, vol. 37, no. 6, pp. 1424–1440,
June 2019.

[19] H. Shuai, J. Fang, X. Ai, J. Wen, and H. He, “Optimal real-time
operation strategy for microgrid: An adp-based stochastic nonlinear
optimization approach,” IEEE Transactions on Sustainable Energy,
vol. 10, no. 2, pp. 931–942, April 2019.

[20] X. He, X. Fang, and J. Yu, “Distributed energy management strategy for
reaching cost-driven optimal operation integrated with wind forecasting
in multimicrogrids system,” IEEE Transactions on Systems, Man, and
Cybernetics: Systems, vol. 49, no. 8, pp. 1643–1651, Aug 2019.
[21] J. F. T. D. Z. C. H. Shuai, X. Ai and J. Wen, “Real-time optimization
of the integrated gas and power systems using hybrid approximate
dynamic programming,” International Journal of Electrical Power &
Energy Systems, vol. 118, p. 105776, 2020.

[22] J. Gao, “Machine learning applications for data center optimization,”

2014.

[23] T. T. Teo, T. Logenthiran, W. L. Woo, and K. Abidi, “Intelligent
controller for energy storage system in grid-connected microgrid,” IEEE
Transactions on Systems, Man, and Cybernetics: Systems, pp. 1–9, 2018.
[24] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,
and D. Yu, “Convolutional neural networks for speech recognition,”
IEEE/ACM Transactions on audio, speech, and language processing,
vol. 22, no. 10, pp. 1533–1545, 2014.

[25] Z. Wan and H. He, “Answernet: Learning to answer questions,” IEEE

Transactions on Big Data, pp. 1–1, 2019.

[26] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” arXiv preprint arXiv:1312.5602, 2013.

[27] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[28] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-
forcement learning,” in International conference on machine learning,
2016, pp. 1928–1937.

[29] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Ku-
mar, H. Zhu, A. Gupta, P. Abbeel et al., “Soft actor-critic algorithms
and applications,” arXiv preprint arXiv:1812.05905, 2018.

[30] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering

Fig. 14. The convergence process of the DDQN-RTO algorithm applied in
the IEEE 69-bus microgrid test system.

[6] Y. Zhang, N. Gatsis, and G. B. Giannakis, “Robust energy management
for microgrids with high-penetration renewables,” IEEE Transactions on
Sustainable Energy, vol. 4, no. 4, pp. 944–953, 2013.

[7] S. Wang, H. Gangammanavar, S. D. Eks¸ioˇglu, and S. J. Mason, “Stochas-
tic optimization for energy management in power systems with multiple
microgrids,” IEEE Transactions on Smart Grid, vol. 10, no. 1, pp. 1068–
1079, Jan 2019.

[8] K. Hedman, A. Korad, M. Zhang, A. Dominguez-Garcia, and X. Jiang,
“The application of robust optimization in power systems,” Final Report
to the Power Systems Engineering Research Center, PSERC Publication,
pp. 76–77, 2014.

[9] L. M. Costa and G. Kariniotakis, “A stochastic dynamic programming
model for optimal use of local energy resources in a market environ-
ment,” in 2007 IEEE Lausanne Power Tech, July 2007, pp. 449–454.

[10] H. Shuai, J. Fang, X. Ai, Y. Tang, J. Wen, and H. He, “Stochastic
optimization of economic dispatch for microgrid based on approximate
dynamic programming,” IEEE Transactions on Smart Grid, vol. 10,
no. 3, pp. 2440–2452, 2019.

[11] W. Gu, Z. Wang, Z. Wu, Z. Luo, Y. Tang, and J. Wang, “An online
optimal dispatch schedule for cchp microgrids based on model predictive
control,” IEEE transactions on smart grid, vol. 8, no. 5, pp. 2332–2342,
2017.

[12] X. Kong, X. Liu, L. Ma, and K. Y. Lee, “Hierarchical distributed model
predictive control of standalone wind/solar/battery power system,” IEEE
Transactions on Systems, Man, and Cybernetics: Systems, vol. 49, no. 8,
pp. 1570–1581, Aug 2019.

[13] W.-J. Ma, J. Wang, V. Gupta, and C. Chen, “Distributed energy manage-
ment for networked microgrids using online admm with regret,” IEEE
Transactions on Smart Grid, vol. 9, no. 2, pp. 847–856, 2018.

[14] W. Shi, N. Li, C.-C. Chu, and R. Gadh, “Real-time energy management
in microgrids,” IEEE Transactions on Smart Grid, vol. 8, no. 1, pp.
228–238, 2017.

[15] S. Salinas, M. Li, P. Li, and Y. Fu, “Dynamic energy management for
the smart grid with distributed energy resources,” IEEE Transactions on
Smart Grid, vol. 4, no. 4, pp. 2139–2151, Dec 2013.

23415678373839364041424391011121314Main grid1516171819202122232425262729303128323334354748495054555653575859606162636465444546515269666768Gas turbine generatorDiesel  generaotrWind turbineBatteryBusLineLoadPV panel02505007501000125015003000350040004500Training epochTotal operation cost of the microgrid ($)xiii

the game of go without human knowledge,” Nature, vol. 550, no. 7676,
p. 354, 2017.

[31] X. Le, S. Chen, F. Li, Z. Yan, and J. Xi, “Distributed neurodynamic
optimization for energy internet management,” IEEE Transactions on
Systems, Man, and Cybernetics: Systems, vol. 49, no. 8, pp. 1624–1633,
Aug 2019.

[32] Z. Wan, H. L. H. He, and D. Prokhorov, “Model-free real-time ev
charging scheduling based on deep reinforcement learning,” IEEE Trans-
actions on Smart Grid, 2018.

[33] Y. Chen, Y. Shi, and B. Zhang, “Modeling and optimization of complex
building energy systems with deep neural networks,” in 2017 51st
Asilomar Conference on Signals, Systems, and Computers.
IEEE, 2017,
pp. 1368–1373.

[34] E. Mocanu, D. C. Mocanu, P. H. Nguyen, A. Liotta, M. E. Webber,
M. Gibescu, and J. G. Slootweg, “On-line building energy optimization
using deep reinforcement learning,” IEEE Transactions on Smart Grid,
2018.

[35] Q. Huang, W. Hao, J. Tan, R. Fan, and Z. Huang, “Adaptive power
system emergency control using deep reinforcement learning,” arXiv
preprint arXiv:1903.03712, 2019.

[36] V. Franc¸ois-Lavet, D. Taralla, D. Ernst, and R. Fonteneau, “Deep
reinforcement learning solutions for energy microgrids management,”
in European Workshop on Reinforcement Learning (EWRL 2016), 2016.
[37] V. Bui, A. Hussain, and H. Kim, “Double deep q-learning-based dis-
tributed operation of battery energy storage system considering uncer-
tainties,” IEEE Transactions on Smart Grid, pp. 1–1, 2019.

[38] Y. Ji, J. Wang, J. Xu, X. Fang, and H. Zhang, “Real-time energy man-
agement of a microgrid using deep reinforcement learning,” Energies,
vol. 12, no. 12, p. 2291, 2019.

[39] D. E. Olivares, C. A. Ca˜nizares, and M. Kazerani, “A centralized energy
management system for isolated microgrids,” IEEE Transactions on
smart grid, vol. 5, no. 4, pp. 1864–1875, 2014.

[40] T. A. Nguyen and M. L. Crow, “Stochastic optimization of renewable-
based microgrid operation incorporating battery operating cost,” IEEE
Transactions on Power Systems, vol. 31, no. 3, pp. 2289–2296, May
2016.

[41] A. Sakti, K. G. Gallagher, N. Sepulveda, C. Uckun, C. Vergara, F. J.
de Sisternes, D. W. Dees, and A. Botterud, “Enhanced representations
of lithium-ion batteries in power systems models and their effect on the
valuation of energy arbitrage applications,” Journal of Power Sources,
vol. 342, pp. 279 – 291, 2017.

[42] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction,

1998.

[43] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” in Thirtieth AAAI conference on artiﬁcial
intelligence, 2016.

[44] W. B. Powell, Approximate Dynamic Programming: Solving the curses

of dimensionality.

John Wiley & Sons, 2007, vol. 703.

[45] R. Donida Labati, A. Genovese, V. Piuri, F. Scotti, and G. Sforza, “A
decision support system for wind power production,” IEEE Transactions
on Systems, Man, and Cybernetics: Systems, pp. 1–15, 2018.

[46] J. Liu, W. Fang, X. Zhang, and C. Yang, “An improved photovoltaic
power forecasting model with the assistance of aerosol index data,” IEEE
Transactions on Sustainable Energy, vol. 6, no. 2, pp. 434–442, April
2015.

[47] K. Maciejowska and R. Weron, “Short- and mid-term forecasting of
baseload electricity prices in the u.k.: The impact of intra-day price
relationships and market fundamentals,” IEEE Transactions on Power
Systems, vol. 31, no. 2, pp. 994–1005, March 2016.

[48] E. E. Elattar, J. Goulermas, and Q. H. Wu, “Electric load forecasting
based on locally weighted support vector regression,” IEEE Transactions
on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
vol. 40, no. 4, pp. 438–447, July 2010.

[49] “Wind and pv power from open power system data.” [Online]. Available:
https://data.open-power-system-data.org/ninja pv wind proﬁles

[50] “Historical

load data

(2016).”

[Online]. Available: http://www.

eirgridgroup.com/

[51] “Ieee 69 bus radial distribution data.” [Online]. Available: https:
//github.com/MATPOWER/matpower/blob/master/data/case69.m.

