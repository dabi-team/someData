2
2
0
2

b
e
F
1
1

]

Y
S
.
s
s
e
e
[

2
v
1
7
7
3
0
.
2
0
2
2
:
v
i
X
r
a

Energy Management Based on Multi-Agent Deep
Reinforcement Learning for A Multi-Energy Industrial
Park⋆

Dafeng Zhua,b,c, Bo Yanga,b,c,∗, Yuxiang Liua,b,c, Zhaojian Wanga,b,c, Kai
Mad, Xinping Guana,b,c

aDepartment of Automation, Shanghai Jiao Tong University, Shanghai 200240, China
bKey Laboratory of System Control and Information Processing, Ministry of Education of
China, Shanghai 200240, China
cShanghai Engineering Research Center of Intelligent Control and Management, Shanghai
200240, China
dKey Laboratory of Industrial Computer Control Engineering of Hebei Province, Yanshan
University, Qinhuangdao 066004, China

Abstract

Owing to large industrial energy consumption, industrial production has brought

a huge burden to the grid in terms of renewable energy access and power sup-

ply. Due to the coupling of multiple energy sources and the uncertainty of re-

newable energy and demand, centralized methods require large calculation and

coordination overhead. Thus, this paper proposes a multi-energy management

framework achieved by decentralized execution and centralized training for an

industrial park. The energy management problem is formulated as a partially-

observable Markov decision process, which is intractable by dynamic program-

ming due to the lack of the prior knowledge of the underlying stochastic process.

The objective is to minimize long-term energy costs while ensuring the demand

of users. To solve this issue and improve the calculation speed, a novel multi-

agent deep reinforcement learning algorithm is proposed, which contains the

following key points: counterfactual baseline for facilitating contributing agents

to learn better policies, soft actor-critic for improving robustness and exploring

⋆This work was supported by the National Key Research and Development Program of
China (Grant No.2018YFB1702300), and in part by the NSF of China (Grants No. 61731012,
62103265, 62122065 and 92167205).

∗Corresponding author
Email address: bo.yang@sjtu.edu.cn (Bo Yang)

Preprint submitted to Journal of LATEX Templates

February 14, 2022

 
 
 
 
 
 
optimal solutions. A novel reward is designed by Lagrange multiplier method to

ensure the capacity constraints of energy storage. In addition, considering that

the increase in the number of agents leads to performance degradation due to

large observation spaces, an attention mechanism is introduced to enhance the

stability of policy and enable agents to focus on important energy-related infor-

mation, which improves the exploration eﬃciency of soft actor-critic. Numerical

results based on actual data verify the performance of the proposed algorithm

with high scalability, indicating that the industrial park can minimize energy

costs under diﬀerent demands.
Keywords: Multi-energy management, industrial park, multi-agent,

counterfactual baseline, soft actor-critic, attention mechanism

1. Introduction

With the expansion of industrial production scale, energy demands have

grown rapidly, which is the main driving force of industrial parks to solve the

severe problems of low eﬃciency and high cost. To solve these issues as well

as meet the multi-energy demands, energy hubs (EHs), including combined

heat and power (CHP) units, boilers and energy storages, are introduced for

multi-energy management in the industrial park. In EHs, multi-energy devices

can be used to reduce energy cost [1], optimize facility operation [2], and shift

supply/demand [3].

Many studies have been done on the multi-energy management of industrial

parks. Liu et al.

[4] establish a multi-energy framework based on Stackelberg

game for an industrial park and consider bi-directional energy demand conver-

sion to achieve peak load transfer. Wei et al. [5] propose a locational marginal

price for multi-energy industrial parks to enhance the economic gains and dis-

tribute the electricity bill. Liu et al. [6] evaluate the beneﬁts of eco-industrial

development and demonstrate the feasibility of an accounting-based approach

in an industrial park. Shahidehpour et al. [7] propose a data-driven method to

deal with the challenges of the optimal energy planning for an industrial park.

2

However, these studies mainly concentrate on the optimization of the system to

obtain solutions for energy scheduling, which usually leads to diﬃculties in con-

vergence, and additional coordination overhead of energy devices is essential. To

meet the real-time demand in the multi-energy coupling network, several chal-

lenges are involved: (1) Considering time-varying electricity price and stochastic

demand, energy must be scheduled from the perspective of long-term metric to

maximize the long-term utility. (2) Coordination overhead of energy devices

needs to be reduced to a minimum. Therefore, a self-coordinating method in a

multi-energy industrial park needs to be developed without the interaction of

multiple energy devices.

For long-term optimization, the dynamic programming is used to optimize

the long-term energy cost of microgrids [8], which is improper for multi-energy

storage systems with high uncertainty [9]. In the past few years, with the rapid

development of artiﬁcial intelligence (AI), deep reinforcement learning (DRL)

has become the focus of attention because it successfully solves the challenging

sequential decision-making problem in energy systems [10]. DRL combines the

information perception of deep learning (DL) and the decision making of rein-

forcement learning (RL) [11]. Xu et al. [12] use RL based diﬀerential evolution

to determine the associated parameters and optimal strategy in an industrial

energy system. Ebell et al. [13] present an algorithm based on RL to control res-

idential power ﬂows with a battery system and a photovoltaic system. Mocanu

et al. [14] explore DRL to perform optimization schedules for energy manage-

ment in buildings. Ye et al. [15] propose a novel real-time energy management

strategy for multi-energy systems based on DRL method to minimize the en-

ergy costs of end-users. Although DRL has been successfully applied in energy

management [16], it is still a challenge to solve the multi-energy management

problem in an industrial park. Most existing DRL-based studies only take into

account one energy carrier, few utilize DRL to solve multi-energy management

problem. Although few studies consider multi-energy scheduling, most of these

studies [17, 18, 19] use single-agent DRL to manage the system energy con-

sumption where management is carried out in a centralized framework, which

3

is intractable under the condition of a large number of variables.

To reduce computational complexity and improve robustness, multi-agent

reinforcement learning (MARL) is introduced where energy devices [20] or pro-

duction resourses [21] regarded as multiple agents can perceive the environment

and independently adjust their energy policies to achieve the optimal perfor-

mance [22]. Diﬀerent constraints and diversity of energy are the main factors

for multiple agents to make decisions in the energy management problem. Ad-

ditionally, the cooperation of multiple agents can improve the calculation speed

and lead to more reliable solutions [23]. However, since the environment sensed

by agents may become nonstationary due to changing polices of other agents,

only assuming that the agents are isomorphic and selecting actions related to

the maximum Q-value may cause performance degradation. In response to this

phenomenon, we use diﬀerent input dimensions of the actor and critic networks

to extend the actor-critic framework, and deploy them on energy device and en-

ergy management center to deal with nonstationary problems. The critics that

approximate Q-value functions take global actions and observations as inputs,

which implies the property of stationary, while energy devices make decisions

based on the local observations and policies. The distributed policies of all

energy devices are learned through the interaction with the environment.

In this paper, we consider energy scheduling in an industrial park, where

multi-energy devices, including energy generation, storage and conversion de-

vices, provide energy to users. If each energy device aims at its own performance

objectives under given local information, it may cause poor reward due to inter-

ference of other energy devices. Therefore, to reduce the interference and ensure

the performance of each energy device, optimization metric is considered from

the perspective of energy management center. In order to obtain the optimal

performance based on the distributed policies of each energy device, a new policy

formulation method should be designed. The stochastic renewable energy and

multi-energy demand have a joint impact on system performance. Therefore,

energy should be scheduled from the perspective of whole and long-term met-

ric instead of individual and short-term performance. Diﬀerent from existing

4

research [24], the energy management problem is constructed as a partially-

observable Markov decision process, which aims to develop distributed joint

energy scheduling for each energy device to obtain whole performance met-

ric. Rather than explicitly solving the problem in a time slot, the policies are

learned from previous experience to minimize the long-term cost and ensure the

demand of users, which implicitly solves the coupling eﬀect and makes full use

of the underlying statistical characteristics of stochastic demand and renewable

energy.

In order to achieve optimal energy scheduling, the synergy among energy

source, storage and load is essential, which requires the full cooperation of mul-

tiple energy devices. Therefore, we aim in proposing a multi-agent deep rein-

forcement learning (MADRL) algorithm based on the counterfactual baseline

and soft actor-critic (SAC), which can not only learn the implicit multi-energy

devices relationship, but also combine credit assignment and attention mecha-

nism to encourage contributing agents to learn the important information and

improve their learning eﬃciency. There are some studies on applying MADRL

to solve the multi-device scheduling problems in complex cooperative scenar-

ios [25]. MADRL is characterized by obtaining eﬃcient and reliable solutions

without establishing complex models, especially in the cooperative setting of

interdependent, sequential and correlated industrial production. Diﬀerent from

other MADRL [26, 27], the proposed algorithm not only improves the stability

of policies by integrating observable information into its own action value func-

tion estimation according to the importance of the information, but also has

better scalability as the number of agents increases. The main contributions of

this paper are summarized as follows.

• To solve the nonstationary problem and reduce coordination overhead and

interference of energy devices, an multi-energy management framework

(MEMF) achieved by centralized training at the industrial energy man-

agement center and decentralized execution at each energy device side is

proposed, where the center assists in formulating joint energy scheduling

5

policies for each energy device.

• To reduce the calculation complexity and improve the calculation speed, a

MADRL energy scheduling algorithm based on the counterfactual baseline

and SAC in an industrial park is proposed, which encourages agents to

learn important information by allocating reward and exploring all possi-

ble optimal paths to schedule energy devices and minimize the long-term

energy cost. In addition, a novel reward is designed by Lagrange multiplier

method to ensure the capacity constraints of energy storage.

• To avoid the ineﬃciency caused by the non-selective use of all information,

and improve the stability of the policy and the eﬃciency of cooperation

among energy devices/agents, an attention mechanism is introduced to

enable agents to focus on important energy-related information, such as

time-varying energy demand and price, which improves the exploration

eﬃciency of SAC, instead of learning all the information in the industrial

environment.

The remainder of this work is organized as follows. In Section 2, the system

model of an industrial park is presented. In Section 3, a MADRL algorithm

based on the attention mechanism is adopted.

In Section 4, the numerical

results based on actual data are shown. The paper is concluded and future

research is given in Section 5.

2. System Model

2.1. Industrial Park

We consider a system including electricity and gas utility companies and an

industrial park with three types of energy: electricity, heat and gas. The park

consists of users, photovoltaic panels and EHs, including CHP units, boilers,

water tanks and batteries, as shown in Fig. 1. The park can harvest renewable

energy generated by photovoltaic panels, and generate heat and electricity with

ﬁxed ratios by CHP units. Meanwhile, the park can store extra energy by

6

Table 1: Nomenclature

Interpretation
time period, t ∈ {1, 2, ..., T }
EH, k ∈ {1, 2, ..., K}
industrial user, i ∈ {1, 2, ..., I}
agent, j ∈ {1, 2, ..., N }
electricity of battery k
thermal energy of hot water tank k
electricity charged into battery k
thermal energy charged into hot water tank k
electricity discharged from battery k
thermal energy discharged from hot water tank k
electricity generation of CHP unit k

Symbol
t
k
i
j
Bk(t)
Wk(t)
Cke(t)
Ckh(t)
Dke(t)
Dkh(t)
EkCHP (t)
HkCHP (t) heat generation of CHP unit k
GkCHP (t) gas consumption of CHP unit k
Hkb(t)
Gkb(t)
E(t)
G(t)
Eo(t)
Xtot(t)
R(t)
Xi(t)
pe(t)
pg(t)
po(t)
π
rπ(t)
γ
W π
γ
aB
aCHP
Q
L(φQ)
yj
J(πθ)
ρ
b(s)
zj
Aj(s, a)

heat generation of boiler k
gas consumption of boiler k
electricity purchased from the electricity company
gas purchased from the gas company
electricity sold back to the electricity company
total available energy, X ∈ X = {E, H, G}
renewable energy generation
energy demand of user i
electricity price
gas price
electricity price sold back to the electricity company
policy of energy scheduling
reward of the industrial park
discounted factor
discounted function
action of battery agent
action of CHP agent
action value function
regression loss of Q-network
target function
cumulative rewards
parameter which balances maximum entropy and rewards
baseline of Q-value function
a weighted sum of contribution from other agents
advantage function

7

Natural gas plant

EH 1

EH K

Water tank

CHP unit

Photovoltaic 
panels

Boiler

Battery

Users

Power grid

Natural gas

Heat

Electricity

Fig. 1. Energy ﬂows of the industrial park

batteries and water tanks for the demand in the future. The energy management

center takes charge of the energy market of the industrial park, the operation of

EHs, and the energy trading with the gas and electricity utility companies. The

industrial park has K EHs. The energy devices are modeled for EH k in the

next section. For ease of reference, the nomenclature is summarized in Table 1.

2.2. Energy Hub

The model of EH k, including a battery, a hot water tank, a CHP unit and

a boiler, is denoted as:

Bk(t + 1) = Bk(t) + ηckeCke(t) −

Wk(t + 1) = Wk(t) + ηckhCkh(t) −

0 ≤ Bk(t) ≤ Bk,max

1
ηdke

Dke(t)

1
ηdkh

Dkh(t)

(1)

(2)

(3)

8

0 ≤ Wk(t) ≤ Wk,max

0 ≤ Cke(t) ≤ Cke,max, 0 ≤ Dke(t) ≤ Dke,max

0 ≤ Ckh(t) ≤ Ckh,max, 0 ≤ Dkh(t) ≤ Dkh,max

EkCHP (t) = ηkpgGkCHP (t)

HkCHP (t) = ηkhgGkCHP (t)

0 ≤ EkCHP (t) ≤ EkCHP,max

0 ≤ HkCHP (t) ≤ HkCHP,max

Hkb(t) = ηkbgGkb(t)

(4)

(5)

(6)

(7)

(8)

(9)

0 ≤ Hkb(t) ≤ Hkb,max

(10)

where (1)-(6) denote the model of the electricity and heat storages. The amount

Bk(t + 1) of electricity storage at time slot t + 1 is equal to the amount Bk(t)

at time slot t, plus the amount ηckeCke(t) of charging and minus the amount
Dke(t)
ηdke

of discharging. The charging source is the low-price utility power and the

remaining renewable energy after serving demand, and its amount is determined

by the proposed algorithm. The amount Wk(t + 1) of equivalent thermal energy

storage is equal to the amount Wk(t), plus the amount ηckhCkh(t) of charging
and minus the amount Dkh(t)
ηdkh

of discharging. (7) and (8) denote that CHP unit k

generates electricity EkCHP (t) and heat HkCHP (t) by consuming gas GkCHP (t)

simultaneously, and the eﬃciencies are ηkpg and ηkhg, respectively. (9) and (10)

denote that boiler k generates heat Hkb(t) by consuming gas Gkb(t), and the

eﬃciency is ηkbg.

2.3. Energy Trading with Utility Companies

The industrial park purchases electricity E(t) at price pe(t) and natural gas

G(t) at price pg(t) from the electricity and gas utility companies, respectively.

In addition, the industrial park can sell electricity Eo(t) at price po(t) back

to the electricity company when the electricity is surplus. The constraints of

9

energy trading with utility companies are:

0 ≤ E(t) ≤ Emax

0 ≤ G(t) ≤ Gmax

0 ≤ Eo(t) ≤ Eo,max

(11)

2.4. Energy Balance

The total available energy for industrial users depends on the energy ﬂows

of EHs and utility companies.

K

Etot(t) =

[EkCHP (t) + Dke(t) − Cke(t)]

X
k=1

+ R(t) + E(t) − Eo(t)

Gtot(t) = G(t) −

K

K

X
k=1

[GkCHP (t) + Gkb(t)]

Htot(t) =

[HkCHP (t) + Hkb(t) + Dkh(t) − Ckh(t)]

X
k=1

(12)

where R(t) is the renewable energy generation. Etot(t), Gtot(t) and Htot(t) de-

note the total available electricity, gas and heat, respectively. The total available

energy domain Xtot(t) can be denoted as:

I

X
i=1

Xi(t) ≤ Xtot(t)

(13)

where Xi(t) is the energy demand of user i for energy X ∈ X at time slot t,
and X = {E, G, H} denotes the set of electricity, gas and heat.

2.5. Optimization Problem

In the industrial park, the reward rπ(t) = Eo(t)po(t)−E(t)pe(t)−G(t)pg(t)+

b1 − b2|Xtot(t) −

I
i=1 X ∗

i (t)| consists of the revenue of electricity sold to the

P

electricity utility company, the payment of purchasing electricity and gas from

10

the utility companies, and the utility of matching the supply and target demand

X ∗

i (t). b1 and b2 are the utility coeﬃcients.

According to the reward of the industrial park, the optimization problem of

the energy scheduling is to ﬁnd a policy π to maximize the time average energy

utility r:

max
M (t)

rπ = max
M (t)

lim
T →∞

1
T

T

X
t=1

rπ(t)

s.t. (1) − (13)

(14)

where M (t)={Dke(t), Cke(t), Dkh(t), Ckh(t), Eo(t), E(t), G(t), Xtot(t)}. The

problem above is an average Markov decision process (MDP) problem over a

long-term horizon, which is usually solved by dynamic programming (DP). How-

ever, DP can obtain the optimal strategy under the condition that the environ-

mental information is available, which is unrealistic, especially with a long-term

horizon. Additionally, the use of DP will cause a lot of computational overhead.

Thus, a novel method independent of the environmental information needs to

be developed.

The lack of the transition probability models of demand and state infor-

mation causes the invalidation of DP. RL is an eﬀective approach, which can

achieve the eﬀect of DP with incomplete information of the environment and

less calculation [28]. Thus, RL is used to solve the above problem.

In the long-term MDP, the discounted function provides a reliable way for

evaluating policies. The discounted function W π

γ is set as the cumulative sum

of discounted rewards with regard to a discounted factor γ, which is denote as

W π

γ = lim
T →∞

E[

T

X
t=1

γtrπ(t)]

(15)

π∗ is regarded as the optimal policy of discounted function, which means
γ ≥ W π

γ for all actions and states. Based on Laurent series expansion [29],

∗

W π

11

the discounted function W π

γ is expanded as

W π

γ =

rπ
1 − γ

+ W π + wπ(γ)

(16)

where W π = limT →∞ E[
P
to policy π, and wπ(γ) converges to 0 as γ → 1.

T

t=1(rπ(t)− rπ)] denotes the bias function with regard

(1 − γ)(W π

γ − W π
γ )

∗

lim
γ→1

= lim
γ→1

(rπ

∗

− rπ)

+ lim
γ→1

(1 − γ)(W π

= lim
γ→1

(rπ

∗

− rπ)

∗

∗

+ wπ

(γ) − W π − wπ(γ))

(17)

Since 0 < γ < 1 and W π

γ ≥ W π

γ , rπ

∗

∗

≥ rπ for γ → 1, which means that the

policy π∗ is optimal policy of reward when π∗ is optimal policy of discounted

function for γ → 1.

Therefore, if γ → 1, the problem (14) of reward maximization is converted

into the problem of discounted function maximization

max
M (t)

W π
γ

s.t. (1) − (13)

(18)

The maximization problem of discounted function can generally be solved

by RL algorithm. It is straightforward to solve the long-term average reward

problems in a centralized method. However, the centralized method may cause

a huge overhead due to the presence of many variables. To avoid the overhead,

a joint policy for each energy device needs to be executed without coordination

overhead so that each energy device makes decision independently based on the

local observations.

12

3. MADRL Algortihm

Various energy devices, including CHP units, boilers, batteries, water tanks

and photovoltaic panels, play a key role in adjusting the level and type of energy

consumption and supply, which may reduce energy cost, and improve energy ef-

ﬁciency and reliability. Due to multi-energy coupling, random renewable energy

and demand, the energy management problem is diﬃcult to deal with by tradi-

tional methods like DP, which require a priori information of random processes

and have high computational complexity. Therefore, we adopt RL algorithm

which can achieve the eﬀect of DP without corresponding conditions and less

calculation. In addition, the cooperation of multiple agents can improve the cal-

culation speed and lead to more reliable solutions. To schedule energy devices

eﬃciently, a novel MADRL algorithm is adopted where the energy devices are

regarded as the agents.

3.1. Partially-Observable Markov Decision Process

The interaction of multiple agents is constructed as a partially-observable

Markov decision process (POMDP). The POMDP is deﬁned by a 5-tuple (S, A, Ts,

R, π),

i.e., a state set S, where observation sets, O1, O2,

..., ON , denote

partial observable information about the states, such as time and electricity

price; action sets of N agents, A1, A2, ..., AN ; a state transition function
Ts : S × A1 × ... × AN → S′, which denotes the probability distribution of the

next state; a reward set of agents R; and policy sets π1, π2, ..., πN , which map

observations to actions. Both transition and reward functions depend on actions

and states of each agent. In the industrial park, multiple energy devices/agents

interact with the industrial environment and learn the optimal policies to max-

imize the cumulative energy utility. Each agent obtains local information and

takes actions at time slot t, and then the multi-agent system transfers from the

current state to the next state and allocates corresponding rewards to agents.

Details of state, action and reward for each agent are as follows:

13

• State: In the industrial park, each energy device/agent has its own obser-

vation, including its energy consumption and generation. In addition to

observation, the state also contains the time and energy price.

• Action: For battery agent, its task is to control the charging or discharging

state of the battery, which should comply with the battery restrictions

mentioned above. The actions of battery agent consist of three types:

discharging, idle and charging, which are between -1 and 1. The actions

are equally divided into 20 steps as follows:

1, ..., 0.3, 0.2, 0.1

Charging

0

Idle

(19)

−0.1, −0.2, ..., −1 Discharging

aB =





where the action aj = 1 means energy device j is working at full capacity,

and aj = 0 means energy device j is idle, and negative and positive values

denote discharging and charging, respectively. The water tank agent has

the same action sets as the battery agent. The actions of CHP agent

are between 0 and 1, which are equally divided into 10 steps aCHP =

0, 0.1, 0.2, ..., 1. The boiler agent has the same action sets as the CHP

agent.

• Reward: According to the reward function given in Section 2.5, the reward

of the industrial park is expressed as r(t) = Eo(t)po(t) − E(t)pe(t) −
I
i=1 X ∗
device/agent is assumed to receive a same reward rj(t) = r(t). Therefore,

G(t)pg(t) + b1 − b2|Xtot(t) −

In this paper, each energy

i (t)|.

P

all agents will aim to maximize the common reward, which can be achieved

by the proposed algorithm in the following section.

Although the constraints of energy charging/discharging of batteries (5) and

water tanks (6), heat and electricity generation of CHPs (8) and heat generation

(10) of boilers are satisﬁed by setting the maximum action value of agents,

traditional RL cannot guarantee the capacity constraints of batteries (3) and

14

water tanks (4). To guide the policy toward a solution of constraint satisfying,

we add a penalty term to the reward function of the battery agent k. The

reward is revised as

rBk

(t) = rBk (t) − λBk (t)(Bk(t) − Bk,max)

(20)

The penalized Q function is denoted as

QBk

(st, at) = E[

∞

X
t′=t

′

γt

−trBk

(t′ + 1)]

= E[

∞

X
t′=t+1

′

γt

−t−1(rBk (t′) − λBk (t′)(Bk(t′) − Bk,max))]

(21)

= QBk (st, at) −

∞

X
t′=t+1

′

γt

−t−1λBk (t′)E[Bk(t′) − Bk,max]

The corresponding Lagrange multiplier can be updated as

λBk (t + 1) = clip(λBk (t) + ζBk (Bk(t) − Bk,max), 0, 1)

(22)

clip(x, 0, 1) =

where ζBk > 0 is the updating rate.

if x < 0

if 0 ≤ x ≤ 1

if x > 1

0,

x,

1,





When Bk(t) − Bk,max > 0, the reward function rBk

< rBk . When Bk(t) −

Bk,max ≤ 0, the reward function rBk
obtain larger reward, it will satisfy the battery capacity constraint. In addition,

≥ rBk . Since the battery agent aims to

the existence of positive and negative rewards can improve the learning eﬃciency

of the agent. When λBk (t) → 0, the learned policy can guarantee the capacity

constraint of the battery, and QBk
tank agent adopts the same method as the battery agent to satisfy the capacity

(st, at) converges to QBk (st, at). The water

constraint of the water tank.

15

3.2. DRL Framework

The existence of multi-energy coupling indicates that the action taken by one

energy device will aﬀect the performance of other energy devices. As a result,

the environments observed partially by agents become nonstationary. However,

the traditional reinforcement learning (RL) is generally suited in a stationary

environment. Thus, the policy cannot be learned completely independently. To

solve this issue, one way is to acquire all actions and states of agents, but it will

cause a huge overhead. Therefore, both completely decentralized and centralized

methods are impractical. To balance the performance and overhead, a SAC

model in discrete domain [30, 31], which can improve exploration ability and

robustness, is introduced to achieve centralized training by the critic network

and decentralized execution by the actor network.

During the centralized training, the interaction among the agents and the

energy management center is carried out. The center obtains the states, actions,

rewards and other information from each agent and learns the implicit relation-

ship of multi-agent. The gradients of Q about the joint actions are calculated

on the center and sent to the agents. During the decentralized execution, each

agent only needs local information and gradients to update the policy π, and

then acts respectively according to the established policies.

In this way, the

proposed algorithm can reduce the complexity without coordinating diﬀerent

agents. In order to ensure the convergence of the results, the experience replay

is adopted, which uses a small size of random sample to obtain the gradients

and the loss of the value Q.

3.3. Centralized Training

The action value function Qφ(st, at) = E[

′

∞
t′=t γt

−trt′ (st′ , at′ )] denotes the

P

cumulative rewards after each agent takes actions. Therefore, Q obtains the

implicit relationship among agents, i.e., the interaction among energy devices in

the industrial park. The energy management center receives transition function

Ts from all agents, and stores these information in the replay buﬀer D. For

simplicity, the time subscript t of variables is removed. The regression loss of

16

Q-network is denoted by

L(φQ) =

N

X
j=1

ED[Qφj (s, a) − yj]2

yj = rj + γEπ[Qφ′

j

(s′, a′)]

(23)

where yj is the target function. γ is the discount factor which balances the long-

term reward and immediate reward, and s′ and a′ denote the next observation

state and action, respectively.

In the policy gradient method [32], agent j selects a policy to maximize its
expected cumulative rewards J(πθ) = ED,π[Qφ(s, a)], and its gradient is denoted

as

∇θj J(πθ) = ED,π[∇θj log(πθj (aj|sj))Qφj (s, a)]

(24)

To explore more useful information and be more robust, the maximum en-

tropy [33] is introduced to adjust the policy gradient method:

∇θj J(πθ) = ED,π[∇θj log(πθj (aj|sj))(−ρ log(πθj (aj|sj))

(25)

+ Qφj (s, a) − b(s))]

where ρ is the parameter which balances maximum entropy and rewards, and

b(s) is a baseline of the Q-value function, which will be introduced in detail

below. Meanwhile, the target function is also adjusted as

yj = rj + γEπ[Qφ′

j

(s′, a′) − ρ log(πθ′

j

(a′

j|s′

j)]

(26)

To reduce the diﬃculty and computational complexity of selecting a baseline,

the current action value function is used to solve the marginal distribution of

the current policies, which will be elaborated in the following section.

3.4. Attention Mechanism

The attention mechanism is introduced to improve the learning eﬃciency

of the agents and the stability of the policies. The key point of the attention

17

unshared

shared

Attention 
Head

MLP

scalar product

scalar product

MLP

MLP

Fig. 2. The calculation process of Qφj (s, a) for agent j based on attention mechanism

mechanism is that each agent can selectively focus on information that is more

conducive to obtaining greater rewards when learning the critic. It is suitable

for centralized training and decentralized execution, as shown in Fig. 2.

The critic receives all actions a = (a1, ..., aN ) and all states s = (s1, ..., sN )

of agents to calculate Qφj (s, a) for agent j. Qφj (s, a) is a function about the

state and action of agent j, and the contribution zj:

Qφj (s, a) = gj(hj(sj, aj), zj)

(27)

where gj and hj denote the multi-layer perceptron (MLP) function. zj denotes

a weighted sum of contribution from other agents:

zj =

X
l6=j

ρlFl =

X
l6=j

ρlf (Tshl(sl, al))

(28)

where Fl denotes a one-layer embedding function of agent l transformed by

shared matrix Ts, and f denotes a leaky rectiﬁed linear unit (ReLU). The at-

tention mechanism can be regarded as a key-value model where agents query

the information to estimate the value function [34]. As the name implies, the

18

essence of the attention mechanism is to screen the input information to retain

valuable information and ﬁlter out unimportant information. In mathematical

language, its expression is the attention weight, which is multiplied by the input

vector to get the ﬁltered information. In the attention mechanism, the target to

be ﬁltered is the encoder output, and the input source for generating the atten-

tion weight is the hidden layer output from the decoder. In general, a bilinear

mapping is used to calculate attention weight. And the bilinear matrix is the

parameter matrix of the fully connected layer that needs to be learned. In order

to reduce the rank of the bilinear matrix, the product of the low-rank matrices

Uk and Vq is used to obtain attention weight ρl by fusing ej = hj(sj, aj) and el

[35]:

ρl = pl exp(eT

l U T

k Vqej)

(29)

where pl is the linear coeﬃcient; ej and el are transformed to a query value

and a key value, respectively. The match between query value and key value is

adjusted according to the ranks of the Vq and Uk to prevent the gradient from

disappearing [36].

In the experimental design, several attention heads are used. Each head

uses a set of independent parameters (Uk, Vq, Ts), which generate the contri-

bution of other agents to agent j, and contributions from all attention heads

are connected as a vector. It is worth mentioning that each head can focus on

the weighted contribution of other agents from a diﬀerent perspective. In addi-

tion, the weight’s feature extractors, keys and values are shared among multiple

agents, so that the features of all agents are in the same space after network pro-

cessing and transformation. It is feasible to share parameters between diﬀerent

agents no matter in a cooperative environment or a competitive environment,

because the approximation of the action value function in a multi-agent sys-

tem is essentially a multi-objective regression problem. This parameter sharing

mechanism allows each agent to learn eﬀectively in complex environments. Just

by adding more encoders during training, this approach can be extended to in-

clude other information, such as global state, rather than just local actions and

19

observations.

Owing to the parameter sharing, critics are updated together to minimize

the joint loss function:

φQ ← φQ − α∇L(φQ)

L(φQ) =

N

X
j=1

ED[Qφj (s, a) − yj]2

yj = rj + γEπ[Qφ′

j

(s′, a′) − ρ log(πθ′

j

(a′

j|s′

j))]

(30)

where α denotes the update stepsize, and φ′ and θ′ denote the network pa-

rameters of the target critics and actors, respectively. Qφ′

j

is the action-value

estimate of agent j based on states and actions of all agents.

After introducing attention mechanism, the actors are updated as:

θj ← θj + β∇θj J(πθ)

∇θj J(πθ) = ED,π[∇θj log(πθj (aj|sj))(Qφj (s, a)

(31)

− ρ log(πθj (aj|sj)) − b(s, aK))]

where K represents the set of all other agents except j. To evaluate the contribu-

tion of speciﬁc action, an advantage function which uses a baseline is introduced.

The baseline is obtained by solving the marginal distribution problem of the cur-

rent agent’s strategy, which can avoid designing additional default actions and

reduce simulation calculations. In addition, the advantage function can solve the

credit allocation problem in the multi-agent environment, i.e., assign rewards

to encourage those agents that are more helpful to the entire multi-agent task,

and then promote them to learn excellent strategies. The advantage function is

expressed as:

Aj(s, a) = Qφj (s, a) − b(s, aK)

b(s, aK) = Eπ[Qφj (s, (aj , aK))]

(32)

Diﬀerent from general advantage function which requires a global reward

and same action space, the proposed advantage function based on the attention

20

mechanism can achieve a more ﬂexible and general form of baseline without

these requirements. This makes the weighted sum of encodings zj for other

agents and the decomposition of encoding ej for agent j more simple. For

discrete policies, the expected return Qj(s, (aj, aK)) for each possible action

that agent j might take can be used to calculate the baseline. Its expectation

is denoted by:

Eπ[Qφj (s, (aj, aK))] =

X
a′
j ∈Aj

π(a′

j |sj)Qj(s, (a′

j, aK))

(33)

where aj needs to be removed from Qj, and a Qj value is output for each

action. The observation encoding ej = hoj (oj ) is added for each agent to replace

ej = hj(sj, aj) mentioned above, and function g is adjusted to output values

for all possible actions instead of outputting a value for the input action. For

continuous policies, the above expected return can be estimated by learning a

value head or sampling from the policy of agent j. The implementation process

of MADRL algorithm is shown in Algorithm 1.

Algorithm 1 : MADRL Algorithm based on Counterfactual Baseline and SAC
1: Initialize critic and actor networks, the target network parameters of each agent, and

replay buﬀer D.

(s′, a′) − ρ log(πθ′

j))]

j

j |s′
(a′
joint

the

loss

function:

L(φQ) =

using
ED,π[∇θj log(πθj (aj |sj))(Qφj (s, a) − ρ log(πθj (aj |sj)) − b(s, aK ))]

gradient:

policy

∇θj J(πθ)

=

Initialize observation states s
for t = 1, ..., T do

2: for episode=1, ..., E do
3:
4:
5:
6:
7:
8:
9:
10:
11:

Select and execute actions a
Acquire rewards r and observation states s′
Calculate Qφ(s, a) according to Fig. 2.
Store transitions in replay buﬀer D
for agent j do

Sample random samples from D
Set yj = rj + γEπ [Qφ′
Update
j=1 ED[Qφj (s, a) − yj ]2
actor
Update

j
critic by minimizing

the

PN

12:

13:

end for

14:
15: end for
16: end for

21

Table 2: Learning Parameters

Parameters
Discount factor (γ)
Balance parameter (ρ)
Number of attention heads
Size of replay buﬀer D
Size of random samples

Values
0.95
0.01
4
1000
32

4. Simulation

In this section, the simulation based on the real data is conducted to evalu-

ate the performance of the proposed algorithm using PyTorch-Gym framework,

which combines the PyTorch tensor library with the OpenAI Gym architecture.

4.1. Setup

For simplicity, we consider an industrial park consisting of one EH and three

factories. Each EH has a CHP unit, a water tank, a battery and a boiler.

Therefore, the number of agents is N = 4. Then, the scalability of the proposed

algorithm will be veriﬁed by increasing the number of agents/EHs. For diﬀerent

EHs, the coeﬃcients of same kinds of energy devices are set to be same. The pa-

rameters of eﬃciency are ηcke = ηdke = ηckh = ηdkh = 98%, ηkpg = ηkhg = 35%,

ηkbg = 80%, respectively. The reward coeﬃcients are b1 = 20, b2 = 2. Other pa-

rameters are summarized as follows: pg(t) = 0.3 yuan/kWh, Bk,max = 4MWh,

Cke,max = Dke,max = 1MWh, Wk,max = 4MWh, Ckh,max = Dkh,max = 1MWh.

The price provided by the State Grid Jiangsu Electric Power Co., Ltd [37] is

shown in Fig. 3(a). The target electricity load provided by PJM [38] is given in

Fig. 3(b). The data of photovoltaic systems provided by Renewables.ninja [39]

is shown in Fig. 9(a).

For the training of all experiments, the discount factor γ is set as 0.95.

The parameter ρ which balances maximum entropy and rewards is 0.01, and 4

attention heads are used in the attention mechanism. In addition, we use 32

samples for the minibatch of random sample from replay buﬀer D, and the size

of D is 1000. The learning parameters of simulation are shown in Table 2.

22

1

0.5

)
h
W
k
/
¥
(
e
c
i
r

P

0

0

)
h
W
M

(

d
n
a
m
e
D

2.2
2
1.8
1.6
1.4
1.2

0

Factory 1
Factory 2
Factory 3

5

10

15

Time (hour)

20

25

(a) Electricity price

5

10

15

20

25

Time (hour)

(b) Electricity demand

Fig. 3. Energy data.

Table 3: Diﬀerent Methods

Methods
Base
Interaction
DDPG
DDPG Observation
MADDPG
DDPG Observation
SAC
Restricted Version
Proposed Algorithm SAC

Restricted Attention
Attention

To verify the eﬀectiveness of the proposed algorithm, several methods are

used to compare as follows: deep deterministic policy gradient algorithm (DDPG)

[40] and multi-agent deep deterministic policy gradient algorithm (MADDPG)

[41]. A restricted version of the proposed algorithm is also used for comparison,

and the attention weight of the version is ﬁxed to be 1/(N −1), which causes the

model fail to focus on speciﬁc agents. All methods above are carried out with

the same condition. The parameters of each algorithm are adjusted according to

performance, and they remain unchanged for that algorithm. The comparison

of diﬀerent methods is shown in Table 3.

4.2. Performance Veriﬁcation

To verify the performance of the reward designed by Lagrange mechanism,

the comparison under conventional reward mechanism and Lagrange reward

mechanism is given. During training, the battery charging often exceeds the

23

 
 
105

4

3

2

1

s
d
r
a
w
e
R

0

0

500

1000

1500
Episodes

Lagrange Mechanism
Conventional Mechanism

2000

2500

3000

Fig. 4. Total rewards under revised reward and traditional reward.

capacity limit under the conventional reward mechanism.

In order to obtain

the total rewards under the conventional mechanism, we give a great penalty

to prevent the electricity of batteries from exceeding the capacity limit. The

comparison of total rewards under the conventional mechanism and Lagrange

mechanism is given in Fig. 4. The total rewards under the Lagrange mechanism

is higher than the one under the conventional mechanism, and the learning eﬃ-

ciency under the Lagrange mechanism is better than the one under the conven-

tional mechanism. Therefore, the Lagrange reward mechanism can improve the

learning eﬃciency and ensure that the battery capacity meets the constraints.

In order to verify the performance of the proposed algorithm, assuming that

all information is known, the optimal solution can be directly calculated by ex-

haustive method (EM). Fig. 5 shows the costs across 24 time slots implemented

by diﬀerent methods in the industrial environment, and Table 4 shows the total

cost implemented by diﬀerent methods. According to the ﬁgure and table, the

cost of proposed algorithm is lower than the costs of other methods. The opti-

mal solution has the same trend with proposed algorithm. The total cost can

continuously approach the theoretical optimal cost after continuous iteration.

However, since each agent in proposed algorithm cannot know global data, the

total cost is about 3.7% diﬀerent from the theoretical optimal cost.

Since DDPG easily causes overestimation of Q and can not explore state

action space well, the total cost under DDPG is high. MADDPG transfers the

state transition information of all agents to the critic of each agent, and the ob-

servation spaces in the environment are relatively large for agents, so the total

24

8000

6000

4000

2000

)
¥
(

t
s
o
C

0

0

DDPG MADDPG Restricted Version Proposed Algorithm EM

5

10
Time (hour)

15

20

Fig. 5. Costs across 24 time slots under diﬀerent methods.

Table 4: Total cost implemented by diﬀerent methods

Total cost (¥)
Methods
90240
DDPG
86908
MADDPG
Restricted Version
83138
Proposed Algorithm 74133
71493
EM

cost under MADDPG is high. Due to the lack of the attention mechanism, the

restricted version also causes a high cost. Fig. 5 denotes that the proposed algo-

rithm achieves lower costs in most cases. At the low-price time, the costs under

the proposed algorithm may be higher because the park purchases low-price

electricity to charge into battery instead of consuming electricity discharged by

battery. When the electricity price is high, it can discharge the battery instead

of purchasing much high-price electricity. Thus, the total cost of the proposed

algorithm is lower. It is worth noting that the cost is high and ﬂuctuates greatly

under DDPG at high-price time, which causes the cost of DDPG to be lower

than the cost of the proposed algorithm at 19:00.

Fig. 6 shows the convergence situation implemented by diﬀerent methods.

The proposed algorithm achieves faster convergence than the restricted version

owing to the attention mechanism. The proposed algorithm converges in about

1600 iterations, while the restricted version converges in about 2300 iterations.

Although DDPG achieves fast convergence, its reward is lower. The reward of

the proposed algorithm is greater than those of other methods.

In addition,

considering the stochastic of the demand, the comparison of the rewards under

25

 
105

4

2

0

s
d
r
a
w
e
R

-2

0

DDPG MADDPG Restricted Version Proposed Algorithm

500

1000

1500
Episodes

2000

2500

3000

Fig. 6. Rewards implemented by diﬀerent methods.

105

s
d
r
a
w
e
R

5
4
3
2
1
0

DDPG MADDPG Restricted Version Proposed Algorithm

1

2
3
Demand Index

4

(a) Comparison of the rewards under diﬀerent demands

Demand 1
Demand 2
Demand 3
Demand 4

)
h
W
M

(

d
n
a
m
e
D

7

6

5

4

3

0

5

10

15

Time (hour)

20

25

(b) Diﬀerent demands

Fig. 7. Rewards implemented by diﬀerent methods under diﬀerent demands.

diﬀerent demands is given in Fig. 7(a), which veriﬁes the performance of the

proposed algorithm when the demand changes in Fig. 7(b).

Then, Fig. 8 shows a comparison of the rewards obtained by the proposed

algorithm and other methods as the number of agents increases. There is the

same trend when the reward coeﬃcients change. When the number of agents

increases, it means that more energy can be scheduled, so the demand of users

will also increase proportionally to make full use of these energy devices. Al-

though MADDPG performs well on the version of 4 agents, the performance

cannot be maintained as agents are added. The rewards of the proposed algo-

rithm remain stable as the number of agents increases. Table 5 shows that the

improvement of the proposed algorithm grows with the number of agents over

26

 
s
d
r
a
w
e
R

4

3

2

1

105

DDPG
MADDPG

Restricted Version
Proposed Algorithm

4

12
8
Number of Agents

16

Fig. 8. Rewards implemented by diﬀerent methods as the number of agents increases.

Table 5: The improvement of proposed algorithm over diﬀerent methods as agents are added

Number of Agents
Over DDPG (%)
Over MADDPG (%)
Over Restricted Version (%)

4
27
8.7
2.7

8
51
19
4.5

12
114
26
6.4

16
194
38
13

other methods. The reason is that the proposed algorithm can focus on informa-

tion which need more attention by the attention mechanism, unlike MADDPG

which uses all information non-selectively. Therefore, the performance of the

proposed algorithm is better as the number of agents increases.

Fig. 3(a) shows that the electricity prices are high during 8:00-11:00 and

17:00-20:00. At this time, the industrial park uses the CHP unit to generate

electricity instead of purchasing high-price electricity and simultaneously gener-

ate heat to supply the heat demand, which is shown in Figs. 9-11. At the same

time, since the CHP unit generates enough heat, the boiler does not generate

heat. At other time, the boiler serves unsatisﬁed heat demand by consuming

natural gas, which is shown in Figs. 10(a) and 11(b). The battery is charged at

low-price time 1:00-3:00 and 6:00, and discharged at high-prices time 9:00-11:00

and 20:00, which is shown in Fig. 9. Considering that battery charging and

discharging have a small impact on the total cost, the strategy of battery agent

is not good enough due to battery capacity limitations and changing electricity

prices. The thermal energy of the hot water tank is charged during 8:00-9:00

and 17:00-20:00 since CHP unit generates extra heat as shown in Figs. 10(b),

and the thermal energy of the hot water tank is discharged to supply the heat

demand at other time as shown in Figs. 10(a). As shown in Figs. 9-11, the

proposed algorithm achieves the multi-energy complementation, multi-device

27

)
h
W
M

(

y
t
i
c
i
r
t
c
e
E

l

10

8

6

4

2

0

0

)
h
W
M

(

y
t
i
c
i
r
t
c
e
E

l

10

8

6

4

2

0

0

CHP PV Battery Grid

5

10

15

20

25

Time (hour)

(a) Electricity generation/discharging

Battery Grid

Load

5

10

15

20

25

Time (hour)

(b) Electricity consumption/charging

Fig. 9. Electricity proﬁles under the proposed algorithm.

cooperation and multi-energy supply. Therefore, the proposed algorithm can

eﬀectively reduce the costs of factories.

5. Conclusion

In this paper, we study the multi-energy management problem of an indus-

trial park, which is imperative for today’s industrial production. We present a

MEMF to achieve centralized training and decentralized execution for energy

devices. To obtain the optimal scheduling policy of each energy device, we de-

sign a novel multi-agent deep reinforcement learning algorithm based on the

counterfactual baseline and SAC. Then, an attention mechanism is introduced

to focus on key information, which improves the exploration eﬃciency of soft

actor-critic. At last, based on real data, the performance of DRL algorithms in

terms of the number of agents and attention degree of information is analyzed,

showing that the proposed algorithm is an eﬀective solution for industrial energy

management problems.

In this paper, the energy of industrial production is suﬃcient supplied by

energy hubs. In actual industrial parks, some factories may have huge energy

demand which cannot be satisﬁed due to the capacity of transformers. In our fu-

ture work, the energy scheduling of actual industrial production with insuﬃcient

28

 
 
5

4

3

2

1

)
h
W
M

(

t
a
e
H

0

0

5

4

3

2

1

)
h
W
M

(

t
a
e
H

0

0

Boiler CHP Water tank

5

10

15

20

25

Time (hour)

(a) Heat generation/discharging

water tank

Load

5

10

15

Time (hour)

20

25

(b) Heat consumption/charging

Fig. 10. Heat proﬁles under the proposed algorithm.

)
h
W
M

(

s
a
G

12

9

6

3

0

0

)
h
W
M

(

s
a
G

12

9

6

3

0

0

5

5

Gas company

20

25

Boiler CHP

10

15

Time (hour)

(a) Gas Purchase

10

15

Time (hour)

20

25

(b) Gas consumption

Fig. 11. Gas proﬁles under the proposed algorithm.

29

 
 
 
 
energy supply will be further investigated. Furthermore, the joint optimization

of production scheduling and multi-energy generation/utilization is of interest.

References

[1] X. Lu, Z. Liu, L. Ma et al., ”A robust optimization approach for optimal

load dispatch of community energy hub,” Applied Energy, vol. 259:114195,

Feb. 2020.

[2] M. Jadidbonab, B. Mohammadi-Ivatloo, M. Marzband and P. Siano, ”Short-

Term Self-Scheduling of Virtual Energy Hub Plant Within Thermal Energy

Market,” IEEE Transactions on Industrial Electronics, vol. 68, no. 4, pp.

3124-3136, Apr. 2021.

[3] Heidari A, ”Stochastic eﬀects of ice storage on improvement of an energy

hub optimal operation including demand response and renewable energies,”

Applied Energy, vol. 261:114393, Mar. 2020.

[4] N. Liu, L. Zhou, C. Wang, X. Yu and X. Ma, ”Heat-Electricity Coupled

Peak Load Shifting for Multi-Energy Industrial Parks: A Stackelberg Game

Approach,” IEEE Transactions on Sustainable Energy, vol. 11, no. 3, pp.

1858-1869, Jul. 2020.

[5] J. Wei, Y. Zhang, J. Wang and L. Wu, ”Distribution LMP-based Demand

Management in Industrial Park via a Bi-level Programming Approach,” IEEE

Transactions on Sustainable Energy, vol. 12, no. 3, pp. 1695-1706, Feb. 2021.

[6] Z. Liu, M. Adams, R. Cote, Y. Geng, J. Ren, Q. Chen et al., ”Co-beneﬁts

accounting for the implementation of eco-industrial development strategies

in the scale of industrial park based on emergy analysis,” Renewable and

Sustainable Energy Reviews, vol. 81, no.1, pp. 1522-1529, Jan. 2018.

[7] M. Shahidehpour, C. Li, X. Wang, W. Huang and T. Nengling, ”Two-Stage

Full-Data Processing for Microgrid Planning With High Penetrations of Re-

30

newable Energy Sources,” IEEE Transactions on Sustainable Energy, vol. 12,

no.4, pp. 2042-2052, May 2018.

[8] P. Zeng, H. Li, H. He and S. Li, ”Dynamic Energy Management of a Micro-

grid Using Approximate Dynamic Programming and Deep Recurrent Neural

Network Learning,” IEEE Transactions on Smart Grid, vol. 10, no. 4, pp.

4435-4445, Jul. 2019.

[9] S. Gupta, V. Kekatos and W. Saad, ”Optimal Real-Time Coordination of

Energy Storage Units As a Voltage-Constrained Game,” IEEE Transactions

on Smart Grid, vol. 10, no. 4, pp. 3883-3894, July 2019.

[10] Y. Li, H. He, A. Khajepour, H. Wang, J. Peng, ”Energy management for

a power-split hybrid electric bus via deep reinforcement learning with terrain

information”. Applied Energy, vol. 255:113762, Aug 2019.

[11] P. Kou, D. Liang, C. Wang, Z. Wu and L. Gao, ”Safe deep reinforcement

learning-based constrained optimal control scheme for active distribution net-

works”, Appl Energy, vol. 264:114772, April 2020.

[12] Z. Xu, G. Han, L. Liu, M. Martinez-Garcia and Z. Wang, ”Multi-Energy

Scheduling of an Industrial Integrated Energy System by Reinforcement

Learning-Based Diﬀerential Evolution,” IEEE Transactions on Green Com-

munications and Networking, vol. 5, no. 3, pp. 1077-1090, Sept. 2021.

[13] N. Ebell, F. Heinrich, J. Schlund and M. Pruckner, ”Reinforcement Learn-

ing Control Algorithm for a PV-Battery-System Providing Frequency Con-

tainment Reserve Power,” 2018 IEEE International Conference on Communi-

cations, Control, and Computing Technologies for Smart Grids (SmartGrid-

Comm), 2018, pp. 1-6.

[14] E. Mocanu et al., ”On-Line Building Energy Optimization Using Deep

Reinforcement Learning,” IEEE Transactions on Smart Grid, vol. 10, no. 4,

pp. 3698-3708, Jul. 2019.

31

[15] Y. Ye, D. Qiu, X. Wu, G. Strbac and J. Ward, ”Model-Free Real-Time

Autonomous Control for a Residential Multi-Energy System Using Deep Re-

inforcement Learning,” IEEE Transactions on Smart Grid, vol. 11, no. 4, pp.

3068-3082, Jul. 2020.

[16] C. Guo, X. Wang, Y. Zheng, and F. Zhang, ”Real-time optimal energy

management of microgrid with uncertainties based on deep reinforcement

learning,” Energy, vol. 238:121873, Jan. 2022.

[17] A. Sheikhi, M. Rayati, A. M. Ranjbar, ”Demand side management for a

residential customer in multi-energy systems,” Sustainable Cities and Society,

pp. 63-77, 2016.

[18] M. Weigold, H. Ranzau, A. Schaumann, T. Kohne, N. Panten and E. Abele,

”Method for the application of deep reinforcement learning for optimised

control of industrial energy supply systems by the example of a central cooling

system”, CIRP Annals, vol. 70, no. 1, pp.17-20, 2021.

[19] X. Wang, Y. Liu, J. Zhao, C. Liu, J. Liu and J. Yan, ”Surrogate model

enabled deep reinforcement learning for hybrid energy community operation”,

Applied Energy, vol. 289:116722, May 2021.

[20] N. Ebell and M. Pruckner, ”Coordinated Multi-Agent Reinforcement

Learning for Swarm Battery Control,” 2018 IEEE Canadian Conference on

Electrical and Computer Engineering (CCECE), 2018, pp. 1-4.

[21] M. Roesch, C. Linder, C. Bruckdorfer, A. Hohmann and G. Reinhart, ”In-

dustrial Load Management using Multi-Agent Reinforcement Learning for

Rescheduling,” 2019 Second International Conference on Artiﬁcial Intelli-

gence for Industries (AI4I), 2019, pp. 99-102.

[22] N. Ebell, M. Gutlein and M. Pruckner, ”Sharing of Energy Among Coop-

erative Households Using Distributed Multi-Agent Reinforcement Learning,”

2019 IEEE PES Innovative Smart Grid Technologies Europe (ISGT-Europe),

2019, pp. 1-5.

32

[23] L. Busoniu, R. Babuska and B. De Schutter, ”A Comprehensive Survey of

Multiagent Reinforcement Learning,” IEEE Transactions on Systems, Man,

and Cybernetics, Part C (Applications and Reviews), vol. 38, no. 2, pp. 156-

172, March 2008.

[24] A. T. D. Perera and P. Kamalarubann, ”Applications of reinforcement

learning in energy systems”, Renewable and Sustainable Energy Reviews, vol.

137:110618, March 2021.

[25] T. T. Nguyen, N. D. Nguyen and S. Nahavandi, ”Deep Reinforcement

Learning for Multiagent Systems: A Review of Challenges, Solutions, and

Applications,” IEEE Transactions on Cybernetics, vol. 50, no. 9, pp. 3826-

3839, Sept. 2020.

[26] R. Lu, Y. Li, Y. Li and Y. Ding, ”Multi-agent deep reinforcement learning

based demand response for discrete manufacturing systems energy manage-

ment”, Applied Energy, vol. 276:115473, Oct. 2020.

[27] J. Li, T. Yu, and B. Yang, ”A data-driven output voltage control of solid

oxide fuel cell using multi-agent deep reinforcement learning”, Applied Energy,

vol. 304:117541, Dec. 2021.

[28] R. S. Sutton and A. G. Barto, ”Reinforcement Learning: An Introduction,”

in IEEE Transactions on Neural Networks, vol. 9, no. 5, pp. 1054-1054, Sept.

1998.

[29] Laurence A. Baxter, ”Markov Decision Processes: Discrete Stochastic Dy-

namic Programming,” Technometrics, vol. 37, no.3, Mar. 2012.

[30] X. Wu, X. Li, J. Li, P. C. Ching, V. C. M. Leung and H. V. Poor, ”Caching

Transient Content for IoT Sensing: Multi-Agent Soft Actor-Critic,” IEEE

Transactions on Communications, vol. 69, no. 9, pp. 5886-5901, Sept. 2021.

[31] W. Wang, N. Yu, Y. Gao and J. Shi, ”Safe Oﬀ-Policy Deep Reinforcement

Learning Algorithm for Volt-VAR Control in Power Distribution Systems,”

IEEE Transactions on Smart Grid, vol. 11, no. 4, pp. 3008-3018, July 2020.

33

[32] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, ”Policy

gradient methods for reinforcement learning with function approximation,”

in Advances in Neural Information Processing Systems, pp. 1057-1063, 2000.

[33] T. Haarnoja, A. Zhou, P. Abbeel and S. Levine, ”Soft actor-critic: Oﬀ-

policy maximum entropy deep reinforcement learning with a stochastic actor,”

in International Conference on Machine Learning, vol. 80, pp. 1861-1870, Jul.

2018.

[34] Oh, J., Chockalingam, V., Lee, H. et al., ”Control of memory, active per-

ception, and action in minecraft,” in International Conference on Machine

Learning, pp. 2790-2799, 2016.

[35] J. H. Kim, K. W. On, W. Lim, J. W. Ha, and B. T. Zhang, “Hadamard

Product for Low-rank Bilinear Pooling,” 2016, arXiv prepint, arXiv:

1610.04325.

[36] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.

N., Kaiser,  L., and Polosukhin, I., ”Attention is all you need,” in Advances

in Neural Information Processing Systems, pp. 6000-6010, 2017.

[37] The price provided by the State Grid Jiangsu Electric Power Co., Ltd,

http://www.js.sgcc.com.cn/html.

[38] PJM hourly load, https://dataminer2.pjm.com.

[39] Renewables.ninja, https://www.renewables.ninja.

[40] T. P. Lillicrap, J. J. Hunt, A. Pritzel et al., ”Continuous control with deep

reinforcement learning,” in International Conference on Learning Represen-

tations, 2016.

[41] R. Lowe, Y. Wu, A. Tamar et al, ”Multi-agent actor-critic for mixed

cooperative-competitive environments,” in Advances in Neural Information

Processing Systems, pp. 6382-6393, 2017.

34

