0
2
0
2

r
p
A
8
2

]

G
L
.
s
c
[

4
v
5
0
2
9
0
.
5
0
9
1
:
v
i
X
r
a

EVALUATING RECOMMENDER SYSTEMS FOR AI-DRIVEN
BIOMEDICAL INFORMATICS

A PREPRINT

William La Cava, Heather Williams, Weixuan Fu, Steve Vitale, Durga Srivatsan, Jason H. Moore∗
Institute for Biomedical Informatics
Department of Biostatistics, Epidemiology and Informatics
University of Pennsylvania
Philadelphia, PA 19104

April 29, 2020

Motivation: Many researchers with domain expertise are unable to easily apply machine learning to their bioinformatics
data due to a lack of machine learning and/or coding expertise. Methods that have been proposed thus far to automate
machine learning mostly require programming experience as well as expert knowledge to tune and apply the algorithms
correctly. Here, we study a method of automating biomedical data science using a web-based platform that uses AI to
recommend model choices and conduct experiments. We have two goals in mind: ﬁrst, to make it easy to construct
sophisticated models of biomedical processes; and second, to provide a fully automated AI agent that can choose and
conduct promising experiments for the user, based on the user’s experiments as well as prior knowledge. To validate
this framework, we experiment with hundreds of classiﬁcation problems, comparing to state-of-the-art, automated
approaches. Finally, we use this tool to develop predictive models of septic shock in critical care patients.
Results: We ﬁnd that matrix factorization-based recommendation systems outperform meta-learning methods for
automating machine learning. This result mirrors the results of earlier recommender systems research in other domains.
The proposed AI is competitive with state-of-the-art automated machine learning methods in terms of choosing optimal
algorithm conﬁgurations for datasets. In our application to prediction of septic shock, the AI-driven analysis produces a
competent machine learning model (AUROC 0.85 +/- 0.02) that performs on par with state-of-the-art deep learning
results for this task, with much less computational effort.

Availability: PennAI is available free of charge and open-source. It is distributed under the GNU public license (GPL)
version 3.
Contact: lacava@upenn.edu
Supplementary information: Software and experiments are available from epistasislab.github.io/pennai.

Keywords Artiﬁcial Intelligence · Automated Machine Learning

1

Introduction

Experimental data is being collected faster than it can be understood across scientiﬁc disciplines [1]. The hope of many
in the data science community is that widely accessible, open-source artiﬁcial intelligence (AI) tools will allow scientiﬁc
insights from these data to keep abreast of their collection [2]. AI is expected to make signiﬁcant improvements to
scientiﬁc discovery, human health and other ﬁelds in the coming years. One of the key promises of AI is the automation
of learning from large sets of collected data. However, at the same time that data collection is outpacing researchers,
methodological improvements from the machine learning (ML) and AI communities are outpacing their dissemination
to other ﬁelds. As a result, AI and ML remain steep learning curves for non-experts, especially for researchers pressed
to gain expertise in their own disciplines.

Specialized researchers would beneﬁt greatly from increasingly automated, accessible and open-source tools for AI.
With this in mind, we created a free and open-source platform called PennAI2 (University of Pennsylvania Artiﬁcial

∗Corresponding author: jhmoore@upenn.edu
2http://github.com/EpistasisLab/pennai

 
 
 
 
 
 
A PREPRINT - APRIL 29, 2020

Intelligence) that allows the non-expert to quickly conduct a ML analysis on their data [2]. PennAI uses a web
browser-based user interface (UI) to display a user’s datasets, experiments, and results, shown in Fig. 1. In order to
automate the user’s analysis, PennAI uses a bootstrapped recommendation system that automatically conﬁgures and
runs supervised learning algorithms catered to the user’s datasets and previous results.

In addition to its use as a data science tool, PennAI serves as a test-bed for methods development by AI researchers
who wish to develop automated machine learning (AutoML) methods. AutoML is a burgeoning area of research in the
ML community that seeks to automatically conﬁgure and run learning algorithms with minimal human intervention.
A number of different learning paradigms have been applied to this task, and tools are available to the research
community [3, 4, 5, 6, 7, 8] as well as commercially3 [9]. A competition around this goal has been running since 20154
focused various budget-limited tasks for supervised learning [10].

Despite what their name implies, many leading AutoML tools are highly conﬁgurable and require coding expertise to
operate [3, 4, 5, 6, 7, 8], which leaves a gap for adoption to new users. Furthermore, AutoML tools typically wrap many
ML analyses via the automation procedure, obscuring the analysis from the user and preventing user intput or guidance.
In contrast to these strategies, PennAI interacts with the user through algorithm recommendations, and can learn from
both its own analysis and that of the user. This interaction is achieved using a web-based recommender system.

Recommender systems are well known inference methods underlying many commercial content platforms, including
Netﬂix [11], Amazon [12], Youtube [13], and others. There has been limited adoption of recommender systems as
AutoML approaches [14, 15], much less an attempt to compare and contrast different underlying strategies. To ﬁll
this gap, we benchmark several state-of-the-art recommender systems in a large experiment that tests their use as an
AutoML strategy.

Our ﬁrst goal is to assess the ability of state-of-the-art recommender systems to learn the best ML algorithm and
parameter settings for a given dataset over a set number of iterations in the context of previous results. We compare
collaborative ﬁltering (CF) approaches as well as metalearning approaches on a set of 165 open-source classiﬁcation
problems. We ﬁnd that the best algorithms (matrix factorization) work well without leveraging dataset metadata, in
contrast to other AutoML approaches. We demonstrate the ability of PennAI to outperform Hyperopt and perform
on par with AutoSklearn, two popular AutoML tools for Python. Our second goal is to test PennAI in application to
predictive modeling in the biomedical context. To this end, we use PennAI to develop predictive models of septic shock
using the MIMIC-III [16] critical care database. We ﬁnd that PennAI is useful for quickly ﬁnding models with strong
performance, in this instance producing a model with performance on par with complex, effort-intensive recurrent deep
neural networks. We include a sensitivity analysis of the predictive model of septic shock produced by PennAI that
lends credence to its predictions.

2 Background

In this section, we brieﬂy review AutoML methodologies, which are a key component to making data science
approachable to new users. We then describe recommender systems, the various methods that have worked in other
application areas, and our motivation for applying them to this relatively new area of AutoML.

2.1 Automated Machine Learning

AutoML is a burgeoning area of research in the ML community that seeks to automatically conﬁgure and run learning
algorithms without human intervention. A number of different learning paradigms have been applied to this task, and
tools are available to the research community as well as commercially. A competition around this goal has been running
since 2015 5 focused various budget-limited tasks for supervised learning [10].

A popular approach arising from the early competitions is sequential model-based optimization via Bayesian learning [3],
represented by the auto-Weka, AutoSklearn and Hyperopt packages [4, 5, 6]. These tools parameterize the combined
problem of algorithm selection and hyperparameter tuning and use Bayesian optimization to select and optimize
algorithm conﬁgurations.

Auto-sklearn incorporates metalearning into the optimization process [17, 18] to narrow the search space of the
optimization process. Metalearning in this context refers to the use of the “metafeatures" of the datasets, such as
predictor distributions, variable types, cardinality etc. provide information about algorithm performance that can be
leveraged to choose an appropriate algorithm conﬁguration, given these properties for a candidate dataset. Auto-sklearn

3H2O
4http://automl.chalearn.org/
5http://automl.chalearn.org/

2

A PREPRINT - APRIL 29, 2020

uses metalearning to narrow the search space of their learning algorithm. In lieu of metalearning, PoSH AutoSklearn [8],
an update to AutoSklearn, opted to bootstrap AutoSklearn with an extensive analysis to minimize the conﬁguration
space. ML conﬁgurations were optimized on a large number of datasets beforehand, and the initial conﬁgurations were
narrowed to those that performed best over all datasets. This tool effectively replaced metalearning with bootstrapping;
our experiments in Section 4 provide some evidence supporting a similar strategy.

Another popular method for AutoML is tree-based pipeline optimization tool TPOT [7]. TPOT uses an evolutionary
computation approach known as genetic programming to optimize syntax tree representations of ML pipelines.
Complexity is controlled via multi-objective search. Benchmark comparisons of TPOT and AutoSklearn show trade-offs
in performance for each [19].

There are many commercial tools providing variants of AutoML as well. Many of these platforms do not focus on
choosing from several ML algorithms, but instead provide automated ways of tuning speciﬁc ones. Google has created
AutoML tools as well using neural architecture search [9], a method for conﬁguring the architecture of neural networks.
This reﬂects their historical focus on learning from sequential and structured data like images. H2O uses genetic
algorithms to tune the feature engineering pipeline of a user-chosen model. Intel has focused on proprietary gradient
boosted ensembles of decision trees [10].

A main paradigm of many AutoML is that they wrap several ML analyses and return a single (perhaps ensemble) result,
thereby obscuring their analysis from the user. Although this does indeed automate the ML process, it removes the
user from the experience. In contrast to these strategies, PennAI uses a recommender system as its basis of algorithm
recommendation with the goal of providing a more intuitive and actionable user experience.

There has a line of research utilizing recommender systems for algorithm selection [20, 21, 15, 14]. One recent
example is Yang et al. [14], who found that non-negative matrix factorization could be competitive with AutoSklearn for
classiﬁcation. A recent workshop6 also solicited discussion of algorithm selection and recommender systems, although
most research of this nature is interested in tuning the recommendation algorithms themselves [22].

Ultimately, the best algorithm for a dataset is highly subjective: a user must balance their wants and needs, including the
accuracy of the model, its interpretability, the training budget and so forth. PennAI’s coupling of the recommendation
system approach with the UI allows for more user interaction, essentially by maintaining their ability to “look under the
hood". The user is able to fully interface with any and all experiments initialized by the AI in order to, for example,
interrupt them, generate new recommendations, download reproducible code or extract ﬁtted models and their results.
In addition, by making it easy to launch user-chosen experiments, PennAI makes it possible to train the recommender
on user-generated results, in addition to its own. A summary of the differences in features between PennAI and several
other AutoML tools is shown in Table 1. By developing PennAI as a free and open-source tool, we also hope to
contribute an extensible research platform for the ML and AutoML communities. The code is developed on Github and
documents a base recommender class that can be written to the speciﬁcation of any learning algorithm. We therefore
hope that it will serve as a framework for bring real world users into contact with cutting edge methodologies.

2.2 Recommender Systems

Recommender systems are typically used to recommend items, e.g. movies, books, or other products, to users, i.e.
customers, based on a collection of user ratings of items. The most popular approach to recommender systems is
collaborative ﬁltering (CF). CF approaches rely on user ratings of items to learn the explicit relationship between
similar users and items. In general, CF approaches attempt to group similar users and/or group similar items, and then
to recommend similar items to similar users. CF approaches assume, for the most part, that these similarity groupings
are implicit in the ratings that users give to items, and therefore can be learned. However, they may be extended to
incorporate additional attributes of users or items [23].

Recommenders face challenges when deployed to new users, or in our case, datasets. The new user cold start
problem [24] refers to the difﬁculty in predicting ratings for a user with no data by which to group them. With datasets,
one approach to this problem is through metalearning. Each dataset has quantiﬁable traits that can be leveraged to
perform similarity comparisons without relying on algorithm performance history. In our experiments we benchmark a
recommender that uses metafeatures to derive similarity scores for recommendations, as has been proposed in previous
AutoML work [5, 18].

Recommender systems are typically used for different applications than AutoML, and therefore the motivations
behind different methods and evaluation strategies are also different. For example, unlike typical product-based
recommendation systems, the AI automatically runs the chosen algorithm conﬁgurations, and therefore receives more
immediate feedback on its performance. Since the feedback is explicitly the performance of the ML choice on the

6http://amir-workshop.org/

3

A PREPRINT - APRIL 29, 2020

Table 1: A comparison of AutoML tool characteristics and PennAI.

Free Open Source Code-Free Learns from User
(cid:88)

(cid:88)

Tool

Methodology

PennAI
HyperOptSklearn
AutoSklearn
TPOT
H20.AI

Recommender Systems
Bayesian
Bayesian + Metalearning
Genetic Programming
Genetic Algorithms

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

given dataset, the ratings/scores are reproducible, less noisy, and less sparse than user-driven systems. This robustness
allows us to measure the performance of each recommendation strategy reliably in varying training contexts. As
another example, many researchers have found in product recommendation that the presence or absence of a rating
may hold more weight than the rating itself, since users choose to rate or to not rate certain products for non-random
reasons [25]. This observation has led to the rise of implicit rating-based systems, such as SVD++ [26], that put more
weight on presence/absence of ratings. In the context of AutoML, it is less likely that the presence of results for a given
algorithm conﬁguration imply that it will outperform others. Furthermore, the goal of advertising-based, commercial
recommendation systems may not be to ﬁnd the best rating for a user and product, but to promote engagement of the
user, vis-a-vis their time spent browsing the website. To this end, recommender systems such as Spotlight [27] are
based on the notion of sequence modeling: what is the likelihood of a user interacting with each new content given the
sequence of items they have viewed? Sequence-based recommendations may improve the user experience with a data
science tool, but we contend that they do not align well with the goals of an approachable data science assistant.

3 Methods

Here we will brieﬂy describe the user interface of PennAI. In Section 3.1, we describe the recommender systems that
we benchmark in our experiments for automating the algorithm selection problem. Fig. 1 gives an overview of the data
science pipeline. Users upload datasets through the interface or optionally by pointing to a path during startup. At that
point, users can choose between building a custom experiment (manually conﬁguring an algorithm of their choice) or
simply clicking the AI button. Once the AI is requested, the recommendation engine chooses a set of analyses to run.
The AI can be conﬁgured with different termination criteria, including a ﬁxed number of runs, a time limit, or running
until the user turns it off. As soon as the runs have ﬁnished, the user may navigate to the results page, where several
visualizations of model performance are available (Fig. 1.C)).

PennAI is available as a docker image that may be run locally or on distributed hardware. Due to its container-based
architecture, it is straightforward to run analysis in parallel, both for datasets and algorithms, by conﬁguring the docker
environment. For more information on the system architecture, refer to the Supplemental Material.

3.1 Recommendation System

In order to use recommender systems as a data science assistant, we treat datasets as users, and algorithms as items. The
goal of the AI is therefore as follows: given a dataset, recommend an algorithm conﬁguration to run. Once the algorithm
conﬁguration has been run, the result is now available as a rating of that algorithm conﬁguration on that dataset, as if
the user had rated the item. This is a nice situation for recommender systems, since normally users only occasionally
rate the items they are recommended. We denote this knowledge base of experimental results as D = {rad, rbe, . . . },
where rad is the test score, i.e. rating, of algorithm conﬁguration a on dataset d. In our experiments the test score is the
average 10-fold CV score of the algorithm on the dataset.

With a few notable exceptions discussed below, the recommenders follow this basic procedure:

1. Whenever new experiment results are added to D, update an internal model mapping datasets d to algorithm

conﬁgurations, a.

2. Given a new recommendation request, generate ˆrad, the estimated the score of algorithm conﬁguration a on

dataset d. Do this for every ad pair that has not already been recommended.

3. Return recommended algorithm conﬁgurations in accordance with the termination criterion, in order of best

rating to worst.

Note that the knowledge base D can be populated not only by the AI, but by the user through manual experiments
(Fig. 1.A) and by the initial knowledge base for PennAI. In production mode, the knowledge base is seeded with

4

A PREPRINT - APRIL 29, 2020

Figure 1: Overview of the UI. A) Users upload datasets and choose a custom experiment (right), or allow the AI to
run experiments of its choosing by clicking the AI button (left). B) Experiments are tabulated with conﬁguration
and performance information. The user may download scripts to reproduce the experiment in python, or export the
ﬁtted model. C) The results page displays experiment information and statistics of the ﬁtted model, including various
performance measures (confusion matrix, receiver operating characteristic (ROC) curve, etc.) as well as feature
importance scores for the independent variables.

approximately 1 million ML results generated on 165 open-source datasets, detailed here [28]. The user may also specify
their own domain-speciﬁc cache of results. Below, we describe several recommender strategies that are benchmarked
in the experimental section of this paper. Most of these recommenders are adapted from the Surprise recommender
library [29].

3.1.1 Neighborhood Approaches

We test four different neighborhood approaches to recommending algorithm conﬁgurations that vary in their deﬁnitions
of the neighborhood. Three of these implementations are based the k-nearest neighbors (KNN) algorithm, and the other
uses co-clustering. For each of the neighborhood methods, similarity is calculated using the mean squared deviation
metric.

In the ﬁrst and second approach, clusters are derived from the results data directly and used to estimate the ranking of
each ML method by computing the centroid of rankings within the neighborhood. Let N k
d (a) be the k-nearest neighbors
of algorithm conﬁguration a that have been run on dataset d. For KNN-ML, we then estimate the ranking from this
neighborhood as:

ˆrad =

(cid:80)

b∈N k
(cid:80)

d (a) sim(a, b) · rbd
d (a) sim(a, b)
b∈N k

5

(1)

Build Experiment: Shock Train Summary      BCAFor KNN-data, we instead deﬁne the neighborhood over datasets, with N k
dataset d that have results from algorithm a. Then we estimate the rating as:
a (d) sim(d, e) · rae
a (d) sim(d, e)
e∈N k

e∈N k
(cid:80)

ˆrad =

(cid:80)

A PREPRINT - APRIL 29, 2020

a (d) consisting of the k nearest neighbors to

(2)

Instead of choosing to deﬁne the clusters according to datasets or algorithms, we may deﬁne co-clusters to capture
algorithms and datasets that cluster together. This is the motivation behind co-clustering [30], the third neighborhood-
based approach in this study. Under the CoClustering approach, the rating of an algorithm conﬁguration is estimated
as:

ˆrad = ¯Cad + (µa − ¯Ca) + (µd − ¯Cd)
(3)
where ¯C is the average rating in cluster C. As Eqn. 3 shows, clusters are deﬁned with respect to a and d together and
separately. Co-clustering uses a k-means strategy to deﬁne these clusters. In case the dataset is unknown, the average
algorithm rating, µa, is returned instead; likewise if the algorithm conﬁguration is unknown, the average dataset rating
µd is used. In case neither is known, the global average rating µ is returned.

Finally, we test a metalearning method dubbed KNN-meta. In this case, the neighborhood is deﬁned according to
metafeature similarity, in the same way as other approaches [17, 18, 5]. We use a set of 45 metafeatures calculated from
the dataset, including properties such as average correlation with the dependent variable; statistics describing the mean,
max, min, skew and kurtosis of the distributions of each independent variable; counts of types of variables; and so on.

Rather than attempting to estimate ratings of every algorithm, KNN-meta maintains an archive of the best algorithm
conﬁguration for each dataset experiment. Given a new dataset, KNN-meta calculates the k nearest neighboring datasets
and recommends the highest scoring algorithm conﬁgurations from each dataset. KNN-meta has the advantage in cold
starts since it does not have to have seen a dataset before to reason about its similarity to other results; it only needs to
know how its metafeatures compare to previous experiments. KNN-meta has the limitation, however, that it can only
recommend algorithm conﬁgurations that have been tried on neighboring datasets. In the case that all of these algorithm
conﬁgurations have already been recommended, KNN-meta will recommends uniform-randomly from algorithms and
their conﬁgurations.

3.1.2 Singular Value Decomposition

The singular value decomposition (SVD) recommender is a CF method popularized by the top entries to the Netﬂix
challenge [11]. Like other top entrants [31, 32], SVD is based on a matrix factorization technique that attempts to
minimize the error of the rankings via stochastic gradient descent (SGD). Each rating is estimated as

ˆr = µ + bd + ba + qT

a pd

(4)

where µ is the average score of all datasets across all learners; ba is the estimated bias for algorithm a, initially zero;
bd is the estimated bias for dataset d, initially zero; qa is a vector of factors associated with algorithm a and pd is the
vector of factors associated with dataset d, both initialized from normal distributions centered at zero. Ratings are
learned to minimize the regularized loss function

(cid:88)

L =

rad∈D

(rad − ˆrad)2 + λ (cid:0)b2

a + b2

d + ||qa||2 + ||pd||2(cid:1)

(5)

One of the attractive aspects of SVD is its ability to learn latent factors of datasets and algorithms (qd and qd) that
interact to describe the observed experimental results without explicitly deﬁning these factors, as is done in metalearning.
A historical challenge of SVD is its application to large, sparse matrices, such as the matrix deﬁned by datasets and
algorithms (in our experiments this matrix is about 1 million elements). SVD recommenders address the computational
hurdle by using SGD to estimate the parameters of Eqn. 4 using available experiments (i.e. dataset ratings of algorithms)
only [33]. SGD is applied by the following update rules:

bd ← bd +γ(ead − λbd)
ba ← ba +γ(ead − λba)
pd ← pd +γ(eadqa − λpd)
qa ← qa +γ(eadpd − λqa)
where ead = rad − ˆrad. To facilitate online learning, the parameters in Eqn. 6 are maintained between updates to the
experimental results, and the number of iterations (epochs) of training is set proportional to the number of new results.

(6)

6

A PREPRINT - APRIL 29, 2020

Figure 2: Diagram describing the experimental design. We conduct 300 trials of the experiment, each of which uses a
different sampling of datasets. For each trial, 1000 iterations are conducted. The AI (recommender system) is initially
trained on n_init ML results from the Knowledge Base. Then, each iteration, n_recs recommendations are made by the
AI for one dataset. These recommended ML conﬁgurations are retrieved from the Knowledge Base and used to update
the AI for the next iteration.

3.1.3 Slope One

Slope one [34] is a simple recommendation strategy that models algorithm performance on a dataset as the average
deviation of the performance of algorithms on other datasets with which the current dataset shares at least one analysis
in common. To rate an algorithm conﬁgurations a on dataset d, we ﬁrst collect a set Ra(d) of algorithms that have been
trained on d and share at least one common dataset experiment with a. Then the rating is estimated as

ˆrad = µd +

1
Ra(d)

(cid:88)

dev(a, b)

b∈Ra(d)

(7)

3.1.4 Benchmark Recommenders

As a control, we test two baseline algorithms: a random recommender and an average best recommender. The Random
recommender chooses uniform-randomly among ML methods, and then uniform-randomly among hyperparameters
for that method to make recommendations. The Average recommender keeps a running average of the best algorithm
conﬁguration as measured by the average validation balanced accuracy across experiments. Given a dataset request, the
Average recommender recommends algorithm conﬁgurations in order of their running averages, from best to worst.

4 Experiments

The goal of our experiments is to assess different recommendation strategies in their ability to recommend better
algorithm conﬁgurations for various datasets as they learn over time from previous experiments. The diagram in Fig. 2
describes the experimental design used to evaluate recommendation strategies under PennAI.

Datasets We assessed each recommender on 165 open-source classiﬁcation datasets, varying in size (hundreds to
millions of samples) and origin (bioinformatics, economics, engineering, etc). We used datasets from the Penn Machine
Learning Benchmark (PMLB) [35]. PMLB is a curated and standardized set of hundreds of open source supervised
machine learning problems from around the web (sharing many in common with OpenML [36]). In previous work,
we conducted an extensive benchmarking experiment to characterize the performance of different ML frameworks
on these problems [35, 28]. The benchmark assessed 13 ML algorithms over a range of hyperparameters detailed in
Table 1 of the Supplemental Material on these problems. This resulted in a cache of over 1 million ML results across a
broad range of classiﬁcation problems which we use here to assess the performance of each recommender with a known
ranking of algorithms for each dataset. For the experiment in this paper, we used a subset of these results consisting of
12 ML algorithms (dropping one of three naïve Bayes algorithms) with 7580 possible hyperparameter conﬁgurations.

Evaluation of Recommender Systems The ﬁrst experiment consisted of 300 repeat trials for each recommender.
In each trial, the recommender began with a knowledge base of ninit experiments that consist of single ML runs on
single datasets. For each iteration of the experiment, the recommender was asked to return nrecs recommendations
for a randomly chosen dataset. Once the recommendation was made, the 5-fold CV results on training data for the

7

A PREPRINT - APRIL 29, 2020

recommended algorithm conﬁgurations were fed back to the recommender as updated knowledge. Note that this
experiment mirrors a reinforcement learning experiment in which the actions taken by the recommender (i.e., the
recommendations it makes) determine the information it is able to learn about the relationship between datasets and
algorithm conﬁgurations. To assess the quality of these recommendations without overﬁtting, we used a separate, hold-
out test set performance score for each algorithm conﬁguration on each dataset. We report the scores of each algorithm
conﬁguration on these heldout data throughout Section 5. For our experiments, we varied ninit ∈ [1, 100, 10000] and
nrecs ∈ [1, 10, 100]. These settings control sensitivity to 1) starting with more information and 2) exploring more
algorithm options during learning.

4.1 Comparison to State-of-the-Art

Based on the results of our ﬁrst experiment (Section 4), we chose a ﬁnal conﬁguration for PennAI and benchmarked
its performance against two other state-of-the-art AutoML tools: AutoSklearn [5] and HyperOptSklearn [6]. For
AutoSklearn, we restricted the search space to ML conﬁgurations to bring it closer to the search spaces of PennAI and
HyperOptSklearn that do not use feature preprocessors. Otherwise we used default settings of both AutoSklearn and
HyperOptSklearn. For this comparison, we performed a leave-one-out style analysis, meaning that PennAI is trained
on results from all other datasets prior to iteratively making recommendations for a given dataset. This leave-one-out
analysis corresponds to the applied case, in which PennAI is deployed with a pre-trained recommender and must run
experiments for a newly uploaded dataset. For each method, we assessed the generalization performance of the returned
model after a given number of evaluations. For AutoSklearn, we assessed its performance as a function of wall-clock
run-time, since there was not an apples-to-apples way to control the number of algorithm evaluations.

Comparison Metrics Since we have the complete results of ML analyses on our experiment datasets, we assessed
recommendations in terms of their closeness to the best conﬁguration, i.e. that conﬁguration with the best holdout
performance on a dataset among all algorithms in our exhaustive benchmark. Each algorithm conﬁguration is primarily
assessed according to its balanced accuracy (BA), a metric that takes into account class imbalance by averaging
accuracy over classes[37]. Let the best balanced accuracy on a given dataset be BA∗
d. Then the performance of a
recommendation is assessed as the relative distance to the best solution, as:

∆Balanced Accuracyad =

(BA∗

d − BAad)
BA∗
d

(8)

In addition to Eqn. 8, we assessed the AI in terms of the number of datasets for which it is able to identify an “optimal"
conﬁguration. Here we deﬁned “optimal" algorithm conﬁgurations to be those that score within some small threshold
of the best performance for a dataset. This deﬁnition of optimality is of course limited, both by the ﬁnite search space
deﬁned by the algorithm conﬁguration options and by the choice of thresholding (we tried 1% and 5%). Nonetheless,
this deﬁnition gives a practical indicator of the extent to which AI is able to reach the best known performance.

4.2

Illustrative Example

In addition to testing different recommendation strategies within PennAI, we applied PennAI to the task of generating a
classiﬁcation model for predicting patient’s risk of septic shock. For the shock task, we used data from the MIMIC-
III [16] critical care database and preprocessed it according to prior work [38]. In addition to the binning process
described by Harutyunyan et al., we calculated autocorrelations for each predictor at 5 different lags in order to capture
time series features. This resulted in a prediction problem with 8346 training patients, 6284 test patients, and 60
dependent variables.

We used the best performing PennAI conﬁguration from our experiments. We began by allowing the AI to suggest and
run 10 experiments. We then manually chose 5 additional algorithm conﬁgurations to run, using default settings in
PennAI. PennAI’s results page was studied to validate models on the training set, and to pick a ﬁnal model for download.
We then evaluated this downloaded model on the testing set using PennAI’s model export functionality. Screenshots of
this process are shown in Fig. 1.

As a point of comparison, we trained a long-term-short-term memory (LSTM) deep learning model using the architecture
from [38] and training for 100 epochs. We also compared the results to septic shock models from literature [39]. We
discuss these results in Section 5.2.

8

A PREPRINT - APRIL 29, 2020

Figure 3: Experiment results for each recommendation strategy. Each plot shows the median ∆ Balanced Accuracy
(Eqn. 8 for 300 trials with error bars denoting 95% conﬁdence intervals. From left to right, the number of recommenda-
tions made per dataset increases; from top to bottom, the number of experiments in the initial knowledgebase increases.
A lower ∆ Balanced Accuracy indicates that ML conﬁgurations being recommended are closer to the best known
conﬁguration.

5 Results

Results for the PMLB experiment are shown in Figure 3. Recommenders are ﬁrst compared in terms of median ∆
Balanced Accuracy (Eq. 8) in Fig. 3. In Fig. 4, we look at the fraction of datasets for which SVD is able to ﬁnd an
optimal conﬁguration under different experiment treatments. In the Supplemental Material, we visualize the behavior of
a subset of recommenders in order to gain insight into which algorithms are being selected and how this compares to
the underlying distribution of algorithm performance in the knowledgebase.

Let us ﬁrst focus on the performance results in Fig. 3. We ﬁnd in general that the various recommender systems are able
to learn to recommend algorithm conﬁgurations that increasingly minimize the gap between the best performance on
each dataset (unkown to the AI) and the current experiments. SVD performs the best, tending to reach good performance
more quickly than the other recommendation strategies, across treatments. KNN-data and KNN-ml are the next best
methods across treatments; KNN-ML shows a sensitivity to the number of recommendations per iteration, indicating it
requires more results to form good clusters. For most experimental treatments, is a gap between those three methods
and the next best recommenders, which vary between SlopeOne, KNN-meta, and CoClustering for different settings.

As we expected due to its cold-start strategy, KNN-meta turns out to work well initially, but over time fails to converge
on a set of high quality recommendations. The collaborative ﬁltering recommendaters are generally able to learn quickly
from few examples compared to the metalearning approach. This difference in performance has been found in other
domains, particularly movie recommendations [40].

9

02004006008000.100.150.200.250.300.35 Balanced Accuracyn_init =1, n_recs =1AverageCoClusteringKNN-dataKNN-metaKNN-mlRandomSlopeOneSVD02004006008000.100.150.200.250.300.35n_init =1, n_recs =1002004006008000.100.150.200.250.300.35n_init =1, n_recs =10002004006008000.100.150.200.250.300.35 Balanced Accuracyn_init =100, n_recs =102004006008000.100.150.200.250.300.35n_init =100, n_recs =1002004006008000.100.150.200.250.300.35n_init =100, n_recs =1000200400600800Iteration0.100.150.200.250.300.35 Balanced Accuracyn_init =10000, n_recs =10200400600800Iteration0.100.150.200.250.300.35n_init =10000, n_recs =100200400600800Iteration0.100.150.200.250.300.35n_init =10000, n_recs =100A PREPRINT - APRIL 29, 2020

Figure 4: Cumulative success rates across all datasets using
PennAI, AutoSklearn, and HyperOptSklearn. The success
rate is the fraction of datasets for which the recommender
has trained an algorithm conﬁguration that achieves a test set
balanced accuracy within 1 or 5% of the best performance
on that dataset.

Figure 5: ∆BA across all datasets, using PennAI, Au-
toSklearn, and HyperOptSklearn. Error bars indicate the
95% conﬁdence intervals.

Given 100 initial experiment results and 10 recommendations per iteration, the SVD recommender converges to
within 5% of the optimal performance in approximately 100 iterations, corresponding to approximately 7 training
instances per dataset. Note that the performance curves begin to increase on the right-most plots that correspond to 100
recommendations per dataset. In these cases, the recommender begins recommending algorithms conﬁgurations with
lower rankings due to repeat ﬁltering, described in Section 3.

5.1 Comparison to AutoML

In Fig. 4 and 5, we compare PennAI with SVD to AutoSklearn and Hyperopt, two widely used AutoML tools. We
choose to use the SVD algorithm for PennAI since it is shown to perform the best in terms of ∆ Balanced Accuracy in
our prior analysis. The side-by-side graphs of Fig. 4 show the percent of datasets for which a given method is able to
return an algorithm within 5% (left) and 1% (right) of the best performance, as a function of computational effort. The
results show AutoSklearn ﬁnding best conﬁgurations most often for the 5% threshold, and AutoSklearn and PennAI
ﬁnding best conﬁgurations at about the same rate for 1% threshold. In both cases, the performance of AutoSklearn and
PennAI differs only by a few percent, whereas HyperOptSklearn tends to under-perform those two methods. Fig. 5
considers the same results without a threshold, instead showing the average reduction in ∆ Balanced Accuracy (Eqn. 8)
across datasets for each method. By this metric, AutoSklearn performs the best, although its performance is not
signiﬁcantly different than that of PennAI, as observed by the overlap of their error bars.

5.2

Illustrative Example

The screenshots in Fig. 1 show the steps in the septic shock model ﬁtting procedure. In Table 2, the ﬁtted models are
detailed, including the ML conﬁguration, 5-fold CV AUROC score. Based on these scores, the gradient boosting model
shown in bold was selected and exported from PennAI. This model achieved an AUROC of 0.85 ± 0.02 on the test data,
compared to a mean of 0.86 ± 0.02 for the LSTM model trained for 100 epochs. An advantage of the PennAI-generated
model is that it took less than 2 minutes to train, whereas the LSTM model took more than 30 hours, using an NVIDIA
GeForce GTX 970. Two caveats of this time comparison are that 1) the LSTM is multi-task, i.e. it makes predictions for
25 different phenotypes; and 2) the results were trained on different hardware. The PennAI results were generated on a
single thread, Intel(R) Core(TM) i7-6950X CPU @ 3.00GHz. The LSTM results were generated using an NVIDIA
GeForce GTX 970 graphics processing unit. Nevertheless, the training time difference of 915x is substantial.

Both model performances are in a similar range to state-of-the-art early detection systems recently deployed to identify
septic shock in critical care patients [39]. Figure 6 shows the cross-validation ROC curves for the gradient boosting
model of shock, as well as a sensitivity analysis of the ﬁnal model. The two most important factors to prediction are the
mean Glasgow coma scale rating for the patient and their minimum systolic blood pressure reading. The importance of
these two factors lends credence to the model, since they are important for assessing septic shock [39]. The Glasgow

10

100101102103104Iterations for Hyperopt and PennAI2025303540455% Threshold Success RateHyperopt-SklearnPennAI SVD100101102103104Iterations for Hyperopt and PennAI5101520251% Threshold Success Rate102103104105Time (s) for Autosklearn102103104105Time (s) for AutosklearnAutoSklearn100101102103104Iterations for Hyperopt and PennAI0.100.150.200.250.30 Balanced Accuracy Across DatasetsPennAI SVDHyperopt-Sklearn102103104105Time (s) for AutoSklearnAutoSklearnA PREPRINT - APRIL 29, 2020

Table 2: Fitted models chosen by the user and AI for predicting shock. Area under the receiver operating curve
(AUROC) scores are reported with 95% conﬁdence intervals. For the selected model shown in bold, we report AUROC
on the test set, which is within 1% of state-of-the-art results for this task (LSTM, bottom row).
5-fold CV AUROC

Hyperparameters (Sklearn syntax)

Test AUROC

Source

Model

Training time

Logistic Regression

KNN Classiﬁer

Support Vector

Classiﬁcation Tree

Random Forest

Gradient Boosting

LSTM

penalty=L2, C=1.0, Dual=False
penalty=L1, C=1e-3, Dual=False
n_neighbors=1, weights=‘distance’
n_neighbors=7, weights=‘uniform’
n_neighbors=11, weights=‘uniform’
kernel=rbf, C=1.0, degree=3, gamma=0.01
kernel=polynomial, C=1.0, degree=3, gamma=0.01
criterion=entropy, max_depth=10
criterion=gini, max_depth=10
criterion=Gini, max_features=‘sqrt’, n_estimators=100
criterion=entropy, max_features=‘log2’, n_estimators=100
learning_rate=0.01, max_depth=5, n_estimators=500
learning_rate=0.1, max_depth=5, n_estimators=500
learning_rate=0.1, max_depth=3, n_estimators=500
learning_rate=1, max_depth=1, n_estimators=100
multi-task, 100 epochs

user
AI
AI
AI
AI
user
user
AI
user
user
AI
AI
AI
AI
AI
Harutyunyan et al.[38]

0.84 ± 0.03
0.81 ± 0.03
0.68 ± 0.02
0.81 ± 0.03
0.82 ± 0.03
0.70 ± 0.03
0.68 ± 0.03
0.75 ± 0.02
0.73 ± 0.06
0.88 ± 0.06
0.83 ± 0.06
0.90 += 0.05
0.89 += 0.05
0.89 += 0.05
0.88 += 0.06
-

-
-
-
-
-
-
-
-
-
-
-
0.85 ±0.02
-
-
-
0.86 ± 0.02

13 s
16 s
1 m 1 s
11 m 3 s
11 m 13 s
7 m 55 s
26 m 8 s
5 s
6 s
2 m 7 s
1 m 16 s
1 m 58 s
2 m 25 s
1 m 18 s
10 s
30 h 21 m

coma scale is an indicator of assessment of the patient’s consciousness and therefore a likely indicator for adverse
events. A drop in systolic blood pressure is a tell-tale signature of septic shock and is used as a clinical diagnostic [41].

6 Conclusion

In this paper we propose a data science tool for non-experts that generates increasingly reliable ML recommendations
tailored to the needs of the user. The web-based interface provides the user with an intuitive way to launch and monitor
analyses, view and understand results, and download reproducible experiments and ﬁtted models for ofﬂine use. The
learning methodology is based on a recommendation system that can learn from both cached and generated results.
We demonstrate through the experiments in this paper that collaborative ﬁltering algorithms can successfully learn to
produce intelligent analyses for the user starting with sparse data on algorithm performance. We ﬁnd in particular that a
matrix factorization algorithm, SVD, works well in this application area.

PennAI automates the algorithm selection and tuning problem using a recommendation system that is bootstrapped with
a knowledgebase of previous results. The default knowledgebase is derived from a large set of experiments conducted
on 165 open source datasets. The user can also conﬁgure their own knowledgebase of datasets and results catered
to their application area. In our application example, we used PennAI with a generic knowledgebase of datasets to
successfully train and validate a predictive model of septic shock; we found that PennAI was able to quickly suggest
a state-of-the-art model, with little user input. In the future, we hope to further improve PennAI’s ability to handle
domain-speciﬁc tasks by creating knowledgebases for particular areas such as electronic health records and genetics.

We also hope that PennAI can serve as a testbed for novel AutoML methodologies. In the near term we plan to extend
the methodology in the following ways. First, we plan to implement a focused hyperparameter tuning strategy that can
ﬁne-tune the models that are recommended by the AI, similar to AutoSklearn [5] or Hyperopt [6]. We plan to make
this process transparent to the user so that they may easily choose which models to tune and for how long. We also
plan to increasingly automate the data preprocessing, which is, at the moment, mostly up to the user. This can include
processes from imputation and data standardization to more complex operations like feature selection and engineering.

7 Acknowledgments

The authors would like thank the members of the Institute for Biomedical Informatics at Penn for their many contri-
butions. Contributors included Sharon Tartarone, Josh Cohen, Randal Olson, Patryk Orzechowski, and Efe Ayhan.
We also thank John Holmes, Moshe Sipper, and Ryan Urbanowicz for their useful discussions. The development of
this tool was supported by National Institutes of Health research grants (K99 LM013256-01, R01 LM010098 and R01
AI116794) and National Institutes of Health infrastructure and support grants (UC4 DK112217, P30 ES013508, and
UL1 TR001878).

References

[1] Manju Bansal. Big Data: Creating the Power to Move Heaven and Earth, 2014.

11

A PREPRINT - APRIL 29, 2020

Figure 6: Top: ROC curve for ﬁve validation folds of the ﬁnal septic shock model. Bottom: Permutation importance
values for dataset features, with larger values indicating features that are more important for prediction.

[2] Randal S. Olson, Moshe Sipper, William La Cava, Sharon Tartarone, Steven Vitale, Weixuan Fu, John H. Holmes,
and Jason H. Moore. A system for accessible artiﬁcial intelligence. Genetic Programming Theory and Practice,
2017. arXiv:1705.00594.

[3] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential Model-Based Optimization for General
Algorithm Conﬁguration. In Carlos A. Coello Coello, editor, Learning and Intelligent Optimization, Lecture
Notes in Computer Science, pages 507–523. Springer Berlin Heidelberg, 2011.

[4] Lars Kotthoff, Chris Thornton, Holger H. Hoos, Frank Hutter, and Kevin Leyton-Brown. Auto-WEKA 2.0:
Automatic model selection and hyperparameter optimization in WEKA. The Journal of Machine Learning
Research, 18(1):826–830, 2017.

[5] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank Hutter.
Efﬁcient and robust automated machine learning. In Advances in Neural Information Processing Systems, pages
2962–2970, 2015.

[6] Brent Komer, James Bergstra, and Chris Eliasmith. Hyperopt-sklearn: automatic hyperparameter conﬁguration

for scikit-learn. In ICML workshop on AutoML, 2014.

[7] Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore. Evaluation of a tree-based pipeline
optimization tool for automating data science. In Proceedings of the Genetic and Evolutionary Computation
Conference 2016, pages 485–492. ACM, 2016.

[8] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Practical automated
machine learning for the automl challenge 2018. In International Workshop on Automatic Machine Learning at
ICML, 2018.

12

A PREPRINT - APRIL 29, 2020

[9] Esteban Real. Using Evolutionary AutoML to Discover Neural Network Architectures, March 2018.
[10] Isabelle Guyon, U Paris-Saclay, Hugo Jair Escalante, Sergio Escalera, U Barcelona, Damir Jajetic, James Robert
Lloyd, and Nuria Macia. A brief Review of the ChaLearn AutoML Challenge:. JMLR Workshop and Conference
Proceedings, 64:21–30, 2016. ZSCC: NoCitationData[s0].

[11] James Bennett and Stan Lanning. The netﬂix prize. In Proceedings of KDD cup and workshop, volume 2007,

page 35. New York, NY, USA., 2007.

[12] Brent Smith and Greg Linden. Two decades of recommender systems at Amazon. com. Ieee internet computing,

21(3):12–18, 2017.

[13] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta,
Yu He, Mike Lambert, and Blake Livingston. The YouTube video recommendation system. In Proceedings of the
fourth ACM conference on Recommender systems, pages 293–296. ACM, 2010.

[14] Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. OBOE: Collaborative Filtering for AutoML

Initialization. arXiv:1808.03233 [cs, stat], August 2018. arXiv: 1808.03233.

[15] Nicolo Fusi, Rishit Sheth, and Melih Elibol. Probabilistic matrix factorization for automated machine learning. In

Advances in Neural Information Processing Systems, pages 3348–3357, 2018.

[16] Alistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li-wei H. Lehman, Mengling Feng, Mohammad Ghassemi,
Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G. Mark. MIMIC-III, a freely accessible critical
care database. Scientiﬁc Data, 3(1):1–9, May 2016. Number: 1 Publisher: Nature Publishing Group.

[17] Pavel B. Brazdil, Carlos Soares, and Joaquim Pinto Da Costa. Ranking learning algorithms: Using IBL and

meta-learning on accuracy and time results. Machine Learning, 50(3):251–277, 2003.

[18] Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. Metalearning: Applications to Data

Mining. Springer Science & Business Media, November 2008.

[19] Adithya Balaji and Alexander Allen. Benchmarking Automatic Machine Learning Frameworks. arXiv:1808.06492

[cs, stat], August 2018. arXiv: 1808.06492.

[20] David Stern, Horst Samulowitz, Ralf Herbrich, Thore Graepel, Luca Pulina, and Armando Tacchella. Collaborative

expert portfolio management. In Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence, 2010.

[21] Mustafa Mısır and Michèle Sebag. Alors: An algorithm recommender system. Artiﬁcial Intelligence, 244:291–314,

2017.

[22] Tiago Cunha, Carlos Soares, and André C. P. L. F. de Carvalho. Metalearning and Recommender Systems: A
literature review and empirical study on the algorithm selection problem for Collaborative Filtering. Information
Sciences, 423:128–144, January 2018.

[23] Lyle H. Ungar and Dean P. Foster. Clustering methods for collaborative ﬁltering.

In AAAI workshop on

recommendation systems, volume 1, pages 114–129, 1998.

[24] Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar, and David M. Pennock. Methods and metrics for cold-start
recommendations. In Proceedings of the 25th annual international ACM SIGIR conference on Research and
development in information retrieval, pages 253–260. ACM, 2002.

[25] Benjamin M. Marlin and Richard S. Zemel. Collaborative prediction and ranking with non-random missing data.

In RecSys, 2009.

[26] Yehuda Koren. Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model. In KDD,

page 9, 2008.

[27] Maciej Kula. Spotlight. GitHub, 2017.
[28] Randal S. Olson, William La Cava, Zairah Mustahsan, Akshay Varik, and Jason H. Moore. Data-driven Advice for
Applying Machine Learning to Bioinformatics Problems. In Paciﬁc Symposium on Biocomputing (PSB), August
2017. arXiv: 1708.05070.

[29] Nicolas Hug. Surprise, a Python library for recommender systems. 2017.
[30] T. George and S. Merugu. A Scalable Collaborative Filtering Framework Based on Co-Clustering. In Fifth IEEE
International Conference on Data Mining (ICDM’05), pages 625–628, Houston, TX, USA, 2005. IEEE.
[31] Robert M. Bell, Yehuda Koren, and Chris Volinsky. The bellkor solution to the netﬂix prize. KorBell Team’s

Report to Netﬂix, 2007.

[32] Robert M. Bell and Yehuda Koren. Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpola-

tion Weights. In icdm, volume 7, pages 43–52. Citeseer, 2007.

13

A PREPRINT - APRIL 29, 2020

[33] Genevieve Gorrell. Generalized Hebbian algorithm for incremental singular value decomposition in natural
language processing. In 11th conference of the European chapter of the association for computational linguistics,
2006.

[34] Daniel Lemire and Anna Maclachlan. Slope One Predictors for Online Rating-Based Collaborative Filtering.

arXiv:cs/0702144, February 2007. arXiv: cs/0702144.

[35] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore. PMLB: A
Large Benchmark Suite for Machine Learning Evaluation and Comparison. BioData Mining, 2017. arXiv preprint
arXiv:1703.00512.

[36] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: Networked Science in Machine

Learning. SIGKDD Explor. Newsl., 15(2):49–60, June 2014.

[37] Digna R. Velez, Bill C. White, Alison A. Motsinger, William S. Bush, Marylyn D. Ritchie, Scott M. Williams,
and Jason H. Moore. A balanced accuracy function for epistasis modeling in imbalanced datasets using multi-
factor dimensionality reduction. Genetic Epidemiology: The Ofﬁcial Publication of the International Genetic
Epidemiology Society, 31(4):306–315, 2007. ZSCC: 0000325.

[38] Hrayr Harutyunyan, Hrant Khachatrian, David C. Kale, Greg Ver Steeg, and Aram Galstyan. Multitask learning
and benchmarking with clinical time series data. Scientiﬁc Data, 6(1):96, December 2019. arXiv: 1703.07771.

[39] Katharine E. Henry, David N. Hager, Peter J. Pronovost, and Suchi Saria. A targeted real-time early warning score
(TREWScore) for septic shock. Science translational medicine, 7(299):299ra122–299ra122, 2015. Publisher:
American Association for the Advancement of Science.

[40] István Pilászy and Domonkos Tikk. Recommending New Movies: Even a Few Ratings Are More Valuable Than
Metadata. In Proceedings of the Third ACM Conference on Recommender Systems, RecSys ’09, pages 93–100,
New York, NY, USA, 2009. ACM. event-place: New York, New York, USA.

[41] Mervyn Singer, Clifford S. Deutschman, Christopher Warren Seymour, Manu Shankar-Hari, Djillali Annane,
Michael Bauer, Rinaldo Bellomo, Gordon R. Bernard, Jean-Daniel Chiche, Craig M. Coopersmith, Richard S.
Hotchkiss, Mitchell M. Levy, John C. Marshall, Greg S. Martin, Steven M. Opal, Gordon D. Rubenfeld, Tom
van der Poll, Jean-Louis Vincent, and Derek C. Angus. The Third International Consensus Deﬁnitions for Sepsis
and Septic Shock (Sepsis-3). JAMA, 315(8):801–810, February 2016.

[42] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, and others. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12(Oct):2825–2830, 2011.

Supplementary Material

System Architecture

PennAI is a multi-component architecture that uses a variety of technologies including Docker7, Python, Node.js8,
scikit-learn9, FGLab 10, and MongoDb11. The architecture is shown in Figure 7. The project contains multiple docker
containers that are orchestrated by a docker-compose ﬁle. The central component is the controller engine, a server
written in Node.js. This component is responsible for managing communication between the other components using a
REST API. A MongoDb database is used for persistent storage. The UI component is a web application written in
javascript that uses the React library to create the user interface and the Redux library to manage UI state. The interface
supports user interactions including uploading datasets for analysis, requesting AI recommendations for a dataset,
manually specifying and running machine learning experiments, and displaying experiment results in an intuitive way.
The AI engine is written in Python. As users make requests to perform analysis on datasets, the AI engine will generate
new machine learning experiment recommendations and communicate them to the controller engine. The AI engine
contains a knowledgebase of previously run experiments, results, and dataset metafeatures that it uses to inform the
recommendations it makes. The knowledgebase is bootstrapped with a collection of experiment results generated
from the PMBL benchmark datasets. Instructions and code templates are provided to allow easy integration of custom
recommendation systems. The machine learning component is responsible for running machine learning experiments

7https://www.docker.com/
8https://nodejs.org
9http://sklearn.org
10https://kaixhin.github.io/FGLab/
11https://www.mongodb.com/

14

A PREPRINT - APRIL 29, 2020

Figure 7: Diagram describing the architecture of PennAI.

on datasets. It has a Node.js server that is used to communicate with the controller engine, and uses python to execute
Scikit-learn experiments on datasets and communicate results back to the central server. A PennAI instance can support
multiple instances of machine learning engines, enabling multiple experiments to be run in parallel.

Additional Experiment Details

In Table 3, the parameter spaces of each ML algorithm that was used in our experimental analysis is shown. In total,
there were 7580 combinations.

8 Additional Results

The ﬁnal set of plots in Fig. 8 show the frequency with which SVD, KNN-ML, and SlopeOne recommend different
algorithms in comparison to the frequency of top-ranking algorithms by type (the top left plot). Here we see that
SVD gradually learns to recommend the top ﬁve algorithms in approximately the same ranking as they appear in the
knowledgebase. This lends some conﬁdence to the relationship that SVD has learned between algorithm conﬁgurations
and dataset performance.

15

A PREPRINT - APRIL 29, 2020

Figure 8: Heatmaps of three different recommendation strategies on the PMLB benchmark showing how often each
ML was recommended each iteration. These plots show the experiment treatment with nrecs = 10 and ninit = 100.
The top ﬁgure shows the number of datasets for which each ML algorithm has a conﬁguration that is within 1%
of the best performance on that dataset. The second ﬁgure shows SVD recommendations; over several iterations,
it learns to approximate the frequency distribution of best ML models, i.e. GradientBoostingClassiﬁer, followed
by RandomForestClassiﬁer and ExtraTreesClassiﬁer. The third plot shows KNN-ml recommendations; in this case,
SGDClassiﬁer ends up being recommended more often than is supported by its benchmark performance. The ﬁnal plot
shows the performance of Average recommender; the distribution of algorithm recommendations is more wide, and
tends to drift away from the best algorithm choices after several iterations.

16

020406080# Datasets with Nearly Optimal ResultsSVCSGDClassifierRandomForestClassifierLogisticRegressionKNeighborsClassifierGradientBoostingClassifierExtraTreesClassifierDecisionTreeClassifierBernoulliNBAdaBoostClassifierAlgorithmIterationAdaBoostClassifierBernoulliNBDecisionTreeClassifierExtraTreesClassifierGradientBoostingClassifierKNeighborsClassifierLogisticRegressionRandomForestClassifierSGDClassifierSVCAlgorithmSVDlowmedhigh# RecommendationsIterationAdaBoostClassifierBernoulliNBDecisionTreeClassifierExtraTreesClassifierGradientBoostingClassifierKNeighborsClassifierLogisticRegressionRandomForestClassifierSGDClassifierSVCAlgorithmKNN-mllowmedhigh# RecommendationsIterationAdaBoostClassifierBernoulliNBDecisionTreeClassifierExtraTreesClassifierGradientBoostingClassifierKNeighborsClassifierLogisticRegressionRandomForestClassifierSGDClassifierSVCAlgorithmAveragelowmedhigh# RecommendationsA PREPRINT - APRIL 29, 2020

Table 3: Analyzed algorithms with their parameters settings. The methods and parameters are named according to
Scikit-learn nomenclature[42].

Algorithm name

AdaBoostClassiﬁer

BernoulliNB

DecisionTreeClassiﬁer

ExtraTreesClassiﬁer

Parameter

learning_rate
n_estimators

alpha
ﬁt_prior
binarize

min_weight_fraction_leaf
max_features
criterion

n_estimators
min_weight_fraction_leaf
max_features
criterion

Values

[0.01, 0.1, 0.5, 1.0, 10.0, 50.0, 100.0]
[10, 50, 100, 500, 1000]

[0.0, 0.1, 0.25, 0.5, 0.75, 1.0, 5.0, 10.0, 25.0, 50.0]
[’true’, ’false’]
[0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]

[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]
[0.1, 0.25, 0.5, 0.75, ’log2’, None, ’sqrt’]
[’entropy’, ’gini’]

[10, 50, 100, 500, 1000]
[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]
[0.1, 0.25, 0.5, 0.75, ’log2’, None, ’sqrt’]
[’entropy’, ’gini’]

GradientBoostingClassiﬁer

KNeighborsClassiﬁer

LogisticRegression

MultinomialNB

PassiveAggressiveClassiﬁer

RandomForestClassiﬁer

SGDClassiﬁer

SVC

loss
learning_rate
n_estimators
max_depth
max_features

n_neighbors
weights

C
penalty
ﬁt_intercept
dual

alpha
ﬁt_prior

C
loss
ﬁt_intercept

[’deviance’]
[0.01, 0.1, 0.5, 1.0, 10.0]
[10, 50, 100, 500, 1000]
[1, 2, 3, 4, 5, 10, 20, 50, None]
[’log2’, ’sqrt’, None]

[1, 2, . . . , 25]
[’uniform’, ’distance’]

[0.5, 1.0, . . . , 20.0]
[’l2’, ’l1’]
[’true’, ’false’]
[’true’, ’false’]

[0.0, 0.1, 0.25, 0.5, 0.75, 1.0, 5.0, 10.0, 25.0, 50.0]
[’true’, ’false’]

[0.0, 0.001, 0.01, 0.1, 0.5, 1.0, 10.0, 50.0, 100.0]
[’hinge’, ’squared_hinge’]
[’true’, ’false’]

n_estimators
min_weight_fraction_leaf
max_features
criterion

[10, 50, 100, 500, 1000]
[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]
[0.1, 0.25, 0.5, 0.75, ’log2’, None, ’sqrt’]
[’entropy’, ’gini’]

loss
penalty
alpha
learning_rate
ﬁt_intercept
l1_ratio
eta0
power_t

C
gamma
kernel
degree
coef0

[’hinge’, ’perceptron’, ’log’, ’squared_hinge’, ’modiﬁed_huber’]
[’elasticnet’]
[0.0, 0.001, 0.01]
[’constant’, ’invscaling’]
[’true’, ’false’]
[0.0, 0.25, 0.5, 0.75, 1.0]
[0.01, 0.1, 1.0]
[0.0, 0.1, 0.5, 1.0, 10.0, 50.0, 100.0]

[0.01]
[0.01]
[’poly’]
[2, 3]
[0.0, 0.1, 0.5, 1.0, 10.0, 50.0, 100.0]

17

