0
2
0
2

r
a

M
1
2

]

G
L
.
s
c
[

1
v
5
1
6
9
0
.
3
0
0
2
:
v
i
X
r
a

DP-Net: Dynamic Programming Guided Deep
Neural Network Compression

Dingcheng Yang*, Wenjian Yu*, Ao Zhou**, Haoyuan Mu*, Gary Yao*** and Xiaoyi Wang**

*BNRist, Dept. Computer Science & Tech., Tsinghua University
**Beijing Engineering Research Center for IoT Software and Systems, Beijing University of Technology
***Department of Electrical Engineering and Computer Science, Case Western Reserve University
{ydc19, muhy17}@mails.tsinghua.edu.cn;yu-wj@tsinghua.edu.cn
S201861539@emails.bjut.edu.cn;wxy@bjut.edu.cn
gxy76@case.edu

Abstract—In this work, we propose an effective scheme (called
DP-Net) for compressing the deep neural networks (DNNs). It
includes a novel dynamic programming (DP) based algorithm to
obtain the optimal solution of weight quantization and an opti-
mization process to train a clustering-friendly DNN. Experiments
showed that the DP-Net allows larger compression than the state-
of-the-art counterparts while preserving accuracy. The largest
77X compression ratio on Wide ResNet is achieved by combining
DP-Net with other compression techniques. Furthermore, the
DP-Net is extended for compressing a robust DNN model with
negligible accuracy loss. At last, a custom accelerator is designed
on FPGA to speed up the inference computation with DP-Net.

Index Terms—Dynamic Programing, Neural Network Com-

pression, Robust Model, Weight Quantization.

I. INTRODUCTION

Deep neural networks (DNNs) have been demonstrated to
be successful on many tasks. However,
the size of DNN
model has continuously increased while it achieves better
performance. As a result, the storage space of DNN becomes a
major concern if we deploy it on resource-constrained devices,
especially in edge-computing applications like face recognition
on mobile phone and autonomous driving.

In recent years, there are many studies on compressing DNN
models. The proposed techniques consist of pruning [1], [2],
knowledge distillation [3], low-bit neural network [4], compact
architectures [5] and weight quantization [6]. Two kinds of
weight quantization method, vector quantization and scalar
quantization, were proposed in [7]. The scalar quantization
was then employed in several work on model compression
[6], [8], [9]. Low-bit representation of neural network can be
regarded as a variant of scalar quantization, which restricts
the weights to low-bit ﬂoating-point numbers [4]. Therefore,
an accurate weight quantization provides the upper bound of
performance for the corresponding low-bit representation.

In previous work, the K-means clustering solved with the
Lloyd’s algorithm [10] was used for the weight quantization.
However, the Lloyd’s algorithm is heuristic. Its result is not
optimal and is sensitive to the initial solution, as revealed by
experiments in [8]. In this work, we consider the K-means
clustering in scalar quantization, but do not use the Lloyd’s

algorithm. Notice the K-means clustering of multi-dimensional
data is NP-hard [11]. A key contribution of this work is a
dynamic programming (DP) based algorithm with O(n2K)
complexity, which produces the optimal solution of the scalar
quantization problem. Its advantages over the Lloyd’s algo-
rithm lead to better performance in DNN compression.

On the other hand, recent work shows DNNs are vul-
nerable to adversarial examples [12], which can be crafted
by adding visually imperceptible perturbations on images. A
robust model against the adversarial attacks is needed for
security-critical tasks, and becomes a focus [13]. However,
there is few work devoted to compress the robust DNN model.
We demonstrate the proposed DP-Net compression scheme
is also applicable to the robust DNN model, enabling large
compression with negligible accuracy drop.

To validate the speedup in inference brought by the DP-Net,
we have also designed a custom accelerator with speciﬁc im-
plementation of matrix-vector multiplication. The experiments
on FPGA show that we can accelerate the computation with
fully-connected (FC) layer and convolutional layer remarkably.
The major contributions of this work are as follows.

1) A DP based algorithm is proposed to obtain the op-
timal solution of scalar quantization. Based on it and
a clustering-friendly training process, a compression
scheme called DP-Net is proposed, which exhibits larger
compression of DNNs like FreshNet, GoogleNet than
Deep K-means [14] and other counterparts. Our exper-
iment show that with DP-Net, pruning and Huffman
coding, the storage of Wide ResNet is reduced by 77X,
while achieving higher accuracy than Deep K-means.
2) DP-Net is also extended to the state-of-the-art TRADES
model for robust DNNs [13], and achieves 16X compres-
sion with negligible accuracy loss for ResNet-18.

3) To show the inference acceleration brought by DP-Net,
we have designed a custom accelerator on FPGA. Ex-
periments on GoogleNet reveal that at least 5X speedup
can be achieved for inference computations.

 
 
 
 
 
 
Fig. 1. An example of storage formats. The top table represents the normal
storage for 9 32-bit ﬂoating-point numbers. The bottom table represents the
quantized storage format, where only two 32-bit ﬂoating-point numbers (3.5
and 7.2) are stored along with 9 bits indicating which value each number is.

II. PROBLEM FORMULATION

Regard the weights of DNN as a sequence of vectors
W = {W1, W2, · · · , Wm}, where Wi ∈ Rni. We consider the
scalar weight quantization problem and express the clustering
result as C = [C1, C2, · · · , Cm] ∈ RK×m, where K is the
number of clusters and column vector Ci contains the K
cluster centers. Without the quantization and using 32 bit
ﬂoating-point numbers, the number of bits used for storage
is 32 (cid:80)m
i=1 ni. After the quantization, each element of Wi is
one of the K elements in Ci. So, we just need log2 K·(cid:80)m
i=1 ni
bits to encode the index and 32mK bits to store the cluster
centers. Fig. 1 provides an example of the storage formats.
This scalar quantization leads to the DNN compression ratio:
32 (cid:80)m

i=1 ni

r =

log2 K · (cid:80)m

i=1 ni + 32mK

(1)

Training a DNN with consideration of weight quantization
can be formulated as an optimization problem. If the loss
function of training DNN is denoted by f (W ), the problem
formulation should be:

minW,C f (W ),
s. t. C = [C1, C2, · · · , Cm] ∈ RK×m,
W = {W1, W2, · · · , Wm} , and ∀i, j, Wi,j ∈ Ci .

(2)

The constraint means every element in column vector Wi
appears in the vector Ci containing cluster centers.

III. THE DP-NET SCHEME FOR DNN COMPRESSION

Before presenting the DP-Net scheme, we show the draw-
back of the conventional weight quantization. In Fig. 2(a),
the weights in the ﬁrst convolution layer of the FreshNet [9]
are visualized. It shows that the weights follow the Gaussian
distribution, which is consistent with some Bayesian methods
assuming that the neural network parameters are Gaussian
[15] and the phenomenon observed by [9] that the weights of
learned convolutional ﬁlters are typically smooth. This means
that the learned weights may be unsuitable for clustering or
quantization. Thus, the network has to be retrained to com-
pensate the accuracy loss caused by quantization [8]. However,
for each weight the cluster it belongs to is ﬁxed during the
retraining. This is the same as what’s done for HashedNet [6],
i.e randomly grouping each weight and then training, except
for a different initial solution. Fixing the clusters means a ﬁxed
network architecture. This is disadvantageous, because recent
study has shown that the architecture of a network is more
important than the initial solution [16].

(a)

(b)

Fig. 2. The histograms of weights in the FreshNet’s ﬁrst convolution layer
obtained from (a) a normal training, and (b) a clustering-friendly training.

Inspired by Deep K-means [14], we propose to directly
optimize the Lagrangian function of (2) during the training
process. The problem becomes an unconstrained optimization:

f (W ) + λ

min
W,C

m
(cid:88)

ni(cid:88)

i=1

j=1

min
1≤k≤K

(Wi,j − Ci,k)2,

(3)

where λ is the Lagrange multiplier. By minimizing this new
loss function, we can train a clustering-friendly network (an
example is shown in Fig. 2(b)). This is different from Deep
K-means, as we are not optimizing the relaxed form of (3)
and thus achieve better accuracy. The idea of DP-Net is to
perform such a clustering-friendly training through alterna-
tively optimizing W and C, where after every t epochs of
stochastic gradient descent (SGD) based optimization of W ,
we optimize C by solving a scalar K-means clustering. The
novelty of DP-Net also includes a dynamic programming (DP)
based algorithm to obtain the optimal solution of the scalar
clustering, which is employed again to perform the weight
quantization after training.

A. DP Based Algorithm for Scalar Quantization

Although the K-means clustering of d-dimensional data
is NP-hard for any d ≥ 2, the optimal solution of scalar
quantization (corresponding to the clustering with d = 1) can
be obtained in polynomial time, based on DP and Theorem 1.

Theorem 1. Let x1 ≤ x2 ≤ · · · ≤ xn be n scalars which
need to be clustered into K classes. The clustering result is
expressed as the index set p = {p1, p2, · · · , pn}, which means
xi belongs to the pi-th cluster (1 ≤ pi ≤ K). If the loss
function of the clustering is

g(p) =

n
(cid:88)

(xi − cpi)2 ,

i=1

(4)

where c1 < c2 < · · · < cK are cluster centers, an optimal
solution p satisﬁes: 1 = p1 ≤ p2 ≤ · · · ≤ pn = K.

Proof. Suppose the ascending array {ci} are the cluster centers
for the optimal solution. Let c(cid:48)
2 =
(c2 + c3)/2, · · · , c(cid:48)
K = ∞. We can
construct a clustering by setting pi = j, if c(cid:48)
j−1 ≤ xi < c(cid:48)
j.
Obviously, pi minimizes g(p) and corresponds to an optimal
solution satisfying 1 = p1 ≤ p2 ≤ · · · ≤ pn = K.

K−1 = (cK−1 + cK)/2, c(cid:48)

1 = (c1 +c2)/2, c(cid:48)

0 = −∞, c(cid:48)

Theorem 1 infers that by suitable interval partition we can
get the optimal clustering of the weights (see Fig. 3). Let x1 ≤

BitsValueValueValueValueValueValueValueValueValue323.53.57.27.27.23.53.53.57.2BitsValueValueValueValueValueValueValueValueValue1001110001normalquantized323.57.2                                                               · · · ≤ xN be the sorted weights, and Gn,k be the minimum
loss for clustering the ﬁrst n weights into k clusters. We have

Gn,k = min

k−1≤i<n

Gi,k−1 + h(i + 1, n),

(5)

(cid:80)q

i=l(xi − c)2,
where k − 1 ≤ i < n and h(l, q) = minc
it means the minimum clustering error (loss) for clustering
xl, xl+1, · · · , xt to one cluster. The situation when n = k is
trivial, and we have Gk,k = 0. So, we consider the situations
with n > k. To pursue the optimal clustering, we need to
enumerate all possible i which represents the largest index of
scalar not belonging to the k-th cluster. Then, the minimum
quantization error has two parts. One is the minimum quanti-
zation error that clustering x1, · · · , xi into k − 1 clusters, i.e.
Gi,k−1. The other part is the minimum quantization error that
quantizing xi+1, · · · , xn into one cluster, which is h(i + 1, n).
The latter part can be easily calculated, and the mean of scalars
should be the cluster center. So,

h(l, q) =

=

q
(cid:88)

i=l
q
(cid:88)

i=l

(xi −

1
q − l + 1

q
(cid:88)

j=l

xj)2

x2
i − (

q
(cid:88)

i=l

xi)2/(q − l + 1)

(6)

Based on (5) and (6) and utilizing the dynamic programming
skills [17], we derive Algorithm 1 for optimally solving the
scalar quantization problem. It should be pointed out that, this
algorithm can be easily extended to other kinds of clustering,
such as that the L2 norm in the loss function (4) is replaced
with L1 norm. The time complexity of Alg. 1 is O(n2K). We
will make sure that n is not very large when applying it to the
weight quantization.

B. More Details of DP-Net

For an FC layer with an nf c × mf c matrix, we divide
the parameters into nf c parts, each part contains n = mf c
weights, which means we cluster the weights row by row. For
a convolutional layer with an nconv × mconv × hconv × wconv
tensor, we divide the parameters into nconv parts and each part
contains n = mconv × hconv × wconv weights. In this way, the
number of weights on which we do clustering will be no more
than 10000 for most mainstream DNN. So, the computational
time for the DP based algorithm is much less than that for
training the DNN. We call this quantization manner a ﬁne-
grained scalar quantization. The compression ratio of the ﬁne-
grained quantization is less than that of quantizing all weights
as a whole, but the drop is small. We can see from (1) that
the summation of ni log2 K terms dominates the denominator.
Thus, the drop of compression ratio is negligible.

In spite of small drop of compression ratio, the ﬁne-grained
quantization can improve the accuracy. Clustering an FC layer

Fig. 3.

Illustration of the optimal solution of clustering n scalars.

Algorithm 1 DP based scalar quantization
Input: n scalars x1 ≤ x2 · · · ≤ xn, number of clusters K.
Output: The K cluster centers in the optimal solution.

1: Deﬁne an n × K array G, whose elements are all ∞.
2: Deﬁne an n × K array L, whose element Li,k represents
the index of the smallest scalar within the same class as
xi, in the optimal solution of clustering the ﬁrst i scalars
into k classes.

3: G0,0 ← 0
4: for i ← 1 to n do
5:
6:

s1 ← 0, s2 ← 0
for j ← i downto 1 do

(cid:46) temporary variable

s1 ← s1 + xj, s2 ← s2 + x2
j

(cid:46) s1 = (cid:80)i

k=j xk, s2 = (cid:80)i

c ← s2 − s2
for k ← 1 to K do

1/(i − j + 1)

if Gi,k > Gj−1,k−1 + c then
Gi,k ← Gj−1,k−1 + c
Li,k ← j

k=j x2
k
(cid:46) Eq. (6)

(cid:46) Eq. (5)

7:

8:
9:
10:

end if

end for

11:
12:
13:
14:
15:
16: end for
17: centers ← [ ]
18: while K > 0 do
19:

end for

20:
21:
22: end while
23: return centers

l ← Ln,K
centers.insert(
n ← L − 1, K ← K − 1

(cid:80)n
i=l xi
n−l+1 )

(cid:46) The center for last cluster.

with an n × m matrix into 8 classes with the ﬁne-grained
quantization can be seen as clustering nm scalars into 8n
clusters. This greatly expand the parameter space, compared
with clustering all the weights into 8 classes as a whole.

C. Extension for Compressing Robust DNN Model

The idea of making DP-Net applicable to the robust DNN
models is to add the quantization error, i.e. the minimum
loss Gn,K, as a regularization term to the loss function
optimized during the training process. We consider the state-
of-the-art TRADES (TRadeoff-inspired Adversarial DEfense
via Surrogate-loss minimization) model for robust DNN [13].
The new formulation for training becomes:

min
W,C

+ λ

{L(f (X; W ), Y ) + max

X (cid:48)∈B(X,(cid:15))

γL(f (X; W ), f (X (cid:48); W ))

m
(cid:88)

ni(cid:88)

i=1

j=1

min
1≤k≤K

(Wi,j − Ci,k)2},

(7)

where f (X; W ) is the output vector of learning model given
parameters W , L(x, y) is the cross-entropy loss function and γ
is a regularization parameter to trade off between accuracy and
robustness. With this formulation, we can train a clustering-
friendly robust model.

𝑥2𝑥3𝑥4𝑥5𝑥7𝑥8𝑥6𝑥9𝑥𝑛−1𝑥𝑛𝑐1……𝑐2𝑐3𝑐𝐾𝑥1D. Inference Acceleration

There are existing work on accelerating neural network
on FPGA [18] and CPU [19]. In [18], the DNN model is
compressed by scalar quantization so that it becomes small
enough to be stored in SRAM, thus increasing the speed of
memory access. In addition to this beneﬁt, speciﬁc technique
for DP-Net can be developed for more inference acceleration.
In the inference stage the major computation can be decom-
posed as matrix-vector multiplications, for the FC layer or the
convolutional layer. Each matrix-vector multiplication consists
of many vector dot products. For simplicity, we just consider
accelerating the vector dot product. For two n-dimensional
vectors a, b, normally we need to perform n additions and n
multiplications to make the dot product. With DP-Net, one
vector, say b, is presented as [cp1, cp2 , · · · , cpn]T , where c is a
K-dimensional vector and p is the n-dimensional index vector
(1 ≤ pi ≤ K). Then, we have aT b = (cid:80)K
ai),
where Si is the set of indices j for which pj = i, and we only
need to perform K multiplications. This brings acceleration.
is deployed on FPGA, signiﬁcant speedup can
If DP-Net
be expected, as multiplication is executed much slower than
addition in FPGA. Notice n is usually larger than 1000, while
K is no more than 16 in our experiments.

k=1 ck((cid:80)

i∈Sk

IV. EXPERIMENTAL RESULTS

In this section, we present

the experimental results to
demonstrate the effectiveness of DP-Net. We conducted ex-
periments on CIFAR-10 [20] and compared DP-Net with
two baselines for compressing TTConv [21] and FreshNet
[9]. The ﬁrst baseline is Deep K-means [14], which is the
work most relevant to ours. The second baseline is using
the Lloyd’s algorithm to do clustering during the training of
DP-Net. We call it KMeans-Net. Furthermore, we used the
three-stage pipeline to compress Wide ResNet [22]. Then, we
did experiments on the ImageNet dataset, for compressing
GoogleNet. We also applied our method to the robust models.
The results show that our extended DP-Net works well and
has good performance. Finally, we show the result of the
custom accelerator for DP-Net on FPGA. In all experiments,
the compression ratio (CR) is obtained with (1) and then
rounded to an integer.

A. Experiment Setup

DP-Net has only two hyperparameters: λ, the regularizier
factor in (3); t, the clustering frequency during training. We
choosed λ = 100 for all experiments if not explicitly stated.
The value of t varies for different datasets because it affects
the number of training epochs for the model to converge.

1) On CIFAR-10: We choosed t = 20 for all experiments
on CIFAR-10 with normal model and used SGD with cosine
annealing for training. The learning rate reduces from 0.05
to 0.01 for TTConv and FreshNet. As for Wide ResNet, the
learning rate is initialized at 0.1. The inference accuracy of
the pretrained models we obtained for TTConv, FreshNet and
Wide ResNet are 91.45%, 87.51%, 93.58%, respectively. They
are higher or equal to those reported in [14]. For TTConv

and Wide ResNet, we trained 300 epochs. Because FreshNet
is prone to overﬁtting [9], we just trained 150 epochs and
obtained a better result than training 300 epochs. The number
of training epochs used for DP-Net is consistent with the
pretrained model. We use random cropping, random horizontal
ﬂipping and cutout for data augmentation.

2) On ImageNet: Our experiment settings are consistent
with the PyTorch example1 except training epochs. We use
the pretrained GoogleNet model provided by PyTorch, whose
top-1 accuracy is 69.78% and top-5 accuracy is 89.53%. The
PyTorch’s ofﬁcial example trained 90 epochs, and made the
learning rate decay 10X every 30 epochs. We just trained 30
epochs and made the learning rate decay 10X every 10 epochs
because the pretrained model provided a good initial solution.
We choose t = 3 for ImageNet because 3 epochs are enough
for SGD to converge when the cluster centers are ﬁxed.

3) For Robust Model: We did experiments on the robust
models trained by TRADES [13]. The training settings and
evaluating settings are consistent with their public code2. We
choose t = 5 because their training epochs are small. At ﬁrst,
we used a small network which consists of four convolutional
layers and three FC layers. This network is proposed by
[23]. We called it SmallCNN and trained it on the MNIST
dataset. The pretained model achieved 99.46% accuracy on
testing dataset and 96.87% accuracy under a powerful attack
algorithm named PGD [24]. Then, we trained a robust ResNet-
18 model on CIFAR-10, which achieved 92.15% accuray on
testing dataset and 40.29% accuracy under the PGD attack.

4) Custom Accelerator: As a hardware platform, the Zynq
SoC (XC7Z045FFG900-2) is used, mounted on the ZC706-
evaluation-board, and operating at 100MHz. It offers 218,600
Look-Up Tables (LUTs), 437,200 Flip-Flops, 545 units of 36k
block RAM (BRAM) and 900 DSP48E units. The Vivado
2018.2 is used for RTL behavioral simulation.

B. Result on CIFAR-10

In this subsection, we show the effectiveness of the proposed

DP-Net for compressing several DNNs on CIFAR-10.

1) Compressing TT-Conv: The TT-Conv model [21] con-
tains six convolutional layers and one FC layer. The authors
of TT-Conv used Tensor Train Decomposition to compress
the convolutional layer by 4X while the accuracy decreases
by 2%. Deep K-means [14] achieves a better result while
compressing 4X, with less accuracy loss. Our experimental
results are listed in Table I. From it we see that with only 3
bits used to represent a scalar (equivalent to CR of 10X), there
is no or negeligible accuracy loss.

The results in Table I also demonstrate two interesting
the accuracy of DP-Net with 3 bits is
phenomena. First,
1.18% higher than KMeans-Net, which means that using the
Lloyd’s algorithm instead of DP greatly reduces the accuracy.
This phenomenon veriﬁes the value of the proposed DP
based algorithm. Second, our DP-Net performs better than

1https://github.com/pytorch/examples/tree/master/imagenet
2https://github.com/yaodongyu/TRADES

the pretrained network even after the quantizing process,
possibly because the regularized factor can prevent network
from overﬁtting, like what a L2-norm regularized factor often
behaves. Therefore, we believe that performing a suitable
weight quantization can not only reduce the size of model,
but also prevent overﬁtting and improve performance.

TABLE I
THE RESULTS OF COMPRESSING TT-CONV. ∆ MEANS THE CHANGE OF
ACCURACY COMPARED TO THE PRETRAINED MODEL.

Model
TT Decomposition [21]
Deep K-means [14]
Deep K-means [14]
KMeans-Net (3 bits)
DP-Net (3 bits)

CR ∆(%)
-2.0
4
+0.05
2
-0.04
4
-0.87
10
+0.31
10

The λ and t are hyperparameters in our method. We
conducted two experiments to study the sensitivity of our
method to them, with the results plotted in Fig. 4. From
the ﬁgure we see that DP-Net is not very sensitive to the
hyperparameters. The phenomenons mentioned previously still
exist. DP-Net always produces higher accuracy than KMeans-
Net under the same conﬁgurations in all experiments. And,
most DP-Net models with 3-bit representation performs better
than their pretrained models. This shows that a clustering-
friendly network with appropriate number of clusters may have
better generalization than normal network, which make our
method meaningful even when compression is not needed.

(a)

(b)

Fig. 4. The results of compressing TT-Conv. (a) The accuracy change for
varied λ with t = 20. (b) The accuracy change for varied t with λ = 100.

2) Compressing FreshNet: In [9], it is observed that the
learned convolutional weights are smooth and low-frequency.
So, they quantized the weights of their self-designed model
called FreshNet on frequency domain, and then trained the
quantized network. This model’s accuracy decreased by 6.51%
when the CR is 16; with the same CR the accuracy is only
reduced by 1.3% if Deep K-means is used. This veriﬁes that
training a clustering-friendly network is better than training
a quantized network. The comparison between DP-Net and
related work are shown in Table II. From it we see that
DP-Net’s accuracy is the highest, and much better than the
methods in [9] and the Deep K-means [14] with same CR
equal to 16. We think this shows that the scalar quantization
is better in preserving accuracy, which is consistent with the
experimental results on TT-Conv.

3) Compressing Wide ResNet: We compressed Wide
ResNet [22] by combining pruning, quantization and Huffman

TABLE II
THE RESULTS OF COMPRESSING FRESHNET. ∆ MEANS THE CHANGE OF
ACCURACY COMPARED TO THE PRETRAINED MODEL.

Model
Hashed Net [6]
FreshNet [9]
Deep K-means [14]
DP-Net (2 bits)
KMeans-Net (2 bits)

CR ∆(%)
-9.79
16
-6.51
16
-1.3
16
-0.57
16
-0.76
16

coding (the three-stage pipeline). The sparsity for each layer
and the pruning method are the same as what was done in
[14], where a total CR=47 is achieved. So, the only difference
is we replace the Deep K-means with our DP-Net for the
weight quantization step. The results are listed in Table III. It
shows that when the CR reaches 50, the accuracy of the three-
stage pipeline including Deep K-means suffered signiﬁcant
decrease. In contrast, using the proposed DP-Net yields an
accuracy loss less than 3% even at a compression ratio of
77X. Our accuracy drop is also remarkably less than that using
Deep K-means at CR=50.

TABLE III
THE RESULTS OF COMPRESSING WIDE RESNET WITH THE THREE-STAGE
PIPELINES. ∆ MEANS THE CHANGE OF ACCURACY COMPARED TO THE
PRETRAINED MODEL.

Model
Deep K-means [14]
Deep K-means [14]
DP-Net (3 bits)
DP-Net (2 bits)

CR ∆(%)
-2.23
47
-4.49
50
-1.71
50
77
-2.94

C. Result on ImageNet

We also evaluated our method on GoogleNet trained on
ImageNet ILSVRC2012 dataset. Both Deep K-means and the
proposed DP-Net are used to compress GoogleNet. Table IV
shows DP-Net is also better than Deep K-means on large-scale
dataset. The accuracy of Deep K-means decreased quickly
when CR>4. So, the accuracy of Deep K-means was not
reported in [14] for larger CR. In contrast, the accuracy of
DP-Net is improved even if we use 4 bits to represent a scalar
value, which implies a 7X compression ratio. Furthurmore,
even if GoogleNet is compressed by 10X, the accuracy of DP-
Net remains higher than that of Deep K-means with CR=4.

TABLE IV
THE RESULTS OF COMPRESSING GOOGLENET. ∆† AND ∆‡ ARE THE
CHANGES OF TOP1-ACCURACY AND TOP5-ACCURACY COMPARED TO THE
PRETRAINED MODEL, RESPECTIVELY.

Model
Deep K-means [14]
DP-Net (3 bits)
DP-Net (4 bits)

CR ∆†(%) ∆‡(%)
-1.14
4
-0.88
10
+0.2
7

-1.95
-1.56
+0.3

D. Result for Robust Model

To test the effectiveness of extended DP-Net on the robust
model, we compressed the SmallCNN for MNIST and ResNet-
18 for CIFAR-10. The models are trained by TRADES [13],
where a minimax loss function is used to make the model

                D F F X U D F \  F K D Q J H ' 3  1 H W   E L W V  . 0 H D Q V  1 H W   E L W V  ' 3  1 H W   E L W V  . 0 H D Q V  1 H W   E L W V             W    D F F X U D F \  F K D Q J H ' 3  1 H W   E L W V  . 0 H D Q V  1 H W   E L W V  ' 3  1 H W   E L W V  . 0 H D Q V  1 H W   E L W V robust. Because there is few work on compressing a robust
model, we consider a baseline called DP-Net WR for compar-
ison. DP-Net WR directly quantizes the weights by DP without
training the clustering-friendly network. Table V shows the
accuracy loss of SmallCNN on MNIST and ResNet-18 on
CIFAR-10, respectively. From them, we see that compressing
a robust model is more challenging. For example, directly
quantizing the SmallCNN to 2 bits makes 0.36% accuracy
drop on natural images, but the accuracy drop on adversarial
examples becomes 5.86%. The results in Table V show that
the extended DP-Net can compress the robust models by 14X
and 16X with negligible accuracy loss. In the results of DP-
Net WR, |∆adv| < |∆nat| for ResNet-18, because the original
accuracy on adversarial examples is much less than the original
accuracy on natural images (40.29% versus 92.15%).

TABLE V
THE RESULTS OF COMPRESSING THE ROBUST SMALLCNN (ON MNIST)
AND RESNET-18 (ON CIFAR-10). ∆nat AND ∆adv ARE THE ACCURACY
CHANGES FOR NATRUAL IMAGES AND ADVERSARIAL IMAGES (UNDER
PGD ATTACK) COMPARED TO THE PRETRAINED MODEL, RESPECTIVELY.

Model
DP-Net WR (2 bits)

CR Model ∆nat(%) ∆adv(%)
14 SmallCNN
Extended DP-Net (2 bits) 14 SmallCNN
16 ResNet-18
Extended DP-Net (2 bits) 16 ResNet-18

-0.36
-0.05
-22.19
-1.45

-5.86
-2.05
-16.15
+0.41

DP-Net WR (2 bits)

E. Custom Accelerator

To demonstrate the speciﬁc technique proposed in Section
III.D, we test the runtime of DNN inference with the DP-
Net deployed on FPGA. The experiments involve an FC
layer and a convolutional layer from GoogleNet compressed
by DP-Net with 4-bit representation, and they are multi-
plied by random vectors. For the convolutional layer with
nconv×mconv×hconv×wconv weights, we ﬁrst reshape it to an
nconv × mconvhconvwconv matrix. The baseline approach for
matrix-vector multiplication takes in the matrix represented
by 32-bit ﬂoating-point numbers stored in BRAM and/or
distributed RAM, and the random vector stored in distributed
RAM (ROM) to facilitate fast access. It adopts a pipelined
design technique, and performs ﬂoating-point multiply-add
operation by rows. The operation is implemented with the
Floating-point (7.1) IP Core provided by Xilinx, based on
DSP Slice Full Usage. For our approach based on DP-Net, the
quantized/compressed matrix is small enough to be stored in
the distributed RAM synthesized by LUTs. And, the technique
in Section III.D is implemented to perform the matrix-vector
multiplication. The experimental results are listed in Table VI.
It shows that the custom accelerator for our DP-Net is at least
5X faster than the the baseline approach without compression.
In this experiment, 4-bits representation means that 16 clusters
are used for preserving the accuracy of GoogleNet. For other
DNN models, the weights can be clustered into fewer classes
with DP-Net, which in turn would lead to larger compression
ratio and more inference acceleration on FPGA.

TABLE VI
THE TIME OF A MATRIX-VECTOR MULTIPLICATION ON FPGA.
Matrix Source Matrix Size Algorithm Time (ms) Speedup

FC layer

1000 × 1024

Convolutional
layer

384 × 1728

Baseline
Ours
Baseline
Ours

61.5
11.9
39.8
7.60

–
5.1X
–
5.2X

V. CONLUSION

In this paper, a DNN compression scheme called DP-
Net is proposed, which includes training a clustering-friendly
network and using a dynamic programming (DP) based algo-
rithm to obtain the optimal solution of weight quantization.
Exhaustive experiments have been carried out to show that
the DP based algorithm is better than the existing method
for weight quantization. And, DP-Net can be combined with
other compression methods. An experiment on Wide ResNet
demonstrates this and shows 77X compresion ratio with less
than 3% accuracy drop. DP-Net is also extended to compress
the robust DNN models, showing 16X compression with
negligible accuracy loss. Lastly, a technique based on DP-
Net for inference acceleration is proposed. The experiment on
FPGA shows that it brings 5X speedup to the computation
associated with FC layers and convolutional layers.

REFERENCES

[1] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in

Proc. NIPS, 1990, pp. 598–605.

[2] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efﬁcient neural network,” in Proc. NIPS, 2015, pp. 1135–
1143.

[3] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural

network,” arXiv preprint arXiv:1503.02531, 2015.

[4] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio,
“Binarized neural networks,” in Proc. NIPS, 2016, pp. 4107–4115.
[5] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “MobileNets: Efﬁcient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, 2017.

[6] W. Chen, J. Wilson, S. Tyree, K. Weinberger, and Y. Chen, “Compress-
ing neural networks with the hashing trick,” in Proc. ICML, 2015, pp.
2285–2294.

[7] Y. Gong, L. Liu, M. Yang, and L. Bourdev, “Compressing deep
convolutional networks using vector quantization,” arXiv preprint
arXiv:1412.6115, 2014.

[8] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep
neural network with pruning, trained quantization and huffman coding,”
in Proc. ICLR, 2016.

[9] W. Chen, J. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen, “Com-
pressing convolutional neural networks in the frequency domain,” in
Proc. ACM SIGKDD, 2016, pp. 1475–1484.

[10] S. Lloyd, “Least squares quantization in PCM,” IEEE Trans. Information

Theory, vol. 28, no. 2, pp. 129–137, 1982.

[11] M. Mahajan, P. Nimbhorkar, and K. Varadarajan, “The planar k-means
problem is NP-hard,” in International Workshop on Algorithms and
Computation. Springer, 2009, pp. 274–285.

[12] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J.
Goodfellow, and R. Fergus, “Intriguing properties of neural networks,”
in Proc. ICLR, 2014.

[13] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan,
“Theoretically principled trade-off between robustness and accuracy,”
in Proc. ICML, 2019, pp. 7472–7482.

[14] J. Wu, Y. Wang, Z. Wu, Z. Wang, A. Veeraraghavan, and Y. Lin,
“Deep k-means: Re-training and parameter sharing with harder cluster
assignments for compressing deep convolutions,” in Proc. ICML, 2018,
pp. 5363–5372.

[15] K. Ullrich, E. Meeds, and M. Welling, “Soft weight-sharing for neural

network compression,” in Proc. ICLR, 2017.

[16] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the

value of network pruning,” in Proc. ICLR, 2018.

[17] S. E. Dreyfus and A. M. Law, “Art and theory of dynamic programming,”

1977.

[18] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and
W. J. Dally, “EIE: Efﬁcient inference engine on compressed deep neural
network,” in Proc. ISCA, 2016, pp. 243–254.

[19] M. Sotoudeh and S. S. Baghsorkhi, “C3-ﬂow: Compute compression

co-design ﬂow for deep neural networks,” in Proc. DAC, 2019, p. 86.

[20] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features

from tiny images,” Citeseer, Tech. Rep., 2009.

[21] T. Garipov, D. Podoprikhin, A. Novikov, and D. Vetrov, “Ultimate
tensorization: Compressing convolutional and FC layers alike,” arXiv
preprint arXiv:1611.03214, 2016.

[22] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in Proc. the
27th British Machine Vision Conference (BMVC), 2016, pp. 87.1–87.12.
[23] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in Proc. IEEE Symp. Security Privacy (SP), 2017, pp. 39–57.
[24] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine learning

at scale,” in Proc. ICLR, 2016.

