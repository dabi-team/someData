1
2
0
2

p
e
S
2
1

]

G
L
.
s
c
[

2
v
1
3
2
7
0
.
5
0
1
2
:
v
i
X
r
a

Bilevel Programs Meet Deep Learning:
A Unifying View on Inference Learning Methods

Christopher Zach
Chalmers University of Technology
Gothenburg, Sweden
christopher.m.zach@gmail.com

Abstract

In this work we unify a number of inference learning methods, that are proposed
in the literature as alternative training algorithms to the ones based on regular error
back-propagation. These inference learning methods were developed with very
diverse motivations, mainly aiming to enhance the biological plausibility of deep
neural networks and to improve the intrinsic parallelism of training methods. We
show that these superﬁcially very different methods can all be obtained by succes-
sively applying a particular reformulation of bilevel optimization programs. As a
by-product it becomes also evident that all considered inference learning methods
include back-propagation as a special case, and therefore at least approximate
error back-propagation in typical settings. Finally, we propose Fenchel back-
propagation, that replaces the propagation of inﬁnitesimal corrections performed
in standard back-propagation with ﬁnite targets as the learning signal. Fenchel
back-propagation can therefore be seen as an instance of learning via explicit target
propagation.

1

Introduction

In this work we analyze several instances of inference learning methods and how these approaches
can be obtained as suitable reformulations of deeply nested minimization problems. We use inference
learning [36] as an umbrella term for all learning methods based on explicit optimization (inference)
of hidden unit activations. Inference learning includes methods such as layered contrastive Hebbian
learning (CHL, [39]), equilibrium propagation [35], lifted neural networks [8, 14, 21], and predictive
coding networks (PCN, [38, 36]). In a nutshell, all these methods can be interpreted as certain
mathematical reformulations of deeply nested optimization problems. Speciﬁcally, learning in
layered feed-forward neural network can be stated as the following nested optimization problem,

min
θ0,...,θL−1

(cid:96)(z∗

L) s.t.

z∗
1 = arg min
z1

E1, (x, z1; θ0)

z∗
k+1 = arg min
zk+1

Ek(z∗

k, zk+1; θk)

(1)

k)L

for k = 1, . . . , L. The values (z∗
k=1 are usually refered to as the network activations determined
in a forward pass, and the collection (θ0, . . . , θL−1) are the trainable network parameters. The
conversion between a DNN architecture and Eq. 1 is not unique. E.g., for ReLU-based DNNs the
choice for Ek can be Ek(zk, zk+1; θk) = (cid:107)zk+1 − [θkzk]+(cid:107)2 [8] or Ek(zk, zk+1; θk) = (cid:107)zk+1 − θkzk(cid:107)2 +
ı≥0(zk+1) [42], where [·]+ := max{0, ·} and ıC is the extended valued characteristic function of a
set C. In our work it will become clear, that the differences between inference learning methods is
largely induced by different (but equivalent) choices for the underlying nested optimization problems
and by selected direction for processing the nesting levels (top-down or bottom-up). Since all these
approaches yield the standard back-propagation (BP) method at least as a limit case, understanding
the connections between the inference learning methods may also provide insights into regular
back-propagation. In particular, some inference learning methods can be interpreted as instances of

Preprint.

 
 
 
 
 
 
Method
CHL
Convex CHL
FLN & LPOM Fenchel/Proximal

Ek
Fenchel
Proximal

MAC
BP & PCN
Fenchel BP

Penalizer
Penalizer
Fenchel/Proximal

Direction
Bottom-up
Bottom-up
Top-down
Any
Top-down
Top-down

Linearized outer loss
No
No
No
No
Yes
Yes

Inference
Layer-convex
Convex
Layer-Convex
Non-convex
Closed-form
Closed-form

Table 1: A taxonomy of inference learning methods uniﬁed in this work: Contrastive Hebbian
learning (CHL, [39]) and convex CHL [40], Fenchel lifted networks (FLN, [14]) and lifted proximal
operator machines (LPOM, [21]), method of auxiliary coordinates (MAC, [8]), back-propagation
(BP) and predictive coding networks (PCN, [38, 36]). Fenchel back-propagation (Fenchel BP) is a
generalization of standard back-propagation discussed in Section 4.2.

target propagation [5, 20, 6, 29, 25]—where desired neural activations instead of (inﬁnitesimal) error
signals are propagated through the network—and as the basis for learning methods using layer-local
losses (including [12, 2]), hence this work may also shed new light on those methods.

Table 1 summarizes the relations between inference learning methods. As it can be seen, the methods
differ in the choice of Ek, the direction of unravelling the nesting levels, and whether an additional
linearization step is applied. Typical choices for Ek are given in column Ek, which can be based on a
penalizer of the forward model zk+1 = fk(zk; θk), using convex (Fenchel) conjugates, Gk+1(zk+1) −
2 (cid:107)zk+1 −Wkzk(cid:107)2 + Fk+1(zk+1). The last column indicates the
k+1Wkzk, or proximal mappings, 1
z(cid:62)
difﬁculty of the resulting inference task needed to determine the network activations/targets. The
different realizations have also an impact on the required level of synchronization points: BP and
PCN use a dedicated forward and backward pass (which is relaxed in [36]), whereas CHL requires
two global (free and clamped) but non-directed phases for learning. Lifted neural networks such as
the method of auxiliary coordinates (MAC) [8] and lifted proximal operator machines (LPOM) [21]
(and the closely related lifted Fenchel networks [14]) need local synchronization between layers due
to the non-convexity of the underlying objective function. Fenchel back-propagation is an algorithm
similar to back-propagation and is presented in Section 4.2.

Not all instances of inference learning methods belong to this taxonomy. One aspect that the above
methods have in common is, that they include back-propagation as a limit case, which is violated e.g.
for the inference learning methods proposed in [42, 3] (as demonstrated in [40]).

Our main contribution is establishing connections between certain reformulations for deeply nested
optimization tasks and inference learning methods. Singly nested i.e. bilevel optimization has been
leveraged in the machine learning literature for parameter learning in variational and dictionary-
based models (e.g. [24, 18, 43]) and for meta-learning (such as [7, 11, 32, 22]). The resulting
optimization methods are often based on surrogates for the original bilevel optimization tasks
using KKT conditions [7, 18, 43]. Unrolling a minimization problem is a common technique in
machine learning to replace the inner optimization task with a ﬁnite sequence of local minimization
steps [10, 23, 28, 11] (which is similar but not identical to back-propagation through time [37]). If
the inner problem is twice differentiable, implicit differentiation is applicable for gradient-based
optimization of the outer objective [31, 1, 17, 4, 34], which has been more recently extended to allow
constraints in the inner problem [13].

2 Surrogates for Bilevel Programs

Since the learning problem in deep networks in Eq. 1 is a nested minimization task, we ﬁrst consider
singly nested i.e. bilevel problems. For an outer loss (cid:96)(·) and inner objective E(·; ·) we introduce the
bilevel minimization problem

L (θ ) := (cid:96)(z∗)

s.t. z∗ = arg minz E(z; θ ),

(2)

where we assume that E has a unique minimizer for each feasible value θ . We also require that
the outer loss (cid:96) is differentiable, bounded from below and that (cid:96)(z) < ∞ for all z. The ﬁrst step is
to rewrite Eq. 2 using an optimal value reformulation for bilevel programs [30] (and see [9] for a

2

discussion of several reformulations for bilevel programs),

(3)
The corresponding Lagrangian relaxation is (using β −1 to parametrize the non-negative multiplier),

s.t. E(z; θ ) ≤ minz(cid:48) E(z(cid:48); θ ),

minθ ,z (cid:96)(z)

maxβ >0 minθ ,z (cid:96)(z) + 1
β

(cid:0)E(z; θ ) − minz(cid:48) E(z(cid:48); θ )(cid:1).

(4)

By ﬁxing a multiplier β > 0 we arrive at the main quantity of interest in this work:
Deﬁnition 1. The contrastive surrogate for the bilevel minimization problem in Eq. 2 is deﬁned as
follows,

L β (θ ) := min

z

(cid:8)(cid:96)(z) + 1

β E(z; θ )(cid:9) − min

z

1
β E(z; θ )

= (cid:96)(ˆz(β )) + 1

β E(ˆz(β ); θ ) − 1

β E(z∗; θ ),

where β > 0 is the “spacing parameter.” The minimizers of the two sub-problems are denoted as

ˆz(β ) := arg minz β (cid:96)(z) + E(z; θ )

z∗ := arg minz E(z; θ ),

where the dependence on θ is implicit.

(5)

(6)

For brevity we will often write ˆzβ instead of ˆz(β ). We call this surrogate contrastive in analogy with
e.g. contrastive Hebbian learning [27, 39] that relies on solving two minimization tasks; one to obtain
a loss-augmented “clamped” solution and one for the “free” solution not using any information from
the outer loss (cid:96). The contrastive surrogate is a “min-min-max” instance, as one aims to minimize w.r.t.
θ , but the two subproblems in z have opposing signs. Fortunately, in our setting solving minz E(z; θ )
is typically (but not always) easy and often has a closed-form solution. If E(z; θ ) is convex in z, then
one can obtain a “min-min-min” problem using duality [41].

Naturally, we are most interested in the setting when β ≈ 0, where the contrastive surrogates are
good approximations of the optimal value reformulation (Eq. 3), and it is therefore important to better
understand the ﬁnite difference 1
|β =0+. If E is
d ˆzβ
dβ |β =0

β (E(ˆzβ ; θ ) − E(z∗; θ )) in the limit β → 0+, i.e.
differentiable w.r.t. z, then (by the chain rule) the relevant quantity to analyze is
(due to strong differentiability). The simplest setting is when (cid:96) and E are sufﬁciently regular:
Proposition 1. If (cid:96) is differentiable and E is twice continuously differentiable, then

dE(ˆzβ ;θ )
dβ
d ˆzβ
dβ |β =0+ =

and

d

dβ ˆz(β )(cid:12)

(cid:12)β =0 = −(cid:0) ∂ 2

∂ z2 E(z∗; θ )(cid:1)−1

(cid:96)(cid:48)(z∗)

d

dβ E(ˆz(β ); θ )(cid:12)

(cid:12)β =0 = − ∂

∂ z E(z∗; θ )(cid:0) ∂ 2

∂ z2 E(z∗; θ )(cid:1)−1

(cid:96)(cid:48)(z∗).

(7)

(8)

The proof is based on implicit differentiation and is detailed (together with all following results) in
the appendix. By taking the partial derivative w.r.t. θ , we arrive at a familiar expression for gradient
back-propagation through minimization [31, 1, 17, 4, 34],

d
dθ

L (θ ) = − ∂ 2

∂ θ ∂ z E(z∗; θ )

= lim
β →0+

1
β

∂
∂ θ

(cid:17)−1

(cid:16) ∂ 2
∂ z2 E(z∗(θ ); θ )
(cid:0)E(ˆzβ ; θ ) − E(z∗; θ )(cid:1).

(cid:96)(cid:48)(z∗)

(9)

Hence, we may facilitate gradient-based minimization of θ e.g. by choosing β ≈ 0 and updating θ
according to

θ (t+1) ← θ (t) − η (t)
β

(cid:0)E(ˆzβ ; θ ) − E(z∗; θ )(cid:1),

∂
∂ θ
where η (t) is the learning rate at iteration t. With the assumptions given in Prop. 1 we know that
in the limit β → 0+ we obtain an exact gradient method. Implicit differentiation can be extended
to allow (smooth) constraints on z in E(z; θ ) [13]. One advantage of the surrogates in Def. 1 over
implicit differentiation is, that the former requires only the directional derivative d ˆz(β )
dβ |β =0+ to exist,
which is a weaker requirement than existence of a strong derivative dz∗(θ )/dθ as it can be seen in
the following simple, but important example.

(10)

3

Example 1 (ReLU activation). Let E(z; θ ) be given as E(z; θ ) = 1
Then z∗ = [θ x]+ and ˆzβ = [θ x − β g]+, and therefore

2 (cid:107)z − θ x(cid:107)2 + ı≥0(z) and (cid:96)(z) = g(cid:62)z.

d ˆzβ
dβ

(cid:12)
(cid:12)β =0+ = lim
β →0+

[θ x − β g]+ − [θ x]+
β

= D g,

where D is a diagonal matrix with

D j j =

(cid:26)−1
0

if (θ x) j > 0 ∨ (cid:0)(θ x) j = 0 ∧ g j ≥ 0(cid:1)
if (θ x) j < 0 ∨ (cid:0)(θ x) j = 0 ∧ g j < 0(cid:1).

Thus, the gradient w.r.t. θ is given by

∂

∂ θ limβ →0+ L (β ) =

d ˆzβ
dβ

(cid:12)
(cid:12)β =0+x(cid:62) = D g x(cid:62).

(11)

(12)

(13)

This derivative always exists, which is in contrast to back-propagation and implicit differentiation [13],
which have both undeﬁned gradients at (θ x) j = 0.

This example can be extended to the following result:
Proposition 2. Let

E(z; θ ) = f (z; θ )

s.t. gi(z) ≥ 0,

(14)

where f and every gi are twice continuously differentiable w.r.t. z. We assume all constraints are active
at the solution z∗ = arg minz E(z; θ ), i.e. gi(z∗) = 0 for all i, and ignore inactive constraints with
gi(z∗) > 0. We also assume that ∂ 2 f (z∗;θ )
is (strictly) positive deﬁnite and that a suitable constraint
qualiﬁcation holds (such as LICQ). Let W ∗ be the indices of weakly active contraints at z∗, and

∂ z2

c := (cid:96)(cid:48)(z∗)

If A is (strictly) p.d., then

A := ∂ 2

∂ z2 f (z∗; θ )+∑i λ ∗

i

∂ 2
∂ z2 gi(z∗)

B := ∂

∂ z g(z∗).

(15)

d

dβ ˆz(β )(cid:12)

(cid:12)β =0+ = ˙z,

(16)

where ˙z is the unique solution of the following strictly convex quadratic program,

2 ˙z(cid:62)A˙z + c(cid:62) ˙z s.t. B˙z ≥ 0 ∀i /∈ W ∗ : Bi,: ˙z = 0.
Remark 1. If we reparametrize ˙u = A1/2 ˙z, then (with C := BA−1/2) the QP in Eq. 17 is equivalent to

(17)

1

1

2 (cid:107) ˙u + A−1/2c(cid:107)2 s.t. C ˙u ≥ 0 ∀i /∈ W ∗ : Ci,: ˙u = 0,
which projects the natural descent direction −A−1/2c to the feasible (convex) region induced by
strongly and weakly active constraints.

(18)

Prop. 2 is slightly different to Prop. 4.6 in [13], which is not applicable when constraints are weakly
active. It assigns a sensible derivative e.g. for DNN units with ReLU activations as seen in the above
example (at this point this applies only to networks with a single layer, and we refer to Section 3 for
a discussion of deep networks). Hard sigmoid and hard tanh activation functions are obtained by
contraining z to be inside the hyper-cubes [0, 1]n and [−1, 1]n, respectively, where n is the dimension
of z. If we have only strongly active constraints, then (using the Woodbury matrix identity)

d ˆz(β )
dβ

(cid:12)
(cid:12)β =0+ = − lim
µ→∞

(cid:16)
A + µB(cid:62)B

(cid:17)−1

(cid:96)(cid:48)(z∗)

= −(cid:0)I − A−1B(cid:62)(BA−1B(cid:62))−1B(cid:1)A−1(cid:96)(cid:48)(z∗)

as in [13]. When there are many weakly active constraints, then solving the QP in Eq. 17 is
not straightforward. Fortunately, when modeling standard DNN layers, the subproblems typically
decouple over units, and we have at most a single (strongly or weakly) active constraint (as in the
ReLU example above), and closed-form expressions are often available. Once d ˆz(β )/dβ |β =0+ is
determined, gradient-based optimization of θ is straightforward:

4

Corollary 1. In addition to the assumptions in Prop. 2, let f (z, θ ) be differentiable in θ . Then

∂

∂ θ limβ →0+ L β (θ ) =

∂ f (z∗; θ )
∂ θ

·

d ˆz(β )
dβ

(cid:12)
(cid:12)β =0+.

(19)

This result can be extended if the constraints depend on θ by incorporating the multipliers λ ∗.
So far we focused on the limit case β → 0+, where the contrastive surrogates yield a generalized
chain rule. In practice it is absolutely possible to use non-inﬁnitesimal values for β when optimizing
L β . We obtain under convexity assumptions the following approximation property:
Proposition 3. Let (cid:96) be convex and E(·; θ ) be strongly convex with parameter ν > 0 for all θ . Then
we obtain

L β (θ ) ≤ L (θ ) ≤ L β (θ ) + β

2ν (cid:107)∂ (cid:96)(z∗)(cid:107)2

(20)

for all β > 0, where ∂ (cid:96)(z∗) is any subgradient of (cid:96) at z∗.

This result immediately implies, that if (cid:96) has bounded (sub-)gradients (such as the (cid:96)1 loss), then L β
stays within a ﬁxed band near L for all θ .

3 Surrogates for Deeply Nested Minimization

The previous section has discussed single level surrogates for bilevel minimization tasks, and in this
section we move towards deeply nested problems. A nested problem (with L nesting levels) is given
as follows (recall Eq. 1),

Ldeep(θ ) := (cid:96)(z∗

L) s.t. z∗
1 = arg minz1E1(z1; θ )
k = arg minzk Ek(zk, z∗
z∗

k−1; θ )

(21)

for k = 2, . . . , L. We assume that each arg-min is unique and can be stated as a function,

z∗
k = arg minzk Ek(zk, zk−1; θ ) = fk(zk−1; θ ).
This is the case e.g. when Ek is strictly convex in zk. In order to avoid always handling E1 specially,
we formally introduce z0 as argument to E1 = E1(z1, z0; θ ). z0 could be a purely dummy argument
or—in the case of DNNs—the input signal provided to the network.

Gradient-based optimization of Eq. 21 is usually relying on back-propagation, which can be seen
as instance of the chain rule (e.g. [33]) or as the adjoint equations from an optimal control perspec-
tive [19]. In contrast to these approaches, we proceed in this and in the next section by iteratively
applying the contrastive surrogates introduced in Section 2. We have two natural ways to apply
the contrastive surrogates to deeply nested problems: either top-down (outside-in) or bottom-up
(inside-out), which are discussed separately below.

3.1 Bottom-Up Expansion

The ﬁrst option is to expand recursively from the innermost (bottom) nesting level, i.e. replacing the
subproblem

with

minz2 E2(z2, z∗

1; θ )

s.t. z∗

1 = arg minz1 E1(z1; θ )

(cid:110)

E2(z2, z1; θ ) + 1
β1

(cid:111)

E1(z1; θ )

min
z1,z2

− min
z1

1
β1

E1(z1; θ ),

which yields a problem with L − 1 levels. Continuing this process yields the following proposition:
Proposition 4. Let βββ = (β1, . . . , βL) be a vector of positive spacing parameters. Applying the
contrastive surrogate in Def. 1 recursively on Eq. 21 from the innermost to the outermost level yields
the following global contrastive objective,
(cid:110)
(cid:96)(zL) +∑L

(cid:110)
∑L

(22)

L βββ

(cid:111)

(cid:111)

.

k=1

k=1

GC(θ ) := min
z1,...,zL

Ek(zk,zk−1;θ )
l=k βl

∏L

Ek(zk,zk−1;θ )
l=k βl

∏L

− min
z1,...,zL

5

We also introduce the free “energy” E βββ and its loss-augmented (“clamped”) counterpart ˆE βββ ,

E βββ (z; θ ) := ∑L
ˆE βββ (z; θ ) := (cid:96)(zL) + E βββ (z; θ ).

1
l=k βl

k=1

∏L

Ek(zk, zk−1; θ )

Hence, L βββ (θ ) is the difference of the optimal clamped (loss-augmented) and free energies,

L βββ

GC(θ ) = min

z

ˆE βββ (z; θ ) − min
z

E βββ (z; θ )

= min
ˆz

max
ˇz

ˆE βββ (ˆz; θ ) − E βββ (ˇz; θ ).

(23)

(24)

The minimizers ˇz and ˆz are called the free and clamped solution, respectively. We call L βββ
GC a globally
contrastive loss, since it is the difference of two objectives involving all (nesting) layers. This bottom-
up expansion requires (approximately) solving two minimization tasks to obtain the free and the
clamped solution in order to form a gradient w.r.t. θ . In analogy to learning in Boltzmann machines,
these two minimization problems are referred to as free phase and the clamped phase [27, 39].
When E βββ is jointly convex in z (which is sufﬁcient to model general function approximation such as
ReLU-based DNNs), the “min-max” problem structure can be converted to a “min-min” one using
convex duality [40, 41]. By construction we also have E βββ (ˆz; θ ) ≥ E βββ (ˇz; θ ), and if E βββ is strictly
convex, then E βββ (ˆz; θ ) = E βββ (ˇz; θ ) iff ˆz = ˇz. Hence, learning in this framework is achieved by reducing
the gap between a clamped (loss-augmented) and a free energy that cover all layers. If we set all βk
identical to a common feedback parameter β > 0, then the free energy E βββ reduces to

E βββ (z; θ ) := ∑L

k=1 β k−1−LEk(zk, zk−1; θ ).

(25)

Typically β ∈ (0, 1) and therefore later layers are discounted. Such contrastive loss between dis-
counted terms is referred as contrastive Hebbian learning, and their relation to classical back-
propagation for speciﬁc instances of Ek is analysed in [39, 40].
One advantage of the global contrastive framework over the approach described in the next section
is, that the free and the clamped energies, E βββ and ˆE βββ , can be chosen to be jointly convex in all
activations z1, . . . , zL while still approximating e.g. general purpose DNNs with ReLU activation
functions to arbitrary precision. By modifying the terms Ek connecting adjacent layers, one can also
obtain Lipschitz continuous deep networks by design, that exhibit better robustness to adversarial
perturbations [15].

3.2 Top-Down Expansion

Applying the analogous procedure described in Section 3.1 from the outermost (top) level successively
to the inner nesting levels yields a different surrogate for deeply nested problems:
Proposition 5. Let βββ = (β1, . . . , βL) with each βk > 0 and let ˇEk and ˜Ek be given by

ˇEk(zk−1; θ ) := minz(cid:48)

k

Ek(z(cid:48)

k, zk−1; θ )

˜Ek(zk, zk−1; θ ) := Ek(zk, zk−1; θ ) − ˇEk(zk−1; θ )

(26)

(27)

for each k = 1, . . . , L. Applying the expansion in Def. 1 recursively on Eq. 21 from the outermost to
the innermost level yields the following local (or layer-wise) contrastive objective,
(cid:110)
(cid:96)(zL) +∑L

˜Ek(zk, zk−1; θ )

(28)

(cid:111)
.

L βββ

k=1

1
βk

LC(θ ) = min
z1,...,zL

We also introduce

˜E βββ (z; θ ) := ∑L

k=1

1
βk

˜Ek(zk, zk−1; θ ).

(29)

Similar to the global contrastive setting, ˜E βββ (z; θ ) ≥ 0 for all z; and if Ek(zk, zk−1; θ ) is strictly convex,
then ˜E βββ (z; θ ) = 0 iff zk = arg minz(cid:48)
k, zk−1) for all k = 1, . . . , L. Learning the parameters θ aims
to reduce the discrepancy measure ˜E βββ . In contrast to the global contrastive framework, the learning

Ek(z(cid:48)

k

6

task is a pure min-min instance, and it requires only maintaining one set of network activations z. One
downside of the local contrastive method is, that ˜E βββ will be difﬁcult to optimize in any interesting
setting (i.e. with a non-linear activation function), even if (cid:96) and all Ek are convex. This is due to the
˜E βββ (z; θ ) = 0 iff zk = fk−1(zk−1), which is a non-linear constraint in general
following reasoning:
(unless fk−1 is linear). Hence, with at least one fk being nonlinear, the set {z : zk = fk−1(zk−1)∀k} is
non-convex, and therefore ˜E βββ cannot be jointly convex in z1, . . . , zL. While joint convexity is out of
reach, it is possible to obtain instances that are layer-wise convex (cf. Section 3.2.2).

3.2.1 Auxiliary coordinates and predictive coding networks

Starting from very different motivations, both predictive coding networks [38, 36] and the method of
auxiliary coordinates [8] arrive at the same underlying objective,

EMAC(z; θ ) = (cid:96)(zK) +∑

µk
2 (cid:107)zk− fk−1(zk−1; θ )(cid:107)2,

(30)

k
which is essentially a quadratic penalizer for the (typically non-linear and non-convex) constraint
2 (cid:107)zk − fk−1(zk−1; θ )(cid:107)2,
zk = fk−1(zk−1; θ ). By identifying µk = 1/βk and by using Ek(zk, zk−1; θ ) = 1
we identify EMAC as an instance of a local contrastive objective (Eq. 28). At the same time it
can be also interpreted as an instance of the global contrastive objective (Eq. 22), after equating
µ −1
l=k βl, since the optimal free energy satisﬁes minz E βββ (z; θ ) = 0. The two interpretations
k = ∏L
differ in the meaning of the multipliers µk, depending on whether they encode discounting of later
layers. In contrast to lifted proximal operator machines discussed in the next section (Section 3.2.2),
inferring the activations (zk)L
k=1 is usually not even layerwise convex.

3.2.2 Lifted Proximal Operator Machines

A different choice for Ek is inspired by convex conjugates and the Fenchel-Young inequality: let
Gk(·; θ ) be l.s.c. and convex for k = 1, . . . , L, and deﬁne Ek as follows,

(31)
where the trainable parameters θ = (W0, . . . ,WL−1) are the weight matrices. Recall ˇEk(zk−1; θ ) =
minzk Ek(zk, zk−1; θ ) (Prop. 5), and therefore we obtain

Ek(zk, zk−1; θ ) := Gk(zk; θ ) − z(cid:62)

k Wk−1zk−1,

ˇEk(zk−1; θ ) = minzk Ek(zk, zk−1; θ )
= minzk Gk(zk; θ ) − z(cid:62)
= −G∗

k(Wk−1zk−1),

k Wk−1zk−1 = − max

z(cid:62)
k Wk−1zk−1Gk(zk; θ )

zk

where G∗

k is the convex conjugate of Gk. Thus, in this setting we read
˜Ek(zk, zk−1) = Ek(zk, zk−1) − ˇEk(zk−1) = Gk(zk) − z(cid:62)

k Wk−1zk−1 + G∗

k(Wk−1zk−1),

hence L βββ

LC specializes to the following lifted proximal operator machine objective,
1
βk

k Wk−1zk−1+G∗

(cid:0)Gk(zk) − z(cid:62)

(cid:96)(zL) +∑L

LPOM(θ ) = min
z1,...,zL

L βββ

k(Wk−1zk−1)(cid:1).

k=1

(32)

(33)

(34)

By collecting terms dependent only on zk, minimizing over zk (for ﬁxed zk−1 and zk+1) amounts to
solving the following convex minimization problem,
(cid:1) + 1
βk+1

k+1(Wkzk) − z(cid:62)

(cid:0)Gk(zk) − z(cid:62)

k Wk−1zk−1

k zk+1

k W (cid:62)

minzk

(cid:0)G∗

(35)

1
βk

(cid:1),

hence the minimization task in Eq. 34 is block-convex w.r.t. z1, . . . , zL. The above choice for Ek
is a slight generalization of lifted proximal operator machines proposed by [21], where a similar
block-convex cost is proposed for DNNs with element-wise activation functions fk. In the notation
of [21], Gk corresponds to ˜fk and its convex conjugate G∗
k is represented by ˜gk. The ability to utilize
general convex Gk e.g. allows to model soft arg-max layers (often referred to as just soft-max layers)
as LPOM-type objective by using the convex conjugate pair
k(zk−1) = log∑ j ezk−1, j .
G∗
Gk(zk) = ∑ j zk, j log zk, j

This activation function cannot be represented in the original LPOM framework, since it requires
coupling between the elements in zk−1.

7

4 Fenchel back-propagation

First, we observe that Props. 1 and 2 also hold if the outer loss (cid:96) in the respective reformulation is
replaced by its ﬁrst-order Taylor approximation at z∗ = arg minz E(z; θ ),

¯(cid:96)(z; z∗) := (cid:96)(z∗) + (z − z∗)(cid:62)(cid:96)(cid:48)(z∗),
since these results only make use of the derivative (cid:96)(cid:48)(z∗). Hence, in this section we consider to replace
(cid:96) by its linear approximation at z∗:
Deﬁnition 2. For a differentiable outer loss (cid:96), an inner objective E and β > 0 the linearized con-
trastive surrogate is given by

(36)

¯L β (θ ) := min
z

(cid:8) ¯(cid:96)(z; z∗) + 1

β E(z; θ )(cid:9) − min

z

1
β E(z; θ ).

(37)

Props. 1 and 2 remain valid with L β replaced by ¯L β . It is now natural to ask what happens if we use
¯L β as building block to convert deeply nested programs, Eq. 21. It can be shown (see the appendix)
that the bottom-up traversal of the nesting levels (corresponding to L βββ
GC) yields non-promising
reformulations and are therefore of no further interest. Thus, we focus on the top-down approach
below.

In order to apply the linearized contrastive surrogate (Eq. 37) in the top-down direction, we need the
following result in order to apply the surrogate recursively when the outer loss is itself a minimization
task:
Proposition 6. Let a bilevel problem be of the form

miny h(y, z∗)

s.t. z∗ = arg minz E(z; θ ),

(38)

where h(y, z) is differentiable w.r.t. z for all y. Then for a chosen β > 0 the linearized contrastive
reformulation is given by

minz

(cid:110)¯h(y∗, z; z∗) + 1

(cid:111)
β E(z; θ )

− 1

β minz E(z; θ ),

where y∗ := arg miny h(y, z∗) and

¯h(y, z; z∗) := h(y, z∗) + (z − z∗)(cid:62) ∂

∂ z h(y, z∗).

(39)

(40)

With this proposition we are able to apply the linearized contrastive surrogate in top-down direction:
Proposition 7. Let βββ = (β1, . . . , βL) with each βk > 0. We recursively deﬁne the following quantities:

z∗
1 := arg minz1 E1(z1; θ )

k := arg minzk Ek(zk; z∗
z∗

k−1)

and

˜Ek(zk, zk−1) := Ek(zk, zk−1) − minz(cid:48)

Ek(z(cid:48)
k−1) := ˜Ek(zk, zk−1) + (zk−1 − z∗

k, zk−1)
k−1)(cid:62) ∂

¯Ek(zk, zk−1; z∗

k

∂ zk−1

˜Ek(zk, z∗

k−1)

for k = 2, . . . , L. We also introduce for k = 1, . . . , L − 1,

¯zL := arg min
zL
¯zk := arg min
zk

¯(cid:96)(zL; z∗

EL(zL, z∗

L) + 1
βL
¯Ek+1(¯zk+1, zk; z∗

1
βk+1

L−1)

k) + 1
βk

Ek(zk, z∗

k−1).

(41)

(42)

(43)

Then the linearized local contrastive surrogate

¯L βββ

¯L βββ

LC(θ ) = ¯(cid:96)(¯zL; z∗

L) +

LC for the deeply nested program Eq. 21 is given by
L
∑
k=1

¯Ek(¯zk, ¯zk−1; z∗

k−1).

(44)

1
βk

Observe that in ¯L βββ
w.r.t. z1, . . . , zL. The quantities z∗
obtained in a respective backward pass by solving linearly perturbed forward problems.

LC is given in closed-form and does not require further inference (i.e. minimization)
L are determined in a forward pass, where as ¯zL, . . . , ¯z1 are

1, . . . , z∗

8

4.1 Standard Error Back-propagation

In view of the forward and backward pass outlined in Prop. 7, it should not come as a surprise that
standard error back-propagation can be obtained with a suitable choice for Ek. In particular, if we use
2 (cid:107)zk − fk−1(zk−1)(cid:107)2, where fk−1 is the desired forward mapping between layers k − 1
Ek(zk, zk−1) = 1
and k, then we obtain a family of back-propagation methods. Observe that with this choice we have
Ek(z∗

k−1) = 0, and

k, z∗

∂
∂ zk

and therefore

˜Ek+1(¯zk+1, z∗

k) = f (cid:48)

k(z∗

k)(cid:62)( fk(z∗

k) − ¯zk+1) = f (cid:48)

k(z∗

k)(cid:62)(z∗

k+1 − ¯zk+1)

¯zL = z∗

L − βL(cid:96)(cid:48)(z∗
L)

¯zk = z∗

k − βk
βk+1

We introduce εk := ¯zk − z∗

k to obtain the relations

k(z∗
f (cid:48)

k)(cid:62)(z∗

k+1− ¯zk+1).

εL = −βL(cid:96)(cid:48)(z∗
L)

εk = βk
βk+1

k(z∗
f (cid:48)

k)(cid:62)εk+1.

(45)

(46)

(47)

The recursion for εk is essentially back-propagation (and equivalent to the proposed backward pass in
predictive coding networks [26]). Standard back-propagation is obtained if β1 = . . . = βL, otherwise
one obtains a descent (but not necessarily steepest descent) direction.

4.2 Fenchel Back-Propagation

In analogy to Section 3.2.2 we consider Ek+1(zk+1, zk) = Gk+1(zk+1) − z(cid:62)
k+1Wkzk, where Gk+1 is a
strictly convex function for each k. Gk+1 is chosen such that arg minzk+1 Ek+1(zk+1, zk) = fk+1(Wkzk)
for a desired activation function fk+1. We recall from Section 3.2.2 that

˜Ek+1(zk+1, zk) = Gk+1(zk+1) − z(cid:62)

k+1Wkzk + G∗

k+1(Wkzk)

(where G∗

k+1 is the convex conjugate of Gk+1) and therefore (using Prop. 6)

∂
∂ zk

˜Ek+1(zk+1, z∗

k) = W (cid:62)
k

(cid:0)∂ G∗

k+1(Wkz∗

k) − zk+1

(cid:1) = W (cid:62)
k

(cid:0) fk+1(Wkz∗

k) − zk+1

(cid:1).

(48)

Thus, the realization of

¯L βββ

LC in this setting is given by
L) +∑L

˜Ek(¯zk, z∗

k=1

1
βk

¯L βββ

FenBP(θ ) = ¯(cid:96)(¯zL; z∗

k−1) +∑L−1
where FenBP stands for Fenchel back-propagation. In contrast to standard back-propagation, non-
inﬁnitesimal target values are propagated backwards through the layers.
It also combined the
power to model non-differentiable activation functions with the efﬁciency of back-propagation.
Specializing this further to ReLU activation functions (using Gk(zk) = 1
2 (cid:107)zk(cid:107)2 + ı≥0(zk) and thus
G∗

2 (cid:107)[u]+(cid:107)2) yields the following relations,

k+1 − ¯zk+1),

k(u) = 1

k)(cid:62)W (cid:62)

(¯zk − z∗

k (z∗

1
βk+1

(49)

k=1

and consequently z∗

˜Ek(zk, zk−1) = 1
k = [Wk−1z∗

2 (cid:107)zk(cid:107)2 + ı≥0(zk) − z(cid:62)
L = WL−1z∗
k−1]+, z∗
(cid:16) 1
2 (cid:107)zL(cid:107)2 − z(cid:62)

L WL−1z∗

L−1

(cid:17)

k Wk−1zk−1 + 1

2 (cid:107)[Wk−1zk−1]+(cid:107)2,

L−1 for the forward pass, and

¯zL = arg min
zL
= WL−1z∗

¯zk = arg min
zk≥0
= (cid:2)Wk−1z∗

L (cid:96)(cid:48)(z∗
z(cid:62)
L) + 1
βL
l−1 − βL(cid:96)(cid:48)(z∗
L)
k−1(z∗
k W (cid:62)
z(cid:62)

1
βk+1
k−1 + βk
βk+1

W (cid:62)

k+1 − ¯zk+1) + 1
βk
k+1)(cid:3)

k (¯zk+1 − z∗

+

(cid:16) 1
2 (cid:107)zk(cid:107)2 − z(cid:62)

k Wk−1z∗

k−1

(cid:17)

(50)

(51)

determine the backward pass. This means that the forward and backward passes in Fenchel BP
are similar to back-propagation in terms of computation efﬁciency, as the only modiﬁcation are the
details of the back-propagated error signals. As seen in Ex. 1 the error signal 1
k) converges
βk
to a generalized derivative of the activation function in the limit case βk → 0+.

(¯zk − z∗

9

Figure 1: Training progress for 784-256-128-64-10 ReLU MLP trained on MNIST. (Left) Uniform
Glorot weight initialization, (right) Negative weight initialization. Solid curves correspond to training
errors (in %) and dashed ones are test errors.

Remark 2. There is a subtle but important issue regarding the correct way of computing ∂
∂Wk
In view of Eq. 49 it is easy to eventually obtain to an incorrect gradient

¯L βββ

FenBP.

whereas the correct one is given by,

∂
∂Wk

¯L βββ

FenBP = 1
βk+1

(cid:0)z∗

k+1 − ¯zk+1

(cid:1) ¯z(cid:62)
k ,

∂
∂Wk

¯L βββ

FenBP = 1
βk+1

(cid:0)z∗

k+1 − ¯zk+1

(cid:1) (z∗

k)(cid:62).

(52)

(53)

The reason is, that we have to ignore the linearization terms (¯zk − z∗
k+1 − ¯zk+1) in Eq. 49,
since they only appear when linearizing ˜Ek with respect to a lower level variable zk−1. Using only the
1
k) term to determine the gradient w.r.t. Wk yields the correct answer in Eq. 53. This
βk+1
gradient is the contribution from a single training sample and is accumulated accordingly over the
training set or a respective mini-batch for gradient-based learning.

˜Ek+1(¯zk+1, z∗

k)(cid:62)W (cid:62)

k (z∗

5 Discussion

In this work we show that several apparently unrelated inference learning methods can be uniﬁed
via the framework of contrastive surrogates for deeply nested optimization problems. By applying
Prop. 2 successively on each level we obtain the following corollary as one consequence of this
framework:
Corollary 2. Let (cid:96) be differentiable and Ek, k = 1, . . . , L, satisfy the assumptions in Prop. 2, then we
have

d
dθ

Ldeep(θ ) = ∂

∂ θ lim
βββ →0+

L βββ

GC(θ ) = ∂

∂ θ lim
βββ →0+

L βββ

LC(θ ) = ∂

∂ θ lim
βββ →0+

¯L βββ

LC(θ ).

(54)

Thus, the methods discussed in Sections 3 and 4.2 are equivalent to standard back-propagation in
their respective limit case (and approximate back-propagation for sufﬁciently small values βk > 0).
Hence, constructive arguments speciﬁc to a proposed method (such as the ones given in [39, 40]) are
not strictly necessary. Additionally it follows, that back-propagation can be extended to rely solely
on one-sided directional derivatives instead of regular (strong) derivatives of the activation function.

We conclude by pointing out that in practice the contrastive surrogates are valid objectives to train a
DNN in their own right. While the analysis conducted in this paper is largely addressing the limit
case βk → 0+, when all the discussed methods converge essentially to back-propagation, even ﬁnite
values for βk > 0 lead to valid learning losses. This is due to L βββ
LC attaining the
minimal possible objective value only for a perfect predictor (leading also to a minimal target loss
(cid:96)). Empirical results of existing inference learning methods are given in the respective literature.
At this point we illustrate the numerical behavior of Fenchel BP in Fig. 1 for a 4-layer multi-layer
perceptron (MLP) trained on MNIST using ADAM [16] as stochastic optimization method (batch size
50, learning rate 0.001, uniform Glorot or negative weight initialization). The training error (solid
lines) and the test error (dashed lines) are depicted using standard and Fenchel back-propagation
with different ﬁxed values for βk = β ∈ {0.01, 1, 1000} for all k. It can be seen in Fig. 1(left), that
Fenchel BP for small but non-vanishing choices of β reduces the training error similarly well as
standard back-propagation, and progress is slowed down only for a large value of β = 1000. On
the other hand, a value of β (cid:29) 0 is able to escape the vanishing gradient problem (Fig. 1(right)).
In particular, an all-negative weights initialization for the same ReLU-based MLP—which will not

LC and ¯L βββ

GC, L βββ

10

01020304050100101EpochsError(%)01020304050101102EpochsError(%)Std.BPFenBPβ=1/100FenBPβ=1FenBPβ=1000progress using back-propagation due to vanishing gradients—can be successfully trained by Fenchel
BP using a sufﬁciently large value for β (achieving 1.7%/2.79% training and test error compared to
88.66%/88.65% for standard BP and Fenchel BP with β ∈ {0.01, 1}). Understanding and exploring
this property and other variations of Fenchel BP is subject of future work.

Acknowledgements We are grateful to Guillaume Bourmaud and to anonymous reviewers for
helpful feedback. This work was partially supported by the Wallenberg AI, Autonomous Systems
and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.

References

[1] Luis B Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial environ-

ment. In Artiﬁcial neural networks: concept learning, pages 102–111. 1990.

[2] Ehsan Amid, Rohan Anil, and Manfred K Warmuth. Locoprop: Enhancing backprop via local loss

optimization. arXiv preprint arXiv:2106.06199, 2021.

[3] Armin Askari, Geoffrey Negiar, Rajiv Sambharya, and Laurent El Ghaoui. Lifted neural networks. arXiv

preprint arXiv:1805.01532, 2018.

[4] Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889–1900,

2000.

[5] Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target propaga-

tion. arXiv preprint arXiv:1407.7906, 2014.

[6] Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, and Zhouhan Lin. Towards biologi-

cally plausible deep learning. arXiv preprint arXiv:1502.04156, 2015.

[7] Kristin P Bennett, Gautam Kunapuli, Jing Hu, and Jong-Shi Pang. Bilevel optimization and machine
learning. In IEEE World Congress on Computational Intelligence, pages 25–47. Springer, 2008. Bilevel
optimization to model minimizing test error subject to parameters are minimizing training error.

[8] Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In

Artiﬁcial Intelligence and Statistics, pages 10–19, 2014.

[9] Stephan Dempe and Alain B Zemkoho. The bilevel programming problem: reformulations, constraint

qualiﬁcations and optimality conditions. Mathematical Programming, 138(1):447–473, 2013.

[10] Justin Domke. Generic methods for optimization-based modeling. In Artiﬁcial Intelligence and Statistics,

pages 318–326. PMLR, 2012.

[11] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In International Conference on Machine
Learning, pages 1568–1577. PMLR, 2018. Proves that unrolling converges to correct solution.

[12] Thomas Frerix, Thomas Möllenhoff, Michael Moeller, and Daniel Cremers. Proximal backpropagation.

arXiv preprint arXiv:1706.04638, 2017.

[13] Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks: A new hope. arXiv

preprint arXiv:1909.04866, 2019.

[14] Fangda Gu, Armin Askari, and Laurent El Ghaoui. Fenchel lifted networks: A lagrange relaxation of

neural network training. arXiv preprint arXiv:1811.08039, 2018.

[15] Rasmus Kjær Høier and Christopher Zach. Lifted regression/reconstruction networks. In British Machine

Vision Conference, 2020.

[16] Diederik P Kingma and Jimmy Ba. A method for stochastic optimization. In ICLR, 2015.

[17] Charles D Kolstad and Leon S Lasdon. Derivative evaluation and computational experience with large

bilevel mathematical programs. Journal of optimization theory and applications, 65(3):485–499, 1990.

[18] Karl Kunisch and Thomas Pock. A bilevel optimization approach for parameter learning in variational

models. SIAM Journal on Imaging Sciences, 6(2):938–983, 2013.

[19] Yann LeCun. A theoretical framework for back-propagation. In Proceedings of the 1988 connectionist

models summer school, volume 1, pages 21–28, 1988.

[20] Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation. In
Joint european conference on machine learning and knowledge discovery in databases, pages 498–515.
Springer, 2015.

[21] Jia Li, Cong Fang, and Zhouchen Lin. Lifted proximal operator machines. arXiv preprint arXiv:1811.01501,

2018.

11

[22] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit
differentiation. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1540–1552.
PMLR, 2020.

[23] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization
through reversible learning. In International conference on machine learning, pages 2113–2122. PMLR,
2015.

[24] Julien Mairal, Francis Bach, and Jean Ponce. Task-driven dictionary learning. IEEE transactions on

pattern analysis and machine intelligence, 34(4):791–804, 2011.

[25] Alexander Meulemans, Francesco S Carzaniga, Johan AK Suykens, João Sacramento, and Benjamin F

Grewe. A theoretical framework for target propagation. arXiv preprint arXiv:2006.14331, 2020.

[26] Beren Millidge, Alexander Tschantz, and Christopher L Buckley. Predictive coding approximates backprop

along arbitrary computation graphs. arXiv preprint arXiv:2006.04182, 2020.

[27] Javier R Movellan. Contrastive hebbian learning in the continuous hopﬁeld model. In Connectionist

Models, pages 10–17. Elsevier, 1991.

[28] Peter Ochs, René Ranftl, Thomas Brox, and Thomas Pock. Techniques for gradient-based bilevel optimiza-
tion with non-smooth lower level problems. Journal of Mathematical Imaging and Vision, 56(2):175–194,
2016.

[29] Alexander G Ororbia and Ankur Mali. Biologically motivated algorithms for propagating local target
representations. In Proceedings of the aaai conference on artiﬁcial intelligence, volume 33, pages 4651–
4658, 2019.

[30] Jirí V Outrata. A note on the usage of nondifferentiable exact penalties in some special optimization

problems. Kybernetika, 24(4):251–258, 1988.

[31] Fernando J Pineda. Generalization of back-propagation to recurrent neural networks. Physical review

letters, 59(19):2229, 1987.

[32] Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit

gradients. arXiv preprint arXiv:1909.04630, 2019.

[33] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-

propagating errors. Nature, 323(6088):533–536, 1986.

[34] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The

graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.

[35] Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-based

models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.

[36] Yuhang Song, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz. Can the brain do backpropaga-
tion?—exact implementation of backpropagation in predictive coding networks. NeuRIPS Proceedings
2020, 33(2020), 2020.

[37] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE,

78(10):1550–1560, 1990.

[38] James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a
predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5):1229–1262,
2017.

[39] Xiaohui Xie and H Sebastian Seung. Equivalence of backpropagation and contrastive hebbian learning in a

layered network. Neural computation, 15(2):441–454, 2003.

[40] Christopher Zach and Virginia Estellers. Contrastive learning for lifted networks. In British Machine

Vision Conference, 2019.

[41] Christopher Zach and Huu Le. Truncated inference for latent variable optimization problems. In Proc.

ECCV, 2020.

[42] Ziming Zhang and Matthew Brand. Convergent block coordinate descent for training tikhonov regularized
deep neural networks. In Advances in Neural Information Processing Systems, pages 1721–1730, 2017.

[43] Pan Zhou, Chao Zhang, and Zhouchen Lin. Bilevel model-based discriminative dictionary learning for
recognition. IEEE transactions on image processing, 26(3):1173–1187, 2016. Supervised dictionary
learning using KKT conditions.

12

A Proofs

Proof of Prop. 1. In the differentiable setting ˆz(β ) satisﬁes

0 = β (cid:96)(ˆz(β ) +

∂ E(ˆz(β ); θ )
∂ z

(55)

and total differentiation w.r.t. β at 0 yields

0 = (cid:96)(cid:48)(ˆz(β )) +

∂ 2E(ˆz(β ); θ )
∂ z2

d ˆz(β )
dβ

⇐⇒

d ˆz(β )
dβ

= −

(cid:18) ∂ 2E(ˆz(β ); θ )
∂ z2

(cid:19)−1

(cid:96)(cid:48)(ˆz(β ))

(56)

Letting β → 0 and using that ˆz(0) = z∗ yields the claimed relation,

d ˆz(β )
dβ

(cid:12)
(cid:12)β =0 = −

(cid:18) ∂ 2E(z∗; θ )
∂ z2

(cid:19)−1

(cid:96)(cid:48)(z∗).

(57)

The second relation in Prop. 1 is obtained by applying the chain rule.

Proof of Prop. 2. We restate Eq. 14 after introducing slack variables s ≥ 0

s.t. gi(z) − si = 0
The slack variables corresponding to strongly active constraints are forced to 0. Now ˆz(β ) satisﬁes (where we
simply write ˆz instead of ˆz(β ))

E(z; θ ) = f (z; θ )

(58)

∀i /∈ W ∗ : si = 0.

i(ˆz) = 0,
where λi ∈ R is the Lagrange multipliers for gi(z) − si, and (cid:96)(cid:48), f (cid:48) and g(cid:48)
derivative w.r.t. β yields

β (cid:96)(cid:48)(ˆz) + f (cid:48)(ˆz) +∑i λig(cid:48)

i are derivatives w.r.t. z. Taking the total

(59)

where ˙z and ˙λ are shorthand notations for d ˆz/dβ and dλ /dβ . We introduce

(cid:96)(cid:48)(ˆz) + β (cid:96)(cid:48)(cid:48)(ˆz)˙z + f (cid:48)(cid:48)(ˆz)˙z +∑i λ (cid:62)

i g(cid:48)(cid:48)

i (ˆz)˙z + g(cid:48)(ˆz)(cid:62) ˙λ = 0,

A := f (cid:48)(cid:48)(z∗) +∑i λ (cid:62)

i g(cid:48)(cid:48)

i (z∗)

B := g(cid:48)(z∗).

Thus, the above reads for β → 0+:

(60)

(61)

(cid:96)(cid:48)(z∗) + A˙z + B(cid:62) ˙λ = 0,
Taking the derivative of the constraint g(z) − s = 0 yields B˙z = ˙s. If the i-th constraint is an equality constraint,
then si is constant 0 and therefore ˙si = 0. Otherwise ˙si ≥ 0. Overall, we have a linear system of equations,
(cid:19) (cid:18) ˙z
˙λ

(cid:18)−(cid:96)(cid:48)
˙s

B(cid:62)
0

(cid:19)
.

(63)

(62)

A
B

=

(cid:19)

(cid:18)

This system together with ˙s ≥ 0 corresponds to the KKT conditions of the following quadratic program,

2 ˙z(cid:62)A˙z + c(cid:62) ˙z
1

s.t. B˙z = ˙s

˙s ≥ 0

∀i /∈ W ∗ : ˙si = 0,

which is equivalent to Eq. 17.

Proof of Prop. 3. Recall that
L β (θ ) = min
z

(cid:96)(z) + 1
β

(cid:0)E(z; θ ) − E(z∗; θ )(cid:1) = (cid:96)(ˆzβ ) + 1

β

(cid:0)E(ˆzβ ; θ ) − E(z∗; θ )(cid:1).

By using the optimality of ˆzβ , in particular (cid:96)(ˆzβ ) + β −1E(ˆzβ ; θ ) ≤ (cid:96)(z) + β −1E(z) for all z, we obtain:
(cid:96)(ˆzβ ) + β −1E(ˆzβ ; θ ) ≤ (cid:96)(z∗) + β −1E(z∗) ⇐⇒ (cid:96)(ˆzβ ) + β −1(cid:0)E(ˆzβ ; θ ) − E(z∗; θ )(cid:1)
(cid:125)

(cid:124)

≤ (cid:96)(z∗)
(cid:124) (cid:123)(cid:122) (cid:125)
= L (θ )

(cid:123)(cid:122)
= L β (θ )

⇐⇒ L β (θ ) ≤ L (θ ),

which shows the lower bound. Now let E(z; θ ) be strongly convex in z for all θ with parameter ν > 0. Thus,

E(z; θ ) ≥ E(z(cid:48); θ ) + ∇zE(z(cid:48); θ )(cid:62)(z − z(cid:48)) + ν

2 (cid:107)z − z(cid:48)(cid:107)2 =⇒ E(z; θ ) ≥ E(z∗; θ ) + ν

2 (cid:107)z − z∗(cid:107)2,

and therefore

min
z

(cid:110)

(cid:96)(z) − (cid:96)(z∗) + β −1E(z; θ )

(cid:96)(cid:48)(z∗)(cid:62)(z − z∗) + 1

(cid:111)
β E(z; θ )
β E(z∗; θ ) + ν

2β (cid:107)z − z∗(cid:107)2(cid:111)

(cid:111)

(cid:110)

(cid:110)

≥ min
z

≥ min
z

(cid:96)(cid:48)(z∗)(cid:62)(z − z∗) + 1

= 1

β E(z∗) − β

2ν (cid:107)(cid:96)(cid:48)(z∗)(cid:107)2,

13

(64)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

where the ﬁrst inequality uses the convexity of (cid:96) and the second one appies the strong convexity of E. The last
line is obtained by closed form minimization over z. Rearranging the above yields
2ν (cid:107)(cid:96)(cid:48)(z∗)(cid:107)2,

(cid:0)E(ˆz; θ ) − E(z∗; θ )(cid:1) + β

(cid:96)(z∗) ≤ (cid:96)(ˆz) + 1
β

(72)

Combining this with the lower bound ﬁnally provides

(cid:96)(ˆz) + 1
β

(cid:0)E(ˆz; θ ) − E(z∗; θ )(cid:1) ≤ (cid:96)(z∗) ≤ (cid:96)(ˆz) + 1
β

(cid:0)E(ˆz; θ ) − E(z∗; θ )(cid:1) + β

2ν (cid:107)(cid:96)(cid:48)(z∗)(cid:107)2,

(73)

i.e. Eq. 20.

Proof of Prop. 4. We start at the innermost level and obtain
s.t. z∗

(cid:96)(z∗
L)

k−1; θ )

k = arg min
zk

min
θ

k = 3, . . . , L

Ek(zk; z∗
(cid:26)

z∗
2 = arg min
z2

min
z1

(cid:8)E2(z2, z1; θ ) + 1
β1

E1(z1; θ )(cid:9) − min
z1

1
β1

(cid:27)

E1(z1; θ )

.

(74)

We deﬁne ˇE1(θ ) := minz1 E1(z1; θ )/β1. Hence the above reduces to
s.t. z∗

k−1; θ )

k = arg min
zk

(cid:96)(z∗
L)

min
θ

k = 3, . . . , L

Ek(zk; z∗
(cid:26)

z∗
2 = arg min
z2

min
z1

(cid:8)E2(z2, z1; θ ) + 1
β1

(cid:27)
E1(z1; θ )(cid:9) − ˇE1(θ )

.

(75)

Applying the expansion on z∗

(cid:96)(z∗
L)

min
θ

3 yields
k = arg min
zk

s.t. z∗

Ek(zk; z∗
k−1; θ )
(cid:26)
(cid:26)

k = 4, . . . , L

z∗
3 = arg min
z3

min
z2

E3(z3, z2; θ ) + 1
β2

min
z1

− 1
β2

min
z1,z2

(cid:8)E2(z2, z1; θ ) + 1
β1

(cid:8)E2(z2, z1; θ ) + 1
β1

(cid:27)
E1(z1; θ )(cid:9) − ˇE1(θ )
(cid:27)
E1(z1; θ ) − ˇE1(θ )(cid:9)
.

The ˇE1(θ ) terms cancel, leaving us with
(cid:96)(z∗
L)

s.t. z∗

k = arg min
zk

min
θ

k−1; θ )

k = 4, . . . , L

Ek(zk; z∗
(cid:26)

z∗
3 = arg min
z3

min
z1,z2

(cid:110)
E3(z3, z2; θ ) + 1
β2

− min
z1,z2

(cid:8) 1
β2

E2(z2, z1; θ ) + 1
β1β2

(cid:111)

E1(z1; θ )

E2(z2, z1; θ ) + 1
β1β2
(cid:27)
E1(z1; θ )(cid:9)

.

We deﬁne

and generally

U2(θ ) := min
z1,z2

(cid:8) 1
β2

E2(z2, z1; θ ) + 1
β1β2

E1(z1; θ )(cid:9)

Uk(θ ) := min
z1,...,zk

(cid:40) k
∑
l=1

1
j=l β j

∏k

(cid:41)

Ek(zk, zk−1; θ )

.

(76)

(77)

(78)

(79)

Thus, we obtain

(cid:96)(z∗
L)

min
θ

s.t. z∗

k = arg min
zk

z∗
3 = arg min
z3

Expanding until z∗

L−1 yields

Ek(zk; z∗
(cid:26)

min
z1,z2

k−1; θ )

k = 4, . . . , L

(cid:110)
E3(z3, z2; θ ) + 1
β2

E2(z2, z1; θ ) + 1
β1β2

(cid:111)

E1(z1; θ )

(cid:27)

−U2(θ )

.

(80)

(cid:96)(z∗
L)

min
θ

s.t. z∗

L = arg min
zL

min
z1,...,zL−1

(cid:40)

(cid:40) L
∑
k=1

1
∏L−1
l=k βl

(cid:41)

(cid:41)

Ek(zk, zk−1; θ )

−UL−1(θ )

(81)

A ﬁnal expansion step ﬁnally results in

(cid:40)

(cid:40)

min
θ

min
z1,...,zL

(cid:96)(zL) +

(cid:41)

L
∑
k=1

Ek(zk, zk−1; θ )
∏L−1
l=k βl

− min
z1,...,zL

(cid:40) L
∑
k=1

Ek(zk, zk−1; θ )
∏L−1
l=k βl

(cid:41)(cid:41)

= min
θ

L βββ

GC(θ ),

(82)

since the UL−1(θ ) terms cancel again.

14

(83)

(84)

Proof of Prop. 5. We start at the outermost level L and obtain

(cid:18)

min
θ

subject to z∗

(cid:8)(cid:96)(zL) + 1
βL

1
min
βL
zL
k = arg minzk Ek(zk, z∗
L = fL(z∗
k−1; θ ) for k = 1, . . . , L − 1. Using z∗
EL(z∗
L−1; θ ) =: ˇEL(z∗
L−1; θ ) = EL( fL(z∗
L, z∗
the above objective reduces to

L−1; θ ), z∗

L−1; θ )(cid:9) − min
zL

EL(zL, z∗

EL(zL, z∗

(cid:19)

L−1; θ )

k−1; θ ) and
L−1; θ )

(cid:19)

(cid:18)

min
θ

EL(zL, z∗

(cid:8)(cid:96)(zL) + 1
βL
(cid:0)EL(zL, z∗

min
zL
(cid:8)(cid:96)(zL) + 1
βL

L−1; θ )(cid:9) − 1

βL
L−1; θ ) − ˇEL(z∗

ˇEL(z∗
L−1; θ )(cid:1)(cid:9)

L−1; θ )

min
zL
k = arg minzk Ek(zk, z∗
k−1; θ ) for k = 1, . . . , L − 1. We have a nested minimization with L − 1 levels

subject to z∗
and a modiﬁed main objective. We apply the conversion a second time and obtain

= min
θ

(85)

(cid:18)

(cid:26)

min
θ

min
zL−1

min
zL

(cid:110)

(cid:96)(zL) + 1
βL

(cid:0)EL(zL, zL−1; θ ) − ˇEL(zL−1; θ )(cid:1)(cid:111)

+ 1
βL−1

EL−1(zL−1, z∗

L−2; θ )

(cid:27)

− min
zL−1

1
βL−1
(cid:110)

= min
θ

min
zL−1,zL

EL−1(zL−1, z∗

L−2; θ )

(cid:19)

(86)

(cid:96)(zL) + 1
βL

(cid:0)EL(zL, zL−1; θ ) − ˇEL(zL−1; θ )(cid:1) + 1
βL−1

(cid:0)EL−1(zL−1, z∗

L−2; θ ) − ˇEL−1(z∗

L−2; θ )(cid:1)(cid:111)

subject to the constraints on z∗

k for k = 1, . . . , L − 2. By continuing this process we ﬁnally arrive at

min
θ

min
z1,...,zL

(cid:26)
(cid:96)(zL) +∑L

k=1

1
βk

(cid:27)
(cid:0)Ek(zk, zk−1; θ ) − ˇEk(zk−1; θ )(cid:1)

as claimed in the proposition.

Prop. 6. We introduce g(z) := miny h(y, z) and y∗(z) = arg miny h(y, z), and observe that

(87)

(88)

∂ h(y∗(z), z)
∂ y
since ∂yh(y∗(z), z) = 0 by construction. This together with the deﬁnition of the linearized surrogate (Eq. 37)
yields the result.

∂ h(y∗(z), z)
∂ z

∂ h(y∗(z), z)
∂ z

dy∗(z)
dz

dg(z)
dz

(89)

=

+

=

,

·

Proof of Prop. 7. For brevity we omit the explicit dependence of E on θ below. Applying the linearized
contrastive surrogate on the outermost level of the deeply nested program in Eq. 21 yields

(cid:110)

(cid:96)(z∗

L) + (zL − z∗

L)(cid:62)(cid:96)(cid:48)(z∗

L) + 1
βL

min
zL

EL(zL, z∗

L−1)

(cid:111)

− min
zL

1
βL

EL(zL, z∗

L−1)

s.t. z∗

k = arg min
zk

Ek(zk, zk−1)

(90)

(cid:96)(zL). Now we apply Prop. 6 with

for k = 1, . . . , L − 1, where (cid:96)(cid:48)(zL) = d
dzL
L)(cid:62)(cid:96)(cid:48)(z∗
L) + (zL − z∗

g+
L (zL−1) := min
zL

(cid:96)(z∗

(cid:110)

L) + 1
βL

and therefore
∂
∂ zL−1
where ¯zL and z∗

EL(¯zL, zL−1)

∂
∂ zL−1

L (zL−1) = 1
g+
βL
L are the respective minimizers,
L) + (zL − z∗

L)(cid:62)(cid:96)(cid:48)(z∗

(cid:96)(z∗

(cid:110)

L) + 1
βL

¯zL := min
zL

Hence, the linearization of the outer loss in Eq. 90 at z∗

EL(zL, zL−1)

(cid:111)

g−
L (zL−1) := min
zL

1
βL

EL(zL, zL−1)

(91)

∂
∂ zL−1

g−
L (zL−1) = 1
βL

∂
∂ zL−1

EL(z∗

L, zL−1),

(92)

EL(zL, zL−1)

(cid:111)

z∗
L := arg min
zL

1
βL

EL(zL, zL−1).

(93)

L−1 is given by
(cid:111)
EL(zL, z∗

(cid:110)

L)(cid:62)(cid:96)(cid:48)(z∗

(cid:96)(z∗

L) + (zL − z∗

zL−1 (cid:55)→ min
zL
+ (zL−1 − z∗
= (cid:96)(z∗
+ (zL−1 − z∗
= (cid:96)(z∗

L−1)(cid:62) 1
∂
∂ zL−1
βL
L)(cid:62)(cid:96)(cid:48)(z∗
L) + (¯zL − z∗
L−1)(cid:62) 1
∂
∂ zL−1
βL
L)(cid:62)(cid:96)(cid:48)(z∗
L) + (¯zL − z∗

L−1)

L) + 1
βL
L−1)(cid:1)
L−1) − EL(z∗
L, z∗
EL(z∗
L−1) − 1
βL
L−1)(cid:1)
L, z∗
L−1) − EL(z∗
L−1) + (zL−1 − z∗

(cid:0)EL(¯zL, z∗
L) + 1
βL
(cid:0)EL(¯zL, z∗
L) + 1
βL

EL(¯zL, z∗

hL(¯zL, z∗

L, z∗

L−1)

L−1)(cid:62) 1
βL

− min
zL

1
βL

EL(zL, z∗

L−1)

(94)

(95)

(96)

∂
∂ zL−1

hL(¯zL, z∗

L−1)

15

using hk(zk, zk−1) := Ek(zk, zk−1) − minz(cid:48)
contrastive surrogates we arrive at

k

Ek(z(cid:48)

k, zk−1). Thus, after the second step of applying the linearized

L)(cid:62)(cid:96)(cid:48)(z∗

L) + 1
βL

hL(¯zL, z∗

L−1)

(cid:96)(z∗

L) + (¯zL − z∗
(cid:110)
(zL−1 − z∗

+ min
zL−1
= (cid:96)(z∗

L) + (¯zL − z∗

L−1)(cid:62) 1
∂
∂ zL−1
βL
L)(cid:62)(cid:96)(cid:48)(z∗
L) + 1
βL

hL(¯zL, z∗

hL(¯zL, z∗

L−1) + 1
βL−1
L−1) + (¯zL−1 − z∗

EL−1(zL−1, z∗

L−2)

(cid:111)

L−1)(cid:62) 1
βL

∂
∂ zL−1

1
− min
βL−1
zL−1
hL(¯zL, z∗
L−1) + 1
βL−1

EL−1(zL−1, z∗

L−2)

(97)

hL−1(¯zL−1, z∗

L−2)
(98)

subject to z∗
yields the claim of Prop. 7.

k = arg minzk Ek(zk, zk−1) for k = L − 2, . . . , 1. Continuation of these steps until the innermost level

B Bottom-Up Traversal for Linearized Surrogates

k := arg minzk Ek(zk, zk−1) (and z∗

Using the linearized surrogate (Eq. 37) is only possible if (cid:96)(·) is differentiable and each Ek(zk, zk−1) is differen-
tiable in zk−1. Recall that z∗
1 = arg minz1 E1(z1; θ )). Proceeding analogously to
Prop. 4 but using the linearized contrastive surrogates yields a modiﬁed global contrastive objective, where the
clamped and free energies are suitable linearized versions of E βββ and ˆE βββ :
Proposition 8. Let βββ = (β1, . . . , βL) be positive spacing parameters. Applying the linearized contrastive
surrogate (Eq. 37) recursively on Eq. 21 from the innermost to the outermost level yields the following linearized
global contrastive objective,

¯L βββ

GC(θ ) := min
z1,...,zL

(cid:110)

(cid:96)(z∗

L) + (zL − z∗

L)(cid:62)(cid:96)(cid:48)(z∗

(cid:111)
L) + ¯E βββ (z; θ )

− min
z1,...,zL

¯E βββ (z; θ ),

where

¯E βββ (z; θ ) :=

L
∑
k=2

Ek(zk, z∗

k−1; θ ) + (zk−1 − z∗

k−1)(cid:62)∂zk−1 Ek(zk, z∗

k−1; θ )

∏L

l=k βl

+

E1(z1; θ )
∏L
l=1 βl

.

(99)

(100)

In contrast to the global contrastive objective in Prop. 4, the resulting subproblem ¯E βββ (z; θ ) is generally not
convex in z even when E βββ is. zk depends on both zk−1 and zk+1 in ¯E βββ , hence minimizing ¯E βββ (and its loss-
augmented variant) poses usually a difﬁcult, non-convex optimization problem in z1, . . . , zL. Consequently, the
resulting linearized global contrastive objective

GC is of little further interest.

¯L βββ

C Julia Source Code

The plots in Fig. 1 in the main text were obtained by running the Julia code below.

using Statistics, MLDatasets
using Flux, Flux.Optimise
using Flux: onehotbatch, onecold, crossentropy
using Base.Iterators: partition
using ChainRulesCore, Random

train_x, train_y = MNIST.traindata(); test_x, test_y

= MNIST.testdata()

train_X = Float32.(reshape(train_x, 28, 28, 1, :));
test_X = Float32.(reshape(test_x, 28, 28, 1, :));

train_Y = onehotbatch(train_y, 0:9); test_Y

= onehotbatch(test_y, 0:9)

########################################################################

# Configure settings
use_negative_init = false
use_FenBP = true

const beta = 0.01f0

########################################################################

16

my_relu(x) = relu(x)

function ChainRulesCore.rrule(::typeof(my_relu), x)

z = my_relu(x)
function pullback(dy)

zz = my_relu(x - beta*dy)
return NoTangent(), (z - zz) / beta

end
return z, pullback

end

# We use this definition to prevent Flux from optimizing away our custom AD rule.
function my_act(x; use_FenBP = use_FenBP)

return use_FenBP ? map(my_relu, x) : relu.(x)

end

########################################################################

glorot_neg_uniform(rng::AbstractRNG, dims...) = (-abs.((rand(rng, Float32, dims...)

.- 0.5f0) .* sqrt(24.0f0 / sum(Flux.nfan(dims...)))))

glorot_neg_uniform(dims...) = glorot_neg_uniform(Random.GLOBAL_RNG, dims...)

########################################################################

# Fix random seed for repeatability across runs
Random.seed!(42)

m = Chain(

x -> reshape(x, :, size(x, 4)),
Dense(28*28, 256, init = use_negative_init ? glorot_neg_uniform : Flux.glorot_uniform),
my_act,
Dense(256, 128), my_act,
my_act,
Dense(128, 64),
softmax
Dense(64, 10),

)

loss(x, y) = sum(crossentropy(m(x), y))
accuracy(x, y) = mean(onecold(m(x), 1:10) .== onecold(y, 1:10))

eta = 0.001f0; opt = ADAM(eta)

train = ([(train_X[:,:,:,i], train_Y[:,i]) for i in partition(1:50000, 50)])

epochs = 50
for epoch = 1:epochs

iter = 1
total_loss = 0.f0; train_accuracy = 0.f0

for d in train

ps = params(m)
gs = gradient(ps) do

l = loss(d...); total_loss += l; train_accuracy += accuracy(d...); l

end
update!(opt, ps, gs); iter += 1

end

total_loss /= iter; train_accuracy /= iter
test_accuracy = accuracy(test_X, test_Y)

println(epoch, " loss: ", total_loss,

" (approx) train accuracy: ", 100*train_accuracy,
" test accuracy: ", 100*test_accuracy,
" (approx) train error: ", 100-100*train_accuracy,
" test error: ", 100-100*test_accuracy)

end

17

