8
1
0
2

l
u
J

8

]

R
T
.
n
i
f
-
q
[

1
v
7
8
7
2
0
.
7
0
8
1
:
v
i
X
r
a

Financial Trading as a Game

Financial Trading as a Game:
A Deep Reinforcement Learning Approach

Huang, Chien-Yi
Department of Applied Mathematics
National Chiao Tung University
Hsinchu, Taiwan

Editor: ABC XYZ

cyhuang.am03g@nctu.edu.tw

Abstract

An automatic program that generates constant proﬁt from the ﬁnancial market is lucrative
for every market practitioner. Recent advance in deep reinforcement learning provides a
framework toward end-to-end training of such trading agent. In this paper, we propose an
Markov Decision Process (MDP) model suitable for the ﬁnancial trading task and solve it
with the state-of-the-art deep recurrent Q-network (DRQN) algorithm. We propose several
modiﬁcations to the existing learning algorithm to make it more suitable under the ﬁnancial
trading setting, namely 1. We employ a substantially small replay memory (only a few
hundreds in size) compared to ones used in modern deep reinforcement learning algorithms
(often millions in size.) 2. We develop an action augmentation technique to mitigate the
need for random exploration by providing extra feedback signals for all actions to the
agent. This enables us to use greedy policy over the course of learning and shows strong
empirical performance compared to more commonly used (cid:15)-greedy exploration. However,
this technique is speciﬁc to ﬁnancial trading under a few market assumptions. 3. We
sample a longer sequence for recurrent neural network training. A side product of this
mechanism is that we can now train the agent for every T steps. This greatly reduces
training time since the overall computation is down by a factor of T . We combine all of
the above into a complete online learning algorithm and validate our approach on the spot
foreign exchange market.
Keywords:
foreign exchange

deep reinforcement learning, deep recurrent Q-network, ﬁnancial trading,

1. Introduction

In this paper we investigate the eﬀectiveness of applying deep reinforcement learning algo-
rithms to the ﬁnancial trading domain. Financial trading, diﬀers from the gameplay domain
or robotics, posts some unique challenges. We point out some of them that we believe hold
the key to successful application.

1.1 The Financial Trading Task

One way to describe the ﬁnancial trading task is as the following:

”An agent interacts with the market trying to achieve some intrinsic goal.”

1

 
 
 
 
 
 
Huang, Chien-Yi

Note that the agent needs not to be human; algorithmic trading now accounts for large
amount of trading activities in modern ﬁnancial markets. Common interaction involves
agent observing newly coming ﬁnancial data or submitting new order to the exchange, etc.
The intrinsic goal, say, for a hedge fund manager may be a risk-adjusted measure e.g. trying
to reach a 15% annual return target under a speciﬁed volatility threshold. The goal for a
naive trader may be simply to pursuit highest proﬁt without properly take into account the
risk incurred. An extreme example is that an individual that trades only for the ”gambling
sensation” and doesn’t care about the ﬁnancial market at all.

Although the above description is quite general, there are some characteristics of such

task:

1. The agent interacts with the ﬁnancial market at discrete time steps even though the
time steps may be extremely close, say, in the high frequency trading, trading decisions
can be made in the matter of milliseconds.

2. There is a set of legal actions an agent can apply to the market from naively submitting
market orders with a ﬁxed position size to submitting a fully speciﬁed limit order.

3. The ﬁnancial market produces new information available to the agent at each time
step enables the agent to make trading decisions. However, the agent doesn’t have
full clue on how the data is generated.

4. The agent has the potential to alter, although without full control over, the ﬁnancial
market if it is powerful enough. Hence it is not entirely realistic to consider the market
to be fully exogenous to the agent.

With these characteristics, we desire a uniﬁed framework for training such agent. This

is part of the motivation behind this thesis.

1.2 Motivation

With many successful stories of deep reinforcement learning, a natural question to ask is:

”Can an artiﬁcial agent learn to trade successfully?”

Success is deﬁned in terms of the degree the agent is reaching its intrinsic goal. One of
the most fundamental hypotheses of reinforcement learning is that goals of an agent can be
expressed through maximizing long-term future rewards. Reward is a single scalar feedback
signal that reﬂects the ”goodness” of an agent’s action in some state. This is called the
reward hypothesis.

Deﬁnition 1 (Reward Hypothesis) All goals can be described by maximization of expected
future reward.

The four characteristics mentioned above resembles that of reinforcement learning. A
branch of machine learning that studies the science of sequential decision-making. Rein-
forcement learning has recently received a considerable amount of attention due to solving
challenging control tasks that are infeasible before. The motivation behind this thesis is
therefore to see if the recently proposed techniques migrate to the ﬁnancial trading task
and to see how far we can go with these techniques.

2

Financial Trading as a Game

1.3 Challenges

We identify four major challenges to applying reinforcement learning to ﬁnancial trading:

1. Lack of baseline. With a large body of work published on applying deep reinforce-
ment learning to video gameplay and robotics. There is relatively little work on how
to apply the same algorithm to ﬁnancial trading Li (2017). There is no clear baseline
nor a suitable MDP model, network architecture or a set of hyperparameters can be
followed on early stage experiments.

2. Data quality and availability. Financial data are diﬃcult to obtain in high resolu-
tion. Usually only open, high, low and close prices (OHLC) data are freely accessible
which, may not be suﬃcient to produce successful trading strategy. The ﬁnancial time
series itself is non-stationary, posting challenges to standard, gradient-based learning
algorithms.

3. Partially observability of ﬁnancial markets. No matter how ”complete” our
input state is, there will always be a degree of unobservability in the ﬁnancial market.
We are unable to observe, for every market participator, their consensus on current
market condition.

4. Exploration and exploitation dilemma. Despite the sophistication of modern
deep reinforcement learning algorithms, usually a naive exploration policy is used. For
example, (cid:15)-greedy exploration in valued-based methods and Boltzmann exploration
in policy-based methods Sutton and Barto (1998). This is infeasible in ﬁnancial
trading setting since random exploration would inevitably generate huge amount of
transaction costs and hurt performance.

1.4 Contributions

The contributions of this thesis are three-fold:

1. We propose a Markov decision process (MDP) model for general signal-based ﬁnancial
trading task solvable by state-of-the-art deep reinforcement learning algorithm with
publicly accessible data only. The MDP model is easily extendable with more sophis-
ticated input features and more complex action spaces with minimal modiﬁcations to
model architecture and learning algorithm.

2. We modify the existing deep recurrent Q-network algorithm in a way that it’s more
suitable for the ﬁnancial trading task. This involves using a substantially smaller
replay memory and sampling a longer sequence for training. We are surprised by
the above two discoveries since in deep reinforcement learning, usually a large replay
memory is used and length of the sampled sequences are usually only a few time steps
long. We also discover workable hyperparameters for the DRQN algorithm able to
solve the ﬁnancial trading MDP through random search. We also develop a novel
action augmentation technique to mitigate the need for random exploration in the
ﬁnancial trading environment.

3

Huang, Chien-Yi

3. We achieve positive return on 12 diﬀerent currency pairs including major and cross
pairs under transaction costs. To the author’s best knowledge, this is the ﬁrst suc-
cessful application on real ﬁnancial data using pure deep reinforcement learning tech-
niques. Numerical results presented in this paper can serve as benchmarks for future
studies.

This thesis is structured as follows: in section 2, we give a detailed description on the
proposed method including data preparation, feature extraction, model architecture and
learning algorithm. In section 3, we combine all of the proposed techniques into a single
online learning algorithm.
In section 4, we evaluate our algorithm on the spot foreign
exchange market and provide numerical results.

2. Method

In this section we provide detailed description on the proposed MDP model, model archi-
tecture as well as the learning algorithm.

2.1 Data Preparation and Feature Extraction

We download tick-by-tick forex data from TrueFX.com from January 2012 to December 2017.
We pick 12 currency pairs, namely AUDJPY, AUDNZD, AUDUSD, CADJPY, CHFJPY,
EURGBP, EURJPY, EURUSD, GBPJPY, GBPUSD, NZDUSD and USDCAD. For diver-
sity, both major and cross pairs are included. We then resample the data into 15-minute
intervals with open, high, low, close prices and tick volume. The reason for choosing forex
over other asset classes is the ease of accessing high resolution data, often at very low or no
cost.

2.2 Financial Trading MDP

In this section we give deﬁnitions of the state space, action space and reward function of
the ﬁnancial trading MDP.

2.2.1 State Space ∈ R198

The state representation is a 198-dimensional vector consists of the following three parts:

• Time feature ∈ R3

Since the foreign exchange market has the longest opening hours among all ﬁnancial
markets.
In order for our agent to diﬀerentiate diﬀerent market sessions, we add
the minute, hour and day of week of the current time stamp to be part of the state
representation. This is encoded via a sinusoidal function

(cid:18)

sin

2π

(cid:19)

t
T

where t is the current value (zero-based numbering) and T is number of possible values
for t.

4

Financial Trading as a Game

• Market feature ∈ R16×12

We extract 16 features from OHLCV data containing 8 most recent log returns on
both closing price and tick volume. A running Z-score normalization of period 96
is then applied to each dimension of the 16 input features. We also clip the value
by 10 after normalization to eliminate outliers. We utilize price features from all 12
currency pairs in the hope that deep neural networks can extract useful intermarket
features from the data.

• Position feature ∈ R3

The agent’s current position is encoded via a 3-dimensional one-hot vector indicates
if the current position is of
whether the current position is of -1, 0 or +1 unit, e.g.
+1 unit, the encoding would be [0, 0, 1].

2.2.2 Action Space

We adopt a simple action set of three values {-1, 0, 1}. Position reversal is allowed (results
in double amount of transaction costs). Note that when the current position is +1 and the
agent again outputs +1 at the next time step, no trading action will be executed. This
sometimes refers to as target orders where the output indicates the target position size,
not the trading decision itself. This simpliﬁes the action space deﬁnition and makes the
implementation easier.

2.2.3 Reward Function

We deﬁne the reward function as portfolio log returns at each time step, i.e.

rt = log

(cid:19)

(cid:18) vt
vt−1

(1)

where vt is the portfolio value (account balance plus unrealized PnL from open positions).
With the above deﬁnition, the portfolio value vt satisﬁes a simple recursive relation

vt = vt−1 + at · c · (ct − ot) − dt

(2)

where at is the output action, c is the (constant) trade size, ot, ct are the current open,
close prices and dt is the commission term. The commission dt is computed by

dt = c · |at − at−1| · spread.

(3)

We use spread as a principled way of measuring the cost for making trading decisions.
The spread we consider here, unlike real spreads, is kept ﬁxed along the course of learning.
This is for the ease of comparing diﬀerent currency pairs since the width of the spread varies
from pair to pair.

Deﬁning the reward function this way, the return Gt has a nice interpretation as the
future discounted log returns. When facing action selection, the agent is eﬀectively picking
actions with the highest log returns. We prefer log returns over arithmetic returns as they
are additive which is more natural in the RL setting.

5

Huang, Chien-Yi

2.3 Fully Exploit with Action Augmentation

Random exploration is unsatisfying in the ﬁnancial trading setting since transaction costs
occur with a change of position. We propose a simple technique to mitigate the need for
exploration by providing the agent with reward signal for every action. This is possible
since the reward is easily computable after the price at the current timestep is observed
using equation (1). For example, if the unrealized PnL for the current step is +10 after we
execute action +1, then we immediate know that if we were to execute action -1, we would
get a reward of -10 and 0 for action 0. Therefore the portfolio value vt can be computed
(therefore the reward signal) for all actions.

On the other hand, the only part of the state that would be altered if we were to take
other actions is the agent’s position. This is known as the zero market impact hypothesis
which states that the action taken from the market participator has no inﬂuence on the
current market condition. We also assume order issued by the agent always executes at the
next opening price. That is, we always know the position for the next step if the output
action is determined.

Now we are able to update Q-values for all actions. We write down a novel loss function

in vector form called action augmentation loss,

L(θ) = E(s,a,r,s(cid:48))∼D

(cid:20)

θ ← θ − α∇θL(θ)

(cid:107)r + γQθ−(s(cid:48), arg max
a(cid:48)

Qθ(s(cid:48), a(cid:48))) − Qθ(s, a)(cid:107)2

(cid:21)

(4)

(5)

where Qθ− denotes the target network.

2.4 Model Architecture

We use a four-layered neural network as function approximator to represent the optimal
action-value function q∗. The ﬁrst two are linear layers with 256 hidden units and ELU
Clevert et al. (2015) activation. The third layer is an LSTM layer with the same size. The
fourth layer is another linear layer with 3 output units. The network is relatively small with
approximately 65,000 parameters.

2.4.1 Weight Initialization

Weight initialization is crucial for successful training of deep neural networks. We follow
initialization scheme presented in He et al. (2015) for weight matrices in both hidden layers
and input-to-hidden layer in LSTM. We follow Le et al. (2015) to initialize all hidden-to-
hidden weight matrices to be identity. We set all biases in the network to be zero except
for the forget gate in the LSTM which are set to be 1. We sparsely initialize the output
layer weight matrix with Gaussian distribution N (0, 0.001).

2.5 Training Scheme

In this section we combine all of the above and present a complete learning algorithm that
we will evaluate in section 5 on the spot foreign exchange market.

6

Financial Trading as a Game

Q(St)

output

ht−1

ht

· · ·

LST M

hidden

•

•

hidden

St

Figure 1: Model architecture.

2.5.1 Modified Training Scheme

After some experiment with the original updating scheme for DRQN, we proposed the
following modiﬁcations:

1. We discover that using a relatively small replay memory is more eﬀective. It is diﬀerent
from the ”common knowledge” in value-based deep reinforcement learning where large
replay memories (often millions in size) are used. This makes intuitive sense since in
ﬁnancial trading recent data points are more important than those from the far past.
The performance decreases if we enlarge the replay memory.

2. We sample a longer sequence from the replay memory than the number of steps used
in the DRQN paper. The reason behind this modiﬁcation is that, a successful trading
strategy involves opening a position at the right time and holding the position for a
suﬃciently long period of time then exiting the position. Sampling a short sequence
can’t eﬀectively train the network to learn the desired long-term dependency.

3. We ﬁnd that it is unnecessary to train the network for each step since we are sampling
a longer sequence. Hence we only train the network for every T time steps. This
signiﬁcantly reduces computation since the number of backward passes are reduced
by a factor of T . This is also beneﬁcial for real-time trading since trading decision
can be carried out with low latency and training can be deferred after market close.

2.5.2 A Complete Online Learning Algorithm

We adopt the above updating scheme to the original DRQN algorithm and propose a com-
plete online learning algorithm that we will evaluate in the next section. We discard the
common forwalk-walk optimization process that involves slicing the dataset into consec-
utive training and testing sets. Since each training set constructed this way are largely
overlapped, strong overﬁtting is observed in our early stage experiments. We therefore op-

7

Huang, Chien-Yi

timize our network in a purely online fashion that most resembles real-time trading. We
term the resulting algorithm ﬁnancial deep recurrent Q-network (Financial DRQN).

Algorithm 1 Financial DRQN Algorithm
1: Initialize T ∈ N, recurrent Q-network Qθ, target network Qθ− with θ− = θ,

dataset D and environment E, steps = 1

2: Simulate env E from dataset D
3: Observe initial state s from env E
4: for each step do
5:

6:

steps ← steps + 1
Select greedy action w.r.t. Qθ(s, a) and apply to env E
Receive reward r and next state s(cid:48) from env E

7:
8: Augment actions to form T = (s, a, r, s(cid:48)) and store T to memory D
9:

10:

11:

12:

if D is ﬁlled and steps mod T = 0 then
Sample a sequence of length T from D
Train network Qθ with equation (4) and (5)

end if
Soft update target network θ− ← (1 − τ )θ− + τ θ

13:
14: end for

In practice, we ﬁnd it useful to implement a simple OpenAI Gym-like environment
Brockman et al. (2016) for training. Since most open source backtesting engine is diﬃcult
to work with under the RL paradigm.

3. Experiment

In this chapter we present numerical results for the ﬁnancial DRQN algorithm on 12 currency
pairs, test the algorithm against diﬀerent spread settings and investigate the usefulness of
the proposed action augmentation technique.

3.1 Hyperparameters

In this section, we validate our approach on the spot foreign exchange market. We believe
our method can be extended to other ﬁnancial markets with minimal modiﬁcation. Hyper-
parameters used in Algorithm 1 are listed below which are quite standard in modern deep
reinforcement learning literature. Hyperparameters and model architecture are kept ﬁxed
across all experiments.

Hyperparameters
Learning timestep T
Replay memory size N
Learning rate

Value
96
480
0.00025
Optimizer Adam1
0.99
0.001

Discount factor
Target network τ

8

Financial Trading as a Game

We did not do an exhaustive search over hyperparameters but stick to ones that shows

good empirical results.

3.2 Simulation Result

We need additional parameters for trading simulation. The parameters are mainly used to
compute trading statistics such as annual return and Sharpe ratio. Parameters used are
listed below and kept ﬁxed for every simulation.

Simulation Parameters
Initial cash
Trade size
Spread (bp3)
Trading days

Value
100,0002
100,000
0.08
252 days/year

Below we present numerical results for 12 currency pairs. We consider two baselines:
buy-and-hold and ”sell-and-hold” since some of the currency pairs show constant down
trend throughout the test period. The one producing larger gain is used as baseline. Proﬁt
and loss are reported in terms of cumulative percentage returns. Every experiment is carried
out for 5 times. This serves as a ”robustness test” for the proposed approach. Equity curve
averaged over 5 runs are plotted in blue curve with one standard deviation range in shaded
area.

Table 1 summarizes the performance for each currency pair. Annual return and risk-
adjusted metrics are calculated by ﬁrst calculating the daily return4 and then annualized by
multiplying by factor 252(
252 for Sharpe and Sortino ratios.) Baselines are also provided
in the parenthesis for annual return. Maximum drawdown (MDD) and log return correlation
between the baseline is also computed using daily return.

√

Additional statistics on the overall trading activities is summarized in Table 2. We
discover that the agent favors higher win rates (around 60%) while maintaining a roughly
equal average proﬁt and loss per trade (about 2 basic points in diﬀerence). The trading ex-
pectation is calculated using the win rate and average PnL. Trading frequency is calculated
by dividing the length of the data by total number of trades.

3.3 Eﬀect of the Spread

Since spread is the only source for market friction, it is meaningful to examine algorithm
performance under various spread settings. We experiment with spread levels 0.08, 0.1, 0.15
and 0.2 basic points5 and discover the following facts:

1. Generally, wider spread results in worse performance. It ﬁts our intuition since the

transaction costs paid are proportional to the width of the spread.

1. The Adam optimizer Kingma and Ba (2014)
2. Initial cash is 100,000 in base currency.
3. We keep bp to be 0.0001 for non-JPY quoted currencies and 0.01 for JPY-quoted currencies.
4. We group one-step PnL into consecutive 96 steps to form the ”daily” PnL.
5. Spread levels are taken from a leading online breaker Interactive Brokers.

9

Huang, Chien-Yi

Net Proﬁt
93876.40
GBPUSD
55046.00
EURUSD
85658.40
AUDUSD
98984.80
NZDUSD
71585.40
USDCAD
61927.80
EURGBP
260776.20
AUDNZD
CADJPY
8923129.40
AUDJPY 11404412.00
CHFJPY 28816485.20
EURJPY 13576373.50
GBPJPY 26931635.80

Return Sharpe Sortino
2.5
1.6
2.7
4.0
2.5
3.5
12.4
3.1
3.3
6.3
3.2
5.8

16.2% (-3.5%)
9.5% (-1.6%)
14.8% (-4.2%)
17.1% (-1.2%)
12.2% (4.0%)
12.8% (1.1%)
34.3% (-2.8%)
20.4% (3.2%)
25.1% (2.0%)
60.8% (7.0%)
23.6% (6.1%)
39.0% (4.7%)

1.5
1.0
1.7
2.2
1.4
1.8
5.7
1.8
2.0
3.1
1.9
2.9

MDD Corr
-8.63% -0.09
-11.76% 0.01
-6.96% 0.02
-4.17% -0.04
-6.21% 0.11
-5.51% -0.21
-1.21% 0.02
-25.24% 0.20
-11.69% 0.18
-7.71% 0.31
-12.90% 0.18
-7.73% -0.07

Table 1: Annualized simulation results

GBPUSD
EURUSD
AUDUSD
NZDUSD
USDCAD
EURGBP
AUDNZD
CADJPY
AUDJPY
CHFJPY
EURJPY
GBPJPY

Num Trades Win Rate Avg Proﬁt Avg Loss Expect Freq
4.22
4.48
4.47
4.32
5.25
4.36
3.66
5.31
5.25
4.36
4.58
4.48

-87.33
-77.12
-66.6
-69.34
-80.16
-54.58
-67.18
-8612.01
-9883.08
-9294.91
-10801.23
-14503.05

70.25
60.67
54.52
52.17
63.46
37.76
49.93
6410.1
7092.02
7287.77
7483.41
10791.52

2.83
1.76
2.74
3.06
2.71
1.93
6.83
340.1
428.92
898.92
445.0
864.67

57.2%
57.2%
57.2%
59.6%
57.7%
61.2%
63.2%
59.6%
60.7%
61.5%
61.5%
60.8%

33133
31215
31263
32382
26636
32032
38173
26332
26638
32089
30509
31204

Table 2: Trading statistics

2. The agent stays proﬁtable for most of the currency pairs under 0.15 basic points of
spread. Proﬁtable strategies cannot be discovered under 0.2 basic point of spread for
currency pair USDCAD and EURGBP.

3. An interesting discovery is that wider spread does not always lead to worse perfor-
mance. For some of the JPY-quoted currency pairs the performance actually en-
hanced. We deem that a slightly wider spread forces the agent to locate a more
reliable strategy that is more robust under market change.

3.4 Eﬀectiveness of Action Augmentation

We investigate how useful is the action augmentation technique by comparing it to a tra-
ditional (cid:15)-greedy policy with (cid:15) = 0.1. With action augmentation, performance improves

10

Financial Trading as a Game

0.08 bp

6.1%
0.1%
7.3%
12.4%

16.2% 18.8%
GBPUSD
5.8%
9.5%
EURUSD
AUDUSD 14.8% 10.0%
NZDUSD 17.1% 14.2%
9.0%
USDCAD 12.2%
3.8%
EURGBP 12.8%
AUDNZD
CADJPY
AUDJPY
CHFJPY
EURJPY
GBPJPY

0.1 bp 0.15 bp 0.2 bp
6.7%
1.1%
5.2%
4.2%
6.9% -3.4%
-0.2% -3.8%
34.3% 35.9% 29.9% 23.4%
20.4% 32.4% 18.9% 14.8%
25.1% 26.4% 15.3% 10.2%
60.8% 79.8% 56.1% 43.5%
23.6% 35.6% 17.2% 15.2%
39.0% 44.4% 31.0% 27.0%
23.8% 26.3% 16.7% 11.9%

Table 3: Annualized return under diﬀerent spreads.

and standard deviation narrows, showing the algorithm is more robust and reliable. Ta-
ble 4 lists performances for both (cid:15)-greedy policy and action augmentation. We gain an
additional 6.4% annual return in average when we use action augmentation.

GBPUSD
EURUSD
AUDUSD
NZDUSD
USDCAD
EURGBP
AUDNZD
CADJPY
AUDJPY
CHFJPY
EURJPY
GBPJPY

(cid:15)-greed Act Aug
13.7%
7.1%
6.4%
9.5%
-4.1%
7.1%
28.1%
17.9%
20.3%
57.0%
15.0%
30.9%
17.4%

Gain
16.2% 2.5%
9.5% 2.4%
14.8% 8.4%
17.1% 7.6%
12.2% 16.3%
12.8% 5.8%
34.4% 6.3%
20.4% 2.5%
25.0% 4.8%
60.8% 3.8%
23.6% 8.6%
39.0% 8.1%
23.8% 6.4%

Table 4: Annualized returns with and without action augmentation.

4. Conclusion

We conclude and point future directions for this thesis in this chapter.

11

Huang, Chien-Yi

4.1 Achievements

The achievements of this thesis can be summarized as follows:

1. We propose an MDP model for signal-based trading strategies that is ﬂexible to future
extensions with minimal modiﬁcations to model architecture and learning algorithm.

2. We modify the existing deep recurrent Q-network learning algorithm to make it more
suitable in the ﬁnancial trading setting. Especially, we propose an action augmenta-
tion technique to mitigate the need for random exploration. We also use a substan-
tially smaller replay memory compared to ones used in value-based deep reinforcement
learning.

3. We give empirical results on the proposed algorithm for 12 currency pairs and achieve
positive results under most simulation settings. To the author’s best knowledge, this is
the ﬁrst positive result achieved by pure deep reinforcement learning algorithm under
transaction costs. Strategies discovered by the agent exhibit low or no correlation
between baselines.

4. We discover a counter-intuitive fact that a slightly increased spread leads to better
overall performance. This phenomena is observed for over half of the currency pairs.
We think a slightly higher spread forces the agent to discover more robust and reliable
trading strategies over the learning process. However, further widening the spread
destroys performance.

4.2 Future Work

There are many potentials for future improvements on the proposed method. We list some
of them that we believe are most important and interesting:

1. Expand state space and action space. We may augment more input features such
as price data from other markets (even ones seemly unrelated at ﬁrst glance), macro
data (released news from politics and economics, fundamental data such as economic
indices). For the action space, we may give the agent more freedom when making
trading decisions such as deciding how much to invest (i.e. the position size) or even
posting limit orders. This would requires a more complex action space and a careful
output representation of an action.

2. Apply reinforcement learning to diﬀerent trading scenarios, e.g. high frequency trad-
ing, pair trading or long-term equity investment. This is a further test for robustness
of our method. A portfolio combining many diﬀerent strategies can be created to suit
investors’ needs.

3. Make use of distributional reinforcement learning Bellemare et al. (2017) to take risk-
adjusted actions. In distributional reinforcement learning, rather than learning the
expected return E[Q(s, a)], the entire distribution over Q(s, a) is learned. This is
possible due to a distributional variant of the Bellman equation,

Q(s, a) D= R(s, a) + γQ(S(cid:48), A(cid:48)).

12

Financial Trading as a Game

In this thesis, we choose actions solely to maximize the expected return. That is, we
blindly maximize proﬁt without taking risk into account. This is unsatisfying since
it is clear that we would prefer an trading decision that comes with lower variance
although it may be less proﬁtable. Since the entire distribution is learned, we are able
to choose action with the highest expected Q-value and the lowest standard deviation
of the Q-value, i.e.

a = arg max
a∈A

E[Q]
(cid:112)Var[Q]

.

This way, we choose actions with the highest Sharpe ratio and the strategy would be
more suitable for modern investors.

13

Huang, Chien-Yi

Appendix

In this appendix we provide equity curves for all experiments done in section 4.

Figure 2: Performance under 0.08 basic points of spread.

Figure 3: Performances under diﬀerent spreads.

14

Financial Trading as a Game

Figure 4: Performances with and without action augmentation.

References

Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on rein-

forcement learning. arXiv preprint arXiv:1707.06887, 2017.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie
Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep
network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289,
2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers:
In Proceedings of the

Surpassing human-level performance on imagenet classiﬁcation.
IEEE international conference on computer vision, pages 1026–1034, 2015.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980, 2014.

Quoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton. A simple way to initialize recurrent

networks of rectiﬁed linear units. arXiv preprint arXiv:1504.00941, 2015.

Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1.

MIT press Cambridge, 1998.

15

