JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ?, MONTH 2022

1

Nonlinear Discrete Optimisation of
Reversible Steganographic Coding

Ching-Chun Chang

2
2
0
2

b
e
F
6
2

]

V
C
.
s
c
[

1
v
3
3
1
3
1
.
2
0
2
2
:
v
i
X
r
a

Abstract—Authentication mechanisms are at the forefront of
defending the world from various types of cybercrime. Steganog-
raphy can serve as an authentication solution by embedding a
digital signature into a carrier object to ensure the integrity of the
object and simultaneously lighten the burden of metadata man-
agement. However, steganographic distortion, albeit generally
imperceptible to human sensory systems, might be inadmissible
in ﬁdelity-sensitive situations. This has led to the concept of
reversible steganography. A fundamental element of reversible
steganography is predictive analytics, for which powerful neural
network models have been effectively deployed. As another core
aspect, contemporary reversible steganographic coding is based
primarily on heuristics and therefore worth further study. While
attempts have been made to realise automatic coding with neural
networks, perfect reversibility is still unreachable via such an
unexplainable intelligent machinery. Instead of relying on deep
learning, we aim to derive an optimal coding by means of
mathematical optimisation. In this study, we formulate reversible
steganographic coding as a nonlinear discrete optimisation prob-
lem with a logarithmic capacity constraint and a quadratic
distortion objective. Linearisation techniques are developed to
enable mixed-integer linear programming. Experimental results
validate the near-optimality of the proposed optimisation algo-
rithm benchmarked against a brute-force method.

Index Terms—Combinatorial mathematics, nonlinear discrete

optimisation, reversible steganographic coding.

I. INTRODUCTION

S TEGANOGRAPHY is the art and science of concealing

information within a carrier object. This terminology
encompasses a wide range of techniques and applications,
including but not limited to covert communications [1], own-
ership identiﬁcation [2], copyright protection [3], broadcast
monitoring [4], and traitor tracing [5]. An important applica-
tion of steganography is data authentication, which plays a
vital role in cybersecurity. The advent of data-centric artiﬁcial
intelligence is accompanied by cybersecurity concerns. It has
been reported that machine-learning models are vulnerable to
adversarial attacks such as invisible perturbations crafted to
cause wrong decisions [6], poisonous data collected for re-
training during deployment [7], and malware codes hidden
in neural network parameters [8]. A proper authentication
mechanism ensures that the integrity of data has not been
undermined and that the identity of users has not been forged,
thereby serving as a precaution against these insidious threats.
Digital signatures are an authentication mechanism based
upon modern cryptography [9]. This mechanism can be incor-
porated into a trustworthy forensic camera in such a way that

C.-C. Chang is with the University of Warwick, Coventry, UK (email:

c.c.chang@warwickgrad.net).

Digital Object Identiﬁer xx.xxxx/2022

photographs are generated and stored along with digital sig-
natures [10]. However, storing such auxiliary metadata might
entail the risk of accidental loss and mismanagement during
the data lifecycle. Steganography can serve as a potential
remedy to embed the auxiliary information about the data
into the data itself in an invisible manner. Yet, steganography
distortion, albeit generally imperceptible to human sensory
systems, might not be admissible in some ﬁdelity-sensitive
situations such as legal proceedings, medical diagnosis, and
military reconnaissance. This is where the notion of reversible
computing comes into play [11]–[15].

A fundamental element of reversible steganography, in com-
mon with lossless compression, is predictive analytics [16]–
[18]. Prediction error modulation is a cutting-edge reversible
steganographic technique composed of an analytics module
and a coding module [19]–[25]. The recent development of
deep learning has advanced the frontier of reversible steganog-
raphy. It has been reported that deep neural networks can
be applied as powerful predictive models [26]–[29]. Despite
an inspiring progress in the analytics module, the design of
the coding module based largely on heuristics. While there
are studies on end-to-end deep learning that attempts to use
neural networks for automatic reversible computing, perfect
reversibility cannot be promised [30]–[32]. From a certain
point of view, it is hard for neural networks, as a monolithic
black box, to learn the intricate logics of reversible computing.
Explainability of intelligent machinery is an ongoing open
research topic [33]–[36]. Therefore,
it seems advisable to
follow the modular framework at the time of writing.

This study is in pursuit of developing an optimal coding for
reversible steganography. We model reversible steganographic
coding as a mathematical optimisation problem and propose
an optimisation algorithm for addressing the nonlinear nature
of this problem. The remainder of this paper is organised
as follows. Section II outlines the background regarding re-
versible steganography. Section III formulates the nonlinear
discrete optimisation problem and discusses the complexity of
brute-force search. Section IV presents linearisation techniques
for tackling the nonlinear discrete optimisation problem. Sec-
tion V analyses the optimality of solutions through simulation
experiments. Section VI provides concluding remarks.

II. BACKGROUND

Prediction error modulation is a reversible steganographic
technique that consists of an analytics module and a coding
module. The analytics module begins by splitting a cover
image into the context and query sets, denoted by c and q,

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ?, MONTH 2022

2

Algorithm 1 Encoding

Input: cover, ω
Output: stego

(cid:46) analytics module
[c, ˜q] ← split(cover)
[˜c, ˜q] ← predict([c, 0])

(cid:46) coding module
ε ← q − ˜q
ε(cid:48) ← modulate(ε, ω)
q(cid:48) ← ˜q + ε(cid:48)
stego ← merge(c, q(cid:48))

Algorithm 2 Decoding

Input: stego
Output: cover, ω

(cid:46) analytics module
[c, q(cid:48)] = split(stego)
[˜c, ˜q] = predict([c, 0])

(cid:46) coding module
ε(cid:48) = q(cid:48) − ˜q
[ε, ω] = demodulate(ε(cid:48))
q = ˜q + ε
cover = merge(c, q)

carrier. While the peak frequency implies the highest capacity,
this capacity-greedy strategy is not necessarily optimal
in
terms of minimising distortion.

A. Problem Modelling

According to the typical law of error, the frequency of
an error can be expressed as an exponential function of its
numerical magnitude, disregarding sign [37]. In other words,
the frequency distribution of prediction errors is expected to
centre around zero. In general, a smaller absolute error tends
to have a higher occurrence. A special exception is that the
occurrence of zero might be lower than the occurrence of a
certain absolute error considering that the latter is the sum
of both positive and negative error occurrences. Consider an
absolute error histogram as shown in Figure 2. The problem
of reversible steganographic coding is to establish a mapping
between the values in [0, n] and the values in [0, n + ϑ], where
ϑ denotes the extra quota and is typical deﬁned to be less than
or equal to the number of successive empty bins in the absolute
error histogram. Encoding is a one-to-many mapping that links
a cover value to one or more stego values. A message digit can
only be represented if the connections are greater than one.
Different cover value can never yield the same stego value
in order to avoid an overlap between values and ambiguity

Fig. 1: Workﬂow of reversible steganography with prediction error modulation.

respectively. A conventional way is to divide pixels into two
halves according to a chequered pattern. Then, a predictive
model is applied to predict the query pixel intensities from
the context pixel intensities. A contemporary practice is to
employ an artiﬁcial neural network in computer vision. The
coding module embeds a message ω into the cover image by
modulating the prediction errors ε = q − ˜q. The modulated
errors ε(cid:48) is then added to the predicted intensities, causing
distortion to the query pixels. The stego image is created by
merging the context set c and the modulated query set q(cid:48). The
decoding procedure is similar to the encoding procedure. It be-
gins by predicting the query pixel intensities. Since the context
set is kept unchanged, the prediction in the decoding phase is
guaranteed to be identical to that in the encoding phase given
the same predictive model. The message is extracted and the
query set is recovered by demodulating the prediction errors.
The image is reversed to its original state by merging the
context set and the recovered query set. The procedures for
encoding and decoding are depicted schematically in Figure 1
and also provided in Algorithms 1 and 2. We would like to
note that the message may contain some auxiliary information
for handling pixel intensity overﬂow. This paper does not
go into details of every aspect of the stego-system; instead,
our study focuses on mathematical optimisation of reversible
steganographic coding.

III. NONLINEAR DISCRETE OPTIMISATION
The essence of reversible steganographic coding is to desig-
nate one or multiple error values as the carrier and to determine
how the values change to represent different message digits.
A conventional heuristic for reversible steganographic coding
is to choose the prediction errors of the peak frequency as the

... 0 1 0 0 1 0 0 1 0 ...coverstego context setcover query setstego query setstego errors cover errors message ENCODING DECODING predicted imagePredictiveNeural NetworkJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ?, MONTH 2022

3

Fig. 2: Example of absolute error distribution with highlighted zero occurrences.

in decoding. Therefore, a cover value of non-zero occurrence
may also be changed to a different stego value even if it is
not assigned to represent any digit. We conﬁne that each cover
value can only be mapped to the nearest available stego values
since a non-cross mapping reduces the problem dimension
drastically. We choose to start from the mapping of value 0 to
the mapping of value n because it is advisable to allocate a
slighter cumulative distortion to a value of higher occurrence.
An example of a cover/stego mapping is illustrated in Figure 3.
Let us denote by ai the frequency of the value i and xi the
number of extra cover-to-stego links for the value i. The total
number of links for the value i equals xi + 1. The number of
bits can be represented by the value i is log2(xi + 1) and thus
the capacity is computed by

C =

n
(cid:88)

i=0

ai log2(xi + 1).

(1)

Given the total number of cover-to-stego links xi, the probabil-
ity of changing a cover value to each stego value is 1/(xi +1).
The deviations of the ﬁrst to the last stego value are yi + 0
to yi + xi respectively, where yi denotes the sum of all the
previous extra links (i.e. the cumulative deviation). Hence,
the expected distortion in terms of the squared deviations is
computed by

Fig. 3: Example of reversible steganographic coding.

We can simplify the algebraic expression by

(0 + yi)2 + · · · + (xi + yi)2
xi + 1
(02 + 2yi · 0 + y2

i ) + · · · + (x2
xi + 1

i + 2yi · xi + y2
i )

(02 + · · · + x2

i ) + 2yi(0 + · · · + xi) + y2

i (xi + 1)

xi + 1

xi(xi + 1)(2xi + 1)
6(xi + 1)

+

2yixi(xi + 1)
2(xi + 1)

+

y2
i (xi + 1)
xi + 1

1
3

x2
i +

1
6

xi + xiyi + y2
i .

(4)

=

=

=

=

The reason for computing squared deviations rather than
absolute deviations is that image quality is often measured
by the peak signal-to-noise ratio (PSNR), which is deﬁned
via the mean squared error (MSE). Our goal
is to solve
for the decision variables xi ∈ {0, . . . ϑ} that minimise the
distortion objective subject to the capacity constraint. The sum
of all the extra cover-to-stego links cannot exceed the quota
ϑ. To summarise, the mathematical optimisation problem for
reversible steganographic coding is

n
(cid:88)

min D =

s.t. C =

i=0
n
(cid:88)

i=0

(cid:18) 1
3

ai

x2
i +

1
6

xi + xiyi + y2
i

(cid:19)

,

ai log2(xi + 1) ≥ payload,

n
(cid:88)

i=0

xi ≤ ϑ,

var. xi ∈ {0, · · · , ϑ},

∀i = 0, . . . , n.

D =

n
(cid:88)

i=0

ai

(cid:18) (0 + yi)2 + · · · + (xi + yi)2
xi + 1

(cid:19)

,

where

yi =

i−1
(cid:88)

j=0

xj.

(2)

(3)

B. Brute-Force Search

Brute-force search is a baseline method for benchmarking
optimisation algorithms. The solution space that exhausts all
possible combinations of the decision variables is equal to
(ϑ + 1)n+1 ∈ O(cn). By taking into account of the quota con-
straint, we can reduce the solution space from the number of
possible combinations to the number of feasible combinations.
In number theory and combinatorics, the partition function

2341023410567coverstegoJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ?, MONTH 2022

4

part(t) computes the number of ways of writing t as a sum
of positive integers in [1, t]. Let Λt denote a matrix of part(t)
rows and t columns that enumerates all possible partitions:

Λt =






λ1
...
λpart(t)




 =






λ1,1
...
λpart(t),1

λ1,t
...

· · ·
. . .
· · · λpart(t),t




.

(5)

Each vector λ(cid:96) represents a possible partition in which each
element is the quantity of a candidate integer (i.e. the sum-
mand). For example, Λ2, Λ3 and Λ4 are

1
(cid:20)
2
0

2

(cid:21)
0 λ1
1 λ2

,

1

3
1

0

2
0
1
0

3


0 λ1
0 λ2

1 λ3

,

1

4
2


0


1

0

2
0
1
2
0
0

3
0
0
0
1
0

4


0 λ1
0 λ2


0 λ3


0 λ4

1 λ5

.

The total number of feasible solutions can be calculated by
adding up the number of feasible solutions given by each
individual partition matrix from Λ1 to Λϑ (due to the quota
constraint); that is,

ϑ
(cid:88)

t=1

feasible(Λt, n∗),

(6)

where n∗ = n+1 denotes the number of integers in [0, n]. For
each matrix Λt, the number of feasible solutions is computed
by summing up the number of possible combinations given by
each partition vector λ(cid:96), denoted by

feasible(Λt, n∗) =

part(t)
(cid:88)

(cid:96)=1

comb(λ(cid:96), n∗).

(7)

Fig. 4: Example of binary integer decision variable.

The number of combinations can be approximated by

t
(cid:89)

(cid:18)n∗ − (cid:80)i−1

j=1 λ∗
j

(cid:19)

λ∗
i

(cid:19)(cid:18)n∗ − λ∗
1

(cid:19)

λ∗
2

i=1
(cid:18)n∗
λ∗
1

· · ·

(cid:18)n∗ − λ∗

1 − λ∗
2 − · · · − λ∗
λ∗
t

t−1

n∗!
λ∗
1!(n∗ − λ∗

1)!

(n∗ − λ∗
λ∗
2!(n∗ − λ∗

1)!
1 − λ∗

2)!

× . . .

×

n∗!

j )!

j=1 λ∗

2! . . . λ∗

t !(n∗ − (cid:80)t
λ∗
1!λ∗
n∗(n∗ − 1)(n∗ − 2) . . . (n∗ − ((cid:80)t
1!λ∗
λ∗
n∗(n∗ − 1)(n∗ − 2) . . . (n∗ − (t − 1))

2! . . . λ∗
t !

j=1 λ∗

j − 1))

≈ nt.

λ∗
1!λ∗

2! . . . λ∗
t !

=

=

=

=

≤

(cid:19)

(9)

A combination is a selection of values from a set of n∗ values
based on a given partition vector, as expressed by

Hence, the complexity of this speed-up brute-force algorithm
is approximately equal to

comb(λ(cid:96), n∗) =

t
(cid:89)

(cid:18)n∗ − (cid:80)i−1

j=1 λ∗
j

i=1

λ∗
i

(cid:19)
,

(8)

ϑ
(cid:88)

part(t)
(cid:88)

t=1

(cid:96)=1

comb(λ(cid:96), n∗) ≈

ϑ
(cid:88)

t=1

part(t) · nt ∈ O(nc).

(10)

where λ∗
i = λ(cid:96),i is the simpliﬁed notation regardless of the
index of the partition vector. It is a product of t binomial
coefﬁcients and each term is to choose (and remove) an
unordered subset of λ∗
i values from the remaining values in the
set of n∗ values. Let us take Λ3 for example. The number of
combinations for partition vectors λ1, λ2 and λ3 are computed
as follows:

comb(λ1, n∗) =

comb(λ2, n∗) =

comb(λ3, n∗) =

(cid:18)n∗
3

(cid:18)n∗
1

(cid:18)n∗
0

(cid:19)(cid:18)n∗ − 3

(cid:19)(cid:18)n∗ − 3 − 0

0

0

(cid:19)(cid:18)n∗ − 1

(cid:19)(cid:18)n∗ − 1 − 1

1

0

(cid:19)(cid:18)n∗ − 0

(cid:19)(cid:18)n∗ − 0 − 0

0

1

(cid:19)
,

(cid:19)
,

(cid:19)
.

IV. LINEARISATION
The difﬁculty of our optimisation problem lies in the
nonlinear nature of the capacity constraint and the distortion
objective. To apply off-the-shelf optimisation tools, we have
to tackle these nonlinearities.

A. Logarithmic Capacity Constraint

The capacity constraint involves the calculation of logarithm
of variables log2(xi+1). The logarithmic function is nonlinear.
A useful linearisation trick is to remodel the problem with
binary integer variables. We binarise each decision variable
xi with the domain [0, ϑ] into a 0/1 vector or a one-hot vector
of length ϑ + 1, as illustrated in Figure 4. The vector consists
of 0s with the exception of a single 1 of which the position
indicates the value of xi; that is,
i , · · · , xϑ

i ] ∈ {0, 1}ϑ+1,

xi = [x0

(11)

JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ?, MONTH 2022

5

such that

1 · x

(cid:124)
i = 1,

∀i = 0, . . . , n.

We can retrieve xi by the dot product of vectors

xi = [0, · · · , ϑ] · x

(cid:124)
i = vx

(cid:124)
i .

Accordingly, the quota constraint becomes

n
(cid:88)

i=0

vx

(cid:124)
i ≤ ϑ.

(12)

(13)

(14)

In a similar manner, the logarithm can be derived by the dot
product of vectors

(cid:124)
log2(xi + 1) = [log2(0 + 1), · · · , log2(ϑ + 1)] · x
i

= vlogx

(cid:124)
i .

Hence, we rewrite the capacity constraint as

C =

n
(cid:88)

i=0

(cid:124)
i .
aivlogx

B. Quadratic Distortion Objective

(15)

(16)

The distortion objective involves three nonlinear terms x2
i ,
y2
i and xiyi. These terms are quadratic functions of variables.
The ﬁrst term can be approached by the dot product as before;
that is

(cid:124)
x2
i = [02, . . . , θ2] · x
i = vsqx

(cid:124)
i .

(17)

The remaining two terms contain the partial sum of variables
yi, which is computed by

yi =

i−1
(cid:88)

j=0

vx

(cid:124)
j .

(18)

To linearise the univariate quadratic term y2
i and the bivariate
quadratic term xiyi, we introduce two non-negative continuous
slack variables zy2
≥ 0 and zxiyi ≥ 0. Replacing the quadratic
terms with the dot product and the slack variables results in a
linear distortion objective
n
(cid:88)

(cid:19)

i

xi + zxiyi + zy2

i

.

(19)

(cid:18) 1
3

ai

vsqx

(cid:124)
i +

1
6

D =

i=0

We begin by solving this mixed-integer linear programming
problem, which does not yet reﬂect the quadratic terms regard-
ing the cumulative distortion, and obtain an initial solution for
˜xi. The slack variables would be zeros because the objective
is to minimise distortion. To make the slack variables reﬂect
the quadratic terms properly, we add the following constraints

zy2

i

≥ y2
i ,

(20)

zxiyi ≥ xiyi.
In this way, we reformulate the problem with a nonlinear
objective into the problem with a linear objective and nonlinear
constraints. We make use of the solution obtained previously
to linearise these nonlinear constraints and solve the mixed-
integer linear programming problem iteratively. To begin with,
we express the variables in terms of the previous solution:

xi = ˜xi + δxi,
yi = ˜yi + δyi,

(21)

where ˜xi and ˜yi are treated as constants. Then, we apply the
Taylor series to approximate the univariate quadratic term as

f (yi) = f (˜yi + δyi)

= f (˜yi) + f (cid:48)(˜yi)δyi + · · ·
= ˜y2
i + 2˜yiδi + · · ·
≈ ˜y2
i + 2˜yi(yi − ˜yi)
= 2˜yiyi − ˜y2
i ,

and similarly the bivariate quadratic term as

f (xi, yi) = f (˜xi + δxi, ˜yi + δyi)

δyi + · · ·

δxi +

∂f
∂xi

= f (˜xi, ˜yi) +

∂f
∂yi
= ˜xi ˜yi + ˜yiδxi + ˜xiδyi + · · ·
≈ ˜xi ˜yi + ˜xi(yi − ˜yi) + ˜yi(xi − ˜xi)
= ˜xiyi + ˜yixi − ˜xi ˜yi.

(22)

(23)

As a result, the nonlinear constraints are transformed into the
linear constraints

2˜yiyi − zy2
˜xiyi + ˜yixi − zxiyi ≤ ˜xi ˜yi.

≤ ˜y2
i ,

i

(24)

To recapitulate, the nonlinear discrete optimisation problem
is approached by means of an iterative method that solves a
mixed-integer linear programming problem with binary integer
variables and non-negative continuous slack variables:

n
(cid:88)

min D =

s.t. C =

i=0
n
(cid:88)

i=0

(cid:18) 1
3

ai

(cid:124)
i +
vsqx

1
6

vx

(cid:124)
i + zxiyi + zy2

i

(cid:19)

,

aivlogx

(cid:124)
i ≥ payload,

n
(cid:88)

vx

(cid:124)
i ≤ ϑ,

i=0
(cid:124)
1 · x
i = 1,
2˜yiyi − zy2
˜xiyi + ˜yixi − zxiyi ≤ ˜xi ˜yi,

≤ ˜y2
i ,

i

var. xi ∈ {0, 1}ϑ+1,
≥ 0,

i

zy2
zxiyi ≥ 0,

∀i = 0, . . . , n,

∀i = 0, . . . , n,

∀i = 0, . . . , n,

∀i = 0, . . . , n,

∀i = 0, . . . , n,

∀i = 0, . . . , n,

∗

xi = vx

(cid:124)
i & yi =

i−1
(cid:88)

j=0

vx

(cid:124)
j .

V. SIMULATION

We carry out experimental analysis on the optimality of
the proposed method benchmarked against
the brute-force
method. The experimental setup is described as follows. We
apply the residual dense network (RDN) as the predictive
model [38]. This neural network model is characterised by a
tangled labyrinth of residual and dense connections, and has its
origin in low-level computer vision (e.g. super-resolution, de-
noising and debluring). The model is trained on the BOSSbase
dataset [39], which originated from an academic competition

JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ?, MONTH 2022

6

Aeroplane

Lena

Mandrill

Peppers

Fig. 5: Absolute error histograms with highlighted empty bins.

Aeroplane (ϑ = 1 & n = 55)

Lena (ϑ = 1 & n = 55)

Mandrill (ϑ = 1 & n = 55)

Pappers (ϑ = 1 & n = 55)

Aeroplane (ϑ = 2 & n = 55)

Lena (ϑ = 2 & n = 55)

Mandrill (ϑ = 2 & n = 55)

Pappers (ϑ = 2 & n = 55)

Aeroplane (ϑ = 3 & n = 55)

Lena (ϑ = 3 & n = 55)

Mandrill (ϑ = 3 & n = 55)

Pappers (ϑ = 3 & n = 55)

Aeroplane (ϑ = 4 & n = 55)

Lena (ϑ = 4 & n = 55)

Mandrill (ϑ = 4 & n = 55)

Pappers (ϑ = 4 & n = 55)

Fig. 6: Payload–distortion curves for optimality analysis against brute-force search.

01020304050absolute error0.000.050.100.150.200.250.300.350.400.45relative frequency01020304050absolute error0.000.050.100.150.200.250.300.350.400.45relative frequency01020304050absolute error0.000.050.100.150.200.250.300.350.400.45relative frequency01020304050absolute error0.000.050.100.150.200.250.300.350.400.45relative frequency0.00.10.20.30.40.5payload0.00.20.40.60.81.0distortionoptim.brute-force0.00.10.20.30.40.5payload0.00.20.40.60.81.0distortionoptim.brute-force0.00.10.20.30.40.5payload0.00.20.40.60.81.0distortionoptim.brute-force0.00.10.20.30.40.5payload0.00.20.40.60.81.0distortionoptim.brute-force0.00.10.20.30.40.50.60.7payload0.00.51.01.52.02.53.0distortionoptim.brute-force0.00.10.20.30.40.50.60.7payload0.00.51.01.52.02.53.0distortionoptim.brute-force0.00.10.20.30.40.50.60.7payload0.00.51.01.52.02.53.0distortionoptim.brute-force0.00.10.20.30.40.50.60.7payload0.00.51.01.52.02.53.0distortionoptim.brute-force0.00.10.20.30.40.50.60.70.80.9payload0.01.02.03.04.05.06.0distortionoptim.brute-force0.00.10.20.30.40.50.60.70.80.9payload0.01.02.03.04.05.06.0distortionoptim.brute-force0.00.10.20.30.40.50.60.70.80.9payload0.01.02.03.04.05.06.0distortionoptim.brute-force0.00.10.20.30.40.50.60.70.80.9payload0.01.02.03.04.05.06.0distortionoptim.brute-force0.00.10.20.30.40.50.60.70.80.91.01.1payload0.01.02.03.04.05.06.07.08.09.010.0distortionoptim.brute-force0.00.10.20.30.40.50.60.70.80.91.01.1payload0.01.02.03.04.05.06.07.08.09.010.0distortionoptim.brute-force0.00.10.20.30.40.50.60.70.80.91.01.1payload0.01.02.03.04.05.06.07.08.09.010.0distortionoptim.brute-force0.00.10.20.30.40.50.60.70.80.91.01.1payload0.01.02.03.04.05.06.07.08.09.010.0distortionoptim.brute-forceJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ?, MONTH 2022

7

for digital steganography. This dataset comprises a large
collection of greyscale photographs covering a wide variety
of subjects and scenes. The algorithms are tested on selected
images from the USC-SIPI dataset [40]. All the images are
resized to a resolution of 256 × 256 pixels via Lanczos
resampling [41]. The border pixels along with a half of the
rest pixels are designated as the context. Accordingly, the
number of query pixels equals (254 × 254)/2. We display
both distortion and capacity as divided by the number of query
pixels.

Figure 5 shows the absolute error distribution for each test
image. It is observed that most of the error values are within
the range from about 30 to 50. We conservatively set n = 55
in the sense that nearly every value of non-zero occurrence
is included. We implement the algorithms with respect to
different quota settings (ϑ = 1, 2, 3, 4). Figure 6 evaluates
the performance of the proposed optimisation algorithm. Each
point of the curve indicates the minimum distortion of a solu-
tion under a speciﬁc capacity constraint. In the vast majority of
cases, the solutions found by the proposed method are identical
to those given by the brute-force method. When failing to ﬁnd
the optimal solutions, the reached objective values are within
a small distance from the optimal ones. Hence, even though
the optimal solutions cannot be always guaranteed, the results
suggest that the proposed method can achieve a near-optimal
performance.

VI. CONCLUSION
This paper studies a mathematical optimisation problem in
reversible steganography. We formulate the prediction error
coding as a nonlinear discrete optimisation problem. The ob-
jective is to minimise distortion under a constraint on capacity.
We discuss the complexity of a brute-force method and the
linearisation techniques for logarithmic capacity constraint and
quadratic distortion objective. The problem is transformed into
an iterative mixed-integer linear programming problem with
binary integer variables and slack variables. Our simulation
results validate the near-optimality of the proposed algorithm.

REFERENCES

[1] J. Fridrich, M. Goljan, P. Lisonek, and D. Soukal, “Writing on wet
paper,” IEEE Trans Signal Process., vol. 53, no. 10, pp. 3923–3935,
2005.

[2] I. J. Cox, J. Kilian, F. T. Leighton, and T. Shamoon, “Secure spread
spectrum watermarking for multimedia,” IEEE Trans. Image Process.,
vol. 6, no. 12, pp. 1673–1687, 1997.

[3] M. Barni, F. Bartolini, V. Cappellini, and A. Piva, “A DCT-domain
system for robust image watermarking,” Signal Process., vol. 66, no. 3,
pp. 357–372, 1998.

[4] G. Depovere, T. Kalker, J. Haitsma, M. Maes, L. de Strycker, P. Termont,
J. Vandewege, A. Langell, C. Alm, P. Norman, G. O’Reilly, B. Howes,
H. Vaanholt, R. Hintzen, P. Donnelly, and A. Hudson, “The VIVA
project: Digital watermarking for broadcast monitoring,” in Proc. Int.
Conf. Image Process. (ICIP), Kobe, Japan, 1999, pp. 202–205.

[5] Shan He and Min Wu, “Joint coding and embedding techniques for
multimedia ﬁngerprinting,” IEEE Trans. Inf. Forensics Secur., vol. 1,
no. 2, pp. 231–247, 2006.

[6] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in Proc. Int. Conf. Learn. Represent. (ICLR), San
Diego, CA, USA, 2015, pp. 1–11.

[7] L. Mu˜noz Gonz´alez, B. Biggio, A. Demontis, A. Paudice, V. Wongras-
samee, E. C. Lupu, and F. Roli, “Towards poisoning of deep learning
algorithms with back-gradient optimization,” in Proc. ACM Workshop
Artif. Intell. Secur. (AISec), Dallas, TX, USA, 2017, pp. 27–38.

[8] T. Liu, Z. Liu, Q. Liu, W. Wen, W. Xu, and M. Li, “StegoNet: Turn deep
neural network into a stegomalware,” in Proc. Annu. Comput. Secur.
Appl. Conf. (ACSAC), Austin, TX, USA, 2020, pp. 928–938.

[9] R. L. Rivest, A. Shamir, and L. Adleman, “A method for obtaining digital
signatures and public-key cryptosystems,” Commun. ACM, vol. 21, no. 2,
pp. 120–126, 1978.

[10] G. Friedman, “The trustworthy digital camera: Restoring credibility to
the photographic image,” IEEE Trans. Consum. Electron., vol. 39, no. 4,
pp. 905–910, 1993.

[11] J. Fridrich, M. Goljan, and R. Du, “Invertible authentication,” in Proc.
SPIE Conf. Secur. Watermarking Multimedia Contents (SWMC), San
Jose, CA, USA, 2001, pp. 197–208.

[12] C. De Vleeschouwer, J.-F. Delaigle, and B. Macq, “Circular interpre-
tation of bijective transformations in lossless watermarking for media
asset management,” IEEE Trans. Multimedia, vol. 5, no. 1, pp. 97–105,
2003.

[13] A. M. Alattar, “Reversible watermark using the difference expansion of
a generalized integer transform,” IEEE Trans. Image Process., vol. 13,
no. 8, pp. 1147–1156, 2004.

[14] S. Lee, C. D. Yoo, and T. Kalker, “Reversible image watermarking based
on integer-to-integer wavelet transform,” IEEE Trans. Inf. Forensics
Secur., vol. 2, no. 3, pp. 321–330, 2007.

[15] G. Coatrieux, W. Pan, N. Cuppens-Boulahia, F. Cuppens, and C. Roux,
“Reversible watermarking based on invariant image classiﬁcation and
dynamic histogram shifting,” IEEE Trans. Inf. Forensics Secur., vol. 8,
no. 1, pp. 111–120, 2013.

[16] C. E. Shannon, “A mathematical theory of communication,” Bell Syst.

Tech. J., vol. 27, no. 3, pp. 379–423, 1948.

[17] J. Rissanen, “Universal coding, information, prediction, and estimation,”

IEEE Trans. Inf. Theory, vol. 30, no. 4, pp. 629–636, 1984.

[18] M. Weinberger and G. Seroussi, “Sequential prediction and ranking in
universal context modeling and data compression,” IEEE Trans. Inf.
Theory, vol. 43, no. 5, pp. 1697–1706, 1997.

[19] M. U. Celik, G. Sharma, A. M. Tekalp, and E. Saber, “Lossless
generalized-LSB data embedding,” IEEE Trans. Image Process., vol. 14,
no. 2, pp. 253–266, 2005.

[20] D. M. Thodi and J. J. Rodriguez, “Expansion embedding techniques for
reversible watermarking,” IEEE Trans. Image Process., vol. 16, no. 3,
pp. 721–730, 2007.

[21] M. Fallahpour, “Reversible image data hiding based on gradient adjusted

prediction,” IEICE Electron. Exp., vol. 5, no. 20, pp. 870–876, 2008.

[22] V. Sachnev, H. J. Kim, J. Nam, S. Suresh, and Y.-Q. Shi, “Reversible
watermarking algorithm using sorting and prediction,” IEEE Trans.
Circuits Syst. Video Technol., vol. 19, no. 7, pp. 989–999, 2009.
[23] X. Li, B. Yang, and T. Zeng, “Efﬁcient reversible watermarking based
on adaptive prediction-error expansion and pixel selection,” IEEE Trans.
Image Process., vol. 20, no. 12, pp. 3524–3533, 2011.

[24] I. Dragoi and D. Coltuc, “Local prediction based difference expansion
reversible watermarking,” IEEE Trans. Image Process., vol. 23, no. 4,
pp. 1779–1790, 2014.

[25] H. J. Hwang, S. Kim, and H. J. Kim, “Reversible data hiding using least
square predictor via the LASSO,” EURASIP J. Image Video Process.,
vol. 2016, no. 1, pp. 42: 1–12, 2016.

[26] C.-C. Chang, “Adversarial learning for invertible steganography,” IEEE

Access, vol. 8, pp. 198 425–198 435, 2020.

[27] R. Hu and S. Xiang, “CNN prediction based reversible data hiding,”

IEEE Signal Process. Lett., vol. 28, pp. 464–468, 2021.

[28] C.-C. Chang, “Neural reversible steganography with long short-term
memory,” Secur. Commun. Netw., vol. 2021, pp. 5 580 272:1–14, 2021.
[29] C.-C. Chang, X. Wang, S. Chen, I. Echizen, V. Sanchez, and C.-T.
Li, “Deep learning for predictive analytics in reversible steganography,”
arXiv, 2022.

[30] K.-H. Jung, Z. Zhang, G. Fu, F. Di, C. Li, and J. Liu, “Generative
reversible data hiding by image-to-image translation via GANs,” Secur.
Commun. Netw., vol. 2019, pp. 4 932 782:1–10, 2019.

[31] X. Duan, K. Jia, B. Li, D. Guo, E. Zhang, and C. Qin, “Reversible image
steganography scheme based on a U-Net structure,” IEEE Access, vol. 7,
pp. 9314–9323, 2019.

[32] S.-P. Lu, R. Wang, T. Zhong, and P. L. Rosin, “Large-capacity image
steganography based on invertible neural networks,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. (CVPR), virtual, 2021, pp. 10 816–
10 825.

[33] D. Castelvecchi, “Can we open the black box of AI?” Nature, vol. 538,

no. 7623, pp. 20–23, 2016.

[34] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal,
“Explaining explanations: An overview of interpretability of machine

JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ?, MONTH 2022

8

learning,” in IEEE Int. Conf. Data Sci. Adv. Anal. (DSAA), Turin, Italy,
2018, pp. 80–89.

[35] A. Barredo Arrieta, N. D´ıaz-Rodr´ıguez, J. Del Ser, A. Bennetot,
S. Tabik, A. Barbado, S. Garcia, S. Gil-Lopez, D. Molina, R. Benjamins,
R. Chatila, and F. Herrera, “Explainable artiﬁcial intelligence (XAI):
Concepts, taxonomies, opportunities and challenges toward responsible
AI,” Inf. Fusion, vol. 58, pp. 82–115, 2020.

[36] W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R. M¨uller,
“Explaining deep neural networks and beyond: A review of methods and
applications,” Proc. IEEE, vol. 109, no. 3, pp. 247–278, 2021.

[37] E. B. Wilson, “First and second laws of error,” J. Amer. Statist. Assoc.,

vol. 18, no. 143, pp. 841–851, 1923.

[38] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense
network for image super-resolution,” in Proc. IEEE/CVF Conf. Comput.
Vis. Pattern Recognit. (CVPR), Salt Lake City, UT, USA, 2018, pp.
2472–2481.

[39] P. Bas, T. Filler, and T. Pevn´y, “Break our steganographic system: The
ins and outs of organizing BOSS,” in Proc. Int. Workshop Inf. Hiding
(IH), Prague, Czech Republic, 2011, pp. 59–70.

[40] A. G. Weber, “The USC-SIPI image database: Version 5,” USC Viterbi
School Eng., Signal Image Process. Inst., Los Angeles, CA, USA, Tech.
Rep. 315, 2006.

[41] C. E. Duchon, “Lanczos ﬁltering in one and two dimensions,” J. Appl.

Meteorol., vol. 18, no. 8, pp. 1016–1022, 1979.

PLACE
PHOTO
HERE

Ching-Chun Chang received his PhD in Computer
Science from the University of Warwick, UK, in
2019. He engaged in a short-term scientiﬁc mission
supported by European Cooperation in Science and
the Faculty of Computer
Technology Actions at
Science, Otto-von-Guericke-Universit¨at Magdeburg,
Germany, in 2016. He was granted the Marie-Curie
fellowship and participated in a research and inno-
vation staff exchange scheme supported by Marie
Skłodowska-Curie Actions at
the Department of
Electrical and Computer Engineering, New Jersey
Institute of Technology, USA, in 2017. He was a Visiting Scholar with the
School of Computer and Mathematics, Charles Sturt University, Australia, in
2018, and with the School of Information Technology, Deakin University,
Australia,
in 2019. He was a Research Fellow with the Department of
Electronic Engineering, Tsinghua University, China, in 2020. His research
interests include steganography, watermarking, forensics, biometrics, cyber-
security, applied cryptography, image processing, computer vision, natural
language processing, computational linguistics, machine learning and artiﬁcial
intelligence.

