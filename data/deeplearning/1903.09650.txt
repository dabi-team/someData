Diﬀerentiable Programming Tensor Networks

Hai-Jun Liao,1, 2 Jin-Guo Liu,1 Lei Wang,1, 2, 3, ∗ and Tao Xiang1, 4, 5, †
1Institute of Physics, Chinese Academy of Sciences, Beijing 100190, China
2CAS Center for Excellence in Topological Quantum Computation,
University of Chinese Academy of Sciences, Beijing 100190, China
3Songshan Lake Materials Laboratory, Dongguan, Guangdong 523808, China
4University of Chinese Academy of Sciences, Beijing 100049, China
5Collaborative Innovation Center of Quantum Matter, Beijing 100190, China

Diﬀerentiable programming is a fresh programming paradigm which composes parameterized algorithmic
components and trains them using automatic diﬀerentiation. The concept emerges from deep learning but is not
only limited to training neural networks. We present theory and practice of programming tensor network algo-
rithms in a fully diﬀerentiable way. By formulating the tensor network algorithm as a computation graph, one
can compute higher order derivatives of the program accurately and eﬃciently using automatic diﬀerentiation.
We present essential techniques to diﬀerentiate through the tensor networks contraction algorithms, including
numerical stable diﬀerentiation for tensor decompositions and eﬃcient backpropagation through ﬁxed point it-
erations. As a demonstration, we compute the speciﬁc heat of the Ising model directly by taking the second
order derivative of the free energy obtained in the tensor renormalization group calculation. Next, we perform
gradient based variational optimization of inﬁnite projected entangled pair states for quantum antiferromag-
netic Heisenberg model and obtain start-of-the-art variational energy and magnetization with moderate eﬀorts.
Diﬀerentiable programming removes laborious human eﬀorts in deriving and implementing analytical gradi-
ents for tensor network programs, which opens the door to more innovations in tensor network algorithms and
applications.

I.

INTRODUCTION

Tensor networks are prominent approaches for studying
classical statistical physics and quantum many-body physics
problems [1–3]. In recent years, its application has expanded
rapidly to diverse regions include simulating and designing of
quantum circuits [4–7], quantum error correction [8, 9], ma-
chine learning [10–14], language modeling [15, 16], quantum
ﬁeld theory [17–20] and holography duality [21, 22].

One of the central problems relevant to many research di-
rections is the optimization of tensor networks in a general set-
ting. Despite highly successful optimization schemes for one
dimensional matrix product states [23–28], optimizing tensor
networks in two or higher dimensions has been a challenging
topic. The hardness is partly due to the high computational
cost of tensor contractions, and partly due to the lack of an ef-
ﬁcient optimization scheme in the high dimensional situation.
The diﬃculty is particularly pressing in optimizing tensor
network states for inﬁnite translational invariant quantum sys-
tems. In these cases, the same tensor aﬀects the variational
energy in multiple ways, which results in a highly nonlinear
optimization problem. Optimization schemes based on ap-
proximate imaginary time projection [29–33] have been strug-
gling to deal with nonlocal dependence in the objective func-
tion. Reference [34, 35] apply gradient based optimization
and show it signiﬁcantly improves the results. However, it is
cumbersome and error prone to derive the gradients of ten-
sor network states analytically, which involves multiple inﬁ-
nite series of tensors even for a simple physical Hamiltonian.

∗ wanglei@iphy.ac.cn
† txiang@iphy.ac.cn

This fact has limited broad adoption of the gradient based op-
timization of tensor network states to more complex systems.
Alternative approaches, such as computing the gradient us-
ing numerical derivative has limited accuracy and eﬃciency,
therefore only applies to cases with few variational parame-
ters [36, 37]. While deriving the gradient manually using the
chain rule is only manageable for purposely designed simple
tensor network structures [38].

Diﬀerentiable programming provides an elegant and ef-
ﬁcient solution to these problems by composing the whole
tensor network program in a fully diﬀerentiable manner. In
this paper, we present essential automatic diﬀerentiation tech-
niques which compute (higher order) derivatives of tensor net-
work programs eﬃciently to numeric precision. This progress
opens the door to gradient-based (and even Hessian-based)
optimization of tensor network states in a general setting.
Moreover, computing (higher order) gradients of the output of
a tensor network algorithm oﬀer a straightforward approach
to compute physical quantities such as the speciﬁc heat and
magnetic susceptibilities. The diﬀerentiable programming ap-
proach is agnostic to the detailed lattice geometry, Hamilto-
nian, and tensor network contraction schemes. Therefore, the
approach is general enough to support a wide class of tensor
network applications.

We will focus on applications which involve two dimen-
sional inﬁnite tensor networks where the diﬀerentiable pro-
gramming techniques oﬀer signiﬁcant beneﬁts compared to
the conventional approaches. We show that after solving ma-
jor technical challenges such as numerical stable diﬀerentia-
tion through singular value decomposition (SVD) and mem-
ory eﬃcient implementation for ﬁxed point iterations, one can
obtain the state-of-the-art results in variational optimization of
tensor network states.

The organization of this paper is as follows. In section II

9
1
0
2

l
u
J

2
1

]
l
e
-
r
t
s
.
t
a
m
-
d
n
o
c
[

2
v
0
5
6
9
0
.
3
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
we introduce automatic diﬀerentiation in the context of ten-
sor network algorithms and formulate tensor network contrac-
tions as computation graphs. In section III we present the key
techniques for stable and scalable diﬀerentiable programming
of tensor network algorithms. And section IV we demon-
strate the ability of the approach with applications to classical
Ising model and quantum Heisenberg model on the inﬁnite
square lattice. Finally, we outlook for future research direc-
tions opened by this work in Sec. V. Our code implementation
is publicly available at [39].

II. GENERAL THEORY

Automatic diﬀerentiation through a computation graph is
a uniﬁed framework that covers training neural networks for
machine learning, optimizing tensor networks for quantum
physics, and many more. We ﬁrst review the core idea of
automatic diﬀerentiation and then explain its application to
various tensor network contraction algorithms formulated in
terms of computation graphs.

A. Automatic diﬀerentiation

Automatic diﬀerentiation mechanically computes deriva-
tives of computation process expressed in terms of computer
programs [40]. Unlike numerical diﬀerentiation, automatic
diﬀerentiation computes the value of derivatives to the ma-
chine precision. The performance of automatic diﬀerentiation
has a general theoretical guarantee, which does not exceed
the algorithmic complexity of the original program [41, 42].
Automatic diﬀerentiation is the computational engine of mod-
ern deep learning applications [43, 44]. Moreover, automatic
diﬀerentiation also ﬁnds applications in quantum optimal con-
trol [45] and quantum chemistry calculations such as comput-
ing forces [46] and optimizing basis parameters [47].

Central to the automatic diﬀerentiation is the concept of the
computation graph. A computation graph is a directed acyclic
graph composed by elementary computation steps. The nodes
of the graph represent data, which can be scalars, vectors,
matrices, or tensors [48]. The graph connectivity indicates
the dependence of the data ﬂow in the computation process.
The simplest computation graph is a chain shown in Fig 1(a).
Starting from, say vector valued, input parameters θ one com-
putes a series of intermediate results until reaching the ﬁnal
output L, which we assume to be a scalar. The so called for-
ward evaluation simply traverses the chain graph in sequential
order θ → T 1 → · · · → T n → L.

To compute the gradient of the objective function with re-

spect to input parameters, one can exploit the chain rule

∂L
∂θ

= ∂L
∂T n

∂T n
∂T n−1

· · ·

∂T 2
∂T 1

∂T 1
∂θ

.

(1)

Since we consider the case where the input dimension is larger
than the output dimension, it is more eﬃcient to evaluate the
gradient in (1) by multiplying terms from left to the right using

2

Figure 1. Reverse mode automatic diﬀerentiation on computation
graphs. Black arrows indicate the forward function evaluation from
inputs to outputs. Red arrows indicate backward steps for adjoint
backpropagation. (a) A chain graph. (b) A more general computation
graph. In the backward pass, the adjoint of a given node is computed
according to Eq. (2).

a series of vector-Jacobian products. In terms of the compu-
tation graph shown in Fig. 1(a), one traverses the graph back-
ward and propagates the gradient signal from the output back
to the input. Computing the derivative this way is called re-
verse mode automatic diﬀerentiation. This approach, com-
monly referred to as the backpropagation algorithm [43], is
arguably the most successful method for training deep neural
networks.

It is instructive to introduce the adjoint variable T =
∂L/∂T to denote the gradient of the ﬁnal output L with re-
spect to the variable T . One sees that the reverse mode auto-
matic diﬀerentiation propagates the adjoint from T n = L ∂L
∂T n
with L = 1 all the way back to T i = T i+1 ∂T i+1
∂T i with i =
n−1, . . . , 1, and ﬁnally computes θ = T 1 ∂T 1
∂θ . In each step, one
propagates the adjoint backward via a local vector-Jacobian
product.

The adjoint backpropagation picture generalizes well to
more complex computation graphs. For example, the data
node T 1 in Fig. 1(b) aﬀects the ﬁnal output via two diﬀerent
downstream computation paths.
In the backward pass, one
needs to accumulate all contributions from its child nodes for
its adjoint. In general, the backpropagation rule reads

(cid:88)

T i =

j: child of i

T j ∂T j
∂T i .

(2)

The reverse mode automatic diﬀerentiation algorithm can
be understood as a message passing process on the computa-
tion graph. After a topological sort of the computation graph
deﬁned by the forward pass, one visits the graph backward
from the output node with adjoint L = 1. Each node collects
information from its child nodes to compute its own adjoint,
then passes this information to its parents. Thus, one can com-
pute the gradient with respect to all parameters in one forward

θℒT1T2T1=T2∂T2∂T1(a)(b)θℒT1T2T3T1=T2∂T2∂T1+T3∂T3∂T1and one backward pass. Typically one caches necessary in-
formation in the forward pass for eﬃcient evaluation of the
vector-Jabobian product in the backward pass.

The building blocks of a diﬀerentiable program are called
primitives. The primitives can be elementary operations such
as addition, multiplication, and math functions [49]. Each
primitive has an associated backward function in addition to
the ordinary forward function. The backward function back-
propagates the adjoints according to the vector-Jacobian prod-
uct Eq. (2). Note that one does not need to explicitly instan-
tiate or store the full Jacobian matrices. Moreover, one can
group many elementary computation steps together as a primi-
tive. For example, the linear algebra operations such as matrix
multiplication can be regarded as a primitive. In this way, the
forward pass of these customized primitive can be operated
as a black box. Designing the diﬀerentiable program in such
a modular way allows one to control the level of granularity
of automatic diﬀerentiation. There are several advantages of
crafting customized primitives for domain speciﬁc problems.
First, this can reduce the computation and memory cost. Sec-
ond, in some cases, it is numerically more stable by group-
ing several steps together. Third, one can wrap function calls
to external libraries into primitives, without the need to track
each individual computation step.

Modern machine learning frameworks support automatic
diﬀerentiation via various mechanisms.
For example,
TensorFlow [50] explicitly constructs computation graphs
using a customized language, autograd [51], PyTorch [52]
and Jax [53] track the program execution order at run time,
and Zygote [54] performs source code transformation at the
compiler level. These frameworks allow one to diﬀerenti-
ate through function calls, control ﬂows, loops and many
other computation instructions. Building on these infras-
tructures, diﬀerentiable programming is emerging as a new
programming paradigm that emphasizes assembling diﬀeren-
tiable components and learning the whole program via end-to-
end optimization [44]. Letting machines deal with automatic
diﬀerentiation mechanically has greatly reduced laborious hu-
man eﬀorts and reallocated human attention to design more
sophisticated and creative deep learning algorithms.

Finally, we note that it is also possible to evaluate Eq. (1)
from right to left, which corresponds to the forward mode au-
tomatic diﬀerentiation. The operation of the forward mode au-
tomatic diﬀerentiation is akin to the perturbation theory. One
can compute the objective function and the gradient in a single
forward pass without storing any intermediate results. How-
ever, the forward mode automatic diﬀerentiation is not favor-
able for computation graphs whose input dimension is much
larger than the output dimension [44]. Therefore, the majority
of deep learning work employs the reverse mode automatic
diﬀerentiation. For the same reason, we focus on the reverse
mode automatic diﬀerentiation for tensor network programs.

B. Computation graphs of tensor network contractions

A tensor network program maps input tensors to output ten-
sors, which we assume to be a scalar. Depending on the con-

3

texts, the input tenors may represent classical partition func-
tion or quantum wavefunction, and the outputs can be various
physical quantities of interest. It is conceptually straightfor-
ward to apply automatic diﬀerentiation to a tensor network
program by expressing the computation, and in particular, the
tensor network contractions as a computation graph.

As a pedagogic example, consider the partition function
of inﬁnite one dimensional Ising model Z = limN→∞ Tr(T N)
where T =

is a second rank tensor (matrix) rep-

(cid:32)

(cid:33)

eβ e−β
e−β eβ

resenting the Boltzmann weight. One can numerically access
the partition function in the thermodynamic limit by repeat-
edly squaring the matrix T and tracing the ﬁnal results. The
computation graph shows the simple structure as shown in
Fig. 1(a). Diﬀerentiating with respect to such computational
process involves backpropagating the adjoint through matrix
trace and multiplication operations, which is straightforward.
At this point, it is also worth distinguishing between the
exact and approximate tensor network contraction schemes.
Tensor networks on tree like graph can be contracted ex-
actly and eﬃciently. While other exact approaches, such as
the one used for counting and simulating quantum comput-
ing [55, 56], in general exhibit exponentially scaling with the
problem size. Nevertheless, it is straightforward to apply au-
tomatic diﬀerentiation to these exact algorithms since they
mostly involve tensor reshape and contractions.

We will focus on the less trivial cases of diﬀerentiating
through approximate tensor contractions, which typically in-
volve truncated tensor factorization or variational approxi-
mations. They cover important tensor network applications
which show great advantages over other numerical meth-
ods [1–3].
In particular, we are interested in contracting
inﬁnite tensor network, where the fundamental data struc-
ture is the bulk tensor. The contraction schemes loosely
fall into three categories, the ones based on coarse graining
transformations [57–63], the ones based on the corner trans-
fer matrix [64–66], and the ones based on matrix product
states [2, 24, 67]. Since the last two contraction schemes
are closely related [2], in the following we will focus on au-
tomatic diﬀerentiation for the tensor renormalization group
(Sec. II B 1) and corner transfer matrix renormalization group
approaches (Sec. II B 2) respectively.

1. Tensor renormalization group

Tensor renormalization group (TRG) contracts the tensor
network by factorizing and blocking the bulk tensors itera-
tively [57]. Figure 2(a) shows one step of the TRG iteration
as the computation graph, which includes 1(cid:13) Split the bulk
tensor in two ways using SVD, where we have truncated the
singular values and vectors to a prescribed bond dimension χ.
2(cid:13) Assemble the four 3-leg tensors generated in the last step
into a 4-leg tensor. After this contraction, we obtain a new
tensor for the next iteration.

The TRG method grows the lattice size exponentially fast.
So one quickly reaches the thermodynamic limit after a few
tens of iterations. Note that for numerical stability one needs

4

CTMRG iteration. Due to this reason, the converged environ-
ment tensors will depend on the bulk tensor in a complicated
way.

Unlike the TRG method [57], the CTMRG approach grows
the system size linearly. So one may need to iterate a bit
more steps to reach convergences in CTMRG. On the other
hand, the computational complexity O(d3χ3) and memory
cost O(d2χ2) of CTMRG are smaller than the ones of TRG
in terms of the cutoﬀ bond dimension.

III. TECHNICAL INGREDIENTS

To compute gradients of a tensor network program using
reverse mode automatic diﬀerentiation, one needs to trace the
composition of the primitive functions and propagate the ad-
joint information backward on the computation graph. Thank-
fully, modern diﬀerentiable programming frameworks [50–
54] have taken care of tracing and backpropagation for their
basics data structure, diﬀerentiable tensors, automatically.

What one needs to focus on is to identify suitable primitives
of tensor network programs and deﬁne their vector-Jacobian
products for backpropagation. The key components of tensor
network algorithms are the matrix and tensor algebras. And
there are established results on backward through these op-
erations [68–70]. First of all, it is straightforward to wrap
all BLAS routines as primitives with customized backward
functions. Next, although being less trivial, it is also possible
to derive backward rules for many LAPACK routines such as
the eigensolver, SVD, and QR factorization [68]. By treating
these linear algebra operations as primitives, one can com-
pose a diﬀerentiable program with eﬃcient implementations
of matrix libraries.

There are, however, a few practical obstacles to stable and
scalable implementation of diﬀerentiable tensor network pro-
grams. First, the backward for the eigensolver and SVD may
face numerical instability with degeneracy in the eigenvalues
or singular values. Second, the reverse mode automatic diﬀer-
entiation may incur large memory consumption, which pre-
vents one from reaching the same bond dimension of an ordi-
nary tensor network program. We present solutions to these
problems in below.

A. Stable backward through linear algebra operations

We present several key results on matrix derivatives involv-
ing linear algebra operations that are relevant to tensor net-
work algorithms. Recall the modular nature of reverse mode
automatic diﬀerentiation, one just needs to specify the local
backward function to integrate these components into a diﬀer-
entiable program. We will comment on their connections to
physics literature and pay special attention to stable numeri-
cal implementations [39]. For more information, one can refer
to [68–70].

Figure 2.
(a) The iteration step of TRG. (b) The iteration step of
CTMRG. Each tensor is a node in the computation graph. The prim-
itive functions in the computation graphs are SVD and tensor con-
tractions.

to rescale the tensor elements after each iteration. The com-
putational cost of TRG method scales O(χ6) and the memory
cost scales as O(χ4). After unrolling the iterations, the compu-
tation graph of the TRG method is similar to the simple chain
graph shown in Fig. 1(a). Within each iteration step, the basic
operations are tensor index permutation, truncated SVD and
tensor contractions. Since each of these operations is diﬀer-
entiable, one can backpropagate through the TRG procedure
to compute the derivative of a downstream objective function
with respect to the input tensor.

2. Corner transfer matrix renormalization group

The computation graph of the corner transfer matrix renor-
malization group (CTMRG) [64] has a more interesting topol-
ogy. The goal of CTMRG calculation is to obtain converged
corner and edge tensors which represent the environment de-
grees of freedom of the bulk tensor.

In cases where the bulk tensor has the full symmetry of the
square lattice, the step of one CTMRG iteration is shown in
Fig. 2(b). 1(cid:13) Contract the bulk tensor with the corner and edge
tensors to form a 4-leg tensor. 2(cid:13) Perform truncated SVD to
the 4-leg tensor, keeping the singular dimensions up to the
cut oﬀ χ. Keep the truncated singular matrix as the isomet-
ric projector. 3(cid:13) Apply the isometry to the 4-leg tensor from
the ﬁrst step to ﬁnd a new corner tensor. 4(cid:13) Apply the same
isometry to ﬁnd a new edge tensor for the next step. And iter-
ate this procedure until convergence. One sees that the same
bulk tensor with bond dimension d appears in each step of the

(b)(a)①②①②③ ④ 1. Symmetric eigensolver

The forward pass reads A = UDUT , where the diagonal
matrix D is a diagonal matrix of eigenvalues di and each col-
umn of the orthogonal matrix U is a corresponding eigenvec-
tor. In the computation graph the node A has two child nodes
U and D.

In the backward pass, given the adjoint U and D, we

have [68, 70]

(cid:20)

A = U

D + F (cid:12) (UT U − U

T

(cid:21)
U)/2

UT ,

(3)

where Fi j = (d j − di)−1 if i (cid:44) j and zero otherwise. The
symbol (cid:12) denotes an element-wise Hadamard product. One
can readily check that the gradient is also a symmetric matrix.
Equation (3) can be regarded as "reverse" perturbation the-
ory. When the downstream calculation does not depend on
the eigenstate, i.e. U = 0, the backward equation is related to
the celebrated Hellmann-Feynman theorem [71] which con-
nects the perturbation to the Hamiltonian and its eigenvalues.
Ref. [72] applied this special case of Eq. (3) for inverse Hamil-
tonian design based on energy spectra.

The appearance of the eigenvalue diﬀerence in the denom-
inator of F is a reminder of the ﬁrst order nondegenerate per-
turbation theory. Reference [47] concerned about the stability
of the backward through the eigensolver, thus turned to less ef-
ﬁcient forward mode automatic diﬀerentiation for variational
optimization of Hartree-Fock basis. Actually in many physi-
cal problems the ﬁnal object function depends on part of the
eigenvalues and eigenstates in a gauge independent way, e.g.,
a quadratic form of occupied eigenstates. In these cases, only
the eigenvalue diﬀerence between the occupied and unoccu-
pied states will appear in the denominator of F, which is a
familiar patten in the linear response theory [73]. Therefore,
degenerated eigenvalues would not necessarily cause problem
for these physical applications [74]. In practice, we found that
by using a Lorentzian broadening with 1/x → x/(x2 + ε) with
ε = 10−12, one can stabilize the calculation at the cost of in-
troducing a small error in the gradient, see also [70].

2. Singular value decomposition

A ubiquitous operation in tensor network algorithms is the
matrix SVD, which is used for canonicalization and factor-
ization of tensor networks [1, 2, 25]. The forward pass reads
A = UDV T , where A is of the size (m, n), and U, V T has the
size (m, k) and (k, n) respectively, and k = min(m, n). D is a di-
agonal matrix contains singular values di. In the reverse mode
automatic diﬀerentiation, given the adjoints U, D and V, one
can obtain [69]
A = 1
2

(cid:18)
V T V − V

UT U − U

+ F− (cid:12)

F+ (cid:12)

V T

U

U

(cid:19)(cid:21)

V

(cid:19)

(cid:18)

(cid:20)

T

T

+ UDV T + (I − UUT )UD−1V T + UD−1V

T

(I − VV T ), (4)

for i (cid:44) j and zero otherwise. To
where [F±]i j = 1
d j−di
prevent the numerical issue in case of degenerate singular val-
ues, we use the same Lorentzian broadening as Sec. III A 1 for

± 1

d j+di

5

the ﬁrst term, which works well in our experience. In prac-
tice, for variational tensor network calculation starting from
random tensors, the chance of having exact degenerate eigen-
values is small. And even if this happens, applying the round-
ing is a reasonable solution. While for the cases of degen-
eracy due to intrinsic reasons [59, 61, 75, 76], one will still
obtain the correct gradient as long as the end-to-end gradient
is well deﬁned. Lastly, inverting the vanishing singular values
in Eq. (4) is not a concern since the corresponding space is
usually truncated.

3. QR factorization

QR factorization is often used for canonicalization of tensor
networks [1, 2, 25]. In the forward pass, one factorizes A =
QR, where QT Q = I and R is an upper triangular matrix [77].
Depending on the dimensions (m, n) of the matrix A there are
two cases for the backward function.

For input shape of A matrix m ≥ n, R is a n × n matrix. The

backward pass reads [70]

A = (cid:104)

Q + Q copyltu(M)

(cid:105)

R−T ,

(5)

T

− Q

T
where M = RR
Q and the copyltu function generates
a symmetric matrix by copying the lower triangle of the input
matrix to its upper triangle, [copyltu(M)]i j = Mmax(i, j),min(i, j).
The multiplication to R−T can be dealt with by solving a linear
system with a triangular coeﬃcient matrix.

For the case of m < n, Q is of the size (m, m), and R is a
m × n matrix. We denote A = (X, Y) and R = (U, V), where
X and U are full rank square matrices of size (m, m). This
decomposition can be separated into two steps, ﬁrst X = QU
uniquely determines Q and U, then we calculate V = QT Y.
Applying the chain rule, the backward rule gives

A = (cid:16)(cid:104)

(cid:105)
(Q + VY T ) + Q copyltu(M)

U−T , QV

(cid:17)

,

(6)

where M = UU

T

− (Q + VY T )T Q.

B. Memory eﬃcient reverse mode automatic diﬀerentiation
with checkpointing function

A straightforward implementation of the reverse mode au-
tomatic diﬀerentiation for tensor networks has a large mem-
ory overhead. This is because one needs to store intermediate
results in the forward pass for evaluating the vector-Jacobian
products in the backward pass. The number of stored vari-
ables is related to the level of granularity in the implementa-
tion of the automatic diﬀerentiation. In any case, the memory
consumption of reverse mode automatic diﬀerentiation will
be proportional to the depth of the computation graph. This
is particularly worrying for tensor networks with large bond
dimensions and a large number of renormalization iterations.
The solution to the memory issue of reverse mode auto-
matic diﬀerentiation is a well known technique called check-
pointing [49, 78]. The idea is to trade the computational time

with memory usage. Taking the chain computation graph in
Eq. (1) as an example, one can store the tensor every a few
steps in the forward process. And in the backward pass, one
recomputes intermediate tensors whenever needed by running
a small segment of the computation graph forwardly. In this
way, one can greatly reduce the memory usage with no more
than twice of the computational eﬀort.

In a deeper understanding, checkpointing amounts to de-
ﬁne customized primitives which encapsulates a large part of
the computation graph. These primitives have their own spe-
cial backward rules which locally runs the forward pass again
and then backpropagates the adjoint. Therefore, in the for-
ward pass one does not need to cache internal states of these
checkpointing primitives.

The checkpointing is a general strategy that is applied to
the computation graph of any topological structure. When ap-
plied to tensor network algorithms, it is natural to regard the
renormalization steps shown in Fig. 2 as checkpointing prim-
itives. In this way, one avoids storing some large intermediate
tensors in the forward pass.

C. Backward through ﬁxed point iteration

Fixed point iteration is a recurring pattern in tensor net-
work algorithms. For example, one iterates the function
T i+1 = f (T i, θ) until reaching a converged tensor T ∗ and uses
it for downstream calculations. To compute the gradient with
respect to the parameter θ, one can certainly unroll the iter-
ation to a deep computation graph and directly apply the re-
verse mode automatic diﬀerentiation. However, this approach
has the drawback of consuming large memory if it takes long
iterations to ﬁnd the ﬁxed point.

One can solve this problem by using the implicit function
theorem on the ﬁxed point equation [79]. Taking the deriva-
tive on both sides of T ∗ = f (T ∗, θ), we have

θ = T ∗ ∂T ∗
∂θ

(cid:34)

I −

= T ∗

(cid:34)

T ∗

=

∞(cid:88)

n=0

∂ f (T ∗, θ)
∂T ∗
∂ f (T ∗, θ)
∂T ∗

(cid:35)−1 ∂ f (T ∗, θ)
∂θ

(cid:35)n ∂ f (T ∗, θ)
∂θ

.

(7)

The second line expands the matrix inversion in the square
bracket as a geometric series. Therefore, to backpropagate
through a ﬁxed point iteration, the basic operation is just the
vector-Jacobian products involving the single step iteration
function. And in the backward function one performs itera-
tion to accumulate the adjoint θ until reaching its convergence.
The geometric series show the same convergence rate to the
ﬁxed point as the forward iteration [79].

Many of the tensor network contraction schemes, includ-
ing the CTMRG method reviewed in Sec. II B 2, fall into the
framework of ﬁxed point iterations. Thus, one can use Eq. (7)
for backward through CTMRG calculation, where the itera-
tion is over the RG step shown in Fig. 2(b). We note that
the analytical gradient of inﬁnite tensor network contraction
derived in Refs. [34, 35] contains similar pattern, which is a
summation of geometric series.

6

Similar to the checkpoint technique of Sec. III B, Eq. (7)
also reduces the memory usage in the reverse mode automatic
diﬀerentiation since one does not need to store a long chain of
intermediate results in the forward iteration. Moreover, since
the downstream objective function is independent of how the
ﬁxed point tensor is obtained, one can also exploit an acceler-
ated iteration scheme [80] in the forward process [81]. There
is, however, a caveat when applying Eq. (7) to diﬀerentiating
tensor network RG algorithms. One may need to pay special
attention to the redundant global gauge in the RG iterations to
ensure the ﬁxed point equation indeed holds.

D. Higher order derivatives

Since the gradient can also be expressed as a computa-
tion graph, one can compute the seconder order derivatives
by applying automatic diﬀerentiation to the graph again. In
this way, one can in principle compute arbitrary higher order
derivatives of a program using automatic diﬀerentiation [49].
Deep learning frameworks [50–54] have out-of-the-box sup-
port for computing higher order derivatives.

The ability to compute higher derivatives supports Hessian
based optimization of tensor network states, such as the New-
ton method [82], for tensor network states. However, com-
puting and inverting the full Hessian matrix explicitly could
be prohibitively expensive and unnecessary. One can eﬃ-
ciently compute the Hessian-vector product via (cid:80)
x j =

∂2L
∂θi∂θ j

j

j

∂L
∂θ j

x j

∂
without constructing the Hessian matrix explic-
∂θi
itly [83]. This is suﬃcient for iterative linear equation solvers
used for the Newton method.

(cid:18)(cid:80)

(cid:19)

IV. APPLICATIONS

We present two applications to demonstrate the versatil-
ity of diﬀerentiable programming tensor network approach
for statistical physics and quantum many-body problems.
Our public available code implementation [39] employs
PyTorch [84] with a customized linear algebra automatic dif-
ferentiation library for improved numerical stability, see dis-
cussions in Sec. III A. While we note that one is readily to
reproduce the results with other modern deep learning frame-
works such as autograd [85], TensorFlow [86], Jax [87],
and Zygote [88] frameworks.

A. Higher order derivative of the free energy

Consider an Ising model on the square lattice with inverse
temperature β, its partition function can be expressed as a two

7

is only one input parameter β to be diﬀerentiated. We have
purposely chosen the present approach to show oﬀ the power
of diﬀerentiable programming with the reverse mode auto-
matic diﬀerentiation technique. Backpropagating through the
whole TRG procedure, and in particular the SVD, allows one
to compute physical observables using higher order deriva-
tives. It is remarkable that this works at all given many of the
degenerate singular values due to the Z2 symmetry of the Ising
model [47]. To obtain correct physical results, it is crucial to
implement the SVD backward function in a numerical stable
way as explained in Sec. III A 2.

B. Gradient based optimization of iPEPS

We consider a variational study of the square lattice antifer-

romagnetic Heisenberg model with the Hamiltonian

H =

(cid:88)

(cid:104)i, j(cid:105)

i S x
S x
j

+ S y

i S y

j

+ S z

i S z
j.

(10)

We consider an inﬁnite projected entangled pair state (iPEPS)
as the variational ansatz. The variational parameters are the
elements in the iPEPS

As

uldr

=

,

(11)

where s denotes the physical indices, and the remaining in-
dices u, l, d, r are for virtual degrees of freedom of the bond
dimension D. We initialize the tensor elements with random
Gaussian variables. The overlap of the iPEPS forms a tensor
network, where the bulk tensor is the double layer tensor with
bond dimension d = D2

Tuldr =

=

.

(12)

To contract the inﬁnite tensor network formed by this bulk
tensor we use the CTMRG method reviewed in Sec. II B 2.
We initialize the corner and edge tensors by partially tracing
out legs from the bulk tensor, then perform the CTMRG iter-
ation until we reach convergence in the corner and edge ten-
sors. After contraction, we can evaluate the expected energy
(cid:104)ψ|H|ψ(cid:105)/(cid:104)ψ|ψ(cid:105). Due to the translational invariance of the prob-
lem, it is suﬃcient to consider the expected energy on a bond

L =

(cid:44)

, (13)

where the black rectangle in Eq. (13) is the Hamiltonian
operator acting on a bond. We have performed a basis
rotation to the Hamiltonian so that the ground state will
have a single site unit cell. We use cutoﬀ bond dimension
χ = 30, 50, 80, 100, 144, 160 for D = 2, 3, . . . , 7 respectively.

Figure 3. Energy density and speciﬁc heat of the 2D Ising model.
They are computed by taking the ﬁrst and second order derivative of
the free energy obtained after 30 TRG iteration steps with a cutoﬀ
bond dimension χ = 30. Solid lines are exact solutions [89].

dimensional tensor network with bond dimension D = 2

Z =

.

(8)

The bulk tensor is [90]

Tuldr =

=

√

λuλlλdλr
2

δmod(u+l−d−r,2),

(9)

where λu = eβ + (−1)ue−β. We contract the inﬁnite tensor
network using the TRG approach discussed in Sec. II B 1. We
use a cut oﬀ bond dimension χ = 30 and iterate for 30 TRG
steps. Finally, we obtain the partition function Eq. (8) and the
free energy by tracing out the bulk tensor.

√

Next, we compute the physical observables such as energy
density and speciﬁc heat by directly taking derivatives of the
free energy using automatic diﬀerentiation, as shown in Fig. 3.
One notices that the energy density shows a kink and the
speciﬁc heat exhibits a peak around the critical temperature
βc = ln(1 +
2)/2 ≈ 0.44068679. Unlike numerical diﬀer-
entiation, these results are free from the ﬁnite diﬀerence er-
ror [60, 91]. Accurate computation of higher order derivatives
of the tensor network algorithm will be useful to investigate
thermal and quantum phase transitions. We note that it is in
principle possible to obtain the speciﬁc heat by directly com-
puting the energy variance [35, 92], which, however, involves
cumbersome summation of geometric series expressed in term
of tensor networks.

There are alternative ways to compute the speciﬁc heat with
automatic diﬀerentiation. For example, one can directly com-
pute the energy via using the impurity tensor and then take the
ﬁrst order derivative to obtain the speciﬁc heat. Or, one can
also use forward mode automatic diﬀerentiation since there

0.400.450.50β−1.6−1.4−1.2energydensity−∂lnZ∂βexact0.400.450.50β1.01.52.02.53.0speciﬁcheatβ2∂2lnZ∂β2exactrludrludsrlud8

update algorithms [29–33] [95]. Note that both [35] and our
ansatz contain only half of the variational parameters of the
one in [34], so the energy results are slightly higher than
Ref. [34] at D = 2, 3. However, for larger bond dimensions
D = 4, 5, 6, 7, our calculation reaches the lowest variational
energy for the inﬁnite square lattice Heisenberg model. Fig-
ure 4(b) shows the staggered magnetization measured on the
optimized state, which approaches to the extrapolated QMC
results at larger bond dimensions.

To obtain results for bond dimension D > 4, we need
to employ either the checkpoint technique III B or the ﬁxed
point iteration III C to keep the memory budget low enough
to ﬁt into a single Nvidia P100 GPU card with 12G mem-
ory. It is rather encouraging that with moderate eﬀort one can
reach the state-of-the-art performance in variational optimiz-
ing iPEPS [34, 35]. The success is also a nontrivial demon-
stration that one can indeed stabilize reverse mode automatic
diﬀerentiation for linear algebra operations appeared in scien-
tiﬁc computation [47].

We note that the present approach applies as well to ﬁ-
nite systems or problems with larger unit cells, more com-
plex Hamiltonians [96–99], and more sophisticated contrac-
tion schemes with improved eﬃciency [93], which is promis-
ing to deliver new physical results to quantum many-body
problems.

V. DISCUSSIONS

Computing the gradient via automatic diﬀerentiation sig-
niﬁcantly boosts the power of existing tensor network algo-
rithms. Researchers can focus on the core tensor network con-
traction algorithms without worrying about the tedious gradi-
ent calculations. The computational complexity of automatic
diﬀerentiation is the same as the forward contraction of the
tensor networks.

Besides greatly reducing human eﬀorts, the automatic dif-
ferentiation approach also computes a slightly diﬀerent gra-
dient than Refs. [34, 35]. The present approach computes
numerical exact gradient of an approximated energy density
via automatic diﬀerentiation. While Refs. [34, 35] ﬁrst derive
analytical expression of the energy gradient as inﬁnite ten-
sor networks, and then contract these networks approximately
to obtain approximated gradient. Thus, the two approaches
perform diﬀerentiate the approximation and approximate the
derivative respectively [44]. Other than the general recom-
mendation of Ref. [44], we ﬁnd that diﬀerentiating through
approximated tensor network contraction can be advantageous
for inﬁnite systems whose analytical derivative is complicated
to derive and approximate.

In this paper, we have focused on the high level applica-
tions of automatic diﬀerentiation that diﬀerentiates through
the whole contraction algorithms for optimizing tensor net-
works and computing physical observables. The same tech-
niques are also applicable to low level cases such as ﬁnding
optimal truncation bases or variational transformation of ten-
sor networks [63]. Moreover, besides the optimization of the
expected energy of quantum problems, the approach is also

Figure 4. (a) The relative error in the energy of 2D S = 1/2 an-
tiferromagnetic Heisenberg model compared to previous variational
results [34, 35]. The accuracy is measured relative to the extrapolated
quantum Monte Carlo (QMC) result [94]. (b) A comparison of the
staggered magnetization, where the dashed line is the extrapolated
QMC result [94]. The simple and full update reference data are also
from Ref. [34].

Since the expected energy decreases with the cutoﬀ dimen-
sion [36, 93], the approximated CTMRG contraction gives
variational upper bound to the ground state energy. The ex-
pected energy Eq. (13) has both explicit and implicit depen-
dence on the variational parameters in Eq. (11) via the corner
and edge tensors.

We compute the gradient of Eq. (13) with respect to the
single layer tensor Eq. (11) using automatic diﬀerentiation,
which automatically resolves the intricate structure in the
computation graph Fig. 2(b). The gradient computation takes
time comparable to the forward evaluation of the expected en-
ergy. Then, we optimize the iPEPS using quasi-Newton L-
BFGS algorithm [82] with the automatically computed gra-
dient. One quickly reaches an optimum after a few hundred
function and gradient evaluations. Figure 4(a) shows the rela-
tive error in energy compared to extrapolated quantum Monte
Carlo (QMC) results [94] for various bond dimensions. The
accuracy of the ground state energy is comparable to the state-
of-the-art results [34, 35], which were shown to be more ac-
curate than imaginary time projection based simple and full

10−510−410−310−2energyrelativeerror(a)simpleupdatefullupdateCorboz[34]Vanderstraeten[35]presentwork234567D0.300.320.340.360.380.400.42staggeredmagnetization(b)9

relevant to variational contraction of tensor networks [2, 100].
We expect diﬀerentiable programming techniques will be-
come an integrated part of the standard tensor network tool-
box.

A bonus of implementing tensor network programs using
deep learning frameworks [50, 52–54] is that one can read-
ily enjoy the GPU acceleration. The calculations of this work
were done with a single GPU card. Pushing this line of re-
search further, we envision that it will be rewarding to deploy
tensor network algorithms on emerging specialized hardware
in a larger scale.

Finally, it is useful to comment on the diﬀerence of auto-
matic diﬀerentiation for tensor networks and neural networks.
Typical neural network architectures do not involve sophisti-
cated linear algebra operations. However, with the develop-
ment of tensorized neural networks [101] and applications of
various tensor networks to machine learning problems [10–
14, 102], the boundary between the two classes of networks is

blurred. Thus, results presented this paper would also be rel-
evant to tensor network machine learning applications when
one moves to more sophisticated contraction schemes.

VI. ACKNOWLEDGMENT

We are grateful to Pan Zhang, Zhi-Yuan Xie, Wei Li,
Ling Wang, Dian Wu, Xiu-Zhe Luo, Shuo-Hui Li, Song
Cheng, Jing Chen, Xi Dai, Anders Sandvik, and Frank
Verstraete for useful discussions. We thank Philippe Cor-
boz and Laurens Vanderstraeten for providing the reference
data shown in Fig. 4. The authors are supported by the
National Key Research and Development Project of China
Grant No. 2017YFA0302901 and No. 2016YFA0302400,
the National Natural Science Foundation of China Grant
No. 11888101 and No. 11774398, and the Strategic Prior-
ity Research Program of Chinese Academy of Sciences Grant
No. XDB28000000.

[1] Román Orús, “A practical introduction to tensor networks:
Matrix product states and projected entangled pair states,”
Ann. Phys. (N. Y). 349, 117–158 (2014), arXiv:1306.2164.
[2] Jutho Haegeman and Frank Verstraete, “Diagonalizing trans-
fer matrices and matrix product operators: a medley of ex-
act and computational methods,” Annu. Rev. Condens. Matter
Phys. 8, 355 (2017), arXiv:1611.08519.

[13] Song Cheng, Lei Wang, Tao Xiang, and Pan Zhang, “Tree
tensor networks for generative modeling,” Phys. Rev. B 99,
155131 (2019).

[14] James Stokes and John Terilla, “Probabilistic Modeling With

Matrix Product States,” arXiv (2019), arXiv:1902.06888v1.

[15] Angel J. Gallego and Román Orús, “Language Design and

Renormalization,” arXiv (2017), arXiv:1708.01525.

[3] Román Orús, “Tensor networks for complex quantum sys-

[16] Vasily Pestun and Yiannis Vlassopoulos, “Tensor network lan-

tems,” arXiv (2018), arXiv:1812.04011.

[4] Igor Markov and Yaoyun Shi, “Simulating quantum compu-
tation by contracting tensor networks,” SIAM J. Comput. 38,
963 (2008).

[5] Itai Arad and Zeph Landau, “Quantum computation and the
evaluation of tensor networks,” SIAM J. Comput. 39, 3089
(2010), arXiv:0805.0040v3.

[6] Isaac H. Kim and Brian Swingle, “Robust entanglement renor-
malization on a noisy quantum computer,” arXiv (2017),
arXiv:1711.07500.

[7] William Huggins, Piyush Patel, K. Birgitta Whaley,

and
E. Miles Stoudenmire, “Towards Quantum Machine Learn-
ing with Tensor Networks,” Quantum Sci. Technol. 4, 024001
(2019), arXiv:1803.11537.

[8] Andrew J Ferris and David Poulin, “Tensor networks and
quantum error correction,” Phys. Rev. Lett. 113, 030501
(2014).

[9] Sergey Bravyi, Martin Suchara, and Alexander Vargo, “Eﬃ-
cient algorithms for maximum likelihood decoding in the sur-
face code,” Phys. Rev. A 90, 032326 (2014).

[10] E. Miles Stoudenmire and David J. Schwab, “Supervised
Learning with Tensor Networks,” Adv. Neural Inf. Process.
Syst. 29 (NIPS 2016) , 4799–4807 (2016), arXiv:1605.05775.
[11] E Miles Stoudenmire, “Learning relevant features of data
with multi-scale tensor networks,” Quantum Sci. Technol. 3,
034003 (2018).

[12] Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang,

and Pan
Zhang, “Unsupervised Generative Modeling Using Matrix
Product States,” Phys. Rev. X 8, 31012 (2018).

guage model,” arXiv (2017), arXiv:1710.10248.

[17] F. Verstraete and J. I. Cirac, “Continuous matrix product states
for quantum ﬁelds,” Phys. Rev. Lett. 104, 190405 (2010).
[18] Jutho Haegeman, Tobias J Osborne, Henri Verschelde, and
Frank Verstraete, “Entanglement renormalization for quantum
ﬁelds,” Phys. Rev. Lett. 110, 100402 (2013), arXiv:1102.5524.
[19] Qi Hu, Adrian Franco-Rubio, and Guifre Vidal, “Continu-
ous tensor network renormalization for quantum ﬁelds,” arXiv
(2018), arXiv:1809.05176.

[20] Antoine Tilloy and J. Ignacio Cirac, “Continuous tensor net-
work states for quantum ﬁelds,” Phys. Rev. X 9, 021040
(2019).

[21] Brian Swingle, “Entanglement renormalization and hologra-

phy,” Phys. Rev. D 86, 065007 (2012).

[22] Patrick Hayden, Sepehr Nezami, Xiao Liang Qi, Nathaniel
Thomas, Michael Walter, and Zhao Yang, “Holographic du-
ality from random tensor networks,” J. High Energy Phys. 11,
009 (2016), arXiv:1601.01694.

[23] Steven R. White, “Density matrix formulation for quantum
renormalization groups,” Phys. Rev. Lett. 69, 2863 (1992).
[24] G. Vidal, “Classical simulation of inﬁnite-size quantum lattice
systems in one spatial dimension,” Phys. Rev. Lett. 98, 070201
(2006), arXiv:0605597 [cond-mat].

[25] Ulrich Schollwöck, “The density-matrix renormalization
group in the age of matrix product states,” Ann. Phys. (N. Y).
326, 96–192 (2011), arXiv:1008.3477.

[26] E. M. Stoudenmire and Steven R. White, “Studying Two
Dimensional Systems With the Density Matrix Renor-
malization Group,” Annu. Rev. Condens. Matter Phys.
10.1146/annurev-conmatphys-020911-125018,
3

(2011),

arXiv:1105.1374.

[27] Zeph Landau, Umesh Vazirani, and Thomas Vidick, “A poly-
nomial time algorithm for the ground state of one-dimensional
gapped local Hamiltonians,” Nat. Phys. 11, 566–569 (2015).

[28] V. Zauner-Stauber, L Vanderstraeten, M T Fishman, F Ver-
straete, and J Haegeman, “Variational optimization algorithms
for uniform matrix product states,” Phys. Rev. B 97, 045145
(2018), arXiv:1701.07035v1.

[29] H. C. Jiang, Z. Y. Weng, and T. Xiang, “Accurate Determi-
nation of Tensor Network State of Quantum Lattice Models in
Two Dimensions,” Phys. Rev. Lett. 101, 090603 (2008).
[30] J. Jordan, R. Orús, G. Vidal, F. Verstraete, and J. I. Cirac,
“Classical simulation of inﬁnite-size quantum lattice systems
in two spatial dimensions,” Phys. Rev. Lett. 101, 250602
(2008), arXiv:0703788 [cond-mat].

[31] Philippe Corboz, Román Orús, Bela Bauer, and Guifré Vi-
dal, “Simulation of strongly correlated fermions in two spatial
dimensions with fermionic projected entangled-pair states,”
Phys. Rev. B 81, 165104 (2010), arXiv:0912.0646.

[32] H. H. Zhao, Z. Y. Xie, Q. N. Chen, Z. C. Wei, J. W. Cai, and
T. Xiang, “Renormalization of tensor-network states,” Phys.
Rev. B 81, 174411 (2010), arXiv:1002.1405.

[33] Ho N. Phien, Johann A. Bengua, Hoang D. Tuan, Philippe
Corboz, and Román Orús, “Inﬁnite projected entangled pair
states algorithm improved: Fast full update and gauge ﬁxing,”
Phys. Rev. B 92, 035142 (2015), arXiv:1503.05345.

[34] Philippe Corboz, “Variational optimization with inﬁnite pro-
jected entangled-pair states,” Phys. Rev. B 94, 035133 (2016),
arXiv:1605.03006.

[35] Laurens Vanderstraeten, Jutho Haegeman, Philippe Corboz,
and Frank Verstraete, “Gradient methods for variational opti-
mization of projected entangled-pair states,” Phys. Rev. B 94,
155123 (2016), arXiv:1606.09170.

[36] Didier Poilblanc and Matthieu Mambrini, “Quantum critical
phase with inﬁnite projected entangled paired states,” Phys.
Rev. B 96, 014414 (2017).

[37] Ji-Yao Chen, Laurens Vanderstraeten, Sylvain Capponi, and
Didier Poilblanc, “Non-Abelian chiral spin liquid in a quan-
tum antiferromagnet revealed by an iPEPS study,” Phys. Rev.
B 98, 184409 (2018), arXiv:1807.04385.

[38] Ling Wang, Ying-Jer Kao, and Anders W. Sandvik, “Plaquette
renormalization scheme for tensor network states,” Phys. Rev.
E 83, 056703 (2011), arXiv:0901.0214.

[39] See https://github.com/wangleiphy/tensorgrad for code imple-

mentation in PyTorch.

[40] Michael Bartholomew-Biggs, Steven Brown, Bruce Christian-
son, and Laurence Dixon, “Automatic diﬀerentiation of algo-
rithms,” J. Comput. Appl. Math. 124, 171–190 (2000).
[41] Walter Baur and Volker Strassen, “The Complexity of Partial

Derivatives,” Theor. Comput. Sci. 22, 317 (1983).

[42] Andreas Griewank, “On Automatic Diﬀerentiation,” in Math.
Program. Recent Dev. Appl. (Kluwer Academic Publishers,
1989) pp. 83–108.

[43] David E. Rumelhart, Geoﬀrey E. Hinton,

and Ronald J.
Williams, “Learning representations by back-propagating er-
rors,” Nature 323, 533 (1986).

[44] Atilim Günes Baydin, Barak A. Pearlmutter, Alexey Andreye-
vich Radul, and Jeﬀrey Mark Siskind, “Automatic diﬀerenti-
ation in machine learning: a survey,” J. Mach. Learn. 18, 1–43
(2015), arXiv:1502.05767.

[45] Nelson Leung, Mohamed Abdelhafez, Jens Koch, and David
Schuster, “Speedup for quantum optimal control from au-
tomatic diﬀerentiation based on graphics processing units,”
Phys. Rev. A 95, 042318 (2017), arXiv:1612.04929.

10

[46] Sandro Sorella and Luca Capriotti, “Algorithmic diﬀerentia-
tion and the calculation of forces by quantum Monte Carlo,” J.
Chem. Phys. 133 (2010).

[47] Teresa Tamayo-Mendoza, Christoph Kreisbeck, Roland
Lindh, and Alán Aspuru-Guzik, “Automatic Diﬀerentiation
in Quantum Chemistry with Applications to Fully Variational
Hartree-Fock,” ACS Cent. Sci. 4, 559–566 (2018).

[48] As noted in [103] one will need tensor calculus as a precise
language to present automatic diﬀerentiation, even the object
being diﬀerentiated is a matrix.

[49] Andreas Griewank and Andrea Walther, Evaluating Deriva-
tives (Society for Industrial and Applied Mathematics, 2008).
[50] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jef-
frey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfel-
low, Andrew Harp, Geoﬀrey Irving, Michael Isard, Yangqing
Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur,
Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens,
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng, “TensorFlow: Large-Scale
Machine Learning on Heterogeneous Distributed Systems,”
arXiv (2016), arXiv:1603.04467.

[51] Dougal Maclaurin, David Duvenaud, and Ryan P Adams,
“Autograd: Eﬀortless gradients in numpy,” in ICML 2015 Au-
toML Workshop (2015).

[52] Adam Paszke, Gregory Chanan, Zeming Lin, Sam Gross, Ed-
ward Yang, Luca Antiga, and Zachary Devito, “Automatic dif-
ferentiation in PyTorch,” 31st Conf. Neural Inf. Process. Syst.
(2017), arXiv:1011.1669.

[53] Matthew Johnson, Roy Frostig, and Chris Leary, “Compiling
machine learning programs via high-level tracing,” in SysML
(2018).

[54] Michael Innes, “Don’t unroll adjoint: Diﬀerentiating ssa-form

programs,” arXiv (2018), arXiv:1810.07951.

[55] Stefanos Kourtis, Claudio Chamon, Eduardo R Mucciolo, and
Andrei E Ruckenstein, “Fast counting with tensor networks,”
arXiv (2018), arXiv:1805.00475.

[56] Benjamin Villalonga, Sergio Boixo, Bron Nelson, Eleanor Ri-
eﬀel, Rupak Biswas, and Salvatore Mandr, “A ﬂexible high-
performance simulator for the veriﬁcation and benchmarking
of quantum circuits implemented on real hardware Benjamin,”
arXiv (2018), arXiv:1811.09599v1.

[57] Michael Levin and Cody P Nave, “Tensor renormalization
group approach to two-dimensional classical lattice models,”
Phys. Rev. Lett. 99, 120601 (2007).

[58] Z. Y. Xie, H. C. Jiang, Q. N. Chen, Z. Y. Weng, and Tao
Xiang, “Second Renormalization of Tensor-Network States,”
Phys. Rev. Lett. 103, 160601 (2008), arXiv:0809.0182.

[59] Zheng-Cheng Gu

and Xiao-Gang Wen,

“Tensor-
entanglement-ﬁltering
and
symmetry-protected topological order,” Phys. Rev. B 80,
155131 (2009), arXiv:0903.1069.

renormalization

approach

[60] Z. Y. Xie, Jing Chen, M. P. Qin, J. W. Zhu, L. P. Yang, and Tao
Xiang, “Coarse-graining renormalization by higher-order sin-
gular value decomposition,” Phys. Rev. B 86, 045139 (2012),
arXiv:1201.1144.

[61] Z. Y. Xie, J. Chen, J. F. Yu, X. Kong, B. Normand, and T. Xi-
ang, “Tensor Renormalization of Quantum Many-Body Sys-
tems Using Projected Entangled Simplex States,” Phys. Rev.
X 4, 011025 (2014).

[62] G. Evenbly and G. Vidal, “Tensor Network Renormalization,”

Phys. Rev. Lett. 115, 180405 (2015).

[63] Shuo Yang, Zheng-Cheng Gu, and Xiao-Gang Wen, “Loop
Optimization for Tensor Network Renormalization,” Phys.
Rev. Lett. 118, 110504 (2017).

[64] Tomotoshi Nishino and Kouichi Okunishi, “Corner Transfer
Matrix Renormalization Group Method,” J. Phys. Soc. Jpn. 65,
891 (1996).

[65] Román Orús, “Exploring corner transfer matrices and corner
tensors for the classical simulation of quantum lattice sys-
tems,” Phys. Rev. B 85, 205117 (2012), arXiv:1112.4101.
[66] Philippe Corboz, T. M. Rice, and Matthias Troyer, “Com-
peting States in the t-J Model: Uniform d-Wave State versus
Stripe State Philippe,” Phys. Rev. Lett. 113, 046402 (2014).

[67] R. Orus and G. Vidal, “Inﬁnite time-evolving block decima-
tion algorithm beyond unitary evolution,” Phys. Rev. B 78,
155117 (2008).

[68] Mike Giles, An extended collection of matrix derivative re-
sults for forward and reverse mode algorithmic diﬀerentiation,
Tech. Rep. (2008).

[69] James Townsend, Diﬀerentiating the Singular Value Decom-

position, Tech. Rep. (2016).

[70] Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Eric Meissner,
and Neil D. Lawrence, “Auto-Diﬀerentiating Linear Algebra,”
arXiv (2017), arXiv:1710.08717.

[71] Richard P. Feynman, “Forces on Molecules,” Phys. Rev. 56,

340–343 (1939).

[72] Hiroyuki Fujita, Yuya O Nakagawa, Sho Sugiura,

and
Masaki Oshikawa, “Construction of Hamiltonians by super-
vised learning of energy and entanglement spectra,” Phys. Rev.
B 97, 075114 (2018).

[73] Richard M Martin, Electronic structure: basic theory and
practical methods (Cambridge university press, 2004).
[74] The cases where one has exact degeneracy between occu-
pied and unoccupied eigenstates is itself a singular case. One
should include or exclude all degenerated states in these cases.
and Masaki
Oshikawa, “Entanglement spectrum of a topological phase
in one dimension,” Phys. Rev. B 81, 064439 (2010),
arXiv:0910.1811.

[75] Frank Pollmann, Ari M. Turner, Erez Berg,

[76] Eﬁ Efrati, Zhe Wang, Amy Kolan,

and Leo P. Kadanoﬀ,
“Real-space renormalization in statistical mechanics,” Rev.
Mod. Phys. 86, 647–667 (2014).

[77] One can also require the diagonal of the R matrix to be positive
to ﬁx the redundant gauge degrees of freedom. This can be
easily implemented on top of the existing QR libraries with
backward support.

[78] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin,
“Training deep nets with sublinear memory cost,” arXiv
(2016), arXiv:1604.06174.

[79] Bruce Christianson, “Reverse accumulation and attractive
ﬁxed points,” Optim. Methods Softw. 3, 311–326 (1994).
[80] M. T. Fishman, L. Vanderstraeten, V. Zauner-Stauber,
J. Haegeman, and F. Verstraete, “Faster methods for contract-
ing inﬁnite two-dimensional tensor networks,” Phys. Rev. B
98, 235148 (2018).

[81] A related example is backward through an iterative solver for
the dominant eigenstates x∗ of a matrix A. In the forward pro-
cess, one may use the Krylov space method such as Lanczos or
Arnoldi iterations. While for the backward pass one can lever-
age the fact that the dominant eigenvector is the ﬁxed point of
the power iteration f (x, A) = Ax/||Ax||. Therefore, one can use
Eq. (7) to propagate x∗ back to A. Note that the forward and the
backward iteration functions are decoupled, thus the backward

11

function does not need to be the same as the forward pass, as
long as it ensures x∗ is a ﬁxed point.

[82] Jorge Nocedal and Stephen J. Wright, Numerical optimization,

2nd ed. (Springer, 2006).

[83] Barak A Pearlmutter, “Fast exact multiplication by the hes-

sian,” Neural computation 6, 147–160 (1994).

[84] https://github.com/pytorch/pytorch.
[85] https://github.com/HIPS/autograd.
[86] https://github.com/tensorﬂow/tensorﬂow.
[87] https://github.com/google/jax.
[88] https://github.com/FluxML/Zygote.jl.
[89] Lars Onsager, “Crystal statistics. I. A two-dimensional model
with an order-disorder transition,” Phys. Rev. 65, 117–149
(1944).

[90] Jing Chen, Hai-Jun Liao, Hai-Dong Xie, Xing-Jie Han, Rui-
Zhen Huang, Song Cheng, Zhong-Chao Wei, Zhi-Yuan Xie,
and Tao Xiang, “Phase Transition of the q-State Clock Model:
Duality and Tensor Renormalization,” Chinese Physics Letters
34, 050503 (2017).

[91] Wei Li, Jan von Delft, and Tao Xiang, “Eﬃcient simulation of
inﬁnite tree tensor network states on the Bethe lattice,” Phys.
Rev. B 86, 195137 (2012).

[92] Laurens Vanderstraeten, Michaël Mariën, Frank Verstraete,
and Jutho Haegeman, “Excitations and the tangent space of
projected entangled-pair states,” Phys. Rev. B 92, 201111
(2015).

[93] Z. Y. Xie, H. J. Liao, R. Z. Huang, H. D. Xie, J. Chen, Z. Y.
Liu, and T. Xiang, “Optimized contraction scheme for tensor-
network states,” Phys. Rev. B 96, 045128 (2017).

[94] Anders W Sandvik, “Computational studies of quantum spin
systems,” in AIP Conf. Proc., Vol. 1297 (2010) pp. 135–338.
[95] Although it was expected that the full update method will in
principle reach the same accuracy as the variational method
within the error of Trotter-Suzuki decomposition, it would re-
quire optimizing the iPEPS tensors in a globally optimal way.
[96] Philippe Corboz, Steven R. White, Guifré Vidal, and Matthias
Troyer, “Stripes in the two-dimensional t-J model with inﬁ-
nite projected entangled-pair states,” Phys. Rev. B 84, 041108
(2011).

[97] H. J. Liao, Z. Y. Xie, J. Chen, Z. Y. Liu, H. D. Xie, R. Z.
and T. Xiang, “Gapless Spin-Liquid
Huang, B. Normand,
Ground State in the S=1 /2 Kagome Antiferromagnet,” Phys.
Rev. Lett. 118, 137202 (2017).

[98] Chih-Yuan Lee, B. Normand, and Ying-Jer Kao, “Gapless
spin liquid in the kagome Heisenberg antiferromagnet with
Dzyaloshinskii-Moriya interactions,” Phys. Rev. B 98, 224414
(2018), arXiv:1809.09128.

[99] R. Haghshenas, Shou-Shu Gong,

and D. N. Sheng, “An
iPEPS study of kagome Heisenberg model with chiral interac-
tion: A single-layer tensor-network algorithm,” arXiv (2018),
arXiv:1812.11436.

[100] Laurens Vanderstraeten, Bram Vanhecke,

and Frank Ver-
straete, “Residual entropies for three-dimensional frustrated
spin systems with tensor networks,” Phys. Rev. E 98, 042145
(2018), arXiv:1805.10598.
[101] Catalin Ionescu, Orestis Vantzos,

and Cristian Sminchis-
escu, “Training Deep Networks with Structured Layers by Ma-
trix Backpropagation,” in Proc. IEEE Int. Conf. Comput. Vis.
(2015) pp. 2965–2973, arXiv:1509.07838.

[102] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and
Dmitry P Vetrov, “Tensorizing neural networks,” in Advances
in neural information processing systems (2015) pp. 442–450.
and Joachim Giesen,
“Computing higher order derivatives of matrix and tensor ex-

[103] Soeren Laue, Matthias Mitterreiter,

pressions,” in Advances in Neural Information Processing Sys-
tems 31 (2018) pp. 2750–2759.

12

