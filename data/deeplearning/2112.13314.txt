1
2
0
2
c
e
D
6
2

]
E
S
.
s
c
[

1
v
4
1
3
3
1
.
2
1
1
2
:
v
i
X
r
a

1

Silent Bugs in Deep Learning Frameworks: An
Empirical Study of Keras and TensorFlow

Florian Tambon, Amin Nikanjam, Le An, Foutse Khomh, Giuliano Antoniol

Abstract—Deep Learning (DL) frameworks are now widely used, simplifying the creation of complex models as well as their
integration to various applications even to non DL experts. However, like any other programs, they are prone to bugs. This paper deals
with the subcategory of bugs named silent bugs: they lead to wrong behavior but they do not cause system crashes or hangs, nor show
an error message to the user. Such bugs are even more dangerous in DL applications and frameworks due to the “black-box” and
stochastic nature of the systems (the end user can not understand how the model makes decisions). This paper presents the ﬁrst
empirical study of Keras and TensorFlow silent bugs, and their impact on users’ programs. We extracted closed issues related to Keras
from the TensorFlow GitHub repository. Out of the 1,168 issues that we gathered, 77 were reproducible silent bugs affecting users’
programs. We categorized the bugs based on the effects on the users’ programs and the components where the issues occurred,
using information from the issue reports. We then derived a threat level for each of the issues, based on the impact they had on the
users’ programs. To assess the relevance of identiﬁed categories and the impact scale, we conducted an online survey with 103 DL
developers. The participants generally agreed with the signiﬁcant impact of silent bugs in DL libraries and acknowledged our ﬁndings
(i.e., categories of silent bugs and the proposed impact scale). Finally, leveraging our analysis, we provide a set of guidelines to
facilitate safeguarding against such bugs in DL frameworks.

Index Terms—Deep learning, Bug analysis, Empirical study, Keras, TensorFlow.

(cid:70)

1 INTRODUCTION

Deep Learning (DL) frameworks/libraries such as Tensor-
Flow [1], [2], Keras [3], Pytorch [4], MLlib [5], and Jax [6]
are becoming more and more popular to develop sophisti-
cated and complex applications in various domains; from
face recognition, fraud detection in ﬁnancial companies,
diagnosis and treatment of diseases, and natural language
processing to autonomous vehicles. While users rely on
these libraries and develop their DL applications on top of
them, they are subject to bugs like any other software. Bugs
in DL libraries are important because a bug in a library
can lead to bugs in any application that leverages that
library [7]. Many of these bugs never make into released
versions, others manifest themselves in easy-to-detect ways
(e.g., error messages, program crashes, or never ending
hangs) and some remain silent leading to wrong calculation
in DL models, training or inference without any obvious
symptoms for the user. We refer to silent bugs as bugs that
result in wrong computations/behavior but not in obvious
symptoms like crash, hang, or an error message.

We surmise silent bugs are especially insidious in DL
libraries and applications built on top of them. The primary
reason is that DL applications, as a Machine Learning (ML)
application, belong to the family of “non-testable program”
[8], for which, strictly speaking, oracles do not exist. More-
over, ﬁnding such bugs in DL libraries can be very hard
and tedious. A DL model developed by a user typically

• The authors are with Polytechnique Montr´eal, Montr´eal, QC H3C 3A7,
Canada. (Florian Tambon and Amin Nikanjam contributed equally to this
work.) (Corresponding authors: Florian Tambon and Amin Nikanjam)
E-mail: {ﬂorian-2.tambon, amin.nikanjam,
liano.antoniol}@polymtl.ca

foutse.khomh, giu-

le.an,

consists of many ﬂoating point parameters and operations
(up to billions). Even with a few bugs, the model may still
be created, trained, and appeared to work. A slight decrease
in running time or prediction accuracy is possible but it is
difﬁcult for a user to link a suspicious result/behavior to a
concrete bug in DL libraries (e.g., wrong update of weights
in neural networks during the training). While there are
various explanations for an unexpected behaviour that need
to be ruled out, the worst is the fact that the faulty computa-
tion is not explicitly developed by the user, but implicitly
produced by the library. However, little information can
be obtained from the DL model’s internal mechanism and
training/inference due to the “black-box” and stochastic
nature of DL training/inference. Hence, any bugs in DL
libraries may lead to bugs in the user’s program that the
user might not, easily, become aware of.

As a motivating example, Figure 1 illustrates a bug
reported on TensorFlow repository over GitHub. When sav-
ing a pre-trained model and loading it again, the accuracy
drops to its default value. In the user provided example,
the model and data are very simple and only used to
demonstrate the impact of the reported bug. After training,
the model showed 100% accuracy and the loss dropped
to zero. Then, the developer saved the model to the disk
and immediately loaded it back, and tested the accuracy
again, which dropped to 50%. As the expected behavior, the
accuracy should remain the same after loading a previously
trained and saved model. The bug was acknowledged by
reproducing the issue and providing a gist over Google
Colab (gist is deﬁned as “a simple way to share snippets
and pastes with others” [9]). This bug is related to weights
modiﬁcation when reloading the model, which has been
ﬁxed later and the issue closed. We have classiﬁed this bug

 
 
 
 
 
 
import tensorflow as tf

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units=2,

activation=’softmax’, name=’output’))

model.compile(optimizer=tf.keras.optimizers.Adam(lr=10),

loss=’sparse_categorical_crossentropy’,
metrics=[’accuracy’])

dummy_data_x = [[0, 0], [1, 0], [0, 1], [1, 1]]
dummy_data_y = [0, 1, 0, 1]
print(model.evaluate(x=dummy_data_x, y=dummy_data_y))
model.fit(x=dummy_data_x, y=dummy_data_y, epochs=10)
print(model.evaluate(x=dummy_data_x, y=dummy_data_y))
model.save(’test_model’)
model = tf.keras.models.load_model(’test_model’)
print(model.evaluate(x=dummy_data_x, y=dummy_data_y))

Output:
Before training:
1/1 [===============] - 0s 0s/step - loss: 0.9013 -

accuracy: 0.5000
[0.9013183116912842, 0.5]
After training:
1/1 [===============] - 0s 0s/step - loss: 0.0000e+00 -

accuracy: 1.0000

[0.0, 1.0]
After loading:
1/1 [===============] - 0s 1000us/step - loss: 0.0000e+00 -

accuracy: 0.5000

[0.0, 0.5]

Fig. 1: A reported issue over GitHub [10]. The bug is silent
and has a signiﬁcant impact on the accuracy of the learned
model, classiﬁed as ”Wrong save/reload” in our study.

as ”Wrong save/reload” in this paper.

There are several empirical studies on bugs in DL
software developed using various libraries [11], [12], [13].
Researchers also investigated symptoms, root causes, and
repair patterns of bugs inside TensorFlow [7], [14] and their
fault triggering conditions [15]. However, we are not aware
of existing studies on silent bugs inside DL frameworks, bugs
that do not result in obvious symptoms or misbehaviours.
Therefore, we formulate our research questions as follows:

RQ1 What are the impacts of silent bugs inside DL libraries
on DL programs developed by users? What are the
common consequences of silent bugs?
From which part of the framework’s API do the silent
bugs come from?

RQ2

RQ3 To what extent are the considered silent bugs relevant

for DL developers?

In this paper, we present the ﬁrst empirical study on
characteristics of silent bugs in a widely used API/frame-
work, Keras/TensorFlow, and their impact on user’s pro-
gram to answer our research questions. Fig. 2 illustrates the
schematic diagram of our study in this paper.

With more than 111,000 GitHub repositories using Ten-
sorFlow, currently it is one of the most popular ML frame-
work [16]. Keras is an API designed for DL, running on
top of TensorFlow. The choice of studying Keras is based
on several considerations: (1) Keras is widely used to im-
plement DL applications due to its simple and rich API,
(2) being a top level API using lower level components of
TensorFlow, the root cause of the errors should be easier to
identify, which simpliﬁes the screening for the root cause if
the errors are located in lower-level and (3) there is sufﬁcient
information with documented and frequent updates, to
ensure identifying silent bugs and their existing ﬁxes.

We hence mined TensorFlow GitHub repository [17] for
silent bugs issues related to Keras. We manually analyzed

2

Fig. 2: The main steps of our study.

1,168 issues, gathering and documenting 77 reproducible
silent bugs. Based on the extracted issues, we have identiﬁed
7 categories of bugs and 4 levels of bugs impact. Moreover,
14 components of TensorFlow and Keras were identiﬁed
as responsible for bugs and associated impact. We then
conducted an online survey with 103 DL developers to as-
sess the perceived relevance and completeness of identiﬁed
categories and impact levels. Finally, using our ﬁndings we
propose a set of guidelines for detecting and triaging silent
bugs. This paper makes the following contributions:

• We conduct the ﬁrst systematic study on silent bugs
in DL libraries through a manual analysis of 77 bug
reports out of 1168 closed issues from the repository
of Keras/TensorFlow on GitHub,

• We provide a classiﬁcation of bugs based on ob-
served impact on the users’ programs that create,
train, or infer DL models,

• We identify the components of Keras/TensorFlow

where silent bugs occur,

• We evaluate the relevance of our ﬁndings by a survey

among DL developers,

• We discuss and suggest a set of guidelines to safe-
guard against such bugs including testing and detec-
tion approaches,

• We make the dataset used in this study publicly
available online [18] for other researchers/practition-
ers to replicate or build upon our work.

2 BACKGROUND
In this section, we brieﬂy introduce TensorFlow, Keras and
how their reported bugs are addressed by developers.

2.1 TensorFlow and Keras

TensorFlow is an end-to-end, open-source ML platform
[2]. Dataﬂow graphs are used to deﬁne the computations,
operations and states of ML algorithms. While each node
in a graph represents an individual mathematical operator,
each edge represents data dependencies among nodes us-
ing tensors (n-dimensional arrays) that deﬁne the format

Extractissues1168 issuesIndividual review0 or 1 vote(1073)2 votes(38)3 votes(57)RejectAcceptDiscussion77 issues acceptedIndividualanalysisGroup consultationCategoriesand impact levelOnline survey103 RespondentsFindingsof data transferred between two nodes. TensorFlow is an
infrastructure layer for ﬂexible differentiable programming
that offers gradient computation of arbitrary differentiable
expressions and efﬁcient low-level tensor operations on
CPU, GPU, or TPU. It is also scalable to various devices.
Regarding deployment, it is possible to export graphs to
different external runtime environments such as servers,
browsers, mobile and embedded devices.

Keras is a set of DL API developed in Python that runs
on top of the TensorFlow ML framework [3]. TensorFlow
uses Keras as its high-level API, which provides essen-
tial abstractions and building blocks for developing and
shipping DL solutions. It was designed and implemented
to enable fast experimentation as a key factor in scientiﬁc
research. Keras is featured as: 1) Simple: developing with
Keras does not need high skills/knowledge in DL/ML, so
the developer can focus on the research problem, 2) Flexible:
the progressive disclosure of complexity is adopted in Keras,
so it starts with simple workﬂows that are quick and easy,
while arbitrarily advanced workﬂows are available on the
top of the already build simple workﬂow, and 3) Powerful:
industry-strength performance and scalability are offered in
Keras, as it has been used by NASA, YouTube, or Waymo.
It should be noted that Keras is originally designed to
support multiple backends, including TensorFlow, Theano,
and PlaidML, however as of version 2.4, only TensorFlow is
supported [19].

2.2 Repairing bugs in TensorFlow/Keras

The source code of TensorFlow is maintained on GitHub
[17]. All issues and commits have been reported in this
repository respectively since November 2015. Similar to
other repositories, users will submit an issue when they
face a problem while using the library (e.g., a bug). Such
issues contain information for investigating and diagnosing
the problem including a brief description of the issue, the OS
platform, buggy Keras/TensorFlow version, and the code
snippets to reproduce the buggy behavior. The developers
of TensorFlow (or any other interested users) investigate
the issue and potential ways to address it. Not all reported
issues are bugs and further investigation indicates the root
cause of each issue. Usually the developers ﬁrstly attempt
to reproduce the issue and acknowledge the mentioned
unexpected behaviour. Typically developers and interested
users have useful discussions on the issue to understand
and then ﬁx the potential bugs. If the issue is identiﬁed as
a real bug, it will be tagged as bug and developers start
working to address it. Finally, developers will report or
mention the link to the ﬁxed version (or similar issues) on
the issue asking others to test and acknowledge it. The issue
should be closed then. While other issue tracking systems
like Jira have more advanced issue trackers labeling issues
as “resolved” or “ﬁxed”, the issue tracker of GitHub is
simpler. Therefore, careful inspection of issues and related
discussion is necessary to understand them deeply.

3 SILENT BUGS IN KERAS AND TENSORFLOW

In this section, ﬁrst we describe our methodology to in-
vestigate bugs inside Keras/TensorFlow answering RQ1

and RQ2. The general process consists of the three steps:
(1) Data collection, (2) Manual analysis, (3) Labeling and
classiﬁcation. Then, we discuss our ﬁndings.

3

3.1 Methodology

3.1.1 Data Collection

We collect issues from the issue tracker of TensorFlow
repository over GitHub [17]. In fact, various types of issues
can be found in such repositories including bug reports,
users’ questions (e.g., misunderstanding documentations).
So, it is crucial to identify relevant issues for our study.
Since in this study we aim at investigating characteristics
and consequences of silent bugs, we collected closed issues
labeled as “bug”. The reasons of this decision are: 1) Bugs
involved in such issues have been acknowledged and then
ﬁxed by the framework developers, and 2) Such issues
usually include useful information about the bug that helps
to understand the bug (e.g., discussions among users/de-
velopers, code/version changes, links to related issues and
potential ﬁxes).

We used the GitHub search API [20] to extract relevant
issues. To exclude the search, we have deﬁned the following
search criteria accordingly:

1) The type of issue must be labeled as “bug” since we

are looking for bugs reported by users.

2) The related component of the issue must be labelled
as “keras” since we focus on the Keras API related
bugs.

3) We extracted only the “closed” issues. This decision
aimed to ensure that we would analyze issues that
were solved meaning that a ﬁx exists if there the
issue relates to a real bug.

So, we ﬁltered issues according to the following query

and extracted 1,168 issues this way:

is:issue is:closed label:"type:bug" label:"comp:keras"

3.1.2 Manual inspection
In this paper, our goal is to investigate silent bugs, so in
our inspection we ﬁrst looked for real bugs which are
reproducible. Usually developers who are working on bugs,
attempt to reproduce the bug prior to further investigation,
e.g., using a Google Colab and provide the link on the
issue. We relied on such gist during our inspection. We then
ensure that the bug remains silent during compilation and
execution, i.e., not leading to an error message or crash.
Moreover, ﬁxed bugs were selected during our inspection
process since we were looking for closed bug reports. A hint
to the ﬁxed version or users’ acknowledgment of having the
bug ﬁxed were considered as evidence. Each artifact was
inspected by reviewing different parts, i.e., the raised issue,
code snippets, discussions mentioned by the owner/other
users/developers, suggestions, and recommended ﬁxes. To
summarize, we have used the following inclusion criteria to
evaluate issues:

•

•

Is it an actual bug? i.e., not a mistake or misunder-
standing made by the user,
Is the bug reproducible? e.g., by providing a gist
(usually a Google Colab),

•

•

Is it silent? We regarded as silent any bug that did
not stop the program, for example not causing the
program to crash, hang (with or without an error
message) or fail. Bugs that lead to memory leakage
are not considered as silent as well,
Is the bug ﬁxed eventually? The evidence can be
mentioning the ﬁxed version, ﬁx commit or an ac-
knowledgment from the user who raised the issue.

We excluded all non-included bugs. A shared document
including the link to all artifacts have been used to make
it possible for all authors to work together during the
inspection process.

To select relevant bug reports, all issues were initially
inspected by three reviewers independently, the ﬁrst three
authors, who have more than 3 years of experience in using
TensorFlow/Keras. Each reviewer selected issues according
to the mentioned inclusion criteria in a separate sheet on our
shared document. Afterward, the three reviewers pooled
together their results, yielding a score between 0 and 3 for
each issue (3 meaning all agreed it satisﬁes our inclusion
criteria and 0 none of them). 1032 bugs received a score of
0, 41 received a score of 1, 38 received a score of 2 and 57
received a score of 3 in this step. Issues with a score of 3 (i.e.,
3 positive votes from reviewers) were directly selected for
the next round. Issues with a score of 0 were discarded. We
reexamined all 1-vote bugs making sure that they are not
overlooked, however none of them were accepted after re-
examination. Issues with a score of 2 were further discussed
by all reviewers in a meeting to discuss whether to select
it or not and resolve the conﬂict. 26 were selected after
further discussions and re-inspecting the candidates. Out
of 83 remaining issues, 6 bugs were discarded at the end: 3
turned out to be duplicate (same points but different issue
titles), 2 turned out not to be bugs but users’ mistakes, and
1 had got closed after being open for a long time without
a consensus on resolving it. At the end, we obtained a total
of 77 bugs. We computed Cohen’s kappa coefﬁcient [21] to
assess inter-rater agreement on whether each bug is silent
or not. The coefﬁcient was measured as 0.705 indicating a
substantial level of agreement.

All remaining issues were screened by reviewers in order
to collect the following information: version of TensorFlow
employed by the user posting the issue (buggy version), link
to a gist to reproduce the issue, versions for which the bug
is no longer observed (ﬁxed version) and a commit ﬁxing
the issue if possible.

3.1.3 Labeling and Classiﬁcation

We manually analyzed all issues following an open coding
procedure [22] similar to previous studies [12], [13]. We
aimed to inductively create the categories in a bottom-up
way by analyzing the bugs.

To analyze problems caused by silent bugs and character-
ize them, we focus on their potential impact on the user’s
program and the component of the Keras/TensorFlow re-
sponsible for the bug. Therefore, we investigated each of 77
bug reports from two aspects:

1) What is the effect of the bug? That is, what did it
cause on the program (e.g., accuracy loss, wrong UI
output, ...).

4

2) What are the Keras/TensorFlow components in

which silent bugs occur?

Regarding the impact of the bug, to propose a classiﬁca-
tion label, we used the descriptions of bugs in the issues as
well as gist and/or the example provided to showcase the
problem. We call deﬁned labels impact scenarios. In the case
of the library’s components, we identiﬁed modules of the
Keras API [23] which were involved in the error. Note that
some modules affected are hidden in the API the user has
access to, but are present in the repository of TensorFlow
(e.g., “engine” module) [24]. We considered mainly the API
documentation, yet we will also make use of the TensorFlow
repository. The buggy component was deduced by going
through the bug-ﬁx commit and looking at which ﬁles were
affected by a change. If no commit reference was available,
the reviewer inferred the potential buggy component from
the description and discussion in the issue report.

We performed the labeling process in three rounds. For
the ﬁrst round, to reduce the subjective bias during the
labeling process each reviewer was tasked to independently
indicate the effect of bugs (i.e., impact scenario) by their
own deﬁned descriptive labels. Faulty components were
identiﬁed independently in this round as well.

Moreover, during the ﬁrst round, we tried to analyze the
impact of the bugs in terms of degradation of the ML model
ability to work properly. Given what we observed through
the gathered issues, we came up with a scale to measure the
threat level of an issue based on its consequences. This scale
was elaborated empirically based on the problems reported
by the users and their consequences on the model during
discussions between the reviewers in the ﬁrst round. We
then used it as a guideline in the next rounds of labeling
and each issue was assigned an impact level. The scale can
be found in Table 1. It is divided into four impact levels (1
being the lowest and 4 the highest), each of which needs one
additional criteria than the previous level. Note that only
the “X” criteria are needed to be considered of a given level
while “?” criteria are optional. The deﬁned impact levels in
the reply to the question of “What is the impact caused by the
bug?” are as follows:

• UI element: The issue impacts the user interface ele-
ment, either in a purely cosmetic way or with little to
no impact on the information the user receives.
• Model’s information: The issue impacts information
the user receives from inside the model, e.g., shape of
a model or reported loss value, but does not actually
modify the way the model works or its results.
• Model’s operation: The issue impacts the way the
model works and its process, e.g., hyperparameters,
layer composition, or model processing mechanism
which can lead to some time/speed degradation but
only minor to no changes on the model outputs.
• Model’s result: The issue impacts the way the model
behaves which leads to completely different results
compared to known results or the ﬁxed version.

For example, a bug must impact information of a model to
be considered at least at level 2, higher than bugs that alter
UI elements.

In the second round, we put individual labeling re-
sults together. One of the reviewers checked the results

TABLE 1: Criteria for evaluating impact of bugs. “X” means
that to be considered of this impact level, a bug needs to
meet those criteria. “?” denotes optional criteria.

Impact
levels
Level 1
Level 2
Level 3
Level 4

UI
element
X
?
?
?

Model’s

Model’s
Information Operation

Model’s
Results

X
?
?

X
?

X

and indicated conﬂicts. Then two other reviewers went
through conﬂicting issues, commented on the labeling/im-
pact level/faulty component, and proposed a suggestion
where applicable. Finally, in the third round the reviewers
went through all artifacts to compare their labeling results,
discuss any conﬂict, and ﬁnalize the categories in a team
meeting.

For impact levels, inter-rater agreement was calculated
as linear weighted Cohen’s Kappa [21]. The weights are
deﬁned as the difference between assigned impact levels
where they are different. The rationale being that the threat
is increased as the impact level gets higher values. So, we
consider that raters assessing an issue of level 1 as level 2 is
not as bad as rating an issue of level 1 as level 3. The kappa
coefﬁcient obtained is 0.758 (p-value < 0.05) highlighting
an excellent agreement between raters. For impact scenarios
(categories of bugs), since no classiﬁcation existed for the
problem, we followed an open coding procedure to derive
categories in a bottom-up way by analyzing the bugs. This
allowed us to ﬁnd categories based on our observations
from the extracted issues similar to previous studies [12],
[13]. We therefore could not report inter-rater agreement
due to the lack of a priori deﬁned categories. However,
after resolving conﬂicts and ﬁnalizing labels, we noticed
that 31 bugs were classiﬁed the same as their ﬁnal labels
by all raters, 33 bugs by two raters, and 13 by one rater
respectively, showing initial agreement of 83% (64 out of
77).

Finally, 77 bugs were classiﬁed into 7 impact scenarios
and 14 components of the API were identiﬁed to be affected
by the bugs. Each bug was also assigned with one of the
impact levels deﬁned in Table 1. We kept all the initial labels
and comments on our shared document for any further
discussions.

3.2 Empirical Results

We provided a replication package of our used material and
collected data for reproducibility purposes [18].

3.2.1 RQ1: Impact of bugs

Based on the classiﬁcation and labeling process described,
the identiﬁed impact scenarios of silent bugs are presented
as follows. For each scenario, an example is associated in
order to illustrate the impact level as well as to detail what
consequences could unfold for the given impact scenario.

1. Wrong shape: This category includes any bug leading
to a wrong shape of a tensor in the model without raising
an error.

5

import tensorflow as tf
import numpy as np

class Example(tf.keras.layers.Layer):

def __init__(self, **kwargs):
kwargs["dynamic"] = True
super(Example, self).__init__(**kwargs)

def call(self, inputs):

return inputs

def compute_output_shape(self, input_shape):

return [(None, 2)]

inp = tf.keras.layers.Input(batch_shape=(None, 1))
comp = Example()(inp)

model = tf.keras.models.Model(inputs=[inp], outputs=[comp])
model.summary()

Output Shape

Model: "model"
___________________________________________________________
Layer (type)
===========================================================
input_1 (InputLayer) [(None, 1)]
___________________________________________________________
example (Example)
[(None, (2,))]
===========================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0

Param #

0

0

Fig. 3: An example of Wrong shape scenario.

Unexpected output shape on custom keras dynamic layer [25]:
The code snippet is shown in Figure 3 along with the output.
In this issue, Keras returns bad shape for the user’s custom
layer. As one can see, the output given by Keras differs
from the compute_output_shape function of the custom
Layer. We classiﬁed this error as Impact: 2 as, while it
returns bad information on the model (the shape), it does
not actually alter the way the model should work. If the gist
is trivial, it straightforward to envision how this bug could
be problematic in a realistic settings; in the case a program of
the user would lead to poor accuracy (for instance, because
of a bad parametrization), the shape would become the main
culprit for the bad result, which would possibly take some
time until user ﬁnds out that it does not impact its results.

2. Wrong displayed message: Any bug that shows an
information in UI (including console messages) which will
deceive the user or affect the user understanding of the ML
model belongs to this category.

ﬂoods

Simple

example

model.evaluate()

output
with=characters [26]: In Figure 4, an issue is displayed
where Keras displays a progress bar that is way too long
and which eventually masks part of the UI. The gist used
as well as the output UI are shown in the ﬁgure. Note that
the actual display was drastically shortened as it would
not ﬁt in the paper. The progress bar given by Keras is too
long between train and test phase. The error was linked to
a wrong variable being passed to the iteration routine used
in the training/evaluation loop of the API. We classiﬁed
this error as Impact: 1 as it just affects the UI of the user.
This kind of bug is generally harmless, as they do not
really modify anything. However, this type of bug can have
nasty impact when for instance logging results in a ﬁle; it
would literally ﬁll the log ﬁle with thousands of useless
characters. If the bug presented only generate a limited
number, a case where the number of returned characters
would be very high (or inﬁnite) could potentially result in

6

import tensorflow as tf

mnist = tf.keras.datasets.fashion_mnist
(training_images, training_labels), (test_images,

test_labels) = mnist.load_data()
training_images = training_images / 255.0
test_images = test_images / 255.0
model = tf.keras.models.Sequential([

tf.keras.layers.Flatten(),
tf.keras.layers.Dense(128, activation=tf.nn.relu),
tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer=’adam’,

loss=’sparse_categorical_crossentropy’,
metrics=[’accuracy’])

import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION)
import sys
print(sys.version_info)
tf_a = tf.Variable(1.0)
print(’Variable tf_a initialized to
{}.’.format(tf_a.numpy()))

tf_lr = tf.Variable(0.1, trainable=False)
tf_opt = tf.keras.optimizers.Adam(learning_rate=tf_lr)
@tf.function
def train_step():

with tf.GradientTape() as tf_tape:

tf_loss = tf_a**2

tf_gradients = tf_tape.gradient(tf_loss, [tf_a])
tf_opt.apply_gradients(zip(tf_gradients, [tf_a]))

model.fit(training_images, training_labels, epochs=5)

print(’After one step with learning rate {}...

test_loss = model.evaluate(test_images, test_labels)

Output: Train on 60000 samples
[...]
Epoch 5/5
60000/60000 [==============================] - 2s
38us/sample - loss: 0.2980 - accuracy: 0.8903

10000/1 [================================================
... Literally hundreds of thousands of ‘=‘ ...
===========================================] - 0s
26us/sample - loss: 0.2803 - accuracy: 0.8673

Fig. 4: A sample of Wrong displayed message scenario.

an out-of-memory issue.

3. Wrong save/reload: In this category, we classify all
bugs that change the model, its component or its functionali-
ties either during saving or re-loading. For example, missing
a layer after reload or inconsistent accuracy before and after
saving.

Accuracy is lost after save/load [10]: Figure 1 shows an
issue where reloading saved weights leads to a big drop
in accuracy. We classiﬁed this error as Impact: 4 as it com-
pletely changes the results of the model. If the user would
not be careful, simply using the weights without veriﬁca-
tion, the model would return mostly incorrect predictions.
Moreover, it is important to notice that the weights are not
necessarily the ﬁrst place where one would look to track in
error, especially in a deeper model, as no error/warnings
were thrown. This explains the threats of such errors and
highlight the importance of multiple checking procedure
when using such frameworks.

4. Wrong parameter setting: Any bug affecting the ex-
pected parameters setting of a function or model’s compo-
nents fall in this category.

Passing a Variable as learning rate to Adam optimizer does
not work as expected [27]: We refer to this issue as men-
tioned in Figure 5 where passing a variable to learning rate
and dynamically changing it will not be registered on the
learning rate. As one can see from the provided gist and
output, even though the learning rate variable got set to 0,
the variable of the trainable layer is still being modiﬁed. We
classiﬁed this error as Impact: 3 as it modiﬁed the way of
functioning of the network (the learning rate set). In this
gist, the learning rate setting is pretty drastic, in practice
the user would probably set something akin to a scheduler
and reduce the learning rate slowly. Yet, with this issue,
it could lead to potential convergence issues. In practice
a LearningRateScheduler should be deﬁne instead (following
good practice), so one could argue the mistake is on the
user end, but the fact that no warning is thrown, that no
documentation explicitly forbid this and that the mechanism
is acceptable by the model shows that is more of a potential

’.format(tf_lr.numpy()), end=’’)

train_step()
print(’Variable tf_a is {}.’.format(tf_a.numpy()))
tf_lr.assign(0.0)
for _ in range(10):

print(’After another step, now with learning rate {}...

’.format(tf_lr.numpy()), end=’’)

train_step()
print(’Variable tf_a is {}.’.format(tf_a.numpy()))

Output:
v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
sys.version_info(major=3, minor=5, micro=6,

releaselevel=’final’, serial=0)

Variable tf_a initialized to 1.0.
After one step with learning rate 0.10000000149011612...

Variable tf_a is 0.8999971747398376.

After another step, now with learning rate 0.0... Variable

tf_a is 0.8004083633422852.

After another step, now with learning rate 0.0... Variable

tf_a is 0.7015821933746338.

[...]
After another step, now with learning rate 0.0... Variable

tf_a is 0.07624538242816925.

After another step, now with learning rate 0.0... Variable

tf_a is 0.005127914249897003.

Fig. 5: Example of Wrong parameter setting scenario.

issue.
5. Performance degradation: Any bug affecting the per-
formance of ML experiments (training or inference) is cat-
egorized in this class, e.g., memory usage or running time
(speed). This category does not include changes in predic-
tion accuracy of the model.

TF2.0 - Multiple calls to Keras .ﬁt and .evaluate makes RAM
explode and is 25x slower [28]: In the issue illustrated in Figure
6, consecutive calls to either ﬁt() or evaluate() increases the
used main memory (RAM) even when calling with the same
data. The user complains that such calls take approximately
ten times longer than with TF1.x. According to the provided
gist, after 3,312 consecutive calls of evaluate() using TF2.0,
1508MB of memory is occupied. Using TF1.x, the memory
used is not increased after consecutive calls of evaluate()
(staying at 176MB) while the running time was 25 times
faster than TF2.0. We evaluated this issue as Impact: 3 since
the bug signiﬁcantly affects the operation of the model. Al-
though such consecutive calls is not common in developing
DL application and the bug does not affect the prediction
accuracy, the degraded performance may lead to signiﬁcant
problems during deployment, especially in settings where
the systems speciﬁcation are limited (e.g., on-board system)
or when decision speed is important (e.g., autonomous
driving system).

6. Wrong structure: The category covers all bugs that
modify the expected structure of a model, in particular how
it is handled by the framework.

Keras TimeDistributed on a Model creates duplicate layers,

7

inner_input = keras.layers.Input((2,))
dense = keras.layers.Dense(2,

activation=’relu’)(inner_input)

inner_model = keras.Model(inputs=inner_input,

outputs=dense)

full_input = keras.layers.Input((2,2))
td_2 =

keras.layers.TimeDistributed(inner_model)(full_input)

model = keras.models.Model(full_input, td_2)
model.compile(’SGD’, ’mse’)

model:

from memory_profiler import profile
from time import time
import numpy as np
import tensorflow as tf

model = tf.keras.Sequential([tf.keras.layers.Dense(100,

activation=tf.nn.softmax)])

model.compile(loss=’mse’, optimizer=’sgd’)
@profile
def eval(x, y):

model.evaluate(x, y)

x = np.random.normal(size=(1,100))
y = np.random.normal(size=(1,100))
for i in range(100000):

print(’iteration’, i)
tic = time()
eval(x, y)
print(’timeit’, time() - tic)

Output: iteration 3312
1/1 [==============================] - 0s 4ms/sample -

loss: 1.0205

Filename: reproduce_keras_oom.py

Line
Mem usage Increment Line Contents
================================================

1508.3 MiB 1508.3 MiB @profile

9
10
11 1508.7 MiB 0.4 MiB

def eval(x, y):

model.evaluate(x, y)

timeit 0.09004998207092285

Fig. 6: Example of Performance degradation scenario.

and is inconsistent with TimeDistributed on a Layer [29]: As
shown in Figure 7, the user attempts to wrap a model in a
TimeDistributed layer but this leads to creation of duplicate
nodes in the graph. Following the documentation of the
framework, the user ends up with an additional Dense
Layer (bottom left in Figure 7). This additional layer takes
redundant memory and is created because the user builds
the inner model then rebuilds it again during building the
TimeDistributed model. However, the expected behavior for
wrapping a model is having a similar graph as it is the case
when wrapping a layer. This is a typical example of wrong
structure of the model that is silent and the user may not
notice it until experiencing a bad accuracy or training issue.
We have evaluated this issue as Impact: 2 as it returns false
structure information to the user, which can typically lead
to confusion or potentially an error.
7. Wrong calculation: All bugs modifying the normal way
of computation that are not classiﬁed in other categories will
be classiﬁed in this type, like wrong calculation of gradients.
Keras fails to account for smaller last batch in loss metric
calculation [30]: In the issue displayed in Figure 8, the
evaluate() function computes the mean loss over all batches
in an epoch incorrectly when the dataset size is not evenly
divisible by the batch size. This happens for both training
and validation loss. Actually, the bug affects the reported
epoch loss, but not the training loss used for computing
gradient updates. In the provided gist, there are 3 samples
in the dataset, and the batch size is 2. So, there are 2 batches
of size 2 and 1. If the ﬁrst batch has mean loss of 10 and
the second batch has mean loss of 9, then the mean loss
over the entire dataset is incorrectly computed as (10 + 9)
/ 2 = 9.5. We evaluated this bug having Impact: 2 since
the model’s information is affected but not its functionality.
However, the correct mean loss over the dataset should be
a weighted mean of the batch losses, where the weights are
given by each batch size. Thus, the correct mean loss should

Fig. 7: Example of Wrong structure scenario.

import tensorflow as tf

X = tf.constant([[1], [2], [3]], dtype=tf.float32)
y = tf.constant([[5], [4], [6]], dtype=tf.float32)
model = tf.keras.Sequential([

tf.keras.layers.Dense(1, input_dim=1,
kernel_initializer=’ones’,
bias_initializer=’zeros’)])

model.compile(optimizer=’sgd’, loss=’mean_squared_error’)
def mse(y, y_pred):

assert len(y) == len(y_pred)
return sum((y - y_pred)**2)/len(y)

print(’model.evaluate():’)
print(’- batch_size=1:’, model.evaluate(X, y, batch_size=1,

verbose=0))

print(’- batch_size=2:’, model.evaluate(X, y, batch_size=2,

verbose=0))

print(’- batch_size=3:’, model.evaluate(X, y, batch_size=3,

verbose=0))

print()
print((mse(X[:-1], y[:-1]) + mse(X[-1], y[-1]))/2)

Output:
model.evaluate():
- batch_size=1: 9.666666984558105
- batch_size=2: 9.5
- batch_size=3: 9.666666984558105

tf.Tensor([9.5], shape=(1,), dtype=float32)

Fig. 8: Example of Wrong calculation scenario.

be (10*2 + 9*1) / (2 + 1) = 9.66.

From the extracted data, the distribution of the impact
scenarios can be found on Figure 9. “Wrong calculation” is
the most common type with 30, 95% of all studied bugs. As
the internal mechanism and output of DL models mainly
rely on their calculation, a ﬂaw in a step of the model’s
calculation may lead to wrong results in any of the next
steps. The two second most widespread types of bugs are
“Wrong parameter setting” (19, 05%) and “Wrong displayed

8

3.2.2 RQ2: Faulty components

To identify the buggy components of the library we went
through the bug-ﬁx commit and looked at which ﬁles were
affected by a change. If no commit reference was available,
the reviewer inferred the potential buggy component from
the description and discussion in the issue report. To have a
list of TensorFlow/Keras components, we consider mainly
the Keras API documentation and the repository of Tensor-
Flow [23]. The identiﬁed components are:

• Layer: Layers are the basic building blocks of neural
networks in Keras. A layer consists of a tensor-
in/tensor-out computation function, held in Tensor-
Flow variables which are the layer’s weights.

• Regularization: Regularizers apply penalties on
layer’s parameters or its activity during optimiza-
tion. These penalties are summed into the loss func-
tion that the network optimizes.

• Callbacks: they are objects that can perform par-
ticular actions at various stages of training (e.g., at
the start/end of an epoch or before/after a single
batch). One can use them to log TensorBoard after a
batch to monitor training metrics, save the model to
disk, early stopping, and gather statistics of a model
during training.

• Optimizer: An optimizer is one of the arguments
required for compiling a Keras model to optimize
the model’s parameters during training.

• Activations: Activations can either be used through
an ”activation” layer, or through the activation argu-
ment supported by layers. They are applied to the
output of tensors to produce the required output.
• Metrics: A metric is used to evaluate the perfor-
mance of a model. They are similar to loss functions,
with a difference that their results are not used when
training the model. Any loss function may be used
as a metric.

• Model: These functions are employed to build and
use a model including compilation, evaluating and
predicting.

• Estimator: Estimators aggregate the following func-
tions in a single object: training, evaluation, predic-
tion, export for serving.

• Loss: The purpose of loss functions is to compute the
difference between expected and actual output of a
model that should be minimized during training.
• TF: This encompasses all the functions that are not
Keras related but are used by TensorFlow. In general,
it is composes of basic operation on the tensors such
as Mean, Sum or Slicing.

• Saving: This module regroups everything related
to saving and loading weights/models to/from the
system.

• Engine: The engine module on which Keras frame-
work is based. It contains most of the basic classes
that are used in every other module, e.g., the “base
layer” class from which all layers are then derived.
This module is not meant to be accessible to the user,
but as it is the foundation of the whole framework,
many issues referenced this module.

Fig. 9: Distribution of the impact scenario in the selected
issues.

Fig. 10: Distribution of the impact level based on the sce-
nario affected.

message” (17, 86%). On the other hand, “Wrong structure”
has the least frequency with only 3 samples (3.57%).

We also show in Figure 10, the cross-check analysis of
the impact level given a scenario. The “Wrong calculation”
(e.g., back-propagation gradients being computed wrongly)
has the highest impact with 13 samples of Impact: 4. The
second one is “Wrong save/reload” (e.g., weights not being
properly set when reloading a model) with 6 high-impact
samples. In particular, “Wrong calculation” and “Wrong
save/reload” bugs represent issues that would drastically
affect the results of the model. Moreover in most cases, they
can be only detected through inspection of the model and
even that does not guarantee ﬁnding the root of the error. As
it is expected, “Wrong displayed message” (e.g., summary of
the model includes missed or incorrect information) is the
least inﬂuential bug with 13 low-impact bugs.

Tensorﬂow/Keras is one of the most used frameworks
by ML practitioners. While users employ the framework
“as it is”, without a proper perspective on what is hap-
pening “under the hood”, we provide evidence that such
silent issues can have dramatic impacts, without obvious
symptoms. Although frameworks’ developers may already
be aware of them, it is not the case for all DL frameworks
users. Moreover, we believe such bugs pose new challenges
to the research community deserving further investigation.
Although there exist different DL frameworks, we surmise
silent bugs are likely a non-Tensorﬂow/Keras-speciﬁc issue
due to the similarities of ML frameworks.

Performancedegradation4.76Wrongcalculation30.95Wrongdisplayedmessage17.86Wrongparametersetting19.05Wrongsave/reload16.67Wrongshape7.14Wrongstructure3.5702468101214161820WrongshapeWrongstructurePerformancedegradationWrongparametersettingWrongcalculationWrongsave/reloadWrongdisplayedmessage132122213854215136ImpactLevel12349

4.1 Methodology

We devised a survey form using Google Form [31], a well-
known online tool for creating and sharing surveys and
quizzes. The survey was organized in ﬁve parts and is
inspired by similar empirical studies on bugs using sur-
veys as a validation method [13], [32]. First we asked
demographic questions, i.e., current job title and years of
experience with Tensorﬂow/Keras. The second part was
interested in probing the knowledge of participants about
the notion of “silent bugs”. We ﬁrst asked them whether
or not they were familiar with “silent bugs”. We then gave
them our deﬁnition used in this paper and asked them for
the perceived relevance. In the third part, we wanted to
assess the meaningfulness of the scenario categorization we
did. As such, for every category, we provide the deﬁnition
of the category (with an example gist) and a multiple-choice
question asking the participant about i) How hard it is to
Diagnose, ii) What is their perceived Severity, and iii) How
hard it is to ﬁx according to them. The participant is in-
structed to provide a score on a 5-level Likert scale [33]. We
then asked them whether or not they were ever confronted
with such a type of bug. They could provide feedback if
wanted through comments. The main idea of this part is to
assess the difﬁculty of each category in order to assess if we
can match the impact classiﬁcation of Figure 10 (that is, if for
instance one category is perceived by developers as harder
to diagnosed or more severe and that we have a prevalence
of higher impact level for that category, according to our
scale) and see if such category of bug is rare or not. We then
asked developers for any other category they knew and that
we did not mention in our classiﬁcation.

The fourth part aimed to assess the deﬁned impact
levels. We gave for each level a description along with an
example snippet. We then asked them to judge the relevance
of the impact level (based on 5-level Likert scale [33]). We
ﬁnally gave them an impact scale (see replication package
for table, Table 1 being an updated version of it thanks to
respondent comments) and asked them to evaluate it. The
goal of this part was to assess developers’ opinion: is the
impact scale correct with regard to their experience as well
as to gauge their comprehension of the scale. Finally, in the
last part, we asked developers whether they thought “silent
bugs” as we presented it to them were more, equally or less
problematic than more traditional bugs. After all questions
asked, we provided the developers with the opportunity to
add any optional comment to nuance their views. We did
not ask any questions about the components affected by
bugs since it would need requesting participants to study
the issue, gist and bug-ﬁxing commit.

The target group of candidates for this survey is devel-
opers who use TensorFlow/Keras with some experience in
DL programming. The ﬁrst group of candidates came from
GitHub. To ﬁnd participants with a good understanding of
DL programming over GitHub, we used its REST APIs [34].
We identiﬁed repositories that used TensorFlow and Keras
(mentioned in their description or readme). We excluded
repositories that were not active since 2019. Finally, we
extracted active contributors’ emails (mentioned over their
proﬁle) from 12,192 selected repositories. This process left
us with 3582 unique email addresses. For the second group,

Fig. 11: Distribution of the impact level based on the com-
ponent affected.

• Wrapper: This encompasses function to facilitate us-
age of other library by Keras, mostly Scikit-Learn
functions.

• Backend: The backend module regroups mathemat-
ical or utility functions implemented in a way such
as Keras can use them more efﬁciently along side
its other components, as well as a lot of parameters
settings for precision calculation.

Figure 11 illustrates the distribution of buggy compo-
nents across the selected issues along with the cross-check
analysis of the impact level given a component affected.
The most impacted components of the API are “Engine” (19
samples), “Layer” (14 samples) and “Model” (13 samples).
Note that the “Engine” component is not part of the API
documentation as it is a part of Keras backbone and as such
should not be accessed directly by the user. Hence, one may
expect that this component becomes the most affected one,
as it is part of the core of the framework, which all other
components use. The two others are among the most used
components as they encompass function and classes that
are necessary to deﬁne the model layout and inner work.
Also note the category “TF” that represents any component
not related to Keras. We came across some issues where
users reported the problem being a Keras issue but further
investigation revealed the root cause was not related to
Keras itself, but to TensorFlow other modules. While they
are not Keras components, we show them alongside others
for consistency. The biggest portion of high-impact scenarios
reside in “Engine” component and “Wrapper” is the host of
only 1 low-impact issue. From a different point of view, all
issues in “Saving” are high-impact ones. Moreover, almost
half of issues in “Layer” and “Backend” have high impacts.

4 VALIDATION AND RELEVANCE ASSESSMENT

The goal of this section is to answer RQ3: To what extent
are the considered silent bugs relevant for DL developers? To
do so, we ran a survey to collect views of DL developers
about identiﬁed categories of silent bugs and the impact
scale, assessing their relevance and completeness. In the
following, ﬁrst the methodology followed to conduct the
survey is explained, then the results are presented.

02468101214161820LayerKerasEngineRegularizersKerasSavingCallbacksOptimizersActivationsWrapperMetricModelBackendEstimatorLoss321142141126311215217941214311ImpactLevel1234we referred to issues that we initially collected from the
issue tracker of TensorFlow repository over GitHub (1,168
initial bug reports). We extracted emails of users (from their
Github’s proﬁles) who contributed to the issues, i.e., posting
the issues or participating in discussions. We obtained 605
emails in this way. In overall, we successfully distributed
the survey participation request to 4187 email addresses.
The third group of candidates came from Reddit [35]. To
recruit participants, the questionnaire was posted on two
relevant Reddit channels: deeplearning and MachineLearning.
When sending/posting the questionnaire, we explained the
purpose, scope and the estimated participation duration (10
minutes) of the survey in a message. Moreover, we asserted
that the survey is kept anonymous, but the respondents
were able to provide their emails for further communication
and receiving a summary of the study. The survey spanned
two weeks: the original email was followed by a reminder
after one week. The survey form can be found in our
replication package along with the anonymous answers that
we received [18].

4.2 Validation Results

The survey was open for two weeks resulting in 103 an-
swers. Regarding our question on work experience with
Tensorﬂow/Keras, 9 had less than 1 year of experience, 51
had between 1 and 3 years, 36 had between 3 and 5 years
and 7 had more than 5 years. In the following, we discuss
the validation results and received comments in detail.

4.2.1 Notion of Silent Bugs

When asked, without providing any deﬁnitions, whether
participants were familiar with the notion of “silent bugs”, a
majority (56,3 %) replied yes. This shows that the term is ac-
knowledged by the community even though it might not be
widely used. Unanimity of the respondents (97,1%) found
our deﬁnition of “silent bugs” to be meaningful. Hence, the
way we deﬁned the term seems to be relevant. We note some
comments mentioning that a) warning messages might not
be really considered as silent or b) program hanging could
not be considered as silent. In the case of (a), while warning
messages return something to the user, we considered them
to be silent as they do not break program compilation.
Moreover, as they do not stop the program, there is no stack
trace to indicate the source of the warnings which might
lead the user not to consider them for the current problem (if
noticed). For (b), we chose to reject program hangs because
they are easily noticeable by the user (since the program
does not end).

4.2.2 Categorization of Scenario

Results of our survey for this part are presented in Table 2.
The ﬁrst thing we gathered from our survey is, for each
scenario, roughly half (or more) of the respondents did
not encounter them to their knowledge, with Wrong Structure
being the rarest (28%) and Performance Degradation being the
most common (55%). Indeed, one main problematic aspect
of silent bugs is, they do not necessarily reveal themselves
unless the impact is clearly identiﬁable or the developer is
actively looking for them. For instance, one respondent for
the Wrong parameter setting scenario (Figure 5) mentioned

10

that “This bug is quite hard to debug without DL knowledge.
Mostly, to ﬁnd them, I had to observe loss changes in different
models or in different combination of parameters”. Indeed, if one
just expect this to work, the bug might not be clear dur-
ing training, especially if training converges to acceptable
results as one other respondent pointed out: “I may have not
noticed it if the model converges anyway”.

In general, except for Wrong shape and Wrong displayed
message, there seems to be a consensus among respondents
that bugs belonging in our deﬁned categories are rather
hard/dangerous (3 or more where 1 indicates the easiest
case) in terms of diagnosing, severity and ﬁxing. Wrong
shape is perceived as easier to diagnose and ﬁx, as one can
see from the higher number of respondents who voted for
1 or 2 (both 58%). However, Severity is also rather high
(45% voted for 4 or 5). On the contrary, for Wrong displayed
message, diagnosis and severity are rather low with ﬁxing
difﬁculty being neutral. This can be expected, as those bugs
are generally easier to notice and/or ﬁx or might not even be
ﬁxed in the case of Wrong displayed message as they might not
be severe enough to be bothered with (“Won’t ﬁx it myself”).
If we take the average between those three attributes (Diag-
nosing, Fixing and Severity) for each category, we ﬁnd that
all category are above 3.5 (tendency to be difﬁcult), except
Wrong shape, Wrong displayed message and Wrong structure,
with Wrong save/reload being the highest. Those observations
are mostly consistent with our empirical results presented
on Figure 10. Indeed, the categories with the highest number
of highly impacting bugs (3 or 4 according to our scale) are
the same as those that were deemed the most impactful by
respondents when considering difﬁculty of diagnosing/ﬁx-
ing and severity. Performance degradation was also noted as
one highly impacting (i.e., hard to diagnose/ﬁx and severe),
which was noted in our ﬁgure with all issues of this category
being of level 2. However the small number of issues of
this category prevented us from making any conclusion
while emphasized by respondents. The difference we note
between received feedback and our results is that Wrong
Shape might not necessarily be as impactful, yet similarly
to Performance Degradation, we had too few issues in this
category to have a clear conclusion.

Missing categories suggested by the participants were,
after analysis, mostly related to one of the categories we
introduced or out of scope of our deﬁnition of silent bug
(see subsection 4.2.1). However, we note that some of our
proposed categories could be nuanced or deﬁned in more
ﬁne-grained terms as they might be too general in their
current state, as some developers pointed out. Indeed, bugs
affecting control dependency, ﬂoat approximation or other
Wrong Calculation might occult part of the impact of the
issue, like one developer commented: “when indexing an
embedding matrix with negative numbers, TF on GPU does not
return an error, but returns vectors made of 0s without notifying
the user”. So, having subcategories could be beneﬁcial by
revealing more information in different subcategories of
bugs. One possible category that was brought about but not
studied in this paper concerns documentation issues, that
is when a behavior is either false or not properly explained
in the framework/library documentation. While such issues
are not related to the code, they can still have an impact on
the user.

TABLE 2: Results of validating survey for the impact scenarios (1: very easy and 5: very hard). Respondents acknowledged
that our deﬁned categories, except for Wrong shape, Wrong displayed message, and Wrong structure, are hard/dangerous in
terms of diagnosing, severity and ﬁxing.

11

Responses

Scenarios

Wrong Shape
Wrong Displayed Message
Wrong Parameter Setting
Wrong Save/Reload
Performance Degradation
Wrong Structure
Wrong Calculation

Already
Encountered
51%
34%
48%
47%
55%
28%
45%

Diagnosing
1 — 2 — 3 — 4 — 5

Severity
1 — 2 — 3 — 4 — 5
28% - 30% - 22% - 12% - 8% 8% - 17% - 30% - 28% - 17% 32% - 26% - 24% - 12% - 6%
27% - 19% - 17% - 20% - 16% 23% - 31% - 25% - 14% - 7% 15% - 19% - 27% - 22% - 17%
4% - 4% - 23% - 33% - 36% 10% - 17% - 31% - 22% - 20%
4% - 17% - 23% - 29% - 27%
9% - 12% - 22% - 22% - 35%
14% - 17% - 15% - 25% - 29% 3% - 5% - 17% - 21% - 54%
11% - 13% - 21% - 24% - 31% 5% - 8% - 29% - 33% - 25%
5% - 11% - 20% - 26% - 38%
7% - 16% - 36% - 25% - 17% 2% - 13% - 32% - 27% - 26% 6% - 20% - 34% - 28% - 12%
2% - 13% - 30% - 32% - 23% 5% - 12% - 37% - 25% - 21%
5% - 9% - 32% - 31% - 23%

Fixing
1 — 2 — 3 — 4 — 5

TABLE 3: Results of validation for the impact scale. Re-
spondents have a general agreement on the relevance of all
impact levels.

Impact
level
Level 1
Level 2
Level 3
Level 4

Strongly Disagree
disagree
6%
7%
5%
4%

10%
18%
14%
3%

Responses

Borderline Agree

18%
24%
23%
11%

40%
36%
29%
22%

Strongly
agree
27%
16%
29%
60%

4.2.3 Categorization of Impact

Results of our survey for this part are presented in Table 3.
As one can see a majority of the respondents agree (4 or 5)
that all levels are relevant given their deﬁnitions. A majority
also agree (75,8 %) that our scale is relevant. We note that
some respondents pointed out that some levels could be
fused, in particular 3 and 4, for instance because as pointed
out by one comment: “Bad learning rates can cause more than
just slight alterations in outputs as per my understanding. It can
cause instability as well. Hyperparameters like beta 1 , lr imo
should be level 4.”. In our results, we divided these two levels
as it is possible that affecting part of the model structure
(even hyperparameter) can have some impact on the model
inference, convergence or training but without high impact
on the results itself. As such, each bug should be inspected
individually to assess the impact level accurately. However,
we agree that the difference between the two categories can
be perceived as small.

4.2.4 Comparison with traditional bugs

We then asked respondents, based on all the questions of the
survey and their experience, how problematic they consider
silent bugs compared to traditional ones. By traditional bugs
we mean the ones returning errors or leading to programs
hanging on. A majority (72,8 %) answered that they found
silent bugs to be more problematic.

5 POTENTIAL TRIAGE OF SILENT BUGS

Finding the actual root cause of a silent bug is not straight-
forward. As a matter of fact, we are not aware of effective
procedures to counter such silent bugs, which are partic-
ularly insidious to detect as they do not raise any error,
but can lead to highly impacting consequences. Moreover,

we have seen such issues can have very different causes
compared to the consequences the user faces, which means
diagnosing them is pretty complicated. DL developers have
also conﬁrmed this fact in our survey. Our empirical study
can pave the way for other researchers to propose effective
detection approaches for bugs in DL frameworks. Based the
analysis we did, we highlight some recommendations that
can help in tracking and/or preventing such silent bugs
from impacting the user’s program:

• Reveal as much information as possible: Even though
getting information from models can be tricky be-
cause of their “black-box” nature, any returned in-
formation can help diagnosing a potential silent bug.
For instance, Figure 7 showed a wrong structure that
could be identiﬁed through a representation of the
computation graph. Hence, we advise users to check
the structure and components of models as well as
their functionality (e.g., prediction accuracy of infer-
ence). Moreover, assertion testing about shapes, type
or any easily inferable properties by the users should
be added as an extra precaution to root out potential
silent bugs. Actual unit testing or benchmarking com-
parison might not be easily doable because of the
“non-testable” nature of ML models, but as much
checking as possible should be realized based on
known or accessible information.

•

• Do not blindly trust the framework: As we have shown
in this study, DL frameworks are not infallible, so
users should not blindly trust them. Hence, users
should always be careful and critical of the results
and/or information they get out of their models,
possibly comparing them to their own experience,
a baseline or similar studies.
Following a proper development cycle: MLOps process
can help mitigate the effect of silent bugs, through
a continuous assessment of the system followed by
clear steps. For instance, many silent bugs concern-
ing badly saved weights can be detected if the user
tests the saved work by immediately reloading the
saved model,
instead of trusting everything was
done accordingly. Reloading the model later can lead
the user to believe something else went wrong.
• Redundancy is a must: one of the most effective ways
to root out silent bugs lay in comparing execution of
the code to a known benchmark or results. However,
it might not always be feasible because of the “non-

testable” nature of ML systems that can be circum-
vented through the use of “pseudo-oracle” [36]:

– Regression Testing [37]: using multiple versions
of the framework during the test as some fail-
ures can be introduced with the latest version.
A cross analysis brings a higher chance of
catching faulty behavior early.

– N-versioning [38]: Running multiple semanti-
cally equivalent programs can help ﬁnding the
cause of the issue. Moreover, since multiple
frameworks exist, it is also possible to cross
analyze behaviors of different models under
different frameworks to detect failure from one
of them.

Those approaches are particularly useful to detect
performance/computation bugs which are hard to
diagnose when using only one version.

6 RELATED WORK
Silent Bugs in Software Engineering. Though they might
not be deﬁned by the same name, silent bugs were inves-
tigated in software engineering. For instance, researchers
empirically studied bugs in test code [39]. They noted
the presence and prevalence of silent horrors, that is test
code causing a test to miss a bug in the production code.
Kouwe et al. studied fault-injection to track down silent
failures and noted through their experiments [40] that silent
failures are very common. They assessed the impact of silent
failures and reported that careful considerations are needed
to prevent them from undermining the quality of fault
injection results. Moreover, they noted that silent failures
were scarcely researched.
Empirical studies on bugs in DL. Zhang et al. [11] pub-
lished the ﬁrst empirical study on real-world reproducible
bugs occurring in TensorFlow-based software systems in-
cluding their high-level root causes and symptoms. Then,
Islam et al. [12] extended the investigated cases to include
DL software systems written using other competitive DL
libraries such as Pytorch and Caffe, and studied the relation-
ship and the evolution of different types of DL bugs. Last,
Humbatova et al. [13] proposed a taxonomy of real faults
that occur in DL software systems. They have not studied
root causes, symptoms and reproducibility of bugs while
they proposed a comprehensive taxonomy of faults in DL
programs. However, the focus of all these empirical studies
are on bugs in DL programs developed by DL libraries not
the bugs inside DL libraries. Recently, Jia et al. [7], [14]
investigated symptoms, causes, and repair patterns of bugs
inside TensorFlow as a typical DL library. While their results
are interesting, they have not explored the impact of bugs
on the DL programs and experience of DL users as we do in
this paper. Moreover, non-silent bugs have been considered
with obvious symptoms like crash or build failure but we
study silent bugs inside TensorFlow in this paper which
are more difﬁcult to investigate. Moreover, faults triggers
in TensorFlow have been investigated recently [15]. Faults
triggers are deﬁned as the set of conditions that activated a
fault leading to a failure. Authors aimed to answer research
questions about bug type distribution, ﬁxing time, root
causes, and regression bugs, by examining bug reports.

12

7 CONCLUSION

In this paper, we conducted the ﬁrst empirical study to
understand characteristics of such silent bugs inside DL
libraries that may result in the design of effective debug-
ging techniques for detecting them. We have showcased
77 reproducible silent bugs in TensorFlow and Keras by
inspecting 1,168 issues from their GitHub repository. The
studied bugs were classiﬁed into 7 scenarios and 4 impact
levels according to the way they can impact DL experiments
(designing DL models, training, or inference). We have also
identiﬁed the framework components that are responsible
for the bugs. We have then conducted an online survey with
103 TensorFlow/Keras developers to validate our results.
The participants generally agreed with the signiﬁcant im-
pact of silent bugs in DL libraries and acknowledged our
ﬁndings (i.e., categories of silent bugs and the proposed
impact scale). Finally, we suggested a set of guidelines to
help design effective detection tools for silent bugs.

Although there exist different DL frameworks, as they
tackle the same subject the way of working is not that
different. Thus, our results can be used as the basis to study
silent bugs in other frameworks. A comparison among
the bug types among different frameworks would also be
interesting. As another direction for future work, we plan
to design detection techniques to help developers identify
abnormal behaviors that may be caused by silent bugs and
track their root cause in DL libraries. Moreover, dynamic
analysis can be employed to study the runtime behaviors of
silent bugs.

ACKNOWLEDGMENTS

The authors would like to thank all the participants of
the survey which greatly contributed to improving this
work with their answers. This work is partly funded by
the Natural Sciences and Engineering Research Council of
Canada (NSERC) and the Fonds de Recherche du Qu´ebec
(FRQ).

REFERENCES

“Tensorﬂow,” https://www.tensorﬂow.org/, 2021.

[1]
[2] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for
large-scale machine learning,” in 12th {USENIX} symposium on
operating systems design and implementation ({OSDI} 16), 2016, pp.
265–283.
“Keras,” https://keras.io/, 2021.
“Pytorch,” https://pytorch.org/, 2021.
“MLlib-Spark,” https://spark.apache.org/mllib/, 2021.
“JAX,” https://jax.readthedocs.io/en/latest/, 2021.

[3]
[4]
[5]
[6]
[7] L. Jia, H. Zhong, X. Wang, L. Huang, and X. Lu, “The symptoms,
causes, and repairs of bugs inside a deep learning library,” Journal
of Systems and Software, vol. 177, p. 110935, 2021.

[8] E. J. Weyuker, “On Testing Non-Testable Programs,” The Computer

Journal, vol. 25, no. 4, pp. 465–470, 1982.

[9] W. Wang, G. Poo-Caama ˜no, E. Wilde, and D. M. German, “What is
the gist? understanding the use of public gists on github,” in 2015
IEEE/ACM 12th Working Conference on Mining Software Repositories.
IEEE, 2015, pp. 314–323.

[10] “TensorFlow/Keras

Issue,”

https://github.com/tensorﬂow/

tensorﬂow/issues/42459, 2021.

[11] Y. Zhang, Y. Chen, S.-C. Cheung, Y. Xiong, and L. Zhang, “An
empirical study on tensorﬂow program bugs,” in Proceedings of the
27th ACM SIGSOFT International Symposium on Software Testing and
Analysis, 2018, pp. 129–140.

[12] M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, “A comprehensive
study on deep learning bug characteristics,” in Proceedings of the
2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering,
2019, pp. 510–520.

[13] N. Humbatova, G. Jahangirova, G. Bavota, V. Riccio, A. Stocco, and
P. Tonella, “Taxonomy of real faults in deep learning systems,”
in Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering, 2020, pp. 1110–1121.

[14] L. Jia, H. Zhong, X. Wang, L. Huang, and X. Lu, “An empirical
study on bugs inside tensorﬂow,” in International Conference on
Database Systems for Advanced Applications.
Springer, 2020, pp.
604–620.

[15] X. Du, G. Xiao, and Y. Sui, “Fault triggers in the tensorﬂow
framework: An experience report,” in 2020 IEEE 31st International
Symposium on Software Reliability Engineering (ISSRE).
IEEE, 2020,
pp. 1–12.

[16] “DL Frameworks in 2021,” https://towardsdatascience.com/top-

5-deep-learning-frameworks-to-watch-in-2021-and-why-
tensorﬂow-98d8d6667351, 2021.

[17] “TensorFlow

repository,”

https://github.com/tensorﬂow/

tensorﬂow, 2021.

[18] “Replication

Package,”

https://github.com/amin-nikanjam/

SilentBugsInTensorFlowKeras, 2021.

[19] “Keras

releases,”

https://github.com/keras-team/keras/

releases/tag/2.4.0, 2020.
[20] “Github search api,” 2021.

[Online]. Available: https://docs.

github.com/en/rest/reference/search

[21] J. Cohen, “A coefﬁcient of agreement for nominal scales,” Educa-
tional and psychological measurement, vol. 20, no. 1, pp. 37–46, 1960.
[22] C. B. Seaman, “Qualitative methods in empirical studies of
software engineering,” IEEE Transactions on software engineering,
vol. 25, no. 4, pp. 557–572, 1999.

[23] “tf.keras API,” https://www.tensorﬂow.org/api docs/python/

tf/keras, 2020.

jection experiments,” in 2014 Tenth European Dependable Computing
Conference, 2014, pp. 118–129.

13

Florian Tambon

Amin Nikanjam

[24] “TenforFlow implementation,” https://github.com/tensorﬂow/

tensorﬂow/tree/master/tensorﬂow/python/keras, 2020.

[25] “TensorFlow/Keras

Issue,”

https://github.com/tensorﬂow/

tensorﬂow/issues/32476, 2021.

[26] “TensorFlow/Keras

Issue,”

https://github.com/tensorﬂow/

tensorﬂow/issues/32286, 2021.

[27] “TensorFlow/Keras

Issue,”

https://github.com/tensorﬂow/

tensorﬂow/issues/31324, 2021.

[28] “TensorFlow/Keras

Issue,”

https://github.com/tensorﬂow/

Le An

tensorﬂow/issues/32420, 2021.

[29] “TensorFlow/Keras

Issue,”

https://github.com/tensorﬂow/

tensorﬂow/issues/30486, 2021.

[30] “TensorFlow/Keras

Issue,”

https://github.com/tensorﬂow/

tensorﬂow/issues/38596, 2021.

[31] “Google forms,” https://www.google.ca/forms/about/, 2021.
[32] A. Nikanjam, M. M. Morovati, F. Khomh, and H. Ben Braiek,
“Faults in deep reinforcement learning programs: a taxonomy
and a detection approach,” Automated Software Engineering, vol. 29,
2021.

[33] A. N. Oppenheim, Questionnaire design, interviewing and attitude

measurement. Bloomsbury Publishing, 2000.

[34] (2021) Github REST API. https://docs.github.com/en/rest.
[35] Reddit.
[36] M. Patrick, A. P. Craig, N. J. Cunniffe, M. Parry, and C. A. Gilligan,
“Testing stochastic software using pseudo-oracles,” in Proceedings
of the 25th International Symposium on Software Testing and Analysis,
2016, pp. 235–246.

[37] A. Groce, T. Kulesza, C. Zhang, S. Shamasunder, M. Burnett, W.-
K. Wong, S. Stumpf, S. Das, A. Shinsel, F. Bice et al., “You are
the only possible oracle: Effective test selection for end users
of interactive machine learning systems,” IEEE Transactions on
Software Engineering, vol. 40, no. 3, pp. 307–323, 2013.

[38] A. Biondi, F. Nesti, G. Cicero, D. Casini, and G. Buttazzo, “A safe,
secure, and predictable software architecture for deep learning
in safety-critical systems,” IEEE Embedded Systems Letters, vol. 12,
no. 3, pp. 78–82, 2019.

[39] A. Vahabzadeh, A. M. Fard, and A. Mesbah, “An empirical study
of bugs in test code,” in 2015 IEEE International Conference on
Software Maintenance and Evolution (ICSME), 2015, pp. 101–110.
[40] E. V. D. Kouwe, C. Giuffrida, and A. S. Tanenbaum, “On the
soundness of silence: Investigating silent failures using fault in-

Foutse Khomh

Giuliano Antoniol

