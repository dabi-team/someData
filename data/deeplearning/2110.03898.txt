1
2
0
2

v
o
N
1

]
h
p
-
t
n
a
u
q
[

2
v
8
9
8
3
0
.
0
1
1
2
:
v
i
X
r
a

Diﬀerentiable Programming of Isometric Tensor Networks

Chenhua Geng,1, ∗ Hong-Ye Hu,2, † and Yijian Zou3, ‡
1Institute for Solid State Physics, The University of Tokyo, Kashiwa, Chiba 277-8581, Japan
2Department of Physics, University of California San Diego, La Jolla, CA 92093, USA
3Stanford Institute for Theoretical Physics, Stanford University, Palo Alto, CA 94305, USA
(Dated: November 2, 2021)

Diﬀerentiable programming is a new programming paradigm which enables large scale optimiza-
tion through automatic calculation of gradients also known as auto-diﬀerentiation. This concept
emerges from deep learning, and has also been generalized to tensor network optimizations. Here,
we extend the diﬀerentiable programming to tensor networks with isometic constraints with appli-
cations to multiscale entanglement renormalization ansatz (MERA) and tensor network renormal-
ization (TNR). By introducing several gradient-based optimization methods for the isometric tensor
network and comparing with Evenbly-Vidal method, we show that auto-diﬀerentiation has a better
performance for both stability and accuracy. We numerically tested our methods on 1D critical
quantum Ising spin chain and 2D classical Ising model. We calculate the ground state energy for
the 1D quantum model and internal energy for the classical model, and scaling dimensions of scaling
operators and ﬁnd they all agree with the theory well.

I.

INTRODUCTION

Tensor network has been a powerful tool to study
quantum many-body systems and classical statistical-
mechanical models both theoretically [1, 2] and nu-
merically [3–8]. And recently,
it has been proposed
as an alternative tool
for (quantum) machine learn-
ing tasks, both for supervised learning [9–16], and un-
In the family of tensor
supervised learning [17–22].
networks, the multi-scale entanglement renormalization
ansatz (MERA) and tensor network renormalization
(TNR) are important tensor networks which are inspired
by the idea of renormalization group (RG). They con-
tain tensors that are to be optimized under isometric
constraints, where some tensors are restricted to be iso-
metric.

RG [23–25] plays an important role in modern con-
densed matter physics and high-energy physics. What
lies in the heart of RG is the coarse graining proce-
dure that changes the scale the system. Under RG, mi-
croscopic models ﬂow to diﬀerent ﬁxed points that dis-
tinguish diﬀerent macroscopic phases of matter. More
recently, the concept of RG also ﬁnds applications in
machine learning and artiﬁcial intelligence (AI) [26–30].
Traditionally, RG is performed in the Fourier space,
which involves various approximations. Furthermore, it
has been shown that RG can be performed in real space
using tensor networks.

In the context of quantum many-body physics, MERA
and its variations [4, 31–34] are tensor networks which
perform real-space RG on quantum states. Due to the
speciﬁc structure, MERA is especially suitable for de-
scribing systems with scale invariance such as quantum

∗ xwkgch@issp.u-tokyo.ac.jp
† hyhu@ucsd.edu
‡ zouy@stanford.edu

critical systems. On the other hand, for a classical statis-
tical mechanical model, TNR [34, 35] is a tensor network
algorithm that performs real-space RG for the tensor net-
work that represents the partition function of this model.
Compared to tensor renormalization group (TRG) [36]
which also performs real-space RG, TNR resolves the
computational breakdown of TRG for critical systems.
Additionally, it is known that TNR is closely related to
MERA as formalized in Ref. [37].

From the perspective of theoretical physics, MERA
and TNR can be used to extract universal information
of critical systems, which are in a close relationship with
conformal ﬁeld theory (CFT) [38, 39]. Recently, another
variation of MERA using neural network was proposed
and had been applied to simulation of quantum ﬁeld theo-
ries, and ﬁnding holographic duality based on ﬁeld theory
actions. [27, 40].

From the perspective of numerical studies, one of the
central problems to tensor network is the optimization
of tensors for various systems. Conventional optimiza-
tion algorithms are designed manually and separately for
diﬀerent problems and diﬀerent tensor networks struc-
tures. Recently, diﬀerentiable programming, as a novel
programming paradigm, has been proposed for the op-
timization problems of tensor networks based on gradi-
ent optimization, which is extensively used in deep learn-
ing and artiﬁcial intelligence. Compared with traditional
gradient estimation, such as ﬁnite diﬀerence method, au-
tomatic diﬀerentiation has the merits such as high ac-
curacy and low computational complexity for calculat-
It provides a uni-
ing gradients for many parameters.
ﬁed and elegant solution to many optimization prob-
lems by combining the well-developed automatic diﬀer-
entiation frameworks like PyTorch [41] and TensorFlow
[42]. Recently, it has been successfully applied to TRG
and the optimization of projected entangled pair states
(PEPS) [43]. With the help of diﬀerentiable program-
ming researchers can focus on the part of tensor contrac-
tion in the algorithm without worrying about detailed

 
 
 
 
 
 
and complicated optimization algorithms. However, ten-
sor networks which possess isometric constraints, such
as MERA and TNR cannot be optimized using diﬀeren-
tiable programming directly.
Recently, Hauru et al.

[44] initiated the use of Rie-
mannian optimization to tensor networks with isomet-
ric tensors. Speciﬁcally, they have applied precondi-
tioned conjugate gradient method to MERA and matrix
product states.
In this work, we use a modiﬁed gra-
dient search and show that auto-diﬀerentiation can be
applied to MERA and TNR as well. We explicitly con-
struct the computation graphs for MERA and TNR algo-
rithms. Further, we discuss the gradient-based optimiza-
tion methods and ﬁnd that the combination of Evenbly-
Vidal and gradient-based methods generally have a bet-
ter performance than either of them. Taking 1D quantum
and 2D classical Ising model as examples, we obtain the
ground state energy for the 1D quantum model and inter-
nal energy for the 2D classical model with high accuracy.
We also obtain scaling dimensions of scaling operators at
the critical point within the diﬀerentiable programming
framework.

This paper is organized as follows: In Sec. II we review
the idea of diﬀerentiable programming, including auto-
matic diﬀerentiation, computation graphs and gradient-
based optimization. We introduce our new methods for
optimizing isometric tensors within diﬀerentiable pro-
gramming framework. In Sec. III and IV we brieﬂy re-
view MERA and TNR respectively, and show the results
computed using diﬀerentiable programming. Finally we
make a summary about diﬀerentiable programming and
give our outlook for future research directions in Sec. V.

II. DIFFERENTIABLE PROGRAMMING

The characteristics of diﬀerentiable programming is
automatic diﬀerentiation which computes the derivative
information automatically by the well-developed frame-
works. We ﬁrst review the core concepts of diﬀerentiable
programming and investigate how the derivative infor-
mation are automatically computed for tensor networks.
Then we discuss how to optimize a tensor network, es-
pecially with isometric constraints, by these derivative
information.

A. Computation graphs

Computation graphs are central to the automatic dif-
ferentiation which present how the derivatives are com-
puted with respect to intermediate variables by the chain
rule

∂L
∂θ

=

∂L
∂X n

∂X n
∂X n−1 ...

∂X 1
∂θ

,

(1)

where L is some general objective function used for the
optimization, θ is a general trainable variable and X i

2

are intermediate variables. A computation graph is a
directed acyclic graph where a node represents a tensor
which stores the data and a link indicates the dependence
of data ﬂow during the computation process. The sim-
plest computation graph is the chain graph characterized
by Eq. (1), see Fig. 1.

FIG. 1. An example of the chain graph. The forward and
backward propagation are indicated by black and red arrows.

In diﬀerentiable programming, all the data belongs
to tensors which are represented by higher-order arrays.
The data together with trainable variable θ ﬂows along
the computation graph with a series of intermediate ten-
sors to obtain the ﬁnal output results. This computation
process is referred to as the forward propagation.

In most application scenarios, there are some tensors to
be determined and optimized in the computation graph.
For this purpose, the output results are usually compared
with the expected output data to obtain a scalar, known
as loss function, which evaluates the quality of the com-
putation process. From the loss function, the derivatives
with respect to the intermediate tensors are computed
along the reversed computation graph, which is the so-
called backward propagation. Once a computation graph
has been set up, one can directly obtain the derivative
tensors of output with respect to the input or other in-
termediate tensors through backward propagation. Us-
ing the derivative information of the intermediate ten-
sors, the parameters in the computation graph can be
optimized by various optimization methods which will
be discussed later.

For a general computation graph, the computation of
the derivative of a node need to sum over all contributions
from its child nodes

¯X i =

(cid:88)

j:child of i

¯X j ∂X j
∂X i ,

(2)

where the adjoint variable ¯X = ∂L/∂X is deﬁned as
the derivative of the ﬁnal output L with respect to the
variable X as shown in Fig. 1.
If a node X has sev-
eral diﬀerent paths to ﬂow and aﬀect the ﬁnal output,
the derivative ¯X will account all the contribution among
these paths.

In the conventional tensor network computation,
it
usually involves computing the environment of a ten-
sor which is also a tensor obtained by computing a fully
contracted network without this tensor. These environ-
ment tensors are used to optimize the tensor network,
but it is laborious to draw and determine the details
of the contraction graph of the environment tensor by
hands. Besides, because of the tensor contraction opera-

θ1X2X∂=∂XX22∂=∂XXXX2121tion with a large amount of tensors, computing the envi-
ronment of a tensor is costly for the computer. We note
that the derivation with respect to a tensor is computed
by removing the tensor and compute the contraction of
the remnant tensor network, which is exactly the deriva-
tive of a tensor is exactly the environment of the tensor
so long as the computation graph is constructed prop-
erly. The observation above motivates the application of
automatic diﬀerentiation to optimizing tensor networks
in order to get rid of the tedious environments com-
putation. The diﬀerentiable programming can be eas-
ily implemented using modern machine learning frame-
works, like PyTorch, TensorFlow. Furthermore, these
frameworks provide easy-to-use interfaces with high per-
formance computing units, such as Graphics Process-
ing Units (GPUs) and Tensor Processing Units (TPUs),
which remarkably boost the computation speed.

B. Gradient-based Optimization

Having established the computation graph, we need to
determine the optimization method used in diﬀerentiable
programming. Given the input data D, the optimization
problem is to ﬁnd an optimal mapping f (D) with param-
eters X to minimize the loss function

min
X

L(D, fX (D)).

(3)

Gradient descent method is the earliest and most com-
mon optimization method. The idea of gradient descent
method is to iteratively update the parameter X by sub-
tracting the gradient of the parameter ¯X timed by a fac-
tor η, known as learning rate,

Xt+1 ← Xt − η ¯Xt.

(4)

Some studies have found that in many cases the diﬃ-
culty of optimization comes from the “saddle point” [45]
where the slope is positive in some directions and nega-
tive in another directions. There are several ways to help
the optimization escape saddle points.

For example, the momentum [46] is introduced in gra-
dient descent to simulate the inertia of optimization pro-
cess. In the momentum method the historical inﬂuence
is taken into account

Mt+1 ← βmMt + η ¯Xt,
Xt+1 ← Xt − Mt+1,

(5)
(6)

where βm ≤ 1 is the so-called momentum factor. With
beneﬁting from the extra variable M , the momentum
method can help speed up the convergence and get away
from saddle points.

Another direction of improving gradient descent is to
adjust the learning rate dynamically. For instance, RM-
Sprop [47] uses the historical decayed gradients accumu-

lated up to adjust the learning rate automatically by

(cid:113)

vt+1 ←

βvvt + (1 − βv)( ¯Xt)2,

Xt+1 ← Xt −

η
vt+1

¯Xt,

where βv is a decay factor.

3

(7)

(8)

C. Riemannian Optimization for isometric tensors

In many physics problems, some of the tensors and pa-
rameters are required to be satisﬁed isometric constraints
X T X = I where X is the parameter to be optimized of
the form of matrices and tensors. The isometric con-
straints for tensor contraction is illustrated in Fig. 3 and
Fig. 11. In this paper we assume the parameter X is real
and X T denotes the transpose of X. The generalization
to complex cases is straight forward.

However, the isometric constraints, also referred as
(semi-) orthogonal or (semi-) unitary constraints, ob-
struct the direct application of the optimization methods
in the previous subsection. There are two ways to impose
the constraints during the optimization.

One is the soft-constraint optimization, which allows
the isometries and unitaries away from the constraints
and add the deviation from the constraints into the
loss function. This is known as the Lagrange multiplier
method [48]. Then it becomes an unconstrained problem
which the gradients of tensors are simply derivatives of
tensors. We can directly use the inbuilt optimizers of the
deep learning frameworks to minimize the modiﬁed loss
function which is composed of the energy and deviation,
see Appendix F.

The other is the hard-constraint optimizations, which
restrict the isometries and unitaries to be isometric. This
corresponds to the optimization problems on the Stiefel
manifold [49, 50]. In this case, the gradients of tensors
should be conﬁned in the Stiefel manifold, being diﬀer
from the simply derivatives of tensors which is the gradi-
ent tensors of the whole Euclid space. In the following,
we focus on the hard-constraint optimization.

The hard-constraint optimization requires the tensors
to be optimized satisfying the constraints during the
whole computation process. A widely used strategy
for solving the problem is the Riemannian optimization.
Various methods based on Riemannian optimization have
been studied extensively, including polar decomposition
[51], QR decomposition [52], Cayley transform [50, 53–
56]. The applications of Riemannian optimization have
also been studied in physics like quantum control and
quantum technologies [57, 58].

Basically, the Riemannian optimization includes two
steps: (1) Find the gradient vector in the tangent space
of the current point on Stiefel manifold. (2) Find the de-
scent direction and ensuring the new points on the man-
ifold.

Speciﬁcally, assume the tensor X is one of the tensors
to be optimized with the constraints. Deﬁne the set {X ∈
(cid:82)n×p : X T X = I, n (cid:62) p} as the Stiefel manifold with the
dimension np − 1
2 p(p + 1). And deﬁne the tangent space
at the point X as TX = {Z ∈ (cid:82)n×p : Z T X + X T Z = 0}.
Now the problem becomes to found a point on the Stiefel
manifold with the minimum loss function L

min
X∈(cid:82)n×p

L(X)

s.t. X T X = I.

(9)

For the ﬁrst step, the inner product in the tangent
space TX should be deﬁned to obtain the gradient GX ∈
TX . Let Z1, Z2 ∈ TX , we can deﬁne the Euclidean inner
product as (cid:104)Z1, Z2(cid:105)e = tr(Z T
1 Z2). But it is more widely
used to deﬁne a more natural choice which is the canon-
2 XX T )Z2). In
ical inner product (cid:104)Z1, Z2(cid:105)c = tr(Z T
the following we choose the canonical inner product. It
can be proved [50] that the gradient GX on the manifold
is

1 (I − 1

GX = AX = ¯X −

1
2

(XX T ¯X + X ¯X T X),

(10)

2 X( ¯X T X − X T ¯X)X T . In
where A = ¯XX T − X ¯X T + 1
other words, the gradient GX is obtained by projecting
the derivative ¯X onto the tangent space TX of Stiefel
manifold.

For the second step, the so-called operation retraction
plays an important role. Generally speaking, there are
two classes of retraction to keep the updated point on the
manifold: projection-like and geodesic-like schemes. The
projection-like schemes preserve the constraint by pro-
jecting a point into the manifold such as QR decompo-
sition, while the geodesic-like schemes preserve the con-
straint by moving a point along the geodesic or quasi-
geodesic line such as the Cayley transform.

4

2.

geodesic-like schemes

The point X on the manifold moves along a single pa-
rameter curve Y (η) such that the curve is on the Stiefel
manifold, i.e. Y (η)T Y (η) = I, and the derivative of the
curve at origin is the gradient vector Y (0)(cid:48) = −GX .
One of the choice of the curve is Cayley transform

Y (η) =

(cid:16)

I +

η
2

A

(cid:17)−1 (cid:16)

I −

(cid:17)

A

X,

η
2

(15)

where A is the matrix in Eq. (10). Then update equation
is

(cid:17)

(cid:16)

Xt+1 =

η
2
Note that the inverting a n × n matrix I + η

(cid:17)−1 (cid:16)

I −

I +

η
2

At

At

2 A is costly.
In Appendix B, we show that the computation expense
can be reduced by Sherman-Morrison-Woodbury formula
or an iteration method.

(16)

Xt.

Algorithm improvement: In order to improve the
optimization performance, We apply the optimization
techniques introduced in Sect. II B to our Riemannian
optimization methods. Generally, both the accuracy and
optimization speed could be improved by introducing
the dynamic momentum and adaptive learning rate tech-
niques [56].

To be speciﬁc, instead of directly using the derivative
¯X in the gradient computation Eq. (10), we introduce a
decorated matrix momentum M to replace ¯X. Then the
momentum and the gradient are computed by

Mt+1 ← βmMt + ¯Xt,

GX ← Mt+1 −

1
2

(XtX T

t Mt+1 + XtM T

t+1Xt),

(17)

(18)

(19)

(20)

1. projection-like schemes

Mt+1 ← GX ,
Xt+1 ← RGX

η

(Xt),

The point X on the manifold moves along the gradient
vector a short distance to the new point X − ηGX , where
η is a tunable parameter representing to the learning rate.
But generally, the new point X − ηGX is not the point
on the manifold. We need to project the point X − ηGX
onto Stiefel manifold by various methods.

One way is the QR-decomposition-type retraction by
noting that the Q factor of QR decomposition is orthog-
onal. Then the update equation is

QR = Xt −

Xt+1 = Q.

1
2

ηGX ,

(11)

(12)

Another way is to use SVD as retraction. If the SVD
of a matrix X ∈ (cid:82)n×p is X = U ΣV T , then the projec-
tion map onto Stiefel manifold is π(X) = U In×pV T [59].
Then the update equation is

η

where M0 = 0 and βm is a hyperparameter to be tuned.
Here RGX
(Xt) denotes the retraction operation, which
can be substituted by Eq. 11 - 14 and Eq. (16). Note
that we project the momentum onto the tangent space
of Stiefel manifold.

A variant adaptive learning rate is also utilized by

ηadapt = min(η, αη/((cid:107)A(cid:107) + (cid:15))),

(21)

where αη is a hyperparameter to be tuned and (cid:107)A(cid:107) de-
notes the norm of tensor A. The adaptive learning rate is
such that a large learning rate is used when the norm of
gradient is small. These techniques may improve the gra-
dient diﬀusion problem during the optimization process
and accelerate the convergence speed.

III. APPLICATION TO MERA

U ΣV T = Xt − ηGX ,
Xt+1 = U In×pV T .

(13)

(14)

The multiscale entanglement renormalization ansatz
(MERA) is a variational ansatz for the ground state and

low-energy excited states of critical quantum spin chains.
Numerically, it has been used to extract universal infor-
mation, such as scaling dimensions and operator prod-
uct coeﬃcients from the critical spin chain Hamiltonian.
Furthermore, it has found applications in the context of
holography [60] and emergent geometry [61].

A. Review of MERA

FIG. 3. The isometric constraints for isometries w and disen-
tanglers u.

5

FIG. 2. A one-dimensional
isometries w and disentanglers u.

inﬁnite ternary MERA with

The MERA |ψ(u, w)(cid:105) is composed of layers of isome-
tries w and disentanglers u which satisfy the isometric
constraints,

u†u = uu† = I, ww† = I,

(22)

which are expressed graphically in Fig. 3. Physically, the
MERA can be viewed as a renormalization group ﬂow in
the real space. From the bottom upwards, each layer of
MERA coarse-grains the quantum state. The isometric
constraints of the tensors are essential from both physi-
cal and numerical perspectives. From the physical per-
spective, the isometric constraints keep the norm of the
state and retain the causal structure [32] under the coarse
graining. From the numerical perspective, the isometric
constraints of the tensors ensures that expectation val-
ues of the local operators can be computed in polynomial
time.

Depending on the number of bonds of the isometries
w, there are diﬀerent types of MERA [62] and in this
work we focus on the ternary MERA as in Fig. 2. We
further assume translational invariance, where the u and
w tensors are the same in the same layer. We also assume
scale invariance, where all u’s and w’s are the same above
a certain number of transitional layers. Let the number of
transitional layers to be n, then the variational ansatz is
completely speciﬁed by uτ and wτ , where τ = 1, 2, · · · n+
1 denotes the layers from bottom to top, and un+1 and
wn+1 constitute the scale-invariant layers.

In order to obtain the ground state, one optimizes the

energy function as the loss function,

E =

(cid:104)ψ(u, w)|H|ψ(u, w)(cid:105)
(cid:104)ψ(u, w)|ψ(u, w)(cid:105)

= (cid:104)ψ(u, w)|H|ψ(u, w)(cid:105), (23)

FIG. 4. Ascending superoperators ascend the operator oτ −1
at the layer τ − 1 to the operator oτ at the layer τ . They
include three diﬀerent form AL, AC and AR. The average
ascending superoperators ¯A = (AL + AC + AR)/3.

where in the second equality we have used the normaliza-
tion of the state as a result of the isometric constraints.
It is well known [31] that the computation of the ex-
pectation value of local operators in a MERA involves
two superoperators, the ”ascending superoperator” and
the ”descending superoperator”. The ascending super-
operator ¯A transforms local operators oτ −1 at layer τ − 1
to local operators oτ at layer τ , oτ = ¯Aτ (oτ −1), as shown
in Fig. 4. The descending superoperator, as the adjoint
of the ascending superoperator, transforms the two-site
reduced density matrix ρτ at layer τ to the two-site re-
duced density matrix ρτ −1 at layer τ , ρτ = ¯Dτ (ρτ +1), as
shown in Fig. 5.

The coarse-graining is such that expectation values of

the operator oτ for each layer τ are the same

tr(oρ) = tr(oτ ρτ ).

(24)

At the scale invariant layer T ≡ n + 1, the two-site

reduced density matrix satisﬁes

ρT = ¯DT (ρT ),

(25)

which is the unique eigenoperator of the average ascend-
ing superoperator ¯DT with eigenvalue 1. The eigen-
operator can be approximately computed by the power

FIG. 5. Descending superoperators descend the density tensor
ρτ at the layer τ to the density tensor rhoτ −1 at the layer
τ −1. They include three diﬀerent form DL, DC and DR. The
average descending superoperators ¯D = (DL + DC + DR)/3.

wuw†w==u†u(a)(b)=++†wwoτ1oτ−†uu†ww†uu1τρ−τρ=method, i.e., repeatedly applying ¯DT to any initial state
until convergent.

To summarize, starting with random isometric tensors
w, u under the constraints Eq. (22), the standard com-
putation process of constructing a MERA involves two
steps [31]:

1. Top-bottom: Computing the density tensor ρT at
the top layer by solving the eigenoperator of ¯DT ,
then computing the density tensors ρτ for all lay-
ers τ < T from top to bottom by the descending
superoperators ¯Dτ .

2. Bottom-Top: From bottom to top, update the
isometry and disentangler tensors wτ and uτ , and
compute the Hamiltonian Hτ for all layers τ by as-
cending superoperators ¯Aτ .

For the updating procedure, we can use Evenbly-Vidal
method [31] (see Appendix A) or gradient-based methods
introduced above. Repeating the (1) top-bottom and (2)
bottom-top process, the tensors w and u in MERA will
be constructed.

We note that in diﬀerentiable programming only the
top-bottom approach is used and the bottom-top ap-
proach is computed automatically once the optimization
method is determined.

Scaling dimensions: Once the MERA has been con-
structed and optimized, we can easily extract the scaling
dimensions, which are universal properties of the phase
transition that we can obtain easily from MERA. Take
our ternary MERA as example, consider an on-site op-
erator φα in scaling invariant layers. By the on-site as-
cending superoperator Fig. 6, the operator φα is lifted to
¯Aτ (φα) in the next scale. The scaling operators are found
by the ﬁxed points of the ascending superoperator.

¯Aτ (φα) = λαφα.

(26)

It can be easily shown that the two-point correlator of
such scaling operators are

(cid:104)φα(3r)φα(0)(cid:105) = λ2

α(cid:104)φα(r)φα(0)(cid:105).

(27)

The scaling dimensions can therefore be calculated by
∆α = − log3 λα.

6

FIG. 7. The computation graph for translation invariant
MERA. At the left part the density tensor is iterated several
times in the scaling invariant layer as the input data. The
forward propagation (grey arrow) involves the descending op-
eration with parameters w and u as in Fig. 5. On the bottom
layer, the density matrix is contracted with the Hamiltonian
to obtain the energy as the loss function. The backward prop-
agation (red arrow) computes the derivative tensors ¯wτ , ¯uτ
and ¯ρτ automatically. Finally the parameters w and u are
updated by the gradient optimization method.

B. Auto diﬀerentiation

Computation graph: The computation graph for
the loss function of the optimization of MERA is shown
in Fig. 7. The forward and backward propagation corre-
spond to the top-bottom and bottom-top processes in the
conventional computation, respectively. The top layer re-
duced density matrix ρT is computed by iteration of the
descending superoperator for several layers. The isome-
tries w and disentanglers u serve as network parameters
to be trained. For each layer τ , the reduced density ma-
trix ρτ ﬂows to that of the lower layer ρτ −1 by the de-
scending superoperators. Note that the descending su-
peroperators only involve tensor contractions which can
be backward propagated automatically. At the bottom
layer, the density matrix ρ0 is contracted with Hamil-
tonian H0 to obtain the energy E as the loss function
L. Then the derivative tensors ¯wτ , ¯uτ and ¯ρτ of each
layer τ are computed during automatically by backward
propagation. We note that ¯wτ and ¯uτ equal to the envi-
ronment tensors of w and u, respectively. Finally, due to
Eq. (24), we have

¯ρτ =

∂E
∂ρτ

=

∂tr(Hτ ρτ )
∂ρτ

= Hτ .

(28)

The optimization of wτ and uτ can be done by the
gradient optimization as in Sec. II. This can be combined
with the traditional Evenbly-Vidal algorithm [31] and we
compare the performance below.

Results: We use the critical one dimensional trans-

verse ﬁeld Ising model [63] to test the algorithm.
(cid:16)

(cid:17)

(cid:88)

H0 = −

x σ[r+1]
σ[r]
x

+ λσ[r]
z

,

(29)

FIG. 6. The on-site ascending superoperator acts on the
eigenstate φα with the eigenvalue λα.

r

where λ = 1 for the critical point.

The ground state energy density is known exactly,

Eexact = −

1
2π

(cid:90) π

(cid:113)

−π

(1 − cos k)2 + sin2 k dk = −

.

4
π
(30)

=αφαλ†wwαφTρTρ1Tρ−0ρTwTwTuTu1Tw−1Tu−0HE......We use the PyTorch framework in Python to realize the
auto-diﬀerentiable algorithm with the GPU acceleration.
As comparison of the computation speed, for the opti-
mization of a MERA with χ = 8 in 104 iterations using
Evenbly-Vidal method, our diﬀerentiable programming
with GPU acceleration costs 883 seconds, while the con-
ventional programming without GPU acceleration costs
5109 seconds. For gradient-based methods the GPU can
also accelerate the computation speed. Our test platform
is a laptop with Intel(R) Core(TM) i7-6700HQ CPU @
2.60GHz and NVIDIA GeForce GTX 1060 GPU.

7

It is known that the MERA optimization is vulnera-
ble to be stuck into local minima and gradient diﬀusion.
[44] In the practice of Ising model optimization, we ﬁnd
that if after the ﬁrst several hundred iterations the en-
ergy error is still larger than a threshold, it will be high
probability that the optimization is stuck into local min-
ima. In order to reduce the chance of being stuck into
local minima and improve the success rate of optimiza-
tion, we used a resetting mechanism, see Appendix C for
details of resetting mechanism. The resetting here means
that the network parameters w and u and the two-site
reduced density matrix are set to the initial state. The
number of resetting times can be used as an indicator to
show the stability of optimization. High stability means
that the optimization has a low possibility to be stuck
at a local minima. We can see from Fig. 8 (a) that for
gradient-based methods (SVD, Cayley and the random
mixed methods) the energy errors fall quickly at the be-
ginning, meaning that the possibility of being stuck into
local minima is lower than the Evenbly-Vidal method.
Indeed, we ﬁnd that these methods hardly trigger the re-
setting mechanism, while the Evenbly-Vidal method has
certain possibility to trigger the resetting mechanism.

FIG. 8.
(a) The comparison of optimization curves with
Evenbly-Vidal method (green dashed line), SVD method
(blue solid line), Cayley method (yellow solid line) and the
random mixed method (purple solid line). The max bond di-
mension χ is 12 and the number of transitional layers is 3.
(b) The computed scaling dimensions with the four method
comparing with the exact values (grey dot-dashed line).

In addition to the Riemannian optimization methods
mentioned in II C, we also propose a random mixed
method combining the Evenbly-Vidal method and the
gradient-based methods. In random mixed method, we
use Evenbly-Vidal method as basis method for iterations
and replace an iteration by gradient descent methods
with SVD or Cayley retractions every 5 iterations.

The error in energy for Evenbly-Vidal, SVD, Cayley
and the random mixed methods are shown in Fig. 8 (a).
For SVD and Cayley methods, we introduce dynamical
momentum and adaptive learning rate techniques with
η = 1.0, βm = 0.9 and αη = 4.0. The learning rate is
decayed every 10 iterations with the decay factor 0.999.
Beneﬁting from the momentum and adaptive learning
rate techniques, the errors of energy can be reduced by
more than an order of magnitude.

FIG. 9. Repeated energy errors with Evenbly-Vidal and ran-
dom mixed methods with the same setting as Fig. 8, except
for the max bond dimension χ = 10 here.

In order to speed up convergence, we applied a gradu-
ally lifting bond dimension trick with the bond dimension
χ increasing from 4 to 12, see Appendix C for details of
lifting bond dimension trick. We can see in Fig. 8 (a) and
Fig. 9 that there are some cusps in the errors of energy,
corresponding to the bond dimension lifting.

In Fig. 8 (a), we ﬁnd that although the accuracies of
SVD and Cayley methods are not good as Evenbly-Vidal
method, the combination of them with Evenbly-Vidal
method, corresponding to the random mixed method,
has a better accuracy. We repeat the same optimiza-
tion process several times for Evenbly-Vidal method and
random mixed method as shown in Fig. 9, ﬁnding that
generally the random mixed method has better perfor-
mance in both stability and accuracy. For other models
the performance can also be improved by applying ran-
dom mixed method, see Appendix D.

We can provide an explanation for the performance

cuspscuspsimprovement.
It is known that the main diﬃculty of
the optimization comes from the saddle points [45]. One
idea for escaping saddle points is to introduce ﬂuctuation
such as using random input data in Stochastic gradient
descent (SGD) [64]. However, in our problem the in-
put data is deterministic (determined by w and u of the
top layer in MERA). Therefore we introduce perturba-
tion of the optimization with Evenbly-Vidal method by
randomly choosing SVD and Cayley methods and adding
an iteration with this method into the optimization pro-
cess, as what did in the random mixed method. Diﬀerent
methods possess diﬀerent searching way in the parame-
ter space. Switching the method meaning the changing of
searching way, which can help the optimization process
to escape saddle points. As a result, such a combina-
tion of diﬀerent optimization methods can improve both
stability and accuracy beneﬁting from ”shaking up the
system”.

The scaling dimensions can also be computed using the
optimized MERA. The ﬁrst three scaling dimensions us-
ing the MERA optimized by Evenbly-Vidal, SVD, Cayley
and the random-mixed methods are shown in Fig. 8 (b).
As we can see, our optimized MERA could produce good
value of scaling dimensions for the ﬁrst several scaling di-
mensions. The scaling dimensions of higher order terms
usually needs tensor networks with larger bond dimen-
sions.

IV. APPLICATION TO TNR

The partition function of a d + 1-dimensional classi-
cal statistical-mechanical models or d-dimensional quan-
tum many-body systems can be represented as a d + 1-
dimensional tensor network. One way of computing the
partition function is based on the real-space RG, where
the linear size of the tensor network is reduced at each
step. There are several methods that stood out, includ-
ing the TRG [36], HOTRG [65], TNR [34, 35], Loop-TNR
[66] and Gilt-TNR [67]. Most of the techniques work ex-
tremely well in d = 1, and some of them also work in
d = 2. In this work we focus on tensor network renormal-
ization (TNR) in d = 1, which produces a proper RG ﬂow
for a critical system. In TNR, there is an optimization
over disentanglers and isometries similar to MERA [37].
Therefore, the diﬀerentiable programming techniques in
this work can be applied. This generalizes previous work
where the application of diﬀerentiable programming to
TRG is discussed [43].

A. Review of TNR

Here we brieﬂy review the algorithm of TNR. For more
details we refer to Ref.
[35]. Consider the 2D classical
Ising model on square lattice with inverse temperature

8

FIG. 10. The square lattice of classical spins σ = ±1 rotated
by 45◦ maps into a tensor network.

β. The partition function is

where

Z =

(cid:88)

{σ}

e−βH(σ),

H(σ) = −

(cid:88)

(cid:104)i,j(cid:105)

σiσj,

(31)

(32)

and σi = ±1 is the Ising spin on site i. The partition
function is a square tensor network consisting of four-
index tensors Aijkl which are located in the center of
every second plaquette of Ising spins as in Fig. 10, where

Aijkl = eβ(σiσj +σj σk+σkσl+σlσi).

(33)

Then the patition function Eq. (31) is given by the con-
traction of the tensor network

Z(β) =

(cid:88)

ijk···

AijklAmnojAkrst · · · .

(34)

At each step of the coarse-graining, we approximate
a 2 × 2 block of tensors by inserting isometries and uni-
taries as in Fig. 12, where the tensors vL, vR, u satisfy the
isometric constraints as in Fig. 11 (a)-(c). The tensors
vL, vR, u are determined by minimizing the approxima-
tion error as in Fig. 13.

FIG. 11. The isometric constraints for (a) the isometries vL,
(b) the isometries vR, (c) the disentanglers u and (d) the
isometries w. The bond dimension χ of each index are illus-
trated.

The optimization has been achieved by Evenbly-Vidal
method as in Ref. [35]. Here we use the gradient-based
methods like SVD and Cayley methods discussed in this
paper.

Then, we contract some of the tensors into a tensor B
as deﬁned in Fig. 12. Next, a singular value decomposi-
tion on the tensor B is performed to obtain the isometric

AAAAiσjσkσlσmσnσoσpσqσrσsσtσijklmnoprqst====(a)(b)(c)†Lv†RvLvRvu†uw†w(d)LχRχuχVχHχHχ9

We note that the tensors w should be also optimized in
order to minimize the approximation errors as in Fig. 13.
But here we can apply an alternative method by solving
the eigenvectors of the matrix of the ﬁrst sub-network
of Fig. 15 with contracting the left and right indices, be-
cause of the hermitian property of this sub network.

Repeating the computation above, the original large
tensor network of the partition function can be simpliﬁed
to one or a few tensors, which can be easily computed.
Also note that at each RG step, the tensor A is divided
by the norm of A to prevent the data explosion.

FIG. 12. The original sub-network is approximated by a new
network by inserting pairs of isometries and disentanglers.
The tensor B is deﬁned for convenience.

FIG. 13. The loss function deﬁned as the approximation error
δ.

B. Auto diﬀerentiation

FIG. 15. The deﬁnition of the coarse grained tensor Aout.

tensors uB, vB and the diagonal matrix sB containing
singular values, which is truncated to χ × χ by discard-
ing smallest diagonals. The diagonal matrix sB is then
splited to make a pair of
sB, and we deﬁne new tensors
yL = uB
sBvB as shown in Fig. 14. The
sB and yR =
tensor network is now entirely composed of the tensor on
the left of Fig. 15.

√
√

√

FIG. 14. The approximation of tensor B by making singular
value decomposition.

Finally, we insert the isometry w with the isometric
constraint shown in Fig. 11 (d) and obtain Aout, which
eﬀectively contain the information of four tensors in the
original tensor network.

Computation graph: Within the diﬀerentiable pro-
gramming framework, we show the computation graph of
TNR in Fig. 16.

Given the inverse temperature β, we ﬁrst construct
the tensor network representation for the system. Then
we use TNR to coarse grain the tensor network until
the network only consists of single or a few tensors. At
each RG step, we use the truncation error as the loss
function, and then backward propagate to optimize the
parameters vL, vR and u. We refer to Appendix E for
detail description of the computation graph of TNR.

Since the computation graph for the loss function is
short, the optimization of parameters vL, vR and u is
easy with the fast convergence. At the last step where
only one tensor AT remains in the network, we obtain
the partition function ln Z by taking the tensor trace of
AT and multiplied by the Anorm of all previous steps.

Results: For 2D classical Ising model on an inﬁnite
square lattice, the logarithm of partition function is ex-
actly known [68]

ln Z =

1
2

ln 2 +

1
2π

(cid:90) π

0

ln

(cid:18)

cosh2 2β +

(cid:112)

1
λ

1 + λ2 − 2λ cos 2x

(cid:19)

dx.

(35)

In Fig. 17 (a), we show the computed ln Z accords well
with the exact value as a function of the inverse temper-
ature β by random mixed method. We can see that the
computed results by diﬀerentiable programming ﬁt well
with the exact values.

We show the computed internal energy results E =
in Fig. 17 (b) by ﬁnite diﬀerence method. We ﬁnd

− ∂ ln Z
∂β

the random mixed method can also improve the perfor-
mance of optimization in TNR, as shown in Fig. 18.

As a comparison of computation speed, for the con-
struction of a 28 × 28 TNR with χ = 14 with 800 iter-
ations for each layer using random mixed method, our
diﬀerentiable programming with GPU acceleration costs
708 seconds, while the conventional programming with-

=†Lv†Lv†Rv†RvLvLvRvRvuu†u†uA†A≈B−2=δ≈=BuBsBvLyRyB†Lv†RvLvRv≈=LyRyw†woutA10

FIG. 16. The computation graph for TNR. The grey dotted rounded rectangle indicate one step of TNR computation. The
forward and backward propagation processes appear in each coarse-graining step. The gray arrows show the forward propagation
path, while the red arrows show the backward propagation path.

FIG. 18. The approximation errors of 2D classical Ising model
computed by diﬀerentiable programming with Evenbly-Vidal
method and random mixed method. Here we take approxi-
mation errors of the 3rd, 5th and 7th layers as examples.

V. SUMMARY

In this paper, we extend the application of diﬀeren-
tiable programming to an important family of tensor
networks with isometric constraints such as MERA and
TNR. We show how the MERA and TNR are constructed
and optimized within diﬀerentiable programming frame-
work by explicitly illustrating the computation graphs
of them. We discuss several gradient-based optimization
methods that supplement traditional methods for tensor
networks with isometric constraints and show that the
performance of optimization can be improved by combin-
ing the gradient-based method and traditional methods.
The diﬀerentiable programming in tensor network op-
timization has several advantages. (1) Diﬀerentiable pro-
gramming has an uniﬁed programming paradigm since
we only need to specify the computation graph and the
optimization method for the optimization.
(2) In dif-
ferentiable programming, the gradient can be computed
automatically, which releases the labor on the tedious
contraction computation of environment graphs. (3) Dif-
ferentiable programming has extensibility and ﬂexibility
in practice. It can be modularized, in which one can eas-

FIG. 17. (a) The ln Z of 2D classical Ising model computed
by diﬀerentiable programming with respected to the inverse
temperature β. The blue line represents the exact value while
the orange cross points represent the computed results by dif-
ferentiable programming. The size of system is 28 ×28 and the
max bond dimension here is χ = 14. (b) The internal energy
as the function of β computed by ﬁnite diﬀerence method.

out GPU acceleration costs 2456 seconds.

Scaling dimensions: Once the TNR has been con-
structed and optimized, the scaling dimensions can also
be extracted easily. Here we use the transfer matrix tech-
nique [69] to compute the scaling dimensions with the
2)/2 shown
critical inverse temperature βc = ln (1 +
in Fig. 19. We can see that with the TNR and transfer
matrix technique the computed scaling dimensions have
accurate results even at quite higher orders.

√

β0A0B,0outA,0newA,0exactAδ,0Rv,0Lv0u,0Bu,0Bv,0Bs0w,0Ly,0Ry,0normA1A1B,1outA,1newA,1exactAδ,1Rv,1Lv1u,1Bu,1Bv,1Bs1w,1Ly,1Ry,1normA,2normA...TrTAlnZTA,normTA11

of tensor network’s structure [70]. This will serve as a
uniﬁed method to discover hidden structures of the data.
For example, given the measurement results of a quantum
state, can machine ﬁnd the best tensor network structure
that represents the data? It has been shown that based
on the entanglement data, neural network can establish
diﬀerent hidden geometry (structure) for area law, loga-
rithmic law, and volume law quantum states as an analog
of holographic duality [71]. In tensor network studies, it
is also known that random tensor networks are closely
related to holographic duality [72]. Whether diﬀerent
hidden geometry of tensor network can emerge purely
based on observation data of diﬀerent quantum states is
both an interesting and fundamental question for physics.
Last but not the least, tensor networks with unitary or
isometric constraints are closely related to quantum ma-
chine learning [73], as unitary tensors are represented
by quantum gates. Our methods can also be applied to
simulate and train quantum neural network ansatz for
real-world applications [30].

ACKNOWLEDGEMENTS

We thank Masaki Oshikawa, Yasuhiro Tada, Atsushi
Ueda, Jin-Guo Liu, Song Cheng, Yi-Zhuang You and
Markus Hauru for helpful discussions. The simulations
were performed using the PyTorch [41] and we have the
open source package ”IsoTensor” available on GitHub
[74]. CG was supported by JSPS KAKENHI Grant Num-
bers JP19H01808. HYH is supported by the UC Hell-
man fellowship. YZ is supported by the Stanford Q-Farm
Bloch Postdoctoral Fellowship in Quantum Science and
Engineering.

FIG. 19. The scaling dimensions of 2D classical Ising model
computed by diﬀerentiable programming. The dotted gray
lines indicate the exact values. The size of system is 210 × 210
and the max bond dimension here is χ = 20.

ily change and add diﬀerent network layers and optimiza-
tion methods. (4) We can also beneﬁt from the parallel
computation with the GPU acceleration. (5) Last but not
the least, diﬀerentiable programming can also help with
the optimization of arbitrary network networks when no
traditional optimization methods are known.

With the merits above, the diﬀerentiable programming
can be applied to other tensor networks such as higher-
dimensional MERA or TNR, which have not yet been
implemented previously due to the complexity of the net-
work structures and the high computation expenses. It is
also very interesting and challenging to combine the op-
timization of tensor data together with the optimization

[1] P. Hayden, S. Nezami, X.-L. Qi, N. Thomas, M. Walter,
and Z. Yang, Holographic duality from random tensor
networks, Journal of High Energy Physics 2016, 9 (2016).
[2] F. Pastawski, B. Yoshida, D. Harlow, and J. Preskill,
Holographic quantum error-correcting codes: toy models
for the bulk/boundary correspondence, Journal of High
Energy Physics 2015, 149 (2015).

[3] F. Verstraete and J. I. Cirac, Renormalization algorithms
for Quantum-Many Body Systems in two and higher
dimensions, arXiv e-prints , cond-mat/0407066 (2004),
arXiv:cond-mat/0407066 [cond-mat.str-el].

[4] G. Vidal, Class of quantum many-body states that can
be eﬃciently simulated, Phys. Rev. Lett. 101, 110501
(2008).

[5] G. Vidal, Eﬃcient classical simulation of slightly entan-
gled quantum computations, Phys. Rev. Lett. 91, 147902
(2003).

[6] U. Schollw¨ock, The density-matrix renormalization

group, Rev. Mod. Phys. 77, 259 (2005).

[7] A. Tilloy and J. I. Cirac, Continuous tensor network
states for quantum ﬁelds, Phys. Rev. X 9, 021040 (2019).
[8] F. Verstraete, V. Murg, and J. Cirac, Matrix prod-
uct states, projected entangled pair states, and vari-

ational renormalization group methods for quantum
spin systems, Advances in Physics 57, 143 (2008),
https://doi.org/10.1080/14789940801912366.

[9] A. Novikov, M. Troﬁmov, and I. Oseledets, Exponen-
tial Machines, arXiv e-prints , arXiv:1605.03795 (2016),
arXiv:1605.03795 [stat.ML].

[10] E. Stoudenmire and D. J. Schwab, Supervised learn-
ing with tensor networks, in Advances in Neural Infor-
mation Processing Systems, Vol. 29, edited by D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett
(Curran Associates, Inc., 2016).

[11] I. Glasser, N. Pancotti, and J. I. Cirac, From proba-
bilistic graphical models to generalized tensor networks
for supervised learning, arXiv e-prints , arXiv:1806.05964
(2018), arXiv:1806.05964 [quant-ph].

[12] J. Martyn, G. Vidal, C. Roberts, and S. Leichenauer,
Entanglement and Tensor Networks for Supervised Image
Classiﬁcation, arXiv e-prints , arXiv:2007.06082 (2020),
arXiv:2007.06082 [quant-ph].

[13] Z.-Z. Sun, C. Peng, D. Liu, S.-J. Ran, and G. Su, Gener-
ative tensor network classiﬁcation model for supervised
machine learning, Phys. Rev. B 101, 075135 (2020).

012345scaling dimensionsTNR SC (=20)[14] S. Efthymiou, J. Hidary, and S. Leichenauer, Ten-
sorNetwork for Machine Learning, arXiv e-prints ,
arXiv:1906.06329 (2019), arXiv:1906.06329 [cs.LG].
[15] S. Cheng, L. Wang, and P. Zhang, Supervised Learning
with Projected Entangled Pair States, arXiv e-prints ,
arXiv:2009.09932 (2020), arXiv:2009.09932 [cs.CV].
[16] S. Lu, M. Kan´asz-Nagy, I. Kukuljan, and J. I. Cirac,
Tensor networks and eﬃcient descriptions of classi-
cal data, arXiv e-prints , arXiv:2103.06872 (2021),
arXiv:2103.06872 [quant-ph].

[17] J. Cui, M. Shi, H. Wang, F. Yu, T. Wu, X. Luo,
J. Ying, and X. Chen, Transport properties of thin ﬂakes
of the antiferromagnetic topological insulator MnB i2T
e4, Phys. Rev. B 99, 155125 (2019), arXiv:1901.02217
[stat.ML].

[18] Z.-Y. Han, J. Wang, H. Fan, L. Wang, and P. Zhang,
Unsupervised generative modeling using matrix product
states, Phys. Rev. X 8, 031012 (2018).

[19] J. Stokes and J. Terilla, Probabilistic modeling with
matrix product states, Entropy 21, 10.3390/e21121236
(2019).

[20] I. Glasser, R. Sweke, N. Pancotti, J. Eisert, and J. I.
Cirac, Expressive power of tensor-network factorizations
for probabilistic modeling, with applications from hid-
den Markov models to quantum machine learning, arXiv
e-prints , arXiv:1907.03741 (2019), arXiv:1907.03741
[cs.LG].

[21] X. Gao, E. R. Anschuetz, S.-T. Wang, J. I. Cirac, and
M. D. Lukin, Enhancing generative models via quantum
correlations (2021), arXiv:2101.08354 [quant-ph].

[22] J.-G. Liu and L. Wang, Diﬀerentiable learning of quan-
tum circuit born machines, Phys. Rev. A 98, 062324
(2018).

[23] L. P. Kadanoﬀ, Scaling laws for ising models near Tc,

Physics Physique Fizika 2, 263 (1966).

[24] K. G. Wilson and J. Kogut, The renormalization group
and the (cid:15) expansion, Physics Reports 12, 75 (1974).
[25] K. G. Wilson, The renormalization group and critical

phenomena, Rev. Mod. Phys. 55, 583 (1983).

[26] H.-Y. Hu, D. Wu, Y.-Z. You, B. Olshausen, and Y. Chen,
RG-Flow: A hierarchical and explainable ﬂow model
based on renormalization group and sparse prior, arXiv
e-prints , arXiv:2010.00029 (2020), arXiv:2010.00029
[cs.LG].

[27] S.-H. Li and L. Wang, Neural network renormalization

group, Phys. Rev. Lett. 121, 260601 (2018).

[28] M. Koch-Janusz and Z. Ringel, Mutual

information,
neural networks and the renormalization group, Nature
Physics 14, 578 (2018).

[29] S.-H. Li, Learning Non-linear Wavelet Transformation
via Normalizing Flow, arXiv e-prints , arXiv:2101.11306
(2021), arXiv:2101.11306 [cs.LG].

[30] G. Evenbly, Number-State Preserving Tensor Networks
as Classiﬁers for Supervised Learning, arXiv e-prints ,
arXiv:1905.06352 (2019), arXiv:1905.06352 [quant-ph].

[31] G. Evenbly and G. Vidal, Algorithms for entanglement
renormalization, Phys. Rev. B 79, 144108 (2009).
[32] G. Vidal, Entanglement renormalization, Phys. Rev.

Lett. 99, 220405 (2007).

[33] L. Cincio, J. Dziarmaga, and M. M. Rams, Multiscale
entanglement renormalization ansatz in two dimensions:
Quantum ising model, Phys. Rev. Lett. 100, 240603
(2008).

12

[34] G. Evenbly and G. Vidal, Tensor network renormaliza-

tion, Phys. Rev. Lett. 115, 180405 (2015).

[35] G. Evenbly, Algorithms for tensor network renormaliza-

tion, Phys. Rev. B 95, 045117 (2017).

[36] M. Levin and C. P. Nave, Tensor renormalization group
lattice models,

approach to two-dimensional classical
Phys. Rev. Lett. 99, 120601 (2007).

[37] G. Evenbly and G. Vidal, Tensor network renormaliza-
tion yields the multiscale entanglement renormalization
ansatz, Phys. Rev. Lett. 115, 200401 (2015).

[38] R. N. C. Pfeifer, G. Evenbly, and G. Vidal, Entanglement
renormalization, scale invariance, and quantum critical-
ity, Phys. Rev. A 79, 040301 (2009).

[39] M. Miyaji, T. Takayanagi, and K. Watanabe, From path
integrals to tensor networks for the AdS/CFT correspon-
dence, Phys. Rev. D 95, 066004 (2017).

[40] H.-Y. Hu, S.-H. Li, L. Wang, and Y.-Z. You, Machine
learning holographic mapping by neural network renor-
malization group, Phys. Rev. Research 2, 023369 (2020).
[41] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
et al., Pytorch: An imperative style, high-performance
deep learning library, Advances in neural information
processing systems 32, 8026 (2019).

[42] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,
M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G.
Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,
M. Wicke, Y. Yu, and X. Zheng, Tensorﬂow: A system
for large-scale machine learning, in 12th USENIX Sympo-
sium on Operating Systems Design and Implementation
(OSDI 16) (USENIX Association, Savannah, GA, 2016)
pp. 265–283.

[43] H.-J. Liao, J.-G. Liu, L. Wang, and T. Xiang, Diﬀer-
entiable programming tensor networks, Phys. Rev. X 9,
031041 (2019).

[44] M. Hauru, M. V. Damme, and J. Haegeman, Riemannian
optimization of isometric tensor networks, SciPost Phys.
10, 40 (2021).

[45] R. Pascanu, T. Mikolov, and Y. Bengio, On the diﬃculty
of training recurrent neural networks, in International
conference on machine learning (PMLR, 2013) pp. 1310–
1318.

[46] B. Polyak, Some methods of speeding up the convergence
of iteration methods, USSR Computational Mathematics
and Mathematical Physics 4, 1 (1964).

[47] T. Tieleman, G. Hinton, et al., Lecture 6.5-rmsprop: Di-
vide the gradient by a running average of its recent mag-
nitude, COURSERA: Neural networks for machine learn-
ing 4, 26 (2012).

[48] D. P. Bertsekas, Constrained optimization and Lagrange

multiplier methods (Academic press, 2014).

[49] H. D. Tagare, Notes on optimization on stiefel manifolds,
in Technical report, Technical report (Yale University,
2011).

[50] Z. Wen and W. Yin, A feasible method for optimization
with orthogonality constraints, Mathematical Program-
ming 142, 397 (2013).

[51] P.-A. Absil and J. Malick, Projection-like retractions on
matrix manifolds, SIAM Journal on Optimization 22, 135
(2012), https://doi.org/10.1137/100802529.

[52] T. Kaneko, S. Fiori, and T. Tanaka, Empirical arithmetic
averaging over the compact stiefel manifold, IEEE Trans-
actions on Signal Processing 61, 883 (2013).

13

[72] P. Hayden, S. Nezami, X.-L. Qi, N. Thomas, M. Walter,
and Z. Yang, Holographic duality from random tensor
networks, Journal of High Energy Physics 2016, 9 (2016),
arXiv:1601.01694 [hep-th].

[73] I. Cong, S. Choi, and M. D. Lukin, Quantum convolu-
tional neural networks, Nature Physics 15, 1273 (2019).
[74] C. Geng, Github: Diﬀerentiable isometric tensor network

(2021).

[75] M. E. Fisher, Magnetism in one-dimensional
heisenberg model
Journal

tems—the
American
of Physics
https://doi.org/10.1119/1.1970340.

inﬁnite
343

32,

for

sys-
spin,
(1964),

[53] Y. Nishimori and S. Akaho, Learning algorithms utilizing
quasi-geodesic ﬂows on the stiefel manifold, Neurocom-
puting 67, 106 (2005), geometrical Methods in Neural
Networks and Learning.

[54] B. Jiang and Y.-H. Dai, A framework of constraint pre-
serving update schemes for optimization on stiefel mani-
fold, Mathematical Programming 153, 535 (2015).
[55] X. Zhu, A riemannian conjugate gradient method for op-
timization on the stiefel manifold, Computational opti-
mization and Applications 67, 73 (2017).

[56] J. Li, L. Fuxin, and S. Todorovic, Eﬃcient Rieman-
nian Optimization on the Stiefel Manifold via the Cay-
ley Transform, arXiv e-prints , arXiv:2002.01113 (2020),
arXiv:2002.01113 [cs.LG].

[57] A. Pechen, D. Prokhorenko, R. Wu, and H. Rabitz,
Control landscapes for two-level open quantum systems,
Journal of Physics A: Mathematical and Theoretical 41,
045205 (2008).

[58] A. Oza, A. Pechen, J. Dominy, V. Beltrani, K. Moore,
and H. Rabitz, Optimization search eﬀort over the con-
trol landscapes for open quantum systems with kraus-
map evolution, Journal of Physics A: Mathematical and
Theoretical 42, 205305 (2009).

[59] J. Manton, Optimization algorithms exploiting unitary
constraints, IEEE Transactions on Signal Processing 50,
635 (2002).

[60] G. Evenbly, Hyperinvariant tensor networks and holog-

raphy, Phys. Rev. Lett. 119, 141602 (2017).

[61] M. Nozaki, S. Ryu, and T. Takayanagi, Holographic ge-
ometry of entanglement renormalization in quantum ﬁeld
theories, Journal of High Energy Physics 2012, 1 (2012).
[62] G. Evenbly and G. Vidal, Quantum criticality with
the multi-scale entanglement renormalization ansatz, in
Strongly Correlated Systems: Numerical Methods, edited
by A. Avella and F. Mancini (Springer Berlin Heidelberg,
Berlin, Heidelberg, 2013) pp. 99–130.

[63] P. Pfeuty, The one-dimensional ising model with a trans-

verse ﬁeld, Annals of Physics 57, 79 (1970).

[64] H. Robbins and S. Monro, A stochastic approximation
method, The Annals of Mathematical Statistics 22, 400
(1951).

[65] Z. Y. Xie, J. Chen, M. P. Qin, J. W. Zhu, L. P. Yang,
and T. Xiang, Coarse-graining renormalization by higher-
order singular value decomposition, Phys. Rev. B 86,
045139 (2012).

[66] S. Yang, Z.-C. Gu, and X.-G. Wen, Loop optimization for
tensor network renormalization, Phys. Rev. Lett. 118,
110504 (2017).

[67] M. Hauru, C. Delcamp, and S. Mizera, Renormalization
of tensor networks using graph-independent local trun-
cations, Phys. Rev. B 97, 045111 (2018).

[68] L. Onsager, Crystal statistics. i. a two-dimensional model
with an order-disorder transition, Phys. Rev. 65, 117
(1944).

[69] M. Hauru, G. Evenbly, W. W. Ho, D. Gaiotto, and G. Vi-
dal, Topological conformal defects with tensor networks,
Phys. Rev. B 94, 115125 (2016).

[70] M. Hashemizadeh, M. Liu, J. Miller, and G. Rabusseau,
Adaptive Learning of Tensor Network Structures, arXiv
e-prints , arXiv:2008.05437 (2020), arXiv:2008.05437
[cs.LG].

[71] Y.-Z. You, Z. Yang, and X.-L. Qi, Machine learning spa-
tial geometry from entanglement features, Phys. Rev. B
97, 045153 (2018), arXiv:1709.01223 [cond-mat.dis-nn].

14

Appendix A: Evenbly-Vidal optimization method

In order to optimize the whole tensor network, we need to explain how to optimize a single tensor. Here we brieﬂy
introduce the Evenbly-Vidal optimization method [31, 35]. Taking the isometry tensor w as example, we need to
minimize the energy, or to say loss function

We temperately regard w and w† as independent tensors and Yw is the environment of the tensor w shown in Fig. 20
(a). By applying the singular value decomposition(SVD) on the environment Yw = U SV †, the energy E(w) can be
minimized if we choose the new isometry tensor

E(w) = tr(wYw).

(A1)

w(cid:48) = −V U †.

(A2)

Then a single step of updating is to replace the old tensor w by the new tensor w(cid:48).

The remaining quest is to determine the environment of the tensor. We can automatically obtain the environment
tensors by means of diﬀerentiable programming discussed in the main text, or drawing and contracting the environment
graphs manually. Here we show the environment graphs for the ternary MERA and TNR used in this paper.

FIG. 20. The environment tensors of (a) the isometry w and (b) the disentangler u for MERA, which are the summation of
six parts and three parts correspondingly. The environment tensors of vL, vR and u in TNR are shown in (c).

Appendix B: Reducing computation expense of Cayley method

We can reduce the computation of inverting a n × n matrix to inverting a 2p × 2p (n > 2p) matrix by Sherman-

Morrison-Woodbury formula

(cid:0)B + αU V T (cid:1)−1

= B−1 − αB−1U (cid:0)I + αV T B−1U (cid:1)−1

V T B−1.

(B1)

Deﬁne the concatenated matrices U = [PX ¯X, X] and V = [X, −PX ¯X] where the square brackets here refer to the
matrix concatenation and PX = I − 1
2 XX T . Then we have A = U V T and the matrix inverse term in Eq. (15) becomes

(cid:16)

I +

η
2

A

(cid:17)−1

(cid:16)

=

I +

η
2

U V T (cid:17)−1
(cid:16)
η
2

I +

= I −

η
2

U

V T U

(cid:17)−1

V T .

And the optimization method of Cayley transform becomes

Xt+1 = Xt − ηUt

(cid:16)

I +

η
2

V T
t Ut

(cid:17)−1

V T
t Xt.

(B2)

(B3)

(B4)

There is an alternative way to accelerate the computation of Cayley transform by the iterative estimation of Eq. (15)
2 ηA(X + Y (η)) for Y (η) is exactly Eq. (15).

[50, 56]. Note that the ﬁxed point solution of the equation Y (η) = X − 1

wϒuϒ(a)(b)===ϒuϒLvϒRv(c)†B†B†BBy iterating several steps of the equation

Xt+1,0 = Xt − ηGX ,

Xt+1,k+1 = Xt −

1
2

ηA(Xt + Xt+1,k),

we can update the tensors in the Cayley transform scheme to avoid computing inverse of matrices.

15

(B5)

(B6)

Appendix C: Training details of MERA

Resetting mechanism: In the practice of Ising model optimization of MERA, we ﬁnd that if after the ﬁrst several
hundred iterations the energy error is still larger than a threshold around 10−3, it will be highly possible to be stuck
into local minima with energy errors 10−3 ∼ 10−4. In order to reduce the chance of being stuck into local minima
and improve the success rate of optimization, we used a resetting mechanism. To be speciﬁc, we ﬁrst optimize the
MERA for 700 iterations and check whether the energy error is larger than a threshold value 1.5 × 10−3. If the energy
error is larger than the threshold value, we reset the optimization to the beginning, meaning that all the trainable
parameters in the network are set to there initial values as well as the associated parameters of optimizers.

We note that the Evenbly-Vidal method in our paper has a little diﬀerence from Evenbly and Vidal’s original
In the original algorithm, the update is timely, meaning that once a tensor is updated the next
algorithm [31].
derivative or environment tensor is computed using this updated tensor and so on. within the auto-diﬀerentiation
framework, it’s hard to update tensors timely. That is, we compute all the derivative tensors at once. And then we
use these derivative tensors to update all the tensors to be optimized.

Lifting bond dimension trick: In our MERA optimization we use a gradually lifting bond dimension trick
which can speed up convergence and improve the accuracy. To be speciﬁc, we start the computation with a small
bond dimension χ = 4 in ﬁrst 200 iterations. Then we increase the bond dimension to 6, 7, 8, 9, 10, 12 sequentially by
enlarging the tensors’ size and padding zero values. For the result in Fig. 8, the lifting bond dimension occurs at the
200, 700, 2700, 5700, 8700, 11700, 15700 iterations.

Appendix D: methods comparison of MERA for other models

In this section we compare the Evenbly-Vidal method and random mixed method for critical Heisenberg XY model

and Heisenberg XXZ model [75]

HXY =

HXXZ =

(cid:88)

(cid:16)

r
(cid:88)

(cid:16)

r

x σ[r+1]
σ[r]
x

+ λσ[r]

y σ[r+1]
y

(cid:17)

,

x σ[r+1]
σ[r]
x

+ σ[r]

y σ[r+1]
y

+ λσ[r]

z σ[r+1]
z

(cid:17)

(D1)

(D2)

with λ = 1.0.

The results are shown in Fig. 21. We ﬁnd that for Heisenberg XY and XXZ models the random mixed method can

also improve the optimization performance.

Appendix E: Computation graph of TNR

In this section we explicitly describe the computation graph of TNR used in our paper. The computation graph

for the TNR is illustrated in Fig. 16 in the main text.

The computation graph starts from the inverse temperature β because we would like to investigate the partition
function with diﬀerent beta. Using Eq. (33) we can represent the partition function as the contraction of As. In order
to prevent the data explosion, the A is renormalized to A0 by dividing a constant number Anorm,0 which is taken as
the L2 norm of A here.

The computation graph of TNR for each layer (gray dashed frame in Fig. 16) involve three parts. We take the ﬁrst

layer for example:

(1) Forward propagation: Taking a 2 × 2 sub-network of A0 we make the approximation by inserting projection
operators as in Fig. 12 to obtain the new sub-network Anew,0. Taking a 2 × 2 sub-network of A0 we make

16

FIG. 21. Energy errors with Evenbly-Vidal method and random mixed method for critical Heisenberg XY (left) and XXZ
(right) models. The setting of computation is the same as the Ising model in the main text.

the approximation by inserting projection operators as in Fig. 12 to obtain the new sub-network Anew,0. Then
compare the new sub-network Anew,0 with the original 2 × 2 sub-network of A0 by making contraction of them,
we get the approximation error (cid:107)δ(cid:107) as the loss function. The forward propagation is shown as gray arrows in
Fig. 16.

(2) Backward propagation: From the loss function, the derivative tensors of parameters vL, vR and u are
automatically computed. With the derivative tensors the parameters can be updated by various methods.
Iterating the forward and backward propagation the parameters are optimized. The backward propagation is
shown as red arrows in Fig. 16.

(3) Tensors renormalization: Once the parameters have been optimized, we can make the tensors renormalization

as described in the main text. The tensors renormalization is shown as brown arrows in Fig. 16.

By repeating the coarse-graining procedure layer by layer, we ﬁnally arrive the top layer and obtain the single
tensor AT in our case. Applying the tensor trace on AT and collecting Anorms of every layers, we obtain the partition
function Z and its logarithm ln Z.

Indeed, the brown arrows in Fig. 16 also indicate the forward propagation from β to ln Z. We can compute the
ﬁrst-order or even second derivatives of ln Z with respect to β corresponding to energy density and speciﬁc heat in
further works.

Appendix F: Soft-constraint optimization

The soft-constraint optimization is to relax the constraints and allow the tensors w and u to be away from isometric,

which is known as the method of Lagrange multipliers [48].

Deﬁne the elementwise average error

Terr =

1
3

(cid:0)(cid:10)I − w†w(cid:11) + (cid:10)I − u†u(cid:11) + (cid:10)I − uu†(cid:11)(cid:1) .

(F1)

Here the angle bracket refers to the elementwise average of a matrix. The value of Terr measures how the the tensors
w and u away from the isometric constraints. Adding Terr with the energy E we obtain the modiﬁed loss function

Ll = E + λlTerr,

(F2)

where λl is the Lagrange multiplier. Now the constraints are absorbed into the modiﬁed loss function and we can
optimize the modiﬁed loss function Ll using the usual optimization methods.

The hyperparameter λl should be tuned manually and the optimization is sensitive to the hyperparameter λl. But
the tuning of hyperparameter λl is tricky and elusive depending on the systems. We introduce a dynamic tuning
process of λl here. If λl is too large, the optimization will mainly focus on the constraints and the energy decreasing
will become diﬃcult, while if λl is too small, the optimization will almost ignore the constraints and the energy will
keep deceasing. We use a dynamic tuning strategy:

101102103104iterations104103102101100energy errors of HeisenbergXY modelEvenbly-VidalRandomMix101102103104iterations103102101100energy errors of HeisenbergXXZ modelEvenbly-VidalRandomMix1. Starting with a small λl. After a few iterations switch to a large λl.

2. When the tensors error Terr is lower than a threshold, decrease λl and the threshold exponentially until to a

lower bound.

3. When the tensors error Terr is higher than the threshold, increase λl exponentially until to a higher bound.

By this tuning strategy, the hyperparameter λl is tuned automatically depending on the tensors error Terr such

that the optimization keeps a relative balance between the energy E and the tensor error Terr.

17

FIG. 22. The optimization curves of the soft-constraint method both for the energy error and the orthogonality error. The
bond dimension χ = 8. The number of transitional layers is 4.

Fig. 22 shows the optimization curves of the soft-constraint method both for the energy error and the orthogonality
error by the Adam optimizer. The inner ﬁgure shows how the hyperparameter λl changes during the optimization
process. In this result, the energy error is about ∼ 10−5, which is larger than what we obtained by Evenbly-Vidal
and gradient-based methods in the main text.

