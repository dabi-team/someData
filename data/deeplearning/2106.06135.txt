DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Daochen Zha 1 Jingru Xie 2 Wenye Ma 2 Sheng Zhang 3 Xiangru Lian 2 Xia Hu 1 Ji Liu 2

Abstract

Games are abstractions of the real world, where
artiﬁcial agents learn to compete and cooperate
with other agents. While signiﬁcant achievements
have been made in various perfect- and imperfect-
information games, DouDizhu (a.k.a. Fighting
the Landlord), a three-player card game, is still
unsolved. DouDizhu is a very challenging do-
main with competition, collaboration, imperfect
information, large state space, and particularly
a massive set of possible actions where the le-
gal actions vary signiﬁcantly from turn to turn.
Unfortunately, modern reinforcement learning al-
gorithms mainly focus on simple and small action
spaces, and not surprisingly, are shown not to
make satisfactory progress in DouDizhu. In this
work, we propose a conceptually simple yet ef-
fective DouDizhu AI system, namely DouZero,
which enhances traditional Monte-Carlo methods
with deep neural networks, action encoding, and
parallel actors. Starting from scratch in a single
server with four GPUs, DouZero outperformed
all the existing DouDizhu AI programs in days of
training and was ranked the ﬁrst in the Botzone
leaderboard among 344 AI agents. Through build-
ing DouZero, we show that classic Monte-Carlo
methods can be made to deliver strong results in
a hard domain with a complex action space. The
code and an online demo are released1 with the
hope that this insight could motivate future work.

1
2
0
2

n
u
J

1
1

]
I

A
.
s
c
[

1
v
5
3
1
6
0
.
6
0
1
2
:
v
i
X
r
a

1. Introduction

Games often serve as benchmarks of AI since they are ab-
stractions of many real-world problems. Signiﬁcant achieve-
ments have been made in perfect-information games. For

1Department of Computer Science and Engineering, Texas
3Georgia In-
Correspondence to: Daochen Zha

A&M University 2AI Platform, Kwai Inc.
stitute of Technology.
<daochen.zha@tamu.edu>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

1https://github.com/kwai/DouZero

example, AlphaGo (Silver et al., 2016), AlphaZero (Sil-
ver et al., 2018) and MuZero (Schrittwieser et al., 2020)
have established state-of-the-art performance on Go game.
Recent research has evolved to more challenging imperfect-
information games, where the agents compete or coop-
erate with others in a partially observable environment.
Encouraging progress has been made from two-player
games, such as simple Leduc Hold’em and limit/no-limit
Texas Hold’em (Zinkevich et al., 2008; Heinrich & Sil-
ver, 2016; Moravˇc´ık et al., 2017; Brown & Sandholm,
2018), to multi-player games, such as multi-player Texas
hold’em (Brown & Sandholm, 2019b), Starcraft (Vinyals
et al., 2019), DOTA (Berner et al., 2019), Hanabi (Lerer
et al., 2020), Mahjong (Li et al., 2020a), Honor of Kings (Ye
et al., 2020b;a), and No-Press Diplomacy (Gray et al., 2020).

This work aims at building AI programs for DouDizhu2
(a.k.a. Fighting the Landlord), the most popular card game
in China with hundreds of millions of daily active players.
DouDizhu has two interesting properties that pose great
challenges for AI systems. First, the players in DouDizhu
need to both compete and cooperate with others in a par-
tially observable environment with limited communication.
Speciﬁcally, two Peasants players will play as a team to
ﬁght against the Landlord player. Popular algorithms for
poker games, such as Counterfactual Regret Minimization
(CFR) (Zinkevich et al., 2008)) and its variants, are often
not sound in this complex three-player setting. Second,
DouDizhu has a large number of information sets with a
very large average size and has a very complex and large
action space of up to 104 possible actions due to combina-
tions of cards (Zha et al., 2019a). Unlike Texas Hold’em,
the actions in DouDizhu can not be easily abstracted, which
makes search computationally expensive and commonly
used reinforcement learning algorithms less effective. Deep
Q-Learning (DQN) (Mnih et al., 2015) is problematic in very
large action space due to overestimating issue (Zahavy et al.,
2018); policy gradient methods, such as A3C (Mnih et al.,
2016), cannot leverage the action features in DouDizhu, and
thus cannot generalize over unseen actions as naturally as
DQN (Dulac-Arnold et al., 2015). Not surprisingly, previous
work shows that DQN and A3C can not make satisfactory
progress in DouDizhu. In (You et al., 2019), DQN and A3C
are shown to have less than 20% winning percentage against

2https://en.wikipedia.org/wiki/Dou_dizhu

 
 
 
 
 
 
DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

simple rule-based agents even with twenty days of training;
the DQN in (Zha et al., 2019a) is only slightly better than
random agents that sample legal moves uniformly.

Some previous efforts have been made to build DouDizhu
AI by combining human heuristics with learning and search.
Combination Q-Network (CQN) (You et al., 2019) proposes
to reduce the action space by decoupling the actions into
decomposition selection and ﬁnal move selection. However,
decomposition relies on human heuristics and is extremely
slow. In practice, CQN can not even beat simple heuristic
rules after twenty days of training. DeltaDou (Jiang et al.,
2019) is the ﬁrst AI program that reaches human-level per-
formance compared with top human players. It enables an
AlphaZero-like algorithm by using Bayesian methods to
infer hidden information and sampling the other players’
actions based on their own policy networks. To abstract the
action space, DeltaDou pre-trains a kicker network based
on heuristic rules. However, the kicker plays an impor-
tant role in DouDizhu and can not be easily abstracted. A
bad selection of the kicker may directly result in losing a
game since it may break some other card categories, e.g., a
Chain of Solo. Moreover, the Bayesian inference and the
search are computationally expensive. It takes more than
two months to train DeltaDou even when initializing the net-
works with supervised regression to heuristics (Jiang et al.,
2019). Therefore, the existing DouDizhu AI programs are
computationally expensive and could be sub-optimal since
they highly rely on abstractions with human knowledge.

In this work, we present DouZero, a conceptually sim-
ple yet effective AI system for DouDizhu without the ab-
straction of the state/action space or any human knowledge.
DouZero enhances traditional Monte-Carlo methods (Sut-
ton & Barto, 2018) with deep neural networks, action en-
coding, and parallel actors. DouZero has two desirable
properties. First, unlike DQN, it is not susceptible to over-
estimation bias. Second, by encoding the actions into card
matrices, it can naturally generalize over the actions that are
not frequently seen throughout the training process. Both
of these two properties are crucial in dealing with the huge
and complex action space of DouDizhu. Unlike many tree
search algorithms, DouZero is based on sampling, which
allows us to use complex neural architectures and generate
much more data per second, given the same computational
resources. Unlike many prior poker AI studies that rely on
domain-speciﬁc abstractions, DouZero does not require
any domain knowledge or knowledge of the underlying dy-
namics. Trained from scratch in a single server with only
48 cores and four 1080Ti GPUs, DouZero outperforms
CQN and the heuristic rules in half a day, beats our internal
supervised agents in two days, and surpasses DeltaDou in
ten days. Extensive evaluations suggest that DouZero is
the strongest DouDizhu AI system up to date.

Figure 1. A hand and its corresponding legal moves.

Through building DouZero system, we demonstrate that
classical Monte-Carlo methods can be made to deliver
strong results in large-scale and complex card games that
need to reason about both competing and cooperation over
huge state and action spaces. We note that some work also
discovers that Monte-Carlo methods can achieve competi-
tive performance (Mania et al., 2018; Zha et al., 2021a) and
help in sparse rewards settings (Guo et al., 2018; Zha et al.,
2021b). Unlike these studies that focus on simple and small
environments, we demonstrate the strong performance of
Monte-Carlo methods on a large-scale card game. With
the hope that this insight could facilitate future research
on tackling multi-agent learning, sparse reward, complex
action spaces, and imperfect information, we have released
our environment and the training code. Unlike many Poker
AI systems that require thousands of CPUs in training, e.g.,
DeepStack (Moravˇc´ık et al., 2017) and Libratus (Brown &
Sandholm, 2018), DouZero enables a reasonable exper-
imental pipeline, which only requires days of training on
a single GPU server that is affordable for most research
labs. We hope that it could motivate future research in this
domain and serve as a strong baseline.

2. Background of DouDizhu
DouDizhu is a popular three-player card game that is easy
to learn but difﬁcult to master. It has attracted hundreds of
millions of players in China, with many tournaments held
every year. It is a shedding-type game where the player’s
objective is to empty one’s hand of all cards before other
players. Two of the Peasants players play as a team to ﬁght
against the other Landlord player. The Peasants win if either
of the Peasants players is the ﬁrst to have no cards left. Each
game has a bidding phase, where the players bid for the
Landlord based on the strengths of the hand cards, and a
card-playing phase, where the players play cards in turn.
We provide a detailed introduction in Appendix A.

DouDizhu is still an unsolved benchmark for multi-agent
reinforcement learning (Zha et al., 2019a; Terry et al., 2020).
Two interesting properties make DouDizhu particularly chal-
lenging to solve. First, the Peasants need to cooperate in
ﬁghting against the Landlord. For example, Figure 10 shows
a typical situation where the bottom Peasant can choose to
play a small Solo to help the Peasant on the right-hand side
to win. Second, DouDizhu has a complex and large action

391 Legal Combinations...DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Figure 2. Cards for both states and actions are encoded into a 4×15
one-hot matrix, where columns correspond to the 13 ranks and
the jokers, and each row corresponds to the number of cards of a
speciﬁc rank or joker. More examples are provided in Appendix B.

space due to the combination of cards. There are 27, 472
possible combinations, where different subsets of these com-
binations will be legal for different hands. Figure 1 shows
an example of the hand, which has 391 legal combinations,
including Solo, Pair, Trio, Bomb, Plane, Quad, etc. The
action space can not be easily abstracted since improperly
playing a card may break other categories and directly result
in losing a game. Thus, building DouDizhu AI is challeng-
ing since the players in DouDizhu need to reason about both
competing and cooperation over a huge action space.

3. Deep Monte-Carlo

In this section, we revisit Monte-Carlo (MC) methods and
introduce Deep Monte-Carlo (DMC), which generalizes
MC with deep neural networks for function approximation.
Then we discuss and compare DMC with policy gradient
methods (e.g., A3C) and DQN, which are shown to fail in
DouDizhu (You et al., 2019; Zha et al., 2019a).

3.1. Monte-Carlo Methods with Deep Neural Networks

Monte-Carlo (MC) methods are traditional reinforcement
learning algorithms based on averaging sample returns (Sut-
ton & Barto, 2018). MC methods are designed for episodic
tasks, where experiences can be divided into episodes and
all the episodes eventually terminate. To optimize a policy
π, every-visit MC can be used to estimate Q-table Q(s, a)
by iteratively executing the following procedure:

1. Generate an episode using π.
2. For each s, a appeared in the episode, calculate and
update Q(s, a) with the return averaged over all the
samples concerning s, a.

3. For each s in the episode, π(s) ← arg maxa Q(s, a).

The average return in Step 2 is usually obtained by the dis-
counted cumulative reward. Different from Q-learning that
relies on bootstrapping, MC methods directly approximate

Figure 3. The Q-network of DouZero consists of an LSTM to
encode historical moves and six layers of MLP with hidden dimen-
sion of 512. The network predicts a value for a given state-action
pair based on the concatenated representation of action and state.
More details are provided in Appendix C.1.

the target Q-value. In step 1, we can use epsilon-greedy
to balance exploration and exploitation. The above proce-
dure can be naturally combined with deep neural networks,
which leads to Deep Monte-Carlo (DMC). Speciﬁcally, we
can replace the Q-table with a neural network and use mean-
square-error (MSE) to update the Q-network in Step 2.

While MC methods are criticized not to be able to deal with
incomplete episodes and believed to be inefﬁcient due to the
high variance (Sutton & Barto, 2018), DMC is very suitable
for DouDizhu. First, DouDizhu is an episodic task so that
we do not need to handle incomplete episodes. Second,
DMC can be easily parallelized to efﬁciently generate many
samples per second to alleviate the high variance issue.

3.2. Comparison with Policy Gradient Methods

Policy gradients methods, such as REINFORCE (Williams,
1992), A3C (Mnih et al., 2016), PPO (Schulman et al., 2017),
and IMPALA (Espeholt et al., 2018), are very popular for re-
inforcement learning. They target modeling and optimizing
the policy directly with gradient descent. In policy gradient
methods, we often use a classiﬁer-like function approxi-
mator, where the output scales linearly with the number
of actions. While policy gradients methods work well in
large action space, they cannot use the action features to
reason about previously unseen actions (Dulac-Arnold et al.,
2015). In practice, the actions in DouDizhu can be naturally
encoded into card matrices, which are crucial for reason-
ing. For example, if the agent is rewarded by the action
3KKK because it chooses a nice kicker, it could also gener-
alize this knowledge to unseen actions in the future, such
as 3JJJ. This property is crucial in dealing with very large
action spaces and accelerating the learning since many of
the actions are not frequently seen in the simulated data.

110010001000100010000000000011110000000000000000111010001000ActionEncodeState000100000000000000000000000000000111000000000000000000000000000100000000000000000000000000000111000000000000000000010001000100000000000000000000000000000111000000000000000000010001000100000000000000000000000000000111000000000000000000010001000100000000000000000000000000000111000000000000000000010001EncodeHistorical MovesLSTMLSTMLSTMEncodehLSTMMLPQRepeat6xDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

DMC can naturally leverage the action features to generalize
over unseen actions by taking as input the action features.
While it might have high execution complexity if the action
size is large, in most states of DouDizhu, only a subset of
the actions is legal, so that we do not need to iterate over all
the actions. Thus, DMC is overall an efﬁcient algorithm for
DouDizhu. While it is possible to introduce action features
into an actor-critic framework (e.g., by using a Q-network
as the critic), the classiﬁer-like actor will still suffer from
the large action space. Our preliminary experiments conﬁrm
that this strategy is not very effective (see Figure 7).

3.3. Comparison with Deep Q-Learning

The most popular value-based algorithm is Deep Q-
Learning (DQN) (Mnih et al., 2015), which is a bootstrap-
ping method that updates the Q-value based on the Q-values
in the next step. While both DMC and DQN approximate
the Q-values, DMC has several advantages in DouDizhu.

First, the overestimation bias caused by approximating the
maximum action value in DQN is difﬁcult to control when
using function approximation (Thrun & Schwartz, 1993;
Hasselt, 2010) and becomes more pronounced with very
large action space (Zahavy et al., 2018). While some tech-
niques, such as double Q-learning (van Hasselt et al., 2016)
and experience replay (Lin, 1992), might alleviate this issue,
we ﬁnd in practice that DQN is very unstable and often
diverges in DouDizhu. Whereas, Monte-Carlo estimation
is not susceptible to bias since it directly approximates the
true values without bootstrapping (Sutton & Barto, 2018).

Second, DouDizhu is a task with long horizons and sparse
reward, i.e., the agent will need to go though a long chain of
states without feedback, and the only time a nonzero reward
is incurred is at the end of a game. This may slow down the
convergence of Q-learning because estimating the Q-value
in the current state needs to wait until the value in the next
state gets close to its true value (Szepesv´ari, 2009; Beleznay
et al., 1999). Unlike DQN, the convergence of Monte-Carlo
estimation is not impacted by the episode length since it
directly approximates the true target values.

Third, it is inconvenient to efﬁciently implement DQN in
DouDizhu due to the large and variable action space. Specif-
ically, the max operation of DQN in every update step will
cause high computation cost since it requires iterating across
all the legal actions on a very costly deep Q-network. More-
over, the legal moves differ in different states, which makes
it inconvenient to do batch learning. As a result, we ﬁnd
DQN is too slow in terms of wall-clock time. While Monte-
Carlo methods might suffer from high variance (Sutton &
Barto, 2018), which means it might require more samples to
converge, it can be easily parallelized to generate thousands
of samples per second to alleviate the high variance issue
and accelerate training. We ﬁnd that the high variance of

DMC is greatly outweighed by the scalability it provides,
and DMC is very efﬁcient in wall-clock time.

4. DouZero System
In this section, we introduce DouZero system by ﬁrst de-
scribing the state/action representations and neural architec-
ture and then elaborating on how we parallelize DMC with
multiple processes to stabilize and accelerate training.

4.1. Card Representation and Neural Architecture

We encode each card combination with a one-hot 4 × 15
matrix (Figure 2). Since suits are irrelevant in DouDizhu,
we use each row to represent the number of cards of a
speciﬁc rank or joker. Figure 3 shows the architecture of the
Q-network. For the state, we extract several card matrices to
represent the hand cards, the union of the other players’ hand
cards and the most recent moves, and some one-hot vectors
to represent the number of cards of the other players and
the number of bombs played so far. Similarly, we use one
card matrix to encode the action. For the neural architecture,
LSTM is used to encode historical moves, and the output is
concatenated with the other state/action features. Finally, we
use six layers of MLP with a hidden size of 512 to produce
Q-values. We provide more details in Appendix C.1.

4.2. Parallel Actors

We denote Landlord as L, the player that moves before the
Landlord as U, and the player that moves after the Landlord
as D. We parallelize DMC with multiple actor processes
and one learner process, summarized in Algorithm 1 and Al-
gorithm 2, respectively. The learner maintains three global
Q-networks for the three positions and updates the networks
with MSE loss to approximate the target values based on
the data provided by the actor processes. Each actor main-
tains three local Q-networks, which are synchronized with
the global networks periodically. The actor will repeat-
edly sample trajectories from the game engine and calculate
cumulative reward for each state-action pair. The commu-
nication of learner and actors are implemented with three
shared buffers. Each buffer is divided into several entries,
where each entry consists of several data instances.

5. Experiments

The experiments are designed to answer the following re-
search questions. RQ1: How does DouZero compare with
existing DouDizhu programs, such as rule-based strategies,
supervised learning, RL-based methods, and MCTS-based
solutions (Section 5.2)? RQ2: How will DouZero perform
if we consider bidding phase (Section 5.3)? RQ3: How efﬁ-
cient is the training of DouZero (Section 5.4)? RQ4: How
does DouZero compare with bootstrapping and actor critic

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Algorithm 1 Actor Process of DouZero
1: Input: Shared buffers BL, BU and BD with B entries
and size S for each entry, exploration hyperparameter (cid:15),
discount factor γ

2: Initialize local Q-networks QL, QU and QD, and local

buffers DL, DU and DD
3: for iteration = 1, 2, ... do
4:
5:
6:

Synchronize QL, QU and QD with the learner process
(cid:46) Generate an episode
for t = 1, 2, ... T do

at ←

Q ← one of QL, QU, QD based on position

(cid:26) arg maxa Q(st, a) with prob (1 − (cid:15))
random action with prob (cid:15)
Perform at, observe st+1 and reward rt
Store {st, at, rt} to DL, DU , or DD accordingly

end for
for t = T-1, T-2, ... 1 do (cid:46) Obtain cumulative reward
rt ← rt + γrt+1 and update rt in DL, DU , or DD

end for
for p ∈ {L, U, D} do (cid:46) Optimized by multi-thread

if Dp.length ≥ L then

Request and wait for an empty entry in Bp
Move {st, at, rt} of size L from Dp to Bp

7:

8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20: end for

end if
end for

methods (Section 5.5)? RQ5: Does the learned card playing
strategies of DouZero align with human knowledge (Sec-
tion 5.6)? RQ6: Is DouZero computationally efﬁcient in
inference compared with existing programs (Section 5.7)?
RQ7: Can the two Peasants of DouZero learn to cooperate
with each other (Section 5.8)?

5.1. Experimental Setup

A commonly used measure of strategy strength in poker
games is exploitability (Johanson et al., 2011). However,
in DouDizhu, calculating exploitability itself is intractable
since DouDizhu has huge state/action spaces, and there are
three players. To evaluate the performance, following (Jiang
et al., 2019), we launch tournaments that include the two
opponent sides of Landlord and Peasants. We reduce the
variance by playing each deck twice. Speciﬁcally, for two
competing algorithms A and B, they will ﬁrst play as Land-
lord and Peasants positions, respectively, for a given deck.
Then they switch sides, i.e., A takes Peasants position, and
B takes Landlord position, and play the same deck again.
To simulate the real environment, in Section 5.3, we further
train a bidding network with supervised learning, and the
agents will bid the Landlord in each game based on the
strengths of the hand cards (more details in Appendix C.2).
We consider the following competing algorithms.

Algorithm 2 Learner Process of DouZero
1: Input: Shared buffers BL, BU and BD with B entries
and size S for each entry, batch size M , learning rate ψ

D

L, Qg

2: Initialize global Q-networks Qg
U and Qg
3: for iteration = 1, 2, ... until convergence do
4:
5:
6:

for p ∈ {L, U, D} do (cid:46) Optimized by multi-thread
if the number of full entries in Bp ≥ M then

Sample a batch of {st, at, rt} with M × S in-
stances from Bp and free the entries
Update Qg

p with MSE loss and learning rate ψ

7:
8:
9:
10: end for

end if
end for

• DeltaDou: A strong AI program which uses Bayesian
methods to infer hidden information and searches the
moves with MCTS (Jiang et al., 2019). We use the code
and the pre-trained model provided by the authors. The
model is trained for two months and is shown to have
on par performance with top human players.

• CQN: Combinational Q-Learning (You et al., 2019)
is a program based on card decomposition and Deep
Q-Learning. We use the open-sourced code and the
pre-trained model provided by the authors3.

• SL: A supervised learning baseline. We internally col-
lect 226, 230 human expert matches from the players
of the highest level in league in our DouDizhu game
mobile app. Then we use the same state representation
and neural architecture as DouZero to train super-
vised agents with 49, 990, 075 samples generated from
these data. See Appendix C.2 for more details.

• Rule-Based Programs: We collect some open-
sourced heuristic-based programs, including RHCP4,
an improved version called RHCP-v25, and the rule
model in RLCard package6 (Zha et al., 2019a). In ad-
dition, we consider a Random program that samples
legal moves uniformly.

Metrics. Following (Jiang et al., 2019), given an algorithm
A and an opponent B, we use two metrics to compare the
performance of A and B:

• WP (Winning Percentage): The number of the games

won by A divided by the total number of games.

3https://github.com/qq456cvb/doudizhu-C
4https://blog.csdn.net/sm9sun/article/

details/70787814

5https://github.com/deecamp2019-group20/

RuleBasedModelV2

6https://github.com/datamllab/rlcard

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Table 1. Performance of DouZero against existing DouDizhu programs by playing 10,000 randomly sampled decks. Algorithm A
outperforms B if WP is larger than 0.5 or ADP is larger than 0 (highlighted in boldface). The algorithms are ranked according to the
number of the other algorithms that they beat. The full results of each position are provided in Appendix D.1.

Rank A

B DouZero

DeltaDou

SL

.
RHCP-v2

RHCP

RLCard

CQN

Random

WP ADP WP ADP WP ADP WP ADP WP ADP WP ADP WP ADP WP ADP

1
2
3
4
5
6
7
8

DouZero
DeltaDou
SL
RHCP-v2
RHCP
RLCard
CQN
Random

-

-

-

-

0.586 0.258 0.659 0.700 0.757 1.662 0.764 1.671 0.889 2.288 0.810 1.685 0.989 3.036
0.617 0.653 0.745 1.500 0.747 1.514 0.876 2.459 0.784 1.534 0.992 3.099
0.611 0.853 0.632 0.886 0.813 1.821 0.694 1.037 0.976 2.721
0.515 0.052 0.692 1.121 0.621 0.714 0.967 2.631
0.682 1.259 0.603 0.248 0.941 2.720
0.522 0.168 0.943 2.471
0.889 1.912

0.414 -0.258
0.341 -0.700 0.396 -0.653
0.243 -1.662 0.257 -1.500 0.389 -0.853
0.236 -1.671 0.253 -1.514 0.369 -0.886 0.485 -0.052
0.111 -2.288 0.124 -2.459 0.187 -1.821 0.309 -1.121 0.318 -1.259
0.190 -1.685 0.216 -1.534 0.306 -1.037 0.379 -0.714 0.397 -0.248 0.478 -0.168
0.011 -3.036 0.008 -3.099 0.024 -2.721 0.033 -2.631 0.059 -2.720 0.057 -2.471 0.111 -1.912

-

-

-

-

-

-

-

-

-

-

-

-

• ADP (Average Difference in Points): The average dif-
ference of points scored per game between A and B.
The base point is 1. Each bomb will double the score.

We ﬁnd in practice that these two metrics encourage dif-
ferent styles of strategies. For example, if using ADP as
reward, the agent tends to be very cautious about playing
bombs since playing a bomb is risky and may lead to larger
ADP loss. In contrast, with WP as objective, the agent tends
to aggressively play bombs even if it will lose because a
bomb will not affect WP. We observe that the agent trained
with ADP performs slightly better than the agent trained
with WP in terms of ADP and vice versa. In what follows,
we train and report the results of two DouZero agents with
ADP and WP as objectives, respectively7. More discussions
of the two objectives are provided in Appendix D.2.

We ﬁrst launch a preliminary tournament by letting each pair
of the algorithms play 10,000 decks. We then compute the
Elo rating score for the top 3 algorithms for a more reliable
comparison, i.e., DouZero, DeltaDou, and SL, by playing
100,000 decks. An algorithm wins a deck if it achieves
higher WP or ADP summed over the two games played on
this deck. We repeat this process ﬁve times with different
randomly sampled decks and report the mean and standard
deviation of the Elo scores. For the evaluation with the
bidding phase, each deck is played six times with different
perturbations of DouZero, DeltaDou, and SL in different
positions. We report the result with 100,000 decks.

Implementation Details. We run all the experiments on a
single server with 48 processors of Intel(R) Xeon(R) Silver
4214R CPU @ 2.40GHz and four 1080 Ti GPUs. We use
45 actors, which are allocated across three GPUs. We run a
learner in the remaining GPU to train the Q-networks. Our
implementation is based on TorchBeast framework (K¨uttler
et al., 2019). The detailed training curves are provided in

7For WP, we give a +1 or -1 reward to the ﬁnal timestep based
on whether the agent wins or loses a game. For ADP, we directly
use ADP as the rewards. DeltaDou and CQN were trained with
ADP and WP as objectives, respectively.

Figure 4. Left: Elo rating scores of DouZero, DeltaDou, and SL
by playing 100,000 randomly sampled decks. We report the mean
and standard deviation across 5 different random seeds. Right: Elo
rating scores on Botzone, an online platform for DouDizhu com-
petition. DouZero ranked the ﬁrst among the 344 bots, achieving
an Elo rating score of 1625.11 as of October 30, 2020.

Appendix D.5. Each shared buffer has B = 50 entries with
size S = 100, batch size M = 32, and (cid:15) = 0.01. We
set discount factor γ = 1 since DouDizhu only has a non-
zero reward in the last timestep and early moves are very
important. We use ReLU as the activation function for each
layer of MLP. We adopt RMSprop optimizer with a learning
rate ψ = 0.0001, smoothing constant 0.99 and (cid:15) = 10−5.
We train DouZero for 30 days.

5.2. Performance against Existing Programs

To answer RQ1, we compare DouZero with the baselines
ofﬂine and report its result on Botzone (Zhou et al., 2018),
an online platform for DouDizhu competition (more details
are provided in Appendix E).

Table 1 summarizes the WP and ADP of head-to-head com-
pletions among DouZero and all the baselines. We make
three observationss. First, DouZero dominates all the
rule-based strategies and supervised learning, which demon-
strates the effectiveness of adopting reinforcement learning
in DouDizhu. Second, DouZero achieves signiﬁcantly bet-
ter performance than CQN. Recall that CQN similarly trains
the Q-networks with action decomposition and DQN. The
superiority of DouZero suggests that DMC is indeed an
effective way to train the Q-networks in DouDizhu. Third,
DouZero outperforms DeltaDou, the strongest DouDizhu

WPADP800900100011001200EloratingscoreDouZeroDeltaDouSL0%20%40%60%80%100%0250500750100012501500EloratingscoreDouZero(1625.11)DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

(a) WP against SL

(b) ADP against SL

(c) WP against DeltaDou

(d) ADP against DeltaDou

Figure 5. WP and ADP of DouZero against SL and DeltaDou w.r.t. the number of training days. DouZero outperforms SL with 2 days
of training, i.e., the overall WP is larger than the threshold of 0.5 and the overall ADP is larger than the threshold of 0, and surpasses
DeltaDou within 10 days, using a single server with four 1080 Ti GPUs and 48 processors. We provide the full curves for each position
and the curves w.r.t. timesteps in Appendix D.3.

Table 2. Comparison of DouZero, DeltaDou and SL with the
bidding phase by playing 100,000 randomly sampled decks.

DouZero DeltaDou

SL

WP
ADP

0.580
0.323

0.461
-0.004

0.381
-0.320

AI in the literature. We note that DouDizhu has very high
variance, i.e., to win a game relies on the strength of the
initial hand card, which is highly dependent on luck. Thus,
a WP of 0.586 and an ADP of 0.258 suggest a signiﬁ-
cant improvement over DeltaDou. Moreover, DeltaDou re-
quires searching at both training and testing time. Whereas,
DouZero does not do the searching, which veriﬁes that the
Q-networks learned by DouZero are very strong.

The left-hand side of Figure 4 shows the Elo rating scores
of DouZero, DeltaDou, and SL by playing 100, 000 decks.
We observe that DouZero outperforms DeltaDou and SL
in terms of both WP and ADP signiﬁcantly. This again
demonstrates the strong performance of DouZero.

The right-hand side of Figure 4 illustrates the performance
of DouZero on Botzone leaderboard. We note that Botzone
adopts a different scoring mechanism. In addition to WP, it
gives additional bonuses to some speciﬁc card categories,
such as Chain of Pair and Rocket (detailed in Appendix E).
While it is very likely that DouZero can achieve better
performance if using the scoring mechanism of Botzone as
the objective, we directly upload the pre-trained model of
DouZero that is trained with WP as objective. We observe
that this model is strong enough to beat the other bots.

5.3. Comparison with Bidding Phase

To investigate RQ2, we train a bidding network with su-
pervised learning using human expert data. We place the
top-3 algorithms, i.e., DouZero, DeltaDou, and SL, into
the three seats of a DouDizhu game. In each game, we ran-
domly choose the ﬁrst bidder and simulate the bidding phase
with the pre-trained bidding network. The same bidding net-

Figure 6. Left: The WP against SL w.r.t. training time using dif-
ferent number of actors Right: The WP against SL w.r.t. timesteps
using different number of actors.

work is used for all three algorithms for a fair comparison.
The results are summarized in Table 2. Although DouZero
is trained on randomly generated decks without the bidding
network, we observe that DouZero dominates the other
two algorithms in both WP and ADP. This demonstrates
the applicability of DouZero in real-world competitions
where the bidding phase needs to be considered.

5.4. Analysis of Learning Progress

To study RQ3, we visualize the learning progress of
DouZero in Figure 5. We use SL and DeltaDou as oppo-
nents to draw the curves of WP and ADP w.r.t. the number
of training days. We make two observations as follows.
First, DouZero outperforms SL in one day and two days of
training in terms of WP and ADP, respectively. We note that
DouZero and SL use the exactly same neural architecture
for training. Thus, we attribute the superiority of DouZero
to self-play reinforcement learning. While SL also performs
well, it relies on a large amount of data, which is not ﬂexible
and could limit its performance. Second, DouZero outper-
forms DeltaDou in three days and ten days of training in
terms of WP and ADP, respectively. We note that DeltaDou
is initialized with supervised learning on heuristics and is
trained for more than two months. Whereas, DouZero
starts from scratch and only needs days of training to beat
DeltaDou. This suggests that model-free reinforcement
learning without search is indeed effective in DouDizhu.

Performance against baselinesWinning threshold0102030TrainingDays0.00.20.40.6WP0102030TrainingDays−2−10ADP0102030TrainingDays0.00.10.20.30.40.50.6WP0102030TrainingDays−3−2−10ADP01020304050TrainingHours0.00.10.20.30.40.50.6WP45Actors30Actors15Actors0.00.51.0Timesteps1e90.00.10.20.30.40.50.6WP45Actors30Actors15ActorsDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Figure 7. Left: The WP against SL w.r.t. training time for SARSA
and Actor-Critic Right: The WP against SL w.r.t. timesteps for
SARSA and Actor-Critic.

Figure 8. Average accuracy across the three positions on the human
data w.r.t. the number of training days for DouZero. We ﬁt the
data points with a polynomial with four terms for better visualizing
the trend. The accuracy for SL is 84.2%. DouZero aligns with
human expertise in the ﬁrst ﬁve days of training but discovers novel
strategies beyond human knowledge in the later training stages.
The curves for all the three positions are provided in Appendix D.4.

We further analyze the learning speed when using different
numbers of actors. Figure 6 reports the performance against
SL when using 15, 30, and 45 actors. We observe that using
more actors can accelerate the training in wall-clock time.
We also ﬁnd that all three settings show similar sample
efﬁciency. In the future, we will explore the possibility of
using more actors across multiple servers to further improve
the training efﬁciency.

5.5. Comparison with SARSA and Actor-Critic

To answer RQ4, we implement two variants based on
DouZero. First, we replace the DMC objective with the
Temporal-Difference (TD) objective. This leads to a deep
version of SARSA. Second, we implement an Actor-Critic
variant with action features. Speciﬁcally, we use Q-network
as a critic with action features and train policy as an actor
with action masks to remove illegal actions.

Figure 7 shows the results of SARSA and Actor-Critic with
a single run. First, we do not observe a clear beneﬁt of
using TD learning. We observe that DMC learns slightly
faster than SARSA in wall-clock time and sample efﬁciency.
The possible reason is that TD learning will not help much
in the sparse reward setting. We believe more studies are
needed to understand when TD learning will help. Second,
we observe the Actor-Critic fails. This suggests that simply

Figure 9. Comparison of inference time.

adding action features to the critic may not be enough to
resolve the complex action space issue. In the future, we will
investigate whether we can effectively incorporate action
features into the actor-critic framework.

5.6. Analysis of DouZero on Expert Data

For RQ5, we calculate the accuracy of DouZero on the
human data throughout the training process. We report the
model trained with ADP as objective since the game app
from which the human data is collected also adopts ADP.
Figure 8 shows the results. We make two interesting obser-
vations as follows. First, at the early stages, i.e., the ﬁrst ﬁve
days of training, the accuracy keeps improving. This sug-
gests that the agents may have learned some strategies that
align with human expertise with purely self-play. Second,
after ﬁve days of training, the accuracy decreases dramati-
cally. We note that the ADP against SL is still improving
after ﬁve days. This suggests that the agents may have dis-
covered some novel and stronger strategies that humans can
not easily discover, which again veriﬁes the effectiveness of
self-play reinforcement learning.

5.7. Comparison of Inference Time

To answer RQ6, we report the average inference time per
step in Figure 9. For a fair comparison, we evaluate all
the algorithms on the CPU. We observe that DouZero is
orders of magnitude faster than DeltaDou, CQN, RHCP,
and RHCP-v2. This is expected since DeltaDou needs to
perform a large number of Monte Carlo simulations, and
CQN, RHCP, and RHCP-v2 require expensive card decom-
position. Whereas, DouZero only performs one forward
pass of neural networks in each step. The efﬁcient inference
of DouZero enables us to generate a large number of sam-
ples per second for reinforcement learning. It also makes it
affordable to deploy the models in real-world applications.

5.8. Case Study

To investigate RQ7, we conduct case studies to understand
the decisions made by DouZero. We dump the logs of the
competitions from Botzone and visualize the top actions
with their predicted Q-values. We provide most of the case
studies, including both good and bad cases, in Appendix F.

01020304050TrainingHours0.00.10.20.30.40.50.6WPDouZeroSARSAActor-Critic0.00.51.0Timesteps1e90.00.10.20.30.40.50.6WPDouZeroSARSAActor-Critic0510152025TrainingDays0.30.40.5Accuracy10−310−210−1100Average Inference Time per Step in SecondDeltaDouCQNRHCP-v2RHCPDouZeroSLRLCardRandomDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

RL is based on sampling so that it can easily generalize
to large-scale games. RL has been successfully applied in
some complex imperfect-information games, such as Star-
craft (Vinyals et al., 2019), DOTA (Berner et al., 2019) and
Mahjong (Li et al., 2020a). More recently, RL+search is
explored and shown to be effective in poker games (Brown
et al., 2020). DeltaDou adopts a similar idea, which ﬁrst in-
fers the hidden information and then uses MCTS to combine
RL with search in DouDizhu (Jiang et al., 2019). However,
DeltaDou is computationally expensive and heavily relies
on human expertise. In practice, even without search, our
DouZero outperforms DeltaDou in days of training.

7. Conclusions and Future Work

This work presents a strong AI system for DouDizhu. Some
unique properties make DouDizhu particularly challenging
to solve, e.g., huge state/action space and reasoning about
both competing and cooperation. To address these chal-
lenges, we enhance classic Monte-Carlo methods with deep
neural networks, action encoding, and parallel actors. This
leads to a pure RL solution, namely DouZero, which is
conceptually simple yet effective and efﬁcient. Extensive
evaluations demonstrate that DouZero is the strongest AI
program for DouDizhu up to date. We hope the insight that
simple Monte-Carlo methods can lead to strong policies in
such a hard domain will motivate future research.

For future work, we will explore the following directions.
First, we plan to try other neural architectures, such as con-
volutional neural networks and ResNet (He et al., 2016).
Second, we will involve bidding in the loop for reinforce-
ment learning. Third, we will combine DouZero with
search at training and/or test time as in (Brown et al., 2020),
and study how to balance RL and search. Fourth, we will ex-
plore off-policy learning to improve the training efﬁciency.
Speciﬁcally, we will study whether and how we can improve
the wall-clock time and the sample efﬁciency with experi-
ence replay (Lin, 1992; Zhang & Sutton, 2017; Zha et al.,
2019b; Fedus et al., 2020). Fifth, we will try explicitly mod-
eling collaboration of the Peasants (Panait & Luke, 2005;
Foerster et al., 2016; Raileanu et al., 2018; Lai et al., 2020).
Sixth, we plan to try scalable frameworks, such as SEED
RL (Espeholt et al., 2019). Last but not least, we will test
the applicability of Monte-Carlo methods on other tasks.

Acknowledgements

for building the
We thank our colleagues in Kuai Inc.
DouDizhu environment and the helpful discussions, Qiqi
Jiang from DeltaDou team for helping us set up DeltaDou
models, and Songyi Huang8 from RLCard team for devel-
oping demo. We would also like to thank the anonymous
reviewers and the meta-reviewer for the insightful feedback.

8https://github.com/hsywhu

Figure 10. A case study dumped from Botzone, where the three
players play cards in counter-clockwise order. The Peasant agent
learns to play small Solo to cooperate with the other Peasant to
win the game. Note that the other players’ hands are showed face
up solely for better visualization but are hidden in the real game.
More case studies are provided in Appendix F.

Figure 10 shows a typical case when the two Peasants can
cooperate to beat the Landlord. The Peasant on the right-
hand side only has one card left. Here, the Peasant at the
bottom can play a small Solo to help the other Peasant
win. When looking into the top three actions predicted by
DouZero, we make two interesting observations. First,
we ﬁnd that all the top actions outputted by DouZero are
small Solos with high conﬁdence to win, suggesting that the
two Peasants of DouZero may have learned to cooperate.
Second, the predicted Q-value of action 4 (0.808) is much
lower than that of action 3 (0.971). A possible explanation
is that there is still a 4 out there, so that playing 4 may not
necessarily help the Peasant win. In practice, in this speciﬁc
case, the other Peasant’s only card is not higher than 4 in
rank. Overall, action 3 is indeed the best move in this case.

6. Related Work

Search for Imperfect-Information Games. Counterfac-
tual Regret Minimization (CFR) (Zinkevich et al., 2008) is a
leading iterative algorithm for poker games, with many vari-
ants (Lanctot et al., 2009; Gibson et al., 2012; Bowling et al.,
2015; Moravˇc´ık et al., 2017; Brown & Sandholm, 2018;
2019a; Brown et al., 2019; Lanctot et al., 2019; Li et al.,
2020b). However, traversing the game tree of DouDizhu
is computationally intensive since it has a huge tree with a
large branching factor. Moreover, most of the prior studies
focus on zero-sum settings. While some efforts have been
devoted to addressing the cooperative settings, e.g., with
blueprint policy (Lerer et al., 2020), it remains challeng-
ing to reason about both competing and cooperation. Thus,
DouDizhu has not seen an effective CFR-like solution.

RL for Imperfect-Information Games. Recent studies
show that Reinforcement Learning (RL) can achieve com-
petitive performance in poker games (Heinrich et al., 2015;
Heinrich & Silver, 2016; Lanctot et al., 2017). Unlike CFR,

Top-3 MovesPASSPASSLandlordPeasant (DouZero)Peasant (DouZero)0.9710.8080.784DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

References

Beleznay, F., Grobler, T., and Szepesvari, C. Compar-
ing value-function estimation algorithms in undiscounted
problems. Technical Report TR-99-02, MindMaker Ltd,
1999.

Berner, C., Brockman, G., Chan, B., Cheung, V., Dkebiak,
P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S.,
Hesse, C., et al. Dota 2 with large scale deep reinforce-
ment learning. arXiv preprint arXiv:1912.06680, 2019.

Bowling, M., Burch, N., Johanson, M., and Tammelin, O.
Heads-up limit hold’em poker is solved. Science, 347
(6218):145–149, 2015.

Brown, N. and Sandholm, T. Superhuman ai for heads-up
no-limit poker: Libratus beats top professionals. Science,
359(6374):418–424, 2018.

Foerster, J. N., Assael, Y. M., De Freitas, N., and Whiteson,
S. Learning to communicate with deep multi-agent re-
inforcement learning. arXiv preprint arXiv:1605.06676,
2016.

Gibson, R., Lanctot, M., Burch, N., Szafron, D., and Bowl-
ing, M. Generalized sampling and variance in counterfac-
tual regret minimization. In AAAI Conference on Artiﬁcial
Intelligence, 2012.

Gray, J., Lerer, A., Bakhtin, A., and Brown, N. Human-
level performance in no-press diplomacy via equilibrium
search. arXiv preprint arXiv:2010.02923, 2020.

Guo, Y., Oh, J., Singh, S., and Lee, H.

Genera-
tive adversarial self-imitation learning. arXiv preprint
arXiv:1812.00950, 2018.

Hasselt, H. V. Double q-learning. In Advances in Neural

Information Processing Systems, 2010.

Brown, N. and Sandholm, T. Solving imperfect-information
games via discounted regret minimization. In AAAI Con-
ference on Artiﬁcial Intelligence, 2019a.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In IEEE conference on
computer vision and pattern recognition, 2016.

Brown, N. and Sandholm, T. Superhuman ai for multiplayer

poker. Science, 365(6456):885–890, 2019b.

Brown, N., Lerer, A., Gross, S., and Sandholm, T. Deep
counterfactual regret minimization. In International Con-
ference on Machine Learning, 2019.

Brown, N., Bakhtin, A., Lerer, A., and Gong, Q. Combining
deep reinforcement learning and search for imperfect-
information games. arXiv preprint arXiv:2007.13544,
2020.

Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P.,
Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and
Coppin, B. Deep reinforcement learning in large discrete
action spaces. arXiv preprint arXiv:1512.07679, 2015.

Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,
V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,
I., et al. Impala: Scalable distributed deep-rl with im-
portance weighted actor-learner architectures. In Interna-
tional Conference on Machine Learning, 2018.

Espeholt, L., Marinier, R., Stanczyk, P., Wang, K., and
Michalski, M. Seed rl: Scalable and efﬁcient deep-rl with
accelerated central inference. In International Conference
on Learning Representations, 2019.

Fedus, W., Ramachandran, P., Agarwal, R., Bengio, Y.,
Larochelle, H., Rowland, M., and Dabney, W. Revisit-
ing fundamentals of experience replay. In International
Conference on Machine Learning, 2020.

Heinrich, J. and Silver, D. Deep reinforcement learning
from self-play in imperfect-information games. arXiv
preprint arXiv:1603.01121, 2016.

Heinrich, J., Lanctot, M., and Silver, D. Fictitious self-play
in extensive-form games. In International Conference on
Machine Learning, 2015.

Jiang, Q., Li, K., Du, B., Chen, H., and Fang, H. Deltadou:
Expert-level doudizhu ai through self-play. In Interna-
tional Joint Conferences on Artiﬁcial Intelligence, 2019.

Johanson, M., Waugh, K., Bowling, M., and Zinkevich, M.
Accelerating best response calculation in large extensive
games. In International Joint Conferences on Artiﬁcial
Intelligence, 2011.

K¨uttler, H., Nardelli, N., Lavril, T., Selvatici, M., Sivakumar,
V., Rockt¨aschel, T., and Grefenstette, E. Torchbeast:
A pytorch platform for distributed rl. arXiv preprint
arXiv:1910.03552, 2019.

Lai, K.-H., Zha, D., Li, Y., and Hu, X. Dual policy dis-
tillation. In International Joint Conference on Artiﬁcial
Intelligence, 2020.

Lanctot, M., Waugh, K., Zinkevich, M., and Bowling, M.
Monte carlo sampling for regret minimization in exten-
sive games. Advances in Neural Information Processing
Systems, 2009.

Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A.,
Tuyls, K., P´erolat, J., Silver, D., and Graepel, T. A uni-
ﬁed game-theoretic approach to multiagent reinforcement

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

learning. In Advances in neural information processing
systems, 2017.

Lanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V.,
Upadhyay, S., P´erolat, J., Srinivasan, S., Timbers, F.,
Tuyls, K., Omidshaﬁei, S., et al. Openspiel: A frame-
work for reinforcement learning in games. arXiv preprint
arXiv:1908.09453, 2019.

Lerer, A., Hu, H., Foerster, J. N., and Brown, N. Improving
policies via search in cooperative partially observable
games. In AAAI Conference on Artiﬁcial Intelligence,
2020.

Li, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R.,
Zhao, L., Qin, T., Liu, T.-Y., and Hon, H.-W. Suphx:
Mastering mahjong with deep reinforcement learning.
arXiv preprint arXiv:2003.13590, 2020a.

Li, K., Xu, H., Zhang, M., Zhao, E., Wu, Z., Xing, J., and
Huang, K. Openholdem: An open toolkit for large-scale
imperfect-information game research. arXiv preprint
arXiv:2012.06168, 2020b.

Lin, L.-J. Self-improving reactive agents based on reinforce-
ment learning, planning and teaching. Machine Learning,
8(3-4):293–321, 1992.

Mania, H., Guy, A., and Recht, B. Simple random search
provides a competitive approach to reinforcement learn-
ing. arXiv preprint arXiv:1803.07055, 2018.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529–533, 2015.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
chronous methods for deep reinforcement learning. In
International Conference on Machine Learning, 2016.

Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,
Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,
D., Graepel, T., et al. Mastering atari, go, chess and shogi
by planning with a learned model. Nature, 588(7839):
604–609, 2020.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.

Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., et al. Mastering the
game of go with deep neural networks and tree search.
Nature, 529(7587):484–489, 2016.

Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,
M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-
pel, T., et al. A general reinforcement learning algorithm
that masters chess, shogi, and go through self-play. Sci-
ence, 362(6419):1140–1144, 2018.

Sutton, R. S. and Barto, A. G. Reinforcement learning: An

introduction. MIT press, 2018.

Szepesv´ari, C. Algorithms for reinforcement learning. Mor-

gan and Claypool, 2009.

Terry, J. K., Black, B., Jayakumar, M., Hari, A., Sullivan,
R., Santos, L., Dieffendahl, C., Williams, N. L., Lokesh,
Y., Horsch, C., et al. Pettingzoo: Gym for multi-agent
reinforcement learning. arXiv preprint arXiv:2009.14471,
2020.

Thrun, S. and Schwartz, A. Issues in using function approx-
imation for reinforcement learning. In Proceedings of the
1993 Connectionist Models Summer School Hillsdale, NJ.
Lawrence Erlbaum, 1993.

van Hasselt, H., Guez, A., and Silver, D. Deep reinforce-
ment learning with double q-learning. In AAAI Confer-
ence on Artiﬁcial Intelligence, 2016.

Moravˇc´ık, M., Schmid, M., Burch, N., Lis`y, V., Morrill, D.,
Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowl-
ing, M. Deepstack: Expert-level artiﬁcial intelligence in
heads-up no-limit poker. Science, 356(6337):508–513,
2017.

Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,
Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,
T., Georgiev, P., et al. Grandmaster level in starcraft ii
using multi-agent reinforcement learning. Nature, 575
(7782):350–354, 2019.

Panait, L. and Luke, S. Cooperative multi-agent learning:
The state of the art. Autonomous agents and multi-agent
systems, 11(3):387–434, 2005.

Williams, R. J. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine
Learning, 8(3-4):229–256, 1992.

Raileanu, R., Denton, E., Szlam, A., and Fergus, R. Mod-
eling others using oneself in multi-agent reinforcement
learning. In International conference on machine Learn-
ing, 2018.

Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B.,
Chen, J., Liu, Z., Qiu, F., Yu, H., et al. Towards playing
full moba games with deep reinforcement learning. arXiv
preprint arXiv:2011.12692, 2020a.

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Ye, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H.,
Yang, S., Wu, X., Guo, Q., et al. Mastering complex
control in moba games with deep reinforcement learning.
In AAAI Conference on Artiﬁcial Intelligence, 2020b.

You, Y., Li, L., Guo, B., Wang, W., and Lu, C. Com-
binational q-learning for dou di zhu. arXiv preprint
arXiv:1901.08925, 2019.

Zahavy, T., Haroush, M., Merlis, N., Mankowitz, D. J., and
Mannor, S. Learn what not to learn: Action elimination
with deep reinforcement learning. In Advances in Neural
Information Processing Systems, 2018.

Zha, D., Lai, K.-H., Cao, Y., Huang, S., Wei, R., Guo, J.,
and Hu, X. Rlcard: A toolkit for reinforcement learning
in card games. arXiv preprint arXiv:1910.04376, 2019a.

Zha, D., Lai, K.-H., Zhou, K., and Hu, X. Experience
replay optimization. In International Joint Conference
on Artiﬁcial Intelligence, 2019b.

Zha, D., Lai, K.-H., Zhou, K., and Hu, X. Simplifying
deep reinforcement learning via self-supervision. arXiv
preprint arXiv:2106.05526, 2021a.

Zha, D., Ma, W., Yuan, L., Hu, X., and Liu, J. Rank
the episodes: A simple approach for exploration in
procedurally-generated environments. In International
Conference on Learning Representations, 2021b.

Zhang, S. and Sutton, R. S. A deeper look at experience
replay. NIPS Deep Reinforcement Learning Symposium,
2017.

Zhou, H., Zhang, H., Zhou, Y., Wang, X., and Li, W. Bot-
zone: an online multi-agent competitive platform for
ai education. In Proceedings of the 23rd Annual ACM
Conference on Innovation and Technology in Computer
Science Education, pp. 33–38, 2018.

Zinkevich, M., Johanson, M., Bowling, M., and Piccione,
C. Regret minimization in games with incomplete infor-
mation. In Advances in Neural Information Processing
Systems, 2008.

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

A. Introduction of DouDizhu

As the most popular card game in China, DouDizhu has attracted hundreds of millions of players with many tournaments
held every year. DouDizhu is known to be easy to learn but challenging to master. It requires careful planning and strategic
thinking. DouDizhu is played among three players. In each game, the players will ﬁrst bid for the Landlord position. After
the bidding phase, one player will become the Landlord, and the other two players will become the Peasants. The two
Peasants play as a team to ﬁght against the Landlord. The objective of the game is to be the ﬁrst player to have no cards
left. In addition to the huge state/action spaces and incomplete information, the two Peasants need to cooperate to beat the
Landlord. Thus, existing algorithms for poker games, which usually operate on small games and are only designed for two
players, are not applicable in DouDizhu. In what follows, we ﬁrst give an overview of the game rule of DouDizhu and then
analyze the state/action spaces of DouDizhu. Readers who are familiar with the game may skip Section A.1. Readers who
are not familiar with DouDizhu may also refer to Wikipedia 9 for more introduction.

A.1. Rules

DouDizhu is played with one pack of cards, including the two jokers. Suits are irrelevant in DouDizhu. The cards are ranked
by Red Joker, Black Joker, 2, A, K, Q, J, 10, 9, 8, 7, 6, 5, 4, 3. Each game has three phases as follows.

• Dealing: A shufﬂed pack of 54 cards will be dealt to the three players. Each player will be dealt 17 cards, and the last
three leftover cards will be kept on the deck, face down. These three cards will be dealt to the Landlord, which are
decided in the bidding phase.

• Bidding: The three players will analyze their own cards without showing to other players. The players decide whether
they would like to bid the Landlord based on their hand cards’ strength. There are many versions of bidding rules. In
this paper, we consider a version adopted in most online DouDizhu app. The ﬁrst bidder will be randomly chosen. The
ﬁrst bidder will then decide whether she bids. If the ﬁrst bidder does not bid, the other players will become the bidder
in turn until someone bids. If no one bids, a new pack of cards will be dealt to the players. If one chooses to bid, the
other players will decide whether she accepts the bid or she wants to outbid. Each player only has one chance to outbid.
The last player who bids or outbids will become the Landlord. Once the Landlord is settled, the Landlord will be dealt
with the three cards on the deck. The other two players will play as the Peasants to ﬁght against the Landlord.

• Card-Playing: In this phase, the players will play cards in turn starting from the Landlord. The ﬁrst player can choose
either category of cards such as Solo, Pair, etc. (detailed in the next paragraph). Then the next player must play the
cards in the same category with a higher rank. The next player can also choose “PASS” if she does not have a higher
rank in hand or she does not want to follow the category. If all the other players choose “PASS,” the player who ﬁrst
plays the category can freely play cards in other categories. The players will play cards in turn until one player has no
cards left. The Landlord wins if she has no cards left. The Peasant team wins if either of the peasants has no cards left.
The two Peasants need to cooperate to increase the possibility of winning. A Peasant may still win a game by helping
the other Peasant win even if she has terrible hand cards.

One challenge of DouDizhu is the rich categories, which consist of various combinations of cards. For some categories, the
player can choose a kicker card, which can be any card in hand. One will usually choose a useless card as a kicker card so
that she can more easily go out of hand. As a result, the player needs to carefully plan how to play the cards to win a game.
The categories in DouDizhu are listed as follows. Note that Bomb and Rocket defy the category rules and can dominate all
the other categories.

• Solo: Any single card.

• Pair: Two matching cards of equal rank.

• Trio: Three individual cards of equal rank.

• Trio with Solo: Three individual cards of equal rank with a Solo as the kicker.

• Trio with Pair: Three individual cards of equal rank with a Pair as the kicker.

9https://en.wikipedia.org/wiki/Dou_dizhu

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

• Chain of Solo: ≥Five consecutive individual cards.

• Chain of Pair: ≥Three consecutive Pairs.

• Chain of Trio: ≥Two consecutive Trios.

• Plane with Solo: ≥Two consecutive trios with each has a distinct individual kicker card.

• Quad with Pair: Four-of-a-kind with two sets of Pair as the kicker.

• Bomb: Four-of-a-kind.

• Rocket: Red and Black jokers.

A.2. State and Action Space of DouDizhu

According to the estimation in RLCard (Zha et al., 2019a), the number of information sets in DouDizhu is up to 1083 and
the average size of each information set is up to 1023. While the number of information sets is smaller than that of No-limit
Texas Hold’em (10126), the average size of each information set is much larger than that of No-limit Texas Hold’em (104).
Different from Hold’em games, the state space of DouDizhu can not be easily abstracted. Speciﬁcally, every card matters in
DouDizhu towards winning. For example, the number of cards of rank 2 in the historical moves is crucial since the players
need to decide whether their cards will be dominated by other players with a 2. Thus, a very slight difference in the state
representation could signiﬁcantly impact the strategy. While the size of the state space of DouDizhu is not as large as that of
No-limit Texas Hold’em, learning an effective strategy is very challenging since the agents need to distinguish different
states accurately. DouZero approaches this problem by extracting representations and learning the strategy automatically
with deep neural networks.

DouDizhu suffers from an explosion of action space due to the combinations of cards. We summarize the action space in
Table 3. The size of the action space of DouDizhu is 27, 472, which is much larger than Mahjong (102). It is also much
more complicated than No-limit Texas Hold’em, whose action space can be easily abstracted. Speciﬁcally, in DouDizhu,
every card matters. For example, for the action type Trio with Solo, wrongly choosing the kicker may directly result
in a loss since it could potentially break a chain. Thus, it is difﬁcult to abstract the action space. This poses challenges
for reinforcement learning since most of the algorithms only work well on small action space. In contrast to the previous
work that abstracts the action space with heuristics (Jiang et al., 2019), DouZero approaches this issue with Monte-Carlo
methods, which allow ﬂexible exploration of the action space to potentially discover better moves.

Table 3. Summary of the action space of DouDizhu. We follow the summary provided in RLCard (Zha et al., 2019a).

Action Type

Number of Actions

Solo
Pair
Trio
Trio with Solo
Trio with Pair
Chain of Solo
Chain of Pair
Chain of Trio
Plane with Solo
Plane with Pair
Quad with Solo
Quad with Pair
Bomb
Rocket
Pass

Total

15
13
13
182
156
36
52
45
21, 822
2, 939
1, 326
858
13
1
1

27, 472

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

B. Additional Examples of Card Representations

(a) Solo

(b) Pair

(c) Trio with Solo

(d) Chain of solo

(e) Chain of pair

(f) Plane with solo

(g) Quad with solo

(h) Rocket

Figure 11. Additional examples of encoding different types of cards.

000000000000000000000000000000000000000000000000100000000000000000000000000000000000000011000000000000000000000000000000000000000000000010000000000000000000000000000000111000000000100010001000100010000000000000000000000000000000000000000000110011001100000000000000000000000000000000000000000000000000000000000000100010000000000011101110000000000000000000000000000000000000100010000000000011110000000000000000100010000000000000000000000000000000000000000000000000000000000010001000DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

C. Additional Details of Feature Representation and Neural Architecture

C.1. Action and State Representation

The input of the neural network is the concatenated representation of state and action. For each 15 × 4 card matrix, we ﬁrst
ﬂatten the matrix into a 1-dimensional vector of size 60. Then we remove six entries that are always zero since there is
only one black or red joker. In other words, each card matrix is transformed into a one-hot vector of size 54. In addition
to card matrices, we further use a one-hot vector to represent the other two players’ current hand cards. For example, for
Peasant, we use a vector of size 17, where each entry corresponds to the number of hand cards in the current state. For the
Landlord, the vector’s size is 20 since the Landlord can have at most 20 cards in hand. Similarly, we use a 15-dimension
vector to represent the number of bombs in the current state. For historical moves, we consider the most recent 15 moves and
concatenate the representations of every three consecutive moves; that is, the historical moves are encoded into a 5 × 162
matrix. The historical moves are fed into an LSTM, and we use the hidden representation in the last cell to represent the
historical moves. If there are less than 15 moves historically, we use zero matrices for the missing moves. We summarize
the encoded features of Landlord and each Peasant in Table 4 and Table 5, respectively.

Table 4. Features of the Landlord.

Feature

Action Card matrix of the action

State

Card matrix of hand cards
Card matrix of the union of the other two players’ hand cards
Card matrix of the most recent move
Card matrix of the the played cards of the ﬁrst Peasant
Card matrix of the the played cards of the second Peasant
One-hot vector representing the number cards left of the ﬁrst Peasant
One-hot vector representing the number cards left of the second Peasant
One-hot vector representing the number bombs in the current state
Concatenated matrix of the most recent 15 moves

Table 5. Features of the Peasants.

Feature

Action Card matrix of the action

State

Card matrix of hand cards
Card matrix of the union of the other two players’ hand cards
Card matrix of the most recent move
Card matrix of the most recent move performed by the Landlord
Card matrix of the most recent move performed by the other Peasant
Card matrix of the the played cards of the Landlord
Card matrix of the the played cards of the other Peasant
One-hot vector representing the number cards left of the Landlord
One-hot vector representing the number cards left of the other Peasant
One-hot vector representing the number bombs in the current state
Concatenated matrix of the most recent 15 moves

Size

54

54
54
54
54
54
17
17
15
5 × 162

Size

54

54
54
54
54
54
54
54
20
17
15
5 × 162

C.2. Data Collection and Neural Architecture of Supervised Learning

In order to train an agent with supervised learning, we collect user data internally from a popular DouDizhu game mobile
app. The users in the app have different leagues, which represent the strengths of the users. We ﬁlter out the raw data by
only keeping the data generated by the players of the highest league to ensure the quality of the data. After ﬁltering, we
obtain 226,230 human expert matches. We treat each move as an instance and use a supervised loss to train the networks.
The problem can be formulated as a classiﬁcation problem, where we aim at predicting the action based on a given state,

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

with a total of 27, 472 classes. However, we ﬁnd in practice that most of the actions are illegal, and it is expensive to iterate
over all the classes. Motivated by Q-network’s design, we transform the problem into a binary classiﬁcation task, as shown
in Figure 12. Speciﬁcally, we use the same neural architecture as DouZero and add a Sigmoid function to the output. We
then use binary cross-entropy loss to train the network. We randomly sample 10% of the data for validation purposes and
use the rest for training. We transform the user data into positive instances and generate negative instances based on the
legal moves that are not selected. Eventually, the training data consists of 49, 990, 075 instances. We further ﬁnd that the
data is imbalanced, where the number of negative instances is much larger than that of positive instances. Thus, we adopt
a re-weighted cross-entropy loss based on the distribution of positive and negative instances. We ﬁnd in practice that the
re-weighted loss can improve the performance. We set the batch size to be 8096 and train 20 epochs. The prediction is made
by choosing the action that leads to the highest score. We output the model that has the highest accuracy on the validation
data. We do this process three times for the three positions, respectively. We plot the validation accuracy w.r.t. the number
epochs in Figure 13. The network can achieve around 84% accuracy for all positions.

Figure 12. We use the same neural architecture as DouZero for SL. We add a Sigmoid function to the output and transform the problem
into a binary classiﬁcation task. The agent will perform the action that leads to the highest prediction score.

(a) Landlord

(b) LandlordUp

(c) LandlordDown

Figure 13. Accuracy w.r.t. the number of training epochs of SL for the three positions. LandlordUp stands for the Peasant that moves
before the Landlord. LandlordDown stands for the Peasant that moves after the Landlord.

C.3. Neural Architecture and Training Details of Bidding Network

The bidding phase’s goal is to determine whether a player should become the landlord based on the strengths of the hand
cards. This decision is much simpler than card-playing since the agent only needs to consider the hand cards and the other
players’ decisions, and we only need to make a binary prediction, i.e., whether we bid. At the beginning of the bidding
phase, a randomly chosen player will decide whether to bid or not. Then the other two players will also choose whether to

ActionEncodeState000100000000000000000000000000000111000000000000000000000000000100000000000000000000000000000111000000000000000000010001000100000000000000000000000000000111000000000000000000010001000100000000000000000000000000000111000000000000000000010001000100000000000000000000000000000111000000000000000000010001EncodeHistorical MovesLSTMLSTMLSTMEncodehLSTMMLPPredictionRepeat6xSigmoid05101520Epochs0.20.30.40.50.60.70.8Accuracy05101520Epochs0.30.40.50.60.70.8Accuracy05101520Epochs0.30.40.50.60.70.8AccuracyDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

bid. If only one player bids, then that player will become the landlord. Suppose two or more players bid, the player who
bids ﬁrst will have the priority to decide whether she wants to become the landlord. We extract 128 features to represent
hand cards and the players’ moves, as summarized in Table 6. For the network architecture, we use a (512, 256, 128, 64, 32,
16) MLP. Like the supervised card playing agent, we add a Sigmoid function to the output and train the network with binary
cross-entropy loss. We plot the validation accuracy w.r.t. the number epochs in Figure 14. The network can achieve 83.1%
accuracy.

Table 6. Features of the bidding network.

Feature

Card matrix of hand cards
A vector representing solos of ranks 3 to A
A vector representing pairs of ranks 3 to 2
A vector representing trios of ranks 3 to 2
A vector representing bombs of ranks 3 to 2 and the rocket
The number of cards of rank 2 and the jokers
A vector encoding historical bidding moves

Total

Size

54
12
13
13
14
10
12

128

Figure 14. Accuracy w.r.t. the number of training epochs of the bidding network.

05101520Epochs0.550.600.650.700.750.80AccuracyDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

D. Additional Results of DouZero

D.1. Full WP and ADP Results for Landlord and Peasants

We report the results for Landlord and Peasants in Table 7 and Table 8 for WP and ADP, respectively. We observe that the
advantage of DouZero for Peasants tends to be larger than that of Landlord. A possible explanation is that the two Peasants
agents in DouZero have learned cooperation skills, which could be hardly covered by the heuristics and other algorithms.

Table 7. WP of DouZero and the baselines. L: WP of A as Landlord; P: WP of A as Peasants. If the average WP of L and P is higher
than 0.5, we conclude that A outperforms B and highlight both L and P in boldface. The algorithms are ranked according to the number of
the other algorithms that they beat.
B DouZero
P

RHCP-v2
P
L

DeltaDou
P
L

Random
P
L

RHCP
P
L

RLCard
P
L

Rank A

CQN

SL

L

L

L

P

P

.

1
2
3
4
5
6
7
8

DouZero .4159 .5841 .4870 .6843 .5692 .7494 .6844 .8303 .7253 .8033 .8695 .9089 .7686 .8513 .9858 .9920
.3166 .5130 .4120 .5880 .5130 .7211 .6701 .8165 .7048 .7899 .8563 .8955 .7326 .8351 .9871 .9960
DeltaDou
.2506 .4308 .2789 .5130 .4072 .5928 .5370 .6857 .5831 .6810 .7605 .8650 .6450 .7428 .9599 .9927
SL
.1697 .3156 .1835 .3299 .3143 .4630 .4595 .5405 .5134 .5165 .6813 .7018 .6313 .6116 .9519 .9821
RHCP-v2
.1967 .2747 .2101 .2952 .3190 .4179 .4835 .4866 .4971 .5029 .6718 .6913 .6416 .5640 .9092 .9725
RHCP
.0911 .1305 .1045 .1437 .1350 .2395 .2982 .3187 .3087 .3282 .4465 .5535 .5839 .4603 .9314 .9539
RLCard
.1487 .2314 .1649 .2674 .2572 .3550 .3884 .3687 .4360 .3584 .5397 .4161 .5238 .4762 .8566 .9213
CQN
.0080 .0142 .0040 .0129 .0073 .0401 .0179 .0481 .0025 .0908 .0461 .0686 .0787 .1434 .3461 .6539
Random

Table 8. ADP of DouZero and the baselines. L: ADP of A as Landlord; P: ADP of A as Peasants. If the average ADP of L and P is higher
than 0, we conclude that A outperforms B and highlight both L and P in boldface. The algorithms are ranked according to the number of
the other algorithms that they beat.

.

Rank A

B DouZero
P

L

DeltaDou
P
L

SL

L

P

RHCP-v2
P
L

RHCP
P
L

RLCard
P
L

CQN

L

P

Random
P
L

1
2 DeltaDou
SL
3
RHCP-v2
4
RHCP
5
RLCard
6
CQN
7
Random
8

DouZero -0.435 0.435 -0.342 0.858 0.287 1.112 1.436 1.888 1.492 1.850 2.222 2.354 1.368 2.001 3.254 2.818
0.342 -0.858 -0.476 0.476 0.268 1.038 1.297 1.703 1.312 1.715 2.270 2.648 1.218 1.849 3.268 2.930
-1.112 -0.287 -1.038 -0.268 -0.364 0.364 0.564 1.142 0.658 1.114 1.652 1.990 0.878 1.196 3.026 2.415
-1.888 -1.436 -1.703 -1.297 -1.142 -0.564 -0.209 0.209 0.074 0.029 1.011 1.230 0.750 0.677 2.638 2.624
-1.850 -1.492 -1.715 -1.312 -1.114 -0.658 -0.029 -0.074 -0.007 0.007 1.190 1.328 0.927 -0.432 2.722 2.717
-2.354 -2.222 -2.648 -2.270 -1.990 -1.652 -1.230 -1.011 -1.328 -1.190 -0.266 0.266 0.474 -0.138 2.630 2.312
-2.001 -1.368 -1.849 -1.218 -1.196 -0.878 -0.677 -0.750 0.432 -0.927 0.138 -0.474 0.056 -0.056 1.832 1.992
-2.818 -3.254 -2.930 -3.268 -2.415 -3.026 -2.624 -2.638 -2.717 -2.722 -2.312 -2.629 -1.991 -1.832 -0.883 0.883

D.2. Comparison of Using WP and ADP as Objectives

In our experiments, we ﬁnd that the agents will learn different styles of card playing strategies when using WP and ADP as
objectives. Speciﬁcally, we observe that the agents trained with WP play more aggressively about bombs even if it will lose.
We visualize this phenomenon in Appendix F.4. The possible explanation is that a bomb will not double the points so that
playing a bomb or rocket will not harm WP. Aggressively playing bombs may beneﬁt WP since they will dominate other
payers, which allows them to play hand cards freely. In contrast, the agents trained with ADP tend to be very cautious of
playing bombs since improperly playing a bomb may double the ADP loss if the agents lose the game in the end.

To better interpret the differences between WP and ADP, we show the results of the agents trained with ADP and WP against
the baselines. In Table 9, we report the results of DouZero trained with ADP using WP as the metric. We observe that
the performance is slightly worse than that in Table 7. In Table 10, we show the results of DouZero trained with WP
using ADP as the metric. Similarly, the ADP result is slightly worse than thate in Table 8. We observe similar results if
considering the bidding phase (see Table 11 and Table 12). Finally, we launch a head-to-head competition of these two
agents in Table 13. The results again verify that the agents trained with WP are better in terms of WP and vice versa. The
above results suggest that WP and ADP are indeed different and encourage different card playing strategies.

In addition to WP and ADP, some other metrics could also be adopted in real-word DouDizhu completions. For example,
some apps allow users to double the base score at the beginning of a game. We argue that we should adjust the objectives to
achieve the best performance according to different scenarios.

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Table 9. WP of DouZero against baselines when using ADP as the reward. L: WP of A as Landlord; P: WP of A as Peasants. If the
average WP of L and P is higher than 0.5, we conclude that A outperforms B and highlight both L and P in boldface.

B

A

DouZero (ADP) DeltaDou
P

L

L

P

.

SL

L

P

RHCP-v2
P
L

RHCP
P
L

RLCard
P
L

CQN

L

P

Random
P
L

DouZero (ADP) .4281

.5719

.4177 .6319 .5039 .6815 .6615 .7543 .6950 .7628 .8416 .8668 .7198 .8280 .9801 .9895

Table 10. ADP of DouZero against baselines when using WP as the reward. L: ADP of A as Landlord; P: ADP of A as Peasants. If the
average ADP of L and P is higher than 0, we conclude that A outperforms B and highlight both L and P in boldface.

B

A

DouZero (WP) DeltaDou
P

L

L

P

SL

L

P

.
RHCP-v2
P
L

RHCP
P
L

RLCard
P
L

CQN

L

P

Random
P
L

DouZero (WP) -0.411

0.411

-0.360 0.664 0.224 1.001 1.252 1.880 1.378 1.794 2.094 2.298 1.418 1.872 2.947 2.518

Table 11. DouZero against DeltaDou and SL when using ADP as reward with bidding network.
DouZero (ADP) DeltaDou

SL

WP
ADP

0.535
0.323

0.477
-0.004

0.407
-0.320

Table 12. DouZero against DeltaDou and SL when using WP as reward with bidding network.
DouZero (WP) DeltaDou

SL

WP
ADP

0.580
0.315

0.461
0.075

0.381
-0.390

Table 13. Head-to-head comparison between using ADP and WP as objectives. DouZero (ADP) outperforms DouZero (WP) in terms
of ADP but is worse than DouZero (WP) in terms of WP. The agents tend to learn different skills with different objectives.

DouZero (ADP) vs DouZero (WP) -0.3101 0.4476 .3617 .5151

ADP

L

P

WP

L

P

D.3. Additional Results of Learning Progress

(a) WP against SL

(b) ADP against SL

(c) WP against DeltaDou

(d) ADP against DeltaDou

Figure 15. WP and ADP of DouZero against SL and DeltaDou w.r.t. the number of training days. DouZero outperforms SL with 2
days of training, i.e., the overall WP is larger than the threshold of 0.5 and the overall ADP is larger than the threshold of 0, and surpasses
DeltaDou within 10 days, using a single server with four 1080 Ti GPUs and 48 processors.

OverallPeasantsLandlordOverall Threshold0102030TrainingDays0.00.20.40.6WP0102030TrainingDays−2−101ADP0102030TrainingDays0.00.20.40.6WP0102030TrainingDays−3−2−10ADPDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

(a) WP against SL

(b) ADP against SL

(c) WP against DeltaDou

(d) ADP against DeltaDou

Figure 16. WP and ADP of DouZero against SL and DeltaDou w.r.t. the number of training timesteps, i.e., the number of actions played
by the agent. DouZero outperforms SL with around 5 × 108 training timesteps, i.e., the overall WP is larger than the threshold of 0.5
and the overall ADP is larger than the threshold of 0, and surpasses DeltaDou within 5 × 109 training timesteps, using a single server with
four 1080 Ti GPUs and 48 processors.

D.4. Full Results of DouZero on Expert Data

(a) Landlord

(b) LandlordUp

(c) LandlordDown

Figure 17. Accuracy for the three positions on the human data w.r.t. the number of training days for DouZero. We ﬁt the data points
with a polynomial with four terms for better visualizing the trend. LandlordUp stands for the Peasant that moves before the Landlord.
LandlordDown stands for the Peasant that moves after the Landlord. The accuracies of SL for the Landlord, LandlordUp, LandlordDown
are 83.3%, 86.1%, 83.1%, respectively. DouZero aligns with human expertise in the beginning training stages but discovers novel
strategies beyond human knowledge in the later training stages.

D.5. Training Curves of DouZero

In this work, we train three DouZero agents with self-play for all the positions (i.e., the Landlord and the two Peasants)
without considering the bidding phase. For each episode, a deck will be randomly generated. Then the three agents will
perform self-play with partially-observed states. The generated episodes will be passed to the learner process to update the
three DouZero agents. In what follows, we show the self-play rewards and the losses throughout the training process.

(a) ADP w.r.t. training days

(b) ADP w.r.t. timesteps

(c) WP w.r.t. training days

(d) WP w.r.t. timesteps

Figure 18. ADP and WP w.r.t. training days and the number of timesteps for the Landlord and Peasants of DouZero during the training
progress. At the early stage, the Peasants win the Landlord by a small margin. The peasants become stronger and stronger compared with
the Landlord in the later training stages.

OverallPeasantsLandlordOverall Threshold0.00.51.01.5Timesteps1e100.00.20.40.6WP0.00.51.01.5Timesteps1e10−2−101ADP0.00.51.01.5Timesteps1e100.00.20.40.6WP0.00.51.01.5Timesteps1e10−3−2−10ADP051015202530TrainingDays0.20.30.40.5Accuracy051015202530TrainingDays0.250.300.350.400.450.50Accuracy051015202530TrainingDays0.20.30.40.5AccuracyLandlordPeasantsThreshold0102030TrainingDays−0.6−0.4−0.20.00.20.4ADP0.00.51.01.5Timesteps1e10−0.6−0.4−0.20.00.20.4ADP0102030TrainingDays−0.6−0.4−0.20.00.2WP0.00.51.01.5Timesteps1e10−0.6−0.4−0.20.00.2WPDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

(a) Loss w.r.t. training days

(b) Loss w.r.t. timesteps

Figure 19. Losses for different positions for DouZero trained with ADP as rewards. LandlordUp stands for the Peasant that moves
before the Landlord. LandlordDown stands for the Peasant that moves after the Landlord.

(a) Loss w.r.t. training days

(b) Loss w.r.t. timesteps

Figure 20. Losses for different positions for DouZero trained with WP as rewards. LandlordUp stands for the Peasant that moves before
the Landlord. LandlordDown stands for the Peasant that moves after the Landlord.

LandlordLandlord DownLandlord Up0102030TrainingDays1.01.52.02.53.0Loss0.00.51.01.5Timesteps1e101.01.52.02.53.0LossLandlordLandlord DownLandlord Up0102030TrainingDays0.00.20.40.60.8Loss0.00.51.01.5Timesteps1e100.00.20.40.60.8LossDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

E. More Details of Botzone

Botzone is a comprehensive multi-game, multi-agent online game AI platform hosted by AILab, Peking University10.
Besides DouDizhu, Botzone supports more than 20 games, including Go, Mahjong, and Ataxx, to name a few. Botzone
currently has more than 3,500 users and has hosted various games, from in-class and campus contests within Peking
University to nation- and worldwide game AI competitions such as the recent IJCAI 2020 Mahjong AI competition, which
attracted top AI researchers worldwide.

On the Botzone platform, users upload their bot program as a virtual agent to compete with other bots in a selected game. In
order to do so, a user can either nominate opponents and manually start a game or add their bots to the Botzone Elo system,
where games among bots will be scheduled automatically. A bot added to the Botzone Elo system will also be associated
with the so-called Elo rating score and will be added to the rank list (leaderboard) of the game she plays. Botzone assigns an
initial Elo rating score of 1000 to a new bot in the Elo, and the score is updated after each time the bot played an Elo rating
game, as long as the bot remains in the Elo system.

E.1. Interacting with Botzone

Botzone provides a “Judge” program that runs in the background and interacts with bot programs of users. A bot receives a
request to act from the “Judge” every time he has to take an action, i.e., it is her turn to play cards. In DouDizhu, the input
sent to the bot contains three parts of information: her own hand cards, the public card, and a sequence of cards played by
each player. This is consistent with the incomplete information known to a human player in a common DouDizhu game
setting.

The platform also sets constraints on the ﬁle size, memory, and running time of bots. Each decision made by the bot has to
be completed within 1 second (or 6 seconds for a Python program) with no more than 256 MB of memory. The model size
is limited to 140 MB.

E.2. Botzone Ranking Rules

Botzone maintains a leaderboard for each game, which ranks all the bots in the Botzone Elo system by their Elo rating
scores in descending order. Every ﬁve minutes, Botzone schedules match with randomly selected bots, with priority given to
new bots with recent updates. The Elo rating score of participating bots will be updated after the match.

In the Botzone Elo of DouDizhu (named “FightTheLandlord” on the Botzone platform), each game is played by two bots,
with one bot acting as the Landlord and the other as Peasants. A pair of games are played simultaneously, in which the two
bots will play different roles; that is, the bot who plays the Landlord in one game will play the Peasants in another, and vice
versa. The two games form a match, and the Elo rating of each bot is updated according to the match outcome, as well as
their relative ratings.

The game score of a bot is mainly determined by whether she wins or loses a game. The winning bot receives a score of two
points, while the losing bot receives a score of zero11. To encourage more complicated card-playing strategies, Botzone
associates small points with categories of the cards played by a bot and adds that to the game score. The total game score is
thus the winning point (2 for the winner and 0 for the loser) plus the card-playing advantage, the sum of weights assigned
according to categories of cards (Table 14) played by a bot in the entire game divided by 100. Since by convention, two
Peasants always receive the same game score, Peasants’ game score is the average of their individual scores.

The match score is determined by the sum of game scores of the two games in the round, which further determines how the
Elo rating will change for each player. If the match score of one bot is higher than the other, then the bot is considered the
winner of this match. The winning bot will receive an increase in its Elo rating, while the same amount of rating points will
be taken off from the losing bot.

E.3. Discussion of Ranking Stability

Although Elo rating is generally considered a stable measurement of relative strength among a pool of players in games like
Chess and Go, DouDizhu Elo ranking on Botzone suffers from some ﬂuidity. This could be attributed to the nature of the
high variance of the game and also the design of Botzone Elo. Firstly, the game outcomes of DouDizhu relies on the luck of

10https://wiki.botzone.org.cn/index.php?title=%E9%A6%96%E9%A1%B5/en
11https://wiki.botzone.org.cn/index.php?title=FightTheLandlord

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Table 14. Summary of weights assigned to each category in Botzone.

Action Type

Weight

Solo
Pair
Trio (or with Solo / Pair)
Chain of Solo
Chain of Pair
Chain of Trio
Plane with Solo / Pair
Quad with Solo / Pair
Space shuttle A (2 consecutive Quad)
Bomb
Rocket
Space shuttle B (more than 3 consecutive Quads)
Pass

1
2
4
6
6
8
8
8
10
10
16
20
0

initial hand cards. In particular, if a player with bad hand cards is playing the Landlord, he will have a low chance to win a
game. In practice, the bidding phase could compensate for this randomness, such that the player with bad initial hand cards
can choose not to bid for the Landlord. However, Botzone does not incorporate the bidding phase into the game playing;
rather, the Landlord position is speciﬁed even before the dealing phase happened. Secondly, although two bots exchange
roles between games in each match, these two games are not initialized with the same hand cards. It is not rare to see that
one bot was assigned bad initial hand cards in both games, making it infeasible for her to win the match. Finally, Elo rating
games are not scheduled as frequently on Botzone, potentially due to limited server resources. We observe that, on average,
DouZero has the chance to play one Elo rating game about every 2 hours. As such, it might take a long time for a bot to
achieve a stable ranking. With bots continuously added to or leaving the Elo system, it might be just impossible to observe
absolute stable ranking. Nonetheless, since ranked top on Botzone for the ﬁrst time on October 30, 2020, DouZero has
remained in the top-5 most of the time (at the time of submission deadline, DouZero was still ranked ﬁrst with around 1600
points). While the rank of DouZero is impacted by the high variance of the BotZone platform, DouZero has maintained
an Elo rating score of at least 1480 points during the months between October 30, 2020, to the ICML submission deadline,
suggesting that DouZero has at least 95% chance of winning in a match with an average bot.

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

F. Additional Case Studies

In this section, we conduct case studies for DouZero. We show both per-step decision with ﬁgures and the logs of full
games. For simplicity, we use “T” to denote “10”,“P” to denote “PASS”, “B” to denote Black Joker, and “R” to denote Red
Joker. Each move is represented as “position:move”, where position can be “L” for Landlord, “U” for LandlordUp (i.e.,
the Peasant that moves before the Landlord), “D” for LandlordDown (i.e., the Peasant that moves after the Landlord). For
example, ”L:3555” means Landlord plays 3555, and “U:T” means LandlordUp plays 10. The initial hands are represented
as “H:Landlord Hand; LandlordDown Hand; LandlordUp Hand”. The moves and the hands are separated by “,”. Note that
except Section F.4, we focus on the agent with WP as objective. Thus, the agents tend to ambitiously play bombs
even when they will lose, and will not try to play more bombs when they think they will win.

F.1. Strategic Thinking of the Agents

Figure 21. Case 1: Strategic thinking (turn 7). The LandlordUp has 4444 in hand. However, DouZero strategically chooses
to break the bomb and play 45678. This is because playing this Chain of Solo can empty the hand more quickly. Full game:
H:333456778889TJJKAA2R; 355667999TTJKKA2B; 4445678TJQQQQKA22, L:45678, D:P, U:TJQKA, L:P, D:P, U:45678, L:789TJ,
D:P, U:P, L:3338, D:999J, U:4QQQ, L:P, D:P, U:22, L:P, D:P, U:4.

Figure 22. Case 2: Strategic thinking (turn 1). In this case, 56789TJQ is a very good move because the agent can play another Chain of
Solo afterwards, i.e., TJQKA. Full game: H:333356778889TTJJQQKA; 44599TTJQQKKAA22R; 44556667789JKA22B, L:56789TJQ,
D:P, U:P, L:TJQKA, D:P, U:P, L:7, D:J, U:K, L:P, D:P, U:44, L:88, D:99, U:22, L:3333.

PASSPASSTop-3 MovesLandlordDown (DouZero)LandlordLandlordUp (DouZero)0.7310.5540.554Top-3 MovesLandlordUpLandlordDownLandlord (DouZero)-0.148-0.852-0.852DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Figure 23. Case 3: Strategic thinking (turn 31). The Landlord has two pairs (AA and TT) and a small Solo in hand. DouZero chooses not
to play AA in this turn. This is a nice move because there is a pair of K out there. If the Landlord plays AA, then the TT will be dominated
by the KK in later turns. DouZero patiently chooses PASS and wins the game eventually. Full game: H:33455556889TTJAA22BR;
334677778TTJQKA22; 44668999JJQQQKKKA, L:33, D:22, U:P, L:BR, D:P, U:P, L:6, D:8, U:A, L:2, D:7777, U:P, L:P, D:TJQKA, U:P,
L:5555, D:P, U:P, L:9, D:T, U:P, L:J, D:P, U:K, L:2, D:P, U:P, L:88, D:P, U:JJ, L:P, D:P, U:44999, L:P, D:P, U:66QQQ, L:P, D:P, U:KK,
L:AA, D:P, U:P, L:TT, D:P, U:P, L:4.

Figure 24. Case 4: Strategic thinking (turn 13). When the LandlordUp plays 333J, DouZero chooses not to play 888. This is a nice
move because playing 888 will break a Chain of Solo, i.e., 456789T. Full game: H:45567788899TQKKKK22B; 34557789JJJQQAAA2;
333446669TTTJQA2R, L:7, D:8, U:9, L:Q, D:2, U:P, L:B, D:P, U:R, L:P, D:P, U:333J, L:P, D:P, U:666Q, L:P, D:P, U:TTTA, L:P, D:P,
U:44, L:55, D:77, U:P, L:88, D:QQ, U:P, L:22, D:P, U:P, L:49KKKK, D:P, U:P, L:6789T.

PASSTop-3 MovesLandlordLandlordDownLandlord (DouZero)0.731-1.175PASSPASSTop-3 MovesLandlordUpLandlordDownLandlord (DouZero)-0.339-0.823-0.843PASSDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Figure 25. Case 5: Strategic thinking (turn 4). DouZero can choose a move from many possible legal moves. The top-1 move is nice
because it is a very long Chain of Solo. The second and the third moves are also very good because they choose a good kicker. Full game:
H:34446667899TJQQKA22R; 3455788TJJQKAA22B; 3355677899TTJQKKA, L:3444, D:P, U:P, L:789TJQKA, D:P, U:P, L:6669, D:P,
U:P, L:22, D:P, U:P, L:Q, D:B, U:P, L:R.

F.2. Cooperation of Peasants

Figure 26. Case 1: Cooperation of Peasants (turn 29). Although the Landlord has much better hand than the Peasants (a Bomb
plus a Rocket), the Peasants manage to win the game with cooperation.
In this turn, the LandlordUp chooses PASS so that the
LandlordDown can empty her hand. DouZero has learned not to ﬁght against the teammate. Full game: H:3335556799JJJJQK22BR;
44445677899TQKKAA; 3667888TTTQQKAA22, L:3336, D:P, U:3888, L:JJJJ, D:P, U:P, L:5557, D:P, U:7TTT, L:BR, D:P, U:P, L:99,
D:AA, U:22, L:P, D:P, U:AA, L:22, D:4444, U:P, L:P, D:Q, U:K, L:P, D:P, U:66, L:P, D:P, U:QQ.

PASSPASSTop-3 MovesLandlordUpLandlordDownLandlord (DouZero)0.8710.8470.768PASSTop-3 MovesLandlordLandlordDown (DouZero)LandlordUp (DouZero)1.0260.5840.327PASSDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Figure 27. Case 2: Cooperation of Peasants (turn 33). The LandlordUp plays a T. DouZero chooses PASS because there is no card out
there larger than T in rank. This suggests that DouZero has learned to reason about the cards that have not been played. Full game:
H:334444556689TTJJJQ2R; 3577889TQQKKKKAA2; 356677899TJQAA22B, L:33444455, D:7788KKKK;, U:P, L:P, D:3, U:J, L:2,
D:P, U:B, L:R, D:P, U:P, L:89TJQ, D:P¡ U:P, L:66, D:QQ, U:P, L:P, D:5, U:2, L:P, D:P, U:99, L:JJ, D:AA, D:P, L:P, D:2, U:P, L:P, D:T,
U:P, L:P, D:9.

Figure 28. Case 3: Cooperation of Peasants (turn 36). The LandlordDown plays a Trio with Solo. While LandlordUp has a very
good hand and can win the game by itself, DouZero chooses PASS to let her teammate win. Full game: H:3344566788999JQK222;
34577788TJJQQQKAA; 3455669TTTKKAA2BR; L:45678, D:P, U:P, L:4999, D:5QQQ, U:P, L:6222, D:P, U:BR, L:P, D:P, U:55, L:JJ,
D:P, U:KK, L:P, D:P, U:9, L:Q, D:K, U:A, L:P, D:P, U:3, L:8, D:A, U:P, L:P, D:88, U:P, L:P, D:A, U:P, L:P, D:4777, U:P, L:P, D:3, U:T,
L:K, D:P, U:A, L:P, D:P, U:T, L:P, D:P, U:T, L:P, D:J, U:P, L:P, D:J, U:P, L:P, D:T.

F.3. Comparison of the Models in Early Stage and Later Stage

Table 15. Case 1: Comparison of DouZero in early stage and later stage. DouZero plays the Landlord position on the same deck with
the same opponents. DouZero loses in the early stage but wins the game in the later stage. DouZero in the later stage tends to perform
a better planning of the hand. Speciﬁcally, DouZero in the later stage tends to ﬁrst play small pairs, such as 88 and TT, so that it can
easily empty the hand later.

Logs

Early stage

Later stage

H:455557777889TTKKAA22; 3446668999QQKA2BR; 333468TTJJJJQQKA2, L:88, D:QQ, U:P, L:KK,
D:P, U:P, L:455559, D:P, U:46JJJJ, L:P, D:P, U:3338, L:P, D:3666, U:P, L:P, D:44999, U:P, L:P, D:8, U:K,
L:A, D:2, U:P, L:P, D:K, U:P, L:A, D:BR, U:P, L:P, D:A

H:455557777889TTKKAA22; 3446668999QQKA2BR; 333468TTJJJJQQKA2, L:TT, D:QQ, U:P, L:KK,
D:P, U:P, L:88, D:99, U:TT, L:AA, D:P, U:P, L:477779, D:P, U:46JJJJ, L:P, D:P, U:3338, L:5555, D:BR,
U:P, L:P, D:3666, U:P, L:P, D:8, U:K, L:2, D:P, U:P, L:2

PASSTop-3 MovesLandlordUpLandlordLandlordDown (DouZero)0.838-0.814-0.824PASSPASSTop-3 MovesLandlordDown (DouZero)LandlordLandlordUp (DouZero)0.9900.9790.974PASSDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Table 16. Case 2: Comparison of DouZero in early stage and later stage. DouZero plays the Landlord position on the same deck with
the same opponents. In the early stage, DouZero keeps playing PASS because it does not want to break the Rocket. As result, DouZero
loses the game. In the later stage, DouZero smartly breaks the Rocket to dominate other Solos, and eventually wins the game.

Logs

Early stage

Later stage

H:3355556778889JJQKABR; 344446679JJQQKA22; 367899TTTTQKKAA22, L:9, D:K, U:P, L:A, D:2,
U:P, L:P, D:3, U:9, L:K, D:A, U:P, L:P, D:4, U:Q, L:P, D:P, U:6789T, L:P, D:P, U:3TTT, L:P, D:P, U:KK,
L:P, D:P, U:AA, L:P, D:P, U:22

H:3355556778889JJQKABR; 344446679JJQQKA22; 367899TTTTQKKAA22, L:33, D:66, U:KK, L:P,
D:P, U:6789T, L:P, D:P, U:3TTT, L:P, D:P, U:9, L:Q, D:K, U:P, L:A, D:2, U:P, L:P, D:3, U:Q, L:K, D:A,
U:P, L:B, D:P, U:P, L:6, D:7, U:A, L:R, D:P, U:P, L:555577JJ, D:P, U:P, L:8889

Table 17. Case 3: Comparison of DouZero in early stage and later stage. DouZero plays the Landlord position on the same deck with
the same opponents. In the early stage, the ﬁrst move of DouZero is 9. However there is a 4 in the hand, which causes troubles in later
turns. In the later stage, DouZero plays in a different style by starting with 33 and ﬁnally wins the game. Although playing 9 seems to
be not bad, it may lead to losing the game later.

Logs

Early stage

Later stage

H:33455556889TTJAA22BR; 334677778TTJQKA22; 446688999JJQQQKKKA, L:9, D:T, U:A, L:2,
D:7777, U:P, L:P, D:TJQKA, U:P, L:5555, D:P, U:P, L:6, D:8, U:P, L:J, D:2, U:P, L:R, D:P, U:P, L:88,
D:P, U:JJ, L:AA, D:P, U:P, L:33, D:P, U:66, L:TT, D:P, U:KK, L:P, D:P, U:8999, L:P, D:P, U:QQQK, L:P,
D:P, U:44

H:33455556889TTJAA22BR; 334677778TTJQKA22; 446688999JJQQQKKKA, L:33, D:22, U:P, L:BR,
D:P, U:P, L:6, D:8, U:A, L:2, D:7777, U:P, L:P, D:TJQKA, U:P, L:5555, D:P, U:P, L:9, D:T, U:P, L:J, D:P,
U:K, L:2, D:P, U:P, L:88, D:P, U:JJ, L:P, D:P, U:44999, L:P, D:P, U:66QQQ, L:P, D:P, U:KK, L:AA, D:P, U:P,
L:TT, D:P, U:P, L:4

Table 18. Case 4: Comparison of DouZero in early stage and later stage. DouZero plays the LandlordUp and LandlordDown positions
on the same deck with the same opponent. While DouZero wins both games. DouZero in the later stage looks more reasonable. In turn
16, DouZero in the early stage chooses to break a pair of 2, which is hard to be explained. DouZero in the later stage tends to play
more smoothly and wins the games quickly.

Logs

Early stage

Later stage

H:3455567788TJJQKKKKA2; 33446678899JQQQA2; 3456799TTTJAA22BR, L:345678, D:P, U:P,
L:TJQKA, D:P, U:P, L:55KKK, D:P, U:P, L:7, D:P, U:R, L:P, D:P, U:2, L:P, D:P, U:34567, L:P, D:P, U:J,
L:2, D:P, U:B, L:P, D:P, U:2, L:P, D:P, U:AA, L:P, D:P, U:99TTT

H:3455567788TJJQKKKKA2; 33446678899JQQQA2; 3456799TTTJAA22BR, L:345678, D:P, U:P,
L:TJQKA, D:P, U:P, L:55KKK, D:P, U:BR, L:P, D:P, U:99TTT, L:P, D:P, U:34567, L:P, D:P,
U:AA, L:P, D:P, U:22, L:P, D:P, U:J

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

F.4. Comparison of Using WP and ADP as Objectives

(a) WP (turn 13)

(b) ADP (turn 10)

Figure 29. Case 1: Comparison of using WP and ADP as objectives. The two agents play the same deck twice. It is difﬁcult for the
Landlord to win this game since the LandlordUp has a very good hand. When LandlordUp plays a Plane with Solo, WP agent tends to
ambitiously play bombs because playing bombs has no cost. However, ADP agent tends to be very cautious of playing bombs since it may
lead to a larger loss of ADP. Full game of (a): H:3344445666689JQQKK22; 3556688TJJJQQKKA2; 35668999TTTAAA2BR, L:33,
D:55, U:66, L:QQ, D:KK, U:P, L:22, D:P, U:BR, L:P, D:P, U:58999TTT, L:7777, D:P, U:P, L:8, D:T, U:2, L:P, D:P, U:3AAA. Full game
of (b): H:3344445666689JQQKK22; 3556688TJJJQQKKA2; 35668999TTTAAA2BR, L:33, D:55, U:66, L:KK, D:P, U:AA, L:P, D:P,
U:35999TTT, L:P, D:P, U:8, L:J, D:A, U:2, L:P, D:P, U:BR, L:P, D:P, U:A.

(a) WP (turn 13)

(b) ADP (turn 25)

Figure 30. Case 2: Comparison of using WP and ADP as objectives. The two agents play the same deck twice. While both agents
win this game, they have different styles. The WP agent plays Quad with Solo instead of a bomb because it can empty the hand
more quickly. This is reasonable since playing one more bomb will not double WP. In contrast, the ADP agent ﬁrst plays a 2 so
that it can play a bomb later. The ADP agent will try to play every bomb when it thinks it can win the game. Full game of (a):
H:455557777889TTKKAA22; 3446668999QQKA2BR; 333468TTJJJJQQKA2, L:TT, D:QQ, U:P, L:KK, D:P, U:P, L:88, D:99, U:TT,
L:AA, D:P, U:P, L:477779, D:P, U:46JJJJ, L:P, D:P, U:3338, L:5555, D:BR, U:P, L:P, D:3666, U:P, L:P, D:8, U:K, L:2, D:P, U:P, L:2. Full
game of (b): H:455557777889TTKKAA22; 3446668999QQKA2BR; 333468TTJJJJQQKA2, L:88, D:QQ, U:P, L:KK, D:P, U:P, L:TT,
D:P, U:QQ, L:AA, D:P, U:P, L:9, D:K, U:P, L:2, D:B, U:P, L:P, D:3666, U:4JJJ, L:7777, D:P, U:P, L:2, D:R, U:P, L:5555, D:P, U:P, L:4.

PASSTop-3 MovesLandlordUpLamdlordDownLandlord (DouZero)-0.934-0.940-0.949PASSPASSTop-3 MovesLandlordUpLandlordDownLandlord (DouZero)-0.663-6.906-6.906PASSPASSPASSTop-3 MovesLandlordUpPeasantDownLandlord (DouZero)0.9490.9300.843PASSPASSTop-3 MovesLandlordUpLandlordDownLandlord (DouZero)3.7943.5111.883DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

(a) WP (turn 34)

(b) ADP (turn 43)

Figure 31. Case 3: Comparison of using WP and ADP as objectives. The two agents play the same deck twice. In this game, the Landlord
is very likely to win. The WP agent chooses a more conservative move by playing Quad with Pair to quickly empty the hand. In contrast,
the ADP agent plays a small Solo ﬁrst so that it can play 5555 later to double the ADP. Full game of (a): H:3355556778889JJQKABR;
344446679JJQQKA22; 367899TTTTQKKAA22, L:33, D:66, U:KK, L:P, D:P, U:6789T, L:P, D:P, U:3TTT, L:P, D:P, U:9, L:Q, D:K, U:P,
L:A, D:2, U:P, L:P, D:3, U:Q, L:K, D:A, U:P, L:B, D:P, U:P, L:6, D:7, U:A, L:R, D:P, U:P, L:555577JJ, D:P, U:P, L:8889. Full game of
(b): H:3355556778889JJQKABR; 344446679JJQQKA22; 367899TTTTQKKAA22, L:33, D:66, U:KK, L:P, D:P, U:6789T, L:P, D:P,
U:3TTT, L:P, D:P, U:9, L:Q, D:K, U:P, L:A, D:2, U:P, L:P, D:3, U:Q, L:K, D:A, U:P, L:P, D:4, U:A, L:P, D:P, U:A, L:R, D:P, U:P, L:9,
D:2, U:P, L:B, D:P, U:P, L:77888, D:P, U:P, L:6, D:7, U:2, L:5555, D:P, U:P, L:JJ.

F.5. Bad Cases

Figure 32. Case 1: Bad case (turn 21). The Landlord plays a 2 with only 3 cards left. While the LandlordUp has Black Joker in hand,
DouZero chooses not to play it. Although the Peasants will lose whatever the LandlordUp plays in this speciﬁc case, playing the Black
Joker should have a larger chance to win (if the Red Joker happens to be in the hand of the LandlordDown). Thus, it is worth a try to play
Black Joker. Full game: H: 3556788TQQQKKKAAA22R; 344455677999TTQK2; 334667889TJJJJA2B; L:55, D:P, U:P, L:88, D:P, U:P,
L:3AAA, D:P, U:P, L:7KKK, D:P, U:P, L:6QQQ, D:P, U:JJJJ, L:P, D:P, U:3, L:2, D:P, U:P, L:T, D:K, U:A, L:2, D:P, U:B, L:R.

PASSPASSTop-3 MovesLandlordUpLandlordDownLandlord (DouZero)0.4900.1400.012PASSPASSTop-3 MovesLandlordUpLandlordDownLandlord (DouZero)1.8751.0561.036PASSTop-3 MovesLandlord Down (DouZero)Landlord (DouZero)Landlord Up (DouZero)-0.704-0.800PASSDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Figure 33. Case 2: Bad case (turn 22). This could be a bad case. DouZero aggressively chooses Black Joker as the kicker instead of
K. While it is true that DouZero can win whichever kicker it chooses, choosing Black Joker is risky because this knowledge could be
generalized to other cases with neural networks and results in losing a game. In fact, choosing K as the kicker will always be at least as
good as Black Joker given the current hand. Full game: H:333578889TJQKKAAA22B; 444455779JJJQKA2R; 3566667899TTTQQK2,
L:3335, D:9JJJ, U:P, L:7AAA, D:4444, U:P, L:P, D:Q, U:K, L:P, D:P, U:3TTT, L:P, D:P, U:56789, L:9TJQK, D:P, U:P, L:22, D:P, U:P,
L:888B, D:P, U:P, L:K.

PASSPASSTop-3 MovesLandlordUpLandlordDownLandlord (DouZero)0.9790.949-0.164DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

F.6. Randomly Selected (rather than Cherry-Picked) Full Games

Table 19. Case 1: Randomly selected (rather than cherry-picked) full self-play games of DouZero. DouZero plays the Landlord,
LandlordUP and LandlordDown positions. All DouZero agents were trained using WP as objectives.

Logs

H:335556788899TTJKKA2R; 6677789TTJJQQKA2B; 334444569JQQKAA22, L:33, D:66, U:QQ, L:KK,
D:P, U:AA, L:P, D:P, U:9, L:T, D:A, U:P, L:2, D:B, U:P, L:P, D:9, U:J, L:A, D:2, U:P, L:R, D:P, U:P, L:5559,
D:777J, U:P, L:P, D:J, U:K, L:P, D:P, U:5, L:6, D:8, U:4444, L:P, D:P, U:33, L:88, D:TT, U:22, L:P, D:P, U:6

H:34455777889JQQKA222B; 345668899TTTJQKAA; 33456679TJJQKKA2R, L:44, D:P, U:JJ, L:QQ,
D:AA, U:P, L:P, D:3, U:T, L:J, D:P, U:Q, L:K, D:P, U:A, L:P, D:P, U:9, L:A, D:P, U:2, L:B, D:P, U:P, L:3,
D:4, U:6, L:9, D:Q, U:P, L:2, D:P, U:R, L:P, D:P, U:34567, L:P, D:P, U:KK, L:22, D:P, U:P, L:77788, D:66TTT, U:P, L:P,
D:J, U:P, L:P, D:9, U:P, L:P, D:9, U:P, L:P, D:8, U:P, L:P, D:5, U:P, L:P, D:K, U:P, L:P, D:8

H:34455666789TTJQQKA2R; 335688899TJQA222B; 34457779TJJQKKKAA, L:445566, D:P, U:P, L:6789T,
D:89TJQ, U:9TJQK, L:TJQKA, D:P, U:P, L:Q, D:A, U:P, L:2, D:B, U:P, L:R, D:P, U:P, L:3

H:334447779TTTJQKKK22R; 3345566678TJQKA2B; 556888999JJQQAAA2, L:33444, D:55666, U:888JJ, L:TTT22,
D:P, U:55AAA, L:P, D:P, U:QQ, L:P, D:P, U:6999, L:QKKK, D:P, U:P, L:R, D:P, U:P, L:777J, D:P, U:P, L:9

H:455577899TTJJKKAA22B; 3346789TJJQQAA22R; 334456667889TQQKK, L:4, D:P, U:K, L:A,
D:2, U:P, L:P, D:6789TJ, U:P, L:P, D:4, U:Q, L:A, D:2, U:P, L:B, D:R, U:P, L:P, D:33, U:88, L:TT, D:QQ, U:P, L:KK,
D:AA, U:P, L:22, D:P, U:P, L:JJ, D:P, U:P, L:77, D:P, U:P, L:99, D:P, U:P, L:5558

H:3345666889JQQQKAA22B; 4457777899TJQAA2R; 33455689TTTJJKKK2, L:5, D:9, U:J, L:K,
D:2, U:P, L:B, D:R, U:P, L:P, D:89TJQ, U:P, L:P, D:44, U:P, L:88, D:AA, U:P, L:22, D:7777, U:P, L:P, D:5

H:334679999TTQQKKKAA2B; 3455677888JQKA22R; 344556678TTJJJQA2, L:33, D:55, U:TT, L:QQ,
D:P, U:P, L:TT, D:P, U:JJ, L:AA, D:22, U:P, L:P, D:4, U:Q, L:2, D:P, U:P, L:469999, D:P, U:P, L:7KKK, D:P, U:P, L:B

H:3455667899TTTJJQQKAB; 33567788JJQQKA22R; 3444567899TKKAA22, L:6, D:K, U:A, L:B,
D:P, U:P, L:34567, D:P, U:56789, L:89TJQ, D:P, U:P, L:5, D:6, U:A, L:P, D:P, U:T, L:A, D:2, U:P, L:P, D:5, U:9, L:J,
D:P, U:K, L:P, D:P, U:K, L:P, D:P, U:22, L:P, D:P, U:3444

H:345578TTJJJQKKKA2222; 3334566667789QQKR; 445788999TTJQAAAB, L:3, D:8, U:Q, L:2,
D:P, U:B, L:P, D:P, U:J, L:Q, D:K, U:P, L:A, D:P, U:P, L:4, D:9, U:T, L:P, D:P, U:88, L:TT, D:QQ, U:P, L:P,
D:4, U:9, L:2, D:6666, U:P, L:P, D:5, U:9, L:K, D:R, U:P, L:P, D:33377

H:3334577889TTJQKAAA2B; 345566789TJQQKKA2; 445667899TJJQK22R, L:5, D:6, U:K, L:P,
D:A, U:P, L:2, D:P, U:P, L:T, D:K, U:P, L:B, D:P, U:R, L:P, D:P, U:456789TJQ, L:P, D:P, U:4, L:K, D:2, U:P, L:P,
D:3, U:9, L:A, D:P, U:2, L:P, D:P, U:J, L:A, D:P, U:2, L:P, D:P, U:6

DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning

Table 20. Case 2: Randomly selected (rather than cherry-picked) full games of DouZero played with other agents in Botzone. DouZero
plays the position as noted in the left column. All DouZero agents were trained using WP as objectives.

Position

Logs

Landlord

H:33455778999TTTQKKAA2; 34567788TJQKKA2BR; 344566689JJJQQA22, L:48999TTT, D:BR,
U:P, L:P, D:34567, U:P, L:P, D:TJQKA, U:P, L:P, D:88, U:QQ, L:KK, D:P, U:22, L:P, D:P, U:3666,
L:P, D:P, U:5JJJ, L:P, D:P, U:44, L:AA, D:P, U:P, L:2, D:P, U:P, L:77, D:P, U:P, L:33,
D:P, U:P, L:55, D:P, U:P, L:Q

Peasants

H:34456779TTJKKKAAAA22; 34455668888JQQQ2R; 335677999TTJJQK2B, L:34567, D:P,
U:9TJQK, L:P, D:P, U:T, L:J, D:Q, U:P, L:A, D:2, U:P, L:P, D:445566, U:P, L:P, D:38888Q, U:P,
L:P, D:Q, U:P, L:P, D:R, U:P, L:P, D:J

Landlord

Peasants

Peasants

Landlord

H:334455668899TJQQAA2B; 34567777TJJKKKAA2; 34568899TTJQQK22R, L:33445566, D:P,
U:P, L:9, D:T, U:P, L:Q, D:A, U:P, L:P, D:777JJ, U:P, L:P, D:34567, U:P, L:89TJQ, D:P, U:9TJQK,
L:P, D:P, U:3, L:8, D:A, U:P, L:2, D:P, U:R, L:P, D:P, U:88, L:AA, D:P, U:22, L:P,
D:P, U:4, L:B

H:34455677889TTJJQKK2R; 33557899JQQQKKA22; 344666789TTJAAA2B, L:345678, D:P,
U:6789TJ, L:789TJQ, D:P, U:P, L:4, D:7, U:A, L:2, D:P, U:B, L:R, D:P, U:P, L:5, D:A, U:P,
L:P, D:JQQQ, U:P, L:P, D:55, U:P, L:KK, D:22, U:P, L:P, D:K, U:A, L:P, D:P, U:A, L:P,
D:P, U:3, L:T, D:P, U:2, L:P, D:P, U:66, L:P, D:P, U:44, L:P, D:P, U:T

H:3355667789TJJQA222BR; 334446788TQQKKAA2; 455678999TTJJQKKA, L:33, D:88,
U:TT, L:P, D:P, U:45678, L:89TJQ, D:P, U:P, L:556677, D:QQKKAA, U:P, L:P, D:4, U:A, L:2, D:P, U:P,
L:J, D:P, U:Q, L:A, D:P, U:P, L:2, D:P, U:P, L:2, D:P, U:P, L:BR

H:334566778899TTQQKA2B; 3455689TJJQKKAA22; 344567789TJJQKA2R, L:456789T, D:89TJQKA,
U:P, L:P, D:3, U:4, L:K, D:A, U:2, L:P, D:P, U:34567, L:6789T, D:P, U:P, L:33, D:55, U:P,
L:QQ, D:22, U:P, L:P, D:4, U:J, L:B, D:P, U:R, L:P, D:P, U:789TJQKA

Landlord

H:33455668TTTJJJQKAA22; 3445677789QQKKA2R; 3456788999TJQKA2B, L:4, D:A,
U:P, L:P, D:4, U:9, L:Q, D:K, U:2, L:P, D:P, U:89TJQKA, L:P, D:P, U:B, L:P, D:P, U:3456789

