8
1
0
2
c
e
D
5

]
I

A
.
s
c
[

1
v
3
4
8
1
0
.
2
1
8
1
:
v
i
X
r
a

MLIC: A MaxSAT-Based framework for learning
interpretable classiﬁcation rules(cid:63)

Dmitry Malioutov1 and Kuldeep S. Meel2

1 T.J. Watson IBM Research center
dmal@alum.mit.edu
2 School of Computing, National University of Singapore
meel@comp.nus.edu.sg

Abstract. The wide adoption of machine learning approaches in the
industry, government, medicine and science has renewed the interest in
interpretable machine learning: many decisions are too important to be
delegated to black-box techniques such as deep neural networks or kernel
SVMs. Historically, problems of learning interpretable classiﬁers, includ-
ing classiﬁcation rules or decision trees, have been approached by greedy
heuristic methods as essentially all the exact optimization formulations
are NP-hard. Our primary contribution is a MaxSAT-based framework,
called MLIC, which allows principled search for interpretable classi-
ﬁcation rules expressible in propositional logic. Our approach beneﬁts
from the revolutionary advances in the constraint satisfaction commu-
nity to solve large-scale instances of such problems. In experimental eval-
uations over a collection of benchmarks arising from practical scenarios
we demonstrate its eﬀectiveness: we show that the formulation can solve
large classiﬁcation problems with tens or hundreds of thousands of ex-
amples and thousands of features, and to provide a tunable balance of
accuracy vs. interpretability. Furthermore, we show that in many prob-
lems interpretability can be obtained at only a minor cost in accuracy.

The primary objective of the paper is to show that recent advances in
the MaxSAT literature make it realistic to ﬁnd optimal (or very high
quality near-optimal) solutions to large-scale classiﬁcation problems. We
also hope to encourage researchers in both interpretable classiﬁcation
and in the constraint programming community to take it further and
develop richer formulations, and bespoke solvers attuned to the problem
of interpretable ML.

1

Introduction

The last decade has witnessed an unprecedented adoption of machine learning
techniques to make sense of available data and make predictions to support de-
cision making for a wide variety of applications ranging from health-care analyt-
ics to customer churn predictions, movie recommendations and macro-economic

(cid:63) The names of authors are sorted alphabetically by last name and the order does not

reﬂect contribution

 
 
 
 
 
 
policy. The focus in the machine learning literature has been on increasingly so-
phisticated systems with the paramount goal of improving the accuracy of their
predictions at the cost of making such systems essentially black-box. While in
certain tasks such as ad predictions, accuracy is the main objective, in other
domains, e.g., in legal, medical, and government, it is essential that the human
decision makers who may not have been trained in machine learning can interpret
and validate the predictions [20,33].

The most popular interpretable techniques that tend to be adopted and
trusted by decision makers include classiﬁcation rules, decision trees, and deci-
sion lists [10,29,8,30]. In particular, decision rules with a small number of Boolean
clauses tend to be the most interpretable. Such models can be used both to learn
interpretable models from the start, and also as proxies that provide post-hoc
explanations to pre-trained black-box models [12,1].

On the theoretical front, the problem of rule learning was shown to be com-
putationally intractable [32]. Consequently, the earliest practical eﬀorts such as
decision list and decision tree approaches relied on a combination of heuristically
chosen optimization objectives and greedy algorithmic techniques, and the size
of the rule was controlled by either early stopping or ad-hoc rule pruning. Only
recently there have been some formulations that attempt to balance the accu-
racy and the size of the rule in a principled optimization objective either through
combinatorial optimization, linear programming (LP) relaxations, submodular
optimization, or Bayesian methods [4,25,24][7,34] as we review in Section 5.

Motivated by the signiﬁcant progress in the development of combinatorial
solvers (in particular, MaxSAT), we ask: can we design a combinatorial frame-
work to eﬃciently construct interpretable classiﬁcation rules that takes advantage
of these recent advances? The primary contribution of this paper is to present
a combinatorial framework that enables a precise control of accuracy vs. inter-
pretability, and to verify that the computational advances in the MaxSATcommunity
can make it practical to solve large-scale classiﬁcation problems.
In particular, this paper makes following contributions:

1. A MaxSAT-based framework, MLIC, that provably trades oﬀ accuracy vs.

interpretability of the rules

2. A prototype implementation of MLIC based on MaxSAT that is capable
of ﬁnding optimal (or high-quality near-optimal) classiﬁcation rules from
modern large-scale data-sets

3. We show that in many classiﬁcation problems interpretability can be achieved
at only a minor loss of accuracy, and furthermore, MLIC, which speciﬁcally
looks for interpretable rules, can learn from much fewer samples than black-
box ML techniques.

Furthermore, we hope to share our excitement with applications of constraint
programming/MaxSAT in Machine Learning, and to encourage researchers in
both interpretable classiﬁcation and in the CSP/SAT communities to consider
this topic further: both in developing new SAT-based formulations for inter-
pretable ML, and in designing bespoke solvers attuned to the problem of inter-
pretable ML.

The rest of the paper is organized as follows: We discuss notations and prelim-
inaries in Section 2. We then present MLIC, which is the primary contribution
of this paper, in Section 3 and follow up with experimental setup and results
over a large set of benchmarks in Section 4. We then discuss related work in
Section 5 and ﬁnally conclude in Section 7.

2 Preliminaries

We use capital boldface letters such as X to denote matrices while lower boldface
letters y are reserved for vectors/sets. For a matrix X, Xi represents i-th row of
X while for a vector/set y, yi represents i-th element of y.

Let F be a Boolean formula and b = {b1, b2, · · · bn} be the set of variables
appearing in F . A literal is a variable (bi) or its complement(¬bi). A satisfying
assignment or a witness of F is an assignment of variables in b that makes F
evaluate to true. If σ is an assignment of variables and bi ∈ b, we use σ(bi) to
denote the value assigned to bi in σ. F is in Conjunctive Normal Form (CNF)
if F := C1 ∧ C2 · · · Cm, where each clause Ci is represented as disjunction of
literals. We use |Ci| to denote the number of literals in Ci. For two vectors u
and v over propositional variable/constants, we deﬁne u∨v = (cid:87)
i(ui ∧vi), where
ui and vi denote variables/constants at i-th index of u and v respectively. In
this context, note that the operation ∧ between a variable and a constant follows
standard interpretation, i.e. 0 ∧ b = 0 and 1 ∧ b = b .

We consider standard binary classiﬁcation, where we are given a collection
of training samples {Xi, yi} where each vector Xi ∈ X contains valuation of the
features x = {x1, x2, · · · xm} for sample i, and yi ∈ {0, 1} is the binary label
for sample i. A classiﬁer R is a mapping that takes in a feature vector x and
return a class y, i.e. y = R(x). The goal is not only to design R to approximate
our training set, but also to generalize to unseen samples arising from the same
distribution. In this work, we restrict x and y to be Boolean3 and focus on
classiﬁers that can be expressed compactly in Conjunctive Normal Form (CNF).
We use Ci to denote the ith clause of R. Furthermore, we use |R| to denote the
sum of the counts of literals in all the clauses, i.e. |R| = Σi|Ci|.

In this work, we focus on weighted variant of CNF wherein a weight function
is deﬁned over clauses. For a clause Ci and weight function W (·), we use W (Ci) to
denote the weight of clause Ci. We say that a clause Ci is hard if W (Ci) = −∞,
otherwise Ci is called as soft clause. To avoid notational clutter, we overload
W (·) to denote the weight of an assignment or clause, depending on the context.
We deﬁne weight of an assignment σ as the sum of weight of clauses that σ does
not satisfy. Formally, W (σ) = Σi|σ(cid:54)|=CiW (Ci).

Given F and weight function W (·), the problem of MaxSAT is to ﬁnd an
assignment σ∗ that has the maximum weight, i.e. σ∗ = MaxSAT(F, W ) if ∀σ (cid:54)=
σ∗, W (σ∗) ≥ W (σ). Our formulation will have negative clause weights, hence
MaxSATcorresponds to satisfying as many clauses as possible, and picking the

3 We discuss in Section 3 that such a restriction can be achieved without loss of

generality

weakest clauses among the unsatisﬁed ones. Note that the above formulation
is diﬀerent from the typical deﬁnition of MaxSAT but the diﬀerence is only
syntactic. Borrowing terminology of community focused on developing MaxSAT
solvers, we are solving a partial weighted MaxSAT instance wherein we mark all
the clauses with −∞ weight as hard and negate weight of all the other clauses
and ask for a solution that optimizes the partial weighted MaxSAT formula. The
knowledge of inner working of MaxSAT solvers and encoding of our representation
into weighted MaxSAT is not required for this paper and we defer the details to
release of source code post-publication.

3 MLIC: MaxSAT-based Learning of Interpretable

Classiﬁers

We now discuss the primary technical contribution of this paper, MLIC: MaxSAT-
based Learning of Interpretable Classiﬁers. We ﬁrst describe a metric for inter-
pretability of CNF rules. Since our formulation employs binary features, we
discuss how non-binary features such as categorical and continuous features can
be represented as binary features. We then move on to formulate the problem
of learning interpretable classiﬁcation rules as a MaxSAT query and provide a
proof of its theoretical soundness regarding controlling sparsity of the rules. As
discussed in Section 5, prior work does not provide a sound procedure for con-
trolling sparsity and accuracy. We then discuss the representational power of our
CNF framework – in particular, we demonstrate that the proposed framework
generalizes to handle complex objective function and rules in forms other than
CNF.

3.1 Balancing Accuracy and Intrepretability

While in general interpretability may be hard to deﬁne precisely, in the context
of decision rules, an eﬀective proxy is merely the count of clauses or literals
used in the rule. Rules involving few clauses with few literals are natural for
humans to evaluate and understand, while complex rules involving hundreds of
clauses will not be interpretable even if the individual clauses are. In addition to
interpretability, such sparsity also controls model complexity and gives a handle
of the generalization error.4

First, suppose that there exists a rule R that perfectly classiﬁes all the ex-
amples, i.e. ∀i, yi = R(Xi). Among all possible functions that satisfy this we
would like to ﬁnd the most interpretable (sparse) one:

min
R

|R|

such that R(Xi) = yi, ∀i

Since most ML datasets do not allow perfect classiﬁcation, we introduce a
penalty on classiﬁcation errors. We balance the two terms by a parameter λ,

4 The framework proposed in this paper allows generalization to other forms of rules,

as we discuss in Section 3.6.

where large λ gives more accurate but more complex rules, and smaller λ gives
smaller rules at the cost of reduced accuracy. Let ER be the set of examples on
which our classiﬁer R makes an error, then our objective is5:

min
R

|R| + λ|ER|

such that R(Xi) = yi, ∀i /∈ ER

(1)

3.2 Discretization of Features

In our MaxSAT-based formulation, we focus on learning rules based on Boolean
variables. We do also allow categorical and continuous features for our classiﬁer,
which are pre-processed before being presented to the MaxSAT-formulation. To
handle categorical features one may use the common ‘one-hot’ encoding, where
a Boolean vector variable is introduced with the cardinality equal to the number
of categories. For example a categorical feature with values ’red’, ’green’, ’blue’
would get converted to three binary variables, which take values 100, 010, and
001 for the three categorical values. [[[ If you think this sentence is obvious –
please drop ]]]

For continuous features, we introduce discretization, by comparing feature
values to a collection of thresholds. The thresholds may be chosen for example
based on quantiles of their distribution, or alternatively, on uniform partition of
the range of feature values. Speciﬁcally, for a continuous feature xc we consider
a number of thresholds {τk} and deﬁne two separate Boolean features I[xc ≥ τk]
and I[xc < τk] for each τk. The number of thresholds may vary by feature. Thus,
each continuous feature is represented using a collection of 2q Boolean features,
where q is the number of thresholds.

In principle, one could use all the values occurring in the data as thresholds,
and this would be equivalent to the original continuous features. In practice,
however, such granularity is typically not necessary, and a handful of thresholds
could be used, e.g., age-groups for each 5 years to discretize a continuous age
variable. This typically leads to only a very minor (if any) loss in accuracy, and in
fact improves the presentations and understanding of the rules to human users.
In our experiments, we used 10 thresholds based on the quantiles of the feature
distribution (10-th, 20-th, ... 100-th percentile), unless the number of unique
values of the feature was less than 10, in which case we kept all of them.

We note that we could easily deﬁne arbitrary other Boolean functions of con-
tinuous or categorical variables within our framework. For example, categorical
variables with many possible values (e.g. states or countries) may be grouped
into more interpretable coarser units ( regions or continents). Such groupings
are application speciﬁc and wpuld typically require relevant domain knowledge.
They could perhaps be learned from data, but this is outside the scope of the
current paper.

5 Cost-sensitive classiﬁcation is deﬁned analogously by allowing a separate parameter

for false positives and false negatives.

3.3 Transformation to Max-SAT query

We now describe our Max-SAT formulation for learning interpretable rules.
MLIC takes in four inputs: (i) a (0,1)-matrix X of dimension n × m describing
values of all m features for n samples with Xi corresponding to feature vector
x = {x1, x2, · · · xm} for sample i, (ii) (0,1)-vector y containing class labels yi
for sample i, (iii) k, the desired number of clauses in CNF rule, (iv) the regu-
larization parameter λ. Consequently, MLIC constructs a MaxSAT query and
invokes a MaxSAT solver to compute the underlying rule R as we now describe.
The key idea of MLIC is to deﬁne a MaxSAT query over k × m propositional
variables, denoted by {b1
1 · · · bm
k }, such that every truth assignment σ
deﬁnes a k-clause CNF rule R, where feature xj appears in clause Ri if σ(bj
i ) =
1. Corresponding to every sample i, we introduce a noise variable ηi that is
employed to distinguish whether the labeling for sample i should be considered
as noise or not. Let Bi = {bj

1, · · · bm

1, b2

i | j ∈ [m]}.

The Max-SAT query constructed by MLIC consists of the following three

sets of constraints:

W (Ni) = −λ
1. Ni := (¬ηi);
(cid:17)
(cid:16)
2. V j
V j
= −1
W
i );
i
i
3. Di := (¬ηi → (yi ↔ (cid:86)k

:= (¬bj

l=1(Xi ∨ Bl))); W (Di) = −∞

Please refer to Section 2 for the interpretation of (Xi ∨ Bj). Finally, the set of
constraints Qk constructed by MLIC is deﬁned as follows:

Qk :=

n
(cid:94)

i=1

Ni ∧

i=k,j=m
(cid:94)

i=1,j=1

V j
i ∧

n
(cid:94)

i=1

Di

(2)

Note that the elements of Xi and yi are not variables but constants whose
values (0 or 1) are provided as inputs. Therefore, the set of variables for Qk
is {η1, η2, · · · , ηn, b1
k }. We now explain the intuition behind the
design of Qk.

1 · · · bm

1, · · · bm

1, b2

We assign a weight of −λ to every Ni as we would like to satisfy as many
Ni, i.e. falsify as many ηi as possible. Similarly, we assign a weight of −1 to
every clause V j
i as we are, again, interested in sparse solutions (i.e., ideally, we
would prefer as many V j
i to be satisﬁed as possible). Every clause Di can be
read as follows: if ηi is assigned to false, i.e. sample i is not considered as noise,
then yi = R. As noted in Section 2, equivalent representation of the W (·), as
described above, for MaxSAT solvers involves usage of hard clauses.

Next, we extract R from the solution of Qk as follows.

Construction 1 Let σ∗ = MaxSAT(Qk, W ), then xj ∈ Ri iﬀ σ∗(bj

i ) = 1.

Before proceeding further, it is important to discuss CNF encodings for the
above sets of constraints. The constraints arising from Ni and Vi are unit clauses
and do not require further processing. Furthermore, note that yi is already

known and is a constant. Therefore, when yi is 1, the constraint Di can be
directly encoded as CNF by using equivalence of (a → b) ≡ (¬a ∨ b). Finally,
when yi is 0, we use Tseitin encoding wherein we introduce an auxiliary variable
zj
i corresponding to each clause (Xi ∨ Bj). Formally, we replace Di := (¬ηi →
j=1 ¬(Xi∨Bj))) with (cid:86)k
((cid:87)k
j zj
i →
¬(Xi ∨ Bj). Furthermore, W
= −∞. The following lemma establishes the
theoretical soundness of parameter λ.

j=0 Dj
i where D0
(cid:16)
Dj
i

i := (¬ηi → (cid:87)

i )), and Dj

i := (zj

(cid:17)

Lemma 1. For all λ2 > λ1 > 0, if R1 ← MLIC(X, y, k, λ1) and R2 ←
MLIC(X, y, k, λ2), then |R1| ≤ |R2| and ER1 ≥ ER2.

Proof. First, note that construction of Qk depends only on X and y. Further-
more, the parameter λ inﬂuences only the associated weight function. We denote
weight functions corresponding to λ1 and λ2 as Wλ1 and Wλ2 respectively. Fur-
thermore, let σ1 = MaxSAT(Qk, Wλ1 ) and σ2 = MaxSAT(Qk, Wλ1). If σ1 = σ2,
the lemma trivially holds. We now complete proof by contradiction argument
for the case when σ1 (cid:54)= σ2.

Let |R1| > |R2|. As σ1 (cid:54)= σ2, we have Wλ2 (σ1) ≤ Wλ2 (σ2). Since Wλ(σ) =
|R| + λER, where R is extracted from σ as stated above. Therefore, we have
λ2(ER2 − ER1) ≥ |R1| − |R2|. But we also have Wλ1 (σ1) ≤ Wλ1(σ2), which
implies that λ1(ER2 − ER1) ≤ |R1| − |R2|. Since λ1 > λ2, we have contradiction.
Therefore, it must be the case that |R1| ≤ |R2|.

3.4 Illustrate Example

We illustrate our encoding with the help of a toy example. Let n = 2, m = 3, k =

2 and X =

 and y =







1 0 1

0 1 1



0
. Then we have following clauses:

1

N1 := (¬η1);

N2 := (¬η2);

1 = (¬b1
V 1

1);

2 = (¬b1
V 1

2);

1 = (¬b2
V 2

1);

2 = (¬b2
V 2

2);

1 = (¬b3
V 3

1);

2 = (¬b3
V 3

2);

D1 := (¬η1 → (¬(b1

1 ∨ b3

1) ∨ ¬(b1

2 ∨ b3

2));

D2 := (¬η2 → ((b2

1 ∨ b3

1) ∧ (b2

2 ∨ b3

2))

3.5 Beyond CNF Rules

While CNF formulas are general enough to express every Boolean formula, the
length of representation may not be polynomial size. Therefore, one might won-
der if we can extend MLIC to learn rules in other canonical forms as well. In
fact, early CSP based approaches to rule learning focused on rules in DNF form.

We now show that with a minor change, we are able to learn rules expressible
in DNF. Suppose that we are interested in learning a rule S that is express-
ible in DNF, such that y = S(x), where S is a DNF formula. We note that
(y = S(x)) ↔ ¬(y = ¬S(x)). And if S is a DNF formula, then ¬S is a CNF
formula. Therefore, to learn rule S, we simply call MLIC with ¬y as input and
negate the learned rule.

3.6 Complex Objective Functions

We now discuss how MLIC can be easily extended to handle complex objective
functions. The objective function for MLIC as deﬁned in Equation 1 treats all
features equally. In some cases, the user might prefer rules that contain cer-
tain features. Such an extension is fairly easy to achieve as we need only to
change the weight function corresponding to clauses V j
i . Furthermore, in certain
cases, one might want to minimize the total number of diﬀerent features across
diﬀerent clauses rather than minimize the total number of terms. Such an ex-
tension is fairly easy to handle as we can simply replace (cid:86)k
i with ˆVi where
ˆVi = ((cid:87)k
i ). It is worth noting that the proposed modiﬁcations impact only
the MaxSAT query and does not require any modiﬁcations to the underlying
MaxSAT solver. We believe that such a separation is a key strength of MLIC as
it separates modeling and solving completely.

j=1 ¬bj

j=1 V j

4 Evaluation

To evaluate the performance of MLIC, we implemented a prototype implemen-
tation in Python that employs MaxHS [15] to handle MaxSAT instances. We
also experimented with LMHS [3], another state of the art MaxSAT solver and
MaxHS outperformed LMHS for our benchmarks 6. We conducted an extensive
set of experiments on diverse publicly available benchmarks, seeking to answer
the following questions7:

1. Do advancements in MaxSAT solving enable MLIC to be run with datasets
involving tens of thousands of variables with thousands of binary features?
2. How does the accuracy of MLIC compare to that of state of the art but

typically non-interpretable classiﬁers?

3. How does the accuracy of MLIC vary with the size of training set?
4. How does the accuracy of MLIC vary with λ?
5. How does the size of learnt rules of MLIC vary with λ?

In summary, our experiments demonstrate that MLIC can handle datasets
involving tens of thousands of variables with thousands of binary features. Fur-
thermore, MLIC can generate rules that are not only interpretable but with

6 A detailed evaluation among diﬀerent MaxSAT solvers is beyond the scope of this

work and left for future work

7 The source code of MLIC and benchmarks can be viewed at https://github.com/

meelgroup/mlic

accuracy comparable to that of other competitive classiﬁers, which often pro-
duce hard to interpret rules/models. We demonstrate that MLIC is able to
achieve suﬃciently high accuracy with very few samples.

Dataset

Size # Features RIPPER Log Reg NN

RF

SVC MLIC

TomsHardware

28170

830

Twitter

49990

1050

adult-data

32560

262

credit-card

30000

334

ionosphere

350

564

PIMA

760

134

parkinsons

190

392

Trans

740

64

WDBC

560

540

0.968
(92.8)

0.938
(187.3)

0.852
(0.5)

0.811
(0.7)

0.886
(0.1)

0.774
(0.1)

0.868
(0.1)

0.78
(0.0)

0.961
(0.1)

0.976
(0.2)

0.963
(0.2)

0.801
(0.3)

0.781
(0.1)

0.909
(0.1)

0.749
(0.1)

0.884
(0.1)

0.759
(0.0)

0.936
(0.0)

0.977
(3.4)

0.976
(64.9 )

Timeout

0.969
(2000)

0.965
(6.8)

0.962
(250.9 )

0.962
(1010.0)

0.958
(2000)

0.866
(3.0)

0.844
(41.8 )

Timeout

0.822
(3.9)

0.82
(25.5 )

Timeout

0.755
(2000)

0.82
(2000)

0.926
(1.2)

0.764
(1.3)

0.921
(1.2)

0.788
(1.2)

0.961
(1.3)

0.909
(1.3 )

0.761
(1.3)

0.895
(1.1)

0.788
(1.2 )

0.943
(1.4 )

0.886
(0.1 )

0.889
(15.04)

0.77
(21.4 )

0.736
(2000)

0.879
(1.6 )

0.895
(245)

0.765
(372.3 )

0.797
(1177)

0.955
(3.0 )

0.946
(911)

Table 1: Comparison of classiﬁcation accuracy with 10-fold cross validation for diﬀerent
classiﬁers. For every cell in the last ﬁve columns, the top value represents the accuracy,
while the value sorrounded by parenthesis represent average training time.

4.1 Experimental Methodology

We conducted extensive experiments on publicly available data sets obtained
from UCI repository [6]. The data sets involved both real- and categorical-
valued features. Speciﬁcally, the speciﬁc datasets are: buzz events from two
diﬀerent social networks: Twitter, Tom’s Hardware, Adult Data (adult data),
Credit Approval Data Set (credit data), Ionosphere (Ionos), Pima Indians Dia-
betes (PIMA), Parkinsons, connectionist bench sonar (Sonar), blood transfusion
service center (Trans), and breast cancer Wisconsin diagnostic (WDBC).

For purposes of comparison of the accuracy of MLIC, we considered a va-
riety of popular classiﬁers: (cid:96)1-penalized Logistic regression (LogReg), Nearest

Dataset

Size # Features RIPPER MLIC

TomsHardware

28170

Twitter

49990

adult-data

32560

credit-card

30000

ionosphere

PIMA

parkinsons

Trans

WDBC

350

760

190

740

560

830

1050

262

334

564

134

392

64

540

57.5

78.5

74.5

7.5

3

5

6.5

6

7.5

4

15

51.5

4

5.5

9

6

4

3.5

Table 2: Comparison of RIPPER vis-a-vis MLIC in terms of the size of Rules. Note
that despite using only a small number of literals, the proposed classiﬁer,
MLIC mostly has better accuracy than RIPPER.

neighbors classiﬁer (NN), and the black box random forests (RF), and support
vector classiﬁcation (SVC).

We perform 10-fold cross-validation to perform an assessment of accuracy on
a validation set. We compute the mean across the 10 folds for each choice of a
regularization (or complexity control) parameter for each technique (baseline and
MLIC), and report the best cross-validation accuracy. The number of parameter
values is comparable ( 10) for each technique. For RF and RIPPER we use control
based on the cutoﬀ of the number of examples in the leaf node. For SVC and
LogReg we discretize the regularization parameter on a logarithmic grid. In case
of MLIC we have 2 choices of λ ∈ {1, 10} and number of clauses, k ∈ {1, 2, 3}
and the type of rule as {CNF, DNF}. We set the training time cutoﬀ for each
classiﬁer (on each fold) to be 2000 seconds. Again, note that some classiﬁers
can be much faster than others, but in this paper we focus on the best tradeoﬀ
of accuracy vs interpretability in mission-critical settings, and the training time
(which can be oﬀ-line) is secondary, as long as it is realistic. In this context,
note that testing time for each of these techniques is less than 0.01 seconds for
a given set of labels.

4.2 Illustrative Example

We illustrate the interpertable rules that are computed by MLIC on the iris
data set, which is a simple benchmark and widely used by machine learning
community to illustrate new classiﬁcation techniques. We consider the binary
problem of classifying iris versicolor from the other two species, setosa and vir-
ginica. Of the four features, sepal length, sepal width, petal length, and petal
width, we learn the following rule: R:=

1. (sepal length > 6.3 ∨ sepal width > 3.0 ∨ petal width <= 1.5 ) ∧
2. ( sepal width <= 2.7 ∨ petal length > 4.0 ∨ petal width > 1.2 ) ∧

3. ( petal length <= 5.0)

Let us pause a bit to understand how to apply the above rule. The above
rule implies that when the three constraints are satisﬁed, the ﬂower must be
classiﬁed as Iris otherwise, non-iris. The size of the above rule, i.e. |R| = Σi|Ci| =
3 + 3 + 1 = 7.

4.3 Results

Fig. 1: Plot demonstrating behavior of training and test accuracy vs Size of Training
data for WDBC.

Table 1 presents results of comparison of MLIC vis-a-vis typical non-interpretable

classiﬁers. The ﬁrst three columns list the name, size (number of samples) and
the number of binary features for each Dataset. The next ﬁve columns present
test accuracy of the classiﬁers RIPPER, Logistic Regression (Log Reg), Nearest
Neighbor (NN), Random Forest (RF), and SVC. The ﬁnal column contain the
median test accuracy for MLIC. For every cell in the last ﬁve columns, the top
value represents the accuracy, while the value sorrounded by parenthesis rep-
resent average training time. We draw the following two conclusions from the
table: First, MLIC is able to handle datasets with tens of thousands of examples
with hundreds of features. The scalability of MLIC demonstrates the potential
presented by remarkable progress in SAT solving. Recent research eﬀorts have
often used NP-hardness of the problem to justify the usage of heuristics but our
experience with MLIC shows that SAT solving is able to solve many large-scale
problems directly. Note that when MaxHS times out, it is able to provide the
best solution found so far. In this context, it is worth noting that for some of
the benchmarks, even state of the art classiﬁers such as SVC time out. Secondly,
MLIC is often able to achieve accuracy that is suﬃciently close to accuracy

 0 0.02 0.04 0.06 0.08 0.1 0.12 0.1410%20%30%40%50%60%70%80%90%Test ErrorTraining Data Size %test:1.0train:1.0test:5.0train:5.0achieved by typical non-interpretable classiﬁers but produces easy to state rules
that often have just a few literals.

To demonstrate MLIC’s ability to compute easy to state rules in comparison
to the state of the art classiﬁers such as RIPPER, we computed the size of rules
returned by RIPPER and MLIC. Table 2 presents results of comparison of
MLIC vis-a-vis RIPPER. The ﬁrst three columns list the name, size (number
of samples) and the number of binary features for each Dataset. The next two
columns state the median size of rules returned by RIPPER and MLIC. The
size of a rule is computed as the number of terms involved in a rule. First, note
that except for two cases where RIPPER has produced marginally shorter rules
compared to MLIC, MLIC produces signiﬁcantly shorter rules and sometimes,
these rules could be orders of magnitude larger than those produced by MLIC.
For example, for Toms hardware, the rule produced by RIPPER has 57 terms
compared to just 4 literals for MLIC. Note that with MLIC has better accuracy
than RIPPER. One might wonder if the rule learned by RIPPER could have been
simply transformed into a sparser rule; it is not the case here. Furthermore, it is
worth noting that RIPPER does not provide sound handle to tune rule size and
therefore, user is left to trying out combination of input parameters without any
guarantee of improvement of the interpretability of generated rules, which we
experienced in this case. A in-depth study into failure of RIPPER to generate
sparser rules than MLIC is beyond the scope of this work.

To measure the accuracy of MLIC w.r.t. the size of training data, we consider
test errors when only a fraction of training data is available (we vary it from
10 % to 90 % in steps of 10 %). . Due to lack of space, we present result for
only one benchmark, WDBC, for λ = 1 and 5 and k = 1 in Figure 1. We plot
median training and test accuracy of MLIC over 10 trials, which is also known
as learning curve in machine learning literature. The y-axis represents the error
as the ratio of incorrect predictions to total examples while the x-axis represents
the size of training set. The plot shows how training and test error vary for
λ = 1 and 5. Note that MLIC is able to achieve suﬃciently high test accuracy
with just 40% of the complete dataset. We observe similar behavior for other
benchmarks as well.

Figures 2 and 3 illustrate how training accuracy and rule sizes vary with λ for
one of the representative benchmark, parkinsons. CNF1, CNF2, DNF1, DNF2
refer to invocations of MLIC with (rule type, k) set to (CNF, 1), (CNF, 2),
(DNF,1), and (DNF,2) respectively. For each of the plots, x-axis refers to the
value of λ while y-axis represents Rule size (i.e. |R|) and accuracy for Figure 3
and Figure 2 respectively. First, note that for both CNF and DNF, the accuracy
of rules is generally higher for larger k. Signiﬁcantly, the plots clearly demonstrate
monotonicity of rule size and accuracy with respect to λ. In contrast, the state
of the art interpretable classiﬁer, RIPPER, can lead to rules that can be order
of magnitude larger than those produced by MLIC. For example, for Toms
hardware, the rule produced by RIPPER has 57 terms compared to just 4 literals
for MLIC. In this context, it is worth noting that RIPPER does not provide
sound handle to tune rule size and therefore, user is left to trying out combination

Fig. 2: Plot demonstrating monotone behavior of training accuracy vs λ for CNF and
DNF rules with k = 1 and 2.

of input parameters without any guarantee of improvement of the interpretability
of generated rules.

5 Related Work

There is a long history of learning interpretable classiﬁcation models from data,
including popular approaches such as decision trees [5,29], decision lists [30], and
classiﬁcation rules [10]. While the form of such classiﬁers is highly amenable to
human interpretation, unfortunately, most of the objective functions that arise
for these problems are intractable combinatorial optimization problems. Hence,
most popular existing approaches rely on various greedy heuristics, pruning, and
ad-hoc local criteria such as maximizing information gain, coverage, e.t.c. For
example vaious popular decision rule approaches, such as C4.5.rules [29], CN2
[9], RIPPER [10], SLIPPER [11], all make diﬀerent trade-oﬀs in how they use
these heuristic criteria for growing and pruning the rules.

Recent advances in large-scale optimization and scalable Bayesian inference
gave rise to state-of-the-art black box models. However, many of the same ad-
vances can also be used in the context of interpretable machine learning models.
Some of such recent proposals include Bayesian approaches [23,35], constraint
programming [2], integer programming approaches to learn decision trees [4],
quadratic programming relaxation with a variance-penalized margin objective
[31]. Greedy approaches are used with a principled objective function in ENDER
[17] and Set covering machines [25]. [22] propose a hierarchical kernel learning
approach and [21] use optimization to combine basic Boolean clauses obtained
from decision trees. Linear Programming relaxations based on Boolean Com-
pressed Sensing formulation have been used to learn sparse interpretable rules

Fig. 3: Plot demonstrating behavior of rule size vs λ

and checklists8 in [24,18]. Prior work has considered applications of constraint
programming to learning Bayesian networks [2] and itemset mining [16,28]. In
contrast, we focus on learning sparse interpretable classiﬁcation rules allowing
control of accuracy vs. interpretability.

6 Extensions

In the paper, we have focused on decision rules in the DNF or CNF form, which
is among the most interpretable classiﬁcation methods available. We now de-
scribe a few related classiﬁcation formulations, which are also amenable to being
learned from data using a SAT-based framework. A simple AND-clause can be
considered as a requirement that all of the N literals in the clause are satisﬁed,
while a simple OR-clause requires that at least 1 of the N literals are satisﬁed.
A useful generalization is a “K-of-N” clause [13], which is true when at least K
of the N literals are satisﬁed. In particular, it leads to a very popular decision
rubric called checklists or scorecards, widely used in medicine and ﬁnance, where
a questionnaire asks some questions (e.g., risk factors), and the total number of
positive answers is compared to a pre-determined threshold. LP relaxations have
been considered for learning scorecards from data [19], and our MaxSAT-based
framework can be directly extended. In the case of multi-class classiﬁcation, a
decision rule may be ambiguous, as it does not specify what multi-class label to

8 Note, however, that the objective functions for the integer program and the LP
relaxation in these papers are not the same as sparsity-penalized cost-sensitive clas-
siﬁcation error.

use when several contradictory clauses pointing to diﬀerent labels are satisﬁed
simultaneously. Decision lists [30] enforces an order of evaluation of the rules,
resolving this ambiguity. Bayesian frameworks for learning decision lists have
been considered recently [23]. Perhaps the most well known interpretable classi-
ﬁcation scheme is a decision tree, where literals are arranged as nodes in a binary
tree, and a decision is made by following the path from the root node to one of
the leafs. The decision tree can be converted to an equivalent set of classiﬁcation
rules which correspond to all the paths from the root to the leafs, a more ex-
pensive representation. On the other side, however, certain small decision rules
can lead to very complex decision trees, for example, the ”K-of-N” rule cannot
be eﬃciently encoded using a decision tree. Recent work has considered combi-
natorial optimization to learn compact interpretable decision trees [4]. Beyond
simple Boolean expressions, a variety of weighted classiﬁcation methods can be
used, for example, a weighted linear combination of simple AND clauses – for
instance by using Boosting on a set of classiﬁers based on simple logical clauses.
In future work, we plan to extend our MaxSAT-based framework for all these
related interpretable classiﬁcation approaches.

7 Conclusion

We proposed a new approach to learn interpretable classiﬁcation rules via re-
duction to (MaxSAT). Due to the impressive advances in MaxSAT-solving, our
formulation can ﬁnd optimal or near-optimal rules balancing accuracy and inter-
pretability (sparsity) for large data-sets involving tens or hundreds of thousands
of data points, and hundreds or thousands of features. Furthermore, the ap-
proach separates the modeling from the optimization, and this framework could
be used to solve a wide variety of interpretable classiﬁcation formulations, includ-
ing decision lists, decision trees, and decision rules with diﬀerent cost functions
(including group-sparsity, sharing of the variables, and having prior knowledge
on variable importance). Finally, we demonstrate on experiments that for many
classiﬁcation problems interpretability does not have to come at a high cost in
terms of accuracy.

Furthermore, we hope to share our excitement with applications of constraint
programming/MaxSAT in Machine Learning, and to encourage researchers in
both interpretable classiﬁcation and in the CSP/SAT communities to consider
this topic further: both in developing new SAT-based formulations for inter-
pretable ML, and in designing bespoke solvers attuned to the problem of inter-
pretable ML.

Acknowledgements This work was supported in part by NUS ODPRT Grant,
R-252-000-685-133 and IBM PhD Fellowship. The computational work for this
article was performed on resources of the National Supercomputing Centre, Sin-
gapore https://www.nscc.sg

References

1. Andrews, R., Diederich, J., Tickle, A.: Survey and critique of techniques for ex-
tracting rules from trained artiﬁcial neural networks. Knowledge-based systems
8(6), 373–389 (1995)

2. van Beek, P., Hoﬀmann, H.F.: Machine learning of bayesian networks using con-

straint programming. In: Proc. of CP. pp. 429–445 (2015)

3. Berg, J., Saikko, P., J¨arvisalo, M.: Improving the eﬀectiveness of sat-based prepro-

cessing for maxsat. In: Proc. of IJCAI (2015)

4. Bertsimas, D., Chang, A., Rudin, C.: An integer optimization approach to asso-
ciative classiﬁcation. In: Adv. Neur. Inf. Process. Syst. 25, pp. 269–277 (2012)
5. Bessiere, C., Hebrard, E., O’Sullivan, B.: Minimising decision tree size as combi-

natorial optimisation. In: Proc. of CP. pp. 173–187. Springer (2009)

6. Blake, C., Merz, C.J.: {UCI} repository of machine learning databases (1998)
7. Boros, E., Hammer, P., Ibaraki, T., Kogan, A., Mayoraz, E., Muchnik, I.: An
implementation of logical analysis of data. IEEE Transactions on Knowledge and
Data Engineering 12(2), 292–306 (2000)

8. Breiman, L., Friedman, J., Stone, C., Olshen, R.: Classiﬁcation and regression

trees. CRC press (1984)

9. Clark, P., Niblett, T.: The CN2 induction algorithm. Mach. Learn. 3(4), 261–283

(Mar 1989)

10. Cohen, W.W.: Fast eﬀective rule induction. In: Proc. Int. Conf. Mach. Learn. pp.

115–123. Tahoe City, CA (Jul 1995)

11. Cohen, W.W., Singer, Y.: A simple, fast, and eﬀective rule learner. In: Proc. Nat.

Conf. Artif. Intell. pp. 335–342. Orlando, FL (Jul 1999)

12. Craven, M.W., Shavlik, J.W.: Extracting tree-structured representations of trained

networks. Proc. of NIPS pp. 24–30 (1996)

13. Craven, M.W., Shavlik, J.W.: Extracting tree-structured representations of trained

networks. Proc. of NIPS pp. 24–30 (1996)

14. Davies, J.: Solving MAXSAT by Decoupling Optimization and Satisfaction. Ph.D.

thesis, University of Toronto (2013)

15. Davies, J., Bacchus, F.: Solving maxsat by solving a sequence of simpler sat in-

stances. In: Proc. of CP. pp. 225–239 (2011)

16. De Raedt, L., Guns, T., Nijssen, S.: Constraint programming for itemset mining.

In: Proc. of KDD. pp. 204–212 (2008)

17. Dembczy´nski, K., Kot(cid:32)lowski, W., S(cid:32)lowi´nski, R.: Ender: a statistical framework
for boosting decision rules. Data Mining and Knowledge Discovery 21(1), 52–90
(2010)

18. Emad, A., Varshney, K.R., Malioutov, D.M.: A semiquantitative group testing ap-
proach for learning interpretable clinical prediction rules. In: Proc. Signal Process.
Adapt. Sparse Struct. Repr. Workshop, Cambridge, UK (2015)

19. Emad, A., Varshney, K.R., Malioutov, D.M.: A semiquantitative group testing ap-
proach for learning interpretable clinical prediction rules. In: Proc. Signal Process.
Adapt. Sparse Struct. Repr. Workshop, Cambridge, UK (2015)

20. Freitas, A.: Comprehensible classiﬁcation models: a position paper. ACM SIGKDD

explorations newsletter 15(1), 1–10 (2014)

21. Friedman, J.H., Popescu, B.E.: Predictive learning via rule ensembles. The Annals

of Applied Statistics pp. 916–954 (2008)

22. Jawanpuria, P., Jagarlapudi, S.N., Ramakrishnan, G.: Eﬃcient rule ensemble learn-

ing using hierarchical kernels. In: Proc. of ICML (2011)

23. Letham, B., Rudin, C., McCormick, T.H., Madigan, D.: Building interpretable
classiﬁers with rules using Bayesian analysis. Tech. Rep. 609, Dept. Stat., Univ.
Washington (Dec 2012)

24. Malioutov, D.M., Varshney, K.R.: Exact rule learning via boolean compressed sens-

ing. In: Proc. of ICML. pp. 765–773 (2013)

25. Marchand, M., Shawe-Taylor, J.: The set covering machine. Journal of Machine

Learning Research 3(Dec), 723–746 (2002)

26. Morgado, A., Heras, F., Liﬃton, M., Planes, J., Marques-Silva, J.: Iterative and
core-guided maxsat solving: A survey and assessment. Constraints 18(4), 478–534
(Oct 2013)

27. Narodytska, N., Bacchus, F.: Maximum satisﬁability using core-guided maxsat

resolution. In: AAAI. pp. 2717–2723 (2014)

28. Nijssen, S., Guns, T., De Raedt, L.: Correlated itemset mining in roc space: a

constraint programming approach. In: KDD. pp. 647–656. ACM (2009)

29. Quinlan, J.R.: C4. 5: Programming for machine learning. Morgan Kauﬀmann p. 38

(1993)

30. Rivest, R.L.: Learning decision lists. Mach. Learn. 2(3), 229–246 (Nov 1987)

31. R¨uckert, U., Kramer, S.: Margin-based ﬁrst-order rule learning. Mach. Learn. 70(2–

3), 189–206 (Mar 2008)

32. Valiant, L.G.: Learning disjunctions of conjunctions. In: Proc. Int. Joint Conf.

Artif. Intell. pp. 560–566. Los Angeles, CA (Aug 1985)

33. Varshney, K.R.: Data science of the people, for the people, by the people: A view-
point on an emerging dichotomy. In: Proc. Data for Good Exchange Conf (2015)

34. Wang, T., Rudin, C., Doshi-Velez, F., Liu, Y., Klampﬂ, E., MacNeille, P.: Or’s
of and’s for interpretable classiﬁcation, with application to context-aware recom-
mender systems. arXiv preprint arXiv:1504.07614 (2015)

35. Wang, T., Rudin, C., Liu, Y., Klampﬂ, E., MacNeille, P.: Bayesian or’s of and’s for
interpretable classiﬁcation with application to context aware recommender systems
(2015)

